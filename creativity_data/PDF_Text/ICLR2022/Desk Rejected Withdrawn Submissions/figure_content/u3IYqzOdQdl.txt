Figure 1: An overview of our MiXtUreEnsembles method. The parameter bank holds templatesshared by all ensemble members. The layer weights of each ensemble member are composed as alinear combination of the template parameters. The exact combination strategy is determined by thelinear combination weights α and is learned automatically together with the weights. Therefore,each ensemble member only costs a small number of additional parameters - the size of “com-biner weights” α and the member-specific BatchNorm parameters, making this a memory efficientensembling method.
Figure 2: The MixtureEnsemble training and infeerence pipeline. Each ensemble member is in-stantiated from a shared parameter bank, and is trained independently on the same samples with across-entropy loss. At inference, we average predictions from the members.
Figure 3: On the left, we show the effect of scaling parameters for different efficient ensemblingmethods. MixtureEnsembles scale down most gracefully. On the right, we show the effect of numberof ensemble members for CIFAR-100. We find that across a number of parameter sizes, ensemblesize 4 is best.
Figure 4: Linear interpolations in parameter space, with different numbers of parameters in anensemble of size 4. We plot accuracy vs interpolation point. Because the accuracy takes a significantdip, we can tell that the ensemble members are independent and find different local minima.
