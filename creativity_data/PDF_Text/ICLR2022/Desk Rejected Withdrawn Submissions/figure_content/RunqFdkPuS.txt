Figure 1: The proposed self-supervised modeljointly learns two types of distinct features:modality-invariant features and modality-specificfeatures. The modality-invariant features capturefeatures for multiple modalities in the same met-ric space making the cross-modal retrieval taskpossible, while the modality-specific features en-code complementary information among differentmodalities and the fusion of these features can beused for downstream tasks such as recognition.
Figure 2: An overview of the proposed self-supervised modality-invariant and modality-specificfeature learning for 3D objects. The hidden features for mesh, point cloud, and image are extractedby corresponding encoders, then these hidden features for each modality are mapped into two spacesincluding a universal space for capturing modality-invariant features and a private feature space foreach modality for capturing modality-specific features. The self-supervised learned features can befurther used for various downstream tasks such as 3D cross-modal retrieval and multimodal fusionfor 3D recognition.
Figure 3: The qualitative visualization of modality-specific and modality-invariant features on theModelNet40 dataset.
