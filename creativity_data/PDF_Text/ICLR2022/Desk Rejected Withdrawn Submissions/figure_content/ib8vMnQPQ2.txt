Figure 1:	Comparison between conventional digital systems (left) and the processing-in-memory(PIM) systems (right). Unlike conventional digital systems where quantization is only applied onceon both inputs and weights for efficient integer convolution, processing-in-memory (PIM) systemsrequire an extra quantization due to limited resolution of the analog-to-digital interface. To reduceits effect, the whole convolution is typically decomposed into smaller one and the final results arerecombined together. This extra quantization step significantly deteriorates the accuracy performanceof model running on it and is the main topic of our work. Note the input to PIM quantization can havea much smaller range than 32-bit integers and here we use “INT32” to denote the most general case.
Figure 2:	The problem of implementing MAC on a processing in-memory (PIM) system and ourproposed solution. Compared to conventional digital systems, the extra low-resolution quantizationstep in PIM systems introduces significant information loss, making those models trained withconventional quantization techniques fail. The two non-idealities of the extra quantization step,namely imperfect linearity (which We simply denote as non-linearity) and stochastic thermal noise,further aggravate the errors of PIM-quantization. On the contrary, our method takes into account theideal PIM quantization during training, and applies BN calibration and adjusted precision trainingalgorithm to alleviate the impact of the two non-ideal effects. Note that non-idealities are not directlymodeled during training because PIM quantization in different chips exhibits different linearity andnoise behaviors due to inter-die variations. Therefore, training with limited non-ideal samples maylead to biased results. Our techniques reduce the accuracy gap and improve the robustness for realPIM systems.
Figure 3: Computing error as a function of thestandard deviation of additive noise in our 7-bitPIM chip.
Figure 4: Performance of ResNet20 on CIFAR10 with ideal PIM of different schemes and PIMresolutions. Note that N = 9 for native and N = 144 for differential and bit serial schemes.
Figure 5: The desirable training resolutions (TR)for different inference resolutions (IR) and noiselevels, with bit-serial scheme.
Figure A2: Impact of PIM quantization on the output scale measured by standard deviation. ρ isdefined as in (5d).
Figure A3: Impact of nonlinearity and noise non-idealities on the running statistics. Note this is onesampling results.
Figure A4: Comparing our method of PIM-quantization-aware training with baseline on idealizednoiseless bit-serial PIM systems with different resolutions.
Figure A5: Learning curve comparison with ResNet20 on CIFAR10 for bit-serial PIM system.
Figure A6: Effect of BN calibration for bit-serial PIM systems with idealized and real curve quantiza-tion. We can find BN calibration helps for both our method and the baseline.
Figure A7: Generated idealized 7bit curves with gain and offset variations, where N = 72 for (a)and N = 144 for (b). Gains and offsets are both sampled with normal random distributions, whereNoffset 〜(0,2.04) and Ngain 〜(1,0.024). The standard deviations for them are determined basedon real-chip testing results.
