Under review as a conference paper at ICLR 2022
The Number of Steps Needed for Nonconvex
Optimization of an Optimizer is a Rational
Function of Batch Size
Anonymous authors
Paper under double-blind review
Ab stract
Recently, convergence as well as convergence rate analyses of optimizers for non-
convex optimization have been widely studied. Meanwhile, numerical evaluations
for the optimizers have precisely clarified the relationship between batch size and
the number of steps needed for training deep neural networks. The main contri-
bution of this paper is to show theoretically that the number of steps needed for
nonconvex optimization of each of the optimizers can be expressed as a rational
function of batch size. Having these rational functions leads to two particularly
important facts, which were validated numerically in previous studies. The first
fact is that there exists an optimal batch size such that the number of steps needed
for nonconvex optimization is minimized. This implies that using larger batch
sizes than the optimal batch size does not decrease the number of steps needed
for nonconvex optimization. The second fact is that the optimal batch size de-
pends on the optimizer. In particular, it is shown theoretically that momentum and
Adam-type optimizers can exploit larger optimal batches and further reduce the
minimum number of steps needed for nonconvex optimization than the stochastic
gradient descent optimizer.
1	Introduction
One way to train deep neural networks is to find the model parameters of the deep neural networks
that minimize loss functions called the expected risk and empirical risk using first-order optimiza-
tion methods (Bottou et al., 2018, Section 4). The simplest optimizer is stochastic gradient descent
(SGD) (Robbins & Monro, 1951; Zinkevich, 2003; Nemirovski et al., 2009; Ghadimi & Lan, 2012;
2013). There have been many deep learning optimizers to accelerate SGD, such as momentum
methods (Polyak, 1964; Nesterov, 1983) and adaptive methods, e.g., Adaptive Gradient (AdaGrad)
(Duchi et al., 2011), Root Mean Square Propagation (RMSProp) (Tieleman & Hinton, 2012), Adap-
tive Moment Estimation (Adam) (Kingma & Ba, 2015), and Adaptive Mean Square Gradient (AMS-
Grad) (Reddi et al., 2018) (Table 2 in (Schmidt et al., 2021) lists useful deep learning optimizers).
Convergence and convergence rate analyses of deep learning optimizers have been widely studied
for convex optimization (Zinkevich et al., 2010; Kingma & Ba, 2015; Reddi et al., 2018; Luo et al.,
2019; Mendler-Dunner et al., 2020). Meanwhile, theoretical investigation of deep learning optimiz-
ers for nonconvex optimization is needed so that these optimizers can be put into practice for non-
convex optimization in deep learning (Xu et al., 2015; Arjovsky et al., 2017; Vaswani et al., 2017).
Convergence analyses of SGD for nonconvex optimization were studied in (Fehrman et al.,
2020; Chen et al., 2020; Scaman & Malherbe, 2020; Loizou et al., 2021) (see (Gower et al., 2021;
Loizou et al., 2021) for convergence analyses of SGD for two classes of nonconvex optimization
problems, quasar-convex and Polyak-LojasieWicz optimization problems). For example, Theorem
11in (Scaman & Malherbe, 2020) indicates that SGD with a diminishing learning rate ak = 1 /√k
has O(1 /yJ~K) convergence, where K denotes the number of steps. Convergence analyses of SGD
depending on the batch size were presented in (Chen et al., 2020). In particular, Theorem 3.2 in
(Chen et al., 2020) indicates that running SGD with a diminishing learning rate αk = 1/k and large
batch size for sufficiently many steps leads to convergence to a local minimizer of a sum of loss
functions.
1
Under review as a conference paper at ICLR 2022
Figure 1: Relationships between the optimiz-
ers in terms of the results in Table 1 (relations
s?C,SGD ≤ s?C,NM ≤ s?C,A hold generally,
but those of K (s?C,SGD) ≥ K (s?C,NM) ≥
K (s?C A) depend on momentum coefficient
β)
Figure 2: Relationships between the optimiz-
ers in terms of the results in Table 2 (relations
s?D,SGD = s?D,NM ≤ s?D,A hold generally,
but those of K (s?D,SGD) ≥ K (s?D,NM ) ≥
K(s?D,A)depend on momentum coefficient
β)
Convergence analyses of adaptive methods for nonconvex optimization were studied in (Fang et al.,
2018; Chen et al., 2019; Zhuang et al., 2020; Iiduka, 2021). In (Chen et al., 2019), it was shown that
generalized Adam, which includes the Heavy-ball method, AdaGrad, RMSProp, AMSGrad, and
AdaGrad with First Order Momentum (AdaFom), using a diminishing learning rate ak = 1 /∖Tk
has an O (log K/ √K) convergence rate. AdaBelief (named for adapting stepsizes by the belief in
observed gradients) using ak = 1 /Vk has O (log K//K) convergence (Zhuang etal., 2020). In
(Iiduka, 2021), a method was presented to unify useful adaptive methods such as AMSGrad and
AdaBelief, and it was shown that the method with ak = 1 /√k has an O(1 /√K) convergence rate,
which improves on the results in (Chen et al., 2019; Zhuang et al., 2020).
Meanwhile, in (Shallue et al., 2019), it was studied how increasing the batch size affects the per-
formances of SGD, SGD with momentum (Polyak, 1964; Rumelhart et al., 1986), and Nesterov
momentum (Nesterov, 1983; Sutskever et al., 2013). The relationships between batch size and per-
formance for Adam and K-FAC (Kronecker-Factored Approximate Curvature (Martens & Grosse,
2015)) were studied in (Zhang et al., 2019). In both studies, it was numerically shown that increas-
ing batch size tends to decrease the number of steps K needed for training deep neural networks,
but with diminishing returns. Moreover, it was shown that SGD with momentum and Nesterov mo-
mentum can exploit larger batches than SGD (Shallue et al., 2019), and that K-FAC and Adam can
exploit larger batches than SGD with momentum (Zhang et al., 2019). Thus, it was shown that mo-
mentum and adaptive methods can significantly reduce the number of steps K needed for training
deep neural networks (Shallue et al., 2019, Figure 4), (Zhang et al., 2019, Figure 5). In (Smith et al.,
2018), it was numerically shown that using enormous batch sizes leads to reducing the number of
parameter updates and model training time.
1.1	Motivation
As indicated above, the performance of the optimizer strongly depends on not only learning rate αk
but also the batch size, s. However, the previous studies did not clarify any relationship between αk
and s. Moreover, the previous studies did not show theoretically the relationship between batch size
s and the performance of the optimizer. Hence, the motivation of this paper is to clarify theoretically
the relationship between αk and s and that between s and the number of steps, K, needed for
nonconvex optimization of the optimizer.
1.2	Notation
N denotes the set of nonnegative integers. Let n ∈ N\{0}. We define [n] := {1, 2, . . . , n}. Rd
denotes d-dimensional Euclidean space with inner product〈,, ∙∖ inducing norm ∣∣∙∣. Let S；+ be the
set of d × d symmetric positive-definite matrices and let Dd be the set of d × d diagonal matrices:
2
Under review as a conference paper at ICLR 2022
Dd = {M ∈ Rd×d : M = diag(xi), xi ∈ R (i ∈ [d])}. For a random variable Z, we use E[Z] to
indicate its expectation. Throughout this paper, let e > 0, α > 0, b :=1 一 b (b ∈ (0,1)), 7 := 1 一 Y
(γ ∈ [0, 1)), and H ≥ h0 > 0. The number of samples is denoted by n, Li is the LiPschitz
constant of Pfi: Rd → Rd (i ∈ [n]), and L denotes the maximum value of Li. Nf: Rd → Rd
denotes the gradient of a nonconvex loss function f : Rd → R. D is the upper bound of (xk,i 一 xi)2
((xi) ∈ Rd), where (xk)k∈N = ((xk,i))k∈N is generated by an optimizer. Aα and Bα are positive
constants depending on a learning rate α and Cβ is a positive constant depending on a momentum
coefficient β (see Theorem 3.1 for detailed definitions of the constants). A batch size s? is said to
be optimal if the number of steps K needed for nonconvex optimization is minimized at s? .
1.3	Contribution
The contribution of this paper is to construct a theory guaranteeing the useful numerical results in
(Shallue et al., 2019; Zhang et al., 2019). Table 1 (resp. Table 2) summarizes our results for SGD,
Nesterov momentum (N-Momentum), and Adam-type optimizers with a constant learning rate rule
(resp. diminishing learning rate rule), described in Theorem 3.1 (resp. Theorem 3.2). See Theorem
A.2 in Appendix for other result for the optimizers with a diminishing learning rate rule. Figure 1
(resp. Figure 2) visualizes the relationships between the optimizers for the results shown in Table 1
(resp. Table 2) for an appropriately set momentum coefficient β .
Table 1: Relationship between batch size s and the number of steps K needed for nonconvex
optimization in the sense of (1) of optimizers with constant learning rates
	Constant Learning Rate Ru 	(a® (S) = I, βk = β ∈ [0,b] ⊂		e [0, 1))
	Rational Function	Optimal Batch Size s?	Minimum Steps Ke (S?)
SGD N-Momentum Adam-type	K = 4s 二 「2 S1 - B ɑ K =	4s		dDL 2 n 2 a -e- dDL2n2a	(dDLn )2 e 4 (dDLn )2
	e = (e2 - Ce ) S - Ba K = ʒ_Aas-		7be2 - dDLnβ dDL2n2a	(7be2 - dDLnβ)2 (dDLn )2 H
	e	( e2 - Ce ) S- Ba	72(be2 - dDLnβ[ h]	Y2(be2 - dDLnβ)2h]
Table 2: Relationship between batch size s and the number of steps K needed for nonconvex
optimization in the sense of (1) of optimizers with diminishing learning rates
	Diminishing Learning Rate Rule (ak(S) = ^⅛, βk = lβ ∈ [0, b] ⊂ [0, I)) Ik					
	Rational Function				Optimal Batch Size s?	MinimUmStePS Ke(s?)
SGD	Ke	AaS2	+ Ba	r	2L Lna	2( dDLn )2
		=e	2S-	ʃ		-e4-
N-Momentum	Ke	AaS2	+ Ba	2	√2 Lna	2( dDLn )2
		=	(e2-	CF	ʃ 2		(7be2 - dDLnβ)2
Adam-type	Ke	AaS2	+ Ba		2L Lna	2( dDLn )2 H
		=	(e2-	CF	ʃ	YpHh	γ2 (be2 - dDLnβ) h0
The main contribution of this paper is to clarify that the number of steps K = K needed for
nonconvex optimization in the sense of an -approximation, 1
min E kNf (xk)k2 ≤ 2,	(1)
k∈[K]
of one of SGD, N-Momentum, and Adam-type optimizers can be expressed as a rational function of
batch size s (see the “Rational Function” columns of Tables 1 and 2).
Jensen's inequality guarantees that (1) implies that min k∈ [ K ] E [∣∣Vf (Xk) k ] ≤ e.
3
Under review as a conference paper at ICLR 2022
The explicit forms of the rational functions imply the following two significant facts:
(I)	There exists an optimal batch size s? such that K (s) is minimized. This fact guarantees
theoretically the existences of the diminishing returns shown in (Shallue et al., 2019, Figure
4), (Zhang et al., 2019, Figure 8), which are such that increasing the batch size does not
decrease the number of steps K.
(II)	The optimal batch size s? and the minimum number of steps K (s?) depend on the opti-
mizer. In particular, N-Momentum and Adam-type optimizers can exploit the same sized
or larger batches (s? in Tables 1 and 2 and Figures 1 and 2) than can SGD. Furthermore,
the dependence of N-Momentum and Adam-type optimizers on β allows them to reduce
the minimum number of steps (K (s?) in Tables 1 and 2 and Figures 1 and 2) more than
can SGD (see Section 3 for details).
The relationship between αk and s and the existence of the optimal batch size s? lead to the learning
rate ak (e.g., the constant learning rate is ak = a/s?) for nonconvex optimization in the sense of an
-approximation (1). This result justifies that this nonconvex optimization requires not only for the
batch size to be set appropriately but also the learning rate.
Comparisons of Optimal Batch Sizes for Different Learning Rate Rules
Tables 1 and 2 ensure that K(s?) for the optimizer using constant learning rates is almost the
same as K(s?) for the optimizer using diminishing learning rates. Meanwhile, we would like to
emphasize that the optimal batch size s?C for the optimizer using constant learning rates depends on
β, and the optimal batch size s?D for the optimizer using diminishing learning rates does not depend
on β. For example, under the precision accuracy = 10-1, we can know the optimal batch sizes for
N-Momentum with the frequently used parameter value α = 10-3 are respectively
sC,NM
dDL2n23	?
注2 - dDLnβ and SD，NM
√2 Lne 3
before implementing N-Momentum.
2 Nonconvex Optimization and Optimizers
This section provides the assumptions used in this paper and states optimizers for a nonconvex
optimization problem under the assumptions.
2.1	Assumptions Regarding Loss Function and Gradient Estimation
This paper considers optimization problems under the following assumptions.
Assumption 2.1
(A1) [Loss function] fi : Rd → R (i ∈ [n]) is differentiable and f : Rd → R is defined for all
x ∈ Rd by
1n
f (x) := nff (x),
i=1
where n denotes the number of samples.
(A2) [Gradient estimation] For each iteration k, optimizers sample a batch Sk ⊂ [n] of size
s := ∣Sk I independently of k and estimate the full gradient ^f as
WSk ：= s X ^fi.
i∈Sk
(A3) [Gradient boundedness] There exists a positive number G such that, for all x ∈ X,
E [k^fSk (x)k2] ≤ G2,	(2)
where X is a subset of Rd.
4
Under review as a conference paper at ICLR 2022
Assumption (A1) is a standard one for nonconvex optimization in deep neural networks (see, e.g.,
(Chen et al., 2019, (2)) and (Fang et al., 2018, (1.2))). Assumption (A2) is needed for the optimizers
to work (see, e.g., (Chen et al., 2019, Section 2) and (Fang et al., 2018, Notation section)). Assump-
tion (A3) is used to analyze the optimizers (see, e.g., (Chen et al., 2019, Section 2)). Assumption
(A3) holds if each of the following holds:
(G1) X ⊂ Rd is bounded, the gradient ▽% is LiPschitz continuous with LiPschitz constant Li,
and Si := {x^ ∈ Rd: Pfi (x^) = 0} = 0 (i ∈ [n]), where L := maxi∈[n] Li. (If We
define GkL ：=Sup x∈x P i∈s"∣Vfi (x) k ,thenWecantake G := sup k∈ N GkL ∙)
(G2) X ⊂ Rd is bounded and closed. (If we define Gk := supx∈X Pi∈Sk kPfi(x)k, then we
can take G := supk∈N Gk.)
For examPle, under (G1), the LiPschitz continuity of Pfi , together with the definition of L, ensures
that, for all x ∈ Rd and all i ∈ [ n ], INfi (x) ∣∣ = ∣∣Vfi (x)-Pfi (x^) ∣∣ ≤ Likx- x^k ≤ Lkx-x*k ∙
Accordingly, Pi∈Sk ∣Vfi(x)∣∣ ≤ Ls∣x - x^∣ ≤ Ln∣x - x*∣. The definition of VfSk and
the triangle inequality imPly that there existsG such that ∣VfSk(x)∣2 ≤ ( i∈Sk ∣Vfi(x)∣)2 ≤
(Ln∣x - x^k)2 ≤ G2, i.e., (A3) holds (see Proposition A.1 in Appendix for details).
2.2	Nonconvex Optimization
This paper considers the following problem (Chen et al., 2019; Zhuang et al., 2020).
Problem 2.1 Under Assumption 2.1, we would like to find a point x? ∈ Rd such that
x* ∈ X? := {X ∈ Rd: Vf (x) = 0}.
See the third and fourth paragraphs of Section 1 for the previous studies on Problem 2.1.
2.3	Optimizers
There are many optimizers (Schmidt et al., 2021, Table 2). In this paper, we consider the fol-
lowing algorithm (Algorithm 1), which is a unified algorithm for useful optimizers, for example,
N-Momentum (Nesterov, 1983; Sutskever et al., 2013), AMSGrad (Reddi et al., 2018; Chen et al.,
2019), AMSBound (Luo et al., 2019), modified Adam (M-Adam) (Kingma & Ba, 2015; Iiduka,
2021), and AdaBelief (Zhuang et al., 2020), listed in Table 4 in Appendix.
Algorithm 1 Optimizer for solving Problem 2.1
Require: (αk)k∈N ⊂ (0, +∞), (βk)k∈N ⊂ [0, b] ⊂ [0, 1), γ ∈ [0, 1)
1:
2:
k J 0, xo, m-1 := 0 ∈ Rd, Ho ∈ S；十 ∩ Dd, & ⊂ [n]
loop
3:
4:
5:
6:
7:
8:
mk := βkmk-1 + (1 - βk)VfSk (xk)
mn
1 — Yk+1
Hk ∈ Sd++ ∩ Dd (see Table 4 for examples of Hk)
Find dk ∈ Rd that solves Hkd = -r^k
xk+1 := xk + αkdk
kJk+1
m k
9: end loop
The useful optimizers, such as N-Momentum, AMSGrad, AMSBound, M-Adam, and AdaBelief
(Table 4), all satisfy the following conditions:
Assumption 2.2 The sequence (Hk)k∈N ⊂ Sd++ ∩ Dd, with Hk := diag(hk,i), in Algorithm 1
satisfies the following conditions:
(A4) hk+1,i ≥ hk,i for all k ∈ Nandalli ∈ [d];
(A5) For all i ∈ [d], a positive number Hi exists such that supk∈N E[hk,i] ≤ Hi.
5
Under review as a conference paper at ICLR 2022
Moreover, the following condition holds:
(A6) D := maxi∈[d] supk∈N (xk,i - xi)2 < +∞, where x := (xi) ∈ Rd and (xk)k∈N :=
((xk,i))k∈N is the sequence generated by Algorithm 1.
We define
h∩ := min h0 i and H := max Hi,
i∈[d]	i∈[d]
where hk,i and Hi are defined as in Assumption 2.2. Assumption (A6) is assumed in
(Nemirovski et al., 2009, p.1574), (Kingma & Ba, 2015, Theorem 4.1), (Reddi et al., 2018, p.2),
(Luo et al., 2019, Theorem 4), and (Zhuang et al., 2020, Theorem 2.1). If (A6) holds, then there ex-
ists a bounded set X ⊂ Rd such that (Xk)k∈N ⊂ X. Accordingly, the LiPschitz continuity of Vfi,
the nonemptiness of Si, and (A6) (i.e., (G1)) imply that (A3) with G := LnVdD holds (See Propo-
sition A.1 in APPendix for details). The Previous results in (Chen et al., 2019, P.29), (Zhuang et al.,
2020, p.18), and (Iiduka, 2021) show that (Hk)k∈N in Table 4 satisfies (A4) and (A5). For example,
AMSGrad (resp. M-Adam) satisfies (A4) and (A5) with H = √M (resp. H = PM/(1 一 Q),
where M := supk∈N kVfSk (xk) VfSk (xk)k < +∞ (Iiduka, 2021). AMSBound coincides with
AMSGrad using H = supk∈N l- 1. In general, the performance of AMSGrad with H = √M differs
from the that of AMSBound (i.e., AMSGrad with H = supk∈N lk-1 ) since the optimal batch size and
the minimum number of steps depend on H, as seen in Tables 1 and 2 (see also (Luo et al., 2019,
Section 5) for numerical comparisons of AMSGrad and AMSBound).
3 Main Results
This section gives our results (Theorems 3.1 and 3.2) indicating the relationship between batch size
S and the number of steps Ke needed for (1) for Algorithm 1 with each of constant and diminishing
learning rates (see Tables 1 and 2 for the specific results in Theorems 3.1 and 3.2 with G := Ln/dD
(i.e., under condition (G1))).
3.1 Constant Learning Rate Rule
Theorem 3.1 Suppose that Assumptions 2.1 and 2.2 hold and let > 0 and α ∈ (0, 1].
(i)	Consider Algorithm 1 with
αk = αk (S):= α (s > 0) and βk := β ∈ [0, b] ⊂ [0,1).
s
Then, for all K ≥ 1 and all S > 0,
km∈[iKn]E kVf(xk)k2
dDH s	G2 α	1	VdDGR
≤ 2(1 — b)α K + 2(1 — b)(1 一 Y)2h∩ S + 1 - b β
|--{z---}	|------V-------}	|---{---}
Aα	Bα	Cβ
(ii)	Consider Algorithm 1 with
αk = αk(S)
α (s > 0) and βk := β < min < :
S	dDG
e2,b .
Then, the number of steps Ke needed to achieve an -approximation (1) is expressed as the following
rational function of batch size S:
Ke(S)
AαS2
(2 一 Cβ )S 一 Bα
s ∈ e2
Bα-r, + ∞
(3)

In particular, the minimum value of Ke needed to achieve (1) is
4AαBα
Ke(S?)
dDG2H
(e2 - Ce)2 - (1 - Y)2{(1 - b)e2 - VdDGβ}h0
when
? __ 2Bα __	G2 α
s = e2 - Ce = (1 - Y)2{(1 - b)e2 - √dDGβ}h0 '
6
Under review as a conference paper at ICLR 2022
3.1.1 Discussion of Theorem 3.1
min E [ k^f (xk)k 2] ≤
k∈[K]
[Performance of Algorithm 1] SGD is Algorithm 1 with β = Y = 0 and h0 = H = 1, N-
Momentum is Algorithm 1 with Y = 0 and hq = H = 1, and the Adam-type optimizer is Algorithm
1 with γ ∈ [0, 1) and h%i defined by one of y∕^,i, Vk,i, and yj&k,i (See Table 4). Theorem 3.1(i)
indicates that, for all K ≥ 1, all α ∈ (0, 1], all β ∈ [0, b] ⊂ [0, 1), and all s > 0,
dDSSD K + Ga1	(SGD)，
dDNM S I GG S 1 _|_ VdDNM G R
2(1 -b)a K + 2(1 -b) S +	1-b —β	(N-MOmentUm)，	(4)
dDA H S _|_____G2 S______1 I √dDA G R
2(1 -b)a K + 2(1 -b)(1 -Y)2h. S + T-b~β	(Adam-type)∙
Note that D depends on the optimizer, which we distinguish by the notation DSGD, DNM, and DA .
For fixed s, ifα and β are sufficiently small, (4) indicates that SGD, N-Momentum, and Adam-type
optimizers have approximately O(1/K) convergence. For fixed s and K, if α is sufficiently small,
the second term on the right-hand side of (4) will be small, whereas the first term will be large.
Hence, there is no evidence that Algorithm 1 with a sufficiently small learning rate α would perform
arbitrarily well. For fixed α and K, if s is sufficiently large, again the second term of the right-hand
side of (4) will be small and the first term will be large. Hence, (4) indicates that there is no evidence
that Algorithm 1 with a large batch size s performs better than with a smaller batch size.
[Existence of optimal batch size] The function K(s) defined by (3) satisfies the following:
d Ke( S )
d S
<0
=0
>0
if S ∈
if S = S
B ?
“Cβ ,s ;
? — 2 Ba
=^-C
if S ∈ (S?， +∞).
The above shows that increasing the batch size initially decreases the number of steps K needed
to achieve (1). Then, there is an optimal batch size (S = S?) minimizing K(S). We note that the
optimal batch size depends on the upper bound defined by the right-hand side of (4).
[Comparison of optimal batch sizes] We assume that SGD, N-Momentum, and Adam-type op-
timizers all use the same G. For example, under (G1), We have G = LnyfdD, where D =
max{DSGD， DNM， DA}. From Theorem 3.1(ii), we find that
?	G2α	?	G2α
SC,SGD =下 ≤ sC,NM = (1 - b)e2 -√dDNMGβ ■
Moreover, if (1 - Y)2 ≤ 1/hq0, then we have that
?	_ G2 a / ？ _	G2 a
sC,SGD = F ≤ Sc,a = (1 - Y)2{(1 - b)e2 - √DAGβ}h0 ■
Moreover, if (1 - Y)2 ≤ 1/hq0 holds and if DNM ≤ DA, then
(5)
(6)
s?C,NM ≤ s?C,A ■
The right-hand side of (4) with β = 0 is the smallest. Hence, it might seem that N-Momentum and
Adam-type optimizers are not useful. However, using β 6= 0 leads to the finding that N-Momentum
and Adam-type optimizers exploit larger batches than SGD, as seen in (5) and (6).
[Comparison of minimum numbers of steps] Theorem 3.1(ii) guarantees that the dependence of
N-Momentum and Adam-type optimizers of β allows them to satisfy that
K(s?C,A) ≤ K(s?C,NM) ≤ K(s?C,SGD)	(7)
(see (27), (28), (29), (30), (31), and (32) in Appendix).
3.2 Diminishing Learning Rate Rule
Theorem 3.2 Suppose that Assumptions 2.1 and 2.2 hold and let > 0 and α ∈ (0， 1].
(i)	Consider Algorithm 1 with
α
αk = αk (s) := ~~k (s > 0) and βk ：= β ∈ [0，b] ⊂ [0，1) ■
7
Under review as a conference paper at ICLR 2022
Then, for all K ≥ 1 and all s > 0,
2mE D∣V∕('Q k2]
dDH S	G 2 α	1	VdDGC
≤-------------+--------------------+ ------β
≤ 2(1 - b)α √K +(1 - b)(1 - Y)2h0 s√K + 1 - b β
|--V----}	|------V------}	|-{-----}
Aα	Bα	Cβ
(ii)	Consider Algorithm 1 with
αk = αk (s):= -√k (S > 0) and βk
:=β < min < : b e2, b \ .
l√dDG J
Then, the number of steps K needed to achieve an -approximation (1) is expressed as the following
rational function of batch size S:
K(S)
/ Aa s2 十 Ba ]
[(e2- Ce)S j
(8)
In particular, the minimum value of K needed to achieve (1) is
4AαBα	2dDG2H
Kf( S? )= , C "-"e =---------------=---------
(e2 - Ce)2	(1 - γ)2{(1 - b)e2 - VdDGβ}h0
when
? _ B __	√2Ga
s = A Aa = (1 - Y) PdDHhI .
3.2.1 Discussion of Theorem 3.2
[Performance of Algorithm 1] Theorem 3.2(i) indicates that Algorithm 1 satisfies that, for all
K ≥ 1,allα ∈ (0, 1], all β ∈ [0, b], and all S > 0,
[dDSGD √k + G√K	(SGD),
min E l^kVf (χj )k 21	≤ dD NM__S__+	G G a_1_+ VdD NM G β	(N-Momentum)
kmin, E ∣vJ (xk ) k J	≤	2( 2(1 -b)a √K +	(1 -b)	s√K	+	1 -b β	(NMomentum),
I dD A H s _|_	G2 a	1	∣ √dD AG R	f Adam hme)
12(Γ-Ab)a √K + (1-b)(1-∙γ)2h0 S√K + rɪβ	(Adam-type).
By a similar argument to that in Section 3.1.1, SGD, N-Momentum, and Adam-type optimizers have
approximately O (1 / √K) convergence and there is no evidence that Algorithm 1 with a large batch
size S performs better than with a smaller batch size.
[Existence of optimal batch size] K defined by (8) guarantees that there exists S? such that
dK(S?)/dS = 0, the same as seen in Section 3.1.1 for Theorem 3.1. This implies that there is
an optimal batch size (S = S?) such that K (S) is minimized.
[Comparison of optimal batch sizes] For simplicity, let Us consider the case where (G1) holds.
Theorem 3.2(ii) with G = LnyfdD ensures that the optimal batch sizes for SGD, N-Momentum,
and Adam-type optimizers satisfy that SD,SGD = √DGaD = √2Lna = √DGM = sD,NM∙ Fur-
thermore, if (1 - Y)2 ≤ 1 /(HhO), then sD,SGD = sD,NM ≤ sD,a = (I-√2√Hh0 ∙ Therefore,
N-Momentum and Adam-type optimizers exploit the same sized or larger batches than SGD. Here,
we notice that s?C,SGD, s?C,NM, and s?C,A defined as in (5) and (6) depend on e and β, while s?D,SGD
s?D,NM, and s?D,A do not depend on e and β.
[Comparison of minimum numbers of steps] Again, by a similar argument to (7) in Section 3.1.1,
the restrictions on β (see (27), (28), (29), (30), (31), and (32) in Appendix) imply that
K(s?D,A) ≤ K(s?D,NM) ≤ K (s?D,SGD).	(9)
The previous studies (Kingma & Ba, 2015; Reddi et al., 2018; Luo et al., 2019) used β = 0.9 or
0.99, which is close to 1, for adaptive methods. Meanwhile, a sufficient condition (see (31)) for
Ke(sD,A) ≤ Ke(sD,NM) with D = DNM = DA and G = Ln√dD is β ≤ (1 - b)e2/(LndD),
which implies that adaptive methods using the above β (which is small when the number of samples
n and the number of dimension d are both large and the precision accuracy e is small) are good for
training deep neural networks in the sense that Ke(s?D,A) ≤ Ke(s?D,NM) (see Sections A.8 and A.9
in Appendix for how to set β and its advantage).
8
Under review as a conference paper at ICLR 2022
4 Numerical Comparisons and discussions
Table 3: Number of steps, elapsed time, and training accuracy of optimizers when f(xK) ≤ 10-1
to train ResNet-20 on CIFAR-10
Batch Size	26	27	28	SGD 29	210	211	212	213	214	215
Steps	537500	287500	142500	146875^^―^^	—	—	—	—	—
Time (m)	34.2	20.8	15.4	14.5	—	—	—	—	—	—
Acc. (%)	96.6	96.8	96.6	96.7	—	—	—	—	—	—
Batch Size	N-Momentum 26	27	28	29	210 211 212 213 214 215
Steps Time (m) Acc. (%)	392187^^27734^^12402^^19531^^—^^=~~=~~—^^=~~ 38.2	21.7	14.3	12.6	—	—	—	—	—	— 96.5	96.7	96.7	96.7	—	—	—	—	—	—
Batch Size	M-Adam									
	26	27	28	29	210	211	212	213	214	215
Steps	33593	15234	7226	3125	1367	659	378	320	323	337
Time (m)	14.2	11.3	7.2	6.7	6.4	6.4	6.6	6.6	6.4	6.6
Acc. (%)	96.4	96.6	96.5	96.7	96.7	97.2	97.5	97.7	97.5	99.0
We evaluated the performances of SGD, N-Momentum, and M-Adam with different batch sizes to
train ResNet-20 on the CIFAR-10 dataset with n = 50000. We set α = 10—3, β =10-2, Y = 0.9,
hQ = 10-2, and L = 10. H = 10 were set so as to satisfy Y JHh. < 1, i.e., SD SGD = SDNM <
s?D,A. Table 2 confirms that the optimal batch sizes of the optimizers are such that s?D,SGD =
SD NM = V2Lna ≈ 28 ‹ 213 ≈ √2Lna/(Yy∕HhQ) = SD χ Table 3 shows that the optimizers
with S? (indicated by bold type) could reduce the number of steps more than the ones with other
batch sizes, (9) was satisfied, and Adam with S? performed better than other optimizers. We also
checked that SGD and N-Momentum with S ≥ 210 do not satisfy f(xK) ≤ 10-1 until the stopping
condition, namely, that the number of epochs is 200 and that the value of f of all of the optimizers
is decreasing stably, i.e., the norm of the gradient of f converges to zero.
Finally, we check whether the batch sizes shown in (Shallue et al., 2019) are approximately the same
as the optimal batch sizes. For training CNN on the MNIST dataset, SGD exploited the batch size
between 212 and 214 and N-Momentum exploited the batch size between 213 and 214 (Shallue et al.,
2019, Figure 4(a)). Meanwhile, when n = 55000, α = 10-3, and L ≈ 174 (VirmaUx & Scaman,
2018, Figure 5), the optimal batch sizes are such that SD SGD = SDNM = V2Lna ≈ 13533 ≈
214. Hence, the optimal batch sizes of SGD and N-Momentum are almost the same as the ones in
(Shallue et al., 2019, Figure 4(a)).
5 Conclusion
The main contribution of this paper was to show that the number of steps K(S) needed for noncon-
vex optimization, mink∈ [K] E[∣Rf (Xk) k2] ≤ e2, of a deep learning optimizer is a rational function
of batch size. We showed that there exists an optimal batch size S? such that K (S) is minimized.
Hence, there is no guarantee that the optimizer with a sufficiently large batch size S (> S?) would
perform better than with a smaller batch size. We also showed that the optimal batch size depends on
the optimizer. In particular, it was shown that momentum and adaptive methods can exploit the same
sized or larger optimal batches than can SGD and that, if we can set an appropriate momentum co-
efficient β, then momentum and adaptive methods reduce K(S? ) more than can SGD. Additionally,
numerical results were provided to support the theoretical results in this paper.
9
Under review as a conference paper at ICLR 2022
Acknowledgments
Ethics S tatement
Ethics approval was not required for this study.
Reproducibility Statement
The experimental environment is as follows: two Intel(R) Xeon(R) Gold 6148 at 2.4 GHz CPUs
with 20 cores, 16 GB NVIDIA Tesla V100 at 900 Gbps GPU, Red Hat Enterprise Linux 7.6. The
code was all written in Python 3.8.2 using the NumPy 1.17.3 and PyTorch 1.3.0 packages. Sufficient
conditions for Assumption (A3) and complete proofs of the theoretical results (Theorems 3.1 and
3.2) were included as Appendix.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN.
https://arxiv.org/pdf/1701.07875.pdf, 2017.
Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAMReview, 60:223-311, 2018.
Hao Chen, Lili Zheng, Raed AL Kontar, and Garvesh Raskutti. Stochastic gradient descent in cor-
related settings: A study on Gaussian processes. In Advances in Neural Information Processing
Systems, volume 33, 2020.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of Adam-
type algorithms for non-convex optimization. In Proceedings of The International Conference on
Learning Representations, 2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex
optimization via stochastic path-integrated differential estimator. In Advances in Neural Informa-
tion Processing Systems, volume 31, 2018.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradi-
ent descent method for non-convex objective functions. Journal of Machine Learning Research,
21:1-48, 2020.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization I: A generic algorithmic framework. SIAM Journal on
Optimization, 22:1469-1492, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization II: Shrinking procedures and optimal algorithms. SIAM
Journal on Optimization, 23:2061-2089, 2013.
Robert M. Gower, Othmane Sebbouh, and Nicolas Loizou. SGD for structured nonconvex func-
tions: Learning rates, minibatching and interpolation. In Proceedings of the 24th International
Conference on Artificial Intelligence and Statistics, volume 130, 2021.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge,
1985.
Hideaki Iiduka. Appropriate learning rates of adaptive learning rate optimization algo-
rithms for training deep neural networks. IEEE Transactions on Cybernetics, DOI:
10.1109/TCYB.2021.3107415, 2021.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of The International Conference on Learning Representations, 2015.
10
Under review as a conference paper at ICLR 2022
Nicolas Loizou, Sharan Vaswani, Issam Laradji, and Simon Lacoste-Julien. Stochastic polyak step-
size for SGD: An adaptive learning rate for fast convergence. In Proceedings of the 24th Interna-
tional Conference on Artificial Intelligence and Statistics, volume 130, 2021.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In Proceedings of The International Conference on Learning Representa-
tions, 2019.
James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approxi-
mate curvature. In Proceedings ofMachine Learning Research, volume 37, pp. 2408-2417, 2015.
Celestine Mendler-Dunner, JUan C. Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic opti-
mization for performative prediction. In Advances in Neural Information Processing Systems,
volume 33, 2020.
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on Optimization, 19:1574-
1609, 2009.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence O(1/k2). Doklady AN USSR, 269:543-547, 1983.
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4:1-17, 1964.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
Proceedings of The International Conference on Learning Representations, 2018.
Herbert Robbins and Herbert Monro. A stochastic approximation method. The Annals of Mathe-
matical Statistics, 22:400-407, 1951.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature, 323:533-536, 1986.
Kevin Scaman and CedriC Malherbe. Robustness analysis of non-convex stochastic gradient descent
using biased expectations. In Advances in Neural Information Processing Systems, volume 33,
2020.
Robin M. Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley-
Benchmarking deep learning optimizers. In Proceedings of the 38th International Conference on
Machine Learning, volume 139, pp. 9367-9376, 2021.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of
Machine Learning Research, 20:1-49, 2019.
Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay the learning rate, increase
the batch size. In Proceedings of The International Conference on Learning Representations,
2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference on
Machine Learning, pp. 1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. RMSProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, 4:26-31, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Infor-
mation Processing Systems, volume 30, 2017.
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Advances in Neural Information Processing Systems, volume 31, 2018.
11
Under review as a conference paper at ICLR 2022
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Proceedings of the 32nd International Conference on Machine Learning, volume 37,
pp. 2048-2057, 2015.
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch
sizes? Insights from a noisy quadratic model. In Advances in Neural Information Processing
Systems, volume 32, 2019.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris, and James S. Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in
observed gradients. In Advances in Neural Information Processing Systems, volume 33, 2020.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning, pp. 928-936, 2003.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient
descent. In Advances in Neural Information Processing Systems, volume 23, 2010.
A Appendix
Unless stated otherwise, all relations between random variables are supported to hold almost surely.
Let S ∈ S++. The S-inner product of Rd is defined for all x, y ∈ Rd by(x, y∖s :=〈x, Syi and
the S -norm is defined by IIXllS :=，(x, Sx. The history of process ξ 0, ξ 1,... to time step k is
denoted by ξ[k] = (ξ0,ξ1, . . .,ξk).
A. 1 Sufficient Conditions for Assumption (A3)
Proposition A.1 Assumption (A3) holds if each of the following holds:
(G1) X ⊂ Rd is bounded, the gradient Pfi is LiPschitz continuous with Lipschitz constant Li,
and Si := {x^ ∈ Rd: Pfi (x^) = 0} = 0 (i ∈ [ n ]), where L := max i∈ [ n ] Li. (Ifwe define
Gk,L := supx∈X Pi∈Sk IPfi(x)I, then we can take G := supk∈N Gk,L.)
(G2) X ⊂ Rd is bounded and closed. (If we define Gk := supx∈X i∈Sk IPfi(x)I, then we
can take G := supk∈N Gk.)
,	,	∕~∙j	∕~∙j
Under (A6), G in (G1) and (G2) are respectively G = Ln dD and G = nG, where G :=
maxi∈[n] supx∈X IPfi(x)I.
Proof: The definition of PfSk and the triangle inequality imply that, for all x ∈ Rd and all k ∈ N,
IPfSk (x)I2
1X pfi (x)
i∈Sk
(10)
where the final inequality comes from S ≥ 1. Suppose that (G1) holds. Let x^ ∈ Si (i ∈ [n]). The
Lipschitz continuity of Pfi , together with the definition of L, ensures that, for all x ∈ Rd and all
i ∈ [n],
l∣Vfi(x)Il = IIPfi(x) - Pfi(x")Il ≤ LilIX 一 x*ll ≤ LliX - x*l∣.
Accordingly, we have that, for all x ∈ X and all k ∈ N,
X ∣∣Pfi(x)Il ≤ Ls∣x - x*∣∣ ≤ Ln∣x - x*∣∣.	(11)
i∈Sk
12
Under review as a conference paper at ICLR 2022
Hence, there exists G > 0 such that GkL ≤ LnSup工∈x llʃ 一 x^k≤ G∙ Taking the expectation of
(10), together with (11), thus implies that
E[l^fsk(X)『]≤ El(X l^fi(X)k)	≤ (Lnkx 一 x*k)2 ≤ G2.
i∈Sk
Assumption (A6) implies that there exists a bounded set X ⊂ Rd such that (Xk)k∈N ⊂ X. From
∣Xk — x*k2 = Pi∈[d] (xk,i — Xi)2 ≤ dD, we have that, for all k ∈ N,
GkL ≤ Ln√dD =： G.
Suppose that (G2) holds. Since Nfi is continuous and X is compact, We have that G =
supk∈N Gk < +∞. Taking the expectation of (10) thus implies (A3). Assumption (A6) en-
sures that there exists a bounded, closed set X ⊂ Rd such that (Xk)k∈N ⊂ X. Define Gi :=
supx∈X kNfi(X)k < +∞ and G := maxi∈[n] Gi. Then, we have that, for all X ∈ X,
X kNfi(X)k ≤ SG ≤ nG=: G.
i∈Sk
This completes the proof. □
A.2 Examples of Algorithm 1
We list some examples of Hk ∈ Sd++ ∩ Dd (step 5) in Algorithm 1.
Table 4: Examples of Hk ∈ S++ ∩ Dd (step 5) in Algorithm 1 (δ, Z ∈ [0,1))
	H k	
SGD (βk = γ = 0)	H k is the identity matrix.
N-Momentum (Nesterov, 1983) (γ = 0)	H k is the identity matrix.
AMSGrad (Reddi et al., 2018; Chen et al., 2019) (γ = 0)	Vk = δvk-1 + (1 — δ)NfSk(Xk) ΘNfsk(Xk) V k = (max {vk-1 ,i,vk,i}) d=ι H k = diag(p^kj)	
AMSBound (Luo et al., 2019) (γ = 0)	Vk = δvk-1 + (1 — δ)NfSk(Xk) ΘNfsk(Xk) V k = (max {vk-1 ,i,vk,i}) d=ι	d Vk = (Clip ( √1-, lk, U) ) H k = diag( Vki)
M-Adam (Kingma & Ba, 2015; Iiduka, 2021)	Vk = δVk-1 + (1 — δ)NfSk(Xk) ΘNfk(Xk) V k = -T V k = (max {Vk-1 ,i,V^k,i}) d=ι H k = diag(p^kj)	
AdaBelief (Zhuang et al., 2020) (Sk,i ≤ Sk+1,i is needed)	Sk = (NfSk (Xk) — mk) Θ (NfSk (Xk) — mk) sk = δvk— 1 + (1 一 δ)Sk Sk = 1-Zk H k = diag(P⅞7)	
We define x Θ X for x := (Xi)d=ι ∈ Rd by X Θ X := (x2d=ι ∈ Rd. Clip(∙,l,u): R → R in
AMSBound (l, u ∈ R with l ≤ u are given) is defined for all x ∈ R by
(l if x <l,
Clip(X, l, u) :=	X if l ≤ X ≤ u,
IU if x > u.
While Adam (Kingma & Ba, 2015) uses Hk = diag(√vki,), M-Adam (Iiduka, 2021) uses Hk =
diag(*k,i) to satisfy (A4) (see also Theorem 1 in (Reddi et al., 2018) indicating that Adam does
not always converge).
13
Under review as a conference paper at ICLR 2022
A.3 Lemmas and Theorem
The following are the key lemmas to prove the main theorems in this paper.
Lemma A.1 Suppose that (A1) and (A2) hold and consider Algorithm 1. Then, for all x ∈ Rd and
all k ∈ N,
E hkxk+1 - xk2Hk i ≤ E hkxk - xk2Hki +α2kE hkdkk2Hki
+ 2αk < βkE Kx — Xk, Vf (Xk)〉] + βkE [hx - Xk, mk-1 i],
Y k	Y k
where βk := 1 — βk and Yk := 1 — Yk+1.
(12)
Proof: Let X ∈ Rd and k ∈ N. The definition of Xk+1 implies that
kXk+1 — Xk2Hk = kXk — Xk2Hk + 2αk hXk — X,dkiHk + α2k kdk k
Hk .
Moreover, the definitions of dk, mk, and mk ensure that
hxk - x, dki Hh	= ɪ	hx —	Xk, mki	= β	hx —	Xk, mk-1 i	+ βk	hx	- Xk, NfSk (Xk)i,
Hk YYk	YYk	YYk	k
where βYk := 1 — βk and YYk := 1 — Yk+1. Hence,
∣∣Xk +1 — xkH，≤ ∣∣Xk — xkH，+ 2αk < βk hx — Xk, mk-1 i + βk hx — Xk, VfSk (Xk)i
k	k	Yk	Yk
+α2k kdkk2Hk .
Meanwhile, the relationship between the expectation of the stochastic gradient vector VfSk (X) and
the full gradient vector Vf (X) is as follows: For all X ∈ Rd,
E [ VfSk (x )]= E 1 X Nfi (x) = E [ Nfi (x)] = Nf( X),	(13)
s i∈Sk
where the first equation comes from (A2), the second equation comes from the existence of T such
that [n] = ∪kT=-01Sk, and the third equation comes from (A1). Condition (13) guarantees that
E [hx — Xk, VfSk(Xk)i] = E [E [hx - Xk, VfSk(Xk)i ∣ξ∣[k-1]]]
=E Kx — Xk, E [VfSk (Xk)归[k-1]]〉]
= E[hX — Xk, Vf(Xk)i] .
Therefore, the lemma follows by taking the expectation of (12). 口
Lemma A.2 Algorithm 1 satisfies that, under (A3), for all k ∈ N,
E [kmkk2] ≤ G2.
Under (A3) and (A4), for all k ∈ N,
G2
e [kdk kHk] ≤ (1 — γ)2hS ,
where h0 := min∈d] h0®
Proof: The convexity of ∣∣∙ k2, together with the definition of mk and (A3), guarantees that, for all
k ∈ N,
E [kmkk2] ≤ βkE [mk-1k2] + (1 — βk)E [kVfSk(Xk)k2]
≤ βkE [kmk-1 k2] +(1 — βk)G2∙
14
Under review as a conference paper at ICLR 2022
Induction thus ensures that, for all k ∈ N,
E[∣∣mkk2] ≤ max {km-1 k2,G2} = G2,	(14)
where m-1 = 0 is used. For k ∈ N, Hk ∈ Sd++ guarantees the existence of a unique matrix
Hk ∈ S++ such that Hk = Hk (Horn & Johnson, 1985, Theorem 7.2.6). We have that, for all
x ∈ Rd, ∣XkHk = k Hkxk2. Accordingly, the definitions of dk and mk imply that, for all k ∈ N,
E [kdkkHk] = E [∣H-1Hkdk[≤ ɪE 帆-1∣2 kmkk2 ≤(1-^E 帆- 1∣2 kmkk2 ,
where
IM -I = Mag (h-i )∣∣=max h-i
and Yk := 1 - Yk+1 ≥ 1 - Y. Moreover, (A4) ensures that, for all k ∈ N,
hk,i ≥ h0,i ≥ h0 ：= min h0,i.
i∈[d]
Hence, (14) implies that, for all k ∈ N,
G2
E[kdk k Hk] ≤ (1 — γ)2 h*，
completing the proof. □
We are in the position to prove the following theorem, which leads to Theorems 3.1, 3.2, and A.2.
Theorem A.1 Suppose that Assumptions 2.1 and 2.2 hold and consider Algorithm 1. Let (δk)k∈N ⊂
(0,十 ∞) be the sequence defined by δk := αkβk/γk and Vk(x) := EKxk — x, Vf (Xk)〉] for all
x ∈ Rd and all k ∈ N. Assume that (δk)k∈N is monotone decreasing. Then, for all x ∈ Rd and all
K≥1,
K
Vk (x) ≤
k=1
dDH	G 2
2baκ 2bγ2 h0
K
αk +
k=1
VdDG
∣⅛≠
b
K
βk,
k=1
where b := 1 — b, γ := 1 — γ, D and Hi are defined as in Assumption 2.2, and H := maxi∈[由 H
Proof: Let x ∈ Rd . Lemma A.1 guarantees that, for all k ∈ N,
Vk(X) ≤ 21k {e hkxk - xkHki - E hkxk+1 — xkHkiθ + OYE hkdkkHki
+ βkE KX — xk, mk-1 i].
βk
Summing the above inequality from k = 1 to K ≥ 1 implies that
K	1K 1
∑>(X) ≤ 2 £ δk {e IXk-XkHJ - E [ιxkk+1 - xkHk]}
k=1	|=1----------------{-------------------}
∆K
K
+ 1 X akγk E
2 k=1 β
I--------
(15)
Ak
From the definition of ∆K and E[∣∣xκ +1 — XkHK]/δκ ≥ 0,
Ehkx 1-xkH1 i	K fEhkxk-xkHki
△K ≤	δ 1 —+∑ I	δk—
=2
I
{z
BK
E [xkk - xkHk_ 1]
δk-1
{z
∆ K

}
(16)
15
Under review as a conference paper at ICLR 2022
Since Hk ∈ S；+ exists such that Hk = h] we have ∣罔IHk = ∣∣ Hkx∣∣2 forall x ∈ Rd. Accordingly,
we have
阳k-i(xk - X)『
δk-1
))
From H k = diag(pki), we have that, forall x = (Xi) d=ι ∈ Rd, ∣ Hk x∣2 = P Pd= hk,ix2. Hence,
for all K ≥ 2,
∆ K = E
Accordingly, from (A4) and the monotone decrease of (δk)k∈N, we have that, for all k ≥ 1 and all
i ∈ [d],
hk,i	hk-1,i
--------------2 o.
δk	δk-1
Moreover, from (A6), D := maxi∈[d] supk∈N(xk,i - xi)2 < +∞. Accordingly, for all K ≥ 2,
∆ k ≤ D E
Therefore, (16), E[∣xι - x∣∖]/δι ≤ DEPd=γ hιɪδι], and (A5) imply, for all K ≥ 1,
∆K ≤ DE
+ DE
d
hK,i
i=1
which, together with δκ := ακ (1 - βκ)/(1 - YK +1) ≥ b)K and H = maxi∈[d] Hi, implies
1	dDH
2	K ≤ 2baκ .
Lemma A.2 implies that, for all K ≥ 1,
K	K2
L αkγk	2 ] - L αkγk G
AK := £ ɪ 叫kd k k h J ≤ N ɪ 西'
which, together with Yk ≤ 1 and βk ≤ b, implies that
1	G2	K
-Ak ≤ -H----- a αk.
2	≤ 2bγ2 h0 匕 k
Lemma A.2 and Jensen’s inequality ensure that, for all k ∈ N,
E [∣mk∣] ≤ G.
The Cauchy-Schwarz inequality and (A6) guarantee that, for all K ≥ 1,
BK ：= X	βkE [hx	- Xk, mk-1 i] ≤ X	√dDβ E	[∣mk-11∣]	≤ √dDG X βk.
k=1	βk	k=1 b	b k=1
(17)
(18)
(19)
Therefore, (15), (17), (18), and (19) lead to the assertion in Theorem A.1. This completes the proof.
□
16
Under review as a conference paper at ICLR 2022
A.4 Proof of Theorem 3.1
(i)	Theorem A.1, together with αk = a/s and βk = β, guarantees that, for all K ≥ 1, all S
and all X ∈ Rd,
> 0,
1K
K 与 Vk (X) ≤
dDH S G2α
-H + + -H 
2ba K 2bγ2 h0
1
-+
s
β.
(20)
b
Moreover, there exists m ∈ [K] such that, for all x ∈ Rd,
E [hxm — x, Pf(xm)i] = Vm(x) = min Vk(x)
k∈[K]
1K
≤ K 与 Vk (X) .
(21)
Setting x = xm — Pf (xm), together with (20) and (21), guarantees that
mrnE [kvf (xk)k2] ≤ E [kPf (χm)k2] ≤ dDH K +2bγ2h0
G2 a 1	VdDG
β.
(22)
Aα
|∙{z
Bα

+ 一 +
S
}
b
(ii)	A sufficient condition for (1), i.e.,
km∈[iKn]E[kPf(Xk)k2 ≤e2
is that the right-hand side of (22) is equal to 2 , i.e.,
Aαs2 +BαK + (Cβ — e2)sK = 0,
which implies that
K(s)
Aαs2
S ∈	e2
Ba-r, + ∞

where e2 — Cβ > 0 is guaranteed from β < be2 / VdDG. We have that
<0
dK(s)
dS	{(Cβ — e2)S + Bα}2
{(e2 — Cβ)S - 2Ba } <
ifS ∈	e2
>0
if S = s? = F—
e2 —
if S ∈ (S?, +∞).
0

α
Hence, K (S) attains the minimum K (s*) when S = s*. 口
A.5 Proof of Theorem 3.2
(i)	Theorem A.1, together with ak = a/(sVk) and βk = β, guarantees that, for all K ≥
S > 0, and all X ∈ Rd,
1, all
1K
K k=1 Vk (X) ≤
dDH 1
G2	1
2b ak K + 2即 h3 K
dDH S	G2α 1
K
αk +
k=1
≤ —工---+^ +  -----+^ +
2ba √K	bγ2 h0 sy/K
dDGG
(23)
β,
VdDG
b
β
b
where we use
1K 1	1
K 白 √k ≤ k
k=1
dt
√t
2 √K — 1)≤ -2=.
K Vk
17
Under review as a conference paper at ICLR 2022
An argument similar to the one for showing (21) and (22) ensures that (23) implies that
min E[IVf(xk)k2] ≤
k∈[K]
dDH s G 2 α 1	VdDG
J
2bα √K	bγ2h0 SyTK
l~z~}	|{^~^/
Aα	Bα
+-------
b
β.
(24)
(ii)	A sufficient condition for (1), i.e.,
min E [k^f (xk)k2] ≤ e2
k∈[K]
is that the right-hand side of (24) is equal to e2 , i.e.,
Ααs2 + (Ce — e2 ) S√K + Ba = 0,
which implies that
K(s)
Aa S2 + Ba ɪ 2
(e 2 - Ce) S ʃ
where e2 - Ce > 0 is guaranteed from β < be2 / VdDG. We have that
<0
dK(S)	2(AαS2 + Bα)
if s ∈ (0, s?),
dS	(e2 - Ce)2 S3
(Aα s2 - Bα ) = 0
>0
which implies that K(s) attains the minimum K(s?) when s
if s= s
if s ∈ (s?, +∞),
S?. □
A.6 RELATIONSHIP BETWEEN sAND K(s) FOR ALGORITHM 1 WITH DIMINISHING
LEARNING RATES
The following is a result for Algorithm 1 with diminishing sequences αk and βk .
Theorem A.2 Suppose that Assumptions 2.1 and 2.2 hold and let e > 0, α ∈ (0, 1], and β ∈
[0, b] ⊂ [0, 1).
(i)	Consider Algorithm 1 with
αk = αk (S) := —α= (S > 0) and βk := βk.
Sk
Then, for all K ≥ 1 and all S > 0,
2-∣ dDH s	G 2 a	1 β √ dDG 1
m%E [I∣vf(xk)k ] ≤ 21F √K + (1 - b)(1 - γ)2hs S√K + (1- b)(1-β) K
|
|-----{z----}
Aα
{z
Bα
}
|-------{--------}
Cβ
(ii)	The number of steps K needed to achieve (1) is expressed as the following rational function of
batch size s:
(A，(	((AaS2+Ba) + √As2+Bay+4e2Cs212
尺(S) = I--------------F---------------ʃ .
In particular, the minimum value of K needed to achieve (1) is
K (S?) =
√ AaBα + A AαBα + ∈ 2 Cβ
2
F
√(1 - β)dDHG + r^—edDGH+21—bei—Y2e√dDh^G}
2(1 - b)2(1 - β)(1 - Y)2e4h0
when
s
Gα
(1 - Y) VdDHh
18
Under review as a conference paper at ICLR 2022
Proof: (i) Theorem A.1, together with ak = α/(sjk) and βk = βk, guarantees that, for all K ≥ 1,
all s> 0, and all x ∈ Rd,
1K
K PV Vk( ʃ)≤
dDH 1
--H------+
2b ακ K
G2	1 K
2^Y2h^ K 入 αk +
≤ dDH s	G2α 1	βVdDG 1
-2bα √K ^bγ2 hŋ s√K bβ K
(25)
where we use β := 1 - β,
2
≤ √K,
An argument similar to the one for showing (21) and (22) ensures that (25) implies that
dDH s	G2α	1
@in E [∣Nf(xk ) k ] ≤ —- √= + α 2 .* √J7
k∈[K]	2bα √K bγ2hjɔ s√K
|T"}	ι∙{z∙}
Aα	Bα
βVdDG 1
^^+	~ ~	~i~r .
-bβ-	K
|—{—/
Cβ
(26)
(ii) A sufficient condition for (1), i.e.,
m]n E [皿(Xk) k2] ≤ e2
k∈[K]
is that the right-hand side of (26) is equal to e2, i.e.,
AaS2 √K + Ba√K + Ce s — e2 sK = 0,
which implies that
K(s) =
(Aas2 + Ba ) + pp∕( Aas2 + Ba)2 + 4e2 Ces2
22s
We have that
<0
dVKs _ (4s2 + Ba) + AS2^+BB2+^2C(^'
ds
2 e 2 S 2 y∕(ΛaS2+Ba)2+~4^2CβS2
-(Aas 2 - Ba ) |=0
I > 0
if S ∈ (0,s*),
ifS=SS=æ
if s ∈ (s?, +∞),
.
which implies that K(S) attains the minimum K(s?) when S = s?. □
A.7 CONDITIONS ON β SATISFYING (7)
Ifβ satisfies the condition in Theorem 3.1(ii) and if
then
β ≤ (1 — b)'DSGD — ʌ/DNM 2
一	√dD SGD D NM G
Ke( SC, SGD) = dDSGG
≥ K (s?C,NM )
______dD NM G2___
{(1 — b) e2 — √dDNM Gβ} 2
Moreover, ifβ satisfies the condition in Theorem 3.1(ii) and if
β ≤ (1 - b )(1 - Y) ∖/D SGD h0 - 7D A H e 2
一	(1 - YI PdDSGDDAh G	e,
(27)
(28)
(29)
19
Under review as a conference paper at ICLR 2022
then
K(s?C,SGD) ≥ K(s?C,A)
dDAG2H
(I — Y )2 {(I — b) e 2 — √ dD A Gβ} 2 h0
(30)
If β satisfies the condition in Theorem 3.1(ii) and if
β ≤	(I - b){(I - Y) 7DNMh0 - 7DAH}	e2
{ (I — Y)pdDNMDAh0 — √dDNMDAH}G
then
K (s?C,NM) ≥ K (s?C,A).
(31)
(32)
A.8 HOW TO SET β
Theorems 3.1(ii) and 3.2(ii) under (G1) indicate that the following restriction on β is needed:
β < min[ 1 — b e2,b]∙	(33)
LndD
Let us suppose that the number of samples n and the number of dimension d are both large and the
precision accuracy e is small. Then, β is small. The momentum term mk (step 3 of Algorithm 1)
satisfies
mk= β mk-1 + (1 — β) f (Xk) ≈ ʃ VfSk( xk) if β Satisfies (33),
mk = mk-1 + — Sk xk mk-1	ifβ = 0.99 or 0.9.
Accordingly, using β in (33) puts considerable emphasis on stochastic mini-batch sampling, which
leads to the results such as s?C,SGD ≤ s?C,NM ≤ s?C,A and s?D,SGD = s?D,NM ≤ s?D,A.
A.9 Stochastic gradient complexity
Table 5: Stochastic gradient complexity (SGC) for e-approximation of optimizers (SGD, N-
Momentum, Adam-type, and SPIDER (Fang et al., 2018)) with constant and diminishing learning
A theoretical investigation of Stochastic Path-Integrated Differential EstimatoR (SPIDER) for e-
approximation in nonconvex optimization was reported in (Fang et al., 2018). In particular, The-
orem 2 in (Fang et al., 2018) clarified that SPIDER, which has a constant learning rate, for e-
approximation must use the full-batch gradient with the number of samples n or the stochastic
gradient with batch size √n. Meanwhile, our results show the optimal batch size of SGD, N-
Momentum, and Adam-type optimizers (see Tables 1 and 2). Table 5 indicates that the SGCs of
SGD, N-Momentum, and Adam-type optimizers depend on a positive parameter α. For example,
let us set α = e2 and focus on N-MomentUm using diminishing learning rate. Then, the SGC of
N-MomentUm is O(n3/e2). The SGC of SPIDER is O(n + √n∕e2) (see (Fang et al., 2018, Table
1) for the SGCs of the variance-reduction type of optimizer).
20