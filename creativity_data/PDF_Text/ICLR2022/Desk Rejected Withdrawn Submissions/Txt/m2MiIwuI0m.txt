Under review as a conference paper at ICLR 2022
An Analysis of Attentive Walk-Aggregating
Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph neural networks (GNNs) have been shown to possess strong representation
power, which can be exploited for downstream prediction tasks on graph-structured
data, such as molecules and social networks. They typically learn representations
by aggregating information from the K-hop neighborhood of individual vertices
or from the enumerated walks in the graph. Prior studies have demonstrated the
effectiveness of incorporating weighting schemes into GNNs; however, this has
been primarily limited to K-hop neighborhood GNNs so far. In this paper, we aim
to extensively analyze the effect of incorporating weighting schemes into walk-
aggregating GNNs. Towards this objective, we propose a novel GNN model, called
AWARE, that aggregates information about the walks in the graph using attention
schemes in a principled way to obtain an end-to-end supervised learning method for
graph-level prediction tasks. We perform theoretical, empirical, and interpretability
analyses of AWARE. Our theoretical analysis provides the first provable guarantees
for weighted GNNs, demonstrating how the graph information is encoded in the
representation, and how the weighting schemes in AWARE affect the representation
and learning performance. We empirically demonstrate the superiority of AWARE
over prior baselines in the domains of molecular property prediction (61 tasks)
and social networks (4 tasks). Our interpretation study illustrates that AWARE can
successfully learn to capture the important substructures of the input graph.
1 Introduction
The increasing prominence of ML applications for graph-structured data has lead to the popularity of
graph neural networks (GNNs) in several domains, such as social networks (Kipf & Welling, 2016),
molecular property prediction (Duvenaud et al., 2015), and recommendation systems (Ying et al.,
2018). Several empirical and theoretical studies (e.g., (Duvenaud et al., 2015; Kipf & Welling, 2016;
Xu et al., 2018; Dehmamy et al., 2019)) have shown that GNNs can achieve strong representation
power by constructing representations encoding rich information about the graph.
The most popular approach of learning GNNs involves aggregating information from the K-hop
neighborhood of individual vertices in the graph (e.g., (Kipf & Welling, 2016; Gilmer et al., 2017;
Xu et al., 2018)). An alternative approach for learning graph representations is via walk aggregation
(e.g., (Vishwanathan et al., 2010; Shervashidze et al., 2011; Perozzi et al., 2014)) by means of using
information of the enumerated walks in the graph. Existing studies have shown that walk-aggregating
GNNs can achieve strong empirical performance with concrete analysis of the encoded graph
information (Liu et al., 2019). This can potentially allow emphasizing and aggregating important
walks, which can alleviate the problem of over-squashing exponentially growing information for
distant dependencies (Alon & Yahav, 2020).
It is important to note that the strong representation power of GNNs may not always translate to
learning the best representation amongst all possible ones. While this allows encoding all kinds of
information, a subset of the encoded information that is not relevant for prediction may interfere
or even overwhelm the information useful for learning—leading to sub-optimal performance. A
particularly attractive approach to address this challenge is by incorporating weighting schemes into
GNNs, which is inspired by the powerful empirical performance of attention mechanisms (Bahdanau
et al., 2014; Luong et al., 2015; Xu et al., 2015; Vaswani et al., 2017) for natural language processing
(e.g., (Devlin et al., 2019)) and computer vision tasks (e.g., (Dosovitskiy et al., 2020)).
1
Under review as a conference paper at ICLR 2022
In the domain of graph representation learning, recent studies (Gilmer et al., 2017; VelickoVic
et al., 2017; Yun et al., 2019; Maziarka et al., 2020; Rong et al., 2020) have used the self-attention
mechanism to improVe the empirical performance of GNNs by learning to select important information
and remoVing the irreleVant ones. These studies; howeVer, haVe only explored using attention schemes
for K-hop neighborhood GNNs, and there has been no corresponding work exploring this idea for
walk-aggregating GNNs. Incorporating attention mechanisms into walk-aggregating GNNs can
allow for a finer-grained analysis of the weighting schemes at different granularities of the graph
components. Furthermore, a majority of these prior studies haVe been empirically driVen—lacking a
strong theoretical understanding about success conditions.
In this paper, we propose to theoretically and empirically examine the effect of incorporating
weighting schemes into walk-aggregating GNNs. To this end, we propose a simple, interpretable,
and end-to-end superVised GNN model, called AWARE (AttentiVe Walk-Aggregating GRaph Neural
NEtwork), for graph-leVel prediction. AWARE aggregates the walk information by means of weighting
schemes at distinct leVels (Vertex-, walk-, and graph-leVel) in a principled manner. By Virtue of the
incorporated weighting schemes at these different leVels, AWARE can emphasize the information
important for prediction while diminishing the irreleVant ones—leading to representations that can
improVe learning performance. We perform an extensiVe three-fold analysis of AWARE as summarized
below:
•	Theoretical Analysis: We present provable guarantees for AWARE, identifying conditions when
the weighting schemes improve learning. Prior weighted GNNs (e.g., (VelickoVic et al., 2017;
Maziarka et al., 2020)) do not enjoy similar theoretical guarantees, making this the first provable
guarantee on the learning performance of weighted GNNs (to the best of our knowledge). Further-
more, current understanding of weighted GNNs typically focuses only on the positive effect of
weighting on their representation power. In contrast, we also explore the limitation scenarios when
the weighting does not translate to stronger learning power.
•	Empirical Analysis: We empirically evaluate the performance of AWARE on graph-level predic-
tion tasks from two domains: molecular property prediction (61 tasks from 11 popular benchmarks)
and social networks (4 tasks). For both domains, AWARE overall outperforms both traditional graph
representation methods as well as recent GNNs (including the ones that use attention mechanisms)
in the standard setting (as defined in Section 3).
•	Interpretability Analysis: We perform an interpretation study to support our design for AWARE
as well as the theoretical insights obtained about the weighting schemes. We provide a visual
illustration that AWARE can extract the important sub-graphs for the prediction tasks. Furthermore,
we show that the weighting scheme in AWARE can align well with the downstream predictors.
2	Related Work
Graph neural networks (GNNs). GNNs have been the predominant method for capturing infor-
mation of graph data (Li et al., 2015; Duvenaud et al., 2015; Kipf & Welling, 2016; Kearnes et al.,
2016; Gilmer et al., 2017). A majority of GNN methods build graph representations by aggregating
information from the K-hop neighborhood of individual vertices (Duvenaud et al., 2015; Li et al.,
2015; Battaglia et al., 2016; Kearnes et al., 2016; Xu et al., 2018; Yang et al., 2019). This is achieved
by maintaining a latent representation for every vertex, and iteratively updating it to capture informa-
tion from neighboring vertices that are K-hops away. Another mainstream approach is enumerating
the walks in the graph and using their information (Vishwanathan et al., 2010; Shervashidze et al.,
2011; Perozzi et al., 2014). Liu et al. (2019) use the motivation of aggregating information from the
walks by proposing a GNN model that can achieve strong empirical performance along with concrete
theoretical analysis. Furthermore, properly aggregating important walk information can potentially
alleviate the problem of over-squashing exponentially growing information for distant dependencies
in K-hop neighborhood GNNs as discussed by Alon & Yahav (2020).
Theoretical studies have shown that GNNs have strong representation power (Xu et al., 2018;
Dehmamy et al., 2019; Liu et al., 2019), and have inspired new disciplines for improving their
representations further (Morris et al., 2019; Azizian & marc lelarge, 2021). To this extent, while
the standard setting of GNNs has only vertex features and adjacency information as inputs, many
recent GNNs (Kearnes et al., 2016; Gilmer et al., 2017; Coors et al., 2018; Yang et al., 2019; Klicpera
et al., 2020; Wang et al., 2021) exploit extra information, such as edge features, to gain stronger
2
Under review as a conference paper at ICLR 2022
performance. In this work; however, we focus on analyzing the effect of applying attention schemes
for representation learning, and thus want to perform this analysis under the standard setting first. In
the future, one may possibly incorporate the aforementioned extra information (e.g., edge attributes
and 3D spatial orientation) into our analysis.
GNNs with attention. The empirical effectiveness of attention mechanisms have been demonstrated
on language (Martins & Astudillo, 2016; Devlin et al., 2019; Raffel et al., 2020) and vision tasks (Ra-
machandran et al., 2019; Dosovitskiy et al., 2020; Zhao et al., 2020). This has also been extended to
the K-hop GNN research line where the main motivation is to dynamically learn a weighting scheme
at various granularities, e.g., vertex- and graph-level. Graph Attention Network (GAT) (VelickoVic
et al., 2017) and Molecule Attention Transformer (MAT) (Maziarka et al., 2020) utilize the attention
idea in their message passing functions. Rong et al. (2020) applies an attention mechanism at both
vertex- and edge-levels to better capture the structural information in molecules. ENN-S2S (Gilmer
et al., 2017) adopts an attention module (Vinyals et al., 2015) as a readout function. However, all
such studies are based on K-hop GNNs, and to the best of our knowledge, our work is the first to
bring attention schemes into walk-aggregation GNNs.
3	Preliminaries
Graph data. We assume an input graph G=(V, A) consisting of vertex attributes V and an adjacency
matrix A. We index the vertices by [m]={1, . . . , m}. Suppose each vertex has C discrete-valued
attributes, and the jth attribute takes values in a set of size kj. Let hij∈{0, 1}kj be the one-hot
encoding of the j th attribute for vertex i. Then, the input of vertex i is the concatenation of C
attributes, i.e., hi=[hi1; . . . ; hiC] ∈ {0, 1}K where K= PjC=1 kj. Then V is the set {hi}im=1.
We denote the adjacency matrix by A∈{0, 1}m×m, where Ai,j=1 indicates that vertices i and j
are connected. We denote the set containing the neighbors of vertex i by N (i)={j∈[m] : Ai,j =1}.
For molecular graphs, vertices and edges correspond to atoms and bonds, respectively. For social
network graphs, they correspond to entities (actors, online posts) and the connections between them.
Although many GNNs exploit extra information like edge attributes, our primary focus is on the
effect of attention mechanisms. Hence, we perform our analysis in the standard setting (i.e., only
using vertex attributes and adjacency information).
Vertex embedding. We define an r-dimensional embedding of vertex i by:
fi = W hi ,	(1)
where W=[W1; . . . ; W C]∈Rr×K and W j ∈Rr×kj is the embedding matrix for each attribute j ∈
[C]. We denote the embedding corresponding to V by F = [f1; . . . ; fm].
Walk aggregation. Unlike the typical approach of aggregating K-hop neighborhood information,
walk aggregation enumerates the walks in the graph, and use their information (e.g., (Vishwanathan
et al., 2010; Perozzi et al., 2014)). Liu et al. (2019) utilize the walk-aggregation strategy by proposing
the N-gram graph GNN, which can achieve strong empirical performance, allow for fine-grained
theoretical analysis, and potentially alleviate the over-squashing problem in K-hop GNNs. The
N-gram graph views the graph as a Bag-of-Walks. Specifically, embeddings are constructed for
all walks in the graph through element-wise product of vertex embeddings along the walks. The
embeddings for all walks of length n are aggregated (by summation) to obtain embeddings for
the n-gram walk set. Finally, the graph embeddings are obtained by concatenation of all n-gram
walk set embeddings. Compared to K-hop aggregation strategies, this formulation explicitly allows
analyzing representations at different granularities of the graph: vertices, walks, and the entire graph.
This provides motivation for capitalizing on the N-gram walk-aggregation strategy for incorporating
and analyzing the effect of weighting schemes on walk-aggregation GNNs. The principled design
facilitates theoretical analysis of conditions under which the weighting schemes may be beneficial.
Thus, in this paper, we analyze the effect of incorporating attention weighting schemes on the N-gram
walk-aggregation GNN.
4	AWARE: Attentive Walk-Aggregating Graph Neural Network
We propose AWARE, an end-to-end fully supervised GNN for learning graph embeddings by aggre-
gating information from walks with learned weighting schemes. Intuitively, not all walks in a graph
3
Under review as a conference paper at ICLR 2022
are equally important for downstream prediction tasks. AWARE incorporates an attention mechanism
to assign different contributions to individual walks as well as assigns feature weightings at the vertex
and graph embedding levels. These weights are learned in a supervised fashion for prediction. This
enables AWARE to mitigate the shortcomings of its unweighted counterpart (Liu et al., 2019), which
computes graph embeddings in an unsupervised manner only using the graph topology.
At a high level, AWARE first computes vertex embeddings F, and initializes a latent vertex representa-
tion F(1) by incorporating a feature weighting at the vertex level. It then iteratively updates the latent
representation F(n) using attention at the walk level, before performing a weighted summarization at
the graph level to obtain embeddings f(n) for walk sets of length n. The f(n)’s are concatenated to
produce the graph embedding f[T] (G) for the downstream task. We now provide more details.
Weighted vertex embedding. Intuitively, some directions in the vertex embedding space are likely
to be more important for the downstream prediction task than others. In the extreme case, the
prediction task may depend only on a subset of the vertex attributes (corresponding to some directions
in the embedding space), while the rest may be inconsequential and hence should be ignored when
constructing the graph embedding. AWARE weights different vertex features using Wv ∈ Rr0×r by
computing the initial latent vertex representation F(1) as:
F(1) = σ(Wv F), F is computed using Eqn (1)
where σ is a non-linear activation function, and r0 is the dimension of the weighted vertex embedding.
Walk attention. AWARE computes embeddings corresponding to walks of length n in an iterative
manner, and updates the latent vertex representations in each iteration using such walk embeddings.
When aggregating the embedding of a walk, each vertex in the walk is bound to have a different
contribution towards the downstream prediction task. For instance, in molecular property prediction,
the existence of chemical bonds between certain types of atoms in the molecule may have more
impact on the property to be predicted than others. To achieve this, in iteration n, AWARE updates
the latent representations for vertex i from [F(n-1)]i to [F(n)]i by taking an element-wise product
of [F(n-1)]i with a weighted sum of the latent representation vectors of its neighbors j ∈ N (i).
Such a weighted update of the latent representations implicitly assigns a different importance to each
neighbor j for vertex i. Assuming that the importance of vertex j for vertex i depends on their latent
representations, we consider a score function corresponding to the update from vertex j to i as:
Sji := S(fj,fi).
(2)
To allow different weights [S(n)]ji for different iterations n, we use the latent representations for
vertices from the previous iteration (n-1). In particular, we use the self-attention mechanism:
[Z(n)]j→i = [F(n-1)]j>Ww [F(n-1)]i
where [F(n-1)]i is the latent vector of vertex i at iteration n-1, and Ww∈Rr0×r0 is a parameter
matrix that is learned. We then define the attention weighting matrix used at iteration n as:
e[Z(n)]j→i
[S(n)]ji = Pk∈N (i) e[Z(n)j
(3)
Using this attention matrix Sn, we perform the iterative update to the latent vertex representations via
a weighted sum of the latent representation vectors of their neighbors:
F(n) = F(n-1)(ASn)	F(1)
This update is simple and efficient, and automatically aggregates important information from the
vertex neighbors for the downstream prediction task.
Weighted summarization. Since the downstream task may selectively prefer certain directions in
the final graph embedding space, AWARE learns a weighting Wg ∈ Rr0×r0 to compute a weighted
sum of latent vertex representations for obtaining walk set embeddings of length n as follows:
f(n) = σ(WgF(n))1
where 1 denotes matrix of ones in Rm×m . Walk set embeddings up to length T are then concatenated
to produce the graph embedding f[T] (G) = [f(1), . . . , f(T)].
4
Under review as a conference paper at ICLR 2022
End-to-end supervised training. We sum-
marize the different weighting schemes and
steps of AWARE as a pseudo-code in Algo-
rithm 1. The graph embeddings produced by
AWARE can be fed into any properly-chosen
predictor hθ parametrized by θ, so as to be
trained end-to-end on labeled data. For a
given loss function L, and a labeled data set
{(Gi, yi)} where Gi’s are graphs and yi’s are
their labels, AWARE can learn the parameters
(W, Wv , Ww , Wg) and the predictor θ by opti-
mizing the loss Pi L(yi, hθ (f[T] (Gi))).
The N-Gram walk aggregation strategy termed
Algorithm 1 AWARE (W, Wv , Ww , Wg)
Require: Graph G=(V, A), max walk length T
1:	Compute vertex embeddings F by Eqn (1)
2:	F(1) = σ(WvF)
3:	for each n ∈ [2, T] do
4:	Compute Sn using Eqn (3)
5:	F(n)=F(n-1)(ASn)	F(1)
6:	end for
7:	Set f(n) := σ(WgF(n))1 for 1 ≤ n ≤ T
8:	Set f[T] (G) := [f(1);...;f(T)]
Ensure: The graph embedding f[T] (G)
as the N-Gram Graph (Liu et al., 2019) operates in two steps: first to learn a graph embedding using
the graph topology without any supervision, and then to use a predictor on the embedding for the
downstream task. In contrast, AWARE is end-to-end fully supervised, and simultaneously learns the
vertex/graph embeddings for the downstream task along with the weighting schemes to highlight
the important information in the graph and suppress the irrelevant and/or harmful ones. Secondly,
the weighting schemes of AWARE allow for the use of simple predictors over the graph embeddings
(e.g. logistic regression or shallow fully-connected networks) for performing end-to-end supervised
learning. In contrast, N-Gram Graph requires strong predictors such as XGBoost (with thousands of
trees) to exploit the encoded information in the graph embedding.
5	Theoretical Analysis
For the design of our walk-aggregation GNN with weighting schemes, we are interested in the
following two fundamental questions: What representation can it obtain? Under what conditions
can the weighting scheme improve the prediction performance? This section analyzes the weighting
Sij = S(fi , fj ) for these two questions.1 We assume:
• Wv = Wg = I, the number of attributes is C = 1, and the activation is linear σ(z) = z.
Here, the assumptions simplify the notations, and allow focusing on the effect of Sij . Appendix A.3
presents an analysis for more general Wv , Wg and C > 1.
We will show that the weighting scheme can highlight important information, and reduce irrelevant
information for the prediction, and thus improve learning. To this end, we first analyze what informa-
tion can be encoded in our graph representation, and how they are weighted (Theorem 1). We then
examine why the weighting can help learning a predictor with better performance (Theorem 2). We
focus on the intuition and implications here, and present proofs and more discussion in Appendix A.
Weighted representation in AWARE. We first formally define the walk information in the graph,
and a notion of walk weights (which will be shown to be exactly the weights induced by our method).
Recall that we assume the number of attributes is C = 1. K is the number of possible attribute values,
and the columns of W ∈ Rr×K are embeddings for different attribute values u. Let W(u) denote the
column for value u, i.e., W(u) = W h(u) where h(u) is the one-hot vector of u.
Definition 1 (Walk Statistics). A walk type of length n is a sequence of n attribute values v =
(vι,v2,…，Vn). The walk statistics vector c(n)(G)∈ RKn is the histogram of all walk types of
length n in the graph G, i.e., each entry is indexed by a walk type v and the entry value is the number
of walks with the attribute values v in the graph. Furthermore, let c[T] (G) be the concatenation of
c(1) (G), . . . , c(T) (G). When G is clear from the context, we write c(n) and c[T] for short.
Definition 2 (Walk Weights). The weight of a walk type v = (v1, . . . , vn) is λ(v) :=
Qn-II S(W(vi),W(vi+ι)) where S(∙, ∙) is the weightfunction in Eq equation 2.
1For the other weighting schemes Wv and Wg, we know Wv weights the vertex embeddings fi, and Wg
weights the final embeddings F(n) , emphasizing important directions in the corresponding space. If Wv has
singular vector decomposition Wv = UΣV >, then it will relatively emphasize the singular vector directions
with large singular values. Similar for Wg. See Section 7 for some visualization.
5
Under review as a conference paper at ICLR 2022
We have the following theorem which shows that our representation is a linear mapping of the
weighted walk statistics.
Theorem 1. The embedding f(n) is a linear mapping of the walk statistics c(n):
f(n) = M(n)Λ(n)c(n)
where M(n) is a matrix depending only on W, and Λ(n) is a K n -dimensional diagonal matrix whose
columns are indexed by walk types v, and have diagonal entries λ(v). Therefore, f[T] := MΛc[T]
where M is a block-diagonal matrix with diagonal blocks M(1), M(2), . . . , M(T), and Λ is block-
diagonal with blocks Λ(1) , Λ(2) , . . . , Λ(T).
In words, c(n) is first weighted by our weighting scheme where the count of each walk type v is
weighted by the corresponding walk weight λ(v), and then compressed from the high dimension
RKn to the low dimension Rr. Ideally, we would like to have relatively larger weights on walk types
important for the prediction task and smaller for those not important. This provides the basis for the
focus of our analysis: the effect of weighting for the learning performance.
Learning guarantees of AWARE. To illustrate the effect of weighting, suppose the label is given
by a linear function on c[τ] with parameter β*, i.e., y = hβ*, c[τ]). First, consider learning a linear
function over the weighted features Ac?]. If A is invertible, the parameter Λ-1β* on Λc[τ] has
the same output as β* on c。，and thus has the same loss. So, We only need to learn A-1β*. The
sample size needed will depend on the factor kA-1β* ∣∣2 kAc。k2, which is potentially smaller than
kβ* k2 kc[T] k2 for the unweighted case. This means fewer data samples are needed (equivalently,
smaller loss for a fixed amount of samples).
Now, consider the case of learning over f[T] (G) = MAc[T] that has an extra M (defined in
Theorem 1). We note that c[T] can be sparse compared to its high dimension (since only a very
small fraction of all possible walk types will likely appear in a graph). Well-established results from
compressive sensing show that when M has the Restricted Isometry Property (RIP), learning over
MAc[T] is comparable to learning over Ac[T] . Indeed, Theorem 4 in Appendix A shows when W
is random and the embedding dimension r is large enough, there are families of distributions of W
such that M has RIP for Ac[T]. Thus, we assume M has RIP, and focus on the analysis of how Ww
affects the weighting and the learning.
However, the above intuition is only for learning over a fixed weighting A induced by a fixed Ww ,
while in fact Ww needs to be learned. Our key challenge is to incorporate the learning of Ww in
the analysis. Formally, we consider learning Ww from a hypothesis class W, and let A(Ww) and
f[T] (G; Ww) denote the weights and representation given by Ww. For prediction, we consider binary
classification with the logistic loss `(g, y) = log(1 + exp(-gy)) where g is the prediction and y is
the true label. Let'd (θ, Ww) be the risk of a linear classifier with a parameter θ on fτ (G; Ww)
over the data distribution D. Given M i.i.d. samples {(Gi, yi)}iM=1 from D, consider learning over
Ww ∈ W, kθk2 ≤ Bθ for a regularization coefficient Bθ :
1M
θ,Ww = arg min M X ' (hθ,f[T ](Gi； Ww))，y，.
To derive error bounds, suppose W is equipped with a norm k ∙ k and let N(W, e) be the e-covering
number of W w.r.t. the norm k ∙ k (other complexity measures on W, such as VC-dimension, can
also be used). Furthermore, let β* denote the best linear classifier on ”], and let 'D denote its risk.
Theorem 2. Assume c[T] is s-sparse, M satisfies (2s, e0)-RIP, A(Ww) is invertible and f[T] (G; Ww)
is Lf -Lipschitz over W. For any δ, e ∈ (0, 1), there are regularization coefficient values Bθ such that
with probability ≥ 1 - δ:
'D (θ,d) ≤ 'D + 2e + OQTT W(W)	+ Wm∈nw B(WW )x O √eo + CM)
whereC (W) := logN
8BθL
and
B(Ww ) :
fδ
max kA(Ww)c[T](G)k2kA(Ww)-1β*k2.
G〜D
6
Under review as a conference paper at ICLR 2022
Remark. Theorem 2 shows that the learned model has risk comparable to that of the best linear
classifier on the walk statistics, given sufficient data. Let’s now compare to the unweighted case
(i.e., A is the identity matrix), where log N (W, ELy) reduces to 0, and B(Ww) reduces to
Bo := maxG~D ∣∣C[τ](G)∣∣2∣∣β*∣∣2. So, our method needs extra samples to learn Ww, leading to
the extra error terms related to logN (W, &6；L于).On the other hand, the benefit of weighting
is replacing the factor Bo with minww B(Ww). If there is Ww with B(Ww)《Bo, the error is
significantly reduced. Therefore, there is a trade-off between the reduction of error for learning
classifiers on an appropriate weighted representation and the additional samples needed for learning
an appropriate weighting. When the labels indeed depend on some important walks rather than on all
walks, the weighting can alleviate the problem of over-squashing shown in Alon & Yahav (2020).
Our analysis also reveals that weighting is not panacea: its ability to improve learning depends on (1)
whether it can assign the large weights to important features, and (2) whether the benefit of weighting
important features exceeds the overhead of learning a proper weighting scheme. (1) is generally
accepted while (2) is largely overlooked (at least not explicitly discussed) in existing studies.
An illustrative example. The benefit of weighting can be significant in practice. minWw B(Ww ) can
be much smaller than Bo, especially when some features (i.e., walk types) in c[T] are important while
others are not, which is true for many real-world applications. Suppose c[T] (G) is s-sparse with each
non-zero entry being some constant c. Suppose only a few of the features are useful for the prediction,
i.e., β* is ρ-sparse with each non-zero entry being some constant b, and P《 s. Suppose there is a
weighting Ww that leads to weight Y on the entries corresponding to the P important features (i.e.,
the non-zero entries in β*), and weight U for the other features where |u|《|Y|. Then it can be
shown that
minB (Ww )
B0
≤ Js +(1 一 P)(Y)2. Since P《S and |u|《|Y|, min B(Ww) is much
smaller than Bo, so the weighting can significantly reduce the error.
This example demonstrates that with a proper weighting that highlights important features and
depresses irrelevant ones for prediction, the error can be much smaller than the error without weighting.
If this benefit exceeds the overhead in learning the proper weighting, the learning performance is
improved. This trade-off is indeed visible on many datasets; see our experimental results in Section 6.
The analysis also shows that a trained AWARE model finds a weighting that automatically highlights
important substructures like edges in the graph and provide useful interpretation; see the interpretation
study in Section 7.
6 Experiments
We evaluate AWARE on graph-level prediction tasks from two domains: molecular property prediction
(61 tasks from 11 benchmarks) and social networks (4 benchmarks).2 Specifically, we consider: 37
classification (33 molecular + 4 social networks) and 28 regression (on molecular) tasks in total. For
more details about the datasets, see Appendix B.
Baseline methods. We consider WL kernels (Shervashidze et al., 2011), Morgan fingerprints
(Morgan, 1965), and N-Gram Graph (Liu et al., 2019) as baselines for graph representation learning.
For the predictor on top of the representations, we use SVM for WL kernels, and Random Forest and
XGBoost (Chen & Guestrin, 2016) for Morgan fingerprints and N-Gram Graph. We also consider
several recent end-to-end trainable GNNs that are commonly used, including GCNN (Duvenaud et al.,
2015), GAT (Velickovic et al., 2017), GIN (XU et al., 2018), Attentive FP (Xiong et al., 2019), and
PNA (Corso et al., 2020). Note that we do not consider recent GNN models that use extra edge/3D
information or self-supervised pre-training as baselines to avoid unfair comparison to AWARE—since
our analysis throughout this paper focuses on the standard setting (see Section 3). Attentive FP and
PNA were run without using extra edge information as this is not their main contribution.
Evaluation. We perform single-task learning for each task in each dataset. Each dataset is randomly
split into training, validation, and test sets with a ratio of 8:1:1, respectively. We report the average
performance across 5 runs (datasets are split independently for each run). We select optimal hyperpa-
rameters using grid search. Full hyperparameter details as well as an ablation study on their effects
are presented in Appendices D and F, respectively. For the molecular property prediction tasks, we
2Code has been submitted in the supplementary material, and will be made public upon acceptance.
7
Under review as a conference paper at ICLR 2022
Table 1: Overall performance on all 15 datasets (65 tasks). We report (# tasks with top-1 performance,
# tasks with top-3 performance). Models with no top-3 performance on a dataset are left blank.
Models that are too slow, not well tuned, or not run due to model/dataset incompatibility are marked
with “一”. For full results with error bounds, see Tables S6, S7, and S8 in the appendix.
Dataset	# Tasks	Metric	Morgan FP	WL Kernel	GCNN	GAT	GIN	Attentive FP	PNA	N-Gram Graph	AWARE
IMDB-BINARY	1	ACC	-					(0, 1)	(0, 1)		(1, 1)
IMDB-MULTI	1	ACC	-					(0, 1)	(0, 1)		(1, 1)
REDDIT-BINARY	1	ACC	-				(0, 1)		(0, 1)		(1, 1)
COLLAB	1	ACC	-				(0, 1)		(0, 1)		(1, 1)
Mutagenicity	1	ACC	-		(1,1)				(0, 1)		(0, 1)
TOX2 1	12	ROC	(0, 4)	(0, 2)				(0, 5)	(1, 3)	(4, 11)	(7, 11)
ClinTox	2	ROC				(1, 1)	(0, 1)	(0, 1)	(0, 1)		(1, 2)
HIV	1	ROC	(1, 1)				(0, 1)			(0, 1)	
MUV	17	ROC	(2, 7)	(3, 4)	(0, 8)	(0, 1)	(0, 3)	(1, 2)	(1, 6)	(1, 4)	(9, 16)
Delaney	1	RMSE						(0, 1)		(0, 1)	(1, 1)
Malaria	1	RMSE	(1, 1)						(0, 1)	(0, 1)	
CEP	1	RMSE					(1, 1)	(0, 1)	(0, 1)		
QM7	1	MAE						(0, 1)		(0, 1)	(1, 1)
QM8	12	MAE			(5, 6)		(1, 7)		(0, 1)	(0, 11)	(6, 11)
QM9	12	MAE		一	(3,12)		(4, 7)			(1,11)	(4, 6)
Total	65		-(4,13)―	~(3,6)~	(9,27)	(1, 2)	(6,22)	-(1,13)-	(2,18)	(6,41)	(33,53)
Table 2: Performance on several tasks from both molecular and social network domains. The top-3
and best performing models for each task are highlighted in gray and blue , respectively.
Task
Metric Morgan FP WL Kernel GCNN	GAT
GIN Attentive FP PNA N-Gram Graph AWARE
												
IMDB-BINARY	ACC	—	0.680±0	.022	0.698±0.026	0.568±0.047	0.696±o.o37	0.716±o.o22	O.710±o.oii	0.522±0.036	0.740±o	020
REDDIT-BINARY	ACC	—	0.892±0	.017	0.931±0.013	0.900±0.036	O.933±o.009	0.864±o.029	O.938±0.0io	0.764±0.026	0.949±o	014
COLLAB	ACC	-	0.567±0	.011	0.660±0.009	0.616±o.029	0.669±o.oi4	0.653±0.012	0.675±o.024	0.376±0.119	0.739±o	017
CT_TOX	ROC	0.813±0.036	0.830±0	.057	0.860±0.027	0.828±o.075	0.859±o.063	0.873±o.o53	0.895±o.043	0.849±0.024	0.905±o	038
FDA_APPROVED	ROC	0.795±0.084	0.862±0	.029	0.866±0.028	0.899±o.033	0.883±o.o25	0.870±o.07o	0.879±o.022	0.852±0.044	0.895±o	050
Delaney	RMSE	1.081±0.073	1.160±0	.050	0.762±0.151	0.954±O.151	o∙840±0.070	0.615±O.O26	0.922±0.122	o∙744±0.068	0.585±o	042
QM7	MAE	118.883±2.421	173.582±4.293		76.000±2.743	213.014±io.618	82.681±3.979	74.710±9.079	108.913±25.555	49.661±4.246	39.697±3.40o	
use evaluation metrics from the benchmark paper (Wu et al., 2018), except for the MUV dataset for
which we use ROC-AUC following recent studies (Hu et al., 2019; Rong et al., 2020). For the social
network tasks, we follow the evaluation metrics from (Xu et al., 2018).
Results. Due to brevity of space, we present the relative performance of AWARE compared to the
baselines in Table 1. We present complete results for all tasks with error bounds in Appendix I. In
Table 1, we observe that AWARE achieves the best performance for 33 out of the 65 tasks, while
being ranked in the top-3 performing methods for 53 tasks. In particular, AWARE (even with a simple
fully-connected predictor) significantly outperforms N-Gram Graph (which uses a powerful RF or
XGB predictor) in 44 tasks, and achieves comparable performance for all other tasks. This indicates
that AWARE can successfully learn a weighting scheme to selectively focus on the graph information
that is important for the downstream prediction task. Furthermore, we present complete results for
several tasks from different domains in Table 2. We observe that AWARE enjoys strong performance
compared to baseline models. Furthermore, being able to weight different walks in the graph, AWARE
can improve the performance of N-Gram Graph in these tasks.
Effects of weighting components. We present an ablation study of the impact of the weighting com-
ponents {Wv, Ww, Wg} of AWARE by removing them individually and comparing the performance
with the full model. In Table 3, we see that all three weighting components contribute to improved
performance for most tasks. Notably, there exist tasks for which specific weights lead to a drop in
performance. Aligning with Theorems 1 and 2 in Section 5, this indicates that weighting schemes are
successful in learning important artifacts for the downstream task only under specific conditions. 7 * * * * * * *
7 Interpretation and Visualization
AWARE uses an attention mechanism at the walk level (Ww) to aggregate crucial information from
the neighbors of each vertex (Section 4). While we have demonstrated the empirical effectiveness
of this in Section 6, we now focus on validating our analysis that AWARE can highlight important
substructures of the input graph for the prediction task. For this analysis, we use the Mutagenicity
dataset (Kazius et al., 2005), which comes with the ground-truth information that the ‘mutagen’ label
is assigned to molecules due to presence of specific chemical groups (-NO2 , -NH2) (Debnath et al.,
1991). To find substructures that AWARE uses for its prediction, we compute the importance score for
8
Under review as a conference paper at ICLR 2022
Task	No Wv	No Ww	No Wg	No Wv , Ww or Wg
IMDB-BINARY	-5.03%	+ 1.12%	-1.96%	-7.54%
NR-AR	+1.32%	-0.76%	+0.67%	+0.37%
CT_TOX	-9.00%	-2.09%	+0.70%	-3.07%
FDA_APPROVED	-7.83%	-2.39%	+1.38%	-4.16%
MUV-466	-20.08%	-16.70%	-7.44%	+1.80%
Delaney	-28.45%	-0.18%	-4.69%	-57.80%
Malaria	-0.83%	+2.10%	-1.15%	-2.32%
QM7	-11.59%	+3.74%	-18.45%	-71.39%
Table 3: Change in performance on removing weighting compo-
nents of AWARE. “+" / “-" indicate relative improvement/decrease
in performance with respect to the full AWARE model respectively.
Figure 1: Interpretation of
graph-level attention Wg for
the NR-AR classification task.
(a) Molecule
隈
避产
W a φ
(b) Grad
(c) GNNExplainer
(d) AWARE
Figure 2: Visualization of a random mutagen molecule from Mutagenicity and its important
substructures for accurate prediction captured by different interpretation techniques. Different node
colors indicate different atom types. (a) depicts the original molecule with important mutagenic atom
groups circled in red, such as NO2 and NH2 . (b), (c), and (d) demonstrate important substructures
detected by different methods. (e) is a heatmap for the edge importance scores computed by AWARE.
each bond (edge) of the molecule by using the attention scores computed via Eqn (3) (Specifically for
an edge i-j, we use [S(T)]ij + [S(T)]ji).
To further elaborate this point, we visualize a randomly chosen ‘mutagenic’ molecule in Figure 2
and the important substructures as attributed by different interpretation techniques. Figures 2b and
2c depict the interpretation of the GIN model (Xu et al., 2018) using Grad and GNNExplainer
techniques (Ying et al., 2019). The former computes gradients with respect to the adjacency matrix
and vertex features, while the latter extracts substructures with the closest property prediction to
the complete graph. Although both of these techniques are able to highlight the two NH2 groups
as important for the final prediction, they fail to highlight the NO2 group. For AWARE, we set a
threshold (≥1.0) on the importance scores for highlighting important substructures. AWARE can
successfully highlight both the NH2 and NO2 groups as important in Figure 2d. More examples with
interpretation are provided in Appendix G.
Interpretation for Wg . AWARE uses Wg to selectively weight the embeddings at the graph level for
the prediction task (Section 4). Towards interpreting Wg, we want to analyze how well it aligns with
the predictor for the downstream task. Specifically, we train AWARE for the binary classification NR-
AR task (TOX2 1 dataset) using a linear predictor with parameter w (without a non-linear activation
function). We randomly sample 200 data points, and compute their graph embeddings f[T] (G) from
AWARE. We denote the top three left singular vectors of Wg by {u1, u2, u3}. For a particular ui, we
define vi = [ui , ui , . . . (T times)] to bring ui to the same dimensional space as f[T] (G). Finally, we
plot {v1, v2, v3}, w, and the embeddings f[T] (G) for all 200 samples in Figure 1 using PCA with
n = 2 components. We observe that Wg’s largest singular vector direction aligns very well with the
parameter w of the downstream predictor. This suggests that this weight can successfully emphasize
the directions in the graph embedding space that are important for the prediction.
8	Conclusion
In this work, we present and analyze a novel attentive walk-aggregating GNN: AWARE—providing
the first provable guarantees on the learning performance of weighted GNNs. Our experiments on 65
tasks from two domains show that AWARE overall outperforms traditional and recent baselines. Our
interpretability study lends support to our algorithm design and theoretical insights. Future research
directions include the analysis of our algorithm by incorporating additional graph information.
9
Under review as a conference paper at ICLR 2022
9	Ethical S tatement
Though AWARE can be used for graph-structured data from distinct data domains, we will highlight
the ethical implications of our method for the important domain of molecular property prediction.
Having strong empirical performance for the molecular property prediction domain, AWARE can
potentially be used for efficient drug development process. Physical experiments for this task can
be expensive and slow, which can be alleviated by using AWARE for an initial virtual screening
(selecting high-confident candidates from a large pool before physical screening). A strong empirically
performing model like AWARE can help speed up the process, and provide tremendous cost savings
for this important task. However, deploying an automatic ML prediction model for such a highly
critical task must be done extremely carefully. As evidenced by Section 6, AWARE does not achieve
the best performance for all molecular property prediction tasks. Thus, AWARE may fail to identify
promising chemicals for drug development, and/or make erroneous selections. While the former
may increase the time and cost of the process, the latter might lead to failures in developing the drug.
Nevertheless, with sufficient physical experimentation performed by human experts, such unwanted
events can be minimized while still enjoying the benefits of using AWARE.
10	Reproducibility S tatement
Experiments. To ensure the reproducibility of the empirical results given in Section 6 and Ap-
pendix I, we include our code base in the supplementary material, which contains: (1) instructions
for installing necessary packages, (2) data preprocessing scripts, (3) training scripts, and (4) optimal
hyperparameters to reproduce the results for AWARE and all other baselines. In addition, we provide
information about our hyperparameter search and training strategy in Appendix D and Appendix E,
respectively. Upon acceptance, we will publicly release our code base to ensure reproducibility of our
experiments. Furthermore, full empirical results on 65 tasks along with errors bounds are provided
in Appendix I.
Theory. Complete proofs of the theorems and clear explanations of any assumptions made in Sec-
tion 5 are given in Appendix A.
References
AIDS Antiviral Screen Data. Aids antiviral screen data. https://wiki.nci.nih.gov/
display/NCIDTPdata/AIDS+Antiviral+Screen+Data, 2017. Accessed: 2020-12-
20. 27
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
arXiv preprint arXiv:2006.05205, 2020. 1, 2,7
Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A compressed sensing view
of unsupervised text embeddings, bag-of-n-grams, and lstm. International Conference on Learning
Representations, 2018. 20, 23, 24
Artem V Artemov, Evgeny Putin, Quentin Vanhaelen, Alexander Aliper, Ivan V Ozerov, and Alex
Zhavoronkov. Integrated deep learned transcriptomic and structure-based predictor of clinical trials
outcomes. bioRxiv, pp. 095653, 2016. 27
Waiss Azizian and marc lelarge. Expressive power of invariant and equivariant graph neural networks.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=lxHgXYN4bwl. 2
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate, 2014. URL http://arxiv.org/abs/1409.0473. cite
arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral presentation. 1
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems,pp. 4502-4510, 2016. 2
10
Under review as a conference paper at ICLR 2022
Lorenz C Blum and Jean-Louis Reymond. 970 million druglike small molecules for virtual screening
in the chemical universe database gdb-13. Journal of the American Chemical Society, 131(25):
8732-8733, 2009. 27
Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal sparse
dimensionality reduction and learning in the measurement domain. Techical Report, 2009. 23
Emmanuel J Candes. The restricted isometry property and its implications for compressed sensing.
Comptes rendus mathematique, 346(9-10):589-592, 2008. 23
Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE transactions on
information theory, 51(12):4203-4215, 2005. 23
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the
22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
785-794. ACM, 2016. 7
Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. Spherenet: Learning spherical
representations for detection and classification in omnidirectional images. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 518-533, 2018. 2
Gabriele Corso, Luca Cavalleri, DominiqUe Beaini, Pietro Lid, and Petar VeliCkovic. Principal
neighbourhood aggregation for graph nets. Advances in Neural Information Processing Systems,
33, 2020. 7
Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin
Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds.
correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34
(2):786-797, 1991. 8
Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the representation power of
graph neural networks in learning graph topology. Advances in Neural Information Processing
Systems 32 (NIPS 2019), 2019. 1,2
John S. Delaney. ESOL: Estimating Aqueous Solubility Directly from Molecular Structure. Journal
of Chemical Information and Computer Sciences, 44(3):1000-1005, May 2004. ISSN 0095-2338.
doi: 10.1021/ci034243x. 27
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423. 1, 3
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 1, 3
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy HirzeL Al如
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015. 1, 2, 7
Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing. Bull. Am.
Math, 54:151-165, 2017. 23
Francisco-Javier Gamo, Laura M. Sanz, Jaume Vidal, Cristina de Cozar, Emilio Alvarez, Jose-Luis
Lavandera, Dana E. Vanderwall, Darren V. S. Green, Vinod Kumar, Samiul Hasan, James R.
Brown, Catherine E. Peishoff, Lon R. Cardon, and Jose F. Garcia-Bustos. Thousands of chemical
starting points for antimalarial lead identification. Nature, 465(7296):305-310, May 2010. ISSN
1476-4687. doi: 10.1038/nature09107. 27
11
Under review as a conference paper at ICLR 2022
Kaitlyn M Gayvert, Neel S Madhukar, and Olivier Elemento. A data-driven approach to predicting
successes and failures of clinical trials. Cell ChemicaI biology, 23(10):1294-1301, 20l6. 27
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017. 1, 2, 3, 35
Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla, Roel S.
Sdnchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M. Brockway, and Aldn Aspuru-Guzik.
The Harvard Clean Energy Project: Large-Scale Computational Screening and Design of Organic
Photovoltaics on the World Community Grid. The Journal of Physical Chemistry Letters, 2(17):
2241-2251, September 2011. ISSN 1948-7185. doi: 10.1021/jz200866s. 27
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019. 8
Shiva Prasad Kasiviswanathan and Mark Rudelson. Restricted isometry property under high correla-
tions. arXiv preprint arXiv:1904.05510, 2019. 24
Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for
mutagenicity prediction. Journal of medicinal chemistry, 48(1):312-320, 2005. 8, 27, 31
Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph
convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):
595-608, 2016. 2, 35
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016. 1, 2
Johannes Klicpera, Janek Groβ, and Stephan Gunnemann. Directional message passing for molecular
graphs. arXiv preprint arXiv:2003.03123, 2020. 2
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. arXiv preprint arXiv:1511.05493, 2015. 2
Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised
representation for graphs, with applications to molecules. In Advances in Neural Information
Processing Systems, pp. 8466-8478, 2019. 1, 2, 3,4, 5, 7, 16, 18, 23, 28
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based
neural machine translation. arXiv preprint arXiv:1508.04025, 2015. 1
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention and
multi-label classification. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of
The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 1614-1623, New York, New York, USA, 20-22 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v48/martins16.html. 3
Lukasz Maziarka, Tomasz Danel, Slawomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanislaw
Jastrzkebski. Molecule attention transformer. arXiv preprint arXiv:2002.08264, 2020. 2, 3
HL Morgan. The generation of a unique machine description for chemical structures-a technique
developed at chemical abstracts service. Journal of Chemical Documentation, 5(2):107-113, 1965.
7
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602-4609, 2019. 2
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representa-
tions. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pp. 701-710, 2014. 1, 2, 3
12
Under review as a conference paper at ICLR 2022
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL
http://jmlr.org/papers/v21/20-074.html. 3
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon
Shlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019. 3
Raghunathan Ramakrishnan, Mia Hartmann, Enrico Tapavicza, and O Anatole Von Lilienfeld.
Electronic spectra from tddft and machine learning in chemical space. The Journal of chemical
physics, 143(8):084111, 2015. 27
Sebastian G Rohrer and Knut Baumann. Maximum unbiased validation (muv) data sets for virtual
screening based on pubchem bioactivity data. Journal of chemical information and modeling, 49
(2):169-184, 2009. 27
Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang.
Grover: Self-supervised message passing transformer on large-scale molecular data. arXiv preprint
arXiv:2007.02835, 2020. 2, 3, 8
Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166
billion organic small molecules in the chemical universe database gdb-17. Journal of chemical
information and modeling, 52(11):2864-2875, 2012. 27
Mark Rudelson, Roman Vershynin, et al. Hanson-wright inequality and sub-gaussian concentration.
Electronic Communications in Probability, 18, 2013. 25, 26
Kristof T Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Muller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networks. Nature communications, 8(1):1-8,
2017. 35
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):
2539-2561, 2011. 1, 2,7
Tox21 Data Challenge. Tox21 data challenge 2014. https://tripod.nih.gov/tox21/challenge/, 2014. 27
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕UkaSZ
Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 6000-6010, 2017. 1
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. 2, 3, 7, 28
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
arXiv preprint arXiv:1511.06391, 2015. 3
S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph
kernels. The Journal of Machine Learning Research, 11:1201-1242, 2010. 1, 2, 3
Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Multi-hop attention graph neural
networks. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference
on Artificial Intelligence, IJCAI-21, pp. 3089-3096. International Joint Conferences on Artificial
Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/425. URL https://doi.org/
10.24963/ijcai.2021/425. Main Track. 2
Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning.
Chemical science, 9(2):513-530, 2018. 8
Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong Li, Zhaojun
Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et al. Pushing the boundaries of molecular
representation for drug discovery with the graph attention mechanism. Journal of medicinal
chemistry, 63(16):8749-8760, 2019. 7
13
Under review as a conference paper at ICLR 2022
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings ofMachine Learning Research, pp. 2048-2057,
Lille, France, 07-09 Jul 2015. PMLR. URL http://Proceedings .mlr.ρress∕v37∕
xuc15.html. 1
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018. 1, 2, 7, 8, 9
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365-1374.
ACM, 2015. 27
Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-
Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular
representations for property prediction. Journal of chemical information and modeling, 59(8):
3370-3388, 2019. 2, 28, 35
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp.
974-983, 2018. 1
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. In Advances in neural information processing
systems, pp. 9244-9255, 2019. 9
Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer
networks. In Advances in Neural Information Processing Systems, pp. 11983-11993, 2019. 2
Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10076-10085, 2020. 3
14
Under review as a conference paper at ICLR 2022
Appendix
An Analysis of Attentive Walk-Aggregating
Graph Neural Networks
Appendix A: Our complete theoretical analysis, including the proofs for the results in the main
text, as well as additional results and discussions.
Appendix B:	Details about the datasets that we use in our experiments as well as their license
information.
Appendix C:	Details about the vertex attribute information.
Appendix D: Details about the yperparameters used in our experiments for AWARE and all other
baselines.
Appendix E:	Details about our training strategies.
Appendix F:	Figures on the effect of certain hyperparameters on model performance.
Appendix G: Further details about the MUTAGENICITY dataset, and additional interpretation
examples with statistics of the entire dataset.
Appendix H:	Additional information about our main ablation study from Table 3 as well as two
extra ablation studies.
Appendix I:	Complete experimental results on 65 tasks for AWARE and all baseline methods.
Appendix J:	Complete experimental results on 60 molecular property prediction tasks for AWARE
and models that use extra edge/3D information.
A	Complete Theoretical Analysis
In our algorithm, Wv weights the vertex embeddings fi’s, and Wg weights the final embeddings F(n),
emphasizing important directions in the corresponding space.3 On the other hand, the effect of the
weighting Sij imposed by Ww on the messages is unclear. In this section, we provide theoretical
analysis for the effect of Sij . We make the following simplification assumption:
•	Wv	=	Wg	=	I, the number of attributes is C =	1,	and the activation is linear	σ(z)	= z.
In this simplified case, the only weighting is Sij, which allows our analysis to focus on its effect. We
further assume that the number of attributes on the vertices is C = 1 to simplify the notations; the
generalization of the analysis to the case with C > 1 is straightforward. We also provide analysis for
the general case where Wv , Wg are not the identity matrix and C > 1 in Section A.3.
We will show that our algorithm incorporates the attention weighting Sij in a principled way, and thus
can potentially highlight important information and reduce irrelevant information for the prediction,
which can improve learning. To this end, we first analyze what types of information can be encoded
in our graph embedding and how they are weighted, and then analyze why the weighting can help
learning a predictor with better performance.
A. 1 The Effect of Weighting on Representation
We will show that the representation/embedding f(n) is a linear mapping of a high dimension vector
c(n) into the low dimension embedding space, where the vector c(n) records the statistics about the
walks in the graph.
3This can be seen by factorizing Wv into its singular vector decomposition Wv = UΣV > and observing
that Wv will relatively emphasize the singular vector directions with large singular values and depress those
with smaller singular values. Similar for Wg.
15
Under review as a conference paper at ICLR 2022
We first formally define the walk statistics c(n) (a variant of the count statistics defined in (Liu et al.,
2019)). Recall that we assume the number of attributes is C = 1. K is the number of possible attribute
values, and the columns of the vertex embedding parameter matrix W ∈ Rr×K are embeddings for
different attribute values u. Let W(u) denote the column for value u, i.e., W(u) = W h(u) where
h(u) is the one-hot vector of u.
Definition 3 (Walk Statistics, Restatement of Definition 1). A walk type of length n is a sequence of
n attribute values V = (v1,v2, ∙ ∙ ∙ ,vn). The walk statistics vector c(n)(G) ∈ RKn is the histogram
of all walk types of length n in the graph G, i.e., each entry is indexed by a walk type v and the
entry value is the number of walks with sequence of attribute values v in the graph. Furthermore, let
c[T] (G) is the concatenation of c(1)(G), . . . , c(T) (G). When G is clear from the context, we write
c(n) and c[T] for short.
We also introduce the following notation for describing the linear mapping projecting c(n) to the
representation f(n).
Definition 4 ('-way Column Product). Let A be a d X N matrix, and let' be a natural integer. The
' -way column product of A is a d × N' matrix denoted as A['], whose column indexed by a sequence
(i1,i2, ∙ ∙ ∙ ,i`) is the element-wise productofthe i1,i2,..., i` -th columns of A, i.e., (i1,i2,..., i`)-th
column in A['] is Ai 1 Θ Ai2 Θ ∙…。Ai' where Aj for j ∈ [N] is the j-th column in A, and Θ is the
element-wise product.
This definition is for a general matrix A. Then W [n] is an r by Kn matrix, whose columns are
indexed by walk types V = (vι, v2,…，Vn) and equal W (vι) Θ W (v2) Θ ∙∙∙ Θ W (Vn). So W[n]
matches the definition in the main text.
Definition 5 (Walk Weights). The weight of a walk type V = (V1, . . . , Vn) is
n-1
λ(V) := Y S(W(Vi), W(Vi+1))
i=1
where S(∙, ∙) is the weight function in Eq equation 2.
The following theorem then shows that f(n) can be viewed as a compressed version of the walk
statistics, weighted by the attention weights S.
Theorem 3 (Restatement of Theorem 1). The embedding f(n) is a linear mapping of the walk
statistics c(n):
f(n) = M(n)Λ(n)c(n)
where M(n) is a matrix depending only on W, and Λ(n) is a K n -dimensional diagonal matrix whose
columns are indexed by walk types V and have diagonal entries λ(V). Therefore,
f[T] := MΛc[T]	(4)
where M is a block-diagonal matrix with diagonal blocks M(1), M(2), . . . , M(T), and Λ is block-
diagonal with blocks Λ(1), Λ(2), . . . , Λ(T).
Proof. It is sufficient to prove the first statement with M(n) = W[n] , as the second one directly
follows. To this end, we will first prove the following lemma.
Lemma 1. Let Pi,n be the set of walks starting from vertex i and of length n. Then the latent vector
on vertex i is:
[F(n) ]i = X W(Vp) KF。)]®	⑸
where w(Vp) is the weight for the sequence of attribute values on p, and	k∈p[F(1)]k is the element-
wise product of all the [F(1)]k ’s on p.
16
Under review as a conference paper at ICLR 2022
Proof. We prove the lemma by induction. For n = 1, it is trivially true.
Suppose the statement is true for n - 1. Then recall that [F(n)]i is constructed by weighted-summing
up all the latent vectors [F(n-1)]j from the neighbors j of i, and then element-wise product with
[F(1) ]i = fi . So letting Ni denote the set of neighbors of i, we have by induction
[F(n)]i =	X Sji [F(n-1)]j	[F(1)]i	(6)
j∈Ni
K[F(1)]k I ) ® [F(1)]i	⑺
k∈p
[F(1)]i
By concatenating i to the walks p ∈ Pj,n-1 for all neighbors j ∈ Ni, we obtain the set of walks
starting from i and of length n, i.e., Pi,n. Furthermore, for a path obtained by concatenating i and
P ∈ Pj,n-ι, the weight is exactly Sji ∙ w(vp). Therefore,
[F(n)]i = X X	Sji ∙ W(Vp)卜F(I)]i ©
j∈Ni p∈Pj,n-1
=X W(Vp) K[F(1)]k .	(IO)
By induction, we complete the proof.	□
ESji I E	W(Vp)
j∈Ni	p∈Pj,n-1
X X	SjiW(Vp) I
j∈Ni p∈Pj,n-1
O[F(i)]k 11.	⑻
k∈p
Q[F(i)]k I I	(9)
k∈p
We now use Lemma 1 to prove the theorem statement. Recall that hk is the one-hot vector for the
attribute on vertex k. Let ep ∈ {0, 1}Kn be the one-hot vector for the walk type ofa walk p.
f(n) = F(n)1	(11)
m
= X[F(n)]i	(12)
i=1
m
=XX W(Vp) K[F(1)]k	(13)
E	W(Vp) Q[F(i)]k	(14)
p:walks of length n	k∈p
X	W(Vp) K(Whk)	(15)
p:walks of length n	k∈p
X	W(Vp)W [n]ep	(16)
p:walks of length n
W[n]	X	W(Vp)ep	(17)
p:walks of length n
W[n]Λ(n)c(n).	(18)
The third line follows from Lemma 1. The forth line follows from that the union of Pi,n for all i is
the set of all walks of length n. The sixth line follows from the definition of W [n] and ep . The last
line follows from the definitions of A(n) and c(n).	□
17
Under review as a conference paper at ICLR 2022
Remark. The theorem shows that the embedding f(n) can encode a compressed version of the
weighted walk statistics Λ(n)c(n). Note that similar to Λ(n), c(n) is in high dimension Kn. Its entries
are indexed by all possible sequences of the attribute values (v0, . . . , vn-1), and the entry value is
just the count of the corresponding sequence in the graph. Λ(n)c(n) is thus an entry-wise weighted
version of the counts, i.e., weighting the walks with attribute (v0, . . . , vn-1) by w(v0, . . . , vn-1).
The N-gram graph method is a special case of our method, by setting the message weights S(∙, ∙) to
be always 1 (and thus Λ(n) being an identity matrix). Then we have f(n) = W[n]c(n). Our method
thus enjoys greater representation power, since it can be viewed as a generalization that allows to
weight the features. What is more important, and is also the focus of our study, is that this weighting
can potentially help learn a predictor with better prediction performance. This is analyzed in the next
subsection.
Remark. The weighted walk statistics is then compressed from a high dimension to alow dimension
by multiplying with W[n]. For the unweighted case, the analysis in (Liu et al., 2019) shows that there
exists a large family of W (e.g., the entries of W are independent Rademacher variables) such that
W [n] has RIP and thus c(n) can be recovered from f(n) by compressive sensing techniques. A similar
result holds for the weighted case.
In particular, it is well known in the compressive sensing literature that when W [n] satisfies the
Restricted Isometry Property (RIP), and Λ(n)c(n) is sparse, then Λ(n)c(n) can be recovered from f(n)
(see the review in A.4). That is, f(n) preserve the information of Λ(n)c(n). This is indeed the case for
a wide family of W .
Theorem 4. If r = Ω((nsn log K”e2) where Sn is the sparsity of c(n), then there is a prior distri-
bution over W such that with probability 1 一 exp(-Ω(r1/3)), W[n] satisfies (sn e)-RIP Therefore,
if r = Ω(nsn log K) and A(n)c(n)is the sparsest vector satisfying f(n = W[n] A(n)c(n), then with
probability 1 — exp(-Ω(r1/3)), A(n)c(n)can be recovered from f(n).
Proof. The first statement follows from Theorem 9 in Appendix A.5, and the second follows from
Theorem 7 in Appendix A.4.	□
The distribution of W satisfying the above can be that with (properly scaled) i.i.d. Rademacher entries
or Gaussian entries. Since this is not the focus of our paper, below we simply assume that W has RIP
and focus on analyzing the effect of the weighting on the learning over the representations.
A.2 The Effect of Weighting on Learning
Once we have shown that the embedding f(n) can be viewed as a linear mapping of the weighted
walk statistics to low dimensional representations, we are now ready to analyze if the weighting can
potentially improve the learning.
The intuition for the benefit of appropriate weighting is simple. To illustrate the intuition, first consider
the case where we learn over the weighted features Λc[T] (instead on learning over f[T] (G) = MΛc[T]
which has an additional M). Suppose that the label is given by a linear function on c[T] with parameter
θ*, i.e., y = <θ*, c[τ]i. If A is invertible, the parameter Λ-1β* on Aq。has the same loss as β* on
c[τ]. So we only need to learn A-1β*. The sample size needed to learn ATe* on A”] will depend
on the factor kA-1β*k2kAc[T] k2, which is potentially smaller than kβ* k2 kc[T] k2 for the unweighted
case. This means fewer data samples are needed (equivalently, smaller loss for a fixed amount of
samples).
Now, consider the case of learning over f[T] (G) = MAc[T] that has an extra M. We note that c[T]
can be sparse compared to its high dimension (since likely only a very small fraction of all possible
walk types will appear in a graph). Well-established results from compressive sensing show that when
M has the Restricted Isometry Property (RIP), learning over MAc[T] is comparable to learning over
Ac[T] . Indeed, Theorem 4 in Appendix A shows when W is random and the embedding dimension r
is large enough, there are families of distributions of W such that M has RIP for Ac[T] . Thus, we
assume M has RIP and focus on the analysis of how Ww affects the weighting and the learning. In
practice, our method is more general and the parameters are learned over the data. Still, the analysis
18
Under review as a conference paper at ICLR 2022
in the special case under the assumptions can provide useful insights for understanding our method,
in particular, how the weighting can affect the learning of a predictor over the embeddings.
However, the above intuition is only for learning over a fixed weighting Λ induced by a fixed Ww .
Our key challenge is to incorporate the learning of Ww in the analysis. Formally, we consider
learning Ww from a hypothesis class W, and let Λ(Ww) and f[T] (G; Ww) denote the weights and
representation given by Ww . For prediction, we consider binary classification with the logistic loss
'(g, y) = log(1 + exp(-gy)) where g is the prediction and y is the true label. Let'd (θ, Ww) be
the risk of a linear classifier with a parameter θ on f[T] (G; Ww) over the data distribution D, and let
's(θ, WW) denote the risk over the training dataset S. Suppose we have a dataset S = {(Gi, yi)}M=ι
of M i.i.d. sampled from D, and θ is the parameter over fτ] (G) which is learned Via '2-regularization
with regularization coefficient Bθ :
1M
θ, Ww = argmin	's(θ, Ww):=	X'(hθ, f[τ](Gi； WW)),yj .	(19)
Ww∈W,kθk2≤Bθ	M i=1
To derive error bounds, suppose W is equipped with a norm k ∙ k and let N(W, e) be the e-covering
number of W w.r.t. the norm ∣∣ ∙ ∣∣ (other complexity measures on W, such as VC-dimension, can also
be used). Suppose fτ (G; WW) is Lf -Lipschitz w.r.t. the norm ∣∙∣ on W and the '2 norm on the
representation. Furthermore, let β* denote the best linear classifier on c- and let 'D denote its risk.
Theorem 5 (Restatement of Theorem 2). Assume c[τ] is s-sparse, M satisfies (2s, e0)-RIP, Λ(Ww)
is invertible and f[τ] (G; Ww) is Lf -Lipschitz over W. For any δ, e ∈ (0, 1), there are regularization
coefficient values Bθ such that with probability ≥ 1 - δ:
'D(θ, Ww) ≤ 'D + 2e + O (产MWi! + Wm∈1W B(WW) × O 卜。+ 竽
where
Ce(W) ：=	log N	(W,	+	+ log 1,	B(WW)	：= max	M(WW )c[τ](G)∣2∣Λ(Ww )-1β *∣∣2∙
∖	8Bθ Lf	)	δ	G〜D
C 八 ~	a ∕f∖ 1 衣/方一 FC-T	AI.	A /Y、	I
Proof. Since θ = θ(W) where θ(W) is defined in Lemma 2, by Lemma 2.(1), we have
,'、  〜・ ，/、 -
'd(θ,Ww) ≤ 'S(θ,W
M 卜T+log N(W, 8Bf+logδ))+ e∙
(20)
0	~-
0	~-
Furthermore, since θ, WW are the optimal solution for the regularized regression, then for any
WW ∈ W,
's(θ,Ww) ≤'s(θ(Ww),Ww).	(21)
Then by Lemma 2.(2), we have
's(θ(Ww),Ww) ≤ 'D + O(B(WW) je0 + M (log * + logN (W, 8BLJ))) + e∙ (22)
Combining the above inequalities proves the theorem.	□
Lemma 2. Suppose f[τ] (G; WW) is Lf-Lipschitz w.r.t. the norm ∣∙∣ on W and the '2 norm on the
representation. Let
1M
θ(Ww) = argmin- X '((θ, f[τ] (Gi； WW)i, yj	(23)
kθk2≤Bθ M i=1
be the optimal solution for a fixed Ww.
(1)	For any e, δ ∈ (0, 1), with probability at least 1 - δ, for any Ww ∈ W,
I'd(θ(Ww), Ww) -'s(θ(Ww), Ww)| ≤ O ( jM (rT + logN (W, 8bjL∕ ) + log δ)) + e∙
(24)
19
Under review as a conference paper at ICLR 2022
(2)	Assume that M satisfies the (2s, 0)-RIP, and c[T] is s-sparse. Also assume that Λ-1 (Ww) is
invertible over W. Then for any , δ ∈ (0, 1), there exists an appropriate choice of regularization
coefficient Bθ, such that with probability at least 1 - δ, for any Ww ∈ W,
'd(θ(Ww), Ww) ≤ 'D + O (B(Ww) ;eo + M (log ] + logN
8Bθ Lf
+ . (25)
Proof. (1) We apply a net argument on W. Let X be an /8Bθ Lf -net of W, so |X | ≤
N (W, e∕8Bθ Lf). Then for the given M, any Ww ∈ X and any θ satisfies:
I'd(θ,Ww)-'s(θ,Ww)| ≤ O (^M"(rT+0gN(WlBLjJ+0gIJ) .	(26)
Then for any Ww ∈ W, there exists a Ww ∈ X such that k Ww - Ww ∣∣ ≤ e∕8BθLf. Then letting θ
denote θ(Ww0 ), we have
I'd(θ,Ww) -'s(θ,Ww)| ≤ l'D(θ,Ww)-'d(θ,Ww)l	(27)
+ I'd(θ,Ww) -'s(θ,Ww)|	(28)
+ I'S(θ,Ww) -'S(θ,Ww0)I.	(29)
For any G with label y, we have
I'(hθ, f[T](G; Ww)i, y) - '(hθ, f[T](G; Ww0)i, y)I	(30)
≤ lhθ,f[τ](G; Ww)i -hθ,f[τ](G; Ww)i∣	(31)
=∣hθ,f[τ](G; Ww) - f[τ](G; Ww)i∣	(32)
= ∣θ∣2∣f[T](G; Ww) - f[T](G; Ww0)∣2	(33)
≤ BθLf∣Ww -Ww0∣	(34)
e
≤ 8∙	(35)
Then
i'd(θ,Ww) - 'S(θ,Ww)| ≤ 8e + O (JE 卜T + logN (W，8B⅛) + l0g!J) +1.
(36)
This proves the first statement.
(2) Let X be the set of Λc[T] for G from the data distribution. Since c[T] is s-sparse, Λc[T] is also
s-sparse. Then Λc[T] (G) - Λc[T] (G0) is 2s-sparse for any G and G0, so M satisfies (∆X, e)-RIP.
Then we can apply the theorem for learning over compressive sensing data. In particular, for a
fixed Ww, we apply Theorem 4.2 in (Arora et al., 2018). (The theorem is included as Theorem 8 in
Section A.4 for completeness. Note that choosing an appropriate λ in that theorem is equivalent to
choosing an appropriate Bθ by standard Lagrange multiplier theory.) The statement follows from that
the logistic loss function is 1-Lipschitz and convex, and that the optimal solution over Λ(Ww )c[T]
is AT (Ww )θ* with the same loss as θ* over c% Combining with a net argument similar as above
proves the statement.	□
Remark. Theorem 2 shows that the learned model has risk comparable to that of the best linear
classifier on the walk statistics, given sufficient data. Let’s now compare to the unweighted case (i.e.,
A is the identity matrix), where logN (W, &6；L于)reduces to 0, and B(Ww) reduces to
B0 :=max kc[T ](G)k2kβ* ∣2.
G〜D
(37)
So our method needs extra samples to learn Ww , leading to the extra error terms related to
log N (W, 8B；L于).On the other hand, the benefit of weighting is replacing the factor Bo with
20
Under review as a conference paper at ICLR 2022
minWw B(Ww). If there is Ww with B(Ww)《 Bo, the error is significantly reduced. Therefore,
there is a trade-off between the reduction of error for learning classifiers on an appropriate weighted
representation and the additional samples needed for learning an appropriate weighting.
The benefit of weighting can be significant in practice. minWw B(Ww) can be much smaller than
B0, especially when some features (i.e., walk types) in c[T] are important while others are not, which
is true for many real-world applications.
For a concrete example, suppose c[T] (G) is s-sparse with each non-zero entry being some constant
c. Suppose only a few of the features are useful for the prediction, i.e., β* is ρ-sparse with each
non-zero entry being some constant b, and P《s. Suppose there is a weighting Ww that leads to
weight Υ on the entries corresponding to the ρ important features (i.e., the non-zero entries in β*),
and weight U for the other features where |u|《∣Υ∣. Then it can be shown that
Bo = √sc2pρb2 = bc√ρs,
WwlB(Ww) ≤ PP(Yc)2 + (s - P)(CU)2 pρ(W
and thus
minB(Ww )
Ww
-Bo
(38)
(39)
(40)
≤
Since P《S and |u|《|Y|, min B(Ww) is much smaller than Bo, so the weighting can significantly
Ww
reduce the error. This demonstrate that with proper weighting highlighting important features and
depressing irrelevant features for prediction, the error can be much smaller than the error for without
weighting.
A.3 Analysis for More General Cases
Here we provide the analysis for the more general case where Wv and Wg are not the identity matrix
I and the number of attributes C > 1. We still make the assumption that σ is linear, while the analysis
for nonlinear σ is left for future work.
We will need to generalize the notations. Given a graph, let us define the walk statistics c(n) as
follows (generalizing Definition 1). Recall that C is the number of attributes, kj is the number of
possible values for the j-th attribute. Let KC := QjC=1 kj denote the number of possible attribute
value vector. Also, hij ∈ {0, 1}kj is the one hot vector for the j-th attribute on vertex i, and the
one hot vector for vertex i is hi, the concatenation [hi1, . . . , hiC] ∈ {0, 1}K. The `-th column of
the embedding parameter matrix Wj ∈ Rr×kj is an embedding vector for the `-th value of the
j-th attribute, and the parameter matrix W ∈ Rr×K is the concatenation W = [W1, W1, . . . , WC]
with K = PjC=-o1 kj. Finally, given an attribute vector u = [u1, u2, . . . , uC] where uj is the value
for the j-th attribute, let Let W(u) denote the embedding for u, i.e., W(u) = Wh(u) where
h(u) = [h(u1), h(u2), . . . , h(uC)] and h(uj) is the one-hot vector of uj.
Definition 6 (Walk Statistics for the General Case). A walk type of length n is a sequence of n attribute
value vectors V = (v1,v2, ∙ ∙ ∙ ,vn) where v%'s are a vector of C attributes. The walk statistics vector
c(n) (G) ∈ RKCn is the histogram of all walk types of length n in the graph G, i.e., each entry is
indexed by a walk type v and the entry value is the number of walks with sequence of attribute value
vectors v in the graph. Furthermore, let c[T] (G) is the concatenation of c(1)(G), . . . , c(T) (G). When
G is clear from the context, we write c(n) and c[T] for short.
So the definition is similar to that for the case with C = 1, except that now a walk type considers all
C attributes. Similarly, the definition of the walk weight is the same as that for C = 1, except that
the walk type definition is generalized.
To describe the linear mapping from c(n) to f(n), we need to introduce the following notation.
Definition 7. Let (Wv W){n} be a matrix with KCn column corresponding to all possible length-
n walk types, with the column indexed by a walk type v = (v1, . . . , vn) being (WvW(v1))
(Wv W (v2)) Θ∙∙∙Θ (Wv W (Vn)).
21
Under review as a conference paper at ICLR 2022
The following theorem then shows that f(n) can be a compressed version of the walk statistics,
weighted by the weighting parameter matrix Wv , Wg and also by the attention scores S.
Theorem 6. The embedding f(n) is a linear mapping of the walk statistics c(n):
f(n) = Wg(WvW){n}Λ(n)c(n).	(41)
where Λ(n) is a KCn -dimensional diagonal matrix, whose columns are indexed by walk types v and
have diagonal entries λ(v). Therefore,
f[T] := MΛc[T]	(42)
where M is a block-diagonal matrix with diagonal blocks
Wg (WvW), Wg (Wv W){2} , . . . , Wg(WvW){T}, and Λ is block-diagonal with blocks
Λ(1), Λ(2), . . . , Λ(T).
Proof. The proof is similar to that of Theorem 1. First, we note that Lemma 1 still applies to the
general case. So we can use Lemma 1 to prove the theorem statement. Recall that hk is the one-hot
vector for the attributes on vertex k. Let ep ∈ {0, 1}KCn be the one-hot vector for the walk type of a
walk p.
f(n) = WgF(n) 1	(43)
m
= Wg X[F(n)]i	(44)
i=1
m
=Wg X X W(Vp) K[F(i)]k	(45)
Wg	E	w(vp)	O[F(i)]k
p:walks of length n	k∈p
(46)
Wg	E	w(vp) O(Wv Whk)	(47)
p:walks of length n	k∈p
Wg	X	w(vp)(WvW){n}ep	(48)
p:walks of length n
Wg (WvW){n}	X	w(vp)ep	(49)
p:walks of length n
Wg(WvW){n}Λ(n)c(n).	(50)
The third line follows from Lemma 1. The forth line follows from that the union of Pi,n for all i is
the set of all walks of length n. The sixth line follows from the definitions of (Wv W){n} and ep. The
last line follows from the definition of A(n)and c(n).	□
The theorem shows that in the general case, the embedding f(T ) is also a linear mapping of the
walk statistics c(T), with a more complicated mapping Wg(WvW){n}. Similarly, with properly set
Wg, Wv, W, the linear mapping can satisfy the requirement of compressive sensing, e.g., satisfy RIP.
Then M will also satisfy RIP.
We note that Theorem 2 directly applies to the general case, under the same set of assumptions.
Finally, we observe in our experiments that a nonlinear σ helps the optimization; with a linear σ
the training of the network is less stable and leads to worse performance (see the ablation studies in
Appendix H). However, the analysis of a nonlinear σ is beyond the scope of this paper. We leave it as
an interesting future direction.
22
Under review as a conference paper at ICLR 2022
A.4 Toolbox from Compressive Sensing
For completeness, here we include the review from (Liu et al., 2019) about related concepts in the
field of compressed sensing that are important for our analysis. Please refer to (Foucart & Rauhut,
2017) for more details.
The primary goal of compressed sensing is to recover a high-dimensional k-sparse signal x ∈ RN
from a few linear measurements. Here, being k-sparse means that x has at most k non-zero entries,
i.e., |x|0 ≤ k. In the noiseless case, we have a design matrix A ∈ Rd×N and the measurement vector
is z = Ax. The optimization formulation is then
minimizex0 kx0k0 subject to Ax0 = z	(51)
where kx0k0 is `0 norm of x0, i.e., the number of non-zero entries in x0. The assumption that x is the
sparsest vector satisfying Ax = z is equivalent to that x is the optimal solution for (51).
Unfortunately, the '0 -minimization in (51) is NP-hard. The typical approach in compressed sensing
is to consider its convex surrogate using '1-minimization:
minimizex0 kx0k1 subject to Ax0 = z	(52)
where kx0k1 = Pi |x0i| is the '1 norm of x0. The fundamental question is when the optimal solution
of (51) is equivalent to that of (52), i.e., when exact recovery is guaranteed.
A.4. 1 The Restricted Isometry Property
One common condition for recovery is the Restricted Isometry Property (RIP):
Definition 8. A ∈ Rd×N is (X, )-RIP for some subset X ⊆ RN if for any x ∈ X,
(1 - )kxk2 ≤ kAxk2 ≤ (1 + )kxk2.
We will abuse notation and say (k, )-RIP if X is the set of all k-sparse x ∈ RN.
Introduced by (Candes & Tao, 2005), RIP has been used to show to guarantee exact recovery.
Theorem 7 (Restatement of Theorem 1.1 in (Candes, 2008)). Suppose A is (2k, )-RIP for an
e < √2 — L Let X denote the solution to (52), and let Xk denote the vector X with all but the k-largest
entries set to zero. Then
∣∣X — χ∣∣ι ≤ Co∣∣χk — χ∣∣ι
and
∣∣X — χ∣∣2 ≤ Cok-1∕2∣∣χk - χ∣∣ι.
In particular, if X is k-sparse, the recovery is exact.
Furthermore, it has been shown that A is (k, e)-RIP with overwhelming probability when d =
Ω(klog N) and √dAj 〜N(0,1)(∀i,j)or √dAij 〜U{ — 1, 1}(∀i, j). There are also many others
types of A with RIP; see (Foucart & Rauhut, 2017).
A.4.2 Compressed Learning
Given that AX preserves the information of sparse X when A is RIP, it is then natural to study the
performance of a linear classifier learned on AX compared to that of the best linear classifier on X.
Our analysis will use a theorem from (Arora et al., 2018) that generalizes that of (Calderbank et al.,
2009).
Let X ⊆ RN denote
X = {X : X ∈ RN , ∣X∣0 ≤ k, ∣X∣2 ≤ B}.
Let {(Xi, yi)}iM=1 be a set of M samples i.i.d. from some distribution over X × {—1, 1}. Let ' denote
a λ'-Lipschitz convex loss function. Let'd (θ) denote the risk of a linear classifier with weight
θ ∈ RN, i.e., 'd(θ) = E['(<θ, xi,y)], and let θ* denote a minimizer of'd(θ). Let 'D(θ) denote the
23
Under review as a conference paper at ICLR 2022
• i i' i ∙	i ∙ r`	∙ ,ι	∙ i , n _ τrh rl	A ♦ λ 4 / n ∖	τm Γ λ / / n	A ∖	∖ 1	ι ι , A
risk of a linear classifier With weight θ ∈ Rd over Ax, i.e., 'D (Θa) = E['(<Θa, Axi, y)], and let Θa
denote the weight learned with '2-regularization over {(Axi, yi)}i：
1M
Θa = arg min M 工'(hθ, Ax"i) + λ∣∣θ∣∣2	(53)
where λ is the regularization coefficient.
Theorem 8 (Restatement of Theorem 4.2 in (Arora et al., 2018)). Suppose A is (∆X, )-RIP. Then
with probability at least 1 - δ,
'D (θA) ≤'d (θ*) + O (λ'Bkθ*h ∣+ log 7
Mδ
for appropriate choice of λ. Here, ∆X = {x - x0 : x, x0 ∈ X} for any X ⊆ RN.
A.5 Tools for the Proof of Theorem 4
For the proof, we concern about whether the '-way column product of W has RIP. Existing re-
sults in the literature do not directly apply in our case. But following the ideas in Theorem 4.3
in (Kasiviswanathan & Rudelson, 2019), we are able to prove the following theorem for our purpose.
Theorem 9. Let X be an n × d matrix, and let R be a d × N random matrix with independent
entries Rij such that E[Rij] = 0, E[Ri2j] = 1, and |Rij | ≤ τ almost surely. Let t ≥ 2 be a constant.
Let e ∈ (0,1), and let k be an integer satisfying Sr(X) ≥ CT2kk log 哈 for some universal constant
C > 0. Then with probability at least 1 一 exp(-ce2Sr(X)∕(k2τ4t)) for some universal constant
c > 0, the matrix X R[t] /kX kF is (k, e)-RIP.
Here, sr(X) = kXk2F∕kXk2 is the stable rank of X. In our case, we will apply the theorem with X
being Id×d∕√d where Id×d ∈ Rd×d is the identity matrix.
Proof of Theorem 9. The proof follows the idea in Theorem 4.3 in (Kasiviswanathan & Rudelson,
2019). However, their analysis is for a different type of matrices ('-way Column Hadamard Product).
We thus include a proof for our case for completeness.
Let U ∈ Rdt be a vector with sparsity k, and its entries indexed by sequences (iι, i2,..., it) ∈ [d]0t.
Let ' ∈ [p], and define
t
y' :=	E	u(i1,i2,…,it) ∏R'ij.
(i1,i2,…,it)∈[d]8>t	j=1
(54)
Note that the random variables y' (' ∈ [p]) are independent. We will now estimate the ψ2-norm of y'
and then use the Hanson-Wright inequality (and its corollaries) with a net argument to establish the
concentration for the norm of XR[']u = Xy where y = (y1, . . . , yp).
Let supp(u) be the support of u. By the triangle inequality,
t
ky'kψ2 =	u(i1,i2,...,it )	R'ij
(il ,i2,...,it)∈[d]0t	j=1
Σ
(i1,i2,...,it )∈supp(u)
O(T tkU∣ι)
O 卜t√kku∣).
t
u(i1,i2,...,it )	R'ij
j=1
ψ2
(55)
(56)
(57)
(58)
24
Under review as a conference paper at ICLR 2022
D C Cie2
• 2exp(-E
Pr [∃u ∈ N, ∣kXR[t]uk2 -kX kF | > ekX ∣f] ≤ exp (k log
Next, we choose an (1/2C2)-net N in the set of all k-sparse vectors in Cdt-1 such that
|N| ≤ (k)(6C2)k ≤ exp (klog (Ckd-)) .	(59)
Note that for any k-sparse vector u ∈ Cdt-1, y = R[t]u = (y1, . . . , yp) is a random vector with
independent coordinates such that for any ` ∈ [p],
E[y' = 0, E[y2] = IIuIl2, and ∣∣y'∣ψ2 ≤ CTt√⅛∣∣2∙	(60)
Then by Corollary 1, for any fixed u ∈ Cdt-1 with |supp(u)| ≤ k (and y = R[t]u),
Pr [lkXyk2 - kX kF | > e∣∣X IlF] ≤ 2exp (一	i Sr(X) ) ≤ 2exp (	41tk2 Sr(X)
max' Ily'kψ2	)	∖ τ k
(61)
Together with the union bound over u ∈ N and uSing the aSSumption on Sr(X), we have
Sr(X))
(62)
Finally, we extend the above argument from the net to all k-SparSe vectorS. From Corollary 2, we
have
Pr h∃I ⊆ 园叫田=k, kXR∣tk2 > CIekXkFi ≤ exp (-德Sr(X)) .	(63)
FirSt aSSume that the eventS in equation 62 and equation 63 happen. Any k-SparSe vector u can
be written aS u = a + b, where a ∈ N, and b SatiSfieS |supp(b)| ≤ k and kbk2 ≤ 1/(2C1 ). Let
Ib = supp(b) ⊆ [d]须 and let b be b restricted to Ib. Let RIt be the submatrix of R[t] with columns
indexed by Ib . Then
kXR[t]uk2 = kXR[t]a + XR[t]bk2	(64)
≤ kXR[t]ak2 + kXR[t]bk2	(65)
=kXR[t]ak2 + kXRI]bk2	(66)
≤ kXR[t]ak2 + kXRIbk2kbk2	(67)
≤ (1 + e)kXkF + ɪkXRIbk2	(68)
2C2	b
≤ (1 + e1)kXkF	(69)
where the bound on kXR[t]ak2 is from equation 62 and the spectrum norm bound for kXRI[t] k2 is
from equation 63. Similarly,
kXR[t]uk2 ≥ (1-e2)kXkF.	(70)
Adjusting the constants and removing the conditioning completes the proof.	□
For proving the above Theorem 9, the Hanson-Wright Inequality and its corollaries are useful. We
thus include them here for completeness.
Theorem 10 (Hanson-Wright Inequality (Rudelson et al., 2013)). Let x = (x1 , . . . , xn) ∈ Rn be a
random vector with independent components xi which satisfy E[xi] = 0 and kxi kψ2 ≤ K. Let M be
an n × n matrix. Then for every t ≥ 0,
pr[∣χ>Mχ-E[χ>Mχ]∣>t] ≤ 2exp (-cmin{K4∣tMkF，K2ktMk2}).	(71)
25
Under review as a conference paper at ICLR 2022
Corollary 1 (Subgaussian Concentration (Rudelson et al., 2013)). Let M be a fixed n × d matrix.
Let x = (x1, . . . , xn) ∈ Rn be a random vector with independent components xi which satisfies
E[xi] = 0, E[xi2] = 1 and kxikψ2 ≤ K. Then for every t ≥ 0,
-ct2
Pr[|kMxk2 - kM kF | > t] ≤ 2exP (K 4||叼|2
(72)
Corollary 2 (Spectrum Norm of the Product (Rudelson et al., 2013)). Let B be a fixed n × p matrix,
and let G = (Gij) be a p × d matrix with independent entries that satisfy: E[Gij] = 0, E[Gi2j] = 1,
and kGij kψ2 ≤ K. Then for any a, b > 0,
Pr[kBGk2 > CK 2(akBkF + b√dkBk2)] ≤ 2exp (-a2 sr (B) - b2d).
(73)
26
Under review as a conference paper at ICLR 2022
B Dataset Details
In Table S1, we provide details about the benchmark datasets that we use in our experiments. We
evaluate AWARE on 65 tasks from 15 datasets in total: 61 from molecular property prediction domain
and 4 from social networks. We have a total of 37 classification and 28 regression tasks.
Table S1: Details on the benchmark datasets used in our experiments
Dataset	# of Tasks	Type	Domain
IMDB-BINARY (Yanardag & Vishwanathan,2015)	1	Classification	Social Network
IMDB-MULTI (Yanardag & Vishwanathan, 2015)	1	Classification	Social Network
REDDIT-BINARY (Yanardag & Vishwanathan, 2015)	1	Classification	Social Network
COLLAB (Yanardag & Vishwanathan, 2015)	1	Classification	Social Network
Mutagenicity (Kazius et al., 2005)	1	Classification	Chemistry
TOX2 1 (Tox21 Data Challenge, 2014)	12	Classification	Chemistry
ClinTox (Artemov et al., 2016; Gayvert et al., 2016)	2	Classification	Chemistry
HIV (AIDS Antiviral Screen Data, 2017)	1	Classification	Chemistry
MUV (Rohrer & Baumann, 2009)	17	Classification	Chemistry
Delaney (Delaney, 2004)	1	Regression	Chemistry
MALARIA (Gamo et al., 2010)	1	Regression	Chemistry
CEP (Hachmann et al., 2011)	1	Regression	Chemistry
QM7 (Blum & Reymond, 2009)	1	Regression	Chemistry
QM8 (Ramakrishnan et al., 2015)	12	Regression	Chemistry
QM9 (Ruddigkeit et al., 2012)	12	Regression	Chemistry
Dataset Licenses. The Delaney (Delaney, 2004), CEP (Hachmann et al., 2011), QM7 (Blum &
Reymond, 2009), QM9 (Ruddigkeit et al., 2012), MUV (Rohrer & Baumann, 2009), and MUTA-
genicity (Kazius et al., 2005) datasets are all licensed under the Copyright © of the American
Chemical Society (ACS) which allows free usage of the data and materials appearing in public
domain articles without any permission. The QM8 (Ramakrishnan et al., 2015) dataset is under
Creative Commons Attribution (CC BY) license of the American Institute of Physics (AIP) Publishing
LLC requiring no permission from the authors and publisher for using publicly released data from
the paper. The ClinTox (Gayvert et al., 2016) dataset is under the Copyright © of Elsevier Ltd.
which permits usage of public domain works and open access content without author permissions.
The Malaria (Gamo et al., 2010) dataset is licensed under Copyright © of Macmillan Publishers
Limited that allows usage for personal and noncommercial use. The Tox21 (Tox21 Data Challenge,
2014) dataset was released by NIH National Center for Advancing Translational Sciences for free
public usage as a part of a‘crowdsourced’ data analysis challenge. The HIV (AIDS Antiviral Screen
Data, 2017) dataset was released by NIH National Cancer Institute (NCI) for public usage without
any confidentiality agreement which allows access to chemical structural data on compounds. The
IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, COLLAB (Yanardag & Vishwanathan,
2015) datasets are licensed under ACM Copyright © 2015 under Creative Commons License that
allows free usage for non-commercial academic purposes.
C Description of Vertex Attributes
Molecular Graphs. In general, molecules can be viewed as graphs, where each atom is a vertex
with different attributes and each chemical bond corresponds to an edge. Assume that there are m
vertices in the graph and denote them as i ∈ {0, 1, ..., m - 1}. Each vertex m will then possess useful
attribute information, such as the atom symbol and whether the atom is acceptor or donor.4 Such
vertex attributes are folded into a vertex attribute matrix R ∈ {0, 1}m×C where C is the number of
4Note that the vertex attributes are discrete-valued in general. If there are numeric attributes, they can simply
be padded to the learned embedding for the other attributes.
27
Under review as a conference paper at ICLR 2022
attributes on each vertex i ∈ {0, 1, ..., m - 1}. Here is a concrete example:
Ri,∙ = Ri,0, Ri,1, ..., Ri,6, Ri,7],
atom symbol Ri,0 ∈ {C, Cl, I, F, . . .},
atom degree Ri,1 ∈ {0, 1, 2, 3, 4, 5, 6},
is acceptor Ri,6 ∈ {0, 1},
is donor Ri,7 ∈ {0, 1}.
The matrix R can then be translated into the vertex attribute vector set V using one-hot vectors for
the attributes.
Social Graphs. For the social network graphs that we use in our experiments, we utilize vertex
degrees as vertex attributes (i.e., C = 1).
D Hyperparameter Tuning
AWARE. We carefully perform a hyperparameter sweeping for AWARE on the different candidate
values listed in Table S2.
Table S2: Hyperparameter sweeping for AWARE
Hyperparameters	Candidate values
Learning rate	1e-3, 1e-4
# of linear layers in the predictor: L	1,2,3
Maximum Walk length: T	3, 6, 9, 12
Vertex embedding dimension: r	100, 300, 500
Random dimension: r0	100, 300, 500
Optimizer	Adam
Baseline Methods. For all the molecular baseline methods other than GAT, D-MPNN, Attentive FP,
and PNA, the hyperparameter search strategy outlined in (Liu et al., 2019) has been adopted. For
GAT and D-MPNN, We use their reported optimal hyperparameters (VelickoVic et al., 2017; Yang
et al., 2019). For Attentive FP and PNA, we performed a hyperparameter tuning that included their
reported optimal hyperparameters. For social netWork experiments, We perform hyperparameter
tuning on PNA and AttentiVe FP, and use the optimal hyperparameters reported for other baseline
methods. In addition, for some of the social netWork datasets, We only consider graphs Whose total
number of Vertices is less than a certain threshold (REDDIT-BINARY: 200, COLLAB: 100).
E Training S trategies
Training Details. We train AWARE on 9 classification and 6 regression datasets, each of Which
consisting of multiple tasks, resulting in a total of 37 classification and 28 regression tasks. Each
dataset is split into 5 different sets of training, Validation, and test sets (i.e., 5 different random seeds)
With a respectiVe ratio of 8:1:1. We train the model for 500 epochs and use early stopping on the
Validation set With a patience of 50 epochs. No learning rate scheduler is used.
GPU Specifications. In general, A NVIDIA GeForce GTX 1080 (8GB) GPU model Was used in the
training process to obtain the main experimental results. For some of the bigger datasets, We used an
NVIDIA A100 (40 GB) GPU model. The code is also submitted as a supplementary material to help
With reproducibility.
F	Effects of Different Hyperparameters
In this section, We analyze the effect of different hyperparameters on the prediction performance.
Appendix F.1 demonstrates the effect of the maximum Walk length T and the latent dimension r0,
28
Under review as a conference paper at ICLR 2022
and Appendix F.2 shows the impact of the number of layers L in the final predictor and the vertex
embedding dimension r. In general, the performance is quite stable across different hyperparameter
values. This indicates that our algorithm is friendly towards hyperparameter tuning.
F.1 EFFECT OF T AND r0
" = IOO	rt= 300	/ = 500
NR-AR
NR-AR-LBD
0.825 -
0.800 -
ɔ 0.775 -
*?
O 0.750 -
0.725 -
0.700 -
j±Ld
3	6	9	12
0.900 -
0.875-
0.850 -
0.825 -
0.800 -
0.775 -
ILLL
12
0.92-
0.90 -
0.88 -
0.86-
0.84-
0.82-
NR-AhR
0.900 1
NR-Aromatase
0.875-
U
W 0.850 -
g 0.825 -
0.800 -
IlLd
12
NR-ER
0.800 -
0.775 -
IlH
0 7H Illl
3	6	9	12
0.875-
0.850-
0.825-
0.800-
0.775-
3	6	9	12
SR-ATAD5
0.90 -
0.85-
0.80 -
0.75-
NR-PPAR-gamma
SR-ARE
0.850 -
0.825 -
0.800 -
0.775 -
0.750 -
0.85-
LIUl
3 6 9 12
3
6
9
12
0.725 J
3
6
9
12
SR-HSE	SR-MMP	SR-p53
3
6
9

3
6
9
1≡
Figure S1:	Effect of T and r0 on the prediction performance on the 12 tasks in the Tox21 dataset.
For each pair of T and r0 hyperparameter values, the model was run on 5 different seeds of data and
the average of the 5 runs is reported. Higher is better.
29
Under review as a conference paper at ICLR 2022
F.2 EFFECT OF THE NUMBER OF LAYERS L IN THE PREDICTOR AND VERTEX EMBEDDING
DIMENSION r
r= 100	r= 300	r= 500

0.90-
0.88 -
0.86-
2	3
L
0.84
1
Figure S2:	Effect of the number of linear layers L in the fully connected neural network for graph-
level prediction and the vertex embedding dimension r on the prediction performance on the 2 tasks
in the CLINTOX dataset. For each pair of L and r hyperparameter values, the model was run on 5
different seeds of data and the average of the 5 runs is reported. Higher is better.
30
Under review as a conference paper at ICLR 2022
G Visualization and Interpretation
G. 1 Mutagenicity Dataset
The Mutagenicity dataset (Kazius et al., 2005) has been introduced for the purpose of increasing
accuracy and reliability in mutagenicity predictions for molecular compounds. Mutagenicity of
a molecular compound, among many other attributes, is known to impede its ability to become a
usable drug. A mutagen is a physical or chemical factor that has the potential to alter the DNA
of an organism, which in turn increases the possibility of mutations. The Mutagenicity dataset
contains 4337 molecular structures with 2401 labeled as “mutagen”. Molecular structures in this
dataset contain around 30 atoms on average.
G.2 Additional Examples
Here, we present several more examples for the interpretation of our walk attention mechanism in
addition to Section 7. It can be seen in Figure S3 that our algorithm successfully finds that the NO2
and NH2 atom groups were specifically important for the final model predictions.
(a) Molecules
(b) Grad
(c) GNNExplainer
(d) AWARE
(e) edge importance

Figure S3: Additional examples for interpretation from the Mutagenicity dataset and their impor-
tant substructures for accurate prediction captured by different interpretation techniques. Different
node colors indicate different atom types. (a) depicts the original molecule with important mutagenic
atom groups circled in red, such as NO2 and NH2 . (b), (c), and (d) demonstrate important substruc-
tures detected by different methods. (e) is a heatmap showing the edge importances computed by
AWARE ([S(T)]ij + [S(T)]ji for edge (i, j)). If the importance is greater than or equal to 1.0, the
edge is considered important as in (d). AWARE successfully discovers the important substructures.
31
Under review as a conference paper at ICLR 2022
H Ablation Studies
Here, we include the results of several ablation study where we try to examine the effects of different
variations of our model AWARE. First, we perform an ablation study to examine the impact of each
weighting component Wv , Ww and Wg in AWARE. We individually remove one component from the
model and compare its performance to the full model. We also compare our full model to the version
with linear σ, i.e., σ(z) = z. Table S3 shows that the weighting components mostly lead to better
performance even though there are cases in which they may not. Furthermore, we can also observe
the advantage of using a non-linear activation function σ over a linear one. Second, we analyze the
change in performance when a non-trainable vertex embedding matrix W and a linear σ are used.
Table S4 demonstrates using a trainable random vertex embedding matrix W and a non-linear σ
gives overall better performance. It also shows that even with random W and a linear σ, the method
can still get decent performance—providing justification for the simplification assumptions in our
theoretical analysis. Third, we examine the advantage of using a fully-connected neural network with
multiple linear layers as a predictor over using a simple linear predictor. Table S5 suggests that using
multiple layers in the final predictor leads to better performance in general.
Table S3:	Ablation study I: Change in performance on removing/modifying components of AWARE.
“+" / “-" indicate relatively better/worse performance respectively.
Dataset	Task	No Wv	No W	No Wg	No Wv, Ww or Wg	Linear σ
IMDB-BINARY	IMDB-BINARY	-5.03%	+1.12%	-1.96%	-7.54%	-10.06%
TOX21	NR-AR	+1.32%	-0.76%	+0.67%	+0.37%	-1.12%
ClinTox	CT_TOX	-9.00%	-2.09%	+0.70%	-3.07%	-10.35%
ClinTox	FDA_APPROVED	-7.83%	-2.39%	+1.38%	-4.16%	-10.40%
MUV	MUV-466	-20.08%	-16.70%	-7.44%	+1.80%	-18.73%
Delaney	Delaney	-28.45%	-0.18%	-4.69%	-57.80%	-76.17%
Malaria	Malaria	-0.83%	+2.10%	-1.15%	-2.32%	-5.86%
QM7	QM7	-11.59%	+3.74%	-18.45%	-71.39%	-85.21%
Table S4:	Ablation study II: Change in AWARE’s performance when the vertex embedding matrix W
is randomly initialized and non-trainable, with linear σ. Underline indicates better performance.
Dataset	Task	Metric	Trainable W	Fixed Random W
IMDB-BINARY	IMDB-BINARY	ACC	0.716	0.660
TOX2 1	NR-AR	ROC-AUC	0.776	0.774
ClinTox	CT_TOX	ROC-AUC	0.889	0.764
ClinTox	FDA_APPROVED	ROC-AUC	0.869	0.774
Delaney	Delaney	RMSE	0.612	1.162
Malaria	Malaria	RMSE	1.062	1.126
QM7	QM7	MAE	41.280	96.675
Table S5:	Ablation study III: Change in AWARE’s performance when the final predictor is changed
from a multiple layer NN to a linear predictor. Underline indicates better performance.
Dataset	Task	Metric	Multiple layers	Linear predictor
IMDB-BINARY	IMDB-BINARY	ACC	0.716	0.678
TOX2 1	NR-AR	ROC-AUC	0.776	0.759
ClinTox	CT_TOX	ROC-AUC	0.889	0.880
ClinTox	FDA_APPROVED	ROC-AUC	0.869	0.870
Delaney	Delaney	RMSE	0.612	0.640
Malaria	Malaria	RMSE	1.062	1.070
QM7	QM7	MAE	41.280	415.155
32
Under review as a conference paper at ICLR 2022
I Full Results on All Classification and Regression Tasks
Here, we present complete experimental results on 61 molecular property predictions tasks as well as
4 social network tasks with standard deviations.Table S6 shows the results on the 4 social network
classification tasks. Table S7 and Table S8 present the results on the 33 classification and 28 regression
tasks in the molecular property prediction domain, respectively.
Table S6: In this table, we present the performance of 8 models on 4 classification tasks in the domain
of social networks (Morgan FP is excluded as it works on molecular graphs). Experiments are run on
5 different random seeds, and the average of the 5 reported for each task along with their standard
deviation in the subscript. The top-3 models in each task are highlighted in gray and the best one is
highlighted in blue . Higher is better.
Task	# of Classes	Metric	WL Kernel	GCNN	GAT	GIN	Attentive FP	PNA	N-Gram Graph	AWARE
IMDB-BINARY	2	ACC	0.680±0.022	0.698±0.026	0.568±o.047	0.696±0.037	0.716±0.022	0.710±0.011	0.522±0.036	0.740 ±o.020
IMDB-MULTI	3	ACC	0.403±0.027	0.459±0.033	0∙366±0.025	0∙473±0Q3i	o∙481±0.021	0∙489±0.03i	0.341±0.019	0.499 ±0.026
REDDIT-BINARY	2	ACC	0.892±0.017	0.931±0.013	0∙900±0Q36	0.933 ±0.009	o∙864±0.029	0∙938±0.010	o∙764±0.026	0∙949±0Q14
COLLAB	3	ACC	0.567±0.011	0.660±0.009	0.616±0.029	0.669 ±0.014	0.653 ±0.012	0.675±0.024	0.376±0.ιi9	0.739±o.017
Table S7: In this table, we present the performance of 9 models on 33 classification tasks from the
domain of molecular property prediction. Experiments are run on 5 different random seeds, and
the average of the 5 reported for each task along with their standard deviation in the subscript. The
top-3 models in each task are highlighted in gray and the best one is highlighted in blue . We mark
incompatible task/model pairs with a “一”. Higher is better.
Task	Metric	Morgan FP	WL Kernel	GCNN	GAT	GIN	Attentive FP	PNA	N-Gram Graph	AWARE
Mutagenicity	ACC	-	0.684±0.083	0.758±0.011	O.6O1±0.017	0.747±0.019	0.657±0.029	O.753±0.013	O.5O6±0.0iι	O.757±0.040
NR-AR	ROC	0.763±0.043	0.701±0.068	0.762±0.035	O.754±0.058	0.759±0.048	0.783±0.035	0.786±0.039	0.776±0.049	0.786±0.041
NR-AR-LBD	ROC	0.858±0.048	0.861±0.053	0.844±0.046	0.800±0.056	0.830±0.046	0.839±0.065	0.838±0.045	0.873±0.039	0.865±0.054
NR-AhR	ROC	O.89O±0.0i0	O.876±0.017	0.886±0.017	0.823±0.020	0.872±0.016	O.878±0.0iι	0.901±0.013	0.897±0.008	0.889±0.006
NR-Aromatase	ROC	0.821±0.024	0.818±0.027	0.828±0.024	0.744±0.039	0.760±0.053	O.844±0.0i9	0.837±0.0i8	O.852±0.0i3	0.861±0.019
NR-ER	ROC	0.726±0.036	0.704±0.031	0.737±0.018	0.706±0.042	0.683±0.021	O.747±0.0i4	0.738±0.030	O.754±0.020	0.765±0.028
NR-ER-LBD	ROC	0.838±0.043	0.799±0.033	0.813±0.048	0.764±0.023	0.772±0.032	O.8O8±0.037	0.815±0.039	O.834±0.030	0.853±0.059
NR-PPAR-gamma	ROC	O.84O±0.063	0.845±0.060	0.816±0.036	0.758±0.035	0.780±0.062	0.848±0.053	0.841±0.067	0.857±0.053	0.862±0.040
SR-ARE	ROC	0.820±0.016	0.801±0.029	0.809±0.014	0.735±0.020	0.794±0.020	0.809±0.028	0∙821±0qi9	O∙851±0.014	o∙828±0.011
SR-ATAD5	ROC	o∙85o±0.017	o∙814±0.020	0.827±0.052	0.754±0.052	0.803±0.050	0.807±0.047	0.821±0.055	O∙853±0.025	o∙841±0.025
SR-HSE	ROC	0.797±0.0i9	0.803±0.037	0.774±0.037	0.686±0.038	0.740±0.062	0.787±0.037	0.778±0.027	0.808±0.025	0∙820±0.026
SR-MMP	ROC	0.890±0.007	O.875±0.017	0.877±0.017	0.834±0.014	0.872±0.025	O.895±0.0i8	0.873±0.019	O.9O5±0.0i5	O.905±0.014
SR-p53	ROC	O.844±0.0i2	0.842±0.044	0.818±0.015	0.733±0.036	0.817±0.026	O.8O4±0.026	0.843±0.024	0.860±0.019	0.852±0.030
CT_TOX	ROC	0.813±0.036	0.830±0.057	0.860±0.027	0.828±0.075	0.859±0.063	0.873±0.053	0.895±0.043	0.849±0.024	0.905±0.038
FDA_APPROVED	ROC	0.795±0.084	0.862±0.029	0.866±0.028	0.899±0.033	0.883±0.025	0.870±0.070	0.879±0.022	0.852±0.044	O.895±0.050
HIV	ROC	0∙856±0.012	0.811±0.015	0.813±0.014	o∙783±0.015	0.829±0.0i4	0.796±0.0i6	0.822±0.013	O.843±0.0i7 一	O.825±0.0i4
MUV-466	ROC	0.765±0.i42	0.708±0.130	0.736±0.06i	0.749±0.i09	0.705±0.i34	0.574±0.161	0.713±0.085	O.724±0.i00	0.830±0.078
MUV-548	ROC	0.953±0.036	0.917±0.061	O.96O±0.022	0.764±0.ii7	O.793±0.ιi3	0.865±0.056	0.966±0.0i6	O.925±0.06i	0.976±0.016
MUV-600	ROC	0.536±0.098	O.536±0.106	O.57O±0.09i	0.437±0.095	0.575±0.153	0.508±0.128	0.680±0.m	O.675±0.i08	0.687±0.062
MUV-644	ROC	0.893±0.068	O∙944±0.028	o∙885±0.024	o∙762±0.161	0.749±0.094	0.776±0.133	O∙913±0.069	o∙799±0.085	0∙909±0.029
MUV-652	ROC	0.725±0.131	o∙653±0.139	0.694±0.i77	o∙493±0.124	o∙645±0.071	O.593±0.iii	o∙659±0.124	o∙688±0.117	O∙819±0.084
MUV-689	ROC	0∙676±0.277	o∙735±0.217	0∙671±0.257	o∙553±0.247	o∙775±0.088	O.452±0.220	0.666±0.172	0.669±0.203	O∙833±0.077
MUV-692	ROC	0∙693±0.199	o∙447±0.193	o∙581±0.235	0.626±0.i70	O.629±0.ιi8	0.581±0.i74	0.618±0.209	0.606±0.147	0.639±0.i94
MUV-712	ROC	0.927±0.058	0.889±0.072	0.936±0.038	0.760±0.i62	0.773±0.i95	0.946±0.040	0.881±0.119	0.812±0.103	0.931±0.059
MUV-713	ROC	O.554±0.206	0.787±0.093	0.731±0.i09	0.586±0.i09	0.567±0.183	0.526±0.094	0.648±0.093	0.715±0.089	O.781±0.i5i
MUV-733	ROC	O.7O9±0.i0i	O.7O7±0.108	0.751±0.i29	0.637±0.053	0.558±0.198	0.664±0.136	0.632±0.i68	0.696±0.084	0.819±0.127
MUV-737	ROC	0.791±0.092	0.773±0.07i	0.796±0.082	0.675±0.087	0.723±0.093	0.794±0.063	O.81O±0.iii	0.879±0.049	0.917±0.058
MUV-810	ROC	0.794±0.iii	0.875±0.052	0.714±0.124	0.588±0.166	0.682±0.188	0.604±0.084	0.782±0.i33	O.68O±0.094	O.82θ±0.103
MUV-832	ROC	0.986±0.0i4	0.964±0.034	0.926±0.042	0.923±0.036	0.918±0.129	0.714±0.121	O.96O±0.037	0.969±0.030	0.973±0.027
MUV-846	ROC	0.877±0.i28	0.884±0.066	0.911±0.067	O.863±0.i5i	0.764±0.112	0.857±0.094	O.94O±0.024	O.781±0.i00	0.964±0.027
MUV-852	ROC	0.890±0.096	0.867±0.109	0.882±0.099	o∙743±0.133	o∙735±0.194	0.863±0.047	o∙85o±0.086	o∙834±0.141	O∙917±0.090
MUV-858	ROC	0.701±0.080	0.677±0.186	o∙7o5±0.106	O∙65o±0.205	o∙746±0.134	o∙553±0.147	0∙760±0.110	O∙63o±0.148	o∙657±0.186
MUV-859	ROC	0.530±0.082	0.533±0.094	0.613±0.i73	0.499±0.076	0.607±0.i26	o∙681±0.095	0.604±0.037	0∙724±0.145	0.653±0.i86
33
Under review as a conference paper at ICLR 2022
Table S8: In this table, we present the performance of 9 models on 28 regression tasks from the
domain of molecular property prediction. Experiments are run on 5 different random seeds, and the
average of the 5 reported for each task along with their standard deviation in the subscript. The top-3
models in each task are highlighted in gray and the best one is highlighted in blue . Models that
are too slow are left blank. Lower is better.
Task	Metric	Morgan FP	WL Kernel	GCNN	GAT	GIN	Attentive FP	PNA	N-Gram Graph	AWARE
DELANEY	RMSE	1.081±0.073	1.160±0.050	0.762±0.151	0.954±o.i5i	0.840±0.070	0.615±o.026	0.922±0.122	0.744±o.068	0.585±o.042
MALARIA	RMSE	0.995±o.028	1.090±0.O37	1.141±0.057	1.136±o.O35	1.129±0.032	1.080±0.028	1.O48±o.o22	1.030±O.O39	1.056±0.036
CEP	RMSE	1.274±0.047	1.783±0.083	1.457±0.112	1.344±0.112	1.064±0.057	1.108±0.046	1.153±0.052	I	1∙409±0.029	1.233±0.040
QM7	MAE	118.883±2.421	173.582±4.293	76.000±2.743	213.014±ιo.6i8	82.681±3.979	74.710±9.079	108.913±25.555	49.661±4.246	39.697±3.400
E1-CC2	MAE	0.009±0.000	0.033±0.001	O.007±0.0θi	0.012±o.o02	O.O08±0.001	0.O12±o.00i	0.008±0.001	0.O07±0.00θ	O.0O7±0.000
E2-CC2	MAE	0.011±0.000	0.024±0.001	O.007±oq00	0.O12±o.00i	O.O08±0.000	0.013±o.0θi	0.010±0.000	0.O08±0.00θ	0.O08±0.00θ
F1-CC2	MAE	0.016±0.001	0.071±0.001	0.016±0.002	0.02θ±0.003	0.014±0.001	0.02o±0.002	0.015±0.001	0.015±0.000	O.O13±0.000
F2-CC2	MAE	0.035±0.001	0.080±0.001	0.033±o.0θi	0.O38±o.00i	0.031±o.0θi	0.039±o.0θi	0.032±0.001	0.O30±0.0θi	0.O30±o.002
E1-PBE0	MAE	0.009±0.000	0.034±0.001	O.006±0qoi	0.015±o.O04	0.007±o.00i	O.O12±0.000	0.008±0.001	0.O07±0.00θ	0.O07±0.00θ
E2-PBE0	MAE	0.011±0.000	0.029±0.001	0.O07±0Q00	0.012±0.002	0.0θ8±0.000	0.012±0.001	0.009±0.001	0.007±0.00θ	0.0θ8±0.000
F1-PBE0	MAE	0.014±0.000	0.067±0.001	0.012±0.000	0.O16±o.00i	0.011±0.0O1	0.017±o.0θi	0.013±0.001	0.012±0.000	0.O11±o.00i
F2-PBE0	MAE	0.028±0.001	0.078±0.000	O.025±0.0oι	0.030±o.00i	0.024±o.0oι	0.031±o.0θi	0.025±0.000	O.O24±0.000	0.022±o.0oi
E1-CAM	MAE	0.009±0.000	0.033±0.001	O.006±0qoi	0.012±o.o03	0.007±o.00i	0.012±o.0oι	0.007±0.000	0.O06±0.00θ	0.O06±0.00θ
E2-CAM	MAE	0.010±0.000	0.026±0.001	0.0o6±0Q00	0.011±0.0O1	0.0θ7±0.001	0.013±0.001	0.009±O.000	0.007±0.00θ	0.007±0.00θ
F1-CAM	MAE	0.015±0.001	0.072±0.001	0.O13±0.00θ	0.018±o.0θi	0.O12±o.00i	O.017±0.0θi	0.013±o.0θi	0.013±o.0θi	0.012±o.0oi
F2-CAM	MAE	0.030±0.001	0.080±0.001	0.027±0.001	0.034±o.o03	O.027±0.0oι	0.035±o.0O3	O.027±0.0oι	0.026±o.0oι	0.O24±0.0oi
MU	MAE	0.625±0.003	—	O.506±o.oi9	0.654±0.oιι	0.476±o.oo8	O.562±o.020	0.575±0.012	O.536±o.002	0.535±o.oo7
ALPHA	MAE	3.348±0.018	-	0.533±o.083	1.033±o.144	0.688±o.081	1.076±o.157	3.322±0.661	0.595±o.oo4	0.774±o.035
HOMO	MAE	0.007±0.000	-	O.O04±0.000	0.O08±0.0θi	0.O04±0.00θ	0.O09±0.00θ	0.007±0.001	0.O05±0.00θ	0.006±0.000
LUMO	MAE	0.009±0.000	—	0.0θ4±0.000	0.0θ9±0.002	0.004±0.00O	0.009±O.000	0.008±0.001	0.005±0.0O1	0.005±0.000
GAP	MAE	0.010±0.000	-	0.O06±0.00θ	0.O11±o.00i	O.0O5±0.000	O.O12±0.000	0.010±0.001	0.O07±0.00θ	0.007±0.000
R2	MAE	97.768±0.405	-	30.788±2.295	100.926±8.128	36.583±1.937	82.265±8.864	97.403±18.507	56.776±0.283	83.000±8.78O
ZPVE	MAE	0.008±0.000	—	0.0θ1±0.000	I 0.0θ4±0.002	0.0θ1±0.000	0.0θ2±0.000	0.008±0.001	0.00O±0Q00	0.0θ1±0.000
CV	MAE	1.422±0.010	—	0∙229±o.014	O.541±o.220	O.248±o.oi3	O.521±o.o62	1.318±0.256	0.334±0.004	0.586±o.O42
U0	MAE	14.657±0.153	-	0.906±O.337	1.698±1.589	2.283±o.567	2.715±1.299	22.330±3.091	0.427±o.032	0.090±o.017
U298	MAE	14.647±0.148	-	1.126±0.494	5.110±5.487	2.032±0.453	2.683±1.263	21.365±2.566	O.428±o.o32	O.086±0.o09
H298	MAE	14.650±0.146	—	0∙785±0.292	2∙066±1.159	2.308±0.580	2.930±1.093	20.880±5.738	0∙429±0.032	0.098±0.007
G298	MAE	14.651±0.149	—	0.646±0.169	2.576±1.555	2.269±0.596	4.014±1.422	19.794±3.679	0.427±0.028	O.086±0.0io
34
Under review as a conference paper at ICLR 2022
J Additional Experiments with Models that Use Extra Edge/3D
Information
Although using extra edge/3D information is not in the standard setting (as defined in Section 3), in
this section, we include the complete additional results comparing AWARE to several models that
use extra edge/3D information in their representation learning process (on 60 molecular property
prediction tasks, excluding Mutagenicity). Even though we are aware that there exist more
recent extra edge/3D approaches, we compare AWARE to several important ones: Weave Neural
Network (Kearnes et al., 2016) and Directed Message Passing Neural Networks (D-MPNN) (Yang
et al., 2019) which use extra edge information, and Deep Tensor Neural Networks (DTNN) (Schutt
et al., 2017) and Message Passing Neural Networks (MPNN) (Gilmer et al., 2017) which use extra
3D position information of the vertices. Tables S9 and S10 exhibit the results on the classification and
regression tasks, respectively. The experimental results demonstrate that AWARE remains competitive
even without any extra edge/3D information, outperforming the others in 39 tasks. This indicates that
AWARE exploits the vertex attributes and neighborhood information more effectively.
Table S9: Performance of models that use extra edge/3D information on classification tasks. The
experiments are run on 5 different random seeds, and the average of the 5 reported for each task along
with their standard deviation in the subscript. The best model in each task is highlighted in blue .
ROC-AUC is used as the evaluation metric. Higher is better.
Task	Weave	D-MPNN	AWARE
NR-AR	0.774±o.oιι	0∙729±0.057	0∙786±0.041
NR-AR-LBD	0.824±0.052	o∙816±0.079	0∙865±o.O69
NR-AHR	0.857±0.020	0∙892±0.013	O∙884±0.0io
NR-Aromatase	0.827±0.026	O∙818±o.039	0∙861±o.oi9
NR-ER	0.736±0.019	0∙724±0.035	θ∙76θ±0.017
NR-ER-LBD	0.809±0.048	0∙813±0.040	o∙851±0.056
NR-PPAR-gamma	0.803±0.029	0∙813±o.O46	0∙862±0.040
SR-ARE	0.771±0.032	0∙833±o.oi3	O∙827±o.030
SR-ATAD5	0.765±0.042	O∙819±0.050	o∙841±0.023
SR-HSE	0.749±0.043	0∙753±0.034	o∙810±0.O23
SR-MMP	0.886±0.021	0∙891±0.018	o∙905±0.017
SR-P53	0.787±o.033	0∙834±0.019	o∙841±0.020
CT_TOX	0.844±o.O31	0∙841±0.068	0∙905±0.038
FDA_APPROVED	0∙822±0.074	0∙881±0.021	θ∙895±0.050
HIV	0.556±0.059	0∙831±o.oi5	o∙823±0.008
MUV-466	0.634±0.148	0∙776±0.044	o∙830±0.078
MUV-548	0.821±0.071	0∙932±0.037	O∙973±o.oii
MUV-600	0.575±0.148	0∙559±0.145	θ∙687±0.062
MUV-644	0.786±0.089	0∙806±0.143	0∙883±o.060
MUV-652	0.721±0.133	0∙539±0.155	0∙813±o.io7
MUV-689	0.576±0.300	o∙620±0.274	o∙833±0.O77
MUV-692	0.545±0.166	0∙654±0.133	0∙574±o.146
MUV-7 1 2	0.854±0.065	O∙797±o.ιιι	0∙920±o.o50
MUV-7 1 3	0.686±0.116	0∙593±0.040	0∙781±o.151
MUV-733	0∙819±0.101	0∙746±o.153	θ∙819±0.127
MUV-737	0.784±o.033	o∙805±0.079	0∙879±o.077
MUV-810	0.593±0.165	0∙665±0.123	o∙795±0.120
MUV-832	0∙844±0.128	0∙946±0.056	o∙969±0.021
MUV-846	0∙892±0.058	0∙870±0.131	o∙964±0.027
MUV-852	0∙859±0.060	0∙785±o.ιi9	o∙917±0.090
MUV-858	0∙655±0.155	0∙749±o.i42	0∙657±o.186
MUV-859	0∙609±o.15O	0∙465±o.193	0∙653±o.186
35
Under review as a conference paper at ICLR 2022
Table S10: Performance of models that use extra edge/3D information on regression tasks. The
experiments are run on 5 different random seeds, and the average of the 5 reported for each task
along with their standard deviation in the subscript. Since only QM8 and QM9 tasks include 3D
information, DTNN and MPNN are evaluated only on those. The top-3 models in each task are
highlighted in gray and the best one is highlighted in blue . RMSE is used as the evaluation metric
for the first three tasks and MAE for the rest. Lower is better.
Task	Weave	DTNN	MPNN	D-MPNN	AWARE
Delaney	0.620±0.121	—	—	0.696±o.o65	0.589±0.05i
Malaria	1.471±O.071	—	—	1.070±O.O25	1.061±0.036
CEP J	2.573±0.292	—	—	1∙11θ±0.048	1.252±0.023
QM7 J	59.690±3.O75	—	—	68.938±5.893	40.304±3.670 ―
E1-CC2	0.007±O.OO1	0.007±O,OOO	0.006±0.000	0.006±O.OOO	0.007iO.OOO
E2-CC2	0.008±O.O00	0.007±o,ooo	0.0O7±0.00θ	0.007±o.ooi	0.008±o.ooo
F1-CC2	0.018±o,ooi	0.022±o,ooi	0.020±o.ooi	0.014±o.ooi	0.013±0.000
f2-CC2	0.035±o,ooi	0.041±o,ooo	0.039±o.oo1	0.031±o.ooi	O.O3O±0.002
E1-PBE0	0.007io,ooi	0.006±o.ooι	0.007io.oo2	0.007io.ooi	0.007±o.ooo
E2-PBE0	0.007io,ooo	0.007±o.ooo	0.006±0.ooo	0.008io.ooi	0.008±o.ooo
F1-PBE0	0.015±o,oo2	0.018±o,ooi	0.018±o.oo2	0.012±o.ooi	0.011±0.000
F2-PBE0	0.027±o,oo2	0.034±o.ooi	0.032±o.oo2	0.024±o.ooi	O.O23±0.000
E1-CAM	0.006±o,ooo	0.006±o,ooo	O.0O6±0.000	0.006±o.oo1	0.006±o.ooo
E2-CAM	0.006io,ooo	0.007io,ooo	0.006±o.ooo	0.006±0.000	0.007io.ooo
F1-CAM	0.016±o.ooi	0.019±o.ooi	0.019±o.ooi	0.012±o.ooi	O.O12±0.00i
F2-CAM	0.030±o,oo2	0.036±o.ooi	0.034±o.oo2	0.026±o.ooo	O.O24±0.00i
MU	0∙617±O,O13	θ∙244±0.006 I	0.283±O.O72	0.464±O.O13	0.535io.oo7
ALPHA	0∙680iO,O76	θ∙387±0.043	0.696±O.O94	0.408iO.O26	0.774iO.O35
HOMO	0.005±o,ooo	0.OO3±0.000	0.004iO.OOO	0.003±o.ooo	0.006±o.ooo
LUMO	0.005±o.ooo	0.003±o.ooo	0.004±o.ooo	O.OO3±0.000	0.005±o.ooo
GAP	0.007±o,ooo	0.005±o.ooo	0.006±o.ooo	0.005±0.000	0.007±o.ooo
R2	J	33.679±2.587	10.485±2.982	26.759±14.689	41.083±4.O77	83.000±8.78O
ZPVE	0.001±o,ooo	0.000±o.ooo	0.002±O.OO1	0.OOO±0.000	0.001±o.ooo
CV	0.254±o.o25	0.125±o.032	0.280±o.o92	0.173±o.oo6	0.586±O.O42
U0	2.419±1.339	1.429±o.558	3.478±3.14o	0.673±i.o24	0.090±0.017
U298	1.375±o.245	0.880io.344	2.665±3.479	0.557±o.79i	0.086±0.009
H298	1.472±o.221	0.959±o.424	1.189±o.421	0.522io.7O8	0.098±0.007
G298	1.948±o.691	0.809±o.356	1.264±o.413	0.581±o.555	0.O86±0.010
36