Under review as a conference paper at ICLR 2022
Prototypical Variational Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Variational autoencoders are unsupervised generative models that implement latent
space regularization towards a known distribution, enabling stochastic synthesis
from straightforward sampling procedures. Many works propose various regular-
ization approaches, but most struggle to compromise between proper regularization
and good reconstruction quality. This paper proposes distributing the regularization
through the latent space using prototypical anchored clusters, each with an optimal
position in the latent space and following a known distribution. Such schema
enables obtaining an appropriate number of clusters with solid regularization for
better reconstruction quality and improved synthesis control. We experiment with
our method using widespread exploratory benchmarks and report that regular-
ization anchored on prototypes’ coordinates or cluster centroids neutralizes the
adverse effects regularization terms often have on autoencoder reconstruction
quality, matching non-regularized autoencoders’ performance. We also report ap-
pealing results for interpreting data representatives with simple prototype synthesis
and controlling the synthesis of samples with prototype-like characteristics from
decoding white noise around prototype anchors.
1	Introduction
Deep generative models learn data distributions and can provide realistic synthesis once trained
(Karras et al. (2018); Brock et al. (2019); van den Oord et al. (2017); Bond-Taylor et al. (2021)). Most
popular approaches involve energy-based models, variational autoencoders, generative adversarial
networks, autoregressive models, normalizing flows, and various hybrid approaches that learn implicit
or explicit training data distribution. We focus on the latter.
Variational autoencoders (VAE) (Kingma & Welling (2014)) were one of the first deep generative
models to propose learning explicit training set distributions efficiently. They introduced a regulariza-
tion schema that warrants a latent space with good sampling properties using known distributions,
more stable training than GANs, and more efficient sampling mechanisms than autoregressive models
(Germain et al. (2015)). Like autoencoders, these models consist of an encoder network that maps
the input data x into a latent representation z and a decoder that maps the representation back into
the original data. To enable efficient synthesis, they further implement a regularization term that
forces the latent space to follow a prior distribution p(z), as observed in Figure 1a. VAEs usually
compromise between two targets: high reconstruction quality and suitable regularization of the
latent space distribution. A common trick for training VAEs is to assume that posteriors and priors
are normally distributed, which allows simple Gaussian reparametrization for end-to-end training
(Rezende et al. (2014); Kingma & Welling (2014)).
While standard VAE implementations (Kingma & Welling (2014)) use KL regularization loss to
enable mapping the latent space distribution to a known standard normal distribution, many works
have explored alternate VAE regularization functions to avoid invalid latent space loci that negatively
impact the decoder reconstruction performance. InfoVAE (Zhao et al. (2017)) explored information
maximization theories to propose regularization terms using maximum mean discrepancy (MMD)
theory (Gretton et al. (2012)). MMD implements a divergence by measuring how different the
moments of two distributions are, assuming that two distributions are identical if and only if all their
moments are the same, using kernel embeddings. Wasserstein auto-encoders (Tolstikhin et al. (2018);
Arbel et al. (2019)) use the Wasserstein distance in two different setups using either MMD or an
adversarial training schema to regularize the latent space efficiently. More recently, Vector Quantized
Variational Autoencoders (VQ-VAE) (van den Oord et al. (2017)) introduced the quantization of
1
Under review as a conference paper at ICLR 2022
latent space delivering synthesis with improved reconstruction quality, even overcoming the results
delivered by many popular GAN (Razavi et al. (2019)). Since the optimized quantized codes are
discrete following a categorical distribution, one cannot use them directly to generate new samples.
In van den Oord et al. (2017) the authors train a PixelCNN using the codes as priors to generate
novel examples, which increases much the complexity for stochastic synthesis compared to merely
controlling sampling from known latent distribution as in standard VAE.
Different works explored latent space regularization in autoencoders for deep clustering, assuming
that objects from the same class share similar features and should be somewhat grouped in the
feature space. Xie et al. (2016) proposed the Deep Embedded Clustering method, a framework
that alternately learns feature representations and clustering assignments using pre-defined template
cluster distributions. Fard et al. (2020) presented the deep-k-means algorithm, which applies k-means
in an AE embedding space to jointly cluster and learn feature representations. They proposed to use a
k-means clustering loss as the limit of a differentiable function, enabling training the network using
back-propagation (Fard et al. (2020)).
Other deep clustering methods dismiss using decoders to learn latent space distributions. Genevay et al.
(2019) proposed a differentiable deep clustering method with cluster size constraints, rewriting the
k-means clustering algorithm as an optimal transport problem with entropic regularization term. Also
benefiting from optimal transport, YM. et al. (2020) imposed an equipartition clustering constraint
and used a fast version of the Sinkhorn-Knopp algorithm (Knight (2008)) to find an approximate
optimal transpor solution. Recently, Swapping Assignments between multiple Views (SwAV) (Caron
et al. (2020b)) combined contrastive learning and prototypical clustering, reporting impressive results
in self-supervised learning. Similar to YM. et al. (2020), SwAV performs online clustering under an
equipartition constraint for each mini-batch using the Sinkhorn-Knopp algorithm.
This paper builds on this literature and presents a novel approach for VAEs latent space regularization
using prototypes online clustering. Our approach does not require significant changes in encoder-
decoder architectures and implements effective regularization using known distribution clustering
around optimal prototype anchor coordinates. We demonstrate that such distributed organization of
the latent space neutralizes adverse effects on the reconstruction quality observed when the regular-
ization group together very dissimilar samples, as often happens with standard regularization terms.
Our paper experiments with three public computer vision benchmarks (MNIST from LeCun et al.
(2010), CIFAR10, and CIFAR100 from Krizhevsky et al. (2012)) and report that our regularization
approach neutralizes the commonly observed adverse effects of regularization on autoencoder recon-
struction quality with an appropriate number of prototypes, matching non-regularized autoencoders’
performance. We report exciting results for model interpretability from prototype anchor synthesis
and investigate synthesis control capabilities from decoding white noise around prototype anchors.
We present the following contributions:
•	A method for efficient autoencoders regularization using prototypes.
•	Improved reconstruction quality using prototypical regularization compared to standard
variational autoencoders.
•	Increased stochastic synthesis control with decoding samples around prototype anchors.
2	Method
2.1	Standard VAEs formulation
As previously stated, standard variational autoencoders implement regularization through a sampling
schema that maps encoded features into a known (commonly normal) distribution 1a. Let us assume
an autoencoder that learns encoding qθ and decoding pθ distributions through the encoder and decoder
networks. A variational autoencoder should then minimize a cost function that is composed by a
reconstruction term that targets building outputs very similar to the inputs, and a regularization term,
that targets creating a latent space with a known distribution, as following:
LV AE = Lreg + Lrec	(1)
The reconstruction loss Lrec is commonly the mean squared error between a sample x and its
corresponding reconstruction x, or the binary cross-entropy that We use in this paper and is defined
2
Under review as a conference paper at ICLR 2022
as the following:
1N
Lrec = - N E Xilog Xi + (1 - Xi)lθg (1 - ^i)	(2)
i=1
where Xi is the i-th scalar value in the model output, Xi is the corresponding original input, and N is
the number of scalar values in x.
(a) Vanila Variational Autoencoders
Figure 1: Variational autoencoder with standard and prototypical regularization schemas. While the
standard regularization considers a unique anchor distribution targeting N(0, 1), our approach con-
siders regularization using K distributed anchors in the latent space targeting N (vk,1) distributions.
Unique Anchor
Distribution
Divergence Loss
(b) Prototypical Variational Autoencoders
Optimal Prototypes'
Coordinates Loss
>∣ 3dA1010」d
E 3d±olo,ld
I molɑld
Z adAlolold
The regularization loss is implemented in standard VAEs (Kingma & Welling (2014)) as the KL
divergence to a known distribution, as follows:
LKL (p, q) =	KL(p(z|X), q(z))
j
-XX p(i)log M
ji
(3)
where X is a given sample and KL(p, N(0, 1)) is the KL regularization loss between the latent space
distribution pθ and the standard normal distribution for each j latent space dimension, as described in
Kingma & Welling (2014).
Another popular regularization term was firstly proposed by Zhao et al. (2017) and builds on
maximum mean discrepancy (MMD) theory (Gretton et al. (2012)) to model the regularization loss
as the distance between different distribution moments using kernel embeddings, as follows:
LMMD(p,q) = Ep(z),p(z0)[w(z, z0)] + Eq(z),q(z0) [w(z, z0)] - 2Ep(z),q(z0)[w(z, z0)]	(4)
where w(z, z0) is any universal kernel (in this work we used the Gaussian kernel, such that w(z, z0)
-
e-
kz-z0k2
-2σ2-
).
Our paper proposes implementing VAE regularization using prototypical online clustering, where
each cluster follows a known, normal distribution. We propose using two auxiliary losses 1b, one for
finding optimal prototype coordinates, and the other constraining the cluster distribution around the
prototype to follow a known distribution, using MMD. Our regularization procedure finds optimal
prototype coordinates, assigns samples to the best match prototype using optimal transport, and
makes each cluster follow a normal distribution using the MMD loss. In the following, we detail each
of those steps and describe the overall optimization schema, including the reconstruction loss.
2.2 Constraining prototype-anchored clusters distribution
Let K be the number of trainable clusters/prototypes V = [v1, ..., vk], and vk ∈ Rd the prototype
coordinate or cluster centroid associated with the k - th prototype. To implement the local regular-
ization term given a prototype, we propose a slight modification of MMD loss that considers q(z) to
follow a normal distribution with the mean value centered at the k - th prototype with coordinates
3
Under review as a conference paper at ICLR 2022
(a) MNIST prototypical representatives
10-prot
(b) CIFAR-10 prototypical representatives
Figure 2: Exploring interpretability using synthesis from prototypes coordinates. For MNIST it is
possible to observe that prototype coordinates usually reconstruct to known digits, especially with
prototypes larger than 10, as we have 10 digits represented in the dataset. For CIFAR-10, one can
notice that the learned clusters are gathered around role models that depict associations between
foreground and background patterns, which become more detailed with the increase of the number of
prototypes available for anchoring clusters.
vk . Such regularization term follows the standard MMD regularization implementation but considers
only samples xi assigned to vk for each prototype.
Since the regularization loss must account for all V prototypes, formally, we define it as:
K
Lreg = ΣΣLMMD (pθ (z|xk), N (Vk, 1))
k=1 i
(5)
where vk are the latent space coordinates of prototype Vk , xik are the samples assigned to prototype
vk, and N(vk, 1) is a normal distribution array with means equal to vk and standard deviations equal
to 1.
4
Under review as a conference paper at ICLR 2022
2.3 Finding optimal prototypes’ coordinates in the latent space
To run the prototype-anchored regularization schema, we need to establish means for computing
optimal prototype coordinates and assigning samples to prototypes. We use a similar approach defined
in Caron et al. (2020b), which relies on optimal transport to tune the prototypes by maximizing the
coherence between OT outcome and prototypes layer softmax output.
Optimal Transport Optimal transport can be interpreted as the search for the optimal plan that
transport a probability vector r onto another probability vector c, which are both presented in the
simplex denoted by Pd := {o ∈ Rd+ : oT1d = 1} (Cuturi (2013)), where 1d is a d-dimensional
vector with all elements equal to one. The optimal coupling/transportation plan, can be seen as the
joint probability distribution between r and c. Using the notation in Cuturi (2013), lets consider
U (r, c) ∈ Rd×d as the space of probability distribution with marginals r and c, also know as the
transportation polytope:
U(r, C) := {P ∈ R+×d∣P 1d = r, PTId = c}	(6)
For two distributions o1 and o2 drawn for r and c respectively, any P ∈ U(r, c) is the joint probability
matrix of (o1, o2). With this notation, the entropy h of the joint probability P ∈ U(r, c) and the
marginal r ∈ Pd can be expressed as
dd
h(r) = -	rilogri,	h(P) = - pi,j log pi,j.	(7)
i=1	i,j=1
Then, the optimal transport solution is the matrix P ∈ U(r, c) that minimize the following equation
dM (r, c) := min hP, Mi,	(8)
P ∈U (r,c)
where h., .i is the Frobenius dot-product, M ∈ Rd×d is the cost matrix that represents the pairwise
cost of transporting bin ri to bin ci, and dM (r, c) is a distance between r and c (Cuturi (2013)).
To avoid undesirable sparse solutions, Cuturi (2013) proposed an entropic regularization term that
smooths the prediction and allows for an efficient solver using the Sinkhorn-Knopp algorithm. The
entropic function of the joint probability matrix h(P) is strongly concave and subject to h(P) ≤
h(r) + h(c) = h(rcT). Hence, the authors use -h(P) as a regularization function to obtain an
approximate solution as follows
dεM (r, c) := min hP, Mi - εh(P),	(9)
P ∈U (r,c)
where ε is a trade-off parameter that controls the smoothness of the prediction. For more details about
entroPic regularization of OT We refer the reader to CUtUri (2013); Peyre et al. (2019).
Learning the prototype coordinates Using the entropic regularized OT, finding optimal prototypes
boils doWn to minimize the cross-entroPy betWeen the softmax of the PrototyPes assignments p and
the codes/assignment q obtained Using the OT algorithm:
Lprot = - X qk log Pk, where Pk = P呻(T((Zvk)、,
V	∑k0 exP( 1 (ZTVkO)
(10)
Where the PrototyPe assignments are the dot ProdUct betWeen the featUre vector z and each PrototyPe
vk . Using the notation of Caron et al. (2020a), given B featUres in a mini-batch, We comPUte
the codes Q = [q1, q2, ..., qB] in order to maximize the similarity betWeen the set of featUres
Z = [z1, z2, ..., zB] and the PrototyPes V = [v1, v2, ..., vK] as folloWs
min Tr(QTVTZ) + εh(Q).
(11)
Similar to Caron et al. (2020a); YM. et al. (2020) We also imPose an eqUiPartition constraint to ensUre
that the clUster assignments Partition the data in groUPs of eqUal size. Hence, the sPace of the OT
solUtions reads:
5
Under review as a conference paper at ICLR 2022
Algorithm 1 Prototypical Variational Autoencoders training loop
Input: batch of samples D = {Bi}iN=1
Input: ε > 0, epochs
Input: warm-up=10
Initialize: encoder eθ, decoder dθ, and prototypes V with random weights
1:	for i = 1 to epochs do
2:	for each Bi in D do
3:	if i > warm-up then
4:	Obtain the feature vectors Zi
5:	Compute the prototype scores VT Zi
6:	Compute the codes Qi through Sinkhorn constrained to equipartition
7:	Convert prototype scores to probabilities Pi
8:	Compute prototype loss Lprot using Pi and Qi
9:	Update eθ weights and V with a gradient step using Lprot
10:	Compute MMD regularization loss Lreg
11:	Update eθ weights with a gradient step using Lreg
12:	end if
13:	Obtain the feature vectors Zi
14:	Decode the features vectors
15:	Compute reconstruction loss Lrec
16:	Update θ with a gradient step using Lrec
17:	end for
18:	end for
Q = {Q ∈ rK×b∣Q1b = Kk 1k, QT 1k = B 1b}.	(12)
This formulation is equivalent to 9 and can be efficiently solved using iterative matrix multiplication
using the Sinkhorn-Knopp algorithm (Cuturi (2013)).
2.4 Optimizing an autoencoder with prototypical latent space regularization
For running the autoencoder optimization, we implemented three sequential gradient optimization
procedures at each training step, as shown in the pseudo-code Algorithm 1. The first optimizes the
encoder and prototype weights using Lprot and targets to find optimal prototype coordinates using
the OT lead. The second optimizes the encoder weights using Lreg to make the samples assigned to
each prototype follows a normal distribution centered at the fixed prototype coordinates. The third
optimizes the encoder and decoder weights using a binary cross-entropy reconstruction loss (Lrec).
We also implement a warm-up training schema, where we train the autoencoder using only the
reconstruction loss, so we have reasonable embedding to introduce the regularization. The same
warm-up configuration is used for VAE-KL and InfoVAE models to enable a fair performance
comparison.
3	Experiments
The scope of our contribution is explicitly related to functional regularization of autoencoders’ latent
space, and therefore we compare our proposal to other two variational autoencoders, specifically those
using KL (Kingma & Welling (2014)) and MMD (Zhao et al. (2017)) divergence regularizations, and
also to a baseline non-regularized autoencoder.
3.1	Datasets and experimental setting
Our experiments used the MNIST, CIFAR-10, and CIFAR-100 public benchmarks, released under
the Creative Commons Attribution-Share Alike 3.0 and MIT licenses, respectively. We evaluated the
latent space organization characteristics and the impact on the reconstruction quality.
6
Under review as a conference paper at ICLR 2022
Figure 3: t-SNE plots considering a non-regularized autoencoder, a variational autoencoder with KL
divergence loss, and our models with 1, 10, 30 and 50 prototypes for the MNIST dataset. One can
observe that increasing the number of prototypes implement disambiguation of digits in the latent
space.
The encoder architecture consists of three (two for MNIST experiments) convolution blocks and a
bottleneck dense layer. We employed two down-sampling stages (one for each convolutional block)
to reduce the spatial input dimension by four before submitting the outcome to the bottleneck dense
layer. After the convolutional and dense layers, we also applied ReLU activation functions. The
decoder receives an input array z with the size of the latent space dimension that is ingested to a
dense layer to be reshaped into 256 activation maps of size 4 × 4 (7 × 7 for MNIST). These maps
serve as input to consecutive transposed convolution layers that up-sample the data to the original
size. A final convolution using three filters (one for MNIST) is applied to deliver the final outcome.
For the baseline variational autoencoder models, we implemented two standard dense layers for
optimizing μχ and σχ that hold the standard normal distribution parameters from which the sampling
function derives z . For implementing our prototypical VAE model, we replaced this module with
a single dense layer with 256 nodes that serves as a projection head. At inference, the dimension
of the projection head is B × 256, which represents a 256-value embedding for each sample in the
batch to be consumed by the decoder. The prototype dense layer receives this embedding and outputs
a B × K tensor, which is the activation for each sample in the batch considering each prototype.
The weights in the prototype layers, with a dimension of 256 × K also represent the K prototypes
coordinates.
We used the Adam optimizer for optimizing all trained models with a learning rate of 0.001, beta1 as
0.9, and beta2 as 0.999. We trained the models for 600 epochs for CIFAR-10 and 100 epochs for
MNIST, with 256 data samples per batch, sufficient to achieve convergence. We set initial seeds to
Tensorflow environment and NumPy library to allow a fairer comparison between different trained
models and enable our tests’ reproducibility. All experiments were carried out using V100 GPUs.
3.2	Exploring latent data prototypical clusters
We used the MNIST dataset for exploring our model skill for clustering the latent space around
prototypes. Considering that MNIST data comprises a straightforward representation of digits ranging
from 0 to 9, we expected the network to optimize clusters around the digit shapes. Therefore, as
cluster centroids, the prototypes should be decoded into a faithful representation of digits if the
number of prototypes is sufficient to discriminate unique digit characteristics, which might be good
for model interpretability uses. Figure 2a depicts the decoded prototypes for different configurations
of number of prototypes K. It is possible to observe that for K = 1, 2, 5, the number of prototypes
used is insufficient to deliver single-digit homogeneous clusters, as we have ten classes in this dataset.
With V = 10, we observe that most digits have an associated cluster, but digits 4 and 5 seem not to
be represented by their own clusters. That impression is also supported by Figure 3. When K = 20
7
Under review as a conference paper at ICLR 2022
it seems the model delivers at least one cluster for each digit, even if some clusters seem to group
ambiguous samples. Observing Figures 2a and the t-SNE (Van der Maaten & Hinton (2008)) plots
(3), it is possible to visually conclude that 30 prototypes deliver a fair clustering, and 50 prototypes
can separate the digits unequivocally in the latent space.
Considering the CIFAR-10 prototypes decoded information, one can visually inspect interesting
outcomes that depict background/foreground relationships and shed some light on which criteria
the model uses for gathering samples in the latent space. When we use only one prototype, the
corresponding prototype decoded image is brownish, almost uniform. With two prototypes, the
method seems to cluster the samples around a light background and dark foreground role model
cluster and another one with the exact opposite. We observe diverse, interesting patterns with ten
prototypes, some dedicated to samples with blue-sky background, others with a grass-like background.
For 30-prototypes, the patterns are more intricate, and we observe that prototypes begin to detail for
the background/foreground similarity criteria optimized in the training process.
Figure 4: Stochastic synthesis examples considering 10 prototypes manually chosen corresponding
to the 10 digits from our 30-prototype trained model.
For exploring controllable stochastic synthesis outgrowths, we selected ten prototypes from our 30-
prototype trained model, one for each digit, and output stochastic synthesized samples generated by
simply adding white noise to the respective prototype coordinate. As observed in Figure 4, by simply
adding noise to selected prototype coordinates and feeding that to the decoder, we can synthesize
stable, high-quality samples for the same digit. As depicted in Figure 2a, since the 10-prototype model
was not able to cluster every digit, we decided to select 10 different digits from the 30-prototype
model manually.
3.3	Exploring regularization effects on the reconstruction quality
We evaluated the impact on reconstruction quality using the CIFAR datasets to investigate if our
regularization term could alleviate the reconstruction quality burden usually observed in autoencoders
with regularized latent spaces. One can visually inspect in Figure 5 that with a proper amount
of prototypes, our method can deliver reconstructions with quality comparable to non-regularized
autoencoders, which means that it manages to neutralize adverse blurring effects caused by the regu-
larization. It is also possible to observe that VAE-ML and InfoVAE counterparts deliver considerably
more blurred outcomes and that configurations with 1 and 2 prototypes are insufficient to revert the
regularization effects completely, as expected, but 10 and 30 prototypes seem to be sufficient for that.
We also evaluated the reconstruction quality during the training process using the mean-squared error
considering training and testing samples’ reconstruction performance. Figure 6 show that our method
can match non-regularized autoencoder reconstruction performance with ten prototypes for both
CIFAR-10 and CIFAR-100 datasets. One can also visually inspect that the result with one prototype is
comparable to VAE-KL and InfoVAE as expected, and a significant improvement is already observed
using two clusters. Increasing the number of prototypes beyond 10 does not increase reconstruction
quality performance for CIFAR-10 and CIFAR-100 datasets.
4	Conclusion
This paper proposed an efficient regularization approach for variational autoencoders using prototype-
anchored latent space clustering, where each cluster follows a known distribution. Our approach
8
Under review as a conference paper at ICLR 2022
Figure 5: Reconstructed CIFAR-10 samples for different models.
(a) Reconstruction error on training samples
(b) Reconstruction error on testing samples
Figure 6: Reconstruction mean-squared error evolution for training and testing sets during training.
One can observe that at converge, with a proper number of prototypes, our model matches non-
regularized autoencoders’ performance.
enables obtaining an appropriate number of clusters with solid regularization for better reconstruction
quality and improved synthesis control. We also explored prototype decoding to understand better the
similarity criteria used for gathering samples in the latent space and observed an increased control for
stochastic synthesis using prototypes as reference landmarks for creating samples with anticipated
characteristics. We reported our results using public benchmarks and showed our model implements
an effective organization of the latent space that alleviates the adverse effects of regularization on
autoencoder reconstruction quality, matching non-regularized autoencoders’ performance.
9
Under review as a conference paper at ICLR 2022
References
Michael Arbel, Anna Korba, Adil SALIM, and Arthur Gretton. Maximum mean discrepancy
gradient flow. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
944a5ae3483ed5c1e10bbccb7942a279- Paper.pdf.
Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G. Willcocks. Deep generative modelling:
A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models,
2021.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020a.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. Proceedings of
Advances in Neural Information Processing Systems (NeurIPS), 2020b.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural
information processing systems, 26:2292-2300, 2013.
Maziar Moradi Fard, Thibaut Thonet, and Eric Gaussier. Deep k-means: Jointly clustering with
k-means and learning representations. Pattern Recognition Letters, 138:185-192, 2020.
Aude Genevay, Gabriel Dulac-Arnold, and Jean-Philippe Vert. Differentiable deep clustering with
cluster size constraints. Technical report, arXiv, 2019. URL https://arxiv.org/abs/
1910.09036. 1910.09036.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
for distribution estimation. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 881-889, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/germain15.html.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal of Machine Learning Research, 13(25):723-773, 2012. URL
http://jmlr.org/papers/v13/gretton12a.html.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=Hk99zCeAb.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Philip A. Knight. The sinkhorn-knopp algorithm: Convergence and applications. SIAM J. Matrix
Anal. Appl., 30(1):261-275, March 2008. ISSN 0895-4798. doi: 10.1137/060659624. URL
https://doi.org/10.1137/060659624.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). University of Toronto, 2012. URL http://www.cs.toronto.edu/~kriz/
cifar.html.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
10
Under review as a conference paper at ICLR 2022
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images
with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As-
sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
5f8e2fa1718d1bbcadf1cd9c7a54fb8c- Paper.pdf.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Eric P. Xing and Tony Jebara (eds.), Pro-
ceedings of the 31st International Conference on Machine Learning, number 2 in Proceedings
of Machine Learning Research, pp. 1278-1286, Bejing, China, 22-24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/rezende14.html.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-encoders.
In International Conference on Learning Representations, 2018. URL https://openreview.
net/forum?id=HkL7n1-0b.
Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learn-
ing. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In Proceedings of the 33rd International Conference on Machine Learning (ICML), pp. 478-487,
2016.
Asano YM., Rupprecht C., and Vedaldi A. Self-labelling via simultaneous clustering and rep-
resentation learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=Hyx-jyBFPr.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational
autoencoders. CoRR, abs/1706.02262, 2017. URL http://arxiv.org/abs/1706.02262.
11