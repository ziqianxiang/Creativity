Under review as a conference paper at ICLR 2022
Weight Expansion:	A New Perspective on
Dropout and Generalization
Anonymous authors
Paper under double-blind review
Ab stract
While dropout is known to be a successful regularization technique, insights into
the mechanisms that lead to this success are still lacking. We introduce the con-
cept of weight expansion, an increase in the signed volume of a parallelotope
spanned by the column or row vectors of the weight covariance matrix, and show
that weight expansion is an effective means of increasing the generalization in
a PAC-Bayesian setting. We provide a theoretical argument that dropout leads
to weight expansion and extensive empirical support for the correlation between
dropout and weight expansion. To support our hypothesis that weight expansion
can be regarded as an indicator of the enhanced generalization capability endowed
by dropout, and not just as a mere by-product, we have studied other methods that
achieve weight expansion (resp. contraction), and found that they generally lead
to an increased (resp. decreased) generalization ability. This suggests that dropout
is an attractive regularizer, because it is a computationally cheap method for ob-
taining weight expansion. This insight justifies the role of dropout as a regularizer,
while paving the way for identifying regularizers that promise improved general-
ization through weight expansion.
1	Introduction
Research on why dropout is so effective in improving the generalization ability of neural networks
has been intensive. Many intriguing phenomena induced by dropout have also been studied in this
research (Gao et al., 2019; Lengerich et al., 2020; Wei et al., 2020). In particular, it has been
suggested that dropout optimizes the training process (Baldi & Sadowski, 2013; Wager et al., 2013;
Helmbold & Long, 2015; Kingma et al., 2015; Gal & Ghahramani, 2016; Helmbold & Long, 2017;
Nalisnick et al., 2019) and that dropout regularizes the inductive bias (Cavazza et al., 2018; Mianjy
et al., 2018; Mianjy & Arora, 2019; 2020).
A fundamental understanding of just how dropout achieves its success as a regularization technique
will not only allow us to understand when (and why) to apply dropout, but also enable the design
of new training methods. In this paper, we suggest a new measure, weight expansion, which both
furthers our understanding and allows the development of new regularizers.
Broadly speaking, the application of dropout leads to non-trivial changes in the weight covariance
matrix. As the weight covariance matrix is massively parameterized and hard to comprehend, we
abstract it to the signed volume of a parallelotope spanned by the vectors of the matrix—weight
volume (normalized generalized variance (Kocherlakota & Kocherlakota, 2004)) for short. Dropout
increases this weight volume. Figure 1 illustrates the weight expansion obtained by dropout and
visualizes how it improves the generalization ability.
This leads to our hypotheses that (1) weight expansion reduces the generalization error of neural
networks (Section 3.1), and that (2) dropout leads to weight expansion (Section 3.2). We hypoth-
esize that weight expansion can be an indicator of the increased generalization ability and dropout
is ‘merely’ an efficient way to achieve it.
The weight volume is defined as the normalized determinant of the weight covariance matrix
(determinant of weight correlation matrix), which is the second-order statistics of weight vectors
when treated as a multivariate random variable. For example, when the weight vector is an isotropic
Gaussian, the weight volume is large and the network generalizes well. Likewise, if the weights
1
Under review as a conference paper at ICLR 2022
Figure 1: Visualization of weight volume and features of the last layer in a CNN on MNIST, with
and without dropout during training. Plots (b) and (d) present feature visualization for normal and
dropout CNN, respectively. We can see that the dropout network has clearer separation between
points of different classes - a clear Sign of a better generalization ability. As We Win argue in this
paper, this better generalization is related to the “weight expansion” phenomenon, as exhibited in
plots (a) and (c). For (a) and (c), the Weight volume of the normal CNN and the dropout CNN is
0.1592 and 0.3239, respectively. That is, the application of dropout during training “expands” the
weight volume. For visualization purpose, We randomly select three dimensions in the Weight co-
variance matrix and display their associated 3-dimensional parallelotope. Both (a) and (c) contains
3 × 3 = 9 such parallelotopes. We can see that those parallelotopes have a visibly larger volume in
the dropout netWork. Details of the netWorks are provided in Appendix A.
are highly correlated, then the Weight volume is small and the netWork does not generalize Well.
Technically, as a determinant, the Weight volume serves as a measure of the orthogonality of the
roWs/columns of the Weight covariance matrix. The more orthogonal the roWs/columns are, the
larger is the Weight volume—Which makes the different classes more distinguishable. Figure 1
visualizes that, With increasing Weight volume, the decision boundaries of different classes become
clearer. Note that, this is different from the “weight orthogonality” proposed in Saxe et al. (2013);
Mishkin & Matas (2015); Bansal et al. (2018): Weight volume is a statistical property of Weight
matrices treated as random variables, While orthogonality treats Weight matrices as deterministic
variables (more discussions are provided in Appendix B).
In addition to our theoretical arguments that Weight expansion promotes generalization and dropout
leads to Weight expansion (Section 3), our empirical results also support our hypotheses. To mitigate
estimation errors, We consider tWo independent Weight volume estimation methods in Section 4, i.e.,
sampling method and Laplace approximation. Section 5.1 demonstrates that the tWo methods lead
to consistent results, Which shoW that using dropout brings about Weight expansion (coherence to
hypothesis (2)) and generalization-improvement (coherence to hypothesis (1)) across data sets and
netWorks.
Then, We embed the Weight volume (a measurable quantity) to a neW complexity measure that is
based on our adaptation of the PAC-Bayesian bound (Section 3.1). Our experiments in Section 5.2,
conducted on 3 × 288 netWork configurations, shoW that, comparing to existing complexity mea-
sures Which do not consider Weight volume, the neW complexity measure is most closely correlated
With generalization (coherence to hypothesis (1)) and dropout (coherence to hypothesis (2)) for
these dropout netWorks With different dropout rates.
Finally, We use disentanglement noises (Section 4.1) to achieve Weight expansion and further to
improve generalization (or other noises to achieve contraction and further to destroy generalization,
Section 5.3). This in particular supports hypothesis (1) and indicates that Weight expansion is a key
indicator for a reduced generalization error, While dropout is a computationally inexpensive means
to achieve it. (Disentanglement noises, e.g., are more expensive.)
2	Preliminaries
Notation. Let S be a training set With m samples draWn from the input distribution D , and
S ∈ S a single sample. Loss, expected loss, and empirical loss are defined as '(fw (s)),
LD(fw) = Es〜D['(fw(s))], and LS(fw) = [ Ps∈S['(fw(s))], respectively, where fw(∙) is
a learning model. Note that, in the above notation, We omit the label y of the sample s, as it is clear
from the context. Let W , Wl , and wij be the model Weight matrix, the Weight matrix of l-th layer,
and the element of the i-th roW and the j-th column in W , respectively. Note that We omit bias for
convenience. To make a clear distinction, let W be the multivariate random variable of vec(W )
(vectorization of W), and wij be the random variable ofwij. W0 and WF denote the Weight matrix
before and after training, respectively. Further, we use W * to represent the maximum-a-posteriori
2
Under review as a conference paper at ICLR 2022
(MAP) estimate of W. Note that W F can be seen as an approximation of W * after training. We
consider a feedforward neural network fw(∙) that takes a sample so ∈ S as input and produces an
output hL, after L fully connected layers. At the l-th layer (l = 1, . . . , L), the latent representation
before and after the activation function is denoted as hl = Wlal-1 and al = actl(hl), respectively,
where act(∙) is the activation function.
Node Dropout. We focus on node dropout (Hinton et al., 2012; Srivastava et al., 2014), a popular
version of dropout that randomly sets the output ofa given activation layer to 0. We adopt a formal
definition of node dropout from Wei et al. (2020), which is more general than the usual Bernoulli
formalism. For the layer l with the output of the activation function al ∈ RNl and a given probability
ql ∈ [0, 1), we sample a scaling vector ηldo ∈ RNl (do is short for dropout) with independently and
identically distributed elements

with probability ql ,
with probability 1 - ql .
(1)
for i = 1, . . . , Nl, where Nl is the number of neurons at the layer l. With a slight abuse of notation,
we denote by ηldo a vector of independently distributed random variables, each of which has a zero
mean, and also a vector with each element being either -1 or ι-q∙ when referring to a specific
realization. As such, applying node dropout, we compute the updated output after the activation as
aldo = (1 + ηldo) al. That is, with probability ql, an element of the output with node dropout aldo
is reset to 0, and with probability 1 一 qι it is rescaled with a factor 1-1^. The index of the layer l
will be omitted when it is clear from the context. With different realizations of ηdo , a number of
subnetworks with different connectivity will be produced during training.
3	Dropout and Weight Expansion
In this section, we first formally define the weight volume, which has been informally introduced
in Figure 1. We aim to answer the following two questions: 1. Why does large weight volume
promote generalization? 2. Why does dropout expand the weight volume?
Definition 3.1 (Weight Volume) Let Σl = E[(Wl - E(Wl))(Wl - E(Wl))|] be the weight co-
variance matrix in a neural network. The weight volume (also called as normalized generalized
variance or determinant of weight correlation matrix) is defined as
Vol(Wl) , Qt粤 ∈ [0,1],	⑵
i[Σl]ii
where the denominator is the product of the diagonal elements in Σl.
Intuitively, the weight volume comes from the geometric interpretation of the determinant of the
matrix det(Σl ), with normalization over the diagonal elements of the matrix. Because Σl is a
covariance matrix and thus positive semi-definite, we have Vol(Wl) ∈ [0, 1] since 0 ≤ det(Σl) ≤
Qi[Σl]ii. We note that Vol(Wl) = 0 implies that the the weight covariance matrix does not have full
rank, suggesting that some weights are completely determined by the others, while Vol(Wl ) = 1
suggests that the weights Wl are uncorrelated. The larger Vol(Wl) is, the more dispersed are the
weights.
Before presenting empirical results, we first provide theoretical support for our first hypothesis by
adapting the PAC-Bayesian theorem (McAllester, 1999; Dziugaite & Roy, 2017) (Section 3.1): We
show that a larger weight volume leads to a tighter bound for the generalization error. In Section 3.2,
we argue that dropout can expand weight volume through the theoretical derivation.
3.1	Why does large weight volume promote generalization?
First, we support hypothesis (1) on the relevance of weight expansion through an extension of the
PAC-Bayesian theorem, and then verify it empirically in Section 5. The PAC-Bayesian framework,
developed in McAllester (1999); Neyshabur et al. (2017); Dziugaite & Roy (2017), connects weights
with generalization by establishing an upper bound on the generalization error with respect to the
Kullback-Leibler divergence (DKL) between the posterior distribution Q and the prior distribution
P of the weights. In the following, we extend this framework to work with Vol(Wl).
3
Under review as a conference paper at ICLR 2022
ΦE⊃-O> 1⅛ΦΛΛ
(a) Normalized covariance matrix (normal)
Figure 2: We train two small NNs (64-32-16-10 network with/without dropout) on MNIST. (b)
shows the changes in weight volume along with epochs. There are four sampling points (①②③④) to
track the absolute normalized covariance matrix (correlation matrix) of normal gradient updates (a)
and dropout gradient updates (c). The results for VGG16 on CIFAR-10 are provided in Appendix C.
Let P be a prior and Q be a posterior over the weight space. For any δ > 0, with probability 1 - δ
over the draw of the input space, we have (McAllester, 1999; Dziugaite & Roy, 2017)
EW〜Q [LD (fW)] ≤ EW〜Q [LS (fW)] +
IDKL (Q||P )+lnm
2∣	2(m - 1)
(3)
Given a learning setting, Dziugaite & Roy (2017); Neyshabur et al. (2017); Jiang et al. (2020b)
assume P = N(μP, ΣP) and Q = N(μQ, Σq). Thus, DKL(Q||P) can be simplified to
DKL(Q||P) = 2 (tr(∑-1 ∑Q) - k + (μp - μQ)|(∑P)-1 (μp - μQ) + ln d|(^) , (4)
where k is the dimension ofW, tr denotes the trace, and det denotes the determinant. The derivation
is given in Appendix D. To simplify the analysis, Neyshabur et al. (2017; 2018a) instantiate the prior
P to be a vec(W0) (or 0) mean and σ2 variance Gaussian distribution, and assume that the posterior
Q to also be a vec(W F) mean spherical Gaussian with variance σ2 in each direction.
We relax their assumption by letting Q be a non-spherical Gaussian (the off-diagonal correlations
for same layer are not 0, whereas those for different layers are 0), while retaining the assumption on
the σ2 variation in every direction (but allowing arbitrary covariance between them). This retains
both tr(ΣP-1ΣQ) = k and i [Σl,P]ii = i[Σl,Q]ii for all l, which provides the following.
DKL(QIIP ) = 1 X( ≡⅛W0iF +ln 鹏)
2	l	σl	det(Σl,Q) *
=1X( IIWF - W0IIF + ln 1	)
= 2/( σ2	+ vol(Wι) >
(5)
Details of the derivation are provided in Appendix D. From Equations (3) and (5), we notice that the
PAC-Bayesian upper bound of the generalization error becomes smaller when vol(Wl) increases,
which is also demonstrated by wide experiments in Section 5.
3.2 Why does dropout expand the weight volume?
In the above PAC-Bayesian framework, we assume the (non-spherical) Gaussian distributed weights
and connect weight volume with generalization. In this subsection, to connect weight volume with
dropout through correlation (of gradient updates and weights), and further to support hypothesis
(2), we consider the arbitrarily distributed (with finite variance) gradient updates of l-th layer,
∆l for a normal network and ∆ldo for the corresponding dropout network (all learning settings are
same except for dropout). That is, vec(∆l)〜Mδ(4δi , ∑δi), and dropout noise ηfo is applied on
gradient updates through
vec(∆do) = (1 + 1 0 nd。) Θ VeC(∆ldo0),
where the random vector (1 + 1 0 nd。) ∈ RNlT Nl with 0 being Kronecker product. As we aim to
study the impact of nd。on the correlation of gradient updates, We assume that vec(∆do0) also obeys
the distribution Mδ (44,, ∑δi) (for simplifying correlation analysis). Thus, with a slight abuse of
notation, denote (1 +10 nd。) Θ vec(∆do0) as nd。to be applied on gradient updates of corresponding
neurons for notational convenience. Then, we can now obtain Lemma 3.1.
4
Under review as a conference paper at ICLR 2022
Lemma 3.1 (Dropout reduces correlation of gradient updates.) Let ∆ij and ∆i0j0 be the gradi-
ent updates for wij and wi0j0 of some layer in a normal network, where i 6= i0. Then the (absolute)
correlation between ∆idjo and ∆id0oj0 in a dropout network can be upper bounded by that in the nor-
mally trained network with the same setting, i.e.,
ρ∆idjo,∆id0oj0	≤ (1 - q)ρ∆ij,∆i0j0 .	(6)
Details of the proof are given in Appendix E. The equality holds if, and only if, q = 0 or ρ∆ij ,∆i0j0 =
0. Lemma 3.1 shows that the dropout noise decreases correlation among gradient updates, which
is also shown by experiments in Figure 2 and Appendix C. That is, the correlation among gradient
updates in a normal network (Figure 2a) is obviously larger than the correlation in a dropout network
(Figure 2c). Next, Lemma 3.1 leads to the following lemma.
Lemma 3.2 (Dropout reduces correlation of updated weights.) The (absolute) correlation of up-
dated weights in a dropout network is upper bounded by that in the normal network. That is,
I	-	ICov(Wij + ∆ij , Wi0j0 + ∆i0 j0 )∣
IPwij +△?*" +”o/ ≤ y=====v==========v====
V (Var(Wij + ∆ij) + q 1-q " ) (Var(Wi0j0 + AjO) + —1-qj)
≤ IPwij+∆ij,wi0j0+∆i0j0 I.
(7)
The equality holds if and only if q = 0 or Cov(Wij + ∆ij , Wi0j0 + ∆i0j0 ) = 0, and the full proof
is given in Appendix F. It is clear that weight volume (equals the determinant of weight correlation
matrix) is one of measures of correlation among weights (refer to Figure 1), and small correlation
can lead to large weight volume. We also provide more discussions about the relationship between
correlation and weight volume in Appendix G. Consistent with Lemma 3.2, we show in addition, that
gradient updates with large correlation (Figure 2a) can lead to a large correlation of weights—and
thus to a small weight volume (Figure 2b), while gradient updates with small correlation (Figure 2c,
dropout) can lead to a small correlation of weights—and thus to a large weight volume.
4	Weight Volume Approximation
To mitigate estimation errors, we use two methods to estimate the weight volume. One is the sam-
pling method, and the other is Laplace approximation (Botev et al., 2017; Ritter et al., 2018) of
neural networks, which models dropout as a noise in the Hessian matrix. Laplace approximation
enables us to generalize dropout to other disentanglement noises (Section 4.1). Note that some of
other popular regularizers, e.g., weight decay and batch normalization, improve generalization not
through weight expansion (more discussions are provided in Appendix H).
Sampling Method. First, we use a sharpness-like method (Keskar et al., 2017; Jiang et al., 2020b)
to get a set of weight samples drawn from (W + U) such that |L(fW +U) - L(fW)| ≤ , where
vec(U)〜N(0, σUI) is a multivariate random variable obeys zero mean Gaussian with σu = 0.1
(more details are given in Appendix I). The covariance between Wij and Wi0j 0 can then be computed
by Cov(Wij, Wi0j0) = E[(Wij - E[Wij])(Wi0j0 - E[Wi0j0])]. Finally, we can estimate vol(W)
according to Equation (2).
Laplace Approximation. Laplace approximation has been used in posterior estimation in Bayesian
inference (Bishop, 2006; Ritter et al., 2018). It aims to approximate the posterior distribution
p(W|S) by a Gaussian distribution, based on the second-order Taylor approximation of the ln pos-
terior around its MAP estimate. Specifically, for layer l and given weights with an MAP estimate
W； on S, We have
lnP(Wl|S) ≈ lnP(Vec(W*)|S)- ∣ (Wl- Vec(WJ))|∑-1(Wi - Vec(Wι*)),	(8)
where Σl-1 = Es [Hl] is the expectation of the Hessian matrix over input data sample s, such that
the HeSSian matrix Hl is given by Hl = ∂ved(WfW¾wl).
It is worth noting that the gradient is zero around the MAP estimate W*, so the first-order Taylor
polynomial is inexistent. Taking a closer look at Equation (8), one can find that its right hand side
is exactly the logarithm of the probability density function of a Gaussian distributed multivariate
5
Under review as a conference paper at ICLR 2022
random variable with mean Vec(Wl*) and covariance ∑ι, i.e., WI 〜 N(Vec(Wj), ∑), where ∑
can be viewed as the covariance matrix of Wl .
Laplace approximation suggests that it is possible to estimate Σl through the inverse of the Hessian
matrix, because Σl-1 = Es[Hl]. Recently, Botev et al. (2017); Ritter et al. (2018) have leveraged in-
sights from second-order optimization for neural networks to construct a Kronecker factored Laplace
approximation. Differently from the classical second-order methods (Battiti, 1992; Shepherd, 2012),
which suffer from high computational costs for deep neural networks, it takes advantage of the fact
that Hessian matrices at the l-th layer can be Kronecker factored as explained in Martens & Grosse
(2015); Botev et al. (2017). I.e.,
Hl = al-1aι-1 ③
∂ 2'(fw (S))
∂hι ∂hι
'------{------}
Hl
Aι-ι Z) Hι,
(9)
X—{—}
Al-1
where Aι-1 ∈ RNl-1 ×Nl-1 indicates the subspace spanned by the post-activation of the previous
layer, and Hι,do ∈ RNl ×Nl is the Hessian matrix of the loss with respect to the pre-activation of
the current layer, with Nι-1 and Nι being the number of neurons at the (l - 1)-th and l-th layer,
respectively. Further, through the derivation in Appendix J, we can estimate Vol(Wι ) efficiently by
having
det(∑ι) ≈ det ((EsAι-i])T)Nl ∙ det ((EsHι])T)NlT	(10)
and
Y[∑ι]ii ≈ (Y [(Es[Aι-ι])T] JNl ∙ (Y [(EsHι])T]dd)Nlτ.	(11)
Note that, for the case with dropout, dropout noise ηdo is taken into account to compute
Es[Aι-ι] and Es[Hι] (Appendix K). That is, Es[Ad-J = Esw。[ad-ι(a]ι)1] and Es[Hdo]=
Es,ηd0 []hdfWz(So)], where。巴 and hfo are the activation and pre-activation of the corresponding
layer in a dropout network.
4.1	Other Disentanglement Noise
Whilst introducing dropout noise can expand weight volume (as shown in the experiments of
Sections 5.1 and 5.2), it is interesting to know (1) whether there are other noises that can
lead to weight expansion, and, if so, whether they also lead to better generalization perfor-
mance, and (2) whether there are noises that can lead to weight contraction, and, if so, whether
those noises lead to worse generalization performance.
Algorithm 1 Gradient update with disentanglement noise.
Input: minibatch {si}in=1, activation noise n。, node loss
noise nh, noise strengths λ1, λ2.
A Forward Propagation: apply noise λι ∙ n。to activations.
A Back Propagation: apply noise λ? ∙ n% to node loss.
A Calculate g = ɪ- Pn=I VW ('(fw(Si))) by consider-
ing both noises as indicated above.
A Use g for the optimization algorithm.
As indicated in Equation (9) and
Appendix J, the weight covariance
matrix can be approximated by the
Kronecker product of (E[Aι-1])-1
and (E[Hι])-1. Disentanglement
noises, as an attempt, can be in-
jected into either term during the gra-
dient update procedure to increase
det(E[Aι-1])-1 and det(E[Hι])-1
for the purpose of weight expansion.
The first option is to inject ηa to activations during forward propagation, as follows.
Activation Reverse Noise n。to increase det(E[A])-1. We define n。as n。〜N(μ, r(∑a)), where
Σa = (E[A])-1 and r(Σ) is to reverse the signs of off-diagonal elements of the matrix Σ.
The second option is to inject nh into the node loss during backpropagation, as follows.
Node Loss Reverse Noise n% to increase det(E[H])-1. We define n% as n% 〜N(μ, r(∑h)), where
Σh = (E[H])-1.
Algorithm 1 presents our simple and effective way to obtain disentanglement noise, where λ1 and
λ2 are hyper-parameters to balance the strengths ofn。 and nh. In the experiments in Section 5.3, we
will assign expansion noises and contraction noises according to their impact on the weight volume,
where expansion noises are obtained from Algorithm 1 and contraction noises are obtained from
stochastic noises on weights.
6
Under review as a conference paper at ICLR 2022
Table 1: Weight volume on VGG networks (weight decay = 0).
	Method	VGG11	VGG11(dropout)	VGG16	VGG16(dropout)	VGG19	VGG19(dropout)
	Sampling	0.06±0.02	0.13±0.02	0.05±0.02	0.12±0.02	0.04±0.02	0.12±0.02
	Laplace	0.0568	0.1523	0.0475	0.1397	0.0453	0.1443
	Generalization Gap	-0.8030-	0.5215	-0.8698-	0.2996	-171087-	0.1055
	Sampling	0.07±0.02	0.14±0.02	0.06±0.02	0.14±0.02	0.04±0.02	0.12±0.02
	Laplace	0.0537	0.1578	0.0409	0.1620	0.0506	0.1409
	Generalization Gap	-3∏208-	2.1635	-37541-	1.2714	-4.4787-	0.4373
5 Experiments
We have conducted a number of experiments to test our hypotheses that (1) weight expansion helps
reduce the generalization error and (2) dropout increases the weight volume. More than 900
networks are trained. First, we confirm the role of weight expansion in connecting dropout and gen-
eralization by estimating the weight volume with two independent methods (Section 5.1). Second,
we extend the PAC-Bayesian theorem with the awareness to weight volume, and show that the new
complexity measure can significantly improve the prediction of the generalization error for dropout
networks (Section 5.2). Finally, we consider other noises that can lead to either weight expansion
or weight contraction, and study their respective impact on the generalization performance (Sec-
tion 5.3). All empirical results are both significant and persistent across the models and data sets
we work with, and support either hypothesis (1) or hypothesis (2) or both. In our experiments, we
consider VGG-like models (Simonyan & Zisserman, 2014) and AlexNet-like models (Krizhevsky
et al., 2012) for CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), ImageNet-32 (Deng et al., 2009),
and SVHN (Netzer et al., 2011). For sampling method, we default to the settings of = 0.05 for
CIFAR-10/SVHN, and = 0.1 for CIFAR-100/ImageNet-32.
5.1 Measuring weight volume
0.06
0
0.08
0.04 4
CIFAR-IO Sampling
CIFAR-IO Laplace
CIFAR-100 Sampling
CIFAR-100 Laplace
CIFAR-IO Sampling
CIFAR-100 Sampling
0.15	0.3
Dropout Rate
0.45
Figure 3: Weight volume w.r.t. dropout rate.
First experiment studies the correlation be-
tween weight volume and the application of
dropout during training. Weight volume is hard
to measure exactly, so we use two different,
yet independent, estimation methods—Laplace
approximation and the sampling method as in
Section 4—to improve our confidence in the re-
sults and robustly test our hypotheses.
Table 1 presents the estimated weight volume
and the generalization gap on several VGG
networks on CIFAR-10/100. We see that
dropout leads, across all models, to a persistent
and significant increase of the weight volume
(hypothesis (2)). At the same time, the generalization error is reduced (hypothesis (1)). We default
to the setting of weight decay with 0 in the experiments of Table 1, Figure 3, and also present similar
results with weight decay 0.0005 (and more results on ImageNet-32) in Appendix L.
After confirming the qualitative correlation between weight volume and dropout, we consider the
quantitative effect of the dropout rate on the weight volume. As Figure 3 shows, for the 4 VGG16
models with dropout rate 0.0, 0.15, 0.3, 0.45, respectively, we see a clear trend on CIFAR-10/100
that the weight volume increases with the dropout rate (hypothesis (2)). The generalization gap is
also monotonically reduced (CIFAR-10: 0.86, 0.51, 0.30, 0.16; CIFAR-100: 3.75, 2.14, 1.07, 0.39)
(hypothesis (1)).
In summary, these experiments provide evidence to the correlation between dropout, weight expan-
sion, and generalization. This raises two follow-up questions: (1) can the weight volume serve as
a proxy for the generalization gap? and (2) can techniques other than dropout exhibit a similar be-
havior? Answers to these questions can shed light on the nature of the correlation between these
concepts.
5.2	Performance of the New Complexity Measure
A complexity measure can predict generalization gap without resorting to a test set. Inspired by
the experiments in Jiang et al. (2020b), we have trained 288*3=864 dropout networks (with dif-
7
Under review as a conference paper at ICLR 2022
Table 2: Mutual Information of complexity measures on CIFAR-10.
Complexity Measure	VGG				AlexNet			
	Dropout	GG(∣ω∣=2)	GG(∣ω∣=1)	GG(∣ω∣=0)	Dropout	GG(∣ω∣=2)	GG(∣ω∣=1)	GG(∣ω∣=0)
Frob Distance	0.0233	0.0243	0.0127	-0.0039-	0.0644	0.0289	0.0159	0.0094
Spectral Norm	0.0231	0.0274	0.0139	0.0001	0.1046	0.0837	0.0832	0.0786
Parameter Norm	0.0535	0.0252	0.0122	0.0008	0.0567	0.0814	0.0793	0.0744
Path Norm	0.0697	0.0129	0.0053	0.0027	0.1530	0.0425	0.0307	0.0177
Sharpness α0	0.1605	0.0127	0.0053	0.0001	0.1583	0.0877	0.0639	0.0418
PAC-Sharpness	0.0797	0.0200	0.0109	0.0023	0.1277	0.0552	0.0324	0.0097
PAC-S(Laplace)	0.5697	0.0837	0.0623	0.0446	0.7325	0.1248	0.1123	0.1052
PAC-S(Sampling)	0.6606	0.0857	0.0623	0.0433	0.5917	0.1014	0.0921	0.0847
Figure 4: Disentanglement noise vs. dropout. We have trained 2 VGG16 networks (Left) and
2 AlexNet (Right) on ImageNet-32 and CIFAR-100 respectively, with normal training, dropout
training, disentanglement noise training (volume expansion) and stochastic noise training (volume
contraction). For each data set, we present their test losses in different plots. More experiments on
the CIFAR-10, SVHN are given in Appendix N.
ferent rates: 0, 0.15, 0.3) on CIFAR-10 for VGG-like and AlexNet-like structures and CIFAR-100
for VGG-like structures to compare our new complexity measure—which incorporates the weight
volume—with a set of existing complexity measures, including PAC-Sharpness, which was shown
in Jiang et al. (2020b) to perform better overall than many others. Following Jiang et al. (2020b);
Dziugaite et al. (2020a), we use mutual information to measure the correlation between various
complexity measures, and dropout and generalization gap (GG, defined as test loss - training loss).
The details of experimental settings (e.g., GG(∣ω∣=2)) and mutual information are given in AP-
pendix M.1.
We compare our new complexity measures (Equation (5), with Laplace approximation and sam-
pling method respectively) with some existing ones, including Frobenius (Frob) Distance, Param-
eter Norm (Dziugaite & Roy, 2017; Neyshabur et al., 2018b; Nagarajan & Kolter, 2019), Spectral
Norm (Yoshida & Miyato, 2017), Path Norm (Neyshabur et al., 2015a;b), Sharpness α0, and PAC-
Sharpness (Keskar et al., 2017; Neyshabur et al., 2017; Pitas et al., 2017). Among them, PAC-
Sharpness is suggested in Jiang et al. (2020b) as the the most promising one in their experiments.
Table 2 presents the mutual information as described above on CIFAR-10 for VGG-like models and
AlexNet-like models. Details of the above complexity measures are given in Appendix M.2. More
empirical results on CIFAR-100 are given in Appendix M.3. We can see that, for most cases concern-
ing the generalization error, i.e., ∣ω | = 0,1, 2 (fix 0, 1, 2 hyper-parameters respectively, and compute
corresponding expected mutual information over sub-spaces, details are shown in Appendix M.1),
our new measures, PAC-S(Laplace) and PAC-S(Sampling), achieve the highest mutual information
among all complexity measures. On AlexNet, the mutual information of other complexity mea-
sures increase, but our new measures still perform best. Generally, the mutual information values of
GG for our measures are significantly higher than those for others (which supports hypothesis (1)),
because our complexity measures can predict generalization-improvement from weight expansion,
which is mainly caused by dropout in our network configurations. Meanwhile, hypothesis (2) is
also empirically verified as our complexity measures are most closely correlated with dropout.
5.3	Disentanglement Noise other than Dropout
The experiments in Sections 5.1 and 5.2 provide support to the hypotheses (1)&(2) that dropout,
weight expansion, and generalization are correlated, with an indication that weight expansion is
the proxy to the connection between dropout and the generalization error. Our final experiment is
designed to test this observation by considering the replacement of dropout.
We consider the disentanglement noise as presented in Algorithm 1, and train a set of VGG16
networks and AlexNet on CIFAR-10, CIFAR-100, Imagenet-32 and SVHN, with normal training,
8
Under review as a conference paper at ICLR 2022
dropout training, disentanglement noise training (volume expansion), and stochastic noise training
(volume contraction), respectively. As stochastic noises on weights may lead to either weight vol-
ume contraction or leave it unchanged, we collect the contraction cases for our experiments. Figure 4
shows that volume expansion improves generalization performance, similar to dropout. Likewise,
volume contraction leads to a worse generalization performance. This relation, which further sup-
ports hypothesis (1), is persistent across the examined networks and data sets. This suggests that
the weight expansion, instead of dropout, can serve as a key indicator of good generalization per-
formance, while dropout provides one method to implement volume expansion. Disentanglement
noises may be a valid replacements for dropout.
6	Related Work
From the perspectives of optimizing the training process, related work includes (Baldi & Sadowski,
2013; Wager et al., 2013; Kingma et al., 2015; Helmbold & Long, 2015; 2017; Gal & Ghahra-
mani, 2016; Nalisnick et al., 2019). For example, Wager et al. (2013) considers dropout training as
adaptive regularization; Gal & Ghahramani (2016) interprets dropout with a spike-and-slab distribu-
tion by variational approximation; and Kingma et al. (2015) re-parameterizes noise on the weights
as uncertainty in the hidden units, and proposes variational dropout, a generalization of Gaussian
dropout.
Recent research has started to provide a narrative understanding of dropout’s performance (Zhang
et al., 2021), and providing variational dropout (Wang & Manning, 2013; Kingma et al., 2015; Gal &
Ghahramani, 2016; Gal et al., 2017; Achille & Soatto, 2018; Fan et al., 2020; Lee et al., 2020; Pham
& Le, 2021; Fan et al., 2021). And Cavazza et al. (2018); Mianjy et al. (2018); Mianjy & Arora
(2019); Arora et al. (2019); Mianjy & Arora (2020) provide a theoretical analysis to understand
dropout from the perspective of regularizing the inductive bias. Different from these, we consider
explaining dropout through the rigorously defined concept of weight expansion.
Other than dropout, there are works studying how other factors affect the generalization of neural
networks (Neyshabur et al., 2018a; Chatterji et al., 2020; Dziugaite et al., 2020b; Sen et al., 2020;
Zitong et al., 2020; Hrayr et al., 2020; Mahdi et al., 2020; Menon et al., 2021; Montero et al., 2021),
and the relation between gradient noises and generalization (Keskar et al., 2017; Keskar & Socher,
2017; Jastrzebski et al., 2017; Smith & Le, 2018; Xing et al., 2018; Chaudhari & Soatto, 2018;
Jastrzebski et al., 2019; Li et al., 2019; Zhu et al., 2019; Jin et al., 2020; Wen et al., 2020) was studied.
There was also a NeurIPS 2020 Competition on Predicting Generalization in Deep Learning (Jiang
et al., 2020a; Lassance et al., 2020; Natekar & Sharma, 2020).
Since the invention of the PAC-Bayesian framework (McAllester, 1999), there have been a num-
ber of canonical works developed in the past decades, including the parametrization of the PAC-
Bayesian bound with Gaussian distribution (Seeger, 2002; Langford & Caruana, 2002; Langford
& Shawe-Taylor, 2003), the early works about generalization error bounds for learning problems
(Maurer, 2004; Germain et al., 2009; Welling & Teh, 2011; Parrado-Hernandez et al., 2012) and
the recent works about PAC-Bayes for machine learning models (Dwork et al., 2015a;b; Blundell
et al., 2015; Alquier et al., 2016; Thiemann et al., 2017; Dziugaite & Roy, 2018; Rivasplata et al.,
2019; Letarte et al., 2019; Rivasplata et al., 2020; Perez-Ortiz et al., 2021b; HaddoUche et al., 2021;
Perez-Ortiz et al., 2021a). Specifically, Langford & Shawe-Taylor (2003); Germain et al. (2009);
Parrado-Hernandez et al. (2012) demonstrate that one can obtain tight PAC-Bayesian generalization
bounds through multiplying the weight vector norm with a constant, this is different with the idea
of expanding weight volume, as weight volume is a statistical property of weight matrices treated as
random variables, while weight vector norm treats weights as deterministic variables (weight vector
norm can be seen as the first term in Equation (5), while weight volume is the second term). It is
interesting that Alquier et al. (2016) also optimizes PAC-Bayesian bound through a Gaussian pos-
terior with a full covariance matrix. We use a similar assumption (correlated Gaussian posterior)
but the main differences are: we develop it under another PAC-Bayesian branch (Dziugaite & Roy,
2017; Neyshabur et al., 2017; 2018a) and apply it on DNNs with connection to dropout noise.
7	Conclusion
The paper introduces ‘weight expansion’, an intriguing phenomenon that appears whenever dropout
is applied during training. We have provided both theoretical and empirical arguments, which show
that weight expansion can be one of the key indicators of the reduced generalization error, and that
dropout is effective because it is (an inexpensive) method to obtain such weight expansion.
9
Under review as a conference paper at ICLR 2022
8	Ethics Statement and Reproducibility S tatement
Ethics Statement. In this work, we present theoretical and empirical results to justify two hypothe-
ses: (1) weight expansion helps reduce the generalization error and (2) dropout increases the
weight volume. All the empirical results are obtained on publicly available benchmark data sets
(e.g., CIFAR-10/100, SVHN, ImageNet-32). Thus, our work does not involve any ethics issues.
Reproducibility Statement. In Section 3, to theoretically support hypothesis (1) (through PAC-
Bayesian framework) and hypothesis (2) (through correlation of gradient updates and weights),
we have clearly explained (or cited) the assumptions (e.g., Gaussian distributed weights, arbitrarily
distributed gradient updates), and have made the complete proofs (or derivations, for Section 3.1,
Lemma 3.1 and Lemma 3.2) in Appendices D to F, and have discussed the relationship between
correlation of weights and weight volume in Appendix G (with a simple proof and two simulations).
As for the empirical results in Sections 5.1 to 5.3, experiments in Sections 5.1 and 5.2 verify both
hypothesis (1) and hypothesis (2), while Section 5.3 further supports hypothesis (1). We have
uploaded the corresponding scripts to the supplementary file. All these results are conducted and
validated on multiple data sets and multiple network architectures.
References
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. TPAMI, 2018.
Pierre Alquier, James Ridgway, and Nicolas Chopin. On the properties of variational approximations
of gibbs posteriors. JMLR, 2016.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. NeurIPS, 2019.
Pierre Baldi and Peter J Sadowski. Understanding dropout. NIPS, 2013.
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep networks? NeurIPS, 2018.
Roberto Battiti. First-and second-order methods for learning: between steepest descent and newton’s
method. Neural computation, 1992.
Christopher M Bishop. Pattern recognition and machine learning. 2006.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In ICML, 2015.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors
with subword information. TACL, 2017.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. ICML, 2017.
Jacopo Cavazza, Pietro Morerio, Benjamin Haeffele, Connor Lane, Vittorio Murino, and Rene Vidal.
Dropout as a low-rank regularizer for matrix factorization. AISTATS, 2018.
Niladri Chatterji, Behnam Neyshabur, and Hanie Sedghi. The intriguing role of module criticality
in the generalization of deep networks. ICLR, 2020.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. 2018 Information Theory and Applications Workshop
(ITA), 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth.
Generalization in adaptive data analysis and holdout reuse. NIPS, 2015a.
10
Under review as a conference paper at ICLR 2022
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon
Roth. Preserving statistical validity in adaptive data analysis. In STOC, 2015b.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. UAI, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential
privacy. NeurIPS, 2018.
Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero,
Linbo Wang, Ioannis Mitliagkas, and Daniel M Roy. In search of robust measures of generaliza-
tion. NeurIPS, 2020a.
Karolina Gintare Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero,
Linbo Wang, Ioannis Mitliagkas, and Daniel Roy. In search of robust measures of generalization.
NeurIPS, 2020b.
Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with
structured dropout. ICLR, 2020.
Xinjie Fan, Shujian Zhang, Korawat Tanwisuth, Xiaoning Qian, and Mingyuan Zhou. Contextual
dropout: An efficient sample-dependent dropout module. ICLR, 2021.
Farzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral nor-
malization. ICLR, 2018.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. ICML, 2016.
Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. NIPS, 2017.
Hongchang Gao, Jian Pei, and Heng Huang. Demystifying dropout. International Conference on
Machine Learning, 2019.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. Pac-bayesian learn-
ing of linear classifiers. In ICML, 2009.
Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe-Taylor. Pac-bayes un-
leashed: generalisation bounds with unbounded losses. Entropy, 2021.
David P Helmbold and Philip M Long. On the inductive bias of dropout. JMLR, 2015.
David P Helmbold and Philip M Long. Surprising properties of dropout in deep networks. JMLR,
2017.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Harutyunyan Hrayr, Reing Kyle, Greg Steeg Ver, and Galstyan Aram. Improving generalization by
controlling label-noise information in neural network weights. ICML, 2020.
Stanisaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. ICANN, 2017.
Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos
Storkey. On the relation between the sharpest directions of dnn loss and the sgd step length.
ICLR, 2019.
Yiding Jiang, Pierre Foret, Scott Yak, M. Daniel Roy, Hossein Mobahi, Karolina Gintare Dziu-
gaite, Samy Bengio, Suriya Gunasekar, Isabelle Guyon, and Behnam Neyshabur. Neurips 2020
competition: Predicting generalization in deep learning. 2020a.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. ICLR, 2020b.
11
Under review as a conference paper at ICLR 2022
Gaojie Jin, Xinping Yi, Liang Zhang, Lijun Zhang, Sven Schewe, and Xiaowei Huang. How does
weight correlation affect the generalisation ability of deep neural networks. NeurIPS, 2020.
Armand Joulin, EdoUard Grave, Piotr BojanoWski, Matthijs Douze, Herve Jegou, and Tomas
Mikolov. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651,
2016.
Armand Joulin, Edouard Grave, Piotr BojanoWski, and Tomas Mikolov. Bag of tricks for efficient
text classification. In EACL, 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by sWitching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR,
2017.
Yoon Kim. Convolutional neural netWorks for sentence classification. In EMNLP, 2014.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
zation trick. NIPS, 2015.
S Kocherlakota and K Kocherlakota. Generalized variance. Encyclopedia of Statistical Sciences,
2004.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification With deep convo-
lutional neural netWorks. NIPS, 2012.
John Langford and Rich Caruana. (not) bounding the true error. NIPS, 2002.
John Langford and John ShaWe-Taylor. Pac-bayes & margins. NIPS, 2003.
Carlos Lassance, Louis Bethune, Myriam Bontonou, Mounia Hamidouche, and Vincent Gripon.
Ranking deep learning generalization using label variation in latent geometry graphs. arXiv
preprint arXiv:2011.12737, 2020.
Beom Hae Lee, TaeWook Nam, Eunho Yang, and Ju Sung HWang. Meta dropout: Learning to
perturb latent features for generalization. ICLR, 2020.
Benjamin Lengerich, Eric P Xing, and Rich Caruana. On dropout, overfitting, and interaction effects
in deep neural netWorks. arXiv preprint arXiv:2007.00823, 2020.
Gael Letarte, Pascal Germain, Benjamin Guedj, and Francois Laviolette. Dichotomize and general-
ize: Pac-bayesian binary activated deep neural netWorks. NeurIPS, 2019.
Yuanzhi Li, Colin Wei, and Tengyu Ma. ToWards explaining the regularization effect of initial large
learning rate in training neural netWorks. NeurIPS, 2019.
Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural netWork for text classification With
multi-task learning. arXiv preprint arXiv:1605.05101, 2016.
Haghifam Mahdi, Negrea Jeffrey, Khisti Ashish, Daniel Roy M., and Gintare Dziugaite Karolina.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. NeurIPS, 2020.
James Martens and Roger Grosse. Optimizing neural netWorks With kronecker-factored approximate
curvature. ICML, 2015.
Andreas Maurer. A note on the pac bayesian theorem. arXiv preprint cs/0411099, 2004.
David A McAllester. Pac-bayesian model averaging. In COLT, 1999.
12
Under review as a conference paper at ICLR 2022
Krishna Aditya Menon, Singh Ankit Rawat, and Sanjiv Kumar. Overparameterisation and worst-
case generalisation: friend or foe? ICLR, 2021.
Poorya Mianjy and Raman Arora. On dropout and nuclear norm regularization. ICML, 2019.
Poorya Mianjy and Raman Arora. On convergence and generalization of dropout training. NeurIPS,
2020.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. ICML, 2018.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2015.
Llera Milton Montero, JH Casimir Ludwig, Ponte Rui Costa, Gaurav Malhotra, and Jeffrey Bowers.
The role of disentanglement in generalisation. ICLR, 2021.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672, 2019.
Eric T. Nalisnick, Jose MigUel Hernandez-Lobato, and Padhraic Smyth. Dropout as a structured
shrinkage prior. ICML, 2019.
Parth Natekar and Manik Sharma. Representation based complexity measures for predicting gener-
alization in deep learning. arXiv preprint arXiv:2012.02775, 2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimiza-
tion in deep neural networks. NIPS, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. COLT, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring general-
ization in deep learning. NIPS, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. ICLR, 2018a.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. To-
wards understanding the role of over-parametrization in generalization of neural networks. arXiv
preprint arXiv:1805.12076, 2018b.
Emilio Parrado-Hernandez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. Pac-bayes
bounds with data dependent priors. JMLR, 2012.
Maria Perez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John
Shawe-Taylor, Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic
neural networks. arXiv preprint arXiv:2109.10304, 2021a.
Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certifi-
cates for neural networks. JMLR, 2021b.
Hieu Pham and Quoc V Le. Autodropout: Learning dropout patterns to regularize deep networks.
AAAI, 2021.
Konstantinos Pitas, Mike Davies, and Pierre Vandergheynst. Pac-bayesian margin bounds for con-
volutional neural networks. arXiv preprint arXiv:1801.00171, 2017.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural
networks. ICLR, 2018.
Omar Rivasplata, Vikram M Tankasali, and Csaba Szepesvari. Pac-bayes with backprop. arXiv
preprint arXiv:1908.07380, 2019.
13
Under review as a conference paper at ICLR 2022
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, and John ShaWe-Taylor. Pac-bayes analysis
beyond the usual bounds. In NeurIPS, 2020.
AndreW M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural netWorks. arXiv preprint arXiv:1312.6120, 2013.
Matthias Seeger. Pac-bayesian generalisation error bounds for gaussian process classification. Jour-
nal of machine learning research, 3(Oct):233-269, 2002.
Wu Sen, Hongyang Zhang R., Valiant Gregory, and Re Christopher. On the generalization effects of
linear transformations in data augmentation. ICML, 2020.
Adrian J Shepherd. Second-order methods for neural networks: Fast and reliable training methods
for multi-layer perceptrons. Springer Science & Business Media, 2012.
Karen Simonyan and AndreW Zisserman. Very deep convolutional netWorks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. ICLR, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple Way to prevent neural netWorks from overfitting. JMLR, 2014.
Maosong Sun, Jingyang Li, Zhipeng Guo, Z Yu, Y Zheng, X Si, and Z Liu. Thuctc: an efficient
chinese text classifier. GitHub Repository, 2016.
Niklas Thiemann, Christian Igel, Olivier Wintenberger, and Yevgeny Seldin. A strongly quasiconvex
pac-bayesian bound. In ALT, 2017.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. NIPS,
2013.
Sida Wang and Christopher Manning. Fast dropout training. ICML, 2013.
Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of
dropout. ICML, 2020.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
ICML, 2011.
Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. Interplay
betWeen optimization and generalization of stochastic gradient descent With covariance noise.
AISTATS, 2020.
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A Walk With sgd. arXiv preprint
arXiv:1802.08770, 2018.
Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.
Hao Zhang, Sen Li, YinChao Ma, Mingjie Li, Yichen Xie, and Quanshi Zhang. ToWards under-
standing and improving dropout in game theory. ICLR, 2021.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and JinWen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. ICML,
2019.
Yang Zitong, Yu Yaodong, You Chong, Steinhardt Jacob, and Ma Yi. Rethinking bias-variance
trade-off for generalization of neural netWorks. ICML, 2020.
14
Under review as a conference paper at ICLR 2022
APPENDIX
A Details of Figure 1	16
B Weight Volume and Weight Orthogonality	16
C Supplementary for Figure 2	16
D Detailed Derivation of PAC-Bayes	17
E Proof of Lemma 3.1	18
F Proof of Lemma 3.2	19
G Weight Volume and Correlation	19
H Other Regularizers with Weight Volume	20
I Supplementary for Sampling Method	20
J Estimate Weight Volume	20
K Embedding Dropout Noise ηdo to Covariance Matrix Σldo	21
L Supplementary for Section 5.1	21
M Details of Experiments in Section 5.2	22
N Supplementary for Experiment 5.3	24
O Experiments on Text Classification	24
P	Future Work	24
Q	Rebuttal to Reviewer iaGH	26
R	Rebuttal to Reviewer wW51	29
S	Rebuttal to Reviewer w6em	33
T	Rebuttal to Reviewer nB1c	34
15
Under review as a conference paper at ICLR 2022
鬻翦场®短凄落器意力-
Figure 5: We have trained two VGG16 networks (with/without dropout) on CIFAR-10. (b) shows the
changes in weight volume along with epochs. There are four sampling points (①②③④) to track the
absolute normalized covariance matrix of normal gradient updates (a) and dropout gradient updates
(c).
Table 3: Network Structure for Figure 1.
确超¾透矍A潮噩算B-
岐建盛甥舞辑魏蕊哪门一
馨一尊 ⅛6⅛程一A⅛j图
⅛¾⅛⅛⅛湿忌⅛E1⅛-
5i去魏鬣≡遹舞逢整
藤就猛颗噬盘密刹强雷一
■募缪警⅛'翻窜鸳鞫叩
才区诚⅛制岑M第船学
r^s⅛¾^⅛gIiic
嫩海逋蠹覆嫩蹩海第总ɔart
g⅛iL)p
1 : 39
'.'''.C
S -⅛⅛s---t I⅛s⅛⅛⅛≡l
瓣翻≡«牖橇贼蕊■理.
3%皤落海舞富途落-
⅛H鑫梓段濯济筏畿一
蕊就遨檄漏繁啄戳嬲蟒一
希津奥随蹩舞微睡蠹徽一
Normal CNN	ConV-64-(3,3)	ConV-128-(3,3)	MaXPool-(7,7)	Dense-128		Dense-2	Softmax
DroPoUt CNN	ConV-64-(3,3T	ConV-128-(3,3T	MaXPool-(7,TT	Dense-128	Dropout-0.3	Dense-2	Softmax
A Details of Figure 1
As Table 3 shows, we use the above two networks to train MNIST with ReLU activation function,
0.01 learning rate, 128 minibatch size, stochastic gradient descent (SGD), 150 epochs, with and
without dropout. Normal CNN and dropout CNN contain multiple convolutional layers with 64 and
128 channels, following with 128 and 2 hidden units. We visualize the features of Dense-2 layer in
Figure 1.
B	Weight Volume and Weight Orthogonality
For clarification, weight volume is not the orthogonality of weight matrices, but the “normalized
generalized variance” of weight matrices. It is a statistical property of weight matrices when treated
as random variables, which is different from the orthogonality technique in Saxe et al. (2013);
Mishkin & Matas (2015); Bansal et al. (2018) that treat weight matrices as deterministic variables.
Specifically, by treating the elements in weight matrices as random variables (to fit the PAC-Bayes
framework), the weight volume is defined as the determinant of the “covariance matrix” of weight
matrices (see Definition 3.1). The optimization of the determinant of the covariance will not neces-
sarily lead to orthogonality of weight matrices. It appears that weight volume considers the statistical
correlation between any two weights (treated as random variables), whereas weight orthogonality
considers the linear space correlation between any two columns in weight matrices.
E.g., multivariate random variable W is made up of two terms, say W = vec(W) + UW , where
vec(W) is the vectorization of weight matrix and UW is the 0 mean multivariate random variable.
Weight volume relies on the covariance matrix of UW , while weight orthogonality depends on W .
C Supplementary for Figure 2
Figure 5 also shows that the correlation among gradient updates in the normal network (Figure 5a) is
larger than the correlation in dropout network (Figure 5c) and large correlation of gradient updates
can lead to small weight volume (large correlation of weights), while small correlation of gradient
updates can lead to large weight volume (small correlation of weights).
16
Under review as a conference paper at ICLR 2022
D Detailed Derivation of PAC-Bayes
McAllester (1999); Dziugaite & Roy (2017) consider a training set S with m ∈ N samples drawn
from a distribution D. Given the prior and posterior distributions P and Q on the weights W
respectively, for any δ > 0, with probability 1 - δ over the draw of training set, we have
Ew~q[Ld(fw)] ≤ Ew~q[Ls(fw)] +
，Dkl(Q||P) + ln 等
2∣	2(m - 1)
(12)
Equation (4): Let P = N(μp, Σp) and Q = N(μQ, ∑q), We can get
DKL(N(μQ, ∑q)∣∣N(μp, ∑p)) = /[ln(Q(x)) - ln(P(x))]Q(x)dx
=/[2 ln det ∑P - 2(X - μQ)Tςq1(x - μQ) + 2(X - μP)Iς-1(x - μP)]Q(X)dx
=2 ln dαf ∑P - 2tr{E[(x - μQ)(X - "q)1^-) + 2tr{E[(x - "p)tς-I(X - μp)]} (13)
2	det ΣQ	2	2
1	det ΣP	1	1	1	1	1
=2 ln det ∑Q - 2tr(Ik) + 2(μQ - Mp)tςp (μQ - μP) + 2tr(，P ςQ)
=9 [trN-咆Q) + (μQ - μP)|ς-I(MQ - μP) - k + ln Haf \P| i .
2	det(ΣQ)
Equation (5): Neyshabur et al. (2017); Jiang et al. (2020b) instantiate the prior P to be a vec(W0)
mean, σ2 variance Gaussian distribution and the posterior Q to be a vec(W F) mean spherical Gaus-
sian With variance σ2 in each direction. Relax their assumption, We assume Q is a non-spherical
Gaussian (the off-diagonal correlations for same layer are not 0, those for different layers are
0), then We can get
DKL(Q||P)
||WF - W0IIF	11 det(∑P)
2σ2	+2 det(∑Q)
X ||WlF - WOIIF , 1 1 Y det3l,P)
V ―2σ2 — + 21n ll det‰)
1	X ( ||WlF - Wl0||F , 1 detNl,P)、
2	N―σ2 —+1ndet(∑lQ)J
1	XIIWlF-Wl0IIF .1τι Qi[∑l,P]ii)
2 4—W — + 1nEQT)
1	XZ IIWlF - Wl0IIF +1τl Qi[∑l,Q]ii ʌ
2	V —W — + 1nEQT)
1X ZIIWF	- W0IIF	+ 1τι 1	)
2 /(	σ∙2	+ 1n	Vol(Wl) J.
(14)
17
Under review as a conference paper at ICLR 2022
E Proof of Lemma 3.1
Proof E.1 Let ∆ j, ∆%j be the gradient updatesfor Wj, Wij ofsome layer in a normal network,
i = i0, ∆j and ∆dj be the gradient updatesfor Wj Wijin the same setting dropout network. Let
vec(∆)〜M∆(μ∆, ∑∆) andvec(∆do) = (1 +10ηdo)Θvec(∆do0), vec(∆do0)〜M∆(μ∆, ∑∆).
Note that, as we aim to study the impact of ηdo on the correlation among gradient updates, we
assume vec(∆) and vec(∆do0) obey the same arbitrary distribution M∆. As dropout noise and
gradient update are independent, dropout noises for different neurons (kernels) are independent, we
have
Cov(∆j ∆j) = E∆do [(∆ j - μ∆do)(∆dj - 〃△/)]
∆do
=(I - q)2E∆do 0 [	- μ∆ij )(1 -q - μ∆ij,)] +(1 - q)q
∆idoj
E∆do0 [(1 - q - μ∆ij ∕)(-μ∆j)] +(1 - q)qE∆do 0 [	- μ∆j)(一μ∆ij，)]
+ q2 [(-μ∆ijo )(-μ∆j)]
(15)
—
μ∆ij)(∆?j - μ∆ij0
=E∆ [(△”' - MAj)(4ij0 - μ∆i∕j∕
= Cov(∆j, ∆j),
and
Var(∆阴=E∆do [(∆j - μ∆do 力
(I- q)E∆do0 [
-μ∆j)2] + q(-μ∆j)2
1 - q
E∆ [(△j-μ∆j)2]
+ qμ‰
1 - q
(16)
…9.
+吟
1 - q
1 一 q
Var(∆j)
1 一 q
qμ∆
2
+ q.∆j
1 - q
Similarly, Var(∆dj) = Var(△；"“)
1-q
.Thus, we have
+
Rj
Cov(∆idjo, ∆idoj0)
ρ∆do,∆do, I = -/	==
j ij	,Var(∆do)Var(∆j)
__∣Cov(∆j, ∆ij) ∣
∕∖ Var(∆j) i q&\( VarQij ) ∣ q吟ij
V (ɜ-	+ ɪ^	ɪ-g	+ 1-q
< (1-q)∣Cov(∆j, ∆j)∣
一 ∙√Var(∆j )Var(∆ij)
=(1 - q)∣P∆j,∆ij'∣
≤ IPAjAij0I ∙
(17)
18
Under review as a conference paper at ICLR 2022
F Proof of Lemma 3.2
Proof F.1 As dropout noise and weight are independent, and W 〜NW, we have
Cov(Wij, δ^0 ) = Ew,∆do [(Wij- Nwij)Zj - μ∆do 0)]
∆id0oj00
=(I - q)Ew,∆do0 [(Wij - μWij )(1 — 3q - μ∆i0j0 )] + qEw [(wij - μWij )(-μ∆i0j0 )]
=Ew,∆do0 [(Wij- Mwij)0i0j0 - μ∆i0j0)]
=Ew,∆ [(wij - μWij)(∆i0j0 - μ∆i0j0 )]
(18)
= Cov(Wij, ∆i0j0),
Cov(Wij, ∆idjo ),Cov(Wi0j0, ∆id0oj0) and Cov(Wi0j0, ∆idjo ) are similar. From Equations (15) and (16),
we can get
ρwij+∆idjo,wi0j0+∆id0oj0
=-Cv(wj+∆jLwj±∆⅛)-
,Var(wij + △1)Var(Wi 0j0 + ∆d0j0)
_	ICoV(Wij, W") + CoV(∆jo, ∆dj0) + CoV(Wij, ∆dj0) + CoV(Wi0j0, ∆djo)∣
《(Var(Wij) + Var(△$) + 2Cov(Wj, △*)) (Var(Wi0j0) + Var(∆dj0) + 2Cov(Wi0j0, ∆d0j0))
_	ICoV(Wij, Wi0j0) + Cov(∆ij, ∆i0j0) + CoV(Wij, ∆i0j0) + CoV(Wi0j0, ∆ij)∣
个(Var(Wij) + Var(∆djo) + 2Cov(Wj, ∆j)) (Var(Wij`) + Var(∆d0j0) + 2Cov(Wi0j0, △”))
_ ICoV(Wij, Wi0j0) + Cov(∆ij, ∆i0j0) + Cov(Wij, ∆i0j0) + CoV(Wi0j0, ∆ij)1
∖l(Var(Wij) + Var-qij) + ⅞j + 2CoV(Wij, △))
1
J (Var(Wi0j0) + '弋。j + ¾ji + 2CoV(Wi0j0, ʌi,j,))
ICoV(Wij + △ij, Wi0j0 + △i0 j0) i
r (Var(Wij + △+) + qV：—；j)(Var(Wi0j0 + Zj ) + qVa：-qi0j0))
ICoV(Wij + ʌij, Wi0j0 + AiOjO)1
VZVar(Wij + △ )Var(Wi0j0 + Aij)
Iρwij+∆ij,wi0j0+∆i0j0 I.
G Weight Volume and Correlation
Since the definition of weight volume in Definition 3.1, the weight volume equals the determinant
of the weight correlation matrix.
It is difficult to theoretically argue an ‘exact’ relationship (e.g., perfect positive or negative) between
weight volume and correlation of weights, but they have a rough negative relationship. Take a simple
example, let n-dimensional weight correlation matrix C have the same correlation ρ, it is easy to
infer that (absolute) correlation is negative related with weight volume, as
det(C) = (1 - ρ)n-1(l + (n - 1)ρ),	(19)
where d学。)=(n - n2)ρ(1 - ρ)n-2 and n ≥ 2. Also, We demonstrate the relationship between
weight volume and average (absolute) correlation with 10000 samples for a 3-dimensional correla-
tion matrix (Figure 6a) and a 4-dimensional correlation matrix (Figure 6b) respectively. Although
weight volume is not monotonically increasing as (absolute) correlation decreases, we can find a
rough negative relationship.
19
Under review as a conference paper at ICLR 2022
Figure 6: (a) We sample 10000 3-dimensional correlation matrices and show their determinant and
average absolute correlation. (b) We sample 10000 4-dimensional correlation matrices and show
their determinant and average absolute correlation. (c) We train four small NNs (64-32-16-10 with
normal, dropout, weight decay, and batch normalization, respectively) on MNIST. The figure shows
the changes in weight volume along with epochs for these four networks.
H Other Regularizers with Weight Volume
As dropout can generate independent noises but other regularizers, like weight decay and batch
normalization, cannot. Thus, dropout can introduce randomness into the network, while weight
decay and batch normalization cannot (these two regularizers improve generalization not through
weight volume, through other keys). We have described in detail why this dropout noise can expand
weight volume and why weight volume is correlated with generalization, theoretically (Section 3)
and empirically (Section 5, Figures 2 and 5).
Also, in Figure 6c, we can see that dropout expands weight volume compared with normal network,
while weight decay and batch normalization have hardly any impact on weight volume.
I Supplementary for Sampling Method
Sampling Method: As mentioned in Section 4, we use sharpness-like method (Keskar et al., 2017)
to get a set of weight samples (W + U). To get the samples from the posteriori distribution steadily
and fastly, we train the convergent network with learning rate 0.0001, noise U and 50 epochs, then
collect corresponding 50 samples. As the samples are stabilized at training loss and validation
loss but with different weights, we can treat them as the samples from same posteriori distribution.
Details are given in the code.
J	Estimate Weight Volume
The approximation ofEs[Hl] is two-fold: (1) Only the diagonal blocks of the weight matrix are cap-
tured, so that the correlation between blocks is decoupled; (2) Assume Al-1 and Hl are independent
(Botev et al., 2017; Ritter et al., 2018). Then, we have
Es [Hl ] = Es[Ai-ι ③ Hl] ≈ Es [A1-1 ]③ Es [Hl ].	(20)
Finally, Σl can be approximated as follows:
∑ι = (Es[Hι ])-1 ≈ (Es[Ai-ι ])-1 ③(Es [Hi ])-1.	(21)
Therefore, by separately computing the matrix inverse of Es [Al-1] and Es [Hl], whose sizes are
significantly smaller, we can estimate Σl ∈ RNl-1Nl ×Nl-1Nl efficiently.
Further, the weight volume can be approximated by using
det(∑ι) ≈ det ((Es[Ai-ι])-1 )Nl ∙ det ((EsH])-1 )Nl-1	(22)
and
Y[∑i]ii ≈ (Y [(Es[Ai-1 ])-1]cc)Nl ∙ (Y [(Es[Hl])-1]dd)Nj.	(23)
20
Under review as a conference paper at ICLR 2022
Table 4: Weight Volume on VGG Networks (ImageNet-32, weight decay = 0).
Method	VGG11	VGG11(dropout)	VGG16	VGG16(drοpοut)
Sampling	0.015±0.01	0.089±0.01	0.016±0.01	0.097±0.01
Laplace	0.0198	0.0985	0.0194	0.0942
Generalization Gap
7.0388
2.0501
6.3922
1.2430
VGG19
0.017±0.01
0.0214
7.5187
VGG19(dropout)
0.086±0.01
0.0977
0.7910
Table 5:	VGG11 Structure
Conv-64 MaxPooling Conv-128 MaxPooling Conv-256 DroPoUt(0.3) Conv-256 MaxPooling Conv-512
Dropout(0.3)
Conv-512 Maxpooling	Conv-512 DroPoUt(0.3)	Conv-512 Maxpooling	DroPoUt(0.1)	Dense-512	DroPoUt(0.1) DenSe-(num ClaSSeS)
Table 6:	VGG16 StrUCtUre
Conv-64	DroPoUt(0.3)	Conv-64	Maxpooling	Conv-128	Dropout(0.3)	Conv-128	Maxpooling	Conv-256	Dropout(0.3)
Conv-256	Dropout(0.3)	Conv-256	Maxpooling	Conv-512	Dropout(0.3)	Conv-512	Dropout(0.3)	Conv-512	Maxpooling
Conv-512 Dropout(0.3) Conv-512 Dropout(0.3) Conv-512 Maxpooling Dropout(0.1) Dense-512 Dropout(0.1) Dense-(num classes)
Table 7:	VGG19 StruCture
Conv-64	Dropout(0.3)	Conv-64	Maxpooling	Conv-128	Dropout(0.3)	Conv-128	Maxpooling	Conv-256	Dropout(0.3)
Conv-256	Dropout(0.3)	Conv-256	Dropout(0.3)	Conv-256	Maxpooling	Conv-512	Dropout(0.3)	Conv-512	Dropout(0.3)
Conv-512	Dropout(0.3)	Conv-512	Maxpooling	Conv-512	Dropout(0.3)	Conv-512	Dropout(0.3)	Conv-512	Dropout(0.3)
Conv-512 Maxpooling Dropout(0.1) Dense-512 Dropout(0.1) Dense-(num classes)
Table 8: Weight Volume on VGG Networks (ImageNet-32, weight decay = 0).
Method	VGG11	VGG11(drοpοut)	VGG16	VGG16(dropout)
Sampling	0.015±0.01	0.089±0.01	0.016±0.01	0.097±0.01
Laplace	0.0198	0.0985	0.0194	0.0942
Train Loss	0.0093	TT571	0.0085	1.6577
Test Loss	7.0481	3.2072	6.4007	2.9007
Loss Gap	7.0388	2.0501	6.3922	1.2430
Top-1 Train Acc	0.9953	0.7088	0.9953	05949
Top-1 Test Acc	0.2738	0.3694	0.3116	0.3908
Top-1 Acc Gap	0.7215	0.3394	0.6837	0.2041
Top-5 Train Acc	0.9999	0.8023	0.9999	0.8248
Top-5 Test Acc	0.5014	0.6170	0.5502	0.6444
Top-5 Acc Gap	0.4985	0.1853	0.4497	0.1804
Table 9: Weight Volume on VGG Networks (ImageNet-32, weight decay = 0.0005).				
Method	VGG11	VGG11(drοpοut)	VGG16	VGG16(dropout)
Sampling	0.021±0.01	0.097±0.01	0.019±0.01	0.091±0.01
Laplace	0.0268	0.0877	0.0284	0.0891
Train Loss	0.7344	2.3641	0.6708	2.6208
Test Loss	4.8745	3.2244	4.6388	3.0396
Loss Gap	4.1410	0.8603	3.9680	0.4188
Top-1 Train Acc	0.9926	0.53T2	0.9932	0.4849
Top-1 Test Acc	0.3265	0.4033	0.3732	0.4266
Top-1 Acc Gap	0.6661	0.1279	0.6200	0.0583
Top-5 Train Acc	0.9999	0.7839	0.9999	0.7419
Top-5 Test Acc	0.5691	0.6531	0.6145	0.6777
Top-5 Acc Gap	0.4308	0.1308	0.3854	0.0642
Table 10: Weight volume on VGG networks (weight decay = 0).
	Method	VGG11	VGG11(drοpοut)	VGG16	VGG16(dropout)
	Sampling	0.06±0.02	0.13 ±0.02	0.05±0.02	0.12±0.02
	Laplace	0.0568	0.1523	0.0475	0.1397
	Train Loss	0.0083	0.0806	0.0203	0.3002
	Test Loss	0.8113	0.6021	0.8901	0.5998
	Loss Gap	0.8030	0.5215	0.8698	0.2996
	Train Acc	0.9973	0.9698	0.9933	0.8813
	Test Acc	0.8563	0.8645	0.8435	0.8401
	Acc Gap	0.1410	0.1053	0.1498	0.0412
	Sampling	0.07±0.02	0.14±0.02	0.06±0.02	0.14±0.02
	Laplace	0.0537	0.1578	0.0409	0.1620
	Train Loss	0.0187	0.2608	0.0706	1.0324
	Test Loss	3.1395	2.4244	3.8247	2.3038
	Loss Gap	3.1208	2.1635	3.7541	1.2714
	Train Acc	0.9943	0.9274	0.9912	0.6398
	Test Acc	0.5764	0.5911	0.4867	0.5188
	Acc Gap	0.4179	0.3363	0.5045	0.1210
K EMBEDDING DROPOUT NOISE ηdo TO COVARIANCE MATRIX Σldo
For Laplace approximation (Section 4), the standard dropout, as a noise term inaldo = (1+ηldo)al,
can be framed into the posterior estimation via Hessian matrices in Equation (9).
L S upplementary for Section 5.1
As Table 4 shows, on the ImageNet-32, dropout still leads to persistent, and significant, increase on
the weight volume, across all three models. At the same time, the generalization error is reduced.
21
Under review as a conference paper at ICLR 2022
Table 11: Weight volume on VGG networks (weight decay = 0.0005).
o'nviij - oo'nviij
Method	VGG11	VGG11(dropout)	VGG16	VGG16(dropout)
Sampling	0.05±0.02	0.13 ±0.02	0.05±0.02	0.13±0.02
Laplace	0.0512	0.1427	0.0433	0.1346
Train Loss	0.2166	0.2984	0.1739	0.2438
Test Loss	0.6368	0.5997	0.5346	0.4788
Loss Gap	0.4202	0.3013	0.3607	0.2350
Train Acc	0.9940	0.9801	0.9958	0.9878
Test Acc	0.9048	0.9118	0.9234	0.9340
Acc Gap	0.0892	0.0683	0.0724	0.0538
Sampling	0.05±0.02	0.15 ±0.02	0.06±0.02	0.15±0.02
Laplace	0.0496	0.1469	0.0413	0.1659
Train Loss	0.4389	0.7612	0.5403	0.8890
Test Loss	2.4366	2.3672	2.3149	2.1516
Loss Gap	1.9977	1.6060	1.7746	1.2626
Train Acc	0.9976	0.9654	0.9889	0.9T33
Test Acc	0.6511	0.6765	0.6980	0.6970
Acc Gap	0.3465	0.2889	0.2909	0.2163
In addition, we show the VGG network structures in Tables 5 to 7, and also demonstrate the de-
tails (including training loss, training accuracy, test loss, test accuracy) of the VGG experiments
on ImageNet-32 (Tables 8 and 9) and CIFAR-10/100 (Tables 10 and 11). All models for CIFAR-
10/100 are trained for 140 epochs using SGD with momentum 0.9, batch size 128, weight decay 0 or
5 × 10-4, and an initial learning rate of 0.1 that is divided by 2 after every 20 epochs. All models for
ImageNet-32 are trained for 100 epochs using SGD with momentum 0.9, batch size 1024, weight
decay 0 or 5 × 10-4, and an initial learning rate of 0.1 that is divided by 2 for each 20 epochs, and
learning rate of 0.0005 after 50th epoch.
All results in Tables 8 to 11 also suggest that dropout can expand the weight volume and narrow
generalization gap.
M Details of Experiments in Section 5.2
M.1 Experimental Settings for Section 5.2
We consider both VGG-like models, with dropout rate={0.0, 0.15, 0.30}, parameterized over
ΩvGG={batch size={64, 256}, initial learning rate={0.1, 0.03},1 depth={11, 16, 19}, activation
function={ReLU, tanh}, weight decay={0.0, 0.0005}, width={1.0, 0.5}2 }, and AlexNet-like mod-
els, parameterized over。人日,which has the same parameters except that depth={7, 9, 10}. There-
fore, either VGG or AlexNet has 2 * 2 * 3 * 2 * 2 * 2 * 3 = 288 configurations. Let Ψ be the set of
configurations.
Formally, we use a normalized mutual information to quantify the relation between random vari-
ables Vco and VPa with Pa ∈ Ω ∪ {GE}, where ‘co’ denotes complexity measure and ‘GE’
generalization error. For example, Vdepth is the random variable for network depth.
Given a set of models, we have their associated hyper-parameter or generalization gap
{pa(fθ,ψ)∣Ψ ∈ Ψ}, and their respective values of the complexity measure {co(fθ,ψ)∣Ψ ∈ Ψ}.
Let Vco(ψi, ψj) , sign(co(fΘ,ψi ) - co(fΘ,ψj )), VPa(ψi, ψj) , sign(Pa(fΘ,ψi ) - Pa(fΘ,ψj )).
We can compute their joint probability p(Vco, VPa) and marginal probabilities p(Vco) and p(VPa).
The mutual information between Vco and VPa can be computed as
I(Vco,VPa) = X X P(Vco，VPa)Iog PBV^^，I(Vco，VPa)
=⅛pT ,	(24)
where I(Vco, VPa) can be further normalized as I(Vco, VPa) to enable a fair comparison between
complexity measures, H(∙) is the marginal entropy. For the generalization gap, We consider not
only I(Vco, VGE), but also the mutual information with some of the hyper-parameters fixed, i.e.,
Eω⊂Ω(I(Vco, VGE∣ω)), for the cases of ∣ω∣ = 0,1,2. Note that, when ∣ω∣ = 0, it is the same as
I(yco, Vge). Intuitively, Eω⊂Ω (I(Ko,VGE∣ω)) with ∣ω∣ = 2 is the expected value of I(Ko,Vge)
1The learning rate decays 50% per 20 epochs.
2For VGG-like models, width = 1 means the model has 64, 128, 256, 512, 512 channels in its structure,
while width = 0.5 means that the model has 32, 64, 128, 256, 256 channels in its structure.
22
Under review as a conference paper at ICLR 2022
Table 12: Mutual Information of Complexity Measures on CIFAR-100.
Complexity Measure	VGG			
	Dropout	GG(∣ω∣=2)	GG(∣ω∣=1)	GG(∣ω∣=0)
Frob Distance	0.0128	-0.0172-	0.0068	0.0001
Spectral Norm	0.0109	0.0237	0.0141	0.0074
Parameter Norm	0.0122	0.0168	0.0086	0.0042
Path Norm	0.0370	0.0155	0.0025	0.0009
Sharpness α0	0.0333	0.0342	0.0164	0.0030
PAC-Sharpness	0.0390	0.0346	0.0166	0.0019
PAC-S(Laplace)	0.6225	0.1439	0.1256	0.1083
PAC-S(Sampling)	0.5364	0.1001	0.0852	0.0787
volume expansion — volume contraction	dropout	normal
Figure 7: Disentanglement noise v.s. dropout. We train 2 VGG16 networks (Left) and 2 AlexNet
(Right) on CIFAR-10 and SVHN respectively, with normal training, dropout training, disentangle-
ment noise training (volume expansion) and stochastic noise training (volume contraction). For each
dataset, we present their test losses in different plots.
When 2 hyper-parameters in Ω are fixed. Note that, as We aim to study the relationship be-
tween dropout and generalization, we only compute Eω⊂α (I(Vco, VGE∣ω)) with ∣ω| = 0,1, 2 and
I (Vco , Vdropout ) for all complexity measures.
M.2 Complexity Measures
In the following, we show the details of complexity measures.
Frobenius (Frob) Distance and Parameter Norm (Dziugaite & Roy, 2017; Nagarajan & Kolter,
2019; Neyshabur et al., 2018b):
coFrob(fW) =X||WlF - Wl0||2F.	(25)
l
coPara(fW ) = X ||WlF ||2F .	(26)
l
Spectral Norm (Yoshida & Miyato, 2017):
coSpec(fW) =X||WlF -Wl0||22.	(27)
l
Path Norm: Path-norm was introduced in (Neyshabur et al., 2015b) as an scale invariant complexity
measure for generalization and is shown to be a useful geometry for optimization (Neyshabur et al.,
2015a). To calculate path-norm, we square the parameters of the network, do a forward pass on an
all-ones input and then take square root of sum of the network outputs. We define the following
measures based on the path-norm:
coPath(fW) =	fW 2 (1),
l
where W 2 = W ◦ W is the element-wise square operation on the parameters.
Sharpness-based PAC-Bayes:
CoPAC-S (fW) = X kw⅛2W^,
(28)
(29)
23
Under review as a conference paper at ICLR 2022
where σ is estimated by sharpness method. That is, let Vec(U0)〜N(μuo, σ021) be a sample from
a zero mean Gaussian, where σ is chosen to be the largest number such that maxσ0 ≤σ |L(fW +U 0) -
L(fW)| ≤ (Keskar et al., 2017; Pitas et al., 2017; Neyshabur et al., 2017; Jiang et al., 2020b).
PAC-Bayes with Weight Volume:
kWlF - Wl0k2F	1
CoPAC-S(LaPlace/Sampling) (fW) =	|_	^^	+ ln vo](W?)卜	(30)
where vol(Wl ) is estimated by Laplace approximation (PAC-S(Laplace)) and sampling method
(PAC-S(Sampling)).
M.3 Supplementary for Section 5.2
As Table 12 shows, for all cases concerning the generalization error on CIFAR-100 (VGG), i.e.,
∣ω∣= 0,1,2, PAC-S(Laplace) or PAC-S(SamPling) is still a clear winner and is also closely correlated
with dropout.
N Supplementary for Experiment 5.3
As Figure 7 shows, on the CIFAR-10 and SVHN, weight volume expansion improves generalization
performance, similar to dropout. This is in contrast to volume contraction, which leads to worse
generalization performance.
O Experiments on Text Clas sification
Table 13: Text classification for THUCNews .
Method	FastText	FastText(dropout)	TextRNN	TextRNN(dropout)	TextCNN	TextCNN(dropout)
Sampling	0.04±0.02	0.08±0.02	-0.03±0.02	0.06±0.02	0.05±0.02	0.09±0.02
Laplace	0.0366	0.0831	0.0126	0.0458	0.0453	0.0850
Train Loss	0.01	0.08	0.04	0.07	0.02	0.04
Test Loss	0.66	0.25	0.46	0.29	0.61	0.42
Loss Gap	0.65	0.17	0.42	0.22	0.59	0.38
Train Acc	0.9958	0.9844	0.9841	0.9751	0.9944	0.9899
Test Acc	0.9011	0.9217	0.9037	0.9102	0.9022	0.9129
Acc Gap	0.0947	0.0627	0.0804	0.0649	0.0922	0.0770
To make our empirical results more robust, we also implement FastText (Bojanowski et al., 2017;
Joulin et al., 2016; 2017), TextRNN (Liu et al., 2016) and TextCNN (Kim, 2014) to classify THUC-
News data set (Sun et al., 2016) (180000 training data, 10000 validation data, 10000 test data) with
and without dropout. The results of these text classification tasks in Table 13 also indicate that
dropout noise can narrow generalization gap and expand the weight volume.
P	Future Work
In this paper, we introduce ‘weight expansion’, an intriguing phenomenon that appears whenever
dropout is applied during training. We have provided both theoretical and empirical arguments,
which show that weight expansion is one of the key indicators of the reduced generalization error,
and that dropout is effective to obtain weight expansion.
For the next steps, we believe the following few directions are worthy of exploration and research.
•	First, at the moment, we consider weights as a vector, without taking into consideration the
structural information that the weights can be closely coupled according to the structures
of the layers. It is interesting to know whether or not weight volume may capture some
structural information of network (e.g., convolutional layer).
•	Second, it will be useful to develop more PAC-Bayesian optimization methods (Dziugaite
& Roy, 2017; Letarte et al., 2019; Perez-Ortiz et al., 2021b) to help expand weight volume.
24
Under review as a conference paper at ICLR 2022
While the methods in the paper have been shown effective and efficient, they are by no
means optimal. It is not hard to see that such optimization problem is highly intractable,
so an optimal solution may not be practical. Nevertheless, we believe there are rooms for
improvement, and any progress in this direction will be significant for not only the problem
in this paper but also other problems that may involve PAC-Bayesian optimization.
•	Third, activation functions have been argued to have significant impact on not only the
trained models but also the training process, related to the first point, it is interesting to
understand how weight volume is related to activation functions (e.g., ReLU, tanh).
•	Last but not least, it can be a significant topic to study whether the weight volume - When
combined with the PAC-Bayesian adversarial generalization bound (Farnia et al., 2018)-
can provide any guidance to the hot topic of adversarial generalization.
25