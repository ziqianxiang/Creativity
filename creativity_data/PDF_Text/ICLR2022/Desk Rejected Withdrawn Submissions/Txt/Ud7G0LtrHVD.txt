Under review as a conference paper at ICLR 2022
Are Vision Transformers Robust to Patch-
wise Perturbation ?
Anonymous authors
Paper under double-blind review
Ab stract
The recent advances in Vision Transformer (ViT) have demonstrated its impres-
sive performance in image classification, which makes it a promising alternative
to Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input
image as a sequence of image patches. The patch-wise input image representation
makes the following question interesting: How does ViT perform when individual
input image patches are perturbed with natural corruptions or adversarial pertur-
bations, compared to CNNs? In this work, we conduct a comprehensive study on
the robustness of vision transformers to patch-wise perturbations. Surprisingly,
we find that vision transformers are more robust to naturally corrupted patches
than CNNs, whereas they are more vulnerable to adversarial patches. Based on
extensive qualitative and quantitative experiments, we discover that ViT’s stronger
robustness to natural corrupted patches and higher vulnerability against adversar-
ial patches are both caused by the attention mechanism. Specifically, the attention
model can help improve the robustness of vision transformers by effectively ig-
noring natural corrupted patches. However, when vision transformers are attacked
by an adversary, the attention mechanism can be easily fooled to focus more on
the adversarially perturbed patches and cause a mistake.
1	Introduction
Recently, Vision Transformer (ViT) has achieved impressive performance (Dosovitskiy et al., 2020;
Touvron et al., 2021), which makes it become a potential alternative to convolutional neural networks
(CNNs). Unlike CNNs, ViT processes the input image as a sequence of image patches. Then, a
self-attention mechanism is applied to aggregate information from all patches. Existing works have
shown that ViT are more robust than the popular CNNs when the whole input image is perturbed with
natural corruptions or adversarial perturbations (Bhojanapalli et al., 2021). In this work, instead, we
study the robustness of ViT to patch-wise perturbations based on its special patch-based architecture.
Two typical types of perturbations are considered to compare the robustness between ViTs and
CNN (e.g., ResNets (He et al., 2016)). One is natural corruptions (Hendrycks & Dietterich, 2019),
which is to test models’ robustness under distributional shift. The other is adversarial perturba-
tions (Szegedy et al., 2014), which are created by an adversary to specifically fool a model to make
a wrong prediction. Surprisingly, we find ViT does not always perform more robustly than ResNet.
When individual image patches are naturally corrupted, ViT performs more robust than ResNet.
However, when input image patch(s) are adversarially attacked, ViT shows a higher vulnerability.
Digging down further, we find the reason behind is that the self-attention mechanism of ViT can
effectively ignore the natural patch corruption, while it’s also easy to manipulate the self-attention
mechanism to focus on an adversarial patch. This is well supported by rollout attention visual-
ization (Abnar & Zuidema, 2020) on ViT. As shown in Fig. 1 (a), ViT successfully attends to the
class-relevant features on the clean image, i.e., the head of the dog. When one or more patches are
perturbed with natural corruptions, shown in Fig. 1 (b), ViT can effectively ignore the corrupted
patches and still focus on the main foreground to make a correct prediction. In Fig. 1 (b), the atten-
tion weights on the positions of naturally corrupted patches are much smaller even when the patches
appear on the foreground. In contrast, when the patches are perturbed with adversarial perturbations
by an adversary, shown in Fig. 1 (c), ViT is successfully fooled to make a wrong prediction because
the attention of ViT is misled to focus on the adversarial patches instead.
1
Under review as a conference paper at ICLR 2022
(a) Clean Image
(b) with Naturally Corrupted Patch
Figure 1: Images with patch-wise perturbations (top) and their corresponding attention maps (bot-
tom). The attention mechanism in ViT can effectively ignore the naturally corrupted patches to
maintain a correct prediction, whereas it is forced to focus on the adversarial patches to make a
mistake. The images with corrupted patches are all correctly classified. The images with adversary
patches in subfigure 1c are misclassified as dragonfly, axolotl, and lampshade, respectively.
(c) with Adversarial Patch
Based on the patch-based architectural structure of vision transformers, we further investigate the
sensitivity of ViT against patch positions and patch alignment of adversarial patches. First, we
discover that ViT is insensitive to different patch positions, while ResNet shows high vulnerability
on the central area of input images and much less on corners. We attribute this to the architecture
bias of ResNet where pixels in the center can affect more neurons than the ones in corners. In
contrast, each patch within ViT can equally interact with other patches regardless of its position.
Further, we find that for ViT, the adversarial patch designed to attack one particular position can
successfully transfer to other positions of the same image as long as they are aligned with input
patches. In contrast, the ones on ResNet hardly do.
Our main contributions can be summarized as follows:
•	We discover that ViT is more robust to natural patch corruption than ResNet, whereas it is
more vulnerable to adversarial patch perturbation.
•	Based on extensive analysis, we find that the self-attention mechanism, the core building
block of vision transformers, can effectively ignore natural corrupted patches to maintain a
correct prediction but be easily fooled to focus on adversarial patches to make a mistake.
•	We show that ViT and ResNet exhibit different sensitivities against patch positions and
patch alignment of adversarial patch attacks due to their different architectural structures.
2	Related Work
Robustness of Vision Transformer The robustness of ViT have achieved great attention due to
its great success in many vision tasks (Bhojanapalli et al., 2021; Naseer et al., 2021). On the one
hand, (Bhojanapalli et al., 2021; Paul & Chen, 2021) show that vision transformers are more robust
to natural corruptions (Hendrycks & Dietterich, 2019) compared to CNNs. On the other hand,
(Shao et al., 2021) demonstrates that ViT achieves higher adversarial robustness than CNNs under
adversarial attacks. These existing work, however, mainly focus on investigating the robustness of
ViT when a whole image is naturally corrupted or adversarially perturbed. Instead, our work focuses
on the patch-based architecture trait of ViT and study the robustness of ViT to patch-based natural
corruption and adversarial perturbation.
Adversarial Patch Attack The work (Papernot et al., 2016) shows that adversarial examples can
be created by perturbing only a small amount of input pixels. Further, (Brown et al., 2017) success-
fully create universal, robust and targeted adversarial patches. These adversarial patches therein are
often placed on the main object in the images. (Karmon et al., 2018) proposes a strong adversarial
patch attack method. They show that the created adversarial patches do not have to cover any main
object and can be placed at image corners. In this work, we apply the adversarial patch attack in
(Karmon et al., 2018) to ViT and place adversarial patches aligned with image patches.
2
Under review as a conference paper at ICLR 2022
Model	Pretraining DataAug Input Size WeightStand GroupNorm Weight Decay
ResNet (He et al., 2016)	N	N	224	N	N	Y
BiT (Kolesnikov et al., 2020)	Y	N	480	Y	Y	N
ViT (Dosovitskiy et al., 2020)	Y	N	224/384	N	N	N
DeiT (Touvron et al., 2021)	N	Y	224/384	N	N	N
Table 1: Comparison of popular ResNet and ViT models: The difference in model robustness can
not be blindly attributed to the model architectures. It can be caused by different training settings.
Model	Model Size	Clean Accu	Model	Model Size	Clean Accu
ResNet50	25M	78.79	ResNet18	12M	69.39
DeiT-small	22M	79.85	DeiT-tiny	5M	72.18
Table 2: Fair base models: DeiT and counter-part ResNet are trained with the exact same setting.
Two models of each pair achieve similar clean accuracy with comparable model sizes.
3	Experimental Setting to Compare ViT and ResNet
Background Given an input image x ∈ RH×W×C, ResNet (He et al., 2016) composed of a set of
residual blocks takes x as input. The extracted feature maps in the l-th block is xl ∈ RHl ×Wl ×Cl
where Hl,Wl,Cl are the height, the width and the number of channels of feature maps. The final
feature maps are flattened and mapped into the output. Different from ResNet, ViT (Dosovitskiy
et al., 2020) first reshapes the input X into a sequence of image patches {xi ∈ R(H∙ JP)×(P2∙C)}N=ι
where P is the patch size and N is the number of patches. A class-token patch is concatenated to
the patch sequence. A set of self-attention blocks is applied to obtain patch embeddings of the l-th
block {xli}iN=1. The class-token patch embedding of the last block is mapped to the output.
Fair Base Models We list the state-of-the-art ResNet and ViT models and part of their training
settings in Tab. 1. The techniques applied to boost different models are different, e.g. pretraining.
Previous work (Hendrycks et al., 2019; Chen et al., 2020) have shown that weight decay, data aug-
mentation and pre-training can affect model robustness. In addition, our investigation finds weight
standardization and group normalization have a significant impact on model robustness (See details
in Appendix A). This indicates that the difference in model robustness can not be blindly attributed
to the model architectures if models are trained with different settings. Hence, we build fair base
models to compare ViT and ResNet as follows.
First, we follow (Touvron et al., 2021) to choose two pairs of fair model architectures, DeiT-small
vs. ResNet50 and DeiT-tiny vs. ResNet18. The two models of each pair (i.e. DeiT and its counter-
part ResNet) are of similar model sizes. Further, we train ResNet50 and ResNet18 using the exactly
same setting as DeiT-small and Deit-tiny in (Touvron et al., 2021). In this way, we make sure
the two compared models, e.g., DeiT-samll and ResNet50, have similar model size, use the same
training techniques, and achieve similar test accuracy (See Tab. 2). The two fair base model pairs
are used across this paper for a fair comparison.
Adversarial Patch Attack We now introduce adversarial patch attack (Karmon et al., 2018) used
in our study. The first step is to specify a patch position and replace the original pixel values of
the patch with random initialized noise δ. The second step is to update the noise to minimize the
probability of ground-truth class, i.e. maximize the cross-entropy loss via multi-step gradient ascent
(Madry et al., 2017). The adversary patches are specified to align with input patches of DeiT.
Evaluation Metric We use the standard metric Fooling Rate (FR) to evaluate the model robust-
ness. First, we collect a set of images that are correctly classified by both models that we compare.
The number of these collected images is denoted as P . When these images are perturbed with natu-
ral patch corruption or adversarial patch attack, we use Q to denoted the number of images that are
misclassified by the model. The Fooling Rate is then defined as FR = Q. The lower the FR is, the
more robust the model is.
3
Under review as a conference paper at ICLR 2022
Model	the Number of Naturally Corrupted Patches				the Number of Adversarial Patches			
	32	96	160	196	1	2	3	4
	 ResNet50	3.7	18.2	43.4	49.8	30.6	59.3	77.1	87.2
DeiT-small	1.8	7.4	22.1	38.9	61.5	95.4	99.9	100
ResNet18	6.8	31.6	56.4	61.3	39.4	73.8	90.0	96.1
DeiT-tiny	6.4	14.6	35.8	55.9	63.3	95.8	99.9	100
Table 3: Fooling Rates (in %) are reported. DeiT is more robust to naturally corrupted patches than
ResNet, while it is significantly more vulnerable than ResNet against adversarial patches.
4	Robustness of ViT to Patch-wise Perturbations
Following the setting in (Touvron et al., 2021), we train the models DeiT-small, ResNet50, DeiT-
tiny, and ResNet18 on ImageNet 1k training data respectively. Note that no distillation is applied.
The input size for training is H = W = 224, and the patch size is set to 16. Namely, there are
196 image patches totally in each image. We report the clean accuracy in Tab. 2 where DeiT and its
counter-part ResNet show similar accuracy on clean images.
4.1	Patch-wise Natural Corruption
First, we investigate the robustness of DeiT and ResNet to patch-based natural corruptions. Specif-
ically, we randomly select 10k test images from ImageNet-1k validation dataset (Deng et al., 2009)
that are correctly classified by both DeiT and ResNet. Then for each image, we randomly sam-
ple n input image patches xi from 196 patches and perturb them with natural corruptions. As in
(Hendrycks & Dietterich, 2019), 15 types of natural corruptions with the highest level are applied to
the selected patches, respectively. The fooling rate of the patch-based natural corruption is computed
over all the test images and all corruption types. We test DeiT and ResNet with the same naturally
corrupted images for a fair comparison.
We find that both DeiT and ResNet hardly degrade their performance when a small number of
patches are corrupted (e.g., 4). When we increase the number of patches, the difference between
two architectures emerges: DeiT achieves a lower FR compared to its counter-part ResNet (See
Tab. 3). This indicates that DeiT is more robust against naturally corrupted patches than ResNet.
The same conclusion holds under the extreme case when the number of patches n = 196. That is:
the whole image is perturbed with natural corruptions. This is aligned with the observation in the
existing work (Bhojanapalli et al., 2021) that vision transformers are more robust to ResNet under
distributional shifts. More details on different corruption types can be found in Appendix B.
In addition, we also increase the patch size of the perturbed patches, e.g., if the patch size of the
corrupted patch is 32 × 32, it means that it covers 4 continuous and independent input patches as
the input patch size is 16 × 16. As shown in Fig. 2 (Left), even when the patch size of the perturbed
patches becomes larger, DeiT (marked with red lines) is still more robust than its counter-part ResNet
(marked with blue lines) to natural patch corruption.
4.2	Patch-wise Adversarial Attack
In this section, we follow (Karmon et al., 2018) to generate adversarial patch attack and then com-
pare the robustness of DeiT and ResNet against adversarial patch attack. We first randomly select
100 images from ImageNet-1k validation set that are correctly classified by both models we com-
pare. Following (Karmon et al., 2018), the '∞-norm bound, the step size, and the attack iterations
are set to 255/255, 2/255, and 10K respectively. The averaged FR is reported.
As shown in Tab. 3, DeiT achieves much higher fooling rate than ResNet when one of the input
image patches is perturbed with adversarial perturbation. This consistently holds even when we
increase the number of adversarial patches, sufficiently supports that DeiT is more vunerable than
ResNet against patch-wise adversarial perturbation. When more than 4 patches (〜2% area of the
input image) are attacked, both DeiT and ResNet can be successfully fooled with almost 100% FR.
4
Under review as a conference paper at ICLR 2022
O O
2
(％∖~) sωsα CTC=OOU-
I16
Natural Corruption Patch Size (P×P)	Adversarial Patch Size (P×P)
Figure 2: DeiT with red lines shows a smaller FR to natural patch corruption and a larger FR to
adversarial patch of different sizes than counter-part ResNet.
When we attack a large continuous area of the input image by increasing the patch size of adversarial
patches, the FR on DeiT is still much larger than counter-part ResNet until both models are fully
fooled with 100% fooling rate. As shown in Fig. 2 (Right), DeiT (marked with red lines) consistently
has higher FR than ResNet under different adversarial patch sizes.
Taking above results together, we discover that DeiT is more robust to natural patch corruption than
ResNet, whereas it is significantly more vulnerable to adversarial patch perturbation.
5 Understanding the Robustness of ViT to Patch-wise
Perturbations
In this section, we visualize and analyze models’ attention to understand the different robustness
performance of DeiT and ResNet against patch-wise perturbations. Although there are many exist-
ing methods, e.g., (Selvaraju et al., 2017), designed for CNNs to generate saliency maps, it is not
clear yet how suitable to generalize them to vision transformers. Therefore, we follow (Karmon
et al., 2018) to choose the model-agnostic gradient visualization method to compare the gradient
(saliency) map (Zeiler & Fergus, 2014) of DeiT and ResNet when they are attacked by adversarial
patches. Specifically, we first obtain the gradients of input examples towards the predicted classes,
sum the absolute values of the gradients over three input channels, and visualize them.
Qualitative Evaluation As shown in Fig. 3 (a), when we use adversarial patch to attack a ResNet
model, the gradient maps of the original images and the images with adversarial patch are similar.
This is consistent with the observation in the previous work (Karmon et al., 2018). On the contrary,
the adversarial patch can change the gradient map of DeiT by attracting more attention. As shown
Figure 3: Gradient Visualization: the clean image, the images with adversarial patches, and their
corresponding gradient maps are visualized. We use a blue box on the gradient map to mark the
location of the adversarial patch. The adversary patch on DeiT attracts attention, while the one on
ResNet hardly do.
5
Under review as a conference paper at ICLR 2022
Quantitative Evaluation We also measure our observation on the attention changes with the met-
rics in (Karmon et al., 2018). In each gradient map, we score each patch according to (1) the
maximum absolute value within the patch (MAX); and (2) the sum of the absolute values within the
patch (SUM). We first report the percentage of patches where the MAX is also the maximum of the
whole gradient map. Then, we divide the SUM of the patch by the SUM of the all gradient values
and report the percentage.
As reported in Tab. 4, the pixel with the maximum gradient value is more likely to fall inside the
adversarial patch on DeiT, compared to that on ResNet. Similar behaviors can be observed in the
metric of SUM. The quantitative experiment also supports our claims above that adversarial patches
mislead DeiT by attracting more attention.
	Towards ground-truth Class				Towards misclassified Class			
	SUM		MAX		SUM		MAX	
	 Patch Size	16	32	16	32	16	32	16	32
ResNet50	0.42	1.40	0.17	0.26	0.55	2.08	0.25	0.61
DeiT-small	1.98	5.33	8.3	8.39	2.21	6.31	9.63	12.53
ResNet18	0.24	0.74	0.01	0.02	0.38	1.31	0.05	0.13
DeiT-tiny	1.04	3.97	3.67	5.90	1.33	4.97	6.49	10.16
Table 4: Quantitative Evaluation: Each cell lists the percent of patches in which the maximum
gradient value inside the patches is also the maximum of whole gradient map. SUM corresponds to
the sum of element values inside patch divided by the sum of values in the whole gradient map. The
average over all patches is reported.
Besides the gradient analysis, another popular tool used to visualize ViT is Attention Rollout (Ab-
nar & Zuidema, 2020). To further confirm our claims above, we also visualize DeiT with Attention
Rollout in Fig. 4. The rollout attention also shows that the attention of DeiT is attracted by adver-
sarial patches. The attention rollout is not applicable to ResNet. As an extra check, we visualize
and compare the feature maps of classifications on ResNet. The average of feature maps along the
channel dimension is visualized as a mask on the original image. The visualization also supports
the claims above. More visualizations are in Appendix D. Both qualitative and quantitative analysis
above verifies our claims that the adversarial patch can mislead the attention of DeiT by attactting it.
(a) Attention on ResNet18 under Adversary Patch Attack
(b) Attention on DeiT-tiny under Adversary Patch Attack
Figure 4: Attention Comparison between ResNet and DeiT under Patch Attack: the clean image, the
adversarial images, and their corresponding attention are visualized. The adversary patch on DeiT
attract attention, while the ones on ResNet hardly do.
However, the gradient analysis is not available to compare ViT and ResNet on images with natural
corrupted patches. When a small number of patch of input images are corrupted, both Deit and
ResNet are still able to classify them correctly. The slight changes are not reflected in vanilla gra-
dients since they are noisy. When a large area of the input image is corrupted, the gradient is very
noisy and semantically not meaningful. Due to the lack of a fair visualization tool to compare DeiT
and ResNet on naturally corrupted images, we apply Attention Rollout to DeiT and Feature Map
Attention visualization to ResNet for comparing the their attention.
6
Under review as a conference paper at ICLR 2022
(a) Attention on ResNet18 under Natural Patch Corruption
(b) Attention on DeiT-tiny under Natural Patch Corruption
Figure 5: Attention Comparison between ResNet and DeiT under Natural Patch Corruption: the
clean image, the naturally corrupted images, and their corresponding attention are visualized. The
patch corruptions on DeiT are ignored by attending less to the corrupted patches, while the ones on
ResNet are treated as normal patches.
I
The attention visualization of these images is shown in Fig. 5. We can observe that ResNet treats
the naturally corrupted patches as normal ones. The attention of ResNet on natually patch-corrupted
images is almost the same as that on the clean ones. Unlike CNNs, DeiT attends less to the corrupted
patches when they cover the main object. When the corrupted patches are placed in the background,
the main attention of DeiT is still kept on the main object. More figures are in Appendix D.
6	Probing the Robustness of ViT to Adversarial Patches
In this section, we further investigate the properties of adversary patches created on DeiT from
the perspectives of patch positions, patch alignment, and patch attack effectiveness. Concretely,
we answer the following three questions: Does DeiT show similar vulnerability in different patch
positions? Is it necessary to keep adversary patch postion aligned with input patches of DeiT? Are
the patch attacks still able to fool DeiT in various attack settings?
6.1	Sensitivity to Patch Positions
To investigate the sensitivity against the location of adversarial patch, we visualize the FR on each
patch position in Fig. 6. We can clearly see that adversarial patch achieves higher FR when attacking
DeiT-tiny than ResNet18 in different patch positions. Interestingly, we find that the FRs in different
patch positions of DeiT-tiny are similar, while the ones in ResNet18 are center-clustered. A similar
pattern is also found on DeiT-small and ResNet50 in Appendix E.
22 29 39 42 34 39 36 39 39 35 26 33 22 19
56 7 8 910
LPled % xpu- UEn-。。
56 7 8 910
LPdxpu- UEn-。。
13
13
1	2	3	4	5	6	7	8	9 10 11 12 13 14
Row index of Patch
1	2	3	4	5	6	7	8	9 10 11 12 13 14
Row Index of Patch
(a) Adversarial Patch Attack FRs on ResNet18 (b) Adversarial Patch Attack FRs on DeiT-tiny
Figure 6: Patch Attack FR (in %) in each patch position is visualized. FRs in different patch positions
of DeiT-tiny are similar, while the ones in ResNet18 are center-clustered.
7
Under review as a conference paper at ICLR 2022
Considering that ImageNet are center-biased where the main objects are often in the center of the
images, we cannot attribute the different patterns to the model architecture difference without fur-
ther investigation. Hence, we design the following experiments to disentangle the two factors, i.e.,
model architecture and data bias. Specifically, we select two sets of correctly classified images from
ImageNet 1K validation dataset. As shown in Fig. 7a, the first set contains images with corner bias
where the main object(s) is in the image corners. In contrast, the second set is more center-biased
where the main object(s) is exactly in the central areas, as shown in Fig. 7b.
(a) Corner-biased Images
(b) Center-biased Images
Figure 7: Collection of two sets of biased data: the fist set contains only images with corner-biased
object(s), and the other set contains center-biased images.
We apply patch attack to corner-biased images (i.e., the first set) on ResNet. The FRs of patches in
the center area are still significantly higher than the ones in the corner (See Appendix F). Based on
this, we can conclude that such a relation of FRs to patch position on ResNet is caused by ResNet
architectures instead of data bias. The reason behind this might be that pixels in the center can affect
more neurons of ResNet than the ones in corners.
Similarly, we also apply patch attack to center-biased images (the second set) on DeiT. We observe
that the FRs of all patch positions are still similar even the input data are highly center-biased (See
Appendix F). Hence, we draw the conclusion that DeiT shows similar sensitivity to different input
patches regardless of the content of the image. We conjecture it can be explained by the architecture
trait of ViT, in which each patch equally interact with other patches regardless of its position.
6.2	Alignment of Adversarial Patches with Input Patches
Attack with Unaligned Patches All the previous sections study the case that the patch-wise per-
turbation is added onto an individual input patch xi or an area includes multiple input patches. In
other words, the adversarial patch is perfectly aligned with the input patch. In this section, we fo-
cus on different architectures’ sensitivity to the alignment between adversarial patches and input
patches. Specifically, we apply adversarial patch of the same size as a single input patch to a random
area in the image. We find that DeiT becomes less vulnerable to adversarial patch attack, e.g., the
FR on DeiT-small decreases from 61.5% to 47.9%. Intuitively, when the adversarial patch is not
perfectly aligned with input patch, i.e., the attacker can only manipulate part of patch pixels rather
than a full patch, the attention of DeiT is less likely to be manipulated. Similarly, we also apply
an patch attack to a random image area on ResNet. As expected, the FR on ResNet stays similar
(aligned 30.6 vs. unaligned 31.2) because ResNet does not process a whole image based on patches.
Therefore, we can conclude that DeiT is sensitive to the alignment between adversarial patch and
input patch whereas ResNet is not due to their different architecture structures.
(Un)aligned Transfer of Adversarial Patch Perturbation (Karmon et al., 2018) shows that the
adversarial patch created on an image on ResNet is not effective anymore when shifted even a
single pixel away. We also conduct similar experiments on DeiT. We find that the adversarial patch
perturbation on DeiT does not transfer well either when only shifted a single-pixel away. However,
when shifted to match another input patch exactly, the adversarial patch is still highly effective.
Namely, the adversarial perturbation can be still effective when aligned with a different patch. The
reason behind this is that, when the adversarial patch is switched to another patch, the network
attention can still be misled as shown in Sec. 5. When shifted in a single pixel, the structure of
8
Under review as a conference paper at ICLR 2022
Trans-(X,Y) ∣ (0,1)		(0, 8)	(0, 16)	(0,32) I (1,0)		(0, 8)	(16, 0)	(32,0) I (1,1)		(8, 8)	(16, 16)
ResNet50	0.06	0.17	0.31	0.48	0.06	0.20	0.18	0.40	0.08	0.20	0.35
DeiT-small	0.27	0.13	8.43	4.26	0.28	0.19	8.13	3.88	0.21	0.22	4.97
ResNet18	0.22	0.33	0.46	0.56	0.19	0.34	0.49	0.68	0.15	0.23	0.49
DeiT-tiny	2.54	2.32	29.15	18.19	2.30	1.73	28.37	17.32	2.11	2.29	21.23
Table 5: Transferability of adversarial patch across different patch positions of the the image. Trans-
lation X/Y stands for the number of pixels shifted in rows or columns. When they are shifted to
cover other patches exactly, adversarial patches transfer well, otherwise not.
perturbation is destroyed due to the patch split of DeiT. Additionally, We find that the adversarial
patch perturbation barely transfers across images or models regardless of the alignment. Details can
be found in Appendix G.
6.3	Patch Attack under different Settings
Iterations of Patch Attack In our experiment, as in (Karmon et al., 2018), the attack iteration is
set to 10k. We also check how many iterations are required to attack the classification successfully.
The required iterations are averaged on all patch postions of the misclassified images. The required
attack iterations on DeiT-tiny is less than that on ResNet18 (65 vs. 342). The observation also holds
on DeiT-small and ResNet50 (294 vs. 455). This experiment shows DeiT is more vulnerable than
ResNet from another perspective.
Imperceptible Patch Attack In this work, we use unbounded local patch attacks where the pixel
intensity can be set to any value in the image range [0, 1]. The adversarial patches are often visible,
as shown in Fig. 1. In a more popular setting of adversarial attack and defense, the maximally
allowed change of the input value is 8/225, in which the adversarial perturbation is imperceptible
human vision. We also compare ResNet and DeiT under this setting.
In the case of a single patch attack, the attacker achieves FR of 2.9% on ResNet18 and 11.2% on
DeiT-tiny. More scores and visualization of the images with imperceptible perturbation can be found
in Appendix I. DeiT is still more vulnerable than ResNet when individual patches are attacked with
imperceptible perturbation. When the patch size to attack is set to be the whole image size, it is
exactly the same as the standard attack. We show that both ResNet and DeiT can be easily fooled
When the standard attack setting is applied.
Targeted Patch Attack A targeted attack can be achieved by setting the attack objective to maxi-
mize the probability of the target class. We also compare DeiT and ResNet under the targeted attack
above. In the experiment, we randomly select a target class except for the ground-truth class for
each image. In the case of a single attack patch, the attacker achieves FR of 15.4% on ResNet18
and 32.3% on DeiT-tiny. Under targeted attack, DeiT is more vulnerable than ResNet. The claim
also holds on the other model pair (ResNet18 7.4% vs. DeiT-small 24.9%). Visualization of the
adversarial patches is in Appendix J.
7	Conclusion
Based on the patch-based architectural trait of ViT, we investigate its robustness against two types of
patch-wise perturbations: natural patch corruption and adversarial patch attack. Compared to con-
volutional networks (e.g., ResNet), vision transformer (e.g., DeiT) is more robust to natural patch
corruption, whereas it is significantly more vunerable against adversarial patch. Extensive analysis
reveals that the self-attention mechanism of vision transformers can effectively ignore natural cor-
rupted patches but be easily misled to adversarial patches to make a mistake. Further, we find that
DeiT and ResNet exhibit different sensitivities against patch positions and patch alignment of adver-
sarial patch attacks due to their different architectural structures. We believe our work can shed light
on improving robustness of transformer-based models and spur future work on investigating ViT
variants, e.g., MLP-Mixer (Tolstikhin et al., 2021), and the robustness of transformers to token-wise
perturbations in NLP.
9
Under review as a conference paper at ICLR 2022
References
Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics (ACL), 2020.
Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and An-
dreas Veit. Understanding robustness of transformers for image classification. arXiv preprint
arXiv:2103.14586, 2021.
Tom B Brown, Dandelion Mane, AUrko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665v1, 2017.
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial
robustness: From self-supervised pre-training to fine-tuning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 699-708, 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In ICLR, 2019.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness
and uncertainty. In International Conference on Machine Learning, pp. 2712-2721. PMLR, 2019.
Danny Karmon, Daniel Zoran, and Yoav Goldberg. Lavan: Localized and visible adversarial noise.
In International Conference on Machine Learning, pp. 2507-2515. PMLR, 2018.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision-
ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part
V 16, pp. 491-507. Springer, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. 2017.
Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz
Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv preprint
arXiv:2105.10497, 2021.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European sympo-
sium on security and privacy (EuroS&P), pp. 372-387. IEEE, 2016.
Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint
arXiv:2105.07581, 2021.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robust-
ness of visual transformers. arXiv preprint arXiv:2103.15670, 2021.
10
Under review as a conference paper at ICLR 2022
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. ICLR, 2014.
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An
all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herve Jegou. Training data-effiCient image transformers & distillation through attention. In
International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
A	The Training Setting Can Affect Model Robustness
We train ResNet18 on CIFAR10 in the standard setting (He et al., 2016). To study the impact
of training settings on model robustness, we train models with different input sizes (i.e., 32, 48,
64), with or without Weight Standardization and Group Normalization to regularize the training
process. The foolong rate of single patch attack is reported. Especially, with our experiments,
we find that Weight Standardization and Group Normalization can have a significant impact on
model robustness (See Tab. 6). The two techniques are applied in BiT (Kolesnikov et al., 2020) to
improve its performance. However, they are not applied to standard ViT and DeiT training settings.
Hence, the robustness difference between ViT and BiT cannot be attributed to the difference of
model architectures.
Note that a comprehensive study of the relationship between all factors of training and model ad-
versarial robustness is out of the scope of this paper. We aim to point out that these factors can have
an impact on model robustness to different extents. The robustness difference cannot be blindly
attributed to the difference of model architectures. We need to build new fair base models to study
the robustness of ResNet and ViT.
Model	Input Size			Model		Training Techniques		
ResNet18	32	48	64	ResNet18	No	WS	GN	WS + GN
Clean Accuracy	93.4	93.8	93.7	Clean Accu	93.4	93.6	92.0	93.8
FR of Patch Attack	35.9	42.2	39.2	Patch Attack FR	35.9	51.3	52.6	71.1
Table 6: Study of the training factors on the relation to model robustness: While the input size has
minor impact on model robustness, Weight Standardization (WS) and Group Normalization (GN)
can change model robustness significantly.
B	Natural Patch Corruption with Different Levels and Types
Models can show different robustness when the inputs are corrupted with different natural noise
types. To better evaluate the model robustness to natural corruption, the work (Hendrycks & Diet-
terich, 2019) summarizes 15 common natural corruption types. The averaged score is used as an
indicator of model robustness. In this appendix section, we show more details of model robustness
to different noise types. As show in Fig. 8 and 9, The FR on DeiT is lower than on ResNet. We
conclude that DeiT is more robust than ResNet to natural patch corruption.
Furthermore, we also investigate the model robustness in terms of different noise levels. As shown in
Fig. 10 and 11. The different colors stand for different noise level. S1-S5 corresponds to the natural
corruption severity from 1 to 5. In each noise type, the left bar corresponds to ResNet variants and
the right one to DeiT variants. We can observe that DeiT show lower FR in each severity level.
Namely, the conclusion drawn above also holds across different noise levels.
11
Under review as a conference paper at ICLR 2022
Number of Perturbated Patches： 32
4
2 O
(求 UDω15α CTC-OOn-
Number of Perturbated Patches： 96
20
(求 UDω15α CTC=OOn-
(求 UD CTC-OOn-
Figure 8: Comparison of ResNet50 and Deit-small on Naturally Corrupted Patches
105 O
(求 U-ω1α CTC-OOn-
(求 UD 31su CTC=OOn-
o
Number of Perturbated Patches： 32
.	I	ResNetl8
OilllimUiUL
Number of Perturbated Patches： 96
40 -
20-
Number of Perturbated Patches： 160
ResNetie
50-
最
φ
hiLLauMiiT
Figure 9: Comparison of ResNet18 and Deit-tiny on Naturally Corrupted Patches
12
Under review as a conference paper at ICLR 2022
0 5 0
5 2 0
(求 Uα CTC-OOn-
20
O
(求 UDω15α CTC=OOn-
Figure 10: Comparison of ResNet50 and Deit-small on Patches Corrupted with Different Levels
O
(求 UD φ⅛^ CTC-OOn-
■ s3
■ s5
Number of Perturbated Patches： 32
kiiiumuiiu
105 O
(求 UDω1α CTC-OOn-
Number of Perturbated Patches： 96
■ s5
ΛHxlWJ
o 3p。
Number of Perturbated Patches： 160
Figure 11: Comparison of ResNet18 and Deit-tiny on Patches Corrupted with Different Levels
6 4 2 O 5
(求 U-)ω15α6U=OOL- n
S， -- (求 UDe36U--OOL
13
Under review as a conference paper at ICLR 2022
C Gradient Visualization of Adversarial Images
We first get the absolute value of gradient received by input and sum them across the channel di-
mension. The final values are mapped into gray image scale. We also mark the adversarial patch
With a blue bounding box in the visualized gradient maps.
The adversarial patch noises With different patch sizes (i.e., P=16 and P=32) are shoWn on DeiT and
ResNet in Fig. 17, 18, 19, and 20. In each roW of these figures, We fist shoW the clean image and
visualize the gradients of inputs as a mask on the image. Then, We shoW the images With patch noises
on different patch positions, and the gradient masks are also shoWn folloWing the corresponding
adversarial images.
D Attention Rollout on DeiT and Feature Map Masks on ResNet
In this appendix section, We shoW more Attention Rollout on DeiT and Feature Map Masks on
ResNet. The adversarial patch noises With different patch sizes are shoWn (i.e., P=16 and P=32) in
Fig. 21, 22, 23, and 24. In each roW of these figures, We fist shoW the clean image and visualize the
attention as a mask on the image. Then, We shoW the images With patch noises on different patch
positions, and the attention masks are also shoWn folloWing the correspond adversarial images.
The rollout attention on DeiT and Feature Map mask on ResNet on naturally corrupted images are
shoWn in Fig. 25, 26, 27, and 28. We can observe that ResNet treats tha corrupted patches as normal
ones. On DeiT, the attention is slightly distract by naturally corrupted patches When they are in the
background. HoWever, the main attention is still on the main object of input.
E	Fooling Rates of Each Patch on ResNet50 and DeiT- small
The FRs in different patch positions of DeiT are similar, While the ones in ResNet are center-
clustered. A similar pattern can also be found on DeiT-small and ResNet50 in Fig. 12.
UXed Jo xφpu- UE-0。
22	28	32	37	39
25	25	20	27	23
31	27	26	25	26
34	26	26	25	36
37	27	30	29	33
39	28	29	30	32
39	29	34	40	42
39	37	41	42	41
42	30	40	42	42
38	30	31	33	38
32	25	33	36	42
29	17	27	31	34
22	14	25	29	28
17	16	24	27	37
37 30 31 27
27 23 22 21
32 34 31 24
39 37 34 31
39 40 32 32
36 41 39 43
42 42 37 47
41 44 46 39
43 48 44 43
38 45 45 41
39 43 40 36
37 35 32 32
39 27 30 34
29 33 30 29
28	24	27	23	12
17	17	23	18	14
29	26	18	22	12
30	26	25	18	19
33	23	25	25	21
40	30	27	27	27
43	39	33	30	28
32	36	34	35	28
31	32	29	36	32
34	30	30	28	30
32	30	21	22	26
31	32	25	20	25
25	21	15	16	18
28	26	18	14	9
uhed°xφpu- UE-0。
13
14
1	2	3	4	5	6	7	8	9 10 11 12 13 14
Row Index of Patch
(a) Adversarial Patch Attack FRs on ResNet50
67	52	62	57	65
62	43	49	59	54
62	51	51	55	54
66	61	51	57	63
65	57	60	62	61
56	56	59	67	67
68	60	66	64	65
67	67	56	69	59
62	64	64	62	67
65	57	55	51	60
50	61	55	57	59
55	46	56	55	59
47	50	51	47	62
59	61	62	67	70
59	69	67	56	62
60	70	69	63	55
60	62	74	64	58
61	71	69	60	60
68	68	71	66	58
70	76	75	69	62
72	68	74	71	71
71	76	72	69	69
72	73	76	78	72
63	70	66	68	70
60	66	72	68	59
51	57	61	70	65
55	61	70	58	53
72	63	68	60	64
58 61 54 71
60 54 57 58
51 50 56 60
64 61 57 58
62 67 59 59
55 57 57 61
68 63 59 71
61 64 67 65
62 58 59 70
61 57 58 54
59 56 59 64
55 56 47 51
42 47 58 62
57 60 60 67
13
14
1	2	3	4	5	6	7	8	9 10 11 12 13 14
：W	Row Index of Patch
(b) Adversarial Patch Attack FRs on DeiT-small
Figure 12:	Patch Attack FR (in %) in each patch position is visualized on ResNet50 and DeiT-small.
F Fooling Rates of Each Patch on ResNet and DeiT on biased data
In the coner-biased image set, the FR on ResNet is still center-clustered, as shoWn in Fig. 13a. In
the center-biased image set, the FR on DeiT is still similar on different patch postions, as shoWn in
Fig. 13b.
14
Under review as a conference paper at ICLR 2022
43	45	49	55	45	47	48	46	50	45	48	41	43	40
45	38	40	40	43	41	46	42	41	40	39	40	35	40
47	44	46	47	43	52	50	43	45	39	39	39	37	37
57	49	46	57	55	63	55	52	49	43	42	38	39	36
61	51	51	53	52	63	58	58	57	49	43	37	39	40
55	48	51	57	58	60	56	58	52	46	43	45	37	42
50	52	53	57	57	53	59	53	51	49	49	41	39	37
52	47	52	55	57	58	55	58	54	53	49	42	40	42
58	48	49	55	57	60	56	57	53	53	48	41	39	37
53	49	51	51	54	54	51	55	53	51	43	45	40	37
48	44	49	50	48	53	47	52	50	48	41	42	38	42
42	42	47	53	48	52	48	52	44	46	39	39	38	37
45	43	40	41	39	46	46	44	36	37	36	38	34	36
43	43	40	45	45	46	49	50	44	38	35	40	33	35
13
14
1	2	3	4	5	6	7	8	9 10 11 12 13 14
Row Index of Patch
(a)	FRs of ResNet18 on Corner-biased Data
77 79 71 77
66 65 61 71
70 66 68 70
72 69 73 66
68 72 72 70
72 69 75 71
68 72 75 79
69 78 75 73
75 75 69 78
71 69 72 72
68 68 65 70
73 66 61 62
65 68 71 69
77 72 74 74
70 75 76
71 74 75
65 78 75
68 71 74
72 79 76
70 74 81
71 78 78
75 78 81
79 73 76
71 76 77
73 71 72
70 77 79
72 64 80
76 79 77
76 69 77
72 74 74
78 69 72
69 69 72
70 68 73
75 74 68
74 76 76
80 76 77
78 74 78
81 72 74
83 75 73
64 75 69
72 67 68
75 75 75
73 72 71 73
64 70 66 68
73 66 76 69
74 79 73 68
75 72 69 71
75 68 74 73
73 69 70 73
72 70 70 80
74 70 78 82
71 65 66 74
70 65 75 66
64 64 66 69
68 69 69 69
70 71 73 69
13
14
1	2	3	4	5	6	7	8	9 10 11 12 13 14
Row Index of Patch
(b)	FRs of DeiT-tiny on Center-biased Data
Figure 13:	Patch Attack FR (in %) in each patch position is visualized on ResNet18 and DeiT-tiny
on biased data.
G Transferability of Adversarial Patches acros s Images,
Models, and Patch Positions
As shown in Tab. 7, the adversarial patch noise created on a given image hardly transfer to other
images. When large patch size is applied, the patch noises on DeiT transfer slightly better than the
ones on ResNet.
Models	ResNet50	DeiT-small	ResNet18	DeiT-tiny
across images (Patch Size=16)	3.5	2.1	3.4	6.4
across images (Patch Size=112)	8.1	13.4	10.6	21.5
Table 7: Transferability of adversarial patch across different images
The transferbility of adversrial noise between Vision Transformer and ResNet has already explored
in a few works. They show that the transferability between them is remarkablely low. As shown in
Tab. 8, the adversarial patch noise created on a given image does not transfer to other models.
Patch Size=16						Patch Size=112		DeiT-tiny
Models	ResNet50	DeiT-small	ResNet18	DeiT-tiny	ResNet50	DeiT-small	ResNet18	
ResNet50	-	0.3	0.16	2.2	-	5.25	8	11.75
DeiT-small	0.04	-	0.09	1.79	5.5	-	9.25	12.25
ResNet18	0.09	0.22	-	1.9	5.75	5	-	12
DeiT-tiny	0.04	0.13	0.06	-	5.5	5	9.25	-
Table 8: Transferability of adversarial patch across models
When they are transfered to another patch, the adversarial patch noises are still highly effective.
However, the transferability of patch noise can be low, when the patch is not aligned with input
patches. The claim on the patch noise with size of 112 is also true, as shown in Tab. 9.
15
Under review as a conference paper at ICLR 2022
Model	ResNet50	DeiT-small	ResNet18	DeiT-tiny
across positions (0, 4)	6.25	5.25	11.25	12.75
across positions (0, 16)	5.75	34.5	11.5	54
across positions (0, 64)	6	22	9.5	30.75
across positions (4, 0)	6.5	5.75	9.75	12.5
across positions (16, 0)	7.25	35	10.25	54
across positions (64, 0)	5.5	18.25	9.25	31
across positions (4, 4)	6	4.75	8.5	13.5
across positions (16, 16)	4.5	18.5	9	33
across positions (64, 64)	6	9.75	8.25	17.5
Table 9: Transferability of adversarial patch across patch positions
H Convergence Speed in Each Patch on ResNet50 and DeiT-small
As shown in Fig. 14, the required attack iterations on DeiT is also less than counter-part ResNet.
13
14
6 7 8 9 0 1 2
111
ed J。Xv-Mc- CES-OU
329	269	300	421	232	430	362	174	438	295	104	283	98	164
263	426	228	459	480	481	417	204	294	397	449	170	248	46S
197	357	341	383	403	451	588	421	405	611	203	320	335	107
271	471	351	477	264	233	321	302	596	389	254	244	160	175
271	245	543	545	748	318	376	273	319	348	324	379	369	304
411	405	471	274	488	680	228	397	206	344	236	279	343	113
361	426	327	452	397	295	301	322	435	350	184	185	250	239
478	465	614	425	594	468	236	302	315	299	322	487	227	145
359	439	510	425	213	541	204	181	549	254	418	188	361	228
700	674	605	474	384	279	558	360	354	318	245	248	297	231
630	483	411	329	478	318	515	399	285	483	213	234	325	282
315	227	360	318	257	100	610	298	244	346	246	274	271	230
222	324	520	227	179	371	278	289	282	140	300	229	295	361
135	263	436	441	405	509	491	243	391	198	194	181	291	95
1	2	3	4	5	6	7	8	9	10	11	12	13	14
Row index of Patch
6 7 8 9 0 1 2
111
IPd Jo Xv-Mc- CES-OU
74	50	71	59	76	59	59	45	84	64	59	73	49	153
42	63	35	43	114	56	42	42	72	48	79	47	51	78
48	60	64	51	53	61	108	73	85	60	78	76	39	77
55	48	51	54	49	77	75	68	61	68	54	69	73	68
78	54	61	89	52	103	69	67	73	45	52	40	56	64
65	43	72	82	56	71	85	81	61	76	45	51	47	64
70	93	54	58	60	63	59	59	45	63	58	45	58	65
49	62	84	47	59	76	51	65	53	88	69	69	64	68
69	59	63	49	54	69	83	55	50	66	63	46	69	89
57	62	68	64	59	35	87	46	71	160	67	62	71	110
51	62	116	61	43	62	42	65	65	53	46	74	93	46
38	66	96	100	41	123	57	71	63	50	36	68	55	50
75	68	58	69	50	43	82	45	58	59	60	65	73	138
63	77	54	48	48	94	69	58	59	75	77	64	61	66
(a) Required Attack Iterations on ResNet18
13
14
1	2	3	4	5	6	7	8	9	10 11 12 13 14
Row index of Patch
(b) Required Attack Iterations on DeiT-tiny
6 7 8 9 0 1 2
111
ed ‰0 Xv-Mc- CES-OU
316	499	618	556	736	641	406
414	615	372	668	343	448	344
646	625	530	575	472	555	465
798	549	349	444	518	654	705
508	468	553	457	335	664	529
758	495	377	318	514	280	335
731	405	586	406	606	552	340
562	936	492	462	392	525	611
579	353	611	410	382	388	450
744	350	435	377	464	247	452
635	287	595	509	780	485	639
729	239	530	429	454	508	444
582	190	623	511	358	805	277
265	258	350	556	486	360	520
392
545
378
249
598
419
301
273
405
323
509
417
605 669
466 357
666
470
196
251
197
100 153
379 541
327 421 401
309 589
215
454
324
263
102
550
347
417
313
242
259
521 406
553 618 574
307 269
498 441
847
678
393
499
590
118
176
312
368 487
248 428
677 365 656
375 576
341 417
311 376
620
257
440
747
553
630
322
657
425
257
457
407
403
592
613
630 277 384
177 314
314
440 389 402 448 236
118
6 7 8 9 0 1 2
111
IPd ‰0 Xv-Mc- CES-OU
161	121	183	148	153	108	227	164	118	281	300	187	137	161
240	230	262	258	170	423	389	464	369	294	345	372	292	246
439	204	385	353	340	272	301	541	349	354	197	292	422	291
559	560	225	253	429	275	544	265	260	286	405	340	344	255
501	358	534	366	353	563	396	636	283	324	416	358	298	204
174	299	373	244	286	327	325	239	300	206	335	224	363	277
224	269	209	298	204	359	350	258	307	273	351	471	356	209
203	455	238	280	220	218	308	208	314	339	241	307	307	190
270	259	378	226	374	365	321	261	320	445	425	204	311	298
283	311	218	201	446	182	442	360	292	376	275	420	285	190
217	367	300	347	180	353	360	334	379	404	388	483	364	221
212	254	225	199	230	143	353	405	340	328	238	411	335	191
214	217	160	158	263	162	195	302	229	159	199	215	339	246
126	166	300	313	193	230	137	309	187	124	224	238	2Q7	294
13
14
1	2	3	4	5	6	7	8	9	10 11 12 13 14
Row index of Patch
(c) Required Attack Iterations on ResNet50
13
14
1	2	3	4	5	6	7	8	9	10 11 12 13 14
Row index of Patch
(d) Required Attack Iterations on DeiT-small
Figure 14: Convergence of Adversarial Patch Attack on Different Architectures
16
Under review as a conference paper at ICLR 2022
I More Settings and Visualization of Adversarial Examples
with Imperceptible Noise
In the standard adversarial attack, the artificial noise can be placed anywhere in the image. In our
adversarial patch attack, we conduct experiments with different patch sizes, which are multiple times
the size of a single patch. The robust accuracy under different attack patch sizes is reported in Tab. .
We can observe that DeiT is more vulnerable than ResNet under imperceptible attacks.
Model	Patch-Size=16	Patch-Size=32	Patch-Size=112	Patch-Size=224
ResNet50	2.9	20.9	98.3	100
DeiT-small	4.1	38.7	100	100
ResNet18	3.1	26.0	99.1	100
DeiT-tiny	11.2	46.8	100	100
Table 10: Adversarial Patch Attack with Imperceptible Perturbation . FRs are reported in percentage.
The clean images and the adversarial images created on different models are shown in Fig. 15. The
adversarial examples created with imperceptible patch attack are not quasi-distinguishable from the
corresponding clean images for human vision.
(a) Clean Images
(b) Adversarial Examples on ResNet18
(c) Adversarial Examples on DeiT-tiny
(d) Adversarial Examples on ResNet50
(e) Adversarial Examples on DeiT-small
Figure 15: Visualization of Adversarial Examples with Imperceptible Patch Noise: The adversarial
images with patch noise of size 112 in the left-upper corner of the image are visualized. Please
Zoom in to find the subtle difference.
17
Under review as a conference paper at ICLR 2022
J Visualization of Adversarial Patch Noise
Besides reporting the FRs, we also visualize the adversarial patch noises created on ResNet and
DeiT. The adversarial patch noise created with Equation (??) are shown in Fig. 16a and 16c. We are
not able to recognize any object in the target class.
Following (Karmon et al., 2018), we enhance the attack algorithm where we place the patch noise
on different patch positions in different images in each attack iteration. From the visualization of
the created noise in Fig. 16b and 16d, we can recognize the object/object parts of the target class on
both ResNet and DeiT. In this appendix section, we conclude that the recognizability of adversarial
patch noise is dependent more on attack algorithms than the model architectures.
baseball c-cκk iP©d j⅞g□ar FrOg volcano
b-aseball cαck iPod j⅞gu⅞r Frog VGkanc
(a) Patch Noise on ResNet50 under the 1st Setting
baseball ∙c∙otk iPcιd j⅞g□sr FFog volcano
(c) Patch Noise on DeiT-small under the 1st Setting
Figure 16: Visualization of Adversarial Patch Perturbations under different Settings: In the 1st
setting, the patch noise is created to fool a single classification in a given patch position. The goal
in the 2nd setting to mislead the classifications of a set of images at all patch positions.
(b) Patch Noise on ResNet50 under the 2nd Setting
baseball cock iPod j⅞guar Frog volcano
(d) Patch Noise on DeiT-small under the 2nd Setting
18
Under review as a conference paper at ICLR 2022

KIOf ∙
Figure 17: Gradient Visualization on DeiT-small with Attack Patch size of 32
19
Under review as a conference paper at ICLR 2022
Figure 18: Gradient Visualization on ResNet50 with Attack Patch size of 32
■. ■
20
Under review as a conference paper at ICLR 2022

Figure 19: Gradient Visualization on DeiT-tiny with Attack Patch size of 32
21
Under review as a conference paper at ICLR 2022
Wl
Figure 20: Gradient Visualization on ResNet18 with Attack Patch size of 32
22
Under review as a conference paper at ICLR 2022
Figure 21: Rollout Attention on DeiT-small with Attack Patch size of 32 on Adversarial Images
23
Under review as a conference paper at ICLR 2022
Figure 22: Averaged Feature Maps of ResNet50 as Attention with Attack Patch size of 32 on Ad-
versarial Images
24
Under review as a conference paper at ICLR 2022
Figure 23: Rollout Attention on DeiT-small with Attack Patch size of 16 on Adversarial Images
25
Under review as a conference paper at ICLR 2022
Figure 24: Averaged Feature Maps of ResNet50 as Attention with Attack Patch size of 16 on Ad-
versarial Images
26
Under review as a conference paper at ICLR 2022
Figure 25: Rollout Attention on DeiT-small with Attack Patch size of 32 on Corrupted Images
27
Under review as a conference paper at ICLR 2022
Figure 26: Averaged Feature Maps of ResNet50 as Attention with Attack Patch size of 32 on Cor-
rupted Images
28
Under review as a conference paper at ICLR 2022
Figure 27: Rollout Attention on DeiT-small with Attack Patch size of 16 on Corrupted Images
29
Under review as a conference paper at ICLR 2022
Figure 28: Averaged Feature Maps of ResNet50 as Attention with Attack Patch size of 16 on Cor-
rupted Images
30