Under review as a conference paper at ICLR 2022
Group-disentangled Representation Learning
with Weakly-Supervised Regularization
Anonymous authors
Paper under double-blind review
Ab stract
Learning interpretable and human-controllable representations that uncover fac-
tors of variation in data remains an ongoing key challenge in representation learn-
ing. We investigate learning group-disentangled representations for groups of fac-
tors with weak supervision. Existing techniques to address this challenge merely
constrain the approximate posterior by averaging over observations of a shared
group. As a result, observations with a common set of variations are encoded to
distinct latent representations, reducing their capacity to disentangle and general-
ize to downstream tasks. In contrast to previous works, we propose GroupVAE, a
simple yet effective Kullback-Leibler (KL) divergence-based regularization across
shared latent representations to enforce consistent and disentangled representa-
tions. We conduct a thorough evaluation and demonstrate that our GroupVAE
significantly improves group disentanglement. Further, we demonstrate that learn-
ing group-disentangled representations improve upon downstream tasks, includ-
ing fair classification and 3D shape-related tasks such as reconstruction, classifi-
cation, and transfer learning, and is competitive to supervised methods.
1 Introduction
Decomposing data into disjoint independent factors of variations, i.e., learning disentangled repre-
sentations, is essential for interpretable and controllable machine learning (Bengio et al., 2013).
Recent works have shown that disentangled representation is useful for abstract reasoning (van
Steenkiste et al., 2019), fairness (Locatello et al., 2019a; Creager et al., 2019), reinforcement learn-
ing (Higgins et al., 2017b) and general predictive performance (Locatello et al., 2019b). While there
is no consensus on the definition of disentanglement, existing works define it as learning to sepa-
rate all factors of variation in the data (Bengio et al., 2013). According to this definition, altering a
single underlying factor of variation should only affect a single factor in the learned representation.
However, works in learning disentangled representations (Higgins et al. (2017a); Chen et al. (2018);
Locatello et al. (2019b)) have shown that this setting comes with a trade-off between the precision
of the representation and the fidelity of the samples. Therefore, learning precise representations for
finer factors, i.e., each factor of variation, may not be practical or desirable. We deviate from this
stringent assumption to learn group-disentangled representations, in which a group might include
several factors of variation that can co-variate. For instance, groups of interest may be content, style,
or background. As a result, a change in one component might affect other variables in a group but
not on other groups.
We present GroupVAE, a Variational Autoencoder (VAE) based framework that leverages weak su-
pervision to learn group-disentangled representations. In particular, we use paired observations that
always share a group of factors. Existing group-disentangled approaches (Bouchacourt et al., 2018;
Hosoya, 2019) enforce disentangled group representations by using an average or product of ap-
proximate group posteriors. However, as group representations are dependent on the observations
used for the average or product, observations belonging to the same group may not be encoded to
the same latent representations. We address this inconsistency challenge by incorporating a sim-
ple but effective regularization based on the Kullback-Leibler (KL) divergence. Our idea builds
on maximizing the Evidence Lower Bound (ELBO) of the Variational Autoencoders (VAEs) while
minimizing the Kullback-Leibler (KL) divergence between the latent variables that correspond to
the group shared by the paired observations.
1
Under review as a conference paper at ICLR 2022
6 Sdno-6 pφ-IEqs
q*iM SlndU - palrod
encoding
decoding
(a) Model
KL
KL
(b) Generative	(c) Inference
Figure 1: GroupVAE’s architecture visualization and graphical model. We visualize the com-
plete model, including model weights in (a) as well as show the (b) generative and (c) inference parts
as graphical models. The model visualization shows two paired inputs, one pair sharing “style” and
the other sharing “content”. The KL minimization depends on the group g that is shared. For in-
stance, for input (x(1) , x0(1) , g(1) = “style”), GroupVAE objective only minimizes the KL between
the style latent variables. Shaded nodes denote observed quantities in (b) and (c), and unshaded
nodes represent unobserved (latent) variables. Dotted arrows represent minimizing the KL diver-
gence between variables during inference.
In summary, we make the following contributions:
1.	We propose a way of learning disentangled representations from paired observations that
employs KL regularization between the corresponding groups of latent variables.
2.	We propose Group Mutual Information Gap (group-MIG), a mutual information-based
metric for evaluating the effectiveness of group disentanglement methods.
3.	Through extensive evaluation, we show that our GroupVAE’s effectiveness on a wide range
of applications. Our evaluation shows significant improvement for group disentanglement,
fair facial attribute classification, and 3D shape-related tasks, including generation, classi-
fication, and transfer learning.
2	Background & Notation
Variational Autoencoder (VAE). Consider observations X = {x(1) , . . . , x(N) }, x(i) ∈ RD
sampled i.i.d. from distribution pX and latent variables z. A Variational Autoencoder (VAE) learns
the joint distribution p(x, z) = pθ(x∣z)p(z) where pθ(x|z) is the likelihood function of observa-
tions x given z, θ are the model parameters ofp and p(z) is the prior of the latent variable z. VAEs
are trained to maximize the evidence lower bound (ELBO) on the log-likelihood log p(x). This
objective averaged over the empirical distribution is given as
1N
L = N E (Eq[logp(x( )|z)] - KL(qφ(z∣χ( ))∣∣p(z)),	⑴
i=1
where qφ(z|x) denotes the learned approximate posterior, φ the variational parameters of q and KL
denotes the Kullback-Leibler (KL) divergence. VAEs (Kingma & Welling (2014)) are frequently
used for learning disentangled representations and serve as the basis of our approach.
Weakly-supervised group disentanglement. We assume the observations X = {x(1) , . . . , x(N) }
and the data generating process can be described by M distinct groups G = {g1 , . . . , gM}. Each
2
Under review as a conference paper at ICLR 2022
group splits X into disjoint partitions with arbitrary sizes. Each group consists of non-overlapping
sets of factors of variations. For example, images of 3D shapes (Burgess & Kim, 2018)1 can be
described through three groups: shape2, background3 and view. Without loss of generality, we
define two groups gC (content) and gS (style independent of content) to describe the generative and
inference process. We assume having paired observations (x, x0) for training in a weakly-supervised
setting. Each pair of observations shares the same group, i.e., in our case either content c ∈ gC or
style s ∈ gS. During inference, the exact values for content and style are unknown, but only that
(x, x0 ) share a certain group is known. For each observation x, we define two latent variables: zc
for content and zs for style. The goal for group-based disentanglement is that the representation for
the same group as close to each other to ensure consistency.
3 Learning Group-Disentangled Representations
In the following, we introduce GroupVAE, a deep generative model which learns disentangled rep-
resentations for each group of factors. For simplicity, we limit the formulation of GroupVAE to two
groups, content and style, although GroupVAE can be applied to any number of groups. This sec-
tion first describes the generative and inference model and then introduces our main contributions -
the KL regularization and inference scheme. We visualized the generative and inference model in
Figures 1b and 1c.
Inference and generative model. Our model uses paired observations (x, x0) in a weakly-
supervised setting. We sample x from the empirical data distribution pX and conditionally sample
x0 in an i.i.d. manner, so that x and x0 belong to the same group g, i.e.,
X 〜PX(x);	X0 必.p(x0∣x,g).	(2)
Given x, we define two latent variables, zc as content and zs as style variables. The data is explained
by the generative process:
P(Zc)= N(Zc；0,l); P(Zs)= N(Zs；0, I); p(x∣z) = Pθ (x∣Zc, Zs) = fθ (x; Zc, Zs).	(3)
Both zc and zs are assumed to be independent of each other and are sampled from a Normal dis-
tribution with zero mean and diagonal unit variance. fθ is a suitable likelihood function4 which is
parameterized by a deep neural network. The generative model shown in Figure 1b is also known as
the decoding part seen in Figure 1a.
To perform inference, we approximate the true posterior P(Z|X) with the factorized approximate
posterior qφ(Z∣x) = qφ(Zc∣x) ∙ qφ(Zs∣x) that uses a neural network to amortize the the variational
parameters. We specify the inference model as
qφ(ZcIX) = N(μφ,c(χ), diag(σ2,c(X))); qφ(ZsIX) = N(μφ,s(χ), diag([,s(X))), ⑷
where both approximate posteriors are assume to be a factorized Normal distributions with mean μφ
and diagonal covariance diag(σφ2 (X)) . The inference model is visualized as a graphical model in
Figure 1c and as the encoding part in Figure 1a. The generative and inference models visualized in
Figure 1 apply to X0 as well.
VAE objective for paired observation. Given paired observations (X, X0), the VAE framework
maximizes the Evidence Lower Bound (ELBO)
ELBO = Eq[logP(XIZ) + logP(X0IZ0)] -KL(q(ZIX)IIP(Z)) - KL(q(Z0IX0)IIP(Z0))
{^^^^^^^™
reconstruction losses
"^^^^~^^^{^^^^^^^^""^
KL between approximate
posterior and prior of z
、------------{------------}
KL between approximate
posterior and prior of z0
(5)
|
}
|
}
- LpairedVAE ,
1Samples are shown in Figure 1.
2The group shape contains factors such as shape category, shape size and shape color.
3The group background contains factors such as floor color, wall color.
4Suitable likelihood functions are, e.g., a Bernoulli likelihood for binary values or a Gaussian likelihood for
continuous values.
3
Under review as a conference paper at ICLR 2022
which consists of the reconstruction losses of the observations x and x0 (first two terms) and KL
divergence between approximate posterior q and prior p of the latent variables z and z0 (third and
fourth term). This is a straightforward application of the original ELBO in Equation (1) to two sets
of observations, x and x0.
KL regularization for group similarity. Rather than defining an average representation for
groups as in (Bouchacourt et al., 2018; Hosoya, 2019), we propose to enforce consistency between
the latent variables by minimizing KL divergence between the latent variables z=g and z=0 g . Here,
g denotes the group shared between observations x and x0 . z=g and z=0 g denote the corresponding
group variable, e.g., if x and x0 share group c then the corresponding latent variables are zc and zc0 .
Given paired observations x, x0 from the same group g, our objective is to minimize
LKLreg = KL(q(Z=g|X)IIq(Z=g|x0)).	⑹
The KL divergence has analytical solutions for Gaussian and Categorical approximate posteriors
and is unaffected by the number of shared observations. The analytical solutions can be found in
Appendix A.2.
GroupVAE objective and inference.
Given a paired observation (x, x0) in the
sharing group g, we combine the ELBO in
Equation (5) and our proposed KL regu-
larization in Equation (6). Our proposed
model, GroupVAE, has the following min-
imization objective
LGroupVAE = LpairedVAE + γLKLreg , (7)
where we treat the degree of regularization
γ as a hyperparameter. We propose an al-
ternating inference strategy to encourage
variation in both of the latent variables. If
we only utilize observations that belong to
one group, e.g., paired observations that
always share content, we can obtain a triv-
ial sOlUtiOn fOr the cOntent latent variable
Algorithm 1 GroupVAE Inference
1:	while training() do
2:	g(1),..., g(n) — getRandomGroups()
3:	X — getMiniBatch()
4:	X0 - getPairedObservation(X,g(1),...,g(n))
5:	# encode x(i)
6:	∀x(i) ∈ X : Z = (Zci, z(i))〜q(zCi, Zf)Ix⑶)
7:	# encode x(i)0
8:	∀x(i)0	∈ X0	: Z0	=	(Zc(i)0,Zs(i)0)
q(Zc(i)0, Zs(i)0|x(i)0)
9:	# calculate loss according to Equation (7)
10:	L — Pi LGrOUPVAE (x(i), x0(i) , Z⑴,Z0(i),g(i))
11:	# update gradient g and parameters (θ, φ)
12:	(gθ , gΦ) . ( d∂L , ∂L )
13:	(θ,Φ) — (θ,Φ) + α(gθ, gφ)
14:	end while
by encOding cOnstant latent variables. We OvercOme this cOllaPse by alternating the grOUP that the
ObservatiOns belOng tO dUring training. In ParticUlar, dUring inference we randOmly samPle a grOUP
g ∈ {C, S} and the Paired ObservatiOn (x, x0) accOrding tO grOUP g. We then minimize the KL di-
vergence Of the cOrresPOnding latent variable. The inference’s PseUdO cOde is shOwn in AlgOrithm 1.
3.1 Related Work
Unsupervised learning of disentangled representations. VariOUs regUlarizatiOn methOds fOr Un-
sUPervised disentangled rePresentatiOn learning have been Presented in existing wOrks (Higgins
et al., 2017a; Kim & Mnih, 2018; Chen et al., 2018). Even thOUgh UnsUPervised methOds have
shOwn PrOmising resUlts tO learn disentangled rePresentatiOns, LOcatellO et al. (2019b) shOwed in a
rigOrOUs stUdy that it is imPOssible tO disentangle factOrs Of variatiOns withOUt any sUPervisiOn Or
indUctive bias. Since then, there has been a shift tOwards weakly-sUPervised disentanglement learn-
ing. OUr wOrk fOllOws this stream Of wOrks and fOcUses On the weakly-sUPervised regime instead Of
an UnsUPervised One.
Weakly-supervised learning of disentangled representations. ShU et al. (2020) investigated dif-
ferent tyPes Of weak sUPervisiOn and PrOvided a theOretical framewOrk tO evalUate disentangled reP-
resentatiOns. LOcatellO et al. (2020) PrOPOsed tO disentangle grOUPs Of variatiOns with Only knOw-
ing the nUmber Of shared grOUPs which can be cOnsidered as a cOmPlementary cOmPOnent tO OUr
methOd. Similar tO OUr methOd, bOth these wOrks fOllOw a weakly-sUPervised setUP. HOwever, bOth
aPPrOaches fOcUs On the disentanglement Of fine-grained factOrs, whereas OUr fOcUs is tO disentangle
grOUPs. BefOre the cOncePt Of Paired ObservatiOns was cOined by ShU et al. (2020) as “match Pair-
ing”, it was already Used fOr geOmetry and aPPearance disentanglement (KOssaifi et al., 2018; Tran
4
Under review as a conference paper at ICLR 2022
et al., 2019) and group-based disentanglement (Bouchacourt et al., 2018; Hosoya, 2019). Closest to
our work is MLVAE (Bouchacourt et al., 2018) and GVAE (Hosoya, 2019). For group-disentangled
representations, MLVAE uses a product of approximate posteriors, whereas GVAE uses an empirical
average of the parameters of the approximate posteriors. A thorough analysis of both works is in
Appendix B. In contrast, we employ a simple and effective KL regularization that has no dependency
on the batch size.
Alignment between factors of variations and learned representations. Closely related to our
work and group-based disentanglement concepts are studies that learn specific latent variables cor-
responding to one or several factors of variations (or labels). Dupont (2018) used both continuous
and discrete latent variables to improve unsupervised disentanglement of mixed-type latent factors.
Creager et al. (2019) proposed to minimize the mutual information between the sensitive latent vari-
able and sensitive labels. Similarly, Klys et al. (2018) proposed to minimize Mutual Information
(MI) between the latent variable and a conditional subspace. Both works (Creager et al., 2019;
Klys et al., 2018) require either supervision, sensitive labels, or conditions to estimate the mutual
information, whereas we only use weak supervision for learning disentangled group representations.
Concurrent to our work, Sinha & Dieng (2021) proposed to use a KL regularization for learning a
VAE with representation that is consistent with augmented data. While Sinha & Dieng (2021) use
the KL regularization to enforce the encoding to be consistent with changes in the input, our goal is
to split the representation into subspaces that correspond to the different groups of variations.
4 Evaluation
Here, we evaluate our GroupVAE and compare itto existing approaches. We show that our approach
outperforms existing approaches for group-disentanglement and disentanglement on existing disen-
tanglement benchmarks. Within the context of evaluating group disentanglement, we propose a
MI-based evaluation metric to assess the degree of group disentanglement. Further, we demonstrate
that our approach is generic and can be applied to various applications, including fair classification
and 3D shape-related tasks (reconstruction, classification, and transfer learning).
4.1 Weakly-supervised group-disentanglement
Experimental settings. We used three standard
datasets on disentangled representation learning: 3D
Cars (Reed et al., 2014), 3D Shapes (Burgess & Kim,
2018) and dSprites (Matthey et al., 2017). Despite the
fact that these image datasets are synthetic, disentangling
the factors of variation remains a difficult and unresolved
task (Locatello et al., 2019b; 2020). We use Mutual
Information Gap (MIG) (Chen et al., 2018) and our
proposed metric group-MIG for quantitative evaluation
different approaches. We compare our model, Group-
VAE, to unsupervised methods (β-VAE (Higgins et al.,
2017a) and FactorVAE (Kim & Mnih, 2018)) as well
as weakly-supervised methods (AdaGVAE (Locatello
et al., 2019b), MLVAE (Bouchacourt et al., 2018), and
GVAE (Hosoya, 2019)). For all methods, we ran a
hyperparameter sweep varying regularization strength
for five different seeds. We report the median group-MIG
and MIG.
Figure 2: Example of failed content-
style disentanglement with high MIG.
The heatmap shows the MI of each pair
of factors and latent dimensions. Al-
though content and style have not been
separated in the corresponding latent di-
mensions, the MIG is still very high (=
0.76). In contrast, group-MIG considers
where the groups are captured, and thus,
the group-MIG is much lower (= 0.34).
group-MIG for evaluating group disentanglement. The Mutual Information Gap (MIG) (Chen
et al., 2018) is a commonly used evaluation metric for disentanglement. This metric measures the
normalized difference between the latent variable dimensions with highest and second-highest MI
values. The higher the MIG, the greater the degree of disentanglement is. However, MIG can
still be high if the style latent variable disentangles all factors of variation whereas the content
variable collapse to a constant value. An example of a failure in group disentanglement is shown
in Figure 2. Therefore, we introduce group-MIG, a metric based on MIG, which addresses this
5
Under review as a conference paper at ICLR 2022
Table 1: Quantitative disentanglement results. We report median group-MIG and median MIG
over five hyperparameter sweeps of different seeds (higher is better). Since the unsupervised ap-
proaches and AdaGVAE do not learn group disentangled representations, we cannot report group-
MIG for these groups and denote it with -. We highlight in bold the best results.
3D Cars	3D Shapes	dSprites
Type	Model
		group-MIG	MIG	group-MIG	MIG	group-MIG	MIG
unsup.	β-VAE	-	0.08	-	0.22	-	0.10
unsup.	FactorVAE	-	0.10	-	0.27	-	0.14
weakly-sup.	AdaGVAE	-	0.15	-	0.56	-	0.26
weakly-sup.	MLVAE	0.24	0.07	0.47	0.32	0.11	0.22
weakly-sup.	GVAE	0.27	0.08	0.45	0.31	0.14	0.21
weakly-sup.	GroupVAE (ours)	0.48	018	0.60	0.31	0.54	0.27
Z6=j gi) ,	(8)
issue and quantitatively estimates the mutual information between groups and corresponding latent
variables. We define group-MIG as
1 m 1
~maxι(z=g~; max 1 (Z=gi ； gi ) - max
m i=1 H(gi )	j 6=i
where m is the number of groups, gi is the ground truth group, andI (z;	gi) is an empirical estimate
of the MI between continuous variable z and gi . The values of group-MIG is small if the group
factors are not represented in the corresponding latent vectors, even though the factor is disentangled
within the other variables.
Group labeling. We define the following groups based on the fine-grained factors for each dataset:
•	dSprite:s gC = [shape, scale], gS = [orientation, x-position, y-position]
•	3D Shapes: gC = [obj. color, obj. size, obj. type], gS = [floor color, wall color, azimuth]
•	3D Cars: gC = [obj. type], gS = [elevation, azimuth]
Results. We consistently outperform weakly-supervised disentanglement models w.r.t. median
group-MIG over five hyperparameter sweeps of different seeds by at least 25% (3D Shapes). Fur-
ther, we also improve on disentanglement w.r.t. MIG for two out of three datasets (3D Cars,
dSprites). In addition, we show interpolation samples of MLVAE, GVAE, and GroupVAE5 for 3D
Shapes in Figure 3. Both MLVAE and GVAE are not able to capture azimuth in the latent represen-
tations. Moreover, GVAE encodes almost all factors into the style part and collapses to a constant
representation in the content part. The interpolations of GroupVAE show content and style disen-
tanglement, although some factors such as object size and type for 3D Shapes remain entangled. As
we assume that factors in a group can co-variate, this result is expected as object size and type are
in the same group.
4.2	Application to fair classification
We examine the problem of learning fair representations for classification problems as an applica-
tion of our method. In particular, we want to learn fair group representation in which members
of any (demographic) groups have an equal probability of being assigned to the positive predicted
class. Deep learning algorithms have been proven to be biased against specific demographic groups
or populations (Mehrabi et al., 2021). It is critical that classification models can produce accurate
predictions without discriminating against certain groups in high-stakes and safe-related applica-
tions. In this context, we propose to learn fair representations by learning two distinct groups of
representations: a predictive representation for evaluating the downstream task and a representation
to account for the sensitive factors, e.g., gender- or age-specific attributes. The latter representation
is solely utilized for training and not for downstream tasks.
Learning fair representations consist of a two-step optimization scheme. First, we train GroupVAE
with pairs of observations sharing either sensitive and non-sensitive attributes. Second, we train a
simple MLP for attribute classification using the non-sensitive mean representation. We measure
5We selected models with median group-MIG over five hyperparameter sweeps of different seeds.
6
Under review as a conference paper at ICLR 2022
1234567890
ZZZZZZZZZJ
IUSlUoqUoN UIφwoo
(a) MLVAE
Wall color
Floor color
Obj. color
WSlUOqUON UIφwoo
(b) GVAE
Floor color
Obj. color
Wall color
Obj. type/
azimuth
H⅛
z5 Lil J -- I ..
Z6」」=
J 用WT*
z；
z10 ⅛∙J ■』■」■
Obj. color/
obj. size
Obj. type/
obj. size
1234567890
ZZZZZZ Zzz
Z
WSlUOqUON WeuIo。
(c) GroupVAE (ours)
Figure 3: Interpolations of 3D Shapes. We show samples from our model GroupVAE and the
baseline models (MLVAE and GVAE) with median group-MIG over five hyperparameter sweeps.
For each subplot, we show random inputs (first column), its reconstructions (second column) and
reconstruction when interpolating the latent variables (remaining columns) of each latent dimension
(row-wise). The factors annotated on the right side are those with a high level of mutual information
(MI > 0.25). For all three models z1 to z5 is supposed to capture style (non-content) while z5 to z10
is supposed to capture content.
Table 2: Fair classification results on the test set of dSpriteUnfair and CelebA. We report test
accuracy and Demographic Parity (DP) for each sensitive attribute with an average of five experi-
ments. We report the standard error for all test accuracies, but leave out the standard error for all DP
results as they were < 0.002. We highlight in bold the best results. The column Fair learning refers
to whether a model uses any supervision during the fair representation learning phase. For the final
classification, all models use full supervision.
Fair learning	Model	Test acc. ↑	Demographic parity (DP) J∈ [0, 1]	
			“shape” J	“scale” J
X	MLP	99.07 ±0.06	0.007	0.008
X	CNN	99.04 ±0.05	0.002	0.002
✓ (supervised)	FFVAE	98.60 ±0.12	0.004	0.004
✓ (weakly-superv.)	GroupVAE	99.18 ±0.08	0.002	0.002
(a) Results for dSpritesUnfair predicting “x-position”.
Demographic Parity(DP) J∈ [0, 1]
Fair learning	Model	Test acc. ↑	“Male”	“Young”
X	MLP	97.89 ±0.01	0.99	0.99
X	CNN	98.46 ±0.03	0.95	0.93
✓ (supervised)	FFVAE	97.79 ±0.01	0.04	0.04
✓ (weakly-superv.)	GroupVAE	98.23 ±0.02	0.01	0.02
(b) Results for CelebA predicting “bald”.
Demographic parity (DP) J∈ [0, 1]
Fair learning	Model	Test acc. ↑	‘BigNose’	‘HeavyMakeup’	‘Male’	‘WearingLipstick’
X	MLP	77.24 ±0.29	0.09	0.15	0.06	0.04
X	CNN	79.90 ±0.06	0.11	0.15	0.03	0.06
✓ (supervised)	FFVAE	97.75 ±0.03	0.03	0.02	0.03	0.03
✓ (weakly-superv.)	GroupVAE	97.88 ±0.01	0.01	0.02	0.02	0.01
(c) Results for CelebA predicting “attractive”.
classification accuracy and Demographic Parity (DP). DP measures whether the predictive outcome
is independent of a sensitive attribute. A completely fair model would attain a DP value of 0.0,
whereas a biased model can have a DP up to 1.0. We compare against MLP and CNN baselines,
7
Under review as a conference paper at ICLR 2022
and FFVAE (Creager et al. (2019)) which learns fair representations by using a supervised loss on
the sensitive attributes and a total correlation loss. We used two datasets: dSpritesUnfair (Creager
et al. (2019); TraUbIe et al. (2020)) and CeIebA (LiU et al. (2015)). dSpritesUnfair is a modified
image dataset based on dSprites with binarized factors of variations and is sampled with shape and
x-position being highly correlated. For CelebA, an image dataset of celebrity faces with 40 binary
attribUte labels, we predict “bald” and “attractive” in two separate experiments. For predicting
“bald”, we Use the attribUtes “male” and “yoUng” as sensitive attribUtes whereas we Use the attribUtes
“BigNose”, “HeavyMakeUp”, “Male” and “WearingLipstick” as sensitive attribUtes for predicting
“attractive”. We argUe that these attribUtes have a weak correlation bUt a strong correlation with
the predictive attribUte. However, several CelebA attribUtes significantly correlate, making this a
difficUlt dataset for fairness classification. We refer to the Appendix C.2 for the detailed experimental
settings.
Results. We report the fair classification resUlts in Table 2. Overall, the resUlts in Table 2 show
that weakly-sUpervised fair representation learning (GroUpVAE) oUtperforms sUpervised fair repre-
sentation learning (FFVAE). FUrther, we either get competitive or even oUtperform the sUpervised
baselines (MLP, CNN). SUrprisingly, when evalUating dSpritesUnfair the demographic parity for all
models is relatively low, and the strong correlation between shape and x-position does not seem
to affect the classification. The test accUracy and DP of the sensitive attribUtes of all the competi-
tive models are very close to each other. Nevertheless, among all models, oUr method achieves the
highest test accUracy and lowest DPs. For predicting “bald” in CelebA, even thoUgh both MLP and
CNN baselines achieve high test accUracy, the DPs shows an extremely biased classification towards
gender-specific and male-specific attribUtes. In contrast, oUr GroUpVAE achieves the lowest DPs
bUt still attain competitive classification accUracy, i.e., second highest test accUracy after the CNN
performance. When predicting “attractive”, GroUpVAE decreases the bias of all sensitive attribUtes
and increases the test accUracy compared to all other models.
4.3	Application to 3D point cloud tasks
In addition to evalUating image datasets, we show experiments on 3D point cloUds for reconstrUction
and classification. We experimented with FoldingNet (Yang et al. (2018)), a deep aUtoencoder that
learns to reconstrUct 3D point cloUds in an UnsUpervised way. Unlike VAEs, the FoldingNet aU-
toencoder is deterministic and does not optimize the representation to be a probabilistic distribUtion.
Instead of converting the aUtoencoder into a VAE, we Use a similar approach as Ghosh et al. (2019).
We assUme the embedding of aUtoencoder to be Normal distribUted with constant variance. Given
this assUmption, the KL divergence between the corresponding embeddings redUces to a simple L2
regUlarization, and we can inject noise to regUlarize the decoding. We evalUate three tasks, 3D point
cloUd reconstrUction, classification, and transfer learning. We measUre the Chamfer Distance (CD)
and the Earth Mover’s Distance (EMD) to assess reconstrUction qUality and report accUracy to as-
sess classification and transfer learning performance. We compare to FoldingNet (UnsUpervised)
and DGCNN (sUpervised) (Wang et al. (2019)), a dynamic graph-based classification approach. For
assessing the transfer learning capability, we Use a linear SVM classifier on the extracted represen-
tation. We Used two datasets for training: FG3D (LiU et al. (2021)) and ShapeNetV2 (Chang et al.
(2015)). FG3D contains 24,730 shapes with annotations of basic categories (Airplane, Car, and
Chair) and fine-grained sUb-categories. ShapeNetV2 contains 51,127 shapes with annotations of 55
categories. For transfer learning, we also Use ModelNet40 (WU et al. (2015)).
Results. Table 3a shows that weakly-sUpervised training improves Upon 3D point cloUd recon-
strUction for both FG3D and ShapeNetV2. Table 3b shows the classification and transfer resUlts. OUr
approach GroUpFoldingNet improves point cloUd classification compared to the original FoldingNet
and is competitive with the sUpervised approach when training with FG3D. We oUtperform both sU-
pervised and UnsUpervised transfer learning performances when training with FG3D and evalUating
ShapeNetV2 and ModelNet40. We are competitive to the sUpervised approach when training with
ShapeNetV2 and evalUating on ModelNet40. In particUlar, the transfer learning performance with
FG3D as the training set highlights the capabilities of weakly-sUpervised groUp disentanglement as
it can learn 3D point cloUds of three classes and transfer it to ShapeNetV2, a large-scale dataset
with 55 classes. We also visUalize point cloUd reconstrUctions and interpolations of three different
classes Using oUr approach in FigUre 4. The reconstrUctions show that oUr approach is better than
8
Under review as a conference paper at ICLR 2022
Table 3: Evaluation of 3D point cloud reconstruction, classification, and transfer learning.
We report Chamfer Distance (CD) and Earth Mover Distance (EMD) for quality of reconstruction
and accuracy for classification and transfer learning tasks. Best results without full supervision are
highlighted in bold.
Reconstruction J
Type	Model	FG3D		ShapeNetV2	
		CD	EMD	CD	EMD
unsupervised	FoldingNet	0.9539	0.9340	2.9867	1.5576
weakly-superv.	GroupFoldingNet (ours)	0.7519	0.8191	2.6891	1.3009
(a) Reconstruction results for FG3D and ShapeNetV2.
Linear SVM ACC ↑
Type	Model	Training dataset	Test dataset	#classes	Test ACC ↑	ShapeNetV2	ModelNet40
supervised	DGCNN _ _		FG3D			FG3D		_ _ 3 _	_ 99.26			50.53 _ _	_ _74.25_ _
unsupervised	FoldingNet	FG3D	Fg3D	——3	—-98.27	8工45 ——	80:04 —
weakly-superv.	ours	FG3D	FG3D	3	98.57	87.24	81.39
supervised	_DGCNN		ShapeNetV2	ShapeNetV2	_ _55 _	_ _94.4		-	_ _90.02 _
unsupervised	FoldingNet	'-ShapeNetV2 一	^ ShapeNetVr	— 一55 --	—-81.51	-	8740 —
weakly-superv.	ours	ShapeNetV2	ShapeNetV2	55	82.62	-	89.97
(b) Classification and transfer learning of representations.
(a) Reconstructions.	(b) Interpolations between two different samples.
Figure 4: Qualitative samples of ShapeNetV2. We show reconstructions of FoldingNet and our
approach in (a) and show interpolations of our approach in (b).
FoldingNet in reconstructing finer details. Further, the interpolations show that our approach can
learn an interpretable representation.
5 Conclusion & discussion
We proposed a simple KL regularization for VAEs to enforce group disentanglement through weak
supervision. We empirically showed that our model outperforms existing approaches in group disen-
tanglement. Further, we demonstrated that learning group-disentangled representations outperforms
performance on fair image classification and 3D shape-related tasks (reconstruction, classification,
and transfer learning) and is even competitive to supervised approaches.
There are several possible directions for future work. In comparison to unsupervised representa-
tion learning, weakly-supervised learning, by definition, requires some weak form of supervision.
Although we only need knowledge of whether two observations share a specific group, this limits
the approach. Further, we require group labels for the entire dataset for training and evaluation.
For real-life applications, datasets may not be fully labeled, and performance may suffer under this
setting. Future investigation of group disentanglement in a low data or a “semi” weakly-supervised
regime can allow group disentanglement learning to transfer to large-scale and more realistic set-
tings. Another promising direction is investigating models with more than two groups. Even though
we chose to focus on applications with two groups in this work, our method can generalize to more
than two groups, which is a promising direction for future work.
9
Under review as a conference paper at ICLR 2022
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning a review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder:
Learning disentangled representations from grouped observations. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 32, 2018.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,
Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d
model repository. arXiv preprint arXiv:1512.03012, 2015.
Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montreal, Canada,pp. 2615-2625, 2018.
Elliot Creager, David Madras, Jom-Henrik Jacobsen, Marissa A Weis, Kevin Swersky, Toniann
Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. arXiv
preprint arXiv:1906.02589, 2019.
Emilien Dupont. Learning disentangled joint continuous and discrete representations. In Advances
in Neural Information Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2θl8, December 3-8, 2018, Montreal, Canada, pp. 708-718,
2018.
Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf. From
variational to deterministic autoencoders. arXiv preprint arXiv:1903.12436, 2019.
Irina Higgins, LoiC Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017a.
Irina Higgins, Arka Pal, Andrei A. Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: improving zero-shot
transfer in reinforcement learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1480-1490. PMLR,
2017b.
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the varia-
tional evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS,
2016.
Haruo Hosoya. Group-based learning of disentangled representations with generalizability for novel
contents. In IJCAI, pp. 2506-2513, 2019.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983,
2018.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Con-
ference on Learning Representations, ICLR 2014, Banff, Canada, April 14-16, 2014, Conference
Track Proceedings, 2014.
10
Under review as a conference paper at ICLR 2022
Jack Klys, Jake Snell, and Richard S. Zemel. Learning latent subspaces in variational autoencoders.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, NicoIo Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con-
ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montreal, Canada,pp. 6445-6455, 2018.
Jean Kossaifi, Linh Tran, Yannis Panagakis, and Maja Pantic. GAGAN: geometry-aware generative
adversarial networks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 878-887. IEEE Computer Society,
2018.
Zhuo Li, Hongwei Wang, Miao Zhao, Wenjie Li, and Minyi Guo. Deep representation-decoupling
neural networks for monaural music mixture separation. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 32, 2018.
Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Fine-grained 3d shape classification
with hierarchical part-view attentions. IEEE Transactions on Image Processing, 2021.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Scholkopf, and
Olivier Bachem. On the fairness of disentangled representations. In Advances in Neural Informa-
tion Processing Systems, pp. 14611-14624, 2019a.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning
of disentangled representations. In international conference on machine learning, pp. 4114-4124.
PMLR, 2019b.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and
Michael Tschannen. Weakly-supervised disentanglement without compromises. arXiv preprint
arXiv:2002.02886, 2020.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relax-
ation of discrete random variables. In 5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,
2017.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6):1-35, 2021.
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of
variation with manifold interaction. In International conference on machine learning, pp. 1431-
1439. PMLR, 2014.
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disen-
tanglement with guarantees. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Samarth Sinha and Adji B Dieng. Consistency regularization for variational auto-encoders. arXiv
preprint arXiv:2105.14859, 2021.
Linh Tran, Jean Kossaifi, Yannis Panagakis, and Maja Pantic. Disentangling geometry and appear-
ance with regularised geometry-aware generative adversarial networks. International Journal of
Computer Vision, 127(6):824-844, 2019.
Frederik Trauble, Elliot Creager, Niki Kilbertus, Anirudh Goyal, Francesco Locatello, Bernhard
Scholkopf, and Stefan Bauer. Is independence all you need? on the generalization of representa-
tions learned from correlated data. arXiv e-prints, pp. arXiv-2006, 2020.
11
Under review as a conference paper at ICLR 2022
Sjoerd van Steenkiste, Francesco Locatello, Jurgen Schmidhuber, and Olivier Bachem. Are dis-
entangled representations helpful for abstract visual reasoning? In Hanna M. Wallach, Hugo
Larochelle, Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp.
14222-14235, 2019.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):
1-12, 2019.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Foldingnet: Point cloud auto-encoder via
deep grid deformation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 206-215, 2018.
12
Under review as a conference paper at ICLR 2022
A	GROUPVAE
A.1 Joint Learning of Continuous and Discrete Groups
The generative model defined in the main Section 4 assumes both content and style representations
to be Gaussian distributed. However, many data-generating processes rely on discrete factors which
is usually difficult to capture with continuous variables. In these cases, we can define the generative
model as
p(zc) = Cat(π),	(9)
p(zs) =N(0,I),	(10)
p(x|zc, zs) = Bernoulli(fθ(zc, zs)).	(11)
For inference, we use a Gumbel-Softmax reparameterization Jang et al. (2017); Maddison et al.
(2017), a continuous distribution on the simplex that can approximate categorical samples for zc .
Similar to the KL divergence between two Normal distributions, the KL divergence between two
Categorical distributions can also be computed in closed form.
A.2 Closed-form Solutions for the KL Regularization
In the case of both Z 〜 N(μ,σ2) and z0 〜N(μ0, σ02) being factorized Gaussian distributions, the
KL regularization has the analytical solution
σ0	σ2 + (μ — μ0)2	1
KL(z∣∣z0) = log 厂 + —2στ^ — 2.	(12)
In the case of z Categorical(π) and z0 Categorical(π0), the KL has the analytical solution
KL(z∣∣z0) = X∏i log πi-.	(13)
i	πi0
B Analysis of Existing Group-Disentanglement Approaches
In this Section, we give further details about the content approximate posterior proposed by Boucha-
court et al. Bouchacourt et al. (2018), and Hosoya Hosoya (2019). Further, we analyze the proposed
approaches and show its limitations.
B.1 MLVAE AND GVAE
As described in Subsection 3, we restrict to two groups and define corresponding latent variables zc
and zs given observation x6. However, both works also apply to any number of groups. For paired
observations (x, x0) with shared group factor c, the loss objectives for MLVAE (Bouchacourt et al.
(2018)) and GVAE (Hosoya (2019)) are
LMLVAE =LpairedVAE — βKL(qφ(Zc, Zs |x) ∣∣p(z)) — βKL(qφ(Zc, Z：，∣X0)∣∣p(z)),	(14)
LGVAE =LpairedVAE — βKL(qφ(Zc, Z： |x) ∣∣p(z)) — βKL(qφ(Z., Z：，∣X0)∣∣p(z)).	(15)
The loss objectives LGVAE and LMLVAE are very similar. The only exceptions are the group approxi-
mate posteriors, Zc for LGVAE and Zc for Lgvae.
Bouchacourt et al. (2018) assume the group approximate posterior to be a product of the individual
approximate posteriors sharing the same group Zc
Zc 〜N(μφ,c(X), diag(σ2,c(X)) ∙N(μφ,c(XO), diag(σφ,c(XO))).	(16)
The product of two or more Normal distributions is Normal distributed, and thus the KL term can
still be calculated in closed-form.
Hosoya (2019) uses an empirical average over the parameters of the individual approximate poste-
riors. The group approximate posterior is defined as
zc 〜N(0.5 ∙ μφ,c(x) + 0.5 ∙ μφ,c(x0), 0.5 ∙ diag(σ2,c(x)) + 0.5 ∙ diag(σφ,c(x0))).	(17)
6In similar fashion, we define two latent variables zc0 and zs0 for observation x0.
13
Under review as a conference paper at ICLR 2022
floor color	0	0	0 0.02	α	0	0	0	0	0	-1.0
wall color	0	0.01	0.02 龌	0.04	0	0	0	0	0	-O-B
azimuth	0		0 0.02	0	0	0	0	0	0	-0.6
object color	0.01	0	0OQ5	0.01	0.05	0	0	0	0	-0.4
object size	0.06	0	0.060.01	0.01	0.1	0	0	0	0	-0.2
object type	0.01	0.01	0.010.01	0.01		0	0	0	0	-0.0
Zl Z2 Z3 Z4 Z5 ⅞ Z7 Z8 Z9 Z10
(a) 3DShapes: MI between latent dimensions
and factors of variation of a trained GVAE
model with MIG = 0.55 and group-MIG =
0.44.
0.5
-0.4
c
αj
103
u
a
∑0∙2
O.
ɛ 0.1
6
0.0
0.0	0.1	0.2	0.3	0.4	0.5
group-M∣G (non-content)
(c) dSprites: MIG w.r.t. dif-
ferent number of shared ob-
servations for MLVAE and
GVAE.
(b) dSprites: group-MIG of
content and style information
for all hyperparameter runs.
Figure 5: Collapse and sensitivity of existing weakly supervised group disentanglement models.
(a) shows mutual information (MI, higher is better) for a GVAE model trained on 3DShapes. (b)
plots both group-MIG (higher is better) w.r.t. content and style information trained on dSprites. (c)
plots MIG (higher is better) w.r.t. number of shared observations.
B.2	Analysis
Both MLVAE and GVAE enforce disentanglement through the β-regularization in the last two terms
of Equations (14, 15). This regularization was also used in β-VAE Higgins et al. (2017a) which
regularizes a trade-off between disentanglement and reconstruction. The two KL terms in Equations
(14, 15) can be decomposed similar to the ELBO and KL decomposition in Hoffman & Johnson
(2016); Chen et al. (2018). We consider the objective in Equation (14) averaged over the empirical
distribution p(n). Each training sample denoted by a unique index and treated as random variable
n. We simplify q(z|xn) = q(z|n) and refer to q(z) = PiN q(z|n)p(n) as the aggregated posterior
Hoffman & Johnson (2016). We can decompose the first KL in Equation (14)7 as
Ep(n)
KL(q(Zc, Zs∣n)l∣p(z))
KL(q(Zc, Zs,n)∖∖q(Zc, Zs)p(n)) + EKL(q(Zj)∣∣p(Zj))
j
+ KL(q(Zc)∖∖ Y q(Zcj)) + KL(q(Zs∖∖ Y q(Zsj))),
jj
、----------V----------} 、-----------V----------}
content total correlation	style total correlation
(18)
where q(z) = q(zc, Zs) = q(zc) ∙ q(zs). We ShoW the full derivation in the next Subsection B.3.
Minimizing the averaged KL between the content and style latent variables (q(Zc, Zs∖n)) and the
prior p(Z ) also leads to minimization of the total correlation of content variables and style variables
(the last two terms in Equation (18)). The total correlation quantifies the amount of information
shared between multiple random variables, i.e., low total correlation indicates high independence
between the variables. Even though this objective motivates disentangled content and style repre-
sentations, the group representation depends on the number of samples used for the averaging. Fur-
ther, both Bouchacourt et al. (2018) and Hosoya (2019) only average over the content group. There
are no structural nor optimization constraints that prevent the style latent variable from encoding all
factors of variation.
Sensitivity to group batch size. MLVAE and GVAE use different types of averaging over group
latent variables. In realistic settings, always having a certain number of observations that share
the same group variations can be difficult. For instance, when training MLVAE and GVAE with
dSprites, the performance and its variance is correlated with the number of shared observations. We
visualized these findings in Figure 5 (c).
7We can decompose the KL of GVAE in Equation (15) similarly.
14
Under review as a conference paper at ICLR 2022
Visualization of collapse. We visualize such behavior in Figure 5 (a) on a GVAE model trained
on 3DShapes with two groups of variations c ={object color, object size and object type} and s =
{floor color, wall color, azimuth}. Ideally, z1 - z5 contains high mutual information with group
factors s and z6 - z10 contains high mutual information with group factors c. However, most
information is captured in z1 - z5, whereas only a little information about object type is contained
in z6 - z10.
B.3	KL Decomposition
Here, we show the full derivation for Equation (18). For a given group g the KL decompose as
follows:
Ep(n) ∣KL(q(Zc, Zs∣n)∣∣p(z))
Ep(n) ∣Eq(Zc,Zs |n) [ log q(Zc, Zs |n) - log P(Z)]
Ep(n) ∣Eq(Zc,Zs∣n)[log q(z°, zs|n) - logp(z) + ∣ogq(z°, Zs) - logq(zC, Zs)
=0
+ log ∏ q(Zi)- log[∏ q(zc,i) ∏ q(zs,j)]
i	ij
X--------------------7--------------------Z
=0
Eq(Zc,Zs,n j log "用 + Eq(Zc,Zs) [ log Q Fs)、1
q(	)[	q(zc, zs)」q( c, s)L ∏i q(Zc,i)∏j∙q(ZsjH
+ Eq(∕ log 口 1
p(Z )
Eq(Z …)]log ⅛⅛Pf ] + Eq(-)[log rfq⅛!
+Eq(ZjlOg ∏⅛b ] +Eq(Z) [χlog %]
(19)
(20)
(21)
(22)
(23)
KL(q(Zc, Zs,n) l∣q(Zc, Zs )p(n)),+KL(q(Zc)ll ∏q(Zc,i)) + KL(q(Zsll ∏q(Zs,i)))
'	一，-	J	i	i
Index-code MI
x----------------7-----------------Z '-------------------7-----------------Z
content total correlation	style total correlation
+ EKL(q(Zi)IIp(Zi)),
i
、----------{---------Z
Dimension-wise KL
(24)
where p(n) denote the empirical data distribution.
C Experimental Setup
C.1 Disentanglement Study
All hyperparameters for optimization and model architectures are listed in Table 4. We compare our
approach, GroupVAE, to four different models: β-VAE Higgins et al. (2017a), AdaGVAE Locatello
et al. (2020), MLVAE Bouchacourt et al. (2018) and GVAE Hosoya (2019). To fairly compare all
models, we used the same architecture and optimization settings for all models and only varied the
range of the regularization. We ran five experiments for every hyperparameter set with different
random seeds (= [0, 1, 2, 3, 4]). In total, we ran 240 experiments. Each experiment ran on GPU
clusters consisting of Nvidia V100 or RTX 6000 for approximately 2-3 hours.
Datasets and group sampling. We evaluated our approach on three datasets: 3D Cars Reed et al.
(2014), 3D Shapes Burgess & Kim (2018) and dSprites Matthey et al. (2017). All datasets contain
15
Under review as a conference paper at ICLR 2022
Parameters	Values	Architecture
Batch size	64	qφ(z∣x) Conv 32 X 4 X 4 (Stride 2), ReLU act.,
Latent dimension	10	Conv 32 × 4 × 4 (Stride 2), ReLU act.,
Optimizer	Adam	Conv 64 X 4 X 4 (Stride 2), ReLU act.,
Adam: beta1	0.9	Conv 64 X 4 X 4 (Stride 2), ReLU act.,
Adam: beta2	0.999	FC 256, ReLU act., FC 2 X 10
Adam: epsilon	1e-8	Pθ(x|z) FC 1024, ReLU act., Reshape (64, 4, 4),
Adam: learning rate	5e-4	TransposeConv 64 X 4 X 4 (Stride 2), ReLU act.,
Training iterations	300,000	TransposeConv 32 X 4 X 4 (Stride 2), ReLU act.,
—		TransposeConv 32 X 4 X 4 (Stride 2), ReLU act.,
		TransposeConv 3 X 4 X 4 (Stride 2)
(a) Common hyperparameters.	(b) Common model architectures.
Model	Parameter	Values
β-VAE Higgins et al.(2017a)	β	[1,2,4,6,8,16]
AdaGVAE Locatello et al. (2020)	β	[1,2,4,6,8,16]
MLVAE Bouchacourt et al. (2018)	β	[1,2,4,6,8,16]
GVAE Hosoya (2019)	β	[1,2,4,6,8,16]
GroupVAE	λ	[1, 2, 8, 16, 32, 64]
(c) Model hyperparameters.
Table 4: Experimental setup for the disentanglement study. We list hyperparameters, model
architectures and hyperparamter common to the disentanglement study.
images of size 64 × 64 with pixels normalized between 0 and 1. For training, given observations x
and groups g1 , . . . , gm , we sample uniformly g from all groups and the observation x0 uniform from
all observations which share the same group values as x.
Evaluating disentanglement. In addition to comparing group disentanglement, we also used MIG
Chen et al. (2018) to compare the models’ ability to disentangle all factors of variation. Chen et al.
(2018) proposed MIG as an unbiased and hyperparameter-dependent evaluation metric to measure
the mutual information between each ground truth factor and each dimension in the computed repre-
sentation. The MIG is calculated as the average difference between the highest and second-highest
normalized mutual information of each factor. The score is computed as
1K 1
MIG = KfH(Q (In(Zj(k ； V) - maX)In(Zj Vk )),
(25)
where j(k) = arg maxj In(Zj ； Vk) andK is the number of known factors.
C.2 Fairness
We ran five experiments for every hyperparameter set with different random seeds (= [0, 1, 2, 3, 4]).
In total, we ran 550 experiments. Each experiment ran on GPU clusters consisting of Nvidia V100
or RTX 6000 for approximately 2-3 hours.
Models For the fair classification experiments we used the same common hyperparameters and
model architecture as in the disentanglement studies (Table 4 (a) and (b)) for GroupVAE, GVAE
and MLVAE. In addition, we implemented two simple baselines, an Multi-Layer Perceptron (MLP)
and a Convolutional Neural Network (CNN). The architecture for these two models are described in
Table 5. For the supervised fair classification, we implemented FFVAE Creager et al. (2019) with
the same encoder and decoder networks as in Table 4 (b) and the FFVAE discriminator as in Table 5.
The baselines are trained with a cross-entropy loss between the logits of the network and the binary
label “HeavyMakeup”. We used different number of latent dimensions which is shown in Table 5
(c).
16
Under review as a conference paper at ICLR 2022
Architecture
FFVAE discriminator FC 1000,LeakyReLU(0.2)act., FC 1000,LeakyReLU(0.2)act.,
FC 1000, LeakyReLU(0.2) act., FC 1000, LeakyReLU(0.2) act.,
FC 1000, LeakyReLU(0.2) act., FC 2
fMLP	FC 128, ReLU act., FC 128, ReLU act., FC 128, FC 2
fCNN	Conv 1 × 6 × 5, ReLU act., MaxPool
Conv 6 × 16 × 5, ReLU act., MaxPool, FC 120,
ReLU act., FC 84, ReLU act., FC 2
(a) Additional model architecture.
—			Dataset	Parameters	Values
Model	Parameter	Values	CelebA	Latent dimensions	[[3, 37], [40, 40]]
FFVAE	α	[0,1,100,300,500,1000]		[sensitive, non-sensitive]	
	γ	[10, 20, 30, 40, 50, 100]	dSpritesUnfair	Latent dimensions [sensitive, non-sensitive]	[[5, 5]]
(b) Additional model hyperparameter.
(c) Dataset-specific hyperparameters.
Table 5: Experimental settings for fair classification. We list hyperparameters of FFVAE and the
MLP and CNN baselines.
Sensitive and non-sensitive latent variables. Similar to the content and style disentanglement
setup, we define two groups, sensitive and non-sensitive. GroupVAE can be optimized to learn from
weakly supervised observations sharing either sensitive or non-sensitive group values. FFVAE Crea-
ger et al. (2019) can be seen as the supervised approach of learning sensitive and non-sensitive repre-
sentations. FFVAE maximizes the ELBO objective (reconstruction loss and KL divergence between
approximate posterior and prior). In addition, the objective regularizes the discriminative ability of
the sensitive latent variable with α in a supervised manner (how well can the model classify sensi-
tive labels from sensitive latent variable?) and the disentanglement with γ (how well is the sensitive
latent variable disentangled from the non-sensitive latent variable?).
Datasets. For comparability with FFVAE Creager et al. (2019), we used similar dataset settings
for CelebA Li et al. (2018) and dSpritesUnfair. Both datasets contain images with pixels normalized
between 0 and 1. We used the pre-defined train, validation, and test split of CelebA Li et al. (2018),
whereas in dSpritesUnfair we use a random split of 80% train, 5% validation, and 15% test.
dSpritesUnfair. dSpritesUnfair is a modified version of dSprites Matthey et al. (2017). The two
components are the binarization of the factors of variation and biased sampling. dSprites contains
images which are described by five factors of variation. We binarized the factors of variations
following these criterion Creager et al. (2019):
•	Shape ≥ 1
•	Scale ≥ 3
•	Rotation ≥ 20
•	X-position ≥ 16
•	Y-position ≥ 16
Similar to TraUble et al. (2020), We enforce correlations between shape and X-Position through a
biased sampling. In the training set, we sample these two factors from a joint distribution
p(s, x) IX exp( - (S- X) ),	(26)
2σ2
where σ determines the strength of the.correlation and is set to σ = 0.2 in our experiments. The
smaller σ, the higher the correlation between the two factors.
Model selection. As shown in Creager et al. (2019), there is a trade-off between classification
accuracy and demographic parity. Thus, model selection based on only one of these metrics com-
promises the other. We propose to use the difference between the two metrics as a way to do model
17
Under review as a conference paper at ICLR 2022
selection. We coin this metric FairGap (FG) and define it as
FG = Ey=ɪ}-I1I
Accuracy
Ey1叫y = 1|a = 1] - E[y = 1|a = 0]|.
V1----------------L--------------'
demographic parity
(27)
FG is high if accuracy is high and the average demographic is low, resulting in a fair classifier. We
select the model on the test set of CelebA and dSprites based on the FG of the validation set.
18