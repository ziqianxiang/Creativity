Under review as a conference paper at ICLR 2022
Strongly Self-Normalizing Neural Networks
with Applications to Implicit Representation
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Recent studies have show that wide neural networks with orthogonal linear lay-
ers and Gaussian Poincare normalized activation functions avoid vanishing and
exploding gradients for input vectors with the correct magnitude. This paper
introduces a strengthening of the condition that the activation function must be
Gaussian Poincare normalized which creates robustness to deviations from stan-
dard normal distribution in the pre-activations, thereby reducing the dependence
on the requirement that the network is wide and that the input vector has the cor-
rect magnitude. In implicit representation learning this allows the training of deep
networks of this type where the linear layers are no longer constrained to be or-
thogonal linear transformations. Networks of this type can be fitted to a reference
image to 1/10th the mean square error achievable with previous methods. Herein
is also given an improved positional encoding for implicit representation learning
of two-dimensional images and a small-batch training procedure for fitting of neu-
ral networks to images which allows fitting in fewer epochs, leading to substantial
improvement in training time.
1	Introduction
Sitzmann et al. (2020) have proposed the use of periodic activation functions, specifically the use
of the sine in the fitting of neural networks to, for example, images and signed distance functions.
They argue that if the weights are uniformly distributed on [-c/ √f, c/ √f] where C = √6 and f
is the fan in, then the pre-activations are approximately standard normal distributed irrespective of
the depth of the network and that the derivatives of these networks are networks of the same type
(SIRENs, or sinusoidal representation networks), ensuring that these guarantees are applicable also
to derivatives of networks of this type.
This is not the case (see appendix A): if the pre-activations were standard normal distributed, then
the activations would have mean zero and variance 2(1 - e-2) ≈ 0.43 and the pre-activations of
the next layer mean zero and variance 1 c2(1 — e-2) = 1 — e-6 ≈ 0.86.
Furthermore, this provides no guarantees about the derivatives with respect to any parameter.
Such guarantees can be obtained by using higher values of c, indeed, with sufficiently high values
of c it can be assured (see appendix A) that if the pre-activations are N (0, c2/6)-distributed then
the pre-activations of the next layer will be as well and that if the gradient with respect to one
layer has become N (0, c2/6)-distributed, then the pre-activation of the layer before it will also be
N(0, c2/6)-distributed.
When training networks with large constant learning rates it is not immediately apparent that these
guarantees are of any benefit, in appendix A there are training curves from which it is apparent that
SIREN networks with C = √6 learn faster than when larger values of C are used during the portion
of training before learning rates are reduced.
However, when appropriate training procedures are used it is possible to benefit from the guarantees
obtainable in the case of SIREN networks with larger C. It is possible to fit a SIREN with width 256
and five hidden layers to the 512 × 512 Cameraman test image used for the same purpose inSitzmann
1
Under review as a conference paper at ICLR 2022
et al. (2020) and achieve a PSNR of 57.5, provided that an initial learning rate of 5 ∙ 10-4 is used
with a learning rate schedule involving the halving of the learning rate when the MSE plateaus for
60 epochs. SIRENs with C = √6 can with the same training method achieve PSNR 52.99. The
method used bySitzmann et al. (2020) for fitting SIRENs to images involves a constant learning rate
of 110-4 and 15,000 epochs and full-batch training and achieves a PSNR of approximately 50.
Lu et al. (2020) have proposed a type of neural network, bidirectionally self-normalizing neu-
ral networks (BSNNs), for which they prove similar guarantees as those available for large-c
SIRENs: provided that the layers are orthogonal linear transformations that are uniformly distributed
on the orthogonal group in the Haar sense followed by activation functions that are Gaussian-
Poincare normalized, meaning that the activation function f satisfies Ez~n(0,1)[f (z)2] = 1 and
Ez~N(0,1)[ f (z)2] = 1; f and its derivative are Lipschitz continuous, and the input vector is thin-
shell concentrated in the sense that for all > 0 P{lnlkxk2 - 1| >	→ 0 as n → ∞ and that
the layers are wide it can assured that the squared magnitude of the input to each layer is n and the
magnitude of the derivative of any loss function E with respect to the input to any layer is the same.
The guarantee that a thin-shell concentrated vector has its norm preserved under forward propagation
in a BSNN is comparable to the guarantee that in a SIREN with sufficiently large c the pre-activations
all have the distribution, while the guarantee that the derivative of a loss function with respect to
the input to any layer always has the same magnitude corresponds to the assurance provided in
appendix A for SIRENs with sufficient large c that the backward propagation, once the gradient is
approximately N(0, c2/6)-distributed the gradient to earlier layer will be as well.
We consider a condition under which a BSNN with orthogonal initialization but no orthogonal-
ity constraint performs well in implicit representation learning. Among the BSNNs satisfying this
condition is one all of whose derivatives are networks of the same type.
2	Motivation
In a BSNN of some fixed finite width the pre-activations are uniformly distributed on the sphere
with radius equal to the square root of the dimension, which in high dimension in a certain sense
closely approximates the normal distribution so that the trainability guarantees of (Lu et al., 2020)
can be obtained about the distribution of the activations even though the condition they impose is on
the expectation of the activation function and its derivatives applied to a standard normal distributed
random variable.
It is possible to impose a stronger condition, that Ez~w[f (z)2] = Ez~w [ f (z)2] = 1 for all
distributions W such that W = -W. This is the case precisely if f (x)2 - 1 and f (x)2 - 1 are odd
functions.
We consider the ideal case where vectors with magnitude exactly equal to the square root of di-
mension are forward propagated through neural networks of different width with different activation
functions, all of which are such that Ez~n(0,1)[f (Z)2] = 1 and some such that f (x)2 - 1 is an odd
function.
In fig. 1. a measure of deviation of the magnitude of the pre-activations of BSNNs from the square
root of the dimension is calculated for different activation functions and network widths. The ac-
tivation functions giving the the two lowest values of this measure measure (the curves marked
gold and black) both satisfy the stronger condition (although that achieving the lowest average,
,2/(1 + e-x) without satisfying the part of the Gaussian Poincare normalization condition that
concerns the derivativ so that the guarantees ofLu et al. (2020) with regard to backward propagation
do not hold and so that that function is not usable for our purposes).
Had we looked, instead of at deviations of the magntiude from the square root the dimension, at
ratios of magnitude of output layers to input layers, we would see no benefit of activation functions
satisfying the stronger condition.
What the stronger condition assures is not that the magnitudes are preserved more exactly, but that
the magnitude of a vector whose magnitude deviates from the square root of the dimension is brought
closer to that magnitude. This can be see in fig. 2. showing a measure of the deviation of the
magntiude of the pre-activations from the square root of the dimension where the initial input vector
2
Under review as a conference paper at ICLR 2022
0	100	200	300	400	500
layer width
Figure 1: Log plot of Pk | kxknk - 1| where Xk is the pre activations over 400 layers, averaged over
ten runs, as a function of the network width. Gold: ,2/(1 + e-x), Black: √2sin(x + n/4), Red:
GP normalized tanh, Turquoise: GP normalized GELU, Blue: GP normalized ELU, Purple: GP
normalized leaky ReLU, Green: GP normalized ReLU.
1234567
layer
L P100 kχ(l)k	(l)
Figure 2: | 100 √=1	- 1| for k=1,..,7 where x『 are the pre-activations for layer k during
forward propagation of x(0l) and x(0l) is a random normal vector transformed by normalization
to have norm 空 where n is the dimension. Colour corresponds to activation function. Gold:
,2/(1 + e-x), Black: √2sin(x + n/4), Red: GP normalized tanh, Turquoise: GP normalized
GELU, Blue: GP normalized ELU, Purple: GP normalized leaky ReLU, Green: GP normalized
ReLU.
magnitude of 1 the square root of the dimension. For some of the activation functions the deviation
tends to zero, but in the case of the activation function satisfying the stronger condition, they are
zero immediately.
We will be concerned with relaxations of networks of the following type:
Definition 1 (Strongly bidirectionally self-normalizing neural networks). A neural network is said
to be a SBSNN if it has orthogonal weight matrices that are uniformly distributed in the Haar sense
3
Under review as a conference paper at ICLR 2022
and the activation is differentiable, Lipschitz continuous with Lipschitz continuous derivative and
has the property that f (x)2 — 1 and f (x)2 — 1 are odd functions.
3	Characterization of SBSNNs
Lemma 1. Iff(x)2 — 1 and f (x)2 — 1 are odd functions then there is an even function W taking
values in {1, —1} such that f (x)2 — 1 = sin(2 0x w(s)ds).
Proof. Let u(x) = f (x)2 — 1 and v(x) = f (x)2 — 1. These are then odd functions.
Because v(χ) = —v(—x) we see thatf (x)2 — 1 = —(f (—x)2 — 1) so that f (x)2 + f (—x)2 = 2.
Also, dχ(x) = 2f (x)f (x) so that dχ(x)2 = 4f (x)2 f (x)2. Because U(X) = f (x)2 — 1 we can
write f (x)2 = U(X) + 1 and in turn that 需(x)2 = 4(u(x) + 1)f (x)2.
We now multiply f (x)2 + f(—x)2 = 2 by 4(U(X) + 1) f (x)2 ∙ 4(u(—x) + 1) f (—x)2 and
simplifying using the definition of 需 we obtain that 4(u(—x) + 1)需(x)2+4(u(—x) + 1)2 需(x)2 =
2 ∙ 4(U(X) + 1) ∙ 4(u(—x) + 1).
Further simplifying and using that U is an odd function We obtain that 4需(x)2(u(x) + 1 — u(x) +
1) = 2 ∙ 42(1 — u(x)2). Further simplifying We obtain dχ(x)2 = 4(1 — u(x)2). By definition
U(x) > —1. Suppose that U(x) > 1 then U(—x) = —U(x) < —1, which is impossible. Con-
sequently |U(x)| ≤ 1. Taking the square root of 1 — U(x)2 is thus permissible and we obtain
| 需(x)| = 2√1 — n(x)2.
Let W(X) = sgn(% (x)). Since * is the derivative of an odd function it is even and W is even.
Now du(x) = 2w(x),1 - u(x)2, consequently √ du 尸 =2w(x)dx and
arcsin(x) + C = 2 J； w(s)ds for some constant C. Thus u(x) = sin(2 J； w(s)ds — C). Because
U is odd it is necessary that u(0) = 0 and thus that C = π ∙ n. Since sin(x + π) = — sin(x) for
all x u(x) = sin(2S J； w(s)ds) with S ∈ {1, —1} and by setting w(x) = SW(X) the conclusion
holds.	□
Theorem 1. f is an activation function of an SBSNN, i.e. such that f is differentiable, Lipschitz
continuous, has a Lipschitz continuous derivative and satisfies that f (x)2 — 1 and f (x)2 — 1 are
odd functions, precisely if f (x) = ± √2sin(x + π∕4) or f (x) = ± √2cos(x + π∕4).
Proof. By lemma 1 there is an even function W taking values in {1, —1} such that f (x)2 — 1 =
sin(2 R0x W(s)ds). Let I(x) = R0x W(s)ds. Then f (x)2 = 1 + sin(2I (x)) = cos(I (x))2 +
Sin(I(x))2 + 2sin(I(X))Cos(I(x)) = (Cos(I(x)) + sin(I(x)))2 = (Sin(I(x) + π∕4))2. Thus
there exists some function S(x) such that f (x) = S(x)√2sin(ʃX w(s)ds + π∕4) where S(x) takes
values in {1, —1}.
Since S takes values in {1, —1} and √2sin( ʃX w(s)ds + π∕4) is continuous, if S jumps atany input
where sin( 0x W(s)ds + π∕4) is not zero, then f(x) is not continuous. Consequently S(x) changes
sign only at points where sin( 0X W(s)ds + π∕4) = 0.
Thus at points where sin( J； w(s)ds) = 0 f 0(x) = S(x)√2cos( J； w(s)ds+π∕4)w(x). Continuity
of f(x) then requires that S(x) jumps precisely where w(x) jumps.
Thus W(x) and S(x) may only jump at point where sin( 0X W(s)ds + π∕4) = 0 and if they jump
at such a point they must jump together. Consequently we can write S(x) = CW(x) where
C ∈ {1, —1}. Thus f (x) = √2Cw(x) cos( f^ w(s)ds + π∕4). If W jumps at x, then since
fX W(s)ds = 0 it follows that cos(ʃX W(s)ds) = 1, so f jumps at x, and f is not continuous,
which is a contradiction.
4
Under review as a conference paper at ICLR 2022
Thus W is constant and f(x) = CSin(Wx + π∕4) where C, W ∈ {1, -1}.
The reverse direction is trivial.	□
As a corollary of this result it follows that all derivatives of SBSNNs are in turn SBSNNs.
Lipschitz continuity of derivatives and the activation function and its derivative is a requirement of
the theory of Lu et al. (2020), but Gaussian Poincare normalized ReLU activation functions, which
do not have a derivative everywhere and which have a derivative which is not Lipschitz continuous
still behave largely according to the theory, with wide Gaussian PoinCare normalized ReLU networks
preserving norms in practice (fig. 1). Consequently there is reason to relax the continuity conditions
so as to admit functions for which the guarantees of Lu et al. (2020) do not hold but which are still
in the spirit of the theory. For this reason we consider the following theorem:
Theorem 2. Let w be an even function taking values in {1, -1} such that ∂ 0x w(s)ds exists where
∂ is a left- or right derivative and S any function taking values in {1, -1} with jumps only when
sin(∕X W(S)ds + π∕4) = 0, then f(x) = √2S(x) sin( J； W(S)ds + π∕4) is such that f (x)2 — 1 and
(∂ f)(x)2 - 1 are odd functions.
Proof. Let as before I(x) = 0x W(s)ds. Then I(x) is odd.
f (x)2 — 1 = 2sin2(I(x) + n/4) — 1 = 2 (-√^ (Sin(I(x)) + Cos(I(x)))) — 1 =
= 1 + 2 sin(I(x)) cos(I (x)) - 1 = 2 sin(I(x)) cos(I (x)) is an odd function since I(x) and
Sin(x) coS(x) are odd functions.
Let g(x) = S(x)√2sin(x + n/4). Because S(x) jumps only when sin(I(x)+ n/4) is zero ∂g exists
and is lim0↑χ S(a)√2cos(x+n∕4) inthe case of the left derivative and lim° Jx S(a)√2cos(x+∏∕4)
in the case of the right derivative.
Similarly, since (∂I)(x) exists it is lima↑x W(a) in the case when ∂ is the left derivative and
limaJx W(a) in the case of the right derivative.
Consequently ∂f = ∂(g ◦ I) exists and is (∂g)(I(x))(∂I)(x). (∂I)(x) is in {1, —1} so
((∂f)(x))2 = (∂g)(I(x))2 = (lima↑χ S(a)√2cos(I(x) + n/4))2 = 2cos2(I(x) + n/4).
Consequently (∂f )(x)2 — 1 = 2cos2(I(x) + n/4)w(x)2 — 1 =
=2( √2 (Cos(I(X))-Sin(I(X)))2—1 = -2cos(I(x)) sin(I(x)), which is an odd function. □
4	Positional encoders
SBSNNs require input which has magnitude equal to the square root of the dimension. Thus effective
use requires the use of a positional encoder.
Mildenhall et al. describe a positional encoder assigning to each co-ordinate the vector
(sin(20np), Cos(20np), ..., sin(2L-1np), Cos(2L-1np)) where p is the co-ordinate. These vectors
have squared magnitude equal to one half their dimension, and by scaling them by √2 we obtain a
vector of the required magnitude.
The method of Mildenhall et al. was originally applied to five-dimensional input, but when applied
to two-dimensional input it introduces obvious patterns in the form of correlations between pixels
along lines where one co-ordinate is constant (see fig. 3).
These patterns can be removed by a slight modification of the encoder of Mildenhall et al.: tak-
ing the input co-ordinate pair (X, y)T we construct two additional co-ordinate pairs by rotating the
original co-ordinate pair by one third and two-thirds a turn around the origin to obtain two more
D2∏∕3(x, y)T, D4∏∕3(x, y)T where Dθ denotes a rotation in the x-y plane by θ, and applying the
encoder of Mildenhall et al. to each and concatenating the three outputs. Use of this positional
encoder avoids line artefacts early (fig. 3) in the network fitting and can be seen to improve final
mean squared error.
5
Under review as a conference paper at ICLR 2022
Figure 3: Early frame during fitting. Left: Using a scaled encoder of the Mildenhall et al. type,
Right: Using the modified encoder. The image being fitted is a scaled-down 256 × 256 version of
the Cameraman test image.
Table 1: Comparison of image fitting of the 512 × 512 Cameraman test image using batch size 256
an initial learning rate of 5 ∙ 10-4 and with a halving of the learning rate when MSE plateaus for 60
epochs.
ARCHITECTURE
FINAL PSNR
SIREN, C =5.1	56.2
SIREN, c = √6	53.2
One SIREN layer followed by SBSNN 55.40
Mildenhall encoder followed by SBSNN 66.9
Rotated encoder followed by SBSNN 67.53
5	Experimental results
Results of image fitting experiments are summarized in table 1. Counting the PSNR 56.2 as what is
achievable with previous methods claim of the abstract follows: a PSNR of 56.2 corresponds to an
MSE of i0-56∙2”0 = 2.39 ∙ 10-6 andaPSNR of 67.53 to an MSE of 10-67.53/10 = 1.7 ∙ 10-7.
Thus the MSE achieved by this method is less than 1/10th that achievable previous methods.
With regard to training time, it is straightforward to see that this method uses substantially less
computation, as it is able to fit an image to superior accuracy in 500 epochs, instead of 15000
epochs. Even so, on large GPUs full batch training will make more effective use of the machine. In
the these experiments however, full batch training took 200 ms per epoch, while training with batch
size 256 took 2048 ms per epoch. Thus total training time is 3000 seconds for the traditional method
vs 1024 seconds with the improved method.
References
Y. Lu, Gould. S., and Ajanthan T. Bidirectionally self-normalizing neural networks. arXiv preprint
arXiv:2006.12169, 2020.
B. Mildenhall, P.P. Srinivasan, M. Tancik, J.T. Barron, Ramamoorthi R., and Ng R. Nerf: Repre-
senting scenes as neural radiance fields for view synthesis. Computer Vision - ECCV2020, 16th
European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, PartI, pp. 405-421.
V. Sitzmann, J.N.P. Martel, A.W. Bergman, and D.B. Lindell. Implicit neural representations with
periodic activation functions. Advances in Neural Information Processing Systems, 33, 2020.
6
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Forward propagation in SIRENs
In relation to the case where the pre-activations are uniformly distributed it has been claimed that
when X 〜 U(-1,1) sin(aX + b) will be Arcsine(-1,1) distributed irrespective of b. This is
only approximately true. The variance of a Arcsine(-1, 1) distribution is 1/2. There is a change
in behaviour at n/2, where the variance finally reaches 1/2 and for values greater than n/2 the
variance remains close to 1/2, but it is in fact not 1/2 in general and the difference is not altogether
small, unless c is large, as can be seen from the graph below. The blue line shows y = 1/2.
This can be resolved either by scaling the initial uniform distribution, thus causing the input to be
the U[-∏∕2, n/2] distributed instead of U[-1,1], by using a precisely scaled activation function,
sin(a∙) where a = ∏2 instead of sin(∙), or by using a heavily scaled activation function sin(a∙) where
a might be 30. This last approach is what has been proposed by Sitzmann et al. (2020) for use in
practice, while they use a = n/2 in a theoretical analysis.
The normal distribution of the pre-activations that occurs in the layers following the first arises as
follows:
The activations from the previous layer are assumed to be Arcsine(-1, 1) distributed and because
of this they have mean zero and variance 1/2.
The synaptic weights are independent of the activations of the previous layer and are
U(-c./√n, c/√n) distributed. This distribution has mean zero and variance 4(2c/，n)2 = 3c2/n.
The pre-activations for the new layer are the dot product of the previous activations and the synaptic
weights of the new layer. The mean of the this is zero and the variance of the product of a single
activation Xi and its corresponding synaptic weight Wi is
Var[WiXi] = E [(WiXi -E[WiXi])2] =
=E Wi2Xi2 - 2WiXiE [WiXi] + E[WiXi]2
= E Wi2 E Xi2 - E [Wi]2E[Xi]2 =
=gc2/n ∙ ] - 0 ∙ 0 = :c2/"
We know now that √nWiXi are independent, identically distributed random variables with mean
zero and variance 6c2 and thus, by the central limit theorem √n P2ι √nWiXi → N(0, 6c2).
Thus Pn=I WiX → N(0,1 c2).
It has been claimed that sin(WTX) will be Arcsin(-1, 1) if C > √6. This is not the case:
Var[sin(kZ)] = 2(1-e-2k2). Consequently, whenC = √6 WTX ~ N(0,1), giving Var[sin(Z)]=
2 (1 - e-2) ≈ 0.43, but if Sin(Z) had been Arcsin(-1,1) it would have variance 1/2.
7
Under review as a conference paper at ICLR 2022
The k at which 2 (1 - e-2k2) = 0.5 - e does not grow quickly when e is made small. The k which
produces e = 10-p is k = Pplog(10)∕2.
This gives a reason to choose higher values of C than √6 or 2√6. Another will come from the
following analysis of backward propagation in SIRENs.
A.2 Backward propagation in SIRENs
Consider the elementwise nonlinearity of a SIREN neuron and the set of synapses which re-
ceive input from it. This is a function f : R → Rn assigning to an input x the vec-
tor (wι Sin(X),…,wn Sin(X))T.	Consider the case when ∂f(E)k are known. Then d∂E =
dE df(X)k = P dE w, CoS(X)
∂f (x)k ∂x = 2^k ∂f (x)k Wk Cos(X).
Treating the gradients ∂f(E)k as inputs and the gradient ∂E We obtain a dual neuron with activation
function cos(∙) and weights Wk. The weight distribution of (Wk)f=ι will be the same as for forward
layers.
Over many fully connected layers their pre-activations, i.e. gradients before the dual activation func-
tion is applied will become approximately normal distributed: when the input distribution has high
variance it will be approximately uniformly distributed on a wide interval, and such a distribution
transformed by the cosine will be approximately Arcsine(-1, 1) distributed. Consequently this will
lead to approximately normal distributed pre-activations.
However, for the cosine of zero-centred normal distribution to be approximately Arcsine(-1, 1) this
distribution must have higher variance than for the sine of the same distribution tobe Arcsine(-1, 1)
distributed. This can be seen by considering the variance Var [cos(kZ)] = 2 + 11 e-2k2 - e-k2 to
the variance Var [sin(kZ)] = 2(1 - e-2k2).
8
Under review as a conference paper at ICLR 2022
The black curve shows the variance of the cosine transformed zero-centred normal distribution and
of the sine-transformed zero-centred normal distribution as a function of the standard deviation of
the input distribution.
A.3 Simulations
The need for higher variance pre-activations becomes apparent in simulations. We consider a SIREN
neural network with 20 layers and width 64 which receives U[-π∕2,π∕2] at initialization, for ini-
tializations with different choices of c.
The blue and black curves show, respectively, the mean of the absolute deviation of the pre-
activations and of the activations from their intended values, c/√6 for the pre-activations and ,1/2
for the activations, divided by those intended values, as a function of c.
In this We show a dual network corresponding to gradient propagation, unrealistically receiving
U[-π∕2,π∕2] distributed input.
9
Under review as a conference paper at ICLR 2022
This gives reason to consider values of c as large as 5.
B Derivations of formulas
B.1 DERIVATION OF THE FORMULA FOR E[sin(kZ)2]
We give the derivation of the formula for E[sin(kZ)2]. Consider a mean of a particular transformed
Wiener process
f(t) = E sin2(cWt) .
Knowing the derivatives
-^-Sin2 (cx) = 2c Sin(Cx)CoS(Cx)
dx
C Sin(2Cx)
d2	2	2
--y sin(x)2 = 2c2 cos(2cx),
dx2
We may apply ItO's lemma
sin2(cWt) = Z csin(2cWs)dWs + ɪ Z 2c2 cos(2cWs)ds
= Z CSin(2CWs)dWs + Z C2 coS(2CWs)ds.
Consequently
f(t) = C2 Z E [coS(2CWs)] ds.
0
NoW, consider
g(t, u) = E [coS(uWt)] .
Applying ItO's lemma We obtain
g(t, u) = E [coS(u(Ws ))] = E
1Z
-u2 coS(uWs )ds
Thus
—ɪu2 Z E [cos(uWs)] ds
20	s
—ɪu2 f g(s,u)ds.
20
10
Under review as a conference paper at ICLR 2022
and	g(t,u) = C(u)e- 1 u2t
. Since	g(0, u) = 1
We have	C(u) = 1
and thus that	g(t,u) = e- 1 u2t.
Thus
f(t) = c2 Z E [cos(2cWs)] ds = c2 Z g(t, 2c) =
tt
c2	e- 1 4c2t = c2	e-2c2t
00
2 e-2tc2 - 1	1 - e-2c2t
c^2C^ = ~Γ~
We can conclude that
E sin(cZ)2
1 - e-2c2
2
B.2 DERIVATION OF THE FORMULA FOR VAR [cos(cZ)]
In the calculation of E[sin(cZ)2] we obtained two results that are relevant also for this calculation,
that
E[cos(uWt)] = e- 1 Uut
and the conclusion of the previous calculation, that
E[sin(cZ)2]
1-
-2c
e
2
2
Because
E sin(cZ)2 + cos(cZ)2 = 1
it immediately follows that
E cos(cZ)2 = 1 - E sin(cZ)2
1-
e
1 + 2 e-2c2
1-
2
11
Under review as a conference paper at ICLR 2022
Using that
Var[X] = E (X -E[X])2 = E[X2] -E[X]2
we obtain
Var [cos (cZ)]=2+2 e
= 1 + 1 e
2 + 2
-2c2
-2c2
—
e 1吟2
- e-c2
12