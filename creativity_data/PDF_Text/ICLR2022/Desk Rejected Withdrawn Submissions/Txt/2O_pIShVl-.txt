Under review as a conference paper at ICLR 2022
Polygonal Unadjusted Langevin Algorithms:
Creating stable and efficient adaptive algo-
RITHMS FOR NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We present a new class of Langevin based algorithms, which overcomes many of
the known shortcomings of popular adaptive optimizers that are currently used for
the fine tuning of deep learning models. Its underpinning theory relies on recent
advances of Euler’s polygonal approximations for stochastic differential equations
(SDEs) with monotone coefficients. As a result, it inherits the stability properties
of tamed algorithms, while it addresses other known issues, e.g. vanishing gradi-
ents in neural networks. In particular, we provide a nonasymptotic analysis and
full theoretical guarantees for the convergence properties of an algorithm of this
novel class, which we named THεO POULA (or, simply, TheoPouLa). Finally,
several experiments are presented with different types of deep learning models,
which show the superior performance of TheoPouLa over many popular adaptive
optimization algorithms.
1	Introduction
Modern machine learning models including deep neural networks are successfully trained when
they are finely tuned via the optimization of their associated loss functions. Two aspects of such
optimization tasks pose significant challenges, namely the non-convex nature of loss functions and
the highly nonlinear features of many types of neural networks. Moreover, the analysis in Lovas
et al. (2020) shows that the gradients of such non-convex loss functions typically grow faster than
linearly and are only locally Lipschitz continuous. Naturally, stability issues are observed, which
are known as the ‘exploding gradient’ phenomenon (Bengio et al., 1994; Pascanu et al., 2013),
when vanilla stochastic gradient descent (SGDs) or certain types of adaptive algorithms are used
for fine tuning. The sparsity of gradients of neural networks is another challenging issue, which
is extensively studied in the literature. For example, momentum methods and adaptive learning
rate methods such as AdaGrad (Duchi et al. (2011)), RMSProp (Tieleman & Hinton (2012)), Adam
(Kingma & Ba (2015)) have been developed to tackle this problem and improve training speed by
diagonally scaling the gradient by some function of the past gradients.
A family of Langevin based algorithms has been another important stream of literature on the
stochastic optimization. They are built on the theoretical fact that the Langevin stochastic differ-
ential equation, (6), converges to its unique invariant measure, which concentrates on the global
minimizers of the objective function as β → ∞, see Hwang (1980). Since the convergence property
remains true for nonconvex optimization problems, the global convergence of the stochastic gradi-
ent Langevin dynamics (SGLD) and its variants has been extensively studied in a nonconvex setting
(Raginsky et al., 2017; Xu et al., 2018; Erdogdu et al., 2018; Brosse et al., 2018; Lovas et al., 2020).
Moreover, itis worth noting that Langevin based algorithms have been akey element in statistics and
Bayesian learning (Roberts & Tweedie, 1996; Durmus & Moulines, 2017; Dalalyan, 2017; Brosse
et al., 2019; Welling & Teh, 2011; Deng et al., 2020a;b).
Motivated by the aforementioned developments in the field, we propose a new class of Langevin
algorithms which is based on recent advances of Euler’s polygonal approximations for Langevin
SDEs. The idea of this new form of Euler’s polygonal approximations for SDEs with monotone
coefficients originates from the articles Krylov (1985) and Krylov (1990). We name this new class
as polygonal unadjusted Langevin algorithms. Moreover, itis versatile enough to incorporate further
1
Under review as a conference paper at ICLR 2022
features to address other known shortcomings of adaptive optimizers. Mathematically, itis described
as follows: Given an i.i.d. sequence of random variables {Xn}n≥0 of interest, which typically
represent available data, the algorithm follows
θ0 := θ0,	θλ+1 := θλ - λGλ(θn,Xn+ι)+ P2λβ-1ξn+ι, n ∈ N,	⑴
where θ0 is an Rd-valued random variable, λ > 0 denotes the step size of the algorithm, β > 0 is
the so-called inverse temperature, (ξn)n∈N is an Rd-valued Gaussian process with i.i.d. components
and Gλ : Rd × Rm → Rd satisfies the following three properties:
1.	For every λ > 0, There exist constants Kλ > 0 and ρι ≥ 0 such that ∣Gλ(θ,χ)∣ ≤
Kλ(1 + ∣x∣)ρι (1 + ∣θ∣) for every θ ∈ Rd and X ∈ Rm.
2.	There exist constants γ ≥ 1/2, K2 > 0 and ρ2, ρ3 ≥ 0 such that for all λ > 0,
∣Gλ(θ,x) - G(θ,x)∣ ≤ λγK2(l + ∣x∣)p2(1 + ∣θ∣)p3
for every θ ∈ Rd andx ∈ Rm, where Gis the (unbiased) stochastic gradient of the objective
function of the optimization problem under study.
3.	There exist constants λmax and δ ∈ {1, 2} such that for any λ ≤ λmax,
θ	2λ
lim^nf E hjθ∣δ,G，(4X0)i- jθ∣δlGλ(θ,χ0)1	> 0.
One obtains our new algorithm THεO POULA by considering the case where Gλ(θ, x) is the vector
with entries Hλ(i,)c(θ, x) as given by (8), for i ∈ {1, . . . , d}. Its name is formed from its description,
namely Tamed Hybrid ε-Order POlygonal Unadjusted Langevin Algorithm and its full detailed anal-
ysis (including its convergence properties) are given in Section 3. We note that THεO POULA and
TUSLA (Lovas et al. (2020)) satisfy the above three properties with δ = 2 and γ = 1/2, whereas
TULA (Brosse et al. (2019)) satisfies them with δ = γ = 1 as it assumes only deterministic gradients
(and thus the i.i.d. data sequence reduces to a constant).
1.1	Related Work: Langevin based algorithms and adaptive learning rate
METHODS
Most research on Langevin based algorithms in the literature has been focused on theoretical aspects.
Raginsky et al. (2017) demonstrated the links between Langevin based algorithms and stochastic op-
timization in neural networks, stimulating further the development and analysis of such algorithms.
Xu et al. (2018) analyzed the global convergence of GLD, SGLD and SVRG-LD. The incorporation
of dependent data streams in the analysis of SGLD algorithms has been achieved in Barkhagen et al.
(2021) and in Chau et al. (2019), and local conditions have been studied in Zhang et al. (2019). Re-
cently, TUSLA of Lovas et al. (2020) has been proposed based on a new generation of tamed Euler
approximations for stochastic differential equations (SDEs) with monotone coefficients in noncon-
vex optimization problems. See Hutzenthaler et al. (2012) and Sabanis (2013) for the rationale of
taming techniques. Despite their elegant theoretical results, the use of Langevin based algorithms
for training deep learning models has been limited in practice as their empirical performance lacked
behind in comparison to other popular adaptive gradient methods. We refer to Appendix F.3 for the
reader who is interested in recent progress on sampling and Bayesian neural networks.
Adaptive learning rate methods such as AdaGrad (Duchi et al. (2011)), RMSProp (Tieleman &
Hinton (2012)) and Adam (Kingma & Ba (2015)) have been successfully applied to neural net-
work models due to their fast training speed. Since the appearance of Adam, a large number of
variants of Adam-type optimizers have been proposed to address the theoretical and practical chal-
lenges of Adam. For example, Reddi et al. (2018) provided a simple example that demonstrates
the non-convergence issue of Adam and proposed a simple modification, called AMSGrad, to solve
this problem. Chen et al. (2019) discussed the convergence of Adam-type optimizers in a noncon-
vex setting. RAdam to rectify the variance of adaptive learning rate has been proposed in Liu et al.
(2020). Wilson et al. (2017) revealed that the generalization ability of adaptive learning rate methods
is worse than a global learning method like SGD. AdaBound of Luo et al. (2019) attempts to over-
come the drawback by employing dynamic bounds on learning rates. Recently, AdaBelief (Zhuang
et al. (2020)) and AdamP (Heo et al. (2021)) demonstrated their fast convergence and good gen-
eralization via extensive experiments. Nevertheless, the convergence analysis of these (and other)
2
Under review as a conference paper at ICLR 2022
adaptive learning rate methods is still restrictive since it is only guaranteed to converge to a station-
ary point (which can be a local minimum or a saddle point) under strong assumptions. Namely, the
stochastic gradient is globally Lipschitz continuous and bounded. Note though that none of these
two assumptions hold true in a typical optimization problem involving neural networks. This is
particularly evident in complex neural network architectures.
1.2	Our contributions
The proposed algorithm, THεO POULA, tries to combines both advantages: namely, global conver-
gence in Langevin based algorithms and powerful empirical performance in adaptive learning rate
methods. To the best of the authors’ knowledge, our algorithm is the first Langevin based algorithm
to outperform popular stochastic optimization methods such as SGD, Adam, AMSGrad, RMSProp,
AdaBound and AdaBelief for deep learning tasks. The major strengths of our work over related
algorithms are summarized as follows:
•	(Global convergence) We provide a global convergence analysis of THεO POULA for
nonconvex optimization where the stochastic gradient of the objective is locally Lipscthiz
continuous. Moreover, non-asymptotic estimates for the expected excess risk are derived.
•	(Stable and fast training) THεO POULA achieves a stable and fast training process us-
ing the (element-wise) taming technique, (element-wise) boosting function and averaging,
which are theoretically well-designed. Furthermore, we validate the effectiveness of the
taming and boosting functions through several empirical experiments.
•	(Good generalization) While THεO POULA behaves like adaptive learning rate methods
in the early training phase, it takes an almost global learning rate near an optimal point.
That is, THεO POULA is quickly switched from adaptive methods to SGD. As a result,
it inherits the good generalization ability of SGD. Our experiments support this fact by
showing that THεO POULA outperforms the other optimization methods in generalization
measured by test accuracy for various deep learning tasks.
2 M otivating Example
The local Lipschitz continuity of gradients and its effect on the performance of optimization methods
are relatively under-studied. Most relevant studies assume that the stochastic gradient is global
Lipscthiz continuous and bounded (Kingma & Ba, 2015; Xu et al., 2018; Brosse et al., 2018; Duchi
et al., 2011; Tieleman & Hinton, 2012; Reddi et al., 2018; Chen et al., 2019; Liu et al., 2020; Luo
et al., 2019; Zhuang et al., 2020) although it is not true for neural network problems. This section
provides a simple, one-dimensional optimization problem that illustrates the convergence issue of
popular stochastic gradient methods when the stochastic gradient is locally Lipschitz continuous,
i.e., the gradient can be super-linearly growing 1.
Consider the following optimization problem:
min u(θ) = min E[U (θ, X)],	(2)
where U : R × R → R is defined as
U(θ, x)
θ2 (1 + iχ≤ι) + Θ30,	∣Θ∣≤ 1,
(2∣θ∣- 1)(1 + lχ≤ι) + θ30,	∣θ∣ > 1,
and X is uniformly distributed over (-2, 2), that is, fx(x) = 11 l∣x∣≤2. Furthermore, the stochastic
gradient G : R × R → R is given by
G(θ, x)
2θ (1 + lχ≤ι)+30θ29, ∣θ∣ ≤ 1,
2(1 + lχ≤1)sgn(θ)+30θ29,网 > 1,
where sgn(∙) is the sign function. Note that the stochastic gradient G is locally LiPSchitz continuous,
which satisfies
∣G(θ,x) - G(θ0,x)∣ ≤ 34(1 + ∣θ∣ + ∣θ0∣)28∣θ - θ0∣
1Lovas et al. (2020) used a similar examPle to show the stability of TUSLA with a different taming function.
3
Under review as a conference paper at ICLR 2022
(a) default settings
(b) different step sizes
Figure 1: Performance of SGD, Adam, AMSGrad, RMSProp and THεO POULA on an artificial
example with the initial value θ0 = 5.0
for all x ∈ R and θ, θ0 ∈ R. Also, the optimal value is attained at θ = 0. See Appendix A for more
details. Following Reddi et al. (2018), adaptive stochastic gradient methods can be generally written
as follows, for n ∈ N,
mn =	φn(GI,…，Gn),
Vn	=	Ψn(Gl,…，Gn),
θn+1	=	θn - λn	InmP
ε+ Vn
(3)
where Gi := G(θi , Xi) is the stochastic gradient evaluated at the i-th iteration, λn is the step size
and all operations are applied element-wise. Table 1 provides the details for some of the most
popular stochastic optimization methods with corresponding averaging functions φn and ψn .
Table 1: Summary of stochastic optimization methods within the general framework. Note that
vbn = max{vbn-1, vn} is defined as vn = (1 - β2)vn-1 + β2G2n.
	SGD		RMSPROP	Adam	AMSGRAD
φn :=	Gn		Gn	(1 -β1)Pin=1β1n-iGi	(1 -β1)Pin=1β1n-iGi
ψn :=	In	(1	β2)diag(Pin=1 β2n-iGi2)	(1 - β2)diag(Pin=1 β2n-iGi2)	diag(vbn)
We use SGD, Adam, AMSGrad and RMSprop to solve the optimization problem with initial value
θ0 = 5. For hyperparameters of optimization algorithms, we use their default settings provided in
PyTorch. Figure 1(a) shows the trajectories of approximate solutions generated by each optimizer.
While SGD, Adam, AMSGrad and RMSProp fail to converge to the optimal solution 0, the proposed
algorithm, THεO POULA, finds the optimal solution with a reasonable step size, say, 0.01.
Intuitively, the undesirable phenomenon occurs because, in the iterating rule (3), the denominator
VVn excessively dominates the numerator mn causing the vanishing gradient problem in the pres-
ence of the superlinear gradient. On the contrary, SGD suffers from the exploding gradient problem.
Moreover, Figure 1(b) highlights that the problematic behavior cannot be simply resolved by ad-
justing the learning rate within the Adam-type framework, while THεO POULA perform extremely
well even in the presence of such violent non-linearities.
3 NEW ALGORITHM: THεO POULA
We propose a new stochastic optimization algorithm by combining ideas from taming methods
specifically designed to approximate Langevin SDEs with a hybrid approach based on recent ad-
vances of polygonal Euler approximations. The latter is achieved by identifying a suitable boosting
4
Under review as a conference paper at ICLR 2022
function (of order ε	1) to efficiently deal with the sparsity of (stochastic) gradients of neural
networks. In other words, the novelty of our algorithm is to utilize a taming function and a boosting
function, rather than designing a new Vn as in Adam-type optimizers.
We proceed with the necessary preliminary information, main assumptions and formal introduction
of the new algorithm.
3.1	Preliminaries and Assumptions
Let (Ω,F,P) be a probability space. We denote by E[X] the expectation of a random variable
X. Fix an integer k ≥ 1. For an Rk-valued random variable X, its law on B(Rk), i.e. the Borel
sigma-algebra of Rk, is denoted by L(X). Scalar product is denoted by〈•，•)，with | ∙ | standing for
the corresponding norm (where the dimension of the space may vary depending on the context). For
any integer q ≥ 1, let P(Rq) denote the set of probability measures on B(Rq). For μ,ν ∈ P(Rk),
let C (μ, V) denote the set of probability measures Z on B(R2k) such that its respective marginals are
μ, v. For two probability measures μ and V, the Wasserstein distance of order P ≥ 1 is defined as
Wp(μ,ν ):=	inf (ZZ ∣θ — θ0∣pZ(dθdθ0)) /P , μ,ν ∈P (Rk).	(4)
ζ∈C(μ,ν) ∖Rkk Rkk
Let (Xn)n∈N be a sequence of i.i.d. Rm-valuedrandom variables generating the filtration (Gn)n∈N
and (ξn)n∈N be an Rd-valued Gaussian process with independent components.
Let F : Rd × Rm → Rd be continuously differentiable function such that E[F (θ, X0)] < ∞ for any
θ ∈ Rd. We consider the following optimization problem
minU(O) = min (E[F(θ,χo)] + / m|2(r+i))	⑸
θ	θ	2(r + 1)
where θ ∈ Rd, η ∈ (0,1) is the regularization parameter and r ≥ 2 + 1. Inthe context of fine tuning
of neural networks, F represents the loss function for the task at hand and θ denotes the vector of the
neural network,s parameters. Note that the regularization term,②;7 ∣θ∣2(r+1), is added in order to
guarantee that the dissipativity property holds, since it is essential for the convergence analysis.
Remark 3.1. For the reader who prefers to consider the optimization problem without the regular-
ization term, i.e. with η = 0, the dissipative condition (B.1) has to be additionally assumed as in the
literature (Raginsky et al., 2017; Xu et al., 2018; Erdogdu et al., 2018). Then, the same analysis can
be applied to obtain our main results without any additional effort. However, it is yet to be proven
theoretically that such an assumption holds in general for neural networks and thus it becomes a
case-by-case investigation. In other words, we present here the formal theoretical statement with
the appropriate regularization term which covers all of these cases.
In particular, r depends on the neural network’s structure, whereas q is described in Assumption 3.1.
Consequently, the stochastic gradient with the regularization term is given by
H(θ,x) := G(θ,x)+ ηθ∣θ∣2r
where G(θ, x) := VθF(θ, x) for all X ∈ Rm, θ ∈ Rd and η = 0 if dissipativity holds for G.
We introduce our main assumptions. The first requirement is that G is locally Lipschitz continuous.
Assumption 3.1. There exists positive constant L1, ρ and q ≥ 1 such that
∣G(θ, χ) - G(θ0, x)| ≤ Lι(1 + ∣x∣)ρ(1 + ∣θ∣ + ∣θ0∣)q-1∣θ - θ0∣.
for all x ∈ Rm and θ, θ0 ∈ Rd. Moreover, h(θ) := E[H (θ, X0)] for every θ ∈ Rd.
Further, conditions on the initial value θ0 and data process (Xn)n∈N are imposed as it is common to
use weight initialization using the uniform or normal distribution, Assumption 3.2 is mild.
Assumption 3.2. The process (Xn)n∈N is a sequence of i.i.d. random variables with
E[∣Xθ∣16ρ(2r+1)] <
∞ where ρ is given in Assumption 3.1. In addition, the initial condition is
such that E[∣θo∣16(2r+1)] < ∞.
5
Under review as a conference paper at ICLR 2022
We refer to Appendix B for further remarks and key observations regarding the consequences of
Assumptions 3.1 and 3.2. We conduct the convergence analysis of THεO POULA by employing
elements of the theory of Langevin SDEs. It is shown that, under mild conditions (satisfied by
Assumptions 3.1 and 3.2), the so-called (overdamped) Langevin SDE, which is given by
dZt = -h(Zt)dt + 2β-1dBdBt, t > 0,
(6)
where h = Vu with a (possibly random) initial condition θo and with (Bt)t≥o denoting a d-
dimensional Brownian motion, admits a unique invariant measure ∏β(dz) 8 exp(一βu(z)). Thus,
for a sufficiently large β , πβ concentrates around the minimizers of (5).
3.2	MECHANISM OF THεO POULA
We introduce the mechanism of THεO POULA, which iterately updates as follows:
θ0 := θo,	θλ+1 := θλ 一 λHλ,c(θn,Xn+ι) + √2λβ-iξn+1,	n ∈ N,
where Hλ,c ：=(或］(。，x),…，HdC(θ, X))T is given by
H(i) (θχ)=	GCi) (θ,x)	(ι + _√λ_ ʌ + /工
λ,c(,)=1 + √λ∣G(i)(θ,x)∣	+ ε + ∣G(i)(θ,x)∣	+ η 1 + √λ∣θ∣2r ,
、-------V------} '------:------；--/	、----V-------}
taming function boosting function regularization term
(7)
(8)
and {ξn}n≥1 is a sequence of independent standard d-dimensional Gaussian random variables. Note
that the taming and boosting functions are defined in (8).
THεO POULA has several distinct features over the existing optimization methods in the litera-
ture. We give an intuitive explanation as to how these features are complementarily harmonized
to improve the performance of the algorithm, and to handle the exploding and vanishing gradient
problems of neural networks. For simplicity, We omit the regularization term, that is, η = 0, and the
noise term, ,2λβTξn+ι, throughout the exposition. Also, we refer to λ as the learning rate and
nθn| := 1+⅞⅜X⅞Γ (1 + ε+∣G(i)(⅞,x…)|) as the SePSiz by the conventionin Kingma
& Ba (2015).
Firstly, the new algorithm utilizes the taming function to control the super-linearly growing gradient.
In a region where the loss function is steep and narrow (the gradient is huge), it is ideal for the
optimizer to take a small stepsize. This is effectively achieved since the growth of the taming
function is proportional to G, but the boosting is close to one when the gradient is huge. The
effectiveness of the taming function is confirmed in the motivating example in Section 2. Note
that the taming function is applied element-wise to scale the effective element-wise learning rate
in contrast to TUSLA of Lovas et al. (2020). This significantly improves the performance of our
new algorithm in solving high-dimensional optimization problems such as the fine tuning of neural
network models.
Secondly, we have designed the boosting function to accelerate training speed and prevent the van-
ishing gradient problem2 * *. When the current parameter is located in a region where the loss function
is flat (the gradient is small), it is desirable for the optimizer to take a large stepsize. As the gradient
gets smaller, the boosting function increases the stepsize ofTHεO POULA by up to √λ∕ε, whereas
the taming function’s contribution decreases. As a result, THεO POULA takes a larger stepsize. In
other words, THεO POULA takes a desirable stepsize depending on the magnitude of the gradient.
Most importantly, the taming and boosting functions do not interfere with each other in any adverse
way. On the contrary, they complement each other in a harmonious way that is evident from our
simulation results.
Thirdly, THεO POULA is quickly converted from adaptive learning rate methods to SGD. In the
early training phase, THεO POULA certainly behaves like adaptive learning rate methods. Then,
2We provide the effectiveness of the boosting function in Appendix E.3 by comparing the performance of
THεO POULA with/without the boosting function. The experiment shows that the addition of the boosting
function brings a significant improvement in test accuracy across different models and data sets.
6
Under review as a conference paper at ICLR 2022
when the current position is approaching an optimal solution where ∣G(i)∣s are close to zero, the
movement of THεO POULA is similar to SGD with a learning rate (1 + √λ∕ε). Consequently,
THεO POULA simultaneously attains two favorable features of fast training in adaptive learning
rate methods and good generalization in SGD. The switching from adaptive learning rates to SGD
has been also investigated by different strategies in Luo et al. (2019) and Keskar & Socher (2017).
Lastly, a scaled Gaussian noise, ∖∕2λβ-1ξn+ι, is added as a consequence of the discretization of the
Langevin SDE. The term is essential to prove the convergence property of THεO POULA. Adding
properly scaled Gaussian noise allows the new algorithm to escape local minima in a similar manner
to the standard SGLD method, see Raginsky et al. (2017).
3.3	Convergence Analysis
We present in this section the main convergence results of THεO POULA to πβ in Wasserstein-1
and Wasserstein-2 distances as defined in (4). The convergence is guaranteed when the learning rate
is less than λmax , which is given by
λmax = min {4η2, 214η2(1ιC4i)2}.	(9)
where nCk is the binomial coefficient ‘n choose k’ and l = 2r + 1. Note that the learning rate
restriction causes no issues a η is typically very small (η 1). Moreover, let T := 1∕λ.
Theorem 3.1 and Corollary 3.1 state the non-asymptotic (upper) bounds between L θnλ and πβ.
An overview of the proofs of our main results can be found in Appendix C.
Theorem 3.1.	Let Assumptions 3.1 and 3.2 hold. Then, for every 0 <λ ≤ λmax and n ∈ N, we
have
W1 (L (θλ), ∏β) ≤Mι√λ + M2e-cλn,
where C, Mi and M2 are constants independent of n and λ. The explicit form of C, Mi and M2 are
given in Table 7.
Corollary 3.1. Let Assumptions 3.1 and 3.2 hold. Then, for every 0 <λ ≤ λmax and n ∈ N, we
have
W2 (L (θλ , ∏β) ≤M3√λ + z2λ4 + M4e-cλn,
where C, z2, M3 and M4 are constants independent of n and λ. The explicit form of C, z2, M3 and
M4 are given in Table 7.
We are now concerned with the expected excess risk of THεO POULA generated by (7), so called
the optimization error of θnλ , defined as
E[u(θn)] - u(θ*)	(10)
where θ* := arg minj∈Rd u(θ). To derive the bound of the expected excess risk, it is again decom-
posed into two parts; E[u(θl)] 一 E[u(θ∞)] and E[u(θ∞)] 一 u(θ*). Here, θ∞ follows the invariant
distribution πβ. The following theorem describes the bound of the expected excess risk of THεO
POULA.
Theorem 3.2.	Let Assumptions 3.1 and 3.2 hold. For any n ∈ N, the expected excess risk of the
n-th iterate of THεO POULA (7) is upper bounded by
E[u(θn)] 一Uo ≤ M5W2(L(On,πβ))+. 2log (ɪ(dβ+1)) + log2
where W2(L(θnλ), πβ) is given in Corollary 3.1 and A, B are given in Remark B.2. and M5, K are
given in Table 7. All the constatns are independent of n and λ.
3.4 AVERAGED THεO POULA
One notes that Theorem 3.1 implies that THεO POULA converges, under suitable decreasing step
size regime, to the invariant measure πβ and thus its performance can be further improved by aver-
aging. It is achieved by averaging of trajectories of the parameters after a user-specified trigger Q,
7
Under review as a conference paper at ICLR 2022
n-Q+ι Pn=Q θ), instead of the last updated parameter θn (Polyak & JUditsky,1992). In particular,
we use a trigger strategy which starts the averaging when no improvement in the validation metric
is seen for a patience number of epochs. For our experiments, we set the patience number to 5.
Our experiments show that averaged THεO POULA performs better than the other stochastic op-
timization methods for language modeling tasks. Moreover, while a learning rate decay, which
requires additional tuning effort, has to be applied for the other optimizers to obtain their best per-
formance, averaged THεO POULA uses a constant learning rate, which is another practical benefit
of our newly proposed algorithm.
4 Empirical performance on real data sets
This section examines the performance of THεO POULA on real data sets by comparing it with
those of other stochastic optimization algorithms including Adam (Kingma & Ba (2015)), AdaBelief
(Zhuang et al. (2020)), AdamP (Heo et al. (2021)), AdaBound (Luo et al. (2019)), AMSGrad (Reddi
et al. (2018)), RMSProp (Tieleman & Hinton (2012)), SWATS (Keskar & Socher (2017)), SGD
(with momentum) and ASGD (Merity et al. (2018)). We conduct the following deep learning exper-
iments: image classficiation on CIFAR-10 (Krizhevsky et al.) and CIFAR-100 (Krizhevsk (2009))
and language modeling on Penn Treebank (Marcus et al. (1999)). Each experiment is run three times
to compute the mean and standard deviation of the best accuracy on the test dataset. We provide de-
tails of the experiments including learning curves and hyperparameter settings in Appendix E.
For our experiments, we consider η = 0 in (5). This is justified by the fact that some form of
dissipativity may exist for specific problems such as the one considered here, although this has not
been verified theoretical so far. In Appendix F.2, we perform additional experiments with η 6= 0,
which show a very similar performance by THεO POULA as in Table 2 without any noticeable loss
of accuracy. This demonstrates that there is no gap between theory and practice of our work.
Image classification We replicate the experiments of VGG11 (Simonya & Zisserman (2015)),
ResNet34 (Ioffe & Szegedy (2016)) and DenseNet121 (Huang et al. (2017)) on CIFAR-10 and
CIFAR-100 in the official implementation of Zhuang et al. (2020). They provide a reliable baseline
of the experiments by comparing the performance of various stochastic optimizers with extensive
hyperparameter search. We search the optimal hyperparameters for THεO POULA among λ =
{1, 0.5, 0.1, 0.05, 0.01} and ε = {1, 0.1, 0.01}. β is chosen among {108, 1010, 1012} across all the
experiments.
Table 2 shows the test accuracy for VGG11, ResNet34 and DenseNet121 on CIFAR-10 and CIFAR-
100. As reported in Table 2, our algorithm achieves the highest accuracy and significantly outper-
forms the other optimizers across all the experiments. In particular, even THεO POULA with the
second best hyperparameter is comparable to AdaBelief and outperforms the other methods, val-
idating that the solutions found by THεO POULA yield good generalization performance. Also,
the improvement of our algorithm is increasingly prominent as the models and datasets are more
complicated and large-scale.
Language modeling We perform language modeling over the Penn Treebank (PTB) with AWD-
LSTMs of Merity et al. (2018). Itis reported that Non-monotonically Triggered ASGD (NT-ASGD)
achieves state-of-the-art performance for the language modeling task with AWD-LSTMs. Motivated
by this observation, we consider averaged THεO POULA for the experiment. Due to a limited
computation budget, we only test ASGD and AdaBelief rather than investigating all the optimizers
in this experiment 3.
For a fair comparison, the averaging scheme has also been applied to AdaBelief, but we have found
that it does not improve the performance of AdaBelief. Instead, AdaBelief uses a development-
based learning rate decay, which decreases the learning rate by a constant factor if the model does
not attain a new best value for multiple epochs. For ASGD and THεO POULA, a constant learning * &
3Since AdaBelief significantly outperforms the other optimizers including vanilla SGD, AdaBound, Yogi
(Zaheer et al. (2018)), Adam, MSVAG (Balles & Hennig (2018)), RAdam, Fromage and AdamW (Loshchilov
& Hutter (2019)) in the same experiment, we believe that we do not need to explore all the optimizers.
8
Under review as a conference paper at ICLR 2022
Table 2: Mean and standard deviation of the best accuracy for VGG11, ResNet34 and DenseNet121
on CIFAR10. THεOPOULA∣ and THεOPOULA* represent the performances of THεO POULA
with the best and second best hyperparameters, respectively.
dataset		CIFAR-10					CIFAR-100			
model	VGG	ResNet	DenseNet	VGG	ResNet	DenseNet
THεO POULAt	92.31	95.43	95.66	70.31	77.60	79.90
	(0.055)	(0.095)	(0.066)	(0.117)	(0.144)	(0.133)
THεO POULA*	91.92	94.92	-95.59-	70.24	76.88	-7876-
	(0.119)	(0.076)	(0.067)	(0.227)	(0.536)	(0.269)
AdaBelief	92.17	95.29	-95.58-	69.50	77.33	-79T2-
(baseline)	(0.035)	(0.196)	(0.095)	(0.111)	(0.172)	(0.382)
Adam	90.79	93.11	-93.21-	67.30	73.02	-74.03-
	(0.075)	(0.184)	(0.240)	(0.137)	(0.231)	(0.334)
AdamP	91.68	95.18	-95.17-	69.41	76.14	-77.58-
	(0.162)	(0.116)	(0.079)	(0.297)	(0.347)	(0.091)
AdaBound	91.81	94.83	-95.05-	68.61	76.27	-77.56-
	(0.272)	(0.131)	(0.176)	(0.312)	(0.256)	(0.120)
AMSGrad	91.24	93.76	-93.74-	67.71	73.51	-74.50-
	(0.115)	(0.108)	(0.236)	(0.291)	(0.692)	(0.416)
RMSProp	90.82	93.06	-92.89-	65.45	71.79	-71.75-
	(0.201)	(0.120)	(0.310)	(0.394)	(0.287)	(0.632)
SGD	90.73	94.61	-94.46-	67.78	77.16	-78.95-
	(0.090)	(0.280)	(0.159)	(0.320)	(0.214)	(0.312)
SWATS	87.29 (4.210)	94.76 (0.565)	-95.04- (0.339)	N/A	73.86 (3.928)	-78.81- (1.812)
rate is used without a learning rate decay. In order to compare with the baseline, we apply gradient
clipping of 0.25 to all optimizers. See Appendix E for more information.
Table 3 shows that THεO POULA attains the lower
test perplexity against the baselines for AWD-LSTM
with one, two and three layers. AdaBelief shows a
comparable performance with ASGD for 2-layer and
3-layer models.
Our experimental results show that THεO POULA
achieves higher accuracy than AdaBelief (known as
the state-of-the-art algorithm for many deep learning
tasks) on image classification and language modeling
tasks for various deep learning models. Furthermore,
it is easier to tune parameters of THεO POULA since
the number of hyperparmeters for THεO POULA is
less than that of Adam-type optimizers.
Table 3: Test perplexity for language mod-
eling tasks on PTB. Lower is better.
# of layers	1-layer	2-layer	3-layer
THεO POULA	82.75	67.15	61.07
	(0.209)	(0.126)	(0.161)
ASGD	82.85	67.53	61.60
(baseline)	(0.308)	(0.171)	(0.094)
AdaBelief	84.46	67.34	61.52
	(0272)	(0.496)	(0.302)
5 Conclusion and Discussion
This paper begins with an example which illustrates that local Lipschitz continuous gradients can
cause serious convergence issues for popular adaptive optimization methods. Such issues manifest
themselves as vanishing/exploding gradient phenomena. It proceeds by proposing a novel optimiza-
tion framework, which is suitable for the fine tuning of neural network models by combining ele-
ments of the theory of Langevin SDEs, tamed algorithms and carefully designed boosting functions
that handle sparse and super-linearly growing gradients. Further, a detailed convergence analysis of
the newly proposed algorithm THεO POULA is provided along with full theoretical guarantees for
obtaining the best known convergence rates. Our experiments confirm that THεO POULA outper-
forms other popular stochastic optimization methods.
We believe that there is much room for improvement of our novel framework. For example, the
improved performance can be further achieved by identifying more efficient taming and boosting
functions, which demonstrates the potential of our framework.
9
Under review as a conference paper at ICLR 2022
References
L. Aitchison. A statistical theory of cold posteriors in deep neural networks. International Confer-
ence on Learning Representations, 2021.
L.	Balles and P. Hennig. The sign, magnitude and variance of stochastic gradients. International
Conference on Machine Learning, 2018.
M.	Barkhagen, N. H. Chau, E. Moulines, M. Rasonyi, S. Sabams, and Y. Zhang. On stochastic
gradient Langevin dynamics with dependent data streams in the logconcave case. Bernoulli, 27
(1):1-33, 2021.
Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
N.	Brosse, E. Moulines, and A. Durmus. The promises and pitfalls of stochastic gradient langevin
dynamics. Advances in Neural Information Processing Systems, 2018.
N.	Brosse, A. Durmus, E. Moulines, and S. Sabanis. The tamed unadjusted Langevin algorithm.
Stochastic Processes and their Applications, 129(10):3638-3663, 2019.
H. N. Chau, E. Moulines, M. Rasonyi, S. Sabanis, and Y. Zhang. On stochastic gradient
Langevin dynamics with dependent data streams: the fully non-convex case. arXiv preprint
arXiv:1905.13142, 2019.
X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for
non-convex optimization. International Conference on Learning Representations, 2019.
A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651-676, 2017.
W. Deng, Q. Feng, L. Gao, and G. Lin. Non-convex learning via replica exchange stochastic gradient
MCMC. International Conference on Machine Learning, 2020a.
W. Deng, G. Lin, and F. Liang. A contour stochastic gradient Langevin dynamics algorithm for
simulations of multi-modal distributions. Conference on Neural Information Processing Systems,
2020b.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12, 2011.
A. Durmus and E. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin
algorithm. The Annals of Applied Probability, 27(3):1551-1587, 2017.
A.	Eberle, A. Guillin, and R. Zimmer. Couplings and quantitative contraction rates for Langevin
dynamics. Annals of Probability, pp. 1982-2010, 2019.
M. A. Erdogdu, L. Mackey, and O. Shamir. Global non-convex optimization with discretized diffu-
sions. Conference on Neural Information Processing Systems, 2018.
B.	Heo, S. Chun, S.J. Oh, D. Han S. Yun, G. Kim, Y. Uh, and J. Ha. Adamp: slowing down
the slodown for momentum optimizers on scale-invariant weights. International Conference on
Learning Representations, 2021.
G. Huang, Z. Liu, L. Maaten, and K. Weinberger. Densely connected convolutional networks. IEEE
conference on computer vision and pattern recognition, pp. 4700-4708, 2017.
M. Hutzenthaler, A. Jentzen, and P. E. Kloeden. Strong convergence ofan explicit numerical method
for sdes with nonglobally lipschitz continuous coefficients. The Annals of Applied Probability, 22
(4):1611-1641, 2012.
C.	Hwang. Laplace’s method revisited: weak convergence of probability measures. The Annals of
Probability, pp. 1177-1182, 1980.
10
Under review as a conference paper at ICLR 2022
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. International Conference on Machine Learning, pp. 448-456, 2015.
S. Ioffe and C. Szegedy. Deep residual learning for image recognition. IEEE conference on computer
vision and pattern recognition, pp. 248-255, 2016.
N. Keskar and R. Socher. Improving generalization performance by switching from adam and sgd.
arXiv:1712.07628, 2017, 2017.
D. Kingma and J. Ba. ADAM: A method for stochastic optimization. International Conference on
Learning Representations, 2015.
A. Krizhevsk. Learning multiple layers of features from tiny images. 2009. URL http://www.
cs.toronto.edu/~kriz∕cifar.html.
A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian institute for advanced research). URL
http://www.cs.toronto.edu/~kriz∕cifar.html.
N. V. Krylov. Extremal properties of the solutions of stochastic equations. Theory of Probability
and its Applications,, 29(2):205-217, 1985.
N. V. Krylov. A simple proof of the existence of a solution to the ItO's equation with monotone
coefficients. Theory of Probability and its Applications,, 35(3):583-587, 1990.
L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning
rate and beyond. International Conference on Learning Representations, 2020.
I.	Loshchilov and F. Hutter. Decoupled weight decay regularization. International Conference on
Learning Representations, 2019.
A.	Lovas, I. Lytas, M. Rasonyi, and S. Sabanis. Taming neural networks with tusla: Non-convex
learning via adaptive stochastic gradient langevin algorithms. arXiv preprint arXiv:2006.14514,
2020.
L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning
rate. International Conference on Learning Representations, 2019.
M.P. Marcus, B. Santorini, M.A. Marcinkiewicz, and Ann Taylor. Treebank-3. 1999. URL https:
//doi.org/10.35111/gq1x-j780.
S. Merity, N. S. Keskar, and R. Socher. Regularizing and optimizing lstm language models. Inter-
national Conference on Learning Representations, 2018.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks.
Proceedings of the 30th International Conference on Machine Learning, 2013.
B.	Polyak and A. Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal
oon control and optimization, 30:835-855, 1992.
M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient Langevin
dynamics: a nonasymptotic analysis. Conference on Learning Theory, 2017.
S. Reddi, S. Kale, and S. Kumar. On the convergence of ADAM and beyond. International Confer-
ence on Learning Representations, 2018.
G. O. Roberts and R. Tweedie. Exponential convergence of Langevin distributions and their discrete
approximations. Bernoulli, 2(4):341-363, 1996.
S.	Sabanis. A note on tamed euler approximations. Electronic Communications in Probability, 18
(47):1-10, 2013.
K. Simonya and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
International Conference on Learning Representations, 2015.
11
Under review as a conference paper at ICLR 2022
T.	Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Pro-
Ceedings of the 28th International Conference on Machine Learning, pp. 681-688, 2011.
F. Wenzel, K. Roth, B. Veeling, J. Swiatkowski, L. Tran, S. Mandt, J. Snoek, T. Salimans, R. Jenat-
ton, and S. Nowzin. How good is the bayes posterior in deep neural networks really? International
Conference on Machine Learning, 2020.
A. C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive gradient
methods in machine learning. Advances in Neural Information Processing Systems, 2017.
P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of Langevin dynamics based algorithms for
nonconvex optimization. Conference on Neural Information Processing Systems, 2018.
M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimiza-
tion. Advances in Neural Information Processing Systems, 2018.
R. Zhang, C. Li, J. Zhang, C. Chen, and A. Wilson. Cyclical stochastic gradient MCMC for bayesian
deep learning. International Conference on Learning Representations, 2020.
Y. Zhang, O. D. Akyildiz, T. Damoulas, and S. Sabanis. Nonasymptotic estimates for Stochastic
Gradient Langevin Dynamics under local conditions in nonconvex optimization. arXiv preprint
arXiv:1910.02008, 2019.
J. Zhuang, T. Tang, Y. Ding, S. Tatikonda, N. Dvornek, X. Papademetris, and J. Duncan. Adabelief
optimizer: adapting stepsizes by the belief in observed gradients. Advances in Neural Information
Processing Systems, 2020.
12
Under review as a conference paper at ICLR 2022
Appendix
A Details of the Experiment in Section 2
This section provides the necessary theoretical details of the experiment in Section 2. We continue
to consider the optimization problem (2). One calculates that
u(θ)
θ30 + 7θ2,	∣θ∣≤ ι,
θ30 + 7(2∣θ∣- 1),
∣θ∣ > 1
and
u0(θ)
30θ29 + 2 θ,
∣θ∣ ≤ 1,
30θ29 + 2sgn(θ), ∣θ∣ > 1.
Note that u(θ) and u0(θ) are continuous since U⑴ =limj]i u(θ) = 11, u(-1) = limθ↑-ι = ɪɪ,
u0(1) = limθ]-ι u0(1) = 67 and u0(-1) = limθ↑ι u0(-1) = -67. Therefore, the minimum value
is attained at θ = 0.
To show that G is locally LiPschitz continuous, We check that for ∣θ∣, ∣θ0∣ > 1 and X ∈ Rd,
∣G(θ, χ) - GW, x)| ≤ (2 + 21 χ≤ι)∣sgn(θ) - sgn(θ0)∣ + 30∣θ29 - θ029∣
≤ 34(1 + ∣θ∣ + ∣θ0∣)28∣θ - θ0∣.
For ∣θ∣, ∣θ∣ ≤ 1, we have
∣G(θ,x) - G(θ0,x)| ≤ (2 + 21 x≤ι)∣θ - θ0∣ + 30∣θ29 - θ029∣
≤ 34(1 + ∣θ∣ + ∣θ0∣)28∣θ - θ0∣.
For 网 ≤ 1, ∣θ∣ > 1, we obtain
∣G(θ,x)- G(θ0,x)∣ ≤ (2 + 2lχ≤ι)∣θ — sgn(θ0)| + 30∣θ29 — θ029∣
≤
≤
(2 + 21 x≤ι)∣θ - θ0∣ + 30∣θ29 - θ029∣
34(1 + ∣θ∣ + ∣θ0∣)28∣θ - θ0∣
where the second inequality follows from the following relations
θ - θ0 ≤ θ - 1 ≤ 0,	for θ0 > 1,
0 ≤ θ+ 1 ≤ θ - θ0,	forθ0 < -1.
B	Key Observations from Assumption 3.1 and 3.2
This section introduces some useful general results, that can be obtained from AssumPtion 3.1 and
3.2. Note that some of the below observations can be also found in Zhang et al. (2019) and Lo-
vas et al. (2020). However, to make our PaPer self-contained, we record all the results which are
necessary for the convergence analysis.
Remark B.1. From Assumption 3.1, one observes that for all θ ∈ Rd and x ∈ Rm
∣G(θ,x)l ≤ K(x)(1 + ∣θ∣q),
where K(x) = 2q(Lι(1 + ∣x∣)ρ + |G(0,x)|).
Remark B.2. From Assumptions 3.1 and 3.2, one obtains that
hθ, h(θ)i = hθ, EG(θ,Xo)i + hθ, ηθ∣θ∣2ri ≥ η∣θ∣2r+2 - E[K(X0)]∣θ∣(1 + ∣θ∣q).
Furthermore, for A = E[K (X0)] and B = (3E[K(X0)])q+2η-q-1, it holds that
hθ, h(θ)i≥ A∣θ∣2 - B.
(B.1)
Proposition B.1. (Lovas et al. (2020)) Let Assumptions 3.1 and 3.2 hold. Then, for every θ,
θ0 ∈ Rd,
hθ - θ0, h(θ) - h(θ0)i ≥ -a∣θ - θ0∣2,
where a = LιE[(1 + ∣Xo∣)ρ](1 + 2∣R∣)q-1 and R is given by
R = max
23ST)+1LιE[(1 + ∣Xo∣)ρ]∖ 2r-1 ∕2qLιE[(1 + ∣Xo∣)ρ]∖ 2r
η
η
Under review as a conference paper at ICLR 2022
Proposition B.2. (Lovas et al. (2020)) Let Assumptions 3.1 and 3.2 hold. Then, one obtains that
∖H(θ, x) - H(θ0,x)∣ ≤ L(1 + ∣x∣)ρ(1 + ∣θ∣ + ∖θ'∖)l∖θ - θ'∖, for all θ ∈ Rd andx ∈ Rm,
where L = Li + 8rη and l = 2r + 1.
Remark B.3. From Assumption 3.1 and the definition of H and H∖ C, one obtains that for θ ∈ Rd,
X ∈ Rm and i = 1, 2,…，d,	'
∣H(i)(θ,x) - H黑(θ,x)∣ ≤
+
≤
+
≤
G(i)(θ,x)-
ηθ(i)∣θ∣2r
∣G(i)(θ,χ)∣
η∣θ(i)∣∣θ∣2r
G(i)(θ,x)
1 + √λ∣G(i)(θ,x)∣
θ(i)∣θ∣2r
η-------f=----
1 + √λ∣θ∣2r
√λ∣Gθ(θ,χ)∣
1 + √λ∣G(i)(θ,x)∣
√λ∣θ∣2r
1 + √λ∣θ∣2r
1+ ε + ∣G(i)(θ,x)∣
+____________√λ∣G(i)(θ,x)∣___________
(1 + √λ∣G(i)(θ,x)∣)(ε + ∣G(i)(θ,x)∣)
√λ∣G(i)(θ,x)∣2 + √λ + √λη∣θ(i)∣∣θ∣4r
which implies that
∣H(θ,x) - H.(θ,x)∣2
d i-	-∣ 2
X √λ∣G(i)(θ,x)∣2 + √λ + √λη∣θ(i)∣∣θ∣4r
i=1 L	-
d
3λX ∣G(i)(θ, x)∣4 + 1+ η2∣θ(i)∣2∣θ∣8r
i=1 L	-
3λ](χ ∣G(i)(θ,x)∣2) + d + η2∣θ∣8r+2
3λ ]∣G(θ,x)∣4 + d + η2∣θ∣8r+2
3λ [s∣K(x)∣4(1 + ∣θ∣4q) + d + η2∣θ∣8r+2 .
Remark B.4. From Assumption 3.1 and the definition of H and H", one calculates that for all
θ ∈ Rd and X ∈ Rm,
≤
≤
≤
≤
∣H (θ,x)∣2 = ∣G(θ,x) + ηθ∣θ∣2r ∣2 ≤
≤
2∣G(θ, x)∣2 +2η2∣θ∣4r+2
4∣K (x)∣2(1 + ∣θ∣2q) +2η2∣θ∣4r+2
and
IHλ,c(θ, X)∣2	=
≤
d
X
i=1
G(i)(θ,x)	A	√λ	ʌ	θ(i)∣θ∣2r ]2
1 + √λ∣G(i)(θ,x)∣( + ε + ∣G(i)(θ,x)∣J + η 1 + √λ∣θ∣2r.
d
X
i=1
-∣G(i)(θ,x)∣	√λ∣G(i)(θ,x)∣	∣θ(i)∣∣θ∣2r ]2
.1 + √λ∣G(i)(θ,x)∣ + (1 + √λ∣G(i)(θ,x)∣)(ε + ∣G(i)(θ,x)∣) + η 1 + √λ∣θ∣2r.
≤
≤
≤
≤
X (∣G(i)(θ,x)∣ + √λ + η∣θ(i)∣∣θ∣2rY
i=1 '
3X (∣G(i)(θ,x)∣2 + λ + η2∣θ(i)∣2∣θ∣4r)
i=1 '
3∣G(θ,x)∣2 + 3λd + 3η2∣θ∣4r+2
6∣K (x)∣2(1 + ∣θ∣2q) + 3λd + 3η2∣θ∣4r+2.
2
Under review as a conference paper at ICLR 2022
C Overview of the Proofs
This section provides an overview of the proofs of our main results. We begin by introducing suitable
Lyapunov functions and auxiliary processes to analyze the convergence of our newly introduced
algorithm. For each m ≥ 1, define the Lyapunov function Vm by
vm(θ) :=(1 + ∣θ∣2)号,θ ∈ Rd	(C.1)
and similarly Vm(X) = (1+ x2) mm for any real X ≥ 0. Both functions are continuously differentiable
and lim∣θ∣→∞ VVm(θ)∕Vm(θ) = 0. Also, define Zj = Z∖t, which is the time-changed LageVin
dynamics governed by
dZλ = -λh(Zλ)dt + P2β-1λdBλ	(C.2)
where B) = Bλt∕√λ is a Brownian motion.
We next define the continuous-time interpolation of the new algorithm, see (7), as
dθλ = -λHλ (仅c, Xdte) dt + P2λβ-1dBλ	(C.3)
with initial condition 给 =θʌ. Henceforth, [x[ denotes the integer part of a positive real X and
dXe = bXc + 1.
Remark C.1. Due to the homogeneous nature of the coefficients of the continuous-time interpola-
tion of THεO POULA (C.3) and when one selects a version of the driving Brownian motion such that
it coincides with ξn at grid points, it follows that the interpolated process (C.3) equals the process
of THε O POULA (7) almost surely at grid points, i.e. £ = θn (a.s), ∀n ∈ N.
Furthermore consider the continuous-time process ζts,v,λ, t ≥ s which is the solution to the SDE
dζs,v,λ = -λh (Zs,v,λ) dt + P2λβ-1dB)	(C.4)
with initial condition Zs,v,λ := v,v ∈ Rd. Let US also define T := ɪ, which allows US to create suit-
able subintervals on the positive real line in order to compare the behaviour of the aforementioned
processes at each such interval.
Definition C.1. Fix k ∈ N and define Zj,k := ZkT,θkT,λ where ZkT,θkT,λ is defined in (C.4).
To derive non-asymptotic (upper) bounds for W1	L	θtλ	, πβ	and	W2	L	θtλ	,	πβ	, the following
decomposition is used in terms of the auxiliary processes θ), Z},n and Ztλ as follows:
Wj(L (θλ), ∏β) ≤ Wj (L M), L (Zλ,n)) + Wj (L (cλ,n) , L (Zλ)) + Wj(L 3λ), ∏β)
forj = 1,2.
C.1 Primary estimates
We collect first the necessary estimates in order to obtain (upper) bounds for W1 L θtλ , πβ and
W2 L θtλ , πβ . All proofs of the lemmas in this section can be found in Appendix D. The fol-
lowing two lemmas provide, uniform in n, moment estimates of the process (θnλ)n≥1.
Lemma C.1. Let Assumptions 3.1 and 3.2 hold. Then, there exist M0 > 0 and λmax, which is
defined in (9), such that for any λ ∈ (0, λmax) and any n ∈ N,
E%ιl2 ≤ (l — η2 √λ)"E∣θo∣2 + 5M2 + 4√λmaxd (β-1 + 2 + 2λmJ
4(1 + λmax)√dM0	2 ʌ ]
+---------n---------+ 4ηMo √λmax∣
and, moreover,
supE∣θn+ι∣2 ≤E∣θo∣2 +「M2 + 4√λmaxd (β-1 + 2 + 2λmaχ)
nη
4(I + λmax)√dM0	2 八]
+-------------------+ 4ηM0 √λmax .
η
3
Under review as a conference paper at ICLR 2022
Lemma C.2. Let Assumptions 3.1 and 3.2 hold. Then, there exist M0 > 0 and λmax, which is
defined in (9), such that for any λ ∈ (0, λmax), n ∈ N, andp ∈ [1, 8(2r + 1)],
E∣θλ+ιl2p ≤ (1- η2λ)nE∣θλ∣2p + A
and
supE∣θ"ι∣2p≤ E∣θλ∣2p + A2p
nη
where Ap is given in Table 7.
Lemma C.3. Let Assumptions 3.1 and 3.2 hold. Then, there exist M0 > 0 and λmax, which is
defined in (9), such that for any λ ∈ (0, λmax) and n ∈ N,
A
E[%(θAτ)] ≤ 2E∣θo∣4 + 2 + 2T
n	η2
where A2, i.e. Ap forp = 2, is given in Table 7.
Proof. From the definition of the Lyapunov function and Remark C.1, we have
EM(CT)]	= E[(i + BnT I2)2]
≤ 2 + 2EBnT I4
≤ 2 + 2E∣θo∣4 + 2 当.
□
Moreover, the necassary moment bounds hold also for the auxiliary process {4λ,n}t≥nτ.
Lemma C.4. (Lemma 3.5. of Chau et al. (2019)) Let Assumptions 3.1 and 3.2 hold. Then,
E[v2(ζλ,n)] ≤ E[V2 (JO)]+ JR +2(CXη-1 + 2M02 (2 + η) + 2d(ηβ)-1 PλmaX) + 1,
C
E[%(<λ,n)] ≤ 2E∣θo∣4 + 2 + 2 当 + 粤,
η 2	cC(4)
where C(P), C(P) are given in Table 7.
Let Pv2 denote the subset of P(Rd) such that every μ ∈ Pv2 satisfies JRd V2(θ)μ(dθ) < ∞.
Moreover, let the following functional be considered
w1,2(μ, V):=	inf [ [ [1 ∧∣θ - θ∣0][(1 + 匕(θ) + 匕(θ0)) Z (dθdθ0)	(C.5)
ζ∈C(μ,ν) √Rd √Rd
where C(μ, ν) is defined in (4). The following lemma states the contraction property of the Langevin
SDE (C.2) in w1,2, which yields the desired result for W1(L(Znλ), πβ).
Lemma C.5. (Proposition 3.14 of Chau et al. (2019)) Let Zt0, t ∈ R+ be the solution of the Langevin
SDE (6) with initial condition Z00 = θ00 which is independent of G∞ and Iθ00 I ∈ L2 . Then,
w1,2 (L (Z)), L (Zt)) ≤ ce-ctw1,2 (L (θo), L (θ0))
where w1,2 is defined in (C.5).
The following two Lemmas combined establish the required W1 (L(θCtλ ), L(Ztλ )) estimate. One
recalls first that for any t > 0, there exists a unique integer m such that t ∈ [mT, (m + 1)T).
Lemma C.6. Let Assumptions 3.1 and 3.2 hold. Then, for 0 < λ < λmax, one obtains
W2 (L (cλ), L (Cλ,m)) ≤ √λPe3a(Cι + C2 + C3)
where C1, C2, C3 are given explicitly in Table 7.
Lemma C.7. Let Assumptions 3.1 and 3.2 hold. Then, for 0 < λ < λmax, one obtains
Wi (L (Gm) , L (Zλ)) ≤√λzι
where z1 is given explicitly in Table 7.
Lemma C.8. Let Assumptions 3.1 and 3.2 hold. Then, for 0 < λ < λmax, one obtains
W2(L (Cλ,m) , L (Zλ)) ≤ λ4z2
where z2 is given explicitly in Table 7.
4
Under review as a conference paper at ICLR 2022
C.2 Proofs of main results
It is assumed throughout the paper that the random variable θo, Gg := σ (Un∈NGn) and (ξn)n∈N
are independent.
Proof of Theorem 3.1. Observe that Wi (L (θη), L (Zλ)) is decomposed as follows:
Wi (L 冏),∏β) ≤ Wi (L 阐,L (Zn)) + Wi (L (zʌ),邛).
Note that there exists a unique integer m such that n ∈ [mT, (m + 1)T). Thus, from the results of
Lemma C.6 and C.7, the first term in the right-hand side is estimated
Wi (L (θn), L (zʌ))	≤	Wi	(L	(θn), L 篇m)) +	Wi	(L	Qm , L	(zʌ))
≤	W2	(L	冏),L 层,m)) +	Wi	(L	(ζn,m), L	(zʌ))
≤ √λ(√e3-(Ci + C2 + C3) + zi).
Consequently, we derive
Wi (L (θn) ,∏β) ≤ √λ( √e3a(Ci + C2 + c3) + zi)+ wi,2 (L (zʌ) ,∏β)
≤ √λ(√e3-(Ci + C2 + C3) + zi) + ^e-cλnWi,2(θ0, ∏β)
≤
√λ( √e3-(Ci + C2 + C3) + zi)+ ce-mc
+ E[V2(θ0)] +
/
Rd
½(θ)∏β (dθ)
1
where Remark C.5 is used for the first inequality.
□
ProofofCorollary 3.1. Let n ∈ [mT, (m +1)T). Then, Lemma C.6 and C.8 and Remark C.5 yield
that
≤
≤
≤
W2 (L (琉,L (zʌ)) + W2 (L (Zn) ,πβ)
W2 (L 冏),L ©，m)) + W2 (L 层,m), L (zʌ)) + W2 (L (Zn), ∏β)
√e3a(Ci + C2 + C3) √λ + z2λ1 + ,2wi,2(L(Zλ),πβ)
√e3a(Ci + C2 + C3) √λ + z2λ4 + √Ce-cλn∕2 ,2wi,2(θo,∏β)
√e30(Ci +。2 +。3)√λ + z2λ1 + j2^e-cm∕2 (1 + E [V2 (θo)] + /, V2(θ)πβ(dθ)).
□
Proof of Theorem 3.2. We begin by decomposing expected excess risk (10) as follows:
E[u(θn)] - u(θ*) ≤ E[u(θn)] - E[u(θ∞)]+ E[u(θ∞)] - u(θ*).
Let us focus on estimating the first part, E[u(θη)] - E[u(θ∞)]. Observe that for θ ∈ Rd
∣Vu(θ)∣ = ∣h(θ)∣ ≤ ri∣θ∣2r+i + 2E[K(Xo)]
by separating the cases ∣θ∣ ≤ 1 and ∣θ∣ > 1 where ri = E[K(Xo)] + η due to Remark B.1. Then,
we have
U(W) — u(v)
≤
Z hVu((1 — t)v + tw), W — Vidt
o
∣ ∣Vu((1 — t)v + tw)∣∣w — v∣dt
o
αi(1 - t)l∣v∣l + aitl∣w∣l + 2E[K(Xo)] ∣w - v∣dt
≤
o
’7⅜τ∣v∣l + 7⅜τ∣w∣l + 2E[K(Xo)]) ∣w - v∣
∖l + 1 l + 1	J
(C.6)
5
Under review as a conference paper at ICLR 2022
where l = 2r +1 and ai = 2lr1 = 2l (E|K(X0)∣ + η). Let P denote the coupling between μ and V
that achieves W2(μ, V) with μ = £(公)and V = L(θ∞). Then, from (C.6), we obtain
Eu(G) — Eu(θ∞)
=Ep[uM)- u(θ∞)]
≤	EP (l^+11 ∣θn∣l + ∣^+11 ∣θ∞∣l + 2EP[K (XO)]) lθn - θ∞∣
<
,(l-+1 ∣θλ∣l + %∣θ∞∣l + 2E[K(X0)]^2 ,Ep∣en -θ∞∣2
where we have used Lemma C.2 for the last inequality.
We take a similar approach in Raginsky et al. (2017) to estimate the second term. From Equation
(3.18), (3.20) in Raginsky et al. (2017), we obtain
1 / e e-8u(θ)	e - 8u(θ)
Eu(θ∞) - u(θ*) ≤ β (-/4 -ʌ- log -ʌ-d - log A)- u*
≤ 色log(2πe(B + 田3)∖ - logΛ -u*
一 2β I Ad J β
(C.8)
where Λ = JRd e-βu(θ)dθ is the normalizing constant.
Using (B.1), we obtain
(θ*, h(θ*)) ≥ A∣θ*∣2 - B
which yields
∣θ*∣2 ≤ √A∙
Moreover, for W ∈ Rd, we have
u(θ*) — u(w) = Z (Vu(w + t(θ* — w)), θ* — Widt
0
= I (Vu(w + t(θ* — w)) — Vu(θ*), θ* — Widt
0
1	1
=	NU(W	+ t(θ*	—	w))	— Vu(θ*),	W —	θ* +	t(θ*	— w)idt∙
0 t-1
From Proposition B.2, we further obtain
—β(u(θ*) — u(w))=
≤
β ∣ u(θ*) — u(w) ∣
11
β -------Kh(W + t(θ* — w)) — h(θ*), W — θ* + t(θ* — w)i∣dt
0 t—1
≤
≤
βLE(1+ ∣ Xo∣)ρ / (1 + ∣ w + t(θ* — w) ∣ + ∣θ*∣)l(1 — t)∣w — θ*∣2dt
0
βLE(1+ ∣ Xo∣)ρ / (1 + ∣ W∣ + ∣θ* — W∣ + ∣θ*∣)'(1 — t)∣ W — θ*∣2dt
0
βLE(1 + ∣ Xo∣)ρ(1 + 2∣θ*∣ + 2∣θ* — W ∣ )l ∣ W -ʃʃ
where we have used the elementary inequality 0 ≤ ∣ W ∣ — ∣ θ* ∣ ≤ ∣ θ* — W ∣ for the last inequality.
6
Under review as a conference paper at ICLR 2022
Define R0 := max{pB∕A, p2d∕(βLE(1 + ∣X0∣)ρ)} and Br(P) = {x ∈ Rd||x — p∖ > r}. Then,
from the above inequality, one further calculates
logΛ
ɪ
≥
>
≥
—	u(θ*) + 1 log I eβWθ*-U(W))dw
β	JRd
—	u(θ*) + 1log Z e-βLEα+ I Xo DP(I+2 I θ*∣+21 θ*-wDliw-P2dw
β	Rd
—	u(θ*) + 1log L	e-βLEα+ i χoI)P(I+4Ro>iw-P2dw
β	BRo (θ*)
—	u®) + ；log[(条)"2 L	fχ(w)dw
β	L∖βK J	JBRO (θ*)	J
—	"θ*)Tog Q (Kβ) /)
(C.9)
where K = LE(1 + ∣Xo∣)ρ(1 + 4Ro)l and fχ is the density function of a multivariate normal
variable X with mean θ* and covariance 磊I&. Here, the last inequality is obtained from the
following inequality:
/
J BRo (θ*)
fχ (w)dw
P(∣X — θ*∣ >R0)
d
≤
≤
≤
KβR2
1
2(1+4Ro)l
1
2
Combining (C.8) and (C.9), we derive
Eu(θ∞) — u(θ*)	≤
≤
d 1
2β log
2πe(B + d∕β)
Ad
d Ke B
2log τbβ+1	+log2.
(C.10)
1
β
Consequently, from (C.7) and (C.10), we derive
Eu<)—u(θ*)	≤ M5W2(L(θn,∏))
1 d Ke B
+ β [2log( T⅛β + 1))+log2
where M5 =比,旧仇∣2l + A + 诰√E∣O1 + 2E[K(X。)].
□
7
Under review as a conference paper at ICLR 2022
D Proofs of Lemmas in Appendix C
ProofofLemma C.l. Define Gfc(θ, x) = 1+λGff 熊X)I (1 + 二@林/)),which is Part of the
adaptive gradient of Hfc(θ, x). One observes that i ∈ {1, ∙ ∙ ∙ , d}
|G 黑 Gx)I
∣Gθ(θ,χ)I	+ √λ
1 + √λ∣GCi)(θ,x)∣
∣g⑴Gx)I
(ε + ∣G(i)(θ,x)∣)(1 + √λ∣G(i)(θ,x)∣)
≤
≤
ɪ + √λ ∣G"x)∣∕ε
√λ +	1 + |G⑴(θ,x)∣∕ε
√λ + √λ
(D.1)
to obtain
d
X θ⑴∙ G黑(θ,x) + η
i=1
0 m
〉
—
∣θ∣2r+2
1 + √λ∣θ∣2r
)+ η	lθlt2
1 + √λ∣θ∣2r
〉
—
Xlθ"-√λ
√d∣θ∣+η dθ√⅛
for all x ∈ Rm and θ ∈ Rd. Then,
2λE
,当道胱瓜…）〉琛
> -2
+ 2ηλ	l”.
1 + √λ∣θn∣2r
(D.2)
on the other hand, due to (D.1),
∣Hλ,c(θ,x)∣2 = (Hλ,c(θ,x), H"(θ,x)i
d
X (Gfc(θ,x) +
i=1 '
θ⑴ ∣θ∣2r	)2
η 1 + √λ∣θ∣2J
d
X (2∣Gλi,c(θ,x)∣2 + 2η2
i=1 '
∣θ⑺ I2∣θ∣4r ʌ
(1 + √λ∣θ∣2r )2)
2df ɪ + √λ)2 + 2η2	”+2
V√λ	(1 + √λ∣θ∣2r )2
4d(1 + λ) +2η2∣θ∣2------lθC------. (D.3)
λ	(1 + √λ∣θ∣2r )2	'	'
≤
≤
≤
which yields that
2λE -病∣Hλ,c(θn,Xn+ι)∣2 θn
CL(1 + λ2) l 2	λ∣θ∣4r	、
—2λ 2d	+ η —
卜	此12	/ (1 + √λ∣θ∣2r)”
-4λd ( ∣4 ∣2)- 2λη2.	(D.4)
>
>
From (D.2) and (D.4), one calculates that
2λE ]hjθn2, Hλ,c(θn, Xn+1)i - 2∣θλ∣2 IHλ,∕θn,Xn+1)|2 θ/
>
+2ηλ ；
-4λd ⅛K - 2λη2=f(θn).
8
Under review as a conference paper at ICLR 2022
Since f(θ) tends to 2η√λ - 2λη2 as |S| → ∞, there exists Mo > 0 such that
f(。：) ≥ η√λ - λη2 = η√λ(1 - √λη)
for all |公| ≥ M0 and λ < 击.Moreover, for λ ≤ 壶,it can rewritten as there exists M0 > 0 such
that
f 胱)≥ η√λ - λη2 =呼	a)
for all |公| ≥ Mq.
Therefore,
E (2λhθn, HΛ,c(θn, Xn+1)i - X2|HXc(0n,Xn+I)IIl∣θλ l≥M0 聆 ≥ ^^~ 怛汗，
implying that
E 腐+1|21 医∣≥M0 此
=E ](忸汗-2λhθn, Hλ,c(θn, Xn+1)i + X2|HA,c(%, Xn+1)|2 + 瓦 &+1|2) l∣θ⅛∣≥M0 θn
≤	(1-吗)腐|2 + 亨.	(D.6)
Let US consider the case of |公 | < Mq. From the fact that
制Hλ,c (θ,x)i
≤
d
X 那)∙ Gλi,C(θ,x)+ η
i=1
网2+
1 + 6怛产
卡 + √λ) + η
≤
通|。| + η
怛产+2
'i+√W7
网2「+2
1 + 6网2,
(D.7)
and (D.3), it can be shown that
E 腐+1|21 医 ∣<M0 θλ
=E 汗-2λhθλ,Hl,c(θn,Xn+1)i + X2|Hl,4n,Xn+1 )|2 + 争 ξn+1f) 1 ∣ θʌ ∣<M。 θ
≤ 汗+与]医 ∣<M0 + E ] (2λKθn, H"(% Xn+1)i| + 入2队。(公,Xn+1)|2) 1 ∣ θʌ ∣<M0 卜:]
≤ 腐|2 + 2λd +2 (√λ + λ3) √dMo + 2η√λM0 + 4d(λ + λ3) + 2η2Ml2λ
≤	(1 - η√√λ)腐|2 + √λ(5ηM2 + 2√λd + 2(1 + λ)√dMo + 4d(√λ + λ2) + 2η2Mg)(D.8)
Consequently, (D.6) and (D.8) yield that
E %1|2 θ
As a result,
E %1|2
βη
9
Under review as a conference paper at ICLR 2022
□
ProofofLemma C.2. For any integer P > 1, ∖θλ+ι∖2p is written as
l%ι∣2p
(1△n|2 + 、 lξn+1∖2 + 21\,
P
where △=碟-》%《(公,Xn+ι). Then, we obtain
2
E[∣θn+1∣2p%]
≤
+
≤
≤
2λ
E ] (Wf + J β -βξin+l + 2On, j 万 ξn+1i) θn
β
Σ
k1+k2 + k3=p
p!
七隹限!
E ∣∆n∣2k1
E[∣∆n∣2p∣θN+2pE ∣∆n∣
2p
X优
k=2 '	∙
>n+1 j2k2(2h∆n,Rn+θ)k3
2P-2h∆n,
^βξn+1i θn
E 卜∆n∣2p-k J J2βλξn+1 J,
2(PT) / 9	.
E[∣∆n∣2p∣θn]+ X (l +p2
l=0
(D、2(P-I)
E[∣∆n∣2p∣θn]+(2P)	X
∖	) l=0
E (∣∆n∣2(PT)TJ L+1
E ∣∆n∣2(pTI
q-1
公
~β^ L+T θn
2βλξn+1 J) 2βλ ∣ξn+1∣2 J θn
E[∣∆n∣2p∣G]+22p-3p(2p- 1)(E[∣∆n∣2p-2∣θn]2βd + (2βλ )%七+产
(D.9)
Define ∣∆n∣2 =破2 + Tn whereTn =心(公,母,°然,X-Qi + λ2∣Hλ,c(θn,Xn+ι)∣2 to write
Eb∆n∣2p eA	= X (k) ∣θn∣2(P-k)E[『k∣G]
k=0
=∣en∣2p + p∣e汗p-2E[rn∣en] + X (Q 腐^力啊琮腐].(d.10)
k=2
Now, we focus on the case where 砌 > M where
M := max ʃM0,1, 2√λmx" + λ辿
(2 - V λmaxn)η
(1 + λmax) Vd 22p-2p(2p - 1)d
η(2 — η)	， ηβ
and M0 is defined in the proof of Lemma C.1. We need the following relations to estimate the
moments of rn for all x ∈ Rd and ∣θ∣ ≥ M,
λ2∣Hλ,c(e,x)∣2 ≤ 4d(λ + λ3) + 2η2λ∣e∣2 (I+*；产)2
≤ 4dλ(1 + λ2) + 2η2λ∣e∣2
≤ 4dλ(1 + λ2)∣e∣ +2η2λ∣e∣2
≤ 2√λη(至。%±』+ pmaXη)∣e∣2
≤ 2√Xηj 2d√λmM1η+ λmaX) + √λmθXη)∣e∣2
≤ 4√λη∣e∣2
(D.11)
10
Under review as a conference paper at ICLR 2022
where we have used the inequality (D.3), 0 ≤ η < 1 and
2√ 2 2√λmaxd(1 + λmax)一
M > --------『--------⇔
(2 - V λmaxη)η
2d√λmaX(I + λmax)
Mη
<2
and note that
2√λmaxd(l + λmaχ)
(2-√λmaxn)n
is finite due to λmax being less than A. Moreover, from (D.7), we
have the following inequality
∣2λ〈/H"(θ,x)i∣ ≤
≤
≤
≤
≤
2(√λ + λ1.5)√d∣θ∣ + 2η√λ∣θ∣2	2,
1 + √λ∣θ∣2r
2√λ(1 + λ)√d∣θ∣ +2η√λ∣θ∣2
2√λη((1 + ⅛x )√d + η)∣θ∣2
2√Xηj(1 + Mηx )√d + ηj∣θ∣2
4√λη∣θ∣2
(D.12)
where the last inequality holds since
M >
η(2 — η)
(1 + λmax)√d	∖
(—M一+η
≤ 2.
⇔
Thus,琛 can be written as
叫“>m}∣rn∣k腐]=’1{1…}(-2λ冏,Hλ,c既,Xn+i)〉+ Λ2∣Hλ,c(θn,Xn+1)∣2)卜
≤ E 1{ | θλ | >M }(∣2λ 优，Hλ,c (θn,Xn+l )i∣ + λ2∣Hλ,c(θn,Xn+l)∣2)的
≤ e[1{∣θλ I >m}(8√λη∣θn∣2)k θn∖ ≤ λ2(8η)k∣θn∣2k.
Moreover, (D.5) implies that
E[1{ I θλ I >M }rn∣θn] ≤ — n2~ ∣θn∣2,
equivalently,
p∣θn∣2p-2E[1{∣θλ∣ >M}rn∣θn] ≤ -p吟 ∣θn∣2p.
(D.13)
Using (D.13), the L2p-norm of An conditional on θn > M is given by
e[1{ ∣θλ I >M}∣An∣2p θ"	≤ ∣θn∣2p + p∣θn∣2p-2E[1{θλ>M}rn∣θn] + X (k) ∣θn∣2(P-k)E[1{θλ>M}∣rn∣k∣G]
-	」	k=2 ` /
≤	∣G∣2p-pη√2λ∣θn∣2p+X (k)腐∣2(p-k)λ2(8η)k∣G∣2k
k=2 `)
≤ ∣G∣2p-pη√2λ∣θn∣2p+∣θn∣2pX ¢) λ2.	。⑷
k=2 ` /
Choose λ such that
λ ≤
1	_	1	1
(27ηpcdP])2	28(8η)2pc2pe ≤ 2&(8η)2(pC/Pl)E-T
2	I 2 I
11
Under review as a conference paper at ICLR 2022
which is equivalent to
k — 1	1
λ F ≤ —---------------
-24(8η)ipq Pe
=	η
2(8η)kpCd Pe
for all integer 2 ≤ k ≤ p. Then, since the following inequality can be obtained
p
EPCkλ2 (8η)k	≤
k=2
≤
p
EPCd P ]λ 2 (8η)k
k=2
1	P
2	X √λη
k=2
Cn
≤ (1 - η√λ)∣θN2p
we have
E 1{∣θλ∣>M}∣∆n∣2p %
and
e[1(∣θλ∣>M}∣∆n∣2p-2 . ≤ (1 -η√λ)腐∣2(P-2) ≤ 击(1 -η√λ)∣θn∣2p.	(d.16)
(D.15)
By combining (D.9), (D.16) and (D.15), we derive
E[1(∣θλ ∣>M}∣%ι∣2p∣θN ≤	(1-n√λ)∣θn∣2p
+ 22p-2p⅞7 W(1 -n√λ)M∣2p + 22p-3p(2p - 1)(2λ)pE∣ξn+ι∣2p
≤ (1 - n√λ)(1 + 22p-2p¾7 1"")∣G∣2p
Me
+ 22p-3p(2p - 1)(2λ丫6陶+产
≤ (1-n2λ)∣G∣2p + 22p-3p(2p - 1)(：/四岛+产	(D.17)
where we used the fact that M ≥ * 飞丁、" for the last inequality.
Consider the case of ∣θη∣ ≤ M. By observing that from (D.3)
1{∣θ ∣≤M}λ2∣Hλ,c(θ,χ)∣2 ≤ λ(4d(1 + λmaχ) + 2n2M2)
and
1{∣ θ∣≤m}∣2λhθ, Hλ,c(θ,x)i∣ ≤ 2λpHq∣Hλ,c(θ,x)∣
≤ 2λ√My∣G(θ, x)∣ + d√λ + 2ηM2r+1
≤ 2λ√M ∖J∖K (x)∣(1 + M q) + d√λ + 2ηM 2r+1,
12
Under review as a conference paper at ICLR 2022
it can be shown that
E 1{∣θ⅛∣≤M}∖rn∖k 第
≤
+
≤
E i{∣θλ∣≤M}(∖2λen, H"(G,Xn+i)〉\+λ2∖Hλ,c(G,χn+ι)，θn
E [1{ ∣ θλ ∣ ≤M} 22λ√M/K(Xn+1)(1 + Mq) + d√λmax + 2ηM2r+1
λ(4d(1 + Amax) + 2η2M 2))
D k λk
^n
where Dk = 2k-1 ((2√M )k (E[K(X0)](l + Mq) + d√λmaχ + 2ηM 2r+1)k/2 + (4d(1 + Amax) +
2η2M2)k 1. Hence, one calculates that
E[1{ ∣θλ ∣≤M }∖∆n∖2pθn	≤
≤
and
E [1{θ⅛≤M}∖∆n∖2p-2 K
mn∖2p+X (k) mn∖2(P-km[i{θλ≤M }∖rn∖k 腐]
k = 1 ∖)
(1 -η2λ)∖θn∖2p + η2AM2p + M2pλ XX (p) λk-1DDk
k=1 ×)
≤ X (P - 1) ∖θn∖2(PT-k)E[1{ ∣θλ ∣≤m}∖rn∖k腐]
k=0 '	)
≤ M2p-2 X (k) Dkλk.
k=0 ')
Consequently, we obtain
E[i{∣θλ ∣≤m}∖θn+1∖2p∖θn]	≤
+
+
(1 -η2λ)∖θn∖2p + η2AM2p + AM2p E (p) λk-1Dk
k=1 ')
A22p-2p(2p - 1)M2p-2 X (k) AkDk
P	k=0 ×)
22p-3p(2p - 1)(2A)PE∖ξn+1∖2p.
(D.18)
By defining
AP = η2M2p + M2p X (k) Am-XDk
k=1 ∖)
+ 22p-3p(2p- 1)(2dMpz2 E (k) AkDk + 2 (勺)p 1dp(2p- 1)!!)
we conclude that
E∖%1∖2p ≤ (1 - η2A)E∖θn∖2p + AAP
∞
≤ (1 - η2A)nE∖θλ∖2p + AAp E(1 - η2A)j
j=0
A
≤ (1 - η2A)nE∖%∖2p + T.
η2
□
13
Under review as a conference paper at ICLR 2022
ProofofLemma C.6. We begin by observing that
E附-ζλ,m∣2 = -2λ ∕[ES,m - θj,h(ζλm)- Hι(%,XQids
=-2λZtTEK"' - G, h(Zλ,m) - h(∕))ds
-2λ j*λm -外，h(Q - h(优j)〉ds
-2λ [τ E4,m - θj, h(θjsj) - H(g3j, X[s] )〉ds
-2λ /] E4,m - θj, H(优j , X[s]) - H(优j , X[s] '))ds
≤ 2λα ∕]Ekλ,m -■∣2ds
+ λ It Ekλ,m - θj∣2ds + Zt 2λE∣h(砂)-h(峪j)∣2ds
2 mT	mT a
+ /] ( - 2λEhZλ'm - θj, h(优J)- H(优j , X[s] )ζj ds
λα I'	_、	_、C	ft 2λ
+ 为	ERm - θj∣2ds +	丁 E∣H Msj,X[s])-Hλ,c(θλ3j,Xrsl)∣2 ds
2 mT	mT a
3λa It EKj,m - g"2ds + Zt A),m + Bj，m + Djmds
J mT	mT
(D.19)
where we have used Proposition B.1 and the Young,s inequality in the first inequality and
B
I
D
∣λ,m
't
)λ,m
t
2λ
瓦E∣h(θj) - h(θλtj)∣2
-2λEθm -碎，h(仅J) - H(碓j, X「t])〉
2λ	, 一、	一、	八
^aEIH(θ[tj,Xdt])-Hλ,c(θ[tj,Xdt])∣.
In addition, from the definition of θλ and (D.3), we have
∣碎-峪j∣4
≤
≤
≤
≤
t
(λ∕
∖人邙
4
which yields
1 方
where Ci
H"WλφX⑸)ds +/2λβτ∣Bλ - B1|
8λ2	λ2∣Hλ,c(碓j,X「t])∣4 + 4β-2B - BjtJ ∣4
8λ2 (4d(1 + λ2) + 2η21% ∣2)2 +4β-2 ∣B) - B[j ∣4
λ225 23d2(i + λ4) + η4叱 ∣4 + β-2∣Bλ - BttJ |4
Je∣G- %∣4 ≤ Ciλ
(D.20)
25/2,8d2(1 + λ3χ) + η4(E∣θ0∣4 + A2∕η2) + β d2.
14
Under review as a conference paper at ICLR 2022
Using Proposition B.2, A：,m can be bounded as follows:
≤
≤
≤
2λ
"Lχ E[(l + ∣θλ∣ + MtJ)2'∣θλ-θ[tj∣2]
?Lx,E(1 + 南 + ∣弓j)4l,E∣4-监j∣4
?Lx 32',(1 + E∣[∣4'+ E"j∣4') JE所而
Ci λ2
(D.21)
where Lχ = L222ρ-1 (1 + E∣X0∣2p) and Ci = ∣LX9l Vz(1 + 2E∣给∣4l +2等)Ci and (D.20) is
used for the last inequality.
To estimate B>m, we observe that
BP = -2λE0m -苑J, h(苑J)- H(峪j, Xdte ))
-2λEg
≤ -2λE E
—
θ, h(弓J)- H(*X「t])》
Om - StJ, h(θλtj) - H(θλtj,Xdte)i Qm
-2λE
(83 - 昭 h(^ʌtj)- H(Strx「t]))
≤ -2λE hλ/ JH入(SSJ,χds])ds - y 至Bi-btj, h(GtJ)- H(StJ,X「t])》
≤ -2λ2E (Hλ(gj,Xdte), h(gj) - H(CtJ ,Xdte )i
≤ 2λ2 /E∣Hι(峪j ,Xdt]) ∣2 JE∣h(峪J) - H(峪j ,Xdt]) ∣2
≤ 2λ2/6E∣K(X0)∣2(1 + E∣"tj 陷)+ 3λd + 3η2E∣"tj ∣4r+2,4E∣H(弓j ,X[t] )∣2
≤ 4λ2j6E∣K(X0)∣2(1 + E∣Cλtj ∣2q) + 3λd + 3η2E∣Cλtj ∣4r+2
× ,4E∣K(X0)∣2(l + E∣"tj 陷)+ 2η2E∣维j ∣4r+2
≤ C2λ2
(D.22)
where
×
4y6E∣K(X0)∣2(l + E∣θ0∣2q + Aq) + 3λd + 3η2∣θλtj ∣4r
j4E∣κ(X0)∣2(i + E∣θ0∣2q + A) + 2η2 (e∣ 用 ∣4r+2 + :
+ 2
A2r+1
η2
Note that we have used the independence of % and XdSe to obtain the second inequality, and used
Remark B.4 and Lemma C.2 to calculate the bound of E|Hλ(% ,X「t] )|2 and E|H(gj ,X「t])12.
Moreover, D：m can be estimated as follows, from Remark B.3,
、…	6λ2「	.	.. .	° —、 一1
D；m ≤ ɪ 8E∣K(X0)∣4(1+ E∣θjtj∣4q)+ d + η2E∣θλtj∣8r+2
≤ C3λ2	(D.23)
where the independence of "§J and XdSe is used and C3 is given by
a
6
一 8E∣K(X0)∣4(1+ E∣θλ∣4q + A2q/η2)+ d + η2(E∣θλ∣8r+2 + A4r+i∕η2).
15
Under review as a conference paper at ICLR 2022
Plugging (D.21), (D.22) and (D.23) into (D.19), one can derive
四优—W,m∖2 ≤ 3λa/ E∖θλ — ξλ'm∖2ds + / (C1 + C2 + C3)λ2ds
≤ 3λα∕ E∖θλ —已m∖2ds +(Ci + C2 + C3)λ< ∞
where the second inequality follows from the fact that (t — mT) ≤ T = ɪ and the use of Grown-
wall,s inequality gives
E∖θλ - 0'm∣2 ≤ cλ
where C = e3a(C1 + C2 + C3).
□
ProofofLemma C.7. Since Zjλ =，0 and t ∈ [mT, (m + 1)T), we can write
m
Wi (L (?m) , L (Z")	≤ E Wi (L (?k) , L (Zy-i))
k=1
m
≤ E w1,2 (L (?k) , L (3-))
k=1
where we have used the fact Wι(μ, V) ≤ w1,2(μ, V) for μ,ν ∈ Pv2 for the second inequality. Using
Remark C.5 and λ(t — kT) ≥ Im — k, we further have
w1,2 (L (yk) , L 俨T)) ≤
^e-cλ(t-kτ)
≤

≤
E1 + V2(%) + V2(4#-1
2
)
≤
×
^e-c(m-k)W2(L(碎T), L(〈*-1))
1 + √E[¼(^λτ)] + √E[¼(ζλTk-1)]
≤
ce
-c(m-fc)√λ√e3a(Cι + C2 + C3) 1 +j2E∖θ0∖4 + 2 + 2 A
A2
+
j2E∣θ0∣4+2+2 A2+CB
(D.24)
where the Cauchy-Schwarz inequality is applied to the third inequality and Lemma C.3, C.6 and C.4
are used for the last inequality. By combining the two inequalities above, we obtain
Wi (L (?m) , L (Z")	≤
'√λp e3α(Cι + C2 + C3) 1 + 2 2E∖θ0 ∖4 + 2 + 2 —2
A2
C
+
≤
[2E∖θ0∖4 + 2 + 2A + C∣∙j Xe-i)
Z1 √λ
where	zι
J2E∖θ0∖4 + 2 + 2 A
1-exp(-C)
T≡Σ
+ 44) ∙
P e3α(Ci + c2 + C3)1	+	J2E∖θ0∣4 + 2 + 2 A	+
□
16
Under review as a conference paper at ICLR 2022
Proof of Lemma C.8. We begin by observing that
W2 (L (Zλ,k) , L 0ι) ) ≤
J2w1,2 (L (?k) , L (I"-1))
≤
λ1/4e-c(m-k)/2 C√e3a(C1 + C2 + C3)
A2
2E∣θo∣4 + 2 + 2 一
η2
+
自仇14+2+2 A2+H)「
where We have used the fact W2 ≤ ,2wι,2 for the first inequality, and the second inequality follows
from (D.24). Consequently, we derive
W2 (L (Zλ,m) , L (Zλ))	≤
m
X W2 (L (Ztλ,k) , L (?k-1))
k=1
≤
+
≤
λ1/4 C√e3α(C1 + C2 + C3)
j2E|&|4+2+2 η2+
λ1/4 Z2
A2
2E∣θo∣4 + 2 + 2 -4
η2
2m
X e-C(m-k)∕2
k=1
where
Zz=√ce 1 m? K1+S2E 仇 |4+2+2 A+S2Eiθ0i4+2+2 A2+c(4))
□
E Details of Experiments
E.1 Image Classification
The experiments are exactly replicated in the official implementation of Zhuang et al. (2020). More
specifically, VGG11, ResNet34 and DenseNet121 are trained for 500 epochs. We apply a weight
deacy of 0.0005 and decay the initial learning rate by 10 after 150 epochs to all optimizers. Batch
normalization proposed in Ioffe & Szegedy (2015) is employed to prevent the models from overfit-
ting and boost the training speed for all three models. The batch size is 128.
Regarding hyperparameter values of Adam, AdaBelief, AdamP, AdaBound, AMSGrad and RM-
SProp, the best hyperparameters are used across all the experiments in Luo et al. (2019) and Zhuang
et al. (2020), which are λ = 0.001, β1 = 0.9, β2 = 0.999 and ε = 10-8. For SGD, we set the
momentum to 0.9 for SGD. For THεO POULA, the best hyperparameters are λ = 0.1, ε = 0.1 and
β= 1012.
Figure 2 shows test accuracy for VGG11, ResNet34 and DenseNet121 on CIFAR-10 and CIFAR-
100.
E.2 Language Modeling
We conduct language modeling for Penn Treebank (PTB) data set. For this task, we train the AWD
LSTM of Merity et al. (2018) for 750 epochs. The details of models can be found in the official
implementation of AWD-LSTM 1.
For NT-ASGD and averaged THεO POULA, the constant learning rate of 30 is used for 2 and 3-
layer LSTMs. For 1-layer LSTMs, we set to 10. ε = 100 and β = 1010 are set across all the
experiments.
1https://github.com/salesforce/awd-lstm-lm
17
Under review as a conference paper at ICLR 2022
(a) VGG11 on CIFAR10
(b) ReSNet34 on CIFAR10
TEST ACCURACY
(c) DenseNet121 on CIFAR10
(d) VGG11 on CIFAR100
(e) ReSNet34 on CIFAR100
(f) DenSeNet121 on CIFAR100
Figure 2: TeSt accuracy for VGG11, ReSNet34 and DenSeNet121 on CIFAR-10 and CIFAR-100.
THεOPOULA* and τHεOPOULA* represent the performances of THεO POULA under the best
and Second beSt hyperparameterS, reSpectively.
For AdaBelief, we used the best hyperparameters reported in Zhuang et al. (2020). Thus, we obtain
the same results of AdaBelief in Zhuang et al. (2020). Specifically, we set β1 = 0.9, β2 = 0.999,
ε = 10-12 and an initial learning rate of 0.01 for 2 and 3-layer LSTMs. λ = 0.001 and ε = 10-16
are used for 1-layer LSTMs.
The averaging is triggered when no improvement has been made for 5 consecutive epochs for THεO
POULA. Also, AdaBelief uses a development-based learning rate decay, which decreases the learn-
ing rate by a constant factor δ if the model does not attain a new best value for k epochs. We
have searched the optimal hyperparameters for the development-based learning rate decay among
δ = {0.1, 0.5} and k = {5, 10, 20}. We have found δ = 0.1 and k = 5 yield the best performance.
Figure 3 displays test perplexity for different AWD-LSTM models on PTB.
Figure 3: Test perplexity for 1, 2 and 3-layer AWD-LSTMs on PTB
E.3 Effectiveness of the boosting function
This subsection empirically tests the effectiveness of the boosting function in our algorithm. THεO
POULA without the boosting function updates the parameter as follows:
θ0 := θ0,	θλ+1 := θλ - λHλ,c (θλ,Xn+ι) + p2λβTξn+ι,	n ∈ N,
18
Under review as a conference paper at ICLR 2022
where Hλ,c ：= (H黑(θ, x),…,破dC(θ, X))T is given by
H⑺(θ x) =______G(C) (θ,x)___+ η
λ,c ,	1 + √λ∣G(i)(θ, x)|	0
θ(i)∣θ∣2r
1 + √λ∣θ∣2r ,
(E.1)
and {ξn}n≥1 is a sequence of independent standard d-dimensional Gaussian random variables.
Indeed, this is a special case of THεO POULA with ε = ∞. We train VGG11, ResNet34 and
DenseNet121 on CIFAR-10 and CIFAR-100 using the iterating rule of (E.1). The hyperparameters
are the same with the best hyperparameters of THεO POULA. As Table 4 shows, THεO POULA
without the boosting function is worsen than THεO POULA. The result indicates that the addition
of the boosting function leads to a meaningful increase of test accuracy, validating that the boosting
function works well for the sparsity of neural networks as expected.
Table 4: The best accuracy for VGG11, ResNet34 and DenseNet121 on CIFAR-10 and CIFAR-
100. THεOPOULA* represents the performances ofTHεO POULA with the best hyperparameters.
THεOPOULA(ε = ∞) means the performance without the boosting function with the same hyper-
parameters.
dataset		CIFAR-10					CIFAR-100			
model	VGG	ResNet	DenseNet	VGG	ResNet	DenseNet
THZOPOULA	92.10	95.43	-95.66	70.31	77.53	-79.90^^
THεOPOULA(ε = ∞)	91.48	94.93	95.26	68.11	75.91	77.99
F Additional experiments
F.1 EFFECT OF β ON THE PERFORMANCE OF THεO POULA
This subsection examines the effect ofβ on the performance of THεO POULA. We conduct exper-
iments for VGG11 and ResNet34 on CIFAR10 and CIFAR100 with different values of β ranging
from 104 to 1012. As shown in Table 5, THεO POULA achieves the highest accuracy when β is
108 〜1012, which is consistent with the phenomenon, so called the cold posterior effect, see Wenzel
et al. (2020) and Aitchison (2021).
Table 5: The accuracy for VGG11 and ResNet34 on CIFAR-10 and CIFAR-100. We use the best
hyperparameters for λ and ε in Appendix E.1.
			β					
model	dataset	104	106	108	1010	1012
VGG	CIFAR10	73.10	91.53	92.31	92.29	92.10
		(0.407)	(0.141)	(0.055)	(0.120)	(0.023)
	CIFAR100	20.69	70.0-	70.28	70.16	70.31
		(0.718)	(0.343)	(0.124)	(0.110)	(0.117)
ResNet	CIFAR10	80.84	94.67	95.42	95.34	95.43
		(0264)	(0145)	(0.117)	(0.141)	(0.095)
	CIFAR100	63.58	77.22	-77.4-	77.6	77.53
		(0103)	(0.291)	(0.036)	(0.208)	(0.143)
F.2 EXPERIMENTS WITH η 6= 0
We conduct additional experiments with η 6= 0 to demonstrate that there is no gap between theory
and practice of our work. When the regularization parameter r is large (possibly, overestimated) and
the dimension of θ is big, | θ |2r becomes substantially huge. As a result, the stochastic gradient of the
regularization term, η ,^^^,in (8) will approximately behave like √λθ(i), which is equivalent
to '2-regularization. In all the numerical experiments in Table 2, we applied a weight decay with
5 X 10-4 for image classification and with 1.2 X 10-6 for language modeling. That is, by choosing
η = 5 x 10-4 √λ and large r, one can obtain accuracy of models with '2-regularization.
19
Under review as a conference paper at ICLR 2022
Table 6 shows that the accuracy for VGG, ResNet and DenseNet on CIFAR-10 and CIFAR-100 With
r = 10 and η = 5 X 10-4 Vλ. One observes a very similar performance by THεO POULA as in
Table 2 without any noticeable loss of accuracy.
Table 6: The accuracy for VGG11, ResNet34 and DenseNet121 on CIFAR-10 and CIFAR-100. We
use the best hyperparameters reported in Appendix E.1 with r = 10 and η = 5 × 10-4√λ.
dataset		CIFAR-10			CIFAR-100		
model	VGG	ReSNet ∣ DenSeNet	VGG	ReSNet	DenseNet
THεO POULA (η = 0)	92.2	95.38 I 95.69	70.07	77.78	80.47
F.3 Sampling from a synthetic multi-modal distribution
We provide a brief overview of recent progress on sampling and Bayesian neural networks to de-
scribe the potential of THεO POULA as a sampling algorithm. Deng et al. (2020b) proposed an
adaptive MCMC algorithm, called CSGLD, which uses a scalable dynamic importance sampler to
flatten the target distribution and reduce the energy barriers to escape local optima. Deng et al.
(2020a) devloped reSGMCMC motivated by replica exchange monte carlo algorithm. In particular,
reSGMCMC obtained the state-of-the art results on CIFAR-10, CIFAR-100 and SVHN in Bayesian
neural networks. Zhang et al. (2020) developed cyclical stochastic gradient MCMC with a cyclical
stepsize schedule.
We replicate the simulation ofa synthetic multi-modal distribution in Deng et al. (2020b) to evaluate
the performance of THεO POULA as a sampling method. A target distribution is π(x) X e-U(X)
where U(x) = P2=ι x(i) T0c3s(1.2nx(i)) and X = (x(1), x(2)). Detail of the setting in the experi-
ment such as hyperparameters, regularizer, training epochs can be found in Appendix D.3 of Deng
et al. (2020b). THεO POULA is compared with SGLD, CSGLD (Deng et al. (2020b)), reSGLD
(Deng et al. (2020a)), cycSGLD (Zhang et al. (2020)). For THεO POULA, we used λ = 0.05,
ε = 1 and T = 0.3. A resampling scheme is used for CSGLD. Figure 4 illustrates that THεO
POULA recovers the target multi-modal distribution successfully without the local trap issue ob-
served in SGLD and cycSGLD.
Figure 4: simulations of a multi-modal distribution.
20
Under review as a conference paper at ICLR 2022
G Table of Constants
Table 7 displays full expressions for constants which appear in the main results of this paper. In
addition, Table 8 shows all main constants and their dependencies on key parameters such as d, β,
the moments of K(X0) and η.
Table 7: Explicit expression for constants with c and C from Proposition 3.14 of Chau et al. (2019).
SYMBOL			Full Expression				
M	max		c∣Mo, 1	2√λmaχd(1 + λmaX) (1 + λmaχ)√d 22p—2p(2p—1)d ] (2 — √λmaxn)n	,	n(2 —n)	,	nB	/				
认		2k-1	(2λmaχ√M )k(E[K(Xo)](1 + Mq) + d√λmaχ + 2ηM 2r+1)k/2 + (4d(1 + λmaχ ) + 2η2M 2)1, k = 1,…，8(2r + 1)				
AP	+ 22p-3p(2p-		η2M2p + M2p PP=1 (P) λm-χDk -1)( 2 吗。—2 Pk = O(P) λmaχ 认 + 2 ( 2Vχ )"1dp (2p -			1)!!),	
				for P = 1, ∙ ∙ ∙ ,8(2r + 1)			
MP			q	+ 4B∕(3A) + 4d∕(3Aβ) + 4(p - 2)∕(3Aβ)			
c(P)				字,P =1,…，8(2r +1)			
C(P)				4APvP(Mp), p = 1,…，8(2r +1)			
Ci			L2220+5/2321 (1 + E∣Xo∣2ρ) J(1 + 2E∣goΓl + 2攀)				
			×∕8d2(1 + λmaχ) + η4(E∣θo∣4 + A2∕η2) + M				
C2		4,6E∣K(Xo)∣2(1 + E∣θo∣2q + 今)+ 3λmaχd + 3η2 |% ∣4r+2					
		× J4E∣K(Xo)∣2(1 + E∣θo∣2q + A) + 2η2 (叫砧四+2 + Ar+ )					
C3	e a	8E∣K(Xo)∣4(1 + E 忸 λ∣4q + A2q∕η2) + d + η2(E∣M∣8r+2 + A4r+1∕η2)				.	
Zi	C√e3α(Cl+C2 + C3) 1 — exp( — c)			1+ ,2E∣θo∣4 + 2 + 2 Al + ,2E∣θo∣4 + 2 + 2 Al + 黯			
							Se
Z2	√Ce3α∕4(Cι+C2 + C3)1/4 1 — exp( — C/2)			(1 + /2E∣θo∣4 + 2 + 212 + ,2E∣θo∣4 + 2 + 2 % + 事			
ai				2l(E[K (Xo)] + η)			
Ro			Ro	:=max{√BTA, √2d∕(βLE(1 + ∣Xo∣")}			
K				LE[(1 + ∣Xo∣)ρ](1 + 4Ro)l			
Mi				(Z1 + √e3α(C1 + C2 + C3))			
M2				宜[1 + E [V2 (θo)] + JRd V2(θ)∏β(dθ)]			
M3				√e3a(C1 + C2 +。3)			
M4				/2C (1 + E [V2 (θo)] + Ad V2(θ)∏β(dθ))			
M5			(l+1 V	l⅛θo∣2l + A + + √E∣θ∞∣2l +2E[K(Xo)])			
Me			2 log (-	A (野 + 1)) - log (1 - e—(RoR-82 )			
21
Under review as a conference paper at ICLR 2022
Table 8: Main constants and their dependency to key parameters
Constant	Key parameters			
	d	β	MOMENTS OF X0	η
A	-	-	O(EK(X0))	-
B	-	-	O(EK(X0)q+2)	O( η⅛)
R	-	-	O(EIX0lρ)	O((η2hq)
a	-	-	O(E|Xo|p(q-1))	O( η(2")(q-i))
Ap	poly(d)	O（d）	O(EK|X0|p/2)	O（≠2）
C	O(e-d)	Inherited from contraction estimates in Eberle et al. (2019)		
C	O(ed)	Inherited from contraction estimates in Eberle et al. (2019)		
22