Under review as a conference paper at ICLR 2022
On the Convergence of Shallow Neural Net-
work Training with Randomly Masked Neu-
RONS
Anonymous authors
Paper under double-blind review
Ab stract
Given a dense shallow neural network, we focus on iteratively creating, train-
ing, and combining randomly selected subnetworks (surrogate functions), towards
training the full model. By carefully analyzing i) the subnetworks’ neural tangent
kernel, ii) the surrogate functions’ gradient, and iii) how we sample and combine
the surrogate functions, We prove linear convergence rate of the training error -
within an error region- for an overparameterized single-hidden layer perceptron
With ReLU activations for a regression task. Our result implies that, for fixed
neuron selection probability, the error term decreases as we increase the number
of surrogate models, and increases as we increase the number of local training
steps for each selected subnetwork. The considered framework generalizes and
provides new insights on dropout training, multi-sample dropout training, as well
as Independent Subnet Training; for each case, we provide corresponding conver-
gence results, as corollaries of our main theorem.
1 Introduction
Overparameterized neural networks have led to unexpected empirical success in deep learning
(Zhang et al., 2021; Goodfellow et al., 2016; Arpit et al., 2017; Recht et al., 2019; Toneva et al.,
2018), but also have led to new techniques in analyzing neural network training (Kawaguchi et al.,
2017; Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2018; Liang et al., 2019; Arora
et al., 2018; Dziugaite & Roy, 2017; Neyshabur et al., 2018; Zhou et al., 2018; Soudry et al., 2018;
Shah et al., 2020; Belkin et al., 2019; 2018; Feldman, 2020; Ma et al., 2018; Spigler et al., 2019;
Belkin, 2021; Bartlett et al., 2021; Jacot et al., 2018). While there is literature that focuses a diverse
set of overparameterized neural network architectures (Frei et al., 2020; Fang et al., 2021; Lu et al.,
2020; Huang et al., 2020; Allen-Zhu et al., 2019a; Gu et al., 2020; Cao et al., 2020) and training
algorithms (Du et al., 2018; Zou et al., 2020; Soltanolkotabi et al., 2018; Oymak & Soltanolkotabi,
2019; Li et al., 2020; Oymak & Soltanolkotabi, 2020), most efforts fall under the following sce-
nario: in each iteration, all parameters of the neural network are updated using a version of gradient
descent. Yet, advances in regularization techniques (Srivastava et al., 2014; Wan et al., 2013; Gal
& Ghahramani, 2016; Courbariaux et al., 2015; Labach et al., 2019), computationally-efficient
(Shazeer et al., 2017; Fedus et al., 2021; Lepikhin et al., 2020; LeJeune et al., 2020; Yao et al.,
2021; Yu et al., 2018; Mohtashami et al., 2021; Yuan et al., 2019; Dun et al., 2019; Wolfe et al.,
2021) and communication-efficient distributed training methods (Vogels et al., 2019; Wang et al.,
2021; Yuan et al., 2019; Dun et al., 2019; Wolfe et al., 2021) deviate from this narrative: One would
-explicitly or implicitly- train smaller, randomly-selected, model versions within the large dense
network, in an iterative fashion. This raises the question:
“Can one meaningfully train an overparameterized ML model
by iteratively training and combining together smaller versions of it?”
We provide a positive answer to this question for single-hidden layer perceptrons with ReLU activa-
tions. This is a non-trivial, non-convex problem setting, that has been used extensively in proving the
behavior of training algorithms on neural networks (Du et al., 2018; Zou et al., 2020; Soltanolkotabi
et al., 2018; Oymak & Soltanolkotabi, 2019; Li et al., 2020; Oymak & Soltanolkotabi, 2020).
1
Under review as a conference paper at ICLR 2022
Figure 1: Training a single hidden-layer perceptron using multiple randomly masked subnetworks.
Here, f(W, x) denotes the full model with weight W, and fml (W, x) denotes the surrogate model
(subnetwork) with only active neurons dictated by the mask mlk at k-th iteration for subnetwork l.
Let us first describe briefly the scenario considered here; this is depicted in Figure 1. Given a single
hidden-layer perceptron Fig.1(a), we sample masks within one training step Fig.1(c), each mask
deactivating a subset of the neurons of the original network’s hidden layer. In a way, each mask
defines a surrogate model, as in Fig.1(b), based on the original network, leading to a collection of
subnetworks. These surrogates independently update their own parameters (possibly on different
data shards), by performing a number of (stochastic) gradient descent steps. Lastly, we update the
weights of the original network by aggregating the parameters of the subnetworks, before the next
iteration starts. Note that multiple masks could share active neurons. When aggregating the updates,
we take the mean of the updated values across all subnetworks that have these neurons active.
Motivation and connection to existing methods. Standard gradient descent computes updates over
all the weights per iteration, based on a loss function. The present methodology computes updates
over weight subsets, based on the generated masks: i.e., the weight updates relate to different loss
functions, each depending on the output of a different subnetwork. When the sampled mask leaves
all neurons active, the training scheme is the same as in training the whole model. Yet, the same
framework connects with existing techniques for training deep neural networks.
Dropout regularization. Dropout (Srivastava et al., 2014; Wan et al., 2013; Gal & Ghahramani,
2016; Courbariaux et al., 2015) is a widely-accepted technique against overfitting in deep learn-
ing. In each training step, a random mask is generated from some pre-defined distribution, and
used to mask-out part of the neurons in the neural network. Later variants of dropout include the
drop-connect (Wan et al., 2013), multi-sample dropout (Inoue, 2019), Gaussian dropout (Wang &
Manning, 2013), and the variational dropout (Kingma et al., 2015). Here, we restrict our attention
to the vanilla dropout, and the multi-sample dropout. The vanilla dropout corresponds to our frame-
work, if in the latter we sample only one mask per iteration, and let the subnetwork perform only
one gradient descent update. The multi-sample dropout extends the vanilla dropout in that it sam-
ples multiple masks per iteration. For regression tasks, our theoretical result implies convergence
guarantees for these two scenarios on a single hidden-layer perceptron.
Distributed ML training. Recent advances in distributed model/parallel training have led to variants
of distributed gradient descent protocols. Instead of centrally aggregating gradient updates per it-
eration, distributed local SGD (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang & Re, 2014;
Zhang et al., 2016) updates all the model parameters only after several local steps are performed
per compute node. This reduces synchronization and thus allows for higher hardware efficiency
(Zhang et al., 2016). Yet, all training parameters are updated per outer step, which could be com-
putationally and communication inefficient. The Independent Subnetwork Training protocol (Yuan
et al., 2019; Dun et al., 2019; Wolfe et al., 2021) goes one step further: it combines model- and
parallel-training methodologies, with local SGD motions to minimize communication and computa-
tional bottlenecks, simultaneously. In particular, IST splits the model vertically, where each machine
contains all layers of the neural network, but only with a (non-overlapping) subset of neurons be-
ing active in each layer. Multiple local SGD steps can be performed without the workers having to
communicate. Yet, the theoretical understanding of IST is currently missing. Our theoretical result
implies convergence guarantees for IST for a single hidden-layer perceptron, and provides insights
on how the number of compute nodes affects the performance of the overall protocol.
Contributions. The present training framework naturally generalizes the approaches above. Yet,
current literature -more often than not- omits any theoretical understanding for these scenarios,
2
Under review as a conference paper at ICLR 2022
even for the case of shallow MLPs. While handling multiple layers is a more desirable scenario (and
is, indeed, considered as future work), our presented theory illustrates how training and combining
multiple randomly masked surrogate models behaves. Our findings can be summarized as follows:
•	We provide convergence rate guarantees for i) dropout regularization (Srivastava et al., 2014),
ii) multi-sample dropout (Inoue, 2019), iii) and multi-worker IST (Yuan et al., 2019), given a
regression task on a single-hidden layer perceptron.
•	We show that the Neural Tangent Kernel (NTK) (Jacot et al., 2018) of surrogate models stays close
to that with initial weights; this implies that it stays close to the infinite width NTK. Consequently,
our work shows that training over surrogate models still enjoys linear convergence.
•	For subnetworks defined by Bernoulli masks with a fixed distribution parameter, we show that
the expectation of aggregated gradient in the first local step is a biased estimator of the gradient
computed on the whole network, with the bias term decreasing as the number of subnetworks
grows. Moreover, the aggregated gradient starting from the second local step stays close to the
aggregated gradient of the first local step. This finding leads to linear convergence of the above
training framework with an error term under Bernoulli masks.
•	For masks sampled from categorical distribution, we provide tight bounds i) on the average loss
increase, when sampling a subnetwork from the whole network; ii) on the loss decrease, when
the independently trained subnetworks are combined into the whole model. This finding leads to
linear convergence with a more desirable error term than the Bernoulli mask scenario.
Challenges. Much work has been devoted to analyzing the convergence of neural networks based on
the NTK perspective (Jacot et al., 2018); see the Related Works section below. The literature in this
direction notice that the NTK remains roughly stable throughout training. Therefore, the network
output can be approximated by the linearization defined by the NTK. Yet, training with randomly
masked neurons poses additional challenges: i) With a randomly generated mask, the NTK changes
even with the same set of weights. Thus, analyzing the convergence of neural networks with masked
neurons requires careful treatment of the surrogate models’ NTKs. ii) Each gradient defined by
the subnetwork is a biased estimator of the gradient computed on the whole network, even in the
first step of the subnetwork update. iii) From the objective perspective, the non-linear activation
function and the gradient aggregation makes the function represented by the updated whole network
not necessarily equal to the linear combination of the updated subnetworks. These three challenges
complicate analysis, thus driving us to treat the NTK, gradient, and combined network function with
special care. We will resolve these difficulties in the proof of the three main theorems.
2	Related Works
The NTK enabled more refined analysis of training overparameterized neural networks (Jacot et al.,
2018; Du et al., 2018; Oymak & Soltanolkotabi, 2020; Song & Yang, 2020; Ji & Telgarsky, 2020;
Su & Yang, 2019; Arora et al., 2019; Mianjy & Arora, 2020; Huang et al., 2021). NTKs can be
viewed as the reproducing kernels of the function space defined by the neural network structure,
and are constructed using the inner product between gradients of pairs of data points. With the
observation that the NTK stays roughly the same with sufficient overparameterization, recent work
has shown that (stochastic) gradient descent achieves zero training loss on shallow neural networks
for regression task, even if when the data-points are randomly labeled (Du et al., 2018; Oymak &
Soltanolkotabi, 2020; Song & Yang, 2020). Later work characterizes the loss update in terms of the
NTK-induced inner-product of the label vector, and notices that, when the label vector aligns with
the top eigenvectors of the NTK, training achieves a faster convergence rate (Arora et al., 2019).
A different line of work explores the structure of the data-distribution in classification tasks, by as-
suming separability when mapped to the Hilbert space induced by the partial application of the NTK
(Ji & Telgarsky, 2020; Mianjy & Arora, 2020). Rather than depending on the stability of NTK, the
crux of these works relies on the small change in the linearization of the network function. This line
of work requires milder overparameterization, and can be easily extended to training stochastic gra-
dient descent without changing the overparameterization requirement. The above literature assumes
all the parameters are updated per iteration.
There is literature devoted to the analysis of dropout training. For shallow linear neural networks,
(Senen-Cerda & Sanders, 2020a) give asymptotic convergence rate by carefully characterizing the
3
Under review as a conference paper at ICLR 2022
local minimas. For deep neural networks with ReLU activations, (Senen-Cerda & Sanders, 2020b)
shows that the training dynamics of dropout converge to a unique stationary set of a projected sys-
tem of differential equations. Under NTK assumptions, (Mianjy & Arora, 2020) shows sublinear
convergence rate for an online version for dropout in classification tasks. Our main theorem implies
linear convergence rate of the training loss dynamic for the regression task on a shallow neural
network with ReLU activations.
Theoretical work on Federated Learning (FL) also focuses on the convergence of neural network
training. FL-NTK (Huang et al., 2021) characterize the asymmetry of the NTK matrix due to the
partial data knowledge. For non-i.i.d. data distribution, (Deng & Mahdavi, 2021) proves conver-
gence for a shallow neural network by analyzing the semi-Lipschitzness of the hidden layer. Our
work differs since we consider training a partial model with the whole dataset. Lastly, there is recent
work on training partially masked neural networks (Mohtashami et al., 2021). The authors consider
minimizing a differentiable objective function under Lipschitz assumptions. We consider the more
frequently used setting of a one-hidden layer perceptron with non-differentiable activation.
3	Training with Randomly Mas ked Neurons
We use bold lower-case letters (e.g., a) to denote vectors, bold upper-case letters (e.g., A) to denote
matrices, and standard letters (e.g., a) for scalars. kak2 stands for the `2 (Euclidean) vector norm,
kAk2 stands for the spectral matrix norm, and kAkF stands for the Frobenius norm. Unless other-
wise stated, p denotes the number of subnetworks, and l ∈ [p] its index; K denotes the number of
global iterations and k ∈ [K] its index; τ is used for the number of local iterations and t ∈ [τ] its
index. We use Mk to denote the mask at iteration k, and E[m^ [∙] = Em。,…,Mk [∙] to denote the total
expectation over masks Mo,..., Mk. Weuse P(∙) to denote the probability of an event, and I{∙} the
indicator function. For distributions, We use N(μ, Σ) to denote the Gaussian distribution with mean
μ and variance Σ. We use Bern(ξ) to denote the Bernoulli distribution with P(X = 1) = ξ for
X 〜Bern(ξ). The categorical distribution overP categories is determined by parameter ξ ∈ Rp and
produces random variables x ∈ [p] such that P(x = i) = ξi. In this paper, we use a one-hot output
of categorical distribution by treating random variables as one-hot vectors X 〜Categorical(ξ),
with P(Xi = 1) = ξi. For a complete clarification of the notations, please refer to Table 1.
3.1	Single Hidden-Layer Neural Network with ReLU activations
We consider the single hidden-layer neural net-
work with ReLU activations, as in:
1m
f (W, a, x) = —m E arσ(hwr, Xi)
r=1
:= f(W, X).
Here, W = [w1, . . . , wm]> ∈ Rm×d is
the weight matrix of the first layer, and a =
[a1, . . . , ar]> ∈ Rm is the weight vector of the
second layer. We assume that each wr is ini-
tialized based onN(0, κ2I). Each weight entry
ar in the second layer is initialized uniformly at
random from {-1, 1}. As in (Du et al., 2018;
Zou et al., 2020; Soltanolkotabi et al., 2018;
Oymak & Soltanolkotabi, 2019; Li et al., 2020;
Oymak & Soltanolkotabi, 2020), a is fixed.
Algorithm 1 RandomlyMaskedTraining
1:	Initialize W0 , a
2:	for k = 0, . . . , K - 1 do
3:	Sample mask Mk ~ D
4:	for l = 1, . . . , p do
5:	Wk,0 一 Wk
6:	for t = 0, . . . , τ - 1 do
∂Lml (W)
7:	Wl…1一 Wl t - η —m—
k,t+1	k,t	∂wr
8:	end for
9:	∆Wk 一 W. Wk
10:	end for
11:	for r = 1, . . . , m do
12:	Wk+1,r - Wk,r + ηk,r P∣=i ∆Wk,r
13:	end for
14:	end for
Consider a p-subnetwork computing scheme.
In the k-th global iteration, we consider each binary mask Mk ∈ {0, 1}m×p to be composed of
subnetwork masks mlk ∈ {0, 1}m for l ∈ [p]. The r-th entry of mlk is denoted as mlk,r. Let
Xk,r = Plp=1 mlk,r denote the number of subnetworks that update neuron r in global iteration k.
Let Nk,r = max{Xk,r, 1} to be the normalizer of the aggregated gradient, Nk⊥,r = min{Xk,r, 1} to
be the indicator of whether a neuron is selected by at least one subnetwork. The surrogate function
defined by a subnetwork mask mlk is given by:
4
Under review as a conference paper at ICLR 2022
1m
fmk(W, X) = √m Ear mk,rσ((Wr, Xi)∙
r=1
With colored text, we highlight the differences between the full model and the surrogate functions.
ξ is a constant to be defined later on, conveniently selected by our theory. We consider training
the neural network using the regression loss. Given a dataset (X, y) = {(Xi, yi)}in=1, the func-
tion output on the whole dataset is denoted as f(W, X) = [f (W, X1), . . . , f(W, Xn)]. Then, the
regression loss of a surrogate model is given by:
Lmk(W) = 1∣y -fmk(W, ER
The surrogate gradient is computed as:
∂Wr
∂Lmlk (W)
W, Xi) -
yi XiI{hWr, Xii ≥ 0}.
N⊥
Moreover, We define the global aggregation step SiZe as ηk r = ʊk,r. Within this setting, the general
,	Nk,r
training algorithm is given by Algorithm 1.
4	Convergence on Two-Layer ReLU Neural Network
We often assume that each entry in the mask mlk『 〜Bern(ξ) is sampled independently, unless
otherWise stated. Here, ξ ∈ (0, 1) represents the Bernoulli distribution parameter. Since the forWard
pass of the surrogate function is a linear combination of ξ-proportion of the neurons’ output, We
multiply each neuron output by a factor of ξ to keep the overall output scaling consistent, as in
(Mianjy & Arora, 2020). For notation clarity, We define:
1m	ξm
Uk' = √m〉： ar ξσ(hw k,r, xii) = √——^X ar σ(hwk,r, xi i ) .
r=1	r=1
We focus on the behavior of the folloWing loss, computed on the Whole netWork over iterations k :
Lk = ky - ukk22, Where uk = hu(k1), . . .,u(kn)i .
This is the regression loss over iterations k betWeen observations y and the learned model uk .
Properties of subnetwork NTK. Recent Works on analyZing the convergence of gradient descent
for neural netWorks consider approximating the function output uk With the first order Taylor ex-
pansion (Du et al., 2018; Arora et al., 2019; Song & Yang, 2020). For constant step siZe η, taking
the gradient descent's -Wk+ι = Wk — ηVwL(W⅛)- first-order Taylor expansion, we get:
n
u(ki+) 1	≈	u(ki)	+	VWu(ki), Wk+1 - Wk	≈ u(ki)	- ξη	X H(k)ij(u(kj)	- yj),	(1)
j=1
where H(k) ∈ Rn×n is the finite-width NTK matrix of iteration k, given by
ξm
H(k)ij = 一〈Xi, Xj i £l{〈Wk,r, Xii ≥ 0, (Wk,r, Xj)≥ 0}.
m	r=1
Compared with previous definition of finite-width NTK, we have an additional scaling factor ξ. This
is because based on our later definition of masked-NTK, we would like the masked-NTK to be an
unbiased estimator of the finite-width NTK. In the overparameteriZed regime, the change of the net-
work’s weights is controlled in a small region around initialiZation. Therefore, the change of H(k)
is small, staying close to the NTK at initialiZation. Moreover, the latter can be well approximated
by the infinite-width NTK:
H∞ = ξ ∙ Ew〜N(0,I) [(Xi, Xj〉I{hw, Xii ≥ 0, hw, Xj)≥ 0}].
(Du et al., 2018) shows that H∞ is positive definite.
Theorem 1. (Du et al., 2018) Denote λ0 := λmin (H∞), the minimum eigenvalue of H∞. If for any
i 6= j it holds that the points Xi, Xj are not co-aligned, i.e., Xi 6k Xj, then we have λ0 > 0.
5
Under review as a conference paper at ICLR 2022
With H(k) staying sufficiently close to H∞, (Du et al., 2018; Arora et al., 2019; Song & Yang,
2020) show that λmin(H(k)) ≥ λ20 > 0. Moreover, Equation 1 implies that
uk+1 - uk ≈ -ξηH(k)(uk - y),
that further leads to linear convergence rate:
Lk+ι ≈ Lk + (VukLk, Uk+ι - Uki ≈ Lk - ξη(uk - y, H(k)(uk - y)i ≈ (1 - ξηλ0) Lk.
In rigorous NTK analysis, the Taylor expansion for both uk and Lk produces error term that reduces
the convergence rate from ηλ0 to γηλ0 with γ ∈ (0, 1) being a constant.
Yet, for neural networks with randomly masked neurons, the situation is trickier: in each iteration,
due to the different masks, the NTK changes even when the weights stay the same. Since, the NTK
of the masked network changes as i) the mask changes, and ii) the network weight changes, we
define the masked-NTK induced by the network weight in k-th iteration and the mask in the k0-th
iteration as follows:
Definition 1. Let mlk0 be the mask of subnetwork l in iteration k0. We define the masked-NTK in
global iteration k and local iteration t induced by mlk0 as:
m
(mfe0 ◦ H(k,t))ij = mm hxi, Xj i X mko,r I{hwk,t,r, Xii ≥ 0, hwk,t,r, Xji ≥0}∙
r=1
with wkl ,t,r denoting the rth weight vector in the kth global iteration and tth local iteration.
Here, with colored text we highlight the main differences to the common NTK definition. Note
that although we are only interested in the masked-NTK with k = k0, to facilitate our anal-
ysis on the minimum eigenvalue of masked-NTK, we also allow k 6= k0 . Note that we have
EMk mlk ◦ H(k, 0) = H(k). Throughout iterations of the algorithm, the following theorem shows
that all masked-NTKs stay sufficiently close to the infinite-width NTK.
Theorem 2. Suppose the number Ofhidden nodes satisfies m = Ω(n2 iog(Kpn∕δ)∕ξλ2). IffOr all k, t
it holds that ∣∣wk,t,r — wo,r ∣∣2 ≤ R ：= KnO, then with probability at least 1 — δ ,for all k, k0 ∈ [K]
and all l ∈ [p], we have:
λmin(mko ◦ H(k,t)) ≥ λ20.
Note that the above theorem relies on the small weight change in iteration (k, t). In order to guar-
antee each subnetwork’s loss decrease, we need to ensure that the i) the weight change is bounded
up to global iteration k (this implies that when a subnetwork is sampled from the whole network, its
weights do not deviate much from the initialization); ii) the weight change during the local training
of the subnetwork is also bounded. This requires an upper bound on the sampled subnetwork’s loss
before the local training starts. The following hypothesis sets up the “skeleton” to construct different
theorems, based on different problems considered.
Hypothesis 1. Assume that for all i ∈ [n] we have kXik2 = 1. Fix the number of global iterations
K. Suppose the number of hidden nodes satisfies m = Ω (n2 Iog(Kpn^/ξλ0), and suppose we use
a constant SteP size η = O (λθ∕n2). Then, if for all k0 ≤ k the following convergence holds with
convergence rate α ∈ (0, 1) and error term αB1 > 0:
E[Mk0],W0,a ky - uk0+1k22 ≤ (1 - α)E[Mk0-1],W0,a ky - uk0 k22 + αB1.	(2)
and, further, the weight perturbation before iteration k is bounded by
kwk,r - w0,rk2 + 2ητ J^mmK QEMk-ι],W0,a [ky - Uk l∣2] + (K - k)B) ≤ R ⑶
with B = JBF + κpξ(1-ξ)pn, then, with probability at least 1 — 4δ, we have:
ky - Uk,t+ιk2 ≤ (1 - ɪɪ2o) ky - Uk,tk2,	(4)
with the surrogate function of the subnetwork function defined by mask l in global iteration k and
local iteration t denoted as
uk,t = fmlk (Wk,t，Xi) U k,t = huk∕，...，Uk,J]
and the local weight perturbation satisfies that for all t ∈ [τ]:
kwk,t,r - wk,rk ≤ ητ√nKEMk-1],W0,a [ky - Ukk2]+ 'nJ2ξ⅛誓	(5)
6
Under review as a conference paper at ICLR 2022
The hypothesis above states that, in a given global step k, given the linear convergence of the loss
(Equation 2) and a small weight perturbation guarantee (Equation 3) in previous iterations k0 ≤ k,
each subnetwork’s local loss also decreases linearly (Equation 4), as well as the weight perturbation
remains bounded (Equation 5). Yet, the above hypothesis does not connect the subnetwork’s loss
with the whole network’s loss through the sampling and aggregation process.
This is the goal in the sections that follow. Our aim is to turn Hypothesis 1 into a series of specific
theorems that cover different cases. In particular, we provide a convergence result for Algorithm 1
for i) masks with i.i.d. Bernoulli entries, where we leverage the aggregation of gradients, and prove
the non-decrease property of subnetwork’s loss to bound the difference between the aggregated
gradient at local iteration t and local iteration 1; and ii) masks with i.i.d Categorical rows, where
we leverage the loss change in sampling and aggregating subnetworks, and depend on the linear
convergence to show loss decrease. In both settings, we prove the bound on weight perturbation
of each subnetwork, as hypothesized above, to bound the aggregated weight perturbation. Also, in
both settings, we show that the required condition in Hypothesis 1 can be satisfied.
4.1	Main Results
While the local gradient descent for each subnetwork makes progress with high probability, when a
large network is split into small subnetworks, the expected error on the dataset increases. Since the
masks are sampled i.i.d. from the joint Bernoulli distribution, the function of each sub-network is
an unbiased estimation of the function represented by the large network. Therefore, we have:
EMk [ky - UkI∣2] = l∣y - ukk2+ EMk [kuk - UkI∣2].
When analyzing the convergence, the last term needs to be carefully dealt with. Moreover, it is not
trivial to show that, when combining the updated network of the local steps, the loss computed on the
whole network is smaller than or equal to the error of each sub-network. Resolving these technical
difficulties, we present our general result for masks sampled from a joint Bernoulli distribution:
Theorem 3. Let the assumption of Theorem 1 hold, i.e., λ0 ≥ 0, and for all i ∈ [n], we have
Ixi I2 = 1 and |yi | ≤ C - 1 for some C ≥ 0. Fix the number of global iterations to K and the
number of local iterations to τ. Assume for each global iteration k, the mask Mk is generated such
that mk『 〜Bern(ξ) with ξ ∈ (0,1]. Let the number of hidden neurons satisfy
m = ω (KKmax{κ⅛4,nκ⅛，K2p})，	⑹
and fix the local StePSize to η = O (maχ{λ0p}nτ )∙ Then With probability at least 1 一 δ we have
0
E[Mk0-ι] [ky - uk0 k2] ≤ (1 一 4ηθτλθ) ky - u0k2 + B1,	⑺
with θ = 1 一 (1 一 ξ)p and
B _ 64(1-ξ)2n3d + 68ητ(θ-ξ2)n3κ2 + (θ-ξ2)nκ2 + (T-1)2pCι
1 = mλ0	+ pλo	十 6τ2 P	十 24τ2
+ 8η2(T『吗十 4η(τ — 1)pnCι,
λ0
4θ2(1 一 ξ)nκ2
Ci =-------------.
p
Overall, given the overparameterization requirement in Equation 9, the neural network training error,
as expressed in Equation 7, drops linearly within an error region, defined by B1 in Equation 7. While
this theorem seems complicated, we can make mild assumptions and simplify the key messages of
this form. In the following sections, We assume that max{K, d,p} ≤ n and K = n-2.
Dropout. The dropout algorithm (Srivastava et al., 2014) corresponds to the case τ = 1,p = 1. For
this assignment, we arrive at the following corollary.
Corollary 1. Under the assumptions in Theorem 3, and the additional assumption that
max{K, d} ≤ n,fixthe number ofiterations to K, the step size to η = O (λ0∕n2), and the number of
hidden neurons satisfies m = Θ (n5κ∕ξ2λ4δ). Suppose we run the dropout algorithm on a two-layer
ReLU neural network. Then, with probability at least 1 一 δ, we have:
E[Mk0-ι] [ky - uk0 k2] ≤ (1 一 4ηξλ0)k ky - u0k2 + O (1 一 ξ)
7
Under review as a conference paper at ICLR 2022
Typically, 1 - ξ is usually referred to as the ”dropout rate”. In our result, as ξ approaches 0, which
corresponds to the scenario that no neurons are selected, the convergence rate approaches 1, meaning
that the loss hardly decreases. In the mean time, the error term remains constant. On the contrary,
as ξ approaches 1, which corresponds to the scenario that all neurons are selected, we get the same
convergence rate of 1 - O (ηλ0) as in previous literature, and the error term decreases to 0.
Multi-Sample Dropout. The multi-sample dropout (Inoue, 2019) corresponds to the scenario where
τ = 1,p ≥ 1. Our corollary below indicates how increasing p helps the convergence.
Corollary 2. Under the assumptions in Theorem 3, and the additional assumption that
max{K,d} ≤ n, fix the number of iterations to K, the SteP size to η = O (λ0∕n2), and let the
number of hidden neurons satisfy m = Θ (n5 κ∕ξθλ0δ). Suppose we run the P-sample dropout algo-
rithm on a two-layer ReLU neural network. Then, with Probability at least 1 - δ, we have:
E[Mk0-ι] [ky - uk01∣2] ≤ (1 - 1 ηθλθ)k Ily - u0k2 + O ( (InK)	+ θ-ξ-).
Based on this corollary, increasing the number of subnetworks p improve the convergence rate since
θ increases as p increases, due to the increasing coverage probability of each neuron. Moreover,
increasing the number of subnetworks help decreasing the error term even when the dropout rate ξ
is fixed. After P is as large as nK, the error term become dominated by the term O ((I - ξ)2∕nκ).
Multi-Worker IST. The multi-worker IST algorithm (Yuan et al., 2019) is very similar to the general
scheme with P ≥ 1 and τ ≥ 1, but with the additional assumption that max{K, d, P} ≤ n, and a
special choice of initialization K = n- 1.
Corollary 3. Under the assumptions in Theorem 3,fix the number of iterations to K, the step size to
η = O (λ0∕nτ max{n,p}), and let the number ofhidden neurons satisfy m = Θ (n5K∕ξθλ0δ). Suppose
we run the IST algorithm on a two-layer ReLU neural network. Then, with probability at least 1 - δ,
we have
E[Mk0-ι] [ky - uk0112] ≤ (1 - 1 ηθτλθ)k Ily - u0k2 + O ((I-K + θ-ξ + (TT) T (1-ξ) ).
While this corollary presents a convergence result for the multi-worker IST, which is missing in
the current literature, it also bears the problem that, when T > 1, the last error term (TT)T2 (1-ξ)
potientially increase as the number of workers (subnetworks) increase. This could again be due to
the fact that some neurons are shared among workers and the gradients from the individual workers
are normalized during aggregation. We present an alternative theorem that resolves this issue, by
making another assumption on the masks generated in each global iteration. In particular, we assume
that, for a P-worker scenario, for each neuron, the mask is generated from a categorical distribution,
where each worker uses the same probability. In this way, the masks endorsed by each worker are
non-overlapping (as stated in (Yuan et al., 2019)), and the union of the masks covers the whole set
of hidden neurons. The following theorem presents the convergence result under this setting.
Theorem 4. Suppose the assumption of Theorem 1 holds, i.e. λ0 > 0, and for all i ∈ [n], we have
IxiI2 = 1 and |yi| ≤ C - 1for some C ≥ 1. Fix the number of global iterations to K, and the
number of local iterations to T. Let ξ = 1∕p ∙ 1p, and suppose that for each r ∈ [m],k ∈ [K],
the mask is generated such that m&r 〜Categorical(ξ). Let the number of hidden neurons satisfy
m = Ω(τ2n2κ maχ{n2, d}∕λ0δ). Then, with constant local step-size η = (λ0∕n2) we have that:
EMk-1],W0,a [ky - Uk I∣2] ≤ (2 + 2 (1 -粤))∙ C2n + O (T(P-Imn d),
holds with probability at least 1 - δ.
This theorem has a couple noticeable properties. First, when the number of workers P = 1, i.e.,
the scenario of multi-worker IST reducing to the full-network training, the error term disappears,
driving further connections between regular and IST training. Second, one can arbitrarily increase
the overparameterization parameter m, which further leads to error term decrease: this suggests that
IST training could be benefited by wider models, an observation made in (Yuan et al., 2019; Dun
et al., 2019; Wolfe et al., 2021).
8
Under review as a conference paper at ICLR 2022
(」0」」山 6ucro」l-)60-
s⅛0M⅛uqns Jo -IeqEnN
Trammg LOSS
(a)	(b)	(c)
Figure 2: Validation experiments on a single hidden layer perceptron.
5	Experiments
We validate our theory with the Communities and Crime Data Set
(US Department of Commerce, Bureau of the Census, 1992; Dua & Graff, 2017; Redmond &
Baveja, 2002). This dataset has 128 features, and we take a subset of 100 samples from the dataset.
We run Algorithm 1 on a one hidden layer perceptron with 4000 hidden neurons. We fix the number
of global iteration to K = 200 and use a constant step size.
In Figure 2a, we plot the logarithm of the mean and variance of the training error dynamic with
respect to the K (for clarity, we only plot the first 125 iterations), which includes the sampling step,
local training steps, as well as the gradient aggregation step. Notice that there are three types of
dynamics, as annotated in the figure: (1) A smooth decrease of training error: This corresponds to
subnetworks’ local training, which is supported by Hypothesis 1 that each subnetwork makes local
progress. (2) The sudden decrease of training error: This corresponds to the aggregation of locally-
trained subnetworks, and is consistent with our proof in Theorem 4. (3) The sudden increase of
training error: This corresponds to re-sampling subnetworks; according to our theory, the expected
average training error increases after sampling.
Figure 2b and Figure 2c provide heatmap results that demonstrate the change of the error term
as we vary the number of subnetworks, number of local steps, and the selection probability. In
Figure 2b, the subnetworks are generated using Bernoulli masks, and training process with a fixed
number of local steps. Note that, as we fix the number of subnetworks and increase the selection
probability, the error decreases (lighter colors in heatmap). Moreover, if we fix the number of
selection probability and increase the number of subnetworks, the training error also decreases. This
is consistent with Theorem 3. In Figure 2c, the subnetworks are generated using categorical masks.
Since for categorical masks, the selection probability is determined by the number of subnetworks,
we instead vary the number of local steps. Note that the training error increases both when we
increase the number of subnetworks (and thus resulting in a smaller subnetwork for each worker)
and increasing the number of local steps. This is consistent with our theoretical result in Theorem 4.
6	Conclusion
We prove linear convergence up to an error region when training and combining subnetworks in
a single hidden-layer perceptron scenario. Our work extends results on dropout, multi-sample
dropout, and the Independent Subnet Training, and has broad implications on how the sampling
method, the number of subnetworks, and the number of local steps affect the convergence rate and
the error region. While our work focus on the single hidden-layer perceptron, we consider multi-
layer perceptrons as an interesting direction: we conjecture that a more refined analysis of each
layer’s output is required (Du et al., 2019; Allen-Zhu et al., 2019b). Moreover, focusing on the
convergence of a stochastic algorithm for our framework is a different research direction.
9
Under review as a conference paper at ICLR 2022
7	Reproducibility Statement
In the theorectical results of this paper, we make assumptions on the dataset similar to previous
work (Du et al., 2018; Song & Yang, 2020; Arora et al., 2019), where i) no two input data points
are co-aligned, ii) the input data points are normalized, and iii) the value of the labels are bounded.
We point out that i) is a standard assumption in machine learning settings, since co-aligned data
points with different labels cannot be fitted to zero training error, and ii) and iii) can ba achieved by
normalizing the dataset. Moreover, the additional assumption of fixing the number of global itera-
tions K is also common in previous literature (Su & Yang, 2019; Allen-Zhu et al., 2019a). Although
increasing K requires a larger overparameterization, we note that, since the learning rate is inde-
pendent of K, achieving an e-error near the error region requires K = Ω(log n/e/iog(i - ηθτλο/4)-1).
This shows that K typically do not affect the overparameterization much. Lastly, we provide proof
of Theorem 2, Hypothesis 1, Theorem 3 and Theorem 4 in section B, C D, and F in the appendix,
respectively. We also provide code for running the experiments as supplementary material.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparame-
terized neural networks, going beyond two layers. Advances in neural information processing
systems, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 242-252. PMLR, 09-15 JUn 2019b. URL https://Proceedings.
mlr.press/v97/allen-zhu19a.html.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, pp.
254-263. PMLR, 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks, 2019.
Devansh Arpit, StanislaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer
look at memorization in deep netWorks. In International Conference on Machine Learning, pp.
233-242. PMLR, 2017.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural netWorks. Advances in Neural Information Processing Systems, 30:6240-6249, 2017.
Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical vieWpoint.
arXiv preprint arXiv:2103.09177, 2021.
Mikhail Belkin. Fit Without fear: remarkable mathematical phenomena of deep learning through the
prism of interpolation. arXiv preprint arXiv:2105.14368, 2021.
Mikhail Belkin, Daniel J Hsu, and Partha Mitra. Overfitting or perfect fitting? risk bounds for
classification and regression rules that interpolate. Advances in Neural Information Processing
Systems, 31:2300-2311, 2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Jinming Cao, Yangyan Li, Mingchao Sun, Ying Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan
Chen, and Changhe Tu. Do-conv: DepthWise over-parameterized convolutional layer. arXiv
preprint arXiv:2006.12030, 2020.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
netWorks With binary Weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
10
Under review as a conference paper at ICLR 2022
Yuyang Deng and Mehrdad Mahdavi. Local sgd optimizes overparameterized neural networks in
polynomial time. arXiv preprint arXiv:2107.10868, 2021.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 1675-1685. PMLR, 09-15 JUn 2019. URL https:
//proceedings.mlr.press/v97/du19c.html.
Simon S DU, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neUral networks. In International Conference on Learning Representations,
2018.
DheerU DUa and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
C. DUn, C. Wolfe, A. Kyrillidis, and C. Jermaine. Resist: Layer-wise decomposition of resnets for
distribUtedtraining. Preprint, 2019.
Gintare Karolina DziUgaite and Daniel M Roy. CompUting nonvacUoUs generalization boUnds for
deep (stochastic) neUral networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
Cong Fang, Jason Lee, PengkUn Yang, and Tong Zhang. Modeling from featUres: a mean-field
framework for over-parameterized deep neUral networks. In Conference on Learning Theory, pp.
1887-1936. PMLR, 2021.
William FedUs, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Vitaly Feldman. Does learning reqUire memorization? a short tale aboUt a long tail. In Proceedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pp. 954-959, 2020.
Spencer Frei, YUan Cao, and QUanqUan GU. Algorithm-dependent generalization boUnds for over-
parameterized deep residUal networks. Advances in neural information processing systems, 2020.
Yarin Gal and ZoUbin Ghahramani. DropoUt as a bayesian approximation: Representing model
Uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neUral networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
Ian Goodfellow, YoshUa Bengio, Aaron CoUrville, and YoshUa Bengio. Deep learning, volUme 1.
MIT Press, 2016.
Yihong GU, Weizhong Zhang, Cong Fang, Jason D Lee, and Tong Zhang. How to characterize the
landscape of overparameterized convolUtional neUral networks. 2020.
Baihe HUang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neUral tangent kernel-based frame-
work for federated learning convergence analysis, 2021.
KaixUan HUang, YUqing Wang, Molei Tao, and TUo Zhao. Why do deep residUal networks generalize
better than deep feedforward networks?—a neUral tangent kernel perspective. Advances in Neural
Information Processing Systems, 33, 2020.
Hiroshi InoUe. MUlti-sample dropoUt for accelerated training and better generalization. arXiv
preprint arXiv:1905.09788, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: convergence and gen-
eralization in neUral networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 8580-8589, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks, 2020.
11
Under review as a conference paper at ICLR 2022
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-
Zation trick. Advances in neural information processing Systems, 28:2575-2583, 20l5.
Alex Labach, Hojjat Salehinejad, and Shahrokh Valaee. Survey of dropout methods for deep neural
networks. arXiv preprint arXiv:1904.13310, 2019.
Daniel LeJeune, Hamid Javadi, and Richard Baraniuk. The implicit regularization of ordinary least
squares ensembles. In International Conference on Artificial Intelligence and Statistics, pp. 3525-
3535. PMLR, 2020.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International conference
on artificial intelligence and statistics, pp. 4313-4324. PMLR, 2020.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, ge-
ometry, and complexity of neural networks. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 888-896. PMLR, 2019.
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean field analysis of deep
ResNet and beyond: Towards provably optimization via overparameterization from depth. In
International Conference on Machine Learning, pp. 6426-6436. PMLR, 2020.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the
effectiveness of SGD in modern over-parametrized learning. In International Conference on
Machine Learning, pp. 3325-3334. PMLR, 2018.
Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S Mann. Efficient
large-scale distributed training of conditional maximum entropy models. In Advances in Neural
Information Processing Systems, pp. 1231-1239, 2009.
Poorya Mianjy and Raman Arora. On convergence and generalization of dropout training, 2020.
Amirkeivan Mohtashami, Martin Jaggi, and Sebastian U Stich. Simultaneous training of partially
masked neural networks. arXiv preprint arXiv:2106.08895, 2021.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring general-
ization in deep learning. Advances in Neural Information Processing Systems, 30:5947-5956,
2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-Bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In International Conference on Machine Learning, pp. 4951-4960.
PMLR, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global con-
vergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
Information Theory, 1(1):84-105, 2020.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers
generalize to ImageNet? In International Conference on Machine Learning, pp. 5389-5400.
PMLR, 2019.
12
Under review as a conference paper at ICLR 2022
Michael Redmond and Alok Baveja. A data-driven software tool for enabling coop-
erative information sharing among police departments. European Journal of Opera-
tional Research, 141(3):660-678, 2002. ISSN 0377-2217. doi: https://doi.org/10.
1016/S0377-2217(01)00264-8. URL https://www.sciencedirect.com/science/
article/pii/S0377221701002648.
Albert Senen-Cerda and Jaron Sanders. Asymptotic convergence rate of dropout on shallow linear
neural networks, 2020a.
Albert Senen-Cerda and Jaron Sanders. Almost sure convergence of dropout algorithms for neural
networks, 2020b.
Vatsal Shah, Soumya Basu, Anastasios Kyrillidis, and Sujay Sanghavi. On generalization of adaptive
methods for over-parameterized linear regression. arXiv preprint arXiv:2011.14066, 2020.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound,
2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822-2878, 2018.
Stefano Spigler, Mario Geiger, Stephane d'Ascoli, Levent Sagun, Giulio Biroli, and MatthieU Wyart.
A jamming transition from under-to over-parametrization affects generalization in deep learning.
Journal of Physics A: Mathematical and Theoretical, 52(47):474001, 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approxi-
mation perspective, 2019.
US Department of Commerce, Bureau of the Census. Census of Population and Housing 1990
United States: Summary Tape File 1a 3a (Computer Files), volume 9575. US Department of
Commerce, Bureau of the Census Producer, Washington, DC and Inter-University Consortium
for Political and Social Research, Ann Arbor, MI, 1992.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network
learning. In International Conference on Learning Representations, 2018.
Thijs Vogels, Sai Praneeth Karinireddy, and Martin Jaggi. PowerSGD: Practical low-rank gradient
compression for distributed optimization. Advances In Neural Information Processing Systems
32 (Nips 2019), 32(CONF), 2019.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning, pp. 1058-1066.
PMLR, 2013.
Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferfish: Communication-efficient
models at no extra cost. Proceedings of Machine Learning and Systems, 3, 2021.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118-126. PMLR, 2013.
13
Under review as a conference paper at ICLR 2022
Cameron R Wolfe, Jingkang Yang, Arindam Chowdhury, Chen Dun, Artun Bayer, Santiago Segarra,
and Anastasios Kyrillidis. GIST: Distributed training for large-scale graph convolutional net-
works. arXiv preprint arXiv:2102.10424, 2021.
Tianyi Yao, Daniel LeJeune, Hamid Javadi, Richard G Baraniuk, and Genevera I Allen. Minipatch
learning as implicit ridge-like regularization. In 2021 IEEE International Conference on Big Data
and Smart Computing (BigComp), pp. 65-68. IEEE, 2021.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks.
In International Conference on Learning Representations, 2018.
B. Yuan, C. Wolfe, C. Dun, Y. Tang, A. Kyrillidis, and C. Jermaine. Distributed learning of deep
neural networks using independent subnet training. arXiv preprint arXiv:1910.02120, 2019.
Ce Zhang and Christopher Re. Dimmwitted: A study of main-memory statistical analytics. Pro-
ceedings of the VLDB Endowment, 7(12):1283-1294, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107-
115, 2021.
Jian Zhang, Christopher De Sa, Ioannis Mitliagkas, and Christopher Re. Parallel SGD: When does
averaging help? arXiv preprint arXiv:1606.07365, 2016.
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the ImageNet scale: a PAC-Bayesian compression approach. In Interna-
tional Conference on Learning Representations, 2018.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient
descent. In Advances in neural information processing systems, pp. 2595-2603, 2010.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning, 109(3):467-492, 2020.
14
Under review as a conference paper at ICLR 2022
A Preliminary and Definition
In the proofs of our theorems, we use extensively the following tools.
Definition 2. (Sub-Gaussian Random Variable) A random variable X is κ2-sub-Gaussian if
κ2t2
E[etx] ≤ eF
Definition 3. (Sub-Exponential Random Variable) A random variable with mean E[X] = μ is
(κ0, α)-sub-exponential if there exists non-negative (κ0, α) such that for all t ≤ α-1
2 02
E[et(X-μ)] ≤ e--^2~
Property 1. (Sub-Exponential Tail Bound) For a (κ0, α)-sub-exponential random variable X with
E[X] = μ, then we have
P(X>μ + t) ≤
--上
J e	2κ02
1上
Ie 2α
02
if o ≤ t ≤ κα
if t > κ02
α
Property 2. (Markov’s Inequality) For a non-negative random variable X, we have
P(X ≥ a) ≤ 1 E[X]
a
Property 3. (Hoeffding’s Inequality for Bounded Random Variables) Let X1, . . . , Xn be indepen-
dent random variables bounded by |Xi| ≤ 1for all i ∈ [n]. Then we have
P
1n
-X Xi
n
i=1
≥t ≤ e-2nt2
Property 4. (Berstein’s Inequality) Let X1 , . . . , Xn be random variables with E[Xi] = 0 for all
i ∈ [n]. If|Xi| ≤ M almost surely, then
______t2∕2_____
Pn=1 E[X2] + Mt/3
Property 5. (Jensen’s Inequality for Expectation) For a non-negative random variable X, we have
E [X2i ≤ (E[X])1
Apart from the properties above, we also need the following definitions to facilitate our analysis.
First, We note that, in the following proofs, We let R = 1λn. Define
Air = {∃w ∈ B(w0,r, R) : I{hw, xii ≥ 0} 6= I{hw0,r, xii ≥ 0}}
to denote the event that for sample xi, the activation pattern of neuron r may change through training
if the Weight vector change is bounded in the R-ball centered at initialization. Moreover, let
Si = {r ∈ [m] : Air}
Si⊥ = [m] \ Si
(8)
to be the set of neurons Whose activation pattern does not change for sample xi if the Weight vector
change is bounded by the R-ball centered at initialization. Moreover, since We are interested in the
loss dynamic computed on the folloWing function
ξm
ukɔ = √——X : ar σ(hwk,r , Xii).
r=1
We denote the full gradient of loss With respect to each Weight vector wr as
dL(Wk) _ ξ S V ( (i)	W"B V ∖、∩l
∂ 一 √——arx arXi(Uk - yi)I{hwk,r, Xii ≥ 0}
∂wr	m i=1
15
Under review as a conference paper at ICLR 2022
B Proof of Theorem 2
Recall the definition of masked-NTK
1m
(mk0 ◦ H(k,t))ij = m hxi, Xji X mko,r I{(wk,t,r , Xi ≥ 0, <wk,t,r, Xj ≥ 0}
To start with, we fix k0 ∈ [K], l ∈ [p], and i, j ∈ [n]. In this case, let
hr = mlk0,r hXi,Xji I{hw0,r, Xii ≥ 0, hw0,r, Xii ≥ 0}
Then we have
1m
(mko ◦ H(0, 0))ij = - X hr
m
r=1
Also we have
EMk,W [hr] = EW〜N(0,I) [EMk [hr]] = H∞
Note that for all r we have |hr | ≤ 1. Thus we apply Hoeffding’s inequality for bounded random
variables and get
P (∣(mko ◦ H(0,0))ij - H∞∣ ≥ t) = Pqm X hr - H∞ ≥ t) ≤ 2e-2mt2
Apply a union bound over i, j gives that with probability at least 1 - 2n2e-2mt2 it holds that
∣∣(mlk0 ◦ H(0, 0))ij - Hi∞j∣∣ ≤t
for all i, j ∈ [n]. Therefore,
kmlk0 ◦ H(0, 0) - H∞k22 ≤ kmlk0 ◦ H(0, 0) - H∞k2F
n
≤ X |(mk，◦ H(0,0))ij - H∞∣2
i,j=1
≤ n2t2
Let t = λ0 gives
kmk，◦ H(0,0)- H∞k2 ≤ λ0
mλ0
holds with probability at least 1 - 2n2 e-^8n2. Next We show that for all k ∈ [k] and t ∈ [τ], as long
as kwk,t,r - w0,rk2 ≤ R for all r ∈ [m], then it holds that
kmlk， ◦ H(k, t) - mlk， ◦ H(0, 0)k2 ≤ 2nκ-1R
Following the argument of (Song & Yang, 2020), lemma 3.2, we have
nm	2
kmko ◦ H(At)-mk，◦ H(O,O)kF ≤ m2 X (Xsr,ij)
with
SrRj= mko,r (I{hw0,r, Xii ≥ 0； hw0,r, xj i ≥ 0} - 1{徇晨冷 xi) ≥ 0；(w；k,t, Xj ≥ 0})
Then sr,i,j = 0 if Air and Ajr happend. In other cases we have |sr,i,j | ≤ 1. Thus we have that
for all i, j ∈ [n]
EMk,W0 [sr,i,j] = ξP (Air ∪ Ajr ) ≤ -ʒ== ≤ 2ξκ-1R
κ 2π
16
Under review as a conference paper at ICLR 2022
and
EMk,Wo h(sr,i,j - EMk,Wo [sr,i,jD2] ≤ EMk,wo,r K,) ≤ ^^ ≤ 2ξκ-1R
Thus applying Bernstein inequality with t = ξκ-1R gives
P (m^^ ^X sr,i,j ≥ 3ξκ IR) ≤ exp
mξR
Therefore, taking a union bound gives that, with probability at least 1 - n2e -^10κ We have that
kmlk0 ◦ H(k, t) - mlk0 ◦ H(0, 0)k2 ≤ kmlk0 ◦ H(k, t) - mlk0 ◦ H(0, 0)kF ≤ 3ξnκ-1R
Using R ≤ κλn0 gives |.力, ◦ H(k,t) - mk, ◦ H(0,0), ≤ ξλ0 ≤ λ40 with probability at least
1 — n2e-m⅛n0. Therefore, We have
kmko ◦ H(k,t)- H∞k2 ≤ λ0
which implies that λma (mk，◦ H(k,t))	≥	λ0 holds with
probability at least 1 -
for a fixed k0 ∈ [K] and l ∈ [p]. Taking a union bound over all k0
and l and plugging in the requirement m = Ω
n2 log KP
ξ -ξλ0-
gives the desired result.
C Proof of Hypothesis 1
In this proof, we follow the idea of (Du et al., 2018). However, the difference is that i) we use
our masked-NTK during the analysis, and ii) we use a different technique for bounding the weight
perturbation. We repeat the requirement stated in the theorem here:
Suppose that, for all k0 < k, the expected network loss enjoys a linear convergence with and addi-
tional error term
E[Mk],W0,a ky - uk0+1k22 ≤ (1 - α) E[Mk-1],W0,a ky - uk0 k22 + αB1
for some α ≤ 1, and
kwk,r - w0,rk2 + 2ητ
α E[Mk-ι],wo
a [ky - ukk2] + (K - k)B
≤R
for all r ∈ [m] with
B = ∖∣B + κPξ(1 - ξ)pn
α
We start focusing on local iteration t ∈ [τ]. We also assume that kwk,t0,r - w0,r k2 ≤ R, and the
probability bound of Lemma 23 holds, and show the local convergence. As in previous works, we
will show that this assumption actually holds by induction after we show the local convergence. For
the local convergence, we are interested in
ky - U k,t+ιk2 = ky - U k,tk2 - 2〈y - uk,t, U k,t+ι - U Ikj + ku k,t+ι - uk,tk2
Again we define uk(J+ι- us? = Ifklt+琛k,t with
IfkIt = -1m X armk,r (σ (<wk,t+1,r, Xi)) - σ ((wkt,r, Xi〉))
m r∈Si
I2(k,t = √1m X armk,r (σ (<wk,t+1,r, Xi)) - σ Kwk,t,r, Xi)))
r∈Si⊥
17
Under review as a conference paper at ICLR 2022
with the definition of Si as defined in equation (8), and notice that, with the 1-Lipchitzness of ReLU,
t| ≤ √m X lσ (<wk,t+1,r , Xi〉)— σ (<wk,t,r , Xi〉)l
≤
r∈Si⊥
m X llwk,t+1,r — wk,t,rl∣2
r∈Si⊥
≤
ɪ X
VZm r∈S⊥
∂wr
2
≤
唔 Xl∣y-uk,t∣∣2
r∈Si⊥
≤ 4ηκ-1√nRky - Uk∕∣2
where the last inequality uses ∣S⊥ | ≤ 4mκ-1R from Lemma 16. Therefore,
Ky — Uk,t,I2,k,t>l ≤ √nm∈[nχ
t∣ ∙ ky — Uk,tk2 ≤ 4ηκ-1nRky — uk,tk2
Similarly, we have
⅛+ι- C)
wlk,t+1,r — wk,t,r ll2
m
≤ X llwkl ,t+1,r — wk,t,r ll2
r=1
2
m
≤η2X
r=1
∂wr
2
≤ η2n2ky - U k,tk2
Lastly, we define mlk ◦ H(k, t)⊥ with
(mk ◦ H(At)⊥)ij = -m hxi, Xji X mk,rI{hwk,r, xii≥0, hwk,r, Xji ≥ 0}
r∈Si⊥
and we have
t = √1m X ar mk,r (wk,t+1,r — wk,r, xi) I{hwk,r, Xii ≥ 0}
m r∈Si
* Xiar mk,r(
n
∂Lmlk (Wk,r )
∂wr
, Xi I{hwk,r, Xii ≥ 0}
m X X mk,r Dj- Ukj)) hxi, xji I{hwk,r , Xii ≥ 0, hwk,r , xji ≥ 0}
m r∈Si j=1
n
ηX (mk ◦ H(At)- mk ◦ H(At)⊥)ij∙ (yj∙ — ukjt))
j=1
—
18
Under review as a conference paper at ICLR 2022
Therefore,
n
(y - U k t,l∖ kJ = η X (yi - Uk(i∖ (mk ◦ H(k, t) - mk ◦ H(k,t)⊥) ∙ ∙ (yj - Uk 叭
k,t 1,k,t	k,t k	k	ij j k,t
i,j=1
=η (y - Uk,t, (mk ◦ H(k,t) - mk ◦ H(k,t)⊥) (y - Uk,t))
≥ *l∣y - Uk,tk2 - ηkmk ◦ H(At)Lgky - Uk,tk2
≥ (ηλ0 - 4ηκ-1nR) ky -Uk,tk2
where the last inequality follows from the fact that
kmk ◦ H(k,t)⊥k2 ≤ kmk ◦ H(k,t)⊥kF
1 n (	∖2
=m2 £ I hxi, Xji E I{hwk,r, Xii ≥0, hwk,r, Xj i ≥ 0} I
i,j=1	r∈Si⊥
2
≤ 二∣s⊥l2
m2
= 16n2κ-2R2
Putting things together gives
ky - uk,t+1k2 ≤ (1-ηλ0+ 16ηκ-1nR + η2n2) ky -Uk,tk2
Choose R ≤ κλ0 and η ≤ 黑 gives
ky - Uk,t+ιk2 ≤ (1 - η20) ky - Uk,tk2
Therefore, for all t ∈ [τ] we have
ky - Uk,tk2 ≤ (1- η20) ky - UIkk2
Next we bound the weight change during the local steps. We have
t-1
kwk,t,r -wk,rk2 ≤η
t0=0
≤ ητ√=ky - UIk k2
m
≤ ητ√= (ky - Uk k2 + kUk - UIk k2)
m
Applying Markov,s inequality to the global convergence, with Probabiltiy at least 1 -枭,it holds
that
ky - Uk k2 ≤ P2KδEMk-ι],W0,a [ky - Uk k2]
By Lemma 25, we have
EMk [kUk- Ukk2] ≤ 4ξ(1- ξ)nκ2
Thus with probability at least 1 - 2pK it holds that
kUk - UIk k2 ≤ 2κ,2ξ(1 — ξ)npK∕δ
19
Under review as a conference paper at ICLR 2022
Plugging in gives
kwk,t,r - wk,rk
ητ √2nK	ll 1	9	∕2ξ(1- ξ)pK
≤	√mδ	EMk-1]W0,a [ky - Ukk2〕+ 2ητκn∖∣	m
Since for iterations k0 ≤ k, we have
2nK 1
kwk,r - w0,r k2 + 2ητ∖J	(工E[Mk-ι],Wo,a [ky - Uk l∣2] + (K -
≤R
Since k < K and α ≤ 1, and
B=vBα1+Kp(IFn ≥ wξ(1-ξ)pn
we have that
kwk,t,r -w0,rk2 ≤ kwk,r - w0,rk2 + kwk,t,r -wk,rk2 ≤ R
which completes the proof.
D Proof of Theorem 3
Before we start the proof, we introduce several notations. Define
(i)	ξ
I1,k = √= Ear (σ(hwk+1,r , Xi)) 一 σ(hwk,r , Xii))
m r∈Si
(i)	ξ
I2ik = √m Σ ar (σ(hwk+1,r, Xii) - σ(hwk,r, Xii))
m r∈Si⊥
Let
I1,k = hI1(1,k),...,I1(,nk)i
and similarly,
I2,k = hI2(1,k),...,I2(,nk)i
Then we have u(ki+) 1 - u(ki) = I1(i) + I2(i) and uk+1 - uk = I1,k + I2,k. Also, we define H(k)⊥ to be
H(k)j = M X(Xi, Xji I{hwk,r, Xii ≥ 0, hwk,r, Xj) ≥ 0}
m
r∈Si⊥
For k-th global iteration, first local iteration, we define the mixing gradient as
p ∂L
gk,r = ηk,r
ηk,r
Σ
l=1
p
X
l=1
p
∂wr
∂Lmlk (Wk)
∂wr
n
'用 XX mkr (Ukci)- yi)arXiI{hwk,r, Xii ≥0}
m
l=1 i=1
m
1
X(fk(i,r) - Nk⊥,ryi)arXiI{hwk,r, Xii ≥ 0}
i=1
where we define the mixing function as
p
ft? = ηk,r X mk,rUkei)
l=1
20
Under review as a conference paper at ICLR 2022
As usual, we let fk,r = fk(1,r), . . . , fk(n,r) . We note that fk(i,r) has the form
ηk,r
1
p
X mlk,rukl(i)
l=1
mp
ar	ηk,r	mlk,rmlk,r0	σ(hwk,r, Xii)
r0=1	l=1
Let νk,r,r0 = ηk,r Plp=1 mlk,r mlk,r0. The mixing function reduce to the form
1m
尸 E arVk,r,rθσ(hWk,r, Xii)
r=1
Also, note that if Nk⊥,r = 0, we have νk,r,r0 = 0.
We prove Theorem 3 by a fashion of induction. Assume that for k0 < k, we have
E[Mk0],W0,a ky — uk0+1 k22 ≤ (1 — α)E[Mk0-1],W0,a ky — uk0k22+	B1
for some α ≤ 1, and the weight perturbation before iteration k is bounded by
2nK	1
kwk,r - w0,rk2 + 2ητ ∖ ξ^ ( ~E[Mk-ι] [ky - Uk k2] + (K -
mδ α
≤R
With α = 4ηθτλo, R = O (KnO) and
B1
64(1 — ξ)2n3d 68ητ (θ — ξ2)n3
mʌθ	+
pλ0
κ2 + (θ — ξ2)nκ2 + (T — 1)2pCι +
6τ2p	24τ2
8η2 (τ — 1)2n4pC1
--------逅-----------+ 4η(τ — 1)pnCι
λ0
and
B = VB1 + κPξ(1 — ξ)pn
α
With such conditions, we have that with probability at least 1 — 4δ, Hypothesis 1 holds. We start by
showing
EMk [ky — uk+1 k22] ≤ (1 — α)ky — uk k22 + αB1
After that, we show that
2nK	1
kwk+1,r — w0,rk2 + 2ητy ~mδ~ ( αEMk] [ky — uk+1k2] + (K -
≤R
to complete the proof. Throughout this proof, we assume that
nm
ΣΣ hw0,r, xii2 ≤ 2mnκ2 — mnR2
i=1 r0=1
and that
kWokF ≤ √2md — √mR
Note that Lemma 22 and Lemma 23 shows that, as long as m = Ω (log δ), the above assumption
holds With probability at least 1 — δ over initialization. Moreover, Lemma 16 shoWs that as long as
(n log δ、
l^λ^
m
with probability at least 1 — δ over initialization we have
∣S⊥ | ≤ 4mκ-1R
21
Under review as a conference paper at ICLR 2022
To start, expanding the loss at iteration k + 1 gives
EMk ky - uk+1 k22	= ky - uk k22	-	2 hy -	uk, EMk	[uk+1	- uk]i +	EMk kuk+1 - uk k22
= ky - uk k22	-	2 hy -	uk, EMk	[I1,k]i	- 2 hy -	uk, EMk [I2,k]i +
EMk kuk+1 - ukk22
Following previous work, we bound the second, third, and fourth term separately. However, the
second term requires a more detailed analysis. In particular, we let
I01,k = I1,k - ηθτ H(k)(y - uk)
Then the loss at iteration k + 1 has the form
EMk	ky -	uk+1k22	= ky - ukk22	- 2ηθτ hy - uk, H(k)(y	- uk)i	+ EMk	kuk+1	- ukk22	-
2 y - uk, EMk I01,k	- 2 hy - uk, EMk [I2,k]i
≤ (1 - ηθτ λ0)ky - ukk22 + 2 y - uk, EMk I01,k	+
2 |hy - uk, EMk [I2,k]i| + EMk kuk+1 - uk k22
where in the last inequality we use λmin (H(k)) ≥ λ0 from (Du et al., 2018), Assumption 3.1.
Moreover, Lemma 6, Lemma 9, and Lemma 10 shows that under the given assumption, with η =
O ( nτm*n,p} ) Wehave
1	16ηθτ ξ2 (1 - ξ)2κ2n3d	2η3ξ2τ (τ - 1)2n4pC1
Ky - Uk,EMk [I1,kDl ≤ 8ηθτλoky - Ukk2 +	/ ξ (m—：)------------+，ξ ( θλJ	p 1
Ky - Uk,EM% [I2,k]il ≤ 8ηθτλoky - ukk2 + ηλ0ξ 北 W )”“
8	24pτ
+ ηλoξ2(τ - 1)2pCι
+	96Tθ
EMk [kuk+1 - Uk ∣∣2] ≤ 8ηθτλoky - Uk k2 + 17η ξ T θ(θ~ξ InIK + η2ξ2λ0(τ - 1)2PnCI
4p
Putting things together gives
1	16ηθτ ξ2 (1 - ξ)2n3d	2η3 ξ2τ (τ - 1)2n4pC1
EMk [ky - Uk+ιk2] ≤ 1 - 7ηθτλo ky - Ukk2 +	/ T ∖ ξ)— + η ξ ( 一 ) P 1
4	mλ0	θλ0
ηλoξ2(θ — ξ2)nκ2	ηλo ξ2(τ - 1)2pCι	17η2 ξ2τ2θ(θ — ξ2)n3κ2
—24TP- + —"— +--------------------------p——+
η2 ξ2λ0τ (τ - 1)PnC1
≤ (1 - 4ηθτλo) ky - Uk k2 + 4ηθτλ0B1
Therefore, We have
E[Mk-ι] [ky - Uk I∣2] ≤ (1 - 4ηθτλθ) ky - U0k2 + BI
This completes the first part of the proof. To shoW the second part, using Jensen’s inequality, We
have EMk [∣y - Uk+1∣∣2] ≤ EMk [ky - Uk+1k2] 2 .With α = 4 ηθτλo, we have
EMk [ky - Uk+1k2] ≤ (1 - 2) ky - Ukk2 + pαB1
and thus by taking total expectation, We have
E[Mk],W0,a [ky - Uk+1k2] ≤ (1 - ]) E[Mk-1],W0,a [ky - Uk ∣∣2] + PαB1
Next, We bound the Weight perturbation using induction. Recall our induction hypothesis
2nK	1
kwk,r - w0,r k2 + 2ητ∖ ~~T I 二EMk-1],W0,a [ky - Uk k2] + (K -
mδ α
≤R
22
Under review as a conference paper at ICLR 2022
We would like to show that
kwk+1,r - w0,rk2 + 2ητʌ/~m~ 0EEMk]，Wo，a [ky - Uk+1k2] + (K - k - I)B) ≤ R
Using the convergence above and the fact that
kwk+1,r - w0,rk2 ≤ kwk,r -w0,rk2 + kwk+1,r -w0,rk2
it suffice to show that
12nK	∣2nK	∣2nKΒ∖
kwk+1,r - wk,rk2 ≤ ητ ∖J ~m EMk-1],W0,a [ky - Uk k2] + 2ητ ∖∣ ^"B - 2力\/ mδα
Recall the definition of B as
B = VBO1 + Kp(IFn
It then suffice to show that
ll	ll	∕2nKτrπ	rιl 一	/
kwk+1,r - w0,rk2 ≤ 〃T旷 ~mδEMk-1],W0,a U' - Uk ㈤ + 2ητκn∖∣
2ξ(1 - ξ)pK
By Hypothesis 1 we have
2ξ(1 - ξ)pK
ll l	ll	∕2nKr	rll ll 1	[
kwk,t,r - Wk,rk2 ≤ ητV ~m~ EMk-1],W0,a [ky - Uk k2] + 2ητκnψ
for all l ∈ [p] and t ∈ [τ]. Then we have
mδ
mδ
p
kwk+1,r - wk,rk2 ≤ ηk,r	mlk,rkwlk,τ,r - wk,rk2
l=1
≤ ητ
∕2nK
V ~m~EMk-1],W0,a [ky -
Uk k2]+2ητκn 产Ξ平
mδ
which completes the proof of the second part. Next, we use over-paramterization to show that the
base case also satisfies the condition above. In particular, we want to show that
2ητ
1EW0,a[ky-
U0k2]+KB
≤ R ≤窑
Equivalently, we want
κλ0 Jn⅛=ω (maχ {1EWo,a[ky -u0 k2]+KB
We use Lemma 26 to get that
EW0,a ky - U0k22 ≤ C2n
Plugging in the bound above and a = 4ηθτλ0, and solving for
gives that
Solving for
23
Under review as a conference paper at ICLR 2022
gives that
m2 = Ω
(n3K3B2η2T2 ʌ
I	κ2λ0δ	)
Using the requirement of η gives
m2 = Ω
ηnK3B2τ
κ2 λoδ
Plugging in the definition of B gives
(nK 3B1	ξ(1-ξ)K 3p∖
mi(θf + —δ-)
Combining the three requirements above gives
m = Ω
n4	nK 2Bi
κ2ξθλ0， κ2θλ0
E Lemmas for Theorem 3
Lemma 1. The expectation of the mixing function satisfies
EMkhfkiri = θuki) + θ(√mξ) arσ(hwk,r0, Xii)
Proof. Note that if Nk⊥,r = 0, then we have fk(i,r) = 0 for all i ∈ [n]. Thus
EMkhfkir | N⊥r = θ] = 0
Moreover, if Nk⊥,r = 1, the expectation can be computed as
pm
EMklfkir | N⊥r = 1] = EMk	√k= XX mk,rmk,r0 ar σ(hwk,r, Xii) | N⊥,r = 1
m l=1 r0=1
ξ
m
X EMk
r0=1
m
p
ηk,r	mk,r mk,r0 | Nk,r = 1 ar0 σ(hwk,r0 , Xii)
l=1
ar0 σ(hwk,r0 , Xii) +
r0=1
F ar σ(hWk,r0, Xii)
by using Lemma 20. Combining the two conditions above gives that
EMkhfkir i = p(N⊥r=1)EMkhfkir ∣ N⊥=1i + p(N⊥r=0)EMkhfkir ∣ N⊥="
θξ S // v∖一θ(I- ξ)	，/B	v \\
=-J=),ar0 σ(hWkro, Xii) +--ar σ(hWk,r0, Xii)
√m 匕	√m
_((i)ɪ θ(1- ξ),, 外
=θuk +	√—— ar σ(hwk,r0, Xii)
□
Lemma 2. The expectation of the mixing gradient satisfies
EMk [gk,r]
θ∂L(Wk)
ξ	∂Wr
+
θ(1-ξ)
m
n
Xiσ(hWk,r , Xii)
i=1
24
Under review as a conference paper at ICLR 2022
Proof. With the result from Lemma 1, we have
1 n	(i)
EMk [gk,r] = √m X (EMk fk(i,r) - yiEMk Nk⊥,r	arxiI{hwk,r, xii ≥0}
θ n	(i)	1 - ξ
=√m ɪ2 (Uk - yi + √m ar σ(hwk,r, Xii)) arxi I{hwk,r , Xii ≥ 0}
θ∂L(Wk)	θ(1-ξ) S ，/
=+	Σxiσ(hwk,r，Xii)
□
Lemma 3. Suppose m ≥ p. If for some R > 0 and all r ∈ [m] the initialization satisfies
m
hw0,r, xii2 ≤ 2mnκ2 - mnR2
r=1
and for all r ∈ [m], it holds that kwk,r - w0,r k2 ≤ R, the expected norm of the difference between
(i)
the mixing function and uk satisfies
EMk kfk,r - ukk22 | Nk⊥,r = 1 ≤
8(θ - ξ2)nκ2
p
Proof. Since EMk νk,r,r0 | Nk⊥,r = 1
r1 , r2 that is not r . Thus
ξ for r0 6= r, we have for r1 6= r2, there is at least one of
EMk (νk,r,r1 - ξ)(νk,r,r2 - ξ) | Nk⊥,r = 1 = 0
and for r 6= r0
VarMk (νk,r,rz | Njr = I)= EMk [(νk,r,rz - ξ)2 | Njr = 1]
Moreover, for r = r0, Lemma 19
EMk [(νk,r,r, - ξ)2] ≤ θ - ξ2
Therefore, using Lemma 21 we have
EMk
fk(,ir) -u(ki)2 | Nk⊥,r
1 Dm	\2
m EMk I (E ar (νk,r,r0 - ξ)σ(hwk,r', Xii))	| N⊥ = 1
1m
嬴 E VarMk (νk,r" N⊥r = 1) σ(hwk,r', Xii)2 +
r0 =1
^^EMk
[(νk,r,r, - ξ)2] σ(hwk,r, Xii)2
Plugging this in gives
θ - ξ
pm
hwk,r0 ,Xii2
r0 6=r
θ - ξ2	2
+ —σ(hwk,r, Xii)
2(θpmξ2) (XIhwo,,, Xii2 + mR2) +2θ-∙^ (hWk,,, Xii + R2)
8(θ - ξ2)κ2
P
EMk [kfk,r- Uk k2 I N⊥r = 1] ≤
8(θ — ξ2 )nκ2
1
≤
≤
≤
m
m
P
□
25
Under review as a conference paper at ICLR 2022
Lemma 4. Under the condition of Lemma 3, the expected norm and squared-norm of the mixing
gradient is bounded by
EMk [kgk,rk2] ≤ 2mθ ky-ukk2+
16θ(θ - ξ2)n2κ2
pm
EMk [kgk,r k2] ≤√nθ ky - ukk2+4nκjθ([mξ)
Proof. Using Lemma 3, we have
EMk	Nk⊥,rkfk,r -ukk22	=P	(Nk⊥,r	= 1)EMk	kfk,r -	ukk22	≤
8θ(θ - ξ2)nκ2
p
According to Jensen’s inequality, we also have
EMk [N⊥rkfk,r - Ukk2] ≤ 2κ ∙ j2Wθ -ξ2)n
Moreover, we have
- yi ar Nk⊥,r xi I{hwk,r, xi i ≥ 0}
N⊥
—Uki)) arN⊥rXiI{(Wk,r, Xii ≥ 0} + ~ξkL
∂L(Wk)
∂wr
Therefore,
EMk kgk,rk22 ≤
2EMk	Nk⊥,r
ξ2
∂L(Wk)
∂wr
2
2
2
+ m EMk
X fk(i,r) -u(ki) arNk⊥,rxiI{hwk,r, xii ≥ 0}
≤
2n
-EMk
mk
n
i,r) -u(ki)2
2nθ	2
+ mrky - ukk2
≤
≤
2n
m (EMk [Nk,r kfk,r - Uk II2] + θky - Uk II2)
2nθ	2	16θ(θ - ξ2 )n2κ2
ky - Uk I2 +
m
pm
This shows the first inequality. To show the second, similarly we have
EMk [Igk,r I2] ≤
≤
Mk ； k」dLWk )	+	√= EMk	X(fkir -	Uki))	ar N⊥rxiI{hwk,r , xii	≥	0} 1	]
ξ	∂Wr	2 m	i=1	2
√m EMk "N⊥r Xlfki)-Uki)I + √nθ ky-ukk2
≤
≤
JmEMk [N⊥rkfk,r - Uk l∣2] + √m ky - Uk l∣2
√nθ.............. ∣θ(θ - ξ2)
I- ky — Uk k2 + 4nκ∖ --------
m	pm
□
Lemma 5. Under the condition of Theorem 3, we have
∣uk(,it) - uk(* i)∣ ≤ ηt√nky -UIkk2
26
Under review as a conference paper at ICLR 2022
and therefore,
Qlki -Ukei)I ≤ ηt√n(ky -ukk2+ IlUk - UIkII2)
Uki)- UkCi)) ≤ 2η2t2n (∣∣y -Uk∣∣2 + IlUk-Uk∣∣2)
Proof. We have
-Ukci)
t —1
≤ η√nX l∣y - uk,j2
t0=o
Therefore,

≤ ηt√nky — U M2
Uki) - UkCi) l ≤ ηt√n (ky - Ukl∣2+ IlUk - Uk∣∣2)
Moreover,
入 l(i)	入 l(i)、	I 入 l(i) 入 l(i)∣ / c2√,2 ∕∣∣	∣∣2∣∣∣	ʌ l ∣∣2∖
uk,t - Uk ) = I uk,t - uk I ≤ 2η t n (ky - uk ∣∣2 + lluk - Ukk2)
□
Lemma 6. Under the condition of Lemma 3, with η ≤
λ0
16(τ —1)n2
,we have
Ky -Uk,EMk [11,kDl ≤ 8ηθτλo∣∣y-ukIi2 +
16ηθτξ2(1 - ξ)2κ2
mλo
n3d + 2η3 ξ2τ (τ — 1)2n4pC1
θλo
Proof. We start by analyzing wfc+1,r - wfc,r. Taking expectation, we have
EMk ∖wk+1,r - wk,r] = -ηEMk
p τ—1 ∂L1
η^ XX
∂ Wr
-ηEMk	ηk
P
,r T X
l=1
∂ Wr
p T—1 (QL
+ ηk,r XXI-
mk (Wk,t) ∂Lmk (Wk)
∂ Wr
∂wr
∂Lmk (WQ
—
P τ —1
，r XX
-ητEMk ∖gk,r] — η^Mk ηk
ALmk (wk,t)	∂Lmk (Wk)
∂wr
∂wr
—
—
ηθτ ∂L(Wk) ηθ(1 - ξ)τ
-------------------
ξ	∂wr
n
EXiσ(hwk,r, Xi)-
i=1
m
P TT / ∂L1
ηEMk	ηk,r E E
mk (Wk,t)	∂Lmk (Wk)
l=1 t=1
∂wr
∂wr
—
27
Under review as a conference paper at ICLR 2022
Therefore
EMkhI(,k i = √ξm X ar EMk HhWk+1,r, Xii) - σ(hwk,r, Xii)]
m r∈Si
ξ
=―^ E ar hEMk [Wk + 1,r — Wk,r] , Xii I{(Wk,r , Xii ≥ 0}
r∈Si
—
√θ= X ar CddW) , x' I{hwk,r , Xii ≥ 0} - η (Elik +
r∈Si
ηξθτ n
m^~ X X(yi - Uk) hxi, Xji I{hwk,r, Xii ≥ 0, hwk,r, xj i ≥ 0} - η (E1,k +
r∈Si j=1
n
ηθτX (H(k)ij - H(k)⊥) (yj - ukj)) - η (嵋 + E^)
j=1
where
θξ(I - ξ)τ	/	\
------3一工 Ja hXi, Xj i σ(hwk,r , Xj i)
m2	r∈Si j=1
Let E1,k
N X"EMk
p τ-1
ηk,rXX
, and E2,k
∂Lmlk	Wkl,t	∂Lmlk (Wk)
∂wr
∂wr
. Then we have
)[，Xi + I{(Wk,r, Xii ≥ 0}
—
EMk [Iι,k] = ηθτ (H(k) - H(k)⊥) (y - Uk) - η (Eι,k + E2,k)
Thus,
EMk I01,k = EMk [I1,k] - ηθτ H(k)(y - Uk)
= ηθτ H(k)⊥ (y - Uk) + η (E1,k + E2,k)
According to Lemma 7 and Lemma 8, we have the bound of E1(,ik) and E2(,ik) as
≤ θξ(1 - ξ)τnκ
≤ ηξτ(T - 1)n2
≤	2
2d
m
θky - Uk l∣2 + ppCI)
Moreover, according to Lemma 17, we have
kH(k)⊥k2 ≤4ξnκ-1R
Let R ≤ 离,wehave
kH(k)⊥k2 ≤ λ0
28
Under review as a conference paper at ICLR 2022
Therefore, we have
n
Ky - Uk, EMk [I1,kD l ≤ ηθτ l〈y — uk, H(k)⊥ (y - Uk)〉l + η X Q (仍—uk") E*[ + 〔(9
i=1
n
=ηθτ 但仿尸心切—Ukl2+ 喘(阳 l + ∣E2ik ∣) η X ∣ m — uk1
i=1
≤ 312ηθτλoky — ukk2+
≤ 3ι2ηθτλoky — ukk2+
max (S? | + |嚼 D η√n∣y - uk∣2
η2θξτ (τ — 1)n2
l∣y ― Uk I∣2+
ηθξ(1 — ξ)τκ↑J-
2n3 d + η2ξτ (τ — 1)n2
l∣y — Ukk2
Using the general inequality that ab ≤ ɪ (α2 + b2), and η ≤
λ0
, we get
一Uk,EMk [ι]kDl ≤ §ηθτλo∣∣y - uk∣∣2 +
16ηθτξ2(1 — ξ)2κ2
mλo
n3d + 2η3 ξ2τ (τ — 1)2n4pC1
θλo
m
2
2
□
Lemma 7. Under the assumption ofTheorem 3 we have that for all k ∈ [K ],i ∈ [n], it holds that
≤ θξ(1 - ξ)τnκV2d
Proof. We have
θξ(i — ξ)τ
θξ(i — ξ)τ
≤
3
m 2
≤
3
m 2
n
ΣΣar (Xi, Xji σ(hwk,r, Xji)
r∈Si j = 1
n
XX Kwk,r , Xiil
r∈Si j = 1
≤
≤
≤
θξ(1 — ξ)τn
θξ(1 — ξ)τn
3
m 2
θξ(1 — ξ)τn
E llwk,r ∣2
r∈Si
X (kwo,r∣2 + R)
r∈Si
∣∣w II , θξ(1 — ξ)τnR
kWokF + —√m—
3
m 2
m
≤ θξ(1 — ξ)τnκ∖∕2d
m
where for the bound of ∣ Wo ∣∣f we use Lemma 22.
□
Lemma 8. Suppose ∣∣wk,t,r — wo,"∣2 ≤ Rfor all r ∈ [m]. Then we have
一，	一、	3
≤ ηξτ(T ― 1)n2 3y ―Uk∣2 +
29
Under review as a conference paper at ICLR 2022
Proof. Since r ∈ Si, the difference between the surrogate gradients of a sub-network has the form
∂Lm (wk,t)	∂Lmk (Wk)
∂Wr
∂ Wr
n
X ar加k,rxj (UIkjt)
j=1
ln
≤ 2⅛r X
j=1
—
Ukj)) I{hwk,r, Xj) ≥。}
—
2
1
—
2
Therefore, using the convexity of '2 -norm,
EMk
P
nk,rf
l=1
ALm NIk)	∂Lm (Wk)
∂Wr
∂ Wr
≤ EMk
2
≤ EMk
ηk,r
P
ηk,rf
l = 1
aLmk WIk)	∂Lmk (Wk)
∂ Wr
∂Wr
ALm (wk,t)	∂Lmk (Wk)
∂Wr
∂Wr
2
—
P
X
l = 1
—
—
2
n
P
≤ EMk
XX)-)|
V	l=1	j=1
By Lemma 5, We have
Ufi)- Ukei)
I ≤ ηt√n (Ily - Uk∣∣2 + IlUk
-UIkl∣2)
Therefore,
Tm X EM
r∈Si
P τ — 1
ηk,r XX
∂Lm (wk,t)	∂Lmk (Wk)
∂Wr
∂Wr
EE Em
t=1 r∈Si
ξ τ—1
而 XXEMk
t=1 r∈Si
片 3 τ —1
此X tX
t=1 r∈Si
ηξτ(τ — 1)n2 /
EMk
ηξτ (T — 1)n 2
P
ηk,rf
l = 1
∂Lm (wk,t)	∂Lmk (Wk)
∂Wr
—
∂Wr
ηk,rfmk,r (Ily - uk∣2 + Ily - Uk ∣∣2)
l=1
θ∣∣y - Uk ∣∣2 + EMk
ηk,r X Mk,r∣uk - Uk ∣2 j
θ∣y - Uk ∣∣2 + PCi)
≤
≤
≤
≤
≤
≤
k
—
2
ξ
T — 1
2
2
k
P
n
√≡ X 八 X
—
2
where the last inequality follows from Lemma 24.
□
Lemma 9. Under the condition of Theorem 3, we have
Ky - Uk, EMk [I2]i∣ ≤ 1 ηθτλo∣y - Uk∣∣2 +
8
ηλoξ2(θ -ξ2)nκ2
24pτ
+ ηλoξ2(τ - 1)2pC1
+	9βΓθ
30
Under review as a conference paper at ICLR 2022
Proof. To start, we notice that Using the I-Lipschitzness of ReLU, we have
EMjKkn=√m EMk
E ar (σ((Wk+ι,r, Xii - σ((Wk+ι,r, Xi))
r∈S⊥
ξ	E 「I / /	/ /
≤√= Σ EMk [∣σ(hwk + 1,r, Xii - σ(hwk + 1,r, xii∣]
Vm r∈S⊥
ξ	E 「I /
≤-⅛ Σ EMk [∣(wk+1,r - wk,r, xii∣]
VMr∈S⊥
ξ
≤ √m E EMk [∣∣wk+1,r - wk,r k2]
VMr∈S⊥
≤
√⅛ XEMk
T — 1 P
“H XX
≤
^7= X (EMk [∣lgk,r∣∣2]+ EMk
rj	r∈S⊥ ∖
τ — 1 p
“U XX
≤
-Uk ∣∣2 +
4ηξκn∣S⊥∣
m
「+
≤
ηξ√n V κ
----EM EMk '
mk
r∈S⊥	L
遮√n∣S⊥∣∣y-Uk ∣2 +
τn
'	τ — 1 p
ηk,rEE 尾,r∣y-u "2
_	t=1 l=1
4ηξκn∣S⊥∣ 不θ(θ - ξ2)
m
P
+
τ r-	T	τ—1 p	,
η⅛n X EMk	ηk,r XX 尾,r∣y-U k ∣2
r∈S⊥	-	t=1 l=1
≤
ηθξτ^n ∣ S⊥ Iky-Ukk2 +
m
4ηξκn ∣ S⊥ ∣
m
I+
「 T—1 P	,
等 X EMk	ηk,r XX 尾,r∣Uk- U k∣2
r∈S⊥	L	t=1 l=1
where in the seventh inequality we use the bound on EMk [∣gk,r k2] from Lemma 4. Moreover,
using Lemma 24 we have
p	____
EMk	ηk,rf Mk,r Iluk - Uk l∣2	≤ VPCi
_	l=1	.
Then we have
EMk [ I I2ik I ] ≤ 厘∣S⊥ Iky-Ukk2 + 4η⅞jS⊥∕θ≡≡ + η⅛a√npcτ∣S⊥ ∣
≤ 8ηθξτ√nκTR∣y - Ukk2 + 16ηξnR｛仪。- ξ ] + 4ηξ(τ - 1)k-1 RPnPCI
31
Under review as a conference paper at ICLR 2022
where in the last inequality We use ∣S⊥ | ≤ 4mκ-1R. Therefore,
|hy - uk, EMk [I2,k]i| = X(yi - u(ki))EMk hI2(i,k) i
i=1
n
≤XIyi-Uki)HEMk [ι2iki∣
i=1
n
≤ mi∈a[nx] ∣∣∣EMk hI2(,ik) i∣∣∣ X ∣∣∣yi - u(ki)∣∣∣
i=1
≤ √n max IEMkhI2,k i∣ ky- ukk2
≤ 8ηθξτκ-1nRky - Uk∣∣2 + 16ηξR Jθθ -pξ )n ∣∣y - Ukk2 +
4ηξ(τ - 1)κ-1nRppC1ky - Uk∣2
≤ 1 ηθτλoky - Ukk2 +
8
ηλoξ2(θ - ξ2)nκ2 + ηλoξ2(τ - 1)2pCι
24pτ
96τθ
where in the last inequality we use R ≤ ^920? and ab ≤ 2(a2 + b2).
□
Lemma 10. Under the condition OfTheOrem 3, with η ≤4&?「nɪ[n p｝，we have
EMk [∣Uk+ι — Uk 112] ≤ 1 ηθτλo∣y — Uk∣∣2 + 17η ξ τ θ(θ_ξ )n κ + η2ξ2λo(τ — 1)2pnCι
Proof. As in previous lemma, we use the Lipschitzness of ReLU to get
EMk	u(ki+) 1 - u(ki)
k+
where
≤ ξmEMJ (X ar (σ(hwk+1,r, Xii)- σ(hwk,r, Xi〉)))
≤ ξ2 XEMk (σ(hwk+1,r, xii) - σ(hwk,r, xii))2
r=1
m
≤ ξ2 X EMk hwk+1,r - wk,r, xii2
r=1
m
≤ ξ2 X EMk ∣wk+1,r - wk,r ∣22
r=1
= ξ2 (D1,k + D2,k)
D1,k = X EMk ∣wk+1,r - wk,r ∣22
r∈Si
D2,k = X EMk ∣wk+1,r - wk,r ∣22
r∈Si⊥
Using Lemma 11 and Lemma 12 we have
Dι,k ≤ (4η2τ2nθ + 4η4θn3τ3(τ - 1)p) ∣∣y - Uk ∣∣2 + 16η T θ(θξ )n κ +4η4n3τ2(τ - 1)2pCι
D2,k ≤ 哈(1 + (τ - 1)p) ky - Ukk2 + 4η2λ0θ(θ -ξ2)τnκ2 + η2τ(T- 1)λ0pC1
18	9p	18
32
Under review as a conference paper at ICLR 2022
Therefore we have
EMk kuk+1 - ukk22 ≤ ξ2n (D1,k + D2,k)
≤ 卜η2ξ2τ2n2θ + 4η4θξ2n4τ3(τ - 1)p + η ,；；，0 (1 + (τ - 1)p)) ky - Uk∣∣2+
16η2ξ2τ2Wθ - ξ4n3κ2 + 4η4ξ2n4τ2(τ - 1)2pC + 4η2λ0ξ2θ(θ - ξ2kn2κ2
p	9p
η2ξ2τ (T — 1)nλopCι
18
With η
≤ 48nτ max{n,p} , We have
EMk [∣Uk+ι — Uk ∣∣2] ≤ 1 ηθτλo∣y — Uk∣2 + 17η ξ T 队P~ξ InKI + η2ξ2λ0(τ — 1)2pnCι
□
Lemma 11.
Dι,k ≤ (4η2τ2nθ + 4η4θn3τ3(τ — 1)p) ∣∣y — Uk ∣∣2 +
16η2τ 2θ(θ Y2 + 4ηin3τ 2(τ - i)2pci
p
Proof. We have
D1,k = η2	EMk
r∈Si
τ-1 p
ηk,rXX
≤ η2	EMk
r∈Si
τ-1 p
τ gk,r + ηk,r
∂Lmk (Wk )
∂wr
τ-1
≤ 2η2τ2 XEMk [∣gk,r∣22] + 2η2(τ — 1)p X X EMk
r∈Si	r∈Si t=1
p
ηk2,r X
l=1
2
∂Lmk (Wk ) Il
Mr l2
Note that for r ∈ Si , we have
III ∂Lmlk Wkl,t	∂Lmlk (Wk)
-
I	∂ wr	∂ wr
X arXi (Ukit)- Ukci)) I{hwk,r, Xii} ≥ 0
i=1	I2
2
=mk,r
m
2
-UkCi)
≤
2
≤ nmr (2η2t2n∣∣y - Ukk2 + 2η2t2n∣Uk - Ukk2)
where in the last inequality we use Lemma 5. Plugging in the bound above and the bound on
EMk [∣gk∣22] from Lemma 4 gives
Dι,k ≤ 4η2τ2nθ∣y - Uk∣2 +	16η	T	"(；	ξ	)n	κ	+ 4η4θn3τ3(τ	-	1)p∣y	- Uk ∣∣2+
p
4η4n3τ3(T- I)PEMk η2,r Emk,rllUk - uIk 112
l=1
≤ (4η2τ2nθ + 4η4θn3τ3(τ — 1)p) ∣∣y — Uk ||2 + ^η T °(0——ξ )n κ——+ 4η4n3τ2(τ — 1)2pCι
□
33
Under review as a conference paper at ICLR 2022
Lemma 12.
D2,k ≤ η 18 0 (I + (T - I)P) ky - Uk k2 +
4η2λ0θ(θ - ξ2)τnκ
9p
2
+
η2τ (τ - 1)λ0pC1
18
Proof.
D2,k = η2	E
r∈Si⊥
Mk bX X ⅛wj∣2
≤ η2τ
r∈Si⊥
≤ η2τ
r∈Si⊥
(
E-
/
E-
τ-1	p ∂L
Mk [kgk,rk2] +∑ EMklMr £ 一
τ-1	p
Mk [kgk,rk2] + Pf EMk Iη2,r∑
t=1	l=1
≤ 2η2τnθ∣s⊥∣ky-Ukk2 + 8η2θ(θ-ξ2"K
m
pm
2
∂wr
∂Lmk (Wk,t
∂wr
2
2
2
∣S⊥I + η2τn⅛X EMk
m
t=1
p
η2,r Emk,rky -U k,tk2
l=1
≤ 2η2τnθ∣s⊥lky - Ukk2 + 8η2θ(θ SK ∣s⊥∣ + 2η2θτ(T- InP∣s⊥∣∣y - uk∣2+
m
2η2τ(T- UnP ∣S⊥∣E:
m
Mk
pm
p
ηk,r Emk,rkUk - U k k2
l=1
m
Using ∣S⊥∣ ≤ 4mκ-1 R with R ≤ ∣κ4n0 gives
D2,k ≤ η 18 0 (I + (T - I)P) ky - Uk k2 +
4η2λ0θ(θ - ξ2)τnκ
9p
2
+
η2τ (τ - 1)λ0p
p
18
EMk η2,r Emk,r kUk - U k k2
l=1
≤ η θτ"0(1 + (t - 1)p) ky - uM∣2 +
18
4η2λ0θ(θ - ξ2)τnκ
9p
2
+
η2τ (τ - 1)λ0pC1
18
□
F Proof of Theorem 4
As in previous theorem, we start by studying ky - Uk+1k22. In the case of a categorical mask, we
have the nice property that the average of the sub-networks equals to the full network
1p
J = P EUk,τ
using this property, Lemma 13 characterize ky - Uk+1 k22 as
1 p	1 p l-1
ky - Uk+ιk2 = PX ky - U k,τk2 - P XX kU k,τ-U k0,τk2
We start by assuming the condition of Hypothesis 1 holds. We proceed by proving the convergence,
then we prove the weight perturbation bound with a fashion of induction. Hypothesis 1 implies that
τ
l∣y - U k,τk2
l∣y - U k k2
ky - UIkk2 - ηλ0 X
t
ky -Ulk I∣2
t0
34
Under review as a conference paper at ICLR 2022
Using the fact that EMk [uk] = uk, We have that
EMk [ky - UkI∣2] = l∣y - ukk2+ EMk [kuk - Uk 112]
Therefore, We have
EMk [ky	- uk+ιk2] = — XEMk [ky -UIkk2]	-	n2-0XX (1-	η20)	EMk	[ky	-UIk∣∣2]-
p l=1	2p t=0 l=1	2
pl
∑2 XXEMkhkUk,τ - Uk,τk2i
p l=1 l0=1
=ky -Ukk2 -守XX (1 — ηλ0)tEMk [ky - Ukk2] +
1p	1 p l	0
~X EMk [kUk - Ulk k2] - ∑2 XX EMJkUk,τ - Uk,τk2]
p l=1	p l=1 l0 =1
Lemma 14 studies the error term kUk 一 UIk k2 and gives
p	p l-1
X kUk - Uk k2 = BXX kU lk - U lkk2
l=1	p l=1 l0=1
Plugging in We have
EMk [ky	-	Uk + 1k2]	≤ ky - Uk k2----ηp X (1 -	η20)	X EMk	[ky	-	UIk k2]	+
pl
* XXEMk [kUIk - UIk k2 - kUlk,τ - Uk,τ k2]
p l=1 l0=1
Let
pl
ιk = -2 X X EMk [kUk - Uk k2 - kUk,τ - Uk,τk2]
p l=1 l0=1
Lemma 15 shows bound of the expectation of ∣k with respect to the initialization. In particular,
under the assumption that the network initialization satisfies kW0,rk2 ≤ K √2md - √mR for some
R ≥ 0, and the weight perturbation is bounded by kwkl ,r - w0,rk2 ≤ R for all r ∈ [m] then we
have for all γ > 0
p τ-1
Ik ≤ 您0 XXEMk [ky - Uk,tk2] + ∣k
with
EW0,a [ι0k] ≤
32ητ (p - 1)2κ2n3d
γλ0 m
35
Under review as a conference paper at ICLR 2022
Using this result, we have that
EMk [ky - Uk + 1||l] = ∣∣y - Uk ∣∣l -
Ily — Uk111 -
ηλo X (
F U
ηλo XX
=tU
tP
X EMk [ly - UIk kl] + ιk
l = 1
)EMk [∣y - UIkιιl]+
E XX C-η
P 1=1 t=0 ∖
1 P l-1
pl XX EMkWi
P l = 1 l0 = 1
t
EMk [∣∣y - uk kl^] +
32ητ (p — 1)2κ2n3d
γλ0m
+
ky - Uk kl - (1-2γhλ0 XX
P	l = 1 t=0
t
EMk [∣∣y - UIk kl] +
32ητ(p - 1)lκln3d	1 X XiiT r i,i'↑
+ pl	EM EMkFkl
γλ0m
≤∣y - Uk kl -上r X
t
l∣y - Ukkl+
32ητ(p - 1)lκln3d	1 X XiiT r i,i'↑
+ pl	EM EMkFkl
γλ0m
Y + (1 - Y)
y — Ukkl +
32ητ (p — 1)2κ2n3d
γλ0m
+
1 P l-1
pl XX EMkWi
ι ι=ι i0=ι
Therefore,
EMk,Wo,a [∣∣y — uk + 1∣∣l] ≤ (Y + (1 -Y)
y - Ukk| +
32ητ (p — 1)lκln3d
γλ0m
Also,
E[Mk] [ky - Uk ∣∣l] ≤ (Y + (I-Y)
y — uokl +
pl X X X E[Mk,]"0i
P l = 1 k0 = 0 l0 = 1
32ητ (p — 1)lκln3d
+
and therefore,
E[Mk],Wo,a [ky - Uk IIl] ≤ (Y + (1 -
l∣y — uokl+
64τ (p - 1)lκln3 d
λ0γ(I-Y)m
32ητ (p — 1)lκln3d
≤ y + (1 —
Next we bound the weight perturbation. Similar to the proof of Theorem 3, it suffice to show that
∕2nK	∕2nK	∕2nKBι
llwk+l,r - w0.r kl ≤ ητy ^mδ^E[Mk-1],Wo,a [∣y - Ukkl] + 2ητy ^yB - 2ητ ∖∣ mjɑ
36
Under review as a conference paper at ICLR 2022
where in our circumstance, We have ξ = P. Again, We recall the definition of B as
B = VBOr + κpξ(1 - ξ)pn
with
It then suffice to shoW that
B1
32ητ (p - 1)2κ2n3d
ll	ll	∣2nK°	rιl	- C /
kwk + 1,r - w0,rk2 ≤ ητy ~mδ~E[Mk-ι]h,Wo,a [ky - ukk2] + 2ητκn∖∣
2ξ(1 - ξ)pK
mδ
By Hypothesis 1 We have
ll l	ll ∕2nKτπ	rιl 一	i
kwk,t,r - wk,rk2 ≤ ητV ~mδ~ EMk-1],W0,a [ky - Uk l∣2] + 2ητκn∖∣
2ξ(1 - ξ)pK
mδ
for all l ∈ [p] and t ∈ [τ]. Then We have
p
lwk+1,r -wk,rl2 ≤ ηk,r	mlk,rlwkl,τ,r -wk,rl2
l=1
≤ ητ
∕2nκτrπ
E
[Mk-1],W0,a
川	∕ξ(1-ξ)pK
[ky -Ukk2] + 2ητκnV	mδ
In the end We use overparameterization to shoW for k = 0. In particular, We shoW
2ητv 誓 αE EW0,a [ky-u0k2]+KB) ≤ R=O(KnO
With
α = (I-Y) (l-(l-* ))≥ (I-^
As before, using Lemma 26, We have
Ewo,a [ky - U0k2] ≤ (EW0,a [|卜 - U0∣2]) 2 = C√n
Using the same technique as in Theorem 3, We have
m = Ω
τ2K max{n4, n2d, pK2 }
((1- Y )2 K2λ0δ√γ	)
G Lemmas for Theorem 4
Lemma 13. The kth global step produce the squared error satisfying
1 p	1 p l-1
ly - uk+1 l22 =P X ky - Uk,τk2 - P XX kuk,τ - Uk,τk2
37
Under review as a conference paper at ICLR 2022
Proof. We have
1p
I∣y - uk+ιk2 = y - - Eu k,τ
p l=1	2
=122 XXDy - u k,τ，y - u k0,τ)
p l=1 l0=1
=P X ky - u k,τk2—P x ky - u k,τk2+p2 XXDy - u k,τ, y - u k,τ E
p l=1	p l=1	p l=1 l0=1
1p
=p X ky - u k,τk2-
P l=1
pp	pp
2P2 (XX (ky - uk,τk2 + ky - uk,τk2) - XXDy - uk,τ,y - uk,τE )
l=1 l0=1	l=1 l0=1
p	pp
=BX ky - uk,τk2 - 羽 XX kuk,τ - uk,τk2
P l=1	2P l=1 l0=1
p	p l-1
=BX ky - uk,τk2 - ∑2 Xx kuk,τ - uk,τk2
P l=1	P l=1 l0=1
□
Lemma 14. We have
p	p	l-1
X kuk-uk I2 = 1XX ku k- u lkk2
l=1	P l=1 l0=1
38
Under review as a conference paper at ICLR 2022
Proof. Using uk
P Pp=I Uk We have
EkUk-Ukk2 = E PEUk-Uk
l=1
l=1
l0=1
2
1p p	p
-1XXDU Ik, U Ik E+X kU Ik k2
p l=1 l2=1	l=1
pp
p
l=1 l0=1
p l-1
1XX ku Ik-U k0k2
p l=1 l0=1
p
p
p
2
□
Lemma 15. Ifthe network initialization satisfies ∣∣W0,rk2 ≤ K√2md — √mR for some R ≥ 0,
and the weight perturbation is bounded by kwlk,r - w0,r k2 ≤ Rfor all r ∈ [m] then for all γ > 0
we have
p τ-1
∣k ≤ ηγλ0XXEMk [ky-uk,tk2] + ∣k
with
EW0,a [ι0k] ≤
32ηκ2τ (p - 1)2n3d
γλ0m
Proof. First, We have
kWkl kF = kW0kF+kWkl -W0kF
κ
κ
m
√2md - κR√m + I X |皿匕-wo,rk2
r=1
V2md — κR√m + R√m
1
2
K λ∕2md
For convenience, denote
σk,(i) = σ (〈wk,r, Xiy) ；	σk,+i),r = σ(<wk,τ,r, Xi
39
Under review as a conference paper at ICLR 2022
Note that
n2
kuk -uk0k2 -kuk,τ-uk0,τk2 = X ((Uk(i) -u")) -(Uk(T)-Uk,τi)
i=1
where
nm
"k' = m XXX ar ar0
i=1 r=1 r0 6=r
(i)	l,(i)
,r - σk+1,r
l0 l0,(i) l0,(i)
- mk,r σk,r - σk,r
mlk,r0	σkl,,(ri0) + σkl,+(i1),r0
- mlk0,r0	σkl0,,r(0i) + σkl0,,r(0i)
Using independence between ar and ar0 we can see that EW0 ,a ιlk,l0 = 0. Moreover, since for
l 6= l0 we have mlk,r 6= mlk0,r, it is obvious that mlk,rmlk0,r = 0 for l 6= l0. Therefore,
nm
ku Uk-U lkk2 -ku k,τ-u lk,τk2 = m XX (mk,r (σlk(i)2 - σ*J - mk； (σ1产-σ：备))+ ιk,l0
i=1 r=1
nm
≤ m XX(kk,r -σk*2r∣ + H,,p
i=1 r=1
- σkl0+,(1i),r2
+ ιlk,l0
For all l ∈ [p], r ∈ [m], and all q > 0 we have
mm
X 喏)2 - σkX∣ ≤ X 卜器-σk+i),r 卜卜婷 + σlk+[r∖
r=1	r=1
m
≤ X ∖∖w'k,r - wk + 1,rl∣2 ∙ (llwk,rl∣2 + UWk+1,/2)
r=1
where the second inequality uses the 1-Lipschitzness of ReLU and the fact that kxik2 = 1. Since
we have for all r ∈ [m], it holds that
τ-1
llwlk,r - wlk+1,rll2 ≤ η X
t=0
40
Under review as a conference paper at ICLR 2022
Then
m
Xσkl,,(ri)2
r=1
-σ
wlk,r2 + wkl+1,r2
≤ η√n∑ ky - U k,tk2 (kWk,rkF + kWk + 1,r kF)
t=0
τ-1
Also, in the last inequality
≤ 4ηκ√mndE ky - U k,tk2
t=0
≤ 4ηκ√mnd X ky - Uk,tk2 + 4ηκqτ√mnd
q t=0
We use 2ab ≤ aq2 + qb2 for all q > 0. Plugging in the choice
8(p — 1)κVn3d
γλ0m
with some γ > 0 gives
m
Xσkl,,(ri)2 - σkl,+(i1)2,r ≤
r=1
Therefore
ηγλ0m
4n(p - 1)
τ-1
X ky - uk,tk2 +
t=0
16ηκ2τ (p - 1)n2d
γλ0
EMk hkuIk- UIk k2 - kuk,τ - uk,τk2i
ηγλ0p
≤ 4(P-1)
τ-1
X (EMk [ky - Uk,tk2] + EMk [ky - Uk,tk2]) +
t=0
16ηκ2 τp(p - 1)n3 d
γλ0m
+ EMk hιlk,l i
Therefore
1 p l-1
ιk = — X X EMk [kuk - UIk k2 -kuk,τ - Uk,τk2]
p l=1 l0=1
p l-1
=≤ 4p(P-01) XX (EMk [ky -Uk,tk2] + EMkhky -Uk,tk2i) +
32ηκ2τ (p - 1)2n3d
γλ0m
p l-1
1XX EMMi
l=1 l0=1
p τ-1
"ɪ2p^ XXEMk [ky - uk,tk2] +
32ηκ2τ(p - 1)2n3d
γλ0m
p l-1
+ P XX EMkhIk,l i
p l=1 l0=1
q
+
□
H Auxiliary Results
Lemma 16. With probability at least 1 - ne-mκ-1R we have |Si| ≤ 4mκ-1 R for all i ∈ [n].
Proof. Note that I{r ∈ Si⊥ } = I{I{Air } 6= 0} = I{Air }. Therefore, we have
m
∣S⊥I = XI{r ∈ S⊥} = RA/
r=1
41
Under review as a conference paper at ICLR 2022
Since Ewo,r [H{Air}] = P (Air) ≤ ι√∏ ≤ KTR, we also have
EW0,r [(1{AiJ- Ewo,r [I{ Air}]) [ ≤ Ewo,r [I{ Air}2] = κ√2π ≤ KTR
Again apply Bernstein inequality over the random variable I{Air} - Ewo,r [I{Air}] with t =
3mκ-1R gives
P (∣S⊥∣ ≤ 4mκ-1R) = P (^X I{Air} ≥ 4mκ-1R^ ≤ exp (—mκ-1R)
□
Lemma 17. Define H⊥ ∈ Rn×n such that
H⊥ = —〈Xi, Xji E I{<Wr, Xii ≥ 0;〈Wr, Xi ≥ 0}
m	一 `
r∈S⊥
If ∣S⊥∣ ≤ 4mκ-1R, then we have
∣∣H⊥∣∣2 ≤ 4nξκ-1R
Proof. We note that
n
∣∣h⊥∣∣2 ≤kH⊥kF = E ∣h⊥∣2
i,j=1
For each i, j pair we have
∣H⊥∣≤ -ξ∣S⊥∣ =4ξκ-1R
J m
Thus
,	, 一 1	1
∣∣H⊥∣∣2 ≤ (∣∣H⊥∣∣F) 2 ≤ (1βn2κ"2∆2)"- =4nξκ-1R
□
Lemma 18. For i.i.d Bernoulli masks with parameter ξ, N⊥ ~ Bern(O) with
θ = P(N⊥r = 1) = 1-(1-ξ)p
Proof. We have
P(N⊥r = 1) = 1 - P(N⊥r = 0)
P
=1 - ∏ P(mk,r =0)
l = 1
=1 - (1 - ξ)p
□
Lemma 19. We have
EMk [(νk,r,r - ξ)[ ≤ θ - ξ
Proof. To start, we notice that Vk,r,r = ηk,r PP=I ml2= ηk,r PP=I m匕=N⊥
EMkM ,r,r] = EMkIN⊥J = θ Moreover, since N⊥2 = N⊥, we have EMk [或,rj
using O ≥ ξ, we have
Therefore
O. Thus,
EMk [(νk,r,r - ξ)2] = EMk [ν2,r,r] - 2ξEMk [vk,r,r] + ξ2
=O - 2ξO + ξ2
≤ O - ξ2
□
42
Under review as a conference paper at ICLR 2022
Lemma 20. For i.i.d Bernoulli masks with parameter ξ, we have
EMk [Vk,r,rθ I N⊥r = 1] =	ff Tr
f r = r
Proof. If = 0, we have
p
EMk νk,r,r0 | Nk,r = 1] = EMk ηk,r X mk,r | Nk,r = 1
l=1
=EMk Nkr | N⊥r = 1
Nk,r
= EMk Nk,r | Nk,r = 1]
=1
If 0 6= , then we have that mlk,r0 is independent from mlk,r and Nk,r. Therefore,
p
EMk	νk,r,r0	|	Nk⊥,r	= 1] =	EMk	ηk,r	mlk,r	|	Nk⊥,r	= 1	EMk	mlk,r0 ]
l=1
=ξEMk Nkr | N⊥r = 1
Nk,r
=ξ
□
Lemma 21. The variance follows
VarMk (νk,r,r0 | Nk,r = 1)
θ-pξ2	if r = r0
0 ifr = r0
Proof. For	6= 0, the expectation of νk2,r,r0 given Nk⊥,r = 1 is
Plp 1 Plp0 1 mlk mlk0 mlk 0 mlk0 0
EMk	[ν2,r,r0	I	N⊥r	=	1]	=	EMk	乙'=1 乙'=1 X	" I	N⊥ = 1
Xk,r
p
EMk [mlk,r0]EMk [mlk,r0]EMk
mkr I N⊥r = 1
Xk,r
EMk
l0
mkr I N>1
Xk,r
+
Xp
EMk [mk,r']EMk [mkr | N⊥r = 1
p
ξ2XXEMk
l=1 l0 6=l
Tmkr I N⊥r = 1] EMk [Xkr I N⊥r = 11 +
Xk,r	Xk,r
p	ml
ξ X EMk X,L i N⊥r = 1
l=1	Xk,r
=ξ2 + ξEMk X- i N⊥r = 1 - ξ2 X EMk Imb I N⊥r = 1
Therefore, the variance of νk,r,r0 given Nk⊥,r = 1 has the form
Var (νk,r,r0 i gNk,r = 1) = EMk [νk,r,r01 Nk,r = 1] - EMk [νk,r,r01 Njr = 1]
=ξEMk X- i N⊥r = 1 - ξ2 X EMk X~~ i N⊥r = 1
43
Under review as a conference paper at ICLR 2022
Let X(p) =	lp=ι ml 〜B(p, ξ), then We have
EMl xk,r |	N⊥r	= 1 =	EMl ι+X(P-1)_
EMk mXr |	N⊥r	= 1 =	P(mk，r =1 | N⊥r = I)EMl 1	+ X(P- 1)_ = θEMl 1 +	X(P-	1)_
Moreover, using reciprocal moments We have
1θ
Mk [1 + X (P - 1) J = Pξ
Therefore
⊥	1	ξ4	1	2
VarMk (νk,r,r0 | NkT = 1) = ξEMk [1 + X(P - 1) ] - θPEMk [1+ X(P - 1)
=θ-eɪ
P
If r = r0, the variance is
VarMk (νr,r | gr = 1) = VarMk (gr | gr = 1) = 0
□
Lemma 22. Suppose K ≤ 1, R ≤ Kd32. With probability at least 1 一 emd/32 we have that
∣∣W0∣∣F ≤ κ√2md — √mR
Proof. For all r ∈ [m], d1 ∈ [d], We have
EMk wr2d1 = K2
Moreover, each wr2d is a (2K2, 2K2)-sub-exponential random variable
E Iet(Wrdι-κ2)] =_11= Z	et(Wrdι-κ2e-Wd1 dwrd1
K 2π -∞
1
κ √2∏
Z∞
∞
e-( 2⅛ T)Wrdι-tκ2 dwrd1
=∙ , /	，	∙ e-tκ
κ√2∏ V(2κ)-1-1
=…≤ e2t2κ4
√1 - 2tκ2 ―
with t ≤ 2K2. Thus, using independence between entries of W0 gives
md
E et(kW0k2F-mdκ2)	≤YY
E et(Wr2d1-κ2)	≤ e2mdt2κ4
r=1 d1=1
Invoking the tail bound of sub-exponential random variable gives
(td
e- 8mdκ4	if 0 ≤ t ≤ 2mdκ2
q	.	≤ ≤ 2
e 4κ2	if t > 2mdκ2
Let t = mdκ2 — 2mκR√2d + mR2. Then
kWokF ≤ 2mdκ2 + mR2 — 2mκR√2d = (κ√2md — √mR)2
with probability at least 1-e-8mtdκ4. Using R ≤ Kyjld we have t ≥ ɪmdκ2. Thus with probability
md
at least 1 - e 32 we have
k W0∣∣F ≤ κ√2md — √mR
□
44
Under review as a conference paper at ICLR 2022
Lemma 23. Assume K ≤ 1 and R ≤ √. With probability at least 1 一 ne-m over initialization, it
holds for all i ∈ [n] that
m
hw0,r , xii2 ≤ 2mκ2 一 mR2
r=1
and thus
nm
ΣΣ hw0,r, xii2 ≤ 2mnκ2 一 mnR2
Proof. To begin, we show that each hw0, xii are Gaussian with zero mean and variance κ2. Using
independence between entries of w0,r, we have
E e-thw0,r,xii	= E
d
e-tw0,r,jxi,j
j=1
d
Y E e-tw0,r,j xi,j
j=1
d
Y22	2
e-t xi,j κ
j=1
e-t2 κ2 Pjd=1 xi2,j
e-t2κ2
where the last equality follows from our assumption that kxi k2 = 1. Next, we treat each ωr,i
hw0,r , xi i2 as a random variable. First, we compute the mean of ωr,i
E[ωr,i]
2
EW0 w02,r,dxi2,d
d1 =1
d
κ2 X xi2,d
d1 =1
2
κ
Then, we show that each ωr,i is sub-exponential with parameter (2κ2, 2κ2).
2	1	∞	2 ωr,i
et(ωr,i-κ )J = -^= J	et(ωr,i-κ )e-Wd√ω,i
∞
=—= I	e-(2K2-t)(√ωri) TH d√ω,i
κ 2π -∞
=;J
κ λ∕2Π V
e-tH2
=1 一 2tκ2
≤ e2tH4
π
・ e-tH2
(2κ2)-1 一 t
45
Under review as a conference paper at ICLR 2022
for t ≤ 2K2. Since each w°,r is independent, We have that each ω-i is independent for a fixed i.
Thus
E
Thus We have
m
etPrm=1(ωr,i-κ2)	= YE et(ωr,i-κ2)	≤ e2mtκ4
r=1
if 0 ≤ t ≤ 2mκ2
if t ≥ 2mκ2
We choose t = mκ2 一 mR2. Since R ≤ √, we have that m^ ≤ t ≤ mκ2. Thus
P (^X ω%i ≥ 2mκ2 - mR2) ≤ e-m8
Apply a union bound over all i ∈ [n] gives that with probability at least 1 - ne-瑞
i ∈ [n] that
it holds for all
m
ωr,i ≤ 2mκ2 - mR2
r=1
Sum over n gives that
nm
ΣΣ hw0,r, xii2 ≤ 2mnκ2 - mnR2
with probability at least 1 - ne-m.
□
Lemma 24. If for some R > 0 and all r ∈ [m] the initialization satisfies
m
hw0,r , xii2 ≤ 2mnκ2 - mnR2
r=1
and for all r ∈ [m], it holds that kwk,r - w0,r k2 ≤ R. Then we have
p
EMk η2,r X mk,r kuk - UIk k2 ≤ CI
l=1
and
一 P	"I ________
EMk ηk,r Emk,r kuk - U Ik k2 ≤ VZpCi
l=1
with
4θ2(1-ξ)nκ2
C1 =---------------
p
Proof. Using reciprocal moments, we have
θ2
EMk [ηk,r] = P (Nk,r = 1) EMk gk,r ∣ Njr = 1] = pξ
To start, we compute that for r 6= r0. Using the independence of mlk,r and mlk,r0, we have
p
EMk	ηk2,r	Xmlk,r(ξ - mlk,r0 )2
l=1
p
EMk ηk2,rmk,r(ξ - mk,r0)2]
l=1
p
EMk ηk2,rmlk,r] EMk (ξ-mlk,r0)2]
l=1
p
ξ(1 - ξ)EMk ηk2,r	mlk,r
ξ(1 - ξ)EMk [ηk,r]
46
Under review as a conference paper at ICLR 2022
For r = r0, we use the idempotent
EMk	p ηk,r	mk,r (ξ - mk,r )	= (1 - ξ)2EMk	p ηk2,r	mIk,
	I=1		I=1
		= (1 - ξ)2EMk [ηk,r]	
Therefore
p2
EMk	¾2,r X mk,r (Uf)- Uti)
l=1
≤ —EMk
p2
ηk,r X mk,r	ar (ξ - mk,r0 )σ(hwk,r0 , xi i)
≤ mEMk	η2,r X Mr X (ξ - mkk,rO)2σ6w5 , Xii)2
l=1 r0=1
1m	p
≤ m X EMk η2,r X mk,r (ξ- mk,rO)2 σ(hwk,r0, Xiii)
r0=1	l=1
ξ(i-ξ)
m
≤
m
EMk [ηk,r]	σ(hwk,rO, Xii)2+
rO=1
(1 - ,O - 2ξi EMk [ηk,r] σ(hWk,r, Xii)2
≤
≤
≤
θ2(1 - ξ) V^ /	∖2	(1 - ξ)2θ2 /	\2
mP- hw hWk,r0，Xii +-------mpξ— hWk,r, Xii
2θ2 (1 — ξ)κ2	2θ2(1 — ξ)2K2
p	mpξ
4θ2 (1 — ξ)κ2
P
where in the last inequality we use m ≥ ξ-1. Thus, we have
p
EMk	η2,r X mk,r Ilufc - uIk 112
l=1
np	2
X EMk	η2,r X mk,r (Uki)- Ukei))
i=1	l=1
≤ C1
≤ EMJ 卜 k,r X mk,r kuk - u Ik k2
Also, we have
L	CrI
- P
EMk ηk,r Emk,r Iluk - u Ik 112
l=1
1
p	1 2
≤ √pEMk η2,r X mk,r Iluk - uIk 112
l=1
Plugging in the previous bound gives the desired result.	□
Lemma 25. If for some R > 0 and all r ∈ [m] the initialization satisfies
m
hw0,r, Xii2 ≤ 2mnκ2 - mnR2
r=1
and for all r ∈ [m], it holds that Iwk,r - w0,r I2 ≤ R. Then we have
EMk [∣∣uk- ukk2] ≤ 4ξ(1 - ξ)nκ2
47
Under review as a conference paper at ICLR 2022
Proof. To start, we have
EmJ(Uki)- Uk(i))2
一EMk
mk
Xm2
ar(ξ - mlk,r)σ(hwk,r, xii)
≤
≤
≤
1m
—X EMk [(ξ - mlkr )2] σ(hwk,r, Xii)2
m
r=1
ξ(I - ξ) X /	∖2
m yk hwk,r，Xii
r=1
2ξ(I - ξ) v^^ /	∖2 _Loe门	e、P2
---------ThW0,r, Xii +2ξ(1 - ξ)R
r=1
≤ 4ξ(1 - ξ)κ2
Therefore,
n2
EMk [kuk-ulkk2] = XEMk (Uki)- U”)	≤ 4ξ(1-ξ)nκ2
i=1
□
Lemma 26. Assume that for all i ∈ [n], yi satisfies |yi| ≤ C - 1 for some C ≥ 1. Then, we have
EW0,a [ky - u0 k22] ≤ C2n
Proof. It is easy to see that
EW0,a hU(0i)i = 0
Note that
EW0,a
mEWo,a I (X arσ(hw0,r, Xi
2m
W X EWo [hw0,r, xii2]
r=1
ξ2 m
一 EEWo
m0
r=1
2 m d
w,r,d0 xi,d0
2
Therefore,
EW0,a
Thus,
=ξmΣ EEWo [w2,r,d0x2,d0
r=1 d0=1
2 m d
=m XX χ2,d0
r=1 d0=1
= ξ2
= yi2 - 2yiEW0,a hU(0i)i + EW0,a
n
EW0,a [ky - u0k22] = XEW0,a
i=1
n
X yi2 + ξ2n ≤ C2n
i=1
□
48
Under review as a conference paper at ICLR 2022
Table 1: Notations
SYMBOL DESCRIPTION
MATHEMATICAL DEFINITION
K	Number of global iterations	K ∈ N+		
k	Index of global iterations	k ∈ [K]		
T	Number of local iterations	τ ∈ N+		
t	Index of local iterations		t ∈ [T ]			
P	Number of subnetworks	P ∈ N+		
l	Index of subnetworks	l ∈ [p]		
ξ	Probability of selecting a neuron	ξ ∈ (0,1]		
ξ	Vector probability of selection a neuron by each worker	ξ ∈ (0,1]p		
η	Constant step size for local gradient update	η ∈ R		
Mk	Binary mask in iteration k	Mk ∈ {0,1}p×m		
mk,r	Binary mask for neuron r in iteration k	mk,r ∈ {0,1}p, the vector of rth column of Mk		
mk	Binary mask for subnetwork l in iteration k	mlk ∈ {0,1}m, the vector of lth row of Mk		
mk,r	Binary mask for neuron r in subnetwork l in iteration k	mlk,r ∈ {0,1} the (l,r)thentry of Mk		
Xk,r	Number of subnetworks selecting neuron r in iteration k	Xk,r = Pp=1 mk,r		
Nk,r	Aggregated gradient normalizer for neuron r in iteration k	Nk,r = max{Xk,r, 1}		
N,r	Indicator of gradient existing for neuron r in iteration k	N⊥r = min{Xk,r, 1}		
ηk,r	Global gradient aggregation step size for neuron r in iteration k	ηk,r = N⊥,r∕Nk,r		
(i) uk	Output of the whole network at global iteration k for sample i	Uki) = √ξm Pm=1 arσ(hwk,r , Xi)		
uk	Output of the whole network at global iteration k for all X	uk =	UkI),...,ukn)i	
^l(i) uk,t	Output of subnetwork l at iteration (k,t) for sample i	US = √m Pm=I ar mk,r σ({wk,t,r , XiE		
U k,t	Output of subnetwork l at iteration (k,t) for all X	Uk,t =	h^k(I),…,uk(n)i	
UkCi)	Output of subnetwork l at iteration (k, 0) for sample i	Gl ⑶=ι√(i) Uk = Uk,0		
U Ik	Output of subnetwork l at iteration (k, 0) for all X	U Ik = U k,0		
Lk	Global loss at iteration k	Lk = Ily — Uk ∣∣2		
Lmk(Wk,t)	Local loss for subnetwork l at iteration (k, t)	Lmk (Wk,t) = ky - fmk (Wk,t)k2		
49
Under review as a conference paper at ICLR 2022
I Generalization Error
In this section, we provide bound on the generalization error for the scenario in Theorem 3.
Theorem 5. Suppose the assumption of the dataset in Theorem 3 holds, and suppose p ≤ n. Fix
Somefailure probability δ, total number of global iterations K = Ω (log n), and Use the initializa-
tion scale K = O(是)and SteP size η = O ( —/ λo G =^= ). Ifthe number of hidden neurons
n	τ n3 max{n,K 2 }
satisfies
m
Ω (勺 max
δ
• poly (θ, ξ)
(9)
Then with probability at least 1 - δ, we have that for any 1-Lipschitze loss function ` such that
'(yi, yi) = 0, it holds that
LD(f (Wk, ∙)) := E(χ,y)~D ['(f (Wk, x), y)] ≤ O
y(H∞)-1y
Jbg 煮δ
n nδ

n
+
I.1 General Structure
For the simplicity of the proof, We treat ξ as constant and use O(∙) analysis. For a third order
tensor A ∈ Rn1×n2×n3, we denote its mode-i matricization as mati(A) ∈ Rnj1 nj2 ×ni with
i, j1, j2 ∈ {1, 2, 3} being different elements. For a matrix A0 ∈ Rn1 ×n2, We denote its vector-
ization as Vec(A) ∈ Rn1 ×n2. To start, we fix K = Ω (log 晟).In this way, based on our learning
rate, we have that∣∣UK - y∣∣2 = O ).Thus, for a 1-Lipschitz loss function '(•), we have
1n	1
LS(f (Wk, ∙)) = - ^X ('(uκ ,yJ - '(yi,y∕) ≤ √=kuK - yk2 = O
n i=1	n
We define the partial derivative tensor Z(k) and the masked partial derivative tensor mlk ◦ Z(k, t) ∈
Rm×d×n as
Z(k)r,:,i
(mk ◦ Z(k,t))r:i
1= arXiI {hwk,r, Xii ≥ 0}
1= ar m4,rxiI {<wk,t,r, X ≥ 0}
Therefore, we have that
wk,t+ι - wk,t = -η (mk ◦ (Z(k,t)) (Uk,t - y)
Let
ηk,1
ηk =	...	∈ Rm×m
50
Under review as a conference paper at ICLR 2022
then we have
τ —1 p
Wk+1 - Wk = XXηk (wk,t+1 - Wk,t)
t=0 l=1
τ — 1 p
=-η XX ηk (mko (Z(k, t))(U k,t- y)
T —1	p	T —1 p
=-ηX(Xηk (mkO(Z(W) I (uk -y) -ηXXηk (mkO(Z(W)(Uk,t -Uk)
t=0	l=1	t=0 l=1
-nθτZ(O)(Uk- y) - ηX (Xηk (mk O(Z(k,t)) - θZ(O)) (Uk- y)-
t=0 l=1
T — 1 P
η XX η (mko (Z(A t))(Ukt- Uk)
We denote
τ — 1 p
△gk,r = nk,r£ Emk,r
t=0 l = 1
∂Wr
θ∂L(Wk)
ξ	∂Wr
—
With this definition, the update of each weight vector can be written as
τ—1 P ∂L
Wk + 1,r	Wk,r = ηηk,rΣΣ-
∂ Wr
ηθτ ∂L(Wk)	八
~ΓFΓ -'"
—
Then we have
(i)	(i)	r(i) , r(i)
uk+1 - Uk = 11,k + 12,k
ξ	(i)
=⅛Σar (Wk+1,r - Wk,r, Xii 1{<Wk,r , Xii ≥ 0} + gk
mr r∈Si	'
ηξ L ∕θτ ∂L(Wk)	A	∖ πry	` ɪ
=----产 ɪ2 ar ∖ ~t-----+--------+ ^gk,r, Xi / I((Wk,r, Xii ≥ 0} 十
√m Mi	∖ ξ	HWr	/
—
nθξτ	Vn^
m^~ E E(Uk	-	yi)(Xi，Xji	I{<Wk,r , Xii	≥	0,	hWk,r, Xji	≥	0}
m r∈Si j=1
-----√= X ar h^gk,r, Xii I((Wk,r, Xii ≥ 0} + Iik
mm r∈Si	'
n
-丽 X(H(k)ij- H(k)⊥) (Uki)-yi)
j=1
--√= ^X ar θgk,r , Xii I{hwk,r, Xii ≥。} + IFk
mm r∈Si	'
Letting Wk ∈ Rn be defined as
ηξ	y~∖	.	∖	ττ r /	、
Wk,i = - -η=	E	ar (∆gk,r,	Xii	I{(Wk,r , Xii	≥ 0}
ʌ/m —*
V	r∈Si
Then we have
Uk+1 - Uk = -ηθτH(k)(Uk - y) + ηθτH(k)⊥(Uk - y) -Wk + 12,k
=-ηθτH∞(Uk - y) - ηθτ(H(k) - H∞)(Uk - y) + ηθτH(k)⊥(Uk - y) - Wk + I2,k
51
Under review as a conference paper at ICLR 2022
and thus
Uk+1 - y = (I - ηθτH∞)(uk - y) - ηθτ(H(k) - H∞)(ufc - y) + ηθτH(k)⊥(uk - y)-
Wk + 12,k
=(I - ηθτ H(O))(Uk- y) + Wk
by denoting
Wk = -ηθτ(H(k) - H(0))(uk - y) + ηθτH(k)⊥(uk - y) - Wk + I2,k
Then
k-1
Uk - y = (I - ηθτH∞)k(u0 - y) + X (I - ηθτH∞)k-k0+1wk，
k0=0
Thus we have
Wk+1 - Wk = ηθτZ(0)(I - ηθτH∞)ky - ηθτZ(0)(I - ηθτH∞)ku0-
k-1
ηθτZ(0) X (I - ηθτH∞)k-k'+1wk-
k0=0
ηX (Xηk (mkO(Z(k,t)) -θz(0)) (Uk- y)-
t=0 l=1
τ-1 p
η XX 以(mko (Z(k,t)) (Ukt- uk)
Therefore, the weight matrix difference can be bounded as
IIWk - W0∣∣F ≤ Q1 + Q2 + Q3 + Q4 + Q5
with
Qi
Q2
k-1
ηθτ X Z(0)(I - ηθτH∞)ky
k0=0
k-1
ηθτX Z(0)(I - ηθτH∞)ku0
k0=0
k-1 k1-1
F
F
Q3 = ηθτE £Z(0)(I- ηθτH∞)k1-k2+W2
kɪ =0 k2=0
F
Q4
Q5
k-1 τ-1 / p	、
ηXX X
ηk' (mb Ο (Z(k0,t)) - θZ(0)卜u” - y)
k0 = 0 t=0 ∖l=1	)
F
k-1 T — 1 P
η XXXnko (mb。(Z(k',t))(u" - UkO)
k0 = 0 t=0 l=1
F
Based on next section, we have
E[Mk],a [Q1] = O
卜 y>(H∞)-1y +
n nκλ0 ∖
1
n (log(n∕δ)) 4
E[Mk],a [Q2] = O
E[Mk],a [Q3] = O
+ 竺2K + K
p
m 4 λ0
E[Mk],a [Q4]= θ(篝
E[Mk],a [Q5] = θ(ητ√nκ)
52
Under review as a conference paper at ICLR 2022
Then, as long as κ = O
and η = O
λo√
TyJn max{n,K2}
we have that
EMk,a[kWk-W0kF]=O	y>(H∞)-1y+1
Thus, with probability at least 1 - δ we have
kWk - W0kF
O δ-
y>(H∞)-1y+1
Moreover, the row-wise weight perturbation is bounded as
k-1
kwk,r -w0,rk2 ≤ kwk,τ -wk,0k2
k0=0
O(ητ√mK)X0(1-8 ηθτλ0)k0+O(ητκn
≤O
n√K
λo δ√m
Consider the functional class
F = {f (W, ∙) ∣∣∣Wk,r - W0,rk2 ≤ R; IlWk - W°∣∣F ≤ R}
with
Then the Rademacher complexity of this functional class is
Thus, with probability at least 1 - δ we have that
1
sup (LD(f) - LS(f)) ≤ 2Rad(F) +
f∈F
log λnδ
δ-1
y(H∞)-1y +
n
O
n
1
log λnδ
n
which implies that
Ld (f)≤ LS (f)+ O fδ-1ry≡ +C = O (δ-1ry≡ + Jii
I.2 BOUNDOFQ1,Q2,Q3,Q4ANDQ5
We bound each of the terms separately. We first note that
1	a1x1I{hw0,1 ,x1 i}	. . .	a1xnI{hw0,1 ,xni}
mat3(Z(0)) = —=	...	...	∈ Rmd×n
m amx1 I{hw0,m, x1 i} . . . amxnI{hw0,m,xni}
and therefore mat3(Z(0))>mat3(Z(0)) = ɪH(0). Let T = ηθτ Pk二(I 一 ηθτH∞)k0.
53
Under review as a conference paper at ICLR 2022
I.2.1 BOUND OF Q1
For Q1, we have
Q21
k-1
ηθτXZ(0)(I-ηθτH∞)k0y
k0=0
2
F
ηθτ vec
2
k-1
ηθτ X mat3(Z(0))(I - ηθτH∞)k0y
k0=0
2
y>T>mat3(Z(0))>mat(Z(0))Ty
=1 y>T>H(0)Ty
ξ
≤ 1 y>τ>H∞Ty + 1 kH(0) - H∞k2 IHl2kyk2
ξξ
≤ 1 y>T>H∞Ty + O (n2p≡p)
ξ	∖	√mλ0	J)
where the last inequality follows from the fact that
k-1
kTk2 ≤ ηθτ X(1-ηθτλ0)k=λ0;
k0=0
kH(0)- H∞k2 ≤ ξn'"δ);	kyk2= n
m
Also, consider the eigen-decomposition H∞ = Pin=1 λivivi> . Note that T and H∞ has the same
eigenvectors. Therefore,
n k-1	n
T=ηθτXX(1-ηθτλi)k0vv>	Xλi-1vivi> = (H∞)-1
i=1 k0=0	i=1
Thus
Q2 ≤ ξ-1y>(H∞)y+OnW
which implies that
Qi ≤ ξ-1 Jy>(H∞)-1y + O
1
n (log(n∕δ))4
m 4 λo
I.2.2	BOUND OF Q2
For Q2, we have
Q2 = kZ(0)TuokF ≤kZ(0)kF kTk2ku0 k2
Note that with probability at least 1 一 δ We have that ku0k2 ≤ nκ.κ. Also, We can bound ∣∣Z(0)kF
as
nm
kZ(0)k2F = XX kZ(0)r,:,ik22
i=1 r=1
nm
=^m∑∑karxiI{hw0,r,xii ≥ 0}
≤n
Thus we have
Q2 ≤√nλ0 ∙ ψδκ = n√δ0
54
Under review as a conference paper at ICLR 2022
I.2.3	Bound of Q3
We start with giving a bound on ∣∣6k ∣∣2. We have that
IIekk2 ≤ ηθτ (∣∣H(k) - H(0)∣2∣Uk - y∣2 + ∣∣H(k)⊥∣∣2kuk - y∣∣2) + ∣∣I2,k∣∣2 + |屈∣2
Among all the terms, ∣∣I2,k ∣∣2 can be bounded using the bound on EMk
EMk [∣l12,k∣∣2] ≤ √nmaxEMknI2：k ∣i
n阂]
≤ O (ητnκ-1R) IlUk - y∣∣2 + O
ηn 2 R
√P
ητκ-1
Moreover, since we have that
∣H(k)⊥∣2 = O (nκ-1R) ;	∣H(k) - H(0)g = O (nκ-1R)
Therefore
EMk [∣lek l∣2] ≤ o (ητnκ 1 R) IlUk- y∣∣2 + EMk [∣lek ∣∣2] +
(ηn 2 R _1
----------------------------------+ ητκ
Vp
What remains is to bound EMk [∣ek ∣∣2]. To do this, we first bound the norm of ∆gk,r. We write
T —1 p
△gk,r = ηk,rf EmIk
t=0l=1
τ — 1
,r
ηk,r∑∑
aLmk (wk,t)	∂Lmk (Wk)
∂Wr
∂ Wr
∂Lmk (Wk)
TdL(Wk)
ξ	∂Wr
dLmk
∂Lmk (Wk)
τ —1 p
η^ XX
—
+
P
—
—
∂Wr
∂ Wr
+
T gk,r
1 ∂L(Wk)
ξ	∂ Wr
—
The second term has norm
EMk
1 ∂L(Wk)
gk，r - ξ	∂Wr
2」
θEMk
n
：r - Uki)) arXiI{(Wk,r, Xii ≥ 0} I N⊥r = 1 +
P (N ⊥ =0) 1 ∂LWk)
PlNkT =0) ξ 幅W
r
2
≤
k [kfk,r - Uk ∣∣2 I N⊥r = 1] +
(1 - θ) √n
√m
lluk - y∣∣2
=o (ʌ/m) Huk- y∣∣2+ o
For the first term, in previous proof, we have
EMk
P
ηk,rf
l=1
HLmk (Wk")	»% (Wk)
∂Wr
∂ Wr
≤ EMk
p	n
√r X mk,rX 阳)-UIkj) I
V l=1 j=1
—
2
with
M(I) - USi)I ≤ ηt√n (l∣y - ukl∣2+ IIUk - Uk∣∣2)
55
Under review as a conference paper at ICLR 2022
Combining the result above, we have
EMk k∆gk,rk2 ≤ η
kuk - yk2 + EMk	ηk,r X mk,r kuk - u Ik k2 j +
kuk - yk2 + O
m (ητ + I)Jkuk - yk2 + O
ητ 2 y∕pCγn
+
√m
Now we bound EMk [kkk2]
EMk [kek k2] ≤√⅛ XEMk [5，「叩
m r∈Si
≤ O (ητ√n (ητ + 1)) kuk - y∣∣2 + O (ητκ,n + η2τ2n
Combining the bound above, with the assumption that R ≤ n and η ≤ Tn ,we have
EMk [kekl∣2] = O(ητVn)kuk - yk2 + O
Using the convergence rate of E[Mk-1] [kuk - yk2], since B1= O(1) we have that
E[Mk] [kekk2] ≤ O(ητ√n)(1 - 1 ηθτλo)kkuo - yk2 + O(ητ√n)(1 - 1 ηθτλo)k+
88
≤ (1 - 1 ηθτλo)kO
8
As in the bound of Q2, we have
k-1k1-1
E[Mk] [Q3] =E[Mk]	ηθτXX Z(0)(I - ηθτH∞)k1-k2+10k2
k1=0k2=0
F
k-1 k1-1
≤ ηθτ ∙√n XX (1 - ηθτλ0)k1-k2+1E[Mk2][同2 k2]
k1=0 k2=0
≤ O 卜 τ2 mρ) θ X(I-8 ηθτλo )k1 +
/	2	2	_____∖ k k-1 kι-1
O ( η2τ2 (	+ PnPCI	θXX(1-ηθτ λ0)k1-k2+1
p	k1=0 k2=0
56
Under review as a conference paper at ICLR 2022
I.2.4	BOUND ON Q4
First, we bound Plp=1 ηk mlk ◦ Z(k, t) - mlk ◦ Z(0, 0) F . In particular, we have
2
p
Xnk (mk ◦ Z(k,t) - mk ◦ Z(0,0))
l=1
1m n p
m XXX l∣ηk,r ar ximk,r sr,i∣∣2
F	r=1 i=1 l=1
mn	p
≤ m XX ηk,r X mk,r ∣Sr,il
mn
Y XX N⊥rMil
r=1 i=1
with
sr,i = I {<wk,t,r, X"} - I{hw0,r , Xii }
Then sr,i = 0 if Air, and |sr,i| ≤ 1 otherwise. Thus
2θR
EWo,Mk [Nk,/",/] = θP(Air) ≤ ^√∏ ≤ θκ R
Also, its variance follows
V arW0,Mk (sr,i) ≤ EW0,Mk sr2,i = θκ-l R
Thus, applying Berstein inequality gives
P(XX N⊥,r lsr,il ≥ 2mnθκ-1R∖ ≤ exP (-mnθR
r=l i=l	10κ
Thus, with probability at least 1 - exp (- ml70KR) We have that
p
Enk (mk ◦ Z(At)- mk ◦ Z(0,0))
l=1
2
≤ nθκ-1R
F
Take a union bound over all k, t, and apply overparameterization gives that with probability at least
1 - δ, it holds that
p
E nk (mk ◦ Z(k, t) - mk ◦ z(0, 0))
l=1
≤ √nθκ-1R
F
Then we bound IlPp=I nk (m^ ◦ Z(0,0)) - ΘZ(0)∣∣f . We have
2
p
Xηk (mk ◦ Z(0,0)) -θZ(0)
l=1
F
mn	p
m χχ∣∣(ηk,r X mlk,r - θ	arXiI{hw0,r, Xii ≥ 0}
mn
m xx3⊥r- θ)2
r=1 i=1
2
2
Since we have EMk Nk⊥,r = θ, and V arMk (Nk⊥,r) = θ-θ2 we can again apply Berstein inequality
to get that
P Xm Xn (Nk⊥,r 一θ)2 ≥ 2mnθ(1 一 θ)) ≤ exp (- m"：；———
r=l i=l
Thus, with probability at least 1 - exp
—
mnθ11-S)) we have that
p
Enk (mk ◦ Z(0,0)) -θZ(0)
l=1
2
≤ 2nθ(1 - θ)
F
57
Under review as a conference paper at ICLR 2022
Take a union bound over all k and apply overparameterization gives that with probability at least
1 - δ, it holds that
p
Enk (mk ◦ Z(0,0)) -θZ(0)
l=1
Thus, we have that
HH p
Enk (mk ◦ Z(k,t)) -θZ(0)
H l=1
with R ≤ K. Therefore,
k-1 τ-1
≤ √nθ(1 - θ)
F
p
Enk (mk ◦ Z(0,0)) -θZ(0)
l=1
p
Enk (mk ◦ Z(k,t) - mk ◦ Z(0, 0))
l=1
≤ O (√n)
Q4 ≤η∑SE Enk (mk◦ Z(k,t)) -θZ(O)	kuk0-yk2
k0=0 t=0
l=1
k-1
≤ O(ητ√n) E
k0=0
=O (ητnK)
Uo G)
F
1 - 1ηθτλo
O(1)
I.2.5 BOUND ON Q5
To bound Q5, we first note that
p
Enk (mk ◦ Z(k,t)) (Uk,t - Uk)
l=1
And, we have
mn p
≤ m XXX ηk,rmk,r (Uk'it) - Uki))2
r=1 i=1 l=1
1m
m∑ (mk,rσ (<wk,t,r,
r=1
m
≤ m X(UWk,t,r
r=1
- wk,r2 + hwk,r, xii2
≤
p
F
m
2
F
+
F
F
since
(σ (<wk,t,r , Xi〉)- σ(hwk,r, Xii))2 +
(mlk,r - ξ)2σ(hWk,r, xii)2
UUWkl,t,r - Wk,rUU22 + hWk,r, Xii2
With
wkl ,t,r
H	ητ ∖∕2nK
-wk,r H2 ≤	√^ E[Mk-ι],Wo,a [旧-Uk ∣∣2]+2ητκny -
2ξ(1 - ξ)pK
mδ
≤ (1- 1 ηθτλ0) O (ητn√K)+o 卜τκn∏κ) := B3
58
Under review as a conference paper at ICLR 2022
we have
Enk (mk ◦ Z(k,t)) (Uk,t - Uk)
l=1	F
m np
≤ m2 X XX ηk,r mk,r (B3 + kwk,r k2)
r,r0=1 i=1 l=1
1nn
≤ nB32 + m xxhwk,，，χii2
r=1 i=1
≤ nB32 + 2nκ2
Therefore
k-1 τ-1
Ea [Q5] ≤ η XX Ea
k0=0 t=0
p
Enk (mk ◦ Z(k,t)) (Uk,t - Uk)
l=1	F
≤ ητ√n
B3 + κ
≤ O(ητ∖∕nκ)
59