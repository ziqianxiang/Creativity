Under review as a conference paper at ICLR 2022
Green CWS: Extreme Distillation and Effi-
cient Decode Method Towards Industrial Ap-
PLICATION
Anonymous authors
Paper under double-blind review
Ab stract
Benefiting from the strong ability of the pre-trained model, the research on Chi-
nese Word Segmentation (CWS) has made great progress in recent years. How-
ever, due to massive computation, large and complex models are incapable of
empowering their ability for industrial use. On the other hand, for low-resource
scenarios, the prevalent decode method, such as Conditional Random Field (CRF),
fails to exploit the full information of the training data. This work proposes a fast
and accurate CWS framework that incorporates a light-weighted model and an
upgraded decode method (PCRF) towards industrially low-resource CWS scenar-
ios. First, we distill a Transformer-based student model as an encoder, which
not only accelerates the inference speed but also combines open knowledge and
domain-specific knowledge. Second, the perplexity score to evaluate the language
model is fused into the CRF module to better identify the word boundaries. Ex-
periments show that our work obtains relatively high performance on multiple
datasets with as low as 14% of time consumption compared with the original
BERT-based model. Moreover, under the low-resource setting, we get superior
results in comparison with the traditional decoding methods.
1 Introduction
Chinese Word Segmentation (CWS) is typically regarded as a fundamental and preliminary step for
NLP tasks, and it has attracted heavy studies these years. Normally CWS is considered as a sequence
labeling problem. That is, each character in a sentence was labeled by symbols among ’B, M, E, S,’
which indicates the beginning, middle, and end of a word, or a word with a single character.
In the past few years, methods have been proposed to improve the performance of CWS. Huang
et al. (2015) use bidirectional LSTM to capture the long dependency features of sequence, and the
proposed BiLSTM-CRF became a strong baseline for the sequence tagging task. Yang et al. (2018)
apply lattice LSTM network to integrate character information with subword information, which
significantly enhances the performance. Tian et al. (2020) propose a neural framework, WMSEG,
which uses a memory network to incorporate wordhood information as strong contextual features
for the encoder.
In addition to designing the ancillary modules for the CWS task, following the development of
pre-trained models (Han et al., 2021), CWS achieves new progress. Huang et al. (2019) fine-tune
BERT for multi-criteria CWS, where a private projection layer is adopted to extract criteria-specific
knowledge. Ke et al. (2020) present a CWS-specific pre-trained model, which employs a unified
architecture to make use of segmentation knowledge of different criteria. Liu et al. (2021) raise a
lexicon enhanced BERT, which combines the character and lexicon features as the input. Besides, it
attaches a lexicon adapter between the Transformer layers to integrate lexicon knowledge into BERT.
Though pre-trained models significantly improve the overall performance of CWS, the practicality
is not that satisfactory. Under industrial scenarios that require a quick response, the whole system
can only afford a few milliseconds for CWS, hence the model structure is dedicate to be simple
and concise to save the inference time. Duan & Zhao (2019) come up with a Transformer variant,
Gaussian-masked Directional (GD) Transformer, to take the unigram features for scoring and using
a bi-affine attention scorer to directly predict the word boundaries. The proposed method gains high
1
Under review as a conference paper at ICLR 2022
speed with comparable performance. Huang et al. (2019) utilize knowledge distillation to get a
shallow model. Meanwhile, the quantization methods are investigated for network acceleration.
Concerning CWS towards low-resource scenarios in industrial, for example, in Table 1, the origi-
nal query comes from a famous Chinese video-sharing social network. In the absence of external
knowledge or strategy, it is challenging to segment the query correctly to recall what we want. The
BiLSTM-based model segments the sentence correctly while fails to satisfy the industrial speed
requirement. We extract test sentences from different corpora, assuming that the time left for the
segmentation for a sentence is 1ms, on account of the average length is 35, then the minimum speed
is around 68KB/s. Obviously, the CNN-based and BiLSTM-based models both fail to meet the
threshold.
	Tools		Segmentation Results	Speed
Baseline	新来的吃鸡主播 The newcome Chiji host	68KB/s
Jieba	新来的吃鸡主播	673KB∕s
Thulac	新来的吃鸡主播	103KB∕s
PKUSeg	新来的吃鸡主播	123KB∕s
CNN-based model	新来的吃鸡主播	36KB∕s
BiLSTM-based model	新来的吃鸡主播	12KB∕s
Table 1: Illustration of different segmentation results, chiji is the alias of a famous mobile game, the
Baseline row gives the golden segmentation result, the minimum speed need is 68KB/s.
So the problem arises, given a light-weighted CWS structure, how to improve the overall perfor-
mance as much as possible? There are two major methods corresponding to different phases to al-
leviate the problem. During the encoding phase, we can make full use of dictionary knowledge. Liu
et al. (2019) adopt two methods, named Pseudo Labeled Data Generation and Multi-task Learning
to make full use of dictionary knowledge to promote the performance of CWS. Also, it is classical
to utilize transfer learning technology, Xu et al. (2017) initialize a student model with the learned
knowledge from the teacher model, and a weighted data similarity method is proposed to train the
student model on low-resource data. Besides, we can ease the problem during the decoding process
by modifying the idiomatic decode paradigm, however, to our best knowledge, there is not much
research on this part.
We optimize our work from two aspects to build a fast and accurate CWS segmenter towards the
industrial context. First, we utilize the knowledge distillation to accelerate the model, and shallow
Transformer-based student models which is distilled as the encoder. Second, we believe that the
sequence-level semantic information is helpful during the decoding process. We propose a novel
CRF, named PCRF, which merges the sequence-level perplexity score into the original CRF, en-
hances the CWS performance in the low-resource scene. Lastly, the segmentation results are post-
processed based on a coarse-grained dictionary extracted from the training corpus.
Overall, the main contributions of this work can be summarized as follows:
•	Model accelerating technique is realized to build a fast segmenter for industrial application.
The light-weighted structure not only achieve the speed request towards real-time scenarios
but also combines open knowledge from the teacher and in-domain knowledge from the
training corpus.
•	An upgraded CRF named PCRF is proposed. Apart from the character-level knowledge
learned by the encoder, the sequence-level knowledge is acquired through a simple n-gram
model. The two kinds of knowledge are jointly sent to the PCRF for decoding.
•	We create two CWS datasets. The original corpus is extracted from e-commerce and video
sharing social network. The two CWS datasets are cross annotated carefully by three ex-
perts in Chinese linguistics. We plan to release these two datasets gradually for further
academic research.
•	Experiments on more than ten corpora, include eight benchmark datasts, two publicly avail-
able datasets, and two hand-crafted datasets, have demonstrated the effectiveness of our
2
Under review as a conference paper at ICLR 2022
model. We believe that the proposed CWS framework is valuable for the industrial sce-
nario, especially in the areas that lack labeling data and demand high inference speed.
2 Background
In this section, we will first describe the problems of the CWS model in industrial and then introduce
the main concepts related to the proposed work.
2.1	Task Difficulty
Towards CWS in the actual scenario, there are three problems to deal with. Firstly, as new media
burst endlessly, sending annotators to tag the massive unlabeled corpus is labor-wasting. Mean-
while, the distribution of the publicly available datasets is very different from the industrial data,
though there may be common knowledge that we can make use of. Secondly, it is easy to train a
high-performance BERT-based CWS model and deploy it in the system. However, given enough
acceleration techniques, the large model still fails to meet the milliseconds or even tenths of a mil-
lisecond speed needs in the rigorous environment. Thirdly, the publicly available CWS tools are
dissatisfied, especially in new areas, which brings much extra work like dictionary mining, semantic
disambiguation, etc.
2.2	Feature Extraction
Feature extraction is the necessary step for CWS task, through feature extraction, the contextual
knowledge can be refined to ensure the label of each character. By utilizing the fine-tune scheme
of the pre-trained models, the characters are first mapped into embedding vectors and then fed into
several encoder blocks, given a sentence X = {x1, x2, . . . , xn}, the feature can be represented as:
hi = BERT (e1,e2, . . . ,en; θ) ,
where θ denotes the parameters learned during the fine-tuning phase.
2.3	Inference Layer
After feature extraction, we will get the scores of each character corresponding to the tag set, specif-
ically the transition score and emission score. The scores will be sent to CRF or MLP decoder to get
the best tagging result. Formally, the probability of a label sequence is formalized as:
p(Y | X)
Ψ(Y | X)
Pγ0∈Ln ψ (Y0 । X)，
where Ψ(Y | X) is the potential function:
n
Ψ(Y | X) = Yψ (X, i, yi-1, yi) ,
i=2
ψ (X, i, yi-1, yi) = exp s(X, i)yi +byi-1yi ,
where y denotes the tag label, b ∈ RlLl×lLl is a trainable parameter denotes the transition score
between the tag set, which can be randomly initialized. byi-1yi means the score from yi-1 to yi,
s(X, i)yi denotes the emission score respective to each tag label for ith character:
s(X, i) =WsThi+bs,
WT ∈ RlLl×lLlandb ∈ R1L1are trainable parameters. If using MLP decoder, the output label can
be easily acquired through the Softmax function.
2.4	Perplexity Score
The perplexity score (PPL), usually used as the evaluation metric for language model, can depict
how reasonable a sentence is. Given a segmented sentence S, the PPL of S can be calculated as:
PPL(S)= P (Wι,w2,wi,...,Wm)T/m	= qQm=i P(WiK1,W12,…,Wi-ι),
3
Under review as a conference paper at ICLR 2022
where wi denotes the word in the sentence, take bi-gram language model as an instance, under
the Markov hypothesis, the current word merely relies on the former word in the sentence, so the
calculation can be reduced to:
PPL(S) ≈ p(w1)p(w2 | w1)p(w3 | w2)..p(wn | wn-1).
3 Model Description
In this work, we propose a hierarchical CWS model towards real industrial scenarios usage. The
model contains two essential parts, a distilled Transformer-based encoder and a concisely upgraded
CRF decoding layer (PCRF). We will describe the two parts respectively.
3.1	Distilled Encoder
To balance the accuracy and computational cost, firstly, we fine-tune the pre-trained model on the
CWS training data to get a relatively high-performance teacher model. Then, we distill knowledge
from the teacher model into a smaller Transformer-based student network. Transformer is recog-
nized to have an advantage over RNN and CNN in feature extraction. During the distillation process,
a cross-entropy (CE) function is adopted to calculate the divergence between the output logits from
the teacher model and student model. The overall loss function, incorporating both distillation and
student losses, is calculated as:
L(x; θs) = α * CE (y,p) + β *CE (σ (& T) σ&,T)),
where x is the input data, p is the hard label predicted by the student model, θs denotes the student
model parameters, ls and lt are the logits of the student and teacher, respectively, σ is the softmax
function parameterized by the temperature T, α and β are coefficients to trade off the two losses.
3.2	PCRF Inference Layer
The conventional CRF is usually served as the decoding component after the neural encoder. By
aggregating the emission score and the transition score, the CRF decode module assigns a score for
each possible sequence of tags through the Viterbi algorithm.
However, it can be troublesome if lacks of sufficient training corpus. In industry, there is often a
need for CWS in new scenarios. Consider that the emission score actually reflects the capability
of the prepositive encoder, which is, the encoder can not perfectly generalize its ability to unseen
words (OOV); thus, the emission score can be biased. In other words, as clearly analyzed in Wei
et al. (2021), the author states that without a hard mechanism to enforce the transition rule, the
conventional CRF can result in the occasional occurrence of illegal predictions, which indicates that
it is possible to lead to wrong tag paths under the current decode framework.
To this end, we believe that it relies solely on the emission score and the transition score are insuffi-
cient, especially in low-resource circumstances. Before that, it was common to segment text based
on a language model, while to our best knowledge, we did not find works to incorporate the lan-
guage model with the decoding process. Our solution is intuitive. Apart from the already acquired
two scores for each character, we will calculate the PPL of the already decoded sequence. For ex-
ample, as illustrates in Figure 1, the sentence 请播放一首将军令 means ”Please play General's
order”. At the index of character 军,one possible deCodedPathiS (请 播放 一首 将 军)，another
is (请 播放 一首 将军),the corresponding decoding score at the character 军 can be formulated
as:
Score1 = EI + TBI + λ * PPL(Path1),
Score2 = EB + TBB + λ * PPL(Path2),
where E is emission score, T is the transition score, λ is the coefficient to measure the importance
of PPL, which ranges between zero to one.
Note that there is a key point, the n-gram language model is derived from a large labeled training
corpus that is not realistic in industrial. There are solutions to deal with such a chicken-and-egg
problem. On one hand, it is acceptable that we can merely depend on the training corpus as long as
the training data is abundant. On the other hand, the main difference between various CWS corpora
4
Under review as a conference paper at ICLR 2022
I
B
CRF Prediction
PCRF Prediction
Input
Figure 1: An decode example of original CRF VS PCRF, at the index of character 军，the emission
score for the label ”I” is smaller than ”B”, unfortunately, the CRF module fail to rectify the illegal
path after the dynamic decoding process ended. In contrast, the perplexity score calculated by the
language model, together with the aforementioned two scores, managed to justify that the character
军 should be the middle of a word rather than the start of a word.
lies in the tagging criteria. Through the investigation in (Fu et al., 2020), the distance between
different datasets can be quantitatively characterized, we adopt the label consistency of words that
appear in the training set as the distance measure metric, defined as:
0	witrain = 0
ψ (wi,Dtrain) =	∣wtrain,lι	OtherWiSe
∣W⅛rain∣	OtherWiSe ,
where witrain,l represents the occurrence of word wi with label l in the training set, Dtrain is the
training set. Naturally, we can replenish the current dataset with available datasets that are adjacent
to it. Unlike the conversational way, we use the adscititious datasets to train the n-gram language
model to fuse the knowledge into the decoding process.
Based on the above discussion, we propose the PCRF decode framework, formally described in
Algorithm 1.
Algorithm 1 PCRF
Require: tag set L, Emission score E = eit, Transition score T = tj, PPL coefficient λ.
Ensure: Empty array Score, Empty array P ath, length of the sentence LS.
1:	while LS is not met do
2:	Add Eit andTij
3:	for t ∈ L do
4:	Get C orresponding PPLt
5:	Scoreit — Eij + Tij +λ * PPLi
6:	end for
7:	U pdate S core and P ath
8:	end while
4 Experiments
Datasets We collected eight standard benchmark CWS datasets, include MSR, AS, PKU and
CityU from SIGHAN2005, UDC from CoNLL 2017 Shared Task (Zeman et al., 2018), SXU from
SIGHAN2008, CTB from (Xue et al., 2005) and CNC corpus. Also, two publicly available datasets
are adopted, include WTB from (Wang et al., 2014) and ZX from (Zhang et al., 2014), the WTB is
collected from Sina Weibo, a popular social media site in China. The ZX dataset is derived from
Zhuxian, a fairy tale novel with a literature genre similar to martial arts novels. Besides, we de-
velop two self-built datasets, the ECOMM, and CAPTION. The original corpus of ECOMM comes
5
Under review as a conference paper at ICLR 2022
Datasets	Words		Phrases		ASL	
	Train	Test	Train	Test	Train	Test
MSR	2223k	252k	50k	8k	46.59	46.58
AS	5461k	123k	70k	4k	11.81	11.14
PKU	1121k	104k	21k	3k	95.92	88.87
CityU	1400k	40k	30k	2k	45.31	45.38
CTB	730k	53k	17k	2k	45.71	43.93
UDC	111k	12k	3k	0.6k	39.22	38.42
CNC	6569k	726k	52k	14k	43.28	43.06
SXU	540k	113k	12k	3k	49.85	50.43
WTB	16k	0.2k	0.6k	0.1k	28.71	32.97
ZX	88k	33k	0.3k	0.7k	39.62	34.39
ECOMM	29k	3k	2k	0.3k	34.26	34.96
CAPTION	215k	24k	20k	3k	9.69	9.82
Table 2: Details of the twelve datasets. Three aspects of information are exhibited. Words represent
the number of tokens that appear in the dataset. Phrases represent the number of tokens in the
dataset whose length is greater than two. ASL is the abbreviation of ”average sentence length,”
which demonstrates the average length of sentences in a dataset.
from the caption of commodities on the e-commerce website, which depicts the features of the com-
modities, and we collect CAPTION dataset from a video sharing social website, mainly the search
queries of the short videos. Three linguistics experts are invited to annotate the two datasets. When
disagreement with the annotation results occurs, they will discuss and align with each other so that
will be no further conflict. The two datasets will be released gradually in the future. The WTB,
ZX, ECOMM, CAPTION datasets are denoted as low-resource datasets in this work. Details of the
datasets after preprocessing are shown in Table 2.
Hyper-parameters We strictly follow the steps of knowledge distillation. Specifically, we adopt
pre-trained BERT (Devlin et al., 2018) as the teacher model, three Transformer-based student models
with layers 1, 3, and 6 are distilled. We use Adam with the learning rate of 5e-5, the batch size is set
to 8, the max length is set to 512, the temperature for distillation is set to 8. We select the last saved
checkpoint (the 20th epoch) as the student model. And the F1 score is used to evaluate our model.
Language Model We adopt KenLM Heafield (2011), a fast and simple language model toolkit to
calculate the PPL score. KenLM provides an intuitive API to train n-gram language models as well
as inferencing.
Preprocessing Among all datasets, extra space between tokens and invalid characters is removed.
We transform AS and CityU from the traditional form into a simplified form. Besides, all tokens are
converted into half-width.
4.1	Overall Results
In this section, We first give the main results on the four low-resource datasets. Then we crop the
benchmark training set to a different scale while keeping the test set still to simulate the low-resource
scenarios. For simplicity, we distill a single-layer Transformer as the base encoder.
Low-resource datasets The pre-trained language models have shown us its magic power. By fine-
tuning the model, it’s not challenging to get a satisfying performance on CWS. We do not intend
to prove the superiority of the pre-trained models in the CWS task. From the perspective of effec-
tiveness and practicality, we choose two kinds of comparison models. The first is the CWS research
works towards low-resource or high-speed needs scenarios, and the second is the widely used neural
network-based CWS tools. The experimental results are shown in Table 3.
Specifically, the truncated (1, 3, or 6 layers) BERT learned from the teacher are used as the encoder,
“KD_1" stands for knowledge distillation with single Transformer layer in Table 3. We use Soft-
max, CRF, and the proposed decode method (PCRF) to implement the decoding process. Overall,
eliminating the contrast models, our model achieves nine best performances among three group ex-
periments (twelve in total), sufficient to proof the effectiveness of our model especially when the
6
Under review as a conference paper at ICLR 2022
Models	ZX	WTB	ECOMM	CAPTION
Jieba	79.45	81.23	86.70	74.96
PKUSeg	87.45	87.24	83.51	71.40
THULac	83.28	82.33	69.35	75.59
Huang et al. (2019)	97.0	93.1	-	-
Huang et al. (2020)	96.77	-	-	-
KD」+SOftmax	92.57	82.46	88.08	75.93
KD_1+CRF	92.57	82.55	88.12	75.95
OUrS(KD_1+PCRF)	94.00	85.65	89.92	81.60
KD.3+Softmax	96.07	90.17	94.61	82.10
KD_3+CRF	96.07	90.17	94.66	82.08
OUrS(KD_3+PCRF)	96.12	90.45	94.16	83.00
KD_6+Softmax	96.74	91.94	95.17	83.17
KD.6+CRF	96.73	91.94	95.22	83.20
OUrS(KD_6+PCRF)	96.71	92.38	94.65	83.31
OUrS(Teacher_12)一	96.69	92.43	95.77	83.79
Table 3: Results on four low-resource datasets.
model is shallow. Under the single-layer setting, our model outperforms the CRF-based model by
1.43%, 3.10%, 1.80%, and 5.65%, respectively, on four datasets. As the model gets deeper, the
results are still convictive. Besides, we noticed that in the ECOMM dataset, the results are not as
persuasive as the rest datasets. We believe the reason could be the distribution of the ECOMM
dataset is far different from the rest datasets. The corpus is more like a vocabulary stack rather than
fluent sentences, so the PCRF modules conversely hurt performance when the model gets deeper.
Low-resource settings The analysis of the former part proves the usefulness of the proposed model.
Nonetheless, given sufficient training data and deeper layers, the results are closer than the results in
low-resource and shallow scenarios. The average divergence between the KD_1+CRF and our model
(KD_1+PCRF) of four in-domain datasets is 11.98%, while 0.96% for the six layers condition, which
means that the proposed method is more robust in the low-resource scenario and shallow layers.
To further evaluate the model towards the low-resource scenario, we invariably keep the validation
set and test set and randomly choose a different ratio of the universal datasets as the new training
data, which simulate the extreme low-resource scenario. The ratio is set to 50%, 30%, 10%, and
1% descendingly. The smaller the number, the closer to the setting of the low-resource scenario.
We adopt the KD_1 as the base encoder. Table 4 shows that our model achieves advantages in
most experiments (30 out of 33), with 1.29% improvement on average under 50% training data,
2.41% improvement on average under 30% training data, 5.07% improvement on average under
10% training data, and 16.60% improvement on average under 1% training data. Meanwhile, we
find that the three experiments with reduced performance all occurred on the AS dataset. Notice
from Table 2 that the average length of the AS dataset is relatively short, as will be discussed in the
next section, the PCRF-based model is more efficient for long sentences.
4.2	Scalability
As mentioned above, unlike some scenarios of competing performance, industrial scenarios repre-
sented by searching, advertising, and recommendation, the speed of CWS is critical as performance.
According to the experiment, the KD_1 based CWS model spends roughly 14% of time consumption
compared with the original BERT model, which meets the requirement of segmentation response.
Since we adopt the proposed PCRF as the decoding module, it relies highly on the language model.
Intuitively the length of a sentence can have a substantial effect on the decoding performance. We
randomly select several test sets and classify the sentence in each test set according to its length.
We denote that the length smaller than 20 as short sentence, the length longer than 50 as the long
sentence, otherwise the medium sentence. Note that the numbers of the long and short test sets are
inconsistent, so we randomly select the size of the smaller dataset from the larger data set to make
the equal size of the two datasets. To directly exhibit the influence of the length of the sentence and
7
Under review as a conference paper at ICLR 2022
RATIO	Models	MSR	AS	PKU	CityU	CTB	SXU	UDC	CNC
	KD _1+Softmax	93.79	95.18	89.75	89.87	90.56	89.69	81.68	95.02
50%	KD_1+CRF	93.79	95.18	89.75	89.88	90.56	89.70	81.69	95.02
	OUrs(KD_1+PCRF)	94.28	94.11	9193	90.46	91.73	91.98	86.12	95.30
	KD_1+Softmax	92.34	94.78	87.78	88.29	89.16	87.96	78.18	94.22
30%	KD_1+CRF	92.34	94.78	87.80	88.29	89.16	87.96	75.46	94.22
	OUrs(KD_1+PCRF	93.84	93.92	90.74	89.95	90.80	91.03	84.07	94.93
	KD _1+Softmax	88.80	93.37	83.53	84.39	84.66	83.09	64.73	91.90
10%	KD_1+CRF	88.81	93.36	83.55	84.39	84.67	83.10	64.73	91.90
	OUrs(KD_1+PCRF)	92.54	93.33	8790	87.36	89.19	88.58	82.18	93.96
	KD_1+Softmax	79.26	86.27	55.55	66.69	55.66	52.61	41.12	82.77
1%	KD_1+CRF	79.27	86.27	55.80	66.69	55.79	52.57	41.18	82.77
	OUrs(KD_1+PCRF)	89.14	90.46	7734	80.16	78.32	84.84	65.93	89.16
Table 4: Results on eight universal dataset results at different scales.
avoid contingency, we use three distilled student models of diverse layers (1, 3, and 6) as the encoder
and conduct experiments on the short and long test groups. Table 5 shows the result divergence
between the two test groups, the results are averaged by the number of student models. Given that
the sentence length is the only variable, we can conclude that the proposed decode method will
benefit when the sentence length is long.
s∕9>∣√φφds
-→- Layer 1
-→- Layer 3
-→- Layer 6
--Original BERT
Figure 2: Inference speed at different batch size, the sequence length is set to 128.
Models	Datasets	CityU	PKU	CTB	MSR	SXU	UDC	CNC
/ɔŋl? RQUgrI	Short	0.7804	0.7465	0.8153	0.9270	0.7878	0.6822	0.9151
CRF Based	Long	0.8236	0.7961	0.7977	0.9270	0.7859	0.6430	0.9090
PCRF Based	Short	0.8346	0.8236	0.8675	0.9080	0.8761	0.7661	0.9364
	Long	0.8739	0.8708	08746	0.9270	0.8878	0.7766	0.9330
Table 5: Results on eight universal dataset results at different scales.
Figure 2	illustrates the inference speed under various batch sizes. We distilled student models with 1,
3, and 6 layers, respectively. With the increase of batch size, the inference speed of each model will
8
Under review as a conference paper at ICLR 2022
increase simultaneously. As the batch size comes to a certain extent, the speed reaches a maximum
point and stop increases any more, and even decreases. Due to the varying length of a sentence
within each batch, we need to pad the different sequences to the same value to facilitate the inference
process. The larger the batch size is, the longer time the pad operation takes. Under the single-layer
model with batch size set to 16, the inference speed is 245KB/s, which can satisfy most scenarios
with strict speed requirements.
Figure 3 shows the impact of coefficient λ. For simplicity, we only select the single-layer-based
model under ratio 1% with datasets from SIGHAN2005. The parameter is used to trade off the
score between PPL and scores output by the encoder. There are two conclusions that we can draw
from the picture. Firstly, under the single-layer model setting, the best coefficient for the different
ratio-based models is inconsistent. Under the ratio of 1% setting, the optimal parameter is 0.7, while
with the ratio of 10%, the optimal parameter is 0.1. Secondly, since that the coefficient λ is used
during the decoding process, it didn’t take part in the training process. In reality, we can ensure the
optimal λ according to the performance on the test set to best fit the actual environment.
TL 6UUBE,lotωd
TL 6UUBE,lotωd
94.3
93 76
94.25
93 76
94.17
93 7
94.08
93 63
92 31	92 38	92 38	92 34
0.1	0.2	0.3	0.4	0.5	0.6	0.7
CofficientS of λ, MSR dataset
93.92
93.35
94.22
94.0
93.54
90.65
0.1	0.2	Q3	0.4	0.5	0 6	0.7
COfficientS of λ, AS dataset
—・-MSR_1%
"■- MSR_10%
—•- MSR_30%
—•- MSR_50%
"AS- AS_1%
TL 6UUBE,lotωd
91.8
90.74
91.86
90.83
91.8
90.77
,91.62
90.6
’87.9	~ 87.99	*87.97	* 87.82
0.2	0.3	0.4	0.5	Q6	0.7
COfficientS of λ, PKU dataset
-∙- AS_10%	90
-∙- AS_30%
一■" AS 50%	88
6UUBE,lotωd
. αtyU-1%
. CityU_10%
—C— CityU_30%
-C— CityU_50%
0.1	Q2	0.3	0.4	0.5	0.6	0.7
COfficients of λ, CityU dataset
—9- PKU_1%
—a- PKU_10%
—P- PKU_30%
. PKU 50%
Figure 3:	The effect of λ on the performance of the model.
5 Conclusion
This paper proposes an efficient CWS framework for low-resource scenarios. It is formulated to
facilitate industrial use. It adopts the knowledge distillation technology to make use of the powerful
pre-trained model. To best suit the practical scenario, we recommend that a distilled single-layer
student model fulfill the speed requirements. To compensate for the knowledge lost during the
distillation process, we propose an upgraded decode method, which introduces the n-gram language
model score into the CRF, enhancing the richness of the knowledge during decoding. Experiments
on diverse datasets, including several popular benchmark datasets, two publicly available datasets,
and two hand-crafted datasets from real industrial scenarios, well demonstrate the effectiveness of
our method.
9
Under review as a conference paper at ICLR 2022
References
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Sufeng Duan and Hai Zhao. Attention is all you need for chinese word segmentation. arXiv preprint
arXiv:1910.14537, 2019.
Jinlan Fu, Pengfei Liu, Qi Zhang, and Xuan-Jing Huang. Is chinese word segmentation a solved
task? rethinking neural chinese word segmentation. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),pp. 5676-5686, 2020.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, et al. Pre-trained models: Past, present and future. AI Open, 2021.
Kenneth Heafield. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth
workshop on statistical machine translation, pp. 187-197, 2011.
Kaiyu Huang, Degen Huang, Zhuang Liu, and Fengran Mo. A joint multiple criteria model in trans-
fer learning for cross-domain chinese word segmentation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pp. 3873-3882, 2020.
Weipeng Huang, Xingyi Cheng, Kunlong Chen, Taifeng Wang, and Wei Chu. Toward fast
and accurate neural chinese word segmentation with multi-criteria learning. arXiv preprint
arXiv:1903.04190, 2019.
Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991, 2015.
Zhen Ke, Liang Shi, Songtao Sun, Erli Meng, Bin Wang, and Xipeng Qiu. Pre-training with meta
learning for chinese word segmentation. arXiv preprint arXiv:2010.12272, 2020.
Junxin Liu, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, and Xing Xie. Neural chinese word
segmentation with dictionary. Neurocomputing, 338:46-54, 2019.
Wei Liu, Xiyan Fu, Yue Zhang, and Wenming Xiao. Lexicon enhanced chinese sequence labelling
using bert adapter. arXiv preprint arXiv:2105.07148, 2021.
Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and Yonggang Wang. Improving chinese word
segmentation with wordhood memory networks. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pp. 8274-8285, 2020.
William Yang Wang, Lingpeng Kong, Kathryn Mazaitis, and William Cohen. Dependency pars-
ing for weibo: An efficient probabilistic logic programming approach. In Proceedings of the
2014 conference on empirical methods in natural language processing (EMNLP), pp. 1152-1158,
2014.
Tianwen Wei, Jianwei Qi, Shenghuan He, and Songtao Sun. Masked conditional random fields for
sequence labeling. arXiv preprint arXiv:2103.10682, 2021.
Jingjing Xu, Shuming Ma, Yi Zhang, Bingzhen Wei, Xiaoyan Cai, and Xu Sun. Transfer deep learn-
ing for low-resource chinese word segmentation with a novel neural network. In National CCF
Conference on Natural Language Processing and Chinese Computing, pp. 721-730. Springer,
2017.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural language engineering, 11(2):207-238, 2005.
Jie Yang, Yue Zhang, and Shuailong Liang. Subword encoding in lattice lstm for chinese word
segmentation. arXiv preprint arXiv:1810.12594, 2018.
Daniel Zeman, Jan Hajic, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre,
and Slav Petrov. Conll 2018 shared task: Multilingual parsing from raw text to universal depen-
dencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual parsing from raw text to
universal dependencies, pp. 1-21, 2018.
10
Under review as a conference paper at ICLR 2022
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. Type-supervised domain adaptation
for joint segmentation and pos-tagging. In Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguistics, pp. 588-597, 2014.
11