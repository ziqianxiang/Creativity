Under review as a conference paper at ICLR 2022
H-ENTROPY SEARCH: GENERALIZING BAYESIAN
Optimization with a Decision-theoretic
Uncertainty Measure
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian optimization (BO) is a popular method for efficiently inferring optima of
an expensive black-box function via a sequence of queries. Existing information-
theoretic BO procedures aim to make queries that most reduce the uncertainty about
optima, where the uncertainty is captured by Shannon entropy. However, an optimal
measure of uncertainty would, ideally, factor in how we intend to use the inferred
quantity in some downstream procedure. In this paper, we instead consider the
H-entropy, a generalization of Shannon entropy from work in statistical decision
theory (DeGroot, 1962; Rao, 1984), which contains a broad class of uncertainty
measures parameterized by a problem-specific loss function corresponding to a
downstream task. We first show that special cases of the H -entropy lead to popular
acquisition functions used in BO procedures such as knowledge gradient, expected
improvement, and entropy search. We then show how alternative choices for the
loss yield a flexible family of acquisition functions for a variety of specialized
optimization tasks, including variants of top-k estimation, level set estimation, and
multi-valued search. For special cases of the loss and design space, we develop
gradient-based methods to efficiently optimize our proposed family of acquisition
functions, and demonstrate that the resulting BO procedure shows strong empirical
performance on a diverse set of optimization tasks.
1	Introduction
A popular class of methods for global optimization of a black-box function f over a design space is
information-based Bayesian optimization (BO), which includes the family of entropy search methods
(Hennig & Schuler, 2012; Herngndez-Lobato et al., 2014; Wang & Jegelka, 2017). As in other
BO procedures, entropy search methods leverage a probabilistic model of the function, p(f), to
select pointwise queries of f for sample-efficient optimization. Specifically, at each iteration t, these
methods query f(xt), where xt ∈ X ⊂ Rd is the design that is expected to yield the largest reduction
in the Shannon entropy of the posterior distribution over the optimal design x* = arg maXχ∈χ f (x),
yielding the query selection criterion
xt = arg max H [x* | Dt] -Ep(yx|Dt) [H [x*|Dt ∪ {(x, yx)}]] ,	(1)
x∈X
where H [x* | Dt] is the differential Shannon entropy of the posterior distribution p(x* | Dt) induced
by the probabilistic model, p(yx | Dt) is the posterior predictive distribution at a point x ∈ X, and
Dt = {(xi, yxi)}it=-11 is a dataset of observations. Intuitively, this criterion queries an input xt that is
expected to most reduce the uncertainty of p(x* | Dt).
Shannon entropy is one measure of uncertainty that we could aim to reduce at each iteration of
Bayesian optimization—however, it is not the only measure, and it is not necessarily the most-ideal
measure for every optimization task. An optimal uncertainty function would, ideally, factor in how
we intend to use an inferred quantity (and our uncertainty about it) in some downstream procedure.
For example, instead of reducing the Shannon entropy of p(x* | Dt), we could aim to shrink the
posterior uncertainty with repect to how x* is used downstream, such that the posterior expected
downstream performance is maximized. Furthermore, there are many optimization variants where we
are interested in reducing the uncertainty about more-complex quantities beyond x* , such as in the
tasks of level set estimation, multi-objective optimization, and top-k estimation.
1
Under review as a conference paper at ICLR 2022
In this paper, we instead consider the H-entropy, a generalization of Shannon entropy from work in
statistical decision theory (DeGroot,1962; Rao, 1984; Grunwald & Dawid, 2004), which contains a
broad class of uncertainty measures for a posterior distribution over f, parameterized by a problem-
specific loss function and set of actions. For a given optimization task, we can define a loss and action
set tailored to the downstream use of the posterior p(f | Dt) after queries are complete. Specifically,
we will assume that, after completion of the BO procedure, we will take the Bayes action with respect
to p(f | Dt)—i.e. the member of the action set that minimizes the posterior expected loss. Then our
H-entropy-based acquisition function can be viewed as choosing an xt to query on f which most
reduces the posterior expected loss of the Bayes action.
We will show that special cases of our acquisition function are equivalent to the entropy search
acquisition functions in Eq. (1), as well as other popular acquisition functions used in BO, such as
the knowledge gradient (Frazier et al., 2009) and expected improvement (Mockus, 1975; Jones et al.,
1998) functions. Moreover, we show that alternative choices for the loss and action set provide a
flexible family of acquisition functions for a variety of specialized optimization tasks. For example,
we define special cases of our acquisition function for the tasks of top-k estimation with diversity
(estimating a set of k optimal designs under a penalty that encourages diversity), generalized level
set estimation (partitioning the design space X based on binned function value), and multi-value
sequence search (estimating a sequence of points with multiple prescribed values under f).
Finally, we present a full BO procedure, H-ENTROPY SEARCH, and show that it is computationally
feasible for a large class of tasks (including each of the examples above). We design a general
recipe for gradient-based optimization of the acquisition function, and show how to implement the
computation techniques using automatic differentiation (Balandat et al., 2020; Paszke et al., 2019) to
accelerate acquisition function optimization. In summary, we provide the following contributions:
•	We introduce a family of acquisition functions based on H -entropy, parameterized by a loss
function ` and action set A. We show that choices of ` and A yield acquisition functions used in
popular BO methods such as expected improvement, knowledge gradient, and entropy search.
•	By customizing ` and A for a given optimization task, we derive new acquisition functions for
specialized optimization settings, including top-k estimation with diversity, generalized level
set estimation, and multi-value sequence search.
•	Under certain conditions on `, A, and the design space X, we provide gradient-based methods
to optimize our proposed acquisition function.
•	We demonstrate that H-ENTROPY SEARCH, using gradient-based acquisition optimization,
shows strong empirical performance on a diverse set of optimization tasks.
2	Setup
Let f : X → Y denote an expensive black-box function that maps from an input search space X to
an output space Y, and where f ∈ F. We assume that we can evaluate f at an input x ∈ X, and will
observe a noisy function value yx = f (x) + , where is drawn from some noise distribution.
We will assume that, at some point after the optimization procedure, we intend to take an action a
from some set of actions A, and then incur some loss based on both this action a and the function f .
We denote this loss as ` : F × A → R. As one example, after the BO procedure, we may be allowed
to make a single guess x* of the function maximizer, and then incur a loss based on the value of the
function at x*. In this case, the action set is A = X and the loss is '(f, a) = '(f, x*) = -f (x*).
We will also assume that our uncertainty about f is captured by a probabilistic model with prior
distribution p(f), which reflects our prior beliefs about f. Given a dataset set of observed function
evaluations Dt = {(xi, yxi)}it=-11, our model gives a posterior distribution over F, denotedp(f|Dt). 3
3	H-ENTROPY SEARCH
We first define the H-entropy, a decision-theoretic notion of uncertainty, which is parameterized by
a problem-specific action set A and loss function `. This uncertainty measure has been introduced
previously as a generalization of Shannon entropy (DeGroot, 1962; Rao, 1984; Grunwald & Dawid,
2004). We adopt the phrase H-entropy and symbol H, which were both used by Rao (1982; 1984;
1987) to refer to this family of entropy functionals.
2
Under review as a conference paper at ICLR 2022
Definition 1. (H-Entropy of f). Given a prior distribution p(f) on functions, and a dataset Dt of
observed function evaluations, the posterior H -entropy with loss ` and action set A is defined to be
h`,a [f | Dt] = Tf Ep(f∣Dt) ['(f,a)] .	(2)
Intuitively, suppose that we must make a decision by choosing an action a ∈ A, where this action
incurs a loss `(a, f) defined by the loss function ` and true function f. Given a posterior distribution
p(f | Dt) that describes our belief about f after observing Dt, the Bayes action a* ∈ A is the action
that minimizes the posterior expected loss, i.e. a* = arg infa∈A Ep(f ①力['(f, a)]. The H-entropy
can then be viewed as the posterior expected loss of the Bayes action.
We propose a family of acquisition functions for BO using this H -entropy, which are similar in
structure to information-theoretic acquisition functions such as entropy search (ES) (Hennig &
Schuler, 2012), predictive entropy search (PES) (Herngndez-Lobato et al., 2014), and max-value
entropy search (MES) (Wang & Jegelka, 2017). Our family of acquisition functions are designed to
select the query xt ∈ X that maximally reduces the uncertainty, as characterized by the H-entropy,
in expectation. We refer to this quantity as the expected H-information gain (EHIG).
Definition 2. (Expected H-Information Gain). Given a prior distribution p(f) on functions and a
dataset of observed function evaluations Dt, the expected H -information gain (EHIG), with loss `
and action set A, is defined to be
EHIGt(χ;', A) = h`,a [f | Dt] - Ep(yχ∣Dt) [H',a [f ]Dt ∪{(χ,yx)}]] ∙	(3)
We note that EHIG is similar in structure to the entropy search acquisition function given in Eq. (1):
the Shannon entropy H[χ* | Dt] in (1) is simply replaced by the H-entropy H',∕[f | Dt]. However,
there are a couple of notable differences arising from this substitution. First, H -entropy characterizes
the uncertainty of p(f | Dt), rather than p(x* | Dt). We will show how this can both generalize
the entropy search acquisition function (as well as a few other popular acquisition functions used
in BO), and also allow us to tailor our acquisition function to infer quantities beyond x*, which
is desirable in optimization variants (e.g. levelset estimation, multi-objective optimization, and
top-k estimation). Second, while the entropy search acquisition function uses the Shannon entropy
to describe uncertainty about the inferred quantity, the H -entropy describes uncertainty using a
problem-specific loss, which can be tailored to the downstream use of the inferred quantity.
We present H -ENTROPY SEARCH, our full Bayesian optimization procedure using the EHIG acquisi-
tion function, in Algorithm 1. This procedure takes as input a loss `, action set A, and prior model
p(f). At each iteration, the procedure optimizes EHIGt(x; `, A) to select a design xt ∈ X to query,
and then evaluates the black-box function on this design to observe an outcome yχt 〜f (xt) + e.
In Section 6 we will describe methods for optimizing the EHIG acquisition function, including
gradient-based procedures for certain cases of X, A, and `.
Algorithm 1 H-ENTROPY SEARCH
Input: dataset D1, prior distribution p(f), action set A, loss `.
1:	for t = 1, . . . , T do
2:	xt J arg maXχ∈χ EHIGt (x;', A)	. Optimize expected H-information gain
3:	yxt 〜f(xt) + e	. Evaluate f at Xt
4:	Dt+1 J Dt ∪ {(xt, yxt)}	. Update dataset
Output: distribution p(f | DT+1)
4	Existing Acquisition Functions for BO as S pecial Cases
We next show how popular acquisition functions developed for BO are special cases of the proposed
EHIG acquisition function family given in Eq. (3), for particular choices of ` and A. In particular, we
will show this for the knowledge gradient (Frazier et al., 2009), entropy search (Hennig & Schuler,
2012; Herngndez-Lobato et al., 2014; Wang & Jegelka, 2017), and expected improvement (MOCkUs,
1975; Jones et al., 1998) acquisition functions. To do so, we will view each acquisition function from
a decision-theoretic perspective: after the BO procedure is complete, we must make some decision
and then incur a loss, and we want to make a sequence of queries that reduce the uncertainty of the
posterior distribution over f in order to help best make this decision with low loss.
3
Under review as a conference paper at ICLR 2022
Knowledge gradient (KG). The knowledge gradient (KG) acquisition function can be written
KGt(X) = Ep(yχ∣Dt) ∣X+1 (X,yX)] - μ↑	⑷
where μJ= = supχθ∈χ Ep(f ∣d. [f (χ0)] is the maximum value of the posterior mean of f given data
Dt, and μ↑+ι(x, yx) = supχ0∈χ Ep(f ∣Dt∪{(χ,yx)}) [f (x0)] is the maximum value of the posterior
mean of f, given both data Dt and observation (X, yx).
It is straightforward to view this acquisition function as a special case of EHIGt. Suppose, after the
BO procedure is complete, we must make a guess X= for the maximizer of f , and then incur a loss
equal to the value of the function at X=. In this case, we can view the action set as A = X, and the
loss function as `(f, a) = `(f, X) = -f (X). Note that the Bayes action will then be equal to the
maximizer of the posterior mean, the H-entropy will be equal to -μ=, and thus the EHIGt will equal
KGt . We formalize this in the following proposition.
Proposition 1. If we choose A = X and `(f, X) = -f (X), then the EHIG is equivalent to the
knowledge gradient acquisition function, i.e. EHIGt(X; `, A) = KGt(X).
Proofof Proposition 1. The proof is given in the appendix.	□
Entropy search (ES, PES, MES). We will restate the entropy search acquisition function given in
Eq. (1) to include a broader family of information-based BO objectives. Let θf ∈ Θ denote a property
of f that we would like to infer. For example, we could set θf = arg maxx∈X f(X) = X= ∈ X,
i.e. the location of the global optimizer of f, or θf = maxx∈X f(X) ∈ R, i.e. the maximum value
achieved by f in X . This generalized entropy search acquisition function can then be written
ESt(X)= H [θf | Dt] - Ep(yχ∣Dt) [H [θf ∣Dt∪{(x,yχ)}]],	(5)
where H[θf | Dt] = - R p(θf | Dt) log p(θf | Dt) df denotes the differential Shannon entropy of
the induced posterior distribution over θf.
We can then view this acquisition function as a special case of EHIGt in the following way. Suppose,
after the BO procedure is complete, we must choose a distribution q from a set of distributions P(Θ),
and then we will incur a loss equal to the negative log-likelihood of q for the true value of θf. In this
case, we view the action set as A = P(Θ) and the loss function as `(f, a) = `(f, q) = - log q(θf).
The H-entropy will then be equal to the Shannon entropy of θ, and thus the EHIGt will be equal to
ESt . We formalize this in the following proposition.
Proposition 2. If we choose A = P(Θ) and`(f, q) = - log q(θf), then the EHIG is equivalent to
the entropy search acquisition function, i.e. EHIGt (X; `, A) = ESt(X).
Proofof Proposition 2. The proof is given in the appendix.	□
Expected improvement (EI). The expected improvement (EI) acquisition function can be written
EIt(X) = Ep(f|Dt) [max(0, ft= - f (X))] .	(6)
where we define ft = max{f(χi)}t-1, for Xi ∈ Dt, where /(Xi) is the posterior expected value of
f at Xi . Note that this definition is equal to the standard formulation of EI in the noiseless setting
(i.e. when yx = f(X) for queried X) and is equal to the plug-in formulation ofEI in the noisy setting,
when yx = f(X) + (Picheny et al., 2013; Brochu et al., 2010).
We can then view this acquisition function as a special case of EHIGt in the following way. Suppose,
after the BO procedure is complete, we incur a loss based on the value of f at the best queried
point Xt ∈ Dt, where Xt = argmax{f(Xi)}t=1. In this case, we can use a time-dependent action
set that depends on the previous queries, i.e. At = {Xi}it=-11 and can define the loss function as
`(f, a) = `(f, Xi) = -f(Xi). The Bayes action will then be equal to Xtt, the H-entropy will be equal
to -f(Xt) = - ft, and the EHIGt will be equal to EIt. We formalize this as follows.
Proposition 3. If we choose At = {Xi}ti=-11, where Xi ∈ Dt, and `(f, Xi) = -f (Xi), then the EHIG
is equal to the expected improvement acquisition function, i.e. EHIGt(X; `, A) = EIt (X).
Proofof Proposition 3. The proof is given in the appendix.	□
4
Under review as a conference paper at ICLR 2022
Practical guidance and summary In summary, via our EHIG framework, we can view multiple
acquisition functions (KG, ES, PES, MES, EI) from a decision-theoretic perspective, and provide
guidance on which to choose based on the preferred loss in a given optimiztion scenario, namely:
•	Knowledge gradient (KG): one should choose this if, after the BO procedure ends, one wants to
know the point x* ∈ X, at which the value f (x*) is expected to be highest.
•	Entropy search on θ (ES, PES, MES): one should choose this if, after the BO procedure ends,
one wants a posterior distribution q over a function property θf , in which the log-likelihood
log q(θf) is expected to be highest.
•	Expected improvement (EI): one should choose this if, after the BO procedure ends, one wants
a dataset Dt in which the maximal queried value maxxi∈Dt f(xi) is expected to be highest.
While the above methods are for standard BO, which focus on estimating a single optimal point under
the black-box function, in the following sections we will show how our EHIG framework extends to
a broader class of specialized optimization tasks.
5	Novel Acquisition Functions for S pecialized Optimization
We use EHIG to derive novel acquisition functions for specialized optimization settings, including
top-k estimation with diversity, (generalized) level set estimation, and multi-value sequence search.
Top-k estimation with diversity. For a discrete design space X, the task of top-k estimation is to
estimate the subset of X with size k that has the highest values under the black-box function f ; when
the domain X is continuous, the task of top-k estimation with diversity aims to solve the constrained
optimization problem: max{xi}k	∈Xk	Pik=1 f(xi)	such that	∀i, j ∈ {1, . . . ,	k},	d(xi, xj)	≥	c,
where d : X × X → R is some predefined distance function on X (e.g. Euclidean distance) and c is a
distance threshold to encourage diversity. This type of task arises, for example, in materials discovery
(Liu et al., 2017), sensor networks (Abbasi et al., 2008), and medicine (Xie, 2018).
To approach this task in our EHIG framework, we design a loss using soft constraints, which are
suitable for our continuous domain. In particular, we define the action set as A = Xk (where an
action a = (a1 , . . . , ak ) ∈ A denotes a set of top-k points) and define the loss function as
`(f,a) = -Xf(ai) - X d(ai,aj).	(7)
i	1≤i<j≤k
Generalized level set estimation. The goal of level set estimation (LSE) is to estimate a subset
of the design space X, where function values are larger than a given threshold c, Sc = {x ∈ X :
f(x) > c}. This task appears in a number of applications, including catalyst design (Zhong et al.,
2020), interactive learning (Boecking et al., 2020), and environmental monitoring (Singh, 2008).
As an initial approach to LSE under our EHIG framework, we focus on carrying out LSE for a
discrete subset of design points X0 ⊂ X of size J, i.e. |X0| = J. We then define the loss and action
set using X0. The action set, defined as A = [0, 1]J, represents a set of weights associated with each
element in X0, which can be interpreted as the confidence of an element belonging to the super-level
set. We define the loss as
`(f, a(x)) = - X a(x) (f (x) - c) .	(8)
x∈X0
such that at optimality a(x) = 1 for each x ∈ X0 with f(x) > c, and a(x) = 0 otherwise. Similarly,
we can consider a generalized level set estimation problem, where we are given m thresholds
satisfying c1 < . . . < cm and we are interested at estimating m + 1 level sets: Si = {x ∈ X : ci <
f(x) < ci+1} for i = {0, . . . , m} (where c0 := -∞ and cm+1 := +∞). In this case, we define the
action set to be A = [0, 1]m×J and the loss to be
m
`(f, a(x)) =-XX
ai(x) (f(x) - ci).	(9)
i=1 x∈X0
When the loss achieves its maximum value, for each i ∈ {1, . . . , m}, ai (x) determines the ci-super
level set, i.e. ai(x) = 1 for each x ∈ X0 with f(x) > ci and ai(x) = 0 otherwise.
5
Under review as a conference paper at ICLR 2022
Multi-value sequence search. Given a black-box function f , the goal of multi-value sequence
search is to estimate a sequence of inputs (x1, . . . , xm) ∈ Xm, each with a different pre-specified
function value (y1~, . . . , ym~). This task arises when we want to estimate the inverse function h :
Y → X (where h returns a point x ∈ X for some desired function value y) for a sequence of values
(y1, . . . , ym). In the context of public health applications, for example, we may be interested in a set
of locations where vaccination rates approximate some pre-specified values (e.g. (20%, . . . , 80%))
when designing the next round of vaccine allocations, as we describe in Section 7.
To solve this problem with our EHIG framework, we let A = Xm, and define the loss to be
m
`(f, a) =X(f(am)-ym~)2.	(10)
i=1
6	Acquisition Optimization
At each iteration of H -ENTROPY SEARCH (Algorithm 1), we optimize the acquisition function
to select the next query xt = arg maxx∈X EHIGt (x; `, A). Historically, zeroth order optimization
routinues have often been used for acquisition optimization in BO. However, recent work has
developed gradient-based methods for optimizing certain acquisition functions (Wilson et al., 2018;
Balandat et al., 2020), which can allow for efficient acquisition optimization over X. We work on
similar methodology here—namely, we develop a gradient-based acquisition optimization procedure
for appropriate settings (i.e. assuming continuous X and A, and certain conditions on `). We have
implemented this gradient based optimization for each of the acquisition functions described in
Section 5, for which we show experimental results in Section 7.
6.1	Gradient-based Acquisition Optimization
Similar to previous related work (Wilson et al., 2018; Balandat et al., 2020), we will provide the
following derivation with a focus on Gaussian process (GP) models of the black-box function f,
though the methodology can be extended to other models in which we can apply the reparameterization
procedure described below to differentiate through posterior model parameters.
Differentiable loss function We first describe a few assumptions that must be satisfied to carry out
the gradient-based optimization procedure.
Denote the posterior expected loss given D by L(D, a) := Ep(f|D) [`(f, a)]. We assume that this
loss function depends only on the function value of f at a finite number of points, i.e. there exists
functions χι(a),…,XK (a), and a function '0 : RK ×A→ R, for K ∈ N, such that
'(f, a)= '0(f (Xι(a)), f (X2(a)),…，f (Xκ (a)), a)∙	(11)
This requirement is satisfied by the loss functions in Section 5. For brevity, denote the sequence
Xι(a),…,XK (a) by Xi：K (a) and f(Xι(a)),…,f(Xκ (a)) by f(Xi:K (a)). We assume that the func-
tions xk and `0 are differentiable with respect to all arguments. Given a dataset D and GP prior, the
posterior distribution of f (XK (a)) is also Gaussian. In particular, there exist functions
μ : Xi：K (a) ×D → RK and U : Xi：k(。)×D → Rk×k .	(12)
such that f (xi：K(a)) = μ(χi:K(a); D) + U(xi：K(a); D)e where e is drawn from a K-dimensional
standard normal distribution. We can combine the above results to get
L(D, a) = Ee ['0(μ(Xi:K(a)； D) + U(xi：K(a); D)e, a)] .	(13)
A key property is that we can compute unbiased gradients of this with respect to both D and a, as
VL(D, a) = Ee [V'0(μ(χ止K(a)； D) + U(xi：K(a); D)e,a)] .	(14)
Differentiable acquisition function For a given input x ∈ X, let y(x, D) denote the posterior
predictive distribution of our model. Note that there exists a deterministic function y(x, D, λ) such
that y(χ, D) = y(x, D, λ) where λ is drawn from a standard normal distribution. Hence, if ' satisfies
Eq. (11), then we can optimize EHIGt with gradient descent. In particular, we can write
inf -EHIGt(x;', A)= inf Eλ[inf Ep(f∣D∪y(x,D,λ)['(f, a)]]
x∈X	x∈X	a∈A
(15)
=inf inf Eλe['0(μ(x, a(λ)) + U(x, a(λ))e, a(λ))]
x∈X a,TA	,
6
Under review as a conference paper at ICLR 2022
where in Eq. (15), to avoid clutter, We define the notation shorthand μ(x, a(λ)) := μ(χLK(a(λ)); D∪
y(χ, D, λ)), and U(χ, a(λ)) := U(χ±K(a(λ)); D ∪ y(x, D, λ)). The important property ofEq. (15)
is thatwe can compute the unbiased gradient ofthe quantity Eλ,e ['0(μ(χ, a(λ)) + U(x, a(λ))e, a(λ))].
In practice, we can also take gradients of a Monte Carlo estimate of Eq. (15) (Balandat et al., 2020),
by fixing samples of λ, e throughout the optimization. Specifically, we can sample λι,…，Xm and
eι,…，eN and approximate Eq. (15) via
inf -EHIGt(x;', A) ≈ inf inf -ɪ? X'0(μ(x, am) + U(x,am)ek,am),	(16)
x∈X	x∈X a1,...,aM NM
m,n
where we use am = a(λm) for brevity. Under the assumptions above, we can compute the unbiased
gradient of this quantity. Using systems such as GPyTorch (Gardner et al., 2018) and BoTorch
(Balandat et al., 2020) we can compute these gradients efficiently via automatic differentiation.
7	Experiments
We evaluate our proposed methods on the three optimization applications described in Section 5:
top-k estimation with diversity, level set estimation, and multi-value sequence search. In each, we
evaluate our method against a set of baselines on real and synthetic black-box functions.
Comparison methods. In each experiment, we compare the following set of acquisition strategies:
•	H-ENTROPY SEARCH (HES). We follow Algorithm 1, using the loss and action set for each task
as described in Section 5, and the Monte Carlo gradient-based acquisition optimization procedure
outlined in Section 6.1.
•	RANDOM SEARCH (RS). At each iteration, we draw a sample xt uniformly at random from X.
•	UNCERTAINTY SAMPLING (US). At each iteration, we select xt = arg maxx∈X p(yx | Dt).
•	Knowledge Gradient (KG). We also wish to compare against a representative existing BO
procedure. KG allows us to carry out a similar Monte Carlo gradient-based acquisition optimization
procedure (as it is a special case of HES) on a sensible loss, as detailed in Section 4.
7.1	Top-k Estimation with Diversity
In our first task, the goal is to find a set of k diverse elements in X, each with a high value of f. To
assess each method, at each iteration we record -'(f, a*) using Eq. (7)——i.e. the negative top-k with
diversity loss of the Bayes action a* = arg infa∈∕ Ep(f ∣d. ['(f, a)] on the true function f-using
the set of queries Dt produced by the given method. Intuitively, if a method makes a set of queries
that yield a good estimate of diverse top-k elements, it will score a high value of -`(f, a*).
In Figure 1 (top row) we show qualitative results on the multimodal Alpine-d function, defined as
Alpine-d(x) = Pid=1 |xi sin(xi) + 0.1xi|, for x ∈ Rd. Here, HES concentrates queries over five
local optima of this function, while KG allocates a majority of samples on only the highest peak,
and both US and RS distribute their queries over the full domain X . We compare performance of
methods quantitatively in Figure 1 (bottom row), where we plot -`(f, a*) versus iteration on two
higher dimensional examples, and can see the advantage of the HES selection strategy.
We also compare performance of each method for this task the Vaccination function (provided
by Yuan et al. (2021)), which returns the vaccination rate for locations in the continential United
States, given an input (latitude, longitude). Here, we restrict the design domain X to the state of
Pennsylvania, due to its rectangular shape. The goal of this task is then to efficiently find a set of
five diverse locations over the state that achieve a high vaccination rate. We show results in Figure 1
(bottom row, right), and see a similar advantage of HES over comparison methods.
7.2	Level Set Estimation
In our second task, the goal is to carry out binary level set estimation. Here, it is easier to assess each
method using a more conventional metric: we produce an estimate of the level a(x) for every x ∈ X0,
using the model’s posterior mean (given the queries selected by a particular comparison method), and
then can record the accuracy of this estimate. Intuitvely, a method will achieve a higher accuracy if it
chooses queries that yield a fine-grained estimate of the function near the boundaries of the level set.
7
Under review as a conference paper at ICLR 2022
Iteration
Iteration
Figure 1: Top-k estimation with diversity. Top row: A comparison of methods on the Alpine-2 function,
showing the set of ground-truth top-k diverse design points (blue squares), queries Dt taken (black dots),
acquisition function optimizer (pink dot), and the estimated set of top-k diverse design points (gold stars).
Bottom row: Plots of -'(f, a*) versus iteration for the set of comparison methods, on the Alpine-3, Alpine-5
and Vaccination functions, averaged over 3 trials, where error bars represent one standard error.
Iteration
Random Search
Uncertainty Sampling
Knowledge Gradient
0.2
H-Entropy Search
0.2	0.4	0.6	0.8	1.0
xι
0.4	0.6	0.8	1.0
xι
0.2	0.4
xι
Pennsylvania Night Light
9 8 7 6 5
. . . . φ
Ooooo
AOJnoov
40	60
Iteration
80	100
Alpine-2
0	20	40	60	80	100
Iteration
0 9 8
. . .
loo
AoEJnooV
Figure 2: Level set estimation. Top and middle rows: A comparison of methods on the Multihills (top) and
Pennsylvania Night Light (middle) functions, showing the ground-truth level set boundary (dashed line) and
queries Dt taken (black or red dots). Bottom row: Plots of accuracy versus iteration for the set of comparison
methods, on three functions, each averaged over 3 trials, where error bars represent one standard error.
In the top row of Figure 2, we show qualitative results on the Multihills function, defined as a mixture
density (details given in appendix), and in the middle row of Figure 2, we show qualitative results on
the Pennsylvania Night Light function1, released by NASA (additional details in the appendix), which
1https://earthobservatory.nasa.gov/features/NightLights
8
Under review as a conference paper at ICLR 2022
Vaccination Rate	Iteration	Iteration
Figure 3: Multi-value sequence search. Left: Visualization of the Vaccination function, along with the
queries Dt taken by HES (black dots), and the estimated sequence (x~,..., x~) (red diamonds), SUCh that
(f (x~),..., f (x~)) = (30%, 40%, 50%, 60%, 70%). Middle and right: Plots of —'(f, a*) versus iteration for
the set of comparison methods, on the Vaccination and Multihills functions, averaged over 3 trials, where error
bars represent one standard error.
returns the relative level of light at a location in Pennsylvania, as queried by a satellite image. The
goal of this experiment is to determine the portion of land at which night light is above a specified
threshold value. In both cases, HES concentrates queries along the boundary of the level set (denoted
as a dashed line). In the bottom row, we plot the accuracy vs. iteration of each method, and see that
this allocation strategy leads to a higher accuracy relative to comparison methods, which allocate
queries to optima (KG) or distribute them over the full domain (US and RS).
7.3	Multi-value Sequence Search
In our third task, the goal is find a sequence of elements whose value under the black-box function
matches some sequence of pre-specified function values (y1~, . . . , ym~).
To assess each method, at each iteration we again record -'(f, a*) from Eq. (10)—i.e. the nega-
tive multi-value sequence loss of the Bayes action a* = arg infa∈∕ Ep(f ∣Dt)['(f, a)] on the true
function f —using the set of queries Dt produced by the given method. Intuitively, if a method
makes a set of queries that yield a good estimate of a sequence of points (x1~ , . . . , x~m) such that
(f(x1~), . . . ,f(x~m)) ≈ (y1~, . . . ,ym~) it will score highly on -'(f, a*).
In Figure 3 (left) we show qualitative results on the Vaccination function (described in Section 7.1).
Here, our goal is to find a sequence of five (latitude, longitude) coordinates with vaccination rates
equal to (y1~, . . . , ym~) = (30%, 40%, 50%, 60%, 70%). Information about sequences of values under
an expensive black-box function such as this can be useful when making policy decisions involving
a vaccine response or allocation. In this case, we see that HES concentrates queries along a route
from the relatively highly vaccinated region in the East to the relatively lowly vaccinated region in
the North. The middle and right plots in Figure 3 provide a quantitive comparison of methods on the
Vaccination and Multihills functions, plotting -'(f, a*) vs. iteration. These again show the benefits
of query selection performed by HES relative to the comparison strategies.
8 Conclusion
In this paper, we take a decision making perspective on acquisition functions in Bayesian optimization:
after the BO procedure is complete, we assume that we must make some decision a* and then incur
a loss '(a*, f). Our goal is then to make a sequence of queries that reduce the uncertainty of the
posterior distribution p(f | Dt) in a way to help best make this decision with low loss. Using
H -entropy (DeGroot, 1962; Rao, 1984), we can define an EHIG acquisition function which carries
this out directly: it selects a point that is expected to maximally reduce the posterior expected loss
of the Bayes action a* . We incorporate this acquisition function into a BO procedure called H-
Entropy Search, and show, under certain conditions, that we can perform efficient gradient-based
optimization of this acquisition function.
There are multiple interesting future directions of study. First, we hope to develop efficient acquisition
optimization procedures for a broader array of settings, such as for non-continuous action sets A or
design spaces X. One interesting avenue is hybrid optimization settings, where we can take gradient
steps with respect to either design or action variables, but must resort to zeroth order optimization
methods for the other. We also wish to further study how the EHIG framework and optimization
strategies proposed in this paper could be used to improve existing BO procedures and provide
insights on existing acquisition functions.
9
Under review as a conference paper at ICLR 2022
References
Ali Abbasi, Ahmad Khonsari, and Navid Farri. Mote: efficient monitoring of top-k set in sensor
networks. In 2008 IEEE Symposium on ComPuters and Communications, pp. 957-962. IEEE,
2008.
Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Benjamin Letham, Andrew Gor-
don Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian opti-
mization. Advances in Neural Information Processing Systems (NeurIPS), 2020.
Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. Interactive weak supervi-
sion: Learning useful heuristics for data labeling. arXiv preprint arXiv:2012.06046, 2020.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.
M H DeGroot. Uncertainty, information, and sequential experiments. Ann. Math. Stat., 33(2):
404-419, 1962.
Peter Frazier, Warren Powell, and Savas Dayanik. The knowledge-gradient policy for correlated
normal beliefs. INFORMS journal on Computing, 21(4):599-613, 2009.
Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. arXiv
preprint arXiv:1809.11165, 2018.
Peter D Grunwald and A Philip Dawid. Game theory, maximum entropy, minimum discrepancy and
robust bayesian decision theory. Ann. Stat., 32(4):1367-1433, August 2004.
Philipp Hennig and Christian J Schuler. Entropy search for Information-Efficient global optimization.
J. Mach. Learn. Res., 13(57):1809-1837, 2012.
Jos6 Miguel Herndndez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy
search for efficient global optimization of black-box functions. In NIPS, 2014.
Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455-492, 1998.
Yue Liu, Tianlu Zhao, Wangwei Ju, and Siqi Shi. Materials discovery and design using machine
learning. Journal of Materiomics, 3(3):159-177, 2017.
Jonas MoCkus. On bayesian methods for seeking the extremum. In Optimization techniques IFIP
technical conference, pp. 400-404. Springer, 1975.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Victor Picheny, Tobias Wagner, and David Ginsbourger. A benchmark of kriging-based infill criteria
for noisy optimization. Structural and Multidisciplinary Optimization, 48(3):607-626, 2013.
C Radakrishna Rao. Differential metrics in probability spaces. Differential geometry in statistical
inference, 10:217-240, 1987.
C Radhakrishna Rao. Diversity and dissimilarity coefficients: A unified approach, 1982.
C Radhakrishna Rao. Convexity properties of entropy functions and analysis of diversity. Lecture
Notes-Monograph Series, pp. 68-77, 1984.
Aarti Singh. Nonparametric Set Estimation Problems in Statistical Inference and Learning. PhD
thesis, University of Wisconsin-Madison, 2008.
10
Under review as a conference paper at ICLR 2022
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In
Proceedings ofthe 34th International Conference on Machine Learning-Volume 70, pp. 3627-3635,
2017.
James T Wilson, Frank Hutter, and Marc Peter Deisenroth. Maximizing acquisition functions for
bayesian optimization. arXiv preprint arXiv:1805.10196, 2018.
Pengtao Xie. Diversity-promoting and large-scale machine learning for healthcare. PhD thesis,
University of Pittsburgh Medical Center, 2018.
Yuan Yuan, Eaman Jahani, Shengjia Zhao, Yong-Yeo Ahn, and Alex Sandy Pentland. Mobility
network reveals the impact of geographic vaccination heterogeneity on covid-19. medRxiv, 2021.
Miao Zhong, Kevin Tran, Yimeng Min, Chuanhao Wang, Ziyun Wang, Cao-Thang Dinh, Phil
De Luna, Zongqian Yu, Armin Sedighian Rasouli, Peter Brodersen, et al. Accelerated discovery of
co 2 electrocatalysts using active machine learning. Nature, 581(7807):178-183, 2020.
11
Under review as a conference paper at ICLR 2022
A	Proofs of Propositions
Here we prove the propositions stated in Section 4.
Proposition 1. If we choose A = X and `(f, x) = -f (x), then the EHIG is equivalent to the
knowledge gradient acquisition function, i.e. EHIGt (x; `, A) = KGt (x).
Proof of Proposition 1. The proof follows directly from the definition of H-entropy and the EHIG,
namely
EHIGt(X) = inf Ep(f N ['(f, a)] - Ep^s Jigf Ep(f s∪{(χ,yχ)}) ['(f, a)]l	(17)
a∈A	a∈A
=i0nXEp(f|Dt) [-f(χ0)] - Ep(yx∣Dt) i0nXEp(fIDt∪{(χ,yx)}) [-f(x0)]	(18)
x ∈X	x ∈X
=-SUp Ep(f∣Dt) [f(x0)] + Ep(yx∣Dt) SUp Ep(f∣Dt∪{(x,yχ)}) [f(x0)]	(19)
x0∈X	x0∈X
=Ep(yχ |Dt) [μJ+l(x,yχ)] - μt	(20)
= KGt(x)	(21)
□
Proposition 2. If we choose A = P(Θ) and `(f, q) = - log q(θf), then the EHIG is equivalent to
the entropy search acquisition function, i.e. EHIGt (x; `, A) = ESt (x).
Proof of Proposition 2. We first prove that under our definition of loss `, the H-entropy H [f | Dt] is
equivalent to the Shannon entropy of the posterior distribution over θf (where, as an example, θf
could be equal to the global maximizer x* of f).
Note that the H -entropy is the expected loss of the Bayes action
q* = arg infq∈P(X)Ep(fD) [- log q(θf)] ∙
We want to show that q* defined above is equal to p(θf | Dt). To do so, note that
q* = arginf q∈P(X )Ep(f∣Dt) [—log q(θf |Dt)]	(22)
=arginfq∈P(x)Ep(θf∣Dt) [—logq(θf ∣Dt)]	(23)
=p(θf |Dt),	(24)
where the first equality holds since
EX[f(g(X))] =EZ[f(Z)],whenZ=g(X).	(25)
Therefore, under this loss and action set, using the definition of the EHIG we can write
EHIGt (x;', A)= H(p(θf | Dt)) — Ep(yχ∣Dt)[H(p(θf |Dt ∪{χ,yχ}))]=ESt(x).	(26)
□
Proposition 3. If we choose At = {xi}it=-11, where xi ∈ Dt, and `(f, xi) = —f(xi), then the EHIG
is equal to the expected improvement acquisition function, i.e. EHIGt (x; `, A) = EIt(x).
Proof of Proposition 3. The first term in Eq. (3) is equal to:
ʌ
H',At[f | Dt] = inf Ep(f∣Dt) ['(f,a)] = — max f(xi) := -f
a∈At	i≤t-1
ʌ
where f(xi) is the posterior expected value of f at xi.
(27)
12
Under review as a conference paper at ICLR 2022
The second term in Eq. (3) is:
Ep(yχ∣Dt) [H',At+ι [f I Dt ∪{(x,yχ)}]]			(28)
=Ep(yx|Dt)	Ep(f |Dt ∪{(x,yx)})	inf `(f, a) a∈At+1	(29)
=Ep(yχ∣Dt) EPpff ∣Dt∪{(χ,yχ)})[		- max(ft*, f (x))]	(30)
=Ep(yx |Dt) [- max( ft,yχ)]			(31)
Putting it together, the EHIGt acquisition function in Eq. (3) will reduce to:			
EHIGt(x;', A) = -ft* - Ep(yχ∣Dt) [- max(ft*,yx)]			(32)
	= Ep(yx|Dt)[max(0,yx - ft*)]		(33)
	= EIt(x).		(34) □
B	Additional Details on Experiments
Here we show results from additional experiments and datasets.
Details on the Multihills function The Multihills function is defined as a mixture density as
follows. MUItihills(x) = Pj=ι WjN(X | μj, Cj), for X ∈ Rd, where N denotes a multivariate
normal density, {μj } are a set of J means, {Cj } are a set of J covarance matrices, and {wj } are a set
of J weights.
Details on the Pennsylvania Night Light function We consider the 2012 gray scale global night-
light raster with resolution 0.1 degree per pixel. The data is downloaded from NASA2. We focus on
Pennsylvania and normalize the raster data before using.
More visualization results for level set estimation on Alpine-2 function. We provide an addi-
tional visualization result for the level set estimation experiment on the Alpine-2 function in Figure 4.
Figure 4: Level set estimation for Alpine-2 function. We show the ground-truth level set boundary with red
dashed line and queries Dt taken with black dots.
Random Search
2https://earthobservatory.nasa.gov/features/NightLights
13