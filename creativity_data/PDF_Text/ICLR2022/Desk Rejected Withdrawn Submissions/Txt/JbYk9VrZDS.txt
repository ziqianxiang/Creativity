Under review as a conference paper at ICLR 2022
Federated Learning with Data-Agnostic
Distribution Fusion
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning has emerged as a promising distributed machine learning
paradigm to preserve data privacy. One of the fundamental challenges of
federated learning is that data samples across clients are usually not independent
and identically distributed (non-IID), leading to slow convergence and severe
performance drop of the aggregated global model. In this paper, we propose a
novel data-agnostic distribution fusion based model aggregation method called
FedDAF to optimize federated learning with non-IID local datasets, based on
which the heterogeneous clients’ data distributions can be represented by the
fusion of several virtual components with different parameters and weights. We
develop a variational autoencoder (VAE) method to derive the optimal parameters
for the fusion distribution using the limited statistical information extracted
from local models, which optimizes model aggregation for federated learning by
solving a probabilistic maximization problem. Extensive experiments based on
various federated learning scenarios with real-world datasets show that FedDAF
achieves significant performance improvement compared to the state-of-the-art.
1	Introduction
Federated learning (FL) has emerged as a novel distributed machine learning paradigm that allows
a global deep neural network (DNN) model to be trained by multiple mobile clients collaboratively.
In such a paradigm, mobile clients train local models based on datasets generated by edge devices
such as sensors and smartphones, and the server is responsible to aggregate the parameters from
local models to form a global model without transferring data to a central server. Federated learning
has been drawn much attention in mobile-edge computing Konecny et al. (2016); SUn et al. (2017)
with its advantages in preserving data privacy Zhu & Jin (2020); Keller et al. (2018) and enhancing
communication efficiency Smith et al. (2018); McMahan et al. (2017); Wang et al. (2020). Besides,
a lot of algorithms have been proposed to improve the resource allocation fairness, communication
efficiency, and convergence rate for federated learning Kairouz et al. (2019); Lim et al. (2020), which
include LAG Chen et al. (2018), Zeno Xie et al. (2019), AFL Mohri et al. (2019), q-FedSGD Li et al.
(2020b), FedMA Wang et al. (2020), etc.
One of the fundamental challenges of federated learning is the non-IID data sampling from
heterogeneous clients. In real-world federated learning scenarios, local datasets are typically non-
IID, and the local models trained on them are significantly different from each other. Aggregating
the local models with simple averaging may cause severe performance degradation in terms of model
accuracy and communication rounds required for convergence. It was reported in Zhao et al. (2018)
that the accuracy of a convolutional neural network (CNN) model trained by FedAvg reduces by
up to 55% for a highly skewed non-IID dataset. The work in Wang et al. (2020) showed that the
accuracy of VGG model trained with FedAvg and its variants dropped from 61% to under 50% when
the client number increases from 5 to 20 on heterogeneous data partition.
Several works have been made to address the non-IID challenge. Li et al. (2020a) modified FedAvg
by adding a dissimilarity bound on local datasets and a proximal term on the local model parameter
to tackle heterogeneity. However, it poses restrictions on the local updates to be closer to the initial
global model, which may lead to model bias. Zhao et al. (2018) proposed a data sharing strategy
to improve training on non-IID data by creating a small subset of data to share between all clients.
However, data sharing could weaken the privacy requirement of federated learning. Several works
adopted clustering based approaches to tackle non-IID settings, where client models were partitioned
1
Under review as a conference paper at ICLR 2022
into clusters and model aggregation in performed in cluster level Chen et al. (2020); Xie et al. (2020);
Ghosh et al. (2020); Duan et al. (2020). However, clustered federated learning may suffer from
privacy leakage with shared data to cluster clients, and its performance relied on the cluster number
which is a hyperparameter needed to be manually adjusted from task to task.
In this paper, we propose a novel data-agnostic distribution fusion method called FedDAF for
federated learning on non-IID data. We introduce a distribution fusion model to describe the
global data distribution as a fusion of several visual components belonging to the same parametric
family of distributions, which is ideal for representing non-IID data generated from heterogeneous
clients. However, applying a distribution fusion for federated learning encounters several difficulties.
First, due to the privacy policy of federated learning, the local datasets are inaccessible and their
distributions are unknown to the server, so it is impossible for the server to form a global distribution
based on observing to data samples. Second, the number of distribution components and their fusion
weights are unspecified without the knowledge of local data, making it a challenging task to develop
such a fusion model for federated learning.
To tackle these issues, we propose an efficient solution to optimize the distribution fusion federated
learning problem with variational inference. Since the local data is inaccessible to the server, our
method is based on the limited statistical information embedded in the normalization layers of DNN
models, i.e., the means and standard deviations of the feature maps (the outputs of intermediate
layers). Those information can be extracted from the received local model parameters, which
can be used to infer a global distribution. We develop a variational autoencoder (VAE) model to
derive the optimal parameters of distribution fusion components based on the observed information,
and applied the derived parameters to optimize federated learning with non-IID data. Extensive
experiments based on a variety of federated learning scenarios with non-IID data show that FedDAF
significantly outperforms the state-of-the-arts.
The contributions of our work are summarized as follows.
•	We propose a novel data-agnostic distribution fusion based model aggregation method
called FedDAF to address the data heterogeneity problem in federated learning. It
represents the global data by a fusion model of several virtual distribution components
with different fusion weights, which is ideal to describe non-IID data generated from
heterogeneous clients.
•	We develop a variational autoencoder (VAE) method to derive the optimal parameters
for the data-agnostic distribution fusion federated learning model. Without violating
the privacy principle of federated learning, the proposed method uses limited statistical
information embedded in DNN models to infer a target global distribution with a maximum
probability. Based on the inferred parameters, an optimal model aggregation strategy can
be developed for federated learning under non-IID data.
•	We conduct extensive experiments using five mainstream DNN models based on four real-
world datasets under non-IID conditions. Compared to FedAvg and the state-of-the-art for
non-IID data (FedProx, FedMA, IFCA, FedGroup, etc), the proposed FedDAF has better
convergence and training efficiency, improving the global model’s accuracy up to 12%.
2	Related Work
Federated learning Konecny et al. (2015) is an emerging distributed machine learning paradigm
that aims to build a global model based on datasets distributing across multiple clients. One of the
standard parameter aggregation methods is FedAvg McMahan et al. (2017), which combined local
stochastic gradient descent (SGD) on each client with a server that performs parameter averaging.
Later, the lazily aggregated gradient (LAG) method Chen et al. (2018) allowed clients to run multiple
epochs before model aggregation to reduce communication costs. The q-FedSGD Li et al. (2020b)
method improved FedAvg with a dynamic SGD update step using a scale factor to achieve fair
resource allocation among heterogeneous clients. The FedMA Wang et al. (2020) method, derived
from AFL Mohri et al. (2019) and PFNM Yurochkin et al. (2019), demonstrated that permutations
of layers could affect the parameter aggregation results, and proposed a layer-wise parameter-
permutation aggregation method to solve the problem. The FedDyn Acar et al. (2021) method
proposed a dynamic regularizer for each client at each round of aggregation, so that different models
are aligned to alleviate the inconsistency between local loss and global loss.
2
Under review as a conference paper at ICLR 2022
Several works focused on optimizing federated learning under non-IID data. Zhao et al. used
the earth mover’s distance (EMD) to quantify data heterogeneity and proposed to use globally
shared data for training to deal with non-IID Zhao et al. (2018). The RNN-based method Ji et al.
(2019) adopted a meta-learning method to learn a new gradient from the received gradients and
then applied it to update the global model. FedProx Li et al. (2020a) modified FedAvg by adding a
heterogeneity bound on local datasets and a proximal term on the local model parameter to tackle
heterogeneity. FedBN Li et al. (2021) suggested keeping the local Batch Normalization parameters
not synchronized with the global model to mitigate feature shifts in non-IID data. FedGN Hsieh et al.
(2020) replaced Batch Normalization with Group Normalization to avoids the accuracy loss induced
by the skewed distribution of data labels. Yang et al. provided theoretical evidence on linear speedup
for convergence of FedAvg under non-IID datasets with partial worker participation Yang et al.
(2021). The federated cluster learning Chen et al. (2020) Xie et al. (2020) Ghosh et al. (2020) Duan
et al. (2020) partitioned clients into clusters to address data heterogeneity, and aggregated different
models for different clusters. For example, IFCA Ghosh et al. (2020) alternately estimated the
cluster identities of the clients and optimized the model parameters for the clusters via gradient
descent. FedGroup Duan et al. (2020) grouped the clients based on the similarities between their
optimization directions to improve training efficiency. The personalized federated learning Smith
et al. (2017) Khodak et al. (2019) Liang et al. (2020) Peng et al. (2020) further adopted multi-task
learning and meta-learning to train personalized model for individual client. Different from clustered
FL and personalized FL that form multiple personalized models, our work focus on training a single
global model from heterogeneous clients. We propose a novel data agnostic fusion with variational
inference to optimize the model aggregation process in federated learning under non-IID conditions,
which has not yet been addressed in the literature.
3	Formulation of Federated Learning with Distribution Fusion
We consider the following federated learning scenario with non-IID data. We assume there are K
clients involved in the learning system, and Dk (k = 1, ∙∙∙ ,K) indicates the data distribution of
the kth client. The learning process repeats for multiple communication rounds. At the beginning
of each round, each client downloads a learning model from the global server, and trains the model
individually with its local data to minimize the local loss, i.e., min L (w, Dk)(k = 1,…，K) where
L(∙) is the loss function and W is the earnable model parameters. The optimal local model of the
kth client is given by:
w，k = arg WmS L (w, Dk).	(1)
After receiving model parameters Wk from local clients, with D to be global distribution of all Dk,
the optimal global model in the server is given by:
w* = arg min L(w = aggr(wι,.…,WK), D),	(2)
w∈S
where aggr(wι, •…，WK) is the strategy of the server to aggregate local models into a global
model. Conventionally, the aggregation strategies are typically in the form of averaging or weighted-
averaging in McMahan et al. (2017) Li et al. (2020a) Wang et al. (2020) Li et al. (2020b) Duan et al.
(2020) Ghosh et al. (2020), etc. In this paper, we explore a distribution fusion method to derive the
optimal model aggregation strategy.
Since both local data and their distributions are unknown to the server, we model the target global
distribution D as a fusion of the distributions with M (M ≤ K) virtual components (M can be
adaptively learned from the task): D = PM=1 ∏mDm, where Dm (m = 1, ∙∙∙ , M) is the
mth virtual distribution component and PmM=1 πm = 1 are the fusion weights. With the above
model, each client’s data distribution Dk can be allocated into several of the M components in
{D)1,..., D m }. To formally describe the distribution fusion, we introduce a distribution allocation
vector C k, thatis defined as a zero-one vector C k = [ ckm∣m = 1,…,M ], where Ckm = 1 if the data
of the kth client can be allocated to the mth virtual distribution component and otherwise ckm = 0.
And bkm = P (ckm = 1|Dk) is normalized conditional probability of how much Dk been allocated
to mth virtual component. With such notation, we consider allocating the distribution of K clients’
data distributions to M virtual components as a probability event, and the distribution fusion model
can be described as:
MK
D= fπmf bkm
m=1	k=1
• Ckm ∙Dk, s.t.,
M
bkm = 1.
m=1
(3)
3
Under review as a conference paper at ICLR 2022
Based on Eq. 3, the objective of model aggregation in Eq. 2 can be formulated as the following
optimization problem:
MK	K	M
(π*,c *, b*) = arg mip L (Y' ∏mY'' bkm ∙ Ckm ∙ w k, D), s.t., >Jk = 1, Y' bkm = L (4)
π,c,b
m=1	k=1	k=1	m=1
The minimization problem in above formulation can be understood as finding the optimal fusion
parameters π, c and b to maximize the probability of allocating the clients’ data distribution to
the most probable virtual distribution components, so that the expected global loss over the target
distribution is minimized. Notice that in an extreme condition where the data are IID among all
clients, the number of virtual components M = 1 and the objective in Eq. (4) equals to simple
averaging, which makes the classical FedAvg McMahan et al. (2017) a special case of our model.
Next, we derive the solution of the optimization problem with a variational inference method.
4 Data-Agnostic Distribution Fusion Based on Variational
Inference
Since local datasets are only accessible by their owners in federated learning for privacy protection,
the local distributions D1,…,DK are unknown to the server, making derivation of target
distribution D difficult. To confront this challenge, we propose a novel idea to use limited statistical
information during model aggregation to approximate the optimal distribution fusion parameters.
In each communication round of federated learning, the clients will update their model parameters
based on local data and report the updated models to the server for model aggregation. Although the
private data is unknown, there are some statistical information embedded in the reported model
parameters which can be used by the server to infer the local distributions. For example, in a
DNN model, the statistical information can be extracted from the normalization layers such as batch
normalization Ioffe & Szegedy (2015), layer normalization Ba et al. (2016), instance normalization
Ulyanov et al. (2016), and group normalization Wu & He (2018), which typically contain the
following statistical variables:
•	μ k, σk: the means and standard deviations of the feature maps (the outputs of intermediate
layers) of the kth client’s DNN model.
•	βk, Yk: the shifted means and scaled Standard deviations Ioffe & Szegedy (2015) of the
feature maps of the kth client’s DNN model.
We use dk = {μk, σk, βk, Yk} to denote the observed statistical variable of the kth client. Since
the real distribution Dk is unknown, we can approximate the objective in Eq. (4) by maximizing the
probability of distribution allocation given the observed models’ statistical information, which can
be expressed as:
MK
(∏^, C ^, b *) = arg min L (废 ∏m^ bkm ∙ Ckm ∙ w k, d),
π,c,b
m=1	k=1
KM
s.t., bkm = P (Ckm = 1|dk),	πk = 1,	bkm = 1.
k=1	m=1
(5)
Hence we convert a data-dependent optimization problem into a data-agnostic problem based on
observable statistical variables to the server. Notice that the proposed method exchanges exactly
the same information between server and clients as conventional FedAvg McMahan et al. (2017),
which will not violate privacy protection in federated learning. Next, we introduce a variational
autoencoder method to derive the optimal model parameters.
4.1	Variational AutoEncoder
Since the probabilities in Eq. (5) are hard to be expressed in mathematical form, we adopt a
variational autoencoder (VAE) method to derive the optimal parameters πm and ck of the fusion
based on variational inference. The plate notions of the VAE are shown in Fig. 1.
•	ck ∈ {0, 1}M is a zero-one vector representing distribution allocation, where Ckm = 1 represents
allocating the distribution of the kth client to the mth virtual component. We assume that ck is
4
Under review as a conference paper at ICLR 2022
sampled from a Bernoulli distribution which is parameterized by λk = {λkm∣m = 1,…，M}, and
λk is sampled from a Beta distribution Beta(ζm, κm) which is parameterized by ζm, κm.
•	bk = {bkm ∈ (0,1)∖m = 1,…，M}, PM=I bkm, = 1, where bkm, represents the allocation
weight of the kth client to mth virtual component, and the sum of weights is 1. We assume that bk
is sampled from a Gaussian prior distribution N(νm, ςm) which is parameterized by νm and ςm.
•	Zk = PM=ι bkm ∙ zm is a latent variable used by the decoder θ to reconstruct the observed dk,
where Θ is the inner product of two vectors, and Zk means the sampled latent vector from every
allocated distribution for kth client from the Gaussian prior distribution N (νm0 , ςm0 ).
As illustrated in Fig. 1, the parameters of
Beta(ζm, κm), N (νm, ςm) and N (νm0 , ςm0 ) can
be inferred with an variational encoder φ
based on 0the 0 observed information dk, i.e.,
{νm , ςm , νm , ςm , ζm , κm } = φ(dk). In the
meanwhile, the variables of bk and Zm are used
to compute a latent variable zk , which is further
fed to a decoder θ to reconstruct the observed data
dk with nonlinear transformation. By optimizing
the parameters of the encoder-decoder, the optimal
allocation vector ck and the weight vector bk can
be derived, which can be further used to derived the
fusion weights πm .
The details of allocation encoder-decoder are
explained as follows. As Zm is not related with
allocation, we will not discuss here.
Figure 1: The variational Bayesian autoencoder
using plate notations, where φ and θ are global
variables representing the encoder’s parameters and
the decoder’s parameters respectively.
Encoder: As shown in Fig. 1, in order to infer the
latent vector Zk , we should derive the variational
posterior qφ(λk, ck, bk). We employ a multi-
head nonlinear model to infer the approximation
of true posterior p(λk, ck, bk∖dk) with variational
posteriors, and apply the stochastic gradient variational Bayes (SGVB) Kingma & Welling (2014)
algorithm to learn the model.
From Fig. 1 we know that variables in variational posterior are conditionally independent with
the priori P(dk). So we can decouple the variables as: qφ(λ, c, b) = QK=I QM=ι qφ(bkm) ∙
qφ(ckm∖λkm) ∙ q®(λkm), where the variational posterior distributions Nalisnick & Smyth (2017)
can be derived by:
qφ (bk ) ~ N( Vm Sm ),
qφ (λk ) 〜Beta(Zm, Km ),
qφ(ck)〜Bernoulli( ɪɪ λkm).
m=1
Decoder: The decoder θ takes the latent variable Zk as input to reconstruct the original observed data.
According to Fig. 1, the derivation of Zk relies on three variables bk, λk, and ck, whose variational
posteriors are Gaussian, Beta, and Bernoulli distribution accordingly, as shown in Eq. (6). We infer
the three latent variables as follows.
Since the posterior ofbk is a Gaussian distribution with differentiable Monte Carlo expectations, it
can be easily inferred with the Stochastic Gradient Variational Bayes (SGVB) estimator Kingma &
Welling (2014).
The posterior of λk is a Beta distribution, which is hard to be inferred with conventional variational
inference algorithms. We approximate the posterior Beta with the Kumaraswamy distribution
Nalisnick & Smyth (2017); Kumaraswamy (1980), a two-parameter continuous distribution also
on the unit interval with a density function defined as:
K umaraswamy(x; ζk, κk) = ζkκkxζk-1(1 - xζk)κk-1,	(7)
5
Under review as a conference paper at ICLR 2022
where ζk and κk are parameters of the distribution. It was proved that the Kumaraswamy
approaches to the Beta albeit with high entropy, and it satisfies the differentiable and non-centered
parameterization (DNCP) property with its closed-form inverse CDF Nalisnick & Smyth (2017).
Therefore the samples of λk can be drawn via the inverse transform of Kumaraswamy:
1	1
λk 〜(1 — ξ κk) Zk, where ξ 〜Uniform (0, 1).
(8)
For the zero-one vector ck, we reparameterize it with the Beta distribution as in Eq. (6). Using the
Gumbel-Max trick to draw samples ck from a Bernoulli distribution with binary probabilities Jang
et al. (2017), we have:	2
ckm = arg miax(gi + log	λki),	(9)
i=1
where gi is an IID sample drawn from Gumbel(0, 1). After deriving bk and ck and sampling latent
vector z k from every component which client k been allocated, We can compute the latent variable
Zk with Zk = PM=ι bkm ∙ Zm. Then we use Zk to reconstruct the original observed data dk with
ʌ
Pθ(μk, σ^k, βk, Yk |zk). The decoder θ can be parameterized by using a deep neural network to learn
the model.
To derive the component weight πm , we use a variant of the EM algorithm Dempster et al. (1977)
with a softmax function:
πm
exp( KK Pk=1 qφ (Ckm ) ∙ bkm )
PM=1 eXP( -K PK=1 qφ(Ckm) ∙ bkm)
(10)
4.2 Optimizing the Variational AutoEncoder
We optimize the proposed variational autoencoder as follows. The dashed lines in Fig. 1 denote
the generative model pθ(zk)pθ(dk |zk), and the solid lines denote the variational approximation
qφ(zkIdk) to the intractable posterior pθ(zk∖dk). We approximate pθ(zk∖dk) with q®(zk|dk) by
minimizing their KL-divergence Joyce (2011):
φ*, θ^ = arg min D kl (qφ (z k∖ d k) ∖∖ Pθ (z k ∖ d k)).	(11)
θ,φ
To derive the optimal value of the parameters φ and θ, we compute the marginal likelihood of dk :
logp(dk) = Dkl(qφ(zk∖dk) ∖∖Pθ(zk∖dk)) + Eqφ(ZfcIdfc) logIp(Zk,dk) ,	(12)
qφ(zk∖dk)
where the first term is the KL-divergence of the approximate distribution and the posterior
distribution; and the second term is called the ELBO (Evidence Lower BOund) on the marginal
likelihood of the k-th client’s dataset.
Since logp(dk) is non-negative, the minimization problem of Eq. (11) can be converted to
maximizing the corresponding ELBO. To solve the problem, we change the form of ELBO as:
pθ(zk, dk)
qφ(zkldk) 产 qφ(zk∖dk)一
Eqφ (Zk Idk)
l	P(Z k) 一
gqφ (Z k ∖ dk)
一一	」
{z
Encoder
+Eqφ(Zkldk)[logpθ(dk∖zk)] .
|	」
Decoder
(13)
The above form is a variational encoder-decoder structure: the model qφ(zk∖dk) can be viewed as a
probabilistic encoder that given an observed statistics dk it produces a distribution over the possible
values of the latent variable zk; Themodelpθ(sk ∖zk) can be refered to as a probabilistic decoder that
reconstructs the value of dk based on the latent variable zk. According to the theory of variational
inference Kingma & Welling (2014), the problem in Eq. (13) can be solved with the SGD method
using a nonlinear deep neural network (DNN) to optimize the mean squared error loss function. The
overall FedDAF model aggregation process is summarized in Algorithm 1.
5	Experiments
In this section, we evaluate the performance of the proposed FedDAF method for federated learning.
5.1	Experimental Setup
Implementation: We implement the proposed FedDAF algorithm and the considered baselines in
PyTorch Paszke et al. (2019). We train the models in a simulated federated learning environment
6
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: The FedDAF model aggregation algorithm.
Initialize w0.
for each communication round t = 0, 1, . . . , T - 1 do
wkt+1 := the model received from client k
ʌ
dk ：= (μk, σ^k, βk, Yk) // statical info from client k
// Periodically conduct the following variational inference process:
repeat
Inference κm, ζ,, νm, ςm, νm0 and ςm0 based on encoder φ
bk, λk, ck ：= sampling from distributions with Eq. 6, 8, 9
Zm := sampling from N(Vm, ς'°rι)
PM b
Zk : mm =1 bkm Zm
Recover zk to dk based on decoder θ with Eq. 13
until VAE converge;
Wt+1 ：= PM=I ∏m PK=1 bkm ∙ Ckm - Wt++1 //model aggregation
broadcast wt+1 to all clients
consisting of one server and a set of mobile clients with wireless network connections. Unless
explicitly specified, the default number of clients is 50, and the learning rate β = 0.01. We conduct
experiments on a GPU-equipped personal computer (CPU: Intel Core i7-8700 3.2GHz, GPU: Nvidia
GeForce RTX 2070, Memory: 32GB DDR4 2666MHz, and OS: 64-bit Ubuntu 16.04).
Models and datasets: Our experiments are based on 5 mainstream deep neural network models:
ResNet18 He et al. (2016), LeNet Lecun et al. (1998), DenseNet121 Huang et al. (2017),
MobileNetV2 Sandler et al. (2018), and BiLSTM.
We use 4 real world datasets: MNIST LeCun et al. (2010), Fashion-MNIST Xiao et al. (2017),
CIFAR-10 Krizhevsky (2009), and Sentiment140 Go et al. (2009). MNIST is a dataset for hand
written digits classification with 60000 samples of 28×28 greyscale image. Fashion-MNIST is an
extended version of MNIST for benchmarking machine learning algorithms. CIFAR-10 is a large
image dataset with 10 categories, each of which has 6000 samples of size 32×32. Sentiment140 is
a natural language process dataset containing 1,600,000 extracted tweets annotated in scale 0 to 4
for sentiment detection.
We generate non-IID data partition according to the work McMahan et al. (2017). For each dataset,
we use 80% as training dada to form non-IID local datasets as follows. We sort the data by their
labels and divide each class into 200 shards. Each client draw samples from the shards to form
η∈ [0, 1],
N (0.5, 1),
a local dataset with probability pr(x)
iofthxe∈rwCilsaes.sj, It means that the client
draws samples from a particular class j with a fixed probability η, and from other classes based on a
Gaussian distribution. The larger η is, the more likely the samples concentrate on a particular class,
and the more heterogeneous the datasets are. By default we set η = 0.5.
(d)
(a)	(b)	(c)
Figure 2:	Convergence of different algorithms. (a) ResNet18 on CIFAR10, (b) DenseNet121 on CIFAR10, (c)
MobileNetV2 on CIFAR10, (d) BiLSTM on Sent140.
7
Under review as a conference paper at ICLR 2022
A3e」n8±s"l
0 20 40 60 80 100
Comm. Rounds
(a)
→- FedDMM
ADeJnUm1-
0.8
0.7
FedAvg
FedProx
Fed-GN
FedMA
FedDMM
0.6
&0-5
Ia
gθ∙4
[0∙3
H 0.2
0.1
0	20 40 60 80 100
Comm. Rounds
(c)
0.7
AUeJnUSV ⅛ωF
0 20 40 60 80 100
Comm. Rounds
(d)
0	20 40 60 80 100
Comm. Rounds
(b)

Figure 3:	Training efficiency of different algorithms. (a) ResNet18 on CIFAR-10, (b) DenseNet121 on CIFAR-
10, (c) MobileNetV2 on CIFAR-10, (d) BiLSTM on Sent140.
5.2 Performance Comparison
We compare the performance of FedDAF with 4 state-of-the-art methods: FedAvg McMahan et al.
(2017), FedProx Li et al. (2020a), Fed-GN Hsieh et al. (2020), and FedMA Wang et al. (2020). The
results are analyzed as follows.
Convergence: In this experiment, we study the convergence of the compared algorithms by showing
the total communication epochs versus train loss. Fig. 2 shows the convergence of different
algorithms for different models on different datasets. It is shown that the loss of all algorithms
tends to be stable after a number of communication rounds. Clearly, FedDAF has the lowest loss,
and converges the fastest among all algorithms.
Training Efficiency: In this experiment, we study the test accuracy versus time during the training
of a DNN model with federated learning. Fig. 3 shown the results of training different models on
different datasets. It is shown that FedDAF trains much faster than the baseline algorithms, and it
reaches higher accuracy in a shorter period.
1.0

rrl.0
0.8
.6
0.4
0.2
0.0
0.0
(d)
(a)
(b)
(c)
T 1.0
0.8
0.6
0.4
0.2
0.0
Figure 4: Visualization of data distribution (only a subset of the original data is illustrated). (a) the original
distribution of MNIST, (b) the inferred distribution of MNIST with FedDAF, (c) the original distribution of
CIFAR-10, (d) the inferred distribution of CIFAR-10 with FedDAF.
Visualization of Data Distribution: To intuitively illustrate how well the proposed FedDAF can
approximate the original data distribution, we use t-SNE van der Maaten & Hinton (2008) to
visualize the original distribution of MNIST and CIFAR-10 and the distribution fusion inferred with
the proposed VAE. The results are shown in Fig. 4. According to the figure, the inferred distribution
fusion looks very close to the original distribution, which implies that the federated server can well
approximate the distribution parameters without accessing to local data.
0.85
0 0.75
K5
g
0.55
(a)	(b)
Figure 5: Comparison of parameter bias. (a)
ResNet18 on CIFAR-10, (b) BiLSTM on Sent140.
0	0.5
Figure 6: Test accu-
racy with different hetero-
geneity η (ResNet18 on
CIFAR-10).
7F5
Ooo
AUeJnUSV ⅛ωF
50	100
# Clients
Figure 7: Test accu-
racy with different num-
ber of clients (ResNet18
on CIFAR-10).
8
Under review as a conference paper at ICLR 2022
Bias of Model Parameters: To show the power of the proposed VAE method for parameter
optimization, we calculate the mean absolute error (MAE) of the statistical parameters
ʌ
(μ k, σ^k, βk, Yk) compared to a centrally-trained model based on global dataset, and the results
are illustrated in Fig. 5(a) and Fig. 5(b). It is shown that FedDAF has a much lower bias in the
statistical parameters than that of the other algorithms, which means that FedDAF provides a better
approximation to the global data distribution.
Table 1: Comparison of average test accuracy on non-IID datasets.
Dataset	CIFAR-10			-FmnIst	MNIST	Sent140
Model	ResNet18	DenseNet121	MobileNetV2	LeNet	LeNet	-BiLSTM
FedAvg	68.78 (± 0.8z9T	63.33 (± 0.67T	54.69 (± 3.9z2T	79.20 (± 1.13T	97.32 (± 0.04广	58.33 (± 2.01)=
FedProx	70.18(± 0.45)	66.85 (± 0.93)	55.03 (± 2.77)	80.03 (± 0.98)	97.55 (± 0.02)	59.73 (± 1.38)
Fed-GN	72.57 (± 0.78)	70.02 (± 1.36)	56.43 (± 1.92)	81.11 (± 0.74)	97.88 (± 0.02)	63.41 (± 1.94)
FedMA	73.43 (± 1.03)	70.13 (± 1.71)	59.61 (± 2.01)	81.02 (± 1.35)	98.06 (± 0.03)	60.86 (± 2.42)
FeSEM	67.78 (± 2.58)	62.65 (± 0.82)	53.82 (± 3.69)	78.18 (± 1.45)	96.24 (± 0.17)	59.57 (± 3.41)
IFCA	73.04 (± 1.45)	70.85 (± 2.03)	58.93 (± 2.45)	80.82 (± 1.29)	97.09 (± 0.11)	60.82 (± 2.74)
FedCluster	72.57 (± 0.78)	68.77 (± 1.38)	58.18( ± 1.22)	79.11 (± 0.74)	97.88 (± 0.02)	63.41 (± 1.94)
FedGroup	74.38 ( ± 1.92)	71.63 (± 0.74)	59.86 (± 2.09)	81.32 (± 2.07)	97.37 (± 0.61)	63.61 (± 3.26)
FedDAF	81.26 (± 0.82)	75.92 (± 1.25)	62.88 (± 1.21)	83.16 (± 0.74)	98.49 (± 0.04)	67.51 (± 1.71)
Global Model Accuracy: In this experiment, we compare the global model accuracy of different
federated parameter aggregation algorithms after training to converge. For thorough comparison,
we include 4 clustered and personalized FL algorithms FeSEM Xie et al. (2020), IFCA Ghosh et al.
(2020), FedCluster Chen et al. (2020), and FedGroup Duan et al. (2020) as additional baselines.
Since clustered and personalized FL methods output multiple models, we show the average results
of those models. We repeat each experiment for 20 rounds and show the average performance in
Table 1. Comparing the global model accuracy of different federated learning methods, FedDAF
significantly outperforms the other algorithms for all DNN models. It outperforms FedMA by
7.83%, 5.79%, and 3.27% for accuracy in ResNet18, DenseNet121, and MobileNetV2 respectively
on CIFAR-10; achieves 2.14% improvement in LeNet on F-MNIST; 0.37% improvement in LeNet
on MNIST; and 6.65% improvement in BiLSTM on Sent140 accordingly. Compared to FedAvg,
the performance improvement of FedDAF is significant, which achieves up to 12.59% higher in
DenseNet121 on CIFAR-10. In comparison to clustered/personalized FL, FedDAF outperforms the
state-of-the-art method FedGroup by 6.88%, 1.84%, 1.12%, and 3.90% separately in the 4 datasets.
In summary, FedDAF achieves the highest accuracy among all baseline algorithms.
Hyperparameter Analysis: We further analyze the influence of two hyperparameters: the
heterogeneity of local datasets and the number of clients involved.
The heterogeneity of local datasets is represented by η, the probability that a client tends to sample
from a particular class. The more η approaches to 1, the more heterogeneous the local datasets
are. Fig. 6 shows the test accuracy under different levels of heterogeneity. As η increases, the test
accuracy of all models decreases. FedDAF yields the highest test accuracy and slowest performance
drop among all compared algorithms, which shows more robust against η .
Fig. 7 compares the test accuracy of the global model for a different numbers of involved clients.
When the number of clients increases from 20 to 100, the accuracy of FedDAF decreases much
slower than that of the baselines, and it achieves the highest test accuracy among all compared
federated learning algorithms in all cases.
6 Conclusion
Developing efficient model aggregation methods against the performance drop in non-IID data is
a key research problem in federated learning. In this paper, we proposed a novel data agnostic
fusion called FedDAF to optimize federated learning with data heterogeneity. In the proposed
method, each client minimized their local model the same as conventional federated learning, and
the server aggregated the local models by allocating the clients’ data distributions into several
virtual components with different mixture weights. The optimal parameters of the distribution
fusion federated learning model were derived by a variational autoencoder (VAE) method. Based on
variational inference, an efficient algorithm was proposed to optimize federated learning on non-IID
data by solving a probabilistic maximization problem. Extensive experiments showed that FedDAF
significantly outperforms the state-of-the-art on a variety of federated learning scenarios.
9
Under review as a conference paper at ICLR 2022
References
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. Federated learning based on dynamic regularization. In International
Conference on Learning Representations (ICLR’21), 2021.
Jimmy Ba, J. Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450, 2016.
Cheng Chen, Ziyi Chen, Yi Zhou, and Bhavya Kailkhura. Fedcluster: Boosting the convergence
of federated learning via cluster-cycling. In IEEE International Conference on Big Data (Big
Data'20),pp. 5017-5026, 2020.
Tianyi Chen, Georgios Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient
for communication-efficient distributed learning. Advances in Neural Information Processing
Systems (NIPS’18), pp. 5050-5060, 2018.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1-38, 1977.
Moming Duan, Duo Liu, Xinyuan Ji, Renping Liu, Liang Liang, Xianzhang Chen, and Yujuan Tan.
Fedgroup: Ternary cosine similarity-based clustered federated learning framework toward high
accuracy in heterogeneous data. CoRR, abs/2010.06870, 2020.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework
for clustered federated learning. In Advances in Neural Information Processing Systems
(NeurIPS’20), 2020.
Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision,
2009. URL http://help.sentiment140.com/home.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR’16), pp. 770-
778, 2016.
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons. The Non-IID data quagmire
of decentralized machine learning. Proceedings of the 37th International Conference on Machine
Learning (ICML’20), 2020.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR’17), pp. 2261-2269, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by
reducing internal covariate shift. Internation Conference on Machine Learning (ICML’15), 1:
448-456, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
5th International Conference on Learning Representations (ICLR’17), 2017.
Jinlong Ji, Xuhui Chen, Qianlong Wang, Lixing Yu, and Pan Li. Learning to learn gradient
aggregation by gradient descent. In Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence (IJCAI’19), pp. 2614-2620, 2019.
James M. Joyce. Kullback-Leibler Divergence, pp. 720-722. 2011.
P. Kairouz, H. McMahan, B. Avent, AUrelien Bellet, Mehdi Bennis, A. Bhagcji, Keith Bonawitz,
and et al. Advances and open problems in federated learning. ArXiv, abs/1912.04977, 2019.
Marcel Keller, Valerio Pastro, and Dragos Rotaru. Overdrive: Making SPDZ great again. In
Advances in Cryptology (EUROCRYPT’18), volume 10822, pp. 158-189, 2018.
Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based
meta-learning methods. In Advances in Neural Information Processing Systems (NeurIPS’19),
volume 32, 2019.
10
Under review as a conference paper at ICLR 2022
Diederik Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR’14), 2014.
Jakub Konecny, H. Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed
optimization beyond the datacenter. NIPS Optimization for Machine Learning Workshop 2015,
pp. pp.5, 2015.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtðrik.	Federated
optimization: Distributed machine learning for on-device intelligence. ArXiv, abs/1610.02527,
2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Ponnambalam Kumaraswamy. A generalized probability density function for double-bounded
random processes. Journal of Hydrology,, 1980.
Yann Lecun, LAl,on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied
to document recognition. In Proceedings ofthe IEEE, pp. 2278-2324, 1998.
Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and
Systems (MLSys’20), pp. 429-450. 2020a.
Tian Li, Maziar Sanjabi, and Virginia Smith. Fair resource allocation in federated learning. In
International Conference on Learning Representations (ICLR’20), 2020b.
Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated
learning on non-IID features via local batch normalization. In International Conference on
Learning Representations (ICLR’21), 2021.
Paul Pu Liang, Terrance Liu, Liu Ziyin, Ruslan Salakhutdinov, and Louis-Philippe Morency. Think
locally, act globally: Federated learning with local and global representations. arXiv preprint
arXiv:2001.01523, 2020.
Wei Yang Lim, Nguyen Cong Luong, D. Hoang, Y. Jiao, Ying-Chang Liang, Qiang Yang, D. Niyato,
and Chunyan Miao. Federated learning in mobile edge networks: A comprehensive survey. IEEE
Communications Surveys & Tutorials, 22:2031-2063, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. Proceedings of the
20th International Conference on Artificial Intelligence and Statistics (AISTATS’17), 54:1273-
1282, 2017.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In
Proceedings of the 36th International Conference on Machine Learning (ICML’19), volume 97,
pp. 4615-4625, 2019.
Eric T. Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In 5th International
Conference on Learning Representations (ICLR’17), 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, and Zachary DeVito. Pytorch: An imperative style, high-performance deep learning library.
In Advances in Neural Information Processing Systems (NeurIPS’19), pp. 8024-8035, 2019.
Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. Federated adversarial domain
adaptation. In International Conference on Learning Representations (ICLR’20), 2020.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen. Mobilenetv2: Inverted residuals and
linear bottlenecks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR’18),
pp. 4510-4520, 2018.
11
Under review as a conference paper at ICLR 2022
V. Smith, S. Forte, C. Ma, M. Takac, M. I. Jordan, and M. Jaggi. Cocoa: A general framework
for communication-efficient distributed optimization. Journal of Machine Learning Research, 18
(230):1-47, 2018.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems (NIPS’17), volume 30, pp. 4424-
4434, 2017.
Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, and Tie-Yan Liu. Ensemble-compression:
A new method for parallel training of deep neural networks. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases (ECML-KDD’17), pp. 187-202, 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing
ingredient for fast stylization. CoRR, abs/1607.08022, 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579-2605, 2008.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning
Representations (ICLR’20), 2020.
Yuxin Wu and Kaiming He. Group normalization. CoRR, abs/1803.08494, 2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for
benchmarking machine learning algorithms. 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In Proceedings of the 36th International Conference on Machine
Learning (ICML’19), volume 97, pp. 6893-6901, 2019.
Ming Xie, Guodong Long, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang. Multi-center
federated learning. CoRR, abs/2005.01026, 2020.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker
participation in non-IID federated learning. In International Conference on Learning
Representations (ICLR’21), 2021.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In
Proceedings of the 36th International Conference on Machine Learning (ICML’19), volume 97,
pp. 7252-7261, 2019.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and V. Chandra. Federated learning
with Non-IID data. ArXiv, abs/1806.00582, 2018.
H. Zhu and Y. Jin. Multi-objective evolutionary federated learning. IEEE Transactions on Neural
Networks and Learning Systems, 31(4):1310-1322, 2020.
12