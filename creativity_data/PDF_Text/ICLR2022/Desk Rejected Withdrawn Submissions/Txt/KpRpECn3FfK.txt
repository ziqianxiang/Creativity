Under review as a conference paper at ICLR 2022
Graph Convolutional Memory using
Topological Priors
Anonymous authors
Paper under double-blind review
Ab stract
Solving partially-observable Markov decision processes (POMDPs) is critical
when applying reinforcement learning to real-world problems, where agents have
an incomplete view of the world. We present graph convolutional memory
(GCM), the first hybrid memory model for solving POMDPs using reinforcement
learning. GCM uses either human-defined or data-driven topological priors to
form graph neighborhoods, combining them into a larger network topology using
dynamic programming. We query the graph using graph convolution, coalescing
relevant memories into a context-dependent belief. When used without human pri-
ors, GCM performs similarly to state-of-the-art methods. When used with human
priors, GCM outperforms these methods on control, memorization, and navigation
tasks while using significantly fewer parameters.
1	Introduction
Reinforcement learning (RL) was designed to solve fully observable Markov decision processes
(MDPs) (Sutton & Barto, 2018, Chapter 3), where an agent knows its true state - a property that
rarely holds in the real world. Problems where agent state is ambiguous, incomplete, noisy, or
unknown can be modeled as partially-observable MDPs (POMDPs). RL guarantees optimal policy
convergence for POMDPs when a belief is maintained over an episode (Cassandra et al., 1994).
Storing information and retrieving it later for belief estimation is known as memory (Moreno et al.,
2018).
Memory models in RL tend to be either general or task-specific. General memory comes from
the field of sequence learning, with recurrent neural networks (RNNs), transformers, or memory
augmented neural networks (MANNs) as notable examples. General memory assumes a sequential
ordering, but makes no other assumptions about the inputs and can be applied to any POMDP. These
models excel in supervised learning, but tend to be costly in terms of training time and number of
parameters. RL exacerbates these problems with its sparse and noisy learning signal (Beck et al.,
2020).
The substantial cost of training general memory drives many to design task-specific memory for
RL applications, like Chaplot et al. (2020); Parisotto & Salakhutdinov (2017); Gupta et al. (2017);
Lenton et al. (2021) which build 2D or 3D maps for navigation, or Li et al. (2018a) which utilizes
dosing information to inform a tree search over hospital patient states. Task-specific memory is
built around human-defined prior knowledge, and aimed at solving a specific task. The downside of
this memory is that it must be implemented by hand for each specific task. Many RL applications
would benefit from task-specific memory, but the implementation is non-trivial: outside of the core
RL research community there are chemists (Zhou et al., 2017), painters (Huang et al., 2019), or
roboticists (Morad et al., 2021) trying to solve field-specific problems using RL.
This paper is the first to propose a hybrid memory model. Our memory model, termed Graph Convo-
lutional Memory (GCM)1 is applicable to any partially-obserable RL task, but utilizes task-specific
topological priors. In other words, GCM builds a graph structure from topological priors, which are
either human-designed or learned entirely from data. GCM has a simple interface, enabling users
to develop topological priors suited to their specific tasks in a few lines of code (Sec. 2.2). Fig. 1
1GCM is available at https://anonymous.4open.science/r/graph-conv-memory-55AD/
README.md
1
Under review as a conference paper at ICLR 2022
N (Ot)
GNN
π
GCM
Figure 1: GCM flowchart for an incoming observation ot . GCM places ot as a node in a graph, and computes
its neighborhood N (ot), and then updates the edge set. Task-specific topological priors are defined via the
neighborhood. A convolutional GNN queries the graph for belief state bt . A policy π uses the belief for
decision making. Compared to transformers or DNCs, GCM is architecturally simple.
presents an overview of our method. GCM builds a graph, and defines local neighborhoods using
said priors, resulting in an expressive graph topology via dynamic programming. The edge structure
induced by the graph provides interpretability, and facilitates simple debugging. We leverage the
computational efficiency and representational power of graph neural networks (GNNs) to extract
contextualized beliefs from the graph. GCM can be applied to any sequence learning problem, but
in this paper we focus on RL. In our experiments, we show that GCM uses significantly fewer pa-
rameters than RNNs, MANNs, or transformers, but performs comparably without any human priors.
With human-designed priors, GCM outperforms the general memory used in our experiments.
2	Graph Convolutional Memory
We model a POMDP following Kaelbling et al. (1998) with tuple (S, A, T, R, Ω, O). At time t
We enter hidden state St ∈ S and receive observation ot ~ O(st) : S → Ω. We sample action
at ∈ A from policy π and follow transition probabilities T(st, at) : S × A → S to the next state
st+1, receiving reward R(st, at) : S × A → R. We learn π to maximize the expected cumulative
discounted reward subject to discount factor γ: E [Pt∞=0 γtR(st, at)]. In an MDP, the policy uses
the true state π(st) : S → A, but in a POMDP our policy uses the belief state π(bt) : B → A. Our
goal in this paper is to construct a latent belief state bt using memory function M and memory state
mt:
(bt, mt) = M(Ot, mt-1).
(1)
2.1 Model Description
We implement GCM following Eq. 1 using Alg. 1. GCM stores a collection of experiences over an
episode, with each experience represented by an observation vertex O and associated neighborhood
N (O). We query the set of experiences using a graph neural network (GNN) to produce a context-
dependent belief bt | ot.	__________________________________________
In detail, at time t, we insert vertex ot into
the graph, producing mt = (Vt, Et) where
Vt = (o1, . . . ot) and Et : Vt × Vt. We deter-
mine the neighborhood N(ot) using topo-
logical priors defined in Sec. 2.2, and up-
date the edges following:
Algorithm 1 Graph Convolutional Memory
Et = Et-1 ∪ {(ot, oi) | i ∈ N(ot)} (2)
1
2
3
4
5
6
7
8
procedure M(ot, mt-1)
V, E — mt-ι
V — V ∪ ot
# Unpack memory
# Add observation
E — E ∪{(ot,θi)}i∈N(ot) # UPdate edges
Z - GNNθ (V,E)
bt — Z [t]
mt — V, E
return bt , mt
#	Get embedding
#	At current vertex
#	Pack into memory
#	Belief and memory
We query the graph for context-dependent 9: end ProCedUre
information using a GNN with layers h ∈
{1...'}. We convolve over oι,...ot to produce hidden representations Zh,...zh for each hidden
layer, propagating information from the hth-degree neighbors of ot into zth . After collecting and
integrating data across the `th-degree neighborhood, we output zt` as the belief. This provides a
mechanism for fast and relevant feature aggregation over memory graphs, depicted in Fig. 2.
As an example, let us consider a visual navigation task where the neighborhood consists of the
previous observation index N(ot) = {t - 1}. Let o1, o2, o3 represent “chair”, “wall”, and “table”,
respectively. The first GNN layer combines o1 , o2 into a “chair-room” embedding and o2 , o3 into a
2
Under review as a conference paper at ICLR 2022
N(ot)
zt2
bt
N(ot)
oagg
z1agg
Graph Layer 1
Graph Layer 2
Figure 2: The two-layer 1-GNN used in all our experiments. Colors denote mixing of vertex information
and dashed lines denote directed edges, forming neighborhoods N (ot), N (oi). The current observation ot
and aggregated neighboring observations oi, oj pass through fully-connected layers (W11, b1), (W21) before
summation and nonlinearity σ, resulting in the first hidden state zt1 (Eq. 3). We repeat this process at oj , ok , ol
to form hidden states zj1 , zk1 , zl1 . The second layer combines embeddings of the first layer and the second-layer
hidden state zt2 is output as the belief state bt . Additional layers increase the GNN receptive field.
“table-room” embedding. The second layer combines “chair-room” and “table-room” into “dining-
room”, and outputs “dining-room” as the belief. We found the 1-GNN defined in Morris et al.
(2019) empirically outperformed graph isomorphism networks (Xu et al., 2019) and the original
graph convolutional network (Kipf & Welling, 2017). GCM can utilize any GNN, but our GNNs are
built from the 1-GNN convolutional layer defined as:
Zh = σ [Whzh-1 + bh + Whagg ({zh-1∣i ∈ N(ot)})]	(3)
with σ representing a nonlinearity and zt0 = ot, zi0 = oi for the base case. At each layer h, weights
and biases W1h, bh produce a root vertex embedding while W2h generates a neighborhood embedding
using vertex aggregation function agg. Separate weights allow the 1-GNN to weigh each hth-degree
neighborhood’s contribution to the belief, ignoring the neighborhood and decomposing into an MLP
for empty or uninformative neighborhoods. The root and neighborhood embeddings are combined
to produce the layer embedding zth (Fig. 2). Notice, the weights W1h , bh in Eq. 3 form an MLP, so
GCM does not require an MLP preprocessor like other memory models (Mnih et al., 2016).
2.2 Topological Priors
We use the shorthand N(ot) to define the open neighborhood of ot over vertices Vt, in edge-list
format. We compute N (ot) using the union of one or more topological priors Φi : Ωt → 2Vt-1, as
in
k
N(ot) : V → 2Vt-1 := [ Φi(Vt).	(4)
i=1
Breaking down the graph connectivity problem into easier neighborhood-forming subtasks is a form
of dynamic programming. The priors are task-specific, because for different tasks, we often want
different connectivity. For example, in control problems, associating memories temporally could
be beneficial in learning a dynamics model. In navigation, spatial connectivity could aid with loop
closures. We have implemented spatial, temporal, latent similarity, and other topological priors in
Tab. 1, but GCM can utilize any mapping from vertices to a neighborhood. We provide task-specific
examples for medicine and aerospace in Appendix B.
Learning Topological Priors We stress the importance of human knowledge in forming the graph
topology, but there are cases where we have no information about the problem, or where designing
a topological prior is non-trivial. A naive formulation of prior learning via gradient descent is
challenging due to a large number of possible edges represented by boolean values. One option is to
use a fully-connected graph with weighted edges similar to Velickovic et al. (2018), but this would
hinder interpretability of the graph (Jain & Wallace, 2019) and be computationally expensive.
Instead, we propose to learn a probability distribution over all edges, from which we sample to
explore the large edge space. We do not know the ideal neighborhood size, so we must learn this
as well. The Gumbel-Softmax Estimator (Jang et al., 2016) enables differentiable sampling from
3
Under review as a conference paper at ICLR 2022
Prior Description	Φ(V ) Definition
Empty: ot has no neighbors and GCM decomposes into an MLP.	0	(5)
Dense: Connects ot to all other observations o1 . . . ot-1.	{1,2,...t-1}	(6)
Temporal: Similar to the temporal prior of an LSTM, where each observation ot is linked to some previous t - c observation.	{t-c}	(7)
Spatial: Connects observations taken within c meters of each other, useful for	ni	||p(oi)-p(ot)||2 ≤ c o i	and 0 < i < t (8)
problems like navigation. Let p(∙) extract the position from an observation.	
Latent Similarity: Links observations in a non-human readable latent space
(e.g. autoencoders). Various measures like cosine or L2 distance may be used
depending on the space. e is an encoder function, d is a distance measure, and
c is user-defined.
ni	d(ea(nodi)0,e<(oit<))t<c o
(9)
Identity: Connects observations where two values are identical, useful in
discrete domains where inputs are related. a, b are indexing functions (a = b
may hold).
ni	a(oain)d-0b<(oit)<=t0 o
(10)
Table 1: Knowledge-based priors for GCM
a categorical distribution using the reparameterization trick from Kingma & Welling (2014). We
leverage this to build a multinomial distribution across all possible edges, and use the maximum a
posteriori (MAP) estimate to form the neighborhood (Fig. 3).
Assume we want to sample between one and K edges at time t for the neighborhood N(ot). First,
we compute logits li for all possible edges (oi, ot), i ∈ {1, . . . t - 1} in Eq. 11, using a neural net-
work φθ, positional encoding ep from Vaswani et al. (2017), and concatenation operator ||. Then,
we sample Gumbel noise gik, k ∈ {1, . . . K} using the inverse CDF of the Gumbel distribution and
uniform random sampling U(0, 1) in Eq. 12. Using Gumbel noise gik, we build a multinomial distri-
bution from K Gumbel-Softmax distributions Xk (Eq. 13) which we “sample” by taking the MAP
(argmax) (Eq. 14). Note that we are not actually sampling from a single categorical distribution, but
taking the MAP over many perturbed categorical (i.e. multinomial) distributions. This approach is
formalized as follows:
li = φθ (ep(oi) || ep(ot))
gk = TOg(TOgu)； U ~U(0,1)
P(Xk) = (Pt-Xp% + gk) 八 1 ≤ i<t)
j=1 exp (lOg lj +gik)
Φ(Vt) := {argmaxP(Xk) | 1 ≤ k ≤ K} .
(11)
(12)
(13)
(14)
Random sampling helps our method explore more neighborhood possibilities while still providing
boolean values for edges. Since we are sampling with replacement, the neighborhood size can vary
from 1 to K, depending on the kurtosis of the learned distribution. For tasks where information
is replicated over many observations, Eq. 14 can learn a flat distribution, sampling many distinct
edges and building a large neighborhood. In cases where a few key observations contain salient
information, we can learn a peaked distribution to more reliably sample these edges.
We do not run into the interpretability trap of transformers explained in Jain & Wallace (2019), be-
cause we learn a distribution of edges rather than attention weights over the edges. In a transformer,
an infinitesimal perturbation in attention weights can cause a significant change in model output. Our
model learns a distribution, where an infinitesimal perturbation corresponds to an infinitesimal shift
of probability mass. An infinitesimally-shifted distribution will not significantly impact the drawn
samples, or in turn the learned prior output. Furthermore, the learned prior should be robust to noisy
distributions, since the distributions themselves are constantly perturbed with Gumbel noise during
learning. We note this sampling methodology might also be useful for designing differentiable hard
and sparse self-attention in transformers.
4
Under review as a conference paper at ICLR 2022
N(o5) = {1, 3}
Figure 3: Generating one to K edges in a non-deterministic manner, using our learned topological prior. We
present an example of the learned prior with K = 3 at t = 5. An MLP computes logits over previous vertices
to produce three distributions Xk; k ∈ {1, 2, 3}, perturbed with Gumbel noise gik. We compute the MAP for
each Xk, resulting in three samples which form a two-edge neighborhood N(o5) containing vertices o1, o3.
This process is fully-differentiable and trained end-to-end with GCM.
3 Experiments
We evaluate GCM on control (Fig. 5a), non-sequential long-term recall (Fig. 5b), and indoor navi-
gation (Fig. 5c). We run three trials for each memory model across all experiments and report the
mean reward per train batch with a 90% confidence interval. We test six contrasting models, and
base our evaluation on the hidden size of the memory models, denoted as |z | in Fig. 4. Nearly all
hyperparameters are Ray RLlib defaults, tuned for RLlib’s built-in models.2 Appendix C defines all
training environment details and Appendix D contains a complete list of hyperparameters.
We compare GCM to an MLP and four alternative mem-
ory models in all our experiments. The MLP model is
a two-layer feed-forward neural network using tanh ac-
tivation. It has no memory, and forms a performance
lower bound for the memory models. The LSTM mem-
ory model is a MLP followed by an LSTM cell, the stan-
dard model for solving POMDPs (Mnih et al., 2016).
GTrXL is a MLP followed by a single-head GRU-gated
transformer XL. The DNC is an MLP followed by a neu-
ral computer with an LSTM-based memory controller.
Our memory model, GCM, uses a two-layer 1-GNN us-
ing tanh activation with sum (cartpole and concentration)
and mean (navigation) neighborhood aggregation. The
GCML is the GCM with the learned topological prior
(Eq. 14) with K = 5, and φθ as a two-layer MLP using
ReLU and LayerNorm (Ba et al., 2016).
KKKK
Oooo
4 3 2 1
SEe-Jedqee-1 ∙Lun
0K
5	10	15	20	25	30
|z|
(a)
Model Meaning of |z |
MLP Layer size
LSTM Size of hidden and cell states
GTrXL Size of the attention head and
Partially Observable Cartpole Our first experiment
evaluates memory in the control domain. We use a
partially observable form of cartpole-v0, where the ob-
servation corresponds to positions rather than velocities
(Fig. 5a). We optimize our policy using proximal policy
optimization (PPO) Schulman et al. (2017). The equa-
tions of motion for the cartpole system are a set of second-
order differential equations containing the position, ve-
locity, and acceleration of the system (Barto et al., 1983).
Using this information, we use GCM with temporal pri-
ors, i.e., N(ot) = {t-1, t-2} (Tab. 1) and present results
in Fig. 6.
position-wise MLP
DNC LSTM size, word width, and num-
ber of memory cells
GCM Size of the graph layers
GCML Size of graph layers and φθ layers
(b)
Figure 4: (a) The number of trainable pa-
rameters per memory model, based on the
hidden size |z |. GCM uses much fewer pa-
rameters than other memory models. (b) The
meaning of |z | with respect to each memory
model, as used in all our experiments.
2The MLP, LSTM, DNC, and GTrXL are standard Ray RLlib (Liang et al., 2018) implementations written
in Pytorch (Paszke et al., 2019). We implement GCM using Pytorch Geometric (Fey & Lenssen, 2019), and
integrate it into RLlib.
5
Under review as a conference paper at ICLR 2022
(a) Partially obs. cartpole
012345
⑸口⑸；口 0
Pointer
J flip →
A
P ： [0,3]
f:[1,5]
at-1 :J
ot
(c) Habitat 3D simulation
(b) Concentration card game
T-«•	L X τ∙ 1 ∙	Γ*	♦	/ / 、El 1	♦	/ F	/F	1 1	1/1	∙ Λ	1*11
Figure 5: Visualizations of our experiments. (a) The classic cartpole control problem, but where r, θ are hidden.
(b) An example state from the long-term non-sequential recall environment with six cards. The observation
space ot contains the value and index of pointer card p and last flipped card f, as well as previous action at-1.
(c) The top-down view of the 3D scene used in our navigation experiment.
Memory Module
MLP
LSTM
GTrXL
DNC
GCM
GCML
Figure 6: Stateless cartpole, where the agent must derive velocity from past observations. OpenAI considers
fully-observable cartpole solved at a reward of 195 (dashed red line), which only GCM can reliably reach in
partially-observable cartpole. Results represent the mean and 90% confidence interval over three trials.
Concentration Card Game Our next experiment evaluates non-sequential and long-term recall
with the concentration card game.3 Unlike reactionary cartpole, this experiment tests memorization
and recall over longer time periods. We vary the number of cards n ∈ {8, 10, 12} with episodes
lengths of 50, 75, 100 respectively. All models have |z| = 32 and train using PPO. We use GCM
with temporal priors for short-term memory and an additional value identity prior between the face-
up card and the card at the pointer, using function V : Ω → N:
N(ot) = {t- 1,t- 2} ∪ {i|v(ot) = v(oi)}.	(15)
In other words, when GCM flips a card face up, it recalls if it has seen that card in the past. We
present the results in Fig. 7.
Navigation The final experiment evaluates spatial reasoning with a navigation task. We use the
Habitat simulator with the validation scene from the 2020 Habitat Challenge (Fig. 5c). We train
for 10M timesteps using IMPALA (Espeholt et al., 2018), examining z ∈ {8, 16, 32} across all
models. Fig. 8 is an ablation study across multiple topological priors and GCM setups. We evaluate
the effectiveness of empty, dense, temporal, spatial, and learned priors (formally defined in Tab. 1).
We also examine whether the larger receptive field induced by the second-degree neighborhood is
helpful with the FlatLocal and FlatGlobal entries. The FlatLocal GCM uses the spatial prior and
replaces the second GNN layer with a fully-connected layer, restricting GCM to its first-degree
neighborhood. The FlatGlobal GCM is the FlatLocal GCM, but with an increased-distance spatial
prior such that the first-degree neighborhood includes the first and second-degree neighborhoods of
the spatial entry. Fig. 9 compares GCM with learned and spatial topological priors against other
memory baselines.
3Rules for concentration are available at: https://en.wikipedia.org/wiki/Concentration
(card_game)
6
Under review as a conference paper at ICLR 2022
n = 8	n = 10	n = 12
lpsω≡e∖l-3d p-eM3H uea
Memory Module
MLP
^tm LSTM
GTrXL
DNC
^^M GCM
^^M GCML
Figure 7: Results from concentration with hidden size |z| = 32, where n is the number of cards. This tests
the agent’s long-term non-sequential memory. The agent receives a small reward for matching a pair of cards,
receiving a cumulative reward of one for matching all pairs. Episode lengths are 50, 75, and 100 respectively.
Results are averaged over three trials and the shaded area represents the 90% confidence interval.
0.0
0.2	0.4	0.6	0.8	1.0 0.0
Training Timestep	×107
0.2	0.4	0.6	0.8	1.0 0.0
Training Timestep ×107
GCM Prior
None None
D^^e Dense
τ^mp, Temporal
s^^i* Spatial
L^n Learned (GCML)
FlatLocal
FlatGlobal
0.2	0.4	0.6	0.8	1.0
Training Timestep ×107
Figure 8: We compare various GCM priors across hidden sizes |z | for the navigation problem. Since navigation
is a spatial problem, the spatial prior performs best. This shows the importance of selecting good priors. Results
are averaged over three trials and the shaded area represents the 90% confidence interval.
P-IeMa= ueəz
0.50
W 0.45
m
,≡ 0.40
s
；0.35
0.30
0.25
0.20
0.15
0.0	0.2	0.4	0.6	0.8	1.0 0.0	0.2	0.4	0.6	0.8	1.0 0.0	0.2	0.4	0.6	0.8	1.0
Training Timestep	×107	Training Timestep	×107	Training Timestep	×107
Memory Module
MLP
LSTM
GTrXL
DNC
^^M GCM
^^M GCML
Figure 9: We compare GCM to other memory baselines for the navigation problem. |z | denotes the hidden size
used across all models. Results are averaged over three trials and the shaded area represents the 90% confidence
interval.
7
Under review as a conference paper at ICLR 2022
4	Discussion
The versatility of GCM compared to other models stems from its representation of experiences as a
graph. This allows it to access specific observations from the past, bypassing the limited temporal
range of LSTM. By using a multilayer GNN to reason over this graph of experiences, GCM can
build embeddings hierarchically, unlike transformers. The importance of hierarchical reasoning is
demonstrated experimentally in Fig. 8, where the GCM outperforms the FlatGlobal GCM, which
merges the first and second-degree neighborhood from the spatial prior into a first-degree neigh-
borhood. To build some intuition about why this is the case, consider an observation graph in a
navigation experiment. Reasoning over this structure hierarchically can break down the task into
manageable subtasks. The first GNN layer can fuse neighborhood viewpoints to represent local sur-
roundings, and the second layer can combine its neighborhood of local surroundings into regions for
planning. On the other hand, in a flat representation, information about the relationships between
individual observations is lost. The flat model receives unstructured observations, and must learn to
differentiate nearby observations from distant ones.
Like Beck et al. (2020), we find sequence learning is much harder in RL than supervised learning.
This is particularly clear in Fig. 7, where introducing one more pair of cards decreases reward. Al-
though general memory models can learn optimal policies in theory, this was not the case given our
timescales. The LSTM performs well but does not reliably solve (i.e. reach 195 reward) stateless
cartpole, even with small 2-dimensional observation and action spaces, and a large number of inner
and outer PPO iterations (Heess et al. (2015), Fig. 6). Even though transformers significantly outper-
form LSTMs in supervised learning (Vaswani et al., 2017), their added complexity seems to hinder
them in RL, at least at single-GPU scales. The memory search space over all past observations is
huge, and determining which observations are useful greatly reduces what the memory model must
learn.
GCM’s graph structure can utilize external information about which experiences are relevant, greatly
reducing the search space. Human intuition is an incredibly useful tool that cannot be easily lever-
aged by transformers, RNNs, or MANNs. This is the key contribution of our work - a prior defined
by afew lines of code can significantly boost RL performance. GCM provides an easy way to embed
this intuition, using more general priors (Tab. 1) or task-specific priors (Sec. 2.2). For more complex
problems where human intuition falls short, GCML can learn a prior. Then, the resulting human-
interpretable graph of observations can inform the development of new, human-derived topological
priors (Appendix A).
In our experiments, we use simple environments to demonstrate how model-dependent memory
connectivity affects performance. Models like LSTM work nearly as well as GCM on problems
like cartpole where a temporal prior makes sense (Fig. 6), but the gap widens on the concentration
environment where non-temporal priors are more suitable (Fig. 7). The navigation ablation study
(Fig. 8) demonstrates how using a suboptimal topological prior can negatively impact performance
- the dense prior (a fully-connected graph) performs nearly as poorly as the empty prior (no edges
at all) in Fig. 8. Exploratory trials of combining human priors with learned priors resulted in perfor-
mance greater than GCML alone, but not as good as GCM with human priors. Future work could
evaluate these mixed priors on more complex problems over longer timescales, where human priors
are less useful.
Across all experiments, GCM with human expertise received significantly more reward than the
next best competitor. We believe that this is remarkable, considering that GCM uses notably fewer
parameters than the other models (Fig. 4). GCML performance was generally less than LSTM, and
most similar to the transformer performance across our experiments. Caveat emptor: we tackled
simple tasks using smaller models, due to our limited computational capacity. These conclusions
might not hold for those who train markedly larger models for billions of timesteps. Given more
compute and longer episodes, we suspect the transformer and possibly GCML would outperform
LSTM, like in Parisotto et al. (2019).
5	Related Work
Memory in Reinforcement Learning We classify RNNs, MANNs, transformers, and related
memory models as general memory. RNN-based architectures, such as long short-term memory
8
Under review as a conference paper at ICLR 2022
(LSTM) (Hochreiter & Schmidhuber, 1997) and the gated recurrent unit (GRU) (Chung et al., 2014)
are used heavily in RL to solve POMDP tasks (Oh et al., 2016; Mnih et al., 2016; Mirowski et al.,
2017). RNNs update a recurrent state by combining an incoming observation with the previous re-
current state. Compared to transformers and similar methods, RNNs fail to retain information over
longer episodes due to vanishing gradients (Li et al., 2018b). By connecting relevant experiences
directly and doing a single forward pass, GCM sidesteps the vanishing gradient issue.
MANNs address limited temporal range of RNNs (Graves et al., 2014). Unlike RNNs, MANNs have
addressable external memory. The differentiable neural computer (Graves et al., 2016) (DNC) is a
fully-differentiable general-purpose computer that coined the term MANN. In the DNC, a RNN-
based memory controller uses content-based addressing to read and write to specific memory ad-
dresses. The MERLIN MANN (Wayne et al., 2018) outperformed DNCs on navigation tasks. The
implementations of the MANNs are much more complex than RNNs. In contrast to transformers or
RNNs, MANNs are much slower to train, and benefit from more compute.
The transformer is the most ubiquitous implementation of self-attention (Vaswani et al., 2017). Un-
til the gated transformer XL (GTrXL), transformers had mixed results in RL due to their brittle
training requirements (Mishra et al., 2018). The GTrXL outperforms MERLIN, and by extension,
DNCs in Parisotto et al. (2019). When learning topological priors, our GCML borrows concepts
from transformers such as positional encodings. The self-attention module in a transformer can be
implemented using a single graph attention layer over a fully-connected graph (Joshi, 2020). Unlike
self-attention in transformers, GCML is hard, sparse, and hierarchical. GCM does not use attention
weights, nor a fully-connected graph.
Similar to our work, Savinov et al. (2018) build an observation graph, but specifically for navigation
tasks, and do not use GNNs. Wu et al. (2019) use a probabilistic graphical model to represent spatial
locations during indoor navigation. Eysenbach et al. (2019); Emmons et al. (2020) build a state-
transition graph similar to our observation graph for model-based RL, but use A* and other methods
to evaluate the graph.
Graph Neural Networks GNNs are most easily understood using a message-passing scheme
(Gilmer et al., 2017), where each vertex in a graph sends and receives latent messages from its
neighborhood. Each layer in the GNN learns to aggregate incoming messages into a hidden repre-
sentation, which is then shared with the neighborhood. Convolutional graph neural networks (Kipf
& Welling, 2017) are a subcategory of GNNs and a generalization of convolutional neural networks
(CNNs) to the graph domain. Convolutional GNNs tend to be efficient in both the computational and
parameter sense due to their use of sliding filters and reliance on batched sums and matrix multiplies.
Our method shares some similarities with graph attention networks (Velickovic et al., 2018), namely
the pairwise-vertex MLP to compute edge significance. Graph RNNs (Ruiz et al., 2020) are a gen-
eralization of RNNs to graph inputs with a fixed number of time-varying vertices, and tackle an
entirely different problem than GCM. Chen et al. (2019); Li et al. (2019); Chen et al. (2020) apply
GNNs to RL for task-specific problems. Beck et al. (2020) implement feature aggregation for RL
in a similar fashion to GNNs. The aggregated memory operator by Zweig et al. (2020) combines a
GNN with a RNN to navigate a graph of states using reinforcement learning. To date, GCM is the
only task-agnostic RL memory model to utilize GNNs.
6	Conclusion
GCM provides a framework to embed task-specific priors into memory, without writing task-specific
memory from the ground up. Embedding custom toplogical priors in the graph is trivial: it involves
implementing a boolean function that determines whether or not observation oi is useful for decision
making at time t. This simple feature makes GCM very versatile. Moreover, with GCML we showed
that GCM can also be purposed to learn topological priors without human input, and in this case
performs similarly to a transformer. When even basic domain knowledge is available (e.g., when the
problem is spatial, or when it follows Newton’s laws) GCM outperforms transformers, LSTMs, and
DNCs, while using significantly fewer parameters.
9
Under review as a conference paper at ICLR 2022
7	Ethics Statement
GCM is entirely open-source and free to use for your own purposes, we just ask you cite our work if
it was helpful to you, and that you not use it in any application that harms others. This research was
funded via public and private grants for application in intelligent navigation, multiagent RL, and
GNN-based systems. It is possible GCM could be used to solve RL tasks for nefarious purposes.
8	Reproducability S tatement
The most up-to-date version of GCM as well as all edge selectors are publicly available on GitHub.
Please follow the GitHub README.md for how to install and use GCM.
Please see README.md in the supplemental material for how to replicate our precise experimental
setup using Docker, and how to rerun the experiments. We use the Ray RLlib open-source imple-
mentations of PPO and IMPALA, and provide all hyperparameters in Appendix D. All experimental
trials were run three times, and we represent the mean and 90% confidence interval for all exper-
iments. Our experiments are non-deterministic due to asynchronous GPU operations, but we took
great care to select hyperparameters that result in consistent and reproducible reward curves, as evi-
dent in the relatively low-variance confidence interval in our results. Environmental details, such as
observation and action spaces are explained in Appendix C.
10
Under review as a conference paper at ICLR 2022
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. jul
2016. URL https://arxiv.org/abs/1607.06450v1http://arxiv.org/abs/
1607.06450.
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike Adaptive Elements
That Can Solve Difficult Learning Control Problems. IEEE Transactions on Systems, Man and
Cybernetics, SMC-13(5):834-846,1983. doi:10.1109/TSMC.1983.6313077.
Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, and Katja Hofmann.
AMRL: Aggregated Memory For Reinforcement Learning. International Conference on Learning
Representations (ICLR), pp. 1-14, 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. jun 2016.
Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. Acting optimally in par-
tially observable stochastic domains. In Proceedings of the National Conference on Artificial
Intelligence, volume 2, pp. 1023-1028, 1994.
Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva,
Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor
environments. In Proceedings - 2017 International Conference on 3D Vision, 3DV 2017, 2018.
doi: 10.1109/3DV.2017.00081.
Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdi-
nov. Learning to Explore using Active Neural SLAM. In International Conference on Learning
Representations (ICLR). arXiv, apr 2020.
Fanfei Chen, John D. Martin, Yewei Huang, Jinkun Wang, and Brendan Englot. Autonomous Ex-
ploration Under Uncertainty via Deep Reinforcement Learning on Graphs. In IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS). arXiv, jul 2020.
Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, Marynel Vazquez,
and Silvio Savarese. A behavioral approach to visual navigation with graph localization networks.
In Proceedings of Robotics: Science and Systems, FreiburgimBreisgau, 2019. doi: 10.15607/rss.
2019.xv.010.
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep
Learning, December 2014, 2014.
Scott Emmons, Ajay Jain, Michael Laskin, Thanard Kurutach, Pieter Abbeel, and Deepak Pathak.
Sparse graphical memory for robust planning. In Advances in Neural Information Processing
Systems, volume 2020-Decem, 2020.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Boron
Yotam, Firoiu Vlad, Harley Tim, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. In 35th
International Conference on Machine Learning, ICML 2018, volume 4, 2018.
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridg-
ing planning and reinforcement learning. In Advances in Neural Information Processing Systems,
volume 32, 2019.
Matthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with PyTorch Geometric.
arXiv, 2019. URL http://arxiv.org/abs/1903.02428.
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In 34th International Conference on Machine Learning,
ICML 2017, volume 3, pp. 2053-2070. PMLR, jul 2017. ISBN 9781510855144. URL https:
//proceedings.mlr.press/v70/gilmer17a.html.
11
Under review as a conference paper at ICLR 2022
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. oct 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adria PUigdomenech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):
471-476, oct 2016. doi： 10.1038∕nature20101.
Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive
mapping and planning for visual navigation. In Proceedings - 30th IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, volume 2017-Janua, pp. 7272-7281. Institute of
Electrical and Electronics Engineers Inc., nov 2017. doi: 10.1109/CVPR.2017.769.
Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver. Memory-based con-
trol with recurrent neural networks. dec 2015. URL https://arxiv.org/abs/1512.
04455v1http://arxiv.org/abs/1512.04455.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Zhewei Huang, Shuchang Zhou, and Wen Heng. Learning to paint with model-based deep reinforce-
ment learning. In Proceedings of the IEEE International Conference on Computer Vision, volume
2019-Octob, pp. 8708-8717, 2019. ISBN 9781728148038. doi: 10.1109/ICCV.2019.00880.
Sarthak Jain and Byron C. Wallace. Attention is not explanation. In NAACL HLT 2019 - 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies - Proceedings of the Conference, volume 1, pp. 3543-3556, 2019.
ISBN 9781950737130. doi: 10.18653/v1/N19-1357.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical Reparameterization with Gumbel-Softmax. 5th
International Conference on Learning Representations, ICLR 2017 - Conference Track Proceed-
ings, nov 2016. URL http://arxiv.org/abs/1611.01144.
Chaitanya Joshi. Transformers are Graph Neural Networks, feb 2020. URL https://
graphdeeplearning.github.io/post/transformers-are-gnns/.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101(1-2):99-134, 1998. ISSN
00043702. doi: 10.1016/s0004-3702(98)00023-x.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings, 2014.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017 - Conference
Track Proceedings, 2017.
Daniel Lenton, Stephen James, Ronald Clark, and Andrew J. Davison. End-to-End Egospheric
Spatial Memory. In International Conference on Learning Representations, sep 2021. URL
http://arxiv.org/abs/2102.07764.
Dong Li, Qichao Zhang, Dongbin Zhao, Yuzheng Zhuang, Bin Wang, Wulong Liu, Rasul Tutunov,
and Jun Wang. Graph attention memory for visual navigation, may 2019.
Luchen Li, Matthieu Komorowski, and Aldo A. Faisal. The Actor Search Tree Critic (ASTC) for
Off-Policy POMDP Learning in Medical Decision Making. arXiv preprint arXiv:1805.11548,
2018a. URL http://arxiv.org/abs/1805.11548.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently Recurrent Neural
Network (IndRNN): Building A Longer and Deeper RNN. In Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, pp. 5457-5466, 2018b. ISBN
9781538664209. doi: 10.1109/CVPR.2018.00572.
12
Under review as a conference paper at ICLR 2022
Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E.
Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement
learning. In 35th International Conference on Machine Learning, ICML 2018, volume 7, pp.
4768-4780, 2018.
Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J. Ballard, Andrea Banino,
Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and Raia
Hadsell. Learning to navigate in complex environments. In 5th International Conference on
Learning Representations, ICLR 2017 - Conference Track Proceedings, 2017.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In 6th International Conference on Learning Representations, ICLR 2018 - Conference
Track Proceedings, 2018.
Volodymyr Mnih, Adria Puigdomenech Badia, Lehdi Mirza, Alex Graves, Tim Harley, Timothy P.
Lillicrap, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In 33rd International Conference on Machine Learning, ICML 2016, volume 4, pp.
2850-2869, 2016. ISBN 9781510829008.
Steven D. Morad, Roberto Mecca, Rudra P.K. Poudel, Stephan Liwicki, and Roberto Cipolla. Em-
bodied Visual Navigation with Automatic Curriculum Learning in Real Environments. IEEE
Robotics and Automation Letters, 6(2):683-690, apr 2021. doi: 10.1109/LRA.2020.3048662.
Pol Moreno, Jan Humplik, George Papamakarios, Bernardo Avila Pires, Lars Buesing, Nicolas
Heess, and Theophane Weber. Neural belief states for partially observed domains. In NeurIPS
2018 workshop on Reinforcement Learning under Partial Observability, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In 33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications
of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2019, pp. 4602-4609, 2019. doi: 10.1609/aaai.v33i01.
33014602.
Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of Memory, Ac-
tive Perception, and Action in Minecraft. Technical report, jun 2016.
Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement
learning. In 6th International Conference on Learning Representations, ICLR 2018 - Conference
Track Proceedings, Vancouver, feb 2017. arXiv.
Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M.
Jayakumar, Max Jaderberg, Raphael LoPez Kaufman, Aidan Clark, Seb Noury, Matthew M.
Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learn-
ing, oct 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Ed-
ward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems, volume 32, 2019.
Luana Ruiz, Fernando Gama, and Alejandro Ribeiro. Gated Graph Recurrent Neural Networks.
IEEE Transactions on Signal Processing, 68:6303-6318, 2020. ISSN 19410476. doi: 10.1109/
TSP.2020.3033962.
Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory
for navigation. In 6th International Conference on Learning Representations, ICLR 2018 - Con-
ference Track Proceedings, 2018.
Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat:
A platform for embodied AI research. In Proceedings of the IEEE International Conference on
Computer Vision, volume 2019-Octob, 2019. doi: 10.1109/ICCV.2019.00943.
13
Under review as a conference paper at ICLR 2022
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LUkasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, volume 2017-Decem, pp. 5999-6009, 2017.
Petar Velickovic, Arantxa Casanova, Pietro Lio, Guillem Cucurull, Adriana Romero, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018 - Conference Track Proceedings, 2018.
Greg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-
Barwinska, Jack Rae, Piotr Mirowski, Joel Z. Leibo, Adam Santoro, Mevlana Gemici, Malcolm
Reynolds, Tim Harley, Josh Abramson, Shakir Mohamed, Danilo Rezende, David Saxton, Adam
Cain, Chloe Hillier, David Silver, Koray Kavukcuoglu, Matt Botvinick, Demis Hassabis, and
Timothy Lillicrap. Unsupervised Predictive Memory in a Goal-Directed Agent. arXiv, mar 2018.
Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, and Yuandong Tian. Bayesian
relational memory for semantic visual navigation. In Proceedings of the IEEE International Con-
ference on Computer Vision, volume 2019-Octob, pp. 2769-2779. Institute of Electrical and Elec-
tronics Engineers Inc., oct 2019. doi: 10.1109/ICCV.2019.00286.
Keyulu Xu, Stefanie Jegelka, Weihua Hu, and Jure Leskovec. How powerful are graph neural
networks? In 7th International Conference on Learning Representations, ICLR 2019, 2019.
Zhenpeng Zhou, Xiaocheng Li, and Richard N. Zare. Optimizing Chemical Reactions with Deep
Reinforcement Learning. ACS Central Science, 3(12):1337-1344, 2017. ISSN 23747951. doi:
10.1021/acscentsci.7b00492.
Aaron Zweig, Nesreen K Ahmed, Ted Willke, and Guixiang Ma. Neural Algorithms for Graph
Navigation. In Learning Meets Combinatorial Algorithms at NeurIPS2020, pp. 1-11, oct 2020.
URL https://openreview.net/pdf?id=sew79Me0W0c.
14
Under review as a conference paper at ICLR 2022
A Interpreting Memory Graphs
One of the strengths of GCM is that the memory is an interpretable graph. We can see precisely
which past observations contribute to the current belief. In this section, we provide an example
memory graph from the cartpole and navigation experiments using the learned topological prior
(GCML).
Figure 10: What do GCML neighborhoods look like for stateless cartpole? At each episodic timestep t > 20,
we record the neighborhood N (ot) GCML produces relative to t. We plot the accumulation of all neighbor-
hoods over an episode, using |z| = 32, K = 5. We see that GCML learns a temporal prior, where each timestep
uses observations from 7 to 10 timesteps ago. Surprisingly, GCML does not use the preceding observation, sug-
gesting that the simulation contains high-frequency variations which prevent an accurate estimation of velocity
between consecutive timesteps. Perhaps a human-defined prior with {t - 7} would produce a smoothed signal,
leading to better performance than our {t - 1, t - 2} prior. With K = 5, the maximum possible neighborhood
size is 5. The mean neighborhood size is 1.54, demonstrating that GCML can learn a peaked distribution,
resulting in sparse graphs.
Navigation GCML Neighborhood
Vertex
Figure 11: What do GCML navigation neighborhoods look like? At each episodic timestep, we record the
neighborhood N (ot) GCML produces. We plot the accumulation of all neighborhoods over an episode, using
|z| = 32, K = 5. Unlike the Fig. 10, vertices here are labeled in an absolute fashion (when they occurred).
We see there are a few “key” observations (red arrows), such as the start vertex o1 or o49 , similar to the use of
keyframes in visual SLAM. Unlike stateless cartpole, information in the navigation task is spread over many
observations. With K = 5, the maximum neighborhood size is 5 and the mean neighborhood size is 4.23,
illustrating a flat distribution, and resulting in a denser graph.
15
Under review as a conference paper at ICLR 2022
Navigation GCML Neighborhood by Distance
(O)Λ∕ uωω□uajnooo EnN
120
100
80
60
40
20
0
0123456789 10
Distance Between Vertices (m)
(a)
Navigation Random Neighborhood by Distance
380
N
W
ω 60
ω
o
⊂
ω
⅛ 40
O
E 20
0
0123456789 10
Distance Between Vertices (m)
(b)
Figure 12: What do GCML navigation neighborhoods look like in the spatial domain? (a) At each episodic
timestep, we record the neighborhood N(ot) GCML produces. We plot the accumulation of all neighborhoods
over an episode, using |z| = 32, K = 5. We bin each vertex in the neighborhood N (ot) by its distance to the
vertex ot . We see GCML looks to be spatially-biased, heavily prioritizing nearby vertices over further vertices.
(b) We shuffle the edge indices from a to see if the task itself is biased towards shorter distances (e.g. maybe
agent spends lots of time in a single room 4m2 in size, and all vertices are within 4m). We find this is not the
case, and that the vertices span a large distance. a and b together prove that GCML indeed learns a spatial prior.
16
Under review as a conference paper at ICLR 2022
(a)
(b)	(c)
Figure 13: (a) A visual representation of the memory graph from the episode used in Fig. 11 and Fig. 12.
Vertices are labeled by timestep and projected into a 2D-plane based on their physical location. Overlapping
vertices (e.g. when the agent rotates in place) are not shown. Here, we find clues for why o49 was so heavily
favored in Fig. 11. We reconstruct the o49 depth image and find it was precisely when the agent left a narrow
corridor (b) to enter a large living room (c). As it explores the living room (o50 - o94), it localizes itself
relative o49, using past information to exit through a different door than it entered (o95), maximizing exploration
reward. We trace the belief at o95 through first-degree neighbor o56 to second-degree neighbor o49 in red. This
introspection leads us to believe we should pay attention to doorways when designing topological priors for
navigation.
17
Under review as a conference paper at ICLR 2022
B	Topological Prior Examples
To demonstrate how simple it is to write topological priors, we provide two examples of task specific
priors in Pytorch. First, imagine a hospital agent where key observations occur when the medicines
administered to the patient change. This could be implemented as such
import torch
class MedicalPrior(torch.nn.Module):
def forward(self, V):
# V is V_t (contains o_t)
meds = get_meds_from_observation(V[:-1])
N = torch.where(meds != meds.roll(1,0))
return N
where get_meds_from_observation slices a tensor of observations to extract the medications
administered at each timestep. Next, assume we are learning a dynamics model for an aircraft in
flight. We want to exclude all past observations after a bird strike takes out an engine, and learn a
new reduced-capability dynamics model
import torch
class AircraftPrior(torch.nn.Module):
max_deviation = 8 # meters per second squared
def forward(self, V):
errors = get_dyn_model_error(V[:-1]) # Obs err using dyn model
latest_damage_event = (errors > max_deviation).nonzero()[-1,0]
N = torch.range(latest_damage_event, V.shape[0])
return N
where get_dyn_model_error returns the acceleration disparity between a learned dynamics
model and sensor measurements for each timestep.
18
Under review as a conference paper at ICLR 2022
C Environmental Details
C.1 Stateless Cartpole
We do not change any of the default environment settings from the OpenAI defaults: we use an
episode length of 200, with a reward of 1 for each timestep survived. The episode ends when the
pole angle is greater than 12 degrees or the cart position is further than 2.4m from the origin. Actions
correspond to a constant force in either the left or right directions. We use the OpenAI gym definition
(mean reward of 195) Brockman et al. (2016) to determine if the agent is successful. The value loss
coefficient hyperparameter was used from the RLlib cartpole example, and thus differs from the
non-cartpole default.
C.2 Concentration Game
The agent is given n/2 pairs of shuffled face-down cards, and must flip two cards face up. If the
cards match, they remain face up, otherwise they are turned back over again. Once the player has
matched all the cards, the game ends. We model the game of memory using a pointer, which the
player moves to read and flip cards (Fig. 5b). The observation space consists of the pointer (card
index and card value) and the last flipped (if any) face-up card. Cards are represented as one-hot
vectors. The agent receives a reward for each pair it matches, with a cumulative reward of one for
matching all the cards.
C.3 Navigation
The navigation experiment operated on the CVPR Habitat 2020 challenge (Savva et al., 2019) val-
idation scene from the MP3D dataset (Chang et al., 2018). We used the same list of agent start
coordinates as used during the challenge. 32 × 32 depth images with range [0.5, 5m] and a 79 de-
gree field of view were compressed into a 64-dimensional latent representations using a 6 layer (3
encoder, 3 decoder) convolutional β-VAE Kingma & Welling (2014) with β = 0.01, ReLU activa-
tion, and batch normalization. The β-VAE was pretrained until convergence using random actions.
The full observations consists of agent heading and coordinates relative to the current start location,
the previous action, and the latent VAE representation. The agent can rotate 30 degrees in either
direction or move 0.25m forward. The agent receives a reward of 0.01 for exploring a new area of
radius 0.20m.
19
Under review as a conference paper at ICLR 2022
D Experiment Hyperparameters
The following hyperparameters used for each experiment across all memory modules. We generally
reduce the learning rate as well as increase the batch size to produce more consistent reward curves.
Nearly all other hyperparameters are Ray RLlib defaults.
Term	Value	RLlib Default
Navigation IMPALA		
Decay factor γ	0.99	X
Value function loss coef.	0.5	X
Entropy loss coef.	0.001	0.01
Gradient clipping	40	X
Learning rate	0.005	X
Num. SGD iters	1	X
Experience replay ratio	1:1	0:1
Batch size	1024	500
GAE λ	1.0	X
V-trace ρ	1.0	X
Cartpole PPO
Decay factor γ	0.99	X
Value function loss coef.	1e-5	X(cartpole-specific)
Entropy loss coef.	0.0	X
Value function clipping	10.0	X
KL target	0.01	X
KL coefficient	0.2	X
PPO clipping	0.3	X
Value clipping	0.3	X
Learning rate	5e-5	X
Num. SGD iters	30	X
Batch size	4000	X
Minibatch size	128	X
GAE λ	1.0	X
Concentration PPO
Decay factor γ	0.99	X
Value function loss coef.	1.0	X
Entropy loss coef.	0.0	X
Value function clipping	10.0	X
KL target	0.01	X
KL coefficient	0.2	X
PPO clipping	0.3	X
Value clipping	0.3	X
Learning rate	3e-4	5e-5
Num. SGD iters	30	X
Batch size	4000	X
Minibatch size	4000	128
GAE λ	1.0	X
20