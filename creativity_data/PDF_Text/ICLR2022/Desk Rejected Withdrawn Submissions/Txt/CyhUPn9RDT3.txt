Under review as a conference paper at ICLR 2022
IQNAS: Interpretable Integer Quadratic
programming Neural Architecture Search
Anonymous authors
Paper under double-blind review
Abstract
Realistic use of neural networks often requires adhering to multiple con-
straints on latency, energy and memory among others. A popular approach
to find fitting networks is through constrained Neural Architecture Search
(NAS). However, previous methods use complicated predictors for the accu-
racy of the network. Those predictors are hard to interpret and sensitive
to many hyperparameters to be tuned, hence, the resulting accuracy of the
generated models is often harmed. In this work we resolve this by introduc-
ing Interpretable Integer Quadratic programming Neural Architecture Search
(IQNAS), that is based on an accurate and simple quadratic formulation of
both the accuracy predictor and the expected resource requirement, together
with a scalable search method with theoretical guarantees. The simplicity of
our proposed predictor together with the intuitive way it is constructed bring
interpretability through many insights about the contribution of different
design choices. For example, we find that in the examined search space,
adding depth and width is more effective at deeper stages of the network
and at the beginning of each resolution stage. Our experiments1 show that
IQNAS generates comparable to or better architectures than other state-
of-the-art NAS methods within a reduced search cost for each additional
generated network, while strictly satisfying the resource constraints.
1	Introduction
With the rise in popularity of Convolutional Neural Networks (CNN), the need for neural
networks with fast inference speed and high accuracy, has been growing continuously. At first,
manually designed architectures, such as VGG Simonyan & Zisserman (2015) or ResNet He
et al. (2015), targeted powerful GPUs as those were the common computing platform for deep
CNNs, until the need for deployment on edge devices and standard CPUs emerged. These are
more limited computing platforms, requiring lighter architectures that for practical scenarios
must comply with hard constraints on real time latency or power consumption. This has
spawned a line of research aimed at finding architectures with both high performance and
bounded resource demands.
The main approaches to solve this evolved from Neural Architecture Search (NAS) Zoph & Le
(2016); Liu et al. (2018); Cai et al. (2018), while adding a constraint on the target latency over
various platforms, e.g., TPU, CPU, Edge-GPU, FPGA, etc. The constrained-NAS methods
can be grouped into two categories: (i) Reward based methods such as Reinforcement-
Learning (RL) or Evolutionary Algorithm (EA) Cai et al. (2019); Tan et al. (2019); Tan &
Le (2019); Howard et al. (2019), where the search is performed by sampling networks and
predicting their final accuracy and resource demands by evaluation over some validation
set. The predictors are typically made of complicated models and hence require many
samples and sophisticated fitting techniques White et al. (2021). Overall this makes those
oftentimes inaccurate, expensive to acquire, and hard to optimize objective functions due
to their complexity. (ii) Resource-aware gradient based methods formulate a differentiable
loss function consisting of a trade-off between an accuracy term and either a proxy soft
penalty term Hu et al. (2020); Wu et al. (2019) or a hard constraint Nayman et al. (2021).
Therefore, the architecture can be directly optimized via bi-level optimization using stochastic
gradient descent (SGD) Bottou (1998) or stochastic Frank-Wolfe (SFW) Hazan & Luo (2016).
1 The full code for search, train and evaluation with trained models will be publicly released.
1
Under review as a conference paper at ICLR 2022
However, the bi-level nature of the problem introduces many challenges Chen et al. (2019);
Liang et al. (2019); Noy et al. (2020); Nayman et al. (2019) and recently Wang et al. (2021)
pointed out the inconsistencies associated with using gradient information as a proxy for the
quality of the architectures, especially in the presence of skip connections in the search space.
This kind of inconsistencies also calls for making NAS more interpretable by extending its
scope from finding optimal architectures to interpretable features Ru et al. (2021) and their
corresponding impact on the network performance.
In this paper, we propose a fast and scalable search algorithm that produces architectures
with high accuracy that strictly satisfy latency constraints. The proposed algorithm is based
on two key ideas, with the goal of constructing an intuitive and simple accuracy predictor
which is interpretable, easy to optimize and without strong reliance on gradient information:
(1) We propose a simple and well preforming quadratic accuracy predictor which is in-
terpretable, easy to optimize and intuitive by measuring the performance contribution
of individual design choices thorough sampling selected sub-networks from an one-shot
model (Bender et al. (2018); Chu et al. (2019); Guo et al. (2020); Cai et al. (2019); Nay-
man et al. (2021)). Many insights about the contribution of various design choices can
be extracted due to the way the predictor is constructed. Moreover, we show that the
performance of our intuitive and simple predictor is comparable to the performance of its
learnable version. Furthermore, its performance matches other more complex predictors,
that are not interpretable and expensive and hard to optimize due to many hyperparameters.
(2) The quadratic form of the proposed predictor allows the formulation of the latency
constrained NAS problem as an Integer Quadratic Constrained Quadratic Programming
(IQCQP). Thanks to this, it can be efficiently solved via a simple algorithm with some
off-the-shelf components.
The optimization approach we propose has several advantages. First, the outcome networks
provide high accuracy and closely comply with the latency constraint. In addition, our
solution is highly efficient, which makes it scalable to multiple target devices and latency
demands. The efficiency is due to the formulation of the problem as an IQCQP and the
deigned algorithm that solves it within few minutes on a common CPU.
2	Related Work
Neural Architecture Search methods automate models’ design per provided constraints.
Early methods like NASNet Zoph & Le (2016) and AmoebaNet Real et al. (2019) focused
solely on accuracy, producing SotA classification models Huang et al. (2019) at the cost
of GPU-years per search, with relatively large inference times. DARTS Liu et al. (2018)
introduced a differential space for efficient search and reduced the training duration to
days, followed by XNAS Nayman et al. (2019) and ASAP Noy et al. (2020) that applied
pruning-during-search techniques to further reduce it to hours.
Predictor based methods recently have been proposed based on training a model to predict
the accuracy of an architecture just from an encoding of the architecture. Popular choices for
these models include Gaussian processes, neural networks, tree-based methods. See Lu et al.
(2020) for such utilization and White et al. (2021) for comprehensive survey and comparisons.
Interpretabe NAS was firstly introduced by Ru et al. (2021) through a rather elaborated
Bayesian optimisation with Weisfeiler-Lehman kernel to identify beneficial topological features.
We propose an intuitive and simpler approach for NAS interpretibiliy for the efficient search
space examined. This leads to more understanding and applicable design rules.
Hardware-aware methods such as ProxylessNAS Cai et al. (2018), Mnasnet Tan et al. (2019),
FBNet Wu et al. (2019), SPNASNet Stamoulis et al. (2019) and TFNAS Hu et al. (2020)
produce architectures that satisfy the required constraints by applying simple heuristics such
as soft penalties on the loss function. HardCoRe-NAS Nayman et al. (2021) and OFA Cai
et al. (2019) proposed a scalable approach across multiple devices by training an one-shot
model Brock et al. (2017); Bender et al. (2018) once. This provides a strong pretrained
super-network being highly predictive for the accuracy ranking of extracted sub-networks,
e.g. SPOS Guo et al. (2020), FairNAS Guo et al. (2020). HardCore-NAS searches by
backpropagation Kelley (1960) over a supernetwork under strict latency constraints for
several GPU hours per network. OFA applies evolutionary search Real et al. (2019) over a
2
Under review as a conference paper at ICLR 2022
complicated multilayer perceptron (MLP) Rumelhart et al. (1985) based accuracy predictor
with many hyperparameters to be tuned. This work relies on such one-shot model, for
intuitively building a simple quadratic accuracy predictor that matches in performance
without any tuning and optimized under strict latency constraints by solving an IQCQP
problem in several CPU minutes.
3	Method
In this section we propose our method for latency-constrained NAS. We search for an
architecture with the highest validation accuracy under a predefined latency constraint,
denoted by T. Our architecture search space S is parametrized by a vector Z ∈ S ⊂ NN,
governing the architecture structure, and w, the convolution weights. We show in Section 3.1
that the latency-constrained NAS problem can be formulated as an IQCQP:
max ACC(Z) = qTZ + ZTQZ	s.t. LAT(Z) = ZTΘZ ≤ T, AS ∙ Z ≤ bS,	Z ∈ Nn ⑴
where q ∈ RN, Q ∈ RN×N,Θ ∈ RN×N, AS ∈ RC×N, bS ∈ RC and ζ ∈ S can be expressed
as a set of C linear equations . We define the accuracy predictor ACC(Z) in section 3.2 and
present the quadratic formula for the latency computation LAT(Z) in section 3.2.1. Finally,
in section 3.3 we propose an optimization method to efficiently solve equation 1.
3.1 Search Space
Aiming at latency efficient architectures, we
adopt the search space introduced in Nay-
man et al. (2021) and illustrated in Figure 1,
which is closely related to those used by Wu
et al. (2019); Howard et al. (2019); Tan et al.
(2019); Hu et al. (2020); Cai et al. (2019).
It integrates a macro search space and a mi-
cro search space. The macro search space is
composed of S stages s ∈ {1, .., S = 5}, each
composed of blocks b ∈ {1, .., D = 4}, and
defines how the blocks are connected to one
another. The micro search space is based
on Mobilenet Inverted Residual (MBInvRes)
blocks Sandler et al. (2018) and controls the
on- + -oo^w><
(9u⅛AUoOIXl
Figure 1: Search space via the one-shot model
internal structures of each block.
Every MBInvRes block is configured by an expansion ratio er ∈ {3, 4, 6} of the point-wise
convolution, kernel size k ∈ {3 × 3, 5 × 5} of the Depth-Wise Separable convolution (DWS),
and Squeeze-and-Excitation (SE) layer Hu et al. (2018) se ∈ {on, off} as shown at the bottom
of Figure 1 and detailed in Appendix B. Each triplet (er, k, se) implies a block configuration
C ∈ C (specified in Appendix B) that resides in a micro search space that is parameterized
by α ∈A = 0 s=^γ 0 D=ι 0 c ∈C as,c, where 0 denotes the Cartesian product.

For each block b of stage s we have αbs,c ∈ {0, 1}|C| and Σc∈C αbs,c = 1. An input feature
map Xs to block b of stage S is processed as follows: Xs+]=TC∈c aS c ∙ O,S c(Xs), where
OsCq) is the operation performed by the block configured according to C = (er, k, se). The
output of each block of every stage is also directed to the end of the stage as illustrated
in the center of Figure 1. Thus, the depth of each stage s is controlled by the parameters
β ∈ B = 0S=ι 0D=ι βb, such that βb ∈ {0, 1}D and ΣD=IβS = 1. The depth is ds ∈
{b | βb = 1 ,b ∈ {1,二D}}, since x；+1 = ΣD=Iβb ∙ xb+「	一
To summarize, the search space is composed of both the micro and macro search spaces
parameterized by α ∈ A and β ∈ B, respectively, such that for all s ∈ {1, .., S}, b ∈
{1, .., D}, C ∈ C:
S = {(α, β)∣α ∈ A, β ∈ B ;嘴,° ∈ {0,1}|C1 ; ΣC∈cas,c = 1; βb ∈ {0,1}D ; ΣD=IβS = 1 }
(2)
3
Under review as a conference paper at ICLR 2022
A continuous probability distribution is induced over the space, by relaxing αbs,c ∈ {0, 1}|C| →
αbs,c ∈ R|+C| and βbs ∈ {0, 1}D → βbs ∈ R+D to be continuous rather than discrete. Therefore,
this probability distribution can be expressed by a set of linear equations and one can view
the parametrization ζ = (α, β) as a composition of probabilities in Pζ (S) = {ζ | ASζ ≤
bs} = {(α, β) | AS ∙ α ≤ bS, AS ∙ β ≤ b[} or as degenerate one-hot vectors in S.
3.2	Quadratic Accuracy Predictors
While many predictor-based methods utilize complicated forms of accuracy predictors (see
White et al. (2021)) that are hard to train and interpret, we introduce a simple predictor of
a quadratic form, which can be computed efficiently while maintaining high accuracy. We
evaluate its accuracy as commonly done for predictors Chu et al. (2019); White et al. (2021),
by showing high ranking correlation between the accuracy of its predictions and the accuracy
measured on a sub-network extracted from the one-shot model. The correlation is measured
via both Kendall-Tau Maurice (1938) and Spearman Spearman (1961) coefficients.
3.2.1	Deriving a Quadratic Accuracy Estimator
We next propose an intuitive and effective way for estimating the expected accuracy of a
given sub-network. We measure the contributions ∆bs,c = E[Acc|Obs,c = Oc, ds = b] - E[Acc]
and ∆bs = E[Acc|ds = b] - E[Acc] of each individual decision and then aggregate all the
contributions while multiplying each by its probability of participation:
SD	SDD
ACC(ζ) = ACC(α, β) = E[Acc] +ΣΣβb∙δs + ΣΣΣΣOscNcβb	⑶
s =1 b =1	s =1 b =1 b = b c∈C
The expectations are with respect to a uniform sampling of sub-networks ζ ∈ S excluding
decisions specified in the conditional events, as illustrated in Figure 2. In practice, while
equation 3 is quadratic in ζ , its vectorized form can be expressed as the following bilinear
formula in α and β :
ACC(ζ) = ACC(α, β) =r+qβTβ+αTQSββ	(4)
where r = E [Acc], qβ ∈ RDS is a vector composed of ∆S and Qaβ ∈ RC- DS × DS is a matrix
composed of ∆bs,c. We define the latency constraint similarly to Nayman et al. (2021):
SDD
LAT(α,β) =∑∑∑∑Osctbc βs = αTeβ	⑸
S =1 b =1 b=b c∈C
where Θ ∈ RC-D^S×D^S is a matrix composed of the latency measurements tS c of every
possible configuration c ∈ C of every block b ∈ {1, . . . , D} in every stage s ∈ {1, . . . , S}.
Accuracy
Contribution
Measurements
max τζ + ζτ ζ
s.t. ζτΘζ ≤ T
40
Super-network The measured candidate decision is assigned with a probability of one
≡ Sampled kɔ Alternative candidate decisions are assigned with zero probability
Sub-networks^) All the rest of the candidate decisions are of uniform probability
Figure 2: (Left) The IQNAS scheme constructs a quadratic accuracy estimator by measuring
the accuracy contribution of individual design choices and then maximizing this objective
under quadratic latency constraints. (Right) Subnetworks’ accuracy predictions by the
proposed estimator vs their measured accuracy. High ranking correlations are achieved.
The expectations E[Acc], E[Acc|Obs,c = Oc, ds = b] and E[Acc|ds = b] are estimated using
Multi-path sampling Nayman et al. (2021). We average Monte-Carlo samples of distinct sub-
networks, sampled uniformly for each input image in the validation set (see Figure 2). Figure 3
4
Under review as a conference paper at ICLR 2022
compares the effectiveness of Multi-path sampling to Single-path sampling, where each batch
passes through a distinct uniformly sampled subnetwork as discussed in Section 4.2.3. This
estimation requires O(N) validation epochs.
How close is the estimation proposed to the expected accuracy of an architecture?
We next present a theorem (with proof in Appendix D) that states that the estimator
in equation 3 approximates well the expected accuracy of an architecture.
Theorem 3.1. Assume {Obs, ds} for s = 1, . . . , S and b = 1, . . . , D are conditional ly inde-
pendent with the accuracy Acc. Suppose that there exists a positive real number 0 < e《1
such that for any X ∈ {OS,ds} the following holds |P[Acc|X] — P[Acc] | < EP[Acc]. Then:
「	^ D ∏s ~∖	(SD	SDD	、
E Acc ∩S =∩ Jb =s Ob	= E[Acc]+ 工工βb ∙ ∆S + 工工工工Obc ∆Q βS,∖ (1 + O(Ne))
-	S = 1	」	∖s =1 b =1	S = 1 b =1 bb = b c∈C	)
Theorem 3.1 and Figure 2 (right) demonstrate the effectiveness of relying on ∆bs,c, ∆bs to
express the expected accuracy of networks. Since those terms measure the accuracy contri-
butions of individual design decisions, many insights and design rules can be extracted from
those, as discussed in section 4.2.2, making the proposed estimator intuitively interpretable.
Furthermore, the transitivity of ranking correlations is used in appendix H for guaranteeing
good prediction performance with respect to architectures trained from scratch.
3.2.2	Learning a Quadratic Accuracy Predictor
One can wonder whether setting the coefficients Qαβ , qβ and r of the bilinear equation 4
according to the estimates in equation 3 yields the best prediction of the accuracy of an
architecture. An alternative approach is to learn those coefficients by solving a linear
regression problem:
n
min. 工 ||r + αT。ɑ + βT。e + αTQaeβi - Acc(αi, βi)||2	(6)
r,qα ,qβ ,Q αβ i =1
where {αi, βi}in=1 and Acc(αi, βi) represent n uniformly sampled subnetworks and their
measured accuracy, respectively. Thus the data collection requires n validation epochs.
One can further unlock the full capacity of a quadratic predictor by allowing the coupling of
all components and solving the following linear regression problem:
n
min 工 || (r + αTg + βTqβ + αTQaeβi) + αTQaαi + βTQββi — Acc(αi, βi)||2
r,qα,qβ ,<$ αβ Q α,C^ β M
A closed form solution to these problems is derived in appendix E. While effective, this
solution requires avoiding memory issues associated with inverting N2 × N2 matrix and also
reducing overfitting by tuning regularization effects over train-val splits of the data points.
Figure 3 shows that the estimator proposed in section 3.2.1 matches the performance of
those learnt predictors while being more sample efficient as discussed in section 4.2.3.
3.2.3	Beyond Quadratic Accuracy Predictors
The reader might question the expressiveness of a simple quadratic predictor and its ability
to capture the complexity of architectures. Indeed, White et al. (2021) present many complex
accuracy predictors and corresponding sampling techniques. To alleviate the reader’s
concerns we show in Figure 3 and Section 4.2.3 that the proposed quadratic estimator of
Section 3.2.1 matches the performance of the commonly used Multi-Layer-Perceptron (MLP)
based accuracy predictor Cai et al. (2019); Lu et al. (2020). The MLP based predictor
is more complex and requires extensive hyperparameter tuning, e.g., of the depth, width,
learning rate and its scheduling, weight decay, optimizer etc. It is also less efficient, lacks
interpretability, and of limited utility as an objective function for NAS, as discussed in
Section 3.3.
5
Under review as a conference paper at ICLR 2022
76.5
ntsH,πtspuφX
58 5
8. 0. 7.
00
)dilos( tneicffieoC
0.25
0.1
256 1,280 2,560 3,840
—B- Bilinear Predictor
—Q- Quadratic Predictor
M MLP Predictor
Estimator (Single-path sampling)
E Estimator (Multi-path sampling)
0.3
)dehsad( rorrE erauqS nae
Validation Epochs
J 5∙10-2
6,400
74
655
7 5. 7
7
)%( ycaruccA tohs-enO
BCFW
Evolution
MIQCP
SUPernework (HardCoRe)
40 45 50 55 60 65 70 75 80 85 90
Latency Formula (milliseconds)
Figure 3:	Performance of predictors vs
samPles. Ours is comParable to comPlex
alternatives and more samPle efficient.
Figure 4:	ComParing oPtimizers for solving
the IQCQP over 5 random seeds. All surPass
oPtimizing the suPernetwork directly.
3.3	Solving the Integer Quadratic Constraints Quadratic Program
Having defined the quadratic objective function of equation 4, the quadratic latency constraint
of equation 5 and the integer (binary) linear constraints of equation 2 that sPecify the search
sPace, we can now use out-of-the-box Mixed Integer Quadratic Constraints Programming
(MIQCP) solvers to oPtimize Problem 1. We use IBM ILOG CPLEX that suPPorts non-convex
binary QCQP and utilizes the Branch-and-Cut algorithm Padberg & Rinaldi (1991) for this
PurPose. A heuristic alternative for oPtimizing an objective function beyond quadratic, e.g.,
of section 3.2.3, under integer constraints is evolutionary search Real et al. (2019). Next we
ProPose a more theoretically sound alternative.
3.3.1	Utilizing the Block Coordinate Frank-Wolfe Algorithm
As Pointed out by Nayman et al. (2021), since Θ is constructed from measured latency in
equation 5, it is not guaranteed to be Positive semi-definite, hence, the induced quadratic
constraint makes the feasible domain in Problem 1 non-convex in general. To overcome this
we adaPt the Block-Coordinate Frank-Wolfe (BCFW) Lacoste-Julien et al. (2013) for solving
a continuous relaxation of Problem 1, such that ζ ∈ R+N . Essentially BCFW adoPts the
Frank-Wolfe Frank et al. (1956) uPdate rule for each of coordinates in ζ = (α, β) Picked uP
at random at each iteration k for any Partially differentiable objective function ACC(α, β):
α = argmax Na ACC(a, βk)T ∙ α s.t. βTΘT ∙ α ≤ T; A(S ∙ α ≤ %	⑺
α
β = argmax NeACC(ak, β)T ∙ β s.t. aTΘ ∙ β ≤ T; AS ∙ β ≤ b^	(8)
β
and δk +1 = (1 - Yk) ∙ δk + Yk ∙ δ for δ ∈ {ɑ, β}, where 0 ≤ Yk ≤ 1 and Nδ stands for the
Partial derivatives with resPect to δ. This aPPlies for both the differentiable MLP Predictor
of section 3.2.3 and the quadratic one of equation 7. Convergence guarantees are Provided
in Lacoste-Julien et al. (2013). Then, we need to Project the solution back to the discrete
sPace of architectures, sPecified in equation 2, as done in Nayman et al. (2021). This steP
could deviate from the solution and cause degradation in Performance.
Algorithm 1 applies the BCFW with line-search for the special case of using the bilinear
objective function sPecified in section 3.2.1 and in equation 6. Together with the bilinear
constraints of equation 5, the resulting problem is a Bilinear Programming (BLP) Gallo &
UIkUCU (1977) with bilinear constraints, i.e., BLCP. For this case, more specific convergence
guarantees can be provided together with the sparsity of the solution, hence no additional
discretization step is required. The following theorem states that after O(1 /e) iterations,
Algorithm 1 obtains an e-approximate solution to problem 1. The proof is in Appendix F.
Theorem 3.2. For each k > 0 the iterate ζk Algorithm 1 satisfies:
4
E[ACC(Zk)] — ACC(Z*) ≤ k+4 (ACC(Z0) — ACC(Z*))
where Z* is the solution of a continuous relaxation of problem 1 and the expectation is over
the random choice of the block α or β .
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Block Coordinate Frank-Wolfe (BCFW) with Line Search for BLCP
input (ao, βo) ∈ {(α, β)∣αΘβ ≤ T,A Aa α ≤ b s ,A^ β ≤ b1}, 0 <p< 1
1:	for k = 0, . . . , K - 1 do
2:	if Bernoulli(p) == 1 then
3:	αk+ι = argmax α(qT + βkQTβ) ∙ α s.t. βTΘt ∙ α ≤ T ; AAa ∙ α ≤ ba and βk +ι = βk
4:	else
5：	βk +1 = argmax 万(qg + αTQaβ) ∙ β s.t. αTΘ ∙ β ≤ T ; AS ∙ β ≤ bA and αk +ι = αk
6:	end if
7:	end for
output CZ * = (αK, βK)
We next provide a guarantee that Algorithm 1 yields a sparse solution, representing a valid
sub-network of the one-shot model up to a single probability vector from those composing α
and β , which contains up to two non-zero entries each, as all the rest are one-hot vectors.
Hence, no further discretization step is required. The proof is in Appendix G.
Theorem 3.3. The output solution (α, β) = ζ* of Algorithm 1 admits:
D
工 | α以 0 = 1 ∀( s, b) ∈ {1,.., S} X {1,.., D }\{(S α ,b α)} and 工 | βb |0 = 1 ∀ S ∈ {1 ,∙∙, S }\{S β }
c∈C	b=1
where | ∙ ∣0 = 1{∙ > 0} and (Sα, bα), Sβ are single block and stage respectively, satisfying:
D
∑ IαSa,cI0 ≤ 2 ;工 |βbβ I0 ≤ 2	⑼
c∈C	b=1
A negligible latency deviation is associated with taking the argmax over the only two couples
referred to in equation 9. Experiments supporting this are described in Section 4.2.4.
4 Experimental Results
4.1	Search for State-of-the-Art Architectures
4.1.1	Dataset and Setting
The train data for the accuracy predictors of
sections 3.2.2 and 3.2.3 is composed of sub-
networks uniformly sampled from the super-
network and their corresponding validation
accuracy is measured over the same 20% of
the Imagenet train set, considered as a val-
idation set. The same validation set is used
for the Monte-Carlo sampling mentioned in
Section 3.2.1. To avoid overfitting we use
regularization when learning accuracy pre-
dictors whose coefficient is tuned over 10% of
the collected data, see appendix E. The test
set for evaluating the ranking correlations
of all the accuracy predictors is composed of
another 500 samples generated uniformly in
the same way. More reproduciblity details
are provided in appendix C.
Figure 5: Imagenet Top-1 accuracy vs latency.
4.1.2	Comparisons with other methods
We compare our generated architectures to other state-of-the-art NAS methods in Table 1
and Figure 5. For the purpose of comparing the generated architectures alone, excluding
the contribution of evolved pretraining techniques, for each model in Table 1, the official
PyTorch implementation Paszke et al. (2019) is trained from a random initialization (besides
7
Under review as a conference paper at ICLR 2022
Model	Latency (ms)	Top-1 (%)	Total Cost (GPU hours)
MnasNetBI	39	74.5	40,000N
TFNAS-B	40	75.0	263N
SPNASNet	41	74.9	288 + 408N
OFA CPU2	42	75.7	1200 + 25N
HardCoRe A	40	75.8	400 + 15N
Ours 40 ms	40	76.1	435 + 8N
MobileNetV3	45	75.2	180N
FBNet	47	75.7	576N
MnasNetAI	55	75.2	40,000N
HardCoRe B	44	76.4	400 + 15N
Ours 45 ms	45	76.5	435 + 8N
MobileNetV2-	70	76.5	150N
TFNAS-A	60	76.5	263N
HardCoRe C	50	77.1	400 + 15N
Ours 50 ms	50	76.8	435 + 8N
EfficientNetB0	85	77.3	
HardCoRe D	55	77.6	400 + 15N
Ours 55 ms	55	77.7	435 + 8N
FairNAS-C	60	77.0	240N
HardCoRe E	61	78.0	400 + 15N
Ours 60 ms	59	77.8	435 + 8N
Table 1: ImageNet top-1 accuracy, latency and cost comparison with other methods. The
total cost stands for the search and training cost of N networks. Latency is reported for (Left)
Intel Xeon CPU and (Right) NVIDIA P100 GPU with a batch size of 1 and 64 respectively.
OFA 2) using the exact same techniques and code, as specified in section 4.1.1. We report
the maximum accuracy between the original paper and our training. We emphasize that
all latency values presented are actual time measurements of the models, running on a
single thread with the exact same settings and on the same hardware. We excluded further
optimizations, such as Intel MKL-DNN Intel (R), therefore, the latency we report may differ
from the one originally reported. It can be seen that networks generated by our method
meet the latency target closely, while at the same time is comparable to or surpassing all
the other methods on the top-1 accuracy over ImageNet with a reduced scalable search
time. The total search time consists of 435 GPU hours computed only once as preprocessing
and additional 8 GPU hours for fine-tuning each generated network, while the search itself
requires negligible several CPU minutes, see appendix A for more details.
4.2	Empirical Analysis of Key Components
In this section we analyze and discuss different aspects of the proposed method.
Model	Latency (ms)	Top-1 (%)
MobileNetV3	28	75.2
TFNAS-D	30	74.2
HardCoRe A	27	75.7
Ours 25 ms	26	76.4
MnasNetA1	37	75.2
MnasNetB1	34	74.5
FBNet	41	75.7
SPNASNet	36	74.9
TFNAS-B	44	76.3
TFNAS-C	37	75.2
HardCoRe B	32	77.3
Ours 30 ms	31	76.8
TFNAS-A	54	76.9
EfficientNetB0	48	77.3
MobileNetV2	50	76.5
HardCoRe C	41	77.9
Ours 40 ms	40	77.5
4.2.1
The Contribution of Different
Terms of the Accuracy Estimator
The accuracy estimator in equation 3 ag-
gregates the contributions of multiple ar-
chitectural decisions. In equation 4, those
decisions are grouped into two groups: (1)
macroscopic decisions about the depth of
each stage are expressed by qβ and (2) mi-
croscopic decisions about the configuration
Variant	Kendall-Tau	Spearman
qβ ≡ 0 Qae ≡ 0	029 0.66	02 0.85
2。。(α, β)-	0.84	—	0.97
Table 2: Contribution of terms.
of each block are expressed by Qαβ .
Table 2 quantifies the contribution of each of those terms to the ranking correlations by
setting the corresponding terms to zero. We conclude that the depth of the network is very
significant for estimating the accuracy of architectures, as setting qβ to zero specifically
decreases the Kendall-Tau and Spearman’s correlation coefficients from 0.84 and 0.97 to 0.29
and 0.42 respectively. The significance of microscopic decisions about the configuration of
blocks is also viable but not as much, as setting Qαβ to zero decreases the Kendall-Tau and
Spearman’s correlation coefficients to 0.66 and 0.85 respectively.
2 Finetuning a model obtained by 1200 GPU hours.
8
Under review as a conference paper at ICLR 2022
4.2.2 Interpretability of the Accuracy Estimator
The way the accuracy estimator in section 3.2.1 is constructed brings many insights about
the contribution of different design choices to the accuracy, as shown in Figure 6.
Deepen later stages: In the left figure ∆bs - ∆bs-1 are presented for b = 3, 4 and s = 1, . . . , 5.
This graph shows that increasing the depth of deeper stages is more beneficial than doing so
for shallower stages. Showing also the latency cost for adding a block to each stage, we see
that there is a strong motivation to make later stages deeper.
Add width and S&E to later stages and shallower blocks: In the middle and right
figures, ∆bs,c are averaged over different configurations and block or stages respectively for
showing the contribution of microscopic design choices. Those show that increasing the
expansion ratio and adding S&E is more significant in deeper stages and at sooner blocks
within each stage. Width and S&E over bigger kernels: Increasing the kernel size is
relatively less significant and is more effective at intermediate stages.
Figure 6: Design choices insights deduced from the accuracy estimator: The contribution of
(Left) depth for different stages, (Middle) expansion ration, kernel size and S&E for different
stages and (Right) for different blocks within a stage.
4.2.3	Ranking Correlation Per Cost for Different Accuracy Predictors
Figure 3 presents the Kendall-Tau ranking correlation coefficients and MSE of different
accuracy predictors that are introduced in Section 3.2 versus the number of epochs of the
validation set required for obtaining their parameters. It is noticable that the simple bilinear
accuracy estimator (section 3.2.1) is more sample efficient, as its parameters are estimated
rather than learned. The effectiveness of the Multi-path sampling Nayman et al. (2021) of
driving each image through a distinct subnetwork for obtaining the accuracy estimator is
very clear comparing to the Single-path counterpart of driving each batch through the same
subnetwork. With access to enough samples, the performance of these all are comparable.
4.2.4	Comparison of Optimization Algorithms
Formulating the NAS problem as IQCQP affords the utilization of a variety of optimization
algorithms. Figure 4 compares the one-shot accuracy and latency of networks generated
by utilizing the algorithms suggested in section 3.3 for solving problem 1 with the bilinear
estimator introduced in section 3.2.1 serving as the objective function. Error bars for
both accuracy and latency are presented for 5 different seeds. All algorithms satisfy the
latency constraints up to a reasonable error of less than 10%. While all of them surpass
the performance of BCSFW Nayman et al. (2021), given as reference, BCFW is superior at
low latency, evolutionary search does well over all and MIQCP is superior at high latency.
Hence, for practical purposes we apply the three of them for search and take the best one,
with negligible computational cost of less than three CPU minutes overall.
5	Conclusion
The problem of resource-aware NAS is formulated as an IQCQP optimization problem. The
quadratic constraints express resource requirements and a quadratic accuracy estimator
serves as the objective function. This estimator is constructed by measuring the individual
contribution of design choices, which makes it intuitive and interpretable. Indeed, its
interpretability brings several insights and design rules. Its performance is comparable to
complex predictors that are more expensive to acquire and harder to optimize. Efficient
optimization algorithms are proposed for solving the resulted IQCQP problem. IQNAS
is a faster search method, scalable to many devices and requirements, while generating
comparable or better architectures than those of other state-of-the-art NAS methods.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
In this paper, we have made efforts to produce explanations and present clear algorithms
describing every step of the different components of our method. In addition, a source code
will be released upon publication that will allow to reproduce every result presented in
the paper. In the appendix, we present an overview of our method and its computational
cost (Appendix A), a clear description of our search space (Appendix B), the experimental
setting with training procedure details to obtain our results (Appendix C), as well as proof
of Theorem 3.1 (Appendix D), and the concise description of the algorithm we used to find
the closed form regularized solution of the linear regression problems described in section 3.2
(Appendix E), to ensure full reproducibility. In addition, we present more theoretical results
related to the convergence of Algorithm 1 and the sparsity of it solution (Appendix F, G,
H).
References
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le.
Understanding and simplifying one-shot architecture search. In International Conference
on Machine Learning, pp. 550-559. PMLR, 2018.
Leon Bottou. Online algorithms and stochastic approXima-P tions. Online learning and
neural networks, 1998.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model
architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Han Cai, Ligeng Zhu, and Song Han. ProXylessnas: Direct neural architecture search on
target task and hardware. arXiv preprint arXiv:1812.00332, 2018.
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one
network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.
Xin Chen, LingXi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search:
Bridging the depth gap between search and evaluation. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 1294-1303, 2019.
XiangXiang Chu, Bo Zhang, Ruijun Xu, and JiXiang Li. Fairnas: Rethinking evaluation
fairness of weight sharing neural architecture search. arXiv preprint arXiv:1907.01845,
2019.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Au-
toaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501,
2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale
Hierarchical Image Database. In CVPR09, 2009.
Marguerite Frank, Philip Wolfe, et al. An algorithm for quadratic programming. Naval
research logistics quarterly, 3(1-2):95-110, 1956.
Giorgio Gallo and Aydin Ulkucu. Bilinear programming: an exact algorithm. Mathematical
Programming, 12(1):173-194, 1977.
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian
Sun. Single path one-shot neural architecture search with uniform sampling. In European
Conference on Computer Vision, pp. 544-560. Springer, 2020.
Elad Hazan and Haipeng Luo. Variance-reduced and projection-free stochastic optimization.
In International Conference on Machine Learning, pp. 1263-1271. PMLR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 770-778, 2015.
10
Under review as a conference paper at ICLR 2022
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural
network. In NIPS Deep Learning and Representation Learning Workshop, 2015. URL
http://arxiv.org/abs/1503.02531.
Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen,
Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and
Yukun Zhu. Searching for mobilenetv3. In 2019 IEEE/CVF International Conference
on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019,
pp. 1314-1324. IEEE, 2019. doi: 10.Π09∕ICCV.2019.00140. URL https://doi.org/10.
1109/ICCV.2019.00140.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 7132-7141, 2018.
Yibo Hu, Xiang Wu, and Ran He. Tf-nas: Rethinking three search freedoms of latency-
constrained differentiable neural architecture search. arXiv preprint arXiv:2008.05314,
2020.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training
of giant neural networks using pipeline parallelism. In Advances in neural information
processing systems, pp. 103-112, 2019.
IBM ILOG CPLEX.	Ibm ilog cplex miqcp opti-
mizer.	https://www.ibm.com/docs/en/icos/12.7.1.0?topic=
smippqt-miqcp-mixed-integer-programs-quadratic-terms-in- constraints.
Intel(R). Intel(r) math kernel library for deep neural networks (intel(r) mkl-dnn), 2019. URL
https://github.com/rsdubtso/mkl-dnn.
Hans Kellerer, Ulrich Pferschy, and David Pisinger. The Multiple-Choice Knapsack Prob-
lem, pp. 317-347. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-
3-540-24777-7. doi: 10.1007∕978-3-540-24777-7J1. URL https://doi.org/10.1007/
978-3-540-24777-7_11.
Henry J Kelley. Gradient theory of optimal flight paths. Ars Journal, 30(10):947-954, 1960.
Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, and Patrick Pletscher. Block-coordinate
frank-wolfe optimization for structural svms. In International Conference on Machine
Learning, pp. 53-61. PMLR, 2013.
Eric Langford, Neil Schwertman, and Margaret Owens. Is the property of being positively
correlated transitive? The American Statistician, 55(4):322-325, 2001.
Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang,
and Zhenguo Li. Darts+: Improved differentiable architecture search with early stopping.
arXiv preprint arXiv:1909.06035, 2019.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search.
arXiv preprint arXiv:1806.09055, 2018.
Zhichao Lu, Kalyanmoy Deb, Erik Goodman, Wolfgang Banzhaf, and Vishnu Naresh Boddeti.
NSGANetV2: Evolutionary multi-objective surrogate-assisted neural architecture search.
In European Conference on Computer Vision (ECCV), 2020.
Kendall Maurice. A new measure of rank correlation. Biometrika, 30(1-2):81-89, 1938.
Niv Nayman, Asaf Noy, Tal Ridnik, Itamar Friedman, Rong Jin, and Lihi Zelnik. Xnas: Neural
architecture search with expert advice. In Advances in Neural Information Processing
Systems, pp. 1977-1987, 2019.
Niv Nayman, Yonathan Aflalo, Asaf Noy, and Lihi Zelnik-Manor. Hardcore-nas: Hard
constrained differentiable neural architecture search. arXiv preprint arXiv:2102.11646,
2021.
11
Under review as a conference paper at ICLR 2022
Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja
Giryes, and Lihi Zelnik. Asap: Architecture search, anneal and prune. In International
Conference on Artificial Intelligence and Statistics, pp. 493-503. PMLR, 2020.
Manfred Padberg and Giovanni Rinaldi. A branch-and-cut algorithm for the resolution of
large-scale symmetric traveling salesman problems. SIAM review, 33(1):60-100, 1991.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, An-
dreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc., 2019.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for
image classifier architecture search. In Proceedings of the aaai conference on artificial
intel ligence, volume 33, pp. 4780-4789, 2019.
Binxin Ru, Xingchen Wan, Xiaowen Dong, and Michael Osborne. Interpretable neural archi-
tecture search via bayesian optimisation with weisfeiler-lehman kernels. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=j9Rv7qdXjd.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal represen-
tations by error propagation. Technical report, California Univ San Diego La Jolla Inst
for Cognitive Science, 1985.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4510-4520, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. In International Conference on Learning Representations, 2015.
Charles Spearman. ”general intelligence” objectively determined and measured. 1961.
Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos, Bodhi Priyantha,
Jie Liu, and Diana Marculescu. Single-path nas: Designing hardware-efficient convnets in
less than 4 hours. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pp. 481-497. Springer, 2019.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 2818-2826, 2016.
Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional
neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 6105-6114. PMLR, 2019. URL http://proceedings.mlr.press/v97/tan19a.html.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard,
and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
2820-2828, 2019.
Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh.
Rethinking architecture selection in differentiable nas. arXiv preprint arXiv:2108.04392,
2021.
Colin White, Arber Zela, Binxin Ru, Yang Liu, and Frank Hutter. How powerful are
performance predictors in neural architecture search? arXiv preprint arXiv:2104.01177,
2021.
12
Under review as a conference paper at ICLR 2022
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong
Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient
convnet design via differentiable neural architecture search. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019, pp. 10734-10742. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/
CVPR.2019.01099.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578, 2016.
13
Under review as a conference paper at ICLR 2022
14
Under review as a conference paper at ICLR 2022
Appendix
A An Overview of the Method and Computational Costs
Figure 7 presents and overview scheme of the method:
Deploy building
blocks on target devices
280 GPU Hours
,I
Search Space
One-shot Model
(Super-network)
Train through
the Heaviest
Sub-network
Train the
One-shot Model
with
MUltipath
Sampling
120GPU Hours
Lookup Table
for Resource
Measurements
35 GPU Hours
Resource Formula
Calculation
Required
Resource
Constraints
■ System Components
Q Resource Measurements
□	Stage 1: One-shot Model Training
□	Stage 2
Search under Constraints
Wh
Measure the
expected
contribution of
each decision
Quadratic
Accuracy
Estimator
IQCP
BCFW
Evo
3N CPU Minutes
10∣
Extract the pretrained Sub-network
□ Stage 3: Fine-tuning
Knowledge
Distillation
Fine-tuning
8N GPU Hours
Figure 7: An Overview scheme of the IQNAS method with computational costs
The search space, latency measurements and formula, supernetwork training and fine-tuning
blocks (1,2,3,5,6,7,10,11,12) are identical to those introduced in HardCoRe-NAS:
We first train for 250 epochs a one-shot model wh using the heaviest possible configuration,
i.e., a depth of 4 for all stages, with er = 6, k = 5 × 5, se = on for all the blocks. Next, to
obtain W*, for additional 100 epochs of fine-tuning Wh over 80% of a 80-20 random split of
the ImageNet train set Deng et al. (2009). The training settings are specified in appendix C.
The first 250 epochs took 280 GPU hours and the additional 100 fine-tuning epochs took
120 GPU hours, both Running with a batch size of 200 on 8×NVIDIA V100, summing to a
total of 400 hours on NVIDIA V100 GPU to obtain both Wh and W*.
A significant benefit of this training scheme is that it also shortens the generation of trained
models. The common approach of most NAS methods is to re-train the extracted sub-
networks from scratch. Instead, we follow HardCoRe-NAS and leverage having two sets
of weights: Wh and W*. Instead of retraining the generated sub-networks from a random
initialization we opt for fine-tuning W* guided by knowledge distillation Hinton et al. (2015)
from the heaviest model Wh . Empirically, as shown in figure 5 by comparing the dashed red
line with the solid one, we observe that this surpasses the accuracy obtained when training
from scratch at a fraction of the time: 8 GPU hours for each generated network.
The key differences from HardCoRe-NAS reside in the latency constrained search blocks
(4,8,9 of figure 7) that have to do with constructing the quadratic accuracy estimator of
section 3.2.1 and solving the IQCQP problem. The later requires only several minutes on
CPU for each generated netowork (compared to 7 GPU hours of HardCoRe-NAS), while the
former requires to measure the individual accuracy contribution of each decision. Running
a validation epoch to estimate E[Acc] and also for each decision out of all 255 entries in
the vector ζ to obtain ∆bs,c and ∆bs requires 256 validation epochs in total that last for 3.5
GPU hours. Figure 3 shows that reducing the variance by taking 10 validation epochs per
measurement is beneficial. Thus a total of 2560 validation epochs requires 35 GPU hours
only once.
Overall, we are able to generate a trained model within a small marginal cost of 8 GPU
hours. The total cost for generating N trained models is 435 + 8N , much lower than the
1200 + 25N reported by OFA Cai et al. (2019) and more scalable compared to the 400 + 15N
reported by HardCoRe-NAS. See Table 1. This makes our method scalable for many devices
and latency requirements.
15
Under review as a conference paper at ICLR 2022
B More Specifications of the Search Space
Inspired by EfficientNet Tan & Le (2019) and TF-NAS Hu et al. (2020), HardCoRe-NAS Nay-
man et al. (2021) builds a layer-wise search space that we utilize, as explained in Section 3.1
and depicted in Figure 1 and in Table 3a. The input shapes and the channel numbers are
the same as EfficientNetB0. Similarly to TF-NAS and differently from EfficientNet-B0, we
use ReLU in the first three stages. As specified in Section 3.1, the ElasticMBInvRes block is
the elastic version of the MBInvRes block as in HardCoRe-NAS, introduced in Sandler et al.
(2018). Those blocks of stages 3 to 8 are to be searched for, while the rest are fixed.
Stage	Input	Operation	Cout	Act	b	C	er	k			se
1	2242 × 3	3 × 3 Conv	32	ReLU	1	i~γ~	"ɪ	3	×	3	~ffΓ
2	1123 × 32	MBInvRes	16	ReLU	1	2	2	5	×	5	on
3	1122 × 16	ElasticMBInvRes	24	ReLU	[2, 4]	3	2	3	×	3	off
4 5 6	562 × 24 282 × 40 142 × 80	ElasticMBInvRes ElasticMBInvRes ElasticMBInvRes	40 80 112	Swish Swish Swish	[2, 4] [2, 4] [2, 4]	4 5 6 7	2 3 3 3	5 3 5 3	× × × ×	5 3 5 3	on off on off
7	142 × 112	ElasticMBInvRes	192	Swish	[2, 4]	8 9	3	5 3	× ×	5 3	Cn on
8	72 × 192	ElasticMBInvRes	960	Swish	1		6				off
9	72 × 960	1 × 1 Conv	1280	Swish	1	10	6	5	×	5	on
10	72 × 1280	AvgPool	1280	-	1	11	6	3	×	3	off
11	1280		Fc		1000	-	1	12	6	5	×	5	on
	(a) Macro	architecture of the one-shot model.				(b)	Confi	guration			s.
Table 3: Search space specifications and indexing. ”MBInvRes” is the basic block in Sandler
et al. (2018). ”ElasticMBInvRes” denotes the elastic blocks (Section 3.1) to be searched
for. ”Cout ” stands for the output channels. Act denotes the activation function used in a
stage. "b" is the number of blocks in a stage, where [b, b] is a discrete interval. If necessary,
the down-sampling occurs at the first block of a stage. ”er” stands for the expansion ratio
of the point-wise convolutions, ”k” stands for the kernel size of the depth-wise separable
convolutions and ”se” stands for Squeeze-and-Excitation (SE) with on and off denoting
with and without SE respectively. The configurations are indexed according to their expected
latency.
C Reproducibility and Experimental Setting
In all our experiments we train the networks using SGD with a learning rate of 0.1, cosine
annealing, Nesterov momentum of 0.9, weight decay of 10-4, applying label smoothing
Szegedy et al. (2016) of 0.1, cutout, Autoaugment Cubuk et al. (2018), mixed precision and
EMA-smoothing.
The supernetwork is trained following Nayman et al. (2021) over 80% of a random 80-
20 split of the ImageNet train set. We utilize the remaining 20% as a validation set for
collecting data to obtain the accuracy predictors and for architecture search with latencies
of 40, 45, 50, . . . , 60 and 25, 30, 40 milliseconds running with a batch size of 1 and 64 on an
Intel Xeon CPU and and NVIDIA P100 GPU, respectively.
The evolutionary search implementation is adapted from Cai et al. (2019) with a population
size of 100, mutation probability of 0.1, parent ratio of 0.25 and mutation ratio of 0.5. It
runs for 500 iterations, while the the BCFW runs for 2000 iterations and its projection step
for 1000 iterations. The MIQCP solver runs up to 100 seconds with CPLEX default settings.
D Proof of Theorem 3.1
Theorem D.1. Consider n independent random variables {Xi}i∈[1,2...,n] conditional ly inde-
pendent with another random variable A. Suppose in addition that there exists a positive
real number 0 < e《1 such that for any given Xi, the following term is bounded by above:
P [ A = a | Xi = Xi ]	1
—P [ A = a ]
16
Under review as a conference paper at ICLR 2022
Then We have that:
E [A|Xι =	xi)...)Xn	=	Xn]	= E [A]	+	^^ (E [A|Xi =	Xi]	— E [A])(1 + O(ne)).	(10)
i
Proof. We consider two independent random variables X, Y that are also conditionally
independent with another random variable A. Our purpose is to approximate the following
conditional expectation:
E [A | X = x,Y = y].
We start by writing :
p [A = a|X = x，Y = y] =P[ X = X, X==yXAY=a FA =a]
Assuming the conditional independence, that is
P [X = x,Y = y | A = a] = P [X = X| A = a] P [Y = y | A = a],
we have
P [A = a | X = x,Y = y]
P [ X = X | A = a ] P [ Y = y | A = a ] P [ A = a ]
P[ A = a|X P[ X]=[ A=[ Y ⅛ ] —	(11)
—	P [ A = a ]
Next, we assume that the impact on A of the knowledge of X is bounded, meaning that
there is a positive real number 0 < e《1 such that:
P [ A = a | X = x ]
P [ A = a ] ~~
一1 ≤ e.
We have:
P [ A = a | X =	X ]	=	P [ A =	a ]	+ P [ A =	a | X =	X ]	—	P [ A	=	a ]
(
P [ A = a ]	1 +
∖
∖
(P [ A = a | X = X ] Λ
I	P[ A = a ] 1J
`-----------“-----------7
^x
Using a similar development for P [A = a|Y = y] we also have:
(
I + /P [A = a|Y = y]
+ V	P [ A = a ]
------	V------
∖-----ey
Plugging the two above equations in (11), we have:
P [ A = a | Y = y ] = P [ A = a ]
P [A = a|X = x,Y = y]
P[ A = a](1 +	)P[ A = a](1 + %)
∖
-1)
—	P [ A = a ]
=P [ A = a] (I + ex )(I + ey )
=P [A = a](1 + eχ + ey) + O(e2)
=P [A = a](1+ (PA = " =x] — 1)+ (PA=a1Y = y] — Z
[	]	<	P [ A = a ]	)	∖	P [ A = a]	”
=P [A = a] (PA = FX =x] + P[A =『Y = y] — 1)+ O(e2)
I 八 P [ a = a ]	P [ A = a ]	J ' '
=P [A = a|X = X] + P [A = a|Y = y] — P [A = a] + O(e2)
+ O (e 2)
17
Under review as a conference paper at ICLR 2022
Then, integrating over a to get the expectation leads to:
E [A|X = x, Y = y] =	aP [A = a|X = x, Y = y] da
a
=E [A|X = x] + E [A|Y = y] — E [A] + O(62)
=E [A] + (E [A|X =x] -E[A])
+ (E[ A | Y = y ] — E [ A]) + O (6 2).
In the case of more than two random variables, X1 , X2 , . . . , Xn , denoting by Un =
E [A|X1 = x1, . . . , Xn = xn] — E [A], and by Vn = E [A|Xn = xn] — E [A], we have
| un - un―1 - Vn | < 6un―1
A simple induction shows that:
Vn + (1-6)Vn-l + (1 — 6)2Vn-2+∙∙∙ + (1-e)nV 1 < Un < Vn + (1+ 6)Vn-l+(1+ 6)2Vn-2+∙∙∙ + (1+ 6)nV 1
Hence:
(1 — 6)	Vi < Un < (1 + 6) n 工 Vi
that shows that
Un =(工 V) (1 + O(n6))
□
Now we utilize Theorem D.1 for proving Theorem 3.1. Consider a one-shot model whose
subnetworks’ accuracy one wants to estimate: E[Acc| ∩sS=1 ∩bD=1Obs, ∩sS=1 ds], with αbs,c =
1Os =Oc the one-hot vector specifying the selection of configuration c for block b of stage s
and βbs = 1ds =b the one-hot vector specifying the selection of depth b for stage s, such that
(α, β) ∈ S with S specified in equation 2 as described in section 3.1.
We simplify our problem and assume {Obs, ds}, ds for s = 1, . . . , S and b = 1, . . . , D are
conditionally independent with the accuracy Acc. In our setting, we have
E Acc∣∩S=1 ∩D=1 Ob； ∩S=1 ds] = E [Acc∣∩S=1 ∩b= 1 {。肘}； ∩S=1 d]	(12)
≈ E [Acc]
SD
+毕 (E [Acc|Ob, ds] — E [Acc]) 16≤ds (1 + O(N)) (13)
S
+ 工(E [Acc|ds] — E [Acc]) 16≤逾(1 + O(N))	(14)
where equation 12 is since the accuracy is independent of blocks that are not participating
in the subnetowrk, i.e. with b > ds, and equations 13 and 14 are by utilizing Theorem D.1.
Denote by bs and cbs to be the single non zero entries of βs and αbs respectively, whose entries
are βbs for b = 1, . . . , D and αbs,c for c ∈ C respectively. Hence βbs = 1b=bs and αbs,c = 1c=cs .
Thus we have,
D
E [Acc|ds = bs] — E [Acc]=工 1 b=bs (E [Acc|ds = b] — E [Acc])
b=1
DD
=工 βb (E [Acc|ds = b] — E [Acc])=工 βb∆S	(15)
b=1	b=1
18
Under review as a conference paper at ICLR 2022
Similarly,
E IAcc | Ob =OCs,ds = b - E [ Acc ]=工 1C=Cs (E [ Acc | Os = Oc,ds = b ] - E [ Acc ])
C∈C
=工 aS,C(E [Acc|OS = Oc, ds = b] - E [Acc])
C∈C
=工*公S,c
C∈C
And since effectively 1b≤ds
ΣD= b βs We have,
(E 即|OS = Ocs,ds= b] - E [Acc]) 1 b≤ds =工…1 b≤ds
c∈C
D
=工工 αb,c ∙ δb,c ∙ βs
b b=bC∈C
Finally by setting equations 15 and 17 into 13 and 14 respectively, We have,
(16)
(17)
E Acc
∩sS=1 ∩bD=1 Obs
∩sS=1ds
CSD	SDD
ΣΣ β^b ∙ ∆ S + ∑∑∑∑ αS,c ∙ ∆ 短
S =1 b =1	S = 1 b =1 bb = b C∈C
βb^ (1 + O(N))
E Deriving a Closed Form Solution for a Linear Regression
We are given a set (X, Y ) of architecture encoding vectors and their accuracy measured on a
validation set.
We seek for a quadratic predictor f defined by parameters Q ∈ Rn×n , a ∈ Rn , b ∈ R such as
f(x) = xTQx + aTx + b
Our purpose being to minimise the MSE over a training-set Xtrain , We seek to minimize:
min
Q,a,b
Σ
(x,y)∈(Xtrain ,Ytrain )
IIXTQx + XTa + b - y∣∣2
(18)
We also have that
XTQX = trace QXXT
Denoting by q the column-stacking of Q, the above expression can be expressed as:
XTQX =trace (QXXT) = q (x 0 x)
where 0 denotes the Kronecker product. Hence, equation 18 can be expressed as:
min 工	Il (x, x 0 X)T (a, q) + b - y 112 ∙	(19)
q,a,b
(x,y)∈(Xtrain ,Ytrain)
Denoting by X = (x, X 0 x) and V = (a, q), we are led to a simple regression problem:
min ∖y	∣∣XTV + b 一 y∣∣2∙	(20)
v,b
(X, y)∈( X train ,Ytrain )
We rewrite the objective function of equation 20 as:
(X T V + b — y) T (X T V + b — y) = V T XX T V + 2( b — y )X T V + (b — y )2
Stacking the X, y in matrices X, Y, the above expression can be rewritten as:
VT X XT V + 2( b 1 — Y) T XT V + (b 1 — Y) T (b 1 — Y)	(21)
Deriving with respect to b leads to: 21TXTV + 2nb - 21TY = 0 Hence:
b = 1 (1 T(Y - XTV)) = 1 ((Y - XTv)T1)
19
Under review as a conference paper at ICLR 2022
We hence have
(b1 - Y)T (b1 -Y) = nb2 -2b1TY+YTY
1
=—(VTX11TXTV - 2YT11TXTV + 2YT11TTV)
=1 v T X11T XT v
n
In addition:
(b 1 - Y)TXTv = 1 (YT11TXTv - vTX11TXTV)- YTXTv
Hence, equation 21 can be rewritten as:
vTXXTv - 1 vTX11TXTv + YT (Id - 111TI XTV
nn
Denoting by I = (Id - n 11T), X = IXT and by Y = IY, and noticing that ITI = I, we
then have:
X T X V = X T Y
To solve this problem We can find am SVD decomposition of X = UDVT, hence:
__ C _ _E	_____ ΓΤ1 _ ʌ
VD2V T v = VDU T Y
that leads to:
v = VD-1U T Y
The general algorithm to find the decomposition is the following:__________________
Algorithm 2 Closed Form Solution of a Linear Regression for the Quadratic Predictors
input {xi = (αi, βi) ∈ Rn, yi = Acc(αi, βi)}iN=1, k = number of principal components
1:	Compute Xi = (Xi, Xi 0 Xi), ∀i ∈ {1,..., N}
2:	Perform a centering on Xi computing Xi = Xi - meani =1 ,...,n (Xi), ∀i ∈ {1,..., N}
3:	Perform a centering on yi computing ^i = y% - mean i =ι ,...,n (yi), ∀ i ∈ {1,..., N}
4:	Define X = stack({Xi}N=I)
5:	Compute a k-low rank SVD decomposition of X, defined as U diag(S)VT
6:	Compute W = V diag(S-1)UTy
7:	Compute b = mean (y — X W)
8:	Define a = W1:n
9:	Reshape the end of the vector W as an n × n matrix, Q = reshape(Wn+1:n+1+n2 , n, n)
output b, a, Q
In order to choose the number k of principal components described in the above algorithm,
we can perform a simple hyper parameter search using a test set. In the below figure, we
plot the Kendall-Tau coefficient and MSE of a quadratic predictor trained using a closed
form regularized solution of the regression problem as a function of the number of principal
components k , both on test and validation set. We can see that above 2500 components,
we reach a saturation that leads to a higher error due to an over-fitting on the training set.
Using 1500 components leads to a better generalization. The above scheme is another way to
regularize a regression and, unlike Ridge Regression, can be used to solve problems of very a
high dimesionality without the need to find the pseudo inverse of a high dimensional matrix,
without using any optimization method, and with a relatively robust discrete unidimensional
parameter that is easier to tune.
F Convergence Guarantees for Solving BLCP with BCFW
with Line-Search
In this section we proof Theorem 3.2, guaranteeing that after O(1 /e) many iterations,
Algorithm 1 obtains an e-approximate solution to problem 1.
20
Under review as a conference paper at ICLR 2022
0.3
)dilos( tneicffieoC uaT-lladneK
)dehsad( rorrE erauqS naeM
2
-
55
2 2 11 -
....................
0000
0	500 1,000 1,500 2,000 2,500 3,000
Number of principal components
Figure 8: Kendall-Tau correlation coefficients and MSE of different predictors vs number of
principal components
F.1 Convergence Guarantees for a General BCFW over a Product Domain
The proof is heavily based on the convergence guarantees provided by Lacoste-Julien et al.
(2013) for solving:
min f(ζ)	(22)
Z ∈M(1)×…×M( n)
with the BCFW algorithm 3, where M(i) ⊂ Rmi is the convex and compact domain of
the i-th coordinate block and the product M(I) × …× M(n) ⊂ Rm specifies the whole
domain, as En=ι mi = m. Z(i) ∈ Rmi is the i-th coordinate block of Z and Z\(i) is the rest of
the coordinates of Z. ▽(i) stands for the partial derivatives vector with respect to the i-th
coordinate block.___________________________________________________________________
Algorithm 3 Block Coordinate Frank-Wolfe (BCFW) on Product Domain
input Zo ∈ M⑴ ×∙∙∙× M(n) ⊂ Rm
1:	for k = 0, . . . , K do
2:	Pick i at random in {1, . . . , n}
3:	Find Sk = argmin S∈m(W)sτ ∙ ▽(() f (Zk)
4:	Let Sk =: 0m ∈ Rm is the zero padding of Sk such that we then assign Sk := Sk
5：	Let Y := k+nn, or perform line-search: Y =: argmin 7z∈[oj 1] f ((1 - Y') ∙ Zk + YJ Sk)
6： Update Zk +1 = (1 - Y) ∙ Zk + Y ∙ Sk
7: end for
The following theorem shows that after O(1 /e) many iterations, Algorithm 3 obtains an
e-approximate solution to problem 22, and guaranteed e-small duality gap.
Theorem F.1. For each k > 0 the iterate Zk Algorithm 3 satisfies:
E[f (Zk)] - f (Z*) ≤ k+n2n (Cf + (f (Z0) - f (Z*)))
where Z* is the solution of problem 22 and the expectation is over the random choice of the
block i in the steps of the algorithm.
Furthermore, there exists an iterate 0 ≤ k ≤ K of Algorithm 3 with a duality gap bounded by
E[g(Z^)] ≤ &(Cf + (f (Zo) - f(Z*)))∙
Here the duality gap g(Z) ≥ f(Z) - f(Z*) is defined as following:
g ( Z)= S ∈M(imax×M( n) ( Z - s )T'f( Z)	(23)
21
Under review as a conference paper at ICLR 2022
and the global product curvature constant Cf = En=1 Cfi) is the sum of the (partial)
curvature constants of f with respect to the individual domain M(i):
sup
/ ∈M⑴ × …×M(n),
s(i) ∈M(i),γ∈ [0, 1],
y(i) = (1 - γ)x(i) + γs(i),
y\(i) = x\(i)
Y f((y)- f(2)-(y(i)-x(i)1 ▽(i)f(X))
(24)
which quantifies the maximum relative deviation of the objective function f from its linear
approximations, over the domain M(i).
The proof of theorem F.1 is given in Lacoste-Julien et al. (2013).
F.2 Analytic Line-Search for Bilinear Objective Functions
The following theorem provides a trivial analytic solution for the line-search of algorithm 3
(line 5) where the objective function has a bilinear form.
Theorem F.2. The analytic solution of the line-search step of algorithm 3 (line 5) with a
bilinear objective function of the form:
f (Z) = £ (Z(()) T ∙ P f) + £ £ (Z(()] ∙ Qfj Y ⑺	(25)
i=1	i=1 j=i
with p(f() ∈ Rmi and Q(f(,j) ∈ Rmi ×mj , reads γ ≡ 1 at all the iterations.
Proof. In each step of algorithm 3 at line 3, a linear program is solved:
S = argmin ▽(i) f (Z)T ∙ SS
s z∈M(i)
(26)
argmin
S z∈M(i)
t +	£
j∈{1,...,(-1}
Qfij)+	£
j∈{(+1,.
• S S
and the Line-Search at line 5 reads:
Y =: argmin
7 z∈[0, 1]
f ((1 - Yz) Y + YJ 可
y=(1
argmin
γY ∈ [0,1]
—	γ B • Z(i) + Y • s
T
+
£	(Zj) • Q (i,j) +	£	(Z (『
j∈{1,...,i-1}	j∈{i+1,...,n}
• y
y=(1
argmin
γY ∈ [0,1]
-	Y') Y(i) + YJ S
▽	(() f (Z)t ∙ y
(27)
Since Z((), S ∈ M(() and Y ∈ [0, 1] then the convex combination of those also satisfies y ∈ M(() ,
hence considering that S is the optimizer of 26 the solution to 27 reads y := S and hence
Y := 1. Thus, effectively the analytic solution to line-search for a bilinear objective function
□
is Y ≡ 1 at all times.
F.3 Solving BLCP by BCFW with Line-Search
In addition to a bilinear objective function as in equation 25, consider also a domain that is
specified by the following bilinear constraints:
n	nn
£ (Z(()) • PM + ££ (Z(()) • QMj) • Z(j) ≤ T ;	2 • Z ≤ b	(28)
i=1	i=1 j=i
22
Under review as a conference paper at ICLR 2022
with p(Mi) ∈ Rmi , Q(Mi,j) ∈ Rmi×mj , A ∈ RC×m and b ∈ RC for C ≤ 0, such that the individual
domain of the i-th coordinate block is specified by the following linear constraints:
≤ T (29)
A(i) ∙ Z(i) ≤ b(i) (30)
min
ζ(i)
where A(i) ∈ RC×mi are the rows r ∈ {1 + E< mj,..., Ej≤% mj } of A and b(i) ∈ Rm are
the corresponding elements of b.
Thus in each step of algorithm 3 at line 3, a linear program is solved:
+ 工(Z(j)y ∙ Q(i，j) + 工(Z(j)y ∙(Q力］Y(()
j∈{1,...,i-1}	j∈{i+1,...,n}
+	∑	(Z町∙ QMj)+ 工(Z⑶了 ∙ (QMj)H Y(() ≤ T
j∈{1,...,i-1}	j∈{i+1,...,n}
A (i) ∙ Z (i) ≤ b (i)
s^t. ( (PMr
And thus equipped with theorem F.2, algorithm 4 provides a more specific version of
algorithm 3 for SOlving BLCP.
Algorithm 4 BCFW with Line-Search on QCQP Product Domain
input Z0 ∈{Z I En=ι (Z(i))T ∙ PM + En=ι En= i (Z(i))T ∙ QMj) ∙ Z(j) ≤ T ; A Y ≤ b}
1:	for k = 0, . . . , K do
2:	Pick i at random in {1, . . . , n}
3:	Keep the same values for all other coordinate blocks Zk\+(i1) = Zk\(i) and update:
Zk+ι=argmin ((P f))T + 工(Z ⑺？ ∙ Qfj + 工(Z ⑺丫 ∙ (Q fjf
j∈{1,...,i-1}	j∈{i+1,...,n}
s.t.
A (i) ∙ S ≤ b (i)
(Z(j))τ ∙ QM) + 工(Z(j))τ ∙ (QMj)广
j∈{i+1,...,n}
• S
・ S ≤ T
4:	end for
In section 3, we deal with n = 2 blocks where Z = (α, β) such that:
Z⑴=α m 1 = D ∙ S . |C| PfI)= Pa PM = 0 A(1) = Ag b⑴=bg Qf,2) = Qae
Z(2) = β m2 = D • S P(f2) = Pβ P(M2) = 0 A(2) = AβS b(2) = bβS Q(M1,2) = Θ
(31)
Thus for this particular case of interest algorithm 4 effectively boils down to algorithm 1.
F.3.1 Proof of Theorem 1
Let us first compute the curvature constants Cf) (equation 24) and C； for the bilinear
objective function as in equation 25.
Lemma F.3. Let f have a bilinear form, such that:
f (2 ) = E n=1 (2 (i)) T ∙ P f) + E n=1 E n= i (ʃ (i)) J Qfj . ʃ j	Cf = 0 ∙
23
Under review as a conference paper at ICLR 2022
Proof. Separating the i-th coordinate block:
n	nn
f (x ) = E( x(I)) ∙ P f) + EE( x(I)) ∙ Q fj ∙ x ⑶	(32)
l=1	l=1 j=l
=(X(i)) T ∙ pf) + 工 卜(j)) T ∙ Qfj ∙ x(i) + 工	X(i) ∙ Qfj ∙ xj) (33)
j ∈{1,...,i-1}	j∈{i+1,...,n}
+ W 11=i (x(l)) T ∙ P f) + W 占 11=i ∙ Ij=i (x(l)) T ∙ Qfj ∙ x ⑺	(34)
where 1A is the indicator function that yields 1 if A holds and 0 otherwise.
Thus for y with y(i) = (1 - γ)x(i) + γs(i) and y\(i) = x\(i) , we have:
f (y) =	(y(i)) T	∙ Pf)	+ W	(y(j)) T ∙	Qfij ∙ y(i)	+ W	y(i)	∙	Qfij	∙	yj)	(35)
j∈{1,...,i-1}	j∈{i+1,...,n}
+ W 11=i (y(l)) T ∙ P f) + W W 11=i ∙ Ij=i (x(1)) T ∙ Qfj ∙ yj)	(36)
=(y(i)) T	∙ pf)	+ W	(x(j)) T ∙ Qfj	∙	y(i)	+ W	y(i)	∙ Qfj	∙ xj) (37)
j∈{1,...,i-1}	j∈{i+1,...,n}
n	nn
+ W 1l=i (x(l)) ∙ P f) + ww 1l=i ∙ Ij=i (x(l)) ∙ Qfj ∙ x(j)	(38)
Hence,
f (y) - f (X) = ▽(i) f (X) ∙ (y(i) - X(i))	(39)
since 34 and 38 cancel out and,
▽(i)f(x) = ((Pf))T +	W	(Xj))T ∙ Qfij) + W	(X(j))T ∙ (QfjT
j ∈{1,...,i-1}	j∈{i+1,...,n}
(40)
Hence we have,
n
Cfi) = 0 ∀i ∈ {1 ,...,n}	;	Cf = WCfi) = 0	(41)
i=1
□
Thus for a bilinear objective function, theorem F.1 boils down to:
Theorem F.4. For each k > 0 the iterate ζk Algorithm 4 satisfies:
2n
E[f (Zk)] - f (Z*) ≤ En (f (Z0) - f (Z*))
where Z* is the solution of problem 22 and the expectation is over the random choice of the
block i in the steps of the algorithm. Furthermore, there exists an iterate 0 ≤ k ≤ K of
Algorithm 4 with a duality gap bounded by E[g(Z^)] ≤ K6nι (f (Z0) — f (Z*)).
And by setting n = 2 with equations 31 for f(Z) := ACC(Z), theorem 3.2 follows.
G Sparsity Guarantees for Solving BLCP with BCFW with
Line-Search
In order to proof 3.3, we start with providing auxiliary lemmas proven at Nayman et al.
(2021). To this end we define the relaxed Multiple Choice Knapsack Problem (MCKP):
24
Under review as a conference paper at ICLR 2022
Definition G.1. Given n ∈ N, and a collection of k distinct covering subsets of {1, 2,…,n}
denoted as Ni, i ∈ {1, 2,…,k}, such that ∪k=ιNi = {1,2,…,n} and ∩k=ιNi = 0 with
associated values and costs pij , tij ∀i ∈ {1, . . . , k}, j ∈ Ni respectively, the relaxed Multiple
Choice Knapsack Problem (MCKP) is formulated as fol lowing:
max
vu
pijuij
i=1 j∈Ni
k
s.t. ∑∑ tij U ij≤T	(42)
i=1 j∈Ni
uij = 1	∀i ∈ {1, . . .,k}
j∈Ni
uij ≥ 0	∀i ∈ {1, . . . , k}, j ∈ Ni
where the binary constraints uij ∈ {0, 1} of the original MCKP formulation Kel lerer et al.
(2004)	are replaced with uij ≥ 0.
Definition G.2. An one-hot vector ui satisfies:
l∣u：Il0 = ∑ MF= ∑ Iu：j>0 = 1
j∈Ni	j∈Ni
where 1A is the indicator function that yields 1 if A holds and 0 otherwise.
Lemma G.1. The solution u* of the relaxed MCKP equation 42 is composed of vectors U*
that are all one-hot but a single one.
Lemma G.2. The single non one-hot vector of the solution u* of the relaxed MCKP
equation 42 has at most two nonzero elements.
See the proofs for Lemmas G.1 and G.1 in Nayman et al. (2021) (Appendix F).
In order to prove Theorem 3.3, we use Lemmas G.1 and G.1 for each coordinate block ζ(i)
for i ∈ {1, . . . , n} separately, based on the observation that at every iteration k = 0, . . . , K of
algorithm 4, each sub-problem (lines 3,5) forms a relaxed MCKP equation 42. Thus replacing
•	U in equation 42 with ζ(i).
•	p with (PfiY + Ej∈{1 ,. ,i-1}(Z(j)) J Qfij + Ej∈{i +1 …}(Z⑺)T ∙ (Q力T
•	The elements of t with the elements of
(PM)T +	∑	(Z⑺丫 ∙ QM) +	工(Z⑺丫 ∙ (Q*T
j∈{1,...,i-1}	j∈{i+1,...,n}
• The simplex constraints with the linear inequality constraints specified by A(i), b(i).
Hence for every iteration theorem 3.3 holds and in particular for the last iteration k = K
which is the output of solution of algorithm 4.
By setting n = 2 with equations 31 for f (Z) := ACC(Z), algorithm 4 boils down to algorithm 1
and thus theorem 3.3 holds for the later as special case of the former.
H On Transitivity of Ranking Correlations
While the predictors in section 3.2 yields high ranking correlation between the predicted
accuracy and the accuracy measured for a subnetwork of a given one-shot model, the ultimate
ranking correlation is with respect to the same architecture trained as a standalone from
scratch. Hence we are interested also in the transitivity of ranking correlation. Langford
et al. (2001) provides such transitivity property of the Pearson correlation between random
variables P, O, S standing for the predicted, the one-shot and the standalone accuracy
respectively:
ICor(P, S) - Cor(P, O) ∙ Cor(O, S)| ≤ √(1 - Cor(P, O)2) • (1 - Cor(O, S)2)	(43)
This is also true for the Spearman correlation as a Pearson correlation over the corresponding
ranking. Hence, while the accuracy estimator can be efficiently acquired for any given
25
Under review as a conference paper at ICLR 2022
one-shot model, the quality of this one-shot model contributes its part to the overall ranking
correlation. In this paper we use the official one-shot model provided by Nayman et al. (2021)
with a reported Spearman correlation of ρP,O = 0.99 to the standalone networks. Thus
together with the Spearman correlation of ρO,S = 0.97 of the proposed accuracy estimator,
the overall Spearman ranking correlation satisfies ρP,S ≥ 0.93.
26