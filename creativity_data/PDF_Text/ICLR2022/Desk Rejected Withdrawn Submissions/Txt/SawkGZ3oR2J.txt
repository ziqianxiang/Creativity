Under review as a conference paper at ICLR 2022
Accelerating Federated Split Learning via
Local-Loss-Based Training
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) operates based on model exchanges between the server
and the clients, and suffers from significant communication as well as client-side
computation burden. Emerging split learning (SL) solutions can reduce the client-
side computation burden by splitting the model architecture between the server
and the clients. However, SL-based ideas still require significant time delay, since
each participating client should wait for the backpropagated gradients from the
server in order to update its model. Also, the communication burden can still be
substantial, depending on various factors like local dataset size and shape of cut
layer activations/gradients. In this paper, we propose a new direction to FL/SL
based on updating the client/server-side models in parallel, via local-loss-based
training specifically geared to split learning. The parallel training of split models
substantially shortens latency while obviating server-to-clients communication. We
provide latency analysis that leads to optimal model cut as well as general guidelines
for splitting the model. We also provide a theoretical analysis for guaranteeing
convergence and understanding interplay among different hyperparameters and
system constraints. Extensive experimental results indicate that our scheme has
significant communication and latency advantages over existing FL and SL ideas.
1	Introduction
Federated learning (FL)(McMahan et al., 2017; Konecny et al., 2016b;a; Li et al., 2020) is being
regarded as a promising direction for distributed learning, as it enables clients to collaboratively
train a global model without directly uploading their data to the server. However, in FL, each client
should repeatedly download the entire model from the server, update the model, and upload it back
to the server. This training process in FL causes significant computation/communication burdens
especially with deep neural networks having large numbers of model parameters. Moreover, when
the computing powers and the transmission rates of the clients are low (e.g., mobile/IoT devices),
FL requires significant time delay for training the model. These issues can limit the application of
FL in practical scenarios aiming to train a large-scale model using local data of clients given low
computing powers and low transmission rates.
Split learning (SL) (Gupta & Raskar, 2018; Vepakomma et al., 2018; Singh et al., 2019; Koda et al.,
2020; Thapa et al., 2020) is another recent approach for this setup, which can reduce the computation
burden at the clients by splitting the model w into two parts before training begins: the first few
layers (client-side model wC) are allocated to the clients, and the remaining layers (server-side model
wS) are allocated to the server. Since each client only need to train the first few layers of the model,
the computational burden at each client is reduced compared to FL.
However, existing SL-based ideas still have two critical issues in terms of latency and communication
efficiency. First, existing SL solutions still require significant time delay, since each participating
client should wait for the backpropagated gradients from the server in order to update its model.
Moreover, the communication burden can still be substantial for transmitting the forward/backward
signals via uplink/downlink communications at each global round.
Contributions: In this paper, we propose a fast and communication-computation efficient solution
that provides a new direction to federated/split learning, by simultaneously reducing the following
three key resources in distributed learning: computation, communication, and latency. The computa-
tion burden at the clients is reduced by splitting the full model into the client-side and server-side
1
Under review as a conference paper at ICLR 2022
(b) SPIitFed
Figure 1: Model update process of FL, SplitFed and our idea. FL suffers from large time delay and communica-
tion/computation burdens for exchanging/UPdating the full model. Although SplitFed can reduce the client-side
computation burden, it still requires large time delay since the clients should wait for the backpropagated signals
from the server in order to update their model. The communication burden can be also large for transmitting
both the forward activations and backward gradients at every global round. The proposed idea enables to save all
three resources (computation, communication, latency) simultaneously via model splitting and local-loss-based
training highly tailored to split learning. Our approach has significant advantage especially when clients having
low computing powers and transmission rates (e.g., mobile/IoT devices) collaborate to train a large-scale model.
Full model
(a) Federated Learning (FL)
(C) Proposed
Parallel/decoupled
training
models, as in split learning. To handle the high communication resource requirement and high
latency requirement of current FL and SL approaches, we propose a local-loss-based training method
highly tailored to the split learning setup. By introducing two different local loss functions (i.e., the
client-side and server-side local losses), the client-side models can be updated without receiving
the backpropagated signals from the server, significantly improving communication efficiency and
latency. Fig. 1 compares our approach1 with FL and the state-of-the-art SL idea, termed SplitFed
(Thapa et al., 2020). Our main contributions can be summarized as follows:
•	We propose a new federated split learning algorithm that can simultaneously save the three
key resources (computation, communication, latency) of current FL/SL systems, via model
splitting and local-loss-based training specifically geared to the split learning setup.
•	We provide latency analysis and provide an optimal solution on splitting the model. We
also provide theoretical analysis to guarantee convergence of our scheme and to understand
interplay among key system constraints and hyperparameters in reaching the convergence.
•	We show via experiments that our approach outperforms existing FL and SL-based ideas in
practical setups where a number of devices having low computing powers and low transmission
rates collaborate to train a large-scale model.
2	Backgrounds and Related Works
Consider a system with a single server and N clients having their own local/private data. FL and
SL are the recent ideas that aim to train a model in this setup. The goal is generally to find W that
minimizes the loss function defined as F(W) = N PN=I Fk(W). Here, Fk(W) is the loss function
at client k defined as Fk(W) = ∣d1j Px∈d^ '(x； w) where Dk is the dataset of client k and '(χ; w)
is the loss computed by the model W and the data sample x.
Federated learning: In FL (McMahan et al., 2017; KOneCny et al., 2016b;a; Li et al., 2020), the
above problem is solved via repeated model download at the clients and aggregation at the server. At
every global round t, the server randomly selects At , a set of K clients participating in FL in this
round. Each client k ∈ At downloads the model Wt from the server and performs local update to
obtain w；+1. The server aggregates the models from all clients to obtain wt+1 = Pk∈A, wk+1
and moves on to the next round. This process is repeated until some stopping condition is met, e.g.,
when the model achieves a desired level of performance. It is shown in (Li et al., 2019) that this
algorithm converges to the optimal solution of the above objective function. However, when training
a large-scale model, FL causes significant communication burden between the server and the clients
for model exchange, and considerable computation burden for training the model at low-powered
clients (e.g., mobile devices), resulting in a large time delay for training.
Split learning: SL (Gupta & Raskar, 2018; Vepakomma et al., 2018; Singh et al., 2019; Koda et al.,
2020; Thapa et al., 2020) is another direction to train the model in this setup while reducing the
computation burden at the clients compared to FL. The basic idea of SL approaches is to split the
1Any aggregation rule (e.g., FedAvg) can be utilized for the model aggregation process of FL, SplitFed, and
our approach.
2
Under review as a conference paper at ICLR 2022
model w into two modules as w = [wC , wS]. The first few layers wC correspond to the client-side
model, and the remaining layers wS correspond to the server-side model. Among existing SL ideas,
SplitFed (Thapa et al., 2020) achieves the state-of-the-art performance by parallelizing SL; SplitFed
enables to train the model in a split learning setup with multiple clients updating their models in
parallel as in FL. At each global round t, the server randomly selects a set At that consists of K
participating clients in the current round. Each client k ∈ At downloads the client-side model wCt
from the server, performs forward propagation with its local data, and sends the output and the
corresponding labels to the server. The server proceeds forward propagation, computes the loss, and
performs backpropagation to update the server-side models in parallel, as in Fig. 1(b). Now the server
transmits the corresponding backpropagated signal to each client. Each client k can update the model
by proceeding backpropagation to obtain wCt ,k. Finally, the server aggregates the updated models
from all clients to obtain wt+1 =* Pk∈A古 wC+1∙ The Server-Side models are also aggregated to
obtain wSt+1 . It can be easily seen that this SplitFed algorithm also converges to the optimal solution
w* of the above objective function, but with a less computation burden at the clients compared to FL.
However, although SplitFed can reduce the client-side computation burden compared to FL, existing
SL-based ideas still have issues in terms of latency and communication efficiency; all participating
clients should receive the backpropagated signals from the server in order to update their models,
which require significant time delay and communication resources at each global round.
Local-loss-based training: Local-loss-based training (N0kland & Eidnes, 2019; Belilovsky et al.,
2019; 2020; Laskin et al., 2020; Xiong et al., 2020) represents schemes that aim to train a model
using local error signals (local loss functions) instead of using the conventional global loss function.
Auxiliary networks are utilized to compute the local error signals. By utilizing the local losses,
layer-wise training (N0kland & Eidnes, 2019; Belilovsky et al., 20l9) or module-wise training
(Belilovsky et al., 2020; Laskin et al., 2020; Xiong et al., 2020) is possible without receiving the
backpropagated signals from the previous layer or module. While existing works on local-loss-based
training consider a centralized setup (i.e., central node having the entire dataset and the model), in this
paper, we specifically focus on a distributed setup (i.e., data distributed across the clients and model
splitted between the client/server) and propose a local-loss-based training method highly tailored to
split learning. Theoretical and experimental results indicate that our new algorithm can provide a
new direction to federated/split learning via local-loss-based training.
Variants of SL: Instead of parallelizing the update process at the server (as in Fig. 1(b)), sequentially
updating the server-side model is also possible (Thapa et al., 2020); given a single server-side model,
it can be updated asynchronously via forward/backward propagation using each client’s output.
Although this sequential update process can speed up training in the beginning, the convergence to
the optimal solution is not guaranteed and often achieves a lower accuracy as shown later in Section
5. Note that SplitFed with this sequential update process also has the same issues on communication
efficiency and latency described above. Finally, we note that the authors of He et al. (2020) also
defines local loss functions in a SL setup but take a different approach. In He et al. (2020), the models
of the clients and the server are updated in an alternative way. In contrast, our work specifically focus
on training the client-side and the server-side models in parallel, which is motivated by the works on
local-loss-based training (Belilovsky et al., 2020) that trains each module of the full model in parallel
in a centralized setup. In other words, in our scheme, the client does not wait until the server-side
update is finished, and the server does not wait until the client-side model update is finished. Since
each client obtains different personalized models, the goal is different from our approach aiming to
obtain a single global model and thus is not directly comparable. Under this different approach/goal,
the latency analysis and convergence analysis are also unique to our work.
3	Proposed Algorithm
In this section, we describe our algorithm that simultaneously handles communication/computation
burden and latency issues of current FL and SL approaches. To reduce the computation burden at the
clients, we first split the model w into the client-side model wC and the server-side model wS, as in
SL. Our goal is to obtain the optimal model w* = [wC* , w*S] after training.
3.1	Local Loss Functions
In order to improve latency and communication efficiency, instead of considering the conventional
loss function that is computed at the output of the model w, our idea is to consider two different local
loss functions specifically geared to the split learning setup.
3
Under review as a conference paper at ICLR 2022
Client-side local loss function: We first describe the client-side local loss function. We introduce an
auxiliary network aC to make a prediction at the client-side and then compute the local loss. Here,
the auxiliary network aC is the extra layers connected to the client-side model wC ; the output of
wC becomes the input of aC. Both convolutional neural networks or multilayer perceptrons (MLP)
can be utilized for the auxiliary network aC, which is our design choice. In this work, we adopt a
MLP for aC as in (Belilovsky et al., 2020; Laskin et al., 2020) to match the dimension between the
output of aC and the target label. This auxiliary network enables to update the client-side model
independently of the layers at the server-side. When training is finished, the auxiliary network is
discarded and prediction is made with the full model w = [wC, wS]. We show later in Section 5 that
only a very small size auxiliary network is sufficient (0.1% of the entire model size |w|) to achieve
the state-of-the-art performance. Our goal is to find WC and aC that minimizes the client-side loss
function FC (∙, ∙) which is the average of local loss functions of all clients:
min FC(WC, aC) = min
wC ,aC	wC ,aC
1N
⅛ X
k=1
FC,k (WC, aC),
(1)
where Fc,k(∙, ∙) is the local loss function of the client-side model at client k, defined as
Fc,k(WC, ac) = ∣Dιj Px∈Dfc '(x; WC, a。). Here, '(χ; wc, ac) is the loss computed based on
input x, client-side model WC and the auxiliary network aC. Any loss function is applicable to our
work (e.g., cross-entropy loss).
Server-side local loss function: The local loss function of the server-side model WS is defined based
on the optimal client-side model WC defined in (1). We would like to find WS that minimizes the
server-side loss function FS(∙):
1N
min FS(WS) = min — ^FsS,ws, WC),	(2)
wS	wS N
k=1
where F5,k (∙, ∙) is the local loss function of the server-side model corresponding to client k, defined
as FS,k(ws, WC) = ∣Dk∣ Px∈Dk '(gw2 (x); WS). Here, note that the input of '(gw>c (x); WS) is
gw% (x), which is defined as the output of the model WC given the input x. We also note that the
auxiliary network is not necessary at the server-side; the loss can be directly computed without the
auxiliary network after finishing forward propagation of the server-side model.
3.2	Algorithm Description
Now we describe our algorithm to solve the above problem. Starting from the initial model W0 =
[WC0 , WS0], we obtain WT = [WTC, WST] after T global rounds. As in (Thapa et al., 2020), we consider
two different servers, the main server and the fed server. The main server updates WS while the fed
server only aggregates the models sent from the clients via FedAvg. The details will be clarified soon.
In the beginning of each global round t, the server randomly selects the participating group At with
K clients. Now we have the following four steps with steps 3 and 4 working in parallel.
Step 1 (Model download): At a specific global round t, each client k ∈ At downloads WCt and atC
from the fed server and lets WtC,k = WtC, atC,k = atC.
Step 2 (Forward propagation and upload): Based on the downloaded model WCt ,k, each client
k performs forward propagation for all data samples x ∈ Dk in a specific mini-batch Dk ⊂ Dk.
Specifically, client k obtains gwt (x) for all x ∈ Dk, which is the output of the model WCt ,k given
an input data x. Then each client k uploads gwt (x) to the main server for all x ∈ Dk.
Step 3 (Client-side model update and aggregation): Now based on the local loss function,
each client k updates its model WCt ,k and the auxiliary network atC,k as WCt+,k1 = WCt ,k -
nNwFC,k(WC,k, aC,k) and aC+1 = aC,k - %▽ aFC,®(WtC,k, aC,k), where ηt is the learning rate at
round t and 十Fgk (wc 卜,aC 卜)isthe derivative for a specific mini-batch, i.e.,十Fgk (wc 卜,aC k)=
∣Dq Px∈Dfc V'(x; WC k, aC 卜). After the model update process, the fed server aggregates the
client-side models as w『 = Kr P/fi.∈At wc+1. The auxiliary networks are also aggregated as
at+1 —工 P	at+1	t "
aC = K 乙k∈At aC,k .
4
Under review as a conference paper at ICLR 2022
Step 4 (Server-side model update and aggregation, working in parallel with step 3): While the
client-side models are being updated using the local errors, in parallel with step 3, the main server
also updates the server-side model. Based on gwt (x) received from each client k in step 2, the
main server performs model update according to wit+ = WS - nNFs,k (WS, WC) in parallel for
all k ∈ At, where VF⅛,k (WS, WC) = ∣^ Σ2χ∈D⅛ V'(gw^ Jx); WS). Now the server-side models
are aggregated as WS+1 = -K P,k∈At WS+1 We note that the model update process in steps 3 and 4
can be repeated for multiple mini-batches to obtain WCt+,k1 and WtS+,k1 before the aggregation process.
After repeating the overall procedure for T global rounds, we discard the auxiliary networks and
obtain the final global model WT = [WCT , WST]. This full global model is utilized at inference stage to
make predictions. The detailed procedure of our method is summarized in Algorithm 1 in Appendix.
3.3	Analysis: Computation, Communication and Latency
Notations and assumptions: Let |W| be the number of model parameters of W, and α be the fraction
of model parameters in WC: We have |wc| = α∣W∣ and |ws| = (1 一 α)∣W∣. We note that the
auxiliary network is our design choice which can be made to have a significantly small number of
parameters, i.e., |a|	|W|. Hence, we neglect the effect of the auxiliary network for latency analysis.
We show later in Section 5 that the state-of-the-art performance can be achieved with negligible size
of auxiliary network (0.1% of the entire model size |W|). For analysis, we assume that all layers have
the same size of q. Let PC and PS be the computing powers at the client and the server, respectively,
where PC PS holds. We assume that the computation time for model update is proportional to
the data size and the model size; for a given local dataset size |D|, model size |W| and computing
power P, the required time for updating the model for one epoch is assumed to be IDPWI. Here,
we assume that the required time for the forward propagation is e|DP|w| while the required time for
the backpropagation is (1-ePD||w|. We consider a full-batch update for analysis. Finally, given a
single client, we let both the uplink transmission rate from the client to server and the downlink
transmission rate from the server to client as R. When K clients are communicating with the server
simultaneously, the transmission rate of each client reduces to R.
Analysis for our scheme: In the beginning of each global round of our scheme, K clients simultane-
ously download WC from the fed server, which requires latency of a|R|K. The forward propagation
and transmitting the output to the main server requires additional latency of aeP||w| + q|D|K. Now
in parallel, each client updates its model and sends it to the fed server for aggregation, while the main
server updates the server-side model. The latency is determined by the maximum value of these two,
which leads to additional delay of max (α⅛K + a(1-P!D||w|, (1-°吧HK ).
Comparison with FL and SplitFed: Table 1 compares the key three metrics of different methods.
SplitFed and our method can have significantly smaller computation load at the clients compared to
FL. Since the proposed idea does not require downlink communication between the server and the
clients for transmitting the gradients, our scheme requires smaller communication load compared to
SplitFed. Here, the gap increases with more clients participating in each round (i.e., with a larger K).
Regarding the latency, we first highlight that regardless of the system parameters, our scheme has
lower latency compared to SplitFed. The gain increases with a larger K. Now we have the following
question: in which conditions are the SL-based methods (including our scheme) better than FL? With
some manipulations, it can be easily seen that SplitFed has lower latency compared to FL if and only
if PS > ( R2D∖ + PCK)-1, |w| > 2q^/(2RK + PCI 一 ¾k) and α < 1 一 02, where α1 = 2qz^
and α2 = (2RK + PDI - ■|DPK)|w|. Since our approach is always faster than SplitFed regardless of
the system parameters, our scheme is also faster than FL under these three conditions. We have the
following two observations from these conditions. First is the effect of model size |W|: it can be seen
from the second condition that the model size |W| should be larger than a certain threshold. It can be
also seen that the third condition can be easily satisfied with a larger |W|. Our second observation
is the impact of the client-side computing power PC and the transmission rate R: it can be seen
from all three conditions that as PC and R become smaller, SplitFed and our scheme have better
latency than FL even with a smaller computing power at the server (i.e., smaller PS) and with a
smaller model size (i.e., smaller |W|), regardless of α. These two observations confirm that SL-based
5
Under review as a conference paper at ICLR 2022
Table 1: Computation load (Per client), total communication load, and latency required for one global round.
Methods Computation	Communication
Latency (total cost)
FL
SplitFed
Ours
|D||w|
α∣D || w|
α∣D || w|
2|w|K
(2q∣D∣ +2α∣w∣)K
(q∣D∣ + 2α∣w∣)K
2|w|K + |D||w|
-R	1 PC
(2q∣D∣ + 2α∣w∣)K , α∣D∣∣w∣ . (1-α)∣D∣∣w∣K
R	1 pc	1	PS
(q|D|+a|wI)K , ae|D||w|	,	, ( αlwΓK I α(I-S)|D||w| (I-a)|D||w|K
R	1 PC +max (-R	1	PC	,	PS
methods (including our scheme) have advantage over FL when clients having low computing powers
and low transmission rates collaborate to train a large-scale model.
Optimal splitting: Now we have another important question: how should we split the model for
our scheme? In other words, what is the optimal α? The following theorem provides a guideline on
splitting the model to minimize latency of our scheme, where the proof is given in Appendix.
Theorem 1 If PS ≤ (r∣D∣ + PlK)-i, optimal α that minimizes the latency of our scheme is
ɑ* = 一i—ι—1 —、一. Otherwise, the overall latency is an increasing function of a.
ps{r∖d∖ + Pck J + 1
When the computing power of the main server PS is smaller than (RDJ + PeK )-i ,the optimal α de-
creases with decreasing client-side computing power PC, since larger latency is required for updating
the client-side model with a smaller PC. Moreover, α* decreases with decreasing transmission rate
R, since exchanging the model parameters between the server and the clients requires more latency
with a smaller R. If PS is larger than the threshold (Rdj + PeK)-i, it is beneficial to assign as
many layers as possible to the server in order to reduce latency.
Here, we note that the schemes relying on model splitting (including SplitFed and our scheme)
require transmissions of activations (at the cut-layer) and the corresponding labels to the server. This
may lead to a privacy issue due to data leakage. A detailed discussion regarding the privacy issue is
described in Appendix.
4 Convergence Analysis
In this section, we provide the convergence behavior of our scheme on non-convex loss functions with
the following standard assumptions inFL (Li et al., 2019; Reisizadeh et al., 2020) and local-loss-based
training (Belilovsky et al., 2020).
Assumption 1 The client-side and server-side loss functions are L-smooth, i.e., ∣∣VFc(W) 一
VFc(v)k ≤ LkW — Vk, ∣VFs(w) — VFS(v)k ≤ LkW — Vk holdforall W and v.
Assumption 2 The squared norm of the stochastic gradient is upper bounded, i.e.,
∣∣V'(x; WC k, aC k)k2 ≤ Gi for all k = 1, 2,...,K and t = 0,1,...,T — 1. Similarly, con-
sidering the server-side loss, we have ∣∣V'(gwt (x); WS 卜)∣2 ≤ G2 for all k = 1,2,...,K and
t = 0, 1, . . . , T — 1.
Assumption 3 The learning rates satisfy t ηt = ∞ and	t ηt2 < ∞.
The smoothness assumption (Assumption 1) is a standard assumption that holds for linear/logistic
regression models and neural networks with sigmoid activations. Assumption 2 is equivalent to
the assumption that the expected squared norm of gradient is uniformly bounded, which is again
commonly adopted in distributed/federated learning (Li et al., 2019; Yu et al., 2019; Stich et al., 2018;
Stich, 2019) and local-loss-based training (Belilovsky et al., 2020). The conditions in Assumption 3
(Robbins & Monro, 1951) can be easily satisfied by setting a diminishing learning rate as η =图.
1+t
In each global round t, the output distribution of a specific client-side model after forward propagation
(which is the input distribution of the server-side model) is determined by WtC,k and Dk. Let
zCt ,k = gwt (x) be the output of the k-th client-side model at global round t, following the
probability distribution of PC k(z). HerePC 卜(Z) is time-varying, and We let PC 卜(Z) be the output
distribution of the k-th client-side model with WC and Dk. We also define the distance between these
two distributions as dC k = k kpC k(Z) — PC k(z)kdz. Now we provide our main theorem which
C,k	C,k	C,k
shows the convergence behaviors of the client/server-side models. The proof is in Appendix.
6
Under review as a conference paper at ICLR 2022
(a) MNIST, IID
(b) FMNIST,IID	(C) CIFAR-10,IID
(d) MNIST, non-IID
(e) FMNIST, non-IID
(f) CIFAR-10, non-IID
Figure 2: Test aCCuraCy versus CommuniCation load.
Theorem 2 Suppose Assumptions 1 and 2 hold. Let ΓT = PtT=-01 ηt. After running Algorithm 1, the
client-side model converges as
ɪX1 ηtE[∣∣VFc(WC,aC)||2] ≤ 4(FC(WC,aC?M FC(WC,aC)) + G1Lɪ X1 η2,⑶
ΓT	3ΓT	2 ΓT
T t=0	T	T t=0
Moreover, the server-side model converges as
1 T-1
k EntE|||VFs(wS)||2] ≤
ΓT t=0
4(FS(WS) - FS(WS))
3Γt
T -1
+ G2
t=0
1 N t L 2
ntNTdak + 2nt ).
k=1	(4)
Consider a diminishing step sizes n =器 which satisfy the conditions in Assumption 3. Then, as
in the results of (Belilovsky et al., 2020), our algorithms Converges to a stationary point: it Can be
seen from (3) that the right-hand side converges to zero as T grows. Regarding the server-side model,
it can be seen from (4) that the sequence of expected gradient norm E[||VFS (WtS)||2] accumulates
around 0 as t≤≤nf ιE[llvFS(wS)||2] ≤ O(γT PT=O1 ntN PN=I dC,k)
This is in the same form
of the result in (Belilovsky et al., 2020) which considers the local-loss-based training method in a
centralized setup. The difference here is that our rate O(言 PT=o1 ntN PN=I d1c k) is expressed as
the average of distance dtC,k of all clients, since we consider a distributed setup with multiple clients.
Since there is only a single global loss function in existing FL and SplitFed frameworks (and thus
backpropagation is not decoupled), we note that the convergence behaviors of previous methods
follow equation (3). Compared to the existing approaches, in our work, two different local loss
functions are considered with decoupled/parallel training: the input distribution of the server-side
model is time-varying, leading to the result in equation (4) for the server-side model.
5	Experiments
We validate our algorithm on MNIST (LeCun et al., 1998), FMNIST (Xiao et al., 2017) and CIFAR-10
(Krizhevsky et al., 2009) datasets. MNIST and FMNIST are split into 60, 000 training samples and
10, 000 test samples, while CIFAR-10 is split into 50, 000 training samples and 10, 000 test samples.
For MNIST and FMNIST, we utilized a CNN having 5 convolutional layers and 3 fully connected
(FC) layers as in AlexNet. The number of model parameters is |W| = 3, 868, 170. For CIFAR-10, we
utilized VGG-11 with |W| = 9, 231, 114.
Data distribution: We distribute the training set of each dataset to the clients for training, and
utilized the original test set of each dataset to evaluate the performance of the global model. We
consider a system with N = 1000 clients. Hence, each client has 60 data samples for MNIST and
FMNIST, and 50 data samples for CIFAR-10. We consider two different data distribution setups, IID
and non-IID. In an IID setup, data samples from each class is equally distributed across all N = 1000
7
Under review as a conference paper at ICLR 2022
(a) MNIST
(b) FMNIST
Figure 3: Test accuracy versus training time in an IID setup.
(c) CIFAR-10
Table 2: Performance of different schemes at a specific time in Fig. 3.
MNIST	FMNIST	CIFAR-10
Methods ∣∣ IID Non-IID ∣∣ IID Non-IID ∣∣ IID Non-IID
FL	92.80%	89.02%	78.21%	75.77%	72.44%	62.84%
SplitFed	96.47%	95.47%	83.42%	82.44%	77.06%	75.02%
Proposed	97.73%	97.01%	86.70%	85.74%	80.72%	78.91%
clients in the system. Hence, each client has all 10 classes in its local dataset. In a non-IID setup,
similar to the data distribution method in (McMahan et al., 2017), the training set is first divided into
5000 shards (12 data samples in each shard for MNIST/FMNIST, and 10 data samples in each shard
for CIFAR-10). Then we allocate 5 shards to each client to model the non-IID scenario.
Baselines: We compare our result with FL and SplitFed (Thapa et al., 2020), the two well-known
frameworks that enable multiple clients to update their models in parallel. Although various model
aggregation methods are applicable to these schemes, for a fair comparison, we adopted FedAvg
(McMahan et al., 2017) for all three methods, FL, SplitFed and our idea. In FL, the entire model w is
updated at each client and sent to the server for aggregation via FedAvg. In SplitFed and the proposed
method, the model w is split into wC and wS, and the updated models are aggregated via FedAvg.
Model splitting and auxiliary network: For the CNN model that is utilized for MNIST and
FMNIST, we split the model and allocate the first 4 convolutional layers to the client, and the
remaining 1 convolutional layer and 3 FC layers to the server: we have |wC | = 387, 840 and
∣ws| = 3,480, 330, i.e., lwCl = 0.10. To minimize the computation burden at the clients, We let the
size of the auxiliary network aC to be significantly small; a single FC layer with 23, 050 parameters
is utilized as the auxiliary netWork aC at the end of wC, Which is 0.60% of the entire model size |w|.
For VGG-11 utilized for CIFAR-10, We split the model as |wC| = 972, 554 and |wS| = 8, 258, 560
to have ||^ = 0.11. AFC layer with 10, 250 parameters is utilized as the auxiliary network ac,
Which is 0.11% size of the entire model w.
Implementation details: At each global round, the server randomly samples K = 300 out of
N = 1000 clients in the system to participate. Cross-entropy loss is adopted for both the client-side
and server-side losses. We consider a fixed learning rate of 0.01 and a momentum of 0.9. The
mini-batch size is set to 10, and the number of epochs at each client is set to one: at each global
round, each client performs 6 local updates for MNIST, FMNIST and 5 local updates for CIFAR-10.
Test accuracy versus communication load: Fig. 2 shows the performance of each method as a
function of communication load in both IID and non-IID scenarios. With only a very small size
auxiliary network, the proposed idea performs better than SplitFed since the downlink communication
for transmitting the backpropagated signals is not required. FL has the worst performance since the
entire model w is transmitted between the server and the clients at every global round.
Test accuracy versus training time: In Fig. 3, we evaluate the test accuracy of each scheme as a
function of training time. The training time is evaluated by the latency results in Table 1, where
the parameters are set to PC = 1, PS = 100, R = 1, β = 0.2. Additional results with other
system parameters are shown in Appendix. Again, it can be seen that our scheme performs better
than SplitFed since each client can update the model directly by its local loss function, without
waiting for the backpropagated signal from the server. Moreover, FL requires significantly larger
communication/computation time compared to our method for transmitting/updating the full model
w at the clients. Table 2 compares the accuracy of each scheme at a specific time (1.5 × 1011 for
8
Under review as a conference paper at ICLR 2022
(c) PC = 1, R = 4
(d) PC = 10, R = 4
(a) PC = 1, R = 1	(b) PC = 10, R = 1
Figure 4:	Effect of client-side computing power PC and transmission rate R. FMNIST is utilized for training
CNN in a non-IID setup. Our scheme is beneficial especially when the clients have relatively small computing
powers and small transmission rates (e.g., mobile/IoT devices).
75
70
65
60
Communication load [TB]
(a) Comm. load, IID
Aoe.Inge»8
γwΓ**i
EProPoSed------Proposed ver. 2
SPlitFed--SplitFed ver. 2
55 Ia ，-------1-----'-----l-----
0	0.5	1	1.5	2	2.5
Communication load [TB]
55
0
80
60
575
打0
E
W 65
2	4	6	8
Training time *1011
5	10	15
Training time χio11
----Proposed-----Proposed ver. 2
----SplitFed-----SplitFed ver. 2
一FL
55
(b) Comm. load, non-IID
(c) Training time, IID
(d) Training time, non-IID
0
Figure 5:	Effect of sequential update process at the server. CIFAR-10 is utilized for training VGG-11.
MNIST, 2.5 × 106 * * * * 11 for FMNIST, and 8 × 1011 for CIFAR-10). The overall results are consistent
with the results in Fig. 3, confirming significant advantage of the proposed idea.
Effect of client-side computing power PC and transmission rate R: In Fig. 4, we observe the
performance of each scheme depending on two important parameters, the client-side computing
power PC and the transmission rate R. Other parameters are set to be same as in Fig. 3. If both
PC and R are small, FL requires significant computation/communication time and thus achieves
lower performance compared to others. However, as PC and R increases, FL shows comparable
performance with our method since updating the entire model at the clients and transmitting the
entire model does not require significant delay with large PC and R. The overall results confirm the
advantage of our scheme in practical scenarios where clients having low computing powers (small
PC) and low transmission rates (small R) aim to train a shared global model.
Variants of SplitFed and proposed scheme: Instead of parallelizing the server-side update process
of SplitFed and our scheme, one can think of updating the server-side model sequentially in the order
of arrivals of the results from the clients (Thapa et al., 2020). Fig. 5 shows the results of this idea
where “Proposed ver. 2” and “SplitFed ver. 2” denote schemes that sequential server-side update
process is applied to our idea and SplitFed, respectively. The parameters are set to be same as in
Fig. 3. As expected, although the sequential update method can speed up training in the beginning,
the convergence to the optimal solution is not theoretically guaranteed and thus achieves a lower
accuracy. One can also consider vanilla SplitNN (Gupta & Raskar, 2018) where only one client
participates in each global round under the splitted model setup. Note that compared to FL, SplitFed
and our scheme, parallel client computation is not available for this conventional SplitNN. Even in
this case, our local-loss-based training geared to split learning can be applied to the training process
of a participating client to improve the communication efficiency and latency of vanilla SplitNN.
Other experimental results: Various other experimental results with different model splitting,
different number of local updates, and different system parameters are given in Appendix.
6 Conclusion
We proposed a new federated split learning scheme that is both fast and efficient in terms of commu-
nication/computation burdens. The key idea is to update the client-side and server-side models in
parallel via local-loss-based training highly tailored to split learning. We provided an optimal solution
on splitting the model to minimize the latency, and presented a theoretical analysis that guarantees
convergence of the proposed method. Extensive experimental results confirmed the advantage of
our idea compared to FL and SplitFed. We believe that our results provide a new direction to the
federated/split learning community for training a large-scale model in practical settings.
9
Under review as a conference paper at ICLR 2022
References
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale
to imagenet. In International conference on machine learning, pp. 583-593. PMLR, 2019.
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of cnns.
In International Conference on Machine Learning, pp. 736-745. PMLR, 2020.
Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents.
Journal of Network and Computer Applications, 116:1-8, 2018.
Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Federated
learning of large cnns at the edge. Advances in Neural Information Processing Systems, 33, 2020.
Yusuke Koda, Jihong Park, Mehdi Bennis, Koji Yamamoto, Takayuki Nishio, Masahiro Morikura,
and Kota Nakashima. Communication-efficient multimodal split learning for mmwave received
power prediction. IEEE Communications Letters, 24(6):1284-1288, 2020.
Jakub Konecny, H Brendan McMahan, Daniel Ramage, and Peter Rich饬rik. Federated optimization:
Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016a.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016b.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Michael Laskin, Luke Metz, Seth Nabarrao, Mark Saroufim, Badreddine Noune, Carlo Luschi, Jascha
Sohl-Dickstein, and Pieter Abbeel. Parallel training of deep networks with local updates. arXiv
preprint arXiv:2012.03837, 2020.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2019.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282, 2017.
Arild N0kland and Lars Hiller Eidnes. Training neural networks with local error signals. In
International Conference on Machine Learning, pp. 4839-4850. PMLR, 2019.
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani.
Fedpaq: A communication-efficient federated learning method with periodic averaging and quanti-
zation. In International Conference on Artificial Intelligence and Statistics, pp. 2021-2031. PMLR,
2020.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. Detailed comparison of
communication efficiency of split learning and federated learning. arXiv preprint arXiv:1909.09145,
2019.
Sebastian U Stich. Local sgd converges fast and communicates little. In International Conference on
Learning Representations, 2019.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. Advances
in Neural Information Processing Systems, 31:4447-4458, 2018.
10
Under review as a conference paper at ICLR 2022
Chandra Thapa, Mahawaga Arachchige Pathum Chamikara, and Seyit Camtepe. Splitfed: When
federated learning meets split learning. arXiv preprint arXiv:2004.12088, 2020.
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health:
Distributed deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564,
2018.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Yuwen Xiong, Mengye Ren, and Raquel Urtasun. Loco: Local contrastive representation learning.
Advances in Neural Information Processing Systems, 33, 2020.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019.
11
Under review as a conference paper at ICLR 2022
A Algorithm Description
The detailed procedure of our method is summarized in Algorithm 1.
Algorithm 1 Proposed Training Algorithm
Input: Initialized model w0 = [w0C , wS0 ] and the auxiliary network a0C
Output: Model wT = [wCT , wST] after T global rounds
for each global round t = 0, 1, ..., T - 1 do
Step 1 and Step 2: Model download, forward propagation and upload
1:	for each client k ∈ At in parallel do
2:	Download wtC and atC from the fed server and let wCt ,k = wtC, atC,k = atC
3:	Calculate gwt (x) for all X ∈ Dk	// Forward propagation
wC,k
4:	Send gw% (x) for all X ∈ Dk and the corresponding labels to the main server
5:	end for
Step 3: Client update and aggregation
1:	for each client k ∈ At in parallel do
2:	wCk1 = WCk — ηt十WFc,k(wc,k, aC,k)	// Update client model with client-side local loss
3:	aC+1 = aC,k - ηt十aFc,k(wC,k, aC,k)	// Update auxiliary network with client-side local loss
4:	Transmit wt+1 and at+1 to the fed server
C,k	C,k
5:	end for
6:	wC+1 = K Pk∈At wtC+,k1	// Aggregation of client-side models
7:	aC+1 = K Pk∈At aC+k	// Aggregation of auxiliary networks
Step 4: Server update and aggregation (in parallel with Step 3)
1:	for k ∈ At in parallel do
2:	Receive gwt (X) from each client k
3:	wS+11 = WS — ηt57Fs,k (WS, WC)	// Update server model with server-side local loss
4:	end fo,r
5:	WSt+1 =K1 Pk∈At WtS+,k1	// Aggregation of server-side models
end for
B Proof of Theorem 1
Case 1:	We first consider the case with
α≥
(5)
PS (RIDi + P⅛β) + 1
which is equivalent to alwRK + "1-¾DlH ≥ (Ir)PDIHK.	Hence, we have
max (αwRκ + a(1-瞿DIH , (I-a)|D1|w|K) = α2Rκ + a(1-瞿Dllwl. Now the latency of our
scheme can be rewritten as
(q|D| + α∣w∣)K
+ αβ∣D∣∣w∣ + α∣w∣K + α(1 — β)∣D∣∣w∣
PC	R	PC
(6)
which is an increasing function of α. To minimize this latency in the range of α ≥
the optimal solution is α
PS ( R1D|+ P1-β ) + 1
Case 2:	Now consider
ps(r⅛ + P1-β ) + 1
α≤
+1
(7)
R
1
1
1
1
which leads to	当K	+	α"lDlH	≤	(Ir)FIHK,	i.e.,
R	PC	PS
max (α⅞κ + α(1-pylH , (I-agllwlκ) = (ITIHK.	In this case, the latency of
12
Under review as a conference paper at ICLR 2022
our scheme becomes
(|w|K , βDIW	|D||w|Kʌ	, q|D| , |D||w|K
lɪ+-P----------P-α++K+
(8)
Here, if PS ≤ (r^ + PeK)-1, the latency is a decreasing function of a and the optimal solution
minimizing the latency becomes α =——1----1--K--in the range of ɑ ≤ 一彳--------1	、一.
Ps(r⅛+P1-β) + 1	- PS(RDI + P-β)+1
Otherwise, i.e., PS > (Rd∣ + PeK)-1, latency is an increasing function of α.
Now we combine the results of both case 1 and case 2. If PS ≤ ( rD∣ + PeK )-1 ,the optimal solution
minimizing the latency becomes a = L-1 「、-. Otherwise, i.e., if PS > (rD∣ + PeK)-1,
pS{R∣D∣ + PCK ) + 1	1 1	C
the latency is an increasing function ofα, which completes the proof.
C Proof of Theorem 2
For notational simplicity, we let fC,k(wCt ) := FC,k (wtC, atC) and fS,k(wSt ) := FS,k(wSt ,wCt ).
C.1 Convergence of client-side model
Due to the L-smoothness of client-side loss function, we can write
FC(wtc1) ≤ Fe(WC) + VFC(WC)T(Wt+1 - WC) + 2kwt+1 - WCk2.	⑼
Note that we have
wm1 = K X (WC -ηtVfC,k (WC))	(IO)
k∈At
=wC -ηt-K X VfC,k(WC)	(II)
k∈At
1	V—7 f f t ∖	1	~~>	X_7 /1 /	∖ Γ∙	♦	∙ ∙ 1 . 1 T-Λ 一 7^Λ T T
where VfC,k(We)=击 Ex∈Dk V'(x; WC,k) for a given mini-batch Dk ⊂ Dk. Hence, we can
rewrite equation 9 as follows:
FC(Wt+I) ≤ FC(wC) - ηtVFe(wC)T (K X VfC,k(wC)) + 2η2
k∈At
2
K X VfC,k(wC)	.
k∈At
(12)
Now by taking the expectations at both sides of equation 12, we have
E[FC(wt+1)] ≤ E[FC(wC)] - ηt EhVFe(wC)T(K X VfC,k(wC))i
k∈At
(13)
+
B2
(14)
In the following, we will bound B1 and B2, respectively.
We first consider B1 . By defining X as X
X = K X (V fC,k(WC) - VfC,k (We)),
k∈At
(15)
13
Under review as a conference paper at ICLR 2022
we can find the lower bound of B1 as
Bi =EhVFC(WC)T (KK X Vfc,k(WC川	(16)
k∈At
=EhVFC(WC)T(X + K X Vfc,k(WC川	(17)
k∈At
≥ EhVFC(WC)T(K X Vfa,k(wC川-J∣E[VFc(WC)TX]∣[.	(18)
._________________k∈At______________}	'	{Z	}
'^^^^^^^^^^^^^^^^^^^^^^^{ι^^^^^^^^^^^^^^^^^^^^^^^}}
C1
Since fC,k(WCt ) := FC,k(WCt , atC) and At is chosen uniformly at random among N clients in the
system, by the law of total expectation, we can write
C1 = E[kVFC(WCt )k2].	(19)
Now we consider C2. Note that since VfC,k(WCt ) is an unbiased estimator of VfC,k(WCt ), we have
kE[Vfo,k(WC) - VfC,k(wC)]∣∣ = 0.	(20)
We also note that E[UTV] ≤ 4E[∣∣U∣∣2] + E[k V∣∣2] holds for any vectors U and V. Hence, We have
C2 = E[VFC(WCt )TX]	(21)
=∣∣E [E [VFc(wC)tX∣Ω]]∣∣	(22)
=∣∣E [VFc(wC)tE [X∣Ω]]∣∣	(23)
≤ 4E[∣VFc(wC)k2]+ E h∣E [X∣Ω]∣2i	(24)
=1 E[∣VFc (wC )k2].	(25)
Where the last equality comes from equation 20.
By inserting equation 19 and equation 25 to equation 18, We obtain
Bi= EhVFC(wC)t(K X Vfc,k(wC))] ≥ 4E[∣VFc(WC)∣2].	(26)
k∈At
NoW We consider B2 in equation 13. We can bound B2 as
B2 =	∣ɪ X Vfc,k(wC)∣2		(27)
	k∈At		
≤ (a)	K X ∣VfC,k(wC)k2 k∈At ∣	2	(28)
=	K X∣ 仔 X V'(x; WC) K k∈At ∣∣ lDk 1 χ∈Dk		(29)
≤ (b)	K Xt ⅛ XXk ∣∣";WC )∣∣2		(30)
≤ (c)	Gi		(31)
Where (a) and (b) comes from the Cauchy-SchWarz inequality and (c) comes from Assumption 2.
By inserting equation 26 and equation 31 to equation 13, We obtain
E[Fc(wC+1)] ≤ E[Fc(wC)] — 3ηtE[∣∣VFc(WC)||2] + GLIJ	(32)
14
Under review as a conference paper at ICLR 2022
Now by summing up for all global rounds t = 0, 1, ...T - 1, we have
3 T-1	G L T-1
E[Fc (WC)] ≤ E[Fc (WC)] - 4 ∑ ηtE[∣∣VFc (WC )||2] + GrNn2
(33)
Finally from FC (WC) ≤ E[Fc (WC)], We can write
1 T-1
厂 EntE|||VFc(WC)||2] ≤
ΓT t=0
4(FC (WC ) — FC (WC D
3Γt
G1L 1
-2-ΓT
T-1
X nt2
t=0
(34)
+
which completes the proof for the client-side model.
C.2 Convergence of server-side model
Due to the L-smoothness of the server-side loss function, following the same procedure of the
client-side model, we have
E[Fs(wS+1)] ≤ E[Fs(wS)] — nt EhVFS(WS)t (" X Vfs,k(WS))]	(35)
k∈At
'---------------{z---------------}
B3
+ LntE[kKK X Vfs,k(wS)k2]	(36)
k∈At
X----------{----------}
B4
A	V—7 C / ≠ ∖	1 X^~'	X—7 f} /	/ ∖	≠ ∖ ∕`	♦	∙ ∙ 1 . A T~∖ 一 7^Λ ɪʃ T	∙11
where Vfs,k(wts)=武 Ex∈Dk V'(gwC,k (x); WS) for a given mini-batch Dk ⊂ Dk. We Will
derive the bounds of B3 and B4 .
We first consider B3 . By defining X as
X = K X (VfS,k(wtS) — VfS,k(wS)),	(37)
k∈At
we can write
EhVFS(wS)t(KK X VfS,k(wS))]	(38)
k∈At
=EhVFS(wS)t(X + KK X VfS,k(wS))]	(39)
k∈At
≥ EhVFS(wS)t(K X VfS,k(wS))] - kE[VFS(WS)tX]k	(40)
k∈At
≥ EhVFS(wS)T(K X VFS,k(wS))] + EhVFS(wS)T(K X (VfS,k(wS)-VFS,k(wS)))]
k∈At	k∈At
〜	一	{Z
C1	C2
(41)
— kE[VFS(wtS)TX] k	(42)
V------V-------}
C3
As can be seen in the derivation for the client-side model, we have
C1 = E[kVFS(wSt )k2]	(43)
and
C3 ≤ 4E[kVFS(wS)k2].	(44)
15
Under review as a conference paper at ICLR 2022
Now we analyze C2 . We have
▽Fs(WS)T (κ X "fs,k (WS) - vF⅛,k(WS》)
k∈At
≥ - ▽FS (WS )T (K X (vfS,k (WS ) -VFsMwS )))
k∈At
≥ - IIvFS (wS )11 K X "fS,k(WS) - vFS,k (wS ))
k∈At
≥ -PG2 κ X (vfS,k(wS) - vFS,k(wS))
I k∈At	I
≥ -PG2 K X ||VfS,k (wS) - VFS,k(wS )l∣
k∈At
=-PG2 K XI ∣D^i X v'(gwc (x); wS) -1D^ X v'(gwc (χ); wS)
k∈At	x∈Dk	x∈Dk
/ v`(z; wS )pCk(z)dz
≥-PG2K X / ∣∣v'(z;wS)l∣pC,k(Z)-p*a,k(z)∣l dz
≥ -G2 K X dC,k.
k∈At
(45)
(46)
(47)
(48)
(49)
(50)
(51)
(52)
(53)
Now we can write
C2 ≥ -E G2 K X dC,K	(54)
k∈At
1N
=-G2 N E dc,k	(55)
since At chosen uniformly at random among N clients in the system.
By utilizing the results of C1, C2, C3, we have
1	3	1N
B3 = E [VFS (wS) (K X V fS,k (wS)) ] ≥ 4 E[kvFS (wS )k ] - G2 N X dC,k	(56)
Following the same procedure of the client-side model, for B4 , we have
B4 = k⅛ X VfS,k(wS)k2 ≤ G2.	(57)
k∈At
Now by inserting the results of equation 56 and equation 57 to equation 35, we have
3	1 N	LG
E[FS(wS+1)] ≤ E[FS(wS)] - -ηtE[∣∣vFS(wS)||2] + ηtG2N 耳dC,k + -Gη2∙	(58)
Summing up for all global rounds t = 0,1, ...T - 1, we have
-T-1	T-1	1 N	-
E[FS(wC)] ≤ E[FS(wC)] - 4 E ηtE[∣∣vFS(wC)∣∣2] + G2Ent N ∑dC,k + 2 叫.
t=0
t=0	k=1
(59)
16
Under review as a conference paper at ICLR 2022
Finally from FS(WS) ≤ E[Fs(WT)], We can write
1 T-1
M ∑ηtE[∣∣VFs(wS川2] ≤
ΓT t=0
4(FS(WS) - FS(WS))
3ΓT
+G2 γT x 卜 t N X dc,k+2 褚)
T t=0	k=1
(60)
which completes the proof for the server-side model.
D Additional Experimental Results
D. 1 Results with different model splitting
To confirm the advantage of the proposed idea further, we performed additional experiments with
different model splitting. Here, we allocate more layers to the clients compared to the setup in the
main manuscript: for training CNN with MNIST and FMNIST, we allocate 5 convolutional layers to
the client and the remaining 3 fully connected (FC) layers to the server: we have |WC | = 997, 920,
|ws| = 2,890, 250, i.e., lwCl = 0.26. A single FC layer with 23,050 parameters is utilized as the
auxiliary network at the end of WC. Hence, the size of the auxiliary network is 0.60% of the full
model W.
In Fig. 6, we plot the test accuracy versus communication load for all schemes. Since more layers
are allocated to the clients compared to the setup in the main manuscript, the communication load
is increased for both our scheme and SplitFed. Hence, the gap between the SL-based methods
(including our scheme) and FL is reduced. We also note that the gap between our scheme and
SplitFed is reduced, since the size of the client-side model |WC| becomes dominant compared to the
size of the cut layer. Our scheme still performs the best, confirming the advantage of our method that
enables the clients to update their models without receiving the backpropagated gradients from the
server.
AOE-lngE -sθl
100
90
....................... O
OOoooo
8 7 6 5 4 3
----Proposed
——SPIitFed
FL
90
80
70
6 60
5o 50
40 40
30
20
0	0.1	0.2	0.3	0.4	0.5
Communication load [TB]
(b) FMNIST,IID
0.1	0.2	0.3
Communication load [TB]
(a) MNIST, IID
Oooooooo
09876543
-n8ISθl
(c) MNIST, non-IID
(d) FMNIST, non-IID
Figure	6: Test accuracy versus communication load. Here, we split the model in a different way compared
to the setup in the main manuscript: more layers are allocated to the clients compared to the plots the main
manuscript.
Fig. 7	shows the test accuracy of different schemes as a function of training time. The parameters are
set to PC = 1, PS = 100, R = 1, β = 0.2. In Table 3, we also compare the test accuracy of each
17
Under review as a conference paper at ICLR 2022
l0OoOoooo
09876543
-n8ISθl
----Proposed
——SPIitFed
FL
10
Training time
15
×1010
(b) FMNIST, IID
(a) MNIST, IID
5
Oooooooo
09876543
-n8θl
---Proposed
——SPIitFed
FL
15
x1010
(d) FMNIST, non-IID
5
10
Training time
(c) MNIST, non-IID
Figure 7:	Test accuracy versus training time. Here, we split the model in a different way compared to the setup
in the main manuscript: more layers are allocated to the clients compared to the plots the main manuscript.
Table 3: Performance of different schemes at a specific time in Fig. 7.
Il MNIST	FMNIST
Methods ∣∣ IID Non-IID ∣∣ IID Non-IID
FL	92.80%	89.02%	78.21%	75.77%
SPlitFed	97.22%	96.83%	83.92%	83.31%
Proposed	98.07%	98.05%	87.48%	87.11%
scheme at a specific time (1.5 × 1011 for MNIST and 2.5 × 1011 for FMNIST). The overall results
confirm the advantage of our local-loss-based training tailored to the split learning setup.
D.2 Results with different system parameters
In Fig. 8, we provide additional results with varying client-side computing power PC and transmission
rate R. The model is splitted as in subsection D.1, and other system parameters are the same as in
Fig. 7. As in the results in the main manuscript, it can be seen that our scheme is always better than
SplitFed since each client can update the model directly by its local loss function. Moreover, as PC
and R increase, FL can have better performance compared to SplitFed. In practical settings where
the clients have relatively low computing powers and transmission rates (e.g., mobile/IoT devices),
our scheme outperforms existing FL and SL-based ideas.
D.3 Results with different number of local updates
To see the effect of number of local udpates, we performed additional experiments with CIFAR-10 by
increasing the number of local epochs from 1 to 5. Hence, the number of local updates is increased
from 5 to 25 compared to the setup in the main manuscript. We consider a non-IID setup where other
setups are exactly the same as in the main manuscript. After consuming 2TB of communication load,
the accuracies are 73.47%, 68.85%, 41.95% for our scheme, SplitFed, FL with 25 local updates. In
contrast, the accuracies are 79.01%, 75.06%, 57.34% with only 5 local updates. It can be seen that
the performance of all schemes (including our method) are degraded with more local updates, which
18
Under review as a conference paper at ICLR 2022
30
0
OOooooo
0 9 8 7 6 5 4
AONnooE Isol
5	10	15
Training time	χio10
(a) Pc =1,R = 1
......................O
Oooooooo
09876543
AONnooE ISoI
(b) PC = 10, R = 1
(c) Pc = 1, R = 4
(d) Pc = 10, R = 4
Figure 8:	Test accuracy with varying system parameters. Here, we split the model in a different way compared
to the setup in the main manuscript: more layers are allocated to the clients compared to the plots the main
manuscript. MNIST is utilized for training CNN in an IID setup. Our scheme is beneficial especially when the
computing powers and and the transmission rates of the clients are low (e.g., mobile/IoT devices).
leads to a slower convergence. This is because the local models become more biased in a non-IID
setup when the number of local updates is too large. Hence, setting the number of local updates too
large should be avoided. The trend is consistent with the results in the main manuscript, confirming
the advantage of the proposed approach.
19