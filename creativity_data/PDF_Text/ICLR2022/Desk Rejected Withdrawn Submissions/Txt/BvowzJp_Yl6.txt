Under review as a conference paper at ICLR 2022
Homogeneous Learning: Self-Attention De-
centralized Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) has been facilitating privacy-preserving deep learning in
many walks of life such as medical image classification, network intrusion detec-
tion, and so forth. Whereas it necessitates a central parameter server for model
aggregation, which brings about delayed model communication and vulnerabil-
ity to adversarial attacks. A fully decentralized architecture like Swarm Learning
allows peer-to-peer communication among distributed nodes, without the central
server. One of the most challenging issues in decentralized deep learning is that
data owned by each node are usually non-independent and identically distributed
(non-IID), causing time-consuming convergence of model training. To this end,
we propose a decentralized learning model called Homogeneous Learning (HL)
for tackling non-IID data with a self-attention mechanism. In HL, training per-
forms on each round’s selected node, and the trained model of a node is sent to
the next selected node at the end of each round. Notably, for the selection, the
self-attention mechanism leverages reinforcement learning to observe a node’s in-
ner state and its surrounding environment’s state, and find out which node should
be selected to optimize the training. We evaluate our method with various sce-
narios for two different image classification tasks. The result suggests that HL
can produce a better performance compared with standalone learning and greatly
reduce both the total training rounds by 50.8% and the communication cost by
74.6% compared with random policy-based decentralized learning for training on
non-IID data.
1	Introduction
Decentralized deep learning (DDL) is a concept to bring together distributed data sources and com-
puting resources while taking the full advantage of deep learning models. Nowadays, DDL such
as Federated Learning (FL) (Konecny et al., 2016) has been offering promising solutions to social
issues surrounding data privacy, especially in large-scale multi-agent learning. These massively
distributed nodes can facilitate diverse use cases, such as industrial IoT (Parimala et al., 2021), envi-
ronment monitoring with smart sensors (Gao et al., 2020), human behavior recognition with surveil-
lance cameras (Liu et al., 2020), connected autonomous vehicles control (Pokhrel & Choi, 2020;
Liu et al., 2019), federated network intrusion detection (Sun et al., 2021; Rahman et al., 2020), and
so forth.
Though FL has been attracting great attention due to the privacy-preserving architecture, recent
years’ upticks in adversarial attacks cause its hardly guaranteed trustworthiness. FL encounters var-
ious threats, such as backdoor attacks (McMahan et al., 2018; Cao et al., 2019; Nguyen et al., 2020),
information stealing attacks (Duan et al., 2021), and so on. On the contrast, fully decentralized
architectures like Swarm Learning (SL) (Warnat-Herresthal et al., 2021) leverages the blockchain,
smart contract, and other state-of-the-art decentralization technologies to offer a more practical so-
lution. Whereas, a great challenge of it has been deteriorated performance in model training with
non-independent identically distributed (non-IID) data, leading to extremely increased time of model
convergence.
Our contributions. We propose a self-attention decentralized deep learning model called Homo-
geneous Learning (HL). HL leverages a shared communication policy for adaptive model sharing
among nodes. A starter node initiates a training task and by iteratively sending the trained model
1
Under review as a conference paper at ICLR 2022
and performing training on each round’s selected node its model is updated for achieving the train-
ing goal. Notably, a node selection decision is made by reinforcement learning agents based on
the current selected node’s inner state and outer state of its surrounding environment to maximize
a reward for moving towards the training goal. Finally, comprehensive experiments and evaluation
results suggest that HL can accelerate the model training on non-IID data with 50.8% fewer total
training rounds and reduce the associated communication cost by 74.6%.
Paper outline. This paper is organized as follows. Section 2 demonstrates the most recent work
about DDL and methodologies for tackling data heterogeneity problems in model training. Section
3 presents the technical underpinnings of Homogeneous Learning, including the privacy-preserving
decentralized learning architecture and the self-attention mechanism using reinforcement learning
agents. Section 4 demonstrates experimental evaluations. Section 5 concludes the paper and gives
out future directions of this work.
2	Related Work
Decentralized Deep Learning. In recent years, lots of DDL architectures have been proposed
leveraging decentralization technologies such as the blockchain and ad hoc networks. For instance,
Li et al. (2021) presented a blockchain-based decentralized learning framework based on the FISCO
blockchain system. They applied the architecture to train AlexNet models on the FEMNIST dataset.
Similarly, Lu et al. (2020) demonstrated a blockchain empowered secure data sharing architecture
for FL in industrial IoT. Furthermore, Mowla et al. (2020) proposed a client group prioritization
technique leveraging the Dempster-Shafer theory for unmanned aerial vehicles (UAVs) in flying ad-
hoc networks. HL is a fully decentralized machine learning model sharing architecture based on
decentralization technology such as token exchanges.
Convergence Optimization. In a real-life application, usually data owned by different clients in
such a decentralized system are skewed. For this reason, the model training is slow and even di-
verges. Methodologies for tackling such data heterogeneity such as FL, have been studied for a long
time. For example, Sener & Savarese (2018) presented the K-Center clustering algorithm which
aims to find a representative subset of data from a very large collection such that the performance of
the model based on the small subset and that based on the whole collection will be as close as pos-
sible. Moreover, Wang et al. (2020) demonstrated reinforcement learning-based client selection in
FL, which counterbalances the bias introduced by non-IID data thus speeding up the global model’s
convergence. Sun et al. (2021) proposed the Segmented-FL to tackle heterogeneity in massively
distributed network intrusion traffic data, where clients with highly skewed training data are dynam-
ically divided into different groups for model aggregation respectively at each round. Furthermore,
Zhao et al. (2018) presented a data-sharing strategy in FL by creating a small data subset globally
shared between all the clients. Likewise, Jeong et al. (2018) proposed the federated augmentation
where each client augments its local training data using a generative neural network. Different from
the aforementioned approaches, HL leverages a self-attention mechanism that optimizes the commu-
nication policy in DDL using reinforcement learning models. It is aimed to reduces computational
and communication cost of decentralized training on skewed data.
3	Homogeneous Learning
3.1	Preliminary
Data Privacy and Decentralized Deep Learning. Centralized deep learning in high performance
computing (HPC) environments has been facilitating the advancement in various areas such as drug
discovery, disease diagnosis, cybersecurity, and so on. Despite its broad applications in many walks
of life, the associated potential data exposure of training sources and privacy regulation violation
have greatly decreased the practicality of such centralized learning architecture. In particular, with
the promotion of GDPR (EU), data collection for centralized model training has become more and
more difficult. For this reason, Google proposed federated learning (FL) to alleviate the limitation of
model training on distributed data. FL allows a client to train its own model based on a local dataset
and achieve a better performance by sharing the training result with others, whereas without sharing
the raw training data. FL quickly acquired intense attention from lots of fields related to sensitive
2
Under review as a conference paper at ICLR 2022
data processing including medical image classification, face recognition, intrusion detection, finance
data analysis, and so forth. Moreover, a fully decentralized deep learning architecture is peer-to-
peer networking of nodes based on decentralization and security technologies such as the token-
exchange, a service capable of validating and issuing security tokens to enable nodes to obtain
appropriate access credentials for exchanging resources without the central server. In this case, each
node owns a local training model and performs both the function of the client and the server based
on a shared communication policy, which is different from FL where the central server plays the key
role in model sharing.
We specifically consider a supervised learning task with C categories in the entire dataset D. Sup-
pose that fθ : x → y denotes a neural network classifier with parameters θ, taking an input xi ∈ x
and outputting a C-dimensional real-valued vector also known as the logit. This neural network
gives a predicted label yi = arg maxfθ (xi) s.t. yi ∈ y. We assume there are K nodes in the net-
work. The kth node has its own dataset Dl(okc)al := {(xi, yi)}iN=(1k) , where xi is the ith training sample,
yi is the corresponding label of xi, yi ∈ {1, 2, ..., C} (a multi-classification learning task), and N(k)
is the sample number in dataset Dl(okc)al. D := {Dl1ocal, Dl2ocal,..., Dliocal}, N = PkK=1N(k). The
goal of the decentralized systems is to achieve a desired performance on the entire data through
sharing local trained models Lt(k) at each round t.
Data Heterogeneity. The challenges related to heterogeneity of nodes in DDL refer to two cate-
gories, i.e., data heterogeneity and hardware heterogeneity. Notably, data heterogeneity results in
time-consuming convergence or divergence of model learning. Let p(x|y) be the common data dis-
tribution of the entire data D. We assume the common distribution p(x|y) is shared by all nodes.
Then, Nodek has pk (y). We first consider an independent and identically distributed (IID) setting,
i.e., pi(x, y) = p(x|y)pi(y) s.t. pi(y) = pj(y) for all i 6= j. Under this assumption, the data dis-
tribution of the entire dataset can be represented by a node’s local data distribution. Unfortunately,
in real-life application, samples held by clients are usually skewed with various data distributions,
i.e., pi(x, y) = p(x|y)pi (y) s.t. pi (y) 6= pj (y) for all i 6= j. Node1 follows p1(x, y) and Node2
follows p2(x, y). We further define and clarify such data heterogeneity as follows: for a nodek’s
local dataset, when its α samples are from a single main data class c(k) subject to α > NCk) and
the remaining samples are randomly drawn from the other C-1 data classes, the heterogeneity level
H(k)of nodek is formulated as H(k)(D(k)ai) = -p(y = C) * log(p(y = c)). Moreover, we assign a
main data class c(k) = k%C to nodek.
Communication Overhead. Though communication overhead in a decentralized learning system
also involves the payload size such as different numbers of model parameters (He et al., 2020; Singh
et al., 2019), it is out of the scope of this research where we focus on different communication dis-
tances between distributed nodes. In particular, for every two nodes i and j, a relative communication
distance di,j is defined in the symmetrical matrix Disi×j, where the bidirectional distances between
two nodes are equal and the distance to a node itself di,j|i=j is zero. Furthermore, each distance
di,j|i6=j in the matrix is a random numerical value between 0 and β (Equation 1).
(d1,1
,	d2,1
Disi×j =	.
d1,2
d2,2
.
.
.
di,2
dι,j ∖
d2,j
.	SUbjectto : di,j∣i=j = 0, di,j = dj,i,⅛j∣i=j ∈ (0,β] (1)
.
.
di,j
Where di,j represents the relative distance from node i to node j.
3.2	Technical Fundamentals of Homogeneous Learning
We propose a novel decentralized deep learning architecture called Homogeneous Learning (HL)
(Fig. 1). HL leverages reinforcement learning (RL) agents to learn a shared communication policy of
node selection, thus contributing to fast convergence of model training and reducing communication
cost as well. In HL, each node has two machine learning (ML) models, namely a local ML task
model L(k) for the multi-classification task and an RL model LDQN for the node selection in peer-
to-peer communications.
3
Under review as a conference paper at ICLR 2022
Node2 is selected from ({sa) ∖k≡K}, Lfqn)
Figure 1: Homogeneous learning: self-attention decentralized deep learning.
3.2.1	Local ML Task Model
Task Model Architecture. We assume the K nodes in HL share the same model architecture as a
local ML task model. Let yi be the layeri’s output of L(k) . yi = fi(Wiyi-1), i = 1, ..., p, y0 = x,
where fi is the activation function, Wi is the weight matrix of layeri, yi-1 represents the output
of the previous layer, and p is the number of layers in L(k). Notably, we employ a three-layer
convolutional neural network (CNN) with an architecture as follows: the first convolutional layer of
the CNN model has a convolution kernel of size 5×5 with a stride of 1 and it takes one input plane and
it produces 20 output planes, followed by a ReLU activation function; the second convolutional layer
takes 20 input planes and produces 50 output planes and it has a convolution kernel of size 5×5 with
a stride of 1, followed by ReLU; the output is flattened followed by a linear transformation of a fully
connected layer, which takes as input the tensor and outputs a tensor of size C representing the C
categories. Moreover, the categorical cross-entropy is employed to compute a loss `. After that, we
apply as a learning function the Adam to update the model, i.e., L(+)1 - Ltk) - η ∙ ▽'(Ltk), D(k)al),
where Lt(k)is nodek’s local ML task model at the round t and η is the learning rate.
3.2.2	Reinforcement Learning Model
Besides the local ML task model, each nodek in HL is also associated with a reinforcement learning
(RL) model LDQN . The goal of the RL model is to learn a communication policy for the node
selection in decentralized learning. There are three main components of the RL model, namely, the
state s, the action a, and the reward r. Then, based on the input state s, the RL model outputs an
action a for the next node selection, and at the same time, updates itself by correlating the attained
reward r with the performed action a. As a result, the recursive self-improvement of the RL model
allows a node to constantly explore the relation between the system’s performance and the selection
policy (the self-attention mechanism in HL), contributing to faster convergence of model learning.
Every round t, a RL model observes the state st from two different sources, i.e., model parameters
st(k) of the selected nodek and parameters of models in the surrounding environment {st(i) |i ∈ K, i 6=
k}. In particular, we employ a deep Q-network (DQN), which approximates a state-value function
in a Q-learning framework with a neural network. Let yiDQN be the layeri’s output of LDQN .
yiDQN = fiDQN (WiDQNyiD-Q1N), i = 1, ..., q, y0DQN = s, where fiDQN is the activation function
of layeri, WiDQN is the weight matrix of layeri, yiD-Q1Nrepresents the output of the previous layer,
4
Under review as a conference paper at ICLR 2022
and q is the number of layers in LDQN. Notably, a DQN model consisting of three fully connected
layers is applied (Fig. 2). The two hidden layers consist of 500 and 200 neurons respectively, using
as an activation function the ReLU. The output layer with a linear activation function consists of K
neurons that output the rewards of selecting each nodek respectively, k ∈ {1, 2, ..., K}. Furthermore,
at each round t, the node with the largest reward will be selected. at = arg maxfLDQN (st), where
LDQN denotes weights of the DQN model. Consequently, the RL model selects and sends the trained
local model Lt(k+)1 of nodek to the next node at . As such, the local ML task model Lt(+at1) of node at
is updated to Lt(+k)1.
Local ML task model training
s. t. k = at
Transfer models to
ɑt = arg max f(st;吸 QN)
Figure 2: Next-node selection based on the RL model of HL.
To understand the training of the RL model, we first define the input state st . The state st is a
concatenated vector of the flattened model parameters of all nodes in the systems. st = {st(k) |k ∈
K}. To efficiently represent the state and compute the RL model prediction, we adopt the principal
component analysis (PCA) to reduce the dimension of the state st from an extremely large number
(e.g. 33580 dimensions for MNIST) to K, where K is the number of nodes. K is adopted due to the
minimum possible dimension of a PCA-based output vector is the number of input samples. Then,
we define the output reward rt . Every round t, a trained ML task model is evaluated on a hold-out
validation set Dval, and the reward rt can be computed from the validation accuracy V alAcct, the
communication distance between the current node k and the next selected node at , and a penalty of
minus one for taking each training step. rt = 32V alAcct-GoalAcc-dk,at -1, where GoalAcc denotes
the desired performance on the validation set and dk,at is the communication distance drawn from
the distance matrix DisiXj. We employ an exponentially increasing function 32。to distinguish
between different validation results when the ML task model is close to convergence when only
small variance is observed in the results. In addition, an episode reward R is the accumulative
reward of the current reward and discounted future rewards in the whole training process of HL.
R=PtT=1 γt-1rt, where T is the total training rounds ofHL in one episode.
With DQN, we often use experience replay during training. A RL model’s experience at each time
step t is stored in a data set called the replay memory. Let et be the model’s experience at time t.
et = (st-1, at, rt, st), where rt+1 is the reward given the previous state-action pair (st-1, at) and
st is the state of the ML task models after training. We assume a finite size limit M of the replay
memory, and it will only store the last M experiences. Moreover, to facilitate constant exploration
of a RL model, epsilon is a factor to control the probability of the next node being selected by the
RL model. In particular, for each round, a random numerical value between 0 and 1 is obtained and
compared with the current epsilon value Epsilonep where ep denotes the current episode. Then if
the randomly picked value is greater than Epsilonep, the next node will be selected by the RL model.
Otherwise, a random action of node selection will be performed. For either case, an experience
sample et = (st-1 , at, rt, st) will be stored in the replay memory. The decentralized learning
terminates when either the model achieves the desired performance on the validation set or exceeds
a maximum number of rounds Tmax, the learning progress of which is called an episode of HL. For
5
Under review as a conference paper at ICLR 2022
each episode, we apply the epsilon decay ρ to gradually increase the possibility of the RL model’s
decision-making.Epsilonep+1 = Epsilonep ∙ e-ρ, where Epsilonep+1 is the computed epsilon for
the next episode and e is the Euler’s number that is approximately equal to 2.718.
Furthermore, at the end of each episode ep, the RL model is trained on a small subset of samples
randomly drawn from the replay memory. Let ` be the mean squared error loss of LtDQN . We adopt
as a learning function the Adam. Then, the optimization of the DQN model is formulated in (5).
The updated DQN model is shared with the next selected node. As such, the RL model performs
better and better in predicting the expected rewards of selecting each node for the next round, which
results in the increase of the episode reward R by selecting the node with the largest expected reward
at each round t.
B
'(Ldqn) = X'(r + βmaxf (ai+ι, si； LDQN),f3, si-i； LDQN))
i=1	ai+1
(2)
θ* = argmin'(θ) subjeCtto θ = LDQN
θ
Where ai+1 denotes the predicted next step’s action that maximizes the future reward, β denotes the
discount factor of the future reward, f denotes a feed-forward function that produces the output of
the RL model with respect to ai based on si, and B denotes the batch size.
Finally, the model training of HL is formulated as Algorithm 1.
Algorithm 1 Model Training of Homogeneous Learning
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Training:
initialize L0dqn
for each episode ep = 1, 2, ... do
initialize L(0k)
a0 = k
for each step t = 0, 1, 2, ... do
while V alACCt < GoalACC and t < Tmax do	. Tmax is the maximum steps per
episode
V alACCt+1, at, Lt(k+)1 = HL(Lt(k), LtDQN)
Send {Lt(+k)1, LtD+Q1N} to at for the next step’s model update
end while
end for
LeDpQ+N1 = arg min PiB=1 `(rt + βmaxf(ai+1, si+1;LeDpQN), f(ai, si; LeDpQN))
LeDpQN	ai+1
Epsilonep+1 = Epsilonep ∙ e-ρ
end for
16: function HL(L(tk), LtDQN)	Lt(+k)1 = T rain(Lt(k), Dl(k) l) . Dl(k) l is Nodek’s local training
t+	local	local
data
178: V alACCt+1 = ACC(Dval; Lt(+k)1)
19	(k)	L(k)
19: st = Lt+1
20: s(ti) = Lt(i) * subject to i ∈ K, i 6= k
21: st= {s(tk) , st(i) | i ∈ K, i 6= k}
22: at= arg maxf (st; LtDQN)
23: rt= 32V alAcct -GoalAcc - dk,at - 1
24: Add {st-1, at, rt, st} to the replay memory
25: return V alACCt+1, at, Lt(+k)1
2e6n:d function
6
Under review as a conference paper at ICLR 2022
4	Evaluation
4.1	Experiment Setup
4.1.1	Dataset.
We applied MNIST (LeCun et al., 2010), a handwritten digit image dataset containing 50,000 train-
ing samples and 10,000 test samples labeled as 0-9, and Fashion-MNIST(Xiao et al., 2017), an
image collection of 10 types of clothing containing 50,000 training samples and 10,000 test samples
labeled as shoes, t-shirts, dresses, and so on. The size of grayscale images in these two datasets
is 28×28. Then, we considered both a 10-node scenario and a 100-node scenario training on the
MNIST dataset and the Fashion-MNIST dataset respectively. The machine learning library we used
to build the system is Tensorflow.
4.1.2	Baseline Models.
To compare the performance of the proposed approach, we considered three different model training
methods as baselines, which are centralized learning with all data collected from all nodes, decen-
tralized learning with a random node selection policy, and standalone learning of the starter node
without model sharing. In detail, for each method, we applied the same local ML task model archi-
tecture and associated model training hyperparameters using the training set data from MNIST and
Fashion-MNIST respectively. The goal of model training is to achieve a validation accuracy of 0.80
for the MNIST classification task and 0.70 for the Fashion-MNIST classification task based on the
hold-out test set of the corresponding dataset.
Regarding the standalone method, we utilized the early stopping to monitor the validation loss of
the model at each epoch with a patience of five, which automatically terminated the training pro-
cess when there appeared no further decrease in the validation loss of the model for the last five
epochs. In the centralized and standalone learning, evaluation was performed at each epoch of the
training. Moreover, in decentralized learning, due to multiple models in a system, the evaluation
was performed on the trained local model of each step’s selected node.
4.1.3	Homogeneous Learning Settings.
Homogeneous Learning (HL) of K = {10, 100} nodes with a heterogeneity level H =
{0.24, 0.56, 0.90} (p(y = c) = {0.6, 0.8, 0.9}) was adopted. Each node k owned a total of 500
skewed local training data with a heterogeneity level of H. In addition, to generate the distance
matrix, the relative communication cost represented by the distance between two different nodes
di,j|i6=j takes a random numerical value between 0 and 0.1. A random seed of 0 was adopted for the
reproducibility of the distance matrix (See A.1). For the local ML task model training, we adopted
an epoch of one with a batch size of 32. A further discussion on the selection of these two hyperpa-
rameters can be found in the section A.2. The Adam was applied as an optimization function with a
learning rate of 0.001.
4.2	Numerical Results
4.2.1	Learning a Communication Policy Based on Deep Q-Networks
As aforementioned, each node has a specific main data class c. We considered a starter node with a
main data class of digit ’0’ in the case of MNIST and a class of T-shirt in the case of Fashion-MNIST.
Then, starting from the starter node, a local ML task model was trained on the current node’s local
data and sent to the next step’s node decided by either the RL model or a random action depending
on the epsilon at the current episode. We adopted an initial epsilon of one and a decay rate of 0.02.
Moreover, the RL model was updated at the end of each episode using the hyperparameters defined
in Table 1. We applied a maximum replay memory size of 50,000 and a minimum size of 128, where
the training of the DQN model started only when there were more than 128 samples in the replay
memory and the oldest samples would be removed when samples were more than the maximum
capacity. Furthermore, in every episode, an agent randomly drew 32 samples from the memory to
update its model, with a total of 120 episodes. The maximum training step is 35 in the case of
MNIST and 100 in the case of Fashion-MNIST.
7
Under review as a conference paper at ICLR 2022
Table 1: Hyperparameters in Homogeneous Learning
ML TASK MODEL
RL MODEL
Epoch	1	Episode	120
Batch size	32	Future reward discount	0.9
Learning rate	0.001	Epsilon decay	0.02
Optimization function	Adam	Epoch	1
Maximum step	35 (MNIST)/100 (Fashion-MNIST)	Batch size	16
		Learning rate	0.001
For each episode, we computed the step rewards and the episode reward for the model training to
achieve the performance goal. With the advancement of episodes, the communication policy evolved
to improve the episode reward thus benefiting better decision-making of the next-node selection.
Figure 3.a illustrates the episode reward and the mean reward over the last 10 episodes during HL
and the corresponding total training rounds for each method when training on MNIST. Figure 3.b
illustrates the episode reward results of HL when training on Fashion-MNIST.
Fashion-MNIST with 10 Clients
Fashion-MNISTwith 100 Clients
MNISTwith 10 Clients
MNIST with 100 Clients
Figure 3: (a) With the increase of episodes, the mean reward over last 10 episodes is gradually
increasing. The DQN model learned a better communication policy by training on samples from
the replay memory, contributing to the systems’ performance in total training rounds. (b) Episode
reward results for the 10-node and 100-node scenarios when applying the Fashion-MNIST dataset.
Compared with MNIST, the policy learning was more unstable with the same hyperparameter set-
ting, however, it showed an increasing episode reward.
4.2.2	Comparisons Regarding Computational and Communication Cost
To compare the performance between HL and the aforementioned three baseline models, we per-
formed a comprehensive evaluation against the metrics of computational cost and communication
cost in the case of MNIST with the 10-node scenario. For each method, we performed 10 individ-
ual experiments with different random seeds. Here, the computational cost refers to the required
total rounds for a system to achieve the training goal, and the communication cost refers to the total
communication distance for the model sharing in decentralized learning.
Computational Cost. As shown in Figure 3.a, due to limited local training data, the standalone
learning appeared to be extremely slow after the validation accuracy reached 0.70. Finally, it termi-
nated with a final accuracy of around 0.75 due to the early-stopping strategy. Moreover, by com-
paring the decentralized learning methods with and without the self-attention mechanism, the result
8
Under review as a conference paper at ICLR 2022
suggests that our proposed method of HL can greatly reduce the total training rounds. In addition,
though centralized learning shows the fastest convergence, it suffers from problems of data privacy.
Communication Cost. In decentralized learning, to train a model, each selected node trains the
current ML task model on its local dataset and sends the trained model to another node, and the
communication cost refers to the network traffic payload for sending the model. We studied the
relative cost by introducing the communication distance between nodes, which is defined as the
total communication distance for model sharing in HL, i.e., from the starter node to the last selected
node.
We performed ten individual experiments for each method and used as final results the best cases
of node selection over the last five episodes when decisions were almost made by the agent and a
learned communication policy was prone to be stable. Figure 4.a illustrates the experiment results of
the total training rounds and the communication cost. The bottom and top of the error bars represent
the 25th and 75th percentiles respectively, the line inside the box shows the median value, and
outliers are shown as open circles. Finally, the evaluation result shows that HL can greatly reduce
the training rounds by 50.8% and the communication cost by 74.6%.
Figure 4: (a) Performance comparison between the random policy-based decentralized learning and
HL. Each error bar illustrates 10 individual experiments’ results. (b) Computational performance
comparison with various heterogeneity levels of training data.
4.2.3	Comparisons Regarding Various Heterogeneity Levels
We further studied the performance of the proposed method with different heterogeneity levels H =
{0.24, 0.56, 0.90} (p(y = c) = {0.6, 0.8, 0.9}). We evaluated the performance in the 10-node
scenario training on MNIST. In addition, for the case of H = 0.90, we applied a maximum training
step of 80 instead due to a more challenging convergence of the ML task model using the highly
skewed local training data. Figure 4.b illustrates a computational performance comparison between
the proposed HL and the baseline decentralized learning with a classical random policy.
5	Conclusion
Decentralized deep learning (DDL) leveraging distributed data sources contributes to a better neu-
ral network model while safeguarding data privacy. Despite the broad applications of DDL models
such as federated learning and swarming learning, the challenges regarding edge heterogeneity es-
pecially the data heterogeneity have greatly limited their scalability. In this research, we proposed a
self-attention decentralized deep learning method of Homogeneous Learning (HL) that recursively
updates a shared communication policy by observing the system’s state and the gained reward for
taking an action based on the observation. We comprehensively evaluated the proposed method by
comparing with three baseline models for two different image classification tasks in both a 10-node
scenario and a 100-node scenario, applying as criteria the computational and communication cost.
The evaluation result shows that HL can greatly reduce the cost when training on skewed decen-
tralized data with various heterogeneity levels. In future, a decentralized model leveraging various
communication policies at the same time to achieve diverse goals is considered for the further study
of this research.
9
Under review as a conference paper at ICLR 2022
References
General data protection regulation. https://gdpr-info.eu. Accessed: 2021-09-22.
Di Cao, Shan Chang, Zhijian Lin, Guohua Liu, and Donghong Sun. Understanding distributed
poisoning attack in federated learning. In 25th IEEE International Conference on Parallel
and Distributed Systems, ICPADS 2019, Tianjin, China, December 4-6, 2019, pp. 233-239.
IEEE, 2019. doi: 10.1109/ICPADS47876.2019.00042. URL https://doi.org/10.1109/
ICPADS47876.2019.00042.
Moming Duan, Duo Liu, Xianzhang Chen, Renping Liu, Yujuan Tan, and Liang Liang. Self-
balancing federated learning with global imbalanced data in mobile systems. IEEE Trans. Par-
allel Distributed Syst., 32(1):59-71, 2021. doi: 10.1109/TPDS.2020.3009406. URL https:
//doi.org/10.1109/TPDS.2020.3009406.
Yujia Gao, Liang Liu, Binxuan Hu, Tianzi Lei, and Huadong Ma. Federated region-learning for
environment sensing in edge computing system. IEEE Transactions on Network Science and
Engineering, 7(4):2192-2204, 2020. doi: 10.1109/TNSE.2020.3016035.
Chaoyang He, Murali Annavaram, and Salman Avestimehr. Group knowledge transfer: Feder-
ated learning of large cnns at the edge. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Had-
sell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Process-
ing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/a1d4c20b182ad7137ab3606f0e3fc8a4-Abstract.html.
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
Communication-efficient on-device machine learning: Federated distillation and augmentation
under non-iid private data. CoRR, abs/1811.11479, 2018. URL http://arxiv.org/abs/
1811.11479.
Jakub Konecny, H. Brendan McMahan, Felix X. Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. In NIPS
Workshop on Private Multi-Party Machine Learning, 2016.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang Yan. A blockchain-
based decentralized federated learning framework with committee consensus. IEEE Netw., 35(1):
234-241, 2021. doi: 10.1109/MNET.011.2000263. URL https://doi.org/10.1109/
MNET.011.2000263.
Boyi Liu, Lujia Wang, Ming Liu, and Chengzhong Xu. Lifelong federated reinforcement learning:
A learning architecture for navigation in cloud robotic systems. CoRR, abs/1901.06455, 2019.
URL http://arxiv.org/abs/1901.06455.
Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen, Lican Feng, Tianjian
Chen, Han Yu, and Qiang Yang. Fedvision: An online visual object detection platform powered
by federated learning. Proceedings of the AAAI Conference on Artificial Intelligence, 34(08):
13172-13179, Apr. 2020. doi: 10.1609/aaai.v34i08.7021. URL https://ojs.aaai.org/
index.php/AAAI/article/view/7021.
Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang. Blockchain and
federated learning for privacy-preserved data sharing in industrial iot. IEEE Transactions on
Industrial Informatics, 16(6):4177-4186, 2020. doi: 10.1109/TII.2019.2942190.
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially pri-
vate recurrent language models. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=BJ0hF1Z0b.
10
Under review as a conference paper at ICLR 2022
Nishat I. Mowla, Nguyen H. Tran, Inshil Doh, and Kijoon Chae. Federated learning-based cognitive
detection of jamming attack in flying ad-hoc network. IEEE Access, 8:4338-4350, 2020. doi:
10.1109/ACCESS.2019.2962873.
T. Nguyen, P. Rieger, Markus Miettinen, and A. Sadeghi. Poisoning attacks on federated learning-
based iot intrusion detection system. 2020.
Mani Parimala, R. M. Swarna Priya, Quoc-Viet Pham, Kapal Dev, Praveen Kumar Reddy Mad-
dikunta, Thippa Reddy Gadekallu, and Thien Huynh-The. Fusion of federated learning and in-
dustrial internet of things: A survey. ArXiv, abs/2101.00798, 2021.
Shiva Raj Pokhrel and Jinho Choi. Federated learning with blockchain for autonomous vehicles:
Analysis and design challenges. IEEE Transactions on Communications, 68(8):4734-4746, 2020.
doi: 10.1109/TCOMM.2020.2990686.
Sawsan Abdul Rahman, Hanine Tout, Chamseddine Talhi, and Azzam Mourad. Internet of things
intrusion detection: Centralized, on-device, or federated learning? IEEE Network, 34(6):310-
317, 2020. doi: 10.1109/MNET.011.2000286.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
URL https://openreview.net/forum?id=H1aIuk-RW.
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. Detailed comparison of
communication efficiency of split learning and federated learning. CoRR, abs/1909.09145, 2019.
URL http://arxiv.org/abs/1909.09145.
Yuwei Sun, Hiroshi Esaki, and Hideya Ochiai. Adaptive intrusion detection in the networking
of large-scale lans with segmented federated learning. IEEE Open J. Commun. Soc., 2:102-
112, 2021. doi: 10.1109/OJCOMS.2020.3044323. URL https://doi.org/10.1109/
OJCOMS.2020.3044323.
Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. Optimizing federated learning on non-iid
data with reinforcement learning. In 39th IEEE Conference on Computer Communications, IN-
FOCOM 2020, Toronto, ON, Canada, July 6-9, 2020, pp. 1698-1707. IEEE, 2020. doi: 10.1109/
INFOCOM41043.2020.9155494. URL https://doi.org/10.1109/INFOCOM41043.
2020.9155494.
Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathya-
narayanan Manamohan, Saikat Mukherjee, Vishesh Garg, Ravi Sarveswara, Kristian Handler,
Peter Pickkers, N. Ahmad Aziz, Sofia Ktena, Christian Siever, Michael Kraut, Milind Desai,
Bruno Monnet, Maria Saridaki, Charles Martin Siegel, Anna Drews, Melanie Nuesch-Germano,
Heidi Theis, Mihai G. Netea, Fabian Theis, Anna C. Aschenbrenner, Thomas Ulas, Monique M.B.
Breteler, Evangelos J. Giamarellos-Bourboulis, Matthijs Kox, Matthias Becker, Sorin Cheran,
Michael S. Woodacre, Eng Lim Goh, Joachim L. Schultze, and German COVID-19 OMICS Ini-
tiative (DeCOI). Swarm learning for decentralized and confidential clinical machine learning,
2021.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/
abs/1708.07747.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. CoRR, abs/1806.00582, 2018. URL http://arxiv.org/abs/
1806.00582.
A Appendix
A.1 Communication Distance Matrix
Figure 5 illustrates the generated distance matrix Di×j in the 10-node scenario when applying a β
value of 0.1 and a random seed of0.
11
Under review as a conference paper at ICLR 2022
Figure 5: The adopted distance matrix Di×j in the 10-node scenario.
A.2 Optimization of Model Distribution Representation
Under the assumption of data heterogeneity, to allow a reinforcement learning (RL) agent to effi-
ciently learn a communication policy by observing model states in the systems, a trade-off between
the batch size and the epoch of local foundation model training was discussed. Figure 6 illustrates
the trained models’ weights distribution in the 10-node scenario after applying the principal compo-
nent analysis (PCA), with different batch sizes and epochs applied to train on the MNIST dataset.
Moreover, it shows a 100-node scenario where each color represents nodes with the same main data
class. As shown in the graphs, various combinations of these two parameters have different distri-
bution representation capabilities. By comparing the distribution density and scale, we found that
when adopting a batch size of 32 and an epoch of one the models distribution was best represented,
which could facilitate the policy learning of an agent.
Figure 6: Optimization of model distribution representation in the 10-node scenario.
Batch size: 32, Epoch: 5
-0.75 -0.50 -0.25 0.00	0.25	0.50	0.75	1.00
First principal component
12