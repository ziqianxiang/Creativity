Under review as a conference paper at ICLR 2022
Predicting Unreliable Predictions by
Shattering a Neural Network
Anonymous authors
Paper under double-blind review
Ab stract
Generalization error bounds measure the deviation of performance on unseen test
data from performance on training data. However, by providing one scalar per
model, they are input-agnostic. What if one wants to predict error for a specific
test sample? To answer this, we propose the novel paradigm of input-conditioned
generalization error bounds. For piecewise linear neural networks, given a weight-
ing function that relates the errors of different input activation regions together, we
obtain a bound on each region’s generalization error that scales inversely with the
density of training samples. That is, more densely supported regions are more
reliable. As the bound is input-conditioned, it is to our knowledge the first gen-
eralization error bound applicable to the problems of detecting out-of-distribution
and misclassified in-distribution samples for neural networks; we find that it per-
forms competitively in both cases when tested on image classification tasks.
1 Introduction
Maximizing generalization is the problem of quantifying and minimizing model error on unseen test
data. This includes model selection, the focus of much of the classical work on generalization (Mohri
et al., 2018), which involves bounds on expected test error that vary with model or model class, and
sample selection (Lee et al., 2018), which involves test error metrics for a given model that vary with
location in input space. Whilst model selection is an important part of the training process, sample
selection is important for testing, because even the best selected model invariably makes mistakes
and being able to discard erroneous predictions is critical for safe deployment. Unlike generalization
bounds for model selection, the link to measuring generalization error for sample selection methods
is often not explicit. This paper proposes a novel metric for the class of piecewise-linear neural
networks that bridges both categories: a generalization error bound that is sample dependent.
What is the reason for targeting neural networks? In piecewise-linear neural networks, the linear
function applied varies with location in input space. Specifically, for each linear activation region
in input space there is a distinct linear function that uses a different parameterization or subset of
network weights (Montufar et al., 2014), which for convenience We call the subfunctions of the net-
work. Generalization error bounds measure deviation in empirical error for a given function. Thus,
one can construct an input-dependent generalization error bound by relying on the bound of the sub-
function corresponding to the region the given input belongs to. The main challenge with this idea
is that naively constructing a bound in the typical format of empirical training error plus bound on
generalization gap only allows the generalization error of subfunctions with a well defined empiri-
cal training error (i.e. those seen in training) to be quantified. However, test inputs typically fall in
activation regions that were unseen in training for deep networks. We resolve this by introducing a
weighting function that smooths training error over activation regions, allowing it to be defined for
activation regions without training sample coverage. Then we demonstrate how the bound on this
sample-dependent error can be brought back to model-level by integrating over subfunctions, yield-
ing new model-level bounds that scale with the number of linear activation regions (or equivalently,
binarized activation patterns) of the network. The idea of Occam’s Razor, or pick the simplest model
all else equal, is prevalent in model selection. The novelty of our model-level bounds compared to
existing work is that they use expressivity (Montufar et al., 2014; Hanin & Rolnick, 2019) to mea-
sure model simplicity or complexity, rather than number of parameters or data mean and covariance
metrics (Rivasplata et al., 2020). An intuitive interpretation is that one can expect generalization
ability to scale with the degree of abstraction or information loss implemented by model inference.
1
Under review as a conference paper at ICLR 2022
Figure 1 illustrates input-dependent generalization error bounds for a model trained on the half
moons dataset. The network is shattered into subfunctions corresponding to linear activation regions,
and a generalization error bound is derived that uses smooth training sample density. Whereas
model-level bounds produce one value for the model, considering the model as a composition of
individual functions allows us to discriminate risk based on input region.
(a)	(b)	(c)	(d)	(e)
Figure 1: Shattering a neural network trained on the halfmoons dataset into co-dependent linear
subfunctions to obtain a heatmap of unreliability across the input space. The model is a MLP with
two hidden ReLU layers of 32 units each. (a) Training data. (b) Traditional model-level error bound:
shown here is the single model bound from Mohri et al. (2018, eq. 2.17). (c) Linear activation
regions. (d) Ours: subfunction error bound as unreliability. (e) Unreliability heatmap in 2D.
2 Input-dependent generalization error bounds
2.1	Notation
Let f : X → Y be a piecewise linear neural network comprised of linear functions and ReLU
activation functions. Let V = (vj)1≤j ≤M, ordered according to any fixed ordering, be the set of M
ReLUs in f where ∀j : vj : X → {0, 1} is 1 if the ReLU is positive valued when f(x) is computed,
and 0 otherwise. Let patterns set P ⊂ {0, 1}M, P = (pi)1≤i≤C be the set of all feasible ReLU
activation decisions, meaning all possible binary patterns induced by traversing the input space X .
In general this is not equal to the full bit space {0, 1}M, and computing it exactly is an open problem
(Montufar et al., 2014). There is a bijection between these patterns and the linear activation regions
of the network, as each linear activation region is uniquely determined by the set of ReLU activation
decisions (Raghu et al., 2017). Let activation region ai correspond to pattern pi:
ai ,{x ∈ X | ∀j ∈ [1, M] : pij = vj (x)},	(1)
Each pattern pi defines a distinct linear function hi : X → Y , or subfunction, by the replacement
of the ReLU corresponding to each vj (non-linear) by the identity function if pij = 1 or the zero
function otherwise (both linear). Let the set of all subfunctions be H = (hi)1≤i≤C, ordered identi-
cally to P. Since activation regions are non-overlapping polytopes in input space, each input sample
belongs to a unique activation region, so inference with model f can be rewritten as f (x) = hi (x)
where x ∈ ai . For convenience we overload the definition of H so the subfunction induced for any
input sample x is H(x) , hi where x ∈ ai .
2.2	From model-level bounds to input-level bounds
Consider a training dataset S containing N sampled pairs, S ∈ (X × Y)N, where samples are drawn
iid from distribution D. Define the empirical error or risk of the full network on this dataset:
1
RS (f) , ∣S∣
|S|
|S|
r(Sn;f),
n=1
(2)
where r is a bounded error function on samples: 0 ≤ r((x, y); f) ≤ 1 for all x, y ∈ (X × Y) and
S n is the n-th element ofS . Note empirical error, e.g. proportion of incorrect classifications, is not
necessarily the objective function for training; it is fine for the latter to not have finite bounds.
2
Under review as a conference paper at ICLR 2022
Let us quantify the empirical error for a subfunction hi using traditional empirical error (eq. (2)). The
activation region dataset is Si , S ∩ ai with Ni , |Si | samples drawn iid from its data distribution
Di, defined as PDi (x, y) = PD (x, y|x ∈ ai). Then:
1i
RSi (N) =寸 ∑r(Sn； f),
Ni
i n=1
and for any δ ∈ (0, 1], with probability > 1 - δ:
__ L O ，一、r	O ，一、
E[RbSi(hi)] ≤ RbSi (hi) +
Si
(3)
(4)
This uses a Hoeffding-inequality based model-level
bound (Mohri et al., 2018, eq. 2.17) on each sub-
function. There are several issues with this formu-
lation. First, it treats the empirical error of dif-
ferent subfunctions as independent when in gen-
eral, they are not. Since different activation re-
gions are bounded by shared hyperplanes, and hence
the subfunctions share parameters, there exists use-
ful evidence for a subfunction’s performance out-
side its own activation region. Second, test samples
overwhelmingly induce unseen activation patterns in
deep networks, and the bound in eq. (4) is infinite if
Ni = 0, making this quantity uninformative for the
purposes of comparing subfunctions.
This motivates the following empirical risk metric
for subfunctions. Fix non-negative weighting or
closeness distance function between subfunctions,
k : H × H → R≥0. For any subfunction hi ∈ H,
define its probability mass:
Figure 2: Each linear activation region in 2D
input space (plane) is mapped to a unique
subfunction, activation decision pattern, and
set of training samples (triangles). A smooth
density is defined given the number of sam-
ples in each region.
P(h ) ,	Ej∈[1,C] Njk(hi,hj)
( i)一 P1∈[1,C] Pj∈[i,c] Nj k(hι,hj),
so by construction	P(hi) = 1,	(5)
i∈[1,C]
which quantifies how densely hi is locally populated by training samples. Rewrite the empirical
error of the network as an expectation over subfunction empirical error:
RRS(f) = N X r((x,y);f)
(x,y)∈S
(XXX∈S pj∈[i,c] PJ) k(H (X),hj) i∈XC]P(hi)
k(H(x), hi) r((x, y); f)
1 X P(h) X	k(H(X),hi)r((x,y♦ f)
NiiC] (i)(x⅛sPj∈[1,C]P(hj)k(H(X),hj)
=E [1 X	k(H(X),hi)r((x,y);f) -
=E [N 七∈s Pj∈[i,c]P(hj) k(H(X),hj)]
Γ -1	7/7 7∖7∖7∙π / 1 \	1
_E 1	X k(hl ,hi) Nl RSl (hl)	, ER* h ʌ!
=E [N l∈⅛] Pj∈mP(hj) k(hl,hj)I ,E[RS(hi)].
(6)
(7)
(8)
(9)
(10)
Theorem 2.1 (Subfunction true error bound). Assume that DS is a distribution over size-N
datasets that have the same activation region data distributions and dataset sizes (Di and Ni, ∀i ∈
[1, C]) as S. Consider S as a dataset drawn iid from DS. Then ∀i ∈ [1, C], for any given δ ∈ (0, 1],
3
Under review as a conference paper at ICLR 2022
with probability > 1 - δ:
subfunction true error bound (STEB)
Z	入	_、
. Λ	, ^ ..	.一
RDS (hi) , E[RS(hi)] ≤
、	-丫一	J
true or generalization error
RSZiJ+	p⅛ vloNδ，
empirical error | {z J
generalization gap bound
(11)
proof in appendix A. Intuitively this implies that the more a subfunction is surrounded by samples
for which the model makes accurate predictions - both from its own region and regions of other
subfunctions, weighted by the weighting function k - the lower its empirical error and bound on
generalization gap, and thus the lower its bound on true error, given any fixed δ . The further that
test samples fall from densely supported training regions or subfunctions, the less likely the model
is to generalize. Unlike eq. (4), this bound is finite even for subfunctions without training samples
because such subfunctions are assigned non-zero density (fig. 2), given a positive-valued weighting
function.
2.3 Weighting function selection
Figure 3: Given a family of Gaussian
weighting functions, the best weighting
function according to eq. (15) trades off
Although p(h)can be very large in general, note firstly
that it is tempered by a factor of √N, and secondly
that if one were to optimize the weighting function k
by minimizing the distance between true error and STEB
(eq. (11)), this necessarily involves finding a function that
yields high densities for activation regions, otherwise the
distance to true error would be large. Now we discuss the
problem of selecting such a weighting function.
The ideal weighting function k and probability parameter
δ produce a bound for each subfunction hi that reflects
the subfunction’s true error accurately, i.e. minimizes the
difference with the expected true error (without weighting
function) ifwe had unlimited samples of its activation re-
gion ai:
min
k,δ
_ n ^	z, 、r
E[RbSi (hi)] -
Si
(12)
precision of the smooth empirical error
with sample density.
Being limited to dataset instance S, we do not have access
,	J 7，，	E 「A /7、ITrTr
to subfunction hi’s true error ESi [RSi (hi)]. However, we
can construct an estimate by taking the samples x, y that k includes in the subfunction’s smooth
empirical error RS(hi) (that is, all the training samples, for positive valued k) and adjusting their
error values to reflect what they would be if the sample did belong to a?. ThiS improves on RS(hi),
the weighted error of surrounding activation regions, by transforming it into a weighted error of the
target activation region ai itself. Let g : X × Y → X × Y be the shifting function that moves
samples into region ai, defined as g(x,y)，argmin(χθ,yθ)~d ∣∣(x0,y0) - (x,y)k s.t. x0 ∈ ai. Using
the Taylor expansion, assuming an error function r with bounded first order gradients:
∀(x, y) ∈ S : r(g(x, y); f) = r((x, y); f) + O(∣(x, y) - g(x, y)∣)	(13)
Replacing the true error in eq. (12) with the estimate constructed using shifted samples:
X w(X)k(H(X),hi)r(g(x，y);f)-(RS(hi) + P(hi)Vl≡
(x,y)∈S
∑ w⅛k(H(X),hi)O(k(x,y)-g(x,y)k) +
(x,y)∈S
ɪʌ/ɪogl
P(hi) V 2N
(14)
(15)
1
N
≤
1
N
4
Under review as a conference paper at ICLR 2022
where w(x) , j∈[1,C] P(hj) k(H(x), hj) is the weight normalization term. We draw 2 conclu-
sions from this analysis. First, weighting value k(hi, hj) for any hi , hj should decrease with the
distance between their activation regions. This is evident from eq. (15) as x is in region H(x) and
g(x, y)[0] is in region hi, so larger distances should be penalized by smaller weights. However,
k(hi , hj ) cannot be too low (for example 0 for all hj 6= hi given any hi) because the magnitude of
p(h)would be large. Thus the best k combines error precision (upweight near regions and down-
weight far regions) with support (upweight near and far regions). The need to minimize weight with
distance justifies restricting the search for k within function classes where output decreases with
activation region distance, such as Gaussian functions of activation pattern distance. An example of
this trade-off is illustrated in fig. 3.
Secondly, in practice there is no need to explicitly minimize eq. (15), which would involve estimating
the Lipschitz constant for error function r. A simple alternative is to use a validation set metric that
correlates with how well k, δ minimize eq. (15) (i.e. approximate the true error), such as the ability of
the bound to discriminate between validation samples that are accurately or inaccurately predicted,
and select k and δ such that they minimize the validation metric. This is the approach we take in our
experiments.
2.4 From input-level bounds back to model-level bounds
Corollary 2.2 (Expectation reconstructed true error bound). Taking an expectation over sub-
functions yields the following model-level bound. In expectation over subfunctions hi, the following
holds with probability > 1 - δ :
}1
expectation reconstructed true error bound (E-RTEB)
E[RD S (hi)]≤ E
RS (hi) + P(⅛)∕oi^#
RbS(f)+C
(16)
Corollary 2.3 (Union reconstructed true error bound). Taking a union bound over subfunctions
yields the following model-level bound. For any δ > 0, the following holds for all subfunctions hi
with probability > 1 - δ:
union reconstructed true error bound (U-RTEB)
一 , . , ^..
E[RDS(hi)] <	RS(hi) +
hi
(17)
proof in appendix B. It is clear that eq. (17) is the tighter and thus more useful bound, since the
generalization gap scales with √log C instead of linearly with C (eq. (16)). These bounds share
some similar characteristics with classical model-level bounds; see Mohri et al. (2018) for a review.
For instance, larger datasets are better, because the bounds scale inversely withN, and generalization
ability corresponds to sample efficiency, because in order to attain the same bound with smallerN,
one decreases model complexity or the number of subfunctions C.
A key difference with existing work is the metric for model complexity is not the number of parame-
ters (capacity), but the number of subfunctions or feasible activation regions (expressivity) combined
with training sample density (eq. (17)). Whereas expressivity and density are dependent on training
data and training procedure, capacity is not. We conjecture that there is a potentially interesting
avenue to the recent double descent phenomenon (Belkin et al., 2019; Nakkiran et al., 2019) where
it was observed, counter to accepted wisdom, that generalization in deep networks can be improved
by increasing the number of parameters. Whereas Belkin et al. (2019) proposed the norm of network
weights as an alternative complexity measure to the number of parameters; eq. (17) suggests com-
bining the number of subfunctions (equivalently, number of activation regions) with training sample
density of the activation regions. An intuitive interpretation of this bound is in order to maximise
generalization, one should minimize the number of distinct behaviours within the model, whilst en-
suring the data support for each of those behaviours is above some threshold, because generalization
is strongly penalized by low training sample density.
Minimizing the number of binary activation patterns across layers corresponds to minimizing worst-
case entropy of the binary representation, since the worst-case distribution for entropy is uniform and
5
Under review as a conference paper at ICLR 2022
entropy of uniform distributions scales with the number of states (proof in appendix C). Therefore
from an information-theoretic perspective, minimizing eq. (17) over models corresponds to maxi-
mizing information loss from input space to feature space, or maximising the abstractness of the
model’s data representation. This is the same principle used by the information bottleneck (Tishby
et al., 2000; Ahuja et al., 2021), which minimizes an upper bound on the entropy of continuous
network representations, and implicitly by sparse factor graphs (Goyal & Bengio, 2020) and feature
discretization methods (Liu et al., 2021) including discrete output unsupervised learning methods (Ji
et al., 2019), which are all methods that build low expressivity into the model as a prior for improved
generalization. Information loss also increases invariance of the feature representation with respect
to changes in input sample, which is similar to Ganin et al. (2016); Sun & Saenko (2016) that impose
invariance of the feature distribution with respect to changes in input datasets. In short, this work
supports the idea that information loss in model representations is key for robust generalization, by
providing a generalization error bound that explicitly scales with model expressivity.
Data represenations that satisfy the criterion of low expressivity and high training sample den-
sity (eq. (17)) exhibit a high degree of abstraction and usage across training data, by defini-
tion. These two measures are co-dependent; for example decreasing the number of activation
regions by removing a particular hyperplane edge in shattered input space monotonically in-
creases training sample density across activation regions, since it monotonically increases in the
newly enlarged region and stays constant elsewhere. This explicitly illustrates how increasing
abstraction improves robustness of the activation pattern to low level data variations, by defi-
nition of enlarging the input region of the pattern. (However expressivity and density are not
completely redundant measures since one can modify density across regions by moving hyper-
planes without changing the total number of regions.) Meanwhile, information can only mono-
tonically decrease with the application of successive non-stochastic functions (Ji et al., 2019).
Furthermore, note that summing the bound in eq. (17) over subfunctions implies a uniform op-
timal distribution of densities across activation regions, all else equal, because ∀P ∈ RC≥0 :
min X s∑f S.t. X Pi = 1 implies ∀i ∈ [1,C] : Pi = ɪ, (18)
PP	C
i∈[1,C] i	i∈[1,C]
proof in appendix D, aS one would expect from the Shape of the re-
ciprocal function Since it penalizeS Small denSitieS diSproportionately
to larger oneS. In Summary, thiS SuggeStS that piecewiSe-linear neu-
ral networkS that generalize optimally reSemble a chain of maximal in-
formation “SqueezeS” or loSSeS of Sample information (C in eq. (17)),
where SampleS are evenly partitioned by activation patternS (eq. (18))
and empirical training error iS Still minimized i.e. training data can Still
be fitted (RS(hi) in eq. (17)). Interestingly, this is exactly the kind of
efficient hierarchical repreSentation implemented by ontologieS Such aS
balanced k-d trees (Cormen et al., 2009), which minimize the number of
input regions given any fixed number of discriminative hyperplanes by
Figure 4: Input regions
for a balanced k-d tree on
2D space. Color denotes
unique decision set.
ensuring each hyperplane only partitions one superset region, whilst maximizing search efficiency
with balanced partitioning resulting in uniform density across input regions (fig. 4).
3	Related work
Input-dependent generalization error bounds brings together several areas of machine learning re-
search that are not typically combined. The first is generalization error bounds intended for model
selection (Mohri et al., 2018; Akaike, 1974). The second is sample-level metrics intended for out-of-
distribution or unreliable in-distribution sample selection (Ovadia et al., 2019). The third is work on
linear activation regions, motivated by characterizing neural network expressivity (Montufar et al.,
2014; Raghu et al., 2017; Hanin & Rolnick, 2019).
So far we have focused on generalization error bounds and linear activation regions; now we review
existing ideas for sample selection, which is necessary to understand baselines in the practical exper-
iments. Well known sample uncertainty or unreliability metrics include the maximum response of
the final softmax prediction layer (Hendrycks & Gimpel, 2017; Geifman & El-Yaniv, 2017; Cordella
et al., 1995; Chow, 1957), its entropy (Shannon, 1948), or its top-two margin (Scheffer et al., 2001),
6
Under review as a conference paper at ICLR 2022
all conditioned on the input sample. Liang et al. (2017) combines maximum response with tempera-
ture scaling and input perturbations. Jiang et al. (2018) combines the top-two margin idea with class
distance. Some ideas use distance to prototypes in representation space, which is similar at high
level to ours if one assumes prototypes are in high-density regions. Lee et al. (2018) trains a logistic
regressor on layer-wise distances ofa sample’s features to its nearest class, with the idea that distance
to features of the nearest class should scale with unreliability. This was shown to outperform Liang
et al. (2017). Sehwag et al. (2021) is an unsupervised variant of Lee et al. (2018). Tack et al. (2020)
clusters feature representations instead of using classes, using distance to nearest cluster as unreli-
ability. In Bergman & Hoshen (2020) the clusters are defined by input transformations; we were
unable to get this working in our setting as models appear to suffer from feature collapse across
transformations when not trained explicitly for transformation disentanglement. Non-parametric
kernel based methods such as Gaussian processes provide measures of uncertainty that also scale
with distance from samples and can be appended to a neural network base (Liu et al., 2020). Zhang
et al. (2020) assume density in latent space is correlated to reliability, using residual flows (Chen
et al., 2019) for the density model. If multiple models trained on the same dataset are available
(which we do not assume), one could use ensemble model metrics such as variance, max response
or entropy (Lakshminarayanan et al., 2016); an ensemble can also be simulated in a single model
with Monte Carlo dropout (Geifman & El-Yaniv, 2017; Gal & Ghahramani, 2016).
Many of these works seek to predict whether a sample is out-of-distribution (OOD) for its own sake,
which is an interesting problem, but we care about 1) epistemic uncertainty in general, including in-
distribution misclassification, not just OOD 2) in the context of the main model trained for a practical
task, or in other words, exposing what the task model does not know using the task model itself, as
opposed to training separate models on the data distribution specifically for outlier detection.
4	Experiments
We tested the ability of the input-conditioned bound in eq. (11) to predict out-of-distribution and
misclassified in-distribution samples. Taking pre-trained VGG16 and ResNet50 models for CI-
FAR100 and CIFAR10, we computed area under false positive rate vs. true positive rate (AUROC)
and area under coverage vs. effective accuracy (AUCEA) for each method. Definitions are given
in appendix G. These metrics treat predicting unreliable samples as a binary classification problem,
where for out-of-distribution, ground truth is old distribution/new distribution, and for misclassi-
fied in-distribution, ground truth is classified correct/incorrect. Method output is accept/reject. All
methods produce a metric per sample that is assumed to scale with unreliability, so 1K thresholds
for discretizing into accept/reject decisions were uniformly sampled across the maximum test set
range, yielding the AUROC and AUCEA curves. In-distribution misclassification validation set
was used to select hyperparameters, i.e. we use a realistic setting where OOD data is truly un-
seen for all parameters. For our method, we used Gaussian weighting with standard deviation ρ
(k(hi, hj) = e- Hamming(Pi,pj )2/(2p2))and took log of the bound to suppress large magnitudes.
There are usually some caveats when porting theory to practical experiments, and in this case there
were 2. First, for tractability on deep networks, we use a coarse partitioning of the network by taking
activations from the last ReLU layer only, so the subfunctions are piecewise linear instead of purely
linear. In this case activation regions are still disjoint, eq. (11) becomes a bound on piecewise linear
Table 1: Summary of out-of-distribution and misclassified in-distribution results, by difference to
the top performing method in each architecture × dataset setting. Values are difference in AUROC
and average ± standard deviation is shown over all architecture × dataset settings. Higher is better.
	Out-of-distr.	Misc. in-distr.	Average
Residual flows density (Chen et al., 2019)	-0.538 ± 2E-01	-0.356 ± 4E-02	-0.447 ± 1E-01
GP (Liu et al. (2020) w/ fixed features)	-0.204 ± 2E-01	-0.159 ± 1E-01	-0.181 ± 1E-01
Class distance (Lee et al., 2018)	-0.214 ± 1E-01	-0.334 ± 9E-02	-0.274 ± 1E-01
Margin (Scheffer et al., 2001)	-0.037 ± 2E-02	-0.007 ± 7E-03	-0.022 ± 1E-02
Entropy (Shannon, 1948)	-0.025 ± 2E-02	-0.002 ± 2E-03	-0.014 ± 1E-02
Max response (Cordella et al., 1995)	-0.034 ± 2E-02	-0.008 ± 8E-03	-0.021 ± 1E-02
MC dropout (Geifman & El-Yaniv, 2017)	-0.061 ± 3E-02	-0.048 ± 2E-02	-0.054 ± 2E-02
Cluster distance (Tack et al., 2020)	-0.052 ± 9E-02	-0.021 ± 7E-03	-0.036 ± 5E-02
Subfunctions (ours)	-0.007 ± 1E-02	-0.006 ± 4E-04	-0.007 ± 6E-03
7
Under review as a conference paper at ICLR 2022
Table 2: Results for models trained on CIFAR10 on out-of-distribution detection vs CI-
FAR100/SVHN. AUROC shown, higher is better. For equivalent table on CIFAR100, see table 5.
	→ CIFAR100		→ SVHN	
	VGG16	ResNet50	VGG16	ResNet50
Residual flows density	0.513 ± 2E-04	0.513 ± 1E-04	0.084 ± 1E-04	0.084 ± 1E-04
GP	0.810 ± 1E-02	0.575 ± 2E-02	0.844 ± 2E-02	0.473 ± 9E-02
Class distance	0.673 ± 7E-02	0.468 ± 3E-02	0.806 ± 6E-02	0.462 ± 1E-01
Margin	0.829 ± 2E-03	0.825 ± 5E-03	0.854 ± 4E-02	0.856 ± 2E-02
Entropy	0.853 ± 2E-03	0.822 ± 6E-03	0.869 ± 3E-02	0.858 ± 3E-02
Max response	0.829 ± 3E-03	0.827 ± 5E-03	0.850 ± 4E-02	0.858 ± 2E-02
MC dropout	0.776 ± 6E-03	0.807 ± 3E-03	0.778 ± 6E-02	0.838 ± 3E-02
Cluster distance	0.862 ± 4E-03	0.867 ± 3E-03	0.870 ± 5E-02	0.892 ± 1E-02
Subfunctions (ours)	0.858 ± 4E-03	0.862 ± 2E-03	0.886 ± 2E-02	0.915 ± 2E-02
Table 3: Results for models trained on CIFAR10. Predicting misclassification on in-distribution test.
Higher is better. For equivalent table on CIFAR100, see table 6.
	VGG16		ResNet50	
	AUCEA	AUROC	AUCEA	AUROC
Residual flows density	0.442 ± 5E-02	0.520 ± 1E-02	0.492 ± 3E-02	0.577 ± 1E-02
GP	0.983 ± 1E-03	0.865 ± 8E-03	0.943 ± 5E-03	0.625 ± 2E-02
Class distance	0.948 ± 2E-02	0.669 ± 8E-02	0.900 ± 3E-02	0.471 ± 6E-02
Margin	0.982 ± 2E-03	0.900 ± 4E-03	0.848 ± 1E-02	0.894 ± 4E-03
Entropy	0.989 ± 1E-04	0.914 ± 3E-03	0.984 ± 1E-03	0.890 ± 5E-03
Max response	0.980 ± 3E-03	0.898 ± 4E-03	0.832 ± 1E-02	0.895 ± 5E-03
MC dropout	0.982 ± 6E-04	0.845 ± 6E-03	0.976 ± 2E-03	0.868 ± 1E-02
Cluster distance	0.988 ± 4E-04	0.901 ± 2E-03	0.981 ± 2E-03	0.867 ± 2E-03
Subfunctions (ours)	0.988 ± 3E-04	0.907 ± 3E-03	0.983 ± 2E-03	0.889 ± 6E-03
subfunction error and still holds. Second, computing the set of feasible activation regions (or even
the size of this set) is currently intractable, so we use the full bit space, P = {0, 1}M. This means the
bound is computed in an altered subfunction space where some infeasible subfunctions are assigned
a non-zero density, affecting the weight normalization. An upside of using the full bit space is it
allows us to make some computational savings when computing the bound, detailed in appendix E.
predicted
UoleμlqIJ⅛IPI≡UoHnqljaWPJOJrLO
Wn⅛PUno⅛
Figure 5: Sample confusion matrix, OOD for
CIFAR10 → CIFAR100 on ResNet50. Random
samples from top 20% in each quadrant shown.
These 2 limitations mean that the performance at-
tained by the method in the experiments, already
good, is actually a lower bound that is likely im-
provable if more tractable implementations are
found.
Subfunctions and entropy were found to be the
top 2 methods overall in each category (table 1),
with subfunctions better on average. Cluster dis-
tance also performed well on CIFAR10, but was
penalized by poor performance on CIFAR100
and particularly VGG16 (tables 5 and 6), which
is a more difficult dataset for determining out-
lier status as the in-distribution classes are more
finegrained. We conclude that subfunctions and
entropy were good predictors of unreliability for
both in-distribution and out-of-distribution sce-
narios. The AUCEA results for the in-distribution
setting (tables 3 and 6) mean that using either
to filter predictions would have raised effective
model accuracy (accuracy of accepted samples)
to 90 〜92% from 70 〜73% for the origi-
nal CIFAR100 models, and to 98 〜99% from
8
Under review as a conference paper at ICLR 2022
91 〜92% for the original CIFAR10 models (table 4), on average over thresholds. Entropy is Sim-
pler and computationally cheaper than subfunction error bound, but suffers from the drawback that
it can only be computed exactly if model inference includes a discrete probabilistic variable. This
is the case for these experiments but not in general (e.g. consider MSE objectives), whereas our
method does not have this restriction as it bounds risk more generally.
Empirically, we observed for subfunctions that reliable in-distribution images tended to be proto-
typical images for their class, whilst OOD images erroneously characterized as reliable tended to
resemble the in-distribution classes, as one would expect (fig. 5). To test this hypothesis further, we
took a CIFAR10 model and plotted the rankings of samples from each CIFAR100 class in fig. 6,
where each box denotes the median and first and third quartiles. The classes are ordered by me-
dian. We identified the superclasses in CIFAR100 with the highest semantic overlap with CIFAR10
classes (mostly vehicles and mammals), whose classes are coloured green. It is clear from the corre-
lation between green and lower unreliability ranking that subfunction error bounds rate OOD classes
semantically closer to the training classes as more reliable. Note that even the exceptions towards
the right are justified, because the inclusion of e.g. bicycles, lawn mowers and rockets in “vehicles”
is questionable; certainly these objects do not correspond visually to the vehicle classes in CIFAR10.
In addition, we ran the same plot for the other methods and found that in every case the correlation
between reliability and semantic familiarity was less strong (appendix I).
Figure 6: OOD for CIFAR10 → CIFAR100 on ResNet50. 10k CIFAR100 test samples were ranked
by unreliability (log STEB). Boxplots summarize rankings per class (lower = less unreliable). Green
denotes superclasses similar to CIFAR10: carnivores, omnivores, herbivores, mammals, vehicles.
5 Conclusion
We proposed a new form of input-conditioned generalization error bound for piecewise linear neu-
ral networks. This is done by shattering the network into activation regions and corresponding
subfunctions before computing smoothed error according to a weighting function. The bound on
this input-conditioned error indicates that dense regions are more reliable, whilst integrating over
subfunctions suggests that fewer activation regions with higher training sample density are indica-
tive of better generalization at model-level. In experiments we verified that the input-conditioned
bound is empirically meaningful and can be used to detect misclassified in-distribution as well as
OOD samples, but challenges remain in improving computational tractability.
9
Under review as a conference paper at ICLR 2022
Ethics statement
Given that this work proposes a theoretical approach for assessing the generalization of neural net-
works, the authors do not foresee any specific negative social impacts.
Reproducibility statement
The code for reproducing all results and figures has been released with this paper.
References
Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish.
Invariance principle meets information bottleneck for out-of-distribution generalization. arXiv
preprint arXiv:2106.06607, 2021.
Hirotugu Akaike. A new look at the statistical model identification. IEEE transactions on automatic
control ,19(6):716-723,1974.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
OfSciences,116(32):15849-15854, 2019.
Liron Bergman and Yedid Hoshen. Classification-based anomaly detection for general data. arXiv
preprint arXiv:2005.02359, 2020.
Ricky TQ Chen, Jens Behrmann, David Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for
invertible generative modeling. arXiv preprint arXiv:1906.02735, 2019.
Chi-Keung Chow. An optimum character recognition system using decision functions. IRE Trans-
actions on Electronic Computers, pp. 247-254, 1957.
Luigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for
improving classification reliability of multilayer perceptrons. IEEE Transactions on Neural Net-
works, 6(5):1140-1147, 1995.
Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to
algorithms. MIT press, 2009.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059.
PMLR, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The journal of machine learning research, 17(1):2096-2030, 2016.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. arXiv preprint
arXiv:1705.08500, 2017.
Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition.
arXiv preprint arXiv:2011.15091, 2020.
Boris Hanin and David Rolnick. Deep ReLU networks have surprisingly few activation patterns.
Advances in Neural Information Processing Systems, 32:361-370, 2019.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In ICLR, 2017.
Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classification and segmentation. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 9865-9874, 2019.
Heinrich Jiang, Been Kim, Melody Y Guan, and Maya R Gupta. To trust or not to trust a classifier.
In NeurIPS, pp. 5546-5557, 2018.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. arXiv preprint arXiv:1807.03888, 2018.
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution
image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.
Dianbo Liu, Alex Lamb, Kenji Kawaguchi, Anirudh Goyal, Chen Sun, Michael Curtis Mozer, and
Yoshua Bengio. Discrete-valued neural communication. arXiv preprint arXiv:2107.02367, 2021.
Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshmi-
narayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. arXiv preprint arXiv:2006.10108, 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
GUido Montufar, Razvan Pascanu, KyUnghyUn Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. arXiv preprint arXiv:1402.1869, 2014.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
Evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the ex-
pressive power of deep neural networks. In international conference on machine learning, pp.
2847-2854. PMLR, 2017.
Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvari, and John Shawe-Taylor. Pac-bayes analysis
beyond the usual bounds. arXiv preprint arXiv:2006.13057, 2020.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for in-
formation extraction. In International Symposium on Intelligent Data Analysis, pp. 309-318.
Springer, 2001.
Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised
outlier detection. arXiv preprint arXiv:2103.12051, 2021.
Claude E Shannon. A mathematical theory of communication. The Bell system technical journal,
27(3):379-423, 1948.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443-450. Springer, 2016.
Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive
learning on distributionally shifted instances. arXiv preprint arXiv:2007.08176, 2020.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In
European Conference on Computer Vision, pp. 102-117. Springer, 2020.
11
Under review as a conference paper at ICLR 2022
A Proof for Theorem 2.1
This follows the Chernoff/Hoeffding bounding technique. Fix model f, patterns P and thus sub-
functions set H. Let the true data distribution for each activation region ai be Di . Assume that
DS is a distribution over size-N datasets that have the same activation region data distributions
and dataset sizes (Di and Ni, ∀i ∈ [1, C]) as S. Consider S as a dataset drawn iid from DS.
Without loss of generality, since dataset order is immaterial for the error metrics, assume the index
n of sample Sn determines its activation region and distribution from which it is drawn indepen-
dently of other samples. That is, ∀n	∈	[1,	N] : Sn ~	DJn iid for some fixed J ∈	[1, C]n.
Recall that Di is constructed as PDi (x,	y)	=	PD(x, y|x ∈	ai). Call the inputs and targets	XS and
YS, ∀n ∈ [1, N] : (XSn, YSn) , Sn. Recall that empirical error function for samples is bounded:
∀x, y ∈ (X × Y) : 0 ≤ r((x, y); f) ≤ 1 and weighting function k is non-negative valued.
Choose any subfunction hi ∈ H, i ∈ [1, C]. Then ∀e > 0,t > 0, S ~ DS iid:
^
^
^
P(RS (hi) - RD S (hi) ≥ e) = P(RS (hi) - E [RS (hi)] ≥ e)
—-

MRS(hi)-ES[RS(hi)]) ≥ ete)
(19)
(20)
≤ e-t
ʌ.
ʌ.
e-t
.^... . . - . ...
E[et(RS (hi)-ES [RS(hi)])]
, PN ( k(H(XS),hi) r(Snf)	E
E[et 2,n=1 INPj∈[i,c] P(hj)k(H(Xn),hj)-ESn
k(H(XS),hi) r(Sn;f)
N Pj∈[1,C] P(hj ) k(H(XS),hj )
(21)
(22)
N	+(	k(H(XS),hi) r(Snf)	E
=e-te Y E [et'NPj∈[i,c]P(hj) k(H(Xn),hj)	Sn
n=1
N	t2 ( _1____0)2
'n P(hi)	-0
≤ e-te e 8
n=1
t2
----t---—te
=e 8N P(hi)2
k(H(XS),hi) r(Snf)	]、
N Pj∈[1,C] P(hj ) k(H(XS),hj)]
(23)
(24)
(25)
]
by making use of the following:
1.
2.
3.
4.
eq. (20) → eq. (21) : Markov’s inequality. For random variable Z ≥ 0 and constant a > 0,
P(Z ≥ a) ≤ EZl.
eq. (21) → eq. (22): definition of RS(hi) and linearity of expectation.
eq. (22) → eq. (23): RS(hi) isa sum over independently drawn samples. ∀hj ∈ H : P(hj)
is constant because activation region dataset sizes are constant.
eq. (23) → eq. (24) : first bound the length of the range of the exponent. ∀i ∈ [1, C], n ∈
Γ1 N 0	k(H(Xn ),hi)r(Snf)	/
[1,N] : 0 ≤ N Pj∈[i,c] P(hj) k(H (Xn) ,hj) ≤
k(H(XS),hi)
N P(hi) k(H(XS),hi) = N P(hi).
Subtract-
ing a constant does not change the length of the range of a random variable. Then ap-
ply Hoeffding’s lemma: for random variable Z where a ≤ Z ≤ b and E[Z] = 0, then
∀t > 0 : E[etZ] ≤ e t (b8 a) holds.
Find the optimal t as the one that yields the tightest bound:
t2
t----—t,e
Vt e 8N P(hi)2
0, t = 4N P(hi)2,
(26)
which is a minimum because the second derivative is positive. Substitute into eq. (25) :
P (RS (hi) - RD S (hi) ≥e) ≤ e-2NJ P(hi)2.
(27)
The symmetric case can be proved in the same way:
_ ，^... ，一、 ___________ ，一、 、 _ __________ ，
P(RS(hi) - RDS (hi) ≤ Y = P(RDS (hi) - RS(hi) ≥ E) ≤ e
-2N 2 P(hi)2
(28)
^
1
12
Under review as a conference paper at ICLR 2022
specifically because swapping the order of the subtraction in eq. (23) does not change the value of
the squared range used in eq. (24). Combining eq. (27) and eq. (28):
P (|RbS (hi) - RD S(hi)l ≥ e) ≤ 2e-2NJ P(hi)2.	(29)
Setting the right hand side to δ and solving for completes the derivation. Namely for any δ > 0,
the following hold with probability ≤ δ :
IRS(hi) — Rds (hi)1 ≥	ɪʌ/logi P(hi) V 2N	(30)
. . ^.. . RDS (hi) — RS(hi) ≥	1	Sslogl P(hi)V 2N .	(31)
Hence the following hold with probability > 1 — δ :		
_ . ，-、 ^, , , . RDS (hi) <RS(hi) +	1 s∕lθgT P(hi) V 2N	(32)
_ . ，-、 ^, , , . RDS (hi) ≤ RS(hi) +	1 s∕logT P(hi) V 2N .	(33)
		.
B Proof for Corollary 2.3
Use shorthand ^，RS(hi),ri，RDJhi) to denote empirical error and true error. Start from
eq. (30) which stated for any subfunction with index i and δ > 0, the following holds with probabil-
ity ≤ δ:
lri-ril≥ P⅛V⅞:	(34)
Take a union bound over all the subfunctions, i.e. with some probability ≤ Cδ, where C is the
number of subfunctions, these hold:
-	1	/log 2^
ORk lri-ril≥ 所"	(34)
ORk ri ≥ ^ + ɪ ʌ/ɪɑgδ	(35)
i P(hi) V 2N	' '
the latter since eq. (35) is less likely than eq. (34). So with probability > 1 - Cδ:
ANDk
(36)
Rewrite δ0 = Cδ . In summary, for any δ0 > 0, the following holds for all subfunctions hi in the
network, with probability > 1 - δ0 :
ri < ^ +
(37)
.
C Proof for entropy of discrete uniform distribution
Let X by discrete random variable with n states and a uniform distribution. Then its entropy H(X)
is given by:
H(X)
log - log = — log = log n
nn	n
x∈X
(38)
which increases monotonically with n
.
13
Under review as a conference paper at ICLR 2022
D Proof for eq. (18)
∀P ∈ RC≥0, write the Lagrangian for eq. (18) as:
L(P,λ)= X P - λ((X Pi
i∈[1,C] i	i∈[1,C]
Setting the partial derivatives to 0:
∀i	：	∂L	—	- λ = o→P	=上
∀i:	∂Pi	= p2 λ = 0→Pi= √λ
dL = -( X Pi)+1=0
i∈[1,C]
Equation (40) and eq. (41) combined imply the uniform distribution, ∀i : Pi
minimizes rather than maximizes Ρi∈[i c] p1:. Proof by contradiction: for C
2 Δ____> ⊥
2 0.5 →⊥
(39)
(40)
(41)
ɪ. Further, this
2 ɪ I ɪ
2, 0.8 + 0.2 <
.
E Efficient computation of bound
For normalization in eq. (5), we reduce computational complexity from exponential O(N2M) to
linear O(M) (derivations below):
X X Nj k(hl, hj) = NXd(b) Mb ) ,u,	(42)
l∈[1,C] j∈[1,C]	b=0
s.t. Nj >0
and for normalization in eq. (10), from exponential O(N 22M) to polynomial O(N2 + M3):
∀ 1∈[1,C] : X P(hj) k(hι,hj) = 1 X Ni Z(Hamming(pι,Pi)),	(43)
s.t. Nl>0 j∈[1,C]	u i∈[1,C]
s.t. Ni>0
where ∀a ∈ [0, M] : z(a) ,XM d(b) Xb	a)M-a)
d(a + (b - c) - c).	(44)
Normalization term in eq. (5). Computed once for the network.
Nj	k(hl,	hj)	=	Nj	k(hl,	hj)	= N	d(b)	Mb	,u,
l∈[1,C] j∈[1,C]	j∈[1,C] l∈[1,C]	b=0
s.t. Nj >0	s.t. Nj >0
(45)
where the trick is that l∈[1,C] k(hl , hj ) = l∈[1,C] d(Hamming(pl , pj )) is the same for all hj
due to completeness of the bit space P, and the specific value can be found by looping through
all possible Hamming distances b. This reduces the computational complexity from O(N 2M ) to
O(M), because the number of populated activation regions, i.e. length of loop over j, is upper
bounded by the number of samples N, and Mb can be iteratively updated in O(1) in the loop over
b:
M — (b — 1)
b
(46)
Normalization term in eq. (10). Computed ∀l ∈ [1, C] s.t. Nl > 0:
14
Under review as a conference paper at ICLR 2022
P(hj) k(hl,hj)
j∈[1,C]
1 X X	k(hι,hj) k(hj, hi) Ni
u j∈[1,C] i∈[1,C]
s.t. Ni>0
1 X Ni X k(hι,hj) k(hj,h)
i∈[1,C]
s.t. Ni >0
j∈[1,C]
=z(Hamming(pl ,pi)) (eq. (44))
人
from k(hl ,hj )
Ni	zd}(|b{)
i∈[1,C]	b=0
s.t. Ni >0
c=0
Hamming(pl , pi)
- Hamming(pl , pi)
b-c
from k(hj ,hi)
人
d(Hamming(pl , pi) + (b - c) - c) .
(47)
(48)
(49)
(50)
1
u
z
{
M
b
c
z
{
Note the loops over i and l only need to range over populated activation regions due to the inclusion
of their sample counts (Ni and Nι) as multiplicative factors in their loop contents, which means the
iterations are bounded by O(N). The problematic loop is over j, i.e. all subfunctions/activation
regions whether populated by samples or not, which is O(2M) when P is the full bit space {0, 1}M.
However, similar to eq. (45) above, it is also this fullness that we exploit.
Now we explain eq. (50) in detail. The value
z(Hamming(pι , pi)) corresponds to the sum over j in
eq. (49): loop over every bit pattern pj in P , multiply its
closeness to pι with its closeness to pi (both fixed outside
this loop), and sum. Now group pj by bit distance to pι, b.
Consider one instance of b (fig. 7). Within this group for
b, the closeness of every pj to pι is constant, namely equal
to d(b). But they have different distances to pi. However
we know the number of bits that are different between pι
and pi: this is Hamming(pι, pi). Consider a specific in-
stance of pj . Visualize pι turning into pj by flipping bit
one at a time; each will either bring the pattern closer to or
further away from pi . There is a budget of b different bits
to flip to reach pj . Say c of those flips brought it closer
(i.e. turned the bit value at that position into the same as
Figure 7: Sphere of pj that are b bits
away from pι . For illustration only.
pi ’s). Then we know b - c brought it further. So the number of different bits between pj and pi is
exactly Hamming(pι, pi) + (b - c) - c. And the number ofpj that fit this description is the number
of ways of choosing c from the different bits between pι and pi multiplied by the number of ways of
choosing b - C from the shared bits between pι and pi: (Hamming(Pl,Pi))(M-Hamb-Ing(Pl,Pi)).
Conveniently, gq = 0 for g > q, so there is no need to add special cases to exclude the cases where
the number we seek to choose is greater than the number available.
Equation (50) enables computational complexity reduction of eq. (48), including the outer loop over
l, from exponential to polynomial time: O(N 22M) to O(N2 + M3) (excluding computation of u).
The M3 comes from constructing lookup table z, with the cubed power coming from looping over
all possible values for Hamming(pι, pi), b and C. This subsumes M2 to compute ∀q ∈ [0, M], ∀g ∈
[0, M] : qg in the same iterative manner as eq. (46).
F Additional results tab les
15
Under review as a conference paper at ICLR 2022
Table 4: Test accuracy of original models.
Dataset	Model	Accuracy
CIFAR100	VGG16	0.704 ± 1E-03
CIFAR100	ResNet50	0.729 ± 8E-03
CIFAR10	VGG16	0.920 ± 2E-03
CIFAR10	ResNet50	0.908 ± 5E-03
Table 5: Model trained on CIFAR100. Out-of-distribution detection (AUROC) vs CIFAR10/SVHN.
	→ CIFAR10		→ SVHN	
	VGG16	ResNet50	VGG16	ResNet50
Residual flows density	0.495 ± 2E-04	0.495 ± 2E-04	0.090 ± 2E-04	0.090 ± 2E-04
GP	0.708 ± 6E-03	0.404 ± 3E-02	0.830 ± 3E-02	0.396 ± 8E-02
Class distance	0.627 ± 4E-02	0.513 ± 4E-02	0.833 ± 3E-02	0.579 ± 1E-01
Margin	0.716 ± 2E-03	0.736 ± 3E-03	0.771 ± 5E-03	0.790 ± 3E-02
Entropy	0.725 ± 3E-03	0.745 ± 3E-03	0.786 ± 6E-03	0.814 ± 4E-02
Max response	0.719 ± 3E-03	0.740 ± 3E-03	0.776 ± 6E-03	0.799 ± 3E-02
MC dropout	0.693 ± 3E-03	0.725 ± 4E-03	0.771 ± 1E-02	0.795 ± 3E-02
Cluster distance	0.639 ± 1E-02	0.754 ± 4E-03	0.561 ± 2E-02	0.810 ± 5E-02
Subfunctions (ours)	0.738 ± 2E-03	0.750 ± 7E-03	0.797 ± 8E-03	0.807 ± 3E-02
Table 6: Model trained on CIFAR100. Predicting misclassification on in-distribution test (AUCEA
and AUROC).
	VGG16		ResNet50	
	AUCEA	AUROC	AUCEA	AUROC
Residual flows density	0.293 ± 1E-02	0.622 ± 1E-02	0.293 ± 2E-02	0.630 ± 1E-02
GP	0.882 ± 1E-03	0.803 ± 2E-03	0.670 ± 2E-02	0.384 ± 2E-02
Class distance	0.838 ± 3E-02	0.739 ± 6E-02	0.627 ± 7E-02	0.476 ± 3E-02
Margin	0.899 ± 2E-03	0.852 ± 4E-03	0.870 ± 6E-03	0.855 ± 4E-03
Entropy	0.895 ± 2E-03	0.856 ± 5E-03	0.916 ± 4E-03	0.859 ± 4E-03
Max response	0.899 ± 2E-03	0.853 ± 4E-03	0.864 ± 7E-03	0.857 ± 4E-03
MC dropout	0.894 ± 1E-03	0.828 ± 8E-03	0.898 ± 5E-03	0.841 ± 2E-03
Cluster distance	0.833 ± 9E-03	0.722 ± 2E-02	0.900 ± 5E-03	0.824 ± 7E-03
Subfunctions (ours)	0.904 ± 1E-03	0.864 ± 3E-03	0.902 ± 4E-03	0.827 ± 4E-03
16
Under review as a conference paper at ICLR 2022
G Metrics
Evaluation metrics were area under the graphs of coverage vs. effective accuracy (AUCEA) and
false positive rate vs. true positive rate (AUROC), with the latter as standard for OOD experiments.
TP+FP	TP
coverage = TP + FP + TN + FN	effective accuracy = precision = TP + FP	(51)
FP	TP
false positive rate = FP + TN true positive rate = TP + FN.	(52)
H Additional experimental details
Experiments were averaged over 5 random seeds. The classification models were trained with SGD
optimization with learning rate 0.1, momentum 0.9, weight decay 5e-4 and standard schedules: 100
epochs with learning rate ^0.1 every 30 epochs (CIFAR10) and 200 epochs with learning rate *0.2
every 60 epochs (CIFAR100). For each unreliability metric, each threshold yielded one set of ac-
cept/reject decisions for the test set which yielded one point in each evaluation graph, and area under
graph was computed using the trapezoidal rule. For the in-distribution setting, AUCEA corresponds
to effective model accuracy (i.e. of accepted samples) averaged over different thresholds, and thus
can be compared against original model accuracy.
H. 1 Subfunction error bound hyperparameters
The searched values for likelihood parameter δ were [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9] and for weighting function standard deviation parameter ρ were [32, 48, 64, 98, 128, 192,
256, 384, 512] (VGG16) and [4, 6, 8, 12, 16, 24, 32, 48, 64] (ResNet50). Validation set AUCEA
was used for selection. The selected values are in table 7.
Table 7: Hyperparameters used for subfunction error. Brackets denote number of seeds.
Dataset	Model	P	δ
CIFAR100	VGG16	128.0 (#: 5)	0.001 (#: 5)
CIFAR100	ResNet50	16.0 (#: 5)	0.001 (#: 4), 0.1 (#: 1)
CIFAR10	VGG16	48.0 (#: 3), 98.0 (#: 1), 64.0 (#: 1)	0.001 (#: 4), 0.9 (#: 1)
CIFAR10	ResNet50	16.0 (#: 3), 24.0 (#: 2)	0.001 (#: 4), 0.3 (#: 1)
H.2 Dataset statistics
All results infer unreliability of the test sets. The datasets are publicly available.
Dataset	Train	Validation	Test
CIFAR10 (Krizhevsky, 2009)	42500	7500	10000
CIFAR100 (Krizhevsky, 2009)	42500	7500	10000
SVHN (Netzer et al., 2011)	62269	10988	26032
H.3 Computational resources
Experiments were run given a shared cluster of machines with approximately 140 GPUs. Jobs re-
quired less than 24GB GPU memory. For subfunction error, normalization constants were computed
first in a pre-computation phase. This took up most of the runtime and was parallelized by splitting
jobs by architecture, dataset, seed and ρ; each job took approximately 20 minutes. Subsequent in-
ference on the test sets (i.e. computing unreliability of unseen samples) took approximately 2 - 10
minutes per combination of architecture, dataset and seed.
17
Under review as a conference paper at ICLR 2022
I Additional B oxplots for other methods
Settings apart from the method are the same as fig. 6. We measured the Spearman correlation
coefficient between semantic similarity (green=1, purple=0) and reliability (- median unreliability
rank for each class) and found it was lower for all baselines compared to subfunctions, which had a
correlation coefficient of 0.511. The closest baseline method was MC dropout (0.433).
Figure 8: Entropy.
Figure 9: Max response.
Figure 10: Margin.
18
Under review as a conference paper at ICLR 2022
Figure 11: Class distance.
EnUEnbe
,dn*⅛

13: MC dropout.
6UMUe」AI三q,≡-alun
14: Cluster distance.
Figure 15: Residual flows density.
19