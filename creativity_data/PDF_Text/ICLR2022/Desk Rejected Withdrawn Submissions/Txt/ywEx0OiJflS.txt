Under review as a conference paper at ICLR 2022
Multi-Class Classification from
Single-Class Data with Confidences
Anonymous authors
Paper under double-blind review
Ab stract
Can we learn a multi-class classifier from only data of a single class? We show
that without any assumptions on the loss functions, models, and optimizers, we
can successfully learn a multi-class classifier from only data of a single class
with a rigorous consistency guarantee when confidences (i.e., the class-posterior
probabilities for all the classes) are available. Specifically, we propose an empirical
risk minimization framework that is loss-/model-/optimizer-independent. Instead
of constructing a boundary between the given class and other classes, our method
can conduct discriminative classification between all the classes even if no data
from the other classes are provided. We further theoretically and experimentally
show that our method can be Bayes-consistent with a simple modification even
if the provided confidences are highly noisy. Then, we provide an extension of
our method for the case where data from a subset of all the classes are available.
Experimental results demonstrate the effectiveness of our methods.
1 Introduction
In supervised learning, the annotations of a huge number of instances may not be easily obtained
in many practical applications due to the concerns including but not limited to time consumption,
expenditure, and privacy preserving. For these reasons, many weakly supervised learning (WSL)
frameworks (Zhou, 2018) have been studied in various scenarios recently, including semi-supervised
learning (Chapelle et al., 2006; Zhu & Goldberg, 2009; Niu et al., 2013; Li & Zhou, 2015; Sakai
et al., 2017; Li & Liang, 2019; Guo et al., 2020), positive-unlabeled learning (Elkan & Noto,
2008; du Plessis et al., 2014; 2015; Sansone et al., 2019), unlabeled-unlabeled learning (Lu et al.,
2019; 2020), noisy-label learning (Natarajan et al., 2013; Han et al., 2018a;b; Zhang et al., 2019),
complementary and partial-label learning (Ishida et al., 2017; Yu et al., 2018; Ishida et al., 2019;
Feng et al., 2020a; Katsura & Uchida, 2020; Chou et al., 2020; Cour et al., 2011; Feng et al., 2020b;
Lv et al., 2020), similarity-based learning (Bao et al., 2018; Shimada et al., 2019; Cao et al., 2021),
and positive-confidence learning (Ishida et al., 2018).
In this paper, we investigate a novel weakly supervised learning scenario called Single-Class Con-
fidence (SC-Conf) classification, where only data of a single class annotated with the confidences
(i.e., the class-posterior probabilities for all the classes) are required for training a discriminative
multi-class classifier with convergence guarantee on estimation error, without any data from other
classes. Such a WSL framework can be widespread in many real-world scenarios. For example, in
the scenario of climatic disaster forecasting, in order to conduct ordinary supervised classification,
we deploy sensors to collect data of normal climate and different kinds of meteorological disasters.
However, under extreme climate conditions, e.g., typhoons, tornados, and snowstorms, the sensors
may not work properly and we can only collect data from the distribution of normal climate. To
construct a climatic disaster forecasting system that can discriminate different kinds of climates
with only data of normal climate, we can ask meteorologists to annotate the data with a confidence
score and train the system using our proposed framework with only data of normal climate. Another
example is about market investigation. The investigators of each company aim to predict the con-
sumers’ tendency of purchasing the products of their companies and other competitors. However,
due to privacy concerns, there may be data isolation, i.e., each company can only have access to their
customers’ information. To accomplish the investigation with the isolated data, the investigators can
gather the purchase history of their customers and transform the purchase amount of each company
1
Under review as a conference paper at ICLR 2022
Ordinary Classification	One-Class Classification	PConf Classification	SC-Conf Classification
・▲ Oi OZx
Labeled examples High Pconfto Low Pconf Underlying examples	SC-Conf examples
Figure 1: Illustration of the proposed method and related works. The points with dotted lines are not necessary
in the training process and are only shown for illustration. We further show in Section 4 that even We have
no information about the value of confidence scores, we can still conduct consistent SC-Conf classification
only with the class with the maximum class-posterior probability: y0 = argmaXy∈γsp(y∣x). We also show in
Section 5 that it is possible to learn from a subset of all the classes.
into confidence between 0 and 1 by pre-processing. With the isolated data and confidence scores, the
predictor can be trained by SC-Conf classification.
To apply the SC-Conf classification framework in more realistic scenarios, we show that with a
small modification, the result of our method with extremely noisy confidence can be consistent
with that with accurate confidence, i.e., our framework is noise-robust. To justify our claims, both
infinite-sample and finite-sample analysis are provided.
Furthermore, we extend our method to the case where we can collect data from a subset of all
the classes with confidences. For example, when conducting multi-class classification of climatic
disasters including sandstorm, snowstorm, drought, and flood, we can train the classifier using data
only with a coarse label ‘wind storm’ instead of collecting data from all four classes. Here we don’t
have to specify the true labels of each instance, e.g., the further division of data with coarse label
‘wind storm’ into ‘sandstorm’ and ‘snowstorm’ is unnecessary. We refer to this framework as Subset
Confidence (Sub-Conf) classification in the rest part of this paper.
Our contributions in this paper are three-fold:
•	We provide an unbiased estimator of ordinary classification risk with only data of a single class
and their confidences. An optimizer-independent empirical risk minimization (ERM) framework
with no assumptions on loss functions and models is proposed. We further establish the estimation
error bound to show the consistency of the proposed method.
•	We show that if unlabeled samples are available, our proposed method is noise-robust and can
be classifier-consistent and can converge to the optimal classifier with high probability even with
extremely noisy confidence. Furthermore, we give a novel finite-sample convergence analysis on
the misclassification rate of this method.
•	An extension of our ERM framework to the case where data from a subset of all the classes with
confidences are provided.
Extensive experiments with deep neural networks on both benchmark and real-world datasets are
conducted for demonstrating the usefulness of our proposed methods.
2	Related Work
In this section, we introduce related studies of the proposed SC-Conf classification framework. We
briefly illustrate the proposed problem and related problems in Figure 1.
Multi-Class Classification For ordinary K -class multi-class classification problem, X ⊂ Rd is
the feature space and Y = [K] is the label space. Suppose each example (x, y) ∈ X × Y is drawn
independently from an unknown distribution with density p(x, y). To train a learning function for
2
Under review as a conference paper at ICLR 2022
multi-class classification g : X → RK, we have to minimize the classification risk below:
R(g) = Ep(χ,y) ['(g(χ), y)],	(1)
where '(∙, ∙) : RK ×Y → R+ is the multi-class loss function and Ep(χ,y)[∙] is the expectation on
distribution with density p(x, y). The predicted label of x is given as f(x) = argmaxy∈Y gy(x),
where gy (x) is the y-th element of g(x). Since we are not aware of the joint density p(x, y), we
collect identically and independently distributed examples {(xi, yi)}in=1 and minimize the empirical
risk (the sample mean of the losses of the collected examples) instead.
Multi-Positive and Unlabeled Learning. Multi-positive and unlabeled learning (Xu et al., 2017)
is a WSL framework that can train multi-class classifiers using labeled data from K - 1 classes,
unlabeled data collected from the distribution with density p(x), and the class-prior probability of
the unseen class. However, it is quite hard to cope with these requirements in real-world scenarios.
On the other hand, our proposed SC-Conf classification framework only needs data of a single class
and their confidence while it can still train multi-class classifiers.
One-Class Classification. One-class classification (Breunig et al., 2000; Tax & Duin, 1999;
Scholkopf et al., 2001; Ruff et al., 2018) aims at 'describing, a given class with data of a single class
rather than conducting discriminative classification. Furthermore, it cannot construct classification
boundaries between all of the K classes with a finite-sample convergence guarantee since it has no
access to the information of the rest K-1 classes. Compared with one-class classification, our proposed
SC-Conf classification framework can construct discriminative multi-class classifiers with only data
of a single class by utilizing the confidence scores. Due to the discussion above, it can be seen that
the one-class classification methods are not suitable in the problem of multi-class classification.
Positive-Confidence Classification. In Ishida et al. (2018); Shinoda et al. (2020), Pconf classifi-
cation is proposed to train discriminative binary classifiers only from positive examples and their
positive-confidence (class-posterior probabilities of positive class). It can be regarded as a special case
of our proposed SC-Conf classification when the number of classes is K = 2 since the class-posterior
probability of the negative class can be immediately obtained when the Pconf is given. Nevertheless,
it is still confined in the binary classification setting. When applied to multi-class classification, the
Pconf framework can only discriminate the class regarded as the ‘positive’ class from the rest of all
the classes, while the SC-Conf can conduct discriminative classification between all the classes, as
shown in Figure 1.
3	SC-Conf Classification
In this section, we formulate the SC-Conf classification problem. In ordinary multi-class classification
problem, we collect i.i.d. labeled data {(xi, yi)}in=1 from the joint distribution p(x, y) and conduct
ERM (Vapnik, 1998) by minimizing the unbiased risk estimator of classification risk (1) constructed
by the labeled data. However, under the SC-Conf classification framework, we are only given data
from a single class ys with confidence: S = {xi, ri}in=1, where xi is an instance drawn independently
from the distribution of a single class with density p(x|y = ys) and ri is a confidence vector given
by {p(y|xi)}yK=11. Based on these information, we show how to construct an unbiased risk estimator
and conduct ERM without any instance from the other K - 1 classes in this section.
3.1	Unbiased Risk Estimator
We denote the class-prior probability of class ys as πys. Let r(x) = {p(y|x)}yK=1 and ri (x) =
p(y = i|x). Ep(χ∣ys)[∙] is the expectation on the distribution with density p(x∣ys). The following
theorem shows that the classification risk can be recovered with the supervision above:
Theorem 1. The multi-class classification risk (1) can be recovered from data drawn from a single
class if their confidence scores are given:
R(g) = ∏ysEp(χ∣ys) [PK=1 rrys(g)'(g(x), y)i,	⑵
wherep(ys|x) > 0 for all x in the support of the distribution with density p(x).
1 Note that in real-world applications, the equation ri = p(y|xi) may not hold due to the noisy supervision.
We analyze the effect of noisy confidence in Section 4.
3
Under review as a conference paper at ICLR 2022
The proof is shown in Appendix A. A sketch of the proof is that (2) is essentially duplicating
(x, r) into (x, p(y = 1|x)) , (x, p(y = 2|x)) , ..., (x, p(y = K|x)) and then applying importance
weighting to adjust the difference between the distribution of a single class and all the classes. It
is noteworthy that the requirement thatp(ys|x) > 0 is widespread and used in the previous works
(Ishida et al., 2018; Shinoda et al., 2020). It can also be easily satisfied in many practical applications.
For example, when the market investigators of a leading company analyzing the tendency of their
potential consumers, they will find that almost all of the consumers (i.e., x) have interest of their
products (i.e., ys) since their company plays a dominant role in the market. According to Theorem
1, we can see that Eq. (2) is an equivalent risk expression of Eq. (1). Then we can directly get the
unbiased risk estimator of Eq. (2):
Rsc(g) = ∏ys Pi=I PK=1 *'(g(χi),y).	⑶
Since Eq. (2) does not include the expectation over the data of the other K - 1 classes, only data
from the single class ys are used in Eq. (3) for constructing the unbiased risk estimator. Then the
following work is to conduct ERM by minimizing the unbiased risk estimator in Eq. (3). Since the
class-prior probability πys can be safely ignored without changing the result of ERM, we do not have
to estimate πs in the process of SC-Conf classification.
3.2	Estimation Error Bound
Here We analyze the consistency of the proposed unbiased risk estimator RSC(g) in (3) and give
the convergence rate of its estimation error. First of all, let G = [Gy]yK=1 be a class of K-valued
functions that We consider in ERM. Assume that there is Cg > 0 that supg∈G kgk∞ ≤ Cg. The loss
function '(g(x),y) is Lipschitz continuous for all kgk∞ ≤ Cg with Lipschitz constant l` > 0 and
upper-bounded by C' > 0. At the end, we assume that the confidence of the given single class is not
too small that there exists Cr > 0 such thatp(ys|x) > Cr holds almost surely. Though we can omit
the p(ys|x) and analyze the biased version of RSC(g), for simplicity, only the unbiased version is
analyzed here. Denote by gsc the minimizer of RSC (g) in (3) and g* the minimizer of (1). Then the
estimation error is given by R(g) - R(g*). The following theorem gives the estimation error bound:
Theorem 2. For any 0 < δ < 1, with probability at least 1 - δ:
RIgSC ) - R(g*) ≤ 4√2CysL' PK=I Rn(Gy ) + 2∏ysC'√⅞2,	(4)
where Rn(Gy) is the Rademacher complexity (Bartlett & Mendelson, 2001) of Gyon i.i.d. samples
with size n drawn from a single class p(x|ys).
The definition of Rademacher complexity and the proof is given in the Appendix B. Generally, the
Rademacher complexity Rn(Gy) has an upper bound of CGy /√n (Golowich et al., 2018), where
CGy is a positive number determined by the function class Gy. We can see that the risk of empirically
optimal classifier R(gsc) converges to that of the optimal classifier R(g*) in the rate of Op(1∕√n),
which is the optimal parametric rate in probability and cannot be improved without additional
assumptions (Mendelson, 2008).
4	Consistent Noise-Robust Method for SC-Conf Classification
In Section 3, it can be seen that the confidences play an important role in the construction of the
unbiased risk estimator (3). However, in practical applications, the confidences {ri }in=1 are often
inaccurate, and we can only get the noisy confidence scores {*}□ generated by a corrupted
distribution. Since *=ri, the risk estimator proposed in Section 3 is biased in general if we simply
plug the noisy confidence into (3). As a result, the learning guarantee in the previous section no longer
holds and thus the use of ERM is not reasonable. Can we obtain a provably consistent multi-class
classifier with data of a single class and extremely noisy confidence scores? Surprisingly, in this
section, we give a positive answer to this question.
We propose a refined method called Noise-Robust Single-Class Confidence (NoRSC-Conf) classifi-
cation, where only data of a single class, unlabeled data, and noisy confidence scores are provided.
Then we show that this method is classifier-consistent (Yu et al., 2018; Feng et al., 2020b; Lv et al.,
4
Under review as a conference paper at ICLR 2022
2020): the predictions of the classifier generated from noisy supervision is still infinite-sample consis-
tent to those of the optimal classifier of classification risk R(g). Based on this result, we establish the
ERM framework for NoRSC-Conf classification. Since there is a gap between the convergence on the
corrupted distribution and the original one, we cannot directly analyze the finite-sample consistency
of NoRSC-Conf classification with traditional techniques on bounding the uniform convergence.
To justify the use of ERM, we exploit the connection between the noisy and original distribution
and give a novel finite-sample analysis on the misclassification rate of NoRSC-Conf classification.
Finally, we show that the weight φ(x) used in the construction of NoRSC-Conf classification can be
estimated from unlabeled data and data of a single class by applying the density ratio matching-based
method (Sugiyama et al., 2012) and give its estimation error bound.
4.1	Formulation of NoRSC-Conf Classification
In NoRSC-Conf classification, we have i.i.d. unlabeled data Su = {xiu }in=u1 drawn from the distribu-
tion With marginal density p(x), data of a single class With noisy confidence scores S = {xi, ri}n=「
Here the generation of data of a single class {xi}in=1 is the same with that in the previous section,
and the noisy confidence {r}n=ι are generated by a corrupted distribution with density p(y |x).
Suppose the optimal classifier gbayes = argmin。measurable RR(g) is in the function class G, which
is a Widely used assumption in classifier-consistency analysis, e.g., the Assumption 1 in Yu et al.
(2018). Straightforwardly, we give the the formulation and show the infinite-sample consistency of
the NoRSC-Conf classification method:
Theorem 3. For each instance x, if argmaxy∈γp(y∣x) = argmaxy∈γ p(y∣x) andthe CIaSSifiCatiOn-
calibrated losses (e.g., softmax cross entropy loss, mean squared error) (Tewari & Bartlett, 2007) are
uSed, the following reweighed riSk formulation iS claSSifier-conSiStent:
Rs) = Ep(χ∣ys) [φ(X) PK=I ry (XMg(X), y)],	⑸
where φ(x) = Pp(X)), ry (x) = p(y∣x), andp(ys∖x) > 0 forall X in the support ofthe distribution
with denSity p(X).
This assumption is realistic in various scenarios, e.g., when the generation process of supervision
is corrupted by some class-conditional noise (Zhang et al., 2021) or some noise that meets the
row-diagonally dominant conditional (Gui et al., 2021). An intuitive explanation is that the decision
boundary of the Bayes-optimal classifier is only determined by the class with the maximum class-
posterior probability of each instance. Then we only have to adjust the distribution difference between
the distribution of a single class and all the classes. The detailed proof is shown in Appendix C. With
regard to the assumption on the noisy confidence, Theorem 3 only requires that for each instance
X, the class with the largest posterior probability in the original distribution should still have the
largest value among all the classes in the noisy distribution. Notice that since there is no assumption
on the exact value of the confidence scores, the confidence scores can be of arbitrary values in the
K-dimensional probability simplex as long as the assumption above is satisfied. This assumption
is realistic in practice. For example, when asking the annotators for labeling the data of a single
class with confidence scores, they may not be sure about the exact value of the true class-posterior
probability of each class. However, it can be easier for them to identify the class of argmaxy∈Y p(y∖X)
and give the highest but not necessarily the accurate confidence score to this class. Though this kind
of confidence score is not accurate in general, according to Theorem 3, it is meaningful in the setting
of NoRSC-Conf classification and can be used for generating consistent classifiers.
Denote by g* the minimizer of (5), then Theorem 3 means that g* and g* have the same prediction,
i.e., argmaxy∈γgy(x) = argmaXy∈γgy(x) for all the x. Thus the prediction of the minimizer of
(5) is infinite-sample consistent to that of the ordinary classification risk (1). Based on this result and
φi = φ(Xi), it is common practice to conduct ERM by minimizing the unbiased estimator of (5):
R(g) =1 Pn=Iφ Py=Iry '(g(χi),y).	⑹
Denote by g the minimizer of the empirical risk (6). Using the same technique of bounding the
uniform convergence as in Section 3.2, we can get the finite-sample analysis of the result: R(g) →
T~i / ~ ⅛ ∖ -Ik τ	,11	.,1	∙ C~	IJI	1	∙ r-'	∙	1	,,1	11
R(gy). Nevertheless, in the scenario of multi-class classification, we are concerning about the problem
5
Under review as a conference paper at ICLR 2022
,1,.C,1	∙	1	∙ r' ,♦	∙ 1 CQ	,1	∙	1	1∙ . ∙1	.,11	♦,	/	∖	,,1
that if the misclassification risk of g on the original distribution With density p(x, y) converges to the
optimal one, i.e., if R01(g) → Roι(g0ι)2, where R01 (g) = Ep(χ,y) [l(argmaXy,∈γgy，(x) = y)]
is the equivalent expression of misclassification rate of classifier g, and g0ι = argming∈gR01(g)
is the optimal classifier of misclassification rate. Since there is difference between the noisy and
original distribution, we cannot get the convergence result of R01(g) directly from the fact that
R(g) → R(g*). Thus the finite-sample analysis of the misclassification rate of g is non-trivial.
In the following section, we will give the answer to this question by exploiting the connection between
p(y |x) and p(y∣x) and give a finite-sample analysis on the misclassification rate of g.
4.2	Finite-Sample Analysis on the Misclassification Rate
In this section, we giveanon-trivial finite-sample analysis on the misclassification rate of g: R01(g)-
R01(go1). To begin with, we first give the convergence analysis on R(g) → R(g*).
Lemma 1. Based on the assumptions in previous parts, for any 0 < δ < 1, with probability at least
1 - δ:
凤g) - R(g*) ≤ 4√CL PK=1 Rn(Gy) + 爰 q⅞2,	(7)
where Rn(Gy) is the Rademacher complexity of Gy on i.i.d. samples with size n drawn from a single
classp(x|ys).
Combining this lemma with the Corollary 26 in (Zhang, 2004), we can directly bound the following
term w.r.t. 0-1 loss: R01(g) - R01(g*):
Lemma 2. If the loss function is classification-calibrated, then there exists a concave function ξ
on [0, +∞) such that ξ(0) = 0 and ξ(δ) → 0 as δ → 0+. Furthermore, for any 0 < δ < 1, with
probability at least 1 - δ:
R01(g) - R01(g*) ≤ ξ(R(g) - R(g*))∙	(8)
where R01(g) is the misclassification rate of g on the noisy distribution with density p(x, y) =
P(y∣x)p(x). The validity ofthe density p(x, y) is shown in the ProofofTheorem 3.
The lemma above shows that R01(g) → R01(g*) in probability as n → +∞. However, this
conclusion is still not informative enough: since there is a gap between the noisy density p(x, y) and
the original distribution p(x, y), the conclusion R01(g) → R01(g*) in probability cannot directly
give an answer to the question that if the risk of the empirical minimizer of NoRSC-Conf classification
R01(g) can converge to the optimal misclassification rate R01 (g0ι) on the original distribution in
probability. To answer this question, we have to make use of the connection between p(x, y) and
p(x, y). For each X ∈ X, let ∆(x) = p(y∣x)-p(y0∣x), where y and y0 are the classes with the largest
and the second largest posterior possibilities, respectively. Then we have the following conclusion:
Theorem 4. Suppose inf x∈X ∆(x) > 0, then for any 0 < δ < 1, with probability at least 1 - δ:
Rθl(g) - Roi (g0l) ≤ infx∈X∆(χ) ξ(R(g) - R(g*))∙	(9)
We prove this conclusion in Appendix D. Combining Lemma 1, Theorem 4, and the property of
function ξ, we can finally say that with the increasing of sample Size n, the misclassification rate of g
converges to the optimal misclassification rate Ro1(go*1) in probability. According to this conclusion,
we can see that the minimizer of the empirical risk 6 possesses finite-sample consistency, which
justifies the use of ERM.
Remark 1. A representative scenario is that only the class with maximum class-posterior probability
is identified for each data point. Then the noisy confidences are given in the form of one-hot codes:
r(x) = [0, •…，1, •一，0], where the only non-zero element 1 is on the y§c-th position and y§c is
the class with maximum class-posterior probability. In this scenario, almost all the details of the
confidence of each class are lost and the given confidences are extremely noisy. It can be seen
that in this scenario, infx∈X ∆(x) = 1. According to Theorem 4, the result of our NoRSC-Conf
classification can still converge to the Bayes-optimal classifier in probability.
2According to the property of classification-calibrated loss, the minimizer g* of R(g) with surrogate loss
' is also the minimizer of misclassification rate R01 (g) among all the measurable functions, where 1(∙) is the
indicator function.
6
Under review as a conference paper at ICLR 2022
4.3	EFFICIENT ESTIMATION OF DENSITY RATIO φ(x) WITH UNLABELED DATA
Though the infinite-sample consistency and the finite-sample consistency of our NoRSC-Conf
classification have been established, there is still an important problem to be solved: the estimation of
the weight term φ(x). In this section, we show that additional information of the class labels is not
required in the estimation of the weight term φ(x) and only the data from only a single class and
unlabeled data are needed.
Let φ(x) be the true density ratio p(x)/p(x|ys) and φ(x) be the estimated one. We can empirically
estimate φ(x) by the density ratio matching method (Sugiyama et al., 2012). Here, we give the
definition of Bregman divergence, which measures the discrepancy between two density ratio models:
Definition 1. (Sugiyama et al., 2012) For any differentiable and strictly convex function η: R → R,
Vη(t) is the subgradient of η. The Bregman divergence of η between the true density ratio φ(x) and
the estimated density ratio φ(x) is given as:
Bn (ΦkΦ) = / p(x∣ys)Vη(φ(x))φ(x)dx - / p(x∣ys)η(φ(x))dx — / p(x)Vη(φ(x))dx,
and its unbiased estimator is given as:
Bn (Φkφ) = 1 pn=1 Vη(φ(Xi))φ(Xi) - n P 乙 η(φ(χi))—十 P 乜 Vη(φ(χu)).	(10)
Since we are given data from the single class ys and the unlabeled data, we can estimate the true
density ratio by minimizing the empirical Bregman divergence (10). Denote by Φ the function class
of ratio model and φ = argmm^∈φBn (φ∣∣φ) and assume that the true ratio model φ ∈ Φ,we have
the following estimation error bound:
Theorem 5.	Based on the assumptions above, for any 0 < δ < 1, with probability at least 1 - δ:
Bn(ΦkΦ*) ≤ C1Rn(Φ) + C2Rnu(Φ)+ Mi J⅛4 + M2√⅛
where Rn(Φ) is the Rademacher complexity of function class Φ on i.i.d. samples with size n
drawn from a single class p(x|ys), Run (Φ) is the Rademacher complexity of function class Φ
on i.i.d. samples with size nu drawn from marginal distribution with density p(x). Meanwhile,
C1, C2 , M1, M2 are constants.
The proof is given in Appendix E. This estimation error bound guarantees that the estimated ratio φ
converges to the true one in the rate of Op(1∕√nU +1∕√n). In other words, our estimated ratio model
will be more accurate as the numbers of data from the single class ys and unlabeled data increases.
5 Sub-Conf Classification
In many practical situations, the data may not be collected from only a single class ys ∈ Y but from
a subset Ys ⊂ Y of all the classes. Here we give the data generation process of this kind of data.
Suppose we have data drawn from a subset of all the classes and their confidence: {xi, ri}i=1, where
the confidence is the same with that in Section 3 and the data {xi}in=1 are i.i.d. samples drawn from
an unknown distribution with conditional density p(x|y ∈ Ys). Then we can get an equivalent risk
expression of the classification risk (1), which is similar to the formulation (2):
Theorem 6.	The multi-class classification risk (1) can be recovered from data drawn from a subset
of the collection of all the classes, i.e., Ys ⊂ Y, if their confidence scores are given:
R(g) =
πYsEp(χ∣y∈Ys)[Py=ι rYs(χX) '(g(χ),y)],
where πYs = Pys∈Ys πys andrYs(x) = Pys∈Ys rys(x).
(11)
The proof is shown in Appendix F. With this conclusion, we can approximate the classification risk
(1) with data from a subset of all the classes as:
R^sub(g) = πYs Pn=I PK=I r⅛ '(g(χi), y),
i
(12)
where riYs = Py ∈Y riys. Then it is routine to conduct ERM based on the unbiased risk estimator
(12). When |Ys| = 1, the Sub-Conf classification is equivalent to the SC-Conf classification, which
indicates that Sub-Conf classification is the generalization of SC-Conf classification. Similarly, the
class-prior probability πYs can be eliminated in the process of ERM.
7
Under review as a conference paper at ICLR 2022
Remark 2. A potential way of constructing equivalent risk expression with data from classes Ys ⊂ Y
is the convex combination of SC-Conf classification:
K ry (x)
R(g) = ΣSys∈γs	"yπysEp(XIys)	[ΣSy=ι	'(g(x),y)J，s.t. ΣSys∈γs	αys	=	1，αys	≥	0，
which is a straightforward extension of SC-Conf classification. However, this method requires that
we have the exact label ys ∈ Ys for each instance. In contrast, it can be seen from the formulation of
the empirical risk (12) that the exact class label for each instance is not necessary in our Sub-Conf
classification method, which shows that our Sub-Conf classification method is non-trivial.
It can be seen that Sub-Conf classification and SC-Conf classification share the nearly identical
risk formulation and data generation process and we can get a similar conclusion for Sub-Conf
classification on its convergence analysis by substituting ys with Ys . Given space limitations, we
omit the convergence analysis of Sub-Conf classification and show the specific formulation of
Noise-Robust Sub-Conf classification in the Appendix G.
6	Experiments
In this section, we experimentally show the usefulness of our proposed methods for training deep
models on two benchmark datasets. The implementation is conducted on NVIDIA GeForce RTX
3090 GPUs based on Pytorch (Paszke et al., 2019) and Sklean (Pedregosa et al., 2011).
Baselines: In the experiments, SC-Conf, NoRSC-Conf, and Sub-Conf are short for ERM with risk
estimators (3), (6), and (12). We compare our methods to a simple weighted classification estimator:
Rw(g) = n pn=ι PK=1 ry'(g(xi),y).	(13)
This binary version of this estimator was used as a baseline in Pconf classification (Ishida et al., 2018).
We regard ERM using this risk estimator with Weighted in the following parts. Though this estimator
is a natural idea of utilizing the confidences when the data is collected from all the classes, it is biased
generally in our setting where only data of a single class or a subset of all the classes are available.
We also offer the result of learning with fully-supervised data. The details of the used datasets and
our experimental setups on them are shown in this section.
Datasets: We evaluate the performance of the proposed methods and baselines on Fasion-MNIST
dataset (Xiao et al., 2017) and CIFAR-10 dataset (Krizhevsky et al., 2009). In the experiments,
we use data of a single class or a subset of all the classes to train a classifier that can conduct
classification successfully on all the classes. The detailed information of the used datasets are listed
in the supplementary materials.
Setup: We briefly introduce the setting of our experiments and the detailed statistics are shown
in the supplementary materials. We train the proposed methods and baseline methods with 3-layer
multi-layer perceptron and DenseNet-161 (Huang et al., 2017) with softmax cross-entropy loss on
Fashion-MNIST and CIFAR-10, respectively. Adam (Kingma & Ba, 2015) is used as the optimization
algorithm. The validation accuracy of the used methods are calculated according to their empirical
risk estimators on a validation set consisted of SC/Sub-Conf data. Then we do not have to collect
additional labeled data for validation. Though we can ask annotators for the values of confidences in
real-world applications, we simulate the confidences here by a probabilistic model. For generating the
confidences, we use the separated labeled dataset for training a probabilistic model with the same loss
function as in the last paragraph and this dataset is separated from any other process of experiments.
Since the minimizer of cross-entropy loss is a good estimator of the class-posterior probability (Yu
et al., 2018; Lv et al., 2020; Feng et al., 2020b), we use the output of the training and validation sets
after a softmax layer as their confidences. As to the noisy confidences, we consider the scenario
in Remark 1. Since most of the elements are zero in this scenario, the SC/Sub-Conf methods are
not applicable here since the denominators in (3) and (12) may be zeros. We conduct experiments
on the proposed NoRSC-Conf method and the baseline weighted method to show the robustness of
NoRSC-Conf against extreme noise.
Results: The experimental results are shown in Table 1 and 2. As we can see, when accurate
confidences are used, SC/Sub-Conf outperforms NoRSC-Conf and Weighted in almost all the cases.
When the size of the subset of all the classes increases, the performance of SC/Sub-Conf is even
8
Under review as a conference paper at ICLR 2022
Table 1: Mean and standard deviation of the classification accuracy over 10 trials for the Fashion-MNIST
dataset. The proposed methods were compared with the baseline Weighted method and fully-supervised method,
with different classes used for training. The best and equivalent methods are shown in bold based on the 5%
t-test, excluding fully-supervised method.
Used Classes		SC/Sub-Conf	NoRSC-Conf	Weighted	Supervised
Coat	Accurate	54.66±2.17^^	^^49.72±2.35^^	49.63±1.82	
	Noisy	-±-	49.64±3.06	49.29±3.87	
Sandal	Accurate	56.02±4∙55	45.65±6.42	44.59±7.98	
	Noisy	-±-	44∙50±5.19	40.28±4.85	
Shirt	Accurate	7L44±4.77	60.26±7.45	57.90±7.46	80.13±2.75
	Noisy	-±-	59.16±4∙51	54.23 ±4.33	
					
Bag	Accurate	7L29±2.99	66.04±2.03	68.31±1.56	
	Noisy	-±-	63.19±4∙24	60.67±2.70	
Trouser & Pullover	Accurate	62.05±2.92	54.25±4.17	53.86±4.68	
	Noisy	-±-	53.74±2.73	50.56±3.87	
Dress & Sneaker &Ankle Boot	Accurate	76.96±2.03	74.64±2.38	73.06±2.43	
	Noisy	-±-	72.14±2∙57	71.42±3.23	
Table 2: Mean and standard deviation of the classification accuracy over 5 trials for the CIFAR-10 dataset.
The proposed methods were compared with the baseline Weighted method and fully-supervised method, with
different classes used for training. The best and equivalent methods are shown in bold based on the 5% t-test,
excluding fully-supervised method.
Used Classes		SC/Sub-Conf	NoRSC-Conf	Weighted	Supervised
Dog	Accurate	53.10±0.93^^	51.52±0.76	51.60±0.98	
	Noisy	-±-	~41.77±0.70-	39.94±0.74	
Airplane & Dog	Accurate	~56.73±0.42-	55.69±0.68	54.47±1.21	
	Noisy	-±-	~46.47±0.59-	45.42±0.83-	
Airplane & Deer & Ship	Accurate	57.12±1.63	53.69±1.70	54.68±0.64	65.62±0.84
	Noisy	-±-	51.11±0.65	48.96±1.00	
Airplane & Cat & Frog	Accurate	~60.11±0.27-	58.95±0.61	57.47±1.26	
& Truck	Noisy	-±-	~54.06±0.47-	52.56±0.85	
Airplane & Cat & Frog	Accurate	~63.25±1.53-	61.19±0.95	59.89±1.43	
Ship & Truck	Noisy	-±-	~58.95±1.05-	54.42±2.38	
comparable to that of fully-supervised learning. Notice that since we only have data from a single
class or a subset of all the classes, the training samples used for SC/Sub-Conf learning are far less than
the whole training set. For a fair comparison, the number of training samples used for fully-supervised
learning is reduced to that of the subset with the most samples. When only the noisy confidences are
given, NoRSC-Conf outperforms the weighted classification baseline in eight cases and is comparable
to it in three cases. This result shows that NoRSC-Conf can alleviate the effect of noisy supervision
and identify the Bayes-optimal classifier, which aligns with our theoretical analysis in Section 4.
7	Conclusion
In this paper, we propose a novel weakly supervised learning setting and effective algorithms for
provably consistent multi-class classification from data of a single class or a subset of all the classes
equipped with confidences. We make three key contributions in this paper. Firstly, we propose an
unbiased risk estimator for multi-class classification from data of a single class with confidences and
provided the estimation error analysis. Secondly, we theoretically show that the proposed method
can be robust to extreme noise and converge to the Bayes-optimal classifier in probability, and both
infinite and finite-sample analyses on the misclassification rate are given. Finally, we extend our
method to the case where data from a subset of all the classes are available. The experimental results
demonstrate the usefulness of our algorithms.
9
Under review as a conference paper at ICLR 2022
References
Han Bao, Gang Niu, and Masashi Sugiyama. Classification from pairwise similarity and unlabeled
data. In ICML,pp. 461-470, 2018.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. In COLT, pp. 224-240, 2001.
Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jorg Sander. LOF: Identifying
density-based local outliers. In SIGMOD, pp. 93-104, 2000.
Yuzhou Cao, Lei Feng, Yitian Xu, Bo An, Gang Niu, and Masashi Sugiyama. Learning from
similarity-confidence data. CoRR, abs/2102.06879, 2021.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien (eds.). Semi-Supervised Learning. The
MIT Press, 2006.
Yu-Ting Chou, Gang Niu, Hsuan-Tien Lin, and Masashi Sugiyama. Unbiased risk estimators can
mislead: A case study of learning with complementary labels. In ICML, pp. 1929-1938, 2020.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. CoRR, abs/1812.01718, 2018.
Timothee Cour, Benjamin Sapp, and Ben Taskar. Learning frompartial labels. J. Mach. Learn. Res.,
12:1501-1536, 2011.
Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from
positive and unlabeled data. In NeurIPS, pp. 703-711, 2014.
Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Convex formulation for learning
from positive and unlabeled data. In ICML, pp. 1386-1394, 2015.
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In KDD,
pp. 213-220, 2008.
Lei Feng, Takuo Kaneko, Bo Han, Gang Niu, Bo An, and Masashi Sugiyama. Learning from multiple
complementary labels. In ICML, pp. 3072-3081, 2020a.
Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably
consistent partial-label learning. In ICML, 2020b.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In COLT, pp. 297-299, 2018.
Xian-Jin Gui, Wei Wang, and Zhang-Hao Tian. Towards understanding deep learning from noisy
labels with small-loss criterion. In IJCAI, pp. 2469-2475, 2021.
Lan-Zhe Guo, Zhi Zhou, and Yu-Feng Li. Record: Resource constrained semi-supervised learning
under distribution shift. In KDD, pp. 1636-1644, 2020.
Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor W. Tsang, Ya Zhang, and Masashi Sugiyama.
Masking: A new perspective of noisy supervision. In NeurIPS, pp. 5841-5851, 2018a.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, pp. 8536-8546, 2018b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, pp. 463-469, 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In CVPR, pp. 2261-2269, 2017.
Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary labels.
In NeurIPS, pp. 5639-5649, 2017.
10
Under review as a conference paper at ICLR 2022
Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positive-confidence
data. In NeurIPS 2018,pp 5921-5932, 2018.
Takashi Ishida, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. Complementary-label
learning for arbitrary losses and models. In ICML, pp. 2971-2980, 2019.
Yasuhiro Katsura and Masato Uchida. Bridging ordinary-label learning and complementary-label
learning. In ACML, pp. 161-176, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998.
Yu-Feng Li and De-Ming Liang. Safe semi-supervised learning: A brief introduction. Frontiers
Comput. Sci., 13(4):669-676, 2019.
Yu-Feng Li and Zhi-Hua Zhou. Towards making unlabeled data never hurt. IEEE Trans. Pattern
Anal. Mach. Intell., 37(1):175-188, 2015.
Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for
training any binary classifier from only unlabeled data. In ICLR, 2019.
Nan Lu, Tianyi Zhang, Gang Niu, and Masashi Sugiyama. Mitigating overfitting in supervised
classification from two unlabeled datasets: A consistent risk correction approach. In AISTATS, pp.
1115-1125, 2020.
Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification
of true labels for partial-label learning. In ICML, pp. 6500-6510, 2020.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Ronald Ortner,
Hans Ulrich Simon, and Sandra Zilles (eds.), ALT, pp. 3-17, 2016.
C. Mcdiarmid. On the method of bounded differences. Surveys in Combinatorics, 1989.
Shahar Mendelson. Lower bounds for the empirical minimization algorithm. IEEE Trans. Inf. Theory,
54(8):3797-3803, 2008.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
Adaptive computation and machine learning. MIT Press, 2012. ISBN 978-0-262-01825-8.
Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NeurIPS, pp. 1196-1204, 2013.
Gang Niu, Wittawat Jitkrittum, Bo Dai, Hirotaka Hachiya, and Masashi Sugiyama. Squared-loss
mutual information regularization: A novel information-theoretic approach to semi-supervised
learning. In ICML, pp. 10-18, 2013.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In NeurIPS, pp. 8024-8035, 2019.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay.
Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12:2825-2830, 2011.
Lukas Ruff, Nico Gornitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert A. Vandermeulen, Alexan-
der Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In ICML, pp.
4390-4399, 2018.
11
Under review as a conference paper at ICLR 2022
Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Semi-supervised
classification based on classification from positive and unlabeled data. In ICML, pp. 2998-3006,
2017.
Emanuele Sansone, Francesco G. B. De Natale, and Zhi-Hua Zhou. Efficient training for positive
unlabeled learning. IEEE Trans. Pattern Anal. Mach. Intell., 41(11):2584-2598, 2019.
Bernhard Scholkopf, John C. Platt, John Shawe-Taylor, Alexander J. Smola, and Robert C.
Williamson. Estimating the support of a high-dimensional distribution. Neural Comput., 13
(7):1443-1471, 2001.
Takuya Shimada, Han Bao, Issei Sato, and Masashi Sugiyama. Classification from pairwise similari-
ties/dissimilarities and unlabeled data via empirical risk minimization. CoRR, abs/1904.11717,
2019.
Kazuhiko Shinoda, Hirotaka Kaji, and Masashi Sugiyama. Binary classification from positive data
with skewed confidence. In IJCAI, pp. 3328-3334, 2020.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estima-
tion in Machine Learning. Cambridge University Press, 2012. ISBN 978-0-521-
19017-6.	URL http://www.cambridge.org/de/academic/subjects/
computer-science/pattern-recognition-and-machine-learning/
density-ratio-estimation-machine-learning?format=HB.
David M. J. Tax and Robert P. W. Duin. Support vector domain description. Pattern Recognit. Lett.,
20(11-13):1191-1199, 1999.
Ambuj Tewari and Peter L. Bartlett. On the consistency of multiclass classification methods. J.
Mach. Learn. Res., 8:1007-1025, 2007. URL http://dl.acm.org/citation.cfm?id=
1390325.
Vladimir Vapnik. Statistical Learning Theory. Wiley, 1998. ISBN 978-0-471-03003-4.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/abs/
1708.07747.
Yixing Xu, Chang Xu, Chao Xu, and Dacheng Tao. Multi-positive and unlabeled learning. In IJCAI,
pp. 3182-3188, 2017.
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary
labels. In ECCV, pp. 69-85, 2018.
Mingyuan Zhang, Jane Lee, and Shivani Agarwal. Learning from noisy labels with no change to the
training process. In ICML, volume 139, pp. 12468-12478, 2021.
Tong Zhang. Statistical analysis of some multi-category large margin classification methods. J.
Mach. Learn. Res., 5:1225-1251, 2004. URL http://jmlr.org/papers/volume5/
zhang04b/zhang04b.pdf.
Zhenyu Zhang, Peng Zhao, Yuan Jiang, and Zhi-Hua Zhou. Learning from incomplete and inaccurate
supervision. In KDD, pp. 1017-1025, 2019.
Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National Science Review, 005
(001):44-53, 2018.
Xiaojin Zhu and Andrew B. Goldberg. Introduction to Semi-Supervised Learning. Synthesis Lectures
on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2009.
12
Under review as a conference paper at ICLR 2022
A Appendix
B Proof of Theorem 1
Proof.
πys Ep(x|ys)
-X 等'(g(x),y)
rys (x)
y=1
πys Z X pp(yx) '(g(x) y)p(χ∖ys)dx
ZX posy '(g(X),y)p⑶x)dx
K
p Ep(X)'(g(χ),y)p(y∖χ)dχ
y=1
K
ZX
y=1
'(g(χ), y)p(χ, y)dχ = R(g).
□
C Proof of Theorem 2
To prove the Theorem 2, we first give the definition of Rademacher complexity:
Definition 2. (Rademacher complexity) Let Zι, ∙∙∙ ,Zn be n i.i.d. random variables drawn from
a probability distribution μ and F = {f : Z → R} be a class of measurable functions. Then the
expected Rademacher complexity of function class F is given by:
1n
Rn (F)= Ezι,…,Zn 〜μEσ SUpf ∈F- £/f(Zi) ,	(14)
ni=1
where σι, ∙∙∙ ,σn are the Rademacher variables that take the value from { — 1, +1} evenly.
Based on the setting and assumptions in Section 3.2, we have the following lemma for bounding the
uniform convergence:
Lemma 3. For any 0 < δ < 1, the following inequality holds with probability at least 1 — δ:
supg∈G IRSC(g) — R(g)∣ ≤
2√2∏ysL'
K
X Rn (Gy )
y=1
(15)
where Rn(Gy) is the Rademacher complexity of Gy on i.i.d. samples with size n drawn from a single
classp(X∖ys).
Proof. We begin with proving that the one direction supg∈g RSC(g) — R(g) is bounded with
probability at least 1 — 2. Suppose an instance Xi is changed by xi, we can see that then change of
supg∈G RSC(g) — R(g) is no greater than ∏ys C'/nCr . By applying the McDiarmid,s inequality
(Mcdiarmid, 1989), with probability at least 1 — δ, the following inequality holds:
SUp Rsc(g) — R(g) ≤ Eχι,∙∙∙ ,xn SUp RSC(g) — R(g)
g∈G	g∈G
Denote by L(g(x)) = PK=I 裳 * '(g(x), y). It is easy to show that L(g(x)) is C-Lipschitz w.r.t.
g(x) due to the fact that P∖ι ry = 1 and rys ≥ Cr. Since RSC(g) is unbiased, it is routine to
13
Under review as a conference paper at ICLR 2022
show that (Mohri et al., 2012):
Ex 1 ,…,Xn
sup RSC (g) — R(g)
g∈G
≤ 2∏ys Rn (L。G)
2√2‰ l` K	…
≤	Cys ' ERn(Gy),
Cr	y=1
The last inequality holds according to the Talagrand’s contraction inequality (Maurer, 2016).
The proof of the other direction suPg∈G R(g) — RSC (g) is similar. Thus We conclude the proof. □
Then we can begin to prove the Theorem 2:
Proof.
R(gsc) — R(g*) =(R(gsc) — RSC(gSC)) + (RSC(gSC) — RSC(g* )) + (RSC(g*) — R(g"))
≤ (R(<7sc) — RSC(gSC)) + (RSC(g*) — R(g*))
≤ 2sup ∣R(g) — RSC(g)∣,
g∈G
Where the second inequality holds according to the definition of ERM. Combining this conclusion
with the previous lemma, we can conclude the proof.	□
D	Proof of Theorem 3
Proof. First of all, we give the definition of classification-calibrated loss:
Proposition 1. (Tewari & Bartlett, 2007) If the classification-calibrated losses are used, the opti-
mal classifier g* that minimizes the classification risk among all the measurable functions is also
Bayes-optimal classifier: the classifier that minimizes the classification risk w.r.t. 0-1 loss, i.e.,
misclassification rate. In other words, the prediction of the classifier on x satisfies this condition:
argmaxy∈Ygy(x) = argmaxy∈Y p(y|x)..
Denote by p(x, y) = p(y ∣x)p(x). Notice that the following holds:
KK
I Ep(X,y)dχ = / £p(y|x)p(x)dx
y=1	y=1
p(y∣x) p(x)dx
p(x)dx = 1
14
Under review as a conference paper at ICLR 2022
Then We can see that p(x, y) is the density of a Validjoint distribution on XXY. Then We have:
K
凤 g) = Ep(x∣ys) φ(X) X ry(X)'(g(x),y)
y=1
K
=p p(χ∣ys)Φ(χ) Ery(x)'(g(x),y)dx
y=1
K
=p p(x) Ery(x)'(g(x),y)dx
y=1
K
=/ EP(X,y)'(g(χ),y)dχ
y=1
=Ep(χ,y) ['(g(x, y))]
Let g* = argming∈G R(g). According to the definition of the classification-calibrated loss, we can
see that gbayes = argmin。measurable R(g) is Bayes-optimal. Since gbayes is in G, we have that
R(gbayes) ≥ R(g*). However, since R(gbayes) = mm。measurable R(g), we have that R(gbayes) ≤
R(g*). Combining the two conclusions, we know that R(g∣bay^s) = R(g*). Then we can see that
g* = argmin。measurabieR(g), which indicates that g* is also BayeS-optimal.
According to the definition of the Bayes-optimal classifier, we have that argmaxy∈γgy(x) =
argmaXy∈Yp(y∣x).	Since argmaxy∈γp(y∣x) = argmaxy∈γp(y∣x), we have that
argmaXy∈Yg*(x) = argmaXy∈γp(y∣x). Then we can see that g* is also the Bayes-optimal classifier
of R(g), which shows the classifier-consistency of R(g).	□
E Proof of Lemma 1 and Theorem 4
We begin with the proof of Lemma 1:
Proof. We prove this lemma by bounding the uniform convergence as in the proof of Theorem 2.
First we prove the following technical lemma:
Lemma 4. For any 0 < δ < 1, the following inequality holds with probability at least 1 - δ:
supg∈G ∣R(g) - R(g)l ≤
2√2L'
Cr
X Rn(Gy)+ C，
rn
y=1	r
(16)
where Rn(Gy) is the Rademacher complexity of Gy on i.i.d. samples with size n drawn from a single
classp(X|ys).
CCE 1	.1	, ,1 T	N/ ∖ N/ ∖ 一	1	1	∙,1	∙1∙,	, 1	,
Proof. We only prove that the direction supg∈g R(g) 一 R(g) is bounded with probability at least
1 一 2 since the proof of another direction is completely symmetric. Suppose an instance Xi is
^/
changed by xi, we can see that then change of suPg∈G R(g) 一 R(g) is no greater than C'/nC since
φi ≤ 1/Cr. By applying the McDiarmid’s inequality (Mcdiarmid, 1989), with probability at least
1 一 δ, the following inequality holds:
^ ^
sup R(g) - R(g) ≤ Eχι,…，Xn sup R(g) - R(g)
g∈G	Lg∈G
Denote by L(g(x)) = φ(x) PK=I * * '(g(x), y). It is easy to show that L(g(x)) is C-Lipschitz
w.r.t. g(X) due to the fact that PyK=1 ry = 1 and φ(X) ≤ 1/Cr. Then it is routine to show that
15
Under review as a conference paper at ICLR 2022
(Mohri etal., 2012):
Eχι,∙∙∙,xn sup R(g) - R(g) ≤ 2R„ (Lo G)
g∈G	一
2√2L` 3,、
≤	ERn(Gy).
r	y=i
The last inequality holds according to the Talagrand,s contraction inequality (Maurer, 2016).
The proof of the other direction is similar. Thus we conclude the proof.	口
Then we can begin to prove Lemma 1 as in the proof of Theorem 2:
R(g) - R(g*) =(R(g)- R(g)) + (R(g) - R(g*)) + (r@ ) - R(g*))
≤ (R◎- R(g)) + (R(g*)- R(g*))
≤ 2sup I R(g)- R(g)∣,
g∈G 1	1
where the second inequality holds according to the definition of ERM. Combining this conclusion
with the previous lemma, we can conclude the proof.	口
Then we prove the Theorem 4:
Proof. First we reformulate the expression of R01 (g) - R01 (g*). According to the proof of Theorem
3, g* is the Bayes-optimal classifier of R(g). Denote by f (x) = argmaXyeYgy(x) the decision
function w.r.t. g. Using the definition of Bayes-optimal classifier, we have the following equations:
R01G) - R01(g*) = Ep(χ,y) [1(f (x)) = y)] - Ep(χ,y) [1(argmaXy∈γIy = y)]
=EP(X) [Ep(y∣x) [1(f(x)) = y) - 1(argmaXy∈γg; = y)]]
=EP(X) 1 -P(f(χ)∣χ) - (1 - maχP(y∣χ))]
=EP(X) maχP(y∣χ) -p(f(χ)∣χ)
=EP(X) maχP(y∣χ) -p(f(χ)∣χ)
=EP(X) 1(f(x) = argmaXy∈γp(y∣x)) (m∣XP(y∣x) -P(f(x)∣x))].
Since infx∈x ∆(x) ≤ (maXy∈γp(y∣x) - p(f (x)∣x)) almost surely for all the X ∈ X, we have the
following inequalities:
1(f (x) = argmaXy∈γp(y∣x)) (maXy∈γp(y∣x) -p(f(x)∣x)) = 1(f (x) = argmaXy∈γp(y∣x)) * infx∈x ∆(x),
(f (x) = argmaXy∈γ p(y∣x)).
1(f (x) = argmaXy∈γp(y∣x)) (maXy∈γp(y∣x) -p(f(x)∣x)) > 1(f (x) = argmaXy∈γp(y∣x)) * infx∈x ∆(x),
(f (x) = argmaXy∈γ p(y∣x)).
Then we have the following inequality:
1(f (x) = argmaXy∈γ p(y∣x))
"x p(y∣x)- p(f(X)IX)
≥ 1(f (x) = argmaXy∈γ p(y∣x))*∕nf ∆(x)
Then we can further give the lower bound of R01(g) - R01(g*):
y∈Y
~	, ^ .	~	,	, . 一	, . , .	, . . . I	,	,,,
R01(g) - R01(g ) = EP(X) 1(f(x) = argmaXy∈γp(y∣x)) maχp(y∣x) -p(f(x)∣x)
≥ EP(X) 1(f (x) = argmaXy∈γp(y∣x)) * XnX ∆(x)
16
Under review as a conference paper at ICLR 2022
Since the factor infx∈X ∆(x) is irrelevant to x, we can further give the following conclusion:
Ep(X) [l(f(x) = argmaxy∈γp(y∣x))]
≤ infχ⅛x) W 向-").
(17)
Since g* ∈ G is also the Bayes-optimal classifier of R(g) as stated in the proof of Theorem 3, We
can learn that R01(g*) ≤ Roι(g0ι). Notice that g* is in G, then R01(g*) ≥ R01(go1). Combining
the two inequalities, we can see that R01(g*) = Roιa(g0ι). Then we can get the expression of
R01(g) - r01 (g01):
R01(g) - R01(g01) = R01(g) - R01(g )
=Ep(x,y) [1(f(x)) = y)] - Ep(x,y) [1(argmaxy∈γ为=V)∖
=Ep(X) [Ep(y∣χ) [1(f(χ)) = y) - 1(argmaχy∈γgy = y)∖∖
= Ep(x) 1 - p(f (x)|x) - 1 - max p(y|x)
= Ep(X) my∈aYx p(y|x) - p(f(x)|x)
Ep(X)
max p(y|x)- p(f(X)Ix)
Ep(X) 1(f (x) 6= argmaxy∈Y p(y|x)) maxp(y|x) - p(f (x)|x)
Since argmaXy∈γp(y∣x) = argmaXy∈γp(y∣x) and (maxy∈γp(y∣x) — p(f (x)∣x)) ≤ 1, we can
get the following inequality:
R01(g) - R01(g01) ≤ Ep(X) [1(f(x) = argmaxy∈γp(y|x))\ .
Combining the inequality above, (17), and Lemma 2, we can conclude the proof of Theorem 4. □
F Proof of Theorem 5
Proof. BSC(φ) = n Pn=I (vη(φ(xi))φ(xi) - η(φ(xi))) and BU(φ) = -ɪ Pn=I Vη(φ(Xu)).
Bno(φ) and Bn(φ) are their expectations, respectively. Then the following work is to bound the
uniform convergence. Assume that there is C^ > 0 that supφ∈φkφk∞ ≤ C^. The function η(φ(x))
is Lipschitz continuous for all kΦk∞ ≤ C^ with Lipschitz constant Ln > 0 and upper-bounded by
Cη > 0. Vη is also Lipschitz continuous with constant L0η > 0. Then we first give the technical
lemma:
Lemma 5. For any 0 < δ < 1, With probability at least 1-δ:
SUP IBSC(φ) - Bn(C (φ)∖ ≤ 2LB Rn(φ) + CB ∖ g δ .	(18)
φ∈Φ	V 2n
SUP |BU向-Bn(φ) I ≤ 2LηRuu (Φ) + LnJlIgɪ.	(19)
φ∈Φ	V 2nu
T-I /» Tk τ	, 1 , ^r∖ ,1∙? (C / 7 ∖ ∙ ɪ ∙ 1 ∙ j	, ♦	∙ .1 ɪ ∙ 1 ∙ j	, ,T	T /	, rʌ T	1
Proof. Notice that BSC (φ) IS Lipschitz continuous with Lipschitz constant LB = L01 C^ + 2Lη and
its absolute value is bounded by CB = max{Cη, LηC^}. By using the McDiarmid,s inequality,
Union bound, and the Talagrand’s contraction inequality, we can learn that with probability at least
1-δ:
SuP IBSC(φ) - BSC (3)| ≤ 2LBRn(φ) + CB ∖ g δ
φ∈Φ	2n
17
Under review as a conference paper at ICLR 2022
Notice that BU (φ) is Ln-Lipschitz continuous and is bounded by Ln. In the same way, We can see
that with probability at least 1-δ:
SUp IBU(Φ)-
φ∈Φ
BU (φ)∣ ≤ 2L0nRUu(Φ) + Ln
□
Then we can begin to prove the Theorem 5:
Bn(φkφ ) = Bn(φkφ ) - Bn(φkφ)
=Bn(φkφ ) - Bn(φkφ ) + Bn(φkφ ) - Bn(φkφ) + Bn(φkφ) - Bn(φkφ)
≤ 2 sup IBn(ΦkΦ) - Bn(ΦkΦ)I
φ∈Φ
≤ 2sup (IBSC (Φ) - BSC (Φ)ι + IBU (Φ) - BU (Φ)ι)
φ∈Φ 1	7
≤ 2sup IBSC(φ) - BSC(φ)∣ +2sup IBU(φ) - BU(Φ)∣.
ʌ	ʌ
φ∈Φ	φ∈Φ
According to the technical lemma above and the union bound, we can show that with probability at
least 1 - δ:
Bn(ΦkΦ*) ≤ 4LβRn(Φ) + 2CB
察 +4Ln Rnu (φ) + 2Ln
which concludes the proof.
□
G Proof of Theorem 6 and the Formulation of Noise-Robust
Sub-Conf Learning
Proof.
LXsn)Ep3y∈Ys) Il (PysT p(∕) %⑺,")!#
OEny) Z X (PFiyγχ⅛ 以C))p(xIy ∈Ys)dx
DZ X (⅛⅞Yi'…)P …
ZXK ( p(x
y=1 p(
,y e Ys)
∈ Ysx
'(g(χ),y) )p(<Iχ)dχ
K
/ E(P(X)'(g(χ),y))p(yIχ)dχ
y=1
K
/ ∑S'(g(x), y)p(χ)p(yIχ)dχ
y=1
R(g).
□
Then we begin to give the formulation of Noise-Robust Sub-Conf learning:
18
Under review as a conference paper at ICLR 2022
Definition 3. The risk estimator of Noise-Robust Sub-Conf learning is defined as follow:
1n K
Rsub (g) = n∑S°i £ry '(g(χi),y).	(20)
i=1	y=1
where φ(x)
=p(χ∣y∈'Ys) andφi=
p(χi∣y∈Ys) .
Notice that if we substitute p(x|y ∈ Ys) with p(x|ys), (20) is converted to (6). We can also get the
infinite and finite-sample consistency of learning with (20) by conducting such substitution.
H Additional Information of Benchmark Experiments
H. 1 Detailed Information of Benchmark Datasets
In Section 6, we used 2 widely-used large-scale benchmark datasets. Here, we report the sources of
these datasets and the way we split them.
•	Fashion-MNIST (Xiao et al., 2017). It is a 10-class dataset of fashion items. Each instance
is a 28*28 grayscale image. Source: https://github.com/zalandoresearch/
fashion-mnist.
•	CIFAR-10 (Krizhevsky et al., 2009). It is a 10-class dataset for 10 different objects and
each instance is a 32*32*3 colored image in RGB format. Source: https://www.cs.
toronto.edu/~kriz∕cifar.html.
In the experiments, we first split 30% of the dataset for generating confidences. Then we further split
10% of the dataset for density ratio estimation. Finally we use 10% of the dataset as the validation
set. The rest of the dataset are used as the training set.
H.2 Detailed Information of the Models and Optimization Algorithm
For generating confidence scores, the model used for Fashion-MNIST is a 3-layer MLP (d-100-
100-100-10) with ReLU and Resnet-18 (He et al., 2016) is used for CIFAR-10. For density ratio
estimation, the output dimension is changed to 1 and other settings are the same. For classification, a
3-layer MLP (d-500-500-500-10) with ReLU is used for Fashion-MNIST, and DenseNet-161 is used
for CIFAR-10.
Adam with default momentum was used for optimization in this paper. For generating confidence
scores, the epoch number, batch size, learning rate, and weight decay are set to 20, 100, 1e-4, and
1e-4 for Fashion-MNIST, respectively. For density ratio estimation, the epoch number is reduced
to 10 and other hyperparameters are the same. When training the classifiers, the epoch number is
increased to 100. For CIFAR-10, the learning rate and weight decay are changed to 1e-3 and other
hyperparameters are the same as those of Fashion-MNIST.
I Experimental Results on More B enchmark Datasets
In this section, we show the performance of our methods and baseline methods on more benchmark
datasets. We use two widely-used benchmark datasets and report the sources of these datasets. the
way we split them is the same as that in the previous section.
•	MNIST (LeCun et al., 1998). Itis a grayscale dataset of handwritten digits from 0 to 9, where
the size of the images is 28*28. Source: http://yann.lecun.com/exdb/mnist/.
•	Kuzushiji-MNIST (Clanuwat et al., 2018). It is a 10-class dataset of cursive Japanese
characters (’Kuzushiji’). Source: https://github.com/rois-codh/kmnist.
For generating confidence scores, the model used is a 3-layer MLP (d-100-100-100-10) with ReLU.
For density ratio estimation, the output dimension is changed to 1 and other settings are the same.
For classification, a 3-layer MLP (d-500-500-500-10) with ReLU is used.
19
Under review as a conference paper at ICLR 2022
Table 3: Mean and standard deviation of the classification accuracy over 20 trials for the MNIST dataset.
The proposed methods were compared with the baseline Weighted method and fully-supervised method, with
different classes used for training. The best and equivalent methods are shown in bold based on the 5% t-test,
excluding fully-supervised method.
Used Classes	∣		SC/Sub-Conf	NoRSC-Conf	I Weighted	I Supervised
8	I Accurate ∣	89.52±0.60	87.87±1.27	I 87.68±1.20	I
	I Noisy I	—±—	68.33±3.62	I 63.39±2.65	I
7&9	I Accurate ∣	88.46±0.92	80.08±1.86	I 84.13±2.00	I
	I Noisy I	—±—	73.48±1.43	I 71.61±1.78	J 94.82±0.53
0&2&3	I Accurate ∣	91.20±0.26	88.43±0.53	I 90.66±0.18	I
	I Noisy I	—±—	88.33±0.15	I 88.32±0.47	I
1&5&7&9	I Accurate ∣	91.28±0.21	90.89±0.16	I 89.72±0.45	I
	I Noisy I	—±—	86.50±0.72	I 83.85±1.20	I	
Adam with default momentum was used for optimization in this paper. For generating confidence
scores, the epoch number, batch size, learning rate, and weight decay are set to 20, 1000, 1e-4,
and 1e-4, respectively. For density ratio estimation, the epoch number is reduced to 10 and other
hyperparameters are the same. When training the classifiers, the epoch number is increased to 100.
The experimental results are shown in Table 3 and 4.
It can be seen from the experimental results that our SC/Sub-Conf learning method outperform other
baseline methods in most cases and the NoRSC-Conf learning method can remain effective under
extreme noise, which supports our claims in Section 6.
20
Under review as a conference paper at ICLR 2022
Table 4: Mean and standard deviation of the classification accuracy over 20 trials for the Kuzushiji-MNIST
dataset. The proposed methods were compared with the baseline Weighted method and fully-supervised method,
with different classes used for training. The best and equivalent methods are shown in bold based on the 5%
t-test, excluding fully-supervised method.
Used Classes		SC/Sub-Conf	NoRSC-Conf	Weighted	Supervised
’Tsu‘	Accurate	68.45±1.42	67.33±1.08	64.55±1.28	
	Noisy	—±—	38.14±2.12	34.99±2.27	
’Ha‘	Accurate	68.19±0.56	67.07±0.65	66.86±0.67	
	Noisy	—±—	51.67±1.26	48.21±1.63	
’Ma‘	Accurate	66.20±0.93	63.67±1.17	63.56±0.96	76.06±0.32
	Noisy	—±—	38.84±2.41	35.80±2.35	
’Su‘ & ’Na‘	Accurate	69.98±0.72	67.85±0.40	68.99±0.59	
	Noisy	—±—	62.17±1.26	61.95±1.11	
’Ki‘ & ’Ya‘ & ’Re‘	Accurate	70.23±0.59	68.64±0.62	69.77±1.58	
	Noisy	—±—	64.60±1.09	64.47±0.88	
‘Ki’ & ’Su‘ & ’Ha‘ & ’Re‘	Accurate	70.48±0.47	70.08±0.51	68.56±0.47	
	Noisy	—土—	66.86±0.74	65.42±0.81	
21