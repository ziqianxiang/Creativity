Under review as a conference paper at ICLR 2022
The Details Matter: Preventing Class Col-
lapse in Supervised Contrastive Learning
Anonymous authors
Paper under double-blind review
Ab stract
Supervised contrastive learning optimizes a loss that pushes together embeddings
of points from the same class while pulling apart embeddings of points from
different classes. Class collapse—when every point from the same class has the
same embedding—minimizes this loss but loses critical information that is not
encoded in the class labels. For instance, the “cat” label does not capture unlabeled
categories such as breeds, poses, or backgrounds (which we call “strata”). As a
result, class collapse produces embeddings that are less useful for downstream
applications such as transfer learning and achieves sub-optimal generalization error
when there are strata. We explore a simple modification to supervised contrastive
loss that prevents class collapse by uniformly pulling apart individual points from
the same class. More importantly, we introduce a theoretical framing to analyze
this loss through a view of how it embeds strata of different sizes. We show that
our loss maintains distinctions between strata in embedding space, even though it
does not explicitly use strata labels. We empirically explore several downstream
implications of this insight. Our loss produces embeddings that achieve lift on
three downstream applications by distinguishing strata: 4.4 points on coarse-to-fine
transfer learning, 2.5 points on worst-group robustness, and 1.0 points on minimal
coreset construction. Our loss also produces more accurate models, with up to 4.0
points of lift across 9 tasks.
1 Introduction
Supervised contrastive learning has emerged as a promising method for training deep models, with
strong empirical results over traditional supervised learning (Khosla et al., 2020). Recent theoretical
work has shown that under certain assumptions, class collapse—when the representation of every
point from a class collapses to the same embedding on the hypersphere, as in Figure 1—minimizes
the supervised contrastive loss LSC (Graf et al., 2021). And modern deep networks, which can
memorize arbitrary labels (Zhang et al., 2016), are powerful enough to produce class collapse.
Although class collapse minimizes LSC and produces accurate models, it loses information that is not
explicitly encoded in the class labels. For example, consider images with the label “cat.” As shown in
Figure 1, some cats may be sleeping, some may be jumping, and some may be swatting at a bug. We
call each of these semantically-unique categories of data—some of which are rarer than others, and
none of which are explicitly labeled—a stratum. Distinguishing strata is important; it empirically
can improve model performance (Hoffmann et al., 2001) and fine-grained robustness (Sohoni et al.,
2020), and it is also critical in applications such as medical imaging (Oakden-Rayner et al., 2020).
But LSC maps the sleeping, jumping, and swatting cats all to a single “cat” embedding, losing strata
information. As a result, these embeddings are less useful for common downstream applications in
the modern machine learning landscape, such as transfer learning.
In this paper, we explore a simple modification to LSC that prevents class collapse. We introduce a
theoretical framing to understand how this modification affects embedding quality by studying how it
embeds strata in embedding space. We evaluate our loss both in terms of embedding quality, which
we evaluate through three downstream applications, and end model quality.
In Section 3, we present our modification to LSC , which prevents class collapse by changing how
embeddings are pushed and pulled apart. LSC pushes together embeddings of points from the
same class and pulls apart embeddings of points from different classes. In contrast, our modified
1
Under review as a conference paper at ICLR 2022
Learning	Unlabeled Strata Function
Models
Figure 1: Classes contain critical information that is not explicitly encoded in the class labels.
Supervised contrastive learning (left) loses this information, since it maps unlabeled strata such
as sleeping cats, jumping cats, and swatting cat to a single embedding. We introduce a new loss
function Lspread that prevents class collapse and maintains strata distinctions. Lspread produces
higher-quality embeddings, which we evaluate with three downstream applications.
Transfer Learning
Robustness
Minimal
Coresets
Applications of
Embeddings
loss Lspread includes an additional class-conditional InfoNCE loss term that uniformly pulls apart
individual points from within the same class. This term encourages points from the same class
to be maximally spread apart in embedding space, which discourages class collapse (see Figure 1
middle). Surprisingly, even though Lspread does not use strata labels, it still produces embeddings
that qualitatively appear to retain more strata information than those produced by LSC (see Figure 2).
In Section 4, motivated by these empirical observations, we build off previous theoretical work (Graf
et al., 2021; Wang & Isola, 2020) to study how well Lspread preserves distinctions between strata in
the representation space. We propose a simple thought experiment considering the embeddings that
the supervised contrastive loss generates when it is trained on a fraction of the dataset. This setup
enables us to distinguish strata based on their sizes by considering how likely it is for them to be
represented in the sample (larger strata are more likely to appear in a small sample). When strata do
not appear in the sample, we view them as out-of-distribution data with embeddings characterized by
their information-theoretic properties. As a result, we can show that the supervised contrastive loss
has different effects on different strata depending on their size and distribution—and that Lspread
increases the magnitude of these differences. Colloquially, rarer and more distinct strata are farther
away from common strata, and we show that this property is important for embedding quality.
In Section 5, we empirically explore several downstream implications of these insights. We demon-
strate that Lspread produces embeddings that retain more information about strata, which enables
lift on a number of downstream applications that require strata recovery. We present three such
downstream applications to evaluate embedding quality in this paper:
•	We evaluate how well Lspread’s embeddings encode fine-grained subclasses with coarse-to-fine
transfer learning. Lspread achieves up to 4.4 points of lift across four datasets.
•	We evaluate how well embeddings produced by Lspread can recover strata in an unsupervised
setting by evaluating robustness against worst-group accuracy and noisy labels. We use our insights
about how Lspread embeds strata of different sizes to improve worst-group robustness by up to 2.5
points and to recover 75% performance when 20% of the labels are noisy.
•	We evaluate how well we can differentiate rare strata from common strata by constructing limited
subsets of the training data that can achieve the highest performance under a fixed training strategy
(the coreset problem). We construct coresets by subsampling points from common strata. Our
coresets outperform prior work by 1.0 points when coreset size is 30% of the training set.
In addition, we find that Lspread produces higher-quality models, outperforming LSC by up to 4.0
points across 9 tasks. Finally, we discuss related work in Section 6 and conclude in Section 7.
2	Background
We present our generative model for strata (Section 2.1. Then, we discuss supervised contrastive
learning—in particular the SupCon loss LSC from Khosla et al. (2020) and its optimal embedding
distribution Graf et al. (2021)—and the end model for classification (Section 2.2).
2
Under review as a conference paper at ICLR 2022
•	Plane
•	Car
B Bird
•	Cat
• Deer
0.98 Sim
w/ Center
0.76 Sim
w/ Center
Avg Cos.
Sim: 0.88
LSC(Class Collapse)
0.92 Sim
w/ Center
____Avg Cos.
Sim: 0.49
0.22 Sim i
w/ Center ∣
LsPread (No Class COllapse)
* Dog
* Frog
• Horse
• Ship
• Truck
Common Stratum
Rare Stratum
(Rare Pose)
Figure 2: Lspread produces embeddings that are qualitatively better than those produced by LSC .
We show t-SNE visualizations of embeddings for the CIFAR10 test set and report cosine similarity
metrics (average intracluster cosine similarities, and similarities between individual points and the
class cluster). Lspread produces lower intraclass cosine similarity and embeds images from rare strata
further out over the hypersphere than LSC .
2.1	Data Setup
We have a labeled input dataset D = {(xi, yi)}NN=ι, where (x, y)〜P for X ∈ X and y ∈ Y =
{1, . . . , K}. For a particular data point x, we denote its label as h(x) ∈ Y with distribution p(y|x).
We assume that data is class-balanced such that p(y = i)=六 for all i ∈ Y. The goal is to learn a
model p(y |x) on D to classify points.
Data points also belong to categories beyond their labels, called strata. Following Sohoni et al.
(2020), we denote a stratum as a latent variable z, which can take on values in Z = {1, . . . , C}. Z
can be partitioned into disjoint subsets S1 , . . . , SK such that if z ∈ Sk, then its corresponding y label
is equal to k. Let S(c) denote the deterministic label corresponding to stratum c. We model the data
generating process as follows. First, the latent stratum is sampled from distribution p(z). Then, the
data point X is sampled from the distribution Pz = p(∙∣z), and its corresponding label is y = S(z)
(see Figure 2 of Sohoni et al. (2020). We assume that each class has m strata, and that there exist at
least two strata, zι, z2, where S(zι) = S(z2) and SUPP(Z1) ∩ SUPP(Z2) = 0.
2.2	Supervised Contrastive Loss
Supervised contrastive loss pushes together pairs of points from the same class (called positives) and
pulls apart pairs of points from different classes (called negatives) to train an encoder f : X → Rd .
Following previous works, we make three assumptions on the encoder: 1) we restrict the encoder
output space to be Sd-1, the unit hypersphere; 2) we assume K ≤ d + 1, which allows Graf et al.
(2021) to recover optimal embedding geometry; and 3) we assume the encoder f is infinitely powerful,
meaning that any distribution on Sd-1 is realizable by f (x).
SupCon and Collapsed Embeddings We focus on the SupCon loss LSC from Khosla et al.
(2020). Denote σ(x,x0) = f (x)>f (x0)∕τ, where T is a temperature hyperparameter. Let B
be the set of batches of labeled data on D and P(i, B) = {p ∈ B\i : h(p) = h(i)} be the
points in B with the same label as Xi. For an anchor Xi, the SupCon loss is LSC(f,Xi,B) =
-1	∖、	ICer _ exp(σ(xi,Xp))__ ∖xτhara Pf 才 R、ferma ncaItlva naira	QnrI R∖ /	fcTTna nao^
|P(i,B)∣ 乙p∈P(i,B)	log	Pa∈B∖i exp(σ(xi,xa)) ,where P(i,B)	forms PoSitiVe PairS	and B\i	IormS neg-
ative pairs.
The optimal embedding distribution that minimizes LSC contains one embedding per class, with the
per-class embeddings collectively forming a regular simplex inscribed in the hypersphere Graf et al.
(2021). Formally, if h(X) = i, then f(X) = vi for all X ∈ B. {vi}iK=1 makes up the regular simplex,
defined by: a) PiK=1vi = 0; b) kvik2 = 1; and c) ∃cK ∈ R s.t. vi>vj = cK for i 6= j. We describe
this property as class collapse and define the distribution of f(X) that satisfies these conditions as
collapsed embeddings.
3
Under review as a conference paper at ICLR 2022
End Model After the supervise contrastive loss is used to train an encoder, a linear classifier W ∈
RK×d is trained on top of the representations f(x) by minimizing cross-entropy loss over softmax
scores. We assume that kWy k2 ≤ 1 for each y ∈ Y. The end model’s empirical loss can be defined
as L(W, D) = Pχ ∈d - log Pxpf(Xi) Wh>χi? . The model Uses Softmax scores constructed with
xi ∈D	j=1 exp(f (xi) Wj)
f(x) and W to generate predictions p(y|x) = p(y∣f (x)). Finally, the generalization error of
the model on P is the expected cross-entropy between p(y|x) and p(y|x), namely L(χ, y, f)=
Eχ,y [- log p(y∣f (x))].
3	Method
We now highlight some theoretical problems with class collapse under our generative model of strata
(Section 3.1). We then propose and qualitatively analyze a loss function Lspread (Section 3.2).
3.1	Theoretical motivation
We describe conditions when collapsed embeddings minimize generalization error on coarse-to-fine
transfer and the original task. We find that these conditions do not hold when distinct strata exist.
Consider the downstream coarse-to-fine transfer task (x, z) of using embeddings f(x) learned on
(x, y) to classify points by fine-grained strata. Formally, coarse-to-fine transfer involves learning an
end model with weight matrix W ∈ RC×d and fixed f(x) (as described in Section 2.2) on points
(x, z), where we assume the data are class-balanced across z.
Observation 1. Class collapse minimizes L(x, z, f) if for all x, 1) p(y = h(x)|x) = 1, meaning
that each X is deterministically assigned to one class, and 2) p(z∣x) = m1 where Z ∈ Sh⑺.The
second condition implies thatp(x|z) = p(x|y) for all z ∈ Sy, meaning that there is no distinction
among strata from the same class. This contradicts our generative model assumptions.
Similarly, we characterize when collapsed embeddings are optimal for the original task (x, y).
Observation 2. Class collapse minimizes L(x, y, f) if, for all x, p(y = h(x)|x) = 1. This contra-
dicts our generative model assumptions.
Proofs are in Appendix D.1. We also show in Appendix C.1 that a one-to-one encoder obeys the
Infomax principle (Linsker, 1988) better than collapsed embeddings on new distributions (x0, y0).
These observations suggest that a distribution over the embeddings that preserves strata distinctions
and does not collapse classes is more desirable.
3.2	MODIFIED CONTRASTIVE LOSS Lspread
We introduce the loss Lspread, a weighted sum of two contrastive losses Lattract and Lrepel. Lattract
is a supervised contrastive loss, while Lrepel encourages intra-class separation. For α ∈ [0, 1],
Lspread= αLattract + (1 - α)Lrepel .
(1)
For a given anchor xi, define xiaug as an augmentation of the same point as x. Define the set of
negative examples for i to be N (i, B) = {a ∈ B\i : h(a) 6= h(i)}. Then,
L ( f B) _	_1	∣	____________exp。(Xi, Xp))__________
attract , Xi，	|P(i,B)1 p∈P^B;exP(σ(xi,xp)) + Pa∈N(i,B) exP(σ(xi, Xa))
Lrepel(f,Xi,B) = - log P MP。(Xi,,U'))
p∈P (i,B) exp(σ(Xi, Xp))
(2)
(3)
Lattract is a variant of the SupCon loss, which encourages class separation in embedding space
as suggested by Graf et al. (2021). Lrepel is a class-conditional InfoNCE loss, where the positive
distribution consists of augmentations and the negative distribution consists of i.i.d samples from the
same class. It encourages points within a class to be spread apart, as suggested by the analysis of the
InfoNCE loss by Wang & Isola (2020).
4
Under review as a conference paper at ICLR 2022
Qualitative evaluation Figure 2 shows t-SNE plots for embeddings produced with LSC versus
Lspread on the CIFAR10 test set. Lspread produces embeddings that are more spread out than those
produced by LSC and avoids class collapse. As a result, images from different strata can be better
differentiated in embedding space. For example, we show two dogs, one from a common stratum and
one from a rare stratum (rare pose). The two dogs are much more distinguishable by distance in the
Lspread embedding space, which suggests that it helps preserve distinctions between strata.
4	Theoretical Analysis
In this section, we first apply current theoretical tools to Lspread to understand the optimal embedding
distributions. These tools do not fully capture strata behavior. In Section 4.1, we propose a simple
thought experiment about the distances between strata in embedding space when trained under a finite
subsample of data to explain our prior qualitative observations. Then, in Section 4.2, we analyze how
Lspread produces better representations for both coarse-to-fine transfer and the original task (x, y).
Existing Analysis Previous works have studied the geometry of optimal embeddings under con-
trastive learning (Graf et al., 2021; Wang & Isola, 2020; Robinson et al., 2020), but their techniques
cannot analyze strata. As an example, we adopt and expand the analysis from Wang & Isola (2020).
First, we set up some notation to analyze Lspread asymptotically. For an anchor x, the positive
example x+ is drawn fromp+(∙∣χ) = p(∙∣h(χ+) = h(χ)). Negative examples X- are drawn from
p-(∙∣x) = p(∙∣h(x-)). An augmentation Xaug is drawn fromPa(∙∣x).
Theorem 1. Define n+ = |P (i, B)|, n- = |N (i, B)|. As n+, n- → ∞, the population-level loss
over batches and anchors, which we denote as Lspread(f, n+, n-) (see Definition 2), converges to:
lim Lspread (f, n , n ) - (1 - α) log n - α log n = Lalign(f) + Lunif orm (f) + Lneg (f),
n+,n- →∞
where
1.	Lalign(f) = -(αEχ,χ+〜p+ [σ(x,x+)] + (1 - α)Eχ,χaug〜pa [σ(x,xaug)]) is minimized when
all points from a class collapse to one embedding.
2.	Luniform(f) = (1 一 α)Eχ〜P [log Eχ+〜p+(∙∣χ) [exp(σ(x, x+))]] is minimized when points in
each class are distributed uniformly on the hypersphere.
3.	Lneg (f) = αEχ 〜P [log Ex-〜p-(∙∣χ) [exp(σ(x, x-))]] is minimized when points from a class
collapse to one embedding and collectively form a regular simplex inscribed in the hypersphere.
The proof is in Appendix D.2. While the optimal distributions for Lunif orm and Lneg suggest
better spread both among and within classes, several issues prohibit a closer analysis of Lspread for
strata. First, the individual minima of Luniform and Lneg do not intersect, which prevents us from
characterizing Lspread’s optimal distribution. Second, even if a unique optimal distribution were
deduced in this way, it would not capture strata. This is because this approach models embeddings
not as a mapping from X, but as a distribution on the hypersphere based on information in the loss
function. However, strata are unknown at training time and thus impossible to incorporate explicitly
into the loss. Therefore, we need another explanation for our empirical observations that strata
distinctions are preserved in embedding space under Lspread .
4.1	Geometry of Strata under Supervised Contrastive Loss
We propose a simple thought experiment based on subsampling the dataset—randomly sampling
a fraction of the training data—to analyze strata. Consider the following: we subsample a fraction
t ∈ [0, 1] of a training set of N points from P. We use this subsampled dataset Dt to learn an encoder
ft, and we study the average distance under f between two strata Z and z0 from the same class as t
varies.
The average distance between Z and z is δ(ft,z,z0) = ∣∣Ex〜pz [ft(x)] — Ex〜pz0 [ft(x)]∣∣2 and
depends on whether z and z0 are both in the subsampled dataset. We have three cases (with
probabilities stated in Appendix C.2) based on strata frequency and t—when both, one, or neither of
the strata appears in Dt :
5
Under review as a conference paper at ICLR 2022
1.	Both strata appear in Dt The encoder ft is trained on both Z and z0. For large N, We can
approximate the setting by considering ft trained on infinite data from these strata. The optimal
embedding distribution from Theorem 1 is defined on these strata, so δ(ft, z, z0) can be analyzed
by examining properties of that distribution. Note that if LSC Were used, δ(ft , z, z0) converges to
0. This case occurs With probability increasing in p(z), p(z0), and t.
2.	One stratum but not the other appears in Dt Without loss of generality, suppose that points
from z appear in Dt but no points from z0 do. Since the downstream classifier p(y∣ft(χ)) is a
function of distances in embedding space, We can equivalently consider hoW the end model learned
using the “source” distribution containing z performs on the “target” distribution of stratum z0 .
Borrowing from literature in domain adaptation, the difficulty of this out-of-distribution problem
depends on both the divergence between source and target distributions and the capacity of
the overall model. For instance, the H∆H-divergence from Ben-David et al. (2010; 2007),
which is studied in lower bounds in Ben-David & Urner (2012), and the discrepancy difference
from Mansour et al. (2009) capture both concepts. Moreover, Lspread and LSC induce different
end model hypothesis classes, which can help explain why Lspread better preserves strata distances.
This case occurs with probability increasing in p(z) and decreasing in p(z0) and t.
3.	Neither strata appears in Dt The distance δ(ft, z, z0) is at most 2DTV(Pz, Pz0) (total variation
distance), regardless of how the encoder is trained. This case occurs with probability decreasing
inp(z),p(z0), and t.
We make two observations from these cases. First, if z and z0 are both common strata, then as t
increases, the distance between them depends on the optimal asymptotic distribution. Therefore, if
we set α = 1 in Lspread , these common strata will collapse. Second, if z is a common strata and z0 is
uncommon, the second case occurs frequently over randomly sampled Dt, and thus the strata are
separated based on the difficulty of the respective out-of-distribution problem. We thus arrive at the
following insight from our thought experiment:
Common strata are more tightly clustered together, while rarer and more semanti-
cally distinct strata are far away from them.
Figure 3 demonstrates this insight. Points from the largest subclasses (dark blue) cluster tightly,
whereas points from small subclasses (light blue) are scattered throughout the embedding space.
4.2 Implications
We discuss theoretical and practical implications of
our subsampling argument. First, we show that on
both the coarse-to-fine transfer task (x, z) and the
original task (x, y), embeddings that preserve strata
yield better generalization error. Second, we discuss
practical implications arising from our subsampling
argument that enable new applications.
Subclass Size
• 500 Points
• 250 Points
• 100 Points
• 50 Points
Figure 3: Points from large subclasses cluster
tightly; points from small subclasses scatter
(CIFAR100-Coarse, unbalanced subclasses).
Theoretical Implications Consider f1, the encoder
trained on D with N points using Lspread, and sup-
pose a mean classifier is used for the end model. On
coarse-to-fine transfer, generalization error depends
on how far each stratum center is from the others.
Lemma 1. The generalization error on the
coarse-to-fine transfer task is at most
L(x,z,fι) ≤ Ez [ log (Pi:S(i)=S(z) exp(-δintra (z, i)) + Pi:S(i)6=S(z) exp(-δinter (z, i)) - 1,
where δintra(z, i) and δinter(z, i) are quantities that scale with the distances between strata z and
i depending on if S(z) = S(i) (see Appendix D.3 for exact expressions). Note that δintra (z, i)
corresponds with the distances considered in our thought experiment in Section 4.1.
The larger the distances between strata, the smaller the upper bound on generalization error. A similar
result holds on the original task (x, y), but there is an additional term that penalizes points from the
same class being too far apart.
6
Under review as a conference paper at ICLR 2022
End Model Perf.
Dataset	LSS	LSC	Lspread
CIFAR10	89.7	90.9	91.5
CIFAR10-Coarse	97.7	96.5	98.1
CIFAR100	68.0	67.5	69.1
CIFAR100-Coarse	76.9	77.2	78.3
CIFAR100-Coarse-U	72.1	71.6	72.4
MNIST	99.1	99.3	99.2
MNIST-Coarse	99.1	99.4	99.4
Waterbirds	77.8	73.9	77.9
ISIC	87.8	88.7	90.0
Coarse-to-Fine Transfer
Dataset	LSS	LSC	Lspread
CIFAR10-Coarse	71.7	52.5	76.1
CIFAR100-Coarse	62.0	62.4	63.9
CIFAR100-Coarse-U	61.9	59.5	62.4
MNIST-Coarse	97.1	98.8	99.0
Figure 4: Left: End model performance training with Lspread on various datasets compared against
contrastive baselines. All metrics are accuracy except for ISIC (AUROC). Lspread produces the best
performance in 7 out of 9 cases, and matches the best performance in 1 case. Right: Performance
of coarse-to-fine transfer on various datasets compared against contrastive baselines. In these tasks,
we first train a model on coarse task labels, then freeze the representation and train a model on
fine-grained subclass labels. Lspread produces embeddings that transfer better across all datasets.
Lemma 2. The generalization error on the original task is at most L(x,y,fι) ≤
Ez [log (exp(-δintra(z)) + Pi=S(Z) exp(-δinter(Z))) + 吉方加偌⑶].λ > 0 is a constant.
δintra (z) scales with the average distances between z and other strata in its class, which we analyze
in Section 4.1. δinter(z) scales with the average distances between z and strata in other classes. See
Appendix D.3 for exact expressions.
Practical Implications Our discussion in Section 4.1 suggests that training with Lspread better
distinguishes strata in embedding space. As a result, we can use differences between strata of
different sizes for downstream applications. For example, unsupervised clustering can help recover
pseudolabels for unlabeled, rare strata. These pseudolabels can be used as inputs to worst-group
robustness algorithms, or used to detect noisy labels, which appear to be rare strata during training
(see Section 5.2 for examples). We can also train over subsampled datasets to heuristically distinguish
points that come from common strata from points that come from rare strata. We can then downsample
points from common strata to construct minimal coresets (see Section 5.3 for examples).
5	Experiments
This section evaluates Lspread on embedding quality and model quality:
•	First, in Section 5.1, we use coarse-to-fine transfer learning to evaluate how well the embeddings
maintain strata information. We find that Lspread achieves lift across four datasets.
•	In Section 5.2, we evaluate how well Lspread can detect rare strata in an unsupervised setting. We
first use Lspread to detect rare strata to improve worst-group robustness by up to 2.5 points. We
then use rare strata detection to correct noisy labels, recovering 75% performance under 20% noise.
•	In Section 5.3, we evaluate how well Lspread can distinguish points from large strata versus points
from small strata. We downsample points from large strata to construct minimal coresets on
CIFAR10, outperforming prior work by 1.0 points at 30% labeled data.
•	Finally, in Section 5.4, we show that training with Lspread improves model quality, validating our
theoretical claims that preventing class collapse can improve generalization error. We find that
Lspread improves performance in 7 out of 9 cases.
Datasets and Models Figure 4 (left) lists all our datasets. CIFAR10, CIFAR100, and MNIST are
the standard computer vision datasets. We also use coarse versions of each, wherein classes are
combined to create coarse superclasses (animals/vehicles for CIFAR10, standard superclasses for
CIFAR100, and <5, ≥5 for MNIST). In CIFAR100-Coarse-U, some subclasses have been artificially
imbalanced. Waterbirds and ISIC are real-world datasets with documented hidden strata (Sagawa
et al., 2019; Codella et al., 2019; Sohoni et al., 2020). We use a ViT model (Dosovitskiy et al., 2020)
7
Under review as a conference paper at ICLR 2022
Dataset	Sub-Group Recovery		
	LCE	LSC	Lspread
Waterbirds	56.3	47.2	59.0
ISIC	74.0	92.5	93.8
	Worst-Group Robustness		
Waterbirds	88.4	86.5	89.0
ISIC	92.0	93.3	92.6
Noisy Label Performance, CIFAR10
Coreset Performance, CIFAR10
Figure 5: Left: Unsupervised strata recovery performance (top, F1), and worst-group performance
(AUROC for ISIC, Acc for others) using recovered strata. Center: Performance of models under
various amounts of label noise for the contrastive loss head. Right: Performance of a ResNet18
trained with coresets of various sizes.
(4x4, 7 layers) for CIFAR and MNIST and a ResNet50 for the rest. For the ViT models, we jointly
optimize the contrastive loss with a cross entropy loss head. For the ResNets, we train the contrastive
loss on its own and use linear probing on the final layer. More details in Appendix E.
5.1	Coarse-to-Fine Transfer Learning
In this section, we use coarse-to-fine transfer learning to evaluate how well Lspread retains strata
information in the embedding space. We train on coarse superclass labels, freeze the weights, and
then use transfer learning to train a linear layer with subclass labels. We use this supervised strata
recovery setting to isolate how well the embeddings can recover strata in the optimal setting.
Figure 4 (right) reports the results. We find that Lspread produces better embeddings for coarse-to-fine
transfer learning than LSC and LSS. Lift over LSC varies from 0.2 points on MNIST (16.7% error
reduction), to 23.6 points of lift on CIFAR10. Lspread also produces better embeddings than LSS,
since LSS does not encode superclass labels in the embedding space.
5.2	Robustness Against Worst-Group Accuracy and Noise
In this section, we use robustness to measure how well Lspread can recover strata in an unsupervised
setting. We use clustering to detect rare strata as an input to worst-group robustness algorithms, and
we use a geometric heuristic over embeddings to correct noisy labels.
To evaluate worst-group accuracy, we follow the experimental setup and datasets from Sohoni
et al. (2020). We first train a model with class labels. We then cluster the embeddings to produce
pseudolabels for hidden strata, which we use as input for a Group-DRO algorithm to optimize
worst-group robustness (Sagawa et al., 2019). To evaluate robustness against noise, we introduce
noisy labels to the contrastive loss head on CIFAR10. We detect noisy labels with a simple geometric
heuristic: points with incorrect labels appear to be small strata, so they should be far away from other
points of the same class. We then correct noisy points by assigning the label of the nearest cluster in
the batch. More details can be found in Appendix E.
Figure 5 (left) shows the performance of unsupervised strata recovery and downstream worst-group
robustness. We can see that Lspread outperforms both LSC and LCE on strata recovery. This
translates to better worst-group robustness on the Waterbirds task, outperforming LSC by 2.5 points,
and LCE by 0.6 points.
Figure 5 (center) shows the effect of noisy labels on performance. When noisy labels are uncorrected
(purple), performance drops by up to 10 points at 50% noise. Applying our geometric heuristic (red)
can recover 4.8 points at 50% noise, even without using Lspread. But Lspread recovers an additional
0.9 points at 50% noise, and an additional 1.6 points at 20% noise (blue). In total, Lspread recovers
75% performance at 20% noise, whereas LSC only recovers 45% performance.
5.3	Minimal Coreset Construction
Now we evaluate how well training on fractional samples of the dataset with Lspread can distinguish
points from large versus small strata by constructing minimal coresets for CIFAR10. We train a
8
Under review as a conference paper at ICLR 2022
ResNet18 on CIFAR10, following Toneva et al. (2019), and compare against baselines from Toneva
et al. (2019) and Paul et al. (2021). For our coresets, we train with Lspread on subsamples of the
dataset and record how often points are correctly classified at the end of each run. We bucket points in
the training set by how often the point is correctly classified. We then iteratively remove points from
the largest bucket in each class. Our strategy removes easy examples first from the largest coresets,
but maintains a set of easy examples in the smallest coresets.
Figure 5 (right) shows the results at various coreset sizes. For large coresets, our algorithm outper-
forms both methods from Paul et al. (2021) and is competitive with Toneva et al. (2019). For small
coresets, our method outperforms the baselines, providing up to 5.2 points of lift over Toneva et al.
(2019) at 30% labeled data. Our analysis helps explain this gap; removing too many easy examples
hurts performance, since then the easy examples become rare and hard to classify.
5.4	Model Quality
Finally, we confirm that Lspread produces higher-quality models and achieves better sample com-
plexity than both LSC and the SimCLR loss LSS. Figure 4 (left) reports the performance of models
across all our datasets. We find that Lspread achieves better overall performance compared to models
trained with LSC and LSS in 7 out of 9 tasks, and matches performance in 1 task. We find up to
4.0 points of lift over LSC (Waterbirds), and up to 2.2 points of lift (AUROC) over LSS (ISIC). In
Appendix F, we additionally evaluate the sample complexity of contrastive losses by training on
partial subsamples of CIFAR10. Lspread outperforms LSC and LSS throughout.
6	Related Work and Discussion
From work in contrastive learning, we most directly extend Wang & Isola (2020) and Graf et al.
(2021), who study representations on the hypersphere along with Robinson et al. (2020). We take
inspiration from Arora et al. (2019), who use a latent classes view to study self-supervised contrastive
learning. Similarly, Zimmermann et al. (2021) considers how minimizing the InfoNCE loss recovers
a latent data generating model. We initially started from a debiasing angle to study the effects of
noise in supervised contrastive learning inspired by Chuang et al. (2020), but moved to our current
strata-based view of noise instead. Recent work has also analyzed contrastive learning from the
information-theoretic perspective (Oord et al., 2018; Tian et al., 2020; Tsai et al., 2020), but does not
fully explain practical behavior (Tschannen et al., 2020), so we focus on the geometric perspective in
this paper because of the downstream applications. Our work builds on the recent wave of empirical
interest in contrastive learning (Chen et al., 2020a; He et al., 2019; Chen et al., 2020b; Goyal et al.,
2021; Caron et al., 2020) and supervised contrastive learning (Khosla et al., 2020).
Our treatment of strata is strongly inspired by Sohoni et al. (2020) and Oakden-Rayner et al. (2020),
who document empirical consequences of hidden strata. We are inspired by empirical work that has
demonstrated that detecting subclasses can be important for performance (Hoffmann et al., 2001;
d’Eon et al., 2021) and robustness (Duchi et al., 2020; Sagawa et al., 2019; Goel et al., 2020).
Each of our downstream applications is a field in itself, and we take inspiration from recent work
from each. Our noise heuristic is similar to the ELR (Liu et al., 2020) and takes inspiration from a
various work using contrastive learning to correct noisy labels and for semi-supervised learning (Li
et al., 2021; Ciortan et al., 2021; Li et al., 2020). Our coreset algorithm is inspired by recent work in
coresets for modern deep networks (Ju et al., 2021; Sener & Savarese, 2018; Paul et al., 2021), and
takes inspiration from Toneva et al. (2019) in particular.
7	Conclusion
We propose a new supervised contrastive loss function to prevent class collisions and produce higher-
quality embeddings. We show that our loss function maintains strata distinctions in embedding space
and explore several downstream applications. Future directions include encoding hierarchies in the
contrastive loss functions and extending our work to more modalities, models, and applications. We
hope that our work inspires further work in more fine-grained supervised contrastive loss functions
and new theoretical approaches for reasoning about generalization and strata.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement For theoretical results, the main assumptions are listed in Section 2. A
glossary of terms is provided in Appendix A, definitions are provided in Appendix B, and proofs are
provided in Appendix D. Full experimental details about datasets and models are given in Appendix E.
Code will be publicly released before publication.
Ethics Statement We hope that our work encourages the community to consider strata as a tool to
analyze and evaluate the generalization of machine learning methods from a different perspective.
We hope that our methods and analysis inspire future work in using contrastive learning to improve
the robustness of machine learning models, especially when understanding the actions of different
approaches on strata is important for safety or fairness.
References
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint
arXiv:1902.09229, 2019.
Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled
target samples. In Nader H. Bshouty, Gilles Stoltz, Nicolas Vayatis, and Thomas Zeugmann (eds.),
Algorithmic Learning Theory, pp. 139-153, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg.
ISBN 978-3-642-34106-9.
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1):151-175, 2010.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural
Information Processing Systems, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning.
PMLR, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Ching-Yao Chuang, Joshua Robinson, Antonio Torralba, and Stefanie Jegelka. Debiased contrastive
learning. volume 33, 2020.
Madalina Ciortan, Romain Dupuis, and Thomas Peel. A framework using contrastive learning for
classification with noisy labels, 2021.
Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David
Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion
analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging
collaboration (isic). arXiv preprint arXiv:1902.03368, 2019.
Greg d’Eon, Jason d’Eon, James R Wright, and Kevin Leyton-Brown. The spotlight: A general
method for discovering systematic errors in deep learning models. arXiv preprint arXiv:2107.00758,
2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference
on Learning Representations, 2020.
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses for latent
covariate mixtures. arXiv preprint arXiv:2007.13982, 2020.
10
Under review as a conference paper at ICLR 2022
Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup
performance gap with data augmentation. In International Conference on Learning Representations,
2020.
Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat
Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual
features in the wild. arXiv preprint arXiv:2103.01988, 2021.
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised con-
Strastive learning. In International Conference on Machine Learning, pp. 3821-3830. PMLR,
2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
Achim Hoffmann, Rex Kwok, and Paul Compton. Using subclasses to improve classification learning.
In European Conference on Machine Learning, pp. 203-213. Springer, 2001.
Jeongwoo Ju, Heechul Jung, Yoonju Oh, and Junmo Kim. Extending contrastive learning to unsuper-
vised coreset selection. arXiv preprint arXiv:2103.03574, 2021.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Mschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. volume 33, 2020.
Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In International Conference on Learning Representations, 2020.
Junnan Li, Caiming Xiong, and Steven C.H. Hoi. Semi-supervised learning with contrastive graph
regularization. In ICCV, 2021.
R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105-117, 1988. doi:
10.1109/2.36.
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
Systems, 33, 2020.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Leland McInnes, John Healy, Nathaniel Saul, and LUkas GroBberger. Umap: Uniform manifold
approximation and projection. Journal of Open Source Software, 3(29), 2018.
Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification
causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of
the ACM conference on health, inference, and learning, pp. 151-159, 2020.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding
important examples early in training. arXiv preprint arXiv:2107.07075, 2021.
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with
hard negative samples. arXiv preprint arXiv:2010.04592, 2020.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
In International Conference on Learning Representations, 2019.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In International Conference on Learning Representations, 2018.
11
Under review as a conference paper at ICLR 2022
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left
behind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural
Information Processing Systems, 33, 2020.
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What
makes for good views for contrastive learning? arXiv preprint arXiv:2005.10243, 2020.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and
Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning.
In International Conference on Learning Representations, 2019.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised
learning from a multi-view perspective. In International Conference on Learning Representations,
2020.
Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. In International Conference on Learning
Representations, 2020.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929-9939. PMLR, 2020.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD
Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations, 2016.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep
features for scene recognition using places database. Advances in Neural Information Processing
Systems, 27:487-495, 2014.
Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. arXiv preprint arXiv:2012.08850, 2021.
12
Under review as a conference paper at ICLR 2022
We provide a glossary in Appendix A. Then we provide definitions of terms in Appendix B. We
discuss additional theoretical results in Appendix C. We provide proofs in Appendix D. We discuss
additional experimental details in Appendix E. Finally, we provide additional experimental results in
Appendix F.
A	Glossary
The glossary is given in Table 1 below.
Symbol	Used for
LSC Lspread x y D h(x)	SupCon (see Section 2.2), a supervised contrastive loss introduced by Khosla et al. (2020). Our modified loss function. It is defined empirically in Section 3.2 and theoretically in 2. Input data x ∈ X . Class label y ∈ Y = {1, . . . , K}. Dataset of N points {(xi, yi)}iN=1 drawn i.i.d. from P. The class that x belongs to, i.e. h(x) is a label drawm from p(y|x). This label information is used as input in the supervised contrastive loss.
p^(y∣χ) z Sk S(c) Pz m d f	The end model’s predicted distribution over y given x. A stratum is a latent variable z ∈ Z = {1, . . . , C} that further categorizes data beyond labels. The set of all strata corresponding to label k (deterministic). The label corresponding to strata c (deterministic). The distribution of input data belonging to stratum z, i.e. X 〜p(∙∣z). The number of strata per class. Dimension of the embedding space. The encoder f : X → Rd maps input data to an embedding space and is learned by
Sd-1 τ σ(x, x0) B P(i,B) {vi}iK=1 W	minimizing the contrastive loss function. The unit hypersphere, formally {v ∈ Rd : kvk2 = 1}. Temperature hyperparameter in contrastive loss function. Notation for f(x)>f(x ). τ Set of batches of labeled data on D. Points in B with the same label as Xi, formally {p ∈ B\i : h(p) = h(i)}. A regular simplex inscribed in the hypersphere (see Definition 1). The weight matrix that parametrizes the downstream linear classifier
L(W, D) L(x,y,f)	(end model) learned on f (X). The empirical cross entropy loss used to learn W over dataset D (see (7)). The generalization error of the end model of predicting output y on X using encoder f (see (8) and (9)).
Lattract Lrepel	A variant on SupCon that is used in Lspread that pushes points of a class together (see (2)). A class-conditional InfoNCE loss that is used in Lspread to pull apart points within a class (see (3)).
α xaug N(i,B) p+ (∙∣χ) P-"X) Pa(∙∣x) t Dt ^ ft八 δ(ft,z,z0)	Hyperparameter α ∈ [0, 1] controls how to balance Lattract and Lrepel. An augmentation of data point X. Points in B with a label different from that of Xi, formally {a ∈ B\i : h(a) 6= h(i)}. The distribution for positive example x+, p(∙∣h(x+) = h(x)) given anchor x. The distribution for negative example x-, p(∙∣h(x-) = h(x)) given anchor x. The distribution for augmented point Xaug given anchor X. Fraction of training data t ∈ [0, 1] that is varied in our thought experiment. Randomly sampled dataset from P with size equal to t ∙ N fraction of D. Encoder trained on sampled dataset Dt . 0 The distance between centers of strata z and z0 under encoder ft, _ ,	^	,.		 -八-	-八，、r.. namely δ(ft,z,z ) = IIEx〜Pz[ft(x)] - Ex〜pz, [ft(x)]∣∣2∙	 Table 1: Glossary of variables and symbols used in this paper.
B Definitions
We restate definitions used in our proofs.
13
Under review as a conference paper at ICLR 2022
Definition 1 (Regular Simplex). The points {vi}iK=1 form a regular simplex inscribed in the hyper-
sphere if
1.	PiK=1 vi = 0
2.	kvi k = 1 for all i
3.	∃cK ≤ 1 s.t. vi>vj = cK for i 6= j
Definition 2 (Lspread). We present the population-level version of Lspread defined in Section 3.2,
Lspread(f,n+,n-). Recall that σ(x,x0) = f (x)>f (x0)∕τ.
Lspread(f, n , n ) = αLattract (f, n ) + (1 - α)Lrepel (f, n ),	(4)
where
Lattract(f,n ) = Ex〜P,	- log
x+ 〜p+ (∙∣ x),
{x-}n-ι 〜p-(∙ιx)
exp(σ(x, x+ ))
exp(σ(x, x+)) + Pin=-1 exp(σ(x, xi-))
+	exp(σ(x, xaug))
LrepeI (f, n+ ) = Ex 〜P,	- log ----------------------n+--------------
xaug〜pa(∙∣x), L	exρ(σ(χ, Xaug)) + Ei=I exp(σ(x, x+))
{x+}n+ι~p+(∙ιx)
(5)
(6)
Definition 3 (Downstream model). Once an encoder f(x) is learned, the downstream model consists
of a linear classifier trained using the cross-entropy loss:
L(W, D) = X - log eχp(f (Xi)> Wh(Xi))
Xi∈D	PK=1 exp(f(Xi)>Wj)
(7)
Define W := argminkW k2 ≤1 L(W, D). Then, the end model’s outputs are the probabilities
p^(y∣χ) = ρ(y∣f (χ))
eχp(f(χ)>Wy)
PK=1, exp(f(x)>Wj)
(8)
and the generalization error is
L(χ,y,f) = Ex,y [-logp^(y∣f(χ))].
(9)
C Additional Theoretical Results
C.1 TRANSFER LEARNING ON (X0, y0)
We now show an additional transfer learning result on new tasks (X0, y0). Formally, recall that we learn
the encoder f on (x, y) 〜P. We wish to use it on a new task with target distribution (x0, y0) 〜P0.
We find that an injective encoder f(X) is more appropriate to be used on new distributions than
collapsed embeddings based on the Infomax principle (Linsker, 1988).
Observation 3. Define fc(y) as the mapping to collapsed embeddings and f1-1(X) as an injective
mapping, both learned on P. Construct a new variable y with joint distribution (x0, e) 〜p(y∣x) ∙
p0(X0) and suppose that ye ⊥ y0|X0. Then, by the data processing inequality, it holds that I (ye, y0) ≤
I(x0, y0) where I(∙, ∙) is the mutual information between two random variables. We apply fc to y and
f1-1 to X0to get that
I(fc(ye),y0) ≤I(f1-1(X0),y0)
Therefore, f1-1 obeys the Infomax principle (Linsker, 1988) better on P0 than fc. Via Fano’s
inequality, this statement implies that the Bayes risk for learning y0 from X0is lower using f1-1 than
fc.
14
Under review as a conference paper at ICLR 2022
C.2 PROBABILITIES OF STRATA z, z0 APPEARING IN SUBSAMPLED DATASET
As discussed in Section 4.1, the distance between strata z and z0 in embedding space depends on if
these strata appear in the subsampled dataset Dt that the encoder was trained on. We define the exact
probabilities of the three cases presented. Let Pr(z, z0 ∈ Dt) be the probability that both strata are
seen, Pr(z ∈ Dt, z0 ∈/ Dt) be the probability that only z is seen, and Pr(z, z0 ∈/ Dt be the probability
that neither are seen.
First, the probability of neither strata appearing in Dt is easy to compute. In particular, we have that
Pr(z, z0 ∈/ Dt) = (1 - p(z) - p(z0))tN.
Second, the probability of z being in Dt and z0 not being in Dt can be expressed as Pr(z ∈ Dt|z0 ∈/
Dt) ∙ Pr(z0 ∈ Dt). Pr(z0 ∈ Dt) is equal to (1 - P(ZDYN, and Pr(Z ∈ Djz0 ∈ Dt) = 1 - Pr(Z ∈
Dt∣z0 ∈ Dt) = 1 - (1 - p(z∣z ∈ Z∖z0))tN. Finally, note that p(z∣z ∈ Z∖z0) = ι-P()0). Putting
this together, we get that Pr(Z ∈ Dt, Z0 ∈/ Dt) = (1 - p(Z0))tN - (1 - p(Z0) - p(Z))tN, and we can
similarly construct Pr(Z0 ∈ Dt, Z ∈/ Dt).
Lastly, the probability of both Z and Z0 being in Dt is thus Pr(Z, Z0 ∈ Dt) = 1 - Pr(Z, Z0 ∈/ Dt) -
Pr(Z0 ∈Dt,Z ∈/ Dt)-Pr(Z ∈Dt,Z0 ∈/ Dt) = 1+(1-p(Z0)-p(Z))tN -(1-p(Z0))tN -(1-p(Z))tN.
C.3 Performance of collapsed embeddings on coarse-to-fine transfer and
ORIGINAL TASK
Lemma 3. Denote fc to be the encoder that collapses embeddings. Then, the generalization error
on the coarse-to-fine transfer task using fc and a linear classifier learned using cross entropy loss is
at least
L(x, Z, fc) ≥ log(m exp(1) + (C - m) exp(cK) - 1
where cK is the dot product of any two different class-collapsed embeddings. The generalization
error on the original task under the same setup is at least
L(x, y, fc) ≥ log(exp(1) + (K - 1) exp(cK)) - 1
Proof. We first bound generalization error on the coarse-to-fine transfer task. For collapsed em-
beddings, f(x) = vi when h(x) = i, where h(x) is information available at training time that
follows the distribution p(y|x). We thus denote the embedding f(x) as vh(x). Therefore, we write
the generalization error with an expectation over h(x) and factorize the expectation according to our
generative model.
Eχ,z,h(x) [- log P(z∣f (x))]
CK
-X X
z=1 h(x)=1
p(x, z, h(x)) logp(z∣h(x))dx
CK
-X X
z=1 h(x)=1
p(z)p(x∣z)p(h(x)∣x) logp(z∣h(x))dx
-X XX Zp(z)p(χ∣z)p(h(χ)∣χ)log pCxpf>(χ)WzW dx
z=1 h(x)=1	i=1 exp(fh(x) i)
C
Ep(Z)Ex 〜Pz
z=1
KC
Xp(y∣x)( - v>Wz + logXexp(v>Wi))
y=1	i=1
Furthermore, since the W learned over collapsed embeddings satisfies Wz = vy for S(Z) = y,
we have that log PiC=1 exp(vy>Wi) = m exp(1) + (C - m) exp(cK) for any y, and our expected
15
Under review as a conference paper at ICLR 2022
generalization error is
C
Ep(Z)Ex〜Pz [-p(y = S(z)∣x) - Pky = S(z)∣x)δ + log(mexp⑴ + (C - m) exp(cκ))]
z=1
C
=log(mexp(1) + (C - m)exp(cκ)) - CK - (1 - CK) EP(Z)Ex〜Pz [p(y = S(z)∣x)]
z=1
This tells us that the generalization error is at most log(m exp(1) + (C - m) exp(CK)) - CK and at
least log(m exp(1) + (C - m) exp(CK)) - 1.
For the original task, we can apply this same approach to the case where m = 1, C = K to get that
the average generalization error is
Eh(x) ∣L(x,y, fι)] =log(exp(1) + (K - 1)exp(cK))
C
-CK -(I-CK) EP(Z)Ex〜Pz [p(y = S(Z)|x)]
z=1
This is at least log(exp(1) + (K - 1) exp(CK)) - 1 and at most log(exp(1) + (K - 1) exp(CK)) -
CK.	□
D Proofs
D.1 Proofs for Section 3.1
First, we characterize the optimal linear classifier (for both the coarse-to-fine transfer task and the
original task) learned on the collapsed embeddings.
Lemma 4 (Downstream linear classifier for coarse-to-fine task). Suppose the dataset Dz is class-
balanced across Z, and the embeddings satisfy f (x) = vi if h(x) = i where {vi}iK=1 form the regular
simplex. Then the optimal weight matrix W? ∈ RC×d that minimizes L(W, Dz) satisfies W? = Vy
for y = S(Z).
Proof. Formally, the optimization problem we are solving is
minimize - X X log	[p^Wz)	(10)
y=1 z⅛,	PC=i exp(v>Wj)
s.t. kWzk22 ≤ 1∀Z ∈ Z	(11)
The Lagrangian of this optimization problem is
K	KC	C
X X -vy>Wz +mXlog Xexp(vy>Wj) +Xλi(kWik22-1)
y=1 z∈Sy	y=1	j=1	i=1
and the stationarity condition w.r.t. Wz is
K
-vS(z) + m
y=1
Vy exp(v>Wz)
PC=1 exp(v> Wj)
+ 2λz Wz = 0
(12)
>
Substituting Wz = VS(z), we get -VS(z) + m PK=I PC=eXpVyv>S(；)：)+ 2λzVS(Z) = 0. Using the
factthat v>Vj = δ for all i = j, this equals -vs(z) +m∙ VS(ZmeχpXeχp-m⅛δ)) Vy +2λzvs(z) = 0∙
16
Under review as a conference paper at ICLR 2022
We next use the fact that PK=I Vi = 0 to get that λz = 2 (1 - m ∙ m eχPX⅞1)--⅞δeχp(δ) ) ≥ 0,
satisfying the dual constraint. We can further verify complementary slackness and primal feasibility,
since kWz?k22 = 1, to confirm that an optimal weight matrix satisfies Wz? = vy for y = S(z).
□
Corollary 1. When we apply the above proof to the case when m = 1, we recover that the optimal
weight matrix W? ∈ RK×d that minimizes L(W, D) for the original task on (x, y)〜P satisfies
Wy? = vy for all y ∈ Y.
We now prove Observation 1 and 2. Then, we present an additional result on transfer learning on
collapsed embeddings to general tasks of the form (x0, y0)〜P0.
Observation 1. Class collapse minimizes L(x, z, f) iffor all x, 1) p(y = h(x)|x) = 1, meaning
that each X is deterministically assigned to one class, and 2) p(z∣x) = * where Z ∈ Sh(x). The
second condition implies thatp(x|z) = p(x|y) for all z ∈ Sy, meaning that there is no distinction
among strata from the same class. This contradicts our generative model assumptions.
Proof. We write out the generalization error for the downstream task, L(x, z, f)
Eχ,z [- logp(z∣x)] using our conditions thatp(y = h(x)∣x) = 1 andp(z∣x) = m.
C
L(x,z,f) = — I p(x) Ep(Z∣x)lοgp(z∣f (x))dx
z=1
-Zp(x) Xp(z∣x)lοg	；p(f (X)TWz)	dx
J	Z=1	PC=1 exp(f (x)>Wi)
G /	/ ∖ 1 V 1	exp(f(x)>Wz)
-y=i ∙L(x)=yp3 'm z∈Sy θg pC=ι exp(f (x)>Wi)
To minimize this, f(X) should be the same across all X where h(X) is the same value, since p(Z|X)
does not change across fixed h(X) and thus varying f(X) will not further decrease the value of this
expression. Therefore, we rewrite f(X) as fh(x). Using the fact that y is class balanced, our loss is
now
L(x,y,z) = - - XX /	P(X)Iog
m y=1 z∈Sy x:h(x)=y
exp(fh>(x)Wz)
PZI exp(fh(χ)Wz)
dX
-ɪ X X log	exp(f>Wz)
C y=1 z∈Sy	Pi=1 exp(f>Wi)
We claim that fy = vy and Wz = vy for all S(Z) = y minimizes this expression. The corresponding
Lagrangian is
K	KC	K	C
X X -fy>Wz+mXlog Xexp(fy>Wi) +Xνy(kfyk22-1)+Xλi(kWik22-1)
y=1 z∈Sy	y=1	i=1	y=1	i=1
The stationarity condition with respect to Wz is the same as (12), and we have already demonstrated
that the feasibility constraints and complementary slackness are satisfied on W. The stationarity
condition with respect to fy is
-EWz + m ∙
z∈Sy
PC=IWexpfyTW)
PC=1 exp(f>Wi)
+ 2λyfy = 0
Substituting in Wi = vs(i) and fy = vy, we get- Pz∈Sy vy+m∙ PPCvSexIexp(：>：；；" +2λyvy = 0∙
Using the definition of the regular simplex, this simplifies to -mvy + m mm：x；xp)+) Cmmy)蓝(；)+
17
Under review as a conference paper at ICLR 2022
2λyvy = 0. We thus have that λy
m (1_____m(eXP(I)-exp(6» 、CInrl 巾Q fpαcihi1itv rnnctraintc
^2 (1 — m exp(1) + (C-m)exp(δ)b and the feasibility constraints
are satisfied. Therefore, fy = Wz = vy for y = S(z) minimizes the generalization error L(x, z, f)
when p(h(x)∣x) = 1 and p(z∣x) = mm.
Because p(z∣x) = mm and p(y = h(x)∣x) = 1, this means that p(z) = JX.h(χ)=s(z) p(z,x)dx =
m Rx：h(x)=S(Z)P(X) = mK = C. p(z) being class balanced means that P(XIz) = P(Zpxzp(X)=
Kp(X) = p(ypxyp(X) = ρ(χ∣y). Therefore, this condition suggests that there is no distinction among
the strata within a class.
□
Observation 2. Class collapse minimizes L(X, y, f) if, for all X, P(y = h(X)|X) = 1. This contra-
dicts our generative model assumptions.
Proof. This observation follows directly from Observation 1 by repeating the proof approach with
z = y, m = 1.
Lastly, suppose it is not true that P(y = h(X)|X) = 1. Then, the generalization error on the original
task is L(x,y,f) = _ RX PK=I p(x)p(y∣x) logp(y∣f (x)), which is minimized when p(y∣f (x))=
p(y∣χ). Intuitively, a model constructed with label information, p(y∣h(χ)), will not improve over one
that uses X itself to approximate P(y|X).
□
D.2 Proof of Theorem 1
We first demonstrate that the limit of Lspread (f, n+, n-) under Definition 2 has the decomposition
presented in our theorem. In Lattract, we divide the numerator and denominator by n-, and in Lrepel
we divide the numerator and denominator by n+ :
Lattract (f,n-) = E - log-------exp9(x，x+------------- +log n
_	n-eχP(σ(χ,x+)) + n-Pn=Iexp(σ(χ,χi ))_
Lrepel (f, n ) = E
exp(σ(X, Xaug))
—log---------------------------+--------------
n+ eχp(σ(χ,χaug)) + n+ En=I eχp(σ(χ, x+))
+ log n+
We can write Lspread (f, n+, n-) as
Lspread(f, n+, n-)-α log n- - (1 - α) log n+ = -αE σ(X,X+) - (1 - α)E [σ(X, Xaug)]
+ αE
1	+	1 n
log ( ^- exp(σ(x, x+)) +------ Σ exp(σ(X, Xi-))
n	n i=1
+ (1 - α)E
1	1 n+
log — exp(σ(x,xaug)) + — EeXp(σ(x,x+))
n+	n+
i=1
Taking the limit n+ , n- → ∞ yields
lim	Lspread(f, n+, n-) - αlogn- - (1 - α) log n+
n+ ,n- →∞
= -αE σ(X, X+) - (1 - α)E [σ(X, Xaug)]
+ αEX log EX- exp(σ(X,X-)) + (1 - α)EX log EX+ exp(σ(X, X+))
= Lalign(f) + Lneg (f) + Lunif orm (f)
Next, we analyze these individual loss components. Following Wang & Isola (2020)’s notation, define
Uμ(u) = J exp(u>v∕τ)dμ(v).
18
Under review as a conference paper at ICLR 2022
Lalign(f) This loss component is Lalign(f) = - αE [σ(x, x+)] + (1 - α)E [σ(x, xaug)] . This
is minimized by making each σ(x, x+) and σ(x, xaug) as large as possible, e.g. f (x) = f (x+) =
f (xaug ) for each anchor x.
Lunif orm(f) This loss component is Luniform(f) = (1 - α)Ex [log Ex+ [exp(σ(x, x+))]]. Con-
ditioning on the label of x and using the definition of x+, we can write this as
Lunif orm (f) = (1 - α)Ey Ex|h(x)=y log Ex+ |h(x+)=y exp(σ(x, x ))
(1 - α)	p(y = i)	p(x|h(x) = i
i∈Y
p(x+ |h(x+) = i) exp(σ(x, x+))dx+ dx
To minimize this, We look at a relaxation of optimizing over a set of K measures, {μi}K=ι where
each μi represents the class conditional distribution with density p(x∣h(x) = i) (since the encoder is
assumed to be infinitely powerful). Then, to minimize Lunif orm (f), we want to solve
minimizes稔}K ɪ y^p(y = i) / ( log / exp(σ(x, x+))dμy(x+) )dμi(x)
= i∈Y
minimize{μi}K=1 £p(i) / log Uμτ(u)dμi(u)
i∈Y
Since there are no pairwise terms where measures of different classes interact, the minimum of the
above expression is obtained with μ? = argmin*a / log Uyii(u)dμi(u) for all i ∈ Y. This is the
exact form of the expression that Wang & Isola (2020) analyzes for Luniform , where they use a
measure μ on the entire dataset rather than a class-conditional μ%. Therefore, using their analysis
approach, we can directly conclude that μ? = σd-ι, the normalized surface area measure on Sd-1.
Therefore, this component is minimized when points of each class are distributed uniformly on the
hypersphere.
Lneg (f) This loss component is Lneg (f) = αEx [log Ex- [exp(σ(x, x-))]]. Conditioning on the
label of x and using the definition of x- , we can write this as
Lneg (f ) = αEy Ex|h(x)=y log Ex- |h(x- )6=y exp(σ(x, x ))
=αX p(y=i) Z p(xly=i)(log ZP …D ……>
We consider a relaxation of this problem into K one-versus-all problems. That is, we compute
optimal pairs of measures corresponding to P(x|y = i), P(x|y 6= i) for each i. Let y’ be an indicator
variable for if y = i or not. For notation, define ρ(0) = P(y0 = 0) and ρ(1) = P(y0 = 1). Our
problem is now equivalent to analyzing the binary setting with the following objective function to
minimize:
ρ(0) P(x|y0 = 0) log P(x-|y0 = 1) exp(σ(x, x-))dx- dx
+ρ ⑴ ∕p3y0 = 1) (log ∕p(X-IyO = 0) exp(σ(x, x-))dχ-)dx
Since the encoder is assumed to be infinitely powerful, we can consider optimizing over the class-
conditional measures μo and μι in M(Sd-1), the set of Borel probability measures on Sd-1. The
optimization problem is now
minimize*。,*]
ρ(0)
/ 卜og/exp(σ(x,x-))dμ1(x))dμ0(x)
+
/ (log/ exp(σ(x, x-))dμo(x)
dμ1(x)
19
Under review as a conference paper at ICLR 2022
The expression we want to minimize is thus
minimizeμi,μ-i P(O)
/ log Uμι (u)dμo(u) + ρ(1)
/ log Uμo (u)dμι(u)
(13)
Following the approach of Wang & Isola (2020), We analyze the distributions μ?, μ? that minimize
this expression in three steps. First, we show that the minimum of (13) exists, i.e. the infimum is
attained for some two measures. Second, we show that Uμ? is constant μj-almost surely, and vice
versa. Lastly, we use this to show that the collapsed embeddings distribution minimizes (13).
1.	Minimizers of (13) exist.
Let m be a sequence such that
Jim p(O) / log Uμm (U)dμm(U) + P(I) / log Uμm (U)dμm(U)
="inμ IP(O)∕log Uμι(U)d〃0(U)+P ⑴]log Uμo (U)dμι(U
Using Helly,s Selection Theorem twice, there exists a subsequence n such that {(μn,μ?)}n
converges to a weak cluster poinnt (μ0, μ?). Because {log U*g}n is uniformly bounded and
continuously convergent to log Uμ? and same for μ? and μ?, it holds that
p(O) / log U*?(U)dM?(〃)+ p(1) / log Uμ? (U)dμ?(U)
=nlim∞ p(O) / log Uμn (U)dμn(U) + p(1) / log Uμn (U)dμn(U))
and therefore μ?, μ? achieve the infimum of (13).
2.	Uμ? is constant μ0-almost surely and U*? is constant μ?-almost surely, for any minimizer
(μ0,μ?) of (13).	°
Formally, define (μ?, μ?) to be a solution of (13), i.e.
〃?,〃？ ∈ argminμ0,μι P(O) /log Uμι (U)d〃0(U)+ P(I) /lθg U“°(U)d〃1(U)
Define the Borel sets where μ? has positive measure to be T = {T ∈ M(SdT) : μ?(T) > o}.
Define the conditional distribution of μ? on T for some T ∈ T as μ?,T, where μ?,T (A)=
μ1(A∩τ)	,	,
μ? (T).
Now we consider a mixture (1 - α)μ? + αμ0,T. The first variation of μ? states that
0=∂α IP(O)
/log U"? (U)d((I- α)μ? + αμ?,T )(u) + P(I) /log U(1-a)“? + a”?,T dμ?(U)
α=0
P(O) /logU"?(U)d(μ?,T - μ?)(u) + P(V)I
—
dμj(u),
Where we,ve used the fact that ∂dα。(一的“?+.“?,T (U)Ia=O = ∂dα R exp(u>v∕τ)d((1 - α)μ? +
ɑμ?T)(v)∣a=o = Uμ? T(U) - Uμ?(u). Therefore, due to symmetry the optimality conditions
using the first variation are
ρ(0)
ρ(1)
/log Uμ? (U)d(μ0,τ - 〃?)(〃)+ P(DZ UUfTU)
Z log U"?(U)d(μ1,τ - μ?)(U) + P(O) Z j1；U))
dμ?(U) = ρ(1)
dμ0(u) = P(O)
(14)
(15)
Now, let {T0n}n∞=1 be a sequence of sets in T0 such that
n→→m / Uμ? (U)dμ?,Tn (U)= /up / Uμ? (U)dM?,T0 (u) = U?,0
20
Under review as a conference paper at ICLR 2022
and similarly let {T1n}n∞=1 be a sequence of sets in T1 such that
nlim∞ / Uμ? (U)dμ?,Tn(U) = ɪsupɪ / Uμ? (U)dμ°,τ1 (u) = U0,1
It holds that μO({U : Uμ? (u) ≥ U?,。}) = O, μ0Tn ({u : Uμ? (U) ≥ U?,。}) = O and similarly
M?({u : Uμ? (u) ≥ U?, J) = O, μ?,Tn ({u : Uμ? (u) ≥ U?,1}) = O.
This implies that asymptotically Uμ? is constant μ? Tn -almost surely:
/ Uμ? (U) — I Uμ? (u0)dμι,τn (u0)卜μ?,Tn(U)
=2 Z max (θ,Uμ? (u) - Z Uμ? (u')dμ1,Tn (u"))dμ1,Tn (u)
≤2 (U?,1- / Uμ? (u)dμ?,T1n (U)	→ O
And the same holds that Uμ? is constant μ? Tn -almost surely. As a result,
limn→∞ RlogUμ?(u)dμ1,Tn(u) = logU0,1 andlimn→∞ RlogUμ:(u)dμ?,Tn(u) = logU?,。.
We now revisit (14) with a mixture over μ? and μo,Tn:
P(I) = P(O)/logUμ?(u)d(μo,τn -μ?)(U) + P(I) /` 「d”?(U)
≥ P(O) ∕log Uμ? (U)dG0,Tn - μ?)(U) + Uθ~ U Uμ? (U)dμ0,τn (U)
Taking the limit of both sides as n → ∞, we get
P(I) ≥ P(O)Iog U1,0 - P(O) / log Uμ? (U)dμ?(U) + U^~ U0,0
and rearranging and doing the same to (15) yields
≥ log U0?,1
≥ log U1?,0
-/ log Uμ? (u)dμ0(u)
-/log Uμ?(u)dμ?(U)
3.
Note that Jensen,s inequality and the definition of U?,。tell us that ʃlog Uμ? (u)dμ?(U) ≤
log R Uμ:(u)dμ?(U) ≤ log U?,。，which means that ρ∣(0)(1 - U?0) ≥ O. However, applying
the same logic also tells us that pp(ɪ)(1 - U?1) ≥ O. The only case in which this is possible is
when U?,0 = U?j. In which case equality is obtained. Therefore, this means that for optimal
仙?,仙1, it holds that
/log U"?(u)d〃?(u) = log/ U"?(u)d〃?(U)
/log Uμ?(u)dμ?(u) = log/ U"?(u)d〃?(U)
Collapsed embeddings minimize Lneg.
We return to the original objective function (13). Now, the optimal value of this expression can be
written as
P(O)Iog / U"?(u)d〃?(u) + ρ(1) log / Uμ? (u)dμ?(U)
=log / Uμ? (u)d〃?(u) = log /	exp(u>v∕τ)dμ?(U)dμ?(V)
21
Under review as a conference paper at ICLR 2022
We can see that when K = 2 our optimal distribution is to have collapsed embeddings with dot
product -1, i.e. opposite of each other on the hypersphere. For larger K, μo and μι represent
a one-versus-all scenario. Collectively, having embeddings per class with probability 1 on a
single point is part of the optimal solution for one-versus-all measures since there is nothing in
these objective functions that enforce intra-class spread. We abuse notation and refer to μ% as the
embedding for class i. Given this property, we can revisit Lneg ; minimizing it is equivalent to
minimizing
K
ElogEeXp(〃[〃j /)
i=1	j 6=k
We can show using KKT conditions that setting μ% = Vi for all i is an optimal distribution.
In particular, the Lagrangian is PK=IlogPj=iexp(μ>μj/丁) + PK=I λi(kμi∣∣2 - 1), and the
stationarity condition is
Pj=kμ eχp(μ"/) + X μ exp(μ>μk/)
Pj=k eχp(μ>μj /)	i=k Pj=i eχp(μ> μj /)	k μk
This has a solution of λi = ( Klexp(δ/TL、, and all other conditions hold. Therefore, We have
(K-I) exp(δ∕τ)
shown that collapsed embeddings minimize Lneg .
D.3 Proofs for Section 4.2
Lemma 1. Denote f1 to be an encoder learned on D with N using Lspread. Using f1 and a mean
classifier where Wz = Ex〜pz [fι(x)], the generalization error on the Coarse-to-fine transfer task is
at most
L(x,z,fι) ≤ Ez log(£exp(-Sintra(z, i)) + £exp(-Sinter(z, i))) - 1
i:S(i)=S(z)	i:S(i)6=S(z)
where δintra(z,i) = λ(1 δ(fι,z,i)2 - 1
for some λ > 0.
and δinter (z,i) = λ( 2 ∣∣Ez[fι(x)] - Ei[fι (x)]k2 - 1)
Proof. The generalization error is
Zv ^∖ F L Γ1	exp(fι(x)>Wz)	]]
L(x,z,fι) = -Ez Ex~Pζ log K---------/f 、>m、
i=1 exp(f1(x)>Wi)
C
=Ez Ex〜Pz -fι(x)>Wz + log Xexp(fι(x)>Wi)
i=1
Using the definition of the mean classifier,
一，	O .	_	_
L(X,z,f1) = Ez -1 + Ex 〜Pz
=-1+ Ez Ex〜Pz
C
log X exp (fι (x)>Ex〜Pi [fl (x)])
i=1
log X exp (fι(x)>Ei[fι(x)])J
1-t ∙	11	.1	∙	.	.	.	ʌ	-	C	1	.1	.
Since	f1(x)	is bounded, there exists a constant λ >	0 such that
Ex〜PzhlogPC=1 exp (fι(x)>Ei[fι(x)])] ≤ log (PC=I exp (λEz[f1(x)]>Ei[f1(x)])).	We
22
Under review as a conference paper at ICLR 2022
can also rewrite the dot product between mean embeddings per strata in terms of the distance between
them:
L(x,z,fι) ≤-1 + Ez log (Xexp (λEz[fi(x)]>Ei[fi(x)]))"|
i=1
=-1 + Ez log (Xexp ( - 2kEz[fι(x)] - Ei[fι(x)]k2 + λ))
i=1	2
Note that when i and z satisfy S(i) = S(z), kEz[f1(x)] - Ei[f1(x)]k can be written as δ(f1, z, i),
which we have analyzed. Therefore,
L(x,z,fι) ≤-1+ Ez log(X exp ( - 2δ(fι,z,i) + λ)
i∈SS(z)
+ Xexp ( - 2kEz[fl(x)] - Ei[fl(x)]k2 + λ))
i:S(i)6=S(z)
This directly gives us our desired bound.
□
Lemma 2. Denote f1 to be an encoder learned on D with N using Lspread. Using f1 and a mean
classifier where Wy = Ex〜p(∙∣y) [fι(x)], the generalization error on the original task Is at most
L(x,y,fl) ≤ Ez log ( exp(-δintra(ζ))+ X exp(-δinter (Z))) + Jδintra(z)
i6=S(z)	y
where δintra(z)	=	η Pzo∈Ss(z) P(ZlS(Z))(2δ(fι,z,z0)2 - 1)and big(z)	=
ηpzo∈s(z)P(ZlS(Z0))(2kEz[fi(x)] - EzOfι(X)]k2 - 1 forsomeη > 0∙
Proof. The generalization error is
Zv f∖ 而 IF	exp(fl(x)>WS(Z))]]
L(X,y,f1) =-EzPTlog PK=I exp(fι(x)>Wi)]]
K
=Ez Ex〜Pz -fι(x)>Ws(z) + logXexp(fι(x)>Wi)
i=1
We substitute in the definition of the mean classifier to get
L(x,y,fι) = Ez - XP(ZlS(Z))Ez[fι(x)]>Ezo[fι(x)]
z0∈SS(z)
K
+ Ex〜Pz log Xexp (XP(ZlSi)fι(x)>Ezo[fι(x)])
i=1	z0∈Si
We can rewrite the dot product between mean embeddings per strata in terms of the distance between
them:
L(x,y,fι) =Ez	XP(ZlS(Z)) ∙ (2kEz[fι(x)] - Ezo[fι(x)]∣∣2 - 1)
z0∈SS(z)
K
+ Ex〜PzlogXexp X X P(ZiSi)fι(x)>Ezo[fι(x)])
i=1	z0∈Si
23
Under review as a conference paper at ICLR 2022
ɪɪ τ	∙ . 11 ττn Γ ? / ∖ 1	ττn Γ r / ∖ 1 11 ∙ .ι	ι	♦	c/ ；	/、•>♦•>	ι
We can write kEz [f1 (x)] - Ez0 [f1 (x)]k in the above expression as δ(f1, z, z0), which we have
analyzed:
L(x,y,fι) =Ez	Xp(z0∣S(z)) ∙ (1 δ(fι,z,z0)2 - 1)
z0∈SS(z)
K
+ Ex〜PzlogXexp ( X p(z0∣Si)fι(x)>Ezθ[fι(x)])
i=1	z0∈Si
From our previous proof, there exists λ > 0 such that this is at most
L(x, y,fι) ≤Ez	Xp(z0∣S(z)) ∙ (2δ(fι,z, z0)2 - 1)
z0∈SS(z)
+ log (Xexp( X p(z0∣Si)λEz[fι(x)]>Ezθ[fι(x)]))
i=1	z0 ∈Si
=Ez	XP(ZlS(Z)) ∙ (1 δ(fι,z, Zy-1)
z0∈SS(z)
+ log (X exP ( X P(ZlSi) ( - 2 kEz[fι(x)] - Ez，[fι(x)]k2 + λ)))
i=1	z0∈Si
We can simplify the log term in this expression by considering ifi = S(Z), in which case the distances
between strata centers can be written using δ (f1, Z, Z0):
log (Xexp ( X P(ZlSi)( - 2kEz[fι(x)] - Ez，[fι(x)]k2 + λ)))
i=1	z0∈Si	2
= log (exp (XP(ZlS(Z))(- 2δ(fι,Z,Z0)2 + λ))
'	z0 ∈Ss(z)
+ X exp ( X ( - 2kEz[fι(x)] - Ez0[fl(x)]k2 + λ)))
i=S(z)	z0∈Si	)
Putting everything together, the generalization error is at most
L(x,y,fι) ≤Ez	XP(ZlS(Z)) ∙ (2δ(fι,Z,Z0)2 - 1)
z0 ∈Ss(z)
+ log f exp (XP(ZlS(Z))(- 2δ(fι,Z,Z0)2 + λ))
'	z0∈Ss(z)
+ X exp ( X ( - 2kEz[fι(x)] - Ez0[fι(x)]k2 + λ)))
i=S(z)	z0∈Si	)
This directly gives us our desired bound.
□
E Additional Experimental Details
E.1	Datasets
We first describe all the datasets in more detail:
24
Under review as a conference paper at ICLR 2022
•	CIFAR10, CIFAR100, and MNIST are all the standard computer vision datasets.
•	CIFAR10-Coarse consists of two superclasses: animals (dog, cat, deer, horse, frog, bird)
and vehicles (car, truck, plane, boat).
•	CIFAR100-Coarse consists of twenty superclasses. We artificially imbalance subclasses to
create CIFAR100-Coarse-U. For each superclass, we select one subclass to keep all 500
points, select one subclass to subsample to 250 points, select one subclass to subsample to
100 points, and select the remaining two to subsample to 50 points. We use the original
CIFAR100 class index to select which subclasses to subsample: the subclass with the lowest
original class index keeps all 500 points, the next subclass keeps 250 points, etc.
•	MNIST-Coarse consists of two superclasses: <5 and ≥5.
•	Waterbirds (Sagawa et al., 2019) is a robustness dataset designed to evaluate the effects
of spurious correlations on model performance. The waterbirds dataset is constructed by
cropping out birds from photos in the Caltech-UCSD Birds dataset (Welinder et al., 2010),
and pasting them on backgrounds from the Places dataset (Zhou et al., 2014). It consists
of two categories: water birds and land birds. The water birds are heavily correlated with
water backgrounds and the land birds with land backgrounds, but 5% of the water birds are
on land backgrounds, and 5% of the land birds are on water backgrounds. These form the
(imbalanced) hidden strata.
•	ISIC is a public skin cancer dataset for classifying skin lesions (Codella et al., 2019) as
malignant or benign. 48% of the benign images contain a colored patch, which form the
hidden strata.
E.2 Hyperparameters
For all model quality experiments for Lspread, we first fixed τ = 0.5 and swept α ∈
[0.16, 0.25, 0.33, 0.5, 0.67]. We then took the two best-performing values and swept τ ∈
[0.1, 0.3, 0.5, 0.7, 0.9]. For LSC and LSS, we swept τ ∈ [0.1, 0.3, 0.5, 0.7, 0.9]. Final hyperpa-
rameter values for (τ, α) for Lspread were (0.9, 0.67) for CIFAR10, (0.5, 0.16) for CIFAR10-coarse,
(0.5, 0.33) for CIFAR100, (0.5, 0.25) for CIFAR100-Coarse, (0.5, 0.25) for CIFAR100-Coarse-U,
(0.5, 0.5) for MNIST, (0.5, 0.5) for MNIST-coarse, (0.5, 0.5) for ISIC, and (0.5, 0.5) for waterbirds.
For coarse-to-fine transfer learning, we fixed τ = 0.5 for all losses and swept α ∈
[0.16, 0.25, 0.33, 0.5, 0.67]. Final hyperparameter values for α were 0.25 for CIFAR10-Coarse,
0.25 for CIFAR100-Coarse, 0.25 for CIFAR100-Coarse-U, and 0.5 for MNIST-Coarse.
E.3 Applications
We describe additional experimental details for the applications.
Robustness Against Worst-Group Performance We follow the evaluation of Sohoni et al. (2020).
First, we train a model on the standard class labels. We evaluate different loss functions for this
step, including Lspread, LSC, and the cross entropy loss LCE . Then we project embeddings of
the training set using a UMAP projection (McInnes et al., 2018), and cluster points to discover
unlabeled subgroups. Finally, we use the unlabeled subgroups in a Group-DRO algorithm to optimize
worst-group robustness (Sagawa et al., 2019).
Robustness Against Noise We use the same training setup as we use to evaluate model quality,
and introduce symmetric noise into the labels for the contrastive loss head. We train the cross entropy
head with a fraction of the full training set. In Section 5.2, we report results from training with 20%
labels to cross entropy. We report additional levels in Appendix F.
We detect noisy labels with a simple geometric heuristic: for each point, we compute the cosine
similarity between the embedding of the point and the center of all the other points in the batch that
have the same class. We compare this similarity value to the average cosine similarity with points
in the batch from every other class, and rank the points by the difference between these two values.
Points with incorrect labels have a small difference between these two values (they appear to be small
strata, so they are far away from points of the same class). Given the noise level as an input, we
25
Under review as a conference paper at ICLR 2022
Table 2: Performance of Lspread compared to LSC and using Lattract on its own.
Dataset	End Model Perf.			
	LSS	LSC	Lattract	Lspread
CIFAR10	89.7	90.9	91.3	91.5
CIFAR100	68.0	67.5	68.9	69.1
rank the points by this heuristic and mark the fraction of the batch with the smallest scores as noisy.
We then correct their labels by adopting the label of the closest cluster center.
Minimal Coreset Construction We use the publicly-available evaluation framework for coresets
from Toneva et al. (2019).1 We use the official repository from Paul et al. (2021)2 to recreate their
coreset algorithms.
Our coreset algorithm proceeds in two parts. First, we give each point a difficulty rating based on
how likely we are to classify it correctly under partial training. Then we subsample the easiest points
to construct minimal coresets.
First, we mirror the set up from our thought experiment and train with Lspread on random samples
of t% of the CIFAR10 training set, taking three random samples for each of t ∈ [10, 20, 50] (and
we train the cross entropy head with 1% labeled data). For each run, we record which points are
classified correctly by the cross entropy head at the end of training, and bucket points the training
set by how often the point was correctly classified. To construct a coreset of size t%, we iteratively
remove points from the largest bucket in each class. Our strategy removes easy examples first from
the largest coresets, but maintains a set of easy examples in the smallest coresets.
F	Additional Experimental Results
In this section, we report three sets of additional experimental results: the performance of using
Lattract on its own to train models, sample complexity of Lspread compared to LSC, and additional
noisy label results (including a bonus de-noising algorithm).
F.1 PERFORMANCE OF Lattract
In an early iteration of this project, we experienced success with using Lattract on its own to train
models, before realizing the benefits of adding in an additional term to prevent class collapse. As
an ablation, we report on the performance of using Lattract on its own in Table 2. Lattract can
outperform LSC, but Lspread outperforms both. We do not report the results here, but Lattract also
performs significantly worse than LSC on downstream applications, since it more direclty encourages
class collapse.
F.2 Sample Complexity
Figure 6 shows the performance of training ViT models with various amounts of labeled data for
Lspread, LSC, and LSS. In these experiments, we train the cross entropy head with 1% labeled data
to isolate the effect of training data on the contrastive losses themselves.
Lspread outperforms LSC and LSS throughout. At 10% labeled data, Lspread outperforms LSS by
13.9 points, and outperforms LSC by 0.5 points. By 100% labeled data (for the contrastive head),
Lspread outperforms LSS by 25.4 points, and outperforms LSC by 10.3 points.
1https://github.com/mtoneva/example_forgetting
2https://github.com/mansheej/data_diet
26
Under review as a conference paper at ICLR 2022
Sample Complexity, CIFAR10
■ L_spread
-→- L_SC
→- L_SS
Figure 6: Performance of training ViT with Lspread compared to training with LSC and LSS on
CIFAR10 at various amounts of labeled data. Lspread outperforms the baselines at each point. The
cross entropy head here is trained with 1% labeled data to isolate the effect of training data on the
contrastive losses.
F.3 Noisy Labels
In Section 5.2, we reported results from training the contrastive loss head with noisy labels and the
cross entropy loss with clean labels from 20% of the training data.
In this section, we first discuss a de-noising algorithm inspired by Chuang et al. (2020) that we
initially developed to correct for noisy labels, but that we did not observe strong empirical results
from. We hope that reporting this result inspires future work into improving contrastive learning.
We then report additional results with larger amounts of training data for the cross entropy head.
F.3.1 Debiasing Noisy Contrastive Loss
First, we consider the triplet loss and show how to debias it in expectation under noise. Then we
present an extension to supervised contrastive loss.
Noise-Aware Triplet Loss Consider the triplet loss:
L .	_e	LlC(T_______eχpmx,x+))_________
triplet	χ~P-χ+[p+"X),	exp(σ(x, x+)) + exp(σ(x, x-))
Now suppose that we do not have access to true labels but instead have noisy labels denoted
by the weak classifier ye := h(x). We adopt a simple model of symmetric noise where pe =
Pr(noisy label is correct).
+	++
We use yeto construct P+ and P- as p(x+|h(x) = h(x+)) andp(x-|h(x) 6= h(x-)). For simplicity,
we start by looking at how the triplet loss in (16) is impacted when noise is not addressed in the
binary setting. Define Ltnroiipslyet as Ltriplet used with Pe+ and Pe-.
27
Under review as a conference paper at ICLR 2022
Lemma 5. When class-conditional noise is uncorrected, Ltnroiipslyet is equivalent to
(e3 + (1 -例3)Ltriplet + e(i - e)E	X〜P
x+ ,x+ 〜p+ (∙∣ x)
- log
exp(σ(x, x1+ ))
exp(σ(x, x1+ )) + exp(σ(x, x2+ ))
+ e(i - P)E	X〜P
χ-,x-〜p- (TX)
-log exp(b(x,xI))-
exp(σ(x, x1- )) + exp(σ(x, x2- ))
+e(i - P)E	X〜P
x+ 〜p+ (∙∣ x)
X-〜p- (∙∣ X)
exp(σ (x, x- ))
—log ----------------------------
exp(σ(x, x+)) + exp(σ(x, x-))
Proof. We split Ltnroiipslyet depending on if the noisy positive and negative pairs are truly positive and
negative.
isy _ E	ι	eχρ(σ(χ, χ十))
plett	e+Sp+P∙∣X) [	°g exp(σ(x, x+)) + exp(σ(x, e-))
e-〜e-(∙∣X)
+	-	exp(σ(x, x ))
=P(A(X) = h(x+),h(X) = h(X ))E	X〜P	— log ——「——+τv^----------n———TT
X+〜p+(∙∣x)	exp(σ(x, x+)) + exp(σ(x, x ))
X-〜p-(∙∣X)
+ P(h(x) = h(xe+), h(x) = h(xe-))E	X〜P
X+ ,X+ 〜p+ (TX)
+ p(h(x) = h(e+), h(x) = h(e-))E	X〜P
x-,x-〜P-(TX)
-log_________exp(b(x, x+))________-
exp(σ(x, x1+)) + exp(σ(x, x2+ ))
-log	exp(b(X,x-))________-
exp(σ(x, x1- )) + exp(σ (x, x2- ))
+	-	exp(σ(x, x-))
+ P(A(X) = h(x+),h(X) = h(x ))E X〜P	— log ——--——+γrη--------「---------vʌ
x+〜p+(∙∣x) L	exp(σ(x, x+)) + exp(σ(x, x-))
X-〜p- (TX)
Define Pe = P(noisy label is correct). Note that
P(A(x) = A(xe+), A(x) 6= A(xe-)) = Pe3 + (1 - Pe)3
(i.e. all three points are correct or all reversed, such that their relative pairings are correct). In addition,
the other three probabilities above are all equal to p(1 一 访.	□
We now show that there exists a weighted loss function that in expectation equals Ltriplet .
Lemma 6. Define
Letriplet = E	X〜P,	— w+σ(x, x+) + w-σ(x, e—) + wι log ( exp (σ(x, e+)) + exp (σ(x, x-
e+,e+〜P+(∙∣χ) L	'	'	'
e-,e-〜P-(∙∣χ)
一W2 log ((exp(σ(x, e+)) + exp(σ(x, e+))) ∙ (exp(σ(x, e-)) + exp(σ(x, e-)))ʃj
where
+	Pe2	+ (1	- Pe)2	—	2Pe(1	-	Pe)	Pe2	+ (1 - Pe)2	Pe(1	-	Pe)
W =W =	Wi =W2 =
(2万-1)2	(2e -1)2	1	(2万-1)2	(2万一1)2
Then, E Le
triplet = Ltriplet.
28
Under review as a conference paper at ICLR 2022
Proof. We evaluate E -w1σ(x, xe1+) + w2σ(x, xe1-) and the other terms separately. Using the same
probabilities as computed in Lemma 5,
E	-w1σ(x,	xe1+)	+ w2σ(x, xe1-)	=	-(pe2	+ (1 -	pe)2)w1E	σ(x, x1+)	-	2pe(1	- pe)w1E	σ(x, x1-)
+ (pe2 + (1 - pe)2)w2E σ(x, x1-) + 2pe(1 - pe)w2E σ(x, x1+)
= -E σ(x, x1+)
We evaluate the remaining terms:
E w3 log exp σ(x, xe1+) + exp σ(x, xe1-)	=
(pe2 + (1 - pe)2)w3E log exp σ(x, x1+) + exp σ(x, x1-)
+ pe(1 — p)w3E [log ((exp(σ(x, x+)) + exp(σ(x, e+))) ∙ (exp(σ(x, x-)) + exp(σ(x, e-))))].
E	w4 log	exp	σ(x, xe1+)	+ exp	σ(x, xe2+)	+ E	w4 log	exp	σ(x, xe1-)	+ exp	σ(x, xe2-)	=
(pe2 + (1 — pe)2)w4E log exp σ(x, x1+) + exp σ(x, x2+)
+ 4pe(1 — pe)w4E log exp σ(x, x1+) + exp σ(x, x1- )
+ ((1 — pe)2 + pe2)w4E log exp σ(x, x1- ) + exp σ(x, x2- )
Examining the coefficients, we see that
(pe2 + (1 — pe)2)w3 — 4pe(1 — pe)w4
(pe2 + (1 —pe)2)2	4pe2(1 — pe)2
----：------------... -：-------.—
(2p — 1)2--------(2pe — 1)2
p(1 —汾W3 —铲 + (1 — P)2)W4 = P(I- *2 +(2- P)2)
(2p — 1)2
(P2 + (1— P)2)P(1—汾
(2p — 1)2
which shows that only the term E
pletes our proof.
log exp σ(x, x1+) + exp σ(x, x1- )
persists. This com-
We now show the general case for debiasing Lattract .
Lemma 7. Define m = n + 1 (as the “batch size” in the denominator), and
Leattract = E X〜P	- w+σ(x,e+) + W-σ(x,x-)
{xei+}im=1
{xej-}jm=1
m
+ wk log
k=0
(17)
(18)
w+ and w- are defined in the same was as before. w~ = {w0 , . . . wm} ∈ Rm+1 is the solution to the
system Pw = e2 where e2 is the standard basis vector in Rm+1 where the 2nd index is 1 and all
others are 0. The i, jth element of P is Pij = PeQi,j + (1 — Pe)Qm-i,j where
Qi,j
{pmin{j,m-i} Sl m-j )(1 - p)i-j+2kpem+j-i-2k
Pmin{i,m-j} (m-j)(. j'+ J(1 - pe)j-i+2kpem-j+i-2k
j≤i
j>i
Then, E Lattract = Lattract.
We do not present the proof for Lemma 7, but the steps are very similar to the proof for the triplet
loss case. We also note that a different from of E Leattract must be computed for the multi-class
case, which we do not present here (but can be derived through computation).
0
□
29
Under review as a conference paper at ICLR 2022
Geometry-correction, L_spread
Geometry-correction, L SC
Expectation-based noisy correction
L SC, no correction
Geometry-correction, L spread
Geometry-correction, L SC
Expectation-based noisy correction
L SC, no correction
Figure 7: Performance of models under various amounts of label noise for the contrastive loss head,
and various amounts of clean training data for the cross entropy loss.
Observation 4. Note that the values of Qi,j have high variance in the noise rate as m increases.
Also note that the number of terms in the summation of Qi,j increase combinatorially with m. We
found this de-noising algorithm very unstable as a result.
F.3.2 Additional Noisy Label Results
Now we report the performance of denoising algorithms with additional amounts of labeled data for
the cross entropy loss head. We also report the performance of using Lattract to debias noisy labels.
Figure 7 shows the results. Our geometric correction together with Lspread works the most consis-
tently. Using the geometric correction with LSC can be unreliable, since LSC can learn memorize
noisy labels early on in training. The expectation-based debiasing algorithm Lattract occasionally
shows promise but is unreliable, and is very sensitive to having the correct noise rate as an input.
30