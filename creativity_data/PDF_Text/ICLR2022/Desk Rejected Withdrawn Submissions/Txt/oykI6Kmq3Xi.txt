Fast Convergence of Optimistic Gradient Ascent in
Network Zero-Sum Extensive Form Games
Anonymous authors
Paper under double-blind review
Ab stract
The study of learning in games has thus far focused primarily on normal form games. In contrast, our
understanding of learning in extensive form games (EFGs) and particularly in EFGs with many agents
lags far behind, despite them being closer in nature to many real world applications. We consider
the natural class of Network Zero-Sum Extensive Form Games, which combines the global zero-sum
property of agent payoffs, the efficient representation of graphical games as well the expressive power
of EFGs. We examine the convergence properties of Optimistic Gradient Ascent (OGA) in these
games. We prove that the time-average behavior of such online learning dynamics exhibits O(1/T)
rate convergence to the set of Nash Equilibria. Moreover, we show that the day-to-day behavior also
converges to Nash with rate O(c-t) for some game-dependent constant c > 0.
1 Introduction
Extensive Form Games (EFGs) are an important class of games which have been extensively studied for more than 50
years (Kuhn, 1950b; Koller and Megiddo, 1992). EFGs capture various settings where several selfish agents sequentially
perform actions which change the state of nature, with the action-sequence finally leading to a terminal state, at which
each agent receives a payoff. The most ubiquitous examples of EFGs are real-life games such as Chess, Poker, Go etc.
Recently the application of the online learning framework has proven to be very successful in the design of modern AI
which can beat even the best human players in real-life games (Tammelin et al., 2015; Brown and Sandholm, 2017a;b).
At the same time, online learning in EFGs has many interesting applications in Economics, AI, machine learning and
sequential decision making that extend far beyond the design of game-solvers (ArieIi and Babichenko, 2016; P&rolat
et al., 2021).
Despite its numerous applications, online learning in games is far from well understood. From a practical point of view,
testing and experimenting with various online learning algorithms in EFGs requires a huge amount of computational
resources due to the large number of states in EFGs of interest (Zinkevich et al., 2008; Kroer et al., 2018; Brown and
Sandholm, 2018a; Rowland et al., 2019). From a theoretical perspective, it is known that online learning dynamics may
oscillate, cycle or even admit chaotic behavior even in very simple settings (Piliouras and Shamma, 2014; Palaiopanos
et al., 2017; Mertikopoulos et al., 2018b; Vlatakis-Gkaragkounis et al., 2019; Leonardos and Piliouras, 2021). On the
positive side, there exists a recent line of research into the special but fairly interesting class of two-player zero-sum
EFGs, which provides the following solid claim: In two-player zero-sum EFGs, the time-average strategy vector
produced by online learning dynamics converges to the Nash Equilibrium (NE), while there exist online learning
dynamics which exhibit day-to-day convergence (Zinkevich et al., 2008; Lanctot et al., 2009; Kroer et al., 2018; Farina
et al., 2019a; Wei et al., 2020; 2021b). Since in most settings of interest there are multiple interacting agents, all the
above motivates the following question:
Question. Are there natural and important classes of multi-agent extensive form games for which online learning
dynamics converge to a Nash Equilibrium? Furthermore, what type of convergence is possible? Can we only guarantee
time-average convergence or can we also prove day-to-day convergence (also known as last-iterate convergence) of the
dynamics?
In this paper we answer the above questions in the positive for an interesting class of multi-agent EFGs called Network
Zero-Sum Extensive Form Games. A Network EFG consists of a graph G = (V, E) where each vertex u ∈ V represents
a selfish agent and each edge (u, v) ∈ E corresponds to an extensive form game Γuv played between the agents
u, v ∈ V . Each agent u ∈ V selects her strategy so as to maximize the overall payoff from the games corresponding to
1
her incident edges. The game is additionally called zero-sum if the sum of the agents’ payoffs is equal to zero no matter
the selected strategies.
Network Zero-Sum EFGs are an interesting class of multi-agent EFGs for various reasons. First of all, their global
constant-sum property1 (the edge-games are not necessarily zero-sum) is very natural for closed systems in which
selfish agents competes over a fixed set of resources (Daskalakis and Papadimitriou, 2009a; Cai and Daskalakis, 2011a).
For example, consider the users of an online poker platform playing Head’s up Poker (2-player poker). Each user
can be thought of as a node in a graph and two users are connected by an edge (corresponding to a Head’s up Poker
game) if they play against each other. Note that here, each edge/game differs from another due to the differences in
the dollar/blind equivalence. Each user u selects a poker-strategy to utilize against the other players, with the goal of
maximizing her overall payoff. In addition, Network Zero-Sum EFGs are also interesting due to the fact that descriptive
complexity scales polynomially with the number of agents. Multi-agent EFGs that cannot be decomposed into pairwise
interactions (i.e., do not have a network structure) admit an exponentially large description with respect to the number
of the agents (Kearns et al., 2001; Babichenko, 2014). In this work, we analyze the convergence properties of the
online learning dynamics produced when all agents of a Network Zero-Sum EFG update their strategies according to
Optimistic Gradient Ascent:
Informal Theorem. When the agents of a network zero-sum extensive form game update their strategies through
optimistic gradient ascent, their time-average strategies converge with rate O(1/T) to Nash Equilibrium, while the
day-to-day mixed strategies converge with rate O(c-t) for some game-dependent constant c > 0.
Our Contributions. To the best of our knowledge, this is the first work establishing convergence to equilibrium of
online learning dynamics in network extensive form games with more than two agents. As already mentioned, there
has been a stream of recent works establishing the convergence of online learning dynamics in two-player zero-sum
EFGs. However, there are several key differences between the two-player and the network cases. All the previous
works concerning the two-player case follow a bilinear saddle point approach. Specifically, due to the fact that in
the two-agent case any Nash Equilibrium coincides with a min-max equilibrium, the set of Nash Equilibria can be
expressed as the solution to the following bilinear saddle-point problem:
minχ∈xmaxy∈γ x> ∙ A ∙ y = maxy∈γminχ∈χ x> ∙ A ∙ y
In the two-player case, one can show that online learning dynamics converge to Nash Equilibrium by showing that they
converge to the solution of the above saddle-point problem.
However, in the network case there is no min-max equilibrium, and thus there is no such connection between the Nash
Equilibrium and saddle-point optimization. To overcome this difficulty, we establish that optimistic gradient ascent in
Network Zero-Sum EFGs can be equivalently described as optimistic gradient descent in a two-player symmetric game
(R, R) over a treeplex polytope X . We remark that both the matrix R and the treeplex polytope X are constructed from
the Network Zero-Sum EFG. Using the zero-sum property of Network EFGs, we show that the constructed matrix R
satisfies the following ‘restricted’ zero-sum property:
x> ∙ R ∙ y + y> ∙ R ∙ x = 0 for all x,y ∈ X	(1)
Indeed, Property (1) is a generalization of the classical zero-sum property A = -A> . In general, the constructed
matrix R does not satisfy R = -R> and Property 1 simply ensures that the sum of payoffs equal to zero only when
x, y ∈ X . Our technical contribution consists of generalizing the analysis of Wei et al. (2020) (which holds for classical
two-player zero-sum games) to symmetric games satisfying Property (1).
Related Work. Network Zero-Sum Normal Form Games (Daskalakis and Papadimitriou, 2009a; Cai and Daskalakis,
2011a; Cai et al., 2016) are a special case of our setting, where each edge/game is a normal-form game. Network
zero-sum normal-form games present major complications compared to their two-player counterparts. The most
important of these complications is that in the network case, there is no min-max equilibrium. In fact, different Nash
Equilibria can assign different values to the agents. All the above works study linear programs for computing Nash
Equilibria in network zero-sum normal-form games. Cai and Daskalakis (2011a) introduce the idea of connecting a
network zero-sum normal form game with an equivalent symmetric game (R, R) which satisfies Property (1). This
generalizes the linear programming approach of two-player zero-sum normal-form games to the network case. They also
show that in network normal-form zero-sum games, the time-average behavior of online learning dynamics converge
with rate Θ(1∕√T) to the Nash Equilibrium.
1equivalent to the global zero-sum property.
2
The properties of Online Learning in two-player zero-sum EFGs have been studied extensively in literature. Zinkevich
et al. (2008) and Lanctot et al. (2009) propose no-regret algorithms for extensive form games with O(1∕√T) average
regret and polynomial running time in the size of the game. More recently, regret-based algorithms achieve O(1/T)
time-average convergence to the min-max equilibrium (Hoda et al., 2010a; Kroer et al., 2018; Farina et al., 2019a) for
two-player zero-sum EFGs. Finally, Lee et al. (2021a) and Wei et al. (2021a) establish that Online Mirror Descent
achieves O(c-t) last-iterate convergence (for some game-dependent constant c ∈ (0, 1)) in two-player zero-sum EFGs.
2	Preliminaries
2.1	Extensive Form Games
Definition 1. A two-player extensive form game Γ is a tuple Γ := hH, A, Z, p, Ii where
•	H denotes the states of the game that are decision points for the agents. The states h ∈ H form a tree rooted
at an initial state r ∈ H.
•	Each state h ∈ H is associated with a set of available actions A(h).
•	Each state h ∈ H admits a label Label(h) ∈ {1, 2, c} denoting the acting player at state h. The letter c
denotes a special agent called a chance agent. Each state h ∈ H with Label(h) = c is additionally associated
with a function σh : A(h) 7→ [0, 1] where σh(α) denotes the probability that the chance player selects action
α ∈ A(h) at state h,	α∈A(h) σh (α) = 1.
•	Next(α, h) denotes the state h0 := Next(α, h) which is reached when agent i := Label(h) takes action
α ∈ A(h) at state h. Hi ⊆ H denotes the states h ∈ H with Label(h) = i.
•	Z denotes the terminal states of the game corresponding to the leaves of the tree. At each z ∈ Z no further
action can be chosen, so A(Z) = 0 for all Z ∈ Z. Each terminal state Z ∈ Z is associated with values
(u1 (z), u2(z)) where pi(z) denotes the payoff of agent i at terminal state z.
•	Each set of states Hi is further partitioned into information sets (I1, . . . ,Ik) where I(h) denotes the informa-
tion set of state h ∈ Hi. In the case thatI(h1) = I(h2) for some h1, h2 ∈ H1, then A(h1) = A(h2).
Information sets model situations where the acting agent cannot differentiate between different states of the game due
to a lack of information. Since the agent cannot differentiate between states of the same information set, the available
actions at states h1, h2 in the same information set (I(h1) = I(h2)) must coincide, in particular A(h1) = A(h2).
Definition 2. A behavioral plan σi for agent i is a function such that for each state h ∈ Hi, σi (h) is a probability
distribution over A(h) i.e. σi (h, α) denotes the probability that agent i takes action α ∈ A(h) at state h ∈ Hi.
Furthermore it is required that σi(h1) = σi(h2) for each h1, h2 ∈ Hi with I(h1) = I(h2). The set of all behavioral
plans for agent i is denoted by Σi.
The constraint σi(h1) = σi(h2) for all h1, h2 ∈ Hi with I(h1) = I(h2) models the fact that since agent i cannot
differentiate between states h1, h2, agent i must act in the exact same way at states h1, h2 ∈ Hi.
Definition 3. For a collection of behavioral plans σ = (σ1, σ2) ∈ Σ1 × Σ2 the payoff of agent i, denoted by Ui(σ), is
defined as:
Ui(G) = Epi(Z) ∙ π(h,h0)∈P(z) σLabel(h)(h, αh0)
z∈Z	、--------------{z------------}
probability that state z is reached
where P(Z) denotes the path from the root state r to the terminal state Z and αh0 denotes the action α ∈ Hi such that
h0 = Next(h, α).
Definition 4. A collection ofbehavioral plans σ* = (σj,σg) is called a Nash Equilibrium if for all agents i = {1, 2},
Ui(σi,σ-i) ≥ Ui(σi,σ-J forall σi ∈ ∑i
The classical result of Nash (1951) proves the existence of Nash Equilibrium in normal form games. This result also
generalizes to a wide class of extensive form games which satisfy a property called perfect recall (Kuhn and Tucker
(1953); Selten (1965)).
3
Definition 5. A two-player extensive form game Γ := hH, A, Z,p,Ii has perfect recall if and only if for all states
h1, h2 ∈ Hi with I(h1) = I(h2) the following holds: Define the sets P(h1) ∩ Hi := (p1, . . . ,pk, h1) and P(h2) ∩
Hi := (q1, . . . ,qm,h2). Then:
1.	k = m.
2.	I (p`) = I (q') for all' ∈{1,...,k}.
3.	p'+ι ∈ Next(P',α,i) and q'+ι ∈ Next(qg,α,i) for some action α ∈ /(p`) (since A(p') = A(qg)).
Before proceeding, let us further explain the perfect recall property. As already mentioned, agent i cannot differentiate
between states h1, h2 ∈ Hi whenI(h1) = I(h2). In order for the state h1 to be reached, agent i must take some specific
actions along the path P(h1) ∩ Hi := (p1, . . . ,pk, h1). The same logic holds for P(h2) ∩ Hi := (p1, . . . ,pk, h1).
In case where agent i could distinguish P(h1) ∩ Hi from set P(h2) ∩ Hi, then she could distinguish state h1 from
h2 by recalling the previous states in Hi . This is the reason for the second constraint in Definition 5. Even if
I(p`) = I(q') for all' ∈ {1,..., k}, agent i could still be able to distinguish hi from h? if p'+ι ∈ Next(p`, α, i) and
q'+i ∈ Next(q', ɑ0, i). In such a case, agent i can distinguish hi from h2 by recalling the actions that she previously
played and checking if the `-th action was α or α0 . The latter case is encompassed by the third constraint.
2.2 Extensive Form Games in Sequence Form
A two-player extensive form game Γ can be captured by a two-player bilinear game where the action spaces of the
agents are a specific kind of polytope, commonly known as a treeplex (Hoda et al., 2010a). In order to formally define
the notion of a treeplex, we first need to introduce some additional notation.
Definition 6. Given an two-player extensive form game Γ, we define the following:
•	P(h) denotes the path from the root state r ∈ H to the state h ∈ H.
•	Level(h) denotes the distance from the root state r ∈ H to state h ∈ H.
•	Prev(h, i) denotes the lowest ancestor ofh in the set Hi. In particular,
Prev(h, i) = argmaxh0∈P(h)∩HiLevel(h0).
•	The set of states Next(h, α, i) ⊆ H denotes the highest descendants h0 ∈ Hi once action α ∈ A(h) has been
taken at state h. More formally, h0 ∈ Next(h, α, i) if and only if in the path P(h, h0) = (h, hi, . . . , hk, h0),
all states h` ∈/ Hi and hi = Next(h, α).
Definition 7. Given a two-player extensive form game Γ, the set XiΓ is composed by all vectors xi ∈ [0, 1]|Hi|+|Z|
which satisfy the following constraints:
1.	xi(h) = 1	forall h ∈ Hi with Prev(h, i)	=	0.
2.	xi(hi) =	xi(h2) if there exists h0i, h02	∈	Hi	such that hi	∈ Next(h0i, α, i),	h2 ∈ Next(h02,	α, i)	and
I (h0i ) = I (h02 ).
3.	Pα∈A(h) xi(Next(h, α, i)) = xi(h) for all h ∈ Hi.
A vector xi ∈ XiΓ is typically referred to as an agent i’s strategy in sequence form. Strategies in sequence form come as
an alternative to the behavioral plans of Definition 2. As established in Lemma 1, there exists an equivalence between a
behavioral plan σi ∈ Σi and a strategy in sequence form xi ∈ XiΓ for games with perfect recall.
Lemma 1. Consider a two-player extensive form game Γ with perfect recall and the (|Hi | + |Z|) × (|H2| + |Z|)
dimensional matrices AiΓ, A2Γ with [AiΓ]zz = pi (z) for all terminal nodes z ∈ Z and 0 otherwise. There exists a
polynomial-time algorithm transforming any behavioral plan σi ∈ Σi to a vector xσi ∈ XiΓ such that
U1(σ1,σ2) = χ>ι ∙ Af ∙ xσ2	and	U2(σ1,σ2) = x>? ∙ A§ ∙ xσι
Conversely, there exists a polynomial-time algorithm transforming any vector xi ∈ XiΓ to a vector σxi ∈ Σi such that
x> ∙ Af ∙x2 = U1(σχ1 ,σχ2)	and	x> ∙ Af ∙ xi = U2(σχ1 g?)
4
To this end, one can understand why strategies in sequence form are of great use. Assume that agent 2 selects a
behavioral plan σ? ∈ ∑2. Then, agent 1 wants to compute a behavioral plan σj ∈ ∑ι which is the best response to σ2,
namely σj := argmaXσι∈∑ι U1(σ1,σ2). This computation can be done in polynomial-time in the following manner:
Agent 1 initially converts (in polynomial time) the behavioral plan σ2 to xσ2 ∈ X2Γ, which is the respective strategy
in sequence form. Then, she can obtain a vector x； = argmaXχι∈χγ x> ∙ Ar ∙ x2. The latter step can be done in
polynomial-time by computing the solution of an appropriate linear program. Finally, she can convert the vector x； to a
behavioral plan σχ* ∈ Σ; in polynomial-time. Lemma 1 ensures that σχ* = argmaXσ1 ∈∑1 U1(σ1, σ2).
The above reasoning can be used to establish an equivalence between the Nash Equilibrium (σj,σ2) of an EFG
Γ := hH, A, Z , p, Ii with the Nash Equilibrium in its sequence form.
Definition 8. A Nash Equilibrium of a two-player EFG Γ in sequence form is a vector (x1；, x；2) ∈ X1Γ × X2Γ such that
•	(x；)>	∙	Aγ	∙ x2	≥	(xι)>	∙ Ar ∙ x2 for all x； ∈	Xr
•	(x2)>	∙	Ar	∙ x；	≥	(x2)>	∙ Ar ∙ x； for all x2 ∈	Xj
Lemma 1 directly implies that any Nash Equilibrium of an EFG (。；，。；)∈ Σ; X ∑2 of Definition 4 can be converted
in polynomial-time to a Nash Equilibrium in the sequence form (x；, x；) ∈ Xγ × Xf and vice versa.
2.3 Optimistic Mirror Descent
In this section we introduce and provide the necessary background for Optimistic Mirror Descent (Rakhlin and Sridharan,
2013a). For a convex function ψ : Rd 7→ R, the corresponding Bregman divergence is defined as
Dψ(x,y) := ψ(x) — ψ(y) — hVψ(y),x — yi
If ψ is γ-strongly convex, then Dψ (x,y) ≥ γ ∣∣x — y∣∣. Here and in the rest of the paper, we note that ∣∣ ∙ ∣∣ is shorthand
for the L2-norm.
Now consider a game played by n agents, where the action of each agent i is a vector xi from a convex set Xi . Each
agent selects its action xi ∈ Xi so as to minimize her individual cost (denoted by Ci(xi, x-i)), which is continuous,
differentiable and convex with respect to xi . Specifically,
Ci(λ∙xi + (1 — λ) ∙ xi,x-i) ≤ λ ∙ Ci(xi,x-i) + (1 — λ) ∙ Ci(xi,x-i) forall λ ∈ [0,1]
Given a step size η > 0 and a convex function ψ(∙) (called a regularizer), OptimisticMirrorDescent (OMD) sequentially
performs the following update step for t = 1, 2, . . .:
Xtt = argminx∈Xi {η〈x, FitT(x)〉+ Dψ (x,xt)}	⑵
xt+1 = argminx∈Xi {η〈x, Ftt(x)〉+ Dψ(x,xt)}	⑶
where Ft(Xt) = VxiCt(xt,x-t) and Dψ(x,y) is the Bregman Divergence with respect to ψ(∙). If the step-size η
selected is sufficiently small, then Optimistic Mirror Descent ensures the no-regret property (Rakhlin and Sridharan,
2013a), making it a natural update algorithm for selfish agents (Hazan, 2019). To simplify notation we denote the
projection operator of a convex set X； as Πχ* (x) := argmaxx*∈χ* ∣x — x； ∣ and the squared distance of vector X
from a convex set X； as dist2(x, X；) := ∣∣x — Πχ* (x)∣2.
3 Our Setting
We introduce the concept of a Network Zero-Sum Extensive Form Game, which is a network extension of the two
player EFGs we have introduced in Section 2.
3.1	Network Zero-Sum Extensive Form Games
A network extensive form game is defined with respect to an undirected graph G = (V, E) where nodes V (|V | = n)
correspond to the set of players and each edge (u, v) ∈ E represents a two-player extensive form game Γuv played
between agents u, v. Each node/agent u ∈ V selects a behavioral plan σu ∈ Σu which they use to play all the two-player
EFGs on its outgoing edges.
5
Definition 9 (Network Extensive Form Games). A network extensive form game is a tuple Γ := hG, H, A, Z, Ii where
•	G = (V, E) is an an undirected graph where the nodes V represents the agents.
•	Each agent u ∈ V admits a set of states Hu at which the agent u plays. Each state h ∈ Hu is associated with
a set A(h) of possible actions that agent u can take at state h.
•	I(h) denotes the information set of h ∈ Hu. If I(h) = I(h0) for some h, h0 ∈ Hu then A(h) = A(h0).
•	For each edge (u, v) ∈ E, Γuv is a two-player extensive form game with perfect recall. The states of Γuv are
denoted by Huv ⊆ Hu ∪ Hv.
•	For each edge (u, v) ∈ E, Zuv is the set of terminal states of the two-player extensive form game Γuv where
puΓuv (z) denotes the payoffs of u, v at the terminal state z ∈ Zuv. The overall set of terminal states of the
network extensive form game is the set Z := ∪(u,v)∈E Z uv.
In a network extensive form game, each agent u ∈ V selects a behavioral plan σu ∈ Σu (see Definition 2) that they use
to play the two-player EFG’s Γuv with (u, v) ∈ E. Each agent selects her behavioral plan so as to maximize the sum of
the payoffs of the two-player EFGs in her outgoing edges.
Definition 10. Given a collection of behavioral plans σ = (σ1, . . . , σn) ∈ Σ1 × . . . × Σn the payoff of agent u, denoted
by Uu(σ), equals
Uu (σ) :=	X	puΓuv(σu,σv)
v<u,v)∈E
Moreover a collection σ* = (σj,..., σn) ∈ ∑ι X ... X ∑n is called a Nash Equilibrium if and only if
Uu(σU,σ-U) ≥ Uu(σu,σ-U) forall σu ∈ ∑u
As already mentioned, each agent u ∈ V plays all the two-player games Γuv for (u, v) ∈ E with the same behavioral
plan σu ∈ Σu. This is due to the fact that the agent cannot distinguish between a state h1 , h2 ∈ Hu withI(h1 ) = I(h2)
even if h1 , h2 are states of different EFG’s Γuv and Γuv0. As in the case of perfect recall the latter implies that u cannot
differentiate states h1 , h2 even when recalling the states Hu visited in the past and her past actions. In Definition 11 we
introduce the notion of consistency (this corresponds to the notion of perfect recall for two-player extensive form games
(Definition 5)). From now on we assume that the network EFG is consistent without mentioning it explicitly.
Definition 11. A network extensive form game Γ := hG, H, A, Z, Ii is called consistent if and only if for all players
u ∈ V and states h1 , h2 ∈ Hu with I(h1 ) = I(h2) the following holds: for any (u, v), (u, v0) ∈ E the sets
Puv(h1) ∩ Hu := (p1 , . . . ,pk, h1) and Puv0 (h2) ∩ Hu := (q1, . . . , qm, h2) satisfy:
1.	k = m.
2.	I(p`) = I(q') for all' ∈{1,k}.
3.	p'+ι ∈ Nextruv (p`,ɑ,u) and q'+ι ∈ Nextruv0 (g`,a,u) for some action α ∈ /(p`).
where Puv(h) denotes the path from the root state to state h in the two-player extensive form game Γuv.
In this work we study the special class of network zero-sum extensive form games. This class of games is a generalization
of the network zero-sum normal-form games studied in Cai and Daskalakis (2011b).
Definition 12. A behavioral plan σu ∈ Σu of Definition 2 is called pure if and only if σu (h, α) either equals 0 or
1 for all actions α ∈ A(h). A network extensive form game is called zero-sum if and only if for any collection
σ := (σ1 , . . . , σn) of pure behavioral plans, Uu(σ) = 0 for all u ∈ V.
3.2 Network Extensive Form Games in Sequence Form
As in the case of two-player EFGs, there exists an equivalence between behavioral plans σu ∈ Σu and strategies in
sequence form xu. As we shall later see, this equivalence is of great importance since it permits the design of natural
and computationally efficient learning dynamics that converge to Nash Equilibria both in terms of behavioral plans and
strategies in sequence form.
6
Definition 13. Given a network extensive form game Γ := hG, H, A, Z, Ii, the treeplex polytope Xu ⊆ [0, 1]|Hu|+|Zu|
is the set defined as follows: xu ∈ Xu if and only if
1.	xu ∈ XuΓuv for all (u, v) ∈ E.
2.	xu(h1) = xu(h2) in case there exists (u, v), (u, v0) ∈ E and h01, h02 ∈ Hu with I(h01) = I(h02) such that
h1 ∈ NextΓuv (h01 , α, u), h2 ∈ NextΓuv0 (h02, α, u) and I(h01) = I (h02).
The second constraint of Definition 13 is the equivalent of the second constraint in Definition 7. To this end, we remark
that the linear equations describing the treeplex polytope Xu can be derived in polynomial-time with respect to the
description of the network extensive form game. In Lemma 2 we formally state and prove the equivalence between
behavioral plans and strategies in sequence form.
Lemma 2. Consider the matrix Auv of dimensions (|Hu| + |Zu|) × (|Hv | + |Zv|) such that
[Auv]	=	puΓuv (h)	if h1 = h2 = h ∈ Zuv
h1h2	0	otherwise
There exists a polynomial time algorithm converting any collection of behavioral plans (σ1, . . . , σn) ∈ Σ1 × . . . × Σn
into a collection of vectors (x1, . . . , xn) ∈ X1 × . . . × Xn such that for any u ∈ V,
Uu(σ) = x>∙ X	Auv ∙xv
v<u,v)∈E
In the opposite direction, there exists a polynomial time algorithm converting any collection of vectors (x1, . . . , xn) ∈
X1 × . . . × Xn into a collection of behavioral plans (σ1, . . . , σn) ∈ Σ1 × . . . × Σn such that for any u ∈ V,
x> ∙	X	Auv ∙ Xv = Uu(σ)
v∙.(u,v)∈E
Definition 14. A Nash Equilibrium ofa network extensiveform game G in sequence form is a vector (x；,..., Xn) ∈
X1 × . . . × Xn such that for all u ∈ V :
(xu)> ∙	X	Auv	∙ Xv	≥ x> ∙ X	Auv	∙	Xv	for all Xu	∈ Xu
v<u,v)∈E	v<u,v)∈E
Corollary 1. Given a network extensiveform game, any Nash Equilibrium (σj ,...,σn) ∈ ∑ι X ... X ∑n OfDefinition 4
can be converted in polynomial-time to a Nash Equilibrium (X1；, . . . , X；n) ∈ X1 × . . . × Xn of Definition 14 and vice
versa.
4 Our Results
In this work, we study the convergence properties of Optimistic Gradient Ascent (OGA) when applied to network
zero-sum EFGS. OGA is a special case of Optimistic Mirror Descent where the regularizer is ψ(a) = ɪ ∣∣a∣∣2, which
means that the Bregman divergence Dψ (x, y) equals ɪ ∣∣x 一 yk2. Since in network zero-sum EFGs each agent tries to
maximize her payoff, OGA takes the following form:
Xtu = argmaxx∈Xu
n(X, X	Auv ∙ xV-1∖ 一 Dψ (x,xU)
∖ v∙.(u,v)∈E	/
Xu+1 = argmaxx∈Xu
ηb,	X	Auv ∙Xv)
∖	v∙.(u,v)∈E	/
(4)
(5)
In Theorem 1 we claim and describe the Θ(1∕T) convergence rate for the time-average strategy produced by OGA.
Theorem 1. Let {X1, X2, . . . XT} be the vectors produced by Equations (4),(5) for some initial strategies
X0 := (X01, . . . , X0n). There exist game-dependent constants c1, c2 > 0 such that ifη ≤ 1/c1 then for any u ∈ V :
X> ∙	^X	Auv	∙	Xv	≥ x> ∙	^X	Auv	∙ Xv 一 Θ (c1 j2)	for all X ∈	Xu
v∙.(u,v)∈E	v∙.(u,v)∈E
where Xu = PT=I Xu/T.
7
Applying the polynomial-time transformation of Lemma 2 to the time-average strategy vector X = (Xi,..., Xn)
produced by Optimistic Gradient Ascent, we immediately get that for any agent u ∈ V ,
Uu(σ^u,σ-u) ≥ Uu(σu,σ-u) - Θ(cι ∙ c2∕T) forall σu ∈ ∑u
In Theorem 2 we establish the fact that OGA admits last-iterate convergence to NE in network zero-sum EFGs.
Theorem 2. Let {X1, X2, . . . XT} be the vectors produced by Equations (4),(5) for η ≤ 1∕c3 when applied to a network
zero-sum extensive form game. Then, the following inequality holds:
dist2(xt, X*) ≤ 64dist2(x1, X*) ∙ (1 + cι)-t
where X * denotes the set ofNash Equilibria, ci := min { 163) , 2 }
and c3 , c are positive game-dependent constants.
We conclude the section by providing the key ideas towards proving Theorems 1 and 2. For the rest of the section, we
assume that the network extensive form game is zero-sum. Before proceeding, we introduce a few more necessary
definitions and notations. We denote as X := Xi × . . . × Xn the product of treeplexes of Definition 13 and define the
|X | × |X | matrix R as follows:
=	-[Auv]h1h2	if(u,v)∈E
:h1),(v:h2)	0	otherwise
The matrix R can be used to derive a more concrete form of the Equations (4),(5):
Lemma 3. Let {Xi, X2, . . . , XT} be the collection of strategy vectors produced by Equations (4),(5) initialized with
X0 := (Xi0, . . . , X0n) ∈ X. The equations
Xt = argminχ∈χ {η〈x, R ∙ xt-1) + Dψ (x, Xt)}	(6)
Xt+1 = argminχ∈χ {η〈x, R ∙ Xt) + Dψ (x, Xt)}	(7)
produce the exact same collection of strategy vectors {Xi, . . . , XT} when initialized with X0 ∈ X.
To this end, we derive a two-player symmetric game (R, R) defined over the polytope X. More precisely, the X-agent
selects X ∈ X so as to minimize X>Ry while the y-agent selects y ∈ X so as to minimize y>RX. Now consider the
Optimistic Mirror Descent algorithm (described in Equations (2),(3)) applied to the above symmetric game. Notice
that if X0 = y0, then by the symmetry of the game, the produced strategy vector (Xt, yt) will be of the form (Xt, Xt)
and indeed Xt, Xt will satisfy Equations (6), (7). We prove that the produced vector sequence {Xt}t≥ι converges to
symmetric Nash Equilibrium.
Lemma 4. A strategy vector X* is an -symmetric Nash Equilibrium for the symmetric game (R, R) if the following
holds:
(x*)> ∙ R ∙ X* ≤ x> ∙ R ∙ X* + E for all X ∈ X
Any -symmetric Nash Equilibrium X* ∈ X is also an -Nash Equilibrium for the network zero-sum EFG.
A key property of the constructed matrix is the one stated and proven in Lemma 5. Its proof follows the steps of the
proof of Lemma B.3 in Cai and Daskalakis (2011b) and is presented in Appendix B.5.
Lemma 5. x> ∙ R ∙ y + y> ∙ R ∙ X = 0 for all x, y ∈ X.
Once Lemma 5 is established, we can use to it to prove that the time-average strategy vector converges to an E-symmetric
Nash Equilibrium in a two-player symmetric game.
Lemma 6. Let (Xi, X2, . . . , XT) be the sequence of strategy vectors produced by Equations (6),(7) for
η ≤ min{1∕8kRk2, 1}. Then,
min >χτ R 金〉θ (D2kRk2
minx∈XX ∙ R ∙ X ≥ -θ I  T—
where X = PT=I xs∕T and D is the diameter ofthe treePlexpolytope X.
D2 kRk2
Combining Lemma 5 With Lemma 6, We get that that the time-average vector X is a Θ ( —T^ J-SymmetnC Nash
Equilibrium. This follows directly from the fact that X> ∙ R ∙ X = 0. Then, Theorem 1 follows by direct application of
Lemma 4. For completeness, We present the complete proof in Appendix B.7.
8
By Lemma 5, it directly follows that the set of symmetric Nash Equilibria can be written as:
X* = {x* ∈ X : min x> ∙ R ∙ x* = 0}.
x∈X
Using this, we establish that Optimistic Gradient Descent admits last-iterate convergence to the symmetric NE of the
(R, R) game. This result is formally stated and proven in Theorem 3, the proof of which is deferred to Appendix B.8.
Theorem 2 then follows directly by Theorem 3 and Lemma 4.
Theorem 3. Let {x1, x2, . . . xT} be the vectors produced by Equations (6),(7) for η ≤ min(1/8kRk2, 1). Then:
dist2(Xt, X*) ≤ 64dist2(x1, X*) ∙ (1 + C2)-t
where C2 := min { 16,1。, 1} with C being a positive game-dependent constant.
5 Experimental Results
In order to better visualize our theoretical results, we experimentally evaluate OGA when applied to various network
extensive form games. As a sort of sanity check for our results, we first experimented with small random network
games, where each bilinear game between the players on the nodes is a randomly generated extensive form game.
As part of the experimental process, for each randomly generated game we ran a hyperparameter search to find the value
of η which gave the best convergence rate. As can be seen in Figure 1(a)-(b), we are able to obtain fast convergence in
the day-to-day sense to the Nash Equilibrium. We measure the log of the distance between each player’s strategy at
time t and the set of Nash Equilibria, given by log(dist2(xt, X*)). Note here that in all our experiments, we use the
last iterate of the strategy vectors as the Nash Equilibrium value.
In Figure 1(c), we show results of our experiments with an oft-studied simplification of poker, namely Kuhn poker.
In our experiment, we model a situation whereby each player is playing against multiple other agents. We see that
although the time needed for the players to converge is significantly greater, with a careful choice of η we can guarantee
convergence to the set of Nash Equilibria for all players. Further experiments and detailed game descriptions can be
found in Appendix C.
---Game 1 (η = 0.2)
一 Game 2 (η = 0.3)
--- Game 3 (η = 0.9)
一 Game 4 (η = 0.24)
——Game 5 (η = 0.7)
Figure 1: Simulations using OGA in network extensive form games, where each player is involved in 2 or more different
games and must select their strategy accordingly. The plots shown are: (Left) Day-to-day convergence to the Nash in
3-player random network extensive form games. (Center) Day-to-day convergence to the Nash in 4-player random
network extensive form games. Note the significantly longer time needed to achieve convergence compared to the
3-player experiment. (Right) Convergence to the Nash in 5-player Kuhn poker.
((%-×z2Ms6o
6	Conclusion
In this paper, we provide a formulation of Network Zero-Sum Extensive Form Games, which encode the setting where
multiple agents compete in pairwise games over a set of resources. We analyze the convergence properties of Optimistic
Gradient Ascent in this setting, proving that OGA results in both time-average and day-to-day convergence to the set of
Nash Equilibria. In order to show this, we utilize a transformation from network zero-sum extensive form games to
two-player symmetric games and subsequently show the convergence results in the symmetric game setting. This work
represents an initial foray into the world of online learning dynamics in network extensive form games, and we hope
that this will lead to more research into the practical and theoretical applications of multi-agent extensive form games.
9
Reproducibility S tatement
In order to ensure reproducibility for our theoretical results, we have made efforts to clearly describe the proof of all of
our results in Appendix B. Moreover, due to the complicated nature of the network extensive form games we study, we
have also used a substantial amount of space in the main text (see Sections 2 and 3) to fully describe our setting and all
necessary details of such. In addition, for our experimental results we present the descriptions of all simulations run in
Appendix C. Finally, within the supplementary material we have also included a folder which contains the source code
used to generate the figures seen throughout the paper.
References
I.	Arieli and Y. Babichenko. Random extensive form games. J. Econ. Theory, 166:517-535, 2016.
W. Azizian, I. Mitliagkas, S. Lacoste-Julien, and G. Gidel. A tight and unified analysis of gradient-based methods for a
whole spectrum of differentiable games. In International Conference on Artificial Intelligence and Statistics, pages
2863-2873. PMLR, 2020.
Y. Babichenko. Query complexity of approximate nash equilibria. In D. B. Shmoys, editor, Symposium on Theory of
Computing, STOC 2014, New York, NY, USA, May 31 - June 03, 2014, pages 535-544. ACM, 2014.
J.	P. Bailey and G. Piliouras. Multiplicative weights update in zero-sum games. In Proceedings of the 2018 ACM
Conference on Economics and Computation, pages 321-338, 2018.
M.	Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold’em poker is solved. Science, 347(6218):
145-149, 2015.
M. Bowling, N. Burch, M. Johanson, and O. Tammelin. Heads-up limit hold’em poker is solved. Commun. ACM, 60(11):
81-88, Oct. 2017. ISSN 0001-0782. doi: 10.1145/3131284. URL https://doi.org/10.1145/3131284.
N. Brown and T. Sandholm. Safe and nested endgame solving for imperfect-information games. In The Workshops
of the The Thirty-First AAAI Conference on Artificial Intelligence, Saturday, February 4-9, 2017, San Francisco,
California, USA, volume WS-17 of AAAI Workshops. AAAI Press, 2017a.
N. Brown and T. Sandholm. Libratus: The superhuman AI for no-limit poker. In C. Sierra, editor, Proceedings of the
Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August
19-25, 2017, pages 5226-5228. ijcai.org, 2017b.
N. Brown and T. Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359
(6374):418-424, 2018a.
N.	Brown and T. Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359
(6374):418-424, 2018b.
Y. Cai and C. Daskalakis. On minmax theorems for multiplayer games. In D. Randall, editor, Proceedings of the
Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California,
USA, January 23-25, 2011, pages 217-234. SIAM, 2011a. doi: 10.1137/1.9781611973082.20. URL https:
//doi.org/10.1137/1.9781611973082.20.
Y. Cai and C. Daskalakis. On minmax theorems for multiplayer games. In Proceedings of the twenty-second annual
ACM-SIAM symposium on Discrete algorithms, pages 217-234. SIAM, 2011b.
Y. Cai, O. Candogan, C. Daskalakis, and C. H. Papadimitriou. Zero-sum polymatrix games: A generalization of
minmax. Math. Oper. Res., 41(2):648-655, 2016.
C. Daskalakis and I. Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization.
arXiv preprint arXiv:1807.04252, 2018a.
C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization. arXiv preprint
arXiv:1807.03907, 2018b.
10
C. Daskalakis and C. H. Papadimitriou. On a network generalization of the minmax theorem. In S. Albers, A. Marchetti-
Spaccamela, Y. Matias, S. E. Nikoletseas, and W. Thomas, editors, Automata, Languages and Programming, 36th
Internatilonal Colloquium, ICALP 2009, Rhodes, Greece, July 5-12, 2009, Proceedings, Part II, volume 5556 of
LectureNotes in ComputerScience, pages 423-434. Springer, 2009a. doi:10.1007/978-3-642-02930-1\_35. URL
https://doi.org/10.1007/978-3-642- 02930-1_35.
C. Daskalakis and C. H. Papadimitriou. On a network generalization of the minmax theorem. In International
Colloquium on Automata, Languages, and Programming, pages 423-434. Springer, 2009b.
C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou. The complexity of computing a nash equilibrium. In J. M.
Kleinberg, editor, Proceedings of the 38th Annual ACM Symposium on Theory of Computing, Seattle, WA, USA, May
21-23, 2006, pages 71-78. ACM, 2006.
C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training gans with optimism. arXiv preprint arXiv:1711.00141,
2017.
G. Farina, C. Kroer, and T. Sandholm. Optimistic regret minimization for extensive-form games via di-
lated distance-generating functions.	In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/
b030afbb3a8af8fb0759241c97466ee4- Paper.pdf.
G. Farina, C. K. Ling, F. Fang, and T. Sandholm. Efficient regret minimization algorithm for extensive-form correlated
equilibrium. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6-Buc, E. B. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5187-5197, 2019b.
G. Farina, A. Celli, A. Marchesi, and N. Gatti. Simple uncoupled no-regret learning dynamics for extensive-form
correlated equilibrium. CoRR, abs/2104.01520, 2021. URL https://arxiv.org/abs/2104.01520.
Y. Gao, C. Kroer, and D. Goldfarb. Increasing iterate averaging for solving saddle-point problems. arXiv preprint
arXiv:1903.10646, 2019.
N. Golowich, S. Pattathil, and C. Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player
games. arXiv preprint arXiv:2010.13724, 2020.
E. Hazan. Introduction to online convex optimization. CoRR, abs/1909.05207, 2019. URL http://arxiv.org/
abs/1909.05207.
S. Hoda, A. Gilpin, J. Pena, and T. Sandholm. Smoothing techniques for computing nash equilibria of sequential games.
Math. Oper. Res., 35(2):494-512, 2010a.
S. Hoda, A. Gilpin, J. Pena, and T. Sandholm. Smoothing techniques for computing nash equilibria of sequential games.
Mathematics of Operations Research, 35(2):494-512, 2010b.
M. Jain, D. Korzhyk, O. Vanek, V. Conitzer, M. Pechoucek, and M. Tambe. A double oracle algorithm for zero-
sum security games on graphs. In The 10th International Conference on Autonomous Agents and Multiagent
Systems-Volume 1, pages 327-334, 2011.
M. J. Kearns, M. L. Littman, and S. P. Singh. Graphical models for game theory. In Proceedings of the 17th Conference
in Uncertainty in Artificial Intelligence, UAI ’01, page 253-260, San Francisco, CA, USA, 2001. Morgan Kaufmann
Publishers Inc. ISBN 1558608001.
D. Koller and N. Megiddo. The complexity of two-person zero-sum games in extensive form. Games and economic
behavior, 4(4):528-552, 1992.
C. Kroer, G. Farina, and T. Sandholm. Smoothing method for approximate extensive-form perfect equilibrium. arXiv
preprint arXiv:1705.09326, 2017.
11
C. Kroer, G. Farina, and T. Sandholm. Solving large sequential games with the excessive gap technique. In S. Bengio,
H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montreal, Canada, pages 872-882, 2018. URL https://proceedings.neurips.cc/
paper/2018/hash/e836d813fd184325132fca8edcdfb40e-Abstract.html.
A. Y. Kruger. Error bounds and metric subregularity. Optimization, 64(1):49-79, 2015.
Kuhn. Simplified two-person poker. Contributions to the Theory of Games, I:97-103, 1950a.
H. Kuhn. Extensive form games. Proceedings of National Academy of Science, pages 570-576, 1950b.
H. W. Kuhn and A. W. Tucker. Contributions to the Theory of Games, volume 2. Princeton University Press, 1953.
M. Lanctot, K. Waugh, M. Zinkevich, and M. H. Bowling. Monte carlo sampling for regret minimization in extensive
games. In Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural
Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009.
Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada, pages 1078-1086.
Curran Associates, Inc., 2009.
C. Lee, C. Kroer, and H. Luo. Last-iterate convergence in extensive-form games. CoRR, abs/2106.14326, 2021a. URL
https://arxiv.org/abs/2106.14326.
C.-W. Lee, C. Kroer, and H. Luo. Last-iterate convergence in extensive-form games. arXiv preprint arXiv:2106.14326,
2021b.
S.	Leonardos and G. Piliouras. Exploration-exploitation in multi-agent learning: Catastrophe theory meets game
theory. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative
Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial
Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 11263-11271. AAAI Press, 2021.
T.	Liang and J. Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial
networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 907-915. PMLR,
2019.
T. Lin, Z. Zhou, P. Mertikopoulos, and M. Jordan. Finite-time last-iterate convergence for multi-agent learning in games.
In International Conference on Machine Learning, pages 6161-6171. PMLR, 2020.
H. B. McMahan, G. J. Gordon, and A. Blum. Planning in the presence of cost functions controlled by an adversary. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 536-543, 2003.
P. Mertikopoulos, B. Lecouat, H. Zenati, C.-S. Foo, V. Chandrasekhar, and G. Piliouras. Optimistic mirror descent in
saddle-point problems: Going the extra (gradient) mile. arXiv preprint arXiv:1807.02629, 2018a.
P. Mertikopoulos, C. H. Papadimitriou, and G. Piliouras. Cycles in adversarial regularized learning. In A. Czumaj,
editor, Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2018, New
Orleans, LA, USA, January 7-10, 2018, pages 2703-2717. SIAM, 2018b.
M. MoravCik, M. Schmid, N. Burch, V Lisy, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling.
Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
D. Morrill, R. D’Orazio, R. Sarfati, M. Lanctot, J. R. Wright, A. R. Greenwald, and M. Bowling. Hindsight and
sequential rationality of correlated play. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,
Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on
Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 5584-5594.
AAAI Press, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16702.
J. Nash. Non-cooperative games. Annals of mathematics, pages 286-295, 1951.
12
G. Palaiopanos, I. Panageas, and G. Piliouras. Multiplicative weights update with constant step-size in congestion
games: Convergence, limit cycles and chaos. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus,
S. V. N. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages
5872-5882, 2017.
J. Perolat, R. Munos, J. Lespiau, S. Omidshafiei, M. Rowland, P. A. Ortega, N. Burch, T. W. Anthony, D. Balduzzi,
B. D. Vylder, G. Piliouras, M. Lanctot, and K. Tuyls. From Poincare recurrence to convergence in imperfect
information games: Finding equilibrium via regularization. In M. Meila and T. Zhang, editors, Proceedings of the
38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pages 8525-8535. PMLR, 2021.
J. Perolat, R. Munos, J.-B. Lespiau, S. Omidshafiei, M. Rowland, P. Ortega, N. Burch, T. Anthony, D. Balduzzi,
B. De Vylder, et al. From poincare recurrence to convergence in imperfect information games: Finding equilibrium
via regularization. In International Conference on Machine Learning, pages 8525-8535. PMLR, 2021.
G. Piliouras and J. S. Shamma. Optimization despite chaos: Convex relaxations to complex limit sets via poincare
recurrence. In C. Chekuri, editor, Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA 2014, Portland, Oregon, USA, January 5-7, 2014, pages 861-873. SIAM, 2014. doi: 10.1137/1.
9781611973402.64. URL https://doi.org/10.1137/1.9781611973402.64.
L.	D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical notes of the
Academy of Sciences of the USSR, 28(5):845-848, 1980.
A. Rakhlin and K. Sridharan. Optimization, learning, and games with predictable sequences. In C. J. C. Burges,
L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States, pages 3066-3074, 2013a.
A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Conference on Learning Theory, pages
993-1019. PMLR, 2013b.
M.	Rowland, S. Omidshafiei, K. Tuyls, J. Perolat, M. Valko, G. Piliouras, and R. Munos. Multiagent evaluation
under incomplete information. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche-Buc, E. B. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 12270-
12282, 2019.
R. Selten. Spieltheoretische behandlung eines OligoPolmodells mit nachfragetragheit: Teil i: Bestimmung des
dynamischen preisgleichgewichts. Zeitschrift fur die gesamte Staatswissenschaft/Journal of Institutional and
Theoretical Economics, pages 301-324, 1965.
Y. Shoham and K. Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical foundations. Cambridge
University Press, 2008.
O. Tammelin, N. Burch, M. Johanson, and M. Bowling. Solving heads-up limit texas hold’em. In Q. Yang and M. J.
Wooldridge, editors, Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,
IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pages 645-652. AAAI Press, 2015.
E. Vlatakis-Gkaragkounis, L. Flokas, and G. Piliouras. Poincare recurrence, cycles and spurious equilibria in gradient-
descent-ascent for non-convex non-concave zero-sum games. In H. M. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alche-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 10450-10461, 2019.
B.	Von Stengel. Efficient computation of behavior strategies. Games and Economic Behavior, 14(2):220-246, 1996.
B.	Von Stengel and F. Forges. Extensive-form correlated equilibrium: Definition and computational complexity.
Mathematics of Operations Research, 33(4):1002-1022, 2008.
13
C.	Wei, C. Lee, M. Zhang, and H. Luo. Linear last-iterate convergence in constrained saddle-point optimization.
In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net, 2021a. URL https://openreview.net/forum?id=dx11_7vm5_r.
C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Linear last-iterate convergence in constrained saddle-point optimization.
arXiv preprint arXiv:2006.09517, 2020.
C.-Y. Wei, C.-W. Lee, M. Zhang, and H. Luo. Last-iterate convergence of decentralized optimistic gradient de-
scent/ascent in infinite-horizon competitive markov games. arXiv preprint arXiv:2102.04540, 2021b.
M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with incomplete information.
In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2007/
file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf.
Appendix
A	Additional Related Work
The related works presented in Section 1 are primarily focused on research which is directly related to our topic of
study, namely network generalizations of zero-sum extensive form games. However, there is a large body of work
which studies many adjacent areas of interest.
Extensive Form Games. As elucidated in the main text, extensive form games are widely studied due to their
numerous applications. The problem of computing Nash Equilibria in extensive form games is of major interest,
with several works utilizing techniques such as CFR methods (Zinkevich et al., 2008), LP methods (Shoham and
Leyton-Brown, 2008) and double-oracle algorithms (McMahan et al., 2003; Jain et al., 2011). Of particular note is the
success of works utilizing CFR-based algorithms to study poker variants (Brown and Sandholm, 2018b; Bowling et al.,
2015; Moravcik et al., 2017). Two-player EFGs can be written in sequence form (as described in the main text), which
allows for them to be written as bilinear saddle-point problems. This connection allows for the design of algorithms
that utilize first order methods to achieve approximate convergence to the Nash (Kroer et al., 2017; Gao et al., 2019).
Online Learning in Games. In this paper we study the properties of a particular online learning algorithm, Optimistic
Gradient Ascent, for network zero-sum extensive form games. In normal form zero-sum games, recent results have
shown that algorithms such as Gradient Descent Ascent and Multiplicative Weights Update do not converge in the
last-iterate sense, even in the simplest of instances (Bailey and Piliouras, 2018; Vlatakis-Gkaragkounis et al., 2019). In
contrast, optimistic variants of these algorithms have been shown to be effective in guaranteeing last-iterate convergence
(Daskalakis et al., 2017; Daskalakis and Panageas, 2018a). As described in the main text, some of these results have
been extended to two-player extensive form games. Specifically, optimistic gradient descent and multiplicative weights
update, as well as the versions thereof with dilated regularizers, have been studied by Lee et al. (2021b) and Wei
et al. (2020) in the two-player setting. This line of research into extensive form games is not limited to discrete time
algorithms. Perolat et al. (2021) show that a continuous learning dynamic known as Follow the Regularized Leader
(FTRL) exhibits last-iterate convergence in monotone two-player zero-sum EFGs.
B Omitted Proofs
B.1	PROOF OF LEMMA 1
We first describe how a behavioral plan σi can be transformed to a vector xi(h) ∈ Xi. For any h ∈ Xi we let
xi(h) := Π(h,h0)∈P(h)∩Xiσi(h, αh0) where αh0 is the action α ∈ A(h) such that h0 = Next(h, α). We set xi(h) := 1
for all h ∈ H with Prev(h, i) = 0. Notice that by definition Uι(σ) = Pz∈z xι(z) ∙ Pι(z) ∙ x2(Z) = x> ∙ Ar ∙ x2
and respectively U2(σ) = Pz∈z χ2(z) ∙ P2(z) ∙ χι(z) = x> ∙ Ar ∙ xi.
14
Up next we show that all the constraints are satisfied. Consider the a state h ∈ Hi and the states h0 ∈ Next(h, α, i)
for some α ∈ A(h). Notice that for each h0 ∈ Next(h, α, i), xi(h0) = xi(h)σi(h, α). This implies that
Pα∈A(h) xi(Next(h, α, i)) = xi(h) since Pα∈A(h) σi(h,α) = 1.
Now let h1, h2 ∈ Hi where h1 ∈ Next(h01, α, i), h2 ∈ Next(h02, α, i) and I(h1) = I(h2). Consider the set
P(h1) ∩ Xi := {p1, . . . ,pk, h1} and P(h2) ∩ Xi := {q1, . . . , qk, h2}. Due to the perfect recall property, m = k and
I(p`) = I(q'). Thus, Xi(hι) = Xi(h2).
UP next We show how a vector Xi ∈ Xi can be converted to a behavioral plan σ% ∈ ∑i. Let σi(h, α) := xχ(h, for some
h0 ∈ Next(h, α, i). Notice that due the third constraint, xi(h0) = xi(h00) for all h0, h00 ∈ Next(h, α, i) and thus σ(h, α)
is well-defined. For h ∈ Hi let hα ∈ Next(h, α, i). By the third constraint we get that Pα∈A(h) σ(h, α) = 1. Finally
let hi, h2 ∈ Hi with I(hι) = I(h2) then σ(hι,α) = xi(h1) for some hi ∈ Next(hι,α, i) and σ(h2, α) = xi(h2) for
1 , 2 i	1	2	1 ,	xi(h1)	1	1 , ,	2 ,	xi(h2)
some h2 ∈ Next(h2, α, i). As a result, by the second constraint we get that σ(hi, α) = σ(h2, α) for all α ∈ A(h).
B.2	Proof of Lemma 2
We first describe how a behavioral plan σu ∈ Σu can be transformed to a vector xu(h) ∈ Xu. If there exists a game
Γuv with (u, V) ∈ E such that Prevr (h, U) = 0 we set Xu(h) := 1. Let us first verify that the above assignment
is valid i.e. if Prevr (h, U) = 0 for some (u, V) ∈ E then Prevr (h, U) = 0 for all (u, ν0) ∈ E. Notice that
Puv(h) ∩ Xu = {h} and thus by the second constraint of Definition 13, P uv0 (h) ∩ Xu = {h} for all (u, v0) ∈ E. Now
for the remaining nodes h ∈ Hu we select an arbitrary two-player EFG Γuv ((U, V) ∈ E) containing the state h and set
xu(h) := Π(h,h0)∈Puv(h)∩Xuσu(h, αh0) where αh0 is the action α ∈ A(h) such that h0 = Nextruv (h, α, U). We again
need to argue that xu(h) is independent of the arbitrary choice of the game Γuv. Let assume that state h also belongs in
the two-player EFG Γuv0 for some (U, V0) ∈ E. Again by the second constraint of Definition 11 we know that for the
sets Puv(h) ∩ Xu = {pi, . . . ,pk, h} and P uv0 (h) ∩ Xu = {qi, . . . , qm, h} the following holds:
1.	k = m.
2.	I(p`) = I(q') for all' ∈ {1,..., k}.
3.	p'+i ∈ Nextruv (p`, α, U) and q'+i ∈ Nextruv0 (g`, α, U) for some action α ∈ /(p`).
Since I(p`) = I(q') means that σu (p`, α) = σu(q', α) for all α ∈ A(p') = A(q'), we get that
Π(h,h0)∈Puv(h)∩Xuσu(h, αh0) = Π(h,h0)∈Puv0 (h)∩Xu σu(h, αh0)
Conversely, we show how a strategy in sequence form xu ∈ Xu can be converted to behavioral plan σu ∈ Σu . Given a
state h ∈ Hu we consider an edge (U, V) ∈ E such that Γuv containing h ∈ Hu and set
x (h0)	uv
σ(h, α):= ———— for some h0 ∈ Nextr (h, α, U)
xu(h)
We first need to show that this is a valid probability distribution, Ph∈A(h)σu(h, α) = 1. Since xu ∈ Xuruv, the second
constraint of Definition 7 ensures that
xu(Next(h, α, U)) = xu(h)
α∈A(h)
The latter implies that h∈A(h)σu(h, α) = 1.
We now need to establish that σ(h, ∙) is independent of the selection of the edge (u, v) ∈ E. Let h be a state of the game
Γuv0 for some (U, V0) ∈ E. By constraint 2 of Definition 13, for any h0 ∈ Nextruv (h, α, U) and h00 ∈ Nextruv (h, α, U)
we have that Xu(h0) = Xu(h00) and thus σ(h, α) = XUw.
Finally we need to argue that if hi, h2 ∈ Hu with I(hi) = I(h2), then σ(hi, α) = σ(h2, α) for all α ∈ A(hi) =
A(h2). Let σ(hι, α) = xu(h，for some h； ∈ Next(hι,α, U) and σ(hi, α) = xu(h，for some h ∈ Next(h2, α, u).
xu(h)	xu(h)
Then by Constraint 3 of Definition 11 we get that x(h0i) = x(h02) and thus σ(hi, α) = σ(h2, α).
15
B.3	Proof of Lemma 3
First, since Equations (6), (7) are defined on the product of treeplexes X, let us decompose the equations from the
perspective of an arbitrary agent u. Specifically, for some xU，U ∈ {1,..., n} it holds that the inner producthx, R∙xt-1i,
X ∈ X can be decomposed into inner products of the form hx, R ∙ xU-1i, where X is now in the individual treeplex Xu.
Moreover, by the definition of matrix R, we can substitute the following:
	R(U:h1 ),(v:h2 ) = —[A	]h1 h2
for all (u, v) ∈ E and 0 otherwise. Effectively, from the perspective of player u, the product of R and xt gives us
P(U v)∈E Au,v ∙ Xv∙ This gives us the following:
XU	= argminχ∈χu < η∕χ, - X	AUv ∙ XvT) + Dψ (x,xU);	⑻ (∖	v:(u,v)∈E	/ Z	X
Xt+1 XU	=argminχ∈χu <	η ∕χ, - X AUv	∙ Xv)	+ Dψ (χ,χU):	⑼ I	∖	v:(u,v)∈E	/
Finally, we can just take the negative of the terms inside the braces to obtain Equations (4), (5). Hence, for every
strategy vector X updated using Equations (6),(7), the constituent strategy vectors for each player u are exactly the same
as Equations (4), (5). Thus if the initial conditions X0 are the same, for all time t the collection of strategy vectors
{X1, . . . , XT} are the same between both formulations.
B.4	Proof of Lemma 4
Let X := (Xi,..., Xn) be an E-SymmetriC Nash Equilibrium. Now consider the vector x0 ∈ X defined as follows:
Xu0 = Xu0 for all u0 = U and XU is an arbitrary vector in Xu. By the definition of the E-SymmetriC Nash Equilibrium we
get that
X> ∙ R ∙ X — (x0)> ∙ R ∙ X ≤ E
NOtiCe that (X0)> ∙ R ∙ x= - Pv<u,v)∈E (XU)> ∙ AUv ∙ Xv- -Pu=U P5(u0,v)∈E X>0 ∙ Au0v ∙ Xv.Thus we get
— X (xU)> ∙ AUv ∙ Xv + X (Xu)> ∙ AUv ∙ Xv ≥ —E for all XU ∈ XU
v<U,v)∈E	v<U,v)∈E
Theorem 1 follows by repeating the same argument for all agents u ∈ V .
B.5 Proof of Lemma 5
We first prove a simpler version of Lemma 5 where X = y ∈ X.
Lemma 7. x> ∙ R ∙ x = 0 for all X ∈ X.
Proof. Consider a vector X ∈ X. To simplify notation let X := (X1, . . . , Xn) where each vector XU ∈ XU. Let σUx ∈ Σ
denote the behavioral plan for agent u constructed from the vector XU ∈ XU as described in Lemma 2. By the zero-sum
property of Definition 12, we get that
X X UUUv(σUx,σvx)=0
u∈ V v<U,v)∈E
By Lemma 2 We get that U"(σx) = Pv<U,v)∈E Uuv(σU, σvχ) = Pv<U,v)∈E x> ∙ AUv ∙ xv meaning that
X X	X> ∙ AUv ∙ Xv =0
u∈ V v<U,v)∈E
As a result, we get that x> ∙ R ∙ X = 0.	口
We will also utilize the following result:
16
Lemma 8. Consider a node u ∈ V and its neighbors Nu = {v1, v2, . . . , vk}. Let xu ∈ Xu represent a mixed strategy
for u and xv a mixed strategy of the neighbor v ∈ Nu. For any fixed collection {xv}v∈Nu the quantity
X χ> ∙ Avu ∙Xu + X χ> ∙ Auv ∙ Xv
v∈Nu	v∈Nu
remains constant over the range of xu.
Proof. For any vector x := (x1, . . . , xn) ∈ X, consider the vector x0 ∈ X such that x0v = xv for all v 6= u. By
Lemma 7 we get that
x> ∙ R ∙ x — (x0)> ∙ R ∙ x0 = 0
The latter directly implies that
X x>∙ Avu ∙ Xu + X x>∙ Auv ∙ Xv= X x>∙ Avu ∙ Xu + X (xu)>∙ Auv ∙ Xv
v∈Nu	v∈Nu	v∈Nu	v∈Nu
for all Xu,Xu ∈ X^.	□
Proof of Lemma 5. Consider vectors X, y ∈ X . Consider the vector y0 ∈ X such that yv = yv0 for all v 6= u. We first
show that
x> ∙ R ∙ y + y> ∙ R ∙ x = x> ∙ R ∙ y0 + (y0)> ∙ R ∙ x
Let Nu denote the neighbors of agent u ∈ V ,
xt ∙ R ∙ y + y> ∙ R ∙ x — xt ∙ R ∙ y0 — (y0)τ ∙ R ∙ x = ^X x> ∙ Avu ∙ yu + v∈Nu	E x> ∙ Auv ∙ yv v∈Nu
+ X y> ∙ Avu ∙ Xu + v∈Nu	X y> ∙ Auv ∙ Xv v∈Nu
-X χ> ∙ Avu ∙ yu- v∈Nu	X X> ∙ Auv ∙ yv v∈Nu
- X (yv)t ∙ Avu ∙ Xu v∈Nu	-X (yu)t ∙ Auv ∙ Xv v∈Nu
=X χ> ∙ Avu ∙ yu - v∈Nu	X X> ∙ Auv ∙ yv v∈Nu
—X> X> ∙ Avu ∙ yu — ∑(yu)>∙ Auv ∙ Xv
v∈Nu	v∈Nu
=0
where the last equality follows by Lemma 8. By gradually transforming vector y to vector X We get that x> ∙ R ∙ y +
y> ∙ R ∙ X = 2 ∙ x> ∙ R ∙ X = 0.	□
B.6 Proof of Lemma 6
Applying Lemma 1 of Rakhlin and Sridharan (2013b) to our setting, we obtain:
Lemma 9 (Rakhlin and Sridharan (2013b)). Let {Xt, Xt} be the sequences produced by Equations (6),(7). Then,
T	T	D2	1 T
E(Xt)> ∙ R ∙ Xt — minEXT ∙R∙Xt ≤ — + 2EkR∙Xt-R∙Xt-1k2
1T	1T
+2 X kXt - Xtk2 -而 X [kXt - Xtk2 + kXt - Xt+1k2
where D is the diameter of the treeplex polytope X .
17
Setting η = min{1∕(8 ∙ ∣∣R∣∣2), 1} in Lemma 9 We get that
TT X(χt)>∙ R ∙ χt— min ^X x> ∙ R ∙ xt ≤ t=1	x∈X t=1	D2	1 T	1 T ~η~+2 χ ∣∣r ∙X -R ∙ x- k - 4η ∙ χ [kχ -X k + kχ -X + k ]
≤	D2	1 T	T ~η~+2 X ∣∣r ∙χt -R ∙χt-11∣2 - 2kRk2 ∙ X [kχt - χtk2+kχt -χt+ι ∣∣2.
≤	竺 + 呼 X kχt-XtTk2-IRk2 ∙ X kχt - χt-1k2 η	2	t=1	t=1 D2
≤
一	η
Setting X = PT=I xs/T and using the fact that (χt)> ∙ R ∙ Xt = 0 we get minχ∈χ χ> ∙ R ∙ X ≥ - D TRk
B.7	Proof of Theorem 1
Let X the time-average vector produced by Equations (6),(7). By Lemma 6, we have
minχ∈χx>∙ R ∙ X ≥-Θ (DlRr )
Using the fact that X> ∙ R ∙ X = 0 we get that
X> ∙ R ∙ X ≤ minχ∈χx> ∙ R ∙ X + Θ (DkRl
meaning that (X, X) is a Θ (D TRk ) -approximate symmetric Nash Equilibrium of the symmetric game (R, R). By
Lemma 4 we get that X is a Θ (D TRk ) -approximate NE for the original network zero-sum EFG.
B.8	Proof of Theorem 3
First of all, in the proof of this theorem and in the lemmas presented within the proof, let X* := {x* ∈ X : minχ∈χ x> ∙
R ∙ X* = 0}, which describes the set of symmetric Nash Equilibria.
In order to establish Theorem 3, we follow the approach and notation of Wei et al. (2020), with minor modifications
along the way to apply the steps to our setting. Applying Lemma 1 of Wei et al. (2020)) to the Equations (6), (7) we get
the following lemma:
Lemma 10 (Wei et al. (2020)). Let {Xt, Xt}t≥ι be the sequence ofstrategy vectors produced by Equations (6), (7)for
η ≤ 1/8kRk2. Then,
15	1
η(R ∙ Xt)>(Xt - x) ≤ Dψ(X,Xt) - Dψ(X,Xt+1) - Dψ(Xt+1,Xt)-"Dψ(Xt,Xt) + "Dψ(Xt,Xt 1)
Since for OGD we have that Dψ (χ) = ɪ ∣∣x∣∣2, we can write the above inequality as:
2η(R ∙ xt)>(xt — x) ≤ IIXt — xk2 - kXt+1 — xk2 - kXt+1 — xtk2 — τf-kxt — Xtk2 + T^kXt — Xt-Ik2	(10)
16	16
To simplify notation let x* := Πχ* (Xt) ∈ X* meaning that x* is a symmetric Nash Equilibrium for the symmetric
game (R, R) and let us apply Equation 10 with X = X*. Now the LHS of Equation 10 takes the following form
2η(xt)> ∙ RT ∙ (xt — x*)	= —2η(xt)> ∙ RT ∙ x*	((xt)> ∙ RT ∙ xt = 0)
=—2η(x*)> ∙ R ∙ xt
= —2η(xt)> ∙ R ∙ x* (by Lemma 5)
≥0
18
where the last inequality follows by the fact that (x*, x*) is a symmetric Nash Equilibrium of the game (R, R). Since
the LHS of Equation 10 is greater or equal to 0 we get that,
kxt+1 - Πχ*(xt)k2 ≤ kxt 一Πχ*(xt)k2 - kxt+1 -*2 - 15kxt -xtk2 + ɪkxt —xt-1k2
16	16
By definition, the left hand side of the above is bounded below by dist2(xt+1, X*). Thus, We have the following
inequality,
dist2(xt+1, X*) ≤ dist2(xt, X*) — kxt+1 — xtk2 - 15kxt - xtk2 + ɪkxt - xt-1k2
16	16
(11)
Now, We define Θt := ∣∣xt 一 Πχ*(xt)k2 + 表kxt 一 xt-1k2 and ξt := ∣∣xt+1 一 xt∣2 + ∣∣xt 一 xt∣2 and rewrite
Equation 11 as follows,
Θt+1 ≤ Θt - 15ξt
一	16、
(12)
As in Wei et al. (2020), we now lower bound ξt by a quantity related to dist2(xt+1, X*) which will then give Us a
convergence rate for Θt . To do so we need to establish a property that is known as saddle-point metric subregularity
(Wei et al. (2020)).
Lemma 11. (Saddle-Point Metric Subregularity (SP-MS)) For any x, x0 ∈ X \ X*,
(R ∙ x)>(x — X0)
SUp ʌ~~H------Ti∣—— ≥ C ∙ kx ― nX*(X)k
χ0∈X	kx 一 x0k
for some game-dependent constant c > 0.
We present the proof of Lemma 11 in Section B.9. To this end, we remark that once the proof of Lemma 11 is
established, the proof of Theorem 3 follows by the analysis of Wei et al. (2020). For the sake of completeness, we
conclude the section with this analysis.
Lemma 12 (Wei et al. (2020)). If the parameter η in Equations (6), (7) is selected less than 1/8kRk2 then for any
t ≥ 0 and x0 ∈ X with x0 = xt+1,
kxt+1 - xtk2 + Ilxt - χtk2 ≥ 32η2
81
[(R ∙ Xt+1)>(xt+1 - x0)]j
kxt+1 - x0k2
where [a]+ := max{a, 0}, and similarly, for x0 6= xt+1
kxt+1 一 xt+1k2 + kxt - xt+1k2 ≥ 32η2
81
[(R ∙ xt+1)>(xt+1 - x0)]j
kxt+1 - x0k2
Now taking the telescoping sum of Equation 12 over t, we get:
Θ1 ≥ Θ1
一θt≥ 竺
16
T-1
Xξt
t=1
15X (kxt+1 - xtk2 + kxt -xtk2) ≥ 3∣Xkxt 一xjk2
t=1	t=2
≥
where the final inequality follows due to strong convexity of ɪkxk2. Now, since the rightmost term is a summation
of nonnegative terms and is upper bounded by a finite constant, we have that kxt-1 一 xt k → 0 as T → ∞. Thus, xt
converges to a point as T → ∞. In addition, due to Theorem 1, we know that the time-average value of the iterates
converge to a Nash Equilibrium. Combining these two observations, we can thus conclude that xt indeed converges to a
Nash Equilibrium in the last-iterate sense.
To show the explicit rate of convergence, we will require a few additional observations. First, note that the following
inequality holds for Equation 12:
kxt+1 — xtk2
≤ ξt ≤ 16θt ≤…≤ 16θ1
(13)
19
Then we have:
ξt ≥ 1 kxt+1 -χtk2 + 1(kxt+1 - χtk2 + kχt - xtk2)
≥ 1 kxt+1 - xtk2 + 16η^ sup
2	81 x0∈X
[(R ∙ Xt+1)>(Xt+1 - x0)]j
kxt+1 - x0k2
≥ 1 kxt+1-χtk2 + 16η2C2 kxt+1 - Πχ * (xt+1)k2
2	81
≥ min {1681C , 2} (kxt+1 -xtk2 + kxt+1 - πx*(Xt+1)k2)
= C2 Θt+1
(Lemma 12)
(SP-MS condition)
(Equation 13)
Now, we can show the explicit convergence rate as follows. Combining the above inequality with Equation 12, we
obtain
Θt+1 ≤ Θt -C2Θt+1	(14)
This immediately implies that Θt+1 ≤ (1 +C2)-1Θt. By iteratively expanding the right hand side of the inequality,
we can equivalently write:
Θt ≤ (1+C2)-t+1Θ1 ≤ 2Θ1(1+C2)-t	(15)
Next, notice that Θ1 is precisely dist2(X1, X*). Moreover, by using the triangle inequality, We Can write:
dist2(xt, X*) ≤ ∣∣xt - Πχ* (Xt+1)k2
≤ 2kxt+1 - Πχ* (Xt+1)k2 +2kxt+1 - xtk2
≤ 32Θt+1 ≤ 32Θt
Combining this observation with Equation 15 we get that
dist2(xt, X*) ≤ 64dist2(x1, X*)(1 + C2)-t
where C2 = min { 168^, 2 }, which completes the proof of Theorem 3.
B.9 Proof of Lemma 11
Lemma 11 follows easily from Lemma 13, the proof of which is presented in Section B.10.
Lemma 13. For any x ∈ X the following holds:
—min x0>R ∙ X ≥ C ∙∣∣x — Πχ*(x)k∙	(16)
for some game-dependent constant c ∈ (0, 1).
ProofofLemma 11. Consider the LHS of the inequality in Lemma 13 and note that 一 minχo∈χ x0>Rx = 0 if and only
if X ∈ X*.
20
Let D denote the diameter of X which is assumed to be finite. Then,
max
x0∈X
(R ∙ x)>(x — x0)
kx - x0k
≥ max -J-(R ∙ x)>(x — x0)
x0∈X D
=m max x>RV(X — x0)
D x0∈X
=m max[xτRτx — x> R>x0]
= m max[—xTRTx0]
D x0∈X
=—ɪ min xτRτx0
=—ɪ min x0TRx
c
≥D kx — ∏χ*(x)k
(xτ Rτ x = 0)
(Lemma 13)
□
B.10 Proof of Lemma 13
The proof of this lemma follows the basic steps in the proof of Theorem 5 in Wei et al. (2020), with some necessary
modifications. We remind the reader that for the purposes of the proof, we defined the set of symmetric Nash Equilibria
as X* = {x* : minχ∈χxτ ∙ R ∙ x* = 0}. The proof is split into several auxiliary lemmas/Claims, which can then be
combined to show the required result.
Lemma 14. The set X * is a polytope.
Proof. Let x* ∈ X* then mi∏χ∈χ xτ ∙ R ∙ x* = 0. Since X is a polytope the minimum value is attained in one of the
vertices of polytope X, the set of which is denoted by V(X). Thus
min xτ ∙ R ∙ x* = min vτ ∙ R ∙ x* = 0
x∈X	v∈V(X)
As a result, the set X* can be equivalently described as the set of vector x* ∈ X that additionally satisfy VT ∙ R ∙ x* ≥ 0
for all vertices Vi ∈ V(X).	□
Let us describe X in the following polytopal form:
X := {x : ατ ∙ x ≤ βi for i = 1,...L}
where L is a positive integer. Consider also the following polytopal form of the set X * as
X* := {x* ∈ X : cτ ∙ x* ≥ 0 for i = 1,..., K}
where Ci := VT ∙ R with Vi denoting the i-th vertex of polytope X and K denotes the number of different vertices.
Now fix a specific X ∈ X \X* and let x* := Πχ* (x). The vector x* satisfies some of the polytopal constraints with
equality. These constraints are called tight, and without loss of generality we can assume that
•	ατ ∙ x* = βi for i = 1,...,'
•	cτ ∙ x* = 0 for i = 1,...,k
Lemma 15. The vector X ∈ X violates at least one tight Constraint oftheform {cτ ∙ X = 0 for i = 1,...,k}.
Proof. Let assume that {cτ ∙ X = 0 for i = 1,...,k}. Since x ∈ X* there exists at least one vertex V ∈ V(X) such that
vτ ∙ R ∙ x < 0. The latter implies that there exits x0 ∈ X lying in line segment between X and x* such that VT ∙ R ∙ X ≥ 0
for all vertices V ∈ V(X). The latter implies that x0 ∈ X* which contradicts with the fact that x* = Πχ* (x).	□
21
Now, note that the normal cone of X * at x* is
NX* = {x0 — x* : x* = Πχ* (x0)}
From a standard result in linear programming literature (Wei et al., 2020), we know that the normal cone can be written
in the following form:
i ∙ a% + X qi ∙ Ci for some pi, qi ≥ θ}
Again, following the steps of Wei et al. (2020), we have the following claim:
Claim 1. For any x ∈ X such that x* = Πx∈X * (x) the vector x — x* belongs in the set
k
a + Eqi ∙ Ci
i=1
Pi,qi ≥ 0,α> ∙ (XPi ∙ ai + Xqi ∙ Ci) ≤ θ}
Proof. As mentioned previously, we know that x — x* belongs in the normal cone of x*, Nx* . Thus it can be expressed
as p'=ι pi ∙ ai + Pk=I qi ∙ Ci with Pi, qi ≥ 0. As such, We need only additionally show that X — x* satisfies the
following:
ai> (x — x*) ≤ 0, ∀i ∈ 1, . . . , `
Notice that for all i = 1, . . . , `, we have:
ai> (x — x*) = (ai>x* — bi) + ai> (x — x*)
= ai> (x* + x — x*) — bi
= ai> x — bi ≤ 0
(i-th constraint is tight at x*)
(x ∈ X)
□
Claim 2. X — x* can be written as P'=ιPi ∙ a + Pik=i qi ∙ Ci with 0 ≤ Pi, qi ≤ C0∣∣x — x*k for all i and some
problem-dependent constant C0 < ∞.
Proof. Note that ∣∣χ-χ*k ∈ Mx* because 0 = X — x* ∈ Mx* and Mx* is a cone. Furthermore, ∣∣χ-χ*k ∈ {v ∈ RM :
∣∣v∣∣∞ ≤ 1}. Thus, ∣∣χ-χ*k ∈ Mχ* ∩{v ∈ RM : ∣∣v∣∣∞ ≤ 1}, which is a bounded subset of the cone Mχ*.
We will argue that there exists large enough C0 > 0 such that:
k
Pi ∙ αi + Eqi ∙ Ci : 0 ≤ Pi,qi ≤ C0, ∀i
i=1
First note that P is a polytope. For every vertex v of P, the smallest C0 such that V belongs to the left-hand side set
above is the solution to the following linear program:
⊇Mx* ∩{v∈RM : kvk∞ ≤ 1} :=P.
mm	Cv
pi,qi ,CV
`k
s.t. V = EPi ∙ αi + Eqi ∙ Ci, 0 ≤ Pi,qi ≤ Cv.
i=1	i=1
Since V ∈ Mχ*, this LP is always feasible and admits a finite solution Cv < ∞. Now, let C0 = maxv∈ν(p) where
V(P) is the set of all vertices of P. Then, since any V ∈ P can be expressed as a convex combination of points in V(P),
v can thus be expressed as P'=ι Pi ∙ ai + Pk=I qi ∙ Ci where 0 ≤ Pi, qi ≤ C0. As a result, ∣∣χ-χ*k can be written as
P'=ι Pi ∙ ai + Pik= ι qi ∙ Ci where 0 ≤ Pi, qi ≤ C0, so it follows that X — x* can be written as: P'=ι Pi ∙ ai + Pk=I qi ∙ Ci
where 0 ≤ Pi, qi ≤ C0kx — x* k.
□
22
Now, again following Wei et al. (2020), we can piece together all of the auxiliary results to show Lemma 13. Define
Ai := α>(X - x*) and Ci := c>(x - x*). By Claim 2, We can write X - x* as P'=ιPi ∙ α% + Pk=I qi ∙ Ci where
0 ≤ Pi, qi ≤ C0kx — x*k. Thus：
`	k	`	k>
XPi ∙ Ai	+ X qi	.	Ci	= (XPi	∙	αi	+ X qi	∙	Ci)	(x	- x*) = kx - x*∣∣2
Moreover, since X - x* ∈ Mχ* by Claim 1, We have
``
EPi ∙ Ai = EPi ∙ αi ≤ 0
i=1	i=1
and
qi	∙ Ci	≤ ( max	Ci]	qi	≤ ( max Ci]	kC0kx	— x*k
i=1 i i i∈{1,...,k} i i=1 i i∈{1,...,k} i
The first inequality follows because Pi
Lemma 15) and 0 ≤ qi ≤ C0kx - x* k.
≥ 0. The second inequality follows because maxi∈{1,...,k} Ci > 0 (by
Combining the above, we obtain:
i∈maxk} Ci ≥ 记k"-" k
Now, note that:
max	Ci =	max	(Ci>x -	di)	≤ max	(Ci>x	-	di)	= max(x0>Rx)
i∈{1,...,k}	i∈{1,...,k} i	i∈{1,...,|V(X)|} i	x0∈X
where the last equality follows from the formulation of problem constraints in the proof of Lemma 14. Finally, by
combining the last two statements, we can conclude that
-min(x0>Rx) ≥ —ɪ-kx - x*k.
χ0∈x'	7 - kC0"	11
Here k and C0 only depend on the set of tight constraints at x*. There are only finitely many sets of tight constraints, so
there exists a constant C > 0 such that - minχo∈χ(x0>Rx) ≥ 击∣∣x - x*k holds for all X and x*, completing the
proof.
C Additional Experimental Details
In this section we provide more details about our experimental results from Section 5.
Random Network Extensive Form Games. In our simulations, we first generated random zero-sum extensive form
games on both a 3-node graph where every player plays against the other two players, as well as a dense 4-node graph
(shown in Figure 2). Specifically, each game is characterized by a 3 × 3 symmetric matrix which represents the sequence
form of an extensive form game written as a matrix. For each run of the simulation, we first create the games which are
to be played. Then, we optimize for the choice of stepsize η, selecting the value that gives the fastest convergence rate
to the Nash Equilibrium. In the plots, in order to reduce visual clutter we present the squared distance from the Nash
for only one of the players. In addition, in order to more clearly show the fast rate of convergence, we compute the
logarithm of dist2(xt, X*) in the plots. It is worth noting that the 4-node graph takes significantly longer to arrive at
the last iterate compared to the 3-node graph.
Kuhn Poker. Kuhn poker is a simplified version of poker proposed by Kuhn (1950a). The deck contains only three
cards, namely Jack, Queen and King. Each player is dealt one card, and the third is left unseen. Player 1 can either
check or bet, and subsequently Player 2 can also either check or bet. Finally, if Player 1 checks in round 1 and Player 2
bets in round 2, Player 1 gets another round to fold or call. Eventually, the player with the highest card wins the pot. In
the sequence form representation of the game, Kuhn poker has dimension |X | × |X | = 13 × 13 and the corresponding
payoff matrix can be easily computed by hand. For the simulation we show in Figure 1, we run an experiment with 5
players on a graph where each player plays in exactly two Kuhn poker games with randomized initial conditions. This
limitation was set in order to reduce the convergence time, since empirically we observe that increasing the number of
players greatly increases the convergence times.
23
G1
Figure 2: 4-node graph for randomized EFGs. Each node represents a player and each edge represents a game Gi
between the corresponding players.
Fold Call
Fold Call
Fold Call
Figure 3: Extensive form representation of Kuhn poker from the perspective of one player. The blue nodes represent
decision points for the player, green nodes represent observation points (either the player observes their card or the
other player takes an action) and finally the black nodes denote the terminal states of the game.
24
Time Average Convergence. In the main text we show figures that exhibit last iterate convergence. Our theoretical
results also guarantee time-average convergence to the Nash Equilibrium set (Theorem 1). In Figure 4 we show
empirical evidence for time-average convergence in each of the simulations performed. In the plots, we take the
difference between the cumulative averages of the strategy probabilities and the time-average value calculated from the
data in order to centre the oscillations at 0. Note that in general, the time required to converge to the Nash is much
faster compared to the last-iterate convergence times.
Figure 4: Time average convergence of all strategies in OGA simulations. (Left) 20-node Matching Pennies game;
(Right) 4-node random extensive form game; (Bottom) 5-node Kuhn poker game.
A note on scaling. An empirical observation from our simulations is that the number of nodes in the network as well
as the sparsity of the graph plays a major role in convergence times, particularly the last-iterate convergence times.
This intuitive observation presents an interesting challenge when modeling truly large-scale problems. For instance, a
setting such as Texas Hold’em poker admits a huge number of parameters (of order 1018). Even in the two-player case
this is prohibitively large, and this issue is compounded if we are in the multiplayer setting. As an illustrative example,
consider a network game where every agent plays the ubiquitous zero-sum game, Matching Pennies, against two other
players. Figure 5 shows that the convergence times drastically increase when we go from a 4-node graph to a 20-node
graph. Similarly, in our experiments with extensive form games in sequence form, it becomes difficult to simulate
larger games (such as Leduc poker, which has dimension |X | × |X | = 337) once there are multiple players playing in
several games. This is a practical limitation which represents an interesting divide between our theoretical results and
the reality of many large-scale, real world games. It is certainly a fascinating research direction to find ways to bridge
this gap in future research.
25
----Player 1
----Player 2
----Player 3
----Player 4
—Player 5
----Player 6
----Player 7
----Player 8
----Player 9
----Player 10
—Player 11
----Player 12
----Player 13
----Player 14
----Player 15
----Player 16
—Player 17
----Player 18
----Player 19
----Player 20
Figure 5: Simulations using OGA in network Matching Pennies games. (Left) Convergence times for 4-player game;
(Right) Convergence times for 20-player game.
26