Under review as a conference paper at ICLR 2022
Sample Complexity of Offline Reinforcement
Learning with Deep ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
Offline reinforcement learning (RL) leverages previously collected data for policy
optimization without any further active exploration. Despite the recent interest in
this problem, its theoretical foundations in neural network function approxima-
tion setting remain limited. In this paper, we study the statistical theory of offline
RL with deep ReLU network function approximation. In particular, we establish
the sample complexity of O (κ1+"α ∙ e-2-2d/a) for offline RL With deep ReLU
networks, where κ is a measure of distributional shift, d is the dimension of the
state-action space, α is a (possibly fractional) smoothness parameter of the under-
lying Markov decision process (MDP), and e is a user-specified error. Notably,
our sample complexity holds under tWo novel considerations, namely the Besov
dynamic closure and the correlated structure that arises from value regression for
offline RL. While the Besov dynamic closure generalizes the dynamic conditions
for offline RL in the prior Works, the correlated structure renders the existing anal-
yses improper or inefficient. To our knoWledge, our Work is the first to provide
such a comprehensive analysis for offline RL With deep ReLU netWork function
approximation.
1	Introduction
Offline reinforcement learning (Lange et al., 2012; Levine et al., 2020) is a practical paradigm of
reinforcement learning (RL) Where logged experiences are abundant but a neW interaction With the
environment is limited or even prohibited. The fundamental offline RL problems concern With hoW
Well previous experiences could be used to evaluate a neW target policy, knoWn as off-policy eval-
uation (OPE) problem, or to learn the optimal policy, knoWn as off-policy learning (OPL) problem.
We study these offline RL problems With infinitely large state spaces, Where the agent must rely
on function approximation such as deep neural netWorks to generalize across states from an offline
dataset Without any further exploration. Such problems form the core of modern RL in practical
settings (Levine et al., 2020; Kumar et al., 2020; Singh et al., 2020), but no Work has provided a
comprehensive and adequate analysis of the statistical efficiency for offline RL With neural netWork
function approximation.
On the theoretical side, predominant sample-efficient results in offline RL focus on tabular environ-
ments With small finite state spaces (Yin & Wang, 2020; Yin et al., 2021; Yin & Wang, 2021), but
as these methods scale With the number of states, they are infeasible for the settings With infinitely
large state spaces. While this tabular setting has been extended to large state spaces via linear en-
vironments (Duan & Wang, 2020; Tran-The et al., 2021), the linearity assumption often does not
hold for many RL problems in practice. Theoretical guarantees for offline RL With general and
deep neural netWork function approximations have also been derived, but these results are either
inadequate or relatively disconnected from practical settings. In particular, While the finite-sample
results for offline RL with general function approximation (Munos & Szepesvari, 2008; Le et al.,
2019) depend on an inherent Bellman error Which could be large or uncontrollable in practice, other
analyses (Yang et al., 2019) rely on an inefficient data splitting technique to deal with the highly
correlated structures arisen in value regression for offline RL and use a relatively strong dynamic
assumption. It therefore remains unclear whether offline RL can provably work in a more general
dynamic condition and the highly correlated structure of value regression.
1
Under review as a conference paper at ICLR 2022
In this paper, we provide a statistical theory of both OPE and OPL with neural network function
approximation in a broad generality. In particular, our contributions are:
•	First, we achieve a generality for the guarantees of offline RL with neural network function
approximation via two novel considerations: (i) we introduce a new structural condition
namely Besov dynamic closure which generalizes the existing dynamic conditions for of-
fline RL with neural network function approximation and even includes MDPs that need
not be continuous, differentiable or spatially homogeneous in smoothness; (ii) we take into
account the highly correlated structure of the value estimate produced by a regression-based
algorithm from the offline data. This correlated structure plays a central role in the statis-
tical efficiency of an offline algorithm but the prior results (Munos & Szepesvari, 2008;
Le et al., 2019; Yang et al., 2019) improperly ignore this structure or avoid it using an
inefficient data splitting approach.
•	Second, we prove that an offline RL algorithm based on fitted-Q iteration (FQI) can achieve
the sample complexity of O (κ1+d∕ɑ ∙ e-2-2d∕a) where K measures the distributional shift
in the offline data, d is the input dimension, α is a smoothness parameter of the underlying
MDP, and is a user-specified error. Notably, our guarantee holds under a general condition
encompassing the dynamic conditions in the existing works while it does not require any
data splitting as in (Yang et al., 2019). The data splitting approach splits the offline data
into K disjoint folds where K is the number of iterations in their algorithm. As the sample
complexity of such data splitting scales linearly with K where K can be arbitrarily large in
practice, the guarantee in (Yang et al., 2019) is highly inefficient for offline RL. Moreover,
our analysis also improves upon the analysis in (Le et al., 2019) that incorrectly ignores the
correlated structure of offline value estimate.
Notation. Let Lp(X,μ) = {f : X → R | kfkp,μ := (RX ∣f∣pdμ)"p < ∞} be the space of
measurable functions for which the p-th power of the absolute value is μ-measurable, C0(X)=
{f : X → R | f is continuous and kf k∞ < ∞} be the space of bounded continuous functions,
Ca(X) be the Holder space with smoothness parameter α ∈ (0, ∞)∖N, Wpm(X) be the Sobolev
space with regularity m ∈ N and parameter p ∈ [1, ∞], and X ,→ Y be continuous embedding from
a metric space X to a metric space Y. Denote by P(Ω) the set of probability measures supported in
domain Ω. For simplicity, We use ∣∣ ∙ ∣∣μ for ∣∣ ∙ ∣∣p,μ when P = 2. Denote by ∣∣ ∙ ∣∣o the 0-norm, i.e.,
the number of non-zero elements, and a ∨ b = max{a, b}. For any two real-valued functions f and
g, We write f (∙) . g(∙) if there is an absolute constant C independent of the function parameters (∙)
such that f(∙) ≤ c ∙ g(∙). We write f(∙) X g(∙) if f(∙) . g(∙) and g(∙) . f(∙). We write f(∙) ` g(∙)
if there exists an absolute constant C such that f (•) = C ∙ g(∙).
2	Related Work
The majority of the theoretical results for offline RL focus on tabular settings and mostly on OPE
task where the state space is finite and an importance sampling -related approach is possible (Precup
et al., 2000; Dudlk et al., 2011; Jiang & Li, 2015; Thomas & Brunskill, 2016; Farajtabar et al., 2018;
Kallus & Uehara, 2019). The main drawback of the importance sampling-based approach is that
it suffers high variance in long horizon problems. The high variance problem is later mitigated by
the idea of formulating the OPE problem as a density ratio estimation problem (Liu et al., 2018;
Nachum et al., 2019a; Zhang et al., 2020a;b; Nachum et al., 2019b) but these results do not provide
sample complexity guarantees. The sample-efficient guarantees for offline RL are obtained in tabular
settings in (Xie et al., 2019; Yin & Wang, 2020; Yin et al., 2021; Yin & Wang, 2021). Jiang &Li
(2016) derive Cramer-Rao lower bound for discrete-tree MDPs.
For the function approximation setting, as the state space of MDPs is often infinite or continuous,
some form of function approximation is deployed in approximate dynamic programming such as
fitted Q-iteration, least squared policy iteration (Bertsekas & Tsitsiklis, 1995; Jong & Stone, 2007;
Lagoudakis & Parr, 2003; GrUneWaIder et al., 2012; Munos, 2003; Munos & Szepesvari, 2008; An-
tos et al., 2008; Tosatto et al., 2017), and fitted Q-evaluation (FQE) (Le et al., 2019). A recent
line of work studies offline RL in non-linear function approximation (e.g, general function approx-
imation and deep neural network function approximation) (Le et al., 2019; Yang et al., 2019). In
particular, Le et al. (2019) provide an error bound of OPE and OPL with general function approxi-
2
Under review as a conference paper at ICLR 2022
mation but they ignore the correlated structure in the FQI-type algorithm, resulting in an improper
analysis. Moreover, their error bounds depend on the inherent Bellman error that can be large and
uncontrollable in practical settings. More closely related to our work is (Yang et al., 2019) which
also considers deep neural network approximation. In particular, Yang et al. (2019) focused on an-
alyzing deep Q-learning using a disjoint fold of offline data for each iteration. Such approach is
considerably sample-inefficient for offline RL as their sample complexity linearly scales with the
number of iterations K which is very large in practice. In addition, they rely on a relatively re-
stricted smoothness assumption of the underlying MDPs that hinders their results from being widely
applicable in more practical settings.
Since the initial version of this paper appeared, a concurrent work studies offline RL with general
function approximation via local Rademacher complexities (Duan et al., 2021). While both papers
independently have the same idea of using local Rademacher complexities as a tool to study sample
complexities in offline RL, our work differs from (Duan et al., 2021) in three main aspects. First, we
focus on infinite-horizon MDPs while (Duan et al., 2021) work in finite-horizon MDPs. Second, we
derive an explicit sample complexity while the sample complexity in (Duan et al., 2021) depends
on the critical radius of local Rademacher complexity. Bounding the critical radius for a complex
model under the correlated structure is highly non-trivial. Duan et al. (2021) provided the specialized
sample complexity for finite classes, linear classes, kernel spaces and sparse linear spaces but it is
unclear how their result can apply to more complex models such as a deep ReLU network. Moreover,
we propose a new Besov dynamic closure and establish the sample compelxity using a uniform
convergence argument which appear absent in Duan et al. (2021).
3	Preliminaries
We consider reinforcement learning in an infinite-horizon discounted Markov decision process
(MDP) with possibly infinitely large state space S, continuous action space A, initial state distribu-
tion ρ ∈ P(S), transition operator P : S × A → P(S), reward distribution R : S × A → P([0, 1]),
and a discount factor γ ∈ [0, 1). For notational simplicity, we assume that X := S × A ⊆ [0, 1]d
but our results readily generalizes to the case when A is finite.
A policy π : S → P(A) induces a distribution over the action space conditioned on states. The Q-
value function for policy π at state-action pair (s, a), denoted by Qπ(s, a) ∈ [0, 1], is the expected
discounted total reward the policy collects if it initially starts in the state-action pair,
Qπ(s,a):= En
∞
EY trt∣so = s,ao = a
=0
where r ~ R(st,at),at ~ π(∙∣st), and St ~ P(∙∣st-ι,αt-ι). The value for a policy π is Vn =
Es~ρ,a~∏(∙∣s) [Qn (s, a)], and the optimal value is V * = max∏ Vn where the maximization is taken
over all stationary policies. Alternatively, the optimal value V * can be obtained via the optimal
Q-function Q* = max∏ Qn as V * = Es~ρ [max。Q*(s, a)]. Denote by Tn and T * the Bellman
operator and the optimality Bellman operator, respectively, i.e., for any f : S × A → R
[T f ] (S, a) = Er~R(s,a) [r] + YEs0~P(∙∣s,a),a0~π(∙∣s0) [f (S , a )]
[Tf](s,a) = Er~R(s,a)[r]+ γEs0~p(∙∣s,a) [maXf(s0,a0)],
we have TπQn = Qn and T*Q* = Q*.
We consider the offline RL setting where a learner cannot explore the environment but has access
to a fixed logged data D = {(Si, ai, S0i,ri)}in=1 collected a priori by certain behaviour policy η. For
simplicity, we assume that {Si}in=1 are independent and η is stationary. Equivalently, {(Si, ai)}in=1
are i.i.d. samples from the normalized discounted stationary distribution over state-actions with
respect to η, i.e., (si,αi) i'i.d' μ(∙, ∙) := (1 - Y) P∞=0 YtP(St = ∙,at = ∙∣ρ, η) where Si 〜
P(∙∣Si,αi) and a% 〜η(∙∣Si). This assumption is relatively standard in the offline RL setting (Munos
& Szepesvari, 2008; Chen & Jiang, 2019; Yang et al., 2019) and is used merely for the sake of
theoretical analysis. The goals of OPE and OPL are to estimate Vn and V *, respectively from D.
The performance of OPE and OPL estimates are measured via sub-optimality gaps.
3
Under review as a conference paper at ICLR 2022
For OPE. Given a fixed target policy ∏, for any value estimate V computed from the offline data D,
the sub-optimality of OPE is defined as
.^ . _ ^.
SUbOPt(V; π) = ∣Vπ - V|.
For OPL. For any estimate ∏ of the optimal policy ∏* that is learned from the offline data D, We
define the sup-optimality of OPL as
SUbOPt(∏) = EP [V*(s) - Q*(s,∏(s))],
where EP is the expectation with respect to (w.r.t.) S 〜ρ.
3.1	Deep ReLU Networks as Function Approximation
In practice, the state space is often very large and complex, and thus function approximation is
required to ensure generalization across different states. Deep networks with the ReLU activation
offer a rich class of parameterized functions with differentiable parameters. Deep ReLU networks
are state-of-the-art in many applications, e.g., (Krizhevsky et al., 2012; Mnih et al., 2015), including
offline RL with deep ReLU networks that can yield superior empirical performance (Voloshin et al.,
2019). In this section, we describe the architecture of deep ReLU networks and the associated
function space which we use throughout this paper. Specifically, a L-height, m-width ReLU network
on Rd takes the form of
fθL,m(x) = W(L)σ W(L-1)σ .. .σ W(1)σ(x) + b(1) .. . + b(L-1) +b(L),
where W(L) ∈ R1×m, b(L) ∈ R, W(1) ∈ Rm×d, b(1) ∈ Rm, W(l) ∈ Rm×m, b(l) ∈ Rm,∀1 < l <
L, θ = {W(l), b(l)}1≤l≤L, and σ(x) = max{x, 0} is the (element-wise) ReLU activation. We define
Φ(L, m, S, B) as the space of L-height, m-width ReLU functions fθL,m(x) with sparsity constraint
S, and norm constraint B, i.e., PlL=1(kW (l)k0 +kb(l)k0) ≤ S, max1≤l≤L kW(l)k∞∨kb(l)k∞ ≤ B.
Finally, for some L, m ∈ N and S, B ∈ (0, ∞), we define the unit ball of ReLU network function
space FNN as
FNN := f ∈ Φ(L, m, S, B) : kfk∞ ≤ 1 .
We further write FNN(X) to emphasize the domain X of deep ReLU functions in FNN but often
use FNN when the domain context is clear. The main benefit of deep ReLU networks is that in
standard non-parametric regression, they outperform any non-adaptive linear estimator due to their
higher adaptivity to spatial inhomogeneity (Suzuki, 2018).
3.2	Regularity
In this section, we define a function space for the target functions for which we study offline RL.
Note that a regularity assumption on the target function is necessary to obtain a nontrivial rate of
convergence (GyOrfi et al., 2002). A common way to measure regularity of a function is through
the Lp-norm of its local oscillations (e.g., of its derivatives if they exist). This regularity notion
encompasses the classical Lipschitz, Holder and Sobolev spaces. In particular in this work, we
consider Besov spaces. Besov spaces allow fractional smoothness that describes the regularity of a
function more precisely and generalizes the previous smoothness notions. There are several ways
to characterize the smoothness in Besov spaces. Here, we pursue a characterization via moduli of
smoothness as it is more intuitive, following (Gine & Nickl, 2016).
Definition 3.1 (Moduli of smoothness). For a function f ∈ Lp(X) for some p ∈ [1, ∞], we define
its r-th modulus of smoothness as
ωrt,p(f) := sup k∆rh(f)kp,t>0,r∈N,
0≤h≤t
where the r-th order translation-difference operator ∆rh = ∆h ◦ ∆rh-1 is recursively defined as
∆h(f )(∙) ：= (f(∙ + h)- f (∙))r = XX (k) (-1)r-kf (∙ + k ∙ h).
4
Under review as a conference paper at ICLR 2022
Remark 3.1. The quantity ∆rh(f) captures the local oscillation of function f which is not necessarily
differentiable. In the case the r-th order weak derivative Dr f exists and is locally integrable, we
have
lim △"?(X) = Drf(X), ωr-(fl ≤ ∣∣Drfk and ωr+r0(f) ≤ ω^(Drf).
h→0	hr	,	tr	p	tr	r0
Definition 3.2 (Besov SPaCe Bp,q(X)). For 1 ≤ p,q ≤ ∞ and α > 0, we define the norm ∣∣ ∙ ∣∣Bα
of the Besov space B^(X) as IIfIlBa,q ：= IIfIlp + |力br where
W°∞(一嗯厂 1 ≤ q< ∞,
1	ωtac+ι(f)	_
supt>o T⅛-,	q = ∞,
is the Besov seminorm. Then, Bpα,q ：= {f ∈ Lp(X )： IfIBα < ∞}.
Intuitively, the BesoV seminorm |力Ba roughly describes the Lq-norm of the lp-norm of the a-
p,q
order smoothness of f. Having defined Besov spaces, a natural question is what properties Besov
spaces have and how these spaces are related to other function spaces considered in the current
literature of offline RL? It turns out that Besov spaces are considerably general that encompass
Holder spaces and Sobolev spaces as well as functions with spatially inhomogeneous smoothness
(Triebel,1983; Sawano, 2018; Suzuki, 2018; Cohen, 2009; Nickl & POtSCher, 2007). We summarize
the key intriguing characteristics of Besov spaces and their relation with other spaces:
•	(Monotonicity in	q)	For 1 ≤ p ≤ ∞, 1 ≤	q1	≤	q2	≤ ∞ and α ∈ R,	Bpα,q	(X	) ,→
Bpα,q2 (X );	,
•	(With Lp spaces) L2(X ),→ B20,2(X ), Bp0,1(X ),→ Lp(X ),→ Bp0,∞(X )for 1 ≤ p ≤ ∞,
and Bpα,q (X ),→ Lr(X )for α > d(1/p - 1/r)+ where r = bαc + 1;
•	(WithC0(X ))Bpα,q(X ),→ C0(X )for 1 ≤ p, q ≤ ∞, α > d/p;
•	(With Sobolev spaces) B2m,2(X )= W2m(X )for m ∈ N;
•	(With Holder spaces) B∞,∞(X) = Cα(X) for α = (0, ∞)∖N.
In particular, the Besov space Bp,q reduces into the Holder space Ca when P = q = ∞ and a is
positive and non-integer while it reduces into the Sobolev space W2α when p = q = 2 and α is a
positive integer. We further consider the unit ball of Bpα,q(X ):
Bα,q(X) ：= {g ∈ Bp,q ： kgkB?,q ≤ 1 and kg∣∞ ≤ 1}.
To obtain a non-trivial guarantee, certain assumptions on the distribution shift and the MDP reg-
ularity are necessary. Here, we introduce such assumptions. The first assumption is a common
restriction that quantifies the distribution shift in offline RL.
Assumption 3.1 (Concentration coefficient). There exists κ* < ∞ such that k dν k∞ ≤ κμ for any
realizable distribution ν, where a distribution ν is said to be realizable if there exist t ≥ 0 and policy
∏ such that V(s, a) = P(St = s, at = a∣sι 〜ρ, ∏), ∀s, a.
Intuitively, the finite κ* in Assumption 3.1 asserts that the sampling distribution μ is not too far away
from any realizable distribution uniformly over the state-action space. κμ is finite for a reasonably
large class of MDPs, e.g., for any finite MDP, any MDP with bounded transition kernel density,
and equivalently any MDP whose top-Lyapunov exponent is negative (Munos & Szepesvari, 2008).
Chen & Jiang (2019) further provided natural problems with rich observations generated from hid-
den states that has low concentration coefficients. These suggest that low concentration coefficients
can be found in fairly many interesting problems in practice. We present a simple (though stronger
than necessary) example for which Assumption 3.1 holds.
Example 3.1. If the transition density P (s0 |s, a)is sufficiently stochastic and the behaviour policy ν
has a sufficient uniform coverage over the action space, i.e., there exist absolute constants c1, c2 > 0
such that for any s, s0 ∈ S, there exists an action a ∈ A such that P(s0∣s, a) ≥ 1/ci and η(a∣s) ≥
1/c2, ∀s, a, then we can choose Kμ = cq.
5
Under review as a conference paper at ICLR 2022
Next, we introduce a completeness assumption.
Assumption 3.2 (BeSOV dynamic closure). Vf ∈ FNN(X),∀∏,Tπf ∈ B，,q(X) for some p,q ∈
[1, ∞] and α > p∧2.
Assumption 3.2 signifies that for any policy π, the Bellman operator Tπ applied on any ReLU net-
work function in FNN(X) results in a BesoV function in Ba4(X). Moreover, as Tπf f = T*f
where ∏f is the greedy policy w.r.t. f, Assumption 3.2 also implies that T*f ∈ Ba国(X) if
f ∈ FNN(X). This kind of completeness assumption is relatively standard and common in the
offline RL literature (Chen & Jiang, 2019); yet our Besov dynamic closure is sufficiently general
that encompasses almost all the previous completeness assumptions in the literature. For example,
a simple (yet considerably stronger than necessary) sufficient condition for Assumption 3.2 is that
the expected reward function r(s, a) and the transition density P(s0|s, a) for each fixed s0 are the
functions in the Besov space Bpa,q (X), regardless of any function approximator f and any policy
π. Such a condition on the transition dynamic is common in the RL literature; for example, linear
MDPs Jin et al. (2020) posit a linear structure on the expected reward and the transition density as
r(s, a) = hφ(s, a), θi and P(s0|s, a) = hφ(s, a), λ(s0)i for some feature map φ : X → Rd0 and
signed measures λ(s0) = (λ(s0)1, . . . , λ(s0)d0). To make it even more concrete, we present a simple
example for the sufficient condition above.
Example 3.2 (Reproducing kernel Hilbert space (RKHS) with Matern kernels). Define kh,ι the
Matern kernel with smoothness parameter h > 0 and length scale l > 0. If both the expected
reward function r(∙) and the transition density gs，9)：= P(s0∣∙) at any s0 ∈ S are functions in
the RKHS OfMatem kernel kh,ι where h = a 一 d/2 > 0 and l > 0, then Assumption 3.2 holds
for p = q = 2. This is due to the norm-equivalence between the above RKHS and the Sobolev
space W2a(X) (Kanagawa et al., 2018) and the degeneration from Besov spaces to Sobolev spaces
asB2a,2(X) =W2a(X).
More generally, our Besov dynamic closure assumption also encompasses the dynamic condition
considered in the prior result (Yang et al., 2019). In particular, as remarked earlier, the Besov space
Ba,q reduces into the Holder space Ca and Sobolev space Wa atP = q = ∞,α ∈ (0, ∞)∖N, and at
p = q = 2, α ∈ N, respectively. Moreover, our dynamic assumption only requires the boundedness
of a very general notion of local oscillations of the underlying MDP; that is, the underlying MDP
can be discontinuous or non-differentiable (e.g., when α ≤ 1/2 and p = 2), or even have spatially
inhomogeneous smoothness (e.g., when p < 2).
The condition α > & guarantees a finite bound for the compactness and the (local) Rademacher
complexity of the considered Besov space. When p < 2 (thus the condition above becomes α >
d/p), a function in the corresponding Besov space contains both spiky parts and smooth parts, i.e.,
the Besov space has inhomogeneous smoothness (Suzuki, 2018). In particular, when α > d/p,
each equivalence class [f]λ, f ∈ Bpa,q(Rd), i.e., modulo equality λ-almost everywhere, contains a
unique continuous representative. In addition, this representative has partial derivatives of order at
least α 一 d/p; thus α 一 d/p is called the differential dimension of the Besov space. Finally, we
remark that linear MDPs (Jin et al., 2020) corresponds to Assumption 3.2 with α = 1 and p = q
on a p-norm bounded domain. However, the additional condition a > & is not necessary for the
particular case of linear MDPs. This is due to the fact that there is a closed-form solution to the
value regression problem in linear MDPs and the size of the linear models for MDP is controllably
small without any additional smoothness assumption (rather than the completeness assumption). Of
course, our analysis addresses significantly more complex and general settings than linear MDPs
which we believe is more important than recovering the optimal condition in linear MDPs.
4	Algorithm and Theory
4.1	Algorithm
Now we turn to the main algorithm and the main result. We study a FQI-type algorithm, namely
least-squares value iteration (LSVI) for both OPE and OPL with the pseudo-code presented in Al-
gorithm 1 where we denote Pn(s, a) = ρ(s)π(a∣s). The algorithm is nearly identical to (Duan &
Wang, 2020) but with deep neural network function approximation instead of linear models. As
such, it can be considered as a generalization.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 Least-squares value iteration (LSVI)
1:	Initialize Q0 ∈ FNN .
2:	for k = 1 to K do
3:	If OPE (for a fixed policy π): yi - ri + Y JA Qk-∖(s[, α)π(dα∣si), ∀i
4:	If OPL: yi -ri + γmaXa0∈A Qk-i(si, α0),∀i
5：	Qk — argminf∈Fnn n1 Pn=If(Si, ai) - yi)2
6： end for
7:	If OPE, return VK = ∣∣Qk∣∣ρ∏ = ,Eρ(s)∏(a∣s) [Qk(s,α)2]
8:	If OPL, return the greedy policy πK w.r.t. QK .
The idea of LSVI is appealingly simple: it does the best it could with all the offline data using
least-squares regression over a function space. The algorithm arbitrarily initializes Q0 ∈ FNN and
iteratively computes Qk as follows: at each iteration k, the algorithm constructs a new regression
data {(xi, yi)}in=1 where the covariates xi are (Si, ai) and the Bellman targets yi are computed
following dynamic programming style. In particular, depending on whether this is an OPE or OPL
problem, yi are computed according to line 3 and line 4 of Algorithm 1, respectively. It then fits the
function class FNN to the constructed regression data by minimizing the mean squared error at line
5. This type of algorithm belongs to the fitted Q-iteration family (Munos & Szepesvari, 2008; Le
et al., 2019) that iteratively uses least-squares (value) regression to estimate the value functions. The
main difference in the algorithm is here we use deep neural networks as function approximation for
generalization to unseen states and actions in a complex MDP.
On the computational side, solving the non-convex optimization at line 5 of Algorithm 1 can be
highly involved and stochastic gradient descent is a dominant optimization method for such a task
in deep learning. In particular, (stochastic) gradient descent is guaranteed to converge to a global
minimum under certain structural assumptions (Du et al., 2019a;b; Allen-Zhu et al., 2019; Nguyen,
2021). Here, as we only focus on the statistical properties of LSVI, we assume that the minimizer
at line 5 is attainable. Such a oracle assumption is common when analyzing the statistical properties
ofan RL algorithm with non-linear function approximation (Yang et al., 2019; Chen & Jiang, 2019;
Duan et al., 2021; Wang et al., 2019; 2020; Jin et al., 2021).
4.2	Correlated S tructure
We remark the correlated structure in Algorithm 1. The target variable yi computed at line 3 and line
4 of the algorithm depends on the previous estimate Qk-1 which in turn depends on the covariate
xi := (Si, ai). This induces a complex correlated structure across all iterations where the current
estimate depends on all the previous estimates and the past data. In particular, one of the main
difficulties caused by such correlated structure is that conditioned on each xi , the target variable
yn is no longer centered at [T*Qk-ι](xn) for OPL (or at [TπQk-ι](xn) for OPE, respectively),
i.e., E [[T*Qk-ι](xn) 一 yn∣xn] = 0. ThiS correlated structure hinders a direct use of the standard
concentration inequalities (e.g. Hoeffding’s inequality, Bernstein inequality). Prior results either
improperly ignore the correlated structure in their analysis (Le et al., 2019) or directly avoid it by
estimating each Qk on a separate fold of the original data (Yang et al., 2019). The data splitting
approach in (Yang et al., 2019), which splits the original data into K disjoint folds, helps remove
the correlated structure but scales the sample complexity linearly with K where K can be arbitrarily
large. In contrast, we overcome the correlated structure via a uniform convergence argument by
considering deterministic coverings of the target function space T*Fnn without the need for the
inefficient data splitting.
4.3	Theoretical Analysis
Our main result is a sup-optimality bound for LSVI in both OPE and OPL settings.
Theorem 4.1. Under Assumption 3.1 and Assumption 3.2, for any > 0, δ ∈ (0, 1], K > 0, if n
satisfies that n & (*)1+ α log6 n + *(log(1∕δ) + log log n) ,then with probability at least 1 一 δ,
7
Under review as a conference paper at ICLR 2022
the sup-optimality of Algorithm 1 is
ʃSubOpt(VK; π) ≤ √μ e + (1-K)2∕2 for OPE,
ɪ SubOPt(πK) ≤ (Y√2 e + 41-γ)3∕2 for OPL
In addition, the optimal deep ReLU network Φ(L, m, S, B) that obtains such sample complexity (for
both OPE and OPL) satisfies
L N log N,m N N log N, S N N, and B N N1/d+⑵"Si),
1 + (2+ αg+d))
where ι := d(p-1 一 (1 + [a[)-1)+ and N N n	1+2α	is the number of parameters to
approximate a function in the Besov space.
Remark 4.1. The role of deep ReLU networks in offline RL is to guarantee a maximal adaptivity
to the (spatial) regularity of the functions in Besov space and obtain an optimal approximation er-
ror rate that otherwise were not possible with other function approximation such as kernel methods
(Suzuki, 2018). Moreover, by the equivalence in the functions that a neural architecture can com-
pute (Yarotsky, 2017), Theorem 4.1 also readily holds for any other continuous piece-wise linear
activation functions with finitely many line segments M where the optimal network architecture
only increases the number of units and weights by constant factors depending only on M .
Remark 4.2. The optimal ReLU network that realizes our sample complexity can be further sim-
plified as L = O (log n) and m = O(√n log n). That is, the optimal ReLU network is relatively
“thinner” than overparameterized neural networks that have been recently studied in the literature
(Arora et al., 2019; Allen-Zhu et al., 2019; Hanin & Nica, 2019; Cao & Gu, 2019; Belkin, 2021)
where the width m is a high-order polynomial of n. As overparameterization is a key feature for
such overparameterized neural networks to obtain a good generalization, it is natural to ask why a
thinner neural network in Theorem 4.1 also guarantees a strong generalization for offline RL even
when the network is not in the overparameterization regime? Intuitively, it is due to that the optimal
ReLU network in Theorem 4.1 is regularized by a strong sparsity which resonates with our practical
wisdom that we can use a sparsity-based regularization to prevent over-fitting and achieve a better
generalization. In particular, as the total number of parameters in the considered neural network is
p = md + m + m2 (L 一 2) = O(N2 log3 N) while the number of non-zeros parameters S only
scales with N, the optimal ReLU network in Theorem 4.1 is relatively sparse.
Theorem 4.1 states that LSVI incurs a sub-optimality which consists of the statistical error (the
first term) and the algorithmic error (the second term). While the algorithmic error enjoys the fast
linear convergence to 0, the statistical error reflects the fundamental difficulty of the problems. The
statistical errors for both OPE and OPL cases are bounded by the distributional shift Kμ, the effective
horizon 1/(1 一 γ), and the user-specified precision e for n satisfying the inequality given in Theorem
4.1. In particular, the sample complexity does not depend on the number of states as in tabular MDPs
(Yin & Wang, 2020; Yin et al., 2021; Yin & Wang, 2021) or the inherent Bellman error as in the
general function approximation (Munos & Szepesvari, 2008; Le et al., 2019). Instead, it explicitly
scales with the (possible fractional) smoothness α of the underlying MDP and the dimension d of
the input space. Importantly, this guarantee is established under the correlated structure of the value
estimate in the algorithm and the Besov dynamic closure encompassing the dynamic conditions of
the prior results. Thus, Theorem 4.1 is the most comprehensive result we are aware of for offline RL
with deep neural network function approximation.
Moreover, to further develop an intuition on our sample complexity, we compare it with the prior
results. Regarding the tightness of our result, our sample complexity e-2-2d/a (ignoring the log
factor and the factor pertaining to κμ and effective horizon) nearly matches the nonparametric re-
gression's minimax-optimal sample complexity e-2-d/a (Kerkyacharian & Picard, 1992; Gine &
Nickl, 2016) even though in our case we deal with a more complicated correlated structure in a
value iteration problem instead of a standard non-parametric regression problem. This gap is neces-
sary and expected due to the correlated structure in the algorithm. We remark that it is possible to
retain the rate e-2-d/a if We split the offline data D into K (given in Algorithm 1) disjoint subsets
and estimate each Qk in Algorithm 1 using a separate disjoint subsets. This however scales the
sample complexity linearly with K which could be arbitrarily large in practice.
8
Under review as a conference paper at ICLR 2022
Table 1: The state-of-the-art (SOTA) statistical theory of offline RL with function approximation.
Here, the distributional shift measure κ can be defined differently in different works.
Work	Function	Regularity	Tasks	Sample complexity	Remark
Yin & Wang (2020)	Tabular	Tabular	-OPE	o(含 F ∙∣a∣2)	minimax-optimal
Duan & Wang (2020)	Linear	Linear	OPE	O（含∙ d	minimax-optimal
Le et al. (2019)	General	General	OPE/OPL	N/A	improper analysis
Yang et al. (2019)	ReLU nets	Holder	OPL	O (K ∙ κ2+α ∙ e-2-d)	no data reuse
This work	ReLU nets	Besov	OPE/OPL	O (κ1+d ∙e-2-2d)	data reuse
To show the significance of our sample complexity, we summarize our result and compare it with
the prior results in Table 1. From the leftmost column to the rightmost one, the table describes the
related works, the function approximations being employed, the regularity conditions considered to
establish theoretical guarantees, the offline RL tasks considered, the sample complexity obtained,
and the important remarks or features of each work. Here, |S | and |A| are the cardinalities of the
state and action space when they are finite. Specifically, the “data reuse” in Table 1 means that
an algorithm reuses the data across all iterations instead of splitting the original offline data into
disjoint subsets for each iteration and the regularity column specifies the regularity assumption on
the underlying MDP. Based on this comparison, we make the following observations. First, with
simpler models such as tabular and linear MDPs, it requires less samples to achieve the same sub-
optimality precision e than more complex environments such as Holder and BesoV MDPs. This
should not come as a surprise as the simpler regularities are much easier to learn but they are too
strong as a condition to hold in practice. Second, as remarked earlier that Besov smoothness is
more general than Holder smoothness considered in (Yang et al., 2019), our setting is more practical
and comprehensive as it covers more scenarios of the regularity of the underlying MDPs than the
prior results. Third, our result obtains an improved sample complexity as compared to that in (Yang
et al., 2019) where we are able to get rid of the dependence on the algorithmic iteration number K
which can be arbitrarily large in practice. On the technical side, we provide a unifying analysis that
allows us to account for the complicated correlated structure in the algorithm and handle the complex
deep ReLU network function approximation. This can also be considered as a substantial technical
improvement over (Le et al., 2019) as Le et al. (2019) improperly ignores the correlated structure
in their analysis. In addition, the result in (Le et al., 2019) does not provide an explicit sample
complexity as it depends on an unknown inherent Bellman error. Thus, our sample complexity
improves over the result of the data splitting method and holds with in a broader context by our
Besov dynamic closure.
Finally, we provide a detailed proof for Theorem 4.1 in Section A. The proof has four main com-
ponents: a sub-optimality decomposition for error propagation across iterations, a Bellman error
decomposition using a uniform convergence argument, a deviation analysis for least-squares value
regression with deep ReLU networks using local Rademacher complexities via a localization argu-
ment, and an upper bound minimization step to obtain an optimal deep ReLU architecture.
5 Conclusion
We presented the sample complexity of offline RL with deep ReLU network function ap-
proximation. We proved that the FQI-type algorithm can achieve the sample complexity of
O (κ1+4∕ɑ ∙ e-2-2d∕a) under highly correlated structures and a general dynamic condition namely
the Besov dynamic closure. We also provided various insights into the benefits and the effects of
deep neural networks in offline RL.
We close with a future direction. Although the finite concentration coefficient assumption is rel-
atively standard in offline RL, can we develop a weaker, non-uniform assumption that can still
accommodate offline RL with non-linear function approximation? While such a weaker data cov-
erage assumptions do exist for offline RL in tabular settings (Rashidinejad et al., 2021), it seems
non-trivial to generalize this condition to the function approximation setting.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Mach. Learn., 71
(1):89-129, April 2008. ISSN 0885-6125. doi: 10.1007/s10994-007-5038-2. URL https:
//doi.org/10.1007/s10994-007-5038-2.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Ann.
Statist., 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282. URL https://doi.
org/10.1214/009053605000000282.
Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the
prism of interpolation. arXiv preprint arXiv:2105.14368, 2021.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Pro-
ceedings of 1995 34th IEEE Conference on Decision and Control, volume 1, pp. 560-564. IEEE,
1995.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 32:10836-10846,
2019.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 1042-1051. PMLR,
2019.
Albert Cohen. A primer on besov spaces, 2009. URL http://cnx.org/content/
col10679/1.2/>.
Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. In ICML, volume 97 of Proceedings of Machine Learning
Research, pp. 1675-1685. PMLR, 2019a.
Simon S. Du, XiyU Zhai, BarnabaS Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In ICLR (Poster). OpenReview.net, 2019b.
Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approx-
imation. CoRR, abs/2002.09516, 2020.
Yaqi Duan, Chi Jin, and Zhiyuan Li. Risk bounds and rademacher complexity in batch reinforcement
learning. arXiv preprint arXiv:2103.13883, 2021.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv
preprint arXiv:1103.4601, 2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. arXiv preprint arXiv:1802.03493, 2018.
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and
inference: Application to causal effects and other semiparametric estimands. arXiv preprint
arXiv:1809.09953, 2018.
Evarist Gine and Richard Nickl. Mathematical foundations of infinite-dimensional statistical mod-
els, volume 40. Cambridge University Press, 2016.
Steffen Grunewalder, GUy Lever, Luca Baldassarre, Massimiliano Pontil, and Arthur Gretton. Mod-
elling transition dynamics in mdps with RKHS embeddings. In ICML. icml.cc / Omnipress, 2012.
10
Under review as a conference paper at ICLR 2022
Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. A Distribution-Free Theory of
Nonparametric Regression. Springer series in statistics. Springer, 2002.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv
preprint arXiv:1909.05989, 2019.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning,
2015.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning.
In ICML, volume 48 of JMLR Workshop and Conference Proceedings, pp. 652-661. JMLR.org,
2016.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of
RL problems, and sample-efficient algorithms. CoRR, abs/2102.00815, 2021. URL https:
//arxiv.org/abs/2102.00815.
Nicholas K. Jong and Peter Stone. Model-based function approximation in reinforcement learning.
In AAMAS, pp. 95. IFAAMAS, 2007.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evalu-
ation in markov decision processes, 2019.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaus-
sian processes and kernel methods: A review on connections and equivalences. arXiv preprint
arXiv:1807.02582, 2018.
Gerard Kerkyacharian and DominiqUe Picard. Density estimation in besov spaces. Statistics &amp;
probability letters, 13(1):15-24, 1992.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. J. Mach. Learn. Res., 4:
1107-1149, 2003.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. Springer, 2012.
Hoang Minh Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In
ICML, volume 97 of Proceedings of Machine Learning Research, pp. 3703-3712. PMLR, 2019.
Yunwen Lei, Lixin Ding, and Yingzhou Bi. Local rademacher complexity bounds based on covering
numbers. Neurocomputing, 218:320-330, 2016.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In NeurIPS, pp. 5361-5371, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Remi Munos. Error bounds for approximate policy iteration. In ICML, pp. 560-567. AAAI Press,
2003.
11
Under review as a conference paper at ICLR 2022
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. J. Mach. Learn.
Res.,9:815-857, 2008.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of
discounted stationary distribution corrections, 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. ArXiv, abs/1912.02074, 2019b.
Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. arXiv preprint arXiv:2101.09612, 2021.
R. Nickl and B. M. Potscher. Bracketing metric entropy rates and empirical central limit theorems
for function classes of besov- and sobolev-type. Journal of Theoretical Probability, 20:177-199,
2007.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning,
ICML ’00, pp. 759-766, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc.
ISBN 1558607072.
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline rein-
forcement learning and imitation learning: A tale of pessimism. arXiv preprint arXiv:2103.12021,
2021.
Patrick Rebeschini. Oxford Algorithmic Foundations of Learning, Lecture Notes: Maximal In-
equalities and Rademacher Complexity, 2019. URL: http://www.stats.ox.ac.uk/
~rebeschi∕teaching/AFoL∕2 0∕material∕lecture02.ρdf. Last visited on Sep.
14, 2020.
Yoshihiro Sawano. Theory of Besov Spaces, volume 56. Springer, 2018. ISBN 978-981-13-0835-2.
Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog:
Connecting new skills to past experience with offline reinforcement learning. arXiv preprint
arXiv:2010.14500, 2020.
Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces:
optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139-2148, 2016.
Samuele Tosatto, Matteo Pirotta, Carlo D’Eramo, and Marcello Restelli. Boosted fitted q-iteration.
In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 3434-3443. PMLR,
2017.
Hung Tran-The, Sunil Gupta, Thanh Nguyen-Tang, Santu Rana, and Svetha Venkatesh. Combining
online learning and offline learning for contextual bandits with deficient support. arXiv preprint
arXiv:2107.11533, 2021.
H. Triebel. Theory of function spaces. 1983.
Cameron Voloshin, Hoang M Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy
evaluation for reinforcement learning. arXiv preprint arXiv:1911.06854, 2019.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Provably efficient reinforcement learning
with general value function approximation. CoRR, abs/2005.10804, 2020. URL https://
arxiv.org∕abs∕2005.10804.
Yining Wang, Ruosong Wang, Simon S. Du, and Akshay Krishnamurthy. Optimism in reinforce-
ment learning with generalized linear function approximation. CoRR, abs/1912.04136, 2019.
URL http://arxiv.org/abs/1912.04136.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for reinforce-
ment learning with marginalized importance sampling, 2019.
12
Under review as a conference paper at ICLR 2022
Zhuoran Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep q-learning. CoRR,
abs/1901.00137, 2019.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Ming Yin and Yu-Xiang Wang. Asymptotically efficient off-policy evaluation for tabular reinforce-
ment learning. In AISTATS, volume 108 of Proceedings of Machine Learning Research, pp.
3948-3958. PMLR, 2020.
Ming Yin and Yu-Xiang Wang. Characterizing uniform convergence in offline policy evaluation via
model-based approach: Offline learning, task-agnostic and reward-free, 2021.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline
policy evaluation for reinforcement learning. In Arindam Banerjee and Kenji Fukumizu (eds.),
The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April
13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pp. 1567-
1575. PMLR, 2021. URL http://proceedings.mlr.press/v130/yin21a.html.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of
stationary values. ArXiv, abs/2002.09072, 2020a.
Shangtong Zhang, Bo Liu, and Shimon Whiteson. Gradientdice: Rethinking generalized offline
estimation of stationary values. ArXiv, abs/2001.11113, 2020b.
13
Under review as a conference paper at ICLR 2022
A Proof of Theorem 4.1
We now provide a complete proof of Theorem 4.1. The proof has four main components: a sub-
optimality decomposition for error propagation across iterations, a Bellman error decomposition us-
ing a uniform convergence argument, a deviation analysis for least squares with deep ReLU networks
using local Rademacher complexities and a localization argument, and a upper bound minimization
step to obtain an optimal deep ReLU architecture.
Step 1: A sub-optimality decomposition
The first step of the proof is a sub-optimality decomposition, stated in Lemma A.1, that applies
generally to any least-squares Q-iteration methods.
Lemma A.1 (A sub-optimality decomposition). Under Assumption 3.1, the sub-optimality of VK
returned by Algorithm 1 is bounded as
SubOpt(VK) ≤
√κ~	Y K/2
√μ 0≤m≤aχ-i kQk+1 - TπQkkμ + (1⅛72	for OPE,
4-κ2 0≤m≤αx-1 kQk+1- T*Qkkμ+ (4γ- Y)3/2 for OPL.
where we denote kf kμ := RR μ(dsda)f (s, a)2,∀f : S ×A → R.
The lemma states that the sub-optimality decomposes into a statistical error (the first term) and an
algorithmic error (the second term). While the algorithmic error enjoys the fast linear convergence
rate, the statistical error arises from the distributional shift in the offline data and the estimation
error of the target Q-value functions due to finite data. Crucially, the contraction of the (optimality)
Bellman operators Tπ and T* allows the sup-optimality error at the final iteration K to propagate
across all iterations k ∈ [0, K - 1]. Note that this result is agnostic to any function approximation
form and does not require Assumption 3.2. The result uses a relatively standard argument that
appears in a number of works on offline RL (Munos & Szepesvari, 2008; Le et al., 2019).
Proof of Lemma A.1. We will prove the sup-optimality decomposition for both settings: OPE and
OPL.
⑴ For OPE. We denote the right-linear operator by Pπ ∙ : {X → R}― {X — R} where
(Pπ f)(s,a) := / f (s0, a0)π(da0∣s0)P(ds0∣s, a),
X
for any f ∈ {X → R}. Denote Denote ρπ(dsda) = ρ(ds)π(da∣s). Let ek := Qk+1 — TπQk, ∀k ∈
[0, K - 1] and K = Q0 - Qπ . Since Qπ is the (unique) fixed point of Tπ , we have
Qk — Qn = Tπ Qk-1 — Tπ Qn + ek-1 = YPπ (Qk-1 — Qn) + ek-1.
By recursion, we have
K	1 K+1 K
QK — Qn = E(YPn )k ek = ；— E ak Ak ek
1 — Y
k=0	k=0
where ak := 11-γK+ι,∀k ∈ [K] and Ak := (Pπ)k,Vk ∈ [K]. Note that PK=O αk = 1 and Ak's
are probability kernels. Denoting by |f| the point-wise absolute value |f (s, a)|, we have that the
following inequality holds point-wise:
1 — YK+1 K
IQK - Qπ | ≤   ---αkA akAk Iek |.
1—Y
k=0
We have
IlQK - Qπkp∏ ≤ (I(I -Yy) Z Pms)n(da|S) (X ak AkIekI(S,aɔ)
14
Under review as a conference paper at ICLR 2022
(a)
≤
(1 - YK+1)2
(i-γ )2
K
I ρ(ds)π(da∖s) ɪ2 akAkek(s, a)
J	k=0
(b)
≤
(1 - 7K+1)2
(i-γ)2
K
I ρ(ds)π(da∖s) ɪ2 akAkek(s, a)
k=0
ρ(ds)π(da∖s)
K-1
E akAkek(s,a) + αK
k=0
(d) (1 - 7K+1)2 ' f「
≤ (i - γ)2	I J μ(ds,da)
K-1
E akκμel(s,a) + αK
k=0
(1 - 7K+1)2
(i-γ)2
αkκμ∣∣ek ∣∣μ + αK
(N)「U
≤ --κμ .0 max Ilek∣∣2 + —--------------.
-(1 - γ)2 0≤k≤K-i 11 kllμ (1 - Y)
The inequalities (a) and (b) follow from Jensen,s inequality, (c) follows from ∣∣Q0∣∣∞, ∣∣Qπ∣∣∞ ≤ 1,
and (d) follows from Assumption 3.1 that ρπAk = ρπ(Pπ)k ≤ κμμ. Thus we have
SubOpt(VK; π) = ∖VK - Vπ ∖
=Eρ,∏[QK(s,a)] - Eρ[Qπ(s,a)]
≤ Eρ,∏[∖QK(s,a) - Qn(s,a)∖]
≤ q/Eρ,π [(QK(S,a) - Qn(s,a))2]
=IQK - Qπkρ∏
≤ -γ--μ	maχ	∣∣ek ∣∣μ +~~j-ɪ/ʒ-.
1 — Y 0≤k≤K-1	(1 — γ)1/2
(ii) For OPL. The sup-optimality for the OPL setting is more complex than the OPE setting but
the technical steps are relatively similar. In particular, let ek-ι = T*Qk-ι — Qk, Yk and π*(s)=
argmaXɑ Q*(s, a), Vs, we have
Q* - QK = Tπ*Q* - Tπ* QK-ι + Tπ* QK-ι - T*QK-1 +eK-ι
X--------------------------------------V---------}
≤0
≤ YPπ (Q* - QK-I) + eK-I
K-1
≤ X γK-k-1(pπ* )K-k-1ek + YK(Pπ* )K(Q* - Q0)(by recursion).	(1)
k=0
Now, let ∏k be the greedy policy w.r.t. Qk, we have
Q* - QK = Tπ*Q* -TπK-1 QK-1 + TπK-1QK-i- T*QK-1 +eK-1
x z-}	、	{z	}
≥T πΚ-1Q*	≥0
≥ YPπK-1(Q* - QK-1) + eK-1
K-1
≥ X Y K-k-1 (P πK-1 ...P πk+1 )ek + YK (P πK-1 ...Pπ0 )(Q* - Q0).	(2)
k=0
Now, we turn to decompose Q* - QπK as
Q* - QπK = (Tπ*Q* - Tπ* QK) + (Tπ*QK - TπKQK) +(TπKQK - TFKQnK)
V------------------------------------V-----}
≤0
≤ YPπ*(Q* - QK) + YPπκ(QK - Q* + Q* - QnK).
15
Under review as a conference paper at ICLR 2022
Thus, we have
(I - YPKK)(Q* - QKK) ≤ γ(Pπ* - PKK)(Q* - Qk).
Note that the operator (I - YPπK)-1 = P∞0(γPπK)i is monotone, thus
Q* - QnK	≤	Y(I	- YPπκ )-1Pπ* (Q*	-	Qk )-Y(I	- YP 欠K )-1P πK (Q* -	Qk ).	(3)
Combining Equation (3) with Equations (1) and (2), we have
Q* - QnK ≤ (I - yPπK)-1
YKi (Pπ* )κ-kek + YK+1 (Pπ* )k+1(Q* - Qo)
—
(I - yPTK )-1
YK-k
(PkK ... Pπk+1 )ek + YK+1(PnK ... Pn )(Q* - Qo)
Using the triangle inequality, the above inequality becomes
Q* - QnK ≤
2γ(i- YK+1)
(1 - Y)2
akAk|ek| + ακAK|Q* - Qo|
where
Ak
AK
1-γ (I - γP tk )-1 ((Pn* )k-k + P tk ...P nk+1) , ∀k < K,
1-γ (I - γPπκ )-1 ((Pn* )k+1 + Pnκ ...Pno),
αk = Yκ-k-1(1 - γ)∕(1 - YK+1), ∀k < K,
ακ = YK(I-Y)/(1 - yk+1).
Note that Ak is a probability kernel for all k and Ek a = 1. Thus, similar to the steps in the OPE
setting, for any policy π, we have
kQ*- Qnκ心 ≤
K-1
I ρ(ds')π(da∖s')	akAkek(s, a) + aκ
k=0
2
2
≤
K-1
I μ(ds, da) ɪ2
J	k=0
74Y⅛ max kek∣2 + JYK∖ .
(1 - γ)4 o≤k≤κ-1l1 kllμ + (1 - Y)3
≤
Thus, we have
kQ* - Qnκ kρ
2γ√κμ
≤ (1-τF
max Ilek IlU +
o≤k≤κ-111 kllμ
2γκ/2+1
(1 - γ)3/2 .
Finally, we have
SUbOPt(πκ) = EP [Q*(s, π*(s)) - Q*(s, ∏κ(s))]
≤ EP [Q*(s,π*(s)) - Qnκ(s,π*(s)) + Qnκ(s,∏κ(s)) - Q*(s,∏κ(s))]
≤kQ*- Qnκkρ∏* + IQ* - Qnκkρ∏κ
4Y√κ7
≤ (1-77
max
o≤k≤K-1
llek ∣∣μ +
4γ K/2+1
(1 - Y)3/2 .
□
16
Under review as a conference paper at ICLR 2022
Step 2 : A Bellman error decomposition
The next step of the proof is to decompose the Bellman errors kQk+ι — TπQk ∣∣μ for OPE and
kQk+ι 一 T*Qk ∣∣μ for OPL. Since these errors can be decomposed and bounded similarly, we only
focus on OPL here.
The difficulty in controlling the estimation error ∣Qk+ι — T*Qk∣∣2,μ is that Qk itself is a random
variable that depends on the offline data D. In particular, at any fixed k with Bellman targets {yi}in=1
where yi = ri + γ maxa0 Qk(s0i, a0), it is not immediate that E [[T*Qk](xi) - yi|xi] = 0 for each
covariate xi := (si , ai ) as Qk itself depends on xi (thus the tower law cannot apply here). A naive
and simple approach to break such data dependency of Qk is to split the original data D into K
disjoint subsets and estimate each Qk using a separate subset. This naive approach is equivalent to
the setting in (Yang et al., 2019) where a fresh batch of data is generated for different iterations.
This approach is however not efficient as it uses only n/K samples to estimate each Qk. This is
problematic in high-dimensional offline RL when the number of iterations K can be very large as
it is often the case in practical settings. We instead prefer to use all n samples to estimate each
Qk. This requires a different approach to handle the complicated data dependency of each Qk. To
circumvent this issue, we leverage a uniform convergence argument by introducing a deterministic
covering ofT*FNN. Each element of the deterministic covering induces a different regression target
{ri + γ maxa0 Q(s0i, a0)}in=1 where Q is a deterministic function from the covering which ensures
that E 卜i + Y max。，Q(Si, a0) 一 [T*Q](xi)∣xJ = 0. In particular, we denote


yiQk
n
r + γ max Qk (si,a0), ∀i and fQk := Qk+1 = arg inf X l(f (xi),yQk), and fQk = T *Qk,
a0	f∈FNN i=1
where l(x, y) = (x - y)2 is the squared loss function. Note that for any deterministic Q ∈ FNN,
We have fQ(χι) = E[yQ∣χι],∀χι, thus
E(if- ifQ)= kf - fQkμ,∀f,
(4)
where f denotes the random variable (f (χι)-yQ)2. Now letting fQ := arg inf f ∈Fnn kf-fQ ∣∣2,μ
be the projection of f*Q onto the function class FNN, we have
maχIlQk+1—T*Qkkμ=maxIIfQk- fQkkμ	(≤)	SUP	IIfQ	-	fQkμ = SUp	E(lfQ-IfQ)
k	k	Q∈FNN	Q∈FNN	f	f*
(c)
≤ sup
Q∈FN N
sup
Q∈FNN
{E(l∕Q - lfQ )+ En(lf Q - lfQ )}
n(E — En)(lfQ — If Q )+ En (lf Q - If ?)}
≤ sup (E — En)(l fQ — l f Q ) + sup En(l fQ — IfQ ),
Q∈Fnn	*	Q∈Fnn	⊥	*
V-----------------{----------------} ×-----------------------------}
I1 ,empirical process term
(5)
{^^^^^≡
I2 ,bias term
where (a) follows from that Qk ∈ FNN, (b) follows from Equation (4), and (c) follows from that
En[lfQ] ≤ En[lfQ], ∀f, Q ∈ FNN. That is, the error is decomposed into two terms: the first term
I1 resembles the empirical process in statistical learning theory and the second term I2 specifies the
bias caused by the regression target f*Q not being in the function space FNN .
Step 3 : A deviation analysis
The next step is to bound the empirical process term and the bias term via an intricate concentration,
local Rademacher complexities and a localization argument. First, the bias term in Equation (5) is
taken uniformly over the function space, thus standard concentration arguments such as Bernstein’s
inequality and Pollard,s inequality used in (Munos & Szepesvari, 2008; Le et al., 2019) do not apply
here. Second, local Rademacher complexities (Bartlett et al., 2005) are data-dependent complexity
measures that exploit the fact that only a small subset of the function class will be used. Lever-
aging a localization argument for local Rademacher complexities (Farrell et al., 2018), we localize
17
Under review as a conference paper at ICLR 2022
an empirical Rademacher ball into smaller balls by which we can handle their complexities more
effectively. Moreover, we explicitly use the sub-root function argument to derive our bound and
extend the technique to the uniform convergence case. That is, reasoning over the sub-root func-
tion argument makes our proof more modular and easier to incorporate the uniform convergence
argument.
Localization is particularly useful to handle the complicated approximation errors induced by deep
ReLU network function approximation.
Step 3 .a: Bounding the bias term via a uniform convergence concentration
INEQUALITY
Before delving into our proof, we introduce relevant notations. LetF-G := {f-g : f ∈ F, g ∈ G},
let N (e, F,『k) bethe e-covering number of F w.r.t.『k norm, H (e, F, ∣∣∙∣∣) := log N (e, F,『k) be
the entroPic number, let N[] (e, F, ∣∣∙∣∣) be the bracketing number of F, i.e.,the minimum number of
brackets of『k -size less than or equal to e, necessary to cover F, let H[] (e, F, k1|) = log NO (e, F,『
k) be the ∣∣ ∙ ∣∣-bracketing metric entropy of F ,let F∣{χi}n=ι = {(f (xi),..., f (χn)) ∈ Rn∣f ∈ f},
and let T*F = {T*f : f ∈ F}. Finally, for sample set {χi}n=ι, we define the empirical norm
kf kn := q 1 Pn=If(Xi)2 .
We define the inherent Bellman error as "fnn := supq∈Fn. inff ∈Fnn Ilf — T*Qkμ. ThiS implies
that
dFNN := SUp jnf kf—T*Qkμ = SUp E(IfQ—lfQ).	⑹
Q∈FNN f∈FNN	Q∈FNN	⊥
We have
|lf - lg | ≤ 4|f - g| and |lf - lg | ≤ 8.
We have
H(e,{fQ - IfQ : Q ∈ FNN}∣{xi,yi}i=ι,n-1k∙ kι)
≤ H(4, {fQ -fQ : Q ∈ FNN}∣{xi}n=ι,n-1k∙kι)
≤ H (4, (F- T *Fnn )∣{xi}n=ι,n-1k∙kι)
≤ H(∣,Fnn∣{xi}n=ι,n-1k∙kι) + H(∣,T*Fnn∣{xi}n=ι,n-1k∙kι)
≤ H (8e, FNN l{xi}n=1, k ∙ k∞) + H (|,T*FNN, k ∙ k∞)
For any e0 > 0 and δ0 ∈ (0, 1), it follows from Lemma B.2 with e = 1/2 and α = e02, with
probability at least 1 - δ0, for any Q ∈ FNN, we have
En(IfQ	-	IfQ )	≤	3E(lfQ	- IfQ )	+ e02	≤	3dFNN + e02,	⑺
given that
n ≈ F (logH∕δZ) + log EN (而,(FNN - T *FNN )l{xi}n=l ,n-1k ∙ IlI)).
e02	40	=
Note that if we use Pollard,s inequality (Munos & Szepesvari, 2008) in the place of Lemma B.2, the
RHS of Equation (7) is bounded by e0 instead of e02(i.e., n scales with O(1∕e04) instead of O(1∕e02)).
In addition, unlike (Le et al., 2019), the uniform convergence argument hinders the application of
Bernstein’s inequality. We remark that Le et al. 2019 makes a mistake in their proof by ignoring the
data-dependent structure in the algorithm (i.e., they wrongly assume that Qk in Algorithm 1 is fixed
and independent of {si, ai}in=1). Thus, the uniform convergence argument in our proof is necessary.
18
Under review as a conference paper at ICLR 2022
Step 3 .b: B ounding the empirical process term via local Rademacher
COMPLEXITIES
For any Q ∈ FNN , we have
If - IfQ| ≤ 2|fQ - fQ| ≤ 2,
V[lfQ - lfQ] ≤ E[(lfQ - lfQ )2] ≤ 4E(fQ - fQ)2.
Thus, it follows from Lemma 1 (with α = 1/2) that with any r > 0, δ ∈ (0, 1), with probability at
least 1 - δ, we have
sup{(E - En)(l^Q - lfQ): Q ∈ FNN, kfQ - fQkμ ≤ r}
≤ SUP{(E - En)(If - lg) : f ∈ FNN, g ∈ T*F, kf - gkμ ≤ r}
≤ 3ERn {lf - lg ： f ∈ Fnn, g ∈ T*Fnn, kf - gk1 ≤『} + 2《2r"⑷ +
n	3n
{ , f u τ uτ*	2 v } Lr alog(1∕δ) 工 28log(1∕δ)
≤ 6ERn {f - g ： f ∈ FNN, g ∈ T FNN, kf - gkμ ≤ r ʃ + 2V -三----1---3^-----♦
STEP 3.c: Bounding kQk+ι - T*Qk∣∣μ using localization argument via sub-root
FUNCTIONS
We bound ∣Qk+ι - T*Qk ∣∣μ using the localization argument, breaking down the Rademacher Com-
plexities into local balls and then build up the original function space from the local balls. Let ψ be
a sub-root function (Bartlett et al., 2005, Definition 3.1) with the fixed point r* and assume that for
any r ≥ r* , we have
ψ(r) ≥ 3ERn {f - g ： f ∈ Fnn, g ∈ T*Fnn, kf - g∣l ≤ r} .	(8)
We recall that a function ψ : [0, ∞) → [0, ∞) is sub-root if it is non-negative, non-decreasing and
r → ψ(r)∕√r is non-increasing for r > 0. Consequently, a sub-root function ψ has a unique fixed
point r* where r* = ψ(r*). In addition, ψ(r) ≤ √rr*, ∀r ≥ r*. In the next step, we will find a
sub-root function ψ that satisfies the inequality above, but for this step we just assume that we have
such ψ at hand. Combining Equations (5), (7), and (8), we have: for any r ≥ r* and any δ ∈ (0, 1),
if kfQk-1 - fQk-Ik2,μ ≤ r, with probability at least 1 - δ,
kfQk-1 - fQk-1 k2,μ ≤ 2ψ(r) + 2产三 + 丝L + 3dF + C
,μ	Vn	3n
≤√r* + 2 广三+28粤也+ (√3dF + e0)2,
n	3n
where
n ≈ 4^02 (lθgW" + log EN (20, (FNN - T *FNN )l{xi}n=1, n-1k ∙ IlI)).
Consider r0 ≥ r* (to be chosen later) and denote the events
Bk ：= {kfQk-1 - fQk-1 k2,μ ≤ 2k ro}, Vk ∈ {0,1,…,l},
where l = log2(r10) ≤ log2(*).We have Bo ⊆ Bi ⊆ ... ⊆ Bl and since kf - gk，≤
1,∀∣f ∣∞, ∣g∣∞ ≤ 1, we have P(Bl) = LIfkfQkT - fQkT k( ≤ 2iro for some i ≤ l, then
with probability at least 1 - δ, we have
kfQk-1 - fQk-1 k2, μ ≤ E +2j 2i+1r0 …+ 28L + (√3dFNN + e，)2
,μ	Vn	3n
≤ 2i-1r0,
19
Under review as a conference paper at ICLR 2022
if the following inequalities hold
E+2y≡≡δ) ≤ 12i-ι√ro,
n2
”色→(√3dFNN+ 仔 ≤ 12一『0.
3n	2
We choose r0 ≥ r* SUch that the inequalities above hold for all 0 ≤ i ≤ l. This can be done by
simply setting
√r0=2-1 (p2ir*+2r2+1 l°n2,δ)-! ∣i=0+s 2i-ι( 28iogn2^+(√3dFNN+^°)2j i
.dFNN+J+rogn/ɪ)+√κ.
Since {Bi} is a sequence of increasing events, we have
P(B0)=P(B1)-P(B1∩B0c)=P(B2)-P(B2∩B1c)-P(B1 ∩B0c)
l-1
=P(Bl)-XP(Bi+1∩Bic)≥1-lδ.
i=0
Thus, with probability at least 1 - δ, we have
k 产k-1-fQk-1kμ . dFNN + J +产号画+ F	(9)
where
n ≈ -j^T2 (lθg(8l∕6 + log EN(不，(FNN - T*FNN) Kxi}n=1, n-1k∙ k I))) ∙
402	20
Step 3.d: Finding a sub-root function and its fixed point
It remains to find a sub-root function ψ(r) that satisfies Equation (8) and thus its fixed point. The
main idea is to bound the RHS, the local Rademacher complexity, of Equation (8) by its empirical
counterpart as the latter can then be further bounded by a sub-root function represented by a measure
of compactness of the function spaces FNN and T* FNN .
For any > 0, we have the following inequalities for entropic numbers:
H(e, FNN - T*FNN, k ∙ Iln) ≤ H(e∕2, FNN, k∙ Iln) + H(e∕2, T*FNN, k ∙ Iln),
(a)
H(e,FNN,∣Hln) ≤ H(e,FNN∣{xi}n=ι, k∙k∞) . N[(logN)2 +log(1∕e)], (10)
H(e,T*Fnn, k ∙ kn) ≤ H(e,T*Fnn, k ∙ k∞) ≤ H[](2e,T*Fnn, k ∙ k∞)
(b)	(c)
≤ H[](2e,Bα,q(X)，k∙k∞) . (2e)-dα,	(11)
where N is a hyperparameter of the deep ReLU network described in Lemma B.9, (a) follows
from Lemma B.9, and (b) follows from Assumption 3.2, and (c) follows from Lemma B.8. Let
H := FNN - T*FNN, it follows from Lemma B.5 with {ξk := e∕2k}k∈N for any e > 0 that
∞
EσRn{h ∈ H -H : IIhkn ≤ e} ≤ 4 ^X 2k-T
k=1
H(e∕2k-1, H, k∙kn)
n
∞
≤ 4 X 2⅛
k=1
∞
≤ T X 2-(
-√n k=1
H(e∕2k,Fnn, k ∙ k∞)
∞
+ 4 X 2fe^r
k=1
H(e∕2k,TπFnn, k ∙ k∞)
k-DqN ((log N )2 +log(2k∕e)) + √n X 2-(kτ) J(—L
n
n
20
Under review as a conference paper at ICLR 2022
√√n PN ((log N )2 +log(1∕e)) +
where we use √α + b ≤ √α + √¾ ∀α, b ≥ 0, P∞1 2k⅛ < ∞, and P∞1
k-1
< ∞.
It now follows from Lemma B.4 that
EσRn{f ∈F,g ∈ T*F : f - g? ≤ r}
v ∙ f If κ> r/M ∣∣∕1∣∣ V x , J2rH(e/2,H, k ∙ kn)-
≤ inf EσRn{h ∈ H - H ： ∣∣h∣∣μ ≤ e} ÷ V ---n------
「e ,--------------------- e1-2⅛	12r ,------------------- /2r	-d^l
. 7PN((IOgN)2 +lOg(I∕e)) H-----+- + \ 一PN((IOgN)2 +log(4/e)) + \ 一(e/2)M
L√n	√n	Vn	V n	J e nn-β
X n-β-1/2 ,N(log2 N + log n) + n-β(1-晟)-1/2 + ^r Jn(log2 N + log n) + √rn-1 (1-等)=:ψι(r),
where β ∈ (0, α) is an absolute constant to be chosen later.
Note that V[(f - g)2] ≤ E[(f - g)4] ≤ E[(f - g)2] for any f ∈ FNN,g ∈ T^Fnn. Thus, for
any r ≥ r*, it follows from Lemma B.1 that with probability at least 1 - ɪ, we have the following
inequality for any f ∈ FNN, g ∈ T*Fnn such that ∣∣f - g|，≤ r,
kf-gkn
≤ kf - gkμ + 3ERn{(f - g)2 ： f ∈ Fnn,g ∈ T*Fnn Jf - g[ ≤ r} + J2r^°gn + 56logn
μ	μ	n 3 n
≤ kf - gkμ + 3ERn{f - g ： f ∈ Fnn, g ∈ T*Fnn Jf - g[ ≤ r} + JIBiIn + 56logn
μ	μ	n 3 n
≤ r + ψ(r) + r + r ≤ 4r,
if r ≥ r* V 2nn V 56‰n∙ For such r, denote Er = {f - g* ≤ 4r}∩{∣∣f - f*∣∣μ ≤ r}, we have
P(Er) ≥ 1 - 1/n and
3ERn{f - g ： f ∈ FNN,g ∈ T*FNN, kf - gkj ≤ r}
=3EEσ Rn{f - g ： f ∈ Fnn , g ∈ T *Fnn ,kf - g|£ ≤ r}
≤ 3E IErEσ Rn{f - g ： f ∈ FNN, g ∈ T *FNN, kf - gIlj ≤ r} + (1 - 1Er )
≤ 3E EσRn{f - g ： f ∈Fnn,g ∈ T*Fnn, If - gm ≤ 4r} + (1 - 1%)
≤ 3(ψι(4r) + 1)
n
.n-β-1/2 JN(log2 N + log n) + n-β(1-2a)-1/2 + ^r JN(log2 N + log n)
+ √rn-1 (1-鼻)+ n-1 =： ψ(r)
It is easy to verify that ψ(r) defined above is a sub-root function. The fixed point r* of ψ(r) can be
solved analytically via the simple quadratic equation r* = ψ(r*). In particular, we have
√r7 . n-1/2 JN(log2 N + logn) + n-I(I-βd)+ n-2-1 [N(log2 N + logn)]1/4
+ n-2(I-2α)-1 + n-1/2
.n-4((2㈤.1)+1) JN(log2 N + log n) + n-1 (I-管)+ n-β(I-妥)-1 + n-1/2	(12)
It follows from Equation (9) (where l . log(1∕r*)), the definition of d^NN, Lemma B.9, and
Equation (12) that for any e0 > 0 and δ ∈ (0,1), with probability at least 1 - δ, we have
max ∣∣Qk+1 - T*Qk k“ . N-α* + e0 + n-4((2e)A1)+1) √N(log2 N + logn) + n-2(I-萼)
k
21
Under review as a conference paper at ICLR 2022
+ n- 2(I-2α)- 1 + n-1∕2piog(1∕δ) + log log n	(13)
where
n & 4ɪ02 UOg(I/6 + loglog n + log EN (20, (FNN - T * FNN )l{xi}n=l, n-1 ∙ k ∙ IlI))).
(14)
Step 4: Minimizing the upper bound
The final step for the proof is to minimize the upper error bound obtained in the previous steps
w.r.t. two free parameters β ∈ (0, d) and N ∈ N. Note that N parameterizes the deep ReLU
architecture Φ(L,m,S,B) given Lemma B.9. In particular, we optimize over β ∈ (0, Od) and
N ∈ N to minimize the upper bound in the RHS of Equation (13). The RHS of Equation (13) is
minimized (up to log n-factor) by choosing
N Xn 2 ((2β∧i 讲and β=(2+α(α‰)1
(15)
which results in N N n2(2β+1) 20d+d
. At these optimal values, Equation (13) becomes
max kQk+1 — T*Qk kμ . e0 + n-2(2α2++d + d)	log n + n-1/2 plog(1∕δ) + log log n, (16)
k
where we use inequalities n-2(1-2α)-2 ≤ n- 1(I-βd) X N-a/d =n-2(2<2+d + α)
Now, for any e > 0, we set e0 = e/3 and let
n-1(2α+d + α) log n . e/3 and n-1/2 Plog(1∕δ) + Ioglog n . e∕3.
It then follows from Equation (16) that with probability at least 1 — δ, we have maxk IQk+1 —
T*Qk∣μ ≤ e if n simultaneously satisfies Equation (14) with e0 = e/3 and
2α I d
1 1、2α + d ɪ ɑ	2。	d	]
n & (	)	(log2 n) 2α+d + α and n & -2 (log(1∕δ) + log log n).	(17)
Next, we derive an explicit formula of the sample complexity satisfying Equation (14). Using Equa-
tions (13), (17), and (15), we have that n satisfies Equation (14) if
n
n
n
& ± [n*2⅛ (log2 n + log(1∕e))],
& (表)1+α ,
& *(log(1∕δ)+loglog n).
(18)
Note that β ≤ 1∕2 and d ≤ 2; thus, we have
(γ - 2β + 1 d
2	2α + d
Hence, n satisfies Equations (17) and (18) if
≤ 1 + - ≤ 3.
α
1+α	ι
n &	log6 n+Hlog(1∕δ)+loglog n).
B Technical Lemmas
Lemma B.1 (Bartlett et al. (2005)). Let r > 0 and let
F⊆ {f :X → [a, b] :V[f(X1)] ≤r}.
22
Under review as a conference paper at ICLR 2022
1.	For any λ > 0, we have with probability at least 1 - e-λ,
sup (Ef - Enf) ≤ inf ( 2(I + α)E [RnF] + ∖---+ (b - a) (3 +-) -
f∈F	α>0	n	3 α n
2.	With probability at least 1 - 2e-λ,
sup (Ef - Enf) ≤ inf 22(1 + F Eσ[RnF] + r∕2rλ +(b - a) f 1 + 1+	1+ °、) λ
n	σn
f∈F	α∈(0,1)	(1 - α)	n	3 α	2α(1 - α)	n
Moreover, the same results hold for supf ∈F (Enf - Ef).
Lemma B.2 (Gyorfi et al. (2002, Theorem 11.6)). Let B ≥ 1 and F be a set offunCtions f : Rd →
[0, B]. Let Z1, ..., Zn be i.i.d. Rd -valued random variables. For any α > 0, 0 < < 1, and n ≥ 1,
we have
P n	1 Pn=I f(Zi)- E[f(Z)]	、ɪ V∕11?入〃ae F 7n -1	-3e2αnλ
VuF α + 1 Pn=ι f (Zi)+ E[f (Z)] > e∫ ≤ 4EN(彳,FlZ1,n k ∙k1)exp t^40B^ J
Lemma B.3 (Contraction property (Rebeschini, 2019)). Let φ : R → R be a L-Lipschitz, then
EσRn (φ OF) ≤ LEσRnF.
Lemma B.4 (Lei et al. (2016, Lemma 1)). Let F be a function class and Pn be the empirical
measure Supported on Xi,..., Xn 〜μ, then for any r > 0 (which can be Stochastic w.r.t Xi), we
have
Eσ Rn{f ∈ F ： kf kn ≤ r} ≤ inf ,σ Rn {f ∈F-F : kf kμ ≤ e} + J2r log N(e/2, F, k∙kn) .
n	>0	n
Lemma B.5 ( Lei et al. (2016, modification)). Let X1, ..., Xn be a sequence of samples and Pn be
the associated empirical measure. For any function class F and any monotone sequence {ξk}k∞=0
decreasing to 0, we have the following inequality for any non-negative integer N
Eσ Rn{f ∈ F ： kf kn ≤ ξθ} ≤ 4 XX ξk-1∕ Iog ". ,F,".'' + ξχ .
k=1	n
Lemma B.6 (Pollard’s inequality). Let F be a set of measurable functions f : X → [0, K] and let
e > 0, N arbitrary. If{Xi}iN=1 is an i.i.d. sequence of random variables taking values in X, then
P (sup N XX f (Xi) - E[f(Xi)] >e) ≤ 8E[N(e∕8, F"N)] e-N⅛.
Lemma B.7 (Properties of (bracketing) entropic numbers). Let e ∈ (0, ∞). We have
1.	H (e, F ,k∙k) ≤ H[](2e, F ,"•");
2.	H (e, F∣{xi }n=ι, nT∕p∙k∙kp) = H (e, F, k∙"p,n) ≤ H (e, FK”*, "∙"∞) ≤ H (e, F, k•
k∞)for all {xi}in=1 ⊂ dom(F).
3.	H (e, F-F, IHD ≤ 2H (e∕2, F, ∣∣∙ k)), where F-F := {f - g : f,g ∈F}.
Lemma B.8 (Entropic number of bounded Besov spaces (NickI & Potscher, 2007, Corollary 2.2)).
For 1 ≤ p, q ≤ ∞ and α > d∕p, we have
H[](e,Bα,q(X ),k∙1∞) . e-d∕α.
23
Under review as a conference paper at ICLR 2022
Lemma B.9 (Approximation power of deep ReLU networks for Besov spaces (Suzuki, 2018, a mod-
ified version)). Let 1 ≤ p,q ≤ ∞ and α ∈ (p∧∧2, ∞). For sufficiently large N ∈ N, there exists a
neural network architecture Φ(L, m, S, B) with
L logN,m NlogN,S N, andB Nd-1+ν-1
where V := O-δ and δ := d(p-1 一 (1 + [a[)-1)+ such that
sup	inf
f*∈B⅛q (X " ∈φ(L,w,s,B)
kf 一 f*k∞ . Niad
24