Under review as a conference paper at ICLR 2022
Learning Sampling Policy for Faster Deriva-
tive Free Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Zeroth-order (ZO, also known as derivative-free) methods, which estimate a noisy
gradient based on the finite difference with two function evaluations, have at-
tracted much attention recently because of its broad applications in machine learn-
ing community. The function evaluations are normally requested on a point plus
a random perturbations drawn from a (standard Gaussian) distribution. The accu-
rateness of noisy gradient highly depends on how many perturbations randomly
sampled from the distribution, which intrinsically conflicts to the efficiency ofZO
algorithms. Although there have been much effort made to improve the efficiency
ofZO algorithms, however, we explore anew direction, i.e., learn an optimal sam-
pling policy based on reinforcement learning (RL) to generate perturbation instead
of using totally random strategy, which make it possible to calculate a ZO gradient
with only 2 function evaluations. Specifically, we first formulate the problem of
learning a sampling policy as a Markov decision process. Then, we propose our
ZO-RL algorithm, i.e., using deep deterministic policy gradient, an actor-critic RL
algorithm to learn a sampling policy which can guide the generation of perturbed
vectors in getting ZO gradients as accurate as possible. Since our method only
affects the generation of perturbed vectors which is parallel to existing efforts of
accelerating ZO methods such as learning a data driven Gaussian distribution, we
show how to combine our method with other acceleration techniques to further
improve the efficiency of ZO algorithms. Experimental results with different ZO
estimators show that our ZO-RL algorithm can effectively reduce the query com-
plexity of ZO algorithms especially in the later stage of the optimization process,
and converge faster than existing ZO algorithms.
1 Introduction
Gradient based optimization is the dominant method in machine learning. However, in many fields
of science and engineering, explicit gradient information Vf (x) is difficult or even infeasible to ob-
tain. Zeroth-order (ZO, also known as derivative-free) optimization, where the optimizer is provided
with only function values f(x) (zeroth-order information) instead of explicit (first-order) gradients
Vf (x), has attracted an increasing amount of attention. ZO optimization (ZOO) can address a wide
range of problems and has been studied in a large number of fields such as optimization, online
learning and bioinformatics (Koch et al., 2018; Mania et al., 2018; Vemula et al., 2019). One of the
famous applications of ZOO is to generate prediction-evasive adversarial examples in the black-box
setting (Liu et al., 2018; Papernot et al., 2017), e.g., crafted images with imperceptible perturbations
to deceive a well-trained image classifier into misclassification.
The ZO gradient which is also referred to as evolutionary strategies (Huning, 1976) is calculated
based on the finite difference approach (Strikwerda, 2004) and normally with the following formula.
1q
V f (X) = — £[f (x + μui) — f (x)]ui	(1)
μq i=1
where μ > 0 is the smoothing parameter, {ui}q=1 are the random perturbed vectors (direc-
tions) usually drawn from a distribution p(u) which could be standard Gaussian or uniform dis-
tribution on a unit sphere (Nesterov & Spokoiny, 2017; Duchi et al., 2012). Essentially, if the
smoothing parameter μ approaches infinitesimal, lim*→o f (x + μu) 一 f (x) is exactly calculating
1
Under review as a conference paper at ICLR 2022
Perturbed Vectors
ZO Gradient
Direction
Standard
Gaussian
Distribution
Random
Sampling
RL based
Policy
True Gradient
Direction
Learned Gaussian
Distribution
signSGC
RL based
Policy
Perturbed
Vectors u
Figure 1: (a) Comparison of the ZO gradient directions obtained by sampling perturbed vectors
from the standard Gaussian distribution and the learned Gaussian distribution. (b) Comparison of
the ZO gradient directions obtained by a random sampling policy and a RL based policy. (c) The
architecture of ZO optimizer.
Random
Sampling
Distribution
"((Ud)
Distribution
2θ gradient
estimator
the directional derivative Vuf (x) along a direction u, which means We use q directional deriva-
tives to approximate the ground truth gradient Vf(x) due to Vf(x) = JU(Vuf(x)u)p(u)du.
From this perspective, the accurateness of ZO gradient highly depends on how many perturbed
vectors randomly sampled from the distribution. For a practical ZOO algorithm, e.g., vanilla
ZOO Algorithm 1, we should balance the number q of perturbed vectors and the accurateness
of ZO gradient to make the overall query complexity* 1 1 of ZOO algorithm as optimal as possible.
To improve the efficiency of ZO algorithms,
there have been much effort made in the com-
munities of machine learning and optimiza-
tion. Specifically, different from sampling per-
turbed vectors using a standard Gaussian dis-
tribution N(0, I), a few ZOO algorithms (Ma-
heswaranathan et al., 2019; Ruan et al., 2019)
use a learned non-isotropic Gaussian distri-
bution N(0, Σ) to generate perturbed vectors.
The co-variance matrix Σ of these Gaussian
may not be a scale of the identity matrix (il-
lustrated in Fig. 1.a). The rationale of these
methods is easier to illustrate using the black-
box adversarial attack example. For this type of task, there is usually a well-defined significant
subspace that is more prone to attack, and perturbed directions through this subspace naturally leads
to faster learning convergence. Similar to this, Evolutionary Strategies (ES) such as Natural ES
(Wierstra et al., 2008), CMA-ES (Hansen, 2006), and Guided ES (Maheswaranathan et al., 2019)
were also proposed to guide the sampling direction in ZOO.
As discussed above, the learned sampling distribution is beneficial to calculate a more accurate ZO
gradient. Parallel to learn a sampling distribution, a natural question is whether it is feasible to
learn a sampling policy to generate one gradient oriented perturbed vectors instead of using multiple
Algorithm 1 Vanilla ZOO Algorithm
Input: Smoothing parameter μ, learning rate η
and q.
Output: x ∈ Rd
1:
2:
3:
4:
5:
for k = 0 to K - 1 do
Sampling q perturbed vectors from the
standard Gaussian distribution Ui 〜
N (0, Id).
Calculating the ZO gradient Vf (Xk).
Obtain the next update xk+1 = xk -
ηVf(xk).
end for
1The overall number of queries of function evaluations is called query complexity.
2
Under review as a conference paper at ICLR 2022
randomly sampled perturbed vectors to approximate the ground truth gradient. The quick answer
is yes because it can be easily proved that there must exist one perturbation vector u0 such that
▽u0f (x)u0 = Ju(Vuf (x)u)p(u)du = Vf (x). If We could find such perturbation vector u0, the ZO
gradient can be directly approximated with only two queries as follows.
Vf (x) = 1[f(x + μu0) - f (x)]u0	(2)
μ
The subsequent issue is hoW to find this kind of perturbed vector u0 to approximate the gradient as
accurate as possible, Which is still an open problem and even not be Well noticed in the community
as far as We knoW. In this paper, We Will take the approach of Reinforcement Learning (RL) to learn
a good sampling policy to solve this issue.
As it regains its popularity recently because of star projects like AlphaGo (Wang et al., 2016),
DQN (Mnih et al., 2015) and AlphaStar (Arulkumaran et al., 2019), RL (Mnih et al., 2015) is a
formal frameWork in Which a learning agent can continuously optimize its policy to obtain higher
cumulative reWards While interacting With an uncertain environment. If We take the ZOO algorithm
as the RL agent Whose goal is to optimize a ZOO problem as fast as possible. The target function f,
current parameter xt and the calculation mechanism of noisy gradient (i.e., Eq. (1)) are all parts of
the environment. Each step, the agent picks a perturbation vector and pass it over to the environment
for execution. The environment takes its step to update xk and output xk+1 as the neW observation
for the RL agent. The RL agent then receives a reWard of f (xk+1) - f(xk). This is a typical
RL problem, Which can be solved by any model-free reinforcement learning method. Ideally, With
sufficient learning, the agent Will become smart enough in selecting a good perturbation point upon
every move it make, Which is essentially a good sampling policy. We provide Fig 1.(a) to intuitively
shoW the benefit of using the learned sampling distribution, and also provide Fig 1.(b) to intuitively
demonstrate the benefit of using the RL based policy. The effective prediction of gradient oriented
perturbed vector by RL can reduce the query complexity and speed up the convergence of the ZOO
algorithm.
In this paper, We propose a zero-order algorithm based on reinforcement learning (ZO-RL) to learn
the sampling policy in ZOO using the policy gradient algorithm. Specifically, We use an actor-critic
algorithm called deep deterministic policy gradient (DDPG) (Lillicrap et al. (2015), With tWo neural
netWork function approximators. Compared With the stochastic policy gradient algorithm, deter-
ministic policies have the advantages of requiring less data to be sampled, and stable performance
in a series of tasks With continuous action spaces. The RL based policy guide the optimizer to esti-
mate more accurate ZO gradients in the parameter space to reduce the variance. Especially, We can
combine our ZO-RL With existing the ZOO algorithms that utilize the improved parameter update
rule and learned sampling distribution. Experimental results for different ZOO problems shoW that
our ZO-RL algorithm can effectively reduce the variances of ZO gradient by learning the sampling
policy, and converge faster than existing ZOO algorithms in different scenarios.
Contributions. The main contributions of this paper are summarized as folloWs.
1.	We propose to learn the sampling policy by reinforcement learning instead of using random
sampling as in the standard ZOO algorithms to generate perturbed vectors.
2.	We conduct extensive experiments to shoW that our ZO-RL algorithm can effectively re-
duce the variances of ZO gradients by learning a sampling policy, and converge faster than
existing ZOO algorithms in different scenarios.
2	Related Work
In order to construct ZO gradients that are closer to the true gradient direction and enable the ZOO
algorithm to obtain convergence With feWer queries, existing Works focus on learning an adaptive
Gaussian distribution N(0, Σ) as discussed previously. Thus, they sample the perturbed vectors by
Ui 〜N(0, Σ) that the Co-Variance matrix Σ may not be a scale of the identity matrix. Specifically,
MahesWaranathan et al. (2019) utilized evolution strategies to let co-variance matrix Σ track a loW-
dimensional subspace, Which is related With recent history of ZO gradients during optimization.
Ruan et al. (2019) utilized RNN to learn an adaptive co-variance matrix Σ and dynamically guide the
sampling distribution. By learning the significant sampling distribution, more accurate ZO gradient
3
Under review as a conference paper at ICLR 2022
can be obtained for a fixed query budget, which can improve the convergence of ZOO algorithms.
In this paper, from different angle, we apply RL to learn a smarter sampling policy to replace the
plain random sampling used in existing methods. It also should be noted that, our proposed ZO-RL
algorithm is paralleled to the existing ZOO algorithms of learning a sampling distribution. Thus,
it is possible to combine them together to generate a better ZO gradient estimation with only two
queries.
As discussed above, our paper considers using RL to accelerate the ZO algorithms. Interestingly,
there is one opposite research direction compared to our paper, i.e., utilizing ZO algorithms to op-
timize RL model (Mania et al., 2018; Vemula et al., 2019). Specifically, policy gradient methods
(Mnih et al., 2016) is a popular type of RL approach used in complex and uncertain environments
(Arulkumaran et al., 2019), which relies on random exploration in the sampling distribution to learn
sampling policy for guiding sampling direction, and directly updates a RL agent in the policy space
using stochastic gradient descent. In particular, Mania et al. (2018); Vemula et al. (2019) used ZO
gradients instead of explicit gradients to train static, linear policies for continuous control problems
to achieve higher sample efficiency.
3	Learning Sampling Policy in Zeroth-Order Optimization
We consider the problem of finding a sampling
policy that encourages the sampled perturbed
vectors to be more efficient in calculating the
ZO gradients and enable the ZOO algorithm to
obtain convergence with fewer queries. In the
this paper, we use RL to learn a smarter sam-
pling policy compared to plain random sam-
pling policy. RL provides a framework in
which the agent can learn the best action to take
by subsequently receiving rewards from the en-
vironment with which it interacts. We view
the each query of ZOO algorithm as the exe-
cution of a fixed policy in a MDP as a tuple
(S,A,Psa,R):
Figure 2: Illustration of learning sampling policy
for ZOO base on RL.
1.	State space S ⊂ Rq: We choose sk = xk ∈ S to describe the current state ofZO algorithm,
where xk is the point location for the kth iteration.
2.	Action space A ⊂ Rp: We choose ak = {u0}k ∈ A as the action, where {u0}k is the
perturbed vector.
3.	Transition probability Psa = P(∙∣s, a): Unknown in the model-free RL.
4.	Reward function R(s, a) : S ×A → R: We consider the reward function R(sk, ak) = rk =
f(xk+1) - f(xk) as the difference between the function values at the current point location
xk and the immediately preceding point location xk+1 after the action ak is performed,
which encourages the learned policy to reach the minimum of the function value as quickly
as possible.
Through the framework of MDP, we propose aZOO algorithm based on reinforcement learning (ZO-
RL). At each query of ZO-RL algorithm, the agent outputs the perturbed vectors {ui}k according
to the current state xk . Then, the agent receives rewards and next state xk+1 by interacting with the
environment, and learns the sampling policy to maximize rewards. We show the illustration of ZOO
base RL in Figure 2.
In the following, we first introduce the principle of our ZO-RL algorithm. Then, we introduce the
network structure and the batch normalization technique which are used in our ZO-RL algorithm.
Finally, we discuss to combine our ZO-RL with existing ZOO algorithms to further accelerate the
existing ZOO algorithms.
4
Under review as a conference paper at ICLR 2022
Figure 3: Illustration of of ZO-RL algorithm.
3.1	Principle of our ZO-RL Algorithm
Since the action space is continuous in ZOO, we use the deterministic sampling policy. Compared
with the stochastic policy gradient algorithm, the deterministic policy has the advantages of requiring
less data to be sampled so that it achieve higher efficiency for the algorithm, and performing stably
in a series of tasks with continuous action space. Thus, to find the optimal policy to approach
the true gradient direction, we use deep deterministic policy gradient (DDPG) to learn sampling
policy π. DDPG is an actor-critic and model-free algorithm (Konda & Tsitsiklis, 2000) for RL over
continuous action spaces and output deterministic actions in a stochastic environment to maximize
cumulative rewards.
The DDPG has two neural network function approximators. One is called the actor network which
learns a deterministic sampling policy π (x∣θπ) with neural network weights θπ. The other is called
the critic network, which outputs a state-action value function Q(s,a∣θQ) with neural network
weights θ Q to evaluate the value of the action performed. In addition, DDPG creates a copy of
the actor and critic networks, Q0(s, a∣θQ0) and π0(x∣θπ0) respectively, that are used for calculating
the target values. The weights of these target networks are updated by making them slowly track
the learned networks: θ0 → τθ0 + (1 - τ)θ0 with τ 1. This means that the target values are
constrained to change slowly, greatly improving the stability of learning. This simple change moves
the relatively unstable problem of learning the action-value function closer to the case of supervised
learning.
At each iteration of ZO-RL, we use actor network to output action {ui} according to current state xk.
Then, we transfer the action {ui} to ZO Oracle, calculate the ZO gradient g and output the next state
xk+1, and use the critic network to output the reward of this action. We call {{ui}k, xk, xk+1, rk}
a transition. Good transitions can accelerate the learning speed of the agent. Thus, in order to
better initialize actor network, we alternately use the random sampling policy to sampling perturbed
vectors and interact with the current environment to obtain ZO gradient estimator g. We store these
transitions generated by random sampling policy into relay memory buffer, and use them to update
the actor network. We use a cosine similarity ρ to calculate the similarity of gradient directions
obtained by plain random sampling and our sampling policy π (x∣θπ):
=g ∙ g
P = ≡H≡
(3)
where g is the ZO gradient estimator defined in (1) with the perturbed vectors generated by random
sampling policy, and g is the ZO gradient estimator defined in (2) with the perturbed vector generated
by our RL based policy. Set a threshold , if P < , we use the random sampling policy to generate
perturbed vectors, otherwise we use the learned policy π (x∣θπ) to generate perturbed vector.
At each iteration, we minimize a squared-error loss L to update the critic network parameter:
min L =
θQ
k=1
(4)
5
Under review as a conference paper at ICLR 2022
where yk represents the TD target denoted as
yk = Yk + γQ0"+ι,∏0(χk+ι ∣θπj∣θQ')	(5)
We maximize the cumulative reward using a sampled policy gradient to the actor network parameter:
1N
Vθ∏ J ≈ N £ V{ui}Q(s,alθQ)Is=Xi,a=∏(χi) ∙ Vθ∏ π(xlθπ )|Xi	⑹
where J = Es〜β,a〜∏ [R(s, a)] represents the expected cumulative reward, β is the distribution of
state space.
We show the illustration of our ZO-RL algorithm in Fig. 3 and summarize our ZO-RL algorithm
in Algorithm 2. Note that lines 4-6 of Algorithm 2 corresponds to the vanilla ZO algorithm (i.e.,
Algorithm 1). This is because the ZO estimator with RL is not accurate in the early stage. However,
if ρ > which means that the accurateness of ZO estimator with RL is acceptable, the updating
rules of ZOO immediately switch to the ZO estimator with RL.
Algorithm 2 Zeroth-Order Optimization for Reinforcement Learning
Input: Hyper-parameter G smoothing parameter μ, the number of sampled perturbed vectors q and
learning rate η, mini-batch size N.
Output: Sampling policy π(x∣θπ).
1:	Initialize ρ = 0.
2:	for k = 1 to K do
3:	if ρ < then
4:	Randomly sample q perturbed vectors {ui }iq=1 from the standard Gaussian distribution
N(0,Id).
5:	Calculate the ZO gradient estimator gk according to (1) with the q perturbed vectors.
6:	Obtain the next update xk+i = Xk 一 η ∙ gk.
7:	end if
8:	Sample perturbed vectors U according to the sampling policy ∏(χk ∣θπ).
9:	Calculate the ZO gradient estimator gk according to (2) with the perturbed vectors u0.
10:	if ρ < then
11:	Update P based on gk and gk according to (3).
12:	else
13:	Obtain the next update Xk+i = Xk 一 η ∙ gk.
14:	end if
15:	Store transition {{ui}k/{u0}k, Xk, Xk+i, rk} in a replay memory buffer.
16:	Observe N transitions from replay memory buffer to update the actor network and critic
network.
17:	end for
3.2	Network Structure and Batch Normalization
The choice of the structure of the critic and actor nets is important because they are used not only to
evaluate sampling policies, but also to learn sampling policies. We choose the convolutional neural
network (CNN) (Sezer & Ozbayoglu, 2018) both for the critic net and the actor net.
The difficulty of policy gradient method is due to the lack of information gradient of policy perfor-
mance. Specifically, gradients may not exist due to non-smoothness of the environment or policy,
or may only be available as high-variance estimates because the environment usually can only be
accessed via sampling. In order to make the problem smooth and have a way of to estimate its
gradients, we add noise in action space, which is done by sampling the actions from an appropriate
distribution. In our ZO-RL, We select action {ui}k = π(xk∣θπ) + Nk according to to the current
policy and exploration noise, where Nk(0, I) is a standard Gaussian distribution.
The parameters of the ZO optimizer have different descent rates in different dimensions and the
range may be different in different environments. This may make it difficult for the network to learn
efficiently and find hyper-parameters that generalize the scale of state values in different environ-
ments. One way to address this issue is to manually scale features so that they are in a similar range
6
Under review as a conference paper at ICLR 2022
across environments and units. We address this problem by adapting one of the latest techniques in
deep learning, called batch normalization (Santurkar et al., 2018). This technique normalizes each
dimension of a sample in a mini-batch to have unit mean and variance. In addition, batch normal-
ization maintains a running average of the mean and variance to be used for normalization during
testing. In deep networks, batch normalization is used to minimize the co-variance bias during
training, by ensuring that each layer receives whitened inputs.
3.3	Combining Our ZO-RL with Existing ZOO Algorithms
In this subsection, we discuss how to combine our ZO-RL with an existing ZOO algorithm based
on the parameter update rule or the learned sampling distribution. For example, (Ruan et al., 2019)
proposed a ZO optimization algorithm called ZO-LSTM, which replaces parameter update rule as
well as guided sampling rule to sample the perturbed vectors with learned recurrent neural networks
(RNN). Especially, they updated the parameter through a Long Short-Term Memory (LSTM) net-
work called UpdateRNN:
一 ,ʌ ʌ, ..
Xt = χt-ι + UpdateRNN (57 f(χt))	(7)
where xt is the optimizer parameter at iteration t. UpdateRNN can reduce the negative impact of
high variances of ZO gradient due to long-term dependence, in addition to learning to compute
parameter updates adaptively by exploring the loss landscape. They use another LSTM network
called QueryRNN to learn the sampling distributions. They dynamically predict the converiance
matrix Σk :
∑t = QUeryRNN(Vf(xt), ∆χt-ι])	(8)
QueryRNN can increase the sampling probability in the direction of the bias of the estimated gradi-
ent or the parameter Update of the previoUs iteration.
AlthoUgh they considered both the sampling distribUtion and the parameter Update rUle, they still
Used random sampling for the pertUrbed vectors. Using the RL based policy on the learned sam-
pling distribUtion can fUrther speed Up the convergence of ZOO algorithms. ThUs, we can combine
oUr ZO-RL algorithm with ZO-LSTM algorithm. Especially, we first train the UpdateRNN Using
standard GaUssian random vectors as qUery directions. Then we freeze the parameters of the Up-
dateRNN and train the QUeryRNN. Finally, we Use the previoUs work as a warm start and Use oUr
ZO-RL in the pre-learning distribUtion to learn the sampling policy. In addition, other ZOO algo-
rithms based on parameter Update rUles sUch as (Lian et al., 2016; Chen et al., 2017), can be directly
combined with oUr ZO-RL algorithm.
4 Experiments
In this section, we empirically demonstrate the sUperiority of oUr proposed ZO optimizer on a prac-
tical application (black-box adversarial attack on MNIST dataset) and a synthetic problem (non-
convex binary classification problems on benchmark datasets). To show the effectiveness of the
learned sampling policy, we compare the convergence behavior of oUr proposed ZO optimizer with
existing ZOO algorithms Under a same qUery nUmber. To show that oUr algorithm can estimate the
groUnd-trUth gradient direction more accUrately, we coUnt the cosine similarity between the groUnd-
trUth gradient direction and the ZO gradient direction compUted by existing ZO algorithms and oUr
proposed ZO optimizer.
Specifically, we obtain ZO gradient estimator along sampled directions via ZO Oracle. Since oUr
algorithm is the first one to learning the sampling policy, we compare the performance of the ZO
gradient estimators sampled form the standard GaUssian distribUtion and two learned GaUssian dis-
tribUtions, i.e. Using different covariance matrix Σ. In addition, we compare the algorithm of
synchronoUsly learning sampling and distribUtion policy by combining oUr algorithm with other
algorithms. The five algorithms for calcUlating ZO gradient estimators are sUmmarized as follows: 1 2 3 *
1. ZO-GS (Wang et al., 2019): Randomly Sampling the pertUrbed vectors ui from a standard
GaUssian distribUtion.
2. ZO-LSTM (RUan et al., 2019): They learned the GaUssian sampling rUle and dynamically
predicted the covariance matrix Σ for qUery directions with recUrrent neUral networks.
3. GUided ES (Maheswaranathan et al., 2019): They let the covariance matrix Σ be related
with the recent history of sUrrogate gradients dUring optimization.
7
Under review as a conference paper at ICLR 2022
4.	ZO-RL: Our proposed ZO algorithm learns the sampling policy through reinforcement
learning.
5.	ZO-RL-LSTM: Our proposed ZO algorithm combined with ZO-LSTM to learn sampling
policy on a learned Gaussian distribution.
4.1	Implementation
For each task, we tune the hyper-parameters of baseline algorithms to report the best performance.
We coarsely tune the constant δ on a logarithmic range {0.01; 0.1; 1; 10; 100; 1000} and set the
learning rate of baseline algorithms to η = δ∕d, where d is the dimension of dataset. We set the
smoothing parameter μ = 0.01 in all experiments. To ensure fair comparison, all optimizers use the
same number of query directions in each iteration to obtain the ZO gradient.
4.2	Adversarial Attack to Black-box Models
We consider generating adversarial examples to attack black-box DNN image classifier and for-
mulate it as a zeroth-order optimization problem. The targeted DNN image classifier F (x) =
[F1,F2,…，FK] takes as input an image X ∈ [0,1]d and outputs the prediction scores of K classes.
Given an image xo ∈ [0,1]d and its corresponding true label to ∈ [1, 2, ∙∙∙ , K], an adversarial
sample x is visually similar to the original image x0 but leads the targeted model F to make wrong
prediction other than to. The black-box attack problem is normally formulated as follows.
max{Ft0 (x) - max Fj (x), 0} + ckx - xo kp	(9)
x	j 6=t0
where the first term is the attack loss which measures how successful the adversarial attack is and
penalizes correct prediction by the targeted model. The second term is the distortion loss (p-norm
of added perturbation) which enforces the perturbation added to be small and c is the regularization
coefficient. In our experiment, we use `1 norm (i.e., p = 1), and set c = 0.1 for MNIST attack
task. Due to the black-box setting, one can only compute the function values of the above objective,
which leads to ZOO problems (Chen et al., 2017). Note that attacking each sample xo in the dataset
corresponds to a particular ZOO problem instance, which motivates us to train aZO optimizer offline
with a small subset, and apply it to online attack to other samples with faster convergence (which
means lower query complexity) and lower final loss (which means less distortion). We randomly
select 50 images that are correctly classified by the targeted model in each test set to train the
optimizer and select another 50 images to test the learned optimizer. The number of sampled query
directions is set to q = 20 for MNIST.
4.3	Non-Convex B inary Classification Problems
We consider a binary classification problem with a non-convex least squared loss function
mi□w∈Rd * Pn=ι(yi - 1/(1 + e-w Xi))2. Here (χi,yi) is the ith data sample containing feature
xi ∈ Rd and label yi ∈ {-1, 1}. We compare the algorithms on benchmark datasets (heat scale,
german and a9a2). All the algorithms only access to the ZO oracle of function value evaluations.
We use the same set of hyper-parameters for different datasets and repeated runs in the experiments.
The number of query directions are set to q = 20. For each dataset, we repeat the experiment 10
times and report the average and the standard deviation.
4.4	Discussion and Analysis
Fig. 4 shows the black-box attack loss versus query number using different ZOO algorithms. Fig.
5 shows the non-convex least squared loss versus query number using different ZOO algorithms.
The loss curves are averaged over 10 independent random trails and the shaded areas indicate the
standard deviation. The results clearly show that our ZO-RL algorithm can effectively reduce the
query complexity of ZOO algorithms especially in the later stage of the optimization process, and
our ZO-RL-LSTM can always obtain the best results by combining learned sampling policy and
sampling distribution. This is due to the fact that our ZO-RL algorithm learn a smarter sampling
policy though RL instead of random sampling.
Fig. 6 plots the cosine similarities between ZO gradient estimator and ground-truth gradient for
non-convex binary classification problems. The cosine similarities curves are averaged over 10
2http://archive.ics.uci.edu/ml/datasets.html
8
Under review as a conference paper at ICLR 2022
Query Number
(a) MNIST TEST 1
Query Number
(b) MNIST TEST 2
Query Number
(c) MNIST TEST 3
Query Number
(a) a9a
Figure 4: Adversarial attack to black-box models.
Query Number
(b) german
(c) heat scale
1
<a.
0.8
口
⅛ 0.6
.目
7 0.4
υ
另0.2
U
...ZO-SGD
...Guided ES
...ZO-LSTM
---ZO-RL
---ZO-RL-LSTM
0 ------'------'------'-----'------'
0	1000	2000	3000	4000	5000
Query Number
(a) a9a
0.6
(b) german
Figure 5: Non-convex binary classification problems.
0.5
0.4
⅛
I03
S
ω 0.2
.S
爸0.1
0 --------'------'-------'-------'------'
0	1000	2000	3000	4000	5000
Query Number
0.8
台
⅛0.6
.目
S 0.4
ɔ
.S
S 0.2
U
ZO-SGD
GUided ES
ZO-LSTM
ZO-RL
ZO-RL-LSTM
0 --------'------'-------'-------'------'
0	1000	2000	3000	4000	5000
Query Number
(c) heat scale
Figure 6: Cosine similarities between ZO gradient estimator and ground-truth gradient for non-
convex binary classification problems.
independent random trails and the shaded areas indicate the standard deviation. The results show
that the direction of the ZO gradient estimator generated by our ZO-RL algorithm is closer to the
direction of the ground-truth gradient compared to other ZOO algorithms. In the later stage of the
optimization process, the convergence of ZO gradient leads to the reduction of the reward obtained
by our ZO-RL algorithm in exploring action space. Thus, the cosine similarity decreases after the
convergence of the ZOO algorithm.
5 Conclusion
We proposed a new reinforcement learning based sampling policy for generating the perturbations
in ZOO instead of using the existing random sampling. The learned sampling policy guides the
perturbation (direction) in the parameter space to estimate a ZO gradient as accurate as possible.
Since our method only affects the generation of perturbed vectors, it can be used with other accel-
eration techniques to further improve the efficiency of ZO optimization. Especially, our ZO-RL can
be combined with the existing ZO algorithms that could further accelerate them. Experimental re-
sults on different ZOO algorithms show that our ZO-RL algorithm can effectively reduce the query
complexity of ZO algorithms especially in the later stage of the optimization process, and converge
faster than existing ZO algorithms in different scenarios.
9
Under review as a conference paper at ICLR 2022
References
Kai Arulkumaran, Antoine Cully, and Julian Togelius. Alphastar: An evolutionary computation
perspective. In Proceedings of the genetic and evolutionary computation conference companion,
pp. 314-315, 2019.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26, 2017.
John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic
optimization. SIAM Journal on Optimization, 22(2):674-701, 2012.
Nikolaus Hansen. The cma evolution strategy: a comparing review. In Towards a new evolutionary
computation, pp. 75-102. Springer, 2006.
Alois Huning. Evolutionsstrategie. optimierung technischer systeme nach prinzipien der biologis-
chen evolution, 1976.
Patrick Koch, Oleg Golovidov, Steven Gardner, Brett Wujek, Joshua Griffin, and Yan Xu. Autotune:
A derivative-free optimization framework for hyperparameter tuning. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 443-452,
2018.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Xiangru Lian, Huan Zhang, Cho-Jui Hsieh, Yijun Huang, and Ji Liu. A comprehensive linear
speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-
order. Advances in Neural Information Processing Systems, 29:3054-3062, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Liu Liu, Minhao Cheng, Cho-Jui Hsieh, and Dacheng Tao. Stochastic zeroth-order optimization via
variance reduction method. arXiv preprint arXiv:1805.11811, 2018.
Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided
evolutionary strategies: Augmenting random search with surrogate gradients. In International
Conference on Machine Learning, pp. 4264-4273. PMLR, 2019.
Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, pp. 1805-1814, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937. PMLR, 2016.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519, 2017.
Yangjun Ruan, Yuanhao Xiong, Sashank Reddi, Sanjiv Kumar, and Cho-Jui Hsieh. Learning to
learn by zeroth-order oracle. arXiv preprint arXiv:1910.09464, 2019.
10
Under review as a conference paper at ICLR 2022
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? arXiv preprint arXiv:1805.11604, 2018.
Omer Berat Sezer and Ahmet Murat Ozbayoglu. Algorithmic financial trading with deep convolu-
tional neural networks: Time series to image conversion approach. Applied Soft Computing, 70:
525-538, 2018.
John C Strikwerda. Finite difference schemes and partial differential equations. SIAM, 2004.
Anirudh Vemula, Wen Sun, and J Bagnell. Contrasting exploration in parameter and action space:
A zeroth-order optimization perspective. In The 22nd International Conference on Artificial In-
telligence and Statistics, pp. 2926-2935. PMLR, 2019.
Fei-Yue Wang, Jun Jason Zhang, Xinhu Zheng, Xiao Wang, Yong Yuan, Xiaoxiao Dai, Jie Zhang,
and Liuqing Yang. Where does alphago go: From church-turing thesis to alphago thesis and
beyond. IEEE/CAA Journal of Automatica Sinica, 3(2):113-120, 2016.
Jun-Kun Wang, Xiaoyun Li, and Ping Li. Zeroth order optimization by a mixture of evolution
strategies. 2019.
Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies.
In 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational
Intelligence), pp. 3381-3387. IEEE, 2008.
11