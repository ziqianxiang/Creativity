Under review as a conference paper at ICLR 2022
Revisiting the Monotonicity Constraint in Co-
operative Multi-Agent Reinforcement Learn-
ING
Anonymous authors
Paper under double-blind review
Ab stract
QMIX, a popular MARL algorithm based on the monotonicity constraint, has been
used as a baseline for the benchmark environments, such as Starcraft Multi-Agent
Challenge (SMAC), Predator-Prey (PP). Recent variants of QMIX target relaxing
the monotonicity constraint of QMIX to improve the expressive power of QMIX,
allowing for performance improvement in SMAC. However, we find that such
performance improvements of the variants are significantly affected by various
implementation tricks. In this paper, we revisit the monotonicity constraint of
QMIX, (1) we design a novel model RMC to further investigate the monotonicity
constraint; the results show that monotonicity constraint can improve sample
efficiency in some purely cooperative tasks; (2) we then re-evaluate the performance
of QMIX and these variants by a grid hyperparameter search for the tricks; the
results show QMIX achieves the best performance among them, achieving SOTA
performance on SMAC and PP; (3) we analyze the monotonic mixing network
from a theoretical perspective and show that it can represent any tasks which can
be interpreted as purely cooperative. These analyses demonstrate that relaxing
the monotonicity constraint of the mixing network will not always improve the
performance of QMIX, which breaks our previous impressions of the monotonicity
constraints.
1	Introduction
Multi-agent cooperative games have many complex real-world applications such as, robot swarm
control [7; 34; 14], autonomous vehicle coordination [3; 38], and sensor networks [36], a complex
task always requires multi-agents to accomplish together. Multi-Agent Reinforcement Learning
(MARL), is used to solve the multi-agent systems tasks [34].
In multi-agent systems, a typical challenge is a limited scalability and inherent constraints on agent
observability and communication. Therefore, decentralized policies that act only on their local
observations are necessitated and widely used [37]. Learning decentralized policies is an intuitive
approach for training agents independently. However, simultaneous exploration by multiple agents
often results in non-stationary environments, which leads to unstable learning. Therefore, Centralized
Training and Decentralized Execution (CTDE) [10] allows for independent agents to access additional
state information that is unavailable during policy inference.
Many CTDE learning algorithms have been proposed for the better sample efficiency in cooperative
tasks[33]. Among them, several value-based approaches achieve state-of-the-art (SOTA) perfor-
mance [19; 30; 35; 20] on such benchmark environments, e.g., Starcraft Multi-Agent Challenge
(SMAC) [21], Predator-Prey (PP) [2; 16]. To enable effective CTDE for multi-agent Q-learning, the
Individual-Global-Max (IGM) principle [23] of equivalence of joint greedy action and individual
greedy actions is critical. The primary advantage of the IGM principle is that it ensures consistency
of policy with centralized training and decentralized execution. To ensure IGM principle, QMIX [19]
was proposed for factorizing the joint action-value function with the Monotonicity Constraint [30],
however, limiting the expressive power of the mixing network.
1
Under review as a conference paper at ICLR 2022
To improve the performance of QMIX, some variants of QMIX 1, including value-based ap-
proaches [35; 20; 30; 24] and a policy-based approach [37], have been proposed with the aim
to relax the monotonicity constraint of QMIX. However, while investigating the codes of these vari-
ants, we find that their performance is significantly affected by their implementation tricks. Therefore,
it is left unclear whether monotonicity constraint indeed impairs the QMIX’s performance.
In this paper, we investigate the monotonicity constraint and implementation tricks (Appendix B)
in cooperative MARL. (1) Firstly, we propose a novel method, RMC, for studying the impact of
monotonicity constraints in the some purely cooperative tasks, i.e, SMAC and Predator-Prey. The
experimental results show that monotonicity constraint significantly improves the performance of
RMC in SMAC and PP, and RMC outperforms the previous policy-based algorithms. (2) Next, we
re-test the performance of QMIX and its variants by a grid hyperparameter search for the tricks; and
the results show that the Fine-tuned QMIX can solve almost all hard scenarios of SMAC, achieving
SOTA performance. (3) Then, we discuss the properties of monotonicity constraints from a theoretical
perspective; and we prove that QMIX can represent any purely cooperative tasks.
All these results show that relaxing the monotonicity constraint of the mixing network will not always
improve the performance of QMIX; and the monotonicity constraint works well in multi-agent
tasks which can be interpreted as purely cooperative, even if the task can also be interpreted as
competitive.
2	Background
Dec-POMDP. We model a multi-agent cooperative task as decentralized partially observable Markov
decision process (Dec-POMDP) [15], which composed of a tuple G = hS, U, P, r, Z, O, N, γi.
s ∈ S describes the true state of the environment. At each time step, each agent i ∈ N := {1, . . . , N}
chooses an action ui ∈ U, forming a joint action u ∈ UN. All state transition dynamics are defined
by function P (s0 | s, u) : S × UN × S 7→ [0, 1]. Each agent has independent observation z ∈ Z,
determined by observation function O(s, i) : S × N 7→ Z. All agents share the same reward function
r(s, u) : S × UN → R and γ ∈ [0, 1) is the discount factor. The objective function, shown in Eq. 1,
is to maximize the joint value function to find a joint policy π = hπ1, ..., πni.
J (π) = EuI 〜π1,…,uN〜πN,s〜T
∞
γtrt st,ut1, . . . ,utN
t=0
(1)
Centralized Training and Decentralized Execution (CTDE). To resolve the non-stationary prob-
lem for MARL, CTDE is a popular paradigm [30] which allows for the learning process to utilize
additional state information [10]. Agents are trained in a centralized way, i.e., learning algorithms, to
access all local action observation histograms, global states, and sharing gradients and parameters. In
the execution stage, each individual agent can only access its local action observation history τi .
QMIX and Monotonicity Constraint. As a popular CTDE algorithm in cooperative MARL, QMIX
[19] learns a joint action-value function Qtot which can be represented in Eq. 2,
Qtot(s, u; θ, φ) =gφ s, Q1 τ1, u1 ; θ1 ,
∂Qtot (s, u; θ,φ) ≥ 0
∂Qi (Ti,u1θi)-,
.,Qn (T N ,uN; θN))
∀i ∈ N
(2)
where φ is the trainable parameter of the monotonic mixing network, which is a mixing network
with monotonicity constraint, and θi is the parameter of the agent network i. Benefiting from the
monotonicity constraint in Eq. 2, maximizing joint Qtot is precisely the equivalent of maximizing
individual Qi , resulting in and allowing for optimal individual action to maintain consistency with
optimal joint action. QMIX learns by sampling a multitude of transitions from the replay buffer and
minimizing the mean squared temporal-difference (TD) error loss:
1These algorithms are based on the mixing network from QMIX, so we call the variants of QMIX.
2
Under review as a conference paper at ICLR 2022
1b
L(θ) = 2 X (yyi - Qtot(S,u; θ, φ))2]
i=1
(3)
where the TD target value y = r + γ maxu0 Qtot (S0, u0; θ-, φ-) and θ-, φ- are the target network
parameters copied periodically from the current network and kept constant for a number of iterations.
However, the monotonicity constraint limits the mixing network’s expressiveness, which may fail to
12	-12	-12
-12	-O-	-O
-12	~0~~	~0
(a) Payoff matrix
-12	-12	-12
-12	-O-	^0-
-12	~0~~	~0
(b) QMIX: Qtot
Table 1: A non-monotonic matrix game. Bold text indicates the reward of the argmax action.
learn in non-monotonic cases [12] [20]. Table 1a shows a non-monotonic matrix game that violates
the monotonicity constraint. This game requires both robots to select the first action 0 (actions are
indexed from top to bottom, left to right) in order to catch the reward 12; if only one robot selects
action 0, the reward is -12. QMIX may learn an incorrect Qtot which has an incorrect argmax action
as shown in Table 1b.
3 Related Works
In this section, we introduce these variants of QMIX; and we provide the details of these algorithms
in Appendix E.
Value-based Methods To enhance the expressive power of QMIX, Qatten [35] introduces an attention
mechanism to enhance the expression of QMIX; QPLEX [30] transfers the monotonicity constraint
from Q values to Advantage values [13]; QTRAN++ [24] and WQMIX [20] further relax the
monotonicity constraint through a true value network and some theoretical constraints; however,
Value-Decomposition Networks (VDNs) [28] only requires a linear decomposition where Qtot =
PiN Qi , which can be seen as strengthening the monotonicity constraint.
Policy-based Methods LICA [37] completely removes the monotonicity constraint through a policy
mixing critic. For other MARL policy-based methods, DOP [31] learns the policy networks using the
Counterfactual Multi-Agent Policy Gradients (COMA) [6] with the Qi decomposed by QMIX.
To improve the efficiency of QMIX under parallel training 2, VMIX [26] combines the Advantage
Actor-Critic (A2C) [25] with QMIX to extend the monotonicity constraint to value networks, i.e.,
replacing the value network with the monotonic mixing network, as shown in Figure 1 and Eq. 4.
Figure 1:	Architecture for VMIX: ∣∙ | denotes absolute value operation, decomposing Vtot into Vi.
(4)
Vtot(s; θ, φ) = gφ (s, V1 (T1; θ1) ,...,V N(T N; θN))
第 ≥0,	∀i ∈N
2We find that this problem can be solved by training QMIX with Adam [8]
3
Under review as a conference paper at ICLR 2022
where φ is the parameter of value mixing network, and θi is the parameter of agent network. With the
centralized value function Vtot , the policy networks can be trained by policy gradient (Eq. 5),
1T
gi = ∣d∣ EE口 log 和(ut I τt)
|D| τ∈D t=0
At
θi
(5)
where At = r + Vtot(St+ι) - Vtot(St) is the advantage value function [13], and D denotes sampled
trajectories. At last, we briefly describe the properties of these algorithms in Table 2.
Algorithms	Type	Attention	Monotonic Constraint Strength	Off-policy
VDNs	Value-based	No	Very Strong	Yes
QMIX	Value-based	No	Strong	Yes
Qatten	Value-based	Yes	Strong	Yes
QPLEX	Value-based	Yes	Medium	Yes
WQMIX	Value-based	No	Weak	Yes
VMIX	Policy-based	No	Strong	No
LICA	Policy-based	No	No	No
RMC	Policy-based	No	Strong	Yes
Table 2: Properties of coopertive MARL algorithms. The analysis of the monotonicity constraint
strength is in the Appendix E.3.
All these algorithms show that their performance exceeds QMIX in SMAC, yet we find that they do
not consider the impact of various code-level optimizations (Appendix B) in the implementations.
Moreover, the performance of these algorithms is not even consistent in these papers. For
example, in papers [30] and [31], QPLEX and DOP outperform QMIX, while in paper [16], both
QPLEX and DOP underperform QMIX.
4 RMC
-----► Qι(τ1,u1)
-----► Qn(jh, ‰)
Gradient
Figure 2:	Architecture for RMC: ∣∙ ∣ denotes absolute value operation, implementing the mono-
tonicity constraint of QMIX. W denotes the non-negative mixing weights. Agent i denotes the policy
network which can be trained end-to-end by maximizing the Qtot
To study the impact of monotonicity constraint in pratical multi-agent tasks, we propose an novel
end-to-end Actor-Critic method, called RMC. Specifically, we use the monotonic mixing network as a
critic network, shown in Figure 2. Then, in Eq. 6, with a trained critic Qθπ estimate, the decentralized
policy networks πθi can then be optimized end-to-end simultaneously by maximizing Qθπ with the
policies ∏θ. as inputs; and the Ei [H (∏θ.( ∣ Zi))]] is the Adaptive Entropy [37]. We use a novel
two-stage approach to train the actor-critic network of RMC, as shown in Algo. 1.
max Et,st ,u1,...,τn Qc (st,π1ι	(∙	I	τt1)	,...,πθn	(∙	I	τtn))	+	Ei	[H	(πθi	(∙ I	τi))]]	(6)
As the monotonicity constraint on the critic (Figure 2) is theoretically no longer required as the critic
is not used for greedy action selection. RMC can swith to non-monotonic mode by removing the
absolute value operation in the monotonic mixing network. In this way, RMC can also be easily
extended to non-monotonic tasks. Beside, since RMC is trained end-to-end, it can also be used for
continuous control tasks.
4
Under review as a conference paper at ICLR 2022
5	Experiments Setup
In this section we first introduce the environments and the evaluation criteria for our experiments.
5.1	Benchmark Environment
These environments include the purely cooperative tasks, i.e, SMAC and DEPP; and the non-
monotonic matrix games.
StarCraft Multi-Agent Challenge (SMAC) is used as our main benchmark testing environment,
which is a ubiquitously-used multi-agent cooperative control environment for MARL algorithms [30;
19; 24; 20]. SMAC consists of a set of StarCraft II micro battle scenarios, whose goals are for
allied agents to defeat enemy agents, and it classifies micro scenarios into Easy, Hard, and Super
Hard levels. QMIX and VDNs achieves a 0% win rate in Super Hard scenarios such as, corridor,
3s5z_vs_3s5z, and 6h_vs_8z [21]. SMAC mainly uses a shaped reward signal calculated from the
hit-point damage dealt, some positive reward after having enemy units killed and a positive bonus for
winning the battle; Intuitively, these positive rewards can be interpreted as purely cooperative.
Difficulty-Enhanced Predator-Prey (DEPP) In vanilla Predator-Prey (PP) [11], three cooperating
agents control three predators to chase a faster robot prey (the prey acts randomly). The goal is to
capture the prey with the fewest steps possible. We leverage two difficulty-enhanced Predator-Prey
variants to test the algorithms: (1) the first Discrete Predator-Prey (Discrete PP) [2] requires two
predators to catch the prey at the same time to get a reward; (2) In the Continuous Predator-Prey
(Continuous PP), the prey’s policy is replaced by a hard-coded heuristic, that, at any time step, moves
the prey to the sampled position with the largest distance to the closest predator. DEPPs only reward
the predators when they catche preys, so the DEPPs can also be considered as purely cooperative
tasks.
We explain in detail in Sec. 7.2 why SMAC and DEPP can be interpreted as purely cooperative tasks.
Non-monotonic Matrix Game We evaluate performance of the algorithm in competitive cases in
two non-monotonic matrix games from [23] and (b) [12], shown in Sec. 6.4.
5.2	Parallel Sampling
To quickly sample from the complex environments, 8 rollout processes for parallel sampling are
used for SMAC and Discrete PP; and 4 rollout processes are used for Continuous PP. Specifically,
our experiments collect 10 million samples within 9 hours with a Core i7-7820X CPU and a GTX
1080 Ti GPU in SMAC. This also ensures that we have enough samples to evaluate the convergence
performance of the algorithms.
5.3	Evaluation Metric
Our primary evaluation metric is the function that maps the steps for the environment observed
throughout the training to the median winning percentage (episode return for Predator-Prey) of the
evaluation. Just as in QMIX [19], we repeat each experiment with several independent training runs
(five independent random experiments).
6	Experiments
In this section, we first study the effects of the monotonicity constraint in purely cooperative tasks
with RMC and VMIX. Next, as the past studies evaluate the performance of QMIX’s variants with
inconsistent implementation tricks, we retested their performance based on the normalized tricks.
Then, we also study the monotonicity constraint in two non-monotonic matrix games.
6.1	Ablation Study of Monotonicity Constraint
Since our proposed algorithm RMC can easily switch between monotonic and non-monotonic modes,
we can evaluate the effects of monotonicity constraints in practical tasks effectively. The ablation
5
Under review as a conference paper at ICLR 2022
experiments in Figure 3 demonstrates that the monotonicity constraint significantly improves the
performance of RMC in SMAC and Continuous PP. To explore the generality of monotonicity
constraints, we extend the ablation experiments to VMIX [27]. We already know that VMIX adds
the monotonicity constraint to the value network of A2C; and it learns the decentralized policies by
advantage-based policy gradient (Sec. 2). Therefore, the monotonicity constraint is not necessary for
greedy action selection for VMIX either. We can evaluate the effects of the monotonicity constraint
by removing the absolute value operation in Figure 1. The ablation experiment in Figure 4 shows that
the monotonicity constraint also improves the sample efficiency in value networks.
* UIM Su- ec⅞βz
Figure 3: Comparing RMC w./ and w./o. monotonicity constraint (remove absolute value operation)
on SMAC and Continuous Predator-Prey.
Figure 4: Comparing VMIX with and without monotonicity constraint on SMAC.
0.0	0.05	0.1	0.15	0.2	0.25
Sampling steps per process (mil)
Sampling steps per process (mil)
The above experimental results indicate that the monotonicity constraint can improve the sample
efficiency in some purely cooperative tasks, such as SMAC and DEPP.
6.2	Re-Evaluation
We then normalize the mainly tricks for all these algorithms for the re-evaluation, i.e, we perform grid
search schemes on a typical hard environment (5m_vs_6m) and super hard environment (3s5z_vs_3s6z)
to find a general set of hyperparameters for each algorithm (details in Appendix C). As shown in
Table 3 3, the test results on the hardest scenarios in SMAC and DEPP demonstrate that, (1) The
performance of values-based methods and VMIX with normalized tricks exceeds the test results in the
past literatures [21; 30; 16; 20; 27] (details in Appendix D.2). (2) QMIX outpeforms all its variants.
(3) The linear VDNs is also relatively effective. (4) The performance of the algorithm becomes
progressively worse as the monotonicity constraint decreases (QMIX > QPLEX > WQMIX and
RMC, VMIX > LICA, details in Appendix E.3) in the benchmark environment.
The experimental results, specifically (2), (3) and (4), show that these variants of QMIX that relax the
monotonicity constraint do not obtain better performance than QMIX in some purely cooperative
tasks, either SMAC or DEPP.
3Note that our experimental results (we use StarCraft 2, SC2.4.10) are not always comparable with the
previous works , which use SC2.4.6.
6
Under review as a conference paper at ICLR 2022
Scenarios	Difficulty	Value-based					Policy-based			
		QMIX	VDNs	Qatten	QPLEX	WQMIX I	LICA	VMIX	DOP	RMC
2c-vs-64zg	Hard	100%	100%	100%	100%	93%	100%	98%	84%	100%
8m_vs_9m	Hard	100%	100%	100%	95%	90%	48%	75%	96%	95%
3s_vs-5z	Hard	100%	100%	100 %	100%	100%	3%	96%	100%	96%
5m_vs_6m	Hard	90%	90%	90%	90%	90%	53%	9%	63%	67%
3s5z_vs_3s6z	S-Hard	75%	43%	62%	68%	6%	0%	56%	0%	75%
corridor	S-Hard	100%	98%	100%	96%	96%	0%	0%	0%	100%
6h_vs_8z	S-Hard	84%	87%	82%	78%	78%	4%	80%	0%	19%
MMM2	S-Hard	100%	96%	100%	100%	23%	0%	70%	3%	100%
27m_vs_30m	S-Hard	100%	100%	100%	100%	0%	9%	93%	0%	93%
Discrete PP	-	40	39	-	39	39	30	39	38	38
Avg. Score	(Hard+)	94.9%	91.2%	92.7%	92.5%	67.4%	29.2%	67.4%	44.1%	84.0%
Table 3: Median test winning rate (episode return) of MARL algorithms with normalized tricks.
S-Hard denotes Super Hard. We compare their performance in the most difficult scenarios of SMAC
and the Discrete PP.
6.3	Finetuned-QMIX
Next, we perform a hyperparameter search for QMIX for each scenario of SMAC (Appendix. C).
As shown in Table 4, the Finetuned-QMIX attains extraordinary high win rates in all hard and super
hard SMAC scenarios, far exceeding vanilla QMIX.
Senarios	Difficulty	QMIX (batch size=128)	Finetuned-QMIX
2s_vs_1sc	Easy	100%	100%
2s3z	Easy	100%	100%
1c3s5z	Easy	100%	100%
3s5z	Easy	100%	100%
10m_vs_11m	Easy	98%	100%
8m_vs_9m	Hard	84%	100%
5m_vs_6m	Hard	84%	90%
3s_vs_5z	Hard	96%	100%
bane_vs_bane	Hard	100%	100%
2c_vs_64Zg	Hard	100%	100%
corridor	Super Hard	0%	100%
MMM2	Super Hard	98%	100%
3s5z_vs_3s6z	Super Hard	3%	85% (envs = 4)
27m_vs_30m	Super Hard	56%	100%
6h_vs_8Z	Super Hard	0%	93% (λ = 0.3)
Table 4: Best median test win rate of Finetuned-QMIX and QMIX in all scenarios.
6.4	Non-monotonic Matrix Games
In this section we first show the Qtot learned by QMIX in two non-monotonic matrix games; then we
propose a simple trick that may improve the performance of QMIX in such environments.
Table 5c and 5d show the Qtot learned by QMIX for the two non-monotonic matrix games (Table 5a
and 5b). Specifically, Table 5b shows that the finetuned QMIX can learn the correct optimal action
for payoff matrix 5d, while the Qtot is not consistent to that of payoff matrix 5d. However, Table 5c
shows that QMIX learns incorrect argmax action for payoff matrix 5a.
Reward Shaping To resolve the incorrect argmax action in above non-monotonic matrix game
(Table 5a), we investigate whether QMIX can learn a correct argmax action by reshaping the task’s
reward function without changing its goal. We find that the reward -12 in Table 5a does not assist the
agents in finding the optimal solution. Then, as shown in Table 6, this non-monotonic matrix can be
solved by simply replacing the insignificant reward -12 with -0.5. Because the reward function for
reinforcement learning is usually set by the users. In practice, this tip hints that we can improve the
performance of QMIX in some tasks by increasing the scale of the important rewards of the tasks;
and reduce the scale of rewards that may cause disruption.
7
Under review as a conference paper at ICLR 2022
8	-12	-12
-12	-O-	-O
-12	~0~~	~0
(a) Payoff matrix 1
12	0	10
-0-	To-	To-
"T0~~	to~~	^τo~~
(b) Payoff matrix 2
-12	-12	-12
-12	-O-	-O
-12	~0~~	~0~~
(c) QMIX: Qtot for Payoff matrix 1
12.0	0.3	9.9
^0^	-4.44~	-1LΓ~
"T≡~	~≡09~	^Γ9~~
(d) QMIX: Qtot for Payoff matrix 2
Table 5:	Non-monotonic matrix games from (a) [23] and (b) [12]; and the learned Qtot (c) and (d)
for Table (a) and (b); Bold text indicates the reward of the argmax action.
The results of this experiment further demonstrate that some non-monotonic games may not be truly
non-monotonic, but rather have poorly designed reward functions.
8.0	-0.5	-0.5
^-ʊʃ	-O-	-O
~≡05^	~0~~	~0~~
(a) Reshaped Payoff matrix 1
8.0	-0.3	-0.3
^03-	^≡0.3-	^-ʊʃ
~^03~	~≡03~	--03~
(b) QMIX: Qtot
Table 6:	We replace the insignificant reward -12 with reward -0.5 for Matrix Game 5a. QMIX learns
a Qtot which has a correct argmax. Bold text indicates argmax action’s reward.
7	Discussion
7.1	Theory
To better understand the monotonicity constraint, we first make a theoretical analysis for it. Our core
assumption is that the joint action-value function Qtot can be represented by a non-linear mapping
fφ(s; Q1, Q2, ...QN), but without the monotonicity constraint.
Definition 1. Cooperative tasks. For a task with N agents (N > 1), all agents have a common goal.
Definition 2. Semi-cooperative Tasks. Given a cooperative task with a set of agents N. For all
states S Ofthe task, ifthere is a subset K ⊆ N, K = 0, where the Qi, i ∈ K increases while the other
Qj , j ∈/ K are fixed, this will lead to an increase in Qtot.
As a counterexample, the collective action problem (social dilemma) is not Semi-cooperative task.
i.e., since the Q value may not include future rewards when Y j 1, the collective interest in the present
may be detrimental to the future interest.
Definition 3. Competitive Cases. Given two agents i and j, we say that agents i andj are competitive
if either an increase in Qi leads to a decrease in Qj or an increase in Qj leads to a decrease in Qi .
As an examples, the matrix game as in Table 1a is a cooperative task with competitive cases. As the
random samples in reinforcement learning may lead to different behavioral preferences of agents.
If one agent prefers action 0 (Like hunting) and the other agent prefers action 1 or 2 (Like sleeping
or entertaining), they will have a conflict of interest (Those who like to entertaining will cause the
hunter to fail to catch the prey).
Definition 4. Purely Cooperative Tasks. Semi-cooperative tasks without competitive cases.
Proposition 1. Purely Cooperative Tasks can be represented by monotonic mixing networks.
Proof. Since the monotonic mixing network is a universal function approximator of monotonic
functions, for a Semi cooperative task, if there is a case (state s) that cannot be represented by a
monotonic mixing network, i.e., dQQt ⑸ < 0, then an increase in Qi must lead to a decrease in
Qj,j = i (since there is no Qj decrease, by Def. 2, the constraint d¾Qt(S) < 0 does not hold).
8
Under review as a conference paper at ICLR 2022
Therefore, by Def. 3 this cooperative task has a competitive case which means it is not a purely
cooperative task.	□
7.2	Why monotonicity constraints work well in SMAC and DEPP?
In this section, we future discuss why the monotonicity constraint works well in these purely
cooperative tasks. First we explain in detail why SMAC and DEPP can be interpreted as purely
cooperative tasks. In practice, (1) For the SMAC, we can decompose the hit-point damage dealt
linearly, and divide the units killed rewards to the agents near the enemy evenly, the victory rewards
to all agents. This approximate linear decomposition 4 also explains why the VDNs also work well in
SMAC (Table. 3). (2) For the DEPP, we can divide the reward for catching prey evenly to the nearest
predators. These simple positive rewards of SMAC and DEPP make these agents have only a shared
goal, i.e, to kill all enemies or capture preys. Intuitively, these linear and fairly assigned rewards
allow the agents to work in a purely cooperative mode. Therefore, QMIX can represent a optimal
solution of SMAC, i.e., a purely cooperative decomposition of Q values.
Then, just as in RMC’s implementation (Figure 2), the monotonicity constraint reduces the range of
values of each mixing weight by half, the hypothesis space is assumed to decrease exponentially by
(1 )N (N denotes the number of weights). By Proposition 1, the Q value decomposition mappings of
the SMAC and DEPP are subsets of the hypothesis space of monotonic mixing network. Therefore,
using the monotonicity constraint can allow for avoiding searching invalid parameters, leading to a
significant improvement in sampling efficiency.
Our analysis shows that QMIX works well if a multi-agent task can be interpreted as purely coop-
erative, even if it can also be interpreted as competitive. That is, QMIX will try to find a purely
cooperative interpretation for a complex multi-agent task.
8	Conclusion
In this paper, we investigate the influence monotonicity constraint and implementation tricks in
cooperative MARL tasks. Our analyses show that relaxing the monotonicity constraint of the mixing
network will not always improve the performance of QMIX. What’s more critical is that monotonicity
constraint can improve sample efficiency in some purely cooperative tasks, such as SMAC and DEPP.
Benefiting from the monotonicity constraint, the fine-tuned QMIX achieves SOTA performance in
SMAC. These facts imply that we can design reward functions in the real multi-agent task that can be
interpreted as purely cooperative, improving the learning sample efficiency of the MARL.
9	B roader Impact
Many complex real-world multi-agent cooperative problems can be simulated as CTDE multi-agent
tasks. Specifically, decentralized agents can be applied to robot swarm control, vehicle coordination,
and network routing. Applying MARL to these scenarios often requires a large number of samples
to train the model, which implies high implementation costs, such as thousands of CPUs, power
resources, and expensive robotic equipment (damaged drones or autonomous cars). Therefore, there is
an urgent need to avoid any and all waste of such resources. In this work, we shows the monotonicity
constraint and implementation tricks can help to improve the sample efficiency in some purely
cooperative tasks, thereby reducing the wasting of resources. In addition, we are hopeful that this
paper will call on the community to be more fair in comparing the performance of algorithms.
4As Qπ (s, u) = Eπ [Pk∞=0 γ k rt+k+1 | s, u], the reward is linearly assignable meaning that Q value is
linearly assignable.
9
Under review as a conference paper at ICLR 2022
References
[1]	Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, ManU Orsini, Sertan Girgin, Raphael
Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What Matters In On-Policy Reinforcement Learning? A Large-Scale
Empirical Study. arXiv:2006.05990, 2020.
[2]	Wendelin Boehmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In ICML
2020, 13-18 July 2020, Virtual Event, pp. 980-991, 2020.
[3]	Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen. An overview of recent progress in the
study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9
(1):427-438, 2012.
[4]	Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. arXiv
preprint arXiv:2009.04416, 2020.
[5]	Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Implementation Matters in Deep Policy Gradients: A Case
Study on PPO and TRPO. arXiv:2005.12729, 2020.
[6]	Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon
Whiteson. Counterfactual multi-agent policy gradients. In AAAI-18, New Orleans, Louisiana,
USA, February 2-7, 2018, pp. 2974-2982. AAAI Press, 2018.
[7]	Maximilian Huttenrauch, Adrian Sosic, and Gerhard Neumann. Guided deep reinforcement
learning for swarm systems. arXiv preprint arXiv:1709.06011, 2017.
[8]	Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR
2015, San Diego, CA, USA, May 7-9, 2015, 2015.
[9]	Tadashi Kozuno, Yunhao Tang, Mark Rowland, Remi Munos, Steven Kapturowski, Will Dabney,
Michal Valko, and David Abel. Revisiting peng’s q (λ) for modern reinforcement learning.
arXiv preprint arXiv:2103.00107, 2021.
[10]	Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82-94, 2016. ISSN 09252312.
[11]	Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In NeurIPS 2017, December 4-9,
2017, Long Beach, CA, USA, pp. 6379-6390, 2017.
[12]	Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multi-
agent variational exploration. In NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pp. 7611-7622, 2019.
[13]	Volodymyr Mnih, Adria PUigdOmeneCh Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1928-1937,
2016.
[14]	Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent
manipulation via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224,
2019.
[15]	Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Pomdps for robotic tasks with
mixed observability. 5:4, 2009.
[16]	Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin Bohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised
policy gradients. arXiv e-prints, pp. arXiv-2003, 2020.
[17]	Jing Peng and Ronald J Williams. Incremental multi-step q-learning. In Machine Learning
Proceedings 1994, pp. 226-232. Elsevier, 1994.
10
Under review as a conference paper at ICLR 2022
[18]	Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pp.
759-766. Morgan Kaufmann, 2000.
[19]	Tabish Rashid, Mikayel Samvelyan, Christian Schroder de Witt, Gregory Farquhar, Jakob N.
Foerster, and Shimon Whiteson. QMIX: monotonic value function factorisation for deep multi-
agent reinforcement learning. In ICML 2018, Stockholmsmassan, Stockholm, Sweden, July
10-15, 2018, pp. 4292-4301, 2018.
[20]	Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted QMIX: Expand-
ing Monotonic Value Function Factorisation. arXiv preprint arXiv:2006.10800, 2020.
[21]	Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. arXiv preprint arXiv:1902.04043, 2019.
[22]	John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[23]	Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Hostallero, and Yung Yi. QTRAN:
learning to factorize with transformation for cooperative multi-agent reinforcement learning. In
ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 5887-5896, 2019.
[24]	Kyunghwan Son, Sungsoo Ahn, Roben Delos Reyes, Jinwoo Shin, and Yung Yi. QTRAN++:
Improved Value Transformation for Cooperative Multi-Agent Reinforcement Learning.
arXiv:2006.12010, 2020.
[25]	Adam Stooke and Pieter Abbeel. Accelerated methods for deep reinforcement learning. arXiv
preprint arXiv:1803.02811, 2018.
[26]	Jianyu Su, Stephen Adams, and Peter A Beling. Value-decomposition multi-agent actor-critics.
arXiv preprint arXiv:2007.12306, 2020.
[27]	Jianyu Su, Stephen Adams, and Peter A. Beling. Value-Decomposition Multi-Agent Actor-
Critics. arXiv:2007.12306, 2020.
[28]	Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Grae-
pel. Value-Decomposition Networks For Cooperative Multi-Agent Learning. arXiv preprint
arXiv:1706.05296, 2017.
[29]	Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
[30]	Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex
Dueling Multi-Agent Q-Learning. arXiv:2008.01062, 2020.
[31]	Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Off-Policy
Multi-Agent Decomposed Policy Gradients. arXiv:2007.12322, 2020.
[32]	Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In ICML 2016, New York City,
NY, USA, June 19-24, 2016, pp. 1995-2003.
[33]	Ermo Wei, Drew Wicke, David Freelan, and Sean Luke. Multiagent Soft Q-Learning. arXiv
preprint arXiv:1804.09817, 2018.
[34]	Yuchen Xiao, Joshua Hoffman, and Christopher Amato. Macro-action-based deep multi-agent
reinforcement learning. In Conference on Robot Learning, pp. 1146-1161. PMLR, 2020.
[35]	Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A General Framework for Cooperative Multiagent Reinforcement Learning.
arXiv preprint arXiv:2002.03939, 2020.
11
Under review as a conference paper at ICLR 2022
[36]	Chongjie Zhang and Victor R. Lesser. Coordinated multi-agent reinforcement learning in
networked distributed pomdps. In AAAI 2011, San Francisco, California, USA, August 7-11,
2011. AAAI Press, 2011.
[37]	Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning Implicit Credit
Assignment for Multi-Agent Actor-Critic. arXiv preprint arXiv:2007.02529, 2020.
[38]	Ming Zhou, Jun Luo, and Julian Villella et al. Smarts: Scalable multi-agent reinforcement
learning training school for autonomous driving, 2020.
12