Under review as a conference paper at ICLR 2022
S2C2 - An orthogonal method for Semi-
Supervised Learning on ambiguous labels
Anonymous authors
Paper under double-blind review
Ab stract
Semi-Supervised Learning (SSL) can decrease the required amount of labeled
image data and thus the cost for deep learning. Most SSL methods assume a
clear distinction between classes, but class boundaries are often ambiguous in
real-world datasets due to intra- or interobserver variability. This ambiguity of an-
notations must be addressed as it will otherwise limit the performance of SSL and
deep learning in general due to inconsistent label information. We propose Semi-
Supervised Classification & Clustering (S2C2) which can extend many deep SSL
algorithms. S2C2 automatically estimates the ambiguity of an image and applies
the respective SSL algorithm as a classification to certainly labeled data while
partitioning the ambiguous data into clusters of visual similar images. We show
that S2C2 results in a 7.6% better F1-score for classifications and 7.9% lower in-
ner distance of clusters on average across multiple SSL algorithms and datasets.
Moreover, the output of S2C2 can be used to decrease the ambiguity of labels with
the help of human experts. Overall, a combination of Semi-Supervised Learning
with our method S2C2 leads to better handling of ambiguous labels and thus real-
world datasets.
1 Introduction
In recent years, Semi-Supervised Learning (SSL) has shown great potential in solving one of the
main issues in deep learning for image classification: the required amount of labeled image data
and the high cost associated with the labeled data generation. By leveraging unlabeled data, the
amount of labeled data and its labeling cost can be decreased to 10% or even 1% while maintaining
classification performance (35; 19; 8; 42; 6) or even boost performance further (40; 29) on already
large labeled datasets like ImageNet (22).
However, these successes have been achieved on curated benchmark datasets where the labels have
been manually cleaned (21). We focus in this paper on how to apply SSL to new uncurated and un-
labeled datasets and the corresponding issues. When we annotate new data, we will often encounter
a variability / inconsistency between the annotations of different annotators or over time. This is-
sue is called intra- or interobserver variability (IIV) and is a common issue when annotating data
(28; 18; 31; 33; 16; 26; 4; 11; 17; 10). The literature names different possible reasons for this vari-
ability such as low resolution (28), bad quality(14; 33), subjective interpretations of classes (17; 26)
or mistakes (18; 25).
We assume that this variability can be modeled for each image with an unknown soft probability dis-
tribution l ∈ [0, 1]k for a classification problem with k classes. We call a label and its corresponding
image certain if all annotators would agree on the classification (l ∈ {0, 1}) and ambiguous if they
would disagree (l ∈ (0, 1)). In other words, ambiguous images are likely to have different annota-
tions due to IIV while certain images do not. The issue of ambiguous images is that the unknown
distribution l can only be estimated with expensive operations like actually acquiring multiple an-
notations. Real-world example images with certain and ambiguous labels are given in Figure 3 and
detailed definitions are given in subsection 2.1.
The question is how to apply SSL to a dataset with expected IIV and thus ambiguous labels. The
better we approximate the unknown label l the more annotations / annotation time we need and
thus we negate the desirable benefit of SSL to require fewer labeled samples. However, low quality
1
Under review as a conference paper at ICLR 2022
Figure 1: Benefit of Semi-Supervised Classification & Clustering (S2C2) over Semi-Supervised
Learning (SSL) - Real-world datasets often suffer from intra- or interoberserver variability (IIV)
during the annotation and thus no clear separation of classes is given as in common benchmark
datasets. Images with a high variability between the annotations have therefore an ambiguous label.
SSL can be confused by these ambiguous labels (see lightning bolt) which results in inconsistent
predictions. Our method S2C2 can be used in combination with SSL to identify ambiguous images
automatically and cluster them, while classifying the rest as usual. Therefore, we avoid the ambi-
guity of the labels during training and generated cluster proposals which can be used to create more
consistent labels.
approximations of l are less consistent and lower the performance of trained models (3; 41) and addi-
tional experiments in subsection 3.6. One goal of this paper is to highlight samples that benefit from
careful (re-)labeling to approximate l better while not increasing the required labels / annotations
much.
For this purpose, we propose Semi-Supervised Classification & Clustering (S2C2) which simultane-
ously distinguishes between ambiguous and certain images, classifies the certain images and clusters
visually similar ambiguous images. S2C2 is not just another SSL algorithm but can be combined
with many existing deep SSL algorithms and is thus orthogonal to them. We will show that this
approach leads to better classifications and more compact clusters across multiple semi-supervised
algorithms and non-cureated datasets. Following previous literature (32), we will demonstrate the
improved consistency of labels based on proposals from S2C2 and the beneficial insights into the
model due to the ambiguity estimation. A graphical summary is provided in Figure 1.
The key contributions of this paper are:
(1)	S2C2 allows a SSL algorithm to predict on avarage a 7.6% better F1-score for classifications and
a 7.9% lower inner distance of clustering across multiple algorithms and non-curated datasets. This
results in better handling of ambiguous images and thus real-world datasets.
(2)	Our method S2C2 can easily extend many deep SSL algorithms and is therefore orthogonal to
them. It can be implemented without a noticeable trade-off in terms of run-time or memory con-
sumption.
(3)	We give a proof-of-concept that the S2C2 proposals can be used to create faster and more con-
sistent labels in comparison to the non-extended algorithms and a consensus process. This leads to
higher quality data for further evaluation or model training.
1.1 Related Work
Our method is mainly related to Semi-Supervised Learning, Handling Noisy Data and Classification
& Clustering.
Semi-Supervised Learning (7) is mainly developed on curated benchmark datasets (22; 9; 21)
where the issue of IIV is not considered. In contrast to other SSL research (8; 42; 6; 12; 35), we are
not evaluating on these curated benchmarks but work with new real-world datasets for two reasons.
Firstly, curated datasets do not suffer so much from IIV because they were already cleaned. Recent
research indicates that even these datasets suffer from erros in the labels which negatively impact
the performance (28; 3). Secondly, if we want to evaluate the IIV issue, we need an approximation
of the variability of the label for each image e.g. in the form of multiple annotations per image.
However, this information is often not provided at current state-of-the-art benchmarks.
2
Under review as a conference paper at ICLR 2022
Handling Noisy Data is often defined as handling mistakes in the labels (36; 18; 1). Our method
S2C2 automatically trains the extended SSL algorithm on only the certain images and thus filters
for the difficult and possible noisy ambiguous images. However, these ambiguous images are not
necessarily wrongly labeled but we still consider them uninformative for SSL. Instead, we only
require them to form visually homogeneous clusters. In contrast to common noise estimation (36;
18; 1; 25), we do not just ignore or relabel these images because a ambiguous label describes a
variability between multiple classes and we do not want to loose this knowledge during training.
The combination of Classification & Clustering was investigated in (30; 27; 5). However, the
methods only use classical approaches and no deep neural network which makes it more difficult to
extend the methods to real-world image data. Clustering with deep neural networks has been used
successfully in image classification (38; 15; 31) and has been combined with pairwise classification
constraints (34). We simultaneously calculate a classification and a clustering with a deep neural
network on real-world image data.
2 Method
Our method Semi-Supervised Classification & Clustering (S2C2) is not a individual method but an
extension for most SSL algorithms such as (2; 35; 37; 24; 23). We can extend any image classifica-
tion model with S2C2 as long as it is compatible with the definition of an arbitrary SSL algorithm
below.
2.1 Definitions
We assume that every image x ∈ X has an unknown soft probability distribution l ∈ [0, 1]k for
a classification problem with k classes. This assumption is based on two main reasons. Firstly,
inconsistent annotations exist due to subjective opinions from the annotators, e.g. the grading of
an illness (17). A hard label l ∈ {0, 1}k could not model such a difference over the complete
annotator population. Secondly, if we look for example at biological processes, there exist images
of intermediate transition stages between two classes, e.g. the degeneration of a living underwater
organism to dead biomass (31).
An image and its corresponding label l are ambiguous if i,j ∈ {1, .., k} exist with i 6= j, li > 0 and
lj > 0. Otherwise the image and its label are certain. The ambiguity of a label is 1 -maxi∈{1,..,k}li.
An image might be ambiguous because it is actually an intermediate or uncertain combination of
different classes as stated above. For this reason, we view ambiguous images not just as wrongly
assigned images.
A SSL algorithm uses a labeled dataset Xl and an unlabeled dataset Xu for the training of a neural
network Φ with X = Xi IjXu. For all images X ∈ Xi a hard label l is available while no label
information is available for x ∈ Xu. The output pn(x) := Φ(x) is a probability distribution over
the k classes.
2.2 S2C2
Our method S2C2 extends an arbitrary SSL algorithm. This SSL algorithm passes an image x
through the network Φ and predicts a classification pn(x) ∈ [0, 1]k. S2C2 calculates two addi-
tional outputs without a noticeable impact on training time or memory consumption: a clustering
assignment po(x) ∈ [0, 1]k0 with k0 > k and an ambiguity estimation pa(x) ∈ [0, 1]. The clus-
ter assignment partitions visually similar images in more clusters than classes exist (overclustering
with k0 > k). The ambiguity estimation is used to determine if a classification (pn (x)) or an
(over)clustering (po(x)) is used as the final output. If pa (x) < 0.5 the image is predicted as cer-
tain and thus the classification is used. Otherwise, the image is estimated as ambiguous and the
clustering is used as output.
The network is trained by minimizing the following loss function:
L(X) = LSSL(X) ∙ [1 - Pa(x)] + λaLa(x)
+ λcE-ι Lce-1 (x) ∙ [1 - Pa (x)] +1§Ls(x) ∙ Pa(X)
(1)
3
Under review as a conference paper at ICLR 2022
Figure 2: Our method S2C2 and an extended arbitrary SSL method - The SSL algorithm passes an
image x through the network Φ and outputs a classification pn (x). We add two additional outputs:
an overclustering po(x) and a ambiguity estimation pa(x). The ambiguity estimation pa(x) is used
to determine if the classification or the overclustering output is used for our method S2C2.
The first three loss terms correspond to the outputs pn(x), pa(x) and po(x) respectively. The last
term is optional and stabalizes the training. The λ values are weights to balance the impact of each
term. The first loss LSSL is the loss calculated by the original SSL algorithm and is only scaled with
[1 - pa(x)] to prevent the original SSL training on images the network predicts as ambiguous.
The second loss LA improves the ambiguity estimation. As stated above, the underlying distribution
l is unknown and thus we do not know during training if x is ambiguous or certain. However, we
can expect to know or be given an prior probability pA ∈ [0, 1] of the expected percentage of
ambiguous images in the total dataset. Based on this probability, we can estimate a Pseudo-Label
of the ambiguity of each image in a batch during training. The loss LF is the binary cross-entropy
between the Pseudo-Label h(x) and pa (x). The formulation is given below with i the index of the
image x inside the given batch when all images inside the batch are sorted in ascending order based
on pa .
LA(x) = CE(h(x), pa(x))
=-(1 — h(x)) ∙ ln(pa(x = 0)) — h(x) ∙ ln(Pa(X = 1)) With
1 i ≤ batch size ∙ PA
h(x) = 0 else
(2)
The third loss LCE-1 incentivises visually homogeneous clusters of the images by pushing images
from different classes into different clusters and it has been shoWn to improve classification based
on overclustering (31).
k
CE-I(Po(x),Po(X)) = - XPo(X)C ∙ ln(1 - Po(x0)c).	⑶
c=1
The loss Was presented in (31) and is also scaled With [1 — Pa(X)] because it uses an estimate of the
class for an image Which could be Wrong for ambiguous images. In contrast to (31), We select the
image X0 from the same batch as X and not as an additional input With a different class label. For the
selection, We use either the ground-truth label l of X if it is available or the Pseudo-Label based on
the netWork prediction Pn(X)
The forth term LS is the cross-entropy (CE) betWeen Po (X) andPo(X0) for tWo differently augmented
versions X, X0 of the same image. This loss is scaled With f(X) and incentivises that augmented ver-
sions of the same ambiguous image are in the same output cluster. We use CE because it indirectly
minimizes also the entropy of Po(X) Which leads to sharper predictions. Many SSL algorithms al-
ready use a differently augmented version X0 ofX as secondary input (2; 35; 37; 23; 15) Which alloWs
an easy computation. OtherWise, the forth term is not calculated and treated as zero.
3	Experiments
3.1	Datasets
A main contribution of this paper is that our method can be applied to many semi-supervised algo-
rithms across different real-World ambiguous datasets Without major changes. While many datasets
4
Under review as a conference paper at ICLR 2022
Table 1: Overview of the used datasets - # is an abbreviation for number. The class imbalance is
given as the percentage of the smallest and largest class with regard to the complete dataset. PA is
the expected prior ambiguity probability of the dataset. n is the average of annotations per image.
Name	# classes Input size [px]	# Images	Class Imbalance [%] pA [%] n
		Train		Val	Unlabeled	Smallest	Largest		
Plankton (31)	10	96x96	1964	2456	7860	4.16	30.37	44	24
Turkey (39)	2	96x96	1299	1542	5199	9.66	90.33	22	3
Mice Bone (33)	3	224x224	277	169	278	10.81	63.98	65	3
CIFAR-10H (28)	10	32x32	1600	2000	6400	9.88	10.16	32	51
(28; 18; 31; 33; 16; 26; 4; 11; 17; 10) suffer from annotation variability, we do not know the
unknown underlying distribution l to evaluate the ambiguity or any related metrics. We can ap-
proximate l with the average over multiple annotations from humans. An annotation is the hard
coded guess a = (a1, ..., ak) ∈ {0, 1}k of the class for an image from a human with exactly one
i0 ∈ {1, ..., k} : a0i = 1 and for all j ∈ {1, ..., k} \ {i0} : aj = 0. We assume that the approximation
l as the average of n annotations is identical to the unknown distribution l for n -→ ∞. This leaves
the issue that we need multiple annotations per image for a dataset with ambiguous labels which are
often not available. However, all datasets summarized in Table 1 have multiple annotations and thus
allow the approximation of l . Nine visual examples for all datasets are given in Figure 3 and the
datasets are shortly introduced below.
The Plankton dataset was introduced in (31). The dataset contains 10 plankton classes and has mul-
tiple labels per image due to the help of citizen scientists. In contrast to (31), we include ambiguous
images in the training and validation set and do not enforce a class balance which results in a slighlty
different data split as shown in Figure 3.
The Turkey dataset was used in (39). The dataset contains cropped images of potential injuries which
were separately annotated by three experts as not injured or injured.
The Mice Bone dataset is based on the raw data which was published in (33). The raw data are
3D scans from collagen fibers in mice bones. The three proposed classes are similar and dissimilar
collagen fiber orientations and not relevant regions due to noise or background. We used the given
segmentations to cut image regions from the original 2D image slices which mainly consist of one
class. We generated ambiguous GT labels on 10% of the generated images by averaging over three
own classifications from an expert.
The CIFAR-10H (28) dataset provides multiple annotations for the test set of CIFAR-10(21). This
dataset is interesting because it illustrates that even the hard labels from benchmark datasets like
CIFAR-10 are based on soft labels due to IIV.
Figure 3: Example images for the ambiguous real-world datasets - All datasets have certain im-
ages (red & blue) and ambiguous images between these classes (grey). The classes are Bubble &
Copepod, Not Injured & Injured, Similar & Dissimilar Orientations and Dog & Cat respectively.
For all datasets, we split our images X into a labeled Xl and unlabeled Xu training set. We keep
additional images as a validation subset. On Xl, we use for each image a random hard label sampled
from the corresponding l. This simulates that we only have a noisy approximation of the true ground
5
Under review as a conference paper at ICLR 2022
truth label l. On Xu, we can only use the image information and not any label information. The
validation data is used to compare the trained networks and to detect issues like overfitting.
As stated above the approximation of l is only possible with multiple annotations per image. For this
reason the inclusion of classical benchmarks like (21; 9) is difficult because no multiple annotations
or labels are given. For the CIFAR-10 dataset (21), multiple annotations are only available for the
test set as the above mentioned CIFAR-10H dataset (28) and thus we can evaluate on this subset.
For the STL-10 dataset (9), only one annotation / label per image is given. We still include some
results of this dataset to illustrate the performance on previous benchmarks.
3.2	Metrics
We want to measure the quality of classification and clusters over the certain and ambiguous data
respectively which we assume are better proposals in the annotation process or evaluation by experts.
Based on this reasoning, we decided to use the weighted F1-Score on certain data and the mean inner
distance on ambiguous data, The ambiguity is determined by the network output pa . We define the
metrics in detail below and give in subsection 3.6 a proof-of-concept for the higher consistency of
labels based on proposals selected by the defined metrics.
During training we do not enforce a balance between ambiguous and certain predictions to keep the
required prior knowledge minimal and give more flexibility to the model. This leads to two issues.
Firstly, our network might predict almost all samples as certain (or ambiguous). This would make
the clustering (or classification) metric not meaningful due to too few samples. Hence, we avoid this
issue by calling a training degenerated ifno more than 10% of the validation data are either predicted
as ambiguous or certain. Secondly, due to the definition of the datasets, some classes might have no
or very few certain (predicted) images and an averaging over their classes can lead to non-intuitive
metrics. This issue can also cause instability of a metric due to a low sampling size in a class.
Hence, we use the weighted F1-Score on certain images. We use the weighted version based on
the number of images per class to avoid instability due to the second issue mentioned above. For
the ambiguous images, we use the mean inner euclidean distance (d) to the centroid on the soft /
ambiguous gt labels. The equation for a set of clusters of images X is given in Equation 4 with
sets C ∈ X as clusters and the corresponding approximated soft label distribution lx for each image
X ∈ C. The centroid per cluster is given as μc.
d(X) :
|xXi X ∣⅛ X 优
C∈X	x∈C
-μc∣∣2 with μc
|C| X lx
x∈C
(4)
We use the vanilla (unchanged) SSL algorithms as baseline experiments. For these experiments
and some ablation experiments, we have no ambiguity prediction pa (x). In this case, we assume all
images tobe certain and use pn (x) as output. We often noticed that the classification improved while
the clustering degenerated and the other way round. Therefore, we determine the best performance
by looking at the difference (d-F1) between distance and F1-Score (smaller is better). In general,
we have 3 runs per setup but we exclude results that degenerate as described above. We report
the best of these runs based on the (d-F1)-score or the mean and standard deviation over all non-
degenerated runs. All scores are calculated on the validation data which is in general about 20% of
all the data (see details in Table 1).
3.3	Implementation Details
All methods use the same code base1 and share major hyperparameters which is crucial for valid
comparisons (20). We use the prior ambiguity pA = 0.6 and loss weights λCE-1 = 10, λf =
0.1 and λs = 0.1 across all experiments. The additional losses LA and LS are only applied on
the unlabeled data while LCE-1 is also calculated on the labeled data. These hyperparameters
were determined heuristically on the Plankton Dataset with Mean-Teacher and show strong results
across different methods and datasets as shown in subsection 3.4. Most likely these parameters
are not optimal for an individual combination of a method and a dataset but they show the general
usability across methods and datasets. We refer to the supplementary for more detailed insights
about individual hyperparameters.
1https://github.com/google-research/fixmatch + own S2C2 code, main pseudo code in supplementary
6
Under review as a conference paper at ICLR 2022
Table 2: Performance across different methods and datasets - The vanilla algorithm is highlighted
in light grey. Better results in comparison to the vanilla algorithm are marked bold. The definition of
the metrics are given in subsection 3.2. CE stands for supervised Cross-Entropy training. All values
are given in %. Reasons for exclusion: H - Hardware Restrictions
Methods
Plankton
Turkey
Mice Bone
CIFAR10H
STL-10
	F1 ↑	d，	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑	d J	(d-F1) J	F1 ↑
CE	86.71	30.45	-56.26	83.84	42.98	-40.86	69.55	54.75	-14.80	67.71	55.80	-11.91	80.48
CE + S2C2	78.24	23.41	-54.84	85.79	27.64	-58.14	93.88	36.58	57.30	78.27	54.52	-23.75	88.45
Mean-Teacher (37)	88..72	25.84	-62.88	81.82	45.12	-36.70	66.41	48.83	-17.58	73.53	46.93	-26.59	80.67
Mean-TeaCher(37) + S2C2	91.30	24.84	-66.46	86.45	33.92	-52.53	89.4	35.11	-54.73	85.13	52.44	-32.69	89.28
Pi-Model (23)	87.57	28.43	-59.14	82.11	39.46	-42.65	68.15	54.11	-14.04	71.53	49.13	-22.40	82.56
Pi-Model (23) + S2C2	79.79	19.08	-60.71	87.43	23.33	-64.10	88.01	30.99	-57.02	83.05	43.40	-39.65	89.54
Pseudo-Label (24)	87.62	27.42	-60.20	82.37	44.88	-37.49	66.60	57.03	-9.57	69.70	53.30	-16.40	82.48
Pseudo-Label (24) + S2C2	89.31	31.76	-57.55	83.44	35.04	-48.41	86.58	37.52	-49.06	83.74	51.32	-32.42	88.87
FixMatch (35)	85.81	30.29	-55.52	82.14	43.33	-38.81	H	H	H	78.09	41.99	-36.10	89.35
FixMatch (35) + S2C2	87.20	31.28	-55.92	83.56	28.17	-55.39	H	H	H	83.09	49.49	-33.60	91.45
Figure 4: Comparison of SSL and S2C2 - Each datapoint rep-
resents an independent training run depending on the weighted
F1-Score (F1) and the mean inner distance (d). The color and
marker types define the used dataset and method respectively.
Table 3: Consistency compar-
ison of generated labels from
proposals for the Mice Bone
dataset - The first column de-
scribes by which algorithm
the propsals were generated.
The Cohen’s kappa coeffi-
cient κ measures the agree-
ment of two repetitions and
Time gives the averaged an-
noatation time in minutes.
Methods
Time
No Proposals
SSL
SSL + S2C2
0.7135
0.7047
0.8437
13.95
7.5
7.13
κ
3.4	Results
The comparison between different semi-supervised algorithms and their extension with S2C2 is
given in Table 2. The best results were selected as described in subsection 3.2 and the complete
results are given in the supplementary. We see that S2C2 improves the classification and clustering
performance across the majority of classes and methods by 5 to 10%. (d-F1) is improved by up to
40% for 16 out of 19 method-dataset-combinations. On average, we achieve a 7.6% higher F1-score
for certain classifications and a 7.9% lower inner distance for clusterings of ambiguous images if
we look at all non excluded method-dataset-combinations. Even on STL-10 a dataset without the
possibility to evaluate ambiguous labels S2C2 creates up to 9% better classifications.
In Figure 4, we plot all used runs for the evaluation based on their F1 and d-score. Overall, we
see the most benefit on the Mice Bone and Turkey dataset which we attribute to the worse initial
approximation of l. In most cases, we can see an improvement in F1 and d metric but also a greater
variability in the training results. We think this variability is introduced because the network has
to decide automatically what a certain and ambiguous image is without any direct guidance. The
different vanilla algorithms achieve quite similar results for each dataset. Only FixMatch achieves
a more than 5% better F1-Score on the curated STL-10 and CIFAR-10H dataset. In general, we
see that S2C2 can be beneficially applied to a variety of datasets and methods and predicts better
classifications and more compact clusters. In subsection 3.6, we will show that this defines also
better proposals during the annotation process.
3.5	Ablation
We pooled the runs between all methods to evaluate the impact of the individual components of our
method S2C2. We report the best and the average across all methods in Table 4. We averaged over
7
Under review as a conference paper at ICLR 2022
Table 4: Complete ablation results for averaging over different methods - The vanilla algorithms
as baseline are highlighted in light grey. Each row below that extend this baseline individually
with CE-1 (31), Clustering & Classification (CC) or both (S2C2). CC can be interpreted as S2C2
without CE-1. The prior ambiguity estimate pA is given if used in brackets. Results that improve
over the baseline are marked in bold. The definition of the metrics are given in subsection 3.2. The
column ’Ambiguous’ gives the percentage of predicted ambiguous data and the last column gives
the number of runs over which we averaged.
F1	d	(d-F1)	Ambiguous	# Runs
best mean ± std best mean ± std best mean ± std best mean ± std
CIFAR10-H
Baseline	0.7809	0.7153	±	0.0359	0.4199	0.5027	±	0.0469	-0.3611	-0.2126	±	0.0827	-	-	15
+ CE-1	0.7383	0.7191	±	0.0164	0.4692	0.4929	±	0.0243	-0.2691	-0.2262	±	0.0404	-	-	12
+CC(pA = 0.6)	0.8565	0.7471	±	0.1246	0.8657	0.8768	±	0.0129	0.0092	0.1297	±	0.1374	0.6145	0.5923	± 0.0322	12
+ S2C2 (pA = 0.32)	0.6656	0.6970	±	0.0469	0.2155	0.3684	±	0.1227	-0.4501	-0.3286	±	0.0836	0.2910	0.3115	± 0.0140	12
+ S2C2 (pA = 0.6)	0.8305	0.7457	±	0.1097	0.4340	0.4741	±	0.0584	-0.3965	-0.2716	±	0.0928	0.6125	0.5860	± 0.0290	15
Plankton
Baseline	0.8872	0.8652	±	0.0212	0.2584	0.2915	±	0.0240	-0.6287	-0.5737	±	0.0444	-	-	15
+ CE-1	0.8896	0.8803	±	0.0060	0.2540	0.2690	±	0.0098	-0.6356	-0.6113	±	0.0154	-	-	12
+ CC (pA = 0.6)	0.8919	0.9128	±	0.0427	0.4085	0.7702	±	0.1630	-0.4833	-0.1426	±	0.1375	0.6242	0.5927	± 0.0127	12
+ S2C2 (pA = 0.44)	0.8625	0.9049	±	0.0340	0.2192	0.3269	±	0.0526	-0.6433	-0.5780	±	0.0305	0.4365	0.4451	± 0.0204	11
+ S2C2 (pA = 0.6)	0.9130	0.8768	±	0.0640	0.2484	0.3004	±	0.0750	-0.6646	-0.5764	±	0.0416	0.6164	0.5893	± 0.0202	14
TUrkey
Baseline	0.8211	0.8213	±	0.00s69	0.3946	0.4428	±	0.0209	-0.4265	-0.3786	±	0.0230	-	-	15
+ CE-1	0.7998	0.7998	±	0.0000	0.3338	0.3338	±	0.0000	-0.4660	-0.4660	±	0.0000	-	-	12
+ CC (pA = 0.6)	0.8527	0.8264	±	0.0469	0.3400	0.3435	±	0.0408	-0.5127	-0.4829	±	0.0128	0.5837	0.5646	± 0.0427	12
+ S2C2 (pA = 0.22)	0.7998	0.7998	±	0.0000	0.1675	0.2252	±	0.0646	-0.6322	-0.5746	±	0.0646	0.5000	0.3674	± 0.2054	4
+ S2C2 (pA = 0.6)	0.8743	0.8432	±	0.0350	0.2333	0.3270	±	0.0692	-0.6410	-0.5162	±	0.0643	0.8093	0.6387	± 0.2354	12
15 runs except for the additional ablations with 12 runs where we excluded FixMatch due to the
12 times higher required GPU hours. We excluded degenerated runs which lowers the number for
some ablations further. The complete results are given in the supplementary. Across the datasets,
we see the best (d-F1)-scores are achieved by S2C2. The impact of the components varies between
the datasets. We see that CE-1 positively impacts the clustering results which confirms the benefit
of using CE-1 for overclustering (31). CC reaches in all cases a better F1-Score than the baseline
and even surpasses S2C2 sometimes. However, the inner distance (d) may increase as well. We
conclude that CC and CE-1 on their own can lead to improvements but only the combination of
both parts results in a stable algorithm across datasets and methods. If we use the correct amount
of ambiguity PA in each dataset as pa, We see that in general the F1-Score decreases and d-score
improves. We attribute this difference to the lower prior ambiguity pA because S2C2 tries to predict
more certain than ambiguous images. This leads to a loWer inner distance but also includes more
difficult images in the classification of the certain data. We believe this parameter is essential for
balancing the improvements in the F1- and d-score for a specified usecase.
3.6	Insights
Consistency - We shoWed that S2C2 can lead to better classifications and clusterings than SSL
alone. Our assumption from above Was that using the predictions of S2C2 as proposals during
the annotation process leads to more consistent and thus higher quality labels. We give a proof-
of-concept on the Mice Bone dataset and the SSL algorithm MeanTeacher (37) in Figure 3. The
data Was reannotated by an expert tWo times for each given proposal. We see that the consistency is
similar betWeen using no proposals and the SSL classification predicitions. While the SSL proposals
can decrease the annotation time by about 50%, too many ill predicted images exist Which need to
be corrected. The usage of the better classification and visually homogeneous clusters of S2C2 lead
to an increased κ coefficient of over 10% While achieving an even faster annotation time than SSL.
Impact of ambiguous labels - We stated that high quality labels lead to better model training (3).
We verify this statement on the Plankton and CIFAR-10H dataets in Table 5. We see for all super-
vised and semi-supervised methods that used trainings labels based on the complete distribution of l
(argmax l, column A) leads to an improvement ofup to 10% in comparison to sampling the training
label from l (column A1). Nevertheless, in our Work We used the approximation based on a sample
from l because in a real-World task We Would have access to l only at a high cost. If We remove
8
Under review as a conference paper at ICLR 2022
Table 5: Impact of ambiguous labels - Macro FI-SCore for different methods and across three
different subsets on the validation data from the Plankton and CIFAR10-H dataset. Columns: A1 -
Labels are sampled from l; A - Labels are the maximum class ofl; C -No ambiguous labels/images
are used
Methods	Plankton	CIFAR10-H
	A1	A	C	A1	A	C
CE	86.71	8835	96.10	67.71	68.89	86.57
Mean-Teacher (37)	88.72	88.94	96.00	73.56	75.06	86.96
Pi-MOdel (23)	87.57	89.03	96.41	71.53	72.75	87.19
PSeUdO-Label (24)	87.62	88.41	96.20	69.70	71.82	87.15
FixMatch (35)	80.29	90.24	98.86	76.15	79.15	90.37
Plankton	Turkey	Mice Bone	CIFARIO-H
tions - Wrong classifications based on the normal head are highlighted in red.
the ambiguous images entirely from the dataset (column C), the results improve again by 8 to 15%.
This indicates that ambiguous images are a major issue during the training process.
Interpretability - Many SSL algorithms interpret the probability of the largest value of pn (x) as
confidence (13). We qualitatively illustrate in Figure 5 that using our ambiguity prediction pa(x) can
lead to better interpretability and fewer errors. We show 6 randomly picked examples for selected
classes across the datasets and extended results in the supplementary. The images in each row
have a similar value for pn (x) and pa (x). The first row presents highly confident predictions on
certain predicted images and shows no errors in the given random picks. The middle row shows
highly confident predictions on ambiguous predicted images. Some of these images are false and
would lower the performance without the additional ambiguity prediction. The last row shows non-
confident or uncertain (0.4 < pa(x) < 0.6) predictions which are often wrong.
4	Conclusion
In real-world datasets, we often encounter ambiguous labels for example due to intra- or interob-
server variability. We propose our method S2C2 which is our orthogonal extension to many semi-
supervised algorithms and allows to classify images with certain labels and cluster ambiguous ones.
S2C2 also automatically determines which image to treat as certain or ambiguous only based on a
given prior distribution pA. On average, we achieve an increased F1-Score of 7.6% and a lower inner
distance in clusterings of 7.9% over all method-dataset-combinations. We give a proof-of-concept
that ambiguous labels can negatively impact the classification performance, annotations based on
proposals from S2C2 are more consistent and the ambiguity prediction can give more insight in
the model reasoning. Therefore, SSL algorithms with S2C2 are better suited to handle real-world
datasets including ambiguous labeled images either by an improved classification / clustering or as
a proposal during the annotation process with more insight.
9
Under review as a conference paper at ICLR 2022
References
[1]	Algan, G., Ulusoy, I.: Image Classification with Deep Learning in the Presence of Noisy
Labels: A Survey. Knowledge-Based Systems (2020)
[2]	Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel, C.A.: Mixmatch: A
holistic approach to semi-supervised learning. In: Advances in Neural Information Processing
Systems. pp. 5050-5060 (2019)
[3]	Beyer, L., Henaff, O.J., Kolesnikov, A., Zhai, X., van den Oord, A.: Are We done with ima-
genet? (2020)
[4]	Brunger, J., Dippel, S., Koch, R., Veit, C.: 'Tailception': using neural networks for assessing
tail lesions on pictures of pig carcasses. Animal 13(5), 1030-1036 (2019)
[5]	Cai, W., Chen, S., Zhang, D.: A simultaneous learning framework for clustering and classifi-
cation. Pattern Recognition 42(7), 1248-1259 (2009)
[6]	Caron, M., Goyal, P., Misra, I., Bojanowski, P., Mairal, J., Joulin, A.: Unsupervised Learning
of Visual Features by Contrasting Cluster Assignments. Proceedings of Advances in Neural
Information Processing Systems (NeurIPS) (2020)
[7]	Chapelle, O., Scholkopf, B., Zien, A., Scholkopf, B., Zien, A.: Semi-supervised learning.
IEEE Transactions on Neural Networks 20(3), 542 (2006)
[8]	Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton, G.: Big Self-Supervised Models
are Strong Semi-Supervised Learners. Advances in Neural Information Processing Systems 33
pre-proceedings (NeurIPS 2020) (2020)
[9]	Coates, A., Ng, A., Lee, H.: An analysis of single-layer networks in unsupervised feature
learning. In: Proceedings of the fourteenth international conference on artificial intelligence
and statistics. pp. 215-223 (2011)
[10]	Culverhouse, P., Williams, R., Reguera, B., Herry, V., Gonzalez-Gil, S.: Do experts make mis-
takes? A comparison of human and machine identification of dinoflagellates. Marine Ecology
Progress Series 247, 17-25 (2003)
[11]	De Fauw, J., Ledsam, J.R., Romera-Paredes, B., Nikolov, S., Tomasev, N., Blackwell, S.,
Askham, H., Glorot, X., O’Donoghue, B., Visentin, D., Others: Clinically applicable deep
learning for diagnosis and referral in retinal disease. Nature medicine 24(9), 1342-1350 (2018)
[12]	Grill, J.B., Strub, F., Altche, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C.,
Pires, B.A., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu, K., Munos, R., Valko, M.: Boot-
strap your own latent: A new approach to self-supervised Learning. Advances in Neural Infor-
mation Processing Systems 33 pre-proceedings (NeurIPS 2020) (2020)
[13]	Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural networks. In:
International Conference on Machine Learning. pp. 1321-1330. PMLR (2017)
[14]	Jenckel, M., Parkala, S.S., Bukhari, S.S., Dengel, A.: Impact of Training LSTM-RNN with
Fuzzy Ground Truth. In: ICPRAM (2018)
[15]	Ji, X., Henriques, J.F., Vedaldi, A., Ji, X., Henriques, J.F., Vedaldi, A.: Invariant information
clustering for unsupervised image classification and segmentation. In: Proceedings of the IEEE
International Conference on Computer Vision. pp. 9865-9874. No. Iic (2019)
[16]	Jungo, A., Meier, R., Ermis, E., Blatti-Moreno, M., Herrmann, E., Wiest, R., Reyes, M.: On
the effect of inter-observer variability for a reliable estimation of uncertainty of medical image
segmentation. In: Medical Image Computing and Computer Assisted Interventions, MICCAI.
pp. 682-690. Springer (2018)
[17]	Karimi, D., Nir, G., Fazli, L., Black, P.C., Goldenberg, L., Salcudean, S.E.: Deep Learning-
Based Gleason Grading of Prostate Cancer From Histopathology Images—Role of Multiscale
Decision Aggregation and Data Augmentation. IEEE Journal of Biomedical and Health Infor-
matics 24(5), 1413-1426 (2020)
10
Under review as a conference paper at ICLR 2022
[18]	Karimi, D., Dou, H., Warfield, S.K., Gholipour, A.: Deep learning with noisy labels: exploring
techniques and remedies in medical image analysis. Medical Image Analysis 65 (2020)
[19]	Kim, B., Choo, J., Kwon, Y.D., Joe, S., Min, S., Gwon, Y.: SelfMatch: Combining Contrastive
Self-Supervision and Consistency for Semi-Supervised Learning (NeurIPS) (2021)
[20]	Kolesnikov, A., Zhai, X., Beyer, L.: Revisiting self-supervised visual representation learning.
In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp.
1920-1929 (2019)
[21]	Krizhevsky, A., Hinton, G., Others: Learning multiple layers of features from tiny images.
Tech. rep., Citeseer (2009)
[22]	Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional
neural networks. In: Advances in neural information processing systems. vol. 60, pp. 1097-
1105. Association for Computing Machinery (2012)
[23]	Laine, S., Aila, T.: Temporal ensembling for semi-supervised learning. In: International Con-
ference on Learning Representations (2017)
[24]	Lee, D.H.: Pseudo-label: The simple and efficient semi-supervised learning method for deep
neural networks. In: Workshop on challenges in representation learning, ICML. vol. 3, p. 2
(2013)
[25]	Li, J., Socher, R., Hoi, S.C.H.: DivideMix: Learning with Noisy Labels as Semi-supervised
Learning. In: International Conference on Learning Representations. pp. 1-14 (2020)
[26]	Ooms, E., Zonderland, H., Eijkemans, M., Kriege, M., Mahdavian Delavary, B., Burger, C.,
Ansink, A.: Mammography: Interobserver variability in breast density assessment. The Breast
16(6), 568-576 (dec 2007)
[27]	Peikari, M., Salama, S., Nofech-mozes, S., Martel, A.L.: A Cluster-then-label Semi- super-
vised Learning Approach for Pathology Image Classification. Scientific Reports (April), 1-13
(2018)
[28]	Peterson, J., Battleday, R., Griffiths, T., Russakovsky, O.: Human uncertainty makes classi-
fication more robust. Proceedings of the IEEE International Conference on Computer Vision
2019-Octob, 9616-9625 (2019)
[29]	Pham, H., Dai, Z., Xie, Q., Luong, M.T., Le, Q.V.: Meta Pseudo Labels (2020)
[30]	Qian, Q., Chen, S., Cai, W.: Simultaneous clustering and classification over cluster structure
representation. Pattern Recognition 45(6), 2227-2236 (2012)
[31]	Schmarje, L., Brunger,J., Santarossa, M., Schroder, S.M., Kiko, R., Koch, R.: Fuzzy Overclus-
tering: Semi-supervised classification of fuzzy labels with overclustering and inverse cross-
entropy. Sensors (2021)
[32]	Schmarje, L., Koch, R.: Life is not black and white - Combining Semi-Supervised Learning
with fuzzy labels. Proceedings of the Conference ”Lernen, Wissen, Daten, Analysen”, (2021)
[33]	Schmarje, L., Zelenka, C., Geisen, U., Gluer, C.C., Koch, R.: 2D and 3D Segmentation ofUn-
certain Local Collagen Fiber Orientations in SHG Microscopy. In: DAGM German Conference
of Pattern Regocnition, vol. 11824 LNCS, pp. 374-386. Springer (2019)
[34]	SmieJa, M., Struski,匕，Figueiredo, M.A.T.: A Classification-Based Approach to Semi-
Supervised Clustering with Pairwise Constraints (2020)
[35]	Sohn, K., Berthelot, D., Li, C.L., Zhang, Z., Carlini, N., Cubuk, E.D., Kurakin, A., Zhang,
H., Raffel, C.: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Con-
fidence. Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS
2020) (2020)
11
Under review as a conference paper at ICLR 2022
[36]	Song, H., Kim, M., Park, D., Lee, J.: Learning from Noisy Labels with Deep Neural Networks:
A Survey. arXiv preprint arXiv:1406.2080 (2020)
[37]	Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results. In: ICLR (2017)
[38]	Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., Van Gool, L.: Scan:
Learning to classify images without labels. In: Proceedings of the European Conference on
Computer Vision. pp. 268-285 (2020)
[39]	Volkmann, N., Brunger, J., Stracke, J., Zelenka, C., Koch, R., Kemper, N., Spindler, B.: SO
MUCH TROUBLE IN THE HERD: DETECTION OF FIRST SIGNS OF CANNIBALISM IN
TURKEYS. In: Recent advances in animal welfare science VII Virtual UFAW Animal Welfare
Conference. p. 82 (2020)
[40]	Xie, Q., Luong, M.T., Hovy, E., Le, Q.V., Luong, M.T., Le, Q.V., Hovy, E., Le, Q.V.: Self-
Training With Noisy Student Improves ImageNet Classification. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 10684-10695. IEEE (jun 2020)
[41]	Yun, S., Oh, S.J., Heo, B., Han, D., Choe, J., Chun, S.: Re-labeling imagenet: From single to
multi-labels, from global to localized labels. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 2340-2350 (June 2021)
[42]	Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow Twins: Self-Supervised Learning
via Redundancy Reduction (2021)
12