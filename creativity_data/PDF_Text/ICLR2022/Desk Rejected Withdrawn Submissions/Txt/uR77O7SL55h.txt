Under review as a conference paper at ICLR 2022
Scalable Sinkhorn Backpropagation
Anonymous authors
Paper under double-blind review
Ab stract
Optimal transport has recently gained increasing attention in the context of deep
learning. A major contributing factor is the line of work on smooth relaxations
that make the classical optimal transport problem differentiable. The most promi-
nent example is entropy regularized optimal transport which can be optimized ef-
ficiently via an alternating scheme of Sinkhorn projections. We thus experienced
a surge of deep learning techniques that use the Sinkhorn operator to learn match-
ings, permutations, sorting and ranking, or to construct a geometrically motivated
loss function for generative models. The prevalent approach to training such a
neural network is first-order optimization by algorithmic unrolling of the forward
pass. Hence, the runtime and memory complexity of the backward pass increase
linearly with the number of Sinkhorn iterations. This often makes it impractical
when computational resources like GPU memory are scarce. A more efficient
alternative is computing the derivative of a Sinkhorn layer via implicit differen-
tiation. Our main contribution is deriving a simple and efficient algorithm that
performs this backward pass in closed form. It is based on the Sinkhorn operator
in its most general form - with learnable cost matrices and target capacities. We
further provide a theoretical analysis with error bounds for approximate inputs.
Finally, we demonstrate that, for a number of applications, replacing automatic
differentiation with our module often improves the stability and accuracy of the
obtained gradients while drastically reducing the computation cost.
1	Introduction
Computing similarities between probability distributions is a fundamental problem in machine learn-
ing. In the most general case, tools like the Kullback-Leibler (KL) divergence (Kullback & Leibler,
1951) establish a notion of distance between such measures μ and V. In practice, μ and V are of-
ten themselves embedded in a metric space (Ω, dΩ). Common examples include spaces of images,
voxel grids, point clouds, 3D surface meshes, or generic Euclidean features. For this class of prob-
lems, optimal transport (OT) proves to be a powerful formalism (Villani, 2003; Peyre et al., 2019).
The Kantorovich formulation (Kantorovich, 1942) of OT yields a well-defined distance function
over probability measures on Ω that respects the geometric structure induced by dn.
The pioneering work of Cuturi (2013) further proposes to add an entropy regularization term to the
standard Kantorovich objective. This relaxation dramatically reduces the cost of computing the OT
distance between discrete measures μ, V from a super-cubic to a quadratic runtime complexity. A
crucial contributing factor that popularized entropy regularized OT in the context of deep learning
is that the resulting Sinkhorn divergence is differentiable. As a result, a number of works show
how the Sinkhorn divergence can be used as a loss function to train a machine learning model
(Luise et al., 2018; Genevay et al., 2018; Klatt et al., 2020; Ablin et al., 2020). In this work, we
consider an extension of this idea via the more general Sinkhorn operator. This operator is defined
as the mapping (C, a, b) 7→ P of a cost matrix C ∈ Rm×n and the source and target capacities
a ∈ Rm, b ∈ Rn to the transportation plan P ∈ R+m×n. In comparison, the Sinkhorn divergence
(a, b) 7→ hC , P iF is a special case of this operator (Cuturi, 2013, Eq. (2)).
It is nowadays common practice to design a deep neural network as the composition of various
mathematical functions or “layers”. Some of those layers may themselves be defined as the solution
of an (inner) optimization problem, as long as the mapping induced by the inner optimization is well
defined and differentiable. In order to train such a neural network, we need to solve a bilevel opti-
mization problem. The most common approaches to that end are algorithmic unrolling of the inner
1
Under review as a conference paper at ICLR 2022
Figure 1: Overview of a typical workflow with an embedded Sinkhorn layer. We consider a neu-
ral network whose input are e.g. images, 3D point clouds, voxel grids, surface meshes, etc. The
Sinkhorn layer maps the cost matrix C and marginals a, b to the transportation plan P via itera-
tive matrix scaling. In general, the whole network potentially contains learnable weights before and
after the Sinkhorn layer. To compute the gradients EC', Va', Vb') during training, instead of Us-
ing naive automatic differentiation, we propose a novel customised backward pass which computes
gradients efficiently and in closed form, see Algorithm 1.
optimization loop and implicit differentiation (Amos & Kolter, 2017; Gould et al., 2019; Bai et al.,
2019; Blondel et al., 2021). There is a vast literature on applications of combining the Sinkhorn op-
erator with deep learning, see our related work discussion in Section 4. The majority of approaches
unroll the forward optimization steps and apply automatic differentiation to compute gradients of
the resulting iterative scheme. Since the computation graph for all iterations needs to be maintained,
the resulting computation cost is often prohibitively high for GPU processing. To alleviate this cost,
there are a number of approaches related to our work that apply implicit differentiation to specific
subclasses of problems, like learning with a fixed cost matrix or immutable input capacities (Luise
et al., 2018; Flamary et al., 2018; Campbell et al., 2020; Cuturi et al., 2020; Klatt et al., 2020). We
can recover most such methods as special cases of our framework. In our work, we derive, from first
principles, an efficient algorithm to compute gradients of a generic Sinkhorn layer see Fig. 1. Our
contribution can be summarized as follows:
1.	We leverage the implicit function theorem to compute gradients of the Sinkhorn operator in
its most general form. We further use the Schur complement of the corresponding vector-
Jacobian product (VJP) to construct an efficient backpropagation algorithm (Algorithm 1).
2.	We provide theoretical guarantees for the accuracy of the resulting gradients, in dependency
of the approximation error in the forward pass (Theorem 5).
3.	Finally, we show that our algorithm can be used out of the box to enhance existing ap-
proaches that use a differentiable Sinkhorn layer. Plugging in our module often improves
the quantitative results while using significantly less GPU memory.
2	Background
Optimal transport Optimal transport enables us to compute the distance of two probability mea-
SUres on the same domain Ω ⊂ Rd. In this work, we focus on the specific case of discrete probability
measures μ := Pm=I 6几6&% and V := P；=i bjδyj, defined over the sets of points {xι,..., xml} and
{y1, . . . , yn}, where δxi is the Dirac measure at xi. Such measures are fully characterized by the
probability mass vectors a ∈ ∆m and b ∈ ∆n that lie on the probability simplex
∆m = {a ∈ Rm∣ai ≥ 0, a>lm = 1},	(1)
where mm ∈ Rm is the vector of all ones. We can then define the distance between μ and V as
d(μ, ν):=
min hP, CiF.
P ∈Π(a,b)
(2)
The transportation plan P ∈ Π(a, b) determines a discrete probability measure on the product space
{xι,..., Xm} X {yι,..., yn}, whose marginal distributions coincide with μ and V. Consequently,
2
Under review as a conference paper at ICLR 2022
P is contained in the transportation polytope Π(a, b) defined as
Π(a,b) := {P ∈ Rm×n∣PIn = a, P>Im = b}.	⑶
The cost matrix C ∈ Rm×n specifies the transportation cost from individual points xi to yj . Choos-
ing Ci j := ∣∣Xi 一 yjk2 for P ≥ 1, e.g. yields the so-called Wasserstein distance d(∙, ∙) = Wp(∙, ∙),
see Villani (2003).
Entropy regularization Evaluating the distance d(μ, V) in practice requires solving the linear
assignment problem (LAP) from Equation 2. This can be done via specialized algorithms like the
Hungarian algorithm (Kuhn, 1955) or the Auction algorithm (Bertsekas, 1979), as well as recent
solvers (Rubner et al., 1997; Pele & Werman, 2009). However, most approaches are computationally
heavy and slow in practice (Cuturi, 2013). A popular alternative is augmenting the LAP objective in
Equation 2 with an additional entropy regularizer, giving rise to the Sinkhorn operator
Sλ(C, a, b) := arg min hP, CiF 一 λh(P),	(4)
P ∈Π(a,b)
Where λ > 0. The seminal Work of Cuturi (2013) shoWed that the additional entropy regulariza-
tion term h(P) = 一 Pi,j Pi,j (log Pi,j 一 1) alloWs for an efficient minimization of Equation 4.
Specifically, this can be done via a scheme of alternating Sinkhorn projections
Sλ0) ：=exp(一；。)，and
Sλt+1) =TC(T (S,)).
(5)
The operators Tc(S) ：= S0(ImImS)Θ(lmb>) andTr (S) := S0(SInI>)Θ(a 1>) correspond
to renormalizations of the columns and rows of Sλ(t), where denotes the Hadamard product and
denotes element-wise division. As shown by Cuturi (2013), in the limit this scheme converges to a
minimizer S? —→→ Sλ OfEqUation 4. In practice, We can use a finite number of iterations T ∈ N
to achieve a sufficiently small residual.
3	Method
3.1	Problem formulation
The integration of the Sinkhorn operator from Equation 4 into deep neural netWorks has become pop-
ular for solving a Wide range of practical tasks, see our discussion in Section 4. A major contributing
factor is that the entropy regularization makes the mapping Sλ : Rm×n × Rm × Rn → Rm×n dif-
ferentiable. To alloW for first-order-optimization, We need to compute
(C, a,b)	-	P * := Sλ(C, a, b) and	(6)
▽p'	→	(Vc', Va', Vb'),	⑺
Which denote the forWard pass and the backpropagation of gradients, respectively. Those expressions
arise in the context of a typical WorkfloW Within a deep neural netWork With a scalar loss ' and
learnable parameters before and/or after the Sinkhorn operator Sλ, see Fig. 1 for an overvieW.
A common strategy is to replace the exact forWard pass Sλ (C, a, b) in Equation 6 by the approxi-
mate solution Sλ(τ ) from Equation 5. Like the original solution in Equation 4, Sλ(τ) is differentiable
W.r.t. (C, a, b). Moreover, the mapping (C, a, b) 7→ Sλ(τ) consists of a small number of matrix
scaling operations that can be implemented in a feW lines of code, see Equation 5.
For the backWard pass in Equation 7, an overWhelming number of applications noWadays rely on
automatic differentiation in environments like PyTorch (Paszke et al., 2019) or TensorFloW (Abadi
et al., 2016). While this solution is straightforWard and convenient, it has fundamental shortcomings:
First of all, the computation time for the backWard pass increases With the number of Sinkhorn
iterations τ. Moreover, the computation graph needs to be maintained for all τ matrix scaling
iterations. For many applications, this makes it infeasible or limits the resolution since the GPU
memory is often the computational bottleneck in practice. Instead, to address these shortcomings,
in the next section We utilize implicit differentiation in order to introduce a closed-form expression
for computing the gradients of ' W.r.t both the marginals a, b and the cost matrix C.
3
Under review as a conference paper at ICLR 2022
3.2	Backward pass via implicit differentiation
The goal of this section is to derive the main result stated in Theorem 3, which is the key motiva-
tion of our algorithm in Sec. 3.3. To this end, we start by reframing the optimization problem in
Equation 4 in terms of its KarUsh-KUhn-TUcker (KKT) conditions:
Lemma 1. The transportation plan P * is a global minimum ofEquation 4 iff
C + λ log(p*) + In 0 α* + β* 0 Im
K(c, a,b,p*, α*, β*):=	(1> 0 Im)p* — a	= 0ι,	(8)
(In 0 Op* — b
where l := mn + m + n. Here, α* ∈ Rm and β* ∈ Rn are the dual variables corresponding to the
two equality contraints in Equation 3. We further define c, p* ∈ Rmn as the vectorized versions of
C, P* ∈ Rm×n, respectively, and assume log(p) := -∞,p ≤ 0.
Proof. The fUnction K contains the KKT conditions corresponding to the optimization problem in
EqUation 4. The proposed identity therefore follows directly from the (strict) convexity of EqUa-
tion 4, see Erlander & Stewart (1990). Apart from the two eqUality constraints, EqUation 3 contains
additional ineqUality constraints Pi,j ≥ 0. Those are however inactive and can be dropped, becaUse
the entropy term in EqUation 4 invariably yields transportation plans in the interior of the positive
Orthant Pi,j > 0, see Peyre et al. (2019, p. 68).	□
Establishing this identity is an important first step towards compUting a closed-form gradient for the
backward pass in EqUation 7. It reframes the optimization problem in EqUation 4 as a root-finding
problem K(∙) = 0. In the next step, this then allows us to explicitly construct the derivative of the
Sinkhorn operator Sλ(∙) via implicit differentiation:
Lemma 2. The KKT conditions in Equation 8 implicitly define a continuously differentiable function
(c, a, b) 7→ (p, α, β) with the Jacobian matrix1
J:
d [p; α; β]
d [c； -a； —t>]
-1
λ diag(p)-1	E	∈ R(l-1)×(l-1)
E >	0
;_______ - /
:=K
(9)
Note that the last entry ofb := b-n and β := β-n is removed. This is due to a surplus degree of
freedom in the equality conditions from Equation 3, see part (b) of the proof. Likewise, for
E =[1 n 0 Im In 0 ‰] ∈ Rmn×(m+n),	(10)
the corresponding last column is removed E := E：,-(m+n).
Sketch of the proof. The basis for this proof is leveraging the implicit function theorem (IFT), which
we show in two parts, (a) and (b). See Appendix C.1 for the full proof.
(a)	We first establish that the matrix K is invertible. We can then prove by direct computation
.i . -r^-	ι . . ι	. ∙ ι ι	c" c	i '	. ∙	C . Γ	Gl EI
that K corresponds to the partial derivative of K from Equation 8 wrt. [p; α; β]. The
identity from Equation 9 then follows directly from the IFT, since the partial derivative of
K w.r.t. [c; —a; — b] is simply Q「° dK-= = Il.
d[c; —a； —b]
(b)	To show why the last condition from E is removed, we first assert that the kernel ker(E> )
is (m — 1)(n — 1) dimensional. The rank-nullity theorem then yields that the subspace
spanned by the columns ofE is mn— (m— 1)(n — 1) = m+n— 1 dimensional. Removing
the last condition E := E:,-(n+m) ensures that the m + n — 1 columns of E are linearly
independent and K is invertible.
□
1For brevity we use the short hand notation [v; u] := [v>, u>]> for stacking vectors v, u vertically.
4
Under review as a conference paper at ICLR 2022
In principle, we can use Lemma 2 directly to compute the backward pass from Equation 7. However,
the computational cost of inverting the matrix K in Equation 9 is prohibitive. In fact, even storing
the Jacobian J in the working memory of a typical machine is problematic, since it is a dense matrix
with O(mn) rows and columns, where m, n > 1000 in practice. Instead, we observe that computing
Equation 7 merely requires us to compute vector-Jacobian products (VJP) of the form v>J. The
main results from this section can therefore be summarized as follows:
Theorem 3 (Backward pass). For P = P *, the backward pass in Equation 7 can be computed in
closed form by solving the following linear system:
λdiag(P)T El Γ V/ ] _ Γ-Vp'^
.E>	0][-v[a ⑻' =[0 -
(11)
Proof. This identity follows trivially from Lemma 2 by applying the chain rule
V ~ ` = d d [p; Ca β] ! v ~ ' = -K-1V	` ' = K-1 -Vp'	(12)
V[c;-a;-b]' = I ∂ [c; -a; -5]	V[p；a；e]'= K	V[p;a；e]' = K ]01	(12)
The last equality holds, since the loss' does not depend on the dual variables α and β, see Fig. 1. □
3.3	Algorithm
In the previous section, we derived a closed-form expression of the Sinkhorn backward pass in
Theorem 3. This requires solving the sparse linear system in Equation 11, which has O(mn) rows
and columns, and thus amounts to a worst-case complexity of O(m3n3). We can further reduce the
computation cost by exploiting the specific block structure of K, which leads to our algorithm:
1
2
3
4
5
6
7
Algorithm 1: Sinkhorn operator backward
Input : VP `, P, a, b
Output: VC', Va', Vb'
T — P Θ VP'.
T 一 T：：,-n, P - P：,-n ∈ Rm×n-1.
t(a) 一 TIn, t(b) 一 T> lm.
-1
Va'	diag(a) P	t(a)
Vi'∖ J [ P>	diag⑻][Mb).
Vb' J [Vb';0].
U J Va'l> + ImVb'>.
VC'J -λ-1(T -PΘ U).
See Appendix A for a PyTorch implementation of this algorithm. We now show that the resulting
gradients VC', Va', Vb' from Algorithm 1 are indeed solutions of the linear system in Theorem 3.
Theorem 4. Let a, b be two input marginals and P = P* the transportation plan resulting from
the forward pass in Equation 6, then Algorithm 1 solves the backward pass defined in Equation 7.
Sketch of the proof. The main idea of this proof is showing that Algorithm 1 yields a solution
V[c;a;ib]' of the linear system from Equation 11. To that end, we leverage the Schur complement
trick which yields the following two expressions:
V[a;b]' = (E> diag(p)E)TE> diag(p)Vp'.	(13a)
Vc' = -λ-1(diag(p)Vp' - diag(P)EV3/).	(13b)
In Appendix C.2 we further show that these two identities in their vectorized form are equivalent to
Algorithm 1 in matrix notation.	□
5
Under review as a conference paper at ICLR 2022
3.4 Practical considerations
Error bounds Theorem 4 proves that Algorithm 1 computes the exact gradients YC', Va', Vb',
given that P = P * is the exact solution of Equation 4. In practice, the operator Sλ in Equation 6 is
replaced by the Sinkhorn approximation Sλ(τ) from Equation 5 for a fixed, finite τ ∈ N. This small
(τ)
discrepancy in the approximation P = Sλ ≈ P * propagates to the backward pass as follows:
Theorem 5 (Error bounds). Let P* := Sλ(C, a, b) be the exact solution of Equation 4 and let
P(τ) := Sλ(τ) be the Sinkhorn estimate from Equation 5. Further, let σ+, σ-, C1, C2, > 0, s.t.
P* - P(τ) F < and that for all P for which kP - P* kF < we have mini,j Pi,j ≥ σ-,
maxi,j Pij ≤ σ+ and the loss ' has bounded derivatives ∣∣Vp'∣∣2 ≤ Ci and ∣∣Vp'∣∣F ≤ C2.
For K = ∣∣E"∣2, where E* indicates the Moore-Penrose inverse of E, the difference between the
gradients VC '*, Va'*, Vb'* of the exact P * and the gradients VC '(τ), Vɑ'(τ), Vb '(τ) of the ap-
proximate P(τ), obtained via Algorithm 1, satisfy
||V[a;b]'* - V[a;b]'(T)||F ≤κ ^^C1 + C2^ ||P* - P(T)IlF, and (14a)
∣∣Vc'* - VC'(τ) IIF ≤λ-1σ+ Ci + C2) ∣∣P* - P(T) IIF.	(14b)
We provide a proof in Appendix C.3, as well as an empirical evaluation in Appendix B.1.
Computation cost In comparison to automatic differentiation (AD), the computation cost of Al-
gorithm 1 is independent of the number of Sinkhorn iterations τ . For square matrices, m = n, the
runtime and memory complexities of AD are O(τ n2). On the other hand, our approach has a run-
time and memory complexity of O(n3) and O(n2) respectively. We show empirical comparisons
between the two approaches in Sec. 5.1. Another compelling feature of our approach is that none of
the operations in Algorithm 1 explicitly convert the matrices P, VP', VC',…∈ Rm×n into their
vectorized form p, Vp', V°', ∙∙∙ ∈ Rmn. This makes it computationally more efficient since GPU
processing favors small, dense matrix operations over the large, sparse linear system in Equation 11.
Marginal probability invariance As discussed in Lemma 2, the last element of b needs to be
removed to make K invertible. However, setting the last entry of the gradient Vbn' = 0 to zero still
yields exact gradients: By definition, the full marginal b is constrained to the probability simplex
∆n, see Equation 1. In practice, we apply an a priori softmax to b (and analogously a). For some
applications, b can be assumed to be immutable, if we only want to learn the cost matrix C and not
the marginals a and b. Overall, this means that the gradient of b is effectively indifferent to constant
offsets of all entries, and setting Vbn' = 0 does not contradict the statement of Theorem 3.
4 Related work
There is a vast literature on computational optimal transport (OT) (Villani, 2003; Peyre et al.,2019).
In the following, we provide an overview of related machine learning applications. Our approach
is based on entropy regularized optimal transport pioneered by Cuturi (2013). The resulting differ-
entiable Sinkhorn divergence can be used as a loss function for training machine learning models
(Frogner et al., 2015; Feydy et al., 2019; Chizat et al., 2020). To allow for first-order optimization,
two common approaches for computing gradients are implicit differentiation (Luise et al., 2018; Cu-
turi et al., 2020; Klatt et al., 2020) and automatic differentiation (Genevay et al., 2018; Ablin et al.,
2020). Relevant applications of the Sinkhorn divergence include computing Wasserstein barycen-
ters (Cuturi & Doucet, 2014; Solomon et al., 2015; Luise et al., 2019), dictionary learning (Schmitz
et al., 2018), as well as using a geometrically meaningful loss function for autoencoders (Patrini
et al., 2020) or generative adversarial networks (GAN) (Genevay et al., 2018; Salimans et al., 2018).
More recently, several approaches emerged that use the Sinkhorn operator as a differentiable trans-
portation layer in a neural network. Potential applications include permutation learning (Santa Cruz
et al., 2017; Mena et al., 2018), ranking (Adams & Zemel, 2011; Cuturi et al., 2019), sorting via
reinforcement learning (Emami & Ranka, 2018), discriminant analysis (Flamary et al., 2018) and
6
Under review as a conference paper at ICLR 2022
computing matchings between images (Sarlin et al., 2020), point clouds (Yew & Lee, 2020; Yang
et al., 2020; Liu et al., 2020) or triangle meshes (Eisenberger et al., 2020; Pai et al., 2021). Most of
these approaches rely on automatic differentiation of the Sinkhorn algorithm to address the resulting
bilevel optimization problem. In our work, we follow the recent trend of using implicit differen-
tiation for the inner optimization layer (Amos & Kolter, 2017; Gould et al., 2019; Blondel et al.,
2021). Similar ideas are pursued in (Luise et al., 2018; Klatt et al., 2020; Campbell et al., 2020) for
the special subcases of learning with loss functions that are independent of the cost matrix or the
marginals. Other approaches compute the input cost matrix via Bayesian inverse modeling (Stuart
& Wolfram, 2020) or smooth the OT linear assignment problem (LAP) directly (Pogan et al., 2019).
5 Experiments
We now provide practical evidence for the merits of our Algorithm 1. In Sec. 5.1, we compare its
computation cost to automatic differentiation (AD). In Sec. 5.2 and Sec. 5.3, we show results on
two common classes of applications where we want to learn the marginals a and the cost matrix C
respectively. For all experiments, We assume a fixed GPU memory (VRAM) budget of 24GB - any
setting that exceeds this limit is deemed out of memory (OOM).
5.1	Computation cost
We empirically compare the computation cost of our algorithm With the standard automatic differ-
entiation approach, see Fig. 2. All results Were computed on a single NVIDIA Quadro RTX 8000
graphics card. In practice, the computation cost of both approaches primarily depends on the pa-
rameters m, n, τ . It is for the most part indifferent to other hyperparameters and the actual values
of C, a, b. We therefore use random (log normal distributed) cost matrices ln G,j 〜 N(0,1) and
uniform marginals a = b = §In with m = n ∈ {10,100,1000}. For each setting, we report
the cost of the forWard and backWard pass averaged over 1k iterations. Depending on m, n, our
approach is faster for τ & 40, 50, 90 iterations. Note that our backward pass is independent of the
number of forward iterations τ . Finally, the memory requirements are dramatically higher for AD,
since it needs to maintain the computation graph of all τ forward iterations. In practice, this often
limits the admissible batch size or input resolution, see Sec. 5.2 and Sec. 5.3.
# Snikhoni iterations
100
10-1
10-2
# Sinkhom iterations
# Sinkhorn iterations
Figure 2: Computational complexity. We compare the runtime per iteration (top row) and GPU
memory requirements (bottom row) of our approach (blue) and automatic differentiation (orange).
We consider a broad range of settings with quadratic cost matrices of size m = n ∈ {10, 100, 1000}
and τ ∈ [10, 2000] Sinkhorn iterations. For the runtime, we show both the total time (solid lines)
and the time of only the backward pass (dashed lines). Both ours and AD were implemented in the
PyTorch (Paszke et al., 2019) framework, where memory is allocated in discrete units, which leads
to a large overlap for the minimum allocation size of 2MB (bottom row, left plot).
7
Under review as a conference paper at ICLR 2022
AD
Ours
Figure 3: Wasserstein barycenter. A comparison between our method (top row) and automatic
differentiation (bottom row) on the application of image barycenter computation. In each cell, we
show 5 centroids of 4 input images (corners) with bilinear interpolation weights. The predictions
of our approach are more stable, even for very few Sinkhorn iterations τ . Moreover, AD is out of
memory for τ ≥ 200. Here, the input images have a resolution of n = 642 and we set λ = 0.002.
5.2	Wasserstein barycenters
Barycenter computation is a standard example of an inverse problem based on the Sinkhorn operator
in Equation 4. The main idea is to interpolate between a collection of objects {b1, . . . , bk} ⊂ Rn
as a convex combination with weights that lie on the probability simplex w ∈ ∆n, see Equation 1.
Specifically, the goal is to optimize for the marginal a* via the expression
k
a* := arg min	wid(a, bi) with d(a, b) := min hP, DiF - λh(P),	(15)
a∈∆n i=1	P ∈Π(a,b)
where D ∈ Rn×n denotes the squared pairwise distance matrix between the domains of a and
b. We use the Adam optimizer (Kingma & Ba, 2014) for the outer optimization in Equation 15.
The inner optimization is a special case of Equation 4. Overall, Equation 15 allows us to compute
geometrically meaningful interpolations in arbitrary metric spaces. We consider the explicit tasks
of interpolating between images in Fig. 3 and functions on manifolds in Appendix B.2. Note that
there are a number of specialized algorithms that minimize Equation 15 in a highly efficient manner
(Cuturi & Doucet, 2014; Solomon et al., 2015; Luise et al., 2019). We mainly consider the barycenter
problem a useful toy example to assess the stability of our algorithm in comparison to AD. On the
other hand, the interpretation as a generic optimization problem is overall more flexible and allows
us to trivially extend it to related tasks like image clustering, see Appendix B.2.
5.3 Permutation learning and matching
Number sorting The Sinkhorn operator is nowadays a standard tool to parameterize approximate,
learnable permutations within a neural network, see the second paragraph of our related work discus-
sion in Sec. 4. One work that clearly demonstrates the effectiveness of this approach is the Gumbel-
Sinkhorn (GS) method (Mena et al., 2018). The main idea is to learn the natural ordering of sets
of input elements {x1, . . . , xn}, see Appendix B.3 for more details. Here, we consider the concrete
example of learning to sort real numbers from the unit interval xi ∈ [0, 1] for n ∈ {200, 500, 1000}
numbers. We insert our Sinkhorn module in the GS network and compare the performance with the
vanilla GS method in Fig. 4. Without further modifications, our method significantly decreases the
error at test time, defined as the proportion of incorrectly sorted elements.
Point cloud registration A number of recent methods use the Sinkhorn operator as a differen-
tiable, bijective matching layer for deep learning (Sarlin et al., 2020; Yew & Lee, 2020; Yang et al.,
2020; Liu et al., 2020; Eisenberger et al., 2020). Here, we consider the concrete application of rigid
point cloud registration (Yew & Lee, 2020) and show that we can improve the performance with
our backward algorithm, see Table 1. While our results on the clean test data are comparable but
8
Under review as a conference paper at ICLR 2022
Q∞3 UOIfOdOJd
# Sinkhorn iterations
# Sinkhorn iterations
# Sinkhorn iterations
Figure 4: Number sorting. We show that we can improve the Gumbel-Sinkhorn method (Mena
et al., 2018) directly with our Sinkhorn module. Specifically, we consider the task of permutation
learning to sort random number sequences of length n ∈ {200, 500, 1000}, see Mena et al. (2018,
Sec 5.1) for more details. We replace AD in the GS network with our module (blue curves) and
compare the obtained results to the vanilla GS architecture (orange curves). Our approach yields
more accurate permutations while using much less computational resources - GS is out of memory
for τ > 200, 100, 50 forward iterations, respectively. For all settings, we show the mean proportion
of correct test set predictions (solid lines), as well as the 10 and 90 percentiles (filled areas).
slightly worse than the vanilla RPM-Net (Yew & Lee, 2020), our module generalizes more robustly
to partial and noisy observations. This indicates that, since computing gradients with our method is
less noisy than AD, it helps to learn a robust matching policy with less overfitting. We provide more
details on the RPM-Net baseline and qualitative comparisons in Appendix B.3.
6 Discussion
Our experiments clearly demonstrate that, for a broad range of applications, combining our Sinkhorn
module with existing approaches often improves the performance. On the other hand, AD has
an empirically faster runtime for very few Sinkhorn iterations τ ≈ 10 and high n ≥ 1000. In
our experiments, however, we see a clear trend that, to a certain point, increasing τ leads to more
accurate results. Choosing τ is generally subject to a trade-off between the computation cost and
accuracy. The main advantage of implicit differentiation is that it proves to be much more scalable
than AD, since the backward pass is independent ofτ. For the same reason, it also has a dramatically
lower GPU memory demand, see Fig. 2. We therefore believe that the theoretical and empirical
insights we provide have the potential to open up new avenues for future research, for which naive
backpropagation schemes are computationally intractable or not sufficiently accurate.
		clean data	90%	partial 80%	70%	σ = 0.001	noisy σ=0.01	σ=0.1
Rot. MSE	RPM	0.2329-	63.7670	69.6706	74.4227	37.2575^^	50.3618	66.1996
	Ours	0.3231	10.3078	24.5810	40.7793	1.7912	2.2110	4.0208
Trans. MSE	RPM	0.0014-	0.2659	-^0.3079	0.3462	0.1603^^	0.2100	0.2740
	Ours	0.0033	0.0797	0.1582	0.2420	0.0149	0.0181	0.0303
Chamf. dist.	RPM	0.0005-	4.3413	-^4.6829	4.9581	2.2077^^	3.0492	4.6935
	Ours	00054	05498	1.4291	2.2080	00783	0.1237	0.4562
Table 1: Point cloud registration. We compare the quantitative performance of RPM-Net (Yew &
Lee, 2020) and our approach on ModelNet40 (Wu et al., 2015). The two architectures are identical
except for the altered Sinkhorn module. For all results, we follow the training protocol described in
Yew & Lee (2020, Sec. 6). Moreover, we assess the ability of the obtained networks to generalize
to partial and noisy inputs at test time. For the former, we follow Yew & Lee (2020, Sec. 6.6) and
remove up to 70% of the input point clouds from a random half-space. For the noisy test set, we add
Gaussian white noise N(0, σ) with different variances σ ∈ {0.001, 0.01, 0.1}. For all settings, we
report the rotation and translation errors, as well as the Chamfer distance to the reference surface.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} symposium on operating systems design and imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Pierre Ablin, Gabriel Peyre, and Thomas Moreau. Super-efficiency of automatic differentiation for
functions defined as a minimum. In International Conference on Machine Learning, pp. 32-41.
PMLR, 2020.
Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint
arXiv:1106.1925, 2011.
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning, pp. 136-145. PMLR, 2017.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural
Information Processing Systems, 32:690-701, 2019.
Dimitri P Bertsekas. A distributed algorithm for the assignment problem. Lab. for Information and
Decision Systems Working Paper, MIT, 1979.
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-
Lopez, Fabian Pedregosa, and Jean-Philippe Vert. Efficient and modular implicit differentiation.
arXiv preprint arXiv:2105.15183, 2021.
Ethan D Bolker. Transportation polytopes. Journal of Combinatorial Theory, Series B, 13(3):251-
262, 1972.
Dylan Campbell, Liu Liu, and Stephen Gould. Solving the blind perspective-n-point problem end-
to-end with robust differentiable geometric optimization. In European Conference on Computer
Vision, pp. 244-261. Springer, 2020.
Lenaic Chizat, Pierre Roussillon, Flavien Leger, FranCOis-Xavier Vialard, and Gabriel Peyre. Faster
wasserstein distance estimation with the sinkhorn divergence. Advances in Neural Information
Processing Systems, 33, 2020.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In International
conference on machine learning, pp. 685-693. PMLR, 2014.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
optimal transport. Advances in Neural Information Processing Systems, 32, 2019.
Marco Cuturi, Olivier Teboul, Jonathan Niles-Weed, and Jean-Philippe Vert. Supervised quantile
normalization for low-rank matrix approximation. arXiv preprint arXiv:2002.03229, 2020.
Marvin Eisenberger, Aysim Toker, Laura Leal-Taixe, and Daniel Cremers. Deep shells: Unsuper-
vised shape correspondence with optimal transport. In Advances in Neural Information Process-
ing Systems, volume 33, pp. 10491-10502. Curran Associates, Inc., 2020.
Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. arXiv
preprint arXiv:1805.07010, 2018.
Sven Erlander and Neil F Stewart. The gravity model in transportation analysis: theory and exten-
sions, volume 3. Vsp, 1990.
Jean Feydy, Thibault SejOUrne, FranCOiS-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and
Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681-2690.
PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
Remi Flamary, Marco Cuturi, Nicolas Courty, and Alain Rakotomamonjy. Wasserstein discriminant
analysis. Machine Learning,107(12):1923-1945, 2018.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio.
Learning with a wasserstein loss. In Proceedings of the 28th International Conference on Neural
Information Processing Systems-Volume 2, pp. 2053-2061, 2015.
AUde Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617.
PMLR, 2018.
Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope.
arXiv preprint arXiv:1909.04866, 2019.
L Kantorovich. On the transfer of masses: Doklady akademii nauk ussr. 1942.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Marcel Klatt, Carla Tameling, and Axel Munk. Empirical regularized optimal transport: Statistical
theory and applications. SIAM Journal on Mathematics of Data Science, 2(2):419-443, 2020.
Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
quarterly, 2(1-2):83-97, 1955.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, 22(1):79-86, 1951.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Liu Liu, Dylan Campbell, Hongdong Li, Dingfu Zhou, Xibin Song, and Ruigang Yang. Learn-
ing 2d-3d correspondences to solve the blind perspective-n-point problem. arXiv preprint
arXiv:2003.06752, 2020.
Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. Differential properties of
sinkhorn approximation for learning with wasserstein distance. Advances in Neural Information
Processing Systems, 31:5859-5870, 2018.
Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with
free support via frank-wolfe algorithm. Advances in Neural Information Processing Systems, 32:
9322-9333, 2019.
G Mena, J Snoek, S Linderman, and D Belanger. Learning latent permutations with gumbel-
sinkhorn networks. In ICLR 2018 Conference Track, volume 2018. OpenReview, 2018.
Gautam Pai, Jing Ren, Simone Melzi, Peter Wonka, and Maks Ovsjanikov. Fast sinkhorn filters:
Using matrix scaling for non-rigid shape correspondence with functional maps. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 384-393, 2021.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in Artifi-
cial Intelligence, pp. 733-743. PMLR, 2020.
Ofir Pele and Michael Werman. Fast and robust earth mover’s distances. In 2009 IEEE 12th inter-
national conference on computer vision, pp. 460-467. IEEE, 2009.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019.
11
Under review as a conference paper at ICLR 2022
Marin Vlastelica Pogan, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentia-
tion of blackbox combinatorial solvers. In International Conference on Learning Representations,
2019.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 652-660, 2017.
Yossi Rubner, Leonidas J Guibas, and Carlo Tomasi. The earth mover’s distance, multi-dimensional
scaling, and color-based image retrieval. In Proceedings of the ARPA image understanding work-
shop, volume 661, pp. 668, 1997.
Tim Salimans, Dimitris Metaxas, Han Zhang, and Alec Radford. Improving gans using optimal
transport. In 6th International Conference on Learning Representations, ICLR 2018, 2018.
Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual
permutation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3949-3957, 2017.
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:
Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition, pp. 4938-4947, 2020.
Morgan A Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi,
Gabriel Peyre, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based
unsupervised nonlinear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643-678,
2018.
Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian BUtScher, Andy Nguyen,
Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transporta-
tion on geometric domains. ACM Transactions on Graphics (TOG), 34(4):1-11, 2015.
Andrew M Stuart and Marie-Therese Wolfram. Inverse optimal transport. SIAM Journal on Applied
Mathematics, 80(1):599-619, 2020.
Cedric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 1912-1920, 2015.
Lei Yang, Wenxi Liu, Zhiming Cui, Nenglun Chen, and Wenping Wang. Mapping in a cycle:
Sinkhorn regularized unsupervised learning for point cloud shapes. In European Conference on
Computer Vision, pp. 455-472. Springer, 2020.
Zi Jian Yew and Gim Hee Lee. Rpm-net: Robust point matching using learned features. In Proceed-
ings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11824-11833,
2020.
12
Under review as a conference paper at ICLR 2022
A PyTorch implementation
Our proposed algorithm can be easily integrated into existing deep learning architectures. In the
following, we provide our PyTorch (Paszke et al., 2019) implementation of this module. The for-
ward pass is the standard Sinkhorn matrix scaling algorithm (Cuturi, 2013) with a robust log-space
implementation. The backward function contains an implementation of our Algorithm 1.
import torch
class Sinkhorn(torch.autograd.Function):
@staticmethod
def forward(ctx, c, a, b, num_sink, lambd_sink):
log_p = -c / lambd_sink
log_a = torch.log(a).unsqueeze(dim=1)
log_b = torch.log(b).unsqueeze(dim=0)
for _ in range(num_sink):
log_p -= (torch.logsumexp(log_p, dim=0, keepdim=True) - log_b)
log_p -= (torch.logsumexp(log_p, dim=1, keepdim=True) - log_a)
p = torch.exp(log_p)
ctx.save_for_backward(p, torch.sum(p, dim=1), torch.sum(p, dim=0))
ctx.lambd_sink = lambd_sink
return p
@staticmethod
def backward(ctx, grad_p):
p, a, b = ctx.saved_tensors
m, n = p.shape
grad_p *= -1 / ctx.lambd_sink * p
K = torch.cat((
torch.cat((torch.diag(a), p), dim=1),
torch.cat((p.T, torch.diag(b)), dim=1)),
dim=0)[:-1, :-1]
t = torch.cat((
grad_p.sum(dim=1),
grad_p[:, :-1].sum(dim=0)),
dim=0).unsqueeze(1)
grad_ab, _ = torch.solve(t, K)
grad_a = grad_ab[:m, :]
grad_b = torch.cat((grad_ab[m:, :], torch.zeros([1, 1],
device=device, dtype=torch.float32)), dim=0)
U = grad_a + grad_b.T
grad_p -= p * U
grad_a = -ctx.lambd_sink * grad_a.squeeze(dim=1)
grad_b = -ctx.lambd_sink * grad_b.squeeze(dim=1)
return grad_p, grad_a, grad_b, None, None
B Additional experiments
B.1	Gradient accuracy
Theorem 5 states that the error of our backward pass in Algorithm 1 can be bounded by the error
of the forward pass in Equation 6. We now assess the magnitude of this error in practice by re-
visiting the experiments on image barycenter computation and number sorting from Sec. 5. Since
the ground truth gradients are unknown in general, we define the (approximate) ground truth gra-
dients as NC'* := NC'(TmaX), Va'* := V。'(Tmax) for TmaX ：= 10,000. In Fig. 5, we compare
the error of our approach to AD, averaged over all settings and iterations from the experiments in
Fig. 3 and Fig. 4 respectively. These results show clearly that, on average, our approach produces
13
Under review as a conference paper at ICLR 2022
Figure 5: Gradient accuracy. We empirically assess the accuracy of the error bound discussed in
Theorem 5. Specifically, We show the accuracy of the gradients Va' for the image barycenter exper-
iment in Sec. 5.2 and VC' for the number sorting experiment in Sec. 5.3. While both distributions
have a large overlap, the gradients from our approach are noticeably more accurate on average. Note
that both comparisons show histograms on a log scale.
Figure 6: Image barycenter gradients. A qualitative comparison of our gradients (3rd row), the AD
gradients (4th row), and the ground truth gradients (last row) for the image barycenter experiment
from Sec. 5.2. Specifically, we consider the task of interpolating between two input images (1st row)
with uniform interpolation weights w1 = w2 = 0.5. We show intermediate snapshots of the obtained
barycenter image (2nd row) for different numbers of gradient descent iterations t ∈ {0, . . . , 70} that
result from minimizing the energy in Equation 15.
14
Under review as a conference paper at ICLR 2022
Input pair
AD
Ours
Figure 7: Manifold barycenter. We compute barycenters of two circular input distributions on the
surface of a sphere (first row). Specifically, we compare the results of minimizing Equation 15 with
AD (second row) and our gradients (third row). The sphere is discretized as a triangular mesh with
5000 vertices. On this resolution, AD is out of memory for τ ≥ 200 Sinkhorn iterations whereas
ours is still feasible for τ = 1000. The obtained interpolations produce the slightly elongated shape
of an ellipse since the surface of the sphere has a constant positive Gaussian curvature.
τ=5
τ = 10
τ = 50
τ = 100
rs
ur
O
2 7β
/，，Q0
r ⅞⅛ 72
JI3f∙
ʃo R/f
∙ua 7。
f4∙*Q0
, 3, 74
<ll∙g3
ʃo Rzf
♦ 0 4∙73
/6 7Q。
■S6，a
JIfVJ
Γo H/ f
,2a 9B
, 6
ZB⅛ 7⅛
JIJrB
S O∙7/ Q
AD
f i 2 V3
/4 7q0
■ B6 7a>
f ʌ ɪ VB
, 6，，。
• Z 7∙
Jls<∙
/ S 63
t ⅛ 7 ɪ
¥ 7 H 5
Figure 8: MNIST k-means clustering. A comparison of our approach and automatic differentiation
on the task of k-means image clustering. For both approaches, we show the predicted k = 25 clusters
for τ ∈ {5, 10, 50, 100} Sinkhorn iterations. We choose more than 10 clusters to capture several
different appearances and styles for each digit. To make individual results more comparable, we
use an identical random initialization of the cluster centroids for all settings. For AD, the maximum
permissible batch sizes for the 4 settings are 512, 256, 64, 32, whereas ours consistently allows for a
batch size of 1024.
15
Under review as a conference paper at ICLR 2022
Clean
Partial 70%
Input
Noisy σ = 0.1
Ours
Ours
Ours
RPM-Net
RPM-Net
RPM-Net
Ours
Ours
Ours
RPM-Net
RPM-Net
RPM-Net
Ours
Ours
Ours
RPM-Net
RPM-Net
RPM-Net
Figure 9: Point cloud registration. A qualitative comparison of RPM-Net (Yew & Lee, 2020)
and the augmented version with our custom backward pass, see Alg. 1. The increased stability of
the gradients predicted by our algorithm directly translates into more robust generalization results:
Both methods are trained and tested on separate subsets of the 40 object categories in ModelNet40
(Wu et al., 2015), see Yew & Lee (2020, Section 6) for more details. Both methods yield accurate
predictions for the clean test data, as indicated by the corresponding quantitative results in Table 1.
On the other hand, our approach shows significant improvements when generalizing to noisy test
data and partial inputs. In each row, we show a different test pair with the input pose X (1st column,
blue), as well as the overlap of the reference pose Y (orange) and the predicted pose (blue) for the
clean, noisy, and partial settings.
Ours
RPM-Net
Ours
RPM-Net
Ours
RPM-Net
16
Under review as a conference paper at ICLR 2022
more accurate gradients than AD. Intuitively, ours yields optimal gradients for a suboptimal forward
pass P * ≈ SSτ, see Theorem 5, whereas AD yields approximate gradients for an approximate
forward pass. To further illustrate this point, we show a qualitative comparison of the gradients of
both approaches on the task of image barycenter computation in Fig. 6. These results demonstrate
the usefulness of our custom backward pass. Our gradients and the ground truth start disagreeing in
certain regions for t ≥ 50, but only after the barycenter optimization converges. Compared to the
vanilla AD approach, the gradients are much closer to the ground truth and therefore more useful.
B.2	Manifold barycenters and image clustering
In Section 5.2, we show that our approach can be used to compute Wasserstein barycenters of im-
ages. We can leverage the same approach to interpolate between distributions on more general man-
ifold domains. Specifically, we can minimize Equation 15 for a set of input marginals {b1, . . . , bk}
and use squared pairwise geodesic distances D ∈ Rm×n as the input cost matrix. In Fig. 7, we show
the predicted barycenters for our method and AD with two circular input distributions on a sphere.
Moreover, we can use a similar approach and leverage Equation 15 to predict a clustering of a set
of input images. To that end, we apply the k-means algorithm which alternates between computing
cluster centroids (by minimizing Equation 15) and assigning all images bi to their respective corre-
sponding centroid (defined as the centroid with minimum distance d, right side of Equation 15). We
show results on the 60k images from the MNIST dataset (LeCun, 1998) in Fig. 8. While our algo-
rithm is robust to varying Sinkhorn iterations τ, AD’s results vary significantly. For small τ ≤ 10,
the Sinkhorn approximation is not sufficiently exact. For higher τ , memory constraints force AD to
use a smaller batch size which again leads to instabilities.
B.3	Details on permutation baselines
Gumbel-Sinkhorn networks As outlined in Sec. 5.3, the goal of the Gumbel-Sinkhorn method
(Mena et al., 2018) is to learn how to sort a set of input elements {x1, . . . , xn}. To this end, the
cost matrix C is parameterized via a permutation-invariant neural network architecture (set en-
coder), conditioned on the input elements {x1, . . . , xn}. The matrix C, together with marginals
a = b = In are then passed through a differentiable Sinkhorn layer.2 The final output P is a bis-
tochatic matrix which encodes an approximate permutation. The training objective is a geometric
loss, minimizing the distance of the sorted elements xi to their natural ground-truth ordering. At
test time, the Hungarian algorithm (Kuhn, 1955) is applied to the learned cost matrix C to obtain an
exact, hard permutation. In Sec. 5.3, we show the concrete application of sorting n real numbers,
sampled randomly from the uniform distribution Xi 〜 U(0,1). More specifically, We consider sepa-
rate training and test sets of 4096 and 128 random sequences each and report the error, defined as the
proportion of incorrectly placed numbers in the test set, see Fig. 4. To provide a more complete pic-
ture, we report quantitative results on the task of generalizing to different test sets here. Specifically,
we train the vanilla GS method and our approach for τ = 200 (the maximum number for which GS
is below the GPU memory limit) for n = 200 numbers sampled from U(0, 1). We then investigate
the error on test sets sampled from different distributions Xi 〜 U(s, t) with s < t in Table 2. Even
though the variance is quite high, these results indicate that our method evidently helps to reduce
overfitting. Note, that the improved generalization is observed for the setting τ = 200, n = 200
where the performance of both methods on the standard test set is almost identical, see Fig. 4.
RPM-Net A number of recent works use the Sinkhorn operator as a differentiable matching mod-
ule for deep learning (Sarlin et al., 2020; Yew & Lee, 2020; Yang et al., 2020; Liu et al., 2020;
Eisenberger et al., 2020). The standard strategy of such methods is to use a learnable feature ex-
tractor to obtain descriptors FX ∈ Rm×p, FY ∈ Rn×p on pairs of input objects X and Y in a
siamese manner. We can then define the cost matrix C := D as the squared pairwise distance ma-
trix Di,j = F:X,i - F:Y,j 22. For fixed, uniform marginals a and b, the Sinkhorn operator then yields
a soft matching P = Sλ(C, a, b) ∈ Rm×n between the two input objects. The baseline method
RPM-Net we consider in Sec. 5.3 uses this methodology to obtain a matching between pairs of input
2Strictly speaking, the choice of marginals a = b = In does not fit in our framework, since we require
a, b ∈ ∆n, see Equation 1. However, we can simply use a = b = nIn and scale the resulting transportation
plan P by a constant factor of n.
17
Under review as a conference paper at ICLR 2022
	U(1, 2)	U (10,11)	U(100, 101)	U(-1,0)
Ours	0.2964 ± 0.0744	0.3340 ± 0.3059	0.3531 ± 0.1380	0.3552 ± 0.2116
Gumbel-Sinkhorn	0.3163 ± 0.0976	0.3620 ± 0.3179	0.3955 ± 0.1478	0.4678 ± 0.2526
Table 2: Number sorting generalization. We assess the capability of our approach (first row) and
the vanilla Gumbel-Sinkhorn method (second row) (Mena et al., 2018) to generalize to various test
sets U(s, t) with s < t. We train both methods to sort sets of n = 200 numbers xi from the uniform
distribution on the unit intervalU(0, 1) with τ = 200 Sinkhorn iterations. For each test set, we show
the mean proportion of false predictions, as well as the empirical standard deviation, obtained from
50 test runs per setting.
point clouds. As a feature extractor, it uses PointNet (Qi et al., 2017) with a custom input task point
cloud, see Yew & Lee (2020, Sec. 5.1) for more details. Using the obtained soft correspondences, a
simple SVD transformation then yields the optimal rigid transformation between the two input point
clouds. In order to train their model, RPM-Net uses automatic differentiation of the Sinkhorn layer.
We can now demonstrate that replacing AD with our backward algorithm improves the performance.
To that end, we revisit the experiments from Yew & Lee (2020, Sec. 6): We use 20 separate object
identities of the ModelNet40 dataset (Wu et al., 2015) as our train and test set respectively. In Fig 9,
we now show a qualitative comparison corresponding to the results in Table 1 in the main paper. On
the standard test set, both approaches produce comparable, high-quality results. On the other hand,
our method significantly improves the robustness when generalizing to noisy data or partial views.
C Proofs
In the following, we provide proofs of Lemma 2, Theorem 4 and Theorem 5 from the main paper.
C.1 Proof of Lemma 2
Proof. As mentioned above, the key for proving this statement is applying the implicit function
theorem. We start by (a) showing that this indeed yields the identity from Equation 9, and then (b)
justify why removing the last (m + n)-th equality condition from E is necessary.
(a)	First of all, we can verify by direct computation that the matrix K from Equation 9 corre-
sponds to the partial derivatives of the KKT conditions from Equation 8
K = ∂ ∂Κ(c, a,b, P, α, β)
I	∂ [p; α; β]
(16)
where the notation (∙)-ι,-ι means that the last row and column is removed and where
l = mn + m + n. Furthermore, K is invertible (since the solution lies in the interior
Pi,j > 0, see Peyre et al. (2019, p. 68)), and the m + n - 1 columns of E are linearly
dependent, see (b). Consequently, the implicit function theorem states that K implicitly
defines a mapping (c, a, b) 7→ (p, α, β) whose Jacobian is
d __ d [p; α; β]
J -- P
∂	c; -a; -b
K	-1
[p; α; β] -l,-l ∂ [c; -a; -b]
'---------------------------------
{z∖^^^
=Il-1
= -K-1.	(17)
-l,-l
K
—
(b)	As part of the proof in (a), we use the fact that the columns of E are linearly independent.
Verifying this statement also provides insight as to why removing the last row of b, β and
the last column of E is necessary. Intuitively, the columns of E contain one redundant
condition: The identity
m
Xi Pi,n - 0 Q⇒ E>,m+np - 0,	(18)
i=1
18
Under review as a conference paper at ICLR 2022
follows directly from the other m+n- 1 conditions. More formally, we can take the kernel
ker(E>) = {P ∈ Rm×n∣E>p = 0},
(19)
of E> ∈ Rm+n×mn and observe that dim(ker(E>)) = (m - 1)(n - 1), see Bolker
(1972, Sec. 1). The rank-nullity theorem then implies that the dimension of the subspace
spanned by the columns of E is of dimension mn - (m - 1)(n - 1) = m + n - 1.
Consequently, removing the redundant condition in Equation 19 from E yields the reduced
E ∈ Rmn×m+n-1 with = m + n - 1 linearly independent columns.
C.2 Proof of Theorem 4
Proof. We want to show that the gradients NC', Va', Vb' obtained with Algorithm 1 are equivalent
to the solution of Equation 11. To that end, we start by applying the Schur complement trick to the
block matrix K. This yields the expression
(K-1):,1
λ-1 diag(p)(Imn + E(E> diag(p)E)TE> diag(p))
(ɪ-i--	,	、工、——1 ~-τ	-	,	、
E> diag(P)E) E> diag(p)
(20)
for the first mn columns of its inverse. In the next step, we can insert this expression in Equation 11
and invert the linear system of equations
Vc '
-V[a；b]'
K-1
-Vp'
0
-(K-1)：,1:mnVp'
(21)
Further simplification yields the following identities for the gradients of C , a and b
Vc'
V[a；M
-λ-1 diag(p)(Imn — E(E> diag(p)E)-1 E> diag(p))Vp'
(E> diag(P)E) 1E> diag(p)Vp'
—λ-1 (diag(p)Vp' — diag(P)E V
(E> diag(P)E)-1
[a涧 ')
E> diag(p)Vp'
(22)
where the latter equality results from substituting the obtained expression for V^.b^' in the first
block row. In the remainder of this proof, we can show line by line that these expressions yield
Algorithm 1. The main idea is to first compute the second block row identity in Equation 22 and
then use the result to eventually obtain Vc' from the first block row:
l. 1 The first line defines the matrix T := PVP' via the Hadamard product . In vectorized
form it corresponds to the expression
t = diag(P)Vp'.
(23)
l. 2 As detailed in Lemma 2, we remove the last equality condition from E to obtain E . Equiv-
alent considerations require us to introduce the truncated versions T, P ofT, P.
l. 3 The operator E> then maps t to the vector
E>t = [In 0 Im	In 0 Im]> t
(1> 0 Im)t
(In 0 ɪm)t,
T In
T> Im
(24)
that contains its row and column sums. In terms of the truncated E, the last row of EqUa-
tion 24 gets removed, thus
t(a)
t(b)
(25)
19
Under review as a conference paper at ICLR 2022
E> diag(P)E
diag(a) P
P> diag(b) .
(26)
l. 4 A direction computation reveals that
(1> 0 Im) diag(p)(In 0 Im) (1> 0 Im) diag(P)(In 0 Im)
(In 0 Im)diag(p)(In 0 Im) (In 0 ɪm)diag(P)(In 0 Im)
diag(P In)	P
_	P >	diag(P >lm).
The linear system in L 4 of Algorithm 1 therefore yields the gradients Vq同' by inserting
Equation 23, Equation 25 and (the reduced version of) Equation 26 into the second block
row identity from Equation 22.
L 5 We can expand Vb' to Vb' by appending zero Vbn' = 0 as the last entry. Since b is
constrained to the probability simplex ∆n this gradient is exact for all entries, see the
discussion in Section 3.4.
l. 6 Having computed the gradients V[a;b]', we can now insert them in the first row of Equa-
tion 22. Here, the reduced and the original expressions are equivalent
ɪ___ _________ .
E V[a;b]' = EV[a；b]'	(27)
because l. 5 specifies Vbn' = 0. Thus,
EV[a；b]' =	[In	0	Im	In	0	ɪm]	V[a；b]' = In 0 Va' + Vb' 0	Im	=：	U,	(28)
defines the vectorized version of U from l. 6.
l. 7 Putting everything together, we insert the identities from Equation 23 and Equation 27 into
the first block row of Equation 22
Vc' = -λ-1(diag(p)Vp' — diag(P)E V[a;b]') = -λ-1(t — diag(p)u),	(29)
which is equivalent to the matrix-valued expression in l. 7.
□
C.3 Proof of Theorem 5
Proof. The key for constructing the error bounds in Equation 14a and Equation 14b is finding a
bound for the first-order derivatives d∂pc' and d%b]'. For brevity, We introduce the short-hand
notation P := diag(P). Furthermore, We define the projection ofx onto the column space of E as
∏e X ：= arg min Ilx — y∣∣p = E arg min ∣∣x — E ZkP,	(30)
y∈span(E)	z∈Rm+n-1
where (，)p :=(P 1 ∙, P 1 ∖?. In matrix notation, Equation 30 reads
∏e = E (E > P E )-1E >P.	(31)
Using Equation 13a and Equation 13b from the proof of Theorem 4, we can then rewrite the back-
ward pass compactly as
V[a；b]' = E *∏e Vp',	and	(32a)
Vc' = -λ-1(P(I — ∏E )Vp'),	(32b)
The first identity follows from EtE = I, since the columns of E are linearly independent (see part
(b) of the proof of Lemma 2 in Appendix C.1). Direct substitution of Equation 13a into Equation 13b
immediately yields Equation 32b. To differentiate Vq*' and V°', we apply the chain rule which
in turn requires a closed-form expression for the derivative of the projection operator ΠE . Since it
is defined as the solution of an optimization problem, we apply the implicit function theorem to the
gradient of the objective in Equation 30, i.e.
Vz (2kx - EZkP) = E>PEz - E>Px = 0.	(33)
20
Under review as a conference paper at ICLR 2022
The Jacobian of the mapping p → ∏e x can therefore be written in terms of the IFT as
∂ ∏e X
∂ P
E(EyPE)TET diag(x - ∏ex),
(34)
This auxiliary result implies that the Jacobians of the mappings P → Vq同' and P → Vc' defined
in Equation 32a and Equation 32b are
dVPb]' =Et∂P∏eVp' = (ETPE)TET diag((I - ∏e)Vp() + EEeVp',	and (35a)
d∂p' = - λ-1 (diag((I - ∏e)Vp') - ∏E diag((I - ∏e)Vp') + P(I - ∏E)Vp')
=-λ-1 f(I - ∏E) diag((I - ∏e)Vp') + P(I - ∏e)Vp∕j.
(35b)
In order to bound the errors of these two gradients, we first derive an upper bound for the norm of
the operator ∏e. An important insight is that we can precondition ∏e via P 1
P2 ∏e P- 2 x =	arg min ∣∣x - y∣∣∣,	(36)
y∈span(P2 E)
which results in an orthogonal projection P2 ΠEP-1. Since such projections have a spectral radius
of at most 1, we can bound the norm of ∏e as
Il ∏e Il ∣ ≤∣∣P-2 Il ∣∣∣P1 ∏eP- 1II ∣∣∣P2 Il ∣ ≤∣∣P-1 II ∣∣∣P 1II ∣,
and equivalently show for the complementary projector I - ∏e that
∣∣I - ∏eII∣ ≤ IIP - 2II∣IIP1 (I - ∏e )P - 1II∣IIP 2II∣ ≤ IIP - 1II∣IIP 1II∣
The Jacobians of the backward pass can then be bounded as
dva∕
∂ p
≤ I I (E tp E)-1e tII∣II(i - ∏e )Vp'I I∣ + IIE Ee Vpq^
F
=I I E t∏E P-1II∣II(I - ∏E )Vp'II∣ + IIE Ee vp'I
≤ I I E tII∣IIP, - 2ii∣iii - ∏eIi∣iivp'I i∣+iie tII∣II∏EII∣nvp'I If
≤ I I e tll∣llp - 2ll∣llP 2ll∣(llP - 1ll∣∣vp'l l∣ + IIvp'I∣F)
≤^rσ+Ci+C), and
MIL P-1IIi - ∏EII∣IIi - ∏eII∣IIvp'II∣ + λ-1IIPj(I - ∏e )II∣IIvp'IIf
'≤λ-1IIP - 1II∣IIP 2II∣IIVp'I I∣ + λ-1IIP 2II∣IIvp'I If
≤λ-%+ (σ-Ci+ C∣),
(37)
(38)
(39a)
(39b)
where the constants σ-, σ+, Ci, C∣ > 0 are as defined in Theorem 5, and where we use the identity
IIA diag⑸ l l F ≤ llAll∣lldiag⑸llF = llAll∣llbll∣.	(40)
As a direct consequence, we obtain the bounds from Equation 14a and Equation 14b, since the
bounded derivatives imply the Lipschitz continuity of the differentiable map P → V[c；a；b]'.
□
21