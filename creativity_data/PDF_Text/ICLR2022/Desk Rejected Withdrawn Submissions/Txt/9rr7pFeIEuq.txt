Under review as a conference paper at ICLR 2022
A general sample complexity analysis of
VANILLA POLICY GRADIENT
Anonymous authors
Paper under double-blind review
Abstract
We adapt recent tools developed for the analysis of Stochastic Gradient Descent
(SGD) in non-convex optimization to obtain convergence guarantees and sample
complexities for the vanilla policy gradient (PG) - REINFORCE and GPOMDP
Our only assumptions are that the expected return is smooth w.r.t. the policy pa-
rameters and that the second moment of its gradient satisfies a certain ABC as-
sumption. The ABC assumption allows for the second moment of the gradient
to be bounded by A ≥ 0 times the suboptimality gap, B ≥ 0 times the norm
of the full batch gradient and an additive constant C ≥ 0, or any combination of
aforementioned. We show that the ABC assumption is more general than the com-
monly used assumptions on the policy space to prove convergence to a stationary
point. We provide a single convergence theorem under the ABC assumption, and
4
show that, despite the generality of the ABC assumption, We recover the O(e 4)
sample complexity of PG. Our convergence theorem also affords greater flexibil-
ity in the choice of hyper parameters such as the step size and places no restriction
on the batch size m. Even the single trajectory case (i.e., m =1) fits within our
analysis. We believe that the generality of the ABC assumption may provide theo-
retical guarantees for PG to a much broader range of problems that have not been
previously considered.
1	Introduction
Policy gradient (PG) is one of the most popular reinforcement learning (RL) methods for computing
policies that maximize long-term rewards (Williams, 1992; Sutton et al., 2000). The success of PG
methods is due to their simplicity and versatility, as they can be readily implemented to solve a wide
range of problems (including non-Markov and partially-observable environments) and they can be
effectively paired with other techniques to obtain more sophisticated algorithms such as the actor-
critic (Konda & Tsitsiklis, 2000; Mnih et al., 2016), natural PG (Kakade, 2002), trust-region based
variants (Schulman et al., 2015; 2017), and variance-reduced PG (Papini et al., 2018; Shen et al.,
2019; Xu et al., 2020b; Yuan et al., 2020; Pham et al., 2020).
Unlike value-based methods, a solid theoretical understanding of even the “vanilla” PG has long
been elusive. Recently, a more complete theory of PG has been derived by leveraging the RL
structure of the problem together with tools from convex and non-convex optimization. Due to
space constraints, we defer a thorough review of recent results to App. A.
In this paper, we focus on the sample complexity of PG for reaching a FOSP (first-order stationary
point). We show how PG can be analysed under a very general assumption on the second moment of
the estimated gradient called the ABC assumption, which includes most of the bounded gradient type
assumptions as a special case. Under the ABC and a smoothness assumption on the expected return,
we obtain convergence guarantees and the sample complexity for both REINFORCE (Williams,
1992) and GPOMDP (Sutton et al., 2000; Baxter & Bartlett, 2001). Our sample complexity analysis
recovers both the well known O(e-2) iteration complexity of exact PG and the Oe(e-4) sample
complexity of REINFORCE and GPOMDP under weaker assumptions than had previously been
explored. Furthermore, our analysis is less restrictive when it comes to the hyper-parameter choices.
In fact, our results allow for wide range of step sizes and place almost no restriction on the batch size
m, even allowing for single trajectory sampling (m = 1), which is uncommon in the literature. The
generality of our assumption allows us to unify much of the fragmented results in the literature under
1
Under review as a conference paper at ICLR 2022
Table 1: Overview of different convergence results for vanilla PG methods. The darker shaded cells
contain our new results. The medium shaded cells contain previously known results that we recover
as special cases of our analysis, and extend the permitted parameter settings. White cells contain
existing results that we could not recover under our general analysis.
Guarantee*	Setting**				Reference (our results in bold)	Bound	Remarks
Sample complexity of stochastic PG for FOSP		ABC			Thm.34	O(e-4)	Weakest asm.
		E-LS			Papini (2020) Cor. 47	O(e-4)	Weaker asm.; Wider range of parameters; Recover O(e-2) for exact PG; Improved smoothness con- stant
Sample complexity of stochastic PG forGO	ABC + ]		PL		Thm. G2	-~ . O(e-1)	Recover linear conver- gence for exact PG
	IABC∣+ Weak PLl				Thm. G4	O(e-3)	Recover O(e-1) for exact PG
Sample complexity of stochastic PG for AR	-∣LS]+FI + compatible				Liu et al. (2020)	O(e-4)	
	Softmax + log barrier (26)				Zhang et al. (2020b) Cor.∣4710	O(e-6)	Constant step size; Wider range of parameters; Extra phased learning step unnecessary
Iteration complexity of the exact PG forGO	Softmax + log barrier (26)				飞garwaletal.l2021下 Cor.丽	O(e-2)	Improved by 1 - Y
	Softmax (23)				—Mei et al.72020)- Thm. G4	O(e-1)	
	Softmax + entropy (84)				—Mei et al.(亚0)- 'Thm.∣G∙2∣r	linear	
	LS+ bjection + PPG				Zhang et al. (2020a)	O(e-1)	
	TabUIar + PPG				Xiao&Lf(2021)	O(L)	
	LQR				Fazel et al. (2018)	linear	
* Type of convergence. FOSP: first-order stationary point; GO: global optimum; AR: average regret to the
global optimum.
** Setting. bijection: Asm.1 in Zhang et al. (2020a) about occupancy distribution; PPG: analysis also holds
for the projected PG; FI: Asm. 2.1 in Liu et al. (2020) on Fisher information; compatible: Asm. 4.4 in Liu et al.
(2020) on function approximation error; Tabular: direct parametrized policy; LQR: linear-quadratic regulator.
one guise. Indeed, we show that the analysis of Lipschitz and smooth policies, Gaussian polices,
softmax tabular polices with or without a log barrier regularizer are all special cases of our general
analysis (see hierarchy diagram further down in Figure 1).
Recently, there has also been much work on establishing the convergence ofPG to a global optimum
(i.e., the best-in-class policy) (Fazel et al., 2018; Agarwal et al., 2021; Zhang et al., 2020a; Mei
et al., 2020; Liu et al., 2020; Zhang et al., 2020b; 2021). This usually requires more restrictive
assumptions and specific RL settings (e.g., tabular). While our primary focus here is convergence
to a stationary point, under the ABC and smoothness assumptions, we also establish the global
optimum convergence theory when an additional (weak) gradient domination assumption is verified
(App. G). Table 1 provides a complete overview of our results, how they recover existing results, as
well as cases where we could not directly apply our general analysis.
We believe that the generality of the ABC assumption may provide theoretical guarantees for PG for
a broader range of problems that have not been previously considered, and help unify our current
understanding of PG and the many assumptions currently in use.
2
Under review as a conference paper at ICLR 2022
2 Preliminaries
Markov decision process (MDP). We consider a continuous MDP given by {S, A, P, R, γ, ρ},
where S is a state space; A is an action space; P is a Markovian transition model, where P(s0 |
s, a) is the transition density from state s to s0 under action a; R is the reward function, where
R(s, a) ∈ [-Rmax, Rmax] is the bounded reward for state-action pair (s, a) ; γ ∈ [0, 1) is the
discounted factor; and ρ is the initial state distribution. The agent’s behaviour is modelled as a
policy ∏ ∈ ∆(A)S, where ∏(∙ | S) is the density distribution over A in state S ∈ S. We consider the
infinite-horizon discounted setting.
Let p(τ | π) be the probability density of a single trajectory τ being sampled from π, that is
∞
p(τ | π) = ρ(S0)	π(at | St)p(St+1 | St,at).	(1)
t=0
With a slight abuse of notation, let R(τ) = P∞=0 γtR(St, at) be the total discounted reward accu-
mulated along trajectory τ . We define the expected return of π as
J(∏) =f Eip(∙∣π) R(T)].	⑵
Policy gradient. We introduce a set of parametrized policies {πθ : θ ∈ Rd}, with the assumption
that πθ is differentiable w.r.t. θ. We denote J(θ) = J(πθ) and p(τ | θ)=pθ (τ) = p(τ | πθ). The
PG methods use gradient ascent in the parametrized space of θ to find the policy that maximizes the
expected return J (θ). That is, the policy with the optimal parameters θ* ∈ arg supθ∈Rd J (θ) would
give the optimal expected return J* =f J(θ*). In general, J(θ) is a non-convex function.
The gradient VJ(θ) of the expected return has the following structure
VJ(θ) =
R(τ)Vp(τ | θ)dτ
/ R(T) (Vp(T | θ"p(T
| θ)) p(τ | θ)dτ
(3)
∞∞
=ET 〜p(∙∣θ) [R(T)V log p(T | θ)] (=1) Eτ X γtR(St, at) X Vθ logπθ(at0 | St0) .
t=0	t0=0
In practice, we cannot compute this full gradient, since computing the above expectation would
require averaging over all possible trajectories T 〜 p(∙ | θ). We resort to an empirical estimate
of the gradient by sampling m truncated trajectories Ti = (so, a0, r0, si,…，sh-i, aH-ι,rH-ι)
obtained by executing πθ for a given fixed horizon H ∈ N. The resulting gradient estimator is
m H-1	H-1
V mJ (θ) = - XX YtR(St,at) ∙ X Vθ log ∏θ (at | sii,).	(4)
i=1 t=0	t0=0
The estimator (4) is known as the REINFORCE gradient estimator (Williams, 1992).
The REINFORCE estimator can be simplified by leveraging the fact that future actions do not de-
pend on past rewards. This leads to the alternative formulation of the full gradient
VJ(θ) = Eτ
∞
X γtR(st,at)
t=0
Vθ log πθ(ak | sk)
(5)
)
which leads to the following estimate of the gradient known as GPOMDP (Baxter & Bartlett, 2001)
m H-1	t
V mJ (θ) = - XX Y tR(sit,ait) X Vθ log ∏θ (ak | Sk) .	(6)
i=1 t=0	k=0
Notice that both REINFORCE and GPOMDP are the truncated versions of unbiased gradient esti-
mators. More precisely, they are unbiased estimates of the gradient of the truncated expected return
JH(θ)d=efEτ hPtH=-01 YtR(St,at)i 1.
Equipped with gradient estimators, vanilla policy gradient updates the policy parameters as follows
θt+1 = θt + ηtVbmJ(θt)	(7)
where ηt > 0 is the step size at the t-th iteration (see also Algorithm 1 in App. A).
1We allow H to be infinity so that J∞ (∙) = J(∙).
3
Under review as a conference paper at ICLR 2022
3 Non-convex optimization under ABC assumption
We use V m,J (θ) to denote the unbiased policy gradient estimator of RJH (θ) used in (7). It can be
the full gradient estimator VJ(θ) when H = m = ∞, or one of the truncated gradient estimators de-
fined in (4) or (6). All our forthcoming analysis relies on the following common assumptions.
Assumption 3.1 (Smoothness). There exists L > 0 such that, for all θ, θ0 ∈ Rd, We have
|J(θ0) - J(θ)-hVJ(θ),θ0-θi∣ ≤ 2 kθ0-θk2.	(8)
Assumption 3.2 (Truncation). There exists D, D0 > 0 such that, for all θ ∈ Rd, we have
KVJH(θ), VJh(θ) -VJ(θ)i∣ ≤ DYH,	(9)
kVJH(θ) -VJ(θ)k ≤ D0γH.	(10)
We recall that given the boundedness of the reward function, we have | J(θ) 一 JH(θ)∣ ≤ RR-maxYH
by the definition of J(∙) and JH(∙). As such, when H is large, the difference between J(θ) and
JH (θ) is negligible. However, Asm. 3.2 is still necessary, since in our analysis we first prove that
kVJH(θ)k2 is small, and then rely on (10) to show that kVJ (θ)k2 is also small.
We also make use of the recently introduced ABC assumption (Khaled & Richtarik, 2020)2 which
bounds the second moment of the norm of the gradient estimators using the norm of the truncated
full gradient, the suboptimality gap and an additive constant.
Assumption 3.3 (ABC). There exists A, B, C ≥ 0 such that the policy gradient estimator satisfies
E ∣∣VmJ(θ)∣∣21 ≤ 2A(J* - J(θ)) + B kVJH(θ)k2 + C, Vf) ∈ Rd. (ABC)
The ABC assumption effectively summarizes a number of popular and more restrictive assumptions
commonly used in non-convex optimization. Indeed, the bounded variance of the stochastic gradi-
ent assumption (Ghadimi & Lan, 2013), the gradient confusion assumption (Sankararaman et al.,
2020), the sure-smoothness assumption (Lei et al., 2020) and different variants of strong growth as-
sumptions proposed by Schmidt & Roux (2013); Vaswani et al. (2019) and Bottou et al. (2018) can
all be seen as specific cases of Asm. 3.3. The ABC assumption has been shown to be the weakest
among all existing assumptions to provide convergence guarantees for SGD for the minimization
of non-convex smooth functions. A more detailed discussion of the assumption for non-convex
optimization convergence theory can be found in Thm. 1 in (Khaled & Richtarik, 2020).
We state our main convergence theorem, that we will then develop into several corollaries.
Theorem 3.4. Suppose that Assumptions 3.1, 3.2 and 3.3 hold. Consider the iterates θt of the
PG method (7) with stepsize η = η ∈ (0,逢)where B = 0 means that η ∈ (0, ∞). Let
δo d= J* 一 J(θo). It follows that
2	2δo(1 + LAη2)T	LCη	∕2D(3 - LBn)	« 八 H
0≤m≤iT-ιE [kVJ(θt)k2] ≤ nT(2-LBnn) + 2-⅛ + ( 2(-LBn	+ D YH)YH
(11)
In particular if A = 0, we have
E hkVJ(θu)『]≤	+ - + ✓ 2D3-LBn) + D02γH) γH,	(12)
n±(2 — LBn)	2 — LBn	∖ 2 — LBn
where θu is uniformly sampled from {θo, θ1,…，θτ-1}.
2While Khaled & RiChtarik (2020) refer to this assumption as expected smoothness, we prefer the alternative
name ABC to avoid confusion with the smoothness of J.
4
Under review as a conference paper at ICLR 2022
We give the proof of Thm. 3.4 in App. C.1. While Thm. 3.4 is based on Thm. 2 in (Khaled &
Richtarik, 2θ2θ), our proof has to take care of the specific structure of PG estimators, notably the
bias due to the truncation.
Thm. 3.4 provides a very general characterization of the performance of PG as a function of all the
constants involved in the assumptions on the problem and the policy gradient estimator. From (11)
we can derive the sample complexity of PG as follows.
Corollary 3.5. Consider the setting of Thm. 3.4. Given e > 0, let η = min {√=, ɪ, ^^ }
and the horizon H = O (log e-1). If the number of iterations T satisfy
12δoL ʃ	12δoA
T ≥ —^2— max B B, —^2—,
then mino≤t≤τ-i E [∣NJ(0t)『]=O(e-2).
Despite the generality of the ABC assumption, Cor. 3.5 recovers the best known iteration com-
plexity for vanilla PG in several well known special cases. For instance (13) recovers the O(e-2)
iteration complexity of the full gradient method as a special case. To see this, let H = m = ∞ and
V mJ (θ) = VJ (θ) in (7), thus Asm. 3.2 and 3.3 hold automatically with A = C = D = D0 = 0
andB =1. By (13) we require T = O(e-2) iterations to reach an e-stationary point. Thus, for
any policy and MDP that satisfy the smoothness property (Asm. 3.1), the exact full PG converges
to a FOSP in O(e-2) iterations. This is the state-of-the-art convergence rate for the exact gradient
descent on non-convex objectives without any other assumptions (Beck, 2017). As we can rarely
access the exact full gradient in practice, in general A, C, D, D0 are not all 0.
From Cor. 3.5, notice that there is no restriction on the batch size m. By choosing m = O(1),
Eq. (13) shows that with TH = O(e-4) samples (i.e., single-step interaction with the environ-
ment and single sampled trajectory per iteration), the vanilla PG either with updates (4) or (6) is
guaranteed to converge to a stationary point. Our sample complexity to achieve an e-FOSP for the
stochastic vanilla PG is the same as (Papini, 2020; Zhang et al., 2020c; Xiong et al., 2021) but
improve upon them by recovering the exact full PG analysis, providing wider range of parameter
choices and using the weaker ABC assumption (see Sec. 4.1 for more details). In short, for both the
exact and stochastic PG, we recover the state-of-the-art dependency on e under the ABC assumption.
4 Applications
In this section we show how the ABC assumption can be used to unify many of the current assump-
tions used in the literature. In Figure 1 we collect all these special cases in a hierarchy tree. Then
for each special case we give the sample complexity of PG as a corollary of Thm 3.4. Each of our
corollaries matches the best known results in these special cases, while also providing a wider range
of parameter choices and, in some cases, improving the dependency on some terms in the bound
(e.g., the discount factor γ).
4.1 Expected Lipschitz and smooth policies
We consider the recently introduced expected Lipschitz and smooth policy (E-LS) assumptions
proposed by Papini et al. (2019)3.
3While Papini et al. (2019) refers to this assumption as smoothing policy, we prefer the alternative name ex-
pected Lipschitz and smooth policy, as they not only induce the smoothness of J (see Lemma 4.4), but also the
Lipschitzness (see Lemma D.1). In Papini et al. (2019), they also assume that Ea〜∏^([$)[kVθ log∏θ(a | s)k]
is bounded, while it is a direct consequence of (14) by Cauchy-Schwarz inequality.
5
Under review as a conference paper at ICLR 2022
Softmax With log barrier (26) ∣-----a ABC
Softmax with entropy (84)
Gaussian (46) (unbounded action space) --> E-LSl
Gaussian (46) (bounded action space) ----> IiLSIl <-----------
Softmax (23)
Figure 1: A hierarchy between the assumptions we present throughout the paper. An arrow indicates
an implication.
Assumption 4.1 (E-LS). There exists constants G,F > 0 such that for every state S ∈ S, the
expected gradient and Hessian of log ∏θ(∙ | S) satisfy
Ea-∏θ(∙∣s) [kVθ log∏θ(a | S)『]≤ G2,	(14)
Ea〜∏θ(∙∣s) [∣∣V2log∏θ(a | s)∣∣] ≤ F.	(15)
We call the above Expected Lipschitz and Smooth (E-LS), due to the expectation of a 〜 ∏θ(∙ | s),
in contrast to the following more restrictive Lipschitz and smooth policy (LS) assumption without
expectation
∣∣Vθ log ∏θ (a | s)k ≤ G and	∣∣V2 log π (a | s)∣∣ ≤ F,	(LS)
for all (S, a) ∈S×A. This more restrictive (LS) assumption is widely adopted in the analysis of
vanilla PG (Zhang et al., 2020c) and variance-reduced PG methods, e.g. (Shen et al., 2019; Xu et al.,
2020a;b; Yuan et al., 2020; Huang et al., 2020; Pham et al., 2020; Liu et al., 2020; Zhang et al., 2021).
It is also a relaxation of the element-wise boundness of 品 log ∏θ (a | s)∣ and ∣ ^l^- log ∏θ (a | S)I
assumed by Pirotta et al. (2015) and Papini et al. (2018)
4.1.1 Expected Lipschitz and smooth policy is a special case of ABC
In the following lemma we show that (E-LS) implies the ABC assumption.
Lemma 4.2. Under Asm. 4.1, consider a truncated gradient (
ASm. 3.3 holds with A = 0,b = 1 - m1 andC = m,thatis,
E ||V mJ (θ)
『≤(1- L) ∣VJh(θ)k2 + -,
m	m
or
(16)
HG2R2
where m is the mini-batch size, and V =	(-Tmax when using REINFORCE gradient estima-
2 ∙R 2
tor (4) or V =「m3x when using GPOMDP gradient estimator (6).
Bounded variance of the gradient estimator. Interestingly, from (16) we immediately obtain
Var [VmJ(θ)i = E ∣∣VmJ(θ)∣21 -∣VJh(θ)∣2 (Il V -1:”≤ A，	⑺
which was used as an assumption by Papini et al. (2018); Xu et al. (2020a;b); Yuan et al. (2020);
Huang et al. (2020); Liu et al. (2020). Yet (17) need not be an additional assumption since it is a
direct consequence of Asm. 4.1.
The (LS) and (E-LS) form the backbone of our hierarchy of assumptions in Figure 1. In particular,
(LS) implies (E-LS), and thus ABC is the weaker (and most general) assumption of the three. We
formalize this statement in Cor. 4.3.
6
Under review as a conference paper at ICLR 2022
Corollary 4.3. The (ABC) assumption is the weakest condition compared to (LS) and (E-LS).
4.1.2 Sample complexity analysis for stationary point convergence
Of independent interest to the ABC assumption, Asm. 4.1 also implies the smoothness of J(∙) and
the truncated gradient assumptions as reported in the following lemmas.
Lemma 4.4. Under Asm. 4.1, J(∙) is L-smooth, namely ∣∣ V2 J(θ)∣∣ ≤ L for all θ which is a
sufficient condition of Asm. 3.1, with
R	C
.(G2+F).
L
(18)
The smoothness constant (18) is tighter by a factor of1-γ as compared to the smoothness constant
proposed in (Papini et al., 2019). This is the tightest upper bound of V2J(∙) We are aware of in the
existing literature (see App. A).
D
D0GRmax
(1 - γ)3/2
and	D0 = GRmx	—+ H.
1 — Y 1 1 — Y
(19)
As a by-product, in Lemma D.1 in the appendix, We also show that J(∙) is Lipschitz under Asm. 4.1
with a tighter Lipschitzness constant, as compared to (Papini et al., 2019; Xu et al., 2020b; Yuan
et al., 2020). See more details in App. D.5.
Now we can establish the sample complexity of vanilla PG for the expected Lipschitz and smooth
policy assumptions as a corollary of Thm. 3.4 and Lemmas 4.2, 4.4, and 4.5.
Corollary 4.6. Suppose that Asm. 4.
in (7) with a mini-batch sampling of size m and constant step size
∩AT _ .	_ ,..	.	.	.	.	.
=J * — J (θo). The PG method applied
η ∈ (0, L(1 — 1/m)),
satisfies
E hkVJ(θu)k2i ≤ —7——2δ0——rxv +	——Lνη——rxv
ηT (2 - Lη (1 — ml))	m (2 - Lη (1 - ml))
+ 22D (3 - Lη(1 - mm)) + D"H! 7h
十 1	2 - Ln (1 - ml)	+ Y Y ,
where ν, L and D, D0 > 0 are provided in Lemmas 4.2,4.4 and 4.5, respectively.
(20)
(21)
We first note that Cor. 4.6 imposes no restriction on the batch size, allowing us to analyse both exact
full PG and its stochastic variants REINFORCE and GPOMDP. For exact PG, i.e., H = m = ∞,
we recover the O(1∕T) convergence. This translates to an iteration complexity T = O G) with
a constant step size η
1 to guarantee E [∣∣VJ(θu)『]
O(e2). On the other extreme, when
m =1, by (20) we have that η ∈ (0, ∞), i.e., we place no restriction on the step size. In this case,
we have that (21) reduces to
E [kVJ(θu)k2i ≤ 洋 + LVn + (3D + D02YH) YH
Thus the stepsize n controls the trade-off between the rate of convergence j and leading constant
term LVn. Using Cor. 4.6, next we develop an explicit sample complexity for PG methods.
7
Under review as a conference paper at ICLR 2022
Corollary 4.7. Consider the setting of Corollary 4.6. For a given e > 0, by choosing the mini-
batch size m such that 1 ≤ m ≤ 2V, the step size η = jɪm, the number of iterations T such
that
≥m ≥ 8δ0Lν = (O (TI-H^)	forREINFORCE	(22)
e4 e	e4	IO ((1-1尸)for GPOMDP
and the horizon H = O ((1 - Y)Tlog (1/e)), then E [∣∣VJ(θU)『]=O(e2).
Remark. Given the horizon H = O (1 - γ)-1log (1/e) , we have that (22) shows that the
sample complexity of GPOMDP is a factor of log (1/e) smaller than that of REINFORCE.
Cor. 4.7 greatly extends the range of parameters for which PG is guaranteed to converge within the
existing literature. It shows that it is possible for vanilla policy gradient methods to converge with a
mini-batch size per iteration from 1 to O(e-2) and a constant step size chosen accordingly between
O(e2) and O(1), while still achieving the Tm × H = Oe e-4 optimal complexity.
In particular, both Cor.4.4 in Zhang et al. (2020c) and Prop.1 in Xiong et al. (2021) establish O (e-4)
for FOSP convergence by using the more restrictive assumption (LS). Papini (2020) obtain the same
results with the weaker assumption (E-LS), which is also our case. However, we improve upon all of
them by recovering the exact full PG analysis, allowing much wider range of choices for the batch
size m and the constant step size η to achieve the same optimal sample complexity Oe e-4 .
In terms of the freedom of the hyperparameter choices, our result is novel. Indeed, to achieve the
optimal sample complexity, Papini et al. (2018); Shen et al. (2019); Xu et al. (2020a;b); Yuan et al.
(2020); Liu et al. (2020); Zhang et al. (2021) do not allow a single trajectory sampled per iteration.
They require the batch size m to be either e-1 or e-2. Otherwise, when m =1, their analysis would
not return the optimal rate of convergence. The existing analysis that allow m =1that we are
aware of are (Zhang et al., 2020b) and (Huang et al., 2020). However, when m > O(1), the analysis
of Huang et al. (2020) does not benefit from larger batch sizes and thus fail to match the optimal
sample complexity for large batch sizes. The comparison with (Zhang et al., 2020b) will be detailed
in Sec. 4.2.1 under the specific setting of softmax tabular policy with log barrier regularization.
4.2 Softmax tabular policy
In this section, we instantiate the FOSP convergence results of Cor. 4.6 and 4.7 in the case of the
softmax tabular policy. Combined with the specific properties of the softmax, our general theory
also recovers the average regret of the global optimum convergence analysis for the softmax with
log barrier regularization (Zhang et al., 2020b) and brings new insights of the theory by leveraing
the ABC assumption analysis.
Here, the state space S and the action space A are finite. For all θ ∈ R|S||A|
and any state-action
pair (s, a) ∈S × A, consider the following softmax tabular policy
πθ(s | a)
def	exp(θs,a)
一Pa0∈A exP(θs,aO)
(23)
We show that the softmax tabular policy satisfies (E-LS) as illustrated in the following lemma.
Lemma 4.8. The softmax tabular policy satisfies Asm.
for all S ∈ S, we have
Ea 〜∏θ(∙∣s) hkvθ log πs,a(θ)k2i	≤	1 -国,
Ea 〜∏θ(∙∣s) [|V log ∏s,a(θ)∣∣]	≤ L
and F = 1, thatis,
(24)
(25)
Remark. The softmax tabular policy also satisfies (LS) but with a bigger constant (see App. E.2).
8
Under review as a conference paper at ICLR 2022
Lemma 4.8 and the results in Section 4.1 immediately imply that Asm. 3.1, 3.2 and 3.3 are verified.
Thus, as a consequence of Cor. 4.6, we have the following sample complexity for the softmax tabular
policy.4
Corollary 4.9 (Informal). Given e > 0, there exists a range of parameter choices for the batch
size m s.t. 1 ≤ m ≤ O(e-2), the step size η s.t. O(e2) ≤ η ≤ O⑴,the number of iterations
T and the horizon H such that the sample complexity of the vanilla PG (either REINFORCE or
GPOMDP) is Tm X H = O ((；”连) to achieve E [∣∣VJ(θu)『]=O(e2).
4.2.1 Global optimum convergence of softmax with log barrier
REGULARIZATION
Leveraging the work of Agarwal et al. (2021) and our Thm. 3.4, we can establish a global optimum
convergence analysis for softmax policies with log barrier regularization.
Log barrier regularization is often used to prevent the policy from becoming deterministic. Indeed,
when optimizing the softmax by PG, policies can rapidly become near deterministic and the optimal
policy is usually obtained by sending some parameters to infinity. This can result in an extremely
slow convergence of PG. Li et al. (2021) show that PG can even take exponential time to converge.
To prevent the parameters from becoming too large and to ensure enough exploration, a log barrier
regularization term is commonly used to keep the probabilities from getting too small (Williams &
Peng, 1991; Mnih et al., 2016). The regularized objective is defined as
λ
Wi
Lλ(θ)d=efJ(θ)+
logπθ(a | s)+λlog |A|.
s,a
(26)
Similar to the softmax, we show in App. E.3 that Lλ (θ) is smooth and satisfies the (ABC) assump-
tion. Thus, from Thm. 3.4, We have {θt}t≥o converges to aFOSP of Lλ(∙). We postpone the formal
statement of this result to App. E.3 for the sake of space. Besides, thanks to Thm. 5.2 in (Agarwal
et al., 2021), the FOSP of Lλ(∙) is directly linked to the global optimum of J(∙). As a by-product,
We can also establish a high probability global optimum convergence analysis (App. E.4).
In the folloWing corollary, We shoW that We can leverage the versatility of Thm. 3.4 to derive yet
another type of result: a guarantee on the average regret W.r.t. the global optimum.
Corollary 4.10. Given e > 0, consider the batch size m such that 1 ≤ m ≤、_东7,the
step size O(e3) ≤ η = (1-/j m ≤ O(1) with L, V in the setting of Cor. E.5, the horizon
H = O (log-1/' ) and the number of iterations T such that Tm × H ≥ O (「,12。
have J* - T P=1 E [J(θt)] = O(e).
),we
This result recovers the sample complexity Oe(e-6) of (Zhang et al., 2020b). However, Zhang et al.
(2020b) do not study vanilla policy gradient. Instead, they add an extra phased learning step to
enforce the exploration of the MDP and used a decreasing step size. Our result shows that such
extra phased learning step is unnecessary and the step size can be constant. We also provide a wider
range of parameter choices for the batch size and the step size with the same sample complexity.
5 Discussion
We believe the generality of Thm. 3.4 opens the possibility to identify a broader set of configurations
(i.e., MDP and policy space) for which PG is guaranteed to converge. In particular, we notice that
Asm. 4.1 despite being very common, is somehow restrictive, as general policy spaces defined by
e.g., a multi-layer neural network, may not satisfy it, unless some restriction on the parameters is
imposed. Another interesting venue of investigation is whether it is possible to identify counterparts
of the ABC assumption for variance-reduced versions of PG and for the improved analysis of (Zhang
et al., 2021) leveraging composite optimization tools.
4The exact statement is similar to Cor. 4.7. For the sake of space here we report a more compact statement.
9
Under review as a conference paper at ICLR 2022
References
Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1-76, 2021.
J. Baxter and P. L. Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelli-
gence Research, 15:319-350, Nov 2001. ISSN 1076-9757. doi: 10.1613/jair.806.
Amir Beck. First-Order Methods in Optimization. SIAM-Society for Industrial and Applied Math-
ematics, Philadelphia, PA, USA, 2017. ISBN 1611974984.
Leon Bottou, Frank E. Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018. ISSN 0036-1445. doi: 10.1137/16M1080173.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gra-
dient methods for the linear quadratic regulator. In Jennifer Dy and Andreas Krause (eds.), Pro-
ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pp. 1467-1476. PMLR, 10-15 Jul 2018.
Saeed Ghadimi and Guanghui Lan. Stochastic first- and zeroth-order methods for nonconvex
stochastic programming. SIAM journal on optimization, 23(4):2341-2368, 2013. ISSN 1052-
6234.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient meth-
ods. In Hai Daume In and Aarti Singh (eds.), Proceedings ofthe 37th International Conference on
Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4422-4433.
PMLR, 13-18 Jul 2020.
Sham M Kakade. A natural policy gradient. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.),
Advances in Neural Information Processing Systems, volume 14. MIT Press, 2002.
Ahmed Khaled and Peter Richtarik. Better theory for Sgd in the nonconvex world, 2020.
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. In S. Solla, T. Leen, and K. Muller (eds.),
Advances in Neural Information Processing Systems, volume 12. MIT Press, 2000.
Yunwen Lei, Ting Hu, Guiying Li, and Ke Tang. Stochastic gradient descent for nonconvex learning
without bounded gradient assumptions. IEEE Transactions on Neural Networks and Learning
Systems, 31(10):4394-4400, 2020. doi: 10.1109/TNNLS.2019.2952219.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can
take exponential time to converge. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of
Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning
Research, pp. 3107-3110. PMLR, 15-19 Aug 2021.
Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced)
policy gradient and natural policy gradient methods. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 7624-7636. Curran Associates, Inc., 2020.
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In Hal DaUme III and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 6820-6829. PMLR, 13-18 Jul 2020.
A. Yu. Mitrophanov. Sensitivity and convergence of uniformly ergodic markov chains. Journal of
Applied Probability, 42(4):1003-1014, 2005. ISSN 00219002.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning
Research, pp. 1928-1937, New York, New York, USA, 20-22 Jun 2016. PMLR.
10
Under review as a conference paper at ICLR 2022
Matteo Papini. Safe policy optimization. 2020.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. In Proceedings of the 35th International Confer-
ence on Machine Learning, volume 8θ,pp. 4026-4035. PMLR, 2018.
Matteo Papini, Matteo Pirotta, and Marcello Restelli. Smoothing policies and safe policy gradients,
2019.
Nhan Pham, Lam Nguyen, Dzung Phan, Phuong Ha Nguyen, Marten van Dijk, and Quoc Tran-Dinh.
A hybrid stochastic policy gradient algorithm for reinforcement learning. In Silvia Chiappa and
Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial
Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 374-
385. PMLR, 26-28 Aug 2020.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision
processes. Machine Learning, 100(2):255-283, Sep 2015. ISSN 1573-0565. doi: 10.1007/
s10994-015-5484-1.
Karthik Abinav Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The
impact of neural network overparameterization on gradient confusion and stochastic gradient de-
scent. In Hal DaUme In and Aarti Singh (eds.), Proceedings ofthe 37th International Conference
on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8469-
8479. PMLR, 13-18 Jul 2020.
Mark Schmidt and Nicolas Le Roux. Fast convergence of stochastic gradient descent under a strong
growth condition, 2013.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 07-09 Jul 2015. PMLR.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017.
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
gradient. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 5729-5738. PMLR, 09-15 Jun 2019.
Sebastian U. Stich. Unified optimal analysis of the (stochastic) gradient method, 2019.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and
K. Muller (eds.), Advances in Neural Information Processing Systems 12, pp. 1057-1063. MIT
Press, 2000.
Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-
parameterized models and an accelerated perceptron. In Kamalika Chaudhuri and Masashi
Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intel-
ligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 1195-1204.
PMLR, 16-18 Apr 2019.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229-256, 1992.
Ronald J. Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241-268, 1991. doi: 10.1080/09540099108946587.
Lin Xiao and Lihong Li. A tutorial on policy gradient methods. In SIAM Conference on Optimiza-
tion, 2021.
11
Under review as a conference paper at ICLR 2022
Huaqing Xiong, Tengyu Xu, Yingbin Liang, and Wei Zhang. Non-asymptotic convergence of adam-
type reinforcement learning algorithms under markovian sampling. Proceedings of the AAAI
Conference on Artificial Intelligence, 35(12):10460-10468, May 2021.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. In Ryan P. Adams and Vibhav Gogate (eds.), Proceedings of The 35th
Uncertainty in Artificial Intelligence Conference, volume 115 of Proceedings of Machine Learn-
ing Research, pp. 541-551. PMLR, 22-25 Jul 2020a.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
variance reduction. In International Conference on Learning Representations, 2020b.
Huizhuo Yuan, Xiangru Lian, Ji Liu, and Yuren Zhou. Stochastic recursive momentum for policy
gradient methods, 2020.
Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational
policy gradient method for reinforcement learning with general utilities. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing
Systems, volume 33, pp. 4572-4583. Curran Associates, Inc., 2020a.
Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. On the convergence
and sample efficiency of variance-reduced policy gradient method, 2021.
Junzi Zhang, Jongho Kim, Brendan O’Donoghue, and Stephen Boyd. Sample efficient reinforce-
ment learning with reinforce, 2020b.
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Bayar. Global convergence of policy gradient
methods to (almost) locally optimal policies. SIAM Journal on Control and Optimization, 58(6):
3586-3612, 2020c. doi: 10.1137/19M1288012.
12