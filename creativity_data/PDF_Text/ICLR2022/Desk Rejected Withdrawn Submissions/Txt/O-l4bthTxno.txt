Under review as a conference paper at ICLR 2022
Not All Regions are Worthy to be Dis-
tilled: Region-aware Knowledge Distillation
Towards Efficient Image-to-Image Transla-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Recent progress in image-to-image translation has witnessed the success of gener-
ative adversarial networks (GANs). However, GANs usually contain a huge num-
ber of parameters, which lead to intolerant memory and computation consumption
and limit their deployment on edge devices. To address this issue, knowledge dis-
tillation is proposed to transfer the knowledge learned by a cumbersome teacher
model to an efficient student model. However, previous knowledge distillation
methods directly train the student to learn teacher knowledge in all the spatial re-
gions of the images but ignore the fact that in image-to-image translation a large
number of regions (e.g. background regions) should not be translated and teacher
features in these regions are not worthy to be distilled. To tackle this challenge, in
this paper, we propose Region-aware Knowledge Distillation which first localizes
the crucial regions in the images with attention mechanism. Then, teacher features
in these crucial regions are distilled to students with a region-wise contrastive
learning framework. Besides distilling teacher knowledge in features, we fur-
ther introduce perceptual distillation to distill teacher knowledge in the generated
images. Experiments with four comparison methods demonstrate the substantial
effectiveness of our method on both paired and unpaired image-to-image trans-
lation. For instance, our 7.08× compressed and 6.80× accelerated CycleGAN
student outperforms its teacher by 1.36 and 1.16 FID scores on Horse→Zebra
and Zebra→Horse, respectively. Codes have been released in the supplementary
material and will be released on GitHub soon.
1	Introduction
Excellent breakthroughs have been attained with state-of-the-art generative adversarial networks
(GANs) in generating high-resolution, high-fidelity, and photo-realistic images and videos (Shaham
et al., 2019; Brock et al., 2018; Goodfellow et al., 2014; Isola et al., 2017; Zhu et al., 2017). Due to its
powerful ability of representation and generation, GANs have evolved to the most dominant model in
image-to-image translation. However, the advanced performance of GANs is always accompanied
by tremendous parameters and computation, which have limited their usage in resource-limited
edge devices such as mobile phones. To address this issue, knowledge distillation is proposed to
improve the performance of an efficient student model by mimicking the features and prediction of
a cumbersome teacher model. Following previous research on image classification (Romero et al.,
2015; Tung & Mori, 2019), some recent works try to directly apply knowledge distillation to image-
to-image translation but their improvements are not significant (Li et al., 2020a;c).
In this paper, we argue that the reason leading to failure in previous image-to-image translation
knowledge distillation methods is the spatial redundancy of teacher features. More specifically, in
image-to-image translation, usually only a few regions of the images are required to be translated.
For example, in the well-known Horse→Zebra task, only the regions of horses need to be translated
while the regions of background should be preserved. Even in some tasks where all the regions
in images are required to be translated, there are usually some more crucial regions. However,
previous knowledge distillation methods directly employ the student to mimic teacher features in all
1
Under review as a conference paper at ICLR 2022
the regions while ignoring the fact that not all regions are worthy to be distilled. Since the student
has much fewer parameters than their teachers, they are not able to learn all teachers knowledge.
As a result, the student should pay more attention to knowledge distillation in the crucial regions
instead of learning all the regions with the same priority. Unfortunately, different from the other
vision tasks such as object detection, there is no annotations on crucial regions in image-to-image
translation, especially unpaired image-to-image translation. Thus, it is still challenging to localize
and make good use of these crucial regions.
To tackle this challenge, in this paper
we propose a novel knowledge distillation
method referred to as Region-aware Knowl-
edge Distillation. Different from previous
knowledge distillation methods, the teacher
model in our method not only transfers its
knowledge in features to students but also
tells the student which region should be
learned. Concretely, we first propose to lo-
Input
域
Attention Select Regions with Crucial Regions
Large Attention
Figure 1: The paradigm of localizing crucial re-
gions in region-aware knowledge distillation on
Horse→Zebra with CycleGANs.
calize the crucial regions in images with a parameter-free attention mechanism, where the attention
value of a region is decided by its mean absolute value across the channel dimension. As pointed out
in previous works (Zhou et al., 2016; Zhang & Kaisheng, 2021; Zagoruyko & Komodakis, 2017),
this attention mechanism can reveal the importance of each region with no requirements on addi-
tional supervision. Then, we select several the regions with the large attention values as the crucial
regions in an image. As shown in Figure 1, in Horse→Zebra, this method can localize the regions
of horses while filtering the regions of background. Since no additional labels and parameters are
required, it can be easily utilized in all kinds of datasets and models.
After localizing the crucial regions in the image, we then apply a region-wise contrastive learning
framework to distill teacher knowledge in crucial regions. Motivated by previous works on con-
trastive learning and knowledge distillation (Park et al., 2020; Tian et al., 2019), instead of directly
minimizing the distance between students and teachers in the feature space, we propose to maxi-
mize the mutual information between features of students and teachers in the same crucial region,
while pushing away the features of students and teachers in different crucial regions. Concretely,
during the training period, the features of students and teachers in the same crucial region are con-
sidered as a positive pair and their features in different crucial regions are regarded as negative pairs.
Then, by optimizing these pairs with InfoNCE loss (Oord et al., 2018), the distance between positive
pairs is minimized while the distance between negative pairs is maximized, which transfers teacher
knowledge in the crucial regions to the student.
Besides distilling teacher knowledge in its features, motivated by the well-known perceptual loss
utilized in image super-resolution (Johnson et al., 2016), we introduce the perceptual distillation to
distill teacher knowledge in the generated images. Instead of directly training the students to mimic
the generated images from teachers pixel by pixel, we apply an ImageNet pre-trained model to
extract the semantic features of the generated images from students and teachers and then minimize
their L2-norm distance. Compared with the previous pixel-level distillation methods, perceptual
distillation is based on differences between high-level image feature representations extracted from
the pre-trained models and thus it is more robust and efficient.
In summary, we mainly make the following contributions in this paper.
•	We propose Region-aware Knowledge Distillation which first localizes the crucial regions
in an image depending on the attention values and then distills teacher knowledge in these
crucial regions with a region-wise contrastive learning framework.
•	We propose perceptual distillation to transfer teacher knowledge in the generated images.
Instead of learning the generated images pixel by pixel, the student is trained to generate
images which has similar semantic features to images generated from teachers.
•	Experiment results with four comparison methods have demonstrated the effectiveness of
our method on both paired and unpaired image-to-image translation in terms of both quan-
titative and qualitative results. For instance, our 7.08× compressed and 6.80× accelerated
CycleGAN student outperforms its teacher by 1.36 and 1.16 FID scores on Horse→Zebra
and Zebra→Horse, respectively.
2
Under review as a conference paper at ICLR 2022
2	Related Work
2.1	GANs for Image-to-Image Translation
Generative Adversarial Network (GAN), which is composed of a generator for image generation
and a discriminator for discriminating the real and generated images, have become the most pop-
ular model in image-to-image translation (Goodfellow et al., 2014). Pix2Pix is proposed to apply
conditional GAN (Mirza & Osindero, 2014) to the task of image-to-image translation on paired
datasets (Isola et al., 2017). Then, Pix2PixHD improves the resolution of generated images with
multi-scale neural networks and boundary maps (Wang et al., 2018b). Based on these efforts, Wang
et al. further propose Vid2Vid to perform video-to-video translation (Wang et al., 2018a).
A more challenging task in this domain is how to perform image-to-image translation on unpaired
datasets. CycleGAN, DualGAN, and DiscoGAN are proposed to address this issue by regularizing
the training of generators with the cycle consistency loss (Zhu et al., 2017; Yi et al., 2017; Kim
et al., 2017). Recently, Park et al. propose to replace the cycle consistency loss with a patch-
wise contrastive loss, which minimizes the mutual information between the corresponding input and
output patches (Park et al., 2020). Besides image style transfer, GANs have also been utilized in the
other tasks, such as single image super resolution (Ledig et al., 2017; Wang et al., 2018c), image
deblurring (Kupyn et al., 2018) and so on.
The tremendous storage and computation consumption in GAN have promoted the research on its
compression. Wang et al. propose a unified GAN compression framework with knowledge distil-
lation, channel pruning, and quantization (Wang et al., 2020). Li et al. propose to compress GANs
with once-for-all net architecture search and naive feature knowledge distillation (Li et al., 2020a).
Shu et al. propose to investigate and prune the unimportant weights in GANs with a co-evolutionary
approach (Shu et al., 2019). Recently, Jin et al. introduce an inception residual block into generators
and prune it with a one-step pruning algorithm (Jin et al., 2021).
2.2	Knowledge Distillation
Knowledge distillation has become one of the most effective techniques for model compression (Bu-
CilUa et al., 2006; Hinton et al., 2014). It first trains a cumbersome teacher model and then transfers
its knowledge to a lightweight student model. Previous knowledge distillation usually aims to dis-
till the knowledge in the logits (softmax outputs) (Hinton et al., 2014; Zhang et al., 2018). Then,
abundant methods have been proposed to distill the knowledge in the features and its variants, such
as attention (Zagoruyko & Komodakis, 2017; Zhang & Kaisheng, 2021) and the gram matrix (Yim
et al., 2017). Recently, some research has been proposed to distill the relation between different sam-
ples (Park et al., 2019; Tung & Mori, 2019) and pixels (Liu et al., 2020; Li et al., 2020b). Another
popular trend in knowledge distillation is to maximize the mutual information between students and
teachers with contrastive learning. Tian et al. first propose the contrastive representation distilla-
tion framework which regards the representation of the same image from students and teachers as
a positive pair in contrastive learning. Then Chen et al. extend this idea with the Wasserstein dis-
tance (Chen et al., 2020). In this paper, we extend this framework into the patch-wise contrastive
learning (Park et al., 2020) for knowledge distillation on image-to-image translation with GANs.
In the last several years, there has been some research proposed to apply knowledge distillation to
the compression of GANs. Li et al. propose to improve the performance of student generators with
the naive feature distillation (Li et al., 2020a). Then, Li et al. propose the semantic relation pre-
serving knowledge distillation, which computes and distills the relation between different patches in
generators (Li et al., 2020c). Jin et al. propose to distill the knowledge in features with global ker-
nel alignment which enables knowledge distillation without additional adaptation layers (Jin et al.,
2021). Recently, Liu et al. propose content-aware GAN compression to compress unconditional
GANs, in which the main content such as human faces in an image is first parsed with an additional
parser network and then distilled to students. However, the training of the parser network needs
additional annotation, which is rare in real-world application.
3
Under review as a conference paper at ICLR 2022
Teacher
Encoder
C×W×H
Attention
abs()
mean(dim=1)
HWXH
Crucial Retions
Select K Regions with
the Largest Attention as
(a)	Find the Crucial Regions in Images with Attention
Select
Teacher
Encoder
oC×W×H Features in
''	Crucial
,z√z Regions
Query	Key
-5c×w×h
IS c×k
Softma(x
(Student Feature) (Teacher Feature)
M [〃/])
3>
.
Γ 1 0 0 ^
Student
Encoder
Select
Features in
Crucial
飞c×k
Ground Truth
Contrastive Learning Optimization
0 1 0
0 0 1
(b)	Region-wise Contrastive Distillation on the Crucial Regions
Figure 2: The overview of Region-aware Knowledge Distillation (best viewed in color). (a) Step-1:
Find the crucial regions in the to be translated image by applying the attention module to teacher
features. Note that the attention module is composed of an absolute value operation and a mean
operation on the channel dimension. Then, K regions with the largest attention values are selected
as the crucial regions (here K=3). (b) Step-2: Based on the crucial regions found in Step-1, select
student and teacher features on these crucial regions and discard the features in unimportant regions.
Student features and teacher features in the same region are considered as a positive pair (such as
and ) and the others are regarded as negative pairs (such as and ). All these pairs are
optimized in a contrastive learning framework with InfoNCE loss.
3	Methodology
3.1	Region-wise Contrastive Learning for Knowledge Distillation
Given two set of images X and Y , image-to-image translation aims to find a mapping function
F : RC×H×W → RC×H×W which maps images in X to Y. Note that C, H, W indicates the
number of channels, height and width of the image, respectively. Usually, F can be divided into an
encoder Fenc followed by a decoder Fdec. Given an image x, then its intermediate feature can be
formulated as Fenc(x) ∈ Rc×w×h where c, w and h denotes its number of channels, width and height
respectively. For convenience, we reshape it into Rc×wh, where Fenc(x)[:, i] indicates the feature of
i-th region. The corresponding index set of regions can be formulated as S = {1, 2, 3, ..., wh}.
In this paper, we adopt a noise contrastive estimation framework (Oord et al., 2018) to maximize the
mutual information between the features between students and teachers. Given a query v, a positive
key v+ and a set of negative keys {v1-, v2-, ..., vN- }. The InfoNCE loss can be formulated as
LInfoNCE(v, v+, v-) = - log
exp(v ∙v+/τ)
eχp(V ∙ v+/t )+Pn=I eχp(V ∙ V-IT)
(1)
where τ is a temperature hyper-parameter. Regarding the features of students and teachers at the
same region as positive pairs and the other features as the negative pair, we can extend InforNCE to
4
Under review as a conference paper at ICLR 2022
a region-wise contrastive distillation framework, which can be formulated as
wh
LRegionDis(X,	FSL, FTc)	= Ex〜X E LInfONCE(Fenc(X) [:, j],	FTc(X)	[:,j],	{FT/j]	|	j'	∈ Sj =	i}),
{Z	'Z	{Z
i=1
^z^^
Query
^^^^{^^^^™
Positive Key
where the scripts S and T are utilized to distinguish students and teachers.
^^^^^{^^^^^^^^^^^—
Negative Keys
(2)
3.2	Region-aware Knowledge Distillation
It is generally acknowledged that the attention value of each pixel shows its importance (Zhang &
Kaisheng, 2021). In this paper, we define the attention value of a region as its absolute mean value
across the channel dimension, which can be formulated as
A:Rc
×wh
-a-bs→() Rc×wh
mean(dim=1)
wh
(3)
R
Then, given a teacher feature, FeTnc(X), its attention map can be denoted as A(FeTnc)(X). Then, we
select K regions with the largest attention values as the crucial regions in this image. Denote the
index set of regions as PK, then the feature of the crucial regions can be formulated as G(X) =
stack({Fenc [:,i]},i ∈ Pk) ∈ Rc×K. Denote its index set as S0 = {1, 2, ..., K}, then our region-
aware knowledge distillation can be formulated as
K
LRegionAware(X, GS, GT) = Ex〜X X LInfONCE(GS(X)[：, i], GT(x)[：, i], {GT[： j] | j ∈ S,j = i}),
i=1
(4)
It is Observed that the main difference between EquatiOn 2 and EquatiOn 4 is that EquatiOn 4 applies
knOwledge distillatiOn tO Only the K crucial regiOns fOund by A instead Of all the regiOns.
3.3 Perceptual Distillation
Perceptual distillatiOn is prOpOsed tO dis-
till teacher knOwledge in the generated im-
ages. Usually, the native knOwledge distilla-
tiOn methOds directly minimize the L1 nOrm
distance between each pixel, which can be fOr-
mulated as
LNaive Distill =Ex〜X IFS (X)-FT(x)∣1 (5)
In cOntrast, mOtivated by previOus research in
image super-resOlutiOn, in this paper we prO-
pOse perceptual distillatiOn, which minimizes
the difference between students and teachers
On the semantic features extracted by a Im-
ageNet pre-trained mOdel. DenOte the pre-
trained model as J(∙), then its loss function can be formulated as
LPercep. Distill =Ex 〜x I J。FS (x)-J。FT (χ)∣2.	(6)
Based on Equation 4 and 6, now we can formuate the overall loss function as
-------->
Student
Generator
Teacher
----->，'& Image
TeaCher ∣≡⅛∙⅞*w^^
Generator举党步⅛i羲
Pretrained
Model
Student
Image
Teacher
Image’s Feature
Minimize
Student
Image’s Feature
Figure 3: The overview of perceptual distillation.
A ImageNet pre-trained model is utilized to ex-
tract the features of images generated by students
and teachers. Then, the distance between these
extracted features are minimized for distillation.
Loverall = α ∙ LRegionAware + β ∙ LPercep. Distill + LOrigin ,
(7)
where LOrigin indicates the origin training loss of GANs. α and β are two hyper-parameters in-
troduced to balance different loss functions. Sensitivity studies on them have been conducted and
shown in Appendix D. For the image-to-image translation models which have two generators such
as CycleGAN, the distillation loss are applied to the two directions, respectively.
4	Experiments
4.1	Experiment Setting
Models, Datasets and Comparison Methods We evaluate our method on three image-to-image
translation models including CycleGAN, Pix2Pix and Pix2PixHD, and two datasets including
5
Under review as a conference paper at ICLR 2022
Horse^⇒Zebra and Edges2Shoes. Horse^⇒Zebra is an unpaired image-to-image translation
dataset which translates images of horses to zebras and vice versa. Edges2Shoes is a paired image-
to-image dataset which maps the edges of shoes to their natural images. Besides, experiments on
Cityscapes have also been conducted and shown in Appendix A. The students in our experiments
have the same neural network depth but fewer channels compared with their teachers. Four GAN
knowledge distillation methods are utilized for comparison. Note that some of these methods in-
cludes both neural network pruning and knowledge distillation and we only compare our method
with the knowledge distillation parts in these comparison methods for fairness. To obtain more
reliable results, we run 8 trials for each experiment and report their average and standard deviation.
Evaluation Settings Frechet Inception Distance (FID), which measures the distance between the
distribution of features extracted from the real and the synthetic images, is utilized as the metric for
all the experiments. A lower FID indicates the synthetic images have better quality. Please refer to
the codes in the supplementary material for more details.
EKPZ tl 0∞8M
OSJOH PJqOZ
Input
Student Student Teacher
w/o KD with KD
Input
Student
w/o KD
Student
with KD
Teacher
Figure 4: Qualitative results on Horse→Zebra and Zebra →Horse. A 15.81× compressed Cycle-
GAN is utilized as the student. Results on Edges2Shoes are shown in Appendix B.
4.2	Experiment Results
Quantitative Result Quantitative results of our methods compared with four knowledge distilla-
tion methods have been shown in Table 1. It is observed that: (a) Our method leads to consistent and
6
Under review as a conference paper at ICLR 2022
significant performance improvements (FID reduction) on various datasets and models. On average,
it leads to 12.65 and 5.08 FID reduction on unpaired and paired image-to-image translation tasks,
respectively. (b) Our method outperforms the other four kinds of image-to-image translation knowl-
edge distillation methods by a large margin. On average, it outperforms the second-best method
by 7.77 FID. (c) Not all the knowledge distillation methods work well on GAN for image-to-image
translation. Directly applying the naive Hinton knowledge distillation (Hinton et al., 2014) leads to
very limited and even sometimes negative effects. For example, it leads to 1.91 FID increment on the
Pix2Pix student for the Edges2Shoes task. (d) Compared with paired image-to-image translation,
there are more performance improvements on unpaired image-to-image translation with our method.
Table 1: Quantitative comparison between different knowledge distillation methods. Numbers in
the brackets indicate the ratio of compression and acceleration. A lower FID indicates better per-
formance. ∆ indicates the relative increment compared with the student trained without knowledge
distillation (higher is better). Each experiment is averaged from 8 trials.
Models	Dataset	#Params (M)	FLOPs (G)	Method	Metric	
					FIDZ	∆↑
		11.38	49.64	Teacher	61.34±4.35	一
		0.72 (15.81×)	3.35 (14.82×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	85.04±6.88 84.08±3.78 83.97±5.01 81.74±4.65 82.37±8.56	一 0.96 1.07 3.30 2.67
CycleGAN	Horse→Zebrea			Ours	71.04±6.21	14.00
		1.61 (7.08×)	7.29 (6.80×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	70.54±9.63 70.35±3.27 68.58±4.31 68.94±2.98 67.31±3.01	一 0.18 1.96 1.60 3.23
				Ours	59.98±5.48	10.56
		11.38	49.64	Teacher	138.07±4.01	一
		0.72 (15.81×)	3.35 (14.82×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	152.67±5.07 148.64±1.62 151.32±2.31 151.09±3.67 149.73±3.94	一 4.03 1.35 1.58 2.94
CycleGAN	Zebra→Horse			Ours	142.39±4.40	10.28
		1.61 (7.08×)	7.29 (6.80×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	141.86±1.57 142.03±1.61 141.32±1.27 141.16±1.31 140.98±1.41	一 -0.17 0.54 0.70 0.88
				Ours	136.91±2.90	15.76
		54.41	6.06	Teacher	59.70±0.91	一
Pix2Pix	Edges2Shoes	13.61 (4.00×)	1.56 (3.88×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	85.06±0.98 86.97±3.49 83.63±3.12 84.01±2.31 84.39±3.62	一 -1.91 1.43 1.05 0.67
				OUrS	77.51±3.28	7.55
		45.59	48.36	Teacher	41.59±0.42	一
Pix2PixHD	Edges2Shoes	1.61 (28.23×)	1.89 (25.59×)	Origin Student Hinton et al. 2014 Li and Lin et al. 2020a Li and Jiang et al. 2020c Jin et al. 2021	44.64±0.54 45.31±0.63 44.03±0.41 43.90±0.36 43.97±0.17	一 -0.67 0.61 1.28 1.21
				OUrS	42.03±0.20	2.61
7
Under review as a conference paper at ICLR 2022
Figure 5: The visualization of the learned similarity between student features in one region and
teacher features in all regions. For each line, we plot the similarity when six different regions
of students are selected. Three of these regions come from horses and the others come from the
background. A Lighter region indicates a higher similarity.
This may be caused by the fact that there is less labeled supervision in unpaired image-to-image
translation and thus the knowledge from teachers is more helpful. (e) A high ratio of acceleration
and compression can be achieved by replacing the teacher model with the distilled student model.
For example, our method leads to 7.08× compression and 6.80× acceleration on CycleGAN stu-
dents in terms of the number of parameters and FLOPs. The compressed students outperform their
teachers by 1.36 and 1.16 FID on Horse→Zebra and Zebra→Horse, respectively.
Qualitative results Qualitative results of our method on Horse→Zebra and Zebra→Horse have
been shown in Figure 4. It is observed that the student model trained without knowledge distillation
always can not translate the whole body of horses and zebras. In contrast, the student model trained
with our methods does not have this issue. Moreover, on Horse→Zebra, the student model trained
by our method sometimes outperforms its teacher on the effect of removing the stripes in zebras.
5 Discussion
5.1 Ablation Study
There are mainly three modules in the
proposed region-aware knowledge dis-
tillation, including (a) localizing the
crucial regions in images with attention
mechanism (b) performing knowledge
distillation with region-wise contrastive
learning, and (c) distilling knowledge
in the generated images with perceptual
distillation. A series of ablation stud-
ies have been conducted to demonstrate
their effectiveness. As shown in Table 2:
Table 2: Ablation studies of the three main modules in
our method are Horse→Zebra with CycleGAN students.
Each experiment is averaged from 8 trials. Reported re-
sults are FID (lower is better).
(a)	Crucial Region (b)	Contrastive Distillation (c)	Perceptual Distillation	XX	X	X	X X	XX	x	X xxx	XX
Horse→Zebra	70.54 65.53 61.10 67.52 59.98
(i) The basic framework of applying contrastive learning to knowledge distillation is beneficial even
without the other two modules. (ii) By only distilling the features in the crucial regions, 5.01 FID
reduction can be achieved. (iii) Individual usage of perceptual distillation leads to 3.02 FID reduc-
tion and applying it to the other two modules reduces FID from 61.10 to 59.98. These observations
demonstrate that each module in our method is indispensable.
8
Under review as a conference paper at ICLR 2022
Ablations on Distilling the Crucial Regions To further show the effectiveness of only distilling
the crucial regions, we have compared the following three schemes: (a) distilling regions with the
largest attention (our scheme) (b) distilling the regions with the least attention and (opposite to
our scheme) (c) randomly choose regions for knowledge distillation. Our experiments show that
the three schemes achieve 59.98, 72.54, and 65.53 FID on Horse→Zebra with 7.08× compressed
CycleGAN students, respectively. It is observed that our scheme (a) and its opposite scheme (c)
achieves the best and the worst performance, respectively. These results show that there is a positive
relation between the attention value of a region and the benefits from distilling this region.
5.2	Visualizing the Similarity between Students and Teacher
The similarity of features from students and teacher have been visualized in Figure 5. For each line,
we select student features of six different regions in an image. Note that three of the student regions
are selected from the body of the horses and the other three regions comes from the background.
Then, we compute the similarity between teacher features in all the regions and the student feature
in the selected region. It is observed that when computing the similarity with respect to student
features in regions of horses, teacher regions in the horse body have a much higher value than the
background regions. When computing the similarity with respect to student features of background
regions, teacher regions of the horses become are and teacher regions in the background are light.
This result shows that there is a high similarity between student features and teacher features on the
same location, which demonstrates the effectiveness of knowledge distillation.
5.3	Knowledge Distillation Can Stabilize GAN Training
The training of GAN is usually not sta-
ble due to their complex network archi-
tectures and loss functions. In this paper,
we find that the proposed knowledge dis-
tillation can alleviate this problem. Fig-
ure 6 shows the FID curves of Cycle-
GAN students in different training epochs
on Horse→Zebra and Zebra→Horse. It
is observed that (a) Both the training of
students with and without knowledge dis-
tillation are stable in the early several
epochs. (b) After the early epochs, the
Figure 6: The FID curve of CycleGAN students
trained with and without knowledge distillation on
Horse→Zebra and Zebra→Horse.
training of the student without knowledge distillation becomes unstable and sometimes collapses
(marked with circles). In contrast, the distilled student is more stable during the whole training
period. Its undulations are much smaller than the student trained without knowledge distillation.
6 Conclusion
Motivated by the observation that a large number of regions in image-to-image translation are not
worthy to be distilled, this paper proposes region-aware knowledge distillation. First, attention
mechanism is utilized to localize the crucial regions in the to be translated images. Then, a region-
wise contrastive learning framework is employed for knowledge distillation, which maximizes the
mutual information between the features of students and teachers in the same region. Besides, per-
ceptual distillation is also introduced to transfer teacher knowledge in the generated images. Abun-
dant experiments with four comparison methods have been conducted to demonstrate the effective-
ness of our method. On average, 12.65 FID and 5.08 FID reduction can be observed on unpaired
and paired image-to-image translation tasks, respectively. Our 7.08× compressed and 6.80× ac-
celerated CycleGAN student outperforms its teacher by 1.36 and 1.16 FID on Horse→Zebra and
Zebra→Horse respectively. In the discussion period, detailed ablation studies results have further
shown the effectiveness of each module in our method. Besides, visualization results and FID curves
during the training period show that our knowledge distillation method enables students to learn the
similarity between different regions and stabilize the training of GANs.
9
Under review as a conference paper at ICLR 2022
References
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Cristian Bucilua, Rich Caruana, and Alexandru NicUlescU-MiziL Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining,pp. 535-541. ACM, 2006.
Liqun Chen, Zhe Gan, Dong Wang, Jingjing Liu, Ricardo Henao, and Lawrence Carin. Wasserstein
contrastive representation distillation. arXiv preprint arXiv:2012.08674, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
processing systems, 27, 2014.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In
NeurIPS, 2014.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 1125-1134, 2017.
Qing Jin, Jian Ren, Oliver J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang, and Sergey
Tulyakov. Teachers do more than teach: Compressing image-to-image models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13600-13611, 2021.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. In International Conference on Ma-
chine Learning, pp. 1857-1865. PMLR, 2017.
Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. Deblur-
gan: Blind motion deblurring using conditional adversarial networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 8183-8192, 2018.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Effi-
cient architectures for interactive conditional gans. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 5284-5294, 2020a.
Xiaojie Li, Jianlong Wu, Hongyu Fang, Yue Liao, Fei Wang, and Chen Qian. Local correlation
consistency for knowledge distillation. In European Conference on Computer Vision, pp. 18-33.
Springer, 2020b.
Zeqi Li, Ruowei Jiang, and Parham Aarabi. Semantic relation preserving knowledge distillation for
image-to-image translation. In European Conference on Computer Vision, pp. 648-663. Springer,
2020c.
Yifan Liu, Changyong Shu, Jingdong Wang, and Chunhua Shen. Structured knowledge distillation
for dense prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Yuchen Liu, Zhixin Shu, Yijun Li, Zhe Lin, Federico Perazzi, and Sun-Yuan Kung. Content-aware
gan compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 12156-12166, 2021.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
10
Under review as a conference paper at ICLR 2022
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired
image-to-image translation. In European Conference on Computer Vision, pp. 319-345. Springer,
2020.
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3967-3976,
2019.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.
Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan: Learning a generative model from
a single natural image. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pp. 4570-4580, 2019.
Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi Tian, and Chang Xu.
Co-evolutionary compression for unpaired image translation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 3235-3244, 2019.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv
preprint arXiv:1910.10699, 2019.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 1365-1374, 2019.
Haotao Wang, Shupeng Gui, Haichuan Yang, Ji Liu, and Zhangyang Wang. Gan slimming: All-in-
one gan compression by a unified optimization framework. In European Conference on Computer
Vision, pp. 54-73. Springer, 2020.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. arXiv preprint arXiv:1808.06601, 2018a.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 8798-8807, 2018b.
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen
Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings
of the European conference on computer vision (ECCV) workshops, pp. 0-0, 2018c.
Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-
to-image translation. In Proceedings of the IEEE international conference on computer vision,
pp. 2849-2857, 2017.
Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4133-4141, 2017.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-
mance of convolutional neural networks via attention transfer. In ICLR, 2017.
Linfeng Zhang and Ma Kaisheng. Improve object detection with feature-based knowledge distilla-
tion: Towards accurate and efficient detectors. In ICLR, 2021.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR,
pp. 4320-4328, 2018.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference
on computer vision, pp. 2223-2232, 2017.
11
Under review as a conference paper at ICLR 2022
Table 3: Quantitative comparison between different knowledge distillation methods on Cityscapes
with Pix2Pix. Numbers in the brackets indicate the ratio of compression and acceleration. A higher
mIoU indicates better performance. ∆ indicates the relative increment compared with the student
trained without knowledge distillation (higher is better). Each experiment is averaged from 8 trials.
Models	Dataset	#Params (M)	FLOPs (G)	Method	Metric	
					mIoU↑	∆↑
		54.41	96.97	Teacher	46.51±0.32	一
				Origin Student	41.35±0.22	一
Pix2Pix	Cityscapes			Hinton et al. 2014	40.49±0.41	-0.86
		13.61 (4.00×)	24.90 (3.88×)	Li and Lin et al. 2020a	41.52±0.34	0.17
				Li and Jiang et al. 2020c	41.77±0.30	0.42
				Jin etal. 2021	41.29±0.51	-0.06
				Ours	42.41±0.25	1.06
Input
w/o KD
with KD
Ground
Truth
A	Experiments on Cityscapes
Experiments on Cityscapes with Pix2Pix are shown in Table 3. Following previous works (Jin et al.,
2021), we adopt the mIoU of a pre-trained segmentation model on the generated images as the
performance metric on Cityscapes. A high mIoU indicates better performance. It is observed that
the Pix2Pix student trained with our method leads to 1.06 mIoU improvements compared with the
baseline, which outperforms the second-best knowledge distillation method by 0.64 mIoU.
B	Qualitative Results on Edges2Shoes
Qualitative results on Edges2Shoes are shown in Figure 7. It is observed that the distilled student
outperforms the student trained without knowledge distillation by a large margin. The distilled
student has much better details such as the shoe string and the highlight on the shoes.
C Tricks: Random Projection Heads
As pointed out by many previous works on contrastive learning, the architecture and training meth-
ods of the projection heads have a significant influence on the performance of contrastive learning. In
this paper, we fix the parameters of projection head and do not train them during the whole training
period. Surprisingly, we find this trick can stabilize student training and lead to better performance.
12
Under review as a conference paper at ICLR 2022
68 I
66-
64-
62-
60-
58-
56』
50	75	100	125 150
K
Figure 8: Sensivity studies on the three hyper-parameters with CycleGAN on Horse→Zebra.
D Sensitivity Study
There are mainly three hyper-parameter α, β and K introduced in our method. α and β are uti-
lized to balance the magnitude of different loss functions and K is the number of crucial regions
selected in an image. The sensitivity studies results on Horse→Zebra with CycleGAN students have
been shown in Figure 8. It is observed that all our method is not sensitive to the choice of hyper-
parameters. Even in the worst siatuation, our methods still outperforms the baseline (70.54 FID) and
the second-best method (67.31 FID) by a clear margin.
13