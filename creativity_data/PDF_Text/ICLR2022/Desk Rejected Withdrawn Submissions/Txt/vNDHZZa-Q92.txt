Under review as a conference paper at ICLR 2022
Neural friendly extensions: training neural
NETWORKS WITH SET FUNCTIONS
Anonymous authors
Paper under double-blind review
Ab stract
Integrating discrete computational steps into deep learning architectures is an important considera-
tion when learning to reason over discrete objects. However, many tasks that involve discrete choices
are defined via (combinatorial) set functions, and therefore pose challenges for end-to-end training.
In this work, we explore a general framework for continuously and tractably extending such dis-
crete functions that enables training via gradient-based optimization methods. Our neural friendly
extension framework includes well-known constructions as special cases (such as the Lovasz and
multilinear extensions). Moreover, it facilitates the design of novel continuous extensions based
on problem-specific considerations, including constraints. We demonstrate the versatility of our
framework on tasks ranging from combinatorial optimization to image classification.
1	Introduction
End-to-end differentiability is a cornerstone of modern deep learning. Consequently, neural networks are still faced
with significant obstacles when it comes to reliably performing elementary combinatorial tasks such as finding shortest
paths on graphs and sorting numbers. Such tasks typically involve non-differentiable operations on sets of discrete
objects, a feature that does not align well with the gradient-based optimization paradigm predominant in deep learning.
Various mechanisms for dealing with non-differentiable nodes in the computational graph of a neural network have
been proposed. Previous work has intensively studied the problem of backpropagation through stochastic operations,
as well as through discrete deterministic operations of specific forms. The former include methods for differentiable
sampling (Jang et al., 2017; Maddison et al., 2017), while the latter include differentiable versions of sorting (Blondel
et al., 2020), QP and SDP solvers (Amos & Kolter, 2017; Wang et al., 2019), and shortest path algorithms (PoganCiC
et al., 2019; Berthet et al., 2020).
Differently, this work focuses on back-propagating through black-box set functions—general discrete deterministic
functions that map sets of discrete entities onto real numbers. The class of set functions is rather general: it includes
functions that are ubiquitous in machine learning applications like the argmax, entropy, and rank, as well as functions
that are routinely used to describe structural properties like coloring numbers, covers, and cuts in a graph (Boros &
Hammer, 2002). Submodular set functions have found many further applications in machine learning (Bach, 2013;
Bilmes, 2013; Krause & Jegelka, 2013). Set functions can also be viewed as atomic blocks that may be composed to
perform complex algorithmic computations (Velickovic & Blundell, 2021), and could be used at intermediate stages
of a model or be incorporated directly in the objective function. Perhaps the most well known solution for discrete
function optimization is REINFORCE (Williams, 1992), also known as the score function estimator, which allows for
an unbiased gradient estimate. In practice, the score function estimator is hard to train as it suffers from high variance.
This has been the impetus for works that aim to reduce variance through control variates, also known in reinforcement
learning as baselines (Gu et al., 2017; Liu et al., 2018; Grathwohl et al., 2018; Wu et al., 2018; Cheng et al., 2020).
We propose neural friendly extensions (NFEs), a general framework for designing differentiable extensions of black-
box set functions. NFEs are based on an axiomatic approach meant to ensure their appropriateness for neural network
1
Under review as a conference paper at ICLR 2022
training: 1) they can be computed efficiently in closed-form, 2) their minima correspond to the minima of the discrete
function, and 3) any continuous point can be discretized without increasing the cost function.
Defining and using NFEs is simple: the main idea is to interpret the continuous output of a neural network as the
expectation of a discrete distribution supported on a small number of well chosen sets. The NFE at that point is
then equal to the expected value of the discrete function under the same distribution. The aforementioned procedure
provides a principled recipe for deriving differentiable extensions based on problem-specific considerations (such as
simple constraints) by selecting an appropriate distribution. Intriguingly, it also includes as special cases well-known
extensions of discrete functions, such as the Lovasz (Lovasz, 1983) and multilinear extensions (CalinescU et al., 2011).
We demonstrate that NFEs can be used to train models on a variety of tasks that involve discrete computational steps.
We find that NFEs provide a competitive alternative when it comes to extending and optimizing set functions arising
in settings including combinatorial optimization and even standard image classification problems. Furthermore, we
show how the choice of a suitable support for the distribution of our relaxation can yield improved performance by
better aligning the extension with the structure of the problem.
2	Related work
The subject of differentiation through discrete modules in neural architectures has witnessed remarkable development
in recent years. While our work falls under the same broad category, an important distinction from prior work is
our focus on enabling the extension and differentiability of arbitrary set functions, instead of focusing on a specific
operation like sampling or sorting.
Differentiating through black-box functions. The score function estimator is an established approach when opti-
mizing black-box functions. There are two key differences between the score estimator and our framework: 1) The
score estimator enables differentiation by calculating a (high variance) gradient estimate from samples of a learned
distribution using the log derivative trick; the only necessary condition is that the probabilities of the distribution are
differentiable with respect to the model parameters. In our framework, the extension is entirely deterministic and
avoids the introduction of stochastic nodes in the computation graph. 2) Our extension provides evaluations of the
function at continuous points, which could be then combined in a differentiable manner with other operations in the
forward pass, while the score estimator is restricted to the original domain of the function.
In the context of neural combinatorial optimization, an unsupervised framework for differentiable relaxations of com-
binatorial objective functions has been recently proposed (Karalias & Loukas, 2020). The distributional assumptions
of the proposed instantiation of that framework correspond to the multilinear extension (Calinescu et al., 2011), and
the proposed relaxations can be employed in the case where a closed form expectation can be derived and efficiently
computed. In contrast to that, our extensions can, in principle, be designed without imposing strong conditions on the
set function and therefore can be used to train directly with a discrete objective.
Differentiating through discrete non-differentiable operations. Research on this subject has focused on develop-
ing gradient estimation techniques that enable backpropagation through specific non-differentiable operations. Those
may appear at several points in a learning pipeline, either in the loss function or as intermediate steps in the forward
pass. Sorting and ranking have been at the forefront of those developments and efficient differentiable versions have
already been designed (Blondel et al., 2020; Grover et al., 2018; Xie et al., 2020). Sampling is another operation that
has attracted a lot of attention, as it introduces stochastic nodes in the computation graph which are non differentiable.
A straightforward way to circumvent the problem is the Straight-Through Estimator (Bengio et al., 2013), which treats
the sampling operation as an identity map in the backward pass to yield a biased estimate of the gradient. Sampling has
also been the motivation behind the use of the Gumbel-Softmax trick (Maddison et al., 2017; Jang et al., 2017), which
allows differentiable sampling from discrete (categorical) distributions. The Gumbel trick has since been extended to
enable sampling subsets instead of one-hot vectors (Xie & Ermon, 2019) and sequences without replacement (Kool
et al., 2019). Gumbel-Softmax can be seen as a special case of the more general framework, Stochastic Softmax Tricks
2
Under review as a conference paper at ICLR 2022
(SST) (Paulus et al., 2020), which enables unbiased gradient estimation with differentiable samples that are obtained
as solutions to a regularized convex program. A crucial difference between gradient estimation with SST and our
approach is that the former address the non-differentiability of the samples that are used as input to (loss) functions,
while our framework addresses the non-differentiability and the lack of continuity of the functions themselves. Indeed,
unbiased gradient estimation in the case of SST is achieved under the assumption that the function operating on the
samples is continuous and differentiable.
SST in general can be understood from the perspective of perturbation models (Papandreou & Yuille, 2011; Hazan
& Jaakkola, 2012), where samples are drawn from a distribution by optimizing a random objective. Building on the
concept of perturbation models, recent work (Berthet et al., 2020) has proposed a way to backpropagate through black-
box solvers. This is one of several important papers in that direction, which aims to integrate solvers as differentiable
layers in neural architectures (Amos & Kolter, 2017; AgraWaI et al., 2019; Pogancic et al., 2019; Wang et al., 2019;
Paulus et al., 2021). While the above works on solver integration can allow for differentiable optimization of discrete
functions, they do not provide a recipe on hoW to compute their values at continuous points, and they generally rely
on specific assumptions about the function (e.g., linear costs) or the problem formulation (e.g., SDP or QP). Instead,
our frameWork has minimal requirements for the set functions that are being extended.
While Works on gradient estimation through differentiable sampling and solver integration are different in both
methodology and goals from ours, We emphasize that our continuous extension frameWork is compatible With such
approaches. For instance, one could obtain soft samples With a stochastic softmax trick Which could then be evaluated
With an extension of a set function. We believe there is ample opportunity for creative combinations of those ideas.
3	Differentiating through set functions
The common practice of training via backpropagation makes it challenging to incorporate functions defined on discrete
spaces into deep netWorks. This section proposes a frameWork for differentiating through one of the most fundamental
classes of discrete functions capable of modelling collections of objects: functions Whose input domain is a set.
Importantly our frameWork applies to functions that are only defined at discrete values by providing a method for
extending set functions onto continuous domains.
Let Ω ⊆ 2[d] denote a family of subsets of a ground set [d] = {1,...,d}, where each S ∈ Ω is associated with
the binary vector 1S ∈ {0, 1}d Whose ith entry is one if i ∈ S, and zero otherWise. We consider the problem of
minimizing functions
f : Ω → R with Ω ⊆ 2[d],
where f (S) is ill-defined whenever S ∈ Ω. Without loss of generality we will always assume that f is centered with
f (0) = 0. Such set functions appear in a wide variety of problems, including the important case of selecting nodes or
edges ofa graph, e.g., to find a maximum cover or a shortest path.
Due to the discrete topology of the domain Ω, the definition of the derivative of f : Ω → R cannot even be formu-
lated. To resolve this, we propose a method for constructing a new function F : [0, 1]d → R that admits continuous
optimization.
3.1	WHAT PROPERTIES SHOULD A NEURAL FRIENDLY EXTENSION F OF A SET FUNCTION f POSSESS?
In order for optimizing F to be a useful proxy for optimizing f as part of a deep network, it is not sufficient for F
to merely be an extension of f (i.e., to agree with f on all X ∈ Ω). Without additional constraints on F, an arbitrary
extension may introduce spurious minima which do not correspond to good solutions to f. Equally fundamentally, an
arbitrary extension may not be continuous or differentiable, which are needed to allow gradient-based optimization.
We argue that the following properties are essential when training F as a proxy for f :
3
Under review as a conference paper at ICLR 2022
Figure 1: Neural friendly extensions: f (x) is not defined for non-integral points x. To assign a function value to
non-integral X We first reinterpret X as an expectation over integral points 1s in the domain Ω of f. Then, in place of
the ill-defined quantity f (x) = f (ES〜px[1s]) we obtain a well-defined value F(X) .= ES〜px[f (S)] by exchanging
the order of operation of f and the expectation so that f is only evaluated at points S in its domain Ω.
P1. (Extension) F(1s) = f (S) for all S ∈ Ω.
P2. (Minimality) The minima of F belong to the convex hull of the minima of f .
P3. (Continuity) F is Lipschitz continuous (and hence almost everywhere differentiable).
P4. (Rounding) Given an X ∈ [0,1]d, we can efficiently recover an S ∈ Ω for which f (S) ≤ F(x).
P1 requires that F is a functional extension of f . P2 suggests that by minimizing F we should expect to recover good
solutions to f. P3 enables gradient-based optimization. Finally, P4 asks that, starting from a fractional point in [0, 1]d,
we can easily recover a discrete solution that is at least as good. This last point is important since downstream tasks
often require exact discrete values (e.g. computing the size ofa clique can only be done for a discrete set of nodes).
3.2	Neural friendly extensions
We propose a general framework for defining extensions via convex combinations of f evaluated at discrete points.
Phrased probabilistically, we define F to be the expected value of f over a distribution of points in Ω.
Definition (Proper NFE). We call F a proper NFE of f : Ω → R if and only if
F(x) .= ES〜pχ [f (S)] = X Px(S)f (S) and X = ES〜p.[1s] for all X ∈ Hull(Ω),	(1)
S∈Ω
where Px is any distribution supported on Ω and Hull(Ω) denotes the convex hull of points 1s for S ∈ Ω. If there is
an X for which X = ES〜px [1s] , we call F an improper extension.
The requirement that X = ES〜px [1s] ensures that proper extensions F do not introduce bad new minima (see P2 on
minimality). Thus, to extend a set function f at some continuous point X one needs to identify a distribution Px over Ω
whose expectation is X. Figure 1 provides a toy example of a proper extension in which Px is a categorical distribution
supported over k sets such that X = Pik=1 Px (Si)1Si, which exists whenever X lies in the convex hull of {1Si}ik=1.
The function F and its gradient, if it exists, are, respectively, given by
F(X) = Xk ,Px(S,)f(S,) and VxF(X) = Xk , f (S,) YxPx(Si).
i=1	i=1
4
Under review as a conference paper at ICLR 2022
Notice that, although f does not have gradients, F does whenever Pxpx exists. Crucially, the computation of both
quantities can be completed using k calls to the black-box discrete function f, which ensures that the extension remains
tractable whenever the support ofpx is small.
In the following, we provide sufficient conditions on px under which F satisfies properties P1-4.
P1. (Extension) The property that F is a mathematical extension of f, can easily be enforced by requiring that for all
S ∈ Ω We have pis = δs (the point mass at S).
P2. (Minimality) The minimality property follows from the definition of a proper NFE without additional assumptions
on px :
Proposition 1. Consider f : Ω → R such that mins∈Ω f (S) < 0. The following statements hold:
L minx∈Hull(Ω) F(X)= minS∈Ω f (S)
2. argminx∈Hull(Ω) F(X) ⊆ HUIKargminIS:S∈Ω f (S))
The assumption on f is mild. Recalling that we always assume that f (0) = 0 the assumption merely asserts that the
minimization problem does not have the trivial solution S = 0.
Both parts of this result can be understood intuitively. First, F is defined as a convex combination of points f(S) with
S ∈ Ω, and so cannot attain a smaller value than the smallest f (S). Second, if there are multiple distinct minima
{S1 , . . . , Sm } of f, then a linear combination of these points also attains the minimum. One can then show that since
x = Es~px [Is], any minima of F must be a convex combination of minima of f (see Appendix A for proof).
P3. (Continuity) If F is Lipschitz continuous, then F is differentiable almost everywhere (Rademacher’s theorem),
enabling the optimization of F using first-order gradient methods. It is straightforward to deduce that F is Lipschitz
continuous (and hence P3 holds) whenever the mapping X → px(S) is itself Lipschitz continuous for all S ∈ Ω and
PS f(S) is finite. For completeness, we include a proof in Appendix A (Lemma 1).
P4. (Rounding) Our framework provides a simple way to recover exact discrete solutions whose function value are at
least as good as the originally obtained fractional solution F(X). Specifically, given solution X ∈ [0, 1]d, simply return
an S that attains the minimum of f over the support ofpx. That is, any S solving minS:px(S)>0 f(S). Such an S is
guaranteed to yield a solution at least as good as X since F(X) is an expectation over px. This minimization can be
done efficiently so long as the cardinality of the support of px is not too large, as is typically the case for the examples
given in Section 3.3. If the support is large, drawing samples from px yields a discrete solution at least as good as X
with high probability.
3.3	Examples of neural friendly extensions
Next, we demonstrate the generality of our framework by illustrating how it captures several interesting extensions.
Lovasz extension. The LOVaSz extension or Choquet integral (Choquet, 1954) is a widely known continuous ex-
tension. While it is defined for any set function, it has been particularly impactful in the context of submodular set
functions: a central result by LOVaSz (1983) states that this extension is convex if and only if f is submodular. This
insight allows to reduce submodular minimization to convex optimization, forming the basis for the first polynomial-
time submodular minimization algorithm (Grotschel et al., 1981).
The LOVaSz extension is a special case of our framework. For a point X ∈ [0,1]d, suppose w.l.o.g. that X is sorted so
that xi ≥ X2 ≥ ... ≥ xd. Then the LOVaSz extension is defined as
d
F(x) = E(Xi — xi+ι)f (Si),	(LOVaSz extension)
i=1
5
Under review as a conference paper at ICLR 2022
where Si = {1, . . . , i} is the set of the indices of the i largest coordinates of x.
To formulate the Lovasz extension as a NFE We set Px(Si) = Xi - Xi+ι for each i ∈ [d] (We take xd+i = 0) and
assign all remaining probability mass to the empty set px(0) = 1 - xi. So Px(S) is supported on {0} ∪ {Si}id=ι,
i.e., the level sets of x, and satisfies X = Es~px [Is]. Properties P2 and P4 follow automatically from the fact that it
is a proper NFE. Similarly, it is easy to see that pis = δs for all S ∈ Ω and so P1 also holds. Using Lemma 1, we can
also prove that P3 holds. We refer the reader to Appendix A.1.1 for the details.
Singleton extension. Consider an arbitrary set function f : Ω → R, where Ω = {Eι,...,Ed} contains only sets
Ei = {i} with cardinality one. Functions operating on single items may arise, e.g., when taking an arg max over
several values. To define Px(S), we again assume that xi ≥ x2 ≥ . . . ≥ xd and then select Si = {i} for all i ∈ [d].
We then define our distribution as Px(Si) = xi - xi+i with xd+i = 0 and assign all remaining probability mass to
the empty set px(0) = 1 - xi. It is easy to see that P1 holds since pis = δs for all S ∈ Ω, and P4 holds since
Px is supported on d sets. P3 follows as a consequence of P3 holding for the LovaSz extension. Finally, we show in
Appendix A.1.2 that although F is improper, P2 still holds.
Fixed cardinality extension. It is often necessary to search over subsets of a particular size (e.g., when deciding if
a graph has a k-clique). In this case it is natural to define a px that is supported on sets of a fixed size k . Consider
the following construction: for 1 ≤ i ≤ n - k, let Si = {i, i + 1, . . . , k + i} and take the corresponding coefficient
to be px(Si) = xi+k-i - xi+k where as before we assume that x is sorted high to low and that xd+i = 0. P1 and
P4 both hold. However, although this construction yields an efficiently computable expression for F, it fails to satisfy
x = Es~px [1s] and is therefore an improper NFE. In Section 3.4 we will propose a general procedure for remedying
this by carefully modifying px (and f) in order to create a proper NFE for which P2 holds. Finally, P3 fails to hold in
this case, although it satisfies the weaker property of piecewise linearity.
Multilinear extension. The multilinear extension is another well known tool for set function optimization, including
submodular functions (Calinescu et al., 2011). The extension takes the form
F(x) ..=	f(S) xi (1 - xi).	(multilinear extension)
S⊆[d]	i∈S i/S
This fits our framework by fixing px(S) = Qi∈s Xi Qi∈s(1 - Xi), i.e., a product distribution over items. It is easy
to see that X = Es~px [1s] and so P2 holds. It is also apparent that P1 holds, and that px is Lipschitz continuous,
implying that the corresponding F is also Lipschitz continuous (P3). The challenge of the multilinear extension is the
complexity of computing F that naively entails summing over all subsets S ⊆ 2[d]. Still, several functions of interest
admit an efficient computation. For instance, the graph cut, set cover, and facility location functions can be extended in
O(d2) time (Iyer et al., 2014). More generally, any f that is a O(1) degree polynomial can be extended in polynomial
time. For linear f, both the multilinear and LovaSz extension become linear functions.
3.4	Constructing proper NFEs from improper ones
An NFE is described as improper if there is an X for which X = Es~px [1s] and called a proper NFE if equality
holds always. Proper NFEs are preferred since Proposition 1 ensures that P2 holds: that F does not introduce any new
bad minima. However, there may still be instances where one wishes to define F using an improper NFEs that have
other desired properties. One example is the construction given in Section 3.3 of an extension whose distribution is
supported on sets of fixed cardinality. Although this is improper, it has the distinction of searching over sets of a fixed
size, and so may still be of interest for problems for which we are searching for sets ofa particular size.
In this section our goal is, given an improper F defined by px, to provide a principled way of constructing a similar
proper NFE F0 that can be optimized in place of F, with the important benefit of knowing that F0 does not behave
badly by introducing bad minima (i.e., F0 satisfies P2). The forthcoming proposition states the result, which for each
point x where X = Es~px [1s], constructs a proper F0 by adding a carefully designed penalty to the improper F.
6
Under review as a conference paper at ICLR 2022
	ENZYMES	Max-Cut PROTEINS	IMDB-Binary	Max-Clique		
				ENZYMES	PROTEINS	IMDB-Binary
Straight-through	0.877±0.084	0.904±0.174	0.531±0.253	0.556±0.128	0.309±0.443	0.795±0.354
Erdos penalty loss	0.946±0.032	0.930±0.063	0.992±0.019	0.837±0.172	0.900±0.139	0.893±0.149
Greedy alg.	0.969±0.025	0.977±0.034	1.0O0±0.00θ	0.973±0.076	0.978±0.072	0.950±0.071
Lovasz NFE	0.968±0.200	0.971±0.021	0.959±0.048	0.720±0.247	0.875±0.125	0.903±0.214
Table 1: Unsupervised combinatorial optimization: Approximation ratios for combinatorial problems. Values closer
to 1 are better.
Proposition 2. Let Px be a distribution for which x0 = ES〜px [Is] = X and define W .= min ({xi∕χi}d=ι ∪ {1}).
There exists a perturbed distribution p0x (S, z) with p0x(S, 1) = px(S) such that
F0(x) .= E(s,z)〜Px f (S, z)], With f (S, z) .= f f (S) if Z = 0 and Z 〜BemoUUi(W)
β if z = 0,
is a proper NFE.
Note that W measures the closeness of p0x to px and as x0 → x we have W → 1 and so p0x → px in distribution. In
order words as x0 → x we recover the original extension F(x). Although the construction introduces a “perturbed”
distribution p0x, that approximates px, the distribution p0x does not feature at all in the computations required to compute
F0 in practice. This is because F0 can be efficiently computed using F by the law of total expectation: F0(x) =
WF(x) + (1 - W)β. So computationally evaluating F0 amounts to adding a penalization term to F that punishes more
when (1 - W) is large. This occurs precisely when x0 and x are far apart, and so the proper extension F0 can also be
viewed as encouraging the improper extension F to search over X for which X ≈ ES〜px [1s].
We experimentally validate the practical usefulness of this principled and computational efficient technique in Section
4.2 where we apply it to convert the improper k-subset extension into a proper extension. We show that this proper
extension performs well at the task of finding k-cliques.
4	Experiments
4.1	Unsupervised combinatorial optimization
Set functions commonly appear in combinatorial objectives, as they are essential in describing properties of discrete
structures like graphs and sets. Here, we leverage continuous extensions to formulate differentiable unsupervised loss
functions for combinatorial problems on graphs. We test on two problems, the maximum clique (constrained) and the
maximum cut (unconstrained). Our models are trained in a minimal setting in order to emphasize the contribution of
the loss function and are not meant to establish state of the art results. We compare with two other neural approaches.
The unsupervised relaxations from Karalias & Loukas (2020) and the Straight-Through estimator (Bengio et al., 2013).
We use the LovaSz extension as an instantiation of our framework. As a sanity check, we also list the performance of
a non-neural greedy heuristic.
In all cases except for the greedy heuristic, given an input graph G = (V, E), a neural network is trained to produce
a vector X ∈ [0,1]|V|. For the maximum cut problem, we use the LovaSz extension to train the network to minimize
the negative cut function f (S; G) = -Cut(S; G), where S are subsets of the graph G. For the Erdos framework, we
derive a continuous relaxation by calculating the expected value of the discrete objective. For the Straight-Through
7
Under review as a conference paper at ICLR 2022
1.0-
8o6^
^0.4-
⅛ 0.2-
0.0	,
/^subset Lovasz greedy random
PROTEINS (k=3)
PROTEINS (k=4)
Figure 2: Learning to find k-cliques: higher F1-score is better. The k-subset extension, which defines F as a convex
combination of sets of size k, is better aligned with the task and significantly improves over the LoVaSz extension.
estimator, we sample from the distribution of level sets of x using random thresholds and average the evaluations of
the objective on those. Then we backpropagate through the sampling operation by treating it as the identity mapping.
For the maximum clique problem, the Lova§z extension is trained directly with a function that incorporates the con-
straint as a multiplicative term. For the Erdos framework, We use the loss that the authors provided in the paper. For
the Straight-Through, We use the same discrete function as in the Lova§z extension.
We train with a 60-20-20 split and display the test performance of the models that achieved the best validation accuracy.
We report the mean approximation ratio across the test set along with the standard deviation. We train modern graph
neural network architectures on real world datasets with graph sizes of up to a few hundred nodes. For details of the
setup, we refer the reader to Appendix B.1.
Results. The results reported on Table 1 suggest that the Lova´z extension indeed provides a way to directly min-
imize a set function. It is able to outperform the Erdos probabilistic loss (Karalias & Loukas, 2020) on two datasets
on max-cut and is competitive on max-clique. On the other hand, the Straight-Through estimator delivers inconsistent
results, especially on max-clique where an objective with constraints is optimized.
4.2	Constraint satisfaction problems
Constraint satisfaction problems are an important class of widely studied combinatorial problems (Kumar, 1992;
Cappart et al., 2021). In this section we study the applicability of our framework to one such problem: the detection
of k-cliques. Given a graph G = (V, E), the goal is to determine ifit contains a clique of size k or more.
We demonstrate the adaptability of our framework by building the cardinality into F. Specifically, we take Ω to be the
family of subsets of the node set V of size exactly k, and use the fixed set size extension discussed in Section 3.3. The
goal is to learn to output true if we successfully find a k-clique, and false otherwise. We compare this approach
to the LOVaSz extension, which searches over sets of different cardinalities, and two non-neural baselines: 1) Greedily
constructing a set S, starting with S = 0, iterating over all nodes i and updating S J S ∪{i} for the the first i for
which S ∪ {i} is a clique (termination when no such i exists). We return true if the terminal set S is a k-clique and
false otherwise. 2) Randomly generating 1000 sets of size k and returning true if any of these sets is a k-clique,
and false otherwise.
Results. Figure 6 shows that by specifically searching over sets of size k using the fixed set size extension we are
able to significantly improve the performance compared to the LOVaSz extension. While the LOVaSz extension performs
broadly comparably to the greedy baseline, the k-subset extension shows a clear improvement over both. Meanwhile,
the random sampling baseline fails in all settings, confirming the non-triviality of the tasks.
4.3	Directly minimizing training error
The problem of non-differentiability also occurs in settings far beyond combinatorial optimization. Consider for
example the standard supervised learning setup. Inference using a K-way classifier h : X → RK is typically per-
8
Under review as a conference paper at ICLR 2022
96
90
97
96
95
-	95-	e0~ -=-  	-≡- m≡H . C" B…-
■ ■■ ɪ ；：・・■】■：：：■ I
94
0
singleton exp hinge MSE xent singleton exp hinge MSE xent singleton exp hinge MSE xent singleton exp hinge MSE xent
(ours)	(ours)	(ours)	(ours)
SVHN
CIFAR10
Cifarioo
tinylmageNet
Figure 3: Extending supervised training error: We treat the training error as a non-differentiable loss function, and
directly minimize the via the singleton set extension defined in Section 3.3.
SSOl 6uππei3
MSE (epochs)
0.6-
0.4-
0.2-
0.8-
1.0-
train loss
train error
-1‰
r
80 P
5'
g
40 λ
e
-20g
0.0—1----1-----1-----1----1-O
O 50	1∞	150	200
hinge (epochs)
Figure 4: CIFAR10: Unlike common losses that indirectly optimize training error (e.g., cross-entropy), the singleton
extension loss closely approximates the exact (non-differentiable) training error at the same numerical scale.
formed by returning the class with the largest score y(x) = argmaxk=ι,…,k h(x)k, and an overall training er-
ror n Pin=ι 1{yi = y(χi)} is calculated using a labeled training dataset {(χi, yi)}=、. The training error is non-
differentiable since the error on a single sample is calculated using the set function i → 1{y = i}. Since training
error cannot be directly minimized, it is standard practice to optimize a differentiable surrogate objective such as the
cross-entropy loss.
Existing methods for indirectly optimizing training accuracy come with powerful motivations -e.g., via the maximum
likelihood principle for cross-entropy. Our goal is therefore not to show an improvement over these widely used
methods, or to argue that our framework is more principled. Instead, we aim to show that it is possible to derive a valid
approach to supervised classification from the perspective of set function extension. To this end, we use the singleton
extension example given in Section 3.3 as our extension of i → 1{y = i}.
We train ResNet-18 classifiers on several vision datasets, and compare the singleton extension loss to a number of
standard supervised losses: cross-entropy, mean squared error, hinge, and exponential loss. We tune the learning rate
for each loss using a simple exhaustive grid search over the interval lr ∈ {0.01,0.05,0.1,0.2} on a held out validation
set. We report average test set performance over three random seeds.
Results. Figure 3 shows that the singleton extension loss performs comparably to the baselines, and is robust enough
to be effective on {100,200}-way classification tasks where the exponential and hinge losses fail. This demonstrates
the practical viability of our extension approach for classification problems. The unique feature of the singleton loss
is that since it is a direct extension of the empirical training error, its training loss closely tracks the training error (see
Figure 4). While all training losses are, as expected, highly correlated with training error (all have Pearson correlation
of more than 0.99) the singleton loss is the only loss able to directly track training error at the same numerical scale.
5 Conclusion
Although there has been significant focus on gradient estimation techniques in the literature, extending functions to
continuous domains in the context of neural network training has remained relatively unexplored. We have presented
“neural friendly extensions”, a principled method for designing continuous and differentiable extensions of set func-
tions. As we have verified experimentally, our framework offers viable alternatives when training neural networks
with set functions in both classical settings such as image recognition, as well as combinatorial problems like k-clique
detection and max-cut.
9
Under review as a conference paper at ICLR 2022
References
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter. Differentiable
convex optimization layers. Advances in Neural Information Processing Systems, 32:9562-9574, 2019.
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In International
Conference on Machine Learning, pp. 136-145. PMLR, 2017.
F.	Bach. Learning with Submodular Functions: A Convex Optimization Perspective. Foundations and Trends in
Machine Learning, 2013.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, and Francis R Bach. Learning
with differentiable pertubed optimizers. In NeurIPS, 2020.
J. Bilmes. Deep mathematical properties of submodularity with applications to machine learning. Tutorial at the
Conference on Neural Information Processing Systems (NIPS), 2013.
Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable sorting and ranking. In
International Conference on Machine Learning, pp. 950-959. PMLR, 2020.
Ravi Boppana and Magnus M Halldorsson. Approximating maximum independent sets by excluding subgraphs. BIT
Numerical Mathematics, 32(2):180-196, 1992.
Endre Boros and Peter L Hammer. Pseudo-boolean optimization. Discrete applied mathematics, 123(1-3):155-225,
2002.
G. Calinescu, C. Chekuri, M. Pal, and J. Vondrak. Maximizing a submodular set function subject to a matroid con-
straint. SIAM J. Computing, 40(6), 2011.
Quentin Cappart, Didier Chetelat, Elias B. Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Com-
binatorial optimization and reasoning with graph neural networks. In Zhi-Hua Zhou (ed.), Proceedings of the
Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 4348-4355. International Joint
Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/595. URL https:
//doi.org/10.24963/ijcai.2021/595. Survey Track.
Ching-An Cheng, Xinyan Yan, and Byron Boots. Trajectory-wise control variates for variance reduction in policy
gradient methods. In Conference on Robot Learning, pp. 1379-1394. PMLR, 2020.
G.	Choquet. Theory of capacities. Annales de l’Institut Fourier, 5:131-295, 1954.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on
Representation Learning on Graphs and Manifolds, 2019.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void:
Optimizing control variates for black-box gradient estimation. In International Conference on Learning Represen-
tations, 2018.
M. Grotschel, L. Lovasz, and A. Schrijver. The ellipsoid algorithm and its consequences in combinatorial optimization.
Combinatorica, 1:499-513, 1981.
Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting networks via contin-
uous relaxations. In International Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2022
S Gu, T Lillicrap, Z Ghahramani, RE Turner, and S Levine. Q-prop: Sample-efficient policy gradient with an off-policy
critic. In 5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings,
2017.
Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.gurobi.com.
Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx.
Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.
Tamir Hazan and Tommi Jaakkola. On the partition function and random maximum a-posteriori perturbations. In
Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1667-
1674, 2012.
Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes. Monotone closure of relaxed constraints in submodular optimization:
Connections between minimization and maximization: Extended version. In UAI, 2014.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In Int. Conf. on Learn-
ing Representations (ICLR), 2017.
Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for combinatorial
optimization on graphs. In NeurIPS, 2020.
Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k trick
for sampling sequences without replacement. In International Conference on Machine Learning, pp. 3499-3508.
PMLR, 2019.
A. Krause and S. Jegelka. Submodularity in Machine Learning: New directions. Tutorial at the International Confer-
ence on Machine Learning (ICML), 2013.
Vipin Kumar. Algorithms for constraint-satisfaction problems: A survey. AI magazine, 13(1):32-32, 1992.
Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian Peng, and Qiang Liu. Action-dependent control variates for
policy optimization via stein identity. In International Conference on Learning Representations, 2018.
Laszlo Lovasz. SUbmodUlar functions and convexity. In Mathematical programming the state ofthe art, pp. 235-257.
Springer, 1983.
C Maddison, A Mnih, and Y Teh. The concrete distribUtion: A continUoUs relaxation of discrete random variables. In
Int. Conf. on Learning Representations (ICLR), 2017.
Christopher Morris, Nils M. Kriege, Franka BaUse, Kristian Kersting, Petra MUtzel, and Marion NeUmann. TUdataset:
A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation
Learning and Beyond (GRL+ 2020), 2020. URL www.graphlearning.io.
George PapandreoU and Alan L YUille. PertUrb-and-map random fields: Using discrete optimization to learn and
sample from energy models. In 2011 International Conference on Computer Vision, pp. 193-200. IEEE, 2011.
Anselm PaUlUs, Michal Rolinek, Vit MUsil, Brandon Amos, and Georg MartiUs. Comboptnet: Fit the right np-hard
problem by learning integer programming constraints. In Marina Meila and Tong Zhang (eds.), Proceedings of the
38th International Conference on Machine Learning, volUme 139 of Proceedings of Machine Learning Research,
pp. 8443-8453. PMLR, 18-24 JUl 2021. URL https://proceedings.mlr.press/v139/paulus21a.
html.
Max Benedikt PaUlUs, Dami Choi, Daniel Tarlow, Andreas KraUse, and Chris J Maddison. Gradient estimation with
stochastic softmax tricks. In NeurIPS 2020, 2020.
11
Under review as a conference paper at ICLR 2022
Marin Vlastelica Pogancic, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. Differentiation ofblackbox
combinatorial solvers. In International Conference on Learning Representations, 2019.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph
attention networks. In International Conference on Learning Representations, 2018.
Petar Velickovic and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 202LISSN 2666-3899.
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using
a differentiable satisfiability solver. In International Conference on Machine Learning, pp. 6545-6554. PMLR,
2019.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine
learning, 8(3):229-256, 1992.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade, Igor Mordatch, and
Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. In International
Conference on Learning Representations, 2018.
Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous relaxations. In IJCAI, 2019.
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas Pfister. Differentiable
top-k with optimal transport. Advances in Neural Information Processing Systems, 33, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Interna-
tional Conference on Learning Representations, 2018.
Li Yujia, Tarlow Daniel, Brockschmidt Marc, Zemel Richard, et al. Gated graph sequence neural networks. In
International Conference on Learning Representations, 2016.
12
Under review as a conference paper at ICLR 2022
A Section 3 proofs
Proposition 3. Consider f : Ω → R such that f (0) = 0 and mins∈Ω f (S) < 0. The following statements hold:
1.	minχ∈Hull(Ω) F(x)=mins∈Ω f (S)
2.	argminx∈Hull(Ω) F(X) ⊆ HUIKargminIS:S∈Ω f (S))
The assumptions on f are mild. Every set function can be shifted (without changing its minima) to ensure f(0) = 0,
in which case the non-negativity condition mins∈Ω f (S) < 0 merely asserts that the minimization problem does not
have the trivial solution 0.
Proof. The inequality minχ∈Hull(Ω) F(X) ≤ mins∈Ω f (S) automatically holds since Ω ⊆ Hull(Ω), so it remains to
show the reverse. To this end let X ∈ Hull(Ω) be an arbitrary point. Then,
F(X)= Es~pχ[f (S)]
=X Px(S) ∙ f (S)
S∈Ω
≥ X Px(S) ∙ min f (S)
S ∈ Ω
S∈Ω
=minf(S)
where the inequality holds since We know that minS∈Ω f (S) ≤ 0, and the last equality simply uses the fact that
Ps∈ω Px(S). This proves the first claim.
To prove the second claim, assume that X minimizes F(x) over X ∈ Hull(Ω). This implies that the inequality in the
above derivation must be tight, which is true if and only if
Px(S) ∙ f (S) = Px(S) ∙ min f (x)	for all S ∈ Ω.
x∈Ω
This implies that either px(S) = 0 or f (S) = minx∈Ω f (x). Since X = Epx [1S] = Ps∈ωPx(S) ∙ 1S ≥
PS：PX(S)>0Px(S) ∙ 1S. This is precisely a convex combination of points 1S for which f (S) = minS∈Ω f (S),
proving the claim.
□
Lemma 1. If the mapping X 7→ Px(S) is Lipschitz continuous and f(S) is finite for all S, then F is also Lipschitz
continuous.
Proof. The Lipschitz continuity of F(X) follows directly from definition:
∣F(x) - F(x0)I = X Px(S) ∙ f (S) - X Pxo(S) ∙ f (S)
S∈Ω	S∈Ω
=X(Px(S) -Px0(S))f(S) ≤ (2kLιnaχf(S)) ∣∣x - x0k,
S∈Ω	∈
where L is the maximum Lipschitz constant of x 7→ Px(S) over any S, whereas k is the maximal size of the support
of any Px.	口
13
Under review as a conference paper at ICLR 2022
A.1 Extension examples
A.1.1 LOVASZ EXTENSION
Recall the definition: X is sorted so that xi ≥ χ2 ≥ ... ≥ xd. Then the Lovasz extension corresponds to taking
Si = {1, . . . , i}, and letting px(Si) = xi - xi+1, the non-negative increments of x (where recall we take xd+1 = 0).
We show that the Lovasz extension satisfies conditions P1-4. For convenience, We introduce the shorthand notation
ai = px (Si) = xi - xi+1
P1: since we assume X is sorted, it has the form X = (1, 1, . . . , 1, 0, 0, . . . 0)>. Therefore, for each j < i we have
、-----------------------------------------------------------------{z----}
i times
aj = xj - xj+1 = 1 - 1 = 0 and for each j > i we have aj = xj -xj+1 = 0-0 = 0. The only non-zero probability is
ai = xi - xi+1 = 1 - 0 = 1. Finally, by definition Si corresponds exactly to the vector (1, 1, . . . , 1, 0, 0, . . . 0)> = X,
so we see that Px = δχ for X ∈ Ω.
X------{------}
i times
P2: this follows directly from Proposition 1 once we confirm that the LOvaSz extension is a proper probabilistic
extension, i.e., X = Es~px [Is]. Consider an xi. Then every Sj with j ≥ i contains element i, and no Sj with j < i
contain i. So the ith coordianite of the expectation Es~px [1s] is precisely
dd
ai =	(xi - xi+1 ) = xi - xd+1 = xi
i=j	i=j
which verifies that the LOVaSz extension is a proper probabilistic extension.
P3: By Proposition 1, our goal is to show that x → Px(S) is Lipschitz for all S ∈ Ω.
Lemma 2. Let Px be as defined for the LOVaSz extension. Then X → Px(S) is Lipschitz for all S ∈ Ω.
Proof. First note that Px is piecewise linear, with one piece per possible ordering x1 ≥ x2 ≥ . . . ≥ xd (so d! pieces in
total). Within the interior of each piece Px is linear, and therefore Lipschitz. So in order to prove global Lipschitzness,
it suffices to show that Px is continuous at the boundaries between pieces (the Lipschitz constant is then the maximum
of the Lipschitz constants for each linear piece).
Now consider a point X with x1 ≥ . . . ≥ xi = xi+1 ≥ . . . ≥ xd . Consider the perturbed point Xδ = X - δei with
δ > 0, and ei denoting the ith standard basis vector. To prove continuity of Px it suffices to show that for any S ∈ Ω
we have Pxδ (S) → Px(S) as δ → 0+.
There are two sets in the support of Px whose probabilities are different under Pxδ, namely: Si = {1, . . . , i} and
Si+1 = {1, . . . , i, i + 1}. Similarly, there are two sets in the support of Pxδ whose probabilities are different under Px,
namely: Si0 = {1, . . . , i - 1, i + 1} and Si0+1 = {1, . . . , i, i + 1} = Si+1.
So it suffices to show the convergence Pxδ (S) → Px(S) for these four S. Consider first Si:
Pxδ (Si) - Px(Si) = 0 - (xi - xi+1) = 0
where the final equality uses the fact that xi = xi+1. Next consider Si+1 = Si0+1:
Pxδ (Si+1) -Px(Si+1) = (x0i+1 - x0i+2) - (xi+1 - xi+2) = (x0i+1 - xi+1) - (x0i+2 - xi+2) = 0
Finally, we consider Si0 :
14
Under review as a conference paper at ICLR 2022
pxδ(Si0) -px(Si0) = (x0i - x0i+1) - (xi - xi+1)
= (x0i+1 - xi+1) - (x0i+1 - xi+1)
= (xi+1 - δ - xi+1) - (x0i+1 - xi+1)
=δ→0
completing the proof.	□
P4: px defines a family of distributions, each of which is supported on d sets. So evaluation of F, and recovery of
integral solutions, requires d evaluations of f .
A.1.2 Singleton set function extension
Recall the setup. We consider f : Ω → R with Ω = {Eι,..., Ed} where Ei = {i}. To define Px(S), assume
that X is sorted so that xι ≥ x2 ≥ ... ≥ xd. For 1 ≤ i ≤ d let Si = Ei = {i}. As with the LovaSz extension,
let Px(Si) = Xi - xi+i. Note that in this case, X will range over the convex hull of Ω, which is the d-dimensional
simplex. Again we check P1-4.
P1, P3, and P4 all follow by the same argument as the LOvaSz extension.
P2: The ith coordinate of ES〜px [Is] is Px(Ei) ∙ lEi = Xi — Xi+i which is not equal to Xi in general. ThereforePx
defines an improper probabilistic extension, and the proof of Proposition 1 therefore fails. However, in this case, it
turns out that we can give an alternative proof that Proposition 1 is true, and therefore P2 (minimality) still holds in
this case despite being an improper probabilistic extension. To see this, consider the same assumptions as Proposition
1. For x ∈ Hull(Ω),
F(X)
≥
≥
≥
≥
d
XPx(Ei)f(Ei)
i=i
d
X(Xi - Xi+i)f(Ei)
i=i
d
(Xi - Xi+i) minf(Ej)
i=i	j∈[d]
(Xi - Xd+i) minf(Ej)
j∈[d]
xi ∙ m[nf (Ej)
jm∈[idn]f(Ej)
where the final inequality follows since minj∈[d] f(ej) < 0. Taking X = (1, 0, 0, . . . , 0)> shows that inequalities can
be made tight, and the first statement of Proposition 1 holds. For the second statement, suppose that X ∈ Hull(Ω)
minimizes F. Then all the inequality in the preceding argument must be tight. In particular, tightness of the final
inequality implies that xi, and since X ∈ Hull(Ω) = {x : Xi ∈ [0,1], Pi Xi = 1}, we conclude that Xi = 0 for all
i > 1. In this case, since the first inequality must also be tight, hence
d
d
f(Ei)=	(Xi - Xi+i)f(Ei) =	(
i=i
i=i
Xi-Xi+i)jm∈i[dn]f(Ej)=jm∈i[dn]f(Ej)
15
Under review as a conference paper at ICLR 2022
and so we conclude that x = (1, 0, 0, . . . , 0)> belongs to arg minj ∈[d] f(Ej), finishing the proof.
Constructing proper NFEs from improper ones
Here we prove Proposition 2, which shows how to take an improper NFE and construct a proper NFE that approximates
it. We restate the proposition for completeness.
Proposition 4. Let Px be a distribution for which x0 .= Es~px [Is] = X and define w ..= min {xi/x0i}id=1 ∪ {1} .
There exists a perturbed distribution p0x (S, z) with p0x(S, 1) = px(S) such that
F0(x) .= E(s,z)~p0t [f (S, z)], With f (S, z) .= f f (S) if Z = 1	and Z 〜BemoUUi(W)
x	β	if z = 0,
is a proper NFE.
Proof. Our goal is to construct a new distribution p0x that is 1) similar to px, and 2) yields a proper NFE. We search
for a p0x of the form:
Px(S, Z) = Px(S) ∙ 1{z = 1} + qx,w(S) ∙ 1{z = 0}
where Z 〜Bernoulli(w) with parameter W ∈ [0,1], and qx,w is a distribution of our choice. Our aim is to choose W to
be as large as possible so that P0x approximates Px well. Recall that once we have chosen P0x , we replace the improper
extension F with the following proper extension
F0 (x) .= E(s,z)~pX [f(S,z)]	with	f(S,z) .= f( (S) if b = 1,
β if = 0.
where β is a penalty applied when S is sampled from the proper distribution qx,w.
Firstwechooseqx,w. WewantX = Es~pχ[1s] = wEs~p*[1s] +(1 -w)Es~qχJ1s] = wx0 + (1 -w)Es~qχJ1s]
which is satisfied if we select qx,w such that Es~qx,w [1s] = (x - wx0)∕(1 - w). Any qx,w with this property will
do, so take qx,w to be the distribution implied by the LOvaSz extension of f at (x - wx0)∕(1 - w). Finally, we
wish to choose W to be as large as possible. Noting that the only constraints we must respect are that W ∈ [0, 1] and
[(x - Wx0)/(1 - W)]i ≥ 0 for each coordinate i, we may fix W = maxt∈[0,1] t subject to [x - Wx0]i ≥ 0 for all i. The
latter optimization problem admits the closed-form solution the minimum between mini{xi∕χi} and 1.	口
B	Experimental configurations
B.1 Unsupervised combinatorial optimization
Discrete Objectives. For the maximum cut objective, since we are training with gradient descent, we minimized
fcut(S;G) = -cut(S;G),	(2)
where S is an input set. It should be noted that the Lovasz extension of the cut function on a graph, corresponds to the
graph total variation. For the maximum clique problem, we select the discrete objective that yielded the most stable
results across datasets. It is defined as
fclique(S; G) = -W(S)q2(S),	(3)
where q(S) = 2W(S)/(|S|2 - |S|)) and W(S) = P(i,j)∈E WijPiPj.
16
Under review as a conference paper at ICLR 2022
Baselines. We compare with recent work on unsupervised combinatorial optimization Karalias & Loukas (2020).
We use the official implementation of the code and the corresponding loss function for the maximum clique problem.
For the maximum cut problem, we use the probabilistic methodology described in the paper to derive a loss function.
The resulting loss for an input graph G and learned probabilities p is
Lmaxcut (p; G) = a - E[cut(S)] = a -	pidi +	wij pi pj ,	(4)
i∈V	(i,j)∈E
where a is an upper bound on the size of the optimal solution, which we omit in practice as it does not affect the
optimization. In the original implementation, the code makes use of randomization in the input of the neural network.
Specifically, each set of probabilities p ∈ [0, 1]|V | is computed as f(δi; G) = p, where δi ∈ [0, 1]|V | is a one hot
encoding of a randomly chosen node i. This allows for different sets of probabilities conditioned on different inputs
to be produced for each graph. Then each set is decoded with the method of conditional expectation into discrete
outputs. In order to ensure accurate comparisons, we only apply the method of conditional expectation on just one set
of probabilities produced from one randomly generated input. Our implementation of the Lova´z extension just picks
the best level set in the support of px so this establishes a level playing field for the comparisons.
We also compared with the Straight-Through gradient estimator (Bengio et al., 2013). This estimator can be used
to pass gradients through samples from a distribution, by assuming that the sampling/thresholding operation is the
identity. Initially, we observed that by sampling sets from a product distribution on the outputs of a neural network
was not able to minimize the loss function. In order to obtain a working baseline with the straight-through estimator,
we restricted our attention to the distribution of level sets induced by the output of a neural network. Specifically,
given x ∈ [0, 1]|V | outputs from a neural network, we sample indicator vectors 1Sk, where Sk = {xj |xj ≥ tk} where
tk ∈ [0, 1] is a randomly chosen threshold. Then our loss function was computed as
1K
L(x； G) = K Ef(ISk),	(5)
k=1
where f was the clique or the cut functions used in the LovaSz setting, and K the total number of samples, which We
set to 100. This can be viewed as a differentiable stochastic estimate of the Lovasz extension. It should be noted that
this use of the Straight-Through estimator is bound to fail for a general non-differentiable black box f, as no gradients
will be able to flow in the backward pass. Furthermore, we have also experimented with different variants of the score
function estimator. However, it demonstrated extremely unstable behavior during training and was unable to converge.
As it can be seen in figures 6 and 7, our extension minimizes the loss easily across epochs, but the rewards remain
unstable during training for the estimator.
We also reported results for greedy heuristics on both combinatorial problems. These are both heuristics provided
with the NetworkX package (Hagberg et al., 2008). For the max-cut, the heuristic begins with an empty set and adds
at each step the node that maximally increases the cut. For the max-clique heuristic, the implementation relies on an
approximate indepedendent set heuristic that utilizes subgraph exclusion (Boppana & Halldorsson, 1992).
Ground truths. We obtain ground truths for all our datasets and for each combinatorial problem by expressing it as
a mixed integer program and solving it in Gurobi (Gurobi Optimization, LLC, 2021).
Implementation details and datasets. We conduct our experiments using the Pytorch Geometric API (PyG) (Fey
& Lenssen, 2019). The datasets PROTEINS, ENZYMES, and IMDB-Binary are part of the TUDatasets collection of
graph benchmarks (Morris et al., 2020) available through the PyG API. For our models, we use as input features the
node degrees, followed by a Graph Isomorphism Network (GIN) layer (Xu et al., 2018) and multiple iterations of the
Gated Graph Convolution layer (Yujia et al., 2016). We tune the width (64-256), the iterations (3-12), the learning rate
(0.00001,0.001) and the batch size (4,128) of our models with a grid search over fixed values in the reported intervals.
We train for 500 epochs and report the test performance of the model with the best validation score.
17
Under review as a conference paper at ICLR 2022
10.05 -
10.00 -
9.95-
9.90-
9.85-
9.80
9.75-
Score estimator reward
10.10 -
20	40	60	80	100

9.70
0
epochs
Figure 5:	Score function estimator reward over 100 epochs on maximum clique.
Lovasz Extension Loss
-0.65 -
0 5 0 5
7 7 8 8
- - - -
PJeMaJ
-0.90 -	ʌʌ
-0.95」，	，	，	，	，	、
0	20	40	60	80	100
epochs
Figure 6:	LovaSz extension loss over 100 epochs.
B.2	Constraint satisfaction problems
Ground truths. As before, we obtain ground truths for all our datasets and problems by expressing it as a mixed
integer program and using the Gurobi solver (Gurobi Optimization, LLC, 2021).
Implementation details and datasets. Again we use the Pytorch Geometric API (PyG) (Fey & Lenssen, 2019) and
access the datasets through TUDatasets collection of graph benchmarks (Morris et al., 2020) available through the
PyG API. For the model architecture We use a single GAr layer (VelickoVic et al., 2018) followed by multiple gated
graph convolution layers to extract features x = g(G) ∈ [0, 1]|V |, where x contains a single value in [0, 1] for each
node. We use the following same set function as we did for max-clique,
fclique(S; G) = -w(S)q2(S).
For both the LoVaSz extension and k-subset extension, we perform an exhaustive grid search over the follow-
ing hyper parameters: learning rate lr = {0.00001, 0.00005, 0.0001}, GNN widths w ∈ {64, 128}, batch size
bs ∈ {32, 64, 128}, and depths d ∈ {8, 10}. Finally, we use the correction technique for improper NFEs described in
Section 3.4, in which we optimize a penalization of the improper extension:
W ∙ F(X) + (1 - W) ∙ β
where recall that x0 = ES〜px [Is] = X and we define W = min ({χi∕χ∕d=ι ∪ {1}). We also tune the penalty β
during HPO, searching over the two values β ∈ {10, 15}.
After completing the exhaustive grid search, we select the single model that achieves the highest F1-score on a valida-
tion set, and report as the final result the F1-score of that same model on a separate test set.
18
Under review as a conference paper at ICLR 2022
B.3	Directly minimizing training error
Recall that for a K-Way classifier h : X → RK with y(x) = arg maxk=ι,…,k h(x)k, We consider the training error
1n
n £1{y，= y(xi)}
n i=1
calculated over a labeled training dataset {(xi, yi)}in=1 to be a discrete non-differentiable loss. The set function in
question is y 7→ 1{yi 6= y}, Which We relax using the singleton method described in Section 3.3.
Training details. For all datasests We use a standard ResNet-18 backbone, With a final layer to output a vector of
the correct dimension. All models are trained for 200 epochs, except for on SVHN, Which We uses only 100. We use
SGD With momentum mom = 0.9 and Weight decay wd = 5 × 10-4 and a cosine learning rate schedule. We tune the
learning rate for each loss via a simple grid search of the values lr ∈ {0.01, 0.05, 0.1, 0.2}. For each loss We select
the learning rate With highest accuracy on a validation set, then report the average accuracy on a test set over three
separate runs.
19