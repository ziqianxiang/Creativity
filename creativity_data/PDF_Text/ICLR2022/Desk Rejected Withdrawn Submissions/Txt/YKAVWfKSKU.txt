Under review as a conference paper at ICLR 2022
Deep Dirichlet Process Mixture Models
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we propose the deep Dirichlet process mixture (DDPM) model,
which is an unsupervised method that simultaneously performs clustering and fea-
ture learning. As a member of the Bayesian nonparametrics family, the traditional
Dirichlet process mixture model is able to adapt the number of its mixture com-
ponents. However its modelling capacity is restricted since the clustering is per-
formed in the raw feature space, rendering it inapplicable to complex domains like
images and texts. Our method alleviates this limitation by using the flow-based
generative model, which is a deep invertible neural network, to learn more expres-
sive features. These two seemly orthogonal models are unified by the Monte Carlo
expectation-maximization algorithm, and during its iterations Gibbs sampling is
used to generate samples from the posterior. This combination allows our method
to exploit the mutually beneficial relation between clustering and feature learning.
We conducted comparison experiments on four clustering benchmark datasets.
The clustering performance of DDPM shows a significant gain over DPM in most
cases and is competitive compared to other popular methods. Furthermore, the
learned representation of DDPM is shown to be efficient and universal to boost
other methods’ performance.
1	Introduction
Clustering is one of the most long-standing and fundamental tasks in computer science. Besides the
well-known k-means algorithm (MacQueen (1967)) and Gaussian mixture models (GMMs) (Bishop
(2006)), a plethora of methods have been proposed. In those early investigations clustering is per-
formed in the raw feature space, and the models lack the capacity of learning or improving the
expressiveness of the representation.
With the recent success of deep neural networks (DNNs), a new line of research termed deep clus-
tering has emerged (Xie et al. (2016); Yang et al. (2016); Jiang et al. (2017); Caron et al. (2018)).
These works are based on the intuitive principal that good representation encourages better cluster-
ing, and reversely good clustering can lead to better representation. Their methods take advantage
of the success network structures, such as convolution neural networks (CNNs) (Krizhevsky et al.
(2012)) and variational autoencoders (VAEs) (Kingma & Welling (2014)). Their superior perfor-
mance demonstrates the benefits of jointly clustering and representation learning.
However, all these methods share the same insufficiency - they consider the number of clusters
(i.e. k in k-means) as a hyperparameter that need to be specified by the user. This brings severe
restriction to their applications: 1) most of these algorithms are sensitive to the choice of k; 2) users
lack the prior knowledge of the number of clusters; 3) this value itself may be time-varying (e.g. k
may increase as more data is accumulated); 4) In some scenarios, particularly for large dataset, there
is no golden ground truth for this value (Li et al. (2018)).
Dirichlet process mixture (DPM) models, which belong to the Bayesian nonparametrics family, is
a popular method that could solve this conundrum (Antoniak (1974)). Its solid mathematical back-
ground originates from the Dirichlet process (Ferguson (1973)), which is an infinite generalization of
the Dirichlet distribution. DPM is able to model the data with possibly infinite number of mixtures,
and the exact value of mixtures is rigorously inferred by the Bayesian principle. Furthermore, DPM
also possesses the ability to adapt the number of mixtures as more data arrives. It is thus desirable to
combine the strengths of DPM and deep neural networks, which could lead to a clustering method
that simultaneously adjusts its number of mixture components and learns better representation.
1
Under review as a conference paper at ICLR 2022
In this paper we propose the deep Dirichlet process mixture (DDPM) model, which brings the above
idea into realization. Our method bridges the standard DPM with the recently proposed flow-based
generative models (Dinh et al. (2015); Kingma & Dhariwal (2018)), which is a special kind of invert-
ible deep neural networks that learns better representation through density transformation. The over-
all clustering process is guided by the Bayesian principle, while the optimization of model param-
eters is derived from the Monte Carlo expectation-maximization (EM) algorithm (Bishop (2006)).
During the iterations, Gibbs sampling is used to obtain samples from the posterior as in the standard
DPM literature. DDPM also works as a generative model, so that unseen new samples could be
obtained by introducing noises to the learned features.
2	Related work
Clustering and representation learning are both among the most well-investigated topics in computer
science. Besides the well-known k-means (MacQueen (1967)), GMMs (Bishop (2006)) and their
variants, the recent success of deep neural networks has inspired a new paradigm called deep cluster-
ing. The works of (Yang et al. (2016); Caron et al. (2018)) try to take advantage of the convolution
neural networks in computer vision tasks. Their major difference lies in the loss function and the
training scheme. Other DNN structures are also exploited. Xie et al. (2016) proposed Deep Em-
bedded Clustering (DEC), which jointly optimizes deep embeddings and performs clustering based
on features extracted from deep autoencoders. In (Jiang et al. (2017)) Variational Deep Embedding
(VaDE) was introduced, which combines the variational autoencoder with the GMM model. How-
ever all these works share the same insufficiency, i.e., the number of the clusters is a hyperparameter
that need to be specified by the user. As aforementioned this presents challenges for their appli-
cations in real world scenarios, where prior knowledge about the number of clusters is generally
unavailable.
Dirichlet process mixture models (Antoniak (1974)), which belong to the Bayesian nonparametrics
family, is one of the most popular methods that are capable of inferring the number of clusters
automatically. This distinguishing ability is further utilized and strengthened. Teh et al. (2006)
proposed Hierarchical Dirichlet processes, which can share mixture components among different
clusters. The maximum margin DPM (MMDPM) introduces a discriminate model for clustering,
which bridges DPM and the SVM classifier (Chen et al. (2016)). All these works operate in the raw
feature space, without the ability to learn more expressive representation.
Naturally one may wonder whether it is possible to simultaneously decide a clustering model’s ca-
pacity and learn better (possibly nonlinear) features. Ehsan Abbasnejad et al. (2017) proposed to
use an infinite mixture of VAEs to model the data. Since the number of effective VAEs may increase
as more data arrives, their model can adapt to the complexity of the data. In the paper of (Nalisnick
& Smyth (2017)) stick-breaking variational autoencoders (SB-VAEs) were presented, which model
the latent variables in VAEs to be infinite dimensional. Dirichlet processes and particularly the stick-
breaking construction serve as the cornerstones in their method. Our work distinguishes from theirs
in both goal and methodology: their works focus on improving the performance of semi-supervised
classification tasks, while our research considers the unsupervised task of clustering where the num-
ber of mixtures and better representation need to be jointly learned. Both of their models are based
on VAEs while our method utilizes the recently proposed invertible deep neural network, which
demonstrates superior performance in various density estimation and computer vision tasks (Dinh
et al. (2015); Kingma & Dhariwal (2018)).
3	Methodology
3.1	Overview
Consider the input as a set X = {xi ∈ RD}iN=1. Our first step is to use a standard dimension reduc-
tion technique to extract representative features, so that the following clustering can be performed
in the lower dimensional feature space. In our work we use the stack autoencoder (Vincent et al.
(2010); Xie et al. (2016)) to extract the features as Y = {yi = he (xi) ∈ Rd}iN=1 (d D), where
he and hd denote the encoder and decoder functions respectively such that hd(he (x)) ≈ x.
2
Under review as a conference paper at ICLR 2022
Next the features are transformed by a nonlinear learnable function f (y; θ) into Z = {zi =
f (yi ; θ) ∈ Rd}iN=1. Clustering is performed in this transformed space. We assume that each zi
follows an isotropic Gaussian distribution. Suppose that zi belongs to the k-th cluster, the likelihood
is given by:
p(z∕μk ,λk) = N (z∕μk ,λ-1I)
(2∏)2 exp (--k-||zi - μk ||2),
(1)
where μk and λk denote the mean and the precision of the k-th cluster. Note that the isotropic
Gaussian assumption does not restrict the model’s capacity since the transformation f (yi; θ) is
nonlinear, which is implemented by a deep neural network in practice. The cluster assignment
variables are denoted as c = {ci ∈ {1, ..., K}}iN=1, where ci = k indicates that zi belongs to the
k-th cluster. Here the total number of clusters K is unknown to us and could be arbitrarily large.
From the Bayesian perspective, the task of clustering is equivalent to the inference of
P(CNμk }k=ι, {λk}K=ι |Y; θ, φ)∙
(2)
Here Φ is the set of hyperparameters that specify the prior, which will soon be introduced in the
next section. We emphasize the challenges of the task: 1) the number of clusters K is unknown; 2)
θ parameterizes a deep neural network which needs to be learned; 3) the cluster information (i.e.
μk and λk) need to be computed at the same time. In what follows We will see how the proposed
method can address all these challenges under a unified framework.
3.2	Model specification
Likelihood in the feature space Eq. (1) describes the likelihood function in the transformed
space. Now we derives the likelihood in the feature space (i.e. before the transformation) as follows:
p(Y|c, {μk }K=ι, {λk }K=i； θ, φ) = ∏。(丫/4金,λc; θ, φ)
—TT df Iw ∖ df∖y df (yi; θ) I
=1]p(ZilμCi , λCi)I det ∂y |.
(3)
The last term in Eq. (3) is due to the change of variable Zi = f (yi； θ). Recall that p(z∕μca ,λcj is
given in Eq (1).
For a general nonlinear function f(yi; θ) implemented by a neural network, the Jacobian term in
Eq. (3) is analytically intractable. To address this problem we utilize the NICE model (Dinh et al.
(2015)), which is a special kind of deep neural network with the appealing property that the determi-
nant of Jacobian can be trivially computed. The basic idea of NICE is that, in each layer it splits the
input x into two parts as x = {x1, x2}, and defines the output as y = {y1, y2} where y1 = x1 and
y2 = x2 + σ(x1 ). It is easy to verify that for such function the determinant of Jacobian equals one.
After stacking multiple such layers, we have a highly nonlinear function whose Jacobian term can
be trivially cancelled out. Another useful property of NICE is that the transformation is invertible.
Particularly ifZi = f(yi; θ), yi = f-1(Zi; θ) can also be easily computed. Interested readers may
refer to (Dinh et al. (2015)) for further details.
Dirichlet process mixture model in the transformed space After the transformation Zi =
f(yi; θ), we can now perform clustering in this transformed space. In our problem the number
of clusters K is unknown. The Dirichlet process mixture (DPM) models (Antoniak (1974); Neal
(2000); Li et al. (2019)) is one of the most popular tools in this situation. Here we present a concise
review of DPM models, and in the next subsection we will see how to connect it with NICE.
Given a measurable space (Θ, A) where A is a σ-algebra defined on Θ, the Dirichlet process
(DP) (Ferguson (1973)) is characterized by a probability measure G0 on the measure space, and
a positive scaling parameter α. A DP is a random probability measure over (Θ, A), denoted as
G 〜DP(Go, α), such that for any partition (Ai,…,Ar) of Θ we have
(G(AI),…，G(Ar))〜Dir(α0G0(Ai),…，α0Go(Ar)),
where Dir is the finite-dimensional Dirichlet distribution. In other words, a DP is a “distribution
over distribution”.
3
Under review as a conference paper at ICLR 2022
The DPM model is based on DP. Under our formulation, it is defined as
G∣Go,α 〜DP(Go,α)
μk, λkIG 〜G(μk, λk)
zi |μk ,λk 〜P(Zi lμk,λk )
(4)
(5)
(6)
The likelihood in (6) is given in Eq. (1), i.e. in our work the DPM model is applied in the transformed
space Z = {zi = f(yi; θ)}iN=1. We define the base distribution G0 as the conjugate prior of the
likelihood, which is the normal-gamma distribution (Bishop (2006))1:
μk ,λk lμ0, κ0, α0,β0 〜NG(μk, λk lμ0,κ0, α0, βO)
=N (μk ∣μo, (κo λk )-1l)Gamma(λk∣α0,β0).	⑺
{n-i,k
N — 1 + α
α
N — 1 + α
A key property of DPM models is that the marginalized conditional distribution of the cluster as-
signment variable has closed from:
n-i,k > 0
(8)
n-i,k = 0
where c-i = c \ {ci }, N is the number of all data points, and n-i,k is the size of the k-th cluster
excluding the i-th datum. Intuitively the first formula is the probability of assigning the i-th datum
into the k-th existing cluster, while the second formula is the probability of assigning it to a new
cluster. The proof of this result is available in many related literature (Gorur & RasmUssen (2010);
Chen et al. (2016); Li et al. (2019)).
We collect all the hyperparameters in the DPM model as Φ = {α, μo, κo, ɑ0, βo}, which was used
in Eq. (2). To keep the representation succinct we will suppress Φ in the following discussions,
unless it is explicitly needed.
3.3	Parameter estimation with Monte Carlo EM
To combine NICE and DPM models, the key question is how to learn the deep neural network, or
in other words how to optimize the parameters θ in f(yi; θ). In this subsection we will address this
challenge with the Monte Carlo expectation-maximization (EM) algorithm.
To apply Monte Carlo EM We consider c, {μk}K=ι and {λk}K=ι in Eq. (2) as hidden random
variables, Y as the observed variables, and θ as the set of parameters need to be optimized. To
keep the representation succinct, we denote H = {{μk}3ι, {λk}3J∙ Following the maximal
likelihood principle, the optimal θ* is:
θ* = argmaxθ p(Y∣θ).
(9)
This can be solved by the well-known expectation-maximization (EM) algorithm (Bishop (2006)),
which iterates between the E-step and M-step until converges:
E-step: Q(θ, θ(old)) = EH,c|Y,θ(old) [log p(H, c, YIθ)]
M-step: θ(new) = arg maxθQ(θ, θ(old))
θ(Old) 一 θ(new)
(10)
(11)
(12)
In the case that the E-step (10) has no closed-form solution, it can be numerically estimated as
Q(θ, θ(Old)) ≈ G XlogP(H(O),c(g), Y∣θ),	(13)
where H(g), c(g) 〜 p(H, cIY, θ(old)) are i.i.d. samples and G is the sample size. This method is
called the Monte Carlo EM algorithm (Bishop (2006)).
1In our work we consider μk as a scalar, while in the standard normal-gamma distribution it is a scalar.
4
Under review as a conference paper at ICLR 2022
As θ denotes the parameters of a deep neural network, we can use stochastic gradient descent to find
the maximal value in Eq. (11):
∂
Θt+1 一 θt + λs-Q(θ, θ(°ld))	(14)
∂θ
λ∂
≈ θt + -Gs∑ dθ log P(H(g),c(g), Y∣θ),	(15)
g
where λs is the learning rate.
Finally to complete the picture, we need to:
•	Present the analytical form of the complete data likelihood P(Hs), c(g), Y∣θ), and partic-
ularly the derivative of the log-likelihood in Eq. (15);
•	Develop a method to obtain the samples H(g), Cs)〜p(H, c|Y, θ(Old)).
These two questions will be addressed respectively in the following two subsections.
3.4	The complete data likelihood
With our discussions in the model specification section, the complete data likelihood can be derived
as follows:
P(H(O), c(g), γ∣θ) =P(YIH(O), c(g)； e)p(H(g), C(O))	(16)
=P(ZIH(O), c(O))P(H(O), c(O)) Y ∣ det f (yi; θ) ∣	(17)
=P(Z|H(O), c(O))P(H(O))P(c(O))	(18)
=Y P(ZiμO),λCO)) Y P(〃kO), λkO))P(C(O)),	(19)
ik
where P(Zi∣μC°) λCO)) is given in Eq. (1). As We are interested in optimizing the neural network's
parameters θ, and the likelihood only involves θ through Zi = f(yi; θ), the derivative of the log-
likelihood is:
ɪ logP(H(O), c(O), Y∣θ) = X ɪ logP(Zi∣μCO),λCO))	(20)
∂ θ	∂ θ
i
=X -λcO)(Zi-〃的 ffθ).	(21)
i
Since f(yi; θ) is implemented by a DNN, its derivative can be automatically computed by many
modern machine learning frameworks like PyTorch (Paszke et al. (2019)).
3.5	Gibbs sampling
Next we consider how to obtain the samples H(O), C(O)〜p(H, c∣Y, θ(old)) = p(H, CIZ(Old)),
where we define that Z(old) = {Zi(old) = f(yi; θ(old))}iN=1. This can be achieved by Gibbs sam-
pling, which states that we can sample from the joint distribution by iteratively sampling from the
conditional distribution of each variable while keeping others fixed (Bishop (2006)). So in what
follows we will derive the conditional distribution of each variable.
Conditional distribution of μk and λk:
P(μk, λk∣H \ {μk, λk}, c, Z(Old))=P(μk, λk∣c, Z(Old)) = p(〃k,λk|[Z(old)]k)
= NG(μk, λk ∣μn, Kn, αn, βn),	(22)
5
Under review as a conference paper at ICLR 2022
where [Z(old)]k = {zi ∈ Z(old) |ci = k} denotes all the latent variables which are assigned to the
k-th cluster. So the result is a normal-gamma distribution, with parameters given by:
nk = #[Z(old)]k,
Zk = n1k	X Zi
zi∈[Z(old)]k
κoμo + nk Zk
μn =	I
κ0 + nk
κn = κ0 + nk ,
nkd
αn = α0 +-2 ,
βn = βθ + 2 X ∣∣Zi - Zk||2 +
zi∈[Z(old)]k
Kon ||Zk - μo∣∣2
2(κo + nk)
Detailed proof of the result can be found in the Appendix.
Conditional distribution of ci :
•	If n-i,k > 0 (assign to an existing cluster):
logp(ci = k∣c-i, Z(old), H) =logP (Ci = k | c-i,α) + logp(z∕μk,λk) + const
n ik
= log ʊʌ-------+ log N(z∕μk, λ- 1I) + const	(23)
N -1+α
•	If n-i,k = 0 (assign to a new cluster):
log p(ci = k|c-i, Z(old), H)
= logp(zi∣μo, κo, αo,βo)p (Ci = k | C-i,α) + const
= log Jp(Zi Iμ, λ)NG(μ,λ∣μ0,κ0,α0,β0)dμdλ + logP (Ci = k | c-i,α) + const
= log Γ(αn0 ) - logΓ(α0) + α0 log β0 - α0n log βn0 +
1(log κo - log Kn) - nd log 2π + log ： — + const,
2	2	N -1+α
(24)
where κ0n = κ0 + 1, α0n = α0 + d/2, βn0 = β0 +
κo∣∣Zi - μo∣∣2
2(κo + 1)
Since the normal-gamma distribution is the conjugate prior of the likelihood, the integration in the
third line of Eq. (24) is analytically tractable (Murphy (2007)).
Our method is summarized in Algorithm 1 listed above.
4 Experiments
4.1	K-agnostic clustering metrics
When the number of clusters K is unknown, the unsupervised clustering accuracy (ACC) used
by existing K-fixed clustering methods (e.g. Xie et al. (2016)) is no longer suitable to evaluate
the clustering performance. Firstly, ACC is only available when the number of clusters matches
the ground truth. Second, regardless of that constraint, ACC is still not suitable and may even
cause misdirection. The first issue can be addressed by adopting a modified ACC metric we termed
ACC*, which is computed by assigning the majority sample label as the cluster label. To illustrate
the second problem we can consider an extreme case when each sample constitutes a single cluster,
the ACC* is exactly 100% but the algorithm learns nothing from the data. Due to the reasons above
we focus on 3 well defined K-agnostic criterion for performance comparison: clustering F1 score
(F score), adjust random index (ARI) (Steinley (2004)) and V-measure score (V score) (Rosenberg &
Hirschberg (2007)). The ACC* score is only presented for reference purpose. The detailed definition
of the metrics can be found in the Appendix.
4.2	Experiment settings
Dataset preprocessing: We evaluate our method on 4 widely used benchmark datasets including
MNIST, STL-10, Reuters10K, and HHAR. MNIST is a widely used handwritten digit database.
STL-10 is an image recognition dataset, which contains unlabeled data for unsupervised learning.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 DDPM(X, he(∙), Φ, λs)
Require: Input dataset X; encoder he(∙); hyperparameters Φ; learning rate λs
Ensure: Cluster parameters {μk, λk}3i； cluster assignment vector c.
1:	Initialize neural network’s parameters θ
2:	Y - {he(Xi)∣Xi ∈ X}i=ι.
3:	for epoch in {1, ..., EPOCHS} do
4:	Z(Old) 一 {f(yi; θ)}i=ι
5:	\\ E-step; iterations for the Gibbs sampling
6:	for t in {1,…,GIBBS.STEPS} do
7:	For each k sample μk,λk 〜p(μk,λk |H \ {μk,λk}, c, Z(Old)) by Eq. (22)
8:	For each i sample Ci 〜p(c∕c-i, Z(Old), H) by Eq. (23) and Eq. (24)
9:	end for
10:	For each k sample μkg),λkg), for each i sample Cig)
11:	\\ M-step; optimization of the NICE model
12:	for t in {1,…，OPTSTEPS} do
13:	Sample a batch Y(b) J {yi ∈ Y}B=ι
14:	Vθ J ∂∂θ logp(H(g) c(g), Y(叫θ) (Eq. (21))
15:	θ J θ + λsVθ
16:	end for
17:	end for
18:	Return sampled cluster parameters {μkg),λkg) }3i, and the cluster assignment vector c(g)
Table 1: Performance comparison on various datasets. The best performance is bold highlighted.
The value of ACC* is only listed for reference. Superscript dp (ddp) means using the final k value
of DPM (DDPM). Compared with baselines capable of inferring k (GMeans and DPM), DDPM
almost always achieves the best results.
Dataset	Metrics	KMeansdp	KMeansddp	DECdp	DECddp	GMeans	DPM	DDPM
	F score	0.4831	0.5832	0.5221	0.605	0.1146	0.5894	0.7047
	V score	0.5976	0.6493	0.6069	0.6517	0.4358	0.6312	0.7370
HAR	ARI	0.4099	0.5117	0.4499	0.5315	0.0904	0.5050	0.6339
	ACC*	0.7740	0.7947	0.7906	0.7897	0.8542	0.7325	0.7441
	K	11	8	11	8	234	11	8
	F score	0.3070	0.2335	0.3604	0.2985	0.1255	0.4639	0.6388
	V score	0.6510	0.6253	0.6855	0.6582	0.5314	0.6619	0.7192
MNIST	ARI	0.2822	0.2132	0.3348	0.2754	0.1126	0.4307	0.6106
	ACC*	0.9440	0.9527	0.9628	0.9637	0.9257	0.8909	0.9388
	K	56	81	56	81	440	56	81
	F score	0.4565	0.4695	0.506	0.519	0.2512	0.4315	0.4294
	V score	0.6003	0.5939	0.6469	0.6607	0.4830	0.5631	0.5512
STL-10	ARI	0.4162	0.4248	0.4438	0.4564	0.2140	0.3715	0.3688
	ACC*	0.8058	0.8003	0.6875	0.69	0.7455	0.7188	0.6903
	K	32	31	32	31	115	32	31
	F score	0.2268	0.3244	0.241	0.3767	0.0933	0.4126	0.5333
	V score	0.3956	0.4332	0.4374	0.4632	0.3147	0.3138	0.3743
REU-10K	ARI	0.1523	0.2282	0.1676	0.2759	0.0581	0.1211	0.2801
	ACC*	0.8704	0.8719	0.9029	0.8854	0.8951	0.6902	0.6957
	K	26	17	26	17	252	26	17
Reuters10K is a text classification dataset consists of the TF-IDF features of the word, and HHAR
is a sensor signal classification dataset. We flattened the images in MNIST as vectors, and used the
pretrained ResNet-50 (He et al. (2016)) to subtract features for STL-10. After the preprocessing,
MNIST contains 10 classes of 786-dimensional training samples and 7000 samples for each class;
STL-10 contains 10 classes of 2048-dimensional training samples and 1300 samples for each class;
Reuters10K contains 4 classes of 2000-dimensional training samples and 10000 samples in total;
HHAR contains 10 classes of 561-dimensional training samples and 10200 samples in total.
7
Under review as a conference paper at ICLR 2022
ARI
F score
0.8
V score
flow repr . ■ flow repr ■ . flow repr
06 ae repr H . 06 ae repr H H 07 . ae repr
till lLII lLLI
0.90
0.85
0.80
0.75
0.70
0.65
0.8
0.6
0.4
0.2
ACC*
Uh
= ae? I
Gk$-陶例~屋5- Gfsu—例物SaFIS	GfsU-机〜屋一9IS	丽：M
ARI	F score	V score	ACC*
0.8
flow repr . ' flow repr ■ o.8^ flow repr _
ae repr	ae repr	ae repr
I I ： J
LLLI。LLLL LLLl
GMea导好黑站华Fans	GMea屋峰威6女最OFeaZ)S
1.00-
0.95
0.90
0.85
0.80
0.75-
flow repr
ae repr
GMea 建 Lmea 衿m` 卧 mea~	GMea 建 IF松2腌
Figure 1:	The performance of the presented methods improve significantly using the learned fea-
tures, demonstrating DDPM's ability of learning better representation. The experiment is conducted
on HAR(ISt row) and MNIST(2nd row). 81-means means k-means with k = 81, etc.
cluster l(y=0)
cluster 3(y=l)
CjUSter 9(y=6)
cluster 12(y=2)
2 t Z N 2
1 Z 2 2 2
Z 2 Z∙ N 2
NNNZZ
2.1 22 2.
cluster 6(y=3)
ClUSter 7(y=4)
"OH#
√ √ ½ √ V
¥ "，夕 q
夕 ¥ 4 ” ⅛l
cluster 25(y=5)
4> 4。6 0
“666
GGG € G
6 G G 6 6
duster 15(y=7)
7	7	7	7	1
7	7	7	7	7
7	7	7	7	7
7	7	7	r7	7
7 777;
cluster 16(y=8)
CjUSter 4(y=9)
q ” βj r
4q7C7 ?
夕彳夕q >
。夕午q ,
Figure 2:	The generated numbers of MNIST from DDPM cluster centers with noise scale 4.5.
Methods and parameters: Considering that there is no deep clustering can adjust the number of
clusters K, we consider the original Dirichlet Process Mixture Model as our main baseline. Other
popular clustering algorithms includes k-means, GMeans (Zhao et al. (2008)) and DEC. For each
dataset we use the same pretrained autoencoder for all methods to ensure a fair comparison. Follow-
ing the prior work of (Jiang et al. (2017)), we use the network structure of d-500-500-2000-10 for
encoder and 10-2000-500-500-d for decoder, where d denotes the dimension of preprocessed input
samples. All the layers are fully connected with ReLU activation. Similar to DEC, we perform 10
random restarts when initializing all clustering models and pick the result with the best objective
value. We follow Guo et al. (2017) to set hyperparameters for DEC. Hyperparameters of DPM and
DDPM are provided in the Appendix. Core code and pretrained models are both available in our
code repository http://github.com/anony/DDPM (link available in the formal version.)
4.3 Results
Clustering performance: The main comparison results are presented in Table 1, where top 1 scores
are highlighted with bold font. ACC* is listed but not considered to be a formal comparison criterion,
since it is highly sensitive to K. In our experiment we assume K is unknown in prior, so many
8
Under review as a conference paper at ICLR 2022
DDPM Epoch 0
DDPM Epoch 3
DDPM Epoch 7
Figure 3:	The representation learning process of DDPM on MNIST. See the most evident example:
the cluster of number 1 gets denser and more concentrated as the training progresses.
algorithms like k-means and DEC cannot be directly applied. To address this issue we use the final
optimized K values of DPM and DDPM for them to make the comparison fair.
DDPM outperforms DPM by a large margin in 3 datasets (HHAR, MNIST and Reuters10K) and
performs closely to DPM in STL-10. It indicates DPM can benefit a lot from our iterative learning
framework in most situations, while remains indistinguishable to DPM in the worst case. Compared
with all the baselines, DDPM clearly outperforms others in 2 datasets (HHAR and MNIST), and
becomes slightly suboptimal in 1 dataset (STL-10). Note that the baselines except GMeans and DPM
are unable to adjust K, so their better numerical results in STL-10 cannot imply their superiority in
our setting.
Representation quality: To demonstrate that DDPM is capable of learning better features, we exam
and compare the features before and after being processed by the model. Specifically, we apply the
k-means and GMeans clustering algorithms in the feature space (i.e. {yi = he(xi) ∈ Rd}iN=1) and
the transformed space (i.e. {zi = f(yi; θ) ∈ Rd}iN=1). The results are presented in Figure 1. It
can be observed that using the features learned by DDPM consistently leads to better performance,
demonstrating its feature learning ability. We also visualize the learning process of DDPM on Figure
3. We randomly select 5K samples from the MNIST datasets and visualize the variation of their t-
SNE embedding over different epochs. In each cluster, there is a box that shows the ground truth
label and the number of samples. We can see the clusters becomes denser and more concentrated as
the training progresses, which is potentially beneficial for label discrimination.
DDPM as a generative model: Benefit from the reversibility of the flow model, DDPM can also
be utilized as a generative model. We select the largest cluster for each ground truth label, and
generate 25 random samples by adding scaled noise to the cluster centers. Specifically for each
selected cluster k, We obtain μ^ = μk + ne, where e 〜N(0,1) is a Gaussian noise and n is the
noise scale. Since the flow model is invertible, we can obtain the sample as X = hd(f-1(^; θ)).
The visualization results are presented in Figure 2. It can be observed that DDPM is capable of
generating clear handwritten digits with sharp edges.
5 Conclusion
In this paper we propose the deep Dirichlet process mixture (DDPM) model, which jointly achieves
clustering and feature learning in an unsupervised fashion. Our method combines the strengths of
Bayesian nonparametrics models and deep neural networks: Based on the Dirichlet process mixture
(DPM) models, DDPM inherits the ability in adapting the number of mixture components. This
enables our model automatically adjust its capacity without relying on extrinsic prior knowledge
or assumptions. The invertible deep neural network component, which is based on the flow-based
model, further enables DDPM to learn complex and nonlinear features. Experimental results suggest
that DDPM could learn more expressive representation, and achieves better clustering performance
compared to other popular methods. As for future work, we would like to improve the training
efficiency by employing other inference techniques, such as the variational inference framework
(Blei & Jordan (2006)). It is also interesting to extend our method to more sophisticated DPM
models like the hierarchical Dirichlet processes (Teh et al. (2006)).
9
Under review as a conference paper at ICLR 2022
References
Charles E Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric
problems. The annals of statistics, pp.1l52-1174, 1974.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
David M Blei and Michael I Jordan. Variational inference for Dirichlet process mixtures. Bayesian
analysis, 1(1):121-143, 2006.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 132-149, 2018.
Gang Chen, Haiying Zhang, and Caiming Xiong. Maximum margin Dirichlet process mixtures for
clustering. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2016.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components es-
timation. In Workshop of the 3rd International Conference on Learning Representations (ICLR
workshop), 2015.
M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Infinite variational autoencoder
for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 5888-5897, 2017.
Thomas S Ferguson. A Bayesian analysis of some nonparametric problems. The annals of statistics,
pp. 209-230, 1973.
Dilan Gorur and Carl Edward Rasmussen. Dirichlet process Gaussian mixture models: Choice of
the base distribution. Journal of Computer Science and Technology, 25(4):653-664, 2010.
Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with
local structure preservation. In Ijcai, pp. 1753-1759, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
(CVPR), pp. 770-778, 2016.
Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2(1):193-218,
1985.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: An unsupervised and generative approach to clustering. In International Joint Con-
ference on Artificial Intelligence (IJCAI), 2017.
Diederik P Kingma and Prafulla Dhariwal. Glow: generative flow with invertible 1× 1 convolutions.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems
(NeurIPS), pp. 10236-10245, 2018.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the 2nd
International Conference on Learning Representations (ICLR), 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems (NeurIPS), 25:
1097-1105, 2012.
Xiaopeng Li, Zhourong Chen, Leonard KM Poon, and Nevin L Zhang. Learning latent superstruc-
tures in variational autoencoders for deep multidimensional clustering. In International Confer-
ence on Learning Representations (ICLR), 2018.
Yuelin Li, Elizabeth Schofield, and Mithat Gonen. A tutorial on Dirichlet process mixture modeling.
Journal of mathematical psychology, 91:128-144, 2019.
10
Under review as a conference paper at ICLR 2022
James MacQueen. Some methods for classification and analysis of multivariate observations. In
Proceedings of the 5th Berkeley Symposium on mathematical statistics and probability, pp. 281-
297, 1967.
Kevin P Murphy. Conjugate Bayesian analysis of the Gaussian distribution, 2007.
Eric T. Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In Proceedings of
the 5th International Conference on Learning Representations (ICLR), 2017.
Radford M Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of
computational and graphical statistics, 9(2):249-265, 2000.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems (NeurIPS), pp. 8024-
8035, 2019.
Andrew Rosenberg and Julia Hirschberg. V-measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the joint conference on empirical methods in natural
language processing and computational natural language learning, pp. 410-420, 2007.
Douglas Steinley. Properties of the hubert-arable adjusted rand index. Psychological methods, 9(3):
386, 2004.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical Dirichlet pro-
cesses. Journal of the american statistical association, 101(476):1566-1581, 2006.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and
Leon Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network
with a local denoising criterion. Journal of machine learning research, 11(12), 2010.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International conference on machine learning (ICML), pp. 478-487. PMLR, 2016.
Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations
and image clusters. In Proceedings of the IEEE conference on computer vision and pattern recog-
nition (CVPR), pp. 5147-5156, 2016.
Zhonghua Zhao, Shanqing Guo, Qiuliang Xu, and Tao Ban. G-means: a clustering algorithm for
intrusion detection. In International Conference on Neural Information Processing (NeurIPS),
pp. 563-570, 2008.
A Appendix: K-agnostic clustering metrics
Adjust Rand Index: The Adjusted Rand Index (ARI) is an adjust version of the Rand Index (RI)
which restrict its value to range (0,1). The Rand Index computes a similarity measure between two
clusterings by considering all pairs of samples and counting pairs that are assigned in the same or
different clusters in the predicted and true clusterings. Random (uniform) label assignments have a
ARI score close to 0.0 and a perfect match gets score 1.0. If C is a ground truth class assignment
and K the clustering, let’s define a as the number of pairs of elements that are in the same set in
C and in the same set in K, define b as the number of pairs of elements that are in different sets in
C and in different sets in K. The unadjusted Rand index is then given by RI = acNb, where CN
is the total number of possible pairs in the dataset. The Rand index can not guarantee that random
label assignments will get a value close to zero (esp. if the number of clusters is in the same order
of magnitude as the number of samples). To counter this effect we can discount the expected RI of
random labelings by defining the adjusted Rand index as :
ARI
RI - E [RI]
max(RI) — E [RI]
11
Under review as a conference paper at ICLR 2022
V-measure Score: The V-measure is defined based on conditional entropy and cares more about
the homogeneity h = 1 - HHCCK) and completeness C = 1 - HHKC) of clusters, where H(C) is
the entropy of the classes and H(C | K) is the conditional entropy of the classes given the cluster
assignments:
H(C)
|C|
-X Znc ∙ log(勺，H(CI K)
c=1
|C| |K|
-XX 等∙ log
c=1 k=1
The homogeneity encourages each cluster contains only members of a single class and completeness
prefers all members of a given class assigned to the same cluster. The V-measure score is then
defined by:
h ∙ c
h + c
V = 2 ∙
Similar to ARI, the score also has a bounded score between 0.0 and 1 where one side means the
worst and another the best. To give an intuitive interpretation, clustering with bad V-measure can
be qualitatively analyzed in terms of homogeneity and completeness to better feel what “kind” of
mistakes is done by the assignment.
Clustering F1 Score: The F1 score for clustering evaluation is just a traditional F1 score calculated
based on a pair confusion matrix. Similar to ARI, The pair confusion matrix (Hubert & Arabie
(1985)) computes a 2 by 2 similarity matrix between two clusterings by considering all pairs of
samples and counting pairs that are assigned into the same or into different clusters under the true
and predicted clusterings.
B Appendix: Hyperparameters
Here we provide the tuned parameters for DDPM and DPM on all datasets (shared for them):
MNIST：ko = 0.3, αo = 10000, βo = 250000, α = 0.00001, Edpm = 2, Eflow = 1
HAR：ko = 1,αo = 200000, βo = 10000, α = 0.0001, Edpm = 12, Eflow = 3
REUTERS-10K：K0 = 1,α0 = 5000, βo = 10000, α = 0.00001, Edpm = 12, Eflow = 3
STL：ko = 1,αo = 100000, βo = 10000, α = 0.00001, Edpm = 10, Eflow = 3
Edpm and Eflow denotes optimization epochs of DPM and Flow in every round.
C Appendix: definition and posterior of the normal-gamma
DISTRIBUTION
Here we present the definition of the normal-gamma distribution and the derivation of the posterior
distribution. Note that in our work We consider the mean variable μ as a vector, while in the standard
normal-gamma distribution it is viewed as a scalar. This causes slight differences in the derivation
from the standard literature.
Definition 1 (Normal-gamma). Random variables μ ∈ RD and λ ∈ R follows the normal-gamma
distribution, denoted as
(μ, λ)〜NG(μ, λ∣μ0, κo, α0, βo),
if the probability density function is defined as
f (μ, λ∣μo, κo, αo, βo) = N(μ∣μo, (κoλ)-1I)Gamma(λ∣αo, βo).
Theorem 1 (Posterior of normal-gamma). After making n observations D = {x1, . . . , xn}, with
each Xi 〜N(μ, λ-11) follows the i.i.d. isotropic normal distribution, the posterior distribution of
the parameters (μ, λ) is
(μ,λ)〜NG(μn,Kn,αn,βn),
12
Under review as a conference paper at ICLR 2022
where
μn
'n
•n
βn
κ0μ0 + nx
K0 + n ,
K0 + n,
α0 + nD/2,
1	, 1 Xk ⅛∣∣2 , K0n||x - μ0H2
β0 +2⅛ "Xi-X||2 +	2(K0 + n)	∙
Kl
a
Proof. Likelihood:
P(D I μ, λ)
1
(2π)nD∕2
λn*exp(-2 X X (Xij-%∙)2
———ɪ	λnD/2 exp
(2π)nD∕2	P
j=1i=1
λ D Γ
-2 £ Mj — χj)2 + E(xij - Xj)
j=i
———ɪ	λnD/2 exp
(2π)nD∕2	P
i=1
n
nHμ - x||2 + £ H
一	i=1
λ
n
Conjugate prior:
NG (μ, λ | μ0, K0, a0, β0) =f N (μ | μ0, (κ0λ)-1 Ij Gamma (λ | a0, rate = β0)
ZNG (μo, κ0, ɑ0, βo)
λ D2 exp
∣μ - μ01|2 λα0-1e-λβ0
ZNG (μo, K0, a。，β0)
ɪ λα0f
ZNG
Γ(α0) (2π
βα0	(而
百 eχp ( — 2 [κ0Hμ - μ0H2 + 2β0]),
1
D
Posterior:
p(μ, λ | D) H NG (μ, λ | μ0, K0, α0, β0) p(D | μ, λ)
b λDe-(κ0λ∣lμ-μdl2)∕2λα0-iɛ-β0λ X λnD∕2e-2 PD=I Pn=I(Xi-μ)2
H λD λa0+nD/2-lɛ-β0ʌɛ-(A/2)[k0Hm-μ0H2 + Pi l lxi-μll2]
To proceed, we need the following equations:
n
E ||xi- μH2 = nHμ - x||2+ £ ||xi- χH2,
i=1
i=1
n
κ0Hμ - μ0n2+n"μ - x||2=(K0+n)hμ - μnn2+κ0nKχ: n||2
where
K0μ0 + nx
μn =	I
K0 + n
Hence
K0||M — M0||2 + X ||xi — "||2 = κ0H" — M0||2 + n||〃 一 x||2 + X ||xi - x||2
i
i
(K0 + n) ”μ-μn 112 + KnJ'止 + X 1|~ - x|12.
KQ+n	Zr
13
Under review as a conference paper at ICLR 2022
So we have
p(μ, λ | D) H	λD2e-(λ∕2)(κo+n)∣∣μ-μn∣∣2
X λao+nD∕2-J-βολe-(λ∕2) Pi ∣∣χi-x∣∣2e-(λ∕2)"。叫:十仔口
ZN (μ | μn, ((κo + n)λ)-1l) × Gamma (λ | αα + nD/2, βn),
where
βn=βo+1 X>-χ∣∣2+κonBμ∣f.
i=1
□
14