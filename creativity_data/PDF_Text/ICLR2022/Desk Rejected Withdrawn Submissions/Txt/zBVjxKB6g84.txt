Under review as a conference paper at ICLR 2022
Understanding Clipping for Federated
Learning: Convergence and Client-Level
Differential Privacy
Anonymous authors
Paper under double-blind review
Ab stract
Providing privacy protection has been one of the primary motivations of Federated
Learning (FL). Recently, there has been a line of work on incorporating the
formal privacy notion of differential privacy with FL. To guarantee the client-level
differential privacy in FL algorithms, the clients’ transmitted model updates have
to be clipped before adding privacy noise. Such clipping operation is substantially
different from its counterpart of gradient clipping in the centralized differentially
private SGD and has not been well-understood. In this paper, we first empirically
demonstrate that the clipped FedAvg can perform surprisingly well even with
substantial data heterogeneity when training neural networks, which is partly
because the clients’ updates become similar for several popular deep architectures.
Based on this key observation, we provide the convergence analysis of a differential
private (DP) FedAvg algorithm and highlight the relationship between clipping bias
and the distribution of the clients’ updates. To the best of our knowledge, this is
the first work that rigorously investigates theoretical and empirical issues regarding
the clipping operation in FL algorithms.
1 Introduction
First proposed by Konecny et al. (2016), Federated Learning (FL) is a distributed learning framework
that aims to reduce communication complexity and to provide privacy protection during training. The
popular FedAvg algorithm (Konecny et al., 2016) has been proposed to reduce the communication cost
by using periodic averaging and client sampling. There has been many extensions of this algorithm,
mostly by modifying the local update directions (Karimireddy et al., 2020; Zhang et al., 2020; Liang
et al., 2019). Even though FL algorithms have the goal of privacy protection, recent works have shown
that they are vulnerable to inference attacks and leak local information during training (Zhao et al.,
2020; Zhu & Han, 2020; Wei et al., 2020b). As a result, striking a balance between formal privacy
guarantees and desirable optimization performance remains one of the fundamental challenges in FL.
Recently, various FL algorithms (Geyer et al., 2017; Truex et al., 2020; 2019; Wang et al., 2020;
Triastcyn & Faltings, 2019) have been proposed to provide the formal guarantees of differential
privacy (DP) (Dwork et al., 2006). In these algorithms, the clients perform multiple local updates
between two communication steps, and then perturbation mechanisms are added to aggregate updates
across individual clients. In order for the perturbation mechanism to have formal privacy guarantees,
each client’s model update needs to have a bounded norm, which is ensured by applying a clipping
operation that shrinks individual model updates when their norm exceeds a given threshold. While
there has been prior work that studies the clipping effects on stochastic gradients (Bassily et al.,
2014; Chen et al., 2020; Song et al., 2021) in the differentially private SGD (Abadi et al., 2016),
there has not been any work on providing understanding how clipping the model updates affect the
optimization performance of FL subject to DP. Our work provides the first in-depth study on such
clipping effects.
Contributions. In this work, we will conduct rigorous theoretical analysis and provide extensive
empirical evidence to understand how to best protect client-level DP for FL algorithms. Specifically,
we make the following contributions:
1
Under review as a conference paper at ICLR 2022
1)	We analyze the existing model and difference clipping strategies for clipping-enabled FedAvg and
prove that difference clipping outperforms model clipping. Our result provides theoretical insight
into designing FL algorithms with clipping operation.
2)	We empirically show that the performance of the clipping-enabled FedAvg depends on the structure
of the neural network being used - when the structure of the network induces concentrated clients'
updates, the performance drop becomes negligible.
3)	We provide the convergence analysis of the clipping-enabled FedAvg algorithm and highlight the
relationship between clipping bias and the distribution of the clients’ updates. Our result leads to a
natural guarantee of client-level DP for FedAvg.
To the best of our knowledge, this is the first work that rigorously investigates theoretical and
empirical issues regarding the clipping operation in FL algorithms.
1.1 Preliminaries & Related Work
Federated learning typically considers the following optimization problem:
N
mxn f(x)，X fi(x) , where fi(x) = Eξ〜Di F(x; ξ),	(1)
i=1
where N is the number of participating clients; the ith client optimizes a local model fi , which is the
expectation of a loss function F(x; ξ), where the expectation is taken over local data distribution Di.
At each communication round t, the server samples a subset of clients Pt and broadcasts the global
model parameters xt . The sampled clients perform Q steps of SGD updates and compute the total
update differences ∆xt's, and then the server aggregates the update differences to update the global
model. In Algorithm 1, we present a slightly generalized FedAvg algorithm from Karimireddy et al.
(2020); Yang et al. (2021), in which the server uses a stepsize ηg to perform its update. When ηg = 1,
the algorithm becomes the same as the original FedAvg.
In this work, we study FL subject to the rigorous privacy guarantees of Differential Privacy (DP)
(Dwork et al., 2006), whose formal definition is given below.
Definition 1.1. (Dwork et al., 2006) An algorithm M is (, δ)-differentially private if
P(M(D) ∈ S) ≤ eP (M(D0) ∈ S) + δ,	(2)
where D and D0 are neighboring datasets, S is an arbitrary subset of outputs of M.
The common mechanism used to protect DP in centralized training is straightforward: 1) clip
the stochastic gradient with the so-called clipping operation (3); 2) add a random perturbation
Z 〜N(0, σ2I) to the clipped quantity (Abadi et al., 2016). The clipping operation is the key step to
guarantee DP as the noise level σ2 is determined by the clipping threshold c (Dwork & Roth, 2014):
clip(gt,c)= gt ∙ min{l,}.	(3)
However, DP is more complex in FL than that in centralized training. Two key factors distinguish FL
from existing DP machine learning framework are:
• Data distribution: unlike centralized training, in FL the data are naturally distributed on the
clients, and the ClientS Can Potentiany have Very different data distributions. In the Centralized
Algorithm 1 FedAvg Algorithm
1:	Initialize: x0，x0,i = 1,...,N
2:	for t = 0, . . . , T - 1 (stage) do
3:	for i ∈ Pt ⊆ [N] in parallel do
4:	Update agents’ xit,0 = xt
5:	for q = 0, . . . , Q - 1 (iteration) do
6:	Compute stochastic gradient gt,q with E[gt,q] = ^fi(xt,q)
7:	Local update: xit,q+1 = xti,q - ηlgit,q
8:	Global averaging: ∆xt = XtQ - xt, xt+1 = xt + η 吉 Piieptt ∆xi
2
Under review as a conference paper at ICLR 2022
setting, the recent work Chen et al. (2020) has shown that the distribution of the samples affects
the performance of the DP-SGD, but how heterogeneous data distribution affects the design and
analysis of FL algorithm that protects DP is unclear.
•	Local updates: as described in Algorithm 1, the clients will perform multiple local update steps
before sending the model to the server, and itis well-known that when Q > 1, the data heterogeneity
will cause performance degradation in FedAvg even without clipping and perturbation (Khaled
et al., 2019). Although there are multiple alternatives of how the DP mechanism can be applied to
FL algorithms, none of those mechanisms has a rigorous theoretical guarantee, and it is not clear
how to properly balance the optimization performance and privacy guarantees.
These two factors result in different definitions and clipping operations in FL.
DP definitions in FL: Based on the distribution pattern of the client and local datasets, two DP
definitions correspond to the neighboring datasets in Definition 1.1, are commonly considered in FL
algorithm design:
•	Sample-level differential privacy (SL-DP): SL-DP directly follows the centralized DP and protects
each local sample so that the server could not identify one sample from the union of all local
datasets, i.e., D = SiN=1 Di, and D, D0 differ by one sample ξ. SL-DP fits in the cross-silo FL
scenario that has a relatively small number of clients, each with a large dataset. E.g., SL-DP is used
in medical image classification application to protect patients’ personal information (Choudhury
et al., 2019). However, in the Google Keyboard application (Hard et al., 2018) where each client is
an application user, SL-DP that only protects one sample (i.e., an input record) will not be sufficient
to protect the user’s personal information.
•	Client-level differential privacy (CL-DP): CL-DP has a stricter privacy guarantee compared with
SL-DP. It requires that the server cannot identify the participation of one client by observing the
output of the local updates, i.e., D = {Di}iN=1, and D, D0 differ by one dataset Di. CL-DP is
suitable for the cross-device FL scenario such as the Google Keyboard application, which has a
large number of distributed clients.
Clipping operation in FL: Based on different DP requirements and the algorithm structures, a number
of FL algorithms have been proposed which protect DP to some extent.
To protect SL-DP, Truex et al. (2019) proposes to clip and inject noise to every local update. That
is, some Gaussian noise is added to the stochastic gradients git,q given in Algorithm 1. However, as
intermediate updates are kept local and private, the clipping and perturbation to the local steps appear
to be unnecessary, and such operations result in significant performance degradation. Moreover, it
is not clear how such kind of operation impact other aspects of the algorithm performance (such as
algorithm convergence, quality of solutions, etc.)
To protect CL-DP, Wei et al. (2020a) proposes to clip the local models to be transmitted directly.
Similarly, Truex et al. (2020) assumes that the model parameters are upper and lower bounded by
some constant and directly apply perturbations to the local models. However, this scheme also
significantly reduces the training and test accuracy empirically and has no theoretical convergence
guarantee. Recently, Geyer et al. (2017) proposes to clip the difference between the input model
and the output models of the FedAvg algorithm. In particular, one can replace the update directions
∆χi,s of line 8 in Algorithm 1 by their clipped versions as expressed below:
cliP(∆χt,c) = ∆χt ∙min{1,k∆X^},	χt+1 = Xt + ηg高 X clipOχt,C).	(4)
It is shown that such a scheme has better numerical performance than model clipping, but no
convergence proof for the algorithm is given. Reference Triastcyn & Faltings (2019) also clips the
update difference and proposed Bayesian DP to measure the privacy loss and only demonstrates the
numerical performance of the proposed algorithm. D2P-Fed (Wang et al., 2020) follows the same
clipping strategy and further apply compression and quantization during communication to improve
communication efficiency while having DP guarantee, but its convergence guarantee only applies to
the non-clipping version.
In summary, despite extensive recent research about DP-enabled FL, there are still a number of
technical challenges and open research questions in this area. First, it is not clear how various kinds
3
Under review as a conference paper at ICLR 2022
of clipping operations can affect the performance of FL algorithms. Second, it is not clear how to add
noise to balance the convergence of FL algorithms and its CL-DP guarantee.
2 Clipping Issues in FL
As discussed above, clipping is a key operation in providing DP guarantee for FL algorithms.
Therefore, to design algorithms that protect DP in FL, the first step is to understand how clipping
affects the convergence performance of a FL algorithm. Towards this end, we start with analyzing
two common clipping strategies, and identify their theoretical properties. Then we provide a series
of empirical studies to demonstrate how system parameters such as training models, datasets and
data distributions can affect the performance of clipping-enabled FedAvg algorithm. These empirical
studies will be combined with our theoretical analysis in the next section to provide a comprehensive
understanding about the optimization performance and CL-DP guarantees in FL.
2.1	Model clipping versus Difference Clipping
The two major clipping strategies used in protecting CL-DP for FL algorithms are local model
clipping and local update difference clipping, as we describe below.
1.	Model clipping (Wei et al., 2020a): The clients directly clip the models sent to the server. For
FedAvg algorithm, this means performing clip(xit,Q, c). This method appears to be straightforward,
but clipping the model directly results in relatively large clipping threshold, so it requires to add
larger perturbation.
2.	Difference clipping (Geyer et al., 2017): The clients clip the local update difference between
the initial model and the output model according to (4). This method needs to record the initial
model, the update difference typically has smaller magnitudes than the model itself, so the
clipping threshold and the perturbation can be smaller than using model clipping. Note that when
Q = 1, the difference clipping is equivalent to the standard mini-batch gradient clipping (i.e., the
DP-SGD), but in the general case where Q > 1, their behaviors are very different.
Below we analyze how they perform on simple quadratic problems. Our results indicate that the
difference clipping strategy is more preferable, because it is less likely to have strong impact on the
optimization performance. The full proofs of the claims are given in Appendix A.3.
Claim 2.1. Given any constant clipping threshold c, there exists a convex quadratic problem, for
which FedAvg with model clipping does not converge to the global optimal solution with any fixed
Q ≥ 1 and ηl > 0.
Claim 2.2. For all linear regression problem with fixed clipping threshold c, there exist ηl and local
update step Q ≥ 1 such that FedAvg with difference clipping converges to the global optimal solution.
Furthermore, there exist a linear regression problem such that under the same c, ηl and Q, FedAvg
with difference clipping converges to a better solution with smaller loss than the original FedAvg.
Remark 1. To prove Claim 2.1, we construct a problem whose magnitude of the optimal solution is
larger than the clipping threshold. Then FedAvg with model clipping will converge to a stationary
point with magnitude bounded by the clipping threshold, therefore the algorithm will not converge to
global optimal solution.
The technique to prove the first part of Claim 2.2 is related to the analysis for centralized gradient
clipping algorithms in Song et al. (2020). The main difference is that our algorithm consider Q steps
of local update before clipping. We show that by allowing multiple local updates, FedAvg algorithm
with difference clipping optimizes the sum of the Huberzied re-weighted local loss functions. By
properly choosing the learning rate ηl for each local loss function, we can balance the re-weighting
factors so that the optimal solution to the new loss function matches the solution to the original
problem.
The above claims indicate that the difference clipping should outperform the model clipping in terms
of convergence guarantees. Therefore, in the subsequent analysis, we will focus on understanding the
difference clipping enabled FL algorithms. In particular, we consider the Clipping-Enabled FedAvg
(CE-FedAvg) algorithm described in Algorithm 2, which combines the difference clipping with the
slightly generalized FedAvg algorithm described in Algorithm 1 (which uses two stepsizes ηl , ηg, one
for local and one for global updates, respectively). The reason to consider such a bi-level-stepsize
version of FedAvg is that, it has been proved to have superior performance, especially when not all
clients participate in each round of communication (Karimireddy et al., 2020; Yang et al., 2021).
4
Under review as a conference paper at ICLR 2022
Algorithm 2 Clipping-enabled FedAvg Algorithm (CE-FedAvg)
1:	Initialize: x0，x0,i = 1,...,N
2:	for t = 0, . . . , T - 1 (stage) do
3:	for i ∈ Pt ⊆ [N] in parallel do
4:	Update agents’ xit,0 = xt
5:	for q = 0, . . . , Q - 1 (iteration) do
6:	Compute stochastic gradient gt,q With E[gt,q] = ^fi(xt,q)
7:	Local update: xit,q+1 = xti,q - ηlgit,q
8:	Compute update difference: ∆xit = xit,Q - xit,0
9:	Clip: AXt = clip(∆χi, c), where clip(∙) is defined in (3)
10:	Global averaging: xt+1 = xt + η 高 Pi∈pt ∆Xt
2.2 Empirical Results
Experiment Setting. To have a thorough understanding about how the difference clipping can
impact the FedAvg, we conduct numerical experiments with different models, datasets and local data
distributions. We compare the test accuracies between CE-FedAvg and the original FedAvg. Note that
in this set of experiments we do not consider the privacy issues yet, so we do not add perturbation.
To have a fair comparison, we set Q, T, N, |Pt|, ηl and ηg to be identical for both FedAvg and
CE-FedAvg. We first run the original FedAvg, compute k∆xit k and average over all clients i and
iterations t to obtain A and choose the clipping threshold C = 0.5∆.
We run the algorithm using AlexNet (Krizhevsky et al., 2012) and ResNet-18 (He et al., 2016) with
EMNIST dataset (Cohen et al., 2017) and Cifar-10 dataset (Krizhevsky et al., 2009) for comparison.
We split the dataset in two different ways: 1) IID Data setting, where the samples are uniformly
distributed to each client; 2) Non-IID Data setting, where the clients have unbalanced samples.
Details are described below. For EMNIST digit classification dataset, each client has 500 samples
without overlapping. In the IID case, each client has around 50 samples of each class and in the
Non-IID case, there are 8 classes each has around 5 samples and 2 classes each has 230 samples
on each client. For the Cifar-10 dataset, in the IID case (resp. Non-IID case), each client also has
500 samples (resp. 50 samples); these samples can overlap with those on the other clients and the
samples on each client are uniformly distributed in 10 classes, i.e., each client has 50 samples (resp.
5 samples) from each class.
In addition to the image classification problem, we also run the algorithm using the stacked LSTM
model used in Reddi et al. (2021) with Shakespeare dataset Caldas et al. (2018) on the NLP problem.
The dataset is also split in two different ways: 1) IID Data setting, where samples are uniformly
distributed, each client has 3712 samples; 2) Non-IID Data setting where samples are split by the
clients according to the way given in Caldas et al. (2018). Performance Degradation. In Table 1,
we compare the classification results produced by using AlexNet and ResNet-18 on the two datasets.
There are three interesting observations: 1) The data distribution will greatly affect the clipping
performance in FL. When data are IID across the clients, clipping has far less impact on the final
accuracy, otherwise the clipping will introduce some accuracy drop to the trained models; 2) Clipping
has quite different impact on different models - the best accuracy of the models drops 0.10% and
3.60% for ResNet-18 and AlexNet on EMNIST, respetively. The drop is 1.55% for ResNet-18 and
7.30% for AlexNet on Cifar-10, comparing CE-FedAvg with non-clipped version on the Non-IID
data; 3) Data complexity also affects the behavior of the CE-FedAvg - the accuracy drop on Cifar-10
dataset is much larger than that on EMNIST dataset.
The empirical experiments show that heterogeneous data distribution among the clients is one of the
main causes of the different behavior between the clipped and non-clipped algorithms. And the data
heterogeneity issue is unique in FL where the data cannot be shared.
Table 1: The testing accuracy of a) FedAvg and clipping-enabled FedAvg, on IID and Non-IID data. The 4th
and 6th columns display both the accuracy of clipping-enabled FedAvg, and its difference with FedAvg.
Model	dataset	IID(%)	IID Clipping (diff.)(%)	Non-IID (%)	Non-IID Clipping (diff.)(%)
AlexNet	EMNIST	98.20	98.01 (-0.19)	95.60	92.00 (-3.60)
	Cifar-10	66.01	61.18 (-4.83)	57.14	49.84 (-7.30)
ResNet-18	EMNIST	99.61	99.59 (-0.02)	95.43	95.33 (-0.10)
	Cifar-10	76.36	75.83 (-0.53)	59.46	57.91 (-1.55)
5
Under review as a conference paper at ICLR 2022
(a) AlexNet, IID
(b) AlexNet, Non-IID (c) ResNet-18, IID (d) ResNet-18, Non-IID
Figure 1: The distribution of local updates for AlexNet and ResNet-18 on IID and Non-IID data at
communication round 16 for EMNIST dataset. Each blue dot corresponds to the local update from one
client. The black dot shows the magnitude and the cosine angle of averaged local update at iteration t.
(a) AlexNet, IID
(b) AlexNet, Non-IID (c) ResNet-18, IID (d) ResNet-18, Non-IID
Figure 2: The distribution of local updates for AlexNet and ResNet-18 on IID and Non-IID data at
communication round 16 for Cifar-10 dataset.
Update Difference Distribution. To further understand the clipping procedure, we plot in Fig. 1, Fig.
2 and Fig. 3 the magnitudes of local updates k∆xit k and the cosine angles between the last iteration’s
global update and δXt: CoST (DδxJ, ∣p1t∣ PiePt-ι △XtTE/aXik Il ∣p1t∣ Pi∈Pt-ι Axt-1ID .
Due to page limitation, we only put the distribution of communication round T = 16. More detailed
results are given in Appendix A.2. In the plots, we mainly focus on the variance of the magnitudes of
the clients’ update difference (i.e., the blue dots). Larger variance indicates that the updates made by
different clients are more different from each other.
Compare Fig. 1 with Fig. 2 we can see that the update magnitudes on EMNIST dataset are more
concentrated than that on Cifar-10 dataset by having smaller mean and variance. Similarly, by
comparing Fig. 1a with Fig. 1b or Fig. 1c with Fig. 1d, or Fig. 3a with Fig. 3b, it is clear that
the local update magnitudes are more concentrated on IID data than on Non-IID data. Moreover,
ResNet-18 has a more concentrated distribution of update magnitudes than AlexNet. Importantly,
comparing Table 1 with Fig. 1 and Fig. 2, one can observe that the drop in final accuracy of a model
caused by clipping is correlated with the degree of concentration of update magnitudes, as AlexNet
with less concentrated update magnitudes suffers more from clipping, while ResNet-18 exhibits the
opposite behavior.
The above results about the update difference distributions match the accuracy results in Table 1, in
the sense that clipping performs worse when update differences distribution has a larger divergence
and vise versa. Inspired by this observation, in the next subsection, we will characterize the impact of
clipping based on the degree of concentration in local updates and develop the convergence analysis
of CE-FedAvg.
3	Convergence Analysis of Clipping-Enabled FedAvg
In this section, we analyze the theoretical performance of CE-FedAvg as well as its randomly
perturbed version, in order to gain a better understanding of our previous empirical observations and
the trad-off between the convergence performance of FedAvg and its DP guarantees.
Towards this end, we will provide the convergence analysis and privacy guarantees for the DP-FedAvg
algorithm ( Algorithm 3). Compared to CE-FedAvg, this algorithm further adds a random perturbation
zit to the locally clipped model differences. During the communication, we assume that the attacker
6
Under review as a conference paper at ICLR 2022
(a) LSTM, IID	(b) LSTM, Non-IID
Figure 3: The distribution of local updates for Stacked LSTM on IID and Non-IID data at communication
round 16 for Shakespeare dataset. Each blue dot corresponds to the local update from one client. The black dot
shows the magnitude and the cosine angle of averaged local update at iteration t.
can only observe the aggregated update P,i∈pt ∆xt, and this can be guaranteed by using secure
aggregation (Bonawitz et al., 2017) or assuming secure uplinks of the clients.
Despite the similar mechanism used in DPSGD and DP-FedAvg, let us point their major differences:
in DPSGD, the goal is to protect SL-DP, while DP-FedAvg is to protect CL-DP. The key difference in
DP-FedAvg is that the local dataset size is large enough so that after performing multiple local update
steps, the resulting model has relatively good performance. By doing so, we can largely reduce the
number of communication and the corresponding privacy noise added per communication. Note that
DP-FedAvg becomes DPSGD with the following choices of hyperparameters: 1) enlarge the client
number to be the same as the size of the dataset, 2) decrease the local dataset size to 1; 3) decrease
the number of local update to 1; 4) decrease the privacy noise accordingly.
3.1	Convergence Analysis
Theorem 3.1 (Convergence of DP-FedAvg). For Algorithm 3, assume ∣∣Vfi(x) - Vfi(y)k ≤
Lkx	- yk, V	i,x,y,	minxf(X)	≥	f*;E[kgt,q	-	Vfi(Xt,q)k2]	≤	σ2,	I∣gt,qk	≤
G, ∀ t, q, i, ∣Vfi (x) - Vf (x)∣2 ≤ σg2 , ∀i, where L is the Lipschitz constant of gradient,
σl2 and σg2 are intra-client and inter-client gradient variance, G is the bound on stochastic gradient.
By letting ηηι ≤ min{ 4¾, 6QLpP_1)} αnd ηι ≤ √61QL, We hav
T XX E[αtkvf(χt)k2]
t=1
≤ O"";f*)+ 25η2LQ(σ2 + 6Qσg)γι(T) + 包华σ2
ηgηlQT	2	P
|
^≡{^^^^^^^^≡
standard terms for FedAvg
γ2(T)+
2ηg Ldσ 2
ηιPQ
|	'
caused by privacy noise
T
+ G2 T X E
t=1
N ^X (Iai - αi | + Iai - αt I)
i=1
^"{z^^^^^^^^^^^^^^^^^^^^
caused by clipping
6T
+ η nILQGT TE
t=1
,
1N
P ^X(Iai - ai |2 + Iai - a |2)
i=1
^^^^™{z^^^^^^^^^^^^^^^^^^^^^^^^^
caused by clipping
Where P := lPtl, αt := max(c,ηι kp Q-1 V k), αt := max(c,ηι kE[P Q-1 gt∙q ]k), α := NN P=1 ɑt; d
is the dimension of x, γι(T) = T PT=I E[αt] ≤ 1, γ2 (T) = T PT=I E[(at)2] ≤ 1
Algorithm 3 DP-FedAvg Algorithm
1: Initialize: x0，x0,i = 1,...,N
2: for t = 0, . . . , T - 1 (stage) do
3: for i ∈ Pt ⊆ [N] in parallel do
4:	Update agents’ xit,0 = xt
5:	for q = 0, . . . , Q - 1 (iteration) do
6:	Compute stochastic gradient git,q with E[git,q] = vfi (xit,q)
7:	Local update: xit,q+1 = xti,q - ηlgit,q
8:	Compute update difference: ∆xit = xit,Q - xit,0
9:	Clip and perturb: ∆Xi = clip(∆xi, c) + zt, where clip(∙) is defined in (3)
10:	Global averaging: xt+1 = Xt + 为房 Pi∈pt ∆Xi
7
Under review as a conference paper at ICLR 2022
In the bound of Theorem 3.1, the standard terms are inherited from standard FedAvg with two-sided
learning rates which can yield a convergence rate of O(√1qt + 1) When setting n = √QP and
ηι = √^1QL ∙ When there is no clipping bias and privacy noise, Theorem 3.1 exactly recovers the
standard convergence bounds for FedAvg up to a constant, see Theorem 1 in Yang et al. (2021). In
addition to the standard terms, we have extra terms caused by the privacy noise zit and the clipping
operation. We highlight the terms caused by clipping which characterize the estimation bias caused
by clipping. The bias canbe decomposed into terms caused by ∣αt - (y1t∖ and terms caused by ∣α1t - αt |.
Notice that since ∖(t - αt∖ ≤ ηι∖k PQ-Igt,qk - kE[PQ-Igt,q]k∖, it is clear E[∖(t - αt∖] Will
be small if the stochastic local updates have similar variance or magnitudes in norm. This term
characterizes the bias caused by local update variance. In addition, E[∖(t - (t∖] will be small if the
expected local model updates have similar magnitudes in norm across clients and E[∖(t - (t∖] = 0 if
kE[∆x1t]k = kE[∆xtj]k, ∀i,j. This term shows the bias caused by cross-client update variance.
In FL, sometimes each client will have limited amount of data, and the local model updates can be
performed with small σι or even σι = 0 (full batch update). Thus, the bias caused by ∖αt - αt∖ can
be small and is avoidable. However, the bias caused by ∖(t - ɑt∖ is unavoidable since this term
will not diminish even each client updates its local model with full batch gradient. In addition, this
term might be large with heterogeneous data distribution since the heterogeneity may induce quite
disparate gradient distributions across clients. Thus, it is crucial to investigate the bias caused by
∖(t - αt∖ in practice. Note that ∖(t - ɑt∖ is fully controlled by differences in magnitudes of local
model updates when σl = 0 for fixed c. Going back to Fig. 1, we do see that how such differences in
update magnitudes can be affected by both the neural network models and data heterogeneity.
3.2	Differential Privacy Guarantee
The privacy guarantee of DP-FedAvg can be characterized by standard privacy theorems on Gaussian
mechanism. We rephrase Abadi et al. (2016, Theorem 1 ) for client privacy in Theorem 3.2.
Theorem 3.2 (Privacy of DP-FedAvg). There exist constants u and v so that given the number of
iterations T, for any E ≤ uq2T with q = N and ∖Pt∖ = P, ∀t, Algorithm 1 is (e, δ)-differentially
privatefor any δ > 0 if σ2 ≥ V C PN2n δ).
The privacy-utility trade-off of DP-FedAvg can be analyzed by substituting σ2 from Theorem 3.2
into Theorem 3.1. To get more insights on how parameters like T, ηg, ηl and E affect DP-FedAvg, let
us consider simplified Theorem 3.1 in Corollary 3.2.1 with c ≥ ηl QG and σ2 substituted . If c0 < G
in Corollary 3.2.1, then there will be extra bias terms inherited from the bound in Theorem 3.1.
Corollary 3.2.1 (Convergence with privacy guarantee). Assume all assumptions in Theorem 3.1, for
any clipping threshold c = ηlQc0 with c0 ≥ G, and set σ2 as in Theorem 3.2, for any (E, δ) satisfying
the constraints in Theorem 3.2, we have
T XX E[kνf (xt)k2] ≤ O (ηgη1Qτ+η2Q2+ηPl) + O (ηg ηιQT^n(1))	⑸
t=1	'-------------{z------------} '----------{z-------}
standard terms for FedAvg	caused by privacy noise
and the best rate one can get from the above bound is O( N) by optimizing % ,η ,Q,T.
A direct implication of Corollary 3.2.1 is that the big-O convergence rate of DP-FedAvg is the same
as differentially private SGD (DP-SGD) in terms of d, E, and N (the number of samples in DP-SGD).
4 Numerical Experiments
In the experiment, we compare the performance of FedAvg, CE-FedAvg and DP-FedAvg on two
datasets. In both experiments, we set client number N = 1920, the number of client participates in
each round ∖Pt∖ = 80, ∀ t, the number of local iterations Q = 32 and the mini-batch size 64. The
clipping threshold is set to 50% of the average (over clients and iterations) of local update magnitudes
recorded in FedAvg. For DP-FedAvg we set the clipping threshold the same as in CE-FedAvg, we
fix the number of communication rounds and privacy budget for the algorithms to obtain the noise
variance that needs to be added. Among all the experiments, we fix privacy budget δ = 10-5.
8
Under review as a conference paper at ICLR 2022
(a) MLP, = 1.5
(b) AlexNet, = 1.5	(c) MobileNetV2, = 1.5	(d) ResNet-18, = 5
Figure 4: The test accuracy of FedAvg, CE-FedAvg and DP-FedAvg on different models on EMNIST. The
privacy budgets for MLP, AlexNet and MobileNet are = 1.5 while for ResNet, we set = 5.
(a) MLP, = 1.5
(b) AlexNet, = 1.5
(c) ResNet-18, = 1.5
Figure 5: The test accuracy of FedAvg, CE-FedAvg and DP-FedAvg on different models on Cifar-10. The
privacy budgets for MLP, AlexNet and ResNet are = 1.5.
EMNIST dataset. We use the digit part of the EMNIST dataset, which has 240K training samples
and 40K testing samples. We distribute the data in the Non-IID way described in Section II and each
client has 125 samples. We conduct experiments on a 2-layer MLP with one hidden layer, AlexNet,
ModelNetV2 (Sandler et al., 2018) and ResNet-18. The results are listed in Table 2 and Figure 4.
Cifar-10 dataset. The dataset we use is the Cifar-10 dataset, which has 50K training samples and
10K testing samples. We distribute the data in the IID way described in Section II and each client
has 500 samples. We conduct experiments on a 2-layer MLP with one hidden layer, AlexNet and
ResNet-18. The results are listed in Table 3 and Figure 5.
Discussion. Let us discuss the relation between our empirical observations and the theoretical results.
1)	It appears that when the underlying machine learning model is structured (e.g., many layers, has
convolution layers, skip connections, etc), the update difference of FedAvg becomes concentrated,
yielding a better clipping performance (as suggested by the terms related to clipping in Theorem 3.1);
2)	When the model has too many parameters and/or layers, they are sensitive to privacy noise. This is
reasonable since the error term caused by privacy noise in Theorem 3.1 is linearly dependent on the
size of the model d and the square of the LiPschitz constant L (note, that n` H 1/L). From Herrera
et al. (2020, Corollary 3.3), we know that L increases exponentially with the number of layers.
Therefore, larger and deePer models are Potentially more sensitive to Privacy noise.
3)	We conjecture that, to ensure good Performance of DP-FedAvg, we need to Pick a neural network
that is structured enough, while not having too many variables and too many number of layers.
Table 2: The accuracy difference between a) FedAvg and cliP-enabled FedAvg and b) cliP-enabled FedAvg
and DP-FedAvg. The cliPPing threshold is 0.5 of the average magnitude and Privacy budget = 1.5 for MLP,
AlexNet and MobileNetV2 and = 5 for ResNet-18.
Model	# Parameters	# Layers	Accuracy (%)	CliPPing (diff.)(%)	DP (diff.)(%)
MLP	159K^	2	94.0-	93.1 (-1.84)	92.8 (-0.29)
AlexNet	3.3M	7	96.4	94.9 (-1.47)	94.7 (-0.16)
MobileNetV2	2.3M	24	97.8	97.4 (-0.35)	95.8 (-1.62)
ResNet-18	11.1M	18	95.2	95.3 (+0.15)	91.5 (-3.76)*
Table 3: The accuracy difference between a) FedAvg and CE-FedAvg and b) CE-FedAvg and DP-FedAvg. The
cliPPing threshold is 0.5 of the average magnitude and Privacy budget = 1.5 for MLP, AlexNet and ResNet-18.
Model	# Parameters	# Layers	Accuracy (%)	CliPPing (diff.)(%)	DP (diff.)(%)
^LP	616K	2	51.90	44.51 (-7.39)	43.60 (-0.90)
AlexNet	3.3M	7	66.01	61.18 (-4.83)	61.36 (+0.18)
ReSNet-18	11.1M	18	76.36	75.83 (-0.53)	70.68 (-5.15)
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan,
Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for
privacy-preserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security, pp. 1175-1191, 2017.
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konecny, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings.
arXiv preprint arXiv:1812.01097, 2018.
Xiangyi Chen, Steven Z Wu, and Mingyi Hong. Understanding gradient clipping in private sgd: A
geometric perspective. Advances in Neural Information Processing Systems, 33, 2020.
Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla, Yoonyoung Park, Grace
Hsu, and Amar Das. Differential privacy-enabled federated learning for sensitive health data. arXiv
preprint arXiv:1910.02578, 2019.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Cynthia Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating Noise to Sensitivity in Private
Data Analysis, pp. 265-284. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006. ISBN
978-3-540-32732-5. doi: 10.1007/11681878_14. URL http://dx.doi.org/10.1007/
11681878_14.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Frangoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Calypso Herrera, Florian Krach, and Josef Teichmann. Estimating full lipschitz constants of deep
neural networks. arXiv preprint arXiv:2004.13135, 2020.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt疝ik. First analysis of local gd on
heterogeneous data. arXiv preprint arXiv:1909.04715, 2019.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
10
Under review as a conference paper at ICLR 2022
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance
reduced local sgd with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv
Kumar, and H Brendan McMahan. Adaptive federated optimization. International Conference on
Learning Representations, 2021.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018.
Shuang Song, Om Thakkar, and Abhradeep Thakurta. Characterizing private clipped gradient descent
on convex generalized linear problems. arXiv preprint arXiv:2006.06783, 2020.
Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of
dimensionality in unconstrained private glms. In International Conference on Artificial Intelligence
and Statistics, pp. 2638-2646. PMLR, 2021.
Aleksei Triastcyn and Boi Faltings. Federated learning with bayesian differential privacy. In 2019
IEEE International Conference on Big Data (Big Data), pp. 2587-2596. IEEE, 2019.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.
Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei. LDP-Fed: Federated
learning with local differential privacy. In Proceedings of the Third ACM International Workshop
on Edge Systems, Analytics and Networking, pp. 61-66, 2020.
Lun Wang, Ruoxi Jia, and Dawn Song. D2p-fed: Differentially private federated learning with
efficient communication. arXiv preprint arXiv:2006.13039, 2020.
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek,
and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance
analysis. IEEE Transactions on Information Forensics and Security, 15:3454-3469, 2020a.
Wenqi Wei, Ling Liu, Margaret Loper, Ka-Ho Chow, Mehmet Emre Gursoy, Stacey Truex, and
Yanzhao Wu. A framework for evaluating gradient leakage attacks in federated learning. arXiv
preprint arXiv:2004.10397, 2020b.
Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation
in Non-IID federated learning. International Conference on Learning Representations, 2021.
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A federated learning
framework with optimal rates and adaptivity to Non-IID data, 2020.
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients.
arXiv preprint arXiv:2001.02610, 2020.
Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated Learning, pp. 17-31. Springer,
2020.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Theorem 3.1
By Lipschitz smoothness, we have
f (Xt+1) ≤ f (Xt) + hVf (χt),χt+ι - Xti + 2kxt+ι - xtk2.	(6)
Before we proceed, we define following quantities to simplify notation:
αt :=____________c__________ at :=______________c____________
i'	max(c,ηιk PQ-1 gt,qk),	i' max(c,ηιkE[PQ-1 gt,q]k),
Q-1	Q-1
∆t := -ηι X gt,q ∙ ɑt, Ati = -η X gt,q ∙ at,
q=0	q=0
Q-1	Q-1
∆t :=	-ηι	X	gt,q ∙ at,	∆i	:= -ηι X	Vfi(Xt,q)	∙ at	P :=	|Pt|,
q=0	q=0
1N
at := N X at,
i=1
(7)
where the expectation in at is taken over all possible randomness.
By using the above definitions, the model difference between two consecutive iterations can be
expressed as:
χt+ι - χt = ηgp E (At + zt),
i∈Pt
with Zt 〜N(0, σ2I). Using the above expressions, and take an conditional expectation of (6)
(conditioned on Xt), we obtain:
E[f(Xt+1)] ≤ f(Xt) +ηg Vf(Xt),E
= f(Xt) +ηg Vf(Xt),E
P X	At+Zt	++2 η2E	I Il P X	At+Zt
i∈Pt	i∈Pt
PX At#++2 ηgE U PXAtI〕+2 2η P σ2d,
(8)
where d in the last expression represents dimension of Xt; in the last equation we use the fact that Zit
is zero mean.
Next, we will analyze the bias caused by clipping, through analyzing the first order term in (8).
Towards this end, we have the following series of relations:
Vf(Xt), E
PX aU
i∈Pt
(=i)
Vf(Xt),E
P Ei [ X Ai]
i∈Pt
PP
Vf(xt),PPE NfAi
i=1
=
N
D
=	Vf(Xt),E
+ Vf(Xt),E
抵X At- A tl >+( Vf(Xt),E
F XA t - AtD
BX *
i=1
(9)
where (i) we takes expectation on the randomness of the client sampling, i.e., Ei At = NPiN=1 Ait .
The first two terms of RHS of the above equality can be viewed as bias caused by clipping. The first
12
Under review as a conference paper at ICLR 2022
order predicted descent can be analyzed from the last term by completing the square:
	卜f(xt),E N X =E "*vf(χt),NN X Ni) =-η2tQ kvf (χt)∣2-% E[∣ 志 X δ t∣2] +* E ]∣pQVf(Xt)-√⅛η⅛ X A J,	(10) '	AZ	}
where (i) comes from E∆t = ∆t, (ii) is becauseha, b)= -2 |同|2 - 2 |回|2 + 1 ∣∣a - b∣∣2 holds
true for any vector a, b.
We further upper bound A1 as
	ALQE k f(xt)-左 X W Vfi(Xt,q J =QE [∣ 备 X QX1Vfi(Xt)-Vfi(Xt,q )∣ j 1 N Q-1 ≤NE EE[∣∣Vfi(xt)-Vfi(xi,q)k2] i=1 q=0 N Q-1 ≤ N XX L2E[∣xt - xi,q k2] i=1 q=0 ≤L25Q2ηl2(σl2+6Qσg2)+L230Q3ηl2∣Vf(Xt)∣2	(11)
where the first inequality comes from Jensen’s inequality, the second inequality comes from
L-smoothness and the last inequality is due to (Reddi et al., 2021, Lemma 3).
Now we turn to upper bounding the second order term in (8), as follows
E B I	∣ P iXt』口 Γ∣ 1	∣∣2]	Γ∣ 1	∣∣2]	Γ∣ 1	∣∣2]
≤3E	PL-i∣j +3e	PiXtδi-工∣j +3e ∣PiXt力(12)
13
Under review as a conference paper at ICLR 2022
We can bound the expectation in the last term of (12) as follows:
E 口 P ib
=EI "(ηιX1 gt,q ∙αt
2
≤ηl2E 2
≤2E
Q-1
P XX
i∈Pt q=0
2
2
Vf (χt,q) ∙ αt +2
P X ∑1(Vf (χi,q) - gi,q) ∙ αt∣
i∈Pt q=0
PiXt δ W P η2α2Qσ2
(13)
where the last inequality is because the assumption that E[kgit,q - Vfi(xit,q)k2] ≤ σl2. Let us further
bound the expectation in the first term of (13) as:
E U P Xt δ I=P E (Is δ I
=)PE EiX∣∆t∣2+Eij X D∆i,∆j
i∈Pt	i6=j∈Pt
N2
(=) PE N x∣δt∣ + P(P - 1)DEiδEj δjE
i=1
=P E( P x∣δ i∣2+P(P -1)
i=1
(14)
where in (i) we expand the square and take expectation on the randomness of client sampling, and (ii)
is due to independent sampling the clients with replacement so that Ei,j ∆it, ∆tj = Ei ∆it, Ej ∆tj .
Additionally, note we have:
E X∣∆ t∣2=) E X η2(at)2
i=1	i=1
Q-1
Vfi(Xt) + Vfi(Xit,q) -Vfi(Xt)
q=0
(ii)	N	Q-1	Q-1
≤ 2η2αt X Q2 X EIVfi(Xt)+Vfi(xt,q)∣∣ + Q2 X ∣∣Vfi(Xt)∣∣2
i1	q0
q0
(iii)
≤ 2η2αtN(L25Q2η2(σ2 + 6Qσg) + L230Q3η2∣∣Vf(xt)k2 + 2Q3∣∣Vf(xt)k2 + 2Q3σg
=10Nη4αtL2Q2 σ2 +4Nη2αtQ3(15L2η2 + 1)(∣∣Vf (xt)k2 + σg).
(15)
where (i) comes from the definition of ∆t； (ii) comes from the factthat ∣∣a + b∣∣2 ≤ 2(ka∣∣2 + |网|2)；
in (iii) we apply (11) to the first term and bound the second term by the assumption that kVfi(X) -
Vf(X)k2 ≤ σg2.
2
14
Under review as a conference paper at ICLR 2022
Combining (8)-(15), we have
Ef(Xt+1)] ≤f(xt) - ηgη2αtQkV∕(xt)k2 — 啥E [卷 Xδi|
t K^t
+ ηgη2a (5L2Q2η2 (σl +6Qσg) + 30L2Q3η2kVf(xt)k2)
+ % (Vf(xt), E NX XX ∆t - ∆( ) + ηg (Vf(xt), E
N XX N - KD
i=1
+ 3Lη2(P — 1) E Jk XX ∆i∣] + 3Lηgη2(at)2Qσ2 + IL n P σ2d
+ P η4ηgαtL2Q2 σl + P η2ηgαt Q3 (15 L2 ηl + 1)(kVf (xt)k2 + 焉)
+3LT1P？-δtl 1+3LTlPXtδt-M	(16)
When ηg ηι ≤ min{ √⅛Qq , 6QLPP_1)} and ηι ≤ √61QL ,the above inequality simplifies to
Ef(Xt+1)] ≤f(xt) — ηgη4αQkVf(xt)k2
+ 吗宏(1+ 空泮)L2Q2 (σ2 +6Qσg)
2P
+ηg	Vf(Xt),E
NN
X X XL Nl + + ηg (Vf(xt),E X X At- △：
i=1
i=1
N
N
D
+ 3Lηgη2(at)2Qσ2 + L η2 ɪ σ2d
+3LT P XN-δ [巾臼1 P Xt δ t—工1	(17)
Sum over t from 1 to T, divide both sides by TηgηιQ∕4, and rearrange, We have
1T
T ∑E[atkVf(xt)k2]
t=1
4
≤ 斤^(Ef(X1)] — Ef(XT +1)])
TηgηlQ
TT
+ 10η2L2 Q(1 +	P^ )(σ2 + 6Qσg) T X αt + P~ηg ηισ2 T X(αt)2 + 2L ∑τgPdσ2
P	T	P T	ηP
t=1	t=1	l
+ ɪ X ɪE [/
T ⅛ ηιQ [∖
6L 1 T
+ mηg T X E
1
Vf(Xt),E Xf∆ — ∆t
i=1
l2
1 二、
Vf(Xt),E MΣ› i - At
i=1
D)
P X ∆i-δ t
i∈Pt
+ηQηgTXE UPXtδt-AtIl.	(18)
N
N
+
Upper-bounding the last four terms using kgit,q k ≤ G yields the desired result.
□
A.2 Additional Numerical Experiments
In this section, We provide additional numerical results Which cannot be placed in the main paper due
to page limitation.
15
Under review as a conference paper at ICLR 2022
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 64
(e) Non-IID, t = 0
(f) Non-IID, t = 2
(h) Non-IID, t = 64
(g) Non-IID, t = 8
Figure 6: The distribution of local updates for MLP on IID and Non-IID data at different communication rounds
for EMNIST dataset. Each blue dot corresponds to the local update from one client. The black dot shows the
magnitude and the cosine angle of global model update at iteration t.
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 64
(e) Non-IID, t = 0
(f) Non-IID, t = 2	(g) Non-IID, t = 8	(h) Non-IID, t = 64
Figure 7: The distribution of local updates for AlexNet on IID and Non-IID data at different communication
rounds for EMNIST dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
We run the algorithm using AlexNet (Krizhevsky et al., 2012) and ResNet-18 (He et al., 2016) with
EMNIST dataset (Cohen et al., 2017) and Cifar-10 dataset (Krizhevsky et al., 2009) for comparison.
In addition to the image classification problem, we also run the algorithm using the stacked LSTM
model used in Reddi et al. (2021) with Shakespeare dataset Caldas et al. (2018) on the NLP problem.
We plot the change of the distributions of the update differences of different algorithms listed in the
main paper. Notice that in all models and datasets, the distributions of the magnitude in the IID cases
are more concentrated than the corresponding Non-IID cases. Also, the distributions of the same
model trained on EMNIST dataset are more concentrated than trained on Cifar-10 dataset.
16
Under review as a conference paper at ICLR 2022
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 32
(e) Non-IID, t = 0	(f) Non-IID, t = 2	(g) Non-IID, t = 8	(h) Non-IID, t = 32
Figure 8: The distribution of local updates for MobileNetV2 on IID and Non-IID data at different communication
rounds for EMNIST dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 32
(e) Non-IID, t = 0	(f) Non-IID, t = 2	(g) Non-IID, t = 8	(h) Non-IID, t = 32
Figure 9: The distribution of local updates for ResNet-18 on IID and Non-IID data at different communication
rounds for EMNIST dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
A.3 Quadratic Example
A.3. 1 Proof of Claim 2.1
Given a fixed clipping threshold c, consider the following quadratic problem
i=1
17
Under review as a conference paper at ICLR 2022
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 64
(f) Non-IID, t = 2
(h) Non-IID, t = 64
(e) Non-IID, t = 0
(g) Non-IID, t = 8
Figure 10: The distribution of local updates for MLP on IID and Non-IID data at different communication
rounds for Cifar-10 dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 64
(e) Non-IID, t = 0
(f) Non-IID, t = 2	(g) Non-IID, t = 8	(h) Non-IID, t = 64
Figure 11: The distribution of local updates for AlexNet on IID and Non-IID data at different communication
rounds for Cifar-10 dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
where we have N = 3 clients. By applying model clipping to FedAvg, one round update can be
expressed as:
13
x+ = 3 ECliP(λx + (1 - λ)bi,c),
λ=(1-ηl)Q ∈ (0, 1),
(19)
where ηl is the loCal stePsize.
18
Under review as a conference paper at ICLR 2022
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 32
(e) Non-IID, t = 0	(f) Non-IID, t = 2	(g) Non-IID, t = 8	(h) Non-IID, t = 32
Figure 12: The distribution of local updates for ResNet-18 on IID and Non-IID data at different communication
rounds for Cifar-10 dataset. Each blue dot corresponds to the local update from one client. The black dot shows
the magnitude and the cosine angle of global local model update at iteration t.
(a) IID, t = 0	(b) IID, t = 2	(c) IID, t = 8	(d) IID, t = 64
(e) Non-IID, t = 0
(f) Non-IID, t = 2	(g) Non-IID, t = 8
(h) Non-IID, t = 64
Figure 13: The distribution of local updates for Stacked LSTM on IID and Non-IID data at different
communication rounds for Shakespeare dataset. Each blue dot corresponds to the local update from one
client. The black dot shows the magnitude and the cosine angle of global local model update at iteration t.
Suppose that the algorithm converges, then we will have solution x+ = x = x∞ . This implies that
13
-ECliP(λx∞ + (1 - λ)bi, C) = x∞
i=1
(20)
Let us set b1 = b2 = -0.5c, b3 = kc, then it is easy to verify that the oPtimal solution of the Problem
is given by x?=(与I)C > 0. However, When k > 4, from (20) We can see that x∞ ≤ C and x? > c.
Therefore, the only possibility is that x∞ = 3-λ2λC ≤ C = x?, and this holds true for any λ ∈ (0,1).
So the stationary solution of FedAvg With model cliPPing to this Problem Will not converge to the
original optimal solution no matter hoW We choose Q and ηl .
19
Under review as a conference paper at ICLR 2022
A.3.2 Proof of Claim 2.2
First, we prove that using difference clipping, FedAvg can converge to global optimal by carefully
selecting Q and ηl. Consider the following convex quadratic problem
N1
f (X) = £ 2(Aix - bi)2.
i=1 2
By applying FedAvg with update difference clipping, one round of update can be expressed as:
1N
x+ = x - N ECliP(AiVfi(x), C)
i=1
(21)
Λi = (I-(I-ηlAiTAi)Q)(AiTAi)-1.
In order for the Problem to Converge to the original Problem, it is easy to verify that the following
Condition has to hold:
N
X CliP(ΛiVfi(x*),c) =0.
i=1
The above examPle Can be viewed as using gradient desCent to oPtimize a Problem with the following
gradient
Vfi0(x) =
ΛiVfi(x)
cΛiVfi(χ)
kΛiVfi(x)k
kΛiVfi (x)k ≤ c,
otherwise.
(22)
Note that in general it is hard to write down the exaCt loCal Problems fi0 that satisfies the above
Condition, but when x ∈ R is a sCalar, fi0(x) is the Huberized loss of Λifi(x) Song et al. (2021)
_ ∫Λifi(x)	if ∣ΛiAi(Aix -bi)∣≤ c,
fi x	ɪe J Afi(x)∣ - 1 c2 otherwise.
(23)
In general, the re-weighted Problem does not have the same solution as the original Problem, but
we Can seleCt ηl and Q (determined by on x? and fi ’s) so that f0(x) has the same solution as
f (x). For examPle, one set of Parameters that satisfy the above requirement is Q = 1, ηl =
1/ maxi{kVfi(x?)k}. In this Case, Λi = Iηl, and when ηl is small enough, the CliPPing will not be
aCtivate when x = x? and PiN=1 CliP(ΛiVfi(x?), c) = PiN=1 ηlVfi(x?) = 0.
Next, we show that CliPPing-enabled FedAvg Can outPerform the non-CliPPed version. Note that
when Q > 1, even when η is small suCh that the CliPPing is not aCtivated, the algorithm will not
Converge to the original solution. So in general one Cannot draw the ConClusion about whether
CliPPing helPs or hurts the PerformanCe of FedAvg. Consider the following Problem:
3
f(x) = Xfi(x),
i=1
fι(x) = 2(x - 4)2, f2(x) = 2(2x - 1)2, f3(x) = 1(6x + 1)2.
(24)
As Vf (x) = (x - 4) + (4x - 2) + (36x + 6) = 41x, the oPtimal solution of this Problem is x? = 0.
Table 4 show the stationary Points of FedAvg under different ChoiCe of Parameters. When Q = 1,
FedAvg is equivalent to SGD and CliPPing hurts the PerformanCe of FedAvg. However, when Q is
large, CliPPed FedAvg has a better PerformanCe than the non-CliPPed version, in the sense that the
stationary solution it obtains are closer to the global optimal solution x* = 0.
20
Under review as a conference paper at ICLR 2022
		Q =	1	Q=	∞
c=	∞	x∞	=0^	x∞	13 二 ɪ
c	=1	x∞	1 二2	x∞	_ 9 二 3
Table 4: Stationary points of FedAvg with gradient clipping for (24) under different parameter
settings.
21