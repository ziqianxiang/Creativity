Under review as a conference paper at ICLR 2022
Numerical Solution of Fredholm Integral
Equations of the Second Kind using Neural
Network Models
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel method based on a neural network with one hidden layer
and the collocation method for solving linear Fredholm integral equations of the
second kind. We first choose the space of polynomials as the projection space
for the collocation method, then approximate the solution of a integral equation
by a linear combination of polynomials in that space. The coefficients of this
linear combination are served as the weights between the hidden layer and the
output layer of the neural network while the mean square error between the exact
solution and the approximation solution at the training set as the cost function. We
train the neural network by the gradient decent method with Adam optimizer and
find an optimal solution with the desired accuracy. This method provides a stable
and reliable solution with higher accuracy and saves computations comparing with
previous neural network approaches for solving the Fredholm integral equations
of the second kind.
1	Introduction
Many problems in engineering and mechanics can be converted into an integral equations. Fredholm
integral equations as an important category of integral equations raise naturally in signal and image
processing. There are many approaches have been exploited so far for solving integral equation. Two
popular ones among them are the collocation method and Galerkin method. In (Liu, Yuzhen, Shen,
Lixin, Xu, Yuesheng Yang, Hongqi, 2016) the multiscale collocation method was proposed to solve
a Fredholom integral equation of the first kind, which models an image blurring process. As the deep
learning neural network getting more popular, a wave of interest in applying it in solving all kinds of
mathematical equations emerged recently. Lagaris (I. E. Lagaris, A. Likas, D. I. Fotiadis, 1998) used
artificial neural networks to solve ordinary differential equations and partial differential equations.
Sohrab Effati and Reza Buzhabadi used neural network to solve Fredholm integral equations of the
second kind (Effati, S., Buzhabadi, R, 2012). Since the solution was attained totally by learning,
the computational cost of this approach will be expensive. More precisely, the weights of the neural
network need to be updated each time and one has to re-compute the integral which involves the
product of the kernel and the derivative of the approximated solution with respect to the weights.
What’s more, when using a numerical method to evaluate the integral, numerical errors will be
inevitably introduced and will accumulate with increasing the number of iterations.
The main contribution of this paper is to construct an neural network for the numerical simulation of
the integral equation (1) based on a traditional collocation method. Guaranteed by the Weierstrass
theorem, the polynomials are dense in L2 on a compact set. We propose to project the solution
of the integral equation onto the space of polynomials first, which coincides with the classical ap-
proach, then we use an neural network to train coefficients which can be treated as the weights of
the NN model such that the mean square error between the left hand side and the right hand side of
the integral equation (1) at the training set is minimized. This approach overcomes the problem of
traditional collocation method which requires the number of basis identical to the number of collo-
cation points in order to have a square matrix after the discretization. Furthermore, for the integrals
which involve the product of the kernel function and the basis functions, our approach only needs to
evaluate them once as long as the number of the basis is fixed. We save the results of these integrals
in a matrix which will act as evaluations of our activation function at the input values.
1
Under review as a conference paper at ICLR 2022
We organize this paper in four sections. In section 2, we present the formulation of the NN-
Collocation method for the second kind FIEs. Numerical experiments are provided in Section 3
to demonstrate the performance of the proposed method.
2	NN-Collocation formulations
In this work, we consider an integral equation taking a form of
u(χ) + μ	k(χ, y)u(y) dy = f(χ), χ, y ∈ S ⊂ Rl,
S
(1)
where x = (x1, x2, . . . , xl), y = (y1, y2, . . . , yl), f is known, and u is unknown. For simplicity, we
denote
K(u) = k(x, y)u(y) dy,
s
which is called the integral operator. With this notation, the above integral equation can be rewritten
as:
(I + μK)u = f.	(2)
where I is the identity operator. We now describe the traditional collocation method for solving
integral equation (1), For n ∈ N, the collocation method seeks vectors wn := [wj : j ∈ Zn] such
that:
un =	wjψj	(3)
j∈Zn
is the solution of
(I + μK)un(xi) = f(xi), i ∈ Zm	(4)
where Sm = {xi : i ∈ Zm } is a finite subset of S, ψj is the basis function of the projection space.
The equivalent system of equation form can be written as:
(En + μKn)Wn = fn
(5)
where
En = {Eij , i ∈ Zm , j ∈ Zn },
Kn = {Kij , i ∈ Zm, j ∈ Zn },
fn = {fi , i ∈ Zm},
with
Eij = ψj (xi), Kij
(xi, y)ψj (y) dy
and fi
f(xi).
s
The NN-Collocation method based on this idea, try to minimize the cost function which is formu-
lated by the means square error with L2 regularization:
C = ɪ X Ln(f(Xi, Wn), fi) + λ∣∣Wnk2	(6)
m
i∈Zm
with the loss function:
_	, O ,	. .	, O ,	、	一、C
Ln(f(Xi, Wn), fi) = (f(Xi, Wn)- fi)	(7)
where f (xi, Wn) := (I + μK)un(xi), λ is the regularization parameter, kwnk2 represents the
Euclidean norm of Wn . With the notation used in (5), the loss function (7) of our NN-Collocation
model can be rewritten as:
Ln(f(Xi, Wn),fi) = I X (Eij + μKij )wj - fi j	(8)
j∈Zn
In other words, we use {(xi, fi)}im=0 as our training data set to implement a learning process for
a trail model such that the MSE between the predicted value of operator I + μK and exact value
of this operator at the input data is minimized while emphasizing on maximizing margin to avoid
overfitting problem.
2
Under review as a conference paper at ICLR 2022
Input	Hidden	Ouput
layer	layer	layer
φ1(xi)
The above process can be interpreted as a feed-forward network with one hidden layer. The weights
between the input layer and the hidden layer of the network are all set to 1, and the activation
function on the jth neuron of the hidden layer is φj(x) = (I + μK)ψj (x), the weights between
hidden layer and output layer are wi’s. The network is illustrated in Fig. 1.
We intend to find wn by using the gradient descent algorithm. To this end, we need to calculate the
∂∂Wn from (8) as follows
∂L
=--- = 2( ɪ2 (Eij + μKij )wj - Zi)(Eij + μKij ).	(9)
∂wj
j∈Zn
The most computing cost is from calculating the Kij , which involves evaluating integrals, but only
once. We save these results in a matrix which act as the evaluations of the activation functions at the
training data set. In this paper, we use the Gauss-quadrature for the evaluation of Kij . The whole
procedure is summarized in Algorithm 1.
Algorithm 1 Our Algorithm
1:	Input regularization parameter λ, number of training data m, training set Sm := {xi : i ∈ Zm}.
Initialize the weights {wj : j ∈ Zn} by Xavier initialization ( Glorot, X. amp; Bengio, Y, 2010).
2:	Compute φj∙ (Xi) := Eij + μKj, and store the result in matrix A with Aij = φj- (xi), i ∈
Zm , j ∈ Zn.
3:	ComPUte ∂Wj = m1 Ei∈Zm Iwj + 2λwj accordingto chain rule.
4:	Update the weights wj with the Adam optimizer.
5:	Compute the approximation of (3) by using the optimal weights {wi : i ∈ Zn }.
3 Results
We give four numerical examples to illustrate our model’s performance, the first two models are
just picked randomly, the last two having polynomials as exact solution. Our projection space are
polynomial space with shifted Legendre polynomials on interval [0, 1] of degree up to n as basis.
We picked m = 2000 numbers equally on interval [0, 1] as our training set. Testing set are equally
spaced m = 1800 numbers on interval [0.01, 0.98].
Example 1.	Consider the following linear FIE of the second kind
u(x) -
Z 2ex+y u(y) dy =
0
ex
3
Under review as a conference paper at ICLR 2022
Table 1: The MSE error for Example 1.
n	MSE on Training set	MSE on Testing set
1-	1.09689663e-06	9.59122925e-07
^4-	1.27886941e-07	1.12720023e-07
	1.8071884e-08	2.2415795e-08
-6-	1.0746993e-09	2.98199195e-09
~~	1.72614602e-09	1.53534553e-09
x
with the exact solution u(χ) = 2-^.
Our solution has order of accuracy around e - 09 for n = 6 after 20000 iteration with optimal
weights
wn = [-0.23858496, -0.15568655, -0.11762842, 0.02070347, -0.0158611, 0.00271201].
Figure 2(a) shows the exact solution, approximated solution, Figure 2(b) shows the accuracy, which
are measured by Mean Square Error (MSE). The numerical results can be found in Table 1.
Figure 2: Example 1: (a) Exact solution vs NN-Collocation approximate solution with n=6; (b)
MSE between the exact solution and the approximation solution.
Example 2.	Consider the following linear FIE of the second kind given by ( M.H. Reihani, Z.Abadi,
2007)
u(x) + 3 / e2x-5u(y) dy = e2x+ 3,
with the exact solution u(x) = e2x.
Our solution has order of accuracy around e - 11 for n = 7 after 30000 iteration with optimal
weights
wn = [1.87982320, 2.75563720, 1.95524998, 5.05200551e - 01,
2.75259395e - 01, 5.81048575e - 04, 1.72820486e - 02].
Figure 3(a) shows the exact solution, approximated solution, Figure 3(b) shows the MSE. The nu-
merical results can be found in Table 2.
Example 3.	Consider the following linear FIE of the second kind
u(x) + Z x(exy
0
- 1)u(y) dy = ex - x,
with the exact solution u(x) = 1.
4
Under review as a conference paper at ICLR 2022
(a)
Figure 3: Example 2: (a) Exact solution vs NN-Collocation approximate solution with n=6; (b)
MSE between the exact solution and the approximation solution.
(b)
Table 2: The MSE error for Example 2.
n	MSE on Training set	MSE on Testing set
	000474284	0.00475947
^4-	7.23014255e-05	7.26806487e-05
	8.22319408e-07	6.70043825e-07
-6-	8.39516517e-08	1.02247121e-07
Q~~	2.89689923e-11	2.40565254e-11
Our solution has order of accuracy around e - 15 for n = 4 after 30000 iteration with optimal
weights
wn = [9.99998676e - 01, 2.92296511e - 06, -2.37639142e - 06, 9.37088253e - 07]
And order of accuracy around e - 16 for n = 3 after 50000 iteration with optimal weights
un = [9.99999927e - 01, 1.43643199e - 07, -8.92090066e - 08]
Figure 4(a) shows the exact solution, approximated solution, Figure 4(b) shows the accuracy mea-
sured in MSE. The numerical results can be found in Table 3.
(a)
NN-Collocation approximate solution with n=6; (b)
Figure 4: Example 3: (a) Exact solution vs
MSE between the exact solution and the approximation solution.
5
Under review as a conference paper at ICLR 2022
Table 3: The MSE error for Example 3.
n	MSE on Training set	MSE on Testing set
	1.09178389e-16	9.83884687e-17
^4-	2.72825599e-15	2.19793864e-15
	8.72100376e-12	7.84314389e-12
^6-	7.75215323e-11	6.39991518e-11
~~	8.23544625e-11	9.99376595e-11
Table 4: The MSE error for Example 4.
a	n	MSE on Training set	MSE on Testing set
-0-		2.00812521e-14	1.7978776e-14
-0-	^4-	2.20219156e-12	2.10087744e-12
		1.25071358e-15	1.12903254e-15
"3~~	^7~~	3.17235264e-14	2.71073112e-14
Example 4.	Consider the following linear FIE of the second kind
u(x) -	9xyu(y) dy = ax2 - 4x2,
0
with the exact solution u(x) = (a - 4)x(x - 8).
Our solution has order of accuracy around e - 14 for a = 0, n = 3 after 30000 iteration, with
optimal weights
wn = [-1.33333235, 4.49999805, -2.66666542]
And order of accuracy around e - 15 for a = 5, n = 3 after 30000 iteration, with optimal weights
wn = [0.33333309, -1.12499951, 0.66666636]
Figure 5(a) and Figure 6(a) show the exact and approximated solution for a = 0, n = 3 and a =
5, n = 3, respectively. Figure 5(b) and Figure 6(b) show the corresponding accuracy measured in
MSE. The numerical results can be found in Table 4.
Figure 5: Example 4: (a) Exact solution vs NN-Collocation approximate solution with a = 0 and
n = 3; (b) MSE between the exact solution and the approximation solution with a = 0 and n = 3.
4 Conclusion
This novel approach for solving linear Fredholm integral equations of the second kind combines the
traditional Collocation method and the neuron network learning model, but with different activation
6
Under review as a conference paper at ICLR 2022
Figure 6: Example 4: Exact solution vs NN-Collocation approximate solution with a=5,n=3; (b)
MSE between the exact solution and the approximation solution with a = 5, n = 3.
functions on the same layer, which differs from the classical NN model. The computational cost
is low since the involved integrals need to be evaluated once. This NN model works very well
on integral equations with polynomial solutions. It is worth pointing out that the accuracy of the
model on testing set are better than that on the training set, which means that our model is stable
and reliable. The limitation of this approach is that it only works for linear integral equation. For
nonlinear integral equation, we can also try a similar way with different activation functions, but that
would involve a large amount of computations.
References
A collocation method solving integral equation models for image restoration. Journal of Integral
Equations and Applications. 28. 263-307. 10.1216/JIE-2016-28-2-263
”Artificial neural networks for solving ordinary and partial differential equations,” in IEEE Transac-
tions on Neural Networks, vol. 9, no. 5, pp. 987-1000.
A neural network approach for solving Fredholm integral equations of the second kind. Neural
ComPUt and APPlic 21, 843-852.
”Deep Learning,” MIT Press
Understanding the difficUlty of training deeP feed-forward neUral networks. Proceedings of the Thir-
teenth International Conference on Artificial Intelligence and Statistics, in Proceedings of Ma-
chine Learning Research 9:249-256.
Rationalized Haar fUnctions method for solving Fredholm and Volterra integral eqUations, JoUrnal
of ComPUtational and APPlied Mathematics,VolUme 200,Pages 12-20,ISSN 0377-0427.
7