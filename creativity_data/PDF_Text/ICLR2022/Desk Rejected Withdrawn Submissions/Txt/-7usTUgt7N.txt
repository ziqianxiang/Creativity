Under review as a conference paper at ICLR 2022
Implicit vs Unfolded Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
It has been observed that graph neural networks (GNN) sometimes struggle to
maintain a healthy balance between the efficient modeling long-range dependencies
across nodes while avoiding unintended consequences such oversmoothed node
representations or sensitivity to spurious edges. To address this issue (among other
things), two separate strategies have recently been proposed, namely implicit and
unfolded GNNs. The former treats node representations as the fixed points of a
deep equilibrium model that can efficiently facilitate arbitrary implicit propagation
across the graph with a fixed memory footprint. In contrast, the latter involves
treating graph propagation as unfolded descent iterations as applied to some graph-
regularized energy function. While motivated differently, in this paper we carefully
quantify explicit situations where the solutions they produce are equivalent and
others where their properties sharply diverge. This includes the analysis of conver-
gence, representational capacity, and interpretability. In support of this analysis,
we also provide empirical head-to-head comparisons across multiple synthetic and
public real-world node classification benchmarks.
1	Introduction
Given graph data with node features, graph neural networks (GNNs) represent an effective way
of exploiting relationships among these features to predict labeled quantities of interest, e.g., node
classification (Wu et al., 2020; Zhou et al., 2018). In particular, each layer of a message-passing
GNN is constructed by bundling a graph propagation step with an aggregation function such that
information can be shared between neighboring nodes to an extent determined by network depth.
(Kipf & Welling, 2017; Hamilton et al., 2017; Kearnes et al., 2016; Velickovic et al., 2018).
For sparsely-labeled graphs, or graphs with entity relations reflecting long-range dependencies, it is
often desirable to propagate signals arbitrary distances across nodes, but without expensive memory
requirements during training, oversmoothing effects (Oono & Suzuki, 2020; Li et al., 2018), or
disproportionate sensitivity to bad/spurious edges (Zhu et al., 2020b;a; Zugner et al., 2019; Zugner &
Gunnemann, 2019). To address these issues, at least in part, two distinct strategies have recently been
proposed. First, the framework of implicit deep learning (Bai et al., 2019; El Ghaoui et al., 2020) has
be applied to producing supervised node embeddings that satisfy an equilibrium criteria instantiated
through a graph-dependent fixed-point equation. The resulting so-called implicit GNN (IGNN)
pipeline (Gu et al., 2020), and related antecedents (Dai et al., 2018; Gallicchio & Micheli, 2020),
mimics the behavior of a GNN model with arbitrary depth for handling long-range dependencies, but
is nonetheless trainable via a fixed memory budget and robust against oversmoothing effects.
Secondly, unfolded GNN (UGNN) architectures are formed via graph propagation layers patterned
after the unfolded descent iterations of some graph-regularized energy function (Chen & Eldar, 2021;
Liu et al., 2021; Ma et al., 2020; Pan et al., 2021; Zhang et al., 2020; Zhu et al., 2021). In this way, the
node embeddings at each UGNN layer can be interpreted as increasingly refined approximations to an
interpretable energy minimizer, which can be nested within a bi-level optimization framework (Wang
et al., 2016) for supervised training akin to IGNN. More broadly, similar unfolding strategies have
previously been adopted for designing a variety of new deep architectures or interpreting existing
ones (Gregor & LeCun, 2010; Hershey et al., 2014; Sprechmann et al., 2015; He et al., 2017).
As such, IGNN and UGNN are closely related in the sense that their node embeddings are intentionally
aligned with a meta-criterion: either a fixed-point for IGNN or an energy minimizer for UGNN, where
in both cases a useful inductive bias is introduced to address similar desiderata. And yet despite these
commonalities, there has thus far been no systematic examination of the meaningful similarities and
differences. We take a first step in this direction via the following workflow. First, after introducing
the basic setup and notation in Section 2, we overview the IGNN framework in Section 3, including its
1
Under review as a conference paper at ICLR 2022
attractive reliance on memory-efficient implicit differentiation and existing convergence results. Next,
Section 4 introduces a general-purpose UGNN framework that encompasses a variety of existing
unfolded models as special cases, including models with graph attention, broad graph propagation
operators, and nonlinear activation functions. Sections 5 and 6 provide comparative analysis of
the relative strengths and weaknesses of IGNN and UGNN models with respect to convergence
guarantees and model expressiveness. We conclude with empirical comparisons in Section 7. Overall,
our contributions can be summarized as follows:
•	Although the original conceptions are unique, we consider a sufficiently broad design space of
IGNN and UGNN models such that practically-relevant, interpretable regions of exact overlap can
be established, analyzed, and contrasted with key areas of nonconformity.
•	Within this framework, we compare the relative ability of IGNNs and UGNNs to converge
to optimal (or locally-optimal) solutions per various model-specific design criteria. Among
other things, this analysis reveals that IGNNs enjoy an advantage in that (unlike UGNNs) their
implicit layers do not unavoidably induce symmetric propagation weights. In contrast, we show
that UGNNs are more flexible by accommodating a broader class of robust nonlinear graph
propagation operators while still guaranteeing at least local convergence.
•	We investigate the consequences of symmetric (layer-tied) UGNN propagation weights. In
particular, we prove that with linear activations, a UGNN can reproduce any IGNN representation,
and define sufficient conditions for equivalency in broader regimes. We also show that UGNN
layers with symmetric, layer-tied weights can exactly mimic arbitrary graph convolutional network
(GCN) models (Kipf & Welling, 2017) characterized by asymmetric, layer-specific weights without
introducing any additional parameters or significant complexity. Collectively, these results suggest
that the weight symmetry enforced by UGNN models may not be overly restrictive in practice.
•	Empirically, we provide the comprehensive, head-to-head comparisons between equivalently-sized
IGNN and UGNN models while evaluating computational complexity and memory costs. These
results also serve to complement our analytical findings.
2	Basic Setup
Consider a graph G = {V, E}, with n = |V| nodes and m = |E| edges. We define L ∈ Rn×n as the
Laplacian of G, meaning L = D - A = B>B, where D and A are degree and adjacency matrices
respectively, while B ∈ Rm×n is an incidence matrix. We also let Ae = A + I (i.e., A with self loops)
and denote D as the corresponding degree matrix.
Both IGNNs and UGNNs incorporate graph structure via optimized embeddings Y * ∈ Rn×d that
are a function of some adjustable weights W, i.e., Y * ≡ Y * (W) (for simplicity of notation, We will
frequently omit including this dependency on W), where by design ∂Y*(W)∕∂W is computable,
either implicitly (IGNN) or explicitly (UGNN). We may then insert these embeddings within an
application-specific meta-loss given by
'θ (θ, W) , pn= 1 D(g [y*(W); θ],ti),	(1)
where g : Rd → Rc is some differentiable node-wise function with parameters θ and c-dimensional
output tasked with predicting ground-truth node-wise targets ti ∈ Rc. Additionally, yi* (W) is the
i-th row of Y*(W), n0 < n is the number of labeled nodes (we assume w.l.o.g. that the first n0 nodes
are labeled), and D is a discriminator function, e.g., cross-entropy for classification, squared error for
regression, etc. Given that Y* (W) is differentiable by construction, we can optimize `θ (θ, W) via
gradient descent to obtain our final predictive model.
At a conceptual level then, the only difference between IGNNs and UGNNs is in how the correspond-
ing optimal embeddings Y* (W) are motivated and computed. We introduce the specifics of each
option in the following two sections.
3	Overview of Implicit GNNs
IGNN models are predicated on the fixed-point update
Y (k+1) =σ hPY(k)Wp+f(X;Wx)i ,	(2)
2
Under review as a conference paper at ICLR 2022
where Y (k) are node embeddings after the k-th iteration, W = {Wp , Wx } is a set of two weight
matrices, P ∈ Rn×n is an arbitrary graph propagation operator (e.g., P = A), and σ is a nonlinear
activation function. Additionally, f : Rn×d0 → Rn×d is a base predictor function that converts the
initial d0-dimensional node features X ∈ Rn×d0 into d-dimensional candidate embeddings, e.g.,
f (X; Wx) =XWxorf(X;Wx) =MLP[X;Wx].
Now assume that σ is a differentiable component-wise non-expansive (CONE) mapping as specified
in (Gu et al., 2020). This condition stipulates that σ applies the same function individually to each
element of its input, and that this function satisfies kx - yk ≥ kσ(x) - σ(y)k for all {x, y} ∈ R2
(with some abuse of notation, we will overload the definition of σ when the meaning is clear from
context). Furthermore, We assume that the weights Wp are such that λpf(∣P 0 Wp|) < 1, where | ∙ |
denotes the element-wise absolute value and λpf refers to the Peron-Frobenius eigenvalue.1
Under these conditions, it has been shown in (GU et al., 2020) that limk→∞ Y (k) = Y *, where Y *
satisfies the fixed-point equation Y * = σ [PY *Wp + f (X; Wχ)]. Therefore, as an IGNN forward
pass we can iterate (2) K times, where K is sufficiently large, to compute node embeddings Y(K) ≈
Y* for use within the meta-loss from (1). For the IGNN backward pass, implicit differentiation
methods (Bai et al., 2019; El Ghaoui et al., 2020), carefully tailored for handling graph-based models
(Gu et al., 2020), can but used to compute gradients of Y* with respect to Wp and Wx . Critically,
this implicit technique does not require storing each intermediate representation {Y (k)}kK=1 from
the forward pass, and hence is quite memory efficient even if K is arbitrarily large. As K can be
viewed as quantifying the extent of signal propagation across the graph, IGNNs can naturally capture
long-range dependencies with a K-independent memory budget unlike more traditional techniques.
4	A General-Purpose Unfolded GNN Framework
In this section we will first introduce a general-purpose energy function that, when reduced with
simplifying assumptions, decomposes into various interpretable special cases that have been proposed
in the literature, both as the basis for new UGNN models as well as motivation for canonical GNN
architectures. Later, will describe the generic proximal gradient descent iterations designed to
optimize this energy. By construction, these iterations unfold in one-to-one correspondence with the
UGNN model layers of each respective architecture under consideration.
4.1	Flexible Energy Function Design
Unlike IGNN models, the starting point of UGNNs is an energy function. In order to accommodate a
broad variety of existing graph-regularized objectives and ultimately GNN architectures as special
cases, we propose the rather general form
mn
'Y(Y; W,f,ρ, Be, φ) , kY — f(X; Wx)kWf +Xρ ([BYWpY>B>]) +X φ (yi; Wφ),⑶
i=1	ii i=1
where W = {Wx, Wf , Wp, Wφ} is a set of four weight matrices, ρ : R → R is a differentiable
function, and B ∈ Rm×n is a function of A that can be viewed as a generalized incidence matrix.
Furthermore, φ : Rd → R is an arbitrary function (possibly non-smooth) of the node embeddings
that is bounded from below, and the weighted norm ∣∣∙∣∣ wis defined such that kX k2W = tr[X>WX].
The above loss is composed of three terms: (i) A quadratic penalty on deviations of the embeddings
Y from the node-wise base model f (X; Wx), (ii) a (possibly) non-convex graph-aware penalty that
favors smoothness across edges (see illustrative special cases below), and (iii) an arbitrary function of
each embedding that is independent of both the graph structure A and the original node features X .
As a representative special case, if Be = B , then the second term from (3) satisfies
m
XρhBeYWpY>Be>iii = X ρyi-yj2Wp,	(4)
i=1	{i,j}∈E
which penalizes deviations between the embeddings of two nodes sharing an edge. In particular, if ρ
is chosen to be concave and non-decreasing on [0, ∞), the resulting regularization favors robustness
1The Peron-Frobenius (PF) eigenvalue is a real, non-negative eigenvalue that exists for all square, non-
negative matrices and produces the largest modulus.
3
Under review as a conference paper at ICLR 2022
to bad edges, and could be useful for handling heterophily graphs or adversarial attacks (Yang et al.,
2021). And if in addition to the assumption from (4), φ is set to zero, ρ is an identity mapping,
Wf = I, and Wp = λI with λ > 0, then (3) collapses to the more familiar loss
'y(Y; Wχ,f) , kY - f(X； Wχ)kF + λtr [Y>LY] ,	(5)
as originally proposed in (Zhou et al., 2004) to smooth base predictions from f (X; Wx) using a
quadratic, graph-aware regularization factor tr Y> LY] = P{i,j}∈E yi - yj 2. This construction
has also formed the basis of a wide variety of work linking different GNN architectures (Ma et al.,
2020; Pan et al., 2021; Zhang et al., 2020; Zhu et al., 2021); more details to follow in Section 4.2.
Similarly, if we allow for broader choices of B 6= B with L , B>B = π(A) for some function
π : Sn → Sn over the space of n-dimensional PSD matrices Sn , then (5) can be generalized by
swapping L for this L as proposed in (Ioannidis et al., 2018). Candidates for L include normalized
graph Laplacians (Von Luxburg, 2007) and various diffusion kernels (Klicpera et al., 2019b) designed
according to application-specific requirements, e.g., graph signal denoising.
4.2 Descent Iterations That Form UGNN Layers
We now derive descent iterations for the general loss from the previous section (and later more
interpretable special cases) that will be mapped to UGNN layers. For this purpose, let U(k) denote a
single gradient descent step along (3), excluding the possibly non-differentiable φ-dependent term,
and evaluated at some candidate point Y(k). We may compute such an abridged gradient step as
U(k) = Y(k) - a hBe>Γ(k)BeY(k) (Wp + Wp>) + Y(k) (Wf + W>) - f (X; W)i ,	(6)
where α is the step size and Γ(k) is a diagonal matrix with i-th diagonal element given by
Yik) = dρ(z)∣	.	⑺
∂z	z=diaghBeY (k)Wp(Y (k))>Be>i
We then have the following:
Lemma 4.1 Ifρ has Lipschitz continuous gradients, then the proximal gradient update
Y(k+1) = proxφ (U(k)) , argmYn〉U(k) — YkF + X φ ®i； Wφ),	⑻
i
is guaranteed to satisfy 'γ(Y(k+1); W,f,ρ,B,φ) ≤ 'γ(Y(k); W,f,ρ,B,φ) forany α ∈ (0,1/L],
where L is the Lipschitz constant for gradients of (3) w.r.t. Y, excluding the non-smooth φ term.
The function proxφ : Rd → Rd is known as the proximal operator2 associated with φ (separably
extended across all n nodes in the graph), and the proof follows from basic properties of gradient
descent (Bertsekas, 1999) and proximal methods (Combettes & Pesquet, 2011); see Appendix.
Generally speaking, the mapping Y(k) 7→ U(k) from (6) can be viewed as a (possibly nonlinear)
graph filter, while proxφ from (8) serves as an activation function applied to the embedding of each
node. Collectively then, Y (k+1) = proxφ U(k) provides a flexible template for UGNN layers that
naturally reduces to common GNN architectures per various design choices. And analogous to IGNN,
We can execute K UGNN steps to approximate some Y*, which could be a fixed-point, stationary
point, or global optimum; more on this in Sections 4.3 and 5 below.
For example, consider the selection Pi φ (yi; Wφ) = Pi,j I∞[yij < 0], where yij is the (i, j)-th
element of Y and I∞ is an indicator function that assigns an infinite penalty to any yij < 0. The
proximal operator then becomes proxφ(U) = ReLU(U), i.e., a ReLU function that element-wise sets
negative values to zero. If we add this φ-dependent term to (5) (as a special case of (3)), the resulting
update becomes
Y (k+1) = proxφ (U(k) = ReLU (Y (k) -α h(λL + I) Y(k) - f(X;W)i .	(9)
2If (8) happens to have multiple solutions, then the proximal operator selects one of them so as to remain a
proper deterministic function of its argument, which is relevant for forming later connections with IGNN.
4
Under review as a conference paper at ICLR 2022
And based on observations from (Ma et al., 2020; Pan et al., 2021; Zhang et al., 2020; Zhu et al.,
2021), when we initialize with Y (0) = f (X; W) = XW and apply simple reparameterizations, the
D-1/2AD-1/2XW from (Kipf
&	Welling, 2017). Subsequent iterations are no longer equivalent, although the inherent differences
(e.g., the skip connections from the input layer) are useful for avoiding oversmoothing effects that
can at times hamper deep GCN models (Oono & Suzuki, 2020; Li et al., 2018; Rong et al., 2020).
Additionally, the widely-used APPNP architecture (Klicpera et al., 2019a) is also a special case of (9)
When the ReLU operator is removed (i.e., φ is zero), α = ɪ^, and L is changed to the symmetric
normalized Laplacian (Von Luxburg, 2007). And as a final representative example, if we adopt a
nonlinear choice for ρ, then L in (9) Will be replaced by L(k) , B>Γ(k) B, Where Γ(k) rescales graph
edges. As discussed in (Yang et al., 2021), this instantiates a form of graph attention, Whereby the
attention Weights produced by a concave, non-decreasing ρ add robustness to spurious edges.
4	.3 UGNN Fixed Points and Connections with IGNN
While UGNN layers are formed from the descent steps of a graph-regularized energy, for a more
direct comparison With IGNN, it is illuminating to consider the situation Where the update Y (k+1) =
proxφ U (k) is iterated With k becoming sufficiently large. In this case, We may ideally reach a fixed
point Y * that satisfies
Y *	= proxφ (Y * - α [B>Γ*By * (% + W>) + Y * (Wf + W>) - f (X; W)])
=proxφ(Y* [I - αWf - αWp1] + α [l - B>Γ*B] Y*Wp + αf (X; W)) ,	(10)
Where Wps , Wp + Wp> and Wfs , Wf + Wf> are symmetric Weight matrices, and Γ* is defined
analogously to (7). It now follows that if Ws = 11 - Ws,α = 1 and Γ* = I (i.e., P is an identity
mapping), and we define the propagation operator as P = I - B >B ≡ I - L, then (10) reduces to
Y* = proxφ [PY*Wps + f (X; W) .	(11)
This expression is exactly equivalent to the IGNN fixed point from Section 3 when we set σ to the
proximal operator of φ and we restrict Wp to be symmetric. For direct head-to-head comparisons
then, it is instructive to consider what CONE activation functions σ can actually be expressed as the
proximal operator of some penalty φ. In this regard we have the following:
Lemma 4.2 A continuous CONE function σ : R → R can be expressed as the proximal operator of
some function φ iff σ is also non-decreasing.
The proof follows from results in (Gribonval & Nikolova, 2020). This result implies that, at least
with respect to allowances for decreasing activation functions σ, IGNN is more flexible than UGNN.
However, the relative flexibility of IGNN vs UGNN fixed points becomes much more nuanced once
we take into account convergence considerations. For example, the proximal gradient iterations from
Section 4.2 do not require that proxφ is non-expansive or CONE to guarantee descent, although it
thus far remains ambiguous how this relates to obtainable fixed points of UGNN. To this end, in the
next section we will evaluate when such fixed points exist, what properties they have, convergence
conditions for reaching them, and importantly for present purposes, how UGNN fixed points relate to
their IGNN counterparts.
5	Convergence Comparisons
In this section we first detail situations whereby convergence to a unique fixed point, that also may
at times correspond with a unique UGNN global minimum, can be established. In these situations
IGNN facilitates stronger guarantees in terms of broader choices for Wp and σ . Later, we examine
alternative scenarios whereby UGNN has a distinct advantage with respect to convergence guarantees
to potentially local solutions.
5
Under review as a conference paper at ICLR 2022
5.1	Convergence to Unique Global Solutions
To facilitate the most direct, head-to-head comparison between IGNN and UGNN, we consider a
restricted form of the UGNN energy from (3). In particular, let
n
'Y(Y; W,f,φ)，kY - f(X; Wχ)kWf + tr [y>LYWp∖ + Xφ (yi),	(12)
i=1
which is obtainable from (3) when ρ is an identity mapping and φ is assumed to have no trainable
parameters. We also define Σ，10 Wf + L 0 Ws, With minimum and maximum eigenvalues given
by λmin(Σ) and λmax(Σ) respectively.
Theorem 5.1 If proxφ is non-expansive and λmin(Σ) > 0, then (12) has a unique global minimum.
Additionally, starting from any initialization Y(0), the iterations Y (k+1) = proxφ U(k) applied to
(12) with α ∈ (0,1∕λmaχ(Σ)] are guaranteed to converge to this global minimum as k → ∞.
Corollary 5.1.1 For any non-decreasing CONE function σ and symmetric Wp ≡ Wps satisfying
kWps k2 < kP k2-1 , there will exist a function φ such that the fixed point of (2) is the unique global
minimum of (12) when Wf is chosen such that Wfs = I - Wps and P = I - L.
Lemma 5.2 For any non-expansive σ (not necessarily element-wise) and Wp (not necessarily sym-
metric) satisfying kWpk2 < kP k2-1, iterating (2) will converge to a unique fixed point.3
Hence from Theorem 5.1 and Corollary 5.1.1, ifWe are Willing to accept symmetric graph propagation
Weights Wps (and a non-decreasing σ), UGNN enjoys the same convergence guarantee as IGNN, but
With the added benefit of an interpretable underlying energy function. In contrast, When Wp is not
symmetric as in Lemma 5.2, We can only guarantee a unique IGNN fixed point, but We are no longer
able to establish an association With a specific UGNN energy. In fact, it does not seem likely that any
familiar/interpretable functional form can even possibly exist to underlie such a fixed point When Wp
is not symmetric. This is largely because of the folloWing simple result:
Lemma 5.3 There does not exist any second-order smooth function h : Rn×d → R such that
∂h(Y; W)∕∂Y = YW for all (asymmetric) matrices W ∈ Rd×d with d > 1 and n ≥ 1.
And by continuity arguments, a similar result Will apply to many non-smooth functions. Consequently,
With asymmetric Weights there is no obvious Way to associate fixed points With stationary points as
We have done for UGNN.
5.2	Broader Convergence Regimes
As We move to alternative energy functions With nonlinear dependencies on the graph, e.g., ρ not
equal to identity, meaningful (albeit possibly Weaker) convergence properties for UGNN can still be
established. In particular, We consider convergence to local minima, or more generally, stationary
points of the underlying objective. HoWever, because (3) may be non-convex and non-smooth, We
must precisely define an applicable notion of stationarity.
While various possibilities exist, for present purposes we define Y* as a stationary point of (3) if
0∈
∂f ['Y (Y*; W,
f, ρ, Be , φ)
,where ∂f denotes the Frechet subdifferential. The latter generalizes
the standard subdifferential as defined for convex functions, to non-convex cases. More formally, the
Frechet subdifferential (Li et al., 2020) of some function h(Y) is defined as the set
∂F [h(Y)]	= {S ：	h(Y)	≥ h(Z)	+ tr	[S>	(Y - Z)]	+ o (kY -	ZkF)	∀Y},	(⑶
which is equivalent to the gradient when h is differentiable and the regular subdifferential when h is
convex. Based on this definition, we have the following:
Theorem 5.4 Assume that Wp and Wf are PSD, φ is continuous with a non-empty Fenchel sub-
differential for all Y4 and ρ is a concave, non-decreasing function with Lipschitz-continuous gra-
dients. Additionally, starting from any initialization Y(0), let {Y (k) }k∞=0 denote a sequence gen-
erated by Y (k+1) = proxφ U(k) with step size parameter α ∈ (0, 1/L]. Then all accumulation
3An analogous result has been shown in (Gu et al., 2020), but restricted to CONE mappings (not arbitrary
non-expansive mappings) and with a dependency on the less familiar Peron-Frobenius eigenvalue; see Section 3.
4For minor technical reasons, we also assume φ is such that limkY k→∞CY(Y; W,f,P,B,φ) = ∞.
6
Under review as a conference paper at ICLR 2022
points of {Y (k)}∞=o are stationary points of(3). Furthermore, limk→∞ 'γ (Y (k); W, f, ρ, B,φ)
'y (Y *; W, f, ρ, B, φ) for some stationary point Y *.
The proof is based on Zangwill’s global convergence theorem (Luenberger, 1984) and additional
results from (Sriperumbudur & Lanckriet, 2009); see Appendix for further details.
Corollary 5.4.1 If in addition to the conditions from Theorem 5.4, φ is non-expansive and ρ is
[(∙)2]
chosen such that the composite function ρ
is convex, then limk→∞ 'γ(Y(k); W, f, ρ, Be, φ)
'y (Y*; W, f, ρ, B, φ), where Y* is a global minimizer of(3).
These results both apply to situations where the k-dependent UGNN graph propagation operator
P(k) , I - Be>Γ(k)Be is nonlinear by virtue of the Y (k)-dependency of Γ(k), i.e., P (k)Y (k)Wps 6=
P Y (k)Wps for any fixed P. In contrast, it is unknown (and difficult to determine) if general nonlinear
alternatives to PYWp in (2) will converge.
5.3	Recap of Relative IGNN and UGNN Flexibility
In terms of model expressiveness, the advantage of the IGNN framework is two-fold: (i) it allows for
asymmetric weights Wp while still providing a strong convergence guarantee, and (ii) it allows for
decreasing activation functions. However, the latter is likely much less relevant in practice, as most
deep models adopt some form of non-decreasing activation anyway, e.g., ReLU, etc.
In contrast, UGNN models are more flexible than IGNN in their accommodation of: (i) nonlinear
graph propagation through the graph attention mechanism described in Section 4.2, and (ii) expansive
proximal operators. While the latter may seem like a mere technicality, expansive proximal operators
actually undergird a variety of popular sparsity shrinkage penalties, which have been proposed for
integration with GNN models (Zheng et al., 2021). For example, the `0 norm and related non-convex
regularization factors (Chen et al., 2017; Fan & Li, 2001) are expansive and can be useful for favoring
parsimonious node embeddings. And overall, with only mild assumptions, UGNN at least guarantees
cost function descent across a broad class of models per Lemma 4.1, with even convergence to
stationary points possible in relevant situations from Theorem 5.4 that IGNN cannot emulate.
6	How Limiting Are Symmetric (Layer-Tied) Propagation Weights ?
Previously we observed that the primary advantage IGNN has over UGNN, at least in terms of model
expressiveness, is that IGNN places no restriction that the propagation weight matrix Wp need be
symmetric, although both models ultimately involve an architecture with layer-tied weights unlike
typical message-passing models. e.g. GCN. To better understand the implications of this distinction,
we will now explore the expressiveness of GNN models with symmetric, layer-tied weights.
6.1	Fixed-Point Equivalency with Symmetric and Asymmetric Weights
In this section we examine situations whereby UGNN models are able to reproduce, up to some
inconsequential transform, the fixed points of an IGNN, even when the latter includes an asymmetric
propagation weight matrix Wp . However, because it is challenging to analyze general situations with
arbitrary activation functions, we first consider the case where σ is an identity mapping; we also
assume that f(X; Wx) = XWx as adopted in (Gu et al., 2020). We then have the following:
Theorem 6.1 For any Wp ∈ Rd×d, Wx ∈ Rd0×d, X ∈ Rn×d0, and P that admit a unique IGNN
fixed point Y* = PY*Wp + XWχ, there exists a Y0 ∈ Rn×d0, WX ∈ Rd0×d0, right-invertible
transform T ∈ Rd×d0, and symmetric Wps ∈ Rd0 ×d0, such that
Y0T = PY0TWs + XWx, with ∣∣Y0 - Y*k < e, ∀e > 0.	(14)
This result implies that an UGNN model with φ = 0 can produce node-wise embeddings Y = Y0T
capable of matching any IGNN fixed-point with arbitrary precision up to some transform T. And
given that T can be absorbed into the meta-loss output layer g from (1) (which is often a linear layer
anyway), the distinction is negligible.
7
Under review as a conference paper at ICLR 2022
Proceeding further, if we allow for nonlinear activation functions σ, we may then consider a more
general, parameterized family of penalty functions φ(y; Wφ) such that the resulting proximal operator
increases our chances of finding a UGNN fixed point that aligns with IGNN. However, some care must
be taken to control UGNN model capacity to reduce the possibility of trivial, degenerate alignments.
To this end, we consider proximal operators in the set
Sσ = {proxφ : y → Gσ(Cy)∣with {G, C, } chosen such that proxφ is proximal operator.}, (15)
where the matrices G and C can be aggregated into Wφ . We then derive sufficient conditions under
which UGNN has optimal solutions equivalent to IGNN fixed points (it remains an open question if a
necessary condition can be similarly derived). See Appendix for details.
6.2	UGNN Capacity to Match Canonical GCN Architectures
The previous section considered the alignment of fixed points, which are obtainable after executing
potentially infinite graph propagation steps, from models with and without symmetric propagation
weights. Somewhat differently, this section investigates analogous issues in the context of the
possible embeddings obtainable after k steps of graph propagation. Specifically, we evaluate the
expressiveness of a canonical GCN model with arbitrary, layer-independent weights versus a UGNN
model structured so as to effectively maintain an equivalent capacity (up to a right-invertible linear
transformation as discussed above).
Theorem 6.2 Let YG(kC+N1) = σ P YG(kC)NWp(k) + βYG(kC)N denote the k-th layer of a GCN, where
Wp(k) is a layer-dependent weight matrix, σ = proxφ for some function φ, and β ∈ {0, 1} determines
whether or not a residual connection is included. Then with α = 1, ρ an identity mapping, and
f(X; Wx) set to zero, there will always exist a right-invertible T, initialization Y (0), and symmetric
weights Wps and Wfs such that the k-th iteration step computed via (8) satisfies Y (k+1) T = YG(kC+N1).
Given the close association between GCNs and certain UGNN models, Theorem 6.2 suggests that, at
least in principle, symmetric layer-tied weights may not be prohibitively restrictive in the finite layer
regime. And as can be observed from the proof, this equivalency result is possible using a matching
parameter budget instantiated through an expanded hidden dimension but constrained via block-
sparse weight matrices Wps and Wfs. Of course in practice we cannot always guarantee that reliance
on symmetric weights will not have unintended consequences that may in certain circumstances
adversely impact performance.
7	Experiments
As prior work has already showcased the value of IGNN and UGNN models across various graph
prediction tasks, in this section we narrow our attention to complementary experiments designed
to elucidate some of the particular issues raised by our analysis in previous sections. To this end,
we begin by exploring the interplay between the type of graph propagation weights (symmetric vs
asymmetric), the graph propagation path length (finite as with UGNN vs approximately infinite as
with IGNN), and model expressiveness. For this purpose we design the following quasi-synthetic
experiment: First, we train separate IGNN and UGNN models on the ogbn-arxiv dataset, where
the architectures are equivalent with the exception of Wp and the number of propagation steps (see
Appendix for all network and training details). Additionally, because UGNN requires symmetric
propagation weights, a matching parameter count can be achieved by simply expanding the UGNN
hidden dimension. Once trained, we then generate predictions from each model and treat them as
new, ground-truth labels. We next train four additional models separately on both sets of synthetic
labels: (i) An IGNN with architecture equivalent to the original used for generating labels, (ii) an
analogous UGNN, (iii) an IGNN with symmetric weights, and (iv) a UGNN with asymmetric weights
(equivalent to IGNN with finite propagation steps). For all models the number of parameters is a
small fraction of the number of labels to mitigate overfitting issues.
Results are presented in Table 1, where the columns indicate the label generating models, and the
rows denote the recovery models. As expected, the highest recovery accuracy occurs when the
generating and recovery models are matched such that perfect recovery is theoretically possible
by design. We also observe that UGNN with asymmetric weights (denoted “UGNN/as”) performs
8
Under review as a conference paper at ICLR 2022
significantly worse recovering IGNN data, indicating that truncated propagation steps can reduce
performance when the true model involves long-range propagation. Similarly, IGNN with symmetric
weights (IGNN/s) struggles somewhat to recover IGNN labels, indicating that symmetric weights
may not always be able to exactly mimic asymmetric ones across all practical settings, which is
not surprising. IGNN/s is however reasonably effective at recovering UGNN data given that the
later involves symmetric weights and finite propagation steps. In general though, the fact that the
performance of all models varies within a range of a few percentage points suggests a significant
overlap of model expressiveness as might be expected.
Next, in terms of time and memory consumption, we compare UGNN and IGNN on the Amazon
Co-purchase benchmark, which has been advocated in (Gu et al., 2020) as a suitable data source for
testing IGNN. Results are shown in Figures 1 and 2 based on executing 100 steps of training and
evaluation on a single Tesla T4 GPU; see Appendix for further details. Clearly, IGNN maintains a
huge advantage in terms of memory consumption, while UGNN has a faster runtime provided the
number of propagation steps is not too large.
Finally, we compare the node classification accuracy of UGNN and IGNN models across a number
of standard benchmarks in Table 2; see Appendix for dataset details and training protocols, etc. For
these results, we do not enforce an equivalent architecture, but instead tune the models for the best
accuracy. We also consider a variant of each model whereby the propagation matrix, either Wp or
Wps, is set to identity and rescaled to ensure IGNN convergence (denoted “w/I” in the table). Overall,
UGNN with Wps = I performs the best, indicating that in practice symmetric weights may not be
a hindrance. Furthermore, on the heterophily datasets Wisconsin, Texas, Actor, and Cornell (Pei
et al., 2019) the nonlinear graph propagation capabilities of UGNN can be especially advantageous
as has been previously discussed in (Yang et al., 2021). Note however that these commonly-used
benchmarks (even Amazon Co-Purchase) may be somewhat inadequate for demonstrating the full
effectiveness of truly long-range graph propagation as facilitated by IGNN, and determining the
prevalence of such data in real-world graphs is a valuable direction for future work.
		Cora	Citeseer	Pubmed	Arxiv	Wisconsin	Texas	Actor	Cornell	Amazon
IGNN	80.9 ± 1.0	69.6 ± 1.0	78.7 ± 0.7	71.1±0.2	83.1 ± 5.6	79.2 ± 6.0	35.5 ± 2.9	76.0 ± 6.2	84.4 ± 0.4
IGNN w/I	83.3 ± 0.7	71.2 ± 0.8	79.4 ± 0.6	68.0 ± 1.2	63.4 ± 9.2	66.7 ± 8.7	33.2 ± 3.4	64.9 ± 4.7	87.8 ± 0.0
UGNN	81.6 ± 0.5	70.0 ± 0.9	78.9 ± 0.4	68.6 ± 0.2	81.2 ± 5.1	78.7 ± 7.0	36.4 ± 1.0	81.1 ±7.3	80.0 ± 0.9
UGNN w/I	83.3 ± 0.3	74.1 ± 0.5	80.7 ± 0.5	72.9 ± 0.2	86.7 ± 4.2	84.6 ± 3.8	37.4 ± 1.5	86.8 ± 5.1	89.1 ± 0.0
Table 2: Accuracy results on node classification benchmarks.
8	Conclusions
This work has closely examined the relationship between IGNNs and UGNNs, shedding light on
their similarities and differences. In this regard, representative take-home messages are as follows:
(i) IGNN has a clear advantage when long-range propagation is required; however, standard node
classification benchmarks are not generally adequate for fully showcasing this advantage (e.g., as
evidenced by Table 2 results). (ii) UGNN symmetric propagation weights do not appear to be a
significant hindrance, while UGNN nonlinear propagation operators can be advantageous relative to
IGNN when unreliable edges are present. (iii) IGNN has a major advantage in memory costs, while
UGNN sometimes has a practical advantage in complexity and accuracy (although with more refined
benchmarks the latter may not always be true). (iv) Both models benefit from the inductive biases of
their respective design criteria, which occasionally overlap but retain important areas of distinction.
9
Under review as a conference paper at ICLR 2022
References
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. arXiv preprint
arXiv:1909.01377, 2019.
Dimitri Bertsekas. Nonlinear Programming. Athena Scientific, 2nd edition, 1999.
Siheng Chen and Yonina C Eldar. Graph signal denoising via unrolling networks. In ICASSP
2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 5290-5294, 2021.
Yichen Chen, Dongdong Ge, Mengdi Wang, Zizhuo Wang, Yinyu Ye, and Hao Yin. Strong NP-
hardness for sparse optimization with concave penalty functions. In International Confernece on
Machine Learning, 2017.
Patrick L Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing.
In Fixed-point algorithms for inverse problems in science and engineering, pp. 185-212. Springer,
2011.
Hanjun Dai, Zornitsa Kozareva, Bo Dai, Alex Smola, and Le Song. Learning steady-states of iterative
algorithms over graphs. In International Conference on Machine Learning, pp. 1106-1114, 2018.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Tsai. Implicit deep
learning. arXiv preprint arXiv:1908.06315, 2020.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. JASTA, 96(456):1348-1360, 2001.
Claudio Gallicchio and Alessio Micheli. Fast and deep graph neural networks. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 34, pp. 3898-3905, 2020.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In International
Conference on Machine Learning, 2010.
Remi Gribonval and Mila Nikolova. A characterization of proximity operators. Journal ofMathemat-
ical Imaging and Vision, 62(6):773-789, 2020.
Fangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Implicit graph
neural networks. In Advances in Neural Information Processing Systems, 2020.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Proceedings of the 31st International Conference on Neural Information Processing Systems,
pp. 1025-1035, 2017.
Hao He, Bo Xin, Satoshi Ikehata, and David Wipf. From bayesian sparsity to gated recurrent nets. In
Advances in Neural Information Processing Systems, 2017.
John Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of
novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. 2020.
Vassilis N Ioannidis, Meng Ma, Athanasios N Nikolakopoulos, Georgios B Giannakis, and Daniel
Romero. Kernel-based inference of functions on graphs. In D. Comminiello and J. Principe (eds.),
Adaptive Learning Methods for Nonlinear System Modeling. Elsevier, 2018.
Steven M. Kearnes, Kevin McCloskey, Marc Berndl, Vijay S. Pande, and Patrick Riley. Molecular
graph convolutions: moving beyond fingerprints. J. Comput. Aided Mol. Des., 30(8):595-608,
2016.
D. Kinderlehrer and G. Stampacchia. An Introduction to Variational Inequalities and Their Applica-
tions. Classics in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM,
3600 Market Street, Floor 6, Philadelphia, PA 19104), 1980. ISBN 9780898719451.
10
Under review as a conference paper at ICLR 2022
Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019a.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph
learning. In Advances in Neural Information Processing Systems, 2019b.
Jiajin Li, Anthony Man-Cho So, and Wing-Kin Ma. Understanding notions of stationarity in
nonsmooth optimization: A guided tour of various constructions of subdifferential for nonsmooth
functions. IEEE Signal Processing Magazine, 37(5):18-31, 2020.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and Jiliang Tang. Elastic
graph neural networks. In International Conference on Machine Learning, 2021.
D.G. Luenberger. Linear and Nonlinear Programming. Addison-Wesley, Reading, Massachusetts,
second edition, 1984.
Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In 8th International Conference on Learning Representations, ICLR, 2020.
Xuran Pan, Shiji Song, and Gao Huang. A unified framework for convolution-based graph neural
networks, 2021. URL https://openreview.net/forum?id=zUMD--Fb9Bt.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. In International Conference on Learning Representations, 2019.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In 8th International Conference on Learning
Representations, ICLR, 2020.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Pablo Sprechmann, Alex Bronstein, and Guillermo Sapiro. Learning efficient sparse and low rank
models. IEEE Trans. Pattern Analysis and Machine Intelligence, 37(9), 2015.
Bharath Sriperumbudur and Gert Lanckriet. On the convergence of the concave-convex procedure.
In Advances in Neural Information Processing Systems, 2009.
Petar Velickovic, GUillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR, 2018.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416,
2007.
Zhangyang Wang, Qing Ling, and Thomas Huang. Learning deep `0 encoders. In AAAI Conference
on Artificial Intelligence, volume 30, 2016.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems, 32(1):4-24, 2020.
11
Under review as a conference paper at ICLR 2022
Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng Zhang,
Zengfeng Huang, and David Wipf. Graph neural networks inspired by classical iterative algorithms.
In International Conference on Machine Learning, 2021.
Hongwei Zhang, Tijin Yan, Zenjun Xie, Yuanqing Xia, and Yuan Zhang. Revisiting graph convolu-
tional network on semi-supervised node classification from an optimization perspective. CoRR,
abs/2009.11469, 2020.
Xuebin Zheng, BingXin Zhou, Junbin Gao, Yu Guang Wang, Pietro Lio, Ming Li, and Guido Montufar.
How framelets enhance graph neural networks. In International Conference on Machine Learning,
2021.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf.
Learning with local and global consistency. Advances in Neural Information Processing Systems,
2004.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018.
Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra.
Graph neural networks with heterophily. arXiv preprint arXiv:2009.13566, 2020a.
Jiong Zhu, Yujun Yan, LingXiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond
homophily in graph neural networks: Current limitations and effective designs. In Advances in
Neural Information Processing Systems, NeurIPS, 2020b.
Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural
networks with an optimization framework. arXiv preprint arXiv:2101.11859, 2021.
Daniel ZUgner and Stephan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. In 7th International Conference on Learning Representations, ICLR, 2019.
Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks on neural networks
for graph data. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial
Intelligence, IJCAI, 2019.
A	Proofs
A. 1 Notations
For simplicity, in this section we denote 'γ(Y; W, f, ρ, B, φ) in (3) by 'γ(Y), denote φ (yi； Wφ)
by φ(yi), and denote f(X; W) by f(X). And the smooth part of 'γ is denoted by 'Y(Y)=
'γ (Y) - Pi φ(yi). And we possibly ignore the parameter of the function (e.g. 'γ).
We denote the vectorize of a matriX M by vec(M) and the kronecker product of two matrices M1
and M2 denoted by Mi 0 M2.
For Fenchel subdifferential respect to Y, we denote it by ∂Y, ignoring the subscript F used in main
paper.
A.2 Proof of Lemma 4.1
For proof of Lemma 4.1, we shall first state a property of Lipschitz continuous function:
Fact A.1 ((Bertsekas, 1999)) for any '(y) whose is L-Lipschitz, it satisfies
L
∀y, z,'(y) ≤ '(z) + V'(z)>(y — z) + 2Ily — z∣∣2	(16)
12
Under review as a conference paper at ICLR 2022
From Fact A.1, we have that
L
∀z, y, '(y) ≤ '(z) + V'(z)>(y - z) + 2ky - zk2	(17)
LL	L
='(z) + V'(z)>y - V'(z)>z + 2ky∣∣2 + -∣∣z∣∣2 - 2-y>z	(18)
1	L1	2	2
= '⑶-2LkV'(z)k2	+	2	LkV'(z)k2	+ kyk2 + kzk2 +	-V'(z)>y	-	-V'(z)>z -	2y>z
2L	2	L	L	L
(19)
='(z)- -LkV'(z)k2 + 2 y - Z-L V'(z)]1.	(20)
Taking '(y) = 'Y(y), y = Vec(Y) and Z = Vec(Y(k)), and let
βγ(Y(k)) = 'Y(Y(k)) - 2 ∣∣V'Y(Y(k)) ∣∣2,	(21)
We can define an upper-bound of 'γ:
'y(Y ) = 2α ∣∣Y - Fk)-aV' (Mk))i∣∣F+Xφ(yi)+βY Y(k) ,	(22)
i
and adopting (20) We have that:
'Y(Y) = 'Y(Y) + X φ(yi) ≤ 'Yιpp).	(23)
i
(Notice since α ≤ 1, 'γ is also 1 -Lipschitz). It's not difficult to check 'Yupp)(Y) = 'γ (Y) when
Y = Y(k). Also, since adding terms Which is not related to Y does not affect the arg min result in
(8), we have that
Y(k+1) = arg min ； k Y - U(k) kF + X Φ5	(24)
Y 2α
i
= argmin2αl∣Y - U(k)kF + X°®i) + By(Y(k))	(25)
i
= arg min '(Yιpp) (Y).	(26)
Then we have
'Y (Y (k+1)) ≤ '(Yιpp)(Y (k+1)) ≤ '(Yιpp) (Y(k)) ='Y (Y (k))	(27)
A.3 Proof of Lemma 4.2
This lemma is a straight-forward corollary of Theorem 1 in (Gribonval & Nikolova, 2020). For clarity,
we excerpt the theorem here:
Fact A.2 (Theorem 1 in (Gribonval & Nikolova, 2020)) Let H be some Hilbert space (e.g. Rk)
andY ⊂ H be non-empty. A function σ : Y → H is the proximal operator of a function φ : H → R∪
{+∞} if and only if there exists a convex l.s.c. (lower semi-continuous) function ψ : H → R∪ {+∞}
such that for each y ∈ Y, σ(y) ∈ ∂ ψ(y).
Since our σ is component-wise, we only need to consider Y = H = R. Furthermore, since
σ(x) is continuous, its indefinite integration exists. Let ψ(x) = σ(x)dx, which must be convex
since its derivative is non-decreasing. Then according to Fact A.2, there exists a function φ whose
proximal operator is σ. Conversely, Also if there exists φ such that σ = proxφ, from Fact A.2 it is
subdifferential of some convex function ψ, then it is non-decreasing.
13
Under review as a conference paper at ICLR 2022
A.4 Proof of Lemma 5.3
We only need a counterexample to prove this. Suppose n = 1 and d = 2, consider W
0
0
1
0,
Y = [a b],thenYW = a0
If there exists any second order smooth function h such that ∂a = 0 and ∂h = a,we have that
∂2h	∂2h _
∂a∂b	∂b∂a ,
which contradicts the second order smoothness assumption of h.
A.5 Proof of Theorem 5.1
First, we will show that 'Y is strongly convex under conditions given. We have
Vec (∂Yy) = VechyWf - f (X; W) + LYWsi	(28)
=(Ws 乳 I + Ws 乳 L) vec(Y) + vec [f (X; W)]	(29)
=Σvec(Y) + vec [f (X; W)]	(30)
and therefore the Hessian of 'Y is
∂V⅛ = ∑.
(31)
We also know that when λmin(Σ) > 0, 'sY is strongly convex respect to Y, thus has a unique global
minimum.
As for the case where non-smooth penalty term φ included, we first introduce another theorem from
(Gribonval & Nikolova, 2020).
Fact A.3 (Proposition 1 in (Gribonval & Nikolova, 2020)) Let H be some Hilbert space (e.g. Rk),
a function σ : H → H defined everywhere is the proximity operator of a proper convex l.s.c function
φ : →R ∪ {+∞} if and only if the following conditions hold jointly:
1.	there exists a (convex l.s.c) function ψ such that for each y ∈ H, σ(y) ∈ ∂ψ(y);
2.	σ is non-expansive.
We already assumed σ be a proximal operator, thus through Fact A.2 we know condition 1 is already
satisfied, and since we further assumed non-expansive σ, condition 2 is also satisfied. Therefore from
Fact A.3 we conclude that φ is convex. And since 'Y is the summation of a strongly convex part 'sY
and a convex part Pin=1 φ(yi), it is strongly convex and have unique global minimum.
Furthermore, the λmax(Σ) is the Lipschitz constant of 'sY, and from theorem 5.4 we know when
α ≤ 1∕λmax(Σ) the algorithm converges to the stationary point, which is the global optimum.
_ 〜〜一一 __ 一 一- -. 一一一 一 一一一 一一 一 ~ _
Proof of Corollary 5.1.1 Taking Wf = I - Ws and P = I - L,we have that
Σ = I - P ③ Ws.	(32)
By the spectral properties of Kronecker product, we have
λmin(Σ) = 1 - Wss	2 kP k2 >0,	(33)
and shifting the order of the inequality gives kWss k2 < kPk2-1.
Proof of Lemma 5.2 We first convert (2) to vector form:
vec (Y(k+1)) = σ h(Ws 0 P) vec (Y(E)) + vec(f)] ,	(34)
14
Under review as a conference paper at ICLR 2022
where f = f (X; Wχ), ignoring the parameters for simplicity. Let M = Wp 0 P, from the spectral
properties of knronecker product, we have
kMk2 = kWpk2kPk2 < 1.	(35)
By definition of matrix norm we have
∀x1, x2, kM (x1 - x2)k2 ≤ kMk2kx1 - x2k2,	(36)
which means M (as a linear transformation) is a contraction mapping on Euclidean space. We
also assumed σ is contraction mapping, thus from Banach’s fixed point theorem(Kinderlehrer &
Stampacchia, 1980) we conclude that (34), as well as (2), has a unique fixed point.
A.6 Proof of Theorem 5.4 and Corollary 5.4.1
The proof is followed by Zangwill’s convergence theorem, we state it here:
Fact A.4 (Zangwill’s Convergence Theorem (Luenberger, 1984)) Let A : X → 2X be a set-
valued function, G ⊂ X be a solution set (which can be any set we are interested in), for any sequence
{xk}k∞=1 such that xk+1 ∈ A(xk), if following conditions hold jointly:
1.	{xk|1 ≤ k ≤ ∞} is contained in some compact subset of X.
2.	there exists a continuous function ` such that
(a)	if X ∈ G ,then ∀y ∈ A(x),'(y) < '(x).
(b)	if x ∈ G, then ∀y ∈ A(x),'(y) ≤ '(x).
3.	the mapping A is closed at all point of X \ G i.e. {(x, y)|x ∈ clos (X \ G) , y ∈ A(x)} is a
closed set, where clos means set closure.
then the limit of any convergent subsequence of {xk }k∞=1 is a solution i.e. inside G.
To prove Theorem 5.4 We define the solution set as the set of generalized fixed points of 'γ:
G = {z Z ∈ arg min 2α ∣∣Y -(Z - αV'Y(Z))∣∣F + φ(Y)}	(37)
Then the proof is composed of three steps:
1.	showing 'γ (Y) is a descent function;
2.	showing the process is closed and Y(k)s are bounded;
3.	showing that the solution set defined in (37) is the set of all stationary points.
The proof for step 1 is already finished in the proof of Lemma 4.2. Notice that by definition of the
solution set, the strict descent condition on X - G is automatically satisfied since if Y(k) 6∈ G then
'Y (Y(k)) > min ɪ ∣∣Y - U(k)1+ φ(Y) = 'γ (Y(k+1)) .	(38)
To further prove the convergence using Fact A.4, we also need to illustrate that the process is closed
and the iterations Y(k) are limited in a bounded set. The former is a straight-forward deduction
of Lemma 6 in (Sriperumbudur & Lanckriet, 2009), and the latter one is easily shown as follows.
Consider the set
S = {Y |'y (Y) ≤ 'γ (Y ⑼)}.	(39)
By Lemma 4.2, Y(k) ∈ S and by the assumption that 'γ (Y) → ∞ when ∣∣Y ∣∣ → ∞, we know S is
bounded.
So far, by Fact A.4, we have proved any accumulation point is in G, but we defined G in a way that it
is the set of fixed points of the algorithm. We then connect fixed points with stationarity as promised
in the Theorem 5.4 i.e. Step 3.
15
Under review as a conference paper at ICLR 2022
Lemma A.1 (Step 3)
Z ∈ argminɪ ∣∣Y - (Z - αV'Y(Z))∣∣F + φ(Y) 0 0 ∈ ∂z'γ(Z)	(40)
Proof
1 Z ∈ arg min 飞—∣∣ Y -(Z — αV'γ (Z))∣f + φ(Y)	(41)
1	2 O 0 ∈ ∂γ	— IIY -(Z - αV'Y(Z))kF + φ(Y) 2α	Y=Z	(42)
o 0 ∈V'Y(Z) + ∂zφ(Z)	(43)
Q⇒ 0 ∈ ∂z('Y(Z) + φ(Z)) = ∂z'y(Z)	(44)
Here we used the summation property and local minimality property of Fenchel subdifferential,
further illustration of those properties can be found in (Li et al., 2020).
For the last part of the theorem, it follows directly from the continuity of 'γ. Additionally, for
Corollary 5.4.1, following the same deduction of Theorem 5.1, 'γ will be strongly convex so there
will only be one stationary point which is the global optimum.
A.7 Proof of Theorem 6.1
First consider a case which allows T and Wps to have complex values. Hereinafter we denote the
space of all complex-valued matrices with shape d1 × d2 by Cd1 ×d2.
For preciseness, we recall Jordan’s decomposition theorem below. Let J(λ) be Jordan block matrix
-λ	1
λ 1
J (λ) =	λ
.We have:
1
λ
Fact A.5 (Jordan) For every W ∈ Rd×d, there exists a complex-valued invertible matrix P ∈ Cd×d
ΓJ (λι)	]
J (λ2 )
and complex-valued block-diagonal matrix Ω =	.	SUCh that W =
.
.
J(λk)
PΩP-1, where λj ∈ C is the j-th eigenvalue of W and the size of J(λj) is the algebraic multiplicity
ofλj.
Corollary A.1.1 If a square matrix W ∈ Rd×d has d distinct eigenvalues the there exists P, Λ ∈
Cd×d where P is invertible and Λ is diagonal, such that W = PΛP-1.
Then, we shall show that actually on the complex domain “almost” every matrix can be diagonalized,
which means, for each matrix W, either it itself can be diagonalized, or there’s a diagonalizable
matrix W0 that is arbitrarily closed to W .
Corollary A.1.2 For every square matrix W ∈ Rd×d and any > 0, there exists a diagonalizable
square matrix W0 ∈ Rd×d such that ∣W0 - W ∣ ≤ .
Proof Let W = RΩR-1 and E = diag[eι	62 •… ∈n] such that the diagonal of Ω + E is
2
diStinctand e2 < d∣R∣2∣∣R-1k2 .
Since Ω is upper-triangle, its eigenvalues are the elements in its diagonal, which are distinct. Thus
according to Corollary A.1.1, Ω + E is diagonalizable, suppose Ω + E = QAQ-1. Let W0 =
R(Ω + E)R-1 = RQAQ-1R-1, it is apparent W0 also diagonalizable.
16
Under review as a conference paper at ICLR 2022
Now consider the difference between W0 and W. We have kW0 - Wk2 = kPEP-1 k2 ≤
kEk2kRk2kR-1k2 < 2.
Note the norm in Corollary A.1.2 can be any matrix norm.
Lemma A.2 The solution of equation PY W + XWx = Y is continuous respect to W as long as a
unique solution exists.
Proof The solution of this equation is VeC(Y) = (I - W> 0 P) 1 VeC(XWx) is continuous.
Theorem A.3 ∀Wp ∈ Rd×d that admits unique fixed point for IGNN and Wx ∈ Rd0 ×d, suppose
Y ∈ Rn × d is the only solution of PYW + XWx = Y ,then there exists a Y 0 ∈ Cn×d, WX ∈ Cd0 乂 &',
right-invertible T ∈ Cd×d0 and herimitian WS ∈ Cd0×d0, such that
PY0rΓW^s + XWx = Y0T, with ∣∣Y0 — Y∣∣ < e, Ve > 0, Y0 ∈ Cn×d.	(45)
Proof By Corollary A.1.2 and Lemma A.2, there exists diagonalizable W0 ∈ Rd×d such that
PY 0W0 + XWx = Y0 for some Y0 satisfies ∣Y0 - Y ∣ < e.
Suppose now W0 = RΛR-1 where Λ is diagonal and R is invertible. We have
PY0RΛR-1 + XWx = Y0,	(46)
and right-multiply by R at each side of this equation we get
P(Y0R)Λ + XWxR = Y0R.	(47)
By choosing T = R, Ws = Λ, WX = WxR-1, We proved the proposition.	■
Next, we consider restricting this result to the real domain. This is straight-forward since every
complex linear transformation can be converted to real linear transformation by consider real and
imaginary parts separately. Hereinafter, We denote the real part of an complex-valued matrix M by
M(r) and imaginary part by M(i) .
Proof of Theorem 6.1 Let Y, T, Ws, WX are matrices output by Theorem A.3. Note the last three
are complex-valued, T is invertible and Ws is hermitian.
Consider T = [T(/)加].
T(r)工(r)-加工(i).Thus T
Since T is invertible, let LX = T-1, then we have I = (TLX)⑺ =
, 了，、1
Xr)	= T(r)X(r) 一 T(i)X(i) = I. This means T is right-invertible.
Then let Wsp
WXps(r)
-WXps(i)
WXs
WX ps(i) . Since WXps is hermitian
p(r)
, we have that WX ps(r) = WX ps(>r) and
WX ps(i) = -WXps(>i), which ensures Wps constructed here is symmetric.
At last, let WX = [Wx(r) Wx(i)].
Now it,s easy to verify that PY0TWS + XWX = Y0T.
A.8 Proof of Theorem 6.2
First we will illustrate that with some value of the parameters in UGNN, the iteration step from (8)
can form a GCN layer with symmetric shared weights, with or without residual connections. We set
P to be identity (so that Γ = I), B satisfies that B> B = I 一 P (e.g. if P = A = DT/2 ADT/2 is
normalized adjacency, then B is normalized incidence matrix B = DT/2B).
17
Under review as a conference paper at ICLR 2022
For a GCN model without residual connections, let Wfs = I - Wps, then we have Y (k+1) =
σ(PY(k)WpS). Consider Y⑼=［可黑 0 0 … where YGC)N istheinputofGCN,andthe
ith 0 here denote a block matrix of all zero with the same size as YG(iC)N. When we let
Wps =	0 Wp(1)> 0 . .	Wp(1) 0 Wp(2)> . .	0 Wp(2) 0 . .	0 0 Wp(3) . .	...	0 ...	0 ...	0 .. ..	,	(48)
	. 0	. 0	. 0	. 0	.. ∙∙∙ Wpk)	
	0	0	0	0	...	0	
it is not difficult to verify that the kth block of Y(k) is YG(kC)N . Note the 0s in the (48) are block
matrices of all zeros and with proper size.
For GCN model with residual connection, let Wfs = I - Wps - Wrs, then Y (k+1) = σ(P Y (k)Wps +
Y (k)Wrs). We take the same Wps as before (48), and let
	"0 I 0 0 …0-	
	I 0 I 0 …0	
	0 I 0 I …0	
Wrs =		 . 	 .	.	.	(49)
	.	.	.	.	.. 0	0	0	0	…I	
	_0 0 0 0 …0_	
Since Wrs and Wps are both symmetric, apparently Wfs is also symmetric. Also notice that since
residual connection exists, the size of all the Wp(i)s are the same, so the size of I in the (49) is the
same as Wp(i).
B A sufficient Condition of Equivalence
In this section, we consider general nonlinear functions σ and penalty term φ. Hereinafter we denote
proxφ by σ. We add mild assumption on σ and consider a general type of σ.
Constraint on σ To simplify the discussion, we assume σ(x) is a proximal operator of some penalty
function and is differentiable with respect to x.
This assumption is consistent with practice. Firstly, commonly used non-linearities like tanh,
sigmoid and ReLU are all proximal operators since they are all continuous and component-wise
non-decreasing (By Lemma 4.2). Also, although at some point ReLU and Leaky-ReLU are not
differentiable, we can approximate them by
ReLU(x) ≈ ɪ log (exp(rx) + 1)
r
and
Leaky-ReLU(x,p) ≈ J log (exp(rx) + exp(prx))
with big enough r, which are differentiable and do not affect their practical attributes.
σ Considered
Fact B.1 A twice-differentiable function ψ is convex iff its Hessian Hψ is positive semi-definite.
As discussed in the main paper, it would cause some degenerated cases if we allow a general class of
proximal operators. Also it would too difficult to handle in this case. Therefore, we only consider
proximal operators in a special while general family
S = {σ : x → Gσ(Cx)|G, C are chosen such that σ is proximal operator}
18
Under review as a conference paper at ICLR 2022
Now We consider under What value of G and C We can ensure σ is proximal operator. From Fact A.2,
we know ∃φ, σ = proxφ iff σ is subgradient of some l.s.c convex function. Since we have assumed
σ is continuous and differentiable, σ(x)dx is tWice differentiable. Thus from Fact B.1 We knoW
H (x) = dσ∂(χ) is positive semi-definite, and so do Hessian of σ(x), i.e. H(X)=需.From the
chain rule we know
~ ..
H(X)
∂σ(x)
∂x
∂Gσ(C x)
∂x
∂σ(Cx) ∂Cx
∂C x ∂x
GH(Cx)C.
From the deduction above, we can conclude that:
Lemma B.1 σ : Rd → Rd, x → Gσ(Cx) is proximal operator if ∀x ∈ CRd, GH(X)C is positive
semi-definite where H(x) = dσ∂x.
The Condition of General Fixed-Point Alignment
Theorem B.2 For σ satisfying the assumptions above, Wp ∈ Rd×d that admits unique fixed point
for IGNN, and Wx ∈ Rd0×d, suppose Y is the only solution of Y = σ(PY W + XWx), a sufficient
condition of
∃φ, right invertible T ∈ Rd×d , symmetric Wps ∈ Rd ×d and WX ∈ Rd×d such that
σ(PYTWp + XWx) = YT
is that
∃ right-invertible T ∈ Rd×d0, C ∈ Rd0×d,Wps ∈ Rd0×d0 such thatTWpsC= Wp
and ∀x ∈ C>R, T>H(x)C> is P.S.D,
where σ(x) = proxφ(x) and H(x)
∂σ(x)
∂x .
Proof We have assumed σ ∈ S, thus σ(x) = Gσ(Cx). We want to prove that
YT = σ(pγτws + XWx)
=σ (PYTWsC + XWxC )G
. . . ~
=σ(PYTWsC + XWx)G (Assume WXC = Wx).
(50)
(51)
(52)
A sufficient condition for this equation to hold is T = G and Y = σ(AYTWpsC + XWX) =
σ(AY Wp + XWX). Comparing the last two terms, apparently it holds when TWpsC = Wp . And to
ensure σ ∈ S, from Lemma B.1 we know it means T > H (C >x)C > be positive semi-definite for all
x.
We can verify this Theorem B.2 by linear case. If σ is linear, then H(Cx) = I, thus simply
taking C = T-1 (on complex field) we have TWpsT-1 can generate any real matrix W and
T> H(C> x)C> = T> T-> = I is positive semi-definite.
C Experiment Details
C.1 Datasets
In this paper, we used Cora, Citeseer and Pubmed (Sen et al., 2008) and OGBN-arxiv(Hu et al.,
2020) which are commonly used datasets to evaluate GNN models in general purpose. We also used
Wisconsin, Texas, Actor and Cornell datasets introduced by (Pei et al., 2019), which are small but
with a strong property of heterophily, and is used to test models’ performance on heterophily graphs.
We also include results on amazon co-purchase which is originally used in IGNN(Gu et al., 2020)
and TWIRLS(Yang et al., 2021). The experiment settings generally follows (Yang et al., 2021).
The statistic of different datasets are summarized in Table 3.
19
Under review as a conference paper at ICLR 2022
Table 3: Dataset statistics. Note that Amazon co-purchase does not have node features.
Dataset	# Nodes	# Edges	Feature Dim	# Classes
Cora	2,708	5,429	1,433	7
Citeseer	3,327	4,732	3,703	6
Pubmed	19,717	44,339	500	3
OGBN-Arxiv	169,343	1,166,243	128	40
Texas	183	309	1,703	5
Wisconsin	251	499	1,703	5
Actor	7,600	33,544	931	5
Cornell	183	295	1,703	5
Amazon	334,863	2,186,607	-	58
C.2 Model Implementation
For model implementation, we directly adopt IGNN’s code and make a small modification of
TWIRLS’s code to make it a general UGNN.
In UGNN, since it is too general to consider all the choice of ρ and φ, we adopt ρ(Y ) = Y which
produces Γ = I and φ(x) = I∞ [x < 0] which produces proxφ = ReLU, as mentioned in section
4.2. Also, We introduce another hyper-parameter λ and let Ws = (1 + λ)I - Ws and Ws = λW,
thus the iteration becomes
Y(k+1) 一 (1 - α - αλ)Y(k) + αλPY(k) W + αf (X; W),	(53)
which covers model described in (11) when λ = 1 and α = ι+λ5 but is more general.
When projecting Ws (or W in UGNN) to the space that admits unique fixed point, in IGNN’s original
paper it uses ∣∣ ∙ ∣∣inf (GU et al., 2020), which would break the symmetry, based on our result in Section
5.1, in our implementation when the model weight is symmetric we project ∣∙∣2 instead of ∣ ∙ ∣∣inf.
For propagation matrix we always use normalized adjacency P = A.
C.3 Further Details about Label Recovering Task
In this task, we use model with f(X) = XWx and g [y↑ (W); θ] = Wgy* (W) where Wg ∈ Rc×d is
a learned matrix that maps the propagated node features to the output space. We set the hidden size
to 32 in asymmetric case and 34 in symmetric case to ensure nearly the same number of parameters
(we deliberately set the hidden size to this small to ensure the models do not overfit). For generating
models, we first train them using the original labels of the dataset by 500 steps. The number of
propagation steps is set to 2, λ is set to 1, α is set to 2 for UGNN.
C.4 Further Details about Memory and Timing
In this section we adopt amazon co-purchase dataset with label ratio = 0.6. The hyper-parameters are
set the same as the original IGNN paper (Gu et al., 2020) for all the models. As mentioned before, in
UGNN we use λ = 1,α = 2 and explicitly times the parameter matrix by 2 to exactly match the
iteration of (11), under consideration that not hurting the training dynamics.
C.5 Further Details about Best-Model Comparing
It is worth mentioning that we did not traverse all possible implementations of the two models
(especially for UGNN since it has more general forms), and therefore, the comparisons have some
5Actually, they differ by a coefficient of 2 which can be absorbed into W and Wx since they are all learned
parameters. When comparing UGNN with IGNN, we explicitly multiplies this 2 to eliminate the potential
impact on training dynamics
20
Under review as a conference paper at ICLR 2022
limitations. The hyper-parameters that we considered include the specific implementation of f(X; W)
(f (X; W) = XW or f (X; W) = AXW), the number of MLP layers in f and g, the way of
initialization (Y (0) = f(X) or Y (0) = 0) and learning rate, weight decay rate and dropout rate. And
for UGNN the number of propagation steps, values of α and λ are also considered. Also, UGNN
w/I results in the table come from results of TWIRLS(Yang et al., 2021), while the other three were
conducted by us.
21