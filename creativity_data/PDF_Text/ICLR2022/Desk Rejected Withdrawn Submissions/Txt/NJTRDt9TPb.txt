Under review as a conference paper at ICLR 2022
Diverse Imitation Learning via
Self-Organizing Generative Models
Anonymous authors
Paper under double-blind review
Ab stract
Imitation learning is the task of replicating expert policy from demonstrations,
without access to a reward function. This task becomes particularly challenging
when the expert exhibits a mixture of behaviors. Prior work has introduced latent
variables to model the variations of the expert policy. However, our experiments
show that the existing works do not exhibit appropriate imitation of individual
modes. To tackle this problem, we adopt an encoder-free generative model for be-
havior cloning (BC) to accurately distinguish and imitate different modes. Then,
we integrate it with GAIL to make the learning robust towards compounding er-
rors at unseen states. We show that our method significantly outperforms the state
of the art across multiple experiments.
1	Introduction
The goal of imitation learning is to learn to perform a task from expert trajectories without a reward
signal. Towards this end, two approaches are studied in the literature. The first approach is behavior
cloning (BC) (Pomerleau, 1991), which learns the mapping between individual expert states-action
pairs in a supervised manner. It is well known that BC neglects long-range dynamics within a
trajectory. This leads to problem of compounding error (Ross & Bagnell, 2010; Ross et al., 2011),
that is, when a trained model visits states that are slightly different from those visited in expert
demonstrations, the model takes erroneous actions, and the errors quickly accumulate as the policy
unrolls. Therefore, BC can only succeed when training data is abundant.
An alternative approach, known as inverse reinforcement learning (IRL) (Ziebart et al., 2008), recov-
ers a reward function that makes the expert policy optimal. Such approaches often require iterative
calculation of the optimal policy with reinforcement learning (RL) in an inner loop, making them
computationally expensive. The computational cost can be reduced by jointly optimizing a policy—
via RL, and a cost function—via maximum causal entropy IRL (Ziebart et al., 2008; 2010). The
resulting algorithm is called generative adversarial imitation learning (GAIL) (Ho & Ermon, 2016),
and is closely connected to generative adversarial networks (Goodfellow et al., 2014). GAIL has
shown better promise than BC, both in sample efficiency in the number of expert trajectories, and in
robustness towards unseen states.
Expert demonstrations of a particular task can display variations, even given the same initial state.
This diversity can often stem from multiple human experts performing the same task in different
ways. Alternatively, it might arise from pursuing multiple unlabeled objectives, e.g. a robotic arm
moving towards different goals. In such scenarios, neither BC nor GAIL enjoys any explicit mech-
anism to capture different modes of behavior. Particularly, BC averages out actions from different
modes. On the other hand, GAIL learns a policy that captures only a subset of control behaviors,
because adversarial training is prone to mode collapse (Li et al., 2017; Wang et al., 2017).
Imitation of diverse behaviors is addressed in several prior works. Fei et al. (2020); Merel et al.
(2017) use labels of expert modes in training. These works differ from ours in that we assume ex-
pert modes are unlabeled. InfoGAIL (Li et al., 2017) and Intention-GAN (Hausman et al., 2017)
augment the objective of GAIL with the mutual information between generated trajectories and the
corresponding latent codes. This mutual information is evaluated with the help of a posterior net-
work. Wang et al. (2017) use a variational autoencoder (VAE) module to encode expert trajectories
into a continuous latent variable. These latent codes are supplied as an additional input to both the
policy and the discriminator of GAIL. Nevertheless, as we show in our experiments, both InfoGAIL
and VAE-GAIL perform poorly in practice.
1
Under review as a conference paper at ICLR 2022
Proposing a method for multimodal imitation learning requires finding the correspondence between
expert trajectories and a latent variable. Wang et al. (2017) suggest learning this correspondence
with VAEs. However, this imposes the difficulty of designing an isolated (as opposed to end-to-end)
recurrent module to encode sequences of state-action pairs. Moreover, VAEs are mainly well suited
for continuous latent variables. We propose to address these challenges by removing the recurrent
encoder module and instead directly searching for an optimal latent variable. Vondrick et al. (2016);
Bojanowski et al. (2017); Hoshen et al. (2019) have recently considered similar encoder-free models
for non-sequential data.
In this work, we propose an encoder-free model for multimodal BC. We name it self-organizing
generative model (SOG), indicating that it can learn semantically smooth latent spaces. We derive
SOG as a generative model optimizing the maximum likelihood objective. Moreover, we devise an
extension to it enabling training with high dimensional latent spaces. Finally, we combine SOG—as
a BC method—with GAIL, to get the best of both worlds. This attempt is inspired by the empirical
study of Jena et al. (2020), where in a unimodal setting, a combination of BC with GAIL is shown
to yield better performance in fewer training iterations.
In a nutshell, this work mainly focuses on imitation of diverse behaviors while distinguishing differ-
ent modes. Our contributions are as follows:
1.	We introduce SOG as an encoder-free generative model that can be used for multimodal BC. We
show that SOG procedure is closely connected to optimization of the maximum likelihood ob-
jective. Besides, we introduce an extension to SOG that enables learning with high-dimensional
latent spaces.
2.	We integrate SOG with GAIL to benefit from accuracy of SOG and robustness of GAIL at the
same time. Thus, the resulting model (a) can generate trajectories that are faithful to the expert,
(b) is robust to unseen states, (c) learns a latent variable that models variations of the expert
policy, and (d) captures all modes successfully without mode collapse.
3.	Our empirical results show that our model considerably outperforms the baselines.
2	Background
2.1	Preliminaries
We consider an infinite-horizon discounted Markov decision process (MDP) as a tuple
(S, A, P, r, ρ0, γ). Namely, S denotes the state space, A denotes the action space, P : S × A × S →
R represents the state transition probability distribution, r : S × A → R is the reward function,
ρ0 : S → R≥0 represents the distribution over initial states, and γ ∈ (0, 1) is the reward discount
factor.
Let π : S × A → R≥0 denote a stochastic policy function. Demonstrations of the policy π are se-
quences of state-action pairs, sampled as follows: so 〜ρo, at 〜∏(at∣st), st+ι 〜P (st+ι∣st, at).
When states and actions generated by policy π, we define expected return as Eπ [r(s, a)] :=
Eπ [Pt∞=0 γtr(st, at)].
2.2	Imitation Learning
The goal of imitation learning is to replicate the expert policy from demonstrations without access
to the underlying reward function. Two approaches exist for this problem. The first approach is BC,
which treats the state-action pairs as inputs and outputs of a policy function, and optimizes for the
L2 loss between the expert actions and the predicted actions. The second approach is GAIL, which
optimizes the following objective:
min max Eπθ [log D(s, a)] +EπE [log (1 - D(s, a))] - λH(πθ).	(1)
πθ D
In this equation, πθ denotes the policy function realized by a neural network with weights θ,
and πE denotes the expert policy. Moreover, D denotes the discriminative classifier that distin-
guishes between state-action pairs from the trajectories generated by πθ and πE. Lastly, H(πθ) :=
E∏θ [- log∏θ(a|s)] represents the discounted causal entropy of the policy ∏θ (Bloem & Bambos,
2014). We provide additional details about optimization of Equation (1) in Appendix A.
2
Under review as a conference paper at ICLR 2022
2.3	Imitation of Diverse Expert Trajectories
We model the variations in the expert policy with a latent variable z . In particular, z is sampled
from a discrete or continuous prior distribution p(z) before each rollout, and specifies the mode of
behavior. Therefore, we formalize the generative process of a multimodal expert policy as follows:
Z ~ p(z), so ~ Po, at ~ ∏e(at∣St, z), St+ι ~ P(st+ι∣αt, st).
3	Method
In this section, we aim to propose an algorithm for imitation of diverse behaviors. We first intro-
duce a generative model for learning the distribution of arbitrary datasets. Namely, let x and y
respectively denote an input data point and a corresponding output data point. We assume that y is
generated through a two-stage random process:
1.	A latent variable z (independent of x) is sampled from a prior distribution p(z). This prior can
be a given discrete distribution over K categories with probability masses π1 , . . . , πK, or an
arbitrary continuous distribution, e.g. a multivariate Gaussian p(z) = N (z; 0, I).
2.	A value y is sampled from a conditional distribution p(y|z, x; f) of the following form:
p(y|z, x; f) = N(y; f (z, x), σ1 2 3 4 5 6 7 8 9 10 11 12 13I),	(2)
where f maps z and x to the mean of the distribution, and σ2 is the isotropic noise variance in
the space of y .
Now consider a dataset D = {(xi, yi)}iN=1, consisting of N i.i.d samples generated by the process
above. To estimate this model, one needs to restrict the estimation of function f within a family of
parameterized functions, e.g. neural networks fθ with weight parameters θ. One set of models can
be derived by maximizing the log-likelihood of the observed data, over both the shared parameters
θ, and an assignment zi ∈ {1, . . . , K} for each data point (xi, yi):
N
max	logp(yi |zi, xi; θ),	where Z = {z1, . . . , zN} and zi ∈ {1, . . . , K}.	(3)
θ,Z
, i=1
The rest of this section is organized as follows. In Section 3.1, we introduce an algorithm named
Self-Organizing Generative Model (SOG) to efficiently solve the optimization problem of Equa-
tion (3). In Section 3.2, we adopt this algorithm for multimodal imitation learning in BC and GAIL
settings. In Section 3.3, we briefly mention the relationship between optimization of Equation (3)
(as adopted by SOG), and the principle of maximum likelihood estimation, where the data likelihood
is marginalized over the latent variable. We expand this relationship in the appendix due to space
limits. Finally, in Section 3.4, we elaborate on some additional aspects of the SOG model.
3.1	The SOG Algorithm in General
In Algorithm 1, we propose a method to optimize Equation (3) in both cases of discrete and contin-
uous latent variables.
Algorithm 1 Self-Organizing Generative Model(SOG)
1: Define: Loss function L (y, y) := ||y - y||2
2: Input: Initial parameters of policy network θo
3: for epoch = 1, 2, . . . do
4:	for iteration = 1, 2, . . . do
5:	Sample a minibatch of Ndata data points (xi, yi) from the dataset.
6:	for i = 1, 2, . . . , Ndata do
7:	Sample Nz latent codes Zj 〜p(z).
8:	Calculate zi := arg minzj L (fθ (zj, xi), yi).
9:	end for
10:	Calculate LSOG = PiN=da1ta L (fθ(Zi, xi), yi) and its gradients w.r.t. θ.
11:	Update fθ by stochastic gradient descent on θ to minimize LSOG .
12:	end for
13: end for
3
Under review as a conference paper at ICLR 2022
This algorithm performs a two-step optimization over each batch of data: (1) given current θ, find
the best latent codes; (2) given the best latent codes, take a gradient step on θ. Both steps decrease
the same loss function, which guarantees convergence if gradient steps were taken over the entire
data. For mini-batch settings, convergence can be empirically verified through our experiments.
In the case of discrete latent variables, each z correspond to one of the K categories. This effec-
tively results in an exhaustive search over z ∈ {1, . . . , K}. In the continuous case, it is impossible
to enumerate all candidates of z . Therefore, the optimal z is searched over samples of the prior
distribution p(z).
We justify the name of the algorithm in Section 3.4.
3.2	Adopting SOG for Multimodal Imitation Learning
In the following, we introduce two approaches for multimodal imitation learning: (1) adopting SOG
for multimodal BC, and (2) combining multimodal BC with GAIL using SOG.
Multimodal behavior cloning with SOG. We adopt Algorithm 1 for learning the relationship be-
tween state-action pairs in a multimodal BC setting. We enforce an additional constraint that the
latent code is shared across each trajectory. Hence, we propose Algorithm 2.
Algorithm 2 SOG-BC: Multimodal Behavior Cloning with SOG
1:	Define: Loss function L (a, a) := ||a 一 a||1 2 3 4 5 6 7 8 9
2:	Input: Initial parameters of policy network θ0; expert trajectories TE 〜∏e
3:	for iteration = 0, 1, 2, . . . do
4:	Sample state-action pairs XE 〜TE with the same batch size.
5:	Sample Nz latent codes Zi 〜p(z) for each trajectory.
6:	Calculate ZE := argminzi ETE [L (fθ(zi, s), a)].
7:	Calculate LSOG = ETE [L (fθ(ZE, s), a)] and its gradients w.r.t. θ.
8:	Update fθ by stochastic gradient descent on θ to minimize LSOG .
9:	end for
Multimodal combination of BC and GAIL using SOG. Next, we introduce Algorithm 3, where
we combine SOG, as a means for BC, with GAIL to ensure robustness towards unseen states. This
algorithm is inspired by Jena et al. (2020), which in a unimodal setting optimizes a weighted sum of
BC loss and the GAIL “surrogate” loss (see Appendix A).
Algorithm 3 SOG-GAIL: Multimodal Combination of BC and GAIL
1: Input: Initial parameters of policy and discriminator networks, θo, w0; expert trajectories TE 〜
πE
2: for i = 0, 1, 2, . . . do
3:	Sample a latent code Zi 〜p(z), and subsequently a trajectory Ti 〜∏θi (∙∣Zi).
4:	Sample state-action pairs Xi 〜Ti and XE 〜TE with the same batch size.
5:	Update the discriminator parameters from wi to wi+1 with the gradient
ETi [Vw log Dw(s, a)] + ETE [V® log(1 - Dw(s, a))].
6:	Calculate the surrogate loss LPPO using the PPO rule with the following objective
ETi [Vw log Dw (s, a)] - λHH(πθi).
7:	Calculate the SOG loss LSOG per Algorithm 2.
8:	Take a policy step from θi to θi+1 w.r.t. the objective LPPO + λS LSOG .
9: end for
4
Under review as a conference paper at ICLR 2022
3.3	Analysis of SOG
Algorithm 1 optimizes the non-marginalized likelihood of Equation (3). However, it can be shown
that this algorithm is also related to optimizing the marginalized likelihood of the data, i.e.:
max
θ
N
X log
i=1
p(yi|z, xi; θ)p(z)dz.
(4)
We characterize the relationship between Algorithm 1 and Equation (4) in the appendix due to space
limits. We analyse discrete and continuous latent variables separately in Appendices C and D.
3.4	Further Properties of SOG
Sample complexity. Training procedure of Algorithm 1, given a high-dimensional continuous latent
variable, requires a prohibitively large number of latent code samples (see step 7 of the algorithm).
Therefore, in Appendix F, we propose a computationally efficient extension of this algorithm suited
for high dimensions of the latent space.
Self-organization in the latent space. In Appendix E, we theoretically derive that latent codes
corresponding to nearby data points get organized close to each other. In addition, visual results in
Figure 4 and Appendix F show that different regions of the latent space organize towards generating
different modes of data. Hence, the phrase “self-organization” in naming of Algorithm 1 is justified.
4	Experiments
In our experiments, we demonstrate that our method can recover and distinguish all modes of be-
havior, and replicate each robustly and with high fidelity. We evaluate our models, SOG-BC and
SOG-GAIL, against two multimodal imitation learning baselines: InfoGAIL (Li et al., 2017) and
VAE-GAIL (Wang et al., 2017).
4.1	Experiment Setup
We adopt an experiment from Li et al. (2017) in which an agent moves freely at limited veloci-
ties in a 2D plane. In this experiment, the expert produces three distinct circle-like trajectories.
In another series of experiments, we evaluate our model on several complex robotic locomotion
tasks, simulated via the MuJoCo simulator (Todorov et al., 2012). These experiments, borrowed
from Rakelly et al. (2019), include multimodal tasks with discrete or continuous modes. Discrete
tasks include Ant-Fwd-Back, Ant-Dir-6, HalfCheetah-Fwd-Back, Humanoid-Dir-6, Walker2d-Vel-
6, and Hopper-Vel-6. Additionally, we conduct two experiments with continuous modes, namely
FetchReach and HalfCheetah-Vel. All these experiments are named after standard MuJoCo environ-
ments. The suffix “Fwd-Back” indicates that the modes correspond to forward or backward moving
directions. Besides, the suffix “Dir-6” implies six moving directions, namely k ∙ 2π∕6 angles. On
the other hand, “Vel” and “Vel-6” indicates that the expert selects velocities uniformly at random,
resulting in different modes of behavior. The sets of expert velocities are listed in Appendix B. Later,
we refer to the environments Ant, HalfCheetah, Humanoid, Walker2d, and Hopper, as “locomotion”
tasks.
The FetchReach experiment involves a simulated 7-DoF robotic arm (Plappert et al., 2018). At the
beginning of each episode, a target point is uniformly sampled from a cubic region. The expert
controls the arm to reach the desired target within a tolerance range.
We explain more details about our experiment setup in Appendix B.
4.2	Evaluation
Visualization. In Figure 1, we visualize the generated trajectories of four locomotion tasks with
discrete modes. Throughout this figure, we can see that both SOG-BC and SOG-GAIL successfully
learn the different modes. Moreover, we observe that SOG-BC performs slightly more accurately
than SOG-GAIL. We discuss the differences between the two algorithms in a later paragraph, where
5
Under review as a conference paper at ICLR 2022
we compare them in terms of their robustness. In Figure 2, we visualize the velocities of Walker2d-
Vel-6 and Hopper-Vel-6 experiments under different policies. We observe that while SOG-BC suc-
cessfully distinguishes and reconstructs the desired velocities, the baseline models fail.

səl。二ɔ
Θo
9xαJUV
5PEpqIPM 工 JUV
9—qa—poupumH
Expert	VAE-GAIL	InfoGAIL
SOG-BC	SOG-GAIL (λS = 1)
Figure 1: Discrete latent variable: locomotion towards multiple directions. Trajectories of the imitated
policies for four tasks. Different latent codes at inference time are color coded. From top to bottom: number
of modes are 3, 2, 6, 6; number of trajectories per latent code are 1, 3, 3, 3. Both SOG-BC and SOG-GAIL
precisely separate and imitate the modes in all the experiments. However, VAE-GAIL and InfoGAIL fail to
separate the modes and to imitate the expert.
200 400 600 800	0	200 400 600 800	0	200 400 600 800	0	200 400 600 800
Expert	VAE-GAIL	InfoGAIL	SOG-BC
Figure 2: Discrete latent variable: locomotion at six velocities. Velocities generated by the policies vs
rollout time steps. Three rollouts per mode are visualized. Each mode is marked with a distinct color. Both
SOG-BC and SOG-GAIL separate and imitate most of the modes, producing the desired velocities. VAE-GAIL
and InfoGAIL exhibit an inferior performance.
SOG-GAIL (λs = 1)

sɪə/rpejəBEM g,ə/rjəddoH
6
Under review as a conference paper at ICLR 2022
In Figures 3 to 5, we visualize the results of continuous experiments FetchReach and
HalfCheetah-Vel. In these figures, SOG-BC and SOG-GAIL produce similar plots, hence we drop
the SOG-GAIL plot.
In Figure 3, we plot the trajectories of the Fetch robotic arm for different samples of the latent
variable. We observe that SOG-BC is able to control the robotic arm towards any point in the 3D
space. In Figure 4, we plot the reached targets for different choices of the latent code, and observe a
semantically meaningful interpolation in the latent space. This video contains animations of the test
rollouts.
(a) Expert
(b) VAE-GAIL
(c) InfoGAIL
(d) SOG-BC
Figure 3: Continuous latent variable: FetchReach. Sample trajectories of the robotic arm. In (b), (d) expert
trajectories are embedded and fed to the learned model for reconstruction. In (c), since InfoGAIL lacks any
module for encoding expert trajectories, no target point is considered, and random codes are used to generate
trajectories. Blue circles mark different targets. Colors of the trajectories are chosen at random for better visual
distinction. Compared to the baselines, SOG-BC better reaches different targets.
Figure 4: Self-organization in the latent space: FetchReach. In each plot, two dimensions among the three
dimensions of the latent space are fixed, whereas the third is varied. Each line corresponds to the location of
the robotic arm at the final state of different trajectories. In each line, two latent dimensions take a fixed value,
while the third varies. Lines are randomly colored for better visual distinction.
In Figure 5, we plot the spread of instantaneous horizontal velocities of the HalfCheetah agent as the
latent variable varies. In SOG-BC and InfoGAIL, the horizontal axis corresponds to the 1-D latent
code used to train the model. However, since VAE-GAIL requires a high dimensional latent variable
for best performance, the correspondence between latent codes and the model generated horizon-
tal velocities cannot be directly visualized. Therefore, we utilize the following fact: if the latent
embedding of an expert trajectories is fed to the learned policy, the generated trajectory produces
similar velocities as the expert trajectory. Thus, for VAE-GAIL we plot the variations in the gener-
ated velocities as a function of the target velocity of the corresponding expert trajectory. We observe
that compared to the baselines, SOG-BC better associates the latent variable to goal velocities in the
range [1.5, 3]. Also, SOG-BC produces a narrower error band, which indicates better certainty given
fixed latent codes. This video illustrates the imitated behavior of SOG-BC at different velocities as
the latent code varies.
Metrics. In Table 1, we use ground-truth reward functions to evaluate the collected rewards under
different policies. We explain these ground-truth rewards in Appendix B. In each entry of the table,
we report the best iteration in terms of the mean reward. Particularly, we calculate mean and std of
the rewards across 100 rollouts. To find the mode-specific reward for each latent code, we select a
7
Under review as a conference paper at ICLR 2022
-------------1-----------1------------
1.5	2.0	2.5	3.0
Desired Speed
0j------1--------1-------1--------r-
0.2	0.4	0.6	0.8
Latent Code
Oj------1--------1-------1-------r-
0.2	0.4	0.6	0.8
Latent Code
(a) VAE-GAIL
(b) InfoGAIL
(c) SOG-BC
Figure 5: Continuous latent variable: HalfCheetah-Vel. In (a), since VAE-GAIL uses a high latent di-
mension, in the horizontal axis we consider the desired velocity of the expert trajectory corresponding to the
latent embedding. However, in (b), (c) the horizontal axis corresponds to (the CDF of) the 1-D Gaussian latent
variable. The CDF is applied for better visualization. In SOG-BC, different velocities in range [1.5, 3] are
replicated with a higher certainty.
Table 1: Mean and standard deviation of the rewards for locomotion tasks.
Data set	SOG-BC	SOG-GAIL (λs = 1)	InfoGAIL	VAE-GAIL	Expert
Circles	992.1 ± 1.0	985.9 ± 10.8	766.0 ± 67.9	912.3 ± 8.6	998.3 ± 0.1
Ant-Fwd-Back	1165.2 ± 32.1	1101.0 ± 61.7	220.6 ± 296.3	-385.3 ± 67.0	1068.7 ± 109.7
Ant-Dir-6	1073.2 ± 206.6	1023.2 ± 166.1	-14.5 ± 87.6	-572.9 ± 62.9	1031.7 ± 253.7
HalfCheetah-Fwd-Back	221.6 ± 957.3	1532.6 ± 148.5	484.2 ± 919.4	84.0 ± 152.2	1686.0 ± 135.4
Humanoid-Dir-6	5996.0 ± 326.4	5457.8 ± 640.9	1333.94 ± 461.4	2285.5 ± 946.1	6206.6 ± 292.6
Walker2d-Vel-6	1915.3 ± 402.1	1698.7 ± 650.0	947.3 ± 105.3	1183.6 ± 68.0	1964.6 ± 394.6
Hopper-Vel-6	2222.3 ± 431.1	2015.30 ± 547.3	1216.8 ± 70.8	1065.8 ± 30.1	2229.7 ± 438.6
correspondence of latent codes to actual modes that gives the best expected sum of rewards. This
metric measures whether all modes are separated and properly imitated, because each mode has a
distinct reward function. Across all experiments, SOG-BC performs significantly better than the
baselines. The only exception is HalfCheetah-Dir, where SOG-GAIL performs better. We attribute
this latter observation to the difficulty of the task, which is alleviated by the explorations of GAIL.
In Table 2, we evaluate the FetchReach experiment through two metrics. First, we collect the
achieved targets under different policies, and clip them within the cubic region from which the
targets are sampled. Then, we quantify the desired uniformity of the distribution of the achieved tar-
gets over the cube. Since uniform distribution is the maximum entropy distribution over rectangular
supports, we alternatively estimate the entropy of the achieved targets via the method of Kozachenko
& Leonenko (1987). As a second metric, we embed expert trajectories, and measure the hit rate of
the reconstructed trajectories given the same targets as the expert.
In the HalfCheetah-Vel experiment, to measure the correspondence between the latent variable and
generated instantaneous horizontal velocities, we estimate their mutual information through the
method of Kraskov et al. (2004). The results are presented in Table 3.
Table 2: Metrics for reached targets in the FetchReach experiment: average hit rate and estimated entropy of
achieved targets (higher is better).
Metric	SOG-BC	SOG-GAIL (λs = 1)	InfoGAIL	VAE-GAIL	Expert
Entropy (nats)	2.05	1.89	0.27	0.85	2.18
Hit Rate	100%	97.0%	N/A	18.6%	100%
Table 3: Mutual information between the latent variable (target velocity of the embedded trajectory in the case
of VAE-GAIL) and generated velocities in HalfCheetah-Vel.
SOG-BC	SOG-GAIL (λs = 1)	InfoGAIL	VAE-GAIL
1.584	1.431	0.145	0.750
8
Under review as a conference paper at ICLR 2022
"VUeUDged
(a) Unperturbed policy
Figure 7: Optimal choice of λS. Parameter λS is the coef-
ficient of the SOG loss in Algorithm 3. Performance of Al-
gorithm 3 is illustrated as λS varies. We consider two cases
of with/without perturbations. These perturbations are im-
posed by taking random actions with a chance of 20%. The
vertical axis is normalized such that the best performance
in unperturbed settings achieves a score of 1, and a random
policy achieves 0. In Figure (b), we observe that the policy
trained with higher values of λS shows less robustness to
perturbations in several experiments .
"VUeUDged
T- Circles
-B- Ant-Fwcl-Baclc
Ant-Dlr-6
-B- HaIKheetah-Fwd-Back
→- HaIfCheetah-VeI
(a)	Goal (b) SOG-GAIL (c) SOG-BC
(b)	Perturbed policy
Figure 6: Robustness of SOG-BC vs SOG-
GAIL: Ant-Dir-6. To create unseen situa-
tions, we switch the latent codes to those cor-
responding to the directions shown in (a). In
(b), (c) we show three trajectories generated in
different colors. We observe that the ant agent
trained with SOG-GAIL performs the desired
task flawlessly, while the one trained with SOG-
BC often topples over and fails the task.

Robustness. We design two experiments to show that SOG-GAIL learns policies that are robust
towards unseen states. In the first experiment, upon generating test trajectories for Ant-Dir-6, we
switch the movement angles to all six possible directions. Since the motion angles within each expert
trajectory are consistent, such changes create unseen circumstances. We observe that maneuvers
with acute angles causes the model trained with SOG-BC to topple over and make the episode
terminate. In contrast, SOG-GAIL manages to switch between all six directions.
Secondly, we vary the coefficient λS in SOG-GAIL. High values of λS in limit lead to SOG-
BC. We observe that increasing λS improves scores in Table 1 across all experiments (except for
HalfCheetah-Dir). We now perturb each learned policy by taking random actions with a 20% prob-
ability. We observe that under this perturbation, several of the experiments perform worse at higher
values of λS . This indicates that combination of SOG with GAIL is more robust to perturbations.
The results of our two tests are respectively depicted in Figures 6 and 7.
4.3	Comparison
Our empirical results show that SOG-BC and SOG-GAIL distinguish and learn different modes of
behavior better than the baseline models, particularly in the discrete case. This advantage can be
attributed to the differences between the methods in inducing mode separation in GAIL. Specifi-
cally, VAE-GAIL and InfoGAIL directly modify the GAIL optimization to encourage multimodal-
ity. However, our method learns multiple modes in GAIL indirectly and through integration with
multimodal BC.
5	Concluding Remarks
We introduce a generative model that enables imitation of diverse behaviors when expert demon-
strations exhibit a mixture of behaviors. We provide theoretical analysis showing the objective of
our model is closely connected to marginalized likelihood of the data. the conditional distribution
between input and output variables, when the underlying relationship depends on an unobserved
variable. This model shows better performance for both cases of discrete and continuous latent vari-
ables. Across multiple complex experiments, we show that our model precisely reconstructs expert
trajectories, while simultaneously distinguishing the underlying modes. We also demonstrate that a
natural combination of our generative model with adversarial training boosts the robustness of the
imitated policies to unseen states.
9
Under review as a conference paper at ICLR 2022
References
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.
Christopher M Bishop. Pattern Recognition and Machine Learning. springer, 2006.
Michael Bloem and Nicholas Bambos. Infinite Time Horizon Maximum Causal Entropy Inverse
Reinforcement Learning. In 53rd IEEE Conference on Decision and Control, pp. 49l1-4916.
IEEE, 2014.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the Latent
Space of Generative Networks. arXiv preprint arXiv:1707.05776, 2017.
Siddhartha Chib. Marginal likelihood from the Gibbs output. Journal of the american statistical
association, 90(432):1313-1321,1995.
Cong Fei, Bin Wang, Yuzheng Zhuang, Zongzhang Zhang, Jianye Hao, Hongbo Zhang, Xuewu
Ji, and Wulong Liu. Triple-gail: a Multi-Modal Imitation Learning Framework with Generative
Adversarial Nets. arXiv preprint arXiv:2005.10622, 2020.
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. arXiv preprint
arXiv:1406.2661, 2014.
Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, and Joseph Lim. Multi-Modal
Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets. arXiv
preprint arXiv:1705.10479, 2017.
Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. arXiv preprint
arXiv:1606.03476, 2016.
Yedid Hoshen, Ke Li, and Jitendra Malik. Non-Adversarial Image Synthesis with Generative Latent
Nearest Neighbors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 5811-5819, 2019.
Rohit Jena, Changliu Liu, and Katia Sycara. Augmenting GAIL with BC for sample efficient imita-
tion learning. arXiv preprint arXiv:2001.07798, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
LF Kozachenko and Nikolai N Leonenko. Sample estimate of the entropy of a random vector.
Problemy Peredachi Informatsii, 23(2):9-16, 1987.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating Mutual Information. Phys-
ical review E, 69(6):066138, 2004.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable Imitation Learning from
Visual Demonstrations. arXiv preprint arXiv:1703.08840, 2017.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Josh Merel, Yuval Tassa, Dhruva TB, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne,
and Nicolas Heess. Learning human behaviors from motion capture by adversarial imitation.
arXiv preprint arXiv:1707.02201, 2017.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-
ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-
ment learning: Challenging robotics environments and request for research. arXiv preprint
arXiv:1802.09464, 2018.
10
Under review as a conference paper at ICLR 2022
Dean A Pomerleau. Efficient Training of Artificial Neural Networks for Autonomous Navigation.
Neural computation, 3(1):88-97,1991.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient Off-Policy
Meta-Reinforcement Learning via Probabilistic Context Variables. In International conference
on machine learning, pp. 5331-5340. PMLR, 2019.
StePhane Ross and Drew Bagnell. Efficient Reductions for Imitation Learning. In Proceedings
of the thirteenth international conference on artificial intelligence and statistics, pp. 661-668.
JMLR WorkshoP and Conference Proceedings, 2010.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A Reduction of Imitation Learning and Struc-
tured Prediction to No-Regret Online Learning. In Proceedings of the fourteenth international
conference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Confer-
ence Proceedings, 2011.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust Region
Policy Optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
Dimensional Continuous Control Using Generalized Advantage Estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
Optimization Algorithms. arXiv preprint arXiv:1707.06347, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from
unlabeled video. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pp. 98-106, 2016.
Ziyu Wang, Josh Merel, Scott Reed, Greg Wayne, Nando de Freitas, and Nicolas Heess. Robust
Imitation of Diverse Behaviors. arXiv preprint arXiv:1707.02747, 2017.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum Entropy Inverse
Reinforcement Learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling Interaction via the Principle of
Maximum Causal Entropy. In ICML, 2010.
11
Under review as a conference paper at ICLR 2022
A Optimization of GAIL
GAIL optimizes the objective of Equation (1) in two alternating steps: (1) a gradient ascent step to
increase it with respect to the discriminator parameters, and (2) a Trust Region Policy Optimization
(TRPO) (Schulman et al., 2015a) or Proximal Policy Optimization (PPO) (Schulman et al., 2017)
step to decrease it with respect to the policy parameters.
TRPO and PPO are policy optimization algorithms that take the biggest possible policy improvement
step without stepping so far that performance accidentally deteriorates. In particular, PPO achieves
this by optimizing the following “surrogate” objective at ith iteration:
LPPO (s, a, θi, θ)=min (π⅛a⅛ Aπθi (s, a), g(e, Aπθi (s, a))),⑸
where A is the advantage function (Schulman et al., 2015b), and g is a “clipping” function for some
small parameter > 0:
g(,A)=	((11-+))AA AA ≥< 00.
(6)
B Details of the Experiment Setup
In the Circles environment, the observed state at time t is a concatenation of positions from time
t - 4 to t. Expert demonstrations appear in three modes, each trying to produce a distinct circle-like
trajectory. The expert tries to maintain a (2π∕100) rad/s angular velocity along the perimeter of a
circle. However, its displacement at each step incurs a 10%-magnitude 2D Gaussian noise from the
environment.
To train an expert policy for MuJoCo locomotion tasks, we used Pearl (Rakelly et al., 2019), a few-
shot reinforcement learning algorithm. For the FetchReach experiment, we considered a pretrained
DDPG+HER model as expert (Lillicrap et al., 2015; Andrychowicz et al., 2017).
In Section 4, we measured the rewards of different experiments as follows. For the Circles exper-
iment, we applied a Gaussian kernel to the distance of the agent from the perimeter of each of the
three reference trajectories. Hence, each step yields a reward between 0 and 1. For all MuJoCo
locomotion environments, i.e. Ant, HalfCheetah, Walker2d, Hopper, and Humanoid, we used the
original rewards introduced in Rakelly et al. (2019). We ran all experiments with four random seeds
and presented the results for the best training iteration in terms of expected sum of rewards.
In experiment with varied velocities, having a suffix “Vel” in their name, target velocities of the
expert are sampled uniformly from the values listed in Table 4. As a reference, HalfCheetah has a
torso length of around 1 m.
Table 4: Set of possible expert velocities in different environments.
Environment	WaIker2d-Vel	Hopper-Vel	HalfCheetah-Vel
Possible Velocities	{-1.5,-1,-0.5, 0.5,1,1.5}	{-2, -1.5, 0.5, 1, 1.5, 2}	Interval of [1.5, 3]
Episode lengths in different environments are listed in Table 5.
Table 5: Episode lengths in different environments.
Environment	Circles	Ant	HalfCheetah	Humanoid	Walker2d	Hopper	FetchReach
Episode Length	1000	200	200	1000	1000	1000	50
We used the same network architectures across all algorithms. The policy network passes the states
and latent codes through separate fully connected layers, and then adds them and feeds them to the
final layer. The discriminator receives state-action pairs and produces scores for the GAN objective.
The posterior network for InfoGAIL has the same architecture as the discriminator, except it has a
12
Under review as a conference paper at ICLR 2022
action	score
(a) Policy network
(b) The discriminator
Figure 8: Neural network architectures. (a) the policy network, (b) the discriminator in GAIL.
final softmax layer in the discrete case. For continuous latent codes, InfoGAIL adopts a Gaussian
posterior. The architectures are illustrated in Figure 8.
For each of the algorithms SOG-GAIL, InfoGAIL, and VAE-GAIL, we pretrain the policy network
with behavior cloning for better performance. Since the policy network has an input head for the
latent code, in SOG-GAIL we provide the latent codes according to Algorithm 2. Also, because
InfoGAIL is not equipped with a module to extract the latent codes prior to training, we feed random
samples as the input latent codes during pretraining.
Across all experiments, we varied the coefficients λI, corresponding to the mutual information term
in InfoGAIL, and λs from SOG-GAIL (Algorithm 3), at values of λ = i * 10j; i = 1,2, 5; 0.01 ≤
λ≤1.
C Discrete Latent Variables
Expectation-maximization (EM) is a well-known method for optimizing the maximum likelihood
objective in the models involving latent variable. In this section, we demonstrate how the iterative
procedure of Algorithm 1 can be derived as a special case of the EM algorithm for Equation (4).
C.1 The EM Algorithm in General
First, we introduce the EM method in general. Let X = {xi}iN=1, Y = {yi}iN=1, Z = {zi}iN=1 con-
stitute a dataset generated by the procedure described in Section 3, and let Π = {πk }kK=1 denote the
set of probability masses for the discrete latent codes. The EM framework defines a total likelihood
function for an arbitrary distribution q(Z) as follows:
L(q(Z); θ, Π)=EZ〜q(z)log {p(Y, Z|X; θ, Π)} + H(q(Z))	⑺
=EZ〜q(z)log {p(YX； θ, ∏)} + Ezf(Z) log {P(ZX(Y; ' n) }	⑻
≡logp(data∣θ, ∏) - DKL(q(Z) || P(Z|X, Y； θ, ∏))	(9)
After initialization of model parameters, the optimization proceeds by alternating between two steps:
1.	Expectation Step (E Step).
N
q^+1(Z) - argmaxL® θt, ∏t) = P(ZIX, Y； θt, ∏t) ≡ Y r%	(10)
q	i=1
13
Under review as a conference paper at ICLR 2022
2.	Maximization Step (M Step).
θt+1, Πt+1 - arg max L(qt+1; θ, Π)	(11a)
θ,Π
=argmax Ez~qt+ι [log P (Y, Z |X; θ, Π)]	(11b)
θ,Π
NK
= arg max	ritk (log πk + logp(yi|xi, zi = k; θ))	(11c)
θ,Π	i=1 k=1
where
ritk=PpZ =k | xi, yi; θt, πt) = P=⅛⅛θ‰
(12)
Finally, we introduce the update rules in the M step for θ, Π. Since the M step does not have a
closed-form solution w.r.t. θ, on can instead perform a batch gradient ascent step. This approach
for an intractable M step is known as generalized EM (Bishop, 2006). Also, it is straightforward to
derive the following update rule for Π by maximizing the objective of Equation (11c) subject to the
constraint PkK=1 πk = 1:
∏k J ∣⅛⅛.
Pi rttι
(13)
The E step can be carried out in two ways:
1.	Soft. using all the posterior probabilities in Equation (12), so that each data point has a soft-
distribution over the latent codes at each iteration step. This corresponds to the original EM
algorithm above.
2.	Hard. approximating the posterior probabilities by a one-hot distribution, driven by the best
latent code for the M step. The method leads to Algorithm 1.
In the following subsections, we elaborate on each method.
C.2 Hard Assignment EM (Algorithm 1)
Hard assignment EM has a long history in the ML community and the assumptions made to derive
this algorithm are well known and are used to motivate and justify the K-means algorithm from an
EM perspective (Bishop, 2006). The underlying assumptions for hard EM are that πk = 1/K; k =
1, . . . , K , in the generative model, and that in the update equations, we replace the probabilities
rik in Equation (12) with binary values. This hard assignment is performed by allocating all the
probability mass to the categorical variable for which the posterior probability is the maximum.
Hence, the E step becomes
Ytk=P(Z = k | Xi yi, θt, Πt) = {0, kthe黑mink f(Zk，xi; θ) - yil1 ,	(14)
and the M step of Equation (11c) (w.r.t. θ) simplifies to:
arg min	min ||f (Z = k, x; θ) - y||2;	(15)
θk
θ	i=1
which is equivalent to the update step in Algorithm 1. For our generative model, the following
observations make the hard EM an appropriate choice:
1.	Joint learning of latent codes and the network parameters: The parameterized function
f(x, Z; θ), embodied by a large enough DNN, has almost-universal approximation capabil-
ity, and thus any mode in the dataset can be well approximated by setting a fixed one-hot code
Z, and training the network. Equally, if the modes are already known then one could train the
network jointly across all modes by assigning the same one-hot code for data points belonging
to the same mode. Of course, we do not know the mode of each data point, but for any given
θ, we can use the network to assign the most likely mode to a data point, and then update θ
to reinforce that it would most likely come true in the future: that is, the network is updated
so as increase the likelihood of the data point for the currently assigned mode. This is where
14
Under review as a conference paper at ICLR 2022
Figure 9: Self-organization and Mode Assignment: In the experiment described in Appendix C, the assign-
ment of the same winning latent code to data points belonging to the same mode in the ground truth data happens
very quickly. We create additional one dataset with less separation between ground truth modes compared to
the one we used in Appendix C. By the end of epoch 1 for data where the modes are very well separated, and
by the end of epoch 3 in data with modes that are closer together, the mode separation is almost complete.
the self-organization principles discussed Appendix E play a crucial role as well. Under certain
assumptions about the smoothness of f(z, x; θ), it shows that two sufficiently close data points
will have nearby winning latent codes (or the same winning latent codes in the discrete case
(see Proposition E.2). Thus, the data points belonging to the same data-mode will have the
same winning latent code, and thus boosting the specialization of the neural network to that
mode. The ground truth mode heterogeneity of data points assigned to any latent code will
quickly go to zero.
2.	Log likelihood of data in SOG vs soft EM: When the modes of the dataset are well separated,
as shown in Appendix C.3, soft EM also converges to approximately one-hot posterior codes.
Given any data point (x, y), the posterior probability of the winning code Z* at convergence,
are PEM(Z = Z* |x, y, θ) = 1 - eE, and let us assume that PSOG(Z = Z* |x, y, θ) = 1 -
S, and that S < E . Next consider the standard data likelihood expression for any k ∈
{1,...,K}:
logp(y |x; θ)	= logp(y |z = k x； θ) - log P(Z = klχ, y; θ)
、-----V--------} S----------------V-----------}	P(Z)
marginal data	-(Model Loss)	、	Sz	J
log-likelihood for a Model	gap
(16)
Assuming a balanced data set, we have P(Z = k) = 1/K, and putting Z = z* we get:
logp(y|x; θs) - logp(y|x; Θe)=
[logp(y |z = Z*, x； θs) - logp(y∣Z = Z*, x； Θe)] +log(1 - ∈e) Tog(I - es)∙ (17)
S--------V----------} S----------V----------}
-(SOG Loss)	-(EM Loss)
As discussed next, at convergence we expect -(SOG Loss) ≥ -(EM-Loss), while log(1 - E) -
log(1 - S) ≈ S - E < 0. Thus, the data log likelihood of both the models will be almost
the same at convergence.
C.3 Soft Assignment EM
First we observe that if the dataset has distinct modes, then the posterior distribution in the EM ap-
proach always tends to one-hot. For simplicity, let us assume that the modes in dataset are balanced
and have the same number of data points. This leads to uniform cluster prior distribution at conver-
gence: πkt → 1/K, k = 1, . . . , K. The posterior assignment distribution of a data point over the K
clusters (Equation (12)) is then simply proportional to the conditional likelihood for the clusters, i.e.
rt = P(Z = k | X y,； θt πt) = P(Ui|xi,z = k； θt) πk ≈ P(Ui区,z = k θt)
P	P P P	PK=IP(Ui|xi,z = l； θt) πl	PK=IP(Ui|xi,z = l； θt)
Consider a specific data point (xi , yi). Along the training using EM algorithm, at some step t, one
of the cluster, κ, will have higher likelihood compared to the alternatives (because the likelihood
conditioned on different clusters remaining perfectly equal is very unlikely), i.e.
P(yi | Xi ,Z = K θt) > P(yi | Xi ,z = 1； θt), if l = κ,	(18)
15
Under review as a conference paper at ICLR 2022
This leads to a higher posterior for this “winning” cluster compared to its alternatives:
ritκ > ritl ,	if l 6= κ,
Therefore, due to the M step, the “winning” cluster gains more weight in the loss function, and tends
to be improved more than the alternatives, so that Equation (18) remains to hold, and the “winning
p(yi | xi , z = κ)
margin ,———.------------,even increases.
p(yi | xi , z = l)
As the result of such positive feedback, the posterior distribution tends to one-hot and ritκ ≈ 1, while
ritl ≈ 0 for l 6= κ.
Now we show that this EM posterior can not be exactly one-hot and will always have a finite gap
when the likelihood function is assumed to be Gaussian, i.e. p(yi | xi, z) = N (y; fθ(z, x), σ2I),
and the gap is a function of σ . Because the Gaussian distribution is non-zero in the whole domain,
and the prediction function fθ has finite output, the posterior for the non-dominating clusters is
always finite, i.e. ritl > 0 is finite, even for l 6= κ. This has two consequences:
1.	The non-dominating clusters will have non-negligible contribution to the loss (unlike in the
case of SOG where the search is done over one-hot codes). The loss of EM algorithm (we
focus on the M-step, because it has direct correspondence to the loss of SOG) for a data point
(xi , yi ) is given by the expected negative log probability
L(θ) = - X ritl log p(yi | xi,z = l ; θ).
l
Now that the ritl is not one-hot, log p(yi | xi, z = l ; θ) will have contribution to the loss
for l 6= κ, making it larger than the loss attained by SOG, where ritl = 0 for l 6= κ. This is
confirmed by the plot Figure 12b in where the loss of EM drops significantly when calculated
after converting the posterior to exactly one-hot like in SOG.
2.	As a consequence, the contribution to the loss from the non-dominating clusters will keep
the convergence point slightly away from the optimal solution that maximizes the dominating
conditional likelihood p(yi | xi , z = κ ; θ).
Experiment setup. In this section, we illustrate some of the points we have made above regarding
soft and hard variants of EM, using a toy experiment. This experiment demonstrates that both
variants are able to recover the structure (three modes) in the dataset and illustrate the effectiveness
of Algorithm 1. We use a multi-modal linear generative model to generate the dataset of tuples
{(xi, yi,Zi)}N=ι. Concretely, Xi 〜N (0,I2), yi = Xi +wzi+/,where Zi 〜Categorical( 3, 1, 1),
Wi 〜N(0,0.012I2). That is, Xi ∈ R2 is drawn from a unit Gaussian distribution, and yi ∈ R2
is computed by offsetting Xi by amount of wzi ∈ R2 according to its corresponding mode zi ∈
{1, 2, 3}.
In both the EM and SOG we train a linear model fθ(z, X) = θ one-XOt(Z) , parameterized by
θ ∈ R5×2 on the data such that the marginal likelihood is maximized.
Results. The experiments on the toy example showed that both variants are able to produce satis-
factory fitting results. As we can see in Figure 10, both EM and SOG recover the structure (three
modes) existing in the ground truth Y (colors are randomly assigned to distinguish different modes).
Furthermore, as expected, the soft variant ultimately leads to one-hot posterior distribution among
latent codes in the discrete code case, which is assumed by SOG. Figure 11 shows the posterior
distribution from EM algorithm gradually converges from almost uniform distribution in epoch 2 to
one-hot in epoch 100 when its loss converges, which justifies SOG’s way of approximation to the
posterior. Although there are no significant visual distinctions among the synthesized samples in
Figure 10, we comment that SOG is more reliable to attain good performance in terms of loss value
and convergence speed compared to EM algorithm. Below we provide a detailed analysis.
In Figure 12a, the training curves of the two algorithms are plotted in both linear and logarithm
scale, and they show that SOG not only has a faster convergence speed, but also a lower loss at
convergence, despite the fact that the posterior distribution of the EM algorithm goes to one-hot
just like the hard-assignment used by SOG (shown in Figure 11). This raises the question: now
that EM behaves like SOG asymptotically, why does it have inferior performance? The reason is
that, limited by the Gaussian likelihood assumption (which is boundless), the EM approach can
16
Under review as a conference paper at ICLR 2022
0.5-
0.0-
-0.5-
4-1.0 -
-1.5-
-2.0-
-2.5-
-10	1
y∑
(a) The ground truth Y
0.5-
0.0-
-0.5-
-ι.o-
-1.5 -
-2.0-
-2.5-
-10	1
yι
(b) The EM predictions
0.5-
0.0-
-0.5-
£ -1.0-
-1.5 -
-2.0-
-2.5-
-10	1
yι
(c) The SOG predictions
Figure 10:	Three-mode clustering toy example. Ground truth and model predictions at convergence. Points
are colored-coded with the latent code assignments from the EM algorithm.
epoch = 0
epoch = 2
1000-

o
o

a
mode (k)
8
O
1000-
oɪn
0.0
epoch = 10
0.5
epoch = 100
0.5
I=I
I=I
I=I


1
2
3
posterior prob of mode k (η⅛)
posterior prob of mode k (η⅛)
Figure 11:	Posterior distribution of soft EM along training
never attain purely one-hot posterior, which is shown as follows. In particular, the EM approach
maximizes the expected likelihood of the data (where expectation is taken over all the latent codes),
whereas the SOG maximizes the likelihood of the data conditioned at the code where it is maximum.
Both of the consequences can be alleviated by decreasing the standard deviation σ assumed in the
Gaussian likelihood. Indeed, when we decrease σ by two orders of magnitude from its original value
1 to 0.01, the training curve ofEM and SOG match as shown in Figure 12c.
D Continuous Latent Variables
In this section, we analyse the behavior of the data log likelihood when SOG is applied for the
continuous latent variables case. Consider the following identity (Chib, 1995), a direct result of the
Bayes’ rule:
logp(y∣χ; θ) = iogp(y∣z, χ; θ) - log p(zlx, y;θ) ； ∀z.	(19)
×------{z------}	×--------V---------}	P(Z)
data	SOG objective | {z '	,
log-likelihood	gap
This identity describes a gap between the objectives in SOG and the data log-likelihood. The rest of
this section aims to show that under certain reasonable assumptions about the posterior distributions
of the latent codes, the SOG algorithm increases the overall data log-likelihood function and reaches
a maximum, before it starts overfitting, when the posterior distributions become too narrow. We
also show that such overfitting can be avoided, and the data log-likelihood be kept high by using a
limited number of latent code samples over which the minimum loss is calculated in the first step of
SOG.
17
Under review as a conference paper at ICLR 2022
algo
——EM
SOG
O 20	40	60	80 IOO
epoch
0	20	40	60	80	100
epoch
0	20	40	60	80	100
epoch
(a)	σ = 1 for EM. Upper: linear-
scale; lower: log-scale
(b)	σ = 1 for EM. Posterior is con-
verted to one-hot when calculating
the loss for plotting. Upper: linear-
scale; lower: log-scale
(c)	σ = 0.01 for EM. Upper:
linear-scale; lower: log-scale
Figure 12: The training curve of the EM algorithm vs that of SOG.
Algorithm 1 searches for a latent code z* that maximizes the SOG term above. In Equation (19),
the marginal data log-likelihood term is not a function of the latent variable. Therefore,
z * = arg max log p(y|z, x; θ)
z
argmaxlog P(zx y θ.
(20a)
(20b)
We assume the model to be realized with a high-capacity neural network. Hence, we can assume an
arbitrary form for the posterior distribution in Equation (20b), e.g. a Gaussian (Kingma & Welling,
2013):
P(ZX y; θ) = N(z; μθ(x, y), ∑θ(x, y)).
(21)
For a Gaussian form of the prior and the posterior, the gap in Equation (20b) can be written in closed
form:
z* = arg max —ɪ [(z — μ)TΣ-1(z — μ)T] — ɪ log det Σ + zττZ
z2	2	2
_ [(I — Σ)-1μ, if I — Σ » 0
unbounded,	otherwise
(22a)
(22b)
where μ and Σ denote a shorthand for μe(x, y) and ∑θ(x, y). Since the model is trained to create
mutual information between the data points and latent codes, the posterior distribution develops
more certainty than the prior, that is I > Σ. Hence, Equation (22b) yields z* = (I 一 Σ)-1μ.
18
Under review as a conference paper at ICLR 2022
Next, We use the z* derived above to obtain the gap term in closed form:
p(z|x, y; θ)
g(μ, ∑) ≡ max log —p(z)——	(23a)
=-1 [(z - μ)T∑-1(z - μ)T] I - 1 log det ∑	(23b)
2	lz=z*	2
=1 [zT(I - Σ-1)z + 2μTΣ-1z - μτΣ-1 μ] I	- 1Iogdet Σ (23c)
2	jlz=z*	2
=I(QTμ)τ(I	- Λ)-1(Qτμ) - Ilogdet Σ	(23d)
=1 μτ (Q(I -	Λ)Qτ)-1 μ - 1logdet	Σ	(23e)
=1 μτ (I - ∑)-1μ - 1logdet Σ	(23f)
≤ kμk2 PTr((I	- Σ)-2) - 1 log det Σ	(23g)
=kμk2 PTr((I	- Λ)-2) - 1 log det Λ	(23h)
≡ h(μ, Λ),	(23i)
where in Equations (23d) and (23h), the covariance matrix is diagonalized as Σ = QAQT. Also,
Equation (23g) follows from the fact that∀A, B 占 0; Tr(AB) ≤ PTr(A2) Tr(B2).
Now we more closely study the properties of the gap upper bound h(μ, Λ), and show that the gap
is not large under the following assumptions and arguments:
1.	Since Algorithm 1 optimizes its objective for latent codes sampled from a Gaussian prior dis-
tribution, μ remains bounded.
2.	Along the initial training phase of the model, the SOG objective increases and the gap term in
Equation (19) decreases, leading to a increased data log-likelihood.
Our multi-modal generative model with latent variable has a fundamental assumption: The
conditional distribution of the output variable y given the input variable x, can be related to the
latent variable z, such that y|(x, z) has less uncertainty compared to y|x. Quantitatively, this
means I(y; z | x) = H(z|x) - H(z|y, x) = H(z) - H(z|y, x) > 0, where I(y; z | x) is the
mutual information between y and z conditioned on x, H(z) is the information entropy of z,
and H(z|y, x) is the conditional entropy of z conditioned on x and y.
At the beginning of the training of the model, the mutual information I(y; z | x) is zero because
the model has no knowledge of the data, and no corresponding structure has been established
in the latent space yet. Along the training, the mutual information increases. For Gaussian
posterior p(z | x, y), this directly leads to a lower determinant of the covariance matrix, which
further leads to a decrease in h(μ, Λ), as shown in Figure 13.
3.	The Figure 13 shows that the gap becomes large when the posterior variance becomes too small.
This happens when the model overfits and a small region in the latent space is over-specialized
for certain data, hurting generalization ability.
However, such over-specialization can hardly happen in SOG, because the randomized search
of the latent code limits the “resolution” of the latent code choice and bounds the posterior
variance.
E	The Self-Organization Effect
In this section, we discuss how the self-organization phenomenon, that is the formation of simi-
lar output data in different regions of the latent space, naturally emerges in Algorithm 1. In the
following, we present three propositions demonstrating different aspects of the self-organization
phenomenon in the latent space, under the assumption that the function f (∙) is a sufficiently smooth
function of its inputs (x, z), e.g. it is Lipschitz continuous:
1.	Continuity in the data space: We discuss that for every data point, there is a neighborhood in
the data space such that for the same latent code z, the variations in the L2-loss in Algorithm 1
over all points in the neighborhood is small.
19
Under review as a conference paper at ICLR 2022
Figure 13: A contour plot demonstration of h(μ, Λ) for two-dimensional case. The axes correspond to the
diagonal elements of Λ = diag(σ2, σ2). In this example, kμk = 3.
2.	Two sufficiently close data points will have nearby winning latent codes (or the same winning
latent codes in the discrete case): We show that if for a data point, a particular latent code
results in a significantly better L2-loss than another latent code, then the same will also hold
within a neighborhood of that data point. Thus, if z is a winning code for a data point, then the
points in its neighborhood will also have winning codes close to z (or the same winning code
z in the discrete case).
3.	Network training can be a collaborative process: We demonstrate that if we update the network
parameters θ to improve the L2-loss for a particular data point (at a latent code z), then the same
update will also collaboratively improve the loss for other data points within the neighborhood
of the main data point (w.r.t. the same latent code z).
All three propositions provide insights on why the SOG training algorithm converges fast. In partic-
ular, in the mini-batch optimization in any implementation of the SOG algorithm, these propositions
imply that the parameter updates for a sample batch of data points, reduce the loss for points neigh-
boring to the sampled data points as well.
Formally, we shall make the assumption that the network f is a Lipschitz continuous function with
the constant M. We also define the δx -δy -neighborhood ofa data point (x, y) as B(x1, y1; δx, δy) =
{(x, y) | kx - x1 k < δx/M, ky - y11∣ < δy}. For later convenience, we define δ = δx+δy. Finally,
we denote d(x, y, z; θ) := ∣∣f (x, z; θ) - y)∣∣.
Proposition E.1. For any data point (x1, y1), if (x2, y2)	∈	B(x1 , y1; δx, δy), then
∣d(xι, yι, z; θ) - d(x2, y2, z; θ)∣ < δχ + δy = δ
Proof. From (x2, y2) ∈ B(xι, yι； δχ,δy), follows that ∣∣xι - x21∣ < δχ/M, ∣yι - y2∣∣ < δy.
Therefore:
Ik	f(XI,z;θ) - yιk - kf (X2,z; θ) - y2k | ≤ kf (xι,z;θ) - yι - (f (X2, z; θ) - y2)k (24a)
≤ kf (XI,z;θ) - f (X2, z; θ)k + kyι - y2k (24b)
≤ M∣X1 - X2∣ + ∣y1 - y2∣	(24c)
< δx + δy = δ.	(24d)
where Equations (24a) and (24b) follow reverse triangle inequality and triangle inequality, respec-
tively. Besides, Equation (24c) follows from the Lipschitz continuity assumption.	□
20
Under review as a conference paper at ICLR 2022
Proposition E.2. Let (x1, y1) be a data point, and zi, zj be two choices of the latent code.
d(x1, y1, zi; θ)
If-------------— > C > 1 for a constant C, and d(xι, yι, Zj; θ) is finite, then ∃δχ, δy such that
d(x1 , y1 , zj ; θ)
∀(X2, y2) ∈ B(xι, yi； δχ,δy),	d(x2, y2, Nz θ) > 1
d(x2, y2, zj; θ)
Proof. ∀η > 0, ∃δx, δy such that δx + δy = δ < ηd(x1, y1, zj; θ). Then using proposition E.1, the
following sequence of inequalities holds
d(x2,y2,zi) ≥ d(x1, y1, zi) - δ	(25a)
>Cd(x1,y1,zj)-δ	(25b)
>	Cd(x1, y1,zj) - ηd(x1, y1,	zj)	(25c)
= (C	- η)d(x1, y1,zj)	(25d)
>	(C	- η)(d(x2, y2, zj) - δ)	(25e)
>	(C	- η)(1 - 1-^) d(x2,	y2, Zj)	(25f)
where the step from (25e) to (25f) is done by realizing the fact that
ηd(χ2, y2, Zj) > η(d(χι, yi, Zj) - δ) > (1 - η)δ
and therefore δ < I-Lnd(x2, y2, Zj).
It can be easily verified that there always exists a small enough value ofη → 0+ such that
(C - η) (1-占)>1
and therefore
d(x2, y2, Zi) > d(x2, y2, Zj).
□
Proposition E.3. Let (xi, yi) be a data point and let Z be a choice of the latent code, respec-
tively. Also assume an improvement update of the network parameters θi to θ2 that ensures
d(xi, yi, Z; θi) > d(xi, yi, Z; θ2).
Then, there exists δx and δy such that
∀(x2, y2) ∈ B(xi, yi; δx, δy), d(x2, y2, Z; θ2) < d(mδxa,xδy)(θi),
where
d(Wy)(θ) = (X y)∈BmXaxrδ δ ) d(x，y，苞 θ)
(x,y)∈B(x1,y1 ; x, y )
is the largest reconstruction distance for data points in the neighborhood of(xi, yi)
Proof. Let a = d(xi, yi, Z; θi) - d(xi, yi, Z; θ2) > 0, and again δ = δx + δy, then ∀δx, δy, and
∀(x2, y2) ∈ B(xi, yi; δx, δy)
d(x2, y2, z； θ2) ≤ d(ma,Xy)(θ2)	(26a)
≤ d(xi, yi, Z; θ2) + δ	(26b)
≤ d(xi, yi, Z； θi) - a+δ	(26c)
≤ d(mδxa,xδy)(θi) -a+δ	(26d)
where Equation (26b) follows from Proposition E.1.
Since a is finite, ∃δx , δy such that δ < a, and hence
d(x2, y2, Z； θ2) < d(mδxa,xδy)(θi).
□
21
Under review as a conference paper at ICLR 2022
3
2J22z∕llr3
g
ʃ
y
q
ι
ɜ
S
ʒ
5
7
7
7
q
v
G
G
O
O
(b) Embedding of MNIST test data in 2 dimensions
(a) Synthesized MNIST digits



Figure 14: MNIST dataset. Output results of Algorithm 4 for the latent dimension of 2. (a) Synthesized
images (b) Embedding of MNIST test data in 2 dimensions by the search mechanism of the SOG algorithm.
Color coding represents ground truth digit labels. It is evident that despite the unsupervised training of the SOG
algorithm, data samples corresponding to each digits are evenly organized across the latent space.
F Extension of Algorithm 1 to Large Latent Spaces
Algorithm 1 suffers from an exponential growth of the number of samples required, as the latent
space dimension increases. To address this, one can consider a coordinate-wise search of the la-
tent code z , which results in Algorithm 4. This modification reduces the computational cost from
exponential to linear.
Algorithm 4 Coordinate-Wise Search in SOG
1:	Define: Loss function L(y, y) := ||y - y||2, block SiZe ∆
2:	for epoch = 1, 2, . . . do
3:	for iteration = 1, 2, . . . do
4:	Sample a minibatch of Ndata data points (xi, yi) from the dataset.
5:	for i = 1, 2, . . . , Ndata do
6:	InitialiZe latent code zi = 0 ∈ Rd
7:	for b = 1, 2,..., (d∕∆) do
8:	InitialiZe candidate latent codes zj := zi; j = 1, . . . , Nz.
9:	Replace the b’th ∆-element block in zj with a sample from N(0, I∆×∆); j =
1,...,Nz.
10:	Calculate zi := arg minzj L(fθ(zj,xi),yi).
11:	end for
12:	end for
13:	Calculate LSOG = PiN=da1ta L(fθ (zj, xi), yi) and its gradient w.r.t. θ.
14:	Update fθ by stochastic gradient descent on θ to minimiZe LSOG .
15:	end for
16:	end for
In Figures 14 to 16, we present visual results of Algorithm 1 demonstrating the capability of our
method in fitting complex distributions. The axes in Figures 14 and 15 are the CDFs of the Gaussian
latent codes for better visualiZation. We used the network architecture of the generator in Radford
et al. (2015).
22
Under review as a conference paper at ICLR 2022
U««
L_山山八
L⅛l5⅝h∙: - ∙





U

Figure 15: Fashion MNIST. Output results of Algorithm 4 for the latent dimension of 8. Each 6 × 6 block
(or block of blocks, etc.) sweeps over one dimension of the latent space. Due to limited space, only part of the
entire latent space is plotted. Full results are provided in the link (See the SummPlementary material zip file in
the review stage). Different latent dimensions have smoothly captured semantically meaningful aspects of the
data, i.e. type of clothing, color intensity, thickness, etc.
23
Under review as a conference paper at ICLR 2022
(a) CelebA ground-truth data
(b) Reconstruction by SOG algorithm
Figure 16: CelebA. Output results of Algorithm 4 for the latent dimension of 16. (a) Ground truth samples.
(b) Reconstructed samples synthesized by SOG. Note that the reconstructed images learned many of the salient
features of the original faces, including the pose of the faces, as well as the hairstyle in most cases.
24