Under review as a conference paper at ICLR 2022
BO-DBA: Query-Efficient Decision-Based Ad-
versarial Attacks via Bayesian Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Decision-based attacks (DBA), wherein attackers perturb inputs to spoof learning
algorithms by observing solely the output labels, are a type of severe adversarial
attacks against Deep Neural Networks (DNNs) that require minimal knowledge
of attackers. Most existing DBA attacks rely on zeroth-order gradient estimation
and require an excessive number (>20,000) of queries to converge. To better un-
derstand the attack, this paper presents an efficient DBA attack technique, namely
BO-DBA, that greatly improves the query efficiency. We achieve this by introduc-
ing dimension reduction techniques and derivative-free optimization to the process
of closest decision boundary search. In BO-DBA, we adopt the Gaussian pro-
cess to model the distribution of decision boundary radius over a low-dimensional
search space defined by perturbation generator functions. Bayesian Optimization
is then leveraged to find the optimal direction. Experimental results on pre-trained
ImageNet classifiers show that BO-DBA converges within 200 queries while the
state-of-the-art DBA techniques using zeroth order optimization need over 15,000
queries to achieve the same level of perturbation distortion.
1	Introduction
Recent advances in computation and learning have made deep neural networks (DNNs) an important
enabler for a plethora of applications. However, DNNs have also shown vulnerabilities to adversar-
ial examples - a type of maliciously perturbed examples that are almost identical to original samples
in human perception but can cause models to make incorrect decisions (Szegedy et al. (2013)). Such
vulnerabilities can lead to severe and sometimes fatal consequences in many real-world DNN ap-
plications such as autonomous vehicles, financial services, and robotics. Therefore, it is critical to
understand limitations of current learning algorithms and identify their vulnerabilities, which in turn
helps to improve the robustness of learning.
According to the knowledge of attackers, adversarial attacks can be categorized into three primary
types:white-box attacks, score-based attacks, and decision-based attacks. In the white-box setting
(Goodfellow et al. (2014); Madry et al. (2017)), the attacker requires complete knowledge of the
architecture and parameters of the target network. In score-based attacks (SBA), the attacker can
only access the queried soft-label output (real-valued probability scores) of the target model. In
decision-based attacks (DBA), also known as hard-label attacks, wherein only the hard label of a
given input is available. Fig. 1 illustrates the accessible information of the target model for each of
the three adversarial attacks.
Of the three attacks, DBA can lead to severe and ubiquitous threats to practical systems because
of the minimal required knowledge of the victim model, and has attracted great interests recently.
However, DBA is also the most challenging adversarial attack to design because of the relative in-
sensitivity of model outputs to input perturbation - it is often difficult for the attacker to determine
whether the change of perturbation is preferred or not when the target model’s prediction does not
change. To launch DBA attacks, an attacker shall discover the decision boundary where a slight
change of perturbation will cause the model to yield different prediction labels. Following this idea,
current research on DBA attacks formulates the problem of finding minimum adversarial perturba-
tion (to make the attack stealthy) into the problem of finding the direction resulting in a minimum
boundary radius. This approach is known as Cheng’s formulation (Cheng et al. (2018)). Because of
the smoothness of Cheng’s formulation, most existing works (Chen et al. (2020); Cheng et al. (2018;
1
Under review as a conference paper at ICLR 2022
Hidden Layers	Output Layer
Figure 1: An illustration of the three types of adversarial attack. A white-box attack can access to the whole
model; a score-based attack can access to the soft-label output layer; a decision-based attack can only access
the predicted label. With an unnoticeable perturbation, ”Pinwheel” is classified as “Rapeseed”
2019); Li et al. (2021)) solve the problem via zeroth-order optimization methods which require tens
of thousands of queries to estimate the gradient in every optimization step.
In reality, cloud-based machine learning platforms often set a limit on the allowed number of queries
within a certain period of time. For example, the Google cloud vision API currently allows 1,800
requests per minute. Therefore, improving query efficiency is critical for successful DBA attacks in
practical systems. While current research is mostly focused on improving the efficiency of zeroth-
order gradient estimation (Cheng et al. (2019); Chen et al. (2020); Li et al. (2021),Zhang et al.
(2021)), the overall query complexity of DBA attacks remains impractically high. A recent work
RayS (Chen & Gu (2020)) reduces the query complexity of (Cheng et al. (2018)) via hierarchical
searching which is a derivative-free method. However, the searching space of RayS is the original
data sample space which can be high dimensional and usually requires a large number of queries
to find the optimal solution. Essentially, RayS is a straightforward search-based approach without
exploitation of stochastic information like decision boundary radius distribution. These observations
raise the following question - can we solve Cheng’s formulation in lower-dimensional sampling
space via derivative-free optimization approach to achieve better query efficiency?
In this paper, we answer this question affirmatively by proposing an efficient DBA attack, namely
BO-DBA. We summarize our main contributions as follows:
•	We propose a novel decision-based adversarial attack named BO-DBA, which for the first
time solves Cheng’s formulation via a derivative-free approach in low dimensional sam-
pling subspace. We show that our method is more effective than previous decision-based
attacks in the terms of query efficiency and attack success rate.
•	Different from previous work, most of which rely on zeroth-order optimization methods
that introduce tremendous query cost during gradient approximation, we use Gaussian Pro-
cess to model an accurate probability distribution of decision boundary radius and find the
optimal direction via Bayesian Optimization.
•	Moreover, BO-DBA does not rely on the smoothness of Cheng’s formulation as in previous
zeroth order optimization methods with dimension reduction. Our method can support
more complex or non-differentiable functions to achieve dimension reduction and allow
the attacker to have better control over subspace selection.
•	We demonstrate the superior efficiency of our algorithm over several state-of-the-art
decision-based attacks through extensive experiments. For example, BO-DBA requires
75× fewer queries than zeroth-order optimization methods and 5× to 10× fewer than
RayS.
This rest of the paper is organized as follows: Section 2 reviews adversarial attacks and technical
preliminaries. Section 3 describes our technical intuition and Section 4 elaborates the design of
BO-DBA. Experimental results are provided in Section 5. We conclude the paper in Section 6.
2
Under review as a conference paper at ICLR 2022
2	Related work
2.1	Black-Box Adversial attacks
Black-box attacks are one type of adversarial attacks against learning systems where the attacker
has no knowledge about the model and can only observe inputs and their corresponding output by
querying the model. According to the types of model outputs, black-box attacks can be classified
into decision-based attacks (DBA) and score-based attacks (SBA). SBA attackers can access the
real-valued probability or the score of each output class while DBA attackers only can access the
output labels which may not necessarily be real-valued.
SBA attacks assume the attacker can access the real-valued confidence scores such as class probabil-
ities. Although there are some transfer based methods (Papernot et al. (2017; 2016)), effectiveness
of these methods is not quite satisfactory because a carefully-designed substitute model or even ac-
cess to part of the training data is required. On the other hand, optimization-based SBA aims to
approximate gradient via zeroth-order optimization methods. ZOO (Chen et al. (2017)) adopts the
zeroth-order gradient estimation to optimize the confidence score of perturbed inputs. One break-
through of SBA design was made by Ru et al. (2019) and Shukla et al. (2019), which only need less
than a thousand queries to fool the ImageNet classifier. Both attacks perform Bayesian optimization
on some low dimensional inputs and then up-sample them to perturbations using traditional image
up-sampling algorithm or deep generative models. Co et al. (2019) further improves the attack suc-
cess rate by designing a perturbation generator that can produce natural details using procedural
noises (Lagae et al. (2010)). Those Bayesian Optimization based methods model the distribution of
soft-decision over the input space to locate the possible adversarial examples that satisfy the lp norm
constraint.
DBA attacks are detrimental to learning systems because the minimal requirements on the knowl-
edge of attackers. There have been several DBA techniques in the literature. In Boundary Attack
(Brendel et al. (2017)), a perturbed example is generated starting with a large perturbation sampled
from a proposed distribution. It then iteratively reduces the distance of the perturbed example to
the original input through a random walk along the decision boundary. Opt-Attack (Cheng et al.
(2018)) first proposed Cheng’s formulation which turns DBA problem into the problem of finding
the optimal direction that leads to the shortest l2 distance to decision boundary and optimized the
new problem via zeroth-order optimization methods. Chen et al. (2020) and Cheng et al. (2019)
furthermore improve Cheng et al. (2018)’s query efficiency via estimating the sign of gradient or
optimizing the hyperparamters of optimizing procedural. Recently, Zhang et al. (2021) further re-
duce the query complexity of zeroth-order gradient estimation by projecting the input space into
a low dimensional subspace as long as the projection does not violate the smoothness of Cheng’s
formulation. Meanwhile, Chen & Gu (2020) explores solving the Cheng’s formulation via gradient
free methods like hierarchical searching on input dimension.
2.2	Bayesian Optimization
Bayesian optimization (BO) is a sequential optimization method particularly suitable for problems
with low dimension and expensive query budgets Mockus (2012) such as black-box optimization. It
contains two main components - a probabilistic surrogate model, usually a Gaussian Process (GP),
for approximating the objective function, and an acquisition function that assign a value to each
query that describes how optimal the query is.
Gaussian Process is a statistic surrogate that induces a posterior distribution over the objective
functions RasmUssen (2003). In particular, a Guassian Process GP(μo, ∑o) can be described by a
prior mean function μo and positive-definite kernel or covariance function ∑o. In this paper, We
adapt the Matern 5/2 Kernel Shahriari et al. (2015) as the covariance function, which is defined as:
ς(X,x0) = (I + √5r + 5r2 )exp(-√5r)
Where r = x - x0 and l is the length-scale parameter Snoek et al. (2012).
Acquisition Function in Bayesian optimization is a function that evaluates the utility of model
querying at each point, given the surrogate model, to find the optimal candidate query point for
3
Under review as a conference paper at ICLR 2022
the next iteration Brochu et al. (2010). Expected Improvement (EI) and Upper Confidence Bound
(UCB) are the two most popular acquisition functions that have been shown effective in real black-
box optimization problems Shahriari et al. (2015). In black-box adversarial attacks, most studies
Shukla et al. (2019); Co et al. (2019) adopted EI as the acquisition function because of its better
convergence performance Shahriari et al. (2015); Snoek et al. (2012). In this paper, we also use EI
as the acquisition function which is defined as:
EIn(X) = Enmax(h(x) — hn, 0)]
where h is the objective function and hn is the best observed value so far. En[∙] = En[^|Di：n-i]
denotes the expectation taken over the posterior distribution given evaluations of h at xι, •…，xn-ι.
3	Technical intuition
In this section, we introduce the technical intuition of BO-DBA attack. We first take a overview of
the Cheng’s formulation and analyze the limitation of previous proposed gradient-based methods.
Then we describe the motivation of our design.
3.1	Overview of Cheng’ s formulation
The classification model F takes images as inputs and outputs a K—dimensional vector which
represents confidence scores over K—classes (we will take images as examples in the rest of this
paper). In decision-based setting, we can define a Boolean-value objective function hb : [0, 1]d →
{—1, 1} as following:
hb(γ)
sign{max[Fi(x + γ)] — Fy(x + γ)}
i6=y
sign{Fk (x + γ) — max[Fi(x + γ)]}
i6=k
(Untargeted)
(Targeted)
(1)
where x ∈ Rd is the targeted data sample and y ∈ {1...K} is its true label. γ ∈ Rd is the perturbation
added to the input data. k ∈ {1...K} represents the target label. Notes that the output of F is
unavailable for decision-based attacker. So the objective function hb can be consider as a black-box
function:
hb(γ) =	1—1
Attack success
Attack failed
(2)
Obviously, directly maximize hb is very difficult because hb is neither continuous nor differentiable.
To overcome this problem, Cheng et al. (2018) reformulate the decision-based attack problem as:
min g(θ) where g(θ) = arg min hb(x0 + ∆θ) = 1
θ	∆>0
(3)
In Cheng’s formulation, g(θ) represents the decision boundary radius from input x along the ray
direction theta. Then the DBA attack problem can be converted to find the ray direction with min-
imum decision boundary radius regarding the original example x. While most of the prior works
focusing how to solve the formula by estimating the gradient through zeroth order optimization, the
decision-only access makes solving (3) query inefficient. Specifically, the decision boundary radius
g(θ) is typically estimated by a binary search procedure and approximation of the gradient of g(θ)
via finite difference requires multiple rounds of computation of g(θ). RayS, on the other hand, adopts
hierarchical searching to solve Cheng’s formulation in a gradient-free fashion. However, straight-
forward searching will discard stochastic information that can be utilized in optimization methods
like stochastic gradient or distribution of decision boundary radius. Moreover, RayS conducts the
searching process on input space directly which will also introduce significant query complexity
especially when the input dimension is large (e.g. color images).
In order to overcome the problems mentioned above, we propose BO-DBA attack which contains
two critical design: (1) Bayesian Optimization is utilized to directly find the ray direction with the
highest probability to generate the closest decision boundary and (2) a perturbation generator is
adopted to reduce the dimension of the input search space.
4
Under review as a conference paper at ICLR 2022
3.2	Technical Intuition
We first discuss how to reduce the input space by involving the perturbation generator in Cheng’s
formulation. We define perturbation generator as a function S : Rd0 → Rd that takes low dimen-
sional inputs δ0 ∈ Rd0 (d0 << d) and outputs an image-size perturbation δ ∈ Rd. Then we can
formulate our objective function g0 (δ0) as:
min g0(δ0) where g0(δ0) = arg min (hb(x0 + ∆ 4 S(：) ) = 1)	(4)
δ	∆>0	∖	|S (δ0)∣	)
In this formulation, we define the search direction in (3) using normalized perturbation generated
by S: θ = ∣S(δ0)∣. The value of g0 Can be evaluated Via multiple decision-based queries which We
will discuss in section 4. Note that, although prior work (Zhang et al. (2021)) has also adapted a
projection matrix to reduce the searching space in decision-based attack, Zhang et al. (2021) still
aims to solve problem (3) by approximating gradient g0(θ). This requires the projection matrix to
preserve smoothness of the objective function. On the other hand, our method adopts derivative-free
optimization which allows us to support more complex or non-differentiable perturbation genera-
tion functions to gain better control over searching space selection. For example, the Perlin noise
generator will reduce the input space from all possible images into images of Perlin noise.
Figure 2: Workflow of Bayesian Optimization in BO-DBA
With objective function g0(δ0) available, the optimization problem (4) is solvable using the Bayesian
Optimization. Adopting the logic in Figure (2), the attacker will query the boundary radius g0(δ0)
on the searching direction θ generated by the low dimensional input δ0 . The optimizer models the
distribution of distances over the input space and acquires the next most possible optimal input for
querying until an adversarial example near enough is found. In particular, for each iteration t, based
on observation set {δi0, g0(δi0)}it=-11, we use Gaussian process to model the radius distribution of all
possible directions. Then we use Acquisition Function to select δt0 with the highest probability to
generate the lowest radius (smallest perturbation) according to the statistic distribution. Then we
query the model to compute g0(δt0) and add the result {δt0 , g0(δt0)} to the observation set.
Compared to the score-based methods that also adopt BO (Co et al. (2019); Ru et al. (2019)) (which
formulates the SBA attack as a constrained optimization problem that maximizes the probability
score of incorrect label), our optimization framework focuses on optimizing the boundary distance
which can be evaluated via querying the decision-based model solely. Moreover, our algorithm does
not rely on predefined distortion constraints like score-based BO, where the attacker needs to define
the required boundary distance beforehand to trade the success rate for perturbation quality.
4 Decision-based Bayesian Optimization Attack
In this section, we describe an optimization framework for finding adversarial instances for a classi-
fication model F in detail. First we discuss how to compute g0 (δ0) up to certain accuracy using the
Boolean-valued function hb . Then we will solve the optimization problem via Bayesian Optimiza-
tion and present our full algorithm.
5
Under review as a conference paper at ICLR 2022
Algorithm 1: Distance Evaluation Algorithm
input : Boolean-valued query function hb of target model, original image x0, low dimensional
input δ0 , increase step size η, stopping tolerance , maximum distance ∆max
output: g0(δ0) θ J於	// Compute the searching direction
// Fine-grained search
if hb (x0 + ηθ) = -1 then
Vlow - X0 + ηθ, Vhigh - X0 + 2ηθ;
while hb (vhigh) = -1 do
vlow《-Vhigh, Vhigh《-Vhigh + ηθ；
if |Vlow | ≥ ∆max then
r return g0(δ0) = ∆maX
end
end
else
I Viow J 0, Vhigh J xο + ηθ;
end
// Binary search between [Vlow, Vhigh]
while |Vhigh - Vlow > | do
Vmid J (Vhigh + Vlow)/2;
if hb(Vmid) = -1 then
I Vhigh j- Vmid;
else
I Viow j- Vmid;
end
end
return g0(δ0) = |Vhigh|;
4.1	Distance Evaluation Algorithm
Algorithm 1 elaborates how to evaluate g0(δ0) via queries on Boolean-value function hb:
First, the attacker computes the search direction locally θ = ∣S(∖)∣. For a given low dimensional
input δ0, attacker first generates an image-size perturbation S(δ0) via the perturbation generator S.
Then normalize S(δ0) into a unit vector ∣S(')∣ to represent the search direction。. It is easy to notice
that for any given input δ0, there is always a search direction θ that can be computed.
To evaluate the distance from input x0 to the decision boundary along the direction θ, the attacker
performs a fine-grain search and then a binary search. For simplicity, we assume the l2 distance
here, but the same procedure can also be applied to other distance measurements as long as vector
operations are well defined in their respective spaces. In the fine-grained search phase, we cumu-
latively increase the search distance to query the points {x0 + ηθ, x0 + 2ηθ, . . . } one by one until
hb(x0 + iηθ) = 1. Then we conduct a binary search between the interval [x0 + (i - 1)ηθ, x0 + iηθ],
within which the classification boundary is located. Note that, in practice the fine-grained search
may exceed the numerical bounds defined by the image (or other type of samples). We can simply
assign a maximum distance (e.g., the distance between all-black image and all-white image) for this
searching direction. Unlike the gradient-based method that needs an accurate result to evaluate the
gradient, Bayesian Optimization only needs statistical knowledge about each direction.
4.2	Bayesian Optimization
The detailed procedure of BO-DBA is presented in Algorithm 2. At beginning, we sample T0 ran-
dom low dimensional inputs δ0 from the input space and evaluate the distance g0(δ0) using Algorithm
1. Then we iteratively update the posterior distribution of the GP using available data D and query
new δ0 obtained by maximizing the acquisition function over the current posterior distribution ofGP
until a valid adversarial example within the desired distortion is found or the maximum number of
6
Under review as a conference paper at ICLR 2022
Algorithm 2: Bayesian Optimization for DBA
input : Targeted input x°, Guassian process model GP, Acquisition function A, Initialization
sample size T0, Maximum sample size T, Distance evaluation function g0(∙), stopping
tolerance e, D = 0.
output: Adversarial Examples x0
// Intialization
fort = 0, 1, 2..., T0 - 1 do
Generate input δ0 randomly;
D 一 D ∪(δ0,g0(δ0));
end
Update the GP on D;
// Optimization via GP and Acquisition function
while t < T do
t V— t + 1;
δt0 V arg maxδ0 A(δ0, D);
if ∣g0(δ0)| > e then
I D — D ∪ (δ0, g0(δ0)) and update the GP;
else
θ — S(δ0).
θ = Wtπ;
return x0 + g0(δt0)θ;
end
end
// Return nearest adversarial example
θ = ∣⅛⅛ | (δ∖ g(δD) ∈ D SUch that g(δ',) ≤ g(δ0) ∀(δ0, g0(δ0)) ∈ D;
return xo + g0(δQθ;
iteration is reached. Note that the query budget shall be larger than the number of iterations because
we need multiple queries to evaluate the distance in Algorithm 1. The alternative stop condition of
the optimization procedure is to set a maximum acceptable query budget.
5	Experiments
In this section, we carry out an experimental analysis of our BO-DBA attack. We first compare
BO-DBA with other decision-based attack baselines on naturally trained models and models with
run-time adversarial example detection (Xu et al. (2017)). Then, we examine how different types
of perturbation generators affect attack success rate and perturbation quality. All experiments are
carried out on a 2080 TI GPU, with code available online.1
5.1	Performance Evaluation
BaSelineS: We first compare our algorithm with opensouced state-of-the-art decision-based attacks:
Opt-Attack (Cheng et al. (2018)), HSJA (Chen et al. (2020)), SignOPT (Cheng et al. (2019)) and
RayS (Chen & Gu (2020)). For those attacks, we adopt the same hyperparamter settings in orignal
papers.
Data and ModelS: We use two distinct DCN architectures pre-trained on ImageNet (Deng et al.
(2009)): ResNet-50 (He et al. (2016)) and Inception V3 (Szegedy et al. (2016)). ResNet-50 takes
input images with dimensions 224 × 224 × 3 while Inception V3 take images with dimensions
299 × 299 × 3. In addition to defenseless, we also carried out experiments on a classifier that has
the run-time adversarial sample detection function (Xu et al. (2017)). The function detects abnormal
inputs by comparing a DNN model’s prediction on the original input with that on squeezed inputs
(by reducing the color bit depth of each pixel or spatial smoothing).
1https://github.com/zzs1324/BO-DBA.git
7
Under review as a conference paper at ICLR 2022
Metrics: To measure the efficiency, we use the average l∞ distance between perturbed and original
samples over a subset of test images. For each method, we restrict the maximum number of queries
to 1000. As an alternative metric, we also evaluate the attack success rate (ASR). An adversarial
attack is considered a success if the distortion distance between generated adversarial examples
and original image does not exceed a given distance threshold. In this paper, we use the distance
threshold (l∞ ≤ 哉)to define the ASR.
ResNet, No Detection	Inception, No Detection	ResNet, Detection	Inception, Detection
8uss5-
8us--0 8-
QUEffltt-Q 8-
QUEffltt-Q 8-
esccSseuns
O 200	400	600	800 IQQO	O 200	400	600	«00 IOOO	O 200	400	600	800 IQOO	O 200	400	600	800 lβ(K
——BO-DBA ——HSJA ——Opt-Attack ——RayS ——SignOPT
Figure 3: Average distance versus query budgets on ImageNet with ResNet, Inception, and ResNet with adver-
sarial example detection, and Inception with adversarial example detection from left to right columns. 1st row:
untargeted l∞ distance. 2nd row: untargeted Attack Success Rate
Results: Figure 3 shows the average l∞ distance and attack success rate against the query bud-
gets. Column 1&2 compares the l∞ distortion and attack success rate of our framework with four
baseline DBA attacks on the defenseless classifier. We can see that BO-DBA consistently achieves
a smaller distortion within 1000 queries than baseline methods. As a derivative-free method, BO-
DBA can converge within 200 query budgets, while zeroth order optimization based techniques like
OPT-Attack, HSJA and SignOPT2 need over 15,000 queries to achieve the same level of pertur-
bation distortion (Chen et al. (2020)). Although RayS adopts another derivative-free method, it is
able to achieve similar perturbation distortion only after around 1000 queries. The obvious advan-
tage of query efficiency of BO-DBA is mainly attributed to facts: 1) BO-DBA adopts the Bayesian
Optimization to utilize the statistical information while RayS relies soley on a straightforward hi-
erarchical searching; 2) BO-DBA reduces the searching space via perturbation generators, which
results in a much higher convergence rate. Similar results can be seen in the Attack Success Rate as
shown in row 2 of Figure 3. Column 3&4 of Figure 3 shows the l∞ distortion and the attack success
rate against the number of queries for all baseline methods on classifier equipped with adversarial
example detection mechanism. Again we can see that BO-DBA achieves the highest overall attack
success rate and best query efficiency as compared with the other four hard-label attack baselines.
5.2	Effect of Perturbation Generator
Baselines: We explore the influence of different perturbation generators on attack efficiency and
perturbation quality when combined with our framework. In general, we can divide the tested per-
turbation generators into three types: procedural noises generatorsCo et al. (2019), interpolation-
based functionRu et al. (2019) and clustering-based function Shukla et al. (2019). Procedural
noises use the random function to generate an image with complex and intricate details which are
widely used in computer graphicsLagae et al. (2010). For procedural noises, we consider Perlin
and Gabor noise. Interpolation-based functions are widely used in image rescaling and we consider
bilinear (BILI) and bicubic (BICU) interpolation. For the clustering-based function, we consider
nearest-neighbor (NN) and clustering (CL).
2Note that the relatively weak performance of SignOPT is due to the fact that SignOPT is designed for l2
norm attack while this experiment is under the l∞ norm setting.
8
Under review as a conference paper at ICLR 2022
Generator	UAR	ASR	LPIPS	12	L∞
Perlin	26.9%	87%	0.087	9.28	0.035
Gabor	29.4%	77%	0.135	15.70	0.842
BILI	4.7%	27%	0.158	43.44	0.279
BICU	5.9%	26%	0.155	42.14	0.266
CL	9.0%	67%	0.259	18.92	0.862
NN	11.9%	55%	0.115	24.94	0.891
Table 1: Perturbation Generators Evaluation
Metrics: In addition to the same evaluation matrix used in Section 5.1, we also measure the inherent
properties of perturbation generators like University evasion rate (UAR)Co et al. (2019) which refers
to perturbation’s ability to apply across a dataset or to other models. Given a model f, a perturbation
s, input x ∈ X and a threshold , the UAR of s over X is:
|{x ∈ X : arg max f(x + s) 6= τ (x)|
|X|
∣s∣∞ ≤ E
WhereT(X) is the true label of X and Weset e = 255.
Results: Table 1 compares the perturbation quality and inherent properties of different perturbation
generators. We can see that perturbation generators that belong to the same category have similar
inherent properties. For example, procedural noise generators have a higher UAR than other genera-
tors. In terms of distortion quality, We found each noise generator has a distinct distortion quality in
different distortion measurements. For example, NN generator has relatively high 12GI∞ distortion
but loW LPIPS, Which means this type of noise usually has less perceptional significance than value
significance.
6	Conclusion
In this paper We introduce a neW decision-based attack BO-DBA Which leverages Bayesian opti-
mization to find adversarial perturbations With high query efficiency. With the optimized perturba-
tion generation process, BO-DBA converges much faster than the state-of-the-art DBA techniques.
As compared to existing decision-based attack methods, BO-DBA is able to converge Within 200
queries While the state-of-the-art DBA techniques need over 15,000 queries to achieve the same level
of perturbation distortion.
References
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, With application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.
Jianbo Chen, Michael I Jordan, and Martin J WainWright. Hopskipjumpattack: A query-efficient
decision-based attack. In 2020 ieee symposium on security and privacy (sp), pp. 1277-1294.
IEEE, 2020.
Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 1739-1747, 2020.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural netWorks Without training substitute models. In
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26, 2017.
Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-
efficient hard-label black-box attack: An optimization-based approach. arXiv preprint
arXiv:1807.04457, 2018.
9
Under review as a conference paper at ICLR 2022
Minhao Cheng, Simranjit Singh, Patrick Chen, Pin-Yu Chen, Sijia Liu, and Cho-Jui Hsieh. Sign-opt:
A query-efficient hard-label adversarial attack. arXiv preprint arXiv:1909.10773, 2019.
Kenneth T Co, LUis Munoz-Gonzalez, Sixte de Maupeou, and Emil C Lupu. Procedural noise
adversarial examples for black-box attacks on deep convolutional networks. In Proceedings of
the 2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 275-289,
2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Ares Lagae, Sylvain Lefebvre, Rob Cook, Tony DeRose, George Drettakis, David S Ebert, John P
Lewis, Ken Perlin, and Matthias Zwicker. A survey of procedural noise functions. In Computer
Graphics Forum, volume 29, pp. 2579-2600. Wiley Online Library, 2010.
Huichen Li, Linyi Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, and Bo Li. Nonlinear projection
based gradient estimation for query efficient blackbox attacks, 2021.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Jonas Mockus. Bayesian approach to global optimization: theory and applications, volume 37.
Springer Science & Business Media, 2012.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519, 2017.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63-71. Springer, 2003.
Binxin Ru, Adam Cobb, Arno Blaas, and Yarin Gal. Bayesopt adversarial attack. In International
Conference on Learning Representations, 2019.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):
148-175, 2015.
Satya Narayan Shukla, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Black-box adversarial
attacks with bayesian optimization. arXiv preprint arXiv:1909.13857, 2019.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
10
Under review as a conference paper at ICLR 2022
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. arXiv preprint arXiv:1704.01155, 2017.
Jiawei Zhang, Linyi Li, Huichen Li, Xiaolu Zhang, Shuang Yang, and Bo Li. Progressive-scale
boundary blackbox attack via projective gradient estimation. arXiv preprint arXiv:2106.06056,
2021.
11