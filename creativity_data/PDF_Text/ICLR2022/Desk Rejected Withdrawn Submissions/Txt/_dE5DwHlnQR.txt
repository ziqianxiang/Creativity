Under review as a conference paper at ICLR 2022
Informative Robust Causal Representation
for Generalizable Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
In many real-world scenarios, such as image classification and recommender sys-
tems, it is evidence that representation learning can improve model’s performance
over multiple downstream tasks. Existing learning approaches rely on establishing
the correlation (or its proxy) between features and the downstream task (labels),
which typically results in a representation containing cause, effect and spurious
correlated variables of the label. Its generalizability may deteriorate because of
the unstability of the non-causal parts. In this paper, we propose to learn causal
representation from observational data by regularizing the learning procedure
with mutual information measures according to our hypothetical causal graph.
The optimization involves a counterfactual loss, based on which we deduce a
theoretical guarantee that the causality-inspired learning is with reduced sample
complexity and better generalization ability. Extensive experiments show that the
models trained on causal representations learned by our approach is robust under
adversarial attacks and distribution shift.
1	Introduction
Learning representations from purely observations concerns the problem of finding a low-dimensional,
compact representation which is beneficial to prediction models for multiple downstream tasks. It
is widely applied in many real-world applications like recommendation system, searching system
etc.(Sun et al., 2018; Okura et al., 2017; Zhang et al., 2017; Shi et al., 2018). Generally, the feature
representation learning is to map the collected features into a representation space, which is then
leveraged for training a prediction model. Most of the representation learning methods are designed
based on the information of correlation between the feature and downstream labels only. Some
recent works reckon that the representation produced by this schema is not able to achieve robustness
and generalization (ScholkoPf et al., 2021; Peters et al., 2017; ZhoU et al., 2021), since spurious
correlated features miss essential information which actually influence the system (Pearl, 2009).
For example, obtaining thermometer enables the prediction of the air temperature, but manually
putting the thermometer into hot water does not change the air temperature. It is obvious that
thermometer scale is not always a stable feature to predict temperature, because non-causal relations
exist. Therefore, the causal information rather than correlation is more robust and general for
prediction models.
Causal representation learning is an effective approach for extracting invariant, cross-domain stable
causal information, which is believed to be able to improve sample efficiency by understanding the
underlying generative mechanism from observational data (Scholkopf et al., 2021; Ay & Polani,
2008). Recently, some approaches learn the causal representation making use of certain causal
property provided by the specified priors. One perspective is to learn feature representations with its
structure specified by causal models, assuming that the raw data contains both cause variables and
effect variables, like causal disentangled representation learning form images (TraUbIe et al., 2020;
Yang et al., 2021; Shen et al., 2020; Suter et al., 2019; Dittadi et al., 2020) and physical environments
(Ahmed et al., 2020; Sontakke et al., 2021). The representations have good interpretability, but
probably suffer from redundancy because the learning process may generate some parts unrelated
to the downstream tasks. Another perspective is to utilize the causal relation between features and
labels (Suter et al., 2019; Kilbertus et al., 2018; Scholkopf et al., 2012; Wang & Jordan, 2021). They
are interpreted as causal or anti-causal learning, tackling cases such as the causal direction between
observational feature set X and downstream labels Y is X → Y and finding the cause information for
1
Under review as a conference paper at ICLR 2022
Figure 1: The figure demonstrates a typical case of a causal system, and we extend to two more
general settings: (a) the causal graph without confounders, (b) the causal graph with confounder.
downstream prediction. This only covers a set of simple cases like handwritten numeral recognition,
where the handwritten numeral X is produced by the number Y in brains. However, its effectivity is
not guaranteed in circumstances where causal graph is complex and unknown previously.
In this paper, we deal with the general problem of learning minimal sufficient causal representations
for downstream prediction. Considering more complex scenario like recommendation system and
fault detection system. Fig. 1 (left) describes the relations among observations and predicted
labels, which can be generalized to the setting of Fig. 1 (a)(b). Generally, the observational data
X = [paY, ndY, dcY, ncY], which consists of a mixture of the minimal sufficient parent modes
paY, non-descendants ndY, descendants dcY or uncorrelated features ncY ofY. For unconfounded
case, Fig. 1 (a), we formalize a basic causal graph to describe relations among paY , ndY , dcY .
Note that no edge between ndY and Y exists, otherwise one can make adjustments by merging
ndY into paY or dcY . In confounded case, Fig. 1 (b), we extend the basic one by considering
causal edges between the nodes in paY or ndY with dcY . To learn a good representation Z from X
consisting of the aforementioned components, we propose an approach from an information-theoretic
perspective. Treating the information propagation along the causal graph as a natural generative
process, a fact is that the information contained in root paY suffers from degradation along causal
paths, based on which we deduce inequalities on the mutual information between node pairs and
design a method to learn the paY from observational data. Based on the inequality, we deduce
a tractable mutual information based causal representation learning objective. To find a robust
paY , we take a perspective of adversarial attack, and propose an counterfactual vulnerable (CV)
term to quantify the necessary and sufficient representation of causal information. In addition, we
theoretically analyze the generalization ability under finite sample by information theory, and provide
a condition that Z = paY is the minimizor of the finite-sample upper bound of a downstream
prediction error over all possible solutions in the space of Z. The theory provides an intuition for the
claim that sample efficiency is improved when causal information is obtained.
Our Contribution
(i)	We propose an information-theoretical approach to learn causal representation from data, using an
explicit causal graphical model to describe the generative process of the real-world system.
(ii)	We propose a novel quantification of the causal effect of the representation on the downstream
labels by measuring the interventional vunlerability, based on which a robust learning approach is
proposed correspondingly to optimize our model for causality-inspired learning. This integrates the
concept of intervention in causality with the domain of representation learning.
(iii)	We theoretically analyze the sample efficiency of the learning approach by giving a generalization
error bound with respect to finite sample size. The theorem depicts a quantitative link between the
amount of causal information contained in the learned representation, and the sample complexity of
the model on downstream tasks.
(iv)	Comprehensive experiments to verify the merits of method are conducted, including testing cases
for the model’s generalization ability when adversarial attack in representation space and distribution
shift on dataset exist.
2	Related Works
Causal Representation Learning is a set of approaches to find reusable causal information and causal
mechanism from observational data. Motivated by learning generalizable and stable information
2
Under review as a conference paper at ICLR 2022
from data, causal representation approaches from several different perspectives have been proposed
in literature. Under the framework of corss-domain learning, the pioneering work (Zhou et al.,
2021; Wang & Jordan, 2021; Shen et al., 2021) consider the heterogeneity across multiple domains
under the out-of-distribution settings (Gong et al., 2016; Li et al., 2018; Magliacane et al., 2017;
Zhang et al., 2015). They learn causal representations from observational data by invariant causal
mechanism across multi-domains. Leveraging and combining the idea of causal structure learning,
some works use structural causal models to describe causal relationship inside the mixed observational
data (Yang et al., 2021; Shen et al., 2020) and perform learning by minimizing a loss containing a
structure learning part. On the other hand, based on the recently proposed independence between
cause and mechanism principle (ICM), several work focus on the assymmetry between cause and
effect. (Sontakke et al., 2021; Steudel et al., 2010) use the asymmetrical Komolgorov Complexity
(Janzing & SchOlkopf, 2010; Cover, 1999) relationship between cause and effect for causal learning,
and similar ideas are utilized by (Parascandolo et al., 2018; Steudel et al., 2010). Compared with
previous work, different from ICM, our paper considers the relationship between cause and effect
from the perspective of mutual information Belghazi et al. (2018); Cheng et al. (2020), and we
utilize the characteristics that cause information will reduce along the causal generative process, a
phenonemon also widely know as data processing inequality (Kullback, 1997; Cover, 1999). We
put our attentions on the generalization ability of the causal representation. Most existing causal
inference in generalization focuses on distribution shift setting (Vapnik, 1999; Rojas-Carulla et al.,
2018; Meinshausen, 2018; Peters et al., 2016). Arjovsky et al. (2019) aim at finding representation
containing invariant information and analyze the generalization ability of method. Different with
them, in our paper, we theoretically consider the generalization ability of causal representation under
finite sample in probability approximate correctly view (PAC) (Shalev-Shwartz & Ben-David, 2014;
Shamir et al., 2010), and demonstrate that causal representation has low generalization error, which
also supports the claim that causal information is sample efficient.
3	Preliminaries
Structural Causal Models.(Pearl, 2009) In causality, the causal graphical model is represented
quantitatively by a functional model named Structural Equation Model (SCM). The variable Y is
represented as a function of its parental variable X, and additional noise ,
Y =f(X,),X⊥
Under different assumptions, the model can be estimated from observational data up to certain
degree by linear and nonlinear regression, or independence test based approaches. The information
propogation is described by the functional models, which also enables the estimation of causal effect
as well and causal direction under the graphical model.
Counterfactual Estimation.(PearL 2009) Pearl,s causality gives a 3-step approach 一 'abduction-
action-prediction’, for counterfactual estimation. In first abduction step, let O = {X, Y } denote
observational data, X is the cause of Y , we should infer the posterior of the exogenous variable
q(|O). Action approach is implemented by the language of do-operations, where do(X = x0) sets
the variable to be a value of x0 and the prediction approach is to observe the change of the output
under such action. The counterfactual result is thus the interventional output specified by the SCMs.
The information propogation along the causal path results in the observed counterfactual predictions
which are later used as a base for quantifying the causality inside the system.
Mutual Information.(Cover, 1999) We explore the causal characteristics on mutual information
level. Mutual information is an entropy based measure of the mutual dependence between variables.
Definition 1. The mutual information between two random variables X, Z is define
I(X; Z)=ZZ ZX PXZ (x，z)log (pXX⅛⅛)dxdz
(1)
Data Processing Inequality (DPI).(Cover, 1999) DPI is an information theoretic concept which
describes the decreasing of the information along the Markov chain.
Definition 2. (Data Processing Inequality) If three random variables form the Markov chain
X - Z - Y, there exist an inequality:
I(X; Z) ≥ I(X; Y)
(2)
3
Under review as a conference paper at ICLR 2022
4	Method
In this section, we demonstrate a method to specify the minimal sufficient parents information paY
from mixed observation X. We firstly analyze the information propogation among different causal
variables under two typical causal graphs, based on which we propose an objective function with
mutual information constraint for casual representation learning. To optimize the model under such
function to fulfill our hypothetical causal structure, an counterfactual vulnerability based approach is
also introduced.
4.1	Causal Representation Learning Based on Mutual Information
Denote X ∈ X as d-dimensional observational data like context information or features in real-world
system, and Y ∈ Y as the labels of downstream tasks. Each pair of sample (x,y) is drawn i.i.d
fromjoint distribution p(x, y). As is shown in Fig. 1, We use PaY = [paY, PaY,…PaYI] ∈ Rp1
to denote the variables including all observable parent nodes of Y in the causal graph. Similarly,
dcY ∈ Rp2 and ndY ∈ Rp3 denote the descendant and non-descendant nodes of Y, respectively.
We give two possible graphical formulations of the relationship between X and Y in Fig. 1. Fig. 1
(a) is the unconfounded scenario with no confounding node between PaY , Y and dcY exists, and
Fig. 1 (b) shows the confounded case. The only difference lies on the additional edge from PaY to
dcY , since some nodes in PaY will direct point to the nodes in dcY , which makes PaY confound
dcY and Y . The data generative process is formulated by a Structure Causal Model (SCM) as
Y = g1(paY,1),ndY = g2(paY,2),dcY = g3(Y,3)
PaY ⊥1,PaY ⊥2,Y ⊥3,
(3)
where 1, 2 and 3 are assumed to be Gaussian noise, with a distribution N (0, βI). Since the
information flow among the variables described by the aforementioned causal graph is important, we
consider the relationship among paY , dcY and ndY instead of considering that between X and Y
only. The difficulty lies on distinguishing among paY , dcY and ndY , when one only observes X, a
mixture of them. To start with, we firstly consider the unconfounded case (Fig. 1 (a)). Let I(X : Y )
denote the mutual information between X and Y , and H(X) denote the entropy of X. From Data
Processing Inequality (DPI) we can find the the inequality of mutual information inside SCM.
I(PaY; ndY, dcY) ≤ I(PaY; ndY, Y )
(4)
In confounded case shown in Fig. 1 (b), DPI does not apply since dcY is generated by both PaY and
Y , dcY . We thus decompose the mutual information in confounded case as
I(PaY;ndY,dcY) = I(PaY;ndY,Y,dcY) - I(PaY;Y |dcY,ndY)
|
}
{^^^^^^^^^^^^^^^^^^*^
≥0
≤ I(PaY;ndY,Y) + I(PaY;dcY|Y,ndY)
= I(PaY;ndY,Y) + I(PaY;3) = I(PaY; ndY,Y)
(5)
The equality is achieved when all the function deterministic in causal system and H() = 0.
Compared with Eq. 4, the right hand side of inequality in confounded case is with an additional
entropy term H(PaY). However, the information specified by the causal model is unknown when
only the observation X is given. We develop an algorithm to learn representations based on such
hypothetical structure using the presented inequalities. Let Z = φ(X) denote representation extracted
from original observation X, where φ : X → Z is the representation extraction function. For the
causal system shown in Fig. 1, an important fact is that PaY is the minimal sufficient statistics of the
observational data and Y. If the mapping function maps the X to be PaY , it is a function that finds
the minimal sufficient statistics of causal system. The minimal sufficient is defined as follows:
Definition 3. (Minimal Sufficient Statistic (Lehmann & Scheffe, 2012)). Let X, Y be random
variables. Z0 is sufficient for Y if and only if for ∀x ∈ X, z0 ∈ Z, y ∈ Y, p(x|z0,y) = p(x|z0). A
sufficient statistic Z* is minimal if and only if for any sufficient statistic Z, there exists a deterministic
function f such that Z* = f (Z) almost everywhere w.r.t X.
In this paper, we argue that a satisfactory solution of the representation is that Z is equal to PaY .
To optimize the model under such objective based on the Eq. 4, we formulate this as a minmax
optimization problem, and provide theoretical analyses for such approach. The following theorem,
proven in Appendix A, illustrates the equivalence between satisfying Eq. 4 and 5 and obtaining
minimal sufficient statistics.
4
Under review as a conference paper at ICLR 2022
Theorem 1. Let Z0, Z ∈ Z, Z = φ(X) is minimal sufficient statistics of (Y, ndY) if and only if
paY = arg min I(Z; ndY , dcY)
Z	(6)
s	.t.I (Z; Y, ndY) = maxZ0 I (Z0; Y, ndY)
By Theorem 1, when the underlying causal information is unavailable from observational data, we
use above minmax approach to identify the causal information. The process of finding an optimal
representation z = φ(x) is alternatively formulated as maximizing the following Lagrangian function:
δ(φ) = max I(φ(X); Y, ndY) -λI(φ(X); ndY, dcY)
φ 、------{z-------}	、--------{---------}	⑺
1	2
Note that since the information of ndY and dcY is not revealed, the above objective function is not
able to be optimized directly. To get an tractable form of this objective function, we firstly present the
inequalities below (proven in Appendix A).
Lemma 1. Suppose the features and labels are X, Y, where X consists of the minimal sufficient
parents, descendants and non-descendants as X = [paY , ndY , dcY]. The following inequality
holds
1.	I(paY ; ndY, dcY) ≤ I(paY; X)
2.	I(paY;Y) ≤ I(paY;ndY,Y)
The equality is achieved when all the functions in causal system is deterministic and H() = 0. We
use Lemma 1 (1) and (2) to substitute 1 and 2 in Eq.7 respectively, to get the transformed lower
bound of the objective function. The new objective function is tractable since one no longer needs to
specify dcY and ndY in advance.
δ(φ) ≥ L(φ)=maxI(φ(X); Y) - λI(φ(X); X)	(8)
φ
The above objective function coincides with deep Information Bottleneck (IB). The difference is that
IB is deduced from Rate Distortion Theorem in information theory, and it holds under the structure of
Markov Chain instead of a causal graph (i.e. Fig. 1). In this paper, the IB setting is generalized into
causal space, by bridging minimal sufficient statistics with root cause variables in the hypothetical
causal graph. Although the tractable objective (Eq. 8) enables the optimization of the model with
the mapping function φ, one challenge that it cannot correctly discern paY and dcY like intractable
objective Eq.7. Since dcY and paY are both closely related to Y, the learned φ(X) may be a
mixture of them, which makes the representation not optimal to our expectation. In the next section,
we follow the counterfactual estimation process to get sufficient and necessary cause paY .
4.2 Learning Representation by Counterfactual Estimation of PNS
In this section, we introduce a method to learn the minimal sufficient parental information of the
labels. The core idea is to consider the counterfactual identifiable probability of necessary and
sufficient (PNS) cause of predicted labels (Pearl, 2009; Wang & Jordan, 2021). Under this schema,
obtaining the cause information is transformed into a tractable task satisfying counterfactual PNS on
Y. We design a robust intervention vulnurability term to quantify the degree of satisfied PNS, and
obtain a set of parameters that maximize the PNS probability, in order to find sufficient and necessary
causal information.
Definition 4. (PNS Distribution Pearl (2009)) Suppose we observe a data point with representa-
tion Z = z and label Y = y. The probability of necessary and sufficiency (pns) of I{Z = z} for
I{Y = y}:
PNS =Ep(Z6=z,Y6=y)P(Y(Z=z) =y|Z 6=z,Y 6= y) + Ep(Z=z,Y =y)P (Y (Z 6=z) 6=y|Z=z,Y =y)
s--------------------------V-------------------} S-------------------V-------------------}
1 sufficient	2 necessary
(9)
PNS is identified if and only if Z is the parent of Y (Pearl, 2009). To be more specific, sufficient
(Eq. 9 1 ) describes the ability of the causal representation to generate labels and necessary (Eq.
5
Under review as a conference paper at ICLR 2022
9 2 ) measures the probability of negative counterfactual label if we intervene on representation z.
Although theoretically well sound, the estimation of PNS is with some difficulty since P(Y (Z = z) =
y|Z 6= z, Y 6= y) and P(Y (Z 6= z) 6= y|Z = z, Y = y) are both counterfactual distributions, which
means based on the observation that Zz, Y y, the probability of Y = y if Z is intervened to be z. As
illustrated in Preliminary, counterfactual estimation normally follows a 3-step procedure in Pearl’s
framework, under which we propose a constrained method for counterfactual estimation. Considering
the causal generative process Y = f(paY, 1), the 1 is regarded as a random noise perturbing the
pay inside a ball with finite diameter. We treat the inference approach as the process of adversarial
attack (Szegedy et al., 2013; Ben-Tal et al., 2009; Biggio & Roli, 2018) and define the ’Actions’-step
in counterfactual estimation as
z0 = z + 1, z0 ∈ B(z, β)
Z0 = z + €1, Z0 ∈ B(z, β)
(10)
where B(z, β ) is Wasserstein ball, in which the p-th Wasserstein distance (Panaretos & Zemel, 2019)
Wp 1between z and z0 is smaller than β . Then, we define an counterfactual vulnerability (CV) to
measure PNS from the perspective of mutual information. The defined counterfactual vulnerability
term quantifies the vulnerability of prediction model (classification) under input perturbations, which
is formed later. It adjusts the parameterized Z to fit PNS, which later can be estimated via maximum
likelihood. To simplify the learning method, we consider an objective equivalent to Definition 4 as:
Ep(Z6=z,Y 6=y)P (Y (Z = z) = y|Z 6= z, Y 6= y)
- Ep(Z=z,Y =y) P (Y (Z 6= z) = y|Z = z, Y = y)
(11)
Definition 5. (Counterfactual Vulnerability) Let Z0, ZZ0 denote intervened variables on Z = φ(X),
∀z0 ∈ B(z, β), Zz0 ∈ B(Zz, β), D and D0 denote datasets sample from p(z0, y) and p(Zz0, y), the
vulnerability of robust counterfactual estimation is defined as
min	CVB
z0∈B(z,β),Z0∈B(Z,β)
min -ɪ- log P (Y = y∣Z0 = z0) 一 min -ɪ- log P (Y = y|Z0 = Z0)
z0∈B(z,β) |D| 乙 g ( y|	) zo∈B(z,β) |D0| J g (	y|	)
z0,y	z0,y
min H(Y|Z0) 一 min H(Y|ZZ0) + H(Y) 一 H(Y)
z0∈B(z,β)	zz0∈B(zz,β)
(12)
min I(Y; Z0) 一 min I(Y; ZZ0)
z0∈B(z,β)	zz0∈B(zz,β)
Remark. We can interpret the approach under the Pearl’s ’Abduction-Action-Prediction’ three-level
framework. ’Abduction’-step: In Definition 5, search for the worst €1 under minimum process of CV
term, which capture the vulnerability in system. ’Action’-step: The Eq. 10 describes. ’Prediction’-
step: we use the formulation of SCMs y = f(Z, €1) to predict counterfactual results.
Combining the CV term with original objective δ(φ), we get the final objective function optimized
by minmax approach. To simplify the objective function, we only consider to optimize I(Z0 : Y)
rather than I(Z : Y) + I(Z0 : Y) since if the worst case I(Z0 : Y) is satisfied, I(Z0 : Y) is satisfied.
The robust optimization objective function is Lrb φ, where
max zo∈B(z,m,in∈B(z,β) I (φ(X);Y)- λI (φ(X); X)+CVB
≥Lrb (φ) = max
φz
min ,ʃ(Z0; Y) - λI(Z; X) 一 I(Z0; Y)
∕∈B(z,β),z0∈B(z,β) C---- ---------} i—{z一}
(13)
1 positive
{z
2 negative

Our model learning is accompanlished by the minmax procedure. Intuitively speaking, the mini-
mization procedure helps to identify the counterfactual interventional output under pertubations, or
equalvalently, infer the exougeous variables form a perspective of adversarial learning. The maximiza-
tion procedure is to identify a mapping function that learns the representation most likely to satisfy
the PNS condition. The optimizatin of positive term aims at finding sufficient parents’ information,
and the negative term is to find necessary parents’ information, so that the final optimization objective
can extract minimal sufficient parents from observation data in high probability.
1/p
infγ∈Γ(μ,ν) rz×z δ (Z, ZO)P dY (Z, ZO)),
measures on Z × Z
Γ(μ,ν is the collection of all probability
6
Under review as a conference paper at ICLR 2022
Table 1: Overall Results on CPC and PCIC
Dataset	Method	p=∞				p=2			
	Metrics	AUC	ACC	advAUC	advACC	AUC	ACC	advAUC	advACC
	base(robust)	0.5	0.1017	-05	0.1017	0.5	0.1012	0.5	0.0928
	base(standard)	0.7144	0.7076	0.4506	0.4305	0.7103	0.7612	0.4489	0.4017
	IB(standard)	0.7113	0.7265	0.5776	0.6952	0.7124	0.7066	0.5647	0.6642
CPC	r-CVAE(robust)	0.7182	0.7749	0.6694	0.7196	0.7139	0.7574	0.6611	0.7426
	r-CVAE(standard)	0.7183	0.7703	0.6467	0.6809	0.7124	0.7692	0.6402	0.691
	CaRR(robust)	0.7271	0.7834	0.6851	0.7501	0.7224	0.7874	0.6776	0.7607
	CaRR(standard)	0.7225	0.7543	0.6606	0.6995	0.7175	0.778	0.6661	0.7378
	base(robust)	0.5534	0.5875	0.5388	0.6257	0.5605	0.6498	0.5264	0.6287
	base(standard)	0.6177	0.6517	0.5231	0.589	0.6269	0.6615	0.519	0.5581
	IB(standard)	0.6242	0.6532	0.5741	0.6199	0.6216	0.6537	0.5768	0.6233
PCIC	r-CVAE(robust)	0.6261	0.6699	0.6314	0.6557	0.6324	0.6586	0.6272	0.6459
	r-CVAE(standard)	0.635	0.6537	0.6126	0.64	0.6282	0.6508	0.6161	0.64
	CaRR(robust)	0.6429	0.6787	0.6335	0.6714	0.6465	0.6782	0.6330	0.6645
	CaRR(standard)	0.648	0.6802	0.6173	0.6709	0.6361	0.6778	0.6208	0.6414
5 Sample Complexity
In this section, we theoretically analyze the proposed algorithm by the probability approximately
correct (PAC) framework. We start from the perspective of information theory, and it in fact can
be generalized to deep learning models. We answer the question that why causal representation
containing cause information enhances the generalization ability. Different with traditional PAC
learning theorem, we analyze the risk by mutual information, which follows the framework of
information bottleneck (Shamir et al., 2010). We provide a finite sample bound of generalization
ability. The bound measures the relationship between I(Z; Y ) and its estimation I(Z; Y ). Here, we
provide theoretical justification with following theory (proven in Appendix A):
Theorem 2. Let Z = φ(X), where φ : X → Z be a fixed arbitrary function, determined by a known
conditional probability distribution p(z|x). Let m be sample size drawn from p(X, Y ). For any
confidence parameter 0 < δ < 1, it holds with a probability of at least 1 - δ, that
1.	General case (Z = φ(X))
. , . ^ , ..
|I(Y; Z) - I(Y; Z)| ≤
Plog(∣Y∣∕δ) (∣Y∣√jZ!log(m) +1 √jZ!Iog(IYI)) + e|Y|
√m
where m ≥ C log(∣Y∣∕δ)∣Z∣e2
2.	Ideal case (Z = φ(X) = paY)
., . ^, ..
|I(Y; Z) - I(Y; Z)∣ ≤
pclog(∣γ∣∕δ) (∣γ∣√βlog(m) + 1 PZllog(∣Y∣)) + e∣Y∣
√m
(14)
(15)
where m ≥ Clog(∣Y∣∕δ)βe2
Remark. The theorem provides a generalization bound under finite sample settings. It shows that
when representation Z fully contains parents’ information paY , we achieve a sample complexity
bound as m ≥ C log(∣Y∣∕δ)βe2, where β refers to Eq. 3. The minimum number of samples needed
reduces from |Z | to β, which is a better bound since in most of cases we assume |Z | β. This
shows that z = paY gives the reduced sample complexity and tightened generalization bound. The
theorem also serves as a general solution to causality prediction problems, supporting the claim that a
better prediction is achieved with causal variables, compared to that with correlated variables.
6 Experiments
In this section, we conduct extensive experiments to verify the effectiveness of our framework. In the
following, we begin with the experiment setup, and then report and analyze the results.
6.1	Datasets
Our experiments are based on several real-world benchmarks. We focus on the application of
recommendation system and evaluate our method on click through rate (CTR) prediction.Yahoo!
7
Under review as a conference paper at ICLR 2022
R32 is an online music recommendation dataset, which contains the user survey data and ratings for
randomly selected songs. The dataset contains two part. The uniform (OOD) set and the nonuniform
(I.I.D.) set. Coat Shopping Dataset3 is a commonly used dataset which collected from web-shop
rating on clothing. The self-selected ratings are the I.I.D. set and the uniformly selected ratings are
the OOD set. PCIC4 is a recently released dataset for debiased recommendation, where we are
provided with the user preferences on uniformly exposed movies. CPC This is a dataset for CTR
prediction, which includes events from real-world searching system.
6.2	Experimental Setup
Implementation of Our Method. We name our method as CaRR. All the objective functions are
defined under information-theoretical formulation. We evaluate Eq. 13 in two parts. The first positive
part (Eq. 13 1 ) is evaluated by the following parameterized objective (Alemi et al., 2016):
I(Z0; Y) - λI(Z; X) ≥ ED[Ez0∈B(z,β)[logPg(y|z0)] - λDκL(qφ(z∣x)∣∣pθ(z))]	(16)
we use PGD attack (Madry et al., 2017) with ∞-norm and 2-norm to get intervened Z0. We set
Pθ(z) as N(y, 1) to avoid trivial representations. For the negative term (Eq. 13 Q)), let Z = Z + b,
while b is hyperparameter denoting degree of bias. In our experiments, we set b = 0.8 when y = 0,
b = -0.8 when y = 1. Then we use negative cross entropy to approximate mutual information.
More implementation details are shown in Appendix B.
Compared Method. For all the compared methods, we use the same model architecture, with
different training strategies. The model consists of representation learning module Z = φ(x) and
the downstream prediction module y = g(z), with each module implemented by neural networks.
Base model has no additional constraints on representation, and the optimization is to minimize the
cross-entropy between y and learned y. We involve a recently proposed variational estimation with
information bottleneck (IB) (Alemi et al., 2016), extend the condition VAE (CVAE (Sohn et al.,
2015)) by robust training process as r-CVAE, whose objective function is similar with CaRR but
without a negative term (Eq. 13 Q2 ). We conduct ablation studies by comparing our proposed method
CaRR with the r-CVAE to evaluate the effectiveness of negative term. We evaluate our method
on two main aspects: (i) Generalization of the model under distribution shifts and (ii) Robustness
under adversarial attack on representation space. For (i), we evaluate our method on OOD and I.I.D.
setting on Yahoo! R3 and Coat. For (ii), the standard mode of adversarial attack (β = 0) means that
we do not perturb original Z. In robust mode, we set β = {0.1, 0.2, 0.1, 0.3} for PCIC, Yahoo! R3,
Coat, and CPC, respectively.
Metrics. We use commonly used evaluation metrics AUC/ACC (Rendle et al., 2012; Gunawardana
& Shani, 2009) in CTR prediction and additionally evaluate metrics AUC/ACC on advasarially
perturbed evaluation dataset as adv-ACC/ adv-AUC (Madry et al., 2017).
6.3	Result Analysis
Table 1 shows overall experimental results on CPC and PCIC, based on which we find that in
most cases, our method achieves better performance in terms of AUC and ACC, compared to
base methods. For example, both standard and robust modes of CaRR achieve the best AUC
64.29%, 64.8% respectively, which is the best among all the compared methods on PCIC. This
observation demonstrates the effectiveness of our idea. In robust training mode, our method gets best
performance when adversarial metrics are considered. For example, in PCIC dataset, our method
reaches 63.35%, which increases 9.47% against base methods on adv-AUC. Robust training of CaRR
is also better than the standard training, winning with a margin around 1.62%. The results show that
the robust learning process with exogenous variables involved enhances the adversarial performance
on perturbed samples. On the other hand, in standard training mode, CaRR achieves better adversarial
performance than baselines including base method and IB. Although the robust training deteriorates
the performance of on normal dataset, it will help to identify the causal representation, which benefits
downstream prediction under adversarial attack. For instance, we find that standard training of CaRR
on PCIC has an AUC of 64.8%, which is better than the performance under robust training (64.29%).
But contrary conclusions are drawn on adversarial performance. The result supports that causal
representation we learned is more robust. The performance of base method in robust training mode is
2https://webscope.sandbox.yahoo.com/catalog.php?datatype=r
3https://www.cs.cornell.edu/ schnabts/mnar/
4https://competition.huaweicloud.com/information/1000041488/introduction
8
Under review as a conference paper at ICLR 2022
Table 2: Overall Results on Yahoo!R3-OOD and Yahoo!R3-IID
Dataset	Method	p=∞				p=2			
	Metrics	AUC	ACC	advAUC	advACC	AUC	ACC	advAUC	advACC
	base(robust)	0.5	0.4508	-05^^	0.4508	0.5	0.4545	0.5	0.4537
	base(standard)	0.6198	0.6097	0.5212	0.5189	0.621	0.6099	0.5139	0.5188
	IB(standard)	0.6181	0.6063	0.5333	0.5149	0.6184	0.6069	0.5431	0.5255
Yahoo!R3-OOD	r-CVAE(robust)	0.6122	0.623	0.5883	0.5907	0.6119	0.6251	0.5866	0.5875
	r-CVAE(standard)	0.6186	0.6252	0.5836	0.584	0.6183	0.6273	0.5807	0.5794
	CaRR(robust)	0.6271	0.6290	0.6008	0.6009	0.6278	0.6297	0.603	0.6021
	CaRR(standard)	0.6293	0.6285	0.587	0.5862	0.6284	0.6297	0.5918	0.594
	base(robust)	0.5	0.6001	-0.5^^	0.5997	0.5	0.6	0.5	0.6
	base(standard)	0.7334	0.7483	0.6267	0.6251	0.7346	0.752	0.6260	0.6103
	IB(standard)	0.7291	0.7513	0.6361	0.6721	0.7348	0.7521	0.6418	0.6775
Yahoo!R3-IID	r-CVAE(robust)	0.7431	0.7299	0.7097	0.6999	0.7382	0.7239	0.7081	0.7006
	r-CVAE(standard)	0.7428	0.7302	0.7063	0.6984	0.7416	0.7286	0.7035	0.6957
	CaRR(robust)	0.7460	0.7330	0.7162	0.7076	0.7436	0.7291	0.7199	0.7116
	CaRR(standard)	0.7447	0.7344	0.7053	0.6998	0.742	0.7276	0.7069	0.6981
worst in most of cases, indicating that robust training process will largely influence the learning of the
model and ruin the prediction model. Table 2 shows the results on Yahoo! R3, which all contain I.I.D.
validation and test sets, and also OOD dataset. We find that our method has a better generalization
ability. For example, in Yahoo! R3 OOD, our method increases the performance by 1.9% and 8.2%,
in terms of ACC and adv-ACC, compared with base method. The performance of r-CVAE is close to
CaRR, since it is a modified version of our method, which only includes the positive term in Eq. 13
but with the negative one dropped. The difference between performance of CaRR and that of r-CVAE
shows the effectiveness of the negative term in the objective function of CaRR. Fig. 2 demonstrates
how robust training degree (β = {0.1, 0.3, 0.5, 0.7, 1.0}) influences the downstream prediction under
adversarial settings. We conduct the experiments on the attacked real-world dataset by PGD attacker.
From Fig. 2, we find that our method is better than base method, because the base model’s ability on
standard prediction is broken by adversarial training. When β is small, our method behaves closely
to the r-CVAE in all the datasets. When β gets larger, the difference between performance of CaRR
and that of r-CVAE continuously enlarges in Yahoo!R3. In PCIC, the gap becomes the largest among
Figure 2: Results under different adversarial perturbation degree β on three datasets. Axis-X is the
attack degree β. Axis-y is the adv-AUC under attacked test dataset.
7 Conclusions
In this paper, we deal with the problem of representation learning for robust deep models. We argue
that when the observations contain mixed information of the labels, that is, the cause, effect and other
unrelated variables, learning a representation which preserves mostly the cause information results in
satisfactory generalizability of the models. By information-theoretically analyzing our hypothetical
graphical model, we propose a causality-inspired representation learning method by regularized
mutual information based approach, which achieves effective learning via counterfactual based
model tuning, with guaranteed sample complexity reduction under certain assumptions. Extensive
experiments on real data set show the effectiveness of our algorithm, supporting our claim of robust
learning.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement All the supplementary materials are included in Appendix. Appendix A
contains the proof of Theorem 1, Theorem 2 and Lemma 1. The implementation detail of method
and the experimental results on Coat dataset are available in Appendix B and Appendix C.
References
Ossama Ahmed, Frederik Trauble, AnirUdh Goyal, Alexander Neitz, YoshUa Bengio, Bernhard
Scholkopf, Manuel Wuthrich, and Stefan Bauer. Causalworld: A robotic manipulation benchmark
for caUsal strUctUre and transfer learning. arXiv preprint arXiv:2010.04296, 2020.
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.
Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Nihat Ay and Daniel Polani. Information flows in causal networks. Adv. Complex Syst., 11(1):
17-41,2008. doi: 10.1142/S0219525908001465. URL https://doi.org/10.1142/
S0219525908001465.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon
Hjelm, and Aaron C. Courville. Mutual information neural estimation. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 530-539. PMLR, 2018. URL http://proceedings.
mlr.press/v80/belghazi18a.html.
Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton
university press, 2009.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.
Pattern Recognition, 84:317-331, 2018.
Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. CLUB: A
contrastive log-ratio upper bound of mutual information. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119
of Proceedings of Machine Learning Research, pp. 1779-1788. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/cheng20b.html.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Andrea Dittadi, Frederik Trauble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole
Winther, Stefan Bauer, and Bernhard Scholkopf. On the transfer of disentangled representations in
realistic settings. arXiv preprint arXiv:2010.14407, 2020.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Scholkopf.
Domain adaptation with conditional transferable components. In International conference on
machine learning, pp. 2839-2848. PMLR, 2016.
Asela Gunawardana and Guy Shani. A survey of accuracy evaluation metrics of recommendation
tasks. Journal of Machine Learning Research, 10(12), 2009.
Dominik Janzing and Bernhard Scholkopf. Causal inference using the algorithmic markov condition.
IEEE Transactions on Information Theory, 56(10):5168-5194, 2010.
Niki Kilbertus, Giambattista Parascandolo, and Bernhard Scholkopf. Generalization in anti-causal
learning. arXiv preprint arXiv:1812.00524, 2018.
Solomon Kullback. Information theory and statistics. Courier Corporation, 1997.
Erich Leo Lehmann and Henry Scheffe. Completeness, similar regions, and unbiased estimation-part
i. In Selected Works ofEL Lehmann, pp. 233-268. Springer, 2012.
10
Under review as a conference paper at ICLR 2022
Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, and Dacheng Tao. Domain generalization
via conditional invariant representations. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M
Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions.
arXiv preprint arXiv:1707.06422, 2017.
Nicolai Meinshausen. Causality from a distributional robustness point of view. In 2018 IEEE Data
Science Workshop (DSW), pp. 6-10. IEEE, 2018.
Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. Embedding-based news rec-
ommendation for millions of users. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 1933-1942, 2017.
Victor M Panaretos and Yoav Zemel. Statistical aspects of wasserstein distances. Annual review of
statistics and its application, 6:405-431, 2019.
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard Scholkopf. Learning
independent causal mechanisms. In International Conference on Machine Learning, pp. 4036-4044.
PMLR, 2018.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant
prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series
B (Statistical Methodology), pp. 947-1012, 2016.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. The MIT Press, 2017.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian
personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618, 2012.
Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for
causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris Mooij.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471, 2012.
Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner,
Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the
IEEE, 109(5):612-634, 2021.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696-2711, 2010.
Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Disentangled
generative causal representation learning. arXiv preprint arXiv:2010.02637, 2020.
Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards
out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.
Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. Heterogeneous information network
embedding for recommendation. IEEE Transactions on Knowledge and Data Engineering, 31(2):
357-370, 2018.
11
Under review as a conference paper at ICLR 2022
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep
conditional generative models. Advances in neural information processing systems, 28:3483-3491,
2015.
SUmedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Scholkopf. Causal curiosity: Rl
agents discovering self-supervised experiments for causal representation learning. In International
Conference on Machine Learning, pp. 9848-9858. PMLR, 2021.
Bastian Steudel, Dominik Janzing, and Bernhard Scholkopf. Causal markov condition for submodular
information measures. arXiv preprint arXiv:1002.4020, 2010.
Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, and Chi Xu. Recurrent
knowledge graph embedding for effective recommendation. In Proceedings of the 12th ACM
Conference on Recommender Systems, pp. 297-305, 2018.
Raphael Suter, Djordje Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In International
Conference on Machine Learning, pp. 6056-6065. PMLR, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Frederik Trauble, Elliot Creager, Niki Kilbertus, Anirudh Goyal, Francesco Locatello, Bernhard
Scholkopf, and Stefan Bauer. Is independence all you need? on the generalization of representations
learned from correlated data. arXiv e-prints, pp. arXiv-2006, 2020.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks,
10(5):988-999, 1999.
Yixin Wang and Michael I Jordan. Desiderata for representation learning: A causal perspective.
arXiv preprint arXiv:2109.03795, 2021.
Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae:
disentangled representation learning via neural structural causal models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9593-9602, 2021.
Kun Zhang, Mingming Gong, and Bernhard Scholkopf. Multi-source domain adaptation: A causal
view. In Twenty-ninth AAAI conference on artificial intelligence, 2015.
Yongfeng Zhang, Qingyao Ai, Xu Chen, and W Bruce Croft. Joint representation learning for top-n
recommendation with heterogeneous information sources. In Proceedings of the 2017 ACM on
Conference on Information and Knowledge Management, pp. 1449-1458, 2017.
Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A
survey. arXiv preprint arXiv:2103.02503, 2021.
12
Under review as a conference paper at ICLR 2022
A	Theoretical Proof
A.1 Proof of Theorem 1
follows directly from the following two lemmas. We denote by F(X) the set of probabilistic functions
of X into an arbitrary target space, and by S(Y ) the set of sufficient statistics for Y .
Lemma 2. Let Z be a probabilistic function of X. Then Z is a sufficient statistic for (Y,ndY) if and
only ifI(Y,ndY;paY) = I(Y,ndY; Z) = maxZ0∈F(X) I(Y,ndY; Z0)
Proof. The proof of Lemma is similar with Lemma 12 in Shamir et al. (2010), the difference is that
we focus on the joint variable of Y , ndY rather than Y . For every Z0 which is a probabilistic function
of X, we have Markov Chain Y, ndY - X - Z0, so we have I(Y, ndY ; X) ≥ I(Y, ndY ; Z0). We
also have Markov Chain Y, ndY - Z - X, so we have I(Y, ndY ; X) ≤ I(Y, ndY ; Z) Then assume
I(Y, ndY; Z) = I(Y, ndY; X). Since we also have Markov Chain Y, ndY - X - Z, it follows that
Y, ndY and X are conditionally independent given Z, hence Z is a sufficient statistic.
□
Lemma 3. Let Z is sufficient statistics of Y. Then Z is minimal sufficient statistic for Y if and only
ifI(ndY,dcY;paY) = I(ndY,dcY; Z) = maxZ0∈S(Y,ndY) I(ndY,dcY; Z0)
Proof. First, let Z be a minimal sufficient statistic, and let Z0 be some sufficient statistic. Since
there is an function Z = f(Z0), we have Markov Chain (ndY, dcY) - Y - Z0 - Z, we could have
I(ndY, dcY; Z) ≤ I(ndY, dcY; Z). Similarity, following the proof process of Lemma 13 in ?, we
get the above Lemma.	□
A.2 Proof of Lemma 1
Proof. For the Lemma 1 (1) is hold since
I(paY;ndY,dcY) ≤ I(paY;ndY,dcY,paY) ≤ I(paY;X).	(17)
For Lemma 1 (2) in main text.
Firstly, we introduce Kolmogorove Complexity K(x), which denotes the shortest description length
of string x. Previous works give some useful technical results about Kolmogorove Complexity in
causality.
Definition 6 (Algorithmic Mutual Information). Let x, y be two strings. Then the algorithmic mutual
information of x, y is:
I(X : y) := K(y)- K(y∣x*).	(18)
The mutual information is the number of bits that can be saved in the description of y when the
shortest description of x is already known.
Lemma 4 (Entropy and Kolmogorov Complexity (VaPnik, 1999)). Let X = xι, X2 •…，Xn bea sting
whose symbols xj ∈ X are drawn i.i.d. from a probability distribution P(X) over the finite alphabet
X. Slightly overloading notation, Set P(x) := P(xι) •…P(xn).. Let H(∙) denote the Shannon
entropy of a probability distribution. Then there is a constant c for n such that
H(P(X)) ≤ 1 E(K(x∣n)) ≤ H(P(X)) + Mogn + C	(19)
n	nn
where E(∙) is short hand for the expected value with respect to P (X). Hence
lim 1 E(K (x)) = H (X)	(20)
n→∞ n
Lemma 5. (Recursive Form (Janzing & Scholkopf, 2010)) Given the strings xι, ∙∙∙ ,Xn and a
directed acyclic graph G. Then the Kolmogrove Complexity has the recursive form:
n
K (xι,..., Xn) := X K (xj | pa；)	(21)
j=1
13
Under review as a conference paper at ICLR 2022
I(ndy,y : Pay) ：=K(ndy,y) - K(ndy,y∣pay*)
:=K(ndy,y) - K(ndy|pay*) - K(y∣pay*)
:=+K(y) + K(dy) - I(ndy : y)
-K(ndy |Pay*) - K(y|Pay*)	(22)
:=+I(y : Pay) + I (ndy : Pay) - I(ndy : y)
X----------{z-----------}
≥0
：+ I(y : Pay)
Lemma 4 already shows that
lim LE(I(ndy, y : Pay)= I(ndy,y; Pay, lim ^E(y : Pay) = I(y; Pay)
n→∞ n	n→∞ n
We can accomplish the proof of Lemma 1
(23)
□
A.3 Proof of Theorem 2
The proof follows process in Shamir et al. (2010) Theorem 3. The sketch of proof contains two steps:
(i) we decompose the original objective |I(Y ; Z) - I(Y ; Z)| into two parts. (ii) for each part, we
deduce the deterministic finite sample bound by concentration of measure arguments on L2 norms of
random vector.
ʌ ʌ ʌ
|I(Y; Z)- I(Y; Z)| ≤ |H(Y|Z) - H(Y∣Z)∣ + |H(Y) - H(Y)|	(24)
Let h(x) denote a continuous, monotonically increasing and concave function.
0
h(x) =	x log(1/x)
11/e
x=0
0 < x ≤ 1/e
x > 1/e
(25)
ʌ , ..
for the term |H(Y|Z) - H(Y|Z)|
|H(Y|Z) - H(Y∣Z)I = ∑(p(z)H(Y I Z)-P(Z)H(YI Z))
z	(26)
≤	XP(Z)(H(YI	z)	-	H(YI z)) + X(P(Z)-	P(Z))H(YI	z)
z z
For the first summand in this bound, we introduce variable to help decompose P(yIZ), where is
independent with the parents Pay (i.e. ⊥ Pay)
EP(Z)(H(Y I z) - H(Y	I	z))	≤	EP(Z)E(p(y	I Z)log(p(y	I	z))	- p(y	I	z)	log(p(y	I z)))
z z y
≤ XP(Z) Xh(IP(y i z) -P(y i Z)I)
zy
=X P(Z) Xh (∣X P(e i Z)(P(y i z, e) - P(y i z, ©J)
=XP(z) X h(kP(y | z, e) - P(y | z, e)kPV(P(e | Z)))
zy
(27)
where ml V(x) denote the variance of vector x. For the second summand in Eq. 26.
X(p(z) -P(Z))H(YI z) ≤ kP(z) - P(z)k ∙ ,V(H(Y∣ Z))	(28)
z
14
Under review as a conference paper at ICLR 2022
For the summand |H(Y) - H(Y)|:
|H(Y) - H(Y)1 = fp(y)b虱p(y)) -P(y)iog(P(y))
y
≤ Xh(Ip(y) -p(y)[)
y
=Xh (IXXP(C | Z)(P(Z)P(y|e) -p(z)p(y⑹)
y	z
≤ Xh(kp(z)p(ylc) - P(Z)p(y⑹kPV(p(e | Z)))
y
(29)
Combining above bounds we get:
|I(Y； Z)- I(Y Z)I ≤E h(kp(z,yle) - p(z,y|e)k ,V(p(e | Z)))
y
+ XP(Z) X h(kp(y | ZQ- p(y | z, e)∣∣PV(p(e | Z)))	(30)
zy
+ kp(z) - P(Z)k∙ q(H(YI Z))
Let P be a distribution vector of arbitrary cardinality, and let P be an empirical estimation of P based
on a sample of size m. Then the error ∣∣ρ - Pk will be bounded with a probability of at least 1 - δ
2+√2log(1∕δ)
kρ - ρk≤—√m一
(31)
Following the proof of Theorem 3 in Shamir et al. (2010), to make sure the bounds hold over
∣Y∣ + 2∣ quantities, we replace δ in Eq. 31 by δ∕(∣Y∣ + 2∣, than substitute ∣∣p(z, y∣e) - p(z, y∣e)∣
∣IP(y I z, c) - p(y I z, e)k, kp(z) - P(z)k, by Eq. 31.
II(Y； Z) - I(Y; Z)I ≤(2+ √2log((IYI +2)∕δ))

,ʌ , ..
V(H(YI Z))
m
+ 2IYIh (2 + p2ι°g≡Wδ) ∕v(p⅛≡i
(32)
There exist a constant C, where 2 +，2log((3 +2)∕δ) ≤，Clog((3)∕δ). From the fact that
variance of any random variable bounded in [0, 1] is at most 1/4, we analyze the bound under two
different cases:
In general case (z = φ(x)),
V(p(e I z)) ≤ 号	(33)
let m denote the number of sample, we get a lower bound of m, which is also known as sample
complexity.
C
m ≥ Zlog(IYI∕δ)IZIe2	(34)
In ideal case( z = Pay) z ⊥ :
V (P(C I z)) ≤ β
C
m ≥ jlog(IYI∕δ)IβIe2
(35)
(36)

Clog(IYI∕δ)V(p(e I Z))
m
≤ ∕C0il≡≤ 1∕e
(37)
15
Under review as a conference paper at ICLR 2022
Then, from the fact that (Shamir et al. (2010)):
h(鼠)=(/m log( W))
≤ √v log(√m) + 1/e
≤	√m	,
We can get the upper bound of second summand in Eq. 32 as follows
(38)
x h ZC log((∣γ∣)∕δ) JV(Pm |Z))
PCIog(IY|/6)log(m) (|Y|PV(P(W | Z))) + e|Y|
≤	2√m
(39)
In general case:
Eq.39 ≤
PC log(∣γ∣∕δ)log(m) (∣Y∣PZ) + e IYI
2√m
(40)
In ideal case:
Eq.39 ≤
，Clog(∣γ∣∕δ)log(m) (∣Y∣√β) + eIYI
2√m
For the first summand in Eq. 32, we follow the fact (Shamir et al. (2010) Theorem 3) that:
V(H(Y I Z)) ≤ IZIlog2(IYI)
(41)
(42)
Finally we accomplish the proof of Theorem 2.
B	Experimental Details
B.1	Datasets
Yahoo! R3 The nonuniform (OOD) set contains samples of users deliberately selected and rate the
songs by preference, which can be considered as a stochastic logging policy. For the uniform (I.I.D.)
set, users were asked to rate 10 songs randomly selected by the system. The dataset contains 14,877
users and 1,000 items. The density degree is 0.812%, which means the dataset only records 0.812%
of rating pairs.
CPC The dataset contains 85000 samples for training and 15000 samples for validation and test. The
recommended item list will be exposed to query of the system by nonuniform recommendation policy.
The data includes 29 dimensions of matching features, which include query and item features.
Coat The dataset was collected by an online web-shop interface.In training dataset, users were asked
to rate 24 coats selected by themselves from 300 item sets. In test dataset, it collects the userrates on
16 random items from 300 item sets. Just as Yahoo! R3,the training dataset is a non-uniform dataset
and the test dataset is uniform dataset. The dataset provides side information of both users and item
sets. The feature dimension of user/item pair is 14/33.
PCIC The dataset was collected from a survey by questionnaire about the rate and reason why the
audience like or dislike the movie. Movie features is collected form movie-review pages. The training
data is biased dataset that 1000 users were asked to rate the movies they care from 1720 movies. The
validation and test set is the user preference on uniformly exposed movies. The density degree is
0.241%.
For evaluation, Yahoo! R3 and Coat dataset both have two validation (include test) datasets. The
I.I.D. set is 1/3 data from nonuniform logging policy, and OOD set consists of the data generated
under a uniform policy. For PCIC dataset, we train our method on non-uniform datasets and perform
evaluations on uniform dataset.
16
Under review as a conference paper at ICLR 2022
B.2	Implementation Details
The hyper-parameters are determined by grid search. In specific, the learning rate and batch size
are tuned in the ranges of [10-1,10-2,10-3,10-4] and [64,128,256, 512,1024], respectively. The
weighting parameter λ is determined in [0.001]. Perturbation degree are β = {0.1,0.2,0.1,0.3} for
Coat, Yahoo!R3, PCIC and CPC separately. The representation dimension is empirically set as 64.
All the experiments are conducted based on a server with a 16-core CPU, 128g memories and an
RTX 5000 GPU. The deep model architecture is shown as follows:
(1)RePreSentatiOn learning method φ(x): If dataset is Yahoo!R3 or PCIC, in which only user id and
item id is the input, we should firstly use an embedding layer. The representation function architecture
is.
•	Concat(Embedding(user id, 32), Embedding(item id, 32))
•	Linear(64, 64), ELU()
•	Linear(64, representation dim), ELU()
Then for the dataset Coat and CPC, the feature dimension is 29 and 47 separately. It do not use
embeding layer at first. The representation function architecture is.
•	Linear(64, 64), ELU()
•	Linear(64, representation dim), ELU()
(2)Downstream Prediction Model g(z):
•	Linear(representation dim, 64), ELU()
•	Linear(64, 2)
Figure 3: The figure demonstrates the model architecture of CaRR
The figure shows the model architecture. The model consists of two parts, the representation learning
part and downstream prediction part. As illustrated in Section 6.2, our final objective is:
ED[Ez0∈B(z,β)[logPg(y|z0)] - XDKL(qψ(z∖x)∖∖pθ(z)) - E*∈B(z,β)[logPg(y∣z0)]]	(43)
For the representation learning part, we firstly use encode function φ(∙) to get representation Z and get
the intervened z. Then we perturb the learned Z by PGD attack procedure and perturb the Z by random
perturbation to find the worst case correspnding to the worst downstream loss. Finally we put z0 and
z0 into the downstream prediction model g(∙) to calculate y. The likelihood in Eq. 43 is estimated by
cross entropy loss. Note that the perturbation approach would block the gradient propagation between
representation learning process and downstream prediction by some implementation ways. Thus
17
Under review as a conference paper at ICLR 2022
we use the conditional Gaussian prior pθ(z) = N (y1, I) rather than standard Gaussian distribution
pθ(z) = N(0, I) to calculate KL term. If gradient propagation is blocked, by using conditional prior,
the learning process of representation z and exogenous embedded in z0 will not be influenced. The
form of conditional Gaussian prior is more general pθ(Z) = N(Z(y), I), where Z(∙) could be any non
trivial function like linear function even neural network.
C Additional Results
Due to the page limit in main text, we demonstrate the additional test results on Coat dataset in this
section. The table contains both IID and OOD setting, the base method on adversarial AUC and ACC
achieves 70.34% and 71.04% on Coat dataset, but unsatisfactory on other dataset, possibly because
the coat dataset is not sensitive to adversarial attack.
Dataset	Method	p=∞				p=2			
	Metrics	AUC	ACC	advAUC	advACC	AUC	ACC	advAUC	advACC
	base(robust)	0.5586	0.5569	0.5479	0.5451	0.5593	0.556	0.5441	0.5412
	base(standard)	0.5659	0.5724	0.3874	0.4024	0.5642	0.5687	0.3128	0.3317
	IB(standard)	0.5659	0.5681	0.4701	0.4796	0.5659	0.5713	0.5442	0.5495
Coat-OOD	r-CVAE(robust)	0.5616	0.5569	0.5595	0.5549	0.5611	0.5569	0.5585	0.5543
	r-CVAE(standard)	0.5636	0.5607	0.554	0.55	0.5632	0.5588	0.5511	0.5444
	CaRR(robust)	0.5716	0.5730	0.566	0.5657	0.5712	0.5731	0.5653	0.5671
	CaRR(standard)	0.5689	0.5710	0.5600	0.5594	0.5699	0.5695	0.5604	0.5598
	base(robust)	0.7156	0.7232	0.7034	0.7107	0.7195	0.7261	0.7001	0.7057
	base(standard)	0.7191	0.7217	0.4911	0.487	0.7235	0.7255	0.3642	0.3515
	IB(standard)	0.7162	0.72	0.6023	0.6017	0.7182	0.7222	0.694	0.696
Coat-IID	r-CVAE(robust)	0.7172	0.7243	0.7149	0.722	0.7138	0.72	0.7113	0.7176
	r-CVAE(standard)	0.7199	0.7278	0.7095	0.7169	0.717	0.7264	0.7086	0.7006
	CaRR(robust)	0.7255	0.7331	0.7205	0.7265	0.7257	0.7325	0.7223	0.7306
	CaRR(standard)	0.7253	0.7350	0.7095	0.7155	0.725	0.7321	0.7162	0.7232
Table 3: Additional results on PCIC With Standard error
PCIC			AUC	std	ACC	std	adv_AUC	std	adv_ACC	std
	p=2	CaRR	0.6416	0.0078	0.6803	0.0014	0.619	0.004	0.6625	0.0041
standard		r-CVAE	0.6328	0.0023	0.6725	0.0042	0.5893	0.0419	0.6429	0.0201
	p=∞	CaRR	0.6447	0.0041	0.6817	0.0043	0.6148	0.011	0.664	0.0104
		r-CVAE	0.6358	0.014	0.6779	0.0066	0.6138	0.0062	0.6601	0.0048
	p=2	CaRR	0.6363	0.0045	0.6709	0.0042	0.6332	0.0024	0.6576	0.0006
robust		r-CVAE	0.63	0.0075	0.674	0.0069	0.6187	0.0051	0.6493	0.0013
	p=∞	CaRR	0.639	0.007	0.6761	0.0024	0.6225	0.0057	0.6638	0.001
		r-CVAE	0.6363	0.0066	0.6733	0.0058	0.6088	0.0098	0.6596	0.0124
Table 4: Additional results on Yahoo!R3 OOD With Standard error
Yahoo!R3 OOD			AUC	std	ACC	std	adv_AUC	std	adv_ACC	std
	p=2	CaRR	0.6276	0.0001	0.6255	0.0022	0.5917	0.0071	0.5917	0.0072
standard		r-CVAE	0.6233	0.0005	0.6243	0.002	0.5865	0.0022	0.5872	0.0025
	p=∞	CaRR	0.629	0.0011	0.6257	0.0002	0.5966	0.0049	0.5965	0.0042
		r-CVAE	0.6253	0.0023	0.6249	0.0014	0.5855	0.0016	0.5863	0.0019
	p=2	CaRR	0.6242	0.0009	0.6307	0.0012	0.6008	0.0009	0.601	0.0016
robust		r-CVAE	0.6191	0.0013	0.6241	0.0051	0.5882	0.0014	0.5907	0.0009
	p=∞	CaRR	0.6238	0.0011	0.6284	0.0017	0.5993	0.0019	0.5999	0.0026
		r-CVAE	0.6186	0.001	0.6235	0.0028	0.5886	0.0014	0.5912	0.0012
18
Under review as a conference paper at ICLR 2022
Table 5: Additional results on Yahoo!R3 I.I.D. With Standard error
Yahoo!R3 I.I.D.			AUC	std	ACC	std	adv_AUC	std	adv_ACC	std
	p=2	CaRR	0.7493	0.0004	0.7495	0.0015	0.7188	0.0015	0.7072	0.0013
standard		r-CVAE	0.7487	0.0001	0.7529	0.0027	0.7202	0.0029	0.7099	0.0027
	p=∞	CaRR	0.7497	0.0004	0.7503	0.0019	0.7191	0.0023	0.7099	0.0026
		r-CVAE	0.7488	0.0001	0.7515	0.0008	0.7191	0.0021	0.7072	0.0015
	p=2	CaRR	0.7374	0.0024	0.7158	0.0061	0.7247	0.0026	0.7159	0.0036
robust		r-CVAE	0.7376	0.0018	0.7151	0.0045	0.7194	0.0020	0.7082	0.0021
	p=∞	CaRR	0.7378	0.0015	0.7168	0.0015	0.7210	0.0031	0.7107	0.0040
		r-CVAE	0.7341	0.0007	0.7093	0.0035	0.7180	0.0017	0.7080	0.0016
Table 6: Additional results on Coat OOD With Standard error
Coat OOD			AUC	std	ACC	std	adv_AUC	std	adv_ACC	std
	p=2	CaRR	0.5725	0.0005	0.5732	0.0005	0.5608	0.0003	0.5601	0.0004
standard		r-CVAE	0.5671	0.0005	0.5649	0.0006	0.5586	0.0002	0.554	0.0001
	p=∞	CaRR	0.5705	0.0013	0.5718	0.0017	0.5643	0.0001	0.5659	0.0006
		r-CVAE	0.5656	0.0005	0.5643	0.0007	0.5527	0.0074	0.5478	0.0081
	p=2	CaRR	0.5705	0.0015	0.5675	0.0015	0.5674	0.0002	0.565	0.0012
robust		r-CVAE	0.5634	0.0014	0.5591	0.0018	0.5572	0.0009	0.5522	0.0003
	p=∞	CaRR	0.5707	0.0017	0.5681	0.0024	0.5653	0.0019	0.5659	0.0011
		r-CVAE	0.5629	0.0017	0.5586	0.0028	0.559	0.0004	0.5544	0.0007
Table 7: Additional results on Coat I.I.D. With Standard error
Coat I.I.D.			AUC	std	ACC	std	adv_AUC	std	adv_ACC	std
	p=2	CaRR	0.7248	0.0011	0.7305	0.0016	0.7069	0.0023	0.7125	0.0036
standard		r-CVAE	0.7129	0.0009	0.7206	0.0022	0.7023	0.0041	0.7059	0.0061
	p=∞	CaRR	0.7283	0.0013	0.7355	0.0015	0.7125	0.0007	0.7196	0.001
		r-CVAE	0.7106	0.0029	0.7184	0.0033	0.7029	0.0008	0.7106	0.0094
	p=2	CaRR	0.7265	0.0032	0.7331	0.0027	0.7196	0.0046	0.7261	0.0042
robust		r-CVAE	0.7087	0.0005	0.7169	0.0016	0.7058	0.002	0.7141	0.0036
	p=∞	CaRR	0.7276	0.0028	0.7339	0.002	0.7208	0.0023	0.727	0.0019
		r-CVAE	0.7147	0.0023	0.7222	0.0026	0.7105	0.0039	0.7181	0.0043
19