Under review as a conference paper at ICLR 2022
Active Learning: Sampling in the
Least Probable Disagreement Region
Anonymous authors
Paper under double-blind review
Ab stract
Active learning strategy to query samples closest to the decision boundary can be
an effective strategy for sampling the most uncertain and thus informative samples.
This strategy is valid only when the sample’s “closeness” to the decision bound-
ary can be estimated. As a measure for evaluating closeness to a given decision
boundary of a given sample, this paper considers the least probable disagreement
region (LPDR) which is a measure of the smallest perturbation on the decision
boundary leading to altered prediction of the sample. Experimental results show
that the proposed LPDR-based active learning algorithm consistently outperforms
other high performing active learning algorithms and leads to state-of-the-art per-
formance on various datasets and deep networks.
1	Introduction
Active learning (Cohn et al., 1996) is a subfield in machine learning for attaining sample efficiency
by intelligently selecting a small subset of unlabeled samples for their labels to be used in training.
In many real-world learning problems, a large collection of unlabeled samples is assumed available,
and the labels of the most informative samples are iteratively queried for retraining the model based
on various query strategies such as uncertainty sampling (Lewis & Gale, 1994; Scheffer et al., 2001;
Culotta & McCallum, 2005; Wang et al., 2010; Nguyen et al., 2021), model change (Settles et al.,
2008; Freytag et al., 2014; Ash et al., 2020), Bayesian active learning (Pinsler et al., 2019; Shi & Yu,
2019), core-set (Sener & Savarese, 2018), error reduction (Roy & McCallum, 2001; Yoo & Kweon,
2019), variance reduction (Schein & Ungar, 2007), discriminative sampling (Sinha et al., 2019;
Gissin & Shalev-Shwartz, 2019; Zhang et al., 2020; Gu et al., 2020), feature matching between
unlabeled and validation dataset (Gudovskiy et al., 2020), active Thomson sampling (Bouneffouf
et al., 2014), minimize the redundancy by clustering (Yang et al., 2021), two-way exploration (Zhang
et al., 2015), and adaptive batch mode (Chakraborty et al., 2014). Active learning attempts to achieve
high accuracy using as few labeled samples as possible.
In uncertainty-based SamPling-the most popular strategy, quantifying precisely the uncertainty re-
mains an open question, and this paper is focused on this issue. This strategy is often considered for
its simplicity and relatively low computational load, and it enhances the performance of the current
model by utilizing the labels of the unlabeled samples whose predicted class is most vague (Settles,
2009; Yang et al., 2015; Sharma & Bilgic, 2017). It is generally understood that unlabeled sample
closest to the decision boundary is the most informative as the sample is the most uncertain (Balcan
et al., 2007; Kremer et al., 2014; Ducoffe & Precioso, 2018). Balcan et al. theoretically show that
selecting unlabeled samples with the smallest margin to the decision boundary attains exponential
improvement over random sampling in terms of sample complexity for binary classification with
linear separators (Balcan et al., 2007). However, many uncertainty-based sampling for multiclass
classification with deep network do not take into account the closeness of the sample to the decision
boundary for the reason that in multiclass classification with deep network, it is difficult to identify
samples closest to the decision boundary as the sample’s closeness based on Euclidean distance is
often not readily measurable (Ducoffe & Precioso, 2018; Mickisch et al., 2020).
This paper proposes a closeness measure that can be evaluated in multiclass classification with deep
network as a measure of uncertainty. We assume that the most uncertain and thus the most infor-
mative samples will have their labels most “sensitive” to the smallest perturbation of the decision
boundary, and that these samples are “closest” to the decision boundary. Here, the closeness are
1
Under review as a conference paper at ICLR 2022
defined as the sample’s sensitiveness to the perturbation of the decision boundary, and it is based on
the disagree metric between the decision boundary and its perturbation. The main contributions of
this paper are summarized as follows.
1.	This paper defines a measure of sample’s closeness to the decision boundary referred to
as the least probable disagreement region (LPDR) based on the disagree metric between
hypotheses.
2.	This paper introduces a hypothesis sampling method with a measure of disagreement in
sampled hypotheses, referred to as the disagree ratio, for obtaining the sample order in
terms of LPDR without evaluating the LPDR.
3.	This paper proposes a high performing active learning algorithm of querying unlabeled
samples closest to the decision boundary in terms of LPDR.
2	Related Work
Various forms of uncertainty measure have been studied. Entropy (Shannon, 1948) based uncertainty
sampling strategy queries unlabeled samples yielding the maximum entropy from the predictive dis-
tribution, but it does not perform well for multiclass-classification tasks as entropy does not equate
well with the closeness to the decision boundary (Joshi et al., 2009). Mutual information based
strategy which includes the BALD (Houlsby et al., 2011), DBAL (Gal et al., 2017), and Batch-
BALD (Kirsch et al., 2019) queries unlabeled samples yielding the maximum mutual information
between predictions and model parameters. The DBAL approximates the posterior of the model
parameters of deep network by MC-dropout sampling, but each batch selection is independently
conducted, and this leads to data inefficiency as correlations between data points in the batch are
not taken into account (Kirsch et al., 2019). To address this deficiency, BatchBALD is introduced,
but BatchBALD theoretically computes all possible mutual information between batch-wise predic-
tions and model parameters, and for this reason, it is not appropriate for large query size. Variation
ratio (Freeman, 1965) with ensemble method (Beluch et al., 2018) based on query by committee
(QBC) strategy (Seung et al., 1992) queries unlabeled samples yielding the maximum variation
ratio in labels predicted by the multiple networks, but it requires high computational load: each net-
work belonging to the ensemble must be individually trained. Gradient based strategy (Ash et al.,
2020) measures uncertainty as the gradient magnitude with respect to parameters in the final layer
and queries unlabeled samples where these gradients span a diverse set of directions, but it requires
high computational load when the dimension of parameters is large.
3	Least Probable Disagreement Region (LPDR)
This section theoretically defines LPDR and proposes “empirical LPDR” to approximate LPDR
based on sampling from the instance and hypothesis spaces. In addition, a brute-force method for
evaluating the empirical LPDR of each sample is described in determining the order of the closeness
to the decision boundary.
3.1	Definition of LPDR
Let X , Y , D, and H be respectively the instance space, the label space, the joint distribution over
(x, y) ∈ X × Y, and the hypothesis space of h : X → Y . The (pseudo) metric between two
hypotheses, referred to as disagree metric, is defined as the probability of the disagreement region in
X where labels are predicted differently by the two hypotheses (Hanneke et al., 2014; Hsu, 2010).
For h1, h2 ∈ H, the disagree metric between h1 and h2 is defined as:
ρ(hι, h2):= PX〜D[hι(X) = h2(X)]
where X is random variable from D . The LPDR of a sample for a given hypothesis is defined as the
least probable disagreement region for the hypothesis that contains the sample. For given h ∈ H, let
C∕∕f ∖ 1	, 1	, Cl	.1	T	F ∙ ,1 f	一，，
H(h, x) be the set of hypotheses disagreed with h on x ∈ X :
H(h, x) := {h ∈ H : h(x) = h(x)},
2
Under review as a conference paper at ICLR 2022
p(hι,h) = ∣6>ι∣∕τr
p(h2Jι) = ∖θ2∖∕π
P(h3,h) = ∖θ3∖∕π
(a)
h2,h3 ∈ H(h,x)
Lfl{x) = ∣¾∣∕π
(b)
rr・	1	O T ɪʌɪʌŋ 1	∙ ∙	1 T CTʌn Z ∖ ɪ ʃʌʃʌŋ r r ・	? ∙	1 ∙	1	.61・
Figure 1: Examples of LPDR and empirical LPDR. (a) LPDR ofx for given h in binary classification
with the linear classifier. Here x is uniformly distributed in R1 2 * * . The h2 and h3 disagree with h in
prediction on x, and ∣θ2|/n is the infimum for {ρ(h, h) : h ∈ H(h, x)}. Thus, Lh(x) = ∣θ21∕∏. (b)
Empirical LPDR of xi for h on MNIST dataset. The set of x-axis values of the blue dots for xi are
{ρS (hc, h) : hc ∈ HC(h, Xi)}, and Lh(Xi) = mm{ρs (hc, h) : hc ∈ HC(h, xi)} (blue arrow).
J1 八 T CTʌn n r ? ∙ λ n ι r 11
then the LPDR of X for h is defined as follows:
Lh(x) := Inf	ρ(h,h).
h∈H(h ,x)
The LPDR of X for h is the measure of closeness in terms of the disagree metric between h and
h that can alter the predicted label of X. The sample with the smallest LPDR is assumed to be
the sample most sensitive to the small perturbation of the decision boundary and thus closest to the
decision boundary.
Figure 1a shows an example of LPDR of X for given h in the binary classification with a set of
linear classifier, H = {h : h(X) = sgn(XTw), w ∈ W = R2 } where X is uniformly distributed
on X = R2. In X , X is a data point and w is represented as lw = {X : XTw = 0}. Let θi
be the angle between Iwi and lw, then the ρ(hi, h) = ∣θi∣∕π where the Unit of θi is radian and
—π ≤ θi ≤ π since hi(x) = h(x) for all X between Iwi and Iw. Here, h1 (x) = h(x), while
h2(x) = h(x) and h3(x) = h(x), thus h2, h3 ∈ H(h, x). In this case, ∣θ2 ∣∕π is the infimum for
C / ∙ι G∖ ∙ι /∙/∕G	. t	C	-T /	∖ I 八 I/
{ρ(h, h) : h ∈ H(h, x)}, therefore Lh(x)=仇∣∕π.
ɪ T	「J	1	J	∙	」 」 F ；	∙11 1	//FF	. 1 ∙ 1	1 Γ*	1111	1
Henceforth, unless otherwise stated, h will denote the hypothesis learned from labeled samples
1	.	1 1 r» ι ɪ CTXC n 7 ∙	ι . ι	ι ι ι ι	ι ι . ι ι
denoted by L and LPDR for h is evaluated on unlabeled samples denoted by U in this paper.
3.2 EMPIRICAL LPDR
In general, it is infeasible to explicitly evaluate LPDR for the following two reasons: 1) ρ cannot
be explicitly evaluated when D is unknown, and 2) it is difficult to evaluate ρ(h, h) for all h ∈ H
especially when |H| = ∞.
To address the first reason, ρ(h, h) can be approximated as

1S
1X
i=1

where I[∙] is an indicator function, and X1,..., XS are i.i.d. random variables from D. By strong
ʌ
ʌ
law of large number, ∣ρs (h, h) — ρ(h, h)| → 0 with probability 1 as S → ∞.
To address the second issue, we consider HC ⊂ H of size C satisfying the following property.
Property 1 For any given X ∈ U and any h ∈ H with ρ(h, h) ≤ maxx∈U Lh (x), there exists
hc ∈ HC ⊂ H satisfying that hc(X) = h(X) and that for any > 0,
∣ρ(h,h) — ρ(hc,h)∣ < e
as C → ∞.
3
Under review as a conference paper at ICLR 2022
For HC satisfying Property 1, we define empirical LPDR as follows:
T /	∖	∙ C	/7	1~ ∖
Lh(X)=	inf ʌ Ps(hc,h)∙
hc∈Hc (h,x)
(1)
ɪ τ	i'	∙	_ T /	FfC∖	C 7	_ C t	7∕∖∕f∕∖1	El	∙ ∙ IT -rʌ-rʌ τ-1
Here for given	x ∈ U	and h, HC (h, x)	= {hc	∈ HC : hc(x) 6= h(x)}.	The empirical LPDR
converges to LPDR and the rank-orders based on the empirical LPDR and LPDR are equivalent
when
logC → 0
S
(2)
as min(S, C) → ∞ by the following theorem.
Theorem 1 Suppose that HC is the set of hypotheses satisfying Property 1 where U = {xi}iM=1 and
Eq. 2 holds, then the following two statements can be made.
1.	(convergence) For any x ∈ U,
.~ , _ ..
ILh(X)- Lh(X)I → 0.
2.	(rank-order consistency) If Lh(Xi) = Lh(Xj) for i= j, then
Lh(Xi) < Lh (Xj )--------------------------------⇒ Lh(Xi) < Lh(Xj)
with probability tending to 1 as min(S, C) → ∞, where S is the number of i.i.d. random variables
for approximating ρ and C = IHC I.
The proof of Theorem 1 is deferred to Appendix A.1.
3.3	Brute-force Search for Empirical LPDR
To construct HC satisfying Property 1, we consider Gaussian sampling of parameters corresponding
to hypotheses with gradually increasing variance based on the following conjecture:
Conjecture 1 Suppose that h is sampled with W 〜N(w, Iσ2) where w, W ∈ W are parameters
ofh, h respectively, then E[ρ(h, h)] is continuous and strictly increasing with σ.
The theoretical and empirical verifications of Conjecture 1 are presented in Appendix C.1. At first,
many h(k) are sampled with w(k)〜 N(W,Iσk) , and a set of hypotheses H(k) is constructed as
HIk) = {hnk : (k - 1)e < ρs(hk), h) < ke}N=ι by the selection of sampled hypotheses. Then,
HC can be constructed as:
K
HC = [ H(k)
k=1
with {σk}K=ι such that σk < σk+ι where K is the smallest value satisfying that HC(h, X) = 0 for
all X ∈ U. Here, for any h ∈ H with ρ(h, h) ≤ maxχ∈u Lh(X), there exists k ∈ [K] such that
(k 一 1)e ≤ ρ(h, h) ≤ ke, and thus ∣Ps (hnk, h) — ρ(h, h)∣ < e for all h* ∈ Hlkk. Sampling large N
hypotheses from each σk provides a high probability that there exists h(nk) such that h(nk) (X) = h(X)
for X ∈ U, so that Property 1 is stochastically satisfied. Also, by setting S	logC, Eq. 2 is
satisfied. Consequently, the empirical LPDR of X for h is obtained by Eq. 1.
Figure 1b depicts an example of Lh(Xi) for Xi ∈ U on MNIST dataset. The Xi is the ith sample
ordered by empirical LPDR. The set of x-axis values of the blue dots on the horizontal line, whose
y-axis value is i, are {ρs (hc, h) : hc ∈ HC (h, Xi)}. Thus, Lh (Xi) is the x-axis value of the leftmost
blue dot for Xi (indicated by the blue arrow).
4
Under review as a conference paper at ICLR 2022
(a)
(b)
Figure 2: The need for regulating σ (MNIST). (a) The disagree ratios of the samples goes to 0 as σ
1	1	, C C	♦	∕ι ∖ El τπ Γ /7八1。	7 一 c/ 1	,ι	ι
decreases and goes to 0.9 as σ increases. (b) The E[ρS (h, h)] for h ∈ HB decreases as the number
of labeled samples increases when HB is constructed with a static σ for all acquisition steps.
3.4	Rank of LPDR
Samples with small LPDR are more sensitive to perturbation of the decision boundary, thus the label
of the samples are more likely to be altered by the hypothesis perturbation. That is the disagreement
in sampled hypotheses with h may have an inverse relation with empirical LPDR. This paper defines
a measure of the disagreement in sampled hypotheses with h on x, referred to as disagree ratio:
D(h, x):二
|Hb(h, x)|
IHB 1
1N
-1X
N乙
n=1
I[hn(x) = h(x)]
1	Zl /	/ ?	∖ C 1	-C/	7	(	∖	∕7,/	∖1zl∕	Cl Λ 1∖T	17	∙	Il ∙,ι
where HB(h, x) = {hn ∈ HB ： hn(x) = h(x)}, HB = {hn}N=ι, and hn is sampled with Wn ~
N(W, Iσ2). Then, We formulate the following conjecture:
Conjecture 2 Suppose that HB is constructed with W ~ N(W, Iσ2) and 0 < σ < ∞. Then,
D(h, xι) > D(h, X2) V⇒ Lh(xι) < Lh(X2)	(3)
with probability tending to 1as |HB | → ∞ where D(h, x) = |HB (h, x)|/|HB |.
The theoretical and empirical verifications of Conjecture 2 are presented in Appendix C.2. The Eq. 3
of Conjecture 2 implies that we can use D(h, x) to identify the order of Lh(x), required to query
closest samples to hypothesis. Our motivation is to find a measure to identify the order of LPDR
without evaluating the empirical LPDR. We would like the measure evaluation to be computationally
lighter than that of the empirical LPDR.
Obtaining the order of LPDR by the disagree ratio can reduce time over the brute-force search for
empirical LPDR in Section 3.3. Let M , K, N, and S be the number of unlabeled samples, grid
for σk , sampled hypotheses for each σk, and i.i.d. random variables from D for approximating ρ,
respectively. The brute-force search requires time complexity of O(M × K × N × S). While, using
the disagree ratio requires the time complexity of O(M × N). In the results of the active learning
performance comparison between evaluating empirical LPDR by brute-force search and evaluating
the order of empirical LPDR by using disagree ratio on various datasets, there is no significant
difference in the performance between the two methods (See Appendix D).
4	LPDR-based Active Learning
This section introduces the proposed LPDR-based active learning algorithm with the disagree ratio
referred to as ‘DRAL’ and its variation with the weighted disagree ratio referred to as ‘DRAL+’.
This paper considers a pool-based active learning that queries q most informative samples from
randomly sampled pool data P ⊂ U of size m where U is unlabeled samples.
4.1	Variance for Disagree Ratio
When constructing HB, setting σ is an important issue as shown in the following theorem:
5
Under review as a conference paper at ICLR 2022
Algorithm 1 DRAL
Input:
L0, U0 : Initial labeled and unlabeled samples
σ2 : Initial variance for sampling
ρ* : Target disagree metric (= q/m)
Procedure:
for step t = 0 to T - 1 do
Obtain Wt by training with Lt
for n = 1 to N do
Wn ~ N(Wt, Iσ2)
P = PS(hn,ht)
σ — σe-β(ρ0-ρ*) where β > 0
end for
D(ht, Xi) for i ∈ IPt = {j : Xj ∈ Pt ⊂ Ut}
工* = arg maxI⊂Ipt,∣I∣ = q Pi∈I D(ht, Xi)
Lt+1 = Lt∪{ (xi , yi )}i∈I* , Ut+ 1 = Ut∖{xi}i∈I*
end for
Algorithm 2 DRAL+
Input:
Lo,U0,σ2, ρ*
Procedure:
for step t = 0 to T - 1 do
Obtain Wt by training with Lt
Evaluate empirical error εt of ht
forn= 1 to N do
Wn ~ N(Wt, Iσ2)
Evaluate empirical error εn of hn
Yn = e-(εn-εt)
0
P0 = PS (hn, ht ), then update σ
end for
Dw (ht, Xi) for i ∈ IPt
I* = arg maxI⊂Ipt,∣I∣=q Pi∈I Dw(ht, Xi)
Update Lt+1 and Ut+1
end for
Theorem 2 Consider the binary classification with the linear classifier on bounded X, i.e., H =
{h : h(x) = sgn(xTw)} and supX∈X kxk∞ < ∞. Suppose that HB = {hn}nN=1 is constructed
with Wn 〜N(W, Iσ2) for gi^ven h ∈ H. Then, for all X ∈ X, 1) D(h, x) → 0 when σ → 0 and 2)
D(h, x) → 1/2 when σ → ∞ in probability as N →∞, where D(h, x) = |HB (h, x)|/|HB |.
The proof of Theorem 2 is deferred to Appendix A.2. The implication of Theorem 2 is that when
σ is too small or too large, it would be difficult to obtain the order of LPDR by comparing the
disagree ratios. In practice, N is finite, thus setting σ for HB is more important. Figure 2a shows
that the disagree ratios of samples converge to 0 as σ → 0 and converge to 0.9 as σ increases
when |HB | = 100. Thus, it is required to set an appropriate σ, but finding an appropriate σ is
computationally prohibitive. Additionally, the behavior of D(h, x) with the fixed σ is not same with
respect to the number of labeled samples. Figure 2b shows that E[ρS (hn, h)] for hn ∈ HB decreases
as the number of labeled samples increases on MNIST dataset when HB is constructed with a static
σ = 0.1 for all acquisition steps in active learning. Decrease in E[ρS (hn, h)] leads to decrease in
|HB(h, x)| for each x, eventually D(h, x) converges to zero.
E FF	.1	1 1	F, K	1,1	1	∙ ∙ , ∙	,	,	1	EI^∕7 f∖l
To address these problems, σ needs to be regulated at each acquisition step to keep E[ρS (hn, h)]
static. In Figure 6c of Appendix C.1, E[ρS (hn, h)] is almost a linear function of logσ in the ascen-
βE[	(h h)],
sιon, that is, σ a eβE[ρs(hn,h)] for some β > 0. Based on this observation, we can keep E[ρs (hn h)]
static by updating σ as follows:
σ 一 σe-β(ρs-ρ*)
where ρ* is the target disagree metric (see Appendix E: E[ρ, (hn h)] is securely guided towards the
target value). The remaining question is how to determine an appropriate ρ*. To identify q most
informative samples from unlabeled sample set of size m, we set ρ* = q/m and achieved high
performance (see Appendix F: the proposed algorithm shows high performance when ρ* = q/m).
4.2	Algorithm with Disagree Ratio (DRAL)
The proposed LPDR-based active learning algorithm with the disagree ratio referred to as ‘DRAL’
is provided in Algorithm 1. Let Lt, Ut and Pt ⊂ Ut be labeled samples, unlabeled samples, and
pool data of size m at step t respectively. At step t, Wt is obtained by training with Lt, then hn is
sampled with Wn 〜N(wt, Iσ2) for n = 1,...,N. Here, σ is updated so that the E[ρs (hn ht)]
achieves the target value ρ*. Then, DRAL queries the top q unlabeled samples having the highest
disagree ratio from Pt .
4.3	Algorithm with Weighted Disagree Ratio (DRAL+)
When calculating the disagree ratio, each sampled hypothesis is given equal weight. However, we
observe performance variation among the sampled hypotheses (See Appendix G). For this reason,
6
Under review as a conference paper at ICLR 2022
Table 1: Settings for data and acquisition size. Acquisition size denotes the number of initial labeled
samples + query size for each step (the size of pool data) → the number of final labeled samples.
Dataset	Model	# of parameters sampled / total	Data size train / validation / test	Acquisition size		
MnIst	S-CNN	-1.3K/1.2M	55,000/5,000/10,000	20	+20 (2,000)	→ 1,020
CIFAR10	K-CNN	5.1K/2.2M	45,000/5,000/10,000	200	+400 (4,000)	→ 9,800
SVHN	K-CNN	5.1K/2.2M	68,257/5,000/26,032	200	+100 (2,000)	→ 10,200
CIFAR100	WRN-16-8	51.3K/11.0M	45,000/5,000/10,000	5,000	+2,000 (10,000)	→ 25,000
Tiny ImageNet	WRN-16-8	409.8K/11.4M	90,000/10,000/10,000	10,000	+5,000 (20,000)	→ 50,000
HAM10000	WRN-16-8	14.3K/11.0M	7,015/ 1,500/ 1,500	500	+300 (3,000)	→ 3,500
this paper introduces weighting factor γn on hn such that more/less weight is placed on hn that
performs better/worse than h on labeled samples. The weighting factor is given below as
Yn = e-(εn-εt),
where εn and εt are the empirical errors of hn and ht respectively. Then, the following weighted
disagree ratio can be defined as shown below as
D h TPn=I YnI[hn(x) = ht(x)]
Dw (ht, x) : =	N	.
n=1 γn
The details of the algorithm and the framework with the weighted disagree ratio referred to as
‘DRAL+’ are provided in Algorithm 2 and Appendix M. In the results of the performance compar-
ison between DRAL+ and DRAL on various datasets, DRAL+ consistently either performs better
than or comparable with DRAL (See Appendix H).
5	Experiments
This section discusses experimental results for performance comparison with the baseline active
learning algorithms on benchmark datasets in deep learning. A total of 6 benchmark datasets
are used for experiments: MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky et al., 2009),
SVHN (Netzer et al., 2019), CIFAR100 (Krizhevsky et al., 2009), Tiny ImageNet (subset of the
ILSVRC dataset containing 200 categories; Russakovsky et al., 2015), and HAM10000 (Tschandl
et al., 2018) datasets. Three CNN networks are used as deep networks: S-CNN, K-CNN (Chollet
et al., 2015) and Wide-ResNet (WRN-16-8; Zagoruyko & Komodakis, 2016). Results are averaged
over 5 repetitions. The settings for data, initial, and query sizes are summarized in Table 1. We
use 100 forward passes for MC-dropout sampling, and ensemble consists of 5 networks of identical
architecture but different random initialization and random batches. There is no significant perfor-
mance difference between when parameter sampling is performed in the entire layer and in the last
layer, thus parameters are sampled in the last layer for computational efficiency. We set hyperparam-
eters for DRAL+ as σ0 = 0.01, β = 1, and N = 100 in convenience as DRAL+ is robust against
hyperparameters (see Appendix I). The details of datasets, networks, and training settings are pre-
sented in Appendix B. Figure 3-5 show plots of test accuracy enlarged in appropriate to accentuate
the performance difference among different methods: initial labeled sample sizes are not shown in
the figures. Figures that include initial labeled sample size are presented in Appendix K.
5.1	RESULTS FOR MNIST, CIFAR10 AND SVHN
A number of experiments are conducted to compare performance of DRAL+ with the baseline ac-
tive learning algorithms including state-of-the-art algorithms. Figure 3 shows the test accuracy with
respect to the number of labeled samples on MNIST, CIFAR10 and SVHN datasets. Each algorithm
is denoted as follows ‘Entropy’: entropy-based uncertainty sampling (Shannon, 1948), ‘Coreset’:
core-set selection (Sener & Savarese, 2017), ‘MC-BALD’: MC-dropout sampling with BALD (Gal
et al., 2017), ‘MC-VarR’: MC-dropout sampling with variation ratio (Ducoffe & Precioso, 2015),
‘ENS-VarR’: ensemble method with variation ratio (Beluch et al., 2018), and ‘BADGE’: batch ac-
tive learning by diverse gradient embeddings (Ash et al., 2020). Overall, DRAL+ either consistently
performs best or comparable with other algorithms on both datasets. Entropy shows the poor per-
formance compared to other uncertainty-based algorithms in all results. Coreset shows the worst
7
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
Figure 3:	The performance comparison of DRAL+ with the baseline active learning algorithms on
MNIST (a), CIFAR10 (b) and SVHN (c) datasets. Overall, DRAL+ consistently either performs
best or comparable with all other algorithms regardless of dataset.
(a)
(b)
Figure 4:	The performance comparison of DRAL+ with the baseline active learning algorithms on
on CIFAR100 (a) and Tiny ImageNet (b) datasets with WRN-16-8 networks. These datasets are
more difficult. DRAL+ outperforms all other algorithms on more difficult tasks.
performance compared to all other algorithms including Random. MC-BALD performs comparable
with DRAL+ on SVHN, but it shows the poor performance on MNIST and CIFAR10. MC-VarR
and ENS-VarR show a significant performance drop compared to DRAL+ on all datasets. BADGE
performs comparable with DRAL+ on MNIST and CIFAR10 datasets, but is shows a significant
performance drop compared to DRAL+ on SVHN dataset. It is observed that the performance of
other algorithms has a relatively strong dataset or network dependency compared to DRAL+ .
Furthermore, the running time of DRAL+ for active learning is comparable to Entropy, MC-BALD,
MC-VarR and Coreset. ENS-VarR requires about 5 times more computational load than DRAL+,
and BADGE requires several times more computational load than DRAL+ when the parameter
dimension and query size are very large such as Tiny ImageNet with WRN-16-8. The details of the
running time is presented in Table 4 of Appendix J.
5.2	Results for CIFAR100 and Tiny ImageNet
Experiments on more difficult task are conducted. Figure 4 shows test accuracy with respect to
the number of labeled samples on Tiny ImageNet dataset with WRN-16-8. CIFAR100 and Tiny
ImageNet are considered to be more difficult task than other benchmark datasets. Even on more
difficult tasks, DRAL+ outperforms all other algorithms.
5.3	Results for HAM 1 0000
Additional experiments are conducted to compare the performance of the algorithms on imbalanced
HAM10000 dataset with WRN-16-8. Figure 5a shows the results of the test accuracy with respect
to the number of labeled samples. DRAL+ outperforms all other algorithms compared. Figure 5b
shows the results of AUC with respect to the number of labeled samples. DRAL+ performs better
than or comparable with all other algorithms.
8
Under review as a conference paper at ICLR 2022
(a)
(b)
Figure 5: The performance comparison on imbalanced HAM10000 dataset with WRN-16-8 in terms
of the test accuracy (a) and AUC (b). DRAL+ performs better than or comparable with all other
algorithms.
Table 2: The mean ± standard deviation of the performance differences (%) relative to DRAL+
for each algorithm and each dataset. The negative value indicates lower performance compared to
DRAL+, and the asterisk (*) indicates that the P-Value ls less than 0.05 in one-sample t-test for the
performance differences. DRAL+ consistently and significantly outperforms other algorithms for
all datasets, while the performance of the algorithms except DRAL+ vary depending on datasets.
	MNIST	CIFAR10	SVHN	CIFAR100	T. ImageNet	HAM10000
DRAL+[ours]	0.00±0.00	0.00±0.00	0.00±0.00	0.00±0.00	0.00±0.00	0.00±0.00
Random	-3.26±0.44*	-1.05±0.26*	-2.94±0.15*	-1.16±0.49*	-0.99±0.45*	-2.24±0.48*
Entropy41	-1.01±0.26*	-0.97±0.35*	-1.75±0.13*	-0.69±0.32*	-1.40±0.36*	-0.69±0.42*
Coreset36	-3.34±0.78*	-4.77±0.19*	-5.93±0.35*	-0.28±0.33	-0.90±0.32*	-1.13±0.33*
MC-BALD14	-1.70±0.48*	-1.25±0.35*	-0.29±0.04*	-0.67±0.22*	-0.58±0.39*	-1.75±0.37*
MC-VarR10	-0.67±0.31*	-0.75±0.46*	-0.90±0.10*	-0.42±0.24*	-0.64±0.49*	-1.63±0.38*
ENS-VarR3	-0.47±0.36*	-0.65±0.48*	-0.82±0.07*	-1.07±0.21*	-1.43±0.30*	-0.52±0.26*
BADGE1	-0.26±0.10*	-0.22±0.44	-1.46±0.05*	-0.31±0.77	-0.79±0.51*	-0.86±0.29*
5.4 Summary
Each cell of Table 2 presents the mean and standard deviation of five AS for each algorithm and
dataset, where A is the average of performance differences relative to DRAL+ over all steps for
each repetition. The negative value indicates lower performance compared to DRAL+ , and the
asterisk (*) indicates the p-value is less than 0.05 in one-sample t-test for the null of no difference
versus the alternative that the DRAL+ is better. DRAL+ consistently outperforms other algorithms
on all datasets, while the performance of the algorithms except DRAL+ vary depending on datasets.
Furthermore, DRAL+ shows significant performance improvement in most cases (39 out of 42).
6 Conclusion
This paper defines a measure of sample’s closeness to the decision boundary of the current network
referred to as the least probable disagreement region (LPDR) based on the disagree metric between
hypotheses. In addition, this paper introduces a hypothesis sampling method with a measure of
disagreement in sampled hypotheses referred to as the disagree ratio for obtaining the order of LPDR
without explicit or empirical evaluation of LPDR for computational efficiency. Based on the order
of LPDR, this paper proposes an uncertainty-based active learning algorithm of querying unlabeled
samples closest to the current decision boundary in terms of LPDR.
The proposed LPDR-based active learning algorithm consistently outperforms all high performing
active learning algorithms and leads to state-of-the-art active learning performance on all datasets in
this paper. In addition, the proposed algorithm is simple enough to perform only parameter pertur-
bation and can be applied to a variety of classification tasks with both shallow and deep networks.
Furthermore, the proposed algorithm runs fast enough to comparable to entropy-based uncertainty
sampling for it requires a low computational load. In conclusion, LPDR-based sampling with the
disagree ratio by parameter perturbation is an effective active learning algorithm based on the un-
certainty of the current network.
9
Under review as a conference paper at ICLR 2022
References
Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal.
Deep batch active learning by diverse, uncertain gradient lower bounds. In International
ConferenceonLearning Representations, 2020. URL https://openreview.net/forum?
id=ryghZJBKPS.
Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In
International Conference on CompUtational Learning Theory, pp. 35-50. Springer, 2007.
William H Beluch, Tim Genewein, Andreas NUrnberger, and Jan M Kohler. The power of ensembles
for active learning in image classification. In Proceedings of the IEEE Conference on COmPUter
ViSiOn and Pattern Recognition, pp. 9368-9377, 2018.
Djallel Bouneffouf, Romain Laroche, Tanguy Urvoy, Raphael Feraud, and Robin Allesiardo. Con-
textual bandit for active learning: Active thompson sampling. In InternatiOnal COnferenCe on
NeUraI InfOrmatiOn PrOCeSsing, pp. 40542. Springer, 2014.
George EP Box. A note on the generation of random normal deviates. Ann. Math. Statist., 29:
610-611, 1958.
Shayok Chakraborty, Vineeth Balasubramanian, and Sethuraman Panchanathan. Adaptive batch
mode active learning. IEEE transactions on neural networks and Iearning systems, 26(8):1747-
1760, 2014.
Francois Chollet et al. Keras. https://keras.io, 2015.
David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models.
JOUrnal of artificial intelligence research, 4:129-145, 1996.
Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In
AAAI, volume 5,pp. 746-751, 2005.
Melanie Ducoffe and Frederic Precioso. Qbdc: query by dropout committee for training deep su-
pervised architecture. arXiv PrePrint arXiv:1511.06412, 2015.
Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. arXiv PrePrint arXiv:1802.09841, 2018.
Linton C Freeman. EIementary applied StatiStics: for StUdentS in behavioral science. John Wiley &
Sons, 1965.
Alexander Freytag, Erik Rodner, and Joachim Denzler. Selecting influential examples: Active learn-
ing with expected model output changes. In EurOPean COnferenCe on COmPUter ViSion, pp. 562-
577. Springer, 2014.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data.
In Proceedings of the 34th International COnference on MaChine Learning-Volume 70, pp. 1183-
1192. JMLR. org, 2017.
Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv PrePrint
arXiv:1907.06347, 2019.
Bin Gu, Zhou Zhai, Cheng Deng, and Heng Huang. Efficient active learning by querying discrimina-
tive and representative samples and fully exploiting unlabeled data. IEEE TranSaCtiOnS on NeUral
NetWOrkS and Learning Systems, 2020.
Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, and Sotaro Tsukizawa. Deep active
learning for biased datasets via fisher kernel self-supervision. In Proceedings of the IEEE/CVF
COnferenCe on COmPUter ViSiOn and Pattern Recognition, pp. 9041-9049, 2020.
Steve Hanneke et al. Theory of disagreement-based active learning. FOUndatiOnS and Trends® in
MaChine Learning, 7(2-3):131-309, 2014.
10
Under review as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on CompUter vision, pp. 1026-1034, 2015.
Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Mate Lengyel. Bayesian active learning for
classification and preference learning. arXiv PrePrint arXiv:1112.5745, 2011.
Daniel Joseph Hsu. Algorithms for active Iearning. PhD thesis, UC San Diego, 2010.
Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image
classification. In 2009 IEEE COnferenCe on COmPUter ViSiOn and Pattern Recognition, pp. 2372-
2379. IEEE, 2009.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acqui-
sition for deep Bayesian active learning. In AdVanCeS in NeUraI InfOrmatiOn PrOCeSSing Systems,
pp. 7024-7035, 2019.
Jan Kremer, Kim Steenstrup Pedersen, and Christian Igel. Active learning with support vector
machines. Wiley InterdiSCiPIinary Reviews: Data Mining and Knowledge Discovery, 4(4):313-
326, 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. https:
//www.cs.toronto.edu/~kriz/cifar.html, 2009.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist, 1998.
David D Lewis and William A Gale. A sequential algorithm for training text classifiers. In SIGIR’94,
pp. 3-12. Springer, 1994.
David Mickisch, Felix Assion, Florens Greβner, Wiebke Gunther, and Mariele Motta. Under-
standing the decision boundary of deep neural networks: An empirical study. arXiv PrePrint
arXiv:2002.01810, 2020.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and A Ng. The street view
house numbers (svhn) dataset, 2019.
Vu-Linh Nguyen, Mohammad Hossein Shaker, and Eyke Hullermeier. How to measure uncertainty
in uncertainty sampling for active learning. MaChine Learning, pp. 1-34, 2021.
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Bayesian
batch active learning as sparse subset approximation. In AdVanceS in NeUraI InfOrmatiOn
PrOCeSSing Systems, pp. 6359-6370, 2019.
Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo estima-
tion of error reduction. ICML, Williamstown, pp. 441^48, 2001.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of COmPUter ViSion, 115(3):211-252, 2015.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for in-
formation extraction. In International SymPOSiUm on Intelligent Data Analysis, pp. 309-318.
Springer, 2001.
Andrew I Schein and Lyle H Ungar. Active learning for logistic regression: an evaluation. MaChine
Learning, 68(3):235-265, 2007.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv PrePrint arXiv:1708.00489, 2017.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In 6th International COnferenCe on Learning RePreSentations, ICLR 2018, VanCOUver,
bC, Canada, APriI 30 - May 3, 2018, COnference TraCk Proceedings. OpenReview.net, 2018.
URL https://openreview.net/forum?id=H1aIuk-RW.
11
Under review as a conference paper at ICLR 2022
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Burr Settles, Mark Craven, and SoUmya Ray. Multiple-instance active learning. In Advances in
neural information Processing systems, pp. 1289-1296, 2008.
H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Proceedings
of the fifth annual WorkShoP on Computational Iearning theory, pp. 287-294, 1992.
Claude E Shannon. A mathematical theory of communication. Bell SyStem technical journal, 27(3):
379T23,1948.
Manali Sharma and Mustafa Bilgic. Evidence-based uncertainty sampling for active learning. Data
Mining and Knowledge Discovery, 31(1):164-202, 2017.
Weishi Shi and Qi Yu. Integrating bayesian and discriminative sparse kernel machines for multi-
class active learning. In AdVanceS in Neural Information Processing Systems, pp. 2285-2294,
2019.
Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In
ProCeedingS of the IEEE International ConferenCe on ComPUter ViSion, pp. 5972-5981, 2019.
Charles Spearman. “General Intelligence” objectively determined and measured. AmeriCan JoUrnaI
OfPSyChology, 15:201-293, 1904.
Simon Tong and Edward Chang. Support vector machine active learning for image retrieval. In
ProCeedingS of the ninth ACM international ConferenCe on Multimedia, pp. 107-118, 2001.
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions. SCientifiC data, 5:
180161, 2018.
Shuo Wang, Jian-Jian Wang, Xiang-Hui Gao, and Xue-Zheng Wang. Pool-based active learning
based on incremental decision tree. In 2010 International ConferenCe on MaChine Learning and
Cybernetics, volume 1, pp. 274-278. IEEE, 2010.
Yazhou Yang, Xiaoqing Yin, Yang Zhao, Jun Lei, Weili Li, and Zhe Shu. Batch mode active learning
based on multi-set clustering. IEEE ACcess, 9:51452-51463, 2021.
Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G Hauptmann. Multi-class active
learning by uncertainty sampling with diversity maximization. International Journal of Computer
Vision, 113(2):113-127, 2015.
Donggeun Yoo and In So Kweon. Learning loss for active learning. In ProceedingS of the IEEE
ConferenCe on ComPUter ViSion and Pattern Recognition, pp. 93-102, 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv PrePrint
arXiv:1605.07146, 2016.
Beichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, and Qingming Huang. State-
relabeling adversarial active learning. In ProCeedingS of the IEEE/CVF ConferenCe on ComPUter
ViSion and Pattern Recognition, pp. 8756-8765, 2020.
Xiao-Yu Zhang, Shupeng Wang, and Xiaochun Yun. Bidirectional active learning: A two-way
exploration into unlabeled and labeled data set. IEEE transactions on neural networks and Iearning
SyStems, 26(12):3034-3044, 2015.
12
Under review as a conference paper at ICLR 2022
A Proofs of Theorems
A.1 Proof of Theorem 1
Since M < ∞ and Lh(Xi) = Lh(Xj) for i = j, there exists δ > 0 such that
δ =mnlLh(Xi)-Lh (Xj 儿
and our strategy is to show that supi ∣Lh∕xi) - Lh(Xi) | < δ∕2 in probability.
At first, We prove the convergence of empirical LPDR to LPDR. Since ∣I[h(X) = h(X)]∣ ≤ 1 for
any h ∈ H, Hoeffding’s inequality implies that for any > 0,
P h∣Ps(h,h)-ρ(h,h)∣ ≥e] ≤ 2e-c1e2s
with c1 > 0, and
2
P sup	∣ps (hc, h) — ρ(hc, h)| ≥ e ≤ 2Ce 1 S	(4)
hc∈Hc (h,x)	_
1	In t Z T~ ∖ I —	- Tn ,1	-TA	, t ∙	1 ∙	,1	. Γ∙	Γ∖ .∖	1' 11	IlF
because |HC (h, X)| ≤ C. Furthermore, Property 1 implies that for any e > 0, the following holds:
For all h ∈ H(h, x),
I /7 τκ∖	/7 τκ∖I	/1、
∣ρ(hc, h) - ρ(h, h)| < e	(5)
as C → ∞ because of Property 1. Additionally, by Eq. 4 and 5, for any e > 0, we have that
\	{∃hc ∈ HC(h, x)	s.t.
...ʌ .
h∈H(h ,x)
,	,一 3、	，3、，	. I
∣PS(hc,h) - ρ(h, h)| < 2e
(6)
with probability equal to or greater than 1 - 2Ce-c12S when C is sufficiently large. It is because
the ∣ρS(hc, h) — ρ(h, h)| ≤ ∣ρS(hc, h) — ρ(hc, h)| + ∣ρ(hc, h) — ρ(h, h)| and Eq. 6 implies that
SuphC∈Hc(^χ) ∣Ps (h, h) - ρ(h, h)| < e with probability equal to or greater than 1 - 2Ce-c1e S.
Then,
inf	ps(hc, h)	— 2e	≤ inf	ρ(h,	h)	≤ inf	ps(hc, h) +	2e	(7)
hc∈Hc (h ,x)	h∈H(h,x)	hc∈Hc (h,x)
with probability equal to or greater than 1 - 2Ce-c12S. We’ll prove the lower bound and upper
bound of Eq. 7 separately.
Let the set HC(h, x) ⊂ HC(h, x) be a smallest subset satisfying the Property 1, i.e. all elements of
HC(h, x) are used to approximate ρ(h, h) for all h ∈ H(h, x). Then, Eq. 6 implies that
inf	ρS (h, x) - 2e ≤ inf	ρ(h, x)
h c∈HC (h ,x)	h∈H(h,x)
with probability equal to or greater than 1 - 2Ce-c12S when S is sufficiently large. In addition, the
property of infimum implies that inf h ∈∏c (h χ) PS (h, x)-2e ≤ inf h ∈n^ @ χ) PS (h, x)-2e, which
proves the lower bound ofEq. 7. Also, Eq 4 implies that PIh ∈∏c@ χ){∣ρS(hc, h) - ρ(hc, h)| < e}
with probability equal to or greater than 1 - 2Ce-c12S when S is sufficiently large. Thus we have
inf	P(h, x) ≤ inf	PS (h, x) + 2e,
hc∈Hc (h ,x)	hc∈Hc (h,x)
and by the property of infimum, infh∈H(h χ) ρ(h, x) ≤ infh ∈∏c@ χ) ρ(h, x), which proves the
upper bound of Eq. 7.
Consequently, by the definition of LPDR and empirical LPDR,
P h∣Lh(x) - Lh(x)l < 2e] ≥ 1 - 2Ce-c1 JS,	(8)
13
Under review as a conference paper at ICLR 2022
which goes to 1 as min(S, C) → ∞ and log C/S → 0 by Eq. 2. This implies the convergence of
empirical LPDR to LPDR. Next, we prove the rank-order consistency between the empirical LPDR
and LPDR of {xi}iM=1. It is trivial that
P
maχ∣Lh(xi) - Lh(Xi)I ≥ 2e
is equal to or less than
MM
XP IjLh(Xi)- Lh(Xi)I ≥2e] = X {l- PbLh(Xi)- Lh(Xi)| <2e]},
i=1	i=1
2S
and it has the upper bound of 2MCe-c1 S by the Eq. 8. Consequently,
max
i∈[M]
I τ /	∖ τ /	∖ I /ʌ
ILh(Xi)- Lh(Xi)| < 2e
(9)
with probability tending to 1 as min(S, C) → ∞ by Eq. 2, and it holds for any such that 0 < 2 <
δ∕2. Then the uniform convergence of empirical LPDR on {Xi}M=ι, denoted by the Eq. 9., implies
that the maximum difference between all pairs of Lh(Xi) and Lh(Xi) is less than the minimum of
pair-wise differences of Lh(Xi)s with probability tending to 1 as min(S, C) → ∞. Therefore, it
implies the rank-order consistency between the empirical LPDR and LPDR:
\ {lh(Xi)- Lh(Xj) > 0=⇒ Lh(Xi)- Lfl(Xj) > 0}
i6=j
with probability tending to 1 as as min(S, C) → ∞ because the contra-positive such that if Lh(Xi)-
Lh (Xj) ≤ 0, then
Lh(Xi) - Lh(Xj) < -δ < 0
and
Lh(Xi)- Lh(Xj) < Lh(Xi) - Lh(Xj) +4e < -δ + 4e ≤ 0
holds uniformly on i 6= j with probability tending to 1 as min(S, C) → ∞ by the Eq. 9 implying
that
Iimax |Lh(Xi)- Lh(Xj)| < 4e = \ {|Lh(Xi)- Lh(Xj)| < 4e}
i6=j
with probability tending to 1 as min(S, C) → ∞.
A.2 Proof of Theorem 2
The disagree ratio is
1N
D(h, x) = N X I IMX) = h(X) 卜
n=1
and hn(x) disagrees with h(x) if Sgn(XTwn) = Sgn(XTW), here, sgn(0) = 1. Letkwk = 1
without the loss of generality, kXk 6= 0 to avoid the null, and note that
XTwn = XTw + σXTen
where en = (Zni,..., Zn∣w∣)T and Znk 〜N(0,12). Then, when N → ∞, ∀x,
D(h, x) →
P [σxTen ≥ -XTw],
P [σxTen < -XTw],
XTw < 0
XTw ≥ 0
in probability.
In the first fold of XTw < 0,
P [σxTen ≥ -xtw] = P [σxTen ≥ IXTw|] = P [σkx∣∣Z ≥ IXTw|] = 1 — Φ (a(X'w)
where Z 〜 N(0,12), Φ is the cumulative distribution function of the normal distribution, and
a (x, w) = IXTwI∕kx∣∣. Note that σxTen 〜N(0, σ2kx∣∣2).
14
Under review as a conference paper at ICLR 2022
Table 3: Settings for training.
Dataset	Model	Epochs	Batch size	Optimizer	Learning Rate	Learning Rate Schedule ×decay [epoch schedule]
MNIST	S-CNN	50	32	Adam	0.001	-
CIFAR10	K-CNN	150	64	Adam	0.0001	-
SVHN	K-CNN	150	64	Adam	0.0001	-
CIFAR100	WRN-16-8	100	128	Nesterov	0.05	×0.2 [60, 80]
Tiny ImageNet	WRN-16-8	200	128	Nesterov	0.1	×0.2 [60,120,160]
HAM10000	WRN-16-8	100	64	Nesterov	0.05	×0.2 [60, 80]
Next, in the second fold of XTw ≥ 0,
P [σxτen < -xTi^] = P [σkx∣∣Z > IxTw|]
1 - Φ (a(X,W)
Here, Φ(∞) = 1 and Φ(0) = 1/2 by the smoothness of Φ. Consequently, in both folds,
D(h, x)→ 1 - Φ (^xw) = {
0,
1/2,
σ→0
σ→∞
in probability as N → ∞.
B	Datasets, Networks and Experimental Settings
B.1	Benchmark Datasets
MNIST (LeCun et al., 1998) is a handwritten digit dataset which has 60, 000 training samples and
10, 000 test samples in 10 classes. Each sample is a black and white image and 28 × 28 in size.
CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) are tiny image datasets which has 50, 000 train-
ing samples and 10, 000 test samples in 10 and 100 classes respectively. Each sample is a color
image and 32 × 32 in size.
SVHN (Netzer et al., 2019) is a real-world digit dataset which has 73, 257 training samples and
26, 032 test samples in 10 classes. Each sample is a color image and 32 × 32 in size.
Tiny ImageNet is a subset of the ILSVRC (Russakovsky et al., 2015) dataset which has 100, 000
samples in 200 classes. Each sample is a color image and 64 × 64 in size. In experiments, Tiny
ImageNet is split into two parts: 90, 000 samples for training and 10, 000 samples for test.
HAM10000 (Tschandl et al., 2018) is a imbalanced dermatoscopic image dataset which has 10, 015
samples in 7 classes. Each sample is a color image and resized to 75 × 75. In experiments,
HAM10000 is split into two parts: 8, 515 samples for training and 1, 500 samples for test.
All datasets are used without any preprocessing of images.
B.2	Deep Networks
S-CNN (Chollet et al., 2015) consists of [3×3×32 conv - 3×3× 64 conv - 2×2 maxpool -
dropout (0.25) - 128 dense - dropout (0.5) - # class dense - softmax] layers, and it is used for
MNIST.
K-CNN (Chollet et al., 2015) consists of [two 3×3×32 conv - 2×2 maxpool - dropout (0.25) -
two 3×3×64 conv - 2×2 maxpool - dropout (0.25) - 512 dense - dropout (0.5) - # class dense -
softmax] layers, and it is used for CIFAR10, SVHN, and CIFAR100.
WRN-16-8 (Zagoruyko & Komodakis, 2016) is a wide residual network that has 16 convolutional
layers and a widening factor 8, and it is used for CIFAR100 and Tiny ImageNet.
15
Under review as a conference paper at ICLR 2022
B.3	Experimental Settings
Training settings regarding number of epochs, batch size, optimizer, learning rate, and learning
rate schedule are summarized in Table 3. The model parameters are initialized with He normal
initialization (He et al., 2015) for all experimental settings. For all experiments, the initial labeled
samples for each repetition are randomly sampled according to the distribution of the training set.
C Verification of Conjectures
C.1 Verification of Conjecture 1
Theoretical verification:
Consider the binary classification with a set of linear classifiers,
H = {h : h(x) = sgn(xTw), w ∈ W = R2}
where x is uniformly distributed on X = R2 . By the duality between w and x (Tong & Chang,
2001), in W, w is a point and x is represented by the hyperplane, lx = {w ∈ W : sgn(xTw) = 0}.
Let h be a sampled hypothesis with W 〜N(W, Iσ2), θ be the angle of W = (W1,W2)τ, i.e.,
tanθ = W2/W1, θ be the angle of W = (w1,w2)τ, i.e., tanθ = w2∕w1, and θχ be the angle
between lx and positive x-axis. Here, θ, θx ∈ [-π + θ, π + θ] in convenience. When θx or π + θx
is between θ and θ, h(x) = h(x), otherwise h(x) = h(x). Thus, ρ(h, h) = ∣θ - θ∖∕π
Using Box-Muller transform (Box, 1958), W can be generated by
wι = Wi + σ ʌ/ —2 log U cos(2πv),	w2 = W2 + σ ʌ/ —2 log U sin(2πv)
where U and V are independent uniform random variables on [0,1]. Then, kw - W ∣∣ = σ√-2log u
and (w2 — W2)∕(w1 — Wi) = tan(2πv), i.e., the angle of W — W is 2πv. Here,
∣Wk sin(θ — θ) = -p—2 log U sin(2πv — θ)	(10)
by using the perpendicular line from W to the line passing through the origin and W (see the Fig-
ure 6a-6b for its geometry), and Eq. 10 is satisfied for all θ. For given U and v, θ is continuous and
the derivative of θ with respect to σ is
dθ = √-2log U sin2(2nv — θ thus ( dθ > 0,	V ∈ (2θ∏,嚼)
dσ	∣Wk sin(2∏v - θ),	[翳 < 0, V ∈ [0,1] \ [合,嗟]
Then,
dρ(h, h)	dθ
-ρ⅛-2 = sgn(θ - θ) dσ>0
θ ∏ + θ∖
where V ∈ {彳 bb


EI	/7 7^ \ •	. •	1 . • .1	•	• ,1	1	Z n /rʌ	/ / , ∕λ∖ /rʌ ɪ ,
Thus, ρ(h, h) is continuous and strictly increasing with σ when V 6= θ∕2π or V 6= (π + θ)∕2π. Let
ρ(h, h) = g(σ, u, v), then
一。，一个… 一。，	…
E[ρ(h, h)] = E[g(σ,U,v)]
g(σ, U, V)h(U)h(V)dUdV
where h(U) = I[0 < U < 1] and h(V) = I[0 < V < 1]. For 0 < σi < σ2,
E[g(σ2, U, V)] — E[g(σi, U, V)] =	g(σ2, U, V)h(U)h(V)dUdV —	g(σi, U, V)h(U)h(V)dUdV > 0
Empirical verification:
Figure 6c shows the empirical results for various datasets with deep networks. The E[ρS (h, h)] is
mostly continuous and strictly increasing with log σ when h is sampled with W 〜 N(W, Iσ2) on
MNIST, CIFAR10, SVHN, CIFAR100, Tiny ImageNet, and HAM10000 datasets. Therefore, these
results empirically substantiates Conjecture 1.
16
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
Figure 6: The verification of Conjecture 1. (a)-(b) Theoretical verification in binary classification
with the linear classifier h(x) = sgn(xTw) on uniformly distributed x ∈ X = R2. Let h be
a sampled hypothesis With W 〜 N(W,Iσ), then ρ(h,h) = ∣θ - θ∣∕π where -π + θ ≤ θ ≤
π + θ. Here, ρ(h, h) is continuous and strictly increasing with σ, thus E[ρ(h, h)] is continuous
and strictly increasing with σ . (c) Empirical verification for various datasets with deep networks.
The E[ρS (h, h)] is mostly continuous and strictly increasing with log σ when h is sampled with
w 〜N(W, Iσ2).
C.2 Verification of Conjecture 2
Theoretical verification:
Consider the binary classification with a set of linear classifiers,
H = {h : h(x) = sgn(xTW), W ∈ W = R2}
where x is uniformly distributed on X = R2 . By the duality between W and x (Tong & Chang,
2001), in W, W is a point and x is represented by the hyperplane, lx = {W ∈ W : sgn(xTW) = 0}.
Let HB be the set of h sampled with W 〜N(W, Iσ2), θ be the angle of W, θ be the angle of w, and
θx be the angle between lx and positive x-axis as in Figure 7a. Here, the lines with angles θx and
ππ
π + θx are same, thus we consider θx ∈ [— ∏ + θ, ∏ + θ]. Let
W (W, x)=
{W : θ ∈ (θx, π + θx)}
{W : θ ∈ (-π + θx, θx)}
一	ʌ
θχ > θ
一	△
θχ <θ
、a，/ 人 ∖ ♦	1 ♦	LI n i f 1 ∖	丁、/ f ∖	IzI / / f λ I /1 Zi / I	τmΓ 一
then W(W, x) is corresponding with H(h, x), and thus D(h, x) = IHB(h, x)|/|HB| → P[w ∈
W(W, x)] with probability tending to 1 as IHB ∣ → ∞. Let d1,d2 be the distances between W and
lx1 , lx2 respectively, and
W1 = W(W, x1) \ W(W, x2),	W2 = W(W, x2) \ W(W, x1)
17
Under review as a conference paper at ICLR 2022
W
W
D(h, x) ≈ P[w ∈ w(w,æ)]
⅛(≈) = ∖θx - θ∖∕ττ
lx
W(w,a?)
w
w ~ΛΓ(w,Iσ2)
.♦」 Origin
D(h, a?i) > Z7(⅛, a?2)< > EoQl) < E%(aι2)
Wi = W(w,a?i)\W(w,®2)
H,2 = W(w, X2) ∖ W(w, JCI)
(a)	(b)
®Q PUB =7 USMlBq .」」OU Nue」
(c)
Figure 7: The verification of Conjecture 2. (a)-(b) Theoretical verification in binary classification
with the linear classifier h(x) = sgn(xTw) on uniformly distributed x ∈ X = R2 . Let HB be
the set of h sampled with W 〜 N(W,Iσ2). The D(h, x) → P[w ∈ W(W, x)] decreases as d
increases with probability tending to 1 as IHB | → ∞. While, Lh(x) = ∣θx - θ∣∕π increases as
d increases. Thus, D(h, x1) > D(h, x2) ⇔ Lh(x1) < Lh(x2). (c) Empirical verification for
various datasets with deep networks the strong negative rank correlation coefficients of from -0.95
to -0.94 between the empirical LPDR and the disagree ratio are observed for log σ ∈ (-6, -2).
as in Figure 7b. Suppose that d1 < d2, then
P[w ∈ W(W, xι)] = P[w ∈ W(W, x2)] + P[w ∈ Wι] - P[w ∈ W2] > 0
since ∣W1∣ = ∣W21 and Φ(w1∣W, σ2) > Φ(w2∣W, σ2) for all pairs of W1 ∈ W1, W2 ∈ W2 that
are symmetric at the origin of W, where φ(∙∣W, σ2) is the probability density function of bivariate
normal distribution with mean W and covariance matrix Iσ2. Thus,
d1 < d2 Q⇒ D(h, x1) > D(h, x2)
with probability tending to 1 as |HB | → ∞.
Meanwhile, Lh(x) = ∣θx — θ∣∕π and di = ∣∣w∣∣ Sin ∣θxi — θ∣, then
7	7	I zʌ	Λ I I zʌ	Λ I	T- /	∖	-T-	/	∖
di <	d2	Q⇒ ∣θχ1	- θ∣	< ∣θχ2	- θ∣	Q⇒ Lh(xι)	< Lh(x2).
Therefore,
ʌ ʌ
D(h, xi) > D(h, x2) ^⇒ Lh(xi) < Lh(x2)
with probability tending to 1 as |HB | → ∞.
Empirical verification:
Figure 7c shows the empirical results for various datasets with deep networks. Spearman’s rank
correlation coefficient (Spearman, 1904) between the empirical LPDR and the disagree ratio is close
to -1 in certain range of σ in all experimental settings-strong negative correlation coefficients of
from -0.95 to -0.94 are observed for log σ ∈ (-6, -2). Therefore, these results empirically
substantiates Conjecture 2.
18
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
(a)
Figure 8: The performance comparison between using empirical LPDR and using the disagree ratio
on MNIST with S-CNN (a), CIFAR10 with K-CNN (b), and CIFAR100 with WRN-16-8 (c). There
is no significant difference in the performance between the two methods. Thus, LPDR-based active
learning can be performed by using the disagree ratio.
(b)
τ->∙	C El τπ Γ /7	ι 1	Li	, ,	Il 「	ι r 11	♦	, ι
Figure 9: The E[ρS (hn, h)] and log σ with respect to the labeling proceeds for all experimental
settings. The proposed algorithm reliably guides the E[ρS (hn, h)] tobe the target value by increasing
the variance of sampling as the number of labeled samples increases.
D	Empirical LPDR vs Disagree Ratio
Figure 8 compares the active learning performance between evaluating empirical LPDR by brute-
force search and evaluating the order of empirical LPDR by using disagree ratio on MNIST with
S-CNN, CIFAR10 with K-CNN, and CIFAR100 with WRN-16-8. In all cases, there is no significant
difference in the performance between the two methods. As a result, LPDR-based active learning
can be conducted without evaluating empirical LPDR instead empirical LPDR order is obtained
through disagree ratio.
E Regulating σ TO Keep E[pS(hn, h)] Static AT p*
T-'∙	C 1	τπ Γ /7	f、1 ∙ A 1	.,1 -t ∙	, , ,1	,♦ 1	♦	L 11
Figure 9a shows E[ρS (hn, h)] in Algorithm 1 with respect to the active learning progress. For all ex-
Penments, the proposed algorithm reliably guides the E[ρs (hn, h)] to be ρ* = q/m (MNIST: 0.01,
CiFAR10: 0.1, SVHN: 0.05, CiFAR100: 0.2, Tiny imageNet: 0.25, HAM10000: 0.1). Figure 9b
shows log σ with respect to the active learning progress. For all experiments, the σ increases as the
labeling proceeds. As the number of labeled samples increases, the larger variance is required to
keep E[ρs (hn, h)] static at ρ for unlabeled samples move away from the decision boundary of h
due to an increase in the network confidence.
F	FiNAL TEST ACCURACY VS ρ*
Figure 10 shows the final test accuracy with respect to ρ* on MNIST, CIFAR10, and CIFAR100
datasets. The results show that the proposed algorithm performs well at around P = q/m (MNIST:
0.01, CIFAR10: 0.1, CIFAR100: 0.2). In addition, the range of ρ*, associated with the high perfor-
mance, is wide; thus, DRAL is robust against the ρ* in the wide range.
19
Under review as a conference paper at ICLR 2022
MNIST (SCNN)	CIFAR10 (KCNN)	CIFAR100 (WRN)
(a)	(b)	(c)
Figure 10: The final accuracy With respect to the ρ* on MNIST (a), CIFAR10 (b), and CIFAR100
(C) datasets. The proposed algorithm shows high performance when ρ* = q/m.
MNIST (SCNN)
320 420 520 620 720 820 920 1000
# of labeled samples
CIFAR10 (KCNN)
• .XΛT×
* ∙τ⅛*
iɪ
SVHN (KCNN)
(％)」0」」3 6uc∙≡⅛
200 1200 2200 3200 4200 5200 6200 720
# of labeled samples
200 1000 2200 3000 4200 5000 5800 7000 7800 9000 9400
# of labeled samples
(a)
(b)
(c)
Cifarioo (wrn)
Tiny ImageNet (WRN)
Hamioooo (wrn)
Figure 11: The empirical errors of the learned and the sampled hypotheses with respect to the
number of labeled samples for MNIST (a), CIFAR10 (b), SVHN (c), CIFAR100 (d), Tiny ImageNet
(e), and HAM10000 (f) datasets. It is observed that the empirical errors of the sampled hypotheses
have various values.
5000 7000 9000 1100013000150001700019000 21000 23000
# of labeled samples
(d)
10000 15000 20000 25000 30000 35000 40000 45000
# of labeled samples
(e)
500 800 1100 1400 1700 2000 2300 2600 2900 3200
# of labeled samples
⑴
• ∙ I TA⅛
:
• ∙.. τ^∩u÷
• τ^⅛
• ∙∙τ⅛*
G Empirical Errors of Sampled Hypotheses
Figure 11 shows the empirical errors on L of the sampled hypotheses with respect to the number
of labeled samples for MNIST, CIFAR10, SVHN, CIFAR100, Tiny ImageNet, and HAM10000
datasets. The empirical errors of sampled hypotheses have various values.
H DRAL+ VS DRAL
Figure 12 shows the performance comparison between DRAL+ and DRAL with respect to the num-
ber of labeled samples on MNIST, CIFAR10, SVHN, CIFAR100, Tiny ImageNet, and HAM10000
datasets. Overall, DRAL+ consistently either performs better or comparable with DRAL regardless
of the experimental settings. When the empirical errors of the sampled hypotheses are mostly zero
such as the results of MNIST, CIFAR100, and Tiny ImageNet, there is no significant difference in
performance between DRAL+ and DRAL. However, when the mean empirical error of the sampled
hypotheses is larger than the empirical error of the learned hypothesis such as the results of CI-
FAR10, SVHN, and HAM10000, DRAL+ brings a significant performance improvement compared
20
Under review as a conference paper at ICLR 2022
(a)
(b)
(c)
(d)
(e)
(f)
Figure 12: The performance comparison between DRAL+ and DRAL on MNIST (a), CIFAR10
(b), SVHN (c), CIFAR100 (d), Tiny ImageNet (e), and HAM10000 (f) datasets. Overall, DRAL+
consistently either performs better than or comparable with DRAL regardless of the experimental
settings.
to DRAL. The larger the variance in the empirical errors of the sampled hypotheses, the larger the
performance gap between DRAL+ and DRAL tends to be.
I	Robustness of DRAL+ against Hyperparameters
DRAL+ has four hyperparameters: 1) the initial σ = σ0 , 2) the positive hyperparameter β, 3)
the number of sampled hypotheses N, and 4) the layer of the network to which sampling is applied.
The σ0 has no significant effect on the performance of DRAL+ for σ is adaptively regulated to make
ρ0 = ρ* while hypothesis sampling. Figure 13 shows the performance comparison With respect to
β, N, and sampling layer on MNIST, CIFAR10, and CIFAR100 datasets. Figure 13a-13c show
that there is no significant performance difference for various β ∈ {0.1, 1, 10} on all datasets. The
robustness against β is based on the sufficient buffer for regulating σ since the range of ρ* associated
with the best performance is wide. Figure 13d-13f show that there is no significant performance
difference for various N ∈ {5,10, 20, 50,100, 200} on all datasets. Figure 13g-13i show that there
is no significant performance difference whether parameter sampling is applied to the entire layers
or on the last layer of the network. The robustness against N or sampling layer is based on the
sufficient discrimination in the disagree ratio for identifying q most informative unlabeled samples
with a small number of sampled hypotheses by setting ρ* = q/m.
J Running Time
In Table 4, the mean of running time for active learning are given for all algorithms and datasets. The
unit is minutes, and the value in parentheses is the ratio to Entropy. The running time of DRAL+
increased by only 1-7% compared to Entropy on all datasets except MNIST, and it is comparable to
MC-BALD, MC-VarR, and Coreset. The relatively large running time in MNIST is because it takes
a very short time to train the model compared to that of the acquisition. Ens-VarR requires about 5
times more computational load than DRAL+ on all datasets, and BADGE requires twice and more
than eight times the computational load on CIFAR100 and TinyImageNet datasets, respectively.
21
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
(d)	(e)	(f)
(g)
(h)
(i)
Figure 13:	The performance comparison with respect to the hyperparameters of DRAL+ on MNIST,
CIFAR10, and CIFAR100 datasets. (a) - (C) β. (d)-⑴ N. (g) - (i) sampling layer. DRAL+ is
robust against β, N, and sampling layer.
Table 4: The mean of running time (minutes) for aCtive learning are given for all algorithms and
datasets. The value in parentheses is the ratio to Entropy. We observe that DRAL+ operates as fast
as Entropy, and that ENS-VarR or BADGE require a large computational load.
	MNIST	CIFAR10	SVHN	CIFAR100	T. ImageNet	HAM10000
DRAL+[ours]	7.9 (139)	100 (107)	192 (104)	396 (103)	4,589 (101)	311(101)
Entropy41	5.7 (100)	93 (100)	186(100)	385 (100)	4,547 (100)	307 (100)
Coreset36	14.4 (254)	139 (149)	237 (128)	439 (114)	4,722 (104)	321 (104)
MC-BALD14	6.8(119)	99 (106)	180 (97)	441 (115)	4,828 (106)	403 (131)
MC-VarR10	6.8(119)	101 (108)	187(101)	443 (115)	4,861 (107)	403 (131)
ENS-VarR3	24.9 (438)	575 (616)	885 (477)	2,274 (591)	23,394 (515)	1,744 (568)
BADGE1	11.0(193)	141 (151)	238 (128)	864 (224)	39,265 (864)	326 (106)
K Results for Test Accuracy
Figure 14	shows the test accuracy with respect to the number of labeled samples from initial to final
step for all experimental settings.
L Robustness of DRAL+ against Initial Labeled Size
Figure 15	shows the performance comparison with respect to the number of initial labeled samples
on MNIST, CIFAR10, and CIFAR100 datasets. There is no significant performance difference ac-
22
Under review as a conference paper at ICLR 2022
(a)	(b)	(c)
(f)
(d)	(e)
Figure 14: The test accuracy with respect to the number of labeled samples from initial to final step
for all experimental settings.
(a)
(b)
(c)
Figure 15:	The performance comparison with respect to the number of initial labeled samples on
MNIST (a), CIFAR10 (b), and CIFAR100 (c) datasets. The proposed algorithm is robust against to
the number of initial labeled samples and performs well even when the initial size is much smaller.
cording to the number of initial labeled samples. The proposed algorithm is robust against to the
number of initial labeled samples and performs well even when the initial size is much smaller.
M Framework of DRAL+
Figure 16	shows the framework of the proposed algorithm.
23
Under review as a conference paper at ICLR 2022
Query unlabeled samples
with Ihe Iiighesl disagree ralio
>aιn∣.Hiιi'2
P = Ps 3n, Et)
p* : Target disagree metric
εn : Error oɪ wn on Q
ɛt : Error of Wt on &
Figure 16: The framework of the proposed algorithm.
24