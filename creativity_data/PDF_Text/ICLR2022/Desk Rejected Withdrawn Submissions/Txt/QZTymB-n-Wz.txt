Under review as a conference paper at ICLR 2022
Effective Certification of
Monotone Deep Equilibrium Models
Anonymous authors
Paper under double-blind review
Ab stract
Monotone Operator Equilibrium Models (monDEQs) represent a class of models
that combine the powerful deep equilibrium paradigm with convergence guaran-
tees. As monDEQs are inherently robust to adversarial perturbations, investigating
new methods to certify their robustness is a promising research direction. Unfor-
tunately, existing certification approaches are either imprecise or severely limited
in their scalability. In this work, we propose the first scalable and precise mon-
DEQ verifier, based on two key ideas: (i) a novel convex relaxation which enables
efficient inclusion checks, and (ii) non-trivial mathematical insights characterizing
the fixpoint operations at the heart of monDEQs on sets rather than concrete in-
puts. An extensive evaluation of our verifier demonstrates that on the challenging
'∞ perturbations it exceeds state-of-the-art performance in terms of speed (two
orders of magnitude) and scalability (an order of magnitude) while yielding 25%
higher certified accuracies on the same networks.
1	Introduction
Deep Equilibrium Models (DEQ) (Bai et al., 2019) and in particular Monotone Operator Equilibrium
Models (monDEQs) (Winston & Kolter, 2020) are promising new architectures, based on implicit
layers solving fixpoint problems at inference time. Their competitive accuracies and high inherent
empirical robustness make an investigation of their certifiable robustness a promising research direc-
tion. Initial investigations, via bounds on their Lipschitz constant (Pabbaraju et al., 2021) or semidef-
inite programming (SDP) (Chen et al., 2021), indicate promising trends. However, in challenging
settings such as '∞-perturbations, LiPSchitz constants are inherently loose and SDP encodings are
prohibitively costly. Further, existing scalable verification approaches based on the propagation of
convex sets, successful for feed forward networks (Singh et al., 2018; Xu et al., 2020), are unable to
handle the implicit layers underlying (mon)DEQs and hence not applicable. A key challenge then is
to design precise certification techniques able to handle fixpoint problems on convex sets.
This work: certification for monDEQs In this work, we address this challenge and introduce a
new convex relaxation, called M-Zonotope, which, for the first time, allows an efficient and precise
propagation of convex sets in the monDEQ setting, overcoming key limitations of prior approaches.
Our new relaxation is based on the Zonotope approximation (Mirman et al., 2018; Wong & Kolter,
2018; Singh et al., 2018) which was successfully used to verify properties of (fixed-depth) classic
neural networks. However, unlike Zonotopes, our relaxation allows for efficient inclusion checks,
critical for handling implicit layers (which require unbounded fixed point iteration). Further, we
ensure that our method is correct via rigorous mathematical analysis of iteration process.
Main Contributions Our key contributions are:
•	We lift neural equilibrium models (with few conditions), such as monDEQs, from concrete
points to sets, enabling their efficient and precise verification (Section 4).
•	Craft, an verification algorithm, using this set formulation and a new convex relaxation,
called M-Zonotope, enabling efficient fixpoint iteration and inclusion checks (Section 5).
•	Our extensive evaluation shows that Craft achieves a new state-of-the-art for monDEQ
verification, outperforming current approaches in precision (25%), speed (two orders of
magnitude) and scalability (one order of magnitude) (Section 6).
1
Under review as a conference paper at ICLR 2022
2	Related Work
We now briefly review related work in neural network verification.
Incomplete Neural Network Verification Incomplete verification approaches (such as ours), un-
like complete methods (Katz et al., 2019; Bunel et al., 2020), are fast and efficient but can be im-
precise, i.e., may fail to certify robustness for inputs even if they are robust. These methods can be
divided into bound propagation approaches (Gehr et al., 2018; Zhang et al., 2018) and those that
generate optimization problems (Singh et al., 2019a; Raghunathan et al., 2018) such as linear pro-
gramming (LP) or semidefinite programming (SDP) formulations. However, existing approaches
cannot be applied to (mon)DEQs without non-trivial extensions as they are unable to express the un-
derlying fixpoint problems. Even when extended in the way we discuss in Section 4, precise bound
propagation methods lack computationally feasible inclusion checks. Alternatively, stochastic de-
fenses like randomized smoothing Lecuyer et al. (2018); Cohen et al. (2019) establish robustness
with high probability but incur significant runtime costs at inference time, a problem further exacer-
bated by the relatively expensive fixpoint iterations needed in (mon)DEQ inference.
Certification of monDEQs Two main approaches have been proposed to certify the robustness of
monDEQs: (i) Pabbaraju et al. (2021) use the special structure of monDEQs to bound the global
Lipschitz constant of the network, and (ii) Chen et al. (2021) adapt an SDP approach by introducing
a semi-algebraic representation of the ReLU-operator used in monDEQs. Depending on the encod-
ing, the latter allows to bound the score difference between classes, the global Lipschitz constant,
or yields an ellipsoidal relationship between inputs and outputs. All three approaches only scale
to an implicit layer size of 87 neurons due to the limitations of the underlying SDP solver. Addi-
tionally, the most effective approach suffers from long runtimes (1350s/per sample) even for these
small networks, making the certification of many inputs or larger networks infeasible. Orthogonally,
Revay et al. (2020) show a way of bounding the Lipschitz constant of a monDEQ by construction.
However, enforcing small Lipschitz constants this way reduces the resulting accuracy significantly,
limiting the utility of the obtained networks.
3	Monotone Operator Equilibrium Models On Points
We now briefly discuss the background for (monotone) deep equilibrium models on concrete points
before extending them to sets of points in Section 4 and discussing their verification in Section 5.
Deep-Equilibrium Models (DEQs) Implicit-Layer (Amos & Kolter, 2017; Ghaoui et al., 2019)
and Deep-Equilibrium-Models (DEQ) (Bai et al., 2019) were recently introduced to enable more
memory efficient model parameterizations. Unlike traditional deep neural networks, which prop-
agate inputs through a finite number of different layers, DEQs, conceptually, apply the same
layer(s) repeatedly until convergence to a fixpoint. This corresponds to an infinite depth model
with parameter-sharing. A DEQ h obtains its final prediction y by applying a linear layer to this
fixpoint:
y = h(x) := Vz* + v,
z* = f(x, z*).
(1)
In practice, fixpoint solvers are used to compute the fixpoint z * of layer f for some input x in-
stead of repeatedly propagating the sample. Gradients can be backpropagated through the fixpoint
directly using the implicit fixpoint theorem without requiring the solver iterations to be unrolled. As
shown in Bai et al. (2019), this constitutes a powerful architecture, achieving almost state-of-the-art
performance on text and vision tasks using significantly less memory.
Monotone Operator Equilibrium Models (monDEQs) A major drawback of general DEQs is
that they do not guarantee the existence or uniqueness of fixpoints. Hence, solvers may not con-
verge to the same fixpoints consistently, rendering their verification infeasible. In response to this,
monDEQs were introduced in Winston & Kolter (2020) as a particular form of DEQs. Parametrizing
f(x, z) = σ(Wz + Ux + b)
(2)
2
Under review as a conference paper at ICLR 2022
with x ∈ Rq,z ∈ Rp,U ∈ Rp×q, W = (1 - m)I - PTP + Q - QT where P,Q ∈ Rp×p and
monotonicity parameter m > 0, they show that the existence and uniqueness of the fixpoint z* is
guaranteed. These properties allow certification that is independent of how a fixpoint was obtained,
yielding far stronger guarantees than possible in the general DEQ setting. Throughout this text we
will use σ := ReLU and discuss considerations for other choices in App. A.1.
Further, Winston & Kolter (2020) derive convergence guarantees for the following iterative solver
strategies to this unique fixpoint. For a monDEQ h with iteration function f(x, z) = ReLU(Wz+
Ux + b), we let gα denote an iteration ofa solver using operator splitting:
•	Forward-Backward Splitting (FB) with
zn+1 = gαF B (x, zn) := ReLU ((1 - α)zn + α(W zn + Ux + b)),	(3)
which converges to z* of f, for any 0 < a < ∣∣i-W口..
•	Peaceman-Rachford Splitting (PR) with [zn+1 , un+1] = gαP R (x, zn, uz), where
un+1/2 = 2zn - un
zn+1/2 = (I+α(I - W))-1(uk+1/2 +α(Ux+b))
un+1 = 2zn+1/2 - un+1/2	(4)
zn+1 = ReLU (un+1)
[zn+1 , un+1] = gα (x, zn, un)
which converges to z* for any α > 0 (Ryu & Boyd, 2016) by computing auxiliary
un, un+1/2, zn+1,2, initialized by u0 = 0. Here [zn+1 , un+1] denotes the concatenation
of zn+1 and un+1.
For both algorithms we initialize z0 = 0. In the following, we will write gα (x, zn , un) to refer to
both PR and FB, for which we assume un to be zero dimensional.
4 Monotone Operator Equilibrium Models On Sets
In this section, we discuss the mathematics of replacing the concrete inputs x and intermediate states
z with sets X and Z of concrete values and over-approximations X and Z thereof. We will then
show how this produces a set containing all concrete fixpoints, used for verification in Section 5.
Fixpoint Iteration Both Forward-Backward
(FB, see Eq. (3)) and Peaceman-Rachford split-
ting (PR, see Eq. (4)) are guaranteed to con-
verge to the fixpoint of f(x, z), as defined in
Eq. (1), for which we write z*(x) to highlight
the dependence on x. That is, when iterated un-
til the quantity zn - zn-1 becomes smaller than
a predetermined stopping criterion, both yield
zn ≈ z*(x).
We lift this idea to sets of points: let X ⊆ Rq
denote a set of inputs, Zn ⊆ Rp, Un ⊆ Rp
the corresponding intermediate values at step
n and Z* := {z* (x) | x ∈ X} the corre-
sponding fixpoints. Similarly, we lift a function
y = h(x) operating on concrete points x, y to
sets Y = h(X) := {h(x) | x ∈ X} and let h#
denote over-approximation Y ⊆ Y = h# (X).
Unlike the concrete case, here we want to iter-
ate until Zn contains the set of all fixpoints Z*.
Fortunately, as we will show, it is sufficient to
Input to iteration step	Output of iteration step
Figure 1: After an over-approximated solver it-
eration Zn+ι = g# (X, Zn) (blue) is contained
in the previous state Zn (green). This implies
that any (exact) iteration ofgα (X, Zn+1) (orange)
will not escape from Zn+1 . This implies contain-
ment of the true fixpoint set (red) Z * ⊆ Zn+1 .
iterate the solver procedure on sets Zn or their over-approximations Zn ⊇ Zn in order to find
3
Under review as a conference paper at ICLR 2022
Z* ⊇ Z*. We formalize this in the following theorem, where We consider a monDEQ h with iter-
ation function f(x, z) and corresponding solver iteration [zn+1, un+1] = gα(x, zn, un) and write
[Zn+1,Un+1] for the set containing vectors [zn+1, un+1].
Theorem 4.1 (Fixed-Point contraction). Let [Zn+ι, Un+ι] = g#(X, Zn, Un) be closed sets over-
approximating zn+1 and un+1 obtained by applying the solver iteration n+ 1 times for some z0, u0
and all inputs x ∈ X. Then:
ʌ ʌ -ʌ -ʌ _ ʌ ... ____________________________________________ ʌ ...
Zn+1 ⊆Zn ∧Un+1 ⊆ Un =⇒ Zj ⊆ Zn+1 ∀ j>n =⇒ Z ⊆Zn+ι	(5)
As illustrated in Fig. 1, once an iteration maps Zn to a subset of itself Zn+1 ⊆ Zn, no further
application of gα will escape this set and hence it must contain the true fixpoint set Z*. Note that
this does not necessarily hold for further applications of the over-approximated g#.
Proof. Let Si := [ZZi,Ui] and Si := [Zi,Uj. Then
_ ʌ ʌ
Sn+1 ⊆ Sn+1 ⊆ Sn	(6)
where the first ⊆ holds by definition and the second by the left hand side in Eq. (5). Now, we
can over-approximate Sn+1 with Sn ⊇ Sn+1 ⊇ Sn+1 and Sn+2 ⊆ Sn+1 follows immediately via
Eq. (6). Then Sj ⊆ Sn+1 for j > n follows by induction and thereby Zj ⊆ Zn+1 ∀j > n.
To prove the second implication, we note that by the convergence guarantee of the concrete iteration:
for any E ∈ R>0 there exists a j ∈ N with j ≥ n + 1 such that we have ∣∣Zj - z*k ≤ e. By the
definition of Zj we also have Zj ∈ Zj. For E → 0 and hence ∣∣Zj - z*∣ → 0 it follows that
z* ∈ Zj ∪ ∂Zj = Zj with the last equality following from the closedness of Zj. Thus for each
x ∈ X, there exists a jx such that z*(x) ∈ Zjx ⊆ Zn+1, where we get the inclusion relation from
the first implication. Finally, Z* ⊆ Sx∈X Zjx ⊆ Zn+1.	□
Fig. 1 tells us that if we consistently apply g# until containment we know that we capture Z*.
However, while practically unlikely, it does not guarantee that when applying a step of a different
iteration function g0# this fixpoint set is preserved. To formally ensure this, we use the specifics of
the Forward-Backward solver:
Theorem 4.2 (Fixpoint set preservation for Forward-Backward splitting). Let Zn be a sound over-
approximation of Z*, i.e., Z* ⊆ Zn. Thenwehavefor 0 < ɑ < 八1-W]修:
Z* ⊆Zn+ι = gFB#(X, Zn)	(7)
Intuitively, Forward-Backward splitting maps all concrete fixpoints onto themselves and hence any
over-approximation of the fixpoint set will map to another over-approximation of the fixpoint set.
In contrast to Theorem 4.1, Theorem 4.2 does not assume that the same iterative solver (includ-
ing hyperparameters) is applied at each step. Instead, it makes a statement about one application
of Forward-Backward splitting using any parameters. We will rely on this in Section 5.2 to apply
Forward-Backward splitting after applying Peaceman-Rachford splitting and optimize hyperparam-
eters in the course of an iteration.
We discuss a similar result to Theorem 4.2 for Peaceman-Rachford splitting in App. A.
Proof. For any concrete fixpoint z* = f(x, z*) = ReLU(Wz* + Ux + b), we consider an
iteration of Forward-Backward splitting as per Eq. (3) with zn = z*:
zn+1 =ReLU((1 -α)z*+α(Wz*+Ux+b)) =z*
'-----V------}
z0
We show this by considering the expression element-wise. Suppose z0 ≤ 0, then due to
z* = f(x, z*) = ReLU(z0), we know z* = 0 and else z* = z0. Then
R LU ((1 - ) * + 0) = ReLU ((1 - α)0 +αz0) if z0 ≤ 0 = *
ReLU ((1 -α)z +αz ) = ReLU ((1 - α)z*+αz*) else =z .
In the first case we know (1 -α)0+αz0 ≤ 0 as z0 ≤ 0. It follows that one step of Forward-Backward
splitting will always map a fixpoint upon itself in the concrete Zn+ι = Zn = z*. Since Zn includes
all fixpoints for X, any sound Zn+ι = g#(X, Zn) includes all fixpoints for X.	□
4
Under review as a conference paper at ICLR 2022
5 Fixpoint Set Iteration with M-Zonotope
To utilize Theorem 4.1 for the verification of monDEQs, we need to be able to represent the over-
approximations (Z , U) in
a way that permits efficient containment checks. Of the commonly used
convex relaxations in neural network verification, only Boxes meet this requirement. However, as
we will show experimentally in Section 6.2, they are too imprecise for practical verification. For
more precise relaxations, such as Zonotopes, the inclusion check is computationally prohibitively
expensive (Kulmburg & Althoff, 2021). To overcome these issues, we introduce a new variant of the
Zonotope relaxation called Mixed-Zonotope (M-Zonotope), discussed in Section 5.1. In Section 5.2
we introduce Craft, our novel verifier for monDEQs, enabled by the M-Zonotope relaxation.
5.1 M-Zonotope
x-r T F ,	-Λ. ιr r-r	.	4 L -rrʌ ∏	∙	1	r L 玲
We denote a M-Zonotope Z ⊆ Rp over-approximating a volume Z ⊆ Z :
Z = AV + diag(b)η + a
(8)
where A ∈ Rp×k is the so-called error coefficient matrix, b ∈ R≥0 p the Box error vector, a ∈ Rp
the centre vector, and ν ∈ [-1, 1]k and η ∈ [-1, 1]p the error terms. If k = p and A is a basis of
Rp, We call Z a proper M-ZonotoPe and else an improper one.
What enables efficient inclusion checks (discussed
shortly) is that an M-Zonotope is proper. While a standard
(proper) Zonotope With at most n error terms (b = 0)
and any Box approximation (A = 0) can apply a similar
inclusion check, an M-Zonotope yields much tighter ab-
straction than either, as it can effectively employ tWice as
many error terms. We visualize this in Fig. 2.
Formally, an M-Zonotope can be seen as a MinkoWski
sum of a Zonotope (Aν) and a hyperbox (diag(b)η), also
called Hybrid-Zonotope (Mirman et al., 2018).
Figure 2: Over-approximations of an
improper M-Zonotope (blue) by a
proper one With (green) and Without
(red) Box component and Box (orange).
Transformations We handle affine transformations of
an M-Zonotope as introduced in Singh et al. (2018), by casting the Box errors as Zonotope errors
and setting A0 = [A, diag(b)] before applying the transformation. This turns a proper M-Zonotope
With non-zero Box component into an improper one With a zero Box component. To encode the
ReLU function for M-Zonotope, We also folloW Singh et al. (2018), by adding neW error terms as a
Box component. Applying this transformer turns a proper M-Zonotope With zero Box component
into a proper one With a non-zero Box component. Both of these operations are sound. HoWever,
While affine transformations can be encoded exactly, ReLUs result in over-approximations.
Consolidating the Error Terms We now discuss how an improper M-Zonotope Z with a not
necessarily full rank A ∈ Rp×k can be over-approximated with a proper M-Zonotope Z0 with a
base A0 ∈ Rp×p . If k > p we consolidate the k old error terms into p new ones, ensuring that the
resulting A0 has full rank and hence forms a basis of Rp . If k ≤ p, we pick a subset with full rank
and complete it to a basis. In monDEQ certification p = dim(z) is the size of the latent dimension.
The theorem below shows how an improper M-Zonotope with an arbitrary number of error terms
can be over-approximated as a proper M-Zonotope.
Theorem 5.1 (Consolidating errors). Let Z = Aν + diag(b)η + a be an improper M-Zonotope
with A ∈ Rp×k and k > p. Letfurther A ∈ Rp×p be a basis of Rp. Then the proper M-Zonotope
Z0 = A0e1 + diag(b)η + a with
A0 = diag(c)A	where C = (∣A-1A∣1)	(9)
is a sound over-approximation Z0 ⊇ Z of the improper one, where 1 = [1]k denotes the k dimen-
sional one vector and | ∙ | the elementwise absolute. We call C the consolidation coefficients.
5
Under review as a conference paper at ICLR 2022
(a) True inclusion
(b) Decomposition of an M-Zonotope into Box and
ZonotoPe components
Zonotope
component
+
Box &
+ center
component
(c) Inclusion check for
both components
Figure 3: Illustration of checking the containment of an improper M-Zonotope (red) in a proper
M-Zonotope (green), by consolidating errors (blue), or concretizing to Box (orange). In Fig. 3b We
show how the proper M-Zonotope can be decomposed into their Box and Zonotope components. In
Fig. 3c we illustrate the containement check of these components individually.
The intuition behind this is shown in Fig. 4. All error vec-
tors (columns) in A (shown as red and blue solid arrows)
are represented as a linear combination of error vectors in
A (red and blue dashed arrows). Now we sum the abso-
lute values (correcting for their orientation) of these con-
tributions ATA over all error vectors to obtain the con-
solidation coefficients c. Finally we multiply the obtained
contributions with the error vectors of the new basis vec-
tors Ali = CiA∙,i (solid black arrows).
Proof. Without loss of generality let a = 0, b = 0 and
Z = AV = pk=ι Aj Vj with k error terms, stored in the
columns of A. We can express the contribution of every
error term as AjVj = AVOj) with VOj) = A-IAjVj
as A is a basis of Rp and hence invertible. From Vj ∈
Figure 4: Illustration of Theorem 5.1.
The solid red and blue vectors span the
Zonotope (green) and can be decom-
posed into the dotted vectors, which are
then consolidated into the solid black ar-
rows. All vectors are scaled by factor 2.
[-1,1] it follows that VOj) ∈ diag(ATAj)Vj) with Vljj ∈ [-1,1]p. This allows us to rewrite
Z= JA Pk=I ATAjVj 1 ⊆ JA Pk=I diag(ATAj )V(j)] = JAdiag(∣A-1A∣1)V]=衰
∀VV ∈	[-1,1]k ʃ -	l∀V(I),...,V(k)∈	[-1,1]p	ʃ	l∀V ∈	[-1,1]p ʃ ,
where the last equality follows from linearity and the choice Vj = ± Sign(A-IAj).	□
Choosing the New Basis To minimize the imprecision incurred when consolidating error terms,
a suitable basis A has to be chosen. Fortunately, in monDEQ verification we repeat the same oper-
ations in each solver iteration leading to M-Zonotope where many error terms are well-aligned. In
particular, they will be stretched mainly by the matrix W with only an additive component of Ux
and some rescaling in the ReLU transformer. To compute a new basis A for the error terms of Z,
we compute the PCA-basis of A. If k < p, we complete the basis with orthogonal vectors.
Containment of M-Zonotope Here, we outline the steps of an inclusion check between M-
Zonotopes. We illustrate this in Fig. 3, with a proper M-Zonotope Z = Av + diag(b)η + a
(green in Fig. 3a) and the improper M-Zonotope zL, = A0ν + diag(b0)η + α0 (red in Fig. 3a). To
check containment ZO ⊆ Z, we decompose both M-Zonotopes into their Zonotope, Box and center
components (shown in Fig. 3b) and show that the Box b and Zonotope A terms contain their respec-
tive counterparts (shown in Fig. 3c). If the centers are not aligned, a 6= aO, we check containment
of the translated Boxes (also shown in Fig. 3c). In case the Box component b of Z is not sufficient
to contain the center difference as well as bO, we allow the Zonotope terms A to compensate for the
remaining center and Box components (not shown).
6
Under review as a conference paper at ICLR 2022
To determine containment of the Zonotope
component, we consolidate the error matrix
A0 with basis A. In Fig. 3a this is shown
in blue. This leads to perfectly aligned er-
ror vectors, enabling us to directly com-
pare the length of corresponding error terms
and show containment if those of Z are all
shorter than the equivalent ones in Z (shown
overlayed in Fig. 3c). More efficiently, we
only compute the consolidation coefficients
and check |A-1A0|1 < 1.
To show containment of the non-negative
Box components, we can simply check that
b ≤ b. However, we observe that nega-
tive values in the difference vector b0 - b
denote directions in which b is larger than
b0 and can hence compensate for differences
in the center terms a0 - a. Positive val-
ues in b0 - b denote directions in which b
is too small to cover b0. Combining these
two, we can derive a residual Box compo-
nent d = max(0, |a0 - a| + b0 - b), that ad-
Algorithm 1: CRAFT
Input: x, e, target t, monDEQ h
Output: whether ∀x0 ∈ X arg maxi h(x0)i = t
ι X -{x0 |||x - x0k∞ ≤ e}
2 Zo - {z*(x)}, Uo — {z*(x)}
3	converged — false
4	for n4-0, 1, ... , nmaχ do
5
6
7
8
9
10
11
12
13
if  converged
Γ Γ7 、八I 1	1-1 1 ( ∖ Γ7 、八I 1 \
[Zn, Un] — Consohdate([Zn,Un])
[Zn+1, U^n+l] = gPR(X ,Zn, Un)
converged — [Zn+ι,Un+ι] ⊆ [Zn,Un]
else
Zn+1 =gFB# (X, Zn)
ʌ ʌ
Y 一 V Zn + V
ʌ ʌ
if Yt-Yi > 0 ∀i = t
I return true
14 return false
ditionally needs to be covered by the Zonotope component. To this end, we can cast d as additional
error terms of A0 and update the Zonotope inclusion check to |A-1A0|1 + |A-1 diag(d)|1 < 1.
This compensation is not necessary in Fig. 3. We formally express this containment check as:
Theorem 5.2 (M-ZonotoPe Containment). Let Z = AV + diag(b)η + a be a proper M-Zonotope
and Z0 = A0ν0 + diag(b0)η0 + a0 an improper one. Z0 is contained in Z if
|A-1A0|1 + |A-1 diag(max(0, |a0 - a| + b0 - b))|1 ≤ 1.	(10)
holds element-wise. A-1 is must exist as Z is proper and therefore A ∈ Rp×p is a basis of Rp.
Proof. Containment is equivalent to showing that for all error terms ν0 ∈ [-1, 1]k, η0 ∈ [-1, 1]p
describing Points in Z0, there exist ν ∈ [-1, 1]p, η ∈ [-1, 1]p of Z such that:
Aν + diag(b)η + a = A0ν0 + diag(b0)η0 + a0.
We subtract a from both sides and over-aPProximate the right hand side by increasing the Box size
by the absolute center difference |a0 - a| yielding b00 := b0 + |a0 - a|. This leaves us to show that
we can find ν, η such that Aν + diag(b)η = A0ν0 + diag(b00)η00 holds for all ν0, η00.
We choose η ∈ [-1, 1]p such that diag(b)η = sign(η00) min(b, diag(b00)∣η00∣), and obtain
Aν = A0ν0 + max(0, diag(b00)η00 - b)
ν = A-1A0ν0 + A-1 max(0, diag(b00)η00 - b)
(*)	(**)
≤ |A-1A0|1 + |A-1 diag(max(0, b00 - b))|1 ≤ 1
where in (*) We use the relation shown in Theorem 5.1 for the sound representation of a decom-
position and the fact that setting η00 to a one vector maximizes diag(b00)η“ and (**) follows from
Eq. (10). Taking the absolute value We obtain ∣ν| ≤ 1 and have shown that both V and η exist. □
Theorem 5.2 postulates a sufficient but not necessary condition. This becomes apparent by the over-
approximation in the proof, the fact that our Box component does not compensate the Zonotope
component and the over-approximation due to error consolidation.
5.2 monDEQ Certification By Fixpoint Set Iteration
Equipped with the building blocks discussed so far, we now introduce CRAFT (short for Convex
Relaxation Abstract Fixpoint iTeration). On a high level, given an input x, target label t, radius and
7
Under review as a conference paper at ICLR 2022
Table 1: Overview over the obtained natural accuracy (Acc.), adversarial accuracy (Bound), the
number of samples for which the fixpoint set iteration converged (Conv.), the certified accuracy
(Cert.), and the average time per sample for 100 samples.
Dataset	Model	Latent Size	# Acc.		# Bound	# Conv.	# Cert.	Time [s]
MNIST	FCx40	40	99	0.05	70	100	36	17.2
	FCx87	87	99	0.05	75	100	30	15.8
	FCx100	100	96	0.05	73	100	23	11.3
	FCx200	200	99	0.05	83	100	26	14.0
	ConvSmall	648	97	0.05	80	100	68	22.4
CIFAR10	FCx200	200	63	2/255	36	100	22	16.8
	ConvSmall	800	55	2/255	32	98	29	41.1
monDEQ h, CRAFT propagates M-Zonotopes, utilizing Theorems 4.1 and 4.2 to determine an over-
approximation Z of the true fixpoint set Z*. Based on this, We compute an over-approximation of
the last layer Y = VZ* + V and then check the logit differences Yt -Yi > 0 ∀i = t (as described
in Singh et al. (2018)) Where Yt - Yi denotes the minimal value of yt - yi for y ∈ Y.
This approach certifies that the true mathematical fixpoints z* (x0), rather than the fixpoint found
by any particular solver, for all x0 With kx0 - xk∞ ≤ lead to correct classification. HoWever, as
any correctly implemented solver is guaranteed to converge to these fixpoints With arbitrary preci-
sion, this yields a stronger certificate. Thus, We, in agreement With prior Work (Chen et al., 2021;
Pabbaraju et al., 2021; Revay et al., 2020), certify properties for the true fixpoints ofa model.
Algorithm 1 shoWs a slightly simplified version of CRAFT. After initializing (line 2) Z0 = U0 =
{z* (x)} to a concrete fixpoint We repeatedly consolidate the M-Zonotopes (Theorem 5.1), ap-
ply the set over-approximation of PR (line 7) and perform an inclusion check via Theorem 5.2
(lines 6 and 8). If the check succeeds, then by Theorem 4.1 We knoW that Zn+1 contains the true
fixpoint set. After this We perform iterations of FB, Which by Theorem 4.2 preserves this fixpoint set
but can decrease the volume of the over-approximation, until We can verify correct classification. In
App. B We discuss the engineering consideration leading to differences betWeen the presented and
implemented version.
6	Experimental Evaluation
We noW evaluate Craft on multiple monDEQs architectures for the CIFAR10 (Krizhevsky et al.,
2009) and MNIST (LeCun et al., 1998) datasets. We first shoWcase neW state-of-the-art results and
then investigate the impact of different algorithmic components in an ablation study.
Experimental Setup We implemented CRAFT in PyTorch (Paszke et al., 2019) and evaluated it
on a single Nvidia TITAN RTX using a 16 core Intel Xeon Gold 6242 CPU at 2.80GHz. As With
prior Work (Chen et al., 2021), We alWays evaluate the first 100 samples of the test set and report
the average certification time for correctly classified samples (Time), the certified accuracy (Cert.)
and the number of samples for Which We found a fixpoint set over-approximation (Conv.). For
implementation and experimental details as Well as (hyper)parameter choices, see App. B and C.
6.1	monDEQ Robustness Certification
In Table 1 We shoW results for a range of fully connected and convolutional monDEQs. Bound de-
notes the number of samples Which Were empirically robust to PGD attacks (Madry et al., 2018). We
generally observe that While the smaller fully-connected netWorks have loWer empirical robustness,
itis easier to certify them, With the smallest netWork yielding the highest certified accuracy. Perhaps
surprisingly, We find on both MNIST and CIFAR10 that convolutional netWorks, While empirically
not quite as robust as large fully connected ones, are comparatively easier to verify, yielding notably
higher certified accuracies and smaller gaps betWeen the knoWn upper bound and certification rate.
8
Under review as a conference paper at ICLR 2022
Comparison with SemiSDP We compare
against the numbers reported for the “robustness
model” (SemiSDP) approach introduced by Chen
et al. (2021), the current state-of-the-art for verify-
ing '∞ robustness properties, on the benchmarks
they propose in Table 2: a fully connected MNIST
network with latent space size 87, the largest their
SDP solver is able to handle, and three perturbation
sizes ∈ {0.01, 0.05, 0.10}. For = 0.01 both
tools are able to certify all 99 empirically robust
Table 2: Comparison of Craft with the ap-
proach from Chen et al. (2021) (SemiSDP)
on the only network they evaluate on (FCx87).
	# Bound	SEMISDP		CRAFT (ours)	
		# Cert.	Time [s]	# Cert.	Time [s]
0.10	8	0	1350	0	9.75
0.05	75	24	1350	30	15.75
0.01	99	99	1350	99	1.40
samples, with CRAFT on average taking only 1.4s compared to the 1350s of SEMISDP. For
= 0.05, CRAFT is 25% more precise (30 vs 24 samples) and with an average run time of 15.75s
is two orders of magnitude faster. While for = 0.1, there exist 8 empirically robust samples
neither tool can verify any. Finally, as shown in Table 1, Craft scales to larger networks and more
challenging datasets than SemiSDP. Chen et al. (2021) also propose two alternative certification
modes, "lipschitz model" and "ellipsoid model", however, neither can verify any property at
= 0.05, hence we omit a detailed comparison.
6.2 Ablation Study
Finally, in Table 3 we report the results ofan ab-
lation study on several key features of Craft.
M-Zonotope We analyse the effectiveness of
M-Zonotope, by setting either b = 0 (no Box)
or A = 0 (no zono). Disallowing the Zono-
tope component leaves a standard Box, which
converges quickly, but fails to prove any prop-
erty. Disallowing the Box component, which
still leaves a M-Zonotope utilzing consolidation
(rather than standard Zonotope), significantly
reduces the range of solver parameters (in par-
ticular α) leading to convergence, up to the
point where we were unable to find such param-
eters for some networks. The line indicates by
f uses PartricUlarly tuned ParametereS while the
other our default hyperparameters, also used for
other experiments.
Table 3: Overview over the obtained natural ac-
curacy (Acc.), adversarial accuracy (Bound), the
number of samples for which the fixpoint set it-
eration converged (Conv.) the certified accuracy
(Cert.), and the average time per sample on FCx87.
Ablation	# Conv.	# Cert.	Time [s]
Reference	100	30	19.30
No zono component	100	0	0.38
No Box component	0	0	-
No Box componentt	100	30	17.24
Maximum ɑ for PR t	99	30	15.75
Only PR	100	5	9.61
Only FwdBwd	100	27	11.45
Only consider contained	100	0	7.14
No joint consolidation	0	0	-
t Significant parameter tuning needed for convergence
Iteration method Increasing the splitting parameter α for PR, which is employed until the fixpoint
set iteration converges (line 7 in Algorithm 1), can reduce certification times by 〜 3.5s at the
cost of a less robust convergence. Only running PR, but not FB (Algorithm 1), retains the strong
convergence but yields much looser bounds allowing us to certify only 5 samples. Running only
FB forces us to choose an α for convergence that yields looser bounds, leading to only 27 samples
being certified. Requiring containment to be shown for the same step in which we certify prevents
all certification and highlights the importance of Theorems 4.2 and A.1.
Joint Consolidation
Z-X	1 ∙ 1 . ∙	^f)	Λ t-r ' 1	1 . 1	1 ∕' ∙ ∙ .1	∙ 11 i'	Λ
Consolidating U and Z independently instead of jointly yields fewer but
independent error terms, precluding any cancellations and preventing convergence.
7	Conclusion
We proposed a novel verification framework, called Craft, for fixpoint-based neural architectures
such as monDEQs. The key challenge was to lift fixpoint iterations from concrete points to sets and
to then approximate these sets with a novel convex relaxation, called M-Zonotope, enabling efficient
inclusion checks. Our extensive experimental evaluation shows that Craft outperforms the current
state-of-the-art in monDEQ certification by two orders of magnitude in terms of speed, one order of
magnitude in terms of scalability, and about 25% in terms of certification rate.
9
Under review as a conference paper at ICLR 2022
8	Reproducibility Statement
During the review process we will make all code and trained models available as supplemental
material, and upon publication release it as a public github repository. Further, App. B and C outline
implementation details and parameter choices. Lastly, Section 6 contains information about the
hardware used to perform all timing experiments.
9	Ethics Statement
We show robustness certification techniques for a particular class of machine learning model. While
such models can be used can be used maliciously, our certification approach does not alter this. As
we obtain mathematically certifiable results, advances building on this work can be used to ensure
robustness of machine learning models in safety critical applications.
10
Under review as a conference paper at ICLR 2022
References
Brandon Amos and J. Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In Proc. of ICML, volume 70, 2017.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.
Rudy Bunel, Jingyue Lu, Ilker Turkaslan, Pushmeet Kohli, P Torr, and P Mudigonda. Branch and
bound for piecewise linear neural network verification. Journal of Machine Learning Research,
21(2020), 2020.
Tong Chen, Jean-Bernard Lasserre, Victor Magron, and Edouard Pauwels. Semialgebraic repre-
sentation of monotone deep equilibrium models and applications to certification. ArXiv preprint,
abs/2106.01453, 2021.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via random-
ized smoothing. In Proc. of ICML, volume 97, 2019.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin T. Vechev. AI2: safety and robustness certification of neural networks with abstract interpreta-
tion. In 2018 IEEE Symposium on Security and Privacy, SP 2018, Proceedings, 21-23 May 2018,
San Francisco, California, USA, 2018. doi: 10.1109/SP.2018.00058.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, and Armin Askari. Implicit deep learning. ArXiv
preprint, abs/1908.06315, 2019.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy A. Mann, and Pushmeet Kohli. On the effectiveness of interval
bound propagation for training verifiabl. ArXiv preprint, abs/1810.12715, 2018.
Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy A. Mann, and Pushmeet Kohli.
An alternative surrogate loss for pgd-based adversarial testing. ArXiv preprint, abs/1910.09338,
2019.
Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth
Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, et al. The marabou framework for Ver-
ification and analysis of deep neural networks. In International Conference on Computer Aided
Verification. Springer, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Adrian Kulmburg and Matthias Althoff. On the co-np-completeness of the zonotope containment
problem. European Journal of Control, 2021.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11), 1998.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. 2019 IEEE Symposium on Security
and Privacy (S&P), 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In Proc. of ICLR, 2018.
Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for
provably robust neural networks. In Proc. of ICML, volume 80, 2018.
Chirag Pabbaraju, Ezra Winston, and J. Zico Kolter. Estimating lipschitz constants of monotone
deep equilibrium models. In Proc. of ICLR, 2021.
11
Under review as a conference paper at ICLR 2022
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, LUca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
LU Fang, JUnjie Bai, and SoUmith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, 2019.
Aditi RaghUnathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying ro-
bUstness to adversarial examples. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montreal, Canada, 2018.
Max Revay, RUigang Wang, and Ian R. Manchester. Lipschitz boUnded eqUilibriUm networks. ArXiv
preprint, abs/2010.01732, 2020.
Ernest K RyU and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math, 15
(1), 2016.
Sadra Sadraddini and RUss Tedrake. Linear encodings for polytope containment problems. In 58th
IEEE Conference on Decision and Control, CDC 2019, Nice, France, December 11-13, 2019,
2019. doi: 10.1109/CDC40024.2019.9029363.
Frangois Serre, Christoph Muller, Gagandeep Singh, Markus Puschel, and Martin Vechev. Scal-
ing polyhedral neUral network verification on GPUs. In Proc. Machine Learning and Systems
(MLSys), 2021.
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin T. Vechev. Fast
and effective robustness certification. In Advances in Neural Information Processing Systems 31:
Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December
3-8, 2018, Montreal, Canada, 2018.
Gagandeep Singh, Rupanshu Ganvir, Markus Puschel, and Martin T. Vechev. Beyond the single neu-
ron convex barrier for neural network certification. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, 2019a.
Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin T. Vechev. An abstract domain for
certifying neural networks. PACMPL, 3(POPL), 2019b. doi: 10.1145/3290354.
Yusuke Tashiro, Yang Song, and Stefano Ermon. Output diversified initialization for adversarial
attacks. ArXiv preprint, abs/2003.06878, 2020.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane S.
Boning, and Inderjit S. Dhillon. Towards fast computation of certified robustness for relu net-
works. In Proc. of ICML, volume 80, 2018.
Ezra Winston and J. Zico Kolter. Monotone operator equilibrium networks. In Advances in Neu-
ral Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Proc. of ICML, volume 80, 2018.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified
robustness and beyond. In Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual, 2020.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural
network robustness certification with general activation functions. In Advances in Neural Infor-
mation Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, 2018.
12
Under review as a conference paper at ICLR 2022
A	Additional Theorems
Theorem A.1 (Fixed-Point set preservation for Peaceman-Rachford splitting). Let h be a monDEQ
with solver iteration [zn+1, un+1] = gαP R (x, zn, un) defined as per Eq. (4) (Peaceman-Rachford
splitting). Let X denote the Set of possible inputs and Z* the set of reachable fixed-points of h
given inputs in X. Let Z be a sound over-approximation of Z*, i.e., Z* ⊆ Z and U a sound over-
approximation of the associated U *. Any application of the Peaceman-Rachford splitting yields an
over-approximation ofthe fixpoint set [J?0, U0] = gPR#(X, Z, U) with Z* ⊆ Z0.
Proof. To prove by contradiction, let Z be a point close to the fixpoint z* s.t. ∣∣z - z*k ≤ E with
map Z0 = gα (x, Z, U) under the fixpoint iterator. Let Us further assume that ∀u ∈ U * an application
of z0 = gα(x, z*, u) does not map back to z*, i.e., ∣z* - z0∣ > d.
•	Recall that gα (x, Z, u) is locally Lipschitz with L < ∞ in u, Z and x as it is the compo-
sition of linear maps of finite width.
•	It follows that ∣gα(x, Z, u) - gα(x, z*, u)k = ∣∣Z0 - z0∣ ≤ Le
•	Hence by the inverse triangle inequality	∣∣z* - Z0∣ ≥ ∣∣ ∣z* - z0∣	-	∣Z0	- z0∣	∣∣	≥	d - Le
•	Choose e ‹ d/(L + 1) =⇒ d — Le >	e =⇒ ∣∣z* — Z0∣ > e
•	It follows that ∣gα(x, Z, U) — z*k > e	∀Z ∈ {z | ∣z - z*∣ ≤	e}	which contradicts	the
convergence guarantee.
It follows that ∃u ∈ U* : z* = gα(x, z*, u).	□
This result implies that we can apply any sound abstraction of Peaceman-Rachford splitting to an
over-approximation of the iteration state [Z, U] and obtain a possibly tighter over-approximation of
this state.
Theorem A.2 (s-step Fixed-Point contraction). Let [Zn+1, Un+ι] = g# (X, Zn, Un) be closed sets
over-approximating Zn+1 and Un+1 obtained by applying the solver iteration n + 1 times for some
Z0, U0 and all inputs x ∈ X. Further let s ∈ N≥1:
ʌ ʌ ʌ ....
Zn+s ⊆ Zn ∧ Un+s ⊆Un	=⇒	Z * ⊆
Zn+s	(11)
Proof. Again we write s = [Z, U] and then define sn+1 := g0(sn) = gα(x, sn). Based on this, we
then let Sn+s = gS(sn = g0(…g0(sn)). Then Eq. (5) follows directly from applying Theorem 4.1
to gS.	□
A. 1 Other Activation Functions
In order for CRAFT to be able to certify monDEQs utilizing an activation function σ anlgously to
the discussed ReLU, we require:
•	We need convergence and uniqueness guarantees for the original monDEQ in the concrete
(via operator splitting); to this end Theorem 1 in Winston & Kolter (2020) requires σ to
be a proximal operator of a CCP function, which most common Deep Learning activation
functions are.
•	In order to utilize FB after convergence we need a version of Theorem 4.2, which shows
that Forward-Backward splitting preserves fixpoints. Our proof of Theorem 4.2 relies on
the ReLU function. However, a more general proof can be constructed similar to that of
Theorem A.1.
•	Lastly, we need an M-Zonotope transformer for the activation function. For many choices
the respective Zonotope transformers (such as those for Sigmoid and Tanh discussed in
Singh et al. (2018)) can be simply adapted.
13
Under review as a conference paper at ICLR 2022
B	Implementation Detail
Algorithm 1 is a slightly simplified version of the Craft algorithm that we actually implemented.
Here we discuss the differences.
Consolidation and Inclusion check In practice we perform the consolidation (line 6) only every
rth iteration and only recompute the PCA basis for consolidation every 30 steps. Since we require
a consolidated basis for the inclusion check (line 8) we always keep the up to 10 last consolidated
[Zk,Uk] and check [Zn+1,Un+1] against all of these. Note that this requires the use of Theorem A.2
rather than Theorem 4.1.
Consolidation after containment Furthermore, after convergence we still apply consolidation to
Zn, every r0 steps, but no inclusion check. We recompute the PCA basis every second check. As
each iteration, the ReLU introduces potentially new error terms this consolidation allows us to keep
the memory-footprint low.
PR after containment In Algorithm 1, after containment we switch from PR to FB. In practice
however, we apply PR for another 20 iterations as it converges more stably. The correctness of this
follows from both Theorem 4.1 and Theorem A.1.
Widening To show containment, not the absolute tightness of the over-approximation of the iter-
ation state is key, but the increase in tightness. However, if we already have a very tight approx-
imation, tightening it further can be very challenging. To overcome this issue, in Algorithm 1 we
一 perhaps counterintuitively - widen our over-approximation as part of the error consolidation by
intentionally introducing looseness:
A =(WmulA 1A|1 + Wadd1) ∙ A	(12)
with the multiplicative and additive widening parameters wmul and wadd, respectively. This increase
in looseness between the current approximation and the exact fixpoint set. Therefore, it can be
easier to tighten the approximation further and hence show containment. After containment has
been shown, we set both parameters to 0.
Termination Heuristics In practice we abort the main loop early in cases where we likely wont
be able to verify the input. Before convergence we abort if the volume of the M-Zonotope [Zn , Un]
reaches a width of 109 in any direction. After convergence we abort if in 3r0 steps we did not observe
any improvement in maxi6=t Yt - Yi (and are about to compute a new PCA basis).
DeepZ Slope Additionally if we reach convergence, but are not able to verify robustness in the
main loop and mini6=t Yt - Yi > -1 we unroll 20 steps of FB into a standard-neural network and
optimize the slopes of the ReLU transformer for 60 optimization steps (Wong & Kolter, 2018; Weng
一	―一一	.„	,	ʌ	ʌ.	C	一一	一一	-	一	___
et al., 2018; Zhang et al., 2018). If mini6=t Yt - Yi > -0.15 we unroll 60 steps and employ 200
optimization steps.
C Parameter Choices & Experiment Details
C.1 Model Training
All networks were trained with monotonicity parameter m = 20.0 using standard minibatch gradient
descent and implicit differentiation as outlined in Winston & Kolter (2020).
C.2 Craft Parameters
Generally we use the default values discussed in App. B unless stated otherwise. For all experiments
we use nmax = 500 and summarize the main parameters in Table 4. By default we use r = 3 and
increase it to 5 on larger MNIST models for better convergence.
14
Under review as a conference paper at ICLR 2022
Table 4: Craft verification parameters.
Dataset	Model	r	r0	αPR	widening
MNIST	FCx40	3	50	0.1	const
	FCx87	3	50	0.1	const
	FCx100	5	50	0.06	const
	FCx200	5	50	0.05	const
	ConvSmall	5	50	0.05	-
CIFAR10	FCx200	3	30	0.06	exp
	ConvSmall	3	30	0.06	exp
Table 5: Comparison of M-Zonotope to other abstract domains, L denotes the number of network
layers or solver iterations in our case, d is the latent space or input dimension, d ≤ ne ≤ Ld is the
number of error terms for Zonotope.
	Propagation		Efficient Inclusion Check	Precise
Interval Bounds / Box	O(L)	✓	✓	X
(Hybrid)Zonotope	O(L2)	X	X	✓
Linear Backward-Mode	O(L2)	X	(✓)	?
Linear Forward-Mode	O(L)	✓	(✓)	X
M-Zonotope	O(L)	V^^	✓	✓
For widening “const” denotes wmul = 1 + 10-3 and wadd = 1 + 10-2, “exp” denotes initialization
with “const” and scaling by 1.1 and 1.2 respectively every second iteration, and “-” denotes no
widening.
All parameters and in particular the values for α used in PR were found by coarse manual search.
Overall we observe large stability with respect to most parameters and in particular the value of α
does not have a large impact on PR, as we show in Section 6.2.
When switching from PR to FB, we apply a line search to determine an optimal αFB (with regard to
certification).
C.3 Adversarial Attack
In order to determine a bound on the certifiable accuracy of the models, we compute their empirical
accuracy with respect to a strong attack. We apply a targeted version (towards all classes) of PGD
(Madry et al., 2018) with 20 restarts, 50 steps utilizing margin loss (Gowal et al., 2019) and 5 step
output diversification (Tashiro et al., 2020).
C.4 Relation to other Convex Relaxations
Here we briefly outline the differences between M-Zonotope and other abstract domains. In par-
ticular, we will highlight why only the M-Zonotope domain, developed for Craft, is suitable for
monDEQ certification. In order to apply Algorithm 1, we require (i) an efficient propagation of
abstract elements through many iterations L of operator splitting, (ii) the ability to perform efficient
inclusion checks, and (iii) high enough precision to enable verification. We summarize this com-
parison inTable 5. We consider the dimension d of the latent space (see Table 1 for representative
values) and di of the input, the number of iterations (or “layers”) L (at least 50 in our case) and the
number of error terms ne (di ≤ ne ≤ di + Ld).
Interval Bounds / Box (Gehr et al., 2018; Mirman et al., 2018; Gowal et al., 2018) Interval Bound
Propagation or the Box domain enable efficient propagation (O(Ld2)) and inclusion checks (O(d)).
However, we show in Table 3 ("No zono component") that its precision is insufficient to verify any
input. Propagation is dominated by a matrix-vector multiplication O(d2) per layer L (which needs
15
Under review as a conference paper at ICLR 2022
to be executed twice) and inclusion checks can be performed by evaluating two inequalities per
dimension d.
(Hybrid)Zonotope (Gehr et al., 2018; Mirman et al., 2018; Wong & Kolter, 2018; Singh et al.,
2018) Zonotopes, HybridZonotopes and improper M-Zonotopes are equivalent in their precision,
which we hence know to be sufficient for verification. Crucially however, there are two major dif-
ferences: (i) without our error consolidation, each ReLU can potentially add an error term allowing
their number to grow to di + Ld making propagation very expensive and (ii) without requiring
the error matrix of proper M-Zonotope to be basis of the Rd, exact inclusion checks become co-
NP-complete (Kulmburg & Althoff, 2021) and even approximate inclusion checks Sadraddini &
Tedrake (2019) become infeasible in high (> 30) dimensions.
Linear Bounds (Zhang et al., 2018; Singh et al., 2019b) Methods based on the propagation of
linear bounds (DeepPoly) are state-of-the-art for applications where runtime is critical (Serre et al.,
2021). Yet, mathematically and empirically they are not necessarily more precise than Zonotope
(Singh et al., 2019b). In a nutshell, for each neuron, these methods keep a lower and upper bound as
a linear function in the previous layers neurons. Thus, linear bounds in terms of the input values can
be computed by repeated substitutions, e.g. zl, the activations in layer l are bounded ll ≤ zl ≤ ul
where ll, ul are linear functions in the input values x. ll and ul can either be obtained by substituting
forward (starting at the first layer) or backward (starting at the last layer).
In forward mode, the bounds are constructed layer-by-layer from the input, similar to Zonotope
bounds, only incurring the cost of matrix-matrix multiplications (O(Ld3)). However, a key differ-
ence is that during the abstraction of non-linear functions, Zonotope transformers introduce only
symbolic error terms which can cancel out later on while DeepPoly transformers introduce concrete
error terms which can’t cancel out later on. This turns out to be absolutely crucial during the iterative
solve, to the extent that errors accumulate exponentially for DeepPoly, while we need widening for
M-Zonotope. We showcase this in an example below.
In the backward construction, ll is constructed by recursive back-substitution starting at the network
output (or latentspace in our case). While intermediate bounds can be computed more cheaply using
a forward propagation method, every inclusion check (after every iteration) would require a full
back-substitution through all preceding iterations. Thus the bounds for layer l have cost O(ld3) (as
bounding one layer is dominated by d-dimensional matrix-matrix multiplications). Thus the overall
complexity becomes O(L2d3). However, while in principle limited in a similar way as forward
propagation, back-substituting the linear expressions describing the outputs of affine layers allows
for much more cancellations to happen before too many error terms are introduced, which leads to
tighter bounds. We refer interested readers to Singh et al. (2019b) for more detail on these methods.
While their cost or lack of precision already makes both variants of DeepPoly unsuitable for mon-
DEQ verification, the actual inclusion check would be conceptually and computationally simple,
see Lemma 1 in Sadraddini & Tedrake (2019) for a linear programming approach with 4d2 + di
variables or Equation 9 for an approach based on 2d LPs with d + di variables. Thus, depending on
the encoding the overall complexity will be O((d4+m + di2)n) or O((d3+m + di2d)n) where n is the
number of bits (O(ddi)) to encode the problem and 0 ≥ m ≥ 1 (depending on the exact algorithm).
Imprecision of Forward-mode Linear Bounds To showcase the cancellation issue, consider the
following simple example:
- 0.5 ≥ x0 ≥ 0.5
x1 = ReLU (x0)
x2 = 2x1 + 1
x3 = x2 - x1 .
Using the Zonotope or M-Zonotope relaxation we can apply the transformers from Singh
et al. (2018), to obtain the over-approximations X0, X1, X2, X3 of reachable values variables
x0, x1, x2, x3. If we want to track the reachable values with linear bounds (by following the Deep-
Poly transformers as described in Singh et al. (2019b)) we obtain a li , ui as lower and upper bound
for each xi.li and ui are linear functions in the concrete values of l0, u0. Both approaches are shown
below:
16
Under review as a conference paper at ICLR 2022
X0	= 0 + 2 V0	VO ∈ [-1, 1]	-0.5 =: lO	≤	xO	≤	UO := 0.5
X1	二 8+ 4 VO + 1 + VI	V1 ∈ [-1, 1]	lO	≤	x8	≤	2 uo + 4
X2	_ 10 I 2j,	I 2 I j, 二 ^8^ + 4 VO + 8 + VI		2lO + 1	≤	x2	≤	uo + 4
X3	二 8+ 4 VO + 8 + VI		2lo - 8 Uo + 4	≤	x3	≤	-l0 + uO + 4
0.75	≤ x3 ≤ 1.5		0.0	≤	x3	≤	1.5
We observe that the final bounds for the Zonotope (left) are much tighter than those for DeepPoly
linear bounds. The reason of this is, that the linear bounds do not capture the relation between x2
and x1, while the Zonotope does this via the error terms ν0, ν1. This leads to an large accumulation
of errors when subtracting (or adding with a negative factor) two related terms. In Craft this is
a frequent operation, e.g., to determine Wz + Ux in every iteration or the updates of u in PR,
making rendering it unsuitable.
Note that we can make the DeepPoly bonds slightly tighter in this case by treating the ReLU similar
to the Zonotope case, but this does not elevate the fundamental limitation.
17