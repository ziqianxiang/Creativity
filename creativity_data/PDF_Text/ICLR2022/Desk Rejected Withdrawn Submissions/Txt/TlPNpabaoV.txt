Under review as a conference paper at ICLR 2022
On the Efficiency of Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The efficiency of neural networks is essential in large-scale deployment scenar-
ios such as mobile applications, internet of things, and edge computing. For a
given performance requirement, an efficient neural network should use the sim-
plest network architecture with a minimal number of parameters and connections.
In this paper, we introduce a framework to analyze and obtain efficient neu-
ral networks. In summary, our main contributions are three-fold. Our first con-
tribution is the subnetwork hypothesis to address overfitting issues and help
explain the effectiveness of several key techniques in training efficient net-
works: 1) softmax normalization in output layers may be one major cause of
overparameterization; 2) using log likelihood ratio representation in output layers
can reduce overfitting; 3) weight decaying and structural regularization can also
effectively reduce overfitting. The second contribution is a simple and effective
snapshot-based procedure to prune a well-trained network that minimizes
overfitting - pruning unimportant weights and connections first, and simply ad-
just remaining non-weight parameters using the backpropagation algorithm. Be-
sides, the snapshot-based pruning method can also be used to evaluate the effi-
ciency of trained networks. Finally, we hypothesize that there exist lower bounds
of the total number of bits for representing parameters and connections regarding
performance metrics for a given optimization problem. Rather than focusing on
improving the sole accuracy metric with more complex network architectures, it
is also important to explore the trade-offs between accuracy and the total number
of representation bits, when comparing different network architectures and imple-
mentations.
1 Introduction
Deep learning has achieved tremendous success in large-scale machine learning systems, such as
big-data analytics (Najafabadi et al., 2015), billion-parameter generative models for natural lan-
guage processing (Brown et al., 2020; Radford et al., 2019), and computer vision for self-driving
cars (Grigorescu et al., 2020). A general trend for recent success is the use of neural networks
of ever-increasing model sizes and their exponentially increasing computation power requirement.
Training these gigantic neural network models require tens of thousands parallel computing units
inside dedicated computer clusters with extremely high transient storage capacity and data synchro-
nization bandwidth. Consequently, some of the state-of-the-art models are only accessible to very
few researchers in the machine learning community.
On the other hand, large-scale deployment of machine learning applications in low-power scenarios,
such as mobile applications, internet-of-things (IoT), and edge computing, has put more stringent
requirements on the efficiency of neural network models. For a given problem and performance
metrics, efficient neural network models should have a minimal number of weights and connections,
simple network topology and architecture suitable for low-power computing devices, and low data
bandwidth and transient storage requirements. It is important to investigate the model efficiency
problem to bridge the performance gap between petascale high-end models and low-power neural
architectures for large-scale deployment. However, methods and principles for obtaining efficient
deep neural networks have not yet been thoroughly studied.
In this paper, we introduce a framework to analyze and obtain efficient deep neural networks.
Especially, we identify several key issues in training efficient deep neural networks and propose a
new model compression procedure to prune redundant weights and connections. One important in-
1
Under review as a conference paper at ICLR 2022
sight of our study is the high correlation between overfitting and model efficiency. Overfitting may
improve training accuracy, but it can cause overparameterization. In Section 2, we show that softmax
output layers can introduce non-deterministic effects to the backpropagation algorithm, yielding re-
dundant subnetworks with exploding numbers of parameters. To solve this problem, we propose
the log likelihood ratio (LLR) representation for output layers. We also investigate potential mech-
anisms for weight decaying and structural regularization to reduce overfitting. Furthermore, we
propose a simple and effective snapshot-based pruning procedure to obtain efficient deep neural
networks. We empirically validate this novel approach in Section 3 using various deep learning
architectures including LeNet, ResNet, and DenseNet, on the MNIST, CIFAR-10, and CIFAR-100
datasets. Based on the empirical results, we further discuss the model efficiency regarding informa-
tion cost of model representation. Section 4 reviews prior work in regularization, overfitting, and
model compression, followed by Section 5 that concludes the paper.
2 Efficient Neural Networks
A fundamental assumption of our analysis is that a complex neural network can be decomposed into
subnetworks that are responsible for different operation modes. In other words, the complex non-
linear function of a neural network can be decomposed into groups of sub-functions. Each group
of sub-functions represents one mode of operation. In this way, the efficiency of a neural network
highly depends on the composition and correlation between these groups of sub-functions. Thus,
overfitting may be viewed as forming redundant subnetworks that reduce the efficiency of trained
networks.
In this section, we shows that a critical step for obtaining efficient neural networks is to eliminate
redundant subnetworks by minimizing overfitting. We first analyze the overfitting issues caused
by redundant subnetworks and describe potential mitigating mechanisms. Several hypotheses pre-
sented in this section will also be empirically validated using experiments in Section 3. Finally,
we introduce a novel snapshot-based procedure to obtain efficient deep neural networks by pruning
their unimportant weights and connections. This procedure is also used to analyze the efficiency of
trained networks.
2.1	Softmax Normalization
The softmax function, a.k.a. softargmax, is a normalization function often used as the last activation
function of a neural network (Bishop, 2006). Let Z = {zo, zι,…,z%, ∙…} represents the input
vector, the Softmax output vector Q = {q0,q1, ∙∙∙ ,qi,…} is defined as
=exp(zi)
qi	Pk exP(Zk)
(1)
where qi ∈ [0, 1] and i qi = 1. Thus, the normalized output vector can be interpreted as
marginal probabilities. The softmax output can be naturally combined with the cross entropy func-
tion J = - Pi pi log qi, where pi is the target probability. The derivative of J with respect to zi
takes a simple form of qi - pi (Goodfellow et al., 2016). The simple probabilistic interpretation
and derivative computation make the combination of softmax normalization and cross entropy loss
a pervasive choice for multinomial classification problems. However, potential issues using softmax
normalization with the backpropagation (BP) algorithm has not been fully investigated.
Suppose a neural network G can be decomposed into two or more smaller subnetworks G =
{Go, Gι, ∙∙∙ , Gm, ∙…} with the same feature input X. The final activation Z is the superposi-
tion of the subnetwork activation before the softmax normalization in the output layer
MM
Z= XYm= X fm(X)
m=0	m=0
(2)
where fm is the non-linear function representing subnetwork Gm . The decomposition is done
according to the final activation without considering intermediate hidden layers. The softmax nor-
malization operation has the following properties regarding the relationship between subnetwork
activations (see Appendix A).
2
Under review as a conference paper at ICLR 2022
1.	If the subnetwork activations are linear offset versions of each other, such that Y0 = Y1 -
βι ∙∙∙ = Ym - βm ∙∙∙, the normalization result of the whole network is equivalent to
applying the softmax function to the activation of any subnetwork scaled by M : Q =
softmax(M Ym). Note that the offset between subnetwork activation Ym has no impact on
the softmax output. If the activations Ym are linearly semi-correlated, the generalized
softmax property is applicable, i.e., that Q ≈ softmax(MYm).
2.	If the subnetwork activations are scaled versions of each other, such that Y0 = ɑ1Y1 ∙…=
akYk •… and 1 ≥ αι ≥ ɑ2 ≥ •… ≥ αk ∙∙∙, the normalization operation is equivalent
to applying the softmax function to the scaled principal subnetwork: Q = softmax(SY0),
where S =1 + αι + α2 +---------. The softmax normalization allows proportional integration
of information. A single subnetwork that has very strong activation (higher prediction
probabilities) can dominate over other subnetworks with weak activations. If there are no
dominant subnetworks, the total number of contributing subnetworks may be large and the
whole network tends to be overparameterized.
In short, the softmax function can act as a super combinator for different modes of the neural net-
work, summing and amplifying weak subnetwork activations. This could partially explain why
deep neural networks are so expressive that they are suitable for diverse types of problems. How-
ever, when there are redundant subnetworks that produce linearly correlated activations, the softmax
normalization function make them indistinguishable from each other. The linearly correlated sub-
networks potentially lead to overfitting and overparameterization. We have the following hypothesis
regarding the effects of such redundant subnetworks:
Hypothesis 1: For deep neural networks, the existence of redundant subnetworks combining with
softmax normalization can lead to overfitting and overparameterization when training with the back-
propagation algorithm.
The derivative of the cross entropy loss is linear with regard to the softmax output Q and target
P , and softmax normalization makes it impossible to differentiate between the effects of different
subnetworks, the BP algorithm thus will fine-tune all the parameters without penalizing any indi-
vidual subnetwork. Therefore, the initialization of weights may create redundant subnetworks
that have non-deterministic effects on the training process. For example, Mishkin & Matas
(2016) demonstrated that initialization of weights can affect test accuracy. Such behaviors and
the existence of redundant subnetworks will be validated from empirical results in Section 3.
2.2	LLR Representation
The softmax normalization is mainly used to convert neural network outputs to probabilities. How-
ever, the softmax normalization allows linearly correlated subnetwork activations and potentially
introduces overfitting. Therefore, it is desirable to avoid softmax normalization in output lay-
ers. It turns out that using the log likelihood ratio (LLR) representation in output layers can
avoid normalization and overfitting issues. Given a binary random variable X and P1 (X) =
{probability X is true}, the LLR for X can be defined as
LLR(X )=logτ≡⅛
(3)
Since neural networks can model arbitrary non-linear functions, we can adopt LLR representation
for each component of the outputs Y and target labels T . For both multi-class and multi-label
classification, the problem can be regarded as multiple binary regression problems adopting the
LLR representation for each class. Therefore, output normalization across different classes is not
needed, but loss functions need to be changed accordingly - We introduce the bipolar softplus (BSP)
loss function as defined in Appendix B.1. We demonstrate that the LLR representation combined
with the BSP loss function does not need normalization and avoids the introduction of redundant
subnetworks. The choice of loss functions is not mandatory and there may be better alternative loss
functions. The optimization of loss functions will be addressed in future study. In this paper, we use
empirical results to demonstrate the effectiveness and behavior of this novel scheme. We introduce
the following hypothesis regarding the LLR representation:
Hypothesis 2: For classification problems with deep neural networks, using the LLR representation
for the output layer and the target labels can reduce overfitting and avoid overparameterization
compared with softmax normalization.
3
Under review as a conference paper at ICLR 2022
It is worth emphasizing that the LLR representation has clear physical meanings, which could help
the explainability of neural networks. LLR values are symmetrical and centered around zero, which
can be regarded as a natural normalization point. Note that LLR values have range (-∞, +∞) and
a large magnitude means higher confidence in prediction. Thus, by controlling the LLR magnitude
of the target labels, we can introduce regularization to network outputs.
2.3	Weight Decaying
In previous discussion, potential issues in normalization and representation are analyzed by decom-
posing the activation in the output layer. In a similar fashion, we can also decompose the weights
and activation in each hidden layer as follows.
Suppose feature inputs of a layer can be represented as X = PmM=-01 Xm , and its weight matrix
W can be decomposed as W = PnN=0 Wn, where Wn is non-zero weight components, then the
activation Z can be decomposed as
M-1 N-1	M-1 N-1
Z= X XAm,n= X X(XmWn+B)	(4)
m=0 n=0	m=0 n=0
where B is the bias vector. When the rectified linear unit (ReLU) non-linear function is adopted,
only those activations larger than zero in Eq. 4 are effective. For a given feature input Xk , if there
are multiple Ak,n components that have all positive elements, the weight components Wn can be
effectively combined to reduce the total number of parameters. The redundant weights components
may be different for different input features. The existence of such redundant weight components
may become the source of overfitting and overparameterization. The large ones of these redun-
dant Ak,n components also tend to be working in the linear regime of the ReLU function, which
effectively reduces the non-linear behavior of the network. To reduce overfitting and redundancy,
the weights should have relatively small magnitudes working in the non-linear regime of the ReLU
function, hence we have the following hypothesis regarding weight decaying (L2 regularization).
Hypothesis 3: Limiting the magnitude of weights using weight decaying can reduce overfitting and
overparameterization in deep neural networks when the ReLU activation function is used.
This hypothesis could explain why regularizing weights is an effective technique to improve training
performance. Weight decaying should also be separated from loss regularization, which was first
discussed in Loshchilov & Hutter (2018). In our experiments, however, their AdamW algorithm
turns out to improve the training accuracy by increasing overfitting and overparameterization as
shown in Section 3.2.
2.4	Structural Regularization
The underlying assumption in the analysis of weight decaying is that the outputs of fully-connected
subnetworks can be freely superimposed with each other. Thus, if the combination of subnetworks
are restricted, overfitting issues could be mitigated. A common technique for this purpose is struc-
tural restriction of the subnetworks. Some examples are listed in the following.
1.	Structural pruning - Various techniques to selectively remove connections from the whole
network in training have been proposed and shown to reduce overfitting, such as Wan et al.
(2013). In a sense, stochastic gradient descent (SGD) can also be regarded as adopting
random structural pruning.
2.	Weight sharing - By sharing weights and forcing regular network structures, neural net-
works become more effective and easier to train. Convolutional neural network (CNN) can
be regarded as a prominent type which is often used as feature extraction layers.
3.	Micro-architectural design - By adopting certain topology patterns between or within neural
network layers, the resulting networks are confined to subsets of fully-connected networks,
hence their overfitting issues are mitigated. Skip connections, for example, have been show
to improve training speed and performance (He et al., 2016; Huang et al., 2017).
Many existing optimization techniques for training neural networks could be partially explained
and further analyzed using the subnetwork analysis. The underlying principle is that by reducing
4
Under review as a conference paper at ICLR 2022
the initial functional space, the optimization problem becomes less difficult and easier to converge,
which explains why micro-architecture design can have significant impact on the performance of
neural networks. In Section 3, the effects of structural regularization are partially demonstrated
by comparing the efficiency of different network architectures on the same dataset.
2.5	Snapshot-based Pruning
It is well known that neural networks can be made more efficient in terms of computation and storage
requirements by pruning some of the unimportant weights. For deep neural networks, the iterative
pruning and retraining procedure in Han et al. (2015) has been used for generating efficient neural
networks for low-power applications. However, the iterative procedure requires extra computing
power and processing time. Furthermore, the iterative procedure often requires manually fine-
tuning pruning thresholds. We discuss two important aspects of pruning neural networks in the
following.
1.	Important weights - Deciding which weights are important is the first key issue. In general,
weights with smaller magnitudes are considered unimportant and can be pruned, but this
may not always be the case for different types of components in various network architec-
tures. For example, shared weights in convolutional layers may be more important than
weights in fully connected layers. Even the importance of weights in the same layer may
not be correlated with their magnitudes.
2.	Retrain requirement - After pruning its weights and connections, a pruned neural network
usually needs to be adjusted. It is not clear which aspects of the network need to be mod-
ified. For the iterative pruning and retrain process, the weights and biases between initial
and final iterations may be completely different so that it is hard to analyze the iterative
retraining mechanism.
By analyzing experimental data from extensive empirical studies with different datasets and various
architectures, we have the following observation regarding these two key aspects for pruning neural
networks.
1.	Weight distribution - If the neural network is well trained such that overfitting is minimized,
the weight magnitude distribution correlates better with the weights’ importance. In other
words, weights with smaller magnitudes around zero can be effectively pruned. Different
layers and types of components may need different pruning thresholds but they can be
easily adjusted using macro network attributes.
2.	Essential network - The important weights together with their corresponding connections
define the essence of a trained network; therefore, they should be kept unchanged as a
snapshot. Only the biases need to be adjusted. Other non-weight parameters, such as batch
normalization parameters, may also need to be adjusted as well.
In short, iterative pruning and retraining may not be necessary when a neural network is well-trained.
The first step in obtaining efficient neural networks is to adopt efficient training techniques, such
as LLR representation, weight decaying, and structural regularization. After pruning unimportant
weights and connections from trained snapshots, the next step is to simply adjust remaining non-
weight parameters using the BP algorithm. For most architectures, because most of the parameters
are connection weights, adjusting the non-weight parameters requires much fewer iterations than the
initial training process - usually only a few epochs are enough.
Conversely, the efficiency and quality ofa trained network can be evaluated with the effectiveness of
parameter pruning. Refinement of optimization algorithms may also be further examined using this
pruning procedure. If a network is overparameterized, the performance of its pruned versions
deteriorates dramatically as the total number of parameters is reduced. The key discovery
here is the high correlation between overfitting and the efficiency of neural networks.
If trained neural networks are not efficient enough initially, combining iterative techniques with the
proposed snapshot-based pruning method could be beneficial. For very large networks, it should
be noted that using all the methods analyzed in this section may not be enough to yield efficient
deep neural networks using a single-shot training procedure.
5
Under review as a conference paper at ICLR 2022
3 Experiments
We empirically analyze the model efficiency trade-offs in deep neural networks as well as the
overfitting issues in training neural networks to validate the subnetwork assumption and the
analysis of various mitigating methods in previous section. We also demonstrate the effective-
ness of the proposed snapshot-based pruning procedure in obtaining and evaluating efficient neural
networks.
3.1	Methodology
The trade-offs between accuracy and total number of parameters are analyzed with various archi-
tectures and datasets using the following procedure: 1) the neural network is first trained using
different hyper-parameter settings and output representation; 2) the trained networks are
pruned using different threshold settings, and non-weight parameters are retrained using the
BP algorithm; 3) test accuracy and total number of parameters of the pruned networks are
averaged over at least 10 different experiment runs using the same pruning settings.
The pruning thresholds are set according to the standard deviation of weights’ magnitudes. Two
different thresholds are used for convolutional layers and linear layers, respectively. For ex-
ample, the threshold value for convolutional layers is calculated by multiplying the pruning
setting with the standard deviation of weights in all convolutional layers. Weights with magni-
tudes lower than the threshold value are pruned. In all cases, simple linearly-spaced pruning
settings are used without further fine-tuning. However, optimization of the pruning settings is
possible by taking into account the structural attributes of given network architectures.
Other hyper-parameter settings and detailed analysis of the results are included in Appendix C. In
the following, we focus on the efficiency trade-offs using different architectures on several datasets.
3.2	MNIST Classification
We first conduct experiments on the MNIST dataset (LeCun et al., 1998) using the LeNet-300-100
and LeNet-5 architectures (LeCun et al., 2015). In Figure 1 (left), the distribution of weights when
training with LLR representation is compared with the case of softmax normalization. Using LLR
representation yields a better distribution of weights - the probability of small weights around zero
is higher and these weights can be pruned with less impact on the performance. Furthermore, weight
decaying can push the weights aggressively towards zero as shown in Figure 1 (right). While soft-
max normalization primarily affects output layers, the ReLU function may cause overfitting
in all layers. Therefore, the effect of weight decaying is more prominent and effective as shown
in Figure 1. This observation is consistent with the analysis and hypotheses from Section 2.
Figure 1:	Effects of output representation and weight regularization on the distribution of weights
using (left) LLR representation and (right) weight decaying on the MNIST dataset.
Figure 2	shows the trade-off curves for test errors vs. total number of effective parameters
for all experiment results on the MNIST dataset. Each curve represents 20 trained networks
with the same training settings, each point represents the average total number of weights and
average top-1 errors of the pruned networks for each of the 10 different pruning settings. We
can see that using LLR representation instead of softmax normalization can reduce the total number
6
Under review as a conference paper at ICLR 2022
of parameters for the same accuracy requirement. Using weight decaying also significantly improve
the efficiency of the trained networks. Using both methods yields the most efficient neural networks
with better performance than the ones using the iterative pruning approach from Han et al. (2015),
as shown in Table 1-2 in Appendix C.1. Compared with fully-connected networks, convolutional
neural networks show better performance partially due to inherent structural regularization.
stω Tdol
Parameters (k)
Figure 2:	Test errors vs. total parameters trade-offs on the MNIST dataset using softmax normal-
ization and LLR representation: (left) LeNet-300-100 and (right) LeNet5
We found that the AdamW optimizer with weight decaying may increase training accuracy by
increasing overfitting and yield less efficient networks, as demonstrated in Figure 3. Compared
with previous results, the optimal pruned model sizes are dramatically increased and using
weight decaying does not improve the efficiency of trained networks.
-5
(X)」。击dol
---1---Softmax
—*— Softmax + WD
Q LLR + WD
1
25	30	35
0.8
0.75
Q LLR + WD
---1---SOftmax
—*— Softmax + WD
(£」。击'dol
0.45 ----------1---------1----------1----------1---------1----------1
65	70	75	80	85	90	95
Parameters (k)
40	45	50	55	60	65
Parameters (k)
Figure 3:	Test errors vs. total parameters trade-offs on the MNIST dataset using AdamW optimiza-
tion with (left) LeNet-300-100 and (right) LeNet5 architectures
3.3 CIFAR- 1 0 Classification
Several ResNet (He et al., 2016) and DenseNet (Huang et al., 2017) architectures are used for ex-
periments on the CIFAR-10 dataset. We compare ResNet with 20/32/56 layers to DenseNet with
40/60/100 layers and a growth rate k = 12. To reduce the total number of parameters, bottleneck
layers are enabled for DenseNet. Further comparison and analysis of the overfitting issues are
provided in Appendix C.2.
Figure 4 summarizes the experiment results on the CIFAR-10 dataset. A weight-decaying setting
of 1e-4 is used with softmax normalization, which is the default setting to obtain the best accu-
racy results without pruning. For comparison purpose, experiments for LLR representation
use a weight-decaying setting of 5e-4. Although using softmax yield the best training accuracy,
the trained networks are overparameterized compared with those using LLR representation.
Therefore, the trade-off curves can be used to judge the efficiency of trained networks. Curves
closer to the bottom-left region on the figure represent more efficient networks. Both ResNet
and DenseNet architectures show similar trends in terms of efficiency. The trade-off curves for the
same architecture with different initial model sizes seem to be bounded by a single theoretical
curve. In the energy efficient region with small number of parameters, the error rate goes down
rapidly with a small increase in the number of parameters; while in the high accuracy region, small
7
Under review as a conference paper at ICLR 2022
increases in accuracy require exponentially increasing numbers of parameters. In this regard, the
ResNet-32 and the DenseNet-60 architectures may offer better alternative trade-offs in efficiency.
Parameters (k)
Figure 4: Test errors vs. total parameters trade-offs on the CIFAR-10 dataset using softmax normal-
ization and LLR representation: (left) ResNet and (right) DenseNet
3.4 CIFAR- 1 00 Classification
Figure 5 summarizes the experiment results using different ResNet and DenseNet architectures on
the CIFAR-100 dataset. For all experiments using either softmax normalization or LLR repre-
sentation, the same weight-decaying settings are used. In terms of efficiency trade-offs, we see a
similar trend as before: linear increase in accuracy tends to require exponential increase in network
capacity. We notice that the difference between softmax normalization and LLR representa-
tion is less prominent for the DenseNet architecture. One possible reason is that the BSP loss
function is not yet fully optimized for the DenseNet architecture. Another possible reason is
that the effects of weight decaying are more prominent than softmax normalization for the
DenseNet architecture with larger initial model sizes.
Parameters (k)
Figure 5: Test errors vs. total parameters trade-offs on the CIFAR-100 dataset using softmax nor-
malization and LLR representation: (left) ResNet and (right) DenseNet
Parameters (k)
4	Related Work
Historically, deep neural networks using sigmoid or hyperbolic tangent activation functions were
difficult to train using backpropagation (Rumerlhar, 1986) due to the vanishing gradient problem
(Glorot & Bengio, 2010). The introduction of ReLU activation function (Nair & Hinton, 2010)
greatly improves training speed for deep learning, yielding improved prediction accuracy in many
new applications. However, using the ReLU activation function also tends to introduce overfitting
issues as shown in this paper.
Regularization using modified loss functions can alleviate overfitting but with limited effects. Data
augmentation is another method to reduce overfitting and improve generalization performance.
Dropout, i.e., randomly selected units are dropped during training, was introduced in Hinton et al.
(2012) and Srivastava et al. (2014) as an effective method to prevent overfitting. This idea was
8
Under review as a conference paper at ICLR 2022
extended to randomly dropping connections in Wan et al. (2013). Batch normalization is another
method to reduce overfitting and improve training speed. Nevertheless, it is not completely clear
how in principle these methods work, and they still can not fully eliminate overfitting issues in deep
neural networks.
Overfitting is also related to the size of a neural network. Excessively large networks tend to intro-
duce overfitting, and vice versa. It is also desirable to minimize model sizes for processing speed
and systematic scaling purposes. A straightforward way for compressing over-parameterized neural
networks is to prune trivial weights and retain only important connections, which is similar to the
development of mammalian brain (Rauschecker, 1984). Pruning unimportant weights and connec-
tions after training is a common way to obtain efficient neural networks. Early work in Hassibi &
Stork (1993); Hassibi et al. (1994); LeCun et al. (1989) uses the statistics from backpropagation to
trim trained networks.
Recently, Han et al. (2015) proposed an iterative pruning and re-training procedure for efficient
model compression. Similarly, pruning filters were proposed for convolutional networks in Li et al.
(2016). However, iterative pruning and re-training is generally difficult, requiring extra processing
time and resources. Furthermore, the iterative pruning process is opaque and requires try-and-error
in selecting pruning thresholds for parameters in different layers. The lottery ticket hypothesis from
Frankle & Carbin (2018) tries to explain why the iterative pruning procedure can work, but the
empirical results therein are not conclusive enough.
Alternatively, one-shot pruning techniques try to train sparse neural networks directly without iter-
ative operations (Lee et al., 2019; Zhang & Stadie, 2019; Wang et al., 2020). However, Liu et al.
(2019) observe that previous state-of-the-art pruning techniques may not provide better performance
compared with randomly initialized networks. Their observations could be partially explained using
our subnetwork analysis on structural regularization effects.
5	Discussion and Future Work
In this paper, we identify several important issues affecting overfitting in training deep neural net-
works. The key finding is that reducing overfitting is critical for obtaining efficient neural networks.
Itis demonstrated with several datasets and network architectures that a simple snapshot-based prun-
ing procedure can generate efficient deep neural networks. However, more empirical validation re-
sults using other neural network architectures and larger datasets are required to further validate the
proposed approach. Quantizing the parameters will further compress neural network models, which
is not considered here for brevity but could be a natural extension in future work.
The snapshot-based retrain method can also be useful in real-world applications, where we only
need to store pruned weights and connections, while biases and other optimization parameters can
be restored using new datasets. This could be a very important optimization in cloud and edge
computing applications. For transfer learning, neural networks trained with old datasets may be ef-
fectively retrained using new datasets, given that the underlying neural models are similar in nature.
We further analyze the efficiency trade-offs in training deep neural networks. For a given opti-
mization problem with given objective and dataset, we should consider structural information, in
additional to weights, as representation cost of trained networks. For the small-scale network ar-
chitectures used in this study, few extra parameters are needed to specify the network topology and
connections. However, for large-scale networks, the parameters for describing the network topology
and connections should also be included in the representation cost of models. When we compare
neural network performance, domain-specific knowledge for designing network architectures should
be considered as additional information. We hypothesize that there exist lower-bounds of total num-
ber of bits for representing parameters and connections with regard to given performance metrics
for an optimization problem. Rather than focusing on improving the sole accuracy metric with more
complex network architectures, we should also explore the trade-offs between accuracy and total
number of representation bits when comparing different network architectures and implementations.
Several hypotheses regarding training efficient deep neural networks are put forward and empirically
validated with experiments. Although rigorous proofs are not provided, we hope that they will
encourage further discussion and research efforts on the trade-offs between model performance and
complexity.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
The authors of this paper regard it critical to ensure all empirical results in this paper can be consis-
tently reproduced. For each experiment case with different parameters and optimization settings, the
results are generated with at least 10 runs with different random seed initialization. We also cross-
check our results with different references. Furthermore, for some of the experiments, we have
verified the results using several machine learning frameworks including PyTorch, TensorFlow, and
Matlab Deep Learning Toolbox. Finally, we will publish the source codes for this work on GitHub
and provide bug fixes and updates.
References
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256.JMLR Workshop and Conference Proceedings, 2010.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT press Cambridge, 2016.
Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu. A survey of deep learning
techniques for autonomous driving. Journal of Field Robotics, 37(3):362-386, 2020.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. Advances in Neural Information Processing Systems, 28, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. Morgan Kaufmann, 1993.
Babak Hassibi, David G Stork, Gregory Wolff, and Takahiro Watanabe. Optimal brain surgeon:
Extensions and performance comparison. Advances in Neural Information Processing Systems,
1994.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.
Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, and Lawrence D Jackel. Optimal
brain damage. In NIPs, volume 2, pp. 598-605. Citeseer, 1989.
10
Under review as a conference paper at ICLR 2022
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20(5):14, 2015.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learn-
ing Representations, 2019.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In International Conference on Learning Representations, 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.
Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422,
2016.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In Icml, 2010.
Maryam M Najafabadi, Flavio Villanustre, Taghi M Khoshgoftaar, Naeem Seliya, Randall Wald,
and Edin Muharemagic. Deep learning applications and challenges in big data analytics. Journal
of Big Data, 2(1):1-21, 2015.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
JP Rauschecker. Neuronal mechanisms of developmental plasticity in the cat’s visual system. Hu-
man neurobiology, 3(2):109-114, 1984.
DE Rumerlhar. Learning representation by back-propagating errors. Nature, 323:533-536, 1986.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning, pp. 1058-1066.
PMLR, 2013.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In International Conference on Learning Representations, 2020.
Matthew Shunshi Zhang and Bradly Stadie. One-shot pruning of recurrent neural networks by
jacobian spectrum evaluation. arXiv preprint arXiv:1912.00120, 2019.
11
Under review as a conference paper at ICLR 2022
A The softmax properties
A.1 Proof of the softmax property
Given two linearly correlated vectors Y0 = (y0,0,y0,1,…，y0,κ-1), Yi = (y1,0,y1,1, ∙ ∙ ∙ ,y1,κ-1)
of length K such that Y1 = Y0 + β1 , where β1 is a scalar, then each component of the softmax
normalization vector Q = softmax(Y0 + Y1) can be calculated as
=exp(y0,n + y1,n)
n Pk eχp(y0,k + yi,k)
_	exp(2y0,n + βι)
Pk exp(2y0,k + βι)
=exp(β1)exp(2y0,n)
exp(βι) PkeXp(2y0,k)
=exp(2y0,n)
Pk eχp(2y0,k)
If We add a third vectors Y2 = (y2,0, y2,1,…,y2,κ-1) SUch that Y2 = Y + β2, then each ComPo-
nent of the softmax normalization vector Q = softmax(Y0 + Y1 + Y2) can be calculated as
=	exp(2y0,n + βι + y2,n)
qn	PkeXp(2y0,k + βi + y2,k)
=	exp(3y0,n + βι + β2)
Pk exp(3y0,k + βi + β2)
=	exp(βι + β2)exp(3y0,n)
exp(βi + β2) Pk exp(3y0,k)
=exp(3y0,n)
Pk eχp(3y0,k)
Using generalization for Z = P0M-1 Ym, where Ym are linear offset versions of each other, sUch
that Y0 = Yi - βι = Y2 - β2 =…=YM-1 - Bm -i, We have
softmax(Z) = softmax(MY0) = softmax(MYm)
A.2 Proof of the generalized softmax property
Given two vectors Y0 = (y0,0,y0,1,…，y0,κ-1), Yi = (y1,0 ,y1,1, ∙ ∙ ∙ ,y1,κ-1) of length K SUCh
that Yi = Y + βι, where βι = (β1,0,β1,1, •…，β1,κ-1). Without any loss of generality, we
assume that Y0,0 ≤ Yq,i ≤ ∙∙∙ ≤ Y0,κ-i = Ymax. Define the maximal variation of βι as δι such
that ∣βι,k - βi,n | ≤ δi for any k,n ∈ Z and 0 ≤ k,n ≤ K — 1. If δi is insignificant relative to Yq,
i.e., that
eXp(δi) = o(eXp(Ymax)),	(5)
where o(∙) is the little-o notation, we define Yq, Yi as linearly semi-correlated vectors. Then, each
comPonent of the softmax normalization vector Q = softmax(Y0 + Yi) can be calculated as
=exp(y0,n + yi,n)
n	Pk eχp(yo,k + yi,k)
= exp(2yo,n + βi,n)
Pk exp(2yo,k + βi,k)
=_________exp(βi,n)exp(2y0,n)_________
exp(βi,n) Pk exp(2yo,k + βi,k - βi,n)
=_________exp(2yo,n)_________
Pk exp(2y0,k + βi,k - βi,n)
12
Under review as a conference paper at ICLR 2022
Note that the denominator of qn is mainly determined by the largest components of Y0 , and thus we
have the following approximation
≈	exp(2yo,n)
qn ~ Pk exp(2yo,k + δι)
≈	exp(2yo,n)
~ Pk eχp(2yo,k)
If We add a third linearly semi-correlated vectors Y2 = (y2,0, y2,1, ∙∙∙ , y2,κ-1) SUch that Y2 =
Y0 + β2, where β2 = (β2,0,β2,1,…，β2,κ-1) and ∣β2,k - β2,n∣ ≤ δ2 for any k,n ∈ Z and
0 ≤ k, n ≤ K - 1. Then each component of the softmax normalization vector Q = softmax(Y0 +
Y1 + Y2 ) can be calcUlated as
=	exp(2yo,n + βι,n + y2,n)
n	Pk exp(2yo,k + βι,k + y2,k)
=	exp(3yo,n + βι,n + β2,n)
Pk exp(3y0,k + β1,k + β2,k)
_________________exp(βι,n + β2,n) exp(3yo,n)____________
exp(β1,n + β2,n) PkeXP(3y0,k + 8l,k - β1,n + β2,k - β2,n)
≈	exp(3yo,n)
~ Pk exp(3yo,k + δι + δ2)
≈	exp(3yo,n)
~ Pk exP(3yo,k)
Using generalization for Z = P0M-1 Ym, where Ym are linearly semi-correlated of each other and
Y0 = Yi - βι = Y2 - β2 =…=YM-1 - Bm-1, We have
softmax(Z) ≈ softmax(MY0) ≈ softmax(MYm)
Note that the above relation holds as long as the variations in βm is insignificant according to (5),
while the magnitUdes of βm do not matter.
B Loss Functions for LLR representation
B.1 Bipolar SoftPlus Loss
Given a set of N neUral network oUtpUts {Yn} and corresponding targets {Tn}, the bipolar softplUs
(BSP) loss is defined as
1 N-1
BSP(Y, T) = βN E log(1 + e-βsgn(Tn)Yn)	⑹
βN n=0
where β is a constant and sgn(x) retUrns the sign ofx as
(1, x > 0
sgn(x) = 0, x = 0	(7)
[-1, x < 0
C Experiment Settings and Detailed Results
C.1 MNIST Classification
The MNIST dataset of handwritten digits has a training set of 60,000 examples and a test set of
10,000 examples. Each image contains 28 × 28 monochrome pixels for one digit. The pixel valUes
are converted to range (0, 1) with dataset normalization.
13
Under review as a conference paper at ICLR 2022
Two architectures are used in the experiments: 1) the LeNet-300-100 is a three-layer fully connected
network with 300 and 100 hidden nodes, 2) the LeNet-5 architecture has two convolutional layers
with 20 and 50 filters and two fully connected layers with 800 and 500 hidden nodes.
Data augmentation is used to randomly shift each image horizontally and vertically by 0 or 1 pixel.
The batch size for training is set to 128, and the Adam optimizer (Kingma & Ba, 2014) is used with
default parameters: α = 0.001, β1 = 0.9, β2 = 0.999,and ε = 10-8. A weight decaying setting of
4e-4 is used for both the LeNet-300-100 and LeNet-5 architectures in corresponding cases. At least
20 runs with random seeds are carried out for each experiment case.
Table 1 summarizes the top-1 error rates, total number of parameters after pruning, and compres-
sion rate for different methods with the LetNet-300-100 architecture. The output layer uses a fixed
pruning setting of 0.75, and the hidden layers use pruning settings θk = 1.0 + 0.05 × k, where
k = 0,1,…,9. The total numbers of pruned parameters and compression ratio in Table 1 are
obtained using the largest pruning threshold.
Compared with LLR representation, results using softmax normalization have higher training errors
and test errors. This indicates that LLR representation can mitigate the overfitting issues and improve
accuracy in both training and testing. Figure 6 (left) also compares the effects of overfitting between
softmax normalization and LLR representation with the LetNet-300-100 architecture. Using both
LLR representation and weight decaying can yield more efficient networks than the iterative method
from Han et al. (2015).
Table 1: Comparing MNIST performance and weight pruning for LeNet-300-100
Methods	Top-1 Error (%)			Parameters	Compression
	Training	Test	Pruned		
Softmax	0.70 ± 0.04	1.36 ± 0.10	1.48 ± 0.10	62.8 ± 0.5K	4.2×
LLR	0.64 ± 0.03	1.22 ± 0.08	1.47 ± 0.10	57.4 ± 0.6K	4.6×
Softmax+WD	1.65 ± 0.05	1.48 ± 0.12	1.51 ± 0.09	20.3 ± 0.4K	13.1×
LLR+WD	1.37 ± 0.04	1.30 ± 0.12	1.51 ± 0.08	19.3 ± 0.3K	13.8×
Iterative Pruning		1.6	1.6	22K	12×
Epoch
Figure 6: Training and test errors versus epochs using Softmax normalization and LLR representa-
tion: (left) LetNet-300-100 and (right) LeNet-5
Epoch
Table 2 summarizes the top-1 error rates, total number of parameters after pruning, and compression
rate for different methods with the LetNet-5 architecture. Fully-connected layers use a fixed pruning
setting of 1.25. For convolutional layers, the pruning settings are set as θk = 0.5+0.1×k, where k =
0,1,…，9. The total numbers of pruned parameters and compression ratio in Table 2 are obtained
using the largest pruning threshold. Weight sharing and inherent structural regularization of CNN
further mitigate the overfitting issues in training. Using LLR representation and weight decaying,
the accuracy of the pruned network is even better than the accuracy in training and testing. The
snapshot-based method generates efficient networks with 31K parameters and the state-of-the-art
14
Under review as a conference paper at ICLR 2022
performance, better than the ones using the iterative method from Han et al. (2015). Figure 6 (right)
also compares the effects of overfitting between softmax normalization and LLR representation.
Table 2: Comparing MNIST performance and weight pruning for LeNet-5
Methods	Top-1 Error (%)			Parameters	Compression
	Training	Test	Pruned		
Softmax	0.19 ± 0.02	0.63 ± 0.09	0.74 ± 0.07	82.5 ± 1.0K	5.2×
LLR	0.14 ± 0.02	0.53 ± 0.07	0.66 ± 0.06	70.0 ± 1.9K	6.2×
Softmax+WD	0.65 ± 0.03	0.69 ± 0.10	0.63 ± 0.05	33.5 ± 1.2K	12.9×
LLR+WD	0.67 ± 0.02	0.64 ± 0.10	0.60 ± 0.06	31.0 ± 0.9K	13.9×
Iterative Pruning		0.8	0.8	36K	12×
For comparison purpose, the results using the AdamW algorithm from Loshchilov & Hutter (2018)
are summarized in Table 3 and 4 for the LeNet-300-100 and LetNet-5 architectures, respectively.
The accuracy differences between training and testing are always larger than previous results using
weight decaying and the original ADAM algorithm. The results show that using the AdamW algo-
rithm may generate overparameterized networks. Thus, the snapshot-based pruning method can be
a valuable tool for evaluating optimization algorithms.
Table 3: Comparing MNIST performance using AdamW Optimizer for LeNet-300-100
Methods	Top-1 Error (%)			Parameters	Compression
	Training	Test	Pruned		
Softmax	0.71 ± 0.05	1.31 ± 0.08	1.47 ± 0.10	62.8 ± 0.3K	4.2×
LLR	0.65 ± 0.03	1.19 ± 0.08	1.44 ± 0.10	57.4 ± 0.9K	4.6×
Softmax+WD	0.70 ± 0.04	1.31 ± 0.12	1.41 ± 0.11	62.8 ± 0.4K	4.2×
LLR+WD	0.62 ± 0.03	1.22 ± 0.08	1.42 ± 0.07	57.7 ± 0.5K	4.6×
Table 4: Comparing MNIST performance using AdamW Optimizer for LeNet-5
Methods	Top-1 Error (%)			Parameters	Compression
	Training	Test	Pruned		
Softmax	0.18 ± 0.03	0.67 ± 0.08	0.55 ± 0.07	91.6 ± 1.0K	4.7×
LLR	0.15 ± 0.02	0.56 ± 0.07	0.50 ± 0.05	80.3 ± 3.1K	5.4×
Softmax+WD	0.18 ± 0.02	0.67 ± 0.09	0.55 ± 0.05	93.7 ± 0.9K	4.6×
LLR+WD	0.14 ± 0.02	0.57 ± 0.06	0.50 ± 0.04	78.1 ± 2.3K	5.5×
C.2 CIFAR- 1 0 Classification
The CIFAR-10 datasets (Krizhevsky et al., 2009) consists of 50,000 images in the training set and
10,000 images in the test set. Each sample contains 32 × 32 color images drawn from 10 classes.
The data batch size of 128 is used for training. For data augmentation, the standard mirroring and
shifting scheme is used. The SGD optimizer is used with initial learning rate of 0.05 and momentum
of 0.9, the learning rate is multiplied by 0.1 after 100 and 150 epochs. Weight decaying settings of
6e-4 and 5e-4 are used for the ResNet and DenseNet architectures, respectively. Initial training
runs use 200 epochs, while for non-weight parameter adjustment after pruning only 20 epochs are
needed. At least 10 runs with random seeds are carried out for each experiment case.
The top-1 error rates, total number of parameters after pruning, and compression ratio for CIFAR-10
dataset with the ResNet architectures are summarized in Table 5. Fully-connected layers use a fixed
pruning setting of 0.75. For convolutional layers, the pruning settings are set as θk = 0.5 + 0.05 × k,
where k = 0,1,…，9. The total numbers of pruned parameters and compression ratio in Table 5 are
15
Under review as a conference paper at ICLR 2022
obtained using the largest pruning threshold. For all cases, using LLR representation yields better
performance and less parameters after pruning. For the ResNet-56 case, using LLR representation
with weight decaying reduces the total number of parameters to about 200K without significant loss
of performance.
The results in Table 6 show better performance for DenseNet architectures as compared with the
ResNet architectures. Fully-connected layers use a fixed pruning setting of 0.75. For convolutional
layers, the pruning settings are set as θk = 0.5 + 0.05 X k, where k = 0,1,…,9. The total
numbers of pruned parameters and compression ratio in Table 6 are obtained using the largest
pruning threshold.
For DenseNet-60 with less than 90K parameters, the performance is comparable to ResNet-56 with
200K parameters. Therefore, overfitting issues with the DenseNet architecture are less prominent
than the ResNet architecture. Figure 7 summarize the efficiency trade-offs for both ResNet and
DenseNet architectures. Compared with ResNet architecture, the initial DenseNet model sizes are
larger, the effects of weight decaying are more prominent than the softmax normalization, and the
difference between softmax normalization and LLR representation for DenseNet is smaller.
Table 5: Comparing CIFAR-10 performance and weight pruning for ResNet-20/32/56
Architecture	Methods	Top-1 Error (%)			Parameters	Comp Ratio
		Training	Test	Pruned		
ResNet-20	Softmax	0.53 ± 0.03	7.58 ± 0.19	10.0 ± 0.27	77.8 ± 0.3K	3.5×
	LLR	1.45 ± 0.07	7.9 ± 0.12	10.5 ± 0.32	69.6 ± 0.5K	3.9×
ResNet-32	Softmax	0.17 ± 0.03	6.77 ± 0.19	8.32 ± 0.24	130.7 ± 1.0K	3.6×
	LLR	0.50 ± 0.03	6.90 ± 0.17	8.62 ± 0.17	118.2 ± 1.0K	4.0×
ResNet-56	Softmax	0.07 ± 0.01	6.05 ± 0.19	7.08 ± 0.32	232.8 ± 1.0K	3.7×
	LLR	0.17 ± 0.02	6.03 ± 0.16	7.06 ± 0.15	213.3 ± 1.7K	4.0×
Table 6: Comparing CIFAR-10 performance and weight pruning for DenseNet-40/60/100
Architecture	Methods	Top-1 Error (%)			Parameters	Comp. Ratio
		Training	Test	Pruned		
DenseNet-	Softmax	0.26 ± 0.04	7.01 ± 0.31	9.19 ± 0.26	49.7 ± 0.3K	3.6×
40	LLR	0.50 ± 0.05	7.14 ± 0.19	9.34 ± 0.38	49.6 ± 0.1K	3.7×
DenseNet-	Softmax	0.12 ± 0.03	5.87 ± 0.10	7.10 ± 0.26	87.5 ± 0.6K	3.7×
60	LLR	0.11 ± 0.02	6.00 ± 0.15	7.34 ± 0.23	87.3 ± 0.5K	3.7×
DenseNet-	Softmax	0.03 ± 0.01	4.96 ± 0.17	5.61 ± 0.12	198.1 ± 0.7K	4.0×
100	LLR	0.03 ± 0.01	4.93 ± 0.22	5.55 ± 0.17	198.5 ± 2.1K	4.0×
C.3 CIFAR- 1 00 Classification
The CIFAR-100 datasets (Krizhevsky et al., 2009) consists of 50,000 images in the training set and
10,000 images in the test set. Each sample contains 32 × 32 color images drawn from 100 classes.
The 100 classes are grouped into 20 superclasses. The data batch size of 128 is used for training.
For data augmentation, the standard mirroring and shifting scheme is used. The SGD optimizer is
used with initial learning rate of 0.05 and momentum of 0.9, the learning rate is multiplied by 0.1 af-
ter 100 and 150 epochs. A weight decaying setting of 5e-4 is used for both the ResNet and DenseNet
architectures. Initial training runs use 200 epochs, while for non-weight parameter adjustment after
pruning only 20 epochs are needed. At least 20 runs with random seeds are carried out for each
experiment case.
The top-1 error rates for trained and pruned networks, total number of parameters after pruning,
and compression ratio for CIFAR-100 dataset are summarized in Table 7 and 8. Fully-connected
16
Under review as a conference paper at ICLR 2022
7
6.5
(求)」0=山∙dol
100	150	200	250	300	350	400	450
Parameters (k)
6
5.5
- 5 7 L
(求)」0=山do1
—I—DenSeNet40 - SoftmaX
O DenSeNet40 -LLR
*	DenSeNet60 - Softmax
―►—DenseNet60 - LLR
■ DenSeNet100 - SoftmaX
Q DenSeNet100 - LLR
50	100	150	200	250	300	350	400
Parameters (k)
Figure 7: Test errors vs. total parameters trade-offs on the CIFAR-10 dataset using Softmax normal-
ization and LLR representation: (left) ResNet and (right) DenseNet
layers use a fixed pruning setting of 0.75. For convolutional layers, the pruning settings are set
as θk = 0.5 + 0.05 X k, where k = 0,1,…,9. The total numbers of pruned parameters and
compression ratio in Table 7 and 8 are obtained using the median pruning threshold.
Table 7: Comparing CIFAR-100 performance and weight pruning for ResNet-20/32/56
Architecture	Methods	Top-1 Error (%)			Parameters	Comp Ratio
		Training	Test	Pruned		
ResNet-20	Softmax	13.5 ± 0.1	33.0 ± 0.4	38.2 ± 0.4	120.0 ± 0.6K	2.3×
	LLR	24.8 ± 0.3	35.7 ± 0.3	39.9 ± 0.7	109.7 ± 0.5K	2.5×
ResNet-32	Softmax	5.8 ± 0.2	31.3 ± 0.3	34.8 ± 0.5	207.0 ± 0.4K	2.3×
	LLR	15.1 ± 0.3	32.4 ± 0.3	34.0 ± 0.6	189.3 ± 0.8K	2.5×
ResNet-56	Softmax	1.4 ± 0.1	28.7 ± 0.3	30.9 ± 0.3	377.5 ± 1.2K	2.3×
	LLR	6.8 ± 0.2	30.2 ± 0.4	30.0 ± 0.3	346.6 ± 2.5K	2.5×
Table 8: Comparing CIFAR-100 performance and weight pruning for DenseNet-40/60/100
Architecture	Methods	Top-1 Error (%)			Parameters	Comp. Ratio
		Training	Test	Pruned		
DenseNet-	Softmax	9.3 ± 0.4	29.9 ± 0.2	34.0 ± 0.6	73.7 ± 0.2K	2.6×
40	LLR	12.7 ± 0.2	30.6 ± 0.5	34.8 ± 0.4	76.2 ± 0.2K	2.5×
DenseNet-	Softmax	2.0 ± 0.1	27.2 ± 0.4	29.2 ± 1.2	129.5 ± 1.7K	2.6×
60	LLR	4.2 ± 0.2	27.8 ± 0.4	29.8 ± 1.1	134.3 ± 1.3K	2.5×
DenseNet-	Softmax	0.1 ± 0.0	23.4 ± 0.6	24.0 ± 0.5	297.5 ± 3.3K	2.8×
100	LLR	0.3 ± 0.1	24.0 ± 0.1	23.8 ± 0.3	308.4 ± 1.7K	2.7×
17