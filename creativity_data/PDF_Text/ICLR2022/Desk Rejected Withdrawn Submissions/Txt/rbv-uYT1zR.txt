Under review as a conference paper at ICLR 2022
Coherence-Based Document Clustering
Anonymous authors
Paper under double-blind review
Ab stract
Latent Dirichlet Allocation or Non-negative Matrix Factorization are just two
widely used algorithms for extracting latent topics from large text corpora. While
these algorithms differ in their modeling approach, they have in common that hy-
perparameter optimization is difficult and is mainly achieved by maximizing the
extracted topic coherence scores via grid search. Models using word-document
embeddings can automatically detect the number of latent topics, but tend to have
problems with smaller datasets and often require pre-trained embedding layers for
successful topic extraction. We leverage widely used coherence scores by integrat-
ing them into a novel document-level clustering approach using keyword extrac-
tion methods. The metric by which most topic extraction methods optimize their
hyperparameters is thus optimized during clustering, resulting in ultra-coherent
clusters. Moreover, unlike traditional methods, the number of extracted topics or
clusters does not need to be determined in advance, saving an additional optimiza-
tion step and a time- and computationally-intensive grid search. Additionally, the
number of topics is detected much more accurately than by models leveraging
word-document embeddings.
1	Introduction
Unsupervised document clustering for extracting latent topics in large collections of documents has
gained increasing importance with the ever growing availability of textual data. Prominent algo-
rithms such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) or Latent Semantic Analysis
(LSA) (Landauer et al., 1998) are specifically designed to extract topics from documents. Non-
negative Matrix Factorization (NMF)(Lee & Seung, 1999; Xu et al., 2003) or other general cluster-
ing algorithms (e.g. K-means) on the other hand are clustering algorithms that are also applicable
to document topic extraction. While all of the mentioned document clustering algorithms can pro-
duce reliable results, the underlying assumptions differ quite strongly. LDA (Blei et al., 2003) for
example assumes that documents are mixtures of latent topics whereas the Gibbs Sampler Dirichlet
Multinomial Model (GSDMM) and the Gamma Poisson Mixture Model (GPM) assume that each
document is determined from a single topic (Mazarura & De Waal, 2016).
However, all the above methods have in common that the number of extracted topics must be pre-
specified manually. The ideal number of topics is often optimised either via human assessment of
the created topics or via a grid search by maximizing the extracted topics coherence scores (Syed
& Spruit, 2017; Newman et al., 2010; Mimno et al., 2011) with respect to the algorithms hyper-
parameters, e.g. the number of extracted topics or distributional parameters as in the LDA (Blei
et al., 2003; Wallach et al., 2009). Other measures such as likelihood-based perplexity metrics (if
suited for the chosen clustering algorithm) are negatively correlated with measures that are based on
human evaluation (Chang et al., 2009) and are thus not well suited for optimising hyperparameters.
Other, non-probabilistic methods that do not need a pre-specification of the number of extracted
topics make frequent use of word- and document embeddings (Angelov, 2020; Grootendorst, 2020a)
arguing that a dense area of documents can be interpreted as these documents covering the same
topic. The number of these dense areas, thus, automatically determines the number of extracted
topics. While these algorithms tend to perform very well on extremely large data sets, pre-trained
embeddings can be used when analysing smaller data sets. However, in our applications we find
severe overestimation of the number of topics and a lack of accurate recognition of all topics present,
even with pre-trained embeddings for medium sized data sets (<5.000 documents).
1
Under review as a conference paper at ICLR 2022
In this paper, we reverse a commonly used hyperparmeter optimization process and use coherence
scores for document clustering. We show that by subsidizing simple keyword extraction methods
and clustering documents based on their respective coherence, both, probabilistic as well as semantic
embedding based topic extraction methods are outperformed for medium sized data sets. Coherent
clusters are formed and appropriate numbers of topics are found without pre-specification. Addi-
tionally, we find that simpler keyword-extraction methods outperform transformer based keyword
extraction methods as introduced by Grootendorst (2020b).
The remainder of the paper is structured as follows: Section 2 outlines our proposed methodology.
First, coherence scores are presented in Section 2.1. Second, the extraction of a documents most
defining words is described in section 2.2. Section 2.3 outlines our proposed clustering algorithm. In
Section 3, applications of our proposed algorithm are presented and bench-marked with conventional
methods. Section 4 provides a conclusion.
2	Methodology
Let V = {w1, . . . , wn} be the vocabulary of words and D = {d1, . . . , dM} be a corpus,
i.e. a collection of documents. Each document is then assumed to be a sequence of words
di = [wi1, . . . , wini] where wij ∈ V and ni denotes the length of document di. There are dif-
ferent interpretations of topics, but mostly a topic tk from a set of topics T = {t1 , . . . , tK} is
interpreted to be a distribution over words (Blei et al., 2003; Mazarura & De Waal, 2016). LDA, as
well as NMF additionally assume that documents are random mixtures over latent topics.
Unlike LDA, we do not assume that documents are mixtures of topics. Rather, we argue that a
document itself best describes a single topic. That is, a document deals with only a single topic,
while a single topic can be covered by multiple documents. A document that covers different content,
by combining these topics into a single document, ensures that it is indeed a single topic with
different sub-contents. We follow this reasoning because the very existence of a document that
addresses multiple topics makes it clear that these multiple topics are closely related by their common
appearance in a single document. We thus take a clustering approach and assume a cluster to be
a collection of documents or even a single document covering similar contents and make use of
LDA’s most used hyperparameter optimization metric, the coherence score. Subsequently, a topic
is merely a set of words best describing the documents of a cluster. A cluster is thus best described
as a set of documents and hence as a set of all words contained in the documents, γk = {d1 =
[w11, . . . ,w1n1] , d2 = [w21, . . . ,w2n2] , . . . , dM = [wM1, . . . ,wMnM]}. Note the difference
between a cluster, γk, being a set of documents and a topic, tk, being a distribution over words.
The order of the documents or words is not important and the words describing the cluster best, can
be found with e.g. term frequency-inverse document frequency (TF-IDF) document representations
(Salton & Buckley, 1988) or other keyword extraction methods and thus represent the clusters topic
tk .
2.1	Coherence Scores
Coherence scores are often used to evaluate the quality of artificially created topics from automated
topic modelling. A generated topic is said to be better when the topic is more coherent. Coherence,
the property of being logical and consistent, is defined in terms of topics as a quality measure that
evaluates a topic by the extent to which it consists of words that frequently occur together in the
observed documents. A topic, consisting of words or phrases that do not often co-occur within
documents1 would thus be evaluated as a bad topic. Interpreting a topic merely as a set of words, a
common coherence measure for a topic tk is given below:
Jmax j-1
C(tk) =	log
j=2 i=1
P(Wj , Wi∣tk ) + E
p(wi∣tk)
(1)
where p(wj, wi|tk) denotes the probability that the words wj and wi co-occur within a document,
calculated as the number of documents containing both words divided by the total number of docu-
1e.g. “Deep Learning” and “World War 2”, or “Nuclear Power Plant” and “Tennis”
2
Under review as a conference paper at ICLR 2022
ments for a given topic tk. p(wi|tk) denotes the probability that word wi occurs in a document and
is hence calculated as the number of documents containing word wi divided by the total number of
documents. E is a regularisation parameter to ensure that C(tk) ∈ R and is often specified as M. For
topic model evaluation these probabilities are dependent on the topic tk, i.e. p(wj, wi|tk),p(wi|tk),
with Jmax being set to a fixed number that indicates how many of the topics most probable words
are taken into account.
A coherence measure can thus be adapted to the document-level, by simply replacing the sum that
is taken over the words contained in a topic by the words contained in that document. A documents
coherence is hence evaluated by analyzing the co-occurence of the words contained in that single
document, in comparison to the co-occurence of these words in all other documents.
ns j-1
C(ds)=XXlog
j=2 i=1
P(Wj ,Wi) + E
P(Wi)
(2)
for Wj 6= Wi , with ns being the length of the document. If we adapt the coherence score metric to
single documents, the probabilities are hence no longer dependent on a topic tk. Since documents
may contain a single word more than once, the score is restricted for the case where Wi = Wj .
A coherence measure between two documents, d1 and d2 or more precisely a coherence score for
document d1, given document d2 , can thus be calculated as
n1 n2
C(d1|d2) = XXlog
j=1 i=1
P(Wj, Wi) + E
P(Wi)
(3)
with P(Wj) Wi) = 1 if Wj = Wi. We found that using 京 as E works best. Hence, n1 denotes the
length (number of words) of document d1 and n2 denotes the length of document d2 . Since Wj and
Wi are words from two different documents, we set P(Wj, Wi) = 1 for the case where the words are
identical, since documents that have exactly the same wording are obviously very coherent. Note,
that the notation C(d1|d2) implies a coherence measure for document d1 given document d2. Thus,
computing the coherence scores for all document combinations in a corpus and storing them in a
matrix, would lead to a M × M coherence score matrix, Φ, where the entry Φ1,2 = C(d1|d2) would
be the coherence score for document 1 given document 2.
Note, however, that it can be seen in formula (3) that the coherence of d1 given d2 is not necessarily
the same as the coherence of d2 given d1, C(d1|d2) 6= C(d2|d1) since
JI
XXlog
P(Wj ,Wi)+ E )必 X XX l (P(Wi)Wj ) + E ∖
Fw^ 一产 i=1 j=1 o队 P(Wj) J
(4)
with P(Wj , Wi ) = 1 if Wj = Wi . Thus, Φ is not symmetrical.
2.2	Most defining words
The lengths of the documents have a big impact on the coherence score for the documents, as we
take the sums over all words of both documents. Longer documents would thus lead to larger
coherence scores. To circumvent this, we adapt the formula by not summing over all word-to-word
combinations in both documents, but similar to the topic coherence by summing over the documents
k most defining words. The documents most defining words could easily be extracted by using e.g.
the TF-IDF document representation (Salton & Buckley, 1988), formally denoted by:
k
tf-idf(W) = frequency(w) ∙ [log ——J + 1],
(5)
where k is the total number of words in the dictionary, K(w) is the total number of documents
wherein the word appears and f requency(W) is self explanatory the words frequency in the docu-
ment. The words with the largest TF-IDF scores can thus be interpreted as the most defining words
of a document. We compute the representations with Scikit-learns (Pedregosa et al., 2011) built-in
3
Under review as a conference paper at ICLR 2022
TF-IDF vectorizer. The two sums in formula (3) thus are only built upon a pre-specified number
of keywords, e.g. 20. Note, however, that the probabilities p(wj, wi), p(wi) are still taken over all
words in all documents.
As a second keyword extraction method we use Transformer based document embeddings (Vaswani
et al., 2017). As described by Grootendorst (2020b), we subsidize a pre-trained BERT Model (De-
vlin et al., 2018; Reimers & Gurevych, 2019) to create these document embeddings. The keywords
are extracted by computing the cosine similarity between the single word vectors and the complete
document vector. The similarity between word w and document d would thus be
Ssmm= ∣∣ww×dd∣∣,
(6)
where
nn
W X d = ɪ2 Wi ∙ di
i=1
and
i=1
∖
||W|| × ||d||
nn denotes the vectors dimension in the feature space and is identical for d and W. As Grootendorst
(2020b) points out, the words best representing the document would have a smaller distance to the
document in the embedding space.
2.3	Clustering
To create coherent clusters from a corpus, we make use of the documents coherence scores (3)
following a simple clustering rule: First, we assume each document to be an independent cluster,
where document di is denoted as cluster γi . Clusters are being combined into larger clusters, by
combining clusters that are the most coherent to one another being clustered together. As such, if
C(γι |γ2) is the maximum coherence score for γι given all other possible γ's, the clusters γι and Y
would simply be combined in the form of:
γk = γ1 ∪ γ2 .	(7)
γk denotes here a newly formed cluster that is the union of the clusters γ1 and γ2 . A cluster is sub-
sequently a combination of documents and can be interpreted in the same way as a single document.
γk can consequently be expressed as γk = {γ1, γ2}.
As described in formula (4), C(γ1∣γ2) = C(γ2∣γ1). Hence, it could happen that e.g. γι has a
maximum coherence score given γ2, but γ2 has a largest coherence score given γ3 or even multiple
maximum coherence scores given multiple different clusters. If supposedly C(γ2 ∣γ3) > C(γ2 ∣γι),
the formed cluster γk would be of the form:
γk = γ1 ∪ γ2 ∪ γ3 .
For this cluster to be complete, it would require that no other γi has a maximum coherence score
given any cluster included in γk and γ3 needs to have a maximum coherence score given either γ1 or
γ2 . We call this asymmetrical appearance of maximum values the clusters chain size, δ. It becomes
clear that a clusters chain size, δ, is only smaller than the total number of clusters, if at any point in
the chain, a cluster γi has a maximum coherence score given a cluster further back in the chain, γj .
Figure 1: Cluster building for a given maximum chain size ν
4
Under review as a conference paper at ICLR 2022
Thus, we form clusters based on these maximum coherence value chains, were we limit the cluster
size to an arbitrary number, ν. Only clusters are being clustered together, when δ ≤ ν . A limit to this
chain size is necessary, as otherwise a cluster that shares no words at all with all other clusters and
thus has the same coherence scores given all other clusters could end up forming a super cluster. By
subsidizing the coherence scores, we make inherently sure that only coherent clusters are formed.
Thus, even single documents could end up forming a complete cluster when they do not share any
words with all other clusters. Additionally, no pre-specification of topic distributions (Wallach et al.,
2009) is necessary and the cluster sizes are detected automatically.
As we form our document clusters in an iterative manner, it is not required to pre-specify an exact
number of desired output clusters. However, a pre-specification of a maximum number of formed
clusters is advised, as the clustering rule stops clustering, when this number, η, is reached. Hence,
pre-specifying an η of e.g. 20 could lead to any number of clusters between 1 and 19, depending
on the coherence between the clusters. If this number is not specified, we would either end up with
a number of clusters larger than ν, when the clusters all have the same coherence score towards
another, or form very few or even a single cluster. Thus, in contrast to LDA or NMF, a researcher
does not need to either know the exact number of topics in advance or iterate over every possible
number of topics, maximizing after coherence scores.
The presented clustering rule and subsequent topic extraction is used as in the form below:
1.	initialize ν as the maximum chain size
initialize η as the maximum number of extracted clusters
initialize each document di as a cluster γi
2.	While number of clusters > η
•	Compute most defining words for clusters
•	Compute coherence score matrix Φ
•	Combine clusters, γa, γb where Φγa,γb ≥ Φγa,γj for all γj 6= γb and δ ≤ ν
3.	Compute the resulting clusters most defining words
3	Application and Comparison
For testing we use three different, independent, labelled data sets and compare the presented cluster-
ing approach with two of the most frequently used topic modelling approaches, the LDA (Blei et al.,
2003) and the NMF (Xu et al., 2003; Lee & Seung, 1999) as well as two state of the art document em-
bedding leveraging topic models, BERTopic (Grootendorst, 2020a) and Top2Vec (Angelov, 2020).
For The LDA as well as the NMF we use the gensim implementation (Rehurek & Sojka, 2011)
and optimize the number of extracted topics / clusters with the U-mass coherence score, iterating
over a sensible range of topics. For text preprocessing we follow the usual techniques and remove
stopwords, numbers and symbols and lemmatize the documents. For both algorithms, the LDA and
NMF we use the bag-of-words feature representations as the simplest and fastest form of feature
extraction. For BERTopic and Top2Vec, no preprocessing is required as is no pre-specification of
hyperparameters. For even more accurate testing we use both, pre-trained embedding layers2 as well
as training from scratch. For the presented clustering rule we set the number of extracted keywords
to 20.
The three used data sets as well as their topic prevalences can be seen in Table 1.
2universal-sentence-encoder (Cer et al., 2018) for Top2Vec and paraphrase-MiniLM-L6-v2 (Reimers &
Gurevych, 2019) for BERTopic
5
Under review as a conference paper at ICLR 2022
Table 1: Used Data Sets and their Topic Distributions
Prevalence			
Topics	Data Set A	Data Set B	Data Set C
Business	0.23		0.1
Politics	0.19		0.1
Entertainment	0.17		0.1
Sports	0.23		0.1
Technology	0.18		0.1
Graphics			0.1
Food			0.1
Space			0.1
Medicine			0.1
History		0.16	0.1
Physics		0.25	
Computer Science		0.20	
Biology		0.20	
Maths		0.07	
Geography		0.03	
Accounts		0.09	
All algorithms are tested on all three data sets and either the optimal number of topics after com-
puting coherence scores or the respective algorithms optimal solution is analysed. We find that both
NMF and LDA are close to the true number of topics for all three data sets but create completely
meaningless topics. While BERTopic and Top2Vec create very meaningful topics, they severely
overestimate the number of true topics in nearly all cases. Only for a perfectly balanced data set
BERTopic and Top2Vec accurately extract the true number as well as the true meaning of the topics.
The presented clustering rule, abbreviated as CBC in the following, very accurately detects the true
number of topics, especially for TF-IDF keywords, generates meaningful and human-interpretable
clusters, and is the only algorithm that can handle severe imbalancedness in a data set. The true
number of topics as well as the algorithms extracted number of topics can be seen in Table 2.
Table 2: Data Set Comparison For All Used Methods. The presented method is abbreviated to CBC.
For LDA and NMF Coherence scores are used for finding the optimal number of topics.
Original number of topics	Data Set A 5	Data Set B 7	Data Set C 10
LDA	2	9	3
NMF	2	5	7
Top2Vec	29	22	13
BERTopic	44	45	15
Top2Vec pre-trained	10	19	13
BERTopic pre-trained	40	45	15
CBC - TFIDF	5	8	9
CBC - BERT Keywords	4	7	14
3.1	DATA SET A: BBC DATA
First, a BBC data set covering five topics, namely business, entertainment, politics, sports and tech-
nology is analyzed. The categories are fairly balanced across the data set which is comprised of
2.225 documents. We initialize ν as 15 and η as 80. The TF-IDF based rule3 extracts 5 clusters
as can be seen in Figure 2 . The four topics business, politics, technology and sports are clearly
identified, whereas entertainment, the least prevalent category, is merged within the sports cluster.
3See appendix for the results using the BERT keywords (6)
6
Under review as a conference paper at ICLR 2022
Interesting is also the distinction between Russian and American business as seen in the wordclouds
#1 and #2.
reP°rt bank-eʃ .≡ιshare
慧 USia
companyf
rise lmarke√
one∩ew Iabourwork
government
Figure 2: Cluster extraction for data set A, covering 5 topics
sports, tech. ν = 15, η= 80. 5 clusters are extracted.
: business, entertainment, politics,
In Comparison, both the NMF and LDA have the largest coherence score for a number of two topics.
Whereas the LDA uncovers the topics sport and politics, the NMF generates two nearly identical
topics (see Figure 3).
game 邛
Oget—,
R导9暹盍fflæ良:此gC)
(W≡≡WS
, s 州VoUick
a≡M
(a)	Extracted Topics from
LDA
""'.could “用.Ig祁号 τ -
Ψθtui⅛^
sa⅛⅛r
嗔疆拴y部露勰ajso.
minimuiŋvag^K澈二臂芯ŋ RU 哂
Mwould +；二
sɪ, ∙," β ∏'^p.r gaιFe.liket im6
saιd^θ^⅛r
gover nmen tpeople
(b)	Extracted Topics from
NMF
Figure 3:	2 Extracted Topics from LDA and NMF for data set A
BERTopic as well Top2Vec overestimate the number of true topics severely, whereas Top2Vec with
pre-trained embeddings is closest to the truth with 10 extracted topics (see wordclouds in the ap-
pendix, (7)). However, Top2Vec fails completely to recognize a humanly interpretable topic of busi-
ness which is the most prevalent topic in the data set. Instead, several very similar topics describing
sports and technology are extracted.
3.2 DATA SETB
Second, we analyze a data set covering 7 topics, namely computer science, physics, math, biology,
geography, history and accounting. The categories prevalence in this data set is fairly unbalanced
with the least prevalent category, geography only making up 3% of the total amount of documents
(3.035).
We initialize the maximum chain size ν as 15 and the maximum number of clusters η as 80. The
clustering rule clusters the documents to 8 clusters for the TF-IDF keyword extraction. The words
best describing the clusters and thus representing the topics can be seen in Figure 4. CBC subsi-
dizing BERT keywords extracts very accurately 7 topics, but fails to identify all present topics (see
Appendix, Figure (10)).
7
Under review as a conference paper at ICLR 2022
p5噫超9塞诧维is七苍1⅛
ByJ S θ data
PPOgrarnQ ∩ θ⅛d∩memory
tWO e ⅞O1f
instructιdn
“".'-disk ∙orτ,τιc r∙ need
enterprise*吃n 冷 W
u naneɪng activities-- I OW Stat⅞mgnti"t∙!∙*>' '.~~"∙ S
cash equivalent! dividend ιnclude
GaSlT ow
cash- OWrIVh
照就就⅛∙期琼(蓝韭⅛⅛M⅛窃由Λ
SnarG capital ,,"
application money"-γr,∙,∣ <∙p⅛<jλissue debenture ≈
company
acco晶吧那	,
equity share
'=⅛¾r y Iper sBare.：
舞盥船缢短期N03思思篷警I
sgoveπnment≡
J begin .	. read
state；”
・ a P°v5eΓuɔɪake tiore = ιn⅛ude.
unite state
force :ion'ɪsi!? Wat~
tria l⅛b31Bf>o≡
posf `e df amount
error U∩t
i cash book c 」
jb⅛δ?Frengord,Jate' particulars
fa⅛om^decay
i∩ueιeιts
onmaQS⅛j . give
nuclei use neuɪron
proton
⅛yjPber,
eleetʃon L,1^1
nuclei
Figure 4:	Cluster extraction for data set B covering 7 topics: computer science, physics, math,
biology, geography, history, accounting. ν = 15, η = 80. 8 clusters are extracted.
Except for the least prevalent category, geography, all categories are identified, with the category
accounts being clustered into three separate topics (wordclouds 3 - 5 in Figure 4). In Comparison,
neither the LDA nor the NMF identify a geography topic, whereas the LDA identifies 9 topics and
the NMF 5 topics (see Appendix). Neither BERTopic or Top2Vec either with or without pre-trained
embeddings are close to the true number of topics and again severely overestimate the true number
of topics also failing to extract a cluster covering the topic of geography.
3.3 DATA SET C
Last, a data set covering 10 topics, namely graphics, technology, space, sport, business, politics,
medicine, history, entertainment and food is analyzed. This data set is perfectly balanced with all
categories making up 10% of all 1.000 documents. We initialize the maximum chain size ν as 15
and the maximum number of clusters η as 80. Our Clustering approach extracts 9 clusters using the
TF-IDF keywords which are represented in Figure 5.
∖ /0 二1 rtan
V t- d F”
f ： θ u⅛s∣
⅛Lι⅛ɔ ⅛a.⅛etj
.w9m,en ∣athens l ,二 men .yθaΓ^
taκeχ ιr∩θ⊂
g°race
Olympic^ lɔ粕Lk爸宴
athlete year 9id .
l'≡-≈-~-j.build
i∈⅛yptuvπ
CCa «'、1 / M I around .ςo∣jntry '√'forum∣ W
period
USbeCCIt V 嗝”.E
1,tempie JanCient
nmake .Zlabour
P≡art≡an1
y-s⅛dpteoplg7iι⅛earp.ubnc
government
world WarNti喔热萨S£.
ge≡ma n
two f ra∏cebegin _.,,muse
≡≡war..
f reηchonebfitish
⅛⅛fe!∣3⅛p½⅛j.
ɪ Stifdough large ' ：： n(iχ H . cθyeιtop
¾f≡βZ	f UVeg 偌沙 Ie …
aΘQ≡fnιK⅛
,⅛,i n C hl rSJW⅛ o in bʃri e a almond ~' cornstarchpiece
idi5(=c⅛⅛ι;information -include
e UW 瞰：：：
g. .y8*^ V¾∣pepp⅛.e
空 OrlemAlzN
⅛^lo⅛∏∣ influ四
胆热烈award
Mlmiyeai
tool	ma*
IjJC学te_5 ― -r∙^woθ
design
imageuse
color 1 1 '*<graphιrndesign
Figure 5:	Cluster extraction for data set C covering 10 topics: graphics, technology, space, sport,
business, politics, medicine, history, entertainment, food. ν = 15, η = 80. 9 clusters are extracted
Most of the initial 10 topics can be clearly identified and are easily distinguishable from one another.
Only the medical topic is not clearly identifiable. In comparison, after optimizing with coherence
scores, the LDA identified 3 topics and the NMF 7 topics. Interestingly, neither identifies any form
of cluster, related to food (see figures in Appendix). This perfectly balanced data set is the only data
8
Under review as a conference paper at ICLR 2022
set for which both BERTopic as well as Top2Vec accurately recognize the true number of topics and
precisely extract humanly interpretable topics representing the true categories covered in the data
set.
4 Conclusion
The proposed clustering rule accurately detects the inherent topics in the dataset and outperforms
LDA and NMF in terms of human-interpretable topics. Even BERTopic and Top2Vec are outper-
formed in terms of finding the perfect number of topics as well as recognizing all topics prevalent
in the data set. Interestingly, we find that TF-IDF keyword extraction outperforms Transformer
based keyword extraction (Grootendorst, 2020b). Through the simple optimization process of sub-
sidizing the documents coherence scores, we successfully create ultra-coherent clusters and thus
ultra-coherent topics. As Rosner et al. (2014) found that the U-mass coherence score can even
be outperformed by other, similar scores, an extension of the method using different types of coher-
ence scores promises additional insights into creating even more coherent and humanly interpretable
clusters.
9
Under review as a conference paper at ICLR 2022
References
Dimo Angelov. Top2vec: Distributed representations of topics. arXiv preprint arXiv:2008.09470,
2020.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
machine Learning research, 3:993-1022, 2003.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv
preprint arXiv:1803.11175, 2018.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. Reading
tea leaves: How humans interpret topic models. In Advances in neural information processing
systems, pp. 288-296, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Maarten Grootendorst. Bertopic: Leveraging bert and c-tf-idf to create easily interpretable topics.,
2020a. URL https://doi.org/10.5281/zenodo.4381785.
Maarten Grootendorst. Keybert: Minimal keyword extraction with bert., 2020b. URL https:
//doi.org/10.5281/zenodo.4461265.
Thomas K Landauer, Peter W Foltz, and Darrell Laham. An introduction to latent semantic analysis.
Discourse processes, 25(2-3):259-284, 1998.
Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factor-
ization. Nature, 401(6755):788-791, 1999.
J. Mazarura and A. De Waal. A comparison of the performance of latent dirichlet allocation and the
dirichlet multinomial mixture model on short text. In 2016 Pattern Recognition Association of
South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech), pp.
1-6, 2016.
David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Opti-
mizing semantic coherence in topic models. In Proceedings of the 2011 conference on empirical
methods in natural language processing, pp. 262-272, 2011.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. Automatic evaluation of topic
coherence. In Human language technologies: The 2010 annual conference of the North American
chapter of the association for computational linguistics, pp. 100-108, 2010.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
Radim Rehurek and Petr Sojka. Gensim-python framework for vector space modelling. NLP Centre,
Faculty of Informatics, Masaryk University, Brno, Czech Republic, 3(2), 2011.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/
abs/1908.10084.
Frank Rosner, Alexander Hinneburg, Michael Roder, Martin Nettling, and Andreas Both. Evaluating
topic coherence measures. arXiv preprint arXiv:1403.6397, 2014.
Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval.
Information processing & management, 24(5):513-523, 1988.
Shaheen Syed and Marco Spruit. Full-text or abstract? examining topic coherence scores using
latent dirichlet allocation. In 2017 IEEE International conference on data science and advanced
analytics (DSAA), pp. 165-174. IEEE, 2017.
10
Under review as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
匕Ukasz Kaiser, and Inia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Hanna M Wallach, David M Mimno, and Andrew McCallum. Rethinking lda: Why priors matter.
In Advances in neural information processing systems, pp. 1973-1981, 2009.
Wei Xu, Xin Liu, and Yihong Gong. Document clustering based on non-negative matrix factoriza-
tion. In Proceedings of the 26th annual international ACM SIGIR conference on Research and
development in informaion retrieval, pp. 267-273, 2003.
A Appendix
A.1 RESULTS FOR DATA SET A
The cluster extraction of CBC for data set A is here presented subsidizing the BERT-keywords.
Similar to the TF-IDF extraction, ν and η are set to 15 and 80 respectively. The topics politics and
technology are visibly detected whereas the topic sports seems to be included in two topics (#1, #3).
MIrneW，one 3
g 潞Fment
⅛P⅛labour
timebclaimPxcl'1	..,
online™
汴暹心Sei
≡? TmeK
ɔ I One
onlineIjκ)bιι⅛g RtIflXle*樵 *√,■-ir ,wτelease-rep∙riumber∙^^≡tbroadband ;
Figure 6:	Cluster extraction for data set A covering 5 topics: business, entertainment, politics,
sports, tech. ν = 15, η = 15. 4 clusters are extracted.
Due to these unclear distinctions between the topics we favour the TF-IDF based clustering rule.
Top2Vec topic extraction using pre-trained embeddings comes with 10 identified topics fairly close
to the true number of 5 topics. However, the most prevalent topic, business is not identified at all.
There are 4 very clearly identifiable sports topics, even distinguishing between tennis, Olympia and
football.
SrfQk i DTn⅛lW½≡
谒至se黑66由虚≡需
refo⅛ζ deficit allegations parliamentary said Politiciansexports
∙d⅛i^uTnηsXd‰ I electronicnet ∣k5∣handhelch ∣i
⅛co∩suSe⅞b r Oadb and
as⅛⅞fa … W 厂 「…de；；:S
:≡∖ eleδδπt岸“t 翳hQ9L9gιes揶黑;
gg≡sg≡≡
∖ / F IlU ^"∖ U ∣antl T -threat ∙ targets
Yt最上旦》星:ʤEqa£工£工QyCS
Topic 9
Sso⅛afevirUSeS
S pywa fee≡^B∣
…曲号 JUramS «	_ Lt
WlndOWSH!"Q[ Q>QJJ
Topic 1
osca noxxdawar d sp「OqUCerS
⅛≡≡≡⅛s≡
fŋ kn-y∩ΩeU∣ 1 ∏hedχe Qd ⅛diLcehag ^sb
≡o∏1williams :HnjIudry"：.总高、甲哽
潞⅛te翻葬照 1⅛盛翠梅
Fndd-i r∣, c→-hewitt I
toμrhnjmentoiymeιc
且骗甚嘉工•al⅛b⅛ndaucan⅛
S≡β≡i
Figure 7:	Top2Vec topic extraction for data set A covering 5 topics: business, entertainment, politics,
sports, tech.
11
Under review as a conference paper at ICLR 2022
A.2 RESULTS FOR DATA SET B
For the second data set covering 7 topics, LDA extracts 9 topics (Figure 8). Topics as accounts (#2,
#8) or biology (#5) are clearly distinguishable.
-xτprce
二Ea屋
-………………	-l'-'<' V J- S S
V rs.yeJS
-I ....0 , ⅛ ArRntyear.V
bal 厂
stat'ementsd S S θ"t S ⅛
Part/
st ə t Cs
∩ewwa⅛
military .....	匚,SZ
SUoDX_ find
X twowg加［爸
numben
represent	、,“、	PEbS
CeI1?«ICalIP盟牌£吧∙
⅛azt,erse
L- timeʃegist,er
∣∣CO cache
Pnc⅛emo 号 y
data address
iħst1rutctiQn
VOggeLRea^oteqW
噌 UUnmCV r fιsure
frequency黑印对y⅛乎
Gurxent
Cl ιχςιιι.tresistance ∙ ValUediode
⅛⅛5s⅛,p.Xca.sh⅞aiι
lsh¾θ
debentures amount
7l1,, ,...T∩'ucle..∣ partidevʤ:e ,
⅛enθfgy
Figure 8:	LDA topic modeling for data set B
The NMF topic model extracts 5 topics for the 7 topic data set. The topics geography as well as
accounts and biology are not distinguishable at all. History seems to be split up into two topics (#2
and #5).
(a∩y
^ney^CalIθ≡
share
a∏1ountFS ιι pe;
appΓ⅛catιon allotment
energy ∏OOj∕ may
g 喘"'b"
⅛⅛KI≡,≡ft
ifl≡ ..-"` `河 m
different ,»»<∙ ；!„： cache二PIjlllfe∏P1N丸囿ge
吊痴S ɪ ə ɪ θ-
E	igovernment .； =-*,r ' *
≡Wnιte
p1⅛hbecbme21
s..oγg^
gmemoιryuse⅛
≡Tuctidδtn
⅞UI⅛⅛r i ∏51 r`u c t iotns,rlte
Figure 9:	NMF topic modeling for data set B
CBC in combination with BERT-keywords extracts accurately 7 topics for the 7 topic data set. The
topics physics (#7) and history (#2) are clearly distinguishable. The topic accounts is visibly mixed
into multiple clusters.
12
Under review as a conference paper at ICLR 2022
CpVoOne
7plln ，
H⅛aO
share capital 「s per
'-■∙•■'issue = debg,rɪɪure∣i'j~rnaι,slt1risscaρ,j.tal C debeιjturesjrs?
company
per share1	L
・ 1 bank c∣account
equity share
'amount，Shmre ΓS
.I∩I⅛b^C Qmesountryjg
government^
le3Z".S*3Wn≡h
⅛⅛fSxl i ∙ , . end PυxxαM-⅛^lr⅛ι SOViet.
unite Etate
.⅛⅛8∏⅛iJ∙J,⅛ry leader⅛↑≡o_®.Jj-onmake1,r PaLJy
比节i笆隹，启正吾胃籁H
P0St7^f amount
⅛roraCCOU∩t
F total . goods.	raSh
I cash book c 吧！/
s⅛alance r- α r c 广 rl-。” 湍需?嬴，"1.■■
KEtmt J、G L U I Vldatθ particulars
caib1.∣ number-
USe期浮…霏
One，
：’\四9需埼OrjM ”‘巳
instruction
clock cycle	.	'	.	. λ block
阳言it荀s∙'斯莘宛舞落篇
u naneɪng activities-™^ LOW Statement' ..a, ∙ I⅛1-
cash equivalent! dividend ιnclude
CZaSlT ow
CaSh二 OWr=Vh
ι α ɔ-1 I
OP0闻息且篇⅛盛ies赢MaCX4提
Figure 10:	Cluster extraction for data set B covering 7 topics: computer science, physics, math,
biology, geography, history, accounting. ν = 15, η = 80. 8 clusters are extracted.
A.3 RESULTS FOR DATA SET C
LDA topic modelling extracts three topics for the last, 10 topic data set. The three extracted topics
seem to cover history, space and technology.
Figure 11: LDA topic modeling for data set C
NMF topic modelling extracts 7 topics. Medicine is visibly split up into two topics (#4 and #6).
Space, history and image are visibly distinguishable.
13
Under review as a conference paper at ICLR 2022
a hig叱r_jiuniversjty 'T^r~itechnqLogy XeleSCOPe
space
ShUtt厂
l,, Gent er
... ^.⅛98r≡n,
明由剧/V&F星
se≡a≡
_tr
x	VlSUal LWlOn
COE, 「 PrOgranl contact
10, mat W"h唬-ap"cs Ua
software	模髭 e
FIe	PaCKame
av^ι^abh(d⅛ay
a⅛p≡ai≡
S., . StUd哺"
∣Γ⅛ ι≈l"πa∖, U
aprildcvclop … health
BHffiT" mɛdical
QJ-O `
⅛1∣ Il
,plc
ɔ F" "*1°ysn⅛s∖
see≈OT∙,^ _
h WdQcUJ
issue χeR9ft
卜…,among[ (j1
cancer
centermedical
¾!33
I
■U」「翻bit可祥触a「“
P∞be
tl ISftl
Figure 12:	NMF topic modeling for data set C
CBC subsidizing BERT-keywords extracts 14 clusters. However. setting η to 80 as done previously
would result in 76 clusters. Pre-specifying η anywhere between 15 and 75 results in 14 clusters and
specifying η < 15 would result in 2 clusters.
.」-…∙ -work - τasysltem force⅛
g⅛t>an1
靶嚣 KnyuSew。
R抹漏出en(¾⅛ X „ time ru55,dn
I image .
design
i IUa ,
U ɔ Umake tool
graplɪie^ design',s∖rea,e ^
≡⅛
M ,p评IemOrlemakeW
娅管黑
see PhOneI阪 U l L c
study . hiv
patient
⅛easea ɪʤ
« 1HS QJ- C ɑ ɪ-ʃɪ θws Ji⅞ 11 qγ-∙ ~ =r∙⅛l.?
company
k^,pne
■ M many ɪ
⅛⅛
useRτpj‰^
make∏et tj"ɔ t
software' ； o " 钎IT si,r,
people
i温 Nmob⅛l'e∣⅛CFQSθft
I ` ɪ vpte
味上medicLne […⅛
加O 飞
Ξ5 ɪ- know M
⅛⅛⅛a
.slue Slw"C。。八
,图留网&C ta!⅛espoon
cool waterweliminUten1ιχture
FggUgrmak答:二:
government
,issue ɪb⅛ir
plampartyi Uk
el ect i onPS⅛Jbζ,rto r i e"s
⅛ar
^≡ ・© ,∣⅞5vS
≡l≡
,RSgnes 工 Ulm, s .
心驾丫建恁门… god ,,
EegyPtte 叩 Ie
TtePeri6暹
pharaohςt-ruleF Z⅛
H"Γ"CiQ⅛ge help physician-
i S总冏依qc
I medicine
address∞sfthank
quality
th ea Itihels
a 勺,Wun - cxS
Figure 13:	Cluster extraction for data set C covering 10 topics: graphics, technology, space, sport,
business, politics, medicine, history, entertainment, food. ν = 15, η = 15. 14 clusters are extracted
14