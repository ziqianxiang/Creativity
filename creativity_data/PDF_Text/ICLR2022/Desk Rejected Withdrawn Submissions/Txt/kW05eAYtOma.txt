Under review as a conference paper at ICLR 2022
Rethinking Pareto Approaches in Constrained
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Constrained Reinforcement Learning (CRL) burgeons broad interest in recent
years, which pursues both goals of maximizing long-term returns and constrain-
ing costs. Although CRL can be cast as a multi-objective optimization problem, it
is still largely unsolved using standard Pareto optimization approaches. The key
challenge is that gradient-based Pareto optimization agents tend to stick to known
Pareto-optimal solutions even when they yield poor returns (i.e., the safest self-
driving car that never moves) or violates the constraints (i.e., the record breaking
racer that crashes the car). In this paper, we propose a novel Pareto optimization
method for CRL with two gradient recalibration techniques to overcome the chal-
lenge. First, to explore around feasible Pareto optimal solutions, We use gradient
re-balancing to let the agent improve more on under-optimized objectives at each
policy update. Second, to escape from infeasible solutions, we propose gradient
PertUrbatiOn to temporarily sacrifice return to save costs. Experiments on the Safe-
tyGym benchmarks show that our method consistently outperforms previous CRL
methods in return while satisfying the cost constraints.
1	Introduction
By virtue of the close relationship to real-world applications, Constrained Reinforcement Learn-
ing (CRL) burgeons broad interest in recent years. Unlike the traditional RL, which targets maxi-
mizing cumulative rewards only, CRL pursues rewards while satisfying specific constraints (Achiam
et al., 2017; Ding et al., 2020; Wachi & Sui, 2020; Satija et al., 2020). For example, in the scenario
of auto-pilot, the well-trained agent should arrive at the destination accurately and meets the safety
constraints in the meantime (Kong et al., 2021).
Most existing works formulate the CRL problem as a Constrained Markov Decision Pro-
cess (CMDP) (Altman, 1999), which incorporates constraints and rewards into the same framework.
Aside from returning a scalar reward after each action like conventional MDPs, CMDPs send back
one or multiple cost signals independent of reward. Constraints are expressed explicitly in CMDPs
by limiting the expected sum of each cost in the corresponding region.
Essentially, the purpose of CRL is to maximize rewards while controlling costs, which would be
naturally associated with the Multi-objective optimization (Deb, 2014). In recent years, Pareto
approaches (Sener & Koltun, 2018; Lin et al., 2019), which find a steep gradient that benefits to
all objective, have been generally leveraged to multi-objective optimization. The ultimate goal of
Pareto approaches is finding a Pareto-optimal (Pareto, 1897) solution, in which no objective can
be advanced without harming any other objectives. However, algorithms for CRL seldom consider
experience from the Pareto optimization area because existing Pareto approaches perform poorly in
practical CRL problems (Tessler et al., 2018).
According to our analysis, existing Pareto approaches are not practical at CRL because they can only
find trivial Pareto-optimal policy. As shown in Fig.1(b), when the gradients to optimize reward and
cost disagree, Pareto approaches will synthesize anew gradient biased to the shorter one in direction.
Suppose the policy is near-optimal about one objective (i.e. short gradient) while underdeveloped in
another objective. In that case, this biased gradient will instead pay more attention to the objective
with better performance. Policy updated by this biased gradient will be either too risky to be aware
of the constraint on cost or too conservative to interact with the environment, which leads to an
imbalanced development in rewards and costs. Meanwhile, restricted by the feature of simultaneous
1
Under review as a conference paper at ICLR 2022
improving all objectives, Pareto approaches cannot sacrifice one objective in exchange to advance
another, which is a necessary skill to find a policy feasible to cost constraint in CRL. Thus, though
existing Pareto approaches are able to find Pareto-optimal policy, the results not only have imbalance
performance in terms of rewards and costs, but also are unable to guarantee that the constraint is met.
To tackle the aforementioned defects, we import gradient re-balancing and gradient perturbation
mechanism to apply Pareto approaches in CRL. Gradient re-balancing re-defines the length of gra-
dient and ensures we focus more on the objective in need. While gradient perturbation forces the
Pareto optimizer to take confined attention on reward to improve cost by a bounded extent.
In this paper, we propose a novel CRL paradigm from the perspective of Pareto-optimal. With
definitions on CRL and Pareto-optimal (Section 2), we rethink the connections between existing
CRL methods and the concept of Pareto-optimal (Section 3.1). Then we analyze the pros and cons
of applying Pareto optimization approaches to the CRL problem (Section3.2). Furthermore, we
design a practical algorithm for CRL (Section 4) named CONTROL (abbr. for Constraints adaptive
Pareto Reinforcement Learning) with two radient recalibration techniques. At last, We conduct
experiments on a benchmark of CRL, the SafetyGym environment, the results of which demonstrate
the superiority of CONTROL comparing to the state-of-the-art baselines (Section 5).
2	Preliminary
2.1	Constrained Markov Decision Process
Markov Decision Process A normal Markov Decision Process (Sutton & Barto, 1998) can be
described as a quadruple (S, A, P, R). Precisely, S denotes the state set; A denotes the action set;
P is the distribution returning the probability of transiting to s0 assuming we take action a in s,
denoted as P(s0|s, a); R : S × A × S → R is the reward function, which delivers reward r as
soon as the transition s → s0 is accomplished. We make decisions for choosing actions by a policy
π : S → ∆A, which is a distribution over A. In this work, we parameterize our policy πω by a
neural network with parameters ω ∈ Rk .
In an MDP, we take an action a 〜 π from initial state so 〜 ρo (so) iteratively, and transit to a
new state according to P, yielding a finite or infinite trajectory T = (so, ao, ri, si, a`, r2,...)〜π.
Given a policy π, we are able to evaluate the goodness of a state or action by state-value function
V (s), action-value function Q(s, a), and advantage-value function A(s, a):
VRπ (st) = Eat,st+1,...
∞
γlrt+l ,	QπR (st, at) = Es
t+1,at+1,...
l=o
AπR (s, a) = QπR (s, a) - VRπ (s).
∞
X γlrt+l
l=o
(1)
In Eq(1), γ ∈ [0, 1] is the discount factor, which weighs the future reward and instant reward. The
goal of reinforcement learning is to discover an optimal policy π* for MDP by solving:
arg max Es0〜ρ°,τ〜∏ [VR∏ (so)].
π
(2)
Constrained MDP In this paper, we concentrate on CMDP with only one kind of cost, which is
consistent with the settings in Achiam et al. (2017); Tessler et al. (2018); Yang et al. (2019). The
main difference between CMDP and MDP is CMDP has a cost function C : S × A × S → R.
Therefore, the feedback of the environment in one transition is a vector (r, c) ∈ R2, where c ∈ R+
is the value of cost. Similarly, we have value functions for cost: VCπ (st) , QπC (st, at) , AπC (s, a) by
switching the reward r to the cost c in Eq(1).
Formally, the policy optimization problem in CMDP is
max Jr(∏) = Eso〜ρo,τ〜∏ [唳(so)],
π	π	(3)
S.t. JC (π) = -Eso^ρo,τ^π ∖Vα (SO)] ≥ ζ,
where ζ < 0 denotes the predefined constraint threshold. In Eq.(3), JR (π) and JC (π) represents
the performance of π with respect to reward and cost, respectively. Specifically, here we let JC (π)
negative for readability.
2
Under review as a conference paper at ICLR 2022
Figure 1: An illustration of Pareto
direction ∆(ω).
(a): ∆(ω) generalizes both gra-
dients when they converge in
direction.
(b): ∆(ω) is biased to the shorter
gradient when Vω JR (∏ω) and
Vω JC (∏ω) disagree in direction.
(c):	∆(ω) coincides with one
gradient in some cases.
2.2 Pareto-optimal
To make clearer definitions, we introduce related concepts under the problem settings of CRL di-
rectly. To understand Pareto-optimal, we need a rule to compare which policy is better first.
Definition 2.1 (Dominate). For two policies π, π0, we say π dominates π0, denoted as π	π0 i.i.f
JR(π) ≥ JR(π0), JC (π) ≥ JC(π0) and and at least one inequation is strictly holds.
By Definition 2.1, we know that a policy π is better than π0 when π is not worse than π0 over reward
and cost, and outperforms π0 on at least one objective.
Definition 2.2 (Pareto-optimal Policy). We call an policy πω is global Pareto-optimal i.i.f. ∀ω0 ∈
Rk, ∏ω0 * ∏ω is invalid, i.e. ∀ω0 ∈ Rk, ∏ω√ * ∏ω；We call an policy ∏ω is local Pareto-optimal i.i.f.
there exists a neighborhood U ⊂ Rk ofω s.t. ∀ω0 ∈ U, πω0 πω is invalid, i.e. ∀ω0 ∈ U, πω0 πω.
Without otherwise specification, we use Pareto-optimal referring to local Pareto optimal hereafter.
A local Pareto-optimal policy is guaranteed to be a global Pareto-optimal policy only if JR(πω ) and
JC(πω) are concave over ω in Rk, which is invalid in most Deep RL setting. From the angle of
optimizing target, we notice that if π is a Pareto-optimal policy we cannot advance any J(π) while
keeping the performance of another. This leads to the definition of Pareto direction:
Definition 2.3 (Pareto direction). Given a parameterized policy πω, if an vector v ∈ Rk is an
updating direction ofω which can boost at least one J(πω) without harming another J(πω), then
v is a Pareto direction ofπω. Namely, if hVω JR (πω), vi, hVω JC (πω), vi ≥ 0, then v is a Pareto
direction of π, where h , i is the standard inner product and at least one inequation strictly holds.
Pareto-optimal Searching To search a Pareto-optimal policy, the most straightforward idea is
designing an iteration ω0 = ω + η(ω)∆(ω), where ∆(ω) is a Pareto direction of ω, and η(ω) ∈ R+
is stepsize. With appropriate η, we could ensure πω0 * πω in each updating until πω is Pareto-
optimal. IntuitiVely, ∆(ω) should be a linear combination of gradients of JR(πω), JR(πω), i.e.
∆(ω) = βRVωJR(πω) + βCVωJC(πω), in which βR,βC are called Pareto weights.
Both Fliege & Svaiter (2000) and DeSideri (2012) are established works for Pareto-optimal search-
ing. Under the problem settings of CRL, the Pareto weights are obtained in DeSideri (2012) by
solving following Quadratic Programming (QP):
min	∣∣βRVω Jr(∏s ) + βc Vω JC (∏ω )||2
βR,βC∈R
s.t. βR,βC ≥ 0, βR+βC = 1.
(4)
Fig.1 illustrates the geometric relationship among VωJR(πω),VωJC(πω) and ∆(ω) by three cases.
As a result of constraints in Problem(4), ∆(ω) ends on the line segment determined by endpoints
of VωJR(πω) and VωJC(πω). Since we minimize the length of ∆(ω), ∆(ω) is normally a perpen-
dicular vector of VωJR(πω) - VωJC(πω) (Fig.1a,b) and sometimes it coincides with one of the
gradient vectors (Fig.1c).
In Appendix A, We first re-elaborate the algorithms proposed in Fliege & SVaiter (2000) and Desideri
(2012) under the CRL framework. Furthermore, we provide concise proofs for their effectiveness in
finding Pareto direction and show that they are fundamentally identical in CRL problems. Finally,
we make completeness proof for possible extreme situations when applying Pareto-optimal in CRL.
3
Under review as a conference paper at ICLR 2022
Pareto frontier
Threshold of cost
Feasibility region
Final policy
Figure 2: An illustration of how various methods find Pareto-optimal policies. (a): Linear scalar-
ization can find Pareto-optimal policy but need may not be Pareto-feasible; (b): Lagrangian meth-
ods can find feasible policy, while they may fails for too conservative to explore the environment;
(c):CPO can find Pareto-optimal policy but cannot find Pareto-feasible policy if initial policy is too
fat to reach the feasibility region; (d) Our method can find Pareto-feasible policy, while not each
step is a Pareto direction.
Parameter updating route
Initial policy
3	Pareto-optimal in CRL
3.1	Connections to Prior CRL Works
The mainstream approaches to solve such problems could be grouped into two genres: (i) La-
grangian methods (Borkar, 2005; Tessler et al., 2018; Stooke et al., 2020); (ii) Trust Region meth-
ods (Achiam et al., 2017; Yang et al., 2019). Lagrangian methods combine primal reward-oriented
and cost-oriented objectives into one dual min-max problem, converting CMDPs into unconstrained
MDPs. While Trust Region methods attempt to determine a trust region in parameter space, where
policies updated inside would not violate constraints with worst-case bound on reward performance.
Prior CRL algorithms are fundamentally finding Pareto-optimal policy (See proofs in Appendix C),
which is a conclusion that unifies our work and existing works in methodology. To be consequen-
tialism, any non-Pareto-optimal policy is unworthy of being regarded as a solution to a CMDP, since
there must be a better one. Nevertheless, not every Pareto-optimal policy makes sense in a CMDP.
What we truly need is Pareto-feasible policy:
Definition 3.1 (Pareto-feasible Policy). We call an policy πω is Pareto-feasible ifπω is local Pareto-
optimal while satisfying constraints.
Fig.2 (a), (b) and (c) illustrate how prior CRL algorithms search Pareto-optimal policy. However,
only in some occasions they are capable of finding a Pareto-feasible policy. In Fig.2 (a), we scalarize
(r, c) per transition with random or preset weights linearly. With diverse selection of weights, the
final policy may be different. Under such conditions, the search route could only reach a Pareto-
feasible policy with weights good enough (route 2). As shown in Fig.2 (b), Lagrangian methods
ensures that JC (π) is stable near the threshold. But they may fail to search Pareto-optimal policy
because they are too conservative to explore and fall into local optima (route 2). For CPO (Achiam
et al., 2017) in Fig.2 (c), it is able to maintain the policy within the feasibility region while increase
JR(π) (route 1). Yet, it may fail to satisfy constraint when initial policy is not feasible. Results in
experiments and proofs in Appendix C could corroborate that above interpretation is not heuristic.
3.2	Pros and Cons for Pareto-optimal in CRL
Advantages of Pareto-optimal Searching in CRL Based on the preceding analysis, we conclude
three advantages for Pareto-optimal searching in CRL:
4
Under review as a conference paper at ICLR 2022
•	Preeminent: Any Pareto-optimal policy ensures its superiority considering both reward and cost.
•	Reachable: Pareto-optimal policy is not unique, and even simple algorithms (e.g. Linear Scalar-
ization) can reach it.
•	Knowledge-free: No prior knowledge is needed to apply Pareto approaches in CRL.
Disadvantages of Pareto-optimal Searching in CRL Despite noting the advantages of Pareto-
optimal Searching, we found two main disadvantages from practical.
First, existing Pareto-optimal algorithms optimize all objectives with consistent extent (Proven in
Theorem 4.2), which will lead to an imbalanced development of JR(πω) and JC (πω). This fact im-
plies that the updating in this iteration would focus on rewards, which is already the better-optimized
objective comparing to cost. Suppose our policy πω is in a situation where JR(πω) is near the lo-
Cal optima while JC (∏ω) is still Under-optimized. In this circumstance, Vω JC (∏ω) is steeper and
longer than Vω JR(∏ω), which is similar to Fig.1(b). As demonstrated in Fig.1(b), the Pareto direc-
tion ∆(ω) is biased to VωJR(πω) and its component in the direction of VωJR(πω) is longer than
Vω JC (∏ω )'s. In extreme cases, if our policy reaches or approaches a trivial Pareto-optimal policy,
it has no chance to escape and find feasible policy.
Second, existing Pareto-optimal approaches overemphasize the simultaneous growth of two ob-
jectives. In fact, in CRL, we need to sacrifice JR(πω ) in exchange for improvement in JC (πω )
to satisfy constraint in certain situations. Suppose we already reached Pareto frontier with a non-
feasible policy. Instead of finding a Pareto direction to advance reward and cost, we prefer to search
policies with less reward performance. Similarly, when our policy is too conservative, we should
encourage it to take risks and pursue higher rewards. Thus, this defect will restrict our control to the
parameter updating route and finally return a trivial policy without guarantee of satisfying constraint.
4 CONTROL: Constraints adaptive Pareto RL
4.1 Gradient Recalibration mechanisms
Gradient Re-balancing This is the corresponding improvement for imbalanced development is-
sue. To tackle this issue, we should adapt VωJR(πω) and VωJC(πω) to achieve a similar degree
of improvement in reward and cost at each iteration of policy parameters. Motivated by recent
works about normalization in gradient (Chen et al., 2018; Mahapatra & Rajan, 2020), we reform
Problem(4) as:
R min⅛ llβRvNJR(πω) + BcvNJC(πω)ll2,
βR ,βC ∈R
s.t. βR,βC ≥0, βR+βC = 1,
where VωN JR (πω), VωNJC(πω) are normalized gradients and defined as:
N7N 丁、、- vω JR(πω )	v7N Tfe 、- vω JC (πω )
V JRg= ∣∣Vω JR (∏ω )||2 2 V JC (πω )= ∣∣Vω JC (∏ω )∣∣2 .
(5)
(6)
In Eq (5), the length of normalized gradient vectors becomes the reciprocal of its original length,
which makes the original longer vector shorter. With solution βΝ = (βR, βC) ∈ Rj, We can find a
better Pareto direction comparing to the Pareto direction derived from Problem (4).
Lemma 4.1. If πω is not Pareto-optimal, ∆N (ω) := βRNVωJR(πω) + βCNVωJC(πω) is a Pareto
direction of πω.
Theorem 4.2. Suppose the iteration paradigm is ω0 = ω + η(ω)∆N (ω) and η(ω) → 0. Then:
(i) if we use ∆(ω) derived from Problem (4), the improvements in reward and cost are consistent.
Specifically, when βR ∈ (0,1), [R ,∏ω ]——[R ,∏ω] → L (ii) if we use Δn (ω) derived from Prob-
JC (πω0 ) - JC (πω )
lem (5), the improvements in reward and cost are proportional to the square length of corresponding
J. , c N N N	QNLQ 八 JR(πω0) - JR(πω)	||Vv JR (πω )||2
gradient. Specifically, when βR ∈ (0,1),   ------ʌ------——r → γτ=——-——.
JC (πω0 ) - JC (πω )	||Vv JC (πω )∣∣2
The proofs to Lemma 4.1 and Theorem 4.2 are provided in Appendix D.
5
Under review as a conference paper at ICLR 2022
According to Theorem 4.2, now we can pay more attention to the objective which needs more
optimization. Note that it is imbalanced even if the improvement ratio is 1, because in practical the
scales of JR(π), JC (π) may be quite different.
Gradient Perturbation This is the corresponding improvement for assisting us to sacrifice re-
wards to satisfy constraint. A naive method for this idea is only optimizing cost when constraint is
violated. But this is not realistic because: (i) focusing on cost too much may drive the agent to be too
conservative to explore and fall into local optima; (ii)sometimes reward and cost are not competitive,
totally ignoring reward is unwise.
In fact, manipulating Pareto weights can achieve the goal of sacrificing rewards when necessary. If
current Pareto direction is unable to promote cost to a desirable extent, we must pay more attention
to cost by raising βC. Motivated by PPO (Schulman et al., 2017), we design a mechanism to control
Pareto weight: when βR is too big, we clip it to a smaller number. Since βR + βC = 1, the range of
βR can influence βC, and an upper bound for βR is also a lower bound for βC .
Specifically, given a clipping threshold t ∈ [0, 1], the original Pareto weight βRN will be clipped to
min(t, βRN). With fixed η(ω) → 0, we can deduce a lower bound for the growth of JC(π):
Theorem 4.3 (The lower bound of JC (π) improvement). Given the parameter updating paradigm
ω0 = ω + η(ω)∆N (ω) and clipping threshold a, we have the lower bound for JC (πω0 ) - JC (πω):
JC (πω0) - JC(πω) ≥
η(ω) [t∣∣Vω JC (∏ω)∣∣2hVN JR(∏ω) - VN JC (∏ω), VN JC (∏ω)i +l] — CDmLx (醍，,∏ω),⑺
where C = 4eγ∕(1 一 γ)2 and E = max§|Ea〜∏ω, [Aπω (s, a)] |. And this bound is strictly positive
related to t.
The proof to Theorem 4.3 are provided in Appendix D, in which we also prove that this lower bound
is tighter than the situation without clipping. Specifically, this theorem holds if swapping JC, JR.
The clipping operation ensures enough Pareto weight on the objective underdeveloped and perturb
the weight if not meeting the clipping threshold. As clipping operation is imported to Problem (5),
∆N(ω) may not be a Pareto direction. But it ensures a lower bound for the improvement on JC (π),
which is vital to find a feasible solution. To obtain a nice lower bound, we must choose proper t and
η(ω). For t we can search by grid and search η(w) by line backtracking (Armijo, 1966).
4.2 Practical Implementation
Base RL model Our method is adaptable and can be applied with any policy-gradient-based RL
algorithm. In this paper, we adopt Actor-critic-based PPO (Schulman et al., 2015) as the base model
of CONTROL. To estimate value functions for both reward and cost, we have two critics to ap-
proach QπR (st, at) , QπC (st, at), separately. Under the framework of PPO, VωJR(πω), VωJC(πω)
is determined as:
VωJR(πω)
∂Es
~∏old,a~∏ω [AπRold (s, a)]
∂ω
, Vω JC (πω )
∂Es
~∏old,a~∏ω
[-AπCold (s, a)]
∂ω
(8)
Pareto-Optimal to Pareto Feasible With Theorem 4.3, policy updating in CONTROL is guaran-
teed to converge to a Pareto-optimal optimal policy with a fixed clipping threshold t. However, this
policy may not be Pareto-feasible. In this case, a search for t and η(ω) is necessary but this will
affect the efficiency of our algorithm.
To choose t and η(ω) with efficiency, we devise a heuristic mechanism which keeps η(ω) as a con-
stant and changes t according to the change of JR(π), JC (π). If the improvement of the performance
about cost in one iteration is not good enough, we decrease t by a little quantity to make a stricter
clip. Similarly, if the performance about reward has not been improved within certain epochs, we
increase t by identical quantity. Particularly, we clip βCN to encourage risky exploration when the
performance about cost reflects that our policy is too conservative. The indicators of performance
are acquired in an on-policy way.
We introduce how CONTROL trains an agent and modifies t in a pseudo-code, which can be found
in Appendix E.
6
Under review as a conference paper at ICLR 2022
Model	Goal-Lvl 1		Goal-Lvl 2		Button-Lvl 1		Button-Lvl 2		Push-Lvl 1		Push-Lvl 2	
	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost
TRPO	25.12	56.14	23.58	213.17	25.83	138.36	25.59	172.61	7.90	47.11	4.69	75.16
PPO	24.99	58.09	22.27	198.65	26.01	150.12	23.93	188.17	3.41	67.12	2.21	71.25
TRPOL	17.03	25.47	5.49	25.27	8.18	32.59	3.73	22.51 ^^^^	4.19	26.31	1.30	23.38
PPOL	13.58	14.21 ^^^^^	1.15	31.66	4.84	23.01 〜^^^	2.38	17.99 ^^^^	2.15	40.20	1.52	17.27
CPO	23.21	42.52	14.37	60.11	18.25	80.25	16.78	74.43	7.21	38.94	1.84	ΓΓΓΓΓ 29.37
MGDA	25.61	40.15	7.31	60.91	6.02	54.62	1.99	11.20 ^^^^	0.80	16.93	0.89	29.77
CONTROL	21.86	23.12 ^^^^^	8.39	20.84 ^^^^	9.38	22.67 〜^^^	6.44	23.83	2.39	ΓΓΓΓΓ 17.17	2.98	20.92
CONTROL-R	13.21	30.80	1.51	50.57	5.94	35.29	2.18	Γ7221 ^^^^	0.75	ΓΓΓΓΓ 8.89	1.61	ΓΓΓΓΓ 17.75
CONTROL-P	17.89	45.22	2.23	42.49	3.90	52.05	3.50	54.74	1.36	ΓΓΓ 31.11	2.25	ΓΓΓΓΓ 28.77
Table 1: Comparison Results of CONTROL and other baselines with cumulative threshold <25.
Model	Goal-Lvl 1		Goal-Lvl 2		Button-Lvl 1		Button-Lvl 2		Push-Lvl 1		Push-Lvl 2	
	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost	Reward	Cost
TRPOL	24.57	28.01 ΓΓΓΓΓ	8.72	50.15	12.90	48.97 ΓΓΓΓΓ	7.27	69.21	6.13	44.72 ΓΓΓΓΓ	2.09	54.41
PPOL	25.11	26.12 ΓΓΓΓΓ	4.73	63.02	10.65	73.15	4.14	62.02	3.67	33.62 ΓΓΓΓΓ	1.62	41.74 ΓΓΓΓΓ
CONTROL	25.29	40.68 ΓΓΓΓΓ	16.38	48.41 ΓΓΓΓΓ	14.66	46.57 ΓΓΓΓΓ	13.89	48.81 ΓΓΓΓΓ	4.15	44.71 ΓΓΓΓΓ	2.52	43.03 ΓΓΓΓΓ
Table 2: Comparison Results of CONTROL and other baselines with cumulative threshold <50.
5	Experiments
We test CONTROL in SafetyGym (Ray et al., 2019), a CRL benchmark with 3 tasks:
•	Goal: In this task, the agent wins rewards by reaching the destinations (green cylinders) and gains
cost for passing traps (blue circles) and hitting movable vases (cyan cubes).
•	Button: In this task, the agent wins rewards by pushing a stationary button (orange balls) and
gains cost for passing traps (blue circles) and hitting obstacles (purple cubes) with a fixed moving
trajectory.
•	Push: In this task, the agent wins rewards by pushing a crossing workpiece (a yellow column) to
a specific destination (green cylinders) and gains cost for passing traps (blue circles) and hitting
towers (Blue cylinders).
For each task, we have two difficulty levels, level-2 environments have more cost-consuming items
than level-1 environments. Each environment is simulated in Mujoco (Todorov et al., 2012) with a
point agent. To better explain the tasks, We provide screenshots for each environment in Fig.3.
We compare CONTROL to baselines from three domains: traditional policy-based RL meth-
ods (TRPO (Schulman et al., 2015),PPO Schulman et al. (2017)); Constrained RL methods (TRPO-
Lagrangian, PPO-Lagrangian, CPO (AChiam et al., 2017)); Pareto approach(MGDA (Desideri,
2012)). Moreover, we make ablation study by CONTROL-R (without gradient re-balancing) and
CONTROL-C (without gradient clipping). We consider mean reward and mean cost as two met-
rics to weigh the effectiveness of models. For all methods we conduct 5 runs (1000 episodes each,
10000 steps each episode) with different random seeds and 5 X 100 episodes of test runs on another
5 random seeds. The threshold for cumulative cost is 25, as recommended in Ray et al. (2019). In
order to show that our method is adaptive to various thresholds, we make another experiment with
threshold 50.
For reproducibility, we list all architectures and hyper-parameters used in experiments in Ap-
pendix F.
(a) Goal-Lvl 1
(b) Goal-Lvl 2
(c) BUtton-LvlI (d) BUtton-Lvl 2	(e) PUsh-LvlI
(f) Push-Lvl 2
Figure 3: Illustration of tasks in SafetyGym
7
Under review as a conference paper at ICLR 2022
Goal-LVl 1
əposɪdwJOd SPJEMOx
Button-Lvl 1	PUsh-LVl 1
əposɪdwJωd SISOo
Goal-LVl 2	Button-Lvl 2	PUsh-LVl 2
əposɪdwEd SPjeMWX
1000
əposɪdwjəd SlSOo
Episodes (X 10000 steps) 1000
1000
Episodes (× 10000 steps) 1000
• TRPO ∙PPO ∙ TRPO-Largrangian ∙ PPO-Largrangian ∙ CPO ∙MGDA ∙ CONTROL
Figure 4: Reward and cost curves in all 6 tasks. A dashed line in cost curves represents the threshold.
An lines are averaged over 5 runs and shaded areas indicate one standard deviation.
5.1 Experimental Analysis
Learning curves are provided in Fig. 4 and results of final tests are listed in Table. 1.
Comparison study In comparison, We prefer policies feasible to constraint, which means, any pol-
icy that fails to satisfy constraint is considered to be worse than any feasible policy. In all tasks, we
can find a feasible policy even in all Lvl2 tasks, in which even Lagrangian methods sometimes fail
to meet the threshold. Moreover, our method outperforms all baselines which find feasible policies
in all tasks except Push-LvlI. We argue that it is because our base model, PPO, also performs poorly
in this task. In fact, all baselines learn limited experience in push tasks. By learning curves, we can
notice that CONTROL's performance on reward improves quickly at the beginning of training and
then decreases apparently. This decline indicates the effectiveness of gradient perturbation.
Baselines analysis (i) Lagrangian methods can find feasible or near-feasible policies in most tasks,
but their performance on Lvl2 tasks are unsatisfactory (ii) CPO is fails to find feasible or even near-
feasible policy, this is because reaching feasibility region in safetyGym is challenging. (iii) As a
deputy of existing Pareto approaches, MGDA’s final policy has biased performance in reward and
cost, which is either high reward and high cost or low reward and low cost.
8
Under review as a conference paper at ICLR 2022
Ablation study By comparing the performance of CONTROL and its variants, we can find that:
(i) both gradient recalibration techniques is vital and effective for CONTROL (ii) the overall perfor-
mance of CONTROL-P is better than CONTROL-R.
Adaptability analysis To demonstrate the adaptability of CONTROL, we compare it with La-
grangian methods under another threshold (<50). We can observe that our method shows consistent
ability in satisfying predefined threshold and pursuing rewards.
6	Related Work
6.1	Constrained Reinforcement Learning
Constrained Reinforcement Learning is a generalized RL with regard to constraints in the environ-
ment. Conventionally, CRL is formulated as CMDP (Altman, 1999), in which the environment
returns both a reward and non-negative costs state-wise. Such problems could be solved by Lin-
ear Programs when the set of states and actions are finite (see Chapter1.6 in Altman (1999)). But
CMDPs with more complex environments are very tricky to handle.
CRL is broadly leveraged in several real-world applications, such as networks (Hou & Zhao, 2017),
smart grids (Gao et al., 2020), and robotics (Dalal et al., 2018). Among all CRL scenarios, safety is
the most common constraint. Safety CRL (Sui et al., 2015; Wachi et al., 2018) has more strict de-
mand in constraints, which also raises the problem of safe exploration(Moldovan & Abbeel, 2012).
As aforementioned, mainstreams of the CRL literature are (i) Lagrangian methods (Borkar, 2005;
Tessler et al., 2018; Stooke et al., 2020); (ii) Trust Region methods (Achiam et al., 2017; Yang
et al., 2019). Besides, model-based CRL methods (Chow et al., 2017; Berkenkamp et al., 2017;
Wachi & Sui, 2020) are worthy of being mentioned, which guarantee agents to explore in states
with traceable from known low-cost states. Notably, Chow et al. (2018; 2019) utilizes Lyapunov
functions techniques to analyze the stability of dynamical systems as safety constraints. However,
most model-based CRL algorithms are restricted to discrete-action domains for their value-based
modeling to environments.
6.2	Pareto Optimization
At present, Pareto optimizing (Fliege & Svaiter, 2000; Desideri, 2012) provides a novel and time-
economical way to solve multi-objective optimization problem by returning gradient descent direc-
tions that are beneficial to all objectives. This gradient is a linear combination of gradients of each
objective, whose weights, called Pareto weight, are computed alongside the training process. Sener
& KoltUn (2018) first adapted the Pareto optimizer in Desideri (2012) to deep learning by designing
an approximate solver of Pareto weights. Similarly, Lin et al. (2019) improved Fliege & Svaiter
(2000) in order to comply with mUlti-objective optimization problem with preference vector.
7	Conclusion
In this paper, we introdUced a novel CRL paradigm named CONTROL from the perspective of
Pareto optimization. The main challenges of applying existing Pareto approaches are imbalanced im-
provement over reward and cost and incapability of escaping from trivial Pareto-optimal policy. To
overcome these challenges, we devise two gradient recalibration techniqUes, gradient re-balancing
and gradient pertUrbation. To be specific, gradient re-balancing redefines original calcUlation method
of Pareto weight and distribUtes more weights to Underdeveloped objective; while gradient pertUr-
bation empowers Us to temporarily sacrifice retUrn to save costs when necessary. Experiments on
a CRL benchmark, SafetyGym, validate the sUperiority of CONTROL and demonstrate a stable
performance in satisfying constraint.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
International Conference on Machine Learning, pp. 22-31. PMLR, 2017.
Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Larry Armijo. Minimization of functions having lipschitz continuous first partial derivatives. Pacific
Journal of mathematics, 16(1):1-3, 1966.
Felix Berkenkamp, Matteo Turchetta, Angela P Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In NIPS, 2017.
Vivek S Borkar. An actor-critic algorithm for constrained markov decision processes. Systems &
control letters, 54(3):207-213, 2005.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient
normalization for adaptive loss balancing in deep multitask networks. In International Conference
on Machine Learning, pp. 794-803. PMLR, 2018.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070-6120, 2017.
Yinlam Chow, Ofir Nachum, Edgar A Duenez-Guzman, and Mohammad Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. In NeurIPS, 2018.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. In International
Conference on Learning Representations, 2019.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
Kalyanmoy Deb. Multi-objective optimization. In Search methodologies, pp. 403-449. Springer,
2014.
Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization.
Comptes Rendus Mathematique, 350(5-6):313-318, 2012.
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo R Jovanovic. Natural policy gradient
primal-dual method for constrained markov decision processes. In NeurIPS, 2020.
Jorg Fliege and Benar Fux Svaiter. Steepest descent methods for multicriteria optimization. Mathe-
matical methods of operations research, 51(3):479-494, 2000.
Yuanqi Gao, Wei Wang, Jie Shi, and Nanpeng Yu. Batch-constrained reinforcement learning for
dynamic distribution network reconfiguration. IEEE Transactions on Smart Grid, 11(6):5357-
5369, 2020.
Chen Hou and Qianchuan Zhao. Optimization of web service-based control system for balance
between network traffic and delay. IEEE Transactions on Automation Science and Engineering,
15(3):1152-1162, 2017.
Sham M Kakade. A natural policy gradient. Advances in neural information processing systems,
14, 2001.
Qi Kong, Liangliang Zhang, and Xin Xu. Constrained policy optimization algorithm for autonomous
driving via reinforcement learning. In 2021 6th International Conference on Image, Vision and
Computing (ICIVC), pp. 378-383. IEEE, 2021.
10
Under review as a conference paper at ICLR 2022
Xiao Lin, Hongjie Chen, Changhua Pei, Fei Sun, Xuanji Xiao, Hanxiao Sun, Yongfeng Zhang,
Wenwu Ou, and Peng Jiang. A pareto-efficient algorithm for multiple objective optimization in
e-commerce recommendation. In Proceedings of the 13th ACM Conference on Recommender
Systems,pp. 20-28, 2019.
Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient de-
scent with controlled ascent in pareto optimization. In International Conference on Machine
Learning, pp. 6597-6607. PMLR, 2020.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In
ICML, 2012.
Vilfredo Pareto. The new theories of economics. Journal of political economy, 5(4):485-502, 1897.
John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987
International Conference on Neural Information Processing Systems, pp. 612-621, 1987.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement
learning. arXiv preprint arXiv:1910.01708, 2019.
Harsh Satija, Philip Amortila, and Joelle Pineau. Constrained markov decision processes via back-
ward value functions. In International Conference on Machine Learning, pp. 8502-8511. PMLR,
2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897. PMLR,
2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In Proceedings of the
International Conference on Learning Representations (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems, pp. 525-536,
2018.
Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning
by pid lagrangian methods. In International Conference on Machine Learning, pp. 9133-9143.
PMLR, 2020.
Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with
gaussian processes. In International Conference on Machine Learning, pp. 997-1005. PMLR,
2015.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
International Conference on Learning Representations, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033.
IEEE, 2012.
Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision pro-
cesses. In International Conference on Machine Learning, pp. 9797-9806. PMLR, 2020.
Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe exploration and optimization of
constrained mdps using gaussian processes. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based
constrained policy optimization. In International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2022
A Assumptions
For the purpose of simplifying analysis, we make following assumptions:
Assumption 1. Vω Jr(∏s ), Nω JC (∏ω) ∈ Rk exist ∀ω ∈ Rk.
Assumption 2. The feasible policy set is not empty, i.e. ∃ a policy π s.t. JC (π) ≥ ζ.
Assumption 3. ∀ω* ∈ Rk which is a local optima in Jr(∏s), exists a neighborhood U ⊂ Rk of ω
s.t. ∀ω0 ∈ U, JR(πω) > JR(πω0 ).
Assumption 4. ∀ω* ∈ Rk which is a local optima in JC (∏ω), exists a neighborhood U ⊂ Rk of ω
s.t. ∀ω0 ∈ U, JC (πω ) > JC (πω0 ).
Assumption 5. An conservative policy πc which yields no reward and cost always exist and is
accessible.
To analyze all assumptions, Assumption 1 is common in Deep Learning and guarantees the existence
of gradient; Assumption 2 is the weakest prerequisite to ensure that a solution to a CMDP exists;
Assumption 3 and Assumption 4 are requirements for completeness proof in Section B. Assumption
5 is common in many CMDPs, where a policy to stay and make no actions is πc .
B	Pareto - optimal searching in CRL
In this section, We first re-elaborate the algorithms proposed in Fliege & Svaiter (2000) and Desideri
(2012)with concise proofs for their effectiveness under the settings of CRL problem. Furthermore,
We shoW that those tWo methods are highly similar fundamentally. Notably, existing methods can
only find Pareto-stationary policy, Which Would be defined later. For completeness, we further
prove that each Pareto-stationary policy is a Pareto-optimal policy with possibility 1 under
Assumption 3,4 to CRL.
Definition B.1 (Pareto-stationary Policy). For a policy πω, πω is Pareto stationary i.i.f. @ a Pareto
direction v ofπω s.t. hVωJR, vi > 0 and hVωJC, vi > 0.
As a necessary condition of Pareto-optimal, Pareto-stationary supplies a stronger condition for
Pareto directions by strict inequations. Moreover, if a policy πω is not Pareto-stationary,
VωJR(πω), VωJC(πω) 6= 0, otherWise hVωJR,vi = 0 or hVωJR, vi = 0 holds ∀v ∈ Rk.
B.1 Steepest descent method (Fliege & Svaiter, 2000) in CRL
As mentioned in Section 2, To search a Pareto optimal policy, the key is designing an iteration
ω0 = ω + η(ω)∆(w), Where ∆(ω) is update vector, and η(ω) ∈ R+ is the stepsize. In Steepest
Descent Methods (SDM), η ∈ R+ is obtained by line backtracking searching in a Armijo-Goldstein
Way(Armijo, 1966), We omit this part because it is not concerned in CONTROL. NoW We are pre-
senting hoW SDM searches ∆(ω).
For a vector v ∈ Rk, define a function fω (v):
fω (v) := min(hVω JR(πω ), vi, hVω JC (πω ), vi).	(9)
Then ∆(ω) is one of the solution of the problem beloW:
max fω(v) - 1 ||v||2.	(10)
v2
Theorem B.1. Ifπω is Pareto-stationary, then ∆(ω) is unique and ∆(ω) = 0 ∈ Rk. Otherwise,
each ∆(ω) is a Pareto direction ofπω.
Proof. As defined, ∆(ω) is the solution of Problem(10). According to Definition B.1, if πω is
Pareto-stationary, then maxv fω (v) ≤ 0. Thus:
max fω(v) — 1 ∣∣v∣∣2 ≤ max fω(v) + max ( — 1 ||v||2) ≤ 0 + 0 = 0.	(11)
v	2	v	v2
12
Under review as a conference paper at ICLR 2022
Note that when V = 0, fω(V) - 1 ||v||2 = 0. So maxv fω(V) - 1 ||v||2 = 0. In addition,
∀v = 0, maxv fω(V) - 1 ∣∣v∣∣2≤ maxv -2∣∣v∣∣2 < 0, so 0 is the only solution, i.e. ∆(ω) = 0.
Now let’s consider the situation when πω is not Pareto-stationary, where ∃V s.t. fω (V) > 0.
For a randomly selected δ ∈ (0, fj(v)):
fω(δv)- 2∣∣δv∣∣2 = δ(fω(v) - 2∣∣V∣∣2) > 0
fω OQ))- 1 11δ3)H2 = max fω (V)- 1 ||v||2 › 0
2	v2
Then fω(∆(ω)) > 2∣∣∆(ω)∣∣2 ≥ 0, which means hVω JR(∏ω), V,Oω JC(∏ω), Vi > 0. Thus,
∆(ω) is a Pareto direction of ∏ω by Definition 2.3.	□
B.2 MULTIPLE-GRADIENT DESCENT ALGORITHM (DESIDERI, 2012) IN CRL
Multiple-gradient descent algorithm (MGDA) is introduced in Section 2.2, which attains Pareto
weights by solving:
min	∣∣βRVω Jr (∏ω )+ βc Vω JC (∏ω)∣∣2,
βR ,βC ∈R
s.t. βR,βC ≥0, βR+βC =1.
According to the Chapter.4 of Boyd et al. (2004), the Problem (4) is a convex optimization problem,
so the solution β* = (βR,βC) ∈ R2 exists. Furthermore, if Vω Jr(∏s), Vω JC(∏ω) = 0, then
Problem (4) is strongly convex and β* is unique.
After finding the solution of this problem, i.e. β*, MGDA determine the Pareto direction ∆(ω) as
βR Vω JR (∏ω )+ βC Vω JC (∏ω ).
Theorem B.2. If πω0 is a Pareto-stationary policy then ∆(ω0) = 0.
Proof. If Vω0 JR(πω0) = 0 or Vω0 JC (πω0) = 0, then (1, 0) or (0, 1) is a trivial solution of Prob-
lem (4).
Now let us discuss the situation that Vω√ JR(∏ω√), Vω√ JC(∏ω√) = 0. Under such conditions, β*
is unique. Since πω0 is Pareto-stationary, by definition ω is one of the solution of the following
problem:
min - Jr0 (πω )
ω∈U
s.t.	JR(πω) ≥ JR(πω0), JC(πω) ≥ JC (πω0)
(12)
We can write the Lagrangian of Problem (12):
L(ω, μ) = -JR(πω ) - μR [ JR(πω ) - JR(πω0)] - MC [ JC (πω ) - JC (πω0 )]	(13)
where μ = (μR, μ0) ∈ Rj is the vector of Lagrange multipliers. By KarUSh-Kuhn-Tucker( KKT)
conditions, for a saddle point (ω0, μ*), where μ*(μR, μC) ∈ Rj, we have:
∂L
∂ω lω=ω0 =0
⇒Vω0 JR(∏ω0 ) + μR Vω0 Jr (限，)+ μC'ω^C (∏ω0 ) = 0
1 + 〃*	L
⇒ 1 + μ+ μC μ* V3'JR(n30) + X
+ μ R + μ C	j=1
*
μC
(14)
1 + μR + μC
Vω0 JC (πω0 ) = 0
AC I I I I 2	'^'> ∩ Q∏∕^l f kα iin； CllanaC C Cf /=?* (	1+μC	μC	、； C fkα Cnl ∖τ Cdlif ；Cn Cf Drchl0^m (1 Λ ʌ
AS || ∙ ||2	≥ 0 and the uniqueness of P ,(]+从* +从*	,]+从* +μ*	) is the only solution of Problem (14).
rτ->ι	A/	八 1 + μ ‰ L7 T / X I	μ‰	L7 T /	∖ C
Then ∆(ω ) = 1+μR +μC VJ JR(nJ ) + 1+μR+μC V" JC (n" ) = 0.
□
13
Under review as a conference paper at ICLR 2022
Theorem B.3. If πω0 is not a Pareto stationary policy, then ∆(ω0) 6= 0 and ∆(ω0) is a Pareto
direction of πω0.
Proof. We first prove that ∆(ω0) = 0. By definition, ∃v ∈ Rk s.t.hVω Jr, Vi > 0, hVω Jc, Vi > 0.
So hv, ∆(ω0)i = βRhVω0 Jr(∏ωθ),vi > 0 + βchVω0 Jc(∏ωθ),vi > 0, which implies ∆(ω0) = 0.
The Lagrangian of the Problem 4 is :
L(β, λ, μ) = IleRVω0JR (π1 ) + βC V ω0 JC (π1 )||2 + λ(βR + Be - I)- 2R 8r - μC Be
where λ,μ = (μR, μc) are Lagrange multipliers and μR, μc ≥ 0, MrBr, μcβc = 0.
Say (β*, λ*,μ*) is a saddle point of above problem, by KKT conditions, We have:
[2∆(ω0) ∙ Vω0 Jr (∏ω0) + λ* - μR = 0,
( 2δ(J) ∙ vs0 JC (πω0) + λ* - μC = 0,
[μR, μC ≥ 0,μRβR = μCβC = 0.
Multiply the first two equations with corresponding BR, BC and sum:
2∆(ω0) ∙ ∆(ω0) + λ(βR + βC) - μRβR - μCβC = 0.
Since μ*β* = 0 and βR + βC = 1, we have λ* = -2∣∣∆(ω0)∣∣2. Note that ∆(ω0) = 0, so λ* < 0.
Thus:
h∆(ω0), Vω0 JR(∏ω0)i = μR	- λ*	≥ -λ*	>	0,	(15)
h∆(ω0), Vω0 Jc(∏ω0)i = μC	- λ*	≥-λ*	>	0,	()
By definition, ∆ω√ is a Pareto direction of ∏ω√.	□
MGDA could be very efficiency because Problem (4) has explicit solution:
0,	B0 < 0
βR=e βo,	0 ≤ βo ≤ ι
1,	B0 > 1
βC = 1 - βR,
(16)
where B0
Vω JC (∏ω ) ∙ (Vω JC (∏ω ) - -V Jr(∏3 ))
∣∣Vω JC (∏ω )-Vω Jr(∏3 )∣∣2
B.3 Discussion of existing methods
SDM and MGDA are basically solving the same optimization problem. Actually, if we transform
the Problem (10) into a constrained optimization problem:
max α - 1 ||v||2	(17)
s.t.	hVωJR(πω),vi ≥ α, hVωJC(πω),vi ≥ α.
It is easy to prove that Problem (17) and Problem (4) are primal-dual problem. As the Slater condi-
tion holds, these two optimization will yield identical optima (Boyd et al., 2004).
In addition, Lemma 1 in Fliege & Svaiter (2000) also points that the Pareto direction ∆(ω) derived
from Problem (17) and Problem (4) is the steepest gradient in MOOP settings.
In the body part, we choose MGDA to represent Pareto-optimal searching algorithm. Because
MGDA is more efficient and comprehensible.
14
Under review as a conference paper at ICLR 2022
B.4 Completeness proof
So far we can only search Pareto-stationary policy instead of Pareto-optimal. To fill the gap, we
make some proof for completeness.
Lemma B.4. For a policy ∏ω, if Vω Jr(∏s ) or Vω JC (∏ω) is 0, then ∏ω is Pareto-optimal.
Proof. If Vω Jr (∏ω) = 0, then ω is a local optima as for J∏ω. According to Assumption 4, there
is a neighborhood U ⊂ Rk of ω s.t. ∀ω0 ∈ U, JC(πω) > JC (πω0 ). Then πω is Pareto-optimal since
∀ω0 ∈ UJC(∏ω) * JC(∏ω). It is similar to the case of Vω JC(∏ω) = 0.	□
Theorem B.5. Ifa policy πω is Pareto-stationary, it is Pareto-optimal with possibility 1.
Proof. Suppose we have a policy πω0 which is Pareto-stationary but not Pareto-optimal. Then, by
Lemma B.4 we know VωJR(πω), VωJC(πω) = 0, otherwise πω0 is Pareto-optimal.
By solving Problem (4), we can obtain a Pareto direction ∆(ω0). By Theorem (B.2), ∆(ω0) = 0,
which implies that VωJR(πω), VωJC(πω) are co-linear.
Note that the reward and cost are independent, which means we can regard Vω JR(πω ), Vω JC (πω )
as random vectors with regard to ω. As our model has many parameters, i.e. k >> 2, the possibility
that VωJR(πω), VωJC(πω) are co-linear is 0.
Thus, a Pareto-stationary ∏ω√ is Pareto-optimal with possibility 1.	□
C Connections to prior works
In this section, we prove that existing CRL algorithms are fundamentally searching Pareto optimal
policy. In other words, if these algorithms converge at a policy πω , then πω is Pareto-optimal, or
even Pareto-feasible when constraint is satisfied in practical. The proofs below follow the notation
and problem definition in Section 2.
C.1 Linear s calarization
Linear scalarization method aggregate all optimizing targets into one by linear summation with
predefined weight vector λ = (λR, λC) ∈ R2+. So now the optimizing problem is:
max λRJR(πω) + λCJC(πω).	(18)
ω
Proposition C.1. If ω* is one of a local optima of above problem, then ∏ω* is Pareto optimal.
Proof. Assume ∏ω* is one of the local optima of Problem (18) but not Pareto optimal, then ∃ω0 ∈ U
s.t. ∏ω0 A ∏ω*, where U ∈ Rk is a neighborhood of ω*. Then, Xr Jr(∏3o) + λ0 JC(∏ωo) >
λRJR(πω* ) + λC JC (πω* ), which is contradict with the original assumption. Thus, the original
proposition holds.	□
This proposition suggests that accessing a Pareto optimal policy is simple and feasible. However,
the policy generated by linear scalarization is strongly related to the choose of λR, λC and may not
satisfies the constraint.
C.2 Lagrangian methods
Lagrangian methods (Tessler et al., 2018; Ray et al., 2019) transform a constrained optimization
problem into a normal min-max optimization problem by adding Lagrange multipliers λ ∈ R+ :
max min L(π, λ) := Jr(π) + λ ∙ (JC(π) — Z),
π λ≥0
s.t. JC (π) ≥ ζ.
(19)
To solve this problem, we can apply gradient ascend on π ‘s parameters and gradient descent on λ.
of which the convergence proof is in (Platt & Barr, 1987). Similarly, we have:
15
Under review as a conference paper at ICLR 2022
Proposition C.2. If π*, λ* is one ofthe solution ofProblem (19), then π* is Pareto-optimal.
Proof. Assume (∏ω*,λ*) is one of the saddle points of above problem but ∏ω* is not Pareto optimal,
then ∃ω0 ∈ U s.t. ∏ω√ * ∏ω*, where U ∈ Rk is a neighborhood of ω*. Then:
JR(πω0 ) + λ* ∙ (JC (πω0 ) - Z) ≥ JR (πω* ) + λ ∙ (JC (πω* ) - Z) ∙
This suggests that (∏ω*, λ*) is not a saddle point, which is contradict with original assumption. □
Though provide a strong reliable guarantee to satisfy constraint, Lagrangian methods may be too
conservative to explore effectively. This is because λ would control the optimization when (JC(π) -
Z) is big, which is common at the beginning of the CRL training session.
C.3 Constrained Policy optimization (Achiam et al., 2017)
Constrained Policy optimization (CPO) is a CRL algorithm by updating policy within a constraint
satisfying region. It updates policy by solving:
πk+1 =argmaχ Es〜∏k,a〜∏ [ARk (s, a)]
s.t. JCi (∏k) - Es〜∏k,a〜∏ [ACk (s,α)] ≥ ζ,	QO)
DKL (∏k∏k) ≤ δ.
To prove that CPO is searching Pareto-optimal policy, we need some basic conclusion in RL:
Lemma C.1 (Kakade (2001)). Given an existing policy πold, then:
Jr(∏) - JR(∏oid) = Es〜∏,a〜∏ [ARold(s,α)],
JC (∏) - JC (∏old) = Es〜∏,a〜∏ [—AClld (s, a)].
Lemma C.2 (Theorem 1 in Schulman et al. (2015)). Let E = maxs∣Ea〜∏ [Aπold(s, a)] |,then:
Es〜Π,a〜∏ [ARold (s, a)] > Es〜∏oid,a〜∏ [ARold (s, a)] 一 C max DKL (∏old, ∏),
Es〜π,a〜π [ACθld (s,a)] > Es~∏oid,a~π [AπCold (s, a)] - C msax DKs L (πold, π) .
where C = 4eγ∕(1 — Y )2 and DK L denotes the Kullback-Leibler divergence of two policies when
making decisions in state s.
Proposition C.3. If CPO converges at πω* , then πω* is Pareto-optimal.
Proof. Suppose πω* is Pareto-optimal is not Pareto-optimal, then ∃ω0 ∈ U s.t. πω0 * πω* , where
U ∈ Rk is a neighborhood of ω*.
The original Problem (20) can absorb one constraint, and transform into:
Πk+1 = arg max Es〜心,0〜∏ [ARk (s,a)] 一 C maxDKL (∏k,∏),
s.t. JCi (πk) - Es〜∏k ,a〜π[AπCk (s, a)] ≥Z,
where the C is consistent with Lemma C.2. As CPO converges at πω*, so:
∏ω* = arg max Es 〜∏ω* ,a 〜∏ [ARω* (s,a)] 一 C max Dss L (∏ω*,∏) .
πs
Since Es〜∏ω* ,a〜∏ω* [ARω* (s, a)] — Cmaxs DKL (∏ω* ,∏ω*) = 0, then:
Es〜πω* ,a〜∏ω0 [ARω* (s,a)] — CmaxDKL (∏ω* ,∏ω0 ) ≤ 0∙
So we have:
JR(πω0 ) — JR(πω* )
=Es〜∏ωθ,ɑ〜∏ω0 [AR” (s, a)]	LemmaC.1
< —Es 〜∏ω0,a 〜∏ω* [ARω0 (s,a)] + CmaxDK L (∏ω0 ,∏ω* )	LemmaC.2
=Es〜∏ω* ,a〜∏J [ARω* (s, a)] — CmaxDKL (∏ω* ,∏ω0) ≤ 0 Formula (23)
s
The Formula (24) is contradict to πω0 * πomega* , thus πω* is Pareto-optimal.
(21)
(22)
(23)
(24)
□
16
Under review as a conference paper at ICLR 2022
D Proofs about CONTROL
D.1 Proofs for Gradient Normalization
Lemma 4.1. If ∏ω is not Pareto-optimal, Δn(ω) := βR Vω JR(∏ω) + βN Vω JC(∏ω) is a Pareto
direction ofπω.
Proof. As a Pareto-optimal policy, πω is Pareto-stationary. By Theorem B.3, we know that
h∆N(ω),VωNJR(πω)i > 0andh∆N(ω),VωNJC(πω)i > 0.
Thus, (Δn(ω), VN JR(∏ω)i > 0 and (Δn(ω), VN JC(∏ω)i > 0. By definition, Δn(ω) is a Pareto
direction of ∏ω.	□
Before proceeding to prove Theorem 4.2, we need a lemma to estimate JR(πω0 ) - JR(πω) and
JC (πω0 ) - JC (πω).
Lemma D.1. With the iteration paradigm ω0 = ω + η(ω)∆(ω), if η(ω) → 0, then JR(πω0) -
JR(πω) = η(ω)h∆(ω), VωJR(πω)i and JC (πω0) - JC(πω) = η(ω)h∆(ω), Vω JC (πω)i.
Proof. We only need to prove one of JR(πω ), JC (πω ), because the other one can be proved in a
similar way. Make a first-order Taylor expansion of JR(∏ω) at ω*:
JR (πω0 ) = JR(πω ) + (ω0 - ω)，Vs JR(πω ) + O [(ω0 - ω)2],
where O (ω0 - ω)2 → 0 if ω0 - ω → 0. Apply Gram-Schmidt orthogonalization to ∆(ω), we
h∆(ω), Vω JR(πω )i	⊥
have ∆(ω) = ---——-——ʌ,, ,, ʌ z ʌ,, VωJR(∏ω) + cVR(ω), where C is a coefficient we are not
l∣vω JRH)II2∣∣δ3∣∣2	r ω r
interested and VR⊥(ω) is a vector orthogonal to VωJR(πω). Thus:
JR(πω0) - JR(πω ) = +(ω0 - ω) ∙ Vs JR(πω ) + O [(ω0 - ω)2]
η(ω)
「(V：JXR(∏ω)iv3Jr(πω) + cv⊥3)] "ωJRg
(25)
+ O [(ω0 - ω)2]
= η(ω)h∆(ω), VωJR(πω)i + O [(ω0 - ω)2] .
Since η(ω) → 0 ⇒ ω0 — ω → 0, then We have: Jr(∏sο) — JR(∏ω) = η(ω)(∆(ω), Vω JR(∏ω)i∙
Similarly, we can infer JC(∏ω0) — JC(∏ω) = η(ω)(∆(ω), Vω JC(∏ω)〉.	□
Theorem 4.2 Suppose the iteration paradigm is ω0 = ω + η(ω)∆N (ω) and η(ω) → 0. Then:
(i) if we use ∆(ω) derived from Problem (4), the improvements in reward and cost are similar.
Specifically, when βR ∈ (0,1), ----------------——-→ 1. (ii) if we use ∆N (ω) derived from Prob-
JC (πω0 ) — JC (πω )
lem (5), the improvements in reward and cost are proportional to the square length of corresponding
gradient. Specifically, when βR ∈ (0,1), [R,∏ω ]——[R,ω ] →
JC (πω0 ) — JC (πω )
llvω JR 5ω)ll2
I∣Vω JC (∏ω )∣∣2.
Proof. By Lemma D.1, we have:
JR(∏ω0 ) — Jr (∏ω ) η(ω)h∆(ω), Vω Jr (∏ω )i _ h∆(ω), Vω Jr(∏3 )i
JC (∏ω0 )— JC (∏ω ) → η(ω)h∆(ω), Vω JC (∏ω )i = <∆(ω), Vω JC (∏ω )i ,
(26)
(i)	when βR ∈ (0,1), ∆(ω) is perpendicular to Vω Jr(∏3) 一 Vω JC(∏ω), which indicates that
h∆(ω), Vω JR (πω ) — Vω JC (πω )i = 0. Thus:
JR (∏ωθ ) — JR(∏ω) → (∆(ω), Vω JRm =]
JC (∏ω0 ) — JC (∏ω )	h∆(ω), Vω JC (∏ω )i	.
(27)
17
Under review as a conference paper at ICLR 2022
(ii)	when βR ∈ (0,1), ∆(ω) is perpendicular to VN Jn(∏ω) - VN JC(∏ω), which indicates that
h∆(ω), VN JR(∏ω) - VN JC (∏ω)i = 0. Thus:
jR(∏ωθ ) —	JR(∏ω ) h∆(ω),	VN Jκ(∏ω)i= ∣∣Vω Jr (∏ω ) ||2 (∆(ω),	Vω JR (∏ω)i= ∣∣Vω Jfi(∏ω )||2
JC (∏ωθ) -	JC (∏ω)	→ h∆(ω),	VN JC (∏ω)i	= ∣∣Vω JC (∏ω ) ||2 (∆(ω),	Vω JC (∏ω)i	= WzJCKlI,
(28)
□
D.2 Proofs for Gradient Perturbation
Lemma D.2 (Theorem」in Schulman et al. (2015)). Let E = max§ |EΟ〜∏ω0 [Aπω (s, a)] |, then:
JC (πω0 ) - JC (πω ) ≥ Es〜πω,a〜πωo [—ACT (St, at)] - CDmLx (πω0, πω ) ,	(29)
where C = 4eγ∕(1 — γ)2.
Theorem 4.3 (The lower bound of JC (π) improvement) Given the parameter updating paradigm
ω0 = ω + η(ω)∆N (ω) and clipping threshold t, we have the lower bound for JC (πω0 ) - JC (πω):
JC (πω0 ) - JC (πω ) ≥
η(ω) [t∣∣Vω JC (∏ω )∣∣2hVN JR(∏ω) - VN JC (∏ω), VN JC (∏ω)i + l] - CDmLx (醍，,∏
ω), (30)
where C = 4eγ∕(1 — γ)2 and E = maxs∣Ea〜∏ω, [Aπω (s, a)] |. And this bound is strictly positive
related to t.
Proof. By Lemma D.2,
JC (πω0 ) - JC (πω ) ≥
e∏ω ,a~∏ω0 [-AπCω (St, at)] -
Es〜∏
ω0,a 〜∏ω0 [-ACω (St, at)] - CDmLx (πω0, πω ) ∙()
The C in last inequation is consistent with the setting in Lemma D.2. Since Vω JC (πω ) is differen-
tiated from Es〜∏ω,a〜∏ω0 [-ACω (st, at)], consider Lemma D.1, then:
JC (πω0 ) - JC (πω )
≥η(ω)h∆N(ω), VωJC(πω)i -CDKmLax(πω0,πω)
=η(ω)hmin(t, βRN)VωN JR(πω) + [1 - min(t, βRN)] VωN JC (πω), VωJC(πω)i - CDKmLax (πω0,πω)
≥η(ω)htVωJR(πω) + (1 -t)VωJC(πω),VωJC(πω)i -CDKmLax(πω0,πω)
=η(ω) [t∣∣Vω Jc (∏ω )∣∣2hVN Jr (∏ω ) - VN JC (∏ω ), VN JC (∏ω )i + l] - CDmLx (限，,∏ω )
(32)
This bound is sensitive to both a and η(ω), and it strictly positive correlated with t, because accord-
ing to Eq (16),〈VN Jr(∏z) - VN JC(∏ω), VN JC(∏ω)i is strictly positive. Otherwise, βN = 0,
the clipping is not working.	□
This lower bound is consistent with the lower bound when clipping is not working (i.e. For-
mula (D.2)) and tighter when clipping is working (i.e. βRN > t) and we perturb the ∆N (ω) to
bias to VωJC(πω).
18
Under review as a conference paper at ICLR 2022
E Pseudo-code of CONTROL
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
Algorithm 1: CONTROL
input : Threshold co for cumulative cost, initial clipping threshold to, mean cumulative reward
of last epoch rLE, mean cumulative cost of last epoch cLE
Initialize actor parameters ω, cost critic θC, reward critic θR randomly;
Initialize to → 0.5, rLE → 0, cLE → 0 for B = 0, 1, ... do
for E = 0, 1, ...N - 1 do
Sample d episodes {τ^ι,τ2, ...τd},τi 〜 πω;
Record mean cumulative reward and cost r, c in {τι, τ2,…τd};
Update θR , θC by MSE loss;
Obtain Vω Jr(∏3 ), Nω JC (∏ω) by Eq (8);
Obtain βRN, βCN by solving Problem (5);
if ^ > Co and c > Cle；	// Constraint is not satisfied and c is worse
then
βRN → min(t, βRN), βCN → 1 - βRN ;	// Clip βRN
Update ω with gradient βRNVωJR(πω) + βCNVωJC(πω);
to → to - t0;	// Decrease clipping threshold
IrLE → r, CLE → C
else if C < Co and CLE < Co and r < Tle ；	// Constraint is satisfied in a
row and r is worse
then
βCN → min(t, βCN), βRN → 1 - βCN ;	// Clip βCN
Update ω with gradient βRNVωJR(πω) + βCNVωJC(πω);
to → to + t0;	// Increase clipping threshold
IrLE → r, CLE → C
else
Update ω with gradient βRVω Jκ(∏ω) + βCVω JC(∏ω);	// No clipping
IrLE → r, CLE → C
end
end
end
return : A policy πω
19
Under review as a conference paper at ICLR 2022
F	Experiment Details
F.1 Architecture details
CONTROL is built on an Actor-Critic framework. To model state value functions with regard to
rewards and costs separately, we adopt two critics. For both actor and critic networks, we use
identical structure, which is an MLP with a 64-dimension hidden layer. Except the output layer, we
apply tanh() as the activation function. For all code-level implementation we used PyTorch.
F.2 Hyper-parameters
Model training For learning rate, we choose 0.0012 for the actor and 0.001 for the critics. Once
sample enough episodes, we reuse the buffers for 50 times. To adapt MDPs in SafetyGym, which
is with continuous states and actions, the output of the actor network is all means of a Multivariate
Gaussian distribution, and the standard deviation is locked as 0.6. While in testing, we resize all
standard deviation to 0.4 to low down the agent’s desire to explore. Besides, we apply GAE (Schul-
man et al., 2016) to estimate advantage with γ = 0.99 and λ = 0.95.
Clipping threshold evolving We initialize the clipping threshold t0 as 0.5, and check if we should
update it every 100000 steps. If so, we increase or decrease it by 0.05.
20