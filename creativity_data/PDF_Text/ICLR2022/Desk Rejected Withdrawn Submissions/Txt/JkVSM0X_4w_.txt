Under review as a conference paper at ICLR 2022
Switch Spaces: Learning Product Spaces with
Sparse Gating
Anonymous authors
Paper under double-blind review
Ab stract
Aligning the geometric inductive bias well with the underlying structure of data is
critical for representation learning. To achieve this goal, we propose switch spaces,
a data-driven representation learning approach. Switch space is a generalization of
product space which is composed of multiple euclidean and non-euclidean (e.g.,
hyperbolic, spherical) spaces. Given N spaces, our model utilizes a sparse gating
mechanism to let each input data point choose K(K V N) spaces and combines
them to form a product space. In doing so, NCK product spaces are generated
automatically in a single model and each input data point is processed by one
of them with greater specialization, making the spaces switchable. In addition,
switch space models are efficient and have a constant computational complexity
regardless of the model size. We apply switch spaces to the knowledge graph (KG)
completion task and propose SwisE which obtains state-of-the-art performance
on benchmark KG datasets. We also show that switch space can help achieve
promising results on the item recommendation task. Model analysis is conducted
to inspect the inner workings of switch space.
1	Introduction
Many machine learning applications involve representation learning (Bengio et al., 2013). Euclidean
and recently non-Euclidean spaces have demonstrated successful applications in modeling data with
certain structures in various fields. Euclidean space has been extensively studied in the literature
and has been serving as the workhorse for decades in representation learning (Bordes et al., 2013;
Mikolov et al., 2013; Hsieh et al., 2017). Non-Euclidean space has gained increasing attention as
it excels at modeling various structural patterns in data (Wilson et al., 2014; Nickel & Kiela, 2017;
Chami et al., 2020b; Vinh Tran et al., 2020; Meng et al., 2019; Ganea et al., 2018). For example,
hyperbolic space, as a continuous version of discrete tree, is more suitable for modeling hierarchies
while spherical space is fitting for cyclical structures.
Recently, efforts have been made to combine multiple different types of spaces (i.e., Euclidean,
spherical, hyperbolic) into product spaces for representation learning. Nevertheless, we argue that
aligning the geometry of the input data with its suitable space is a nontrivial task and product spaces
lack this capability. To be specific, after the signatures of product spaces are determined (e.g.,
spherical space X hyperbolic space), training samples are used to fit the learnable parameters in aɪl
these spaces, regardless of the appropriateness (e.g., parameters in both spherical and hyperbolic
spaces will be updated even if the input data only aligns with the spherical space). Clearly, data
points with different geometric structures should be handled with different component spaces. It
is important for component space models to calibrate their internal parameters by focusing on the
subset of the training data that fits the manifold and ignoring cases they are not good at modeling.
Nevertheless, manually checking the underlying geometry of real-world data is almost impossible
due to its intricacy and large size. Therefore, we need a model that can automatically allocate data
points to their suitable spaces.
To achieve this goal and better model the structural patterns in data, we propose switch spaces which
can address the weakness of product spaces. Switch spaces are data-driven and can automatically
choose suitable spaces for each input data point based on the input-output relationship. In other words,
we let the input data inform the spaces allocation and choose whatever spaces it favours, resulting
in better specialization. In switch space, a sparse gating mechanism is introduced for automatic
1
Under review as a conference paper at ICLR 2022
spaces selection and, as a result, a number of product spaces with different signatures can be formed
automatically, allowing for better diversity and expressiveness. Since only a certain number of spaces
are active in the calculation for each given data point, the computational cost of switch spaces is
linear to the number of active spaces, thus making building larger models possible.
The contributions of this paper are: (a) We propose switch spaces which learn to choose, combine,
and switch spaces via a sparse gating mechanism. It effectively aligns different types of geometric
spaces with the characteristics of the input data, while remaining computationally efficient; (b)
We propose a KG embedding model SwisE, short for Switch space based KG Embedding, and
it obtains state-of-the-art performance on benchmarking KG datasets, outperforming a number of
recent published baselines; We also demonstrate that switch spaces can enhance the effectiveness of
conventional recommendation models; (c) We conduct model analysis to inspect the inner workings
of switch spaces and study the effects of certain modeling choices.
2	Related Work
Non-Euclidean Geometric Representation Learning Non-Euclidean geometric including hyper-
bolic and spherical representation learning has gained increasing interests in a wide spectrum of
applications (Nickel & Kiela, 2017; Vinh Tran et al., 2020; Tay et al., 2018; Balazevic et al., 2019a;
Chami et al., 2019; Liu et al., 2019; Chami et al., 2020a). Hyperbolic space is reminiscent of
continuous version of trees and excels in modeling hierarchical structures, while spherical space is a
more suitable choice for directional similarity modeling and cyclical-structured data. We can find
applications of spherical spaces in text embeddings (Meng et al., 2019), texture mapping (Wilson
et al., 2014), time-warping functions embedding (Wilson et al., 2014). As indicated, different space
has its own specialities and the choice of spaces varies based on the characteristics of data. To
combine the best of different worlds, product spaces with mixed curvatures are proposed (Gu et al.,
2019; Skopek et al., 2020; Bachmann et al., 2020). The component spaces in product spaces are
selected from Euclidean space, hyperbolic space, and spherical space. Each component of product
space has a constant curvature while the resulting mixed space has non-constant curvature, which
makes it possible to capture a wide range of patterns. Empirically, product spaces demonstrate its
efficacy in graph reconstruction, wording embedding with low dimensions (Gu et al., 2019), node
classification in graphs (Bachmann et al., 2020), and image reconstruction (Skopek et al., 2020).
Mixture of Experts Another related line of work is mixture-of-experts (MOE) (Jacobs et al., 1991;
Jordan & Jacobs, 1994). MOE is established based on the divide-and-conquer principle. It divides
problem into homogeneous regions and an expert is responsible for each region. The final prediction
is arrived based on the cooperation between experts via a gating network. As a controller, the
gating networks control the contribution of each experts via a probabilistic gating function. MOE
has been extensively studied in a range of applications such as language modeling and machine
translation (Shazeer et al., 2017; Fedus et al., 2021), multi-task learning (Ma et al., 2018), etc. Various
MOE architectures have also been proposed in the past decades including hierarchical structure (Yao
et al., 2009), sequential experts (Aljundi et al., 2017), deep MOE (Eigen et al., 2013), sparsely-gated
MOE (Shazeer et al., 2017), etc.
Product space model is designed for data with mixed patterns. Yet, it is unclear how to construct a
product space model that makes data (i.e, triplets in KG) with certain patterns handled by its suitable
space. To enhance the alignment between data and its suitable geometric space and calibrate each
component model, the proposed switch spaces take the advantages of the sparse gating mechanism of
MOE (Shazeer et al., 2017). That is, given a pool of spaces, a sparse gating mechanism is integrated
to activate a few spaces for each given data point, assigning data to its suitable spaces without manual
inspection of the inner geometry of input data. Switch space can be viewed as a generalization of
product space but offers better specialization, higher degree of freedom, and superior expressiveness.
3	Preliminaries: The Geometry of Product Spaces
Stereographic Projection model In general, there are three types of constant curvature spaces with
respect to the sign of the curvature. Common realizations are Euclidean space E (flat), hypersphere
S (positively curved) and hyperboloid H (negatively curved). For the latter two, we prefer their
2
Under review as a conference paper at ICLR 2022
stereographic projection model: projected sphere D and Poincare ball P. These models are easier to
optimize, avoiding the problem of non-convergence of norm of points with curvature close to 0, and
the projection is conformal, i.e., does not affect the angles between points (Nickel & Kiela, 2017;
Skopek et al., 2020).
An alternative to vector space in non-Euclidean geometry is gyrovector space (Ungar, 1991), which
defines operations such as vector addition and multiplication. For D and P (jointly denoted as Mc ,
where C denotes curvature), the addition between two points x, y P Mc, also known as Mobius
addition ‘c (for both signs of c), is defined as:
父方寸=p1	—	ICcxxyy	—	c}y}2qχ	+ p1 +	c}χ}2qy	G
’c y “	1 ´ 2ccχ, yy + C2}χ}2}y}2	,	( )
Where x, y is Euclidean inner product. The distance between points in the gyrovector space is defined
dMc(x, y) = a^tan> P^∣ ´ X ’c 丫但，	⑵
|C|
where tanc stands for tan if C > 0 and tanh if C V 0. In both spaces, We have Euclidean geometry
when C → 0. It is easy to prove that: dMc(x, y) -c-^→ 2}x — y}2, which means the gyrospace
distance converges to Euclidean distance when limiting C to zero.
Let TxMc be the tangent space to the point x P Mc. Mapping between (Euclidean) tangent space
and hyperbolic/spherical space is performed with exponential map: TxMc → Mc and logarithmic
map: Mc → TxMc, which are defined as:
iogχ Py)“a⅛ tanL(M} ´X 'c y}2 q}: ’c y∣h2，
c	(3)
eχpx(V) “ X'c ptanc(arC|x22qaCvivj，
where λcx is a conformal factor, defined as λcx “ 2{(1 + C∣x∣22), which is used to transform the metric
tensors between (Euclidean) tangent space and non-Euclidean space.
Product Spaces The product space is defined as the Carte-
sian product of multiple spaces with varying dimensionality and
curvature. Let P denote a product space composed by N inde-
pendent component spaces Mp1q, Mp2q,...,MpNq. The mixed
space P has the form: P “ XN=I Mpi) “ Mpiq X M⑵ X
...X MpNq. The product space P is also equipped with dis-
tance functions. The squared distance between points x, y P P
is defined as: dp(x, y) “ ∑N=1 4%(”)(xm(z), 丫从(，)),where
xMpiq and yMpiq denote the corresponding vectors on the com-
ponent space Mpiq .
Other operations such as exponential map and logarithmic map
are element-wise, which means that we can decompose the
points into component spaces and apply operations on each
component space and then compose them back (e.g., concatena-
tion) to the product space. The signature (i.e., parametrization)
of a product space refers to the types of space, the dimensional-
ity, and curvature of each space.
Figure 1: Example of a switch
space where N “ 3, K “ 2 and
the initial three spaces are Poincare,
Spherical, and Euclidean spaces. It
generates 3 C2 “ 3 resulting prod-
uct spaces including P X D (or writ-
ten as PD), P X E, and D X E.
4	The Proposed Switch Spaces
4.1	The Switch Space Framework
Suppose we have N spaces of different types and Mpciq P tE, D, Pu, i “ t1, ..., Nu. For simplicity,
we assume all spaces have the same dimensionality, b. The goal of switch spaces is to select
K(1 ≤ K V N) spaces out from the given N spaces for each incoming triple (h,r,t) and the
3
Under review as a conference paper at ICLR 2022
selected K spaces will form a product space. As such, each triple will be handled by different product
space. Theoretically, there will be NCK “ PN£)收! possible combinations of product spaces,
allowing a greater degree of freedom. Figure 1 illustrates a simple example of a switch space which
can generate three product spaces automatically.
The proposed switch spaces usually consist of an embedding layer, a sparse gating network, and a
space combination component. In what follows, we will detail the switch space framework using two
real-world applications, knowledge graph completion and item recommendation.
4.2	Switch Spaces for Knowledge Graph Completion
Knowledge graph has emerged as an effective way to integrate disparate data sources and model
underlying relationships. Encoding the entities and relations of KGs into low-dimensional vector
spaces is essential to downstream applications such as missing facts completion, question answering,
information extraction, and semantic reasoning. The learned embeddings are expected to preserve
the key information and relational/structural patterns of KGs.
Given a knowledge graph G with a set of entities E and a set of relations R. Each triple, abbreviated
as ph, r, tq, in G is composed by two entities (i.e., head entity h P E and tail entity t P E), and the
relationship r P R between them.
Embedding Layer In the ith space, entities h, t are represented by vectors ephiq , etpiq P Rb and
relation r is represented by two translation vectors αpriq, βrpiq P Rb and a rotation vector γrpiq P Rb.
Also, each head (tail) entity is associated with a bias term bh pbtq P R. We initialize all the embedding
vectors in tangent space and exponential map will be applied to recover the hyperbolic/spherical
parameters when necessary. There are two benefits of doing so: On the one hand, the embedding
layer can be easily utilized by the sparse gating network defined in Euclidean space. On the other,
standard Euclidean optimization techniques can be applied thanks to the convenient exponential
and logarithmic maps. The overall framework can be optimised in a uniform fashion, avoiding the
cumbersome Riemannian optimization (Bonnabel, 2013).
Sparse Gating Network The sparse gating network is a differentiable data-driven tool used to
select the most suitable K spaces for each data point. The input for the sparse gating network is
constructed from the head entity embedding (tail entity embedding is not used as it shares the same
embedding matrix with the head entity) and the relation embeddings (the rotation vector is not used
as it will be normalized for rotation). The input is defined as:
x “ reph1q, ...,ephNq,αpr1q, ...,αprNq,βrp1q, ...,βrpNqsJ,	(4)
where X is a two-dimensional matrix of shape 3N X b. Then 2-D convolutions with linear transforma-
tions are applied over x to transform it into a vector of dimensionality N. Convolutional operations
can deal with large dimensionality b without introducing too much additional free parameters. In
formal, the last hidden layer of the gating network has the following form:
f (x) “ fι(x) ' randn()∙ ln(1 ' exp(f2(x))),	(5)
where fι and f2 represent CNN layers and f*(x) P RN; function “randn()" creates samples from
the standard normal distribution; the second term is a tunable Gaussian noise (only for training) that
is added to improve the load balancing, i.e., balance the number of samples accepted by each space.
To impose sparsity, we employ a TopK function to obtain the largest K elements from f (xq:
f (x) D TopK(f (x)),
(6)
where the TopK function returns the original value if the element is in the top K list, otherwise it
returns —8. We need N gates g1,g2,.., gN to generate probabilities to control the activeness of each
space. Simply, a softmax gating network is employed. For ´8, it will output a zero gate value.
gi (xq “
exp(f (x)i)
ΣN=1 eχp(f (x)jq
i “ t1, ..., Nu.
(7)
Only spaces with nonzero gate values will be active while other spaces are idle and not computed. This
approach provides the possibility to enlarge the model size without incurring much computational
cost as long as the number of active spaces K is fixed.
4
Under review as a conference paper at ICLR 2022
Figure 2: Architecture of the proposed SwisE. Only K out of N spaces are active for each KG triple.
RotE, RotH, RotS, and SwisE The base models RotE and RotH proposed by (Chami et al., 2020b)
are models of the same form but defined in Euclidean and Poincare spaces, respectively. We contribute
two new models based on them, RotS and SwisE. RotS has the same form as RotE/RotH but is
defined in the spherical space. SwisE is a model that operates in switch space and it uses RotE, RotH,
and RotS as the component model. The architecture of SwisE is shown in Figure 2.
To define RotS, let the ith space Mpiq be spherical. In RotS, the head entity is translated twice via
Mobius addition in the spherical space and rotated once. In formal, the head entity is processed as
follows.
QPiq (h,r) “ ROTATE(exp0(ehiq) ‘c exp0(aP), Yriq) ‘c exp0(βriq),	(8)
where C > 0 and exp0 is the exponential map over origin. ROTATE is a rotation function (same as
RotE/RotH) and γrPiq is the rotation matrix. The transformed head entity is then compared with the
tail entities using squared spherical distance. The final scoring function is as follows:
sMPiq ph,r,t) “ ´dMpiq (QPiq (h,r), exP0(epiq)) ` bhiq ` btiq,	⑼
Same as RotH, we make the curvature relation specific and trainable. That is, each relation has a
curvature parameter and they are trained simultaneously with the model. In the implementation, we
use softplus to convert curvatures to positive values and then enforce it to be positive or negative
based on the type of space we use.
In SwisE, We have N models in total selected from RotE (c = 0), RotH(c < 0), and RotS (c > 0).
Let sMpiq (h, r, t) denote the output of the ith model. Since only K out of N models will be active,
we set the outputs of the inactive models to 0. Similar to the definition of squared distance on the
product space, the scoring function of SwisE also decomposes and we define it as:
N
s(h, r, t) “ log(	gi(x)exp(sMpiq(h,r,t))),	(10)
i“1
where a log-sum-exp technique is adopted to improve the numerical stability and to avoid problems
such as underflow and overflow. Note that multiplying by the gating probability is optional in our
framework. If sMpiq is calculated with squared distance and K “ N, we can recover the squared
distance function of product space by removing gi (x) and the log-sum-exp technique.
Objective Function of SwisE We adopt the following cross-entropy loss as the objective function
of SwisE.
L = E	logp1 + exp(—Yph,r,tqs(h,r,t))),	(II)
Ph,r,t)pΩ
where Yphrtq P {1, —1} is a binary label indicating whether a triple is factual (1) or not (-1).
Ω represents the training collection including both positive and negative triples. Also, We add an
additional regularization for load balancing (discussed in Appendix A). The whole model is optimized
end-to-end in tangent space with the Euclidean Adam optimizer (Kingma & Ba, 2014).
5
Under review as a conference paper at ICLR 2022
M	Model	WN18RR				FB15K-237			
		MRR	HR@1	HR@3	HR@10	MRR	HR@1	HR@3	HR@10
	TransE	0.223	0.013	0.401	0.529	0.332	0.233	0.372	0.531
	BoxE	0.451	0.400	0.472	0.541	0.337	0.238	0.374	0.538
R	DistMult	0.430	0.390	0.440	0.490	0.241	0.155	0.263	0.419
	ConvE	0.430	0.400	0.440	0.520	0.325	0.237	0.356	0.501
	TuckER	0.470	0.443	0.482	0.526	0.358	0.266	0.394	0.544
	RotE	0.494	0.446	0.512	0.585	0.346	0.251	0.381	0.538
C	ComplEx-N3	0.480	0.435	0.495	0.572	0.357	0.264	0.392	0.547
	RotatE	0.476	0.428	0.492	0.571	0.338	0.241	0.375	0.533
Q	QuatE	0.488	0.438	0.508	0.582	0.366	0.271	0.401	0.556
S	MuRS	0.454	0.432	0.482	0.550	0.338	0.249	0.373	0.525
P	MurP	0.481	0.440	0.495	0.566	0.335	0.243	0.367	0.518
	RotH	0.496	0.449	0.514	0.586	0.344	0.246	0.380	0.535
*	M2 GNN	0.485	0.444	0.498	0.572	0.362	0.275	0.398	0.565
‹	SwisE	0.526	0.479	0.549	0.611	0.530	0.500	0.545	0.590
		十 .002	十.004	十.004	十.003	十.006	十.008	十.006	十.005
Table 1: The performance of switch space on WN18RR is achieved with model pD100q4E100(K=2),
and that on FB15K-237 is with pP100q3pD100q2pK “ 4q. R:real coordinate space; C:complex number
space; Q:quaternion space.由 denotes mixed curvature product space. < denotes switch space.
4.3	Switch Spaces for Recommender Systems
Recommender system is an important part in modern e-commerce platforms. It can improve cus-
tomer experience via reducing information overload and increase revenue for companies. Learning
representations that reflect users’ preferences and items characteristics have been a central theme in
the field. We simplify the model details since it follows the same procedure as that of SwisE.
Given a collection of users and items and the interactions observed between them, the goal is to
generate personalized items recommendations for each user. We are interested in metric learning
based collaborative filtering (Hsieh et al., 2017; Vinh Tran et al., 2020). Taken a user vector upuiq P Rb
and an item vector vvpiq P Rb as input. The the preference of user u towards item v in each space is
measured by the squared distance between them:
SMpiq (u, V) = ´dM(i)(exp0(uu), exp0(v0)).	(12)
The input of the gating network is the concatenation of the user and item embeddings. Specifically,
a vector of size R2Nb is used as the input and a linear layer is used in the gating network. We use
S(u, v) to denote the final prediction score and optimize the model with a max-margin hinge loss:
L =	max(0, S(u, v1) ` m ´ S(u, v2)),	(13)
pu,v1,v2qPΨ
where Ψ is the collection of training samples; v1 is the item that u liked and v2 represents a negative
(unobserved) item for the user. max(0, x) is also known as the ReLU function. m is a margin value.
5	Performance Evaluation
We evaluate switch spaces on real-world datasets. The statistics of datasets and hyper-parameter
settings are reported in Appendix C.
5.1	Results ON Knowledge graph completion
Baselines We compare SwisE with a number of baselines, including Euclidean methods
TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ConvE (Dettmers et al., 2018),
TuckER (Balazevic et al., 2019b), RotE (Chami et al., 2020b), BoxE (Abboud et al., 2020); complex
number based methods ComplEx-N3, (Lacroix et al., 2018) and RotatE (Sun et al., 2019); quaternion
model QuatE (Zhang et al., 2019); spherical models MuRS (Wang et al., 2021); hyperbolic methods
MurP (Balazevic et al., 2019a), RotH (Chami et al., 2020b); and product space model M2GNN (Wang
et al., 2021).
6
Under review as a conference paper at ICLR 2022
sωuueti-#
Ooooooo
ShEI D3D4 D2D3 Dt£i D3E1 班历小分4。2石1 DiDj Dιf⅛
Product of Spaces
sωuuetiu- J。#
IOOOO
8000
6000
4000
2000
0
PIPPlP3O1β2 P2P3D1D2 PiPiDiDi P1P2P3O2
Product OfSpaces
Figure 3: Distribution of the learned spaces on the test data of WN18RR (left figure,
pD100q4E100pK “ 2q) and FB15K-237 (right figure, pP100q3pD100q2pK “ 4q) using switch space.
Metrics The performance of different models are evaluated using two standard metrics including
mean reciprocal rank (MRR) and hit rate (HR) with cut-off values t1, 3, 10u. The standard filtered
setting (Bordes et al., 2013) is adopted.
Comparison with the State-of-the-arts
Table 1 compares SwisE with baselines.
We make the following observations.
Firstly, SwisE achieves the new state-of-
the-art results across all metrics on both
datasets. SwisE outperforms Euclidean, hy-
perbolic, spherical, complex-valued, prod-
uct spaces approaches, and obtains a clear
performance gain over the second best
model. Secondly, the second best per-
formers scatter among QuatE, RotH, and
M2GNN. On the WN18RR dataset, RotH
achieves the second best results. On the
FB15K-237 dataset, QuatE and M2GNN
M 		Model 				WN18RR		FB15K-237	
				MRR	HR@3	MRR	HR@3
Single	D500			0.492	0.514	0.293	0.322
	ppioo	q3pD100	q2	0.484	0.498	0.311	0.344
Product	PD100	q4E100		0.479	0.497	0.312	0.344
	ppioo	q4D100		0.468	0.488	0.321	0.356
	pp100	q2PD100	q2E100	0.479	0.496	0.308	0.342
	pp100	q3PD100		0.500		0.530	0.545
Switch	PD100	q4E100		0		0.525	0.531
	ppi00	q4D100		0 		0.515	0.527
	ppi00	q2PD100	q2E100	0		0.526	0.535
Table 2: Performance comparison with product space
models (a few signatures are reported due to length
constraints), K is setto4 on WN18RR and 2 for FB15K-
(P100)3PD100 (D100)4E100 (P100)4D100 (P100)2.00	q2	0.484 0.479	0.498 0.497
	)2E100	0.468 0.479	0.488 0.496
10 0 0 10
Dp1E1DDp
3442
qqqq
0000
0000
1111
PDPP
pppp
achieve on par performances. Thirdly, the
improvement of SwisE over the second best
237.
model on FB15K-237 is larger than that on WN18RR. One possible explanation is that FB15K-237
has a more diverse set of relations and a higher structure heterogeneity (Balazevic et al., 2019a).
0
Comparison with Product Space In the first place, we
compare SwisE with the product space model M2GNN (its
signature is P200D200E200). As shown in Table 1, SwisE
outperforms M2GNN on all metrics. Moreover, compared
with the single space model, QuatE and RotH, M2GNN
does not show much advantages. This observation con-
firms that using pure product space will not offer much
merit for data representation on KG.
Then, we compare SwisE with product space models with Figure 4: Distribution of the learned
varying signatures in Table 2. We observe that the switch spaces of four relations on FB15K-237.
space model constantly outperforms pure product spaces Counts of instances are normalized.
with the same prior signatures. It is worth noting that
SwisE requires merely two/four active spaces to outperform product space models with five active
spaces. Additionally, we find that spherical space dominant models can also obtain very competitive
performances on both datasets. our assumption is that the spherical model can not only model cyclical
structures, but can also capture hyperbolic structures to some extent. We provide a proof using graph
distance ratio in Appendix B.
5.2	Results on Recommender systems
Baselines We compare our method with a number of baselines including Bayesian personalized
ranking based matrix factorization (BPRMF) (Rendle et al., 2009), Euclidean, hyperbolic, spherical,
and product space models. En corresponds to CML (Hsieh et al., 2017), Pn is equivalent to
HyperML (Vinh Tran et al., 2020).
7
Under review as a conference paper at ICLR 2022
Space	Model	MovieLens 100K			MovieLens 1M		
		MAP	P@10	R@10	Map	P@10	R@10
	BPRMF	.211±.003	.254±.001	.176±.004	.120±.001	.185±.001	.078±.001
Single	E100(CML)	.195±.001	.233±.003	.165±.003	.138±.002	.208±.002	.092±.003
	P100(HyperML)	.157±.001	.194±.003	.136±.004	.113±.001	.167±.002	.075±.002
	D100	.161±.001	.193±.002	.137±.001	.116±.001	.170±.004	.077±.001
	pE20q5	.199±.002	.240±.001	.168±.003	.150±.001	.221±.002	.100±.001
	pP20q5	.169±.003	.210±.004	.146±.003	.128±.003	.189±.002	.083±.001
Product	pD20q5	.177±.003	.212±.002	.149±.005	.131±.001	.188±.001	.085±.004
	pD20q3pP20q2	.173±.001	.209±.001	.151±.001	.133±.001	.193±.001	.085±.001
	pP20q2pD20q2E20	.176±.001	.208±.002	.149±.003	.134±.002	.192±.003	.088±.002
	pE20q5	.218±.003	.267±.003	.188±.002	.158±.001	.236±.001	.105±.002
Switch	pP20q5	.191±.002	.233±.005	.162±.004	.142±.003	.213±.005	.094±.005
	pD20q5 pD20 q3 pP20q2	.194±.002	.233±.005	.162±.002	.172±.003	.253±.003	.116±.002
(K=4)		.203±.003	.246±.002	.174±.003	.174±.001	.252±.002	.116±.001
	pP20q2pD20q2E20	.201±.002	.245±.004	.171±.001	.172±.001	.249±.002	.114±.002
Table 3: Recommendation performance on two benchmark datasets.
Figure 5: Visualization of the spaces distribution on subsets of the FB15K-237.
96: film_release_region
129: location-contains
226: tv_program_country_of_origin
3: award_nominated_for
38: phone_sandbox_service_location
73: film_country
172: mailing_address_country
124: country_second_level_divisions
84:	f i I m_prod uction_design_by
85:	film_genre
188: person-nationality
222: tv_program
Metrics We measure the performance based on the widely adopted metrics in recommender systems:
mean average precision (MAP), precision (P@5 and P @10), and recall (R@5 and R@10).
Results Table 3 summarizes the performances. The merits of switch spaces can be further observed
from the comparison. On both datasets, switch space based models achieve the best performances,
outperforming their pure product space counterparts with the same signature. Another clear pattern
is that product space models usually perform better than single space models. Specifically, on
MovieLens 100K, Euclidean space is the most effective. While, on MovieLens 1M, spherical space
dominated models outperform models with other space combinations. This observation is consistent
with the global average curvature we estimated (Appendix C). Interestingly, we find that the matrix
factorization approach (BPRMF) remains competitive on the two datasets and can sometimes achieve
better results than other single space models.
6	Model Analysis
We conduct model analysis using SwisE and dataset FB15K-237.
6.1	Distribution of the Learned Spaces
Figure 3 presents the distribution of the learned space combinations. It is obvious that KG triples are
not randomly distributed. Instead, certain product spaces are preferred in each case. For example,
the product space D2D3 is the most popular on WN18RR. While on FB15K-237, the product space
P1P2P3D1 is preferable. To show more fine-grained examples, we randomly select four relations from
FB15K-237 and visualize the distribution of product spaces in Figure 4. We find that the relations have
their own desirable product spaces as well. For example, relation “/location/location/time_zones"
prefers space P1P2P3D1 but relation “people/person/profession" prefers space P1P3D1D2. This
reconfirms the specification capability of SwisE.
8
Under review as a conference paper at ICLR 2022
Furthermore, we randomly extract two connected graph components from the test set of FB15-237
and visualize them in Figure 5. Each triple can be represented with an edge and its two end nodes.
We render edges with different colors based on the type of product spaces that are used to process
the corresponding triples. From the left graph, we can see some clear clusters and find that each
cluster usually has a dominant color, which suggests that neighborhood triples are more likely to be
processed with the same product space. From the right graph, we find that different relations are
processed by different spaces even if they are located near to each other. This observation indicates
that the type of relations plays a critical role in determining which spaces to use. More examples are
presented in Appendix C.
6.2	Effects of Total Spaces Number
Figure 6 (left) reports the test MRR
and inference time varied across
the different N values with K=2
using switch space model pP100qN
on WN18RR. We observe that: (a)
the model performance does not
benefit from increasing model size
on this dataset; (b) the inference
time remains nearly constant when
3.51----------------------------------------r0.550
N
0.525
0.500
0.475
0.450
0.425
0.400
W
Figure 6: Effect of N (left) and K (right) onWN18RR.
we increase N, confirming its efficiency. This property is important for tasks that prefer a larger
model size.
6.3	Effects of Active Spaces Number
We show the effects of the active space number K using model
pD100q4E100 on WN18RR in Figure 6 (right). We find that, compared
with N, K has a higher impact on the model performance and inference
time. On WN18RR, a small K (e.g., 2) value is enough to obtain the opti-
mal performance. Increasing K will generally degrade the performance.
6.4	Effects of Embedding Dimension
We study the impact of the embedding dimension on WN18RR and report
the results in Figure 7 (Upper). In general, the embedding dimension has
a high impact on the model accuracy. We also find that SwisE performs
worse than RotH when the embedding dimension is small. This might
because that SwisE only has two active spaces. For example, the active
dimension is only 40 for SwisE when the overall dimension is 100, while
RotH has 100 active dimensions in this case. As such, we suggest that
it is better to keep the overall active dimensionality in a reasonable size.
6.5	Convergence
The convergence of SwisE and pure product space model on FB15K-237
is shown in in Figure 7(Lower). Clearly, SwisE will not incur additional
computational cost and converges as fast as the product space model.
Figure 7: Effects of em-
bedding dimensionality
(Upper) and model Con-
vergence (Lower).
7	Conclusion
In this paper, we propose a novel representation learning framework, switch space. Switch space
makes use of a sparse gating network to enhance the alignment between data and geometric spaces
and ensure the given data to be handled by suitable spaces, allowing a greater extent of specifica-
tion in the representation while remaining efficient. With switch spaces, we manage to obtain a
state-of-the-art performance on the knowledge graph completion task and promising results on the
item recommendation task. We believe that switch spaces hold promise for more expressive and
generalizable representation learning and can be applied to many other areas.
9
Under review as a conference paper at ICLR 2022
References
Ralph Abboud, Ismail Ceylan, Thomas Lukasiewicz, and Tommaso Salvatori. Boxe: A box embed-
ding model for knowledge base completion. Advances in Neural Information Processing Systems,
33, 2020.
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with
a network of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3366-3375, 2017.
Gregor Bachmann, Gary BecigneUL and Octavian Ganea. Constant curvature graph convolutional
networks. In International Conference on Machine Learning, pp. 486-496. PMLR, 2020.
Ivana Balazevic, Carl Allen, and Timothy Hospedales. Multi-relational poincare graph embeddings.
In Advances in Neural Information Processing Systems, pp. 4463-4473, 2019a.
Ivana Balazevic, Carl Allen, and Timothy Hospedales. Tucker: Tensor factorization for knowledge
graph completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 5188-5197, 2019b.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on
Automatic Control, 58(9):2217-2229, 2013.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. Advances in neural information
processing systems, 26:2787-2795, 2013.
Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional neural
networks. In Advances in neural information processing systems, pp. 4868-4879, 2019.
Ines Chami, Albert Gu, Vaggos Chatziafratis, and Christopher Re. From trees to continuous embed-
dings and back: Hyperbolic hierarchical clustering. Advances in Neural Information Processing
Systems, 2020a.
Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher Re. Low-
dimensional hyperbolic knowledge graph embeddings. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, pp. 6901-6914, Online, July 2020b. Association
for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.617. URL https://www.
aclweb.org/anthology/2020.acl-main.617.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d
knowledge graph embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep
mixture of experts. arXiv preprint arXiv:1312.4314, 2013.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
Octavian-Eugen Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. arXiv
preprint arXiv:1805.09112, 2018.
Albert Gu, Frederic Sala, Beliz Gunel, and Christopher Re. Learning mixed-curvature representations
in product spaces. In International Conference on Learning Representations, 2019.
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis), 5(4):1-19, 2015.
10
Under review as a conference paper at ICLR 2022
Cheng-Kang Hsieh, Longqi Yang, Yin Cui, Tsung-Yi Lin, Serge Belongie, and Deborah Estrin.
Collaborative metric learning. In Proceedings of the 26th international conference on world wide
web, pp. 193-201,2017.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79-87, 1991.
Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
computation, 6(2):181-214, 1994.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for
knowledge base completion. In International Conference on Machine Learning, pp. 2863-2872,
2018.
Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. In Advances in
Neural Information Processing Systems, pp. 8230-8241, 2019.
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships
in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 1930-1939, 2018.
Yu Meng, Jiaxin Huang, Guangyuan Wang, Chao Zhang, Honglei Zhuang, Lance Kaplan, and Jiawei
Han. Spherical text embedding. In Advances in Neural Information Processing Systems, pp.
8208-8217, 2019.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781, 2013.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning
on multi-relational data. In Icml, 2011.
MaXimillian Nickel and DouWe Kiela. Poincare embeddings for learning hierarchical representations.
In Advances in neural information processing systems, pp. 6338-6347, 2017.
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian
personalized ranking from implicit feedback. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artificial Intelligence, pp. 452-461, 2009.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and MaX
Welling. Modeling relational data With graph convolutional netWorks. In European semantic web
conference, pp. 593-607. Springer, 2018.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural netWorks: The sparsely-gated miXture-of-eXperts layer. arXiv
preprint arXiv:1701.06538, 2017.
Ondrej Skopek, Octavian-Eugen Ganea, and Gary Becigneul. MiXed-curvature variational autoen-
coders. In International Conference on Learning Representations, 2020.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: KnoWledge graph embedding by
relational rotation in compleX space. In International Conference on Learning Representations,
2019.
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Hyperbolic representation learning for fast and efficient
neural question ansWering. In Proceedings of the Eleventh ACM International Conference on Web
Search and Data Mining, pp. 583-591, 2018.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
embeddings for simple link prediction. In International Conference on Machine Learning, pp.
2071-2080. PMLR, 2016.
11
Under review as a conference paper at ICLR 2022
Abraham A Ungar. Thomas precession and its associated grouplike structure. American Journal of
Physics, 59(9):824-834,1991.
Lucas Vinh Tran, Yi Tay, Shuai Zhang, Gao Cong, and Xiaoli Li. Hyperml: A boosting metric
learning approach in hyperbolic space for recommender systems. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp. 609-617, 2020.
Shen Wang, Xiaokai Wei, Cicero NogUeira dos Santos, ZhigUo Wang, Ramesh Nallapati, Andrew
Arnold, Bing Xiang, S Yu Philip, and Isabel F Cruz. Mixed-curvature multi-relational graph neural
network for knowledge graph completion. In The Web Conference, 2021.
Richard C Wilson, Edwin R Hancock, Elzbieta Pekalska, and Robert PW Duin. Spherical and
hyperbolic embeddings of data. IEEE transactions on pattern analysis and machine intelligence,
36(11):2255-2269, 2014.
Bishan Yang, Wen-taU Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knowledge bases. ICLR, 2015.
Bangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-Fei. Hierarchical mixtUre of classification
experts Uncovers interactions between brain regions. Advances in Neural Information Processing
Systems, 22:2178-2186, 2009.
ShUai Zhang, Yi Tay, Lina Yao, and Qi LiU. QUaternion knowledge graph embeddings. In Advances
in Neural Information Processing Systems, pp. 2735-2745, 2019.
12