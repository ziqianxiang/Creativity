Under review as a conference paper at ICLR 2022
CUP: A Conservative Update Policy Algorithm
for Safe Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Safe reinforcement learning (RL) is still very challenging since it requires the
agent to consider both return maximization and safe exploration. In this paper, we
propose CUP, a Conservative Update Policy algorithm with a theoretical safety
guarantee. The derivation of CUP is based on surrogate functions with respect to
our new proposed bounds. Although using bounds as surrogate functions to design
safe RL algorithms have appeared in some existing works, we develop it at least
three aspects: (i) We provide a rigorous theoretical analysis to extend the bounds
with respect to generalized advantage estimator (GAE). GAE significantly reduces
variance while maintains a tolerable level of bias, which is an efficient step for us to
design CUP; (ii) The proposed bounds are more compact than existing works, i.e.,
using the proposed bounds as surrogate functions are better local approximations to
the objective and constraints. (iii) The bound of worst-case safe constraint violation
of CUP is more compact than the existing safe RL algorithms, which explains why
CUP is so good in practice. Finally, extensive experiments show the effectiveness
of CUP where the agent satisfies safe constraints.
1	Introduction
Reinforcement learning (RL) (Sutton & Barto, 1998) has achieved significant successes in many
fields (Mnih et al., 2015; Silver et al., 2017; OpenAI, 2019; Afsar et al., 2021), robotics (Deisenroth
et al., 2013), playing Go (Silver et al., 2016; 2017), Starcraft (Vinyals et al., 2019), Dota (OpenAI,
2019), and recommendation system (Afsar et al., 2021). However, most RL algorithms improve
the performance under the assumption that an agent is free to explore any behaviors. In real-world
applications, only considering return maximization is not enough, and we also need to consider
safe behaviors. For example, a robot agent should avoid playing actions that irrevocably harm its
hardware, and a recommender system should avoid presenting offending items to users. Thus, it
is crucial to consider safe exploration for RL, which is usually formulated as constrained Markov
decision processes (CMDP) (Altman, 1999).
It is challenging to solve CMDP since traditional approaches (e.g., Q-learning (Watkins, 1989) &
policy gradient (Williams, 1992)) usually violate the safe exploration constraints, which is undesirable
for safe RL. Recently, Achiam et al. (2017); Yang et al. (2020); Bharadhwaj et al. (2021) suggest to
use some surrogate functions to replace the objective and constraints. However, their implementations
involve some convex approximations to the non-convex objective and safe constraints, which leads to
many error sources and troubles. Concretely, Achiam et al. (2017); Yang et al. (2020); Bharadhwaj
et al. (2021) approximate the non-convex objective (or constraints) with first-order or second Taylor
expansion, but their implementations still lack a theory to show the error difference between the
original objective (or constraints) and its convex approximations. Besides, their approaches involve
the inverse of a high-dimension Fisher information matrix, which causes their algorithms to require a
costly computation for each update when solving high-dimensional RL problems.
Our Main Work. To address above problems, we propose the conservative update policy (CUP)
algorithm with a theoretical safety guarantee. We derive the CUP bases on some new proposed
surrogate functions with respect to objective and constraints and provide a practical implementation
of CUP that does not depend on any convex approximation to adapt high-dimensional safe RL.
Concretely, in Section 3, Theorem 1 shows generalized difference bounds between two arbitrary
policies for the objective and constraints. Those bounds provide principled approximations to the
1
Under review as a conference paper at ICLR 2022
objective and constraints, which are theoretical foundations for us to use those bounds as surrogate
functions to replace objective and constraints to design algorithms.
Although using difference bound to replace objective or constraints has appeared in some existing
works (e.g., (Kakade & Langford, 2002; Schulman et al., 2015; Achiam et al., 2017)), Theorem
1 improves their bounds at least two aspects: (i) Firstly, our rigorous theoretical analysis extends
the bound with respect to generalized advantage estimator (GAE) (Schulman et al., 2016). GAE
significantly reduces variance while maintains a tolerable level of bias, which is one of the critical
steps for us to design efficient algorithms in the later section. Although Zhang et al. (2020); Kang
et al. (2021) have applied GAE to solve safe RL problems, their approaches are empirical and
lack a theoretical analysis with respect to GAE. Thus, our result provides a theory to illustrate the
effectiveness of the work (Zhang et al., 2020; Kang et al., 2021). (ii) Our new bounds refine classic
difference bounds. For example, our bounds are more compact than Achiam et al. (2017), i,e., using
our new bounds as surrogate functions are better local approximations to the objective and constraints.
Besides, the surrogate functions with respect to our new bounds are more accessible to be estimated
from the samples than the approaches appears in (Kakade & Langford, 2002; Schulman et al., 2015)),
for more discussions, please see Remark 1.
In Section 4, we provide the necessary details of the proposed CUP. The CUP contains two steps:
it performs a policy improvement at first, then it projects the policy back onto the safe region to
reconcile the constraint violation. Theorem 2 shows a lower bound on policy improvement and an
upper bound on constraint violation for CUP at each update. Notably, the result in Theorem 2 shows
the bound of CUP is more compact than state-of-the-art safe RL algorithms: CPO (Achiam et al.,
2017, Proposition 1-2), PCPO (Yang et al., 2020, Theorem 1) and FOCOPS (Zhang et al., 2020),
which provides a partial explanation for why CUP is so good in practice. For more discussions,
please refer to Remark 2. Finally, we provide a practical implementation of sample-based CUP. Such
an implementation allows us to use deep neural networks to train a model. Mainly, CUP does not
depend on any convex approximation for objective and constraints, and it optimizes the objective
according to the first-order optimizer. Extensive high-dimensional experiments on continuous control
tasks show the effectiveness of CUP where the agent satisfies safe constraints.
2	Preliminaries
Reinforcement learning (RL) (Sutton & Barto, 1998) is often formulated as a Markov decision process
(MDP) (Puterman, 2014) that is a tuple M = (S, A, P, r, ρ0 , γ). Here S is state space, A is action
space. P(s0 |s, a) is probability of state transition from S to s0 after playing a. r(∙) : S ×S ×A→ R,
and r(s0|s, a) denotes the reward that the agent observes when state transition from s to s0 after it
plays a. ρo(∙) : S → [0,1] is the initial state distribution and Y ∈ (0,1).
A stationary parameterized policy ∏θ is a probability distribution defined on S ×A, ∏θ(a|s) denotes
the probability of playing a in state s. We use Πθ to denote the set of all stationary policies, where
∏θ = {∏θ : θ ∈ Rp}, and θ is a parameter needed to be learned. Let P∏θ ∈ RlSl×lSl be a state
transition probability matrix, and their components are: P∏θ [s, s0] = Pacι∈A ∏θ(a∣s)P(s0∣s, a)=:
Pπθ (s0 |s), which denotes one-step state transformation probability from s to s0 by executing πθ.
Let T = {st, at, rt+1}t≥0 〜∏θ bea trajectory generated by ∏θ, where so 〜ρo(∙), at 〜∏θ (∙∣sj
st+ι 〜 P(∙∣st, at), and rt+ι = r(st+ι∣st, at). We use P∏θ (St = S |s) to denote the probability of
visiting the state s0 after t time steps from the state s by executing πθ . Due to the Markov property in
MDP, Pπθ (St = S0 |S) is (S, S0)-th component of the matrix Ptπ , i.e., Pπθ (St = S0 |S) = Ptπ [S, S0].
Finally, let dsπ0 (S) = (1 - γ) Pt∞=0 γtPπθ (St = S|S0) be the stationary state distribution of the
Markov chain (starting at s°) induced by policy ∏θ. We define d∏0 (S)= 旧§。〜。。(.)[d∏0 (s)] as the
discounted state visitation distribution on initial distribution ρo(∙).
The state value function of ∏θ is defined as V∏θ (S) = E∏θ [£ ∞=0 γtrt+ι∣so = s], where E∏θ[∙∣∙]
denotes a conditional expectation on actions which are selected by πθ . Its state-action value
function is Qπθ (S, a) = Eπθ [ t∞=0 γtrt+1 |S0 = S, a0 = a], and advantage function is Aπθ (S, a) =
Qπθ (S, a) - Vπθ (S). The goal of reinforcement learning is to maximize J(πθ):
J (πθ) = Es 〜d∏θ (∙)[V∏θ(S)].	⑴
2
Under review as a conference paper at ICLR 2022
2.1	Policy Gradient and Generalized Advantage Estimator (GAE)
Policy gradient (Williams, 1992; Sutton et al., 2000) is widely used to solve policy optimization,
which maximizes the expected total reward by repeatedly estimating the gradient g = VJ(∏θ ).
Schulman et al. (2016) summarize several different related expressions for the policy gradient:
∞
g = VJ(∏θ) = E XΨtVlog∏θ(at∣st) ,	(2)
t=0
where Ψt can be total discounted reward of the trajectory, value function, advantage function or
temporal difference (TD) error. As stated by Schulman et al. (2016), the choice Ψt = A(st, at) yields
almost the lowest possible variance, which is consistent with the theoretical analysis (Greensmith
et al., 2004; Wu et al., 2018). Furthermore, Schulman et al. (2016) propose generalized advantage
estimator (GAE) AGAE(Y,λ)(st, at) to replace Ψt: for any λ ∈ [0,1],
∞
AGAE(Y,λ)(st,at) = X(γλ)'δV+',	(3)
'=0
where δ1 = rt+ι + YV(st+ι) — V(St) is TD error, and V(∙) is an estimator of value function. GAE
is an efficient technique for data efficiency and reliable performance of reinforcement learning.
2.2	Safe Reinforcement Learning
Safe RL (Ray et al., 2019) is often formulated as a constrained MDP (CMDP) M ∪C (Altman, 1999),
which is a standard MDP M augmented with an additional constraint set C. The setC = {(ci, bi)}im=1,
where Ci are cost functions: Ci : S ×A→ R, and limits are bi, i = 1, ∙,m. The cost-return is defined
as: Jci (πθ) = Eπθ [Pt∞=0 γtci(st, at)], then we define the feasible policy set ΠC as:
ΠC = ∩im=1 {πθ ∈ Πθ and Jci(πθ)≤bi}.
The goal of CMDP is to search the optimal policy π? such that
π? = arg max J(πθ).	(4)
πθ ∈ΠC
Furthermore, we define value functions, action-value functions, and advantage functions for the
auxiliary costs in analogy to Vπθ, Qπθ, and Aπθ, with Ci replacing r respectively, we denote them
as Vπcθi, Qcπiθ, and Acπiθ. For example, Vπcθi (s) = Eπθ [Pt∞=0γtCi(st, at)|s0 = s]. Without loss of
generality, we will restrict our discussion to the case of one constraint with a cost function C and
upper bound b. Finally, we extend the GAE w.r.t. auxiliary cost function C:
∞
ACAEg)(st,at) = X(γλ)'δC+',	(5)
where δC = rt+ι + YC(st+ι) 一 C(St) is TD error, and C(∙) is an estimator of cost function c.
3	Generalized Policy Performance Difference Bounds
In this section, we show some generalized policy optimization performance bounds for J(πθ) and
Jc(πθ). The proposed bounds provide some new certain surrogate functions w.r.t. the objective and
cost function, which are theoretical foundations for us to design efficient algorithms to improve policy
performance and satisfy constraints. Additionally, those bounds refine or extend some existing works
(e.g., (Kakade & Langford, 2002; Schulman et al., 2015; Achiam et al., 2017)) to GAE case that
significantly reduces variance while maintains a tolerable level of bias, which is one of the key steps
for us to propose efficient algorithms in the later section.
Before we present our new bounds, let us revisit a classic result about policy performance difference
from (Kakade & Langford, 2002), i.e., the next Eq.(6),
J(πθ) - J(πθ0 ) = (I-Y)TEs〜d∏θ(∙)Ea〜∏θ(∙∣s)[A∏θo (S,a)]∙	⑹
Eq.(6) shows a difference between two arbitrary policies πθ and πθ0 with different parameters θ
and θ0. However, as stated by Zanger et al. (2021), Eq.(6) is very intractable for sampling-based
policy optimization since it requires the data comes from a fixed policy πθ . In this section, our new
bound will refine the result (6). For more discussions about the difference between our new bound
and Eq.(6), please refer to Remark 1.
3
Under review as a conference paper at ICLR 2022
3.1	Some Additional Notations
We use a bold lowercase letter to denote a vector, e.g., a = (aι, a?,…,an), and its i-th element
a[i] =: ai. Let 夕(∙) ： S → R be a function defined on S, δt = r(st+ι |st, at) + Y夕(st+ι)-夕(St)
is TD error w.r.t.夕(∙). For two arbitrary policies ∏θ and n®/, we denote δ∏ζ,∕s) as the expectation
ofTD error, and define △汽∏θ, n®/, S) as the difference between δ∏^,∕s) and δ∏ 0 ,t(s): ∀s ∈ S,
δ总 t(s) =	E	[δf], △汽∏Θ,∏Θ0, s) = E	( ∏θ:MJ，— 1)δf .
πθst~P∏θ (∙∣s) S, " θ, θ , '	st~P∏j 0(∙∣s) N∏θ0(at |st)	/ tJ
at~πθ(Tst)、	at~∏θ0 (∙∣st)
st+1~p(∙"at)	st+ι~P(∙∣st,at)
Furthermore, we introduce two vectors δ'"t, ∆(∏θ, ∏θ0) ∈ R|S|, and their components are:
%,t[s] = δ∏θ,t(s), △汽πθ,πθ0)[s] = △汽πθ,πθ0,s)∙	⑺
Let matrix P(πλθ) = (1 - γλ) Pt∞=0(γλ)tPtπ+1, where λ ∈ [0, 1]. It is similar to the normalized
discounted distribution dρπ0 (S), we extend it to λ-version and denote it as dπλ (S):
∞
d∏θ(s) = ES0~ρo(∙) (I- Y) X YtPry(St = S|S0)
t=0
where Y = γ(-γλ), the probability P∏∏θ)(st = s|so) is the (so, s)-th component of the matrix product
P(πλθ) . Finally, we introduce a vector dλπ ∈ R|S|, and its components are: dλπ [S] = dπλ (S).
3.2	Main Results
Theorem 1 (Generalized Policy Performance Difference). For anyfunction 夕(∙) ： S → R ,for two
arbitrary policies ∏θ and n®0 ,forany p,q ∈ [1, ∞) such that P + ɪ = 1, we define two error terms:
Mλ)(∏θ,∏θ0 )=： kdλθ - dλ.0 kpkδ:θ,tkq,	⑻
∞
lp,,± (πθ,πθ0) =： vɪ^ X γtτEs~dλ 0(∙) I△汽 πθ ,πθ0,s)± Mλ)(πθ ,πθ0)].	⑼
1 -Y t=0	θ
Then, the following bound w.r.t. policy performance difference J(∏θ ) — J(n®0) holds:
Lp,- (πθ,πθ0) ≤ J(πθ) - J(πθ0) ≤ LP,,+ (πθ,πθ0).	(IO)
We provide its proof in Appendix E. The bound (10) is tight, i.e., if ∏θ = n®0, all the three terms in
Eq.(10) are zero identically. From Eq.(9), we know the performance difference bound Lpq (∏θ, n®0)
(10) can be interpreted by two distinct difference parts: (i) the first difference part, i.e., the expectation
△p(∏θ, n®0, s), which is determined by the difference between TD errors of ∏θ and n®0; (ii) the
second difference part, i.e.,the discounted distribution difference ep,,f(λ) (∏θ, ∏θ0), which is determined
by the gap between the normalized discounted distribution of n® and n®0. Thus, the difference of
both TD errors and discounted distribution determine the policy difference J(∏θ) - J(n®0).
The different choices ofp and q lead Eq.(10) to be different bounds. Ifp = 1, q = ∞, we denote
U,t =： kδpθ,tkq = maxst∈S Eat~∏θ(∙∣st),s^i~P(∙∣Stg) [Mp|], then, according to Lemma 2 (see
Appendix E.3), when P = 1, q = ∞, then error ep^) (∏θ, n®0) is reduced to:
p,(λ)	1	Y(1 - λ)
πθ,t
ep,q,t (πθ ,πθ0 )lp=l,q=∞ — 1 - γ ∙ |1 - 2γλ∣S∣∣A∣∣ Es~d∏e 0G)[2DTV (πθ0 ,πθ)[s]],
where DTV(n®0, ∏θ)[s] is the total variational divergence between action distributions at state s, i.e.,
2DTV(n®0,∏θ)[s] = E ∣∏θo(a∣s) - ∏θ(a|s)] .
a∈A
Finally, let 夕= Vna0, the left side of (10) in Theorem 1 implies a lower bound of performance
difference, which illustrates the worse case of approximation error, we present it in Proposition 1.
4
Under review as a conference paper at ICLR 2022
Proposition 1 (Worse case approximation error). For any two policies πθ and πθ0, let πVθ (πθ0 ) =:
suPt∈N+ {四θ t :夕=Vn 0}, then the following bound holds
J(πθ) - J(πθ0)	(11)
` 1 二	ztGAE(γ,λ)/ c、	2Y(I - λ- "θ)	C /	M l
≥ T-^ Es~dλθ0 %~πθ(Is) [Aπθ0	(S,a)- (1 - γλ) |1 - 2γλ∣S∣∣A∣∣ Dτv(πθ0 ,πθ )[s] .
If λ → 0, then the distribution d∏ 0 (∙) is reduced to d∏00 (∙) and the bound (11) is reduced to
J (πθ) - J (πθ0) ≥ τ-γ Es 〜d∏0 0 (∙),a 〜∏θ(∙∣s) hA∏θo(s,a) - 2γeVθ (πθ0 )Dτv(πθ0 ,πθ )[s]i . (12)
Let us review (Achiam et al., 2017, Corollary 1), which shows
J(πθ ) - J(πθ0 ) ≥ ；	Es〜d∏0 0 (∙),a〜∏θ (∙∣s) A∏θ0 (s, a) - 2 ' πθ( θ ) DTV(πθ0，πθ)[s] . (13)
1 - γ θ0	θ	1 - γ
Comparing (12) to (13), our new bound (12) is slightly tighter than the bound shown by (Achiam
et al., 2017). Concretely, our result improves the bound (13) by a factor ι-1γ. Since the refined bound
(11) contains GAE technique that significantly reduces variance while maintains a tolerable level
of bias (Schulman et al., 2016), which implies using the bound (11) as a surrogate function could
improve performance potentially.
Remark 1 (Comparison with (Kakade & Langford, 2002)). The result (11) develops the classic
performance difference (6) at least three aspects. Firstly, the bound (11) extends from the advantage
Aπ 0 (6) to GAE function AπGA0E(γ,λ). Secondly, the following term in Eq.(11):
θθ
(1 - Y)TEs〜d∏ 0 (∙),a〜∏θ(∙∣s) [AGA，E(Y,λ)(s, a)i	(14)
θ0	θ
is an approximation for the difference J(πθ) - J (πθ0 ), while Eq.(6) shows an identity for difference
J(∏) — J(∏e')∙ Comparison to Eq.(6), the proposed Eq.(14) uses the state distribution d∏ , (∙)
instead of dρ∏0 (∙), which is known the first oder expansion with respect to the policy ∏ around the
neighborhood around πθ0 (Kakade & Langford, 2002; Achiam et al., 2017). Finally, although Eq.(6)
provides an identity for J(πθ) - J (πθ0 ), it never shows an error bound of the first oder expansion
for the performance difference J(πθ) - J (πθ0 ), and the proposed bound (11) makes up for such a
weakness. Such a bound (11) can be viewed as the worse-case approximation error, which provides a
fresh surrogate function for us to design algorithms in the later section.
Let φ = Vc 0, Theorem 1 implies an upper bound of cost function as presented in the next Proposition
2, we will use it to make guarantee for safe policy optimization.
Proposition 2. For any two policies ∏ and ∏θi，let e∏ 3，)=：suPt∈N+ {e鲁 t :夕=Vc }, then
θ	θθ
Jc(πθ)-Jc(πθ0)	(15)
1	GAE(γ,λ)	2Y(1 - λ)eπCθ (πθ0 )
≤T-YEs~dπθ0 G),a~πθ(Is) [Aπθ0，C	(S,a)+ (1 - γλ) |1 - 2γλ∣S∣∣A∣∣DTgLe)[s]j ,
where we calculate AπGA0E,(Cγ,λ)(s, a) according to the data sampled from πθ0 and (5).
All above bound results (11) and (15) can be extended for a total variational divergence to KL-
divergence between policies, which are desirable for policy optimization. We obtain
Es〜dλ (∙) [DTV(πθ0 ,πθ)[s]] ≤Es〜d∏ (∙)
θ0	θ0
ʌ/l KL(πθ0 ,πθ )[s] ≤
∖J2Es〜d∏ 0(∙) [KL(πθ0,πθ)[s]],
(16)
where KL(∙, ∙) is KL-divergence, and KL(∏θ,,∏θ)[s] = KL(∏θ'(∙∣s),∏θ(∙∣s)); the first inequality
follows Pinsker,s inequality (CSiSzdr & Korner, 2011) and the second inequality follows Jensen's
inequality. According to (16), we obtain the next Proposition 3.
Proposition 3. All the bounds in (11) and (15) hold ifwe make the following substitution:
Es〜d∏ 0(∙) [Dτv(πθ0,πθ)[s]] J 4；Es〜d∏ 0(∙) [KL(πθ0,π)[s]].
5
Under review as a conference paper at ICLR 2022
4 Methodology: A Conservative Update Policy (CUP)
According to the bounds in Proposition 1-3, we develop new surrogate functions to replace the
objective and constraints. Inspired by two recent works (Yang et al., 2020; Zhang et al., 2020),
we propose the CUP (conservative update policy) algorithm that is a two-step approach contains
performance improvement and projection. Theorem 2 proves the proposed CUP guarantees the policy
improvement and safe constraints.
Step 1: Performance Improvement. According to Proposition 1 and Proposition 3, for an appropri-
ate coefficient αk , we update policy as follows,
∏θk+1 = argmaχβ {Es〜d∏oJ∙),a〜∏θ(∙∣s) ∣AGAE(Y,λ)(s, a)] - °kq∕Es-d⅛βJ∙) [KL(∏θk,∏θ)[s]]}∙
(17)
This step is a typical minimization-maximization (MM) algorithm (Hunter & Lange, 2004), it includes
return maximization and minimization the distance between old policy and new policy.
Step 2: Projection. According to Proposition 2 and Proposition 3, for an appropriate coefficient βk,
we project the policy πθ
k+2
onto the safe constraint set. Concretely, We use a measure D(∙, ∙) (e.g.,
KL divergence or '2-norm) to minimize distance between ∏θ
satisfies the safe constraint:
and πθ , and require the new policy
πθk+1
arg min D ∏θ ,∏θ^ɪɪ ,
∏θ∈∏θ	∖	k+2/
(18)
1
2
s.t. Jc(∏θk) + 1--TEs〜d∏o J),a〜∏θ(∙∣s) [aGAE,C,λ)(s, a)] + βkq∕Es〜d∏eJ∙) [KL(∏θk,∏θ)[s]] ≤ b.
Until now, the particular choice of surrogate function is heuristically motivated, we show the policy
and safe constraint guarantee of the proposed CUP in Theorem 2, and its proof shown in Appendix F.
Theorem 2. Let δk = Es〜dλ (.)[kL ∏^θk, ∏θk+1)[sf∣, if ∏θk and ∏θk+1 are related to (17)-(18),
then the lower bound on policy improvement, and upper bound on constraint violation are
〃	、〃	Y(I- λ)αk √2δk eVθ(πθ0) co(	'C
J (πθk+1) - J (πθk) ≥ - (1-γ) |1- 2γλ∣S∣A∣∣ , J (πθk+1) ≤ b +
Y(I- λ)βk√2δkeCθ (πθ0)
(1- Y) |1- 2γλ∣S∣∣A∣∣ .
Remark 2. Let λ → 0, according to Theorem 2, the performance and cost constraint of CUP satisfies
I(H γ γα γ γ Yak√2δke∏θ (πθ0 ) Tc(H ∖ CJYek√2δkeCθ (πθ0 )	门Q∖
J (πθk+ι ) - J (πθk ) ≥--(1 - Y)---, J (πθk+ι ) ≤ b + --(1 - Y)---.	(19)
The bounds of CUP in (19) achieves at O(O-Y) or O(-βkγγ), which is more compact than the bounds
of CPO (Achiam et al., 2017, Proposition 1-2), PCPO (Yang et al., 2020, Theorem 1) and FOCOPS
(Zhang et al.,2020) where their bounds achieve at O ((1二产).
Practical Implementation Now, we present our sample-based implementation for CUP (17)-(18).
Our main idea is to estimate the objective and constraints in (17)-(18) with samples collected by
current policy πθk, then solving its optimization problem via first-order optimizer. Due to the
limitation of space, we present pseudo-code of CUP in Algorithm 1 (see Appendix B).
For each {(st, at, rt+ι, ct+1)}T=1 〜∏θk, firstly, we update performance improvement step as:
θk+ 2
arg max
θ
{T X ∏θ⅛τ⅛At- αk∕DKL(πθk,πθ)}
(20)
where At is a estimator of AGAE(Y,λ)(s, a), DKL(∏θk,∏θ) = T PN=I KL(∏θk (∙∣st),∏θ(∙∣st)).
Then we update projection step by replacing the distance function D by KL-divergence, and we use a
soft constraint instead of the hard constraint (18),
θk+-
arg min {" X KL (g33∣st),∏θ 3厢)) + βkLcj ,
(21)
6
Under review as a conference paper at ICLR 2022
where Lc = JC + 1⅛Ψ ∙ 1 PT=1 ∏θ(attstt))AC + αk∖∕TPT=IKLnid(atis3^θ(atis3) - b,
JC and AC are estimators for cost-return and cost-advantage correspondingly.
5 Related Work
This section reviews some typical ways to solve safe reinforcement learning: local policy search,
Lagrangian approach, and constrained policy optimization (CPO). We provide more comparisons and
discussion in Appendix A and Table 3.
Local Policy Search and Lagrangian Approach. A direct way to solve CMDP (4) is to apply local
policy search (Peters & Schaal, 2008; Pirotta et al., 2013) over the policy space ΠC, i.e.,
πθk+1 = arg max J(πθ), s.t. Jc(πθ) ≤ b, andD(πθ,πθk) < δ,	(22)
πθ ∈Πθ
where δ is a positive scalar, D(∙, ∙) is some distance measure. For practice, the local policy search (22)
is challenging to implement because it requires evaluation of the constraint function c to determine
whether a proposed point π is feasible (Zhang et al., 2020). Besides, when updating policy according
to samples, local policy search (22) requires off-policy evaluation (Achiam et al., 2017), which is very
challenging for high-dimension control problem (Duan et al., 2016; Yang et al., 2018; 2021a). Thus,
local policy search (22) looks simple, but it is impractical for high-dimension policy optimization.
The standard way to solve CMDP (4) is Lagrangian approach (Chow et al., 2017; Xu et al., 2021)
that is also known as primal-dual policy optimization:
(∏? ,λ?) = arg min max {J (∏θ ) - λ(Jc(∏θ) - b)} .	(23)
λ≥0 πθ ∈Πθ
Although extensive canonical algorithms are proposed to solve problem (23), e.g., (Liang et al., 2018;
Tessler et al., 2019; Paternain et al., 2019; Le et al., 2019; Russel et al., 2020; Xu et al., 2020; Satija
et al., 2020; Chen et al., 2021), the policy updated by Lagrangian approach may be infeasible w.r.t.
CMDP (4). This is hazardous in reinforcement learning when one needs to execute the intermediate
policy (which may be unsafe) during training (Chow et al., 2018).
Constrained Policy Optimization (CPO). Recently, CPO (Achiam et al., 2017) suggests to replace
the cost constraint with a surrogate cost function which evaluates the constraint Jc(πθ) according to
the samples collected from the current policy πθk :
πθk+ι = arg πmaxβ	ES〜d∏θ% (∙),a〜∏θ(∙∣s) [A∏θk (s,a)]	(24)
s∙t∙ Jc(∏θk) + 占ES〜%上(∙),a〜∏θ(∙∣s) hA∏θk(s,a)i ≤ b,	(25)
DKL(ne,πθk) = ES〜d^θɛ (∙)[KL(πθ,πθk)[s]] ≤ δ∙	(26)
Existing recent works (e∙g∙, (Achiam et al∙, 2017; Vuong et al∙, 2019; Yang et al∙, 2020; Han
et al∙, 2020; Bisi et al∙, 2020; Bharadhwaj et al∙, 2021)) try to find some convex approximations
to replace the term A∏θk (s, a) and Dkl(∏θ, ∏θk) Eq∙(24)-(26)∙ Such first-order and second-order
approximations turn a non-convex problem (24)-(26) to be a convex problem, it seems to make a
simple solution, but this approach results in many error sources and troubles in practice∙ Firstly, it
still lacks a theory analysis to show the difference between the non-convex problem (24)-(26) and its
convex approximation∙ Policy optimization is a typical non-convex problem (Yang et al∙, 2021b);
its convex approximation may introduce some error for its original issue∙ Secondly, CPO updates
parameters according to conjugate gradient (Suli & Mayers, 2θ03), and its solution involves the
inverse Fisher information matrix, which requires expensive computation for each update∙ Later,
Yang et al∙ (2020) propose projected-based constrained policy optimization (PCPO) that also uses
second-order approximation, which also results in an expensive computation∙
Instead of using a convex approximation for the objective function, the proposed CUP algorithm
improves CPO and PCPO at least two aspects∙ Firstly, the CUP directly optimizes the surrogate
objective function via the first-order method, and it does not depend on any convex approximation∙
Thus, the CUP effectively avoids the expensive computation for the inverse Fisher information matrix∙
Secondly, CUP extends the surrogate objective function to GAE∙ Although Zhang et al∙ (2020)
has used the GAE technique in experiments, to the best of our knowledge, it still lacks a rigorous
theoretical analysis involved GAE before we propose CUP∙
7
Under review as a conference paper at ICLR 2022
一ff(M.
一THPO
——THPCM.
—CPO
—WO
一PCPO
——OP
O	18	期	90。	48	5M
Epxh
(a) CarButton
i	s⅛ «o «oo edo
⅛ocħ
(b) PointGoal-Level1
ww o	mo «0 eon eoα m∞
⅛ocħ
(c) PointGoal-Level2
WO	28	38
Epoch
(d) CarButton
—WO-L
THPO
一TWW.
一CFO
—WO
KFO
—CUP
(e) PointGoal-Level1
no	«0	«00	mo	ιo∞
Epodi
⑴ PointGoal-Level2
ɑæ
O	no	200	38	«0
Epodi
(g) CarButton
O	28	48	80	m)	180
中(xh
(h) PointGoal-Level1
(i) PointGoal-Level2
Figure 1: Learning curves for reward, cost, and cost rate on Gym ships with three pre-made robot.
6	Experiments
In this section, we show the effectiveness of CUP on three different sets of experiments (including
seven tasks): (i) robots with speed limit (Zhang et al., 2020); (ii) circle task (Achiam et al., 2017);
(iii) robot options and desiderata (Ray et al., 2019).
For task (i), we train different robotic agents to move along a straight line or a two-dimensional
plane, but the robot’s speed is constrained for safety purposes. For task (ii), the agent is rewarded for
running in a wide circle but is constrained to stay within a safe region smaller than the radius of the
target circle. Task (iii) is safety Gym ships with three pre-made robots that we use in the benchmark
environments from (Ray et al., 2019). All of those details are provided in Appendix G.1.
Baseline algorithms. We compare CUP to CPO (Achiam et al., 2017), PCPO (Yang et al., 2020),
FOCOPS (Zhang et al., 2020) in the task (i) and task (ii). To make a more comprehensive comparison,
we compare CUP with the unconstrained algorithms TRPO (Schulman et al., 2015) and PPO (Schul-
man et al., 2017), and compare CUP with two additional safe RL algorithms TRPO-Lagrangian and
PPO-Lagrangian, that combine the Lagrangian approach with TRPO and PPO.
Robots with Speed Limit and circle task. Table 1 shows that both CUP and FOCOPS consistently
enforce approximate constraint satisfaction while CUP has a higher performance than FOCOPS. CUP
outperforms CPO and PCPO significantly for both reward and cost. Those observations suggest the
projection step of CUP helps the agent to learn the safe constraints. We notice PCPO also has a
projection step, CUP performs better than PCPO due to CUP learning the objective and constraints
8
Under review as a conference paper at ICLR 2022
Table 1: Bootstrap mean with 100 bootstrap samples of reward/cost return after training on robot
with speed limit environments. Cost thresholds are in brackets under the environment names.
Environment	FOCOPS		CPO	PCPO	CUP
Walker2d-v3	Reward	1798.1 ± 0.3	1076.9 ± 9.8	1039.5 ± 5.2	2964.3 ± 10.8
(82)	Cost	82.3 ± 0.03	107.82 ± 1.16	100.25 ± 0.67	73.9 ± 0.09
Hopper-v3	Reward	1869.3 ± 2.8	1056.0 ± 5.0	1071.1 ± 4.6	2409.8 ± 5.6
(82)	Cost	83.1 ± 0.1	90.0 ± 8.2	74.8 ± 8.2	80.0 ± 0.1
AntCircle-v0	Reward	1206.3 ± 159.1	423.3 ± 12.6	342.9 ± 5.5	1879.7 ± 79.4
(50)	Cost	44.1 ± 4.2	51.3 ± 1.5	51.2 ± 2.4	49.3 ± 2.0
HumanoidCircle-v0	Reward	963.0 ± 40.0	329.5 ± 1.7	244.5 ± 7.5	1029 ± 49.0
(50)	Cost	50.6 ± 1.9	46.0 ± 0.4	47.1 ± 1.3	48.4 ± 2.8
Table 2: Normalized metrics from the conclusion of training averaged over various slates of environ-
ments and three random seeds per environment.
SGPoint	Jr	MC	pc I SGCar	Jr	MC	PC I SGDoggO	Jr	MC	PC
PPO	1.0	1.0	1.0 I	1.0	1.0	1.0 I	1.0	1.0	1.0
PPO-L	0.552	0.553	0.638 I	0.299	0.241	0.237 I	0.0	0.028	0.288
TRPO	1.077	0.906	0.991 I	1.153	0.899	0.874 I	0.704	1.492	1.108
TRPO-L	0.726	0.672	0.628 I	0.302	0.182	0.226 I	0.061	0.016	0.283
CPO	0.957	0.794	0.869 I	0.801	0.406	0.501 I	0.560	1.071	0.891
PCPO	0.226	0.321	0.830 I	1.066	0.234	0.993 I	0.890	0.843	0.528
CUP	1.303	0.507	0.452 I	1.472	0.191	0.206 I	1.096	0.007	0.214
under a non-convex function while PCPO uses its convex approximation, which is one motivation for
us to propose CUP.
Safety Gym Ships with Three Pre-made Robots. We compare the algorithms on three environ-
ments SGPoint, SGCar, and SGDoggo, which are all six Point/Car/Doggo robot environments with
constraints in Safety Gym. Thus, it is necessary to introduce the rule for comparing the aggregate
performance of algorithms across many environments by (Ray et al., 2019), where it assigns each en-
Vironment E a set of characteristic metrics, Jr, Jc, PE and compute normalized return Jr, normalized
constraint violation Mc, and normalized cost rate pc：
了 _	J(∏θ)	M _ max{0, Jc(∏θ)	- d}	_	_	pc
Jr =	~~JΓ,	MC = max{ 10-6,Jc	- d} ,	Pc	=	Pc.
Figure 1 shows the results from benchmarking unconstrained and constrained RL algorithms on all
Point level 1 and 2 environments, and the approximately constraint-satisfying training run (CostRate
curves). All the cost are shown in [25, 30] In Table 2, we show the normalized metrics from the
conclusion of training averaged over various slates of environments and three random seeds per
environment. All experiments were run with three random seeds. Results show that cost and rewards
trade off happens in SGPoint and SGCar, while CUP achieves the best performance in those two
environments. In the SGDoggo environment, CUP achieves the best performance and constraint
satisfaction over all the baseline algorithms.
7	Conclusion
In this paper, we propose the CUP algorithm with a theoretical safety guarantee. We derive the CUP
bases on some new proposed surrogate functions w.r.t. objective and constraints and the practical
implementation of CUP does not depend on any convex approximation. Extensive experiments on
continuous control tasks show the effectiveness of CUP where the agent satisfies safe constraints.
9
Under review as a conference paper at ICLR 2022
References
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of International Conference on Machine Learning (ICML), volume 70, pp. 22-31,
2017.
M Mehdi Afsar, Trafford Crump, and Behrouz Far. Reinforcement learning based recommender
systems: A survey. arXiv preprint arXiv:2101.06286, 2021.
Eitan Altman. Constrained Markov decision processes. CRC Press, 1999.
Richard Bellman. A markovian decision process. Journal of mathematics and mechanics, 6(5):
679-684, 1957.
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and
Animesh Garg. Conservative safety critics for exploration. In International Conference on
Learning Representations (ICLR), 2021.
Lorenzo Bisi, Luca Sabbioni, Edoardo Vittori, Matteo Papini, and Marcello Restelli. Risk-averse trust
region optimization for reward-volatility reduction. In Christian Bessiere (ed.), Proceedings of the
Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 4583-4589,
2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Yi Chen, Jing Dong, and Zhaoran Wang. A primal-dual approach to constrained markov decision
processes. arXiv preprint arXiv:2101.10895, 2021.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research,
18(1):6070-6120, 2017.
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-
based approach to safe reinforcement learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2018.
Imre Csiszar and Janos Korner. Information theory: coding theoremsfor discrete memoryless systems.
Cambridge University Press, 2011.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics.
Foundations and Trends® in Machine Learning, 2013.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning
(ICML), pp. 1329-1338, 2016.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research (JMLR), 5(Nov):
1471-1530, 2004.
Minghao Han, Lixian Tian, Yuanand Zhang, Jun Wang, and Wei Pan. Reinforcement learning control
of constrained dynamic systems with uniformly ultimate boundedness stability guarantee. arXiv
preprint arXiv:2011.06882, 2020.
David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician, 58(1):
30-37, 2004.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
Proceedings of International Conference on Machine Learning (ICML), volume 2, pp. 267-274,
2002.
10
Under review as a conference paper at ICLR 2022
Bingyi Kang, Shie Mannor, and Jiashi Feng. Learning safe policies with cost-sensitive advantage
estimation, 2021. https://openreview.net/forum?id=uVnhiRaW3J.
Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Interna-
tional Conference on Machine Learning (ICML),pp. 3703-3712, 2019.
Qingkai Liang, Fanyu Que, and Eytan Modiano. Accelerated primal-dual policy optimization for
safe reinforcement learning. arXiv preprint arXiv:1802.06480, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
OpenAI. Openai five defeats dota 2 world champions, 2019. https://openai.com/blog/
openai- five- defeats- dota- 2- world- champions/.
Santiago Paternain, Luiz FO Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained
reinforcement learning has zero duality gap. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.
Jan. Peters and Stefan. Schaal. Reinforcement learning of motor skills with policy gradients. Neural
Netw, 21(4):682-697, 2008.
M. Pirotta, M. Restelli, A. Pecorino, and D. Calandriello. Safe policy iteration. In International
Conference on Machine Learning (ICML), pp. 307-315, 2013.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019.
Reazul Hasan Russel, Mouhacine Benosman, and Jeroen Van Baar. Robust constrained-mdps: Soft-
constrained robust policy optimization under model uncertainty. arXiv preprint arXiv:2010.04870,
2020.
Harsh Satija, Philip Amortila, and Joelle Pineau. Constrained markov decision processes via backward
value functions. In International Conference on Machine Learning (ICML), pp. 8502-8511, 2020.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), pp. 1889-1897,
2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. International Conference on Learning
Representations (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Endre Suli and David F Mayers. An introduction to numerical analysis. Cambridge university press,
2003.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.
11
Under review as a conference paper at ICLR 2022
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems (NeurIPS),pp. 1057-1063, 2000.
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization.
International Conference on Learning Representation (ICLR), 2019.
Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M
Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al. Alphastar:
Mastering the real-time strategy game starcraft ii. DeepMind blog, 2, 2019.
Quan Vuong, Yiming Zhang, and Keith W Ross. Supervised policy update for deep reinforcement
learning. In International Conference on Learning Representation (ICLR), 2019.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade,
Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent
factorized baselines. International Conference on Learning Representation (ICLR), 2018.
Tengyu Xu, Yingbin Liang, and Guanghui Lan. A primal approach to constrained policy optimization:
Global optimality and finite-time analysis. arXiv preprint arXiv:2011.05869, 2020.
Tengyu Xu, Yingbin Liang, and Guanghui Lan. A primal approach to constrained policy optimization:
Global optimality and finite-time analysis. International Conference on Machine Learning (ICML),
2021.
Long Yang, Minhao Shi, Qian Zheng, Wenjia Meng, and Gang Pan. A unified approach for multi-step
temporal-difference learning with eligibility traces in reinforcement learning. In Proceedings of the
Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 2984-2990,
2018.
Long Yang, Gang Zheng, Yu Zhang, Qian Zheng, Pengfei Li, and Gang Pan. On convergence of
gradient expected sarsa (λ). In AAAI, 2021a.
Long Yang, Qian Zheng, and Gang Pan. Sample complexity of policy gradient finding second-order
stationary points. In AAAI, 2021b.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based
constrained policy optimization. In International Conference on Learning Representation (ICLR),
2020.
Moritz A Zanger, Karam DaaboUL and J MariUs Zollner. Safe continuous control with constrained
model-based policy optimization. arXiv preprint arXiv:2104.06922, 2021.
Yiming Zhang, Quan Vuong, and Keith Ross. First order constrained optimization in policy space. In
Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.
12
Under review as a conference paper at ICLR 2022
A	Additional Discussion about Related Work
This section reviews three typical safe reinforcement learning algorithms: CPO (Achiam et al., 2017),
PCPO (Yang et al., 2020) and FOCOPS (Zhang et al., 2020). Those algorithms also use new surrogate
functions to replace the objective and constraints, which resembles the proposed CUP algorithm. The
goal is to present the contribution of our work.
A.1 CPO (ACHIAM ET AL., 2017)
For a given policy πθk , CPO updates new policy πθk+1 as follows:
πθk+l = arg ∏maχ Es〜d∏θ% (∙),a〜∏θ(∙∣s) [A∏θk (s,a)]	(27)
s.t. Jc(πθk ) + 1-γEs〜d∏θ% (∙),a〜∏θ(∙∣s) [AΠθk (S，a)i ≤ b，	(28)
DKL(ne,πθk) = Es〜d/%(∙)[kl(πθ,πθk)[s]] ≤ δ∙	(29)
It is impractical to solve the problem (24) directly due to the computational cost. (Achiam et al.,
2017) suggest to find some convex approximations to replace the term A∏θfc (s, a) and Dkl(∏θ, ∏θk)
Eq.(24)-(26).
Concretely, according to (6), Achiam et al. (2017) suggest to use first-order Taylor expansion of
J(πθ) to replace the objective (24) as follows,
占Es〜d∏θ% (∙),a5θk (∙∣s) ∏θ((Is)) A∏θk (s，a) = J(πθ) - J(πθk) ≈ (θ - θk)>VθJ(πθ).
Similarly, Achiam et al. (2017) use the following approximations to turn the constrained policy
optimization (24)-(26) to be a convex problem,
占Es~d∏θk(.…θk(∙∣ j∏θ≡A∏θjs,a)] ≈ (θ-θk)>VθJc(∏θ)，	(30)
D KL (πθ, πθk ) ≈ (θ - θk )>H(θ - θk ),	(31)
where H is Hessian matrix of Dkl(∏θ, ∏θJ, i.e.,
∂2
H[i,j] =: ∂θ ∂θj Es 〜d∏θk (∙) [KL(πθ ,πθk )[s]],
Eq.(31) is the second-oder approximation of (26).
Let λ? , ν? is the dual solution of the following problem
λ?, ν? = arg max
λ≥0,ν≥0
where g = Ne Es 〜d∏θ% (∙),a 〜∏θ(∙∣s) [A∏θk (s,a)] , a = VθEs 〜d∏θ% (∙),a 〜∏θ(∙∣s) [A∏θk Ga)[，r =
g> Ha, S = a> H-1 a, and c = Jc(πθk) - b.
Finally, CPO updates parameters according to conjugate gradient as follows: if approximation to
CPO is feasible:
θk+1 = θk + bH-I(g - ν*a),
λ?
else,
θk+ι=θk- LHfTa
13
Under review as a conference paper at ICLR 2022
A.2 PCPO (YANG ET AL., 2020)
Projection-Based Constrained Policy Optimization (PCPO) is an iterative method for optimizing
policies in a two-step process: the first step performs a local reward improvement update, while the
second step reconciles any constraint violation by projecting the policy back onto the constraint set.
Reward Improvement:
πθk+1 =arg ∏maxfl Es 〜d∏θ* (∙),a 〜∏θ(∙∣s) [A∏θk (S，a)],
2	πθ ∈ θ	k
s.t.D) κL(πθ ,πθk ) = Es 〜d^ (∙)[KL(πθ ,πθk )[s]] ≤ δ;
Projection:
πθk+ι = arg ∏minθ D (πθ ,πθk+2)，
S.t. Jc(πθk ) + 1-γEs〜d∏θ% (∙),a〜∏θ(∙∣s) hAΠθk(s,a)i ≤ b.
Then, Yang et al. (2020) follows CPO (Achiam et al., 2017) uses convex approximation to original
problem, and calculate the update rule as follows,
θk+1
2^^2δ- _1
θk - ^g>H-igH g - max
J >-2δ-1 a>H-1g + Cʌ
V g> H 1g____________
a>L-1 a
/
L-1a,
where L = I if D is '2-norm, and L = H if D is KL-divergence.
A.3 FOCOPS (ZHANG ET AL., 2020)
Zhang et al. (2020) propose the First Order Constrained Optimization in Policy Space (FOCOPS)
that is a two-step approach. We present it as follows.
Step1: Finding the optimal update policy. Firstly, for a given policy πθk, we find an optimal
update policy π? by solving the optimization problem (27)-(29) in the non-parameterized policy
space.
π? = argm∈χ	Es〜d∏θ% (∙),a〜∏(∙∣s) [A∏θk (S, a)]	(32)
s.t. Jc(∏θk) + 1-γEs〜d∏θ% (∙),a〜∏(∙∣s) [A∏θk (S, a)i ≤ b,	(33)
DKL (πθ,πθk ) = Es〜dgθ (∙)[KL(π,πθk )[s]] ≤ δ∙	(34)
θk
If πθk is feasible, then the optimal policy for (32)-(34) takes the following form:
π*(ais)=πθk (a(ss)) eχp( λ (A∏θk(S,a)- νA∏θk(s,a))),	(35)
where Zλ,ν (S) is the partition function which ensures (35) is a valid probability distribution, λ and ν
are solutions to the optimization problem:
minn λν + Vb + λEs 〜d∏0 (∙),a 〜π*(∙∣s) [Zλ,ν (s)],
λ,ν ≥0	θk
the term b = (1 - γ)(b - Jc(∏θk)).
Step 2: Projection Then, we project the policy found in the previous step back into the parameterized
policy space Πθ by solving for the closest policy πθ ∈ Πθ to π? in order to obtain πθk+1 :
θk+1 = arg min Es^d∏θfc (∙)[KL(πθ,n?)[s]].
14
Under review as a conference paper at ICLR 2022
A.4 Comparison to CUP
Comparing to CPO and PCPO, the implementation of CUP does not depend on any convex approx-
imations. CPO learns its objective with the deep neural network via the first-order method (see
Appendix B).
Concretely, CPO and PCPO approximate the non-convex objective (or constraints) with first-order or
second Taylor expansion, but their implementations still lack a theory to show the error difference
between the original objective (or constraints) and its convex approximations. Additionally, their
approaches involve the inverse of a high-dimension Fisher information matrix, which causes their
algorithms to require a costly computation for each update when solving high-dimensional RL
problems. While the proposed CUP does not depend on any convex approximations, it learns
the policy via first-order optimization approaches. Thus, CUP does not involve the inverse of a
high-dimension Fisher information matrix, which implies CUP requires less memory than CPO and
PCPO.
Although FOCOPS is also a non-convex implementation, it heavily depends on the current best-
satisfied policy. It is known that the current best policy may not be the optimal policy, and FOCOPS
requires to project this policy back into the parametric policy space, which implies FOCOPS reduce
the chances for an agent to explore the environment since it may lose in a locally optimal solution.
While the proposed CUP does not depend on the current optimal policy, in fact, CUP requires the
agent to learn the policy according to (17), the numerical solution is not the current optimal policy,
which helps CUP to explore the environment.
15
UnderreVieW as a ConferenCe PaPersICLR 2022
Table 3: Comparison of some safe reinforcement algorithms.
5∖	Algorithm	Optimization problem	Implementation		Remark
	CPO (Achiam et al., 2017)	7rθfc+ι = argmaxπθe∏θ 叽〜〃界仁(∙),α~τre(∙∣s)卜天” ($，°)]， StjC (7¾)+Ms~d 界 J),α~7Γe(∙∣s) k:%(s,α)] ≤ 仇 ⅛L(πe,πθfc) =Es^o t0[KL(πe,πθfc)[s]] ≤ S	^fc⅛ι = arg maxe gτ(0 - θk), s.t. c + bτ (0 — θk) ≤ O, ∣(0-0fc)τH(0-0fc) ≤ J.		Convex Implementation
	PCPO (Yang et al., 2020)	Reward Improvement 7rθfc+ι = aτg max7reEHe 旧6~尾乳(∙)，α~7rg(∙㈤[在天” (s, ɑɔ]， s.t.力KL(Tre,τreQ =叽〜〃e(.)[KL(πe,πθfc)[s]] ≤ J; Prcjection 7Γθfc+ι = arg minπθ€∏θ Q(Tre, τr%十多)， s.t. Jc(πθfc) +	_ ^s^dPo (∙),α~7rg(∙㈤卜Μ”(s,α)] ≤ 权	Reward Improvement Θk+L = arg maxe gτ(0 - θk∖ s.t,∣(0-0fc)τH(0-0fc) ≤ J; Projection 7Γθfc+ι = argmine ∣(0 - 0fc)τL(0 - θk), s.t. c+ bτ(0 — θk) ≤ O.		Convex Implementation
	FOCOPS (Zhang et al., 2020)	Optimal update policy 7Γ* = arg∏iax7re∏%~d嬴(.),α~7r(.∣s) [Λr"(s,α)], S.t. J (7I^θfc) +	(.),α~π(.∣s) [^πβfc(s,α)] ≤ A ⅛L(πθ,πθfc) = Es_drafc(0[KL(π,πθfc)[s]] ≤ J; Prcjection πθfc+1 = argminπβe∏β	)[KL(πθ,π*)[s]]. 	⅛		Optimal update policy π*(α∣s)= ¾S exp (⅛ C4"" Wa)- vA^θk (s,α))); Projection ^fc+ι = argmh⅛Ms~d思(0[KL(πe,π*)[s]].		Non-Convex Implemen- tation
	CUP (Our Work)	Policy Improvement "+⅛ =arg.断{吼~%J"2⑸㈤黑B(S,α)]	Policy Improvement 0fc+l=arg∏1ax^f		Non-Convex Implemen- tation
		-a* jMs~(⅛"(∙) [KL(7rek,7re)[s]] I， Projection πθfc+1 =argπminβ Oki(M ), s.t/3) +占吼~%(."2⑸[心""S,.)] -l	∣	tfk	l	k	」 +βk.∕κs^dx (ɔ [KL(πθfc,πθ)[s]] ≤ b. V	θk	一0\ Projectio ^fc⅛ι = arg min —工 £=：	"∑2kL(t¾(∙ ISjTre (•厢))J; t=ι	) n {κL °%+JISjTre(∙∣st)) ,„ ɪ -	πe(at∣gt) ʌeɪ fc 1 -7 πθk (αt∣βt) t ʃ'	
Under review as a conference paper at ICLR 2022
B Conservative Policy Update (CPU) Algorithm
Algorithm 1 Conservative Policy Update (CPU)
Initialize: policy network parameters θ0 ; value network parameter ω0 ; cost value function
parameter ν0, step-size ν0 ;
Hyper-parameters: trajectory horizon T; discount rate γ; episode number M, N, mini-batch size
B, positive constant α, η;
for k = 0, 1, 2, . . . do
Collect batch data of M episodes of horizon T in ∪iM=1∪tT=0{(si,t, ai,t, ri,t+1, ci,t+1)} according
to current policy πθk ;
Estimate c-return by discount averaging on each episode: JC = PT=0 Y匕/+1；
Compute TD errors ∪iM=1 ∪tT=0 {δi,t }, cost TD errors ∪iM=1 ∪tT=0 {δiC,t }:
δi,t = ri,t + γVωk (si,t) - Vωk (si,t-1), δi,t = ci,t + γVνk (si,t) - Vνk (si,t-1);
ComputeGAErMi ∪T==0 {A∙m,ACJ: Ai,t = PjH%, AC = P"(γλ)j-tδC;
Compute target function for value function and cost value function as follows,
Vitarget = Ai,t + Vωk(si,t), Vt,arget,C = AC + VC(Si,t);
StOre data: Dk = ∪M=1 ∪T=0 {(ai,t, sit Ai,t, ACt, Vtarget, Vtarget,°)};
∏old - ∏θk;
for i = 0, 1, 2, . . . , M do
θk+2=argmax IT X π⅛a⅛⅛AiLat
end for
πold - πθk+ι ;
νk+1 = (Vk + η(Ji - b))+;
for i = 0, 1, 2, . . . , M do
Policy Improvement
1T
τ EKL(nold(Isi,t),πe(Isi,t))	;
t=1	
Projection
T
θk+i = argmin τ X {KL(πθold (D, πθ (D)+Vk 1 二(T ；；：
;
end for
for each mini-batch {(aj, Sj, A^j, J^c , Vjtarget, Vtarget,c)} of Size B from Dk do
ωk+1
B
arg∏ιin X (Vω (Sj) - Vtarget『，νk+ι
ω j=1
B
arg min X
ν j=1
- Vtarget,C2
;
end for
end for
B.1 Practical Implementation of CUP
In this section, we present the practical implementation of CUP.
Step 1: Policy Improvement
17
Under review as a conference paper at ICLR 2022
For the first step,
πθk+1 =arg ∏maxβ {Es~d∏θJ∙),a~∏θ(∙∣s) IAGAE(γ,λ) Ga)i - αk q∕Es-d∏θfc (∙) [KL(πθk ,πθ)[s]] },
according to
Es~d∏θk()a~πθ(∙ιs) hAGAAE(γ,λ)(s，a)i =叽~/仁()α~πθk(，|S) ∏θ KAGAZ(Y，"(S，a) ,(36)
for each data sampled from ∪iM=1 ∪tT=0 {(si,t, ai,t, ri,t+1, ci,t+1)} according to current policy πθk,
We learn the parameter。k十 ι as follows,
θk+1 = arg max
1 S πθ (ai,t|si,t) ^
T t=1 ∏θk(ai,t∣Si,t) AiL αt
1T
T ∑KL(∏θk (1si,iπθ(∙lsi,t)) ;,
which can be solved via the first order optimizer.
Step 2: Projection
Now, consider the second step:
πθk+ι =arg ∏m∈∏θ D lπθ ,πθk+2)，
s.t. Jc(∏θk) + :j-ɪ-rEs~d∏ (∙),a~∏θ(∙∣s) ∣AGAE(C,λ)(s,a)i + Bkq∕Es~d∏ (∙) [KL(∏θk,∏θ)[s]] ≤ b
1 - γ πθk	k	πθk
We turn the projection step as the following unconstrained problem:
θm≥0{D bθ,πθk+l) + V(JcEk ) +占 Es~dλθJ),a~∏θ(∙∣s) [AGA¾'λ) (s，a)i
+Bk yEs-d∏β (•)[KL(πθk ,πθ)
(37)
In our implementation, we use KL-divergence as the distance measure D(∙, ∙), then
D (πθ,πθk+2) = Es-d∏θfc (∙) [KL (πθk+1 ,πθ) M .
(38)
To simplify the problem, we ignore the term Bk /旧§~壮入(.) [KL(∏θk,∏θ)[s]] due to the follow-
πθk	k
γ(l-λ)√2δk∈Cθ	(πθk )
ing two aspects: (i) firstly, Bk is adapted to the term (i-γλ)∣ι-2γf∣S∣∣A∣∣ , and for the high-
dimensional state space or continuous action space, then Bk is very small; (ii) secondly, if D is a
KL-divergence measure, then the direction of the policy optimization D ∏θ,, ∏θk十 J (38) is pro-
portional to Bk jEs~dλ(∙) [KL(∏θk,∏θ)[s]], thus, in practice, we can only optimize the distance
D (∏θ, ∏θk ι). Above discussions implies that instead of (37), we can consider the problem
min L(θ, ν),
θ,ν≥0
where
L(θ,ν) = D (πθ ,πθk+J + V(Jc(πθk ) + 1--^ Es~d∏θJ∙),a~∏θ(∙∣s) hAGAE,C'λ)(s,a)i - b
Then, according to gradient decent method, we have
θ 一 θ -
L(θ, v)
η^θ~
L(θ,v)
-rη^τ
(39)
V — V
18
Under review as a conference paper at ICLR 2022
Particularly,
L(θνν) = Jc(∏θk) +占Es~d∏θJ∙),a~∏θ(∙∣s) hAGAE,C,λ)(s,a)i - b,
(40)
where the term E§f (∙),a~∏θ(.⑸[AGAEC'λ)(s, a)] can be estimated following the idea as (36).
But recall (17) is a MM-iteration, i.e., We require to minimize Es~dλ (,)KL (∏θ, ∏θk) [s], which
πθk
implies ∏θ is close to ∏θk. Thus it is reasonable E§~d* (∙),a~∏θ(∙∣s) AπGθAE,(Cγ,λ) (s, a) ≈ 0, thus, in
practice, we update ν following a simple way
V - (V - η(Jc(πθk) - b))+,
where (∙)+ denote the positive part, i.e., if X ≤ 0, (x)+ = 0, else (x)+ = x.
Finally, according to (39), for each data sampled from ∪iM=1 ∪tT=0 {(si,t, ai,t, ri,t+1, ci,t+1)} according
to current policy πθk , we learn the parameter θk+1 as follows,
T
θk+1 = argmin τ χ{KL (πθk+1 (∙Mt),πθ (Isi,t))+νk 二 ∏⅛"itt) ACJ,
which can be solved via the first-order optimizer.
19
Under review as a conference paper at ICLR 2022
C Notations
C.1 Matrix Index
In this paper, we use a bold capital letter to denote matrix, e.g., A = (ai,j) ∈ Rm×n, and its (i, j)-th
element denoted as
A[i, j] =: ai,j,
where 1 ≤ i ≤ m, 1 ≤ j ≤ n. Similarly, a bold lowercase letter denotes a vector, e.g., a =
(。1,。2,…,an) ∈ Rn, and its i-th element denoted as
a[i] =: ai ,
where 1 ≤ i ≤ n.
C.2 Key Notations of Reinforcement Learning
For convenience of reference, we list key notations that have be used in this paper.
C.2. 1 Value Function and Dynamic System of MDP.
r∏θ , R∏θ (S), v∏θ , V∏θ (S),	r∏θ ∈ R|S| is the expected vector reward according to ∏θ, i.e., their compo- nents are: r∏θ [s] = Pa∈∕ Pso∈s ∏θ(a∣s)r(s0∣s, a) =: R∏θ (s), S ∈ S. v∏θ ∈ R|S| is the vector that stores all the state value functions, and its components are: v∏θ[s] = V∏θ (s), S ∈ S.
	ρS, P	ρ(s): the initial state distribution of state s; ρ ∈ R|S|, and ρ[s] = ρ(s).
P∏θ^^ P∏θ (s'|s) Pπθ (St = S0 |S)	Single-step state transition matrix by executing ∏θ . Single-step state transition probability from S to S by executing ∏θ, and it is the (s, S0)-th component of the matrix P∏θ , i.e., P∏θ [s, s'] = P∏θ (s' |s). : The probability of visiting the state S0 after t time steps from the state S by executing πθ, and it is the (S, S')-th component of the matrix Pπθ, i.e., p∏θ[s,s0] = p∏θ (St = s0|s).	.	
dsπ0θ (S), dρπ0θ (S) d∏θ	The normalized discounted distribution of the future state S encountered starting at S0 by executing πθ: dsπ0θ (S) =: (1 - γ) Pt∞=0 γtPπθ (St = S|S0). Since so 〜ρ(∙), we define d∏0θ (s) =: Es0〜ρ(∙)[d∏θ (s)]. : It stores all the normalized discounted state distributions dρπ0 (S), ∈ S, i.e., d∏0 ∈ R|S|, and its components are: d∏0 [s] = d∏0 (s).	
C.2.2 EXTEND THEM TO λ-VERSION.
p(λ p∏;(SiS)	P∏2 = (1- γλ) P∞=o(γλ)tP∏+1. p∏λ)(SlS) =： p∏θ) [s,s'] = (I-Yx) P∞=o(γλ)tp∏θ (st+i = SlS).	
r∏θ), R∏θ)(s)	r(λ = P∞=o(γλP∏θ)tr∏θ； R∏θ)(s) =： r∏θ)[s].
Y	Y =「. 1 — γλ		
d∏θ,λ(s) d∏θ(s), d∏θ	d∏θ,λ(s) = (1- Y) P∞=0 YtPni(St = S∣S0). d∏θ(S) = Es0 〜po(∙) [d∏θ,λ(s)], d∏θ[s] = d∏θ(s).	
C.2.3 TD ERROR W.R.T. ANY FUNCTION 夕(∙).
% ,t(s) δ∏ ,t	-δt = r(st+ι∣St, at) + γ^(st+ι) - ^(sj δ∏θ,t (S) = Est~P∏θ (∙∣s),at~∏θ(∙∣st),st+ι~P(∙∣st,at) [δt ]. δΠθ ,t[s]=篇,t(s).
Ng ,πθo, s)	w	∣Y πθ(at|St)	1 ʌ λ√∣ Est~P∏aOGIs),at-%' GIst),st+ι~p(∙lst,aj Ineo (a∕s古)-1J δt .
δF(πθ ,πθ0 )	δF(πθ ,πθ0 Hsl = δF(πθ,πθ0,s).	θ	
20
Under review as a conference paper at ICLR 2022
D Preliminaries
In this section, we introduce some new notations about state distribution, policy optimization and
λ-returns.
D. 1	State Distribution
We use P∏θ ∈ RlSl×lSl to denote the state transition matrix by executing ∏θ, and their components
are:
P∏θ[s,s0] = E∏θ(a∣s)P(s0∣s,a)=: P∏θ(S |s), s,s ∈ S,
a∈A
which denotes one-step state transformation probability from s to s0 .
We use Pπθ (st = s|s0) to denote the probability of visiting s after t time steps from the initial state
s0 by executing πθ. Particularly, we notice if t = 0, st 6= s0, then Pπθ (st = s|s0) = 0, i.e.,
Pπθ (st = s|s0) = 0, t = 0 and s 6= s0.	(41)
Then for any initial state so 〜ρ(∙), the following holds,
Pπθ (st = s|s0) = X Pπθ (st = s|st-1 = s0)Pπθ (st-1 = s0|s0).	(42)
s0∈S
Recall dsπ0 (s) denotes the normalized discounted distribution of the future state s encountered starting
at s0 by executing πθ,
∞
dsπ0θ (s) = (1 -γ)XγtPπθ(st = s|s0).
t=0
Furthermore, since s° 〜ρo(∙),we define
d∏θ (S)= Eso 〜ρo(∙)[d∏θ (s)]= /
s0∈S
ρ0(S0)dsπ0θ (S)dS0
as the discounted state visitation distribution over the initial distribution ρo(∙). We use d∏θ ∈ R|S| to
store all the normalized discounted state distributions, and its components are:
dρπ0θ[S]=dρπ0θ(S), S∈S.
We use ρ0 ∈ R|S| to denote initial state distribution vector, and their components are:
ρ0[S] = ρ0(S), S ∈ S.
Then, we rewrite dρπ0θ as the following matrix version,
∞
dρπ0θ = (1-γ)X(γPπθ)tρ0= (1-γ)(I-γPπθ)-1ρ0.	(43)
t=0
D.2 Objective of MDP
Recall T = {st,at,rt+1}t≥0 〜 ∏θ, according to T, we define the expected return J(∏θ|so) as
follows,
J (πθ |s0)=Eτ 〜∏θ [R(τ )] = i-γ Es 〜d∏θ (∙),a 〜∏θ(∙∣s),s0 〜P(∙∣s,α) hr(s0|s,a)i ,	(44)
where R(T) = Pt≥0 Ytrt+ι, and the notation J(∏θ |so) is “conditional” on so is to emphasize the
trajectory T starting from S0 .
Since so 〜ρo(∙), we define the objective of MDP as follows,
J(πθ) = 1-YEs〜d∏θ (∙),a〜∏θ(∙∣s),s0〜P(∙∣s,α) hr(s0|s，a)i .	(45)
The goal of reinforcement learning is to solve the following optimization problem:
θ? = arg max J(πθ).	(46)
θ∈Rp
21
Under review as a conference paper at ICLR 2022
D.3 Bellman Operatorn
Let Bπθ be the Bellman operator:
Bπθ : R|S| → R|S|,	v 7→ rπθ + γPπθ v,	(47)
where rπθ ∈ R|S| is the expected reward according to πθ, i.e., their components are:
rπθ[s]=	∏θ(a∣s)r(s0∣s, a) =: R∏θ (s), S ∈ S.
a∈A s0 ∈S
Let vπθ ∈ R|S| be a vector that stores all the state value functions, and its components are:
vπθ [s] = Vπθ (s), s ∈ S.
Then, according to Bellman operator (47), we rewrite Bellman equation (Bellman, 1957) as the
following matrix version:
Bπθ vπθ = vπθ .	(48)
Furthermore, we define λ-Bellman operator Bπλ as follows,
∞
Bπλθ = (1 - λ)Xλt(Bπθ)t+1,
t=0
which implies
B∏θ : R|S| → R|S1, v→ r∏λθ) + 俚 ∏λθ) v,	(49)
where
Po) = (1 - λ)X XSλ)tPt+1 r(λ) = XSλP7r )tr7r	N = γ(1 - λ)	(50)
∏ ∏θ	∏ ∏θ , r ∏θ	γ∖∏ ∏θ r ∏θ , Y ] -	.	(JU)
t=0	t=0	1 - γλ
Let
∞
P∏j(s0∣s) = P∏λ)[s, S0] =: (1 - γλ) X(γλ)t (p∏+1 [s, S0]) ,	(51)
t=0
where Ptπ+1 [S, S0] is the (S, S0)-th component of matrix Ptπ+1, which is the probability of visiting S0
after t + 1 time steps from the state S by executing πθ, i.e.,
Ptπ+θ1[S, S0] = Pπθ (St+1 = S0 |S).	(52)
Thus, we rewrite P(πλθ)(S0 |S) (51) as follows
∞
P(πλθ)(S |S) = (1 - γλ) X(γλ)tPπθ(St+1 = S |S), S ∈ S.	(53)
t=0
D.4 λ-RETURN
Furthermore, recall the following visitation sequence τ = {St, at, rt+1}t≥0 induced by πθ, it is
similar to the probability Pπθ (St = S0 |S0), we introduce P(πλθ)(St = S0 |S0) as the probability of
transition from state S to state S0 after t time steps under the dynamic transformation matrix P(πλθ).
Then, the following equity holds
P(πλθ)(St = S|S0) = X P(πλθ)(St = S|St-1 = S0)P(πλθ)(St-1 = S0|S0).	(54)
s0∈S
22
Under review as a conference paper at ICLR 2022
Similarly, let
∞∞
Rni(S) =: r∏λθ)[s] = X(YλP∏θ )trπθ [s] = X(Yλ)t X Pno (st = SIS)Rne (Sj
t=0	t=0	s0∈S
∞
=XX(γλ)tP∏e (st = s0∣s)R∏e (s0).	(55)
t=0 s0∈S
It is similar to normalized discounted distribution dρn0 (S), we introduce λ-return version of discounted
state distribution dnλ (S) as follows: ∀S ∈ S,
∞
d∏θ,λ(s) = (1- Y) X寸Pny(St = S∣so),
t=0
d∏e (S)= Eso〜P0(∙) [d∏θ,λ(S)],
dλnθ [S] = dnλθ (S),
(56)
(57)
(58)
where P(nλθ)(St = S|S0) is the (S0, S)-th component of the matrix P(nλθ) , i.e.,
P(nλθ)(St = S|S0) =: P(nλθ)	[S0, S].
Similarly, P(nλθ)(St = S0 |S) is the (S, S0)-th component of the matrix P(nλθ) , i.e.,
PnT(St = S0|s) =: (P∏λθ))t [s,s0].
Finally, we rewrite dρn0,λ as the following matrix version,
(59)
Remark 3 (λ-Return Version of Bellman Equation). According to Bellman equation (48), vnθ is
fixed point of λ-operator Bnλ , i.e.,
Vne= r∏λθ) + 7P∏λθ)vnβ .
(60)
Recall T = {s^ at, rt+1}t≥0 〜∏θ, according to (60), the value function ofinitial state so is
Vno (SO) = vne [s0] = K? [s0] + 5^^2^ [s0]
=Rnλθ)(sθ) + 7 X PnT(SI = /同)咚0 (s')∙	(61)
s0∈S
23
Under review as a conference paper at ICLR 2022
We unroll the expression of (61) repeatedly, then we have
Vπθ (s0)
=Rni(SO) + Y X Pni(SI = s0 |so) (碟)(S j + Y X Pn))(S2 = s00 ls1 = sjV∏θ (SOj
s0 ∈S	s00 ∈S
'----------------------V----------------------
=Vπθ (s0)
=RnT(S°)+Y X PnT(SI =S |So )R∏λI)(S)
s0∈S
+γ2 X (X哦)(SI = sis。)Pni(S2 = s'0® = s')[ V∏β(S00)
s00 ∈S s0∈S
S---------------------{z--------------------}
(=4"P∏λ)(s2=Sθθ∣S0)
=RnT(S0)+YE Pny(SI = sis。)RnT(S)+γ2 EPny(S2 = s∣so)vπβ (S)
s∈S	s∈S
=Rni(S°)+γ X Pny(SI = s∣so)Rnτ(S)
s∈S
+γ2 X PnT(S2=s|so) (Rnθ)(s)+γ X Pnθ) (s3 =J同=S)Vne (SO)
s∈S	s0∈S
=Rni(S°)+γ X Pny(SI = sis。)Rni(S)+γ2 X Pny(S2 = 5同)氏*心)
s∈S	s∈S
+γ3 X (X Pny (s2 =s|s。)Pny(S3 = s0 |s2 = S))咚。(SO)
sO∈S	s∈S
s-----------------{z----------------}
=R(I)(SO) + YEPny(SI = S∣S0)Rni(S) + Y2 EPny(S2 = s∣S0)Rnλ)(s)
s∈S	s∈S
+ Y3 XPny(S3 = s∣so)Vπβ (S)
s∈S
∞
=XX 千 PnT(St = sis。)Rny(S)(=) L X dne,λ (S)Rn))(s).	@)
s∈S t=。	Y s∈S
According to (44) and (62), we have
J(∏θ) = X PO(SO)Vne (SO) (=) 占 X PO(SO) X 端入⑹吟)⑹
s0∈S	s0∈S	s∈S
=占 X(X PO(sO)dnθ,λ(s)) Rni(S)
Y s∈S s0∈S
X---------{----------}
=dλπe (s)
=1⅛ X 说(S)Rny(S) =占氏〜%(.) hRni(S)i.	(63)
-Y s∈S	-Y
Finally, we summarize above results in the following Lemma 1.
24
Under review as a conference paper at ICLR 2022
Lemma 1. The objective J (πθ) (45) can be rewritten as the following version:
J(∏θ)=占 Xd∏θ(s)R∏λθ)(s) = 1⅛Es~d∏θ(•) h*(s)].
- γ s∈S	-γ
E Proof of Theorem 1
We need the following Proposition 4 to prove Theorem 1, which illustrates an identity for the objective
function of policy optimization.
Proposition 4. For any function 夕(∙) ： S → R, for any policy ∏θ, for any trajectory satisfies
T = {st,at,rt+1}t≥0 ~ ∏θ, let
δt = r(St+i|st,at) + YiXst+1) - 9(st),
δ∏θ ,t(S) = Est~P∏θ (∙∣s),at~∏θ (Tst),st+ι~P(∙∣st,at) [δt ] ,
then, the objective J(πθ) (63) can be rewritten as the following version:
J (πθ >Es0~ρo(∙)[P(SO)] +
1⅛ X d∏θ(s) (X γtλtδ∏θ ,t (S)
- γ s∈S	t=0
(64)
1∞
=%~00(.)[分0)] +1-ɪEs~dλθ(•) EYtλtc,,t(S)
1-	γ	t=0
We present the proof of of Proposition 4 at the end of this section, see Section E.2.
We introduce a vector δ∏θ,t ∈ R|S| and its components are: for any S ∈ S
δ∏θ ,t[S] = δ∏ζ ,t(S).	(65)
Then, we rewrite the objective as the following vector version
∞
J (πθ) = Eso~P0(.)[i?(S0)] 十 厂二 X Ytλthdλθ, δ∏θ,ti,	(66)
1-	γ t=0
where〈•，•〉denotes inner production between two vectors.
E.1 Proof of Theorem 1
Theorem 1 (Generalized Policy Performance Difference) For anyfunction 夕(∙) ： S → R ,for two
arbitrary policy ∏θ and ∏θ> ,forany p, q ∈ [1, ∞) such that P + q = 1, Thefollowing bound holds:
∞∞
1-X Yt λt Mp,,q-t(πθ ,πθ0 ) ≤ J (πθ ) - J (πθ0 ) ≤ 1--ɪ X Yt λt Mp,,q+t(πθ ,πθ0 ),	(67)
1	- γ t=0	1 - γ t=0
where the terms M；q- and M^qjt are defined in (83)-(84).
Proof. (of Theorem 1)
We consider two arbitrary policies πθ and πθ0 with different parameters θ and θ0 , let
0"λ)(∏θ,∏θ0) =： hd∏θ, δ∏ζ,ti - hdʌ o, δ∏ o,ti.	(68)
θθ
According to (66), we obtain performance difference as follows,
∞
J (∏θ) - J (∏Θ0) =I-^ X γtλt (hd∏θ, δ∏θ ,ti-hd∏θ0, %,ti)
- Y t=0
∞
=1--3 X γt*Dteλ(πθ,πθ0),	(69)
1 - Y t=0
25
Under review as a conference paper at ICLR 2022
which requires Us to consider the boundedness of the difference D'『°)(∏θ, 斗，) (68).
Step 1: Bound the term D^,(λ)(∏θ, ∏θo ) (68).
We rewrite the first term of (68) as follows,
hd∏θ, δ∏θ,ti = hd∏β0, δ∏θ,ti + hd∏θ - d∏β0, δ∏θ,ti,	(70)
which is bounded by applying Holder,s inequality to the term hd∏β - d∏ 0, δ∏ζ,J, we rewrite (70) as
follows,
hdλθ0, δ∏θ ,ti-kd∏θ - d∏θ0kp kδ∏θ ,tkq
≤hd∏θ , δ∏θ,ti ≤ hd", δ∏θ ,ti + kdλθ - d∏β 0kpkδ∏θ ,tkq ,	(71)
wherep, q ∈ [1, ∞) and P + 1 = 1. Let
Mλ)(∏θ ,∏Θ0) =: kdλθ - d"kpkδ∏θ ,tkq,
then we rewrite Eq.(71) as follows,
λ ψ ∖	4,(λ)	λ ψ	λ ψ	4,(λ) /
hdπλ0,δπθ ,ti - p,q,t(πθ,πθ0) ≤ hdπλθ, δπθ ,ti ≤ hdλπ 0,δπθ ,ti +p,q,t(πθ,πθ0).	(72)
Let
M,ip(∏θ,∏θ0 )=： hd∏β0,6北,ti-hd", δζ0 ,ti,	(73)
、-----V----} 、------V-----}
Term-I	Term-II
combining the (68) and (72), we achieve the boundedness of Dtp(πθ, πθ0 ) as follows
p	p,(λ)	p	p	p,(λ)
Mt(πθ,πθ0) - p,q,t(πθ,πθ0) ≤ Dt(πθ,πθ0) ≤ Mt(πθ,πθ0) +p,q,t(πθ,πθ0).	(74)
Step 2: Analyze the term Mtp(πθ, πθ0 ) (73).
To analyze (74) further, we need to consider the first term appears in Mtp(πθ, πθ0 ) (73):
Term-I(73)=hdπλ0,δπpθ,ti
=X dλθ0 (S)δpθ,t(s) = Es~d∏ 0(∙) [δ∏θ,t(S)]
s∈S	θ
(=)Es~d∖(∙) [Est~p∏θ (∙∣s)[δ∏θ (St)]].
We notice the following relationship
δπpθ,t(S) = E	[δtp] = E
st~P∏θ (∙∣s)	St~P∏ 0 (∙∣s)
at~πθ(Ilst) 、	at~∏θ° (∙∣st)
st+1~P(Ist,at)	st+ι~P(.∣st,at)
,∏θ(at∣St) δP
πθ0 (at|St) t
(75)
(76)
(77)
which holds since we use importance sampling: for any distributionp(∙) and q(∙), for any random
variable function f (∙),
Ex~p(x)[f (X)] = Ex~q(x)
% f (X)].
According to (75), (77), we rewrite the term hdλπ
πθ
, δπpθ,ti in Eq.(73) as follows,
Term-I (73) = hdλπ 0,δπpθ,ti =	dπλ0
s∈S
(S)
E
st-
一∏θ(at∣St)
(•Is) [∏θ0(at∣st)
δtp
(78)
at~∏θ0 (∙∣st)
∖st+ι~P(∙∣st,at)
0
/

/
26
Under review as a conference paper at ICLR 2022
Now, We consider the second term appears in Mf (∏θ, n8，) (73):
Term-II (73) = hdλπ 0,δπf0,ti
= X dπλθ0 (s)δπfθ0,t(s) = X dπλθ0 (s)
s∈S	s∈S
E
St~P∏ o (∙∣S)
at〜∏θ/ (∙∣st)
∖st+ι~P(∙∣St,at)
[δf]
/
(79)
/ ∖
Finally, take the results (78) and (79) to (73), we obtain the difference between hdλπ 0 , δπfθ,ti and
hdπλ 0 , δπf 0 ,ti, i.e., we achieve a identity for Mtf(πθ, πθ0 ) (73) as follows,
Mtf(πθ,πθ0)(7=3)hdλπ0,δπfθ,ti-hdπλ 0,δπf0,ti
θ
θ
θ
/
∖
(78=,(79)Xdλπ
s∈S
(s)
E
"P,
∏θ (at∣st)
(∙∣s) IΛπθ0(atlst)
- 1	δtf
(80)
at 〜∏θ'(Tst)
∖st+ι~P(∙∣st,at)
0
/
To simplify expression, we introduce a notation as follows,
∆tf(πθ, πθ0, s) =:	E
st 〜Pn /
∏θ (at∣st)
(∙|s) ∣Λ∏θ0(at∣St)
- 1	δtf ,
(81)
at〜∏θ/ (∙∣st)
st+1 〜P(∙∣st,at)
and we use a vector ∆tf(πθ, πθ0 ) ∈ R|S| to store all the values {∆tf(πθ, πθ0 , s)}s∈S:
∆tf(πθ, πθ0)[s] = ∆tf(πθ, πθ0, s).
Then we rewrite hdλπ 0 , δπf ,ti - hdπλ 0 , δπf 0 ,ti (80) as follows,
Mtf(πθ,πθ0)=hdλπ0,δπfθ,ti-hdπλ0,δπf0,ti
(8=0) X dπλ 0 (s)∆tf(πθ, πθ0, s) = hdλπ 0, ∆tf(πθ, πθ0)i.
s∈S
Step 3: Bound on J(πθ) - J (πθ0).
Recall (74), taking above result in it, we obtain
hdλ , ∆tf (πθ, πθ0 )i	- f,(λt) (πθ,	πθ0 )	≤ Dtf (πθ, πθ0 )	≤ hdλ	, ∆tf (πθ, πθ0 )i	+ f,(λt) (πθ, πθ0 ).
π 0 , t , θ	p,q,t , θ	t , θ	π	0 , t , θ	p,q,t , θ
(82)
Finally, let
Mf,-t(πθ, πθ0 ) = hdλ , ∆tf(πθ, πθ0 )i - f,(λt)(πθ, πθ0 )	(83)
p,q,t θ	πθ0	t	θ	p,q,t	θ
/	∖
=Xdπθ0(S)…e0(∙/(∏θ⅞⅛))- 1)M -kdπθ-dπθ0kpkδfθ,tkq
s∈S	θ0
at〜∏θ/ (∙∣st)
∖st+ι~P(∙∣st,at)	)
=Es 〜dλ 0
(•)
E [( πθ叫)-1) δf
st 〜Pnj (∙∣s) L∖∏θ0 (at∣St)	)
at〜∏g∕ (∙∣St)
.st+1 〜P(∙∣st ,at)
- kdπλθ - dπλ 0 kpkδπfθ,tkq.
27
Under review as a conference paper at ICLR 2022
and
Mp,q+(∏ ,πθO) = hdπ o, △汽 油 ,πθ0 )i+^p,ft(πθ ,πθ0)	(84)
θ
/	、
=X%(S)	…Eo(•"(≡⅛⅛- 1)δp]	+ kdπθ-dλθ0kpkδpθ，儿
s∈S	θ0
at〜∏θ/ (∙∣st)
∖st+ι~P(∙∣st,at)	)
=Es〜d∏ 0 (•)
θ0
E	[(应* - 1) δp
St 〜P∏∕ (∙∣s) L∖∏θ0 (at∣St)	)
at〜∏θ/ (∙∣st)
■st+1 〜P(∙∣st ,at)
+ kdπλθ - dπλθ0 kpkδπpθ,tkq.
According to (69) and (82), we achieve the boundedness of performance difference between two
arbitrary policies πθ and πθ 0 :
∞
二; X γtλtMp,,l-t(πθ,πθ0)
1 - γ t=0
'----------------------------}
≤ J(πθ) - J(πθ0) ≤
{z"∖^^^^
= Lp,-
∞
口 X Y tλtMp+t(∏θ ,∏Θ0).
1 - γ t=0
----------------{-----------------}
Lp,+
=:Lp,q,
(85)
□
E.2 Proof of Proposition 4
Proof. (of Proposition 4).
Step 1: Rewrite the objective J(πθ) in Eq.(63).
We rewrite the discounted distribution dπλθ (59) as follows,
ρ0-占 d∏θ + 占 PnTdn。= 0.	(86)
Let 夕(∙)be a real number function defined on the state space S, i.e.,夕：S → R. Then We define a
vector function φ(∙) ∈ R|S| to collect all the values {夕(s)}s∈s, and its components are
φ[s]=夕(s), s ∈ S.
NoW, We take the inner product betWeen the vector φ and (86), We have
0= hP0 -占d∏θ + τ⅛γPnydλθ,Φi
=hρo, Φi-占 hdλθ, Φi + τ⅛3- hP∏λ)d∏θ, Φi.	(87)
We express the first term hρ0, φi of (87) as folloWs,
hp0, φi = X P0(SW(S) = Es〜ρo(∙) [以s)].	(88)
s∈S
We express the second term hdπλ , φi of (87) as folloWs,
-ɪ-ɪ hdλθ, φi = -1⅛ X dn。(SW(S) = - ɪ--ɪ Es~dλθ (∙) 3(S)].	(89)
- γ	- γ s∈S	- γ
28
Under review as a conference paper at ICLR 2022
We express the third term hγP∏θ)dλβ, φi of (87) as follows,
占hp∏λ)d∏θ,Φi =1¾ X (p∏λ)d∏θ) [s0Ms)
γ	γ0
s ∈S
=1⅜ X(X p∏T(s0∣s)dλθ(s)) ∕j.
- γ s0∈S s∈S
(90)
According to Lemma 1, put the results (63) and (87) together, we have
J(πθ)
(63尸:X d∏θ(sMK) + “。- : d∏θ +1⅜ P 初 ∏θ, φ
-γ s∈S	- γ	-γ
=Es0~P0(∙)Kso)] +[ X d∏θ (S)(Rny (s) + 7 X Pny(SISMS0)-奴 S)) , (91)
γ s∈S	s0∈S
where the last equation holds since we unfold (87) according to (88)-(90).
Step 2: Rewrite the term (Rnθ)(s) + Y Pss>∈s Pn))(S' |s)夕(s')-夕(S)) in Eq.(91).
Then, we unfold the second term of (91) as follows,
RnT(S) + 7 X P∏λ)(S0∣SW(Sj- 9(S)	(92)
s0∈S
∞∞
(53)=,(55) X(γλPnθ)trnθ [S] + γ7(1 - γλ) X X(γλ)t Ptn+θ1[S,S0] 9(S0) - 9(S)
t=。	s0∈S t=。
∞∞
(=) X(γλPnθ)trnθ [S] + γ(1- λ) XX(γλ)tPnθ(St+1 = S |S)9(S ) - 9(S).	(93)
t=。	s0∈S t=。
Recall the terms P(nλθ), r(nλθ) [S] defined in (50)-(55),
Rny(S) + Y(1 - λ) X Pny(S0∣s)9(s0)- 9(S)	(94)
s0∈S
We consider the first term R(nλθ)(S) of (92) as follows,
∞∞
Rny(S) (50)=(55) - = X(γλ)tpnβ rnβ [s] = XX (γλ)tPnθ ⑶心网。(St).	(95)
t=。	t=。 st∈S
29
Under review as a conference paper at ICLR 2022
We consider the second term Y Ps∈s P∏θ)(s0∣s)夕(S)- 夕(S) of (92) as follows,
Y £ p∏λI)(S0|S)P(Sj- P(S)
s0∈S
∞
(=)Y(I-Yλ) X X(Yλ)tP∏θ(St+1 = SlS)P(Sj-P(S)
s0∈S t=0
(5=0)Y(1 - λ) XX(Yλ)tPπθ(St+1 = S0|S)P(S0) - P(S)
s0∈S t=0
∞∞
=Y XX(Yλ)tPπθ(St+1 = S |S)P(S ) - X X(Yλ)t+1Pπθ(St+1
s0 ∈S t=0	s ∈S、V=0
{Z
(96)
(97)
s1sW(SO)J -(P(S)
'∙^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^
=P∞=1 (Yλ)tp∏θ (St = Sls)w(s0)
∞∞
=Y XX(Yλ)tP∏θ(St+1 = SlS)P(Sj - ( XX(Yλ)tP∏θ (St = S'|S)P(S j + P(S)
S0∈S t=0	S0∈S t=1
X-------------------------:
}
{z
= Ps0∈S P∞=o(γλ)tP∏θ (st = s0∣s)Rs0)
(98)
∞∞
=Y X X(Yλ)tP∏θ(St+1 = SlS)P(Sj- X X(Yλ)tP∏θ(StIS)P(S),
S0∈S t=0	St∈S t=0
(99)
where the equation from Eq.(98) to Eq.(99) holds since: according to (41), we use the following
identity
Pπθ (S0 = S0 |S)P(S0) = P(S).
S0∈S
30
Under review as a conference paper at ICLR 2022
Furthermore, take the result (95) and (99) to (94), we have
R∏λθ)(s)+ Y X P∏T(s0∣sMsj-以S)
s0∈S
∞
X(Yλ)t( X P∏θ (st IS)Rne (st) + Y X	P∏θ (St+1 = SlSW(Sj
t=0	Is©	S0 ∈S (42)_ '	'一~.................
=Pst∈S P∏θ (St+1=s Ist)P∏θ (St Is)W(S )
-X p∏θ (StIsW(St))
St∈S
∞
X(γλ)t I X P∏θ (St∣S)R∏θ (St) + Y X P∏θ (St∣S) X P∏θ (St+1∣StW(St+1)
t=0	St∈S	St∈S	St+1∈S
-X p∏θ(StIsW(St))
St∈S
/
∞
X(Yλ)t X Pπθ(StIS)
t=0	St∈S
πθ(atISt)	P(St+1ISt, at)r(St+1ISt, at)
at ∈A	St+ι∈S
、------------------------{z---
=Rπθ (St)
(100)
(101)
∖
+ YE ∏θ(at∣St) E P(st+ι∣St,at) 夕(st+ι)-夕(St)
at∈A
St+1 ∈S
{^^^^^^™
=Pπθ (St+1 ISt)
/
∞
(Yλ)t	Pπθ(StIS)	πθ(atISt)	P(st+ι∣st,at) (r(st+ι∣st,at) + Y2(st+ι)-夕(St))
t=0	St∈S	at∈A	St+1 ∈S
(102)
∞
E(Yλ)tESt~P∏θ (∙∣S),at~∏θ(∙∣St),St+ι~P(∙∣St,at) [r(st+ιlst, at) + Y-(st+」) - θ(st)],	(103)
t=0
the equation from Eq.(99) to Eq.(100) holds since:
Pπθ (St+1 IS) (4=2) X Pπθ (St+1 ISt)Pπθ (St IS);
St∈S
the equation from Eq.(100) to Eq.(101) holds since we use the Markov property of the definition of
MDP: for each time t ∈ N,
Pπθ (St+1 = S0 ISt = S) = Pπθ (S0 IS);
the equation (102) the following identity:
πθ(atISt) = 1,	P(St+1 ISt, at) = 1,
at∈A	St+1 ∈S
then
-(St) =	πθ(atISt)	P(St+1 ISt, a)-(St).
at∈A	St+1 ∈S
Step 3: Put all the result together.
31
Under review as a conference paper at ICLR 2022
Finally, let
δt = r(st+i|st,at) + YiXst+1) - φ(st),
δ∏θ ,t(S) = Est~P∏θ (∙∣s),at~∏θ (Tst),st+ι~P(∙∣st,at) [δt ] ,
combining the results (91) and (103), we have
J (πθ >Es0~ρo(∙)[P(SO)] +
占Xd∏θ(s) (Xγtλt篇,t(S)
- γ s∈S	t=0
(104)
1∞
=Es0~ρo(∙)3(So)] +1-ɪEs~dλθ(.) EYtλtc,,t(S)
1- γ	t=0
This concludes the proof of Proposition 4.	口
- dλπθ k1 ≤
- dπλθ k1 ≤
(1- γλ)2
(1- γ) 1- γλkΠπθ0
(1 - γλ)2
(1 - γ) (1-γλk∏∏θ0
-----------γEs~dλ (∙) [2dtv(πθ0,πθ)[s]]
- Ππθ k1,1-πθ
-----------γEs~d∏ 0(∙) [2DTV(πθ0 ,πθ)[s]],
- Ππθ k1,1-θ
E.3 Lemma 2
Lemma 2. Let kΠπ 0 - Ππθ k1,1 denote as the L1,1-norm for the difference between two policy
space {∏θ(a∣S)}(s,α)∈s×A, {∏θ0 (a∣S)}(s,α)∈s×A, i.e.,
kπ∏θθ - π∏θki,1 =： XX lπθ0 (a|S) - πθ (a|S)| .	(105)
s∈S a∈A
The divergence between discounted future state visitation distributions, kdλπ 0 - dλπ k1 , is bounded
as follows,
θ
and
θ0
where
2Dtv(∏θo,∏θ)[s] =: E ∣∏θ0(a∣S) - ∏θ(。国| .
a∈A
Furthermore, we achieve the boundedness of kdπλ 0 - dλπ k1 as follows,
kdπθ0 - dπθk1 ≤ 1-5 • |1 - 2γλYs∣∣A∣∣Es~dλθO h2DTV(ne0,πθ)同],
kdλθ0 - dʌθk1 ≤ T-5 • |1 - 2γλYs∣∣A∣∣Es~dλθo G) h2DTV(ne0,ne)[s]] ∙
Proof. Recall Eq.(59), let
Gπθ	=	I - γ5P(πλθ)-1,	Gπθ0	= I -	γ5P(πλθ)0	-1, D =	P(πλθ)0	-P(πλθ).	(106)
Then, the following holds
Gπ-θ1 - Gπ-10 = I-γ5P(πλθ) - I - γ5P(πλ)0 = γ5D.	(107)
Furthermore, by left-multiplying by Gπθ and right-multiplying by Gπ 0 , we achieve
Gπ 0 - Gπθ = γ5Gπ 0DGπθ.	(108)
32
Under review as a conference paper at ICLR 2022
Grouping all the results from (106)-(108), recall (59),
_ /	, ,x t	/	. .x -1
d∏θ = (1 - Y) X (YPny) P0 = (1 - Y) (I-俚祟)P0 = (1 - Y)G∏θPo,
t=0
then we have
d" - % =(1 - Y) (g% - G∏θ) ρo
(=8)(1 - Y)YGπθ, DGπβρo
(109)~C	fλ
=YGnjDdne ∙
Applying (110), we have
(110)
kd∏β, - d∏θ∣∣ι ≤ YkGnθJι∣∣Ddλθkι.
Firstly, we bound the term IlGna01∣ 1 as follows,
IIg∏θ∕ kl
1 -祖2)T
ɔɔ -1 -1 ∖
≤	PM =，= 1 -得
ι ^ t=0	GI 1 - Y 1 - Y
Now, we analyze the second term as follows,
∣Dd∏β kι
E ED(SlS)d∏θ(S)
s0 ∈S s∈S
曾 X X(Pna0Is)-Pny(SM 说⑶
s' ∈S s∈S
∞
(=) X (I-Yλ) X(Yλ)t X (pnθ, (St+1 = SiS)- pnβ (st+1 = SiS)) dλe (S)
s' ∈S	t=0	s∈S
≤ X 卜1 - Yλ) X(Yλ)t X 归nθ0 (St+1 = SlS)- Pnβ (St+1 = s'|s) J ) dλ0(s)
s∈S ∖	t=0	s' ∈S	)
(109)
(110)
(111)
(112)
(113)
Before we provide a further analyze (113), we need to bound IPne 0 (s⅛+1 = s'|s) - Pne (s⅛+1 = s'∣s)∣.
Let S0 = S, then
Pne (st+1 = s' ∣s) = X Pne (s⅛+1 = s' ∣S1)P∏θ (s1∣S0)
sι∈S
=XXPne (st+1 = s' ∣S2)Pne (s2∣S1)Pne (S1∣S0)
sι∈S s2∈S
XX ∙∙∙ X (YPne (SiISi-I))
sι∈S s2∈S St∈S ∖i=1	/
XX ∙∙∙ X (∏(XP(SiISi-1,αi)∏θ (αi∣Si-1)))
sι∈S S2∈S st∈S ∖i=1 ∖aa∈A	JJ
Similarly, we have
Pne0 (st+1 = s' |s) = X X ∙∙∙ X (Y(X P(SiISi-1,αi)πθ'(αilsi-I)))
(114)
(115)
33
Under review as a conference paper at ICLR 2022
Then, according to the results (114)-(115), let s0 = s, the following holds
E IPπβ∕ (st+1 = SlS) — P∏θ (S⅛+1 = SIS)I
s0 ∈S
Σ
s0 ∈S
XX ∙∙∙X (∏ (XP(SiISi-ι,αi)(∏θ0(αi∣Si-i) 一
sɪ ∈S s2∈S	St∈S ∖i=1 ∖α⅛∈A
∏θ (ai∣Si-i))
))
∕t+1
≤XX…X ∏X∣∏θ0(αi∣Si-i) — ∏θ (ai∣Si-i)∣
sɪ∈S s2∈S st∈S ∖i=1 ai∈A
∑∑∙∙∙∑ π∑∣πθ0 (αi∣Si-i) — πθ(αi∣Si-i)∣ • Σ ∣πθ0 (a1∣S0) — πθ(a1∣S0)∣
sɪ∈S s2∈S	st∈S ∖i=2 ai∈A	a ∖αι∈A
∏( ∑ ∑ ∣∏θ0(ai∣Si-i) — πθ (ai∣Si-i )∣ I • ( E ∣πθθ (a1∣S0) — πθ (a1∣S0)∣
i=2 ∖sa-ι∈S ai∈A	a	∖aι∈A
/
∖tt
E E ∣πθ0 (a∣s)—
s∈S a∈A
X------------------
∏θ (a∣S)∣
∣πθ0 (a∣S) — ∏θ(a∣S)∣	.
(116)
∖	=1π%0-π"JL,i	)
,
Taking the result (116) to (113), we have
一 ∏πθll1 XX ∣πθ0 (α∣s) —πθ (α∣s)∣ d∏θ(S)
，s∈S a∈A
χ--------------------'
∞
=(1— γλ) X(γλ)tk∏∏θ,
t=0
1 — γλ
1 — γλk∏∏θ0 一 ∏πθ 111,1
、	—
= 2Dtv(∏θ/ ,∏θ)[s]
—∏M∣iιEs〜/(∙)[2Dtv(∏θ,,∏θ)[s]]
ES〜必@ (∙) [2DTv(πθ0 ,πθ)[s]].
(117)
J
Finally, according to (111), (112) and (117), we have
- dπβ kι ≤ τ‰ • 1 — γλ∣∣∏	—∏ k1 IES~d"G) [2DTV(πθ0,πθ)[s]].	(118)
θ
□
Recall (105), we have
ll∏πθ∕ — ∏∏θII1,1 = X X ∣πθ0(OIS) ―πθS∣s)∣ ≤ 2∣s∣∣a∣.	(119)
s∈S a∈A
Then, we achieve the boundedness of k∏πβ0 — ∏-∏θ k1,1 as follows,
kd" - d"∣1 ≤ 1一ɪ • ∣1 — 27λ∣S∣∣A∣∣ES~dλθG) [2DTVSθ0,πθ)[s]].	(120)
34
Under review as a conference paper at ICLR 2022
F Proof of Theorem 2
Theorem 2 Let δk = Es〜dλ (,)
[KL 卜θk ,πθk+l) [s]], ifπθk and πθk+1
are related to (17)-(18),
then the lower bound on policy improvement, and upper bound on constraint violation are
J (πθk+1) - J(πθk) ≥ -
Y(I- λ)αk√2δkeVθ (πθ0)
(1- Y) |1 — 2γλ∣S∣∣A∣∣
Jc(πθk+1) ≤ b+
Y(I- λ)βk√2δkeCθ (πθ0)
(1- Y) |1- 2γλ∣S∣∣A∣∣ .
Proof. (of Theorem 2)
According to Bregman divergence, if policy πθk is feasible, policy πθk+1 is generated according to
(18), then the following
KL (πθk, πθk+1) ≥ KL (πθk, πθk+1 ) + KL (πθk+ι, πθk+1)
implies
δk = E3^dλβj∙) [KL (πθk,πθk+ι) [s]] ≥ Es〜d∏°fc(∙) [KL (πθk+ι,πθk) [s]].
According to the asymptotically symmetry of KL divergence if we update the policy within a local
region, then, we have
δk ≥ Es〜d∏θj∙) [KL 卜θk+ι,πθk) [s]] ≥ Es〜d∏θj∙) [KL (πθk+1, πθk) [s] .
Furthermore, according to Proposition 1 and Proposition 3, we have
J(πθk+1) - J(πθk)
1	2Y(1 - λ)Vπθ	(πθk)
≥ T-^ Es^dλθk (∙),a-πθk+ι as) Aπθk " (S'a) - (1 - Yλ) |1 - 2；；|S||A||DTV(πθk ,πθk+ι)[s]
1	Γ 2Y(1 - λ)αkeVθfc+1(πθk) R	"	^-
≥Γ-5Es~%k(')，a~n"+1 as) -(1 - Yλ) |1 - 2Yλ∣s∣A∣∣ V2Es~%%G) [KL(πθk,πθk+ι)[sU
1	Γ Y(I- λ)αk √2δk eVθ	(πθk)] θk+1
≥ 1 - Y s〜d∏θJ∙),a〜πθk+ι(.1s)[	(1 - Yλ) |1 - 2Yλ∣S∣∣A∣∣
Similarly, according to Proposition 1 and Proposition 2, and since policy πθk+1 satisfies
J c(πθk ) + 1 - γ Es 〜dλθ^ (∙),a 〜∏θk+ι (∙∣s) hA∏θk ,C，)(s, a)i + βk qEs^d∏θfc (∙) [KL(πθk ,πθk+ι )[s]] ≤ b，
(121)
and
Jc(πθk+1) - Jc(πθk)
(122)
≤ 1 - Y Es~d∏θk G),a~πθk+ι (Ts)
AπGθAkE,(Cγ,λ)(s,a)+
2γ(1 - λ)βkeCθk+ι (πθk )
(1- γλ) |1- 2γλ∣S∣∣A∣∣
DTV(πθk, πθk+1)[s]
Combining (121)- (123), we have
Jc(πθk+1) - Jc(πθk)
≤b + 1 - γEs〜dλθk G)，a〜πθk+ι (Ts)
≤b + 1 - γEs〜dMk G),a〜πθk+ι (Ts)
(123)
2Y(I - λ)βk€Cθk+ι (πθk) π	"一
(1 - γλ) |1 - 2γλ∣S∣∣A∣∣ V 2Es~%k。∣KL(πθk, πθk+1 )刈
Y(I- λ)βk√2δkeC,	(πθk)
k+1
(1 - γλ) |1 - 2γλ∣S∣∣A∣∣
(124)
□
35
Under review as a conference paper at ICLR 2022
G Experiments
The Python code for our implementation of CUP is provided along with this submission in the
supplementary material.
G.1 Environment
G.1.1 Environment 1: Robots with Speed Limit.
We consider two tasks from MuJoCo (Brockman et al., 2016): Walker2d-v3 and Hopper-v3, where
the setting of cost follows (Zhang et al., 2020). For agents move on a two-dimensional plane, the cost
is calculated as follows, 
C(S, a) = q∕v2 + V ；
for agents move along a straight line, the cost is calculated as
C(s, a) = |vx|,
where vx, vy are the velocities of the agent in the x and y directions respectively.
G.1.2 Environment 2: Circle.
The Circle Environment follows (Achiam et al., 2017), and we use open-source implementation
of the circle environments from https://github.com/ymzhang01/mujoco-circle. Ac-
cording to Zhang et al. (2020), those experiments were implemented in OpenAI Gym (Brockman
et al., 2016) while the circle tasks in Achiam et al. (2017) were implemented in rllab (Duan et al.,
2016). We also excluded the Point agent from the original experiments since it is not a valid agent
in OpenAI Gym. The first two dimensions in the state space are the (x, y) coordinates of the center
mass of the agent, hence the state space for both agents has two extra dimensions compared to the
standard Ant-v0 and Humanoid-v0 environments from OpenAI Gym.
Now, we present some necessary details of this environment taken from (Zhang et al., 2020).
Figure 2: In the Circle task, reward is maximized by moving along the green circle. The agent is not
allowed to enter the blue regions, so its optimal constrained path follows the line segments AD and
BC (figure and caption taken from (Achiam et al., 2017; Zhang et al., 2020)).
In the circle tasks, the goal is for an agent to move along the circumference of a circle while remaining
within a safety region smaller than the radius of the circle. The exact geometry of the task is shown
in Figure 2. The reward and cost functions are defined as:
R(s)
-yvx + XVy 1
1 + | ∖/ x2 + y2 — r|
C(s) = I(|x| > xlim),
where x, y are the positions of the agent on the plane, Vx , Vy are the velocities of the agent along
the x and y directions, r is the radius of the circle, and xlim specifies the range of the safety region.
The radius is set to r = 10 for both Ant and Humanoid while xlim is set to 3 and 2.5 for Ant and
Humanoid respectively. Note that these settings are identical to those of the circle task in Achiam
et al. (2017); Zhang et al. (2020).
36
Under review as a conference paper at ICLR 2022
G.1.3 Environment 3: Safety Gym Ships with Three Pre-made Rob ots.
In Safety Gym environments, the agent perceives the world through a robot’s sensors and interacts
with the world through its actuators. In our paper, we consider three environment: Point, Car, Dog
from (Ray et al., 2019). In this section, the presentation of those environments are taken from (Ray
et al., 2019), for more details, please refer to (Ray et al., 2019, Page 8-10).
(a) Point	(b) Goal	(c) Car (d) Button (e) Doggo (f) Push
Figure 3: Fig (a), (c), (e) show the pre-made robots in Safety Gym. These robots are used in the
benchmark environments. Fig (b), (d), (f) show the tasks for our environments. From left to right:
Goal, Button, Push. In “Goal,” the objective is to move the robot inside the green goal area. In
“Button,” the objective is to press the highlighted button (visually indicated with a faint gray cylinder).
In “Push,” the objective is to push the yellow box inside of the green goal area (figure and caption
taken from (Ray et al., 2019)).
Point: (Figure 3 (a)). A simple robot constrained to the 2D-plane, with one actuator for turning and
another for moving forward/backwards. This factored control scheme makes the robot particularly
easy to control for navigation. Point has a small square in front that makes it both easier to visually
determine the robot’s direction, and helps the point push a box element that appears in one of our
tasks.
Car: (Figure 3 (c)). Car is a slightly more complex robot that has two independently-driven parallel
wheels and a free rolling rear wheel. Car is not fixed to the 2D-plane, but mostly resides in it. For
this robot, both turning and moving forward/backward require coordinating both of the actuators. It
is similar in design to simple robots used in education.
Doggo: (Figure 3 (e)). Doggo is a quadrupedal robot with bilateral symmetry. Each of the four
legs has two controls at the hip, for azimuth and elevation relative to the torso, and one in the knee,
controlling angle. It is designed such that a uniform random policy should keep the robot from falling
over and generate some travel.
All actions for all robots are continuous, and linearly scaled to [-1, +1], which is common for 3D
robot-based RL environments and (anecdotally) improves learning with neural nets. Modulo scaling,
the action parameterization is based on a mix of hand-tuning and MuJoCo actuator defaults, and
we caution that it is not clear if these choices are optimal. Some safe exploration techniques are
action-layer interventions, like projecting to the closest predicted safe action (Dalal et al., 2018), and
these methods can be sensitive to action parameterization. As a result, action parameterization may
merit more careful consideration than is usually given. Future work on action space design might
be to find action parameterizations that respect physical measures we care about—for example, an
action space where a fixed distance corresponds to a fixed amount of energy.
The Safety Gym environment-builder currently supports three main tasks: Goal, Button, and Push
(depicted in Fig. 2). Tasks in Safety Gym are mutually exclusive, and an individual environment can
only make use of a single task. Reward functions are configurable, allowing rewards to be either
sparse (rewards only obtained on task completion) or dense (rewards have helpful, hand-crafted
shaping terms). Task details follow:
Goal: (Figure 3 (b)). Move the robot to a series of goal positions. When a goal is achieved, the goal
location is randomly reset to someplace new, while keeping the rest of the layout the same. The
sparse reward component is attained on achieving a goal position (robot enters the goal circle). The
dense reward component gives a bonus for moving towards the goal.
Button: (Figure 3 (d)). Press a series of goal buttons. Several immobile “buttons” are scattered
throughout the environment, and the agent should navigate to and press (contact) the currently-
37
Under review as a conference paper at ICLR 2022
highlighted button, which is the goal button. After the agent presses the correct button, the environ-
ment will select and highlight a new goal button, keeping everything else fixed. The sparse reward
component is attained on pressing the current goal button. The dense reward component gives a
bonus for moving towards the current goal button.
Push: (Figure 3 (f)). Move a box to a series of goal positions. Like the goal task, a new goal location
is drawn each time a goal is achieved. The sparse reward component is attained when the yellow box
enters the goal circle. The dense reward component consists of two parts: one for getting the agent
closer to the box, and another for getting the box closer to the goal.
Constraint Options and Desiderata
(a) Hazards (b) Vases (c) Buttons (d) Pillars (e) Gremlins
Figure 4: Constraint elements used in our environments (figure and caption taken from (Ray et al.,
2019)).
The constraint elements themselves are:
Hazards: (Figure 4 (a)). Dangerous areas to avoid. These are circles on the ground that are
non-physical, and the agent is penalized for entering them.
Vases: (Figure 4 (b)). Objects to avoid. These are small blocks that represent fragile objects. The
agent is penalized for touching or moving them.
Pillars: (Figure 4 (c)). Immobile obstacles. These are rigid barriers in the environment, which the
agent should not touch.
Buttons: (Figure 4 (d)). Incorrect goals. When using the “buttons” goal, pressing an incorrect button
is penalized.
Gremlins: (Figure 4 (e)). Moving objects. These are simple moving objects that the agent must
avoid contacting. Since they are moving quickly, the agent must stay out of the way of their path of
travel.
Although all constraint elements represent things for the agent to avoid, they pose different challenges
for the agent by virtue of having different dynamics. To illustrate the contrast: hazards provide
no physical obstacle, vases are moveable obstacles, pillars are immovable obstacles, buttons can
sometimes be perceived as goals, and gremlins are actively-moving obstacles. Like reward functions
in Safety Gym, cost functions are configurable in various ways; see the code for details. By default,
cost functions are simple indicators for whether an unsafe interaction has occured (ct = 1 if the agent
has done the unsafe thing, otherwise ct = 0).
Finally, SGPoint, SGCar, and SGDoggo, which are all six Point/Car/Doggo robot environments
with constraints in Safety Gym, and Ray et al. (2019) have provided an implementation for those
environments.
G.2 Details of Experiments
In all of those experiments, we use a two-layer feedforward neural network with a tanh activation for
both policy and value networks. Experiment-specific parameters are as follows:
38
Under review as a conference paper at ICLR 2022
Parameter	Walker2d	Hopper	HumanoidCircle	AntCircle
No. of hidden layers	2	2	2	2
No. of hidden nodes	64	64	64	64
Batch size	2048	2048	50000	50000
Minibatch size	64	64	1000	1000
Rollout length	1000	1000	1000	1000
GAE parameter (cost)	0.95	0.95	0.995	0.995
GAE parameter (reward)	0.95	0.95	0.995	0.995
discounter for cost	0.99	0.99	0.995	0.995
discounter for reward	0.99	0.99	0.995	0.995
learning rate for policy	3 × 10-4	3 × 10-4	3 × 10-4	3 × 10-4
learning rate for value and reward function	3 × 10-4	3 × 10-4	3 × 10-4	3 × 10-4
39