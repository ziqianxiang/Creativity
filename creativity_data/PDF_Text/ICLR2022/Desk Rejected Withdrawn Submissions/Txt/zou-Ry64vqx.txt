Under review as a conference paper at ICLR 2022
FedMorph: Communication Efficient Feder-
ated Learning via Morphing Neural Network
Anonymous authors
Paper under double-blind review
Ab stract
The two fundamental bottlenecks in Federated Learning (FL) are com-
munication and computation on heterogeneous edge networks, restrict-
ing both model capacity and user participation. To address these is-
sues, we present FedMorph, an approach to automatically morph the
global neural network to a sub-network to reduce both the communi-
cation and local computation overloads. FedMorph distills a fresh sub-
network from the original one at the beginning of each communication
round while keeps its ‘knowledge’ as similar as the aggregated model
from local clients in a federated average (FedAvg) like way. The net-
work morphing process considers the constraints, e.g., model size or
computation flops, as an extra regularizer to the objective function. To
make the objective function solvable, we relax the model with the con-
cept of soft-mask. We empirically show that FedMorph, without any
other tricks, reduces communication and computation overloads and in-
creases the generalization accuracy. E.g., it provides an 85× reduction
in server-to-client communication and 18× reduction in local device
computation on the MNIST dataset with ResNet8 as the training net-
work. With benchmark compression approaches, e.g., TopK sparsifi-
cation, FedMorph collectively provides an 847× reduction in upload
communication.
1	Introduction
Federated Learning (FL) (Li et al., 2021; Bonawitz et al., 2019; Kairouz et al., 2019; Li et al.,
2019) ensures data privacy by decoupling the training on the dataset in local clients and model
aggregation in the global server. The iterative process of the local updates optimized on each client
and aggregating the updates by the global server assures the convergence of the global model and
the excellent generalization on test datasets.
(A)	. Despite the benefits gained by the paradigm of local-train and global-aggregation of FL, the
server-clients communication burden and limited local computing resources suspend the deployment
of the State-of-The-Art (SOTA) large neural networks on a wide scale. Recently the lossy model
compression techniques, such as gradient sparsification (Wangni et al., 2017; Zhou et al., 2021) and
quantization (Courbariaux et al., 2016; Hubara et al., 2017), have been studied to relieve the com-
munication issues. Practical as they announced, but the limitations are also apparent. The gradient
sparsification technique reduces the communication burden by selectively updating local gradients.
Hence, they only work for client-to-server communication and have no compression benefits in the
downstream direction. Meanwhile, the quantization technique compresses the communication and
computation by changing the floating operations into low precision operations, and it works in both
directions. However, its compression ability is at most 32× by reducing 32-bits floating operations
to the 1-bit operations, not to mention the performance degradation.
Absorbing the benefits from the former lossy compression techniques, FedDropout (Caldas et al.,
2018) is another trial to reduce the downstream direction communication. It builds upon the basic
idea of dropout (Srivastava et al., 2014) by randomly dropping some neurons of the global model at
each round, resulting in a smaller sub-network with smaller weight matrices before broadcasting it
to local clients. The updated sub-networks by local clients will map back to the global network.
1
Under review as a conference paper at ICLR 2022
Distillation of the ‘knowledge’ from an extensive neural network (Hinton et al., 2015) is another
branch to solve local devices’ communication and computation burdens. The main idea is to train
a small model to keep the output similar to the large one on a sizeable prepared dataset. It has
significant performance on the recent developments (Sanh et al., 2019) (Jiao et al., 2019). However,
it requires a well pre-trained large model and a vast dataset for training the smaller network, which
is not always satisfactory in the context of FL.
(B)	. Besides considering communication and computation budget, broadcasting a raw neural net-
work may also cause a severe generalization problem. While it is almost common sense in the
deep learning community that training a well-designed deep neural network performs better than
a shallow one, the statement only establishes when the training dataset is sufficient to avoid over-
fitting (Pitt & Myung, 2002; Hinton et al., 2012). In FL settings, each local client holds a dataset
in a small ratio of the whole, which is universal when the number of clients is enormous. Hence,
human labor’s well-designed deep neural network works well on the whole dataset, nevertheless
easily overfit the local datasets. The overfitting will impede the number of local iterations, which is
a crucial hyper-parameter in FL settings (McMahan et al., 2017), resulting in a deteriorated model
performance. Kairouz et al. (2019) has a similar statement in their Network Architecture Search
(NAS) section that a well-designed network architecture in centralized settings may not work well
in federated learning settings.
This paper tries to solve the earlier issues by decoupling the network architectures on local clients
and the global server. In detail, the global server maintains a large neural network suitable in a
centralized setting, and keeps the architecture untouched throughout the entire process. During each
communication round, the global server morphs a new sub-network from the maintained architecture
and broadcasts it to selected clients for local optimization. The maintained network optimizes its
weights at the morphing process by learning from the average aggregated network updated by local
clients. Meanwhile, a newly morphed sub-network is constrained by keeping the knowledge similar
to the average aggregated one while minimizing the number of its parameters. By morphing the
shared neural network into a smaller size among all communication rounds, we (1) decrease the
communication in both upload and download directions; (2) relieve the computation overload of
local clients; (3) reduce the generalization error caused by overfitting on local datasets with a large
network.
2	Related Works
Neural Architecture Search Our work is closely related to the NAS (Zoph & Le, 2016), whose
purpose is automatically designing neural network architecture such as the number of layers and
the number of neurons or filters in each layer. The search strategies include random search, evolu-
tionary methods, Bayesian optimization, and gradient-based methods (Elsken et al., 2019). Since
NAS involves a vast search space, recent works focus on improving the speed and efficiency by
attentive sampling (Wang et al., 2021), untrained scheme (Mellor et al., 2021), and block-wisely
search with knowledge distillation (Li et al., 2020). Besides, the network architecture search under
the scenario of federated learning scenario was also explored in (Zhu et al., 2021), aiming to reduce
both computation and communication.
Model Compression To reduce the complexity of deep neural networks, model compression was
first proposed in (BUcilUa et al., 2006), followed by tremendous attention in both academia and
industry. One of the most straightforward method to reduce model size is parameter pruning and
sharing by removing redUndant parameters that are not critical to model performance (Han et al.,
2015; Blakeney et al., 2020). Similarly, the informative parameters can also be measUred and se-
lected by low-rank factorization (Sainath et al., 2013; Denton et al., 2014). To simUltaneoUsly redUce
the compUtation and storage of a deep model, approaches based on transferred or compact convolU-
tional filters were fUrther proposed by designing special strUctUral kernels (Cohen & Welling, 2016;
WU et al., 2016) and achieved benefits in domains with hUman prior. Besides, some other works are
focUsing on transferring the learned knowledge of a large teacher network to a small and lightweight
stUdent network, which yields the concept of knowledge distillation (Hinton et al., 2015; Mirzadeh
et al., 2020; Chen et al., 2021; Gao et al., 2021), which is sUitable for small- or mediUm-size data
sets (Cheng et al., 2018).
2
Under review as a conference paper at ICLR 2022
3	Problem Definition
3.1	Preliminary
In this work, we consider the following federated learning optimization problem:
mwin F(w) , X pkFk(w) ,
(1)
where K is the number of clients, and pk is the weight of the k-th client such that pk ≥ 0 and
PK=ιPk = 1. Suppose the k-th client holds the n training data: Xk,1,Xk,2, ∙∙∙ ,Xk,nk. The local
objective Fk(∙) is defined by
nk
Fk(W), — X'(W； xk,j),
nk j=1
(2)
where '(•;•) is a user-specified loss function. Specifically We consider a C class classification Prob-
lem defined over a compact space X and a label space Y = [C], where [C] = {1, . . . , C}. The data
point {x, y} is a random sample over X × Y. A function fθ : X → S maps x to the probability sim-
plex S, where S = z | PcC=1 zc
1, zc ≥ 0, ∀c ∈ [C] , and z is a C-dimensional column vector.
fθ is parameterized over the hypothesis class w, i.e., the weight of the neural network θ. We define
the loss function `(w, x) with the widely used cross-entropy loss as `(w, x) = Ey [log fθ(w, x)] .
3.2	Algorithm Description
Definition 1. Morphing Set: Given a neural network θ with its parameter weight W, its Morphing
Set (Θ, W) is defined as a set that contains every pair of sub-network and the corresponding weight
of the original network θ.
We define the concept of Morphing Set for the convenience of explanation. Before the description
of the proposed algorithm, we define θo and Wo as the server maintained neural network and its
parameter weights, and denote (Θo, Wo) as the Morphing Set of θo.
Here, we describe one round (say the t-th) of our proposed algorithm. First, the central server
broadcasts the latest model, (θt, Wt), to the selected clients (say the K).
Second, every client (say the k-th) begins with Wtk,e=0 = Wt, and then performs E ≥ 1 local updates
with a randomly selected batch samples ξtk,e as follows,
wk,e+1 - wk,e - ηt,eVFk (Wk,e, ξke)	⑶
Third, unlike Federated Average (FedAvg) (McMahan et al., 2017) or other conventional algorithms
k
letting Wt+1 = P q⅛f be the model weights for next round, our method, in order to compress the
k∈K
communication, morphs a small neural network from θo by minimizing the performance divergence
of the morphed model wt+1 with Wt+1 as follows,
(Θt+1, Wt+1) =	arg min	Lt(W； Wt+1)
(θ,w)∈(Θo,Wo)
= argmin X JS (fθ(W,x)llfθt(Wt+ι,x)),	(4)
(θ,w)∈(Θo,Wo) x∈Xv
k
where Wt+1 = P Kr follows the FedAvg algorithm to be the average aggregated value of local
k∈K
clients updated parameter weights. (θ, W) ∈ (Θo, Wo) denotes the newly morphed network and
its weights, which is a sub-network of the server maintained one. fθ is the corresponding map-
ping function defined on the morphed neural network θ. The Jensen-Shannon (JS) divergence is a
well-known metric for symmetrically measuring the distance of two probabilities. We apply the JS
divergence as the loss function to minimize the output performance of the morphed sub-network
3
Under review as a conference paper at ICLR 2022
Figure 1: FedMorph procedures applied on three fully connected layers. The server maintained
network weights and masks are backward updated according to the knowledge distilled from t round
aggregated network. The network for the (t + 1) round is morphed from the server maintained
network by the soft masks. The intra-layers are constrained by the masks to keep the layer coherent
with the activation dimensions. The color of yellow and green means the corresponding values are
masked and unmasked.
and average aggregated network on the validation compact space. Here, we use the finite-sum on
the validation dataset Xv (without label information) to evaluate the performance divergence.
Finally, the newly morphed sub-network with the updated weights (θt+1 , wt+1) are broadcasted to
selected clients for next round local optimization.
The optimization objective is to maintain as much knowledge as possible from the aggregated model
in previous round, while keeping the morphed network as small as possible. Therefore, we consider
the network architecture as a regularizer and reformat equation (4) as follows,
(Θt+1, Wt+1 )=	arg min	{Lt(w; Wt+ι) + λLc(θ)} ,	(5)
(θ,w)∈(Θo,Wo)
where Lc(θ) is a regularizer to measure the constraints on the number of neural network parameters
(PARAMs) or the computation floating-point operations per second (FLOPs) or any other constraints
as required. λ is an adjustable non-trained variable to balance the two losses.
4	Optimization Solution and Relaxation
4.1	Soft Masks
The optimization process of the equation (5) is incompatible with the sophisticated deep learning
frameworks such as Pytorch (Paszke et al., 2019) or TensorFlow (Abadi et al., 2016) for the reasons
that 1) the network architecture is not a well-defined parameter for optimization; 2) the categorical
loss of the regularizer is incompatible with the neural network optimizer on differentiable functions.
In order to solve the issues mentioned above, we apply the soft-mask trick (Jang et al., 2016; Chaud-
huri et al., 2020), a popular method to relax the categorical variables into continuous. Specifically,
we append a soft-mask layer after each layer in the original neural network, which requires to be
constrained. The soft-mask layer is a parameter vector whose dimension equals the number of con-
strained values of its previous layer. For instance, the dimension of the soft-mask layer would be the
4
Under review as a conference paper at ICLR 2022
Algorithm 1 Communication efficient federated morphing neural network.
1:	for t in {0,…，T 一 1} do
2:	Broadcast the masked results to selected clients;
3:	Local Update wtk on the selected clients according to (3);
4:	Distill knowledge from the aggregated model Wt+ι to (W(O+、, vt+ι) according to (7);
5:	Morph (θo, wto+1) with vt+1 to (θt+1, wt+1) for next step iteration;
6:	end for
7:	return (θo, WTo );
number of the output channels if the previous layer is a conv2d layer; Or it would be the number of
neurons if the previous layer is a linear layer. The activation outputs of the previous layer passing
through the soft-mask layer have the same effect of masking directly on the previous layer.
For each variable in the soft-mask layer (parameter vector), we apply the differentiable S-shape-like
functions f : R → (0, 1) to relax the categorical representations into differentiable variables. The
relaxation function is as follows,
m = S igmoid
⅛) +1)/T).
(6)
As the temperature τ → 0, m approaches {0, 1} with probability {1 一 π, π}, respectively.
l 〜logistic is randomized to increases the expressiveness of the masked channel or neuron. SPecifi-
cally, it leaks information with a possibility even when the channel or neuron is decided to be masked
by ∏. The optimizer operates on V = log( ι;π∏) instead of ∏, as m's learnable parameters for nu-
merical stability. We naively choose the standard logistic function, aka sigmoid function, for our
differentiable relaxation. The comparisons among different kinds of S-shape-like functions is not
the main object, and we leave it for the future research.
With the help of the soft-mask layer, the calculation of the architecture constraints is relaxed to the
L1 norm of the soft-mask layer’s parameters with a non-parametric multiplication factor. Taking the
conv2d as an example, its number of output channels can be approximated as PiC=H1o mi, with CHo
being the number of output channels. Meanwhile, the output channel i would be pruned if vi < 0
(π < 0.5), resulting in a regularized neural network.
Hence, with v(+1 being the neural architecture soft mask parameters at round t + 1, we re-organize
the objective function (5) as follows:
(wO+1, Vt+ι) = arg min {Lt(wo, v; W(+1) + λLc(v)}
(wo,v)
(θ(+1,W(+1) =	(θo,W(o+1) ◦ v(+1,
(7)
where the operation ◦ means the pruning operation on the server maintained network architecture
θo and the corresponding model weights W(o+1 at the round t + 1 according to the mask parameters
v(+1. The whole process of the proposed algorithm can be found from Algorithm 1, and the process
of FedMorph is presented in Figure 1.
4.2	Intra-Layer Constraints
We relaxed the problem by soft-masks previously, and the constraints of two connected layers are
solved in this subsection. The foot-stone of recent SOTA neural architectures, e.g., ResNet, VGG
for Computing Vision (CV) tasks or Transformer, BERT for Natural Language Processing (NLP)
tasks, is the stacking of shallow layers or blocks in depth to increase the express-ability. Pruning
the previous layer will also influence the next layer. Consider the example of a two-layers MLP
f (x) = σ(x * Wι) * W2, with Wi and W2 denotes the weight matrix of linear mappings, and σ
denotes the non-linear activation functions. Assume m1 and m2 be the learned masking layers of
W1 and W2 , taking effect on the columns, whose sizes equal the number of columns of W1 and
W2 , respectively. In order to be coherent with matrix multiplication, the pruned columns of W1
also affect W2, which will prune the corresponding rows of W2. Moreover, we keep the final output
layer unmasked to keep the coherence of the input and output of the morphed model with the server
maintained one.
5
Under review as a conference paper at ICLR 2022
4.3	Understanding FedMorph
Theorem 1.	The learning of server maintained network weights wo with the knowledge distillation
has the same effect of learning with random dropout layer and a regularizer loss.
Proof. The learning process of randomly dropout (Srivastava et al., 2014) is as follows, 1) train-
ing the network weights with dropout layers; 2) evaluate on the test dataset without activating the
dropout layers. By setting τ → 0, the weights of soft-mask layers would generate either 1 or 0 (ap-
proximated) and mimic the random dropouts characters on the neuron level. The knowledge transfer
process of the first equation in (7) equals to the optimization of wo with randomly dropout layers
stacked after every learnable layer. The expectation on (v + l) determines the dropout ratio. The
convergence of wto+1 at each knowledge distillation process is guaranteed with sufficient researches
on deep learning with dropouts.	□
With Theorem 1, we know that the server maintained network will have a good performance on
the validation dataset as long as the aggregated network performs well on it. Now, we want to
study the generalization performance of the morphed network. In other words, how the knowledge
on the training datasets of local clients is perceived by the morphed network via learning from the
aggregated network on the validation dataset.
Theorem 2.	Let a sample US of size m drawn from DS for knowledge distillation at round t, and
δ < 1 being its upper bound of the empirical error such that * LX∈us (wt+i； Wt+ι) ≤ δ. Then the
expectation evaluation error of wt+1 on DT is bounded, with the probability 1 - δ:
'Dt (Wt+i) ≤'Dτ(Wt+l) +
-mlogδ	1
---2-- ) + 2 dH∆H (DS, DT) ,
where dH∆H (DS, DT) measures the distance of the knowledge distillation dataset distribution DS
and local training dataset distribution DT.
With Theorem 2, we know that the error upper bound of the morphed neural network on the lo-
cal training dataset distribution mainly consists of 1) the expectation error of the averaged aggre-
gated model on the local dataset; 2) the empirical error δ of knowledge distillation on the validation
dataset; 3) terms dependent on the divergence between local training dataset distribution and eval-
uation dataset distribution. If the local training dataset and validation dataset follows the same
distribution, we have dH∆H (DS, DT) = 0, then the knowledge perceived by the newly morphed
network is only determined by the empirical risk of knowledge distillation.
5 Experiments
5.1	Setup
Datasets and Networks We evaluate the performance of our method on multiple tasks on differ-
ent networks architectures, specifically on the dataset of CIFAR-10 (Krizhevsky & Hinton, 2010)
with the network of ResNet18 (He et al., 2016) and VGG16 (Simonyan & Zisserman, 2014). All
networks with the corresponding dataset are initialized randomly. We refer the readers to find the
experimental results and details of other datasets and network architectures in the Appendix.
Baselines. We focus on testing our methods against already well-established FL benchmarks.
In particular, we restrict our baseline algorithm to the use of FedAvg, the same algorithm we use
to calculate Wt+i in equation 4. To fairly evaluate the compression ratio of our proposed method,
we combine the tricks of sparsification for client-to-server communication, precisely the Top-K
sparsification trick proposed in (Lin et al., 2017), with the baseline algorithm. We also compare our
method with FedDropout (Caldas et al., 2018), a strategy to compress communications in the server-
to-client direction. Besides, we consider a new benchmark method, ‘FedPruned,’ which applies the
FedAvg algorithm with a sub-network randomly pruned from the global network at initialization.
The FedAvg, FedDropout, and FedPruned are also evaluated under the Top-K sparsification trick
without losing fairness.
6
Under review as a conference paper at ICLR 2022
Table 1: The test accuracy, PARAMs and FLOPs compression ratio of VGG16 and ResNet18 on
CIFAR-10 in IID partition. The compressed model PARAMs and FLOPs are normalized according
to FedAvg. The maximum communication rounds are 4000 for both networks. For FedMorph, we
output the averaged PARAMs and FLOPs before the test accuracy reaching the maximum value.
Method	VGG16 on CIFAR-10			ResNet18 on CIFAR-10		
	Accuracy	Params	Flops	Accuracy	Params	Flops
FedAvg	0.9167			0.9294		
FedAvg 25%	0.9140	1.000	1.000	0.9239	1.000	1.000
FedAvg 10%	0.9008			0.9149		
FedDropout	0.8987			0.9048		
FedDropout 25%	0.8885	0.3610	0.3622	0.8896	0.3607	0.3611
FedDropout 10%	0.8595			0.8589		
FedPruned	0.9063			0.9182		
FedPruned 25%	0.9054	0.3600	0.3705	0.9134	0.3534	0.3734
FedPruned 10%	0.8996			0.9086		
FedMorph	0.9206	0.1066	0.2893	0.9311	0.1172	0.2289
FedMorph 25%	0.9175	0.0946	0.2357	0.9300	0.1165	0.2501
FedMorph 10%	0.9065	0.0884	0.2300	0.9200	0.0975	0.2027
Hyper-Parameters. The dataset is partitioned within 100 clients at the initialization procedure in
an IID fashion, and we randomly sample ten candidates for local optimization during each commu-
nication round. The optimized model gradients would be sparsified by the TopK method at a ratio
of {10%, 25%} before updating to the global server. For local training at each client, we use the
SGD optimizer with momentum to be 0.9 and weight decay to be 5e - 4. We set the static learning
rates of 0.05 for CIFAR-10. Each selected client trains for one epoch per round using a batch size of
100. We set the knowledge distillation learning rates as 0.005 and the batch size as 50 while keep-
ing other hyper-parameters following the local training procedure. We use a static temperature τ of
1e - 3 for all experiments during the whole process. We set the regularizer as the morphed model
parameters and evaluate our method on model PARAMs and FLOPs, and set the regularizer ratios λ
of CIFAR-10 for ResNet18 and VGG16 as 1.2e - 6, and 1e - 6, respectively. The soft-mask layer
parameter v is initialized randomly following the logistic distribution with its mean value being the
preset model compression ratio. In detail, we set the mean value of v to be 0.3 for both networks.
For FedDropout and FedPrune, we set the same initialized compression ratios of 0.6. Moreover,
we set the random seed to be ‘12345’ and built all experiments on a single GPU V100 to make the
results reproducible.
Training Tricks. The knowledge distillation process that happens on the server-side at each
communication round is computation-heavy. Therefore, to overcome the computation overloads,
we 1) morphs the network when the aggregated model weights have changed significantly and 2)
initialize the mapped part of the server maintained network with the average aggregated weights
before the knowledge distillation procedure. In practice, we distill knowledge from the aggregated
model every ten communication rounds and set the optimization epochs as 2 for every dataset.
5.2	Test Accuracy Comparison
We present the accuracy evaluation results as in Figure 2. In order to make a clear tendency, we
apply the moving average on the result data points, saying to replace the point with the unweighted
mean of its nearest 10 points. Besides, we report the maximal accuracy and the average compressed
PARAMs and FLOPs in Table 1 for further clarification. The three main takeaways from these
experiments are: 1) our proposed method produce competitive results (accuracy) comparing to the
benchmarks; 2) FedMorph can further compress PARAMs and FLOPs without degrading the test
accuracy; 3)FedMorph is compatible with the gradient sparsification methods, hence can further
reduce the communication burden in FL settings. With the compressed PARAMs and FLOPs from
Table 1, in other words, We achieves 0⅛84 ≈ 1l× and 的1 * 10 ≈ 113× reduction of CommUni-
cation for CIFAR-10 with VGG16 in both server-to-clients and clients-to-server sides respectively.
Meanwhile, the computation FLOPs achieves 0^^ ≈ 4× reduction for it. Similar analysis on
CIFAR-10 dataset with ResNet18 network, witho.ut degrading the test accuracy comparing with Fe-
7
Under review as a conference paper at ICLR 2022
FedMorph
FedAvg
FedDnopout
PrunnedNet
ResNetl8: sparse=100.0
0.95-
⅛ 0.90-
n
u 0.85 -
<
0.80-
2000	4000 O
Number of Rounds
0.95-
⅛ 0.90-
n
U 0.85 -
<
0.80-
ResNetl8: sparse=25.0
2000	4000 O
Number of Rounds
0.95-
⅛ 0.90-
n
U 0.85-
<
0.80-
ResNetl8: sparse=10.0
2000	4000
Number of Rounds
FedMorph
FedAvg
FedDnopout
PrunnedNet
Figure 2: Test accuracy evaluation on CIFAR-10 with varied sparsification percentages and methods.
initialized
optimized
o
Figure 3:	FedMorph compression ratio for each layer on CIFAR-10.
dAvg, we achieves around 10×, 102×, and 5× reduction of server-to-clients side communication,
clients-to-server side communication, and local clients computation FLOPs, respectively.
5.3	Layer Compression Ratio
Another observation from Table 1 is that FedMorph has unbalanced PARAMs and FLOPs compres-
sion ratios. Therefore, we plot the compressed ratio of each masked layer as presented in Figure
3. The labels on the x-axis denote the masked layers from shallow to deep. The red bar, blue
bar, and blue line denote the initialized compression ratio (soft-mask layer parameters), optimized
compression ratio (soft-mask layer parameters when test accuracy reaches the maximum), and the
trendline of the blue bar with moving average, respectively. It is clear to observe that FedMorph
tends to compress the deep layers while leaving the shallow layers uncompressed for both network
architecture, even though the mask layer parameters are initialized uniform-randomly. The reason is
that the architectures of ResNets and VGGs are like pyramids, which have heavy parameters at deep
layers. The unbalanced compression ratio of each layer proves the effectiveness of the PARAMs
regularizer in our objective function. Besides, by comparing the test accuracy with FedPruned, a
method with randomly pruned neurons at initialization, we know that the PARAMs regularizer can
not only reduce the model size but also aim at not decreasing its accuracy.
5.4	Generalization Gap
It is also necessary to know why FedMorph generalizes better than other baselines. We use the
gap between the training accuracy and test accuracy to measure the generalization performance of
the experimented algorithms, as presented in Figure 4. We calculate the training accuracy by the
mean of local clients’ accuracy, who has been selected for local updates between two consecutive
times of the evaluation on the test dataset. For both datasets, Fedmorph has a significantly smaller
generalization gap than that of FedAvg and FedDropout. It is worthwhile noticing that comparing to
FedMorph, the training accuracy of FedAvg quickly reaches near to 1, and the training accuracy of
8
Under review as a conference paper at ICLR 2022
VGG16： Generalization Gap
> 1.00 -
2 0.95 -
y 0.90-
« 0.85 -
0.80-
ResNetl8: Generalization Gap
> i.oo -
2 0.95 -
y 0.90-
« 0.85-
0.80-
---FedMorph: Tfest
—FedMorph: Ttaln
---FedAvg: Tfest
—FedAszgrTtaIn
---FedDropout: ^ι⅛st
—FedDropout: Taln
0	800	1600	2400	3200	4000	0	800	1600	2400	3200	4000
Number of Rounds
Number of Rounds
Figure 4:	The generalization gaps for different methods on CIFAR-10.
0.9
0.5 ÷
0
&0 8-
ɔ 0.7 -
⅛ 0.6-
VGG16: sparse=100.0
0.9 η----------------------
4000
0.80 4—
3000
2000
Number of Rounds
4000
0.5
0
VGG16: sparse=25.0
&0 8-
ɔ 0.7 -
⅛ 0.6-
2000
Number of Rounds
4000
0.9
0.5
0
VGG16: sparse= 10.0
&0 8-
ɔ 0.7 -
⅛ 0.6-
4000
FedMorph
FedMorphProx
FedAvg
FedProx
2000
Number of Rounds
0.5
0
&0 8-
ɔ 0.7 -
⅛ 0.6-
ResNetl8: sparse=100.0
0.9
2000
Number of Rounds
4000
0.5
0
&0 8-
ɔ 0.7 -
⅛ 0.6-
ResNetl8: sparse=25.0
0.9
2000
Number of Rounds
4000
0.5
0
&0 8-
ɔ 0.7 -
⅛ 0.6-
ResNetl8: sparse= 10.0
0.9
4000
FedMorph
FedMorphProx
FedAvg
FedProx
2000
Number of Rounds
Figure 5:	Test accuracy evaluation on CIFAR-10 with FedMorphProx in NonIID partition settings.
FedDropout is less than its test accuracy. FedAvg and FedDropout perform worse than FedMorph
because FedAvg quickly gets over-fitted on the local dataset, which impedes its generalization well
on the test dataset, while FedDropout somehow under-fits on the local training dataset. FedMorph
balances well on the training and the evaluation parts, away from either over-fitting or under-fitting,
making it generalizes the best among its competitors.
5.5 FedMorph Variant
This section evaluates FedMorphProx, a variant of FedMorph, by replacing the FedAvg algorithm
for aggregation to FedProx (Li et al., 2018), on the NonIID partitions. FedProx limits the over-fitting
of FedAvg on local datasets by constraining the local updated parameters not far away from the
received model parameters at the beginning. In experimental details, we set the number of samples
of each class in each client follows a Dirichlet distribution with α = 0.4 and set the proximal
ratio μ in FedProx as 1e - 2 for CIFAR-10 while keeping other hyper-parameter untouched. We
present the test accuracy results in Figure 5. The FedMorphProx and FedMorph algorithms perform
significantly better than either FedAvg or FedProx. Besides, FedMorphProx performs as well as
FedMorph, proving our method works for FedAvg and other FedAvg-like FL algorithms.
6 Conclusion
In this paper, we proposed that sharing a sub-network among the clients and the server can reduce
edge devices’ communication and computation overloads and increase the generalization accuracy
by avoiding over-fitting of FL on local datasets. The empirical experiments on varied datasets and
network architectures with different FL settings, e.g., IID partition and Dirichlet partition, proved
the superior performance of FedMorph. By analyzing the optimized morphed network, our work
might also guide the design of network architectures in FL settings in the future.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} symposium on operating systems design and imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1):151-175,
2010.
Cody Blakeney, Yan Yan, and Ziliang Zong. Is pruning compression?: Investigating pruning via
network layer similarity. In Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pp. 914-922, 2020.
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny,
S. Mazzocchi, H. B. McMahan, et al. Towards federated learning at scale: System design. arXiv
preprint arXiv:1902.01046, 2019.
Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-MiziL Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 535-541, 2006.
Sebastian Caldas, Jakub Konecny, H Brendan McMahan, and Ameet Talwalkar. Expanding the reach
of federated learning by reducing client resource requirements. arXiv preprint arXiv:1812.07210,
2018.
Shraman Ray Chaudhuri, Elad Eban, Hanhan Li, Max Moroz, and Yair Movshovitz-Attias. Fine-
grained stochastic architecture search. arXiv preprint arXiv:2006.09581, 2020.
Defang Chen, Jian-Ping Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, and Chun Chen. Cross-
layer distillation with semantic calibration. In Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 7028-7036, 2021.
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for deep
neural networks: The principles, progress, and challenges. IEEE Signal Processing Magazine, 35
(1):126-136, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International confer-
ence on machine learning, pp. 2990-2999, 2016.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Li Deng. The mnist database of handwritten digit images for machine learning research [best of the
web]. IEEE Signal Processing Magazine, 29(6):141-142, 2012.
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efficient evaluation. In Proceedings of the 27th In-
ternational Conference on Neural Information Processing Systems - Volume 1, pp. 1269-1277,
2014.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The
Journal of Machine Learning Research, 20(1):1997-2017, 2019.
Mengya Gao, Yujun Wang, and Liang Wan. Residual error based knowledge distillation. Neuro-
computing, 433:154-161, 2021.
10
Under review as a conference paper at ICLR 2022
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-
nov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. The Jour-
nal of Machine Learning Research, 18(1):6869-6898, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,
2019.
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles,
G. Cormode, R. Cummings, et al. Advances and open problems in federated learning. arXiv
preprint arXiv:1912.04977, 2019.
Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Unpublished
manuscript, 40(7):1-9, 2010.
Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang Lin, and Xi-
aojun Chang. Block-wisely supervised neural architecture search with knowledge distillation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
1989-1998, 2020.
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future
directions. arXiv preprint arXiv:1908.07873, 2019.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization. In International Conference on Machine Learning, pp. 6357-
6368. PMLR, 2021.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Re-
ducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887,
2017.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017.
Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without
training. In International Conference on Machine Learning, pp. 7588-7598. PMLR, 2021.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan
Ghasemzadeh. Improved knowledge distillation via teacher assistant. Proceedings of the AAAI
Conference on Artificial Intelligence, 34(04):5191-5198, April 2020.
11
Under review as a conference paper at ICLR 2022
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32:
8026-8037, 2019.
Mark A Pitt and In Jae Myung. When a good fit can be bad. Trends in cognitive sciences, 6(10):
421-425, 2002.
Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-dimensional output targets.
In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 6655-
6659, 2013.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas: Improving neural archi-
tecture search via attentive sampling. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 6418-6427, June 2021.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. arXiv preprint arXiv:1710.09854, 2017.
Bichen Wu, Alvin Wan, Forrest Iandola, Peter H Jin, and Kurt Keutzer. Squeezedet: Unified, small,
low power fully convolutional neural networks for real-time object detection for autonomous
driving. arXiv preprint arXiv:1612.01051, 2016.
Xiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks
with global sparsity constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 3599-3608, 2021.
Hangyu Zhu, Haoyu Zhang, and Yaochu Jin. From federated learning to federated neural architec-
ture search: a survey. Complex & Intelligent Systems, 7(2):639-657, 2021.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
12
Under review as a conference paper at ICLR 2022
A Details of Intral-Layer Constraints
We give details of the intra-layer constraints on these modules, which cannot be sequentially stacked,
e.g., the skip connection. Skip connections, successfully applied in ResNet, a sub-module connect-
ing two unconnected layers of the backbone network, have increased the depth of neural networks
and improved the benchmark results of deep learning. In mathematics, skip connection can be pre-
sented as out = out + skip(x), where x is the input of its previous connected layer, and out is the
output of its last connected layer. The skip connection sub-module can have several stacked simple
layers inside it or output the input directly. For the first condition, we attach the mask of its final
layer with its last connected layer’s mask. For the second condition, we constrain the mask of its
last connected layer to be its previous connected layer’s mask.
B	Experimental Setup
B.1	Summary of Datasets and Networks
We conduct the experiments on the MNIST (Deng, 2012), EMNIST (Cohen et al., 2017), and
CIFAR-10 (Krizhevsky & Hinton, 2010) to consider the tasks on multiple visual computation tasks.
For MNIST and EMNIST, we apply ResNet8 (He et al., 2016) as global network architecture. For
CIFAR-10, we apply ResNet18 (He et al., 2016) and VGG16 (Simonyan & Zisserman, 2014) as the
global networks. The network architectures used in our experiments are simply presented in Table
7, Table 8, and Table 9 at the end of this appendix. We abbreviate the details of datasets as follows,
•	MNIST: is a handwritten digits (0-9) classification task with 60,000 data points in the
training dataset and 10,000 data points in the test dataset. The input of the dataset is a
flattened 784-dimensional (28 × 28) image, and the output is a class label between 0 and 9.
•	EMNIST: is a set of handwritten character digits derived from the NIST Special Database
19 and converted to a 28x28 pixel image format. One unbalanced split, named ByClass,
has 62 unbalanced class labels with 697,932 training samples and 116,323 test samples.
•	CIFAR-10: consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per
class. There are 50,000 data points in the training dataset and 10,000 data points in the test
dataset.
In abbreviation, the corresponding datasets and the networks are summarized in Table 2. The exper-
imental settings can roughly evaluate the performance of FedMorph within different conditions:
•	multiple tasks with the corresponding network architecture and dataset;
•	the same task with different network architectures, e.g., CIFAR-10 with ResNet18 and
VGG16;
•	the same network architecture on different tasks, e.g., MNIST and EMNIST with ResNet8;
•	the huge number of available clients, e.g., EMNIST with 5000 available clients.
Table 2: The summary of datasets and network architectures.
Model	sampletrain	SamPledistill	#classes	PARAMS	FLOPs
MnIst	60,000	10,000-	10	4,904,670	15,302,164
EMNIST	697,932	-116,323-	62	4,931,326	15,328,768
CIFAR10ResNet18	50,000	10,000-	10	11,173,962	556,651,520
CIFAR10VGGH	50,000	10,000 —	10	14,728,266	314,031,616—
B.2	Hyper-Parameters
It is always artistic to find the optimal hyper-parameters applied in deep learning. Instead of op-
timizing the hyper-parameters, we assign them casually but reasonably and present the detailed
hyper-parameters applied in our experiments. The dataset is partitioned within 100 clients in an
artificially IID fashion at the initialization procedure for MNIST and CIFAR-10, and 5000 clients
13
Under review as a conference paper at ICLR 2022
Table 3: The summary of neural networks and hyper-parameters used in our experiments.
Model	lr train	lrdistill	λ	V	#epochs	#rounds	#Clients
ResNet8M N I ST	-0.1-	0.01	1.6e-6	ɪr	1	-1500-	-100^^
ResNet8EMNIST	-01-	0.01	2e-5	~2Γ	5	-3000-	-5000^^
ResNet18CIF AR10	0.05	0.005	1.2e-6	^01^	1	-4000-	-100^^
VGG16CIF AR10	0.05	0.005	1e-6	~01~	1	4000	100
MNIST: sparse=25.0
0.95-
MNIST: sparse=100.0
1000	1500
0.95-
0	500	1000	1500
Number of Rounds
/^0^995
0.991
1000	1500
0.95-
MNIST: sparse= 10.0
>us3uu<
----FedMorph
----FedAvg
----FedDropout
----PrannedNet
0	500	1000	1500
Number of Rounds
0.81
0
⅛ 0.85 -
[0.83-
EMNIST: sparse= 100.0
0.87
0.81
1500	3000	0
Number of Rounds
0.87
0	500	1000	1500
Number of Rounds
EMNIST: sparse=25.0
⅛ 0.85 -
[0.83-
0.81
1500	3000	0
Number of Rounds
0.87
EMNIST: sparse=10.0
⅛ 0.85 -
[0.83-
FedMorph
FedAvg
FedDnopout
PrunnedNet
1500	3000
Number of Rounds
Figure 6: Test accuracy evaluation on MNIST and EMNIST with various sparsification percentages
and methods.
for EMNIST. The updated model would be sparsified by the TopK method at a ratio of {10%, 25%}
before aggregation. For local training at each client, we use the SGD optimizer with momentum to
be 0.9 and weight decay to be 5e - 4. We set the static learning rates of 0.1 for MNIST and EM-
NIST. For CIFAR-10, we select 0.05 as the learning rate. Ten clients are randomly selected to train
for one epoch per communication round using a batch size of 100 for MNIST and CIFAR-10. For
EMNIST, local clients train for five epochs per round. We set the knowledge distillation dataset as
the test dataset without label information, and its training procedure follows the same SGD settings
as the local training procedure. We set the distillation learning rates as one-tenth of that for local
training and the batch sizes as 50, 500, and 50 for MNIST, EMNIST, and CIFAR-10, respectively.
We use a static temperature τ of 1e - 3 for all experiments during each round. We set the regularizer
as the morphed model parameters and evaluate our method on model PARAMs and FLOPs. We set
the regularizer ratios λ to be 1.6e - 6 and 2e - 5 for MNIST and EMNIST. For CIFAR-10 with
ResNet18 and VGG16, λ is 1.2e - 6, and 1e - 6, respectively. The soft-mask layer parameter v is
initialized randomly following the logistic distribution, with its mean value being the preset model
compression ratio. Specifically, we set the mean value of v to be 0.1, 0.2, 0.3, and 0.3, respectively.
For FedDropout and FedPrune, we set the same initialized compression ratios of 0.25, 0.35, 0.6,
and 0.6, respectively, for the four tasks. Moreover, the maximum communication rounds are 1500,
3000, 4000, and 4000. To make the results reproducible, we set the random seed to be ‘12345’ for
all experiments and build the experiments on GPU V100 within Pytorch. In conclusion, the cor-
responding hyper-parameters are listed in Table 3. If not specified, the hyper-parameters are kept
unchanged.
C Additional Experimental Evaluations
This section presents the additional experimental results missed in the main text due to the page
limitation. In the following, we evaluate the performance of FedMorph on the datasets of MNIST
and EMNIST in terms of test accuracy, compression ratio, and generalization gap. At last, we present
the comparison results of FedMorph and its transient networks to help understand the algorithm.
14
Under review as a conference paper at ICLR 2022
Figure 7: FedMorph compression ratio for each layer on MNIST and EMNIST.
EMNIST: Compression Ratio
IffllM
initialized
optimized
Table 4: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasets
of MNIST and EMNIST in IID partition. The compressed model PARAMs and FLOPs are normal-
ized according to FedAvg. The maximum communication rounds are 1500 and 3000, respectively.
For FedMorph, we output the averaged PARAMs and FLOPs before the test accuracy reaches the
maximum value.
Method	MNIST			EMNIST		
	Accuracy	Params	Flops	Accuracy	Params	Flops
FedAvg	0.9954			0.8605		
FedAvg + 25% TopK	0.9950	1.000	1.000	0.8619	1.000	1.000
FedAvg + 10% TopK	0.9943			0.8627		
FedDropout	0.9870			0.8507		
FedDropout + 25% TopK	0.9844	0.0630	0.0705	0.8485	0.1240	0.1328
FedDropout + 10% TopK	0.9819			0.8436		
FedPruned	0.9941			0.8554		
FedPruned + 25% TopK	0.9934	0.0616	0.0765	0.8570	0.1209	0.1327
FedPruned + 10% TopK	0.9930			0.8590		
FedMorph	0.9955	0.0120	0.0550	0.8622	0.1083	0.3643
FedMorph + 25% TopK	0.9951	0.0117	0.0492	0.8628	0.0838	0.2881
FedMorph + 10% TopK	0.9952	0.0118	0.0570	0.8625	0.0901	0.3463
C.1 Test Accuracy Comparison
The results of test accuracy versus the number of communication rounds are presented in Figure
6. FedMorph converges on the dataset of MNIST and EMNIST with ResNet8, and generalizes
slightly better than the benchmarks. The results of MNIST and EMNIST on the same network
architecture of ResNet8 prove that FedMroph generates good results irrelevant to datasets. The
average compression ratio when reaches the maximal test accuracy is presented in Table 4. For
MNIST dataset with ResNet8, We achieves J ≈ 85 × and ∖ * 10 ≈ 847× reduction of
0.0118	0.0118
Server-to-client side communication and Client-to-server side communication, and C EC ≈ 18 ×
0.0570
reduction of local clients computation. For EMNIST dataset with ResNet8, we achieves 0 09°1 ≈
1× and 0 o9oι * 10 ≈ 111 × reduction of server-to-client side communication and client-to-server
side communication, and 0⅛3 ≈ 3× reduction of local clients computation.
C.2 Layer Compression Ratio
We present the layer compression ratio of FedMorph on MNIST and EMNIST in Figure 7. As
expected in the analysis of CIFAR-10 with ResNet18 and VGG16, FedMorph compresses the deep
layers of ResNet8 while keeping the shallow layers untouched to maintain its performance and the
model size.
C.3 Generalization Gap
In order to prove that a small morphed network generates better generalization performance, we
present the generalization gap of MNIST and EMNIST in Figure 8. FedMorph has a significantly
smallest generalization gap for MNIST among the competitors. Comparing with FedMorph, FedAvg
15
Under review as a conference paper at ICLR 2022
MNIST: Generalization Gap
EMNIST: Generalization Gap
---FedMorph: Tfest
—FedMorph: Ttaln
---FedAvg: Tfest
—FedAszgrTtaIn
---FedDropout: ^ι⅛st
—FedDropout: Taln
Figure 8: The generalization gaps for different methods on MNIST and EMNIST.
Table 5: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasets
OfMNIST and EMNIST in NonIJID Partition settings._____________________________________
Method		MNIST				EMNIST		
	Accuracy	Params	Flops	Accuracy	Params	Flops
FedAvg	09928			0.8599		
FedAvg + 25% TopK	0.9919	1.000	1.000	0.8558	1.000	1.000
FedAvg + 10% TopK	0.9912			0.8444		
FedProx	0.9929			0.8587		
FedProx + 25% TopK	0.9924	1.000	1.000	0.8544	1.000	1.000
FedProx + 10% TopK	0.9899			0.8463		
FedMorph	0.9939	0.0117	0.0576	0.8606	0.0986	0.3240
FedMorph + 25% TopK	0.9934	0.0105	0.0495	0.8595	0.0904	0.3105
FedMorph + 10% TopK	0.9920	0.0103	0.0401	0.8588	0.0850	0.2968
FedMorphProx	0.9938	0.0105	0.0506	0.8599	0.1003	0.3252
FedMorphProx + 25%	0.9930	0.0102	0.0480	0.8597	0.0824	0.2761
FedMorphProx + 10% TopK	09934	0.0107	0.0448	0.8589	0.0932	0.3190
Table 6: The test accuracy, PARAMs and FLOPs compression ratio of different methods on datasets
of CIFAR-10 in NonnD Partition._______________________________________________________
Method	VGG16 on CIFAR-10			ResNet18 on CIFAR-10		
	Accuracy	Params	Flops	Accuracy	Params	Flops
FedAvg	0.8323			0.7679		
FedAvg + 25% TopK	0.8262	1.000	1.000	0.7651	1.000	1.000
FedAvg + 10% TopK	0.7758			0.7357		
FedProx	0.8377			0.7667		
FedProx + 25% TopK	0.8015	1.000	1.000	0.7636	1.000	1.000
FedProx + 10% TopK	0.7774			0.7385		
FedMorph	0.8492	0.0903	0.2467	0.8295	0.0948	0.2161
FedMorph + 25% TopK	0.8444	0.0925	0.2497	0.8348	0.1012	0.3125
FedMorph + 10% TopK	0.8360	0.0934	0.2616	0.8278	0.0861	0.2991
FedMorphProx	0.8510	0.0953	0.2737	0.8355	0.0955	0.2210
FedMorphProx + 25% TopK	0.8446	0.0900	0.2611	0.8326	0.0882	0.2181
FedMorphProx + 10% TopK	08311	0.0935	0.2415	0.7357	0.9998	0.0078
is over-fitted as its training accuracy quickly reaches 1. While FedDropout is under-fitted as its
training accuracy converges below that of FedMorph. For EMNIST, the dataset of size 697,932 is
partitioned to 5000 clients, resulting in that each client has lesser than 140 samples. Furthermore,
we set the number of local optimization epochs to be 5. Hence, all methods are easily over-fitted.
However, comparing with FedAvg and FedDropout, FedMorph still performs better than them.
C.4 FedMorphProx with NonIID settings
We set the number of samples of each class in each client follows a Dirichlet distribution with
α = 0.4, and set the proximal ratio μ in FedProx to be 1e - 2 for all tasks. For MNIST and EMNIST,
we decrease the local learning rates to be 0.01 for a smooth convergence while keeping other hyper-
16
Under review as a conference paper at ICLR 2022
1.00
>UE3UU<
0.95-
MNIST： sparse= 100.0
0.995
1500
0.985
1200
500 IOOO
1.00
0.95-
MNIST： sparse=25.0
0.995
0.985
1200
500 IOOO
1.00
0.90 4-
O
0.90
1500 O
1500
0.95
0.90
MNIST： sparse= 10.0
0.995
0.985
1200
500 IOOO
∕≡
1500
FedMorph
FedMorphProx
FedAvg
FedProx
1500 O
1500
Number of Rounds	Number of Rounds	Number of Rounds
EMNIST: sparse= 100.0
&0-8
ɔ 0.7
⅛ 0.6-
0.5-
0
3000
0.83
2400
1000	2000
Number of Rounds
EMNIST: sparse=25.0
&0-8
ɔ 0.7
⅛ 0.6-
0.5-
)	0
0.85
3000
0.83
2400
1000	2000
£0.8 -
ɔ 0.7 -
/ 0.6-
0.5 -
)	0	1000	2000	3000
Number of Rounds
EMNIST: sparse=10.0
Number of Rounds
FedMorph
FedMorphProx
FedAvg
FedProx
3000
3000
Figure 9:	Test accuracy evaluation on MNIST and EMNIST with FedMorphProx in NonIID partition
settings.
0.99-
0.98-
MNIST sparse=100.0
1200	1500
& 0 84-
lŋ
⅛ 0.81 -
u
< 0.78-
0	500	1000	1500
Number of Rounds
EMNIST: sparse=100.0
0	1500	3000
Number of Rounds
>us3uu<
0.85 -
ResNetl8: sparse=100.0
----FedMorph
----Averaged
----Morphed
0	2000	4000
Number of Rounds
Figure 10:	The test accuracy of FedMorph and the transient networks in IID partition settings.
parameter untouched. We present the result of test accuracy versus the number of communication
rounds for MNIST and EMNIST in Figure 9. In our FL settings, FedMroph and FedMorphProx have
slightly better accuracy results than that of FedAvg and FedProx. Besides, FedMorphProx performs
as well as FedMorph, proving our method works for FedAvg and other FedAvg-like FL algorithms.
It is also worthwhile pointing that FedMorph and FedMorphProx converge smoother than FedAvg
and FedProx. We present the maximal test accuracy and the compressed PARAMs and FLOPs of
all datasets in Table 5 and Table 6 for clarification.
C.5 Test Accuracy of the Transient Network
We study the performance of the transient networks, specifically, the morphed network wt of each
communication round t and the average aggregated network Wt of the morphed networks from lo-
cal updates, and denote them as ‘Morphed’ and ‘Averaged.’ The results of the test accuracy versus
the number of rounds for different datasets are presented in Figure 10. Interestingly, FedMorph has
lower test accuracy than the other two transient networks but reaches the highest accuracy when con-
verging. The explanation is that FedMorph learns knowledge from the smaller network ‘Averaged’ at
an idle state, which causes its accuracy to be lower than that of ‘Averaged’ at the begging phase. By
morphing a new network before each communication round, FedMorph can learn knowledge from
offbeat teachers and store the knowledge inside the different parts of its larger network. When the
smaller teacher network becomes converged, FedMorph combines all teachers’ knowledge, hence
generating better performance than the ‘Averaged’ when converging.
D Ablation Study
This section studies the convergence issues with different hyper-parameters, e.g., the initial values
ofv and the regularizer ratio λ. The following experiments are built on CIFAR-10 with ResNet18.
17
Under review as a conference paper at ICLR 2022

ResNetl8: varied v
v=0.05
v=0.1
v=0.2
---v=0.3
v=0.5
—v=0.7
ResNetl8: varied v
0.94-
⅛ 0.93-
ɔ
U 0.92 -
I 0.91 -
0.90 -
0.05 0.1 0.2 0.3 0.5 0.7
-0.2
2000	4000
Number of Rounds
Figure 11:	FedMorph test accuracy versus number of rounds for different initial compression ratios.
ResNetl8: varied 入
>023u<
<
0.86
ResNetl8: varied 入
0.94-
0.92-
0.90-
0.88-
-0.15
-0.10
le-7 le-6 2e-6 5e-6 le-5
0.05
Figure 12: FedMorph test accuracy versus number of rounds for different model parameter regular-
izer ratios.
D.1 THE CONVERGENCE ISSUES WITH DIFFERENT v
For different initial v, we present the test accuracy versus the number of communication rounds and
the maximal test accuracy versus varied v in Figure 11. We keep the hyper-parameter unchanged
and set λ = 1.2e - 6 for different experiments. The left-hand-side (LHS) figure presents that test
accuracy converges faster when the initial v is higher. The right-hand-side (RHS) figure presents the
maximal test accuracy and the average PARAMs compression ratio for different v when it reaches
the maximal test accuracy. We use the size of the triangle and the color bar to denote the value of
PARAMs. It appears that a more significant initial v leads to a higher compression ratio. Moreover,
the maximal test accuracy increases with the initial v and decreases after reaching optimal. In this
condition, the optimal initial v is around 0.3.
D.2 THE CONVERGENCE ISSUES WITH DIFFERENT λ
We present the results of test accuracy versus communication rounds of different regularizer ratios
λ in Figure 12. We set the initial v to be 0.3 for all experiments and keep the hyper-parameters
unchanged. The LHS figure presents that increasing the regularizer ratio to a large value will signif-
icantly deteriorate the test performance. The right-hand-side (RHS) figure presents the maximal test
accuracy and the average PARAMs compression ratio when it reaches the maximal test accuracy for
different regularizer ratios. As can be found from the color bar of triangles, a larger λ generates a
smaller compression ratio but leads to terrible test accuracy. Moreover, an over-small λ can gener-
ate a consistent performance but increases the compression ratio. Therefore, an optimal λ exists to
balance the test accuracy and compression ratio. In this condition, the optimal λ is around 1e - 6.
18
Under review as a conference paper at ICLR 2022
E	Understanding FedMorph
In this section, we want to understand
•	why learning from a small average aggregated sub-network guarantees the performance of
the global model;
•	how much ‘knowledge’ is transmitted from the average aggregated model in the previous
round to the next round.
In the following, we first analyze the convergence of server maintained network weights by consid-
ering the morphing process similar to adding dropout layers in Theorem 1. Then we analyze the
bound of the expected error of the morphed model on both the training and test dataset in Theorem
2 and Theorem 3 with theorems from Domain Adaptation (Ben-David et al., 2010).
E.1 Analysis on Server Maintained Network
Theorem 1.	The learning of server maintained network weights wo with the knowledge distillation
has the same effect of learning with random dropout layer and a regularizer loss.
Proof. The learning process of randomly dropout (Srivastava et al., 2014) is as follows, 1) train-
ing the network weights with dropout layers; 2) evaluate on the test dataset without activating the
dropout layers. By setting τ → 0, the weights of soft-mask layers would generate either 1 or 0 (ap-
proximated) and mimic the random dropouts characters on the neuron level. The knowledge transfer
process of the first equation in (7) equals to the optimization of wo with randomly dropout layers
stacked after every learnable layer. The expectation on (v + l) determines the dropout ratio. The
convergence of wto+1 at each knowledge distillation process is guaranteed with sufficient researches
on deep learning with dropouts.	□
E.2 Analysis on Morphed Network
We want to study how the morphed neural network perceives the knowledge of local training datasets
from the distillation on the average aggregated model with the validation dataset. Our theories and
proofs referred a lot from Ben-David et al. (2010).
Before giving any theories, we make some important definitions to clarify the problem. Define a
domain as a pair consisting of a distribution D on inputs X and the labeling function f : X → [0, 1],
which can have a fractional (expected) value when labeling occurs non-deterministically. Initially,
we construct two domains with the validation dataset for knowledge distillation and the training
datasets for local optimization, denoted by hDS, fSi, and hDT , fTi, naming as source domain and
target domain, respectively. The labeling function fDS (x) and fDT (x) on the sample x generate the
True label of x in corresponding datasets.
Define a hypothesis as a function h : X → {0, 1}. The probability according to the distribution DS
that a hypothesis h disagrees with a labeling function f (which can also be a hypothesis) is defined
as
's(h,f) = Ex〜DS[∣h(x) - f(x)∣]
When we want to refer to the source error (sometimes called risk) of a hypothesis, we use the
shorthand 's(h) = 's(h, fs). We write the empirical source error as 's(h). The similar definitions
can be built on 'τ(h), 'τ(h, fτ) and 'τ(h).
Proposition 1. (Hoeffding’s inequality) Let x1, . . . , xm be independent bounded random variables
with xi ∈ [a, b] for all i, where -∞ < a ≤ b < ∞. Then, for all ζ ≥ 0 we have,
Pr (m X(Xi- E [xi D ≥ζ! ≤eχp(- (b ma)2)
Remark 1. Hoeffding’s inequality provides an upper bound on the probability that the sum of
bounded independent random variables deviates from its expected value by more than a certain
19
Under review as a conference paper at ICLR 2022
amount. Furthermore, if xι,..., Xm are sampled from the same distribution such that X 〜 D, then
Hoeffding’s inequality can be simplized as follows,
Pr (Im Xx Xi- Eχ~D [χ] ≥ζ! ≤ eχp(一(b2⅛)
Before giving the theories, we refer Ben-David et al. (2010) to define the symmetric difference
hypothesis space H∆Hand the bounding of any two hypothesises in different domains as follows.
Definition 2. For a hypothesis space H, the symmetric difference hypothesis space H∆H is the set
of hypotheses
g ∈ H∆H y⇒ g(x) = h(x)㊉ h0(x) for some h,h0 ∈ H
where ㊉ is the XOR function. In words, every hypothesis g ∈ H∆H is the set of disagreements
between two hypotheses in H.
The following simple lemma shows how we can make use of the H∆H-divergence in bounding the
error of our hypothesis.
Lemma 1. For any hypotheses h, h0 ∈ H, the error difference in any two domains S and T is
bounded as follows,
∣'s (h,h0)- 't (h,h0)∣ ≤ 2dH∆H (DS,Dt)
Proof. By the definition of H∆H-distance, we have
dH∆H (DS, DT) = 2 sup IPrx〜Ds [h(x) = h0(x)] - Prx〜Dt [h(x) = h0(x)]∣
h,h0∈H
=2 sup i`s (h,h0) -'t (h,h0)∣ ≥ 2 i`s (h,h0) -'t (h,h0)∣
h,h0∈H
(1)
□
With the bounding of Lemma 1, we give the following two lemmas on domain adaptation on differ-
ent hypothesises.
Lemma 2. For any two hypothesises h, h0 ∈ H,
'τ(h) ≤ 'τ(h0)+ 's(h, h0) + 2dH∆H (DS, DT)
Proof. This proof relies on the triangle inequality for classification error, which implies that for any
labeling functions f1, f2, and f3, we have '(f1, f2) ≤ '(f1, f3) +'(f3,f2). Therefore, we have,
'T (h) ≤ 'T(h0) +'T(h,h0)
≤ 'τ(h0) + 's(h, h0) + 2dH∆H (DS, DT)	(,
□
Lemma 3. Define λ，'s(h*) + 't(h*) with h be the optimized hypothesis of the combined
datasets, then for any hypothesis h, h ∈ H,
's(h) ≤ 'τ(h0) + 's(h, h0) + 2dH∆H (DS,Dt) + λ
Proof. This proof relies on the triangle inequality for classification error, which implies that for any
labeling functions f1, f2, and f3, we have '(f1, f2) ≤ '(f1, f3) +'(f3,f2). Therefore, we have,
's(h0) ≤'s (h*) + 's (h0,h*)
≤'s (h*) + 't (h0, h*) + I's (h0, h*) - 'T (h0, h*)|
≤'s (h*) + 't (h0, h*) + 2dH∆H (Dt,Ds)
≤'s (h*) + 't(h0) + 't (h*) + 2dH∆H (Dt,Ds)
='t(h0) + 2dH∆H (DS, DT) + λ
(3)
20
Under review as a conference paper at ICLR 2022
Combining the above inequality with the truth 's(h) ≤ 's(h0)+'s(h, h0), We obtain the lemma. □
Remark 2. The Lemma 2 and Lemma 3 give the bounding of hypothesis h on both the source and
target domain with another hypothesis h0 on the target domain. With two lemmas mentioned above,
we can study the error of the morphed network on the local training datasets and validation dataset
bounded by the average aggregated network on local datasets.
The following lemma studies the expected error bound of the knowledge distillation process given
the empirical error on m data samples with Hoeffding’s inequality.
Lemma 4. Let a sample US of size m drawn from DS for knowledge distillation at round t, and
δ < 1 being its upper bound of the empirical error, such that *Lx∈us (wt+i； Wt+1) ≤ δ. Then,
with the probability 1 - δ its expectation error is bounded:
E [Lt(wt+ι; Wt+i)] ≤ δ (l + r-m2ogδ
Proof. Because of the truth that Lx(Wt+1； Wt+i) ≥ 0 for ∀χ ∈ US and the empirical loss of
Lχ∈Us (wt+i； Wt+i) is bounded by δ, we obtain
Lx(Wt+1； Wt+i) ≤ mδ,	∀x ∈ US	(4)
It means Lx(Wt+i； Wt+i) is bounded by [0,mδ] for any random variable X ∈ Us. Apply the
Hoeffding’s Inequality here, we obtain as follows,
Pr(ILt(Wt+i； Wt+i) - E [Lt(Wt+E Wt+i)] ∣ ≥ Z) ≤ 2exp ((殖2 )	(5)
Let δ = exp(-^), we easily obtain that Z = δ J-m2og δ. Hence, we know that at least at
probability 1 - δ that
E [Lt(Wt+i； Wt+i)] ≤ Lt(Wt+i； Wt+i) + δ J m2ogδ	⑹
It is also worthy noticing that, δ J -m2og δ → 0 as long as δ → 0 .
□
Theorem 2.	Let a sample US of size m drawn from DS for knowledge distillation at round t, and
δ < 1 being its upper bound of the empirical error such that ・ Lx∈us (Wt+i； W t+i) ≤ δ. Then the
expectation evaluation error of Wt+i on DT is bounded, with the probability 1 - δ:
'Dt (Wt+i) ≤'Dt(Wt+i) +
-m log δ	1
---2--- ) + 2 dH∆H (DS, DT) ,
where dH∆H (DS, DT) measures the distance of the knowledge distillation dataset distribution DS
and local training dataset distribution DT.
Proof. Let 'd『(Wt+i), 'ds(Wt+i) be the error of the morphed model and the average aggregated
model on local training datasets. Combine with Lemma 2, we have
'dt(Wt+i) ≤ 'dt(Wt+i) + 'ds(Wt+i, Wt+i) + 2dχ∆H (DS, DT),	⑺
where 'ds(Wt+i, Wt+i) = EDS [Lt(Wt+i； Wt+i)]. Combine with Lemma 4, we obtain the theo-
rem.	□
Theorem 3.	Let a sample US of size m drawn from DS for knowledge distillation at round t, and
δ < 1 being its upper bound of the empirical error such that ・ Lx∈us (Wt+i； W t+i) ≤ δ. Then the
expectation evaluation error of Wt+i on DS is bounded, with the probability 1 - δ:
'ds (Wt+i) ≤'Dτ(Wt+i) +
+ 2d,H∆H (DS, DT) + λ
21
Under review as a conference paper at ICLR 2022
where dH∆H (DS, DT ) measures the distance of the validation dataset DS and local training
dataset DT, and λ = infh∈h ('d『(h) + 'd, (h)) is the optimal loss on the combined dataset distri-
bution;
Proof. Let 'd『(wt+ι), '。丁 (Wt+ι) be the error of the morphed model and the average aggregated
model on local training datasets. Combine With Lemma 3, We have
'ds (wt+l) ≤ 'Dt (Wt+l) + 'Ds (wt+1, Wt+l) + 2dH∆H (DS, DT) + λ,
(8)
Where 'ds(wt+ι, Wt+ι) = EDS [Lt(wt+ι; Wt+ι)]. Combine With Lemma 4, We obtain the theo-
rem.
□
22
Under review as a conference paper at ICLR 2022
Table 7: ResNet8 Architecture
Layer	Kernel	Stride	Channels
Input	-	-	3
Conv+BN+ReLU	3	1	64
Conv+BN+ReLU	-3-	-1-	64-
Conv+BN	3	1	64
Skip (Conv+BN+ReLU)	1	1	64
Conv+BN+ReLU	-3-	-2-	128-
Conv+BN	3	1	128
Skip (Conv+BN+ReLU)	1	2	128
Conv+BN+ReLU	-3-	-2-	256-
Conv+BN	3	1	256
Skip (Conv+BN+ReLU)	1	2	256
Conv+BN+ReLU	-3-	-2-	5Γ^
Conv+BN	3	1	512
SkiP (Conv+BN+ReLU)	1	2	512
AvgPooling	-4-	-4-	5Γ^
Linear	-	-	10
Table 8: VGG16 Architecture
Layer	Kernel	Stride	Channels
Input	-	-	3
Conv+BN+ReLU	3	1	64
Conv+BN+ReLU	3	1	64
MaxPooling	2	2	64
Conv+BN+ReLU	3	1	128
Conv+BN+ReLU	3	1	128
MaxPooling	2	2	128
Conv+BN+ReLU	3	1	256
Conv+BN+ReLU	3	1	256
Conv+BN+ReLU	3	1	256
MaxPooling	2	2	256
Conv+BN+ReLU	3	1	512
Conv+BN+ReLU	3	1	512
Conv+BN+ReLU	3	1	512
MaxPooling	2	2	512
Conv+BN+ReLU	3	1	512
Conv+BN+ReLU	3	1	512
Conv+BN+ReLU	3	1	512
MaxPooling	2	2	512
AvgPooling	1	1	512
Linear	-	-	10
23
Under review as a conference paper at ICLR 2022
Table 9: ResNet18 Architecture
Layer	Kernel	Stride	Channels
Input	-	-	3
Conv+BN+ReLU	3	1	64
Conv+BN+ReLU	-3-	-1-	64^
Conv+BN	3	1	64
Skip (Conv+BN+ReLU)	1	1	64
Conv+BN+ReLU	3	1	64
Conv+BN	3	1	64
Skip (Conv+BN+ReLU)	1	1	64
Conv+BN+ReLU	-3-	-2-	128-
Conv+BN	3	1	128
Skip (Conv+BN+ReLU)	1	2	128
Conv+BN+ReLU	3	1	128
Conv+BN	3	1	128
Skip (Conv+BN+ReLU)	1	2	128
Conv+BN+ReLU	-3-	-2-	256-
Conv+BN	3	1	256
Skip (Conv+BN+ReLU)	1	2	256
Conv+BN+ReLU	3	1	256
Conv+BN	3	1	256
Skip (Conv+BN+ReLU)	1	2	256
Conv+BN+ReLU	-3-	-2-	5Γ^
Conv+BN	3	1	512
Skip (Conv+BN+ReLU)	1	2	512
Conv+BN+ReLU	3	1	512
Conv+BN	3	1	512
SkiP (Conv+BN+ReLU)	1	2	512
AvgPooling	-4-	-4-	5Γ^
Linear	-	-	10
24