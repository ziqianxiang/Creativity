Under review as a conference paper at ICLR 2022
Learning Algebraic Representation for
Systematic Generalization in
Abstract Reasoning
Anonymous authors
Paper under double-blind review
Ab stract
Is intelligence realized by connectionist or classicist? While connectionist ap-
proaches have achieved superhuman performance, there has been growing evi-
dence that such task-specific superiority is particularly fragile in systematic gen-
eralization. This observation lies in the central debate (Fodor & McLaughlin,
1990; Fodor et al., 1988) between connectionist and classicist, wherein the lat-
ter continually advocates an algebraic treatment in cognitive architectures. In
this work, we follow the classicist’s call and propose a hybrid approach to im-
prove systematic generalization in reasoning. Specifically, we showcase a proto-
type with algebraic representation for the abstract spatial-temporal reasoning task
of Raven’s Progressive Matrices (RPM) and present the ALgebra-Aware Neuro-
Semi-Symbolic (ALANS) learner. The ALANS learner is motivated by abstract
algebra and the representation theory. It consists of a neural visual perception
frontend and an algebraic abstract reasoning backend: the frontend summarizes
the visual information from object-based representation, while the backend trans-
forms it into an algebraic structure and induces the hidden operator on the fly. The
induced operator is later executed to predict the answer’s representation, and the
choice most similar to the prediction is selected as the solution. Extensive ex-
periments show that by incorporating an algebraic treatment, the ALANS learner
outperforms various pure connectionist models in domains requiring systematic
generalization. We further show that the algebraic representation learned can be
decoded by isomorphism and used to generate an answer.
1	Introduction
“Thought is in fact a kind of Algebra.”
—William James (James, 1891)
Imagine you are given two alphabetical sequences of “c, b, a” and “d, c, b”, and asked to fill in
the missing element in “e, d, ?”. In nearly no time will one realize the answer to be c. However,
more surprising for human learning is that, effortlessly and instantaneously, we can “freely general-
ize” (Marcus, 2001) the solution to any partial consecutive ordered sequences. While believed to be
innate in early development for human infants (Marcus et al., 1999), such systematic generalizabil-
ity has constantly been missing and proven to be particularly challenging in existing connectionist
models (Bahdanau et al., 2019; Lake & Baroni, 2018). In fact, such an ability to entertain a given
thought and semantically related contents strongly implies an abstract algebra-like treatment (Fodor
et al., 1988); in literature, it is referred to as the “language of thought” (Fodor, 1975), “physical
symbol system” (Newell, 1980), and “algebraic mind” (Marcus, 2001). However, in stark contrast,
existing connectionist models tend only to capture statistical correlation (Chollet, 2019; Kansky
et al., 2017; Lake & Baroni, 2018), rather than providing any account for a structural inductive bias
where systematic algebra can be carried out to facilitate generalization.
This contrast instinctively raises a question—what constitutes such an algebraic inductive bias? We
argue that the foundation of the modeling counterpart to the algebraic treatment in early human
development (Marcus, 2001; Marcus et al., 1999) lies in algebraic computations set up on mathe-
matical axioms, a form of formalized human intuition and the starting point of modern mathematical
1
Under review as a conference paper at ICLR 2022
reasoning (Heath et al., 1956; Maddy, 1988). Of particular importance to the basic building blocks
of algebra is the Peano Axiom (Peano, 1889). In the Peano Axiom, the essential components of
algebra, the algebraic set and corresponding operators over it, are governed by three statements: (1)
the existence of at least one element in the field to study (“zero” element), (2) a successor func-
tion that is recursively applied to all elements and can, therefore, span the entire field, and (3) the
principle of mathematical induction. Building on such a fundamental axiom, we begin to form the
notion of an algebraic set and induce the operator along with it to construct an algebraic structure.
We hypothesize that such a treatment of algebraic computations set up on fundamental axioms is
essential for a model’s systematic generalizability, the lack of which will only make it sub-optimal.
To demonstrate the benefits of such an algebraic treatment in systematic generalization, we show-
case a prototype for Raven’s Progressive Matrices (RPM) (Raven, 1936; Raven & Court, 1998), an
exemplar task for abstract spatial-temporal reasoning (Santoro et al., 2018; Zhang et al., 2019a). In
this task, an agent is given an incomplete 3 X 3 matrix consisting of eight context panels with the last
one missing, and asked to pick one answer from a set of eight choices that best completes the matrix.
Human’s reasoning capability of solving this abstract reasoning task has been commonly regarded
as an indicator of “general intelligence” (Carpenter et al., 1990) and “fluid intelligence” (Hofs-
tadter, 1995; Jaeggi et al., 2008; Spearman, 1923; 1927). In spite of the task being one that ideally
requires abstraction, algebraization, induction, and generalization (Carpenter et al., 1990; Raven,
1936; Raven & Court, 1998), recent endeavors unanimously propose pure connectionist models that
attempt to circumvent such intrinsic cognitive requirements (Hu et al., 2020; Santoro et al., 2018;
Wang et al., 2020; Wu et al., 2020; Zhang et al., 2019a;b; Zheng et al., 2019). However, these meth-
ods’ inefficiency is also evident in systematic generalization; they struggle to extrapolate to domains
beyond training, as pointed out in (Santoro et al., 2018; Zhang et al., 2019b) and shown in this paper.
To address the issue, we introduce the ALgebra-Aware Neuro-Semi-Symbolic (ALANS) learner. At
a high-level, the ALANS learner is embedded in a general neuro-symbolic architecture (Han et al.,
2019; Mao et al., 2019; Yi et al., 2020; 2018) but has on-the-fly operator learnability and hence semi-
symbolic. Specifically, it consists of a neural visual perception frontend and an algebraic abstract
reasoning backend. For each RPM instance, the neural visual perception frontend first slides a
window over each panel to obtain the object-based representation (Kansky et al., 2017; Wu et al.,
2017) for every object. A belief inference engine latter aggregates all object-based representation
in each panel to produce the probabilistic belief state. The algebraic abstract reasoning backend
then takes the belief states of the eight context panels, treats them as snapshots on an algebraic
structure, lifts them into a matrix-based algebraic representation built on the Peano Axiom and the
representation theory (Humphreys, 2012), and induces the hidden operator in the algebraic structure
by solving an inner optimization (Bard, 2013; Colson et al., 2007). The algebraic representation for
the answer is predicted by executing the induced operator: its corresponding set element is decoded
by isomorphism established in the representation theory, and the final answer is selected as the one
most similar to the prediction.
The ALANS learner enjoys several benefits in abstract reasoning with an algebraic treatment:
1.	Unlike previous monolithic models, the ALANS learner offers a more interpretable account of
the entire abstract reasoning process: the neural visual perception frontend extracts object-based
representation and produces belief states of panels by explicit probability inference, whereas the
algebraic abstract reasoning backend induces the hidden operator in the algebraic structure. The
corresponding representation for the final answer is obtained by executing the induced opera-
tor, and the choice panel with minimum distance is selected. This process much resembles the
top-down bottom-up strategy in human reasoning: humans reason by inducing the hidden rela-
tion, executing it to generate a feasible solution in mind, and choosing the most similar answer
available (Carpenter et al., 1990). Such a strategy is missing in recent literature (Hu et al., 2020;
Santoro et al., 2018; Wang et al., 2020; Wu et al., 2020; Zhang et al., 2019a;b; Zheng et al., 2019).
2.	While keeping the semantic interpretability and end-to-end trainability in existing neuro-
symbolic frameworks (Han et al., 2019; Mao et al., 2019; Yi et al., 2020; 2018), ALANS is
what we call semi-symbolic in the sense that the symbolic operator can be learned and concluded
on the fly without manual definition for every one of them. Such an inductive ability also enables
a greater extent of the desired generalizability.
3.	By decoding the predicted representation in the algebraic structure, we can also generate an
answer that satisfies the hidden relation in the context.
2
Under review as a conference paper at ICLR 2022
This work makes three major contributions: (1) We propose the ALANS learner. Compared to
existing monolithic models, the ALANS learner adopts a neuro-semi-symbolic design, where the
problem-solving process is decomposed into neural visual perception and algebraic abstract rea-
soning. (2) To demonstrate the efficacy of incorporating an algebraic treatment in abstract spatial-
temporal reasoning, we show the superior systematic generalization ability of the proposed ALANS
learner in various extrapolatory RPM domains. (3) We present analyses into both neural visual
perception and algebraic abstract reasoning. We also show the generative potential of ALANS.
2	Related Work
2.1	Quest for Symbolized Manipulation
The idea to treat thinking as a mental language can be dated back to Augustine (Augustine, 1876;
Wittgenstein, 1953). Since the 1970s, this school of thought has undergone a dramatic revival as the
quest for symbolized manipulation in cognitive modeling, such as “language of thought” (Fodor,
1975), “physical symbol system” (Newell, 1980), and “algebraic mind” (Marcus, 2001). In their
study, connectionist’s task-specific superiority and inability to generalize beyond training (Chol-
let, 2019; Kansky et al., 2017; Santoro et al., 2018; Zhang et al., 2019a) have been hypotheti-
cally linked to a lack of such symbolized algebraic manipulation (Chollet, 2019; Lake & Baroni,
2018; Marcus, 2020). With evidence that an algebraic treatment adopted in early human develop-
ment (Marcus et al., 1999) can potentially address the issue (Bahdanau et al., 2019; Mao et al.,
2019; Marcus, 2020), classicist (Fodor et al., 1988) approaches for generalizable reasoning used
in programs (McCarthy, 1960) and blocks world (Winograd, 1971) have resurrected. As a hybrid
approach to bridge connectionist and classicist, recent developments lead to neuro-symbolic archi-
tectures. In particular, the community of theorem proving has been one of the earliest to endorse the
technique (Garcez et al., 2012; Rocktaschel & Riedel, 2017; Serafini & Garcez, 2016): BILP (Evans
& Grefenstette, 2018) and NLM (Dong et al., 2019) make inductive programming end-to-end and
DeepProbLog (Manhaeve et al., 2018) connects learning and reasoning, the latter of which is closely
related to our work, with its differentiable logic inference used in our belief inference engine. Re-
cently, Hudson & Manning (2019) propose NSM for visual question answering where a probabilistic
graph is used for reasoning. Yi et al. (2018) demonstrate a neuro-symbolic prototype for the same
task where a perception module and a language parsing module are separately trained, with the pre-
defined logic operators associated with language tokens chained to process the visual information.
Mao et al. (2019) soften the predefined operators to afford end-to-end training with only question
answers. Han et al. (2019) use the hybrid architecture for metaconcept learning. Yi et al. (2020)
and Chen et al. (2021) show how neuro-symbolic models can handle explanatory, predictive, and
counterfactual questions in temporal and causal reasoning. Lately, NeSS (Chen et al., 2020) exem-
plifies an algorithmic stack machine that can be used to improve generalization in language learning.
ALANS follows the classicist’s call but adopts a neuro-semi-symbolic architecture: it is end-to-end
trainable as opposed to Yi et al. (2020; 2018) and the operator can be learned and concluded on the
fly without manual specification.
2.2	Abstract Visual Reasoning
Recent works by Santoro et al. (2018) and Zhang et al. (2019a) arouse the community’s interest
in abstract visual reasoning, where the task of Raven’s Progressive Matrices (RPM) is introduced
as such a measure for intelligent agents. Initially proposed as an intelligence quotient test for hu-
mans (Raven, 1936; Raven & Court, 1998), RPM is believed to be strongly correlated with human’s
general intelligence (Carpenter et al., 1990) and fluid intelligence (Hofstadter, 1995; Jaeggi et al.,
2008; Spearman, 1923; 1927). Early RPM-solving systems employ symbolic representation based
on hand-designed features and assume access to the underlying logics (Carpenter et al., 1990; Lovett
& Forbus, 2017; Lovett et al., 2010; 2009). Another stream of research on RPM recruits similarity-
based metrics to select the most similar answer from the choices (Little et al., 2012; McGreggor &
Goel, 2014; McGreggor et al., 2014; Mekik et al., 2018; Shegheva & Goel, 2018). However, their
hand-defined visual features are unable to handle uncertainty from imperfect perception, and directly
assuming access to the logic operations simplifies the problem. Recently proposed data-driven ap-
proaches arise from the availability of large datasets: Santoro et al. (2018) extend a pedagogical
RPM generation method (Wang & Su, 2015), whereas Zhang et al. (2019a) use a stochastic image
3
Under review as a conference paper at ICLR 2022
" o~I	p o el~∙ 4
o	Io o <ι
o I e I o “ e ∙.
Neural Visual Perception
Ubject-CNN
(IIIl) Belief State
Operator Inductioa
Operator Executioi
[~~ZZ~~~~i Selected Attribute
S/H JS±S
-ɪ.
Pos ■ Num ■ Type Size Color
Operator Matrix Representations
α = Num
；Λf(¾ι)TM(¾a) →3op6
I U	U
;	M(%)	→ 9
；M(44)TM(%s) T8opl
!	。 H
；	Λf(⅞,β)	→ 9
:=⅛∙ 7⅛, —> op = operator(+)
二二二二二二二一
；M(¾7)7J,Λf(6-8)→ 4 + 2
I ɪ J
Underlying Algebra
Algebraic Abstract Reasoning
Figure 1: An overview of the ALANS learner. For an RPM instance, the neural visual perception
module produces the belief states for all panels: an object CNN extracts object attribute distribu-
tions for each image region, and a belief inference engine marginalizes them out to obtain panel
attribute distributions. For each panel attribute, the algebraic abstract reasoning module transforms
the belief states into matrix-based algebraic representation and induces hidden operators by solving
inner optimizations. The answer representation is obtained by executing the induced operators, and
the choice most similar to the prediction is selected as the solution. An example of the underlying
discrete algebra and its correspondence is also shown on the right.
grammar (Zhu et al., 2007) and introduce structural annotations in it, which Hu et al. (2020) further
refine to avoid shortcut solutions by statistics in candidate panels. Despite the fact that RPM intrin-
sically requires one to perform abstraction, algebraization, induction, and generalization, existing
methods bypass such cognitive requirements using a single feedforward pass in connectionist mod-
els: Santoro et al. (2018) use a relational module (Santoro et al., 2017), Steenbrugge et al. (2018)
augment it with a VAE (Kingma & Welling, 2013), Zhang et al. (2019a) assemble a dynamic tree,
Hill et al. (2019) arrange the data in a contrastive manner, Zhang et al. (2019b) propose a contrast
module, Zheng et al. (2019) formulate it in a student-teacher setting, Wang et al. (2020) build a mul-
tiplex graph network, Hu et al. (2020) aggregate features from a hierarchical decomposition, and Wu
et al. (2020) apply a scattering transformation to learn objects, attributes, and relations. Recently,
Zhang et al. (2021) employ a neuro-symbolic design but requires full knowledge over the hidden
relations to perform abduction. While our work adopts the visual perception module and employs
a similar training strategy from Zhang et al. (2021), the ALANS learner manages to induce the hid-
den relations without access to the ground-truth semantics of them, enabling the on-the-fly relation
induction and systematic generalization on relational learning.
3	The ALANS Learner
In this section, we introduce the ALANS learner for the RPM problem. In each RPM instance, an
agent is given an incomplete 3 X 3 panel matrix with the last entry missing and asked to induce the
operator hidden in the matrix and choose from eight choice panels one that follows it. Formally, let
the answer variable be denoted as y, the context panels as tIo,iui8“1, and choice panels as tIc,iui8“1.
Then the problem can be formulated as estimating P py | tIo,iui8“1, tIc,iui8“1q. According to the
common design (Carpenter et al., 1990; Santoro et al., 2018; Zhang et al., 2019a), there is one
operator that governs each panel attribute. Hence, by assuming independence among attributes, we
propose to factorize the probability of P py “ n| tIo,iui8“1,tIc,iui8“1qas
∏∑Ppy “ n ITa, tio,iU8“i, tic,i}Lιq^ P(Ta I tio,i}8=ι),	(1)
a Ta
where ya denotes the answer selection based only on attribute a and Ta the operator on a.
Overview As shown in Fig. 1, the ALANS learner decomposes the process into perception and
reasoning: the neural visual perception frontend is adopted from (Zhang et al., 2021) and extracts the
4
Under review as a conference paper at ICLR 2022
belief states from each of the sixteen panels, whereas the algebraic abstract reasoning backend views
an instance as an example in an abstract algebra structure, transforms belief states into algebraic
representation by the representation theory, induces the hidden operators, and executes the operators
to predict the representation of the answer. Therefore, in Eq. (1), the operator distribution is modeled
by the fitness of an operator and the answer distribution by the distance between the predicted
representation and that of a candidate.
3.1	Neural Visual Perception
We follow the design in Zhang et al. (2021) and decompose visual perception into an object CNN
and a belief state inference engine. Specifically, for each panel, we use a sliding window to traverse
the spatial domain of the image and feed each image region into an object CNN. The CNN has four
branches, producing for each region its object attribute distributions, including objectiveness (if the
region contains an object), type, size, and color. The belief inference engine summarizes the panel
attribute distributions (over position, number, type, size, and color) by marginalizing out all object
attribute distributions (over objectiveness, type, size, and color). As an example, the distribution of
the panel attribute of Number can be computed as such: for N image regions and their predicted
objectiveness
N
P(NUmber “ k) “	£	∩ Pprj = Ro),	(2)
RoPt0,1uN j “1
Ej Rj“k
where Pprjo) denotes the jth region’s estimated objectiveness distribUtion, and Ro is a binary se-
qUence of length N that sUms to k. All panel attribUte distribUtions compose the belief state of a
panel. In the following, we denote the belief state as b and the distribUtion of an attribUte a as Ppba).
For more details, please refer to Zhang et al. (2021) and Appendix.
3.2	Algebraic Abstract Reasoning
Given the belief states of both context and choice panels, the algebraic abstract reasoning backend
concerns the indUction of hidden operators and the prediction of answer representation for each
attribUte. The fitness of indUced operators is Used for estimating the operator distribUtion and the
difference between the prediction and the choice panel for estimating the answer distribUtion.
Algebraic Underpinning WithoUt loss of generality, here we assUme row-wise operators. For
each attribUte, Under perfect perception, the first two rows in an RPM instance provide snapshots
into an example of group (HaUsmann & Ore, 1937) constrained to an integer-indexed set, a simple
algebra strUctUre that is closed Under a binary operator. To see this, note that an accUrate perception
modUle woUld see each panel attribUte as a deterministic set element. Therefore, RPM instances
with Unary operators, sUch as progression, are groUp examples with special binary operators where
one operand is constant. Instances with binary operators, sUch as arithmetics, directly follow the
groUp properties. Those with ternary operators are ones defined on a three-tUple set from rows.
Algebraic Representation A systematic algebraic
view allows Us to felicitoUsly recrUit ideas in the repre-
sentation theory (HUmphreys, 2012) to glean the hidden
properties in the abstract strUctUres: it makes abstract al-
gebra amenable by redUcing it onto linear algebra. Fol-
lowing the same spirit, we propose to lift both the set
elements and the hidden operators to a learnable matrix
space. To encode the set element, we employ the Peano
Axiom (Peano, 1889). According to the Peano Axiom,
an integer-indexed set can be constrUcted by (1) a zero
element (0), (2) a successor function (S(∙)), and (3) the
principle of mathematical indUction, sUch that the kth el-
ement is encoded as Sk (0). Specifically, we instantiate
the zero element as a learnable matrix M0 and the suc-
T^
M (ba,i+ι)	M (bO,i+2)
M(∙) M-1(∙)
M(∙) M-1(∙)
Figure 2: Isomorphism between the ab-
stract algebra and the matrix-based rep-
resentation. Operator induction is now
reduced to solving for a matrix.
cessor function as the matrix-matrix product parameterized by M . In an attribute-specific manner,
5
Under review as a conference paper at ICLR 2022
the representation ofan attribute taking the kth value is pMaqkM0a. For operators, we consider them
to live in a learnable matrix group of a corresponding dimension, such that the action of an operator
on a set can be represented as matrix multiplication. Such algebraic representation establishes an
isomorphism between the matrix space and the abstract algebraic structure: abstract elements on
the algebraic structure have a bijective mapping to/from the matrix space, and inducing the abstract
relation can be reduced to solving for a matrix operator. See Fig. 2 for a graphical illustration of the
isomorphism.
Operator Induction Operator induction concerns about finding a concrete operator in the ab-
stract algebraic structure. By the property of closure, we formulate it as an inner-level regular-
ized linear regression problem: a binary operator Tba in a group example for attribute a minimizes
arg minT `ba pTq defined as
'apτ)“ ∑ E[}M(ba,i)τ m p^a,i`i) ´ m p^o^F ‰ + λ mF,	⑶
i
where under visual uncertainty, we take the expectation with respect to the distributions in the belief
states of context panels P (boa,iq in the first two rows, and denote its algebraic representation as
M (boa,i q. For unary operators, one operand can be treated as constant and absorbed into T. Note
that Eq. (3) admits a closed-form solution (see Appendix for details). Therefore, the operator can
be learned and adapted for different instances of binary relations and concluded on the fly. Such a
design also simplifies the recent neuro-symbolic approaches, where every single symbol operator
needs to be hand-defined (Han et al., 2019; Mao et al., 2019; Yi et al., 2020; 2018). Instead, we only
specify an inner-level optimization framework and allow symbolic operators to be quickly induced
based on the neural observations, while keeping the semantic interpretability in the neuro-symbolic
methods. Therefore, we term such a design semi-symbolic.
The operator probability in Eq. (1) is then modeled by each operator type’s fitness, e.g., for binary,
p (τa = τa | {io,i}8=ι) 9 exp(—，a(Ta)q.	(4)
Operator Execution To predict the algebraic representation of the answer, we solve another inner-
level optimization similar to Eq. (3), but now treating the representation of the answer as a variable:
y = arg min 优(〃)=旧[加囹,7)7；。〃囹,8)´ M}FS,	⑸
M
where the expectation is taken with respect to context panels in the last row. The optimization also
admits a closed-form solution (see Appendix for details), which corresponds to the execution of the
induced operator in Eq. (3).
The predicted representation is decoded probabilistically as the predicted belief state of the solution,
P(ba = k | Ta)9 exp(Tya — (Ma)kMa}F).	(6)
Answer Selection Based on Eqs. (1) and (4), estimating the answer distribution is now boiled
down to estimating the conditional answer distributions for each attribute. Here, we propose to
model it based on the Jensen-Shannon Divergence (JSD) of the predicted belief state and that of a
choice,
P(ya = n I Ta, tI0,iUL1, tIc,iULι) 9 exp(´dn),	(7)
where we define dan as
dan = DJSD(P(bpa | Ta)}P(bca,n))).	(8)
We also note that by decoding the predicted representation in Eq. (6), a solution can be generated: by
sequentially selecting the most probable operator and the most probable attribute value, a rendering
engine can directly render the solution. The reasoning backend also enables end-to-end training:
by integrating the belief states from neural perception, the module conducts both induction and
execution in a soft manner, such that the gradients can be back-propagated and both the visual
frontend and the reasoning backend jointly trained.
6
Under review as a conference paper at ICLR 2022
3.3	Training S trategy
We train the entire ALANS learner by minimizing the cross-entropy loss between the estimated
answer distribution and the ground-truth selection and an auxiliary loss (Santoro et al., 2018; Wang
et al., 2020; Zhang et al., 2019a; 2021) that shapes the operator distribution from the reasoning
engine, i.e.,
'PPPy | tio,i}8=ι,{ic,i}8=ι),y*)+χλa'(ppτa | 亿拱曰)"),
a
(9)
min
θ,tM0au,tMau
where '(∙) denotes the cross-entropy loss, y< the correct choice in candidates, and y< the ground-
truth operator selection for attribute a, respectively. The first part of the loss encourages the model
to select the right choice for evaluation, while the second part motivates meaningful internal rep-
resentation to emerge. Compared to Zhang et al. (2021), the system requires joint operation from
not only a trained perception module θ, but also the algebraic encodings from the zero elements
and the successor function tM0au and tMau, and correspondingly, the induced operator T. We
notice the three-stage curriculum in Zhang et al. (2021) is crucial for such a neuro-semi-symbolic
system. In particular, we use λa to balance the trade-off in the curriculum: in the first stage, we only
train parameters regarding objectiveness; in the second stage, we freeze objectiveness parameters
and cyclically train parameters involving type, size, and color; in the last stage, we fine-tune all
parameters.
4	Experiments
A cognitive architecture with systematic generalization is believed to demonstrate the following
three principles (Fodor et al., 1988; Marcus, 2001; 2020): (1) systematicity, (2) productivity, and
(3) localism. Systematicity requires an architecture to be able to entertain “semantically related”
contents after understanding a given thought. Productivity states the awareness of a constituent
implies that of a recursive application of the constituent; vice versa for localism.
To verify the effectiveness of an algebraic treatment in systematic generalization, we showcase the
superiority of the proposed ALANS learner on the three principles in the abstract spatial-temporal
reasoning task of RPM. Specifically, we use the generation methods proposed in Zhang et al. (2019a)
and Hu et al. (2020) to generate RPM problems and carefully split training and testing to construct
the three regimes. The former generates candidates by perturbing only one attribute of the correct
answer while the later modifies attribute values in a hierarchical manner to avoid shortcut solutions
by pure statistics. Both methods categorize relations in RPM into three types, according to Carpenter
et al. (1990): unary (Constant and Progression), binary (Arithmetic), and ternary (Distribution of
Three), each of which comes with several instances. Grounding the principles into learning abstract
relations in RPM, We fix the configuration to be 3 X 3Grid and generate the following data splits for
evaluation (see Appendix for details and in-distribution results):
•	Systematicity: the training set contains only a subset of instances for each type of relation, while
the test set all other relation instances.
•	Productivity: as the binary relation results from a recursive application of the unary relation, the
training set contains only unary relations, whereas the test set only binary relations.
•	Localism: the training and testing sets in the productivity split are swapped to study localism.
We follow Zhang et al. (2019a) to generate 10, 000 instances for each split.
4.1	Experimental Setup
We evaluate the systematic generalizability of the proposed ALANS learner on the above three splits,
and compare the ALANS learner with other baselines, including ResNet, ResNet+DRT (Zhang et al.,
2019a), WReN (Santoro et al., 2018), CoPINet (Zhang et al., 2019b), MXGNet (Wang et al., 2020),
LEN (Zheng et al., 2019), HriNet (Hu et al., 2020), and SCL (Wu et al., 2020). We use either
official or public implementations that reproduce the original results. All models are implemented
in PyTorch (Paszke et al., 2017) and optimized using ADAM (Kingma & Ba, 2014) on an Nvidia
Titan Xp GPU. We validate trained models on validation sets and report performance on test sets.
7
Under review as a conference paper at ICLR 2022
Table 1: Model performance on different aspects of systematic generalization. The performance is
measured by accuracy on the test sets. Upper: results on datasets generated by Zhang et al. (2019a).
Lower: results on datasets generated by HU et al. (2020).
Method	MXGNet	ResNet+DRT	ResNet	HriNet	LEN	WReN	SCL	CoPINet	ALANS	ALANS-Ind	ALANS-GT
Systematicity	20.95%	33.00%	27.35%	28.05%	40.15%	35.20%	37.35%	59.30%	78.45%	52.70%	93.85%
Productivity	30.40%	27.95%	27.05%	31.45%	42.30%	56.95%	51.10%	60.00%	79.95%	36.45%	90.20%
Localism	28.80%	24.90%	23.05%	29.70%	39.65%	38.70%	47.75%	60.10%	80.50%	59.80%	95.30%
Average	26.72%	28.62%	25.82%	29.73%	40.70%	43.62%	45.40%	59.80%	79.63%	48.65%	93.12%
Systematicity	13.35%	13.50%	14.20%	21.00%	17.40%	15.00%	24.90%	18.35%	64.80%	52.80%	84.85%
Productivity	14.10%	16.10%	20.70%	20.35%	19.70%	17.95%	22.20%	29.10%	65.55%	32.10%	86.55%
Localism	15.80%	13.85%	17.45%	24.60%	20.15%	19.70%	29.95%	31.85%	65.90%	50.70%	90.95%
Average	14.42%	14.48%	17.45%	21.98%	19.08%	17.55%	25.68%	26.43%	65.42%	45.20%	87.45%
Table 2: Perception accuracy of the proposed ALANS learner, measured by whether the module can
correctly predict an attribute’s value. Left: results on datasets generated by Zhang et al. (2019a).
Right: results on datasets generated by Hu et al. (2020).
Object Attribute	Objectiveness	Type	Size	Color	Object Attribute	Objectiveness	Type	Size	Color
Systematicity	100.00%	99.95%	94.65%	71.35%	Systematicity	100.00%	96.34%	92.36%	63.98%
Productivity	100.00%	99.97%	98.04%	77.61%	Productivity	100.00%	94.28%	97.00%	69.89%
Localism	100.00%	95.65%	98.56%	80.05%	Localism	100.00%	95.80%	98.36%	60.35%
Average	100.00%	98.52%	97.08%	76.34%	Average	100.00%	95.47%	95.91%	64.74%
Table 3: Reasoning accuracy of the proposed ALANS learner, measured by whether the module can
correctly predict the type of a relation on an attribute. Left: results on datasets generated by Zhang
et al.(2019a). Right: results on datasets generated by Hu et al. (2020).
Relation on	Position	Number	Type	Size	Color	Relation on	Position	Number	Type	Size	Color
Systematicity	72.04%	82.14%	81.50%	80.80%	40.40%	Systematicity	69.96%	80.34%	83.50%	80.85%	28.85%
Productivity	-	98.75%	89.50%	72.10%	33.95%	Productivity	-	99.10%	87.95%	68.50%	23.10%
Localism	-	74.70%	44.25%	56.40%	54.20%	Localism	-	70.55%	36.65%	42.30%	33.20%
Average	72.04%	85.20%	71.75%	69.77%	42.85%	Average	69.96%	83.33%	69.37%	63.88%	28.38%
4.2	Systematic Generalization
Table 1 shows the performance of various models on systematic generalization, i.e., systematicity,
productivity, and localism. Compared to results reported in (Hu et al., 2020; Santoro et al., 2018;
Wang et al., 2020; Wu et al., 2020; Zhang et al., 2019a;b; Zheng et al., 2019), all pure connectionist
models experience a devastating performance drop when it comes to the critical cognitive require-
ments on systematic generalization, indicating that pure connectionist models fail to perform ab-
straction, algebraization, induction, or generalization needed in solving the abstract reasoning task;
instead, they seem to only take a shortcut to bypass them. In particular, MXGNet’s (Wang et al.,
2020) superiority is diminishing in systematic generalization. In spite of learning with structural an-
notations, ResNet+DRT (Zhang et al., 2019a) does not fare better than its base model. The recently
proposed HriNet (Hu et al., 2020) slightly improves on ResNet in this aspect, with LEN (Zheng
et al., 2019) being only marginally better. WReN (Santoro et al., 2018), on the other hand, shows
oscillating performance across the three regimes. Evaluated under systematic generation, SCL (Wu
et al., 2020) and CoPINet (Zhang et al., 2019b) also far deviate from their “superior performance”.
These observations suggest that pure connectionist models highly likely learn from variation in vi-
sual appearance rather than the algebra underlying the problem.
Embedded in a neural-semi-symbolic framework, the proposed ALANS learner improves on sys-
tematic generalization by a large margin. With an algebra-aware design, the model is considerably
stable across different principles of systematic generalization. The algebraic representation learned
in relations of either a constituent or a recursive composition naturally supports productivity and
localism, while semi-symbolic inner optimization further allows various instances of an operator
type to be induced from the algebraic representation and boosts systematicity. The importance of
the algebraic representation is made more significant in the ablation study: ALANS-Ind, with al-
gebraic representation replaced by independent encodings and the algebraic isomorphism broken,
shows inferior performance. We also examine the model’s performance under perfect perception
(denoted as ALANS-GT) to see how the proposed algebraic reasoning module works: the gap from
perfection indicates space for improvement for the inductive reasoning part of the model. In the next
section, we further show that the neuro-semi-symbolic decomposition in ALANS’s design enables
diagnostic tests into its jointly learned perception module and reasoning module. This design is in
start contrast to black-box models.
8
Under review as a conference paper at ICLR 2022
Position : N/A
Number : Progression
Type : Distribute Three
Size : Distribute Three
Color
:Distribute Three
CJO		4 4		O ♦
OO□	OOO	4	V ›	
。口口		4	△ △	∙□
3Δ4
<Δ>
4>
Ooo
Oo
▽ △ ‹
Ooq
O O
Ooo
Ooo
4 △ A
▲>▲
4 <
O O
Ooo
∙:
odd
Oo
Position : Distribute Three
Number : N/A
Type : Distribute Three
Size : Progression
Color : Progression
jOOO∣∣A A‹∣∣4 ▲ ^ι∣∙ ⅜ ∙
POO <4 ▼ a
XXXX
XX ✓ X
B
・ ♦
Figure 3: Examples of RPM instances With the missing entries filled by solutions generated by the
ALANS learner. Ground-truth relations are also listed. Note the generated results do not look exactly
like the correct choices due to random rotations during rendering, but are semantically correct.
4.3	Analysis INTO Perception and Reasoning
The neural-semi-symbolic design allows analyses into both perception and reasoning. To evaluate
the neural perception module and the algebraic reasoning module, we extract region-based object
attribute annotations from the dataset generation methods (HU et al., 2020; Zhang et al., 2019a) and
categorize all relations into three types, i.e., unary, binary, and ternary, respectively.
Table 2 shows the perception module’s performance on the test sets in the three regimes of systematic
generalization. We note that in order for the ALANs learner to achieve the desired results shown in
Table 1, ALANs learns to construct the concept of objectiveness perfectly. The model also shows
fairly accurate prediction on the attributes of type and size. However, on the texture-related concept
of color, ALANs fails to develop a reliable notion on it. Despite that, the general prediction accuracy
of the perception module is still surprising, considering that the perception module is jointly learned
with ground-truth annotations on answer selections. The relatively lower accuracy on color could
be attributed to its larger space compared to other attributes.
Table 3 lists the reasoning module’s performance during testing for the three aspects. Note that on
position, the unary operator (shifting) and binary operator (set arithmetics) do not systematically
imply each other. Hence, we do not count them as probes into productivity and localism. in general,
we notice that the better the perception accuracy on one attribute, the better the performance on
reasoning. However, we also note that despite the relatively accurate perception of objectiveness,
type, and size, near perfect reasoning is never guaranteed. This deficiency is due to the percep-
tion uncertainty handled by expectation in Eq. (3): in spite of correctness when we take arg max,
marginalizing by expectation will unavoidably introduce noise into the reasoning process. There-
fore, an ideal reasoning module requires the perception frontend to be not only correct but also
certain. Computationally, one can sample from the perception module and optimize Eq. (9) using
REiNFoRCE (Williams, 1992). However, the credit assignment problem and variance in gradient
estimation will further complicate training.
4.4	Generative Potential
Compared to existing discriminative-only RPM-solving methods, the proposed ALANs learner is
unique in its generative potential. As mentioned above, the final panel attribute can be decoded by
sequentially selecting the most probable hidden operator and the attribute value. When equipped
with a rendering engine, a solution can be generated. in Fig. 3, we use the rendering program from
Zhang et al. (2019a) to demonstrate such a generative potential in the proposed ALANs learner.
5	Conclusion
in this work, we propose the ALgebra-Aware Neuro-semi-symbolic (ALANs) learner, echoing a
normative theory in the connectionist-classicist debate that an algebraic treatment in a cognitive ar-
chitecture should improve a model’s systematic generalization ability. Compared to earlier works,
the ALANs learner employs a neural-semi-symbolic architecture, where the algebraic abstract rea-
soning module transforms visual information into algebraic representation with isomorphism estab-
lished by the Peano Axiom and the representation theory, conducts operator induction, and executes
it to arrive at an answer. in three RPM domains reflective of systematic generalization, the proposed
ALANs learner shows superior performance compared to other pure connectionist baselines.
9
Under review as a conference paper at ICLR 2022
References
Saint Augustine. The confessions. Clark, 1876.
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries,
and Aaron Courville. Systematic generalization: What is required and can it be learned? In
International Conference on Learning Representations (ICLR), 2019.
Jonathan F Bard. Practical bilevel optimization: algorithms and applications, volume 30. Springer
Science & Business Media, 2013.
Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theo-
retical account of the processing in the raven progressive matrices test. Psychological Review, 97
(3):404, 1990.
Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional general-
ization via neural-symbolic stack machines. arXiv preprint arXiv:2008.06662, 2020.
Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and
Chuang Gan. Grounding physical concepts of objects and events through dynamic visual reason-
ing. arXiv preprint arXiv:2103.16564, 2021.
Francois Chollet. The measure of intelligence. arXiv preprint arXiv:191L01547, 2019.
Beno^t Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization. Annals of
operations research,153(1):235-256, 2007.
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic
machines. arXiv preprint arXiv:1904.11694, 2019.
Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
Artificial Intelligence Research, 61:1-64, 2018.
Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why
smolensky’s solution doesn’t work. Cognition, 35(2):183-204, 1990.
Jerry A Fodor. The language of thought, volume 5. Harvard university press, 1975.
Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical
analysis. Cognition, 28(1-2):3-71, 1988.
Artur S d’Avila Garcez, Krysia B Broda, and Dov M Gabbay. Neural-symbolic learning systems:
foundations and applications. Springer Science & Business Media, 2012.
Chi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept
learning. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2019.
Bernard A Hausmann and Oystein Ore. Theory of quasi-groups. American Journal of Mathematics,
59(4):983-1004, 1937.
Thomas Little Heath et al. The thirteen books of Euclid’s Elements. Courier Corporation, 1956.
Felix Hill, Adam Santoro, David GT Barrett, Ari S Morcos, and Timothy Lillicrap. Learning to make
analogies by contrasting abstract relational structure. In International Conference on Learning
Representations (ICLR), 2019.
Douglas R Hofstadter. Fluid concepts and creative analogies: Computer models of the fundamental
mechanisms of thought. Basic books, 1995.
Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. Hierarchical rule induction
network for abstract visual reasoning. arXiv preprint arXiv:2002.06838, 2020.
Drew A Hudson and Christopher D Manning. Learning by abstraction: The neural state machine.
arXiv preprint arXiv:1907.03950, 2019.
10
Under review as a conference paper at ICLR 2022
James E Humphreys. Introduction to Lie algebras and representation theory, volume 9. Springer
Science & Business Media, 2012.
Susanne M Jaeggi, Martin Buschkuehl, John Jonides, and Walter J Perrig. Improving fluid intelli-
gence with training on working memory. Proceedings of the National Academy of Sciences, 105
(19):6829-6833, 2008.
William James. The Principles of Psychology. Henry Holt and Company, 1891.
Ken Kansky, Tom Silver, David A Mely, Mohamed Eldawy, MigUel Lazaro-Gredilla, XinghUa Lou,
Nimrod Dorfman, Szymon Sidor, Scott Phoenix, and Dileep George. Schema networks: Zero-
shot transfer with a generative caUsal model of intUitive physics. In Proceedings of International
Conference on Machine Learning (ICML), 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2014.
Diederik P Kingma and Max Welling. AUto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Brenden Lake and Marco Baroni. Generalization withoUt systematicity: On the compositional skills
of seqUence-to-seqUence recUrrent networks. In Proceedings of International Conference on Ma-
chine Learning (ICML), 2018.
Peter Lancaster. Explicit solUtions of linear matrix eqUations. SIAM review, 12(4):544-566, 1970.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to docUment recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Daniel R Little, Stephan Lewandowsky, and Thomas L Griffiths. A bayesian model of rule induction
in raven’s progressive matrices. In Proceedings of the Annual Meeting of the Cognitive Science
Society (CogSci), 2012.
Andrew Lovett and Kenneth Forbus. Modeling visual problem solving as analogical reasoning.
Psychological Review, 124(1):60, 2017.
Andrew Lovett, Emmett Tomai, Kenneth Forbus, and Jeffrey Usher. Solving geometric analogy
problems through two-stage analogical mapping. Cognitive Science, 33(7):1192-1231, 2009.
Andrew Lovett, Kenneth Forbus, and Jeffrey Usher. A structure-mapping model of raven’s progres-
sive matrices. In Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci),
2010.
Penelope Maddy. Believing the axioms. i. The Journal of Symbolic Logic, 53(2):481-511, 1988.
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.
Deepproblog: Neural probabilistic logic programming. In Advances in Neural Information Pro-
cessing Systems, pp. 3749-3759, 2018.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-
symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In
International Conference on Learning Representations (ICLR), 2019.
Gary Marcus. The algebraic mind. Cambridge, MA: MIT Press, 2001.
Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint
arXiv:2002.06177, 2020.
Gary F Marcus, Sugumaran Vijayan, S Bandi Rao, and Peter M Vishton. Rule learning by seven-
month-old infants. Science, 283(5398):77-80, 1999.
John McCarthy. Programs with common sense. RLE and MIT computation center, 1960.
Keith McGreggor and Ashok Goel. Confident reasoning on raven’s progressive matrices tests. In
Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2014.
11
Under review as a conference paper at ICLR 2022
Keith McGreggor, Maithilee Kunda, and Ashok Goel. Fractals and ravens. Artificial Intelligence,
215:1-23, 2014.
Can Serif Mekik, Ron Sun, and David Yun Dai. Similarity-based reasoning, raven’s matrices, and
general intelligence. In Proceedings of International Joint Conference on Artificial Intelligence
(IJCAI), 2018.
Allen Newell. Physical symbol systems. Cognitive science, 4(2):135-183, 1980.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop, 2017.
Giuseppe Peano. Arithmetices principia: Nova methodo exposita. Fratres Bocca, 1889.
James C Raven. Mental tests used in genetic studies: The performance of related individuals on
tests mainly educative and mainly reproductive. Master’s thesis, University of London, 1936.
John C Raven and John Hugh Court. Raven’s progressive matrices and vocabulary scales. Oxford
pyschologists Press, 1998.
Tim Rocktaschel and Sebastian Riedel. End-to-end differentiable proving. In Advances in Neural
Information Processing Systems, pp. 3788-3800, 2017.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In
Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2017.
Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. Measuring abstract
reasoning in neural networks. In Proceedings of International Conference on Machine Learning
(ICML), 2018.
Luciano Serafini and Artur d’Avila Garcez. Logic tensor networks: Deep learning and logical
reasoning from data and knowledge. arXiv preprint arXiv:1606.04422, 2016.
Snejana Shegheva and Ashok Goel. The structural affinity method for solving the raven’s progres-
sive matrices test for intelligence. In Proceedings of AAAI Conference on Artificial Intelligence
(AAAI), 2018.
Charles Spearman. The nature of “intelligence” and the principles of cognition. Macmillan, 1923.
Charles Spearman. The abilities of man, volume 6. Macmillan New York, 1927.
Xander Steenbrugge, Sam Leroux, Tim Verbelen, and Bart Dhoedt. Improving generaliza-
tion for abstract reasoning tasks using disentangled feature representations. arXiv preprint
arXiv:1811.04784, 2018.
Duo Wang, Mateja Jamnik, and Pietro Lio. Abstract diagrammatic reasoning with multiplex graph
networks. In International Conference on Learning Representations (ICLR), 2020.
Ke Wang and Zhendong Su. Automatic generation of raven’s progressive matrices. In Proceedings
of International Joint Conference on Artificial Intelligence (IJCAI), 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Terry Winograd. Procedures as a representation for data in a computer program for understanding
natural language. Technical report, MIT. Cent. Space Res., 1971.
Ludwig Wittgenstein. Philosophical investigations. Philosophische Untersuchungen. Macmillan,
1953.
Jiajun Wu, Joshua B Tenenbaum, and Pushmeet Kohli. Neural scene de-rendering. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.
12
Under review as a conference paper at ICLR 2022
Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba. The scattering compositional
learner: Discovering objects, attributes, relationships in analogical reasoning. arXiv preprint
arXiv:2007.04212, 2020.
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-
symbolic vqa: Disentangling reasoning from vision and language understanding. In Proceedings
of Advances in Neural Information Processing Systems (NeurIPS), 2018.
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua
Tenenbaum. Clevrer: Collision events for video representation and reasoning. In International
Conference on Learning Representations (ICLR), 2020.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational
and analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019a.
Chi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu, Hongjing Lu, and Song-Chun Zhu. Learning per-
ceptual inference by contrasting. In Proceedings of Advances in Neural Information Processing
Systems (NeurIPS), 2019b.
Chi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Abstract spatial-temporal reasoning
via probabilistic abduction and execution. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2021.
Kecheng Zheng, Zheng-Jun Zha, and Wei Wei. Abstract reasoning with distracting features. In
Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2019.
Song-Chun Zhu, David Mumford, et al. A stochastic grammar of images. Foundations and Trends®
in Computer Graphics and Vision, 2(4):259-362, 2007.
13
Under review as a conference paper at ICLR 2022
A Inducing and Executing Operators
In the main text, we exemplify the induction and the execution process using a binary operator. Here,
we discuss other details regarding the formulation for all three types of operators, i.e., unary, binary,
and ternary.
Unary Operator To induce the unary operator Tua for an attribute a, we solve the following opti-
mization problem
	Tua =	arg min 'U(T), T	(S1)
where	'U(T) = 1{5 ^ (E I	MbaJ)T ´ M(b" 1 +	
	E”	Mba,2)T ´ Me" 1 +	
	EI	>M(bO,4)T ´ M"F 1 +	(S2)
	E	>m (ba,5)T ´ m (ba,6)>F 1 +	
	EI	>m (ba,7)T ´ m (ba,8)>F 1)+	
λau }T}2F .
The indexing follows the row / column major. By taking the derivative with respect to T and setting
it to be 0, we have the following solution,
Tu = ATB	(S3)
where, assuming independence,
A =E [M(ba,ι)TM(ba,ι)‰ + E “MPb≡,2)T”(匕^)] +
E [M(ba,4)TM(bθ,iq] + E [M(ba,5qTM(ba,5)‰ +	(S4)
E [M(ba,7)TM"7)] + 5λUI
and
B =E “M(boa,1qT] E “M(boa,2q] +
E “M(boa,2qT] E “M(boa,3q] +
E “M(boa,4qT] E “M(boa,5q] +	(S5)
E “M(boa,5qT] E “M(boa,6q] +
E “M(boa,7qT] E “M(boa,8q] .
Note that as long as λU > 0, A is a symmetric positive definite matrix and hence is invertible.
Compared to the binary case, the unary operator can be regarded as a special binary operator where
one of the operand is a constant, absorbed into operator learning, and jointly solved.
To predict the answer representation, we solve another optimization problem, i.e.,
My = argmn线(M) = E ”>〃稣8)资 ´ M>F] .	(S6)
Taking its derivative and setting it to 0, we have
Myua = E“M(boa,8q]Tua.	(S7)
Note that this is exactly the execution of the learned operator.
Binary Operator The optimization problem for the binary case can be formulated as
Tba = argmin 以(丁 )，
T
(S8)
1
Under review as a conference paper at ICLR 2022
where
'式Tq = 1/2 ^ (E ∣>MPba,ι)TMM,2)´ MPba,3)>F] `
E ”>MPboMMPbo5 ´ M(ba,6)>Fj)+	(S9)
λba }T}2F .
We note that, assuming independence, the solution satisfies
E “M(bo,ιqTMpba,ι)‰ TE “M(bo,2)M(bo,2)T] +
E “M (bo,4 qT M(bo,4)‰ TE “M pbo,5qM pbo,5qτ ‰ + 2λoτ
“E “M(bo,ιqτ‰ E [M(bo,3)‰ E “M(bo,2)T‰ +
E “M(bo,4qT‰ E [M(bo,6)‰ E “M(bo,5)T].
This is a linear matrix equation and can be turned into a linear equation by vectorization. Using
vec(ATBq “ A b B vec(T q (Lancaster, 1970), where b denotes the Kronecker product, we have
Vec(Taq = A-1B,
(S11)
where
A “E “M(^o,ιqτMpba,ι)‰ b E “M(bo,2)〃”2)口 +
E [M(bo,4)τM(bo,4q‰ b E “M(bo,5qM(bo,5qτ‰ +	(S12)
2λboI
and
B “vec 'E “M(bo,ιqτ‰ E “M(bo,3)] E [M(bo,2)τ‰) +
vec (E [M(bo,4)τ‰ E [M(bo,6)‰ E “M(bo,5qτ‰).
(S13)
Note that A is also symmetric positive definite given positive λbo and hence invertible.
The predicted answer representation is given by
y “ argMmin'o(M) “ E [>M(b^boM(bo^ ´ M>F] ,	(S14)
which can be solved by executing the induced binary operator Mb “ E “M(bo：)] TbaE “M(bo,8)].
Ternary Operator A ternary operation can be regarded as a unary operation on elements defined
on rows / columns. Specifically, we propose to construct the algebraic representation of a row /
column by concatenating the algebraic representation of each panel in it, i.e.,
M(bo,i,bo,i'i,bo,i'2q “ [M(bo,iq; M(bo,i'iq; M(bo,i'2qs.
Then the ternary operator can be solved by
Ta “ argmin'o(Tq,
T
where
'o(τ q“E [>M (bo,1,bo,2,bo,3qτ ´ M (bo,4, bo,5,bo,6q>F] +
λto }T}2F .
Similar to the unary case discussed above,
τto “ ATB
where
A “ E “M(bo,ι,bo,2,bo,3qτMpbaj,bo,2, bo,3q‰ + λoI
and
B “ E “M(bo,ι, bo,2, bo,3qτ‰ E “M(bo,4, bo,5, bo"].
(S15)
(S16)
(S17)
(S18)
(S19)
(S20)
2
Under review as a conference paper at ICLR 2022
Correspondingly, the answer representation can be obtained by first executing the ternary operator
E “M pboa,4, boa,5, boa,6q‰ Tta and slicing it from the result.
To compute the operator distribution, we model it based on the fitness of each operator type,
PpTa “	T	i tio,iULι)	9	exp(—'u(Ta qq	(S2i)
P(Ta “	Ta	∣{Io,i}8=ι)	9	exp(一'a(Ta))	(S22)
P(Ta “	Tta	∣{Io,i}8=ι)	9	exp(一'a(Tta)).	(S23)
B Systematic S plits and In-Distribution Results
In the original work of Zhang et al. (2019a) and Hu et al. (2020), there are four operators: Constant,
Progression, Arithmetic, and Distribute of Three. Progression is parameterized by its step size
(±1/2). Arithmetic includes addition and subtraction. And Distribute of Three is implemented as
shifting and can be either a left shift or a right one. Note that Constant can be regarded as special
Progression with a step size of 0. In this work, we group all four operators into three types: unary
(Constant and Progression), binary (Arithmetic), and ternary (Distribute of Three).
To study systematic generalization in abstract relation learning, we use the RPM generation method
proposed in Zhang et al. (2019a) and Hu et al. (2020) and carefully split data into three regimes:
•	Systematicity: The training set and the test set contain all three types of operators but disjoint
instances. Specifically, the training set has Constant, Progression of ±1, addition in Arithmetic,
and left shift in Distribute of Three, while in the test set there are Progression of ±2, subtraction
in Arithmetic, and right shift in Distribute of Three.
•	Productivity: The training set contains only unary operators and the test set only binary operators.
Specifically, the training set has Constant and all instances of Progression, while the test set all
instances of Arithmetic.
•	Localism: The training set contains only binary operators and the test set only unary operators.
Specifically, the training set has all instances of Arithmetic and the test set Constant and all in-
stances of Progression.
Please see Figs. S1 to S3 for examples in the three splits.
o	o	□ □ ■ ■	OOO O O OOO
■ ■ ■	δ^o OOO	・ ∙ ∙ OOO OOO
	。。 0 0 。。	Q
□	∣Γi	Ila ■ b∣∣b b h ■ ■ ■ ■ ■ ■ □	■			
734
□ □ □	□ □ □	□	■
□ □ □	O □ O		
□ □ □	O □ □	□	■	
5	6	7	8
□ □	>	>	O
□ □		> >	
□ □	>	>	Q
OOO		□ □	
OO	□	□	►
oeo	□	□_		►
►	O7	O	
>>	O		Q
> >>	O	Q	
一	O		1∏
	O	IIq	i 。
12	3	4
5	6	7	8
Figure S1:	A training example (left) and a test example (right) in the systematicity split. Note that
in the training example, the arithmetic relation (in number) is addition and the shifting is always a
left shift (in type, size, and color). In the test example, the shifting becomes a right shift (in type),
the size progression has a step of 2, and the color arithmetic becomes subtraction.
3
Under review as a conference paper at ICLR 2022
OOO		OOO		♦ ♦ ♦	
4 4 4 4	4		4 4 4 4	4		4 4 4	
OOO		oσ∙		Q	
< < < <	OOO ■ ■		VVy		▼ <
1	2	3	4					
・ ∙ ∙ o	OOO ■ 	■		・ ∙ ∙ O O		V q
5	6	7	8
OQ	。V W V > >	，，，
* ∙ O O O ・ ∙		a a 4 4 b 4 4 4公
・ ∙ ∙ ・ ∙	Δ Δ	Q
▲ ▲ AllA ▲ aIIAAAII*** djHFιdki:		
ɪ
ɪ
ɪ
1 ^^
;▲▲▲I
・ ∙ ∙ιι∙∙∙ι
∙∙ ∙∙

▲ ▲
5	6	7	8
Figure S2:	A training example (left) and a test example (right) in the productivity split. Note that
in the training example, the constant rule is applied to number, type, and size, while the progression
rule is applied on color. In the testing example, the arithmetic rule is applied on all attributes.
VVV VVV VVV				Λ	
4 4 4 4 4 4				≡	
* *		ɪɪ		Q	
B	0		:∙		:∙
1 ~234^-
≡S0E
5 6 7 8
	OOO OOO	OOO
• •	•	■
TZ .	Zl	Q
□□□□		
-^1	~234
□□□□
5	6	7	8
Figure S3:	A training example (left) and a test example (right) in the localism split. Note that in the
training example, the arithmetic rule is on all attributes. In the test example, the progression rule is
applied on number and the constant rule on all other attributes.
4
Under review as a conference paper at ICLR 2022
Table S1: Model performance on the original datasets for in-distribution evaluation. The perfor-
mance is measured by accuracy on the test sets. Upper: results on Zhang et al. (2019a). Lower:
results on HU et al. (2020)._________________________________________________________________________
Method	Acc	Center	2x2Grid	3x3Grid	L-R	U-D	O-IC	O-IG
ALANS	74.4%	69.1%	80.2%	75.0%	72.2%	73.3%	76.3%	74.9%
HriNet	45.1%	66.1%	40.7%	38.0%	44.9%	43.2%	47.2%	35.8%
CoPINet	91.4%	95.1%	77.5%	78.9%	99.1%	99.7%	98.5%	91.4%
ALANS	78.5%	72.3%	79.5%	72.9%	79.2%	79.6%	85.9%	79.9%
HriNet	60.8%	78.2%	50.1%	42.4%	70.1%	70.3%	68.2%	46.3%
CoPINet	46.1%	54.4%	36.8%	31.9%	51.9%	52.5%	52.2%	42.8%
Human	84.41	95.45	81.82	79.55	86.36	81.81	86.36	81.81
Table S2: The network architecture used for each branch of the object CNN.
Operator	Parameters
Convolution	r6, 5, 1s
BatchNorm SoftPlus	6
MaxPool	2
Convolution	r16, 5, 1s
BatchNorm SoftPlus	16
MaxPool	2
Linear SoftPlus	120
Linear SoftPlus	84
Linear LogSoftMax	m
Table S1 shows the results of the ALANS learner on the orginal datasets for in-distribution eval-
uation compared to previous published state-of-the-art. Notably, the ALANS learner is relatively
consistent across the two datasets, though inferior on Zhang et al. (2021) potentially due to shortcuts
in it that can be better leveraged by neural models.
C	Implementation Details
C.1 Network Architecture
We use a LeNet-like architecture (LeCun et al., 1998) for each branch of the object CNN. See
Table S2 for the design. Note that the object CNN consists of four branches, including objectiveness,
type, size, and color. The parameters for Convolution denote the output channel size, kernel size,
and stride, respectively. A BatchNorm layer is parameterized by the number of channels, whereas
a MaxPool layer by its stride. An output size is used to specify parameters of a Linear layer. m
equals 2, 5, 6, 10 for objectiveness, type, size, and color, respectively. For numerical stability, we
use LogSoftMax to turn a probability simplex into its log space.
C.2 Other Hyperparameters
For the inner regularized linear regression, we set different regularization coefficients for different
attributes but, for the same attribute, we keep them the same across all three types of operators. For
position, λ “ lθ´4. For number, λ “ lθ´6. For type, λ “ lθ´6. For size, λ “ lθ´6. For color,
λ “ 5 X 10-7. All of the regularization terms in Eq. (10) in the main text are set to be 1 and {Ma}
and {Ma} are initialized as 2 X 2 square matrices.
5
Under review as a conference paper at ICLR 2022
For training, we first train for 10 epochs parameters regarding objectiveness, including the objective-
ness branch, and the representation matrices on position and number. We then perform 2 rounds of
cyclic training on parameters regarding type, size, and color, each of which experiences 10 epochs
of updates in a round. Finally, we fine-tune all parameters for another 10 epochs, totaling up to 80
training epochs. The entire system is optimized using ADAM (Kingma & Ba, 2014) with a learning
rate of 9.5 X lθ´5.
D	Marginalization for Other Attributes
For the attribute of position, we denote its value as Ro, a binary vector of length N, with each entry
corresponding to one of the N windows. Then
N
P(Position = Ro) = 口 Ppr = Ro),	(S24)
j“1
where Pprjo) denotes the jth region’s estimated objectiveness distribution returned by a CNN as in
the main text.
For the attribute of type, the panel attribute of type being k is evaluated as
P (Type = k) = X ( 口 Pprtt = k) ) P (Position = Ro),	(S25)
Ro j,Rjo“1
where P(rjt) denotes the jth region’s estimated type distribution returned by a CNN.
The computation for size and color is exactly the same as type, except that we use the region’s
estimated size and color distribution returned by a CNN.
E More on Neural Visual Perception
•	Why not train a CNN to predict the position and number of objects? The CNN is
trained to predict the type, size, color, and object existence in a window. The object ex-
istence in windows is marginalized to be a Number distribution and Position distribution.
This is a light-weight method for object detection. Nevertheless, it is also possible to use
a Fast-RCNN like method to predict object positions (this implies number) directly. How-
ever, in this way, the framework loses the probabilistic interpretation (the object proposal
branch is currently still deterministic), and we cannot perform end-to-end learning.
•	How does the CNN predict the presence of an object, its type, size, and color given
that it is not trained to do that? For each window, the CNN outputs 4 softmaxed vec-
tors, corresponding to the probability distributions of object existence, object type, object
size, and object color. The spaces for these attributes are pre-defined. CNN’s weights
are then jointly trained in the framework. Such a design follows recent neuro-symbolic
methods (Han et al., 2019; Mao et al., 2019) that also rely on the implicitly trained repre-
sentation. In short, we assign semantics to the implicitly trained representation (probability
distributions for attributes), performs marginalization and reasoning as if they are ground-
truth attribute distributions, and jointly train using only the problem’s target label.
F	More on Algebraic Abstract Reas oning
•	Are the learned operators interpretable and aligned with ground-truth? As different
operators are of different shapes and are represented as matrices with multiple entries, it’s
hard to quantitatively evaluate the fitness in general. However, we have noticed the fol-
lowing patterns of the learned matrices in our case-by-case examination: (1) Approximate
permutation matrices for the shifting operation in unary operators (Progression in position)
and ternary operators (Distribute of Three). Algebraically, shifting elements in a vector is
exactly implemented as the product with a permutation matrix in the representation theory.
(2) Approximate identity matrix that keeps the distribution the same and it corresponds
6
Under review as a conference paper at ICLR 2022
to the Constant relation. (3) Approximate successor matrices that can be understood as
Progression when multiplied.
7