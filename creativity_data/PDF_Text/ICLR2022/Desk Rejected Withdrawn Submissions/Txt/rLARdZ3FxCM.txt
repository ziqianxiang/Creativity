Under review as a conference paper at ICLR 2022
Projective Manifold Gradient Layer
for Deep Rotation Regression
Anonymous authors
Paper under double-blind review
Ab stract
Regressing rotations on SO(3) manifold using deep neural networks is an impor-
tant yet unsolved problem. The gap between Euclidean network output space and
the non-Euclidean SO(3) manifold imposes a severe challenge for neural network
learning in both forward and backward passes. While several works have pro-
posed different regression-friendly rotation representations, very few works have
been devoted to improving the gradient backpropagating in the backward pass.
In this paper, we propose a manifold-aware gradient that directly backpropagates
into deep network weights. Leveraging the Riemannian gradient and a novel pro-
jective gradient, our proposed regularized projective manifold gradient (RPMG)
helps networks achieve new state-of-the-art performance in a variety of rotation
estimation tasks. Our proposed gradient layer can also be applied to other smooth
manifolds such as the unit sphere.
1	Introduction
Estimating rotations is a crucial problem in visual perception that has broad applications, e.g., in
object pose estimation, robot control, camera relocalization, 3D reconstruction and visual odome-
try (Kendall et al., 2015a; Bui et al., 2020; Wang et al., 2019a; Gojcic et al., 2020; Dong et al., 2020).
Recently, with the proliferation of deep neural networks, learning to accurately regress rotations is
attracting more and more attention. However, the non-Euclidean characteristics of rotation space
make accurately regressing rotation very challenging.
As we know, rotations reside in a non-Euclidean manifold, SO(3) group, whereas the unconstrained
outputs of neural networks usually live in Euclidean spaces. This gap between the neural network
output space and SO(3) manifold becomes a major barrier to accurate rotation regression, thus tack-
ling this gap becomes an important research topic for rotation regression. One popular research
direction is to design learning-friendly rotation representations, e.g., 6D continuous representation
from (Zhou et al., 2019) and 10D symmetric matrix representation from (Peretroukhin et al., 2020).
Recently, (Levinson et al., 2020) adopted the vanilla 9D matrix representation discovering that sim-
ply replacing Gram-Schmidt process in the 6D representation (Zhou et al., 2019) by symmetric
SVD-based orthogonalization can make this representation superior to the others.
Despite of the progress on discovering better rotation representations, the gap between a Euclidean
network out space and SO(3) manifold hasn’t been completely filled. The non-Euclidean nature of
SO(3) manifold leads to many unique properties beyond various representations, one of which is
its gradient. For a variable on SO(3) manifold, its gradient can also be on-manifold, as known as
Riemannian gradient. We observe that, for gradient backpropagation from the rotation loss back to
the neural network weights, all the existing works simply rely upon a vanilla auto-differentiation,
yielding off-manifold gradients for predicted rotations. We further point out that most of the ex-
isting works focus on a holistic design of rotation regression that is agnostic to forward/backward
pass without an in-depth study of its gradient in the backward pass. On one hand, methods of Rie-
mannian optimization allow for optimization on SO(3) (Taylor & Kriegman, 1994; Blanco, 2010),
matrix manifolds (Absil et al., 2009) or general Riemannian manifolds (Zhang et al., 2016; Udriste,
2013). However, they are not very useful when it comes to updating the weights of the neural
networks that are Euclidean. On the other hand, approaches like (Hou et al., 2018) incorporate a
Riemannian distance as well as its gradient into the network training, however, they do not deal with
the representation issue.
1
Under review as a conference paper at ICLR 2022
In this work, we want to propose a better manifold-aware gradient in the backward pass of rota-
tion regression. This is a fundamental yet currently under-explored avenue. We begin by making
the observation that the gradient of a loss function with respect to the output rotation is often not
on-manifold. We therefore leverage the Riemannian gradient, connecting the output rotation to a
goal rotation in SO(3). Backpropagating this gradient, we encounter the mapping function (or or-
thogonalization function) that transforms the raw network output to a valid rotation. This projection
is typically a many-to-one map, e.g. different matrices can be orthogonalized to the same rotation
matrix via either Gram-Schmidt process or SVD orthogonalization. This non-bijectivity provides
us with a new design space for our gradient: if we were to use a gradient to update the raw output
rotation, many gradients would result in the same update in the final output rotation despite being
completely different for backpropagating into the neural network weights. This in fact becomes a
supervision problem: which gradient is the best for backpropagation when many of them correspond
to the same update to the output?
We observe that this problem is somewhat similar to some ambiguities or multi-ground-truth is-
sues. One example would be the symmetry issue in pose estimation: an symmetric object, e.g.
a textureless cube, appears the same under many different poses, which needs to be considered
when supervising the pose predictions. For supervising such learning problem, one can leverage an
uncertainty-driven approach that predicts multimodal distribution rather than a single mode (Deng
et al., 2020), however this approach is not feasible for us to get the best gradient. Inspired by the
min-of-N loss proposed by (Fan et al., 2017) and used by (Wang et al., 2019b) for dealing with pose
symmetry, we propose to find the gradient with the smallest norm that can update the final output
rotation to the goal rotation. This back-projection process involves finding an element closest to
the network output in the inverse image of the goal rotation and projecting the network output to
this inverse image space. We therefore coin our gradient projective manifold gradient. One thing
to note is that this projective gradient tends to shorten the network output, causing the norms of
network output vanishing. To fix this problem, we further incorporate a simple regularization into
the gradient, leading to our full solution regularized projective manifold gradient.
Note that our proposed gradient layer operates on the raw network output and can be directly back-
propagated into the network weights. Our method is very general and is not tied to a specific rotation
representation. It can be coupled with different non-Euclidean rotation representations, including
quaternion, 6D representation (Zhou et al., 2019), and 9D rotation matrix representation (Levinson
et al., 2020), and can even be used for regressing other non-manifold variables.
We evaluate our devised projective manifold gradient layers on a diverse set of problems involving
rotation regression: 3D object pose estimation from 3D point clouds/images, rotation estimation
problems without using ground truth rotation supervisions, and self-supervised instance-level ro-
tation estimation (see appendix D for more experiments on camera relocalization). Our method
demonstrates significant and consistent improvements on all these tasks and different all rotation
representations tested. Going beyond rotation estimation, in appendix D.3 we also demonstrate
performance improvements on regressing unit vectors (lie on a unit sphere) as an example of an
extension to other non-Euclidean manifolds.
2	Preliminaries
2.1	Riemannian Geometry
We define an m-dimensional Riemannian manifold embedded in an ambient Euclidean space X =
Rd and endowed with a Riemannian metric G , (Gx)x∈M to be a smooth curved space (M, G).
A vector v ∈ X is said to be tangent to M at x iff there exists a smooth curve γ : [0, 1] 7→ M
s.t. γ(0) = X and γ(0) = v. The velocities of all such curves through X form the tangent space
TxM = {Y(0) | Y : R → M is smooth around 0 and γ(0) = x}. The Riemannian metric G(∙)
equips each point X with an inner product in the tangent space TxM, e.g. hu, vix = uT Gxv.
Definition 1 (Riemannian gradient). For a smooth function f : M 7→ R and ∀(X, v) ∈ TM, we
define the Riemannian gradient of f as the unique vector field gradf satisfying (Boumal, 2020):
Df (x)[v] = hv, gradf(X)ix	(1)
where Df (x)[v] is the derivation of f by v. It can further be shown (see our appendix) that an
expression for gradf can be obtained through the projection of the classical gradient orthogonally
2
Under review as a conference paper at ICLR 2022
onto the tangent space
gradf(x) = Vf(X)U= Πχ(Vf(x)).	(2)
where Πχ : X → TxM ⊆ X is an orthogonal projector with respect to(∙, )x.
Definition 2 (Riemannian optimization). We consider first order optimizers to solve problems of the
form minx∈M f (x). For a local minimizer or a stationary point x? of f, the Riemannian gradient
vanishes gradf (x?) = 0 enabling a simple algorithm, Riemannian gradient descent (RGD):
xk+1 = Rxk (-τk gradf(xk))	(3)
where τk is the step size at iteration k and Rxk is the retraction usually chosen related to the expo-
nential map.
2.2	Rotation representations
There are many ways of representing a rotation: classic rotation representations, e.g. Euler angles,
axis-angle, and quaternion; and recently introduced regression-friendly rotation representations such
as e.g. 5D (Zhou et al., 2019), 6D representations (Zhou et al., 2019) and 10D (Peretroukhin et al.,
2020) representations. A majority of deep neural networks can output an unconstrained, arbitrary
n-dimensional vector x in a Euclidean space X = Rn . For Euler angle and axis-angle representa-
tions which use a vector from R3 to represent a rotation, a neural network can simply output a 3D
vector; however, for quaternions or 6D and 9D representations that lie on non-Euclidean manifolds,
manifold mapping functions π : Rn 7→ M are generally needed for normalization or orthogonal-
ization purposes to convert network outputs to valid elements belong to the representation manifold.
This network Euclidean output space X is where the representation manifolds reside and therefore
are also called ambient space.
Definition 3 (Rotation representation). One rotation representation, which lies on a representation
manifold M, defines a surjective rotation mapping φ : X ∈ M → φ(X) ∈ SO(3) and a representa-
tion mapping function ψ : R ∈ SO(3) → ψ(R) ∈ M, such that φ(ψ) = R ∈ SO(3).
Definition 4 (Manifold mapping function). From an ambient space X to the representation manifold
M, we can define a manifold mapping function π : x ∈ X → π(x) ∈ M, which projects a point x
in the ambient, Euclidean space to a valid element X = π(x) on the manifold M.
We summarize the rotation mappings, representation mappings, the manifold mappings for several
non-Euclidean rotation representations below.
Unit quaternion. Unit quaternions represent a rotation using a 4D unit vector q ∈ S3 double
covering the non-Euclidean 3-sphere i.e. q and -q identify the same rotation. A network with a final
linear activation can only predict X ∈ R4 . The corresponding manifold mapping function is usually
chosen to be a normalization step, which reads πq(X) = X/|X|. For rotation and representation
mapping, we leverage the standard mappings between rotation and quaternion (see appendix F).
6D rotation representation and Gram-Schmidt orthogonalization. 6D rotation representation,
proposed in (Zhou et al., 2019), uses two orthogonal unit 3D vectors (Ci, ^2) to represent a rotation,
which are essentially the first two columns of the corresponding rotation matrix. This representation
lies on Stiefel manifold V2 (R3). Its manifold mapping π6D is done through Gram-Schdimt
orthogonalization. Its rotation mapping 66d is done by adding the third column ^3 = ^ι X ^2. Its
representation mapping ψ6D is simply getting rid of the third column C3 from a rotation matrix.
9D rotation matrix representation and SVD orthogonalization. As this representation manifold
is SO(3), both the rotation and representation mapping functions are simply identity. To map araw
9D network output M to a rotation matrix, we can consider both Gram-Schmidt orthogonalization
and SVD orthogonalization. However, for Gram-Schmidt orthogonalization, the last three dimen-
sions are redundant and will not make any difference with 6D representation. We therefore choose
SVD orthogonalization as the manifold mapping function π9D, as follows: π9D first decomposes
M into its left and right singular vectors (U, V>) and singular values (SV) Σ, M = UΣV>; then
it replaces the SV Σ J Σ0 = diag(1,1, det(UV>)) and finally, computes R = UΣ0V> to get the
corresponding rotation matrix R ∈ SO(3).
2.3	Deep rotation regression
We conclude this section by describing the ordinary forward and backward passes of a neural net-
work based rotation regression, as used in (Zhou et al., 2019; Levinson et al., 2020).
3
Under review as a conference paper at ICLR 2022
Input
signal
Regularized
Projective
Manifold
Gradient
Neural
Network
Backprop.
gRPM =
x~xgp *
+ X(XgP - Xg)
Ambient
space . V
R Raw
^[* output	I
I x	I
Projected
xgp
Representation	Rotation
Manifold	ManifoldE	Rotation Mnfold 二""
一	一	"Output
rotation
开(χ)		-。（父）
Invert		Represent.
& project		mapping
J ¾,1(R,)
Loss
Goal
rotation
C-rL-√
gradC(∕(R)))
Riemannian
optimization
C(J(R))
Figure 1: Projective Manifold Gradient Layer. In the forward pass, the network predicts a raw
output x, which is then transformed into a valid rotation R = φ(π(x)). We leave this forward
pass unchanged and only modify the backward pass. In the backward pass, we first use Riemannian
optimization to get a goal rotation Rg and map it back to Xg on the representation manifold M.
After that We find the element Xgp which is closest to the raw output in the inverse image of Xg, and
finally get the gradient gRPM we want.
Forward and backward passes. Assume, for a rotation representation, the network predicts X ∈ X,
then the manifold mapping ∏ will map X to X = ∏(x) ∈ M, followed by a rotation mapping φ that
finally yields the output rotation R = φ(X) = φ(∏(x)). Our work only tackles the backward pass
and keeps the forward pass unchanged, as shown in the top part of Figure 1. The gradient in the
backward-pass is simply computed using Pytorch autograd method, that is g = f 0(R)φ0(X)∏0(x).
Loss function. For supervising rotation matrix, the most common choice of loss function is L2
loss, ||R - Rgt||2F , as used by (Zhou et al., 2019; Levinson et al., 2020). This loss is equal to
4 - 4 cos(< R, Rgt >), where < R, Rgt > represents the angle between R and Rgt.
3	Method
Overview. In this work, we propose a projective manifold gradient layer, without changing the
forward pass of a given rotation regressing network, as shown in Figure 1. Our focus is to find a
better gradient g of the loss function L with respect to the network raw output X for backpropagation
into the network weights.
Let,s start with examining the gradient of network output X in a general case - regression in EU-
clidean space. Given a ground truth Xgt and the L2 loss ||X - Xgt||2 that maximizes the likelihood
in the presence of Gaussian noise in X, the gradient would be g = 2(X - Xgt).
In the case of rotation regression, we therefore propose to find a proper x* ∈ X for a given ground
truth Rgt or a computed goal rotation Rg when the ground truth rotation is not available, and then
simply use X - X* as our gradient to backpropagate into the network.
Note that finding such a X* can be challenging. Assuming we know Rgt, finding a X* involves
inverting φ and π since the network output R = φ(π(X)). Furthermore, we may not know Rgt
under indirect rotation supervision (e.g., flow loss as used in PoseCNN(Xiang et al., 2017)) and
self-supervised rotation estimation cases (e.g., 2D mask loss as used in (Wang et al., 2020)). In
this work, we introduce the following techniques to mitigate these problems: (i) we first take a
Riemannian gradient to compute a goal rotation Rg ∈ SO(3), which does not rely on knowing Rgt,
as explained in Section 3.1; (ii) we then find the set of all possible Xgs that can be mapped to Rg,
or in other words, the inverse image of Rg under π and φ; (iii) we find Xgp which is the element in
this set closet to X in the Euclidean metric and set it as ”X*”. We will construct our gradient using
this X* , as explained in 3.2. (iv) we add a regularization term to this gradient forming gRPMG as
explained in 3.3. The whole backward pass leveraging our proposed regularized projective manifold
gradient is shown in the lower half of Figure 1.
3.1	RIEMANNIAN GRADIENT AND GOAL ROTATION Rg
To handle rotation estimation with/without direct rotation supervision, we first propose to compute
the Riemannian gradient of the loss function L with respect to the output rotation R and find a goal
rotation Rg that is presumably closer to the ground truth rotation than R.
4
Under review as a conference paper at ICLR 2022
Assume the loss function is in the following form L(f (R)), where R = π(φ(x)) is the output
rotation and f constructs a loss function that compares R to the ground truth rotation Rgt directly
or indirectly. Given R(x) and L(f (R(x))), we can perform one step of Riemannian optimization
yielding our goal rotation Rg J RR(-τgrad L(f (R))), where T is the step size of Riemannian
gradient and can be set to a constant as a hyperparameter or varying during the training. For L2
loss ||R - Rgt||2F, Riemannian gradient is always along the geodesic path between R and Rgt on
SO(3). In this case, Rg can generally be seen as an intermediate goal between R and Rgt dependent
on τ. Gradually increasing τ from 0 will first make Rg approach Rgt starting with Rg = R, and
then reach Rgt where we denote τ = τgt , and finally going beyond Rgt . Although, when Rgt is
available, one can simply set Rg = Rgt , we argue that this is just a special case under τ = τgt . We
will further compare different strategies of how to choose or vary τ in Section 3.3. For scenarios
where Rgt is unavailable, Rg is presumably closer to the desired Rgt if the loss function does not
suffer from local minima. In the sequel, we only use Rg for explaining our methods.
3.2	Projective Manifold Gradient
Given Rg, we can use the representation mapping ψ to find the corresponding Xg = ψ(Rg) on the
representation manifold M. However, further inverting π and finding the corresponding xg ∈ X
is a non-trivial problem, due to the projective nature of π. In fact, there are many xg s that satisfy
∏(xg) = Xg. It seems that We can construct a gradient g = (X - Xg) using any Xg that satisfies
∏(xg) = Xg. No matter which Xg we choose, if this gradient were to update x, it will result in
the same Rg . But, when backpropagation into the network, those gradients will update the network
weights differently, potentially resulting in different learning efficiency and network performance.
Formally, we formulate this problem as a multi-ground-truth problem for x: we need to find the best
x* to supervise from the inverse image of Xg under the mapping ∏. We note that similar problems
have been seen in pose supervision dealing with symmetry as in (Wang et al., 2019b), where one
needs to find one pose to supervise when there are many poses under which the object appears the
same. (Wang et al., 2019b) proposed to use a min-of-N strategy introduced by (Fan et al., 2017):
from all possible poses, taking the pose that is closet to the network prediction as ground truth. A
similar strategy is also seen in supervising quaternion regression, as q and -q stand for the same rota-
tion. One common choice of the loss function is therefore min{L(q, qgt), L(q, -qgt)}(Peretroukhin
et al., 2020), which penalizes the distance to the closest ground truth quaternion.
Inspired by these works, we propose to choose our gradient among all the possible gradients with
the lowest level of redundancy , i.e., we require x* to be the one closet to x, or in other words, the
gradient to have the smallest norm, meaning that we need to find the projection point Xgp of X to all
the valid Xg :
Xgp = argmin ||X - Xg||2	(4)
π(Xg )=xg
We then can construct our projective manifold gradient as gPM = X - Xgp.
Here we provide another perspective why a network may prefer such a gradient. In the case where a
deep network is trained using stochastic gradient descents (SGD), the final gradient used to update
the network weights is averaged across the gradients of all the batch instances. If gradients from
different batch instances contain different levels of redundancy, then the averaged gradient may be
biased or not even appropriate. This argument is generally applicable to all stochastic optimizers
(e.g., Adam (Adams et al., 2020))
Inverting π. There are many ways to solve this projection problem for different manifold mapping
functions π . For example, we can formulate this as a constrained optimization problem. For the
manifold mapping functions we consider, we propose the following approach: we first solve for the
inverse image ∏-1(Xg) of Xg in the ambient space X analytically, which reads ∏-1(Xg) = {xg ∈
X | ∏(xg) = Xg}; we then project X onto this inverse image space. Note that, sometimes only a
superset of this inverse image can be found analytically, requiring certain constraints on Xgp to be
enforced.
Here we list the inverse image of ∏-1(Xg) and the projection point Xg for different rotation repre-
sentations and their corresponding manifold mapping functions π. Please refer to appendix for the
detailed derivations.
Quaternion. With ∏q(x) = x∕∣∣x∣∣, X ∈ R4, and Xg ∈ S3: π-1(Xg) = {x | X = kXg,k ∈
5
Under review as a conference paper at ICLR 2022
R and k > 0}, which is a ray in the direction of Xg starting from the origin. Without considering
the constraint of k > 0 , an analytical solution to this projection point xgp of x onto this line can be
derived: Xgp = (X ∙ Xg)Xg.
6D representation. With π6D as Gram-Schmidt process, x = [u, v] ∈ R6, and Xg ∈ V2(R3):
∏-D(Xg) = {[kιug,k2Ug + k3Vg] | kι, k2,k3 ∈ R and k1,k3 > 0} (the former is aray whereas the
latter spans a half plane). Without considering the constraint of k1, k3 > 0, the projection point Xgp
can be analytically represented as Xgp = [(u ∙ Ug)ug, (v ∙ Ug)ug 十 (V ∙ vg )vg]
9D representation. With ∏9d(x) as SVD orthogonalization to be positive, X ∈ R3×3, and Xg ∈
SO(3), the analytical expression for π9-D1 is available when we ignore the positive singular value
constraints, which gives π-D (Xg) = {SXg | S is an arbitrary symmetric matrix}. We can further
XxT+xgXT
solve the projection point Xgp with an elegant representation Xgp =	gɪ^—.
3.3	Regularized projective manifold gradient for rotation regression
Issues in naive projective manifold gradient. In Figure 2, we
illustrate this projection process for several occasions where X
takes different positions relative to Xg . We demonstrate that
there are two issues in this process. First, no matter where X is
in, the projection operation will shorten the length of our pre-
diction because |Xgp | < |X| is always true. This will cause the
length norm of the prediction of the network to become very
small as the training progresses (see Figure 3). The shrinking
network output will keep increasing the effective learning rate,
preventing the network from convergence and leading to great
harm to the network performance (see Table 3 and Figure 3 for
ablation study).
Figure 2: Projection point Xgp in
the case of quaternion.
Second, when the angle between X and Xg becomes larger than n/2 (in the case of X = X3), the
naive projection Xgp will be in the opposite direction of Xg and can not be mapped back to Xg under
πq, resulting in a wrong gradient. The same set of issues also happen to 6D and 9D representations.
The formal reason is that the analytical solution of the inverse image assumes certain constraints
are satisfied, which is usually true only when either Xg is not far from X or the network is about to
converge.
Regularized projective manifold gradient To solve the first issue, we propose to add a regular-
ization term Xgp - Xg to the projective manifold gradient, which can avoid the length vanishing
problem. The regularized projective manifold gradient then reads:
gRPM = X - Xgp + λ(Xgp - Xg),	(5)
where λ is a regularization coefficient. We intentionally keep the weight of λ, small (usually 0.01)
because: (1) we want the projective manifold gradient (X - Xgp) to be the major component of our
gradient; (2) since this regularization is roughly proportional to the difference in prediction length
and 1, a small lambda can already prevent the length from being vanished and, at the end, the
prediction length will stay roughly constant at the equilibrium under projection and regularization.
To tackle the second problem of reversed gradient, we further propose to take a small τ used in
Riemannian optimization at the beginning of training, leading to a slow warm-up. As the training
progresses, we increase τ , such that the network converges better. Our ablation study will show the
effectiveness of our choice of λ and τ .
4	Experiments
We investigate popular rotation representations and find our methods greatly improve the perfor-
mance in different kinds of tasks. For our regularized projective manifold gradient (in short RPMG),
we apply it in the backpropagation process of Quaternion, 6D and 9D, without changing the forward
pass, leading to three new methods RPMG-Quat, RPMG-6D and RPMG-9D. We compare the fol-
lowing seven baselines: Euler angle, axis-angle, Quaternion, 6D (Zhou et al., 2019), 9D (Levinson
et al., 2020), 9D-Inf (Levinson et al., 2020) and 10D (Peretroukhin et al., 2020). We adopt three
evaluation metrics: mean, median, and 5° accuracy of (geodesic) errors between predicted rotation
6
Under review as a conference paper at ICLR 2022
and ground truth rotation. For most of our experiments, we set the regularization term λ = 0.01
and increase τ from 0.05 to 0.25 by uniform steps. We further show and discuss the influence of
different choices of these two hyperparameters in the ablation studies.
4.1	3D object pose estimation from point clouds
The first experiment is the category-level pose estimation from point clouds. Given one shape point
clouds of a specific category, the network learns to predict the 3D rotation of the input point clouds
from the predefined canonical view of this category(Wang et al., 2019b). We replace the point clouds
alignment task used in (Zhou et al., 2019; Levinson et al., 2020) (which has almost been solved) by
this experiment since it is more challenging and more closed to real-world applications (no canonical
point clouds is given to the network).
We use a PointNet++ (Qi et al., 2017) network as our backbone, supervised by L2 loss between the
predicted rotation matrix R and the ground truth rotation matrix Rgt . We use two different kinds of
data: complete point clouds and depth point clouds, both generated from the airplane point clouds
from ModelNet (Wu et al., 2015). We divide the airplanes into a train split and a test split, following
(Chen et al., 2021). Refer to appendix for more details.
The results are shown in Table 1 and Table 2. We see a great improvement of our methods in all three
rotation representations. One may find 9D-Inf also leads to a good performance, which is actually
a special case of our method with τ = τgt and λ = 1. However, this simple loss may lead to a bad
performance when Rgt is unavailable(see Sec 4.3.2).
	Mean (°)	Med (°)	5°Acc (%)
Euler	124,88	130.37	0,0
Axis-Angle	10,57	7,20	29,1
Quaternion	10,55	8,57	21,7
6D	7,25	6,15	38,2
9D	7,70	6,16	38,0
9D-Inf	3,25	2,57	87,1
10D	7,42	5,34	44,5
RPMG-Quat	5Γ2	2,83	80,4
RPMG-6D	2.85	2,09	92.7
RPMG-9D	3,97	2.02	92,5
10%	30%	50%	70»	∞⅝
Peixentile
180°
450
100
5°
1°
0.1°
Table 1: Pose estimation from complete point clouds. Left: a comparison of methods by mean,
median, and 5° accuracy of (geodesic) errors after 30k training steps. Middle: mean test error
at different points along with the training progression. Right: test error percentiles after training
completes. The legend on the right applies to both plots.
Mean (°) Med (°) 5°Acc (%)
100 ∙
180°
Euler
Axis-Angle
Quaternion
6D
9D
9D-Inf
10D
126.2
125.2
22.91
38.60
29.25
6.39
45.6
132.7
132.0
15.68
25.87
20.1
4.23
31.2
0.0
0.0
4.8
0.9
2.4
60.2
0.9
90
80
70
50
30
20
450
10,
5°∙
1∙
—Euler
,——Axis-Ang Ie
1--Quaternion
—6D
—9D
...9D-lnf
IOD
——RPMG-Quat
——RPMG-SD
——RPMG-9D
RPMG-Quat	10.07	5.77	39.9
RPMG-6D	7,10	4,38	57.8
RPMG-9D	6,53	4.19	61.0	" Itlk M 50fc … ≡ ao% so% 70%	»%
Table 2: Pose estimation from depth point clouds. We report the same metricsntas in Table 1;
IQ
0∙
IOk
40k
0.1°
see
the caption there, All models are trained for 50K iterations,
«
«
Ablation Study We change different regularization coefficients λ and different τ to show the influ-
ence of these two hyperparameters on 6D representation, Our simplest method MG, which directly
uses X - X as the gradient, can outperform the vanilla L2 loss but is worse than those with small
λ, which is consistent with the discussion in Sec3,3, And we can find that λ = 0.01 is better than
λ = 0 from the failures of PMG which only uses X - Xgp as gradient, We show the length vanish-
ing problem without regularization and stablized length with regularization in Figure 3, As for the
precise value of λ, our experiments show that different choices will lead to similar performances,
which implies the robustness of this hyperparameter, For the choices of τ , we also get a similar
conclusion as λ: it is not sensitive in a reasonable range, but it shouldn’t be too large or too small,
just as discussed in Sec 3,3,
4,2	3D object pose estimation from real images
Pascal3D+ (Xiang et al,, 2014) is a standard benchmark for object pose estimation from real images,
We follow the same setting as in (Levinson et al,, 2020) to estimate object poses from single images,
For training we discard occluded or truncated objects and augment with rendered images from (Su
7
Under review as a conference paper at ICLR 2022
			Mean (o)		Complete Med (o)	5oAcc (%)	Mean (o)	Depth Med (o)	5oAcc (%)
L2 w/ 6D		-	-	7.25	6.15	38.2	38.60	25.87	0.9
MG-6D	λ	=1	τsaf e	3.27	2.68	86.1	7.60	4.80	52.6
			τgt	3.37	2.77	85.7	7.69	4.97	50.6
PMG-6D	λ	=0	τsaf e	64.41	40.99	2.8	92.9	91.5	0.2
			τgt	103.2	100.4	0.0	132.7	126.5	0.0
			τinit	3.60	2.30	91.1	7.02	4.38	59.7
	λ=	0.01	τsaf e	3.41	2.04	87.2	23.20	8.71	24.6
RPMG-6D			τgt	4.12	2.22	87.1	23.69	7.77	28.5
			τinit → τsaf e	2.85	2.09	92.7	7.10	4.38	57.8
	λ=	0.005	τinit → τsaf e	3.07	2.11	89.6	7.30	4.26	55.0
	λ	0.1		2.85	2.19	90.9	7.93	4.67	54.7
Table 3: Ablation study of pose estimation from point clouds. We report the same metrics as in
Table 1; see the caption there. We set τinit = 0.05, τsaf e = 0.25. τsaf e means the upper bound of
τ to make sure the Rg will be closer to R than Rgt . Refer to appendix for more details.
Figure 3: Average L2 norm of the network raw output x during training. Left: PMG-6D (w/o
reg. λ = 0). Right: RPMG-6D (w/ reg. λ = 0.01)
et al., 2015). In the Table 1 and Table 2, we report our results on sofa and bicycle categories, given
that (Levinson et al., 2020) only reported the detailed numbers for these two categories.
It can be seen that our method leads to consistent improvements to quaternion, 6D, and 9D repre-
sentations on both sofa and bicycle classes.
Accuracy(%)	Med°
	10o	15o	20o	Err
Euler	60.2	80.9	90.6	8.3
Axis-Angle	45.0	70.9	85.1	11.0
Quaternion	34.3	60.8	73.5	13.2
6D	50.8	76.7	89.0	9.9
9D	52.4	79.6	90.3	9.2
9D-Inf	70.9	88.0	93.5	6.7
10D	50.2	77.0	89.6	9.8
RPMG-Quat	56.6	79.6	90.9	8.9
RPMG-6D	69.6	86.1	92.2	6.7
RPMG-9D	72.5	88.0	95.8	6.7
iao・
45・
10'
5'
——RPMG-Quat
——RPMG-βD
——RPMG-9D
10%	30%	50%	70%	9C⅝
Percentlie
Table 4:	Pose estimation from PASCAL3D+ sofa images. Left: a comparison of methods by 10o
/ 15o / 20o accuracy of (geodesic) errors and median errors after 60k training steps. Middle: median
test error at different points along with the training progression. Right: test error percentiles after
training completes. The legend on the right applies to both plots.
4.3 Rotation Estimation without Ground Truth Rotation Supervision
4.3.1	Using Flow Loss for Rotation Estimation from Point Clouds
We mainly follow the setting of experiment 4.1 with complete airplane point cloud dataset and the
only difference is that we use flow loss ||RX - RgtX ||2F here, where X is the complete point
clouds. Since the format of loss is changed, the previous schedule of τ is not suitable anymore, and
we have to change the value of τ accordingly. Our selection skill is to first choose a τ as we like
and visualize the mean geodesic distance between predicted R and Rg during training. Then we
can roughly adjust τ to make the geodesic distance looked reasonable. For this experiment, we fix
τ = 25 and λ = 0.01. In Table 6, we show our methods again outperform vanilla methods.
8
Under review as a conference paper at ICLR 2022
	Ac	curacy(	%)	Med。
	10。	15。	20。	Err
Euler	28.2	48.1	62.7	15.7
Axis-Angle	5.3	8.1	10.1	79.7
Quaternion	20.8	38.8	54.6	18.7
6D	21.8	39.0	55.3	18.1
9D	20.6	37.6	56.9	18.0
9D-Inf	38.0	53.3	69.9	13.4
10D	23.9	42.3	56.7	17.9
RPMG-Quat	32.3	50.0	65.6	15.0
RPMG-6D	35.4	57.2	70.6	13.5
RPMG-9D	36.8	57.4	71.8	12.5
180°
450
100
5”∙
1∙
0.10	.	.	.	.
10%	30%	50%	70»	∞⅝
Peixentile
Table 5:	Pose estimation from PASCAL3D+ bicycle images. We report the same metrics as Table
4; see the caption there.
4.3.2 Self-supervised Instance-Level Rotation Estimation from Point Clouds
For one complete chair instance Z, given a complete observation X, we estimate its pose R. We
then use chamfer distance between Z and R-1X as a self-supervised loss. The network structure
and training settings are all the same as Experiment 4.1. We simply set τ = 1. The interesting part
here is that vanilla 9D-Inf fails while our methods still perform very well.
	Category-Level Self-Supervise			Instance-Level Self-Supervise			
	Mean (。)	Med (。)	5oAcc (%)	Mean (。)		Med (。)	5。ACC (%)
Euler	12.14	6.91	33.6	Euler	129.3	132.9	0
Axis-Angle	35.49	20.80	4.7	Axis-Angle	36.31	6.98	37
Quaternion	11.54	7.67	29.8	Quaternion	4.04	3.30	74
6D	14.13	9.41	23.4	6D	43.9	6.49	44
9D	11.44	8.01	23.8	9D	2.47	2.02	92.5
9D-Inf	4.07	3.28	76.7	9D-Inf	101.5	96.61	0
10D	9.28	7.05	32.6	10D	2.18	1.91	96.5
RPMG-Quat	4.86	3.25	75.8	RPMG-Quat	2.88	2.38	91.5
RPMG-6D	2.71	2.04	92.1	RPMG-6D	3.08	2.92	89.5
RPMG-9D	3.75	2.10	91.1	RPMG-9D	1.40	1.17	100
Table 6: Rotation estimation without ground truth rotation supervision We report the same
metrics as in Table 1; see the caption there. All models are trained for 30K iterations. Left: Flow
loss for rotation estimation. Right: Self-supervised instance-level rotation estimation.
5	Related Work
Both rotation parameterization and optimization on SO(3) are well-studied topics. Early deep learn-
ing models leverages various rotation representations for pose estimation, e.g., axis-angle (Um-
menhofer et al., 2017; Do et al., 2018; Gao et al., 2018), quaternion (Xiang et al., 2017; Kendall
& Cipolla, 2017; Kendall et al., 2015b) and Euler-angle (Tulsiani & Malik, 2015; Su et al., 2015;
Kundu et al., 2018). Recently, (Zhou et al., 2019) points out that Euler-angle, axis-angle, and quater-
nion are not continuous rotation representations, since their representation spaces are not homeomor-
phic to SO(3). As better representations for rotation regression, 6D(Zhou et al., 2019), 9D(Levinson
et al., 2020), 10D(Peretroukhin et al., 2020) representations are proposed to resolve the discon-
tinUity issue and improve the regression accuracy. A concurrent work (Bregier, 2021) examines
different manifold mappings theoretically and experimentally, finding out that SVD orthogonaliza-
tion performs the best when regressing arbitrary rotations. Originated from general Riemannian
optimization, (Taylor & Kriegman, 1994) presents an easy approach for minimization on SO(3) by
constructing a local axis-angle parameterization, which is also the tangent space of SO(3) manifold.
They backpropagate gradient to the tangent space and use the exponential map to update the cur-
rent rotation matrix. Most recently, (Teed & Deng, 2021) constructs a PyTorch library that supports
tangent space gradient backpropagation for 3D transformation groups, (e.g., SO(3), SE(3), Sim(3)).
This proposed library can be used to implement the Riemannian gradient in our layer.
6	Conclusion and Future Work
Our work tackles the problem of designing a gradient layer to facilitate the learning of rotation
regression. Our extensive experiments have demonstrated the effectiveness of my method coupled
with different rotation representations in diverse tasks dealing with rotation estimation.
9
Under review as a conference paper at ICLR 2022
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.
Henry Adams, M. Aminian, Elin Farnell, M. Kirby, C. Peterson, Joshua Mirth, R. Neville, P. Ship-
man, and C. Shonkwiler. A fractal dimension for measures via persistent homology. arXiv:
Dynamical Systems,pp.1-31, 2020.
Matthew Grimes Alex Kendall and Roberto Cipolla. Posenet: A convolutional network for real-time
6-dof camera relocalization. 2015.
Jose-Luis Blanco. A tutorial on se (3) transformation parameterizations and on-manifold optimiza-
tion. University of Malaga, Tech. Rep, 3:6, 2010.
Nicolas Boumal. An introduction to optimization on smooth manifolds. Available online, May,
2020.
Romain Bregier. Deep regression on manifolds: a 3d rotation case study. CoRR, abs/2103.16317,
2021. URL https://arxiv.org/abs/2103.16317.
Mai Bui, Tolga Birdal, Haowen Deng, Shadi Albarqouni, Leonidas Guibas, Slobodan Ilic, and Nas-
sir Navab. 6d camera relocalization in ambiguous scenes via continuous multimodal inference.
arXiv preprint arXiv:2004.04807, 2020.
Haiwei Chen, Shichen Liu, Weikai Chen, Hao Li, and Randall Hill. Equivariant point network for
3d point cloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 14514-14523, 2021.
Earl A Coddington and Norman Levinson. Theory of ordinary differential equations. Tata McGraw-
Hill Education, 1955.
Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, and Tolga Birdal. Deep
bingham networks: Dealing with uncertainty and ambiguity in pose estimation. arXiv preprint
arXiv:2012.11002, 2020.
Thanh-Toan Do, Ming Cai, Trung Pham, and Ian D. Reid. Deep-6dpose: Recover-
ing 6d object pose from a single RGB image.	CoRR, abs/1802.10367, 2018. URL
http://arxiv.org/abs/1802.10367.
Siyan Dong, Qingnan Fan, He Wang, Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen, and
Leonidas Guibas. Robust neural routing through space partitions for camera relocalization in
dynamic indoor environments. arXiv preprint arXiv:2012.04746, 2020.
Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object recon-
struction from a single image. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 605-613, 2017.
Ge Gao, Mikko Lauri, Jianwei Zhang, and Simone Frintrop. Occlusion resistant object rotation
regression from point cloud segments. In Proceedings of the European Conference on Computer
Vision (ECCV) Workshops, September 2018.
Zan Gojcic, Caifa Zhou, Jan D Wegner, Leonidas J Guibas, and Tolga Birdal. Learning multiview
3d point cloud registration. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pp. 1759-1769, 2020.
Benjamin Hou, Nina Miolane, Bishesh Khanal, Matthew CH Lee, Amir Alansary, Steven McDon-
agh, Jo V Hajnal, Daniel Rueckert, Ben Glocker, and Bernhard Kainz. Computing cnn loss and
gradients for pose estimation with riemannian geometry. In International Conference on Medical
Image Computing and Computer-Assisted Intervention, pp. 756-764. Springer, 2018.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, To-
bias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
neural networks for mobile vision applications.	CoRR, abs/1704.04861, 2017. URL
http://arxiv.org/abs/1704.04861.
10
Under review as a conference paper at ICLR 2022
Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regression with deep
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), July 2017.
Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-
time 6-dof camera relocalization. In Proceedings of the IEEE international conference on com-
Puter vision,pp. 2938-2946, 2015a.
Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-
time 6-dof camera relocalization. In Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), December 2015b.
Abhijit Kundu, Yin Li, and James M. Rehg. 3d-rcnn: Instance-level 3d object reconstruction via
render-and-compare. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Ros-
tamizadeh, and Ameesh Makadia. An analysis of svd for deep rotation estimation. arXiv preprint
arXiv:2006.14616, 2020.
Shuai Liao, Efstratios Gavves, and Cees G. M. Snoek. Spherical regression: Learning viewpoints,
surface normals and 3d rotations on n-spheres. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, Long Beach, USA, June 2019.
Valentin Peretroukhin, Matthew Giamou, David M. Rosen, W. Nicholas Greene, Nicholas Roy, and
Jonathan Kelly. A Smooth Representation of SO(3) for Deep Rotation Learning with Uncertainty.
In Proceedings of Robotics: Science and Systems (RSS’20), Jul. 12-16 2020.
Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning
on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017.
Olinde Rodrigues. Des lois geometriques qui regissent les deplacements d'un Systeme Solide
dans l'espace, et de la variation des Coordonnees provenant de ces deplacements Consideres
inde´pendamment des causes qui peuvent les produire. Journal de mathe´ matiques pures et ap-
Pliquees, 5(1):380440,1840.
Hao Su, Charles R. Qi, Yangyan Li, and Leonidas J. Guibas. Render for Cnn: Viewpoint estima-
tion in images using Cnns trained with rendered 3d model views. In Proceedings of the IEEE
International Conference on ComPuter Vision (ICCV), DeCember 2015.
Camillo J Taylor and David J Kriegman. Minimization on the lie group so (3) and related manifolds.
Yale University, 16(155):6, 1994.
ZaChary Teed and Jia Deng. Tangent spaCe baCkpropagation for 3d transformation groups. In
Proceedings of the IEEE/CVF Conference on ComPuter Vision and Pattern Recognition (CVPR),
2021.
Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE
Conference on ComPuter Vision and Pattern Recognition (CVPR), June 2015.
Constantin Udriste. Convex functions and oPtimization methods on Riemannian manifolds, volume
297. Springer SCienCe & Business Media, 2013.
Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovit-
skiy, and Thomas Brox. Demon: Depth and motion network for learning monoCular stereo. In
Proceedings of the IEEE Conference on ComPuter Vision and Pattern Recognition (CVPR), July
2017.
Florian Walch, Caner Hazirbas, Laura Leal-Taixe, Torsten Sattler, Sebas-
tian HilsenbeCk, and Daniel Cremers.	Image-based loCalization using
lstms for structured feature correlation. In ICCV, October 2017. URL
https://github.com/NavVisResearch/NavVis-Indoor-Dataset.
11
Under review as a conference paper at ICLR 2022
Chen Wang, Danfei Xu, YUke Zhu, Roberto Martln-Martln, CeWU Lu, Li Fei-Fei, and Silvio
Savarese. Densefusion: 6d object pose estimation by iterative dense fusion. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3343-3352, 2019a.
Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari.
Self6d: Self-supervised monocular 6d object pose estimation. In The European Conference on
Computer Vision (ECCV), August 2020.
He Wang, Srinath Sridhar, JingWei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas.
Normalized object coordinate space for category-level 6d object pose and size estimation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2642-
2651, 2019b.
Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep represen-
tation for volumetric shapes. In Computer Vision and Pattern Recognition, 2015.
Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond pascal: A benchmark for 3d object
detection in the Wild. In IEEE Winter Conference on Applications of Computer Vision, pp. 75-82,
2014. doi: 10.1109/WACV.2014.6836101.
Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neu-
ral netWork for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199,
2017.
Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on
riemannian manifolds. arXiv preprint arXiv:1605.07147, 2016.
Yi Zhou, Connelly Barnes, JingWan Lu, Jimei Yang, and Hao Li. On the continuity of rotation
representations in neural netWorks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 5745-5753, 2019.
12
Under review as a conference paper at ICLR 2022
A	Addition on preliminaries
A.1 More on Riemannian Geometry
In this part, we supplement the definitions in Section 2.3 of the main paper to allow for a slightly
more rigorous specification of the exponential map for interested readers.
We denote the union of all tangent spaces as the tangent bundle: TM = ∪x∈MTxM. Riemannian
metric Gx induces a norm kukx , ∀u ∈ TxM locally defining the geometry of the manifold and
allows for computing the length of any curve γ : [0, 1] → M, with γ(0) = x and γ(1) = y as
the integral of its speed: '(γ) = R1 kY(t)kγ(t)dt. The notion of length leads to a natural notion of
distance by taking the infimum over all lengths of such curves, giving the Riemannian distance on
M, d(x, y) = infY '(γ). The constant speed length minimizing curve Y is called a geodeSic on M.
By the celebrated Picard LindeIOf theorem (Coddington & Levinson, 1955), given any (x, V) ∈
TM, there exists a unique maximal1 geodesic Yv such that Yv(0) = X and YV(0) = v. Hence, We
can define a unique diffeomorphism or exponential map, sending x to the endpoint of the geodesic:
Expx(v) = Yv(1). We Will refer to the Well-defined, smooth inverse of this map as the logaritmic
map: Logxy , Expx-1 (v). Note that the geodesic is not the only Way to move aWay from X in the
direction of v on M. In fact, any continuously differentiable, smooth map Rx : TxM 7→ M Whose
directional derivative along v is identity, i.e. DRx(0)[v] = v and Rx(0) = X alloWs for moving on
the manifold in a given direction v. Such Rx, called retraction, constitutes the basic building block
of any on-manifold optimizer as We use in the main paper. In addition to those We also speak of a
manifold projector π : X 7→ M and a tangent Space projector Πx : X 7→ TxM both available for
the manifolds We consider in this paper. Note that, most of these definitions directly generalize to
matrix manifolds such as Stiefel or Grassmann (Absil et al., 2009).
A.2	10D symmetric matrix representation.
(Peretroukhin et al., 2020) presents an alternative representation ofa quaternion: instead of predict-
ing a 4D vector, q is over-parameterized by a 4 × 4 symmetric matrix. The eigen-vector of this
matrix With the smallest eigenvalue constitutes the map from this representation into a unit quater-
nion. We find that this variant of quaternions imposes obstacles in finding the inverse image of the
manifold mapping.
1 maximal refers to the fact that the curve is as long as possible.
13
Under review as a conference paper at ICLR 2022
Figure 4: Illustration for regularized projective manifold gradient. First We project X to X by
π , and compute a Riemannian gradient, which is shown as the green arrow. After getting a next
goal Xg ∈ M by Riemannian gradient, We find the inverse projection Xgp of Xg, which leads to our
projective manifold gradient, shown as the blue arrow. With a regularization term, we can get our
final regularized projective manifold gradient, as the purple arroW.
B PROJECTIVE MANIFOLD GRADIENT ON SO(3)
B.1 DETAILS OF RIEMANNIAN OPTIMIZATION ON SO(3)
Riemannian gradient on SO(3). Since We mainly focus on the SO(3) manifold in this paper, We
Will further shoW the specific expression of some related concepts of SO(3) beloW.
Firstly, SO(3) is defined as a matrix subgroup of the general linear group GL(3):
SO(3) = {R ∈ R3×3 : R>R = I, det(R) = 1}.	(6)
The tangent space of a rotation matrix in SO(3) is isomorphic to R3 making SO(3) an embedded
submanifold of the ambient Eucldiean space X . Hence, SO(3) inherits the metric or the inner
product of its embedding space, X. Since SO(3) is also a Lie group, elements of the tangent space
Ω ∈ TIM can be uniquely mapped to the manifold M through the exponential map:
EXpI (Ω) = I + Ω + 21!(Ω)2 + ；(。)3 + ... .	(7)
where I ∈ SO(3) is the identity matrix. In addition, Ω can be mapped from an element ω =
(ωx , ωy , ωz ) in Euclidean space R3 through a skeW-symmetric operator ΠR : R3 → TRM as
0	-ωz	ωy
ωz	0	-ωx	(8)
-ωy	ωx	0
If the vector ω is rewritten in terms of a unit vector ω and a magnitude θ, the exponential map can
further be simplified as
EXpI(Πi(ω)) = EXpI(Πi(θω)) = I + sinθΠI(ω) + (1 - cosθ)(ΠI(cu))2	(9)
which is well known as the Rodrigues formula (Rodrigues, 1840). Due to the nature of the Lie
group, we can expand the formula in eq. (8) from the tangent space of the identity, TIM, to TRM
by simply multiplying by an R:
ExpR(∏r(ω)) = R(EXpI(Πr(ω))) = R(I + sinθΠR(ω) + (1 - cosθ)(∏R(ω))2)
Following (Taylor & Kriegman, 1994), we have
∂ωχ EXpR (nR 3)J0 = r ∂ωχ (X (>πR (ω))n。=RnR(X)
where X = (1, 0, 0) ∈ R3. For ωy and ωz, there are the similar expressions of the gradient.
Therefore,
gradf(R) = Πr(Vf(R)) = Πr(Vω) = Πr (fR)EXpR(∏R⑷)θ)
(10)
(11)
14
Under review as a conference paper at ICLR 2022
Riemannian gradient descent on SO(3). We are now ready to state the Riemannian optimization
in the main paper in terms of the exponential map:
Rk+1 = ExPRk(-Tk Vω).
(12)
Note that if we consider the most commonly used L2 loss f(R)
||R - Rgt||2F , where
a1 b1 c1	x1
R = a2 b2 c2 ∈ SO(3) and	Rgt = x2
a3 b3 c3	x3
y1 z1
y2 z2	∈ SO(3),
y3 z3
we can get an analytical expression of Vω = (Vωx , Vωy , Vωz) as follows:
Vωχ = fR * RnR(X)
a1 - x1
a2 - x2
a3 - x3
b1 - y1
b2 - y2
b3 - y3
c1
c2
c3
---bbb123!1
3
2 *	(bi * zi - ci * yi)
(13)
Similarly, we have Vωy
2 * Pi3=1(ci * xi - ai * zi) and Vωz = 2 * Pi3=1(ai * yi - bi * xi).
τsafe in ablation study. We have mentioned in the main paper that τ should neither be too large
nor too small. To be more specific, what we want is a small τ at the beginning of training and a
large τ when converging. This is because a small τ can yield Rg closer to R and greatly alleviate
the problem discussed in Section 3.3 at the beginning stage of training. Later in training, a large τ
can help us converge better. The initial τ will not influence the final results too much, and we just
need to choose a reasonable value. But the final τ matters.
Right before convergence, our ideal choice for the final τ would be τgt . Given that the value of τgt
will change according to the geodesic distance between R and Rgt , we instead can find a suitable
constant value to act like τgt when converging, which we denotes as τsafe.
Lemma 1. The final value of τsafe satisfies:
Rgt =	lim	RR(-τsafe grad L(f (R)))	(14)
<R,Rgt >→0
where < R, Rgt > represents the angle between R and Rgt.
Proof. Considering the symmetry, without loss of generality, we assume that R = I. This will
simplify the derivation. Based upon the conclusion in eq. (13), when we use L2 loss, we have
Vω= (2*(z2-y3),2*(x3-z1),2*(y1 - x2)) and grad Lf (R) = ΠR(Vω) = 2(Rg>t - Rgt).
Taking the manifold logarithm of both sides, we get:
LogR (Rgt) =	lim	-τsafegrad Lf (R)	(15)
<R,Rgt>→0
The solution for τsafe can then be derived as follows:
lim - LogR(Rgt) = iim - LogI(Rgt) = iim -	nI(ωgt)
<R,Rgt>→0 grad Lf (R)	<I,Rgt>→0	grad Lf (I)	θ→0 - 2(Rgt - Rgt)
__________________________________nI (ωgt)___________________________________
2((I + sinθΠI(ωgt)> + cosθ(ΠI(ωgt)>)2) - (I + sinθΠI(ωgt) + cosθ(ΠI(ωgt))2))
lim -
θ→0
_________nI(ωgt)_________
2sinθ(ΠI(ωgt)> - ΠI(ωgt))
lim -
θ→0
lim工
θ→0 4sinθ
1
4
(16)
where ΠI(ωgt) = LogI(Rgt) = θΠI(ωgt) and θ =< I, Rgt >
□
15
Under review as a conference paper at ICLR 2022
Note that when We use a T ≤ 4, Rg will always be closer than Rgt to R. So We set T = 4 as our
upper bound of T schedule, and call 4 as Tsafe. Note that this is only true for the L2 loss.
B.2 Derivations of Inverse Projection
For different rotation representations, we follow the same process to find its inverse projection: we
first find the inverse image space π-1(xg), then project x to this space resulting in xgp, and finally
get our (regularized) projective manifold gradient. Please refer to Figure 1 for this process.
Quaternion We need to solve
qgp = argmin ||q - qg||22,	(17)
qg ∈π-1(qg)
where q is in ambient space and qg is the next goal in representation manifold. Recall π-1(Xg) =
{x | X = kXg, k ∈ R and k > 0}, and we can have
l∣q - qg 112 = q2 - 2kq ∙ qg + k2qg	(18)
Without considering the condition of k > 0, We can see when k = qq- = q ∙ qg the target formula
qg
reaches minimum. Note that when using a small T, the angle between qg and q is always very small,
which means the condition of k = q ∙ qg > 0 can be satisfied naturally. For the sake of simplicity
and consistency of gradient, we ignore the limitation of k no matter what value T takes. Therefore,
the inverse projection is qgp = (q ∙ qg )qg.
6D representation We need to solve
[ugp, vgp] = argmin (||u - ug||22 + ||v -vg||22)	(19)
[u- ,v-]∈π-D1([ug Xg ])
where [u, v] is in ambient space and [ug, Vg ] is the next goal in representation manifold. Recall
∏6d([Ug, Vg]) = {[kιUg, k2Ug + k3Vg] | kι, k2, k3 ∈ R and k1,k3 > 0}. We can see that Ug and
vg are independent, and ug is similar to the situation of quaternion. So we only need to consider the
part of Vg as below:
l∣v - Vg||2 = V2 + k2ug + kvg - 2k2v ∙ Ug - 2k3V ∙ Vg	(20)
For the similar reason as quaternion, we ignore the condition of k3 > 0 and we can see when
k2 = V ∙ Ug and k3 = V ∙ Vg, the target formula reaches minimum. Therefore, the inverse projection
is [ugp, Vgp] = [(u ∙ Ug)Ug, (v ∙ Ug)Ug + (v ∙ Vg)Vg]
9D representation For this representation, the situation is quite different from the above two and
obtaining the inverse image π9-D1 is not so obvious. Recall π9D (M) = UΣ0V>, where U and
V are left and right singular vectors of M decomposed by SVD expressed as M = UΣV>, and
Σ0 = d(1, 1, det(UV>)). To find a suitable π9-D1 , the most straightforward way is to only change
the singular values Σg = d(λ0, λ1, λ2), where λ0, λ1, λ2 can be arbitrary scalars, and recompose
the Mg = UΣgV> . However, we argue that this simple method will fail to capture the entire set
of {Mg} that would satisfy π9D(Mg) = Rg ∈ SO(3). This is because different U0 and V0 can
yield the same rotation Rg . In fact, Ug can be arbitrary if Mg = Ug ΣgVg> and UgΣ0gVg> = Rg .
Assuming Rg is known, we can replace Vg> by Rg and express Mg in a different way: Mg =
Ug Σg ∑1r U-IRg. Notice that Ug Σg ∑r U-1 must be a symmetry matrix since Ug is an orthogonal
matrix. Therefore, we get π9-D1(Rg) = {SRg | S is an arbitrary symmetric matrix}. Note that such
Mg ∈ π9-D1(Rg) can’t ensure π9D(Mg) = Rg, because in the implementation of SVD, the order
and the sign of three singular values are constrained, which is not taken into consideration.
We need to solve
Mgp=	argmin (||M-Mg||22)	(21)
Mg∈π9-D1(Rg)
16
Under review as a conference paper at ICLR 2022
We can further transform this optimization objective as follows
33
||M - Mg||22 = ||M - SRg||22 = ||MRg> - S||22 = X X(mij - sij)2
i=1 j=1
3 i-1	3
=	((mij - sij) + (mji - sij) ) +	(mii - sii)
i=1 j=1	i=1
3 i-1	3
= XX(2si2j - 2sij (mji + mij) + mi2j + mj2i) + X(mii - sii)2	(22)
i=1 j=1	i=1
where S = (sij)i,j=1,2,3 and MRg> = (mij)i,j=1,2,3.
{mij + mji
2
mii
(i 6= j)
(i = j)
, in other words, when S equals to
the symmetry part of MRg>, the target formula reaches minimum. Therefore, the inverse projection
admits a simple form Mgp = MRg +Rg M Rg.
C PROJECTIVE MANIFOLD GRADIENT ON S2
C.1 RIEMANNIAN OPTIMIZATION ON S 2
Our methods can also be applied for the regression of other manifolds. Taking S 2 as an example,
which is included in the experiment part of our main paper, we will show the detail of how our
projective manifold gradient layer works in other manifolds.
During forward, The network predicts a raw output X ∈ R3, which is then mapped to X ∈ S2 through
a manifold mapping π(x) = x/|x|. Here we don’t define the rotation mapping and representation
mapping, and we directly compute the loss function on representation manifold S2 .
During backward, to apply a Riemannian optimization, we first need to know some basic concepts
of S2. The tangent space of an arbitrary element X ∈ S2 is TxM, which is a plane. And we can
map a geodesic path V ∈ TxM to an element on the manifold S2 through Eχp^ (V)= cos(||v||)X +
sin(l∣v∣l)∣∣V∣∣, where ∣∣.∣∣ means the ordinal Frobenius norm.
For the definition of the mapping Πχ, which connects Euclidean space R2 and the tangent space
TxM, we need to first define two orthogonal axes Ci, ^ in the tangent plane. Note that the choice of
Ci and ^ won,t influence the final result, which will be shown soon after. To simplify the derivation,
we can assume ground truth unit vector Xgt is known and choose Ci
and C2 = X X Ci. Then we can say Πχ(ω) = ωι^ι + ω2C2, where ω
of exponential mapping with respect to ω is
Log^(xgt)	_ Xgt-(Xgt∙x)
IILog^ (xgt)ll — UXgt-(Xgt∙x)U
(ωi, ω2) ∈ R2. The gradient
∂
而EXPX(nx3) ω=0
石(cos(llω1 6i||)X + sin(llωicil1) ∣∣ω^∣∣ )ω=0 = 61
(23)
Similarly, we have 会EXPX(∏^(ω))l	= C2.
2	ω=0
When using L2 loss, we can have
gradf(X) = Π^ (Vf(X)) = ∏x (Vω) = ∏x ("；? ∂L Eχp^ (∏x (ω))	)
=Π^((2(X — Xgt)Ci, 2(X — Xgt)C2»
= Π^((-2∣∣(X ∙ Xgt)X - Xgt∣∣, 0))
=2((X ∙ Xgt)X — Xgt)	(24)
17
Under review as a conference paper at ICLR 2022
Similar to Eq 16, we can also solve a τsaf e
τsaf e =	lim
<x,Xgt>→0
LOgx(Xgt) = lim θcι = 1
grad Lf(X)	θ→0 2sinθCι	2
(25)
where θ =< x, Xgt >.
Note that in Experiment 4.4, we change the schedule of τ according to this conclusion. We increase
τ from 0.1 to 0.5 by uniform steps.
C.2 Inverse projection
It is exactly the same as quaternion. We can have Xgp = (x ∙ Xg )Xg. For the detail of derivation, see
Sec B.2.
D More experiments
D.1 3D object pose estimation from ModelNet image dataset
In this experiment, we follow the setting in (Levinson et al., 2020) to estimate poses from 2D images.
Images are rendered from ModelNet-10 (Wu et al., 2015) objects from arbitrary viewpoints (Liao
et al., 2019). A MobileNet (Howard et al., 2017) is used to extract image features and three MLPs to
regress rotations. Similarly, we focus on chair and sofa categories which exhibit the least rotational
symmetries in the dataset. Note that we conduct all the experiments by our code rather than quote
the numbers to ensure a fair comparison.
The results are shown in Table 7 and Table 8. Clearly, our RPMG layer boosts the performance of
all three representations significantly. See the curves with the same color for comparison.
Euler
Axis-Angle
Quaternion
6D
9D
9D-Inf
10D
Mean (O) Med (O)	5OAcc (%)
21.46	10.95	10.4
25.71	14.27	7.2
25.75	14.99	6.3
19.60	9.09	19.1
17.46	8.30	23.1
12.10	5.09	49.2
18.40	9.02	19.6
,∙a,5∙4∙a,∙
CJJaIJa CSE
10,
5,∙
RPMG-Quat	13.03	5.90	39.9
RPMG-6D	12.94	4.74	53.1
RPMG-9D	11.93	4.36	58.1	IMIk 2001： 3001： «»k SMk eooi：… ιo% 30κ sow ,o% sow
KeEtlon	Percentlle
Table 7: Pose estimation from ModelNet chair images. We report the same metrics as in Table 1;
see the caption there. All models are trained for 600K iterations.
Euler
Axis-Angle
Quaternion
6D
9D
9D-Inf
10D
Mean(O)	Med(O)	5°Acc(%)
27.46	12.00	9A~
30.25	14.55	6.2
30.00	15.73	5.7
17.51	7.33	27.3
19.75	7.58	24.9
12.48	3.45	69.7
20.89	8.73	19.8
RPMG-Quat	13.02	3.60
RPMG-6D	11.52	2.79
RPMG-9D	10.49	2.41
66.6
77.1
81.7
10
,∙a,5∙4∙a,∙
CJJEa Css
IOOk 200k	300k	«0k
Iteration
uo*
45,
10,
5,
1,
0.1∙∙	.	.	.	.	.
5«Ik	6®k	10%	30%	50%	70«	90%
⅛ιreπtl⅛
Table 8: Pose estimation from ModelNet sofa images. We report the same metrics as in Table 1;
see the caption there. All models are trained for 600K iterations.
D.2 Camera relocalization
The task of camera relocalization is to estimate a 6 Degree-of-Freedom camera pose (rotation and
translation) from visual observations, which is a fundamental component if many computer vision
and robotic applications. In this experiment, we use all the settings (data, network, training strat-
egy, hyperparameters, etc.) of PoseLSTM (Walch et al., 2017) except that we modify the rotation
representations and the gradient layers. We report the results on the outdoor Cambridge Landscape
dataset (Alex Kendall & Cipolla, 2015).
—
18
Under review as a conference paper at ICLR 2022
King’s College Old Hospital Shop Facade St Mary’s Church Average
	T(m)	R(°)	T(m)	R(°)	T(m)	R(°)	T(m)	R(°)	T(m)	R(°)
Euler	1.16	2.85	2.54	2.95	1.25	6.48	1.98	6.97	1.73	4.81
Axis-Angle	1.12	2.63	2.41	3.38	0.84	5.05	2.16	7.58	1.63	4.66
Quaternion	0.98	2.50	2.39	3.44	1.06	6.01	2.59	8.81	1.76	5.19
6D	1.10	2.56	2.21	3.43	1.01	5.43	1.73	5.82	1.51	4.31
9D	1.14	3.03	2.11	3.50	0.88	6.39	1.95	5.95	1.52	4.72
9D-Inf	0.98	2.32	1.89	3.32	1.15	6.36	1.96	6.25	1.50	4.56
10D	1.54	2.62	2.32	3.39	1.20	5.76	1.85	6.69	1.73	4.62
RPMG-Quat	1.04	1.91	2.42	2.72	0.98	4.28	1.82	4.89	1.57	3.45
RPMG-6D	1.55	1.70	2.62	3.09	0.95	5.01	2.44	5.18	1.89	3.75
RPMG-9D	1.57	1.82	4.37	3.12	0.93	4.17	1.92	4.69	2.20	3.45
Table 9: Camera relocalization on Cambridge Landscape dataset. We report the median error
of translation and rotation of the best checkpoint, which is chosen by minimizing the median of
rotation.
Notice that our RPMG layer performs the best on the rotation regression task, but not on the trans-
lation regression. We believe this results from a loss imbalance. We does not change the weights of
the rotation loss and translation loss, otherwise it leads to an unfair comparison with existing results.
We only care about the rotation error here.
D.3 Regression on Other Non-Euclidean Manifolds
In addition to SO(3), our method can also be applied for regression on other non-Euclidean mani-
folds as long as the target manifold meets some conditions: 1. the manifold should support Rieman-
nian optimization. 2. the inverse projection π-1 should be calculable, although it doesn’t need to be
mathematically complete. Here we show the experiment of Sphere manifold S2 .
Unit vector regression For rotational symmetric categories (e.g., bottle), the pose of an object is
ambiguous. We’d rather regress a unit vector for each object indicating the up direction of it. We
use the ModelNet(Wu et al., 2015) bottle point cloud dataset. The network architecture is the same
as in Sec4.1 except the dimension of output is 3.
L2-loss-w/-norm computes L2 loss between the normalized predictions and the ground truth. L2-
loss-w/o-norm computes L2 loss between the raw predictions and the ground truth. MG-3D and
PMG-3D are two variants of our method. See Sec4.1 for details.
Table 10 shows the geodesic error statistics. MG-3D performs on par with L2-loss-w/o-norm, and
PMG-3D leads to a large error since the norm of the predicted vectors is extremely small (103)
without the regularization term. RPMG-3D outperforms all the baselines and variants.
	Mean (°)	Complete Med (°)	1°Acc (%)	Mean(°)	Depth Med (°)	1°Acc (%)
L2 loss w/ norm	8.73	2.71	0.0	6.00	4.41	0.0
L2 loss w/o norm	5.71	1.10	37.4	3.25	1.69	3.0
MG-3D	5.37	1.20	22.2	2.94	2.01	0.0
PMG-3D	21.96	14.79	0.0	32.35	22.56	0.0
RPMG-3D	4.69	0.76	72.7	2.16	1.37	13.1
Table 10: Unit vector estimation from ModelNet bottle complete and depth point clouds. We
report the same metrics as in Table 1 except replace 5° Acc by 1° Acc; see the caption there. All
models are trained for 30K iterations.
E	More Implementation Details
E.1 Experiment 4.1 & 4.3 & 4.4
Data We generate the data from ModelNet dataset (Wu et al., 2015), following the same generation
method as in (Zhou et al., 2019). For complete point clouds, we uniformly sample M rotations for
19
Under review as a conference paper at ICLR 2022
each data point and set them as the ground truth. We apply the sampled rotations on the canonical
point clouds to obtain the input data. For depth point clouds, we render the rotated complete point
clouds to depth images and back-project to partial point clouds.
Network Architecture We use a PointNet++ MSG (Qi et al., 2017) backbone as our feature extrac-
tor. Our network takes input a point cloud with a resolution of 1024. It them performs three set
abstractions to lower the resolution to 512, 128, and finally 1, resulting in a global feature of dimen-
sionality 1024. The feature is finally pushed through a three-layer MLP [1024, 512, N] to regress
rotation, where N is the dimension of the rotation representation.
The learning rate is set to 1e-3 and decayed by 0.7 every 3k iterations. The batch size is 10. For
each experiment, we train the network on one NVIDIA TITAN Xp GPU for 5-7 hours.
F Standard Mapping between Rotation Matrix and Quaternion
The rotation mapping φ : q 7→ R algebraically manipulates a unit quaternion q into a rotation
matrix:
2(q02 + q12 ) - 1
φ(q) =	2(q1q2 +q0q3)
2(q1q2 - q0q3)
2(q02 + q22 ) - 1
2(q1q3 + q0q2)
2(q2q3 - q0q1)
2(q1q3 - q0q2) 2(q2q3 + q0q1)	2(q02 + q32) - 1
where q = (q0, q1,q2, q3) ∈ S3.
In the reverse direction, the representation mapping ψ(R) can be expressed as:
qo = √1 + Roo + R11 + R22
qi = (R21 - Ri2)∕(4 * qo)
q2 = (R02 一 R20)/(4 * qO)
q3 = (R1o - Ro1)∕(4 * qo)
(26)
(27)
Note that q = (q0, q1, q2, q3) and -q = (-q0, -q1, -q2, -q3) both are the valid quaternions pa-
rameterizing the same R.
20