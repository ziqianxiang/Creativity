Under review as a conference paper at ICLR 2022
Towards Safe Reinforcement Learning via
Constraining Conditional Value-at-Risk
Anonymous authors
Paper under double-blind review
Ab stract
Though deep reinforcement learning (DRL) has obtained substantial success, it may
encounter catastrophic failures due to the intrinsic uncertainty caused by stochas-
ticity in both environments and policies. Existing safe reinforcement learning
methods are often based on transforming the optimization criterion and adopting
the variance of the return as a measure of uncertainty. However, the return variance
introduces a bias for penalizing both positive and negative risk equally, deviated
from the purpose of safe reinforcement learning to penalize negative ones only. To
address this issue, we propose to use the conditional value-at-risk (CVaR) as an
assessment of risk, which guarantees that the probability for reaching a catastrophic
state is below a desired threshold. Furthermore, we present a novel reinforcement
learning framework of CVaR-Proximal-Policy-Optimization (CPPO) which for-
malizes the risk-sensitive constrained optimization problem by keeping its CVaR
under a given threshold. To evaluate the robustness of policies, we theoretically
prove that performance degradation under observation disturbance and transition
disturbance depends on the gap of value function between the best state and the
worst state. We also show that CPPO can generate more robust policies under dis-
turbance. Experimental results show that CPPO achieves higher cumulative reward
and exhibits stronger robustness against observation disturbance and transition
disturbance on a series of continuous control tasks in MuJoCo.
1	Introduction
Deep reinforcement learning (DRL) has achieved enormous success on a variety of tasks, ranging
from playing Atari games (Mnih et al., 2013; 2015; 2016), Go (Silver et al., 2016) to manipulating
complex robotics in the real world (Kendall et al., 2019). However, due to the intrinsic stochasticity
in both environments and policies, these methods may result in catastrophic failures (Heger, 1994;
Coraluppi & Marcus, 1999) and the agent may receive significantly negative outcomes. Several
factors can be associated with this phenomenon. One is that traditional DRL only aims at cumulative
reward maximization without considering the Stochasticity of the environment (Garcia & Fernandez,
2015), which may lead to serious consequences with a certain probability and expose our policies to
risk. This can be illustrated briefly in the case of self-driving, where the agent might try to achieve the
highest reward by acting dangerously, e.g. agents may drive along the edge of a curve for reaching
the end with a short time ignoring the danger in driving. Also, the usage of deep neural networks to
construct complicated mappings from a high-dimensional state space S to an action space A in DRL
algorithms can make them vulnerable to adversarial attacks (Huang et al., 2017).
Various efforts have been made on safe reinforcement learning (safe RL) (Heger, 1994; Coraluppi &
Marcus, 1999; Garcia & Fernandez, 2015). Garcia & Fernandez (2015) conduct a comprehensive
survey on safe RL and argue that an array of methods in this area are based on transforming the
optimization criterion by considering the risk of the return. For example, Q-Learning (Heger, 1994)
uses a lower bound to estimate the Q-target in Q-Learning for avoiding the risk; and Geibel &
Wysotzki (2005) propose the expected-value-minus-variance-criterion that subtracts the variance
of the return from the cumulative reward. However, due to the consideration of the worst-case
outcomes (Heger, 1994), one major drawback of those transformed optimization criteria is that
they may lead to overly pessimistic policies, which will focus too much on the worst case and own
poor average performance. Moreover, although variance is a standard measure of the risk of the
policy (Gosavi, 2009; Tamar et al., 2012), it does not distinguish between positive and negative risk
1
Under review as a conference paper at ICLR 2022
and penalizes both equally (SzegO, 2002), deviated form the purpose of safe RL to only penalize
negative ones.
To address the shortcomings of the worst-case outcomes as well as the variance of the return used in
previous objective-modification methods of safe RL (Garcia & Fernandez, 2015; Geibel & Wysotzki,
2005; Heger, 1994), we propose to use conditional value-at-risk (CVaR) for evaluating the risk of
policies. CVaR is a well established metric in economic uncertainty analysis (Alexander & Baptista,
2004; Alexander et al., 2006) and captures the expectation of the random variable to be an outlier with
a given threshold. Unlike variance, CVaR can only capture negative trajectories with relatively low
return. Considering to use CVaR to capture the respectively low return of the trajectory, we naturally
propose to improve the robustness of the on-policy algorithms by CVaR. By integrating CVaR with
Proximal Policy Optimization (PPO) (Schulman et al., 2017), we present a new algorithm called
CVaR-Proximal-Policy-Optimization (CPPO) and notionally analyze policies’ robustness against
different kinds of disturbance. We show that although the observation disturbance and transition
disturbance are structurally different, the performance degradation resulted from each of them is
theoretically dependent on the Value Function Range (VFR), which is introduced as the value function
gap between the best and worst states in this paper. We further show that CPPO can improve the
robustness of policies against observation disturbance and transition disturbance since CVaR can
control the value function of states with relatively low value and further control the VFR value.
Empirically, we compare CPPO to multiple on-policy baselines as well as some previous CVaR-based
methods on various continuous control tasks in MuJoCo (Todorov et al., 2012). Our results show that
CPPO achieves higher cumulative reward in the training stage and exhibits stronger robustness when
we apply perturbations to these environments.
In summary, our contributions are:
•	We analyze the advantages of choosing CVaR as the metric for evaluating the risk of policy
compared with the worst-case outcome as well as the variance of the return. Furthermore,
we propose a constrained optimization problem in order to maximize the cumulative reward
as well as controlling the risk, which can be solved by our CPPO algorithm;
•	We theoretically analyze the performance of trained policies under observation and transition
disturbance, and build a theoretical connection of these two types of structurally different
disturbance. This analysis indicates that our CPPO can improve the robustness of policies;
•	We empirically demonstrate that our method exhibits stronger robustness under observa-
tion/transition perturbations than other common on-policy RL algorithms and previous
CVaR-based RL algorithms in MuJoCo simulator.
2	Background
In this section, we briefly introduce safe reinforcement learning (safe RL) and conditional value-at-risk
(CVaR), which motivate us to adopt CVaR as a metric of risk in safe RL.
2.1	Safe RL
In standard RL setting, the agent interacts with an unknown environment and learns to achieve
the highest long-term return. The task is modeled as a Markov decision process (MDP) of M =
(S , A, R, P , γ), where S and A represent the state space and the action space, respectively; P :
S × A × S → [0, 1] denotes the transition probability that captures the dynamics of the environment;
R : S × A → [-Rmax, Rmax] represents the reward function; and γ is a discount factor. We
use πθ to represent the policy of the agent with parameter θ, which is a mapping from S to A.
At any time step t, the agent perceives current state st ∈ S, chooses its action at ∈ A sampled
from the distribution ∏θ(∙∣sj and obtains a reward rt. All these timesteps consist of a trajectory
T = (so, ao, ri, s1,a1,...). Given an MDP M, the goal of RL is to find the optimal policy ∏θ* with
the highest expected cumulative reward as
∞
maxJ(πθ) , E D(πθ) ,	γtrtπθ
(1)
t=1
where D(πθ) represents the return of the policy πθ, and J(πθ) is the expectation of D(πθ).
2
Under review as a conference paper at ICLR 2022
However, problem (1) only focuses on cumulative reward without considering the risk of the policy,
which may cause catastrophic results (Heger, 1994; Coraluppi & Marcus, 1999). To address this
problem, an array of safe RL methods tend to change the objective in problem (1) in order to
eliminate the uncertainty and avoid the danger. In general, uncertainty can be categorized into two
types, namely, inherent uncertainty and parameter uncertainty (Garcia & Fernandez, 2015). The
inherent uncertainty of RL refers to the transition dynamics in MDP. For example the agent might end
up in completely different situations when repeating its actions from the same starting state. Previous
works (Heger, 1994; Gaskett, 2003) choose the worst-case criterion to address the issue as
max Jinh (πθ) , max min
θ	θ T 〜∏θ
∞
D(τ) , X γtrt
t=1
(2)
A	,	, l' √^ι ɪ	♦ ɪ ɪ	/ t r∖r∖ Λ∖	zA T	∙	∙ ,1 ,1 ∙ 1	,	i'
As a counterpart of Q-Learning, Heger (1994) proposes Q-Learning with the implementation of
(2), and Gaskett (2003) presents β-pessimistic Q-Learning, which adds a parameter β to control
the pessimistic level.
There are also various studies that assess the effectiveness of variance for acquiring safe policies (Sato
et al., 2001; Gosavi, 2009; Tamar et al., 2012). Some previous work (Howard & Matheson, 1972)
considers exponential utility function and formalizes it as the combination of cumulative reward and
the variance of the return V ar(D(πθ)) as
max δ-1 log Eπ [exp(δD(πθ))] = max
θθ
J (∏θ) + 2 Var(D(∏θ)) + O(δ2).
As for the parameter uncertainty of RL, it denotes scenarios where the parameters of the MDP are
unknown or there is a gap between the training and testing environments. Studies conducted by Nilim
& El Ghaoui (2005) and Tamar et al. (2013) assume that the actual transition belongs to a set P and
consider the following problem as
∞
max min Jpar (πθ, P ) , E D(πθ) ,
θ p∈P
γtrt πθ
(3)
t=1
However, previous safe RL methods suffer from serious drawbacks. First, both (2) and (3) are max-
min problems, which do not have general effective solutions and usually have a high computational
complexity. Second, focusing on the worst trajectories may cause over-pessimistic behaviors. For
example, Q-Learning aims to improve the performance under the worst scenario, which can lead
to extremely conservative actions (Heger, 1994). Finally, the direct usage of variance to evaluate
risk is another potential concern because it will penalize not only the possibility of particularly bad
trajectories, but also the good ones, yielding a drop in the agent,s performance (Szegc, 2002).
2.2	CVAR
Value-at-risk (VaR) and conditional value-at-risk (CVaR) are well-established metrics for measuring
risk in economy (Alexander & Baptista, 2004; Alexander et al., 2006). First, we will give the
definition of VaR and CVaR (Chow & Ghavamzadeh, 2014):
Definition 1 (VaR and CVaR). For a bounded-mean random variable Z, the value-at-risk (VaR) of
confidence level α ∈ (0, 1) is defined as:
VaRα (Z) = min{z|F (z) ≥ α},	(4)
where F(z) = P(Z ≤ z) is the cumulative distribution function (CDF); and the condition value-
at-risk (CVaR) of confidence level α is defined as the expectation of the α-tail distribution of Z
as
CVaRa(Z) = Ez〜Z{z∣z ≥ VaRa(Z)}.	(5)
It is easy to prove that (Chow et al., 2015):
lim CVaRa(Z) = max(Z).	(6)
a→1-
Previous works have attempted to use CVaR to analyze the risk-MDP, which considers cost function
C rather than reward function R. Chow & Ghavamzadeh (2014) and Chow et al. (2017) propose
gradient-based methods like policy gradient and actor critic to optimize loss of MDP as well as
keeping the CVaR under certain value. They also propose methods based on value iteration and
Bellman equation to deal with the optimization of risk-MDP with CVaR (Chow et al., 2015). However,
these works ignore the reward in MDP and thus cannot be directly used in RL settings.
3
Under review as a conference paper at ICLR 2022
3	Methodology
We now present our method that maximizes the expected reward while restricting the risk of the
policy. We focus on increasing the agent’s performance on relatively worse trajectories, which loosens
the max-min problem to an constrained optimization problem. Moreover, we can make our policy
less conservative by modifying the parameter α in CVaR. Compared with variance, CVaR is a better
metric for measuring risk, because it, by definition, captures only bad trajectories.
3.1	Problem Formulation
In standard RL, what we receive is the reward signal rather than the risk signal, thus we can only
evaluate the risk of a trajectory by its return. For simplicity, we suppose there exists a decreasing
smoothing function f : R → R with its inverse function f-1 and the risk ofa trajectory τ is f (D(τ)).
For example, the most simple case is that we can use the opposite number of the return to define the
risk of the trajectory, i.e. f (D(τ)) = -D(τ).
First we propose Theorem 1 as below to calculate VaR and CVaR of f (D(τ)):
Theorem 1. For any given policy πθ and its cumulative reward D(πθ), we have:
VaRα(f(D(∏θ))) = min{z∣F0(∏θ)(f-1(z)) ≤ 1 - α}
CVaRα(f(D(∏θ))) = Ez〜d(∏θ){f(z)|f(z) ≥ VaRα(f(D(∏θ))}.
Specially, we consider to take the opposite number of the return of a trajectory as its risk, i.e.
f (D(πθ)) = -D(πθ) and we can prove that
-VaRα(-D(∏θ)) = max{z∣F0(∏θ)(z) ≤ 1 - α},
-CVaRa(-D(n6)) = Ez〜D(∏θ){z|z ≤ -VaRα(-D(πθ))}.
Based on equation (6), we have:
lim -CVaRα (-D (πθ)) = min(D (πθ)),	(7)
α→1-
and if we assume that -CVaRα (-D(πθ)) ≥ β, then we have:
P (D(πθ) ≤ β) ≤ 1 - α.
The proof of Theorem 1 is in Appendix B.1. By this theorem, we can use -CVaRα(-D(πθ)) to
represent the expected reward of the trajectories generated by πθ with relatively lower reward.
As mentioned in Section 2.1, some safe RL objectives, such as problems (2) and (3), are intractable
max-min problems. However, with the property in Eq. (7) of CVaR, we can equally transform problem
(2) as
max Jinh (πθ) = max lim [-CVaRα(-D(πθ))] .	(8)
θ	θ α→1-
We can further loosen problem (8) by assigning α a fixed value, which reforms the original max-min
problem into a solvable optimization problem. Furthermore, to address the pessimism in safe RL, we
balance between the standard RL objective (1) and the safe RL objective (8) after relaxation, which
yields the constrained optimization problem as
max J(πθ)
θ	(9)
s.t. - CVaRα (-D(πθ)) ≥ β,
where α, β are hyper-parameters and we denote the best policy of problem (9) as πc(α, β).
Now we discuss some properties of πc(α, β). Since πc(α, β) is the optimal solution of (9) and
satisfies the constraints. By Theorem 1, we naturally have
P (D(πc(α, β)) ≤ β) ≤ 1 - α,
which means that we can guarantee the probability of policy πc(α, β) generating low-reward trajecto-
ries is below a desired threshold.
Compared with the best policy πs of the standard RL problem (1), πc(α, β) is the policy that
maximizes the expected total reward in a restricted region related to hyper-parameters α, β . Obviously
we have J(πc(α, β)) ≤ J(πs). However, we can also give a lower bound of J (πc(α, β)) as follows:
4
Under review as a conference paper at ICLR 2022
Theorem 2. Assume there exists a constant M > 0 and every trajectory τ = (S0, A0, R1 , S1, A1, ...)
satisfies	t∞=1 γtRt ≤ M, we have
J(πs) - αM
J(∏c(α,β)) ≥ =------------.
1-α
The key of the proof is to consider whether πs satisfies our constraint and the detailed proof of
Theorem 2 can be found in Appendix B.2. Therefore, although πc(α, β) is in a restricted region, its
expected cumulative reward will be no worse than the lower bound we prove in Theorem 2.
3.2 Optimization and Algorithm
We now simplify the constrained problem (9) to an unconstrained one. First, with properties of CVaR,
we can equivalently reformulate problem (9) as
min -J(πθ)
θ,ν
1	+	(10)
s.t. — V +  --E[(-D(∏θ) + V)+] ≤ -β.
1-α
The deviation is provided in Appendix B.3. Then, by using Lagrangian relaxation method (Bertsekas,
1997), we need to solve the saddle point of the function L(θ, V, λ) as
max min L(θ, ν, λ)，-J(∏) + λ - -ν +-----E[(-D(∏θ) + V)+] + β j .	(11)
λ≥0 θ,ν	1 - α
For solving problem (11), we will extend Proximal Policy Optimization (PPO) (Schulman et al.,
2017) with CVaR and propose our algorithm named CVaR Proximal Policy Optimization (CPPO). In
particular, the key point of Policy Gradient methods is to evaluate the gradient (Sutton et al., 2000) of
the objective. Here, we use methods in (Chow & Ghavamzadeh, 2014) to compute the gradient of our
objective function (11) with respected to V, θ, λ as below:
∂νL(θ, V, λ) = -λ +	— Eξ〜∏θ1{ν ≥ D(ξ)})	(12)
1-α
VθL(θ,ν,λ) = -Eξ〜∏θ(Vθ logPθ(ξ)) (D(ξ)-占(-DE + V)+)	(⑶
VλL(θ, v, λ) = -V +  --Eξ〜∏θ (—D(ξ) + V广 + β	(14)
1-	α θ
The key of the deviation is to deform the objective in problem (11) as the integration of trajectories and
the detailed calculation is in Appendix B.4. Moreover, with the increasing of policies’ performance
during training, it’s unreasonable to fix β to constrain the risk of the policy. Thus we consider to
modify β as a function of the risk of trajectories in the current epoch. Based on PPO and the algorithm
by Chow & Ghavamzadeh (2014), we can use the gradient given above to develop an on-policy
algorithm called CPPO (see Algorithm 1 in Appendix A).
4	Theoretical Analysis
In this section, we analyze the robustness of policies against observation and transition perturbations,
and explain why CVaR can improve the robustness of policies.
4.1	Performance against observation Disturbance
For any MDP M and given policy π , we denote its expected cumulative reward and value function as
JM(π) and VM,π, respectively. We define the Value Function Range (VFR) to capture the gap of the
value function between the best state and the worst state as following.
Definition 2 (Value Function Range). For MDP M, we define the Value Function Range (VFR) of
policy π as
VM,∏ = max Vm,∏(s) 一 min Vm,∏(s),	(15)
where VM,π is the value function (Sutton & Barto, 2018) of policy π in MDP M.
5
Under review as a conference paper at ICLR 2022
Moreover, for every state s ∈ M, we can define its discounted future state distribution as dπM (s) =
(1 - Y) P∞=o γtP(St = s∣∏, M). First, We will consider the situation of observation disturbance.
Similar to the setting of SA-MDP (Zhang et al., 2020), we introduce adversary ν : S → S to
describe the disturbance of state and denote the policy disturbed by adversary V as ∏ν, which means
∏ν (∙∣s) = ∏(∙∣ν (s)). We can theoretically calculate and bound the difference of performance between
∏ and ∏ν in Theorem 3 as below:
Theorem 3.	For any policy π and any adversary ν, the reduction of expected cumulative reward of π
against the observation disturbance of ν is:
JM(π) - jM(πν) =τ~^^一
1-γ
1
+「
Es〜dM Ea〜π(IV(S)) ( 1 -
Es〜dM Ea〜π(IV(S)) ( 1 -
π(a∣s)
n(a|V (S))
π(a∣s)
n(a|V (s))
R(s, a).
(16)
Furthermore, an upper bound of it is as follows:
| Jm(∏) - Jm(∏v)| ≤	— maxDTV(∏(∙∣s), π(∙∣ν(S)))Vm,∏
1-γ s
2
+  ----maxDTV(∏(∙∣s),π(∙∣ν(s)) max ∣R(s,a)∣.
(17)
The key of the proof is to analyze the relations of VM,∏ν - VM,∏ with different states and the
complete proof of Theorem 3 is in Appendix B.5, resembling the proof by Kakade & Langford (2002).
Moreover, for the upper bound, Theorem 3 provides a structurally homologous, but tighter bound
than the bound provided in Zhang et al. (2020) since our VFR can be bounded by maxs,a |R(s, a)|,
which is also proven in Appendix B.5. Compared with the victim policy π for given MDP M,
the factors mainly affect the performance of the disturbed policy πV are Total Variation distance
maxs DTV(∏(∙∣s), π(∙∣ν(s)) and the VFR VM,∏. The former one, TV distance, depends on the
victim policy π as well as the disturbance ν and reflects the robustness of the victim policy and
the adversarial ability of the adversary both. However, independent of the adversary, the latter one,
VFR of the policy, only depends on the value functions of π in M, reflecting the robustness of the
victim policy. Thus we can improve the robustness under observation disturbance of the policy by
controlling VFR of the policy.
4.2 Performance against transition Disturbance
Now, we consider the situation of transition disturbance. We assume that the transition P is disturbed
to P and attempt to evaluate the reduction of cumulative reward against the disturbance. Similar to
Theorem 3, we can also theoretically show a similar result as below:
Theorem 4.	For any policy π in MDP M = (S , A, P , R, γ ) and any disturbed environment
M = (S, A, P, R, γ), the reduction of cumulative reward against the transition disturbance is:
JM(π) - JM(π)
P(s0∣s, a)
^ ,. .
P(s0∣s, a)
VM,π(s0).
(18)
Furthermore, an upper bound of the reduction is:
JM(π) - JM(π) ≤
「m,aX DTV (P(IS，a),p(.1s，a))VM,n.
(19)
The proof of Theorem 4 is similar to that of Theorem 3 and is also deferred to Appendix B.5.
Similarly, compared with the victim policy π for a given MDP M, the factors that mainly affect the
performance of π in disturbed environment M are TV distance maxs,。DTV(P(∙∣s, a), P(∙∣s, a))
and the VFR VM,∏. The former one, TV distance, depends on the range of transition disturbance and
reflect the adversarial ability of the adversary, which cannot be controlled by safe RL. Nevertheless,
the latter one VFR only depends on the value functions of π in M and is an intrinsic property of the
victim policy. Therefore, we can improve the robustness of the victim under transition disturbance
policy by controlling VM,π .
6
Under review as a conference paper at ICLR 2022
4.3 Connection between the Ob servation and Transition Disturbance
Observation disturbance and transition disturbance are structurally different, as they affect observation
of the policy and MDP respectively. Although existing literature usually considers them separately,
by Theorem 3 and Theorem 4, we can find out that the effects of them on cumulative reward are
similarly depending on the VFR ^M,∏, which is an inherent property of π and independent of the
adversary. Thus we can improve the robustness of the policy under observation disturbance as well as
transition disturbance by controlling its VFR.
Moreover, We will discuss the connection between controlling the VFR Vm,∏ and CVaR-based
RL. For controlling Vm,∏, it,s more reasonable to maximize min§ Vm∏(S) rather than minimize
maxs VM,π (s). However, as mentioned in Sec 3.1, directly maximizing the value function of the
worst state may cause our policy to be over conservative. Thus it’s more reasonable to loosen
mins Vm,∏(S) to -CVaRa(-V(s)), here S 〜 μ(∙) obeys the initial distribution of the environment.
Our CVaR-based objective (9) imposes a constraint on -CVaRα(-D(τ)) since we can prove that
Theorem 5. For any α ∈ [0, 1], We can prove that -CVaRα(-D(τ)) is a lower bound of
-CVaRα(-V(S)), i.e.
- CVaRα(-D(τ)) ≤ -CVaRα(-V(S))	(20)
The proof of Theorem 5 is deferred to Appendix B.6. Therefore, our CVaR-based methods consider
to constrain -CVaRα(-D(τ)) for improving VFR of the policy and further improve the robustness
of the policy against observation disturbance as well as transition disturbance.
5	Experiments
In this section, we empirically evaluate the performance and the robustness under observation
disturbance and transition disturbance of our method CPPO in a series of continuous control tasks in
MuJoCo (Todorov et al., 2012) against other common on-policy RL algorithms.
5.1	Experiment Setup
Environments. We choose MuJoCo (Todorov et al., 2012) as our experiments environment. As a
robotic locomotion simulator, MuJoCo has an array of different continuous control tasks such as Ant,
Walker2d, HalfCheetah, Hopper, Swimmer and so on, which are widely used for the evaluation of RL
algorithms.
Baselines and Codes. We will compare our algorithm with the common on-policy algorithms and
previous CVaR-based algorithms. For the former, we choose Vanilla Policy Gradient (VPG) (Sutton
et al., 2000), Trust Region Policy Optimization (TRPO) (Schulman et al., 2015) and PPO (Schulman
et al., 2017). For the latter, we implement PG-CMDP (Chow & Ghavamzadeh, 2014) with deep
neural network. And we use Adam (Kingma & Ba, 2015) to optimize all the parameters. The
implementation of all codes, including CPPO and baselines, are based on SpinningUp (Achiam,
2018).
Evaluations. First, we compare the cumulative reward of each algorithm in the training process and
their performance after convergence. For the trained models, in order to measure their robustness
and safety, we compare their performance under transition disturbance and observation disturbance
respectively. For observation disturbance, we apply Gaussian disturbance to the agent’s observation
to study the relationship between the agent’s performance and the magnitude of the disturbance. For
transition perturbation, since MuJoCo is a physical simulation engine and its transition is depend on
its physics parameters, we choose to modify the mass of the agent to change the transition dynamics,
and study the relationship between the the agent’s performance and the mass of the agent.
5.2	Performance in Training Stage
In this part, we compare the performance of our CPPO against common on-policy algorithms as well
as the previous CVaR-based algorithm in MuJoCo environments such as Ant, Halfcheetah, Walker2d,
Swimmer and Hopper. For each algorithm in each task, we train 10 policies with different random
seeds since the environment and environments and policies are stochastic. Table 1 shows the mean
7
Under review as a conference paper at ICLR 2022
Env Method^^^^^^	Ant-v3	HalfCheetah-v3	Walker2d-v3	Swimmer-v3	HoPPer-v3
VPG	12.8± 0.0	-^896.9± 531.1 ^^	628.6± 229.4	48.3± 11.3	888.4± 209.5
TRPO	1625.4± 356.4	2073.8± 741.3	2005.6± 398.7	101.2± 29.3	2391.4± 455.3
PPO	3372.2± 301.4	3245.4± 947.3	2946.3± 944.3	122.0± 7.9	2726.0± 886.0
PG-CMDP	7.4 ± 3.6	928.7± 562.9	596.7± 219.9	55.4± 18.8	1039.2± 21.1
CPPO(ours)	3514.7± 247.2	3680.5± 1121.3	3194.0± 648.2	182.5± 46.0	3144.6± 158.4
Table 1: The cumulative reward (mean ± one std) of best policy trained by VPG, TRPO, PPO and
CPPO in different MuJoCo games. In each column we bold the best performance over all algorithms.
------------VPG
HaIfCheetah
35∞
3000
0
-1∞0
0.0 0.5
1.0	1.5	2.0
Timesteps
PJeMφɑφpo,ffldw
Wa∣ker2d
Swimmer
1.0	1.5	2.0	2.5	3.0
Timesteps 1e6
0.0	0.5	1.0	1.5	2.0	2.5	3.0
Timesteps Iee
CPPO
Hopper
Figure 1: Cumulative reward curves for VPG, TRPO, PPO and our CPPO. The x-axes indicate the
number of steps interacting with the environment, and the y-axes indicate the performance of the
agent, including average rewards with standard deviations.
and variance of the cumulative reward of 10 policies trained by each algorithm in each environment
and we bold the highest cumulative reward over all algorithms. For each algorithm in each task, we
also plot the mean and variance of the ten policies as a function of timesteps in the training stage as
shown in Figure 1. The four subgraphs represent the experimental results on Halfcheetah, Walker2d,
Swimmer and Hopper respectively. The solid line represents the average reward of 10 strategies, and
the part with lighter color represents the variance of them. As we can see from the figure, CPPO
represented by pink has achieved significant performance improvement on HalfCheetah, Swimmer
and Hopper against all baselines. We can also find that our CPPO gain higher cumulative reward of
the worst-case outcome than other baselines on Walker2d.
5.3	Robustness against observation Disturbance in Test Stage
Trained agents may failed in the test stage because of the gap between the observation and the true
state. Consequently, for evaluating the robustness of each algorithm, we add standard Gaussian
disturbance to the observation in the test stage. For this purpose, we plot the performance of the
trained policies under observation disturbance in Figure 2. In each subfigure, the solid line and the
part with lighter color represent the average reward and the variance of 10 strategies respectively.
From the figure, we can found that the performance degradation is positively related to the size of
disturbance, which is shown in Theorem 3. Moreover, since the value function of all states in these
policies are relatively low and VFR of these policies is low, we can discover that VPG and PG-CMDP
stay robustness under observation disturbance, which is shown in Theorem 3. As shown in the figure,
CPPO has made significant progress in Swimmer and Hopper than baselines. Therefore, our CPPO
enables to keep robustness under observation disturbance.
5.4	Robustness against transition Disturbance in Test Stage
Trained agents may also fail in testing stage because of the transition gap between the simulator and
the true environment. Therefore, we evaluate the performance of all algorithms under the transition
disturbance for measuring their robustness and safety. Since MuJoCo is a physics simulator modeled
on the physical world, we can disturb the transition by modifying environment parameters. For this
purpose, we choose to modify the mass of the robot and the default mass of environment HalfCheetah,
8
Under review as a conference paper at ICLR 2022
VPG------------TRPO------------PPO	PG-CMDP
Walker2d	Swimmer
HaIfCheetah
PJeAΛφaφpoffldw
0.0	0.1	0.2	0.3	0.4	0.5
Variance Of Gaussian Noises
0.0	0.1	0.2	0.3	0.4
Variance of Gaussian Noises
PJeΛΛφα:φposldw
----CPPO
3500
Figure 2: Cumulative reward curves for VPG, TRPO, PPO and our CPPO under observation dis-
turbance. The x-axes indicate the range of the disturbance, and the y-axes indicate the average
performance of the algorithm under the state disturbance.
-----------CPPO
Hopper
HaIfCheetah
-----------VPG------------TRPO------------PPO
Walker2d
PG-CMDP
Swimmer
PJeMSaφpo"dw
PJeMa>££φpowdw
Figure 3: Cumulative reward curves for VPG, TRPO, PPO and our CPPO under transition disturbance.
The x-axes indicate the mass of the agent, and the y-axes indicate the average performance of the
algorithm when the mass changes.
Walker2d, Swimmer and Hopper are 6.36, 3.53, 34.6 and 3.53 respectively. Therefore, we draw
Figure 3 to describe the results of agents, which are trained under standard mass condition and tested
under different mass conditions. The solid line represents the average reward of 10 strategies, and
the part with lighter color represents the variance of them. As seen in this figure, the performance
of all algorithms decreases to a certain extent with the change of agent quality (whether it becomes
larger or smaller) and the degree of decline is positively correlated with the quality change, which is
consistent with our theoretical analysis in Theorem 4, that is, the upper bound of the performance
difference of the algorithm is related to the size of the transition disturbance. Similar to the result
under observation disturbance, we can can discover that VPG and PG-CMDP stay robustness under
transition disturbance since their VFR is low, which is also shown in Theorem 4. At the same time, we
can also see that CPPO achieve higher outcome in different tasks, specially in Swimmer and Hopper.
It indicates that our method can improve the robustness of policies under transition disturbance.
6	Conclusions
In this paper, we analyze the advantages of CVaR for evaluating the risk of policy compared with the
worst-case outcome as well as the variance of the return. Furthermore, we consider a risk-sensitive
optimization objective and propose CPPO to solve it. Moreover, we provide theoretical connection of
policies, robustness against observation disturbance and transition disturbance, which are structurally
different. By introducing the notion of value function range (VFR), we indicate that our CPPO can
improve the robustness of policies. Finally, we evaluate our algorithms in various MuJoCo tasks
and show that CPPO obtains better performance as well as stronger robustness than various strong
competitors.
9
Under review as a conference paper at ICLR 2022
Reproducibility S tatement
We ensure the reproducibility of our paper from two aspects. (1) Experiment: The implementation
and result of our experiment are described in Sec. 5. (2) Theory and Method: We provide the pseudo
code of our algorithm in Appendix A. We also provide complete proofs of all the theoretical results
mentioned in the paper in Appendix B.
Ethics S tatement
Deep reinforcement learning may encounter catastrophic failures due to the stochasticity. It is very
imperative to develop safe reinforcement learning algorithms. This paper proposes a CPPO method
to improve the robustness under observation and transition disturbance. Also, this paper provides
theoretical analysis of the connection between observation and transition disturbance. It may promote
the development of safe and reliable reinforcement learning algorithms in the future.
References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018. 5.1
Gordon J Alexander and Alexandre M Baptista. A comparison of var and cvar constraints on portfolio
selection with the mean-variance model. Management Science, 50(9):1261-1273, 2004. 1, 2.2
Siddharth Alexander, Thomas F Coleman, and Yuying Li. Minimizing cvar and var for a portfolio of
derivatives. Journal of Banking & Finance (JBF), 30(2):583-605, 2006. 1, 2.2
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society (JORS),
48(3):334-334, 1997. 3.2
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in mdps. In
Proceedings of the 27th International Conference on Neural Information Processing Systems
(NeurIPS), pp. 3509-3517, 2014. 2.2, 2.2, 3.2, 3.2, 5.1, B.4
Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-
making: a cvar optimization approach. Advances in Neural Information Processing Systems
(NeurIPS), 28:1522-1530, 2015. 2.2, 2.2, B.3
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research
(JMLR), 18(1):6070-6120, 2017. 2.2
Stefano P Coraluppi and Steven I Marcus. Risk-sensitive and minimax control of discrete-time,
finite-state markov decision processes. Automatica, 35(2):301-309, 1999. 1, 2.1
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research (JMLR), 16(1):1437-1480, 2015. 1, 2.1
Chris Gaskett. Reinforcement learning under circumstances beyond its control. Proceedings of the
International Conference on Computational Intelligence for Modelling Control and Automation,
2003. 2.1, 2.1
Peter Geibel and Fritz Wysotzki. Risk-sensitive reinforcement learning applied to control under
constraints. Journal of Artificial Intelligence Research (JAIR), 24:81-108, 2005. 1
Abhijit Gosavi. Reinforcement learning for model building and variance-penalized control. In
Proceedings of the 2009 Winter Simulation Conference (WSC), pp. 373-379. IEEE, 2009. 1, 2.1
Matthias Heger. Consideration of risk in reinforcement learning. In Machine Learning Proceedings
1994, pp. 105-111. Elsevier, 1994. 1, 2.1, 2.1, 2.1
Ronald A Howard and James E Matheson. Risk-sensitive markov decision processes. Management
Science, 18(7):356-369, 1972. 2.1
10
Under review as a conference paper at ICLR 2022
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017. 1
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In In
Proc. 19th International Conference on Machine Learning (ICML). Citeseer, 2002. 4.1
Alex Kendall, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen,
Vinh-Dieu Lam, Alex Bewley, and Amar Shah. Learning to drive in a day. In 2019 International
Conference on Robotics and Automation (ICRA),pp. 8248-8254. IEEE, 2019. 1
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015. URL http://arxiv.org/abs/
1412.6980. 5.1
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013. 1
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015. URL https://doi.org/10.1038/nature14236. 1
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning (ICML), pp. 1928-1937. PMLR, 2016.
1
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780-798, 2005. 2.1
R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. B.3
Makoto Sato, Hajime Kimura, and Shibenobu Kobayashi. Td algorithm for the variance of return
and mean-variance reinforcement learning. Transactions of the Japanese Society for Artificial
Intelligence (JSAI), 16(3):353-362, 2001. 2.1
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning (ICML), pp. 1889-1897.
PMLR, 2015. 5.1
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1, 3.2, 5.1
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016. URL
https://doi.org/10.1038/nature16961. 1
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. 2
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information
processing systems (NeurIPS), pp. 1057-1063, 2000. 3.2, 5.1
Giorgio SzegO. Measures ofrisk. Journal ofBanking & finance (JBF), 26(7):1253-1272, 2002. 1,
2.1
Aviv Tamar, Dotan Di Castro, and Shie Mannor. Policy gradients with variance related risk criteria. In
Proceedings of the 29th International Coference on International Conference on Machine Learning
(ICML), pp. 1651-1658, 2012. 1, 2.1
11
Under review as a conference paper at ICLR 2022
Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. arXiv
preprint arXiv:1306.6189, 2013. 2.1
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012IEEE/RSJ International Conference on Intelligent Robots and Systems,pp. 5026-5033.
IEEE, 2012. 1, 5, 5.1
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations.
Advances in Neural Information Processing Systems (NeurIPS), 33:21024-21037, 2020. 4.1, 4.1,
B.5
12
Under review as a conference paper at ICLR 2022
A Pseudo Code of CPPO
Algorithm 1 CVaR Proximal Policy OPtimization(CPPO)
Require: confidence level α and reward tolerance β, learning rate lrη, lrθ, lrλ, lrφ
Ensure: θ of Parameterized Policy πθ(always be random Policy), φ of Parameterized value function
Vφ.
fork = 1,2, ..., Niter do
Generate N trajectories Dk = {ξi}iN=1 by following the current Policy πθ.
Compute reward Rt of each state si,t in each trajectory ξi and the cumulative reward D(ξi).
Compute advantage estimates At of each state si,t in each trajectory ξi.
Update parameters respectively:
η — n-i『n
(-λ + N∏⅛ XX 1{n ≥ D(ξi)})
θ J θ+lrθ Nt X X vθ min (∏S⅛Γ⅛ At, g(e, At)
1N	λ
-lrθN E(VθlogPθ(ξi))ι--α(-D(ξi)+ n)1{n ≥ D(ξi)}
i=1
λ J λ+irλ(-n+ p⅛⅜ n)+ + β!
N(1 - α)
1NT
φ J φ+lrφ (NT	ΣS2(VΦ(si,t) - RRyV ΦVφ(si,t))
Modify β as a function of the return of current trajectories:
β J g(ξ1, ξ2, ..., ξN)
end for
B	Proofs of Theorems
In this section, we will provide the proofs of theorems proposed in the paper.
B.1 The Proof of Theorem 1
Proof. By definition of VaR and CVaR, we have:
Ff(D(πθ)) (z) = P (f (D(πθ)) ≤ z) = P (D(πθ) ≥ f-1(z)) =1 - FD(πθ) (f -1(z)),
VaRα(f(D(∏θ))) = min{z∣Ff(D(∏θ))(Z) ≥ α}
= min{z|1 - FD(πθ) (f -1(z)) ≥ α}
= min{z∣FD(πθ)(f-1(z)) ≤1 - α},
CVaRα(f(D(∏θ))) = Ew〜f(D(∏θ)){w∣w ≥ VaRα(f(D(∏θ)))}
=Ez〜D(∏θ){f(z)∣f(z) ≥ VaRα(f(D(∏θ))}∙
13
Under review as a conference paper at ICLR 2022
When we set f(Z) = -Z, we can naturally prove that
-VaRα (-Z) = - min{z |FZ (-z) ≤ 1 - α}
= - min{z|1 - FZ(-z) ≥ α}
= max{-z|1 - FZ(-z) ≥ α}
= max{z|FZ (z) ≤ 1 - α},
-CVaRa(-Z) = -Ez〜Z{—z| — Z ≥ VaRɑ(-Z)}.
=Ez〜Z{z| — z ≥ VaRa(-Z)}
=Ez〜Z{z∣z ≤ —VaRa(-Z)}.
If we assume that -CVaRα (-Z) ≥ β, then we have:
P (Z ≤ β) ≤ P (Z ≤ -CVaRα(-Z))
=P(Z ≤ Ew〜Z{w∣w ≤ —VaRa(-Z)})
= P(Z ≤ -VaRα(-Z))
= P(Z ≤ max{z |FZ (z) ≤ 1 — α})
= 1 — α.
So we have proven it.
□
B.2 The Proof of Theorem 2
Proof. Since We assume M is the upper bound of the total reWard of every trajectory, We have
J(πs) ≤ M. We consider tWo scenarios.
In the first case, if ∏ satisfies that -CVaRa(—D(∏s)) ≥ β. Obviously, We have ∏c(α, β)
thus
πs
J(πs) — αM
J(πc(α, β)) = J(πs) ≥
1 — a
Otherwise, We assume that -CVaRa(-D(∏s)) < β, Since -CVaRa(-D(∏c(α, β))) ≥ β, we set
B = -VaRa(-D(∏c(α,β))) and have:
J (πc(α, β)) =	p(τ)D (τ)dτ
T 〜∏c(α,β)
=	p(τ)D(τ)dτ +	p(τ)D(τ)dτ
≥ - αCVaR(-D(πc (α, β))) +	p(τ)B dτ
≥Aα + A(1 — α)
=β.
By the similar Way, We set:
A = — VaRa(—D(∏θ)) = max{z∣Fo(∏θ)(z) ≤ 1 - α},
thus
J(πs) =	p(τ)D (τ)dτ
T〜∏s
=	p(τ)D(τ)dτ +	p(τ)D (τ)dτ
=	p(τ )Adτ +	p(τ )M dτ
= A(1 - α) + Mα
< β(1 - α) + Mα
≤ J (πc(α, β))(1 - α) + Mα.
So we have proven J (∏c(α, β)) ≥ ,("sfM.
□
14
Under review as a conference paper at ICLR 2022
B.3	The Proof of Equivalently Deforming problem (9)
In this part, we will equivalently deforming problem (9) as
max J(πθ)
⇔ min -J(πθ)
⇔1 min -J(πθ)
θ
⇔ min -J(πθ)
⇔ min -J(πθ)
θ,ν
s.t. - CVaRα (-D(πθ)) ≥ β
s.t.CVaRα(-D(πθ)) ≤ -β
s.t. min{ν +	—E[(-D(∏θ) — ν)+]} ≤ —β
s.t. min{-ν + ɪ—E[(—D(∏θ) + V)+]} ≤ —β
s.t. — v +	—E[(—D(∏θ) + ν)+] ≤ —β.
1 — a
Here we derive a formula 1 since CVaR owns the property (Rockafellar et al.; Chow et al., 2015):
CVaRa(Z) =min [η +	E[(Z — n)+]
η∈R 1 — α
(21)
So we have proven it.
B.4	CALCULATING THE GRADIENT OF L(θ, ν, λ)
In this part, we will calculate the gradient ∂ν L(θ, ν, λ), 5θL(θ, ν, λ) and 5λL(θ, ν, λ) of the func-
tion L(θ, ν, λ) by using the methods in (Chow & Ghavamzadeh, 2014):
L(θ, v, λ) = — J(∏θ) + X(—v +  -E[(—D(∏θ) + v)+] + β).
1 — α
First we can expand the expectation as
L(θ,v,λ) = — J (∏θ ) + X(—v +	L E[(—D(∏θ ) + v)+] + β)
1 — α
=XPθ(ξ)D(ξ) — λv + ɪ XPθ@(—D(ξ) + V)+ + λβ.
ι	1 — a
ξξ
We can see that Pθ(ξ) will only depend on θ and ξ, so we have easily calculate the gradient of λ as
VιL(θ, v,λ) = — v + — XPθ(g)(—D(ξ) + v)+ + β
1 — a
ξ
=V+1—α Eξ 〜∏θ (—D(ξ)+v)++β
Then we calculate the gradient of v. Since (D(ξ) — v)+ isn’t differentiable to v at the point of
v = D(ξ), so we consider its semi gradient as
(0	v<D(ξ)
∂ν(—D(ξ) + V)+ = < q(0 ≤ q ≤ 1) V = D⑹
(1	v>D(ξ)
And we can calculate the gradient of v as below:
∂νL(θ, V,λ)= — λ +	— X Pθ(ξ)∂ν(—D(ξ) + V)+
1 — α
ξ
=λ + ɪ XPθ(ξ)1{v > D(ξ)} + -λq- XPθ(ξ)1{v = D(ξ)}
1 — α	1 — α
ξξ
=—λ + 1 ʌ	Xpθ(ξ)1{v ≥ D(ξ)} (Iet q = 1)
1 — a
ξ
=—λ +rλɑEξ〜∏θ1{v ≥ D(ξ)}).
15
Under review as a conference paper at ICLR 2022
Finally, we will calculate the gradient of θ as
VθL(θ, ν,λ) = - X VθPθ(ξ)D(ξ) +	— X VθPθ(ξ)(-D(ξ) + V)+
1-α
ξξ
=X VθPθ(ξ)(-D(ξ) + ɪ(-D(ξ) + v)1{ν ≥ D(ξ)})
1-α
ξ
=X(Vθ logPθ(ξ))Pθ(ξ)(-D(ξ) +	— (-D(ξ) + v)1{v ≥ D(ξ)})
1-α
ξ
=— X(Vθ logPθ(ξ))Pθ(ξ)D(ξ) + X(Vθ logPθ(ξ))Pθ(ξ) λ(v - D(ξ)) 1{v ≥ D(ξ)}
1-α
ξξ
=— Eξ〜∏θ(VθlogPθ(ξ)) (D(ξ)- 1-α(-D(ξ)+ v)+).
So we have calculated these three gradient.
B.5 The Proof of Theorem 3 and Theorem 4
Before proving Theorem 3 and Theorem 4, we first examine a property of dπM :
Lemma 1. For any state s ∈ S, we have:
dM(S) = (I-Y)P(SO = S) + Y X dM(SO) X π⑷S)P(SlS,a).	(22)
s0	a
Proof. Here we’ll prove this lemma. By the definition of dπM(S), we have:
dπM (S) - (1 - Y)P(S0 = S)
∞
=(I- Y) XX γt P(StT = s , St = S|n, M)
∞
=(I- Y) XX Yt+1P (St = S0|n, M)P (St+1 = S|St = s,π, M)
t=0 s0 ∞	(23)
=Y X (1 - γ) X YtP(St = S0∣∏, M) P (si = S∣S0 = S0,∏, M)
s0	t=0
=YXdπM(S0)P(S1 = S|S0 = S0, π,M)
s0
=y X dM(s0) X n(a|S)P(Sls,a).
Thus We have proven it.	□
Now we will prove Theorem 3.
Theorem 3. For any policy π and any adversary ν, we can calculate the reduction of expected
cumulative reward of π against the observation disturbance of ν as
Y
JM(π) - JM (πν ) =1	Y Es 〜dnV Ea 〜∏(∙∣ν(s))
+ 1 - Y Es〜dM Ea〜n(」V(S))
1-
1-
π(a∣s)
n(a|v(s))
π(a∣s)
n(a|v (s))
R(S, a).
(24)
Furthermore, we can give an upper bound of it:
∣JM(∏) - JM(∏ν)| ≤ — maxDTV(∏(∙∣s), π(∙∣v(S)))VM,∏
1-Y s
2
+  ----maxDTV(∏(∙∣s),π(∙∣v(s)) max ∣R(s,a)∣.
(25)
16
Under review as a conference paper at ICLR 2022
Proof. Considering the bellman equation of value function of ∏, ∏ in M, We have:
VM,∏(S) = X π⑷S)IR(S,a) + Y X P(SlS,a0VM,∏(SO)],
VM,∏ν(S) = X n(a|v (S))[R(S,a) + γ X P (S0|S, a)VM,∏ν (S0)].
By subtracting tWo value functions, We can deduce:
VM,∏ν(S)- VM,∏ (S) =Y £(n(a|v (S))- n(a|S)) EP(SlS,a)VM,∏ (SO)
a	s0
+YXn(a|V(S)) XP(SlS,a)(VM,^V(SO)- VM,∏(SO))	(26)
a	s0
+ E[π(a∣ν(S)) —π(a∣S)]R(S, a).
a
Since equation (26) satisfies for every state S, thus We calculate the expectation of equation (26) for
S 〜d^M:
EdM(s)[Vm,∏v(s) - Vm,∏(s)]
s
=y X dM (s) X(n(a|V(S))- n(a|S)) X P(SO|s, a)VM,∏(SO)
s	a	s0
+γ X dM (s) X n(a|V(S)) X P(SlS,a)(VM,∏V(SO)- VM,∏(SO))
s	a	s0
+X dM (s) X[n(a|v (S))-n(a|s)]R(s,a)
sa
=γ X dM (s) X(n(a|V(S))- n(a|S)) X P(SO|s, a)VM,∏(SO)
s	a	s0
+ X(VM,∏ν(SO)- vM,∏(SO)) YXdM(s) Xn(a|V(S))P(SO|s,a)
s0	s	a
+X dM (s) X[n(a|v (S))- n(a|s)]R(s,a).
sa
By Lemma 1, We have:
X dM(s)[Vm,∏v (s) - Vm,∏(s)]
s
=y X dM (s) X(n(a|v (S))-n(a|S)) X P(SlS,a)VM,∏(SO)
s	a	s0
+ X(Vm,∏v(so) - Vm,∏(so)) [dM(so) - (1 - Y)P(S0 = So)]
s0
+X dM (s) X[n(a|v (S))-n(a|s)]R(s,a).
sa
By moving the second term of the right part in (28) to the left part, We can deduce:
(1 - Y) X(VM,∏ν (SO)- VM,∏(SO))P(s0 = SO)
s0
=γ X dM (s) X(n(a|v (S))-n(a|S)) X P(S0|s,a)VM,n(SO)
s	a	s0
+X dM (s) X[n(a|v (S))-n(a|s)]R(s,a),
sa
(27)
(28)
(29)
17
Under review as a conference paper at ICLR 2022
thus:
(1 - γ)( Jm(∏v) - JM⑺)=(1 - Y) ECVMmV (s') - VM,∏(s0))P(so = s0)
s0
=YEdM(S)E(π(α∣V(S))-π(α∣s)) EP(SlS,α)VM,∏(s')
s	a	s0
+£ dM (S)Emaw(S))-π(αs)]R(s,α)
S
a
=YES〜dMEa〜π(∙∣"(s)) ( 1 -
∏⅛(⅛) ES'~p(∙∣s.a)VM,π(s，)
+ES〜dM Ea〜π(∙∣ν(s)) I 1 -
π(a∣S)
π(a∣V(S))
R(s, a).
And we can prove:
Y
JMCn) - JMCnV ) =1-ES 〜4ν Ea 〜∏(∙∣ν(s))
(1 - ∏⅛⅛⅛) ES'~p(∙∣seVM,π(SO)
+ 1 - γ ES〜dM Ea~π(∙∣"(S))
1-
π(a∣S)
n(a∣V(S))
R(s, a).
(30)
SinCe Ea〜π(∙∣V(S)) (1 - π(∕∣ν∣(Sl))
=0, We can subtract a benchmark, which will not affect its value.
Specially, we consider VFR VM,∏ = maxsz VM,π(s') - mins，VM,∏ (s') and we have ∣VM,∏ (s)-
Vm,∏ ∣ ≤ &产 for every state s, thus we can prove that
γ	π(a∣s) IJMCn) - JMg)∣ ≤ 1 - γES〜d^Ea〜π(∙∣ν(S)) 1 - ∏(a∣V(s)) ,1 E	E	1 _ n(a∣s) + T-^^ES~d^Ea~M∙∣V(S)) 1 - π(a∣V(s)) ≤E . E ，一、、I - n(a∣S)	J ES'~P(∙∣s,a)VM,π (s ) - VM,π IR(S,a)∣ VM,π
≤ 1 - γES~d^Ea~"G∣V(S)) 1	∏(a∣V(s)) ,1 E	E	I _	n(a∣S) + 1-^^ES~d^Ea~M∙∣V(S)) 1 - π(a∣V(s))	2 max ∣R(s, a) ∣ s,a
≤ 1--ES〜4j X ∣n(a∣V(S)) - n(a∣s)∣
Ya
VM,π
2
+τ⅛ ES 〜臣 X Ea∣V(S))- n(a∣s)∣ maXIR(s，a)∣
a
=:E,
1-Y
2
+「E
K〜d∏ν maxDtv(π(∙∣s),π(∙∣V(S)))Vm,∏
:〜d∏ν maxDTV(π(∙∣s), π(∙∣V(s)) max ∣R(s, a)∣.
s~dM	s	s,a
(31)
Thus we have proven Theorem 3.
□
18
Under review as a conference paper at ICLR 2022
Furthermore, we will prove that our bound is tighter than the bound in Zhang et al. (2020):
γ
∣Jm(∏) - Jm(∏v)| ≤∑----maxDTV(∏(∙∣s),∏(∙∣ν(S)))VM
1-γ s
+ ----maxDTV(∏(∙∣s), ∏(∙∣ν(S)) max ∣R(s,α)∣
≤工
—1-Y
2
+「
maxDTV(∏(∙∣s),∏(∙∣ν(S))) max ∣Vm,∏(s)|
maxDTV(∏(∙∣s), ∏(∙∣ν(s)) max |R(s, a)|
s,a
(32)
s
s
s
≤
0¾ + τ⅛) max DTV (n(」s)，n(.|v (S)))m,aX |R(s，a)|.
Finally, we will prove Theorem 4 by using the similar method of Theorem 3.
Theorem 4. For any policy π in MDP M = (S ， A， P ， R， γ ) and any disturbed environment
M = (S， A， P， R， γ), the reduction of cumulative reward against the transition disturbance is
JM(π) - JM (n) =	- Es-dπ^i Ea 〜∏ Es'〜户(1 - ,(s/|S，a) ) VM,∏ (S0).	(33)
M 1 - Y M	∖ P(S0∣S,a))
Furthermore, we can give a upper bound of the reduction is therefore
JM(n) - JM(n) ≤ ：;~ɔ- maxDTV(P(1S, a),p(1S, a))VM,∏.	(34)
M 1 - Y s,a
Proof. Similarly, considering the bellman equation of value function of ∏ in M, M, we have
VM,π(S) =	π(a∣S)[R(S, a) + Y	P(S0|S, a)VM,π(S0)],
VM ,∏(S) = X n(a|S)[R(S,a) + γ X P(S0|S,a)VM ,∏(S0)].
By subtracting them, we have
vM,∏(S)- VM,∏(s) =Y Xn(a|S) X(P(SlS,a) - P(SlS,a))VM,∏(SO)
a	s0
+γ X n(a|S) X P(SlS,a)(VM ,∏(s')- VM,∏(SO)).
a	s0
(35)
(36)
Since equation (26) satisfies for every state S, thus we calculate the expectation of equation (26) for
S 〜dM and use Lemma 1:
EdM(S)[VM,∏(s) - vm,∏(s)]
s
=y X dM (s) X n(a|S) X(P(SlS,a) - P(SlS,a))VM,∏(SO)
s	a	s0
+Y X dM(S) X ∏(a∣s) X P(S0∣S, a)(VM,∏(s0) - Vm,∏(so))
s	a	s0
=γ X dM (s) X n(a|S) X(P(SO|s,a) - P(S0|s,a))VM,n(SO)
s	a	s0
(37)
+ ∑(VM,∏(S0) - Vm,∏(so)) YEdM(S)Eπ(a∣s)P(s0∣s,a)
s0	s	a
=y X dM (s) X n(a|S) X(P(S0|s,a) - P(S0|s,a))vM,n(SO)
s	a	s0
+ X(VM,∏(S0) - VM,∏(so)) [dM (so) - (1 - y)P(so = S0)].
s0
19
Under review as a conference paper at ICLR 2022
Similarly, by moving the second term of the right part in (37) to the left part, we can deduce that
(1 - γ) E(VM,∏(s0) - VMn(S0))P(S0 = s0)
s0
=γXdM^(S) Xπ⑷S) X(P(Sls, a)- P(S0|s,⑼VM,∏(SO),
s	a	s0
(38)
thus:
(1-Y)(JM(∏) — JM(∏)) =(I-Y) E(VM,∏(s0) — VM,∏(s0))P(s0 = s0)
s0
=Y X dM(S) X n(a|S) X(P(S0|s, a) - P(S0|s, a))VM,∏(SO) (39)
s	。	s0
P(S0|S, a)	0
=YEs 〜dM E。〜n(.|s)Es0 〜户(∙∣s,a)	1 - P(s0∣s a)	VM,π (S ).
Thus we have proven:
JM(π) - JM(π)
1 7 Es~dM E。〜n(」S)EsO 〜户(∙∣s,α) (1 - ^［，I ) VM,π (SY (40)
1 - γ M	P(S0|S, a)
Similarly, We consider VFR VM,∏ = maxso VM,∏(so) 一 mins，VM,∏ (so) and We have ∣VM,∏ (s) 一
VM
VM,π | ≤ -M2,π for every state s, thus We can prove:
|JM(n) — JM(πν)| ≤7~^~-
1-Y
≤ ɪ
^1 - Y
=Y
1 - Y
=Y
1 - Y
(41)
Es 〜d∏
M
s0
TT7>	TT7>	ΓΛ /C/ I ∖ A / I ∖ ∖ /V
Es〜dbE。〜∏(∙∣s)Dτv(P(∙∣s, α),P(∙∣s, a))VM,∏.
Thus We have proven Theorem 4.
B.6 The Proof of Theorem 5
By Theorem 1, We have
-CVaRa(-V(s)) = Ez〜V(s){z∣z ≤ -VaRα(-V(s))}
=ET{D(τ)|V(so) ≤ -VaRa(-V(s))}
1	(42)
≥ ET{D(τ)∣D(τ) ≤ -VaRa(-D(τ))}
= -CVaRα (-D(τ)).
Here the inequality holds since P(V (S0) ≤ -VaRa(-V (S))) = P(D(τ) ≤ -VaRa(-D(τ))) =
α. Thus We have proven Theorem 5.
□
20