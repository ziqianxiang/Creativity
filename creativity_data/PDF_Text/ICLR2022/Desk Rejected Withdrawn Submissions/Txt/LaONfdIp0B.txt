Under review as a conference paper at ICLR 2022
Theoretical understanding of adversarial
reinforcement learning via mean-field opti-
MAL CONTROL
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial reinforcement learning has been shown promising in solving games
in adversarial environments, while the theoretical understanding is still premature.
This paper theoretically analyses the convergence and generalization of adversar-
ial reinforcement learning under the mean-field optimal control framework. Anew
mean-field Pontryagin’s maximum principle is proposed for reinforcement learn-
ing with implicit terminal constraints. Applying Hamilton-Jacobi-Issacs equation
and mean-field two-sided extremism principle (TSEP), adversarial reinforcement
learning is modeled as a mean-field quantitative differential game between two
constrained dynamical systems. These results provide the necessary conditions
for the convergence of the global solution to the mean-field TSEP. The global
solution is also unique when the terminal time is sufficiently small. Moreover,
two generalization bounds are delivered via Hoeffding’s inequality and algorith-
mic stability. Both bounds do not explicitly depend on the dimensions, norms,
or other capacity measures of the parameter, which are usually prohibitively large
in deep learning. The bounds help characterize how the algorithm randomness
facilitates the generalization of adversarial reinforcement learning. Moreover, the
techniques may be helpful in modeling other adversarial learning algorithms.
1	Introduction
Adversarial reinforcement learning (Uther and Veloso, 1997) has been successfully deployed in
many application areas, including autonomous driving (Behzadan and Munir, 2019; Pan et al., 2019)
and AI gaming (Mandlekar et al., 2017; Pinto et al., 2017; Zhang et al., 2020). Adversarial neural
networks are employed for solving the games in the adversarial environments (Mandlekar et al.,
2017). Moreover, the adversarial neural networks can also improve feature robustness and sam-
ple efficiency (Ma et al., 2018). However, the theoretical understanding of the convergence and
generalization in adversarial reinforcement learning is still premature.
In this paper, we establish the theoretical foundations of adversarial reinforcement learning under the
mean-field optimal control framework (Bensoussan et al., 2013). Our contributions are summarized
as follows.
(i)	Reinforcement learning is modeled from the view of the dynamical system. A new mean-field
Pontryagin’s maximum principle (PMP) (Pontryagin, 1987) is proposed to give the necessary
condition for optimality of this dynamical system. This PMP is extended from E et al. (2019) to
cover the dynamical systems with constraints on the terminal time. This extension helps gen-
eralize the applicable domains to cover control problems with constraints, such as controlling
target (e.g., a vehicle) to reach a certain area.
(ii)	Adversarial reinforcement learning is modeled as a mean-field quantitative differential game
(Pontryagin, 1985); and thus, its corresponding training process is regarded as how to achieve
the optimal control of this game. The mean-field two-sided extremism principle (TSEP) (Guo
et al., 2005) is then presented, which relies on the loss function and terminal constraints. This
mean-field TSEP serves as the necessary conditions of the convergence (or equivalently, the
optimality) of the mean-field quantitative differential game; when the terminal time is small
1
Under review as a conference paper at ICLR 2022
enough, this mean-field TSEP is also a unique solution, and thus serves as the sufficient con-
ditions of the convergence.
(iii)	The learned model of adversarial reinforcement learning is characterized by the viscosity solu-
tion (E et al., 2019) of a mean-field Hamilton-Jacobi-Issacs (HJI) equation (Guo et al., 2005).
We then prove that this viscosity solution is unique. The HJI equation gives a global charac-
terization of adversarial reinforcement learning, while the previously given mean-field TSEP
is a local special case.
(iv)	Two generalization error bounds are proved, which characterize the gap between the expected
mean-field solution and the learned model. The two bounds are obtained from Hoeffding’s
inequality (Pinelis and Sakhanenko, 1986) and algorithmic stability (Mou et al., 2018). The
bounds are of order O e-N and O (1/N), respectively, where N is the number of samples.
They do not explicitly rely on the dimensions, norms, or other capacity measures of the net-
work parameter, which are usually prohibitively large in deep learning. Moreover, the latter
bound helps characterize how the randomness in the training algorithms and the aggregated
step sizes influence the generalization.
Some previous works have been devoted to establish the theoretical foundations of deep learning by
dynamical system viewpoint, since E (2017). Based on the PMP and the method of successive ap-
proximation (Kantorovitch, 1939), new optimization methods are developed by Li et al. (2018); Li
and Hao (2018). Sonoda and Murata (2017) study the continuum limit of training neural networks,
and Chang et al. (2018b;a); Haber and Ruthotto (2017) contribute to the design of network archi-
tecture based on dynamical systems and differential equations. E et al. (2019) propose to employ
mean-field optimal control formulation for explaining deep learning. They prove the mean-field op-
timality conditions of both the Hamilton-Jacobi-Bellman type and the Pontryagin type (Pontryagin,
1987). Similar results are given by Persio and Garbelli (2021) through associating deep learning with
stochastic optimal control (Guo et al., 2005) from the perspective of mean-field games (Lasry and
Lions, 2007). These mean-field results reflect the probabilistic nature of the reinforcement learning.
To the best of our knowledge, this is the first work on developing theoretical foundations for ad-
versarial reinforcement learning. Compared with the existing studies on deep learning theory from
the mean-field optimal control view, this work models a machine learning algorithm as a mean-field
quantitative differential game between two dynamical systems, rather than a single dynamical sys-
tem. Our work may inspire novel designs of optimization methods for adversarial reinforcement
learning. Moreover, the techniques may be of independent interest in modeling other adversarial
learning algorithms, including generative adversarial networks (Goodfellow et al., 2020; Liu and
Tuzel, 2016; Mao et al., 2017), and solving partial differential equations (Zang et al., 2020).
2	Preliminaries
In this section, we bridge reinforcement learning with mean-field optimal control problems. We first
present the optimal control formulation of deep learning as introduced in (Li et al., 2018; Li and
Hao, 2018; E, 2017). A deep residual network with K layers can be represented by
x(k + 1) = x(k) + f(x(k), θ(k)), k = 0,…，K — 1, x(0) = xo, θ(k) ∈ Θ,	(1)
where we get rid of explicit k dependence in f via the usual trick, x0 ∈ Rn is the input data, and
θ(k) is the collection of the parameters in the k-th layer of the deep residual network. In deep
reinforcement learning, the action a(k) is actually a function of the state x(k) and parameters θ(k),
that is a(x(k), θ(k)). The state and action of each time step will affect the state of the next time step
simultaneously, and will bring a reward function L(x(k), a(x(k), θ(k))). By absorbing a(∙) in L, the
reward can be expressed as L(x(k), θ(k)). The final output x(K) of the network may be constrained
by condition g(x(K)) = 0 in some practical problems, and there might be a terminal cost function
Φ(x(K), y0), where y0 represents some known variables corresponding to x0. In deep learning, the
pair (x0, yo) is usually a data point sampled from a distribution μ. Hence, the reinforcement learning
problem seeks the solution of the following problem
K-1
inf
(θ(0),…,θ(K-1))∈ΘK
E(xo,yo)〜μ φ(X(K),yo ) + EL(X(k),e(k))
k=0
s.t. x(k + 1) = x(k) + f (x(k), θ(k)),	k = 0,…，K — 1, x(0) = xo,	g(x(K)) = 0.
(2)
2
Under review as a conference paper at ICLR 2022
To avoid the repeated compositional structure in the difference equation in equation 2, we introduce
the dynamical systems viewpoint and replace the discrete dynamics equation 1 by a continuous
dynamical system in the following. Consider the dynamical system with terminal constraint, the
state equation and target set are
X = f (x, θ), x(0) = xo ∈ Rn, S := {x∣g(x(tf)) = 0},	(3)
where θ ∈ Θ ⊂ Rr is a vector. Then we can define the continuous case of the objective function in
equation 2 as follows
J ⑻=E(xo,yo)〜μ
Φ(x(tf), y0) +
0
tf
L(x(t), θ(t))dt
(4)
where f : Rn × Rr → Rn, θ : [0,tf] → Rr, g : Rn → Rp,p ≤ n, y0 ∈ Rm, Φ : Rn × Rm → R,
L : Rn × Rr → R and tf is the terminal time. We say θ is admissable if θ(t) ∈ Θ for all t ∈ [0, tf].
The optimal control problem is
inf J(θ) s.t. X = f (x,θ),	x(0) = xo, g(x(tf ))=0,	(5)
and the optimal strategy is θ* = arg infθ∈uJ(θ), where
U := {θ : [0, tf] → Θ∣θ is bounded and piecewise continuous, x(tf; θ) ∈ S}.
3	Mean-field optimal control view of reinforcement learning
In this section, we prove the mean-field Pontryagin’s maximum principle with terminal constraints,
which is the necessary condition for the optimality of dynamical systems. Define the Hamilton
function as H(x, θ, ψ) := -L(x, θ) + ψT f(x, θ), where H : Rn × Θ × Rn and ψ ∈ Rn, then we
have the following theorem.
Theorem 3.1 Under the assumption
•	f is bounded, and f and L are continuous w.r.t. θ;
•	f, L and Φ are continuously differentiable w.r.t X, and μ has bounded SuPPort
Let θ* be the optimal strategy, and x*(t) is the corresponding optimal trajectory, then there exists
ψ* : [0,tf] → Rn and ξ ∈ Rp Such that
x"(t) = f(x,θ*(t)),	x*(0) = xo,
Ψ*(t) = -VxH(x*(t),θ*(t),ψ*(t)), ψ*(tf) = -VχΦ(x*(tf),yo) -ξTVxg(X*(tf)),	⑹
E(xo,yo)〜μH (x*(t),θ*(t),ψ*(t))=sup E(x0,yo)〜μH (x* (t) ,θ,ψ* (t))	。。t ∈ [0,tf ].
Theorem 4.1 provide the necessary condition for optimality, which relies on the loss function and
terminal constraints. There is a feedforward ODE, describing the state dynamics under optimal
controls (ΘZ, θd). The evolution of the co-state variable Ψ is defined, characterizing the evolution of
an adjoint variational condition backward in time.
Comparison with existing works. This theorem is a more practical form of Theorem 3 in (E et al.,
2019), which contains implicit terminal constraints. The optimal strategy must globally maximize
the Hamiltonian function for a.e. t ∈ [0, tf]. In fact, here a.e. t ∈ [0, tf] represents the set oft that
makes the optimal strategy continuous with respect to t.
Proof sketch. The proof has two parts: (1) transform the problem into an unconstrained problem
by the Lagrange multiplier method; (2) apply Theorem 3 in (E et al., 2019) to the transformed
problem. A detailed proof is omitted here and is given in Appendix A.1.
3
Under review as a conference paper at ICLR 2022
4 Mean-field quantitative differential game of adversarial
REINFORCEMENT LEARNING
In this section, we model adversarial reinforcement learning as a mean-field quantitative differential
game. In addition to the original deep neural network NNz(∙; θz) : Rn1 → Rn1, an adversarial
neural network NNd(∙; θd) : Rn2 → Rn2 is usually added in adversarial learning, where θz ∈ Θz
and θd ∈ Θd are parameters of NNz and NNd respectively. Assume that both NNz and NNd
are deep residual networks with K layers, thus θz = (θz(0),…，θz(K - 1)) ∈ ΘZ = ΘK and
θd = (θd(0),…，θd(K - 1)) ∈ Θd = ΘK. Similar to Section 3, define the penalty function
L(xz(k), xd(k), θz(k), θd(k)) and the terminal cost function Φ(N Nz (xz0; θz), NNd(xd0; θd), y0),
where y0 ∈ Rm represents some known variable corresponding to xz0 and xd0 .
4.1 Mean-field two-sided extremism principle
In adversarial reinforcement learning, NNz is trained to maximize the loss, while NNd is trained
to minimize the loss. Assume that both NNz and NNd are deep residual networks with the same
number of layers, thus we can formulate the adversarial learning problem as
K-1
ʌinf SUp E(xz0,xd0,yo)〜μ Φ(xz(K),Xd(K),yo) + £ L(xz(k),Xd(k),θz(k),θd(kY)
θz ∈θZ θd∈Θd	L	M	」
subject to
xz(k +1)= xz(k) + fz(xz(k), θz(k)),	k =	0,…，K -	1,	Xz(0) = Xz°,	gz(xz(K))	=	0,
xd(k +1) = xd(k) + fd(xd(k), θd(k)),	k =	0,…，K -	1,	Xd(0) = Xd°,	gd(Xd(K))	=	0.
(7)
Now we consider the dynamical systems viewpoint and translate problem equation 7 into a contin-
uous form. Let the objective function is
J (θz ,θd)= E(xz0 ,Xd0 ,yo∏Jφ(xz(tf ),Xd(tf ),yo)+ Ztt L(Xz (t),Xd(t),θz (t),θd(t))dtl, (8)
0
where θz ∈ Θz	⊂	Rr1, θd ∈ Θd	⊂ Rr2, xz :	[0,tf] →	Rn1,	xd : [0,tf] →	Rn2,
θz : [0,tf] → Rr1,	θd	: [0,tf] → Rr2, gz	: Rn1 → Rp1,	gd : Rn2 →	Rp2,	Φ, Landf are all	func-
tions of appropriate input and output dimensions. Let x = (xzT , xdT )T, x(0) = (xzT (0), xdT (0))T,
f(x, θz, θd) = (fzT (x, θz), fdT (x, θd)T), g(x(tf)) = (gzT (xz (tf)), gdT (xd (tf)))T. Then the state
equation, target set and objective functionals of quantitative differential game are
X = f(x,θz,θd), x(0) = X0, S := {x∣g(x(tf)) = 0},
J (θz ,θd)=E(xo,yo)〜μ
Φ(X(tf), y0) +
0
tf
L(X(t),θz(t),θd(t))dt
(9)
where X : [0, tf] → Rn, g : Rn → Rp, n = n1 + n2 and p = p1 + p2. Define Uz (Ud) as the set
of all admissable θz (θd) that satisfy the terminal constraint gz (Xz (tf)) = 0 (gd(Xd(tf)) = 0). Our
goal is to find the optimal strategy (θ^,θ^) of equation 9 and the corresponding optimal trajectory
x*(t) satisfies
J(θZ, θd) ≤ J(θZ, θd) ≤ J(θz, θd),	∀(θz,θd) ∈Uz X Ud,	(10)
where equation 10 is called the saddle point condition. Now define the Hamilton function of equa-
tion 9 as H(X(t), θz(t), θd(t), ψ(t)) := -L(X(t), θz(t),θd(t)) + ψT (t)f(X(t), θz(t), θd(t)), then
we have the following mean-field TSEP.
Theorem 4.1 Under the assumption,
•	f is bounded and f, L are continuous w.r.t. θz, θd;
•	f,L and Φ are continuously differentiable w.r.t X, and μ has bounded SuPPort
Let (θZ,θ^) ∈ Uz × Ud is the optimal strategy of problem equation 9, x*(t) is the corresponding
optimal trajectory, then there exists ψ* : [0,tf ] → Rn and ξ ∈ Rp such that
4
Under review as a conference paper at ICLR 2022
i)
X *(t) = f(χ,θz,θ 0	x*(0) = X0,
Ψ*(t) = -VxH(x*(t),θZ⑴冏(t),ψ*(t)), ψ*(tf) = -VχΦ(x*(tf),yo)-ξTVχg(x*(tf)),
(11)
ii)
E(x0,yo)〜μH (x*(t),θZ(t)冏(t),ψ*(t))
= sup inf E(x0,yo)〜μH(x*(t),θz ,θd,ψ*(t))	O
θz∈Θz θd∈Θd	(12)
= inf sup E(x0,yo)〜μH(x*(t),θz,θd,ψ*(t)), a.e.t ∈ [0,tf].
θd∈Θd θz∈Θz
Theorem 4.1 introduces the necessary conditions for the convergence of the unique global solution to
the mean-field TSEP, relying on the loss loss function and terminal constraints. Comparing Theorem
4.1 to Theorem 3.1, we can see the main difference is that the maximization condition is turned into
the saddle point condition.
Comparison with existing works. This theorem is a mean-field form of Theorem 7.4.1 in (Guo
et al., 2005) gives general necessary conditions for optimality for our problem.
Proof sketch. The proof has two parts: (1) Fix the optimal strategy θZ and θd respectively, and
transform the problem into two optimal control problems; (2) apply Theorem 3.1 to these two trans-
formed problems and use max-min inequality. A detailed proof is omitted here and is given in
Appendix A.2.
4.2 Small-time uniqueness
Since the necessary condition for optimality has been provided by the TSEP, a natural question is
to understand when the sufficient conditions for optimality can be also provided. In this subsection,
we will establish assumptions which are required for a unique solution of the mean-field TSEP
equations.
Theorem 4.2 Suppose that
•	f is bounded, g is continuously differentiable w.r.t x with bounded and Lipschitz partial
derivatives, μ has bounded support in Rn X Rm;
•	f, L and Φ are twice continuously differentiable w.r.t x, θz and θd with bounded and
LiPschitzpartial derivatives, and ∂f∕∂θz∂θd, ∂L∕∂θz∂θd ≡ 0;
•	H(x, θz, θd, ψ ) is strongly concave in θz, strongly convex in θd and uniformly in x ∈ Rn,
ψ ∈ Rn.
Then for sufficiently small tf, if (θz1, θd1) and (θz2, θd2) are solutions of the mean-field TSEP derived
in Theorem 4.1 and are continuously w.r.t time t, then (θz1, θd1) = (θz2, θd2).
Theorem 4.2 shows that the small tf roughly corresponds to the regime where the reachable set of the
forward dynamics is small, hence the solution is unique. We assume the continuity of θz1, θd1, θz2, θd2
with respect to t in Theorem 4.2. In fact, when θz1, θd1, θz2, θd2 are discontinuous on at most a set with
zero measure, we can also conclude that (θz1(t), θd1(t)) = (θz2(t), θd2(t)) for a.e. t ∈ [0, tf].
Comparison with existing works. To the best of our knowledge, there is no existing method to
prove the uniqueness of TSEP’s solution.
Proof sketch. The proof has two parts: (1) bound the difference of flow-maps driven by two
different controls; (2) apply the first-order optimality condition for (θz1(t), θd1(t)) and (θz2(t), θd2(t)).
A detailed proof is omitted here and is given in Appendix B.
5
Under review as a conference paper at ICLR 2022
4.3	Derivative in Wasserstein space
Now we propose the mean-field Hamilton-Jacobi-Issacs equation, whose solution is a real value
function satisfying the saddle point condition. To begin with, we first introduce the Wasserstein
space and its derivation rules. Let D represent the Frechet derivative on Banach spaces. Namely, if
F : U → V is a mapping between two Banach spaces (U, k ∙ IlU) and (V, k ∙ IlV), then DF(x):
U → V is a linear operator satisfies
IF(x+y) - F(x) - DF (x)(y)IV
Iy IU
→ 0,	as Iy IU → 0.
(13)
Denote X ∈ Rn+m as a random variable, we use the shorthand L2(Ω, Rn+m) for L2((Ω, F, P),
Rn+m ) to represent the set of Rn+m-valued square integrable random variables with respect to a
probability measure P. Then we equip this Hilbert space with the norm IXIL2 := (EIXI2 )1/2 . As
we assumed in the previous section, xo ∈ Rn, yo ∈ Rm are random variables and (xo, yo)〜μ ∈
P2(Rn+m), where P2(Rn+m) denotes the integrable probability measure defined on the Euclidean
space Rn+m. The space P2(Rn+m) can be equipped with a metric by 2-Wasserstein distance
W2(μ,ν) :=inf IkX — Y∣∣L2 X,Y ∈ L2(Ω,Rn+m) with PX = μ,Pγ = νj.
For μ ∈ P2(Rn+m), define ∣∣μ∣∣L2 := (RRn+m ∣∣w∣∣2μ(dw))". Now the variable X ∈
L2(Ω, Rn+m) if and only if its law PX ∈ P2(Rn+m). For any function U : P(Rn+m) → R, we
can lift it into its "extension" U ∈ L2(Ω, Rn+m) (Cardaliaguet, 2012) by U(X) = U(PX), ∀X ∈
L2(Ω,Rn+m). In particular, we have that U is C1 (P2(Rn+m)), if the lifted function U is FreChet
differentiable with continuous derivatives. Since L2 (Ω, Rn+m) can be identified with its dual, if
the FreChet derivative DU(X) exists, by Riesz, theorem, it can be identified with an element of
L2(Ω, Rn+m),
DU(X)(Y) = E[DU(X) ∙ Y], ∀Y ∈ L2(Ω,Rn+m).
One may check that the law of DU(X) does not depend on X but only on the law of X, thus the
derivative of U at μ = PX is defined as DU(X) = ∂μu(Pχ)(X), for some function ∂μu(Pχ):
Rn+m → Rn+m .
4.4	Mean-field HJI equation
Without losing generality, we consider the quantitative differential game (equation 9) with S = Rn.
Motivated by the mean field dynamic programming principle (E et al., 2019) that any last part of
an optimal control is optimal, we consider the quantitative different game with any t0 ∈ [0, tf] as
initial time
x(t) = f (χ(t), θz(t),θd(t)), x(to) = Xto ∈ Rn, S = Rn,
JH (θz, θd,t0, μ) = E(xto ,yo)~μ
Φ(x(tf),y0) +
tf
t0
L(x(t),θz(t),θd(t))dt
(14)
Our goal is to find the optimal strategy (θZ,θ^) ∈ Uz XUd of equation 14 satisfies
JH(θZ,θd,t,μ) ≤ JH(θZ,θ2,t,μ) ≤ JH(θz,θ2,t,μ), ∀t ∈ [0,tf], (θz,θd) ∈Uz ×Ud. (15)
Define v*(t,μ) := JH(θz,θ^, t, μ), we have the following theorem.
Theorem 4.3 Under the assumption
•	f,L and Φ are bounded, and the distribution μ ∈ P2(Rn+m);
•	f, L and Φ are Lipschitz continuous w.r.t x and the Lipschitz constant of f and L are
independent of θz, θd.
Suppose the optimal value function v*(t, μ) of equation 14 exists, then it is the unique viscosity
solution (see definition in appendix C) to the following mean-field HJI equation
∂tv(t,μ) + inf sup ∖ /	[∂μv(t,μ)(x,y)]T[f(x,θz,θd), 0] + L(x,θz,θd)dμ(x,y)卜=0,
θz∈Θz θd∈Θd	Rn+m
v(tf, μ) = /
Rn+m
Φ(x,y)dμ(x,y).
(16)
6
Under review as a conference paper at ICLR 2022
Theorem 4.3 establishes the uniqueness, in the viscosity sense, of the HJI equation and identifies the
value function for the mean-field optimal control problem as the unique solution of the HJI equation.
Comparison with existing works. This theorem is analogous to Theorem 7.4.2 in (Guo et al.,
2005), the state variables we are dealing with are probability measures rather than Euclidean vectors.
Proof sketch. The proof has two parts: (1) Fix the optimal strategy θZ and θd respectively, and
transform the problem into two optimal control problems; (2) apply Theorem 1 and 2 in (E et al.,
2019) to these two transformed problems. A detailed proof is given in Appendix A.2.
4.5 Connection between HJI and TSEP
In what follows, we provide the connection between the HJI equation and TSEP. We will show that
the TSEP can be understood as a local result compared to the global characterization of the HJI
equation. For the value function v(t, μ) in deduced HJI (equation 16), consider the lifted function
V(t, X), where X = (x, y)〜μ. We define the Hamiltonian for the lifted HJI equation as
H(X,DχV(t,X))== inf SUp Eμ [Dχ V(t,X)T[f(x,θz,θd), 0] + L(x,θz, θd)].
θz∈Θz θd∈Θd
(17)
Suppose θ?(X, DX V(t, X)) and θ∖(X, DXV(t, X)) are the corresponding optimal strategies and
define P = DX V(t, X), we have
H(X, P) = Eμ [PT[f (χ,θZ(X, P ),θd (X, P)), 0] + L(χ,θZ (X, P )阅(X, P))],
Eμ [Vθz ,θd [f (X网(X, P)阅(X, P)), 0]P + Vθz ,θd L(χ,θZ(X, P )阅(X, P))] = 0,
where the last equation follows from the first order optimality condition. Define Xt =
Pt = DX V(t, Xt), we can apply the characteristic evolution equations (Subbotina, 2006)
• ∙
Xt = DP H(Xt,Pt),	Pt = -DX H(Xt,Pt).
(18)
(xt, y),
(19)
Plugging equation 18 into equation 19, and let θZ(t) = θ?(Xt, Pt), θ力(t) = θ%(Xt, Pt) andPt is the
first n components of Pt, we have
Xt = f(xt,θZ(t)期(t)), Pt = -Vxf(xt,θZ(t),θ/t))pt -VxL(Xt,θZ(t),θd(t)).	(20)
Ifwe let ψ = -P, the first two equalities of equation 11 in Theorem 4.1 is converted to equation 20.
The Hamilton equation in TSEP can be regarded as the characteristic equations for the HJI equa-
tion originating from μo, which justifies the claim that the TSEP constitutes a local condition as
compared to the HJI equation.
5 Estimation of generalization bounds
So far, we have focused on the mean-field quantitative differential game and mean-field TSEP. How-
ever, the solution of the mean-field TSEP requires a saddle point with respect to an underlying sam-
ple distribution. In this section, we will establish generalization bounds from two aspects, global
minimum of the loss function and algorithmic stability. We define the loss function of each training
sample Xi := (Xzi ,Xdi ,yi), i = 1,…，N as
J0(θz ,θd; Xi)
Φ(Xz(tf), Xd (tf), yi) +
Z tf L(Xz(t)
0
, Xd(t), θz(t), θd(t))dt,
where Xz (0) = Xzi ,Xd(0) = Xdi. Now J (θz ,θd) = Eχ° 〜μJ0(θz ,n;Xo), and we define
1N
JN (θz ,θd) = N∑SJ0(θz ,θd; Xi).
i=1
We first estimate the generalization bounds based on the global minimum of the loss function. The
necessary condition of Hamiltonian of sampled version is expressed as
N
N X H (χθN,θN ,i(t),θN (t),θN (t),ΨθN ,θN ,i(t))
N i=1
N
inf SUp a X H(xθ? ,θd ,i(t),θz ,θd,ψθ ,θd ,i(t)), aet ∈ [0,tf ],
θd∈Θd θz∈Θz N i=1
(21)
7
Under review as a conference paper at ICLR 2022
where θzN and θdN are the solution of sampled TSEP. Note that if Θz and Θd are sufficiently large,
e.g. Θz = Rr1, Θd = Rr2, the solution ΘZ, θd of TSEP satisfies
A* A*	A* A*
F(θZ,θ3(t) := Eμo Vθz,θdH(xθz,θd,ψθz,θd,θZ(t)冏(t)) = 0,	a.e. t ∈ [0,tf],	(22)
while the solution θzN, θdN of sampled TSEP satisfies
1N
FN(θN,θN)(t) := nn X Vθz,θdH(X(Pli,ψ?,θd,i,θN(t),θN(t))=0,	a.e.t ∈ [0,tf].
i=1
(23)
Now, FN is a random approximation ofF and EFN (θz, θd)(t) = F(θz, θd)(t) for all θz, θd and a.e.
t ∈ [0,tf]. Let (U, k ∙ ∣∣u), (V, k ∙ IlV) be Banach spaces and F : U → V. We first provide the
definition of stability, which is the primary condition that ensures the approximation of FN to F.
Definition 5.1 For ρ > 0 and x ∈ U, Sρ(x) := {y ∈ U : ∣x - y∣U < ρ}. The mapping F is stable
on Sρ(x) if there exists a constant Kρ > 0 such that,
∣y - z∣U ≤ Kρ∣F (y) - F(z)∣V,	∀y, z ∈ Sρ(x).
Notice that here not consider θZ and θ% correspond to maximum and minimum, We only care
that they both follow the first-order optimality condition. We define θ = (θzT , θdT )T and redefine
F(ΘZ, θ・d)(•) and FN(θNN, θfN)(∙) as F(θ)(∙), F(θN)(∙), respectively. We follow the proof idea of
Theorem 6 in (E et al., 2019) and get similar results stated in 5.1, Which describes the convergence
of sampled solution to mean-field solution as the number of samples increases.
Theorem 5.1 Assuming that f, L, and Φ are bounded and Lipschitz continuous with respect to
x and the Lipschitz constant of f and L are independent of θz, θd. Let ΘZ, θd be a solution of
F = 0 (equation 22), which is stable on Sρ(((θZ)T, (θ与)T)T) for some ρ > 0. Then there ex-
ists positive constants s0, C, K1, K2, ρ1 < ρ and a random variable θN := ((θzN)T, (θdN)T)T ∈
Sρ(((θZ)T, (θd)T)T), such that
P[kθZ-θN∣L∞ ≥ Cs] ≤ 4exp ! — κN[ ], S ∈ (0,so],
z z	K1 + K2s
N s2
P[kθd - θd I∣l∞ ≥ Cs] ≤ 4eχp - - k + Ks ʃ, S ∈ (O, so],
PJ (θz,θ^) - J (θN ,θN)∣≥ s] ≤ 4exp ʃ - NS 1, S ∈ (0,so],
K1 + K2s
p[fn(θN)= 0] ≤ 4eχP{- K1N‰}∙
The loss function is uniformly bounded under the given assumptions, then we can apply the Ho-
effding’s inequality (Corollary 2 in (Pinelis and Sakhanenko, 1986)). Using Theorem 6 in (E et al.,
2019) and rewriting θ as (θzT, θdT)T, this theorem can be proved. Theorem 5.1 basically shows that
the difference between the optimizer over the whole distribution and the optimizer over finite sam-
ples is bounded, and is exponential decay with the total number of samples N . This bound is not
rely on the training algorithms.
Now we estimate the generalization bounds from the view of algorithmic stability. In the rest of this
section, we redefine the integral form in J, JN, J0 as the discrete sum form equation 7 and redefine
r1, r2as the total dimension of θz , θd. We consider the error by taking expectation with respect to
randomized algorithm and define
er(θz,θd) := EA [J(θz, θd) -JN(θz,θd)].
We update θz and θd alternately, i.e. from the initial value (θz,0, θd,0), update θz by Mz,1 steps
to get (θz,Mz,1, θd,0), then update θz by Md,1 steps to get (θz,Mz,1, θd,Md,1). Keep going until the
algorithm converges, we can get (θz,Mz2 ,θ d,Md2), (θz,Mz,3, θd,Md,3) ∙∙∙.
8
Under review as a conference paper at ICLR 2022
Consider Stochastic Gradient Langevin Dynamics (SGLD), which is a popular variant of stochastic
gradient methods which adds isotropic Gaussian noise in each iteration, e.g.
θz,k+1 = θz,k - nN θz JN (θz,k, θd,O) + ∖j ^eN (O,IrI)∙
We follow the proof idea of Theorem 8 in (Mou et al., 2018) and get the following generalization
bound in expectation of random draw of training data.
Theorem 5.2 Supposes J0(θz,θd; X) is uniformly bounded by C, and ∣∣Vθ J0(θz,θd; X) 一
Vθz J 0(θz ,θd; X 0)k ≤ Lz, ∣Vθd J0 (θz ,θd; X) -V θd J0(θz ,θd; X 0)k ≤ Ld ∀X,X0, then we have
the following generalization bound
E[er(θz,Mz,n, θd,Md,n)] ≤
2n
N ^X min(k1, Mz,i - Mz,i-1) +
N i=1
√βLz C
N
n	Mz,i -Mz,i-1
X( X	ηj )1/2
i=1	j=k1+1
where Mz,0
+ N X min(k2, Md,i 一 Mdn-I) + VZ^Nd
N i=1	N
n	Md,i-Md,i-1
X( X n )1/2,
i=1
j=k2+1
Md,o = 0, kι and k2 are Chosen to satisfy n^ ≤ ln2∕βL2, nk2 ≤ ln2∕βLd.
(24)
Theorem 5.2 obtains a bound of O (1/N), which matches the generalization bounds of stochastic
gradient descent ascent (SGDA) for minimax problems in (Lei et al., 2021). This bound relies on
the aggregated step sizes and do not explicitly depend on the dimensions, norms, or other capacity
measures of the parameter.
Comparison with existing works. Unlike the existing methods to analyze the algorithms with
Mzi 一 Mz,i-ι = 1, Md,i 一 Md,1 = 1,e.g. SGDA, we analyzed the case of taking any Mz,i, Md,i.
As {ηi } is usually monotonically non-increasing, we find that set Mz,1 一 Mz,0 = Mz, Mz,i 一
Mz,i-1 = 0, i > 1 and Md,1 一 Md,0 = Md, Md,i 一 Md,i-1 = 0, i > 1 will lead to the smallest
bound for fixed Mz,n = Mz, Md,n = Md in equation 24. This finding gives an inspiration to the
training method, training one network to make it converge before training the other one.
Proof sketch. The proof is based on the uniform stability of the loss function and standard sym-
metrization argument (Hardt et al., 2016). Using Theorem 8 in (Mou et al., 2018) alternately in the
training process of the two networks, this theorem can be proved. See details in Appendix D.
6 Conclusion
In this paper, adversarial reinforcement learning was considered as the mean-field quantitative dif-
ferential game and the convergence and generalization were analyzed under this framework. We first
proposed a new mean-field PMP for reinforcement learning with terminal constraints in Theorem
3.1. Then we bridged adversarial reinforcement learning to the mean-field quantitative differential
game and proved a mean-field TSEP in Theorem 4.1, which provides the necessary condition for the
convergence to the global optimality. Moreover, the uniqueness of the solution to mean-field TSEP
is shown in Theorem 4.2, where a sufficient small terminal time is required. After that, the mean-
field HJI equation for calculating the optimal loss function is derived in Theorem 4.3. Furthermore,
we have shown that the TSEP is actually a local special case compared to the global characteriza-
tion of the HJI equation. Lastly, we proved two generalization error bounds of order O(e-N) and
O(1/N), which characterize the gap between the expected solution and the learned model. The two
bounds are obtained from Hoeffding’s inequality (Theorem 5.1) and algorithmic stability (Theorem
5.2). Both bounds do not explicitly rely on the dimensions, norms, or other capacity measures of
the network parameter. Moreover, the latter bound characterizes the influence of randomness in the
training algorithms and aggregated step sizes on generalization. Our results allow the possibility
to develop a learning algorithm without referring to the classical methods of deep learning such as
the SGD. A further direction of our ongoing research is based on the study of the TSEP and HJI
equation from a discrete-time perspective in order to relate the theoretical framework directly to the
applications.
9
Under review as a conference paper at ICLR 2022
References
V.	Behzadan and A. Munir. Adversarial reinforcement learning framework for benchmarking col-
lision avoidance mechanisms in autonomous vehicles. IEEE Intelligent Transportation Systems
Magazine, 2019.
A.	Bensoussan, J. Frehse, and P. Yam. Mean field games and mean field type control theory, volume
101. Springer, 2013.
P. Cardaliaguet. Notes on mean field games. Technical report, from P.-L. Lions, lectures at College
de France, 2012.
B.	Chang, L. Meng, E. Haber, L. Ruthotto, D. Begert, and E. Holtham. Reversible architectures for
arbitrarily deep residual neural networks. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 32, 2018a.
B. Chang, L. Meng, E. Haber, F. Tung, and D. Begert. Multi-level residual networks from dynamical
systems view. In International Conference on Learning Representations, 2018b.
W. E. A proposal on machine learning via dynamical systems. Communications in Mathematics
and Statistics, 5(1):1-11, 2017.
W.	E, J. Han, and Q. Li. A mean-field optimal control formulation of deep learning. Research in the
Mathematical Sciences, 6(1):1-41, 2019.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139-144,
2020.
L.	Guo, D. Z. Cheng, and D. X. Feng. Introduction to control theory: from basic concepts to research
frontiers. Science Press, 2005.
E. Haber and L. Ruthotto. Stable architectures for deep neural networks. Inverse problems, 34(1):
014004, 2017.
M.	Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. In International Conference on Machine Learning, pages 1225-1234. PMLR, 2016.
L. Kantorovitch. The method of successive approximation for functional equations. Acta Mathe-
matica, 71(1):63-97, 1939.
J. Lasry and P. Lions. Mean field games. Japanese journal of mathematics, 2(1):229-260, 2007.
Y. Lei, Z. Yang, T. Yang, and Y. Ying. Stability and generalization of stochastic gradient methods
for minimax problems. arXiv preprint arXiv:2105.03793, 2021.
Q. Li and S. Hao. An optimal control approach to deep learning and applications to discrete-weight
neural networks. In International Conference on Machine Learning, pages 2985-2994. PMLR,
2018.
Q. Li, L. Chen, and C. Tai. Maximum principle based algorithms for deep learning. Journal of
Machine Learning Research, 18:1-29, 2018.
M. Liu and O. Tuzel. Coupled generative adversarial networks. Advances in neural information
processing systems, 29:469-477, 2016.
X. Ma, K. Driggs-Campbell, and M. J. Kochenderfer. Improved robustness and safety for au-
tonomous vehicle control with adversarial reinforcement learning. In 2018 IEEE Intelligent Vehi-
cles Symposium (IV), pages 1665-1671. IEEE, 2018.
A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and S. Savarese. Adversarially robust policy learn-
ing: Active construction of physically-plausible perturbations. In 2017 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 3932-3939. IEEE, 2017.
10
Under review as a conference paper at ICLR 2022
X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley. Least squares generative adversarial
networks. In Proceedings ofthe IEEE international conference on computer vision, pages 2794-
2802, 2017.
W.	Mou, L. Wang, X. Zhai, and K. Zheng. Generalization bounds of sgld for non-convex learning:
Two theoretical viewpoints. In Conference on Learning Theory, pages 605-638. PMLR, 2018.
X.	Pan, D. Seita, Y. Gao, and J. Canny. Risk averse robust adversarial reinforcement learning. In
2019 International Conference on Robotics and Automation (ICRA), pages 8522-8528. IEEE,
2019.
L.	D. Persio and M. Garbelli. Deep learning and mean-field games: A stochastic optimal control
perspective. Symmetry, 13(1):14, 2021.
I. F. Pinelis and A. I. Sakhanenko. Remarks on inequalities for large deviation probabilities. Theory
of Probability & Its Applications, 30(1):143-148, 1986.
L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning. In
International Conference on Machine Learning, pages 2817-2826. PMLR, 2017.
L. S. Pontryagin. The mathematical theory of optimal processes and differential games. Trudy
Matematicheskogo Instituta imeni VA Steklova, 169:119-158, 1985.
L. S. Pontryagin. Mathematical theory of optimal processes. CRC press, 1987.
S. Sonoda and N. Murata. Double continuum limit of deep neural networks. In ICML Workshop
Principled Approaches to Deep Learning, volume 1740, 2017.
N. Subbotina. The method of characteristics for hamilton—jacobi equations and applications to
dynamical optimization. Journal of mathematical sciences, 135(3):2955-3091, 2006.
W. Uther and M. Veloso. Adversarial reinforcement learning. Technical report, Tech. rep., Carnegie
Mellon University. Unpublished, 1997.
Y. Zang, G. Bao, X. Ye, and H. Zhou. Weak adversarial networks for high-dimensional partial
differential equations. Journal of Computational Physics, 411:109409, 2020.
K. Zhang, B. Hu, and T. Basar. On the stability and convergence of robust adversarial reinforcement
learning: A case study on linear quadratic systems. Advances in Neural Information Processing
Systems, 33, 2020.
A Appendix
Proof A.1 (Proof of Theorem 3.1) As the ordinary differential equation equation 3 is subject to
terminal constraint g(x(tf), tf) = 0, we transform it into an unconstrained problem by introducing
Lagrangian multiplier vectors ξ ∈ Rp, thus the objective functionals equation 4 transforms into
J2(θ) = E(xo,yo)〜μ
Φ(x(tf), y0) + ξT g(x(tf)) + Z f
L(x(t), θz(t), θd(t))dt
(25)
Then by Mean-field Pontryagin’s maximum principle (Theorem 3 in (E et al., 2019)), the conclusion
can be proved.
Proof A.2 (Proof of Theorem 4.1) When (ΘZ ,θd) exists, we can split problem equation 9 into the
following two optimal control problems.
Problem 1:
X = f (x,θz ,θd), x(0) = X0,
S := {xg (x(tf)) = 0},
J(θz,θd) =E(χo,yo)^μ [φ(x(tf ),yo)+ Ztf L(x(t),θz(t),θd(t))dt ,
0
(26)
11
Under review as a conference paper at ICLR 2022
find θZ = arg infθz∈Uz J(θz M)∙
The Hamilton function of Problem 1 is
Hl(x(t),θz (t),θd(t), ψl(t)) := -L(x(t),θz(t),θd(t)) + ΨT (t)f (x(t),θz (t),θd(t)).
Problem 2:
X = f(x, θZ,θd), x(0) = xo,
S := {xz g (x(tf)) = 0},
J(θz,θd) =E(x0,y0)^μ [φ(χ(tf),yo)+ Ztf L(x(t),θZ(t),θd(t))dt ,
0
find θd = argsuPθd ∈ud J (θz,θd) ∙
The Hamilton function of Problem 2 is
H2(x(t),θZ(t),θd(t), ψ2(t)) := -L(x(t),θZ(t),θd(t)) + ψT(t)f (x(t),θZ(t),θd(t)).
(27)
(θZ,θd) is the optimal strategy of Problem 1 and Problem 2∙ From the definition of
Hι(x,θz, θd, Ψι, t) and H2(x, ΘZ, θd, ψ2,t), by Theorem 3∙1, there exists Lagrangian multiplier
ξ1 , ξ2 ∈ Rp such that
ψι(t) = -VxHI(X (t),θz(t),θrf(t),ψι(t)),
	•,. , ,, ,. ,. ,.. ψ2⑴=-VxH2(X (t),θz(t),θd(t),ψ2 ⑴),	(28) Ψι(tf) = -VxΦ(χ*(tf),yo) - ξTVxg(X*(tf)), Ψ2(tf) = -VxΦ(χ*(tf),yo) - ξTVxg(X*(tf)).
Here both ξι and ξ are determined by g(x*(tf ),tf) = 0, so ξι = ξ∙ Then it can be Obtainedfrom
the uniqueness of linear differential terminal value problem’s solution that
	ψ1 (t) = ψ2 (t),	∀t ∈ [0, tf].
Now let ψ(t) ≡ ψ1(t), ∀t ∈ [0, tf] we have
	H(X*(t),θZ(t) 冏(t),ψ(t)) = Hι(X*(t),θZ(t) 冏(t),ψ(t)), =H2(X*(t),θZ(t)Q(t),ψ(t)), H(X*(t),θzQ(t),ψ(t)) =H1(X*(t),θz9(t),ψ(t)),	( 9) H (X*(t),θZ(t),θd, ψ(t)) =H2(X*(t),θZ(t),θd, ψ(t)).
By Theorem 3∙1,
	E(x0,y0)〜μH (X*(t),θZ(t) 冏(t),ψ*(t)) =SUp E(x0,y0)〜μH(X*(t),θz冏(t),ψ*(t)) θz∈Θz	(30) =inf E(x0,y0)〜μH(X*(t),θZ(t),θd, ψ*(t)),	a.e.t ∈ [0,tf] θd ∈Θd
Thus	inf sUp E(x0,y0)〜μ H(X*(t),θz ,θd,ψ*(t)) θd∈Θd θz∈Θz ≤E(x0,y0)〜μH (X*(t),θZ(t) 冏(t), ψ*(t))	(31) ≤ sUp inf E(x0,y0)〜μH(X*(t),θz,θd,ψ*(t)). a.e.t ∈ [0,tf] θz∈Θz θd∈Θd
On the other hand, from the well known max-min inequality,
	sUp inf E(x0,y0)〜μ H(X*(t),θz ,θd,ψ*(t)) θz∈Θz θd∈Θd	(32) ≤ inf sUp E(x0,y0)〜μH(X*(t),θz,θd,ψ*(t)). a.e.t ∈ [0,tf] θd∈Θd θz∈Θz
Consequently,	E(x0,y0)〜μH (X*(t),θZ(t) 冏(t),ψ*(t)) = inf sUp E(x0,y0)〜μH(X*(t),θz ,θd ,Ψ*(t))	CN θd∈Θd θz∈Θz	(33) = sUp inf E(x0,y0)〜μH(X*(t),θz,θd,ψ*(t)), a.e.t ∈ [0,tf] θz∈Θz θd∈Θd
we have finished the proof∙
12
Under review as a conference paper at ICLR 2022
B Appendix
Before proving Theorem 4.2, we write the express in Theorem 4.1 more compactly. For each control
process θz ∈ L∞([0, tf], Θz) and θd ∈ L∞([0, tf], Θd), we denote by xθz,θd := {xtθz,θd : 0 ≤ t ≤
tf} and ψθz,θd := {ψtθz,θd : 0 ≤ t ≤ tf} the solutions of Hamilton’s Equation equation 11, i.e.
X θz ,θd = f(xθz/d ⑴，θz ,θd),	x0z ,θd = X0,
Ψθz,θd = -VxH(xθz,θd,θz(t),θd(t),ψθz,θd),	ψθz,θd = -VχΦ(xθz,θd,yo) - ξVχg(xθz,θd).
We have the following lemma, which provides an estimate of the difference between xθz1,θd1 , ψ θz1,θd1
and xθz2,θd2 , ψθz2,θd2 .
Lemma B.1 Let θz1, θz2 ∈ L∞([0, tf], Θz) and θd1, θd2 ∈ L∞([0, tf], Θd). Then there exists a con-
stant T0 such that for all tf ∈ [0, T0), it holds that:
kxθz1,θd1 - xθz2,θd2 kL∞ + kψθz1,θd1 - ψθz2,θd2 kL∞ ≤ C (tf)(kθz1 - θz2kL∞ + kθd1 - θd2kL∞),
where C(tf) > 0 satisfies C(tf) → 0 as tf → 0.
Proof B.1 (Proof of Lemma B.1) Denote δθz := θz1 - θz2, δθd := θd1 - θd2, δx := xθz1,θ1d - xθz2,θd2
and δψ := ψ θz1,θd1 - ψθz2,θd2. The first two assumptions of Theorem 4.2 leads to
kδxtk ≤Z tkf(xθsz1,θd1,θz1(s),θd1(s))-f(xθsz2,θd2,θz2(s),θd2(s))kds
0
≤K f kδxskds + K fkδθz(s)kds+K fkδθd(s)kds,
00	0
and so
kδxkL∞ ≤ Ktf kδxkL∞ + Ktf kδθz kL∞ + Ktf kδθdkL∞ .
If tf ≤ T0 := 1/K, we have
Kt
∣∣δxkL∞ ≤ Tj-^-(IlδθzkL∞ + ∣∣δθdkL∞ ).
1 - Ktf
(34)
Similarly,
∣δψt∣ ≤ K∣δxtf∣ +K
Z tf ∣δxs ∣
+∣δψs∣ +∣δθz(s)∣ + ∣δθd(s)∣ds,
∣δψ∣L∞ ≤ (K + K tf)∣δx∣L∞ + K tf (∣δψ ∣L∞ + ∣δθz ∣L∞ + ∣δθd∣L∞),
hence
∣δψ∣L∞ ≤ K(1+ tff ) ∣δx∣L∞ + -ɪtf- (kδθz∣L∞ + ∣δθd∣L∞ ),
1 - Ktf	1 - Ktf
which combined with equation 34 proves the lemma.
We can now prove Theorem 4.2.
Proof B.2 (Proof of Theorem 4.2) By uniform strong concavity and the second assumption of The-
orem 4.2, there exists a λ0 > 0 such that
λokθZ(t) -θ2(t)k2 ≤[EμoVθzH(xθ1,θ1 忠(t),θ2(t),ψθ1,θ1)
-Eμo VθzH(Xθ1,θ1 ,θZ(t),θ1(t), ψθ1,θ1)] ∙ (θZ(t) - θ2(t)),
λokθ1(t) - θ2(t)k2 ≤[EμoVθdH(xθ1,θ1 ,θZ(t),θ2(t),ψθ1,θ1)
-Eμo VθdH(Xθ1,θ1 ,θZ(t),θ1(t),Ψθ1,θ1)] ∙ (θ2(t) - θ1(t)).
13
Under review as a conference paper at ICLR 2022
Note that Eμo Vθz,θd H(Xθz,θd ,θZ ⑴,θ1 ⑴,ψθz,θd ) = Eμo Vθz,θdH(Xθz,θd ,θZ ⑴,θ2⑴,ψθz,θd ) =
0, ∀t ∈ [0, tf] due to the optimality and continuity, then combining the two inequalities above we
have
λ0(kθz1(t)-θz2(t)k2+kθd1(t)-θd2(t)k2)
≤[Eμo Vθz H (xθ1,θ1 ,θZ(t),θ2(t),ψθ1,θ1)
-	Eμo VθzH(xθ2,θ2,θZ(t),θ2(t),ψθ2,θ2)] •⑹(t)-θZ(t))
+ [Eμo Vθd H (xθ1 ,θd ,θ2(t),θ2(t),ψθz,θ1)
-	EμoVθdH(xθ2,θ2,θZ(t),θ2(t), ψθ2,θ2)] ∙ (θ2(t) - θ1(t))
≤EμokVθzH (xθz,θd ,θZ(t),θ2(t),ψθ1,θ1)
-	VθzH(Xtθz2,θd2,θz2(t),θd2(t),ψtθz2,θd2)kkθz1(t)-θz2(t)k
+EμokVθdH (xθ1,θ1 ,θZ(t),θ2(t),ψθ1,θ1)
-VθdH(Xtθz2,θd2,θz2(t),θd2(t),ψtθz2,θd2)kkθd1(t) -
≤K(kδXkL∞ + kδψkL∞)(kδθzkL∞ + kδθdkL∞).
Combining the above with Lemma B.1, we have
kδθzkL∞ + kδθd kL∞ ≤ KCf (kδθzkL∞ + kδθdkL∞ )2 ≤ 2KCf (kδθzkL∞ + kδθd kL∞).
λ0	λ0
C(tf)	→ 0 as tf	→	0,	by taking tf	sufficiently small, so that	2KC(tf)	<	λ0,	which implies
kδθzkL∞ = kδθdkL∞ =0.
C Appendix
Now we introduce the definition of viscosity solution. Consider a function v(t, PX) : [0, tf] ×
P2(Rn+m) → R, the Hamiltonian H(X,∂pxv(t,Pχ)(X)): L2(Ω,Rn+m) X L2(Ω,Rn+m) → R
and Ψ : L2(Ω,Rn+m) → R, where V satisfies
币 + H(X,∂pχ v(t, Pχ)(X)) = 0,	on [0,tf) × L2(Ω, Rn+m),
v(tf, PX) = Ψ(X),	on L2(Ω, Rn+m).
Then the lifted function V (t, X) = v(t, PX ) satisfies
∂V
瓦 + H(X,DXV(t,X))=0,	on [0,tf) × L2(Ω,Rn+m),
V(T,X) = Ψ(X),	on L2(Ω, Rn+m).
(35)
(36)
We say that a bounded, uniformly continuous function u : [0, tf] × P2(Rn+m) → R is a viscosity
solution to equation 35 if its lifted function U : [0,tf ] × L2 (Ω, Rn+m) → R defined by
U(t,X)=u(t,PX),
is a viscosity solution to the lifted equation equation 36, namely:
i)	U(tf ,X) ≤ Ψ(X) and for any test function Y ∈ C 1,1([0,tf] × L2(Ω,Rn+m)) such that the
map U 一 γ has a local maximum at (to, Xo) ∈ [0,tf) × L2 (Ω, Rn+m), one has
∂tγ(t0, X0) + H(X0, Dγ(t0, X0)) ≥ 0.
ii)	U(tf ,X) ≥ Ψ(X) and for any test function Y ∈ C 1,1([0,tf] × L2(Ω,Rn+m)) such that the
map U 一 γ has a local minimum at (to,Xo) ∈ [0,tf) × L2 (Ω, Rn+m), one has
∂tY(to, Xo) + H(Xo, DY(to, Xo)) ≤ 0.
For further details we refer the interested readers to (E et al., 2019).
14
Under review as a conference paper at ICLR 2022
Proof C.1 (Proof of Theorem 4.3) Suppose v0(t,μ) is a viscosity solution to equation 16 and
(θz0 , θd0 ) is the corresponding optimal strategy.
We first fix θz0 , consider
∂t vι(t,μ) + sup
θd∈Θd
v1(tf,μ) =
Rn+m
/	[∂μv(t, μ)(x, y)]T [f (x,θ'z ,θd), 0] + L(x,θ'z ,θd)dμ(x, y)卜=0,
Rn+m
Φ(x,y)dμ(x,y).
(37)
By Theorem 1 and Theorem 2 in (E et al., 2019), v0(t, μ) is the unique viscosity solution to equa-
tion 37 satisfies
V(t,μ) = SUp E(x,y)〜“
θd∈Ud
f Φ(x(tf), y)
+ L(x(t), θz0 (t), θd(t))dt
(38)
Then fix θd0 , similarly we have
v0(t,μ)
,.inf E(x,y)^μ
θz ∈Uz
Zttf
Φ(x(tf), y) + L(x(t), θz (t), θd0 (t))dt
(39)
Nowfor (ΘZ, θd), equation 15 is satisfied, thus v0(t, μ) = v*(t, μ).
D	Appendix
Proof D.1 (Proof of Theorem 5.2) We first give a lemma, then use it alternatively in the training
process of the two networks to prove the theorem.
Define the generalization bound taking expectation with respect to randomized algorithm
er(w) := EA(El(w; z) — ENl(w; z)),
where EN l(w; z) is the average loss at N sample points.
Lemma D.1 (Theorem 8 in (Mou et al., 2018)) Consider n rounds of SGLD with parameters β and
{ηi}. Suppose the loss function l(w; z) is uniformly bounded by C, and∀z, z0, there is ∣∣Vl(w; z)—
Vl(w; z0)∣ ≤ L. By setting ko such that ηk0 ≤ ln 2∕βL2, then we have thefollowing generalization
bound in expectation
2	2k0	√βLC	、1/2
E[er(Wn)] ≤ ~n +	N (工 ηi) / .
i=k0+1
Now let l(w; z) = J0 (θz , θd; X). During the training process, θz and θd are updated alternately,
and the generalization error is accumulating. Using Lemma D.1 alternatively, we can finish the
proof.
15