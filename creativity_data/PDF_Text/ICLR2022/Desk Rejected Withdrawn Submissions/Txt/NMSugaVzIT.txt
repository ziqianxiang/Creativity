Under review as a conference paper at ICLR 2022
Inductive Bias of Multi-Channel Linear Convolu-
tional Networks with B ounded Weight Norm
Anonymous authors
Paper under double-blind review
Ab stract
We provide a function space characterization of the inductive bias resulting from minimizing the `2
norm of the weights in multi-channel linear convolutional networks. We define an induced regularizer
in the function space as the minimum `2 norm of weights of a network required to realize a function.
For two layer linear convolutional networks with C output channels and kernel size K , we show
the following: (a) If the inputs to the network have a single channel, the induced regularizer for any
K is independent of the number of output channels C . Furthermore, we derive the regularizer is a
norm given by a semidefinite program (SDP). (b) In contrast, for networks with multi-channel inputs,
multiple output channels can be necessary to merely realize all matrix-valued linear functions and
thus the inductive bias does depend on C . However, for sufficiently large C , the induced regularizer is
again given by an SDP that is independent of C . In particular, the induced regularizer for K = 1 and
K = D are given in closed form as the nuclear norm and the `2,1 group-sparse norm, respectively, of
the Fourier coefficients. We investigate the applicability of our theoretical results to a broader scope
of ReLU convolutional networks through experiments on MNIST and CIFAR-10 datasets.
1	Introduction
In the study of generalization and model capacity, complexity measures based on magnitude of parameters have long
been argued to play an important role in learning overparametrized models (Bartlett, 1996; Bartlett & Mendelson, 2002;
Neyshabur et al., 2015; Zhang et al., 2017; Bartlett et al., 2017). In particular, the `2 norm of weights (or parameters)
is a prominent complexity measure of interest in the current practice of deep learning, with connections to explicit
regularization (Krogh & Hertz, 1991; Wei et al., 2019) as well as implicit regularization from optimization algorithms
(Ji & Telgarsky, 2019; Gunasekar et al., 2018a) (see additional discussion in Section 1.1).
Importantly, recent results (Lyu & Li, 2020; Nacson et al., 2019; Ji & Telgarsky, 2020) show that in many (not all)
instances of overparametrized classification problems, gradient descent asymptotically lead to solutions that implicitly
control the `2 norm of the parameters. More formally, consider a function class represented as Φ(θ; x) for inputs denoted
as x and parameters (or weights) denoted as θ. For positive homogeneous model classes, which includes ReLU and
linear networks, the predictors from gradient descent on logistic loss asymptotically converges in direction to a first-order
stationary point of the following max-'2 margin problem in parameter space : min ∣∣θk2 s.t., ∀nynΦ(θ; Xn) ≥ 1.
θ
What is the nature of functions learned by controlling `2 norm of parameters? The function space view of controlling
`2 norm of parameters can be understood in terms of the representation cost with respect to the weight norm, that is, the
minimum `2 norm of weights needed to realize a function using a given network architecture Φ(θ, .). This defines an
induced complexity measure over functions, which we also refer as the induced regularizer given as follows:
RΦ(f) := inf ∣θ∣22 s.t., ∀X, f(X) = Φ(θ, X).	(1)
θ
It is easy to see that any learning objective with `2 norm regularization over parameters minθ L(Φ(θ, .)) + λ∣θ∣22 is
equivalent to the corresponding R-regularization over functions: minf L(f) + λRΦ (f).
Even for networks that realize the same model class, minimizing or bounding the `2 norm of weights in different
architectures can lead to remarkably different effects in function space. For example, consider linear networks (networks
with linear activation) with fully connected and convolution layers, which are simply different parameterizations of the
same model class of linear functions. Gunasekar et al. (2018b) showed that for networks with fully connected linear
layers, the induced regularizer is the `2 norm of the linear map realized by the network, while for linear convolutional
network with full dimensional kernels, it is the `1 norm of Fourier coefficients of the linear map. This function space
view reveals that minimizing the `2 norm of weights in these networks has fundamentally different implications for
learned predictors that depends on specific parametrization of function class.
1
Under review as a conference paper at ICLR 2022
In this work, we investigate the induced regularizer for multi-channel linear convolutional networks. In particular, we
study two layer networks that have C output channels, R input channels, and kernel size K . Our main contribution is to
characterize the role of the number of channels on the induced regularizer, for networks with arbitrary kernel size.
1.	Single-channel inputs. Our main result in Theorem 3 shows that for two-layer linear convolutional networks
over single channel inputs, the induced regularizer for any kernel size K is independent the number of output
channels for C . This result is surprising, since it shows that adding more than 1 output channel to the network
does not reduce the `2 -norm representational cost, despite increasing the number of parameters in the network.
To prove this, we construct an semidefinite program (SDP) relaxation which corresponds to the induced
regularizer when C = ∞. In our main result in Theorem 3, we prove that this SDP relaxation is infact tight for
all C ≥ 1. This enables us to deduce invariance of the induced regularizer to the number of output channels C,
and further, it unconvers a convex structure of the induced regularizer for any C ≥ 1.
2.	Proof technique. In our proof of SDP tightness, we use a polynomial representation of convolutions to
implicitly argue the existence of a rank 1 optimal solution. A key lemma in our proof Lemma 4 also shows an
interesting property about convolutions with small kernels of size K < D. To our knowledge, this property as
well as the proof technique involving polynomial representations are new and are of independent interest.
3.	Multi-channel inputs. We further extend our findings to networks with multi-channel inputs. For multi-channel
inputs of dimensions D × R, even realizing all linear functions over the inputs can require multiple output
channels C (see Lemma 6). Hence, the induced regularizer does depend on C, although for large enough
C, we show a restricted form of invariance. In particular, we prove the induced regularizer is invariant to
the number of output channels when C ≥ R ∙ K, and conjecture invariance when C ≥ R. We use this to
characterize the induced regularizer in the special cases of K = 1 and K = D as the nuclear norm and the
`2,1 group sparse norm of the Fourier coefficients, respectively (see Theorems 9-10).
4.	Experiments for gradient descent. Finally, we connect our results to the implicit regularization of gradient
descent. When combined with prior work (e.g. Lyu & Li (2020)), our results suggest hypotheses about the
extent to which models learned with implicit regularization from gradient descent might depend on the number
of output channels. We formulate a testable hypothesis—that the induced regularizer is invariant to C as long
as C ≥ R—and study this hypothesis experimentally. In particular, we validate this hypothesis on MNIST and
CIFAR-10 datasets on two-layer linear convolutional neural networks with different settings of K and C, as
well as networks with ReLU nonlinearity a zero padding.
1.1	Related Work
There is a rich literature of work connecting `2 norm minimization of weights with explicit regularization (Krogh &
Hertz, 1991; Wei et al., 2019) and implicit regularization from the gradient descent (Neyshabur et al., 2015; Zhang
et al., 2017; Bartlett et al., 2017; Gunasekar et al., 2018a;b; Ji & Telgarsky, 2018; 2019; Nacson et al., 2019; Lyu & Li,
2020; Ji & Telgarsky, 2020). While implicit regularization from gradient descent trajectory is not always connected to
`2 norm for regression (for example, see counter examples in Dauber et al. (2020); Razin & Cohen (2020); Li et al.
(2021)), the connection is prominent in many settings of interest. Most relevant to our work is the result by Lyu & Li
(2020) (stated in Section 6), showing that gradient descent on logistic loss asymptotically converge in the direction of
max-'2-margin solution. We combine our results with this prior work to demonstrate that the conclusions from our
analysis also extent to gradient descent solutions in classification problems.
Motivated by the connections to implicit and explicit regularization and generalization, other prior work also study
induced regularizers corresponding to minimizing `2 of weights in different architectures. Among recent work, Savarese
et al. (2019); Ongie et al. (2020) provided a characterization of induced regularizer for infinite width two layer ReLU
neural networks on 1D and higher dimensional inputs respectively. In a work closely related to ours, Gunasekar et al.
(2018b) characterized the induced regularizer for fully connected networks and for linear convolutional network with
full dimensional kernels (K = D) and single-channel networks (R = C = 1). Yun et al. (2021) extended Gunasekar
et al. (2018b) and showed a general connection for linear networks between implicit `1 norm minimization in an
orthonormal basis and the existence of data-independent diagonalizations of the linear operator in each layer. Zhang
et al. (2020) empirically demonstrated such differences arising from fully connected vs convolutional architectures.
However, even within the class of two-layer linear convolutional networks, the conclusions of Gunasekar et al. (2018b);
Yun et al. (2021) do not generalize to nontrivial kernel and channel sizes. For example, in the other extreme kernel size
of K = 1, the induced regularizer is in fact the `2 norm of the linear function which is fundamentally different from
the `1 norm of the Fourier coefficients for K = D, thus emphasizing the importance of our analysis for multi-channel
networks with arbitrary kernel sizes.
2
Under review as a conference paper at ICLR 2022
Lastly, in a complementary approach, Pilanci & Ergen (2020); Ergen & Pilanci (2020a;b); Sahiner et al. (2020) study
the induced regularizer of neural networks including convolutional networks by looking at the bi-dual convex relaxation
of the `2 regularized least squares loss. We elaborate on the connections more in Section 4.2.3. Briefly, in the context of
linear convolutional networks, our results are significantly stronger, as their analysis shows invariance to number of
output channels only in the limit of large C, while we show independence for all C ≥ 1.
1.2	Notation
We typeface vectors, matrices, and tensors using bold characters, e.g., v, x, θ, W, U. We will use zero-based in-
dexing and python style slicing notation to specify the sub-entries of an array variable, e.g., for Z ∈ RD1 ×D2 and
∀d1∈[D1] , ∀d2∈[D2] (where [D] = {0, 1, . . . , D - 1}), the individual entries of Z are denoted as Z[d1, d2], the dt1h row as
Z[d1, :] ∈ Rd2, and the dt2h column as Z[:, d2] ∈ Rd1. We also briefly mention some standard notation for complex num-
bers. Complex numbers are specified in the polar form as z = |z| eiφz with |z| ∈ R+ and φz ∈ [0, 2π); or in Cartesian
form as Z = Re(Z) + i Im(z), where Re(z), Im(z) ∈ R (ref. i = √-1 is the imaginary unit). The complex conjugate
is denoted as Z = |z| e-iφz. The standard inner product between a, b ∈ CD is(a, b)= a>b = PD=OI a[d]b[d], and
analogously extends to matrices.
Unless qualified otherwise, we use k.k to denote the standard Euclidean norm, i.e., `2 norm for vectors and Frobenius
norm for matrices. For arrays a, b, a Θ b denotes entry-wise multiplication and a α b implies proportionality upto
positive scaling. Finally, we define the convolution operator ? as it is used in the neural networks literature:1
Definition 1 (Circular convolution). For u ∈ RK and v ∈ RD with K ≤ D, their D dimensional circular convolution,2
denoted by u ? v, is a vector in RD given as follows:
1 K-1
∀d∈[D], (u ? v)[d] = √= E u[k]v[(d + k) mod D].
D k=O
2	Multi-channel linear convolutional network
We consider two layer linear convolutional networks with multiple channels in the convolution layer. We first focus
on multi-output channel convolutions with single channel inputs described below. We will discuss networks with
multi-channel inputs (e.g., RGB color channels) in Section 5.
The inputs to the network are vectors3 of dimension D denoted as x ∈ RD. The first layer is a convolution layer with
kernel size K and number of output channel C whose weights (parameters) are denoted by U ∈ RK ×C. The output of
the convolution layer on input x is denoted as h(U; x) ∈ RD×C, and is given by h(U; x)[:, c] = U[:, c] ? x for all
0 ≤ c ≤ C- 1. The second layer is a single output linear layer with weights denoted by V ∈ RD×C. Thus, for input
x, the output of the network Φ(U,V; x) with weights (U, V) is given by:
C-1
Φ(U,V; x) = hV, h(U; x)i =X hV[:,c],U[:,c] ? xi.	(2)
c=O
Since, the network described above does not have any non-linearity, the function computed by the network can be
equivalently represented by w(U, V) ∈ RD such that ∀x, Φ(U,V; x) = hw(U, V), xi. Using simple algebraic
manipulations on eq. (2), one can derive w(U, V) as follows:
w(U,V) = X(U[:,c] ? V[:,c]，[	⑶
c=O
where Z denotes the flipped vector of Z ∈ RD, given by z,[d] = z[D — d — 1] for d = 0,1,...D — 1.
Remark. Even for the smallest network in this class with K = C = 1, any linear predictor w ∈ RD can realized as
w(U, V) in eq. (3) (e.g., using U = 1, V = w). In fact, every linear predictor can be represented by multiple networks
with different weights U, V.
1The operator mod refers to the modulo operation for integer division, i.e., P mod D = P — D |_Dp_|.
2Definition 1 corresponds to circular padding, which simplifies analysis. For convolutions with zero-padding, there will be
different edge effects, but we expect qualitatively similar behavior for small padding sizes. We also use a scaling of 1 /√D-this is
merely to simplify notation and does not change the analysis.
3For simplicity we consider 1D vectors x ∈ RD as inputs, but all our results can be extended to 2D inputs x ∈ RW ×H, such as
images, with the corresponding 2D convolutional operator.
3
Under review as a conference paper at ICLR 2022
Fourier domain representation. The convolution operation in Definition 1 permits a simple formulation in the
Fourier domain arising from the Convolution Theorem. We consider discrete Fourier transforms (DFTs) over RD .
Let F ∈ CD×D denote the unitary DFT matrix for RD, i.e., F[k,l] = √De -Dik for 0 ≤ k,l < D; and for any
1 ≤ K ≤ D, let FK ∈ CD×K denote the submatrix of F with the first K columns. For a vector a ∈ RK, we denote
its D dimensional Fourier representation as ba = FKa ∈ CD. The Convolutional Theorem in Fourier domain implies
一 ,	_ .	_X-	^
F(a ? b) = a Θ b.
The discrete Fourier transform wb(U, V) := Fw(U, V) of the linear predictor w(U, V) realized by our network (eq. 3)
can now be expressed as follows: Let Ub = FKU ∈ CD×C and Vb = FV ∈ CD×C denote the D dimensional Fourier
representation of U, V, respectively. We have,
C-1
>
W(U, V) = £U[：,c] Θ V[：,c]= diag(UV ).
c=0
(4)
3	Induced regularizer in the function space
For the network Φ described above we now turn to the function space view of controlling the `2 norm of the weights
(U, V). We recall our discussion in Section 1.1 that this inductive bias is captured by the induced regularizer defined as
the function space representation cost (eq. (1)). In the case of our network Φ, the function class realized is exactly the
set of linear predictors in RD. Thus, for any w ∈ RD the induced regularizer is given as follows:
RK,C(w) := U∈RK×mC,iVn∈RD×C kUk2 + kVk2	s.t.,	w(U,V)=w.	(5)
Remark 1. It immediately follows from eq. (5) are that RK,C (w) is weakly decreasing in both K and C, i.e., ∀C,
R1,C (w) ≥ R2,C (w) ≥ . . . RD,C (w) and ∀K, RK,1 (w) ≥ RK,2(w) ≥ ....
Even within the class of linear convolutional networks, the induced regularizer can exhibit strikingly different properties
for different choices of K, C , and R. For example, for full-dimensional kernels, the induced regularizer is equal to the
`1 norm of the Fourier space representation of the predictor.
Lemma 1 (K = D). (Lemma 7 in Gunasekar et al., 2018b) For any w ∈ RD, RD,1 (w) = 2kwb k1 .
On the other hand, when K = 1, we can show the following (full proof is provided in the Appendix B):
Lemma 2 (K = 1). For any W ∈ RD, it holds that R1,1(w) = 2√D∣∣Wk2 = 2√D∣∣wk2.
The induced regularizer thus behaves fundamentally differently for K = D and K = 1. In particular, the `2
regularization of R1,1 (w) does not induce sparse solutions, while the `1 regularization of RD,1 (w) promotes sparsity
in the Fourier basis.
Since K = 1 and K = D permit closed-form solutions in the Fourier space, one might hope to obtain similarly clean
characterizations for other kernel sizes as well. However, neither the proof technique for Lemma 1 nor the proof
technique for Lemma 2 extend to the case of general kernel sizes. The proof of Lemma 1 uses the fact that for K = D,
the weights U, V ∈ RD are unconstrained in Fourier space. For networks with smaller kernels, the argument breaks as
Ub = FKU is constrained to be in a K < D dimensional space spanned by the columns of FK . Similarly, the proof of
Lemma 1 uses the special structure for K = 1 of the Ub = FKU pointing in the direction of [1, . . . , 1], which does not
extend to larger kernel sizes. In fact, we show in Appendix B.2 that even for K = 2, the induced regularizer R2,1 (w)
takes a much more complex form.
Even though we do not obtain closed form solutions for all kernel sizes K, we derive important properties about the
induced regularizer for general kernel sizes in the following sections.
4	Main technical tool: SDP formulation of induced regularizer
To investigate the induced regularizer for general kernel sizes, we construct a semidefinite program (SDP) relaxation
that turns out to be independent of C. In this section, we describe and analyze this SDP formulation for networks on
inputs with a single channel. (We discuss generalizations to the case of multi-channel inputs in Section 5.)
We first reformulate RK,C as an SDP with a rank constraint, which immediately motivates an SDP relaxation that
provides a lower bound on RK,C. As we will show in Theorem 3, this SDP relaxation is actually tight for all K and C,
which enables us to deduce a number of interesting properties of the induced regularizer.
4
Under review as a conference paper at ICLR 2022
RK,C as an SDP with a rank constraint. Combining the definition of RK,C (w) in eq. (5) with the Fourier
representation of w(U, V) in eq. (4), we have the following:
RKC(w) =	min	kUk2 + kVk2	s.t.,	diag(UbVb>) =wb.	(6)
,	U∈RK×C,V∈RD×C
We observe that we can express the objective of eq. (6) as kUk2 + kVk2 = hUU> , Ii + hVV> , Ii. Similarly, the
constraints are give by ∀d∈[D], hUbVb>, eded>i = wb [d], where {ed}d∈[D] denotes the standard basis, or alternatively, as
∀d∈[D],hUV>, Qdi = W[d], where Qd= FKe4e>F ∈ CK× D
The optimization in eq. (6) over U ∈ RK×C, V ∈ RD×C, can thus be specified in terms of a rank C positive
semi-definite matrix Z ∈ R(D+K)×(D+K) that we define below:
Z U[U> V>]	UU> UV>	0	7
Z = V	= VU> VV> < 0.	(7)
The objective and constraints of eq. (6) can now be expressed as linear functions of Z as hZ, Ii, and ∀d∈[D], hZ, Ardeali
2 ∙ Re(W[d]) and(Z, Admgi = 2 ∙ Im(∖W[d]), respectively, where We define (Areal, Admg) as follows:
Qd
0D
and Aidmg
0K
-i Qd
i Qd
0D
Now, we can formulate RK,C (W) as follows:
RK,C(W) = mZ<in0 hZ, Ii
s.t.,	∀d∈[D], hZ, Ardeali = 2 Re(Wb [d])
∀d∈[D] ,hZ,Aidmgi=2Im(Wb[d])
rank(Z) ≤ C.
(8)
The formulation in eq. (8) is non-convex due to the rank constraint. We obtain a natural convex relaxation by dropping
the rank constraint, leading to the following SDP:
RSKDP(W) = min hZ, Ii	s.t., ∀d∈[D], hZ, Ardeali = 2 Re(Wb [d])
Z<0	(9)
∀d∈[D] ,hZ,Aidmgi =2Im(Wb[d]).	()
Remark. By construction, the relaxation provides lower bounds on the induced regularizer: for any K ≤ D, any C,
and any W ∈ RD, it holds that RK,C (W) ≥ RSKDP(W).
Remark. The symmetry properties of Fourier coefficients of real signals gives US that for any D and any W ∈ RD,
W[p] = W[D 一 p] for P ∈ [D]. Thus, although the optimization problems in (8) and (9) are specified with 2 ∙ D
constraints for simplicity, only D of them are unique.
4.1	Tightness of the SDP Relaxation
Our main technical result is that for any kernel size K, the SDP relaxation is tight (in the case of networks with
single-channel inputs). Thus, the induced regularizer RK,C is equivalent to an SDP that only depends upon on the
kernel size K .
Theorem 3. [SDP tightness] For any K ≤ D, any C, and any W ∈ RD, it holds that RK,C(W) = RSKDP(W).
Proof sketch. We can show directly from the KKT conditions that any minimizer Z of the SDP must have rank at
most K. However, to prove Theorem 3, we need to show that there exists a rank 1 solution that has the same objective
value as Z and satisfies the SDP constraints—this does not follow directly from the KKT conditions. Constructing
this rank 1 solution is the main contribution in the proof of Theorem 3. In particular, the following lemma is a key
intermediate result about the convolutional operation and is of independent interest beyond this paper.
Lemma 4. For any 1 ≤ K ≤ D, and for any vectors a, b ∈ RK, there exists a vector c ∈ RK such that a?a+b?b =
c ? c, where convolutions are w.r.t. dimension D.
5
Under review as a conference paper at ICLR 2022
Figure 1: Linear predictors learned by two layer linear convolutional
network for the task of classifying digits 0 and 1 in MNIST. The sub-figures
depict predictors learned by using gradient descent on the exponential loss
for overparameterized networks with kernel size K = (3, 3) and number
of output channels C ∈ {1, 2, 4, 8}(left to right).
C	K:(1,1)	K : (3, 3)	K : (8, 8)
1	10.581	-4.948-	3.875
2	10.571	4.945	3.910
4	10.578	4.945	3.912
8	10.576	4.946	3.881
Figure 2: RbK,C(fGD) = kUk2 + kVk2 of the
predictor learned by gradient descent on ReLU
convolutional networks with bias on both layers,
with different number of output channels C and
kernel sizes K, on the MNIST task. The values
shown are the medians taken over 5 trials.
For K = D, Lemma 4 follows easily from the Fourier space representation (using z ? z = |bz|2), since in this case bc is
unconstrained and can be explicitly constructed as the square root of the Fourier transform of a ? a + b ? b. However,
this construction does not generalize to kernel sizes K < D. In fact, c does not appear to have an explicit closed-form
for general K < D.
In our proof, we show existence of c by leveraging the polynomial representation of convolutions. Using the polynomial
representations, Lemma 4 can be written in the following form: for any real-coefficient polynomials pa , pb with degree
at most K - 1, there exists a real-coefficient polynomial pc of degree at most K - 1 such that:
xK-1pc(x)pc(1/x) = xK-1pa(x)pa(1/x) + xK-1pb(x)pb(1/x).
The remainder of the proof involves implicitly constructing pc in terms of its roots (and leading coefficient). To do so,
we show that the roots of the polynomial xK-1pa(x)pa(1/x) +xK-1pb(x)pb(1/x) satisfy certain structural properties
which allow us to establish the existence of the desired real-coefficient polynomial pc . The full proof of Theorem 3 can
be found in the Appendix C.
4.2	Implications of SDP Tightness in Theorem 3
The first implication of Theorem 3 is that although the optimization in eq. (5) is non-convex, the SDP formulation
allows us to efficiently compute RK,C (w) exactly. In the remainder of the section, we discuss a number of other
interesting properties that we can deduce from Theorem 3. Proofs of the following results are in Appendix D.
4.2.1	RK,C IS INDEPENDENT OF NUMBER OF OUTPUT CHANNELS C
Theorem 3 directly implies that RK,C is independent of C. This means that the linear predictors obtained by fitting
training data and minimizing RK,C (w) will be invariant to C (apart from RK,C (w) having multiple minimizers).
Based on previous work (e.g. Lyu & Li (2020)), this has implications for the asymptotic behavior of gradient descent.
In particular, we can hypothesize that for networks with single channel input, the number of output channels does not
influence the asymptotic predictor learned from gradient descent.
We provide a detailed empirical evaluation of this hypothesis (along with a generalization of this hypothesis for networks
with multi-channel inputs) in Section 6. As a preview, in Figure 1 we show the predictors learned by gradient descent
on an MNIST task on a two-layer linear convolutional network with kernel size K = 3. Furthermore, our experiments
reported Figure 2 also suggest that our theoretical results might also extend to some cases of networks with ReLU non
linearity and bias. In both cases, we see that the induced regularizer is invariant to the number of output channels. We
defer a more extensive empirical evaluation to Section 6.
4.2.2	RK,C IS A NORM
Another interesting corollary of Theorem 3 is that the induced regularizer is a norm for any K.
Corollary 5. For K ≤ D and any C, RK,C (w) is a norm.
For the end cases of K = 1 and K = D, this norm can be explicitly specified: Rι,c(W) = 2√D∣∣W∣∣ (Lemma 2)
and RD,C (w) = 2kwb k1 (Lemma 1), respectively. For intermediate kernel sizes, RK,C (w) is a norm that interpolates
between the `2 norm and the `1 norm of the Fourier coefficients of the linear predictor. We further use the SDP in (9) to
compute upper and lower bounds on RK,C (W) in terms of the `2 norm and `1 in Fourier space (see Lemma 15).
6
Under review as a conference paper at ICLR 2022
4.2.3	Connection to previous work
Comparison to Pilanci & Ergen (2020); Ergen & Pilanci (2020a;b); Sahiner et al. (2020). These works study
the induced regularizer of neural networks by looking at the bi-dual convex relaxation of the `2 regularized least squares
loss. In comparison to our SDP, this method is a complementary approach to derive lower bounds on the induced
regularizer when minimizing convex losses over datasets. In both cases, the relaxations are trivially tight in the limit of
infinitely many output channels. However, in our work, we use the SDP formulation to show a significantly stronger
result compared to these prior work.
Phrased in the terminology of our work, the results in Pilanci & Ergen (2020) on linear convolutional networks show that
for networks with a single input channel, if the number of output channels (or width) C is larger than a data-dependent
threshold, then RK,C (w) = RSKDP (w). That is, they show that the induced regularizer is independent of C after C
is above a certain large finite value that can be large as the dataset size. In contrast, we show the induced regularizer
independent of C for any C ≥ 1. Further, our analysis and results hold regardless of the dataset and training loss.
Comparison to Gunasekar et al. (2018b). Gunasekar et al. (2018b) characterized the induced regularizer for single-
channel networks with full-dimensional kernels. As we discussed in Section 3, the conclusions and closed-form solution
for K = D do not extend to the full class of two-layer linear convolutional networks. In contrast, our result (Theorem
3) implies properties of the induced regularizer for networks with arbitrary kernel size.
Moreover, these prior works focus on networks with single-channel inputs, whereas we extend our results to networks
with multi-channel inputs in the next section.
5	Networks with multi-channel inputs
While we focused on networks with single-channel inputs in the previous sections, we now expand our results to
networks with multiple input channels (e.g., RGB color channels). How do these conclusions change for multiple input
channels? How does the induced regularizer, now denoted as RK,C,R, depend on the number input channels R?
We again consider two layer convolutional networks akin to Section 2. We first introduce additional notation: The
multi-channel inputs are denotes as X ∈ RD×R, where R denotes the number of input channels. The convolutional
first layer now has kernel size K, output channel size C and input channel size R with weights denoted by a set of
R matrices U = {Ur}r∈[R] with Ur ∈ RK×C. The output of this convolution layer h(U; X) ∈ RD×C is given as
follows:
R-1
∀c∈C, h(U; X)[:, c] = X Ur[:,c]?X[:,r].	(10)
r=0
The second layer is the same as before: a single output linear layer with weights V ∈ RD×C . We denote the equivalent
linear predictor for this network by W(U, V) ∈ RD×R. Following similar calculations as for single input channels,
W(U, V) in signal and Fourier domain (denoted as W(U, V) = FW(U, V)) are given as follows:
C-1
∀r∈[R], W (U, v)[:,r] = X(UJ,c] ? V[：,c]J); and c(U, V)[：,r] = diag(Ur V >).	(11)
c=0
For multi-channel inputs, the set of all linear predictors is the space of matrices W ∈ RD×R, and we define the induced
complexity measure over this matrix space as follows:
RK,C,R(W) := inf X kUrk2+kVk2	s.t.,	W(U, V) =W.	(12)
U,V r∈[R]
5.1	ROLE OF OUTPUT CHANNEL SIZE C
For multi-channel inputs, we first observe that multiple output channels can be necessary to realize all linear maps. To
see this, we show that the sub-network corresponding to each output channel can realize a matrix in RD×R of rank at
most K, which places an upper bound on the total rank achievable by the full network (see a proof in Appendix E.2).
This implies the following lemma:
Lemma 6. For any K, C and R, in order for the the network represented by W(U, V) in eq. (11) to realizes all linear
maps in Rd×r it is necessary that K ∙ C ≥ min{R, D}.
7
Under review as a conference paper at ICLR 2022
In contrast to single input channels, Lemma 6 demonstrates that, the model class realized by linear convolutional
networks over multi-channel inputs, and consequently the induced regularizer, does depend on number of output
channels C. Nonetheless, similar to single input channel networks, we can again obtain an SDP relaxation RSKD,PR(W)
for RK,C,R(W) that is independent of C. Due to space constraints, we defer the SDP relaxation to Appendix E.1.
5.2	Tightness of SDP Relaxation
Unlike networks with single channel input, the SDP relaxation here is not always tight when R > 1, since sufficiently
large C is required to merely realize all matrix-valued linear function over the input space. We can however show a
weaker form SDP tightness from the KKT conditions when there are sufficiently many output channels:
Lemma 7. For any W ∈ RD×R, and any C ≥ RK, it holds that RK,C,R (W) = RSKD,PR (W).
Note that the above bound on C for SDP tightness is not sharp, as we showed for R = 1 in Theorem 3. Based on our
insights from the proof of single-channel SDP tightness in Theorem 3 and additional empirical evidence in Appendix A,
we conjecture that SDP tightness holds when C ≥ R:
Conjecture 8. For any W ∈ RD×R, and any C ≥ R, it holds that RK,C,R (W) = RSKD,PR(W).
In the next subsection, we prove Conjecture 8 in the special cases of K = 1 and K = D. As a consequence, we show
that once C is large enough to realize all linear maps, RK,C,R(W) can be expressed as interesting closed form norms
independent of C in these special cases.
5.3	INDUCED REGULARIZER WHEN K = 1 AND K = D
Theorem 9.	For any W ∈ RD×R, and any C ≥ min{R, D}, the induced regularizer for K = 1 is given by the scaled
nuclear norm kJ∣*:
Ri,c,r(W) = 2√D∣Wk * = 2√D∣Wk *.
Theorem 10.	For any W ∈ RD×R, and any C ≥ 1, the induced regularizer for K = D is given as follows
D-1
RD,c,R(W)=2kWck2,1 := X
d=0
R-1
X Wc[d,r]2.
r=0
∖
From Theorems 9-10 it is evident that the number of input channels R fundamentally changes the nature of induced
complexity measure in the function space and introduces additional structures along the input channels. Even in
the simplest setting of scalar convolution kernels with K = 1, the induced regularizer is no longer a Euclidean or
RKHS norm, and is instead a richer nuclear norm that encourages low-rank properties. For the case of K = D, the
induced regularizer is group-sparse norm on the Fourier coefficients that encourages similar weighting across channels,
while promoting sparsity across frequency components. In comparison to the `1 norm of all Fourier coefficients, this
group-sparse norm is a more structured inductive bias for multi-channel inputs. Additionally, like with the single input
channel case, we also observe that the induced bias has a more intuitive and interesting interpretation in Fourier domain
which is not directly observed in the signal domain.
6	Experiments
We now explicitly connect our findings to the implicit regularization of gradient descent. We formally state the following
result by Lyu & Li (2020) that relating the asymptotic implicit bias of gradient descent to `2 norm minimization of the
parameters:
Theorem. (Paraphrased from Lyu & Li, 2020) Assume that Φ is locally Lipschitz and positive homogeneous with order
L > 0, i.e., ∀θ,α>0, Φ(αθ; .) = αLΦ(θ; .). Consider minimizing an exponential-tailed loss over a separable binary
classification dataset {(xn, yn)}nN=1. Under assumptions of loss convergence, gradient flow on this loss converges in
direction to a first order stationary point (KKTpoint) of the following max-'2 margin problem in parameter space:
min kθk22 s.t., ∀nynΦ(θ; xn) ≥ 1.	(13)
θ
We thus expect the implicit bias from gradient descent to be related to the following max-R-margin problem
min RΦ (f) s.t., ∀nynf(xn) ≥ 1.
8
Under review as a conference paper at ICLR 2022
However, theoretically speaking, there is an important caveat: the theorem by Lyu & Li (2020) shows convergence of
gradient flow direction to a stationary point of the optimization in eq. (14), which might not be a global minimum;
and in fact it need not even be a stationary point of the max-R margin problem in the function space. Moreover, the
non-asymptotic behavior and rate of convergence of the learned predictor are not well-understood.
Nonetheless, if we overlook these caveats, our findings would then have important implications for predictors learned
from gradient descent. In particular, our result regarding the invariance of the induced regularizer with respect to the
number of output channels suggests that the asymptotic behavior of gradient descent is similarly invariant to the number
of output channels. We formalize this as a testable hypothesis.
Hypothesis 1. For a separable binary classification task with R input channels, let wGD be the predictor learned using
stochastic gradient descent on a two-layer convolutional network with kernel size K, C output channels, and R input
channels (where wGD is normalized to have unit margin on the training data). Then, as long as C ≥ R, the induced
regularizer RK,C,R(wGD) is invariant in the number of output channels C.
The primary goal of our experiments is to test this hypothesis. We show experimental support for the hypothesis on
small linearly separable subsets of MNIST (with 128 images of size 28 × 28 balanced across 2 classes) and CIFAR-10
(with 512 images of size 32 × 32 balanced across 2 classes) datasets. Most of our experiments are for multi-channel
linear convolutional networks trained using stochastic gradient descent. We also provide a some experiments on ReLU
networks, where we see support of our hypothesis well beyond our theoretical study.
Throughout the experiments sections, since we cannot always compute RK,C,R(wGD), we approximate it using the
weight norms of the trained network RbK,C,R(wGD) = PU,V kUk2 + kVk2, where U, V here denote the weights of
the trained network.4 The experiments are deferred to Appendix A and we summarize the findings below.
1.
2.
3.
4.
Single input channel binary classification on MNIST. In linear networks, we compare the predictors learned by
gradient descent for K ∈ {1, 3, 8, 16, 28} and C ∈ {1, 2, 4, 8} across 10 runs with random initialization. We
see that both the values of estimated
regularizer Rb as well
as the visualization of linear predictors in signal and
frequency domain are nearly invariant to C (the values overlap within one standard deviation across runs).
3-input channel binary classification on CIFAR-10. In a similar setup to MNIST, we compare K ∈ {1,3, 8, 20}
and C ∈ {1, 2, 3, 4, 8}. As expected from our theory, we see differences in the induced regularizer Rb for
C < 3, but observe invariance to C once C ≥ 3.
ReLU networks for binary classification on MNIST. Although our theory is only for linear networks, our
hypothesis as stated above can also be tested on networks with non-linearity. We repeat our MNIST experiments
on networks with ReLU non-linearity (with and without bias parameters). Interestingly, we observe that the
estimated induced regularizer Rb is invariant to C suggesting a broader scope for our hypothesis. Altogether,
these findings support Hypothesis 1, including in networks beyond the scope of our theoretical results.
We also provide support for our theoretical findings about the role of kernel size. On both MNIST and
CIFAR-10, we show that larger kernel sizes favor sparsity in the frequency domain and that the learned
predictors experience similarity across input channels for smaller kernel sizes.
7	Discussion
Towards understanding neural networks, the representation cost or the induced regularizer viewpoint provides an
abstraction to separate capacity control in the parameter space from the function space implications of the resulting
inductive bias. In this paper, we showed that when minimizing `2 norm of weights, the two basic architectural
components of convolutional networks—number of output channels (width) and kernel size—have interesting effects
even in the simple case of two-layer linear networks. Our results inspire a broader hypothesis about the impact of
number of output channels which we test and provide support for in our experiments. Our experiment show promise
for applicability of our hypothesis beyond our theoretical finding, including to non-linear networks. An immediate
direction for future work thus would be to expand on our experimental findings and conduct an in-depth empirical
investigation of the impact of non-linearity.
There are also many interesting directions to extend our theoretical findings, including: proving tightness of the SDP
relaxation for multiple input channels (we have formalized this as a plausible Conjecture 8); formally establishing the
limiting behavior of gradient descent (without the caveats that we discussed); and exploring architectural features such
as pooling or multiple layers.
4In theory Rb only provides an upper bound on R—but in case of predictors learned by SGD, upon checking instances where R
has a closed form solution, we found that the approximation is quite accurate.
9
Under review as a conference paper at ICLR 2022
References
Peter L. Bartlett. For valid generalization the size of the weights is more important than the size of the network. In
Advances in Neural Information Processing Systems, 1996.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.
Journal of Machine Learning Research ,pp. 463-482, 2002.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In
Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Assaf Dauber, Meir Feder, Tomer Koren, and Roi Livni. Can implicit bias explain generalization? stochastic convex
optimization as a case study. In Advances in Neural Information Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Tolga Ergen and Mert Pilanci. Convex duality of deep neural networks. CoRR, abs/2002.09773, 2020a. URL
https://arxiv.org/abs/2002.09773.
Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization of two- and
three-layer networks in polynomial time. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, 2020b.
Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization
geometry. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1827-1836, 2018a.
Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional
networks. In Advances in Neural Information Processing Systems, 2018b.
Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR, abs/1803.07300, 2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In International Conference
on Learning Representations (ICLR), 2019.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In Advances in Neural
Information Processing Systems, 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In Advances in Neural Information
Processing Systems, pp. 950-957, 1991.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/
exdb/mnist/.
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix
factorization: Greedy low-rank learning. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. In International
Conference on Learning Representations (ICLR), 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason D. Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and
depth-sensitive margins in homogeneous and non-homogeneous deep models. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 4683-4692, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit
regularization in deep learning. In International Conference on Learning Representations (ICLR), Workshop Track
Proceedings, 2015.
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of bounded norm infinite width
relu nets: The multivariate case. In International Conference on Learning Representations (ICLR), 2020.
Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time convex optimization
formulations for two-layer networks. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp.
7695-7705. PMLR, 2020. URL http://proceedings.mlr.press/v119/pilanci20a.html.
10
Under review as a conference paper at ICLR 2022
Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
Jason D. M. Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In
Proceedings ofthe International Conference on Machine Learning (ICML), pp. 713-719, 2005.
Arda Sahiner, Tolga Ergen, John M. Pauly, and Mert Pilanci. Vector-output relu neural network problems are copositive
programs: Convex analysis of two layer networks and polynomial-time algorithms. CoRR, abs/2012.13329, 2020.
URL https://arxiv.org/abs/2012.13329.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm networks look in
function space? In Proceedings of the Conference on Learning Theory (COLT), pp. 2667-2690, 2019.
Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural
nets v.s. their induced kernel. In Advances in Neural Information Processing Systems, pp. 9709-9721, 2019.
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi. A unifying view on implicit bias in training linear neural
networks. In International Conference on Learning Representations (ICLR), 2021.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires
rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C. Mozer, and Yoram Singer. Identity crisis: Memorization
and generalization under extreme overparameterization. In International Conference on Learning Representations
(ICLR), 2020.
11
Under review as a conference paper at ICLR 2022
A Experiments for gradient descent
Theorem. (Paraphrased from Lyu & Li, 2020) Assume that Φ is locally Lipschitz and positive homogeneous with order
L > 0, i.e., ∀θ,α>0, Φ(αθ; .) = αLΦ(θ; .). Consider minimizing an exponential-tailed loss over a separable binary
classification dataset {(xn, yn)}nN=1. Under assumptions of loss convergence, gradient flow on this loss converges in
direction to a first order stationary point (KKTpoint) of the following max-'2 margin problem in parameter space:
min kθk22 s.t., ∀nynΦ(θ; xn) ≥ 1.	(14)
θ
We run our experiments on two layer linear convolutional networks on a subset of MNIST dataset (LeCun & Cortes,
2010) as well a subset of the CIFAR-10 dataset (Krizhevsky, 2009). The input images in MNIST are of size 28 × 28 and
have a single input channel. The input images in MNIST are of size 32 × 32 and have a 3 input channels. We apply 2D
convolutions with kernel sizes K = (K1, K2) and circular padding for image inputs. We consider binary classification
task for both datasets. For MNIST, we predict digits 0 and 1 in MNIST using a balanced sub-sampling of 128 samples
as training data, which ensures linear separability. For CIFAR-10, we predict classes “automobile” and “dog” using a
balanced sub-sampling of 256 samples as training data, which ensures linear separability. The initialization scale was
taken to be 0.001.
We train our network using gradient descent on exponential loss and run gradient descent until the training loss is 10-6.
The initialization scale is taken to be 0.001 in order to reduce the variance arising from the randomness of initialization.
In order to compare the predictors across different architectures, we normalize the weights learned by gradient descent
U, V such that the linear predictor w(U, V) realized by the trained networks has unit margin on the training dataset
(i.e., yhw(U, V), xi ≥ 1 for all training samples (x, y)). Note that for homogeneous models, such positive scaling of
weights does not change the classification boundary of the learned model.5
A.1 Impact of the number of channels on MNIST
For networks with a single-input channel, Hypothesis 1 would imply that RK,C,R(w(U, V)) is invariant in the number
of output channels C regardless of C. To demonstrate this, we repeat the experimental setup on 28 × 28 MNIST images
on networks with multiple output channels C ∈ {1, 2, 4, 8} and across different kernel sizes. As described earlier, we
scale the weights learned by gradient descent U, V such that the linear predictors w(U, V) = wGD have unit margin
on training data. Since it is difficult to directly compute RK,C,R(wGD), we turn to an approximation. In particular, we
compute Rb K,C (w (U, V)) := kUk2 + kVk2. Strictly speaking, this is only an upper bound on the induced regularizer
RK,C,R(w(U, V)).6
Table 1 shows RK,C (w(U, V)) across different values of K and C. We see that for each kernel size, the differences
in RK,C (w(U, V)) across different settings of C are minimal and are usually smaller than the standard deviation for
fixed settings of C . This suggests that the induced regularizer is indeed invariant to the number of output channels, thus
providing evidence for Hypothesis 1 in the case of a single input channel.
C	K=(1,1)	K = (3, 3)	K =(8, 8),	K = (16, 16)	K= (28, 28)
1	10.28 ± 2.34 × 10-5	4.50 ± 1.51 × 10-3	3.32 ± 5.35 X 10-2	3.15 ± 5.81 × 10-2	2.84 ± 1.16 × 10-1
2	10.28 ± 2.00 × 10-5	4.50 ± 1.06 × 10-3	3.30 ± 3.13 × 10-2	3.10 ± 2.63 × 10-2	2.79 ± 1.27 × 10-1
4	10.28 ± 1.00 × 10-5	4.50 ± 7.48 × 10-4	3.30 ± 2.25 × 10-2	3.10 ± 3.33 × 10-2	2.77 ± 8.20 × 10-2
8	10.28 ± 7.83 × 10-5	4.50 ± 6.25 × 10-4	3.29 ± 1.68 × 10-2	3.11 ± 3.27 × 10-2	2.72 ± 7.31 × 10-2
Table 1: RbK,C (w(U, V)) = kUk2 + kVk2 of the predictor learned by gradient descent on linear convolutional networks with
different number of output channels C and kernel sizes K on the MNIST task. We show the mean over 10 trials as well as the
standard deviations are also shown.
Invariance of learned predictors to C. While Hypothesis 1 primarily pertains to the behavior of the induced
regularizer, it also suggests that the predictor wGD will also be independent of the number of channels so long as K
is strictly less than D. If We overlook the caveats, We expect the gradient descent to implicitly learn a max-Rκ,c
margin predictor: minw RK,C (w) s.t., ∀nyn hw, xni ≥ 1. For K < D, our theoretical findings suggest that the
5The code is available at https://github.com/mjagadeesan/inductive-bias-multi- channel-CNN.
6For K = 1 and K = D, We verified that the estimate is close to tight by computing the `2 and `1 norms of Fourier transform of
the predictor, respectively.
12
Under review as a conference paper at ICLR 2022
■■■
(a) C = 1
(b) C = 2

i


(c) C = 4
(d) C = 8
Figure 3:	Linear predictors learned by two layer linear convolutional network for the task of classifying digits 0 and 1 in MNIST.
The sub-figures depict predictors learned by using gradient descent on the exponential loss for overparameterized networks with
C = 1, 2,4 and kernel sizes K ∈ {(1,1), (3, 3), (8, 8), (16,16), (28, 28)} (left to right). The top row in each sub-figure is the
signal domain representation w(U, V), and the bottom row is the Fourier domain representation wb(U, V).
Figure 4:	Explicit RK,C margin predictor on sampled MNIST dataset for kernel sizes K ∈ {(1, 1), (28, 28)} (left to right). The
top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domain representation
wb(U, V).
13
Under review as a conference paper at ICLR 2022
induced regularizer is a norm interpolating between the `2 and `1 norms. This would mean that there is a unique global
minimizer, and thus we would expect that wGD to be invariant to C .
To empirically validate this, we show the learned linear predictors for C = {1, 2, 4} in Figure 3. We observe that the
linear predictors indeed visually appears to be invariant across different settings of C for all for kernel sizes K < D.
For K = (28, 28), there appear to be differences in the predictors—this likely arises from the fact that there are multiple
linear predictors that minimize the `1 norm on the dataset. We nonetheless emphasize that the induced regularizer still
appears to be invariant in this case, although the predictors are not.
A.1.1 Non-linear networks with ReLU activation
Although our theoretical results are restricted to networks with linear activations, it is nevertheless interesting to evaluate
if our conclusions lead to useful heuristics for networks with non-linearity. As a simple demonstration, we repeat our
experiment on MNIST on two-layer convolutional networks with ReLU non-linearity with and without bias parameters
(i.e., networks with a convolution layer, followed by ReLU layer, followed by linear layer).7 As before, we first scale
the weights learned by gradient descent such that the resulting predictor fGD has unit margin on training data. We
then consider the representation cost RΦK,C (fGD), as per equation (1), given by the minimum `2 norm of the weights
needed to realize f. We consider the approximation of RΦK,C (fGD) given by RbΦK,C (fGD) := kUk22 + kVk22 where
U and V are the weights learned by gradient descent. (As before, strictly speaking, this is only an upper bound on the
representation cost RΦK,C (fGD).)
Table 2 and Table 3 show RbΦK,C (fGD) := kUk22 + kVk22 across different settings of C and K, for networks with no
bias as well as networks with bias parameters on both the convolution layer and the fully connected layer.8 Like in the
case of linear convolutional neural networks, RbΦK,C (fGD) is consistent across different settings of C. This suggests
that the implicit bias from gradient might result in predictors that are independent of the number of output channels,
even when there is a ReLU layer, and Hypothesis 1 might hold in much more generality than the scope of theoretical
findings.
C	K:(1,1)	K: (3, 3)	K : (8, 8)	K : (16, 16)	K : (28, 28)
1	11.412	5.160	-3.998-	3.785	3.520
2	11.413	5.155	3.964	3.721	3.539
4	11.414	5.153	3.966	3.719	3.448
8	11.415	5.156	3.971	3.738	3.498
Table 2: Rb K,C (fGD) = kUk2 + kVk2 of the predictor learned by gradient descent on ReLU convolutional networks without bias
parameters, with different number of output channels C and kernel sizes K, on the MNIST task. The values shown are the medians
taken over 5 trials.
C	K:(1,1)	K: (3, 3)	K : (8, 8)	K : (16, 16)	K : (28, 28)
1	10.581	4.948	-3.875-	3.714	3.519
2	10.571	4.945	3.910	3.698	3.413
4	10.578	4.945	3.912	3.712	3.399
8	10.576	4.946	3.881	3.697	3.437
Table 3: Rb K,C (fGD) = kUk2 + kVk2 of the predictor learned by gradient descent on ReLU convolutional networks with bias on
both layers, with different number of output channels C and kernel sizes K, on the MNIST task. The values shown are the medians
taken over 5 trials.
We note that we observed that gradient descent sometimes leads to outliers where RΦK,C (fGD) is very large. For
example, when K = 1, the values of RΦK,C (f) are [11.412, 11.412, 109.471, 11.412, 11.412], where 109.471 appears
to be an outlier. We anticipate that this outlier arises because gradient descent converges to a stationary point, rather
than a local minima, of the max-'2 margin problem in parameter space (See the discussion in Section 1.1). Since our
goal is to investigate the behavior of gradient descent when it does lead to global minima of the max - RΦ margin
problem, we compute the median so that these data points do not affect our estimate.
7The initialization scale was taken to be 0.005 for networks without bias parameters and 0.01 for networks with bias parameters.
8We note that the representation cost includes the magnitude of the weights but not the magnitude of the biases.
14
Under review as a conference paper at ICLR 2022
A.2 Impact of the number of channels on CIFAR- 1 0
We carry out a similar investigation of Hypothesis 1 on the CIFAR-10 dataset for networks with 3-channel inputs. As
discussed in Section 5, we expect that the induced regularizer is not independent of the number of output channels for
C < R, but begins to exhibit invariance once C ≥ R. On the CIFAR-10 dataset, where there are 3 input channels, we
would expect to see invariance once C ≥ 3.
To demonstrate this, we repeat the experimental setup on 32 × 32 CIFAR-10 images on networks with multiple output
channels C ∈ {1, 2, 4, 8} and across different kernel sizes. As described earlier, we scale the weights learned by
gradient descent U, V such that the linear predictor w(U, V) = wGD has unit margin on training data. Since it is
difficult to directly compute RK,C,R(wGD), we turn to an approximation, as we did in the case of single-input channels.
We compute RbK,C,R(w(U, V)) := kUk2 + kVk2 which strictly speaking, this is only an upper bound on the induced
regularizer RK,C,R (w(U, V)).
Table 4 shows RbK,C,3 (w(U, V)) across different values of K and C. We see that for each kernel size, the differences
in RbK,C,3(w(U, V)) across different settings of C are minimal, as long as C ≥ 3 (and often, even when C ≥ 2).
This suggests that the induced regularizer is indeed invariant to the number of output channels when C ≥ 3, thus
providing evidence for Hypothesis 1 in the case of multiple input channels. Moreover, there are non-trivial differences
in RbK,C,3 (w(U, V)) for C = 1 and larger C—this aligns with our theoretical findings in Section 5 that the induced
regularizer does depend on C when it is below R.
While Hypothesis 1 primarily pertains to the behavior of the induced regularizer, we would also expected that there is a
unique global minimizer in most cases, for reasons similar to for the single input channel case. Thus, we would expect
wGD to be invariant to C as long as C ≥ R = 3. To empirically validate this, we show the learned linear predictors for
C = {1, 2, 3, 4, 8} in Figures 5-7. We observe that the linear predictors indeed visually appears to be invariant across
different settings of C.
C	K=(1,1)	K = (3, 3)	K=(8,8)	K = (20, 20)
1	246.04	215.21	202.27	131.16
2	246.26	182.77	168.40	124.66
3	245.98	182.80	165.32	123.56
4	246.29	182.83	164.50	123.37
8	245.58	182.82	164.86	123.59
Table 4: Rb K,C,3 (w(U, V)) = kUk2 + kVk2 of the predictor learned by gradient descent on linear convolutional networks with
different number of output channels C and kernel sizes K on the CIFAR-10 task.
A.2.1 Convolutions with zero padding
We additionally rerun the same setup as in the previous section with zero padding rather than circular padding. Table 5
shows RK,C,3 (w(U, V)) across different values of K and C. Although convolutions with zero padding goes beyond
the scope of our theoretical results, we show that Hypothesis 1 nonetheless still holds.
C	K=(1,1)	K = (3, 3)	K=(8,8)	K = (20, 20)
1	246.04	225.64	217.14	177.07
2	245.98	191.28	186.32	152.58
3	245.94	191.34	182.18	154.84
4	245.98	191.20	182.42	152.48
8	245.34	191.38	180.28	150.88
Table 5: RbK,C,3(w(U, V)) = kUk2 + kVk2 of the predictor learned by gradient descent on linear convolutional networks with
zero padding with different number of output channels C and kernel sizes K on the CIFAR-10 task.
A.3 Varying kernel sizes
While the number of output channels has little influence on the induced regularizer of the learned predictors, we show
that the kernel size can have significant impact, which aligns with our theoretical findings.
15
Under review as a conference paper at ICLR 2022
(a) C = 1
(b) C = 2
(c) C = 3
(d) C
4
(e) C = 8
Figure 5:	Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictors
learned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel size
K = (1,1). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domain
representation wb(U, V).
16
Under review as a conference paper at ICLR 2022
(a) C = 1
(b) C = 2
Figure 6: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictors
learned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel size
K = (3, 3). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domain
representation wb(U, V).
17
Under review as a conference paper at ICLR 2022
(a) C = 1
(c) C = 3
(b) C = 2
(e) C = 8
Figure 7: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict predictors
learned by using gradient descent on the exponential loss for overparameterized networks with C ∈ {1, 2, 3, 4, 8} and kernel size
K = (8, 8). The top row in each sub-figure is the signal domain representation w(U, V), and the bottom row is the Fourier domain
representation wb(U, V).
18
Under review as a conference paper at ICLR 2022
o-
20-
40-
60-
IOO-
o-
20-
40-
60-
IOO-
40-
60-
0
20
40-
60-
0
20
40
60
40
60-
Figure 8: Linear predictors learned by gradient descent on single output channel networks over an augmented input space with
kernel sizes K ∈ {(1, 1), (3, 3), (27, 27), (45, 45), (65, 65), (84, 84)} (left to right). The input images from the MNIST dataset are
augmented by padding with zeros to obtain an image of size 112 × 112 with the signal present only in the top-left 28 × 28 block.
A.3.1 Effect of kernel size on MNIST
Our theoretical findings in Section 4.1 suggest that the induced regularizer interpolates between `2 and `1 norms in
the Fourier domain. The `2 regularization of R1,1(w) does not induce sparse solutions, while the `1 regularization of
RD,1 (w) promotes sparsity in the Fourier basis. This would suggest the following: larger kernel sizes induce sparsity
in the frequency domain.
Explicit optimal solutions for K = (1, 1) and K = (D, D).	First, to illustrate in the extreme cases of K = 1 and
K = D, we explicitly compute the RK,C margin predictor:
mwin RK,C (w) s.t., ∀nyn hw, xni ≥ 1
on the dataset using the closed-form solutions for the induced regularizer in these special cases. In Figure 4, we show
resulting optimal solutions for K = D (a minimum `1 solution) and K = 1 (a minimum `2 solution). While the
solution for K = (1, 1) exhibits no sparsity in the frequency domain, the solution for K = (D, D) exhibits significant
sparsity.
The corresponding values of RK,C are 9.32 for K = D and 2.10 for K = 1. We note that the induced regularizer do
not exactly match those computed on the w(U, V) from gradient descent—this is because the convergence of these
values can be quite slow. Moreover, for K = D, the difference in the predictors likely stems from the minimum
`1 -norm solution being non-unique. We nonetheless show that the qualitative findings apply to gradient descent, despite
the fact that the limiting values have not been reached.
Extension to gradient descent. Consider networks with one output channel C = 1 and compute w(U, V) learned
by gradient descent for networks with different kernel sizes in Figure 3-(a). Notice in the frequency domain plots that
the predictor learned with kernel size K = (1, 1) is not sparse, the predictor learned with K = (3, 3) already starts to
exhibit some sparsity, and the linear predictor learned with K = (28, 28) is highly sparse in the frequency domain.
Since sparsity in the frequency domain promotes a patterned structure in the signal domain, we explore the qualitative
behavior of large kernel sizes in the signal domain in more depth. To do this, we construct an augmented version of the
dataset with 112 × 112 dimensional images where the top-left 28 × 28 region is the original image, while the remaining
space is all 0s. Figure 8 shows the linear predictors learned by running gradient descent on single output channel
networks with different kernel sizes. As K increases, the nonzero region of the predictor becomes larger, eventually
encompassing the full 112 × 112 dimensional space. For large kernel sizes, we can visually see that the predictors are
composed of repetitions of a pattern. This is suggestive of a restricted form of periodic translation invariance, where the
shift size aligns with the size of the patterns.
A.3.2 Effect of kernel size on CIFAR- 1 0
We now examine the role of kernel size for multi-channel networks on CIFAR-10. Our theoretical findings in Section 5
suggest that the induced regularizer interpolates between the nuclear norm for K = 1 and the `2,1 norm for K = D.
This would again suggest the following: larger kernel sizes induce sparsity in the frequency domain. The behavior
across input channels, however, is more nuanced. Both of these norms favor similarities across different input channels
(with the effect intuitively stronger for K = 1 since the nuclear norm is closely related to rank). We explore both of
these effects in the following experiments.
Explicit optimal solutions for K = (D, D). First, to illustrate in the extreme case ofK = D, we explicitly compute
the RK,C margin predictor: minw RK,C (w) s.t., ∀nynhw, xni ≥ 1 on the dataset using the closed-form solutions for
19
Under review as a conference paper at ICLR 2022
(c) `2
Figure 9: Explicit '2,1, '1 and '2 margin predictors on sampled CIFAR-10 dataset. The top row in each sub-figure is the signal
domain representation w(U, V), and the bottom row is the Fourier domain representation wb(U, V)
the induced regularizer in these special cases. In Figure 9, we show resulting optimal solutions for K = D (a minimum
`2,1 solution) along with the optimal `1,1 solution. We visually see that the `2,1 solution favors similarity across input
channels at the expense of greater sparsity in the frequency domain.
The corresponding values of RD,C for the minimum `2,1 norm solution is 82.85 for K = (D, D).) As for the
single-input channel case, we note that the induced regularizer do not exactly match the value of 114.85 of the induced
regularizer computed on the w(U, V) from gradient descent—this is because the convergence of the induced regularizer
is known to be slow. We nonetheless show that the qualitative findings apply to gradient descent, despite the fact that
the limiting value of induced regularizer has not been reached.
Extension to gradient descent. Consider networks with one output channel C = 3 and compute w(U, V) learned
by gradient descent for networks with different kernel sizes in Figure 10. First, the higher kernel does indeed favor
sparsity in the frequency domain, as in the single-input channel case. Nontrivial sparse structures can be observed
even for K = 3. Next, we discuss the predictor across different input channels. Let’s first focus on the extreme case
of K = 1. For sake of comparison, we show the explicit minimum `2 (Frobenius) predictor in Figure 9 (note that
this is not the R1,C margin predictor because the induced regularizer is related to the nuclear norm, not the `2 norm).
As expected, we see that the learned predictor has a greater degree of similarity across channels than the `2 predictor.
For other kernel sizes, Figure 10 also shows some degree of similarity across input channels, although the differences
appear to grow as K becomes larger.
20
Under review as a conference paper at ICLR 2022
(a) K = (1, 1)
(b) K = (3, 3)
(c) K = (8, 8)
(d) K = (20, 20)
(e) K = (32, 32)
Figure 10: Linear predictors learned by two layer linear convolutional network on CIFAR-10 task. The sub-figures depict
predictors learned by using gradient descent on the exponential loss for overparameterized networks with C = 3 and kernel sizes
K ∈ {(1,1), (3, 3), (8, 8), (20, 20), (32, 32)}. The top row in each sub-figure is the signal domain representation w(U, V), and
the bottom row is the Fourier domain representation wb(U, V).
21
Under review as a conference paper at ICLR 2022
B Proofs for Section 3: Induced regularizer in special cases
For a single channel convolutional network (i.e., C = 1), we denote the weights in the first and second layer as U ∈ RK
and V ∈ RD , respectively. Recall that the D dimensional discrete Fourier transform of the weights U, V and the linear
predictor w ∈ RD are denoted as Ub = FKU, Vb = FV, wb = Fw, respectively. Moreover, the Fourier transform is
normalized to be unitary such that for any Z ∈ RK, kz∣∣ = ∣∣bk and FF = FF> = I. Thus, for all the quantities of
interest, we use the `2 norm in signal domain interchangeably with `2 norm in the Fourier domain, e.g., kUk = kUb k,
∣V∣ =∣Vb∣,and∣w∣ =∣wb∣.
In the following proofs, we use the formulation of RK,C (w) in eq. (6) for the case of C = 1 as:
RK1(w) = min ∣Ub ∣2 + ∣Vb ∣2	s.t.,	Ub	Vb = wb.
U∈RK,V∈RD
B.1	Proof of Lemma 2
Lemma 2 (K = 1). For any W ∈ RD, it holds that R1,1(w) = 2√D∣W∣2 = 2√D∣w∣2.
Proof. This statement is trivially true for w = 0, so it suffices to show this for w 6= 0. When the K = 1, the first layer
weight U ∈ R1 ×1 is a scalar. Let this scalar be U = u = 0. We then have U = √= [u,u,...,u]. Since U Θ V = W,
We have V = √DW. This means that R1,1(w) = minu ∣∣U∣∣2 + ∣∣^V∣2 = minu u2 + D∣∣W∣2. By using the AM-GM
inequality (a2 + b2 ≥ 2ab), this is at most 2√D∣W|卜.Moreover, we can pick u2 = √D∣W|卜 to achieve equality. □
B.2	INDUCED REGULARIZER FOR K = 2
In the following lemma we show that for K = 2 the characterization of R2,1 (W) in Fourier space involves a
maximization over a high-degree rational function, and is thus unlikely to admit clean closed-form solutions.
Lemma 11. For any W ∈ RD , it holds that:
R2 i(w) = 2√D.	inf	X -~|W[d]][∕ 小.
,	∖ ɑ∈(-i,i)	1 + α cos (2πd∕D)
Although Lemma 11 does not yield closed form solutions for R2,1 (W), we observe that it hints at some form of
band-pass frequency structure: for any α from the inner optimization, the resulting regularizer is a weighted sum of
Fourier coefficients such that the nearby frequency components of Wb are weighted with nearby values. This band-pass
nature was also observed in a complementary result by Yun et al. (2021) in the context of implicit bias from gradient
descent on a single data point: for any x, it was shown that minw R2,1(W) s.t., W>x > 1 corresponds to a low-pass or
high pass filter depending on the sign of x>x1.
Proof of Lemma 11. We first note that for any U ∈ RK, V ∈ RD we can re-scale the norms so that ∣U∣ = ∣V∣ while
satisfying the constraints of U Θ V = Wb in the definition of RK,1(W). Further, such a scaling would be optimal for
minimizing the '2 norm of weights based on AM-GM inequality that ∣U ∣∣2 + ∣∣V∣2 ≥ 2∣U ∣∣ ∙ ∣Vb ∣. Thus, in the rest
of the proof, we consider the following equivalent formulation of RK,1(W) as:
......^.. ^ ^ _
RK i(w) = min 2∣∣U∣∣ ∙ IlVk	s.t., U Θ V = W	(15)
U∈RK,V∈RD
5 T	,1 .	TTFT.1	∙ . ∙ .1	1	,♦	1	∖ /	∙. 1 11 .1 -CtV ∏
We see that for any U, V satisfying the constraint in the above equation, we have: ∀d∈supp(wb), it holds that V[d]
(where supp(Wb) = {d ∈ [D] : |Wb | 6= 0}). Moreover, at an optimal solution, it is easy to see that V[d] = 0
Wb [d] = 0.
Wb [d]
-^' . ~r
U [d]
^⇒
Let U = [c0, c1]. This means that the objective can be written as
2∣U∣∣Vb ∣ = 2∣U∣t
D-1	,----------------- --------------
X |V[d]∣2 = 2∣U∣ / X	|V[d]∣2 = 2∕c2 + c2
d=0	d∈supp(wb)	∖
X	W [d]∣2
d∈supp(W) |U[d]|2
(16)
22
Under review as a conference paper at ICLR 2022
We write the second term in terms of the signal domain representation of c0 and c1. We see that the Fourier transform
of U is given by U[d] = √= (c0 + c1e-2nid/D) = (c0 + c1 cos(-2πid∕D)) + i (c1 sin(-2πid∕D)). We thus have
that:
|U[d] ∣2 = ɪ [(co + ci cos(-2πid∕D))2 + (ci sin(-2πid∕D))2]
ɪ (c0 + ci + 2c0cι cos(-2πik∕D)).
(17)
D (co + ci)
1+
2cocι
co + ci
cos(-2πid∕D) I .
Let α = 2c0c2. Plugging eq. (17) back into the objective in eq. (16) and using Cos(Z) = cos(-z), We get that for any
c0+c1
U, V satisfying the constraints in the computation of R2,i(w), the objective is in the desired formulation:
2∣∣UkkV k =2√Dt
X	∣W[d]∣2
1 + a cos(2πid∕D)
d∈supp(wb )
(18)
Let us noW consider the domain of α, Which is the only unknoWn in the above equation. Observe that for any co, ci,
2：+；2 ∈ [-1,1]. Moreover, any α ∈ [-1,1] be realized by some values of co and ci. Thus all α ∈ [-1,1] are valid.
Here We further remark that the denominator in eq. (18) is zero if and only if U[d] = 0 for any d ∈ D. HoWever, this
can only happen if wb [d] = 0 as otherWise the constraints U V = w is not satisfied for any V. We can thus, minimize
the RHS of eq. (18) over α ∈ [-1, 1] to obtain R2,i(w).
If We include the terms corresponding to d ∈∕ supp(wb) in the summation in eq. (18), there is a technical condition
than can lead to 0∕0 terms in end cases of α = 1 (When wb [0] = 0) and α = -1 (When wb [D∕2] = 0). To avoid this
technicality, We consider the infimum over α ∈ (-1, 1) rather than minimum over α ∈ [-1, 1]. This is equivalent
because the expression is continuous on the set of α on which it is well-defined. This completes the proof.	□
C Proof of Theorem 3: SDP tightness
Theorem 3. [SDP tightness] For any K ≤ D, any C, and any w ∈ RD, it holds that RK,C(w) = RSKDP(w).
The high-level idea of the proof of Theorem 3 is to take an optimal solution Z to eq. (9), and construct a rank 1 solution
that obtains the same objective and satisfies the constraints. We reiterate the SDP formulation for easy reference:
RSKDP(w) = mZ<ino hZ, Ii
s.t., ∀d∈[D], hZ, Ardeali =2Re(wb[d])	(SDP)
∀d∈[D],hZ,Aidmgi=2Im(wb[d]).
We start with the KKT conditions for (9). The D constraints involving Re(w) correspond to a dual vector λreal ∈ RD;
the D constraints involving Im(w) correspond to a dual vector λimg ∈ RD. To simplify these conditions, we take
λ ∈ CD to be λreal + i ∙ λimg (when λ is a dual-optimal solution, λ is also a Fourier transform of a real vector). The
dual variable for the PSD constraint corresponds to a matrix Γ < 0. In this notation, the KKT conditions are primal
feasibility, along with the following constraints:
Γ = I - " £k	FK λf #
FΛFκ	0D
Γ<0
ZΓ = 0.
Now, to simplify these conditions, suppose that Z is rank L, in which case we can express it as Z =
VU U> V> , where U ∈ RK ×L and V ∈ RD×L . Using that VU is full rank and through some
23
Under review as a conference paper at ICLR 2022
(KKT 1)
(KKT 2)
l=0
0K
FΛFK
simple algebraic manipulations, we obtain the following formulation of the KKT conditions:
L
X Ub [:, l]	Vb [:, l] = wb
FK λf # 4 Id+k
0D
V = ΛU	(KKT 3)
^	— => . ^?Ξ
U = FK FK ΛV.	(KKT 4)
The KKT conditions give useful properties of the solution Z. For 0 ≤ l ≤ L - 1, we let ul = U[:, l] and vl = V[:, l].
From (KKT 3), We see that	_
∀l∈[L]vl = λ Θ bl.
Combining the above With (KKT 1), We obtain:
L-1
W = X λ Θ Ul Θ Ui.
l=0
We noW make the folloWing assertion.
Claim. We claim that in order to prove Theorem 3, it suffices to find a vector U ∈ RK such that:
L-1
u Θ u = X ul Θ ul.	(CONV-REDUCTION)
l=0
The most technical part of the proof is to shoW (CONV-REDUCTION). We Will shortly prove the existence of u in
(CONV-REDUCTION) by proving the intermediate Lemma 4. But before that We Will first justify our above claim.
Proof of claim. Assume (CONV-REDUCTION) holds and let u be a solution that satisfies eq. (CONV-REDUCTION),
and let v = F-1(λ Θ u) such that We have U = λ Θ u. We can take Z* to be
v> ]
u [ u>
v
We can see that (u, v) satisfies the folloWing:
L-1
u Θ U = λ Θ u Θ U = ^X λ Θ Ul Θ Ul = W.
l=0
(19)
Thus, Z* satisfies the feasibility condition for RSDP(W). Moreover, we show that the solution also achieves the optimum
objective value for RK,C (W) as folloWs:
hZ*, Ii = kuk22 + kvk22 = kuuk22 + kvuk22
D-1	D-1
= X |uud|2 + X |vud|2
d=0	d=0
D-1 L-1	D-1 L-1
=) XX l(ul)d∣2 + XX "(Ul"2	QO)
L-1 D-1	L-1 D-1
(=b) X X |(uul)d|2 + X X |(vul)d|2
l=0 d=0	l=0 d=0
= kUk2 + kVk2
=hz,D = RK)P(W)
where (a) follows from (CONV-REDUCTION) and using U = λ Θ U, which follows by definition of v, and (b) follows
from (KKT 3) that Ul = λ Θ Ul.
Thus, we have show that (CONV-REDUCTION) implies that there exists a rank 1 solution Z* that achieves the same
objective value as Z and satisfies the constraints, and hence is minimizer of the SDP as desired.	口
24
Under review as a conference paper at ICLR 2022
Proof of (CONV-REDUCTION). It is more convenient to write (CONV-REDUCTION) in signal space. Taking
inverse Fourier transforms of (CONV-REDUCTION), we need to show that given ul = F>K ubl ∈ RK there exists
u ∈ RK such that the following holds:
L-1
u?u =	ul ?ul,	(21)
l=0
where the convolutions are taken in D dimensional space, i.e., {ul}l are padded with D - K zeros so that ul ?ul ∈ RD.
We can now see that eq. 21 (and hence (CONV-REDUCTION)) indeed holds by recursively applying Lemma 4, which
was stated earlier in the main text and is reiterated below.
Lemma 4. For any 1 ≤ K ≤ D, and for any vectors a, b ∈ RK, there exists a vector c ∈ RK such that a?a+b?b =
c ? c, where convolutions are w.r.t. dimension D.
The proof of Lemma 4 is provided in Section C.1.
We have thus far shown that Theorem 3 follows from Lemma 4: we first established that it suffices to find u ∈ RK
satisfying (21) (or equivalently (CONV-REDUCTION)), which in turn holds from recursively applying Lemma 4. Now,
it suffices to prove Lemma 4; we prove this lemma in the following subsections.
C.1 Proof of Lemma 4
For K = D, Lemma 4 follows easily from the Fourier space representation, since the Fourier space representation of
the vector c can be explicitly constructed as the square root of the Fourier representation of a? a + b ? b. However,
this construction does not generalize to kernel sizes K < D, and Lemma 4 is thus non-trivial in general. In fact, the
vector c does not even appear to have a clean closed-form characterization for general K . To sidestep this issue, we use
a proof technique that enables us to implicitly construct the vector c. We believe that this proof technique could be of
independent interest.
C.1.1 REDUCING LEMMA 4 TO D = 2K - 1 CASE
The first step in the proof of Lemma 4 is to show that Lemma 4 for general K, D follows from the special case where
D = 2K - 1.
Lemma 12. For any K ≥ 1, for D = 2K - 1, and for any vectors a, b ∈ RK, there exists a vector c ∈ RK such that
a ? a + b ? b = c ? c, where convolutions are taken with respect to the base dimension D.
From Lemma 12, it is not difficult to conclude Lemma 4.
Proof of Lemma 4 from Lemma 12. By Lemma 12, we know that Lemma 4 holds when D = 2K - 1. We now show
that this implies the statement for a general value of D. Regardless ofD, we can take the coordinates in [-K+ 1, K - 1]
of a ? a + b ? b and apply Lemma 12 to obtain a vector c ∈ RK . We now show that c satisfies a ? a + b ? b = c ? c,
regardless of the value of base dimension D.
Case 1:	D ≥ 2K - 1. First, notice that for D0 6∈ [-K + 1, K - 1] mod D, we know that
(a?a+b?b)d = 0 = (c ? c)d,
because the vectors are K-dimensional and thus their convolutions will only have at most 2K - 1 nonzero entries. For
d ∈ [-K + 1, K - 1] mod D, we see that by Lemma 12:
(a?a+ b?b)d = (c ? c)d.
for d ∈ [-K + 1, K - 1] mod D.
Case 2:	D < 2K - 1 . By definition, we also know that K ≤ D. We see that (c ? c)d (where the convolution is
taken with base dimension D) is equal to the sum over all d0 such that d and d0 are equal mod D of (c ? c)d0 (where the
convolution is taken with base dimension 2K - 1). Similarly, (a? a + b ? b)d (where the convolution is taken with
base dimension D) is equal to the sum over all d0 such that d and d0 are equal mod D of (a? a + b ? b)d0 (where the
convolution is taken with base dimension 2K - 1). By Lemma 12, we know that (a? a + b ? b)d = (c ? c)d0 for all d0
where the convolution is taken with base dimension 2K - 1. This means that (c ? c)d = (a ? a + b ? b)d where the
convolution is taken with base dimension D.	□
The remainder of the section of devoting to proving Lemma 12. This statement trivially holds with c as the zero vector
if a ? a + b ? b = 0, so for the remainder of the proof, we assume that a ? a + b ? b is nonzero.
25
Under review as a conference paper at ICLR 2022
C.1.2 Introducing the polynomial representation
The key idea for the proof of Lemma 12 is to use the polynomial formulation of convolutions.9 We then use factorization
of the polynomials to implicitly construct c. We use the following notation. Let Pk ⊆ R[x] denote the set of
degree ≤ K - 1 polynomials with real coefficients. For a vector z ∈ RK , we define the polynomial representation
pz(x) ∈ PK-1 to be the polynomial z0 + z1x + . . . + zK-1xK-1. Using the polynomial representations, convolutions
can be expressed as polynomial multiplication:
Fact 1. Let a ∈ RK. The D = 2K - 1 dimensional convolution a ? a has polynomial representation pa?a(x) that is
equivalent to the polynomial xK-1Pa(x) ∙ pa(1∕x) ∈ P2κ-2 up to permuting the coefficients appropriately.
The polynomial representation enables us to construct a vector c in terms of the roots of the relevant polynomials. We
now reformulate Lemma 12 using the polynomial representation. Recall that we wish to show that there exists a vector
c ∈ RK such that
c?c = a?a + b?b.
Using the polynomial representation, we can equivalently write this requirement as pc?c(x) = pa?a(x) + pb?b (x).
Now, applying Fact 1, we see that the polynomial representation of the lemma statement is the following:
XKTPc(X) ∙ Pc(1∕x) = XKTPa(X)Pa(1/x) + XKTPb(X)Pb(1/x).
To simplify notation, we denote the right-hand-side of the previous equation by Q(x):
Q(X) := XK-1Pa(X)Pa(1/X) + XK-1Pb(X)Pb(1/X) ∈ P2K-2 .
In this notation, our goal is to show the following:
Q(X) = XKTPc(X) ∙ Pc(1∕X).
Since there is a 1-to-1 correspondence between polynomials in PK-1 and vectors in RK, it suffices to show that there
exists a polynomial P ∈ PK-1 such that:
Q(X)= XK-IP(X) ∙p(1∕x).	(22)
The remainder of the proof boils down to constructing P ∈ PK-1 such that eq. (22) is satisfied.
C.1.3 Proving the polynomial representation version of the lemma statement
The first property of Q(X) that we leverage is that it is a palindromic polynomial (i.e. a polynomial where the
coefficients form a palindrome) with real coefficients. To see that Q(X) is a palindromic polynomial, notice that
(a?a)d = (a ? a)D-d and (b?b)d = (b ? b)D-d for all 0 ≤ d ≤ D - 1, and so (a?a+b?b)d = (a?a+b?b)D-d
for all 0 ≤ d ≤ D - 1. Now, we can use Fact 1 to conclude that the Xd coefficient of Q(X) is the (d - K + 1) mod D
coefficient of a? a + b ? b. Thus, the Xd coefficient of Q(X) is equal to the X2K-2-d coefficient of Q(X), as desired.
At first glance, it would appear that eq. 22 follows immediately from standard properties of palindromic polynomials
with real coefficients whose roots are known to come in reciprocal pairs.10 However, we cannot obtain eq. (22) from
the palindromic property alone. To see this, consider the following example:
Example 1. Consider the palindromic polynomial with real coefficients X2 + 1 = iX(X - i)(1/X - i). This polynomial
is not expressible as XP0(X)P0(1/X) for any real polynomial P0.
The proof of eq. (22) thus must leverage further structure of Q(X), which we will ultimately extract through examining
the roots of Q(X). Using that C is algebraically closed, we can factor Q(X) into a polynomial cQ Qi(X - αi) with
exactly 2K - 2 roots where the ai need not be distinct. To show eq. (22), it suffices to show that XKTP(X) ∙ p(1∕x)
has roots (with multiplicities) given by the multi-set SQ = {αi } and has leading coefficient c. Drawing upon this
formulation, we will construct P implicitly by the multi-set Sp of its roots (with multiplicities) and its leading coefficient
cp 6= 0: that is, so that P = cp α∈S (X - α).
9E.g., see https://en.wikipedia.org/wiki/Convolution
10This is a standard fact: e.g., see https://en.wikipedia.org/wiki/Reciprocal_polynomial.
26
Under review as a conference paper at ICLR 2022
C.1.4 PROPERTY OF THE ROOTS OF Q(x)
Before we construct Sp and cp, it is helpful to establish properties of the multi-set of roots SQ .
1.	(P1) For every root α with multiplicity m, a is a root and has multiplicity m.
2.	(P2) If ɑ = 0 is a root with multiplicity m, then 1∕a is a root with multiplicity m.
3.	(P3) If a such that ∣α∣ = 1 is a root, then a has even multiplicity.
The first two properties (P1) and (P2) follow from the fact that Q(x) is a palindromic polynomial with real coefficients.
In particular, we see that (P1) follows from the fact that Q(x) has real coefficients so that the roots come in conjugate
pairs, and (P2) follows from standard properties of palindromic polynomials.11
Establishing the critical property of Q(x). The last property, (P3), uses deeper aspects of the structure of Q(x).
In particular, it uses that Q(x) is the sum of polynomials of the form xK-1p0(x)p0(1/x) where p0 ∈ PK-1 has real
coefficients, rather than just an arbitrary palindromic polynomial. To see this, let’s return to Example 1 and observe that
x2 + 1 does not satisfy (P3) since its roots are i and -i. Hence, we use further structure of Q(x) and we show:
Lemma 13. Consider vectors a, b ∈ RK, and letpa,pb ∈ PK-1 be their polynomial representation. If α ∈ C such
that ∣α∣ = 1 is a root of Pa(x)(xKTpa(I/x)) + pb(x)(xKTpb(I/x)) ,then a has even multiplicity.
Proof. Suppose that α is a root of Q(X) and ∣α∣ = 1. We see that
0 = Q(α)
=pa(α)(αKTpa(I/α))+ pb(α)(αKTpb(I/α))
=pa(α)(αKτpa(α)) + pb(α)(αKT pb(α))
=αK-1 (pa(α)pa(α)) + pb(α)pb (α)))
=αK-1 (|pa(α)∣2 + ∣pb(α)∣2).
Since α = 0, this means that ∣pa(α)∣2 + ∣pb(α)∣2 = 0. Thus, pa(α) = 0 andpb(α) = 0. Now, it suffices to show that
α is a root with even multiplicity inpa(x)(xK-1pa(1/x)) and inpb(x)(xK-1pb(1/x)).
We show that α has even multiplicity in pa(x)(xK-1pa(1/x)) (an analogous argument shows this for
pa(x)(xK-1pa(1/x))). Suppose that α has multiplicity m in pa(x). Since pa(x) has real coefficients, we know
that a is a root of pa(x) with multiplicity m. We also know_that l/ɑ = ɑ isaroot with multiplicity m of xk-1pa(1/x)).
Since XK-Ipa(l/x)) has real coefficients, we know that ɑ = α is aroot with multiplicity m of xk-1 pa(1/x)). This
means that α has multiplicity 2m in pa (X)(XK-1pa(1/x)) as desired.	□
We note that (P3) follows immediately from Lemma 13.
C.1.5 CONSTRUCTING THE ROOTS OFp
We construct the multi-set of roots Sp . To do this, we begin by constructing the nonzero roots in Sp , and then we add in
the zero roots with the appropriate multiplicities at the end.
Constructing the nonzero roots. The high-level intuition for nonzero roots is that we ultimately need the roots of
XK-1p(X)p(1/X) to exactly match the roots in SQ. Intuitively, since the roots of p(1/X) are the reciprocals of the roots
of p(X) (with multiplicity preserved), and since polynomials with real coefficients have roots in conjugate pairs, we
wish to divide the real roots into disjoint pairs (α, 1/a) (so that p(x) has root a and p(1/x) has root 1/a) and the
complex roots into disjoint quadruples (α,α, 1/a, 1/a) (so that p(x) has roots a and α andp(1/x) has roots 1/a and
1/a).
Let’s formalize this intuition by constructing an undirected graph G where the vertices are the nonzero roots in SQ
(a root with multiplicity m corresponds to m separate vertices). The edges are defined as follows. When a 6= 1, we
connect a vertex corresponding to a with some vertex corresponding to 1/a, so that the graph forms a bipartite graph (it
is possible to do this because of (P2)). By (P3), we can handle a = 1, and form non-self-loop edges of the form (1, 1).
Let Greal be the subgraph of G consisting of vertices corresponding to real roots, and let Gcomplex be the subgraph of G
11This is a standard fact: e.g., see https://en.wikipedia.org/wiki/Reciprocal_polynomial.
27
Under review as a conference paper at ICLR 2022
consisting of vertices corresponding to roots with nonzero imaginary part. It is easy to see that G = Greal ∪ Gcomplex
and these graphs are disjoint.
Let us now use these graphs to construct the set of nonzero roots in Sp , which will contain half of the vertices in G. For
Greal, we add one vertex from each edge in Greal to Sp0 . For Gcomplex, we can pair up the edges in the graph Gcomplex as
follows. When ∣α∣ = 1, We can pair UP the edges (α, 1∕α) and (α, 1∕a) so that the pairs are disjoint and no edge is
paired with itself (it is possible to do this because of (P1), coupled with the fact that 1∕α = α). When |a| = 1, we
use the fact that α has even multiplicity (see (P3)). Thus, we can pair up each edge (α, 1∕α) with an edge of the form
(α, 1∕α) = (α, 1 /α), so that pairs continue to be disjoint and no edge is paired with itself. Now, for each pair (α, 1∕ɑ)
and (α, 1 ∕α), we add α to Sp and we add a to Sp.
Adding the zero roots. To construct Sp, all that remains is to determine the multiplicity of the zero roots. Let m be
the multiplicity of 0 in SQ. We then simply add m copies of 0 to Sp.
C.1.6 PROOF THAT Sp IS THE CORRECT MULTI- SET OF ROOTS
We first prove that Sp corresponds to the roots of a degree ≤ K - 1 polynomial with real coefficients; we then show
that the multiset of roots of xK-1p(x)p(1∕x) is equal to SQ.
Proof that Sp corresponds to the roots of a polynomial in PK-1. For Sp to be a valid multi-set of roots for a
polynomial p ∈ PK-1, we need to ensure that Sp consists of at most K - 1 elements and that the roots come in
conjugate pairs.
To see that Sp consists of at most K - 1 elements, we do the following root counting argument. Notice that if
(a ? a + b ? b)d = 0, this means that (a? a + b ? b)D-d = 0. In terms of the polynomial representation, this means
that if Q(x) has a zero root with multiplicity m, then the degree of Q(x) is 2K - 2 - m (since Q(x) 6= 0). Thus, we
see that Q(x) has 2K - 2 - m roots not including multiplicity, and 2K - 2 - 2m nonzero roots. By the construction
of S, we see thatpmonic(x) has K - 1 - m nonzero roots including multiplicities, and m zero roots, which amounts to
K - 1 roots in total with multiplicity, as desired.
The fact that the roots come in conjugate pairs follows from the construction of the graphs above. Recall that the real
roots of SQ were partitioned into disjoint pairs (α, 1∕α) (so that p(x) has root α) and the complex roots of SQ were
partitioned into disjoint quadruples (α, α, 1∕α, 1 ∕α) (so that p(x) has roots α and a). This ensures that the roots comes
up in conjugate pairs as desired.
Proof that the multi-set of roots of xK-1p(x)p(1∕x) equals the multi-set SQ. Let pmonic(x) be the monic polyno-
mial given by the roots of Sp , and consider
Q1 (x) := pmonic (x)x	pmonic (1∕x).
It suffices to show that the multi-set of roots of Q1(x) with multiplicities is equal to the multi-set SQ.
For the nonzero roots, we use the construction of Sp . Recall that the real roots of SQ were partitioned into disjoint pairs
(α, 1∕α) (so that p(x) has root α) and the complex roots of SQ were partitioned into disjoint quadruples (α,α, 1∕α, 1∕ɑ)
(so that p(χ) has roots α and a). This, coupled with the fact that the nonzero roots of p(1∕χ) are the inverses of the
nonzero roots of p(x), means that the union of the multi-set of nonzero roots in p(x) with the multi-set of nonzero
roots of p(1∕x) is equal to the multi-set of nonzero roots of Q(x). This means that the multi-set of roots of Q1(x) with
multiplicities is equal to the multi-set SQ .
For the zero roots, we simply need to show that xK-1pmonic(1∕x) has no roots that are 0. Using that the constant term
of xK-1pmonic(1∕x) is equal to the coefficient of xK-1 in pmonic(x), it suffices to show that the coefficient of xK-1
in pmonic(x) is nonzero. To see this, we use the fact that pmonic(x) has K - 1 roots in total with multiplicity by the
argument from the previous paragraph, and so the degree of pmonic(x) must be K - 1. This means that the roots of
Q1(x) with multiplicities are equal to the roots of Q(x).
C.1.7 HANDLING THE LEADING COEFFICIENT OFp
Now, we need to just construct the leading coefficient ofp. As above, let pmonic(x) = Qα∈S (x - α) be the monic
polynomial given by the roots of Sp, and consider Q1(x) = pmonic(x)xK-1pmonic(1∕x). Since Q1(x) and Q(x) have
the same set of roots with multiplicities, we know that Qi(x) = Y ∙ Q(X) for some γ = 0. Let's take Cp = √γ. In order
forp = cp Qα∈S (x - α) to be a polynomial with real coefficients, we need cp to be real.
28
Under review as a conference paper at ICLR 2022
To show that cp is real, it suffices to show that γ is positive. This can be seen as follows. Let cmonic ∈ RD be the vector
with polynomial representation pmonic (x). Now, by Fact 1, we know that the polynomial representation pcmonic?cmonic takes
the form XDpmonic(x)pmonic(1∕x) = χD-K+1Qι(χ), where exponents are taken modulo D. Since Qi(x) = Y ∙ Q(χ),
and since the polynomial representation of (a? a + b ? b) takes the form xD-K+1Q(x) where the exponents are taken
modulo D, we can conclude that cmonic ? cmonic = γ(a? a + b ? b), and thus that the Fourier representations of these
vectors are also equal. It is not difficult to verify that the Fourier representation of cmonic ? cmonic is |bcmonic |2 , and so the
entries of the Fourier representation of consist of all nonnegative real numbers. Similarly, Fourier representation of
a ? a + b ? b is |ba|2 + |bb|2 which consists of entries that are all nonnegative real numbers. This means that γ ≥ 0, as
desired.
C.1.8 Concluding Lemma 12
We have shown that p = cp Qα∈S (x - α) is a polynomial of degree at most K - 1 with real coefficients. Moreover,
we have shown that Q(X) has the same multi-set of roots and the same leading coefficient as XKTp(X) ∙ p(1∕χ). Thus,
we can conclude that eq. (22) holds, and thus we have proven the polynomial representation of Lemma 12. This
concludes the proof of Lemma 12.
C.2 Discussion of the proof technique
We conclude with a discussion of the analysis and highlight the main parts of the proof. At the beginning of the
section, we used the KKT conditions to show that it suffices to prove CONV-REDUCTION, an additive property about
convolutions of for kernel size. We then showed that it suffices to prove a version of this statement for the sum of two
such convolutions, i.e. Lemma 4. We believe that this property could be of independent interest.
The bulk of the proof boiled down to proving Lemma 4 in the special case of D = 2K - 1, i.e. Lemma 12. Proving
Lemma 12 was the core technical contribution in this section. Since c does not necessarily always a clean closed-form
solution as a function of a and b, we needed to construct c implicitly. The polynomial representation of convolutions
enabled us to implicitly construct c via its roots. To construct c and ensure that the corresponding polynomial
representation p had real coefficients, we needed to leverage the structure of the polynomial representation Q(X) of
a?a + b ? b beyond its palindromic structure. (This additional property was proven in Lemma 13.) With this structure,
we were able to factor Q(X) and partition its roots in order to construct the roots ofp.
D Remaining proofs of results in Section 4
D.1 Proof of Corollary 5
Corollary 5. For K ≤ D and any C, RK,C (w) is a norm.
Corollary 5 follows from Theorem 3.
Proof of Corollary 5. It suffices to establish the scalar multiplication property, the triangle inequality, and point
separation.
Scalar multiplication. Let γ ∈ R. By definition, we see that
RK,C (γ w) = min kUk2 + kVk2	s.t.,	diag(UbVb>) = γwb.
Let,s do a change of variables U《-‰U, V J -¼==V to see that
V|Y|	Sign(Y)√ |Y|
Rk,c(YW) = ∣Y∣ min ∣∣Uk2 + ∣∣Vk2	s.t.,	diag(UV>) = W = ∣γ∣Rκ,c(W)
as desired.
Triangle inequality. It suffices to show that if W = W1 + W2, then RK,C (W) ≤ RK,C(W1) + RK,C(W2). By
Theorem 3, it suffices to show that RSKDP(W) ≤ RK,C (W1) + RK,C (W2). Suppose that U1 and V1 are optimal
29
Under review as a conference paper at ICLR 2022
solutions to RK,C(w1), and suppose that U2 and V2 are optimal solutions to RK,C(w2). If we define:
r7	U1U1>
Z1 =	V1U1>
Q 一	U2U2>
Z2 =	V2U2>
Z = Z1 + Z2,
U1V1>
V1V1>
U2V2>
V2V2>
then we see that the SDP objective hZ, Ii = hZ1, Ii + hZ2, Ii = RK,C (w1) + RK,C(w2). Moreover, we see that Z is
a feasible solution to (9) for w. This means that RSkDP (w) ≤ RK,C (w1) + RK,C (w2) as desired.
Point separation. Notice that RK,C (w) = 0, then there exist U and V such that kUk2 + kVk2 = 0. This means that
U = 0 and V = 0, which means that W = 0 as desired. Moreover, if W = 0, then it's clear that Rk,c(W) = 0.	□
D.2 The dual formulation of SDP
In order to analyze the SDP formulation, we consider the dual. We use the formulation of the dual variable in C as
λ ∈ CD . In this form, the dual can be expressed as:
max
λ∈CD
s.t.
Re (hλ, Wb i)
_ _0K	FKΛF ] 4 I
FΛFk	0d	4
To simplify the objective Re (hλ, Wb ), notice that the phases of λ can be set to align with Wb without affecting the
constraint. Thus, we can set the objective to be ∣(λ, W)|. For convenience, we also expand out the conic constraint
in vector form, and this reformulation incurs a factor of 2 on the objective. We thus obtain the following equivalent
formulation of the dual:
max
λ∈CD
s.t.
D-1
2 X ∣λ[d]∣∙∣w[d]∣
d=0
D-1
∀x ∈ CK X ∣X[d]∣2 ∙∣λ[d]∣2 ≤ 1.
d=0
(23)
We now show that strong duality holds for this SDP.
Proposition 14. The SDP in (9) satisfies strong duality.
Proof. To show strong duality, it suffices to show Slater’s condition. We just need to find a solution λ ∈ Cd where the
inequality constraint is not tight. That is, we need to find λ such that ∀χ∈cκ, PD-o1 |b[d] ∣2 ∙ ∣λ[d]∣2 < 1. Let's take
λ = [1/2,0,0,..., 0]. Notice that PD=o1 ∣b[d]∣2 ∙ ∣λ[d]∣2 = 0.5∣b[0]∣2 ≤ 0.5 < 1, as desired.	□
With the dual, along the fact that RSKDP(W) is a norm, we are equipped to prove general upper and lower bounds on the
induced regularizer as well as sharper bounds for patterned vectors.
D.3 Proof of Lemma 15
Lemma 15. For any K ≤ D, any C, and any W ∈ RD:
2ʌ/lkWk2 ≤Rk,c(W) ≤ 2√D∣∣W∣∣2
2kWkι ≤Rk,c(W) ≤ 2q[D] kWkι.
Remark. For the lower bounds, 2kW∣k is tightfor W = [1,0,..., 0] and 2ypDD∣∣Wk is tightfor W = [1,1,..., 1]. For
the upper bounds, 2 J[K] ∣W % is tight when K | D for patterned vectors (see Lemma 16), and 2√D∣W |卜 is tight
for[1,0,...,0].
30
Under review as a conference paper at ICLR 2022
Lemma 15 demonstrates that when K is a small constant, Rk,c(w) is multiplicatively close to Rι,c(W) = 2√D∣∣W|卜.
On the other hand, once K is comparable toD, RK,C (w) is within a constant factor of RD,C (w) = 2kwb k1.
ProofofLemma 15. The bounds of 2∣∣W∣∣ι and 2√D∣∣W|卜 follow in a straightforward way from Lemma 1—2 coupled
with Theorem 3.
The lower bound of 2∣W∣ι follows as: 2∣W∣ι (=) Rd,i(w) = Rd,c(w) ≤ Rk,c(w), where (a) follows from
Lemma 2, (b) follows from Theorem 3, and (C) follows from Remark 1. Similarly, the upper bound of 2√D∣W|卜
follows as: 2√D∣W|卜 =) R1,1(w) = Rι,c(w) ≥ Rk,c(w), where (a) follows from Lemma 2 and (b) follows from
Theorem 3, and (c) follows from Remark 1
The bulk of the proof lies in showing the lower bound of 2 ^PK ∣ W12, and an upper bound of 2 J[K ] ∣ W k「We first
prove the lower bound, and then we prove the upper bound.
Proof of the lower bound 2y/KkW∣2∙ We prove that RK,c(w) ≥ 2ypD|回|卜.It suffices to consider a dual
feasible vector to eq. (23) that achieves an objective 2 JK ∣∣w∣∣2∙ We consider
λ
WD
∣W∣ K
We see that the objective is equal to
D-1	ID
2 £ ∣λ[d]∣∣w[d]∣ = 2dKkw∣∣2,
d=0
as desired. It thus suffices to show that λ satisfies PD-1 ∣X[d]∣2 ∙ ∣λ[d]∣2 ≤ 1 for all X ∈ Cd SUChthatkXk2 ≤ 1. Using
Holder’s inequality, we can bound:
D-1
E |x[d]|2 ∙ lλ[d]l2 ≤ (0≤m≤aχ-1|bb[d]|2
d=0
0≤md≤aDx-1 |xb[d]|2 ∣λ∣22.
We can bound the first term by:
区[d]1 = √D
K-1
X x[k]e-2∏ikd3
k=0
≤√D Xwk- = % ≤√D∙
Moreover, we see
that∣λk = ʌ/ɪ.This means
that (max0≤d≤D-1 ∣x[d]∣2) ∣∣λ∣∣2 ≤ 1, as desired.
Proof of the upper bound 2
不 kWk1.
We prove that RK,c(w) ≤ 2卉Ky∣∣Wbk1
Our main ingredient is
Corollary 5 which tells us that RK,C (w) is a norm. We define T = dD/Ke vectors w0, . . . , wT-1 ∈ RD where
w = PtT=-01 wt, and apply Corollary 5 to obtain that:
T-1
RK,C (w) ≤ X RK,C (wt).
t=0
These vectors are chosen that each RK,C (wt) takes on a simple closed-form solution.
In order to construct the vectors wt, we consider q = √wb, defined so that W = q，? q and ∣∣qk2 = ∣∣q∣∣2 = kWk1. We
define vectors r0, . . . , rT-1 ∈ RD such that PtT=-01 rt = q as follows. Roughly speaking these vectors consist of the
disjoint subsets of the coordinates of q corresponding to the tth block of size K. More formally, for 0 ≤ t ≤ T - 1, let
rt be defined so that r∕l] = q[l] for l ∈ [t ∙ K, min((t + 1) ∙ K 一 1, D 一 1)], and r/l] = 0 otherwise. Let Wt = r，? q
for 0 ≤ t ≤ T - 1. It is evident that Pt rt = qt and hence Pt wt = w.
We now show that RK,C (Wt) ≤ 2krtkkqk. We show this by explicitly constructing solutions to (5), taking advantage
of the fact that rt is effectively a vector in RK that is zero-padded appropriately. This K-dimensional vector r0t ∈ RK
31
Under review as a conference paper at ICLR 2022
is given by rt [k] = r∕(t ∙ K + k) mod D] for 0 ≤ k ≤ K - 1. Now, We wish to write Wt as a convolution (r；＞ ? q=,
for some suitably chosen vector q；. Since Wt = r； ? q and r； is merely a circular shifted version of r；, we can take
qt ∈ RD to be q with the coordinates shifted appropriately. Now, we can rescale r0t and qt so that they have equal `2
norms, and obtain the following vectors: (Jkqkt) r； and (Jj^) q；. These vectors are a feasible solution to eq. (5)
for RK,C(W；) and achieve an objective of 2kr0；kkq；k = 2kr；kkqk, as desired.
Using that RK,C (W；) ≤ 2kr；kkqk for 0 ≤ t ≤ T - 1, we obtain the following bound on RK,C (W):
T-1
RK,C(W) ≤ 2kqk X kr；k.
；=0
Now, notice that since the supports of r； for 0 ≤ t ≤ T - 1 are disjoint, P；T=-01 kr； k2 = kqk2 . Applying AM-GM, this
means that
T-1	T-1
(X kr；k)2 ≤ T(X kr；k2) = Tkqk2.
；=0	；=0
Thus, we have that	_____
Rk,c(W) ≤ 2√Tkqk2 = 2√TkWkι = 2ʌ/[ljkWkι.
□
D.4 RK,C (W) FOR PATTERNED VECTORS
Aside from general bounds on RK,C, the SDP formulation can also be used to analyze the behavior of the induced
regularizer of special classes of vectors. One interesting case is of patterned vectors described as follows: Consider
vectors of the form W(p) = [p, p, . . . , p] ∈ RD consisting of repetitions of a P dimensional pattern p ∈ RP. A useful
property of linear predictors of this form is that they incorporate invariance to periodic translations.
We show an interesting relation between the representation cost RK,C (W(p)) of realizing patterned vectors in RD and
the analogous cost (denoted as R(KP,)C (p)) of realizing p as a linear predictor in RP using a network with the same
values of K and C .
Lemma 16. Consider vectors W(p) = [p, p, . . . , p] ∈ RD specified by p ∈ RP s.t., P divides D.
(a)	For any K ≤ P, it holds that ∀C:
Rk,c(W(P)) = P ∙R(Kι(p).
(b)	For P ≤ K ≤ D if K = P ∙ T for integer T, then ∀C:
rk,c (W(P)) = 2 -DD∑
TP
IIPll1 = 2《K kW kι.
We see that the induced regularizer of repeated patterned vectors is closely related to that of the pattern itself. In
particular, for K ≤ P, we have Rk,c(w(p)) a RKP1(p).
We first prove an upper bound on RK,C (W), and then we prove a matching lower bound. In these proofs, let W = W(P).
Both of these proofs use the standard fact that W[(D/P) ∙ p] = Dpp[p] for 0 ≤ P ≤ P — 1, and W[d] = 0 if
(D/P) - d.
Lemma 17 (Upper bound). Consider vectors W(P) = [P, P, . . . , P] ∈ RD specified by P ∈ RP (such that D is a
multiple of P).
(a)	For any K ≤ P, it holds that ∀C:
Rk,c(w(p)) ≤ P ∙RKP1(P).
(b)	For P ≤ K ≤ D if K = P ∙ T for integer T, then ∀C:
RK,C(W(P)) ≤ 2-=-kpkι.
TP
32
Under review as a conference paper at ICLR 2022
Proof of Lemma 17. It suffices to show an upper bound for the case of a single output channel. We explicitly construct
a pair (u, v) where kuk2 + kvk2 achieves the desired objective.
Case 1:	K ≤ P. We construct (u, v) using an optimal solution up and vp to eq. (5) for R(KP,1) (p). We let u be
defined so that u[k] = DDUUP [k] for 0 ≤ k ≤ K 一 1. We let V be defined tobe V = [vp,..., VP]. Notice that:
kuk2 + kvk2 = P kUpk2 + P kVpk2 = P RK) (p),
as desired. To see that Ul ? V = w, it suffices to show U Θ b = w. Notice that v[(D∕P) ∙ p] = √√DC[p] for
0 ≤ P ≤ P 一 1, and V[d] = 0 if (D/P) - d. This means that (U Θ b)[d] = 0 = W[d] if (D/P) - d as desired. Thus it
suffices to handle (U Θ b)[(D∕P) ∙ p]. Noticethat b[(D∕P )∙p] = √√P bp[p], and U[(D∕P )∙ p] = √√P √√P bp[p] = Up[p].
This means that:
(bΘ V)[(D∕P) ∙p] = √PUp[p]bp[p] = √Pp[p] = W[(D∕P) ∙p],
as desired.
Case 2:	K = T ∙ P. We construct (u, v) using an optimal solution UP and VP to eq. (5) for RP)(P). We let
U = T 3∕4D√p [Up,..., Up] be a scaled version of T repeated copies of Up. We let V = ^1^ [Vp,..., Vp] be a scaled
version of P repeated copies of Vp. Notice that
2	2D	2D 2D	D
kUk2 + kVk2 = √TP kUpk2 + √TP kVp k2 = √TP RPj(P) = 2 √TP kPkι,
as desired. To see that u，? V = w, it suffices to show b Θ V = W. Notice that b[(D∕P) ∙ p] = T 1∕√Pbp[p] for
0 ≤ P ≤ P 一 1, and b[d] = 0 if (D/P) - d. This means that (U Θ b)[d] = 0 = W[d] if (D/P) - d as desired.
Thus it suffices to handle (U Θ b)[(D/P) ∙ p]. Notice that b[(D/P) ∙ p] = Tl√D√√pbp[p], and b[(D/P) ∙ p]=
T√√P √P√T3∕4 Up[p] = T 1/4Up[p]. This means that:
(bΘ V)[(D/P) ∙p]
√Pbp[p]bp[p] = √PP[p] = W[(D/P) ∙p],
as desired.
□
Lemma 18 (Lower bound). Consider vectors W(P) = [P, P, . . . , P] ∈ RD	specified by P ∈ RP (such that D is a
multiple of P).
(a)	For any K ≤ P, it holds that ∀C:
rk,c(W(P)) ≥ P ∙RP1(p).
(b)	For P ≤ K ≤ D if K = P ∙ T for integer T ,then ∀C:
rk,c(W(P)) ≥ 2ʌ/DkWki.
Proof of Lemma 18. We use the dual formulation in eq. (23). Our approach is to construct a dual vector λ ∈ CD so
that eq. (23) achieves the desired objective.
33
Under review as a conference paper at ICLR 2022
Case 1:	K ≤ P. Let λp ∈ CP be the dual optimal solution for RSDP(P). Now, let λ[(D∕P) ∙ p] = √√Pλp[p] for
0 ≤ p ≤ P - 1, and λ[d] = 0 if (D/P) - d. Notice that the objective becomes:
D	P-1
2 X ∣λ[d]∣∙∣W [d]| = 2 X ∣λ[(D∕P) ∙ p]∣∙∣W[(D∕P) ∙ p]∣
d=0	p=0
D P-1
2P ∑∣λp[p]∣∙∣P[p]∣
p=0
D P-1
2 P ∑∣λp[p]∣∙∣P[p]∣
p=0
D RKDP(P)
Dn ，、
P RK,C (P),
where the last equality follows from tightness of the SDP (Theorem 3). It thus suffices to show that λ is dual feasible.
For x ∈ CK such that kxk ≤ 1, consider:
D-1	P-1
X ∣b[d]∣2 ∙∣λ[d]∣2 = X ∣X[(D∕P) ∙ p]∣2 ∙ ∣λ[(D∕P) ∙ p]∣2
d=0	p=0
D P-1
=P ∑ ∣b[(D∕P) ∙p]∣2 ∙∣λp[p]∣2.
p=0
Now, let x(P) ∈ CP be the Fourier representation of x when the base dimension is P. Observe that x[(D∕P) ∙ p] is
equal to ʌ/DX(P)[p]. Thus the above expression is equal to:
DP
PD
P-1	P-1
X Ib(P)[p]|2 ∙ lλP[P]l2 = X Ix(P)[p]l2 ∙ lλP[P]l2.
p=0	p=0
Since λp[p] is dual feasible for the P -dimensional problem, we see that this is at most 1, as desired.
Case 2:	K = T ∙ P. We consider λ[(D∕P) ∙ p] = √D for 0 ≤ P ≤ P - 1, and λ[d] = 0 if (D∕P) - d. Notice that
the objective in eq. (23) is equal to:
D	√DP T	√D
2∑ ∣λ[d]∣∙∣W[d]l = 2√= E IW[(D∕P) ∙p]∣ = 2√= kwkι,
d=0	K p=0	K
as desired. It thus suffices to show that λ is dual feasible. For x ∈ CK such that kxk ≤ 1, we consider
D-1	D P-1
∑∣b[d]∣2 ∙∣λ[d]∣2 = K £ ∣b[(D∕P) ∙p]∣2.
d=0	p=0
Let xo,..., XT-ι ∈ CP be defined so that xt[p] = χ[t ∙ K + p] for 0 ≤ P ≤ P — 1 and 0 ≤ t ≤ T - 1. Now,
let XtP) denote the Fourier representation of X when the base dimension is P, and observe that b[(D∕P) ∙ p]=
34
Under review as a conference paper at ICLR 2022
DP PT-I xb(tP) [p]. Thus we can rewrite the above expression as:
D P P-1
KD P=o
as desired. This completes the proof.
T-1
X xbt(P) [p]
t=0
2
P 1T 1
T xx Tw )[p]∣
p=0 t=0
T-1
xxbt(P)2
t=0
T-1
x kxtk2
t=0
kxk2 ≤1,
□
From these matching upper and lower bounds, we can easily conclude Lemma 16.
ProofofLemma 16. Lemma 16 follows directly from Lemma 17 and Lemma 18.	□
E Appendix for Section 5: Networks with multi-channel inputs
For all the results in this appendix, we recall that the weights of the first and second layer are denoted as U = {Ur }r∈[R]
with Ur ∈ RK×C ∀r∈[R] and V ∈ RD×C, respectively.
E.1 SDP relaxation for multi-channel input networks
The SDP relaxation for RK,C,R is derived similarly to for networks with a single input channel. For any U = {Ur ∈
RK×C}r∈[R], V ∈ RD×C, we define a rank C positive semidefinite matrix Z ∈ R(D+K.R)x(D+K.R),thatrepresents:
U0
Ui
Z =	...
UR
V
[U0> U1>	. . . U>R V>]
< 0.
35
Under review as a conference paper at ICLR 2022
We also define Hermitian matrices Adr, Ad^ ∈ R(D+K∙R)X(D+K∙R) for d ∈ [D],r ∈ [R]. If We let Qd = FKe√e>F,
these matrices take the following form:
Arde,a0l
real
d,1
i Qd
0(R∙K)×(R∙K)	0	0
.
.
.
；0
Qd 0	...	0 i 0D×D
:0
0(R∙K)×(R∙K)	; Qd
.
.
.
；0
0	Qd	…	0 i 0D×D
;0
0(R∙K)×(R∙K)	i 0
.
.
.
_ ] Qd
0 0	...	Qd i 0d×d
-i ∙ Qd 0
0(R∙K)×(R∙K)
i ∙ Qd
0
0 ； 0D×D
；0
0(R∙K)×(R∙K)	J i Qd
0	-i ∙ Qd
0 ； 0D×D
00
0(R∙K)×(R∙K)
i ∙ Qd
-i Qd i 0D
×D
A
0
0
0
0
We also provide a more formal description of these matrices. We provide a block-Wise description of only the upper
diagonal blocks, With loWer diagonal blocks filled to satisfy the Hermitian matrix property. Additionally, for matrices
{Arde,arl, Aidm,rg}d∈[D],r∈[R] any unspecified block is by default treated as zero matrix 0 of appropriate dimension:
•	For r1, r2 ∈ [R] With r2 ≥ r1, the K × K block With indices (r1 : (r1 + 1)K) along roWs and (r2 : (r2 + 1)K)
along columns is given as
Z[r1 : (r1 + 1)K, r2 : (r2 + 1)K] =Ur1Ur>2;
•	For r ∈ [R], the K × D blocks With indices (r : (r + 1)K) along roWs and (RK : (D + RK)) along column,
are given as folloWs:
Z[r : (r+ 1)K, RK : (D + RK)] = UrV>.
Further,
∀d∈[D], Arde,arl[r: (r+1)K,RK: (D + RK)] =Qd, and
Admg[r : (r + 1)K, RK : (D + RK)] = i ∙ Qd
Note that for r0 6= r, the corresponding blocks in Arde,arl0, Aidm,rg0 remain the default zero.
•	Finally, the loWer-right D × D block is given as
Z[RK : (D+RK),RK : (D+RK)] = VV>.
Using this notation, We consider the folloWing SDP relaxation of RK,C,R(W) in terms of Fourier coefficients
Wc = FW:
RSKD,PR(W) = mZ<in0 hZ, Ii
s.t. ∀d∈	[D], r ∈	[R]	hZ,Arde,arli	= 2Re(Wc [d,	r])	(24)
∀d∈	[D], r ∈	[R]	hZ,Aidm,rgi	= 2Im(Wc [d,	r]).
We can check that the SDP formulation With an additional rank constraint of rank (Z) ≤ C is equivalent RK,C,R(W)
and the SDP thus provides a loWer bound: i.e., ∀W, RK,C,R(W) ≥ RSKD,PR(W).
36
Under review as a conference paper at ICLR 2022
E.2 Realizability of linear functions
We show that multiple output channels can be needed to merely realize all linear maps for multi-input-channel networks.
Lemma 6. For any K, C and R, in order for the the network represented by W(U, V) in eq. (11) to realizes all linear
maps in RD×R it is necessary that K ∙ C ≥ min{R, D}.
We prove this lemma by showing that the sub-network corresponding to each output channel can realize a matrix in
RD×R of rank at most K.
Proof. We first reiterate the expressions for the linear predictor W(U, V) in terms ofU, V:
C-1
∀r∈[R], W (U, V)[:,r] = X(Ur [：,c] ? V[：,c]夕.	(25)
c=0
We will now express the above formulation as matrix multiplication using the following new notation: ∀c∈[C] let
Uc ∈ RK ×r denote the representation of first layer weights corresponding to each output channel such that
forallr∈[R]∀c∈[c↑, Uc[:,r] = UJ,c].
For c ∈ [C], consider the following matrix which consists of first K columns of the circulant matrix formed by V[:, c]:
「	V[0,c]	V[D -1,c]
〜	1	V[1,c]	V[0,c]
∀c∈[C], Vc = √D	.	.
V[D-1,c] V[D-2,c]
V[D-K+ 1,c]
V[D-K+2,c]
.
.
.
V[D - K, c]
∈ RD×K
Based on this notation, we can check by following the definitions that for all c,
(Ur [:,c] ? v[:,e](), = VcUr [:,c] = Vc Uc[:,r].
We can thus write W(U, V) as follows:
C-1
W (U, V) = X Vj Uc.	(26)
c=0
DK	DK
We now observe that each term in the summation Vc Uc is of rank utmost K as Vc ∈ Rd×k and Uc ∈ Rd×k.
Thus, for any U, V, rank(W(U, V)) ≤ K ∙ C. From this we conclude that in order to realize all linear maps in the
multi-channel input space of Rd×r, we necessarily need K ∙ C ≥ min{R, D}.
Additionally, in eq. (26) we see that since Uc are unconstrained, each term in the sum can realize any rank 1 matrix.
This implies that C ≥ min{R, D} is a sufficient condition for W(U, V) to realize any W ∈ RD×R. However, from
Theorem 10 we know that this condition is not necessary. It is an open question to derive the tightest necessary and
sufficient conditions.
We finally note that a similar proof can be shown using the Fourier representation in eq. (11).	□
However, in the special cases of K = 1 and K = D, in Theorems 9-10 we show that the SDP is tight once C is
large enough to realize all linear functions, which in these cases is C ≥ R/K. In these end cases, we further derive
interesting closed form expressions of RK,C,R(W).
E.3 Proof of Lemma 7
Lemma 7. For any W ∈ RD×R, and any C ≥ RK, it holds that RK,C,R(W) = RSKD,PR (W).
Proof. We show a stronger statement: any optimal solution to the SDP has rank at most RK. This implies the desired
result, because RK,C,R(W) is equivalent to the SDP with a rank constraint of C.
37
Under review as a conference paper at ICLR 2022
For the remainder of the proof, we let Z be an optimal solution to the SDP and we prove that rank(Z) ≤ RK. Let
rank(Z) = L. We can write:
U0
Ui
Z =	...
UR
V
[U0>
U1>
U>	V>]
R
where the matrices Ur ∈ RK×L for 0 ≤ r ≤ R - 1 correspond to the weights in the convolution layer, the matrix
V ∈ RD×L corresponds to the weights in the linear layer. It suffices to show that there exists a spanning set of the
column space of
「Uo 1
Ui
...
UR
V
that has at most RK elements.
The key ingredient of the proof is the KKT conditions. Using similar logic to Appendix C, we can write the KKT
conditions in the following form:
(KKT 1)
for all 0 ≤ r ≤ R - 1 : Wc [:, r] = Ubr	Vb
-	：FK ΛoF 1 0(R∙K)×(R∙K)	：FK ΛiF . . . 	I FKΛr-iF	WId+kr
_FA°Fk	FΛiFk	...	FΛr-iFk ；	0d×d .	
(KKT 2)
_	R-1
V = X Λr U r	(KKT 3)
r=0
—	ʌ — =T . ʌ
forall 0 ≤ r ≤ R — 1: Ur = FKFKΛr V.	(KKT 4)
where the matrices Λr ∈ CD×D for 0 ≤ r ≤ R - 1 are diagonal and correspond to the relevant dual variables. Let’s
use (KKT 4) to construct a spanning set of the column space of this matrix. Let Sr ur ∈ RK be a basis for the
column space of Ur for each 0 ≤ r ≤ R; since Ur is K × L dimensional, we see that Sr has at most K elements.
We See by (KKT 4) that the set of vectors given by the concatenation of [uo, ui,..., ur-i,F (PR=-I Λrb")] for
ur ∈ Sr for each 0 ≤ r ≤ R spans the column space of the desired matrix. By construction, this spanning set has at
most RK elements, as desired.
□
E.4 PROOFS OF THEOREMS 9-10: INDUCED REGULARIZER FOR K = 1 AND K = D
Theorem 9.	For any W ∈ RD×R, and any C ≥ min{R, D}, the induced regularizer for K = 1 is given by the scaled
nuclear norm kJ∣*:
Ri,c,r (W) = 2√D∣Wk * = 2√D∣Wk *.
Proof. For K = 1, we have that ∀r∈[R], Ur ∈ Ri×C. For this proof, we stack the vectors Ur to obtain Ue ∈ RR×C
such that ∀r∈[R] , U[r, :] = Ur.
We work with the signal space definition of the linear predictor realized by network as: ∀r∈[R] , W(U, V)[:, r] =
PC-OI(Ur [：，c] ? V[：,c]，), (from eq. (11)).
For kernel size of 1, we notice that from the definition of convolution in Definition 1, we have the following:
(Ur [：, c] ? V[:, c]∙ψ = -√^V[:, c] (X V[:, c].	(27)
38
Under review as a conference paper at ICLR 2022
Plugging this back into the expression of W (U, V), we have the following:
1
1
W(U,V)[:,r]
√D
VU[r, :]
=⇒ W(U,V)
√D
>
VUe .
(28)

In the above formulation V ∈ RD×C, U ∈ RR×C are of rank C, but otherwise completely unconstrained. Thus,
they can realize any rank K matrix. So as long as C ≥ min{R, D}, the network can realize any linear predictor
W ∈ RD×R.
The rest of the proof follows from connecting the above expression into the variational characterization of the nuclear
norm. The induced regularizer R1,C,R(W) from eq. (12) for C ≥ min{R, D} can now be expressed as follows:
R1,C,R(W) =	min	kUek2+ kVk2
Ue ∈RR×C,V∈RD×C
>—	~T
s.t.,	√Dw = VUe .
(29)
For C ≥ min{R, D} eq. (29) is exactly the variational definition of nuclear norm (see Rennie & Srebro (Lemma 1
2005)) and thus Ri,c,r(W) = 2√D∣∣Wk* for even unbounded C. The fact that C = min{R, D} is sufficient can
be seen by obtaining the optimum nuclear norm as upper bound from using V = L√Σ and Ue = R√Σ, where
DWw= = LΣRt is the singular value decomposition of √DW. Finally,
we note that based on our normalization of
Fourier transform, we have k Wk*
kWck*
This completes our proof.
Note: For K = 1 we provided the proof of R1,C,R(W) in the signal space of W, but the Theorem can also be
proved in the Fourier domain (similar to the proof of Theorem 10 given below) by first showing that the SDP relaxation
evaluates to the nuclear norm and combining this with the matching upper bound for Ri,c,r(W) shown above. □
Theorem 10.	For any W ∈ RD×R, and any C ≥ 1, the induced regularizer for K = D is given as follows
D-1
RD,C,R(W)=2kWck2,1 := X
d=0
R-1
X Wc[d,r]2.
r=0
∖
Proof. We begin by expressing induced regularizer for full dimensional kernel sizes RD,C,R(W) from eq. (12) in
terms of the Fourier representation of the linear predictor realized by the network W (U, V):
RD,C,R(W) = Uin,Vf kUbrk2 + kVb k2
T
s.t., ∀r∈[R]W[:, r] = diag(UrV ),
(30)
where U, V are of dimensions U = {Ur ∈ RD×C}r∈[R], V ∈ RD×C.
Our proof for the case of networks with multi-channel inputs with K = D follows the following structure:
Step 1. We first show an upper bound on the induced regularizer for single output channel C = 1 with full dimensional
kernel K = D as RD,1,R(W) ≤ 2kWc k2,1 by providing a construction of U , V. It immediately follows
from the monotonicity of RD,C,R that for all C, RD,C,R(W) ≤ RD,1,R(W) ≤ 2kWck2,1. This step also
consequently shows that when K = D, every linear predictor over the multi-channel input space of RD×R is
realizable by a network with even a single output channel.
Step 2. The bulk of our proof lies in matching the upper bound with a lower bound on the dual problem of the SDP in
eq. (24)asRSDD,PR(W) ≥ 2kWc k2,1. This gives us that for all C ≥ 1, RD,C,R(W) ≥ RSDD,PR(W) ≥ 2kWck2,1.
Step 1. Upper bound on the induced regularizer RD,C,R We first show that RD,1,R(W) ≤ 2kWck2,1 =
2 Pd∈[D] kWc [d, :]k. Since RD,R,C (W) is decreasing in C, it suffices to show this for C = 1. For C = 1 and
K = D, we have V ∈ RD and ∀r∈[R] Ur ∈ RD. For full dimensional kernels, the Fourier domain representations
D
U0, U1, . . . , UR, V ∈ CD are all unconstrained beyond the symmetry properties of Fourier transform of real matrices.
Thus consider the following U , V:
一	Cr√『1	一	I~ʌ------
∀de[D]∀re[R], Ubr[d] =	/	,	, and V[d] =，||W[d, :]k.
kWc[d, :]k
(31)
39
Under review as a conference paper at ICLR 2022
It is easy to see that the above U, V satisfy the constraints of RD,1,R(W) in eq. (30) that is ∀r∈[R]W[:, r]
>D
diag(UrV ). Further U0, U1, . . . , UR, V ∈ CD satisfy the required symmetry properties since W is real.
Now computing the objective, we immediately have that kVb k2 = Pd∈[D] kWc [d, :]k, and further,
kUbrk2=
r∈[R]	d∈[D]
X W [d, r]|2
r∈[R] kW[d, ：]k
X
kWc [d, :]k.
d∈[D]
This construction thus gives us the desired upper bound
RD,C,R(W) ≤ RD,1,R(W) ≤ 2kWc k2,1.	(32)
Step 2: Lower bound on the induced regularizer RD,C,R We show the lower bound by lower bounding the dual
problem of the SDP in eq. (24).
For r ∈ [R], let λrreal ∈ RD and λirmg ∈ RD denote the dual variables corresponding to the constraints in the SDP in
eq. (24) for the real and imaginary parts, respectively, ofWc [:, r]. Similar to single input channel proofs, we define
λr = λreal + i ∙ λrmg and Λr = diag(λr). Additionally, We introduce the notation for the matrix obtained by taking
{λr}r as columns: Ξ = [λ0, λ1, . . . , λR] ∈ CD×R such that ∀r, Ξ[:, r] = λr.
Based on Weak duality for the SDP in eq. (24), We have the folloWing:
RDDR(W) ≥」,max、12 ∙ Re(hW, Ξ))
Ξ=[λ0,λ1,...,λR]	33
s.t.,	P(λreal[d] ∙ Ad,rl+λimg[d] ∙ Admg) 4 i.
Our rest of the proof obtains the loWer bound by constructing an appropriate Ξ satisfying the constraints:
From the definitions of {Ad：, Admg}d∈[D],r∈R in Appendix E.1 and using Qd = Fe√e>F (note that for K = D and
FK = F = F>), We have the folloWing:
E(λreal[d]∙Ad,rl+λimg [d] ∙ Adr)=
d,r
一	i FΛoF -
0(R∙D)×(R∙D)	F FAIF
!	.
,	.
.	.
_	_	_	[ FARF
_FAoFK FAiFk	...	FArFk ； 0d×d _
(34)
We state and prove the folloWing claim:
Claim. Ξ satisfies the constraints of the dual problem in the RHS of eq. (33) if max kΞ[d, :]k ≤ 1
Proofofclaim. The relevant constraint in eq. (33) is P(λ,al[d] ∙ Adr + λ*g[d] ∙ Adlg) 4 I. It thus suffices to show
d,r
that for all y ∈ Rd such that kyk = 1, it holds that PR=。IIFAr Fyk2 ≤ 1. We have the following set of inequalities
that prove the claim ∀y： ∣∣y ∣∣ = ι:
RR
X ∣FArFyk2 =) X ∣Ar(Fy)k2
DR	D
=XX ∣λr[d]∣2 ∙∣(Fy)[d]∣2 =) X kΞ[d, :]k2 ∙∣(Fy)[d]∣2
≤ max但[4 ：]『国丫『
d∈[D]
(=c) max kΞ[d, :]k2,
d∈[D]
where (a) follows from F being unitary, (b) from definition of Ξ, and (C) from ∣∣Fyk = ∣∣yk = 1.	口
40
Under review as a conference paper at ICLR 2022
Consider Ξ defined as follows:
—— -
∀	n[d .] — W[d,：]
d∈[D],」,：]=kW[d,:]k
It is easy to check that max kΞ[d, :]k = 1 and thus based on the claim we proved above Ξ satisfies the constraints of
d∈[D]
the dual optimization problem in the RHS of eq. (33). Additionally, the objective evaluates to the desired bound of
_	, ,≤2≥ 一 . 一	, …	..≤2≥..	一 一	_	.	一 一
Re(hW, Ξi) = Pd kW[d, :]k = kWk2,1. We thus have the following lower bound:
RD,C,R(W) ≥ RSDD,PR(W) ≥2Re(hWc,Ξi)=2kWck2,1.
(35)
Conclusion of the proof. The proof of the Theorem follows from combining the matching upper and lower bounds
(W) = 2kWck2,1.
in eq. (32) and eq. (35), respectively, to obtain RD,C,R
□
41