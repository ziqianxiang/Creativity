Under review as a conference paper at ICLR 2022
Towards a Game-Theoretic View of Baseline
Values in the Shapley Value
Anonymous authors
Paper under double-blind review
Ab stract
This paper aims to formulate the problem of estimating optimal baseline values,
which are used to compute the Shapley value in game theory. In the computation of
Shapley values, people usually set an input variable to its baseline value to represent
the absence of this variable. However, there are no studies on how to ensure
that baseline values represent the absence states of variables without bringing in
additional information, which ensures the trustworthiness of the Shapley value. To
this end, previous studies usually determine baseline values in an empirical manner,
which are not reliable. Therefore, we revisit the feature representation of a deep
model in game theory, and formulate the absence state of an input variable. From
the perspective of game-theoretic interaction, we learn the optimal baseline value
of each input variable. Experimental results have demonstrated the effectiveness of
our method. The code will be released when the paper is accepted.
1	Introduction
Deep neural networks (DNNs) have exhibited significant success in various tasks, but the black-box
nature of DNNs makes it difficult for people to understand the internal behavior of the DNN. Many
methods have been proposed to explain the DNN, e.g. visualizing appearance patterns encoded
by deep models (Simonyan et al., 2013; Yosinski et al., 2015; Mordvintsev et al., 2015), inverting
features to the network input (Dosovitskiy & Brox, 2016), extracting receptive fields of neural
activations (Zhou et al., 2015), and estimating the attribution/saliency/importance of input variables
(e.g. pixels in an image, words in a sentence) w.r.t. the output of the model/network (Zhou et al.,
2016; Selvaraju et al., 2017; Lundberg & Lee, 2017).
When we input a sample to a deep model, we focus on studies of estimating the attribu-
tion/saliency/importance of input variables to the model output. To this end, the Shapley value
is widely used and considered as an unbiased measure of an input variable’s attribution (Shapley,
1953; Grabisch & Roubens, 1999; Lundberg & Lee, 2017; Sundararajan & Najmi, 2020). As an
attribution metric, the Shapley value satisfies the linearity, nullity, symmetry, and efficiency axioms,
which ensure the trustworthiness of the attribution.
However, the determination of baseline values is a typical problem with the theoretical foundation of
the Shapley value, which hurts the trustworthiness of the explanation. As Figure 1 (left) shows, the
Shapley value of an input variable is computed as the marginal difference of the model output between
the case of maintaining this variable and the case of removing this variable. The removal of an input
variable is usually implemented by setting this variable to a certain baseline value (or called reference
value), to represent its absence state. In previous studies, people usually simply set the removed input
variables to zero or the mean value over different inputs (Ancona et al., 2019; Dabkowski & Gal,
2017), namely the zero baseline and the mean baseline. Fong & Vedaldi (2017) blurred the input
image and used the smoothed pixel values as baseline values, namely the blurring baseline. Besides,
instead of setting specific baseline values, Covert et al. (2020b); Frye et al. (2021) determined baseline
values conditionally depending on the neighboring contexts, namely the conditional baseline.
Problem with empirical settings of baseline values. Frye et al. (2021) has pointed out that the
incorrect setting of baseline values may lead to dramatically incorrect attributions. We also find that
incorrect baseline values will mistakenly explain a single complex concept as the mixture of massive
simple concepts (see Appendix E.1). In fact, the essence of the problem with baseline values can
be summarized as that baseline values do not satisfy the following two requirements. First, baseline
1
Under review as a conference paper at ICLR 2022
ShaPIey value of i
Φi = ^s⊆N∖{i}
Maintaining variable 也	Removing variable 也
JMask the image by S ∪ {i} = N ∖ (S ∪ {i})	Mask the image by 6=N∖S	/
Figure 1: (Left) The computation of Shapley values. (Right) Previous settings of baseline values,
including the zero baseline, mean baseline, and blurring baseline.
introduce additional introduce additional
information	information
(black dots)	(gray dots)
Zero baseline value Mean baseline value
Blurring the image
cannot remove low-
frequency features
values should remove all information represented by original variable values. Second, baseline values
should not bring in new/abnormal information. Otherwise, the computed Shapley value may not
faithfully reflect the attribution of input variables.
Unfortunately, all the aforementioned baseline values have the above problem. As Figure 1 (right)
shows, the zero baseline and mean baseline would introduce new/abnormal information. The blurring
baseline cannot remove all signals. Besides, the conditional baseline cannot be considered to
faithfully satisfy the linearity and nullity axioms from some aspects, and they are not suitable for
continuous variables due to the computational cost. Detailed analysis is introduced in Section 3.1.
Solutions. In order to solve the above issue in both theory and practice, we define the absence state of
input variables in game theory, i.e. the same theoretic system of defining the Shapley value. Given a
trained model and input samples, we aim to learn baseline values for different input variables, which
satisfy the following two requirements. (1) Our method is supposed to retain all the four axioms
of Shapley values, which ensure the solidness of the Shapley value theory. (2) The baseline value
removes the information of the original input variable without bringing in new information.
In order to define the absence state of an input variable, let us first revisit the feature representation
of a model in terms of game-theoretic interactions. Let an input sample of a deep model has n
variables N = {1, 2, . . . , n}. In the deep model, input variables do not contribute to the model output
individually. Instead, different input variables cooperate with each other to form some interaction
patterns for inference. Thus, each subset of input variables S ⊆ N can be considered as a potential
interaction pattern. To this end, we discover that we can use the Harsanyi dividend (Harsanyi,
1963) I(S) to measure the benefit of the interaction between variables in the subset/interaction
pattern S ⊆ N . In this way, the output of the model can be decomposed as the sum of 2n - 1
interaction patterns, i.e. model output = Ps⊆n,s=d I(S)+constant, as shown in Figure 6. Let us
consider a toy example where a model can be explained to contain two interaction patterns f (x) =
I({x1, x2}) + I({x2, x3, x4}) = w12δ1δ2 + w234δ2δ3δ4. δi ∈ {0, 1} represents the presence/absence
of the variable xi . Each interaction pattern represents an AND relationship between multiple input
variables, e.g. I{x1, x2} = w12δ1δ2 6= 0 if and only if (δ1 = 1) & (δ2 = 1).
The interaction pattern provides us a game-theoretic way to model the absence state of an input
variable. Specifically, if an interaction pattern S has a large absolute value |I(S)|, we consider the
interaction between variables in S has a significant influence on inference. Such interaction patterns
are called salient patterns. Thus, we can count the number of salient patterns S associated with the
variable i ∈ S as the importance of i. In this way, if an input variable is not involved in any salient
patterns, then this variable is negligible for inference, thus can be considered absent. However, it
is difficult to make an input variable not be involved in any salient patterns. Therefore, the absence
state of an input variable is defined as the baseline value that makes the variable i be involved in the
least salient patterns. In addition, the computational cost of learning such optimal baseline values
is exponential. Fortunately, we discover an approximate yet efficient solution to it. Therefore, by
deactivating most salient interaction patterns, the learned baseline value removes most information
from the input variable without bringing in new information.
Contributions of this paper can be summarized as follows. (1) We formulate two requirements for
baseline values, and define the optimal baseline value in game theory. (2) We develop a method to
estimate optimal baseline values, which ensures the reliability and trustworthiness of the Shapley value.
(3) We measure the multi-variate interaction between input variables based on the Harsanyi dividend,
and prove its theoretical connections to other attributions and interactions, which demonstrate the
rigorousness of this metric.
2
Under review as a conference paper at ICLR 2022
2	Related works
Shapley values. The Shapley value (Shapley, 1953) was first proposed in game theory, which
was considered as an unbiased distribution of the overall reward in a game to each player. Some
previous studies used the Shapley value to explain different models (Gromping, 2007; Strumbelj
et al., 2009; Lundberg & Lee, 2017). Sundararajan et al. (2017) proposed Integrated Gradients based
on the AumannShaple (Aumann & Shapley, 2015) cost-sharing technique. Besides the above local
explanations, Covert et al. (2020b) focused on the global interpretability. However, the computational
cost of Shapley value is large. Therefore, Lundberg & Lee (2017); Lundberg et al. (2018); Aas et al.
(2019); Ancona et al. (2019) explored different approximate yet efficient estimations of Shapley
values to speed up the computation.
Some previous studies also discussed problems with baseline values in the computation of Shapley
values. Most studies (Covert et al., 2020a; Merrick & Taly, 2020; Sundararajan & Najmi, 2020;
Kumar et al., 2020) compared influences of baseline values on explanations, without providing any
principle rules for setting baseline values. Besides, Agarwal & Nguyen (2019) and Frye et al. (2021)
used generative models to alleviate the out-of-distribution problem caused by baseline values.
Unlike previous studies, we rethink and formulate baseline values from the perspective of game
theory. We define the absence state of input variables based on the multi-variate interaction, and
further propose a method to learn optimal baseline values.
Interactions. Interactions between input variables of deep models have been widely investigated in
recent years. Many people defined interactions between input variables or weights to explain different
models from different perspectives (Sorokina et al., 2008; Tsang et al., 2018; Murdoch et al., 2018;
Singh et al., 2018; Jin et al., 2019; Cui et al., 2019). In game theory, Grabisch & Roubens (1999)
proposed the Shapley interaction index based on Shapley values. Janizek et al. (2020) extended the
Integrated Gradients method (Sundararajan et al., 2017) to explain pairwise feature interactions in
DNNs. Sundararajan et al. (2020) defined the Shapley-Taylor index to measure interactions over
binary features. In this paper, we use the multi-variate interaction of input variables based on the
Harsanyi dividend (Harsanyi, 1963). Our interaction metric has strong connections to (Grabisch
& Roubens, 1999), but represents elementary interaction patterns in a more detailed manner, and
satisfies the efficiency axiom.
3	Learning baseline values for S hapley values
Preliminaries: Shapley values. The Shapley value (Shapley, 1953) was first introduced in the
game theory, which measures the importance/contribution/attribution of each player in a game. Let
us consider a game with multiple players. Each player participates in the game and receives a
reward individually. Some players may form a coalition and play together to pursue a higher reward.
Different players in the game usually contribute differently to the game, and then the question is
how to fairly assign the total reward in the game to each player. To this end, the Shapley value is
considered as a unique unbiased approach that fairly allocates the reward to each player (Weber,
1988; Lundberg & Lee, 2017; Sundararajan & Najmi, 2020).
Given a game V with n players, let N = {1,2, ∙∙∙ ,n} denote the set of all players and 2N = {S∣S ⊆
N } denote all subsets of players. The game v : 2N 7→ R is represented as a function that maps a subset
of players S ⊆ N to a scalar reward v(S) ∈ R, i.e. the reward gained by all players in S. Specifically,
v(0) represents the baseline reward without any players. Considering the player i ∈ S,if the player i
joins in S, v(S ∪ {i}) - v(S) is considered as the marginal contribution of i. Thus, the Shapley value
of the player i is computed as the weighted marginal contribution of i w.r.t. all subsets of players
S ⊆ N \ {i}, as follows.
φi=xs⊆n∖{i}p(S)[v(S∪{i}) - V(S)I, P(S)=|S|!(n -n!Sl -1)!	⑴
The fairness of the Shapley value is ensured by the following four axioms (Weber, 1988).
(a)	Linearity axiom: If two games can be merged into a new game u(S) = v(S) + w(S), then the
Shapley values of the two old games also can be merged, i.e. ∀i ∈ N, φi,u = φi,v + φi,w; ∀c ∈ R,
φi,c∙u = C ∙ φi,u.
3
Under review as a conference paper at ICLR 2022
Table 1: Analysis about previous choices of baseline values.
Setting of baseline values	Baseline values are are constant or not	Different samples share the same baseline values or not	Shortcomings
Zero (Ancona et al., 2019) (Sundararajan et al., 2017)	X	X	introduce additional information
Mean values (Dabkowski & Gal, 2017)	X	X	一	introduce additional information
Blurring (Fong & Vedaldi, 2017) (Fong et al., 2019)	X	X	cannot remove low-frequency components
Marginalize (marginal distribution) (Lundberg & Lee, 2017)	X	X	assume feature independence
Marginalize (conditional distribution) (Covert et al., 2020b; Frye et al., 2021)	X	X	destroy linearity and nullity axioms (Sundararajan & Najmi, 2020)
(b)	Dummy axiom and nullity axiom: The dummy player i is defined as a player without any
interactions with other players, i.e. satisfying ∀S ⊆ N \ {i}, v(S ∪ {i}) = v(S) + v({i}). Then, the
dummy player’s Shapley value is computed as φi = v({i}). The null player i is defined as a player
that satisfies ∀S ⊆ N \ {i}, v(S ∪ {i}) = v(S). Then, the null player’s Shapley value is φi = 0.
(c)	Symmetry axiom: If∀S ⊆ N \ {i, j}, v(S ∪ {i}) = v(S ∪ {j}), then φi = φj.
(d)	Efficiency axiom: The overall reward of the game is equal to the sum of Shapley values of all
players, i.e. V(N) - v(0) = P∈n φi.
Using Shapley values to explain deep models. Given a trained model f : Rn 7→ R and an input
sample x ∈ Rn, we can consider each input variable (e.g. a dimension, a pixel, or a word) xi as a
player (i ∈ N), and consider the deep model as a game. The model output f (x) is regarded as the
reward v(N). Thus, the Shapely value φi measures the attribution of the i-th variable xi w.r.t. the
model output. In this case, v(S) represents the model output, when variables in S are present and
variables in S = N \ S are absent. People usually set the variables in S = N \ S to their baseline
values, in order to represent their absence states. In this way, v(S) can be represented as follows.
V(S) = f (mask(x, S)), mask(x, S) = XS t bS, (XSt bS)i = {xi, i ∈ S = N \ S ⑵
where mask(x, S) denotes the masked sample, and bi denotes the baseline value of the i-th input
variable. t indicates the concatenation of x's dimensions in S and b's dimensions in S = N \ S.
3.1	Problems with Shapley values
According to Equations (1) and (2), the Shapley value computes the marginal contribution of the
variable i, v(S ∪ {i}) - v(S), under different masked contexts S ⊆ N. All input variables not in S
are set to their baseline values to represent their absence states. Therefore, a good baseline value
should satisfy the following two requirements. (1) First, it removes the information represented by
the original value Xi . (2) Second, it does not introduce additional information to the input.
Otherwise, incorrect baseline values may lead to dramatically incorrect attributions, and mistak-
enly explain a single complex concept as the mixture of massive simple concepts, as discussed in
Section 3.3. To this end, let us discuss existing empirical settings for baseline values (see Table 1).
•	Mean baseline values. The baseline value of each input variable is set to the mean value of this
variable over all samples (Dabkowski & Gal, 2017), i.e. bi = Ex [xi]. This method actually introduces
additional information to the input. As Figure 1 (right) shows, setting pixels to mean baseline values
brings in massive additional gray dots to the image, rather than represent absence states of variables.
•	Zero baseline values. Baseline values of all input variables are set to zero (Ancona et al., 2019;
Sundararajan et al., 2017), i.e. ∀i ∈ N, bi = 0. As Figure 1 (right) shows, just like mean baseline
values, zero baseline values also introduce additional information (black dots) to the input. Note that
because pixels in the input image are usually normalized to zero mean and a unit variance, the setting
of zero baseline values is equivalent to the setting of mean baseline values in this case.
•	Blurring input samples. Fong & Vedaldi (2017) and Fong et al. (2019) remove variables in the
input image by blurring each input variable xi(i ∈ S = N \ S) using a Gaussian kernel. In this
case, different input samples may have different baseline values. This approach can only remove
high-frequency signals, but fails to remove low-frequency signals (Covert et al., 2020a; Sturmfels
et al., 2020).
4
Under review as a conference paper at ICLR 2022
Figure 2: Visualization of contex-
tual pixels j that collaborates with
a certain pixel i (the blue dot).
• For each input variable, determining a different baseline value for this variable given each specific
context S (neighboring variables). Instead of fixing baseline values as constants, some studies use
varying baseline values, which are determined temporarily by the context S in a specific sample x,
to compute v(S∣x) given x. Some methods (Frye et al., 2021; Covert et al., 2020b) define v(S∣x)
by modeling the conditional distribution of variable values in S = N \ S given the context S, i.e.
V(S|x) = Ep(xo∣xs)[f (XS t XS)]. However, these methods apply varying baseline values based on
the dependency between input variables, which do not faithfully satisfy the linearity axiom and the
nullity axiom of the Shapley value from some aspects (please see (Sundararajan & Najmi, 2020) or
Appendix C for proof). Moreover, these methods compute a specific conditional distribution p(X0 |XS)
for each of 2n contexts S with a very high computational cost. By assuming that input variables are
independent with each other, Lundberg & Lee (2017) simplify the above conditional baseline values
to the marginal baseline values, i.e. v(S∣x) = Ep(xo)[f (XS t XS)]. However, the computational cost of
marginal baseline values is still very large, thus being not suitable for continuous variables.
3.2 Multi-variate interactions and absence states of input variables
Multi-variate interactions. In this study, we use the Harsanyi dividend (Harsanyi, 1963) to measure
the interaction benefit, which is the foundation of representing the presence of an input variable.
Given a trained model and the input sample X with n variables N = {1,2, ∙ ∙ ∙ , n}, let V(S) denote the
model output when only variables in S ⊆ N are input into the model, according to Equation (2). Then,
v(N) - v(0) represents the overall inference benefit of the model output owing to all input variables
in x, w.r.t. the model output without given any variables. In a deep model, different input variables
do not contribute to the model output individually. Instead, they interact with each other to form
interaction patterns for inference. We discover that when we use the Harsanyi dividend (Harsanyi,
1963) to quantify the benefit I(S) from the interaction between variables in an interaction pattern
S, the overall benefit v(N) - v(0) can be decomposed into the sum of benefits I(S) of different
interaction patterns S ⊆ N .
V(N) - V(O) = ES⊆N,S=°I(S)	⑶
For an interaction pattern S ⊆ N, if I(S) > 0, the collaboration between variables in S has positive
effects on model output. If I(S) < 0, the collaboration has negative effects. If I(S) ≈ 0, variables in
S do not have collaborations. The Harsanyi dividend is defined to measure the additional benefit from
the collaboration of input variables in S, in comparison with the benefit when they work individually
and when they form smaller patterns. Specifically, v(S) - v(0) denotes the overall benefit from
all variables in S, and then we remove the marginal benefits owing to collaborations of all smaller
subsets L of variables in S, i.e. {I(L)|L ( S, L 6= 0}, as follows.
I(S )=	v (S) - V(O)	— 工 I(L) ⇒ I (S ) =
{ {	{ {	zι	∙ i ∙ u	L(S,L=0
the benefit from all variables in S
(-1)|S|-|L|V(L)
L⊆S
(4)
Let us consider the example when a model uses yhead = sigmoid(xeyes + xnose + xmouth + xears -
constant) to represent the AND relationship yhead — (xeyes) & (xnose) & (xmouth) & (xears). In
this case, the interaction benefit I ({xeyes , xnose, xmouth}) measures the marginal benefit when eyes,
nose, and mouth collaborate with each other, where the benefits from smaller subsets (e.g. the
interaction between eyes and nose) and the individual benefits (e.g. the individual benefit from nose)
are removed. In other words, I ({Xeyes , Xnose , Xmouth}) = v({Xeyes , Xnose , Xmouth}) - v(0) - I ({Xeyes }) -
I {Xnose} - I ({Xmouth}) - I ({Xeyes , Xnose}) - I ({Xeyes , Xmouth}) - I ({Xnose , Xmouth}). Furthermore, Figure 2
visualizes the distribution of contextual pixels j that collaborate with a certain pixel i (the blue dot).
Please see Appendix E.2 for details of the visualization method.
Properties of multi-variate interactions. We extend the linearity, dummy, symmetry axioms of
Shapley values to the above definition of I(S) (please see the Appendix B.1 for details). Besides, the
above metric has also been proven to have a close relationship to the Shapley value. We further prove
its relationship to other game-theoretic interaction metrics in Appendix B.3, including the Shapley
5
Under review as a conference paper at ICLR 2022
interaction index (Grabisch & Roubens, 1999) and the Shapley Taylor interaction index (Sundararajan
et al., 2020).
Counting salient patterns associated with each input variable. According to Equation (3), some
interaction patterns have large absolute values |I(S)|, which have significant influences on the model
output, namely salient patterns. In comparison, other patterns have small absolute values |I(S)|,
having little effects on the model output, namely noisy patterns. According to (Harsanyi, 1963), the
benefit of an interaction pattern consisting of m variables can be fairly assigned to the m variables.
In this way, for each input variable i ∈ N, we can consider the number of salient interaction patterns
associated with this variable as the numerical importance of this variable to the model output.
Definition of absence state and optimal baseline values. The computation of the Shapley value is
conducted based on the assumption that baseline values represent the absence of input variables. The
number of salient patterns associated with an input variable i indicates the importance (presence) of i,
but it is difficult to find a baseline value that removes all salient patterns associated with i. Thus, the
absence state of the variable i can be achieved, when the baseline value of i removes most existing
salient patterns associated with i without triggering new salient patterns. In other words, baseline
values are learned to remove the original information from input variables and to avoid bringing in
new information. Therefore, the learning of the baseline value b of the input variable i is formulated
to sparsify the salient patterns associated with i.
bi = argminbi Xs⊆n ∖{i}1∣i(su{i})l≥τ	(5)
where τ denotes the threshold to determine salient patterns.
3.3 Estimating baseline values to minimize the number of salient patterns
Equation (5) guides the learning of optimal baseline values, but the computational cost of enumer-
ating/counting all salient patterns is exponential. Thus, we need to find an approximate solution to
learning baseline values. Fortunately, the order of interaction patterns provides us with a new per-
spective to solve this problem. The order of the interaction benefits I(S) is defined as the cardinality
of S, i.e. the order m = |S|. To this end, we obtain the following two propositions.
(1)	Correct baseline values ensure sparse high-order interaction patterns. Theoretical analysis and
preliminary experiments (see Table 6 in Appendix E.1) have shown that incorrect baseline values
usually explain a high-order interaction utility (the collaboration between massive variables) as the
sum of massive low-order interaction utilities, which are actually unnecessary. For example, given a
certain set of baseline values, the model may be explained to contain a single high-order interaction
pattern between massive input variables. However, the same model may be explained to encode
massive low-order interactions between a few baseline values, given another set of baseline values.
Thus, we aim to learn baseline values that correctly reflect the logic of the model by using sparse,
salient, and high-order interaction patterns.
(2)	High-order interaction patterns are more likely to be deactivated. From another perspective, the
benefit of each interaction pattern I(S) represents an AND relationship between all variables in S.
The absence of any variable (i.e. setting this variable to its baseline value) deactivates this pattern.
Obviously, high-order interaction patterns depend on massive variables. Thus, high-order interaction
patterns are more likely to be deactivated when some variables are masked during the computation
of Shapley values. If the model is mainly represented by high-order interaction patterns, it is more
likely to deactivate patterns and achieve the minimum pattern number.
Based on the above propositions, we aim to learn baseline values that represent the model using
sparse and salient high-order interaction patterns. According to Equation (3), the weighted sum of
high-order interactions and low-order interactions is relatively stable as V(N) - v(0). It means that
penalizing the influence of low-order interaction patterns will increase the influence of high-order
interaction patterns, thereby pushing the model to mainly use high-order interaction patterns for
inference. Therefore, the objective of learning baseline values can be transformed to a loss function
that penalizes the influence of low-order interactions. In this way, baseline values are learned to
strengthen high-order interaction patterns towards salient patterns by penalizing low-order interaction
patterns towards noisy patterns.
An approximate yet efficient solution. Directly computing I(S) is NP-hard. Therefore, we design
loss functions based on the following multi-order Shapley values and the multi-order marginal benefits
6
Under review as a conference paper at ICLR 2022
σ,∣0(7
σm.,σ∕ 铲)ι
0.3- Learned baseline values by Lmarginal
Learned baseline values by LShapley
Zero baseline values
LUJJdlLluIIilI
0	0.1n	0.2n	0.3n	0.4n	0.5n	0.6n	0.7n	0.8n	0.9n
order m
σtσs⊆M∖(i},∣S∣=m1△%(S)I
2m，22sm\{i},|S|=m，|A%(S)I
■	Learned baseline values by Lmarginal
■	Learned baseline values by LShapley
■	Zero baseline values
0	0.1n	0.2n	0.3n	0.4n	0.5n	0.6n	0.7n	0.8n	0.9n
order m
Figure 3: OUr methods successfully
boost the influence of high-order Shap-
ley values and high-order marginal ben-
efits, and reduce the influence of low-
order Shapley values and low-order
marginal benefits, computed on a func-
tion in (Tsang et al., 2018).
to penalize low-order interaction patterns, which boost the computational efficiency. We prove that the
Shapley value φi can be decomposed into Shapley values of different orders φ(m) (φi = n Pm-I) φ(m)),
as well as marginal benefits of different orders ∆vi(S) (φi = n Pm-O Es⊆n∖{i},∣s∣=m∆vi(S)), which
are proved in Appendix D. φi(m) and ∆vi (S) are given as follows.
φ(m) = ES⊆N∖{i} [v(S ∪ {i}) 一 V(S)] ⇒ φ(m) = ES⊆N∖{i}
|S|=m
∆vi (S) d=ef v
|S|=m
XL⊆SI(L ∪ {i})
(6)
(S∪{i})-v(S)⇒∆vi(S)=	L⊆SI(L ∪ {i})
The order of the marginal benefit ∆vi(S) is defined as m = |S|, the number of elements in S. For
a low order m, φi(m) denotes the attribution of the i-th input variable, when i cooperates with a few
contextual pixels. For a high order m, φi(m) corresponds to the impact of i when it collaborates with
massive contextual variables. Similarly, the order of ∆vi (S) measures the marginal benefit of the i-th
input variable to the model output with contexts composed of m = |S | variables.
Equation (6) has shown that high-order interaction patterns I(S) are only contained by high-order
Shapley values φi(m) and high-order marginal benefits ∆vi (S). In order to penalize the influence of
low-order interaction patterns and boost high-order interaction patterns, we propose the following
two loss functions to penalize the strength of low-order Shapley values, ∣φ(m) |, and to penalize the
strength of low-order marginal benefits, ∣∆vi(S)|, respectively.
LShapley (b) = X	XX ∣φ(m) I,	Lmarginal (b) = X XX
ES⊆N,∣S∣=m AVi (S )|	(7)
m〜Unif(0,λ) x∈X i∈N
m〜Unif(0,λ) x∈X i∈N
where λ > m denotes the maximum order to be penalized. Figure 3 shows the distribution of the ratio
of Pi[∣φ(m) |] and the ratio of 口 Ps⊆n`3,∣s∣=m [∣∆vi (S)|] of different orders, which demonstrates
that our methods effectively boost the influence of high-order interaction patterns. The loss function
on marginal benefits is more fine-grained than the loss function on the multi-order Shapley value.
4 Experiments
Learning baseline values. We used our method to learn baseline values for MLPs and LeNet (LeCun
et al., 1998) trained on the UCI South German Credit dataset (Asuncion & Newman, 2007), the UCI
Census Income dataset (Asuncion & Newman, 2007), and the MNIST dataset (LeCun et al., 1998),
respectively. Based on the UCI datasets, we learned MLPs following settings in (Guidotti et al., 2018).
We learned baseline values using either LShapley or Lmarginal as the loss function. In the computation
of LShaPley, We Set V (S) = log 制就黑葭S)) ∙m the computation of Lmarginal, ∣∆Vi (S )∣ was set to
∣∆vi(S)∣ = ∣∣h(mask(x, S ∪ {i})) - h(mask(x, S))kɪ, where h(mask(x, S)) denoted the output feature
of the second last layer given the masked input mask(x, S), in order to boost the efficiency of learning.
Please see Appendix E.3 for other potential settings of V(S). We used two ways to initialize baseline
values before the learning phase, i.e. setting to zero (Ancona et al., 2019; Sundararajan et al., 2017)
or the mean values over different samples (Dabkowski & Gal, 2017), namely zero-init and mean-init,
respectively. We set λ = 0.2n for the MNIST dataset, and set λ = 0.5n for the simpler data in two
UCI datasets. We compared our methods with five previous choices of baseline values introduced
in Section 3.1, i.e. zero baseline values (Ancona et al., 2019), mean baseline values (Dabkowski &
Gal, 2017), blurring images (Fong & Vedaldi, 2017; Fong et al., 2019), and two implementations of
varying baseline values in SHAP (Lundberg & Lee, 2017) and SAGE (Covert et al., 2020b). Zero
baseline values, mean baseline values, blurring images, and our learned baseline values allowed us
to compute the Shapley value using the sampling-based approximation (Castro et al., 2009). This
7
Under review as a conference paper at ICLR 2022
Zero-init Mean-init
二B'≡MJBE7
SSo- 6u-sn
∙A-dBqs7
SSo- 6u-sn
Input
image
Zero	Mean	Baseline	Ours	OUrs
Baseline Baseline Blurring Baseline in SAGE	LshaPley	LshaPley
value	value	image in SHAP magnitude x2 zero-init mean-init
Figure 4: (Left) The learned baseline values on the MNIST dataset (better viewed
Shapley values produced with different baseline values on the MNIST dataset.
Ours	Ours
Lmarginal	Lmarginal
zero-init	mean-init
二■ ⅛
in color). (Right)

Native-country
HoUrS-Per-Week
CaPital-loss
Capital-gain
Sex
Race
Relationship
Occupation
Marital-status
Education
Workclass
Age
ShaPIey value
USing LShaPley
(zero-init)
Using LShaPIey
(mean-init)
USing LmarginaI
(zero-init)
USing LmarginaI
(mean-init)
0	20	40	60	80	100 1200
Baseline values
learned by our methods
Figure 5: The learned baseline values (left) and Shapley values computed with different baseline
values (right) on the UCI Census Income dataset. Results on the UCI South German Credit dataset
are shown in Appendix E.4.
approximation ensured the unbiased estimation of Shapley values. The stability of the computed
Shapley values with different sampling numbers was examined in Appendix E.5. For the varying
baseline values, we used the code released by (Lundberg & Lee, 2017) and (Covert et al., 2020b).
Figure 4 (left) shows the learned baseline values on the MNIST dataset. Figure 4 (right) compares the
computed Shapley values with different choices of baseline values. Compared to zero/mean/blurring
baseline values, our learned baseline values removed noisy variables on the background, which were
far from the digit in the image. Compared to the baseline values in SHAP, our method yielded more
informative attributions. Shapley values computed using the baseline values in SAGE were dotted.
In comparison, our method generated smoother attributions. Furthermore, our settings satisfied the
linearity axiom and the nullity axiom. Figure 5 shows the learned baseline values and the computed
Shapley values on the UCI Census Income dataset. Unlike our methods, attributions generated by the
zero/mean baseline values conflicted with the results of all other methods.
Verifying baseline values on synthetic functions. People usually cannot determine the ground
truth of baseline values for real images, such as the MNIST dataset. Therefore, we conducted
experiments on synthetic functions with ground-truth baseline values, in order to verify the cor-
rectness of the learned baseline values. We randomly generated 100 functions whose interaction
patterns and ground truth of baseline values could be easily determined. This dataset will be re-
leased after the paper acceptance. The generated functions were composed of addition, subtraction,
multiplication, exponentiation, and the sigmoid operations (see Table 2). For example, for the func-
tion y = sigmoid(3x1x2 - 3x3 - 1.5) - x4x5 + 0.25(x6 + x7)2, xi ∈ {0, 1}, there were three salient
interaction patterns (i.e. {x1 , x2, x3}, {x4, x5}, {x6, x7}), which were activated only if xi = 1 for
i ∈ {1,2,4,5,6,7} and x3 = 0. In this case, the ground truth of baseline values should be b↑ = 0 for
i ∈ {1,2,4, 5,6, 7} and 优=1. Please see Appendix E.6 for more discussions about the setting of
ground-truth baseline values. We used our methods to learn baseline values on these functions and
tested the accuracy. Note that |bi - b* | ∈ [0,1]. If |bi - b↑ | < 0.5, We consider the learned baseline
value correct; otherwise incorrect. We set λ = 0.5n in both LShapley and Lmarginal. Experimental results
are reported in Table 3 and are discussed later.
Verifying baseline values on functions in (Tsang et al., 2018). Besides, we also evaluated the
correctness of the learned baseline values using functions proposed in (Tsang et al., 2018). Among
all the 92 input variables in these functions, the ground truth of 61 variables could be determined (see
Appendix E.6). Thus, we used these annotated baseline values to test the accuracy on these functions.
8
Under review as a conference paper at ICLR 2022
Table 2: Examples of generated functions and their ground-truth baseline values.
Functions (∀i ∈ N,xi ∈ {0,1})
—0.185xι(x2 + x3)2.432 — X4X5X6X7
-x1x2x3 + sigmoid(-5x4x5x6x7 + 2.50) - x8x9
—sigmoid(+4x1 — 4x2 + 4x3 — 6.00) — x4x5x6x7 — x8x9x10
The ground truth of baseline values
b = 0 for i ∈ {1,2,3,4,5,6, 7}
b = 1 for i ∈ {4,5,6, 7}, b = 0 for i ∈ {1,2,3,8,9}
b = 1 for i = 2, b = 0 for i ∈ {1,3,4,5,6,7,8, 9,10}
Table 3: Accuracy of the learned baseline values.
	LShapley			Lmarginal		
	initialize with 0	initialize with 0.5	initialize with 1	initialize with 0	initialize with 0.5	initialize with 1
Synthetic functions	98.06%	98.70%	98.70%	98.06%	98.14%	98.14%
Functions in (Tsang et al., 2018)	88.52%	91.80%	90.16%	86.89%	91.80%	90.16%
Table 4: Accuracy of Shapley values on the ex-
tended Addition-Multiplication dataset when us-
ing different settings of baseline values.
	baseline	mean baseline	baseline values in SHAP	Ours
Accuracy	82.88%	72.63%	81.25%	100%
Table 5: The learned baseline values successfully
recovered original samples from adversarial ex-
amples.
	kxa V - xk2	kb — xk2
MNIST on LeNet	2.33	0.43
MNIST on AlexNet	2.53	1.15
CIFAR-10 on ResNet-20	1.19	1.11
Table 3 reports the accuracy of the learned baseline values on the above functions. In most cases, the
accuracy was above 90%, showing that our method could effectively learn correct baseline values. A
few functions in (Tsang et al., 2018) did not have salient interaction patterns, which caused errors in
the estimation of baseline values.
Correctness of the computed Shapley values. We further verified the correctness of the computed
Shapley values on the extended Addition-Multiplication dataset (Zhang et al., 2021). We added
the subtraction operation to avoid all baseline values being zero. Harsanyi (1963) provided us a
new perspective to compute the Shapley value, i.e. the Shapley value was considered as a uniform
assignment of attributions from each interaction pattern to its compositional variables. This enabled us
to determine the ground-truth Shapley value of variables based on interactions without baseline values.
For example, function f(x) = 3x1x2 + 5x3x4 + x5 where x = [1, 1, 1, 1, 1] contained three interaction
patterns according to the principle of the most simplified interaction. Accordingly, the ground-truth
Shapley values were φ1 = φ2 = 3/2, φ3 = φ4 = 5/2, and φ5 = 1. Please see Appendix E.7 for more
details. We computed Shapley values of variables in the extended Addition-Multiplication dataset
using different baseline values, and compared their accuracy in Table 4. The result shows that our
method exhibited the highest accuracy.
Recovering original samples from adversarial examples. Let x denote the normal sample, and let
xadv = x + δ denote the adversarial example generated by (Madry et al., 2018). According to (Ren
et al., 2021), the adversarial example xadv mainly created out-of-distribution bivariate interactions
with high-order contexts, which were actually related to the high-order interactions in this paper.
Thus, in the scenario of this study, the adversarial utility was owing to out-of-distribution high-order
interactions. The removal of input variables was supposed to remove most high-order interactions.
Therefore, the baseline value can be considered as the recovery of the original sample. In this way, we
used the adversarial example xadv to initialize baseline values before learning, and used Lmarginal to learn
baseline values. If the learned baseline values b satisfy kb - xk1 ≤ kxadv - xk1, we considered that our
method successfully recovered the original sample to some extent. We conducted experiments using
LeNet (LeCun et al., 1998), AlexNet (Krizhevsky et al., 2012), and ResNet-20 (He et al., 2016) on the
MNIST dataset (LeCun et al., 1998) (kδk∞ ≤ 32/255) and the CIFAR-10 dataset (Krizhevsky et al.,
2009) (kδk∞ ≤ 8/255). Table 5 shows that our method recovered original samples from adversarial
examples, which demonstrated the effectiveness of our method. Please see Appendix E.8 for more
discussions.
5 Conclusions
In this paper, we have innovatively defined the absence state of input variables based on the multi-
variate interaction patterns in game theory. Based on this, we have formulated optimal baseline
values for the computation of the Shapley value. Then, we have proposed an approximate yet
efficient method to learn optimal baseline values that represent the absence states of input variables.
Experimental results have demonstrated the effectiveness of our method.
9
Under review as a conference paper at ICLR 2022
Reproducibility Statement. This research mainly focuses on both the theoretical formulation of
baseline values and the learning of baseline values in game theory. Appendix B and Appendix D
provide proofs for all theoretical results in the paper. For the learning of baseline values, we have
discussed all experimental settings about datasets and models in the first paragraph of Section 4 and
Appendix E, which ensure the reproducibility. Furthermore, we will release the code when the paper
is accepted.
References
Kjersti Aas, Martin Jullum, and Anders L0land. Explaining individual predictions when features are
dependent: More accurate approximations to shapley values. arXiv preprint arXiv:1903.10464,
2019.
Chirag Agarwal and Anh Nguyen. Explaining an image classifier’s decisions using generative models.
arXiv preprint arXiv:1910.04256, 2019.
Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a
polynomial time algorithm for shapley value approximation. In International Conference on
Machine Learning,pp. 272-281. PMLR, 2019.
Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
Robert J Aumann and Lloyd S Shapley. Values of non-atomic games. Princeton University Press,
2015.
Peer Bork, Lars J Jensen, Christian Von Mering, Arun K Ramani, Insuk Lee, and Edward M Marcotte.
Protein interaction networks from yeast to human. Current opinion in structural biology, 14(3):
292-299, 2004.
Javier Castro, Daniel G6mez, and Juan Tejada. Polynomial calculation of the shapley value based on
sampling. Computers & Operations Research, 36(5):1726-1730, 2009.
Ian Covert, Scott Lundberg, and Su-In Lee. Feature removal is a unifying principle for model
explanation methods. arXiv preprint arXiv:2011.03623, 2020a.
Ian Covert, Scott Lundberg, and Su-In Lee. Understanding global feature contributions through
additive importance measures. arXiv preprint arXiv:2004.00668, 2020b.
Tianyu Cui, Pekka Marttinen, and Samuel Kaski. Learning global pairwise interactions with bayesian
neural networks. arXiv preprint arXiv:1901.08361, 2019.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. arXiv preprint
arXiv:1705.07857, 2017.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4829-4837,
2016.
Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal
perturbations and smooth masks. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 2950-2958, 2019.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, and Ilya
Feige. Shapley explainability on the data manifold. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=OPyWRrcjVQw.
Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among
players in cooperative games. International Journal of game theory, 28(4):547-565, 1999.
10
Under review as a conference paper at ICLR 2022
Ulrike GrGmPing. Estimators of relative importance in linear regression based on variance decompo-
sition. The American Statistician, 61(2):139-147, 2007.
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini, and Fosca
Giannotti. Local rule-based explanations of black box decision systems. arXiv preprint
arXiv:1805.10820, 2018.
John C Harsanyi. A simplified bargaining model for the n-person cooperative game. International
Economic Review, 4(2):194-220, 1963.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Joseph D Janizek, Pascal Sturmfels, and Su-In Lee. Explaining explanations: Axiomatic feature
interactions for deep networks. arXiv preprint arXiv:2002.04138, 2020.
Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and Xiang Ren. Towards hierarchical importance
attribution: Explaining compositional semantics for neural sequence models. In International
Conference on Learning Representations, 2019.
Alon Keinan, Ben Sandbank, Claus C Hilgetag, Isaac Meilijson, and Eytan Ruppin. Fair attribution of
functional contribution in artificial and biological networks. Neural computation, 16(9):1887-1915,
2004.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. Advances in neural information processing systems, 25:1097-1105,
2012.
I Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and Sorelle Friedler. Problems
with shapley-value-based explanations as feature importance measures. In International Conference
on Machine Learning, pp. 5491-5500. PMLR, 2020.
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Richard Harold Lindeman. Introduction to bivariate and multivariate analysis. Technical report, 1980.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
8a20a8621978632d76c43dfd28b67767-Paper.pdf.
Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for
tree ensembles. arXiv preprint arXiv:1802.03888, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Luke Merrick and Ankur Taly. The explanation game: Explaining machine learning models using
shapley values. In International Cross-Domain Conference for Machine Learning and Knowledge
Extraction, pp. 17-38. Springer, 2020.
Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper
into neural networks, 2015. URL https://research.googleblog.com/2015/06/
inceptionism-going-deeper-into-neural.html.
W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition to
extract interactions from lstms. In International Conference on Learning Representations, 2018.
11
Under review as a conference paper at ICLR 2022
Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Xu Cheng, Xin Wang, Yiting Chen, Jie
Shi, and Quanshi Zhang. Game-theoretic understanding of adversarially learned features. arXiv
preprint arXiv:2103.07364, 2021.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Lloyd S Shapley. A value for n-person games. Contributions to the Theory of Games, 2(28):307-317,
1953.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Chandan Singh, W James Murdoch, and Bin Yu. Hierarchical interpretations for neural network
predictions. In International Conference on Learning Representations, 2018.
Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. Detecting statistical interactions
with additive groves of trees. In Proceedings of the 25th international conference on Machine
learning, pp. 1000-1007, 2008.
Erik Strumbelj and Igor Kononenko. Explaining prediction models and individual predictions with
feature contributions. Knowledge and information systems, 41(3):647-665, 2014.
Erik Strumbelj, Igor Kononenko, and M Robnik Sikonja. Explaining instance classifications with
interactions of subsets of feature values. Data & Knowledge Engineering, 68(10):886-904, 2009.
Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact of feature attribution
baselines. Distill, 5(1):e22, 2020.
Mukund Sundararajan and Amir Najmi. The many shapley values for model explanation. In
International Conference on Machine Learning, pp. 9269-9278. PMLR, 2020.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-3328,
2017.
Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction index.
In International Conference on Machine Learning, pp. 9259-9268. PMLR, 2020.
Michael Tsang, Dehua Cheng, and Yan Liu. Detecting statistical interactions from neural network
weights. In International Conference on Learning Representations, 2018.
Robert J Weber. Probabilistic values for games. The Shapley Value. Essays in Honor of Lloyd S.
Shapley, pp. 101-119, 1988.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. In International Conference on Machine Learning, 2015.
Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and Quanshi Zhang. Interpreting multivariate
shapley interactions in dnns. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pp. 10877-10886, 2021.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. In International Conference on Learning Representations, 2015.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2921-2929, 2016.
12
Under review as a conference paper at ICLR 2022
A	More related works
Shapley values. The Shapley value (Shapley, 1953) was first proposed in game theory, which was
considered as an unbiased distribution of the overall reward in a game to each player. Lindeman
(1980) and GrGmPing (2007) used the Shapley value to attribute the correlation coefficient of a linear
regression to input features. Strumbelj et al. (2009); Strumbelj & Kononenko (2014) used the Shapley
value to attribute the prediction of a model to input features. Bork et al. (2004) used the Shapley value
to measure importances of protein interactions in large, complex biological interaction networks.
Keinan et al. (2004) employed the Shapley value to measure causal effects in neurophysical models.
Sundararajan et al. (2017) proposed Integrated Gradients based on the AumannShapley(Aumann
& Shapley, 2015) cost-sharing technique. Besides above local explanations, Covert et al. (2020b)
focused on the global interpretability.
In order to compute the Shapley value in deep models efficiently, Lundberg & Lee (2017) proposed
various approximations for Shapley valus in DNNs. Lundberg et al. (2018) further computed the
Shapley value on tree emsembles. Aas et al. (2019) generalized the approximation method in
(Lundberg & Lee, 2017) to the case when features were related to each other. Ancona et al. (2019)
further formulated a polynomial-time approximation of Shapley values for DNNs.
Unlike previous studies, we rethink and formulate baseline values from the perspective of game
theory. We define the absence state of input variables based on the multi-variate interaction, and
further propose a method to learn optimal baseline values.
Interactions. Interactions between input variables of deep models have been widely investigated in
recent years. Sorokina et al. (2008) proposed an approach to detect interactions of input variables in
an additive model. Tsang et al. (2018) measured interactions of weights in a DNN. Murdoch et al.
(2018); Singh et al. (2018); Jin et al. (2019) used the contextual decomposition (CD) technique to
extract variable interactions. Cui et al. (2019) proposed a non-parametric probabilistic method to
measure interactions using a Bayesian neural network. In game theory, Grabisch & Roubens (1999);
Lundberg et al. (2018) proposed and used the Shapley interaction index based on Shapley values.
Janizek et al. (2020) extended the Integrated Gradients method (Sundararajan et al., 2017) to explain
pairwise feature interactions in DNNs. Sundararajan et al. (2020) defined the Shapley-Taylor index
to measure interactions over binary features. In this paper, we use the multi-variate interaction of
input variables based on the Harsanyi dividend (Harsanyi, 1963). Our interaction metric has strong
connections to (Grabisch & Roubens, 1999), but represents elementary interaction patterns in a more
detailed manner, and satisfies the efficiency axiom.
B Extended discussions about the interaction benefit based on
Harsanyi dividend
This section provides extended discussions about the interaction benefit based on the Harsanyi
dividend (Harsanyi, 1963), and provides proofs of axioms that the Harsanyi dividend satisfies.
Given a trained model and the input sample X with n variables N = {1,2, ∙∙∙ , n}, let V(S) denote the
model output when only variables in S are given. Then, V(N) - v(0) represents the overall inference
benefit owing to all input variables in x, w.r.t. the model output without given any variables. In an input
sample, different input variables interact with each other for inference, instead of working individually.
The Harsanyi dividend I(S) measures the benefit to the model output from the interaction between
variables in the subset S ⊆ N. Furthermore, the Harsanyi dividend ensures that the overall benefit
V(N) - v(0) can be decomposed into the sum of I(S) of different subsets {S|S ⊆ N,S = 0}.
V(N )-v⑼=XS⊆N,S=3(S)	⑻
For example, let us consider the S1 = {head} denote the bird head pattern in Figure 6. Patches
inside S1 collaborate with each other to form the head pattern, and to contribute an interaction benefit
I (Si) for the model output. Then, the overall benefit of all patches in the image v(N) - v(0) can be
represented as the sum of various patterns, such as the head pattern S1 with its benefit I(S1), the tail
pattern S2 with the benefit I(S2), the torso pattern S3 with the utility I(S3), etc.
The Harsanyi dividend I(S) is defined to measure the additional benefit from the collaboration of
input variables in S, in comparison with the benefit when they work individually or form smaller
13
Under review as a conference paper at ICLR 2022
1(SI)	+	I(SG	+	/(S3)	+	/(S4)	+ …+	v(0)
U(N)
Figure 6: Sketch for the multi-variate interaction. The model output of a DNN can be decomposed
into interaction benefits I(S) of different patterns.
patterns. Specifically, let v(S) — v(0) denote the overall benefit from all variables in S, then we
remove the marginal benefits owing to collaborations of all smaller subsets L of variables in S, i.e.
{I(L)|L C S,L = 0}.
I (S )=f	v(S) — v(0)	— X	I (L)
、 ’-、一 /
the benefit from all variables in S	L$S,L=°
(9)
B.1	PROOF OF AXIOMS OF THE INTERACTION BENEFIT BASED ON HARSANYI DIVIDEND
In Section 3.2 of the paper, we claim that we extend the linearity, dummy, symmetry axioms of
Shapley values to the interaction benefit based on Harsanyi dividend. Here, we provide details and
proofs for these axioms.
(1)	Linearity property (axiom): If we merge outputs of two models, u(S) = w(S) + v(S), then,
∀S ⊆ N, the interaction Iu(S) w.r.t. the new output U can be decomposed into IU(S) = IW(S) +
Iv (S).
• Proof:
Iu(S)= X (-1)1SHLIu(L)
L⊆S
=X (-1) 1 SH L 1 [w(S) + v(S)]
LCS
=X (-1) 1 SIT L 1 w(L)+ X (-1) 1 SIT L 1 V(L)
LCS	LCS
=Iw (S)+ Iv (S)
(2)	Dummy property: The dummy variable i ∈ N satisfies ∀S ⊆ N \ {i}, v(S U {i}) = v(S) +
v({i}). It means that the variable i has no interactions with other variables, i.e. ∀S ⊆ N \ {i},
I (S U{i})=0.
• Proof:
I(S ∪{i})=	X (-1) 1 S1+1tl 1 V(L)
LCSu{i}
=X (-1) 1 s1+1T L 1 V(L) + X (-1) 1 SH L 1 v(L U {i})
LCS	LCS
=X (-1)1 S*1 L 1 v(L) + X (-1) 1 SH L 1 [v(L) + v({i})]
LCS	LCS
=X (-1)1 S*1 L 1 [-v(L) + V(L)] + X (-1) 1 SIT L 1 v({i})
LCS	LCS
=(X(-1) 1 SH L) V({i})
=[(1 + (-1)) i si ] V({i})
=0
14
Under review as a conference paper at ICLR 2022
(3)	Symmetry property: If input variables i,j ∈ N have same cooperations with other variables
∀S ⊆ N∖{i,j}, v(S U {i}) = v(S U {j}), then they have same interactions with other variables,
∀S ⊆ N ∖ {i,j}, I (S U {i}) = I (S U {j}).
• Proof:
I(S u{i})= X (-1)|S|i1v(L)
LCSu{i}
=X (-1) 1 S1+1i 1 v(L)+ X (-1) 1 SIi |V(L ∪{i})
LCS	LCS
=X (-1)|S1+1TLI V(L)+ X (-1)1SITLI v(L u{j})
LCS	LCS
=X (-1)isi-iliv(l)
L⊆Su{j}
=I(S ∪{j})
(4)	Efficiency property, proved by Harsanyi (1963): The output of a model can be decomposed
into interactions of different subsets of variables, v(N) = v(0) + PSCN s=@ I (S).
• Proof:
right = v(0) + X	I (S)
S⊆N,S≠0
=v(0)+ X	X (-1)isi-iliv(l)
S⊆N,S≠0 LCS
=X X (-1)1SITLIV(L)
SCN LCS
=X X (-1)1KIV(L)	% Let K = S \ L
LCN KCN\L
=X [n∑ 1 (nKL1)(T)1 Ki 1 V(L)
LCN IJ K ∣≠0 ∖	1	1	/
=X [(1 + (-1))n- i L i ] V(L)
LCN
=v(N ) = left
B.2 PROOF OF THE CONNECTION TO SHAPLEY VALUES
Let φ(i) denote the Shapley value (Shapley, 1953) of an input variable i. Then, its Shapley value can
be represented as the weighted sum of the Harsanyi dividend, φ(i) = Pscn∖{w} ∣sf⅛τI(S ∪ {i}). This
connection has been proved in (Harsanyi, 1963).
Φi
Σ
SCN ∖{i}
TS⅛1 i (s u{i})
(10)
• Proof:
right =	X	I SI 1+1 I(S ∪ {i})
SCN∖{i} I I 十
=X + [x (-1)i si+it Li V(L) + X (-1) i si t i L1 v(L u{i})
SCN∖{i} I I 十	LCS	LCS
15
Under review as a conference paper at ICLR 2022
X TS⅛Γ X (-1)1S|i1[v(L U{i}) - v(L)]
S⊆N∖{i} '	' ' ɪ L⊆S
X X	IK(EKL [v(L U{i}) -V(L)]	% Let K = S \ L
LKi} κ⊆N∖L∖{i} ∣k∣ + 因 + 1 * * * S
/n — 1— | L|
X	(	X
LCN∖{i} ∖	k=0
(-1)k	(n- 1 - ∣l∣∖
k + ∣l∣ +1 y k j .
L |L|!(n - 1 - |L|)!「L 「.、、	「、_,
E (	n1—3 [v(L U {i}) - v(L)]
LCN ∖{i}
[v(L U{i}) - v(L)]	% Let k = |K|
% by the property of COmbinitOrial number
φi = left
B.3 PROOF OF CONNECTIONS BETWEEN THE HARSANYI DIVIDEND AND OTHER
GAME-THEORETIC INTERACTION METRICS
In this section, we prove the relationship between the Harsanyi dividend and other game-theoretic
interaction metrics, including the Shapley interaction index (Grabisch & Roubens, 1999) and the
Shapley Taylor interaction index (Sundararajan et al., 2020).
Lemma (Connection to the marginal benefit). ∆vτ(S) = PLCT(-1)1TITLIv(L U S) denotes
the marginal benefit (Grabisch & Roubens, 1999) of variables in T ⊆ N \ S given the environment
S. We have proven that ∆vτ (S) can be decomposed into the sum of interaction benefits of T and
sub-environments of S, i.e. ∆vτ(S) = Ps,⊆s ʃ(T U S0).
• Proof: By the definition of the marginal benefit, we have
∆vτ(S) = X (-1)1TITLIV(L U S)
LCT
=X (-1)1THLI X I(K)
LCT	KCLUS
=X (-1)1TITLIX X I (L0U S0) //since L ∩ S = 0
LCT	L0CL S0CS
=X [x (-1)TIiIX I(L0 U S0)
S0CS LCT	L0 CL
X X X (-1)TIiII(L0 U S0)
S0CS L0CT LCT
_	L⊇L0	,
Σ
S0CS
I (S0 U τ)+ X (X (lT ]LL0|)(-1) i t i—i l i I (L0 u s0))
不丁 L0(T V=i l0 i ∖	' ' /	)
= V------------------------{z-------------------}
.	L0CT	.
-	(	Y
X I (S0 U T)+ X	I (L0 U S0) ∙ X (lT--LL'l)(-1)1 T H L1
S0CS	L0CT	l= IL0 I \ II，
一 \	'------------=0------------").
I(S0 U T)
S0CS
Theorem 1 (Connection to the Shapley interaction index) Given a subset of input variables
T ⊆ N, iShaPIey(T) = PSCN∖T |S1；K⅛⅞1lT|)!δVT(S) denotes the ShaPley interaction in-
16
Under review as a conference paper at ICLR 2022
dex (Grabisch & Roubens, 1999) of T. We have proven that the Shapley interaction index can be repre-
sented as the weighted sum of utilities of interaction patterns, IShaPIey(T) = PSCN∖τ 日，I(S U T).
• Proof:
I Shapley (T)_ £
S⊆N∖T
∣s∣!(∣N ∣-∣s ∣-∣τ∣)! ∆vτ ⑻
(∣N∣-∣T∣ + 1)! T( )
1
|N ∣-∣τ I + 1
|N |-|T I
X	JV	X	∆vτ (S)
m = 0	∖ m S S⊆N∖T
| S | =m
1	IN I-ITI	1
∣N∣ - ∣T∣ +1 X	(INHTI) X	X I(L U T)
.......... m = 0 ' m S S⊆N∖T L⊆S
|S|=m L	1
1
∣n ∣-∣t ∣ + 1
IN I-IT I
XX
L⊆N∖T m=ILI
(INI-ITI) Σ2 1 (L U T)
∖ m S S⊆N∖T
| S | =m
S⊇L
1
∣N ∣-∣t ∣ + 1
IN I-IT I
XX
L⊆N∖T m=ILI
1	1 N ∣-∣L∣-
(™) [ m -∣l∣
∣t ∣)i(L U T)
1
∣N ∣-∣t ∣ + 1
IN I-ILI-IT I
X I(L U T)	X
L⊆N∖T	k=0
1
∣N ∣-∣L∣-∣T ∣
k
sz
wL
Then, we leverage the following properties of combinatorial numbers and the Beta function to simplify
theterm WL= PNr IL T …∙(IN -L 1 -1)∙
(IL| + k )
(i)A property Ofcombinitorial numbers. m ∙ (ɪ) = n ∙ (ɪ-；).
(ii)	The definition of the Beta function. For p,q > 0, the Beta function is defined as B(p, q) =
R1 xp-1(1 — x)1-q dx.
(iii)	Connections between combinitorial numbers and the Beta function.
o When p, q ∈ Z+,we have B(p, q) = /'_口.
q^( p-1 )
θ FOr m,n ∈ Z+ and n > m, we have (m) = m.B(Jm+ι,m).
Hence, we leverage the properties of combinitorial numbers and the Beta function to simplify wl.
IN I-ILI-IT I
WL = X
k=0
1	八 N ∣-∣L∣-∣T ∣
(N+T) [	k
IN I-ILI-IT I
X
k=0
∣L∣∙
(ILI+ k) ∙ B(IN∣ - ∣l∣ - ∣t∣ - k + 1, ∣LI + k)
IN I-ILI-IT I
X
k=0
I N ∣-IL ∣-∣ TI ) ∙ b( IN ∣- ∣ L I-IT ∣ - k +1,∣ L ∣ + k)
IN I-ILI-IT I
+ X k ∙
k=0
I N ∣-I L ∣ - ∣ T ∣ ) ∙ b( IN ∣- ∣ L I-IT I-k +1,∣ L ∣ + k)
…①
…②
Then, we solve ① and ② respectively. For ①，we have
17
Under review as a conference paper at ICLR 2022
①
1 ∖L∖ 1N1 岂1 t i ∕∖N∖-∖L∖-∖T∖! ∙ Xi NH LH Ti -k ∙ (1 - x) i L-dx
'0	k=0	∖	)
[1 ∖L∖∙
Jo
I N I X I T I ∕∖N ∖ - ∖l∖ - ∖t ∖! ∙ X ∣ NI - IL I - ITI -k ∙ (1 - x)k
k=0	∖	)
∙(1 - X)ILIT dx
I
■{z
=1
✓
/ ∖L∖ ∙ (1 - X)ILIT dx = 1
Jo
For ②，we have
②=I I E I I (∖N ∖ - ∖L∖ - ∖T∖) ∕∖N ∖ - 2--} ∖-) ∙ B(∖N ∖-∖L∖-∖T ∖- k +1, ∖L∖ + k
i N i - i l i - i T i -1 ∖ ∖ n ∖ — ∖L ∖ — ∖T ∖ — 1 ʌ	/
=(∖N∖-∖L∖-∖T∖)	E	1,	∙ B(∖N∖-∖L∖-∖T∖- k', ∖L∖ + k' + 1
=(∖N∖-∖L∖-∖T∖) f
o
k0 = 0	∖
，1 I NI - IL I - ITI -1
=(∖N∖-∖L∖-∖T∖) [1
o
Σ
k0=o	∖
N - L - T -1
E
k0=o
∖n ∖-∖l∖-∖t ∖- 1∖ ∙ X 1 n 1 - 1 L1 - 1 t 1 -k0-1 ∙ (1 - x) 1 L1 +k° dx
k' I
∖n ∖ - ∖l∖ - ∖t ∖ - 1) ∙ x I NI - IL I - ITI -k0-1 ∙ (1 - x)
k0
∙(1 - x) L dx
I
^^≡{z
=1
=(∖N∖-∖L∖-∖T∖) f 1(1 - x) i Li dx = ∖N∖ -LL- ∖t∖
Jo	∖l∖ + 1
✓
Hence, we have
WL =① + @ = 1 +
∖N∖ - ∖L∖ - ∖T∖
∖L∖ + 1
∖N ∖-∖T∖ + 1
∖L∖ + 1
TherefOre,weprovedthat IShaaey(T) = W-T∣ + ι PL⊆N\T WL ML∪ T) = PLCN\T ∣⅛i I(LU
T).
Theorem 2 (Connection to the Shapley Taylor interaction index) Given a subset of input variables
T ⊆ N, let IShaPIey-Taylor(τ) denote the Shapley Taylor interaction index (Sundararajan et al., 2020)
of order k for T. We have proven that the Shapley taylor interaction index can be represented as the
weighted sum ofinteraction utilities, i.e. IShaPley-TaylOr(T) = I(T) if |T| < k; IShaPley-TaylOr(T)=
PSCN\T (∣S+fc)-1I (S U T) if |T | = k;and I ShaPley-TaylOr(T )=0 if |T | > k.
• Proof: By the definition of the Shapley Taylor interaction index,
{Δvt(0)	if ∖T∖ < k
PNT Ps⊆N∖T (TN⅛)∆VT(S) if ∖T∖ = k
0	1S|	if ∖T∖ > k
When |T| < k, by the definition of the interaction utility in Equation (3), we have
IShaPIey-Tayta∙(k)(τ) = Δvt (0) = E (-1严1 -1 L 1 ∙ V(L)= I(T).
L⊆T
18
Under review as a conference paper at ICLR 2022
When |T| = k, we have
「Shapley-Taylor(k) ∕τ∖ _ k
(T )=同 S⊆N∖T
1
(Ik)
* ∆vτ (S)
k
|N|
∣N ∣-k
X X
m = 0 S⊆N∖T
| S | =m
1
(IN∣-1
< ∣s∣ ∙
* ∆vτ (S)
M X XWly	XI(L U T)
1	1 m = 0 S⊆N∖T 卜 ∣S∣ L L⊆S
|S|=m	L
N	∣N ∣-k
⅛ X X	τU-η	X I(L U T)
1	1 L⊆N∖T m=∣L∣ ∖ ∣S∣ S S⊆N∖T
| S | =m
S⊇L
|N |-k
XX
L⊆N∖T m=∣L∣
k
囱
1 I |N|-|L|- k]KL ∪ T)
F I m-|L| 尸L U T)
∣ N ∣ - ∣ L ∣-k
X I(L U T)	X
LCN∖T	m=0
wL
Just like the proof of Theorem 1, we leverage the properties of Combinitorial numbers and the Beta
function to simplify wL.
WL
∣ N ∣ -1 L ∣-k
X
m = 0
|N|-|L|- k
m
∣ ∣ x∣	(INITL|-k) ∙ (|L| + m)∙ B(|N| - |L| - m, |L| + m)
m=0	m
N - L -k
X |L|・ | |-m|-	∙ B(|N |-|L|- m, |L| + m)	…①
m = 0	∖	)
N - L -k
+ X m ∙ | |-| |-	∙ B(|N | - |L| - m, |L| + m)	…②
m=0	m m ) v	/
Then, we solve (T) and ② respectively. For (T), we have
①=∕1
0
∣L|∙
∣ N ∣ x∣ k (|NITL| - k) ∙ X ∣NHL∣-m-1 ∙ (1 - x)∣L∣+m-1 dx
m=0	∖	m	)
∣L|∙
∣N∣X∣ k (IN| - |L| - k! ∙ x∣N∣-∣L∣-m-k . (1 - x)m
m = 0	∖ m )
-{z
=1
∙xk-1 ∙ (1 - x) ∣ L∣ -1 dx
∕1
0
|L| ∙ xk-1 ∙ (1 - x)∣ L∣ -1 dx = |L| ∙ B(k, |L|)
(∣ l ∣+k-1,
k-1
k
囱
Z1
0
I
1
✓
For (2 , we have
19
Under review as a conference paper at ICLR 2022
②=1 X1 (∣n∣-∣l∣- k) ∙(∣n∣-m--k - 1) ∙ b(∣n∣-∣l∣- m,∣L∣ + m)
|N |-|L|-k-1	/	∖
=X	(∣N∣-∣L∣- k) ∙ ∣ ITm: - 1 ∙ b(∣N∣-∣L∣- m' - 1, ∣L∣ + m' + 1
m0 = 0
∕1(∣N|-|L|- k)
0
/ 1(∣N∣-∣L∣- k)
0
|N |-|L|-k-1 /
X
m0 =0	'
"|N |-|L|-k-1
X
m0 = 0
∣N∣ - ∣L∣ - k - 1) ∙ x|N|-|L|-m0-2 ∙(1 - x)|L|+m'
∣N∣ - ∣L∣ - k - 1) ∙ x|N|-|L|-m0-k-1 . (1 - χ)
/
dx
m0
∙xk-1 ∙ (1 - X)ILI dx
|
■{z
=1
✓
∕1(∣N∣ - ∣L∣ - k) ∙ Xk-1 ∙ (1 - X)ILI dx = (∣N∣ - ∣L∣ - k) ∙ B(k, ∣L∣ + 1)
0
∣N ∣-∣L∣- k
(∣l∣ + 1)。L-Lk)
Hence, we have
WL =(D + @
1	,	∣N ∣-∣L∣- k
(|Lk-T)	(∣l∣ + 1)CL⅛fc)
∣L∣!∙(k - 1)! + ∣N∣-∣L∣- k (∣L∣ + 1)! ∙ (k - 1)!
(∣L∣ + k — 1)!	∣L∣ + 1	(∣L∣ + k)!
∣L∣! ∙ (k - 1)!	∣N∣ - ∣L∣ - k ∣L∣! ∙ (k - 1)!
(∣L∣ + k — 1)!	∣L∣ + k (∣L∣ + k — 1)!
•	∣N∣-∣L∣- k一
.+	∣L∣ + k
∣L∣!∙(k-1)!
(∣L∣ + k - 1)!
∣N∣	∣L∣! ∙ (k - 1)!
7--：----- ∙—---：---------
∣L∣ + k (∣L∣ + k — 1)!
∣N∣	∣L∣!∙ k!
k (∣L∣ + k)!
1
∣N ∣
k
(ILk+k)
Therefore, We proved that when |T| = k, IShaPley-Tayl。")=备 PLCN\t WL ∙ I(L U T)
⅛ PLCN\t jf ∙大∙ I(L U T) = PLCN\t ("k) -1I(L U T).
C Discussion about baseline values based on the conditional
DISTRIBUTION AND THE MARGINAL DISTRIBUTIONS
This section provides more discussions about the baseline values based on the conditional distribution
and the marginal distribution.
•	Baseline values based on the conditional distribution. Instead of fixing baseline values as
constants, some studies use varying baseline values, which are determined temporarily by the context
S in a specific sample x, to compute v(S∣x) given x. Some methods (Covert et al., 2020b; Frye et al.,
2021) define v(S∣x) by modeling the conditional distribution of variable values in S = N \ S given
the context S, i.e. v(S∣x)=旧?(Μ|方,)[f (XS U xg)]. However, these methods apply varying baseline
values based on the dependency between input variables, which do not faithfully satisfy the linearity
axiom and the nullity axiom of the Shapley value from some aspects (Sundararajan & Najmi, 2020).
20
Under review as a conference paper at ICLR 2022
Moreover, these methods compute a specific conditional distributionp(x0|xS) for each of 2n contexts
S with a very high computational cost.
The nullity axiom. The nullity axiom of the Shapley value ensures that for a null player i s.t.
∀S ⊆ N \ {i}, v(S ∪ {i}) = v(S), its Shapley value is φi = 0. This axiom means that if the
model is insensitive to an input variable, then the variable should have zero attributions. It seems
that baseline values based on the conditional distribution satisfy this axiom mathematically, because
φi = ES⊆N∖{i}[v(S ∪ {i}) - V(S)] = 0.
However, as discussed in (Sundararajan & Najmi, 2020), the conditional baseline does not faithfully
satisfy the natural meaning of the nullity axiom. Let us consider an example where the model
f(x) = x1 + x3 and each input sample x = [x1, x2, x3] contains three variables (N = {1, 2, 3}).
Each input variable xi ∈ {0, 1} is a binary variable. In this case, because the variable x2 is not
referenced in the model, the Shapley value of x2 is supposed to be φ2 = 0. We assume that
xι 〜Bernoulli(0.5), x3 〜Bernoulli(0.5), and x2 is totally determined by x3, i.e. x2 = x3 in all
samples. Based on such distribution of input variables, for the input sample x = [1, 1, 1], we have
v(0) = 0.25f ([x； = 0,x2 = 0, x3 = 0) + 0.25f ([x； = 0,x2 = 1, x3 = 1)
+ 0.25f ([x01 = 1, x02 = 0, x03 = 0) + 0.25f ([x01	=	1,	x02	=	1,	x03	=	1) = 1
v({1}) = 0.5f ([x；	=	1, x02	=	0, x03 = 0])	+ 0.5f ([x；	=	1,	x02	=	1,	x03	=	1])	=	1.5
v({2}) = 0.5f ([x0；	=	0, x2	=	1, x03 = 1])	+ 0.5f ([x0；	=	1,	x2	=	1,	x03	=	1])	=	1.5
v({3}) = 0.5f ([x0；	=	0, x02	=	1, x3 = 1])	+ 0.5f ([x0；	=	1,	x02	=	1,	x03	=	1])	=	1.5
v({1, 2})	=	f ([x； = 1, x2	= 1, x03 = x2	=	1) =	2
v({1, 3})	=	f ([x； = 1, x02	= x3 = 1, x3	=	1) =	2
v({2, 3})	=	0.5f ([x0； = 0,	x2 = 1, x3 =	1)	+ 0.5f ([x0； = 1, x2 = 1, x3 = 1) = 1.5
v({1, 2, 3})	= f ([x； = 1, x2 = 1, x3 = 1])	= 2
Thus, the Shapley value of the variable x2 is
Φ2 = 1[v({2}) - v(0)] + 6[v({1, 2}) - v({1}) + v({2, 3}) - v({3})] + ∣[v({1, 2, 3}) - v({1, 3})]
1
3
1
4
. 1 + 1 ∙ (0.5 + 0) + 1 ∙ 0
26	3
6= 0
Obviously, the computed Shapley value of the variables x2 based on the conditional baseline is not
0, which contradicts with the fact that the model does not use the variable x2 in computation. This
phenomenon has also been discussed in (Sundararajan & Najmi, 2020).
The linearity axiom. The linearity axiom of the Shapley value means that when we merge two models
v, w into a new model u, then the Shapley values of variables in the two old models can also be
merged. The baseline value based on the conditional distribution does not naturally satisfy this axiom.
It is because the conditional distribution of input variables may be different in the two old models.
There is an extremeness that some input variables in the old model v do not exist in the other old
model w. In this case, the conditional distribution of variables in the three models u, v, w will be
dramatically different. Please refer to (Sundararajan et al., 2017) for more detailed discussions.
The problem with the computational cost. Besides the above problem with axioms, the conditional
baseline also suffers from the large computational cost, and is not suitable for continuous variables.
In (Covert et al., 2020b), v(S|x) = Ep(χ,∣χs)[f (XS t xg)] is approximately computed as V(S|x)=
Eχ0∈Ω:Xg=xs [f (XS t xg)], where Ω denotes the set of all samples in the dataset. Unfortunately, for
high-dimensional inputs with continuous variables, it is practically impossible to find inputs x0 where
x0S = xS. Frye et al. (2021) propose to use generative models to learn the distribution p(x0|xS) for a
specific context S. However, there are 2n different contexts S, leading to a very high computational
cost.
•	Baseline values based on the marginal distribution. Based on the assumption that input variables
are independent with each other, Lundberg & Lee (2017) simplify the above conditional baseline
to the marginal baseline, i.e. v(S∣x) = Ep(x，)[f (XS t xg)]. First, such assumption of variable
21
Under review as a conference paper at ICLR 2022
independence is not true in real applications, thereby hurting the trustworthiness of the computed
Shapley values. Second, the real distribution of input samples p(x) is unknown. Thus, v(S|x) is
approximated by Eχ,∈Ω [f (XS t xg)] in (Lundberg & Lee, 2017). In this case, the computational cost
is also very large because we need to enumerate all samples in the dataset.
D Multi- order Shapley values and marginal benefits
In Section 3.3 of the paper, we claim that the Shapley value φi can be decomposed into the sum of
Shapley values of different orders φi(m), and the sum of marginal benefits of different orders ∆vi(S).
Furthermore, the multi-order Shapley values and marginal benefits can be re-written as the sum of
interaction benefits. This section provides proofs for the above claims.
First, we have proven the following decomposition of the Shapley value.
n-1	n-1
φi = n X φim) = n X ES⊆N\{i},|S|=m∆Vi(S)	(II)
m=0	m=0
where the Shapley value of m-order φim) def Es⊆n∖{i}∣s∣=m [v(S ∪ {i}) - v(S)], and the marginal
benefit ∆vi (S) d=ef v(S ∪ {i}) - v(S).
• Proof :
φi = X ISMn -1一|S|!) [v(S ∪ {i}) - V(S)]
S⊆N
n-1
XX
m=0 S⊆N,∣S∣ =
∣S∣!(n - 1-∣S∣!)
n!
[v(S ∪ {i}) - v(S)]
n-1
1X X
m=0 S⊆N,∣S∣=m
IS"n-'-J" [v(S " — v(S)]
m
1	n-1
n E ES⊆N,∣S∣=m [v(S ∪ {i}) - V(S)]
m=0
n-1
1X φ(m)
m=0
1 n-1
n Σ2 ES⊆N\{i},|S|=m∆Vi(S)
m=0
Connection between multi-variate interactions and multi-order marginal benefits. Equa-
tion (6) in the main paper shows that the m-order marginal benefit can be decomposed as the
sum of multi-variate interaction benefits. In the supplementary material, this section provides the
proof for such decomposition.
∆vi(S) = XI(L∪{i})	(12)
L⊆S
• Proof :
right = X I(L ∪ {i})
L⊆S
=X [x (-1)lLl + 1TKlV(K) + X (-1)lLl-lKlv(K ∪ {i})
L⊆S K⊆L	K⊆L
= X X (-1)|L|-|K| [V(K ∪ {i}) - V(K)]
L⊆S K⊆L
22
Under review as a conference paper at ICLR 2022
E £(-i严l-lKl∆Vi(κ)
L⊆S K⊆L
-—
(-1)|Pl∆vi(K)	% Let P = L \ K
K⊆S P⊆S∖K
lKl)(-1)p) ∆vi(K)	% LetP =|P|
X (1 + (-1)|S|-|K|)i ∆vi(K)	%Letp=|P|
K⊆S
∑0 ∙ ∆vi(κ )+£
K(S	K=S
|S|-|K|
X
p=0
∆vi(K)
∆vi (S) = left
Connection between multi-order interactions and multi-order Shapley values. Similarly,
Equation (6) in the main paper also shows that the m-order Shapley value can also be decomposed as
the sum of interaction benefits. This section provides the proof for such decomposition.
I(L ∪ {i})
L⊆S
(13)
• Proof :
φ(m) = ES⊆N ∖{i}
|S|=m
ES⊆N,∣S∣ =m∆vi(S)	
ES⊆N,∣S∣=m	XI(L∪{i}) L⊆S
ES⊆N,∣S∣=m	XI(L∪{i}) L⊆S
E More experimental results and details
E.1 Discussion about effects of incorrect baseline values.
This section proves that the incorrect setting of baseline values makes a model/function consisting
of high-order interaction patterns be mistakenly explained as a mixture of low-order and high-order
interaction patterns. This is mentioned in Section 3.3 of the paper. To show this phenomenon, we
compare interaction patterns computed using ground-truth baseline values and incorrect baseline
values in Table 6, and the results verify our conclusion. We find that when models/functions contain
complex collaborations between multiple variables (i.e. high-order interaction patterns), incorrect
baseline values usually generate fewer high-order interaction patterns and more low-order interaction
patterns than ground-truth baseline values. In other words, the model/function is explained as
massive low-order interaction patterns. In comparison, ground-truth baseline values lead to sparse
and high-order salient patterns.
We can understand the effects of incorrect baseline values as follows. When we use ground-truth
baseline values, the absence of any variable will inactivate the interaction pattern. However, when we
use incorrect baseline values, replacing an input variable i with its baseline value cannot completely
remove the original information, or may bring in new information to activate new abnormal patterns.
In other words, the variable i still provides meaningful information to the output. Therefore, incorrect
baseline values cannot represent the absence state of variables, thus damaging the trustworthiness of
the computed Shapley values.
23
Under review as a conference paper at ICLR 2022
Table 6: Comparison between ground-truth baseline values and incorrect baseline values. The last
column shows ratios of multi-variate interaction patterns of different orders rm
Es⊆N,∣S∣=m II(S)I
Ps⊆N,S=0 II(S)I
We consider interactions of input samples that activate interaction patterns. We find that When
models/fUnctions contain a single complex collaborations between multiple variables (i.e. high-
order interaction patterns), incorrect baseline values usually generate a mixture of many low-order
interaction patterns. In comparison, ground-truth baseline values lead to SParSe and high-order
interaction patterns.
Functions (∀i ∈ N, i ∈ {0,1})
f (x) = X1X2X3X4X5
X = [1,1,1,1,1]
Baseline values b
ground truth: b* = [0,0,0,0, 0]
incorrect: b⑴=[0.5,0.5,0.5, 0.5,0.5]
incorrect: b(2) = [0.1,0.2,0.6, 0.0,0.1]
incorrect: b⑶=[0.7,0.1,0.3, 0.5,0.1]
Ratios r
f (x) = sigmoid (5χ1X2X3 +
5X4 - 7.5)
X = [1,1,1,1]
f(χ) = X1(X2 + X3 - x4)3
x = [1,1,1, 0]
ground truth: b* = [0, 0, 0, 0]
incorrect: b⑴=[0.5,0.5, 0.5, 0.5]
incorrect: b(2) = [0.6,0.4, 0.7, 0.3]
incorrect: b⑶=[0.3,0.6, 0.5, 0.8]
ground truth: b* = [0, 0, 0,1]
in correct: b ⑴=[0.5,0.5, 0.5, 0.5]
incorrect: b(2) = [0.2,0.3, 0.6, 0.1]
incorrect: b⑶=[1.0,0.3,1.0, 0.1]
r
b*
b(i)
b。)
b(，)
0.5
0
b* b(l) b(2)	b(3)
m
1	5
r
1
E.2 THE VISUALIZATION METHOD IN FIGURE 2.
This section provides experimental details of the visualization of Figure 2 in the paper. In Figure 2,
the pixel value in the heatmap is computed as p(j∣i) = Es∈ω[1 j∈s], Where i ∈ N and Ω denotes
the set of patterns S whose values ∣∆vi(S)∣ are relatively large among all S ⊆ N ∖{i}. ∆vi(S) is
defined in Equation (6).
Due to the exponential computational cost of enumerating all interaction patterns to obtain the salient
ones in Ω, we used the following greedy strategy for approximation. We first randomly sample an
input variable i and a context S ⊆ N \ {i}, and compute the corresponding value of ∣∆vi(S)∣. We
sample multiple times and find the pair of (i, S) that yields the largest value of ∣∆vi(S)|. Then, we
add/remove some variables to/from the context S to obtain a larger ∣∆vi(S)|. Alg. 1 shows the
pseudo-code of this algorithm.
E.3 OTHER POTENTIAL SETTINGS OF v(S).
In the computation of Shapley values, people usually use different settings of v(S), although the
settings of v(S) do not affect the applicability of our method. Our method of formulating baseline
values is applicable to various settings of v(S). Lundberg & Lee (2017) directly set v(S) =
p(ytruth |mask(X, S)). Covert et al. (2020b) used the cross-entropy loss as v(S). In this paper,
we use v(S) = log 力严mm标Sa) in LShaPley. Besides, we use ∣∆vi(S)∣ = ∣∣h(mask(x,S ∪
{i}) - h(mask(X, S)k1 in Lmarginal on the MNIST dataset to boost the optimization efficiency, where
h(mask(X, S)) denotes the intermediate-layer feature. It is because h(mask(X, S ∪ {i}) makes the
optimization of Lmarginal receive gradients from all dimensions of the feature.
E.4 Experimental res ults on the UCI South German Credit dataset.
This section provides experimental results on the UCI South German Credit dataset (Asuncion &
Newman, 2007). Figure 7 shows the learned baseline values by our method, and Figure 8 compares
Shapley values computed using different baseline values. Just like results on the UCI Census Income
dataset, attributions (Shapley values) generated by our learned baseline values are similar to results
of the varying baseline values in SHAP and SAGE. However, the zero/mean baseline values usually
generated conflicting results with all other methods.
24
Under review as a conference paper at ICLR 2022
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
Algorithm 1: The approximate algorithm to interaction patterns whose ∆vi (S) are large
Input: The set of input variables N, the reward function (model) v(∙), the sampling number of
contexts t, the sampling number of noisy variables m, the convergence threshold , the
number kmax
Output: a specific input variable i, a set Ω consisting of kmx interaction patterns
for all i ∈ N do
Randomly sample a set of subsets {P1,P2,... ,Pt }⊆ 2N\{i}
Si = arg maxp ∣∆vi(P)|
end
imax, Smax = arg max(i,Si) [δVi(Si)I
Initialize Ω = 0
Randomly sample a set of subsets {M1 , M2 , . . . , Mkmax } ⊆
∀k ∈ {1, 2, . . . , kmax}, |Mk| = m
for k from 1 to kmax do
Let S(k) = Smax ∪ Mk ,d(k) = ∣∆vimιx (S(k))∣
Initial Sm(kax) = S(k), d(mka)x = d(k)
while True do
Let dt(mkp) = d(mka)x
for all j ∈ N \{i} do
if j ∈ Sm(kax) then
if∣∆vimx(Sma \{j})| >d(maχ then
SiS — Smkx \ {j}
dmax — | Avimax (Smax \ {j})|
end
else
if ∣∆vimx (S像 ∪ {j})∣ > d(a then
Sma) — Smkx ∪ {j}
dmax— | Avimax (Smax ∪ {j})|
end
end
end
if |d(mka)x - dt(mkp)| < then
I Break
end
end
Ω 一 Ω ∪{Smkx}
end
2N \Smax \{imax} with the size m, i.e.
return imax, Ω
25
Under review as a conference paper at ICLR 2022
40
Baseline values
learned by our methods
30
20
10
0
Using LShaPley (zero-init)
Using LShaPIey (mean-init)
Using Lmarginai (zero-init)
Using Lmarginai (mean-init)
“3
Figure 7: The learned baseline values on the UCI South German Credit dataset.
Values of
the input SamPle
Zero
baseline
values
0
SAGE
0
Mean
baseline
values
0
Ours
IShaPIey
zero-init
」 一
0
Shapley value
Values of
the input SamPIe
SHAP
0
Ours
L
"marginal
zero-init
-
Foreign-worker
Telephone
People-liable
Job
Number-credits
Housing
Other-installment-plans
Age
Property
Present-residence
Other-debtors
Personal-status-sex
Installment-rate
Employment-duration
Savings
Amount (X 0.01)
Purpose
Credit-history
Duration
Status
400	800	1200
Zero	Mean
baseline	baseline	SHAP
values	values
0	0	0
Ours
SAGE	iShapley
zero-init
0
0
Shapley value
Ours
LmarginaI
zero-init
0
Figure 8: Shapley values computed with different baseline values on the UCI South German Credit
dataset.
E.5 S tab ility of the estimated Shapley values
In this section, we conducted an experiment to evaluate the stability of the estimated Shapley
values via the sampling-based approximation method (Castro et al., 2009). Given a certain input
sample, we fixed the sampling number and repeatedly computed the Shapley value multiple times.
Then, we measured the instability of the estimated Shapley value φi for a certain input variable i,
which indicated that whether we could obtain similar Shapley values considering the randomness
in each sampling process. The instability was computed as
Eu,v;u = v ∣φiU)-φ(V)
Ew ∣φ(W)I
∣ , where φi(u)
denote
the estimated Shapley value in the u-th time. Then, we computed the average instability value over
Shapley values of all variables in 20 input samples. We used MLPs learned from the UCI South
German Credit dataset (Asuncion & Newman, 2007) and the UCI Census Income dataset. We
computed the instability when we used different sampling numbers. As Figure 9 shows, on both
datasets, the instability of the estimated Shapley values decreased along with the increase of the
sampling number. We found that when the sampling number was larger than 1000, the instability of
the estimated Shapley values was near or lower than 0.1, which meant the stable computed Shapley
values. Therefore, we set the sampling number to 1000 in all other experiments in this paper.
E.6 Discussion about the setting of ground-truth baseline values .
This section discusses the ground truth of baseline values of synthetic functions in Section 4 of the
paper. In order to verify the correctness of the learned baseline values, we conducted experiments on
synthetic functions with ground-truth baseline values. We randomly generated 100 functions whose
interaction patterns and ground truth of baseline values could be easily determined. As Table 7 shows,
26
Under review as a conference paper at ICLR 2022
Instability
0.16 η
0.12 -
0.08 -
0.04 -
-----census dataset
credit dataset
0	------1-----1----1-----1-----1-----1
0	500	1000	1500	2000	2500	# of sampling
Figure 9: Instability of the Shapley values approx-
imated with different sampling numbers. The in-
stability of the approximated Shapley values de-
creased along with the increase of the sampling
number.
Table 7: Examples of synthetic functions and their ground-truth baseline values.
Functions (∀i ∈ N,xi ∈{0,1})
—0.185xi(X2 + Xe)2432 — X4X5X6X7X8X9X10X11
-sigmoid(-4x1 - 4x2 - 4x3 + 2) - 0.011x4(x5 + x6 + x7 + x8 + x9 + x10 + x11)2.341
0.172x1x2x3(x4 + x5)2.543 — 0.171x6(x7 + x8)2.545 — 0.093(x9 + x10 + x11)2.157
—sigmoid(5x1 + 5x2x3x4 — 7.5) — sigmoid(—8x5x6 + 8x7 + 4) + x8x9x10x11
—x1x2x3 + 0.156(x4 + x5 + x6)1.693 — x7x8x9x10
sigmoid(—3x1x2x3 + 1.5) + 0.197x4x5(x6 + x7 + x8)1.48 + x9x10x11
—sigmoid(5x1 — 5x2x3 + 2.5) — sigmoid(3x4x5x6x7 — 1.5) — 0.365x8 (x9 + x10)1.453
sigmoid(4x1 — 4x2 — 4x3 + 6) — sigmoid(—6x4x5x6x7 + 3) — sigmoid(6x8 — 6x9x10 + 3)
—x1x2x3 + 0.205x4(x5 + x6)2.289 — 0.115(x7 + x8 + x9)1.969 + x10x11x12
—sigmoid(—3x1x2x3 — 3x4x5 + 4.5) — sigmoid(3x6 + 3x7 + 3x8 — 1.5) + x9x10x11
—sigmoid(8x1x2 — 8x3 — 8x4 — 4) — 0.041(x5 + x6 + x7 + x8)2.298 — sigmoid(7x9x10 + 7x11 — 10.5)
—sigmoid(4x1 — 4x2 + 4x3 — 6) — x4x5x6x7 — x8x9x10x11
sigmoid(3x1x2 + 3x3 — 3x4 — 3x5 — 3x6 — 4.5) + sigmoid(6x7x8 + 6x9x10x11 — 9)
—sigmoid(—7x1x2 — 7x3x4 + 10.5) + sigmoid(—5x5 + 5x6 — 5x7x8 — 5x9 + 12.5) + x10x11x12
sigmoid(—6x1 + 6x2 + 6x3 + 3) + 0.229x4x5x6(x7 + x8)2.124 + 0.070x9(x10 + x11 + x12)2.418
x1x2x3x4 — sigmoid(—6x5x6 — 6x7 + 9) + x8x9x10x11
sigmoid(—3x1 + 3x2 — 3x3x4 — 3x5x6 + 7.5) — 0.174x7(x8 + x9 + x10)1.594
—0.34x1(x2 + x3)1.557 — sigmoid(5x4x5x6 — 5x7 — 5x8 — 5x9 — 5x10 — 5x11 — 2.5)
—sigmoid(—8x1x2 + 8x3 + 4) — x4x5x6x7 + 0.457x8(x9 + x10)1.13
sigmoid (—6x1 + 6x2 — 6x3 — 3)+ sigmoid (—6x4X5 — 6x6 + 6x7 +9)+ sigmoid (4x8X9 — 4xιo — 2)
The ground truth of baseline values
b = 0 for i ∈{1,...,11}
健=1 for i ∈{1,2,3},健=0 for i ∈{4,...,11}
b = 0 for i ∈{1,...,11}
b↑ = 1 for i = 7,b↑ =0 for i ∈ {1, 2, 3,4,5,6,8, 9,10,11}
疗=0 for i ∈{1,...,10}
健=0 for i ∈{1,...,11]
q=1 for i = 1,蚌=0 for i ∈ {2,..., 10}
球=1 for i ∈{1, 8}M = 0 for i ∈{2,3,4,5, 6, 7, 9,10}
b = 0 for i ∈{1,..., 12}
b = 1 for i ∈{6,7,8},汇=0 for i ∈{1,..., 5, 9,10,11}
b = 1 for i ∈{3,4},b÷ = 0 for i ∈{1, 2, 5, 6,..., 11}
健=1 for i = 2,健=0 for i ∈ {1,3,4,..., 11}
b =1 for i ∈{4, 5,6}M = 0 for i ∈{1,2, 3, 7, 8,9,10,11}
b = 1 for i = 6,健=0 for i ∈{1, 2,3,4,5,7, 8,..., 12}
b = 1 for i ∈{2,3},汇=0 for i ∈{1, 4, 5,..., 12}
健=0 for i ∈{1,..., 11}
健=1 for i = 2,健=0 for i ∈ {1,3,4,..., 10}
健=1 for i ∈{7,..., 11},健=0 for i ∈ {1,..., 6}
健=1 for i = 3,健=0 for i ∈ {1, 2, 4, 5,6,7,8,9,10}
b =1 for i ∈{1, 3,7,10}M = 0 for i ∈{2,4,5,6, 8, 9}
the generated functions were composed of addition, subtraction, multiplication, exponentiation, and
sigmoid operations.
The ground truth of baseline values in these functions was determined based on interaction patterns
between input variables. In order to represent absence states of variables, baseline values should
activate as few salient patterns as possible, where activation states of interaction patterns were
considered as the most infrequent state. Thus, we first identified the activation states of interaction
patterns of variables, and the ground-truth of baseline values were set as values that inactivated
interaction patterns under different masks. We took the following examples to discuss the setting of
ground-truth baseline values (in the following examples, ∀i ∈ N, xi ∈ {0,1} and b ∈ {0,1}).
• f (x) = x 1x2x3 + sigmoid(x4 + x5 — 0.5) ….Let us just focus on the term of x1x2x3 in f (x).
The activation state of this interaction pattern is x1x2x3 = 1 when ∀i ∈ {1, 2, 3}, xi = 1. In order to
inactivate the interaction pattern, we set ∀i ∈ {1,2,3}, b* * = 0.
• f (x) = —x1x2x3 + (x4 + x5)3 + ….Let us just focus on the term of —x1x2x3 in f (x). The
activation state of this interaction pattern is —x1x2x3 = —1 when ∀i ∈ {1, 2, 3}, xi = 1. In order to
inactivate the interaction pattern, we set ∀i ∈ {1,2,3}, b* = 0.
• f (x) = (xι + x2 — x3)3 + ….Let us just focus on the term of (xι + x2 — x3)3 in f (x). The
activation state of this interaction pattern is (x1 + x2 — x3)3 = 8 when x1 = x2 = 1, x3 = 0. In
order to inactivate the interaction pattern under different masks, we set b； = b2 = 0, b3 = 1∙
• f (x) = Sigmoid(3xιx — 3x3 —1.5) + ….Let us just focus on the term of Sigmoid(3xιx2 — 3x3 —
1.5) in f (x). In this case, x1, x2, x3 form a salient interaction pattern because sigmoid(3x1x2 —
3x3 — 1.5) > 0.5 only if x1 = x2 = 1 and x3 = 0. Thus, in order to inactivate interaction patterns,
ground-truth baseline values are set to b1； = b；2 = 0, b3； = 1.
Ground-truth baseline values of functions in (Tsang et al., 2018). This section provides more
details about ground-truth baseline values of functions proposed in (Tsang et al., 2018). We evaluated
the correctness of the learned baseline values using functions proposed in (Tsang et al., 2018). Among
all the 92 input variables in these functions, the ground truth of 61 variables could be determined and
are reported in Table 8. Note that some variables cannot be 0 or 1 (e.g. x8 cannot be zero in the first
function), and we set ∀i ∈ N, xi ∈ {0.001, 0.999} for variables in these functions instead. Similarly,
we set the ground truth of baseline values ∀i ∈ N, bi； ∈ {0.001, 0.999}. Some variables did not
collaborate/interact with other variables (e.g. x4 in the first function), thereby having no interaction
patterns. We did not assign ground-truth baseline values for these individual variables, and these
variables are not used for evaluation. Some variables formed more than one interaction pattern with
other variables, and had different ground-truth baseline values w.r.t. different patterns. In this case,
27
Under review as a conference paper at ICLR 2022
Table 8: Functions in (Tsang et al., 2018) and their ground-truth baseline values.
Functions (∀i ∈ N, xi ∈ {0.001, 0.999})
Πx1x2√2X3 -SinT (x4) + Iog(χ3 + χ5) - xχ90 J∣8 - X2χ7
∏x1x2 p2∣X3∣ - sin-1 (0.5x4) + log(∣x3 + X51 + 1) + τ+⅛看% - χ2χ7
exp ∣xι - X2∣ + ∣X2X3∣ - X2lx4l + log(x4 + χ5 + χ7 + x8) + X9 + 1+X2-
exp ∣xι - X2∣ + ∣X2X3∣ - x2lx4l + (x1X4)2 + log(x4 + X5 + X2 + x8) + X9 + 1+X^
ι+x2,x2+x2 + √eχp(χ4 + χ5) + ∣X6 + χ7∣ + X8X9X10
exp(∣XlX2∣ + 1) - exp(∣X3 + X4∣ + 1)+ cos(x5 + X6 - X8) + ∖JX + X2 + X20
(arctan(XI) + arctan(XZ))2 + max(χ3x4 + x6, O) - 1 + (χ4χ5X6χ7χ8)2 + (τ+⅛) + P1=1 Xi
X1X2 + 2x3+x5+x6 + 2x3+x4+x5+x7 + sin(χ7 Sin(X8 + X9)) + arccos(0.9χιo)
tanh(xiX2 + X3X4) √,∣X5∣ + exp(X5 + X6) + lθg((X6X7X8)2 +1) + X9X10 + 1+∣X1 o|
Sinh(X1 + X2) + arccoS(tanh(X3 + X5 + X7)) + coS(X4 + X5) + Sec(X7X9)
The ground truth of baseline values
b↑ = 0.999 for i ∈ {5,8,10} M = 0.001 for i ∈ {1,2,7,9}
b↑ = 0.999 for i = 5,b÷ = 0.001 for i ∈ {1, 2,7, 9}
b = 0.999 for i ∈ {3, 5,7, 8}
b = 0.999 for i ∈ {3, 5,7, 8}
健=0.999 for i ∈ {1, 2, 3},a=0.001 for i ∈ {4, 5, 8, 9,10}
b↑ = 0.999 for i ∈ {8, 9,10},b÷ = 0.001 for i ∈ {1, 2,3, 4, 5, 6}
b： = 0.999 for i = 9, bj = 0.001 for i ∈ {1, 2, 3, 4,5, 6, 7, 8}
健=0.001 for i ∈ {1, 2, 3, 4, 5, 6}
bi： = 0.001 for i ∈ {6,7,8,9,10}
bi： = 0.999 fori = 3, bi： = 0.001 fori ∈ {1, 2, 4}
the collaboration between input variables was complex and hard to analyze, so we did not consider
such input variables with conflicting patterns for evaluation, either.
E.7 Discussion about the setting of ground-truth Shapley values.
This section discusses the ground truth of Shapley values in the extended Addition-Multiplication
dataset (Zhang et al., 2021), which is used in Section 4 of the paper. In order to verify the correctness
of the Shapley values obtained by the optimal baseline values in this paper, we conducted experiments
on the extended Addition-Multiplication dataset (Zhang et al., 2021) with ground-truth Shapley
values.
The Addition-Multiplication dataset in (Zhang et al., 2021) contained functions that only consisted
of addition and multiplication operations. For example, f(X) = X1X2 + X3X4 where each input
variable Xi ∈ {0, 1} was a binary variable. Given X = [1, 1, 1, 1], the function contained two salient
interaction patterns, i.e. {X1, X2} and {X3, X4}, and their benefits to the output were I({X1, X2}) =
I({X3, X4}) = 1, respectively. According to (Harsanyi, 1963), the Shapley value is a uniform
distribution of attributions. Therefore, the benefit of an interaction pattern was supposed to be
uniformly assigned to variables in the pattern. Thus, the ground-truth Shapley values of variables
were φ1 = φ2 = 1/2, and φ3 = φ4 = 1/2. However, if the input X = [1, 0, 1, 1], then the pattern
{X1, X2} was deactivated and I({X1, X2}) = 0. In this case, φ1 = φ2 = 0 while φ3 = φ4 = 1/2.
According to the analysis in Appendix E.6, ground-truth baseline values in the Addition-Multiplication
dataset were all zero. Then our method is equivalent to the zero baseline values. Therefore, in order
to avoid all ground-truth baseline values being zero, we added the subtraction operation. We also
added a coefficient before each term in the function to boost the diversity of functions. For example,
f(X) = 3.2X1X2 + 1.5X3(X4 - 1). This function also contained two interaction patterns, but the
ground-truth baseline values of variables were different from the aforementioned function. Here,
b0： = b0： = b：3 = 0 and b4： = 1. Given the input X = [1, 1, 1, 0], all patterns were activated and
f(X) = I({X1, X2}) + I({X3, X4}) = 3.2 + (-1.5) = 1.7. Ground-truth Shapley values of input
♦ 1 1	7	7	.1 rʌ /rʌ 1	7	7	TL /rʌ T T	,` ,ι	,	Jrrrl , 1
variables were φ1 = φ2 = 3.2/2 and φ3 = φ4 = -1.5/2. However, for the input X = [1, 1, 1, 1], the
pattern {X3 , X4} was deactivated, thereby φ3 = φ4 = 0. Note that the above function can also be
considered to contain three patterns (f(X) = 3.2X1X2 + 1.5X3X4 - 1.5X3). According to Occam’s
Razor, we follow the principle of the most simplified interaction to recognize interaction patterns in
the function, i.e. using the least number of interaction patterns. Thus, we consider the above function
f(X) = 3.2X1X2 + 1.5X3(X4 - 1) containing two salient interaction patterns.
Based on the extended Addition-Multiplication dataset, we randomly generated an input sample for
each function in the dataset. Each variable Xi in input samples were independently sampled following
the Bernoulli distribution, i.e. p(Xi = 1) = 0.7. Therefore, for the mean baseline, baseline values
of different input variables were all 0.7. For the baseline value based on the marginal distribution,
which was used in SHAP (Lundberg & Lee, 2017), P(Xi)〜Bernoulli(0.7). Then, We compared the
accuracy of the computed Shapley values of input variables based on zero baseline values, mean
baseline values, baseline values in SHAP, and the optimal baseline values defined in this paper,
respectively. The result in Table 4 shows that the optimal baseline values correctly generated the
ground-truth attributions/Shapley values of input variables.
28
Under review as a conference paper at ICLR 2022
E.8 Discussion about the baseline values learned on adversarial examples.
In Section 4 of the main paper, we show that the learned baseline values can recover original samples
form adversarial examples. This section provides more discussions about this experiment.
Let x denote the normal sample, and let xadv = x + δ denote the adversarial example generated by
adversarial attacks (Madry et al., 2018). A previous study (Ren et al., 2021) defined the bivariate
interaction with different contextual complexities, and found that adversarial attacks mainly created
out-of-distribution bivariate interactions with large contexts. In the scenario of this study, we can
consider such sensitive interactions with large contexts related to high-order multi-variate interaction
patterns. It is because the bivariate interaction between (i, j) with large contexts (i.e. under many
contextual variables S) actually considers the collaboration between i, j and contextual variables in
S. Thus, it also partially reflects I(S ∪ {i, j}), which is defined in this paper.
Therefore, from the perspective of multi-variate interaction patterns, the adversarial utility can be
considered as introducing out-of-distribution high-order interaction patterns in the model. To this
end, setting input variables to baseline values is supposed to remove related interaction patterns to
represent absence states. Particularly, if we set all input variables to their baseline values, many
interaction patterns will be eliminated. Thus, if we initialize baseline values as the adversarial
example, and optimize baseline values using our method, the learned baseline values are supposed
to remove OOD high-order interaction patterns in the adversarial example, and recover the original
sample.
29