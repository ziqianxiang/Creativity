Under review as a conference paper at ICLR 2022
Is deeper better? It depends on locality of
RELEVANT FEATURES
Anonymous authors
Paper under double-blind review
Ab stract
It has been recognized that a heavily overparameterized artificial neural net-
work exhibits surprisingly good generalization performance in various machine-
learning tasks. Recent theoretical studies have made attempts to unveil the mys-
tery of overparameterization. In most of those previous works, overparameter-
ization is achieved by increasing the width of the network, while the effect of
the depth of the network has remained less well understood. In this work, we
investigate the effect of the depth within an overparameterized regime for fully
connected neural networks. To gain an insight into the advantage of depth, we
introduce local and global labels according to abstract but simple classification
rules. It turns out that the locality of a relevant feature for a given classification
rule plays a key role; our experimental results suggest that deeper is better for
local labels, whereas shallower is better for global labels. We also compare the
results of finite networks with those of the neural tangent kernel (NTK), and find
that the NTK does not correctly capture the depth dependence of the generaliza-
tion performance, which indicates the importance of the feature learning rather
than the lazy learning.
1	Introduction
Deep learning has achieved unparalleled success in various tasks of artificial intelligence such as
image classification (Krizhevsky et al., 2012; LeCun et al., 2015) and speech recognition (Hinton
et al., 2012). Remarkably, in modern machine learning applications, impressive generalization per-
formance has been observed in an overparameterized regime, in which the number of parameters in
the network is much larger than that of training data samples. Contrary to what we learn in classi-
cal learning theory, an overparameterized network fits random labels and yet generalizes very well
without serious overfitting (Zhang et al., 2017). We do not have general theory that explains why
deep learning works so well.
Recently, the learning dynamics and the generalization power of heavily overparameterized wide
neural networks have extensively been studied. It has been reported that training of an overparam-
eterized network easily achieves zero training error without getting stuck in local minima of the
loss landscape (Zhang et al., 2017; Baity-Jesi et al., 2018). Mathematically rigorous results have
also been obtained (Allen-Zhu et al., 2019; Du et al., 2019). From a different perspective, theory
of the neural tangent kernel (NTK) has been developed as a new tool to investigate an overparame-
terized network with an infinite width (Jacot et al., 2018; Arora et al., 2019), which explains why a
sufficiently wide neural network can achieve a global minimum of the training loss.
As for generalization, it is recognized that an increased model capacity beyond the interpolation
threshold results in improved generalization (Spigler et al., 2019; Belkin et al., 2019), which can-
not be explained by traditional learning theories based on uniform generalization bounds (Mohri
et al., 2018). This finding triggered detailed studies on the benefit of overparameterization. One
seeks for new complexity measures of deep neural networks that can prove better generalization
bounds (Dziugaite & Roy, 2017; Neyshabur et al., 2017; Nagarajan & Kolter, 2017; Arora et al.,
2018; NeyshabUr et al., 2019; Perez et al., 2019). The behavior of the bias and the variance in an
overparameterized regime has also been actively studied (Neal et al., 2019; D’Ascoli et al., 2020).
These theoretical efforts mainly focUs on the effect of increasing the network width, bUt benefits of
the network depth remain Unclear. It is known that expressivity of a deep neUral network grows ex-
1
Under review as a conference paper at ICLR 2022
ponentially with increasing the depth rather than the width (Poole et al., 2016; Bianchini & Scarselli,
2014; Montufar et al., 2014). However, it is unclear whether exponential expressivity does lead to
better generalization (Ba & Caruana, 2014; Becker et al., 2020). It is also nontrivial whether typi-
cal problems encountered in practice require such high expressivity. While some works (Eldan &
Shamir, 2016; Safran & Shamir, 2017) have shown that there exist simple and natural functions that
are efficiently approximated by a network with two hidden layers but not by a network with one
hidden layer (but also see (Safran et al., 2019)), a recent work (Malach & Shalev-Shwartz, 2019)
has demonstrated that a deep network trained by a gradient-based optimization algorithm can only
learn functions that are well approximated by a shallow network, which indicates that benefits of the
depth are not due to high expressivity of deep networks. Some other recent works have reported no
clear advantage of the depth in an overparameterized regime (Geiger et al., 2019a;b).
To gain an insight into the advantage of the depth, in the present paper, we report our experimental
study on the depth and width dependences of generalization in abstract but simple, well-controlled
classification tasks with fully connected neural networks (FNNs). We introduce local labels and
global labels, both of which give simple mappings between inputs and output class labels. By local,
we mean that the label is determined only by a few components of the input vector. On the other
hand, a global label is determined by a global feature that is expressed as a sum of local quantities,
and hence all components of an input contribute to the global label. Our experiments show strong
depth-dependences of the generalization error for those simple input-output mappings. In particular,
we find that deeper is better for local labels, while shallower is better for global labels. This result
implies that the locality of relevant features would give us a clue for understanding the advantage of
the depth. We also show that the above result is not explained by the neural tangent kernel (NTK)
describing the lazy learning regime (Chizat et al., 2019), in which network parameters stay close to
their initial values. This fact implies the importance of the feature learning regime, in which network
parameters change to learn relevant features.
Consideration of local and global labels in this work is motivated by learning natural phenomena
governed by the laws of physics. In physics, fundamental interactions among constituents of the
system are local, and hence data generated by physical laws are fundamentally constrained by this
fact. Consequently, a relevant feature in natural phenomena should be a local one or a sum of local
quantities (i.e. a global one). Thus, the locality of relevant features is crucially important in physics.
It turns out that such a physics-inspired approach is fruitful for understanding deep learning: the
depth dependence of generalization of FNNs is strongly correlated with locality of relevant features.
In view of the fact that modern applications of deep learning are revolutionizing natural science
(physics, chemistry, biology, and so forth), our finding should give insights into those applications.
The focus of our work is to present an important relation between locality of relevant features and
depth, and we present its bare essentials as a fundamental study by focusing on the simplest setting.
If we consider more sophisticated architectures such as convolutional neural networks (CNNs), it
becomes more difficult to identify the direct connection between depth and locality since we would
then have to take other properties (e.g. the translation symmetry, geometrical distances, and so
on) into account. Even the simplest setting (FNNs and i.i.d. Gaussian data) has not been well
understood, so we should start from understanding of the simplest setting and then go into more
involved situations.
2	Setting
We consider a classification task with a training dataset D = {(x(μ),y(μ)) : μ = 1,2,..., N},
where x(μ) ∈ Rd is an input data and y(μ) ∈ {1, 2,..., K} is its label. In this work, We consider
the binary classification, K = 2, unless otherwise stated. Each input x = (x1, x2, . . . , xd)T is a
d-dimensional vector sampled from i.i.d. Gaussian random variables of zero mean and unit variance,
where aT is the transpose of vector a. For each input x, we assign a label y according to one of the
following rules.
In this work, we consider the k-local label and k-global label. Let us randomly choose k integers
{i1,i2, ...,ik} such that 1 ≤ iι < i2 < •…< ik ≤ d. In the “k-local” label, the relevant feature is
2
Under review as a conference paper at ICLR 2022
given by the product of the k components of an input vector x, that is, the label y is determined by
k-local label:
1 if xi1 xi2 . . . xik ≥ 0;
2 otherwise.
(1)
This label is said to be local in the sense that y is completely determined by just the k components
of an input x.1 For FNNs considered in this paper, without loss of generality, we can choose i1 = 1,
i2 = 2,. . . ik = k because of the permutation symmetry of indices of input vectors.
On the other hand, the k-global label is defined by the sign of the quantity M specified below,
1 if M ≥ 0;	d
k-global label: y= 2 otherwise,	M =	xj+i1xj+i2 ...xj+ik,
(2)
where we impose the periodic boundary condition xd+i = xi . The relevant feature M for this
label is given by an equal-weighted sum of the product of k components of the input vector. Every
component of x contributes to this “k-global” label, in contrast to the k-local label with k < d.
2.1	Network architecture
In the present work, we consider FNNs with L hidden layers of width H . We call L and H the depth
and the width of the network, respectively. The output of the network f(x) for an input vector x ∈
Rd is determined as follows: f (x) = Z(L+1) = w(L+1)z(L) + b(L+1), ZQ)二夕(Wa)ZaT) + b(I))
for l = 1, 2, . . . , L and Z(O) = x, where Z(I) is the output of the lth layer and 夕(x) = max{x, 0}
is the component-wise ReLU activation function. Weight and biases of the lth layer are denoted by
w(l) and b(l), respectively. Let us denote by w the set of all the weights and biases in the network.
We focus on an overparameterized regime, where it is possible to fit random labels and achieve zero
training error.
2.2	Supervised learning
The network parameters w are adjusted to correctly classify the training data. It is done through
minimization of the softmax cross-entropy loss (experimental results for the mean-square loss are
presented in Appendix B). The training of the network is done by the stochastic gradient descent
(SGD) with learning rate η and the mini-batch size B (we fix B = 50 throughout the paper).
Meanwhile, we optimize η > 0 before training (we explain the detail later). Biases are initialized to
be zero, and weights are initialized using the Glorot initialization (Glorot & Bengio, 2010).2 *
The trained network classifies an input χ(μ) to the class y(μ) = argmaXi∈{i,2,...,κ} fi(x(μ)). Let
us then define the training error as Etrain = (1/N) PN=I(1 - δy(μ),y(μ)), which gives the miss-
classification ratio for the training data D. We train our network until Etrain = 0 is achieved, i.e.,
until all the training data samples are correctly classified, which is possible in an overparameterized
regime.
For a training dataset D, we first perform the 10-fold cross validation to optimize the learning rate η
under the Bayesian optimization method (Snoek et al., 2012), and then perform training via the SGD
by using the full training dataset. In the optimization ofη, we try to minimize the miss-classification
fraction for the validation data.
The generalization performance of a trained network is measured by computing the test error Etest ,
which is defined as the miss-classification ratio for the test data with Ntest = 105 samples.
1The locality here does not necessarily imply that k points i1 , i2, . . . , ik are spatially close to each other.
Such usage of the terminology “k-local” is found in the field of quantum information (Kempe et al., 2006).
Although k-global is also called “k-local” in quantum information, it is important to distinguish between local
and global in our context.
2We also tried the He initialization (He et al., 2015) and confirmed that results are similar to the ones
obtained by the Glorot initialization, especially when input vectors are normalized as kxk = 1.
3
Under review as a conference paper at ICLR 2022
2.3	Neural Tangent Kernel
Following Arora et al. (2019) and Cao & Gu (2019), let us consider a network of depth L and
width H whose biases {b(l)} and weights {w(l)} are randomly initialized as b(l) = βb(l) with
b(l) 〜 N(0,1) and WiIl = p2∕nι-ιW(j) with WiIl 〜 N(0,1) for every l, where nι is the number
of neurons in the lth layer, i.e., no = d, nι = n = •…=ul = H. Let us denote by W the set of all
the scaled weights {W(l)} and biases {b(l)}. The network output is written as f (x, W). When the net-
work is sufficiently wide and the learning rate η for W is sufficiently small3, the scaled parameters W
stay close to their randomly initialized values Wo during training, and hence f (x, W) is approximated
by a linear function of W- W0: f (x,W) = f (x,Wo) + V1^f (x,W)∣w=w0 ∙ (W- Wo) (LeeetaL,2019).
As a result, the minimization of the mean-squared error LMSE = (1∕N) PjN=Jf (x(μ),W) - ~(μ)]2,
where y(μ) ∈ RK is the one-hot representation of the label y(μ), is equivalent to the kernel regression
with the NTK ΘiilL)(x, x0) (i, j = 1, 2, . . . , K) defined as
Θ(L)(x,x0) = lim Ew [(Vwfi(x,W))T(Vwfj(x,W))],
H→∞
(3)
where Ew denotes the average over random initializations of W (Jacot et al., 2018). The parameter β
controls the impact of bias terms, and we follow Jacot et al. (2018) to set β = 0.1 in our numerical
experiment. By using the ReLU activation function, we can give an explicit expression of the NTK
that is suited for numerical calculations. Such formulas are given in Appendix A.
It is shown that the NTK takes the form ΘiijL)(x, x0) = δi,j ΘiL) (x, x0), and the minimization of the
mean-squared error with an infinitesimal weight decay yields the output function
N
fNTK(x)= X θ(L)(x,χ("))(K-1)“νy(ν),	(4)
μ,ν=1
where K-1 is the inverse matrix of the Gram matrix Kμν = Θ(L)(x(μ),x(ν)). An input data X is
classified to y = arg maXi∈{i,2,...,K} fNTK(x).
3	Experimental results
We now present our experimental results. For each data point, the training dataset D is fixed and we
optimize the learning rate η via the 10-fold cross validation with the Bayesian optimization method
(we used the package provided in Nogueira (2014)). We used the optimized η to train our network.
At every 50 epochs we compute the training error Etrain , and we stop the training if Etrain = 0.
For the fixed dataset D and the optimized learning rate η, the training is performed 10 times and
calculate the average and the standard deviation of test errors Etest.
We present experimental results for the softmax cross-entropy loss in this section and for the mean-
square loss in Appendix B. Our main result holds for both cases, although the choice of the loss
function quantitatively affects the generalization performance.
3.1	1 -local and 1 -global labels
In the 1-local and 1-global labels, the relevant feature is a linear function of the input vector. There-
fore, in principle, even a linear network can correctly classify the data. Figure 1 shows the gener-
alization errors in nonlinear networks of the varying depth and width as well as those in the linear
perceptron (the network of zero depth). The input dimension is set to be d = 1000. We also plotted
test errors calculated by the NTK, but we postpone the discussion about the NTK until Sec. 3.3.
Figure 1 shows that in both 1-local and 1-global labels the test error decreases with the network
width, and that a shallower network (L = 1) shows better generalization compared with a deeper
one (L = 7). The linear perceptron shows the best generalization performance, which is natural
3Here, the scaled learning rate η defined for Wt can be finite in the large-width limit (Lee et al., 2019;
Lewkowycz et al., 2020). This means that the original learning rate η defined for wt should be proportional to
1/H to enter the NTK regime.
4
Under review as a conference paper at ICLR 2022
(b) 1-global
rorre tset
(a) 1-local
4535
0. .3 0. .2
0 0
.15 0.1 .05
0 0
2000	4000	6000	8000	10000
N
rorre tset
depth 1, width 500 ∣~θ~∣
depth 1, width 2000 ∣~∙~∣
depth 7, width 100 -A~1
depth 7, width 500
linear perceptron (depth 0) ∣~S~∣
NTK, depth 1
NTK, depth 7 -→--
45352515
0. .3 0. .2 0. .1 0. .0
0 0 0 0
2000	4000	6000	8000	10000
N
Figure 1: Test error against the number of training data samples N for different network architectures
specified by the depth and width for (a) the 1-local label and (b) the 1-global label. Test errors
calculated by the NTK of depth 1 and 7 are also plotted. Error bars are smaller than the symbols.
(a) 2-local	(b) 3-local
0.3
0.2
0.1
0.4
0.5
JO-UO-s。-
0
5000	10000	15000	20000	25000	30000
N
(c) 2-global
0.4
0.5
rorre tset
12000	16000	20000	24000
N
(d) 3-global
0.5
0.4
0.3
0.2
0.1
jo,uə-s2
0	5000 10000 15000 20000 25000 30000
N
.2
0
.1 0
rorre tset
10000	15000	20000	25000	30000
N
500
0
Figure 2: Test error against the number of training data samples N for several network architectures
specified by the depth and the width for (a) the 2-local label, (b) the 3-local label, (c) 2-global label,
and (d) 3-global label. Error bars indicate the standard deviation of the test error for 10 iterations of
the network initialization and the training. Test errors calculated by the NTK of the depth of 1 and 7
are also plotted.
because it is the simplest network that is capable of learning the relevant feature associated with the
1-local or 1-global label. Remarkably, test errors of nonlinear networks (L = 1 and L = 7) are
not large compared with those of the linear perceptron, although nonlinear networks are much more
complex than the linear perceptron.
For a given network architecture, we do not see any significant distinction between the results for
1-local and 1-global labels, which would be explained by the fact that these labels are transformed
to each other via the Fourier transformation of input vectors.
3.2	OPPOSITE DEPTH DEPENDENCES FOR k-LOCAL AND k-GLOBAL LABELS WITH k ≥ 2
For k ≥ 2, it turns out that experimental results show opposite depth dependences for k-local and
k-global labels. Let us first consider k-local labels with k ≥ 2. Figure 2 (a) and (b) show test errors
for varying N in various networks for the 2-local and the 3-local labels, respectively. The input
5
Under review as a conference paper at ICLR 2022
Figure 3: Depth dependence of the test error for N = 104 training samples with 2-local and 2-global
labels. The dimension of input vectors is set to be d = 500 for the 2-local label and d = 100 for the
2-global label. The network width is fixed at 500. An error bar indicates the standard deviation over
10 iterations of the training using the same dataset.
dimension d is set to be d = 500 for the 2-local label and d = 100 for the 3-local label. We see
that the test error strongly depends on the network depth. The deeper network (L = 7) generalizes
better than the shallower one (L = 1). It should be noted that for d = 500, the network of L = 1
and H = 2000 contains about 106 trainable parameters, the number of which is much larger than
that of trainable parameters (' 105) of the network of L = 7 and H = 100. Nevertheless, the latter
outperforms the former for the 2-local label as well as the 3-local label with large N , which implies
that a larger number of trainable parameters does not necessarily imply better generalization.
Figure 2 (b) shows that the network of L = 7 and H = 100 fails to learn the 3-local label for small
N. We also see that error bars of the test error are large in the network of L = 7 and H = 100. The
error bar represents the variance due to initialization and training. By increasing the network width
H, both variances and test errors decrease. This result is consistent with the recent observation in
the lazy regime that increasing the network width results in better generalization because it reduces
the variance due to initialization (D’Ascoli et al., 2020).
Next, we consider k-global labels with k = 2 and 3. The input dimension d is set as d = 100 for
the 2-global label and d = 40 for the 3-global label. We plot test errors against N in Fig. 2 for
(c) the 2-global label and (d) the 3-global label. Again we find strong depth dependences; however,
shallow networks (L = 1) outperform deep ones (L = 7), contrary to the results for k-local labels.
For L = 7, we also find strong width dependences; the test error of a wider network more quickly
decreases with N . In particular, in the 3-global label, an improvement of the generalization with N
is minor for L = 7 and H = 100. An increase in the width decreases the test error with N much
faster [see the result for L = 7 and H = 500 in Fig. 2 (d)].
To examine more details of the effect of depth, we plot the depth dependence of the test error for
fixed training data samples. We prepare N = 10000 training data samples for the 2-local and 2-
global labels, respectively. The input dimension is d = 500 for the 2-local label and d = 100 for
the 2-global label. By using the prepared training data samples, networks of depth L and width
H = 500 are trained up to L = 10. The test errors of trained networks are shown in Fig. 3. The
test error of the 2-local label decreases with increasing L, whereas the test error of the 2-global label
increases with L. Thus, Fig. 3 demonstrates the opposite depth dependences for local and global
labels.
3.3	Comparison between finite networks and NTKs
In Figs. 1 and 2, test errors calculated by using the NTK are also plotted. The NTK is calculated as
a closed form for the mean-square loss, while experimental results for finite networks in Figs. 1 and
2 are for the cross-entropy loss. This difference, however, turns out to be not significant: the use of
the mean-square loss for finite networks does not change the conclusion (see Appendix B).
In the case ofk = 1 (Fig. 1) and the 2-global label [Fig. 2 (c)], the test error in the NTK is comparable
to or lower than that in finite networks.
A crucial difference arises for the case of the k-local label with k = 2 and 3 and the 3-global label.
In Fig. 2 (a) and (b), we see that the NTK almost completely fails to classify the data, although
6
Under review as a conference paper at ICLR 2022
(a)
learning rate
(b)
NTK, depth 1
depth 1, width 2000
depth 7, width 2000
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0	0.1	0.2 0.3	0.4 0.5 0.6 0.7	0.8
learning rate
NTK, depth 7
Figure 4: Learning-rate dependence of the test error. (a) Numerical results for the 2-local and 2-
global labels in the network with the depth of 1 and the width of 2000. (b) Numerical results for the
3-global label in the networks with the depth of 1 and 7. The network width is set at 2000 for both
cases. The dotted lines show the test error calculated by the NTK. Each data is plotted up to the
maximum learning rate beyond which the zero training error is not achieved within 2500 epochs (in
some cases training fails due to divergence of network parameters during the training). Error bars
indicate the standard deviation over 10 iterations of the training.
finite networks are successful. In the case of the 3-global label, the NTK of depth L = 7 correctly
classifies the data, whereas the NTK of depth L = 1 fails [see Fig. 2 (d)]. In those cases, the test
error in a finite network does not seem to converge to that in the NTK as the network width increases.
The NTK has been proposed as a theoretical tool to investigate sufficiently wide networks. How-
ever, it should be kept in mind that the learning rate has to be sufficiently small to achieve the NTK
limit (Jacot et al., 2018; Arora et al., 2019). In our experiment, the learning rate has been opti-
mized by performing a 10-fold cross validation. If the optimized learning rate is not small enough
for each width, the trained network may not be described by the NTK even in the infinite-width
limit (Lewkowycz et al., 2020).
In Fig. 4 (a), we plot the learning-rate dependence of the test error for the 2-local label and the 2-
global label in the network of depth L = 1 and width H = 2000. We observe a sharp learning-rate
dependence in the case of the 2-local label in contrast to the case of the 2-global label. In Fig. 4
(b), we compare the learning-rate dependences of the test error for L = 1 and L = 7 in the case of
the 3-global label (H = 2000 in both cases). We see that the learning-rate dependence for L = 1
is much stronger than that for L = 7, which is consistent with the fact that the NTK fails only for
L = 1. As Fig. 4 (b) shows, the deep network (L = 7) outperforms the shallow one (L = 1) in
the regime of small learning rates, while the shallow one performs better than the deep one at their
optimal learning rates.
Figure 4 also shows that the test error for a sufficiently small learning rate approaches the one
obtained by the corresponding NTK. Therefore, the regime of small learning rates is identified as
a lazy learning regime (Ji & Telgarsky, 2019; Chen et al., 2019; Lee et al., 2019), whereas larger
learning rates correspond to a feature learning regime (Lewkowycz et al., 2020). An important
message of Fig. 4 is that we should investigate the feature learning regime to correctly understand
the effect of the depth on generalization.
3.4	Relevance to real dataset in physics
So far, we have considered synthetic datasets (i.i.d. random Gaussian inputs and k-local or k-global
label). Here, we demonstrate that this seemingly artificial setting is relevant to a realistic dataset in
physics, i.e., a dataset constructed from snapshots of Ising spin configurations.
The Ising model has been studied in physics as a quintessential model for ferromagnets (Nishimori
& Ortiz, 2011). It has also been used to understand the bare essentials of metallic alloys and gas-
liquid transitions, etc. At each site i = 1, 2, . . . , d, a binary spin variable σi ∈ {±1} is placed, and
a spin configuration x is specified by a set of spin variables, i.e., x = (σ1, σ2, . . . , σd)T. In the one-
dimensional Ising model at inverse temperature β, spin configurations are generated according to a
Gibbs distribution Pe (x) = e-βH(X)/Zβ, where Ze is a normalization constant known in statistical
7
Under review as a conference paper at ICLR 2022
Figure 5: Test error against the number of training data samples N . A shallow network of depth 1
and width 500 shows better generalization compared with a deep network of depth 7 and width 500.
mechanics as the partition function and
d
H(x) =	σiσi+1	(5)
i=1
is called the Hamiltonian, which gives the energy of the system.
We fix two different inverse temperatures β1 and β2 . For each data sample, we first choose β from
β1 and β2 with equal probability. Next, we generate a spin configuration according to the probability
distribution Pβ(x) by using a Markov-chain Monte Carlo method. Then we ask whether a given spin
configuration x is generated by Pβ1 (x) or Pβ2 (x). This is a classification problem, which can be
solved by machine learning.
We consider supervised learning for this classification problem, where we fix d = 500, β1 = 0.1,
and β2 = 0.3. A training dataset D = {(x(μ),y(μ)) :μ = 1,2,...,N} consists of spin configura-
tions {χ(μ)} and their labels {y(μ)}. If χ(μ) is generated at the inverse temperature βι (or β2), its
label is given by y(μ) = 1 (or 2). After training, We measure the test error by using an independently
prepared test dataset with 105 samples.
The temperature for a spin configuration x is most successfully estimated from measurement of
its energy, i.e., the value of H(x). Therefore H(x) plays the role of a relevant feature in this
classification problem. It is obvious from Eq. (5) that the energy is a 2-global quantity. We therefore
expect that a shalloWer netWork does better than a deeper one.
Experimental results for a shalloW netWork (L = 1 and H = 500) and a deep one (L = 7 and
H = 500) are shoWn in Fig. 5. For small N, both netWorks shoW almost identical performance,
While for larger N, the shalloW netWork clearly outperforms the deep one, Which is consistent With
the conclusion from synthetic datasets in Sec. 3.2.
Apart from this example, thermodynamic quantities in physics are usually expressed in terms of a
sum of local quantities over the entire volume of the system, and hence they are global variables.
Our main result thus implies that shalloWer netWorks are more suitable for machine learning of
thermodynamic systems.
4	Discussion: Chaotic signal propagations in deep networks
It is crucial for our problem to understand Why deeper (shalloWer) is better for local (global) labels.
As a possible mechanism, We suggest that local features can naturally be detected With the help of
chaotic signal propagation through a deep netWork (Poole et al., 2016) due to multiplicative groWth
of a small variation of an input x across each layer.
An important distinction betWeen the k-local label and the k-global label With k d lies in their
stability against perturbations. We can typically change the k-local label of an input x ∈ Rd by
moving X to X = X + Z with ∣∣zkι = O(1), where Z ∈ Rd is a perturbation and ∣∣ ∙ kι stands for the
Manhattan distance (the L1 distance). On the other hand, a stronger perturbation ∣∣z∣ι = O(√d) is
typically needed to change the k-global label.
8
Under review as a conference paper at ICLR 2022
A recent study by De Palma et al. (2019) shows that a randomly initialized pre-trained wide neural
network generates a label such that a perturbation Z with ∣∣zkι = O(√d) is typically needed to
change it4, indicating that O(√d) is a natural scale of resolution for a pre-trained random neural
network. Compared with it, learning the k-local label requires finer resolution: two close inputs x
and x0 with ∣x - x0∣1 = O(1) should result in distinct outputs f(x) and f(x0) when x and x0 have
different labels. A deep neural network can naturally meet such a requirement via chaotic signal
propagation, and hence the depth is beneficial for learning local features unless the depth is not too
large. On the other hand, the k-global label is as stable as a typical label generated by a pre-trained
network, and the chaoticity is not needed: two close inputs X and χ0 with ||x 一 χ0∣ι《O(√d)
typically share the same label and therefore the outputs f(x) and f(x0) should also be close to each
other. In this case, the chaoticity may bring about unnecessarily high resolution and therefore it may
be rather harmful: a shallower network is better for the k-global label. This is a likely explanation
of the result obtained in Sec. 3.2.
Whereas the chaotic signal propagation was originally discussed to explain high expressivity of deep
networks (Poole et al., 2016), we emphasize that the benefit of depth in local labels is not due to
high expressivity since learning the k-local label with k d does not require high expressivity.
Nevertheless, chaoticity of deep neural networks may play a pivotal role here. We also emphasize
that the chaotic signal propagation discussed here should be distinguished from the chaos/order
transition at random initialization (Schoenholz et al., 2017; Lee et al., 2018). Even if the pre-trained
network is not deeply in the chaotic phase, the network can utilize the chaotic signal propagation
during training.
In Appendix C, to support the above intuitive argument, we experimentally show that a deeper
network has a tendency towards learning more local features even in training random labels. That
is, even for learning a completely structureless dataset, learned features in a deep network tend to
be more local compared with those in a shallow one, which is possibly due to chaoticity of deep
networks and potentially explains the result of Sec. 3.2.
5	Conclusion
In this work, we have studied the effect of increasing the depth in classification tasks. Instead of
using real data, we have employed an abstract setting with random inputs and simple classification
rules because such a simple setup helps us understand under what situations deeper networks per-
form better or worse. We find that the locality of relevant features for a given classification rule
plays a key role. In Sec. 4, we have proposed that the chaotic signal propagation in deep networks
gives a possible mechanism to explain why deeper is better in local labels.
It is also an interesting observation that shallower networks do better than deeper ones for the k-
global label, indicating that the depth is not necessarily beneficial. Although our dataset with the
k-local or k-global label may appear artificial, there are some realistic examples in which k-global
label is relevant, i.e., thermodynamic systems in physics. Thermodynamic variables in physics are
usually expressed in the form of Eq. (2), i.e., the sum of local quantities over the entire volume
of the system, and hence they are regarded as global features. Our result therefore implies that
a shallower network is more suitable for machine learning of thermodynamic systems, which is
confirmed in Sec. 3.4. In typical image classification tasks, local features would be important and
deeper networks do better. However, we expect that there are some real datasets in which global
features are more important, such as an example discussed in Sec. 3.4.
We expect that our finding is relevant to real datasets, including the physics models discussed above.
It is an important future problem to understand how to apply our findings to machine learning prob-
lems for a wider class of real datasets.
It is also an important future problem to examine CNNs, in which the spatial distance among k
entries can affect the result. The weight sharing will also play an important role in learning global
labels with translation invariance. Thus, we expect that results for CNNs will be much more involved
than those for FNNs.
4This statement has been proved for binary inputs with the Hamming distance, but we expect that it also
holds for continuous inputs with the Manhattan distance.
9
Under review as a conference paper at ICLR 2022
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning, 2018.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On Exact Computation with an Infinitely Wide Neural Net. In Neural Information Processing
Systems, 2019.
Jimmy Ba and Rich Caruana. Do deep networks really need to be deep? In Advances in Neural
Information Processing Systems, 2014.
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard Ben Arous, Chiara Cam-
marota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing Dynamics : Deep Neural
Networks versus Glassy Systems. In International Conference on Machine Learning, 2018.
Simon Becker, Yao Zhang, and Alpha A Lee. Geometry of Energy Landscapes and the Optimizabil-
ity of Deep Neural Networks. Physical Review Letters, 124(10):108301, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
Iearning practice and the classical bias-variance trade-off. Proceedings of the National Academy
OfSciencesofthe United States of America ,116(32):15849-15854, 2019.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A com-
parison between shallow and deep architectures. IEEE Transactions on Neural Networks and
Learning Systems, 25(8):1553-1565, 2014.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. Advances in Neural Information Processing Systems, 2019.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suffi-
cient to learn deep ReLU networks? arXiv preprint arXiv:1911.12360, 2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Program-
ming. In Neural Information Processing Systems, 2019.
Stephane D'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double Trouble in Double
Descent : Bias and Variance(s) in the Lazy Regime. arXiv preprint arXiv:2003.01054, 2020.
Giacomo De Palma, Bobak Toussi Kiani, and Seth Lloyd. Random deep neural networks are biased
towards simple functions. Advances in Neural Information Processing Systems, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, 2019.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Uncertainty
in Artificial Intelligence, 2017.
Ronen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks. In Pro-
ceedings of Machine Learning Research, 2016.
Mario Geiger, Stefano Spigler, StePhane D'Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019a.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
training in deep neural networks. arXiv preprint arXiv:1906.08034, 2019b.
Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural Networks and the Bias/Variance
Dilemma. Neural Computation, 4(1):1-58, 1992.
10
Under review as a conference paper at ICLR 2022
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of Machine Learning Research, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE International
Conference on Computer Vision, 2015.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, and Others. Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal processing magazine, 29(6):82-97, 2012.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow ReLU networks. arXiv preprint arXiv:1909.12292, 2019.
Julia Kempe, Alexei Kitaev, and Oded Regev. The complexity of the local Hamiltonian problem.
SIAM Journal on Computing, 35(5):1070-1097, 2006.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on
Learning Representations, 2018.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems, 2019.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: The catapult mechanism. arXiv:2003.02218, 2020.
Kaifeng Lyu and Jian Li. Gradient Descent Maximizes the Margin of Homogeneous Neural Net-
works. In International Conference on Learning Representations, 2020.
Eran Malach and Shai Shalev-Shwartz. Is Deeper Better only when Shallow is Good? In Advances
in Neural Information Processing Systems, 2019.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT press, 2018.
Guido Montufar, Razvan Pascanu, KyUnghyUn Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in Neural Information Processing Systems, 2014.
Vaishnavh Nagarajan and J. Zico Kolter. Generalization in Deep Networks: The Role of Distance
from Initialization. In Advances in Neural Information Processing Systems, 2017.
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A Modern Take on the Bias-Variance Tradeoff in Neural Net-
works. In Workshop on Identifying and Understanding Deep Learning Phenomena, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nathan Srebro. Exploring Gener-
alization in Deep Learning. In Advances in Neural Information Processing Systems, 2017.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2022
Hidetoshi Nishimori and Gerardo Ortiz. Elements of Phase Transitions and Critical Phenomena.
Oxford University Press, 2011.
Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki. Gradient Descent can Learn Less
Over-parameterized Two-layer Neural Networks on Classification Problems. arXiv preprint
arXiv:1905.09870, 2019.
Fernando Nogueira. Bayesian Optimization: Open source constrained global optimization tool for
Python, 2014. URL https://github.com/fmfn/BayesianOptimization.
Guillermo Valle Perez, Ard A Louis, and Chico Q Camargo. Deep learning generalizes because
the parameter-function map is biased towards simple functions. In International Conference on
Learning Representations, 2019.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Expo-
nential expressivity in deep neural networks through transient chaos. In Advances in Neural
Information Processing Systems, 2016.
Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural
networks. In International Conference on Machine Learning, 2017.
Itay Safran, Ronen Eldan, and Ohad Shamir. Depth separations in neural networks: What is actually
being separated? In Proceedings of Machine Learning Research, 2019.
Samuel S Schoenholz, Google Brain, Justin Gilmer, Google Brain, Surya Ganguli, Jascha Sohl-
dickstein, and Google Brain. Deep Information Propagation. In International Conference on
Learning Representations, 2017.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian Optimization of Machine
Learning Algorithms. In Advances in Neural Information Processing Systems, 2012.
Stefano Spigler, Mario Geiger, StePhane D'Ascoli, Levent Sagun, Giulio Biroli, and MatthieU
Wyart. A jamming transition from under- To over-parametrization affects generalization in deep
learning. Journal of Physics A: Mathematical and Theoretical, 52(47):474001, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
Deep Learning Requires Rethinking of Generalization. In International Conference on Learning
Representations, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep ReLU networks. Machine Learning,109(3):467-492, 2020.
12
Under review as a conference paper at ICLR 2022
A	Explicit expression of the NTK
We consider a network whose biases {b(l) } and weights {w(l) } are randomly initialized as bi(l) =
βbil) with b(l) 〜N(0,1) and w(j) = p2∕nι-ιW(j) with Wj)〜N(0,1) for every l, where n is
the number of neurons in the lth layer, i.e., no = d, nι = n = •…=ul = H. In the infinite-width
limit H → ∞, the pre-activation f(l) = w(l)z(l-1) + b(l) at every hidden layer tends to an i.i.d.
Gaussian process with covariance Σ(l-1) : Rd × Rd → R which is defined recursively as
’∑ ⑼(χ,χ0) = xTX0 + β2;
d
‹ λ(1)(XxO)= ( ς(IT)(X,x)	ς(IT)(X,χ0) A .	⑹
λ (X,X ) = (Σ(IT)(X0,x) ∑(lτ)(x0,x0)J ;
P(I)(X,x0) = 2E(u,v)〜N(0,Λ(1)) [φ(u)φ(v)] + β2
for l = 1, 2, . . . , L. We also define
ς(I)(X,xO) = 2E(u,v)〜N(0,Λ(1)) [。(U)S(V)] ,	(7)
where S is the derivative of φ. The NTK is then expressed as Θ(L) (x, xo) = δi,jΘ(L) (x, xo), where
L+1	L+1
Θ(L)(X,xo) = X ( Σ(I-I)(X,xo) Y Σ(I )(x,xo) ) .	(8)
l=1	l0=l
The derivation of this formula is given in Arora et al. (2019).
Using the ReLU activation function S(u) = max{u, 0}, we can further calculate Σ(l) (X, XO) and
Σ(I) (x, xo) (Lee et al., 2019), obtaining
√ z	八	√detΛ(l)	Σ(I-I)(X,xo)	∣"π	∕Σ(I-I)(X,xo)
∑0)(x, xo) =  --------+--------— + arctan —, ( , )	+ β2	(9)
'	’	—	—	[2	V √detΛ(l)川
and
ς (I)(X，X0)=2 [ι+Iarctan ( ς√⅛? )].	(IO)
For X = XO, we obtain Σ(l)(X, X) = Σ(0)(X, X) + lβ2 = kXk2∕d + (l + 1)β2. By solving Eqs. (9)
and (10) iteratively, we obtain the NTK in Eq. (8).5
B Results for the mean-square loss
We shall present experimental results for the mean-square loss. Figure 6 shows test errors for varying
N in various networks for (a) the 2-local label, (b) the 2-global label, (c) 3-local label, and (d) 3-
global label. As for the depth dependence, we find qualitatively the same behavior as in the cross-
entropy loss. That is, deeper is better for local labels, whereas shallower is better for global labels.
However, we also find that generalization performance quantitatively depends on the choice of the
loss function. Overall, the cross-entropy loss leads to better generalization than the mean-square
loss. In particular, for the 2-local and 3-local labels, results for the mean-square loss tend to be
closer to those in the NTK, indicating that the network trained with the cross-entropy loss more
easily escapes from the lazy learning regime compared with the network trained with the mean-
square loss. This is natural since the weights in the network trained with the cross-entropy loss will
eventually go to infinity after all the training data samples are correctly classified (Lyu & Li, 2020)
5When β = 0 (no bias), the equations are further simplified as Σ(l) = kx¾x0k Cos θ(l) and ∑(l) = 1 -
θ(l∏ 1), where θ(0) ∈ [0, π] is the angle between X and x0, and θ(l) is determined recursively by
Cosθ(Il = 1 hsinθ(l-1) + (π - θ(l-1))cosθ(l-1)i .
13
Under review as a conference paper at ICLR 2022
(a) 2-local
0.5
(b) 3-local
0.5
0.4
0.
rorre tset
depth 1, width 500 —θ~∣
depth 1, width 2000 —∙~∣
.depth 7, width 100 —ʌ~∣
depth 7, width 500 —*~∣
0
5000
0.5
NTK, depth 1 --v--
NTK, depth 7 一吟一
10000	15000	20000	25000	30000
N
(c) 2-global
depth 1, width 500 —θ~∣
depth 1, width 2000 —∙~∣
depth 7, width 100 T~∣
depth 7, width 500 1~∣
NTK, depth 1 --j√--
NTK, depth 7 一吟一
0.4
0.
rorre tset
5000	10000	15000	20000	25000	30000
rorre tset
rorre tset
.4 .3 .2
00 0
0
10000	15000	20000	25000	30000
N
(d) 3-global
0.5
.4 .3 .2
00 0
0
5000	10000	15000	20000	25000	30000
N
a
a

N
Figure 6: Test error against the number of training data samples N for several networks trained
with the mean-square loss for (a) the 2-local label, (b) the 3-local label, (c) 2-global label, and (d)
3-global label. Error bars indicate the standard deviation of the test error for 10 iterations of the
network initialization and training. Test errors calculated by the NTK of the depth of 1 and 7 are
also plotted.
if no regularization such as the weight decay is used. On the other hand, for the mean-square loss,
the network parameters can stay close to their initial values and therefore remain in the lazy learning
regime throughout training (Arora et al., 2019).
We also find that test errors for the mean-square loss show stronger dependences on the width of
the network, especially for local labels. For example, let us compare Fig. 2 (b) in the main text
and Fig. 6 (b). In the case of the cross-entropy loss, the network of L = 7 and H = 100 fails to
generalize for small N (N . 15000) but generalizes well for larger N . On the other hand, in the
case of the mean-square loss, the same network does not generalize well at least up to N = 30000,
although the network of L = 7 and H = 500 does. Strong width dependences found in Fig. 6 are
consistent with recent rigorous results (Nitanda et al., 2019; Ji & Telgarsky, 2019; Chen et al., 2019;
Zou et al., 2020), which show that the cross-entropy loss requires a much smaller width compared
with the mean-square loss to realize an overparameterized regime.
C Local stability experiment
In Sec. 4, we conjecture that the chaotic signal propagation through a deep network explains why
deeper (shallower) is better for local (global) labels. Here, we experimentally show that a deeper
network has a tendency towards learning more local features even in training random labels.
Rather than considering general perturbations as in Sec. 4, we here restrict ourselves to local per-
turbations, z = ±ve(i) , where v > 0 is the strength of a perturbation and e(i) is the unit vector in
the ith direction (i = 1, 2,...,d). For each sample χ0(μ) in the test data, if the network does not
change its predicted label under any local perturbation for a given v, i.e., arg maxj∈[κ] fj (χ0(μ))=
argmaxj∈[K][f (x0(μ) + ɑve⑴)]forany i ∈ [d] and α ∈ {±1},we say that the network is v-stable
14
Under review as a conference paper at ICLR 2022
Figure 7: Local stability s(v) for a shallow network (L = 1, H = 500), a deep network (L = 7,
H = 500), the 2-local label, and the 2-global label. The networks are trained on training dataset
with randomized labels. We can see that s(v) for a deep network is smaller than that for a shallow
one, which implies that a deeper network has a tendency to learning more local features even if the
same dataset is given.
at x0(μ). We define SW)(V) as S(U)(V) = 1 if the network is V-Stable at x0(μ) and S(U)(V) = 0 other-
wise. The local stability of the network is measured by a quantity S(V) = (1/Ntest) PN=St s(μ)(v).
The V-stability can be defined similarly for the k-local and k-global labels: a given label is said to
be V-stable at χ0(μ) if the label does not change under any local perturbation with strength v. We can
also define S(V) for k-local and k-global labels. In our setting, V = O⑴ is sufficient to change the
k-local label, whereas typically V = O(√d) is needed to alter the k-global label.
We measure S(V) for shallow (L = 1 and H = 500) and deep (L = 7 and H = 500) networks
trained on N = 104 datasets with random labels by using the SGD of the learning rate η = 0.1
and the minibatch size B = 50. The dimension of input vectors is fixed at d = 100. In Fig. 7,
S(V) is plotted for the shallow and deep networks as well as for the 2-local and 2-global labels. We
observe that S(V) in the deep network is smaller than that in the shallow one. Moreover, S(V) in the
shallow network is close to that of the 2-global label. These experimental results indicate that even
for learning a completely structureless dataset, learned features in a deep network tend to be more
local compared with those in a shallow one.
15