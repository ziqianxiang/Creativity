Under review as a conference paper at ICLR 2022
Batched Lipschitz Bandits
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the batched Lipschitz bandit problem, where the expected reward is Lipschitz
and the reward observations are collected in batches. We introduce a novel landscape-aware algorithm,
called Batched Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback setting. In
particular, we show that for a T -step problem with Lipschitz reward of zooming dimension dz, our al-
gorithm achieves theoretically optimal regret rate of O (T⅛⅛) using only O (⅛T) batches. For the
lower bound, we show that in an environment with B-batches, for any policy π, there exists a problem
instance such that the expected regret is lower bounded by Ω (Rz(T) 1-(d+2)	), where Rz(T) is the
regret lower bound for vanilla Lipschitz bandits that depends on the zooming dimension dz , and d is the
dimension of the arm space.
1 Introduction
Multi-Armed Bandit (MAB) algorithms aim to exploit the good options while explore the decision space. These algo-
rithms and methodologies find successful applications in artificial intelligence and reinforcement learning (e.g., Silver
et al., 2016). While the classic MAB setting assumes that the rewards are immediately observed after each arm pull,
real-world data often arrives in different patterns. For example, data from field experiments are often be collected in a
batched fashion (Pocock, 1977), since the field researchers can hardly collect back every questionnaire immediately after
it is filled out. Another example is from distributed computing. In such settings, each leaf node stores its observations
locally and communicate the data every once in a while. From the central node’s perspective, the observation arrives
in a batched version. In such cases, any observation-dependent decision-making should comply with this data-arriving
pattern, including MAB algorithms.
In this paper, We study the batched Lipschitz bandit problem - a MAB problem where the expected reward is Lipschitz
and the observations are collected in batches. The model assumes that the time horizon T is divided into B batches by
a grid T = {to, ∙∙∙ ,tB}, where 0 = to < tι < •… < tB = T. For any tj-ι < t ≤ tj, the reward at time t cannot
be observed until time tj, and the decision made at time t depends only on rewards up to time tj-1. Under the batched
setting, the player may have significantly fewer observations when making decisions (Gao et al., 2019). This limitation
of data interaction causes difficulty for both the exploration and the exploitation procedure. Due to this difficulty, existing
algorithms for Lipschitz bandit (e.g., Kleinberg et al., 2008; Bubeck et al., 2009) fails to solve the batched setting.
To this end, we present a novel adaptive algorithm for batched Lipschitz bandit problems, named Batched Lipschitz
Narrowing (BLiN). BLiN is adaptive in two respects. First, the algorithm learns the landscape of the reward by adaptively
narrowing the arm set, so that regions of high reward are more frequently played. Second, the algorithm determines the
data collection procedure adaptively, so that only very few data communications (number of batches) are needed. We
show that for a T-step problem with Lipschitz reward of zooming dimension dz, BLiN only needs O logT-) batches
to achieve the theoretically optimal regret rate of O (T dz +2). In other words, BLiN achieves the regret rate of the best
existing Lipschitz bandit methods (Kleinberg et al., 2008; Bubeck et al., 2009) using only O (⅛t) batches. Since all
existing Lipschitz bandit methods requires T batches, BLiN significantly saves the data communication cost for Lipschitz
bandit problems.
We also provide complexity analysis for this problem. More precisely, we show that if the observations are col-
lected in B batches, then for any policy π, there exists a problem instance such that the expected regret is at best
Ω (Rz(T) 1-(d+2) ), where Rz(T) is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming
dimension dz (see Section 2 for the exact definition), and d is the dimension of the arm space. This results implies that
Ω( IOg)IodT) batches are necessary to achieve optimal regret rate.
1
Under review as a conference paper at ICLR 2022
1	. 1 Related Works
The history of the Multi-Armed Bandit (MAB) problem can date back to Thompson (1933). Solvers for this problems
include the UCB algorithms (Lai & Robbins, 1985; Agrawal, 1995b; Auer et al., 2002a), the arm elimination method
(Even-Dar et al., 2006; Perchet & Rigollet, 2013), the -greedy strategy (Auer et al., 2002a; Sutton & Barto, 2018), the
exponential weights and mirror descent framework (Auer et al., 2002b).
Recently, with the prevalence of distributed computing and large-scale field experiments, the setting of batched feedback
has captured attention (e.g., Cesa-Bianchi et al., 2013). Perchet et al. (2016) mainly consider batched bandit with two
arms, and a matching lower bound for static grid is proved. It was then generalized by Gao et al. (2019) to finite-armed
bandit problems. In their work, the authors designed an elimination method for finite-armed bandit problem and proved
matching lower bounds for both static and adaptive grid. Soon afterwards, Ruan et al. (2021) provides a solution for
batched bandit with linear reward. Parallel to the regret control regime, best arm identification with limited number of
batches was studied in Agarwal et al. (2017) and Jun et al. (2016). Top-k arm identification in the collaborative learning
framework is also closely related to the batched setting, where the goal is to minimize the number of iterations (or
communication steps) between agents. In this setting, tight bounds are obtained in the recent works by Tao et al. (2019);
Karpov et al. (2020). Yet the problem batched bandit problem with Lipschitz reward remains unsolved.
The Lipschitz bandit problem is important in its own stand. The Lipschitz bandit problem was introduced as “continuum-
armed bandits” (Agrawal, 1995a), where the arm space is a compact interval. Along this line, bandits that are Lipschitz (or
Holder) continuous have been studied. For this problem, Kleinberg (2005) proves a Ω(T2/3) lower bound and introduced
a matching algorithm. Under extra conditions on top of Lipschitzness, regret rate of Oe(T 1/2) was achieved (Cope, 2009;
Auer et al., 2007). For general (doubling) metric spaces, the Zooming bandit algorithm (Kleinberg et al., 2008) and the
Hierarchical Optimistic Optimization (HOO) algorithm (Bubeck et al., 2011a) were developed. In more recent years,
some attention has been focused on Lipschitz bandit problems with certain extra structures. To name a few, Bubeck et al.
(2011b) study Lipschitz bandits for differentiable rewards, which enables algorithms to run without explicitly knowing
the Lipschitz constants. The idea of robust mean estimators (Bubeck et al., 2013; Bickel, 1965; Alon et al., 1999) was
applied to the Lipschitz bandit problem to cope with heavy-tail rewards, leading to the development of a near-optimal
algorithm for Lipschitz bandit with heavy-tailed reward (Lu et al., 2019). Lipschitz bandits where a clustering is used to
infer the underlying metric, has been studied by Wanigasekara & Yu (2019). Contextual Lipschitz bandits have also been
studied by Slivkins (2014) and Krishnamurthy et al. (2019). Yet all of the existing works for Lipschitz bandits assume
that the reward sample is immediately observed after each arm pull, and none of them solve the batched Lipschitz bandit
problem.
2	Settings & Preliminaries
For a (batched) Lipschitz bandit problem, the arm set is a compact doubling metric space (A, dA). The expected reward
μ : A → R is I-LiPSchitz with respect to the metric d/, that is
∣μ(χι) - μ(χ2)∣ ≤ dA(χ1,χ2), ∀χ1,χ2 ∈ A.
In each round t ≤ T, the learning agent pulls an arm Xt ∈ A that yields a reward sample yt = μ(χt) + j, where
Et is a mean-zero independent sub-Gaussian noise. Without loss of generality, We assume that Et 〜N(0,1), since
generalizations to other sub-Gaussian distributions are not hard.
For a batched problem, the time horizon T is divided into B batches, with respect to a grid T = {to, ∙∙∙ , tB}. In this
environment, the reward sample for time t cannot be observed until the current batch is finished. Formally, the reward
sample yt can not be observed until tm = mins{s ∈ T : s ≥ t}.
Similar to most bandit learning problems. the agent’s goal is to minimize the regret:
T
R(T) = X (μ* - μ(Xt)),
t=1
where μ* denotes maxχ∈∕ μ(x). For simplicity, We define ∆χ = μ* - μ(x) (called optimality gap of x) for all X ∈ A.
2.1	Doubling Metric Spaces and the ([0,1]d, k ∙ ∣∣∞) Metric Space
By the Assouad’s embedding theorem (Assouad, 1983), the (compact) doubling metric space (A, dA) can be embedded
into a Euclidean space with some distortion of the metric; See (Wang & Rudin, 2020) for more discussions in a machine
learning context. Due to existence of such embedding, the metric space ([0,1]d, ∣ ∙ ∣∣∞), where metric balls are cubes, is
sufficient for the purpose of our paper. For the rest of the paper, we will use hypercubes in algorithm design for simplicity,
while our algorithmic idea generalizes to other doubling metric spaces.
2
Under review as a conference paper at ICLR 2022
2.2	Zooming Numbers and Zooming Dimensions
An important concept for bandit problems in metric spaces is the zooming number and the zooming dimension (Kleinberg
et al., 2008; Bubeck et al., 2009; Slivkins, 2014), which we discuss now.
Define the set of r-optimal arms as S(r) = {x ∈ A : ∆x ≤ r}. For any r = 2-i, the decision space [0, 1]d can be equally
divided into 2di cubes with edge length r, which we call standard cubes (more commonly referred to as dyadic cubes).
The r-zooming number is defined as
Nr := # {C : C is a standard cube with edge length r and C ⊂ S(16r)} .
In words, Nr is the r-packing number of the set S(16r) in terms of standard cubes. The zooming dimension is then
defined as
dz := min{d : Nr ≤ r-d, ∀r ∈ (0, 1]}.
d
Previously, seminal works (Kleinberg et al., 2008; Slivkins, 2014; Bubeck et al., 2011a) show that the optimal regret
bound for traditionally Lipschitz bandits, where the reward observations are immediately observable after each arm pull,
is
E[R(T)] . Rz(T) , inf < 16roT + 256 X	Nr logT ∖	⑴
r0	r
r=2-i ,r≥r0
in terms of zooming number, and
E[R(T)] . T1-dZ+2 log T
in terms of zooming dimension.
This paper is organized as follows. In section 3, we introduce the BLiN algorithm and give a visual illustration of the
algorithm procedure. In section 4, We prove that BLiN achieves the optimal upper bound using only O (IodgT) batches.
Section 5 provides regret lower bounds for batched Lipschitz bandit problems. An experimental result is presented in
Section 6.
3	Algorithm
In a batched bandit environment, the agent’s knoWledge about the environment does not accumulate Within each batch,
since the reWard samples can only be collected at the end of the batch. Due to this nature, there is no gain from changing
strategies within each batch. This characteristic of the problem suggests a “uniform” type algorithm - we shall treat each
step Within the same batch equally. FolloWing this intuition, in each batch, We uniformly play the remaining arms, and
eliminate arms of low reward. Next we describe the uniform play rule and the arm elimination rule.
Uniform Play Rule: For each batch m, a collection of subsets of the arm space Am = {Cm,ι, Cm,2,…，Cm,∣Am∣}is
constructed. This construction Am consists of cubes, and all cubes in Am have the same edge length rm = 2-(m-1) . We
will detail the construction of Am when we describe the arm elimination rule.
During batch m, each cube in Am is played nm := 16：og T times, where T is the total time horizon. More specifically,
rm
within each C ∈ Am, arms χc,ι, χc,2,…，χc,nm ∈ C are played1. The reward samples {yc,1,yc,2,…，yc,nm}C∈Am
corresponding to {χc,ι, xc,2,…,χc,nm }c∈a will be collected at the end of the this batch. Once the reward feedback
is collected, we can eliminate arms of low reward.
Arm Elimination Rule: At the end of batch m, information from the arm pulls is collected, and we estimate the reward
of each C ∈ Am by μm(C) = n1- PnmL yc,i. Cubes of low estimated reward are then eliminated, according to the
following rule:
• A cube C ∈ Am is removed if μmax - bm(C) ≥ 4rm, where bmax := maxc∈Am bm(C).
After necessary removal of “bad cubes”, each cube in Am that survives the elimination is divided into 2d subcubes of
edge length rm/2. These cubes (of edge length rm/2) are collected to construct Am+1, and the learning process moves
on to the next batch.
Remark 1. Note that only SamPlesfom batch m is Usedfor computing bm. This algorithm design is sufficient to achieve
theoretically optimal regret bound. Yet in practice, one can well keep all historical samples.
The learning process is summarized in Algorithm 1.
1One can arbitrarily play xc,1,xc,2, •…,xc,nm as long as xc,i ∈ C for all i.
3
Under review as a conference paper at ICLR 2022
Algorithm 1 Batched Lipschitz Narrowing (BLiN)
1:
2:
3:
4:
5:
6:
7:
8:
9:
Input. Arm set A = [0, 1]d; time horizon T.
Initialization A1 = {[0, 1]d}; edge length r1 = 1; An upper bound of number of batches B0 (default value B0 = T);
The first grid point t0 = 0.
for m = 1, 2,…，Bo do
Compute nm =
16 log T
r2
rm
Compute the next grid point tm = tm-ι + nm, ∙ ∣Am∣.
For each cube C ∈ Am, Play arms χc,ι,…χc,nm from C, and compute the average payoff bm(C) = EinI yC,i.
Find μmax = maχC∈Am b(C).	R
For each cube C ∈ Am, eliminate C if μmmax - bm(C) > 4rm1.
Equally partition each remaining cube in Am to 2d subcubes and define Am+1 as the collection of these subcubes.
Define rm+1 = rm /2, and repeat.
/* During the algorithm execution, we immediately exits the for-loop terminates once all T pulls are exhausted. */
10:
11:
/* In practice, we can choose a large B0 so that all T pulls are exhausted within the for-loop. */
end for
Cleanup: Arbitrarily play the remaining arms until all T steps are used.
A visualization ofa BLiN run is in Figure 1. In the i-th subgraph, the white cubes are those remaining after the i-th batch.
In this experiment, We set A = [0,1]2, and the optimal arm is x* = (0.8,0.7). Note that x* is not eliminated during the
game. More detailed description and results of this experiment are presented in Section 6.
Figure 1: Partition and elimination process of BLiN (Algorithm 1). The i-th subfigure shoWs the pattern after the i-th
batch for 1 ≤ i ≤ 5. Shaded cubes are those eliminated, While darkgray ones are eliminated at the current batch. For the
total time horizon T = 80000, BLiN needs only 6 batches.
4	Regret Analysis of Algorithm 1
In this section, we provide regret analysis for Algorithm 1. The highlight of the finding is that 2 + dgT2 batches are
dz+2
sufficient to achieve optimal regret rate of O (T dz+2), as summarized in Theorem 1.
Theorem 1. With probability exceeding 1 — T, the T-step total regret R(T) of Algorithm 1 satisfies
dz+1
R(T) ≤ 528TdZ+2 log T,	(2)
where dz is the zooming dimension of the problem instance. In addition, Algorithm 1 only needs 2 + lo0g+τ batches to
achieve this regret rate.
To prove Theorem 1, we first show that the estimator μ is concentrated to the true expected reward μ. This result is stated
in Lemma 1 and can be quickly verified with the Hoeffding’s inequality and Lipschitzness of the expected reward. In the
following analysis, we let Bstop be the batch where the last arm is pulled in BLiN.
Lemma 1. Define
E:
{∣μ(x) - bm(C)| ≤ rm +
∕l6log T
V nm
∀1 ≤ m ≤ Bstop ,
∀C ∈ Am , ∀x ∈ C
)
It holds that P (E) ≥ 1 - 2T -6.
The proof of Lemma 1 is in Appendix A. Next, we show that under event E, the optimal arm is not removed (Lemma 2),
and the cubes that survive elimination are of high reward (Lemma 3).
4
Under review as a conference paper at ICLR 2022
Lemma 2. Under event E, the optimal arm x* = arg max μ(x) is not eliminated.
Proof. We use Cm* to denote the cube containing x* in Am . Here we proof that Cm* is not eliminated in round m.
Under event E , for any cube C ∈ Am and x ∈ C, we have
μ(C) —	μ(Cm)	≤	μ(x)	+ J16IogT	+ Tm	-	μ(x*) +	/ 16IogT	+ Tm	≤ 4rml.
nm	nm
Then from the elimination rule, Cm* is not eliminated.
Lemma 3. Under event E, for any 1≤ m ≤ Bstop, any C ∈ Am and any x ∈ C, ∆x satisfies
∆x ≤ 16Tm .
□
(3)
Proof. For m = 1, (3) holds directly from the LiPschitzness of μ. For m > 1, let Cm-I be the CUbe in Am-ι SUch that
x* ∈ Cm* -1. From Lemma 2, this cube Cm* -1 is well-defined under E. For any cube C ∈ Am and x ∈ C, it is obvious
that x is also in the Parent of C, which is denoted by Cpar. ThUs for any x ∈ C, it holds that
δ — ..*	C	/C*	一 ∕16logT - C	/C 一 ∕16logT 一
△x = μ	μ(χ) ≤ μm-i(Cm-I) + ∖	+ rm-1	μm-1(Cpar) + ∖	+ rm-1,
nm-1	nm-1
where the inequality uses Lemma 1.
Equality J InmgT = rm-ι gives that
δx ≤ μm-1 (Cm-I) - μm-1(Cpar ) + 4rm-1∙
Since the cube Cpar is not eliminated, from the elimination rule we have
bm-1(Cm-I)- Gm-1 (Cpar ) ≤ μm-x1 - Gm-1(Cpar ) ≤ 4rm-1∙
Hence we have
∆x ≤ 8rm-1 = 16rm.
□
With Lemmas 2 and 3, we can Prove the regret gUarantee for Algorithm 1. The regroUPing argUment is similar to the
seminal ones by Kleinberg et al. (2008), bUt we can achieve sUch resUlts with a smaller nUmber of batches.
Lemma 4. Under event E, the T -step total regret R(T) of Algorithm 1 satisfies
R(T) ≤ inBf
(Xx Nrm ∙
m=1
256^ + i6tb TL
Tm
(4)
Proof. Lemma 3 shows that every cUbe C ∈ Am is a sUbset of S(16Tm). ThUs from the definition of zooming nUmber,
we have
|Am | ≤ Nrm .	(5)
Fix a Positive nUmber B. Lemma 3 also imPlies that any arm Played after batch B incUrs a regret boUnded by 16TB, since
the cUbes Played after batch B have edge length no larger than TB. Then the total regret occUrs after the first B batch is
boUnded by 16TBT.
ThUs the regret R(T) can be boUnded by
B	nm
R(T) ≤ X X X ∆xC,i + 16TBT,	(6)
m=1 C∈Am i=1
where the first term boUnds the regret in the first B batches in Algorithm 1, and the second term boUnds the regret after
the first B batches. If the algorithm stops at batch BstOP < B, We define Am = 0 for any BstOP < m ≤ B and inequality
(6) still holds.
By Lemma 3, We have ∆C,i ≤ 16Tm for all C ∈ Am. We can thus bound (6) by
B
R(T) ≤ X |Am| ∙ nm ∙ 16rm + 16tbT
m=1
X256logT
≤ 工 Nrm ∙ —r------ + 16rBT,
m=1	Tm
5
Under review as a conference paper at ICLR 2022
where the second inequality uses (5) and nm, = 16)og T. Then by taking inf on all B, We have
rm
R(T) ≤ iBf (XX Nrm ∙ 256^0gT + 16rB T),
B	m=1	rm
which finishes the proof of the Lemma.
□
Remark 2. Note that the right hand of (4) equals to the optimal regret bound for traditional Lipschitz bandits in terms of
zooming number
Rz(T) =inf I 16r0T + 256 X N logT ∖ .
r0	r
r=2-i, r≥r0
Proof of Theorem 1. Since rm = 2-m+1 and Nrm ≤ rm-dz = 2(m-1)dz, (4) yields that
R(T) ≤
inf 256 X
m=1
2(m-1)(dz +1) logT
+ 16∙2-b+1T
}
By choosing B* = 1+ lθgɪT, we have
dz +2
R(T) ≤512 ∙ 2(B*-1)(dz+1) logT + 16 ∙ T ∙ 2-b* + 1
dz + 1
≤528TB log T.
'~' d dz + 1、
The analysis in Theorem 1 implies that we can achieve the optimal regret rate O (T dz+2 J by letting the for-loop of
Algorithm 1 run B * times and finishing the remaining rounds in the Cleanup step. In other words, B * + 1 batches are
sufficient for Algorithm 1 to achieve the regret bound (2).	口
5 Lower Bounds
In this section, we present the lower bounds for the batched Lipschitz bandit problem. Our lower bounds depend on the
number of batches B . When B is sufficiently large, our lower bounds match the lower bound for the vanilla Lipschitz
bandit problem Θ(Rz (T)). More importantly, this dependency on B gives the minimal number of batches needed to
achieve optimal regret rate.
The lower bound depends on the number of batches, as well as how the grid is determined. The grid can be static or
adaptive. If the grid T is static, T is fixed and independent from the policy. If the grid is adaptive, every grid point tj ∈ T
can be chosen based on the choices and observations before tj-1, and the determination of grid points is part of the policy.
We provide lower bounds for both static and adaptive grids, respectively in Theorem 2 and Theorem 3. The adaptive grid
case is more difficult to analyse and more general. Therefore, we present the core part of the proof for the adaptive-grid
lower bound in the main text. For static grid, we sketch its constructive proof in Theorem 2 in the main text and postpone
the details to the Appendix. The proof is inspired by (Gao et al., 2019) and (Slivkins, 2014) but non-trivially extends both
previous arguments.
Theorem 2 (Lower Bound for Static Grid). For B-batched Lipschitz bandit problem with time horizon T and any static
grid T, for any policy π, there exists a problem instance such that
1
d + 2
1

E[RT(π)] ≥ C ∙ (logT) 1-(d+2)B ∙ Rz(T)1-(d++2)B ,
where c > 0 is a numerical constant independent ofB, T, π and T, Rz (T) is defined in (1), and d is the dimension of the
arm space.
To prove Theorem 2, we first find tk-1, tk ∈ T such that
1___L
1 d+2 ,,
-Itk- ≥ T 1-( d+2 )B
t d+2 -
tk-1
(7)
1__L
1 d+2
Note that we can always find such a pair tk-ι,tk since for any fixed B,	min	max -tk0- ≥ T1-(d+2)
T ={t0,…，tB } k t d+2
k0-1
6
Under review as a conference paper at ICLR 2022
Then We construct a set of problem instances that is difficult to distinguish. Let rk =	, Mk := tk-ιr2 = 4∙, and
+d+2	rk
tk-1
U = {uι,…,UMk} be an arm-set SUCh that d∕(ui, Uj) ≥ rk for any i = j. Then we consider a set of problem instances
I = {Iι,…，IMk}. The expected reward for Ii is defined as
3
4 rk，
5
μι(χ) = 8 8rk,
max < rk, max {μι(u) — d∕(x, u)},,
2 u∈U
x = U1,
x = Uj , j 6= 1,
otherwise.
For 2 ≤ i ≤ Mk , the expected reward for Ii is defined as
μi (X)= <
3
4 rk，
7
8 rk，
5
8 rk，
x = U1 ,
x = Ui,
x = Uj, j 6= 1 and j 6= i,
max < 一, max {μi(u) — d∕(x, u)}
2 u∈U
otherwise.
Instance set I is based on index k which satisfies (7). In our construction, instance Ii has a “peak” located at Ui with
height 7rk (except Ii). Then we prove that no algorithm can distinguish an instance in I from the others in the first
(k — 1) batches, so the total regret is at least rktk, which gives the lower bound we need.
As a result of Theorem 2, we can derive the minimal number of batches needed to achieve optimal regret rate for Lipschitz
bandit problem, which is stated in Corollary 1.
Corollary 1. For a B-batched Lipschitz bandit problem with time horizon T with static grid, at least Ω(空*丁) batches
are needed to achieve the regret rate Θ(Rz (T)).
The detailed proof of Corollary 1 is deferred to Appendix.
Theorem 3 (Lower Bound for Adaptive Grid). For a B-batched Lipschitz bandit problem with time horizon T, for any
adaptive grid T and any policy π, there exists an instance such that
1
d	—	d+2	1
一 .一	1	.	,/1	、B .	. ,/1	、B
E[Rt(∏)] ≥ CB(logT)(中) Rz(T)(中) ,
where c > 0 is a numerical constant independent of B, T, π and T, Rz (T) is defined in (1), and d is the dimension of the
arm space.
Proof. In this proof, we construct a series of ‘worlds’ (sets of problem instances) based on sequences {rj}, {Mj} and
fixed grid {Tj}, which are defined below. In each world, we construct a set of instances using the needle-in-the-haystack
technique. After the policy is given, we can finda world such that the worst-case regret of the policy in this world is lower
bounded by Ω 卜z(T)1-(d⅛)B).
To prove this result, we first define the following fixed grid T = {T0,Tι,…，TB }, where
1-εj
Tj = T 1-εB ,
and ε = d++2. We also define rj = Tε1 B and Mj = rd. From the definition, we have
Tj-Irj = d R2 = Tj.	(8)
rjdB 2	B2
For 1 ≤ j ≤ B, we can find sets of arms Uj = {uj,ι,…,uj,Mj} such that (a) d∕(uj,m, uj,n) ≥ rj for any m = n, and
(b) u1,M1 = •…=UBMB. Then we construct problem instances based on arm sets Ui, ∙∙∙ , UB.
7
Under review as a conference paper at ICLR 2022
Now We construct B different worlds, denoted by Iι,…，ZB. For 1≤ j ≤ B - 1, Ij = {Ij,k}kM=j1-1, and the expected
reward for Ij,k is defined as
ri + rj + rB
2 + 16 + 16 ,
ri	rB
μj,k(x) = ∖^ + 16,
I ri
max S -2, max {μj,k (U) — d∕(x, u)}
For j = B, IB = {IB}. The expected reward for IB is defined as
μB (x)
ri	rB
"2" + Iβ,
max {_2L ,μB (UBMB) - "a(x,ub,Mb )},
x= uj,k,
x = Uj,Mj ,
otherwise.
x= UB,MB ,
otherwise.
(9)
(10)
For all arm pulls in all problem instances, an Gaussian noise sampled from N (0, 1) is added to the observed reward. This
noise corruption is independent from all other randomness.
To link the fixed worlds {Iι, ∙∙∙ ,IB} to the adaptive grid setting, we first note that for any adaptive grid T =
{ti, ∙∙∙ ,tB}, there exists an index j such that in world Ij, (tj-ι,tj] is sufficiently large. More formally, for each
j ∈ [B], we define the event Aj = {tj-i < Tj-i, tj ≥ Tj} and the following quantities
1	Mj -i
Pj = M-1 Σ Pj,k(Aj)
j k=i
for j ≤ B — 1 and
pB := PB (AB ),
where Pj,k(Aj) denotes the probability of the event Aj under instance Ij,k and policy π. For these quantities, we have
the following lemma.
Lemma 5. For any adaptive grid T and policy π, it holds that PB=I Pj ≥ 7.
Lemma 5 implies that there exists some j such that Pj > £. This Lemma is detailedly proved in the Appendix.
Next we proceed with the case where Pj > 8B for some j ≤ B — 1. The case for j = B can be proved analogously.
In world Ij , we use similar method to Theorem 2 to construct a series of instance sets that are difficult to differentiate.
More precisely, for any 1 ≤ k ≤ Mj — 1, we construct a set of problem instances Ij,k = (Ij,k,l)i≤l≤M based on the
constructions in (9) and (10). For l 6= k, the expected reward of Ij,k,l is defined as
3rj
μj,k(x) + 16^,
μj,k,ι(x) = < μj,k(X)，
(ri
max < ɪ, max
{μj,k,ι(U) — dA(χ,u)}
x = Uj,l ,
x ∈ Uj and x 6= Uj,l ,
otherwise.
For l = k, we let μj,k,k = μj,k where μj,k is defined as in (9). For all arm pulls in all problem instances, an Gaussian noise
sampled from N (0, 1) is added to the observed reward. This noise corruption is independent from all other randomness.
Based on this construction, the following lemma gives the lower bound of expected regret, whose proof is in the Appendix.
Lemma 6. For any adaptive grid T and policy π, if index j satisfies Pj ≥ 8B, then there exists a problem instance
I ∈ ∪k≤Mj -i Ij,k such that
一「一，、■.	1	- 1-ε
E [RT(π)]≥ 256B2T 一B .
Since Nr ≤ r-d holds for all instances,
X	Nrr log T ≤ 2r-d-i log T.
r=2-i , r≥r0
Then we have
Rz(T) ≤ inf J 16roT + 512-1τlogTi ≤ 512(logT)d+2T1-壬.
r0	r0d+i
8
Under review as a conference paper at ICLR 2022
Consequently, for any policy π and adaptive grid T, Lemma 5, Lemma 6 and the above inequality shows that there exists
an instance I such that
1
256B2
E [RT (π)] ≥
≥
1____L
1 d+2
T 1-( d+2 )
1
1	1	- d+2 B	1 B
--------zɪ B (log T) 1-(+)B Rz(T) 1-( 率广
256 ∙ 5121-d+2
which concludes the proof.
□
Similar to Corollary 1, We can prove that at least Ω( logθlodT) batches are needed to achieve optimal regret rate. This result
is formally summarized in Corollary 2.
Corollary 2. For a B-batched Lipschitz bandit problem with time horizon T with adaptive grid, at least Ω('姿dT)
batches are needed to achieve the regret bound Θ(Rz (T)).
6	Experiments
In this section, We present numerical studies of BLiN. In the experiments, We use the arm space A = [0, 1]2 and the
expected reward function μ(χ) = 1 - 1 ||x — χι∣∣2 — 10||x — χ2k2, where xi = (0.8,0.7) and x2 = (0.1,0.1). The
landscape of μ is shown in Figure 2(a).
We let the time horizon T = 80000, and report the accumulated regret in Figure 2(b). The regret curve is sublinear, which
agrees with the regret bound (2). Besides, different background colors in Figure 2(b) represent different batches. For the
total time horizon T = 80000, BLiN only needs 6 batches (the first two batches are too small and are combined with the
third batch in the visulization).
(a) Partition
(b) Regret
Figure 2: Resulting partition and regret of BLiN (Algorithm 1). In Figure 2(a), we show the resulting partition of
Algorithm 1. The background color denotes the true value of expected reward μ, and blue means high values. The figure
shows that the partition is finer for larger values of μ. In Figure 2(b), we show accumulated regret of BLiN. In the figure,
different background colors represent different batches. For the total time horizon T = 80000, BLiN needs only 6 batches
(the first two batches are too small and are combined with the third batch in the plot).
7	Conclusion
In this paper, we study the Batched Lipschitz Bandit problem, and propose the BLiN algorithm as a solution. We prove
that BLiN only need O (野T) batches to achieve the optimal regret rate of best previous Lipschitz bandit algorithms
(Kleinberg et al., 2008; Bubeck et al., 2009) that need T batches. This improvement in number of the batches significantly
saves data communication costs. We also provide complexity analysis for this problem. We show that if the observations
are collected in B batches, then for any policy π, there exists a problem instance such that the expected regret is at best
Ω (Rz(T)1-(d+2) ), where Rz(T) is the regret lower bound for vanilla Lipschitz bandits that depends on the zooming
dimension dz. This results implies that Ω( logɔlodT) batches are necessary to achieve the optimal regret rate.
9
Under review as a conference paper at ICLR 2022
References
Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with limited rounds of adaptivity: Coin
tossing, multi-armed bandits, and ranking from pairwise comparisons. In Conference on Learning Theory, pp. 39-75.
PMLR, 2017.
Rajeev Agrawal. The continuum-armed bandit problem. SIAM Journal on Control and Optimization, 33(6):1926-1951,
1995a.
Rajeev Agrawal. Sample mean based index policies by O(log n) regret for the multi-armed bandit problem. Advances in
Applied Probability, 27(4):1054-1078, 1995b.
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments. Journal
of Computer and System Sciences, 58(1):137-147, 1999.
Patrice Assouad. PlongementS Lipschitziens dans Rn. Bulletin de la Societe Mathematique de France, 111:429448,
1983.
Peter Auer, Nicolb Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine
learning, 47(2):235-256, 2002a.
Peter Auer, Nicolb Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem.
SIAM journal on computing, 32(1):48-77, 2002b.
Peter Auer, Ronald Ortner, and Csaba SzePeSvðri. Improved rates for the stochastic continuum-armed bandit problem. In
Conference on Computational Learning Theory, pp. 454-468. Springer, 2007.
Peter J. Bickel. On some robust estimates of location. The Annals of Mathematical Statistics, pp. 847-858, 1965.
SebaStien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvdri. Online optimization in X-armed bandits. Advances
in Neural Information Processing Systems, 22:201-208, 2009.
Sebastien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvdri. X -armed bandits. Journal of Machine Learning
Research, 12(5):1655-1695, 2011a.
Sebastien Bubeck, Gilles Stoltz, and Jia Yuan Yu. Lipschitz bandits without the Lipschitz constant. In International
Conference on Algorithmic Learning Theory, pp. 144-158. Springer, 2011b.
Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gdbor Lugosi. Bandits with heavy tail. IEEE Transactions on Information
Theory, 59(11):7711-7717, 2013.
Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other adaptive adversaries.
Advances in Neural Information Processing Systems, 26:1160-1168, 2013.
Eric W. Cope. Regret and convergence bounds for a class of continuum-armed bandit problems. IEEE Transactions on
Automatic Control, 54(6):1243-1253, 2009.
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for
the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6):1079-1105,
2006.
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched Multi-armed Bandits Problem. Advances in Neural
Information Processing Systems, 32:503-513, 2019.
Kwang-Sung Jun, Kevin Jamieson, Robert Nowak, and Xiaojin Zhu. Top arm identification in multi-armed bandits with
batch arm pulls. In Artificial Intelligence and Statistics, pp. 139-148. PMLR, 2016.
Nikolai Karpov, Qin Zhang, and Yuan Zhou. Collaborative top distribution identifications with limited interaction. In
2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp. 160-171. IEEE, 2020.
Robert Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. Advances in Neural Information Pro-
cessing Systems, 18:697-704, 2005.
Robert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Multi-armed bandits in metric spaces. In Proceedings of the fortieth
annual ACM symposium on Theory of computing, pp. 681-690, 2008.
Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual bandits with continuous
actions: Smoothing, zooming, and adapting. In Conference on Learning Theory, pp. 2025-2027. PMLR, 2019.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics,
6(1):4-22, 1985.
10
Under review as a conference paper at ICLR 2022
Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang. Optimal algorithms for Lipschitz bandits with heavy-tailed
rewards. In International Conference on Machine Learning, pp. 4154-4163, 2019.
Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):
693-721, 2013.
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. The Annals of
Statistics, 44(2):660-681, 2016.
Stuart J. Pocock. Group sequential methods in the design and analysis of clinical trials. Biometrika, 64(2):191-199, 1977.
Yufei Ruan, Jiaqi Yang, and Yuan Zhou. Linear bandits with limited adaptivity and learning distributional optimal design.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pp. 74-87, 2021.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,
Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-
brenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.
Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.
Sean Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, and Christina Yu. Adaptive discretization for model-based
reinforcement learning. Advances in Neural Information Processing Systems, 33:3858-3871, 2020.
Aleksandrs Slivkins. Contextual bandits with similarity information. Journal of Machine Learning Research, 15(1):
2533-2568, 2014.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
Chao Tao, Qin Zhang, and Yuan Zhou. Collaborative learning with limited interaction: Tight bounds for distributed
exploration in multi-armed bandits. In 2019 IEEE 60th Annual Symposium on Foundations of Computer Science
(FOCS), pp. 126-146. IEEE, 2019.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two
samples. Biometrika, 25(3/4):285-294, 1933.
Tianyu Wang and Cynthia Rudin. Bandits for BMO functions. In International Conference on Machine Learning, pp.
9996-10006. PMLR, 2020.
Nirandika Wanigasekara and Christina Yu. Nonparametric contextual bandits in an unknown metric space. In Advances
in Neural Information Processing Systems, volume 32, pp. 14684-14694, 2019.
11