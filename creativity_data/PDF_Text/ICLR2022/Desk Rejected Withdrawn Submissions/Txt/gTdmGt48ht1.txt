Under review as a conference paper at ICLR 2022
On the Double Descent of Random Features Models
Trained with SGD
Anonymous authors
Paper under double-blind review
Ab stract
We study generalization properties of random features (RF) regression in high dimensions optimized
by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotic error bounds
of RF regression under both constant and adaptive step-size SGD setting, and observe the double
descent phenomenon both theoretically and empirically. Our analysis shows how to cope with
multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic
gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical
data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still
generalizes well in the interpolation setting, and is able to characterize the double descent behavior by
the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant
step-size SGD setting incurs no loss in convergence rate when compared to the exact minimal-norm
interpolator, as a theoretical justification of using SGD in practice.
1	Introduction
Harmless interpolation or benign overfitting of over-parameterized neural network (NN) models has received significant
attention in the literature (Zhang et al., 2016; Hastie et al., 2019; Bartlett et al., 2020). This important phenomenon is
also inherently tied to the discovery of the double descent learning curve (Belkin et al., 2019) in deep learning.
Indeed, many key machine learning models, including but not limited to kernel regression (Wu & Xu, 2020; Mei &
Montanari, 2019; Liu et al., 2021b) and neural networks (Nakkiran et al., 2019; Yang et al., 2020; Ju et al., 2020)) first
decrease the test error with increasing number of model parameters in the under-parameterized regime. They then yield
large error when they can first interpolate the data, which is called the interpolation threshold. Finally, the test error
begins to decrease again in the over-parameterized regime in stark contrast to the conventional learning theory results.
Our work partakes in this research vein and studies the random features (RF) model (Rahimi & Recht, 2007) in
the context of double descent phenomenon.1 Briefly, RF model samples random features {ωi}im=1 from a specific
distribution, corresponding to a kernel function. We then construct an explicit map: x ∈ Rd 7→ σ(Wx) ∈ Rm, where
W = [ωι, ∙∙∙ , ωm]τ ∈ Rm×d is the random features matrix and σ(∙) is the nonlinear (activation) function determined
by the kernel. As a result, the RF model training can be viewed as training a two-layer neural network where the
weights in the first layer are chosen randomly and then fixed (a.k.a. the random features) and only the output layer is
optimized, striking a trade-off between practical performance and accessibility to analysis.
An RF model becomes an over-parameterized model if we take the number of random features m larger than that of
training data n. The literature on RF under the over-parameterized regime can be split into various camps according to
different assumptions on the formulation of target function, data distribution, and activation functions (Mei & Montanari,
2019; Ba et al., 2020; d’Ascoli et al., 2020b; Liao et al., 2020; Gerace et al., 2020; Lin & Dobriban, 2021). The existing
theoretical results demonstrate that the excess risk curve exhibits double descent.
Nevertheless, the analysis framework of (most) previous work on RF regression assumes the data to be Gaussian or
uniformly spread on a sphere, and largely relies on the least-squares closed-form solution, including minimal-norm
interpolator and ridge regressor (see comparisons in Table 1 in Appendix A). Such specific data distribution and
dependency on the closed-form solution in fact mismatch practical neural networks optimized by stochastic gradient
descent (SGD) based algorithms on general data distribution.
Our work precisely bridges this gap: We provide a new analysis framework for the generalization properties of RF
models trained with SGD, also accommodating adaptive step-size selection, and provide non-asymptotic results in
1Relevant work on random features model has received the Test-of-Time Award in NeurIPS2017 (Rahimi & Recht, 2007) and
best paper finalist in ICML2019 (Li et al., 2019). Refer to a recent survey (Liu et al., 2021a) for details.
1
Under review as a conference paper at ICLR 2022
under-/over-parameterized regimes on (relatively) general data distribution and activation functions. We make the
following findings and contributions:
•	We characterize statistical properties of several covariance operators/matrices in RF, including ∑m :=
*Ex[σ(Wx∕√d)σ(Wx∕√d)>] and its expectation version Σm := EW[∑m]. We demonstrate that, under
Gaussian initialization, Tr(Σm) is a sub-exponential random variable with O(1) sub-exponential norm; Σm
has only two distinct eigenvalues at O(1) and O(1/m) order, respectively. Such analysis on the spectra of Σm
and Σm (without spectral decay assumption) is helpful to obtain sharp error bounds for excess risk.
•	Based on the bias-variance decomposition in stochastic approximation, we further take into account multiple
randomness sources of initialization, label noise, and data sampling as well as stochastic gradients. We also
derive non-asymptotic error bounds under the adaptive-size SGD setting: the error bounds for bias and variance
as a function of the radio m/n are monotonic decreasing and unimodal, respectively. Importantly, our analysis
holds for both constant and adaptive step-size SGD setting, and is valid under general assumptions on data
distribution and activation functions.
•	Our non-asymptotic results show that, RF regression with SGD still generalizes well for interpolation learning,
and is able to capture the double descent behavior. In addition, we demonstrate that the constant step-size SGD
setting incurs no loss on the convergence rate of excess risk when compared to the exact least-squares closed
form solution. Our empirical evaluations support our theoretical results and findings.
Our analysis sheds light on the effect of SGD on high dimensional RF models in under-/over-parameterized regimes,
and bridges the gap between the minimal-norm solution and numerical iteration solution in terms of optimization and
generalization on double descent. Hence, looking forward to analysis of modern (deep) neural networks under the
realistic setting where m, n, d are all large and comparable is important (and indeed difficult), e.g., (Hu et al., 2020;
Ju et al., 2020) on two-layer neural networks. We expect that our analysis would be helpful for understanding large
dimensional machine learning and neural network models more generally.
2	Related Work and Problem Setting
This section reviews relevant works while formally introducing our problem setting of RF regression with SGD.
2.1	Related works
A flurry of research papers are devoted to analysis of over-parameterized models on optimization (Kawaguchi & Huang,
2019; Allen-Zhu et al., 2019; Zou & Gu, 2019), generalization (or their combination) under neural tangent kernel
(Jacot et al., 2018; Arora et al., 2019; Chizat et al., 2019) and mean-field analysis regime (Mei et al., 2019; Chizat
& Bach, 2020). We take a unified perspective on optimization and generalization but work in the high-dimensional
setting to fully capture the double descent behavior. By high-dimensional setting, we mean that m, n, and d increase
proportionally, large and comparable (Mei & Montanari, 2019; Ba et al., 2020; Liao et al., 2020; d’Ascoli et al., 2020b).
Random features model and double descent: Characterizing the double descent of the RF model is first due to Belkin
et al. (2020) under the one-dimensional setting. Other perspectives derive from random matrix theory (RMT) in high
dimensional statistics (Hastie et al., 2019; Mei & Montanari, 2019; Ba et al., 2020; Liao et al., 2020; Li et al., 2021) and
from the replica method (d’Ascoli et al., 2020b; Rocks & Mehta, 2020; Gerace et al., 2020). Under specific assumptions
on data distribution, activation functions, target function, and initialization, these results show that the generalization
error/excess risk increase when m/n < 1, diverge when m/n → 1, and then decrease when m/n > 1.
Leveraging the bias-variance decomposition analysis in d’Ascoli et al. (2020b); Rocks & Mehta (2020), Adlam &
Pennington (2020); Lin & Dobriban (2021) refine these results by focusing on the analysis of variance due to multiple
randomness sources. (Ba et al., 2020) on RF optimized by gradient descent exhibits the double descent behavior under
the Gaussian data assumption. We refer to comparisons in Table 1 in Appendix A for further details.
Technically speaking, since RF (least-squares) regression involves with inverse random matrices, these two classes
of methods attempt to achieve a similar target: how to disentangle the nonlinear activation function by the Gaussian
equivalence conjecture. RMT utilizes calculus of deterministic equivalents (or resolvents) for random matrices and
replica methods focus on some specific scalar parameters that allows for circumventing the expectation computation. In
fact, most of the above methods can be asymptotically equivalent to the Gaussian covariate model (Hu & Lu, 2020).
Non-asymptotic stochastic approximation: A series of papers on linear/kernel least-squares regression with con-
stant/adaptive step-size SGD often work in the under-parameterized regime, where d is finite and much smaller than n.
For linear least-squares regression (Bach & Moulines, 2013; Jain et al., 2018) and kernel regression (without explicit
2
Under review as a conference paper at ICLR 2022
regularization) (Dieuleveut & Bach, 2016; Dieuleveut et al., 2017), averaged SGD offers a sub-linear rate on bias, while
achieving minimax rates on variance, which leads to a certain O(1/n) convergence rate for excess risk.
Carratino et al. (2018) focus on regularized RF (least-squares) regression with SGD in the under-parameterized
regime. Their analysis largely relies on the Tikhonov regularization in an approximation theory view, which provides a
2r
convergence rate of O(n α+2r) under the regularity condition r ∈ [0,1] and capacity condition α ∈ [0,1].
In the over-parameterized regime, the excess risk in (Chen et al., 2020b) on least squares in high dimensions with
averaged constant step-size SGD can be independent of d, and is further improved to converge with n in (Zou et al.,
2021). Berthier et al. (2020); Varre et al. (2021) also demonstrate this convergence result under min or last-iterate
setting for noiseless least squares. Besides, the existence of multiple descent (Chen et al., 2020a; Liang et al., 2019)
beyond double descent and SGD as implicit regularizer (Neyshabur et al., 2017; Smith et al., 2020) can be traced to the
above two lines of work. Our work shares some similar technical tools with (Dieuleveut & Bach, 2016) and (Zou et al.,
2021) but differs from them in several aspects. We detail the differences in Section 4.
2.2	The problem setting
We study the standard problem setting for RF least-squares regression and adopt the relevant terminologies from
learning theory: cf., (Cucker & Zhou, 2007; Dieuleveut & Bach, 2016; Carratino et al., 2018; Li et al., 2021) for
details. Let X ⊆ Rd be a metric space and Y ⊆ R. The training data {(xi, yi)}in=1 is assumed to be independently
drawn from a non-degenerate unknown Borel probability measure ρ on X × Y . The target function of ρ is defined by
fρ(x) = JY y dρ(y | x), where ρ(∙ | x) is the conditional distribution of P at X ∈ X.
RF least squares regression: We study the RF regression problem with the squared loss as follows:
min E(f),	E(f)	:=	((f (x)	-	y)2ρ(x,y)	=	kf	-	fρkL2	, with f (X)	=	hθ,夕(x)〉Rm ,
f∈H	ρX
where the optimization vector θ ∈ Rm and the feature mapping 夕(x) is defined as
夕(x) := √m [σ(ω>x∕√d),…，σ(ωmx∕√d)] := √mσ(Wx∕√d) ∈ Rm ,
(1)
where W = [ωι, ω2,∙∙∙ , ωm,]T ∈ Rm×d with Wij 〜N (0,1) corresponds to such two-layer neural network initialized
with random Gaussian weights. Then, the corresponding hypothesis space H is a reproducing kernel Hilbert space
f (x) = √mhθ, σ(Wx∕√d)i, θ ∈ Rm, Wij 〜N(0,1)
(2)
with kf k2L2 = RX |f (x)|2 dρX (x) = hf, ΣmfiH with the covariance operator Σm : Rm → Rm
LρX	X
∑m = I 2(x) 0 2(x)dρχ (x),
X
(3)
which is the usually (uncentered) covariance matrix in finite dimensions,2 i.e., ∑m = Eχ[夕(x) 0 夕(x)]. Clearly, ∑m
♦	1	∙ .1	. . ττ T	1 .1	∙ . 1 .	∙ ∙ . ∙	♦	∙ 1 r∙ 1	ɛ`	ττn	Γ / ∖	/ ∖ 1 ι ʌ C
is random with respect to W, and thus its deterministic version is defined as ∑m = Eχ,w[夕(x) 0 夕(x)]. Define
Jm : Rm → L2ρX such that
(Jmv)(.) = (v,3(∙)i, ∀V ∈ Rm ,
we have ∑m = Jm Jm, where Jm denotes the adjoint operator of Jm1.
SGD with averaging: Regarding the stochastic approximation, we consider the adaptive step-size SGD with iterate
averaging (Dieuleveut & Bach, 2016; Zou et al., 2021; Nitanda & Suzuki, 2020): at each iteration t, after a training
sample (xt,yt)〜P is observed, we update the decision variable as
θt = θt-ι + Yt[yt -hθt-ι,夕(xt)i]夕(xt), t = 1,2,...	(4)
initialized at θ0. Here the step-size is given by γt := γ0t-ζ with ζ ∈ [0, 1), which naturally holds for the constant
step-size case by taking ζ = 0. The final output is defined as the average of the iterates3:
n-1
θn ：=- X θt .
n t=0
2In this paper, we do not distinguish Σm and Σm . This is also suitable to other operators/matrices, e.g., Σem .
3We sum up {θt}tn=-01 with n terms for notational simplicity instead of summarizing {θt}tn=0 with n + 1 terms.
3
Under review as a conference paper at ICLR 2022
The optimality condition for Eq. 4 implies E(x,y)〜ρ[(y - <θ*,夕(x)i)夕(x)] = 0, which corresponds to f * = Jm,θ* if
We assume that f * = arg minf ∈h E(f) exists (See Assumption 1). Likewise, We have ft = Jmθt and f = Jmθn.
In this paper, we study the excess risk Ekfn - f * kL 2 instead of Ekfn - fρkL 2 , that follows (Dieuleveut & Bach,
LρX	LρX
2016; Rudi & Rosasco, 2017; Carratino et al., 2018; Li et al., 2021), as f* is the best possible solution in H and the
mis-specification error kf* - fρk2L2 pales into insignificance. Note that the expectation used here is considered with
LρX
respect to the random features matrix W, and the distribution of the training data {(xt, yt)}n=ι (notethat kfn - f *kL2
LρX
is itself a different expectation over ρX).
Notation: For two operators/matrices, A 4 B means B - A is positive semi-definite (PSD). For any two positive
sequences {at}ts=1 and {bt}ts=1, the notation at . bt means that there exists a positive constant C independent of s
such that at ≤ Cbt, and analogously for 〜,&, and -. For any a, b ∈ R, a ∧ b denotes the minimum of a and b.
3	Main results
In this section, we present our main theoretical results on the generalization properties employing error bounds for bias
and variance of RF regression in high dimensions optimized by averaged SGD.
3.1	Assumptions
Before we present our result, we list the assumptions used in this paper.
Assumption 1. (existence of f*) There exists f* ∈ H such that
f* = arg min E(f) ,
f∈H
Remark: This is a standard assumption in learning theory, e.g., (Rudi & Rosasco, 2017; Carratino et al., 2018).
Assuming the existence of f * ∈ H implies that kf kH is bounded in Eq. 2, which is in fact indispensable and standard.
Assumption 2. (high dimensional assumption) We work in the high dimensional regime for some large d, n with
c 6 {d/n, m/n} 6 C for some constants c, C > 0 such that m, n, d are large and comparable. The data point x ∈ Rd
is assumed to satisfy ∣∣xk2 〜 O(d) and the covariance operator ∑d := Eχ[x 0 x] with bounded spectral norm k∑dk2
(finite and independent of d).
Remark: This is common and standard in high dimensional statistics (El Karoui, 2010; Hastie et al., 2019).
Assumption 3. The activation function σ(∙) is assumed to be Lipschitz continuous.
Remark: This assumption is quite general to cover commonly-used activation functions used in random features and
neural networks, e.g., ReLU, Sigmoid, Sin / cos. Under Assumption 2 and 3, EχV[σ(z)]〜O(1) naturally holds as σ(z)
is sub-Gaussian with O(1) norm (Wainwright, 2019, Theorem 2.26) and its finite second moment, i.e., V[σ(z)]〜 O(1).
Recall ∑m := Ex [夕(x) 0 夕(x)] in Eq. 3 and its expectation ∑m := EW [∑m], we make the following fourth moment
assumption.
Assumption 4 (Fourth moment condition). Assume there exists some positive constants r0, r > 1, such that for any
PSD operator A, it holds that
EW [∑mA∑m] 4 EW (Ex ([夕(x) 0 2(x)]A[夕(x) 0 Hx)]) ) 4 r0Ew [Tr(∑mA)∑m] 4 rTr(Σ mA)Σ m.
Remark: This assumption follows (Zou et al., 2021) that requires the data are drawn from some not-too-heavy-tailed
-1
distribution, e.g., Σm2 X has sub-Gaussian, or sub-exponential tails. We make the following remarks:
1)	The special case for A := I is proved by Lemma 4 (introduced in the next subsection) and thus this assumption is a
natural extension. In fact, there is no need to require that this assumption holds for any PSD operator A (this is just for
description simplicity). Validation on some specific PSD operators A is enough in our proof.
-1
2)	Assuming Σm2 X to be sub-Gaussian/exponential is common in high dimensional statistics (Bartlett et al., 2020).
This condition is much weaker than most previous work on double descent that requires the data to be Gaussian (Hastie
et al., 2019; d’Ascoli et al., 2020b; Adlam & Pennington, 2020; Ba et al., 2020), or uniformly spread on a sphere (Mei
& Montanari, 2019; Ghorbani et al., 2021), see comparisons in Table 1 in Appendix A. In stochastic approximation, the
4
Under review as a conference paper at ICLR 2022
boundeness of the fourth moment is also needed, see (Bach & Moulines, 2013; Dieuleveut & Bach, 2016; Jain et al.,
2018; Berthier et al., 2020; Varre et al., 2021) for details.
Assumption 5 (Noise condition). There exists τ > 0 such that
Ξ := Eχ[ε22(x) 0 2(x)] 4 T2Σm ,
where the noise ε := y 一 f *(x).
Remark: This noise assumption is standard in (Dieuleveut & Bach, 2016; Zou et al., 2021) and holds for the standard
noise model y = f *(x) + ε with E[ε] = 0 and V[ε] < ∞ (Hastie et al., 2019). For proof simplicity, We consider the
well-specified case E[ε∣x] = 0 (can be extended to the model mis-sPecified case E[ε∣x] = 0) and thus y 一 f *(X) is
independent of X .
3.2	Properties of covariance operators
Before we present the main results, we study statistical properties of Σm and Σm by the following lemmas (with proof
deferred to Appendix B), that will be needed for our main result.
Lemma 1. Under Assumption 2,and 3, the CoVanance operator ∑m := Eχ,w [夕(x) 0 2(x)] has the same diagonal
elements
12
(Σm)ii =mExEz〜N(0,kx∣∣2∕d)[σ(Z)] 〜O(I/m), i = 1,2,...,m,
and the same non-diagonal elements
Nmbj = mEx (Ez〜N(0,kxk2∕d) [σ(Z)])〜O(I/m), i,j = 1, 2,...,m with i = j .
Accordingly, Σm has only two distinct eigenValues
λl = (∑m)ii + (m — 1)(∑m)ij ^ O(1) , λ
1
em = (∑m)ii —(∑m)ij =嬴ExV[σ(ζ)]〜O



1
m
Remark: Lemma 1 implies tr(∑m) < ∞. In fact, ExV[σ(z)] > 0 holds almost surely as σ(∙) is not a constant, and
1
thus ∑m is positive definite. Our error bounds will largely depend on 入2 = mmExV[σ(z)].
Here we take several examples by taking various activation functions σ(∙) for demonstration.
1)	If we choose σ(x) = [cos(x), Sin(X)「，RF actually approximates the Gaussian kernel with 夕(x) ∈ R2m in Eq. 1.
In this case, (Σm/% = 1/m, and the non-diagonal element admits (Σm)j = mmEx exp (— kxkL). 2) If we choose
the ReLU activation σ(x) = max{x, 0}, RF actually approximates the first-order arc-cosine kernel (Cho & Saul,
2009) with 夕(x) ∈ Rm. We have (Σm% =合Tr(∑d) and (Σm)j = 2m1d∏Tr(∑d) (recall ∑d := Ex[xx>]). The
calculation of the above two cases can be found in Appendix B.1.
Lemma 2. Under Assumption 2,and 3, random Variables kΣmk2, kΣm 一 Σmk2, and Tr(Σm) are sub-exponential,
and haVe sub-exponential norm at O(1) order.
Lemma 3. Under Assumption 2,and 3, we haVe Σe-m2EW(Σ2m)	〜O(1).
Lemma 4. Under Assumption 2,and 3, there exists a constant r > 0 such that EW (∑m,) 4 Ex,w [夕(x) 0 夕(x) 0
夕(x) 0 2(x)] 4 rTr(∑m)∑m.
Remark: Lemma 4isa special case of Assumption 4 if we take A := I and r :=1 + O (春).
3.3	Results for error bounds
Recall the definition of the noise ε = [ει,…，εn> with εt = yt — f *(xt), t = 1,2,...,n,the averaged excess risk
can be expressed as
Ekfn — f*kLρ,χ := EX,W,εkfn — f*kL^ =Eχ,W ,εhfn — f , ∑m (fn — f *))= Eχ,W ,ε^n, ∑m^) ,
where ηn := n Pn-I ηt with the centered SGD iterate ηt := ft — f *. Following the standard bias-variance decompo-
sition in stochastic approximation (Dieuleveut & Bach, 2016; Jain et al., 2018; Zou et al., 2021), it admits
ηt = ft — f = [I — Yt2(Xt) 0 2(xt)](ft-i — f*) + γtεt2(Xt),
5
Under review as a conference paper at ICLR 2022
where the first term corresponds to the bias by taking yt := f * (Xt)
ηbias = [I - γtψ(χt)乳奴Xt)]ηb-as,	ηbias = f*,	(5)
and the second term corresponds to the variance
ηvar = [I - γtψ(χt) 乳以xt)]ηv-rι + γtεtψ(χt), ηvar = 0.	(6)
Accordingly, we have f = ηbias + η产 + f * due to Eεfn = nnias + f * and kf1感乂 = hf, ∑mfi∙
Proposition 1. Based on the above setting, the averaged excess risk admits the following bias-variance decomposition
EX,W ,εkfn - f *kLρχ = EX,W ,εkfn - Eεfn + Eεfn - f *kLρχ = EX, W hnnias,夕团宿皿)+ EX, W ,εhnnar,夕团联：).
|^^^^^^^^^^^^^^^^^™} f^^^^^~^^^^^^l^^^^^"^^^^"}
:=Bias	:=Variance
By decoupling the multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic
gradients), we give precise non-asymptotic error bounds for bias and variance as below.
Theorem 1. (Error bound for bias) Under Assumptions 1, 2, 3, 4 with r0 > 1, if the step-size γt := γ0t-ζ with
ζ ∈ [0, 1) satisfies
γo <min {τrc∑my, rττ(∑□, 2τr(∑mj}〜O ⑴,	⑺
the Bias defined in Proposition 1 holds
Bias
√E[1 - γor0Tr(∑m)]4
Ilf*k2 〜O (nζ-1).
(8)
Remark: In our paper, I - γt∑m (t = 1,2,...,n) is required to be a contraction map by taking γo < 1∕Tr(∑m).
Though Tr(∑m) is a random variable, the condition γo < 1∕Tr(∑m) can be equivalently substituted by γo <
1∕[cTr(∑m)] for some large c (independent of n, m, d) with exponentially high probability. This is because, Tr(∑m)
is a sub-exponential random variable with O(1) norm in Lemma 2, which makes the constant c unnecessary to be
quite large. For example, the probability with exp(-10) < 10-4 and exp(-100) < 10-43 by taking c = 10, or 100 is
enough small in practice. Accordingly, the condition in Eq. 7 can be equivalently substituted by γo <	1 e for
cr Tr(Σm )
some large c. This is also suitable for estimating Variance.
Theorem 2. (Error bound for variance) Under Assumptions 2, 3, 4 with r0 > 1, and Assumption 5 with τ > 0, if the
step-size γt := γot-ζ with ζ ∈ [0, 1) satisfies Eq. 7, the Variance defined in Proposition 1 holds
Variance
γor0τ2____
PE[1 -γor0n(∑mp
mnζ-1, if m 6 n
γoτ 2, if m > n
ʃ O (mnζ-1) , if m 6 n
O (1) , ifm > n .
(9)
Remark: The error bound for Variance is demonstrated to be unimodal, and converges to O(1) in the over-
parameterized regimes, which matches recent results relying on closed-form solution on (refined) variance, e.g.,
(d’Ascoli et al., 2020b; Adlam & Pennington, 2020; Lin & Dobriban, 2021).
4	Proof Outline and Discussion
In this section, we first introduce the structure of the proofs with high level ideas, and then discuss our work with
previous literature in terms of the used techniques and the obtained results.
4.1	Proof outline
We (partly) disentangle the multiple randomness sources on the data X, the random features matrix W, the noise ε,
make full use of statistical properties of covariance operators ∑m and ∑m in Section 3.2, and provide the respective
(bias and variance) upper bounds in terms of multiple randomness sources, as shown in Figure 1.
Bias: To bound Bias, we need some auxiliary notations.
nbX = (I - γt∑m)nbxι, nbx = f*,	With ∑m = Eχ[P(x) 0 夕(χ)].	(10)
6
Under review as a conference paper at ICLR 2022
Figure 1: The roadmap of proofs.
ηb =(I - Yt∑m)ηbXWl, ηb = f , With Σm = Eχ,W[夕(x)③夕(x)] ,	(11)
with the average ηnX := ɪ Pn-O ηbX and ηnXW := n Pn-OI ηbXW. Accordingly, ηbX can be regarded as a "deterministic”
version of ηbias: we omit the randomness on X (data sampling, stochastic gradients) by replacing [夕(x)夕(x)>] with its
expectation Σm. Likewise, ηtbXW is a deterministic version of ηtvX by replacing Σm with its expectation Σm (randomness
on initialization).
By virtue ofMinkowski inequality, the Bias can be decomposed as Bias . B1 + B2+B3, where B1 := Eχ,w [hnnias -
ηnx, ∑m(ηnias - ηnx)i] and B2 := EW [hηnx - ηnxW, Σm(ηnχ - ηnxW)i] and B3 := hηbxW, ΣmηnxWi. Here B3 is a
deterministic quantity that is closely connected to model (intrinsic) bias without any randomness; while B1 and B2
evaluate the effect of randomness from X and W on the bias, respectively. The error bounds (convergence rates) for
them can be directly found in Figure 1.
To bound B3, we directly focus on its formulation by virtue of spectrum decomposition and integral estimation. To
bound B2, we need study ∣∣ηbX -喈叫2 . ∣∣∑mk2kf *k in Lemma 6. To bound B1,it can be further decomposed as (here
we use inaccurate expression for description simplicity) B1 . Pt kηtbX - ηtbXWk22 + Pt EX kHtk2 in Lemma 7, where
Ht-1 := [Σm -夕(Xt) % 夕(Xt )]ηbx 1. The first term can be upper bounded by PtIInbX - ηbxw ∣∣2 . Tr(∑m)nζ ∣∣f*∣∣2
in Lemma 8, and the second term admits Pt EXIlHtk2 . Tr(∑m)kf *∣∣2 in Lemma9.
Variance: To bound Variance, we need some auxiliary notations.
nvx := (I - Yt∑m)nvx 1 + γtεtψ(xt), nvx = 0,	With ∑m = Ex [P(x) % 夕(x)].	(12)
nVXw := (I - γt∑m)nVxw1 + Ytεt2(Xt), nVxw = 0, With Σm = Ex,w[P(x) % 夕(x)],	(13)
with the averaged quantities N := ɪ Pn-I nVX, nnnxW := ɪ Pn-I nVXW. Accordingly, nVX can be regarded as a
"semi-stochastic" version of ηtvar : we keep the randomness due to the noise εt but omit the randomness on X (data
sampling) by replacing [夕(x)夕(x)>] with its expectation Σm,. Likewise, nVXw can be regarded as a "semi-stochastic"'
version of ηtvX by replacing Σm with its expectation Σm (randomness on initialization).
By virtue of Minkowski inequality, the Variance can be decomposed as Variance . V1 + V2 + V3, where
vι :=	ExW,ε[hnnar	-	nnx,∑m(nnar	-	nnx)i],	v2 :=	EXW,ε[hfιnX	-nnxW,∑m(nnx	-nnxW)i], and V3 :=
Ex,w,εhnnXW, ∑mfnXWi. Though V1, V2, V3 still interact the multiple randomness, V1 disentangles some random-
ness on data sampling, v2 discards some randomness on initialization, and v3 focuses on the "minimal" interaction
between data sampling, label noise, and initialization. The error bounds for them can be found in Figure 1.
To bound V3, we focus on the formulation of the covariance operator CtvXW := EX,ε[ftvXW % ftvXW] in Lemma 10 and the
statistical properties of Σem and Σm. To bound V2, we need study the covariance operator CtvX-W := EX,ε[(ftvX -ftvXW)%
7
Under review as a conference paper at ICLR 2022
(ηtvX - ηtvXW)] admitting kCtvX-Wk . kI + Σ-m2Σ2mk2Tr(Σm) in Lemma 11. To bound V1, we need study the covariance
operator Cv-X := EX,ε [(ηVar - ηVX) 0 (ηVar - ηVX)], as a function of Z ∈ [0,1), admitting Trev-X(Z)] 6 Tr[CV-X(0)]
in Lemma 5, and further Ctv-X - Tr(Σm)I in Lemma 12.
4.2	Discussion with previous work
Difference in techniques: Our proof framework follows (Dieuleveut & Bach, 2016) that focuses on kernel regression
with stochastic approximation in the under-parameterized regimes (d is regarded as finite and much smaller than n).
Nevertheless, even in the under-parameterized regime, their results can not be directly extended to random features
model due to the extra randomness on W , coupling with other randomness sources on noise and data sampling, which
makes their proof framework invalid on some points. To be specific, their results depend on (Bach & Moulines, 2013,
Lemma 1) by taking conditional expectation to bridge the connection between E(kαtk2) and Ehαt, Σmαti. This is
valid for B1 but expires on other quantities. Besides, the results in (Carratino et al., 2018) on RF with SGD in the
under-parameterized regimes depend on Tikhonov regularization in an approximation theory view, which appears
invalid for our interpolation learning without any (explicit) regularization.
Some technical tools used in this paper follow (Zou et al., 2021) that focuses on linear regression with constant step-size
SGD in over-parameterized regime, e.g., PSD operators and boundedness of Ctv-X when ζ = 0 in Lemma 12. However,
coupling with multiple randomness sources and adaptive step-size setting (no longer a homogeneous markov chain)
make our analysis intractable. Besides, their results demonstrate that linear regression with SGD generalizes well
(converges with n) but has few findings on double descent. Instead, our result depends on n and m (where d is implicitly
included in m), and is able to explain double descent.
Comparison with previous work: Compared to (Ba et al., 2020) on RF optimized by gradient descent under the
Gaussian data in an asymptotic view, our non-asymptotic result holds for more general data distribution under the
SGD setting. In fact, our data assumption is weaker than most previous work assuming the data to be Gaussian,
uniformly spread on a sphere, or isotropic/correlated features (with spectral decay assumption), except (Liao et al.,
2020). Nevertheless, we extend their asymptotic results relying on the least-squares closed-form solution to non-
asymptotic results under the SGD setting. Compared to (Li et al., 2021) relying on closed-form solution with correlated
features, our result for bias achieves O(1/n) rate under the constant step-size SGD setting, which is better than their
θ(ʌ/logn/n) rate. Their result on variance requires m 6 O (nd) to generalize well while our result does not need this
condition. Besides, our result coincides several findings with refined variance decomposition in (d’Ascoli et al., 2020b;
Adlam & Pennington, 2020; Lin & Dobriban, 2021): the interaction effect can dominate the variance (between samples
and initialization); the unimodality of variance is a prevalent phenomenon.
5	Numerical Validation
In this section, we provide some numerical experiments in Figure 2 to support our theoretical results and findings. Note
that our results go beyond Gaussian data assumption and can be empirically validated on real-world datasets.
山SIΛI Jsg
0O 0.5	1	1.5	2
m/n
(b) step-size
Figure 2: Test MSE (mean±std.) of RF regression as a function of the ratio m/n on MNIST data set (digit 3 vs. 7)
across the Gaussian kernel, for d = 784 and n = 600 in (a) and (b). The interpolation threshold occurs at 2m = n due
to σ(Wx) ∈ R2m. Under this setting, the trends of Bias and Variance are empirically given in (c) and (d).
5.1	Behavior of RF for interpolation learning
Here we evaluate the test mean square error (MSE) of RF regression on the MNIST data set (Lecun et al., 1998) with
minimal-norm solution and adaptive step-size SGD to study its generalization properties, see Figure 2(a) and 2(b).
8
Under review as a conference paper at ICLR 2022
Experimental settings: We take digit 3 vs. 7 as an example, and randomly select 300 training data in these two classes,
resulting in n = 600 for training. Hence, our setting with n = 600 and d = 784 satisfies our realistic high dimensional
assumption. The Gaussian kernel k(x, x0) = exp(-∣∣x - x0∣∣2∕(2σ2)) is used, where the kernel width σ0 is chosen
as σ0 = d in high dimensional settings such that ∣∣x∣∣2∕d 〜O(l) in Assumption 2. In our experiment, each sample
is normalized to zero-mean with deviations to 1. The initial step-size is set to γ0 = 1 and the initial optimization
parameter θ0 is set to the min-norm solution4 corrupted with zero-mean, unit-variance Gaussian noise, which is used to
evaluate their difference on test error. The experiments are repeated 10 times and the test MSE (mean±std.) can be
regarded as a function of the ratio m∕n by tuning m.
SGD vs. minimal-norm solution: Figure 2(a) shows the test MSE of RF regression with averaged SGD (we take
ζ = 0.5 as an example; red line) and minimal-norm solution (blue line). First, we observe the double descent
phenomenon: a phase transition on the two sides of the interpolation threshold at 2m = n when these two optimization
algorithms are employed. Second, in terms of test error, RF with averaged SGD is slightly inferior to that with min-norm
solution, but still generalizes well.
Different step-size: Figure 2(b) shows the test error of RF regression with averaged SGD under three different step-size
settings, i.e., ζ = 0 (red line), ζ = 0.5 (blue line), and ζ = 0.9 (green line). It is not surprising to observe the double
descent phenomenon on these three settings. The constant step-size setting (i.e., ζ = 0) achieves the best generalization
performance in the over-parameterized regime, narrowly followed by the other two adaptive step-size settings, which
experimentally validates the effectiveness of averaged SGD for interpolation learning.
5.2	Behavior of our error bounds
We have experimentally validate the phase transition and corresponding double descent in the previous section, and
here we aim to semi-quantitatively assess our derived bounds for Bias and Variance, see Figure 2(c) and 2(d).
Experimental settings: Since the target function f *, the covariance operators ∑d, ∑m, and the noise ε are unknown on
the MNIST data set, we need some extra assumptions/settings to calculate Bias and Variance for our experimental
evaluation. First, We assume the label noise ε 〜N(0,1), which can in turn obtain f *(x) on both training and test data
due to f *(x) = y - ε. Second, the covariance matrices ∑d and ∑m are estimated by the related sample covariance
matrices. When using the Gaussian kernel, the covariance matrix Σm can be directly computed, i.e., (Σm)ii = 1∕m
and (Σ m)j = mm Ex exp (- kx-k2^, where the expectation is approximated by Monte Carlo sampling with n training
samples. Accordingly, based on the above results, we are ready to calculate ηtbias in Eq. 5, ηtbX in Eq. 10, and ηtbXW in
Eq. 11, respectively, which is further used to approximately compute B1 := Eχ,w [hηnias - η]X, Σm(ηnias -肃')》]
(red line) and B2 := EW [[成X - ηnXW, ∑m(ηnX -请*")》](blue line) and B3 :=(请囚,Σ、成第(reen line). The
(approximate) computation for Variance can be similar achieved by this process.
Error bounds for bias: Figure 2(c) shows the trends of (scaled) B1, B2, and B3. Recall our error bound: B1, B2,
B3 〜O(nζ-1) with Z = 0.5 in our experiment. We find that, all of them monotonically decreases when m increases
from the under-parameterized regime to the over-parameterized regime. These experimental results coincide with our
error bound on them, i.e., converging with n at some certain rate (m and n are in the same order in our experiment).
Error bounds for variance: Figure 2(d) shows the trends of (scaled) V1, V2, and V3. Recall our error bound: in the
under-parameterized regime, V1, V2, and V3 increases with m at a certain O(nζ-1m) rate; and in the over-parameterized
regime, V1 and V2 are in O(1) order while V3 decreases with m. Figure 2(d) shows that, when 2m < n, V1 and V2
monotonically increases with m and then remain unchanged when 2m > n. Besides, V3 is observed to be unimodal:
firstly increasing when 2m < n, reaching to the peak at 2m = n, and then decreasing when 2m > n, which admits the
phase transition at 2m = n. Accordingly, these findings accord with our theoretical results, and also matches refined
results in (d’Ascoli et al., 2020b; Adlam & Pennington, 2020; Lin & Dobriban, 2021): the unimodality of variance is a
prevalent phenomenon.
6	Conclusion
We present the non-asymptotic results for RF regression under the averaged SGD setting for understanding benign
overfitting. Our theoretical and empirical results demonstrate that, the error bounds for variance and bias can be
unimodal and monotonically decreasing, respectively, which is able to recover the double descent phenomenon.
Regarding to constant/adaptive step-size setting, there is no difference between the constant step-size case and the exact
minimal-norm solution on the convergence rate; while the adaptive step-size case will slow down the learning rate, but
does not change the error bound for variance in over-parameterized regime that remains O(1) order.
4In our numerical experiments, we take the regularization parameter fixed with 10-8 to avoid non-singular.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
In this paper, we focus on generalization properties of random features models trained with SGD, working in a
practical setting where n, m, d are large and comparable. The derived theoretical results in terms of optimization and
generalization would have an important positive impact on over-parameterized models, e.g., deep neural networks. Our
theoretical framework presents fair and non-offensive societal consequence.
Reproducibility S tatement
We derived exact non-asymptotic error bounds for high dimensional RF regression trained with SGD in under/over-
parameterized regimes. To support our theoretical results, in our main text, we have discussed that the assumptions used
in this paper are fair and attainable; provided a proof roadmap of our proof framework; and detailed the experimental
settings, e.g., data processing, training/test split, and parameter selection. In the appendix, we have provided the
complete proof of our theoretical claims.
References
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-variance decomposition.
In Advances in neural information processing systems, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In
International Conference on Machine Learning,pp. 242-252. PMLR, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning,
pp. 322-332, 2019.
Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-layer neural
networks: an asymptotic viewpoint. In International Conference on Learning Representations, pp. 1-8, 2020.
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n).
Advances in Neural Information Processing Systems, 26:773-781, 2013.
Peter L. Bartlett, Philip M. Long, Ggbor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. the
National Academy of Sciences, 2020.
M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. SIAM J. Math. Data Sci., 2(4):
1167-1180, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the
classical bias-variance trade-off. the National Academy of Sciences, 116(32):15849-15854, 2019.
Raphael Berthier, Francis Bach, and Pierre Gaillard. Tight nonparametric convergence rates for stochastic gradient
descent under the noiseless linear model. In Advances in Neural Information Processing Systems, volume 33, pp.
2576-2586, 2020.
Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with SGD and random features. In Advances in
Neural Information Processing Systems, pp. 10212-10223, 2018.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own generalization curve.
arXiv preprint arXiv:2008.01036, 2020a.
Xi Chen, Qiang Liu, and Xin T Tong. Dimension independent generalization error by stochastic gradient descent. arXiv
preprint arXiv:2003.11196, 2020b.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the
logistic loss. In Conference on Learning Theory, pp. 1305-1338, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Advances in
Neural Information Processing Systems, pp. 2933-2943, 2019.
10
Under review as a conference paper at ICLR 2022
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural Information Processing
Systems,pp. 342-350, 2009.
Felipe Cucker and Dingxuan Zhou. Learning theory: an approximation theory viewpoint, volume 24. Cambridge
University Press, 2007.
Oussama Dhifallah and Yue M Lu. A precise performance analysis of learning with random features. arXiv preprint
arXiv:2008.11904, 2020.
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes. Annals of
Statistics, 44(4):1363-1399, 2016.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for
least-squares regression. Journal of Machine Learning Research, 18(1):3520-3570, 2017.
S. d’Ascoli, M. Refinetti, G. Biroli, and F. Krzakala. Double trouble in double descent: Bias and variance (s) in the lazy
regime. pp. 2280-2290, 2020a.
StePhane d'Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and
variance (s) in the lazy regime. pp. 2280-2290, 2020b.
Noureddine El Karoui. The spectrum of kernel random matrices. Annals of Statistics, 38(1):1-50, 2010.
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborovd. Generalisation error in
learning with random features and the hidden manifold model. In International Conference on Machine Learning,
pp. 3452-3462, 2020.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in
high dimension. The Annals of Statistics, 49(2):1029-1054, 2021.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional ridgeless
least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Alfred Horn. Doubly stochastic matrices and the diagonal of a rotation matrix. American Journal of Mathematics, 76
(3):620-630, 1954.
Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features. arXiv preprint
arXiv:2009.07669, 2020.
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning
dynamics of neural networks. In Advances in Neural Information Processing Systems, volume 33, pp. 17116-17128,
2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in Neural Information Processing Systems, pp. 8571-8580, 2018.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, Venkata Krishna Pillutla, and Aaron Sidford. A
markov chain theory approach to characterizing the minimax optimality of stochastic gradient descent (for least
squares). arXiv preprint arXiv:1710.09430, 2017.
Prateek Jain, Sham Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic gradient
descent for least squares regression: mini-batching, averaging, and model misspecification. Journal of Machine
Learning Research, 18, 2018.
Peizhong Ju, Xiaojun Lin, and Ness B. Shroff. On the generalization power of overfitted two-layer neural tangent kernel
models. In International Conference on Machine Learning, pp. 5137-5147. PMLR, 2020.
Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep neural networks of
practical sizes. In IEEE Conference on Communication, Control, and Computing, pp. 92-99. IEEE, 2019.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Zhu Li, Jean-Francois Ton, Dino Oglic, and Dino Sejdinovic. Towards a unified analysis of random Fourier features. In
the 36th International Conference on Machine Learning, pp. 3905-3914, 2019.
11
Under review as a conference paper at ICLR 2022
Zhu Li, Zhi-Hua Zhou, and Arthur Gretton. Towards an understanding of benign overfitting in neural networks. arXiv
preprint arXiv:2106.03212, 2021.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and
restricted lower isometry of kernels. In Annual Conference on Learning Theory, pp. 1-32, 2019.
Zhenyu Liao, Romain Couillet, and Michael Mahoney. A random matrix analysis of random fourier features: beyond the
gaussian kernel, a precise phase transition, and the corresponding double descent. In Neural Information Processing
Systems, 2020.
Licong Lin and Edgar Dobriban. What causes the test error? going beyond bias-variance via anova. Journal of Machine
Learning Research, 22(155):1-82, 2021.
Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan A.K. Suykens. Random features for kernel approximation: A
survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp.
1-20, 2021a. doi: 10.1109/TPAMI.2021.3097011.
Fanghui Liu, Zhenyu Liao, and Johan A.K. Suykens. Kernel regression in high dimensions: Refined analysis beyond
double descent. In International Conference on Artificial Intelligence and Statistics, pp. 649-657, 2021b.
Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks. The Annals of
Applied Probability, 28(2):1190-1248, 2018.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and
double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-
free bounds and kernel limit. In Conference on Learning Theory, pp. 2388-2464. PMLR, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent:
Where bigger models and more data hurt. In International Conference on Learning Representations, 2019.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of optimization and implicit
regularization in deep learning. arXiv preprint arXiv:1705.03071, 2017.
Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural tangent kernel
regime. In International Conference on Learning Representations, 2020.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information
Processing Systems, pp. 1177-1184, 2007.
Jason W Rocks and Pankaj Mehta. Memorizing without overfitting: Bias, variance, and interpolation in over-
parameterized models. arXiv preprint arXiv:2010.13933, 2020.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In Advances in
Neural Information Processing Systems, pp. 3215-3225, 2017.
Samuel L. Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization in stochastic
gradient descent. In International Conference on Learning Representations, 2020.
Aditya Varre, Loucas Pillaud-Vivien, and Nicolas Flammarion. Last iterate convergence of sgd for least-squares in the
interpolation regime. arXiv preprint arXiv:2102.03183, 2021.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University
Press, 2019.
Christopher KI Williams. Computation with infinite neural networks. Neural Computation, 10(5):1203-1216, 1998.
Denny Wu and Ji Xu. On the optimal weighted `2 regularization in overparameterized linear regression. In Advances in
Neural Information Processing Systems, pp. 10112-10123, 2020.
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-off for generaliza-
tion of neural networks. In International Conference on Machine Learning, 2020.
12
Under review as a conference paper at ICLR 2022
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. Advances in
Neural Information Processing Systems, 32:2055-2064, 2019.
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Benign overfitting of constant-
stepsize sgd for linear regression. In Conference on Learning Theory, 2021.
13
Under review as a conference paper at ICLR 2022
Appendix
The outline of the appendix is stated as follows.
•	Section A summarizes representative results on random features regarding to double descent under various
settings.
•	Section B provides the proofs of lemmas in Section 3.2 on statistical properties of Σm and Σm.
•	Appendix C introduces preliminaries on PSD operators in stochastic approximation.
•	Section D provides estimation for several typical integrals that are needed for our proof.
•	Section E gives error bounds for Bias.
•	In Section F, we provide the error bounds for Variance.
A Comparisons with previous work
According to the used data assumption, the type of solution, and the derived results, we summarize various representative
approaches in Table 1.
Table 1: Comparison of problem settings on analysis of high dimensional random features on double descent.
	data assumption	solution	result
(Hastie et al., 2019)	Gaussian	closed-form	variance % &
(Ba et al., 2020)	Gaussian	GD	variance % &
Mei & Montanari (2019)	i.i.d on sphere	closed-form	variance, bias % &
(d’Ascoli et al., 2020b)	Gaussian	closed-form	refined 2
(Gerace et al., 2020)	Gaussian	closed-form	%&
(Adlam & Pennington, 2020)	Gaussian	closed-form	refined
(Dhifallah & Lu, 2020)	Gaussian	closed-form	%&
Hu & Lu (2020)	Gaussian	closed-form	%&
(Liao et al., 2020)	general	closed-form	%&
(Lin & Dobriban, 2021)	isotropic features With finite moments	closed form	refined
(Li et al., 2021)	correlated features With polynomial decay on Σd	closed form	interpolation learning
Ours	(at least) sub-exponential data	SGD	variance % &, bias &
1 A refined decomposition on variance is conducted by sources of randomness on data sampling, initialization, label noise to
possess each term d’Ascoli et al. (2020a) or their full decomposition in Adlam & Pennington (2020); Lin & Dobriban (2021).
We mainly discuss the used assumption on data distribution here. It can be found that, most papers assume the data to
be Gaussian or uniformly distributed on the sphere. The following papers admit weaker assumption on data. Given a
correlated features model that is commonly used in high dimensional statistics (Hastie et al., 2019):
1	丁
X = ∑d t, E[ti] = 0, V[ti] = 1, With ∑d := Eχ[xx>],	(14)
where t ∈ Rd has i.i.d entries ti (i = 1, 2, . . . , d) with zero mean and unit variance. In (Li et al., 2021), they further
require that each entry is i.i.d sub-Gaussian and Σd admits polynomial decay on eigenvalues. Lin & Dobriban (2021)
consider isotropic features With finite moment, i.e., taking Σd := I in Eq. 14 and E[ti8+η] < ∞ for any arbitrary
positive constant η > 0. Our model holds for sub-Gaussian and sub-exponential data (at least), and thus the used data
assumption 4 is Weaker than them. In (Liao et al., 2020), it makes no assumption on data distribution but requires that
test data “behave" statistically like the training data by concentrated random vectors. Indeed, their data assumption is
Weaker than ours, but their analysis frameWork builds on the exact closed-form solution from random matrix theory.
Instead, We focus on the SGD setting and thus take a unified perspective on optimization and generalization.
B	Results on covariance operators
T .1 •	. •	.	.1	∕' i' T	TCCd	.	. •	, •	1	. ∙	∕' ∖ '	t ɛ'
In this section, We present the proofs of Lemma 1, 2, 3, 4 on statistical properties of Σm and Σm.
14
Under review as a conference paper at ICLR 2022
B.1 Proof of Lemma 1
Here we present the proof of Lemma 1 and then give two examples by taking different activation functions.
Proof. Recall the definition of Σm , we have
1
Wm := Ex,W[夕(X) X 2(X)] = mEx,Wj〜N(0,1)
σ
∈ Rm×m
ττ τ	• 1 .ι 1 •	1	1	1 •	1 1	. CH	. 1 τ τ	/ ∖ τπ> -m> • .1 • 1	.
We consider the diagonal and non-diagonal elements of Σm, separately. Here We assume σ(∙) : R → R With single-output
for description simplicity, and the results can be easily extended to multiple-output cases, e.g., σ(x) = [cos(x), sin(x)]>
corresponding to the Gaussian kernel.
Diagonal element： The diagonal entry (Σm)n = mEχ,ωjσ( ω√x )σ( ω√x )>] = mEχEω [σ( ω√x )]2 is the same. In fact,
Eω [σ (ω√dx)] is actually a one-dimensional integration by considering the basis (e1,e2,…，ed) with eι = x∕kx∣∣2,
and e2,…,ed any completion of the basis. This technique is commonly used in (Williams, 1998; Louart et al., 2018).
The random feature ω admits the coordinate representation ω = ω1e1 + ω2e2 +------+ B&ed, and thus
ω>x = (ω1e1 + Me +---------+ ωded)>(∣∣x∣∣eι) = kx∣Q1,
Which implies
Ez〜N(0,kχk2∕d)[σ(z)]2 .
exp (-2Ilωk2^ dω
Thatmeans, (ςm)ii = m∙ExEz〜N(0,kχk2/d)[σ(Z)R
TkT	1 ♦	■	■	， ml	1 ∙	1	.	/ H ∖
Non-diagonal element: The non-diagonal entry (Σm)ij
m Ex,ωi,ω∕σ( ω√χ )σ( ω√X )>] = m Ex [Eω σ( ω√x )]2 is the
same due to the independence betWeen ωi and ωj . LikeWise, it can be represented as a one-dimensional integration
1
(Wm)ij =	Ex
m
Ez 〜N (0,kχk2∕d)[σ(ζ)]
2
Accordingly, by denoting a := (Σm)ii and b := (Σm)ij, the covariance operator Σm can be represented as
■~'	~Γ	-------
Σem = (a-b)Im+b11> ∈Rm×m,	(15)
with its determinant det(Σm) = (1 + ambb)(a 一 b)m. Hence, the eigenvalues of Σm can be naturally obtained by the
matrix determinant lemma: λ1(Σm) = a 一 b + bm and the remaining eigenvalues are a 一 b, which concludes the
proof.	□
Here we give the calculation details for the Gaussian kernel that corresponds to the sin ∕ cos activation function
σ(x) = [cos(x), sin(x)]> and arc-cosine kernel that corresponds to the ReLU function σ(x) = max{0, x}.
Regarding to the Gaussian kernel, by virtue of E[cos(a>z)] = cos(μ>Z) exp(-ɪz> Az) for a 〜 N(μ, A) and
ωi 一 ωj 〜N(0, 2Id), we have (for the non-diagonal element)
■~■
-1 Exω-ω-
x,ωi ,ωj
mj
1	X> (ωi 一 ωj )	1	IXI22
mExEωi,3j kos( —√d-)] = mExexp 一一-d-).
The diagonal element admits (Σm)ii = 1∕m.
Regarding to the first-order arc-cosine kernel, denote e := max{0, z} with Z 〜N(0, ∣x∣2∕d), it is subject to the
Rectified Gaussian distribution admitting (Li et al. (2021) also present this)
E[Ze]
IXI2
E[Ze]2
kχk2
2d
V[Ze]
kχk2
2d
15
Under review as a conference paper at ICLR 2022
Accordingly, the diagonal element is
(Σm)ii = Ez〜N(0,kxk2∕d)[σ(Z)F = 2mdExkxk2 = 2mdTrNd),
and the non-diagonal element is
1	21
Nm)ij = mEx(Ez〜N(0,kxk2∕d) [σ(z)]J = 2md∏Tr(Ed)，
with the covariance operator Σd := Ex [xx>].
B.2 Proof of Lemma 2
Proof. According to (Wainwright, 2019, Theorem 2.26), by virtue of the LiPschitz function σ(∙) of Gaussian variables,
we have
P
-Eω〜N(0,Id)σ
6 c exp(-t2 ),
∀t > 0 ,
which implies that σ (ω√>χ) is a sub-Gaussian random variable due to its expectation in the O(1) order. Accordingly,
kΣm - Σm k2 is a sub-exponential random variable with
>
1
m
kΣm - Σmk2 6 kΣmk2 + kΣmk2
+ O(1)
2
σ
6 —Ex
m
2
+ O(1) [Jensen’s inequality]
2


Eχ fExkσ(0m)k2 + Ex U-√χU ) + O(1) [σ: Lipschitz continuous]
1m
md∑
x>]ωi	[using kΣdk2 < ∞]
i=1
.dI3∣2 [here ω 〜N(0,Id)],
where kωk22 is a χ2(d) random variable, and thus kΣm - Σe mk2 has sub-exponential norm O(1). Accordingly, the high
moment EkΣm k2p < ∞ holds for finite p. Following the above derivation, we can also conclude that Tr(Σm) has the
sub-exponential norm O(1), i.e.
Tr(Em) = g ExTr
2
2
.dkωk2.
Likewise, we can derive Tr(E2m) < ∞ in the similar fashion. Besides, our work needs the error bound for the smallest
eigenvalue of ∑m, i.e., λm,. By virtue of Schur-Horn theorem (Horn, 1954), λm, admits
λm 6 min	(Em)ii 〜O
i∈{1,2,...,m}
□
B.3 Proof of Lemma 3
1
Proof. The remark in Lemma 1 demonstrates that Em is positive definite and thus E-m1 exists. Recall the formulation
of Em in Eq. 15 with a := (Em)ii and b := (Em)ij, by virtue of Sherman-Morrison formula, the inverse of Em is
(Ee m)-1
1
a-b
b
a - b + bm
16
Under review as a conference paper at ICLR 2022
Accordingly, we have
Σe-m2EW(Σ2m)2= EW[Σe-m2Σ2m]26EWΣe-m1Σm22
EW
6 Ew (呼)2~O⑴,
[using λ 〜O(ml) in Lemma 1]
2
EW
2
I3∣2 ʌ
d )
[here ω 〜N(0, Id) in Lemma 2]
due to b 〜O(1∕m), λ1 〜O⑴ in Lemma 1, and ∣∣ωk2 〜χ2(d).
□
B.4 Proof of Lemma 4
Proof. The first inequality naturally holds, and so We focus on the second inequality. Denote Φ := Eχ,w [夕(x) 0
夕(x) 0 夕(x) 0 夕(x)], its diagonal elements are the same
φii=Imm2~ Ex(Ez~N (0,kx∣∣2∕d)[σ(Z)F) + m ExEz-N (0,kχ∣∣2∕d)[σ(Z)]4 ~ O (m
Its non-diagonal elements Φij With i 6= j are the same
m- 3
φij =	Ex
(Ez〜N(0,kxk2∕d)[σ(Z)]) Ez〜N(0,kxk2∕d)[σ(Z)]2 + m2 Ex E
z-N (0,kxk22∕d)[σ(Z)]3Ez-N (0,kxk22∕d)[σ(Z)] ,
where the first term is in O(春)order and the second term is in O(焉)order. By denoting a := (Σm)n, b := (Σm)ij
as given by Lemma 1, A := Φii, and B := Φij, the operator rTr(Σem )Σem - Φ can be represented as
rTr(Σe m	)Σm	Φ =	[rm(a	b)	A +	B] Im	+	(rmab	B)11> ,
of which the smallest eigenvalue is rma(a - b) - A + B. Accordingly, to ensure the positive definiteness of
rTr(Σm)Σm — Φ, which implies EW [夕(x) 0 夕(x)]A[夕(x) 0 夕(x)])) 4 rTr(∑m)∑m, we require its smallest
eigenvalue is non-negative, i.e., rma(a	b)	A + B > 0. That means, r should satisfies
A—B	A—B
r > ---；-----=-；--------------------:——------——-
ma(a — b)	m1 ExEz 〜N (0,kxk2∕d)[σ(Z)]2ExV[b(z)]
(16)
Since A — B admits
A — B 6 ɪExEz[σ(Z)]2ExV[σ(Z)] + O
m
then by taking r :=1 + O (m),the condition in Eq. 16 satisfies, and thus rTr(Σm)Σm — Φ is positive definite, which
concludes the proof.	□
C Preliminaries on PSD operators
In this section, we first define some stochastic/deterministic PSD operators that follow (Jain et al., 2017; Zou et al.,
2021) in stochastic approximation, and then present Lemma 5 that is based on PSD operators and is needed to estimate
B1 and V1. Note that, the PSD operators will make the notation in our proof simple and clarity but do not change the
proof itself.
17
Under review as a conference paper at ICLR 2022
Following (Jain et al., 2017; Zou et al., 2021), we define several stochastic PSD operators as below. Given the random
features matrix W , define (for any PSD operator A)
SW := Eχ[夕(x) 0 夕(x) 0 夕(x) 0 夕(x)], SW := Σm 0 ∑m ,
SW ◦ A := Ex [g(x)>g(x)Ag(x) 0 g(x)] , SW ◦ A := ∑mA∑m .
Besides, for any γt (t = 1, 2, . . . , n), define the following operators
(I —	YtTW)	◦ A := Ex [I	—	YtP(x) 0	夕(x)]A[I	—	YtP(x)	0	2(x)],	(I — YtTW)	◦ A := (I — γt∑m)A(I —	γt∑m),
associated with two corresponding operators (that depend on γt)
TW := Σm0I+I0Σm	—	YtSW,	TeW	:=	Σm0I+I0Σm	—	YtSeW.
Clearly, the above operators SW, SeW, (I — YtTW), (I — YtTeW), TW, and TeW are PSD, and SW < SeW. The proof is similar
to (Zou et al., 2021, Lemma B.1) and thus we omit it here.
Further, if γo < 1∕Tr(∑m), I — γi∑m (i = 1,2,...,n) is a contraction map, and thus for any PSD operator A and
step-size Yi , the following exists
∞∞
X(I — YiTeW)t ◦ A = X(I — YiΣm)tA(I — YiΣm)t.
Hence, (TeW)-1 := Yi Pt=0 (I — YiTeW)t exists and PSD.
Based on the above stochastic operators, we define several deterministic PSD ones by taking the expectation over W as
below. For any given Yi (i = 1, 2, . . . , n), we have the following PSD operators
S := EW [Σm 0 Σm], S := Σm 0 Σm ,
T := Σm 0 I + I 0 Σm — YiS, T := Σm 0 I + I 0 Σm — YiS ,
S ◦ A := EW [ΣmAΣm], S ◦ A := ΣmAΣm ,
(I — YiT)	◦ A := EW [(I	—	YiΣm)A(I — YΣm)],	(I	—	YiTe)	◦ A := (I —	YiΣem)A(I —	YiΣem) ,
which implies Te — T = Yi(S — Se).
Based on the above PSD operators, we present a lemma here that is used to estimate B1 and V1.5
Lemma 5. Under Assumptions 1, 2, 3, 4 with r0 > 1, denote
tt
Dv-X := X Y (I - YiTW) ◦ γSB∑m，
s=1 i=s+1
(17)
with a scalar B independent of k, if the step-size Yt := Y0t-ζ with ζ ∈ [0, 1) satisfies
Y0 < min
roτ⅛), 2⅛y}
then Dtv-X can be upper bounded by
Dtv-X 4
YoB	i
1 — Yor0Tr(∑m) .
Remark: The PSD operator I — YiTW cannot be guaranteed as a contraction map since we cannot directly choose
Yo < ns®)奴x)>] for general data x. However, its summation in Eq. 17 can be still bounded by our lemma. In our
work, we set B := r0Tr(Σm) for estimate B1, and B := τ 2r0Y0[Tr(Σm) + Y0Tr(Σ2m)] to bound V1, respectively.
Proof. Our proof can be divided into two parts: one part is to prove Tr[Dtv-X] 6 Tr[Dtv-X(0)] for any ζ ∈ [0, 1); the
other part is to provide the upper bound of Dtv-X(0). We focus on the first part and the proof in the second part follows
(Jain et al., 2017, Lemmas 3 and 5) and (Zou et al., 2021, Lemma B.4).
5Our proofs on the remaining quantities including V2, V3, B2, B3 do not use PSD operators.
18
Under review as a conference paper at ICLR 2022
Denote the constant step-size setting (special case) with ζ = 0 for Dtv-X as
t
Dtv-X(0) := X(I - γ0T W)t-s ◦ γ02BΣm.
s=1
The quantity Tr[Dtv-X] admits the following representation by the definition of I - γiTW
tt
Tr[Dtv-X] =XY Tr (I - γiTW) ◦ γs2BΣm
s=1 i=s+1
= Xt Bγs2	Yt	Tr (Ex[I	- Yi2(x) 0	2(x)]∑m[I	-	Yi2(x)	0	2(x)])
s=1	i=s+1
tt
= B X Ys2 Y Tr ( ∑m - 2γi∑m + γi2∑mEx [P(x) 0 2(x) 0 2(x) 0 2(x)]
s=1	i=s+1
Based on the above results, we have
tt
Tr[Dtv-X(0)]-Tr[Dtv-X] =BXY Tr Σm
s=1 i=s+1
- Ys2 )I - 2(Y03 - Ys2Yi)Σm
+ (Y4 - Y2Y2)Ex S(X) 0 中(X) 0 中(X) 0 3(X)]
tt
>BXY Tr Σm (Y02 - Ys2)I - 2(Y03 - Ys2Yi)Σm + (Y04 - Yi2Ys2)Σ2m
s=1 i=s+1
t tm
= BXYX λj (Y02 - Ys2) - 2(Y03 -Ys2Yi)λj + (Y04 - Yi2Ys2)λj2
s=1 i=s+1 j=1
t tm
BXYX	λjh(Y04-Yi2Ys2) λj -
s=1 i=s+1 j=1
—
—
Accordingly, TrDv-X(0)] - TrDv-X] > 0 holds if λi 6 γ3-γ2γ4-γ2γ2+γ0γ2. This condition can be satisfied by
γ0 -γs γi
λi 6 TrNm) 6 ~1 6
2Y0
where the second inequality holds by γo 6 2丁危).
Y2Yi - Y0Ys + Y0Y2
Y4 - Y2Y2
—
In the next, we give the upper bound for Dtv-X(0). The proof follows (Jain et al., 2017, Lemmas 3 and 5) and (Zou
et al., 2021, Lemma B.4). We just present it here for completeness. We firstly demonstrate thatDtv-X (0) is increasing
and bounded, which implies that the limitD∞v-X (0) exists, and then we seek for the upper bound of this limit. To be
specific, Dtv-X(0) admits the following expression
t-1
Dtv-X(0) := X(I - Y0TW)k ◦ Y02BΣm =Dtv--1X(0) + (I - Y0TW)t-1 ◦ Y02BΣm < Dtv--1X(0),
k=1
which implies that Dtv-X(0) is increasing.
Let At := (I - Y0TW)t-1 ◦ BΣm, and then At = (I - Y0T W) ◦ At-1 . We have
Tr(At) = Tr[(I - YoTW) ◦ At-ι] = Tr(At-I) - 2γoTr(∑mAt-1) + γ2Tr(Sw ◦ At-1)
6 Tr(At-1 ) - 2Y0Tr(ΣmAt-1 ) + Y02r0Tr(ΣmAt-1 )Tr(Σm) [using Assumption 4]
6 Tr[(I - γo∑m)At-ι] 6 (1 - γoλm)Tτ(At-1),	[using γo 6 …也小)]
which implies
∞∞
Tr[Dv-X(0)] 6 γ2 X Tr ((I - γ°TW)t ◦ B∑m) 6 Tr(B∑m) X(I- γoλm)t 6
γoTr(B∑m)
λm
<∞.
19
Under review as a conference paper at ICLR 2022
Accordingly, the monotonicity and boundedness of {Dtv-X(0)}t∞=0 implies that the limit exists, denoted as D∞v-X(0)
with
D∞v-X(0) = (I - γ0T W) ◦ D∞v-X(0) + γ02BΣm,
which implies D∞v-X(0) = γ0 (TW)-1 ◦ BΣm Accordingly, we have
T ◦ D∞-x(0) = TW ◦ Dv-X(O) + YoSW ◦ Dv-X(O) - YoSW ◦ Dv-X(O) [definition of TW]
= γ0BΣm + γ0SW ◦ D∞v-X(0) - γ0SeW ◦ D∞v-X(0)	(18)
4 YoBΣm + YoSW ◦ D∞v-X(O). [usingSW < SeW]
Besides, (TeW)-1 ◦ Σm can be bounded by
∞∞
(TW)TO Σm = Yo X(I - YOTW) ◦ Σm = γo £(I - Yθ∑m)t∑m(I - γo∑m)t
t=o	t=o
∞
4 Yo X(I — Yo∑m)t∑m = I. [using Yo 6 1∕Tr(∑m)]
t=o
(19)
Therefore, D∞v-X(O) can be further upper bounded by
Dv-X(O) 4 Yo(TW)-1 ◦ B∑m + Yo(TW)-1 ◦ S W ◦ Dv-X(O) [using Eq. 18]
4 YoB + Yo(TW)-1 ◦ SW ◦ Dv-X(O) [using Eq. 19]
∞
YoB X[Yo (TeW)-1 ◦ SW]t ◦ I [using telescopic sum]
t=o
∞	t-1
4 YoB X (YO(TW)T ◦ SW)	◦ YO(TW)T ◦ S W ◦ I
t=o	(20)
∞	t-1
4 YoBX Yo(TeW)-1 ◦ SW	◦ Yo(TeW)-1 ◦ Tr(Σm)Σm [using Assumption 4]
t=o
∞
4 YoB X [Yor0Tr(Σm)]t ◦ I [using Eq. 19]
t=o
4
YoB
1 - Yor0Tr(Σm)
I . [using YO < r0tr(∑m)]
Hence, based on the above results, Dtv-X(O) can be further upper bounded by
Dtv-X(O) = (I - YoT W) ◦ Dtv--1X(O) + Yo2BΣm
=(I - YoTW) ◦ Dv-X(O) + y2(SW - SW) ◦ DV-X + Y2B∑m
4 (I - YoTeW) ◦ Dtv--1X(O) + Yo2SW ◦ D∞v-X(O) + Yo2BΣm
4 (I - YoTeW) ◦ Dtv--1X(O) + Yo2r0Tr[D∞v-X(O)]Tr(Σm)Σm +Yo2BΣm [using Assumption 4]
4 (I-YOTW) ◦ Dv-X(O) + 飞B£m ( 1 TlΣm⅛)+1)	QI)
1 - Yo r Tr(Σm )
4 γ2B(τ≡Tr¾+1) XLmy ς
4 YOB(T≡wb+1) I,
which concludes the proof.	口
D Some useful integrals estimation
In this section, we present the estimation for the following integrals that will be needed in our proof by denoting
κ := 1 - ζ ∈ (O, 1].
20
Under review as a conference paper at ICLR 2022
Integral 1: the following integral admits
1
∕*t	广 U	u1-ζ - 1\	1 f[C(tκ-1)] K	广
J U-Z exp ( — c  ------ d du = - J	U-Zu1-κκvκ-1 exp(-vκ)dv
6 - / exp(-x)dx = ( ∧ ∧ t ),
c0	c
where we directly obtain an exact estimation t. Here we change the integral variable vκ := Cu；--1 and
dv	1_K ∕κ∖1/ K	r、K-1	1 1-κ κ-1
而=U1K (C) (UK - I)K= cU1Kκvκ 1.
Accordingly, if we take ζ = 0 in Eq. 22, we have
t	U1-Z - 1	1 Z
J exp ( — c--——--JdU 6 ( -tζ ∧ t J .
Similar to Eq. 23, we have
n	U1-ζ - t1-ζ	nζ
eχp I - λiYo---------τ--- )du 6 (n - t) ∧ -----.
t	1 - ζ	λiγ0
Integral 2: we consider the following integral
U-ζ exp
=(t + 1)1-κ
c
-C(t + 1)1-1-: +1)1-ζ )du
C	vκ
[(t + 1)(1 — x)K — 1]κ-1(1 — x)~κκvκ-1 exp(-vκ)dv with X :=(-)κ-
0	t+1 c
6 — Z κvκ-1 exp(-vκ)dv
c0
=(2ζ ∧ t),
where we change the integral variable vκ := c
(t+1)1-ζ-(u+1)1-ζ
K1/K /U + 1 ∖1-κ
dU =-----―,—(------)
C1/K t + 1
u+1 κ
1-⅛rJ
~-ζ‹^^
1-1
κ
dv
with κ := 1 - ζ such that
κ
—
C
1	( V )KK
1-(E C
1-κ
--κ κ κ ——1
K (t⅛)	dv，
with (u+1 )κ = 1 - (v/(t + 1))κκ∕c and the upper limit of integral is C := c1/K[(t + 1)κ - (U + 1)κ]1∕κ.
U = (t +1)(1 - x)1 - 1 ∈ [1,t], we have (1 - x)1 ∈ [2/(t + 1),1] and accordingly
g(x) := [(t + 1)(1 — x)1 — 1]κ-1(1 — x)6 21-κ(t + 1)κ-1 with X ∈ 0,1 —
2κ
t+1)
as an increasing function of x.
Similar to Eq. 25, we have the following estimation
∕t Y2Z exp (-2eiγo (t +「I : +1尸卜 U . (∧
E Proofs for Bias
In this section, we present the error bound for Bias. By virtue of Minkowski inequality, we have
Ex,w [hηnias, Σmηniasi])2 6( EX ,W [hηnias - ηnx, Σm(ηnias
|
^{^―
,B1
-ηnx)i]J1 + (EW [h,ηbx, ∑mηnxi])2
1
6(BI)2 + (EwknnX-nM ς m(ηnX-那川)2 +[^m^1∙
(22)
(23)
(24)
(25)
Due to
(26)
(27)
{z^
,B2
{z^
,B3
In the next, we give the error bounds for B3, B2, and B1, respectively.
21
Under review as a conference paper at ICLR 2022
E.1 Bound for B3
In this section, We aim to bound B3 := hηnXW, ∑mηnXWi.
Proposition 2. Under Assumption 1, 2, 3, ifthe step-size Yt := γ0t-ζ with Z ∈ [0,1) satisfies γo 6	* , then B3
Tr(Σm )
can be bounded by
nζ-1
B3 .——kf*k2 .
γ0
1
Proof. Due to γo 6	* ,the operator I -γt∑m is a contraction map for t = 1,2,...,n. Take spectral decomposition
Tr(Σm )
Σm = UΛU> Where U is an orthogonal matrix and Λ is a diagonal matrix With (Λ)11 = λ1 and (Λ)ii = λ2
(i = 2, 3, . . . , m) as Σm has only tWo distinct eigenvalues in Lemma 1. Accordingly, We have
----- -- ------
hηnXW, ∑ mηnXWi
2≤	,.,
Yi ∑ m)f
2 MIY(I
t=0 i=
n-1	t
-YiΣm)f *, Σm Xn(I -
t=0 i=
1 =-- n2	n-1	t Xn(I t=0 i=1	~ , ~1 , -Yi∑m)∑mf * O		2		
63 n2	n-1	t Xn(I t=0 i=	-YiΛ )tΛ1	kf*k2 2		[using Σm	~ ~ ~ -l- UeΛeUe>]
6
n-1 t
Xn(I-Yiek )2ekkf *k2
t=0 i=
(28)
1
一max
n k=,2
Note that
6 n X n (1-YieI)2eιkf *k2 + 1 ∑ n (1
t=0 i=	t=0 i=
—
n-1 t	n-1	t
Xn(1-Yieλj)26Xexp -2Y0λej Xi-ζ
t=0 i=	t=0	i=
6 X eχp b2Y0ej/	X1ζdχ)
n-1	(t + 1)1-ζ -1
=Xexp Sλ 1-Z )
n	(t+ 1)1-ζ - 1
6 1 + y exp (-2Y0ej ʌ——1--------d dx
nζ
6 1 + —h ∧ n , [using Eq. 23]
2Y0λj
(29)
here according to Lemma 1, for λ1, the upper bound nv is tighter than n due to λ1 〜O(1); while this conclusion
2γ0λ1
might not hold for λ2 due to λ2 〜O(1∕m). Then, taking Eq. 29 back to Eq. 28, we have
ζ-1	e ζ
hηnXW, ∑ mηnXWi . — kf *k2 + λ2f -r ∧ n) kf *k2
Y0	n	Y0λ2	(30)
nζ-1
.—kf*k2 〜O(nζ-1),
Y0
which concludes the proof.	□
E.2 Bound for B2
Here We aim to bound B2 := EW 国詈一^XW, ∑m(η下 —成刃〉]：=EW [(&胃,∑mα^i]
αtW := ηtbX	- ηtbXW	=	(I-YtΣm)(ηtb-X	-	ηtb-XW	)+Yt(Σem-Σm)ηtb-XW	,
(31)
22
Under review as a conference paper at ICLR 2022
with αW0 = 0. Accordingly, αtW under the adaptive step-size setting can be formulated as
t	t	k-1
αW = X Yk Y (I — Yj ∑m)(Σm — Σm) Y (I - Ys Σm)f * ,
k=1	j=k+1	s=1
where we use the recursion
tt
At := (I — YtΣm)At-1 + Bt = X Y (I — YiΣm)Bs .
s=1 i=s+1
(32)
Note that EW[α?∣αW-∕ = (I 一 γt∑m)αW-ι, following (ZoU et al., 2021, Lemma B.3), We can rewrite B2 as a
double-sum formulation
B2 := EW [han, Σmθni] = EW [h∑m, an 区涌力=T (EW [Σm< 0 αn])
6 Tr (∑mEW [«n X
EW
-2 EW
n2
6 n EW
Σem,	EW[αtWXαWk] +	EW [αtW X αWk]
06k6t6n-1	06k<t6n-1
(33)
Σem,	EW[αtWXαWk] +	EW [αtW X αWk]
06k6t6n-1	06k6t6n-1
2 n-1 n-1	k-1
n XX EW (Y(I-Yj∑m)∑m,EW[a：；aWl)，
and thUs we have the following error boUnd for B2.
Proposition 3. Under Assumption 1, 2, 3, if the step-size Yt := Y0t-ζ with ζ ∈ [0, 1) satisfies
6 i (	1	1	)
Y0 6 iTr(£m),Tr6m)∫ ,
then B2 can be bounded by
B2 . Y0nζ-1kf*k2 .
To boUnd B2, we first show the error boUnd for kαtW k2 for Tr[CtbW] = kαtW k22 by the following lemma.
Lemma 6. Based on the definition of αtW in Eq. 32, under Assumption 1, 2, 3„ if the step-size Yt := Y0t-ζ with ζ ∈ [0, 1)
satisfies
Yo 6 min ∖ 石TJ, »1 、卜,	(34)
Tr(Σm) Tr(Σe m)
we have
kαtWk2 .Y0kΣmk2kf*k.
Proof. According to Eq. 32, we have
kαtWk2 6
t	t	k-1
XYk Y (I-YjΣm)ΣemY(I-YsΣem)f* +
k=1 j=k+1	s=1
t	t	k-1
XYk Y (I-YjΣm)ΣmY(I-YsΣem)f*
k=1 j=k+1	s=1
t	k-1
6	Yk	(I-YsΣem)Σem
k=1 s=1
t k-1
6	Yk	(I - YsΣem)Σem
k=1	s=1
t	t
kf*k+XYk Y (I-YjΣm)Σmkf*k
k=1 j=k+1
t	t
kf*k+XYk Y (I-YjΣm)Σm kf*k,
2	k=1	j=k+1	2
(35)
23
Under review as a conference paper at ICLR 2022
where I - γiΣm and I - γiΣm are contraction maps for i = 1, 2, . . . , n under our condition in Eq. 34.
, we have
For the first term Ptk=1 γk Qsk=-11(I - γsΣem)Σem	= maxi=1,2 P γk Qjk=-11(1 - γj
t	k-1	t	k-1
Ii:=	γk	(1 - γjλei)eλi 6 λei	γk exp -	γjλei
k=1	j=1	k=1	j=1
6 λiγ0
t+1
1
u-ζ exp
-e,γo Ur--ɪ 卜 U
exp(-x)dx
0
1.
[using Eq. 22]
6
Similarly, for the second term Ptk=1 γk Qtj=k+1 (I - γjΣm)Σm , we have
2
tt	t	t
Ii :=	γk	(1 - γjλi)λi 6 λi	γk exp -	γjλi
k=1 j=k+1	k=1	j=k+1
t	-ζ	(t+ 1)1-ζ - (k+ 1)1-ζ
6 λi EYOk Z exp ( - λiγo --------1——--------- \
k=1	1 - ζ
-ζ	t -ζ	(t + 1)1-ζ - (u+ 1)1-ζ
6 λiYot Z + λiY0 J U Z exp ( 一 λiY0-------1—^---------Jdu.
(36)
Dueto k∑m∣∣2 〜 O(1) in Lemma 2, we have
∞
0
max	Ii
i∈{1,2,...,m}
6 λ1Y0t-Z + 21-κ
κvκ-1 exp(-vκ)dv
[using Eq. 25]
6 Y0kΣmk2 + 21-κ
Accordingly, we have
t
kaWk2 6 kf*k + XYk
k=1
.Y0k∑mk2kf*k
t
Y (I - Yj Σm )Σm
j=k+1
[using k∑mk2 〜O(1) by Lemma 2]
tt
XYk Y (1 -Yjλi)λi
k=1	j=k+1
kf *k
□
24
Under review as a conference paper at ICLR 2022
Proof of Proposition 3. Based on the above results, B2 can be bounded by
B2 := EW [h&n，Σmani] 6 Tr (EW 用悄&胃乳 α胃)
2 n-1 n-1	k-1
6 n XX EW (Y (I- YjΣ m)Σ m, EW [αWjαW],)
[using Eq. 33]
n-1 n-1
k-1
2	n-1 n-1 k-1
.n02 Ew [k∑mk2]kf *k2 XX Π (I- YjΣ m)Σ,
m
[using Lemma 6]
t=0 k=t
j=t
2
n-1 n-1
max
t=0 k=t i∈{1,2,...,m}
k-1
-λi	γj
j=t
[using Lemma 2]
2	n-1 n-1	1-ζ	1-ζ
6 0k kf *k2.jmax IXX λ exp (一λiY0 —Γ-7—
n2	i∈{1,2,...,m} t=0 k=t	1-ζ
[using Pk-I Yj 6 Y0 Rtk Χ1ζdx]
γ 2	n-1	n
6 墟kf *k2i∈{max,m} Xλ Lep (-λiγ0
u1-ζ - t1-ζ
-----------du
1 - Z d
2	n-1
6 崇 kf *k2i∈{max m} X λi
n	i∈{1,2,...,m} t=0
6 Y0nζ-1kf*k2 ,
nζ
|而 ∧(n -1)
[using Eq. 24]
where in the last inequality We choose λn^ instead of n -1 fora tight error bound. Finally, We conclude our proof. □
E.3 Bound for B1
Here we aim to bound B1 := Eχ,w [hηbias -ηbX, Σm(ηb,ias -ηb,X)i]. Define aX := ηbias - ηbX, we have
αχ = [I - YtP(Xt) 0 2(xt)]αχ-ι + γt[Σm -夕(Xt) 0 2(xt)]ηbx 1 ,	(37)
with ɑχ = 0 and ηb-1 = Qj=I(I - γjΣm)f *. Accordingly, we have
bi ：= EX ,w [hηnias-ηnx, Σm(ηnias-ηnx)i] = Ew (EX [hαn, Σmαn>D.
Proposition 4. Under Assumption 1, 2, 3, 4 with r0 > 1, if the step-size Yt := Y0t-ζ with ζ ∈ [0, 1) satisfies
)
r0Tr(Σm), 2Tr(Σm) ʃ ,
then B1 can be bounded by
Y0 < min
B1
z	γ0r0nζ-1	- kf *∣∣2
PE[1 - γor0Tr(Σm)]4 " U
〜
O(nζ-1).
To prove Proposition 4, we need a lemma on stochastic recursions based on E[αX∣αX-/ = (I - γtΣm)αX-ι, that shares
the similar proof fashion with (Bach & Moulines, 2013, Lemma 1) and (Dieuleveut & Bach, 2016, Lemma 11).
Lemma 7. Under Assumption 1, 2, 3, 4 with r0 > 1, denoting Ht-1 := [Σm 一夕(Xt) 0 夕(xt)]ηb-1, if the step-size
Yt := Y0t-ζ with ζ ∈ [0, 1) satisfies
1
Y0 < r0Tr(Σm) ,
we have
EX[han, Σmαηi] 6 __1	(XEkakk2(ɪ - -1) + 2X γt+ιEχ||四|2).
n n	2n[1 - Y0r0Tr(Σm)] k=1	k	Yk+1	Yk	t=0
25
Under review as a conference paper at ICLR 2022
Remark: We require k∑mk2 = p^ to avoid the denominator to be zero, which naturally holds as the probability
measure of the continuous random variable kΣm k2 at a point is zero.
Proof. According to the definition of OX in Eq. 37, define Ht-ι := [∑m -夕(Xt) 0 2(xt)]ηb-1, we have
kαXk2 = kαX-ι - Yt(3(χt) 0 3(xt)]αW-ι - Ht-I)k2
=l∣αX-ik2 + Y2kHt-i - [φ(xt) 0 φ(xt)]αX-ik2 + 2γthat-ι,Ht-i - [φ(χt) 0 φ(xt)]aX-ii
6 llaX-ιk2 + 2Y2 (kHt-ιk2 + kk(xt) 0 3(xt)]αX-ιk2) + 2γthαX-ι,Ht-ι - [φ(xt)0 φ(xt)∖αX-1i,
which implies (by taking the conditional expectation)
EX[kaWk2|aW-i] 6 IIaX-Ik2 + 2Y2kHt-Ik2 +2γ2hαX-i, EX[φ(χt) 0 中(Xt) 0 中(Xt) 0 φ(χt)]αX-ii
- 2γt hαt-1, Σmαt-1i	(38)
6 kαtX-1k2 + 2γt2kHt-1k2 + 2γt2r0Tr(Σm)hαtX-1, ΣmαtX-1i - 2γthαtX-1,ΣmαtX-1i
= kαtX-1k2+2γt2kHt-1k2 - 2γt[1 - γtr0Tr(Σm)]hαtX-1, ΣmαtX-1i.
where the first inequality holds by EX [Ht-1] = 0, and the second inequality satisfies by Assumption 4.
By taking the expectation of Eq. 38, we have
EX[kαtXk2] 6EX[kαtX-1k2]+2γt2EX[kHt-1k2]-2γt[1-γtr0Tr(Σm)]EXhαtX-1,ΣmαtX-1i,
which indicates that
1
EX
[han, ^^mani]i 6 — X EX haW, NmaW 6 y∩ 0rτv∕v ∖1 X X EX kak k2(
n n n t=0	t t 2n[1 - γ0r0Tr(Σm)] k=1	k	γk+1	γk
1	1	n-1
+ 讨EX ka0k2 - EEX kaXk2 + ɪz Yt+1EX kHtk2
6 2n[1-MTrRm)] (EEX宿『(Y+ - Y)+2
n-1
X γt+1EXkHtk2	,
t=0
1
due to αW0 = 0.
□
In the next, we present the error bounds for two respective terms in Lemma 7.
Lemma 8. Based on the definition of αtX in Eq. 39, under Assumption 1, 2, 3, 4 with r0 > 1, if the step-size γt := γ0t-ζ
with ζ ∈ [0, 1) satisfies
]
r0Tr(∑m), 2Tr(∑m) ʃ ,
γ0 < min
we have
n-1
EEkakk2(」—
k=1	γk+1
_1).	γ0r0τr(∑m)
Yk ~ 1 - γor0Tr(∑m)
(nζ-1)kf*k2∙
Proof. Based on the definition of atX in Eq. 37, it can be reformulated as
k-1
at = [I - Yt2(Xt) 0 2(Xt)]at-1 + γt [Nm -q(Xt) 0 2(xt)] ɪ ɪ (I - Ys Nm)f
s=1
t	t	k-1
=X Yk Y [I - Yj 2(Xt) 0 2(Xt)][∑m -夕(Xt) 0 2(Xt)] Y (I - Ys ∑m)f * .
k=1	j=k+1	s=1
(39)
26
Under review as a conference paper at ICLR 2022
and accordingly
Cb-X := EX [αX 乳 αX] = (I-YtTW) ◦ Cb-X + γ2(SW - SW) ◦ [ηbX 1 ㊈ ηb-1]
4 (I-YtTW) ◦ Cb-X + YtSW ◦ [nt-1 0 ηt-1]
t-1
4 (I — YtTW) ◦ Cb-X + Ytr0Tr Y(I - Ys∑m)2∑m ∑m(f * 乳 f *) [using Assumption 4]
s=1
4 (I — YtTW) ◦ Cb-X + Ytr0Tr(∑m)∑m(f * 乳 f *) [following Eq. 36: exp(-2%Yot1—--1) 6 1]	(4O)
tt
=r0Tr(∑m) X Y (I-YiTW) ◦ Yt∑m(f* 乳 f *)
s=1i=s+1
41-1%?[" * ㊈ f *). [usingLemma 5]
Accordingly, we have
[using Eq. 40]
n-1
6X
t=1
Yor0Tr(∑m)
1 - Yor0Tr(∑m)
[(t+1)ζ-tζ]kf*kt
Yor0Tr(∑m)
1 - Yor0Tr(∑m)
(nζ-1)kf*kt,
which concludes the proof.
□
Lemma 9. Denote Ht-ι := [∑m —夕(Xt) 0 夕(xt)]nb-1, Assumption 1,2, 3, 4 with r0 > 1, ifthe step-size Yt := Yot-ζ
with ζ ∈ [0, 1) satisfies
1
YO 6 * * * τr(∑□,
we have
n-1
X Yt+ιEχkHtk26 -kf *k2r0Tr(∑m).
Proof.
n-1	n-1
XYt+1EXkHtk2=XYt+1
t=o	t=o
f*,tY-1(I
j=1
- Yj ∑m )EX [∑m
t-1
-P(Xt) 0 2(Xt)]2 ɪɪ(1 - Yj∑m)
j=1

n-1
6	Yt+1
t=o
f*,r0Tr(∑m)htY-1(I-Yj∑m)i2∑m
[using Assumption 4]
n-1	t-1	2
6 kf*k2r0Tr(∑m) X Yt+1 Y(I-Yj∑m) ∑m
t=o	j=1
n-1	t-1
6 kf*k2r0Tr(∑m)	max	X Yt+1 Y(1 - Yjλi)2λi
i∈{1,2,...,m} t=o	j=1
6 kf *k2r0Tr(∑m)	max	Yoλi j u-ζ exp (-2Y0λiu-——-1) du
i∈{1,2,...,m}	o	1 - ζ
6 jkf *k2r0Tr(∑m) [using Eq. 22]
which concludes the proof.
□
27
Under review as a conference paper at ICLR 2022
Based on the above results, we are ready to prove Proposition 4.
Proof. According to Lemma 8, we have
E Pn-I Ekakk2(Yk+l - Yk) < E	Yor0TrNm)	Z _ 1 巾 f*∣∣2
W	2n[1 - γor0Tr(∑m)]〜EW2n[1 - γor0Tr(∑m)]2 (	)kf k
1
6 Yor0nζ-1 √E[Tr(∑m)]2
√E[1 - γor0Tr(∑m)]4
kf *k2
γ0r0nζ-1
〜
√E[1 - γor0Tr(∑m)]4
O(nζ-1),
kf *k2
where the second inequality holds by CaUchy-SchWarz inequality and the last inequality holds by Lemma 2.
According to Lemma 9, we have
M 2 Pn=01 Yt+iEx kHtk2<M
EW 2n[1 - γor0TY(∑m)] 6 EW
r0Tr(Σm)
2n[1 - γ0r0Tr(Σm)]
kf *k2
r0
n
√E[1 - γor0Tr(∑m)]2
kf *k2
1
n
〜O
r0
√E[1 - γor0Tr(∑m)]2
'(n).
kf *k2 [using Lemma 2]
Accordingly, combining the above two equations, we have
B1 := EWEX [han, ∑mC⅛ni] 6
2n[1 - γ0r0Tr(Σm)]
γ0r0nζ-1
nX-1n-1
Ekakk2(-) + 2 X Yt+1EXkHtk2
γk+1	γk	t=0
√E[1 - γor0Tr(∑m)]4
kf*k2,
1
which concludes the proof.
□
E.4 Proof of Theorem 1
Proof. Combining the above results for three terms B1, B2, B3, the Bias can be upper bounded by
Bias 6 (√B1 + √B2 + √B3) 2 6 √3(B1 + B2 + B3)
.______γ0r0nζ-1______kf*k2
〜PE[1 - γor0Tr(∑m)]4lu " '
□
F Proof for Variance
In this section, we present the error bound for Variance. Recall the definition of ηtvX in Eq. 12 and ηtvXW in Eq. 13, and
n-1
ηnx := - X 错,
n t=0
n-1
ηnxw = LX ηvxw,
n t=0
28
Under review as a conference paper at ICLR 2022
by virtue of Minkowski inequality, Variance can be further decomposed as
(Ex,W,ε [hηnar, Σmηnari]) 16(Eχ,W,ε [<殖"- 殖*, ∑m(⅞T - 殖XN^ + (Eχ,W,ε 刖¥, ∑m^Xi]) 2
6(V1)ι +(Ex,w：
{Z
,V1
1
,ε [hηnx-ηnxw, Σm (ηnx - ηnxW)i]) 2+[Ex ,W ,εhηnxW, ∑mηnxw i]2.
---------------------------------}/ X--------------------------/
{z^
,V2
{z^
,V3
(41)
F.1 B ound for V3
- .	.	-	-	__ , 一^rτ τ   一^rτ τ.	-	__ r 一^rτ τ ∣ 一,“，-l	, _	^2≤ 、 一，，<，	..
In this section, We aim to bound V3 := Eχ,w,εhηnXW, ∑mηnXWi∙ Note that Eχ,ε[ηVXw∣ηV-Wι] = (I - γt∑m)nV-，，similar
to Eq. 33 for B2, we have the following expression for V3
V3 := Ex,W,εhηnXW, ∑mηnXWi = EW [Eχ,εh∑m,ηnXW 乳殖防
=F EW
n2
6 n EW
Σm, E	EX ,ε[ηVxw ㊈ ηVxw] + E	Eχ,ε[ηVxw ㊈ ηVxw]
06k6t6n-1
06k<t6n-1
Σm, E	EX ,ε[ηVxw ㊈ ηVxw] + E	Eχ,ε[ηVxw ㊈ ηVxw]
(42)
06k6t6n-1
06k6t6n-1
n-1 n-1	k-1
F XX Ew (Y (I- Yj ∑ m)∑m, EX ,ε[ηVxw X 瑞明,
n t=0 k=t	∖==t	S	XvXW	}/
z
:=CtvXW
and thus We have the folloWing error bound for V3.
Proposition 5. UnderAssumPtion 2, 3, 5 with τ > 0, ifthe step-size γt := γot-ζ with Z ∈ [0,1) satisfies γo 6	,
Tr(Σm )
then V3 can be bounded by
m
2
τ
γ0
1- n
n
2
τ
0
γ0
-m
+
ζ
+
ζ
To prove Proposition 5, We need the folloWing lemma.
ɪ	YCC	^<τrYL7	τn	Γ TrYU _ TrYlJl	1	.	「	CC- .上	,r∕ E /U'	I
Lemma 10. Denote CvXW := EX ,ε[ηVXw X ηVXw], under Assumptions 2, 3, 5 with τ > 0, If γο 6 1∕Tr(∑m), we have
tt
CtvXW 4τ2Xγk2 Y [I-γjΣem]Σm.
k=1	j =k+1
Proof. Recall the definition of ηtvXW in Eq. 13, it can be further represented as
tt
ηvxw = (I-γt∑m)ηvxwι + YEk2(Xk) = X Y (I-Yj∑m)γ⅛εfc夕(Xk) withη0Xw = 0.
k=1 j=k+1
Accordingly, CtvXw admits (with C0vXw = 0)
tt	t	t
CtvXw = X Y (I - γjΣem)2γk2Ξ 4 τ2Xγk2 Y (I - γjΣem)2Σm [using Assumption 5]
k=1 j=k+1	k=1	j=k+1
where we use E[εiεj] = 0 for i = j.	□
In the next, we are ready to bound V3 in Proposition 5.
29
Under review as a conference paper at ICLR 2022


Proofof Proposition 5. Note that λι 〜O(1) and λ?〜O(1∕m) in Lemma 2, We take the upper bound of the integral
ζ
in Eq. 24 to n— for λι. However, according to the order of λ2, if λ? . 1∕n, the exact upper bound is tight. Based on
λ1 γ0
ζ
this, we first consider that m 6 n case such that λ2 & 1∕n, and then focus on the m > n case. Taking n— in Eq. 24
λiγ0
and Y0 in Eq. 26, we have
λi
V3 ：= Eχ,w,εhηnxW, ∑mηnxwi = Eχ,W,εh∑m,乳瞋
6
n-1 n-1	k-1
F XX EW (Y (I- YjΣ m)∑m, EX ,ε[ηVXW X 宿力)
n t=0 k=t	∖j=t	S----------{ZVXW------} /
[using Eq. 42]
z
:=C vXW
2 n-1 n-1	k-1	t	t
6 2Tτ XX Ew (Y (I - Yj Σ m)∑m,X Y Y (I - Yj Σ m)2
t=0 k=t
j=t
s=1	j=s+1
Σm [using Lemma 10]
n-1 n-1
6 2Tτ XXTr
t=0 k=t
2 n-1 n-1 m
.2Tτ xxx
t=0 k=t i=1
k-1
Y (I — Yj Σ m)Σ m X Y Y (I — Yj Σ m)2∑ m [ ^W ^Σ 团2](
j=t
k-1
j=t
s=1	j=s+1
(1-γjλei)λei	γs2	(1-γjeλi)2eλi
s=1	j=s+1
[using Lemma 3]
(43)
t
t
t
t
t
t
γs2 exp -2λiγ0
s=1
(t +1)1-ζ - (s + 1)1-ζ
T-T
1-ζ
k1-ζ - t1-ζ
2 n-1 n-1 m
6 2Tτ xxx
t=0 k=t i=1
i2 exp -λeiγ0

where the last equality holds that 0n t-2ζ dt 6 n for any ζ ∈ [0, 1).
If λ2 . 1/n, that means, m > n in the over-parameterized regime, we have
2τ2 n-1
V3.谓£
n t=0
nζ
Y0 + γ2) + (m - 1)λ2(n - t)t
λ1 γ0 λ1
.2τr (γon1+ζ + (m - 1)Y2e2
.γ0τ2 (n-1+ζ+m)，
n(n - 1)(n + 1)
[since λι 〜O⑴]
6
which concludes the proof.
□
F.2 B ound for V2
Here we aim to bound V2
V2 := Ex,w,ε[hηnx-ηnxw, ∑m(ηnx-ηnxw)i].
Recall the definition of ηtvX and ηtvXW in Eqs. 12 and 13, we have
tt
ηvxw =	(I	— γt∑m)nv"	+	YEk2(Xk)	= X Y (I —	γj∑m)γkεkq(Xk)	with	η0XW	= 0,
k=1 j=k+1
30
Under review as a conference paper at ICLR 2022
and accordingly, we define
αtvX-W:= ηtvX - ηtvXW = (I - γtΣm)αtv-X-1W + γt(Σe m - Σm)ηtv-XW1, with αv0X-W = 0
t t	s-1 s-1
=	(I - γiΣm)γs(Σem - Σm)	(I - Yj夕①^k飞夕(Xk).
s=1 i=s+1	k=1 j=k+1
Proposition 6. Under Assumptions 2, 3, 5 with τ > 0, if the step-size γt := γ0t-ζ with ζ ∈ [0, 1) satisfies
1
γ06 τr(∑□,
(44)
then V2 can be bounded by
(2m
Y0τ F-Z,	If m 6 n
2 n1-ζ
γ0τ2, ifm > n .
To prove Proposition 6, we need the following lemma.
Lemma11. Denote CvX-W ：= Eχ,ε[αJx-w0a：X-W], under Assumptions 2, 3, 5 with T > 0, ifthestep-size Yt := γot-ζ
with ζ ∈ [0, 1) satisfies
v ∙ (	1	1	)
Yo 6 mm	,,
ITr(RTtRm)∫ ,
we have
kCtvX-Wk2.τ2I+Σe-m2Σ2m	Tr(Σem)Y02 [Tr(Σm)Y0 + 1] .
Proof. According to the definition of CtvX-W, it admits the following expression
t t	s-1 s-1
CtvX-W = X Y (I-YiΣm)Ys2(Σem -Σm)X Y (I-YjΣem)2Yk2Ξ(Σem-Σm)(I-YiΣm)
s=1 i=s+1	k=1 j=k+1
t t	s-1 s-1
4 X Y (I - YiΣm)Ys2 (Σem - Σm) X Y (I - YjΣem)2Yk2Ξ(Σem - Σm)(I - YiΣm)
s=1 i=s+1	k=1 j=k+1
t t	s-1 s-1
4τ2XY (I - YiΣm)Ys2(Σem -Σm)X Y(I - Yj Σem )2Yk2 Σm (Σem - Σm )(I - Yi Σm ) ,
s=1 i=s+1	k=1 j=k+1
where the first equality holds by E[εiεj] = 0 for i 6= j and the second inequality holds by Assumption 5.
Accordingly, kCtvX-W k2 can be upper bounded by
t
kCtvX-Wk2 6τ2XYs2
s=1
t	s-1	s-1
Y (I-YiΣm)2Σm(Σem-Σm)2XYk2 Y (I-YjΣem)2
i=s+1	k=1 j=k+1
t
6 τ2XYs2
s=1
t
Y (I - YiΣm)2Σm
i=s+1	2
s-1 s-1
XYk2	Y (I-YjΣem)2Σe2m	I+Σe-m2Σ2m
k=1	j=k+1	2
t
6 τ 2	max	Ys2 exp
q∈{1,2,...,m}
s=1
-2λq Xt Yi
i=s+1
s-1
λq	Yk2
k=1
max exp
p∈{1,2}
—2ep X γj j ep HI+ςm
j=k+1
2
where the second inequality holds by Σm	0 and Σm < 0 such that
(Σm — Σm)2 = Σm(I — Σm1∑m)2 4 ∑+ e旌),
and Tr(AB) 6 kAk2Tr(B) for any two PSD operators A and B.
31
Under review as a conference paper at ICLR 2022
Similar to Eq. 25, we have the following estimation
s-1	s-1
s-1
γk2	(1-γjep)2 6	γk2 exp
k=1 j=k+1
k=1
6 γs2-1 + γ02
—2eP X Yj)
j=k+X
u-2ζ exp - 2epγ0
s1-ζ - (u + 1)1-ζ
1-ζ
du
which implies
s-1
s-1
max	eλ2p	γk2	(1 - γj	eλp)2	6	γ02	eλX2 +γ0λeX	6	2γ0λeX
p=X,2
k=X j=k+X
where we use our condition on the step-size γo 6 e .
Tr(Σm )
Similar to Eq. 25, we have the following estimation
..ʌi ..
2γ0 IΣem I2 ,
(45)
X Y2 exP (-2λq
s=X
Xt γi
i=s+1
t 2	(t + 1)X-ζ 一 (s + 1)X-ζ
6 £ Y2 eχp ( - 2λqYo ʌ-------1-z----------)
s=X
6 Y + Y0 / u-2Z exp ( 一 2λqY0(t + 1「一 (u+1尸卜U
6 Yo + (λγ0 ∧ γ0t),
which implies
t
max	γ2 λq exp
q∈{1,2,...,m}	s q
s=1
Take the above tWo equations 45 and 46 back to Eq. 45
-2λq Xt γi
i=s+1
γ02 kΣm k2 + γ0 .
(46)
τ 2 III + ς m2*mJJ2 (Y2Nm l∣2 + γθ) (γ0 k ς m∣∣2)
τ2Y02JJJI+Σe-m2Σ2mJJJ	(Y0IΣmI2+1)IΣemI2.
Proofof Proposition 6. By virtue of EX,ε[a；X-WIavX-w] = (I 一 γt∑m)αVX-W and Lemma 11, V2 can be bounded by
V2 = ex,w ,ε[hηnx - ηnxw, ∑m(ηnx-ηnxw)i] = EW h∑m, EX ,ε[αnX-W % anx-w]i
2 n-X n-X	k-X
6 W XX EW Y Π (I 一 Yj∑m)∑m, Ex,ε
nA一・	∖ ɪ ɪ	X___
t=0 k=t	∖ j = t
[ηVX-w ㊈ ηVx-w]:
:=C vX-w
22
.-JO ∣∑ mk2Ew (∣∣I+∑ m2∑m∣∣2
n-1 n-1 k-1
[k∑m∣2Y0 + 1]Tr EEn(I- Yj∑m)∑m
t=0 k=t j=t
.nJ0 IIςmk2EW ||£mk2 (I + ςm^,
J m n-1	nζ
m" X=X X λi (λγo ∧ (n - tOj . [using Eq. 24]
In the m 6 n case, we choose nζ /(λiY0), and thus
V2 . -2mY2 ∣Σ m ∣2EW [k∑m∣∣2 JI +Σm2∑m∣Li
n1+ζ
6 τ2γ0
. τ 2γ0
Elle IlC /------ / H	T-
,Ewk∑mk2yEw III + ∑m2
m
1 广.[using Lemma 2 and 3]
nX-ζ
γ0
m J J [using CaUchy-SchWarz inequality]
□
2
2
z
32
Under review as a conference paper at ICLR 2022
If m > n, we have
2 2	n-1
V2 . -Tn2γ0k∑mk2Ew ([Tr(∑m)]2 ∣∣I +∑m2∑mΓ) Xt
t=0
6 τ2γ0k∑m∣∣2pEw[Tr(∑m)]2^Ew ∣∣I + ∑m2∑m∣∣2 [using CaUchy-SchWarz inequality]
. τ2 γ0 , [using Lemma 2 and 3]
which concludes the proof.
□
F.3 B ound for V1
Here We aim to bound V1
V1 := Ex,w ,ε [hηnar-ηnx, Σm(ηnar - ηnx)i].
Recall the definition of ηtvar in Eq. 6 and ηtvX in Eq. 12, We define
αv-X := ηvar - ηvx = [I - Yt2(Xt) 0 2(xt)]αV-X + γt[∑m -夕(Xt) 0 2(xt)]ηvx 1, with αV-x = 0 .
t-1 t-1
=[I - Ytψ(xt) 0 φ(xt)]αv-1 + Yt[∑m -中(Xt) 0 ψ(xt)] ∑ π (I - Yj夕m)7kεk2(Xk )
k=1 j=k+1
t t	s-1 s-1
=	Ys [I - Yi2(Xi) 0 2(Xi)][∑m -2(Xt) 0 O(Xt)W ∏ (I - Yj∑m)Ykεk夕(Xk),
s=1 i=s+1	k=1 j=k+1
and thus the error bound for V1 is given by the following proposition.
Proposition 7. Under Assumption 1, 2, 3, 4 with r0 > 1, and Assumption 5 with τ > 0, if the step-size Yt := Y0t-ζ
with ζ ∈ [0, 1) satisfies
Y0 < min
1	1
r0Tr(∑m), -Tr(∑m)
then V1 can be bounded by
V1 .
τ 2r0Y02
√E[1 - γor0Tr(∑m)]2
(m
n-ζ,	if m 6 n
1, if m > n .
To prove Proposition 7, we need the following lemma. Define Ctv-X := EX,ε[αtv-X 0 αtv-X], we have the following
lemma that is useful to bound Ctv-X .
Lemma 12. Denote Ctv-X := EX,ε[αtv-X0 αtv-X], under Assumptions 1, 2, 3, 4 with r0 > 1, and Assumption 5 with
τ > 0, if the step-size Yt := Y0t-ζ with ζ ∈ [0, 1) satisfies
we have
Y0 < min
1	1
r0Tr(∑m), -Tr(∑m)
Ctv-X 4
γ2r0τ 2[Tr(∑m)+ Y0Tr(∑m)]
1 - Y0r0Tr(Σm)
I.
33
Under review as a conference paper at ICLR 2022
Proof. According to the definition of Ctv-X, it admits the following expression
t t	s-1 s-1
Cv-X = X Y Y2Eχ[I-γi中(Xi)㊈ °(Xi)]2Eχ[∑m - φ(xt)㊈ φ(xt)]2 X Y (I-Yj∑m)2γ2Ξ
s=1 i=s+1	k=1 j=k+1
t-1 t-1
= (I - γtT W) ◦ Ctv--1X + γt2(SW - SeW) ◦ X Y (I - γjΣm)2γk2Ξ [using PSD operators]
k=1 j=k+1
t-1 t-1
4 (I-YtTW) ◦ Cv-T + 吧SS ◦ XY (I-Yj∑m)2γ2Ξ [using SW < SW]	(47)
k=1 j=k+1
t-1 t-1
4 (I-YtTW) ◦ Cv-T + τ2γ2SW ◦ X Y (I - YjΣm)2Yk2Σm [using Assumption 5]
k=1 j=k+1
t-1 t-1
4 (I - YtTW)O Cv-T + τ2γ2r0Tr XY (I - Yj∑m)2γl∑2m ∑m . [using Assumption 4]
k=1 j=k+1
Similar to Eq. 25, we have the following estimation
t-1 t-1
TrXY (I-Yj∑m)2∑mY2
k=1 j=k+1
m	f 一 1	≠ 一 1	m	f 一 1	/	c 一 1
X λ X y2 Y (1-Yj λi)26 X λ X y2 exp ∣-2λi X Yj
i=1 k=1 j=k+1	i=1 k=1	j=k+1
m
m
6Y02Xλi2 1+
1
i=1
t-1u-2Z exp (- 2λiY0 t1-ζ-1(-+I)I-Z bU
m
6 Y02Tr(Σ2m) + X λi2
i=1
[using Eq. 26]
1
6 Y02Tr(Σ2m) + Y0Tr(Σm) ,
where We use the error bound γλ0 instead of the exact one Y2t for tight estimation.
Taking the above equation back to Eq. 47, we have
Ctv-X 4 (I - YtT W) ◦ Ctv--1X + Yt2τ 2r0Y0[Tr(Σm) + Y0Tr(Σ2m)]Σm
tt
4 τ2r0Y0[Tr(∑m) + YoTr(∑m)] X Y (I-YiTW) ◦ Y∑m
s=1 i=s+1
Yi2r0τ2[Tr(∑m) + YoTr(∑m)] 「	「.	1 G
4 ———ʌ~~--ng、 I, [usingLemma5]
1 - Y0r0Tr(Σm)
which concludes the proof.	□
Proofof Proposition 7. Accordingly, by virtue of EX产[。；-，|ɑ；[：] = (I - Yt∑m)ɑt^-X and Lemma 12, V1 can be
bounded by
vι = Eχ,w 产[hηnar - ηn-X, ∑m(ηnar-ηn-X)i] = EW h∑m, Eχ,ε [αn-X 乳 αn-X]i
2 n-1 n-1	-1
6 W XX Ew ( Y (I - Yj ∑m)∑m, Eχ,ε
n L一1L一,	∖ ɪ ɪ	X  
t=0 k=t	∖j=t
[ηv-X ㊈ ηv-X}:
{z
:=Ctv
-X
.τ2⅛0 EW
n2
[Tr(∑m)+ Y0Tr(∑m)] XnX∖ ( nZ ʌz	八)]
1-Y0E(∑m)占 t=0 λi (0 ∧ (n - t)J
[using Lemma 12]
where the last inequality follows the integral estimation in Eq. 24.
34
Under review as a conference paper at ICLR 2022
For m 6 n,we use Xn^, and thus
,	λiγ0 ,
V1
τ 2γ0r0m
---TT^ EW
n1-ζ
-[Tr(∑m) + YoTr(∑m)]-
_ 1 - γor0Tr(∑m) _
τ 2r0γ0	m
PE[1 - γor0Tr(∑m> n-
where we use the CaUchy-SchWarz inequality and Tr(∑m) as a nonnegative sub-exponential random variable with the
sub-exponential norm O(1) in Lemma 2.
For m > n, we use n - t, and thus
V1 . τ 2γ02r0EW
ITr(∑m) + γoTr(∑m)] -
1 - γor0Tr(∑m)	_
pE[1-τYθXm)]2 ".
□
F.4 Proof of Theorem 2
Proof. Combining the above results for three terms V1, V2, V3, we can directly obtain the result for Variance.
Variance 6
(√V1 + √V2 + √V3)2 6 √3(vi + V2 + V3)
γ0r0τ 2
√E[1 - γor0Tr(∑m)]2
(mnζ-T , if m 6 n
nζ-τ +--, if m > n
O O (mnζ-i) , if m 6 n
ɪ O (nζ-i + —) , if m > n
□
35