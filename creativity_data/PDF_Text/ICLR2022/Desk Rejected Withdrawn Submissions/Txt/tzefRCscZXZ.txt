Under review as a conference paper at ICLR 2022
Adversarial Visual Robustness by Causal In-
TERVENTION
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training is the de facto most promising defense against adversarial
examples. Yet, its passive nature inevitably prevents it from being immune to un-
known attackers. To achieve a proactive defense, we need a more fundamental
understanding of adversarial examples, beyond the popular bounded threat model.
In this paper, we provide a causal viewpoint of adversarial vulnerability: the cause
is the spurious correlation ubiquitously existing in learning, i.e., the confounding
effect, where attackers are precisely exploiting these effects. Therefore, a funda-
mental solution for adversarial robustness is by causal intervention. As these vi-
sual confounders are imperceptible in general, we propose to use the instrumental
variable that achieves causal intervention without the need for confounder obser-
vation. We term our robust training method as Causal intervention by instrumen-
tal Variable (CiiV). It’s a causal regularization that 1) augments the image with
multiple retinotopic centers and 2) encourages the model to learn causal features,
rather than local confounding patterns, by favoring features linearly responding
to spatial interpolations. Extensive experiments on a wide spectrum of attackers
and settings applied in CIFAR-10, CIFAR-100, and mini-ImageNet demonstrate
that CiiV is robust to adaptive attacks, including the recent AutoAttack. Besides,
as a general causal regularization, it can be easily plugged into other methods to
further boost the robustness. Codes are available in supplementary materials.
1 Introduction
Despite the remarkable progress achieved by Deep Neural Networks (DNNs), adversarial vulner-
ability (Goodfellow et al., 2015) keeps haunting the computer vision community since it has been
spotted by Szegedy et al. (2013). Over the years, we have witnessed many defenders, who claim
to be “well-rounded”, were soon found to lack fair benchmarking, e.g., adaptive adversary (Croce
& Hein, 2020b; Tramer et al., 2020b), or misconduct the attack, e.g., obfuscated gradient (Atha-
lye et al., 2018). Therefore, the most promising defender remains to be the intuitive Adversarial
Training and its variants (Kannan et al., 2018; Cui et al., 2020). Due to the “training” nature, its
adversarial robustness is largely dependent on the knowledge of attackers and whether the training
set contains sufficient adversarial samples from various attackers as many as possible (Tramer et al.,
2018), yet, brute-forcely enumerating all attackers is prohibitively expensive, making adversarial
training mainly over-fitting to known attackers (Schott et al., 2019). What’s worse, in few-/zero-
shot scenarios, it is even impossible to collect enough adversarial training samples based on the
out-of-distribution/unseen samples (Zhang et al., 2019).
In other words, adversarial training is a “passive immunization”, which cannot react to the ever-
evolving attacks responsively. To proactively achieve adversarial robustness, we have to find the
“origin” of adversarial perturbations. Previous methods blame adversarial vulnerability on the in-
herent flaws in fitting models to the limited high-dimensional data (Goodfellow et al., 2015; Gilmer
et al., 2018; Schmidt et al., 2018). However, simply regarding adversarial samples as “bugs” cannot
explain their well-generalizing behaviors (Xie et al., 2020; Ilyas et al., 2019). Recent studies (Salman
et al., 2020; Ilyas et al., 2019) show that adversarial examples are not “bugs” but predictive features
that can only be exploited by machines. Such results urge us to investigate the essential difference
between machines and humans.
1
Under review as a conference paper at ICLR 2022
(a) Learning from Confounded Data
(b) Attacking by Tampered Confounders
(C) Adversarial Attack on Real Image
Figure 1: (a) A digit classifier confounded by counting edges. (b) Attacking the model through
tampered confounders. (c) Constructing adversarial perturbations through an ensemble of tampered
confounders, e.g., local textures, small edges, and faint shadows.
However, we believe that it is too early for us to shirk responsibility and leave it to the ever-elusive
open problem before we answer the following two key questions:
Q1: What are the non-robust but predictive features? Ilyas et al. (2019) use adversarial examples
to distinguish the robust and non-robust features. However, this will only allow us to recognize the
non-robust features as “adversarial perturbations” again, which is, unfortunately, circular reasoning.
Therefore, we need a fundamental yet different angle to define the robustness of features beyond the
conventional adversarial one.
Q2: Why do complex systems (human vision) ignore these predictive features that simple systems
(DNNs) can capture instead? Given the fact that biological visions are more complex than machines
in terms of both neuron amount (Herculano-Houzel, 2012) and diversity (Masland, 2001; Kim et al.,
2020), there is no reason for human vision to extract “less feature” than machines. Therefore, there
must be a mechanism in human vision that deliberately ignores these features.
In this paper, we answer the above two questions from a causal perspective (Pearl, 2009)—a pow-
erful lens seeing through the generative nature of adversarial attacks. For Q1, we postulate that
non-robust features are confounding effects, which are spurious correlations established by related
but non-causal features. Take Figure 1 (a) as an intuitive example, where a large number of vertical
edges co-occur with the digit “1”. As a result, a model trained by associating samples with labels
will recklessly use the counting of vertical edges—the confounding effect—as the indicator of digit
“1” without learning the overall causal structure. Therefore, once tampered edges are constructed,
which is much easier than editing the entire digit directly, the confounding effect will mislead the
model prediction as shown in Figure 1 (b).
In general, any pattern co-occurred with certain labels can constitute confounders. Most of them
are even imperceptible, like local textures, small edges, and faint shadows. Since DNN models
are based on the statistical association between input and output, they inevitably learn these spu-
rious correlations, which are “predictive” when the distribution of confounders remains the same
in training and testing. However, their brittle nature makes them vulnerable to small perturbations
as shown in Figure 1 (c). In Section 3, we will provide a formal revisit for the adversarial attack
in the causal viewpoint, where we also design a Confounded-Toy dataset to demonstrate how an
adversarial attacker fools the model by exploiting the confounding effect.
Unlike machine vision that scans all the pixels in an image at once, human vision continuously per-
ceives the image using “retinotopic sampling” (Arcaro et al., 2009) via non-uniformly distributed
retinal photoreceptors at each time frame as shown in Figure 2 (a). We conjecture that such a mech-
anism is the answer to Q2, because it can be viewed as causal intervention by using instrumental
variable (Greenland, 2000), denoted as R in Figure 2 (b). With the help of R, the confounded image
observation X is no longer dictated only by the confounders. Since the choice of R is designed to
be independent of C, as it only depends on the structure of retina, its direct effect on Y can thus
be used to mitigate the confounding effect even though C is unobserved. Intuitively, non-robust
confounder patterns are local impulses that won’t perform consistently across different retinotopic
centers. They are either captured or not by a retinotopic observation. Meanwhile, causal features are
consistent structures. Forcing a model to learn features that linearly vary with the change of R can
suppress unstable confounding effects. To this end, in Section 5, we propose the Causal interven-
tion by instrumental Variable (CiiV \si:v\) framework that combines a spatial data augmentation
through retinotopic sampling with a consistency regularization loss as shown in Figure 2 (c).
2
Under review as a conference paper at ICLR 2022
Figure 2: The proposed CiiV framework (detailed in Section 5): (a) the retinotopic augmentation
that serves as the instrumental variable; (b) the proposed causal graph; (c) the causal intervention
made by the proposed regularization that suppresses non-robust confounding effects.
Our key contributions are as follows:
•	We introduce a causal regularization termed CiiV to suppress the learning of non-robust features
in DNN models, which not only offers a proactive defender, but also opens a novel yet funda-
mental viewpoint of adversary research.
•	Extensive experiments on a wide range of settings from the adversarial evaluation checklist (Car-
lini et al., 2019) in CIFAR-10, CIFAR-100, and mini-ImageNet demonstrate that CiiV can with-
stand adaptive attacks, including the state-of-the-art AutoAttack (Croce & Hein, 2020b).
•	As a general regularization that is orthogonal to most of the previous defenders, the proposed
CiiV can be easily plugged into other methods to further boost their adversarial robustness.
2 Related Work
Adversarial Examples. Adversarial examples undermine the reliability and interpretability of DNN
models in various domains (Wang et al., 2021; Qi et al., 2021; Finlayson et al., 2019; Xie et al.,
2017; Xiang et al., 2019; Cisse et al., 2017; Carlini & Wagner, 2018; Huang et al., 2017) and set-
tings (Diao et al., 2021; Moosavi-Dezfooli et al., 2016; Kurakin et al., 2016; Papernot et al., 2016;
Carlini & Wagner, 2017; Zheng et al., 2019). Despite of various defenders proposed to improve
the adversarial robustness, a universal remedy that can proactively defend against all the known
and unknown attackers is still absent. Generally, the existing defenders fall into the following four
categories: adversarial training (Szegedy et al., 2013; Wong et al., 2019; Dong et al., 2020), data
augmentation (Zhang et al., 2018), de-noising (Buckman et al., 2018; Xie et al., 2019), and certified
defense (Cohen et al., 2019). In Section 4 we will systematically revisit them and compare them to
the proposed CiiV from a causal viewpoint.
Causality in Adversarial Robustness. Recently, causality has gradually been accepted as a poten-
tial way to explain adversarial robustness. Zhang et al. (2020b; 2021) provide a causal perspective
to understand the adversarial vulnerability of DNN models; Yang et al. (2019) utilize the super-
vised pixel-wise masking to conduct causal intervention; Singh et al. (2021) attempt to unify the
adversarial robustness with the distributional shift. However, the solutions they provided are either
subject to additional supervisions, complicated causal graph and training strategies, or parallel to the
existing adversarial training variants. Meanwhile, this paper provides a more feasible causal expla-
nation for the adversarial vulnerability, by which we can design an effective plug-and-play causal
regularization.
Causal Graph and Intervention. Pearl’s graphical model (Pearl et al., 2016) is adopted in this
paper, where directed edges indicate the causality between node variables. The causal graph of
the proposed CiiV framework is illustrated in Figure 2 (b), where R, C, X, Y indicate retinotopic
sampling mask, confounding pattern, image, and prediction, respectively. X J C → Y denotes
that confounder C is a common cause, affecting the distribution of both X and Y, e.g., the edge
in Figure 1 (a). X → Y denotes the desired causality that a robust model is expected to learn. To
achieve that, the ultimate goal of causal intervention is to identify the causal effect of X → Y by
removing all spurious correlations (Pearl & Mackenzie, 2018), denoted as P(Y|do(X = x)). It
can be either implemented as active intervention, like the randomized controlled trial, or passive d-
separation (Roy, 2020; Pearl et al., 2016), by which observing the confounder can block the spurious
path, e.g., by conditioning on C, the dependency of path X J C → Y is blocked.
3
Under review as a conference paper at ICLR 2022
3 A Causal View on Adversarial Attack
In causality (Pearl et al., 2016), the total effect and causal effect of a predictive model based on the
input X can be defined as P(Y |X), P(Y |do(X = x)), respectively. Given the proposed causal
graph in Figure 1, the confounding path X J C → Y causes the inequality between the above two,
and thus the confounding effect can be represented as their difference.
Meanwhile, the general adversarial attack can be formulated as maximizing the probability of a
tampered category Y = y within the budget De (Ren et al., 2020), denoted as follows:
max
δ∈D
P(Y = y∣x = X + δ) H Eyilog(PO,
i
(1)
where y = y0(y0 = y) for targeted attack, y = -y for untargeted attack; yi and Pi are i-th entries
of y and prediction p, respectively; δ is the additive perturbation; budget De is usually considered as
an enclosing ball under l2∕l∞ norm within radius e (Zheng et al., 2019; Kurakin et al., 2016).
Notably, a valid De is not allowed to
change the semantic structures, as they
are designed to be imperceptible, i.e.,
the causal effect P (Y ∣do(X = x))
is invariant to δ. Otherwise, the per-
turbation would become a “poisoning”
that is beyond our scope (Tramer et al.,
2020a). Therefore, equation 1 essen-
tially equals to maximize a tampered
confounding effect through perturba-
tions: maxδ P(Y = y∣X = X +
δ) - P(Y = y∣do(X = x + δ)), sub-
ject to P(Y = y∣do(X = x + δ))=
P(Y = y∣do(X = x)), which applies
to all kinds of attacks (Goodfellow et al.,
Figure 3: (a) A Confounded-Toy Dataset with images
that are composed of causal geometries and confound-
ing color blocks. The adversarial examples generated by
the model (c) w/ and (b) w/o the proposed CiiV.
2015; Madry et al., 2018; Brendel et al., 2018; Chen et al., 2017; Kurakin et al., 2016).
To intuitively demonstrate the above causal theories, we design a Confounded-Toy dataset as shown
in Figure 3 (a), where images are composed of causal geometries and confounding color blocks.
Similar to our example in Figure 1, a model directly trained on this dataset will recklessly learn
the stochastic color block C that shows statistical correlation with the category as the indicator of
Y. As a result, adversarial examples generated by a PGD attacker on this model mainly tamper the
confounding patterns (Figure 3 (b)). In contrast, the proposed CiiV regularization forces the model
to learn causal features instead, so it can only be fooled by poisoning the geometry (Figure 3 (c)).
More details of this Confounded-Toy dataset will be introduced in Appendix A.
4 A Causal View on Adversarial Defense
It has been acknowledged that directly adjusting
an unknown confounder C for P (Y ∣do(X = X))
without any assumption is impractical in causality
field (D’Amour, 2019). Due to the fact that adversar-
ial examples are governed by unobserved confounding
effects, most of the existing defending methods have
to either intuitively assume a generative noise ε to be
the underline C or assume C to be certain identifiable
noisy features that can be explicitly purified.
(a) Undermining Confounding Effects
Figure 4: Two common strategies to in-
crease the adversarial robustness.
(b) Purifying Causal Features
Specifically, adversarial training and its variants (Szegedy et al., 2013; Wong et al., 2019; Dong et al.,
2020) together with some certified defenders like randomized smoothing (Cohen et al., 2019) design
some additive noises ε to imitate adversarial perturbations, then undermine the confounding effect by
asking the model to be robust against ε. On the other hand, the de-noising approaches, no matter the
pre-network de-noising (Xie et al., 2018; Samangouei et al., 2018; Buckman et al., 2018) or the in-
network de-noising (Li et al., 2020; Xie et al., 2019) consider confounders tobe explicitly removable
4
Under review as a conference paper at ICLR 2022
patterns. Therefore, these common strategies can be summarized by two graphical operations as
shown in Figure 4.
However, we can neither guarantee the C to be equal to ε, nor ensure all possible C to be disen-
tangled and purified. Relying on the observation of such assumptive C will at best make the above
defenders robust against a subset of potential confounders.
Among all existing defenders, mixup (Zhang et al., 2018) is most related to the proposed CiiV. It
intervenes an image xi by linearly fusing with another image xj , then forces the prediction Y similar
to the same combination of their one-hot labels. Yet, a valid instrumental variable is required to be
independent of the confounder as we will introduce in the next section. Unfortunately, a new image
xj can still depend on the same confounder of xi . Recent studies (Moosavi-Dezfooli et al., 2017;
Zhang et al., 2020a) found that universal adversarial perturbations across images also exist, which
explains why mixup cannot survive strong attackers.
5 Approach
After connecting the adversarial vulnerability to the confounding effect learned by DNN models, the
remaining question is how to obtain the pure causal effect, which is equivalent to applying causal
intervention P(Y |do(X = x)) on the deep learning. Generally, there are four major interventions:
randomized controlled trial, backdoor adjustment, front-door adjustment, and instrumental variable
estimation. However, the randomized controlled trial requires the control over causal features, the
backdoor and front-door adjustments assume confounders or mediators to be observed, which are
impractical for imperceptible adversarial perturbations. Therefore, we are interested in the last in-
strumental variable estimation that does not require such assumptions.
5.1 Instrumental Variable Estimation
According to the definition (Baiocchi et al., 2014; Guo & Small, 2016), a valid instrumental variable
should satisfy: 1) it is independent of the confounder variable; 2) it affects Y only through X. The
instrumental variable can help to extract the causal effect of X → Y from R → X → Y , which is
not confounded by C (d-separated).
To better demonstrate the use of the instrumen-
tal variable (Bowden & Turkington, 1990), we
design two linear confounded models w/ and
w/o the instrumental variable as shown in Fig-
ure 5. All variables are assumed to be linked
(b) With Instrumental Variable
by linear weights w*. The ConfoUnder is an	(a) Without Instrumental Variable
independent variable sampled from a normal
distribution: C 〜 N(0,1). The total effect FigUre 5: ThecaUsalgraPhsW/andw/otheinstru-
and caUsal effect of X → Y can be repre- mental variable. Nodes are assUmed to be linked
sented as Y[X = x] = wxyx + wcyc and throUgh linear associations w*.
Y [do(X = x)] = wxyx, resPectively. Note that we slightly abUse the notation of normalized
effects P(Y |X) and P(Y |do(X = x)), and Use the form of Unnormalized logits for simPlicity.
In the given confoUnded model of FigUre 5 (a), since X is dePendent on the confoUnder C as x =
wcx c + bx , where bx is the indePendent comPonent of X, we cannot directly estimate the caUsal
effect Y [do(X = x)] by simPly aPPlying linear regression on (x, y) Pairs.
If C is observable, the caUsal intervention coUld be condUcted Using the backdoor adjUstment:
P (y|do(x)) = Pc P (y|x, c)P (c). The caUsal effect is thUs estimated from the total effect by the
observed c and its distribUtion:
Y[dο(X = x)] = WxyX + Wcy E C ∙ P(C) = Wxyx,	(2)
c
where the confounding effect degrades to a constant as Ec C ∙ P(c) = 0.
However, if C is Unobservable, both backdoor adjUstment and the caUsal graPh in FigUre 5 (a) cannot
remove the confoUnding effect. To this end, the instrUmental variable R is introdUced as shown in
FigUre 5 (b), where X is now maniPUlated by both C and R as x = WcxC + Wrxr + bx . DUe to
5
Under review as a conference paper at ICLR 2022
the fact that R is independent of C, the weight of causal link X → Y can be learned by applying
different r onto (x, y) pairs, i.e., yri - yrj = wxy(xri - xrj). The causal effect is thus estimated as
follows:
Y [do(X = x)] = yri----yrj X = Wxy x,	(3)
xri	xrj
where subscripts ri 6= rj indicate the value of X and Y under different instrumental variable R.
The d-separated of R → X J C ensures the subtraction eliminating the confounding effect during
training.
5.2 The Proposed CiiV
With the help of instrumental variable R, the causal effect of the above linear example can be easily
estimated. Yet, in practice, the effect of additive R on an image is just as incomprehensible as
the additive perturbation C, which doesn’t introduce any useful inductive bias. Besides, the above
subtraction is also hard to converge during backpropagation, as it may generate confusing gradients
with opposite directions of yri and yrj.
In the proposed CiiV framework, we consider
the retinotopic sampling mask as a multiplying
instrumental variable and use it to augment the
original dataset like Figure 6. Inspired by the
human vision, the retina is known to consist
of photoreceptors and a variety of other neu-
rons (Reddy et al., 2020). Retinotopic sampling
is the result of the non-uniformly spatial dis-
tribution of these receptors (Kolb et al., 1995;
Arcaro et al., 2009), where the central fovea is
^center	X	XT
Figure 6: Examples of retinotopic sampling and
how it serves as the instrumental variable.
%ottorn	X	XT
significantly denser than the peripheral. It means that human vision is spatially lopsided by a cen-
tralized mask, which inspires us to adopt the retinotopic sampling mask with different centers as the
instrumental variable R. Luckily, it also satisfies the requirements of a valid instrumental variable
discussed in Section. 5.1: 1) the pre-defined retinotopic mask is guaranteed to be independent of
any confounder in an image; 2) its effect on the prediction Y can only pass through the change
of causal features, as the non-robust confounders won’t manifest stable patterns under different R.
More detailed motivations behind the proposed CiiV will be discussed in Appendix B.
Therefore, we adopt the multiplying retinotopic mask as our instrumental variable R and design
R → X to be an augmentation function on image Xr = f (χ,r), where f (∙) applies different
retinotopic sampling masks r onto the confounded image x. The function is implemented as a
differentiable multiplication layer and proved not to suffer from gradient obfuscation (Athalye et al.,
2018) in Section 6. Detailed designs and experiments of f(x, r) are investigated in Appendix C.
Intuitively, when an object moves from the corner of our eyes to the center, the recognizability
monotonously increases with the proportion of its captured contour, so we assume that the causal
effect is linearly corresponding to the spatial coverage αr of a retinotopic mask r while the con-
founding effect is not. It is also consistent with previous findings (Du et al., 2020) that visual
confounders are usually high-frequency local components unevenly distributed in space. The rela-
tionship between the total effect and causal effect can thus be written as follows:
Y[X = xr] = wxy xr + wcyc ≈ αrY[do(X = x)] + wcy c.	(4)
Note that we don’t need to explicitly observe the above c. We can directly model the Y [do(X = x)]
by assigning different r instead. The trick lies in the proposed CiiV regularization loss as follows:
LCiiV = X kαrj Y [X = xri] - αriY[X = xrj]k,	(5)
ri 6=rj
where ri and rj are two retinotopic sampling masks with spatial coverage αri and αrj, just like
rcenter and rbottom in Figure 6. Since wcyc is independent of r, the above regularization can thus
force the model to suppress the confounding effect. In practice, we implement CiiV as an L1 loss
on the feature space extracted by the backbone rather than the logit space, as the classifier weights
can be taken out of the above regularization. Otherwise, the LCiiV could hurt the learning of the
classifier. The overall training loss would be the combination of the conventional cross-entropy loss
and the proposed CiiV loss with a trade-off parameter as LAll = LCE + βLCiiV .
6
Under review as a conference paper at ICLR 2022
Datasets	CIFAR-10						CIFAR-100					
Attackers	Clean	FGSM	PGD-10	AA-l∞	AA-12	Overall	Clean	FGSM	PGD-10	AA-l∞	AA-l2	Overall
Baseline	94.42	30.82	-004-	-00-	-00-	25.06	74.53	^3I	00	-00	-00-	15.75
mixup	95.31	50.41	2.23	0.0	0.0	29.59	77.32	16.60	0.49	0.0	0.0	18.88
BPFC	90.21	24.58	6.19	2.92	35.55	31.89	61.48	17.00	10.23	7.17	29.16	25.01
RS	83.44	53.58	47.06	40.10	75.02	59.84	54.63	26.62	20.21	18.50	47.26	33.44
(ours) CiiV	86.89	64.44	50.75	43.23	82.48	65.56	58.88	32.48	23.63	23.05	55.40	38.69
(ours) CiiV+mixup	87.14	65.28	53.49	47.24	81.97	67.02	56.90	35.48	27.56	26.44	53.14	39.90
(ours) CiiV+RandAug	89.12	67.96	55.01	47.14	83.77	68.60	59.26	36.10	26.25	25.59	55.81	40.60
ATFGSM	84.52	54.42	43.84	37.94	60.20	56.18	51.99	26.27	22.54	18.31	31.06	30.03
ArPGD-10	83.94	52.90	47.19	43.18	55.46	56.53	56.48	25.99	22.56	20.04	28.96	30.81
(ours) CiiV+ATFGSM	83.67	67.28	57.96	50.93	80.09	67.99	53.83	39.00	32.20	30.48	50.47	41.20
(OUrS) CiiV+ATpGD-10	81.35	68.11	59.72	54.21	78.97	68.47	51.73	38.59	33.85	32.01	49.39	41.11
Table 1: The performances of white-box attack on CIFAR-10 and CIFAR-100. The upper half
contains the AT-free defenders while the bottom half reports the AT-involved defenders.
6 Experiments
6.1	Datasets and Settings
Datasets. We evaluated the proposed CiiV and other defenders on three benchmark datasets:
CIFAR-10, CIFAR-100, and mini-ImageNet (Vinyals et al., 2016). Both CIFAR-10 and CIFAR-
100 contain 60K samples with the size of 32x32. mini-ImageNet is originally proposed by Vinyals
et al. (2016) for few-shot recognition, which consists of 100 classes and each has 600 images. We
scaled the size of images to be 64x64 and split them into train/val/test sets with 42k/6k/12k images.
Training Details. We followed
Pang et al. (2021)’s project to set all
the hyper-parameters and architec-
tures. All models were trained us-
ing the SGD optimizer with 0.9 mo-
mentum and 5e-4 weight decay. Ex-
periments were conducted on GTX
2080ti GPUs with 128 batch size and
110 total epochs. The learning rate
was started with 0.1 and updated by
the factor of 0.1 at the following
Table 2: The white-box attack on mini-ImageNet.
Datasets	mini-ImageNet					
Attackers	Clean	FGSM	PGD-10	AA-l∞	AA-l2	Overall
Baseline	71.17	1.37	-0.01	~0.0-	-00-	14.51
mixup	73.88	2.96	0.0	0.0	0.0	15.37
BPFC	55.34	9.37	3.58	1.74	31.91	20.39
RS	52.15	15.09	13.25	6.93	45.82	26.65
(ours) CiiV	49.18	19.03	9.02	8.73	46.08	26.41
(ours) CiiV+mixup	48.83	23.93	15.12	11.45	45.47	28.96
(ours) CiiV+RandAug	51.65	32.22	24.82	18.87	48.47	35.21
ATF GSM	45.62	22.12	-922-	3.70-	21.39	20.41
ATP GD-10	49.79	20.20	16.57	13.52	31.92	26.40
(ours) CiiV+ATFGSM	44.66	30.53	23.83	18.76	41.57	31.87
(ours) CiiV+ATP GD-10	42.85	31.72	25.46	19.30	39.72	31.81
epochs {10, 100, 105}. The trade-
off parameter β was also initialized by 0.1 then multiplied by 10 at epochs {25, 50, 75}. Nine
retinotopic centers were selected by using the 1/6, 1/2, and 5/6 of width and height for each image.
ResNet18 (He et al., 2016) was utilized as the default backbone.
Details of Threat Models. We mainly evaluated the defenders on the clean images together with
four threat models: FGSM (Goodfellow et al., 2015), PGD-10 (Madry et al., 2018), AA-l∞ (Au-
toAttack l∞), and AA-l2 (AutoAttack l2) (Croce & Hein, 2020b). For FGSM and PGD-10, the
adversarial perturbations were created under l∞ norm, where the budget radius was 8/255. PGD-
10 ran 10 iterations with step size 2/255. AutoAttack is a recently released parameter-free attack
that achieves the state-of-the-art attacking success rate under various defenders. It also prevents the
model from gaining a false sense of security from the obfuscated gradients (Athalye et al., 2018).
We set the only parameter of AA-l∞ and AA-l2 to be 8/255 and 0.5, respectively.
Details of Defenders. We divided the defenders into Adversarial Training (AT-involved) and AT-
free approaches. For AT-involved, we adopted two popular defenders: ATFGSM, ATP GD-10, using
the same parameters as the corresponding threat models. For AT-free methods, we investigated
mixup (Zhang et al., 2018), BPFC (Addepalli et al., 2020), and randomized smoothing(RS) (Cohen
et al., 2019). The implementations of mixup and BPFC were directly adopted from their official
github repositories. RS was re-implemented in our framework with σ = 0.25 and the number of test
trials n = 10. The proposed CiiV itself is also an AT-free method. As a general regularization that
is parallel to the above algorithms, we investigated its combination with other defenders as well.
6.2	Diagnosis of Adversarial Robustness
The evaluation of adversarial robustness is always controversial as it can easily suffer from flawed
or incomplete attack settings. To better eliminate the potential wrong sense of security, we followed
7
Under review as a conference paper at ICLR 2022
(a) Unbounded Attack Using FGSM
(b) Unbounded Attack Using PGD-10
(C) Convergence Under Unlimited Iterations
Figure 7: (a, b) Unbounded attacks on CIFAR-10 that increase the budget from 8/255 to 96/255.
(c) The convergence of defenders under unlimited attacking iterations using PGD.
Carlini et al. (2019) to design our experiments and conduct a series of sanity checks at the end of
this section.
Adversarial Robustness Against White-
box Attack. As reported in Table 1 and
Table 2, we applied multiple white-box at-
tacks on all three datasets. The proposed
CiiV and its variants achieved better overall
performances among both AT-free and AT-
involved divisions. Note that Random Aug-
mentation(RandAug) (Cubuk et al., 2020) is
not an adversarial defending method, whose
overall performances on three datasets were
just 24.38, 16.03, and 15.22, respectively.
However, combining CiiV with RandAug
Table 3: The performances of targeted PGD-10 under
four different targeting settings: untargeted (UT), tar-
geted by most likely / random / least likely categories
(T-most, T-random, T-least)._________________
Datasets	CIFAR-10			
Settings	UT	T-most	T-random	T-least
CiiV	50.75	55.62	-71.05-	74.37
CiiV+mixup	53.49	55.87	73.64	78.48
CiiV+RandAug	55.01	59.18	74.70	77.79
CiiV+ATFGSM	57.96	59.44	74.31	77.77
CiiV+ATP GD-10	59.72	60.46	75.21	78.42
worked as well as combining CiiV with AT methods, especially in the real-world mini-ImageNet. It
proves that CiiV is indeed a proactive defender that doesn’t rely on observing confounders. We also
found that AT methods made the model significantly overfit the given attacker in all datasets. Be-
sides, when replacing the training samples of CiiV with AT examples, i.e., CiiV+AT, the robustness
came with the price of decreasing clean performances. However, augmenting CiiV with other AT-
free methods like mixup and RandAug improved both clean and adversarial performances, which
further supported our efforts to design a proactive AT-free defender.
Adversarial Robustness Against AutoAttack. The state-of-the-art AutoAttack is an ensemble
of diverse parameter-free attacks (Croce & Hein, 2020b), including their proposed APGD-CE and
APGD-DLR, the black-box Square Attack (Andriushchenko et al., 2020), and the FAB attack (Croce
& Hein, 2020a) that is robust to obfuscated gradients (Athalye et al., 2018). According to our
experiments on AA-l∞ and AA-l2 , the proposed CiiV performed effectively on all of the above
user-independent attacks. Moreover, combining CiiV with other defenders can further improve their
adversarial robustness on both AA settings, proving that the CiiV is a general causal regularization
parallel to most of the previous methods.
Adversarial Robustness Against Targeted Attack. The performances of the proposed CiiV under
untargeted and targeted PGD-10 attacks were reported in Table 3 using CIFAR-10 dataset. The
targeted results were further divided into three protocols: 1) most likely category, 2) least likely
category, and 3) random category. The results revealed that the confounding effect could also be the
cause of ambiguous prediction, as similar categories are easier to be attacked. We also noticed that
the performances under untargeted attack would be closer to the most likely targeted attack when
the robustness of the model increases. It’s probably because the similar categories share the similar
confounder distributions, i.e., environments, and thus utilized by the attacker.
Adversarial Robustness Under Unbounded Attack. To evaluate the validity of defenders, we
demonstrated the performances of CiiV and its variants together with the baseline and two AT mod-
els under unbounded attacking in Figure 7 (a, b). When the budget of the attacker was increased
from 8/255 to 96/255, all performances were either converged to 0% accuracy for the strong PGD
attack or converged to the random guesses for the weak FGSM attack. Any valid defender shouldn’t
survive such an unbounded attack, as it allows the attacker to modify the entire image and erasing
all causal features. We also tested unlimited iterations of PGD attack, all CiiV and its variants are
successfully converged after 100 iterations as shown in Figure 7 (c). Note that AT and CiiV+AT
8
Under review as a conference paper at ICLR 2022
5 UOneq.InWQd UOneqLnWQd
01 u，0l:si"_。
(a) Adversarial Perturbations on CIFAR-10 Dataset
Figure 8: Generated perturbations of models w/ and w/o CiiV on CIFAR-10 and mini-ImageNet.
0BE- u，。 II0d II:
0u=0∙8 >=0
(b) Adversarial Perturbations on mιrn-ImageNet Dataset
are more robust than other defenders in this setting, which is probably caused by the exposure of
adversarial examples during training.
Ablation Studies. In this para-
graph, we evaluated the effective-
ness of different settings and param-
eters of the proposed CiiV. As re-
ported in Table 4, 1) we investigated
the L1 and L2 versions of the CiiV
loss, where the L1 is slightly better
than L2 ; 2) we tried random assign-
ments of the retinotopic centers as
R={rrand}, which is very close to
Table 4: AbStion StUdieS ofCiiV on CIFAR-IOO.
DataSetS	CIFAR-100					
AttackerS	Clean	FGSM	PGD-10	AA-1∞	AA-12	Overall
L1 CiiV	58.88	32.48	23.63-	23.05	55.40	38.69
L2 CiiV	57.93	31.78	22.27	22.28	54.51	37.75
R={rrand}	58.79	32.20	22.35	22.90	55.28	38.30
RetiAug	61.88	31.69	20.29	18.32	53.19	37.07
β = 0.01	59.85	32.00	21.52	20.85	54.23	37.69
β=1.0	55.26	34.48	26.80	25.02	51.82	38.68
NR = 2	56.45	30.47	21.57	21.03	53.39	36.58
NR = 5	58.34	32.01	22.34	22.61	54.58	37.98
our fixed centers; 3) we also reported the performances of retinotopic augmentation only as Re-
tiAug, which had higher clean performance but worSe adverSarial robuStneSS than CiiV. Note that
RetiAug itSelf can alSo be treated aS an approximation of CiiV by aSSigning all α to 1.O. BeSideS,
croSS-entropy loSSeS under different r alSo forced the model to ignore the non-robuSt confounding
patternS; 4) other choiceS of hyper-parameterS of CiiV were alSo reported, we found that β empiri-
cally Served aS a trade-off between clean performance and adverSarial robuStneSS, and applying more
retinotopic Sampling maSkS (larger NR) would make a better eStimation, yet, itS improvementS got
converged. Additional StudieS and experimentS will be given in Appendix.
Visualization. We viSualized the generated PGD perturbationS for modelS w/ and w/o CiiV in
Figure 8. It demonStrateS that the baSeline modelS can be eaSily fooled by imperceptible confounderS
while the propoSed CiiV forceS the model to learn cauSal featureS, aS the adverSarial perturbationS
have to eraSe the Structural patternS to fool the CiiV model.
The Evaluation Checklist. To verify that the propoSed CiiV doeSn’t Suffer from flawed or incom-
plete evaluationS, the above experimentS were deSigned to follow a SerieS of Sanity checkS introduced
by Carlini et al. (2O19): 1) Iterative attackS are better than Single-Step attackS, e.g., PGD vs FGSM in
Table 1&2 and Figure 7. 2) Unbounded adverSarial exampleS become random gueSSing or O% accu-
racy, e.g., Figure 7 (a,b). 3) The accuracy convergeS with the increaSing of attack StepS: Figure 7 (c).
4) InveStigating both targeted attackS and untargeted attackS, e.g., Table 3. 5) USing black-box at-
tackS and the attackS circumventing obfuScated gradientS to avoid the potentially flawed adverSarial
example generation, e.g., the reSultS under AA-l∞ and AA-l2 in Table 1&2.
7 Conclusion
In thiS paper, we preSented a CiiV defender that worked aS a general cauSal regularization without
the need for adverSarial exampleS. CiiV conSiStS of a Spatial data augmentation uSing different
retinotopic Sampling maSkS, and a regularization loSS that encourageS the model to SuppreSS local
confounding patternS by learning featureS linearly reSponding to Spatial interpolationS. We followed
the checkliSt from Carlini et al. (2O19) to deSign our evaluation experimentS and adopted the uSer-
independent AutoAttack (Croce & Hein, 2O2Ob) aS the main indicator of adverSarial robuStneSS.
ExtenSive experimentS on all SettingS proved that CiiV iS robuSt againSt variouS adaptive attackS,
and it can alSo Serve aS a plug-and-play regularization for other defenderS. BeSideS, thiS paper alSo
provideS a fundamental viewpoint of the relationShip between adverSarial robuStneSS and cauSal
intervention, which may guide the deSign of future defenderS
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
Due to the fact that deep learning algorithms have been widely deployed in the present recom-
mendation system, person identification system, and automatic/assist driving system, the potential
ethical problems are also growing. The adversarial robustness field studied by this paper is one of
the core efforts to address these concerns. Without the adversarial robustness, the DNN-based com-
puter vision systems are vulnerable to imperceptible noises, which threats the safety of human life
and property. Specifically, recent studies of physical-based attacks have proved that simply wearing
specially designed clothes can fool an artificial recognition system. The increasing traffic accident
caused by driver-assistance systems further confirms the above concerns. Therefore, we provide a
general causal regularization that could easily be plugged into most of the current adversarial de-
fending methods to further boost the robustness of the system, which may significantly reduce the
chance of failure recognition caused by adversarial perturbations.
Reproducibility S tatement
In the past decades, the open-source movement of the machine learning community has greatly
promoted the development of related fields. To ensure the reproducibility of the proposed method,
we are going to publish our codes on the github. The anonymous version of our entire project is also
available in supplementary materials. More detailed instructions and explanations of our codes will
be added to the project before releasing to the public. All datasets used in this paper are also publicly
available and can be easily found via torchvision package or other public github repositories.
References
Sravanti Addepalli, Arya Baburaj, Gaurang Sriramanan, and R Venkatesh Babu. Towards achieving
adversarial robustness by enforcing feature consistency across bit planes. In CVPR, 2020.
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at-
tack: a query-efficient black-box adversarial attack via random search. In ECCV, 2020.
Michael J Arcaro, Stephanie A McMains, Benjamin D Singer, and Sabine Kastner. Retinotopic
organization of human ventral visual cortex. Journal of neuroscience, 2009.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, 2018.
Michael Baiocchi, Jing Cheng, and Dylan S Small. Instrumental variable methods for causal infer-
ence. Statistics in medicine, 2014.
Roger J Bowden and Darrell A Turkington. Instrumental variables. Cambridge university press,
1990.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. ICLR, 2018.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In ICLR, 2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
Symposium on Security and Privacy (SP). IEEE, 2017.
Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-
text. In Security and Privacy Workshops (SPW), 2018.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. arXiv, 2019.
10
Under review as a conference paper at ICLR 2022
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In
Workshop on artificial intelligence and security, 2017.
Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. Houdini: Fooling deep struc-
tured prediction models. arXiv, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.
Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. In ICML, 2020a.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In ICML. PMLR, 2020b.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In CVPR Workshops, 2020.
Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. Learnable boundary guided adversarial training.
arXiv preprint arXiv:2011.11164, 2020.
Alexander D’Amour. On multi-cause causal inference with unobserved confounding: Counterex-
amples, impossibility, and alternatives. In AISTATS, 2019.
Yunfeng Diao, Tianjia Shao, Yong-Liang Yang, Kun Zhou, and He Wang. Basar:black-box attack
on skeletal action recognition. In CVPR, 2021.
Xinshuai Dong, Hong Liu, Rongrong Ji, Liujuan Cao, Qixiang Ye, Jianzhuang Liu, and Qi Tian.
Api-net: Robust generative classifier via a single discriminator. In ECCV, 2020.
Jiawei Du, Hanshu Yan, Vincent YF Tan, Joey Tianyi Zhou, Rick Siow Mong Goh, and Jiashi Feng.
Rain: A simple approach for robust and accurate image classification networks. arXiv preprint
arXiv:2004.14798, 2020.
Samuel G Finlayson, John D Bowers, Joichi Ito, Jonathan L Zittrain, Andrew L Beam, and Isaac S
Kohane. Adversarial attacks on medical machine learning. Science, 2019.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Watten-
berg, and Ian Goodfellow. Adversarial spheres. Workshop of ICLR, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICML, 2015.
Sander Greenland. An introduction to instrumental variables for epidemiologists. International
journal of epidemiology, 2000.
Zijian Guo and Dylan S Small. Control function instrumental variable estimation of nonlinear causal
effect models. The Journal of Machine Learning Research, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Suzana Herculano-Houzel. The remarkable, yet not extraordinary, human brain as a scaled-up pri-
mate brain and its associated cost. Proceedings of the National Academy of Sciences, 2012.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2017.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv, 2017.
Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adver-
sarial examples are not bugs, they are features. NeurIPS, 2019.
11
Under review as a conference paper at ICLR 2022
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arxiv, 2018.
Edward Kim, Jocelyn Rego, Yijing Watkins, and Garrett T Kenyon. Modeling biological immunity
to adversarial examples. In CVPR, 2020.
Helga Kolb, Eduardo Fernandez, and Ralph Nelson. Webvision: the organization of the retina and
visual system. book, 1995.
Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world,
2016.
Michael Land. Eye movements in man and other animals. Vision research, 2019.
Guanlin Li, Shuya Ding, Jun Luo, and Chang Liu. Enhancing intrinsic adversarial robustness via
feature pyramid decoder. In CVPR, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks, 2018.
Richard H Masland. The fundamental plan of the retina. Nature Neuroscience, 2001.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In CVPR, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In CVPR, 2017.
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial
training. ICLR, 2021.
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In EuroS&P, 2016.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl and Dana Mackenzie. The Book of Why: The New Science of Cause and Effect. Basic
Books, 2018.
Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal inference in statistics: A primer.
John Wiley & Sons, 2016.
Gege Qi, Lijun Gong, Yibing Song, Kai Ma, and Yefeng Zheng. Stabilized medical image attacks.
In ICLR, 2021.
Manish Vuyyuru Reddy, Andrzej Banburski, Nishka Pant, and Tomaso Poggio. Biologically inspired
mechanisms for adversarial robustness. NeurIPS, 2020.
Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and defenses in deep learn-
ing. Engineering, 2020.
Jason A. Roy, 2020.	URL https://www.coursera.org/lecture/
crash-course-in-causality/conditional-independence-d-separation-CGNIV.
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adver-
sarially robust imagenet models transfer better? arXiv preprint arXiv:2007.08489, 2020.
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against
adversarial attacks using generative models. ICLR, 2018.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Ad-
versarially robust generalization requires more data. NeurIPS, 2018.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially
robust neural network model on mnist. In ICLR, 2019.
12
Under review as a conference paper at ICLR 2022
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. ICLR, 2015.
Harvineet Singh, Shalmali Joshi, Finale Doshi-Velez, and Himabindu Lakkaraju. Learning under
adversarial and interventional shifts. arXiv preprint arXiv:2103.15933, 2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. ICLR, 2013.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. ICLR, 2018.
Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Jorn-Henrik Jacobsen.
Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In ICML,
2020a.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. arxiv, 2020b.
Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the
dangers of evaluating against weak attacks. In ICML, 2018.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. NeurIPS, 2016.
He Wang, Feixiang He, Zhexi Peng, Yong-Liang Yang, Tianjia Shao, Kun Zhou, and David Hogg.
Understanding the robustness of skeleton-based action recognition under adversarial attack. In
CVPR, 2021.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training.
In ICLR, 2019.
Chong Xiang, Charles R Qi, and Bo Li. Generating 3d adversarial point clouds. In CVPR, 2019.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial
examples for semantic segmentation and object detection. In ICCV, 2017.
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial
effects through randomization. ICLR, 2018.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In CVPR, 2019.
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial
examples improve image recognition. In CVPR, 2020.
Chao-Han Huck Yang, Yi-Chieh Liu, Pin-Yu Chen, Xiaoli Ma, and Yi-Chang James Tsai. When
causal intervention meets adversarial examples and image masking for deep neural networks. In
ICIP. IEEE, 2019.
Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae:
disentangled representation learning via neural structural causal models. In CVPR, 2021.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. Understanding adversarial exam-
ples from the mutual influence of images and perturbations. In CVPR, 2020a.
Cheng Zhang, Kun Zhang, and Yingzhen Li. A causal view on robustness of neural networks.
NeurIPS, 2020b.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. ICLR, 2018.
13
Under review as a conference paper at ICLR 2022
Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S Dhillon, and Cho-Jui Hsieh. The
limitations of adversarial training and the blind-spot attack. In ICLR, 2019.
Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard
Scholkopf, and KUn Zhang. Adversarial robustness through the lens of causality. arXiv preprint
arXiv:2106.06196, 2021.
Tianhang Zheng, Changyou Chen, and Kui Ren. Distributionally adversarial attack. In AAAI, 2019.
14
Under review as a conference paper at ICLR 2022
A Details of the Confounded-Toy Dataset
In section 3 of the original paper, we in-
troduced a Confounded Toy (CToy) dataset
to demonstrate the equivalence between
the confounding effect and the adversar-
ial perturbation. The proposed CToy
is a three-way classification, containing
triangles, squares, and circles. It has
10k/1k/1k images for train/val/test split, re-
spectively. All samples are 64x64 colour
images. Except for the causal geome-
tries, there are also confounding patterns,
i.e., red/blue/green blocks, with the size
of 4x4 pixels. Different from the de-
terministic geometry, the color of each
block is sampled from a biased distribu-
tion. For triangle, square, and circle images,
each co-occurred block has 80%/10%/10%,
10%/80%/10%, and 10%/10%/80% prob-
ability to be blue/green/red, respectively.
Therefore, if the confounding distribution
stays the same in both training and test-
ing phases, these patterns are indeed “pre-
dictive” features. Yet, learning these con-
founding patterns would significantly re-
duce the generalization ability of the model,
because there will always be samples that
are dominated by rare color blocks, and they
are also more brittle than geometry struc-
tures. The causal graph of the data genera-
(b) Examples of the ConfoUnded-Toy Dataset
(C) More Adversarial Examples on the ConfoUnded-Tby Dataset
Figure 9: (a) The causal graph of the Confounded-
Toy dataset. (b) More examples of the proposed
Confounded-Toy dataset. (c) More adversarial exam-
ples from the baseline model and CiiV counterpart.
au=ases
>=0
tion procedure and more examples of CToy dataset are illustrated in Figure 9 (a,b).
Based on the specifically designed CToy that only contains two patterns, causal shapes and con-
founding colors, we are able to understand which pattern causes the adversarial vulnerability. As
we can see from Figure 9 (c), adversarial examples of an L∞ PGD attack ( is set to 128/255 for
100% attacking success rate, so we can understand which pattern can successfully fool the model)
that generated from a baseline DNN model were mainly erasing the original color blocks, i.e., the
adversarial perturbation is indeed trying to maximize the tampered confounding effect. Specifically,
the attacker changed the blue and red blocks in triangle and circle images to the green points. It even
painted the entire square images to red. The confounding patterns were obviously tampered in these
images while the causal geometries barely changed. On the other hand, the adversarial examples
of the proposed CiiV model didn’t change the overall colors too much, they directly modified the
shapes. It proves that CiiV successfully prevents the model from learning confounding effects, and
thus attacker can only poison the causal geometries.
With the help of the CToy dataset, we are not only able to verify the proposed confounding theories
for adversarial examples but also visualize the working mechanisms of the proposed CiiV frame-
work, i.e., forcing the model to learn from causal patterns rather than the confounding colors.
B Details of The Proposed Causal Graph
In this paper, we firstly attribute the cause of non-robust features, which were originally introduced
by Ilyas et al. (2019) as an explanation of adversarial examples, to the ubiquitous confounding
effect. But how do confounding patterns affect the learning of causal features and thus hurt the
adversarial robustness? We believe the answer is the failure of feature disentanglement (Yang et al.,
2021; Higgins et al., 2017). As shown in Figure 10, a real-world image is usually composed of
both concepts and contexts. Since those contexts often show statistical correlations with the causal
concept, it’s difficult to disentangle the concept from the context through pure observational data,
15
Under review as a conference paper at ICLR 2022
Figure 10: The details of the proposed causal graph for CiiV regularization and how confounding
patterns cause the adversarial vulnerability.
e.g., the grass feature is usually co-occurred with the dog concept, but it’s also shared by other
outdoor images and absent in indoor dog images, so it’s not a valid causal feature. Due to the
unsuccessful feature disentanglement, adversarial perturbations that simply modify the grass texture
would also lead to the collapse of dog feature, which eventually fool the predictor.
However, the feature disentanglement (Yang et al., 2021; Higgins et al., 2017) per se is still an
open question in machine learning. Otherwise, we only need to simply disentangle the robust and
non-robust features then learn a classifier based on robust features. To tackle the adversarial vulner-
ability in practice, we need to bypass the trap of confounder disentanglement and seek help from the
causal intervention without confounder observation, i.e., the instrumental variable estimation. As
we introduced in section 5, there are two requirements for the choice of instrumental variable. The
independence of R can be directly guaranteed by the manual design of retinotopic sampling masks.
To satisfy the second requirement that the effect of instrumental variable R on Y can only pass
through the causal link X → Y , we assume that causal features are global structures that change
consistently across different retinotopic masks while the adversarial patterns are local impulses (Du
et al., 2020) that simply collapse after applying different retinotopic sampling. Note that this as-
sumption limited the scope of our C to those fragile confounding patterns, which is not trying to
disentangle the semantically meaningful confounders. Fortunately, those semantically meaningful
confounders brought by the unbalanced dataset also won’t be utilized as adversarial perturbations,
e.g., the keyboard is usually co-occurred with the monitor and becomes a confounder of the latter,
but the adversarial attack is obviously not allowed to create or erase a keyboard for the monitor
image based on its definition. Therefore, our assumption still guarantees the proposed retinotopic
sampling to be a valid instrumental variable in the adversarial robustness task.
C Details of The Retinotopic Augmentation
In this section, we will introduce the detailed implementation of retinotopic augmentation and the
selection of its hyper-parameters. The proposed retinotopic augmentation layer xr = f(x, r) applies
a centralized mask r onto the image x, which imitates the biological retina that the central fovea has
significantly denser photoreceptors than the peripheral. The mask r is generated by a non-uniform
spatial sampling, whose sampling probability decreases from a given center to the peripheries. We
conjecture that human vision benefits from the visual attention and continuous eye movement (Land,
2019) to implicitly apply diverse r as the instrumental variable estimation. Intuitively, such a con-
jecture also explains why human can increase the recognition accuracy by continually gazing at
different positions of an object, and why attention or focusing is so important in recognition.
To conduct instrumental variable estimation, we adopt 9 sampling centers (x, y) to generate differ-
ent r. As illustrated in Figure 11 (a), they are (w/6, h/6), (w/6, h/2), (w/6, 5h/6), (w/2, h/6),
(w/2, h/2), (w/2, 5h/6), (5w/6, h/6), (5w/6, h/2) and (5w/6, 5h/6), where w and h are the
width and height of each corresponding image. Note that the fixed retinotopic centers are only
used to ensure the diversity of selected candidates, simply choosing 9 random centers could obtain
very similar performances as shown in Table 4. Given the retinotopic center (x, y), we define the
retinotopic sampling mask r as follows:
rij (x, y) =g(k(i,j) - (x,y)k2) +ε > τ,	(6)
16
Under review as a conference paper at ICLR 2022
where i ∈ [0, w],j ∈ [0, h] are the indexes of image pixels, g(∙) is a non-linear mapping that can
be implemented by various functions, ε is uniformly sampled from [0, 1], τ = 0.9 is the sampling
threshold. The spatial coverage αr used in CiiV is defined as the coverage of the retinotopic mask
P rij/(w * h).
Note that the non-linear smoothing function
g(∙) of r(x, y) can take various implementa-
tions, which won’t affect the performances
of the proposed CiiV too much as long as
the sampling frequency decreases from the
center (x, y) to the peripheries as shown in
Figure 11 (a). We intuitively adopt a nor-
malized mapping g(z) = h((max(z) -
z + α)γ), α = 10.0, γ = 0.3, h(z) =
z/max(z) as our default setting, and we
further tested two simpler non-linear func-
tions Candidate1: g1(z) = 1.0 - z/100 and
Candidate2: g2 (z) = 2.5/(0.5 × z0.5). Ac-
cording to the experimental results in Ta-
ble 5, different g(∙) candidates perform very
similarly under all attack settings, which
proves that CiiV is not sensitive to the de-
tailed implementations of r(x, y). The main
reason for us to choose a more complex im-
plementation of g(∙) is that it can dynam-
ically fit the image size. The other two
simpler functions gι(∙) and g2(∙) have to
change parameters for different sizes of im-
ages, which is less convenient than our de-
fault g(∙).
Since the proposed retinotopic augmenta-
tion aims to imitate the continuous obser-
vations in the human vision. The reac-
(a) The Selected Nine RetinotoPic Centers
ω = 0.5
ω = 0,6	ω = 0,7	ω = 0.8
ω = 0.9
(b) Exposure Parameter ω in Retinotopic Sampling
Figure 11: (a) The selected 9 retinotopic centers used
to generate r in the proposed CiiV. (b) The effect of
applying different exposure parameter ω before mul-
tiplying with the retinotopic sampling mask r
tion of biological visual system to differ-
ent light intensities is also important, which
controls the amount of light absorbed by
the retina. Therefore, given the retinotopic
mask r generated by g(∙), the overall retino-
topic augmented image xr can thus be con-
structed by xr = f(x, r) = 1/N i (r ReLU (x + εi)), where ε is the parameter of exposure
intensity uniformly sampled from (-ω, ω) by N times (N and ω is set to 3 and 0.9, respectively, in
our experiments), denotes element-wise multiplication after normalizing the light intensity. The
reason we introduce the function ReLU (x + εi) is to find the best exposure ratio for a dataset. As
we can see from Figure 11 (b), the selection of exposure parameter ω can change the intensity ofan
observed image. The dark environment requires a smaller ω, so we make it as a hyper-parameter for
each dataset. We set ω to 0.9, 0.9, 0.8 for CIFAR-10, CIFAR-100, and mini-ImageNet, respectively.
Intuitively, such a function imitates how human eyes react to different light intensities of the envi-
ronment by controlling the amount of absorbed light. After multiplying with the retinotopic mask
r, the proposed xr = f(x, r) simulates the signals perceived by the biological retina under different
environments and focusing points, which continuously “intervene” the images observed by humans.
We also investigated hyper-parameters of retinotopic augmentation. As we can see from Table 5,
there are trade-offs among different selections. Larger ω can capture more dark details at the cost
of light details, and vice versa. To obtain the balanced results between clean images and adversarial
examples, we chose the ω = 0.9 as our default setting in CIFAR-10. As to the parameter N that
is used to smooth the image after retinotopic augmentation, the larger N we use the less distortion
will be in the generated xr. Therefore, larger N can significantly increase the performance of clean
images while smaller N can increase the performance of adversarial examples by suppressing more
confounding patterns. Although N = 1 would obtain the best overall result, considering the fact
17
Under review as a conference paper at ICLR 2022
Datasets	CIFAR-10					CIFAR-100				
Attackers	Clean	GN	UN	SPSA	BFS	Clean	GN	UN	SPSA	BFS
Baseline	94.42	72.05	74.66	68.60	35.77	74.53	31.44	34.46	27.57	10.39
mixup	95.31	76.02	78.77	71.87	39.82	77.32	40.21	43.69	36.27	18.35
BPFC	90.21	88.90	89.02	88.91	79.48	61.48	60.47	60.53	60.30	52.11
RS	83.44	83.22	83.16	83.35	82.97	54.63	54.46	54.28	54.53	54.41
(ours) CiiV	86.89	86.47	86.61	86.75	85.60	58.88	58.19	58.75	58.63	57.30
(ours) CiiV+mixup	87.14	86.71	87.11	87.07	86.23	56.90	56.41	56.65	56.70	55.91
(ours) CiiV+RandAug	89.12	88.36	88.59	89.00	87.64	59.26	58.82	58.71	59.21	57.85
ATFGSM	84.52	83.02	83.32	82.98	77.86	51.99	51.05	51.27	51.13	45.08
ATPGD-10	83.94	82.49	82.70	82.59	77.33	56.48	54.64	54.97	54.82	47.83
(ours) CiiV+ATFGSM	83.67	83.10	83.20	83.37	82.34	53.83	53.35	53.77	53.69	52.53
(ours) CiiV+ATpGD-10	81.35	80.74	81.02	81.09	80.22	51.73	51.16	51.43	51.39	50.12
Table 6: Gradient-free attacks on CIFAR-10 and CIFAR-100. The upper half contains the AT-free
defenders while the bottom half reports the AT-involved defenders.
Datasets	CIFAR-10						CIFAR-100					
Attackers	Clean	FGSM	PGD-10	AA-l∞	AA-l2	Overall	Clean	FGSM	PGD-10	AA-l∞	AA-l2	Overall
(VGG13) BaSeIine	90.20	10.48	-00	-00-	0.21	20.18	66.05	3.14	-00	-00-	0.05	13.85
(VGG13) CiiV	83.44	58.75	43.62	36.98	79.08	60.37	50.62	32.83	25.58	24.91	47.37	36.26
(VGG13) CiiV+RandAug	83.91	60.60	45.54	40.51	80.63	62.04	52.72	33.24	26.66	25.76	48.64	37.40
(WRN34-10) Baseline	94.93	32.09	-002-	-00-	-00-	25.41	77.74	9.73	-0715-	-00-	~~0β-	17.52
(WRN34-10) CiiV	87.25	59.50	43.89	38.24	82.82	62.34	60.84	34.04	25.58	25.23	57.45	40.63
(WRN34-10) CiiV+RandAug	88.68	64.02	49.23	44.52	83.59	66.01	63.23	38.30	28.82	27.38	59.81	43.51
Table 7: The performances of Baseline, CiiV, and CiiV+RandAug using different backbones.
that clean images occur more often than adversarial examples in real-world applications, we adopted
N = 3 as our default setting in all datasets.
D More detailed studies and experiments
In this section, we demonstrate additional studies and experiments on 1) several gradient-free at-
tacks, and 2) more backbones.
According to Carlini et al. (2019), some
flawed defenders may fail in gradient-free
attacks. Therefore, we further investigated
four gradient-free attackers: 1) GN (Gaus-
sian Noise), 2) UN (Uniform Noise), 3)
SPSA (Uesato et al., 2018), and 4) BFS
(Brute-Force Search) (Carlini et al., 2019).
Since gradient-free attacks are supposed to
be much weaker than gradient-based at-
tacks, we increased the budget ε to 16/255
for all four gradient-free attackers under l∞
constraint. To be specific, GN and UN add
gaussian and uniform noises, respectively,
to input images. BFS ran 100 times of
GN and reported the most vulnerable adver-
Datasets	CIFAR-10					
Attackers	Clean	FGSM	PGD-10	AA-l∞	AA-l2	Overall
Default	86.89	64.44	50.75-	43.23	82.48	65.56
Candidate1	87.03	64.53	50.37	42.46	82.86	65.45
Candidate2	87.53	63.23	48.88	41.05	82.64	64.67
ω=1.2	86.50	64.96	52.04	46.33	82.18	66.40
ω=1	85.96	64.66	50.81	43.88	81.72	65.41
ω = 0.8	87.21	64.19	49.97	41.72	82.74	65.17
ω = 0.6	86.47	62.65	47.70	39.53	81.36	63.54
N=1	82.44	69.40	59.17	57.25	78.16	69.28
N=2	85.84	66.52	55.01	48.36	81.36	64.42
N=4	87.71	63.77	48.39	40.14	83.06	64.61
N=5	88.29	62.22	46.35	37.87	83.92	63.73
Table 5: The performances of CiiV on CIFAR-10
using different designs of function g(∙) to generate
retinotopic sampling mask r, and different hyper-
parameters ω and N to generate xr .
sarial examples. As to the SPSA, it con-
ducted numerical approximation of gradi-
ents to circumvent the potential gradient masking, the hyper-parameters were set as δ=0.1, step=20,
lr=0.1, batch size=16. According to the experiments in Table 6, all the gradient-free attackers were
significantly weaker than the gradient-based attackers as we expected even with the doubled attack-
ing budget, proving that the proposed CiiV won’t be more vulnerable under gradient-free attacks.
We also applied the proposed CiiV and its combination with Random Augmentation, i.e., the AT-
free versions of defenders, into other backbones, e.g., VGG13 (Simonyan & Zisserman, 2015) and
WRN34-10 (Zagoruyko & Komodakis, 2016). As we can see from Table 7, the proposed CiiV and
its variants consistently increased the adversarial robustness under different backbone models.
18