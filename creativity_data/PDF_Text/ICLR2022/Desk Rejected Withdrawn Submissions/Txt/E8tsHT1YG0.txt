Under review as a conference paper at ICLR 2022
Ridgeless Interpolation with Shallow ReLU
Networks in 1D is Nearest Neighbor Curva-
ture Extrapolation and Provably Generalizes
on Lipschitz Functions
Anonymous authors
Paper under double-blind review
Abstract
We prove a precise geometric description of all one layer ReLU networks z(x; θ)
with a single linear unit and input/output dimensions equal to one that interpolate
a given dataset D = {(xi , f(xi))} and, among all such interpolants, minimize
the '2 -norm of the neuron weights. Such networks can intuitively be thought of as
those that minimize the mean-squared error over D plus an infinitesimal weight
decay penalty. We therefore refer to them as ridgeless ReLU interpolants. Our
description proves that, to extrapolate values z(x; θ) for inputs x ∈ (xi,xi+1)
lying between two consecutive datapoints, a ridgeless ReLU interpolant simply
compares the signs of the discrete estimates for the curvature of f at xi and xi+1
derived from the dataset D. If the curvature estimates at xi and xi+1 have differ-
ent signs, then z(x; θ) must be linear on (xi,xi+1). If in contrast the curvature
estimates at xi and xi+1 are both positive (resp. negative), then z(x; θ) is convex
(resp. concave) on (xi,xi+1). Our results show that ridgeless ReLU interpolants
achieve the best possible generalization for learning 1d Lipschitz functions, up to
universal constants.
1 Introduction
The ability of overparameterized neural networks to simultaneously fit data (i.e. interpolate) and
generalize to unseen data (i.e. extrapolate) is a robust empirical finding that spans the use of deep
learning in tasks from computer vision Krizhevsky et al. (2012); He et al. (2016), natural language
processing Brown et al. (2020), and reinforcement learning Silver et al. (2016); Vinyals et al. (2019);
Jumper et al. (2021). This observation is surprising when viewed from the lens of traditional learning
theory Vapnik & Chervonenkis (1971); Bartlett & Mendelson (2002), which advocates for capacity
control of model classes and strong regularization to avoid overfitting.
Part of the difficulty in explaining conceptually why neural networks are able to generalize is that it
is unclear how to understand, concretely in terms of the network function, various forms of implicit
and explicit regularization used in practice. For example, a well-chosen initialization for gradient-
based optimizers can strongly impact for quality of the resulting learned network Mishkin & Matas
(2015); He et al. (2015); Xiao et al. (2018). However, the specific geometric or analytic properties
of the learned network ensured by a successful initialization scheme are hard to pin down.
In a similar vein, it is standard practice to experiment with (weak) explicit regularizers such as
weight decay, obtained by adding an `2 penalty on model parameters to the underlying empirical
risk. While the effect of weight decay on parameters is transparent, it is typically challenging to re-
formulate this into properties ofa learned non-linear model. In the simple setting of one layer ReLU
networks this situation has recently become more clear. Specifically, starting with an observation
in Neyshabur et al. (2014) the articles Savarese et al. (2019); Ongie et al. (2019); Parhi & Nowak
(2020a;b; 2021) explore and develop the fact that `2 regularization on parameters in this setting is
provably equivalent to penalizing the total variation of a certain Radon transform of the network
function (cf eg Theorem 3.2). While the results in these articles hold for any input dimension, in this
article we consider the simplest case of input dimension 1. In this setting, our main contributions are:
1
Under review as a conference paper at ICLR 2022
Xl X2 x3	/ 4
ʃð 第6	/7	①8
Figure 1: A dataset D with m = 8 points. Shown are the “connect the dots” interpolant fD (dashed
line), its slopes Si and the “discrete curvature" ∈i at each Xi.
1.	Given a dataset D = {(xi ,yi)} with scalar inputs and outputs, we obtain a complete
characterization of all one layer ReLU networks with a single linear unit which fit the
data and, among all such interpolating networks, do so with the minimal `2 norm of the
neuron weights. There are infinitely many such networks and, unlike in prior work, our
characterization is phrased directly in terms of the behavior of the network function on
intervals (xi,xi+1) between consecutive datapoints. Our description is purely geometric
and can be summarized informally as follows (see Theorem 3.1 for the precise statement):
• If we order xi < •一 < Xm, then the data itself gives a discrete curvature estimate
e% := Sgn(Si - Si-i),
yi+1 - yi
si ：=
Xi+1 - Xi
at Xi of whatever function generated the data. See Figure 1.
•	If the curvature estimates ei and ei+1 at Xi and Xi+1 disagree (e.g. for i =3in Figure
1), then the network must be linear on (Xi,Xi+1). See Figures 2 and 3.
•	If the curvature estimates ei and ei+1 at Xi and Xi+1 agree and are positive (resp.
negative), then the network function is convex (resp. concave) on (Xi,Xi+1) and lies
below (resp. above) the straight line interpolant of the data. See Figures 2 and 3.
2.	The geometric description of the space of interpolants of D in the previous bullet yields
sharp generalization bounds for learning 1d Lipschitz functions. This is stated in Corollary
3.3. Specifically, if the dataset D is generated by setting yi = f*(χj for f : R → R
a Lipschitz function, then any one layer ReLU network with a single linear unit which
interpolates D but does so with minimal `2 -norm of the network parameters will generalize
as well as possible to unseen data, up to a universal multiplicative constant. To the author’s
knowledge this is the first time such generalization guarantees have been obtained.
2
Under review as a conference paper at ICLR 2022
2 Setup and Informal Statement of Results
Consider a one layer ReLU network
n
z(x) = z(x; θ) := ax + b + ^X W(2) [w(1)x + b([ ,	[t]+ := ReLU(t) = max {0, t} (1)
j=1	+
with a single linear unit1 and input/output dimensions equal to one. For a given dataset
D = {(xi,yi), i = 1,...,m} ,	-∞ <xι < …< Xm < ∞,	yi ∈ R,
if the number of datapoints m is smaller than the network width n, there are infinitely many choices
of the parameter vector θ for which z(x; θ) interpolates (i.e. fits) the data:
z(xi ; θ)=yi ,	∀ i =1, . . . , m.	(2)
Without further information about θ, little can be said about the function z(x; θ) for x in intervals
(xi,xi+1) between consecutive datapoints when n is much larger than m. This precludes useful
generalization guarantees uniformly over all θ, subject only to the interpolation condition (2).
In practice interpolants are not chosen arbitrary. Instead, they are learned by some variant of gradi-
ent descent starting from a random initialization. For a given architecture, initialization, optimizer,
regularizer, and so on, understanding how the learned network uses the known labels {yi} to assign
values of z(x; θ) for x not in the dataset is an important open problem. To make progress, a fruitful
line of inquiry in prior work has been to search for additional complexity measures based on margins
Wei et al. (2018), PAC-Bayes estimates Dziugaite & Roy (2017; 2018); Nagarajan & Kolter (2019),
weight matrix norms Neyshabur et al. (2015); Bartlett et al. (2017), information theoretic compres-
sion estimates Arora et al. (2018), Rachemacher complexity Golowich et al. (2018), etc (see Jiang
et al. (2019) for a review and comparison). While perhaps not explicitly regularized, these complex-
ity measures are hopefully small in trained networks, giving additional capacity constrains.
In this article, we take a different approach. We do not seek results valid for any network architec-
ture. Instead, our goal is to describe completely, in concrete geometrical terms, the properties of one
layer ReLU networks z(x; θ) that interpolate a dataset D with the minimal possible `2 penalty
n
C(θ)=C(θ,n)= X|Wj(1)|2+|Wj(2)|2
j=1
on the neuron weights. More precisely, we study the space of ridgeless ReLU interpolants
RidgelessReLU(D):= {z(x; θ)	|	z(xf； θ)	=	yi	∀(xi,yi)	∈ D,	C(θ)	=	CJ	,	(3)
of a dataset D , where
C* := inf {C(θ, n) | z(xi； n, θ) = yi ∀(xi, yi) ∈ D}.
θ,n
Intuitively, elements in RidgelessReLU(D) are ReLU nets that minimize a weakly penalized loss
L(θ; D) + λC(θ),	λ ≪ 1,	(4)
where L is an empirical loss, such as the mean squared error over D , and the strength λ of the
weight decay penalty C(θ) is infinitesimal. It it plausible but by no means obvious that, with high
probability, gradient descent from a random initialization and a weight decay penalty whose strength
decreases to zero over training converges to an element in RidgelessReLU(D). This article does
not study optimization, and we therefore leave this as an interesting open problem. Our main result
is simple description of RidgelessReLU(D) and can informally be stated as follows:
Theorem 2.1 (Informal Statement of Theorem 3.1). Fix a dataset D = {(xi,yi),i=1,...,m}.
Each datapoint (xi,yi) gives an estimate
e% := sgn(si - si-i),
yi+1 - yi
si :一
xi+1 - xi
for the local curvature of the data (Figure 1). Among all continuous and piecewise linear functions
f that fit D exactly, the ones in RidgelessReLU(D) are precisely those that:
1The linear term ax + b is not really standard in practice but as in prior work Savarese et al. (2019); Ongie
et al. (2019); Parhi & Nowak (2020a) leads a cleaner mathematical formulation of results.
3
Under review as a conference paper at ICLR 2022
•	Are convex (resp. concave) on intervals (xi,xi+1) at which neighboring datapoints agree
on the local curvature in the sense that q = -1 = 1 (resp. 6i = 6i+ι = — 1). On such
intervals f lies below (resp. above) the straight line interpolant of the data (Figs. 2 and 3).
•	Are linear (or more precisely affine) on intervals (xi, xi+1) when neighboring datapoints
disagree on the local CurVature in the sense that 5 -1 = 1.
Before giving a precise statement our results, we mention that, as described in detail below, the
space RidgelessReLU(D) has been considered in a number of prior articles Savarese et al. (2019);
Ongie et al. (2019); Parhi & Nowak (2020a). Our starting point will be the useful but abstract
characterization of RidgelessReLU(D) they obtained in terms of the total variation of the derivative
of z(x; θ) (see (5)).
We note also that the conclusions of Theorem 2.1 (and Theorem 3.1) also hold under seemingly very
different hypotheses from ours. Namely, instead of '2-regularization on the parameters, Blanc et al.
(2020) considers SGD training for mean squared error with iid noise added to labels. Their Theorem
2 shows (modulo some assumptions about interpreting the derivative of the ReLU) that, among all
ReLU networks a linear unit that interpolate a dataset D, the only ones that minimize the implicit
regularization induced by adding iid noise to SGD are precisely those that satisfy the conclusions
of Theorem 2.1 and hence are exactly the networks in RidgelessReLU(D). This suggests that our
results hold under much more general conditions.
Further, our characterization of RidgelessReLU(D) in Theorem 3.1 immediately implies strong
generalization guarantees uniformly over RidgelessReLU(D). We give a representative example in
Corollary 3.3, which shows that such ReLU networks achieve the best possible generalization error
of Lipschitz functions, up to constants.
Finally, note that we allow networks z(x; θ) of any width but that if the width n is too small relative
to the dataset size m, then the interpolation condition (2) cannot be satisfied. Also, we point out that
in our formulation of the cost C(θ) we have left both the linear term ax + b and the neuron biases
unregularized. This is not standard practice but seems to yield the cleanest results.
3 Statement of Results and Relation to Prior Work
Every ReLU network z(x; θ) is a continuous and piecewise linear function from R to R with a finite
number of affine pieces. Let us denote by PL the space of all such functions and define
PL(D) := {f ∈ PL| f(xi)=yi ∀i =1,...,m}
to be the space of piecewise linear interpolants of D. Perhaps the most natural element in PL(D) is
the “connect-the-dots interpolant” fD : R → R given by
`1 (x),
fD (x):=	'i(x),
I 'm-1(x),
where for i =1,. ..,m - 1, we’ve set
x<x2
xi <x<xi+1,i=2,...,m- 2 ,
x>xm-1
'i(χ) ：= (χ — Xi)Si + yi,	Si ：= yi+1yi.
xi+1 — xi
See Figure 1. In addition to fD, there are many other elements in RidgelessReLU(D). Theorem
3.1 gives a complete description of all of them phrased in terms of how they may behave on intervals
(xi,xi+1) between consecutive datapoints. Our description is based on the signs
e% = Sgn (si — Si-1),	2 ≤ i ≤ m
of the (discrete) second derivatives of fD at the inputs xi from our dataset.
Theorem 3.1. The space RidgelessReLU(D) consists of those f ∈ PL(D) satisfying:
1. f coincides with fD on the following intervals:
4
Under review as a conference paper at ICLR 2022
(1a) Near infinity, i.e. on the intervals (-∞, x2), (xm-1, ∞)
(1b) Near datapoints that have zero discrete curvature, i.e. on intervals (xi-1,xi+1) with
i = 2,...,m — 1 such that q = 0.
(1c) Between datapoints with opposite discrete curvature, i.e. on intervals (xi,xi+1) with
i = 2,..., m — 1 such that j ∙ ^i+ι = —1.
2. f is convex (resp. concave) and bounded above (resp. below) by fD between any consec-
utive datapoints at which the discrete curvature is positive (resp. negative). Specifically,
suppose for some 3 ≤ i ≤ i + q ≤ m — 2 that xi and xi+q are consecutive discrete
inflection points in the sense that
Ei—1 = J,	∈i = ∙∙∙ = ∈i+q,	^i-+q = ei+q+1.
If Ei = 1 (resp. j = —1), then restricted to the interval (xi, Xi+q), f is convex (resp.
concave) and lies above (resp. below) the incoming and outgoing support lines and below
(resp. above) fD:
Ei = 1	=⇒	max{'i-ι(x), 'i+q(x)} ≤ f(x) ≤ fD(x)
Ei = -1	=⇒	min{'i-ι(x), 'i+q(x)} ≥ f(x) ≥ /d(x)
for all x ∈ (xi, xi+q).
We refer the reader to §A for a proof of Theorem 3.1. Before doing so, let us illustrate Theorem
3.1 as an algorithm that, given the dataset D, describes all elements in RidgelessReLU(D) (see
Figures 2 and 3):
Step 1 Linearly interpolate the endpoints: by property (1), f ∈ RidgelessReLU(D) must
agree with /d on (-∞, X2) and (xm—1, ∞).
Step 2 Compute discrete curvature: for i = 2,...,m — 1 calculate the discrete curvature Ei at
the data point Xi.
Step 3 Linearly interpolate on intervals with zero curvature: for all i = 2,...,m — 1 at which
Ei = 0 property (1) guarantees that f coincides with the /d on (xi-1,Xi+1).
Step 4 Linearly interpolate on intervals with ambiguous curvature: for all i = 2,...,m — 1
at which e% ∙ ei+、= —1 property (1) guarantees that f coincides with /d on (xi, Xi+ι).
Step 5 Determine ConVexity/concavity on remaining points: all intervals (Xi, Xi+1) on which f
has not yet been determined occur in sequences (xi, Xi+1),..., (xi+q-ι, Xi+q) on which
Ei+j =1or Ei+j =1for all j =0,...,q. If Ei = 1 (resp. Ei = —1), then f is any convex
(resp. concave) function bounded below (resp. above) by fD and above (resp. below) the
support lines-(x), 'i+q(x).
The starting point for the proof of Theorem 3.1 comes from the prior articles Neyshabur et al.
(2014); Savarese et al. (2019); Ongie et al. (2019), which obtained an insightful “function space”
interpretation of RidgelessReLU(D) as a subset of PL(D). Specifically, a simple computation (cf
e.g. Theorem 3.3 in Savarese et al. (2019) and also Lemma A.14 below) shows that fD achieves
the smallest value of the total variation ||Df|| for the derivative Df among all f ∈ PL(D). (The
function Df is piecewise constant and ||Df|| is the sum of absolute values of its jumps.) Part of
the content of the prior work Neyshabur et al. (2014); Savarese et al. (2019); Ongie et al. (2019) is
the following result
Theorem 3.2 (cf Lemma 1 in Ongie et al. (2019) and around equation (17) in Savarese et al. (2019)).
For any dataset D we have
RidgelessReLU(D) = {f ∈ PL(D) |||Df||TV = ||DfD||TV}.	(5)
5
Under review as a conference paper at ICLR 2022
Xγ X2 /3	74	/5	%6	%7	%8
(a)	Step 1
Xl62	/3
啰4	/5	/6	%7	/8
(b)	Step 2
62 = -1
/ =。
Xl62	/3	力4	/5	/6	17	18
(c)	Step 3
Figure 2:	Steps 1 - 3 for generating RidgelessReLU(D) from the dataset D.
6
Under review as a conference paper at ICLR 2022
ʃl力2
力4
13
/5
/6
力7
力8
(a)	Step 4
Xl力2
力4
的


力7
力8
(b)	Step 5. One possible choice of a convex interpolant on (x4,x5) and of a concave interpolant on (x6,x7) is
shown. Thin dashed lines are the supporting lines that bound all interpolants below on (x4,x5) and above on
(x6,x7).
Figure 3:	Steps 4 - 5 for generating RidgelessReLU(D) from the dataset D.
7
Under review as a conference paper at ICLR 2022
Theorem 3.2 shows that RidgelessReLU(D) is precisely the space of functions in PL(D) that
achieve the minimal possible total variation norm for the derivative. Thus, intuitively, functions in
RidgelessReLU(D) are averse to oscillation in their slopes. The proof of this fact uses a simple
idea introduced in Theorem 1 of Neyshabur et al. (2014) which leverages the homogeneity of the
ReLU to translate between the regularizer C(θ) and the penalty ||Df ||	.
Theorem 3.1 yields strong generalization guarantees uniformly over RidgelessReLU(D). To state
a representative example, suppose D is generated by a function f : R → R:
yj = f* (Xj).
Corollary 3.3 (Sharp generalization on Lipschitz Functions from Theorem 3.1). Fix a dataset D =
{(xi,yi),i=1,. ..,m}. We have
SUp	||f||Lip ≤ "//Lip .	⑹
f ∈RidgelessReLU(D)
Hence, if f is L-Lipschitz and Xi = i/m are uniformly spaced in [0,1], then
SUp	sup |f (x) - f*(x)∣ ≤ 2L.	(7)
f ∈RidgelessReLU(D) x∈[0,1]	m
Proof. Observe that for any i =2,...,m- 1 and X ∈ (Xi, Xi+1) at which Df (X) exists we have
Ci(Si—1 - Si) ≤ Ci(Df(X) - Si) ≤ €i (si+ι - si) .	(8)
Indeed, when Ci = 0 the estimate (8) follows from property (1b) in Theorem 3.1. Otherwise, (8)
follows immediately from the local convexity/concavity of f in property (2). Hence, combining (8)
with property (1a) shows that for each i =1,...,m- 1
llDfllL∞(Xi,Xi+ι) ≤ max {|si-1 | , |si|}.
Again using property (1a) and taking the maximum over i =2,...,mwe find
llDfllL∞(R) ≤ max |si| = ||fD ||Lip .
1≤i≤m-1	t
To complete the proof of (6) observe that for every i =1,...,m- 1
|Si|
yi+ι - y
Xi+1 - Xi
f*(xi+l) - f*(xi)
Xi+1 - Xi
≤ ||f* ||Lip
||fD kip ≤ "//Lip .
Given any X ∈ [0, 1], let us write X0 for its nearest neighbor in {i/m, i =1,...,m}. We find
If(x) - f*(χ)l ≤ If(x) - f(X0)I + lf*(χ0) - f*(χ)l ≤ (llfl∣Lip + llf*kip) ∣χ -χ0l ≤ 2L.
Taking the supremum over f ∈ RidgeleSSReLU(D) and X ∈ [0,1] proves (7).	□
Corollary 3.3 gives the best possible generalization error of Lipschitz functions, up to a universal
multiplicative constant, in the sense that if all We knew about f was that it was L-LiPSchitz and
were given its values on {i/m, i = 1,..., m}, then we cannot recover f in L∞ to accuracy that
is better than a constant times L/m. Further, the same kind of result holds with high probability if
Xi are drawn independently at random from [0, 1], with the 2L/m on the right hand side replaced
by C log(m)L/m for some universal constant C>0. The appearance of the logarithm is due to
the fact that among m iid points in [0, 1] the the largest spacing between consecutive points scales
like C log(m)/m with high probability. Similar generalization results can easily be established,
depending on the level of smoothness assumed for f and the uniformity of the datapoints Xi.
In writing this article, it at first appeared to the author that the generalization bounds (7) cannot
be directly obtained from the relation (5) of prior work. The issue is that a priori the relation (5)
gives bounds only on the global value of IIDf II , suggesting perhaps that it does not provide
strong constraints on local information about the behavior of ridgeless interpolants on small intervals
(Xi,Xi+1). However, the relation (5) can actually be effectively localized to yield the estimates (6)
8
Under review as a conference paper at ICLR 2022
and (7) but with worse constants. The idea is the following. Fix f ∈ RidgelessReLU(D). For any
i* = 3,...,m - 2 define the left, right and central portions of D as follows:
DL ：= {(χi,yi), i<i*} , DC := {(xi,yi), i* - 1 ≤ i ≤ i* + 1} , DR := {(χi,yi), i* <i}.
Consider further the left, right, and central versions of f, defined by
fL(x) = {f"
X < Xi*
X > Xi*
fR (X) = {fi**XX,),
X>Xi*
X<Xi*
and
f (X),
fC (X)=	'i*-1(X),
l`i*(X),
Xi*-1 <X< Xi* + 1
X < Xi*-1
X > Xi*+1
Using (5), we have ||DfD ||TV = ||Df||TV. Further,
||Df ||T V ≥ ||DfL ||TV + ||DfC ||TV + ||DfR||TV ,
which, by again applying (5) but this time to DL, DR and fL,fR, yields the bound
||Df ||T V ≥ ||fDL ||TV + ||DfC ||TV + ||DfDR ||TV .
Using that
m	i* -2	m-1
||DfD||TV =	|si	-	si-1| ,	||fDL ||TV =	|si	- si-1| ,	||DfDR ||TV =	|si	- si-1|
we derive the localized estimate
|si* + 1 - si* | + |si* - si*-1| + |si*-1 - 4 s * * * * *i*-2| ≥ ||DfC ||TV
Note further that
||DfC ||TV ≥ max	Df (X) - min	Df(X),
x∈(xi,xi+1)	x∈(xi,xi+1)
where the max and min are taken over those X at which Df (X) exists. The interpolation condition
f(Xi) = yi andf(Xi+1)=yi+1 yields that
max	Df (X) ≥ si	and	min	Df(X) ≤ si .
x∈(xi,xi+1)	x∈(xi,xi+1)
Putting together the previous three lines of inequalities (and checking the edge cases i =2,m- 1),
we conclude that for any i =2,. . ., m - 1 we have
||Df(X)- sillL∞(xi,Xi+ι) ≤ |si+1 - si| + |si - si-1| + |si-1 - si-2| ,
where we set s0 = s1. Thus, as in the last few lines of the proof of Corollary 3.3, we conclude that
IIfIlLip ≤ 7 llf*l∣Lip	and -31 ≤ J
4 Conclusion and Future Directions
In this article, we completely characterized all possible ReLU networks that interpolate a given
dataset D in the simple setting of weakly '2-regularized one layer ReLU networks with a single
linear unit and input/output dimension 1. Moreover, our characterization shows that, to assign labels
to unseen data such networks simply “look at the curvature of the nearest neighboring datapoints
on each side,” in a way made precise in Theorem 3.1. This simple geometric description led to
sharp generalization results for learning 1d Lipschitz functions in Corollary 3.3. This opens many
direction for future investigation. Theorem 3.1 shows, for instance, that there are infinitely many
ridgeless ReLU interpolants of a given dataset D. It would be interesting to understand which
ones are actually learned by gradient descent from a random initialization and a weak (or even
decaying) '2-penalty in time. Further, as already pointed out after the Theorem 2.1, the conclusions
of Theorem 3.1 appear to hold under very different kinds of regularization (e.g. Theorem 2 in Blanc
et al. (2020)). This raises the question: what is the most general kind of regularizer that is equivalent
to weight decay, at least in our simple setup? It would also be quite natural to extend the results
in this article to ReLU networks with higher input dimension, for which weight decay is known to
correspond to regularization of a certain weighted Radon transform of the network function Ongie
et al. (2019); Parhi & Nowak (2020a;b; 2021). Finally, extending the results in this article to deeper
networks and beyond fully connected architectures are fascinating directions left to future work.
9
Under review as a conference paper at ICLR 2022
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning, volume 80, pp. 254-263, 2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in neural information processing systems, pp. 6240-6249, 2017.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory, pp.
483-513. PMLR, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. Uncertainty in
AI. 2017. arXiv:1703.11008, 2017.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential
privacy. NIPS 2018. arXiv:1802.09583, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Proceedings of the 31st Conference On Learning Theory, volume 75 of
Proceedings of Machine Learning Research, pp. 297-299, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. ICLR 2020. arXiv:1912.02178, 2019.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn TUnyasUvUnakooL RUss Bates, AUgUstm Zidek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583-589, 2021.
Alex Krizhevsky, Ilya SUtskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lUtional neUral networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Dmytro Mishkin and Jiri Matas. All yoU need is a good init. ICLR. 2016. arXiv:1511.06422, 2015.
Vaishnavh Nagarajan and J Zico Kolter. Deterministic pac-bayesian generalization boUnds for deep
networks via generalizing noise-resilience. ICLR 2019. arXiv:1905.13344, 2019.
Behnam NeyshabUr, Ryota Tomioka, and Nathan Srebro. In search of the real indUctive bias: On the
role of implicit regUlarization in deep learning. ICLR Workshop. arXiv:1412.6614, 2014.
Behnam NeyshabUr, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neUral
networks. In Conference on Learning Theory, pp. 1376-1401. PMLR, 2015.
Greg Ongie, Rebecca Willett, Daniel SoUdry, and Nathan Srebro. A fUnction space view of boUnded
norm infinite width relU nets: The mUltivariate case. ICRL 2020. arXiv:1910.01635, 2019.
10
Under review as a conference paper at ICLR 2022
Rahul Parhi and Robert D Nowak. Banach space representer theorems for neural networks and ridge
splines. arXiv preprint arXiv:2006.05626, 2020a.
Rahul Parhi and Robert D Nowak. Neural networks, ridge splines, and tv regularization in the radon
domain. arXiv e-prints, pp. arXiv-2006, 2020b.
Rahul Parhi and Robert D Nowak. What kinds of functions do deep neural networks learn? insights
from variational spline theory. arXiv preprint arXiv:2105.03361, 2021.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? COLT arXiv:1902.05040, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484^89, 2016.
VN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events
to their probabilities. Measures of Complexity, 16(2):11, 1971.
O Vinyals, I Babuschkin, J Chung, M Mathieu, M Jaderberg, W Czarnecki, A Dudzik, A Huang,
P Georgiev, R Powell, et al. Alphastar: Mastering the real-time strategy game starcraft ii, 2019.
Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. ICML and arXiv:1806.05393, 2018.
11