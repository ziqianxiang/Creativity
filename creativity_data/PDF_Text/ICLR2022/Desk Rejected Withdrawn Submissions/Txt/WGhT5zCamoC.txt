Under review as a conference paper at ICLR 2022
Wav2Tok: Deep Sequence Tokenizer for Audio
Retrieval
Anonymous authors
Paper under double-blind review
Ab stract
Search over sequences is a fundamental problem. Very efficient solutions exist
for text sequences, which are made up of discrete tokens chosen from a finite
alphabet. Sequences, such as audio, video or sensor readings, are made up of
continuous-valued samples with a large sampling rate, making similarity search
inefficient. This paper proposes Wav2Tok, a deep sequence tokenizer that con-
verts continuous-valued sequences to discrete tokens that are easier to retrieve via
sequence queries. The only information available for training Wav2Tok is pairs of
similar sequences, i.e., depending on how we form the pairs, the similarity seman-
tics are learnt. Wav2Tok compresses the query and target sequences into short
sequences of tokens that are faster to match. Experiments show consistent per-
formance of Wav2Tok across various audio retrieval tasks, namely, music search
(query by humming) and speech keyword search via audio query.
1	Introduction
Sequence Retrieval aims at retrieving sequences similar to a query sequence, with the constraint
that an ordered alignment exists between the query and the target sequence. The examples include
speech search, where the order of constituent units, such as phonemes, syllables or words, remains
same; and music search - query by humming or query by example - where the order of constituent
units, such as relative notes or phrases, remains same. Apart from audio, the problem also extends
to tasks such as handwritten word search and gesture search. One can define similarity metrics
over the sequences using Dynamic Time Warping (DTW) . But those can be very inefficient if the
sequences are continuous valued and have high sampling rates, as in audio. Also, the high variability
of query sequences makes the hand-made metrics cumbersome and ineffective . A more effective
way of sequence retrieval is by mapping them to discrete tokens. For instance, for linguistic content
based speech retrieval, audio is mapped to discrete linguistic tokens, such as phonemes, syllables
or words, and search is performed over these tokens. For query-by-humming based music search,
audio is mapped to discrete melody-related tokens, such as notes. For query-by-example based
music search, audio is mapped to rule-based landmarks in spectrograms [Wang (2003)]. Hence,
each problem uses a domain-specific hand-made mapping to tokens defined by a domain expert.
In this paper, we propose an audio tokenizer which maps the raw audio feature sequence to a se-
quence of tokens. The tokenizer is trained from pairs of query and target sequences, without any
expert-defined tokens. The proposed algorithm makes use of the fact that relevant landmarks appear
in the same order in both the query and the target sequences. This reduces the dimensionality of data
and preserves the information relevant for effective retrieval. Existing text search methods, namely,
locally sensitive hashing, edit distance, etc., can then be used to query over these tokens.
2	Related Work
Sequence Labelling. With expert-defined tokens, various methods are popularly used for mapping
sequences to tokens. In conventional methods, Hidden Markov Models [Rabiner & Juang (1986)]
and Conditional Random Fields [Lafferty et al. (2001)] have been popularly used for sequence la-
belling. These methods involve a significant amount of domain knowledge and many assumptions to
make tractable models, which are avoided by End-to-End learning models such as Recurrent Neural
Networks (RNNs) using Connectionist Temporal Classification framework [Graves et al. (2006)].
1
Under review as a conference paper at ICLR 2022
Sequence labeling can be used for sequence retrieval by converting the sequences to tokens, which
are easy to search over. But this approach inevitably depends upon expert-defined tokens.
Unsupervised Speech Recognition. Modern Representation learning techniques such as Con-
trastive Predictive Coding (CPC) [van den Oord et al. (2018)] and Autoregressive Predictive Cod-
ing (APC) [Chung & Glass (2020)] are able to generate state-of-the-art continuous representations
which encode the slowly varying phoneme features in raw speech. Wav2Vec [Schneider et al.
(2019)] generated continuous representations with an encoder pretrained to distinguish future time-
steps from a set of distractors sampled from a proposal distribution by optimizing a CPC-based
contrastive loss. After pretraining the models, the representations are mapped to phoneme tokens
via minimization of the Connectionist Temporal Classification (CTC) Loss [Graves et al. (2006)].
With inspiration from VQ-VAE [van den Oord et al. (2017)] which marked the beginning of genera-
tion of discrete representations, VQ-Wav2Vec [Baevski et al. (2019)] discretized the representations
generated by Wav2Vec [Schneider et al. (2019)] with either a K-Means Vector Quantizer [Baevski
et al. (2019)] or a Gumble-Softmax based Vector Quantizer [Baevski et al. (2019)] to generate quan-
tized speech representation used to pretrain a BERT [Devlin et al. (2018)]. Wav2Vec 2.0 [Baevski
et al. (2020)] discretized and masked the representations to solve a CPC-based contrastive task over
the quantized representations which are learned jointly. VQ-APC [Chung et al. (2020)] discretized
the representations to solve APC task over the quantized representations which are learned jointly.
Note, the main motivation to discretize raw audio in a latent space for all aforementioned works are
to explicitly control information content encoded in the representations rather than achieve sequence
tokenization. The codes or tokens generated by the models aren’t constrained to be interpretable and
hence initialised in large numbers. Performing CTC fine-tuning in essence groups codes in the vec-
tor quantizer module of each model together and achieves codebook compression to the number of
phonemes or linguistic units. The discretization of representations do result in performance increase
on various downstream tasks, with a pretrained Wav2Vec 2.0 [Baevski et al. (2020)] achieving state
of the art results on speech recognition by being fine-tuned on only 10 minutes of transcribed au-
dio. SeqRQ-AE [Liu et al. (2019)] is the closest to our work which uses a VQ-VAE [van den Oord
et al. (2017)] to estimate representations for each phoneme in speech. The VQ-VAE has a codebook
containing the same number of members as the number of phonemes. The codebook is trained via
phonetic clustering, temporal segmentation and CTC finetuning on a small amount of transcribed
audio.
Neural Audio Fingerprinting. Now Playing [Arcas et al. (2017)] and [Chang et al. (2020)] use
a Neural Network Fingerprinter (NNFP) module outputting representations which are efficient for
search in query-by-example tasks where the difference between query and actual song is pretty
minute in comparison to humming where only the melody is sung. Now Playing[Arcas et al. (2017)]
trains representations by optimizing the Triplet loss [Schroff et al. (2015)] and [Chang et al. (2020)]
trains representations by simulating the Maximum Inner Product Search on mini-batches of repre-
sentations.
Cross Domain Alignment. Given a pair of semantically similar inputs for training, tasks such
as visual question answering (text and image) and machine translation (text) involve learning of
an alignment. The alignment here is not ordered and the inputs may be from different modalities.
Attention models have been used to find alignment between output entities and input regions [Yao
et al. (2018)]. [Chen et al. (2020)] use Gromov-Wasserstein distance between output and input
entities to match them. However, there is no notion of tokens there, rather the salient entities in the
input are represented as vectors in a graph.
Graph Matching. Generation of representations for graphs can be done by Graph Neural Networks
[Gori et al. (2005)]. Graph matching is used to find similarity of structured graphs [Li et al. (2019)].
However, they perform the matching jointly on the pair of inputs, rather than representing each
input independently. This makes them unsuitable for the search problem at hand due to large run-
time complexity. The distance metrics used for graph matching are based on edit distance [Li et al.
(2019)] and Wasserstein distance [Chen et al. (2020)].
2
Under review as a conference paper at ICLR 2022
3	Methodology
3.1	Problem Statement
Given a pair of semantically similar feature sequences (Xi, Xj) of length Ti and Tj respectively and
feature sequence Xk of length Tk with no similarity to either of the feature sequences Xi or Xj , we
aim to map sequence Xi to sequence of tokens Ti of length Ti0 ≤ Ti , sequence Xj to sequence of
tokens Tj of length Tj0 ≤ Tj, and sequence Xk to sequence of tokens Tk of length Tk0 ≤ Tk where all
the tokens belong to a finite alphabet A such that, the edit distance between Ti and Tj is minimum
while that between Tk and Ti as well as Tk and Tj is maximum.
3.2	Model Architecture
Wav2Tok is comprised of an encoder, which, given input x ∈ Rn , outputs a representation z ∈ Rm ,
and a tokenizer, which, given the encoder output z, outputs a token τ which belongs to a finite
K-element alphabet A = {1, ..., K}. Given, a temporal sequence X = [x1, ..., xT] of length T
where xt is the feature vector at time t. Our model performs frame-wise labelling of X to output a
sequence of tokens T = [τ1, ..., τT] where τt ∈ A is the token assigned to input xt. The encoder
f : X 7→ Z encodes xt in a hyper-spherical space Z using a BiLSTM network [Schuster & Paliwal
(1997)] followed by an L-2 normalization layer. The BiLSTM allows zt to summarise information
in both directions, and hence, encode surrounding context. The tokenizer g : Z 7→ T is a K-means
vector quantizer which vector quantizes encoder output zt with K quantizers or cluster centroids
Wkok ∈ Z; k = 1,…，K and outputs token Tt as, Tt = arg maxk{zt ∙ Wkok}. The centroids are
estimated prior to training via spherical K-means clustering in the spherical space Z generated by
the freshly initialised encoder. The centroids are further tuned with training iterations.
3.3	Model Training
Given a batch of tuples of similar sequences (Xi, Xi0) where Xi = [xit ∈ Rn; t = 1, ..., Ti] and
Xi0 = [xit00 ∈ Rn; t0 = 1, ..., Ti0]. The encoder f maps these sequences to representations Zi =
f(Xi) and Zi0 = f (Xi0). The tokenizer g then labels Zi framewise to token sequence Ti and Zi0
framewise to token sequence Ti0 .
For training, the tuple (Zi, Zi0) are concatenated to form the composed sequence Zii0 = [Zi; Zi0]
of length Tii0 = Ti + Ti0 . The tuple (Ti, Ti0 ) are also concatenated to form the token sequence
Tii0 = [Ti; Ti0] of length Tii0 = Ti+Ti0 corresponding to sequence Zii0. Given Tii0 = [T1ii0, ..., TTii0 0]
and Zii0 = [zi1i0, ..., ziTi0 0], we form a K0 ≤ K length list of prototypes Pii0 = {piτi10, ..., piτiK0 0 } where
{T1, ..., TK0} ∈ A are all K0 unique tokens that occur in Tii0 and piτik00 is a prototype for token Tk0 ∈ A
generated from all vectors in Zii0 mapping to Tk0 as,
Tii0
P- pT；	X zti0 IT』，	⑴
t=1 1τtii0=τk0 t=1
τk0
PTkZ= lħ	⑵
Let Pii0 and Pjj0 be lists of prototypes generated given tuples (Xi, Xi0) and (Xj, Xj0), respectively.
We sample tuples of prototypes (PiTi0, PjTj0 0) where PiTi0 is sampled from Pii0 and PjTj0 0 is sampled from
Pjj0.
APProach A. Given (PiTi0, PjTj0 0), we optimize the following loss function L11 to increase the cosine
similarity between PiTi0 and PjTj00 to 1 if T = T0 and to decrease the cosine similarity to 0 if T 6= T0,
Lii(PK,PjjO,tι) = BinXEnt(pτio ∙ PjjO,tι)	(3)
BinXEnt(pi0 ∙pj,,tI) = tι	∙ log(PTiO	∙	PjjO)	+ (I- tι) ∙lOg(I- Pτi0	∙	PTjO)	⑷
3
Under review as a conference paper at ICLR 2022
where BinXEnt(.) is the binary cross-entropy function, p^ ∙ Pj gives a cosine similarity score
as both the vectors are L-2 normalized, and target t1 is 0 if τ 6= τ0 else 1. We also optimize the
following loss function L12 to increase the cosine similarity of piτi0 or pjτj00 with respect to the kth
quantizer wtkok in our tokenizer g : Z 7→ T to 1 if τ or τ0 = k repectively and to decrease the same
to 0 if τ or τ0 6= k ,
K
L12(pii0, PjjO) = X(BinXEnt(Wi，∙ wtjo,tik) + BinXEnt(pjjo ∙ wtlok,t2k))	(5)
k=1
where target t1k is 0 ifτ 6= k else 1 and target t2k is 0 ifτ0 6= k else 1. The constrastive loss function
L1 for APProach A thus formed with the union of the above two loss functions is given as,
LI(Pii，, PjjO,tι) = LII(Pii，, PjjO,tι) + L12(PiiO, PTjO)	⑹
Approach B. Given (PjiO, Pjjo), a relation network R : Rm → R generates relation score
R(PijiO , PjjjO O ) in the range [0, 1] which is closer to 0 if τ 6= τ0 and closer to 1 if τ = τ0. The
relation score acts as a non-linear similarity metric and is given as,
R(PTiO, PjjO)= σ(WT |Pji，-Pjj，|)	⑺
where σ(.) is the sigmoid function, |.| is the element-wise absolute value function, and Wr ∈ Rm
is the weight matrix of the relation network R : Rm 7→ R. Given R(PijiO, PjjjOO ), we optimize the
following loss function L21 to increase the relation score between PijiO and PjjjOO to 1 ifτ = τ0 and to
decrease the relation score to 0 if τ 6= τ0,
L21(PijiO,PjjjOO,t2) = BinXEnt(R(PijiO,PjjjOO),t2)	(8)
where target t2 is 0 if τ 6= τ0 else 1. We also optimize the following loss function L22 to increase
the relation score of PijiO or PjjjOO with respect to the kth quantizer wtkok in our tokenizer g : Z 7→ T
to 1 if τ or τ0 = k respectively and to decrease the same to 0 ifτ or τ0 6= k ,
K
L22(PijiO,PjjjOO) = X(BinXEnt(R(PijiO, wtkok), t1k) + BinXEnt(R(PjjjOO, wtkok), t2k))	(9)
k=1
where target t3k is 0 ifτ 6= k else 1 and target t4k is 0 ifτ0 6= k else 1. The contrastive loss function
L2 for APProach B thus formed with the union of the above two loss functions is given as,
L2(PijiO,PjjjOO,t2) = L22(PijiO,PjjjOO,t2) + L22(PijiO,PjjjOO)	(10)
A spherical K-means vector quantization loss as inspired from the K-means vector quantization
loss in VQ-Wav2Vec [Baevski et al. (2019)] may be iterated to add more movement to the centroids
and the representations. Given prototype PijiO for token τ ∈ A generated from sequence of represen-
tations ZiiO = [zi1iO, ..., ziTiO O] and sequence of tokens TiiO = [τ1iiO, ..., τTiiO O], our vector quantization
loss is given as,
1	TiiO
LSK(PTiO) =	Tii0 ------£(2 -(Sg(Ztij Mk + Y ∙ Zy ∙ Sg(Wjok)))1jti0=j	(II)
t=1 1jtiiO =j t=1
where γ is a positive constant, and Sg(.) is the stop-gradient operator. The stop-gradient operator
converts the parameter input to a constant during gradient computation. Increasing Sg(Zti ) ∙ WTok
pushes WTok towards Zii keeping Zii static and increasing Zii ∙ S虱Wjok) pushes Zti towards WTok
keeping Wtjok static. With γ set to 0, iterating the loss only affects Wtjok.
Given (PijiO, PjjjO), the final loss function L which we minimize to optimize our model parameters is
given as,
L(PTiO, Pjjo ,ta) = La(PjiO, PjjO ,ta) +。∙ (LSK (Pji，)+ LSK (PjjO))	(12)
where α is a positive constant, ta is the target, and a = 1 corresponds to APProach A while a = 2
corresponds to APProach B.
4
Under review as a conference paper at ICLR 2022
3.4 Sequence Segmentation and Compression
Given a sequence of features X = [x1, .., xT] where T is the sequence length, the encoder f
outputs a sequence of L-2 normalised representations Z = [z1, ., zT] and the tokenizer g outputs
a sequence of tokens T = [τ1,  , τT]. The consecutive repetition of tokens in T highlights a
subsequence in Z whose constituent vectors are similar. Given T, we delete all consecutive token
repetitions to form compressed token sequence T = [τtsι ：公,…,TtsT/ ：teT J of length T0 ≤ T where
ts1 = 1,	teT0	= T, and	τts	:te	is the token which occurs in the interval	[tsi	:	tei]	of sequence	T.
Given the set of token onset and offset times {(tsi, tei)|i ∈ {1, ..., T0}} generated via compression
of T to T with tsi and tei denoting the onset and offset time of the i-th token τts :te in T, we
generate compressed sequence Z = [Z& ：& ,…,ZtsTJteT, ] Oflength T0 ≤ T from Z where Zts ：te
is a representation for all consecutive vectors zt in the interval [tsi, tei] of sequence Z mapping to
the same token τts :te and is given as,
Ztsi :tei
1	tei
-ɪ X Zt
te - ts	t
ei	si t=tsi
(13)
~
Ztsi :t.
ei
0
tsi :tei
||Ztsi:tei ||
(14)
A compressor network C performs the aforementioned compression on sequences Z and T of length
T to output sequences Z and T of length T0 ≤ T respectively.
4	Experiments
We test the performance of Wav2Tok for three tasks, viz., Query by Humming, Speech Search and
Keyword Spotting.
4.1	Datasets
We employ the MIR-QBSH dataset for query by humming tasks and TIMIT [Garofolo et al. (1993)]
dataset for speech search and keyword spotting tasks. MIR-QBSH is composed of 4431 singing or
humming recordings (8KHz, 8-bit, monophonic) submitted by 195 subjects. The dataset is further
partitioned into 48 partitions, each composed of recordings corresponding to one of the 48 ground
truth songs. TIMIT is composed of spoken text audio (16KHz, 16-bit, monophonic). We choose 59
words with reasonable number of occurrences as keywords and all other words as non-keywords,
resulting in a dataset of 2169 keyword recordings.
4.2	Baselines
STFT. This method performs DTW over raw STFT features.
Triplet. Representations generated by a BiLSTM encoder [Schuster & Paliwal (1997)] are tuned via
optimizing the Triplet Loss [Schroff et al. (2015)] as done in optimizing neural audio fingerprints
in Now Playing [Arcas et al. (2017)]. Let a be the reference or anchor sample, p be a positive
sample (same sample but augmented), and n be a negative sample (sample with no similarity to a).
Constraining all vectors in the triplet (a, p, n) tobe L-2 normalized, the loss function for each triplet
of is given as,
LT riplet(a, p, n) = max{d(a, p) - d(a, n) + m, 0}	(15)
d(x, y) = ||x - y||	(16)
where max{.} represents the maximum operator,x and y are vectors with same dimension, and m
is the margin which adds the meaning of similarity to the function. The representations were not
constrained to be L-2 normalized.
MIPS. Representations generated by a BiLSTM encoder [Schuster & Paliwal (1997)] are con-
strained to be L-2 normalized and are trained with divergences taken via simulation of Maximum
5
Under review as a conference paper at ICLR 2022
Inner Product Search [Mussmann & Ermon (2016)] on mini-batches of representation vectors as
proposed by [Chang et al. (2020)]. A minibatch {z∕i ∈ {1,…,N}} of size N is composed of N
pairs {zorg , zrep} where representation vector zrep is an augmented replica of the representation
vector zorg. With zj as the augmented replica ofzi in a minibatch of representation vectors, the loss
function to be minimized is given as,
exp(Sim(zi, zj))
LMlPS(Zi, zj) = - log p	eχ□(Sim(z∙ Zk))	(17)
k6=i exp(S im(zi, zk))
Wav2Vec 2.0. A scaled down and modified Wav2Vec 2.0 [Baevski et al. (2020)] model where the
latent Representations generated by a BiLSTM encoder [Schuster & Paliwal (1997)] are masked and
Product Quantized (PQ) with a Gumble Softmax based Vector Quantizer [Baevski et al. (2019)] (see
Appendix B). For a masked time step t, the latent representation Zt is fed to a Transformer encoder
[Vaswani et al. (2017)] which outputs a contextualised representation ct used to distinguish the true
quantized representation qt output by the vector quantizer given Zt from a set of D + 1 candidate
quantized representations q ∈ Qt containing qt and D distractors. The loss function iterated to train
the model is given as,
L	_ ιoσ exp(Sim(Ct, qt)/K)	β L	Z18
LWav2Vec2 - - log -----------L~~+ -× + + 十 P ' Ld	(18)
Eq 〜QteXp(Sim(ct, q"κ)
where Sim(a, b) = 口；尚| gives cosine similarity score, β is a positive constant, and Ld is a
codebook diversity loss for the Gumble Softmax based Vector Quantizer module encouraging equal
use of the codebook entries (see Appendix B).
Wav2Vec Tok. The same architecture as Wav2Vec 2.0 but here the Gumble Softmax Based Vector
Quantizer is used as a Tokenizer which labels the latent representations with K tokens forming
alphabet A = {1, ..., K}. The representations are also trained optimizing loss function [18].
MIPS and Triplet use a 2-layer BiLSTM as encoder with 3.6 million trainable parameters. We use
the LAMB optimizer [You et al. (2020)] and a Cosine Annealing Learning Schedule [Loshchilov
& Hutter (2017)] with a learning rate restart of 0.0001 to train them. Wav2Vec 2.0 and Wav2Vec
Tok use a 2-layer BiLSTM encoder with 3.6 million trainable parameters to generate latent space
and a 2-layer Transformer with 3 attention heads with 8.5 million trainable parameters to generate
context space amounting to models with 12.1 million trainable parameters. Both are trained using
the ADAM [Kingma & Ba (2017)] optimizer and a linear learning schedule warming-up a learning
rate of 0.001 in the first 8% of the training steps.
4.3	Our Models
TokA. Model trained in accordance to Approach A. We do not iterate over the quantization loss
LSK by setting positive constant α in final loss function L to 0 since we are calculating the con-
trastive loss L1 with cosine similarity allowing it to give the same regularisation as LSK .
TokB. Model trained in accordance to Approach B.
TokB+Trans. Model trained in accordance to Approach B but with a Transformer encoder
[Vaswani et al. (2017)] as encoder network instead ofan BiLSTM and α = 0.
TokB+NoSim. Model trained in accordance to Approach B but with batches of tuples of sequences
which need not be semantically similar and α = 0.
TokA, TokB, and TokB+NoSim use a 2-layer BiLSTM as encoder with 3.6 million trainable param-
eters. TokB+Trans uses a 2-layer Transformer with 3 attention heads as encoder with 8.5 million
trainable parameters. We train then using the ADAM [Kingma & Ba (2017)] optimizer and a linear
learning schedule, warming-up a learning rate of 0.001 in the first 8% of the training steps.
4.4	Music Melody Search: Query by Humming
Task. Given a test query audio, we are to find in the search audio database the most similar melody.
6
Under review as a conference paper at ICLR 2022
Experiment Details. We divide the MIR-QBSH dataset into 3 splits of training, validation and
test with split sizes in the ratio 2462 : 844 : 1125. The recordings are all converted to Short
Time Fourier Transform matrices with 513 frequency bins, a window length of 1024 samples (128
ms), and hop length of 512 samples. The training and the validation split consists of pairs of similar
audio. The test set consists ofa search database (with audio from validation split) and a set of queries
(disjoint from both training and validation splits). All the audio in the search dataset are converted
to sequences of representations or tokens, using the same tokenizer as used over the search database.
Given a query sampled from the test split, we perform DTW based search (if representations) or
Edit Distance based search (if tokens) over the search database.
Results. The proposed TokB representation with DTW search performs the best in terms of Mean
Reciprocal Rank (MRR) scores (see Appendix D) as compared to other baselines and proposed
methods, as shown in Table 1. Larger the MRR score, better is the method. Quantization of the
representations to tokens (see TokB ED) makes the search faster, while slightly degrading the per-
formance. To check the robustness of the representation or tokens, we also apply a series of transfor-
mations on the queries in the form of time stretch and pitch shift. Melodic semantics are not altered
by these transformations. Along with the original query (V or vanilla) Table 1 also shows the results
for Time Stretched (TS) query, Pitch Shifted (PS) query, and Time Stretched + Pitch Shifted (TS
+ PS) query. Sequence compression further speeds up the search while lowering the performance
metrics (see Compressed columns in Table 1). ED is faster than DTW and compressed sequences
are faster to search. So, ED based search over compressed tokens is the fastest.
Models	Normal				Compressed			
	V	TS	PS	TS+PS	V	TS	PS	TS+PS
STFTDTW	0.44	0.28	0.38	0.214	-	-	-	-
Triplet DTW	0.278	0.158	0.238	0.14	-	-	-	-
MIPS DTW	0.45	0.15	0.414	0.146	-	-	-	-
Wav2Vec 2.0 DTW	0.75	0.631	0.714	0.581	-	-	-	-
Wav2Vec Tok DTW	0.73	0.62	0.68	0.563	0.537	0.454	0.5	-04-
TokA DTW	0.804	0.75	0.78	0.701	0.6	0.48	0.54	-04-
TokB DTW	0.84	0.8	0.825	0.753	0.722	0.64	0.68	0.57
Wav2Vec Tok ED	0.69	0.62	0.67	-036-	0.414	0.353	0.4	0.336
TokAED	0.66	0.61	0.645	-05-	0.45	0.36	0.425	-03-
TokB ED	0.768	0.741	0.75	0.77	0.63	0.54	0.54	0.48
Table 1: MRR scores for query by humming, with K = 25
Model	Search Time
Normal	
Wav2Vec Tok DTW	3.48
Wav2Vec Tok ED	-02-
TokB DTW	~3.5-
TokB ED	0.32
Compressed	
Wav2Vec Tok DTW	0.4
Wav2Vec Tok ED	0.02
TokB DTW	-06-
TokB ED	0.04
Table 2: Average Search
Time (in s) per query
Effect ofLSK. We study the effect of including LSK to the loss function by setting different values
of α and γ in Eq. 12. We observe an overall performance drop with inclusion of LSK, as shown in
Table 3.
Models	Normal				Compressed			
	V	TS	PS	TS+PS	V	TS	PS	TS+PS
TokB DTW	0.84	0.8	0.825	0.753	0.722	0.64	0.68	0.57
TokB(α = 1,γ = 0) DTW	0.824	0.756	0.8	-071-	0.682	0.523	0.574	0.43
TokB(α = 1,γ = 0.1) DTW	0.81	0.742	0.78	0.676	0.702	0.55	0.621	0.474
TokB ED	0.768	0.741	0.75	0.77	0.63	0.54	0.54	0.48
-TokB(α = 1,γ = 0) ED-	0.685	0.49	0.641	0.42	0.57	0.41	0.46	0.323
TokB(α = 1,γ = 0.1) ED-	0.663	0.459	0.6	0.46	0.474	0.35	0.39	0.3
Table 3: MRR scores for query by humming, with K = 25
Variation in number of Tokens. The effect of varying the size of alphabet A is shown in Table 4.
K = 25 gives the best performance for all models. Although K = 15 and K = 40 gives a small
performance drop in MRR for vanilla queries, the robustness of the tokens to transformed queries
degrades a lot. TokB gave the best performance for almost all the settings of K while Wav2Vec Tok
suffers a large drop in performance for K = 40.
Token Semantics. Query by humming involves similarity based on melody information, which is
carried by the semantic pairing of the audio in training data. As an ablation study, we randomize
this pairing; we call this model TokB+NoSim. The results are presented in Table 5. There is a drop
in performance with TokB+NoSim as compared to TokB, which is more severe for transformed
queries. These results support that the model TokB is learning the semantic information from the
pairs.
7
Under review as a conference paper at ICLR 2022
Models	Normal				Compressed			
	V	TS	PS	TS+PS	V	TS	PS	TS+PS
			K =	15				
Wav2Vec Tok DTW	0.712	0.6	0.67	0.55	0.531	0.428	0.49	0.39
TokA DTW	0.75	0.62	0.7	0.56	0.523	0.375	0.471	0.35
TokB DTW	0.831	0.751	0.8	~0.7-	0.7	0.54	0.6	0.47
Wav2Vec Tok ED	0.66	0.6	0.63	0.53	0.4	0.35	0.37	0.315
TokAED	0.71	0.545	0.65	-05-	0.44	0.385	0.41	0.32
TokB ED	0.773	0.683	0.724	0.62	0.46	0.31	0.42	0.3
			K =	25				
Wav2Vec Tok DTW	0.73	0.62	0.68	0.563	0.537	0.454	0.5	-O-
TokA DTW	0.804	0.75	0.78	0.701	0.6	0.48	0.54	-O-
TokB DTW	0.84	0.8	0.825	0.753	0.722	0.64	0.68	0.57
Wav2Vec Tok ED	0.69	0.62	0.67	0.56	0.414	0.353	0.4	0.336
TokAED	0.66	0.61	0.645	-05-	0.45	0.36	0.425	-03-
TokB ED	0.768	0.741	0.75	0.77	0.63	0.54	0.54	0.48
			K =	40				
Wav2Vec Tok DTW	0.47	0.377	0.45	0.342	0.253	0.235	0.241	0.214
TokA DTW	0.82	0.75	0.79	-07-	0.6	0.45	0.52	0.371
TokB DTW	0.81	0.75	0.78	-07-	0.642	0.534	0.57	0.48
Wav2Vec Tok ED	0.35	0.27	0.33	0.24	0.16	0.15	0.143	0.131
TokAED	0.66	0.56	0.621	0.48	0.46	0.35	0.4	0.283
TokB ED	0.71	0.6	0.665	0.54	0.46	0.35	0.4	~0.3-
V	- Vanilla, TS - T		ime Stretched, PS		- Pitch S	lifted		
Table 4: Effect of varying K : MRR scores for query by humming
Models	Normal				Compressed			
	V	TS	PS	TS+PS	V	TS	PS	TS+PS
TokB DTW	0.84	0.8	0.825	0.753	0.722	0.64	0.68	0.57
TokB+Trans DTW	0.79	0.723	0.765	0.683	0.564	0.434	0.5	-04-
TokB+NoSim DTW	0.83	0.75	0.79	-07-	0.68	0.57	0.63	-05-
TokB ED	0.768	0.741	0.75	0.77	0.63	0.54	0.54	0.48
-TokB+Trans ED	0.72	0.6	0.67	-057-	0.43	0.3	0.373	0.28
TokB+NoSimED	0.724	0.6	0.674	0.53	0.554	0.44	0.473	0.404
Table 5: Ablation Analysis: MRR scores for query by humming
Transformer as Encoder. We use a Transformer with 8.5 million parameters instead of a 3.6
million parameter BiLSTM as our encoder. We call this model as TokB+Trans. There is an overall
drop in performance and robustness of the tokens generated by TokB+Trans incomparison to TokB
(Table 5). Possible causes for such drop could be overfitting by the larger transformer encoder or
the number of attention heads (3, here). Decreasing the number of layers and increasing the number
of attention heads might bring in some improvement in performance.
4.5	Speech Search
Task. Given a query speech audio, we are to find in the search database the audio with similar
speech content.
Experiment Details. We divide the keyword dataset, extracted from TIMIT, into three splits,
viz., training, validation and test with split sizes corresponding to each keyword in the ratio
1414 : 188 : 567. All audio are converted to spectrograms using Short Time Fourier Transform
with 185 frequency bins, a window length of 368 samples (23 ms), and hop length of 92 sam-
ples. We train and test our models in the same fashion as done for query by humming experiments
(Sec. 4.4).
Results. The representations generated by TokB and MIPS perform the best as shown in Table 6.
There is only a slight drop of 0.02 MRR when sequence compression is applied to the representations
generated by TokB (see Compressed columns in Table 6), implying a faster search with high MRR
performance.
8
Under review as a conference paper at ICLR 2022
Models	Normal	Compressed
STFTDTW	0.49-	-
Triplet DTW	-05-	-
MIPS DTW	0.737	-
Wav2Vec Tok DTW	0.64-	05
TokA DTW	0.622	0.624
TokB DTW	0.735	0.71
Wav2Vec Tok ED	~0.4-	07Γ75
TokA ED	0.443	0.364
TokB ED	0.61	0.54	-
Table 6: TIMIT Similarity Search Results
(MRR Scores)
Models	Accuracy	Precision	Recall	F1 Score	Time
STFT DTW	-048-	-066-	0.48	-051-	0.11
Triplet DTW	-047-	-061-	0.47	-049-	0.315
MIPS DTW	-035	-0:68-	0.55	-0:58-	0.383
TokA DTW	-052-	-067-	0.52	-055-	0.352
TokB DTW	-057-	-0:69-	0.57	-06	0.372
TokAED-	-055-	-065-	0.55	-057-	0.204
TokB ED 一	0.63	0.68	0.63	0.62	0.041
Table 7: TIMIT Keyword Spotting Results
4.6	Keyword Spotting
Task. Given a query audio containing an uttered keyword, spot the location of the keyword in a
dataset of long speech recordings. It is considered a hit if the spotted segment in a long audio has a
non-zero overlap with the ground truth segment of the keyword.
Experiment Details. All models are trained on the keyword dataset as done for speech search. We
synthetically generate 564 long utterances each composed of either 60 or 80 non-keywords sam-
pled randomly from TIMIT and 6 to 10 occurrences of keywords sampled from the keyword dataset
extracted from TIMIT. The training set comprises 273 long utterances which are used to set the
detection threshold for the keyword spotters. The test set consists of a query database (with audio
from the validation split we generated from the keyword dataset) and a search database formed with
the remaining 291 long utterances. The recordings are all converted to spectrograms using Short
Time Fourier Transform with 185 frequency bins, a window length of 368 samples (23 ms), and
hop length of 92 samples. We convert the search dataset as well as the query audio into represen-
tation or tokens using a tokenizer. For search, we chunk the long audio into smaller segments. Let
U1 = [u11, ..., u1T] of length T represent the query keyword to be spotted. Here, ut may either be a
representation zt or a token τt or raw STFT feature xt . Let U2 = [u21 , ..., uT 0] represent the long
query of length T0 T. For any time step t in U2, a detection score st is generated as follows,
st
min
l∈{0.5∙T,T,2∙T }
Dist(U1, Ut2:t+l)
(19)
where U∕t+? is the segment of U2 in the time interval (t, t + V) of length l ∈ {0.5 ∙ T,T, 2 ∙ T},
and Dist(.) generates DTW distance when ut = zt or xt, and Edit Distance when ut = τt. We set
detection threshold θ by training our keyword spotter with some training utterances (see Appendix
C). A keyword is detected at time step t in given query iff st ≤ θ. We evaluate performance of
all our keyword spotters based on Accuracy, Precision, Recall, F1 score and Search Time. Many
efficient and faster algorithms exist for text search or genome sequence search but we chose to use
aforementioned approach to test the tokenizer efficacy.
Results. Tokens and representations generated by TokB give the best results in terms of F1 Score,
as shown in Table 7. The tokenization performed by TokB also results in the best search time since
TokB performs the best segmentation of the input utterances. DTW over raw STFT features is fast
because of low dimensionality of the features.
5	Conclusion and Future Work
In this paper, we present a audio sequence tokenizer Wav2Tok that generates semantically meaning-
ful ordered representations (or tokens) that can be used for efficient retrieval by query sequences.
The tokens are learnt from pairs of semantically similar sequences, without any expert generated an-
notations. The tokenizer is tested for audio retrieval tasks involving a variety of semantics - melodic
as well as phonetic. The performance is found to be consistent and better than state of the art models
adapted for retrieval. We would like to apply more efficient search algorithms such as locally sen-
sitive hashing and longest common subsequence search on the generated tokens to further speed up
the search. Also, we are working towards extending the proposed framework to 2D data retrieval.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility
The codes required to reproduce the results of this paper will be made available after the review. The
experiments are performed using standard datasets.
References
Blaise Aguera Areas, Beat Gfeller, RUiqi Guo, Kevin Kilgour, Sanjiv Kumar, James Lyon, Julian
Odell, Marvin Ritter, Dominik Roblek, Matthew Sharifi, and Mihajlo Velimirovic. Now play-
ing: Continuous low-power music recognition. CoRR, abs/1711.10958, 2017. URL http:
//arxiv.org/abs/1711.10958.
Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. CoRR, abs/1910.05453, 2019. URL http://arxiv.org/
abs/1910.05453.
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A frame-
work for self-supervised learning of speech representations. CoRR, abs/2006.11477, 2020. URL
https://arxiv.org/abs/2006.11477.
Sungkyun Chang, Donmoon Lee, Jeongsoo Park, Hyungui Lim, Kyogu Lee, Karam Ko, and Yoon-
chang Han. Neural audio fingerprint for high-specific audio retrieval based on contrastive learn-
ing. CoRR, abs/2010.11910, 2020. URL https://arxiv.org/abs/2010.11910.
Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. Graph optimal
transport for cross-domain alignment. 37th International Conference on Machine Learning, ICML
2020, PartF16814:1520-1531, 2020.
Yu-An Chung and James Glass. Generative pre-training for speech with autoregressive predictive
coding. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 3497-3501, 2020. doi: 10.1109/ICASSP40776.2020.9054438.
Yu-An Chung, Hao Tang, and James Glass. Vector-quantized autoregressive predictive coding. arXiv
preprint arXiv:2005.08392, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL
http://arxiv.org/abs/1810.04805.
J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. Darpa
timit acoustic phonetic continuous speech corpus cdrom, 1993.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. Anew model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist tempo-
ral classification: Labelling unsegmented sequence data with recurrent neural networks. In Pro-
ceedings of the 23rd International Conference on Machine Learning, ICML ’06, pp. 369-376,
New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi:
10.1145/1143844.1143891. URL https://doi.org/10.1145/1143844.1143891.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields:
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning, ICML ’01, pp. 282-289, San Francisco, CA,
USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558607781.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net-
works for learning the similarity of graph structured objects. In International conference on
machine learning, pp. 3835-3845. PMLR, 2019.
10
Under review as a conference paper at ICLR 2022
Alexander H. Liu, Tao Tu, Hung-yi Lee, and Lin-Shan Lee. Towards unsupervised speech recogni-
tion and synthesis with quantized speech representation learning. CoRR, abs/1910.12729, 2019.
URL http://arxiv.org/abs/1910.12729.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.
Stephen Mussmann and Stefano Ermon. Learning and inference via maximum inner product
search. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning, volume 48 of Proceedings of Machine Learn-
ing Research, pp. 2587-2596, New York, New York, USA, 20-22 JUn 2016. PMLR. URL
https://proceedings.mlr.press/v48/mussmann16.html.
L.	Rabiner and B. JUang. An introdUction to hidden markov models. IEEE ASSP Magazine, 3(1):
4-16, 1986. doi: 10.1109/MASSP.1986.1165342.
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael AUli. wav2vec: UnsUpervised
pre-training for speech recognition. CoRR, abs/1904.05862, 2019. URL http://arxiv.
org/abs/1904.05862.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A Unified embedding for face
recognition and clUstering. CoRR, abs/1503.03832, 2015. URL http://arxiv.org/abs/
1503.03832.
M.	SchUster and K.K. Paliwal. Bidirectional recUrrent neUral networks. IEEE Transactions on
Signal Processing, 45(11):2673-2681, 1997. doi: 10.1109/78.650093.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learn-
ing. CoRR, abs/1711.00937, 2017. URL http://arxiv.org/abs/1711.00937.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, E ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa- Paper.pdf.
Avery Wang. An industrial strength audio search algorithm. 01 2003.
Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship for image captioning.
In Proceedings of the European conference on computer vision (ECCV), pp. 684-699, 2018.
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley J. Osher, Yingyong Qi, and Jack Xin.
Understanding straight-through estimator in training activation quantized neural nets. CoRR,
abs/1903.05662, 2019. URL http://arxiv.org/abs/1903.05662.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes, 2020.
11
Under review as a conference paper at ICLR 2022
A Visualization of Tokenization
We present the segmentation of sequences brought about by our tokenizer TokB in the following fig-
ures. Figures 1, 2, 3, present tokenizations of 2 utterances of each of the keywords ’oily’, ’agency’,
and ’carry’ respectively. The red line marks the segments generated as a result of tokenization. The
height of the red line at each time-step of the spectrogram denotes the token assigned to that time-
step. Figures 4, 5, 6, present tokenizations of pairs of songs sharing the same id ’00024’, ’00027’,
and ’00042’ respectively.
(b)
(a)
Figure 1: Tokenization of 2 utterances of the keyword ’oily’
50
75
100
125
150
175
0	25	50	75	0	25	50
(a)	(b)
Figure 2: Tokenization of 2 utterances of the keyword ’agency
12
Under review as a conference paper at ICLR 2022
(a)
Figure 3: Tokenization of 2 utterances of the keyword ’carry
(b)
(a)	(b)
Figure 4: Tokenization of 2 songs with id ’00024’
B Gumble S oftmax based Vector Quantizer
The Gumble Softmax based Vector Quantizer product quantizes input latent representation zt ∈ Rm
with C codebooks each containing K quantizers e ∈ RK× C. Given zt, one of the K quantizers
from each of the C codebooks are chosen resulting in vectors e1, ..., eC which are concatenated and
linearly transformed from Rm to Rd to output qt ∈ Rd . zt is mapped to l ∈ RC ×K logits to give
probability scores for the choice of codeword. The probability pc,k of choosing kth quantizer in cth
codebook is given as,
13
Under review as a conference paper at ICLR 2022
(b)
(a)
Figure 6: Tokenization of 2 songs with id ’00042’
(a)
(b)
Figure 5: Tokenization of 2 songs with id ’00027’
exp(lc,k + nk )∕τ
(20)
where τ is a non-negative temperature, n = -log(-log(u)) and u are samples from the uniform
distribution Unif(0, 1). During forward pass, the codeword is chosen as κ = arg maxj pc,j. The
straight-through gradient estimator [Yin et al. (2019)] is utilized to estimate the gradient during
backward pass.
Codebook Diversity Loss Ld. This loss promotes equal use of all the entries in each of the C
codebooks by maximizing the entropy of the averaged softmax distribution l over the K entries for
14
Under review as a conference paper at ICLR 2022
each codebook Pc across a batch of utterances.
1 CK
Ld = CK E Epc,k logpc,k
c=1 k=1
(21)
C Setting of Detection Threshold for Keyword S potting
Given the utterances are converted to sequences of tokens, we use Edit distance as detection score.
The Edit distance being an integer, we find the mode mhit of the set of detection scores st generated
at time steps t when a keyword is present. We also find the mode mno-hit of the set of detection
scores st generated at time steps t when no keyword is present. The detection threshold is then set
as th = 2 (mno-hit + mhit). If St ≤ th, keyword is spotted else non-keyword.
Given the utterances are converted to sequences of representations, we use DTW distance as de-
tection score. We find the average sahvitg of the set of detection scores st generated at time steps
t when a keyword is present. We also find the average sanvog-hit of the set of detection scores
st generated at time steps t when no keyword is present. The detection threshold is then set as
th = 2 (Sn-hit + sahvitg). If st ≤ th, keyword is spotted else non-keyword.
D Mean Reciprocal Rank
Mean Reciprocal Rank (MRR) is a metric used to evaluate search performances. Given a query, a
search over the database outputs a ranking on the documents in a descending order of some similarity
metric calculated with the query. Let ranki be the rank of the first relevant document for the ith
query sampled from a set of N queries. The MRR score is given as,
1N 1
MRR =τr X —
N	ranki
(22)
15