Under review as a conference paper at ICLR 2022
Enhance the Dynamic Regret via Optimism
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the enhancement method for dynamic regret in online con-
vex optimization. Existing works have shown that adaptive learning for dynamic
environment (Ader) enjoys an O (∖(1 + PT) T) dynamic regret upper bound,
where T is the number of rounds and PT is the path length of the reference strategy
sequence. The basic idea of Ader is to maintain a group of experts, where each
expert obtains the best dynamic regret of a specific path length by running Mirror
Descent (MD) with specific parameter, and then tracks the best expert by Nor-
malized Exponentiated Subgradient (NES). However, Ader is not environmental
adaptive. By introducing the estimated linear loss function 套，the dynamic regret
for Optimistic Mirror Descent (OMD) is tighter than MD if the environment is not
completely adversarial and 坪 is well-estimated. Based on the fact that optimism
can enhance dynamic regret, we develop an algorithm to replace MD and NES in
Ader with OMD and Optimistic Normalized Exponentiated Subgradient (ONES)
respectively, and utilize the adaptive trick to achieve O (∖(1 + PT) MT) dynamic
regret upper bound, where MT 6 O (T) is a measure of estimation accuracy. In
particular, if b ∈ ∂φt, where φt represents the estimated convex loss function
and ∂φt is Lipschitz continuous, then the dynamic regret upper bound of OMD
has a subgradient variation type. Based on this fact, we develop a variant algo-
rithm whose upper bound has a subgradient variation type. All our algorithms are
environmental adaptive.
1	Introduction
The Online Convex Optimization (OCO), which was introduced by Zinkevich (2003), plays a vital
role in online learning as its interesting theory and wide application (Shalev-Shwartz, 2012). The
OCO problem can be viewed as repeated games between the learner and the adversary: At round t,
the learner chooses a map Xt from a hypothesis class C for prediction, and the adversary feeds back
a convex loss function φt, then the learner suffers an instantaneous loss φt (Xt). In general, φt (Xt)
is bounded to exclude the case where the loss can be arbitrarily large.
The appropriate performance metric, namely regret, as described below, comes from game theory
since the framework of OCO is game-theoretic and adversarial in nature (Zinkevich, 2003; Hazan,
2019).
T	T
regret 俎：=φt (xt) —	φt (号)，	(1)
(Z1 ,Z2, ∙∙∙ ,zτ)	t=1	t=1
where Zt ∈ C represents the reference strategy in round t, and 俎 is the algorithm that generates Xt.
Particularly, if Zt ≡ z, we have the following static regret,
T	T
regret 俎=∕φt (Xt )—2 Ψt (Z).
(，,，,,•,,，)	t=1	t=1
Correspondingly, we call Eq. (1) dynamic regret. The static regret used in most literature is usu-
ally written as SuPN∈c regret(…Z 俎.There are plenty of works devoted to designing online
algorithms to minimize static regret (Cesa-Bianchi & Lugosi, 2006; Shalev-Shwartz, 2012; Hazan,
2019; Orabona, 2019). Recently, designing online algorithms to minimize dynamic regret has at-
tracted much attention (Hall & Willett, 2013; Jadbabaie et al., 2015; Mokhtari et al., 2016; Zhang
et al., 2018; Zhao et al., 2020; Campolongo & Orabona, 2021; Kalhan et al., 2021).
1
Under review as a conference paper at ICLR 2022
An online algorithm is environmental adaptive if it maintains the regret upper bound when the envi-
ronment is adversarial, and tightens the upper bound as much as possible when the environment is
not completely adversarial. Optimistic algorithm provides a way to achieve environmental adaptive.
The word “optimistic” refers to the idea that if the learner can predict the impending loss when the
environment is not completely adversarial, then the regret upper bound may be tightened. How to
predict the impending loss is not the focus of the optimistic algorithm. The Optimistic Mirror De-
scent (OMD) was proposed by Chiang et al. (2012) and extended by Rakhlin & Sridharan (2013).
The regret upper bound usually contains some characteristic terms. The following are three well-
known characteristic terms.
•	The path length term (Zinkevich, 2003), Pτ = W=2 ∣∣Zt - Zt-ι ∣∣.
•	The gradient variation term (Chiang et al., 2012), VT = 23 SupC 加% - R4t-11∣2.
•	The function variation term (Besbes et al., 2015), FT = W=2 SupC ∣∣φt 一 ψt-ι ∣∣.
Usually the dynamic regret upper bound contains a path length term. Zinkevich (2003) shows that
mirror descent achieves an O ((1 + PT)巾)dynamic regret upper bound, where T is the number of
games. Zhang et al. (2018) propose a method, namely adaptive learning for dynamic environment
(Ader), achieves an O (∖(1 + PT) T) dynamic regret upper bound, which is optimal in completely
adversarial environment. The main idea of Ader is to run multiple Mirror Descent (MD) in parallel,
each with a different step size that is optimal for a specific path length, and track the best one with
Normalized Exponentiated Subgradient (NES). Zhao et al. (2020) follow the idea of Ader, and try
to utilize smoothness to enhance the dynamic regret.
In this paper, We follow the idea of Ader and develop an algorithm, namely ONES-OMD with
adaptive trick, which achieves an O (∖(1 + PT) MT) dynamic regret upper bound, where MT is a
measure of estimation accuracy. The main idea is to replace MD and NES in Ader with OMD and
Optimistic Normalized Exponentiated Subgradient (ONES) respectively, and utilizes the adaptive
trick. In particular, if the estimated linear loss 坪 in OMD is the subgradient of an estimated convex
loss bt and Abt is Lipschitz continuous, then its dynamic regret upper bound has a subgradient
variation type. For this situation, we develop a variant of ONES-OMD with adaptive trick, the upper
bound of which has a subgradient variation type. All our algorithms are environmental adaptive.
The contributions of this article are summarized as follows.
•	We develop the ONES-OMD with adaptive trick, which achieves an O («(1 + PT) MT)
dynamic regret upper bound.
•	We develop a variant of ONES-OMD with adaptive trick, whose dynamic regret upper
bound has a subgradient variation type.
•	We propose the adaptive trick, which is an extension of the doubling trick. The adaptive
trick gets rid of the explicit dependence of the dynamic regret upper bound on the number
of rounds T.
•	ONES-OMD with adaptive trick and its variant version are all environmental adaptive.
2 Problem Formulation
We denote by〈•，•〉the bilinear map. Let H be a Hilbert space over R. C is a nonempty subset of
H. The bilinear map〈•，•〉defined on H represents its inner product. We formalize OCO problem as
follows. At round t,
the player chooses the strategy Xt ∈ C according to some algorithm,
where C is closed and convex, and P = supæ,y∈c IlX - y∣∣ < +∞, 0 ∈ C,
the adversary (environment) feeds back a convex loss function 的
with dom∂φt ⊃ C and ∣∣∂φt (C)∣ 6 ρ < +∞,
where A represents the subdifferential operator. We choose the dynamic regret as the performance
metric, and design adaptive algorithm to enhance its upper bound.
2
Under review as a conference paper at ICLR 2022
3 Optimistic Algorithm
3.1 Optimistic Mirror Descent
Optimistic Mirror Descent (OMD) in the form of projection is formalized as
京+1 = PC (焉-";),	X； ∈ ∂φt (ɪr), ei = ɪl ∈ c,
^t+ι = PC (et+1 — ηbt+ι),
(2)
where PC represents the projection onto the subset C, η > 0 is the step size, b； ∈ H is the estimated
linear loss function in round t. In Hilbert space, the projection of any point onto a closed convex
subset exists and is unique (See Lemma 1 in Appendix A.1), which leads to xt ,Xt ∈ C, ∀t ∈ N.
Remark 1 If ψt is differentiable and bt+χ = Nφt (xt+ι) ,then OMD (Eq. (2)) becomes
et+1 = PC (et — ηV% (Xt)),	ɪi = Xi ∈ C,
Xt+1 = PC (et+i — ηVφt (et+i)),
Chiang et al. (2012) studied the static regret of Eq. (3).
The following theorem states that OMD has dynamic regret upper bound.
Theorem 1 OMD enjoys the following dynamic regret upper bound,
regret OMD	6 1 + T	^	kZt-号-i k + ； ^	||X；-引|2 - W	∑	kXt - et k2,
⑵&,…，NT)	2η η	t =2	t=1	2η	t=1
where Zt ∈ C represents the reference strategy in round t .
Set bXt； to be null, then OMD degenerates into Mirror Descent (MD), i.e.,
Xt+ι =	PC	(Xt	—	ηX;),	x；	∈ 即t	(Xt),	Xi ∈ C,
and the corresponding dynamic regret upper bound degenerates into the following form,
regret MD 6	2-	+ T	^	kZt- Qi	k + 2 ^	||琢『.
(Z1,Z2，…，nt)	2η η	t=2	2 t=1
(3)
(4)
(5)
Remark 2 Eq. (5) is a slight improvement of the following well-known upper bound (Zinkevich,
2003; Zhang et al., 2018).
regret MD 6 7t2 +
(N1,N2，…，NT)	4η
T	T
T ∑k Zt-zt-1 k+2 2∣kf.
η t=2	2 t=1
Comparing Eq. (4) and Eq. (5), we realize that by introducing the estimated linear loss function
bXt； , the dynamic regret upper bound can be tighter in the case the environment is not completely
adversarial and bXt； is well-estimated, and meanwhile guarantees the same upper bound in the worst
case.
3.2 Optimistic Normalized Exponentiated Subgradient
Optimistic Normalized Exponentiated Subgradient (ONES) is formalized as
et+1 = ʃ(et ◦ e-θlt),	e1 =31 ∈ ri 4,
(6)
wt+1 = ʃ (et+1 ◦ e-θlt+1),
where ʃ is the normalization operator, ◦ is the Hadamard product symbol, θ > 0 is the step size,
It is the loss vector, b is the corresponding estimated vector, ri is the relative interior operator,
and 4 B {w ∣ W ∈ R%+1, ∣∣a∣∣1 = 1} is the probability simplex. The normalization operator ʃ
guarantees that Wt, wt ∈ ri 4n.
3
Under review as a conference paper at ICLR 2022
ONES (Eq. (6)) is equivalent to the following iteration,
et+ι = e - θit,	e+1 = "ee'+l, eι =盯 ∈ Rn+1,
".
%+1 = et+1 - θlt+ι, n)t+ι = W e ,
or the following compact version,
Wt+1 = W (g ◦ e-θ ς% li-θlt+1), W1 ∈ ri 4 .
(7)
The following theorem states that ONES has static regret upper bound.
Theorem 2 ONES enjoys the following static regret upper bound,
regret ONES 6 1 Y W ⑴ ln -w-(^)- + θ Y Illt - Itf - ɪ Y k Wt- wt k2,	(8)
(w,w,∙∙∙ ,w)	θ Zrl	WI ⑺	2 t=ι 11	ll∞	2θ G
where W ∈ 4n represents the reference strategy.
Remark 3 Theorem 2 is a refined version of Theorem 19 of Syrgkanis et al. (2015). The Kullback-
Leibler divergence term allows the regret upper bound to be controlled by the initial value W1 of
ONES.
1~1 . ^√Γ . 1	11 . 1	T 1 - 1~1 1	.	TL T	1 ∙	1 1 ■	. ∙ . 1 1-1 1	f	/TL T 1 - 1~1 ∖ ♦
Set lt to be null, then ONES degenerates into Normalized Exponentiated Subgradient (NES), i.e.,
Wt+1
W(Wt ◦ e-θlt),	Wi ∈ ri 4n^1,
and the corresponding static regret upper bound degenerates into the following form,
regret NES 6 1 Y W (Z)ln -w-(z) + θ Y Mk∞.
(w,w,…，w)	θ ZTJ	w1 (Z)	2 G
(9)
Remark 4 If Wi =系 1n+1 in Eq. (9), where 1n+1 is the all-ones vector in Rn+1, then
regret NES 6
(w,w,…，w)
ln (n + 1)	θ
---------+ H
T
Y kltk2∞ ,
t=1
θ
2
which is a well-known upper bound (Shalev-Shwartz, 2012).
C	.「/C、	1”小、	1.	.. . . . .	. .	. .1.	I	,	..
Comparing Eq. (8) and Eq. (9), we realize that by introducing the estimated linear loss vector lt, the
static regret upper bound can be tighter in the case the environment is not completely adversarial
and lt is well-estimated, and meanwhile guarantees the same upper bound in the worst case.
A typical application scenario of ONES is to combine expert advices. Suppose a group of experts
{ei}∙∈1 provide suggestions to a player, where / is an index set. At round t, the expert ei provides a
suggestion strategy Xt (Z) ∈ C, the player combines experts, suggestions with weight Wt to generate
the final strategy xt =〈Wt, Xt〉，where Xt = {xt (Z)}∙∈ι and Wt is generated by ONES. Then
T	T	T
S ^t (Xt)	—	ψt	(〈W,Xti)	6	{∂φt (Xt),〈Wt	—	W,Xt〉〉= 2( h∂φt	(Xt), Xt〉，Wt	— W).
t=1	t=1	t=1
Choose lt ∈ h∂φt (Xt), Xt〉as the surrogate linear loss, We have
T	T
X φt (Xt) - φt (〈w,Xti) 6 X〈lt, Wt- w〉= regret ONES.
t=1	t=1	(w,w,…，w)
4 Enhancement Method for Dynamic Regret
In this section, we follow the idea of Ader (Zhang et al., 2018) and attempt to enhance the dynamic
regret by replacing MD and NES in Ader with OMD and ONES respectively.
4
Under review as a conference paper at ICLR 2022
We modify the dynamic regret upper bound for OMD (Eq. (4)) by dropping the negative term,
regret OMD 6 匕 +
(N1,3, ∙∙∙ ,zτ )	2
M ∑ k号-Jk + 2 2卜；-琴『6 2r	+ ^-pτ +	2O2St,	(IO)
η t=2	2 t=1	2η η	2
where
τ	τ-1
Pτ = 2 k Zt- Zt-1 k, St = 4 + O-2 ^ 卜；-bf, and 困 ∣ 6 O.
t=2	t=1
After going for the game for T rounds, the value of ST is fixed, however, the path length PT remains
unknown. This implies that the optimal parameter η = ^P (P + 2Pτ) /St/O cannot be determined.
A feasible method is to maintain a group of experts {ei}∙∈E (E is unknown temporarily), where the
expert er∙ operates OMD with a certain parameter ηi, and then composite the experts, suggestions by
weight Wt to obtain the final strategy, i.e., Jtt = {wt,Xt〉，where Xt = {%t (力)卜∈e and Xt (Z) represent
the suggestion of the expert ei. This ingenious way to solve parameter difficulties comes from Zhang
et al. (2018). Note that the dynamic regret can be decomposed as
τ	τ	τ
2% (Jt) - φt(Zt) = 2% (hwt,χt〉)- φt (<1 j,χt>) +2 5t (Xt(J))- 3t (zt)
t=1	t=1	t=1
τ	τ
6 E(it,wt -1 j〉+2Ψt (Xt(J)) - 3t (zt),
t=1	t=1
(11)
where 1 j is the one-hot vector corresponding to the expert e/, and I ∈ {∂φt (Xt), Xt〉，We use ONES
to generate wt . Since the ONES guarantees
τ
regret ONES 6 - 2 W (Z) ln + ^2 Wt- Itll
(S,…M	8 ZiJ	w1 (Z)	2 自 U ∣∣∞
by dropping the negative term in Eq. (8), we have
2 &wt -1 力 6 8 21J(Z)In W^+2 2 lit -引∞ 6 Tn WI(J)+8p2γLT,	(i2)
t=1	i	1	t=1
where
Lt = 4 + p-2O-2 2∣lt- bt∣∣2, and ∣b∣∣ 6 po.
t=1	∞
We rearrange Eq. (10) and Eq. (12) as follows,
V / o	( 、w P(P + 2pτ) , ηj o ɑ - P(P + 2pτ) , ηj o m
/ J φt (Xt(j)) - φt (Zt) 6 —K-+ -^-ST(j) 6 —K--+ -^-MT,
自	2η j	2	2η j	2
V ip ] ∖r-lnw1(J) Jp2o2 ʃ 「-lnw1(J) JP2O2 m	门N
∕j Vt, Wt - 1J)6  8------+ -2~~LT 6-8--+ -2~~MT	(13)
t=1
where
τ-1
Mt = max Lt, max ST (j) ,	ST (J) = 4 + O-2 ?卜；(J)- b (j)∣∣2,	(14)
IJJ	t=1
x； (j) ∈ ∂φt (Xt (j)), Xt (j) is the suggestion strategy of e/,坪(j) is the corresponding estimated
linear loss function for e/ with ∣∣b; (J)∣∣ 6 ©. We call MT the measure of estimation accuracy. When
the environment is not completely adversarial and all b； (J) and It predict accurately, then MT grows
slowly. On the contrary, when the environment is completely adversarial, the prediction will fail and
MT grows linearly.
Now we need to allocate the group of experts. Let MT be fixed. According to 0 6 PT 6 (T - 1) P,
the optimal parameter
η = SP (P2+ 2PT) ∈	p__ h1, √2T - 1 i.
O	O2MT	o√Mt I	」
5
Under review as a conference paper at ICLR 2022
Note that
∃ j ∈ {θ, 1,…，［log2 √T - 1 kθ c E, such that 力 ∈ √^ ∣2，, 2尸1),
We assign the expert group as {ei卜∈e, where the expert ei operates OMD with ηi = ^√= T. Since
ηj 6 η < 2ηj, then for expert e/, the following bound holds,
regret e j 6 P (P: "，) + "Mτ < P(P + ”1) + 理 Mτ = 3 OpP (P + 2Pτ) MT, (15)
/2,/T)	2η	2	η	2	2
which implies that the expert e j reaches an almost optimal upper bound.
Substitute Eq. (13) and Eq. (15) into Eq. (11) yields
X 的(用) -% (Zt) <	1"； "，+ -P2O- mt + 2 opP (P + 2pτ) MT.
t=1
To determine this upper bound, it suffices to choose some appropriate w1 and θ. Let w1 (力) =
β (i + 2)-。，where a > ζ-1 (2), β-1 = Ei∈e (i + 2)-a. ζ-1 (2) ≈ 1.728647238998183 is the root of
equation ζ (α) = 2 on R+, ζ represents the Riemann ζ function, i.e.,
∞1
ζ 3) = ∑ 港,
n=1
a > 0.
Note that β > 1 and η7∙ 6 η, we have
-ln W1 (j) < a ln (j + 2)	and j 6 log2
Thus,
τ
X Ψt (Xt)
a
-Ψt(Zt) < θ ln I2 + log2
, we have
t=1
Let θ 8√t
+ P2 MT + 2 QpP (P + 2pT) MT.	(16)
T
X φt (ɪt)- φt (Zt) < O (pΓ+PTy^T).
t=1
We call the above algorithm ONES-OMD. Comparing with O (γz(1 + PT) T), the upper bound of
Ader proposed by Zhang et al. (2018), and noting that MT 6 O (T), the dynamic regret upper bound
of ONES-OMD is tighter in the case the environment is not completely adversarial and 套，b are
well predicted, and meanwhile guarantees the same rate in the worst case. The estimator 聋 and It
play the pivot role in enhancing the dynamic regret.
Note that the above analysis is based on the premise that MT is fixed, we can utilize the following
adaptive trick to unfreeze MT, like utilizing the doubling trick to unfreeze T to anytime.
Theorem 3 (Adaptive Trick) The adaptive trick
calls ONES-OMD with θ ∞ 2~m and ηi = — 2l~m, for i = 0,1,…，凡
Q
under the constraints that MT ∈ ∣4m, 4m+1) and T ∈ 2 ∣4n, 4n+1) + 1,
where m indicates the stage index of the game. The above execution process achieves an
O ( 1 + PT) MT dynamic regret upper bound.
Remark 5 The idea of adaptive trick is to divide the range of MT into stages of exponentially
increasing size and runs ONES-OMD on each stage. This inspiration comes from the doubling
trick, which divides T into stages of doubling size and runs some appropriate algorithm on each
stage. Shifting from monitoring T to monitoring MT is a crucial step in achieving environmental
adaptation.
To be understood easy, we illustrate the specific execution process for ONES-OMD with adaptive
trick in Algorithm 1.
6
Under review as a conference paper at ICLR 2022
Algorithm 1 ONES-OMD with adaptive trick
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
m W----1, n W--1
for round t = 1,2,… do
n J [log2 √2t- 1 ,m J [log4 Mt ∖, where Mt is calculated according to Eq.(14)
if m changed or n changed then
Construct a set of experts {ei}着 and invoke Algorithm 3 with ηi =*2l~m for ei
Call Algorithm 2 with parameter n and θ X 2-m
end if
Receive the estimated loss vector b from an arbitrary estimating process and send it to Al-
gorithm 2, receive a group of estimated linear losses {坪(0),焉(1), ∙∙∙,坪(n)} from an
arbitrary estimating process and send them to each expert
Get expert advice strategies Xt = {xt (0) ,ɪt (1), ∙∙∙ ,ɪt (n)}, call Algorithm 2 to get the
weight Wt
Output strategy xt = hw, Xti, and then observe loss function 中t
Send It ∈ {∂φt (Xt), Xt i to Algorithm 2, send ∂φt to each expert
end for
Algorithm 2 Subprogram: ONES with parameter n and θ
ɪʌ ♦	n	1 "ri^ r∙	t 1	∙ . t λ
Require: lτ and lτ+1 from Algorithm 1
1: Output W1 (Z) = β (i + 2)-0, i = 0,1, ∙∙∙ ,n at the first call, and each subsequent call follows the
ONES (Eq. (6))
Algorithm 3 Subprogram: OMD with parameter η
Require: ∂φ丁 and 琴十]from Algorithm 1
1: Output x1 ∈ C at the first call, and each subsequent call follows the ONES (Eq. (2))
5 Enhancement Method for Dynamic Regret in S ub gradient
Variation Type
In this section, we follow the idea of Section 4 to study the enhancement method for dynamic regret
in subgradient variation type.
The following corollary states that, under the assumptions that 套 is the subgradient of an estimated
convex loss φbt , and ∂φbt is Lipschitz continuous, the dynamic regret upper bound of OMD has
subgradient variation type.
Corollary 1 If 坪 ∈ ∂φt (eet) and ∂φt is Lipschitz continuous, i.e.,
∃L > 0, such that	∣∣∂φt (x) - ∂φt (^)k 6 L ∣∣x 一 y∣∣,	∀x,y ∈ C,
where φbt represents the estimated convex loss function, then
regret OMD 6 0 (° + *,) + η Y [SUP Ilx如-X+ 1 _LL2 ∣Xt-五 k2∖,
⑵E∙∙e	2η	占 Q∈d∣	11 η √2^	)
where xψt ∈ ∂φt (x), xSt ∈ ∂φt (x), and 1	_±_ is the zero-one indicatorfunction w.r.t. 1	_±_ = 1
/ √2L	/ √2L
iff η > 1 .
JJ /	√2L
Remark 6 Corollary 1 is inspired by Zhao et al. (2020). However, we have not restricted ∂φt to be
Lipschitz continuous.
Similar to Section 4, we also maintain a group of experts, and each expert operates OMD with a
specific parameter. Denote by Xt the vector of expert advice strategies and eXt the vector of all exts.
7
Under review as a conference paper at ICLR 2022
Let
T -1	2	/	T -1	\
Vr = 4 + Q~- X SuP 卜t -X⅛bt∣l , DT = L2ρ~2 P2 + X max kXt-etk2 .	(17)
t=1 X ∈c	∖ t=1	/
The expert Who operates OMD with the parameter η yields the global dynamic regret upper bound
as follows,
P^T) +ηo2(Vτ + D T),
2η
and correspondingly, yields the local dynamic regret upper bound as follows,
P (P + 2Pt )	2	1
---- ------+ ηρ Vt, η 6 ~^.
2η	√2L
In order to be compatible to Vt, we choose lt ∈ ^∂tpt 卜t), Xt) in ONES, where et = het, χt〉. Note
that It ∈ h∂φt (Xt), Xt〉，We have
—
eb, χt E∣∣∞	6 p2 ∣Xtpt	-	ebt∣∣2 6	p2	(附-Xbbt ∣	+ Wft-	e)|『
where
「苏:St
琢-Xt
6 L IlXt - Xtll = L Ilhg- e,Xtik 6 PL kWt - Wtkι .
According to Theorem 2, the static regret upper bound for ONES is
I ∑W ⑴ ln
i
61 ∑ W ⑺ ln
i
W (Z)
Wi (Z)
W (Z)
Wi (Z)
T	2	T
+ iP2 X sup l∣xψt - X φt∣l + pp4 L2 - 2∣l X k Wt - etk2
t=1 X ∈c	t=1
+ 暗O2Vt,	if θ 6 —1—.
√2p2 L
If we choose VT + DT as the measure of estimation accuracy, then the global dynamic regret upper
bound is O (γz(1 + PT) (VT + DT)), and the group of experts is {e几}几∈e, where
E = {θ, 1,... , jlog2 √T -1 kθ ,
the expert e冗 operates OMD with ηβλ = ©7二口 2λ. If we choose VT as the measure of estimation
accuracy, then the local dynamic regret upper bound is O (γz(1 + PT) VT), and the group of experts
is {eμ}μ寻,where
遂=∖∈ ∈ {θ, 1,... , [log? √2T -1
the expert EM operates OMD with η J = ~^√^ 2μ.
备T 2μ 6 √2l1,
We merge two expert groups and utilize ONES to track the best expert. In this case, the initial value
of ONES is
W1 (e2) = β (2 + 2)-α,	2 ∈E,
W1 (Eμ) = β (μ + 2)F, μ ∈ 逐,
where a > ζ-1 (1.5), β-1 = 2λ.∈e (2 + 2)-。+ 2μ∈青(μ + 2)-。. ζ-1 (1.5) ≈ 2.185285451787483
is the root of equation ζ (α) = 1.5 on R+, ζ represents the Riemann ζ function. Let θ X √=,
θ 6 √⅛ ,wehave
T
X φt (Xt) - φt (zt) 6，
t=1
where η = NP (P + 2Pt) /(2VT)/©. We recombine this dynamic regret upper bound as
o H(I + PT) (VT + 1L2Q8+2PT)6e2vrDT
O (p(1 + PT) (VT + DT)),
O (√(1 + PT) VT),	η 6 圭,
To make it easier to follow, we depict the above specific execution process in Algorithm 4.
-~~■
8
Under review as a conference paper at ICLR 2022
Algorithm 4 Subgradient variation version of ONES-OMD with adaptive trick
1:	m <--1, m 0 <---1, n <---1,孔0 <----1
2:	for round t = 1,2,…do
3：屋一|E| - 1,"一曜I - 1, m - [log4 (匕 + Dt)_|, m J [log4 Vt _|, where Vt and Dt are
calculated by Eq. (17)
4:	if (m or m0 or n or n,) changed then
5:	Construct a set of experts {e几}冗∈e ∪ {e必}必∈青 and invoke Algorithm 6 with ηβλ =看2z-m
for e冗，invoke Algorithm 6 with η WN =亳2μ-m° for EMif 青 ≠ 0
6:	Call Algorithm 5 with parameter n, n0 and θ X 2-m°, where θ 6 B∖
2 2p L
7:	end if
8:	Receive the estimated convex loss St from an arbitrary estimating process with Abt to be
Lipschitz continuous, send ∂b to Algorithm 5 and each expert
9:	Call Algorithm 5 to get expert advice strategies xt, et, and the weight Wt
10:	Output strategy xt = hwt, Xti, and then observe loss function 中t
11:	Send It ∈ {∂φt (Xt), Xt i to Algorithm 5, send ∂φt to each expert
12:	end for
Algorithm 5 Subprogram: ONES with parameter n, n0 and θ
Require: lτ and ∂φτ+1 from Algorithm 4
1:	Get expert advice strategies X T and eT, send them to Algorithm 4
2:	Output W1 (e^) = β (λ + 2)-°, λ = 0,1,…，n, w1 (EM) = β (林 + 2)-°,林=0,1,…，n0 at the
first call, and each subsequent call follows the following rule
Wτ+-1 = W(wτ ◦ e-θlτ),	W1 = W1,
lτ+1 ∈ hdφτ+1 (<wτ+1, Xτi) , Xτi ,
Wτ+1 = N (wτ+1 ◦ e-θ "1)
Algorithm 6 Subprogram: OMD with parameter η
Require: ∂φτ and ∂φbτ+1 from Algorithm 4
1: Output X1 = x1 ∈ C at the first call, and each subsequent call follows the ONES (Eq. (2))
6 Conclusions and Future Work
In this paper, we study the enhancement method for dynamic regret in a non-adversarial environment
under the premise of guaranteeing the worst-case dynamic regret in OCO problem. We develop an
algorithm, named as ONES-OMD with adaptive trick. Theoretical analysis shows that our algorithm
achieves an O (∖(1 + PT) MT) dynamic regret upper bound. We also develop a variant of ONES-
OMD with adaptive trick that makes the dynamic regret upper bound have a subgradient variation
type.
Tracking the best expert may be the general approach for online learning with dynamic regret. Op-
timism combined with adaptive trick provides a way to achieve environmental adaptation. We hope
this work encourages further research on smoothed online learning and online learning with delayed
feedback.
Acknowledgments
Paper under double-blind review.
9
Under review as a conference paper at ICLR 2022
References
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations
Research,63(5):1227-1244,2015. doi:10.1287/opre.2015.1408.
HalmBrezis. Functional Analysis, Sobolev Spaces and Partial Differential Equations. Universitext.
Springer-Verlag New York, 2011. ISBN 9780387709130. doi: 10.1007/978-0-387-70914-7.
Nicolo Campolongo and Francesco Orabona. A closer look at temporal variability in dynamic online
learning. arXiv e-prints, art. arXiv:2102.07666, February 2021.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University
Press, 2006. doi: 10.1017/CBO9780511546921.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. In Shie Mannor, Nathan Srebro, and
Robert C. Williamson (eds.), Proceedings of the 25th Annual Conference on Learning Theory,
volume 23 of Proceedings of Machine Learning Research, pp. 6.1-6.20, Edinburgh, Scotland, 25-
27 Jun 2012. JMLR Workshop and Conference Proceedings. URL https://proceedings.
mlr.press/v23/chiang12.html.
Eric Hall and Rebecca Willett. Dynamical models and tracking regret in online convex pro-
gramming. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th In-
ternational Conference on Machine Learning, volume 28 of Proceedings of Machine Learn-
ing Research, pp. 579-587, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https:
//proceedings.mlr.press/v28/hall13.html.
Elad Hazan. Introduction to Online Convex Optimization. arXiv e-prints, art. arXiv:1909.05207,
September 2019.
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online Optimiza-
tion : Competing with Dynamic Comparators. In Guy Lebanon and S. V. N. Vishwanathan (eds.),
Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,
volume 38 of Proceedings of Machine Learning Research, pp. 398-406, San Diego, Califor-
nia, USA, 09-12 May 2015. PMLR. URL https://proceedings.mlr.press/v38/
jadbabaie15.html.
Deepak S. Kalhan, Amrit Singh Bedi, Alec Koppel, Ketan Rajawat, Hamed Hassani, Abhishek K.
Gupta, and Adrish Banerjee. Dynamic online learning via frank-wolfe algorithm. IEEE Transac-
tions on Signal Processing, 69:932-947, 2021. doi: 10.1109/TSP.2021.3051871.
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization
in dynamic environments: Improved regret rates for strongly convex problems. In 2016 IEEE 55th
Conference on Decision and Control (CDC), pp. 7195-7201, 2016. ISBN 978-1-5090-1838-3.
doi: 10.1109/CDC.2016.7799379.
Francesco Orabona. A modern introduction to online learning. arXiv e-prints, art.
arXiv:1912.13213, December 2019.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Shai
Shalev-Shwartz and Ingo Steinwart (eds.), Proceedings of the 26th Annual Conference on Learn-
ing Theory, volume 30 of Proceedings of Machine Learning Research, pp. 993-1019, Prince-
ton, NJ, USA, 12-14 Jun 2013. PMLR. URL https://proceedings.mlr.press/v30/
Rakhlin13.html.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends®
in Machine Learning, 4(2):107-194, 2012. ISSN 1935-8237. doi: 10.1561/2200000018.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of
regularized learning in games. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
7fea637fd6d02b8f0adf6f7dc36aed93- Paper.pdf.
10
Under review as a conference paper at ICLR 2022
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou. Adaptive online learning in dynamic environments.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems, volume 31, pp. 1323-1333. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
10a5ab2db37feedfdeaab192ead4ac0e- Paper.pdf.
Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou. Dynamic regret of convex and smooth
functions. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 12510-12520. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
939314105ce8701e67489642ef4d49e8-Paper.pdf.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on Machine Learning, ICML’03, pp.
928-935. AAAI Press, 2003. ISBN 1577351894.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Proof of Theorem 1
The proof of Theorem 1 relies on the following lemma. Part of the proof is inspired by Zhao et al.
(2020).
Lemma 1 (Theorem 5.2 of Brezis (2011)) Let H be a Hilbert space, and let C ⊂ H be a nonempty
closed convex set. Then ∀x ∈ H, ∃ !ɪo = PC (ɪ), such that〈C - xo,x - ɪoi 6 0.
We rearrange OMD as follows,
yt+1 =	e -办;，	京+1	=	PC	(yt+ι),
4+1 =	Xt+1 — 痴t+i,	%t+ι	=	PC	(yt+ι).
Note that
Ψt (ɪt) - Ψt (Zt) 6 -⑺；,Xt - Zt), Xt ∈ ∂φt (ɪt),
and
-Xtt, Xt - Zt	= -	Xtt	- bXtt , Xt	- eXt+1	+	-Xtt , eXt+1 - Zt	+	-bXtt, Xt	- eXt+1
tt
=-(Xt - Xt ,χt - Xt+ι/ - heet - yt+ι, Zt - Xt+ii - hχt - yt,Xt+ι - Xti,
where
-2	1
-M-bt,Xt -Xt+1〉6 - IIXt-HllkXt-Xt+ik 6 2 IIXt-bt∣∣2 + 2 k%-Xt+ik2,
and
2 kXt - Xe+1 k2 6 2 kXt+1 k2 - 2 kXt k2 + hyt,Xt - Xt+1〉，
since 〈XXt+1 - Xt, yt - Xti 6 0 according to Lemma 1. Thus
〈-x；,Xt - Zt〉6 g 11Xt- bt∣F + 2 IlXt+1 k2 - 2 kXtk2 -〈Xt - yt+1, Zt - Xt+1i -〈Xt,Xt+1 - Xt〉
6 ɪ ∣iXt - bt i∣2+2 k Zt - Xtk2 - 2 k Zt - Xt+1 k2 - 2 k Xt - Xtk2,
since 〈Zt - XXt+1, Xyt+1 - XXt+1〉 6 0 according to Lemma 1. So we have
T	1 T	T	1 T
2	φt(Xt)-φt (Zt) 6 2-	2	(k Zt - Xtk2 - k Zt	- Xt+1 k2) + 2 2	||x；-引 ∣2 - 2- 2	kXt	- Xtk2
t=1	-	t=1	t=1	- t=1
T	T	T
6	2-	kZ1 -ɪ1 k+ - 2 ii-2	w kZt - ZtTk+2 2 iiXt-btι∣-	2-	2	kxt	-Xtk
2	T	T	T
6 2h+£	2	k Zt- Zt-1 k+2	2	∣lXt	-尊『-石 2	kχt- Xtk2.
2-	-	t=2	2	t=1	2- t=1
A.2 Proof of Theorem 2
The proof of Theorem 2 relies on the following lemma.
Lemma 2 (Example 2.5 of Shalev-Shwartz (2012)) 2工- W (ζ) ln W (Z) is 1 -Strongly-Convex w.r.t k∙k1
over the probability simplex.
We rearrange ONES (Eq. (7)) as follows,
Xt+1 = Xt- %,	Xt+1 = M+d'+1,	X1 = D1 ∈ Rn+1,
-~~	ζ->5Γ	n τ υ*, ɪ
Ut+1 = X+1 - °lt+1, wt+1 = Nt+1e	,
12
Under review as a conference paper at ICLR 2022
1	τt^τ	1 HT	. . 1	1 ∙	i'i' ♦ . TL T	.1 .
where Nt+ι and Nt+ι represents the normalization coefficients. Note that
hθit , wt - wi = θ Dit - ibt , wt
= θ Dit - ibt , wt
〜
- wt+1
+ hθit , wet+1 - wi + θibt , wt - wet+1
—	∖ . f—	—	—
- wt+1	+ hUt - Ut+1 , wt+1
-w) + het - Ut,wt - e+ιi,
where
0
θ Dit - ibt , wt -
et+ι) 6 θ∣∣it-it∣∣ kwt- wt+ιkι 6 ~
∙^
Iit- bIL+2 kwt- wt+ι k2,
and
12
2 kwt - wt+1 kι 6 ( et+ι,ln
since hw,ln w〉is 1-strongly-convex w.r.t k∙kι over the probability simplex according to Lemma 2.
Note that
heUt - eUt+1 , wet+1 - wi + heUt - Ut , wt - wet+1 i
= heUt , wt - wi - heUt+1 , wet+1 - wi - hUt , wt - wet+1 i
wet	wet+1	wt
=(ln =, Wt - W	ln ——, Wt+ι 一 W	ln ——,Wt - wt+ι)
Net	Net+1	Nt
= hln wet , wt - wi - hln wet+1 , wet+1 - wi - hln wt , wt - wet+1 i
then
w, ln
wet+1,ln
wt
Wt, ln —
wt
θ2
hθlt, wt- wi 6 ~2 Ii
θ2
6工ii
according to Lemma 2, and thus,
T	1 Tl
hit, Wt- wi 6 θ	(w, ln
1 I IW
θ∖w, ln W
+ w, ln
it - ibt II2
it - ibt III2∞
wt
Wt, ln —
wt
+ w, ln
—
1
2
1 T
2θ ∑ k wt- etk2
t=1
w, ln w-
wT+1
k Wt- et k1
1 T
- 2θ Σ kWt - etk2
t=1
since
T
w, ln
t=1
weT+1
w, ln —∑z—
w1
w, ln w-
w1
w 1	2	w
6 ( w, ln - / - - kW - wT+1 kι 6 ( w, ln —
we1	2	we1
w
w, ln 一 ..
A.3 Proof of Theorem 3
Suppose the game has been played for T rounds, and is in stage m. MT ∈ [4m, 4m+1). Denote by
TS the total rounds number have been played in stage s. T = 2圈 TS. According to Eq. (16), the
dynamic regret upper bound of stage S is
O
a ln 2 + log2
2s + 2p2 β12s + 3 ρ
6 O(P(1 + PT)2s ),
and then
m
'O(P(1 + PT)2s) = O (P(1 + PT)2m) = O (P(1 + PT) MT).
S=1
13
Under review as a conference paper at ICLR 2022
A.4 Proof of Corollary 1
Let Xft = x↑, eft =聋.Note that
N-ebt∣2 6 收t -XftIl + M -eftD2 6 2Mt-Xff + 2L2 kxt -etk2,
where Xft ∈ ∂φt (Xf). According to Theorem 1, the dynamic regret upper bound for OMD is
W + " + η E 卜ft -χft『+ W- W) E kXt- et k2
6 P S +*') + η E (翟卜ft - xbt ∣2 + 1”圭 L2 kXt - et k2).
14