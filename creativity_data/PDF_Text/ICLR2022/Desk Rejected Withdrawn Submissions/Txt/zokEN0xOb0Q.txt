Under review as a conference paper at ICLR 2022
Differential Privacy over Affine Manifolds
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study dataset processing mechanisms generated by linear queries
over affine manifolds. Specifically, the input data are assumed to lie in an affine
manifold. This affine manifold adds to an inherent geometry in the domain of
secrets, which acts as a special constraint that may be known about the data by
adversaries. To take care of the presence of this affine manifold geometry, a new
neighborhood of adjacency between two databases is introduced where the dimen-
sion of the manifold has to be accounted for. We establish necessary and sufficient
conditions on the possibility of achieving differential privacy via structured noise
injection mechanisms where non i.i.d. Gaussian or Laplace noises are calibrated
into the dataset. Next, in light of these conditions, procedures are developed by
which a prescribed privacy budget can be tightly reached with a matching noise
level. Finally, we show that the framework has immediate applications in differ-
entially private cloud-based control, where the affine-manifold data dependency
arises naturally from the system dynamics, and the proposed theories and pro-
cedures become effective tools in evaluating privacy levels and in the design of
provably privacy-preserving algorithms.
1	Introduction
The rapid development in big data and machine learning has sparkled a revolution in various engi-
neering disciplines during the past decade, such as manufacturing, the Internet of Things (IoT), E-
commerce, healthcare, computer vision, etc. Advanced learning representations and efficient train-
ing on large datasets are uncovering the tremendous power hidden in data on a daily basis, enabled
by the drastic improvements in effective data collection, storage, and processing Qiu et al. (2016).
Information about individuals’ identities, preferences, and activities becomes inevitably embedded,
directly or indirectly, in a variety of datasets collected from various sensors and social media. The
underlying privacy risks for individual users in such data-driven applications are becoming increas-
ingly important Zhu & Blaschko (2020), especially so when the datasets involve sensitive private
data such as political associations, biometric fingerprints, and healthcare records Wu et al. (2020).
The fundamental challenge in managing privacy risks of data analysis has been the tradeoff be-
tween protecting datapoint information and maintaining analysis accuracy. Classical work Denning
& Denning (1979); Denning & Schlorer (1980) revealed potential privacy threats facing statistical
database, where an adversary may create sequential queries to infer confidential datapoint. It was
later proven that it is impossible not to reveal information about datapoint in queries unless ran-
dom noise has been injected into the database Dinur & Nissim (2003). The seminar work Dwork
et al. (2006a;b) brought up the idea of differential privacy in quantifying the amount of privacy
risk in a randomized mechanism, where a numerical privacy budget characterizes the probabilistic
similarities at the mechanism output between two datapoints of the mechanism input that are close
(adjancent) with each other. In particular, differential privacy concerns with the privacy risk embed-
ded in a mechanism, denoted by M : D → M, which is a randomized mapping from the input space
D to the output space M Dwork et al. (2006b).
Definition 1. (μ-Adjacency) For any two data X and x0 drawn from the set D ⊆ Rn, they are said
to be μ-adjacent with μ > 0, denoted by (x, x0) ∈ Adj(μ), if ∣∣x 一 x0∣∣0 = 1 and ∣∣x 一 x0∣∣ι ≤ μ.
Definition 2. (Differential Privacy) A randomized mechanism M : D → M is (, δ)-differentially
private under μ-adjacency for e ≥ 0, δ ∈ [0,1), if for all R ⊆ range (M), there holds
P(M(x) ∈ R) ≤ e'P(M(x0) ∈ R) + δ,	∀(x,x0) ∈ Adj(μ).
(1)
1
Under review as a conference paper at ICLR 2022
The ideas and algorithms for differential privacy have been successfully applied in large-scale real-
world dataset analysis, and to areas ranging from deep learning Shokri & Shmatikov (2015); Abadi
et al. (2016); Zhao et al. (2017); Chen et al. (2020) and computer vision Zhu et al. (2020) to control
and distributed computation Scaman et al. (2019); Huang et al. (2015).
1.1	Mechanisms over Affine Manifolds: A Motivating Example
We present an example to show that when the domain of secrets D is an affine manifold, the inherent
geometric constraint can not be ignored.
Example 1. Consider a randomized mechanism
M(x1,x2) := xx21 ++ γγ21	(2)
where γ1, γ2 denote the added random noises for protecting privacy of (x1, x2), which are i.i.d.
drawn from a LaPlace distribution with zero mean and variance q； . Let σγ = μ∕e. There are two
cases.
(i)	[Differential Privacy over Euclidean SPace] Let D = R2. The M is a standard LaPlace
mechanism and Preserves the (e, 0)-differential Privacy of (x1, x2) (Dwork et al. (2014)).
(ii)	[Differential Privacy over Affine Manifolds] Let
D := {(x1, x2)> ∈ R2 : x1 - kx2 = b}, , with k ∈ R>0 and b ∈ R.
Thus, xi (or χ2) is indeed released twice at the output of M, i.e., χι+γι and (xi -b)∕k+γ2
(or kx； + b + γι and x； + γ2). Then it can be seen that ((1 + k)e, 0)-differential privacy
of x1 and ((1 + k)e, 0)-differential privacy of x2 are achieved tightly, by the sequential
composability property of differential privacy (Dwork et al. (2014)). This implies that the
mechanism M is now (ge, 0)-differentially private with g := max{1 + k, 1 + k } over D.
For Case (ii), we may design probabilistically correlated zero-mean Laplacian noises as γ1 = kγ2
with γ2 having variance Qγ = μ∕e under which (e, 0)-differential privacy is also achieved. □
Example 1 is in line with the privacy frameworks of Pufferfish Kifer & Machanavajjhala (2014)
and Blowfish He et al. (2014), where the privacy of data with correlation/constraint is studied. In
particular, the affine manifold can be viewed as constraints about the data known by adversaries. In
Blowfish He et al. (2014), added protection against adversaries who know this constraint is shown
to be possible by specialized policies. It is of interest to understand how the geometry of the affine
manifold can be taken into consideration for possible added protections. Besides being an interest-
ing theoretical study, there are also practical motivations for exploring the affine-manifold geometry
in differential privacy mechanisms. Particularly, in dynamical systems the system state trajectories
always imply a manifold dependency from the system dynamics.
1.2	Problem Definition
We consider the following mechanism with linear queries
M(x) =Fx+γ	(3)
where x ∈ Rn is the input data, F is a matrix in Rm×n , and γ ∈ Rm is a randomly drawn noise.
Definition 3. Denote
Cd := {x ∈ Rn : Dx + b = 0}	(4)
as an affine manifold in Rn, where D ∈ Rq×n andb ∈ Rq. The mechanism (3) is a mechanism over
Cd if the input data x is always taken from Cd, and both D amd b are public.
Let V = {1, . . . , n}. Without loss of generality, we assume rank eD> = q + 1 for all i ∈ V.
This indeed indicates that D is full-row-rank, i.e., rank (D) = q and none of entries in data x is
identifiable from the manifold Cd. With rank (D) = q, there exists a finite number, saying l ∈ N+,
of sets dj := {dj,1, . . . , dj,q} ⊆ V, j ∈ I := {1, . . . , l} such that cardinality |dj | = q, and matrix
Ddj ∈ Rq×q is nonsingular. We denote the set -dj = V∕dj and can rewrite the manifold Cd as
Cjd := x∈Rn :xdj = -Dd-j1D-djx-dj -Dd-j1b}, ∀j ∈I.
2
Under review as a conference paper at ICLR 2022
Definition 4. (Manifold μ-Adjacency) We say X and x0 to be μ-adjacent over the manifold Cd,
denoted by (x, x0) ∈ Adj(μ, Cd), ifthere exist j ∈ I and i ∈ —dj such that
|xi — χi∣ ≤ μ
xk = x0k , ∀k ∈ —dj /{i}	(5)
Xdj — X0dj = —Dd-j1D-dj(X-dj — X0-dj) .
Remark 1. Note that, Definition 1 requires the databases X, X0 to be distinct at only one entry
(i.e., kX — X0k0 = 1) to capture individual’s contribution to the database. However, for any two
databases X, X0 ∈ Cd, it may be impossible for them to differ at only one entry (see Example 1). With
rank(D) = q, X, X0 ∈ Cd may even differ for q + 1 entries. This means individual’s contribution
can no longer be looked into separately, but in groups, forming the reasoning behind Definition 4.
Remark 2. There has been a line of work on differential privacy over metric spaces Holohan et al.
(2015); Alvim et al. (2018); Fernandes (2021). For example, Holohan et al. (2015) has considered
databases with entries over a metric space (U, ρ), where ρ is a metric Holohan et al. (2015). In
this way, an n-dimensional database takes values from Dn with D ∈ U Holohan et al. (2015). Note
that the affine manifold Cd ⊆ Rn in the current study can not be written in the form ofa Cartesian
product Dn. Therefore, the affine manifold is an extension, but not a special case of the differential
privacy frameworks over metric spaces. With Cd, now there is inherent geometry between the entries
in a database, while a database in Dn has homogeneous geometry on individual entries.
Remark 3. Note that the adjacency in Definition 4 is defined by modifying entries in the dataset.
It is also of interest to investigate the adjacency notion by adding or removing records. It can be
seen that the dimension of the resulting X0 by adding or removing records in X may be incompatible
with matrices D and F, violating the manifold constraint (4) and the mechanism (3), respectively. If
the manifold Cd can be separated into several independent sub-manifolds DiXi + bi = 0, the adja-
cency by adding or removing records Xi can be well-defined without manifold violation, though the
mechanism violation problem is still unsolved. Thus the adjacency by adding or removing records
is not applicable to our scenario of linear mechanism (3) with affine manifold (4).
Definition 5. The randomized mechanism M in (3) is (, δ)-differentially over Cd for ≥ 0 and
δ ∈ [0, 1), iffor all R ⊆ range(M),
P(M(x) ∈ R) ≤ eeP(M(X) ∈ R) + δ, ∀(x,x0) ∈ Adj(μ, Cd).	(6)
An implication of the differentially private mechanism M in (3), lies in the fact that entries in the
random noise γ may be probabilistically dependent. We introduce the following definition.
Definition 6. The noise γ is probabilistically structured if there exists Λ ∈ Rm×r with rank (Λ) =
r ≤ m such that γ = Λη, where the entries in η ∈ Rr are i.i.d..
The entries in η ∈ Rr may be from a standard Gaussian or LaPlace distribution, i.e., η 〜 N(0, 1)r
for Gaussian mechanism and η 〜L(0,1)r for Laplace mechanism. The rank condition for A is
without loss of generality as it guarantees a minimal value for the dimension of the random vector
η for a concise investigation of the mechanism.
1.3	Contributions and Related Work
Contributions. Theories in necessary and sufficient conditions regarding whether a prescribed dif-
ferential privacy level can be achieved with structured (non i.i.d.) noise injection are established,
respectively, for Gaussian mechanism and Laplace mechanism. For any expected privacy level, sys-
temic approaches are developed for realizing the privacy budget sufficiently and tightly with struc-
tured noise injection. As a result, a systemic framework for differential privacy over affine manifolds
with linear queries has been established. The obtained theories and proposed approaches are applied
to the cloud-based control problem for feedback systems. In this application, the affine manifold
is shown to have naturally arisen and the framework developed in this work becomes effective in
differential privacy analysis and noise injection mechanism design.
Related work. Differential privacy, proposed in Dwork et al. (2006b), is a rigorous notion for
defining and preserving data privacy. In the last decades, extensive developments have been emerged
3
Under review as a conference paper at ICLR 2022
in such as the mechanism design McSherry & Talwar (2007) and applications to machine learning
Chaudhuri et al. (2011) and deep learning Abadi et al. (2016), etc, advancing the differential privacy
as a gold standard in data privacy. Data correlation may be an important factor influencing privacy
leakage and thus cannot be ignored when designing or analyzing differentially private mechanisms
Kifer & Machanavajjhala (2011); Takbiri et al. (2020). Generalizations of differential privacy have
been developed for probabilistically correlated data, e.g., the Pufferfish privacy proposed in Kifer
& Machanavajjhala (2012; 2014) by employing the notions of discriminative pairs of secrets and
data evolution scenarios that may incorporate the data correlation. In the Pufferfish framework,
domain experts are allowed, specifying the knowledge of e.g. the set of potential secrets and data
correlation, which customizes the framework to the needs of given applications. Along the line of
Pufferfish, many developments have been witnessed recently, such as Pufferfish privacy mechanisms
for correlated data by a Bayesian network in Song et al. (2017) and the composition properties for
time-series data in Song & Chaudhuri (2017). The potential interdependency between data may
also be characterized as deterministic relationships. In He et al. (2014), the Blowfish privacy, as a
generalization of the differential privacy and inspired by the Pufferfish framework, is established for
the data of deterministic constraints/correlation on the notion of policy, specifying the secrets and
constraints that may be known about the data. As a result, this enables to introduce a new notion
of adjacency incorporating the deterministic constraints about the database, such as count query
constraint and marginal constraint. In this respect, this paper can be regarded as a generalization of
the Blowfish framework to the data with affine-manifold constraint.
Differential privacy has also been extended to metric spaces where the domain of secrets can be
from any space with a metric Holohan et al. (2015), where the space can be discrete, continuous, or
even functional. Our work is also an extension of this line of research but with distinctive features
as highlighted in Remark 2. Calibrating noises plays a significant role in maximizing the data utility
while preserving the desired differential privacy. In the seminal work Dwork et al. (2006b), the
notion of sensitivity is introduced to characterize the deviation of Laplace noises ensuring the (, 0)-
differential privacy. Along this line, Nissim et al. (2007) shows that the utility can be improved by
employing data-dependent noises calibrated to the smooth sensitivity. In Balle & Wang (2018) the
necessary and sufficient condition for the Gaussian mechanism is established from the perspective of
privacy loss, yielding the optimal Gaussian noises for (, δ)-differential privacy. For the differential
privacy of functional data, the correct noise level is studied in Hall et al. (2013) by establishing a
measure of sensitivity in the reproducing kernel Hilbert space norm. Taking the noise distribution
into consideration, Geng & Viswanath (2015) shows that the staircase noise distribution is optimal
for (, 0)-differential privacy and the uniform noise distribution is near-optimal for (0, δ)-differential
privacy. In this paper, besides applying the sensitivity as in these results, we also take the noise
structure as a key factor to calibrate the noises. By injecting appropriate correlated noises, it is
shown that the resulting utility may be improved compared to injecting i.i.d. noises. In view of
the adopted calibration tool using noise structure, this paper is closely related to the results in Li
et al. (2015) where the matrix mechanism is designed to answer a workload of linear counting
queries with correlated noises to improve the utility, and also in Chanyaswad et al. (2018) where
the matrix-variate Gaussian mechanism is calibrated by adding a matrix-valued noise drawn from a
matrix-variate Gaussian distribution. Besides, our paper can be regarded as an extension of the both
works by studying the vector-valued mechanism with affine-manifold dependency data.
Notation. Denote by R the real numbers, Rn the real space of n dimension and N the set of natural
numbers. For x ∈ Rn, denote xi as the i-th entry of x, kxk0, kxk1 and kxk as the 0, 1, and 2-norm
of vector x, respectively, and for any set p ⊆ {1, . . . , n} of l elements, xp a vector of dimension l
with each entry as xj with j ∈ p. Denote ei a basis vector of dimension n whose entries are all zero
expect the i-th as one. For matrix A ∈ Rm×n and set p := {p1, . . . ,pl} ⊆ {1, . . . , n} of l elements,
Ap a matrix of dimension m × l with each column being the pi-th column of A with i ∈ p, and we
denote Ep as a matrix of dimension m × l with each column being the basis vector ei , i ∈ p.
2	Differential Privacy Conditions
In this section, we present conditions for the mechanism M in (3) to achieve differential privacy,
under Gaussian and Laplacian mechanisms, respectively.
4
Under review as a conference paper at ICLR 2022
2.1	Gaussian Mechanism
First of all, we study the case with the entries in η ∈ Rr drawn from a standard Gaussian distribution,
i.e. η 〜N(0,1)r. We introduce Λ* = (Λ>Λ)-1Λ>, Ψj = In-Edj (Ddj)TD for j ∈ I, and ∆N =
maxj∈ι kΛtFΨjE-djE>deik for i ∈ V. Denote D⊥ ∈ Rn×(n-q) as a matrix such that DD⊥ = 0
and rank ([D> D⊥]) = n, and define Δn := maχi∈v{∆N} and φ(s) ：= √2π R-∞ e-τ2∕2dτ∙
We present the following result.
Theorem 1.	The mechanism M in (3) with η 〜N(0,1)r achieves (e, δ)-differentialprivacy under
μ-adjacency over Cd ifand only ifthere hold
rank (Λ) = rank ([Λ FD⊥]) = r;
(7)
(8)
2.2 Laplace Mechanism
Next, we consider the Laplace mechanism with η v L(0, 1)r, and study the (, 0)-differential
privacy of the mechanism M(x) over Cd. To this end, define ∆L := maxi∈V{∆iL } with ∆iL =
maxj∈ι ∣∣ΛtFΨjE-djE>djeikι for i ∈ V.
Theorem 2.	The mechanism M in(3) with η 〜L(0,1)r achieves (e, 0)-differentialprivacy under
μ-adjacency over Cd ifand only ifthere hold (7) and
Δl ≤ e∕μ.	(9)
In Theorems 1 and 2, (7) implies the least amount of independent standard Gaussian/Laplace noises
(i.e., r ≥ rank (FD⊥)) and provides a structural property of the noise matrix Λ for the differential
privacy; (8) and (9) quantify the privacy levels that can be achieved by the amount of injected
Gaussian and Laplace noises, respectively.
3	Structured Mechanism Design
In this section, we show how the conditions in Theorem 1 and Theorem 2 will inform efficient
structured randomization design.
3.1	Structured Gaussian Mechanism Design
In this subsection, we discuss for a fixed Gaussian mechanism M and a given privacy budget (, δ),
how we can design the structured randomization Λ so that M achieves (, δ)-differential privacy
under μ-adjacenCy over the affine manifold Cd.
In Algorithm 1, we present a design approach of the Gaussian noise γ := Λη by applying the
two conditions (7) and (8) in Theorem 1 such that the M achieves the prescribed (, δ)-differential
privacy under μ-adjacency over Cd.
Algorithm 1: Structured Gaussian Noise Design Algorithm
Input: Privacy levels E ≥ 0, δ > 0, μ > 0.
1.	Let η 〜N(0,1)r with r = rank (FD⊥);
2.	Let Λ ∈ Rn×r be a matrix sharing the same column space with FD⊥;
3.	Let σ be such that
	φ(μ∆N -	F)	- 5 -	μ⅛	-彳∖	≤ δ	(10) < 2σ	μ∆Nl	k	2σ	μ∆NJ
	with
	∆n := max	∣∣Λ*FΨjE-djE>d ei∣ .	(11) (i,j)∈V×I	j	j
Output: Y = σΛη.
5
Under review as a conference paper at ICLR 2022
It is clear that with Λ := σΛ following the steps 2 and 3 in Algorithm 1, the rank constraint (7) and
the inequality (8) are both satisfied. Regarding the design of Λ at the step 2, it can be easily achieved
by computing the basis vectors spanning the column space of FD⊥ and then assigning each column
of Λ with a basis vector. As for the design of σ from (10), one can either adopt the numerical design
algorithm in Balle & Wang (2018), or disregard the second negative term on the left side of (10),
and use an analytical but less tight lower bound as σ ≥ (μ∆N)/(,Φ-2(δ) + 2 + Φ-1(δ)).
3.2	Structured Laplace Mechanism Design
We now turn to the Laplace mechanism M, and propose a design approach of the random perturba-
tion γ := Λη in Algorithm 2 for (, 0)-differential privacy by Theorem 2.
Algorithm 2: Structured Laplace Noise Design Algorithm
Input: Privacy levels E ≥ 0, δ = 0, μ > 0.
1.	Let η 〜L(0,1)r with r = rank (FD⊥);
2.	Let Λ ∈ Rn×r be a matrix sharing the same column space with FD⊥;
3.	Let σ be such that
σ ≥ μ∆∆l/	(12)
with
AL= max ∣∣ΛtFΨjE-djE>d ei∣∣ι.	(13)
(i,j)∈V×I	j j
Output: Y = σΛη.
Following Algorithm 2, it can be verified that the resulting noise matrix Λ := σΛ fulfills both
requirements (7) and (9) in Theorem 2, achieving the desired (E, 0)-differential privacy over Cd.
4	Application: Differentially Private Control
Emerging applications in cyber-physical systems such as smart girds and intelligent transportations
have inspired cloud-based control system paradigms, where a dynamical system sends its output to
a cloud, and receives feedback control decisions from the cloud Tanaka et al. (2017). The benefit
of cloud-based control is promise in improved control accuracy and system performance since the
cloud holds more information about the environment and other systems; one particular cost of cloud-
based control is leak of private state trajectories to adversaries eavesdropping the communication
between the system and the cloud.
4.1	Cloud-based Control Systems
Consider the cloud-based control systems of the form
x(t + 1) = Ax(t) + Bu(t)
y(t) = Cx(t)
(14)
where the privacy-sensitive system state x(t) ∈ Rnx, the control input u(t) ∈ Rnu, the system
output/measurement y(t) ∈ Rny. In the cloud-enabled setup, the system transmits y(t) to the cloud
which computes a feedback control signal u(t) and then sends it back to the system for implemen-
tation. When the communication networks between the system and the cloud are eavesdropped by
adversaries, information about x(t) might be inferred.
We propose to perturb the output with random noises γ(t) and submit to the cloud the perturbed
output
y(t) ：= Cχ(t)+ γ(t).	(15)
See Fig. 1 for the considered cloud-based control scheme. In the following, our goal is to design the
random noises γ(t) by employing the established results in Section 2 to achieve the desired differ-
ential privacy of system states, while the readers of interest in the remainder controller design can
refer to, e.g. Iinear-quadratic-Gaussian (LQG) control AStrOm (2012) and neural network control
Ge et al. (2013). As Gaussian noises are more convenient and common for tackling in these control
6
Under review as a conference paper at ICLR 2022
Figure 1: Differentially private cloud-based control scheme (e.g., Tanaka et al. (2017)).
techniques, we choose the random noises γ(t) as structured Gaussian noises and thus pursue the
(, δ )-differential privacy with δ > 0.
4.2	Differentially Private Cloud-based Control
Let the system running iterations T ≥ nx and denote the observability matrix
OT := [C; CA;…；CATT].
We impose the following assumption on the system (14).
Assumption 1. (i). The system (14) is observable, i.e., rank (OT) = nx; (ii) The system matrix A
in (14) is nonsingular, i.e., rank (A) = nx.
In the following, we apply the previous results to design the injected random noises γ(t) such that the
differential privacy of the system states trajectory (x(t))tT=-01 is achieved under the desired privacy
level (e, δ, μ).
Affine-Manifold Constraint from Dynamics. We define n := Tnx and the private data x :=
x(0); x(1); . . . ; x(T - 1) ∈ Rn, and can obtain the mechanism M as
M(x) = Fx + γ	(16)
where F = IT 0 C and Y = [γ(0); Y⑴；...;Y(T -1)]. Note that the private data X is naturally and
deterministically correlated by the x-dynamics of (14), i.e., subject to the affine-manifold constraint
Cd= {X:DX+b=0}	(17)
with b = (IT -1 0 B)u, u := u(0); u(1); . . . ; u(T - 2), and
∈ R(n-nx)×n
A -Inx
D =	...	...
A -Inx
Here b is known by the adversaries as the communication messages (i.e., y(t) and u(t)) between
the system and the cloud are eavesdropped. Moreover, there holds rank (D) = n - nx, and by
Assumption 1,
rank e>	= n - nx + 1 ,	∀i = 1, . . . , n .
(18)
Structured Noise Mechanism. With the mechanism (16) and the affine manifold (17), we next apply
Algorithms 1 to design the random noises γ for a (, δ)-differentially private Gaussian mechanism
M . We denote
D⊥ = [Inx; A;…；AT-1] ,	(19)
satisfying DD⊥ = 0 and rank ([D> D⊥]) = n. We define
Oij = OTA1-ivj, (i,j) ∈ [1,T] × [1,nx],
7
Under review as a conference paper at ICLR 2022
where vj is a vector of dimension nx with entries being zero except the j -th being one. Letting
η 〜N(0,1)nx and Λ = FD⊥ =OT, and We design σ such that
Φ(党
—
- eΦ
μ∆ N
2σ
≤δ
(20)
where ∆N =	max	∣∣ A1-jvk II. See the discussions before Section 3.2 for more details
(j,k)∈[1,T]×[1,nx]
on how to derive such σ from (20). As a result, we design the random noise as γ = σOTη, i.e.,
γ(t) = σCAtη, with η 〜N(0,1)nx for t = 0,1,..., T - 1. Then the following result can be
concluded by Theorem 1.
Theorem 3. Given any privacy levels e, δ, μ > 0, let Assumption 1 and the random noise γ(t)=
σCAtη with η 〜N(0,1)nx and σ satisfying (20) for t = 0,1,..., T — L Then the system (14)
with the perturbed output (15) preserves the (e, δ)-differential privacy of the state trajectories under
manifold μ-adjacency.
5	Numerical Validations
In this section, we provide a numerical example to illustrate the effectiveness of the privacy-
preserving clould-based control approach based on the manifold differential privacy.
System setup. Consider the cloud-based control of autonomous vehicles with the dynamical model
Hoh et al. (2011); Yazdani et al. (2018) as
x(t + 1) = Ax(t) + Bu(t)
y(t)	=	Cx(t)
(21)
where x(t) = [p(t); v(t)] with p(t), v(t) as the private position and the velocity, respectively, and
the system matrices
A = 10 T1s , B = TsT2 * /2 , C = [1 0]
with the sampling period Ts = 0.1. In the cloud-based setup, for privacy concern the vehicle
sends perturbed output y(t) = y(t) + γ(t) to the cloud, which then delivers to the vehicle for
implementation a control signal, of the form 1
u(t)	=	—K>(X(t) - Xr (t))
X(t + 1) = AX(t) + Bu(t) + L(y(t) - CX(t))
(22)
with KP = [3.4240; 4.3095], L = [0.8266; 0.6973], and the reference trajectory Xr (t)
[pr (t); vr (t)] = tanh(t); 1 - |tanh(t - 9)|.
Experiments. We note that the v(t)-dynamics in (21) is dependent of v(t), u(t) but independent
of the private state p(t). In other words, the privacy-sensitive positions p(t) are affine-manifold-
dependent on the p(t)-dynamics. In view of this, we apply the previous Gaussian mechanism design
in Subsection 4.2 with A = 1 and C = 1, and by Algorithm 1, take γ(t) = ση with η 〜N(0,1)
and σ satisfying
φ(2σ - eσ)-eW-2σ - e∣)≤ δ∙
In simulations, we let the running time T = 100 and the desired privacy levels (e, δ, μ) =
(1,10-2,1), and then design γ(t) = ση with the standard Gaussian noise η 〜 N(0,1) and
σ = 2.5244.
1Here for simplicity we choose a classical output-feedback controller. However, it is noted that other meth-
ods such as LQG control AStrom (2012) and neural network control Ge et al. (2013) are also applicable with
no influence on the design of noises γ(t) for privacy preservation.
8
Under review as a conference paper at ICLR 2022
Figure 2: Tracking errors p(t) - pr (t)
Results. The resulting tracking errors are presented in Fig. 2, where the tracking velocity error
is asymptotically vanishing, while the tracking position error is not vanishing in the mean-square
sense due to presence of noises for privacy preservation. We also show the relationship between
the tracking performance and the privacy requirements. The simulation results are presented in
Fig. 3. It is clear that a higher privacy level (i.e., a smaller ) leads to larger tracking errors of
both position and velocity in the mean square senses, demonstrating the trade-off between the data
utility and the differential privacy guarantee. Besides, in Fig. 3 we also compare the tracking
performance by calibrating the noises via the proposed structured noise injection approach (r = 1)
to the common approach of using i.i.d. noises (r = T). It can be seen that the calibrated noises
following Algorithm 1 shows better tracking performances under the same privacy requirement,
implying that the proposed structured noise injection approach may improve the data utility.
Figure 3: Mean-square tracking errors p(t) - pr (t) (left) and v(t) - vr (t) (right) under different
privacy requirements and structured noises (r = 1 for the structured noises following Algorithm 1
and r = T for i.i.d. noises).
Reproduction of the results. The code used for producing this numerical example is provided in
the supplementary material.
6	Conclusions
We have studied differential privacy for mechanisms generated by linear queries over affine man-
ifolds. We established necessary and sufficient conditions on whether differential privacy can be
achieved when the affine manifolds encode geometry of the entries in a dataset. The derived frame-
work was applied to differentially private cloud-based control, for which the affine-manifold data
dependency has been encoded in the systems themselves. In future work, it would be interesting
to further investigate extending these results to general differentially private mechanisms with non-
linear queries, and the resulting theories will then have direct applicability in applications such as
differentially private deep learning.
9
Under review as a conference paper at ICLR 2022
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
conference on computer and communications security, pp. 308-318, 2016.
Mario Alvim, Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Anna Pazii. Local differen-
tial privacy on metric spaces: optimizing the trade-off with utility. In 2018 IEEE 31st Computer
Security Foundations Symposium (CSF), pp. 262-267. IEEE, 2018.
Karl J Astrom. Introduction to stochastic control theory. Courier Corporation, 2012.
Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Ana-
lytical calibration and optimal denoising. In International Conference on Machine Learning, pp.
394-403. PMLR, 2018.
Thee Chanyaswad, Alex Dytso, H Vincent Poor, and Prateek Mittal. Mvg mechanism: Differential
privacy under matrix-valued query. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security, pp. 230-246, 2018.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research, 12(3), 2011.
Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach
for learning differentially private generators. Advances in Neural Information Processing Systems
(NeurIPS 2020), 2020.
Dorothy E Denning and Peter J Denning. The tracker: A threat to statistical database security. ACM
Transactions on Database Systems (TODS), 4(1):76-96, 1979.
Dorothy E Denning and Jan Schlorer. A fast procedure for finding a tracker in a statistical database.
ACM Transactions on Database Systems (TODS), 5(1):88-102, 1980.
Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In Proceedings of the
twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,
pp. 202-210, 2003.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data,
ourselves: Privacy via distributed noise generation. In Annual International Conference on the
Theory and Applications of Cryptographic Techniques, pp. 486-503. Springer, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006b.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found.
Trends Theor. Comput. Sci., 9(3-4):211-407, 2014.
Natasha Fernandes. Differential privacy for metric spaces: information-theoretic models for privacy
and utility with new applications to metric domains. PhD thesis, Ecole Polytechnique Paris;
Macquarie University, 2021.
Shuzhi Sam Ge, Chang C Hang, Tong H Lee, and Tao Zhang. Stable adaptive neural network
control, volume 13. Springer Science & Business Media, 2013.
Quan Geng and Pramod Viswanath. Optimal noise adding mechanisms for approximate differential
privacy. IEEE Transactions on Information Theory, 62(2):952-969, 2015.
Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Differential privacy for functions and func-
tional data. The Journal of Machine Learning Research, 14(1):703-727, 2013.
Xi He, Ashwin Machanavajjhala, and Bolin Ding. Blowfish privacy: Tuning privacy-utility trade-
offs using policies. In Proceedings of the 2014 ACM SIGMOD international conference on Man-
agement of data, pp. 1447-1458, 2014.
10
Under review as a conference paper at ICLR 2022
Baik Hoh, Toch Iwuchukwu, Quinn Jacobson, Daniel Work, Alexandre M Bayen, Ryan Herring,
Juan-Carlos Herrera, Marco Gruteser, Murali Annavaram, and Jeff Ban. Enhancing privacy and
accuracy in probe vehicle-based traffic monitoring via virtual trip lines. IEEE Transactions on
Mobile Computing,11(5):849-864, 2011.
Naoise Holohan, Douglas J Leith, and Oliver Mason. Differential privacy in metric spaces: Nu-
merical, categorical and functional data under the one roof. Information Sciences, 305:256-268,
2015.
Zhenqi Huang, Sayan Mitra, and Nitin Vaidya. Differentially private distributed optimization. In
Proceedings of the 2015 International Conference on Distributed Computing and Networking, pp.
1-10, 2015.
Daniel Kifer and Ashwin Machanavajjhala. No free lunch in data privacy. In Proceedings of the
2011 ACM SIGMOD International Conference on Management of data, pp. 193-204, 2011.
Daniel Kifer and Ashwin Machanavajjhala. A rigorous and customizable framework for privacy.
In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database
Systems, pp. 77-88, 2012.
Daniel Kifer and Ashwin Machanavajjhala. Pufferfish: A framework for mathematical privacy
definitions. ACM Transactions on Database Systems (TODS), 39(1):1-36, 2014.
Chao Li, Gerome Miklau, Michael Hay, Andrew McGregor, and Vibhor Rastogi. The matrix mech-
anism: optimizing linear counting queries under differential privacy. The VLDB journal, 24(6):
757-781, 2015.
Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science (FOCS’07), pp. 94-103. IEEE, 2007.
Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. Smooth sensitivity and sampling in private
data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,
pp. 75-84, 2007.
Junfei Qiu, Qihui Wu, Guoru Ding, Yuhua Xu, and Shuo Feng. A survey of machine learning for
big data processing. EURASIP Journal on Advances in Signal Processing, 2016(1):1-16, 2016.
Kevin Scaman, Francis Bach, Sebastien Bubeck, Yin Lee, and LaUrent Massoulie. Optimal Con-
vergence rates for convex distributed optimization in networks. Journal of Machine Learning
Research, 20:1-31, 2019.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security, pp. 1310-1321, 2015.
Shuang Song and Kamalika Chaudhuri. Composition properties of inferential privacy for time-series
data. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pp. 814-821. IEEE, 2017.
Shuang Song, Yizhen Wang, and Kamalika Chaudhuri. Pufferfish privacy mechanisms for correlated
data. In Proceedings of the 2017 ACM International Conference on Management of Data, pp.
1291-1306, 2017.
Nazanin Takbiri, Amir Houmansadr, Dennis L Goeckel, and Hossein Pishro-Nik. Privacy of de-
pendent users against statistical matching. IEEE Transactions on Information Theory, 66(9):
5842-5865, 2020.
Takashi Tanaka, Mikael Skoglund, Henrik Sandberg, and Karl Henrik Johansson. Directed informa-
tion and privacy loss in cloud-based control. In 2017 American Control Conference (ACC), pp.
1666-1672. IEEE, 2017.
Nan Wu, Farhad Farokhi, David Smith, and Mohamed Ali Kaafar. The value of collaboration in
convex machine learning with differential privacy. In 2020 IEEE Symposium on Security and
Privacy (SP), pp. 304-317. IEEE, 2020.
11
Under review as a conference paper at ICLR 2022
Kasra Yazdani, Austin Jones, Kevin Leahy, and Matthew Hale. Differentially private lq control.
arXiv preprint arXiv:1807.05082, 2018.
Jun Zhao, Junshan Zhang, and H Vincent Poor. Dependent differential privacy for correlated data.
In 2017 IEEE Globecom Workshops (GC WkshPs),pp.1-7.IEEE, 2017.
Junyi Zhu and Matthew B Blaschko. R-gap: Recursive gradient attack on privacy. In International
Conference on Learning Representations, 2020.
Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and Yu-Xiang Wang. Private-knn: Practical dif-
ferential privacy for computer vision. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 11854-11862, 2θ20.
12
Under review as a conference paper at ICLR 2022
A Proofs
A.1 Proof of Theorem 1
A.1.1 A technical lemma
In this subsection, we present the necessary and sufficient condition from the perspective of pri-
vacy loss Balle & Wang (2018) for (, δ)-differential privacy of the mechanism (3) over the affine
manifold Cd .
Given any x, x0, we denote the random variables Y = M (x) and Y0 = M (x0), whose probability
density function are given by g(y - Fx) and g(y0 - Fx0), respectively. Then, we define the privacy
loss random variables Lχ,χo = 'χ,χo(Y) and Lχ,χ = 'χo,χ(Y0), where 'χ,χo is the privacy loss
function of mechanism M on a pair of adjacent x, x0 as
……(g(y-F⅛).
In view of the previous definitions, the mechanism output random variable Y = M (x) is trans-
formed into the privacy loss random variable Lx,x0. Moreover, following (Balle & Wang, 2018,
Theorem 5), the (, δ)-differential privacy over Cd can be rewritten in the form of the privacy loss
random variable.
Lemma 1. The mechanism M is (e, δ)-DP under μ-adjacency over the affine manifold Cd if and
only if
P(Lx,x0 ≥ E) - eeP(Lx0,x ≤ -E) ≤ δ	(23)
holdsfor every pair (x, x0) ∈ Adj(μ, Cd).
A.1.2 Proof of necessity
In this following, we suppose that the mechanism M is (E, δ)-differentially private with privacy
levels e, δ, μ, and prove that both (7) and (8) hold, respectively.
Necessity of (7). We first apply the contradiction method to show that (7) holds. We suppose that (7)
is not satisfied, i.e.,
rank (Λ) = rank ([Λ FD⊥]) .	(24)
Then We let Λker ∈ R(m-r)×m be such that ΛkerΛ = 0 and rank ([Λ*; Λker]) = m. Thus, We have
ΛkerFD⊥ = 0, i.e., there exist (x, x0) ∈ Adj(μ, Cd) such that
D(x - x0) = 0,	ΛkerF(x - x0) 6= 0 .
With the pair (x, x0) being the case, we let M* ⊂ Rm such that ΛkerFx ∈ ΛkerM*, ΛkerFx0 ∈
ΛkerM* and Λ*M* = Rr. Then it is observed that
P(M(x) ∈ Mj
=P(M(x) ∈ MJ
= p(ΛkerFx ∈ ΛkerM*, AG + ΛΛη ∈ A^M*)
= P(ΛkerFx ∈ ΛkerM*)p(Λ*Fx + η ∈ Rr)
=1
and similarly,
P(M(x0) ∈ Mj
=P(M(x0) ∈ MJ
= P(ΛkerFx0 ∈ ΛkerM*)P(Λ^Fx0 + η ∈ Rr)
=0
This indicates that there is no (E, δ) such that (1) is satisfied, contradicting with the fact that M is
differentially privat with privacy levels (e, δ, μ). Therefore, (24) does not hold, which completes the
necessity proof of statement (i).
Necessity of (8). In view of the previous analysis, for any x ∈ Rn and any M ⊆ Rm it follows that
P(M(x) ∈ M) = P(ΛkerFx ∈ ΛkerM)P(Λ*Fx + η ∈ Λ*M) .	(25)
13
Under review as a conference paper at ICLR 2022
With this in mind, we suppose that M over Cd is (, δ)-differentially private and (7) is satisfied, and
proceed to show the inequality (8) is also satisfied.
We first show
P(AkerFX ∈ MJ = P(AkerFx0 ∈ Mj ∈ {0,1}	(26)
for all Mi ⊆ Rm-r and all (x, x0) ∈ Adj(μ, Cd). By the definition of Cd, given any
(x, x0) ∈ Adj(μ, Cd) it is clear that D(X - x0) = 0. On the other hand, by (7) We have
AkerFD⊥ = 0. Thus, it can be easily verified that AkerF(X - x0) = 0 for all (x, x0) ∈ Adj(μ, Cd).
This immediately yields (26) by recalling that the event AkerFX ∈ M1 is deterministic.
With (26), we now proceed to show (8). By combining (25) and (26), given any (x, x0) ∈ Adj(μ, Cd)
We have
P M(X) ∈ M ≤ eP M(X0) ∈ M +δ,
∀M ⊆ Rm
0 P(AtFX + η ∈ M2) ≤ eeP(AtFx0 + η ∈ M2) + δ,
∀M2 ⊆ Rr
(27)
It is noted that the lower inequality of (27) indicates that the mechanism M2(x) := AtFX + η over
Cd is (, δ)-differentially private.
Next, we apply this fact and Lemma 1 to the mechanism M2(X) to show (8). It can be verified that
the random variables Y := AtFX + η v N(AtFX, Ir), Y0 := AtFX0 + η v N(AtFX0, Ir) and the
privacy loss function
'χ,χo(y) = (y - AtFX)>AtF(X - X0) + 1 kAtF(X - x0)∣∣2
Then, we can obtain both the privacy loss random variables Lx,x0 and Lx0,x share the same distri-
butions as
LxM v N(η∕2,η)
with η = kAtF(X-X0)k2. According to Lemma 1, the (, δ)-differentialprivacy ofM2 is equivalent
to saying
P(Lx,χ0 ≥ e) - eeP(Lxo,χ ≤-e) ≤ δ
i.e.,
Φ(-√√η + √2η) - eeΦ(-√√η-√2η) ≤ δ	(28)
for all (x, x0) ∈ Adj(μ, Cd). We note that the left side of the above inequality increases as η in-
creases by Balle & Wang (2018). Thus, there must hold
δ
ηmax :=	sup	kAtF(X - X0)k2
(x,x0)∈Adj(μ,Cd)
(29)
where we have defined
We now proceed to show that ηmaχ = μ2∆N∙ For any (x, x0) ∈ Adj(μ, Cd), we recall Definition 4
and observe that for any j ∈ I
X - X0
= Edj(Xdj -X0dj) + E-dj (X-dj -X0-dj)
= -EdjDd-j1D-dj(X-dj -X0-dj) + E-dj (X-dj -X0-dj)
(=a) [In-EdjDd-j1D]E-dj(X-dj - X0-dj)
= ΨjE-dj (X-dj - X0-dj )
where the third equation of (5) has been used to obtain the equation (a). Then, for any i ∈ V with
|xi - xi∣ ≤ μ, by using the first two equations of (5) there always exists j such that i ∈ Pj and
X - X0 = Ψj E-dj (X-dj - X0-dj ) = Ψjei (xi - x0i)	(30)
yielding
kAtF(x - X0)k ≤ kAtFΨjeikμ ≤ μ∆N
Thus, we have ηmaχ = μ2∆N, proving (8) by (29).
14
Under review as a conference paper at ICLR 2022
A.1.3 Proof of sufficiency
With (7) and (8), we now prove that the mechanism M preserves (, δ)-differential privacy over Cd.
We first show that under (8) the mechanism M2(x) := Λ*Fx + η over Cd is (e, δ)-differentially
private. According to Lemma 1, this is equivalent to proving (28) for all (x, x0) ∈ Adj(μ, Cd). Then,
by recalling the fact that ηmaχ = μ2∆N, one can immediately conclude from the (8) that (28) is true
for all (x, x0) ∈ Adj(μ, Cd), i.e.,
P(ΛtFx + η ∈ M2) ≤ eeP(ΛtFx0 + η ∈ M2) + δ, ∀M2 ⊆ Rr .
Thus, by (25) and (26) we further have
P(M(x) ∈ M)
= P(AkerFX ∈ ΛkerM)P(ΛtFx + η ∈ ΛtM)
≤ P(ΛkerFx0 ∈ ΛkerM) (eeP(ΛtFχ0 + η ∈ ΛtM) + δ)
≤ eeP(ΛkerFx0 ∈ ΛkerM)P(ΛtFx0 + η ∈ ΛtM) + δ
= eeP(M(x0) ∈ M) + δ
for all M ⊆ Rm . Therefore, the proof is completed.
A.2 Proof of Theorem 2
A.2. 1 Proof of necessity
Necessity of (7). The proof is the same as the necessity proof of (7) in Appendix A.1.2, and is thus
omitted for simplicity.
Necessity of (9). Similar to the arguments in the necessity proof of (8), we can obtain (27) and
thus the (e, 0)-differential privacy of the Laplace mechanism M2(x) := Λ*Fx + η w.r.t. Cd for
η 〜L(0,1)r, by using (7) and the fact that M(x) is (e, 0)-differentially private over Cd. With this
in mind, we then proceed to show (9) by contradiction.
We suppose that Δl > e∕μ. This, by the definition of Δl and (30), indicates that there exists
(x, x0) ∈ Adj(μ, Cd) such that
MtF(X - x0)k1 = μ∆L >e
With such (X, X0) being the case, we then can always construct a non-empty set S ⊂ Rr such that
ku - ΛtFXk1 = ku - ΛtFX0 - ΛtF(X - X0)k1
= ku - ΛtFX0k1 - kΛtF(X - X0)k1
for all u ∈ S. We then let M2 = {y ∈ Rr : y = u - ΛtFX, u ∈ S}, and observe that
P(ΛtFx + η ∈ M2)
=2r RM2 e-kηk1 dη
=2r RS e-ku-2FxkIdu
=	/ RS e-ku-ΛtFx0kι + kΛtF(x-x0)kι du
=ekA∣F(X-XO)kιP(ΛtFχ0 + η ∈ M2)
> ep(ΛtFx0 + η ∈ M2),
which clearly contradicts with the fact that M2 is (, 0)-differentially private, i.e., for all M2 ⊆ Rr
and all (x, x0) ∈ Adj(μ, Cd), there holds
P(ΛtFx + η ∈ M2) ≤ eep(ΛtFx0 + η ∈ M?).
Therefore, we have Δl ≤ e∕μ, proving (9).
15
Under review as a conference paper at ICLR 2022
A.2.2 Proof of sufficiency
With (7) and (9), we now prove that the mechanism M preserves (, 0)-differential privacy over the
affine manifold Cd.
We first show that under (9) the Laplace mechanism M2(x) := Λ*Fx + η over Cd is (e, 0)-
differentially private. We first observe from the definition of ∆L and (30) that
MtF(X - x0)k1 ≤ μ∆L ≤ e
for all (x, x0) ∈ Adj(μ, Cd). Then, we note that for all M2 ⊆ Rr,
P(ΛtFx + η ∈ M2) ≤ ekNF(X-XO)kιp(ΛtFx + η ∈ M2)
≤ eeP(ΛtFx + η ∈ M2),
proving (, 0)-differential privacy ofM2 over the affine manifold Cd.
Thus, by (25) and (26) we further have
P(M(x) ∈ M)
= P(AkerFX ∈ ΛkerM)P(ΛtFx + η ∈ ΛtM)
≤ P(ΛkerFx0 ∈ ΛkerM) (eeP(ΛtFx0 + η ∈ ΛtM))
= eeP(M(x0) ∈ M).
for all M ⊆ Rm . Therefore, the proof is completed.
A.3 Proof of Theorem 3
To prove Theorem 3, We first need to further elaborate the expressions of ∆N in (11) with the
mechanism M in (16) and the manifold Cd in (17). It is observed that with D in (18), there exist T
sets dj, j ∈ I := {1, . . . , T} such that Ddj ∈ R(n-nx)×(n-nx) is nonsingular and -dj := V/dj =
{(j - 1)nx + 1, . . . , (j - 1)nx + nx}. It then follows that
ΨjE-dj	= (In - Edj(Ddj)TD)E-dj
= E-dj - Edj (Ddj)-1 D-dj
= D⊥A1-j
where the last equality is obtained by using (Ddj )-1D-dj = A1-j; . . . ; A-1; A; . . . ; AT-j and
the definition of D⊥ in (19). This further implies ΛtFΨjE-djE>dei = ΛtOτA1-jE>dei for
i ∈ V andj ∈ I.
It is also noted that for any i ∈ dj, E->d ei = vk with k := i - (j - 1)nx ∈ [1, nx], while for any
i ∈ -dj, E>-d ei = 0. Therefore, we have
∆N := max(i,j)∈v×ι ∣∣ΛtFΨjE-djE>djeik
=max(j,k)∈ι×[i,nχ] ∣∣ΛtOτA1-jVk∣∣ ,
completing the proof.
16