Under review as a conference paper at ICLR 2022
Efficient Semi-Discrete Optimal Transport Us-
ing the Maximum Relative Error between Dis-
TRIBUTIONS
Anonymous authors
Paper under double-blind review
Ab stract
Semi-Discrete Optimal Transport (SDOT) transforms a continuous distribution to
a discrete distribution. However, existing SDOT algorithms for high dimensional
distributions have two limitations. 1) It is difficult to evaluate the quality of
the transport maps produced by SDOT algorithms, because computing a high-
dimensional Wasserstein distance for SDOT is intractable and 2) The transport map
cannot guarantee that all target points have the corresponding source points that
map to them. To address these limitations, we introduce the Maximum Relative
Error (MRE) and the L1 distance between the target distribution and the transported
distribution computed by an SDOT map. If the MRE is smaller than 1, then every
target point is guaranteed to have an area in the source distribution that maps to it.
We propose a statistical method to compute the lower and upper bounds of the MRE
and the L1 distance. We present an efficient Epoch Gradient Descent algorithm
for SDOT (SDOT-EGD) that computes the learning rate, number of iterations, and
number of epochs in order to guarantee an arbitrarily small expected value of the
MRE. Experiments on both low and high-dimensional data show that SDOT-EGD
is much faster and converges much better than state-of-the-art SDOT algorithms.
We also show our method’s potential to improve GAN training by avoiding the
oscillation caused by randomly changing the association between noise and the
real images.
1 Introduction
Optimal Transport (OT) is a principled way of measuring the discrepancy of two distributions by
transforming the source distribution to the target distribution. In recent years, OT has been widely
used in machine learning (Frogner et al., 2015; Arjovsky et al., 2017; Gulrajani et al., 2017; Liu et al.,
2018), computer vision (Salimans et al., 2018; Liu et al., 2019; An et al., 2020a), computer graphics
(Solomon & Vaxman, 2019; Lavenant et al., 2018), etc.
In Semi-Discrete Optimal Transport (SDOT) (Peyre et al., 2019), the source distribution is continuous
while the target distribution is discrete. Recently, a number of methods have applied SDOT to
generative models, an important topic in machine learning. An et al. (2020a) proposed an Auto-
Encoder OT model applying SDOT in the latent space to tackle mode collapse in generative models.
Further, An et al. (2020b) used SDOT to stabilize GAN training and produced highly realistic images.
Leclaire & Rabin (2019; 2020) used SDOT on image patches for texture synthesis and style transfer.
While being a promising approach, compared to its discrete OT counterpart, the optimization of
SDOT is underexplored, especially in high dimensional spaces. One may think of sampling from the
source distribution and turn the SDOT into a discrete OT problem. However, it has been shown that if
the discrete OT has an -suboptimal estimate of the SDOT in a d-dimensional space (Genevay et al.,
2019), then the sample complexity is O(*), which is prohibitively expensive when d is large.
From the optimization point of view, existing SDOT algorithms can be broadly classified into two
categories: deterministic methods and stochastic methods. Deterministic methods (Kitagawa, 2014;
Kitagawa et al., 2016; Levy, 2015; Merigot, 2011; Gu et al., 2013) have faster convergence rates, but
require computing the measure of a polytope. This is intractable for high dimensional distributions,
and thus limits their applicability. Stochastic methods use Stochastic Gradient Descent (SGD) to
1
Under review as a conference paper at ICLR 2022
optimize the SDOT objective and can be applied to any high dimensional distributions. SGD sets
the step size to Θ(1∕t), where t is the iteration number, to ensure convergence (Peyre et al., 2019).
Averaging SGD (ASGD)(Aude et al., 2016; Peyre et al., 2019) uses a step size of Θ(1∕√t) and
averages the past iterates. ASGD is proven to be faster than SGD (Peyre et al., 2019). Based on
ASGD, Leclaire & Rabin (2019; 2020) proposed a multi-layer approach which has been shown to be
much faster than single layer plain ASGD for SDOT. An et al. (2020a) used Monte-Carlo sampling
to estimate the gradient and then the Adam optimizer (Kingma & Ba, 2015) to optimize SDOT.
SDOT algorithms decompose the source space into Laguerre cells (Gu et al., 2013). Each cell is
mapped to a target point by the transport map outputted by an SDOT algorithm. For an optimal SDOT
algorithm, the total probability value of each cell is equal to the probability value of the corresponding
target point. However, all existing stochastic methods (Peyre et al., 2019; Leclaire & Rabin, 2019;
2020; An et al., 2020a) for SDOT in high dimensional space have two limitations. 1) They do not
have a mechanism to evaluate the quality of their computed transport maps, because computing a
high-dimensional Wasserstein distance for SDOT is intractable and 2) These SDOT algorithms (Peyre
et al., 2019; Leclaire & Rabin, 2019; 2020; An et al., 2020a) cannot guarantee that each target point
has an area in the source domain that maps to it. When training a GAN using SDOT, as shown in
Fig. 1, each cell in the noise space is mapped to a real image (e.g. a to A, b to B). If an image does
not have a corresponding area in the noise space that maps to it, we cannot sample a noise point that
can be trained to resemble this image. This potential mode collapse is a consequence of the second
limitation. One additional limitation for MC-Adam (An et al., 2020a) is that the sample complexity
of Monte-Carlo sampling to estimate the gradient of the SDOT dual objective is not established.
To address the above limitations, we make the following contributions:
1)	An SDOT map transforms a continuous source distribution to a discrete target distribution. If
there is at least one probability value equal to 0 in the transported target distribution, this means that
there is at least one target point that does not have an area in the source domain that maps to it. It
further means that the relative error of the probability values for this target point is 1. Thus, if we
force the Maximum value of the Relative Error (MRE) to be less than 1, then all probability values
in the transported distribution will be greater than 0 and each target point is guaranteed to have a
corresponding area in the source domain that maps to it.
2)	We use the MRE and the L1 distance between the target distribution and the transported distribution
obtained by an OT map to measure the quality of an SDOT map. The L1 distance is the L1 norm
of the gradient of the dual SDOT objective. We propose statistical methods to compute the lower
bounds and upper bounds of the MRE and the L1 distance. We establish the number of Monte-Carlo
samples to compute the bounds of the MRE and the L1 distance.
3)	We present an efficient Epoch Gradient
Descent algorithm for SDOT (SDOT-EGD)
that computes the learning rate, number of
iterations and number of epochs in order
to guarantee an arbitrarily small expected
value of the MRE.
4)	Experiments on 1D toy data show that
our computed MRE lower and upper bounds
are very close to each other and the real
MRE and L1 distance always lie in-between
the lower and upper bounds. Experiments
on 1D, 2D, 256D toy data and 256D real-
world data show that SDOT-EGD con-
Figure 1: SDOT matched GAN training mechanism. a,
b in the noise space are mapped to A and B in the image
space, respectively.
verges significantly better and faster than
state-of-the-art SDOT algorithms.
5)	We propose a GAN training mechanism
that uses SDOT-EGD to match the noise
points and the real image during GAN train-
ing shown in Fig. 1, where each cell in the
noise space is mapped to an real image. Experiments on the CelebA-HQ dataset (Karras et al.,
2
Under review as a conference paper at ICLR 2022
2018) and a COVID-19 X-ray image dataset (Konwer et al., 2021) show that our training mechanism
improves GAN training performance by 20%-30% in most cases.
2 Semi-Discrete Optimal Transport
The Monge-Kantorovich dual Optimal Transport (OT) formulation (Villani, 2008) is given as:
Problem 1. Given two bounded domains X and Y and their probability measures μ ∈ P(X), and
ν ∈ P(Y), respectively, find a function v to solve
max w(v)
v
vc(x)dμ(x) + ∕v(y)dν(y)
(1)
where c : X × Y 7→ [0, +∞] is the transport cost function, and vc (x) is the c-transform of v defined
as: vc (x) = miny∈Y {c(x, y) - v(y)}.
Consider μ and V as the source and target distributions, respectively. Problem 1 is the continuous OT
case. In this work, We focus on the semi-discrete OT case, i.e., the source distribution μ is continuous,
but the target distribution V is discrete. Let Y = {yi ∈ Rd}n=ι denote a finite set that contains n data
points. Denote v(i) = v(yi), i = 1, ..., n. ν is a discrete target distribution with probability values in
V being Vi, i = 1, ..., n. Let Vmin be the minimum probability value in V, i.e., Vmin = min{Vi}in=1,
LetI denote the set {1, 2, ..., n}. The Semi-Discrete OT (SDOT) can be formulated as maximizing
)
w(v)
min
i∈I
(C(X,yi) — v(i)) dμ(X) + Ev(i)νi
i∈I
(2)
Let h(x,v) = - mini∈ι (C(X,yi) - v(Q - P∈ v(i)v% and f(v) = Ex〜μh(x,v). Then we have
w(v) =-f(v). Maximizing Eq. (2) is equivalent to minimizing f(v). Since f(v) has the form of
the expectation of the function h(X, v), it allows us to solve SDOT using Stochastic Gradient Descent
(SGD) methods. The gradient of f(v) w.r.t. each element of v is (Kitagawa et al., 2019):
∂v(i)f(v)= Li(v)
dμ(x)—
Vi
(3)
where Li(v) is defined as Li(v) = {X ∈ X : ∀j 6= i, C(X, yi) - v(i) ≤ C(X, yj) - v(j)}. Each Li(v)
is called a Laguerre cell. The Laguerre cells form a partition of the space X, i.e. X = ∪iLi (v). In
SDOT, all points in Li(v) are transported to yi. The gradient of h(X, v) w.r.t. each element of v given
X is:
dv(i)h(x,v) = ILi(V)(X)- Vi	(4)
where ILi(V) is an indicator function of Li(v).
Stochastic Gradient Descent (SGD) is commonly used to optimize SDOT. To ensure the convergence,
the learning rate is typically Θ(1∕t), where t is the iteration number. SGD becomes slow when t
becomes large. Instead, almost all SDOT algorithms are based on Averaging Stochastic Gradient
Descent (ASGD)(Peyre et al., 2019; Aude et al., 2016). ASGD uses a Θ(1∕√t) learning rate and
guarantees that the SDOT objective converges in O(1/√T), where T is the total number of iterations.
3 Semi-Discrete Optimal Transport Using Epoch Gradient Descent
In this section, we introduce the Maximum Relative Error (MRE) and propose probabilistic bounds
to evaluate the quality of an SDOT map outputted by an SDOT algorithm. Then, we will propose
ASGD with a fixed step size and an efficient epoch gradient descent algorithm for solving SDOT. We
start with an assumption.
Assumption 1. We assume that all target points have probability values greater than 0, i.e., ∀i ∈
I, Vi > 0.
Assumption 1 is easily satisfied since we can remove the points that have zero probability value out
of the target point set Y.
3
Under review as a conference paper at ICLR 2022
3.1	The Maximum Relative Error (MRE)
A Semi-Discrete Optimal Transport (SDOT) algorithm divides the space of the source distribution
into non-overlapping Laguerre cells. All the current stochastic optimization methods aim to achieve
an -suboptimal SDOT objective. However, for SDOT, an -suboptimal objective value cannot truly
reflect how well the source space is divided. For example, there might not exist regions in X that
map to certain target points, even though the SDOT objective is -suboptimal, especially when the
number of target points, n, is large.
In this subsection, we introduce the Maximum Relative Error (MRE) to evaluate how well the source
space is divided using an SDOT map. Let p denote the transported distribution from the source
distribution μ using the SDOT output v, i.e. Pi = JL (V) dμ(x) denoting the total probability value of
the Laguerre cell Li(v). If there is a probability value inp that equals 0, e.g. pj = 0, meaning that
there is no area in the source domain that is mapped to yj , then the relative error between pj and νj
is |pj - Vj | /νj = 1. Having the relative errors for all Pi S and Vi S be less than 1 guarantees that all
target points have preimages.
Therefore, we propose to use the Maximum Relative Error (MRE) between two probability distribu-
tions to measure the quality of an SDOT map produced by an SDOT algorithm:
MRE (ν, p) = max ɪpi——Viɪ.	(5)
i∈I	Vi
MRE < 1 indicates all target points have preimages. This is important when applying SDOT to
generative models such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) where
mode collapse will happen when not all target points have preimages.
3.2	SDOT Map Evaluation
In SDOT, the evaluation of how good an SDOT map is in high dimensional space remains an open
problem. This problem boils down to computing the gradient of an SDOT objective, and further
to computing the volume of each Laguerre cell in high dimensional space, which is intractable in
practice (Leclaire & Rabin, 2020). Instead in this paper we propose a probabilistic approximation for
SDOT quality evaluation. In the following theorem, we present the main results that give probabilistic
bounds for evaluating an SDOT map, the L1 distance between the empirical transported distribution
and the target distribution, and the L1 distance between the estimated gradient and the true gradient
of an SDOT objective.
Theorem 1. Suppose Assumption 1 is satisfied, g^ven a continuous source distribution μ, a discrete
target distribution V, target data points Y = {yi ∈ Rd}n=ι, SDOT output V, a precision g and a
2
confidence 1 一 δ, 0 < δ < 1, let ξ = ^-1=^^^, and let P be the discrete transported distribution
using v. Draw d1∕(4δξνmin)] i.i.d. samples from μ and compute the empirical discrete transported
distribution P using V. Let g = P — V be the gradient and g = P — V be the empirical gradient of
def
the SDOTobjective at V. Let d = MRE(V,p) = maxn=ι{∣Pi — v∕∕%} be the empirical MRE, di =
kP — v∣∣ i be the empirical Li distance between P and V. Let ∆ = 4√δξnVmin + ,8δ log(1∕δ)ξVmin,
and ω = ξ2ξ2 + dξ + ξ. Then, with probability ofat least 1 — δ, each ofthefollowing holds: 1
1) the lower bound of MRE(V, P) is lb = max(d — 2ω + 2ξ, 0).
2)	the upper bound of MRE(V, P) is ub = min(d + 2ω + 2ξ, (1 —	Vmin)∕Vmin).
3)	the Li distance between P andP is bounded as kP — p∣∣i ≤ ∆.
4)	the Li distance between g and g is bounded as ∣∣g — gk i ≤ ∆.
5)	the Li distance between P and V is bounded as max(di — ∆, 0) ≤ kP — Vki ≤ min(di + ∆, 2).
The above theorem shows the lower and upper bounds of MRE and the Li distance using Monte-Carlo
sampling with d1∕(4δξVmin)e samples. We use MRE and the Li distance between P and V to estimate
the quality of an SDOT map with lower and upper bounds given by Theorem 1 1), 2) and 5). We
set a precision g and compute ξ using g. The MRE lower bound ∈ib is computed in 1) and the MRE
upper bound ub in 2). 3) gives us the bound of the Li distance between the empirical transported
1All theorem proofs can be found in the appendix.
4
Under review as a conference paper at ICLR 2022
distribution P and the real transported distribution p. 4) gives the bound of the Li distance between
the estimated gradient and the true gradient of the SDOT objective evaluated at v . Interestingly, 3)
and 4) do not depend on the data dimension d. This is very appealing because computing the exact
volume of the Laguerre cells in high dimensional space is intractable. In practice, we can set a desired
confidence 1 - δ and precision g, and use P to approximate p. To estimate the Li distance between
the target distribution ν and the transported distribution p, we need to compute the empirical L1
distance di and use 5) to compute the lower bound and upper bound of the true Li distance. The
smaller the g,the smaller the gap between the upper bound and the lower bound.
The algorithm for computing the MRE bounds is presented in Algorithm 1. In this algorithm, we
simply use the average of the theoretical MRE lower bound and upper bound to estimate the real MRE.
The time complexity of this algorithm is O(dn∕(∕νma)).
3.3 Averaging Stochastic Gradient Descent with A Fixed Step Size
In this subsection, we analyze the convergence
of Averaging SGD with a Fixed step size (AS-
GDF) for SDOT, since our SDOT-EGD is built
upon ASGDF.
Suppose X is bounded. Denote the maximum
transport cost between X and Y by C∞ =
supx,y c(x, y). In the t-th iteration of ASGDF,
t ≥ 1, by randomly drawing a sample xt from
μ, we obtain a function ft(vt-ι) = h(xt, vt-ι)
according to f (v). The SGD recursion of v is
expressed as: vt = vt-i - γft0(vt-i), where ft0
is the gradient of ft .
As MRE is an effective indicator of how well the
source space is divided, we propose an algorithm
that minimizes the MRE. From the definition of
MRE in Eq. (5), we observe that MRE is closely
related to the gradient norm of the objective
function f (v). Let v = T PT=I vt-1. In the
following theorem, we give the optimal step
size γ and the bound of the expectation of the
gradient norm of f at v for a fixed number of
total iterations T .
Theorem 2. With constant step size γ =
VnC∞+1，the expectation of the gradient norm
of function f at V is bounded by E kf 0(v)k2 ≤ T
Algorithm 1 Estimate the Maximum Relative Er-
ror (Estimate-MRE)
1:	Input: data Y = {yi}in=i, source distribution
μ, target discrete distribution {%}之1,SDOT
output v, confidence 1 - δ, precision g.
2:	Output: lb, ub, est
n	2
3:	Let Vmin = mini=1{νi}, ξ = 4(〃+；*+1)2
4:	Let T = d1∕(4δξνmin)e, initialize g = 0
5:	for j = 1 to T do
6:	Sample X 〜μ
7:	Compute the gradient g J Nvh(χ, v)
8:	Update average g = j-1 g + 1 g
9:	end for
10:	Compute P J g + V
11:	Compute d = maχn=ι{[pi — %|/%}
12:	Let Elb = d - 2(，0 + dξ + ξ - ξ)
13:	MRE lower bound lb = max(lb, 0)
_	O-,/ S 屋 一 「、
14:	Let Eub = d + 2(√ξ2 + dξ + ξ + ξ)
15:	MRE upper bound Eub = min(Eub, (1
Vmin)∕Vmin)
16:	Estimated MRE Eest = (Elb + Eub)∕2
、2
+ 10 + 6√nC∞ ).
—
The step size in Theorem 2 is O(1∕√T), with detailed parameters computed for SDOT, and the
convergence rate of the expectation of the gradient norm of the SDOT objective is O(1∕T). These
convergence results match the state-of-the-art convergence results in Bach (2014).
We want to design an ASGDF algorithm that outputs an SDOT map using vg such that MRE(V, P) ≤ E
for any given E > 0. Based on Theorem 2, we show how to set the step size and the number of
iterations in ASGDF in order to achieve a desired expected value of the MRE in the following theorem.
Theorem 3. Suppose Assumption 1 is satisfied, ∀E > 0, with constant step size γ
((4+√√C∞)) and number of iterations T = 4(14+6V√nC∞) , we have E[mre (ν,p)] ≤ e.
eVmin
24
In Theorem 3, the transport cost does not play an important role in choosing γ. For any transport cost
function, and any maximum transport cost C∞, (1 + √nC∞)∕(14 + 6√nC∞) ranges in [1/14,1∕6].
If the target distribution V is uniform, then γ is proportional to 1∕n.
5
Under review as a conference paper at ICLR 2022
The ASGDF for SDOT is shown in Algorithm 2.
In line 4 and 5, we compute the optimal step size
and the maximum number of iterations. Line 14-
17 are standard ASGD operations. Line 6, 9-12
are implemented to check if we can stop early.
We evaluate the MRE for the current v. We use
the est to approximate the real MRE. est < k
means that the current v is good enough and
We return the V . The time complexity of this
algorithm is O (dn2C∞人心 Vmin))∙
3.4 Epoch Gradient
DESCENT FOR SDOT (SDOT-EGD)
Although the ASGDF algorithm ensures the con-
vergence for any given , the objective decreases
sloWly in early iterations. To make ASGDF
converge quickly in early iterations, motivated
by the epoch gradient descent approach (Hazan
& Kale, 2014), We design an Epoch Gradient
Descent (EGD) algorithm for solving SDOT
(SDOT-EGD). 2
We list the steps of the SDOT-EGD algorithm
in Algorithm 3. The algorithm initializes k =
2n, k = 1. At the end of each epoch, We de-
crease k by half. In each epoch, We run the
ASGDF (Algorithm 2) to update the v obtained
from the last epoch. The algorithm terminates
Algorithm 2 Averaging Stochastic Gradient De-
scent with Fixed Step Size for SDOT (ASGDF)
1:	Input: data Y = {yi}in=1, source distribution
μ, target discrete distribution {νi}n=ι, initial
v, confidence 1 - δ, precision g, current pre-
cision k , maximum transport cost C∞
2:	Output： V, V, eib, Cub, Eest
n	2
3:	Let Vmin = mini=ι{νi}, ξ = 4(√i+]+1)2
4:	Let Yk = Ck ∙ Vmin/24 ∙ (1 + √nc∞)∕(14 +
6√nC∞)
5:	Let Tk = d4(14 + 6√nc∞)/(CkVmm)e
6:	Let Te = jn∕ξ]
7:	for j = 1 to Tk do
8:	if j mod Te equals 0 then
9:	Clb, Cub, Cest — Estimate-MRE(Y, μ, V, V,
1 — δ, c*)
10:	if Cest < Ck then
11:	return
12:	end if
13:	else
14:	Sample X 〜μ
15:	Compute the gradient g J Nvh(x,v)
(Eq. (4))
16:	Update v J v - γkg
17:	Update average V J j-1 V + j V
18:	end if
19:	end for
and returns V when the current
Ck < c*/2 or the
estimated MRE is less than or equal to c* . The maximum number of epochs isdlog? (8n∕g)].
In this manner, the algorithm uses a larger step
size at the start of the optimization. Thus, the
objective decreases fast in the beginning. As the
Ck becomes smaller, the step size also decreases
to ensure convergence. The time complexity of
Algorithm 3 is the same as that of Algorithm 2.
4	Applications to GANs
In traditional GAN training methods, a batch of
randomly sampled noise points and a batch of
randomly sampled real data are used to train a
GAN in each iteration. However, this can cause
Algorithm 3 Epoch Gradient Descent for SDOT
(SDOT-EGD)____________________________________
1:	Input: data Y = {yi}in=1, source distribution
μ, target discrete distribution {%}n=ι, confi-
dence 1 - δ, precision c*, maximum transport
cost C∞ .
2:	Output: V.
3:	Initialize C1 = 2n, k = 1, Cest J C1, and
V = 0.
4:	while Ck ≥ g/2 and Cest > c* do
5:	V, v, Clb, Cub, Cest J ASGDF(Y, μ, ν,v, 1 -
δ,g,Ck ,C∞)
6:	k = k + 1, Ck = Ck-1 /2
oscillations in GAN training. For example, a1	7: end while
and a? are noise points that are very close to
each other in a local neighbourhood. A and B
are real images that are very far from each other. In a GAN training iteration, a1 and image A are
sampled to train the GAN. Hence, the generator is trained to generate an image that is similar to A
using a1. But in the next iteration, a? and image B could be sampled to train a GAN. The generator
is trained to generate an image that is similar to B using a?. Therefore, the generator could be trained
to generate very different images using noise points from the same local area in different training
iterations. This leads to unstable GAN training.
We propose the SDOT matched GAN training mechanism shown in Fig. 1, where G is a Generator, a,
b are noise points, and A, B are real images. The leftmost box is the noise space and the rightmost
image is the data manifold. The data manifold has a low-dimensional embedding in the feature space.
2Despite its name, SDOT-EGD is in fact a stochastic method.
6
Under review as a conference paper at ICLR 2022
The feature space and the noise space have the same dimensionality. We use a feature extractor (See
Sec. 5.3) to extract image features. To train a GAN, the feature extractor is fixed and an SDOT map
is computed in advance from the noise space to the feature space. As each region in the noise
space is mapped to an image feature using the SDOT map and the image feature corresponds to a real
image, we have a mapping from the noise points to the real images. For example, in Fig. 1, noise in
the orange cell is mapped to image A and noise in the blue cell is mapped to image B, consistently
throughout the whole GAN training process. A generator is trained to generate images similar to
image A using noise points in the orange cell and images similar to image B using noise points in
the blue cell. This stabilizes GAN training. The detailed training process is in the appendix.
5	Experiments
In the experiments, We compare the proposed SDOT-EGD against the ASGD (Peyre et al., 2019;
Aude et al., 2016) because it is widely used in optimizing SDOT. We compare against the recently
proposed MC-Adam (An et al., 2020a) and the 2-layer ASGD (Leclaire & Rabin, 2020). We also
compare With the original Vanilla-EGD (Hazan & Kale, 2014) because SDOT-EGD is an EGD-type
method. We experiment on 1D, 2D and high dimensional toy data, and high dimensional real data.
5.1	1D Experiments
The SDOT has a closed form solution for 1D data (Leclaire & Rabin, 2020). We use the exact solution
to compute MRE and the Li distance between P nad ν. In this experiment, the source distribution μ is
a uniform distribution in [0, 1]. We sample 1000 target points in [-1, 1] With equal intervals betWeen
every two adjacent points. The target probability for each point is 1/1000.
In Fig. 2 a) and d), the shaded area depicts the computed lower and upper bounds given by our
theoretical analysis. The solid curves in a) and d) are exact values. The exact values always lie inside
the computed bounds. This verifies the correctness of our theoretical analysis of the bounds. Note
that the shaded area in Fig. 2 (a) is too thin to be easily visible because the average upper bound and
lower bound gap is smaller than 0.2. In both figures, SDOT-EGD converges rapidly to 0, while other
curves converge at high values. This demonstrates the effectiveness of SDOT-EGD.
5.2	2D Experiments
In 2D experiments, the source distribution μ is uniformly distributed in [0,1] X [0,1]. We randomly
sample 10000 points in [0, 1] × [0, 1] as target points, with each point having probability value of
1e-4. See Fig. 3 f) for the distribution of these target points. Therefore, an optimal SDOT algorithm
should divide [0, 1] × [0, 1] into 10000 Laguerre cells, with the area of each cell equal to 1e-4.
Fig. 3 visualizes the partition of the space by various methods. Each cell is colored according to
its area from small (blue) to large (red). We observe that SDOT-EGD divides the space into more
uniformly distributed cells.
Since we do not compute the exact MRE and L1 distance between p and ν for 2D data, we compute
the estimated MRE and L1 distance which are the average values of their lower and upper bounds,
respectively. We plot the estimated MRE and L1 curves in Fig. 2 b) and e) as well as their bounds.
Fig. 2 b) shows that SDOT-EGD can achieve much smaller MRE than other methods close to the
1.3 × 108 iteration point. Fig. 2 c) shows that SDOT-EGD converges faster than other methods.
5.3	High Dimension Experiments
To see whether these methods scale to high dimensional data, we evaluate the performance of different
methods in the 256D space. We evaluate various methods on both real-world data and toy data.
The CelebA-HQ dataset contains 29970 unique images. We downsize each image to 16 × 16 size and
transform the RGB images to gray images. The pixel values are re-scaled in [-1, 1]. The 16 × 16
gray images become 256D vectors. These 256D vectors are regarded as features and used as the
target distribution. The source distribution is a uniform distribution in [-1, 1]256. Fig. 2 (c) and (f)
show the performance of the various methods. In Fig. 2 (c), SDOT-EGD always has the lowest MRE.
After 70M iterations, SDOT-EGD’s MRE is close to 0.2. This guarantees that each image has an area
7
Under review as a conference paper at ICLR 2022
b) MRE for 2D toy data.
c) MRE for 256D real face data.
a) MRE for 1D toy data.
2
1
1
0.8
0.8
0.6
0.6
0.4
0.4
0 0.2
0 0.2
500
1000
1500
2000
2500
Iterations (x10000)
1
0 L
0
MC-Adam
------Vanilla EGD
------ASGD
------2-layer ASGD
------SDOT-EGD
1	1 0
3000	(
1 1.6
1 1.6
MC-Adam
Vanilla EGD
------ASGD
-----2-layer ASGD
SDOT-EGD
0.5
1
Iterations (x10000)
1.5
2
x104
1 14
8 14
0
d) L1 distance for 1D toy data. e) L1 distance for 2D toy data.
f) L1 distance for 256D real face data.
Figure 2:	Comparison of MRE and L1 distance in 1D and 2D toy data, and 256D real face data.
In each figure, the shaded area depicts the computed lower bound and upper bound given by our
theoretical analysis. The solid curves in a) and d) are real MRE values. The computed bounds coincide
very well with the real MRE values. This verifies the correctness of our theoretical analysis of the
bounds for SDOT Map evaluation. Solid curves in b) c) e) and f) are estimated curves using the mean
of the lower bound and upper bound. The MRE shaded bands are much thinner than the L1 distance
ones. This means that MRE evaluates SDOT maps more accurately. In all experiments, the proposed
SDOT-EGD converges much better and faster than other methods.
a) MC-Adam.
d) 2-layer ASGD.
b) Vanilla-EGD.
e) SDOT-EGD.
c) ASGD.
f) Target distribution.
Figure 3:	Laguerre cell partition. Cells are colored per their area from small (blue) to large (red).
Cell partition from our method SDOT-EGD is more uniform. MC-Adam has high cell area variance.
ASGD and 2-layer ASGD have some cells that are large. The vanilla-EGD has large cells in the
center, and has small cells in the lower-right and upper-left corners.
8
Under review as a conference paper at ICLR 2022
Figure 4: Images generated by GAN-0GP (SDOT-EGD). Fist row shows images generated on the
CelebA-HQ dataset, and the second row show images generated on the COVOC dataset.
in the source domain that maps to it. All the other methods in Fig. 2 (c) have MRE values greater
than 1. Fig. 2 (f) shows the results using L1 distance. SDOT-EGD surpasses the 2-layer ASGD after
70M iterations and is the best method among all the methods. This shows that SDOT-EGD not only
works well with MRE but also achieves lower L1 distance compared to state-of-the-art methods. The
experiments of 256D toy data can be found in the appendix.
5.4 GAN Experiments
We evaluate the SDOT matched training mechanism in training GANs. We apply the SDOT-EGD
on matching noise points and real images in GAN training. We denote the SDOT-EGD matched
training mechanism using WGAN-GP (Gulrajani et al., 2017) by WGAN-GP (SDOT-EGD) and using
GAN-0GP by GAN-0GP (SDOT-EGD). We perform experiments on two datasets: The CelebA-HQ
dataset (Karras et al., 2018) and a COVID-19 Outcome dataset (COVOC) (Konwer et al., 2021; Zhou
et al., 2021) containing 248 X-ray images. More details of this dataset could be found in the appendix.
The image size used in our experiments is 256 × 256.
Table 1 lists the Frechet Inception DiS- 				
tances (FIDs) (Heusel et al., 2017; Se-	Method	CelebA-HQ	COVOC
gal et al., 2021) of different methods.	AE-OT-GAN	720	-
On the CelebA-HQ dataset, GAN-0GP	WGAN-GP	1294	96.49
(SDOT-EGD) supasses AE-OT-GAN and	WGAN-GP (SDOT-EGD)	9.18	94.91
achieves the state-of-the-art FID of 6.99	GAN-0GP	912	98.76
in 256 × 256 image size. On the CelebA-	GAN-0GP (SDOT-EGD)	6.99	76.30
HQ datset, WGAN-GP (SDOT-EGD) re- duces the FID of WGAN-GP from 12.94	Table 1: FID of the various methods. Image size on both		
to 9.18 and GAN-0GP (SDOT-EGD) re- duces the FID of GAN-0GP from 9.82 to	datasets are 256 × 256.		
6.99. On the COVOC dataset, GAN-0GP
(SDOT-EGD) reduces the FID of GAN-0GP from 98.76 to 76.30. The performance improvements
are bewteen 20% - 30%. This shows the effectiveness of the SDOT matching mechanism in GAN
training. Fig. 4 shows the face images and X-ray images generated by GAN-0GP (SDOT-EGD).
These images look realistic. Please see the appendix to see more generated images.
6 Conclusion
In this paper, we proposed to use the MRE and the L1 distance to measure the performance of an
SDOT map. We proposed to compute the lower and upper bounds of the MRE and the L1 distance.
The number of samples of Monte-Carlo sampling to compute the bounds for SDOT is established.
Furthermore, we proposed the SDOT-EGD algorithm that can achieve an arbitrarily small expected
value of the MRE. Experiments show that SDOT-EGD performs much better than state-of-the-art
SDOT algorithms. We proposed to apply SDOT-EGD in GAN training. Experiments show that a
GAN that uses SDOT-EGD to match the noise points and real data can significantly improve the
performance of its counterpart that does not use SDOT-EGD.
9
Under review as a conference paper at ICLR 2022
Ethics S tatement
The GAN application of this paper generates realistic synthetic images, that can be used for fraudulent
or misinformation purposes which would be a potential negative social impact shared with most GAN
methods. Any methods that try to mitigate potential biases in CelebA-HQ would also address bias
concerns for our paper. The COVOC dataset used in our paper is a de-identified dataset (via DICOM
deidentification process) and does not contain personally identifiable information. The COVOC
dataset is obtained after Institutional Review Board (IRB) approval as non-human subjects research.
Reproducibility S tatement
The assumption of using SDOT-EGD is given in Section 3. The proofs of all the theorems in the
paper are provided in Section A.2. In Section A.3.2 and A.3.3, we run SDOT-EGD multiple times
and plot the error bars in Fig. 7. The standard deviations are very small when SDOT-EGD terminates,
indicating SDOT-EGD is reproducible.
References
Jayadev Acharya, Ilias Diakonikolas, Chinmay Hegde, Jerry Zheng Li, and Ludwig Schmidt. Fast
and near-optimal algorithms for approximating distributions by histograms. In Proceedings of the
34th ACM SIGMOD-SIGACT-SIGAISymposium on Principles ofDatabase Systems, pp. 249-263,
2015.
Dongsheng An, Yang Guo, Na Lei, Zhongxuan Luo, Shing-Tung Yau, and Xianfeng Gu. Ae-ot: a
new generative model based on extended semi-discrete optimal transport. ICLR 2020, 2020a.
Dongsheng An, Yang Guo, Min Zhang, Xin Qi, Na Lei, and Xianfang Gu. Ae-ot-gan: Training gans
from data specific latent distribution. In European Conference on Computer Vision, pp. 548-564.
Springer, 2020b.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the International Conference on Machine Learning, 2017.
Genevay Aude, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-
scale optimal transport. arXiv preprint arXiv:1605.08527, 2016.
Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic
regression. The Journal of Machine Learning Research, 15(1):595-627, 2014.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural
image synthesis. In Proceedings of the International Conference on Learning Representations,
2019.
Siu On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Near-optimal density estimation
in near-linear time using variable-width histograms. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems,
volume 27, pp. 1844-1852. Curran Associates, Inc., 2014.
Ilias Diakonikolas. Beyond histograms: structure and distribution estimation. In Found at http://www.
iliasdiakonikolas. org/stoc14-workshop/diakonikolas. pdf, volume 1, pp. 1. Citeseer, 2014.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learning
with a wasserstein loss. In Advances in Neural Information Processing Systems, 2015.
Aude Genevay, LenaiC Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyre. Sample complexity
of sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1574-1583. PMLR, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, 2014.
10
Under review as a conference paper at ICLR 2022
Xianfeng Gu, Feng Luo, Jian Sun, and S-T Yau. Variational principles for minkowski type problems,
discrete optimal transport, and discrete monge-ampere equations. arXiv preprint arXiv:1302.5472,
2013.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
2017.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for
stochastic strongly-convex optimization. The Journal of Machine Learning Research, 15(1):
2489-2512, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances
in Neural Information Processing Systems, 2017.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. In Proceedings of the International Conference on
Learning Representations, 2018.
Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations, 2015.
Jun Kitagawa. An iterative scheme for solving the optimal transportation problem. Calculus of
Variations and Partial Differential Equations, 51(1):243-263, 2014.
JUn Kitagawa, QUentin MerigoL and Boris Thibert. Convergence of a newton algorithm for Semi-
discrete optimal transport. arXiv preprint arXiv:1603.05579, 2016.
Jun Kitagawa, Quentin Merigot, and Boris Thibert. Convergence of a newton algorithm for semi-
discrete optimal transport. Journal of the European Mathematical Society, 21(9):2603-2651,
2019.
Aishik Konwer, Joseph Bae, Gagandeep Singh, Rishabh Gattu, Syed Ali, Jeremy Green, Tej Phatak,
Amit Gupta, Chao Chen, Joel Saltz, and Prateek Prasanna. Predicting COVID-19 lung infiltrate
progression on chest radiographs using spatio-temporal LSTM based encoder-decoder network.
In Medical Imaging with Deep Learning, 2021. URL https://openreview.net/forum?
id=96BhL_MERil.
Hugo Lavenant, Sebastian Claici, Edward Chien, and Justin Solomon. Dynamical optimal transport
on discrete surfaces. ACM Transactions on Graphics (TOG), 37(6):1-16, 2018.
Arthur Leclaire and Julien Rabin. A fast multi-layer approximation to semi-discrete optimal transport.
In International Conference on Scale Space and Variational Methods in Computer Vision, pp.
341-353. Springer, 2019.
Arthur Leclaire and Julien Rabin. A stochastic multi-layer algorithm for semi-discrete optimal
transport with applications to texture synthesis and style transfer. Journal of Mathematical Imaging
and Vision, pp. 1-27, 2020.
Bruno Levy. A numerical algorithm for 12 semi-discrete optimal transport in 3d. ESAIM: Mathemati-
cal Modelling and Numerical Analysis, 49(6):1693-1715, 2015.
Huidong Liu, Xianfeng Gu, and Dimitris Samaras. A two-step computation of the exact GAN
Wasserstein distance. In Proceedings of the International Conference on Machine Learning, 2018.
Huidong Liu, Xianfeng Gu, and Dimitris Samaras. Wasserstein gan with quadratic transport cost.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4832-4841,
2019.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188,
1989.
11
Under review as a conference paper at ICLR 2022
QUentin Merigot. A multiscale approach to optimal transport. In Computer Graphics Forum,
volume 30, pp.1583-1592. Wiley Online Library, 2011.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In Proceedings of the International Conference on Machine Learning, 2018.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends in
Machine Learning, 11(5-6):355-607, 2019.
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, pp. 1679-1706, 1994.
Jawad Rasheed, Alaa Ali Hameed, Chawki Djeddi, Akhtar Jamil, and Fadi Al-Turjman. A machine
learning-based framework for diagnosis of covid-19 from chest x-ray images. Interdisciplinary
Sciences: Computational Life Sciences, 13(1):103-117, 2021.
Hojjat Salehinejad, Shahrokh Valaee, Tim Dowdell, Errol Colak, and Joseph Barfett. Generalization
of deep neural networks for chest pathology classification in x-rays using generative adversarial
networks. In 2018 IEEE international conference on acoustics, speech and signal processing
(ICASSP), pp. 990-994. IEEE, 2018.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal
transport. In Proceedings of the International Conference on Learning Representations, 2018.
Bradley Segal, David M Rubin, Grace Rubin, and Adam Pantanowitz. Evaluating the clinical realism
of synthetic chest x-rays generated using progressively growing gans. SN Computer Science, 2(4):
1-17, 2021.
Justin Solomon and Amir Vaxman. Optimal transport-based polar interpolation of directional fields.
ACM Transactions on Graphics (TOG), 38(4):1-13, 2019.
Devansh Srivastav, Akansha Bajpai, and Prakash Srivastava. Improved classification for pneumonia
detection using transfer learning with gan based synthetic image augmentation. In 2021 11th
International Conference on Cloud Computing, Data Science & Engineering (Confluence), pp.
433-437. IEEE, 2021.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Cedric Villani. OPtimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Lei Zhou, Joseph Bae, Huidong Liu, Gagandeep Singh, Jeremy Green, Dimitris Samaras, and Prateek
Prasanna. Chest radiograph disentanglement for covid-19 outcome prediction. In International
Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 345-355.
Springer, 2021.
A Appendix
A. 1 The Maximum Relative Error
The Maximum Relative Error (MRE) between the transported discrete distribution p, and the target
distribution ν is defined as:
MRE(ν,p) = max ɪPi——Vii.	(6)
i∈I	νi
Fig. 5 shows the relationship between different MRE values and the quality of their respective SDOT
maps. The source distribution is a continuous uniform distribution in [0, 1] × [0, 1], shown in a)
and b), and the discrete target distribution has 5 points , shown in c), with each point having the
probability of 0.2. Fig. 5 a) and b) show the source space is divided by two SDOT maps. An optimal
SDOT algorithm should divide the source space into 5 regions with equal area. In a), the source space
12
Under review as a conference paper at ICLR 2022
c) Target distribution.
Figure 5: An illustration of the relationship between the MREs and the quality of SDOT maps. The
source distribution is a continuous uniform distribution in [0, 1] × [0, 1], shown in a) and b), and the
discrete target distribution has 5 points , shown in c), with each point having the probability of 0.2.
Fig. 5 a) and b) show the source space is divided by two SDOT maps. An optimal SDOT algorithm
should divide the source space into 5 regions with equal area. In a), the source space is only divided
in 3 regions, and thus 2 target points do not have regions in the source space that are mapped to them.
The MRE in a) is 1.8 > 1. In b), the source space is divided into 5 regions, with each region having
approximately the same area. The MRE is 0.05 < 1. b) is much better than a). This shows an example
of using MRE to measure the qualities of SDOT maps.
is only divided in 3 regions, and thus 2 target points do not have regions in the source space that are
mapped to them. The MRE in a) is 1.8 > 1. In b), the source space is divided into 5 regions, with each
region having approximately the same area. The MRE is 0.05 < 1. b) is much better than a). This
shows an example of using MRE to measure the quality of SDOT maps.
A.2 Proofs of Theorems
Next we introduce Theorem 4 in Chan et al. (2014); Diakonikolas (2014) and propose Lemma 1 that
is important to prove Theorem 1.
Theorem 4. (Chan et al., 2014; Diakonikolas, 2014) Let P be a distribution over [n]. Let P be an
empirical distribution over n obtained by drawing K samples from p, then
1)
E[kP - pkι] ≤ 2√n∕K	(7)
and
2)	for each i ∈ [n], we have
E[∣pi - Pi|] ≤ pPi(1 - Pi)/K	(8)
Lemma 1. Let P be a distribution over [n]. Let P be an empirical distribution over n obtained by
drawing K samples from P. Then with probability of at least 1 - δ, 0 < δ < 1, each of the following
holds
1)
kP -Pki ≤ 2√n∕K + √2log(1∕δ)∕K	(9)
2)	for each i ∈ [n],
IPi - Pi| ≤ κpPi(ι - Pi)∕κ	(10)
where K = 1 / Vδ.
Proof. 1) TheProofismotivatedbytheProofinLemma3.1AcharyaetaL (2015). Let Y = kP — Pk「
We can write Y = g(si,…,SK), where g is a function, Sj ~ P and samples Sj,j ∈ [K ], are mutually
indePendent. As the L1 distance between any two Probability measures are uPPer bounded by 2, the
function g satisfies the following LiPschitz ProPerty:
2
Ig(SI,…,sj ,…,sK ) - g(s1,..,sj,…,sK )∣ ≤ V7
K
for any j ∈ [K]. Using McDiarmid,s inequality (McDiarmid, 1989), we have P(Y > E[Y] + η) ≤
exp(-η2K∕2),∀η > 0. Therefore, P(Y > 2^/K + η) ≤ P(Y > E[Y] + η) ≤ exp(-η2K∕2).
Let δ = exp(-η2K∕2), we have Eq. (9).
13
Under review as a conference paper at ICLR 2022
2)	Each i in Eq. (10) can be proved independently. For a specific i, let us consider Poisson trials
Xj, j = 1, ..., K, where each Xj is a Bernoulli random variable with success probability being pi,
i.e., P(Xj = 1) = pi and P(Xj = 0) = 1 - pi, ∀j = 1, ..., K. Therefore, the expectation of each
Xj is E(Xj) = pi, and the variance is V ar(Xj) = pi(1 - pi). P(Xj = 1) = pi means that, in
the j-th Poisson trial, the probability of drawing the j-th sample falls in the i-th histogram bin is
Pi. Let X = κ^ PK=I Xj, and thus, X follows a Poisson binomial distribution. For the Poission
binomial distribution, E(X) = Pi and Var(X) = pi(1 - Pi)∕K. So, the standard deviation of X is
σ = y/pi(T-P)/K. Eq. (10) is to bound P(|X - p/ ≤ κσ). Using the Chebyshev bound We have
P(|X - pi| ≤ κσ) ≥ 1 -表.Let δ =去 we obtainEq. (10).	□
A.2.1 Proof of Theorem 1
We restate Theorem 1 below:
Theorem 1. Suppose Assumption 1 is satisfied, g^ven a continuous source distribution μ, a discrete
target distribution V, target data points Y = {yi ∈ Rd}n=1, SDOT output V, a precision g and a
2
confidence 1 一 δ, 0 < δ < 1, let ξ = 4(√1+^+甲,and let P be the discrete transported distribution
using v. Draw d1∕(4δξνmin)] i.i.d. samples from μ and compute the empirical discrete transported
distribution P using V. Let g = P — V be the gradient and g = P — V be the empirical gradient of
def
the SDOTobjective at V. Let d = MRE(V,p) = maxn=ι{∣Pi — ν∕∕/} be the empirical MRE, dɪ =
kP - ν∣∣ 1 be the empirical Li distance between P and V. Let ∆ = 4√δξnνmin + ,8δ log(1∕δ)ξνmin,
,ξ2 + dξ + ξ.
and ω
Then, with probability of at least 1 - δ, each of the following holds:
1) the lower bound of MRE(V, P) is lb = max(d - 2ω + 2ξ, 0).
2)	the upper bound of MRE(V, P) is ub = min(d + 2ω + 2ξ, (1 - Vmin)∕Vmin).
3)	the Li	distance	between P andP is bounded as kP 一 p∣∣i ≤ ∆.
4)	the Li	distance	between g and g is bounded as ∣∣g 一 gk i ≤ ∆.
5)	the Li	distance	between P and V is bounded as max(di - ∆, 0) ≤	kP	-	Vki ≤ min(di + ∆, 2).
Proof. We prove 1) and 2) together. The trivial lower bound in 1) is 0 and the trivial upper bound in
2) is (1 - Vmin)/Vmin. Let K = 1/√7. Suppose we draw dn∕e∖ samples from the source distribution
μ, and use V to compute the empirical transported distribution P. Without loss of generality, suppose
d is achieved at dimension j, i.e.,
d=	∣Pj	-	Vj ∣∕Vj ≥	∣Pk	-	Vk ∖∕v,	∀k ∈	[n],	k = j	(11)
Let d* = MRE(V,p) = maxn=i{∣Pi - Vi∖∕Vi}. Without loss of generality, suppose d* is achieved at
dimension i, i.e.,
d* = ∖Pi - Vi∖∕Vi ≥ ∖Pk - Vk∖∕Vk,	∀k ∈ [n], k 6= i	(12)
Let Vmin denote the minimum value in V. Hence,
d* = ∖Pi - Vi∖∕Vi
∣Pi -Pil , Pi
----------+ 一
Vi	Vi
+ d with probability of at least 1 - δ, by applying Lemma 1 and Eq. (11)
(13)
≤
KVZEPi∕n
Vi
Therefore, with probability of at least 1 - δ, we have
(14)
14
Under review as a conference paper at ICLR 2022
To find the upper bound d*, We can find the upper bound of JIi. Let a = K Jnj and X = Jli.
According to Eq. (14), We have |x2 - 1| ≤ ax + d. If x ≥ 1, the right hand side gets larger values
than the case x < 1. Since We are computing the upper bound, We just need to consider the case
x ≥ 1. We need to consider the maximum x that satisfies
(15)
2
x2 - 1 ≤ ax + d
The maximum x that satisfies the above inequality is
	a + λ∕a2 + 4d + 4 	2	 (16)
Therefore, d	* has an upper bound of ʌ	pa4 + 4da2 + 4a2 + a2 d +	2	 (17)
for any a > 0. If d < g,to ensure the precision g can be achieved, we want to choose an E such that
the upper bound is less than or equal to g, i.e.,
	a	,a4 + 4da2 + 4a2 + a2 d +	2	 ≤ E*	(18)
_	∙	一	_	一一.一	ʌ	C
To satisfy Eq. (18), let d = βg,
we have
0 < β < 1. Substituting d by fg and a by KJ --- in Eq. (18),
nνmin
(19)
To get the lower bound,
E ≤ ( 1-β Y	婷
nVmin —k K J 1 + E*
(20)
ʌ
≥ d —
with probability of at least 1-
δ, by applying Lemma 1
Therefore, with probability of at least 1 — δ, we have
d* ≥ Pj - 1 ≥ d ― ^/ɪ.后
νj	nνmin	νj
(21)
To find the lower bound of d*, we can find the minimum |pi/% — 1| that satisfies Eq. (21). Let
y = ^j. According to Eq. (21), we have |y2 — 1| ≥ d — ay. To analyze the minimum value of
|y2 — 1|, we need to analyze two cases: i) d < a, ii) d ≥ a.
i) d < a.
There exists an yo = 1 and |y0 — 1| ≥ d — ayo holds. Therefore, the minimum value of |y2 — 1| is 0
in the case d < a. Thus, the lower bound of d* is 0 in this case.
… ʌ
ii) d ≥ a.
15
Under review as a conference paper at ICLR 2022
In this case, we consider y ≥ 1 and y < 1.
For y ≥ 1, we consider the following inequality:
y2 - 1 ≥ d- ay	(22)
Let
—a + p∕a2 + 4d + 4
y≥ι =-----------2--------- (23)
Since d ≥ a, we can derive from Eq. (23) and obtain y≥1 greater than or equal to 1. It can be verfied
that y≥2 1 - 1 = d - ay≥1, and y≥1 is the minimum y that is ≥ 1 and satisfies Eq. (22). Therefore,
y≥ι - 1 is the lower bound of d for y ≥ 1 in the case d ≥ a.
For y < 1, we consider the following inequality:
1 - y2 ≥ d-ay	(24)
If there is no y that satisfies Eq. (24), we just need to consider y ≥ 1 for the case d ≥ a. Otherwise,
denote the maximum y that is less than 1 and satisfies Eq. (24) by y<1. So, we have 1 - y<2 1 =
d - ay<ι, Since d - ay<ι > d - ay≥ι, the lower bound of d* is always achieved when y ≥ 1 for the
case d ≥ a. Therefore, the lower bound of d* in the case d ≥ a is d - ay≥ι which is equivalent to:
a ʌ/a4 + 4da2 + 4a2 — a2
d--------------------------
(25)
It is easy to verify that if d < a then Eq. (25) is less than 0, and vice versa. Therefore, we can
combine case i) d < a, and case ii) d ≥ a as:
a a	ʌ/a4 + 4da2 + 4a2 — a
max d-----------------------
2,0
(26)
Eq. (26) is the lower bound of d*. To make the lower bound non-trivial, i.e. Eq. (25) ≥ 0, we want to
choose an E that achieves d ≥ a. Substituting a by KJ
nνmin
and d by βE*, we obtain
，≤ (匹 Y
nνmin	κ
(27)
We should choose an that is large enough for efficiency consideration and at the same time satisfies
both Eq. (19) and Eq. (27). This leads to optimizing the following problem
max min ((3 Y 上,与
β	κ	1 + E* κ2
(28)
The solution to the above problem is
(29)
Therefore, the optimal is
2*
e = 2 ( Γλ—---- , -∣ \2 . nνmin
κ2( 1 + E* + 1)2
(30)
0
0
0
0
0
0
0
0
β
2
2
1

1+ + €* + 1
and the optimal a is
*
a =—,	---
1+ + E* + 1
(31)
e^
*
Let ξ
4(√1+e* + 1)2
Thus, ξ = a2/4 and E = 4δξnνmin. Substituting a2 in Eq. (25) by 4ξ and
considering the trivial lower bound 0, we obtain the lower bound in 1). Substituting a2 in Eq. (17) by
4ξ and considering the trivial upper bound (1 - Vmin)/νmin, We obtain the upper bound in 2)..
16
Under review as a conference paper at ICLR 2022
3)	As We draw the samples from μ, We are essentially drawing samples from the real target distribution
p achieved by v. The number of samples is dn/e. Applying Lemma 1 1), we obtain the desired
bound Ofkp - pkι.
4)	kg — gkι = k(P — V) - (p - V)kι = kp - pg ≤ ∆ by applying 3).
5)	∣∣p - vkι = ∣∣p -p + P - vkι. Using the triangle inequality on the upper bound we have
∣∣p - v∣∣ι ≤ ∣∣p -p∣ι + ∣p - v∣ι ≤ di + ∆. As P and V are two probability measures,
kp - v k1 is upper bounded by 2. Using the triangle inequality on the lower bound we have
∣∣p - v∣∣ι ≥ ∣p - v∣ι - ∣∣p -p∣ι ≥ di - ∆. As any norm is greater than or equal to 0, we
have the desired lower bound.	□
A.2.2 Proof of Theorem 2
Before proving Theorem 2, it is necessary to give the bound of the distance between the initial
SDOT output vo, and the optimal SDOT solution v*. The bound of the distance will be used in the
convergence analysis in Theorem 2 and 3. The following theorem shows the range of an optimal
SDOT solution v*.
Theorem 5. There exists at least one optimal solution, denoted by v*, in [-C∞∕2, C∞∕2]n, that
maximizes Eq. (2).
Proof. Maximizing Eq. (2) is equivalent to solving the following problem:
Problem 2. Given two bounded domains X and Y = {yi}n=ι and their probability measures
μ ∈ P(X), V ∈ P(Y), respectively, find afunction U and a vector V to SOlve
max u,v	W(u,v) = — I u(x)dμ(x) + ɪ2 v(i)vi
s. t.	i∈I	(32) - u(x) + v(i) ≤ c(x, yi) ∀x ∈ X, Nyi ∈ Y
If (u* , v* ) is the optimal solution to Problem 2, then ∀r ∈ R, (u* - r, v* - r) is also an optimal
solution to Problem 2, because (u* - r, v* - r) does not change the Laguerre cell decomposition
computed by (u*,v*). Let r be the minimum value in (u*,v*), and let u* - u* - r and v* - v* - r.
So, the minimum value in (u*, v*) is 0.
Next, we show that
inf v*,i ≥ inf u*(x)	and sup v*,i ≥ sup u*(x)	(33)
i∈I	x∈X	i∈I	x∈X
We prove Eq. (33) by contradiction. If infi∈I v*,i < infx∈X u*(x). Without loss of generality,
let v*,k < inf x∈X u*(x). We can construct a new v**, in which v**,i = v*,i, i 6= k and v**,k =
inf x∈X u*(x). The new solution (u*, v**) does not violate the constraints in Eq. (32), and increases
the objective. Therefore, (u* , v* ) is not the optimal solution to Problem 2 which is a contradiction.
Thus, inf i∈I v*,i ≥ inf x∈X u*(x). supi∈I v*,i ≥ supx∈X u*(x) can be proved in a similar way (by
decreasing the maximum value in u*(x) to the maximum value in v*). Recall the minimum value in
(u*, v*) equals 0, we have inf x∈X u*(x) = 0. Recall the constraint -u(x) + v(i) ≤ c(x, yi), ∀x ∈
X, ∀yi ∈ Y, we have supi∈I v*,i ≤ C∞, where C∞ is the maximum transport cost. Therefore,
every element in (u*, v*) lies between 0 and C∞. Let u*** = u* - C∞∕2 and v*** = v* - C∞∕2.
Thus, v*** is an optimal solution that maximizes Eq. (2) and v*** is in [-C∞∕2, C∞∕2]n.
□
Let v0 be 0, and v* be the optimal solution that maximizes Eq. (2) and lies in [-C∞∕2, C∞∕2]n.
Then,
Ilvo - v* Il ≤ √nC∞	(34)
Theorem 2 is to establish the optimal step size γ given a fixed number of iterations T . Before proving
Theorem 2, we introduce the following Proposition in Bach (2014).
17
Under review as a conference paper at ICLR 2022
Proposition 1. (Bach, 2014) Assume 1) f is convex and three-times differentiable, 2) f has a global
minimum attained at v* ∈ V,3) all gradients of f and ft, are bounded by R i.e., kf 0(v) ∣∣ ≤ R, and
kft0 (vt-1)k ≤ R almost surely. 4) ∀t ≥ 1, ft is F -measurable, 5) E[ft0(vt-1)|Ft-1] = f0(vt-1) and
6) Vt = vt-ι 一 Ytft(Vt-ι), where (γt)t≥ι is a deterministic sequence. Let V = T PT=I vt-ι. With
constant step size equal to γ, for any T ≥ 0, we have
E[2γT(f(v)- f(v*)) + ∣vt - v*k2] ≤kvo-v*k2 + Tγ2R2	(35)
and	E [2γT(f(v)- f(v*)) + ∣vt - v*k2]2 ≤ (∣∣V0- v*∣2 +9Tγ2R2)2	(36)
We restate Theorem 2 below:
Theorem 2. With the constant step size
√nc∞ + ι
γ =------——
12 √T
the expectation of the gradient norm offunction f at v is bounded by:
(37)
4
Ekf0(v)k2 ≤ T
+ io + 6√nc∞)
(38)
The proof of Theorem 3 is similar to the proof proposed in Bach (2014), but uses a tighter bound
introduced in the Proposition 1.
Proof. First, We bound T PT=If'(vt-ι).
For fixed step size update vt = vt-1 一 γft0(vt-1), we have
f；(Vt-I) = I(Vt-I-Vt)	(39)
γ
Summing from 1 to the total number of iterations T, we get
TT
1	1	11
T ɪ^f (Vt-I) = T ɪ^[f (Vt-I) — ft(Vt-I)] + YT(VO — Vj + YT(V* — vT)	(4O)
Similar to Bach (2014), we apply the Burkholder-Rosenthal-Pinelis inequality (Pinelis, 1994) [Theo-
rem 4.1] to bound
■ T	∣∣2^l 1/2
m 1 J …	0	…	、」|	4R	2√2R
E TΣS[f (VtT) 一ft(VtT)]|| I ≤ -T + -√T
(41)
T Xf0(VtT)- f0(vt-1)] |
|1T
E T Ef(Vt-1)
| t=1
E
Using Proposition 1 and Minkowski’s inequality:
2 ] 1/2
)≤
+ YT∣V0-v*k + γT [E∣VT - V*k2]1∕2
≤ 4R + 2√TR + γTkvo - v*k + γTP∣V0-v*k2 + Tγ2R2
4R 2√2R 1	kvo - v*k	√TγR
≤ 亍 + F + YT kvo-v*k + ~γr~ + -TT-
4R	4R	2kV0 - V*k
≤ 亍 + √t + —YT—
(42)
18
Under review as a conference paper at ICLR 2022
Next, we bound T PT=1 f 0(vt-ι) - f 0(T PT=1 VtT)
By using the self-concordance property in BaCh (2014) [D.2], We have ∣∣ TT PT=I f0(vt-1)-
f0(T PT=1 Vt-i)k ≤ 2R(1 P：=1 f (vt-1) - f (v*)).
Using Proposition 1, We have
1T	1T
E τ X f0(Vt-I)- f0( τ Xvt-1
t=1	t=1
N]"	/「 T	∏2∖ 1/2
≤ 2R(E T X f(vt-1)- f(v*))
R
≤ γτ (kv0 - v*k + 9Tγ R )
(43)
Summing Eq. (42) and Eq. (43), We get
Ef 0 (T x vt-1
t=1
2 1/2
≤
4R 4R 2∣vo — v*∣
亍 + √T + 一γT―
+ RkvO -v*k2 +9γR3
R [吃 +4 + 9R2γ√T + (kv0-vU/R)2
T T	γT
(44)
≤
If
kvo — v* k + 1/R
3R√T
(45)
the upper bound of E ∣∣f 0 (T PT=I vt-1^ ∣∣	is minimized. For optimal transport, the gradient
is the difference betWeen tWo distributions. Thus, the gradient norm is upper bounded by 2. Therefore,
We choose R = 2. Substituting kv0 - v* k in Eq. (45) by its upper bound in Eq. (34), We get
√nC∞ + 1
12√T
(46)
Take γ, R and the upper bound of kv0 - v* k into Eq. (44), We obtain Eq. (38).
□
A.2.3 Proof of Theorem 3
Theorem 3 establishes an optimal total number of iterations T and the corresponding optimal step
size γ, such that We can achieve an expected value of the MRE for any given > 0.
Theorem 3. Suppose Assumption 1 is satisfied, ∀ > 0, with constant step size
EVmin	(1 + √nC∞)
Y = ^F' (i4 + 6√nc∞)
(47)
and number of iterations
T
4(14 + 6√nc∞)2
e2ν2in
we have
E [MRE (ν, p)] ≤ E
(48)
(49)
19
Under review as a conference paper at ICLR 2022
	1D toy data	2D toy data	256D toy data	256D real face data
MC-Adam	1e-5	1e-4	1e-2	1e-4
Vanilla-EGD	1e-2	1e0	1e+2	1e+3
Table 2: Parameter settings of MC-Adam and Vanilla-EGD for different data.
Proof. Let P be the probability density function achieved by v.
E[MRE(ν,p)] = E max |pi----
i∈I	νi
≤ E La Jpi- ViI -
≤	ιmax
i∈I	νmin
=E jkf0(v)k∞^
νmin
≤ E j f≡ ]
νmin
≤ —pE∣f 0(v)k2	Jensen,s inequality
νmin
(50)
Therefore, if Ekf0(v)k2 ≤ 饪沙*也，then We have E[MRE(ν,p)] ≤ e. According to Eq. (38), We have
EllfO(V)II2 ≤ τ (√t + 10 + 6√nc∞)
4 , 一、2	(51)
≤ τ (14 + 6√nC∞)
Let
4 /	L 一 、2 CC
T (l4 + 6√nc∞) = e2ν2lm	(52)
We obtain Eq. (48). Combining Eq. (37) and Eq. (48), We obtain Eq. (47). With γ in Eq. (47) and
T in Eq. (48), We have E[MRE(ν, p)] ≤ . Hence, We ensure the convergence of MRE for any given
e.	□
A.3 Experiments
In the experiments, we compare SDOT-EGD against ASGD (Peyre et al., 2019; Aude et al., 2016),
2-layer ASGD (Leclaire & Rabin, 2020), MC-Adam (An et al., 2020a) and Vanilla-EGD (Hazan &
Kale, 2014). MC-Adam uses the Monte-Carlo sampling to estimate the gradient, and uses the Adam
optimizer to optimize the SDOT dual objective. The Vanilla-EGD uses an epoch gradient descent
strategy With a fixed number of iterations in each epoch, and decreases the learning rate by half in
each epoch.
A.3.1 Experimental Settings
In all the experiments, we set g = 0.2 and the confidence 1 一 δ to 0.9 in SDOT-EGD, except in the
ablation study part. In all the experiments, We use the quadratic transport cost for all the methods, i.e.,
c(x, y) = lx - yl2. For ASGD, 2-layer ASGD, we use the default parameter settings, because the
initial learning rate 1.0 is widely used in the literature (Leclaire & Rabin, 2019; 2020). we tuned the
learning rate for the Adam optimizer in MC-Adam and the initial learning rate for Vanilla-EGD. The
learning rate for Adam in MC-Adam is tuned in {1e-5, 1e-4, 1e-3, 1e-2, 5e-2, 1e-1} and the initial
learning rate for Vanilla-EGD is tuned in {1e-3, 1e-2, 1e-1, 1e0, 1e+1, 1e+2, 1e+3}. We list the best
learning rates for both methods in Table 2.
In the GAN experiments, we use the same network architecture as in Mescheder et al. (2018) for
256 × 256 size images with the maximum number of filters in each layer set to 512 for all the methods.
We train all the models for 400K iterations with a batch size of 16 on the CelebA-HQ and the COVOC
datasets. We use the RMSProp optimizer (Tieleman & Hinton, 2012) with a learning rate of 1e-4 for
all models. We use the uniform distribution in [-1, 1]256 as the simple distribution to train all the
20
Under review as a conference paper at ICLR 2022
tOdSUefc」0*=e Oanos
0.6
0.5
W 0.4
a
φ
σι
0 0.3
U
'.υ
I
α) ∩ ɔ
q 0.2
Φ
o
U
æ
W 0.1
δ
δ
6
E 0
------MC-Adam
Vanilla EGD
------ASGD
------2-layer ASGD
------SDOT-EGD
500
1000	1500	2000	2500	3000
Iterations (x10000)
Figure 6:	The Kolmogorov distance for different methods.
GANs. For WGAN-GP (SDOT-EGD) and GAN-0GP (SDOT-EGD), we use the method described in
Sec. 5.3 in the main paper to obtain the 256D feature for each image. We compute the SDOT map
using SDOT-EGD to map each noise point in [-1, 1]256 to a 256D image feature. As each 256D
image feature is extracted from each image, we have the matching between each noise point and
each real image. This matching is fixed and is used throughout the WGAN-GP (SDOT-EGD) and
GAN-0GP (SDOT-EGD) training process. The procedure of how we train a GAN with SDOT-EGD
is listed below:
1	Given a dataset of images, we compute the features of these images.
2	We compute the SDOT map using SDOT-EGD between a uniform distribution and the
features in the latent space.
3	Suppose batch size is m in GAN training. We randomly sample m noise points from the
uniform distribution. Using the SDOT map, we find the corresponding m image features.
Using these m image features, we find the corresponding m images. These m noise points
and the m corresponding images form a batch.
4	We use the batch of m noise points and the m images to compute the discriminator and
generator losses in a GAN, and then update the discriminator and generator weights.
5	Go to Step 3 if the training is not converged.
All the experiments are executed on a cluster of machines. Each machine has 4 Intel Xeon CPUs and
128GB RAM. All the GAN experiments are executed on Quadro RTX 8000 GPUs.
A.3.2 1D Experiments
In the 1D experiments, we use the same data as used in the main paper. Similar to Leclaire & Rabin
(2020), we adopt the Kolmogorov distance3 between target and the transported distribution to evaluate
the performance of different methods. Fig. 6 plots the Kolmogorov distances of the various methods.
Compared to other methods, SDOT-EGD converges much faster. It also converges to a much lower
value, almost 0. The 2-layer ASGD decreases faster than MC-Adam, ASGD and Vanilla-EGD, but
remains constant at around 0.01.
To investigate the reproducibility of SDOT-EGD, we rerun SDOT-EGD 10 times and plot the mean
MREs and the standard deviations in Fig. 7 a). As the MRE decreases, the standard deviation decreases,
meaning SDOT-EGD becomes more and more stable. When SDOT-EGD terminates, the standard
deviation is very small.
3The Kolmogorov distance is the L∞ distance between two cumulative distribution functions.
21
Under review as a conference paper at ICLR 2022
Number of iterations/million	5	10	14
ASGD	0.333026	0.333480	0.333549
MC-Adam	0.004680	0.008253	0.011736
Vanilla-EGD	0.333501	0.333580	0.333591
SDOT-EGD	0.333659	0.333667	0.333667
Table 3: The SDOT dual objectives of different methods w.r.t number of iterations. Ground truth is
0.333667.
Time/seconds	250	500	700
ASGD	0.333314	0.333563	0.333610
MC-Adam	0.023410	0.051604	0.066314
Vanilla-EGD	0.333444	0.333551	0.333580
SDOT-EGD	0.333659	0.333667	0.333667
Table 4: The SDOT dual objectives of different methods w.r.t time. Ground truth is 0.333667.
We provide a benchmark in the 1D case. In the 1D case, the Ground Truth (GT) SDOT dual objective
value is 0.333667. In the Tables 3 and 4, we report the SDOT dual objective values of different
methods w.r.t. number of iterations and time, respectively. The 2-layer ASGD is not included because
its objective is neither an SDOT primal nor dual objective. In both tables, SDOT-EGD achieves the
ground truth of 0.333667 faster than all the other methods.
A.3.3 2D Experiments
To investigate the reproducibility of SDOT-EGD, we plot the error bars of SDOT-EGD using 2D
data. The source distribution μ is uniformly distributed in [0,1] X [0,1]. We randomly sample 1000
points in [0, 1] × [0, 1] as target points, with each point having probability value of 1e-3. We rerun
SDOT-EGD 10 times and plot the mean estimated MREs and the standard deviations in Fig. 7 b). As
we can see in this figure, SDOT-EGD has very small standard deviations across multiple runs. Fig. 7
a) and b) together show that SDOT-EGD is a stable approach.
To make it easier to assess the quality of the cell decomposition results, we use a 40 × 40 grid of
1600 points within [0, 1] × [0, 1] as target points shown in Fig. 8 f). We call it 2D grid data. They
are equally spaced horizontally and vertically. Each data point has a probability of 1/1600. So, an
optimal SDOT algorithm should divide the [0, 1] × [0, 1] space into a grid of 40 × 40 square cells.
Fig. 8 shows the cell decomposition results. Cells are colored per their area from small (blue) to large
(red). Cell decomposition from our method SDOT-EGD look more similar to a regular grid and is
more uniform compared to other methods. MC-Adam and ASGD have some cells that are obviously
large. The vanilla-EGD has small cells in the center and large cells in the corners. The 2-layer ASGD
has a high cell area variance and many cells are not square. The Vanilla-EGD and the ASGD have
the number of cells that are smaller than 1600, indicating that some target points do not have area
in the noise space that are mapped to them. MRE est and L1 est are estimated MRE and L1 distance
computed according to our theoretical bounds. The MRE est corresponds very well with the quality
of the Laguerre cell decomposition. SDOT-EGD has much lower MRE est and L1 est, suggesting it
is better than other methods.
A.3.4 256D Experiments
We show the performance of different methods for 256D toy data. The source distribution μ is
uniformly distributed in [0, 1]256. We randomly sample 1000 points in [0, 1]256, and assign the
probability value of 1/1000 to each point. Fig. 9 a) shows that SDOT-EGD terminated at around 2M
iterations achieving an MRE near 0, which is considerably faster than all the other methods. Fig. 9 b)
shows that SDOT-EGD converges to a much lower L1 distance efficiently compared to all the other
methods.
22
Under review as a conference paper at ICLR 2022
12
10
8
6
4
2
(山H≡)」0t山 θ≥-≡-θQ::EnE-XEIAI
200	400	600	800	1000	1200	1400
Iterations (x10000)
0
100
2.5
LU
Q≤
≥
二 2
o
Iii
(υ
W 1.5
⑥
Q≤
E
E
X
ra
≥
1
0.5
200	300	400	500	600	700	800	900
Iterations (x10000)
a)	b)
Figure 7:	The error bar plot for a) 1D data and b) 2D data. The caps mark the standard deviations.
A.3.5 Comparison In Terms of Running Time
We also compare the performance of different methods in terms of running time in Fig. 10 and 11.
From these figures we can observe that the proposed SDOT-EGD can reach lowest MREs or the L1
distances in shorter time compared to other methods. Note that the shaded area in Fig. 10 (a) is too
thin to be seen because the average upper bound and lower bound gap is smaller than 0.2.
It is worth mentioning that estimating the MRE is not expensive. It only involves sampling and matrix
computations, which can be sped up on GPUs. In our 256D real face data experiments, νmin =
3.3e - 5 and e* = 0.2. The MRE estimation process in SDOT-EGD only takes UP approximately 5%
of the total time.
A.3.6 Ablation S tudy
In this experiment, We investigate how different e* and confidence (1 - δ) affect the performance of
SDOT-EGD. We perform experiments using 2D data with 1000 target points.
Fig. 12 a) and b) show the performance of SDOT-EGD with e* fixed to 0.2 and various confidence
values. SDOT-EGD can always converge to 0.2 under different confidence values. When the
confidence is higher, SDOT-EGD requires a higher number of iterations. Fig. 12 c) and d) show
the performances of SDOT-EGD with the confidence fixed to 0.9 and various e*. SDOT-EGD can
always achieve a preset precision e* ranging from 0.1 to 0.9. The smaller e*, the higher the number
of iterations required. As SDOT-EGD can always converge under different parameter settings, SDOT-
EGD is not sensitive to parameter selection. In all the other experiments, we simply set e* to 0.2 and
the confidence 1 - δ to 0.9.
A.3.7 GAN Experiments
In this section, we show more generated images by different methods. We conduct experiments on
the CelebA-HQ dataset (Karras et al., 2018) and the COVID-19 Outcome (COVOC) dataset (Konwer
et al., 2021). The CelebA-HQ dataset (CC BY-NC 4.0 License) contains 29970 unique images. The
original image size is 1024 × 1024. In our experiments, similar to Liu et al. (2019), we resize the
images to 256 × 256 to train GANs. The COVOC dataset contains 248 COVID-19 X-ray images
acquired upon disease presentation. We also use 256 × 256 image size on the COVOC dataset to
train GANs. We use the code as used in Mescheder et al. (2018) (MIT License).
Even though the number of images of the COVOC dataset is small, it is still okay to test on this dataset.
We want to test how much SDOT can benefit GAN training when only a small number of training
images are available. Generating images on the X-ray image dataset could benefit downstream tasks
such as COVID-19 diagnosis (Rasheed et al., 2021), pneumonia diagnosis (Srivastav et al., 2021),
chest pathology classification (Salehinejad et al., 2018), and so on. Training a good GAN on this
23
Under review as a conference paper at ICLR 2022
a) MC-Adam
Number of cells: 1600
MRE est = 0.362
L1 est = 0.072
b) Vanilla-EGD
Number of cells: 1476
MRE est = 1.005
L1 est = 0.157
c) ASGD
Number of cells: 1596
MRE est = 1.002
L1 est = 0.055
d) 2-layer ASGD
Number of cells: 1600
MRE est = 0.863
L1 est = 0.131
e) SDOT-EGD
Number of cells: 1600
MRE est = 0.183
L1 est = 0.050
f) Target distribution.
A 40×40 grid of 1600 points.
Figure 8: Laguerre cell decomposition for a 40×40 grid of 1600 data points. Each data point
has probability of 1/1600. Cells are colored per their area from small (blue) to large (red). Cell
decomposition from our method SDOT-EGD look more similar to a regular grid and is more uniform
compared to other methods. MC-Adam and ASGD have some cells that are obviously large. The
vanilla-EGD has small cells in the center and large cells in the corners. The 2-layer ASGD high cell
area variance and many cells are not square. The Vanilla-EGD and the ASGD have the number of
cells that are smaller than 1600, indicating that some target points do not have area in the noise space
that are mapped to them. MRE est and L1 est are estimated MRE and L1 distance computed according
to our theoretical bounds. The MRE est corresponds very well with the quality of the Laguerre cell
decomposition. SDOT-EGD has much lower MRE est and L1 est, suggesting it is better than other
methods.
24
Under review as a conference paper at ICLR 2022
a) MRE
b) L1 distance
Figure 9: Comparison of MRE and the L1 distance for 256D toy data.
a) MRE for 1D toy data.
4 2 0 8 6 4
(w) .IDw a>e-aET-E-XraS
b) MRE for 2D toy data.
2 0 8 6 4
(w) .IDw a>e-aET-E-XraS
------MC-Adam
Vanilla EGD
------ASGD
2-layer ASGD
SDOT-EGD
100	200	300	400	500	600	700	800	900
Time (in seconds)
c) MRE for 2D grid data.
=odsue-∙laue a:unos pue -a6」e- Uaalaq aoueJS-P ∣.^∣
------MC-Adam
------Vanilla EGD
------ASGD
------2-layer ASGD
------SDOT-EGD
,4,3l
uodsue,la⅛le a。」=。S
------MC-Adam
Vanilla EGD
------ASGD
2-layer ASGD
SDOT-EGD
PUeωmsuaa⅛Maq a3uels-p
Time (in seconds)
Time (in seconds)
'4,'4,3,
uodsue,la⅛le a。」=。S
°s860-462o
PUepUaaMlaq aoueJS-P 二
------MC-Adam
Vanilla EGD
------ASGD
---- 2-layer ASGD
------SDOT-EGD
100	200	300	400	500	600	700	800	900
Time (in seconds)

d) L1 distance for 1D toy data. e) L1 distance for 2D toy data. f) L1 distance for 2D grid data.
Figure 10:	Comparison of MRE and L1 distance in 1D and 2D toy data, and 256D real face data in
terms of running time.
dataset is not easy because a lot of X-ray images look similar, and thus it is a fine-grained image
generation problem. FID is used because it is a commonly used measure to evaluate the quality of
the images generated by GANs (Heusel et al., 2017; Brock et al., 2019). Also, a thorough study was
done by Segal et al. (2021) showing the evidence that the FID could be used to measure the quality of
the chest X-ray images.
The only difference between WGAN-GP and WGAN-GP (SDOT-EGD) is that in WGAN-GP the
noise and real images are randomly matched in a GAN training iteration, but in WGAN-GP (SDOT-
EGD) the noise and the real images are matched using the SDOT matched GAN training mechanism
described in the main paper. The only difference between GAN-0GP and GAN-0GP (SDOT-EGD) is
the similar to the difference between WGAN-GP and WGAN-GP (SDOT-EGD).
25
Under review as a conference paper at ICLR 2022
(3≈w)」。」」山 θ≈s-θ止 EnuJ-XeW
a) MRE for 256D random data.
(山止≈) -lotwω≈s-ω3 UJi-XeW
35	40	45	50	55	60	65
Time (Hours)
；0.1
0 0.05
:0
MC-Adam
Vanilla EGD
------ASGD
--- 2-layer ASGD
-----SDOT-EGD
•5352
vooo-o
Time (in seconds)
500	1000	1500	2000	2500	3000
b) MRE for 256D real face data.
JJOdsue.lrge eo.lnos PUe®6」®UeeMφq eoueω-p -•1-
c) L1 distance for 256D random data. d) L1 distance for 256D real face data.
Figure 11:	Comparison of MRE and L1 distance in 256D toy data, and 256D real face data in terms of
running time.
Fig. 13 a) and b) show the randomly generated images by WGAN-GP, and WGAN-GP (SDOT-EGD),
respectively. WGAN-GP generates several faces with bad quality, whereas WGAN-GP (SDOT-EGD)
generates better faces compared to WGAN-GP. Fig. 14 a) and b) show the randomly generated faces
by GAN-0GP and GAN-0GP (SDOT-EGD), respectively. GAN-0GP (SDOT-EGD) generates much
better faces than GAN-0GP, with every face having good quality. Fig. 15 shows the interpolation
results between faces. Transitions between faces look realistic and smooth.
To show that ignoring samples in SDOT does lead to mode collapse in practice, we design a simple
experiment. We use eight real face images, shown in Fig. 16 a), as training images and compute an
SDOT map between the noise distribution and the eight 16 × 16 downsampled gray images using
SDOT-EGD. To mimic the situation that one target image does not have a source area that maps to it,
we took the dual variable v(0) for image 0 (The first image in Fig. 16 a)) to be -∞ such that image 0
does not have an area in the source domain that maps to it. We train GAN-0GP using this new SDOT
map. After the model is trained, we randomly generate 64 images, shown in Fig. 16 b). We checked
these 64 images and found that, for all other 7 images in the training set, we can find that there are
generated images that resemble them, and there is no generated image that resembles image 0. This
indicates that mode collapse happens. Note that due to a small number of training samples, 8 samples
in Fig. 16 a), a GAN is expected to be overfitting.
Fig. 17 shows the Covid-19 X-ray images randomly generated by WGAN-GP and WGAN-GP
(SDOT-EGD). The images generated by both methods appear comparable. Fig. 18 shows the
Covid-19 X-ray images randomly generated by GAN-0GP and GAN-0GP (SDOT-EGD). In Fig. 18
a), GAN-0GP generates several images with bad quality (row 2 column 3, and row 3 column 3).
GAN-0GP (SDOT-EGD) generates much better images compared to GAN-0GP.
Qualitative results on both face images and Covid-19 X-ray images show the effectiveness of using
SDOT to boost GAN training performance.
26
Under review as a conference paper at ICLR 2022
(山H≡)」。」」山 Θ≥JE-ΘQ::EnE-XEW
a) MRE with a fixed e* = 0.2
----confidence=0.50
----COnfidenCe=0.60
COnfidenCe=0.70
-------COnfidenCe=0.80
COnfidenCe=0.90
confidence=0.99
0.2
b) Li distance with a fixed e*
----COnfidenCe=0.50
----confidence=0.60
confidence=0.70
-------confidence=0.80
confidence=0.90
COnndenCe=0.99
WH≥)」。」」山 EnE-XEW
Iterations (x10000)
c) MRE with a fixed confidence = 0.9
Iterations (x10000)
d) L1 distance with a fixed confidence = 0.9
Figure 12: a) and b) show the performance of SDOT-EGD with e* fixed to 0.2, and various confidence
values. SDOT-EGD can always converge to 0.2 under different confidence values. When the confi-
dence is higher, SDOT-EGD requires a higher number of iterations. c) and d) show the performance
of SDOT-EGD with the confidence fixed to 0.9, and various e*. SDOT-EGD can always achieve
a preset precision e* ranging from 0.1 to 0.9. The smaller e*, the higher the number of iterations
required. As SDOT-EGD can always converge under different parameter settings, SDOT-EGD is
not sensitive to parameter selection. In all the other experiments, we simply set e* to 0.2 and the
confidence 1 - δ to 0.9.
=0.1
=0.2
=0.3
=0.4
=0S
=0.6
=0.7
=0.8
=0.9
27
Under review as a conference paper at ICLR 2022
a) WGAN-GP
Figure 13: Images randomly generated by a) WGAN-GP and b) WGAN-GP (SDOT-EGD). WGAN-
GP generates several faces with bad quality, whereas WGAN-GP (SDOT-EGD) generates better faces
compared to WGAN-GP.
28
Under review as a conference paper at ICLR 2022
a) GAN-0GP
b) GAN-0GP (SDOT-EGD)
Figure 14: Images randomly generated by a) GAN-0GP and b) GAN-0GP (SDOT-EGD). GAN-0GP
(SDOT-EGD) generates much better faces than GAN-0GP, with every face having good quality.
29
Under review as a conference paper at ICLR 2022
Figure 15: Face interpolation by GAN-0GP (SDOT-EGD). Transitions between faces appear good.
30
Under review as a conference paper at ICLR 2022
a) 8 training images.
b) Randomly sampled images.
Figure 16: A simple experiment showing mode collapse happens in a GAN with SDOT if there is an
image, the first image in a), that does not have any area in the noise space that maps to it in SDOT. In
b), there is no generated image that resembles the first image in a). Note that due to a small number
of training samples, 8 samples in a), a GAN is expected to be overfitting.
31
Under review as a conference paper at ICLR 2022
a) WGAN-GP
b) WGAN-GP (SDOT-EGD)
Figure 17: Images randomly generated by a) WGAN-GP and b) WGAN-GP (SDOT-EGD). The
images generated by both methods appear comparable.
32
Under review as a conference paper at ICLR 2022
a) GAN-0GP
b) GAN-0GP (SDOT-EGD)
Figure 18: Images randomly generated by a) GAN-0GP and b) GAN-0GP (SDOT-EGD). GAN-0GP
generates several images with bad quality (row 2 column 3, and row 3 column 3 in a)). GAN-0GP
(SDOT-EGD) generates much better images compared to GAN-0GP.
33
Under review as a conference paper at ICLR 2022
A.4 Discussion
In the SDOT GAN application part, we use the downsized images as features in the latent space.
This kind of features may not be optimal to represent the data manifold. Interestingly, even with
such a simple mapping, “downsampling + RGB to gray”, we achieved significantly better results
compared to GANs without using the SDOT matching mechanism. An alternative approach could
be to use an Auto-Encoder to compute the image features. How to find a better mapping for GAN
with SDOT-EGD to further improve the GAN training performance is very interesting future work. It
is worth noting that such a mapping is only used to find image features. It is well-accepted that the
images lie in a low-dimensional space, a.k.a. the latent space. Given a dataset of images, there exists
a mapping that maps the images to the latent space. Once the mapping is computed on the dataset, it
should be fixed and thus it is deterministic.
When using SDOT-EGD, the source domain X should be bounded such that C∞ is well-defined. In
practice, using a Gaussian as a continuous distribution as the source distribution for SDOT-EGD is
possible. We can truncate the Gaussian at 3 or 6 standard deviations from the mean according to the
precision we need. Using the truncated is a reasonable approximation to train GANs. For example, it
has been shown in BigGAN (Brock et al., 2019), that using a truncated Gaussian can generate high
fidelity images compared to using the original un-truncated Gaussian.
34