Under review as a conference paper at ICLR 2022
ON VALUE OPTIMIZATION IN CONSERVATIVE Q
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Conservatism, the act of underestimating an agent’s expected value estimates, has
demonstrated profound success in model-free, model-based, multi-task, safe and
other realms of offline Reinforcement Learning (RL). Recent work, on the other
hand, has noted that conservatism often hinders learning of behaviors. To that end,
the paper asks the question how does conservatism affect offline learning? The
proposed answer studies Conservative Q Learning in light of value function opti-
mization, approximate objectives that upper bound underestimations and behavior
cloning as auxilary regularization objective. Conservative agents implicitly steer
estimates away from the true value function, resulting in optimization objectives
with high condition numbers. Mitigating these issues requires an upper bounding
objective. These approximate upper bounds, however, impose a strong assumption
on a scaling matrix, a result which is only sparsely fulfilled. Driven by theoretical
observations, provision of an auxilary behavior cloning objective as variational
regularization to estimates results in accurate value estimation, well-conditioned
search spaces and expressive parameterizations. In an empirical study of discrete
and continuous control tasks, we validate our theoretical insights and demonstrate
the practical effects of learning underestimated value functions.
1	Introduction
Consider the scenario wherein an agent learns to drive a car. The agent observes a human driving the
car. This involves paying attention to crucial insights of controlling the vehicle such as steering while
making a turn, accelerating during a green light and monitoring mirrors during brakes. Increasing
amount of observations made available to the agent result in learning finer details of the task which
are not available apriori. For instance, the agent may never learn to make a U-turn if the human
never encountered a U-turn crossing.
Offline Reinforcement Learning (RL) (Kalashnikov et al., 2018; Levine et al., 2020) addresses this
intuitive gap in learning by equipping the driver (the agent in above example) with the ability to
stitch together portions of observations by making use of a dataset of transitions. For instance, the
driver may learn to make a U-turn on its own if it observes the teacher (human) making sharp turns
and slowing down the vehicle at intersections. Adoption of transitions in the offline setting allows
the agent to tackle distributional shift (Kumar et al., 2019) between current and behavior policies.
Recent interests in offline RL (Kumar et al., 2020b; Ajay et al., 2021) textcolorrespare motivated by
conservatively learning value estimates. Conservative Q values, in expectation, yield a lower bound
on the true Q values which facilitates in constraining an agent’s policy to be close to the dataset’s
behavior policy. Additionally, a conservative lower bound generalizes towards a family of objec-
tives which regularize the agent away from the path of Out-Of Distribution (OOD) actions. This
combination of data-driven learning with behavioral constraints demonstrates efficacious learning
with extensions to safe policy improvement. However, a concrete theory of lower bounded Q values
remains unexplored.
Conservative agents often face increased gaps between optimal values and lower-bounded estimates
(Kumar et al., 2021b). This leads the policy to overfit the behavior policy, hence hindering provision
of diverse behaviors. The nature of problem arises twofold. On one hand, conservatism bakes
in a policy distribution which is challenging to optimize in practical settings (Ajay et al., 2021).
On the other hand, conservatism imposes prior conditions on dataset design, a scenario effectively
addressed by alternate paradigms (Sinha et al., 2021).
1
Under review as a conference paper at ICLR 2022
Before asking the question how can conservatism be improved?, one must begin by asking how does
conservatism affect offline learning? The proposed answers realize conservatism in light of value
function optimization, upper bounding approximations and behavior cloning as regularization within
the Conservative Q Learning (CQL) (Kumar et al., 2020b) framework. Our novel contributions are
threefold; (1) We show that optimizing offline policies conservatively steers estimates away from
true Q functions. This gives rise to ill-conditioned objectives driven by high condition numbers.
The challenge is further pronounced with underparameterized function approximators. (2) Upper
bounding approximations, on the other hand, mitigate the above issues but hurt convergence. These
upper bounds impose a challenging assumption on a scaling matrix during policy optimization. (3)
We further show that provision of a behavior cloning objective as variational regularization empiri-
cally results in accurate value estimates, well-conditioned search spaces and expressive parameter-
izations. We validate our theoretical insights and their practical effects in discrete and continuous
control simulations.
2	Related Work
Offline Reinforcement Learning: A myriad of offline RL methods (Levine et al., 2020) provi-
sion learning of policies from a predefined set of behaviors (Wang et al., 2021; Agarwal et al.,
2020). Data-driven learning has demonstrated strides resulting from safe exploration (Bharadhwaj
et al., 2021; Eysenbach et al., 2018) to connection of prior skills with new experience (Singh et al.,
2020) in robotics. Additionally, provision of prior data collection facilitates scalability (Kalash-
nikov et al., 2018) to high-dimensional manipulation scenarios. Recent theoretical advances aim at
tackling the covariance shift (Gelada & Bellemare, 2019) arising from erroneous bootstrapping of
values. Towards this direction, prior efforts highlight the construction of robust off-policy optimiza-
tion techniques (Huang & Jiang, 2020). While off-policy methods (Liu et al., 2020a) highlight their
effectiveness for behavior primitives (Nair et al., 2020; Lange et al., 2012), regularization techniques
(Liu et al., 2020b) depict promise in the model-based (Kidambi et al., 2020) setting.
Pessimistic Learning: Various works develop penalty-based optimization methods for offline RL
(Ghasemipour et al., 2021; Fujimoto et al., 2019; Kumar et al., 2021b; Yu et al., 2021a). Although
centered towards the regularization of policy iteration scheme (massoud Farahmand et al., 2016),
penalties utilized to address distributional shift are instances of pessimistic updates (Jin et al., 2021;
Buckman et al., 2021). Distributional shift may be addressed using policy constraints in the policy
space (Lee et al., 2021) or action sequences in the action space (Zhou et al., 2020). Between these
two extremes is value function regularization (Kumar et al., 2020b; Yu et al., 2021b) which learns
in the policy space and regularizes in the value space. Penalization of Q values is closely related to
uncertainty estimation which does away with erroneous bootstrapping of estimates (Osband et al.,
2016; O’Donoghue et al., 2018). Our work explains conservatism based on the above techniques.
Optimization and Approximation Theory: Prior works in offline RL borrow from statistical opti-
mization (Wang et al., 2021). Analysis of value function regularization is often extended to primal-
dual spaces (Bharadhwaj et al., 2021). This provisions surrogate objective designs which provide
the construction of safe policy guarantees bounding sub-optimality and policy convergence. Alter-
natively, tractable probabilistic approximations (Wu et al., 2021) are employed to explicitly account
for epistemic uncertainty in OOD detection of state-action pairs. Additionally, convergence evalua-
tion (Yin et al., 2021b) and optimality (Xiao et al., 2021) analysis of off-policy optimization relies
on uncertainty in metric spaces. Our theoretical analysis is parallel to these prior efforts.
3	Problem Formulation
The problem utilizes the offline RL setup wherein an agent adheres to an environment in order to
transition to new states and observe rewards by following a sequence of actions. The problem is
modeled as a finite-horizon Markov Decision Process (MDP) (Sutton & Barto, 2018) defined by the
tuple (S, A, r, P, γ) where the state space is denoted by S and action space by A, r presents the
reward observed by agent such that r : S × A → [0, rmax], P : S × S × A → [0, ∞) presents the
unknown transition model consisting of the transition probability to the next state st+1 (or simply
s0) ∈ S given the current state st (or simply s) ∈ S and action at (or simply a) ∈ A at time
step t and Y is the discount factor. The agent's policy ∏ (a∕st) is a function of its parameters θ
and a behavior policy ∏β(at |st) with the discounted marginal state distribution dπβ (st). A dataset
D 〜 dπβ (st)∏β(at∣st) describes (s, a) pairs. Offline RL defines the agent,s objective to maximize
the expected discounted reward Es%at〜D[PT=ι YtTr(st, at)].
2
Under review as a conference paper at ICLR 2022
CQL (Kumar et al., 2020b) is an offline RL algorithm which addresses distributional shift by ob-
taining an expected lower bound on the Q-values. CQL regularizes the Bellman objective uti-
lizing expected Q values Es〜d,。〜∏β [Q(s, a)] under the behavior policy ∏β. To explicitly high-
light the per-sample dependence of Q values on states and actions, we use Q(s, a) and otherwise
denote the value function as simply Q (clear from context). The empirical Q-values of agent’s
policy, Qk, are underestimated at each policy iteration k utilizing the empirical Bellman operator
Bn Q
r(s, a) + γEso〜P(so∣s,a) [Q(s0, a0)] and a tradeoff constant a. The CQL objective, for a
regularization R(μ) = -Dkl(μ,ρ) as the KL-divergence with a prior ρ(a∣s) and μ(s, a) as the
state-action distribution, reduces to CQL(H) expressed in Eq. 1.
CQL(H) = minαEs〜D [lse(Q) - Ea〜∏β[Q(s,a)]] + 2Es,a,so〜D ](Q(s,a)-BπQk)]
`{~^}	`-------{-------} |
soft-maximum expected value
}
empirical Bellman error
(1)
Eq. 1 uses lse(Q) to denote log a exp Q(s, a) with Q as shorthand for Q(s, a). The second term
indicates a pessimistic expected value corresponding to in-disrtibution actions arising from ∏β and
the third term constitutes the empirical Bellman error. We overload notation by denoting H as the
Hessian of CQL(H) and other (explicitly stated) quantities in general.
4 Conservatism in Offline Learning
The proposed answer studies two facets of CQL(H) objective (see Appendix D and Appendix G
for conjugate and dual space analyses respectively). (1) Conservatism results in ill-conditioned
objectives. (2) Tractable approximations to improve the search process impose a strong assumption
during policy optmization. While the analysis is centered to the model-free setting, our results
readily transfer to model-based setting (see Appendix E). To further simplify intuition, suitable
theoretical results are denoted in lu , potential hindrances in red and key insights in boxes .
4.1	Conservative Value Function Optimization
Estimation errors arising from value estimates may hurt training. Prior works study this phenomenon
in light of value and policy-based methods (Van Hasselt et al., 2016; Fujimoto et al., 2018). However,
the effect is often omitted during evaluation of agent’s optimization process. For instance, a natural
question to ask is how do estimation errors affect the space of value functions? The answer to the
above lies in the assessment of value function space curvature. Consider the Hessian H of CQL(H)
in Eq. 2. Here, Softmax(Q) = P「*安；Qas))。))denotes the |A| dimensional softmax over Q and
∏β the |A| dimensional action probabilities from dataset. The first term, being a symmetric matrix
obtained from the outer product of softmax(Q), highlights the dependence of curvature on value
function. The second term denotes action likelihoods from dataset.
H = VQcql(H) = αSoftmax(Q)T(1 - Softmax(Q)) +	∏β	(2)
、---------------{----------------}	l{z}
curvature depends on	curvature depends
estimations	on dataset
To assess the curvature of H, one considers the condition number K = λmax which is the ratio of
largest to smallest eigenvalues of H. κ signifies the smoothness of H curvmatnure by informing how
close to singular H may be or how efficiently search methods will perform in the value space. High
κ values denote ill-conditioned curvatures with the lowest value of κ = 1 being desirable (Nocedal
& Wright, 2006). Upon evaluating the convergence of H as a convex quadratic (see Appendix A),
we obtain a lower bound of Eq. 3 on κ with estimation errors exponentiated around true Q function.
Explanation 1 (Ill-Conditioned Value Space) The conservative framework, for all Q val-
ues, traverses an ill-conditioned value space governed by high condition numbers K,
ksoftmax(Qk) — Softmax(Q) k2 + ∣∣softmax((Qk+1) — Softmax(Q) k 2
—————-——≡≡————-——≡≡ ≤ κ
11 Softmax(Qk) — Softmax(Q)∣2 — ∣∣Softmax(Qk+1) — Softmax(Q)∣2
(3)
Eq. 3 describes geometry of value function space. As per the design of conservative policy
evaluation (Kumar et al., 2020b), Q values Qk+1 at each subsequent iteration k + 1 are un-
derestimated. This results in the following case of Eq. 3 ∣∣Softmax((Qk) - Softmax(Q)∣2 <
3
Under review as a conference paper at ICLR 2022
ksoftmax(Qk+1) - Softmax(Q)∣∣2. The expression indicates increasing values of the lower bound
which shift κ towards higher values (see Appendix A). Intuitively, an increase in the estimation error
lead to ill-conditioned value function space which is found challenging to traverse.
4.2	Value Functions & Underparameterization
We further study the optimization process using parameterization. Recent work notes the phe-
nomenon of underparameterization, wherein value-based methods face a rapid decrease in the rank
of parameters (Kumar et al., 2021a). This decrease is studied in the function approximation setting.
A degradation in parameter rank corresponds to reduced expressivity of the value network. This is
because the effective number of bits required to express the Q function reduce with each iteration.
Other works connect this drop in expressivity to successive iterations of collapsing self-distillation
(Mobahi et al., 2020). While underparameterization is a well-known phenomenon in the deep RL
setting, its link to value optimization process remains unexplored.
We bridge this gap by connecting underparameterization with optimization. Denote t as the number
of rounds for which the rank of value function does not drop, prior work (Mobahi et al., 2020) notes
the relation ofEq. 4 (where ∣∣∏βk ＞，|D|e for constant e).
t = ———-,where K = gk is a constant	(4)
一 K	PW
Since |D| is fixed and ∣∣∏β∣ does not change during training, Eq. 4 emphasizes on the inverse
relationship t α K ∙ Intuitively, an increased value of t, i.e.- the number of steps for which the rank
does not drop, lead to smoothened curvatures of the parameter space. On the other hand, lower
values, i.e.- more frequent underparameterization, result in ill-conditioned search spaces. This leads
us to Explanation 2.
Explanation 2 (Ill-Conditioned Search Space) UnderParamterization in value-based func-
tion approximators inhibits expressivity and induces an ill-conditioned search space char-
acterized by increasing condition numbers K.
Rank degradation arises as a direct consequence of value prediction in Temporal Difference (TD)
learning (Kumar et al., 2021a). The degradation reduces values of t which leads to inaccurate value
estimates. This reduction in t relates to ill-conditioned search spaces with increasing K values over
the course of training. Our empirical evaluation verifies this trend in Section 5.
4.3 Value Functions & BOHNING Approximation
We further address high condition numbers by seeking higher-order alternatives using tractable ap-
proximations. This would provide the policy with increased curvature information. As a suitable
example, we study the well-known Bohning,s quadratic approximation (BOhning, 1992; Murphy,
2012) to lse(Q). Approximating lse allows use to address high variance and inaccurate estimates
during training (Khan, 2012). We seek an approximation which is an upper bound on CQL(H)
and simultaneously a tight lower bound on the true Q values. Towards this goal, consider an upper
bound on the hessian of CQL(H) using the second order Taylor’s series expansion of lse(Q) around
ψ ∈ R|A|,
lse(Q) = lse(ψ) + (Q - ψ)Tg(ψ) + 1(Q - ψ)TH(ψ)(Q - ψ)	(5)
where g(ψ) = exp(ψ-lse(ψ)) denotes the gradient of lse(ψ) andH(ψ) = diag(g(ψ)) -g(ψ)g(ψ)T
its hessian. Forming an upper bound on the hessian H(ψ) ≤ H such that XTHx ≤ XTHX for all x,
H=I In-M+11a10	⑹
Here H is the Bohning Approximation. Utilizing H in place of the true Hessian H(ψ) and SimPIify-
ing Eq. 5 leads to the following formulation,
lse(Q) ≤ 2QTHQ - bQ + C	(7)
4
Under review as a conference paper at ICLR 2022
The above quadratic expression is denoted With Bohn(Q) where b = -(Hψ - g(ψ)) and C =
1 ψTHψ — g(ψ)Tψ + lse(ψ). UPon assessing the convergence of Bohn(Q) (see Appendix B) one
obtains Eq. 8. On comparing this to Explanation 1, we observe that estimation errors are not on the
exponential scale anymore. This leads to an alternate lower bound on κ which empirically presents
improvement in search space.
kQk - Qk2 + kQk+1 - Qk2
八一	ʌ -
kQk- Qk2-kQk+1 - Qk2
≤κ
(8)
Since Bohn(Q) is an upper bound on lse(Q), lse(Q) ≤ Bohn(Q), plugging it into CQL(H) in Eq.
1	objective yields a suitable result CQLBohn(H).
CQLBohn(H) = min αEs〜D [	MQ)- Ea〜∏β [Q]] + ；Es,aW〜D (Q - BnQk)2	⑼
S----{----} S----{-----} S------------{-------------}
approximation expected value	empirical Bellman error
Similar to the original CQL(H) ob-
jective, CQLBohn(H) consists of an
expected value (second term in Eq.
9) and the empirical Bellman er-
ror (third term in Eq. 9). In
contrast to the high-variance soft-
maximum lse(Q), the objective en-
joys a tractable Bohn(Q) approxi-
mation. Fig. 1 (left) presents the
relationship between the two objec-
tives.
With that said, it is only left to
show that CQLBohn (H) establishes
a tight lower bound on true Q val-
ues. However, CQLBohn (H) does
not satisfy this result. The Bohning
Figure 1: (left) Relationship between CQL(H) and
CQLBohn(H) objectives, (right) Diagonal entries ofH as ba-
sis points with unit circle in red. Values in violet lie in over-
estimation regime. Values in golden lie in overconservatism
regime. Values (seldomly encountered) in blue are accurate.
approximation trades off a smoother objective for (1) a lower bound on true Q values and (2) the
key property of lse(Q) to be a contraction (Asadi & Littman, 2017). While we address the latter in
Appendix B showing that Bohn(Q) obeys contraction, the former imposes a strong and challenging
assumption which we discuss next.
We follow the process of CQL (Kumar et al., 2020b) and analyze Eq. 9 by optimizing over Q values
and setting the derivative to 0,
Q = H	BnQ — α(αH + ∏β(a|s))- (Hψ — Softmax(ψ) — ∏β(a∣s))	(10)
^Z ^{z^ S----------------{----------}s--------------y--------------}
scaling matrix estimate	effective tradeoff	underestimation
Eq. 10 provides a general result for Q values at each iteration.
rate which is determined by the entries of the matrix H =(
αH+^β (a|s)
expression consists of a scaling
|s) )-1 . Note the dependence
of iterates on behavior policy distribution ∏β(a|s) which suggests that a dataset with good coverage
may lead to improved convergence.
Assumption on H: We now consider the role of H in optimization process. In order for the policy
evaluation scheme to provide an accurate estimate, the first two terms when combined should yield
the Bellman estimate Bπ Qk . Mathematically, HBπ Qk = BπQk . Note that this result can only be
achieved if the matrix H is exactly equal to the identity H = I. The expression indicates that all
diagonal terms of H must equal to 1. This leads us to Explanation 3.
Explanation 3 (Strong Assumption on H) Smoother conservative objectives require the
scaling matrix H to be identity, H = I.
Fig. 1 (right) presents an intuition of the above result. If the diagonal entries of the matrix lie
inside the unit circle of the basis space (Hii < 1), then the Q values result in collapsing estimates.
5
Under review as a conference paper at ICLR 2022
a .	i i	, i ∙ ∙ , , ∙	, ∙ , trim- AGF	,	∙	ι	ι ii	,
At each subsequent policy iteration, estimates BπQk decay to minimum values and collapse at
the center of unit circle. In case the values lie outside the unit circle (Hii > 1), Q values will
result in overestimations and hence, diverge upon subsequent policy iterations. Thus, in order for
CQLBohn (H) to underestimate Q values, all diagonal entries of the matrix H should lie on the unit
circle (Hii = 1) with off-diagonal entries as 0.
4.4 Conservatism through the lens of Behavior Cloning
Provision of ill-conditioned search spaces and a strong assumption on H hurt convergence to the
true Q function. On one hand, ill-conditioned curvature hinders the optimization process of value
function. While on the other hand, CQLBohn (H) requires H = I.
Our analysis of the optimization process implicitly hints at a potential solution. To improve the
optimization process, one must seek low κ values which would address underparameterization. Ad-
ditionally, these search spaces must facilitate approximations which address distributional shift.
We address the former by following prior works in regularization. Iterative rank degredation may
be evaded utilizing regularization as an auxilary learning signal in the optimization process (Mobahi
et al., 2020). Explicit penalization of Q values fulfills two objectives; (1) the scheme presents an
auxilary objective which motivates the agent to maximize expected returns while matching behaviors
to the dataset. (2) The objective provides a richer learning signal in cases where rewards may
not be informative (Matusch et al., 2020). Based on this insight, we incorporate value function
regularization as a suitable tool for retaining expressive parameterization.
While a variety of regularizers emerge as suitable candidates, we seek the one which best approxi-
mates the dataset. An ideal objective should effectively capture prior transitions with minimum un-
realizable requirements. This would make policy’s optimization convenient. Towards this goal, we
explore the information theoretic alternative of Mutual Information MI{∏; ∏β} (Poole et al., 2019)
between agent and behavior policies ∏ and ∏β respectively. While MI{∏; ∏β} presents increased
expressivity of ∏β prior, the quantity is itself difficult to estimate. We alleviate this by deriving a
variational lower bound objective (Bishop, 2006) (see Appendix C) consisting of the tractable pos-
terior approximation q(∏∣∏β) and entropy regularization H(∏). Such a scheme resembles behavior
cloning with noisy dataset demonstrations (Rajeswaran et al., 2018). We theoretically show that the
scheme is -stable and converges to true Q faster than CQL(H) (see Appendix C).
MI{∏;∏β} ≥ Ep(∏,∏β)[iogq(∏l∏β)]+	H(∏)	(11)
'---------{----------}	z}
approximation	entropy regularization
Explanation 4 (Convergence with Smooth Objectives) Conservative offline learning objec-
tives, for all Q values, yield improved empirical convergence with behavior cloning regular-
ization serving as tighter approximations to Q values.
It is worth noting that naively incorporating the lower bound with returns as Intrinsic Motivation
(IM) may lead to instabilities (Xie et al., 2021). In addition, expected returns being estimated by the
agent are based on off-policy dataset transitions and must be utilized in a similar fashion. Thus, we
propose to use an off-policy variational regularization of Eq. 12 wherein Q values are regularized
following sampled trajectories. We direct the curious reader to Appendix C for the full derivation.
log (Y qt(n|ne)γt-1)]
Qk := Qk + Ep(∏,∏β)
(12)
Eq. 12 comprises of the per-timestep posterior qt(∏∣∏β) ranging up to the horizon length T. Note
the additional reduction in bootstrapping errors due to the exponentiated γt-1 in likelihood. The
posterior is implicitly downweighed over longer temporal spans which prevent compounding of
errors in estimates. The formulation, when combined with CQL(H), leads to the CQL-IM objective.
We validate the soundness of variational optimization and approximations in the next section.
5	Experiments
Motivated by theoretical evidence, our evaluation aims to study the practical effects of underestima-
tions when agents demonstrate significant conservatism. More specifically, our experiments aim at
addressing the following questions in discrete and control action settings;
6
Under review as a conference paper at ICLR 2022
(1)	How does the conservative framework of offline learning compare to the online setting?
(2)	How does underestimation in Q values affect the search space?
(3)	Does underparameterization of the value function hamper learning?
(4)	How much conservatism does CQL(H) demonstrate?
(5)	How does the condition on H imposed by Bohn(Q) explain CQL(H)?
(6)	How much efficacy does the variational approximation yield in practice?
(7)	Are the behaviors learned offline significant from a representational perspective?
5.1	The LineWorld Domain
Figure 2: (top) The Lineworld do-
main, (bottom-left) Average Re-
turns, (bottom-right) Average Re-
gret (results averaged over 100 runs)

We briefly visit the toy example of a linefollower agent in
the Lineworld domain from Kumar et al. (2019) presented in
Fig. 2 (top). The agent starts at the location S and is tasked
to reach the goal location G by consistently moving right as
its optimal policy π* (∙∣s). For each step in the right direc-
tion, the agent observes a reward of +1. For each step of
left action the agent observes a -1 reward resulting in the
termination of the episode. The total number of states cor-
respond to the number of steps between the start and goal
states. The setup consists of two settings, namely online and
offline agents. The online agent utilizes Q-learning and en-
joys its own data collected from the domain. The offline
agent utilizes CQL(H) and is initialized with a dataset of sub-
optimal transitions wherein the behavioral policy ∏β always
opposes the optimal policy π* (∙∣s) by going left. The offline agent is allowed to collect data at
selective intervals after the first 100 steps of simulation (see Appendix I for details).
The evaluation yields insights towards posed questions; (1) Fig. 2 (bottom-left) presents the com-
parison of average returns achieved by online and offline agents for 200 states domain. The online
agent, by virtue of data collection, learns from rich transitions observed in the environment. This
leads to faster and consistent learning. On the other hand, the offline agent learns slowly due to
the sub-optimal preferences of behavioral policy arising from the dataset. Once the agent starts
collecting data, it updates its estimates based on observed transitions and retrieves optimal behavior.
One can further study the scalability of problem by varying number of states in the environment.
Fig. 2 (bottom-right) presents the variation of average regret (calculated with respect to optimal
returns after 100 steps) with the number of states. The offline agent observes a faster accumulation
in regret in comparison to the online agent. Additionally, the difference between regrets of offline
and online agents widens with the former aggregating more regret over larger state spaces. Thus, the
offline learning problem additionally restricts the scalability of agents towards larger state spaces.
5.2	Aerial Control
The setting evaluates agents on a suite of aerial control tasks
simulating Drones (Panerati et al., 2021; Coumans, 2015)
wherein our agent is a CF2X quadcopter controlling the
torques applied to the 4 motor fins at 240 Hz (see Appendix
F for D4RL experiments). At each step, agent’s policy ob-
serves the drone’s position, velocity and orientation as a state.
Policies are evaluated on 4 diverse tasks; (1) takeoff requires
the policy to gradually ascend upward for flight, (2) hover re-
quires the policy to stay mid-air above a given location, (3)
zigzag requires the policy to manoeuvre in a zig-zag flight
pattern and (4) flythrugate, being a significantly challenging
scenario, requires the policy to escape through a narrow gate
opening. Fig. 3 provides an illustration of the tasks with de-
tails deferred to Appendix I.
Figure 3: Aerial tasks takeoff,
hover, zigzag and flythrugate il-
lustrated in their respective colors
moving away from the drone
7
Under review as a conference paper at ICLR 2022
Figure 4: Comparison of average log K
Figure 5: Comparison of the gap Qk - Q
Figure 6: (top) Variation of rank, (bottom) Vari-
ation of log(κ) during training.
We address the theoretical claims of our evaluation by comparing CQL(H) to Behavior Cloning
(BC) and Soft Actor-Critic (SAC) (Haarnoja et al., 2018). Additionally, CQLBohn(H) denotes CQL
with Bohn(Q). All offline policies were trained subject to a pretrained SAC as the behavior policy.
Fig. 4 compares average of condition numbers during training. The plot answers the second ques-
tion, (2) Underestimated Q values pose a challenging search space which is confirmed by high
κ values in log scale. Compared to CQL(H), CQLBohn(H) has a moderately suitable objective.
CQL-IM, on the other hand, proclaims a well-behaved objective reflective of low log(κ) values.
(3) We now verify our claim on underparameterization by studying its effect on the search space.
Following the setting of Kumar et al. (2021a), Fig. 6 (top) presents the evolution in rank of weight
matrix corresponding to last layer of the critic (see Appendix F for additional results). One readily
notices a steady drop for CQL(H) arising from conventional TD updates. CQL-IM, by virtue of
variational regularization, is found robust to the rank degradation phenomenon. Fig. 6 (bottom)
further connects expressivity to search space by observing the evolution of log(κ). Proggressive
degradation of rank drives increasing values of κ for CQL(H). CQL-IM remains robust to this
trend, hence presenting a well-conditioned search space.
Fig. 5 compares conservatism by virtue
of the gap Qk — Q (Kumar et al., 2020b)
wherein Qk are the values predicted by of-
fline methods at iteration k and Q is the
true Q value as per average discounted re-
turns. The comparison is representative of
Method I takeoff ∣ hover ∣ zigzag fl flythrugate ∣
BC	0.86±0.17	0.97±0.02	0.96±0.17	0.98±0.13
SAC (∏β)	0.88±0.21	0.97±0.11	0.94±0.33	0.96±0.08
CQL(H)	0.89±0.07	0.95±0.03	0.89±0.06	0.86±0.15
CQLBohn (H)	0.51±0.23	0.92±0.05	0.67±0.19	0.77±0.27
CQL-IM	0.86±0.06	0.95±0.01	0.94±0.09	0.97±0.08
Table 1: Normalized average returns on Drone Con-
trol tasks over 10 runs. Values in bold denote highest
returns. Values in green indicate best returns between
offline RL policies.
desired approximations by suggesting that
a low Qk — Q gap is indicative of the value
estimates closely resembling the true Q val-
ues. Observed results answer the remaining
questions; (4) CQL(H) learns significantly
conservative Q values in expectation which
steer the agent away from optimality. Es-
timates learned by virtue of excessive un-
derestimations drift away from the true Q
values, hence highlighting their theoretical
Method I Low K ∣ Low rank collapse ∣ Less Conservative ∣ Less assumptions ∣
CQL(H)	X	X	X	✓
CQLBohn(H)	✓	✓	✓	X
CQL-IM	✓	✓	✓	✓
Table 2: Summary of comparisons.
concerns. (5) CQLBohn (H) represents an
empirical upper bound on CQL(H). How-
ever, requirements raised by the Bohn(Q) approximation for scaling matrix H hinder convergence of
CQLBohn (H) towards true Q values. (6) Lastly, values learned by CQL-IM closely match the true
values. This is a direct consequence of learning smoother information-theoretic objectives which
sufficiently represent the true expected returns by virtue of an expressive posterior.
8
Under review as a conference paper at ICLR 2022
Figure 7: 2D t-SNE embeddings of representations learned by the critic parameters.
To further study the impact of erroneous approximations, one can assess the task performance of
methods in Table 1. CQL-IM, by virtue of intrinsic motivation, effectively captures the behavior
policy distribution and is competitive to BC and online SAC. CQLBohn(H) presents consistent re-
turns but falls short of optimal performance.
Random runs in RL may hinder fair comparison between algorithms. For instance, a good run of
a method when combined with its other moderate runs can push its otherwise on-par performance
higher. Following the recommendation of Lones (2021), we validate the statistical significance of
results by carrying out the Mann-Whitney U test (Mann & Whitney, 1947). All 40 seeds (10 seeds
per task) of each algorithm are compared to those of ∏β (online SAC) to yield the U statistic. U here
denotes the statistical significance of performance with higher values being desirable. We obtain
UBC = 123, UCQL = 88, UBohn = 13 and UIM = 97 with subscripts denoting respective algorithms.
Results in Table 1 are validated as BC and CQL-IM have higher statistical significance over CQL(H)
and CQLBohn (H) when compared to ∏β.
(7) Qualitatively, representations learned by the critic parameters link estimation errors with actor’s
behaviors. Fig. 7 presents 2D t-SNE (van der Maaten & Hinton, 2008) embeddings of these rep-
resentations. The critic parameters appropriately capture conservative, optimistic and near-accurate
estimations which are well-seperated in the low-dimensional representation space. We further vi-
sualize behaviors of actors corresponding to critic’s value estimates. In flythrugate, CQL(H) agent
underestimates its Q values while moving towards the gate. This results in the drone’s trajectories
falling short of optimal behavior. CQLBohn (H) overestimates Q values which results in the drone’s
trajectories overshooting optimal behavior. Contrary to above, CQL-IM presents accurate Q values
resulting in near-optimal behaviors.
6	Discussion
Summary: The paper studied conservatism through the lens of value function optimization, upper
bounding approximations and behavior cloning as regularization in the CQL framework (Table 2).
Optimizing underestimated value functions steers the agent away from true value distributions. The
problem is further reflected as a link from underparameterization to ill-conditioned curvatures. A
tractable upper bound approximation to lse(Q) mitigates these challenges but imposes a strong con-
dition on scaling matrix H to be I. Driven by its theoretical concerns, the framework incorporates
behavior cloning as off-policy variational regularization. The presented scheme results in accurate
value estimation, well-conditioned search spaces and expressive parameterizations.
Limitations: Data-driven mechanisms and RL reside at the pinnacle of machine learning. A theoret-
ical study of the two opens several new avenues for future work. We briefly highlight two promising
directions for further perusal; (1) A practical realization of the information-theoretic alternative and
its scalability to higher dimensions remain unbolted. An optimistic perspective towards this problem
is obtained using variational theory. Future directions could utilize alternate objectives with infor-
mation rich training signals for offline policies. (2) Orthogonally, one can seek alternative offline
mechanisms which would utilize tractable approximations to guarantee convergence. A meticulous
inspection of the policy structure sheds promise towards this aspect. We hope that an explanation
of the conservative learning framework serves as a motivating direction towards practical offline
learning problems.
9
Under review as a conference paper at ICLR 2022
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline
reinforcement learning. In International Conference on Machine Learning, 2020.
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline prim-
itive discovery for accelerating offline reinforcement learning. In International Conference on
Learning Representations, 2021.
Anonymous. Should i run offline reinforcement learning or behavioral cloning? In Submitted
to The Tenth International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=AP1MKT37rJ. under review.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In International Conference on Machine Learning, 2017.
Dimitri P Bertsekas. Abstract dynamic programming. Athena Scientific Nashua, NH, USA, 2018.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic Programming, volume 1. Athena Scien-
tific, 1995.
Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Ani-
mesh Garg. Conservative safety critics for exploration. In International Conference on Learning
Representations, 2021.
Nam Parshad Bhatia and Giorgio P SzegO. Stability theory of dynamical systems. Springer Science
& Business Media, 2002.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statis-
tics). Springer-Verlag, 2006.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-
dataset policy optimization. In International Conference on Learning Representations, 2021.
Dankmar BOhning. Multinomial logistic regression algorithm. Annals of the Institute of Statistical
Mathematics, 44(1):197-200, March 1992.
Qi Cai, Mingyi Hong, Yongxin Chen, and Zhaoran Wang. On the global convergence of imitation
learning: A case for linear quadratic regulator. arXiv preprint arXiv:1901.03674, 2019.
Erwin Coumans. Bullet physics simulation. In ACM SIGGRAPH, 2015.
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine. Leave no trace: Learning to
reset for safe and autonomous reinforcement learning. In International Conference on Learning
Representations, 2018.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, 2019.
Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game
theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017.
Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the
covariate shift. In AAAI Conference on Artificial Intelligence, volume 33, 2019.
Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-
max q-learning operator for simple yet effective offline and online rl. International Conference
on Machine Learning, 2021.
10
Under review as a conference paper at ICLR 2022
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, volume 80,pp.186l-1870, 2018.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In
Thirty-second AAAI conference on artificial intelligence, 2018.
Jiawei Huang and Nan Jiang. From importance sampling to doubly robust policy gradient. In
International Conference on Machine Learning, 2020.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning, 2021.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-
ment learning for vision-based robotic manipulation. In Conference on Robot Learning, 2018.
Mohammad Khan. Variational learning for latent Gaussian model of discrete data. PhD thesis,
University of British Columbia, 2012.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. In Neural Information Processing Systems, 2020.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-
learning. arXiv preprint arXiv:2110.06169, 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-
learning via bootstrapping error reduction. In Neural Information Processing Systems, volume 32,
2019.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. In Neural Information Processing Systems, volume 33, 2020a.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. In Neural Information Processing Systems, 2020b.
Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization
inhibits data-efficient deep reinforcement learning. In International Conference on Learning Rep-
resentations, 2021a.
Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for
offline model-free robotic reinforcement learning. In 5th Annual Conference on Robot Learning,
2021b.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45-73. 2012.
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Addressing distribu-
tion shift in online reinforcement learning with offline datasets, 2021.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Xiao Li, Yao Ma, and Calin Belta. Automata guided reinforcement learning with demonstrations.
arXiv preprint arXiv:1809.06305, 2018.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with
stationary distribution correction. In Uncertainty in Artificial Intelligence, 2020a.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy
reinforcement learning without great exploration. In Neural Information Processing Systems,
2020b.
11
Under review as a conference paper at ICLR 2022
Michael A Lones. How to avoid machine learning pitfalls: a guide for academic researchers. arXiv
preprint arXiv:2108.02497, 2021.
Henry B Mann and Donald R Whitney. On a test of whether one of two random variables is stochas-
tically larger than the other. Annals of Mathematical Statistics, 18, 1947.
Amir massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Reg-
ularized policy iteration with nonparametric function spaces. Journal of Machine Learning Re-
Search,17(139):1-66, 2016.
Brendon Matusch, Jimmy Ba, and Danijar Hafner. Evaluating agents without rewards. arXiv
preprint arXiv:2012.11538, 2020.
Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in
hilbert space. In Neural Information Processing Systems, volume 33, 2020.
Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Over-
coming exploration in reinforcement learning with demonstrations. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pp. 6292-6299. IEEE, 2018.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets, 2020.
Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer, 2006.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Neural Information Processing Systems, volume 29, 2016.
Brendan O’Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In International Conference on Machine Learning, pp. 3836-3845,
2018.
Jacopo Panerati, Hehui Zheng, SiQi Zhou, James Xu, Amanda Prorok, and Angela P Schoellig.
Learning to fly-a gym environment with pybullet physics for reinforcement learning of multi-
agent quadcopter control. arXiv preprint arXiv:2103.02142, 2021.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, 2019.
Nived Rajaraman, Lin F Yang, Jiantao Jiao, and Kannan Ramachandran. Toward the fundamental
limits of imitation learning. arXiv preprint arXiv:2009.05990, 2020.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. In Proceedings of Robotics: Science and Systems, Pittsburgh, Penn-
sylvania, June 2018. doi: 10.15607/RSS.2018.XIV.049.
Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Chaining
behaviors from data with model-free reinforcement learning. In Conference on Robot Learning,
2020.
Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4RL: Surprisingly simple self-supervision for
offline reinforcement learning in robotics. In 5th Annual Conference on Robot Learning, 2021.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
12
Under review as a conference paper at ICLR 2022
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9, 2008.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.
Ruosong Wang, Dean Foster, and Sham M. Kakade. What are the statistical limits of offline RL
with linear function approximation? In International Conference on Learning Representations,
2021.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and
Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint
arXiv:2105.08140, 2021.
Chenjun Xiao, Yifan Wu, Tor Lattimore, Bo Dai, Jincheng Mei, Lihong Li, Csaba Szepesvari, and
Dale Schuurmans. On the optimality of batch policy optimization algorithms. arXiv preprint
arXiv:2104.02293, 2021.
Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, and Florian Shkurti. Latent skill
planning for exploration and transfer. In International Conference on Learning Representations,
2021.
He Yin, Peter Seiler, Ming Jin, and Murat Arcak. Imitation learning with stability and safety guar-
antees. IEEE Control Systems Letters, 2021a.
Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal provable uniform convergence in offline pol-
icy evaluation for reinforcement learning. In International Conference on Artificial Intelligence
and Statistics, pp.1567-1575, 2021b.
Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea
Finn. Conservative data sharing for multi-task offline reinforcement learning. arXiv preprint
arXiv:2109.08128, 2021a.
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
Combo: Conservative offline model-based policy optimization. arXiv preprint arXiv:2102.08363,
2021b.
Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforce-
ment learning. In Conference on Robot Learning, 2020.
Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Ay-
tar, Misha Denil, Nando de Freitas, and Scott Reed. Offline learning from demonstrations and
unlabeled experience. arXiv preprint arXiv:2011.13885, 2020.
13
Under review as a conference paper at ICLR 2022
A Conservative Value Function Optimization
A.1 HESSIAN OF CQL(H)
The computation of CQL(H) hessian follows the computaion of its gradient,
VQCQL(H) = α (SoftmaX(Q) - ∏β) + ∏β (Q - BnQk)	(13)
▽QCQL(H) = α SoftmaX(Q)T(1 — SoftmaX(Q))+	∏β	(14)
、----------------{----------------}	l{z}
curvature depends on	curvature depends
estimations	on dataset
It is readily noticeable that CQL(H) is convex in the space of real Q values. More formally,
SoftmaX(Q) ∈ (0, 1] and 1 are |A| dimensional arrays which yields SoftmaX(Q)T(1 - SoftmaX(Q))
as a |A| X |A| positive definite matrix. This leaves ∏β which is an |A| dimensional array of action
probabilities and is always nonnegative. Thus, the complete sum results in a positive definite matrix
with dimensions |A| × |A|.
The first term emphasizes the dependence of estimations on a smooth curvature. Presence of ∏β, on
the other hand, highlights the dependence of curvature on dataset with action likelihoods reflecting
a fixed contribution.
A.2 OPTIMIZATION OF CQL(H) LANDSCAPE
This section studies the space of CQL(H) by analyzing its Hessian in the presence of steepest
descent method. Restating the Hessian of CQL(H),
VqCQL(H) = —α SoftmaX(Q)T SoftmaX(Q) + α SoftmaX(Q) + ∏β	(15)
As before, SoftmaX(Q) here denotes an |A| dimensional array which makes the first term an |A| ×
|A| matrix. The second term remains an |A| dimensional array with α being a scalar. Finally, the
third term ∏β denotes an |A| dimensional array of action probabilities. The sum of second and third
term when broadcasted to each column of the first term results in an |A| × |A| positive definite
matrix.
Since ∏β is held fixed during training, Eq. 15 may be realized as a quadratic expression with the
variable v = SoftmaX(Q), A as a matrix with diagonal entries -2α, d as a vector with entries α and
e as the fixed ∏β,
Quad(V) = gvτAV — dv + e	(16)
Consider the gradient of Quad(V),
V^ Quad(V) = AV — d	(17)
The minimizer V = V is the unique solution to the linear system AV = d. One can compute the
optimal step length ηk that minimizes Quad(V — ηV^ Quad(V)).
1T
Quad(v — ηV^ Quad(v)) = I(V - Nv QUad(V))T A(V - Nv QUad(V))
—dτ (V — ηVv Quad(v))T + e (18)
Setting the derivative w.r.t η to 0, we get,
=Vv Quad(^)τV^ Quad(^)
ηk = Nv QUad(V)T AV. QUad(V)	( )
Utilizing this in the steepest descent update rule yields the following,
Vk+1 =^k - ( £FRVTV W ) V^ Quad(V)	(20)
∖Vv QUad(V) T AV^ QUad(V))
We now study the rate of convergence. Using AV = d, we have,
1 l∣V — Vk2 = Quad(V) — Quad(V)	(21)
14
Under review as a conference paper at ICLR 2022
Utilizing Eq. 20 and noting that V^ Quad(V) = A(^ - v), We have the following,
k^k+1-vk2= Xkvk -vk2	(22)
Where V= (1_____________(Vv QUad(V)F QUad(V))2___________、
Where X = I 1 - (Vv QUad(V)TAVv Quad(^))(Vv QUad(V)TATVv Quad(^)) .
The above results in the following bound,
kVk+1-νk2 ≤ (λn-41 )2kVk -vk2	(23)
λn + λ1
kek+1-vk2 ≤ (K+1) Ilvk -vk2	(24)
where 0 ≤ λι ≤ λ .. ≤ λn are the eigenvalues of A and K = λn.
Dependence of Quad(^) convergence on K highlights its necessity for a well-conditioned objective.
If κ is large, Quad(V) optimization will be ill-conditioned. On the other hand, if K → 1, (⅛+l) →
0, resulting in sound convergence.
We thus establish a lower bound on K which captures estimation errors in V values. Simply rear-
ranging Eq. 24 yields the following,
kvk+1-v∣2 ≤ (κ-ιY
kvk-v∣2 ≤ κ + 1；
(25)
k^k + 1 -v∣2
k^k-v∣2
+1≤K-K
(kvk+1-v∣∣2 ʌ
kvk - v∣2 )
1+
1-
kVk + 1-vk2
kvk-vk2
kVk + 1-V∣∣2
kvk-vk2
≤K
∣∣vk - v∣2 + ∣∣vk+1 - v∣2
∣vk- v∣2 -∣vk+1- v∣2
≤K
(26)
(27)
(28)
TΓ-<∙	11	1	∙	∙ ʌ	r∙,	/ zA∖ F	Ci	/ √^ι ∖	∙ 1	,1	1	1	F
Finally, plugging in v = Softmax(Q) and V = Softmax(Q) provides the lower bound on κ.
∣softmax(Qk) — Softmax(Q)∣2 + ∣∣softmax((Qk+1) — Softmax(Q)∣2 <
∣softmax(Qk) — Softmax(Q)∣2 — ∣∣softmax((Qk+1) — Softmax(Q)∣2 —
(29)
We further prove by example that increasing underestimations drive the condition number to higher
values. Begin by considering the case wherein the error at iteration k+ 1 is twice of error at iteration
k, ∣Softmax(Qk+1) — Softmax(Q)I = 2∣Softmax(Qk) — Softmax(Q) |. Plugging this in Eq. 3 yields
a lower bound of 3. Following one step of policy update the bound becomes,
∣Softmax(Qk+1) — Softmax(Q) ∣2 + ∣∣Softmax((Qk+2) — Softmax(Q)∣2
二  -------ʌ. . ..-------------------------------------------------—— ≤ κ
∣Softmax(Qk+1) — Softmax(Q)∣2 — ∣∣Softmax((Qk+2) — Softmax(Q)∣2
(30)
Considering the case of increasing estimation errors such that the error at iteration k + 2 is thrice the
error at iteration k, ∣Softmax(Qk+2) — Softmax(Q)I = 3∣Softmax(Qk) — Softmax(Q)|. Utilizing
this in Eq. 30 results in a lower bound of5 which is greater than the previous bound of3. The above
indicates that K increases with increasing values of estimation errors.
A.3 RANK COLLAPSE & K
We derive Eq. 4 adapted from Mobahi et al. (2020). Starting with the assumption ∣∣π*∣ > √Ke
which translates into rt > 1 for the iterate rt in the search space. Using the lower bound of Mobahi
15
Under review as a conference paper at ICLR 2022
et al. (2020) (Eq. 141), We arrive at the following results with a(κ) = "0"(；)_++2；0T), b(κ)=
(ro -r1+s)2 and r0 = √Ki kπk.
r = at(κ)ro - b(κ)a(κ)1	(31)
a(κ) - 1
Setting the above expression to 1 yields the following,
log
1 —a(κ)+b(κ)
b(κ)+ro(1-a(κ))
log (a(κ))
(32)
Simplifying the log terms by plugging the expressions for a(κ) and b(κ) results in the following
inequality,
(33)
Thus,
k 0 ɪ _ 1
$、r0 - 1 _ √KI、√KI
t ≥ ---——---≥ ------
κκ
(34)
t
κ
A.4 Additional Related Work
Value Function Regularization: Various recent methods in literature utilize value function penal-
ties (Wu et al., 2019; Kumar et al., 2019; Jin et al., 2021; Buckman et al., 2021). Provision of
penalties to the value function results in constraining agents towards data transitions (Kumar et al.,
2021b). An additional benefit of using behavior penalties is the improvement in policy performance
and overall runtime (Fujimoto & Gu, 2021). Regularization methods may constrain the policy using
predefined sets of divergence metrics such as MMD kernels (Kumar et al., 2019) or KL regulariza-
tion (Wu et al., 2019) in the case of actor-critic algorithms. An alternate way to regularize values is
using explicit parameterizations of agent’s policy (Fujimoto et al., 2019). Beyond policy constraints,
alternate schemes such as Bellman uncertainty functions (Buckman et al., 2021) and uncertainty
quantifiers are also found effective in practice (Jin et al., 2021). Lastly, the more recent class of con-
strained RL methods (Anonymous, 2022) use Bernstein inequalities to obtain generic regularization
terms. These utilize estimates of dynamics and state action pair counts. Our regularization scheme
is motivated by these prior efforts.
Imitation Learning combined with Offline RL: Offline RL methods adapt BC as regularizations
(Fujimoto & Gu, 2021) and auxilary objectives (Rajeswaran et al., 2018; Rajaraman et al., 2020) to
tackle distributional shift. Various works in this domain have witnessed improvements in learning
of behaviors off-policy (Hester et al., 2018). A concrete use-case of this scenario is of learning
from demonstrations (Zolna et al., 2020). BC allows learning of policies from complex multi-
modal human data transitions where naive TD estimates fail to capture behaviors (Wu et al., 2021).
Additionally, provision of demonstrations motivates exploration in real world applications (Nair
et al., 2018) as well as guided search of policies (Li et al., 2018). We combine BC with Offline RL
based on the above directions.
B Conservatism & Approximations
B.1 CONTRACTIVE NATURE OF Bohn(Q)
This section theoretically shows that the contractive nature of softmax lse(Q) is preserved upon
replacing it with Bohn(Q) in CQL(H). More specifically, we show that the quadratic approximation
Bohn(Q) is a contraction for specific values of ψ. Consider two Q-value functions, Q and Q0 and
define a norm ∣∣∙∣∣. Consider the following expression,
Bohn(Q) - Bohn(Q0 )	(35)
16
Under review as a conference paper at ICLR 2022
2QTHQ - bQ + C - 2Q THQ + bQ0 - C	(36)
2(QTHQ - Q0 THQ0) + b(Q - Q)
(37)
≤ 2 QTHQ - Q THQ + b Q - Q	(38)
Wherein the first inequality results from the triangle inequality. Considering the first term in Eq. 38,
≤ 4 QT(IIAI - ∣a∣； ι1∣A∣1TAI)Q - Q T(IIAI - ∣∕∣1]1∣A∣ 1TAI)Q	(39)
Ub QT1IAI1Iq - q0 tq0 + / q0 T1IAI1TAIQO
QtQ - Q0tQ0 + μτ+ι((1TAIQjT(QOTIIAI)T - (1。Q)T(QTLAI)T)
QtQ - Q0 tQ0 +	匚(QOTQO- QtQ)
|A| + 1
≤ 4(A+i) AIQTQ TAIQOTQO
AI
4(∣A∣ + 1)
Using this in Eq. 38, one obtains the following,
QTQ-Q0TQ0
(40)
(41)
(42)
(43)
(44)
=IAI =4(A + 1)		UUUQTQ - Q0TQ0UUU	+b	UUU - (Q - Q )UUU			(45)
=IAI = 4(IA + 1)		(Q-Q0)T(Q+Q0)UUU		+ bUUU	(Q	Q)	UUU	(46)
≤	IAI ≤ 4(IAI + 1)		(Q - Q0)UUUUUU(Q + Q0)UUU		+ bUU	(Q	-Q)	UUU	(47)
=	IAIUq + Q0u + b 4(IAI + 1) +		UUUQ	0 -Q			(48)
Wherein the first inequality results from Cauchy-Schwarz inequality. The above result would be a
contraction if ( ia4(iiai+qJi + b) < 1. More concretely,
IAIQ + Q
4(IAI + 1)
+b<1
(49)
0
Q+Q
0
Q+Q
4(1- b)(∣A∣ + 1)
IAI
4(1- Hψ —g(ψ))(IAI + 1)
IAI
(50)
(51)
1
4
1
4
1
4
Thus, a suitable choice of ψ would allow the upper bound to hold, making the Bohning approxima-
tion a contraction.
17
Under review as a conference paper at ICLR 2022
B.2 LOCAL CONVERGENCE OF Bohn(Q)
The study of Bohn(Q) convergence involves isolating its optimization from the CQL(H) objective.
Our approach closely follows the convergence analysis of convex quadratics in Nocedal & Wright
(2006). Consider the gradient of Bohn(Q) for an estimate Q,
一 „ , ^ . ʌ ^ ....
NQ Bohn(Q) = HQ — b
(52)
The minimizer Q = Q is the unique solution to the linear system HQ = b. One can compute the
optimal step length ηk that minimizes Bohn(Q — nNq Bohn(Q)).
1T
Bohn(Q — nNQ Bohn(Q)) = - (Q — nNq Bohn(Q)) H (Q — nNq Bohn(Q))
—bT (Q — nNq Bohn(Q))
T
+c
(53)
Setting the derivative w.r.t n to 0, we get,
VQ Bohn(Q)TVQ Bohn(Q)
nk	7 X	7-
VQ Bohn(Q)TH VQ Bohn(Q)
Utilizing this in the steepest descent update rule yields the following,
(54)
Qk+1 = Qk -
V A Bohn(Q)TN A Bohn(Q)
」-------' '.Q	V° Bohn(Q)
V^ Bohn(Q)THV^ Bohn(Q)	Q
(55)
We now study the rate of convergence. Using HQ = b, we have,
2kQ — Qk2 =Bohn(Q)- Bohn(Q)
ɪ T, ∙1 ∙ ∙ TΓ∏ LL	F , ∙	, 1 X~l T-¼ 1 / ʌ ∖	TTTT / ʌ √^ι ∖	1	,1 l' 11	∙
Utilizing Eq. 55 and noting that Nq Bohn(Q) = H(Q — Q), we have the following,
(56)
Where X=C- CQBohn(Q)
kQk+1 - Qk2= XkQk - Qk2
(▽q Bohn(Q)T^Q BOhn(Q))2
ITH▽© BOhn(Q))(▽© BOhn(Q)THT▽© BOhn(Q))	.
(57)

二
二
^ ^
The above results in the following bound,
kQk+1- Qk2 ≤
λn — λ1
kQk+1 - Qk2 ≤
λn + λ1
(∕2
kQk - Qk2
kQk - Qk2
where 0 ≤ λ1 ≤ λ2 . . ≤ λn are the eigenvalues of H and κ
λn
λι .
(58)
(59)
Dependence of Bohn(Q) convergence on κ highlights its necessity for a well-conditioned objective.
If K is large, Bohn(Q) optimization will be ill-conditioned. On the other hand, if K → 1, (⅛+l) →
0, resulting in sound convergence.
We thus establish a lower bound on K which captures estimation errors in Q values. Simply rear-
ranging Eq. 59 yields the desired result,
kQk+1- Qk2 ≤ (K-JY
kQk-Qk2 - IK + 1；
kQ k+1- Qk2 +1 ≤ K -J kQ k+1- Qk2
kQk - Qk2	∖ kQk - Qk2
(60)
(61)
I + kQk+1-Qk2 I
+ kQk-Qk2 I V κ
≤ K
I _ kQk+1-Qk2 I —
kQk-Qk2	1
kQk - Qk2 + kQk+1- Qk2
(62)
kQk — Qk2 — kQk+1- Qk2
≤κ
(63)
^
^
18
Under review as a conference paper at ICLR 2022
C Conservatism with B ehavior Cloning
C.1 Lower Bound on Mutual Information
Here we will derive the lower bound on Mutual Information between the agent’s and behavior policy
MI{∏; ∏β}. Denote p(∏, ∏β) as the joint probability distribution of actions sampled from ∏ and
∏β, expanding MI{∏; ∏β} as per definition,
MIE πβ} = lp(π,πβ)log 慌Wdndn
(64)
p(πlπβ )p(πβ )∖' ,ʌ
=J Pmne)l°g( p(π)p(∏β) Jdndne
=Z p(n,∏β)log (pP(Π)β) ) dnd∏β
Upon seeking a tractable approximation q(n∣∏β) to the posterior p(n∣∏β),
q(n|ne) , 1	p(n|ne)ʌ , ,ʌ
J p(n，nβ) (log -∏M + + log 而庖Jde
/ p(∏,∏e )log
ZPgn )log (qp⅛F)
(q⅞∏F)dndπe+Zp(n，ne) log (P⅛?)dndπe
dnd∏e +
/ p(∏l∏e )p(∏e )log
p(n|ne)
q(n|ne)
dnd∏e
(65)
(66)
(67)
(68)
(69)
The second term simplifies to Ene [KL(p(n∣∏e)∣∣q(n∣∏e))] which must be non-negative,
≥
p(∏,∏e) logq(∏∣∏e)d∏d∏e -
/ p(∏l∏e )p(∏e )log p(∏)d∏d∏e
Marginalizing the second term over ∏e yields the lower bound,
=Ep(n,∏β )[log q(n∣∏β)] + H(p(n))
(70)
(71)
C.2 Practical Implementation
This section derives the off-policy variant of IM formulation incorporating the MI{n; ∏e} lower
bound. Begin by explicitly writing the augmented Q value function with rt as the reward at step t,
T
Q = Est 〜D,at 〜∏ X Y JIrt	(72)
t=1
Expanding the Q value recursion till the termination step T ,
rι+Ep(∏,∏β )[log qi(n|ne )]+Yr2+YEp(∏,∏β )[log q2(n|ne )]+∙∙∙+Y T TrT+yt-1Ep(∏,∏β )[log qτ (n|ne)]
(73)
Grouping the reward and log terms separately,
Q = (ri + γr2 + ... + Y T TrT) + (Ep(∏,∏β )[log qι(∏l∏β)]
+ Y Ep(n,∏β )[log q2(n∣ne)] + ... + YT-1Ep(∏,∏β )[log qτ(n∣∏e)])	(74)
where the discounted return in the first term simplifies to Qk and the second term to an expected
sum of logs,
T
Q = Qk + Ep(n,∏β) X Y t-1 log qt(n∣∏e)	(75)
t=1
Moving Yt-1 inside the log,
T
Q = Q k + Ep(n,∏β) X log qt(n∣∏e )γt-1	(76)
t=1
Finally, moving the sum inside log yields the result.
T
Q = Qk + Ep(∏,∏β) log Y qt(∏l∏β)γt-1	(77)
t=1
19
Under review as a conference paper at ICLR 2022
C.3 Convergence Bound
We theoretically show that CQL-IM converges faster to true Q when compared to CQL(H). Begin
by considering the gap between underestimated values BnQk - α (Softmax(Q) - 1)and true values
Q,
BnQk - α ]—(Q) - l] - Q	(78)
V πβ J 2
Using the fixed point property of Bellman operator,
Qk - Qa SSoftmaX(Q) - l!	(79)
V πβ J 2
Using the triangle inequality,
≤kQ卜-Qk2 + α s°ftmax≡ - 1	(80)
πβ	2
We now consider the two terms separately. For the first term we have, following the results of
approximation value iteration convergence (Bertsekas & Tsitsiklis, 1995),
≤k(Bπ)kQ0- Qk2	(81)
YkkQ0 - Q + Q - Qk2	(82)
Here, Q denotes an approximation to the true Q function.
≤ YkkQ0 - Qk2 + YkkQ - Qk2	(83)
which results in the following bound (Bertsekas, 2018),
≤ γk √rmax+γk Jrm-II	(84)
The bound denotes the geometric convergence of policy optimization process. With O(Yk), policy
updates approach local convergence asymptotically in increasing number of iterations k.
Considering the second term, we have,
which is bounded as follows,
exp(Q)
------------7---- - 1
πβ Σa∈A exP(Q(S,⑼	2
exp(Q)
Pa∈A exP(Q(S,⑼
/ ∕A∖
exp(Q)______
πβ Pa∈A exP(Q(S,⑼
(85)
(86)
α
α
- 1	≤ α
2
- 1	≤ ∞
2
Irrespective of the value of ∏β, exponential values of Q do not decay. ThiS leaves the error to persist
at an O(exP(Q)) rate.
We now repeat the same computation for CQL-IM. Explicitly stating the CQL-IM underestimations,
Q = BnQk - α (SoftmaX(Q+ log q - 1)- log q	(87)
Utilizing this to compute the difference from true Q values,
BnQk - a SSoftmaX(Q+ log q)- 1! - log q - Q	(88)
∖	πβ	)	2
As before, using the fixed point of Bellman operator and separating the terms using triangle inequal-
ity,
kQ k - Qk2 + 卜 S fa⅛+^ - 1!+log q∣2	(89)
20
Under review as a conference paper at ICLR 2022
Solving for the first term results in O(γk) convergence for policy updates,
kQk - Qk2 ≤ Yk √rmax+Yk jrm-Sl
And for the second term, we have,
_________exp(Q + log q)_________
∏β ∑a∈A exp(Q(s, a) + log q(s, a))
- 1 + log q
(90)
(91)
which yields the following after manipulation,
∏bc exp(Q)q	八一
----------------X-----------1 + log q
∏βΣa∈A exp(Q(s, a))q(s, a)	)	2
(92)
Denoting ∏bc = P—二(§ Ο)gives Us the following result.
CsoftmaX(Q)
----∏-------1 +log q
(93)
Compared to Eq. 86, Eq. 93 presents the additional ∏bc and log q terms. Here, log q presents
bounded values and hence, bounded convergence following prior results (cai et al., 2019; Yin et al.,
2021a). On the other hand, ∏bc improves the rate of convergence. Note that ∏bc ∈ (0,1] depends
on each state-action pair as softmax(Q) and acts as a scaling term for these exponential values.
ThiS way, the effective rate reduces from O(exp(Q)) of Eq. 86 to O(∏bc exp(Q)). Thus, CQL-IM
converges faster than cQL(H) to true Q values.
c.4 Stability Guarantee
Next, we show that cQL-IM is -stable when compared to cQL(H). To derive this result, we
assume that the gradient of Q values for both objectives is bounded by a constant and Q values
can locally approximate the Bellman update, i.e.- at convergence Q ≈ Bπ Qk.
We need to show that the gradient of cQL-IM is strictly negative definite and lies in the open left-
half plane. Note that this is one of the conditions for Lyapunov stability (Bhatia & Szego, 2002), a
stronger condition for nonlinear systems.
First, differentiating cQL(H) w.r.t parameters of Q, we get,
(α [pexρ≡- πβ]+(Q -Bn Q k)πβ)vQ
(94)
C ∙	X—7 z^ι ,	Fxʌ	*⅛-7Γ ʌ k-	. 1	, ∙	1	,	fl	/ z^ι ∖	^ EI ∙
Since VQ ≤ e and Q ≈ BnQk at convergence, the equation reduces to αsoftmax(Q) 一 ∏β. This
results in 0 for extreme values of softmax(Q) and ∏β (softmax(Q) = 1 and ∏β = 1) indicating that
the gradient is not strictly negative. Intuitively, the result denotes that the critic update of cQL(H)
may not alaways be a descent direction.
considering cQL-IM and differentiating the objective w.r.t parameters of Q,
α
exp(Q + log q)	_ -
Pa exp(Q + log q)	ββ,
+ (Q + log q -BnQk)∏β VQ
(95)
Under the same conditions of bounded gradient VQ ≤ and approximate TD convergence Q ≈
BπQk, Eq. 95 simplifies to - log q. This indicates that cQL-IM is -stable. Upto a constant , the
gradient always corresponds to a descent direction when Q ≈ BπQk.
D Conservatism in the Conjugate Space
D. 1 cQL(H) AS ENTROPY MINIMIZATION
Optimizing Q values in the value function space does not necessarily highlight its consequences
in alternative basis spaces. For instance, policy gradient methods optimize policies directly in the
21
Under review as a conference paper at ICLR 2022
policy space which implicitly explains asymptotic growth of the value function (Sutton & Barto,
2018). However, a converse for the same is hard to accomplish. A similar argument can be made
for CQL(H) upon realizing its implications in the conjugate space.
As a starting point, consider the Legendre transform (convex conjugate of a differentiable func-
tion) (Boyd & Vandenberghe, 2004) of lse(Q). The lse(Q) function is convex in R|A| and has
SoftmaX(Q) as its gradient map (Gao & Pavel, 2017). Denote f * (Qconj) as the Legendre transform
of f(Q) expressed in Eq 96.
f * (Qconj) = SuP (QTonjQ - f (Q)) ； Qconj= YQf (Q)	(96)
Q∈dom f
The quantity Qconj = YQf(Q) represents the Q function in conjugate space with dom f represent-
ing the feasible set of Q values. Substituting f(Q) = lSe(Q) to obtain the conjugate f* (Qconj ),
f(Q) = lSe(Q) =⇒ f*(Qconj) = X Qconj(s, a) log(Qconj(s, a))	(97)
a
Eq. 97 establishes a direct link between f* (Qconj ) and the entropy
of conjugate value distribution. f* (Qconj ) exactly represents the
negative entropy of Qconj function restricted to |A| dimensional
simplex 4|A|, i.e.- -H(Qconj) ∈ 4|A| . One can construct an aux-
ilary objective consisting of -H(Qconj) which is easier to optimize
in comparison to lSe(Q). A simple inspection of the Fenchel-Young
Inequality (Boyd & Vandenberghe, 2004; Kumar et al., 2020a)
f(Q) + f*(Qconj) ≥ QcTonjQ reveals that f*(Qconj) ≥ -f(Q).
Utilizing this for lSe(Q) presents a tractable entropy minimization
objective in the conjugate space, lSe(Q) ≥ min H(Qconj),
Qconj
Figure 8: Intuition behind
min H(Qconj)
Qconj
f*(y) + f(X) ≥ XTy , ∀X ∈ dom f, ∀y ∈ domf*	(98)
= -H(Qconj) ≥ - lSe(Q) + QcTonjQ ,∀Q,∀Qconj	(99)
= -H(Qconj) ≥ - lSe(Q) , ∀Q, ∀ Qconj	(100)
= H(Qconj) ≤ lSe(Q) , ∀Q, ∀ Qconj	(101)
= min H(Qconj) ≤ lSe(Q) , ∀Q	(102)
Qconj
That is, minimizing the entropy in conjugate space is a tractable lower bound objective on the lSe(Q)
function. Utilizing this insight in CQL(H) yields the auxilary entropy minimization objective.
min α E$~d
Q
min H(Qconj)
Qconj
1	_{Z	_}
approximation
-Ea~∏β [Q(s,a川 + 2Es,a,s0 ZD ](q-B Qk)[
1	_{Z	_} 1	_{Z	_}
expected value
empirical Bellman error
(103)
This is illustrated in Fig. 8. In addition to the prior expected value and empirical Bellman error
terms, Eq. 103 now additionally represents an entropy minimization objective min H(Qconj) in
Qconj
the conjugate space. Intuitively, the objective enforces an agent to minimize uncertainty within the
learning signal arising from batched data transitions. Note that this is in addition to predefined maxi-
mum entropy regularization objectives which are a common precept in learning behaviors (Haarnoja
et al., 2018; Singh et al., 2020; Kumar et al., 2019; Levine et al., 2020). But how does minimization
of entropy link to underestimations in the conservative learning framework?
The insight can be further studied by optimizing over Eq. 103 and setting its derivatives w.r.t. Q and
Qconj to 0 while adhering to the KKT conditions. We first optimize over min H(Qconj) to obtain
Qconj
the optimal Qconj = e. Utilizing this to solve the global optimization problem, one obtains the
following expression,
-α∏β + ∏β (Q-B Qk) =0 T Q = Bn Qk + α	(104)
22
Under review as a conference paper at ICLR 2022
Eq. 104 paints a direct significance between the entropy minimization scheme and Q value esti-
mates. The second term in Eq. 104 represents overestimations at each subsequent policy iteration
which regularize the Bellman estimate BπQk. This leads to an accumulation of positive biases over
policy updates which directly depend on α. Since α is a free parameter, its value may be tuned for
reducing overestimation errors. Additionally, Kumar et al. (2020b) note that in the limit of infinite
data, small values of α suffice to yield a lower bound. Thus, elimination of overoptimistic estimates
require either a hand-engineered value or infinite data samples.
D.2 Explaining High Variance
An alternate process to understand conservatism in conjugate space is by considering the Legendre
transform of the complete CQL(H) objective. Similar to initial steps, consider f (Qconj ) as the
CQL(H) objective in Eq. 1. UPon computing Qconj = RQf (Q), one arrives at a value for Qconj
which depends on Q and ∏β,
Qconj = VQf (Q) = α [softmax(Q(s, a)) - ∏β(a|s)] + (Q(s, a) - B Qk)^β(a|s)
(a Softmax(Q(s, a)) + Q(s, a)∏β(a∣s))—
(105)
(106)
∏β(a|s) [α + BnQki
Following the precept from CQL (Kumar et al., 2020b) and only considering the setting wherein
values of α are small,
Qconj= ∏β(a|s)(Q(s, a) — BnQk)	(107)
Intuitively, Qconj denotes the gap between current Q and empirical Bellman estimate Bn Qk which
is scaled by the likelihood of actions under behavior policy ∏β. The expression quantifies howfar is
the current estimate from previous Bellman estimate under the behavior policy distribution.
Rearranging Eq. 107 yields Q = Qconj + Bn Qk. Substituting this in CQL(H) and optimizing over
Qconj by setting the derivative w.r.t. Qconj to 0 yields the following,
+ BnQk) + Qconj — α( ɪ softmax (Qonj + Bn Qk1 — 1' — Qonj = 0	(109)
πβ	lπβ	πβ	) π πβ
Qconj + B^nQk = α (-1Softmax (Qconj + B^nQk1 — 1)	(110)
πβ	πβ	πβ
Qconj = ∏β ∣BnQk| + α 卜oftmax (/onj + BnQk) — ∏β(a|s)	(111)
l{z}l{~}/	l{z} '-----------------{------------------}
scaling estimate tradeoff	underestimation
Eq. 111 consists of4 terms. The first term indicates the rate of decay at which subsequent estimates
in the second term diminish towards finitely small values. The third term forms the tradeoff factor
denoting the scale at which CQL underestimates values. Note the dependence of the third term on
constant α which, as in the previous analysis, intuitively suggests that uncertainty may be traded off
for estimation errors (Kumar et al., 2020b). Lastly, the fourth term indicates potential underestima-
tion of Q values arising from the squashed SoftmaX(Q) distribution. Since μ 8 exp(Q), the gap
SoftmaX(Q) — ∏β intuitively represents a change in μ — ∏β. This highlights a set of fluctuating Q
values which lead to increasing/decreasing Softmax(Q) resulting in high-variance updates.
E	Extensions to Model-based Setting
This section highlights the extension of our analysis to the setting of Conservative Model-based
Policy Optimization (COMBO) (Yu et al., 2021b). In order to strictly adhere to the notation of
23
Under review as a conference paper at ICLR 2022
COMBO, we slightly abuse our notation and denote the state marginal distribution as dπM (s) cor-
responding to thedistribution of states obtained by rolling out policy π in the MDP M. Further,
define the MDP M as the empirical MDP induced by the dataset D corresponding to the behavior
policy ∏β. The model-based setting trains a dynamics model T such that the objective corresponds
to maximum likelihood estimation min Es,a,s，〜D [logT(s0∣s, a)]. Corresponding to the model,
one can construct the learned MDP M = (S, A, r, T, γ). COMBO makes use ofa data aggregation
framework wherein at each iteration, the agent performs k-step rollouts using Tb starting from state
s ∈ D and adds the data generated by the model to a separate dataset Dmodel. The policy is optimized
corresponding to a batch of data sampled from D ∪ Dmodel wherein each datapoint is drawn from
D with probability f ∈ [0, 1] and Dmodel with probability 1 - f ∈ [0, 1]. The empirical objective
utilized by COMBO (Yu et al., 2021b) (Eq. 23) is based on CQL(H) and is expressed as per Eq.
112.
Qk — argmin α (Es〜d枭 [lse(Q)] - Es,a〜D[Q(s,a)]) + 1 Es,ag〜df ](Q(s,a) -BπQk)
(112)
Eq. 112 consists of dπc as the discounted state marginal distribution obtained when policy π is
executed in the MDP Mc and df as an f -interpolation between the offline dataset and model rollouts
df = fd + (1 - f)dπc wherein the notation uses d as a shorthand for the discounted state marginal
distribution corresponding to the dataset dπβ (st).
E.1 Underestimation in COMBO
Prior to our analysis, we summarize the conservative nature of COMBO. Upon optimizing over Eq.
112 by setting the derivative w.r.t. Q to 0, one obtains,
a [softmax(Q) — d] + df (Q -B Q )=0	(113)
Q = Bn Q k - α ( softm.Q)- d )	(114)
Eq. 114 denotes that with high probability, COMBO underestimates its subsequent Q values when
softmax(Q) > d.
E.2 COMBO in Conjugate Space
We analyze COMBO in the conjugate space by substituting the Legendre transform of lse(Q) in Eq.
112,
Q k+1 - arg min a ( Es〜d∏
Q	Mc
(115)
Similar to CQL(H), Eq. 115 presents an entropy minimization objective. We first optimize
min H(Qconj) by setting its derivative to 0 and obtain the optimal Qconj value as 1. Optimizing
Qconj	e
Eq. 115 by using the KKT conditions for min H(Qconj) and setting derivative w.r.t Q to 0,
-dα + df (Q -BnQk) = 0	(116)
Q = Bn Qk + α-d	(117)
df
Eq. 117 highlights the direct dependence of Q values on overestimations.
min H(Qconj) - Es
,a〜D [Q(s, a)]) +2Es,a,s0〜df ](Q -BnQk)
Qconj
E.3 COMBO with Approximations
We evaluate COMBO by fitting a standard tractable approximation to lse(Q) as in the case of
CQL(H),
lse(Q) ≤ 2QTHQ - bQ + C
(118)
24
Under review as a conference paper at ICLR 2022
The above results in Eq. 119.
Qk+1 - argmin α (Es〜d枭 [Bohn(Q)] - Es,a〜D[Q(s,a)]) + 1 Es,ag〜df ](Q(s, a) - BnQk)
(119)
Optimizing Eq. 119 w.r.t. Q and setting the derivative to 0,
a (H(Q + ψ) - Softmax(ψ) - d) + df (Q - BnQk) = 0	(120)
Q [H + df) = df BnQk - α 眄ψ - Softmax(ψ) - d]	(121)
Q = (af) Bπ Q k -(小! hHψ - softmaχ(ψ)- di	(122)
Eq. 122 presents the same scaling problem as found in CQL(H) analysis.
F Additional Bounds
This section further highlights underestimations in CQL while obtaining tighter bounds on CQL(H).
Prior to the complete derivation, we will show that the CQL(H) variant may not always underesti-
mate the true Q values.
F.1 Underestimation in CQL
Here we present the proof of underestimation in CQL Q values as derived in (Kumar et al., 2020b).
Note that the original CQL objective, in the absence of regularization, is given by Eq. 123.
argmin a Eμ [Q(s, a)] + 2Es,a,so〜D ](Q(s, a) - BnQk)	(123)
Optimizing over Eq. 123 by taking the derivative w.r.t Q and setting it to 0, one obtains,
ɑμ + (Q -BnQk)分产=0	(124)
Q = BnQk - α~μ	(125)
πβ
Since Bn Qk ≥ Q, CQL underestimates Q values at each subsequent iteration.
F.2 UNDERESTIMATION IN CQL(H)
We now follow the same process as described above and show that CQL(H) does not always under-
estimate Q values. Recall that the CQL(H) objective is expressed as per Eq. 126.
min a Es〜D Iogfexp(Q) - Ea〜∏β [Q(s,a)]
Q
+ 1 Es,a,s，〜D ](Q(s,a)-BnQk)2
(126)
a
Optimizing over Eq. 126 by setting the derivative w.r.t Q to 0,
-exp(Q) _ --
.Pa exp(Q) i
+ (Q -BnQk)分产=0
(127)
Softmax(Q) - 1+ Q = BnQk
πβ	」
=BnQk - α [Softmax(Q) - 1
L	πβ	一
(128)
O ∙	⅛"J7Γ zA k.	A 1 ∙ l' Ci	//A'、 C	. 1 ，-、，-'、T ZZl ∕∖ 1	,	F	, ∙	, √^ι	1	,	1
Since BnQk ≥ Q only if Softmax(Q) ≥ ∏β, the CQL(H) does not underestimate Q values at each
iteration. However, the relation holds with high probability for small values of α.
The derivation for a tighter bound consists of two stages, (1) first we will show that bound is an
upper bound on the CQL Q values. (2) The second step consists of obtaining Q values as a lower
bound on the true Q values.
25
Under review as a conference paper at ICLR 2022
F.3 UPPER B OUND ON CQL(H)
We begin by writing the first expectation exclusively on both its terms,
α (Es 〜D log X exp(Q J - Es〜D
,a 〜∏β [Q]) + 2 Es,a,s0〜D ](Q -BπQk)[	(129)
By applying Jensen’s inequality twice to the first expectation, one obtains,
≤ α log X Es〜D IeXP(Q)] - Es〜D,a〜∏β [Q]) + 2 Es,a,s0 〜D ](Q -BnQk)[	(130)
Utilizing Hoeffding's Lemma for exponent in the first inner expectation with the difference (Qk -
Qmin)2 wherein Qmin denotes the minimum Q value up till iteration k, we obtain the following
upper bound,
≤ α (log X exp (1 (Qk - Qmin) 2) - Es〜D,a〜∏β [Q]) + 2Es,a,s0〜D [(Q -BπQk)2] (131)
F.4 LOWER BOUND ON Q
Following the same process of optimizing over the upper bound by taking the derivative w.r.t Q and
setting it to 0,
α 4(Qk - Qmin)SoftmaX (1 (Qk - Qmin)) - ∏β +(Q -BnQk)∏β = 0	(132)
二(Q - BnQk )πβ = -α 4 (Qk - Qmin)SoftmaX (w (Qk - Qmin)) - πβ	(133)
Q = BnQk - π~ 4 (Qk - Qmin)SoftmaX (w (Qk - Qmin)) - πβ	(134)
Since Qk - Qmin ≥ 0, SoftmaX(Qk - Qmin) ≥ 0 and 1 (Qk - Qmin)SoftmaX(1 (Qk - Qmin)) ≥
∏β with high probability, Q values at each iteration k are underestimated resulting in Q ≤ BnQk.
F.5 An Alternate Perspective on Approximations
This section provides a statistical perspective towards approximations. Starting with the CQL(H)
objective and applying Jensen’s inequality twice to the first expectation as in the previous section,
≤ α ^logXEs〜D [eXp(Q)] - Es〜d,。〜no [Q]^ + 2Es,a,so〜D ](q -BnQk)[	(135)
One can formulate Eq. 135 as a statistical relation which incorporates the probability of overestima-
tion Pr(eXp(Q) ≥ eXp(BnQk)). Upon utilizing the Chernoff bound Pr(eXp(Q) ≥ eXp(BnQk)) ≤
expEBQQk)) one obtain the following,
≤ α( logXeXp(BnQk)Pr(eXp(Q) ≥ eXp(BnQk))
a
- Es〜D,a〜∏g [Q]) + 2 Es,a,s0 〜D ](q -BnQk)j	(136)
We now make the sub-Gaussianity assumption which bounds the probability of underestimation
with regards to the previous Bellman iterates Pr(eXp(Q) ≥ eXp(BnQk)) ≤ 2eXp(-c(BnQk)2)
with c > 0 as a constant. The assumption, although a strong one in various respects, allows one
26
Under review as a conference paper at ICLR 2022
to reformulate the objective with external tunable parameters such as c. Utilizing the assumption in
Eq. 136 provides an alternate upper bound on CQL(H).
≤ α( logf2exp(BπQk(1 - cBπQk))Pr(exp(Q) ≥ exp(BπQk))
a
— Es〜D,a〜∏β [Q] ) + 2Es,a,s0〜D ](Q - BnQk?
Taking the derivative of Eq. 137 w.r.t. Q values and setting it to 0,
α [Vq log (Pr(exp(Q) ≥ exp(BπQk))) - ∏β] + ∏β(Q - BnQk) = 0
Q = BnQk - α VQ PMeXP(Q) ≥exp(BnQk)) - 1
[∏β Pr(exp(Q) ≥ exp(BπQk))
(137)
(138)
(139)
Eq. 139 depicts the probabilistic nature of underestimations dependent on the probability of overes-
timations, a result that may be understood for future work.
G	Conservatism as Dual Problems
The CQL framework, by virtue of explicit underestimations, gives rise to a theoretically rich set
of objectives in the dual space. One can interpret CQL as a constrained optimization problem and
gain intuition towards its individual components. More concretely, we investigate dual problems of
CQL(H) in detail and study the variation of its individual components by establishing a link between
the conjugate formulations.
G.1 Primer on Duality
For completeness, we revisit the Lagrange dual function. Consider a constrained optimization prob-
lem as in Eq. 140.
min f (x) subject to hi(x) ≤ 0 , i = 1, ..m.	(140)
The Lagrangian L({x, λ}) of the above problem can be expressed as per Eq. 141 with
dom L({x, λ}) = domf × Rm.
m
L({x, λ}) = f(x) + X λihi(x)	(141)
i=1
Here, λ is denoted as the Lagrange multiplier. One can define the Lagrange dual function (Boyd &
Vandenberghe, 2004) g : Rm → R as the minimum value of the Lagrangian over x∀λ ∈ Rm.
g({λ}) = min L({x, λ}) = min f(x) + X λihi (x)	(142)
x∈dom f	x∈dom f
i=1
The dual function, being a pointwise minimum of a family of affine functions of λ, is concave. We
define the dual problem as finding the best lower bound from the Lagrange dual function. This is
mathematically expressed in Eq. 143.
max g({λ}) subject to λ ≥ 0	(143)
G.2 Relationship between Conjugate and Dual Problems
Following the precept from Boyd & Vandenberghe (2004), we establish a direct relationship between
the conjugate and dual of the optimization problem,
L({x, λ}) = min	f(x)+Xm λihi(x)	(144)
x∈dom f
27
Under review as a conference paper at ICLR 2022
Considering hi (x) of the form Aixi ≤ bi, we have the following dual in compact notation,
g({λ}) = min ,(f (X) + λ(ATx - b)) x∈dom f	(145)
=-bλ + min (f (x) + AT λx)	(146)
resulting in the following relationship between conjugate and dual expressions,	
g({λ}) = -bλ - f*(-ATλ)	(147)
G.3 Dual of Soft-Maximum Transform
Minimization of the soft-maximum lse(Q) can be realized as a sub-problem to the CQL(H) with
the constraint enforcing Q values to be underestimated at each subsequent iteration, Q ≤ BπQk .
Mathematically,
min log X exp(Q(s,a))	subject to Q ≤ BπQk	(148)
Q
a
L({Q, λ}) = log X exp(Q(s, a)) + λ(Q - BnQk)	(149)
a
which yields the following dual,
g({λ}) = min (logXexp(Q(s, a)) + λ(Q - BnQk))	(150)
g({λ}) = -λBπ Qk + H(-λ)	(151)
Thus, we have,
max - λB Qk + H(-λ)	subject to λ ≥ 0	(152)
A meticulous inspection of Eq. 152 reveals that the dual problem is an enropy maximization problem
in the inverse variable space of -λ logits.
G.4 DUAL OF CQL(H)
Analogous to previous section, one can consider the dual of complete CQL(H) objective with ex-
plicit underestimation constraints.
Formulating the primal problem, we get,
吗n a Es~D [lse(Q) - Ea~∏β [Q(S, a)]] + 2Es,a,s0~D ](Q(s,a)-BπQk)j s.t.Q ≤BπQk
(153)
g({λ}) = min α E§~d [lse(Q)-6。~*[Q(s,a)]]
Q
+ 1 Es,a,s0~D ](Q(s, a) - Bn Qk)[ + λ(Q - Bn Qk) (154)
=-λBπQk + min (CQL(H) + λQ)	(155)
Q
yielding the following dual problem,
max - λBπQk - CQLconj(-λ) subject to λ ≥ 0	(156)
Observing Eq. 156, one notices that the dual problem maximizes the conjugate CQL objective with
inverse dual variable -λ.
28
Under review as a conference paper at ICLR 2022
H Additional Results
H.1 Value Function Optimization
Figure 9: (left-center left) Variation of rank, (center right-right) Variation of log(κ) during training
The setting further reasons about approximations
by comparing the various sources of estimation er-
rors. Fig. 10 links the variation in average target
values to their respective objectives. CQL(H) ex-
hibits conservatism by virtue of minimizing lse(Q)
values. Underestimations accumulating over the
course of learning steer estimates away from the
true Q function. Bohn(Q), being an upper bound
on lse(Q), provably overestimates Q values. This
Figure 10: Average targets of CQL variants.
in turn, enforces CQLBohn (H) to yield over-optimistic estimates which hurt convergence. Lastly,
lse(Q) with the additional IM objective (lse(Q)+IM) approximates analogously to lse(Q) but high-
lights potential towards convergence.
H.2 Actor Visualizations
We further establish a link between conservative approximations and their effects on learning of
offline policies. Fig. 11, 12 and 13 present the 2D t-SNE (van der Maaten & Hinton, 2008) embed-
dings of representations learned by the last, second and first layers of policies respectively. While
overestimations incurred by the upper bounding CQLBohn (H) are vividly reflected in the offline
policy distribution, it is only seldomly clear to distinguish between conservative and near-accurate
approximations learned by CQL(H) and CQL-IM respectively. The above indicates that although
the value function of CQL-IM is less conservative than that of CQL(H), it is still able to learn
conservative parameters with significant accuracy.
Figure 11: 2D t-SNE embeddings of representations learned by the last layer of policies on hover,
zigzag and flythrugate tasks.
hover	zigzag	flytlιrιιgate
・ CQLBohnm) ・ CQL(H) ・ CQL-IM
Figure 12: 2D t-SNE embeddings of representations learned by the second layer of policies on
hover, zigzag and flythrugate tasks.
29
Under review as a conference paper at ICLR 2022
Figure 13: 2D t-SNE embeddings of representations learned by the first layer of policies on hover,
zigzag and flythrugate tasks.
H.3 Experiments in D4RL Benchmark
* CQL(H) * CQL-IM * CQLB如出)
Figure 14: Variation of rank on 9 challenging D4RL datasets
* CQL阳	* CQL-IM * CQLbuuM
Figure 15: Variation of log(κ) on 9 challenging D4RL datasets
30
Under review as a conference paper at ICLR 2022
This section extends our analysis to the D4RL benchmark. We consider a set of challenging datasets
and domains from the benchmark in order to highlight the improvement in optimization objective
when using CQL-IM. We consider a set of diverse datasets (expert,medium,cloned) including tran-
sitions from the significantly challenging human demonstrations. In addition to these, we consider
domains with high-dimensional complex control (Franka and adroit) and sparse reward settings
(antmaze navigation).
Fig. 14	presents the variation in rank on 9 of the most challenging tasks for CQL(H) and its variants.
CQL(H) presents a faster collapse in rank of value function while CQLBohn (H) and CQL-IM are
found robust to these changes. We also note that while CQLBohn(H) is found more robust than CQL-
IM, it is due to its inability to learn in the presence of the assumption imposed on H. Value function
estimates of CQLBohn(H) either explode or collapse, hindering learning. In certain scenarios (such
as the hamnmer domain), CQLBohn(H) collapses midway due to the H assumption.
Fig. 15	presents the variation in κ (on log scale) for the considered 9 tasks. As before, CQL(H)
is found to maximize condition number values greater than CQLBohn (H) and CQL-IM. It is worth
noting the high errors in CQLBohn(H) for different runs. These indicate that although CQLBohn (H)
optimizes a well-conditioned objective, it still presents instabilities arising from the assumption on
H. CQL-IM, on the other hand, presents consistent values of log(κ) with fewer errors across random
runs.
Dataset	CQL(H)	CQLBohn(H)	CQL-IM
hopper-expert-v0	-242.81	407.26	-377.36
hopper-medium-v0	-274.01	134.81	-267.06
antmaze-umaze-v0	-23.48	40.61	-23.48
antmaze-umaze-diverse-v0	-37.63	88.97	-27.61
kitchen-complete-v0	4.6 × 1011	10.53 × 104	2.03 × 1011
kitchen-partial-v0	4.27 × 1011	8.65 × 106	3.71 × 1011
kitchen-mixed-v0	5.31 × 1011	3.33 × 104	3.47 × 1011
Table 3: Comparison of the gap Qk - Q between value estimates Qk and actual Monte-Carlo returns
Q. Note that negative values denote conservatism and positive values overestimations.
ʌ τ ,	, ∙ 1 ,	/rτ /c 八 F ♦ ,	∙	5 τ . ∙ ι ∙ . ι	, ∙ zA k- z^t
Next, we compare conservatism between CQL(H) and its variants. We utilize the metric Qk - Q
following Kumar et al. (2020b) which presents the gap between estimated Q values Qk and true Q
values Q. Estimated Q values are obtained by the critic function in the actor-critic framework of
CQL. True Q values Q, on the other hand, are obtained as Monte-Carlo returns by explicitly rolling
out the average discounted return. A negative value of Q k — Q denotes that the critic underestimates
true Q function while a positive value denotes overestimations. Optimal estimates are obtained in
the case of Qk = Q.
Table 3 presents the comparison of Qk — Q on hopper, antmaze and kitchen tasks. When compared
to CQL(H), CQLBohn (H) presents positive gaps demonstrating overestimations in the Q function.
CQL-IM, on the other hand, presents low negative values closer to 0 on hopper and antmaze tasks.
The result verifies our theoretical insight of CQL-IM being a better approximation to the true Q
function. We further note the large gaps on kitchen tasks which arise as a consequence of high-
dimensional control and significantly challenging datasets. To note the severity of these gaps, we
emphasize that CQL(H) itself finds it challenging to lower bound Qk. This results in large values
which are further observed in CQL-IM. However, it is also worth noting that these values for CQL-
IM are lower when compared to CQL(H). This indicates that the variational regularization aids
CQL-IM to bring values closer to the true Q function.
Lastly, we provide comprehensive comparison of average returns on the D4RL benchmark by ex-
panding our analysis to more recent baselines. We include policy constrained as well as value
constrained methods. Additionally, we include state-of-the-art algorithms which have demonstrated
success on learning locomotion, navigation and dexterous control from offline transistions. Namely,
31
Under review as a conference paper at ICLR 2022
we consider Bootstrapping Error Accumulation Reduction (BEAR) (Kumar et al., 2019), Behavior
Regularized Actor-Critic (BRAC) (both versions) (Wu et al., 2019), Advantage Weighted Regres-
sion (AWR) (Peng et al., 2019), AlgaeDICE (aDICE) (Nachum et al., 2019), Batch Constrained
Q-learning (BCQ) (Fujimoto et al., 2019) and Random Ensemble Mixture (REM) (Agarwal et al.,
2020). Additionally, we include the recent TD3+BC (Fujimoto & Gu, 2021) baseline which, similar
to our method, regularizes the value function with naive BC for improved runtime and simplicity. We
also add Uncertainty Weighted Actor-Critic (UWAC) (Wu et al., 2021) which demonstrates signifi-
cant improvements in learning from human demonstrations. Lastly, we include Implicit Q-Learning
(IQL) (Kostrikov et al., 2021) which highlights a new state-of-the-art in offline RL.
Table 4 presents the comparison between all methods on 13 tasks from the D4RL benchmark.
TD3+BC presents improvements on locomotion tasks. This arises as a result of the simplicity
of the algorithm where TD learning is combined with BC. UWAC presents improved returns on
complex human demonstration datasets with multi-modalities in data transitions. IQL, being the
recent state-of-the-art, improves over prior baselines on locomotion, navigation and dexterous con-
trol tasks. CQL(H) is found competitive to or improves over baseline methods. CQL-IM presents
similar or improved performance when compared to CQL(H). In the antmaze diverse and kitchen
partial settings, CQL-IM presents best returns by outperforming CQL(H) with a slight margin.
This improvement in performance is a direct consequence of variational regularization introduced
by the combination of BC objective. In settings where data transitions are optimal (such as expert),
CQL-IM learns quickly by leveraging good coverage of the dataset. In settings where dataset is sub-
optimal, the variational regularization in CQL-IM only improves the optimization process. This way,
CQL-IM enjoys the best of both worlds by interpolating between offline RL and Imitation Learning.
Before concluding, we also note the decrease in CQLBohn(H) returns which arises as a direct con-
sequence of the assumption on H interfering with optimization of value estimates.
I Implementation Details
I.1	LineWorld Details
The setup consists of two settings, namely online and offline agents. Policies corresponding to both
agents are approximated using neural networks with 1 hidden layer of 32 units with relu nonlinearity.
Both policies were trained with a learning rate of 0.001, batch size as 32 and γ = 0.95 as these were
the optimal parameters determined after a conventional grid search selection. Agent policies collect
data transitions every 20 steps in a rolling replay buffer with a capacity of 1000 transitions. In case of
the second experiment, the above parameters are kept same corresponding to both agents. The state
space of the environment is changed to the number of states in the set {10, 15, 100, 200, 500, 1000}.
I.2	Environment Details
At each timestep, the agent observes a 20 dimensional vector as its state consisting of the drone’s
position, translational and angular velocities, orientation and quaternion representations in 3D carte-
sian system. Following are the task setup corresponding to each environment for our experiments-
Takeoff: The agent is tasked to lift the drone along Z-axis of world frame. Reward awarded to the
agent is inversely proportional to the distance of drone’s center from goal position (0, 0, 1).
Hover: The agent is tasked to lift the drone along Z-axis of world frame and maintain its position
above the origin. Reward awarded to the agent is proportional to the norm distance of drone’s center
of mass from the goal position.
Flythrugate: The agent is tasked to fly through a gate opening placed at a wide angle on the drone’s
X -axis. Note that this is a significantly challenging scenario in comparison to other tasks as it
requires the agent to solve two problems. Firstly, the policy must learn to identify the location of the
gate. And secondly, the policy must learn to control the drone (by taking off and moving forward).
Reward awarded to the agent is proportional to the norm distance of drone’s center of mass from the
gate opening and inversely proportional to the time spent during simulation.
ZigZag: The agent is tasked to move in a zig-zag flight pattern during simulation. The agent must
first travel leftwards and then move rightwards towards its final goal location. Reward awarded to the
32
UnderreVieW as a ConferenCe PaPersICLR 2022
Dataset	SAC	BC	BEAR	BRAC-p	BRAC-v	AWR	aDICE	BCQ	REM	TD3+BC	UWAC	IQL	CQL(")	CQLBOhn(")	CQL-IM
hopper-expert-vO	0.7	109.0	110.3	6,6	-^3J	-	-	-	-	^^112.2^^	135.0	-	109.9^^	96.38	117.21
hopper-medium-vθ	0.8	29.0	47.6	31.2	32.3	35.9	1.2	54.5	0.6	99.5	88.9	-	58.0	51.78	64.22
antmaze-umaze-vθ	0.0	65.0	73.0	50.0	70.0	-	-	-	-	78.6	-	87.5	74.0	79.13	82.44
antmaze-umaze-diverse-vθ	0.0	55.0	61.0	40.0	70.0	-	-	-	-	71.4	-	62.2	84.0	62.97	85.32
antmaze-medium-play-vθ	0.0	0.0	0.0	0.0	0.0	-	-	-	-	10.6	-	71.2	61.2	53.45	61.76
antmaze-medium-diverse-vθ	0.0	0.0	8.0	0.0	0.0	-	-	-	-	3.0	-	70.0	53.7	34.55	52.19
kitchen-complete-vθ	15.0	33.8	0.0	0.0	0.0	-	-	-	-	-	-	62.5	43.8	48.22	54.25
kitchen-partial-vθ	0.0	33.8	13.1	0.0	0.0	-	-	-	-	-	-	46.3	49.8	32.76	52.43
3	kitchen-mixed-vθ	51.5	0.0	0.0	0.0	0.0	-	-	-	-	-	-	51.0	66.53	47.68	54.14
3	pen-human-vθ	6.3	34.4	-1.0	8.1	0.6	12.3	-3.3	68.9	3.5	-	65.0	71.0	46.56	41.97	54.59
pen-cloned-vO	23.5	56.9	26.5	1.6	-2.5	28.0	-2.9	44.0	-3.4	-	45.1	37.3	39.2	23.5	38.4
hammer-human-vθ	0.5	1.5	0.3	0.3	0.2	1.2	0.3	0.5	0.2	-	8.3	1.4	6.51	4.46	6.51
hammer-cloned-vO	0.2	0.8	0.3	0.3	0.3	0.4	0.3	0.4	0.2	-	1.2	2.1	1.61	0.4	1.46
door-human-vθ	3.9	0.5	-0.3	-0.3	-0.3	0.4	0.0	0.0	-0.1	-	10.7	4.3	9.9	5.2	10.1
door-cloned-vO	0.0	-0.1	-0.1	-0.1	-0.1	0.0	0.0	0.0	-0.1		1.2	1.6	0.4	0.0	0.4
Table 4: Average returns on all 13 D4RL datasets (results averaged over 4 random seeds). Values in bold denote highest returns. We additionally compare with
state-of-the-art baselines BEAR (Kumar et al., 2019), BRAC (Wu et al., 2019), AWR (Peng et al., 2019), AlgaeDICE (aDICE) (Nachum et alɔ 2019), BCQ
(Fujimoto et al., 2019), REM (Agarwal et al., 2020), TD3+BC (1 ujimoto & Gu, 2021), UWAC (WU et al., 2021) and IQL (Kostrikov et al., 2021).
Under review as a conference paper at ICLR 2022
agent is proportional to the norm distance of drone’s center of mass from the intermediate location
for first half of simulation, and the distance from goal location for second half of simulation.
I.3	Hyperparameters
All policies are trained for 1 × 105 timesteps and evaluated over 5 episodes every 1000 timesteps.
Experiments are carried out over 10 random seeds. All policies consists of two hidden layers of
1024 units each with ReLU activations and an output layer of 1024 units with tanh nonlinearity.
Following are training details corresponding to each algorithm-
BC: The expert, being a trained SAC policy, optimally executes actions in the environment for 5
episodes, resulting in state transitions. These transitions are stored in the dataset every 10000 steps.
BC learners were trained with a fixed batch size of 1024 and learning rate of 0.001 as these present
the best results for all tasks.
SAC: We utilize the diagonal Gaussian policy with standard deviation bounds rescaled in the range
[2,-10] and entropy temperature tuned manually to 0.001. We additionally tried lower values of
temperature but it led to deteriorating performance during training. Learning rate for all tasks was
tuned to 0.00003 with higher values presenting significant variance across random seeds.
CQL & Variants: Our CQL implementation is based on the original implementation of CQL(H)
(Kumar et al., 2020b) and makes use of the same architecture setup. The policy is trained with an
increased batch size of 1024 and reduced learning rate of 0.0001 for our experiments as this presents
marginal improvement. Our experiments additionally tried tuning the temperature parameters but
policies were found robust to these components. In the interest of a fair comparison with CQL, we
keep the parameter values of CQL-IM and CQLBohn (H) unchanged. Temperature parameter for
intrinsic motivation in CQL-IM was tuned with the optimal value being equivalent as the entropy
temperature for CQL.
I.4	D4RL DETAILS
Experiments in D4RL were conducted using the author-provided implementation of CQL(H). We
implemented CQL-IM similar to our Aerial Control tasks. BC, in its log form, was found to equally
work well. Following are the taskwise details of the implementation-
Franka: The only change we make is of temperature parameter for the BC term. We considered
values of 0.1, 0.01 & 0.001. The value of 0.01 was found to work best with lower values resembling
behaviors similar to CQL.
Adroit: Temperature parameter was tuned for 0.1 & 0.01 with 0.01 found to work well. Addition-
ally, we tuned the learning rate and found smaller values 1e-4 to work well across random runs.
Locomotion & Antmaze Navigation: We use author-provided default parameters with a tempera-
ture value of 0.001 tuned across 3 random seeds.
I.5	Additional Design Details
This section further elucidates the implementational aspects of our algorithms. Specifically, we
enlist the algorithms utilized in the study along with their advantages and limitations.
Computation of IM: We utilize the variational regularization as intrinsic motivation to the Q value
function. This is implemented using an off-policy variant in our Aerial control experiments and as
likelihood maximization in the D4RL setting. Algorithms 1 and 2 present their implementation with
the term temp as the temperature parameter to balance Q and IM terms. We omit the maximum
entropy regularization step as this is a fairly standard procedure implemented in the CQL framework.
Computation of Bohn(Q): We utilize efficient Jacobian Vector Product (JVP) and Hessian Vector
Product (HVP) to compute the Taylors formulation of Bohn(Q). We restate the quadratic formula-
tion with b = -(Hψ — g(ψ)) and C = 2ψTHψ — g(ψ)Tψ + lse(ψ),
Bohn(Q) = 1QTHQ - bQ + C	(157)
Here, we initialize ψ as a random parameter vector (alternatively a vector of 1s). At each step,
we exactly compute the above expression using efficient JVP (jvp) and HVP (hvp) functions.
34
Under review as a conference paper at ICLR 2022
Algorithm 3 presents the computation of Bohn(Q). The resulting expression is used as is in place
of lse(Q) in CQL.
In addition to Bohn(Q) and MI approximations, we considered other second order methods such
as conventional root finding and quasi-Newton methods. Although suitable for large-scale opti-
mization (Boyd & Vandenberghe, 2004; Nocedal & Wright, 2006), these methods did not provide
sufficient improvements in the RL setting. Most algorithms present large computational and memory
requirements which are difficult to fulfill in the offline RL setting. We provide a summary of these
limitations along with their time and memory complexities (with n as dimension of approximation
matrix) in Table 5.
Algorithm 1 CQL-IM (Off-Policy Variant PyTorch-like)
1:	Initialize temp=0.01, pi, critic, q, buffer, reg=1, T, gamma, CQL;
2:	while not converged do
3:	batch = buffer.sample()
4:	value = critic.forward(batch)
5:	for t in range(1,T) do
6:	regt = q.forward(batch)
7:	regt **= (gamma**t-1)
8:	reg *= regt
9:	end for
10:	reg = torch.log(reg)
11:	value += temp*reg.mean()
12:	CQL.train(batch, value)
13:	end while
14:	return pi
Algorithm 2 CQL-IM (PyTorch-like)
1:	Initialize temp=0.01, pi, critic, q, buffer, reg=1, T, gamma, CQL;
2:	while not converged do
3:	batch = buffer.sample()
4:	value = critic.forward(batch)
5:	reg = q.forward(batch)
6:	reg = torch.log(reg)
7:	value += temp*reg.mean()
8:	CQL.train(batch, value)
9:	end while
10:	return pi
Algorithm 3 Computation of Bohn(Q)
1	Initializejvp, hvp, H, ψ;
2	: procedure B OHN(ψ,Q)
3	:	lse = torch.logsumexp(ψ)
4	:	g = lse.grad()
5	一 一 .ʌ b = -(hvp(H, ψ) - g)
6	- - .一一 一 .ʌ first = 0.5*hvp(hvp(H, ψ),ψ)
7	:	c = first - jvp(g,ψ) + lse
8	bohn = 0.5*hvp(hvp(H, Q),Q) - jvp(b,Q) + c
9	:	return bohn
10	: end procedure
35
Under review as a conference paper at ICLR 2022
Approximation Method	Limitation	Computational Cost	Memory Cost
Newton’s Method	Requires exact computation of the inverse of full Hessian	O(n3)	O(n2)
Gauss-Newton Hessian	Requires solving the full high-dimensional Newton system with line search	O(n2)	O(n2)
Proximal Dogleg Method	Requires defining two hand-engineered thresholds	O(n2)	O(n2)
BFGS Method	Requires an inverse of the Hessian approximation with line search	O(n3)	O(n2)
Broyden Class	Requires an SVD with line search	O(n2)	O(n2)
Limited Memory Quasi-Newton	Requires two-loop recursions to track last iterates	O(n2)	O(n)
Table 5: Additional approximation methods considered with their practical limitations.
The advantages of Bohning approximation when compared to methods in Table 5 are as follows-
•	Bohn(Q) does not require inverse of the approximation
•	Bohn(Q) does not require line search
•	Bohn(Q) does not require hand-engineered thresholds
•	Bohn(Q) does not require recursions or SVD
•	The method is simple to implement (5 lines of code)
•	Approximation is easier to optimize (in libraries such as JAX)
•	The method is easier to scale to higher dimensions by using a limited memory variant
The computational complexity of Bohn(Q) is same as O(n2) and its memory complexity matches
the O(n) rate of Limited Memory Quasi-Newton method.
36