Under review as a conference paper at ICLR 2022
Federated causal discovery
Anonymous authors
Paper under double-blind review
Ab stract
Causal discovery aims to learn a causal graph from observational data. To date,
most causal discovery methods require data to be stored in a central server. How-
ever, data owners gradually refuse to share their personalized data to avoid privacy
leakage, making this task more troublesome by cutting off the first step. A puzzle
arises: how do we infer causal relations from decentralized data? In this paper,
with the additive noise model assumption of data, we take the first step in devel-
oping a gradient-based learning framework named DAG-Shared Federated Causal
Discovery (DS-FCD), which can learn the causal graph without directly touching
local data and naturally handle the data heterogeneity. DS-FCD benefits from a
two-level structure of each local model. The first level learns the causal graph and
communicates with the server to get model information from other clients, while
the second level approximates causal mechanisms and personally updates from its
own data to accommodate the data heterogeneity. Moreover, DS-FCD formulates
the overall learning task as a continuous optimization problem by taking advan-
tage of an equality acyclicity constraint, which can be naturally solved by gradient
descent methods. Extensive experiments on both synthetic and real-world datasets
verify the efficacy of the proposed method.
1	Introduction
The discovery of causal relations among concerned variables is a fundamental and challenging prob-
lem in various fields, such as econometrics (Heckman, 2008), epidemiology (Greenland et al., 1999),
and biological sciences (Imbens & Rubin, 2015). The requirement comes from the need of excavat-
ing the generation process behind data, guiding actions and policies, learning from the past (Pearl
et al., 2016). To achieve this goal, a reliable way is to conduct randomized controlled (control) tri-
als, which, however, may face difficulty or even be ethically forbidden in some cases (Resnik, 2008;
Nardini, 2014). By leveraging the use of directed acyclic graphs (DAGs) to describe the cause-effect
relations among variables, causal discovery (CD), which directly infers the causal relations from ob-
servational data by learning a DAG, brings a new solution for this problem and has received a great
deal of attention (Peters et al., 2017; Glymour et al., 2019).
Various methods (Spirtes et al., 2001; Chickering, 2002; Shimizu et al., 2006; Zheng, 2020) for
learning causal relations from purely observational data have been proposed over the recent decades.
Regularly, (1) collecting data from various sources and then (2) designing a CD algorithm on all
collected data are the common pipeline in this field. However, owing to the issue of data privacy,
data owners gradually prefer not to share their personalized data with others (Kairouz et al., 2019).
Naturally, the new predicament, how do we infer causal relations from decentralized data? has
arisen. In statistical learning problems such as regression and classification, federated learning (FL)
has been proposed to learn from locally stored data (McMahan et al., 2017). Inspired by these
developments in FL, we aim to develop a federated causal discovery (FCD) framework that enables
to learn DAG from decentralized data. Compared to FL in statistical learning, FCD, a structural
learning task, has the following two main differences:
•	Learning objective difference. FL aims to learn an estimator to fit the given conditional distribu-
tion while FCD tries to find the underlying causal structure and the causal mechanism estimator
to fit with the joint distribution of observations.
•	Data heterogeneity difference. FL usually cares about classification tasks where data heterogene-
ity is assumed by some specific distribution shift types such as label shift (the shift of P(Y )) (Lip-
ton et al., 2018) or covariate shift (the shift of P (X)) (Reisizadeh et al., 2020), while FCD handles
1
Under review as a conference paper at ICLR 2022
(a)	(b)
Figure 1: (a) Visualization of Non-IID data. (b) Normalized structural hamming distance (SHD)S
(1)of learned DAG, where MCSL(SeP) (Ng et al., 2019) separately trains model on local dataset
while MCSL(All) trains one model on all data, which however is forbidden in FL.
a generative model where data heterogeneity means the joint distribution shift of all variables (as
shown in Figure 1(a)), which would bring more challenges compared to the model design in the
federated learning paradigm.
To overcome the aforementioned problem, we present DAG-Shared Federated Causal Discovery
(DS-FCD), a gradient-based framework for learning the underlying causal graph from decentralized
data, including the case of Non-Independent and Identically Distributed (Non-IID) data. (1) To al-
leviate the data leakage problem, DS-FCD inherits the merits of FL, which proposes to separately
deploy a local model to each client and collaboratively learn a joint model at the server end. In-
stead of sharing raw data, DS-FCD exchanges model-info among clients and the server to achieve
collaboration. (2) Taking into consideration of the first main difference between FCD and FL, a
two-level structure consisting of causal graph learning (CGL) part and causal mechanism approxi-
mating (CMA) part respectively, is adopted as the local model. (3) Benefiting from this separated
structure, the second difference between FL and FCD can naturally be handled by only sharing CGL
parts of clients during FL and locally updating CMA to get with data heterogeneity. Moreover,
we provide the identifiability conditions for learning the causal graph from decentralized data. Our
contributions are summarized as follows:
•	We introduce FCD, under the assumption that the underlying causal graph among different
datasets remains invariant, while causal mechanisms are allowed to vary when it comes to the
Non-IID setting. We also show the identifiability conditions ofCD from decentralized data.
•	We propose DS-FCD, which separately learns the causal mechanisms on local data and jointly
learns the causal graph to elegantly handle data heterogeneity. Meanwhile, since 0 bits of raw data
is shared but only the CGL parts of models, the privacy protection requirement is guaranteed and
the communication pressure is quite low.
•	We evaluate our proposed framework with data that follows a SEM with an additive noise structure
on a variety of experimental settings, including simulated ablations and real dataset, against recent
state-of-the-art algorithms for showing its superior performance and the ability to use one model
for all settings.
2	Preliminaries
Additive Noise Models (ANM). A causal model is defined as a triple M = hX , E, Fi where E is
a set {eι, e2,…，ed} of exogenous variables and X = {Xι, X2,…，Xd} is a set of endogenous
variables. F = {fι, f2,…，fd} is a set of functions, where each fi, called the causal mechanism of
Xi, maps ei ∪ PAi to Xi, i.e., Xi = fi(PAi， ei), where the PAi correspond to the direct parents of
Xi . M can be leveraged to describe how nature assigns values to variables of interest (Pearl et al.,
2016). Here, we narrow our focus to a commonly used model named ANM, which assumes
Xi = fi(PAi) + ei,	i = 1, 2,…，d,	(1)
2
Under review as a conference paper at ICLR 2022
where i is always taken as a random noise, which is independent of variables in PAi and mutually
independent with any j for i 6= j .
Probabilistic Causal Graphical Models (PCGM). Let X = (Xi, X2,…，Xd) be a vector that
includes all variables in X with index set V := {1,2, ∙∙∙ , d} and P(X) be a marginal distribution
induced from M. A DAG G = (V, E) consists of a nodes setVanda edge setE ⊆ V1 2. Every causal
model M can be associated with a DAG GM , in which each node i corresponds to a variable Xi
and directed edges point from PAi to Xi1 for i ∈ [d]2. A PCGM is defined as a pair hP(X), GMi.
Then GM is called the causal graph associated with M and P(X) is Markovian to GM. Thoughout
this paper, we assume Causal Sufficiency (no hidden variable) (Spirtes et al., 2001) and then P(X)
can be factorized as
d
P(X) = YP(Xi|Xpai)	(2)
i=1
according to GM (Lauritzen, 1996). Xpai is the parental vector that includes all variables in PAi .
Characterizations of Acyclicity A DAG G with d nodes can be represented by a binary adjacency
matrix B = [B：,i |B：,2| …|B：,d] with B：,j ∈ {0,1}d. NOTEARS (Zheng et al., 2018) first formu-
lates a sufficient and necessary condition for B representing a DAG by an equality constraint. The
formulation is as follows:
Tr[eB] - d = 0,	(3)
where Tr[∙] means the trace of a given matrix. e(∙), here, is the matrix exponential operation. To
solve the non-linear model, Ng et al. (2019) proposes to use a mask M, parameterized by a con-
tinuous proxy matrix U , to approximate the adjacency matrix B . To enforce the entries of M to
approximate the binary form, i.e., 0 or 1. A two-dimensional version of Gumbel-Softmax (Jang
et al., 2017) approach named Gumbel-Sigmoid is designed to reparameterize U and to ensure the
differentiability of the model. Then, M can be obtained element-wisely by
Mdd =----------:-----:-----------------.	(4)
1 + exp(-log( Uj + Gumbij )∕τ)
where τ is the temperature, Gumbij = gi1j - gi0j , gi1j and gi0j are two independent samples from
Gumbel(0, 1). For simplicity but equivalence, gi1j and gi0j also can be sampled from -log(log(a))
with a 〜Uniform(0,1). The proof can be found in Appendix D of (Ng et al., 2019). MCSL names
Eq. (4) as Gumbel-Sigmoid w.r.t. U and temperature τ, which is written as gτ (U). Then, the
acyclicity constraint can be reformulated as
Tr[e(gτ(U))] - d= 0.	(5)
3	Problem definition
Here, we first describe the property of decentralized data and the mechanisms of distribution shift
among different clients if there exists data heterogeneity (Huang et al., 2020b; Mooij et al., 2020).
Then, we define the problem, federated causal discovery, considered in this paper.
Decentralized Data and Probability distribution set. Let C = {ci, e2, ∙∙∙ , cm} be the client set
which includes m different clients and S be the only server. The data Dck ∈ Rnck ×d, in which
each observation Dick for ∀i ∈ [nck ] independently sampled from its corresponding probability
distribution Pck (X), represents the personalized data owned by the client ck, where nck is the
number of observations of Dck. The dataset D = {Dc1, Dc2, ∙∙∙ , Dcm } is called a decentralized
dataset and PC (X) = {Pc1 (X), Pc2 (X), .…，P cm (X)} is defined as the decentralized probability
distribution set. IfPck1 (X) = Pck2 (X) for∀ki, k2 ∈ [m], then D is defined as an independent and
identically distributed (IID) decentralized dataset throughout this paper. The Non-IID decentralized
dataset is defined by assuming that there exists at least two clients, e.g., ck1 and ck2, on which the
local data are sampled from different distributions, i.e., Pck1 (X) 6= Pck2 (X).
Assumption 1 (Invariant DAG) For ∀ck, Pck (X) ∈ PC (X) admits the product factorization of
Eq. (2) relative to the same DAG G.
1In this paper, G is only defined over the endogenous variables.
2For simplicity, We use [d] = {1,2, •一，d} to represent the set of all integers from 1 to d.
3
Under review as a conference paper at ICLR 2022
With Assumption 1, it is easy to conclude that distribution shift across Pck (X) comes from the
change of causal mechanisms F or distribution shift of the exogenous variables E . That is to
say, if P (Xck1 ) 6= P (Xck2 ), then, at least one of the following cases occurs. (1) ∃i ∈ [d],
Pck1(Xi|Xpai) 6=Pck2(Xi|Xpai),i.e.,fick1 6= fick2 (2)∃i∈ [d], P ck1 (i) 6=Pck2(i).
Federated Causal Discovery. Given the decentralized dataset D consisting of data from m clients
while the corresponding PC (X) satisfies Assumption 1, the aim of federated causal discovery is to
identify the underlying DAG G from D .
4	Methodology
To solve the aforementioned problem, we formulate a continuous score-based method named DAG-
shared federated causal discovery (DS-FCD). Firstly, we define an objective function that guides
all models from different clients to federally learn the underlying causal graph G (or adjacency
matrix B), and at the same time also learn personalized causal mechanisms for each client. As
shown in Figure 2, for each client ck, the local model consists of a causal graph learning (CGL) part
and a causal mechanisms approximation (CMA) part. The CGL part is parameterized by a matrix
Uck ∈ Rd×d, which would be exactly the same for all clients finally3. To leverage every entry
of Uck for approximating the binary entry of adjacency matrix. A Gumbel-Sigmoid method (Jang
et al., 2017; Ng et al., 2019) represented as gτ(Uck), is further leveraged to transform Uck to a
differentiable approximation of the adjacency matrix. The causal mechanisms fck, f2k,…,f(Ck
are parameterized by d sub-networks, each of which has d inputs and one output. In the learning
process, the CGL parts (specifically Uck) of participating clients are shared with the server S. Then,
the processed information is broadcast to each client for self-updating its own matrix. The details of
our method are demonstrated in the following subsections.
4.1	The Overall Learning objective
Now we present the overall learning objective of FCD as the following optimization problem:
m
arg max X Sck(Dck,Φck,U)
Φ,U	k=1	(6)
subject to gτ(U) ∈ DAGs ⇔ h(U) = Tr[e(gτ(U))] - d = 0,
where Φck := {Φfk, Φ∣k,…，Φ(k} represents the CMA part of the model on Ck. SCk (∙) is the
scoring function for evaluating the fitness of local model of client ck and observations Dck . For
score-based causal discovery, selecting a proper score function such as BIC score (Schwarz, 1978)
or Generalized score (Huang et al., 2018) for the corresponding data generation model can guarantee
to identify up the underlying ground-truth causal graph G because G is supposed to have the maximal
score over Eq. (6). However, the global minimum is hard to reach by using gradient descent method
due to the non-convexity of h(U).
In this paper, the likelihood part leverages the distribution of P (E), i.e., P(E|FCk, G). According to
Eq. (1), we have i = Xi - fi(PAi). To get i, the first step is to select the parental set PAi for
Xi. This can be achieved by B[:, i] ◦ X, where ◦ means the element-wise product. In our paper, for
client ck, we predict the noise by i = Xi - Φi (gτ (U)[:, i] ◦ X), where gτ(U) is to approximate B
and Φi(∙) is parameterized by a neural network to approximate fi. The specific formulation of SCk
would depend on the assumption of noise distribution.
4.2	DAG-shared learning
As suggested in NOTEARS (Zheng et al., 2018), the hard-constraint optimization problem can be
addressed by an Augmented Lagrangian method (ALM) to get an approximate solution. Similar to
penalty methods, ALM transforms a constrained optimization problem by a series of unconstrained
sub-problems and adds a penalty term to the objective function. ALM also introduces a Lagrangian
3Since CGL parts of different clients may not be the same during training, we index them.
4
Under review as a conference paper at ICLR 2022

MDcl∙jg
Figure 2: An overview of DS-FCD. Each solid-line box includes CD on each local client. For client
ck, the CGL part includes a continuous proxy UCk and gτ(∙), the GUmbel-Sigmoid function, which
maps Uck to approximate the binary causal graph. The CMA part uses Φck, a neural network, to
approximate the causal mechanisms. XCk represents observations on Ck and XCk is the predicted
data. X ck firstly goes through the CGL part to select the parental variables and then the CMA part
to get XCk. The server coordinates the FL procedures by leveraging U among clients.
multiplier term to avoid ill-conditioning by preventing the coefficient of penalty term from going
too large. To solve Eq. (6), the sub-problem can be written as
m
arg max X SCk (Φck,Dfk,gτ(U)) - αth(U) - Pth(U)2,	⑺
where αt and ρt are the Lagrangian multiplier and penalty parameter of the t-th sub-problem, re-
spectively. These parameters are updated after the sub-problem is solved. Since neural networks are
adopted to fit the causal mechanisms in our work, there is no closed form for Eq. (7). We solve it
approximately via Adam (Kingma & Ba, 2015). The method is described in Algorithms 1 and 2.
Algorithm 1 DAG-Shared Federated Causal Discovery (DS-FCD)
Inputs: D, C, Parameter-list = {αinit , ρinit , htol , itmax , ρmax , β, γ, r}.
Output: Egτ (Ut), Φt
1:	t — 1,αt — αinit,pt — Pinit	. Parameter Initializing
2:	while t ≤ itmax and h(Ut) ≥ htol and ρ ≤ ρmax do
3:	Ut+ι, Φt+ι — SPS(D, C,αt,pt, itin ,itfi ,r)	. Sub-problem Solving
4:	αt+ι - at + ptE[h(Ut+ι)], t - t + 1	. Coefficients Updating
5:	if E[h(Ut+1)] > γE[h(Ut)] then
6:	ρt+1 = βρt
7:	else
8:	ρt+1 = ρt
9:	end if
10:	end while
Each sub-problem as Eq. (7) is solved mainly by distributing the computation across all clients.
Since data is prevented from sharing among clients and the server, each client owns its personalized
model, which is only trained on its personalized data. The server communicates with clients by
exchanging the parameters information of models and coordinates the joint learning task. To achieve
so, our method alternately updates the server and clients in each communication round.
Client Update. For each model of client ck, there are two main parts, named CGL part parame-
terized by UCk and CMA part parameterized by ΦCk, respectively. Essentially, the joint objective
in Eq. (7) guides the learning process. In the self-updating as described in Algorithm 2, client ck
5
Under review as a conference paper at ICLR 2022
Algorithm 2 Sub-Problem Solver (SPS) for DS-FCD
Input: D, C, Parameter-list = {αt, ρt, itin, itfl , r}.
Output: Unew , Φitin
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Define SPck = Sck - α-h(U)-卷h(U)2
for iin(1, 2,…，itin) do
for each client ck do
Ui,ck, Φi,ck - arg maXφCk U SPck	. Self-updating
end for
if i (% itfl) = 0 or i = itin then
U — Agg(r, C) . Model aggregating: select r clients and collect their US into U, send U to server
Unew J Avg(U)	. Server Updating: average U ∈ U
C J BD(Unew)	. Broadcasting: distribute Unew to all clients
for each client ck do
U i,ck J Unew	. Clients Updating
end for
end if
end for
makes itfl local gradient-based parameter updates to maximize its personalized score defined as
SPck = Sck - αth(U) - Ph(U)2. The other Clients Update procedure is a federally update of
Uck. After receiving Unew from the server S, each client directly replaces its Uck by Unew.
Server Update. After itfl local updates, the server S randomly chooses r clients to collect their Us
to the set U. Then, Us in U are averaged to get Unew, which is then broadcast to clients. Notice
that with assuming that data distribution across clients is IID, Φck of the chosen r clients can also
be collected and averaged to update clients’ local models, which is named as All-Shared FCD (AS-
FCD) in this paper. It is worth noting that AS-FCD can further enhance the performance in the IID
case but introduce some additional communication costs.
4.3 Thresholding
As illustrated in the previous works (Zheng et al., 2018; Ng et al., 2019), the solution of ALM just
satisfies the numerical precision of the constraint. This is because we set htol and itmax maximally
but not infinite coefficients of penalty terms to formulate the last sub-problem. Therefore, some
entries of the output M = Egτ (U) will be near but not exactly 0 or 1. To alleviate this issue, `1
sparsity is added to the objective function. Moreover, after obtaining M , we use a hard threshold
0.5 to prune the dense graph. If G(M) is still not a DAG, we take iterative thresholding to cut off
the edge with the minimum weight until the graph is acyclic.
4.4 Privacy and Costs Discussion
Privacy issues of DS-FCD. The strongest motivation ofFL is to avoid personalized raw data leak-
age. To achieve this, DS-FCD proposes to exchange the parameters for modelling the causal graph.
Here, we argue that the information leakage of local data is rather limited. The server, receiving
parameters with client index, may infer some data property. However, according to the data genera-
tion model (1), the distribution of local data is decided by (1) causal graph, (2) noise types/strengths
and (3) causal mechanisms. The gradient information of the shared matrix is decided by (1) the
type of learning objective and (2) model architecture, which are agnostic to the server. Especially
for the network part, clients may choose different networks to make the inference more complex.
Furthermore, one may leverage some advanced methods (Wei et al., 2020b) for easing this issue, but
this is beyond the main scope of our study. Moreover, if the causal graph is also private information
for clients, this problem can be easily solved by selecting a client to serve as the proxy server. For
the proxy server, it needs to play two roles, including training its own model and taking the server’s
duties. Then, in the communication round, other clients communicate with the proxy server instead
of a real server.
Communication cost. Since DS-FCD requires exchanging parameters between the server and
clients, additional communication costs are raised. In our method, however, we argue that DS-
FCD only brings rather small additional communication pressures. For the case of d variables, a
6
Under review as a conference paper at ICLR 2022
single communication only exchanges a d × d matrix twice (sending and receiving). For the IID
setting, which assumes that local data are sampled from the same distribution, one can also transmit
the neural network together to further improve the performance since causal mechanisms are also
shared among clients. The trade-off between performance and communication costs can also be
controlled by r in Algorithm2, i.e., enlarging or reducing r. Surprisingly, we find that reducing r
does not harm the performance severely (see Table 17 in Appendix A.5 for detailed results).
5 Experimental Results
In this section, we study the empirical performances of DS-FCD on both synthetic and real-world
data. More detailed ablation experiments also can be found in Appendix A.5.
Baselines and Metrics. We compare our method with various baselines including some continu-
ous search methods, named NOTEARS (Zheng et al., 2018), NOTEARS-MLP(Zheng et al., 2020),
DAG-GNN (Yu et al., 2019) and MCSL (Ng et al., 2019), and also two traditional combinatorial
search methods named PC (Spirtes et al., 2001) and GES (Chickering, 2002). We provide two
training ways for these compared methods. The first way is using all data to train only one model,
which, however, is not permitted in FCD since the ban of data sharing in our setting. For the IID
data, the results on this setting can be an approximate upper bound of our method but unobtainable.
The second one is separately training each local model over its personalized data, of which the per-
formances reported are the average results of all clients. We report two metrics named Structural
Hamming Distance (SHD) and True Positive Rate (TPR) averaged over 10 random repetitions to
evaluate the discrepancies between estimated DAG and the ground-truth graph G . Notice that PC
and GES can only reach the completed partially DAG (CPDAG, or MEC) at most, which shares the
same Skelton with the ground-truth DAG G. When we evaluate SHD, we just ignore the direction of
undirected edges learned by PC and GES. That is to say, these two methods can get SHD 0 if they
can identify the CPDAG. The implementation details of all methods are detailed in Appendix A.3.
5.1	Synthetic data
The synthetic data we consider here is generated from Gaussian ANM (Model (1)). The score func-
tion used in all experiments can be seen in Appendix A.1. TWo random graph models named Erdos-
Renyi (ER) and Scale-Free (SF) are adopted to generate causal graph G. And then, for each node Vi
corresponding to Xi in G, We sample a function from the given function sets to simulate fi . Finally,
data are generated according to a specific sampling method. In the folloWing experiments, We take
10 clients and each With 600 observations throughout this paper. According to Assumption 1, data
across all clients share the same causal graph for both IID and Non-IID settings.
5.1.1	IID setting
For the IID setting, all data are generated by an ANM and divided into 10 pieces. Each fi is sampled
from a Gaussian Process (GP) With RBF kernel of bandWidth one (See Table 14 and Table 15 in
Appendix A.5 for results of other functions.) and noises are sampled from one zero-mean Gaussian
distribution With fixed variance. We consider graphs of d nodes and 2d expected edges.
Experimental results are reported in Table 1With nodes 10 and 40. Since all local data are IID,
here, We also provide another effective training method named AS-FCD, in Which the CMA parts
are also shared among clients. In all settings, AS-FCD shoWs a better performance than DS-FCD
due to that more model information are shared during training. While DS-FCD can also shoW a
consistent advantage over other methods. When separately training local models, all models suffer
from data scarcity. Therefore, We can observe that both DS-FCD and AS-FCD perform better than
other methods in the fashion of separate training. NOTEARS and DAG-GNN, as continuous search
methods, obtain unsatisfactory results due to the Weak model capacity and improper model assump-
tion. While BIC score of GES gets a linear-Gaussian likelihood, Which is incapable to deal With
non-linear data4. With the number of nodes increasing, DS-FCD still shoWs better results than the
closely-related baseline method MCSL. HoWever, NOTEAES-MLP can shoW a comparable result
With DS-FCD oWing to the advantage over MCSL.
4Please find the ablation experiment With linear data and more discussions of the experimental results in
Section A.4 of the Appendix
7
Under review as a conference paper at ICLR 2022
Table 1: ResUlts on nonlinear ANM With GP (IID).
ER2 with 10 nodes SF2 with 10 nodes	ER2 with 40 nodes	SF2 with 40 nodes
atad pe
	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
PC	15.3 ± 2.6	0.37 ± 0.10	14.1 ± 4.3	0.44 ± 0.20	84.9 ± 13.4	0.40 ± 0.08	95.0 ± 10.4	0.36 ± 0.07
GES	13.0 ± 3.9	0.50 ± 0.18	9.6 ± 4.4	0.71 ± 0.17	59.0 ± 9.8	0.53 ± 0.08	73.8 ± 11.9	0.47 ± 0.10
NOTEARS	16.5 ± 2.0	0.05 ± 0.04	14.5 ± 1.1	0.09 ± 0.07	71.2 ± 7.2	0.08 ± 0.03	70.8 ± 2.3	0.07 ± 0.03
N-S-MLP	8.1 ± 3.8	0.56 ± 0.17	8.3 ± 2.8	0.51 ± 0.16	45.3 ± 6.8	0.43 ± 0.08	49.2 ± 7.7	0.39 ± 0.09
DAG-GNN	16.2 ± 2.1	0.07 ± 0.06	15.2 ± 0.8	0.05 ± 0.05	73.0 ± 7.7	0.06 ± 0.03	72.4 ± 1.6	0.05 ± 0.02
MCSL	1.9 ± 1.5	0.90 ± 0.08	1.6 ± 1.2	0.91 ± 0.07	25.4 ± 13.1	0.68 ± 0.14	31.6 ± 10.0	0.59 ± 0.13
PC	14.1 ± 2.4	0.31 ± 0.06	13.6 ± 2.7	0.30 ± 0.10	83.8 ± 7.4	0.24 ± 0.03	86.1 ± 4.6	0.23 ± 0.04
GES	12.7 ± 2.7	0.37 ± 0.09	12.7 ± 2.4	0.33 ± 0.11	71.0 ± 6.7	0.29 ± 0.03	73.2 ± 4.4	0.29 ± 0.05
NOTEARS	16.5 ± 2.0	0.06 ± 0.04	14.6 ± 1.0	0.09 ± 0.06	71.1 ± 7.3	0.08 ± 0.03	70.7 ± 2.0	0.07 ± 0.03
N-S-MLP	8.5 ± 2.9	0.56 ± 0.13	8.7 ± 2.9	0.53 ± 0.16	51.0 ± 6.9	0.41 ± 0.06	53.6 ± 5.5	0.39 ± 0.08
DAG-GNN	15.7 ± 2.3	0.11 ± 0.05	14.5 ± 1.0	0.10 ± 0.06	71.5 ± 7.5	0.08 ± 0.02	70.8 ± 1.8	0.07 ± 0.02
MCSL	7.1 ± 3.2	0.83 ± 0.08	6.9 ± 2.8	0.84 ± 0.08	77.3 ± 19.8	0.64 ± 0.11	72.9 ± 16.4	0.58 ± 0.13
DS-FCD	2.4 ± 2.0	0.86 ± 0.13	2.7 ± 2.2	0.86 ± 0.13	36.5 ± 12.1	0.65 ± 0.15	46.4 ± 10.4	0.57 ± 0.13
AS-FCD	1.8 ± 2.0	0.89 ± 0.12	2.5 ± 2.7	0.85 ± 0.15	30.0 ± 12.3	0.74 ± 0.15	31.5 ± 10.0	0.59 ± 0.13
5.1.2	Non-IID setting
As defined in Section3, the Non-IID property of data across clients come from the changes of caUsal
mechanisms or the shift of noise distribUtions. To simUlate the Non-IID data, we firstly generate
a DAG shared by all clients and then decide the types of caUsal mechanisms fick and noises i for
i ∈ [d] for each client ck. In oUr experiments, We allow that fck can be linear or non-linear for
each client. If being linear, fck here is a weighted adjacency matrix with coefficients sampled from
Uniform ([-2.0, -0.5] ∪ [0.5, 2.0]), with eqUal probability. If being non-linear, fick is independently
sampled from GP, GP-add, MLP or MIM fUnctions (YUan, 2011), randomly. Then, a fixed zero-mean
GaUssian noise is set to each client with a randomly sampled variance from {0.8, 1}.
We can see that the conclUsion of experimental resUlts on the Non-IID setting is rather similar to
that of the IID. As can be read from Table 2, DS-FCD always shows the best performances across
all settings. If taking all data together to train one model Using other methods, we can see that data
heterogeneity woUld pUt great troUble to all compared methods while DS-FCD plays pretty well.
Moreover, DS-FCD shows consistent good resUlts with different nUmbers of observations on each
client (see Table 16). NOTEARS takes second place at the setting of 40 nodes becaUse there are
some linear data among clients, which is also the reason that DS-FCD shows lower SHDs on Non-
IID data in Table 2 than Table 1. Compared with Non-linear models, NOTEARS easily fits well
with even fewer linear data.
Table 2: ResUlts on ANM (Non-IID).
ER2 with 10 nodes SF2 with 10 nodes	ER2 with 40 nodes	SF2 with 40 nodes
SHD ； TPR ↑ SHD ； TPR ↑	SHD ； TPR ↑ SHD ； TPR ↑
PC	22.3 ± 4.2	0.41 ± 0.11	21.0 ± 3.6	0.41 ± 0.12	151.9 ± 14.2	0.27 ± 0.08	152.5 ± 5.4	0.26 ± 0.04
GES	26.4 ± 6.2	0.53 ± 0.14	25.4 ± 4.6	0.54 ± 0.13	NaN	NaN	NaN	NaN
NOTEARS	20.4 ± 4.1	0.49 ± 0.14	18.7 ± 3.3	0.45 ± 0.11	164.8 ± 47.4	0.39 ± 0.07	178.1 ± 33.0	0.40 ± 0.10
N-S-MLP	22.8 ± 5.0	0.87 ± 0.07	24.7 ± 3.3	0.88 ± 0.07	344.4 ± 71.9	0.92 ± 0.08	325.0 ± 50.2	0.85 ± 0.08
DAG-GNN	21.2 ± 6.0	0.39 ± 0.11	16.6 ± 3.0	0.48 ± 0.18	146.6 ± 41.6	0.29 ± 0.08	168.2 ± 34.2	0.31 ± 0.09
MCSL	19.4 ± 4.4	0.75 ± 0.19	19.0 ± 4.0	0.81 ± 0.14	118.6 ± 18.1	0.68 ± 0.11	126.9 ± 16.5	0.59 ± 0.12
atad pe
PC	12.5 ± 2.7	0.45 ± 0.07	11.0 ± 2.1	0.49 ± 0.07	65.7 ± 11.0	0.43 ± 0.06	73.7 ± 5.5	0.36 ± 0.05
GES	12.9 ± 2.6	0.58 ± 0.07	10.3 ± 2.8	0.60 ± 0.09	68.2 ± 20.8	0.65 ± 0.09	77.2 ± 13.8	0.60 ± 0.07
NOTEARS	7.6 ± 2.6	0.60 ± 0.11	7.6 ± 1.8	0.58 ± 0.09	34.9 ± 12.7	0.63 ± 0.11	43.4 ± 8.4	0.53 ± 0.10
N-S-MLP	5.2 ± 1.4	0.80 ± 0.05	6.1 ± 1.6	0.76 ± 0.05	46.0 ± 10.2	0.73 ± 0.08	56.0 ± 9.5	0.66 ± 0.09
DAG-GNN	8.2 ± 2.9	0.67 ± 0.12	8.4 ± 2.1	0.67 ± 0.09	45.7 ± 13.5	0.64 ± 0.11	52.7 ± 8.4	0.60 ± 0.11
MCSL	9.2 ± 1.8	0.72 ± 0.06	8.9 ± 2.0	0.71 ± 0.08	76.1 ± 13.7	0.53 ± 0.09	78.1 ± 6.3	0.47 ± 0.07
DS-FCD 1.9 ± 1.6	0.99 ± 0.02 I 2.6 ± 1.3	0.93 ± 0.07 I 24.3 ± 10.2	0.86 ± 0.09 I 33.9 ± 10.9	0.73 ± 0.09
5.2 Real data
We consider a real pUblic dataset named fMRI Hippocampus (Poldrack et al., 2015) to discover the
caUsal relations among six brain regions. This dataset records signals from six separate brain regions
in the resting state of one person in 84 sUccessive days and the anatomical strUctUre provides 7 edges
8
Under review as a conference paper at ICLR 2022
as the ground truth graph (see Figure A.5 in Appendix A.5). Herein, we separately select 500 records
in each of 10 days (see Figure 7 for the normalized data distribution in Appendix A.5), which can
be regarded as different local data. It is worth noting that though this data does not have a real data
privacy problem, we can use this dataset to evaluate the learning accuracy of our method. Here, in
Table 3 we show part of the experimental results while others lie in Table 18 (Appendix A.5). AS-
FCD shows the best performance over all criterion while DS-FCD also performs better than most of
the other methods.
Table 3: Empirical results on fMRI Hippocampus dataset (Part 1).
	All data			Separate data			DS-FCD	AS-FCD
	PC	NOTEARS	MCSL	PC	NOTEARS	MCSL		
SHD J	9.0 ± 0.0	5.0 ± 0.0	9.0 ± 0.6	8.7 ± 1.3	8.0 ± 1.9	8.3 ± 1.7	6.4 ± 0.9	5.0 ± 0.0
NNZ	11.0 ± 0.0	4.0 ± 0.0	12.0 ± 0.6	7.6 ± 1.3	5.4 ± 1.5	9.0 ± 1.7	6.8 ± 0.6	5.0 ± 0.0
TPR ↑	0.43 ± 0.00	0.29 ± 0.00	0.44 ± 0.04	0.26 ± 0.11	0.19 ± 0.18	0.35 ± 0.15	0.27 ± 0.12	0.29 ± 0.00
FDR J	0.73 ± 0.00	0.50 ± 0.00	0.74 ± 0.03	0.76 ± 0.10	0.78 ± 0.19	0.73 ± 0.11	0.72 ± 0.11	0.60 ± 0.00
S
6	Related work
Two mainstreams named constraint-based and score-based methods push the development of causal
discovery. Constraint-based methods, including PC and fast causal inference (FCI) (Spirtes et al.,
2001), take conditional independence constraints induced from the observed distribution to decide
the graph skeleton and part of the directions. Another branch of methods (Chickering, 2002) define
a score function, which evaluate the fitness between the distribution and graph, and identify the
graph G with the highest score after searching the DAG space. To avoid solving the combinatorial
optimization problem, NOTEARS (Zheng et al., 2018) introduces an equivalent acyclicity constraint
and formulates a fully continuous optimization for searching the graph. Following this work, many
works leverages this constraint to non-linear case (Zheng et al., 2020; Lachapelle et al., 2020; Zhu
et al., 2020; Wang et al., 2021), interventional data (Brouillard et al., 2020), time-series data (Pamfil
et al., 2020), and unmeasured confounding (Bhattacharya et al., 2021). DAG-NoCurl (Yu et al.,
2021) and NOFEARS (Wei et al., 2020a) focus on the optimization aspect.
The second line of related work is on the Overlapping Datasets (OD) (Danks et al., 2009; Tillman &
Spirtes, 2011; Triantafillou & Tsamardinos, 2015; Huang et al., 2020a) problem in causal discovery.
OD assumes each dataset owns observations of partial variables and targets learning the integrated
DAG from multiple datasets. In these works, data from different sites need to be put together on a
central server.
The last line is on federated learning (Yang et al., 2019; Kairouz et al., 2019), which provides the
joint training paradigm to learn from decentralized data while avoiding sharing raw data during the
learning process. FedAvg (McMahan et al., 2017) first formulates and names federated learning.
FedProx (Li et al., 2020) studies the Non-IID case and provides the convergence analysis results.
SCAFFOLD leverages variance reduction by correcting client-shift to enhance the training effi-
ciency. Besides these fundamental problems in FL itself, this novel learning way has been widely
co-operated with or applied to many real-world tasks such as healthcare (Sheller et al., 2020), rec-
ommendation system (Yang et al., 2020), and smart transport (Samarakoon et al., 2019).
7	Conclusion
Learning causal structures from decentralized data brings huge challenges to traditional causal dis-
covery methods. In this context, we have introduced the first federated causal discovery method
called DS-FCD, which uses a two-level structure of each local model. During the learning process,
each client tries to learn an adjacency matrix to approximate the causal graph and neural networks
to approximate the causal mechanisms. The matrix parts of participating clients are aggregated and
processed by the server and then broadcast to each client for updating its personalized matrix. The
overall problem is formulated as a continuous optimization problem and solved by gradient descent
methods. Structural identifiability conditions are provided and extensive experiments on various
data sets show the effectiveness of our DS-FCD.
9
Under review as a conference paper at ICLR 2022
8	Ethics Statement
In this paper, we propose DS-FCD to enable causal discovery from decentralized data. We first
formulate the federated causal learning problem and provide DS-FCD to solve this problem, which
is proved to be efficient by various experiments. In future, owing to privacy protection, it would
be hard to collect personalized data and brings great challenges to causal discovery. DS-FCD just
paves a novel way for learning causal relationships while avoiding data sharing.
9	Reproducibility Statement
For the sake of reproducibility of our proposed, we make the following efforts: (i) In Appendix A.3,
we clearly state the implementations of compared methods in this paper and all hyper-parameters of
our model in all settings. (ii) At last, we will open-source the source codes, trained models, detailed
training logs upon acceptance.
References
Rohit Bhattacharya, Tushar Nagarajan, Daniel Malinsky, and Ilya Shpitser. Differentiable causal
discovery under unmeasured confounding. In International Conference on Artificial Intelligence
and Statistics, 2021.
PhiliPPe Brouillard, Sebastien Lachapelle, Alexandre Lacoste, Simon Lacoste-JUlien, and Alexandre
Drouin. Differentiable causal discovery from interventional data. Advances in Neural Information
Processing Systems, 2020.
Peter BUhlmann, Jonas Peters, and Jan Ernest. Cam: Causal additive models, high-dimensional
order search and penalized regression. The Annals ofStatistics, 42(6):2526-2556, 2014.
David Maxwell Chickering. OPtimal structure identification with greedy search. Journal of Machine
Learning Research, 3(Nov):507-554, 2002.
David Danks, Clark Glymour, and Robert Tillman. Integrating locally learned causal structures with
overlapping variables. 2009.
Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graph-
ical models. Frontiers in genetics, 10:524, 2019.
Sander Greenland, Judea Pearl, and James M Robins. Causal diagrams for epidemiologic research.
Epidemiology, pp. 37-48, 1999.
James J Heckman. Econometric causality. International statistical review, 76(1):1-27, 2008.
Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlinear
causal discovery with additive noise models. Advances in Neural Information Processing Systems,
2008.
Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Scholkopf, and Clark Glymour. Generalized score
functions for causal discovery. In ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, 2018.
Biwei Huang, Kun Zhang, Mingming Gong, and Clark Glymour. Causal discovery from multiple
data sets with non-identical variable sets. volume 34, pp. 10153-10161, 2020a.
Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero, Clark Glymour,
and Bernhard Scholkopf. Causal discovery from heterogeneous/nonstationary data. Journal of
Machine Learning Research, 21:1-53, 2020b.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sci-
ences. Cambridge University Press, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In-
ternational Conference on Learning Representations, 2017.
10
Under review as a conference paper at ICLR 2022
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Ad-
vances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Marcus Kaiser and Maksim Sipos. Unsuitability of notears for causal graph discovery. arXiv preprint
arXiv:2104.05441, 2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT
press, 2009.
Sebastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-JUlien. Gradient-based
neural dag learning. In International Conference on Learning Representations, 2020.
Steffen Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Conference on Machine Learning and Sys-
tems, 2020.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with
black box predictors. In International conference on machine learning, pp. 3122-3130. PMLR,
2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics. PMLR, 2017.
Joris M Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts.
Journal of Machine Learning Research, 21:1-108, 2020.
Yongchan Na and Jihoon Yang. Distributed bayesian network structure learning. pp. 1607-1611.
IEEE, 2010.
Cecilia Nardini. The ethics of clinical trials. Ecancermedicalscience, 8, 2014.
Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, and Jun Wang. Masked gradient-based
causal structure learning. arXiv preprint arXiv:1910.08527, 2019.
Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Geor-
gatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data.
In International Conference on Artificial Intelligence and Statistics, 2020.
Judea Pearl. Causality. Cambridge university press, 2009.
Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal inference in statistics: A primer.
John Wiley & Sons, 2016.
Jonas Peters and Peter Buhlmann. Identifiability of gaussian structural equation models with equal
error variances. Biometrika, 101(1):219-228, 2014.
Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard Scholkopf. Identifiability of causal
graphs using functional models. Conference on Uncertainty in Artificial Intelligence, 2011.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements ofcausal inference: foundations
and learning algorithms. The MIT Press, 2017.
Russell A Poldrack, Timothy O Laumann, Oluwasanmi Koyejo, Brenda Gregory, Ashleigh Hover,
Mei-Yen Chen, Krzysztof J Gorgolewski, Jeffrey Luci, Sung Jun Joo, Ryan L Boyd, et al. Long-
term neural and physiological phenotyping of a single human. Nature communications, 6(1):
1-15, 2015.
11
Under review as a conference paper at ICLR 2022
Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. arXiv preprint arXiv:2006.08907, 2020.
David B Resnik. Randomized controlled trials in environmental health research: ethical issues.
Journal of environmental health, 70(6):28, 2008.
SUmUdU Samarakoon, Mehdi Bennis, Walid Saad, and Merouane Debbah. Distributed federated
learning for ultra-reliable low-latency vehicular communications. IEEE Transactions on Commu-
nications, 68(2):1146-1159, 2019.
Gideon Schwarz. Estimating the dimension of a model. The annals ofstatiStics, pp. 461T64, 1978.
Micah J Sheller, Brandon Edwards, G Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrot-
soU, Mikhail Milchenko, Weilin XU, Daniel MarcUs, Rivka R Colen, et al. Federated learning in
medicine: facilitating mUlti-institUtional collaborations withoUt sharing patient data. Scientific
reports, 10(1):1-12, 2020.
Shohei Shimizu, Patrik O Hoyer, Aapo Hyvarinen, Antti Kerminen, and Michael Jordan. A linear
non-gaUssian acyclic model for caUsal discovery. Journal of Machine Learning Research, 7(10),
2006.
Peter Spirtes, Clark Glymour, Richard Scheines, et al. Causation, Prediction, and Search, volume 1.
The MIT Press, 2001.
Robert Tillman and Peter Spirtes. Learning equivalence classes of acyclic models with latent and
selection variables from multiple datasets with overlapping variables. 2011.
Sofia Triantafillou and Ioannis Tsamardinos. Constraint-based causal discovery from multiple in-
terventions over overlapping variable sets. Journal of Machine Learning Research, 16(1):2147-
2205, 2015.
Xiaoqiang Wang, Yali Du, Shengyu Zhu, Liangjun Ke, Zhitang Chen, Jianye Hao, and Jun Wang.
Ordering-based causal discovery with reinforcement learning. International Joint Conference on
Artificial Intelligence, 2021.
Dennis Wei, Tian Gao, and Yue Yu. Dags with no fears: A closer look at continuous optimization
for learning bayesian networks. Advances in Neural Information Processing Systems, 2020a.
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farhad Farokhi, Shi Jin, Tony Q. S.
Quek, and H. Vincent Poor. Federated learning with differential privacy: Algorithms and per-
formance analysis. IEEE Transactions on Information Forensics and Security, 15:3454-3469,
2020b.
Liu Yang, Ben Tan, Vincent W Zheng, Kai Chen, and Qiang Yang. Federated recommendation
systems. pp. 225-239. Springer, 2020.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19,
2019.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural
networks. International Conference on Machine Learning, 2019.
Yue Yu, Tian Gao, Naiyu Yin, and Qiang Ji. Dags with no curl: An efficient DAG structure learning
approach. International Conference on Machine Learning, 2021.
Ming Yuan. On the identifiability of additive index models. Statistica Sinica, pp. 1901-1911, 2011.
Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. Confer-
ence on Uncertainty in Artificial Intelligence, 2009.
Xun Zheng. Learning DAGs with Continuous Optimization. PhD thesis, Carnegie Mellon University,
2020.
12
Under review as a conference paper at ICLR 2022
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. DAGs with NO TEARS:
Continuous Optimization for Structure Learning. Advances in Neural Information Processing
Systems, 31, 2018.
Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Learning sparse
nonparametric DAGs. In International Conference on Artificial Intelligence and Statistics, 2020.
Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal discovery with reinforcement learning. In
International Conference on Learning Representations, 2020.
13
Under review as a conference paper at ICLR 2022
A Appendix
A. 1 Score function in this paper
Throughout all experiments in this paper, we assume the noise type are Gaussian with equal variance
for each local distribution. And, the overall score function utilized in this paper is as follows,
nk d
nk
Sck (Dck, Φck, U) = - 2- XX kDij= - ΦCk (gτ(Uj,:) ◦DCk )k2-λkgτ(U)kι (8)
2nk
i=1 j
In our score function, we take the negative Least Squares loss and a sparsity term, which corresponds
to the model complexity penalty in the BIC score (Schwarz, 1978).
A.2 S tructure identifiability
Besides exploring effective causal discovery methods, identifiability conditions of causal
model (Spirtes et al., 2001) are also important. In general, unique identification of the ground truth
DAG is impossible from purely observational data without some specific assumptions. However,
accompanying some specific data generation assumptions, the causal graph can be identified (Peters
et al., 2011; Peters & Buhlmann, 2014; Zhang & Hyvarinen, 2009; ShimizU et al., 2006; Hoyer et al.,
2008). We first give the definition of identifiability in the decentralized setting.
Definition 1 Consider a decentralized distribution set PC (X) satisfying Assumption 1. Then, G is
said to be identifiable if PC (X) cannot be induced from any other DAG.
Condition 1 (Cond. 19 in (Peters & BUhlmann, 2014)) The triple (fj,P(Xi), P(e,)) doesnotsolve
the following differential equation for all xi, xj with v00(xj - f(xi))f0(xi) 6= 0:
C = ξ 〃(一 νVf0 + N - 2ν"f'f + ν 0f00 +
ν0ν000f00f0	ν0 (f00)2
ν00
Here, f := fj and ξ := logP (Xi), and v := logP (ej) are the logarithms of the strictly positive
densities.
Definition 2 (Restricted ANM. Def. 27 in (Peters & BUhlmann, 2014)) Consider an ANM with d
variables. This SEM is called restricted ANM if for all j ∈ V, i ∈ PAj and all sets S ⊆ V with
PAj \{i} ⊆ S ⊆ PAj \{i, j}, there is an xS with P(xS) > 0, s.t. the tripe
fj(XPAj∖{i},l{z}),P (Xi I XS = XS) ,P(ej)
Xi
satisfies Condition1. Here, the under-brace indicates the input component of fj for variable Xi. In
particular, we require the noise variables to have non-vanishing densities and the functions fj to be
continuous and three times continuously differentiable.
Condition 2 (CaUsal Minimality) Given the joint distribution P(X), P(X) is Markov to a DAG G
but not Markov to any subgraph of G.
Assumption 2 Let a distribution P (X) with X = (Xi, X2, ∙∙∙ , Xd) be induced from a restricted
ANM with graph G, and P(X) satisfies Causal Minimality w.r.t G.
Assumption 3 Let PC(X) satisfy Assumption 1. At least one distribution Pck (X) ∈ PC (X) meets
Assumption 2 and the other distributions are faithful to G.
Proposition 1 Given PC(X) satisfying Assumption 3, and then, G can be identified up from PC(X).
Remark 1 IfPC (X) satisfies Assumption 1, then, each Pck (X) ∈ PC(X) is Markov relative to G.
14
Under review as a conference paper at ICLR 2022
Proof of Prop.1. From Remark 1, we have Pck (X) ∈ PC(X) for ∀ck, is Markov with G. For
each ck ∈ C with Pck (X) does not satisfy Assumption 2, the Completed Partially DAG (CPDAG)
△ 一	........... ............... .	„	.	…	-	.	.,	∙-~_.	V	_____
G (Pearl, 2009), which represents the CPDAG induced by G, can be identified (Spirtes et al., 2001).
(1) That also says that these distributions can be induced from any DAG induced from M(G),
including G definitely. Notice that Skeleton(G) = Skeleton(G) and any Xi J Xjm G is also existed
in G. Then, for those ck with with Pck (X) satisfying Assumption 2, G can be identified. (2) That is
to say, distributions satisfying Assumption 2 can only be induced from G. Then, two kinds of graph,
G and G, are obtained. Therefore, G can be easily identified. With (1) and (2), P ck (X) ∈ P C (X)
for ∀ck can only be induced by G . Then, G is said to be identifiable
A.3 Implementations
The comparing causal discovery methods used in this paper all have available implementations,
listed below:
•	PC and MCSL: Codes are available at gCastle https://github.com/huawei-noah/
trustworthyAI/tree/master/gcastle. The first author of MCSL added the imple-
mentation in this package.
•	NOTEARS and NOTEARS-MLP: Codes are available at the first author’s GitHub repository
https://github.com/xunzheng/notears
•	DAG-GNN: Codes are available at the author’s GitHub repository https://github.com/
fishmoon1234/DAG-GNN
•	GES: an implementation of GES is available at https://github.com/juangamella/
ges
•	CAM: the codes of CAM is available at CRAN R package repository https://cran.
r-project.org/src/contrib/Archive/CAM/
Our implementation is highly based on the existing Tool-chain named gCastle, which includes many
gradient-based causal discovery methods.
A.3.1 Hyper-parameters setting
In all experiments, there is no extra hyper-parameter to adjust for PC (with Fisher-z test and p-value
0.01) and GES (BIC score). For NOTEARS, NOTEARS-MLP and DAG-GNN, we use the default
hyper-parameters provided in their papers/codes. For MCSL, the hyper-parameters need to be mod-
ified are ρinit and β. Specifically, if experimental settings (10 variables and 20 variables) are the
same as those in their paper, we just take all the recommended hyper-parameters. For settings not
implemented in their paper (40 variables exactly), we have two kinds of implementations. The first
one is taking a linear interpolation for choosing the hyper-parameters. The second one is taking the
same parameters as ours. We find that the second choice always works better. In our experiment,
we report the experimental results done in the second way. Notice that CAM pruning is also intro-
duced to improve the performance of MCSL, which however can not guarantee a better result in our
settings. For simplicity and fairness, we just take the direct outputs of MCSL.
Similar to MCSL (Ng et al., 2019) and GraN-DAG (Lachapelle et al., 2020), we implement several
experiments on simulated data with known causal graphs to search for the hyper-parameters and then
use these hyper-parameters for all the simulated experiments. Specifically, We use seeds 1 〜10 to
generate the simulated data to search for the best combination of hyperparameters while all our
experimental results reported in this paper are all conducted using seeds 2021 〜2030.
A.3.2 Hyper-parameters in real-data setting
Most CSL methods have hyper-parameters, more or less, which need to be decided prior to learn-
ing. Moreover, NN-based methods are especially sensitive to the selection of hyper-parameters.
For instance, Gran-DAG (Lachapelle et al., 2020) defines a really large hyper-parameters space for
searching the optimal combination, which even uses different learning rates for the first subproblem
and the other subproblems. MCSL and DS-FCD are sensitive to the selection of ρinit and β when
constructing and solving the subproblem. As pointed out in (Kairouz et al., 2019), NOTEARS focus
15
Under review as a conference paper at ICLR 2022
more on optimizing the scoring term in the early stage and pays more attention to approximate DAG
in the late stage. If NOTEARS cannot find a graph near G in the early stage, then, it would lead to a
worse result.
To alleviate this problem, one may choose to (1) enlarge the learning rate or take more steps when
solving the first few subproblems as Gran-DAG; (2) reduce the value of coefficient ρinit to let the
optimizer pay more attention to the scoring term in the early stages as MCSL. The other trick we
find when dealing with real data is increasing 'ι. This mostly results from that real data may not fit
well with the data generation assumptions in most papers. Therefore, we choose to conduct a grid
search to find the best combination of ρinit , β, `1 for causal discovery on real data.
In the practice of causal discovery, it is impossible to have G to select the hyper-parameters. One
common approach is trying multiple hyper-parameter combinations and keeping the one yielding the
best score evaluated on a validation set (Koller & Friedman, 2009; Ng et al., 2019; Lachapelle et al.,
2020). However, the direct use of this method may not work for some algorithms, such as MCSL,
NOTEARS-MLP, and DS-FCD. This mainly lies in the similar explanations of the property of the
traditional solution of AL. In the late stage of optimization, the optimizer focuses heavily on finding
a DAG by enlarging the penalty coefficient ρ. Then, the learning of causal mechanisms would be
nearly ignored. To address this problem, we firstly report the DAG directly learned by a combination
of hyper-parameters. And then, we replace the parameters part for describing the causal graph with
the learned DAG. Afterwards, we just take the score without DAG constraint to optimize the causal
mechanism approximation part (which may not be the same name in the other algorithms). Finally,
the validation set is taken to evaluate the learned model. The final hyper-parameters used on the real
dataset in our paper is as follows:
Table 4:	The hyper-parameters used on real data.
Para Pinit	β λ'ι
Value I 0.008 2	0.3
A.3.3 Model Parameters
The CGL part in each local model is parameterized by a d × d matrix named U and the Gumbel-
Sigmoid approach is leveraged for approximating the binary form. Each entry in U is initialized as
0. The temperature τ is set to 0.2 for all settings. Then, for the causal mechanism approximation
part, we use 4 dense layers with 16 variables in each hidden layer. All weights in the Network are
initialized using the Xavier uniform initialization.
A.3.4 Training Parameters
Our AS-FCD and DS-FCD reach this point and are implemented with the following hyper-
parameters. We take Adam (Kingma & Ba, 2015) with learning rate 3 × 10-2 and all the obser-
vational data Dck on each client are used for computing the gradient. And the detailed parameters
used in Algorithms 1 and 2 are listed in Table 5. Notice that as illustrated in MCSL (Ng et al., 2019),
Table 5:	The hyper-parameters used on simulated data in this paper.
Para ∣ ainit	ht0ι	itmax iti
nner itfl γ ρ
max	λ'ι
Value I 0	1 X 10-10	25	1000	200 0.25	1 X 1014	0.01
the performance of the algorithm is affected by the initial value of ρinit and the choice of β. Since
a small initial of ρinit and β would result in a rather long training time. As said in (Kaiser & Sipos,
2021), MLE plays an important role in the early stage of training and highly affects the final results.
Therefore, carefully picking a proper combination of ρinit and β will lead to a better result. In our
method, We tune these two parameters via the same scale of experiment with seeds 1 〜10. For
each variable scale and training type, the parameters are adjusted once and are applied to all other
experiments with the same variable scale. We find the combinations of the following parameters in
16
Under review as a conference paper at ICLR 2022
itn
Figure 3: The sensitivity analysis of hyper-parameters
Table 6 work well in our method. Our method also adopts a `1 sparsity term on gτ (U), where the
sparsity coefficient λ'ι is chosen as 0.01 for all settings.
Table 6: The combinations of Pinit and β on simulated data in our method.
	10 nodes		20 nodes		40 nodes	
	ρinit	β	ρinit	β	ρinit	β
AS-FCD	6 × 10-3	10	1 X 10-5	20	1 X 10-11	120
DS-FCD	6 × 10-3	10	6 × 10-5	20	1 X 10-11	120
A.3.5 Sensitivity Analysis of Hyper-parameters
Here, we show the sensitivity analysis of itfl, αinit, and λl1 . From the experimental results in
Figure 3, we find that our method is relatively robust to itfl . That is to say, the itfl can be reduced
to alleviate the pressure of communication costs while the performance can be well kept. λl1 is the
coefficient of l1 sparsity, which will affect the final results. Because we have no sparsity information
of the underlying causal graph, we set λl1 = 0.01 in all settings. When dealing with real data, we
recommend the audiences adjust this parameter by using our parameter-tuning method provided in
the Section A.3.2. The results of αinit are exactly as expected. As discussed before, our method
tries to maximize the likelihood term of the total loss in the early stages, which is important to find
the final ground-truth DAG. If setting a relatively large αinit , the early learning stages would be
affected. Therefore, we recommend directly taking αinit as 0 in all settings.
A.4 More discussions on the experimental results
Here, we give the detailed discussions on the experimental results in the paper. First of all, PC and
GES can only reach the CPDAG (or MEC) at most, which shares the same Skelton with the ground-
truth DAG. When we evaluate SHD, we just ignore the direction of undirected edges learned by PC
and GES. That is to say, these two methods can get SHD if they can identify the CPDAG. Therefore,
the final results are not caused by unfair comparison. For PC, the independence test is leveraged to
decode the (conditional) independence from the data distribution. Therefore, the accuracy would be
affected by (1) the amount of the observations and (2) the effectiveness of”the non-parametric kernel
17
Under review as a conference paper at ICLR 2022
Figure 4: Comparisons with NOTEARS on linear data (IID).
independence test” method. GES leverages greedy search with BIC score. However, the likelihood
part of BIC in GES is Linear Gaussian, which is unsuitable for data generated by the Non-linear
model. NOTEARS is a linear model but the causal mechanisms are non-linear. The reason will
be the unfitness between data and model. Therefore, the comparisons with GES and NOTEARS on
linear IID data are implemented in the next section. DAG-GNN is also a non-linear model. However,
the non-linear assumption of DAG-GNN is not the same as the data generation model assumed in
our paper. The second reason comes from its ”mechanisms approximation” modules are compulsory
to share some parameters. Both NOTEARS-MLP and MCSL have their own advantages. Please
refer to Tables 14 and 15, you will find that NOTEARS-MLP performs better when the non-linear
functions are MIM and MLP while MCSL works better on GP and GP-add models.
A.5 Supplementary Experimental details
Results on Linear model As aforementioned for the IID case, the BIC score of GES takes the
Gaussian likelihood and NOTEARS is a linear model. Therefore, for fair comparison, here, we also
provide the linear version of our method. Since linear data are parameterized with an adjacency
matrix, we can directly take the adjacency matrix as our model instead of a CGL part and a CMA
part. During training, the matrix are communicated and averaged by the server to coordinate the
joint learning procedures. All experiments are implemented with 50 observations on each client.
Table 7: ResUlts on the linear model (IID).
IID-GP	Non-IID
ER2 with 10 nodes
ER2 with 20 nodes
ER2 with 10 nodes
ER2 with 12 nodes
SHD J TPR ↑ SHD J TPR ↑ SHD J TPR ↑ SHD J TPR ↑
GES	11.0 ± 7.8 0.70 ± 0.21 NOTEARS 2.2 ± 3.0	0.90 ± 0.13	8.5 ± 6.1	0.73 ± 0.17 1.8 ± 1.9	0.89 ± 0.10	25.0 ± 24.8 0.78 ± 0.20 3.6 ± 2.7	0.93 ± 0.05	33.2 ± 17.6 0.69 ± 0.16 10.2 ± 8.1	0.80 ± 0.15
GES	15.8 ± 4.5 0.50 ± 0.14 NOTEARS 4.3± 1.9	0.85 ± 0.08	12.6 ± 3.8 0.54 ± 0.13 3.6 ± 2.1	0.83 ± 0.10	35.4 ± 10.3	0.60 ± 0.14 7.6 ± 3.0	0.87 ± 0.06	39.1 ± 6.6	0.52 ± 0.09 14.4 ± 6.5	0.76 ± 0.11
AS-FCD 1.8 ± 1.7	0.91 ± 0.10 ∣ 2.4 ± 2.4	0.86 ± 0.14 ∣ 4.8 ± 4.5	0.90 ± 0.11 ∣ 10.4 ± 7.0	0.79 ± 0.14
From Table 7, we find that oUr method can consistently show its advantage on the linear case. If
yoU consider that why GES with all data still performs worse than oUr method, this resUlts from
the searching method and the credit shoUld be of NOTEARS (oUr baseline method in the above
experiments). FUrthermore, we also added an ablation experiment that considers the effect of data
nUmber on the performance. The details are shown in FigUre 4. We can see that oUr AS-FCD
consistently performs well in the linear case.
Model Mis-specification Here, we add the experiments of model mis-specification, where a Post-
Nonlinear model (PNL) is taken. The data of PNL model is generated according to Xi =
18
Under review as a conference paper at ICLR 2022
Table 8: Results on PNL with GP (IID).					
		ER2 with 10 nodes		SF2 with 10 nodes	
		SHD ；	TPR ↑	SHD ；	TPR ↑
	PC	13.3 ± 3.5	0.43 ± 0.13	13.3 ± 4.4	0.50 ± 0.07
	NOTEARS	17.5 ± 2.1	0.00 ± 0.00	16.0 ± 0.0	0.00 ± 0.00
	MCSL	17.6 ± 2.4	0.01 ± 0.02	16.2 ± 0.6	0.01 ± 0.02
	PC	13.9 ± 3.0	0.34 ± 0.08	13.9 ± 1.4	0.34 ± 0.08
	NOTEARS	17.5 ± 2.1	0.00 ± 0.00	16.0 ± 0.0	0.00 ± 0.00
	MCSL	17.8 ± 2.1	0.01 ± 0.01	16.3 ± 0.5	0.01 ± 0.01
	AS-FCD	17.5 ± 2.1	0.00 ± 0.00	16.0 ± 0.0	0.00 ± 0.00
	DS-FCD	17.5 ± 2.1	0.00 ± 0.00	15.9 ± 0.3	0.01 ± 0.02
σ(fi(XP ai) + Laplace(0, i)), where function fi is independently sampled from a Gaussian pro-
cess with bandwidth one, 6i 〜U[0,1] and σ(∙) is the Sigmoid function. U[0,1] means the uniform
distribution from 0 to 1. The additional experimental results are shown in Table 8.
DS-FCD, carrying the ANM assumption of data, tries to maximize the likelihood of noise distribu-
tion. It is the same as NOTEARS and MCSL. Therefore, the model misspecification would hugely
harm the performance of these methods.
Dense Graph Our method is also implemented on some denser graphs. Experimental results in
Table 9 and Table 10.
Table 9: Results on nonlinear ANM with dense graphs (IID).
		ER4 with 10 nodes		SF4 with 10 nodes		ER4 with 20 nodes		SF4 with 20 nodes	
		SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
	PC	27.3 ± 3.2	0.29 ± 0.07	18.9 士 4.9	0.37 士 0.16	68.2 士 9.5	0.23 ± 0.06	60.2 士 9.3	0.30 ± 0.08
	NOTEARS	34.3 ± 1.7	0.03 ± 0.02	22.7 士 1.3	0.05 ± 0.05	71.8 士 7.2	0.03 ± 0.01	62.8 士 0.9	0.02 ± 0.01
	MCSL	15.5 ± 5.9	0.57 ± 0.15	4.5 士 3.1	0.83 士 0.11	33.8 士 10.4	0.55 ± 0.11	19.8 士 7.5	0.69 ± 0.11
	PC	31.5 ± 2.1	0.14 ± 0.03	20.4 士 0.58	0.21 士 0.03	68.7 士 8.1	0.13 ± 0.03	60.9 士 2.8	0.15 ± 0.02
	NOTEARS	34.3 ± 1.8	0.03 ± 0.01	22.7 士 1.0	0.06 ± 0.04	70.1 士 6.9	0.03 ± 0.01	62.3 士 0.56	0.03 ± 0.01
	MCSL	15.8 ± 3.3	0.61 ± 0.09	8.3 士 4.3	0.78 ± 0.11	49.3 士 11.8	0.63 ± 0.10	39.7 士 5.6	0.73 ± 0.07
	DS-FCD	16.9 ± 4.9	0.53 ± 0.12	5.4 士 3.0^^	0.78 士 0.12	35.4 士 10.9	0.53 ± 0.11	20.7 士 5.1	0.69 ± 0.08
	AS-FCD	17.4 ± 4.8	0.53 ± 0.12	5.5 士 2.8	0.79 ± 0.11	40.7 士 4.8	0.57 ± 0.10	24.1 士 5.8	0.71 ± 0.09
Table 10: Results on nonlinear ANM with dense graphs (Non-IID).
		ER4 with 10 nodes		SF4 with 10 nodes		ER4 with 20 nodes		SF4 with 20 nodes	
		SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
	PC	29.3 ± 1.3	0.23 ± 0.03	20.3 ± 2.1	0.31 ± 0.06	71.9 ± 8.1	0.19 ± 0.03	62.7 ± 2.8	0.22 ± 0.03
	NOTEARS	20.5 ± 2.6	0.45 ± 0.08	12.2 ± 2.9	0.54 ± 0.11	43.2 ± 7.0	0.49 ± 0.08	39.4 ± 6.8	0.47 ± 0.10
	MCSL	20.0 ± 3.2	0.52 ± 0.07	13.7 ± 2.2	0.65 ± 0.07	65.1 ± 7.7	0.33 ± 0.05	59.4 ± 5.3	0.31 ± 0.05
	DS-FCD	8.5 ± 3.7	0.84 ± 0.09	4.5 ± 2.0	0.93 ± 0.07	40.7 ± 14.5	0.74 ± 0.07	39.9 ± 10.8	0.68 ± 0.07
From the above experimental results, we can see that our method shows consistently better perfor-
mance over other methods on the denser graph setting. For the IID case, both AS-DAG and DS-DAG
obtain the nearly low SHD as MCSL trained on all data and far better than all methods trained on
separated data. For the Non-IID case, our DS-FCD still shows the best performance. Compared to
NOTEARS in 20 variables case, DS-FCD shows similar SHD results but much better TPR result.
Therefore, how to reduce the false discovery rate of DS-FCD would be an interesting thing.
Voting method There is another interesting research line (Na & Yang, 2010), which also try to learn
DAG from decentralized data. We add a DAG combination method proposed in (Na & Yang, 2010),
which proposes to vote for each entry of the adjacency matrix to get the final DAG.
From the experimental results in Table 11, we can find that For PC and NOTEARS, the combining
method seems to contribute little improvement. This is because the reported DAGs local clients are
19
Under review as a conference paper at ICLR 2022
Table 11: Comparison with the voting method.
IID-GP	Non-IID
	ER2 with 10 nodes		ER2 with 20 nodes		ER2 with 10 nodes		ER2 with 12 nodes	
	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
PC	14.1 ± 2.4	0.31 ± 0.06	32.7 ± 6.5	0.28 ± 0.07	12.5 ± 2.7	0.45 ± 0.07	28.5 ± 6.3	0.44 ± 0.07
NOTEARS	16.5 ± 2.0	0.06 ± 0.04	31.7 ± 6.0	0.11 ± 0.04	7.6 ± 2.6	0.60 ± 0.11	15.0 ± 3.1	0.62 ± 0.09
MCSL	7.1 ± 3.2	0.83 ± 0.08	24.8 ± 5.5	0.88 ± 0.07	9.2 ± 1.8	0.72 ± 0.06	23.3 ± 5.8	0.56 ± 0.08
PC	13.3 ± 3.0	0.27 ± 0.11	29.7 ± 5.9	0.22 ± 0.05	11.4 ± 3.4	0.36 ± 0.13	25.5 ± 6.8	0.29 ± 0.13
NOTEARS	15.6 ± 2.2	0.11 ± 0.06	32.6 ± 6.2	0.09 ± 0.05	7.8 ± 4.0	0.56 ± 0.20	18.4 ± 11.6	0.49 ± 0.30
MCSL	8.0 ± 3.1	0.85 ± 0.16	18.1 ± 7.8	0.88 ± 0.06	6.9 ± 2.2	0.71 ± 0.13	10.1 ± 4.6	0.79 ± 0.09
DS-FCD	2.4 ± 2.0	0.86 ± 0.12	6.2 ± 4.0	0.85 ± 0.10	1.9 ± 1.6	0.99 ± 0.02	6.2 ± 4.7	0.89 ± 0.09
AS-FCD	1.8 ± 2.0	0.89 ± 0.12	5.0 ± 4.2	0.88 ± 0.11	Nan	Nan	Nan	Nan
too bad to get a good result. For MCSL, this combing method works really well for improving the
performance. The reason is easy to be inferred from the results. For MCSL, DAGs reported by local
clients are of bad SHDs but good TPR, which means that the False Discovery Rates (FDRs) are
high. While the combing method can further reduce the FDRs and keep the TPRs still good. Then,
SHD can be further reduced. Luckily, our DS-FCD still shows the best performances in all settings.
CAM Here, We add one more identifiable baseline named causal additive model (CAM) (Buhlmann
et al., 2014), which also serves as a baseline in MCSL (Ng et al., 2019), GraNDAG (Lachapelle
et al., 2020), and DAG-GAN (Yu et al., 2019).
Table 12: ComParisons With CAM on nonlinear ANM (I2-GP).
		ER2 with 10 nodes		SF2 with 10 nodes		ER2 with 20 nodes		SF2 with 20 nodes	
		SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
All data	CAM	9.5 ± 2.9	0.87 ± 0.09 I	9.1 ± 3.1	0.84 ± 0.10	I 21.4 ± 4.7	0.77,± 0.08	I 26.6 ± 6.1	0.75 ± 0.07
Sep data	CAM	11.8 ± 2.6	0.40 ± 0.10 I	11.1 ± 1.5	0.38 ± 0.11	I 24.3 ± 5.8	0.40 ± 0.07	I 26.8 ± 2.0	0.36 ± 0.06
	DS-FCD	2.4 ± 2.0	0.86 ± 0.12 I	2.7 ± 2.2	0.86 ± 0.13	I 6.2 ± 4.0	0.85 ± 0.10	I 14.7 ± 7.0	0.80 ± 0.11
	AS-FCD	1.8 ± 2.0	0.89 ± 0.12 I	2.5 ± 2.7	0.85 ± 0.15	I 5.0 ± 4.2	0.88 ± 0.11	I 7.8 ± 5.5	0.80 ± 0.14
Table 13: Comparisons with CAM on nonlinear ANM (Non-IID).
		ER2 with 10 nodes		SF2 with 10 nodes		ER2 with 20 nodes		SF2 with 20 nodes	
		SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
All data	CAM	31.9 ± 4.8	0.39 ± 0.15 I	31.8 ± 4.4	0.31 ± 0.17	I 104.6 ± 15.4	0.46,± 0.15 I	116.9 ± 13.8	0.35 ± 0.07
Sep data	CAM	18.0 ± 1.7	0.52 ± 0.04 I	17.8 ± 2.1	0.51 ± 0.3	I 47.5 ± 9.2	0.52 ± 0.04 I	53.0 ± 6.1	0.50 ± 0.03
	DS-FCD	1.9 ± 1.6	0.99 ± 0.02 I	2.6 ± 1.3	0.93 ± 0.07	I 6.2 ± 4.7	0.89 ± 0.09 I	11.5 ± 6.7	0.81 ± 0.14
From result in Table 12 and 13, We can see that our methods alWays shoW an advantage over CAM.
CAM also assumes a non-linear additive noise model for data generation. However, CAM limits the
non-linear function to be additive. In normal ANM, Xi = fi(Xpai) + i while CAM assumes Xi =
Pj∈x(pai) fi—j (Xj) + Ci, which limits the capacity of its model. From the above experimental
results, we can see that our methods show consistent advantages over CAM.
20
Under review as a conference paper at ICLR 2022
eapπv eapdωs
Table 14: ResUlts on nonlinear ANM With different functions (IID, 10 nodes, ER2).
GP	MIM	MLP	GP-add
SHD φ TPR ↑ SHD φ TPR ↑ SHD φ	TPR ↑	SHD φ TPR ↑
PC	15.3 ± 2.6	0.37 ± 0.10	11.0 ± 4.9	0.60 ± 0.16	11.8 ± 4.3	0.61 ± 0.14	14.0 ± 4.7	0.49 ± 0.16
GES	13.0 ± 3.9	0.50 ± 0.18	9.6 ± 4.4	0.71 ± 0.17	15.8 ± 6.0	0.63 ± 0.14	14.4 ± 4.9	0.57 ± 0.17
DAG-GNN	16.2 ± 2.1	0.07 ± 0.06	13.7 ± 2.4	0.26 ± 0.10	18.2 ± 3.3	0.36 ± 0.12	13.3 ± 2.3	0.24 ± 0.10
NOTEARS	16.5 ± 2.0	0.05 ± 0.04	12.1 ± 3.2	0.34 ± 0.13	13.3 ±3.4	0.35 ± 0.15	13.4 ± 2.2	0.23 ± 0.09
N-S-MLP	8.1 ± 3.8	0.56 ± 0.17	1.6 ± 1.3	0.95 ± 0.06	56 ± 1.3	0.81 ± 0.11	6.8 ± 4.0	0.65 ± 0.16
MCSL	1.9 ± 1.5	0.90 ± 0.08	0.7 ± 1.2	0.97 ± 0.06	12.7 ± 3.6	0.58 ± 0.24	1.9 ± 1.7	0.91 ± 0.07
PC	14.1 ± 2.4	0.31 ± 0.06	11.1 ± 3.6	0.48 ± 0.14	13.2 ± 3.6	0.42 ± 0.09	13.5 ± 3.2	0.37 ± 0.12
GES	12.7 ± 2.7	0.37 ± 0.09	10.6 ± 3.3	0.54 ± 0.12	14.6 ± 4.6	0.50 ± 0.13	12.0 ± 2.6	0.48 ± 0.08
DAG-GNN	15.7 ± 2.3	0.11 ± 0.05	11.7 ± 3.3	0.37 ± 0.12	17.7 ± 3.6	0.39 ± 0.11	13.0 ± 2.0	0.26 ± 0.10
NOTEARS	16.5 ± 2.0	0.06 ± 0.04	12.3 ± 3.0	0.33 ± 0.12	13.4 ± 3.4	0.35 ± 0.14	13.3 ± 2.3	0.24 ± 0.09
N-S-MLP	8.5 ± 2.9	0.56 ± 0.13	2.8 ± 1.5	0.93 ± 0.06	6.4 ± 1.3	0.81 ± 0.11	7.4 ± 2.9	0.67 ± 0.13
MCSL	7.1 ±3.2	0.83 ± 0.08	4.4 ± 2.1	0.91 ± 0.06	13.4 ± 3.9	0.57 ± 0.21	6.5 ± 3.5	0.84 ± 0.07
DS-FCD	2.4 ± 2.0	0.86 ± 0.12	2.1 ± 1.4	0.91 ± 0.07	11.1 ± 3.1	0.57 ± 0.20	2.6 ± 1.6	0.87 ± 0.09
AS-FCD	1.8 ± 2.0	0.89 ± 0.12	1.7 ± 1.6	0.91 ± 0.08	10.5 ± 3.5	0.59 ± 0.22	2.4 ± 1.6	0.87 ± 0.08
Table 15: ResUlts on nonlinear ANM With different functions (IID, 20 nodes, ER2).
	GP		MIM		MLP		GP-add	
	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
PC	32.7 ± 9.4	0.48 ± 0.13	22.8 ± 5.8	0.60 ± 0.15	33.7 ± 12.3	0.50 ± 0.13	35.2 ± 8.0	0.50 ± 0.09
GES	27.1 ± 8.5	0.56 ± 0.11	21.5 ± 6.1	0.78 ± 0.09	44.9 ± 12.5	0.65 ± 0.11	41.7 ± 11.6	0.66 ± 0.08
DAG-GNN	32.5 ± 6.8	0.10 ± 0.08	26.7 ± 7.4	0.26 ± 0.13	32.1 ± 10.4	0.38 ± 0.08	27.2 ± 2.4	0.24 ± 0.08
NOTEARS	31.8 ± 6.0	0.11 ± 0.04	25.6 ± 6.1	0.29 ± 0.08	25.3 ± 8.0	0.40 ± 0.09	25.6 ± 3.9	0.28 ± 0.06
N-S-MLP	18.2 ± 4.5	0.52 ± 0.10	4.1 ± 2.0	0.95 ± 0.04	8.0 ± 3.9	0.86 ± 0.07	12.6 ± 2.2	0.70 ± 0.06
MCSL	4.6 ± 4.6	0.90 ± 0.13	1.7 ± 1.6	0.97 ± 0.04	18.1 ± 6.6	0.72 ± 0.14	3.1 ± 1.9	0.92 ± 0.05
PC	32.7 ± 6.5	0.28 ± 0.07	24.4 ± 5.6	0.46 ± 0.11	30.6 ± 8.0	0.41 ± 0.09	29.5 ± 5.6	0.42 ± 0.10
GES	28.6 ± 5.5	0.34 ± 0.06	20.5 ± 3.7	0.61 ± 0.06	34.4 ± 11.3	0.52 ± 0.09	29.3 ± 5.5	0.51 ± 0.07
DAG-GNN	31.7 ± 6.1	0.12 ± 0.04	26.8 ± 5.8	0.26 ± 0.06	34.1 ± 9.7	0.46 ± 0.07	26.5 ± 4.0	0.27 ± 0.05
NOTEARS	31.7 ± 6.0	0.11 ± 0.04	25.7 ± 5.9	0.29 ± 0.07	25.4 ± 7.4	0.42 ± 0.07	25.6 ± 3.8	0.29 ± 0.06
N-S-MLP	19.5 ± 4.7	0.52 ± 0.07	6.5 ± 1.9	0.92 ± 0.03	16.1 ± 8.6	0.86 ± 0.07	16.2 ± 3.3	0.70 ± 0.07
MCSL	24.8 ± 5.5	0.88 ± 0.07	20.4 ± 3.8	0.91 ± 0.05	30.2 ± 5.1	0.67 ± 0.12	16.2 ± 5.3	0.87 ± 0.05
DS-FCD	6.2 ± 4.0	0.85 ± 0.10	8.5 ± 2.8	0.93 ± 0.05	21.4 ± 7.9	0.71 ± 0.14	8.1 ± 3.2	0.85 ± 0.05
AS-FCD	5.0 ± 4.2	0.88 ± 0.11	3.3 ± 2.5	0.92 ± 0.07	20.1 ± 8.3	0.72 ± 0.14	5.6 ± 2.8	0.86 ± 0.06
Table 16: Results on Non-IID setting with the different number of observations, (20nodes, ER2).
	n =100		n =300		n =600		n =900	
	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
PC	55.5 ± 8.5	0.21 ± 0.06	57.3 ± 5.7	0.29 ± 0.07	60.4 ± 9.8	0.32 ± 0.11	62.4 ± 6.6	0.29 ± 0.10
GES	82.8 ± 13.7	0.38 ± 0.12	96.4 ± 14.9	0.48 ± 0.08	102.9 ± 13.6	0.51 ± 0.08	106.3 ± 14.3	0.50 ± 0.11
DAG-GNN	61.8 ± 14.7	0.39 ± 0.07	56.8 ± 9.7	0.37 ± 0.08	57.7 ± 12.0	0.38 ± 0.08	57.9 ± 12.1	0.32 ± 0.08
NOTEARS	58.7 ± 12.8	0.41 ± 0.12	57.6 ± 10.2	0.44 ± 0.06	57.3 ± 12.9	0.43 ± 0.08	59.4 ± 10.3	0.39 ± 0.10
N-S-MLP	111.2 ± 14.4	0.92 ± 0.10	101.0 ± 16.8	0.92 ± 0.05	100.8 ± 14.7	0.90 ± 0.10	97.6 ± 14.8	0.90 ± 0.07
MCSL	49.0 ± 8.1	0.62 ± 0.06	54.0 ± 10.0	0.70 ± 0.10	53.8 ± 9.6	0.73 ± 0.10	57.6 ± 11.6	0.73 ± 0.08
PC	31.2 ± 5.7	0.30 ± 0.05	29.0 ± 5.9	0.39 ± 0.06	28.5 ± 6.3	0.44 ± 0.07	27.9 ± 6.6	0.47 ± 0.08
GES	35.1 ± 8.3	0.48 ± 0.10	31.6 ± 9.8	0.57 ± 0.08	30.0 ± 8.0	0.62 ± 0.06	30.5 ± 10.7	0.64 ± 0.07
DAG-GNN	29.9 ± 7.2	0.66 ± 0.09	20.3 ± 5.0	0.67 ± 0.09	18.5 ± 4.9	0.67 ± 0.09	18.0 ± 5.2	0.66 ± 0.11
NOTEARS	16.3 ± 3.4	0.61 ± 0.08	15.5 ± 3.2	0.60 ± 0.08	15.0 ± 3.1	0.62 ± 0.09	15.2 ± 2.9	0.61 ± 0.09
N-S-MLP	68.0 ± 5.4	0.80 ± 0.04	22.6 ± 3.3	0.79 ± 0.06	12.7 ± 2.6	0.80 ± 0.05	11.8 ± 2.8	0.80 ± 0.05
MCSL	32.8,± 5.4	0.49 ± 0.08	26.4 ± 5.5	0.53 ± 0.09	23.3 ± 5.8	0.56 ± 0.08	23.1 ± 6.5	0.56 ± 0.07
DS-FCD	11.6 ± 5.6	0.83 ± 0.11	7.1 ± 6.1	0.90 ± 0.12	6.2 ± 4.7	0.89 ± 0.09	6.0 ± 5.5	0.91 ± 0.11
Table 17: Results on randomly selecting models-info of partial clients (Non-IID, 20nodes, ER2).
IID	NON-IID
ER2 With 10 nodes ER2 With 20 nodes ER2 With 20 nodes ER2 With 10 nodes
	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑	SHD J	TPR ↑
10%	3.8 ± 2.4	0.78 ± 0.14	8.6 ± 4.8	0.77 ± 0.13	3.8 ± 1.4	0.93 ± 0.05	8.5 ± 5.4	0.89 ± 0.07
20%	3.2 ± 2.0	0.81 ± 0.12	6.7 ±4.8	0.82 ± 0.13	2.5 ± 2.1	0.97 ± 0.04	8.2 ± 5.4	0.87 ± 0.09
m 50%	2.9 ± 1.8	0.83 ± 0.11	5.8 ±4.4	0.85 ± 0.12	1.8 ± 1.4	0.99 ± 0.02	6.3 ± 5.1	0.89 ± 0.10
80%	2.7± 1.9	0.84 ± 0.12	6.0 ± 3.9	0.86 ± 0.10	1.8 ± 1.3	0.99 ± 0.02	5.9 ±4.1	0.90 ± 0.08
100%	2.4 ± 2.0	0.86 ± 0.12	6.2 ± 4.0	0.85 ± 0.10	1.9 ± 1.6	0.99 ± 0.02	6.2 ± 4.7	0.89 ± 0.09
21
Under review as a conference paper at ICLR 2022
Figure 5: Anatomical causal-effect relationships of fMRI Hippocampus dataset
Table 18: Empirical results on fMRI Hippocampus dataset (Part 2).
	All data			Separate data			DS-FCD	AS-FCD
	GES	N-S-MLP	DAG-GNN	GES	N-S-MLP	DAG-GNN		
SHD J	8.0 ± 0.0	9.0 ± 0.0	5.4 ± 0.5	8.3 ± 1.2	11.3 ± 1.0	8.2 ± 1.9	6.4 ± 0.9	5.0 ± 0.0
NNZ	11.0 ± 0.0	12.0 ± 0.0	3.3 ± 0.8	8.5 ± 1.1	14.4 ± 0.8	5.7 ± 1.4	6.8 ± 0.6	5.0 ± 0.0
TPR ↑	0.43 ± 0.00	0.43 ± 0.00	0.23 ± 0.07	0.31 ± 0.17	0.44 ± 0.10	0.17 ± 0.18	0.27 ± 0.12	0.29 ± 0.00
FDR J	0.73 ± 0.00	0.75 ± 0.00	0.52 ± 0.09	0.75 ± 0.12	0.78 ± 0.05	0.80 ± 0.18	0.72 ± 0.11	0.60 ± 0.00
22
Under review as a conference paper at ICLR 2022
-20	0	20	40	-20	0	20	40	-10	0	10
Figure 6: The visualization of simulated Non-IID data with 10 variables, where 6 variables are
randomly selected and two of them are chosen for one subfigure.
23
Under review as a conference paper at ICLR 2022
-4-
-2	0	2	4
-4	-2	0	2	4
-4	-2	0	2	4	-4	-2	0	2	4	-4	-2	0	2	4
Figure 7: Normalized distribution of real data used in this paper.
the
24