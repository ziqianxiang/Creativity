Under review as a conference paper at ICLR 2022
Modular Lagrangian Neural Networks:
Designing Structures of Networks with
Physical Inductive Biases
Anonymous authors
Paper under double-blind review
Abstract
Deep learning struggles at extrapolation in many cases. This issue happens when
it comes to untrained data domains or different input dimensions and becomes
more common in physical problems. Leveraging physical inductive biases can
help relieve this issue and generalise the laws of physics. Based on this idea,
we proposed a kind of structural neural network called Modular Lagrangian Neu-
ral Networks (ModLaNNs). This model can learn from the dynamics of simpler
systems such as three-body systems and extrapolate to more complex ones like
multi-body systems, which is not feasible using other relevant physical-informed
neural networks. We tested our model on double-pendulum or three-body systems
and reached the best results compared with our counterparts. At the same time,
we directly applied our trained models to predict the motion of multi-pendulum
and multi-body systems, demonstrating the intriguing performance in the extrap-
olation of our method.
Figure 1: Learning from three-body systems and extrapolating to multi-body (> 3) systems. The
inputs are the generalised position q and velocity q. The output is the acceleration q. As for
training, multilayer perception (MLP) can only accept data with fixed input size. After users train
the MLP using a dataset of three-body systems, they can only implement this network to simulate
the same system for testing. By contrast, our model can recurrently process input data to obtain the
Lagrangian L, which describes the evolution of the whole system. This feature supports our model
to accept data with different input sizes, extrapolate to multi-body systems with the same laws of
physics, and predict the trajectories with high accuracy.
1
Under review as a conference paper at ICLR 2022
1	Introduction
Deep learning has been widely implemented in various kinds of physical problems such as flow
control using reinforcement learning (Verma et al., 2018), nuclear learning and simulation (Pfau
et al., 2020; Hermann et al., 2020), and super-resolution of the combustion process (Kipf et al.,
2018). These deep-learning models are specialised in generalisation from high-dimensional data.
But it is still hard to build neural networks to learn the laws of physics and inefficient for these
models to extrapolate to regions where data is insufficient.
The fusion of physical knowledge and neural networks cast a light on this problem, which leads to
physics-informed neural networks such as Hamiltonian neural network (HNN) (Greydanus et al.,
2019) or Lagrangian Neural Networks (LNN) (Cranmer et al., 2020). This kind of model can learn
laws of physics such as the conservation of energy to achieve more stable simulations. Despite this
strength, after training, those laws are altogether implicitly encoded in networks, and the input size
is also restrictively decided. These drawbacks still prohibit the performance of these models from
extrapolation. New models are needed for practical use where the input is varying.
The contributions of this work exist in two aspects. First, we introduced Modular Lagrangian Neural
Networks (ModLaNNs), where the network structure integrates with Euler-Lagrangian equations to
describe the dynamical systems accurately. Also, our model will recurrently rebuild the energy of
every element in a system and then construct the Lagrangian to describe the motion of the whole
system. Therefore, the trained model can be applied for simulating a group of systems, as long as
the dynamics of these elements are the same (see Figure 1 for graphic illustration). Previous models
like HNN or LNN do not support this feature.
2	Related Works
Neural Networks for Physical Problems. Researchers have already built different neural networks
for physical problems. For example, they implemented networks in precipitation forecasting (Shi
et al., 2015), chains motions (de Avila Belbute-Peres et al., 2018), and multi-particles interactions
(Battaglia et al., 2016; Kipf et al., 2018), where researchers designed recursive neural networks or
graphical neural networks to induce the relations and interactions among objects. But these models
do not encode any physical knowledge and can achieve nonphysical results.
Dynamical System for Neural Networks. Research also involved explaining neural networks
in dynamical-system views and treating the iteration of the neural network in a continuous way
(Weinan, 2017). Researchers further utilised the Pontryagin's maximum principle to formulate con-
tinuous systems with optimality (Li et al., 2017; Chen et al., 2018). These works expanded the
field of deep learning and pointed out a way to combine the paradigm of engineering and statistics.
However, it is still difficult to widely implement continuous system models in various types of tasks.
Physical Inductive Biases. Recently, several works have focused on designing neural networks
with physical inductive biases (Lutter et al., 2018; 2019; Greydanus et al., 2019; Toth et al., 2019;
Cranmer et al., 2020). These networks follow the problem-solving process in mechanics, learning
specific physical quantities to describe the whole system. In this way, researchers have applied these
networks in the control of the robotic arm (Lutter et al., 2018; 2019) and the prediction of the sea
surface temperature (De Bezenac et al., 2019), but also achieved important features in mechanics
such as energy conservation (Greydanus et al., 2019; Toth et al., 2019; Cranmer et al., 2020). How-
ever, their designs restrict that the network can only accept a fixed input size, so the learnt physical
properties cannot be reused or extended to other situations.
3	Preliminaries
In Lagrangian mechanics, the motion of the whole system is decided by its Lagrangian L, which is
a combination of the kinetic energy T and the potential energy U.
L(t, q,q) := τ(t, q,q) - U(t, q,q),	⑴
where t is time, q is the generalised position, and q is the generalised velocity. To describe dynam-
ical systems, we will introduce the Euler-Lagrange equations with or without constraints and then
derive the related dynamic equations.
2
Under review as a conference paper at ICLR 2022
Euler-Lagrange Equation. In Calculus of variations, the first-order of necessary conditions for
weak extrema leads to the Euler-Lagrange equation of the system (Liberzon, 2011):
∂L(t, q, q) = d_ ∂L(t, q, q)
∂ q	dt	∂q	.
(2)
From Equation 2, We can get the differential equations concerning q and q, thereby describing the
evolution of the system.
For generalisation, We Will consider variational problems With constraints in physics. These con-
straints can be divided into tWo groups, integral constraint in Equation 3 and non-integral constraint
in Equation 4:
Z Mi(t q,q)dt = Ci,
a
Mj (t q, q) = Cj.
(3)
(4)
Where Ci and Cj are constants corresponding to the i-th and j-th constraint equation Mi and Mj .
An approach to deal With these constraints is to utilise a Lagrange multiplier λ?, defining augmented
Lagrangian L(t, q, q) = (L + λ?M)(t, q, q) to substitute L in Equation 2. The multiplier λ? is a
number for an integral constraint, and λ? is a function λ?(t') for a non-integral constraint. λ? = 0
relates to the situation Without any constraint. λ? can be solved by combining Euler-Lagrangian
equation (Equation 2), constraints (Equation 3 and 4) and boundary conditions.
Dynamic Equations. When parameters in Equation 2 such as mass and size are not related to time
t, We can expand this equation as:
∂L(q,q)	d ∂L(t, q, q)	∂2L dq	∂2L dq
∂q	dt	∂q	∂q∂q dt + ∂q2 dt.
Then the resultant force and the acceleration can be calculated in Equation 5:
∂2L.. _ (∂L	∂2L Λ
∂q2 q	Idq ∂q∂(qqJ ,
(5)
4	Method
With Equation 5, We can describe the motion of a dynamical system once obtaining its Lagrangian.
We Will propose the Modular Lagrangian Neural NetWorks (ModLaNN) frameWork to shoW hoW to
construct the Lagrangian and hoW to simulate the dynamics formulated by Equation 5.
Lagrangian is made up of kinetic energy T and potential energy U, and correspondingly, ModLaNN
contains tWo branches to regress the forms of these tWo kinds of energy. In each branch, at least one
RelationCell layer is designed to process the input data. Each RelationCell contains one multilayer
perception (MLP) for regression and a recurrent structure to accept inputs With different dimensions.
The outputs Will be the global velocity and the potential field used to calculate T and U. Then We
can construct the Lagrangian With Equation 1, calculate its derivatives, and derive the accelerations
using Equation 5. The Whole structure is shoWn in Figure 2.
The advantages of our frameWork are that We can directly learn the properties of the elements from
the dataset of a type of system and then construct Lagrangian to describe the Whole system. Our
model can describe other systems With the same elements. This extrapolation feature enables us to
process inputs With different sizes rather than a fixed input size.
For optimisation, because our experiments are all for simulations, We calculated accelerations of
systems and optimised them using L2-loss
L = IIq — q∣∣2 +λ 忸 — e|L，	⑹
3
Under review as a conference paper at ICLR 2022
Figure 2: The compositional graph of the Modular Lagrangian Neural Network (ModLaNN). The
generalised position q and generalised velocity q are inputted into two branches and recursively
processed. Each branch can contain more than one RelationCell R to deal with inputs with dif-
ferent combination forms. For example, the input of Rq1 can be position qi for loop i to regress
gravitational potential, and the input of Rq2 may be positions (qi , qj ) for loop i and j to repre-
sent electric potential field. The outputs are used to calculate kinetic energy T , potential energy U,
and Lagrangian L. The derivatives of L will be utilised for the accelerations q or resultant force f
following Equation 5.
一 口口
φτ
Pu『y
厂




(a) Single pendulum system (b) Multi-pendulum system	(C) Multi-body SyStem
Figure 3: Three dynamical systems for illustrations. (a) shows a single pendulum system. Both
multi-pendulum systems (b) and multi-body systems (c) are highly chaotic systems whose dynamics
are hard to simulate (Shinbrot et al., 1992; Vaidyanathan & Volos, 2016).
where λ is a hyper-parameter, q and E represent the acceleration vector and energy outputted from
the network, and qq and E represent those of the ground truth. kE - E k2 is a regulation term to
guarantee the optimisation of two branches can go through the correct gradient direction together.
To introduce with more details, we present three cases to show how to implement our framework.
Case 1:	Single pendulum systems. A way to involve constraints is to view this system in Cartesian
coordinates shown in Figure 3(a), the Lagrangian then will be:
L(x, y, X, y) = ；mX2 + ；my2 - mgy + mλM(x, y),
where X and y are positions, X and y are velocities, and λ and M(x, y) can be expressed as:
λ⑴=1 (gy ⑴一(X2 ⑴ + y 2 (O)),
M(X,y)= h(X/l)2 + (y/l)2i.
Here l is the length of the pendulum and can be calculated directly from l = X2 + y2 .
Two RelationCell layers are separately used in two branches. One will reconstruct kinetic energy
and the other for gravitational potential. One additional branch consists of the dimensionless con-
straint function M(X, y) multiplying an MLP to model the constraint part. Inputs of this MLP are
derivatives of the kinetic and potential energy and related variations so that the base unit of the
output is the same as energy.
4
Under review as a conference paper at ICLR 2022
Case 2:	Multi-pendulum systems. A multi-pendulum system is n single pendulums that are linked
one by one like Figure 3(b), the Lagrangian of this system in polar coordinates is:
n-1 i-1 1	n-1 i-1
EX⅛, ljθj + miglj cos θj) +XX
miIjIkθjθk cos (θj - θk) I ,
i=0 j=0	i=0 j,k=0,j6=k
where mi, li, θi and θi are mass, pendulum length, angle position and angular velocity related to the
i-th pendulum. g is the standard gravitational acceleration.
Following the ModLaNN structure, in T branch, one RelationCell Rq Will accept Pair (θi,θi) to
regress the velocity expression. Then we will linearly reconstruct these velocities in the global
coordinates to obtain the T. In U branch, another RelationCell is to obtain the global position with
a for loop. Then we can construct L and its derivatives from T and U.
Case 3:	Multi-body systems. For multi-body systems, n particles move within the frame shown in
Figure 3(c). The Lagrangian is
L(χ, y, X, y)
n-1
X
i=0
n-1 i
2mi(x2 + y2) - XX
i=0 j=0
mimj
VZ(Xi - Xj)2 + (yi - y )2
where mi, (Xi, yi), and (Xi, yn) are mass, positions and velocities related to the i-th particle.
For modelling this system, one RelationCell Rq will accept velocities (Xi, y^ to calculate T and
another RelationCell Rq will process the positions pair (X i, y%, X j∙, yj∙) with for loop to calculate U.
The outputs will be used to construct L along with its derivatives.
5 Experiments
Based on the three cases in the last section, we designed three experiments to reveal the performance
of our model compared to counterparts and to examine the extrapolation feature of our model. Our
model was built using PyTorch (Paszke et al., 2019) and experiments were conducted in Ubuntu
20.04 using single core i7@3.7GHz. These experiments can run without the usage of GPU.
Ex.1: Pendulum systems with different lengths. Ex.1 consists of two steps. First, we trained
models using datasets with fixed pendulum lengths (l = 1 m or 4 m) and made comparison. Second,
we trained our ModLaNN model in the dataset with varying lengths. The training and testing data
were collected with lengths in the range of [1, 7] and [7, 9], respectively. For settings, the noise was
set to be 0 or 0.1. The learning rate was set to be 10-2 and the training epochs were 2000. For loss
function in Equation 6, λ was chosen as 10-1.
Training results are shown in Table 1. Our method outperforms others in the first step. ModLaNN
can also handle data with varying lengths in the second step, where the loss is smaller compared
with that in the first step. This result can be explained that training with more data also improves the
model's performance in return.
Table 1: Final loss of three models after training in Ex.1 (σ = 0.1).
Train Loss				Test Loss				
Type	ModLaNN	HNN (Toth et al., 2019)	Baseline (Horniketal., 1989)	ModLaNN			HNN	Baseline
l=1	3.7× 10-1	2.1 × 100	3.3 × 100	4.2	×	10-1	5.9 × 100	4.8 × 100
l=4	2.6× 10-3	2.9 × 10-1	3.4 × 100	2.3	×	10-3	3.5× 10-1	4.8 × 10-1
l ∈ [1, 9]	2.4 × 10-4			3.3	×	10-4		
For step one, Figure 4 illustrates the simulation performance of different models. The length of
the pendulum is 1 m. The dynamics are integrated with models using the ODE solver offered in
SciPy (Virtanen et al., 2020). Our model can ensure the smallest mean squared errors (MSEs)
in coordinates and energy. HNN also performs well but slightly diverges from the ground truth.
Baseline performs the worst, with large MSEs in trajectory and energy.
5
Under review as a conference paper at ICLR 2022
(e) All trajectories
(f) MSE between coordinates
(g) MSE of total energy
Figure 4: Simulation results of a single pendulum system (best view in colour). Colours represent
the change of time marked in the colour bar. Points mark the end of the trajectory. Our prediction
(b) is very close to the ground truth (a). The endpoint of HNN's trajectory (c) implies the difference
between the HNN simulation and the ground truth. (d) reveals that the baseline's trajectory quickly
diverges. (e) puts all trajectories together for better comparison, where HNN's orbit is smaller than
the ground truth. (f) and (g) show our model can maintain minor mean squared errors (MSEs) in
coordinates and energy.
Figure 5: Simulation of pendulum systems with
varying lengths (best view in colour). Each curve
represents one trajectory of a pendulum. The
colour bar marks the time change.
For step two, Figure 5 reveals results when
our model simulates pendulums with different
lengths. Our model can achieve stable results
even when the length is out of the training range
([7, 10]). The constraint part already contained
the information of the length, so once trained
with enough data, it can remain stable to those
domains not covered during training.
Ex.2: Double pendulum systems to multi-
pendulum systems. This experiment was di-
vided into two parts. First, we compared
the performance of different methods based on
double-pendulum datasets. Then we applied
our ModLaNN model to multi-pendulum sys-
tems to show its ability in extrapolation. The
length and mass of each object were set to be 1
m and 1 kg. For training settings, the training epochs were 10000, and the learning rate was chosen
the best from [10-4, 10-1].
Training results are shown in Table 2. The training of the other two models remained divergent
after great efforts. Figure 6 also clearly shows the training results of three models. Our model can
achieve an accurate prediction of the trajectory. The coordinates error and the energy error are all
under control, below 10-4 and 10-1, respectively. The performances of the other two models are
not optimistic, with significant MSEs in coordinates and energy.
For the extension to multi-pendulum systems, we directly loaded the trained model in the first step
and simulated a triple-pendulum and a quadruple-pendulum system. The initial conditions were
chosen arbitrarily. The results are shown in Figure 7. The trajectories of objects in the pendulum
systems are very close to the ground truth, which means our model truly learned the dynamics
within systems. MSEs show that the simulation of higher-order pendulum systems diverges with
time, which is because of the chaotic property of these systems (Shinbrot et al., 1992). Even slight
variances of chaotic systems will lead to extremely different dynamic behaviours.
6
Under review as a conference paper at ICLR 2022
(a) True trajectory
(b) ModLaNN trajectory (ours)
(C) HNN trajectory
(d) Baseline trajectory
Time step (s)
Time step (s)
(e) MSE between coordinates
(f) MSE of total energy
Figure 6: Simulation results of a double pendulum system (best view in colour). Only our model (b)
can make precise predictions close to the ground truth (a), while the HNN model (c) and baseline
model (d) cannot converge. MSEs related to our model in (e) and (f) are much smaller than those of
the other two models.
(a) Object 0's trajectory
(b) Object 1's trajectory	(C) Object 2's trajectory
Triple Pendulum
System
Quadruple Pendulum
System
Figure 7: Simulation results of multi-pendulum (> 2) systems for extrapolations (best view in
colour). The trajectories of the triple pendulum (a-c) and the quadruple pendulum (h-k) are listed
compared with the ground truth. MSEs of different components shown in (d-g) indicate that predic-
tions by our model are close to the ground truth.
7
Under review as a conference paper at ICLR 2022
Ex.3: Three-body systems to multi-body systems. Multi-body systems are also complicated
chaotic systems (Vaidyanathan & Volos, 2016). The procedure of Ex.3 is the same as Ex.2, with
the scenario changing to multi-body systems. We first trained three models using the dataset of
three-body systems and then applied ModLaNN to multi-body systems for extrapolation. The train-
ing epochs were set to be 2000, and the learning rate was chosen the best from [10-4, 10-1].
Table 2 lists the training results and Figure 8 gives a clear illustration of how these three models
perform. The first row of Figure 8 demonstrates the trajectory and the second row reveals the change
of energy. Even though the training and testing losses of every model are lower than 10-3 in Table
2, the prediction of the baseline network diverges fast from the ground truth, causing an intensive
increase in kinetic energy and total energy. ModLaNN and HNN can ensure that total energy is
roughly conservative, while the trajectory of body 1 implies that our ModLaNN model outperforms
HNN.
We can also see from Figure 9 for the extrapolation effects in 4, 5, 6-body systems using ModLaNN.
Each body's trajectory and each part of the energy can be close to the ground truth.
Table 2: Final loss after training in Ex.2 and Ex.3.
Train Loss	Test Loss
TyPe^^ModLaNN HNN	Baseline	ModLaNN HNN Baseline^^
(Toth et al., 2019) (HOmik et al., 1989)
1X2^^4.0 × 10-2^^1.0 × 101	9.0 × 10-4	4.0 × 10-2^^2.3 × 103	4.4 × 102
IX3^^7.7 × 10-5^^2.2 × 10-4	4.5 × 10-4	8.3 × 10-5^^3.1 × 10-4^^4.7 × 10-4
Figure 8: Simulation results of a three-body system (best view in colour). Our Prediction (b) here
is closest to the ground truth (a). The trajectory of object 1 Predicted by HNN (c) is away from the
ground truth. The baseline Performance is the worst, with the orbit (d) and energy (h) divergence.
Our ModLaNN (f) and HNN model (g) are also able to Predict the change of the energy, while our
method can achieve more Precise Predictions to the ground truth (e).
8
Under review as a conference paper at ICLR 2022
(b) ModLaNN trajectory (ours)
(a) True trajectory
(d) ModLaNN energy (ours)
(C) True energy
(e) True trajectory
(f) ModLaNN trajectory (ours)
(g) True energy
(h) ModLaNN energy (ours)
Figure 9: Simulation results of multi-body (> 3) systems for extrapolations (best view in colour).
The trajectories corresponding to 4, 5, 6-body systems (b, f, j) simulated using the ModLaNN model
can grasp the main motion features similar to the ground truth (a, e, i). Our model can also predict
energy change and retain the conservation of total energy (d, h, l) like the ground truth (c, g, k). As
a result, our model can make great extensions to multi-body systems with the same law of physics,
predict the correct motion and ensure the stability of the energy.
6	Conclusion
We proposed a kind of structural neural network called Modular Lagrangian Neural Networks. Com-
pared with relevant models such as LNN and HNN that directly output Lagrangians or Hamiltonians,
our model aims at learning to regress different kinds of energy for objects in a system and then con-
structs the Lagrangian to describe the dynamics of this system. In this way, our models can simulate
the dynamics of more complicated systems as long as they consist of the same objects after training.
Three experiments were conducted to reveal the importance of this feature, where our model has
achieved the best results during training and testing than the counterparts. At the same time, these
trained models could directly make generalisations in multi-pendulum and multi-body systems with
robust extrapolation performance.
References
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, and Koray kavukcuoglu.
Interaction networks for learning about objects, relations and physics. In Advances in Neural
Information Processing Systems (NeurIPS 2016), pp. 4509-4517, 2016.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems (NeurIPS 2018), pp.
6572-6583, 2018.
9
Under review as a conference paper at ICLR 2022
Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho.
Lagrangian neural networks. In International Conference on Learning Representations Workshop
on Integration of Deep Neural Models and Differential Equations, 2020.
Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter. End-
to-end differentiable physics for learning and control. Advances in Neural Information Processing
Systems (NeurIPS 2018), 31:7178-7189, 2018.
Emmanuel De Bezenac, Arthur Pajot, and Patrick Gallinari. Deep learning for physical processes:
Incorporating prior scientific knowledge. Journal of Statistical Mechanics: Theory and Experi-
ment, 2019(12):124009, 2019.
Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances in
Neural Information Processing Systems (NeurIPS 2019), 2019.
Jan Hermann, Zeno Schatzle, and Frank Noe. Deep-neural-network solution of the electronic
schrodinger equation. Nature Chemistry, 12(10):891-897, 2020.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2688-
2697. PMLR, 2018.
Qianxiao Li, Long Chen, Cheng Tai, and E Weinan. Maximum principle based algorithms for deep
learning. The Journal of Machine Learning Research, 18(1):5998-6026, 2017.
Daniel Liberzon. Calculus of variations and optimal control theory. Princeton university press,
2011.
Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning. In International Conference on Learning Representations, 2018.
Michael Lutter, Kim Listmann, and Jan Peters. Deep lagrangian networks for end-to-end learning
of energy-based control for under-actuated systems. In IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS 2019), pp. 7718-7725. IEEE, 2019.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS 2019), pp.
8024-8035. Curran Associates, Inc., 2019.
David Pfau, James S Spencer, Alexander GDG Matthews, and W Matthew C Foulkes. Ab initio
solution of the many-electron schrodinger equation with deep neural networks. Physical Review
Research, 2(3):033429, 2020.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo.
Convolutional lstm network: a machine learning approach for precipitation nowcasting. In In-
ternational Conference on Neural Information Processing Systems (NeurIPS 2015), pp. 802-810,
2015.
Troy Shinbrot, Celso Grebogi, Jack Wisdom, and James A Yorke. Chaos in a double pendulum.
American Journal of Physics, 60(6):491-499, 1992.
Peter Toth, Danilo J Rezende, Andrew Jaegle, SebaStien RaCaniere, Aleksandar Botev, and Irina
Higgins. Hamiltonian generative networks. In International Conference on Learning Represen-
tations, 2019.
Sundarapandian Vaidyanathan and Christos Volos. Advances and applications in chaotic systems,
volume 636. Springer, 2016.
10
Under review as a conference paper at ICLR 2022
Siddhartha Verma, Guido Novati, and Petros Koumoutsakos. Efficient collective swimming by
harnessing vortices through deep reinforcement learning. Proceedings of the National Academy
OfSciences,115(23):5849-5854, 2018.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods, 17:261-272, 2020.
E Weinan. A proposal on machine learning via dynamical systems. Communications in Mathematics
and Statistics, 5(1):1-11, 2017.
11
Under review as a conference paper at ICLR 2022
A Appendix
A.1 Ex.1: Single pendulum systems
Figure 10 is an addition of Figure 4 in Ex.1. It shows the performances of different models in
simulating single pendulum systems. Each model is separately trained twice using two datasets.
The only difference between datasets is the pendulum's length.
(b) ModLaNN trajectory (ours)
(d) Baseline trajectory
(k) Baseline trajectory
-- Baseline NN
■■■ HNN
■■■ ModLaNN (Ours)
一- Baseline NN
HNN
■■■ ModLaNN (Ours)
O 2
4(。)
O	20
Time step (s)
(l) All trajectories
(m) MSE between coordinates
(n) MSE in total energy
Timeline
TimeHne
(a) True trajectory
—Ground truth
---Baseline NN
---HNN
……ModLaNN (Ours)
O 2
q(。)
(e) All trajectories
(h)	True trajectory
(i)	ModLaNN trajectory (ours)
(f) MSE between coordinates
O	20
Time step (s)
---Ground truth
-一・ Baseline NN
---HNN
………ModLaNN (Ours)
Figure 10:	Training results of pendulum systems with different datasets. In (a-g), models are trained
using the dataset in which the pendulum's length is 4 m. The performances of HNN (c) and baseline
model (d) are close to the ground truth (a). In (h-n), models are trained when the length is 1 m. More
significant divergences exist in trajectories predicted by HNN (j) and baseline model (k) compared
with the ground truth (h). By contrast, our model made precise predictions in both situations (b) and
(i). This difference can be seen using MSEs in (f-g) and (m-n). These results show that HNN and
baseline models are sensitive to the chosen datasets, while our models are robust.
A.2 Ex.3: multi-body systems
Figure 11	is an addition of Figure 8 in Ex.3. In this figure, we show the simulation results of three-
body systems with three different initial conditions. The results contain the trend of trajectories and
energy. Our models get better predictions in all three situations and capture the trend of movements
of the whole system.
12
Under review as a conference paper at ICLR 2022
Time step (s)	Time step (s)
Time step (s)
(o) HNN energy
Time step (s)
(P) Baseline energy
Time step (s)
(m) True energy
Time step (S)
(n) ModLaNN energy (ours)
Time step (s)
(x) Baseline energy
Time step (s)
(U) True tnergy
Time step (s)
(W) HNN energy
Time step (s)
(V) ModLaNN energy (ours)
Figure 11: Simulation results of three-body systems in three cases. Three models make their predic-
tions in three initial conditions (a-h), (i-p), and (q-x). Within these cases, our model made the most
precise predictions of the change of motion and energy.
13
Under review as a conference paper at ICLR 2022
A.3 Architecture details
Our model is implemented and trained using PyTorch (Paszke et al., 2019). The model contains two
input branches for calculating the kinetic energy and the potential energy. Each branch consists of
several RelationCells, and Each RelationCell includes an MLP network with three fully connected
layers and two tanh activation layers. The hidden dimension of the MLP is set to be 16 or 50. For
kinetic energy T , the branch will accept data of each object with a for loop. Then the outputs will
be linear reconstructed to generate the velocity in global coordinates and lead to T . For potential
energy U , a different RelationCell with the same setting will accept the combinations of positions
to reconstruct the potential field. Then we will add the outputs all together to get U. Lagrangian
is calculated by L = T - U, and the differentiations of L will be used to generated acceleration
through equation 5.
14