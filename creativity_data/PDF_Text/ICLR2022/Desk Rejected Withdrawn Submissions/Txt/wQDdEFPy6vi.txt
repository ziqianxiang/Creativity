Under review as a conference paper at ICLR 2022
Local Learning Matters:	Rethinking Data
Heterogeneity in Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning (FL) is a promising strategy for performing privacy-preserving,
distributed learning with a network of clients (i.e., edge devices). However, the
data distribution among clients is often non-IID in nature, making efficient opti-
mization difficult. To alleviate this issue, many FL algorithms focus on mitigating
the effects of data heterogeneity across clients by introducing a variety of prox-
imal terms, some incurring considerable compute and/or memory overheads, to
restrain local updates with respect to the global model. Instead, we consider re-
thinking solutions to data heterogeneity in FL with a focus on local learning gen-
erality rather than proximal restriction. Inspired by findings from generalization
literature, we employ second-order information to better understand algorithm ef-
fectiveness in FL, and find that in many cases standard regularization methods are
surprisingly strong performers in mitigating data heterogeneity effects. Armed
with key insights from our analysis, we propose a simple and effective method,
FedAlign, to overcome data heterogeneity and the pitfalls of previous methods.
FedAlign achieves comparable accuracy with state-of-the-art FL methods across
a variety of settings while minimizing computation and memory overhead.
1 Introduction
Federated learning (FL) (Kairouz et al., 2021) enables a large number of clients to perform collab-
orative training of machine learning models without compromising data privacy. In the FL setting,
participating clients are typically deployed in a variety of environments or owned by a diverse set
of users. Therefore, the distribution of each client’s local data can vary considerably (i.e., data het-
erogeneity). This non-IID data distribution among participating devices in FL makes optimization
particularly challenging. As each client trains locally on their own data, they step towards their
respective local minimum. However, this local convergence point may not be well aligned with
the objective of the global model (that is, the model being learned though aggregation at the cen-
tral server). Therefore, the client model often drifts away from the ideal global optimization point
and overfits to its local objective. When such client drifting occurs, the performance of the central
aggregated model is hindered (Kairouz et al., 2021; Li et al., 2021a).
One straight-forward solution to this phenomenon is to simply limit the number of local training
epochs performed between central aggregation steps. However, this severely hinders the conver-
gence speed of the FL system, and many communication rounds are required to achieve adequate
performance. The time to convergence and immense network overhead incurred by such an ap-
proach are often not tolerable for real-world distributed systems. Therefore, effectively addressing
data heterogeneity is of paramount concern in federated learning.
Many algorithmic solutions to this problem have been proposed in the literature (Sahu et al., 2018;
Li et al., 2021b; Karimireddy et al., 2020; Acar et al., 2021). These strategies typically focus on
mitigating the effects of data heterogeneity across clients by introducing a variety of proximal terms
to restrain local updates with respect to the global model. However, by restraining the drift, they
also inherently limit the local convergence potential; less novel information is gathered per com-
munication round. Consequently, many current FL algorithms do not provide stable performance
improvements across different non-IID settings in comparison to classic baselines (Li et al., 2021a).
Furthermore, existing methods have paid little attention to the resource constraints of the client, typ-
1
Under review as a conference paper at ICLR 2022
ically scarce for deployed FL edge devices, and in some cases incur considerable compute and/or
memory overheads on the client in their effort to alleviate client drift.
Motivation. In the centralized training paradigm, network generalization capability has been well
studied to combat overfitting. Even in standard settings where the training and test data are drawn
from a similar distribution, models still overfit on the training data if no precautions are taken. This
effect is further intensified when the training and test data are of different distributions. Various
regularization techniques are introduced to enforce learning generality during training and preserve
suitable test performance. Similarly, overfitting to the local training data of each device in FL is
detrimental to overall network performance, as the client drifting effect creates conflicting objectives
among local models. Thus, a focus on improving model generality should be of primary concern
in the presence of data heterogeneity. Improving local learning generality during training would
inherently position the objective of the clients closer to the overall global objective. However, de-
spite its intuitive motivations, this perspective has been overlooked by the bulk of current FL litera-
ture.
Therefore, in this paper, we propose rethinking approaches to data heterogeneity in terms of local
learning generality rather than proximal restriction. Specifically, we carefully analyze the effective-
ness of various data and structural regularization methods at reducing client drift and improving FL
performance (Section 3). Utilizing second-order information and insights from out-of-distribution
generality literature (Rame et al., 2021; Parascandolo et al., 2020), we identify theoretical indicators
for successful FL optimization, and evaluate across a variety ofFL settings for empirical validation.
Although some of the regularization methods perform well at mitigating client drift, significant re-
source overheads are still incurred to achieve the best performance (see Section 4). Therefore, we
propose FedAlign, a distillation-based regularization method that promotes local learning general-
ity while maintaining excellent resource efficiency. Specifically, FedAlign focuses on regularizing
the Lipschitz constants of the final block in a network with respect to its representations. By fo-
cusing solely on the last block, we effectively regularize the portion of the network most prone to
overfitting and keep additional resource needs to a minimum. Therefore, FedAlign achieves state-
of-the-art accuracy on multiple datasets across a variety of FL settings, while requiring significantly
less computation and memory overhead in comparison to other state-of-the-art methods.
Our contributions are as follows:
•	We approach one of the most troublesome FL challenges (i.e., client drift caused by data hetero-
geneity) from a unique angle than any other previous work. We do not focus on reparameterization
tricks to maintain closeness to the central model, or adjust the aggregation scheme to mitigate the
effects of non-IID data distributions. Rather, we propose the rethinking of this problem from funda-
mental machine learning training principles. In this way, we analyze the performance of standard
regularization methods on FL and their effectiveness against data heterogeneity.
•	Not only do we empirically analyze the performance of regularization methods in FL, we also
propose to take a deeper look. Specifically, we inform our analysis with theoretical indicators of
learning generality to provide insight into which methods are best and why. Our aim is to provide
this valuable knowledge to the FL community to inspire new, productive research directions.
•	Informed by our analysis and examining the pitfalls of previous methods, we propose FedAlign,
which achieves state-of-the-art accuracy while maintaining memory and computational efficiency.
2	Related Work
Federated Learning. In general, federated learning algorithms aim to obtain a collective model
which minimizes the training loss across all clients. This objective can be express as
C
min F (w) = XαcFc(w),	(1)
w
c=1
where Fk(w) is the local loss of device c, and αc is an arbitrary weight parameter with PcC=1 αc =
1. One of the earliest algorithms proposed in FL is Federated Averaging, or FedAvg (McMahan
et al., 2017). This approach simply optimizes the local training loss with standard SGD training,
and aggregates using a weighted average approach with a。= nnc, where n is equal to the number
of training samples on client c, with a total ofn training samples partitioned across all C clients.
2
Under review as a conference paper at ICLR 2022
Recent works attempt to improve over this baseline with two distinct focuses: improvements to the
local training at the client, or improvements to the global aggregation process at the server. In this
work, We focus on local training and Client drift, and therefore We will first discuss methods of this
nature. To mitigate data heterogeneity complications, a common approach is to introduce proximal
terms to the local training loss. For instance, FedProx (Sahu et al., 2018) forms the local objective
Fk (W) + 2 ∣∣w - wt k2, where μ is a hyperparameter, W is the current local model weights, and Wt is
the global model weights from round t. The goal of this reparameterization is to minimize client drift
by limiting the impact of local updates from becoming extreme. More recently, MOON (Li et al.,
2021b) proposes a similar reparameterization idea inspired by contrastive learning. Specifically, the
authors form a local model constrastive loss comparing representations of three models: the global
model, the current local model, and a copy of the local model from the previous round. The goals of
this term are similar to that of FedProx but in feature representation space; to push the current local
representation closer to the global representation. At the same time, the current local model is being
pushed away from the representations of the local model copy of the previous round. Other methods
(Acar et al., 2021; Karimireddy et al., 2020) follow similar ideas; they aim to limit the impact of the
local update or shift the update with a correction term.
However, these approaches have two main downsides. First, by restraining the drift, they also in-
herently limit the local convergence potential. With this, not as much new information is gathered
per communication round. Second, many of these methods incur substantial overheads in mem-
ory and/or computation. For instance, because of its model constrastive loss, MOON (Li et al.,
2021b) requires the storage of three full-size models in memory simultaneously during training,
and forward passing through each of these every iteration. This requires a great deal of additional
resources, which are often already scarce in FL client settings.
Other works focus on the server side of the system, aiming to improve the aggregation algorithm.
Yurochkin et al. (2019) propose a Bayesian nonparametric method for matching neurons across local
models at aggregation rather than naively averaging. However, the presented framework is limited
in application to fully-connected networks, and therefore Wang et al. (2019) extend it to CNNs and
LSTMs. FedNova (Wang et al., 2020) presents a normalized averaging method as an alternative to
the simple FedAvg update. As we focus on the local training, these works are orthogonal to our
work. A few approaches (Yoon et al., 2021; Oh et al., 2020; Shin et al., 2020) propose federated
schemes inspired by the data augmentation method Mixup, using similar averaging techniques on
the local data and sharing the augmented data with the global model or other devices. However, even
though the data is augmented in some way prior to distribution, the sharing of private data from the
client is less than ideal for privacy preservation. Furthermore, sharing additional data worsens the
communication burden on the system, which is a principal concern in FL.
Learning Generality. In traditional centralized training, the practice of regularization of various
forms is common practice for improving generality. Data-level regularization, such as basic data
augmentation and other more advanced techniques (Zhang et al., 2018; Yun et al., 2019), are known
to be quite effective. Other methods introduce a level of noise to the training process via structural
modification; for instance, random or deliberate modifications to the network connectivity (Huang
et al., 2016; Ghiasi et al., 2018; Tompson et al., 2015). Yang et al. (2020) proposes a hybrid approach
that introduces self-guided gradient perturbation to the training process through the use of sub-
network representations, knowledge distillation, and input transformations. As part of this work, we
employ a variety of regularization methods in many FL settings and analyze their performance in
comparison to state-of-the-art FL algorithms.
3	Empirical S tudy
We wish to assess the data heterogeneity challenge of FL from a simple yet unique perspective
of local learning generality. Specifically, we first study the effectiveness of standard regularization
techniques as FL solutions in comparison to state-of-the-art methods.
3.1	Preliminaries
We employ three FL algorithms, namely FedAvg, FedProx, and MOON. These works represent both
classic baselines and current state-of-the-art, and are described in Section 2. For comparison, we
3
Under review as a conference paper at ICLR 2022
employ three state-of-the-art regularization methods: Mixup (Zhang et al., 2018), Stochastic Depth
(Huang et al., 2016), and GradAug (Yang et al., 2020).
Mixup is a data-level augmentation technique that performs linear interpolation between two
samples. Specifically, given two sample-label pairs (xi, yi) and (xj , yj), they are combined as
X = βxi +(1 一 β)xj and y = βyi + (1 — β)y7-, where β 〜Beta(γ, γ).
Stochastic depth is a structural-based method that drops layers during training, thereby creating an
implicit network ensemble of different effective lengths. Specifically, the output of layer (or residual
block) ' is given by z` = σ (λFθ' (Z'-ι) +1 (Z'-ι)), where λ is a Bernoulli random variable, fθ`
is the operation within the network with parameter θ at layer `, I is the identity mapping operation
of residual connections, and σ is a non-linear activation function. The keep probability is defined as
γ = P(λ = 1), where in practice each layer has its own keep probability set with a linear decay rule
Y' = 1 — L (1 — YL), with L denoting the total number of layers (or blocks) in the network.
GradAug is a recent regularization approach that combines data-level and structural techniques in a
distillation-based framework. Specifically, the training loss is defined as
n
LGA = LCE(Fθ(x),y) + μX LKD (Fθ-i (Ti(χ)), Fθ(x)),	(2)
i=1
where Fθωi denotes a slimmed sub-network of fractional width ωi, Ti is a transformation performed
on the input (e.g. resolution scaling), and μ is a balancing parameter between the cross-entropy loss
LCE and the summed Kullback-Leibler divergence (LKD) loss on the sub-networks.
3.2	Experimental Setup
To begin our analysis, we test the accuracy of several state-of-the-art FL algorithms with sev-
eral regularization methods in a common FL setting. We perform experiments using CIFAR-100
(Krizhevsky et al.), an image recognition dataset with 50,000 training images across 100 categories,
and ResNet56 (He et al., 2016) as the model. As common in the literature (Li et al., 2021b; Acar
et al., 2021; He et al., 2020), the dataset is partitioned into K unbalanced subsets using a Dirichlet
distribution (Dir(α)), with the default being α = 0.5. With this data partitioning scheme, it is pos-
sible for a client to have no samples for one or multiple classes. Therefore, many clients will only
see a portion of the total class instances. This makes the setting more realistic and challenging. For
all methods and experiments we use an SGD optimizer with momentum, and a fixed learning rate of
0.01. In our basic setting, training is COndUCted for 25 rounds, With 16 CIientS and 20 local epochs
Per round. Any modifications to this setting in subsequent results will be stated clearly.
We compare the previously described FL algorithms and regularization methods. FedProx, MOON,
and GradAug all have a hyperparameter μ to balance their additional loss terms. We report all
results with the optimal μ for all approaches, being 0.0001, 1.0, and 1.75 for FedProx, MOON,
and GradAug respectively. For Mixup and Stochastic Depth, Y and YL are set to 0.05 and 0.9
respectively. For GradAug specifically, the number of sub-networks n = 2. A two-layer projection
layer is added to the model for MOON as specified in the original paper. Basic data augmentations
(random crop, scale, and normalization) are kept consistent across all methods.
3.3	Results Comparison
The accuracy results are shown in Table 1.
Within the current state-of-the-art FL algo-
rithms (upper portion of Table 1), MOON
achieves the best accuracy. This is expected, as
MOON is the most intricate of the FL methods,
requiring the USage of three individual models
for its COntraStiVe Iearning technique. However,
when we compare with standard regularization
techniques (Mixup, StochDepth and GradAug
in the lower portion of Table 1), we see that
these perform similarly or substantially better.
GradAug particularly stands out, achieving an
Table 1: Results for accuracy (%) on CIFAR-100
and second-order metrics indicating the smooth-
ness of the loss space (λmax, HT) and cross-client
consistency (HN, HD) for each method.
Method	Acc. ↑	λmax ]	HT J	HN J	HD↑
FedAvg	52.9	297	6240	11360	0.98
FedProx	53.0	270	6132	6522	0.98
MOON	55.3	252	5520	5712	0.97
Mixup	54.0	216	5468	15434	0.99
StochDepth	55.5	215	3970	8267	0.97
GradAug	57.1	167	2597	2924	0.96
4
Under review as a conference paper at ICLR 2022
accuracy 〜2% higher than MOON and 〜4% higher than FedAvg and FedProx. StochDepth also
achieves similar accuracy to MOON. Furthermore, these regularization methods bring the same or
better performance than MOON, with much less memory and/or compute requirements. We find
that regularization methods appear to have an advantage in this situation; however, we wish to fur-
ther investigate why this could be the case. Next, we present our in-depth analysis based on the
second-order information in Section 3.4.
3.4	Algorithm Analysis based on Second-order Information
120
IOO
80
60
40
20
0
(a) FedAvg
120
100
80
i 60
40
20
0
, , I
0	20 40	60	80 100 120
(b) GradAug
Figure 1: Visualization of
the parametric loss landscape
with Hessian eigenvectors 0
and 1 for each resulting
global model.
Recent works in the Neural Architecture Search domain (Chen &
Hsieh, 2020; Zela et al., 2020), as well as in network generaliza-
tion (Keskar et al., 2016; Yao et al., 2018; Jiang* et al., 2020),
have noted the importance of the top Hessian eigenvalue (λmaχ)
and Hessian trace (HT) as a predictor of performance and indica-
tor of network generality. Having a lower λmaχ and HT typically
yields a network that is less sensitive to small perturbations in the
networks weights. This has the beneficial effects of smoothing the
loss space during training, reaching a flatter minima, and easing
convergence. These properties are particularly advantageous in fed-
erated learning, where extreme non-IID distributions and limited
local data often make convergence difficult.
Motivated by these insights, We analyze the top Hessian eigenvalue
and Hessian trace of the global models trained with each FL scheme
to provide insight into the effectiveness of each method.
As described in Yao et al. (2020), the top Hessian eigenvalues can
be approximated with the Power Iteration (Yao et al., 2018) method
using a simple inner product and standard backpropagation. How-
ever, since neural networks are extremely high dimensional, just
measuring the top few eigenvalues may not be sufficient. There-
fore, Yao et al. (2020) also find a similar approximation for the trace
utilizing the Hutchinson method (Hutchinson, 1989). We conduct
our analysis with the top Hessian eigenvalues and trace of the final
averaged models using these methods.
In Table 1, we include the results of the Hessian analysis. First, we find that FedAvg has the highest
λmax and HT. FedProx and MooN each result in lower values, indicating some degree of improved
generalization. However, interestingly, we find that regularization methods are most effective at
reducing the λmax and HT , with GradAug having by far the lowest in both values. We visualize
the effect of this reduction in λmaχ and HT in Fig. 1, where it can be seen that GradAUg is able to
smooth out the loss IandSCaPe COnSiderabIy in COmPariSOn to FedAvg.
In the separate field of out-of-distribution (o.o.D.) generalization for centralized training, second-
order information is being found quite useful as a theoretical indicator. Recent works (Parascandolo
et al., 2020; Rame et al., 2021) find that forming representations that are “hard to vary” seem to
result in better o.o.D. performance. More specifically, they show that the resulting loss landscapes
across domains for the learned model should be consistent with each other. In terms of theoretical
indicators, this translates to matching domain-level Hessians, as the Hessian provides an approxima-
tion of local curvature. Similarly, in federated learning, each client is essentially a separate domain.
Therefore, matching Hessians in norm and direction across clients reveals additional detail and rea-
soning behind the effectiveness of each method. In light of these findings in o.o.D. literature, we
analyze the difference in Hessian norm (HN) and the Hessian direction across clients (HD), where
HNk,j = kDiag(Hk)k2F- kDiag (Hj)k2F 2 and
Hk,j =	Diag(Hk) Θ Diag(Hj)
D - kDiag (Hk)kF "g (Hj)kF .
(3)
(4)
Here, Θ is the dot product, Hk and Hj are the Hessian matrices of clients k and j, and |卜|后 is
the Frobenius norm. HNk,j and HDk,j are averaged across all pairs of clients and reported as simply
5
Under review as a conference paper at ICLR 2022
HN and HD in Table 1. For these Hessian matching criteria, a lower HN (less difference) and a
higher HD (essentially the cosine similarity) are desired.
As seen on the right side of Table 1, HD is fairly consistent across all methods. In terms of λmax,
HT , and HD , most methods seem to correlate decently well between these values and performance.
However, there are a couple of cases which require more information. First, Mixup has a similar
HT value as MOON, but lower accuracy. HN provides an essentially detail; the Hessian norms
of Mixup are not nearly as similar across clients as those of MOON. Second, MOON has both a
higher λmax and HT ; however, it achieves similar accuracy to StochDepth. We again find that the
difference is in HN ; the Hessian norms of MOON are more similar across clients than those of
StochDepth. In the end, MOON and StochDepth result in very similar performance.
Take-away. It appears that both the eigenvalue/trace analysis and Hessian matching criteria are
important, and optimal methods should perform well across all these indicators. To understand how
these differences will play out empirically, we conduct a variety of ablations in Section 3.5.
3.5	Ablation Study under Various FL Settings
Data Heterogeneity. Federated systems can be deployed with many different setups and
diverse environments. Therefore, we conduct further analysis across a variety of FL set-
tings to ensure the generality of our findings. First, we examine the effect of vary-
ing the degree of heterogeneity in the client data distributions. The results are shown
All other settings are maintained from Table 1; only the data distribu-
indicates a more heterogeneous distribution.
in Table 2.
tion Dir(α) is varied. A lower α value
As the degree of data heterogeneity decreases, the
effect of client drift should also become less sig-
nificant. Therefore, we expect that the accuracy
for each method will increase, with peak perfor-
mance in the homogeneous setting. All regular-
ization methods, as well as FedAvg, perform as
expected, and find consistent improvement across
the degrees of data distribution. However, we see
that the accuracy improvement of FedProx and
Table 2: Ablation results for data heterogeneity.
Method	α = 0.1	α = 0.5	α=2	homog
FedAvg	45.1	52.9	54.2	56.2
FedProx	45.5	53.0	53.8	54.5
MOON	46.1	55.3	56.3	55.7
Mixup	43.5	54.0	55.5	56.3
StochDepth	48.0	55.5	57.4	58.6
GradAug	48.8	57.1	59.4	60.7
MOON slows as the data approaches homogeneity, with the purely homogeneous setting (“homog”
in Table 2) resulting in lower performance for MOON than α = 2. In their attempt to mitigate
client drift and keep local updates close to the global model, it appears that they also hinder their
own ability to fully learn on minorly heterogeneous or even homogeneous data. This is not ideal for
deployable FL systems, as the degree of heterogeneity is not known ahead of time. Additionally,
even in the most heterogeneous cases, the structural regularization methods perform better than the
standard FL algorithms. For instance, even though StochDepth and MOON achieve similar results
at α = 0.5, we see that StochDepth achieves +1.9% at α = 0.1 as well as similar improvement in
more homogeneous situations. In all settings, GradAug performs the best.
Number of Local Training Epochs. The main purpose for
adequately handling data heterogeneity is to allow for more
productive training on the client each round, therefore reduc-
ing the time to convergence and required communication cost.
Therefore, to examine the training productivity of each method,
we examine their accuracy with various allotted local training
epochs per round (E). The results are shown in Table 3.
Ideally methods should continue to improve in accuracy with
more allotted local training epochs. In Table 3, we see that all
Table 3: Ablation results for num-
ber of local epochs (E).
Method E = 10 E = 20 E = 30
FedAvg	50.6	52.9	53.2
FedProx	50.9	53.0	52.8
MOON	50.3	55.3	55.2
Mixup	505	54.0	54.2
StochDepth	51.1	55.5	56.7
GradAug	53.7	57.1	57.9
methods improve substantially from 10 epochs per round to 20. However, from 20 to 30, the trends
vary substantially. As a baseline, FedAvg slightly improves by +0.3%. Surprisingly, FedProx and
MOON actually experience a slight reduction in accuracy from 20 to 30 epochs (-0.2% and -0.1%,
respectively). Meanwhile, the standard (particularly structural) regularization methods continue to
increase in accuracy. Therefore, these methods illustrate the ability to maintain productive training,
even across a wide range of allotted local epochs.
6
Under review as a conference paper at ICLR 2022
Number of Clients. In real-world FL settings, the number of participating clients can vary widely.
Moreover, only a portion of clients are potentially sampled per round, whether for connectivity
reason or other capacity restrictions of the central system. Therefore, it is crucial that an FL method
can converge under such conditions. We study the affect of client number and client sampling in
Table 4. C = 64 × 0.25 indicates that there are 64 total clients in the system, but only a fraction
(0.25) are sampled each round. The rest of the presented results in Table 4 sample all K clients each
round. C = 64 × 0.25 (100) is run for 100 rounds, and all other settings for the default 25 rounds.
Table 4: Ablation results for varying number of clients K in synchronous and client sampling cases.
Method	C=16	C = 32	C = 64	C=	64 × 0.25	C=	64 × 0.25 (100)
FedAvg	52.9	44.7	34.4		33.2		46.8
FedProx	53.0	45.1	34.8		32.1		46.0
MooN	55.3	45.5	35.2		34.3		49.8
Mixup	54.0	44.8	35.6		34.1		48.7
StochDepth	55.5	47.7	36.1		34.5		51.4
GradAug	57.1	50.5	40.3		38.4		52.5
FedProx and MOON have similar trends with increasing clients, having a slight edge on FedAvg
in all synchronous cases. Interestingly, in the client sampling case (C = 64 × 0.25), FedAvg ac-
tually performs better than FedProx. This is particularly insightful in terms of learning efficiency.
When a small percentage of clients are sampled, only a portion of the dataset is effectively trained
on each round. Therefore, learning efficiency becomes paramount for maintaining suitable conver-
gence. However, methods that focus on staying close to the server representation may suffer from
insufficient local learning and subsequently slower convergence.
The standard regularization methods in the lower portion of Table 4 have similar trends to those
of the upper portion. However, these methods maintain better accuracy than FedAvg in all set-
tings, even in the client sampling case. Starting from a similar accuracy as MOON at 16 clients,
StochDepth maintains higher accuracy than MOON as the number of clients increases. Overall,
GradAug performs the best in all cases. Therefore, even though these regularization methods were
not designed for the FL setting and partial client sampling, they still perform on par with or improve
over current state-of-the-art FL algorithms.
4 Proposed Method - FEDALIGN
Overall, we find that GradAug is particularly effective in
the FL setting, having the highest accuracy in all tested
scenarios along with the lowest λmax, HT , and HN.
However, while this method is quite memory efficient in
comparison to many FL methods (only requires a sin-
gle stored model during training), it does incur addi-
tional training time over the FedAvg baseline. This is be-
cause GradAug requires multiple forward passes through
slimmed sub-networks for the distillation loss. It is possi-
ble to reduce the computation burden by using a smaller
number of sub-networks during the knowledge distilla-
tion process. As seen in Table 5, the wall-clock time of
GradAug can be reduced by 25% when using one sub-
network (n = 1) versus two (n = 2). This change incurs
only a 0.3% accuracy drop; nonetheless, a noticeable gap
Table 5: Analysis of wall-clock time per
round on CIFAR-100 with C = 16 and
E = 20 across four RTX-2080Ti GPUs.
Method		Accuracy (%) Time (s)	
FedAvg		52.9	142
FedProx		53.0	161
MooN		55.3	418
Mixup		54.0	142
StochDepth		55.5	139
GradAug (n =	1)	56.8	230
GradAug (n =	2)	57.1	325
GradAug (n =	3)	57.1	420
GradAug (n =	4)	56.6	513
FedAlign		56.6	167
still remains between GradAug and vanilla FedAvg in wall-clock time. Therefore, the question is,
can we devise a method which provides similar effect and performance as GradAug in FL, but
with substantially less computational overhead? This is particularly important in the FL setting,
where clients are typically deployed devices with minimal memory and computational resources.
To do so, we first take note of the following insights gathered during our analysis: 1) Second-
order information is insightful for understanding the learning generality of neural networks. Par-
ticularly, we find that flatness and consistency in this realm are desirable traits. 2) In practice,
we find that structural regularization, and especially distillation-based like GradAug, is quite ef-
7
Under review as a conference paper at ICLR 2022
fective. Furthermore, the weight sharing mechanisms of such approaches are memory efficient
compared to other methods that rely on global model or previous model storage. Therefore, We
combine these insights into a novel algorithm to optimize for performance and resource needs in FL.
We propose FedAlign, an distillation-based regularization
method that aligns the Lipschitz constants (i.e. top Hes-
sian eigenvalues) of the most critical network components
through the use of slimmed sub-blocks. Fig. 2 shows an
overview of FedAlign, whose design is based on two key
principles. First, motivated by the insights of Section 3.4,
we internally regularize the Liptschitz constants of network
blocks to promote Smooth optimization and consistency
within the model. Recent work (Shang et al., 2021) presents
a quick approximation of the Lipschitz constants for neural
network layers in a differentiable manner. This enables the
use of second-order information in the distillation process,
traditionally between a fully trained teacher and a learning
student. We adapt this technique for distillation-based reg-
ularization with an untrained network in place of the tradi-
tional logit-based loss.
Second, in order to reduce computation in a purposeful
manner, we take note of certain network properties. Par-
ticularly, it has been shown that the final layers of a neural
network are most prone to overfit to the client distribution
(Luo et al., 2021). Therefore, FedAlign is designed with a
focus on these critical points in the network. We then ask
the question, ifwe aim to concentrate our regularization ef-
forts on the final layers, why should we run all networks for
distillation from start to finish like in GradAug? Instead, we
can reuse the intermediate features of the full network as in-
put to just the final block at a reduced width, and therefore
^^Weightsharing
2∩ ωs width
Block L
Block 1
Full network
F"".-
BlockL - 1
£CE
+
ry
，-酷 f° FflT (FKx)L-1)
1, ,4v-__________
Figure 2: The proposed FedAlign for
local client training in FL. Features
Fθ (x)l-i are run through Block L
as normal. The only additional infer-
ence in FedAlign is through Block L
at a reduced width (i.e. sub-block),
reusing features Fθ (x)L-1 as input.
The channels throughout the layers
in the sub-block are a ωS fraction of
the original number. This is accom-
plished via temporary uniform prun-
ing of Block L. Hyperparameter abla-
tions are presented in Appendix A.1.
significantly reduce computation. In this way, we harness the benefits of distillation-based regular-
ization in performance and memory footprint, while effectively mitigating computational overhead.
Combining these two key principles, we form the FedAlign local objective as
LFA = LCE (Fθ (X), y) + μLLip (KS, KF ) ,
(5)
where μ is a balancing constant, LCE is the cross-entropy loss, and LLip is the mean squared error
between the approximated Liptschitz constant vectors KS and KF for the reduced width (i.e. sub-
block) and full width block L, respectively. Specifically, the Lipschitz approximations are calculated
via the spectral norm of a transmitting matrix using feature maps to bypass the need for singular
value decomposition as in Shang et al. (2021). Therefore, in our framework, we use the intermediate
features for these transmitting matrices XF and XS, where
XF = (Fθ (x)L-1)> Fθ (x)L, and	(6)
XS = (Fθ(x)L-1)> FθωS (Fθ(x)L-1) .	(7)
FθωS is the output feature map of the final block L at reduced width ωS; Fθ(x)L and Fθ (x)L-1
are the feature maps outputted by the last and prior to last blocks of the full network (see Fig. 2).
Finally, the spectral norm (SN) ofXF and XS are approximated using the Power Iteration method
(Yao et al., 2018), and therefore KF = kXF kSN and KS = kXSkSN. Looking back to Eq. 5,
one could view LLip as a correction term; however, there is a key distinction between this form of
regularization and that of traditional FL algorithms. Our correction term promotes the local client
models to learn well-generalized representations based on their own data, instead of forcing the
local models to be close to the global model.
As seen in Table 5, FedAlign achieves state-of-the-art accuracy in a resource-efficient manner.
Specifically, FedAlign improves +3.7% over FedAvg while only incurring a small increase in wall-
clock time. In comparison, FedProx and MOON have a much lower accuracy, while incurring
similar or substantially more compute time. Also, both FedPrOx and MOON require the StOrage
8
Under review as a conference paper at ICLR 2022
of at least one additional model in memory, While FedAlign does not. Furthermore, FedAlign at-
tains similar accuracy to GradAUg while realizing a ~48% and ~37% reduction in wall-clock time
compared to the n = 2 and n = 1 variants.
4.1	FedAlign Experiments
We further verify the effectiveness of our method across various settings and datasets. In Table 6,
we examine the performance of FedAlign with the same ablations as in Section 3.5. FedAlign main-
tains strong performance across all settings. It follows closely to GradAug in most cases, despite
requiring substantially less computation. We further investigate the effectiveness of FedAlign and
all other methods across two additional datasets: CIFAR-10 and ImageNet-200. For ImageNet-200,
we randomly sample 200 classes from the classic ImageNet-1k (Russakovsky et al., 2015) dataset.
We employ ResNet56 and ResNet18 as our models on CIFAR-10 and ImageNet-200, respectively.
Hyperparameters for all methods are kept the same as those described in Section 3.2 and Table 6.
For CIFAR-10, we ran a 16 client synchronous and 64 client case with sampling. We note similar
trends to CIFAR-100; regularization methods perform well, particularly in the more realistic client
sampling case. FedAlign continues to provide similarly strong accuracy as GradAug (n = 1) in
both settings. On ImageNet-200, we ran two settings as well. Based on the results in Table 7, both
GradAug and FedAlign maintain higher performance than other methods, especially in the client
sampling case. Interestingly, Stochastic Depth does not perform particularly well in the ImageNet-
200 cases. As mentioned in the original paper (Huang et al., 2016), Stocahstic Depth performs
better with deeper networks. However, with ResNet18, the overall depth of the network is reduced
compared to that in the CIFAR cases. Therefore, as most deployable networks favor width over
depth, regularizing with respect to the width of a network is more applicable to the FL setting. This
highlights an additional benefit of FedAlign, which operates using width reduction in the final block
and maintains relatively high accuracy despite low resource needs.
Discussion. Overall, FedAlign offers stable improvements over classic baselines in a vast array
of settings with little resource overhead. While some methods perform well in specific situations,
consistent performance is especially desirable in FL, as deployed settings can vary widely. Addi-
tionally, we note that GradAug is primarily designed for vision data, requiring that a transformation
(e.g. scaling) be applied to the input of sub-networks. However, FedAlign is not specific to vision in
any way, and therefore could be applied to other data formats. We leave this study for future work.
Table 6: FedAlign ablation results on CIFAR-100. Hyperparamters μ = 0.05 and ωs = 0.8.
MethOd I α = 0.1	α=2	homog E = 10	E = 30 I	C=32	C=64	C	64 × 0.25	C	64 × 0.25 (100)
FedAlign ∣	47.6	58.1	58.6 I 54.5	57.2 I	49.6	39.9		36.7		51.6
Table 7: CIFAR-10 and ImageNet-200 results for all methods.
Method	CIFAR-10 C=16	C	CIFAR-10 二 64 X 0.25 (100)	ImageNet-200 C =16	ImageNet-200 C = 32 X 0.125 (25)
FedAvg	82.1		77.8	60.7	46.9
FedProx	81.8		76.9	61.3	46.7
MOON	83.4		78.7	61.0	47.0
Mixup	80.5		80.3	610	46.7
StochDepth	82.0		79.6	60.4	45.4
GradAug (n = 2) GradAug (n = 1)	84.3		83.1	63.8	48.5
	84.2		81.1	62.5	48.1
FedAlign	83.8		81.2	615	47.5
5	Conclusion
In this work, we study the data heterogeneity challenge of FL from a simple yet unique perspec-
tive of local learning generality. Specifically, we intuitively reason that regularization methods can
effectively alleviate client drift, and confirm these ideas with empirical analysis. Based on our find-
ings, we propose FedAlign, a distillation-based regularization method that promotes local learning
generality rather than proximal restriction, and achieves comparable accuracy with SOTA methods
while maintaining excellent resource efficiency.
9
Under review as a conference paper at ICLR 2022
6	Reproducibility Statement
In Section 3.2, we thoroughly describe our settings to ensure that the results of our empirical study
can be easily replicated. Additional implementation details for FedAlign are discussed in Appendix
A.2. We also plan to make our source code (implemented in PyTorch (Paszke et al., 2019)) publicly
available upon acceptance to make replication simple and accessible. As mentioned in Section 4.1,
we randomly sample 200 classes from ImageNet-1K (Russakovsky et al., 2015) to form ImageNet-
200. The script for generating this dataset and the exact classes sampled will also be included in the
code release.
References
Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and
Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Con-
ference on Learning Representations, 2021. URL https://openreview.net/forum?
id=B7v4QMR6Z9w. 1, 3, 4
Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbation-
based regularization. In International Conference on Machine Learning, pp. 1554-1565. PMLR,
2020. 5
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolu-
tional networks. Advances in Neural Information Processing Systems, 31:10727-10737, 2018.
3
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth
Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh
Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research library and
benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020. 4
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016. 4
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In European conference on computer vision, pp. 646-661. Springer, 2016. 3, 4,
9
M.F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing
splines. Communication in Statistics- Simulation and Computation, 18:1059-1076, 01 1989. doi:
10.1080/03610919008812866. 5
Yiding Jiang*, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=SJgIPJBFvH. 5
Peter Kairouz, H. Brendan McMahan, Brendan Avent, AUrelien BelleL and et al. Advances and
open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1-2):
1-210, 2021. ISSN 1935-8237. doi: 10.1561/2200000083. URL http://dx.doi.org/10.
1561/2200000083. 1
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
International Conference on Machine Learning, pp. 5132-5143. PMLR, 2020. 1, 3
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. 2016.
5
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced
research). URL http://www.cs.toronto.edu/~kriz/cifar.html. 4
10
Under review as a conference paper at ICLR 2022
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos:
An experimental study. arXiv preprint arXiv:2102.02079, 2021a. 1
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10713-10722,
2021b. 1, 3,4
Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity:
Classifier calibration for federated learning with non-iid data. arXiv e-prints, pp. arXiv-2106,
2021. 8
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273-1282. PMLR, 2017. 2
Seungeun Oh, Jihong Park, Eunjeong Jeong, Hyesung Kim, Mehdi Bennis, and Seong-Lyun Kim.
Mix2fld: Downlink federated learning after uplink federated distillation with two-way mixup.
IEEE Communications Letters, 24(10):2211-2215, 2020. doi: 10.1109/LCOMM.2020.3003693.
3
Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard
Scholkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.
2, 5
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf. 10
Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for
out-of-distribution generalization. arXiv preprint arXiv:2109.02934, 2021. 2, 5
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y. 9, 10
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.
On the convergence of federated optimization in heterogeneous networks. CoRR, abs/1812.06127,
2018. URL http://arxiv.org/abs/1812.06127. 1, 3
Yuzhang Shang, Bin Duan, Ziliang Zong, Liqiang Nie, and Yan Yan. Lipschitz continuity guided
knowledge distillation, 2021. 8, 12
MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun
Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. arXiv
preprint arXiv:2006.05148, 2020. 3
Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efficient object
localization using convolutional networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 648-656, 2015. 3
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen-
tations, 2019. 3
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. Advances in neural information
processing systems, 2020. 3
11
Under review as a conference paper at ICLR 2022
Taojiannan Yang, Sijie Zhu, and Chen Chen. Gradaug: A new regularization method for deep neural
networks. Advances in Neural Information Processing Systems, 33, 2020. 3, 4
Zhewei Yao, Amir Gholami, Qi Lei, Kurt Keutzer, and Michael W Mahoney. Hessian-based analysis
of large batch training and robustness to adversaries. Advances in Neural Information Processing
Systems, 31:4949-4959, 2018. 5, 8
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581-590. IEEE, 2020. 5
Tehrim Yoon, Sumin Shin, Sung Ju Hwang, and Eunho Yang. Fedmix: Approximation of mixup
under mean augmented federated learning. In International Conference on Learning Representa-
tions, 2021. URL https://openreview.net/forum?id=Ogga20D2HO-. 3
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019. 3
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna-
tional Conference on Machine Learning, pp. 7252-7261. PMLR, 2019. 3
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hut-
ter. Understanding and robustifying differentiable architecture search. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
H1gDNyrKDS. 5
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018. 3, 4
A Appendix
A. 1 Hyperparameter Ablations of FedAlign
The default hyperparamter setting used throughout the paper is μ = 0.05 and ωs = 0.8. The
performance of FedAlign with various hyperparamters on CIFAR-100 is shown in Table 8. We vary
μ and ωs independently, meaning ωs = 0.8 for the μ ablations, and μ = 0.05 when varying ωs.
Table 8 shows that FedAlign is more sensitive to width, particularly below 0.8. Performance is
relatively consistent across across a wide range of μ values.
Table 8: FedAlign hyperparameter ablations on CIFAR-100.
MethOd μ = 0005 μ = 0.05 μ = 0.1 μ =。5 ∣ ωs =。6 ωs =。7 ωs =。8 ωs =。9
FedAlign 56.4	56.6	56.3	55.6	∣	55.7	55.1	56.6	56.2
A.2 Additional Implementation Details
When calculating XF and XS, the input and output features involved will typically be of different
spatial sizes in practice. Therefore, Shang et al. (2021) utilizes an adaptive average pool operation
in PyTorch to reduce the spatial size of the larger feature map to that of the smaller one. We likewise
employ this operation.
Prior to performing backpropagation, We apply a relative scale to LLip along with the μ scaling pa-
rameter. In PyTorch-style pseudocode: IossJip = μ* (loss_ce.item()/loss_lip.item())* lossdip.
This is to ensure that LLip is on relatively the same scale with LCE, as LLip is generally much
larger on its own.
12