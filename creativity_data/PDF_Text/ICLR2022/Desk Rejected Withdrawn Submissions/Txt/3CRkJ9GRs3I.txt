Under review as a conference paper at ICLR 2022
Understanding ResNet from
a Discrete Dynamical System Perspective
Anonymous authors
Paper under double-blind review
Ab stract
Residual network (ResNet) is one of popular networks proposed in recently years.
Discussion about its theoretical properties is helpful for the understanding of net-
works with convolution modules. In this paper, we formulate the learning process
of ResNet as a iterative system, then we may apply tools in discrete dynamical
systems to explain its stability and accuracy. Due to the backward propagation of
learning process, the module operations vary with the change of different layers.
So we introduce the condition number of modules to describe the perturbation of
output data, which can demonstrate the robustness of ResNet. In addition, the
inter-class and intra-class median principal angle is defined to analyze the clas-
sification efficiency of ResNet. Mathematical description of the learning process
of ResNet is given in a modular manner so that our research framework can be
applied to other networks. In order to verify the feasibility of our idea, several
experiments are carried out on the Dogs vs. Cats dataset, Kaggle: Animals-10
dataset, and ImageNet 2012 dataset. Simulation results are accordance with the
theoretical analysis and prove the validity of our theory.
1	Introduction
Deep learning algorithms have become one of major trends in extracting image features for various
vision tasks (Wu et al. (2021); Yun et al. (2020); Jiao & Zhao (2019)). Theoretical study of those
algorithms can effectively explain the intrinsic principles of different applications (Wiatowski &
Bolcskei (2017); Grohs et al. (2016); Chan et al. (2021); Higham & Higham (2019)), and provide
guidance for the construction of suitable networks (Zhang et al. (2021); Liu et al. (2021); Li et al.
(2021)).
Classification algorithms based on deep learning have great advantages of other traditional approach-
es in computer vision. People try to apply different theories (Bai et al. (2020); Su et al. (2018);
Mengzi Tang (2021))to explore why those algorithms succeed. Qiu & Sapiro (2015) defined dif-
ferent classes as subspaces, and introduced a low-rank transformation of class subspaces to achieve
target classification. Giryes et al. (2016) described the versatility of deep neural network (DNN)
with random Gaussian weights in classification (Gulcu (2020)), which indicates that DNN is a gen-
eral classifier based on the principal angle between two classes. Sokolic et al. (2017) proved that
generalization error of neural network is decided by correlation among different weights. Bejani &
Ghatee (2020) used the condition number of the weight matrix to evaluate the degree of overfitting.
The larger the condition number, the more serious the overfitting. Xiao et al. (2019) utilized the
condition number of the kernel matrix of an infinitely wide neural network as a measure of train-
ability. If the series of condition numbers diverges, the network will not be trained as the number
of channels increases. Whether it is applied to evaluate the degree of overfitting or trainability, the
larger condition number means that the learning network is ill-conditioned (Higham (2002)). In
other words, this network is not suitable for the current task.
Above researches usually deal with general deep learning frameworks, which are not applicable
directly for the real networks used in practice. To solve the problem, one starts theoretical analyses
of specific networks including ResNet (He et al. (2016a); He et al. (2016b); Goceri (2019); Jagatap
& Hegde (2019); Allen-Zhu & Li (2019)). Zhang & Schaeffer (2020) descripted the learning process
of parameters as an optimization control problem. It reveals that the perturbation of the input data is
influenced by the boundary of the learning parameter. Rousseau et al. (2020) classified the input data
1
Under review as a conference paper at ICLR 2022
and classification prediction of ResNet as two spaces respectively. Furthermore, a residual structure
is described as a differential homeomorphism mapping from the input space to the prediction space.
Ruthotto & Haber (2020) interpreted ResNet as a discrete space-time differential equation from
the perspective of difference equations. He used this connection to analyze the stability of a new
network similar to a difference system.
However, there are few discussions about the quantitative properties such as accuracy and stability
of the deep learning networks. In fact, we may describe the learning process of ResNet as a dis-
crete dynamical system. Since the iteration matrices constantly change in the learning process, we
introduce the network condition number and the median principal angle to quantitatively investigate
stability of ResNet parameter learning and accuracy of classification, respectively. The main contri-
butions of this paper are summarized as follows: First, we apply ideas in discrete dynamical systems
to numerically study the stability and accuracy of ResNet, and experiments show that the condition
number and median principal angle concepts proposed in the paper efficiently explained the supe-
riority of the ResNet algorithm. Second, the research framework based on the iteration equations
can be actually generalized to any deep learning networks containing layers, which provides helpful
guidance for the evaluation and construction of novel learning networks.
The remainder of our work is divided into the following parts. We introduce the basic network
module and the ResNet module in section 2. Section 3 mainly gives the definition of condition
number and makes the specific theoretical analysis that is based on ResNet. Section 4 denotes the
median principal angle and shows experimental results of accuracy. We end with overall summary
of the paper in section 5.
2	Related work
ResNet is the most popular deep convolution neural network (CNN) used in many application fields,
such as (He et al. (2016a); Li & Rai (2020); Zhang (2021); Demir et al. (2019)). Its basic block
consists of two convolutional layers and a shortcut connection (an identity connection). As the
number of modules increases, the classification accuracy is significantly improved, and network
robustness and generalization performance have also been improved. Subsequent improvements
based on ResNet also result in efficient algorithms with high classification accuracy (Gao et al.
(2019); Zagoruyko & Komodakis (2016); Radosavovic et al. (2020); Liang et al. (2018)). In order
to theoretically explain advantages of ResNet, we successively introduce the basic network, original
ResNet, and ResNet with BN.
Since the convolution operation in ResNet is linear, we may use matrix multiplication to represent it
as follows. Assume that input vector x ∈ Rn2,the weight matrix W ∈ Rn2×n2, and the characteristic
output obtained after the convolution operation is y ∈ Rn2 . Then the convolution process can be
written as y = WTx.
Basic block: As shown in Figure 1(a), the basic network we introduced here is the backbone of
the convolutional neural network, that is, the input data is convolved to extract features. In order to
compare it with ResNet, we constraint residual block with the same parameters in the basic network.
In other words, the difference between basic block and ResNet block is only a shortcut connection.
xl is the input feature of the l-th module. f : xl → xl+1 is the mapping of the input vector xl to
the output vector xl+1 in the basic network. the output of this layer is xl+1 = f(xl,{Wl(i)}) =
Wl(2)σ(BN(Wl(1)σ(BN(xl)))). where Wl(i) is the i-th weight in the l-th module, i = 1, 2. σ is
the activation function (ReLU).
Basic ResNet block: The ResNet module is composed of the basic block and the identity mapping.
Assume that input vector of ResNet in the l-th module is xl, as shown in Figure 1(b), and the output
vector of l-th module is xl+1 (i.e., the input of the next module). Then the output vector of the
ResNet module is xl+1 = σ[f(xl, {Wl(i)})] + xl, where f(xl, {Wl(i)} = (Wl(2))T σ((Wl(1))T xl).
Basic ResNet block added BN : As shown in Figure 1(c), the l-th module with BN in ResNet
can be described as: The input feature of the l-th module is xl , and the output of this layer is
xl+1 =f(xl,{Wl(i)})+xl.Wheref(xl,{Wl(i)}) = (Wl(2))Tσ(BN((Wl(1))Tσ(BN(xl)))), BN
is the batch normalization operation(Ioffe & Szegedy (2015)).
2
Under review as a conference paper at ICLR 2022
(a) a basic block
(b) a basic ResNet block
(c) a ResNet block with BN
Figure 1: Basic network mudule and ResNet modules. (a) is the backbone module of general convo-
lutional neural networks, i.e., there are only convolution processing, batch normalization and nonlin-
ear activation function. (b) denotes the ordinary ResNet module, which introduce ReLU activation
function between two convolution kernels of the same size in a single module. (c) is that the input
vector first perform batch normalization (BN), then pass activation fuction (ReLU) and convolution.
3	Stability analysis of ResNet
3.1	The condition number
Lemma 1 Let Wl(1), Wl(2) ∈ Rn2×n2 be the two weight matrices of l-th module of ResNet. I ∈
Rn2×n2 is the unit matrix, and xl ∈ Rn2 is the input vector of l-th module. Then the corresponding
weight matrix of whole module is Wl(1) Wl(2) + I, which satisfies the following inequality:
(ml(1)ml(2)+1)kxlk2≤	(Wl(1)Wl(2)+I)Txl	≤ (Ml(1)Ml(2) + 1)kxlk2	(1)
2
where Ml(1), ml(1) are the maximum singular value and the minimum singular value of Wl(1), respec-
tively, and Ml(2) , ml(2) are the maximum singular value and the minimum singular value of Wl(1),
respectively.
The proof procedure of Lemma (1) is presented in Appendix A.
Remark1: According to inequality 1, we may define condition number of Wl(1)Wl(2) + I is:
κl,R(Wl(1)Wl(2) + I) =
MI(I)M(2) + 1
m(1)m(2) + 1
(2)
Remark2: Here weight matrix is always invertible. Assuming that the matrix is not full, the min-
imum singular value of the matrix is zero, and the condition number tends to infinity. Hence, the
network diverges, which is contradictory to the actual classification network. Therefore, the matrix
is full rank, and the singular values are non-negative.
3.2	Stability analysis of the ResNet block
Under the assumptions of equation(2), the condition number of l-th module of the base Network is
(1)	(2)
ki,b (Wl	WI	) =	m(i)ml2)	. Apparently, we have ki,r(W∣	Wv[	+ +	I)	- ” (W))W()) < 0.
3
Under review as a conference paper at ICLR 2022
That is to say, the ResNet network has a smaller condition numbers than the normal network in the
condition of the same input features and the same convolutional kernel. Therefore, ResNet has faster
convergent learning process in the forward propagation than Networks only based on convolution.
Next we try to discuss the evolutional property of ResNet. Take the l-th module as example. Let
(Wl + I)T (xl + ∆xl) = al + ∆al, where ∆xl is the absolute error of the input vector of the l-th
module, al denotes the output vector of the l-th module operation, ∆al is the absolute error of the
output vectoe after training. Then we present perturbation estimate of the output data of l-th module
in the following Lemma.
Lemma 2 The perturbation of output data after the l-th module operation satisfies the following
inequality:
1
Kl,R(Wl + I)
k∆xιk	, k∆aιk	k∆xιk
E≤E≤ κl,R(Wl + I) .E
The proof procedure of Lemma 2 is presented in Appendix B.
Similarly, following the same idea provided in Lemma 2, we can get the perturbation bounds about
basic block. Let WlT (xl + ∆xl) = bl + ∆bl. xl, ∆xl are the same as the case in ResNet. bl, ∆bl are
the output vector and perturbation of the output, respectively. Then, the relative error of the input
data is limited to:
1
Kl,B (Wl)
k∆xik
Ixr
k∆blk
IbT
≤ κι,B(Wl) ∙ ⅛X⅛i
≤
Since κl,R(Wl(1)Wl(2) +I) -κl,B(Wl(1)Wl(2)) < 0, we can easily deduce that
1	k∆xlk /	1	k∆xl k	(3)
	:		 ∙ -T	T- ≤ 	:		 • κl,B (Wl)	IlxlIl — κl,B (Wl+I)	Ixlk .	
κl,B(Wl) ∙k∆* ≥ κl,R(Wl + I) .	k∆xlk ■	. kxlk	(4)
From inequality (3), (4), we can find that the upper limit of relative error of the output data will be
smaller and the lower limit will be larger in ResNet.
ResNet with BN has been shown to have advantage over base network in divergence. The following
Lemma will prove that ResNet with BN indeed has smaller perturbation range than base network.
i.e., the learning process of ResNet with BN is more stable with respect to the same input perturba-
tion.
Let xl+1 is the actual output of l-th module, yl+1 is the actual output vector of the l-th module
with BN, respectively. The loss function are E = ɪ(tl+ι - χl+ι)2, EBN = 11 (tl+ι - yl+ι)2,
Respectively. σ denotes the ReLU activation function, and tl+1 denotes the expected output vector
of l-th module.
Lemma 3 The output vectors of the ResNet module with BN and without BN can be written as
xl+1 = (Wl)T σ(xl)+xl, yl+1 = (Wl)Tσ(BN(xl))+xl, respectively. We have the following equal-
ity
k∆BN(xl)k ≤ k∆xlk
kBN(xl)k ≤ ^FΓ'
The proof procedure of Lemma 3 is presented in Appendix C.
In fact, due to the iteration idea of the modules used in deep learning networks, the procedure analy-
sis approach can be applicable for any other networks such as (Dosovitskiy et al. (2021); Cordonnier
et al. (2020)).
4
Under review as a conference paper at ICLR 2022
4	Accuracy analysis ResNet
4.1	The median principal angle
Qiu & Sapiro (2015) applied the smallest principal angle to estimate inter-class classification result.
Taking into account the influence of external noise, we introduce the median principal angle as the
representation of classification. The larger the inter-class angle, the greater accuracy the classifi-
cation. At the same time, we also introduce the intra-class median principal angle to illustrate the
clustering effect. The smaller the intra-class median principal angle, the better effect the clustering.
Definition 1 (Median principal angle) For two different matrices U, V ∈ Rn2 ×m, for any column
vectors ui, uj of U, vi, vj of V, then the median principal angle between U and V is defined as
θU,V
M ed arccos
ui ∈U,vi ∈V
Uivi
kUik2kvik2.
The median principal angle of U is denoted by
θU=UMedUarccos Eu; Uujk2，i=j.
Among them, U and V are representing two different classes, the elements of those vectors represent
different pixel values pictures. m is the number of examples of each class. Med (median) means to
take the median of all principal angle.
For the median principal angle, we can also analyze the trend of the median principal angle during
the learning process using the study in Section 3. However, in this paper we only demonstrate this
rule by experiment, the theory interpretation will be represented in next work. We choose three
different public datasets, Dogs vs. cats dataset, the kaggle: Animals 10 dataset, and the Imagenet
2012 dataset. For different requirements, we fix the epoch to be 50 and the batchsize to be 64 for
each epoch when training the network, i.e., 3200 iterations for each network. We use intra- i to
denote the median principal angle within the i class, and inter- i, j is denoted the median principal
angle between the i and j classes.
4.2	Accuracy comparison between base network and ResNet
Focusing on accuracy comparison of different Networks, we make three contrast experiments on
Dogs vs. Cats data dataset. The experimental results are shown in Figure 2.
From the results in Figure 2, we can verify that our theoretical research is consistent with practical
application. It can be seen from Figure 2(a) that the basic network 18 cannot accurately make
clustering and classification. The classification accuracy rate is basically maintained at around 55%
in experimental results. Comparing Figure 2(b) with Figure 2(c), it can be found that when BN
is removed from the ResNet, the inter-class stability and intra-class stability decrease. Especially
the perturbation in the interval of 5-20 epoch, the intra-class angle fluctuates greatly, which cause
the training process instable. When the training is completed, the intra-class angle of the ResNet18
network without BN is significantly larger than the intra-class angle of the ResNet18 network with
BN, i.e., the former clustering effect is not dominant. In addition, in the later stage of the iteration,
the distance between the ResNet network classes without BN shows a decreasing trend, which shows
that the classification accuracy rate will be relatively low. Therefore, the comparison between the
two groups can show that the ResNet has relatively good results in accuracy and robustness.
Remark: In order to control the influence of different parameters on the network, the base network
18 in this article only represents the ordinary CNN network, which is different from the VGG (Si-
monyan & Zisserman (2014)) network structure, so it cannot be treated as VGG network.
In order to compare the stability of different convolution layers of ResNet, we selected ResNet34,
ResNet50, and ResNet101 for training in Dogs vs. Cats dataset. The experimental results are shown
in Figure 3. Employing the same dataset to train different types of ResNet (different convolution
layers), it is not necessarily that the more convolution layers, the better the stability and accuracy.
For our small classification task, ResNet18 and ResNet34 have better classification stability and
clustering effect.
5
Under review as a conference paper at ICLR 2022
(a) base18 iteration
(b) Res18 iteration
(c) Res18+BN iteration
Figure 2: Comparison of the accuracy in different networks. (a) shows the comparison of the base
network with 18 convolution layers, where the horizontal scale is the number of epochs, and the
ordinate is the median principal angle (MPA). In-1 represents the median principal angle of the first
class (Cats), in-2 represents the median principal angle of another class (Dogs), and out represents
the median principal angle between cats and dogs; (b) represents the median principal angle when
there is no BN in the ResNet18; (c) represents the median principal angle of ResNet18 that BN is
added in this network.
6
Under review as a conference paper at ICLR 2022
(a) Res34 iterative process
(b) Res50 iterative process
(c) Res101 iterative process
Figure 3: Comparison of the training iteration process of ResNet 34, ResNet 50, ResNet 101. (a)
represents the training iteration process of ResNet34, intra-1, intra-2, and inter-1,2 are the same as
those shown in Figure 2; (b) represents the training iteration process of ResNet50; (c) represents the
training iteration process of ResNet101.
(a) 2-class in Animal 10 dataset
(b) 3-class in ImageNet-2012 dataset
Figure 4: Comparison of the accuracy of different datasets on ResNet34. (a) represents the two-
category iterative process of ResNet34 on the Animal 10 data set. (b) represents the three-category
iterative process of ResNet34 on the Imagenet-2012 dataset, in-1, in-2, and in-3 represent the me-
dian principal in the first category, the second category, and the third category, out-1, out-2, out-
3respectively represent the median principal angle between the first and second classes, the median
principal angle between the first and third classes, and the second and third classes.
7
Under review as a conference paper at ICLR 2022
4.3	Comparison of classification accuracy of different datasets
We illustrate the generalization of ResNet 34 in different datasets. We arbitrarily select two classes
on the Animal 10 dataset for binary classification (except cats and dogs). At the same time, we
do three-classification experiments in ImageNet-2012, where three different classes are randomly
selected for training. The experimental results are shown in Figure 4.
(c) down-sampling in Res101
Figure 5: The trend of the median principal angle of the three different types of ResNet with different
depths. (a) represents the changing trend of the median principal angle within and between classes
in ResNet34 as the number of layers increases. (b) represents the changing trend of the median
principal angle within and between classes in ResNet50 as the number of layers increases. (c)
represents the changing trend of the median principal angle within and between classes in ResNet101
as the number of layers increases.
Combining Figure 3(a) and Figure 4, the classification accuracy of the same network is different in
three different datasets. The training iterative process gradually stabilizes, and the same class and
different classes can be clearly separated. Figure 3(a) and Figure 4(a) can show that, whether it is
classification or clustering, the training iterative process is relatively stable, and the accuracy rate
is also high. Figure 4(b) shows that the initial discontinuity of the network in the training is not
obvious for the distinction within and between classes. However, the distinction within and between
classes has become more and more obvious after epoch10, and the network has gradually stabilized
afterwards.
Figure 4(a) and Figure 4(b) show that ResNet is suitable for different datasets, and the network has
good stability and high accuracy. In addition, it is also suitable for multi-classification tasks, and the
addition of classification tasks has little effect on its stability and accuracy.
8
Under review as a conference paper at ICLR 2022
4.4	Comparison of classification accuracy in different layers
4.1	and 4.2 describe the change process of the median principal angle between and within the class
in iterations. In this section, we compare the change trend of the principal angle between and within
the class for different layers. Different layers represent different numbers of channels. In ResNet34,
ResNet50, and ResNet101, the number of layers is all 5 , which means that there are five different
numbers of channels. In order to eliminate the factors that affect the accuracy of the number of
iterations, we train these three networks on the Dogs vs. Cats dataset, with an iteration epoch of
50 and a batchsize of 64. In the testing phase, before each down-sampling, we extract the middle
principal angle within and between classes. The experimental results are shown in Figure 5.
Figure 5 shows that as the number of layers increases, the intra-class median principal angle de-
creases more obviously, and the inter-class median principal angle increases slowly, but this does
not affect the result of classification. After the last epoch, the distinction between the inter-class
median principal angle and the intra-class median principal angle is especially obvious.
5	Conclusions
Inspired by the evolution idea of discrete dynamical systems, this paper has presented a set of evo-
lution tools to investigate ResNet framework. Condition numbers of ResNet blocks are defined and
analyzed to demonstrate the stability of ResNet. In addition, in order to further explain the accu-
racy of the network, we introduced median principal angles within and between classes to give an
experimental proof of the superiority of ResNet.
In the next step, we plan to apply other theories of discrete dynamical systems, such as periodicity,
bifurcation, etc. To analyze the evolution of the learning process based on different combinations
of different network modules. In particular, we try to set up a systematic framework to evaluate the
efficiency of various networks.
References
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? arXiv
preprint arXiv:1905.10337, 2019.
Chengzu Bai, Ren Zhang, Zeshui Xu, Baogang Jin, Jian Chen, Shuo Zhang, and Longxia Qian. K-
ernel low-rank entropic component analysis for hyperspectral image classification. IEEE Journal
ofSelected Topics in Applied Earth Observations and Remote Sensing,13:5682-5693, 2020.
Mohammad Mahdi Bejani and Mehdi Ghatee. Theory of adaptive svd regularization for deep neural
networks. Neural Networks, 128:33-46, 2020.
Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. Redunet: A
white-box deep network from the principle of maximizing rate reduction. arXiv preprint arX-
iv:2105.10446, 2021.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. 2020.
Ahmet Demir, Feyza Yilmaz, and Onur Kose. Early detection of skin cancer using deep learning
architectures: resnet-101 and inception-v3. pp. 1-4, 2019. doi: 10.1109/TIPTEKNO47231.2019.
8972045.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. 2021.
Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip HS
Torr. Res2net: A new multi-scale backbone architecture. IEEE transactions on pattern analysis
and machine intelligence, 2019.
9
Under review as a conference paper at ICLR 2022
Raja Giryes, Guillermo Sapiro, and Alex M. Bronstein. Deep neural networks with random gaussian
weights: A universal classification strategy? IEEE Transactions on Signal Processing, 64(13):
3444-3457, 2016.
Evgin Goceri. Analysis of deep networks with residual blocks and different activation functions:
classification of skin diseases. In 2019 Ninth international conference on image processing theory,
tools and applications (IPTA), pp. 1-6. IEEE, 2019.
Philipp Grohs, Thomas Wiatowski, and Helmut B?lcskei. Deep convolutional neural networks on
cartoon functions. pp. 1163-1167, 2016.
Talha Cihad Gulcu. Comments on deep neural networks with random gaussian weights: A universal
classification strategy?. IEEE Transactions on Signal Processing, 68:2401-2403, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Catherine F Higham and Desmond J Higham. Deep learning: An introduction for applied mathe-
maticians. Siam review, 61(4):860-891, 2019.
Nicholas J Higham. Accuracy and stability of numerical algorithms. SIAM, 2002.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. pp. 448-456, 2015.
Gauri Jagatap and Chinmay Hegde. Linearly convergent algorithms for learning shallow residual
networks. pp. 1797-1801, 2019. doi: 10.1109/ISIT.2019.8849246.
Licheng Jiao and Jin Zhao. A survey on the new generation of deep learning in image processing.
IEEE Access, 7:172231-172263, 2019.
Xin Li and Laxmisha Rai. Apple leaf disease identification and classification using resnet models.
pp. 738-742, 2020.
Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.
net/forum?id=uCY5MuAxcxU.
Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei Chen, Linbo Qiao, Li Zhou, and Jianfeng
Zhang. Learning for disparity estimation through feature constancy. pp. 2811-2820, 2018.
Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, and Zhaopeng Tu. Un-
derstanding and improving encoder layer fusion in sequence-to-sequence learning. In 9th Inter-
national Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021. OpenReview.net, 2021.
Bernard De Baets Mengzi Tang, Ral Prez-Fernndez. Distance metric learning for augmenting the
method of nearest neighbors for ordinal classification with absolute and relative information. In-
formation Fusion, 65:72-83, 2021. ISSN 1566-2535.
Qiang Qiu and Guillermo Sapiro. Learning transformations for clustering and classification. J.
Mach. Learn. Res., 16(1):187-225, 2015.
Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing
network design spaces. pp. 10428-10436, 2020.
Franois Rousseau, Lucas Drumetz, and Ronan Fablet. Residual networks as flows of diffeomor-
phisms. Journal of Mathematical Imaging and Vision, 62(1), 2020.
10
Under review as a conference paper at ICLR 2022
Lars Ruthotto and Eldad Haber. Deep neural networks motivated by partial differential equations.
Journal of Mathematical Imaging and Vision, 62(3):352-364, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Generalization error of
deep neural networks: Role of classification margin and data structure. In 2017 International
Conference on Sampling Theory and Applications (SampTA), pp. 147-151, 2017.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness
the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification
models. pp. 631-648, 2018.
Thomas WiatoWski and Helmut Bolcskei. A mathematical theory of deep convolutional neural
networks for feature extraction. IEEE Transactions on Information Theory, 64(3):1845-1866,
2017.
Di Wu, Chao Wang, Yong Wu, Qi-Cong Wang, and De-Shuang Huang. Attention deep model With
multi-scale deep supervision for person re-identification. IEEE Transactions on Emerging Topics
in Computational Intelligence, 5(1):70-78, 2021.
Lechao Xiao, Jeffrey Pennington, and Sam Schoenholz. Disentangling trainability and generaliza-
tion in deep learning. 2019.
Sangseok Yun, Jae-Mo Kang, Il-Min Kim, and Jeongseok Ha. Deep artificial noise: Deep learning-
based precoding optimization for artificial noise scheme. IEEE Transactions on Vehicular Tech-
nology, 69(3):3465-3469, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint arX-
iv:1605.07146, 2016.
Amy Zhang, Shagun Sodhani, Khimya Khetarpal, and Joelle Pineau. Learning robust state abstrac-
tions for hidden-parameter block mdps. 2021.
Linan Zhang and Hayden Schaeffer. ForWard stability of resnet and its variants. Journal of Mathe-
matical Imaging and Vision, 62(3):328-351, 2020.
Zhuoren Zhang. Resnet-based model for autonomous vehicles trajectory prediction. pp. 565-568,
2021.
A Appendix
The proof of lemma 1:
Let (Wl(1)Wl(2) + I) be the Weight matrix of the l-th module of ResNet, its condition number
is defined by ki,r(Wi(1)Wi(2) + I) = Ml(I)M(2)+：. According to the definition in the numerical
analysis, the condition number of this Weight matrix is also denoted by
κι,R(Wl⑴Wl⑵ +1) = Ml(I)Wl⑵ +11∣2 JkWl(I)Wl⑵ +1)T1.
In the l-th module of ResNet, We approximately define
M(I)Wl ⑵ + Il = Ml(I)Ml⑵ + 1』(Wl(I)Wl⑵ + I )-1 IL = m⅛τ.
Therefore, the folloWing inequality is satisfied by
(Wl(I)Wl⑵ + I)Txl 2 ≤ ||Wl(1)Wl(2)+iIL ∙kxlk2
= (Ml(1)Ml(2) + 1) kxlk2.
11
Under review as a conference paper at ICLR 2022
Another perspective to consider is that
kxlk2 = (Wl(1)Wl(2)+I)-1(Wl(1)Wl(2)+I)xl
≤ [(W,⑴Wl⑵ +1)-1∣∣2 JkWl⑴Wl⑵ +1)川2.
So the following inequality holds:
(m(1)m(2)+ I) kxl k2 = N~~⑴ kxlk2~~-lɪ
[[(Wl(1)Wl(2)+I)	[[2
≤ NNN(Wl(1)Wl(2)+I)xlNNN .
Therefore, (ml(1)ml(2) + 1)kxlk2 ≤ NN(Wl(1)Wl(2) + I) xlNN ≤ (Ml(1)Ml(2) + 1)kxlk2.
N	N2
lemma 1 is proved.
B	Appendix
The proof procedure of lemma 2:
Let input vector be xl ∈ Rn2 , the process of training ResNet is
(Wl+I)T(xl+∆xl) = (Wl+I)Txl+(Wl+I)T∆xl
= al + ∆al .
So following equations holds
(Wl+I)Txl =al
(Wl + I)T∆xl =∆al
∆xl = ((Wt + I)T)-1 ∙ (∆al)
Therefore,the norm of ∆xl satisfies:
k∆xlk ≤ N(Wl + I)-11 ∙k∆alk .
In other aspect,
kalk = N(Wl + I)T ∙ XlN
≤kWl + Ik∙kχlk,
so
Qlk <	N/W	I n TN IlW	I 川 Balk— /W	I	n Wlk
Ixr ≤	N(Wl+ I) 卜kWl+ Ik∙ E	= κl,R(Wl+	I)	.E.
In other words,
k∆al k	≥ N(Wl +1厂 1∣∣∙kWl + Ik -	k∆xlk _	_	1	k∆xlk
kalk		kxlk	κl,R(Wl + I)	kxlk
Also,
12
Under review as a conference paper at ICLR 2022
k∆aιk = II(Wl + I)T ∙(∆x”∣ ≤kWι + Ik∙k∆xιk,
the norm of ∆xl satisfies
“△xlk ≥ ∣∣W∆+lkk
so
∣δXlk ≥ Ijik ≥ _______________k∆alk_________	1	I* k
kxik ≥ kWl+Ik,kxik ≥ kWι+Ik∙k(Wι+I)-1aιk = ki,r (Wi+I)	kaik .
In other words,
In summary,
lemma 2 is proved.
C Appendix
k∆aik
≤ Kl,R(Wl + I) ∙端平
ι	kaxil ≤ ljik ≤ κ ( π(w, + T) 屿Xd
ki,r(Wi+i) ∙ ^PΓ ≤ ^PΓ ≤ κl,R(Wl + T) ∙ ^hΓ.
The proof process of Theorem 3 is as follows:
When no BN is added to l-th module, the module is represented as
xl+1 = (Wl )Tσ(xl ) + xl,
the loss function is
E = 2(tl + 1 - xl+1)2.
The gradient for Wl is
∂W = -(tl + 1 - xl +1) dxW1 = (tl + 1 - xl+lX-σ(Xl)).
The new weight matrix is
(Wl- η ∙ δWl) = Wl - η ∙ ∂W.
The learning process of network can be reinterpreted as
xl+1 + ∆xl+1 = ((Wl)T + (∆Wl)T)xl.
So
xl+1 = (Wl )Txl, ∆xl+1 = (∆Wl )Txl.
The perturbation range of xl is
k∆χlk
kxl Il
≤ κl,R(Wl + I) ∙
=κl,R(Wl + I) ∙
k∆Wl k
kWl k
κl,R(Wl + I) ∙
k—n(tl+i - χl+ι)Jσ(Xl)Il
kW
κl,R(Wl + I) ∙ ∣∣η(tl+ι - χl+ι)k
kxlk
W.
When BN is added to l-th module, the module is represented as
13
Under review as a conference paper at ICLR 2022
yl+1 = (Wl)T σ(BN (xl)) +xl,
its loss function is
EBN = 2 (tl + 1 - yi+1 )2.
One can calculate gradient for Wl, i.e.,
dEWN = -(tl+1 - yl + 1) d∂W1 = (tl+1 - yl+1)(-σ(BN(Xl))).
The new kernel matrix is
(Wl-η∙∆wl,BN) = Wl-η∙ ∂EWN.
The disturbance range of xl is
k∆BN(xl )k
kBN(xl)k
≤ κl,R(Wl + I) ∙
=κl,R(Wl + I) ∙
k∆Wl,BN k
kWlk
卜 η dEMI
kwl k
κl,R(Wl + I) ∙
k-η(tl+1 - yl+1)(-σ(BN(xl))k
kWl k
κl,R(Wl + I) ∙ kη(tl+ι - yl+ι)k
kBN(xl)k
kWlk
Suppose there exists M greater than zero, subject to
max{kη(tl+1 - xl+1)k , kη(tl+1 -yl+1)k} ≤ M.
Since kBN(xl)k ≤ kxl k, we can directly deduce that
Kl R(Wl + I) ∙ M ∙ kBN(Xl)k ≤ Kl R(Wl + I) ∙ M ∙ ⅛!.
l,R l	kWl k	l,R l	kWl k .
Therefore, the relative error of input data with BN added is less than the relative error of input vector
without BN, i.e.,
k∆BN(xι)k ≤ k∆xι k
kBN(xι)k ≤ ^FΓ.
Lemma3 is proofed.
14