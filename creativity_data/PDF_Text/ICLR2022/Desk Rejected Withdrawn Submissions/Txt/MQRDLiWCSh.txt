Under review as a conference paper at ICLR 2022
Towards Scaling Robustness Verification of
Semantic Features via Proof Velocity
Anonymous authors
Paper under double-blind review
Ab stract
Robustness analysis is important for understanding the reliability of neural net-
works. Despite the significant progress in the verification techniques for both Lp-
and semantic features- neighborhoods, existing approaches struggle to scale to
deep networks and large datasets. For example, we are unaware of any analyzer
that scales to AlexNet trained for ImageNet (consisting of 224x224x3 images). In
this work, we take a step towards scaling robustness analysis. We focus on ro-
bustness to perturbations of semantic features and introduce the concept of proof
guided by velocity to scale the analysis. The key idea is to phrase the verification
task as a dynamic system and adaptively identify how to split it into subproblems
each with maximal proof velocity. We propose a policy to determine the next
subproblem based on the past and by leveraging input splitting, input refinement,
and bound tightening. We evaluate our approach on CIFAR-10 and ImageNet
and show that it can analyze neighborhoods of various features: hue, saturation,
lightness, brightness, and PCA.
1	Introduction
The reliability of deep neural networks (DNNs) has been undermined by adversarial examples: small
perturbations to inputs that deceive the network (e.g., Goodfellow et al. (2015)). To understand the
robustness of a DNN to adversarial example attacks, most existing works propose to analyze the
network’s local robustness, i.e., for a given input and a radius verify that the network is robust
at the e-ball around that input (e.g., Muller et al. (2021); RyoU et al. (2021); XU et al. (2020);
Gowal et al. (2019); Singh et al. (2019); Gehr et al. (2018)). Despite the significant progress in
their efficiency and precision, verifiers of e-ball neighborhoods struggle to scale to deep networks.
That is, at best the maximal verified e-ball neighborhoods consist of indistinguishable images; at
worst, no non-trivial e-ball can be verified. Unfortunately, small certified neighborhoods provide
little insight on the robustness of the network. To illustrate, consider Figure 1 visualizing an e-
ball neighborhood B . Such neighborhoods contain all combinations of every possible perturbation
within the radius e. This limits the analysis to small neighborhoods confined by the closest decision
boundary, and prevents one from understanding other robustness aspects of the network.
In parallel, a new kind of adversarial attacks has been introduced, perturbing semantic features, such
as HSV transformations (Hosseini & Poovendran, 2018), PCA transformations (Zhang et al., 2020b;
Carlini & Wagner., 2017), and colorization and texture attacks (Bhattad et al., 2020). To understand
the robustness of networks to feature attacks, several verifiers have been proposed for analyzing fea-
ture neighborhoods (Mohapatra et al., 2020; Balunovic et al., 2019; Singh et al., 2019). Unlike e-ball
neighborhoods which consider all possible perturbations, feature neighborhoods impose constraints
on the perturbations, restricting them to the feature direction (illustrated by Figure 1). Hence, ro-
bust feature neighborhoods are larger than robust e-balls. To verify large feature neighborhoods,
several techniques have been proposed. One is encoding of the pixels’ dependencies imposed by
the features into the analysis to improve precision. Another technique is splitting the input range
into smaller parts that can be verified more efficiently. Despite the great effort in improving feature
encoding and input partitioning, existing work still struggles to scale to deep networks.
In this work, we present the concept of proof guided by velocity to scale robustness verification
for feature neighborhoods. The key idea is to phrase the verification problem as a dynamic system
driven by a policy that splits the problem into subproblems, each is verified separately, with the
1
Under review as a conference paper at ICLR 2022
Figure 1: Illustration of VeeP and the difference between an e-ball and feature neighborhoods.
goal of maximizing the verification velocity. The challenge is to compute the optimal split. On
the one hand, smaller subproblems are more likely to be verified quickly, and are thus more likely
to progress the overall analysis than larger subproblems whose verification is likely to be slow or
fail. On the other hand, if we split the problem into too many subproblems, the overall analysis will
not terminate in a reasonable time. Splitting into subproblems includes splitting the feature neigh-
borhood into smaller sub-neighborhoods, as well as defining other verification decisions, such as
neuron refinement, which affect the proof velocity. Our dynamic system makes progress iteratively.
At every step, one subproblem is created and verified (by an existing verifier). Based on the veri-
fication results, the policy determines the next subproblem. We design a policy which decides on
the next subproblem based on the previous subproblems. For example, if the verifier failed, the size
of the next sub-neighborhood is decreased. Our policy also decides which neurons may be refined,
based on their predicted effect on the certification result, the refinement gain, and the time overhead.
We implement this concept in a system called VeeP (for velocity prover). In addition to the ideas
presented above, VeeP also builds on the advancements of both e-ball and feature robustness verifica-
tion. In particular, it verifies the subproblems with GPUPoly (Muller et al., 2021), a state-of-the-art
verifier for e-ball neighborhoods, and it encodes the feature relations by building on the ideas of
Semantify-NN (Mohapatra et al., 2020). Combining these ideas with our concept of proof guided
by velocity enables VeeP to be the first verifier that can analyze an AlexNet model for ImageNet.
VeeP can reason about various feature neighborhoods: brightness, HSL (hue, saturation, lightness),
and PCA features. Figure 1 visualizes the meaning of these features. The sea shade and sun di-
rection features are obtained by PCA, which computes semantic features from the dataset. For the
turtle image in Figure 1, the diameter of the maximal e-ball which GPUPoly verifies is 0.0001, while
the diameter of the maximal brightness neighborhood which VeeP verifies is 0.18 (the diameter is
measured in the brightness space) — 1800x bigger than the maximal e.
We evaluate VeeP on ResNet models for CIFAR-10 and AlexNet models for ImageNet over bright-
ness, HSL and PCA neighborhoods. Results show that the verified diameters that VeeP computes for
our feature neighborhoods are 99.1x bigger than the verified diameters that GPUPoly computes for
e-ball neighborhoods. We further show that these verified diameters are on average at least 97% of
the maximal certifiable diameter (which is bounded by the closest adversarial example in the feature
direction). We also compare VeeP to Semantify-NN on MNIST and show that VeeP is 135x faster.
Finally, we run VeeP over PCA neighborhoods and show that analyzing robustness of networks to
dataset-specific features can reveal general robust and non-robust semantic attributes of the network.
To conclude, our main contributions are:
•	A proof guided by velocity framework for robustness verification of feature neighborhoods.
•	A policy deciding on the subproblems based on the analysis of previous subproblems.
•	A system called VeeP, implementing the proof guided by velocity framework and leverag-
ing recent advancements of both e-ball and feature robustness verifiers.
•	An evaluation on CIFAR-10 and ImageNet networks over brightness, HSL, and PCA.
2
Under review as a conference paper at ICLR 2022
2	Related work
In this section, we review the main works on neural network verification, and in particular robustness
verification. We refer the reader to a recent survey for a more detailed summary (Li et al., 2020).
Complete verifiers Several works propose complete robustness verifiers, which always determine
whether or not a neighborhood is robust. Most works focus on verifying robustness of L∞ neighbor-
hoods, where each pixel is bounded by an interval. Some complete verifiers encode the robustness
problem as an SMT/SAT instance and solve it using existing or enhanced solvers (e.g., Pulina &
Tacchella (2012; 2010); Katz et al. (2017); Ehlers (2017)). Others encode the problem as mixed-
integer linear problem (e.g., Tjeng et al. (2019); Xiao et al. (2019)). Other works employ branch-
and-bound approaches to split the certification problem into smaller subproblems (e.g., Bunel et al.
(2018)). Despite the significant effort to scale complete verifiers, for example, using branching
heuristics (Elboher et al., 2020) or abstraction-refinement (Wu et al., 2020), exact robustness verifi-
cation is NP-hard (Katz et al., 2017) and thus cannot scale to large and deep networks.
Incomplete verifiers Another research line propose incomplete verifiers, which rely on overap-
proximation to scale the analysis to deeper networks than possible by complete verifiers. However,
the scalability comes on the expanse of occasionally failing to prove robustness due to the over-
approximation error. Existing works propose different ways to find the optimal balance in the ef-
ficiency and precision trade-off. Some verifiers employ linear relaxations and compute, for every
neuron, lower and upper linear bounds (e.g., Muller et al. (2021); RyoU et al. (2021); XU et al. (2020);
Gowal et al. (2019)). Others rely on semidefinite programming to define convex optimizations which
can be solved in polynomial time (Dathathri et al., 2020; Raghunathan et al., 2018). Other works
bound the neurons’ values by leveraging Lipschitz global and local constants or curvatures (Singla
& Feizi (2020); Fazlyab et al. (2019)). These advancements have enabled reasoning about networks
with 100k neurons for MNIST and CIFAR-10. However, due to the overapproximation error, they
fail to prove robustness of non-trivial neighborhoods for larger networks. In particular, none can
prove robustness of non-trivial neighborhoods for ImageNet networks. Recent works propose prob-
abilistic verifiers providing robustness guarantees with confidence intervals (Zhang et al., 2020a;
2021). Although these verifiers scale to large networks and can reason about ImageNet networks,
their guarantees are probabilistic.
Robustness to features perturbations Several verifiers analyze local robustness to semantic fea-
ture perturbations. Earlier works have focused on rotations, brightness and contrast, and perform
certification by translating the feature neighborhoods to L∞ neighborhoods and then analyzing us-
ing existing verifiers (Singh et al., 2019; Wang et al., 2018a). Because of this translation, the certified
feature neighborhoods are generally small, especially if the feature is non-linear, in which case the
L∞ bounds tend to be too loose. More recent works encode the feature constraints into the verifier
in order to directly certify in the feature domain (Balunovic et al., 2019; Mohapatra et al., 2020).
Balunovic et al. (2019) focus on geometric features and rely on Monte Carlo sampling to overap-
proximate the feature constraints by convex linear bounds. They further refine these bounds by
solving an optimization problem and then verify using a standard verifier. Mohapatra et al. (2020)
propose adding an input layer that encodes the feature and is added to the original network. It can
then run standard verification over this network. Both works focus on analyzing MNIST, CIFAR-10,
and GTSRB. In contrast, we show how to scale feature robustness analysis further in order to prove
robustness of deeper networks (e.g., ImageNet networks).
Adversarial semantic features attacks We study robustness of networks to feature perturbations,
and in particular brightness, PCA, and HSL features. Several works have shown how to craft se-
mantic feature perturbations to obtain adversarial examples. For example, Hosseini & Poovendran
(2018) presented an attack relying on HSV color transformations (which is close to HSL). Other
works linked adversarial examples to PCA features (Zhang et al., 2020b; Bhagoji et al., 2018; Car-
lini & Wagner., 2017). Other kinds of feature attacks include facial feature perturbations (Goswami
et al., 2018), colorization and texture attacks (Bhattad et al., 2020), features obtained using scale-
invariant feature transform (SIFT) (Wicker et al., 2018), and semantic attribute perturbations using
multi-attribute transformation models (Joshi et al., 2019). The works of Ilyas et al. (2019) and Goh
(2019) show that adversarial examples are linked to non-robust features.
3
Under review as a conference paper at ICLR 2022
3	Proof guided by velocity: problem definition
In this section, we define the problem of proof guided by velocity and explain it for the setting of
semantic feature verification. In the following, we denote an input by x ∈ Rd , a neural network
classifier by D : Rd → Rc, and the classification of x by D by class(D(x)).
Neighborhoods, robust neighborhoods, and feature-based neighborhoods Given an input x, a
neighborhood I(x) ⊆ Rd and a classifier D, we say I(x) is robust if ∀x0 ∈ I(x), class(D(x0)) =
class(D(x)). We focus on feature neighborhoods. A feature f : Rd × R → Rd maps an input x
and a diameter δ to the perturbation of x along the feature f by δ . For all features f and inputs x,
f (x, 0) = x. Given a feature f, an input x, and a diameter δ, the feature neighborhood If,δ (x) is
the set of all perturbations of x along f with diameter at most δ: If,δ (x) = {f (x, δ0) | 0 ≤ δ0 ≤ δ}.
Feature neighborhoods can be certified with a natural divide-and-conquer approach: given a target
diameter δ and a sequence δ0 = 0, δ1, . . . , δt such that Σiδi = δ, we can independently certify each
neighborhood If,δi+1 (f (x, δi)), and if all are robust we can infer that If,δ (x) is robust.
Motivation Robustness verifiers struggle to scale to deep networks and large neighborhoods either
because the computation cannot terminate in a reasonable time or that the overapproximation error
accumulates so that the analysis result is inconclusive. To reduce the overapproximation error, many
approaches have been introduced, including splitting the neighborhood into several parts that can be
verified separately, refining neurons’ bounds, and encoding input relations. The decision of which
action to take and with which parameters is key to enable the verifier to complete as fast as possible.
To scale the analysis, we propose to dynamically split the verification problem into subproblems.
That is, the analysis executes a series of verification steps. Each step invokes a call to the verifier
and accordingly picks the next subproblem. The overall computation time is the total execution
time of all steps. A key question that arises is: what is the optimal step series which minimizes the
execution time? To illustrate, assume verifier A can certify a neighborhood with diameter δ = 0.1
in one second and a neighborhood with diameter δ = 0.01 in one millisecond. In this case, if
we can partition the larger neighborhood into ten disjoint neighborhoods with diameter δ = 0.01,
the analysis completes 10x faster than if let A certify the full neighborhood. Beyond improving
execution times, splitting a neighborhood is sometimes the only possibility to successfully certifying
a neighborhood (i.e., avoiding inconclusive certification results). To illustrate, assume that we wish
to verify a neighborhood with diameter δ = 0.1, and that A can certify any neighborhood in one
second. Assume A fails for any neighborhood with diameter δ > 0.01. In this case, if we begin
by partitioning the larger neighborhood into ten disjoint neighborhoods with diameter δ = 0.01, the
analysis completes with a conclusive result within 10 seconds and avoids the wasted time spent on
the failed calls to the analyzer. That is, a good partition into verification steps not only minimizes
the execution times of each step but also minimizes the steps with inconclusive results.
We view the verification problem as a dynamic system, where each step is associated with a state sk.
The state consists of the current subproblem’s diameter δk, the diameter proven so far (given as the
sum of the verified diameters Σj<k δA,j), and other auxiliary data. A verifier A is invoked to solve
the verification subproblem and returns the certified diameter δA,k (which is δk or 0), and possibly
other internal verification decisions. Accordingly, a policy function F picks an action uk to decide
on the next subproblem. The analysis terminates either when the proven diameter is equal to the
target diameter or when the policy picks a subproblem whose diameter is below a threshold δmin
(indicating that the analysis is close to an adversarial example). Figure 2 illustrates our dynamic
system. The challenge is to design a policy minimizing the overall analysis time. We phrase this
goal using the concept of proof velocity. We next provide the definitions.
Verification step series Given a feature function f, a target diameter δ, an analyzer A, and a set
of possible actions U (e.g., input splitting, refining neurons’ bounds), a verification step series is a
sequence (s1, u1), . . . , (sM, uM). Each si is a state, consisting of a diameter, the currently certified
diameter, and auxiliary data. Each ui is a an action in U. A pair (sk, uk) is called a verification step.
The last pair (sM, uM) either satisfies that the certified diameter in sM is equal to the target diameter
δ or that uM proposes a verification step which is below some precision threshold, indicating that
practically the verification should terminate and return the (maximal) certified diameter.
4
Under review as a conference paper at ICLR 2022
Input x
Feature f
Target 6
Classifier D
Sk = (MJ,幅,RQ
A X"/ = 6 or ® < 6lntn?
no
yes
AE6aj
Action uk
maximizing velocity
αk = SA,k + internal
information	' '
Policy 下 P	Verifier A
Figure 2: Our dynamic system for analyzing the robustness of D in the neighborhood If,δ(x).
Verifier We assume a robustness verifier A that takes as input ISk consisting of a neighborhood
If,δk (x) as well as guidance on how to execute internal heuristics. The verifier A returns ak con-
sisting of the diameter δA,k it verified (δk if the analysis succeeded and 0 otherwise) and additional
information on its internal computations which can be useful for the following subproblems. We
assume that A is incomplete and that if A fails, it is due to overapproximation error. However, our
definitions extend to verifiers that can determine that a neighborhood has an adversarial example.
Policy function A policy function F maps a state sk and an analysis result ak to an action uk ∈ U .
This action decides on the next subproblem to verify, which is part of the next state sk+1.
Proof velocity Given a robustness verifier A, we denote the time of verifying a neighborhood
If,δ (x) by tA(If,δ (x)). The proof velocity is the ratio of the certified neighborhood diameter δA
and the analysis time: VA(I)= 4/A(X)). The velocity is either a positive number, if A certified the
neighborhood, or0 if it failed. A zero velocity means that the analysis has to refine this neighborhood
in order to verify it successfully (e.g., by partitioning it) and that we have not gained from this
analysis. We note that this definition also accommodates verifiers that can return failure to indicate
on the existence of adversarial examples. In this case, the velocity is zero because no diameter was
proven robust. When the verifier is clear from the context, we omit the subscript notation.
Problem definition In the problem of proof guided by velocity the goal is to define states, actions,
and a policy function F such that for every input x, feature f, and maximal diameter δ, the policy
returns an action uk which maximizes the proof velocity of the next verification subproblem:
∀k.F (sk, ak) ∈ arg max{v(I sk+1 (x))}
uk∈U
(1)
where Isk+1 is the subproblem sent to the verifier at step k + 1.
4	VeeP
In this section, we present VeeP, our approach for solving proof guided by velocity (Equation (1)).
We start by identifying the main factors affecting the proof velocity of an incomplete verifier. Ac-
cordingly, we define the states, actions, and policy function.
Velocity factors of incomplete verifiers A key factor to expediting the proof velocity is splitting
the verification problem into subproblems for which the verifier does not fail, because any failed
call to the verifier is a waste of time (unless it can return an adversarial example). On the other
hand, if we pick too small diameters, the velocity would be too small and the overall analysis will
not complete in a reasonable time. Thus, the diameters should be adapted based on the verifier,
the neighborhood, and the diameter remained to verify. A second factor to expediting the proof
velocity is identifying the expensive computations of the verifier, which are typically tuned based on
some heuristic. For example, many verifiers employ a quick overapproximation step, followed by
a refinement step which is typically a heavier computation. Examples for refinement steps include
backsubstitution (Singh et al., 2019) and MILP encoding (Tjeng et al., 2019). Since the refinement
5
Under review as a conference paper at ICLR 2022
step is often more expensive, it is not executed for every neuron. The question of which neurons
should be refined is crucial for balancing precision and execution time.
State and actions To consider both factors, our verification state is a pair (δk, Rk) where δk is the
next diameter to verify and Rk is the set of neurons which are allowed to be refined (that is, a neuron
is refined if the analysis requires to refine it and it is in Rk). Our action set consists of action pairs
U = {-, 0, +} × P (D). An action (u1, u2) ∈ U consists ofu1 indicating whether the next diameter
should decrease (-), remain unchanged (0), or increase (+), while u2 is a subset of neurons which
may be refined during the analysis.
VeeP policy The goal of the policy F is to return an action which will lead to a proof of maximal
velocity among the current possibilities. To predict the best action, our key assumption is that there
is a relation between close verification steps. This assumption is often valid when we progress with
small verification steps along the target diameter, and it is what enables us to certify deep networks
trained for high-dimensional datasets, for which verifiers often fail even for very small diameters
because of overapproximation errors. To decide on the next action, our policy relies on the quality
of the previous neighborhoods. We next define this metric.
We assume a verifier that for a neighborhood I computes for every neuron an interval bounding
its values. Formally, we denote by xm,j the jth neuron at layer m and by [lm,j , um,j] the interval
bounding the possible values of xm,j given any y ∈ I. We denote by m = o the output layer.
Using these intervals, the verifier determines whether I is robust with respect to label j by checking
if the lower bound of the jth output neuron is greater than the upper bounds of the other output
neurons: lo,j > max{uo,j0 | j0 6= j}. We use these intervals to define the quality of I as the
difference between that lower bound and the maximal upper bound: Q(I) = lj -max{uj0 | j0 6= j}.
Intuitively, the higher the quality the farther I can expand without risking a certification failure.
Our policy function determines whether to decrease, preserve, or increase the next diameter δk based
on the recent neighborhoods’ quality and recent failures. If there were no recent failures and the last
neighborhoods’ quality is high, then VeeP adapts the diameter based on the recent observed velocity.
Usually, the diameter increases in this case. However, if VeeP observes that the verifier has higher
velocity for smaller diameters, it does not increase the diameter. If there is a failure or the quality is
too low, the diameter decreases. Otherwise, VeeP does not change the diameter.
Determining the set of neurons which can be refined, Rk, is a more subtle task, as it requires to
carefully predict the right balance between precision and execution time. In the extreme case where
we let Rk to contain all neurons, we permit the verifier to be as precise as it can, but it may take
a very long time, thus reducing the velocity significantly. On the other hand, if Rk is the empty
set, we let the verifier to be as fast as it can, but it may be very imprecise, and thus fail, thereby
reducing the velocity to zero. Our policy relies on three metrics to determine whether to include a
node xm,j in Rk : (i) the neuron’s effect on the neighborhood’s quality, (ii) the refinement gain, and
(iii) the execution time. Based on these, our policy picks for Rk all neurons except those with the
lowest effect on the neighborhood’s quality, lowest refinement gain, and highest execution time. To
compute these metrics, we assume the verifier returns for every analyzed neighborhood the intervals
bounding each neuron, before and after refinement (if applicable), and the execution time of each
refinement. We next define the effect ofa neuron on the neighborhood’s quality and refinement gain.
Given a neuron xm,j , its effect on the neighborhood’s quality is the absolute value of the gradient
of Q(I) by Xmj: e(xm,j)，| 智? |. The higher e(xm,j) the higher We expect the neighborhood
quality to be if we refine xm,j . Computing the gradient of a neuron as part of robustness analysis
has been introduced before to improve verifiers’ precision for L∞ neighborhoods (Wang et al.,
2018a;b). In our context, We use the gradients to identify hoW to maximize the proof velocity. A
neuron’s refinement gain is the difference betWeen the interval size before and after refinement. That
is, given a neuron xm,j With interval [lm,j , um,j], Which Was refined to [lrm,j , urm,j], the refinement
gain is g(xm,j ) = [(um,j - lm,j) - (urm,j - lmr ,j )]. The higher g(xm,j) the higher potential the
refinement step has for xm,j .
We note that While our policy focuses only on tWo factors to maximize the proof velocity (the
diameter step δk and the refinement set Rk), VeeP can be extended to consider other factors.
6
Under review as a conference paper at ICLR 2022
Table 1: Models used in our experiments.
Dataset	model	params#	Defense
CIFAR-10	ResNetTiny	311K	PGD (Madry et al., 2018)
CIFAR-10	ResNet18	558K	PGD
CIFAR-10	ResNet34	967K	DiffAI (Mirman et al., 2018)
ImageNet	AlexNetTiny	-444K-	PGD
ImageNet	AlexNet	600K	PGD
5	Evaluation
In this section, we evaluate VeeP. We start with implementation aspects and optimizations. We
then present our experiments. We begin with a comparison to Semantify-NN over MNIST models.
We continue with evaluating VeeP over brightness and HSL features for CIFAR-10 and ImageNet
models. Finally, we evaluate VeeP for PCA features and show that some are more robust than others.
Implementation We implemented VeeP in Python using PyTorch1. For the verifier, VeeP relies
on GPUPoly (Muller et al., 2021). GPUPoly has a heuristic for deciding on the neurons to refine by
backsubstitution, and we extended it to consider Rk (i.e., the set of neurons that the policy allows to
refine). As an optimization, we build on the idea of Semantify-NN (Mohapatra et al., 2020) that en-
codes features as input layers with the goal of encoding pixel relations to reduce overapproximation
errors. Semantify-NN encodes features using standard fully-connected and convolutional layers.
For some features, this approach is practically infeasible for high-dimensional datasets because of
the high memory overhead. To illustrate, assume the input dimension is h × w × 3. The HSL input
layers, as defined in Semantify-NN, map an (R,G,B) triple into a single value in the feature domain,
resulting in a perturbed output of h × w. This output is then translated back to the input domain.
Namely, a fully-connected layer requires (h × w) × (h × w × 3) weights. For ImageNet, where
h = w = 224, this layer becomes too big to fit into a standard memory (over 60GB). Instead, we
observe that, for some features, the feature layer’s weights are mostly zeros and thus this layer can
be implemented using sparse layers (Richter & Wattenhofer, 2018; Ardakani et al., 2017).
Evaluation setup We trained models and ran the experiments on a dual AMD EPYC 7742 server
with 1TB RAM and eight NVIDIA A100 GPUs. We evaluated VeeP on CIFAR-10 (Krizhevsky,
2009), with images of size 32x32x3, and ImageNet (Deng et al., 2009), with images of size
224x224x3. We consider ResNet (He et al., 2016) and AlexNet (Krizhevsky et al., 2012) mod-
els. Since GPUPoly does not support MaxPool layers at the moment, we replaced the MaxPool
layers in AlexNet with convolutional ones. This is justified by Springenberg et al. (2015). The
CIFAR-10 models were taken from ERAN’s repository2, and we trained the ImageNet models. We
summarize the models used in Table 1. To compare VeeP with Semantify-NN, we used two fully-
connected models (2 or 4 hidden layers, 1024 neurons at each) from their repository3 trained for
MNIST (Lecun et al., 1998).
Comparison with Semantify-NN We begin by comparing VeeP with Semantify-NN, a state-of-
the-art verifier for analyzing feature neighborhoods. To scale, Semantify-NN splits the neighbor-
hoods into equal-size sub-neighborhoods, each is verified separately. This can be viewed as using
a constant policy in our framework. For the comparison, we focus on the two MNIST models de-
scribed above and the brightness feature. Unlike VeeP, which verifies general robustness, Semantify-
NN focuses on targeted attacks. That is, it computes the maximal diameter of a neighborhood which
does not contain an input of a target class t. To compare both verifiers on the same task, given
an image x, we first look for the closest diameter δadv reaching an adversarial example in the fea-
ture domain (using a grid search) and check its label to provide to Semantify-NN. We then let both
verifiers certify the maximal diameter up to δadv. We run this experiment for 100 inputs. Table 2
reports the maximal diameter δf and the execution time in seconds tf. The results indicate that both
1We are working towards making our implementation, models, and tests available for reproducibility.
2https://github.com/eth-sri/eran
3https://github.com/JeetMo/Semantify-NN
7
Under review as a conference paper at ICLR 2022
Table 2: VeeP vs. Semantify-NN on 100 brightness neighborhoods and two models.
Dataset	Model	δadv	VeeP		Semantify-NN	
			δf	tf [s]	δf	tf [s]
MNIST	fully-connected 2x1024	0.7000	0.6801	0.06	0.6884	13.03
MNIST	fully-connected 4x1024	0.6822	0.6747	1.25	0.6770	68.72
Table 3: Maximal brightness and HSL neighborhoods vs. maximal e-ball neighborhoods.
Dataset	Model	Feature	d		df	δf	δadv
CIFAR-10	ResNetTiny	Brightness	0.0047	0.4191	0.4191	0.4195
CIFAR-10	ResNetTiny	Hue	0.0047	0.1834	3.7249	3.7390
CIFAR-10	ResNetTiny	Saturation	0.0047	0.1590	0.8630	0.8796
CIFAR-10	ResNet18	Brightness	0.0046	0.4096	0.4096	0.4146
CIFAR-10	ResNet18	Saturation	0.0046	0.1387	0.8227	0.8493
ImageNet	AlexNetTiny	Brightness	0.0025	0.2210	0.2210	0.2226
ImageNet	AlexNetTiny	Hue	0.0025	0.1104	1.6320	1.6360
ImageNet	AlexNetTiny	Saturation	0.0025	0.0787	0.4445	0.4557
ImageNet	AlexNetTiny	Lightness	0.0025	0.1631	0.1764	0.1771
ImageNet	AlexNet	Brightness	0.0009	0.3042	0.3042	0.3112
ImageNet	AlexNet	Hue	0.0009	0.0871	0.4849	0.5337
ImageNet	AlexNet	Saturation	0.0009	0.0817	0.4852	0.5052
ImageNet	AlexNet	Lightness	0.0009	0.2286	0.2398	0.2592
approaches prove more than 97% of the maximal certifiable diameter, however VeeP is 55x-217x
faster than Semantify-NN. This factor is crucial for scaling to high-dimensional datasets and deep
networks. This factor also shows the advantage of our dynamic policy over a constant policy.
Brightness and HSL neighborhoods Next, we evaluate VeeP for four semantic features: bright-
ness, representing a linear feature, and HSL, representing non-linear features. HSL (hue, saturation,
and lightness) is a color space transformation, where hue defines the position in the color wheel,
saturation controls the image’s colorfulness and lightness the perceived brightness. For every model
and feature, we run VeeP on 50 inputs to compute the maximal certifiable diameter δf . We compare
δf to the diameter of the closest adversarial example in the feature domain δadv (obtained using a
grid search). For every δf, we compute df, the average pixel perturbation caused by the feature
perturbation. To compute df for an image x, we look for the maximally perturbed x0 in If,δf (x)
and average the differences between x and x0 . We compare df to d, which is the maximal diameter
of the e-ball neighborhood that GPUPoly verifies. Table 3 reports these results. Our results indicate
that VeeP proves on average at least 97% of the potentially certifiable diameters. The table also
shows that focusing on feature neighborhoods enables us to certify neighborhoods whose diameters
are larger by 99.1x compared to the diameters of the maximally certified e-ball neighborhoods. Fig-
ure 3 illustrates the differences between the certifiable feature neighborhoods and the e-ball ones.
Each triple shows an ImageNet image, the maximally perturbed image in the maximally certified
e-ball, and the maximally perturbed image in the maximally certified feature neighborhood. These
triples exemplify that the certified feature neighborhoods contain images that are visually different,
thereby showing the gain of analyzing robustness of networks to feature perturbations.
PCA features Next, we evaluate VeeP for features derived from the dataset using PCA. Since
PCA features are computed from the dataset (unlike the previous features), they are more likely to
expose robust and non-robust aspects of the model, which is also computed from the dataset. In this
experiment, we focus on the ResNet34 model for CIFAR-10. We further focus on a single class 一
the airplane class- in order to allow PCA extract features unique to that class (e.g., the sky color
feature is more relevant to the airplane class than the car class). We run PCA on the training inputs
to compute the first five PCA dimensions. For each dimension, we consider two features fi+ and
fi-, one for each direction (i.e., ±δ). For each feature and for 100 airplane images, we run VeeP to
compute the maximal diameter. Figure 4 shows the average pixel perturbation df of the maximally
8
Under review as a conference paper at ICLR 2022
Figure 3: Illustration of images in certified e-ball and feature neighborhoods. Each triple shows
an ImageNet image, the maximally perturbed image in the maximal e-ball neighborhood, and the
maximally perturbed image in the maximal feature neighborhood (for the HSL features).
PCA features
Figure 4: Average pixel perturbation df of the maximally certified PCA neighborhoods.
certified PCA neighborhoods. The bars illustrate the semantic meaning of perturbing their feature.
The figure shows there is a large variance between the features. For example, features that add the
sky color blue shades are more robust than ones that add yellow shades. Verifying a large diameter
for a certain feature on many inputs may suggest that the model is robust for perturbations along this
feature. To test this hypothesis, We consider an adversarial attack perturbing a given feature up to a
limit of df over 300 airplane images (different from the previous 100 images). We run this attack,
for the most robust feature f4+ and the least robust feature f1-, with a limit ofdf = 0.2. The attack’s
success rate is 93.3%, for f1-, and only 10.6%, for f4+. This may suggest that PCA features are
linked to global robustness properties; we leave this for future work.
Time analysis Finally, we discuss the execution time of VeeP. On the smaller models, ResNetTiny
and AlexNetTiny, and the DiffAI model, VeeP completes within 14.7 minutes on average. For the
bigger models, VeeP completes within 2.1 hours on average. These execution times can be linearly
reduced by increasing the number of GPUs.
6 Conclusion
We presented the concept of proof guided by velocity to scale feature robustness to deep networks
and high-dimensional datasets. The idea is to phrase the certification problem as a dynamic system,
splitting the task into smaller subproblems, each maximizes the proof velocity. We propose an in-
stantiation of the dynamic system and a policy that builds on past certification results to dynamically
decide on the next subproblem. We implement this concept in VeeP, and we evaluate it for various
kinds of feature neighborhoods. We show that the average diameter of the neighborhoods that VeeP
verifies is at least 97% of the maximal certifiable diameter. VeeP is also the first verifier to prove
robustness of feature neighborhoods for an AlexNet model for ImageNet within a couple of hours.
9
Under review as a conference paper at ICLR 2022
Ethics statement
Our work proposes a scalable verifier to prove the robustness of networks to feature perturbations.
This potentially has a positive impact because it can help gain a better understanding of the network
behavior and possibly increase user trust. However, our work also enables one to identify non-robust
features of the network, which may be exploited by attacks and thus have a negative impact.
References
Arash Ardakani, Carlo Condo, and Warren J. Gross. Sparsely-connected neural networks: To-
wards efficient VLSI implementation of deep neural networks. In 5th International Conference
on Learning Representations, ICLR, 2017.
Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin T. Vechev. Cer-
tifying geometric robustness of neural networks. In Annual Conference on Neural Information
Processing Systems, 2019.
Arjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, and Prateek Mittal. Enhancing robustness of
machine learning systems via data transformations. In 52nd Conference on Information Sciences
and Systems, CISS, 2018.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David A. Forsyt. Unrestricted adversarial
examples via semantic manipulation. In 8th International Conference on Learning Representa-
tions, ICLR, 2020.
Rudy Bunel, Ilker Turkaslan, Philip H. S. Torr, Pushmeet Kohli, and Pawan Kumar Mudigonda. A
unified view of piecewise linear neural network verification. In Advances in Neural Information
Processing Systems 31, 2018.
Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, AISec, 2017.
Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Ue-
sato, Rudy Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy Liang, and Pushmeet
Kohli. Enabling certification of verification-agnostic networks via memory-efficient semidefinite
programming. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE conference on computer vision and pattern recognition,
2009.
Rudiger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Deepak
D’Souza and K. Narayan Kumar (eds.), In Automated Technology for Verification and Analysis,
2017.
Yizhak Yisrael Elboher, Justin Gottschlich, and Guy Katz. An abstraction-based framework for
neural network verification. In Shuvendu K. Lahiri and Chao Wang (eds.), In Computer Aided
Verification - 32nd International Conference, CAV, 2020.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George J. Pappas. Ef-
ficient and accurate estimation of lipschitz constants for deep neural networks. In Advances in
Neural Information Processing Systems 32, 2019.
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar-
tin T. Vechev. AI2: safety and robustness certification of neural networks with abstract interpre-
tation. In IEEE Symposium on Security and Privacy, 2018.
Gabriel Goh. A discussion of ’adversarial examples are not bugs, they are features’: Two examples
of useful, non-robust features. Distill, 2019. doi: 10.23915/distill.00019.3.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In 3rd ICLR, 2015.
10
Under review as a conference paper at ICLR 2022
Gaurav Goswami, Nalini K. Ratha, Akshay Agarwal, Richa Singh, and Mayank Vatsa. Unravelling
robustness of deep learning based face recognition against adversarial attacks. In Proceedings of
34th AAAI, 2018.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Arthur Mann, and Pushmeet Kohli. Scalable verified training
for provably robust image classification. In IEEE/CVF International Conference on Computer
Vision, ICCV, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In IEEE Conference on
Computer Vision and Pattern Recognition Workshops, CVPR Workshops, 2018.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Proc. Systems 32, 2019.
Ameya Joshi, Amitangshu Mukherjee, Soumik Sarkar, and Chinmay Hegde. Semantic adversarial
attacks: Parametric transformations that fool deep classifiers. In International Conference on
Computer Vision, ICCV, 2019.
Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: An
efficient SMT solver for verifying deep neural networks. In 29th Computer Aided Verification
Conference, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. CoRR, abs/1708.07747,
2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems 25, 2012.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. In Proceedings of the IEEE 1998;86(11):2278e324, 1998.
Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. Sok: Certified robustness for deep neural networks.
CoRR, abs/2009.04131, 2020. URL https://arxiv.org/abs/2009.04131.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR, 2018.
Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract interpretation for
provably robust neural networks. In Proceedings of the 35th International Conference on Machine
Learning ICML, 2018.
Christoph Muller, Francois Serre, GagandeeP Singh, Markus PuscheL and Martin Vechev. Scaling
polyhedral neural network verification on gpus. In Proceedings of Machine Learning and Systems
3, 2021.
Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Towards verifying ro-
bustness of neural networks against A family of semantic perturbations. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition, CVPR, 2020.
Luca Pulina and Armando Tacchella. An abstraction-refinement approach to verification of artificial
neural networks. In Computer Aided Verification, 22nd International Conference, CAV, 2010.
Luca Pulina and Armando Tacchella. Challenging SMT solvers to verify neural networks. In AI
Commun. 10.3233/AIC-2012-0525, 2012.
Aditi Raghunathan, Jacob Steinhardt, and Percy Lian. Semidefinite relaxations for certifying robust-
ness to adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS),
2018.
11
Under review as a conference paper at ICLR 2022
Oliver Richter and Roger Wattenhofer. Treeconnect: A sparse alternative to fully connected layers.
In IEEE 30th International Conference on Tools with Artificial Intelligence, 2018.
Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Marian Dan, and Mar-
tin T. Vechev. Scalable polyhedral verification of recurrent neural networks. In Computer Aided
Verification - 33rd International Conference, CAV, 2021.
GagandeeP Singh, Timon Gehr, Markus PuscheL and Martin T. An abstract domain for certifying
neural networks. In Proc. ACM Program. Lang, 2019.
Sahil Singla and Soheil Feizi. Second-order Provable defenses against adversarial attacks. In Pro-
ceedings of the 37th International Conference on Machine Learning, ICML, 2020.
Jost Tobias SPringenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving
for simPlicity: The all convolutional net. In 3rd International Conference on Learning Represen-
tations, ICLR, 2015.
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer Programming. In 7th International Conference on Learning Representations, ICLR, 2019.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety
analysis of neural networks. In 31th Advances in Neural Information Processing Systems, 2018a.
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis
of neural networks using symbolic intervals. In 27th USENIX Security Symposium, 2018b.
Matthew Wicker, Xiaowei Huang, and Marta Kwiatkowska. Feature-guided black-box safety testing
of deeP neural networks. In 24th Tools and Algorithms for the Construction and Analysis of
Systems TACAS, 2018.
Haoze Wu, Alex Ozdemir, Aleksandar Zeljic, Kyle Julian, Ahmed Irfan, Divya GoPinath, Sadjad
Fouladi, Guy Katz, Corina S. Pasareanu, and Clark W. Barrett. Parallelization techniques for
verifying neural networks. In In Formal Methods in Computer Aided Design, FMCAD, 2020.
Kai Yuanqing Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shafiullah, and Aleksander Madry.
Training for faster adversarial robustness verification via inducing relu stability. In 7th Interna-
tional Conference on Learning Representations, ICLR, 2019.
Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya
Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic Perturbation analysis for scalable certified
robustness and beyond. In Advances in Neural Information Processing Systems 33, 2020.
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Black-box certification
with randomized smoothing: A functional oPtimization based framework. In Advances in Neural
Information Processing Systems 33, 2020a.
Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Scalable certified seg-
mentation via randomized smoothing. In Proceedings of the 38th International Conference on
Machine Learning, ICML, 2021.
Yonggang Zhang, Xinmei Tian, Ya Li, Xinchao Wang, and Da. Tao. PrinciPal comPonent adversarial
examPle. In IEEE Trans. Image Process, 2020b.
12