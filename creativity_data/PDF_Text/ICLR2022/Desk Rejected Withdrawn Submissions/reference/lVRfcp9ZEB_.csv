title,year,conference
 Isotropy in the contextual embeddingspace: Clusters and manifolds,2021, In International Conference on Learning Representations
 Intrinsic dimensionestimation: Relevant techniques and a benchmark framework,2015, Mathematical Problems in Engi-neering
 BERT: pre-training of deepbidirectional transformers for language understanding,2018, CoRR
 Frage: Frequency-agnosticword representation,2018, ArXiv
 Word re-embedding via manifold dimensionality retention,2017, In EMNLP
 A structural probe for finding syntax in word representa-tions,2019, In NAACL-HLT
 Maximum likelihood estimation of intrinsic dimension,2004, InProceedings of the 17th International Conference on Neural Information Processing Systems
 Learning to remove: Towards isotropicpre-trained BERT embedding,2021, CoRR
 All-but-the-top: Simple and effective postprocessingfor word representations,2017, CoRR
 Improving language understanding by generative pre-training,2018, 2018
 Language modelsare unsupervised multitask learners,2019, 2019
 Refiningword representations by manifold learning,2019, pp
 Getting in shape: Word embedding subspaces,2019, In IJCAI
 Isobn: Fine-tuning bert with isotropic batchnormalization,2021, In AAAI
