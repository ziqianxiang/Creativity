title,year,conference
 Pruning convolu-tional neural networks with self-supervision,2020, ArXiv
 A simple framework forcontrastive learning of visual representations,2020, ArXiv
 Bigself-supervised models are strong semi-supervised learners,2020, ArXiv
 Exploring simple siamese representation learning,2020, arXiv preprintarXiv:2011
 Towards efficient model compres-sion via learned global ranking,2020, 2020 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) 
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Improved regularization of convolutional neural networkswith cutout,2017, ArXiv
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In ICML
 Bootstrap your own latent: A new approach to self-supervised learning,2020, ArXiv
 Learning both weights and connections for efficientneural network,2015, ArXiv
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR)
 Meta filter pruning to accelerate deep convolutionalneural networks,2019, ArXiv
 Distilling the knowledge in a neural network,2015, ArXiv
 Etude de la distribution florale dans une portion des alpes et du jura,1901, Bulletin de laSociete Vaudoise des Sciences Naturelles
 Compress: Self-supervisedlearning by compressing representations,2020, ArXiv
 Learning multiple layers of features from tiny images,2009, 2009
 The hungarian method for the assignment problem,1955, Naval research logisticsquarterly
 Dhp: Differentiable meta pruning viahypernetworks,2020, ArXiv
 Accelerating convolutionalnetworks via global & dynamic filter pruning,2018, In IJCAI
 Metapruning: Metalearning for automatic neural network channel pruning,2019, 2019 IEEE/CVF International Conferenceon Computer Vision (ICCV)
 Learning efficient convolutionalnetworks through network slimming,2017, 2017 IEEE International Conference on Computer Vision(ICCV)
 Learning sparse neural networks through l0regularization,2018, ArXiv
 Proving the lottery ticket hypothesis:Pruning is all you need,2020, In ICML
 On first-order meta-learning algorithms,2018, ArXiv
 Meta-learning for semi-supervised few-shot classification,2018, ArXiv
 DropNet: Reducing neural network complexity viaiterative pruning,2020, In Hal DaUme In and Aarti Singh (eds
 Meta-learning with network pruning,2020, ArXiv
 Greedy optimization provably wins the lottery: Logarithmic numberof winning tickets is enough,2020, ArXiv
 Barlow twins: Self-supervisedlearning via redundancy reduction,2021, ArXiv
 Neuron-levelstructured pruning using polarization regularizer,2020, In NeurIPS
