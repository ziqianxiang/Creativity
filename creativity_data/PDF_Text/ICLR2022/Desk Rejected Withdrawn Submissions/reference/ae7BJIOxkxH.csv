title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE Conference on Com-Puter Vision and Pattern Recognition
 Data-free learning of student networks,2019, In Proceedings of the IEEEInternational Conference on Computer Vision
 Long live the lottery:The existence of winning tickets in lifelong learning,2021, In International Conference on LearningRepresentations
 Robust overfittingmay be mitigated by properly learned smoothening,2021, In International Conference on LearningRepresentations
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Residual distillation: Towards portable deep neural networks withoutshortcuts,2020, Advances in Neural Information Processing Systems
 Data-free knowledge distillation for deepneural networks,2017, arXiv preprint arXiv:1710
 Shufflenet v2: Practical guidelines forefficient cnn architecture design,2018, In Proceedings of the European conference on computer vision(ECCV)
 Improved knowledge distillation via teacher assistant,2019, arXiv preprintarXiv:1902
 Relational knowledge distillation,2019, In Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Learning deep representations with probabilistic knowledgetransfer,2018, In Proceedings of the European Conference on Computer Vision (ECCV)
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Aggregated residual trans-formations for deep neural netWorks,2017, In Proceedings of the IEEE conference on computer visionand pattern recognition
 Revisiting knowledge distillationvia label smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Regularizing class-wise predictions viaself-knowledge distillation,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
 Be yourown teacher: Improve the performance of convolutional neural networks via self distillation,2019, InProceedings of the IEEE International Conference on Computer Vision
