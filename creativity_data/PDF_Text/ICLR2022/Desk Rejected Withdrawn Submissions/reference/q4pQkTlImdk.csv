title,year,conference
 Adaptive dropout for training deep neural net-works,2013, In Christopher J
 ELECTRA: pre-training text encoders as discriminators rather than generators,2020, In 8th International Conferenceon Learning Representations
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In Jill Burstein
 Reducing transformer depth on demand with struc-tured dropout,2020, In 8th International Conference on Learning Representations
 Contextualdropout: An efficient sample-dependent dropout module,2021, CoRR
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In Doina Precup and Yee Whye Teh (eds
 Dropout as a bayesian approximation: Representing model un-certainty in deep learning,1050, In Maria-Florina Balcan and Kilian Q
 Long short-term memory,1997, Neural Comput
 Categorical reParameterization with gumbel-softmax,2017, In 5thInternational Conference on Learning Representations
 Neural mask generator: Learning to generateadaPtive word maskings for language model adaPtation,2020, In Bonnie Webber
 Pre-training with metalearning for chinese word segmentation,2021, In Kristina Toutanova
 schubert: OPtimizing elements of BERT,2020, In Dan Jurafsky
 Self-normalizing neural networks,2017, In Isabelle Guyon
 ALBERT: A lite BERT for self-suPervised learning of language rePresentations,2020, In8th International Conference on Learning Representations
 Meta droPout: Learning to Perturblatent features for generalization,2020, In 8th International Conference on Learning Representations
 DARTS: differentiable architecture search,2019, In7th International Conference on Learning Representations
 Energy-based imitation learning,2021, InFrank Dignum
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Multi-agentactor-critic for mixed cooperative-competitive environments,2017, In Isabelle Guyon
 Pre-training textrepresentations as meta learning,2020, CoRR
 Learning word vectors for sentiment analysis,2011, In Dekang Lin
 A* sampling,2014, In Zoubin Ghahra-mani
 Building a large annotatedcorpus of english: The penn treebank,1993, Comput
 Improving language under-standing by generative pre-training,2018, 2018
 Optimization as a model for few-shot learning,2017, In 5th Interna-tional Conference on Learning Representations
 DREAM: A challengedataset and models for dialogue-based reading comprehension,2019, Trans
 Policy gradientmethods for reinforcement learning with function approximation,1999, In Sara A
 Learning to learn: IntrodUction and overview,1998, In SebastianThrUn and Lorien Y
 Attention is all yoU need,2017, In Isabelle GUyon
 Regularization of neu-ral networks using dropconnect,2013, In Proceedings of the 30th International Conference on Ma-chine Learning
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Mach
 Transformers: State-of-the-art nat-ural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
 Sg-net: Syntax-guided machine reading comprehension,2020, In The Thirty-Fourth AAAI Conference on Artificial Intel-ligence
 Scheduled drophead: A reg-ularization method for transformer models,2020, In Trevor Cohn
 Neural architecture search with reinforcement learning,2017, In 5th Interna-tional Conference on Learning Representations
