title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Stronger generalization bounds fordeep nets via a compression approach,2018, In International Conference on Machine Learning
 Comparing Dynamics : Deep NeuralNetworks versus Glassy Systems,2018, In International Conference on Machine Learning
 Geometry of Energy Landscapes and the Optimizabil-ity of Deep Neural Networks,2020, Physical Review Letters
 Reconciling modern machine-Iearning practice and the classical bias-variance trade-off,2019, Proceedings of the National AcademyOfSciencesofthe United States of America 
 On the complexity of neural network classifiers: A com-parison between shallow and deep architectures,2014, IEEE Transactions on Neural Networks andLearning Systems
 Generalization bounds of stochastic gradient descent for wide anddeep neural networks,2019, Advances in Neural Information Processing Systems
 On Lazy Training in Differentiable Program-ming,2019, In Neural Information Processing Systems
 Random deep neural networks are biasedtowards simple functions,2019, Advances in Neural Information Processing Systems
 Gradient descent finds globalminima of deep neural networks,2019, In International Conference on Machine Learning
 The Power of Depth for Feedforward Neural Networks,2016, In Pro-ceedings of Machine Learning Research
 Jamming transition as a paradigm to understand the loss landscape of deepneural networks,2019, Physical Review E
 Disentangling feature and lazytraining in deep neural networks,2019, arXiv preprint arXiv:1906
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of Machine Learning Research
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE InternationalConference on Computer Vision
 Neural tangent kernel: Convergence and gen-eralization in neural networks,2018, In Advances in Neural Information Processing Systems
 Polylogarithmic width suffices for gradient descent to achieve arbi-trarily small test error with shallow ReLU networks,2019, arXiv preprint arXiv:1909
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in Neural Information Processing Systems
 Deep learning,2015, Nature
 Deep neural networks as Gaussian processes,2018, In International Conference onLearning Representations
 Wide Neural Networks of Any Depth Evolve as Linear ModelsUnder Gradient Descent,2019, In Advances in Neural Information Processing Systems
 The largelearning rate phase of deep learning: The catapult mechanism,2020, arXiv:2003
 Gradient Descent Maximizes the Margin of Homogeneous Neural Net-works,2020, In International Conference on Learning Representations
 On the number of linearregions of deep neural networks,2014, In Advances in Neural Information Processing Systems
 Generalization in Deep Networks: The Role of Distancefrom Initialization,2017, In Advances in Neural Information Processing Systems
 A Modern Take on the Bias-Variance Tradeoff in Neural Net-works,2019, In Workshop on Identifying and Understanding Deep Learning Phenomena
 Exploring Gener-alization in Deep Learning,2017, In Advances in Neural Information Processing Systems
 The roleof over-parametrization in generalization of neural networks,2019, In International Conference onLearning Representations
 Gradient Descent can Learn LessOver-parameterized Two-layer Neural Networks on Classification Problems,2019, arXiv preprintarXiv:1905
 Deep learning generalizes becausethe parameter-function map is biased towards simple functions,2019, In International Conference onLearning Representations
 Expo-nential expressivity in deep neural networks through transient chaos,2016, In Advances in NeuralInformation Processing Systems
 Depth-width tradeoffs in approximating natural functions with neuralnetworks,2017, In International Conference on Machine Learning
 Deep Information Propagation,2017, In International Conference onLearning Representations
 Practical Bayesian Optimization of MachineLearning Algorithms,2012, In Advances in Neural Information Processing Systems
 A jamming transition from under- To over-parametrization affects generalization in deeplearning,2019, Journal of Physics A: Mathematical and Theoretical
 UnderstandingDeep Learning Requires Rethinking of Generalization,2017, In International Conference on LearningRepresentations
 Gradient descent optimizes over-parameterized deep ReLU networks,2020, Machine Learning
