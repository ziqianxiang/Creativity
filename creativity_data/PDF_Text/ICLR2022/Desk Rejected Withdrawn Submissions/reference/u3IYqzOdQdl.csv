title,year,conference
 A unified architecture for natural language processing: Deepneural networks with multitask learning,2008, In Proceedings of the 25th international conference onMachine learning
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Ensemble methods in machine learning,2000, In International workshop on multi-ple classifier systems
 Deep ensembles: A loss landscape per-spective,2019, arXiv preprint arXiv:1912
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,1050, In international conference on machine learning
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 On calibration of modern neuralnetworks,2017, In International Conference on Machine Learning
 Training independent subnetworks for robustprediction,2020, arXiv preprint arXiv:2010
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, Proceedings of the International Conference on Learning Represen-tations
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Adabits: Neural network quantization with adaptive bit-widths,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)
 Simple and scalable predictiveuncertainty estimation using deep ensembles,2016, arXiv preprint arXiv:1612
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 A signal propagationperspective for pruning neural networks at initialization,2019, In International Conference on LearningRepresentations
 Whym heads are better than one: Training a diverse ensemble of deep networks,2015, arXiv preprintarXiv:1511
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Cross-stitch networks formulti-task learning,2016, In Proceedings of the IEEE conference on computer vision and pattern recog-nition
 Obtaining well calibrated proba-bilities using bayesian binning,2015, In Proceedings of the AAAI Conference on Artificial Intelligence
 Data-free quantizationthrough weight equalization and bias correction,2019, In Proceedings of the IEEE/CVF InternationalConference on Computer Vision
 Adapting convolutional neural networks for geographicaldomain shift,2019, arXiv preprint arXiv:1901
 Neural parameterallocation search,2020, arXiv preprint arXiv:2006
 Non-discriminativedata or weak model? on the relative importance of data and model resolution,2019, In Proceedings ofthe IEEE/CVF International Conference on Computer Vision Workshops
 Adashare: Learning what to sharefor efficient deep multi-task learning,2019, arXiv preprint arXiv:1911
 Shared roots: Regularizing neural networks through multitasklearning,2014, TR2014-762
 Combining ensembles and data augmentation can harm yourcalibration,2020, arXiv preprint arXiv:2010
 Batchensemble: an alternative approach to efficientensemble and lifelong learning,2020, arXiv preprint arXiv:2002
 Hyperparameter ensembles forrobustness and uncertainty quantification,2020, arXiv preprint arXiv:2006
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Facial landmark detection by deepmulti-task learning,2014, In European conference on computer vision
