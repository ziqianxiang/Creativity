title,year,conference
 Unsupervised object discovery andlocalization in the wild: Part-based matching with bottom-up region proposals,2015, 2015
 Semantic correspondence with transformers,2021, arXiv preprint arXiv:2106
 Rethinking attentionwith performers,2020, arXiv preprint arXiv:2009
 Universal corre-spondence network,2016, 2016
 Superpoint: Self-supervised interestpoint detection and description,2018, In Proceedings of the IEEE conference on computer vision andpattern recognition workshops
 An image is worth 16x16 words: Transformers for image recognition atscale,2021, ICLR
 Computer Vision: A Modern Approach,2011, (Second edition)
 Proposal flow,2016, 2016
 Proposal flow: Semantic correspon-dences from object proposals,2018, 2018
 Scnet: Learning semantic correspondence,2017, 2017
 Dynamic contextcorrespondence network for semantic alignment,2019, 2019
 Guided se-mantic flow,2020, 2020
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Imagenet classification with deep convo-lutional neural networks,2012, 2012
 Learning to distillconvolutional features into compact local descriptors,2021, In 2021 IEEE Winter Conference on Ap-plications of Computer Vision (WACV)
 Sfnet: Learning object-aware seman-tic correspondence,2019, 2019
 Correspon-dence networks with adaptive neighbourhood consensus,2020, 2020
 Probabilistic model dis-tillation for semantic correspondence,2021, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR)
 Semantic correspondence as an optimaltransport problem,2020, 2020
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Convolutional hough matching networks,2021, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
 Hyperpixel flow: Semantic correspondencewith multi-layer neural features,2019, 2019a
 SPair-71k: A large-scale benchmark forsemantic correspondence,2019, arXiv prepreint arXiv:1908
 Learning to compose hypercolumns forvisual correspondence,2020, 2020
 R2d2: Repeatable and reliable detector and descriptor,2019, arXiv preprintarXiv:1906
 Efficient neighbourhood consensus networksvia submanifold sparse convolutions,2020, 2020
 Self-attention with relative position represen-tations,2018, In Proceedings of the 2018 Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Language Technologies
 End-to-end mem-ory networks,2015, In C
 Joint recovery of dense correspondence andcosegmentation in two images,2016, 2016
 Sosnet: Secondorder similarity regularization for local descriptor learning,2019, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Training data-efficient image transformers amp; distillation through attention,2021, InInternational Conference on Machine Learning
 Attention is all you need,2017, In Advances in Neural Infor-mation Processing Systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Fastformer: Additive attention can be allyou need,2021, arXiv preprint arXiv:2108
