title,year,conference
 To understand deep learning we need to under-stand kernel learning,2018, arXiv preprint arXiv:1802
 Kernel descriptors for visual recognition,2010, In Advancesin neural information processing systems (NeurIPS)
 Equilibrated adaptive learning rates for non-convex optimization,2015, In Advances in neural information processing systems (NeurIPS)
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Shampoo: Preconditioned stochastic tensor op-timization,2018, In International conference on machine learning (ICML)
 Second order design of multiclasskernel machines,2016, In 2016 International joint conference on neural networks (IJCNN)
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)
 Kernel methods in machine learn-ing,2008, The annals of statistics
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition (CVPR)
 Asymptotic behaviors of support vector machines with gaussiankernel,2003, Neural computation
 Deep kernel: Learning kernel function from datausing deep neural network,2016, In Proceedings of the 3rd IEEE/ACM International conference on bigdata computing
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Preconditioned stochastic gradient descent,2017, IEEE transactions on neural networks andlearning systems (TNNLS)
 Preconditioner on matrix lie group for sgd,2018, arXiv preprint arXiv:1809
 End-to-end kernel learning with supervised convolutional kernel networks,2016, In Ad-vances in neural information processing systems (NeurIPS)
 Deep learning via hessian-free optimization,2010, In International conference on machinelearning (ICML)
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International conference on machine learning (ICML)
 Glove: Global vectors for wordrepresentation,2014, In Empirical methods in natural language processing (EMNLP)
 No more pesky learning rates,2013, In International confer-ence on machine learning (ICML)
 A generalized representer theorem,2001, InInternational conference on computational learning theory
 Lecture 6,2012,5-rmsprop: Divide the gradient by a runningaverage of its recent magnitude
 Enhancing svmswith problem context aware pipeline,2021, In Proceedings of the 27th ACM SIGKDD Conference onKnowledge Discovery & Data Mining
 Large batch size training of neural networks with adversarial training andsecond-order information,2018, arXiv preprint arXiv:1810
 Adahessian: Anadaptive second order optimizer for machine learning,2021, AAAI (Accepted)
