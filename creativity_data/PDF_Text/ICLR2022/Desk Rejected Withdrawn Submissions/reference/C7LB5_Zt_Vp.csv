title,year,conference
 Natural gradient works efficiently in learning,1998, Neural computation
 Adaptive dropout for training deep neural networks,2013, Advances inneural information processing systems
 Learning long-term dependencies with gradientdescent is difficult,1994, IEEE transactions on neural networks
 Bert: Pre-training of deePbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 AdaPtive subgradient methods for online learning andstochastic oPtimization,2011, Journal of machine learning research
 ExPected energy-based restricted boltzmann machinefor classification,2015, Neural networks
 Sigmoid-weighted linear units for neural networkfunction aPProximation in reinforcement learning,2018, Neural Networks
 Demystifying droPout,2019, In International Conference onMachine Learning
 AdaPtive convolutional relus,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 DroPout rademacher comPlexity of deeP neural networks,2016, ScienceChina Information Sciences
 DroPblock: A regularization method for convolu-tional networks,2018, arXiv preprint arXiv:1810
 Deep Learning,2016, MIT Press
 Delving deeP into rectifiers: SurPassinghuman-level Performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 The vanishing gradient problem during learning recurrent neural nets and problemsolutions,1998, International Journal of Uncertainty
 Guided droPout,2019, In Proceedings of the AAAIConference on Artificial Intelligence
 Adam: A method for stochastic oPtimization,2014, arXiv preprintarXiv:1412
 Variational droPout and the local reParameteri-zation trick,2015, Advances in neural information processing systems
 Self-normalizingneural networks,2017, In Proceedings of the 31st international conference on neural information pro-cessing systems
 Zoneout: Regularizing rnnsby randomly preserving hidden activations,2016, arXiv preprint arXiv:1606
 Survey of dropout methods for deep neuralnetworks,2019, arXiv preprint arXiv:1904
 Probact: A probabilistic activation function for deep neural networks,2019, arXiv preprintarXiv:1905
 Rectifier nonlinearities improve neuralnetwork acoustic models,2013, In Proc
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Mish: A self regularized non-monotonic neural activation function,2019, arXiv preprintarXiv:1908
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 Overfitting in adversarially robust deep learning,2020, InInternational Conference on Machine Learning
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Lecture 6,2012,5-rmsprop: Divide the gradient by a runningaverage of its recent magnitude
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Extracting andcomposing robust features with denoising autoencoders,2008, In Proceedings of the 25th internationalconference on Machine learning
 Dropout training as adaptive regularization,2013, Advancesin neural information processing systems
 Regularization of neuralnetworks using dropconnect,1058, In International conference on machine learning
 Fast dropout training,2013, In international conference on machinelearning
 Smoothout:Smoothing out sharp minima to improve generalization in deep learning,2018, arXiv preprintarXiv:1805
 Transformers: State-of-the-art natural language processing,2020, In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations
