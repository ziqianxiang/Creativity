title,year,conference
 Greedy layerwise learning can scaleto imagenet,2019, In International conference on machine learning
 Group knowledge transfer: Federatedlearning of large cnns at the edge,2020, Advances in Neural Information Processing Systems
 Communication-efficient multimodal split learning for mmwave receivedpower prediction,2020, IEEE Communications Letters
 Federated optimization:Distributed machine learning for on-device intelligence,2016, arXiv preprint arXiv:1610
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Learning multiple layers of features from tiny images,2009,2009
 Parallel training of deep networks with local updates,2020, arXivpreprint arXiv:2012
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 On the convergence offedavg on non-iid data,2019, In International Conference on Learning Representations
 Training neural networks with local error signals,2019, InInternational Conference on Machine Learning
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 Detailed comparison ofcommunication efficiency of split learning and federated learning,2019, arXiv preprint arXiv:1909
 Local sgd converges fast and communicates little,2019, In International Conference onLearning Representations
 Sparsified sgd with memory,2018, Advancesin Neural Information Processing Systems
 Splitfed: Whenfederated learning meets split learning,2020, arXiv preprint arXiv:2004
 Split learning for health:Distributed deep learning without sharing raw patient data,2018, arXiv preprint arXiv:1812
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Parallel restarted sgd with faster convergence and lesscommunication: Demystifying why model averaging works for deep learning,2019, In Proceedings ofthe AAAI Conference on Artificial Intelligence
