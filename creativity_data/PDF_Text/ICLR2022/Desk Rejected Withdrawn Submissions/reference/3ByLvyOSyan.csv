title,year,conference
 Normalization propagation:A parametric technique for removing internal covariate shift in deep networks,2016, In InternationalConference on Machine Learning
 HoW to initialize your network? robustinitialization for weightnorm &amp; resnets,2019, In H
 Rezero is all you need: Fast convergence at large depth,2020, CoRR
 Characterizing signal propagation to close the per-formance gap in unnormalized resnets,2020, In International Conference on Learning Representations
 High-performance large-scale imagerecognition without normalization,2021, ArXiv
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR) Workshops
 Metainit: Initializing learning by learning to ini-tialize,2019, In H
 Batch normalization biases residual blocks towards the identity functionin deep networks,2020, Advances in Neural Information Processing Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andStatistics
 How to start training: The effect of initialization and architec-ture,2018, In S
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Identity mappings in deep residualnetworks,2016, In European conference on computer vision
 Centered weight normalization inaccelerating training of deep neural networks,2017, In IEEE International Conference on ComputerVision
 Learning multiple layers of features from tiny images,2009, 2009
 Residual continual learning,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Rehearsal-free continual learning oversmall non-iid batches,2020, In CVPR Workshops
 SGDR: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 The rectified gaussian distribution,1998, Advancesin Neural Information Processing Systems
 Four things everyone should know to improve batchnormalization,2020, In International Conference on Learning Representations
 Towards stabi-lizing batch statistics in backward propagation of batch normalization,2020, In International Confer-ence on Learning Representations
 Residual learning without normalization viabetter initialization,2019, In International Conference on Learning Representations
