title,year,conference
 Eis - efficient andtrainable activation functions for better accuracy and performance,2021, In Igor Farkas
 Tanhsoftâ€”dynamictrainable activation functions for faster learning and better performance,2021, IEEE Access
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Yee Whye Teh and Mike Titterington (eds
 Stochastic estimation of the maximum ofa regression function,1952, Annalsof Mathematical Statistics
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep con-volutional neural networks,2012, In Proceedings of the 25th International Conference on Neural In-formation Processing Systems - Volume 1
 Gradient-based learning applied to document recog-nition,1998, Proceedings of the IEEE
 Rectifier nonlinearities improve neural net-work acoustic models,2013, In in ICML Workshop on Deep Learning for Audio
 Rectified linear units improve restricted boltzmann machines,2010, InJohannes Furnkranz and Thorsten Joachims (eds
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Scalable parallel programming,2008, In 2008 IEEEHot Chips 20 Symposium (HCS)
 A stochastic approximation method,1951, Annals of Mathematical Statistics
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Improving deep neural net-works using softplus units,2015, In 2015 International Joint Conference on Neural Networks (IJCNN)
