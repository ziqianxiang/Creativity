title,year,conference
 The fifth pascal recognizing textualentailment challenge,2009, In TAC
 Incorporating visualsemantics into sentence representations within a grounded space,2019, In EMNLP
 Language modelsare few-shot learners,2020, CoRR
 Electra: Pre-training text encodersas discriminators rather than generators,2019, In International Conference on Learning Representations
 Imagined visual representations as multimodalembeddings,2017, In Thirty-First AAAI Conference on Artificial Intelligence
 Supervised learningof universal sentence representations from natural language inference data,2017, In Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing
 The pascal recognising textual entailment challenge,2006, InJoaquin Quinonero-Candela
 Bert: Pre-training of deep bidirectionaltransformers for language understanding,2019, In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 Automatically constructing a corpus of sentential paraphrases,2005, InProceedings of the Third International Workshop on Paraphrasing (IWP2005)
 The third pascal recognizing textualentailment challenge,2007, In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing
 Thesecond pascal recognising textual entailment challenge,2006, In Proceedings of the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment
 Learning distributed representations of sentences fromunlabelled data,2016, In NAACL HLT 2016
 Learning visually grounded sentencerepresentations,2018, In Proceedings of the 2018 Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies
 Skip-thought vectors,2015, In Advances in Neural Information Processing Systems 28: Annual Conferenceon Neural Information Processing Systems 2015
 Albert:A lite bert for self-supervised learning of language representations,2019, In International Conference on LearningRepresentations
 Distributed representations of sentences and documents,2014, In Internationalconference on machine learning
 Visualbert: A simple andperformant baseline for vision and language,2019, arXiv preprint arXiv:1908
 Oscar: Object-semantics aligned pre-training for vision-language tasks,2020, arXivpreprint arXiv:2004
 Microsoft COCO: common objects in context,2014, In Computer Vision - ECCV2014 - 13th European Conference
 Roberta: A robustly optimized bert pretraining approach,2019, arXiv preprintarXiv:1907
 An efficient framework for learning sentence representations,2018, In6th International Conference on Learning Representations
 Fixing weight decay regularization in adam,2018, 2018
 Vilbert: Pretraining task-agnostic visiolinguistic represen-tations for vision-and-language tasks,2019, In Advances in Neural Information Processing Systems
 Distributed representations of wordsand phrases and their compositionality,2013, In Advances in neural information processing Systems
 Deep contextualized word representations,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies
 Sentence-bert: Sentence embeddings using siamese bert-networks,2019, CoRR
 Recursive deep models for semantic compositionality over a sentiment treebank,2013, In Proceedings of the2013 Conference on Empirical Methods in Natural Language Processing
 Vl-bert: Pre-training of genericvisual-linguistic representations,2020, In ICLR
 Lxmert: Learning cross-modality encoder representations from transformers,2019, InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 GLUE: Amulti-task benchmark and analysis platform for natural language understanding,2018, CoRR
 Neural network acceptability judgments,2018, CoRR
 A broad-coverage challenge corpus for sentenceunderstanding through inference,2018, In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Xlnet:Generalized autoregressive pretraining for language understanding,2019, In Advances in neural informationprocessing systems
 Unified vision-language pre-training for image captioning and vqa,2020, In AAAI
