title,year,conference
 Auxiliary image regularizationfor deep cnns With noisy labels,2016, In 4th International Conference on Learning Representations
 The generalization-stabilitytradeoff in neural netWork pruning,2020, In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020
 Reconciling modern machine-learning practice and the classical bias-variance trade-off,27, Proceedings of the National Academyof Sciences
 Provable benefits ofoverparameterization in model compression: From double descent to pruning neural netWorks,2021, InProceedings of the AAAI Conference on Artificial Intelligence
 The state of sparsity in deep neural netWorks,2019, CoRR
 On the robustness of monte carlo dropout trained With noisy labels,2021, In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Qualitatively characterizing neural netWork optimization prob-lems,2015, In Yoshua Bengio and Yann LeCun (eds
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InNeurIPS
 Learning both Weights and connections forefficient neural netWork,2015, Advances in Neural Information Processing Systems
 DSD: dense-sparse-densetraining for deep neural networks,2017, In 5th International Conference on Learning Representations
 Second order derivatives for network pruning: optimal brainsurgeon,1992, In Proceedings of the 5th International Conference on Neural Information ProcessingSystems
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Simple and effective regularization methods for training onnoisily labeled data with generalization guarantee,2020, In 8th International Conference on LearningRepresentations
 Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels,2018, In International Conferenceon Machine Learning
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, In 5thInternational Conference on Learning Representations
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Snip: single-shot network pruningbased on connection sensitivity,2019, In 7th International Conference on Learning Representations
 A signal propa-gation perspective for pruning neural networks at initialization,2020, In 8th International Conferenceon Learning Representations
 Visualizing the loss land-scape of neural nets,2018, In Advances in Neural Information Processing Systems 31: Annual Con-ference on Neural Information Processing Systems 2018
 Learning to learn from noisy la-beled data,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition
 Dividemix: Learning with noisy labels assemi-supervised learning,2020, In 8th International Conference on Learning Representations
 Gradient descent with early stopping isprovably robust to label noise for overparameterized neural networks,2020, In International conferenceon artificial intelligence and statistics
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEinternational conference on computer vision
 Rethinking the value ofnetwork pruning,2019, In 7th International Conference on Learning Representations
 Learning sparse neural networks throughl_0 regularization,2018, In 6th International Conference on Learning Representations
 Variational dropout sparsifies deepneural networks,2017, In Doina Precup and Yee Whye Teh (eds
 Pruning convolutionalneural networks for resource efficient inference,2017, In 5th International Conference on LearningRepresentations
 Deepdouble descent: Where bigger models and more data hurt,2020, In 8th International Conferenceon Learning Representations
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In ICLR (Workshop)
 Confident learning: Estimating uncertainty in datasetlabels,2021, Journal of Artificial Intelligence Research
 Pervasive label errors in test sets destabilizemachine learning benchmarks,2021, CoRR
 Evaluating machine accuracy on ImageNet,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Learning from noisylabels with deep neural networks: A survey,2020, arXiv preprint arXiv:2007
 Learning from noisy labels by regularized estimation of annotator confusion,2019, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Understandingdeep learning requires rethinking generalization,2017, In 5th International Conference on LearningRepresentations
