title,year,conference
 A transformer-based ap-proach for source code summarization,2020, In Proceedings of the 58th Annual Meeting of the Associ-ation for Computational Linguistics
 Unified pre-training forprogram understanding and generation,2021, In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-nologies
 Learning to represent programswith graphs,2018, In International Conference on Learning Representations
 Code2vec: Learning distributed rep-resentations of code,2019, Proc
 Self-supervised contrastive learning for code retrievaland summarization via semantic-preserving transformations,9781, SIGIR ’21
 Infercode: Self-supervised learning of code represen-tations by predicting subtrees,2021, In 2021 IEEE/ACM 43rd International Conference on SoftwareEngineering (ICSE)
 Codit: Code editing with tree-based neuralmodels,2020, IEEE Transactions on Software Engineering
 Deep learning basedvulnerability detection: Are we there yet,2021, IEEE Transactions on Software Engineering
 Smote: Syn-thetic minority over-sampling technique,2002, J
 A simple framework forContrastive learning of visual representations,2020, In Hal Daume In and Aarti Singh (eds
 BERT: Pre-training of deepbidirectional transformers for language understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Hoppity: Learninggraph transformations to detect and fix bugs in programs,2020, In International Conference on Learn-ing Representations
 Patching astranslation: the data and the metaphor,2020, In 35th IEEE/ACM International Conference on AutomatedSoftware Engineering
 CodeBERT: A pre-trained model for programmingand natural languages,2020, In Findings of the Association for Computational Linguistics: EMNLP2020
 SimCSE: Simple contrastive learning of sentenceembeddings,2021, In Empirical Methods in Natural Language Processing (EMNLP)
 DeCLUTR: Deep contrastive learningfor unsupervised textual representations,2021, In Proceedings of the 59th Annual Meeting of the As-sociation for Computational Linguistics and the 11th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers)
 Globalrelational models of source code,2020, In International Conference on Learning Representations
 Code-searchnet challenge: Evaluating the state of semantic code search,2019, CoRR
 Contrastivecode representation learning,2020, arXiv preprint
 Treebert: A tree-based pre-trainedmodel for programming language,2021, ArXiv
 Learning and evaluatingcontextual embedding of source code,2020, In Proceedings of the 37th International Conference onMachine Learning
 SentencePiece: A simple and language independent sub-word tokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Con-ference on Empirical Methods in Natural Language Processing: System Demonstrations
 Vuldeepecker: A deep learning-based system for vulnerability detection,2018, In Proceedingsof the 25th Annual Network and Distributed System Security Symposium (NDSS‘2018)
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Codexglue: A machine learning benchmark dataset for code understandingand generation,2021, CoRR
 Convolutional neural networks over tree struc-tures for programming language processing,2016, In Proceedings of the Thirtieth AAAI Conference onArtificial Intelligence
 Trex: Learning executionsemantics from micro-traces for binary similarity,2020, CoRR
 Contrastive learningwith hard negative samples,2021, In International Conference on Learning Representations
 DOBF: A deob-fuscation pre-training objective for programming languages,2021, CoRR
 A detection framework for semantic codeclones and obfuscated code,2018, Expert Systems with Applications
 Attention is all you need,2017, In Proceedings of the 31st Inter-national Conference on Neural Information Processing Systems
 CLEAR: contrastivelearning for sentence representation,2020, CoRR
 Devign: Effective vulnera-bility identification by learning comprehensive program semantics via graph neural networks,2019, InAdvances in Neural Information Processing Systems
