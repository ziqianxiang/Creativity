title,year,conference
 Better fine-tuning by reducing representational collapse,2020, In International Conference onLearning Representations
 Learning with pseudo-ensembles,2014, Advances inneural information processing Systems
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Language models not just for pre-training: Fast online neural noisy channel modeling,2020, In Proceedings of the Fifth Conference onMachine Translation
 Semi-supervised sequencemodeling with cross-view training,2018, In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Randaugment: Practical dataaugmentation with no separate search,2019, arXiv preprint arXiv:1909
 An imageis worth 16x16 words: Transformers for image recognition at scale,2020, In International Conferenceon Learning Representations
 Reducing transformer depth on demand withstructured dropout,2019, In International Conference on Learning Representations
 Soft contextual data augmentation for neural machine translation,2019, In Proceedings of the 57thAnnual Meeting of the Association for Computational Linguistics
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Deep networks withstochastic depth,2016, In European conference on computer vision
 Smart:Robust and efficient fine-tuning for pre-trained natural language models through principled regular-ized optimization,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, Technical report
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 R-drop: Regularized dropout for neural networks,2021, arXiv preprint arXiv:2106
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Dropout withexpectation-linear regularization,2016, arXiv preprint arXiv:1609
 Gaussian process training with input noise,2011, Advances inNeural Information Processing Systems
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 Wavenet: A generative model for rawaudio,2016, arXiv preprint arXiv:1609
 Scaling neural machine translation,2018, InProceedings of the Third Conference on Machine Translation: Research Papers
 Semi-supervised semantic segmentation withcross-consistency training,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 A call for clarity in reporting bleu scores,2018, In Proceedings of the Third Conference onMachine Translation: Research Papers
 Adversarialtraining with cycle consistency for unsupervised super-resolution in endomicroscopy,2019, Medicalimage analysis
 Regularization with stochastic transfor-mations and perturbations for deep semi-supervised learning,2016, Advances in neural informationprocessing systems
 Improving neural machine translation mod-els with monolingual data,2016, In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Neural machine translation of rare words withsubword units,2016, In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers)
 A simple but tough-to-beat data augmentation approach for natural language understanding and generation,2020, arXivpreprint arXiv:2009
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, Advances in Neural Information ProcessingSystems
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Interpolationconsistency training for semi-supervised learning,2019, In International Joint Conference on ArtificialIntelligence
 Glue:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP
 Tacotron: Towards end-to-end speechsynthesis,2017, arXiv preprint arXiv:1703
 Unsupervised data augmentationfor consistency training,2020, Advances in Neural Information Processing Systems
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Incorporating bert into neural machine translation,2019, In International Conference on LearningRepresentations
 Fraternal dropout,2018, In Interna-tional Conference on Learning Representations
