title,year,conference
 Binarybert: Pushing the limit of bert quantization,2020, arXiv preprint arXiv:2012
 Auto-split: A general framework of collaborative edge-cloud ai,2021, In Proceedings of the 27th ACMSIGKDD Conference on Knowledge Discovery & Data Mining
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 The lottery ticket hypothesis for pre-trained bert networks,2020, Advances in neuralinformation processing systems
 A survey of model compression and acceleration fordeep neural networks,2017, arXiv:1710
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In NAACL-HLT (1)
 Reducing transformer depth on demand withstructured dropout,2019, In International Conference on Learning Representations
 Compressing bert: Studying the effects ofweight pruning on transfer learning,2020, ACL 2020
 Compression of deep learning models for text: A survey,2020, arXivpreprint arXiv:2008
 Compression of deeplearning models for nlp,2020, In Proceedings of the 29th ACM International Conference on Information& Knowledge Management
 Deberta: Decoding-enhanced bertwith disentangled attention,2021, International Conference on Learning Representations
 Distilling the knowledge in a neural network,2015, ArXiv
 Dynabert: Dynamic bertwith adaptive width and depth,2020, Advances in Neural Information Processing Systems
 Ghostbert: Generatemore features with cheap operations for bert,2021, ACL/IJCNLP
 Tinybert: Distilling bert for natural language understanding,2020, In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP)
 Improving task-agnostic bert distillation with layer mapping search,2021, Neuro-computing
 Kdlsq-bert: A quantizedbert combining knowledge distillation with learned step size quantization,2021, arXiv preprintarXiv:2101
 I-bert: Integer-only bert quantization,2021, arXiv preprint arXiv:2101
 A tutorial on energy-basedlearning,2006, Predicting structured data
 An energy andgpu-computation efficient backbone network for real-time object detection,2019, In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
 Gshard: Scaling giant models with conditionalcomputation and automatic sharding,2021, International Conference on Learning Representations
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Q-bert: Hessian based ultra low precision quantization of bert,2020, In Proceedingsof the AAAI Conference on Artificial Intelligence
 Reasonet: Learning to stop readingin machine comprehension,2017, In Proceedings of the 23rd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining
 Mobilebert:a compact task-agnostic bert for resource-limited devices,2020, In Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics
 Superglue: a stickier benchmark for general-purpose language un-derstanding systems,2019, In Proceedings of the 33rd International Conference on Neural InformationProcessing Systems
 Deebert: Dynamic early exitingfor accelerating bert inference,2020, In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics
 Q8bert: Quantized 8bit bert,2019, arXivpreprint arXiv:1910
 Ternary-bert: Distillation-aware ultra-low bit bert,2020, In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP)
