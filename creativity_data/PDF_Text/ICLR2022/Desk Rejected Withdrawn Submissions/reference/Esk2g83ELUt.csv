title,year,conference
 Revisiting resnets: Improved training and scaling strategies,2021, arXivpreprint arXiv:2103
 High-performance large-scaleimage recognition without normalization,2021, arXiv preprint arXiv:2102
 XcePtion: Deep learning with depthwise separable convolutions,2017, In Proceedingsofthe IEEE conference on computer vision and pattern recognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Container:Context aggregation network,2021, arXiv preprint arXiv:2106
 Lip: Local importance-based pooling,2019, In Proceedingsof the IEEE/CVF International Conference on Computer Vision
 Microsoft coco: Common objects in context,2014, In Europeanconference on computer vision
 Fnas:Uncertainty-aware fast neural architecture search,2021, arXiv preprint arXiv:2105
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 Designingnetwork design spaces,2020, In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition
 Detail-preserving pooling in deepnetworks,2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Proximal policyoptimization algorithms,2017, arXiv preprint arXiv:1707
 Going deeper with convolutions,2015, InProceedings ofthe IEEE conference on computer vision and pattern recognition
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Efficientnetv2: Smaller models and faster training,2021, arXiv preprintarXiv:2104
 Mnasnet: Platform-aware neural architecture search for mobile,2019, In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Resmlp: Feedforwardnetworks for image classification with data-efficient training,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Carafe++: Uni-fied content-aware reassembly of features,2021, IEEE Transactions on Pattern Analysis and MachineIntelligence
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, arXiv preprint arXiv:2102
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Earlyconvolutions help transformers see better,2021, arXiv preprint arXiv:2106
 Incorporating con-volution designs into visual transformers,2021, arXiv preprint arXiv:2103
 Making convolutional networks shift-invariant again,2019, In ICML
