title,year,conference
 Net-trim: convex pruning of deepneural networks with performance guarantee,2017, In Proceedings of the 31st International Conferenceon Neural Information Processing Systems (NeurIPS)
 On second-order group influence functions forblack-box predictions,2020, In Proceedings of the 37th International Conference on Machine Learning(ICML)
 Gradient-based optimization of hyperparameters,2000, Neural computation
 Under-standing the origins of bias in word embeddings,2019, In Proceedings of the 36th International Con-ference on Machine Learning (ICML)
 Kernel feature selectionvia conditional covariance minimization,2017, In Advances in Neural Information Processing Systems(NeurIPS)
 Characterizations of an empirical influence function fordetecting influential cases in regression,1980, Technometrics
 Residuals and influence in regression,1982, New York: Chapmanand Hall
 Learning to prune deep neural networks via layer-wise optimal brain surgeon,2017, In Proceedings of the 31st International Conference on Neural In-formation Processing Systems (NeurIPS)
 Learning both weights and connectionsfor efficient neural networks,2015, In Proceedings of the 28th International Conference on NeuralInformation Processing Systems (NeurIPS)
 Second order derivatives for network pruning: Optimal brainsurgeon,1993, In Advances in Neural Information Processing Systems (NeurIPS)
 Deep residual learning for image recog-nition,2016, In Proceedings of the 29th IEEE conference on computer vision and pattern recognition(CVPR)
 Interpretation of prediction models using the input gradient,2016, In Advances inNeural Information Processing Systems (NeurIPS)
 A new wrapper feature selectionapproach using neural network,2010, Neurocomputing
 Understanding black-box predictions via influence functions,2017, InProceedings of the 34th International Conference on Machine Learning (ICML)
 On the accuracy of influencefunctions for measuring group effects,2019, arXiv preprint arXiv:1905
 Optimal brain damage,1990, In Advances in NeuralInformation Processing Systems (NeurIPS)
 Lassonet: A neural networkwith feature sparsity,2021, Journal of Machine Learning Research (JMLR)
 Lassonet: Neural networks with feature spar-sity,2021, In Proceedings of The 24th International Conference on Artificial Intelligence and Statistics(AISTATS)
 Optimizing millions of hyperparameters byimplicit differentiation,2020, In International Conference on Artificial Intelligence and Statistics
 Gradient-based hyperparameter optimiza-tion through reversible learning,2015, In Proceedings of the 32nd International Conference on MachineLearning (ICML)
 Fast exact multiplication by the hessian,1994, Neural computation
 Pruning algorithms-a survey,1993, IEEE transactions on Neural Networks
 ” why should i trust you?” explaining thepredictions of any classifier,2016, In Proceedings of the 22nd ACM SIGKDD international conferenceon knowledge discovery and data mining
 Feature selection using deep neuralnetworks,2015, In 2015 International Joint Conference on Neural Networks (IJCNN)
 Can you trust this prediction? auditing pointwise reliability afterlearning,2019, In Proceedings of the 22nd International Conference on Artificial Intelligence andStatistics (ICAIS)
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Feature importance estimation Withself-attention networks,2020, In Proceedings of 24th European Conference on Artificial Intelligence(ECAI)
 Supervised featureselection via dependence estimation,2007, In Proceedings of the 24th International Conference onMachine Learning (ICML)
 Sparsifying neural netWork connections for face recog-nition,2016, In Proceedings of the 29th IEEE Conference on Computer Vision and Pattern Recognition(CVPR)
 Axiomatic attribution for deep netWorks,2017, InProceedings of the 34th International Conference on Machine Learning (ICML)
 Regression shrinkage and selection via the lasso,1996, Journal of the Royal StatisticalSociety: Series B (Methodological)
 Feature selection With neural netWorks,2002, Pattern recogni-tion letters
