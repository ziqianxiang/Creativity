title,year,conference
 On thedangers of stochastic parrots: Can language models be too big? In Proc,2021, ACM Conf
 Identifying and reducing gender bias in word-level languagemodels,2019, In Proc
 Language Models are Few-Shot Learners,2005, arXiv e-prints
 Inference time style control for summarization,2021, In Proc
 Deep Extrapolation for Attribute-EnhancedGeneration,2021, arXiv e-prints
 Cocon: A self-supervised ap-proach for controlled text generation,2021, In Proc
 Your gan is secretly an energy-based model and you should use discriminatordriven latent sampling,2020, In Proc
 Wordcraft: a Human-AICollaborative Editor for Story Writing,2021, arXiv e-prints
 Plug and play language models: A simple approach to controlled textgeneration,2020, In Proc
 Hierarchical neural story generation,2018, arXiv e-prints
 RealToxici-tyPrompts: Evaluating neural toxic degeneration in language models,2020, In Proc
 Hafez: an interactive poetrygeneration system,2017, In Proc
 Your classifier is secretly an energy based model and you should treat it likeone,2020, In Proc
 Don¡¯t stop pretraining: Adapt language models to domains and tasks,2020, InProc
 Multi-pair text style transfer for unbalanced data via task-adaptivemeta-learning,2021, In Proc
 The curious case of neural textdegeneration,2020, In Proc
 A distributional approach to controlledtext generation,2021, In Proc
 OpenNMT:Open-Source Toolkit for Neural Machine Translation,1701, arXiv e-prints
 GeDi: Generative Discriminator Guided Sequence Genera-tion,2020, arXiv e-prints
 Controlled Text Generationas Continuous Optimization with Multiple Constraints,2108, arXiv e-prints
 Interpreting text classifiers by learning context-sensitive influence of words,2021, In Proc
 Linguistics Human Lang,2016, Technol
 Towards understand-ing and mitigating social biases in language models,2021, In Proc
 Plug-and-blend: A framework for controllable story generation withblended control codes,2021, In Proc
 Modulating language models withemotions,2021, In Proc
 Recurrentneural network based language model,2010, In Proc
 Exten-sions of recurrent neural network language model,2011, In Proc
 A deep reinforced model for abstractivesummarization,2018, In Proc
 Towards controllable storygeneration,2018, In Proc
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, J
 ¡±why should i trust you?¡±: Explaining thepredictions of any classifier,2016, In Proc
 Tailor: Generatingand Perturbing Text with Semantic Controls,2021, arXiv e-prints
 Interpreting neural networks with nearest neigh-bors,2018, In Proc
 Chinesepoetry generation with planning based neural network,2016, In Proc
 Bayesian learning via stochastic gradient langevin dynamics,2011, InProc
 Simple statistical gradient-following algorithms for connectionist reinforcementlearning,1992, Mach
 Change or not: A simpleapproach for plug and play language models on sentiment control,2021, Proc
 North Amer,2021, Chapter Assoc
 Towards automatic generation of product reviews from aspect-sentiment scores,2017, In Proc
 Fine-Tuning Language Models from Human Preferences,2019, arXive-prints
