title,year,conference
 ETC: Encoding long and structuredinputs in transformers,2020, In Proc
 Adaptive input representations for neural language modeling,2019, InProc
 Neural machine translation by jointlylearning to align and translate,2015, In Proc
 Report onthe 11th IWSLT evaluation camPaign,2014, In Proc
 Rethinking attention with Performers,2021, In Proc
 Reducing transformer dePth on demand withstructured droPout,2020, In Proc
 OPenwebtext corPus,2019, http://Skylion007
 Coordination among neuralmodules through a shared global worksPace,2021, arXiv:2103
 Neural turing machines,2014, arXiv:1410
 Learning totransduce with unbounded memory,2015, In Proc
 Long short-term memory,1997, Neural Computation
 Inferring algorithmic patterns with stack-augmented recurrentnets,2015, In Proc
 High accuracy protein structure predictionusing deep learning,2021, Nature
 Transformers arernns: Fast autoregressive transformers with linear attention,2020, In Proc
 Moses: Open source toolkit for statistical machinetranslation,2007, In Proc
 Self-attentive associative memory,2020, In Proc
 Settransformer: A framework for attention-based permutation-invariant neural networks,2019, In Proc
 Simple recurrent units for highlyparallelizable recurrence,2018, In Proc
 Generating wikipedia by summarizing long sequences,2018, In Proc
 RoBERTa: A robustly optimized BERT pretrainingapproach,2019, arXiv: 1907
 Luna: Linear unified nested attention,2021, arXiv:2016
 Pointer sentinel mixturemodels,2017, In Proc
 Are sixteen heads really better than one? InH,2019, Wallach
 Document-level neuralmachine translation with hierarchical attention networks,2018, In Proc
 News dataset available,2016, https://commoncrawl
 Image transformer,2018, In Proc
 Rational recurrences,2018, In Proc
 In Proc,2021, of ICLR
 A call for clarity in reporting BLEU scores,2018, In Proc
 In Proc,2020, of ICLR
 Efficient content-basedsparse attention with routing transformers,2020, arXiv: 2003
 Linear transformers are secretly fast weightprogrammers,2021, In Proc
 Neural machine translation of rare words withsubword units,2016, In Proc
 Efficient transformers: A survey,2020, arXiv:2009
 A simple method for commonsense reasoning,2018, arXiv:1806
 Attention is all you need,2017, In Proc
 Fast transformers with clusteredattention,2020, arXiv: 2007
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proc
 Cluster-former: Clustering-based sparse transformer for long-range dependencyencoding,2020, arXiv:2009
 Linformer: Self-attentionwith linear complexity,2020, arXiv: 2006
 Memory networks,2015, In Proc
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proc
 Hierarchicalattention networks for document classification,2016, In Proc
 Memory architectures in recurrent neural network language models,2018, In Proc
 Hard-coded Gaussian attention for neural machinetranslation,2020, In Proc
 Big bird: Transformers forlonger sequences,2020, arXiv: 2007
 Defending against neural fake news,2019, In Proc
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In Proc
