title,year,conference
 Federated learning based on dynamic regularization,2021, In InternationalConference on Learning Representations
 Wireless federated distillation for distributededge learning with heterogeneous data,2019, In 2019 IEEE 30th Annual International Symposium onPersonal
 Exploiting unlabeleddata in smart cities using federated learning,2020, arXiv preprint arXiv:2001
 Qsgd: Communication-efficient sgd via gradient quantization and encoding,2017, In Advances in Neural Information ProcessingSystems 
 Fast learning rates for plug-in classifiers under themargin condition,2005, arXiv preprint math/0507180
 Layer normalization,2016, arXiv preprintarXiv:1607
 Learning with pseudo-ensembles,2014, arXiv preprintarXiv:1412
 Remixmatch: Semi-supervised learning with distribution alignment and augmentationanchoring,2019, arXiv preprint arXiv:1911
 Towardsfederated learning at scale: System design,2019, arXiv preprint arXiv:1902
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops
 Good semi-supervised learning that requires a bad gan,2017, arXiv preprint arXiv:1705
 HeteroFL: Computation and communication efficient feder-ated learning for heterogeneous clients,2021, In International Conference on Learning Representations
 Semi-supervised learning by entropy minimization,2005, In CAP
 Federated learning for mobilekeyboard prediction,2018, arXiv preprint arXiv:1811
 Fedml: A research library andbenchmark for federated machine learning,2020, arXiv preprint arXiv:2007
 The non-iid data quagmire ofdecentralized machine learning,4387, In International Conference on Machine Learning
 Measuring the effects of non-identical datadistribution for federated visual classification,2019, arXiv preprint arXiv:1909
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, arXiv preprint arXiv:1502
 Distillation-based semi-supervised federated learning for communication-efficient collaborative training withnon-iid private data,2020, arXiv preprint arXiv:2008
 Communication-efficient distributed sgd with sketching,2019, In Advances in Neural Information Processing Systems
 Federated semi-supervisedlearning with inter-client consistency & disjoint learning,2020, arXiv preprint arXiv:2006
 Improving federated learningpersonalization via model agnostic meta learning,2019, arXiv preprint arXiv:1909
 Towards utilizing unlabeled data in federatedlearning: A survey and prospective,2020, arXiv e-prints
 Adaptive gradient-based meta-learning methods,2019, In Advances in Neural Information Processing Systems
 On the rate of convergence of local averaging plug-in classifica-tion rules under a margin condition,2007, IEEE transactions on information theory
 Federated learning: Strategies for improving communication efficiency,2016, arXivpreprint arXiv:1610
 Learning multiple layers of features from tiny images,2009, 2009
 Temporal ensembling for semi-supervised learning,2016, arXiv preprintarXiv:1610
 Pseudo-label: The simple and efficient semi-supervised learning method fordeep neural networks,2013, In Workshop on challenges in representation learning
 Lotteryfl:Personalized and communication-efficient federated learning with lottery ticket hypothesis onnon-iid datasets,2020, arXiv preprint arXiv:2008
 Fedmd: Heterogenous federated learning via model distillation,2019, arXivpreprint arXiv:1910
 Fedbn: Federated learningon non-iid features via local batch normalization,2021, arXiv preprint arXiv:2102
 Federated learning in mobile edge networks: Acomprehensive survey,2020, IEEE Communications Surveys & Tutorials
 Fedsemi: An adaptive federated semi-supervised learning framework,2020, arXivpreprint arXiv:2012
 Sgdr: Stochastic gradient descent with warm restarts,2016, arXivpreprint arXiv:1608
 Three approaches forpersonalization with applications to federated learning,2020, arXiv preprint arXiv:2002
 Virtual adversarial training: aregularization method for supervised and semi-supervised learning,2018, IEEE transactions on patternanalysis and machine intelligence
 On estimating regression,1964, Theory of Probability & Its Applications
 Readingdigits in natural images with unsupervised feature learning,2011, 2011
 Client selection for federated learning with heterogeneousresources in mobile edge,2019, In ICC 2019-2019 IEEE International Conference on Communications(ICC)
 Semi-supervisedlearning with ladder networks,2015, arXiv preprint arXiv:1507
 Semi-supervised self-training of objectdetection models,2005, 2005
 Regularization with stochastic transfor-mations and perturbations for deep semi-supervised learning,2016, arXiv preprint arXiv:1606
 Communication-efficientfederated distillation,2020, arXiv preprint arXiv:2012
 Probability of error of some adaptive pattern-recognition machines,1965, IEEE Transac-tions on Information Theory
 Federated multi-tasklearning,2017, In Advances in Neural Information Processing Systems
 Fixmatch: Simplifying semi-supervised learning withconsistency and confidence,2020, arXiv preprint arXiv:2001
 A simplesemi-supervised learning framework for object detection,2020, arXiv preprint arXiv:2005
 Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results,2017, arXiv preprint arXiv:1703
 Instance normalization: The missingingredient for fast stylization,2016, arXiv preprint arXiv:1607
 Slowmo: Improvingcommunication-efficient distributed sgd with slow momentum,2019, arXiv preprint arXiv:1910
 Federated evaluation of on-device personalization,2019, arXiv preprint arXiv:1910
 Smooth regression analysis,1964, Sankhyaï¼š The Indian Journal OfStatiStics
 Theoretical analysis of self-training withdeep networks on unlabeled data,2020, arXiv preprint arXiv:2010
 Theoretical analysis of self-training withdeep networks on unlabeled data,2021, In International Conference on Learning Representations
 Group normalization,2018, In Proceedings of the European conference oncomputer vision (ECCV)
 Assisted learning: A framework formulti-organization learning,2020, Advances in Neural Information Processing Systems
 Unsupervised dataaugmentation for consistency training,2019, arXiv preprint arXiv:1904
 Self-training with noisy studentimproves imagenet classification,2020, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Fedmix: Approximation of mixup undermean augmented federated learning,2021, In International Conference on Learning Representations
 Wide residual networks,2016, arXiv preprint arXiv:1605
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
 Improving semi-supervised federated learning by reducing the gradient diversity ofmodels,2020, arXiv preprint arXiv:2008
 Semi-supervisedfederated learning for activity recognition,2020, arXiv preprint arXiv:2011
 Tri-training: Exploiting unlabeled data using three classifiers,2005, IEEETransactions on knowledge and Data Engineering
 Unsupervised domain adaptation forsemantic segmentation via class-balanced self-training,2018, In Proceedings of the European conferenceon computer vision (ECCV)
