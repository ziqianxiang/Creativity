title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Lambdanetworks: Modeling long-range interactions without attention,2021, In InternationalConference on Learning Representations
 Generating long sequences with sparsetransformers,2019, CoRR
 Transformer-xl:Attentive language models beyond a fixed-length context,2019, ArXiv
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Long short-term memory,1997, Neural computation
 Interlaced sparseself-attention for semantic segmentation,2019, ArXiv
 Ccnet:Criss-cross attention for semantic segmentation,2019, 2019 IEEE/CVF International Conference onComputer Vision (ICCV)
 Transformers are rnns: Fast au-toregressive transformers with linear attention,2020, In Proceedings of the International Con-ference on Machine Learning (ICML)
 Reformer: The efficient transformer,2020, ArXiv
 Image transformer,2018, arXiv preprint arXiv:1802
 Compressive transformersfor long-range sequence modelling,2020, ArXiv
 Stand-alone self-attention in vision models,2019, ArXiv
 Efficient content-based sparse attentionwith routing transformers,2020, ArXiv
 Adaptive attention span intransformers,2019, In ACL
 Sparse sinkhorn attention,2020, ArXiv
 Training data-effiCient image transformers & distillation through attention,2020, arXivpreprint arXiv:2012
 Attention is all you need,2017, In Advances in neural informationprocessing Systems
 Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,2020, ArXiv
 Linformer: Self-attentionwith linear complexity,2020, ArXiv
 Pay less attention withlightweight and dynamic convolutions,2019, ArXiv
 Big bird: Transformers forlonger sequences,2020, ArXiv
 Asymmetric non-local neuralnetworks for semantic segmentation,2019, 2019 IEEE/CVF International Conference on ComputerVision (ICCV)
