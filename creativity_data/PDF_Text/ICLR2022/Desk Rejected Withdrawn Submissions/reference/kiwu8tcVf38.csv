title,year,conference
 Katyusha: The first direct acceleration of stochastic gradient methods,2017, JournalofMachine Learning Research
 Natasha 2: Faster non-convex optimization than SGD,2018, In Advances in NeuralInformation Processing Systems
 Nonlinear programming,2016, Athena scientific Belmont
 Large-scale machine learning with stochastic gradient descent,2010, In COMPSTAT'2010
 Optimization methods for large-scale machinelearning,2018, Siam Review
 Accelerated linear convergence of stochasticmomentum methods in Wasserstein distances,2019, In International Conference on Machine Learning
 Momentum via primal averaging: Theoretical insights and learning rate schedulesfor non-convex optimization,2020, Technical report
 On the ineffectiveness of variance reduced optimization for deeplearning,2019, In Advances in Neural Information Processing Systems
 SAGA: A fast incremental gradientmethod with support for non-strongly convex composite objectives,2014, In Advances in neural in-formation processing systems
 SPIDER: near-optimal non-convex op-timization via stochastic path integrated differential estimator,2018, In Advances in Neural InformationProcessing Systems
 Stochastic heavy ball,2018, Electronic Journalof Statistics
 Accelerating stochastic gradient descent using predictive variancereduction,2013, In Advances in neural information processing systems
 Contextual augmentation: Data augmentation by words with paradigmatic re-lations,2018, In Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies
 Learning multiple layers of features from tiny images,2009, Technical report
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Manifold identification in dual averaging for regularizedstochastic online learning,2012, Journal of Machine Learning Research
 Towards deeper understanding of nonconvexstochastic optimization with momentum using diffusion approximations,2018, Technical report
 An improved analysis of stochastic gradient descent withmomentum,2020, Technical report
 Inexact SARAH algorithm for stochasticoptimization,2021, Optimization Methods and Software
 Stochastic proximal gradient descent with acceleration techniques,2014, In Advances inNeural Information Processing Systems
 ProxSARAH: An efficientalgorithmic framework for stochastic composite nonconvex optimization,2020, Journal of MachineLearning Research
 Some methods of speeding up the convergence of iteration methods,1964, Ussr compu-tational mathematics and mathematical physics
 Minimizing finite sums with the stochasticaverage gradient,2017, Mathematical Programming
 Pegasos: Primal estimatedsub-gradient solver for SVM,2011, Mathematical programming
 Very deep convolutional networks for large-scale imagerecognition,2015, 2015
 Cyclical learning rates for training neural networks,2017, In IEEE winter conference onapplications of computer vision
 Hybrid stochasticgradient descent algorithms for stochastic nonconvex optimization,2019, Technical report
 The art of data augmentation,2001, Journal of Computational andGraphical Statistics
 Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron,2019, In International Conference on ArtificialIntelligence and Statistics
 Escaping saddle points faster with stochasticmomentum,2020, In International Conference on Learning Representations
 Spiderboost and momentum:Faster variance reduction algorithms,2019, In Advances in Neural Information Processing Systems
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, Technical report
 A unified analysis of stochastic mo-mentum methods for deep learning,2018, In International Joint Conference on Artificial Intelligence
 On the influence of momentum acceleration on onlinelearning,2016, Journal of Machine Learning Research
 Directacceleration of saga using sampled negative momentum,2019, In International Conference on ArtificialIntelligence and Statistics
