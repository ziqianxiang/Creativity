title,year,conference
 Learning n:m fine-grained structuredsparse neural networks from scratch,2021, In International Conference on Learning Representations
 Language models are few-shot learners,2020, In Larochelle
 The lottery ticket hypothesis for pre-trained bert networks,2020, InLarochelle
 An image is worth 16x16 words: Transformers forimage recognition at scale,2020, arXiv preprint arXiv:2010
	Nvidia A100 tensor core gpu architecture,2020,	https://images
 Attention is all you need,2017, In Guyon
 GLUE: A multi-task benchmark and analysis platform fornatural language understanding,2018, In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:Analyzing and Interpreting Neural Networks for NLP
 Non-structured dnn weight pruning considered harmful,2019, ArXiv
 Transformers: State-of-the-art natural language processing,2020, In Pro-ceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: SystemDemonstrations
 Aligning books and movies: Towards story-like visual explanationsby watching movies and reading books,2015, In 2015 IEEE International Conference on ComputerVision (ICCV)
