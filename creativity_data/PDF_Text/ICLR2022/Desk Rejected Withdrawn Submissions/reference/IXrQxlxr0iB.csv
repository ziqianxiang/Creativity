title,year,conference
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Rethinking attention with performers,2021, In9th International Conference on Learning Representations
 ELECTRA: pre-trainingtext encoders as discriminators rather than generators,2020, In 8th International Conference on LearningRepresentations
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Hotflip: White-box adversarial examplesfor text classification,2018, In Iryna Gurevych and Yusuke Miyao (eds
 Explaining and harnessing adversarialexamples,2015, In Yoshua Bengio and Yann LeCun (eds
 Long document classification from localword glimpses via recurrent attention learning,2019, IEEE Access
 Is BERT really robust? A strong baselinefor natural language attack on text classification and entailment,2020, In The Thirty-Fourth AAAIConference on Artificial Intelligence
 Triviaqa: A large scale distantlysupervised challenge dataset for reading comprehension,1601, In Regina Barzilay and Min-Yen Kan(eds
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Reformer: The efficient transformer,2020, In8th International Conference on Learning Representations
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 ALBERT: A lite BERT for self-supervised learning of language representations,2019, CoRR
 Learn-ing long-range spatial dependencies with horizontal gated recurrent units,2018, In Samy Bengio
 Adversarial training for large neural language models,2020, CoRR
 Hierarchical transformers for multi-document summarization,2019, arXivpreprint arXiv:1905
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Learning word vectors for sentiment analysis,2011, In Dekang Lin
 Virtual adversarial training:A regularization method for supervised and semi-supervised learning,2019, IEEE Trans
 Blockwiseself-attention for long document understanding,2019, arXiv preprint arXiv:1911
 The ACLanthology network corpus,2013, Lang
 Exploring the limits of transfer learning with a unified text-to-text transformer,2020, J
 Hierarchical learning for generation with long sourcesequences,2021, arXiv preprint arXiv:2104
 Intriguing properties of neural networks,2014, In Yoshua Bengio and Yann LeCun(eds
 Synthesizer:Rethinking self-attention in transformer models,2020, CoRR
 Sparse sinkhorn attention,9438, InProceedings of the 37th International Conference on Machine Learning
 Long range arena: A benchmark for efficienttransformers,2020, arXiv preprint arXiv:2011
 Long range arena : A benchmark for efficienttransformers,2021, In 9th International Conference on Learning Representations
 A simple method for commonsense reasoning,2018, CoRR
 Attention is all you need,2017, In Isabelle Guyon
 Linformer: Self-attention withlinear complexity,2020, CoRR
 Constructing datasets for multi-hop readingcomprehension across documents,2018, Trans
 Hi-transformer: Hierarchical interactivetransformer for efficient and effective long document modeling,2021, arXiv preprint arXiv:2106
 Big bird: Transformers forlonger sequences,2020, Advances in Neural Information Processing Systems
 Defending against neural fake news,2019, In Hanna M
 Hibert: Document level pre-training of hierarchicalbidirectional transformers for document summarization,2019, arXiv preprint arXiv:1905
 Aligning books and movies: Towards story-like visual explanations by watchingmovies and reading books,2015, In 2015 IEEE International Conference on Computer Vision
