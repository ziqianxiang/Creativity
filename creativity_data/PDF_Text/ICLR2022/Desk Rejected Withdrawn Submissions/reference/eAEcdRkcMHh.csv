title,year,conference
 Binarybert: Pushing the limit of BERT quantization,2020, CoRR
 Distributed opti-mization and statistical learning via the alternating direction method of multipliers,2011, Found
 Language models are few-shot learners,2020, CoRR
 BERT: Pre-training of deepbidirectional transformers for language understanding,4171, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Compressing BERT: Studying the effectsof weight pruning on transfer learning,2020, In Proceedings of the 5th Workshop on RepresentationLearning for NLP
 Reweighted proximalpruning for large-scale language representation,2019, CoRR
 Reweighted proximalpruning for large-scale language representation,2019, CoRR
 Deepcompression and EIE: efficient inference engine on compressed deep neural network,2016, In 2016IEEE Hot Chips 28 Symposium (HCS)
 Quantization and training of neural networks for efficientinteger-arithmetic-only inference,2018, In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 I-BERT:integer-only BERT quantization,2021, In Marina Meila and Tong Zhang (eds
 Adam: A method for stochastic optimization,2015, In ICLR (Poster)
 Roberta: A robustly optimized BERT pretrainingapproach,2019, CoRR
 Hardware acceleration of fully quantized BERT for efficientnatural language processing,2021, In Design
 Ladabert: Lightweight adaptation of BERT through hybrid modelcompression,2020, In Donia Scott
 Are sixteen heads really better than one?In H,2019, Wallach
 Importance estimation forneural network pruning,2019, In IEEE Conference on Computer Vision and Pattern Recognition
 Stochastic alternating direction method ofmultipliers,2013, In Proceedings of the 30th International Conference on Machine Learning
 Languagemodels are unsupervised multitask learners,2019, 2019
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2019, CoRR
 Exploring the limits of transfer learning with a unifiedtext-to-text transformer,2020, Journal of Machine Learning Research
 Zero: Memory optimizationstoward training trillion parameter models,2019, ArXiv
 Admm-nn: An algorithm-hardware co-design framework of dnns using alternating directionmethods of multipliers,9781, In Proceedings of the Twenty-Fourth International Conference on Archi-tectural Support for Programming Languages and Operating Systems
 Q-BERT: hessian based ultra low precision quantization of BERT,2020, In The Thirty-Fourth AAAI Conference on Artificial Intelligence
 Mobilebert:a compact task-agnostic bert for resource-limited devices,2020, In ACL (2020)
 GLUE:A multi-task benchmark and analysis platform for natural language understanding,2018, In Proceedingsof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP
 Learning structured sparsity in deepneural networks,2016, In Daniel D
 Learning structured sparsity in deepneural networks,2016, In Daniel D
 Transformers: State-of-the-art naturallanguage processing,2020, In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations
 Progressive weight pruningof deep neural networks using ADMM,2018, CoRR
 ProgressiveDNN compression: A key to achieve ultra-high weight pruning and quantization rates usingADMM,2019, CoRR
 Learning n:m fine-grained structured sparse neural networks from scratch,2021, InInternational Conference on Learning Representations
