title,year,conference
 Finding approximatelocal minima faster than gradient descent,2017, In Proceedings of the 49th Annual ACM SIGACTSymposium on Theory ofComputing
 One network fits all? modular versus monolithic task formulations in neural networks,2021, InInternational Conference on Learning Representations
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 Sharp nonasymptotic bounds on the norm of randommatrices with independent entries,2016, Annals of Probability
 Deep learning of representations for unsupervised and transfer learning,2012, In Proceed-ings of ICML workshop on unsupervised and transfer learning
 Perturbation theory for the matrix square root and matrix modulus,2018, arXiv preprintarXiv:1810
 Toward deeper understanding of neural networks:The power of initialization and a dual view on expressivity,2016, In Advances In Neural InformationProcessing Systems
 The mnist database of handwritten digit images for machine learning research,2012, IEEE SignalProcessing Magazine
 Untangling invariant object recognition,2007, Trends in cognitivesciences
 Gradient descent finds globalminima of deep neural networks,2019, In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds
 Gradient descent Provably optimizesover-parameterized neural networks,2019, In 7th International Conference on Learning Representations
 Escaping from saddle points—online stochasticgradient for tensor decomposition,2015, In Conference on learning theory
 Matrix completion has no spurious local minimum,2016, Advancesin Neural Information Processing Systems
 Comments on “deep neural networks with random gaussian weights: A universalclassification strategy?”,2020, IEEE Transactions on Signal Processing
 Dimensionality reduction by learning an invariantmapping,2006, In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR’06)
 Neural collapse under mse loss: Proximity to anddynamics on the central path,2021, arXiv preprint arXiv:2106
 Uniform convergence of adaptive graph-based regularization,2006, In InternationalConference on Computational Learning Theory
 Intrinsic dimensionality estimation of submanifolds in rd,2005, InProceedings of the 22nd international conference on Machine learning
 Approximate nearest neighbors: towards removing the curse ofdimensionality,1998, In Proceedings of the thirtieth annual ACM symposium on Theory of computing
 Accelerated gradient descent escapes saddlepoints faster than gradient descent,2018, In Conference On Learning Theory
 Deep learning without poor local minima,2016, In D
 Supervised contrastive learning,2020, InH
 Learning multiple layers of features from tiny images,2009, N/A
 First-order methods almost always avoid strict saddle points,2019, Mathematical programming
 The benefit of multitaskrepresentation learning,2016, Journal of Machine Learning Research
 How to train your resnet,2018, https://myrtle
 Prevalence of neural collapse during the terminalphase of deep learning training,2020, Proceedings of the National Academy of Sciences
 Atheoretical analysis of contrastive unsupervised representation learning,2019, In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds
 Understanding machine learning: From theory toalgorithms,2014, Cambridge university press
 A global geometric framework fornonlinear dimensionality reduction,2000, science
 On the theory of transfer learning: The importanceof task diversity,2020, arXiv preprint arXiv:2006
 A survey on learning to hash,2017, IEEEtransactions on pattern analysis and machine intelligence
 A survey of transfer learning,2016, Journal ofBig data
 Leveraging sparse linear layers for debuggabledeep networks,2021, arXiv preprint arXiv:2105
