title,year,conference
"â€œlearning-compression"" algorithms for neural netpruning",2018, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 A back-propagation algorithm with optimal use of hidden units,1988, In NIPS
 Nest: A neural network synthesis tool based on agrow-and-prune paradigm,2019, IEEE Transactions on Computers
 Sparse networks from scratch: Faster training without losingperformance,2019, arXiv preprint arXiv:1907
 Rigging the lottery:Making all tickets winners,2019, CoRR
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 Linear modeconnectivity and the lottery ticket hypothesis,2019, CoRR
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Dynamic network surgery for efficient dnns,2016, arXivpreprint arXiv:1608
 Removal of hidden units and weights for back propagation networks,1993, In Proceedingsof 1993 International Conference on Neural Networks (IJCNN-93-Nagoya
 Learning both weights and connections forefficient neural networks,2015, arXiv preprint arXiv:1506
 Optimal brain surgeon and general networkpruning,1993, In IEEE international conference on neural networks
 Robust pruning at ini-tialization,2021, In International Conference on Learning Representations
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of the IEEE internationalconference on computer vision
 Sparsity indeep learning: Pruning and growth for efficient inference and training in neural networks,2021, arXivpreprint arXiv:2102
 Structural learning with forgetting,1996, Neural networks
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv preprint arXiv:1405
 A simple procedure for pruning back-propagation trained neural networks,1990, IEEEtransactions on neural networks
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Optimal brain damage,1990, In Advances in neuralinformation processing systems
 Snip: Single-shot network pruningbased on connection sensitivity,2018, arXiv preprint arXiv:1810
 A signal prop-agation perspective for pruning neural networks at initialization,2020, In International Confer-ence on Learning Representations
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Rethinking the value ofnetwork pruning,2018, arXiv preprint arXiv:1810
 Proving the lottery tickethypothesis: Pruning is all you need,2020, In International Conference on Machine Learning
 Pruning convolutionalneural networks for resource efficient inference,2016, arXiv preprint arXiv:1611
 Data-independent neural pruning via coresets,2019, arXiv preprint arXiv:1907
 Tensorizing neuralnetworks,2015, arXiv preprint arXiv:1509
 ZORB: A derivative-free backpropagation algorithmfor neural networks,2020, CoRR
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Trainingneural networks without gradients: A scalable admm approach,2016, In International conference onmachine learning
 Pruning via iterative ranking of sensitivity Statis-tics,2020, CoRR
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Generalization by weight-elimination with application to forecasting,1991, In Advances in neural information processing systems
 Learning structured sparsity indeep neural networks,2016, Advances in neural information processing systems
 Good subnet-works provably exist: Pruning via greedy forward selection,2020, CoRR
 Drawing early-bird tickets: Towards more efficient trainingof deep networks,2019, arXiv preprint arXiv:1909
