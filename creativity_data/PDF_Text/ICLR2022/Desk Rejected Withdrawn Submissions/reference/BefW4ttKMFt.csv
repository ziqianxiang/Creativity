title,year,conference
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 How to train your maml,2018, arXiv preprintarXiv:1810
 Mirror descent and nonlinear projected subgradient methods forconvex optimization,2003, Operations Research Letters
 Investigating meta-learning algorithms forlow-resource natural language understanding tasks,2019, In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP)
 Rl2 : Fastreinforcement learning via slow reinforcement learning,2016, arXiv preprint arXiv:1611
 Towards a neural statistician,2016, arXiv preprintarXiv:1606
 Meta-learning and universality: Deep representations and gradi-ent descent can approximate any learning algorithm,2018, In International Conference on LearningRepresentations
 Model-agnostic meta-learning for fast adaptationof deep networks,2017, In International Conference on Machine Learning
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,1050, In international conference on machine learning
 Recasting gradient-based meta-learning as hierarchical bayes,2018, In International Conference on Learning Representa-tions
 Few-shot objectdetection via feature reweighting,2019, In Proceedings of the IEEE/CVF International Conference onComputer Vision
 Siamese neural networks for one-shotimage recognition,2015, In ICML deep learning workshop
 A simple weight decay can improve generalization,1992, In Advancesin neural information processing Systems
 Human-level concept learningthrough probabilistic program induction,2015, Science
 Gradient-based meta-learning with learned layerwise metric andsubspace,2018, In International Conference on Machine Learning
 Learning to learn from noisy la-beled data,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition
 Meta-sgd: Learning to learn quickly for few-shot learning,2017, arXiv preprint arXiv:1707
 Personalizing dialogue agentsvia meta-learning,2019, In Proceedings of the 57th Annual Meeting of the Association for Computa-tional Linguistics
 A simple neural attentive meta-learner,2018, In International Conference on Learning Representations
 Meta networks,2017, In International Conference on MachineLearning
 Data augmentation formeta-learning,2021, In International Conference on Machine Learning
 Tadam: task dependent adaptive metricfor improved few-shot learning,2018, In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems
 Meta-learning requires meta-augmentation,2020, arXivpreprint arXiv:2007
 Optimization as a model for few-shot learning,2017, In ICLR
 Learning to reweight examples forrobust deep learning,2018, In International Conference on Machine Learning
 Pacoh: Bayes-optimalmeta-learning with pac-guarantees,2021, In International Conference on Machine Learning
 Meta-learning with memory-augmented neural networks,2016, In International conference on machine learn-ing
 Meta-weight-net: Learning an explicit mapping for sample weighting,2019, Advances in Neural InformationProcessing Systems
 Learning from noisylabels with deep neural networks: A survey,2020, arXiv preprint arXiv:2007
 Meta-transfer learning for few-shotlearning,2019, In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-tion
 Learning to learn,2012, Springer Science & Business Media
 Deep learning and the information bottleneck principle,2015, In2015 IEEE Information Theory Workshop (ITW)
 Matching networks for oneshot learning,2016, Advances in neural information processing systems
 Generalizing from a few examples:A survey on few-shot learning,2020, ACM Computing Surveys (CSUR)
 Meta-learningwithout memorization,2019, arXiv preprint arXiv:1912
 One-shot imitation from observing humans via domain-adaptive meta-learning,2018, arXivpreprint arXiv:1802
