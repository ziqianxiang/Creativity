title,year,conference
 The lottery tickets hypothesis for supervised and self-supervised pre-trainingin computer vision models,2020, arXiv preprint arXiv:2012
 The lottery ticket hypothesis for pre-trained bert networks,2020, arXiv preprintarXiv:2007
 Long live the lottery:The existence of winning tickets in lifelong learning,2021, In International Conference on LearningRepresentations
 Earlybert:Efficient bert training via early-bird lottery tickets,2020, arXiv preprint arXiv:2101
 Gans can play lottery tickets too,2021, InInternational Conference on Learning Representations
 Repvgg:Making vgg-style convnets great again,2021, In Proceedings of the IEEE/CVF Conference on Com-Puter Vision and Pattern Recognition
 Gradient flow in sparse neural net-works and how lottery tickets win,2020, arXiv preprint arXiv:2010
 Gradient flow in sparse neuralnetworks and how lottery tickets win,2020, ArXiv
 Stabilizing thelottery ticket hypothesis,2019, arXiv preprint arXiv:1903
 The state of sparsity in deep neural networks,2019, arXivpreprint arXiv:1902
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In AISTATS
 Learning both weights and connections forefficient neural network,2015, In Advances in neural information processing systems
 Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem,2019, InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Mobilenets: Efficient convolutional neural networks formobile vision applications,2017, arXiv preprint arXiv:1704
 Averaging weights leads to wider optima and better generalization,2018, arXiv preprintarXiv:1803
 Weight smoothing to improve network generalization,1994, IEEE Transac-tions on neural networks
 Winning lottery tickets in deep generativemodels,2020, arXiv preprint arXiv:2010
 Optimal brain damage,1990, In Advances in neuralinformation processing Systems
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Visualizing the loss land-scape of neural nets,2017, arXiv preprint arXiv:1712
 Rethinking the value ofnetwork pruning,2019, In International Conference on Learning Representations
 Goodstudents play big lottery better,2021, arXiv preprint arXiv:2101
 Mish: A self regularized non-monotonic neural activation function,2019, arXiv preprintarXiv:1908
 Searching for activation functions,2017, arXivpreprint arXiv:1710
 Comparing rewinding and fine-tuning in neuralnetwork pruning,2020, In International Conference on Learning Representations
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Rethink-ing the inception architecture for computer vision,2016, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Pruning neural networkswithout any data by iteratively conserving synaptic flow,2020, arXiv preprint arXiv:2006
 Keep the gradients flowing: Using gradientflow to study sparse network optimization,2021, arXiv preprint arXiv:2102
 Attention is all you need,2017, ArXiv
 Picking winning tickets before training bypreserving gradient flow,2020, arXiv preprint arXiv:2002
 Pyhessian: Neural networksthrough the lens of the hessian,2020, 2020 IEEE International Conference on Big Data (Big Data)
 Drawing early-bird tickets: Toward more efficient training ofdeep networks,2020, In International Conference on Learning Representations
 Playing the lottery with rewardsand multiple languages: lottery tickets in rl and nlp,2019, arXiv preprint arXiv:1906
 Revisiting knowledge distillationvia label smoothing regularization,2020, pp
 Revisiting knowledge distillationvia label smoothing regularization,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Gradinit: Learningto initialize neural networks for stable and efficient training,2021, ArXiv
 Gradinit:Learning to initialize neural networks for stable and efficient training,2021, CoRR
