title,year,conference
 Neural machine translation by jointlylearning to align and translate,2015, In Yoshua Bengio and Yann LeCun
 A differentiable BLEU loss,2018, analysis andfirst results
 Style transformer: Unpaired textstyle transfer without disentangled latent representation,2019, CoRR
 Multi30k: Multilingual english-german image descriptions,2016, CoRR
 Mask-predict: Paralleldecoding of conditional masked language models,2019, In Kentaro Inui
 Aligned crossentropy for non-autoregressive machine translation,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Levenshtein transformer,2019,	In H
 A probabilistic formulationof unsupervised text style transfer,2020, In 8th International Conference on Learning Representations
 Toward controlledgeneration of text,2017, In Doina Precup and Yee Whye Teh
 Categorical reparameterization with gumbel-softmax,2017, In 5thInternational Conference on Learning Representations
 Improved natural language generation via loss truncation,2020, InDan Jurafsky
 Deterministic non-autoregressive neural sequencemodeling by iterative refinement,2018, In Proceedings of the 2018 Conference on Empirical Methods inNatural Language Processing
 Deep re-inforcement learning for dialogue generation,1127, In Jian Su
 Hint-based trainingfor non-autoregressive machine translation,2019, In Kentaro Inui
 Improved image captioningvia policy gradient optimization of spider,2017, In IEEE International Conference on Computer Vision
 Effective approaches to attention-based neural machine translation,2015, In Empirical Methods in Natural Language Processing (EMNLP)
 Recur-rent neural network based language model,1045, In Takao Kobayashi
 Human-level control through deep re-inforcement learning,2015, Nat
 Noise isn’t alWays negative: Countering exposure bias inSeqUence-to-sequence inflection models,2020, In Donia Scott
 Transfer reWard learning for policy gradient-based textgeneration,2019, CoRR
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics
 Style transferthrough back-translation,1080, In Iryna Gurevych and Yusuke Miyao
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Sequence level trainingWith recurrent neural netWorks,2016, In Yoshua Bengio and Yann LeCun
 Self-criticalsequence training for image captioning,2017, In 2017 IEEE Conference on Computer Vision and PatternRecognition
 Get to the point: Summarization With pointer-generator netWorks,2017, In Regina Barzilay and Min-Yen Kan
 BLEURT: learning robust metrics for textgeneration,2020, CoRR
 Greedy search with probabilistic n-gram matching forneural machine translation,2018, CoRR
 Minimizing the bag-of-ngrams difference for non-autoregressive neural machine translation,2020, In The Thirty-Fourth AAAIConference on Artificial Intelligence
 Sequence-level trainingfor non-autoregressive neural machine translation,2021, CoRR
 Minimumrisk training for neural machine translation,2016, In Proceedings of the 54th Annual Meeting of theAssociation for Computational Linguistics
 Style transfer from non-parallel text by cross-alignment,2017, In Isabelle Guyon
 Latent space secrets ofdenoising text-autoencoders,2019, CoRR
 Minimum risk annealing for training log-linear models,2006, In NicolettaCalzolari
 Fast structured decodingfor sequence models,2019, In H
 Structured content preservation for unsupervised text styletransfer,2018, CoRR
 Attention is all you need,2017, In Isabelle Guyon
 RtGender: Acorpus for studying differential responses to gender,2018, In Proceedings of the Eleventh InternationalConference on Language Resources and Evaluation (LREC 2018)
 Non-autoregressivemachine translation with auxiliary regularization,2019, In The Thirty-Third AAAI Conference onArtificial Intelligence
 Symmetric crossentroPy for robust learning with noisy labels,2019, In 2019 IEEE/CVF International Conference onComputer Vision
 Beyond BLEU:trainingneural machine translation with semantic similarity,2019, In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics
 A study of reinforcement learningfor neural machine translation,2018, In Proceedings of the 2018 Conference on Empirical Methods inNatural Language Processing
 Google’s neuralmachine translation system: Bridging the gaP between human and machine translation,2016, CoRR
 L_dmi: A novel information-theoretic lossfunction for training deeP nets robust to label noise,2019, In Hanna M
 Generalized cross entroPy loss for training deeP neu-ral networks with noisy labels,2018, In Samy Bengio
 Differentiable lower bound for expected BLEU score,2017, CoRR
