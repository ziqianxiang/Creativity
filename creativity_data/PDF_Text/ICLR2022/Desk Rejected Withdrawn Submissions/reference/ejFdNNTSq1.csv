title,year,conference
 A large annotatedcorpus for learning natural language inference,2015, In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing
 Language models are few-shotlearners,2020, In Hugo Larochelle
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Deep residual learning for imagerecognition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition
 Long short-term memory,899, Neural Comput
 Language modeling with deeptransformers,2019, Interspeech 2019
 Rethinking positional encoding in language pre-training,2021, InInternational Conference on Learning Representations
 Adam: A method for stochastic optimization,2015, In YoshuaBengio and Yann LeCun (eds
 Reformer: The efficient transformer,2020, In8th International Conference on Learning Representations
 SentencePiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations
 Gradient-based learning applied to documentrecognition,1998, Proceedings of the IEEE
 Settransformer: A framework for attention-based permutation-invariant neural networks,3744, In KamalikaChaudhuri and Ruslan Salakhutdinov (eds
 Learning to encode positionfor transformer with continuous dynamical model,2020, In Proceedings of the 37th InternationalConference on Machine Learning
 Pointer sentinel mixturemodels,2017, In 5th International Conference on Learning Representations
 On the relation between position information and sen-tence length in neural machine translation,2019, In Proceedings of the 23rd Conference on Com-putational Natural Language Learning (CoNLL)
 Languagemodels are unsupervised multitask learners,2019,2019
 Learning associative inferenceusing fast weight memory,2021, In International Conference on Learning Representations
 Learning to control fast-weight memories: An alternative to dynamic recurrentnetworks,1992, Neural Computation
 Transformer dissection: An unified understanding for transformer¡¯s attention via the lensof kernel,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Attention is all you need,2017, In Isabelle Guyon
 Superglue: A stickier benchmark for general-purposelanguage understanding systems,2019, In Hanna M
 A broad-coverage challenge corpus forsentence understanding through inference,2018, In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-gies
