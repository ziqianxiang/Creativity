title,year,conference
 What it thinks is important is important: Robustnesstransfers through input gradients,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Adversarialrobustness: From self-supervised pre-training to fine-tuning,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In International Conference on Machine Learning
 Provable robustness of relu net-works via maximization of linear regions,2019, In the 22nd International Conference on ArtificialIntelligence and Statistics
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Adversarially robust distillation,2020, InProceedings of the AAAI Conference on Artificial Intelligence
 Knowledge distillation: Asurvey,2021, International Journal of Computer Vision
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Improved knowledge distillation via teacher assistant,2020, In Proceedings of the AAAIConference on Artificial Intelligence
 Intriguing properties of vision transformers,2021, arXiv preprintarXiv:2105
 Alp-kd: Attention-based layerprojection for knowledge distillation,2020, arXiv preprint arXiv:2012
 Vision transformers are robust learners,2021, arXiv preprintarXiv:2105
 Adversarial robustness through locallinearization,2019, arXiv preprint arXiv:1907
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Locally linearattributes of relu neural networks,2020, arXiv preprint arXiv:2012
 Adversarially robust transfer learning,2019, arXiv preprint arXiv:1905
 On the adversarial robust-ness of visual transformers,2021, arXiv preprint arXiv:2103
 Patient knowledge distillation for bert modelcompression,2019, arXiv preprint arXiv:1908
 Private modelcompression via knowledge distillation,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Onfast adversarial robustness adaptation in model-agnostic meta-learning,2021, In International Confer-ence on Learning Representations
 Knowledge distillation for fast andaccurate monocular depth estimation on mobile devices,2021, In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition
 Pytorch image models,2019, https://github
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer,2016, arXiv preprint arXiv:1612
