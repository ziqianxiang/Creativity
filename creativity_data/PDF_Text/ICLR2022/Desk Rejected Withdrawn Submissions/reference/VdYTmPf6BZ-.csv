title,year,conference
 Obfuscated gradients give a false sense ofsecurity: Circumventing defenses to adversarial examples,2018, In ICML
 Evasion attacks against machine learning at test time,2013, ArXiv
 Towards evaluating the robustness of neural networks,2017, 2017IEEE Symposium on Security and Privacy (SP)
 Cat: Customized adversarialtraining for improved robustness,2020, ArXiv
 Parsevalnetworks: Improving robustness to adversarial examples,2017, In ICML
 Reliable evaluation of adversarial robustness with an ensembleof diverse parameter-free attacks,2020, In ICML
 Explaining and harnessing adversarialexamples,2015, CoRR
 On calibration of modern neuralnetworks,2017, In ICML
 Deep residual learning for imagerecognition,2016, In CVPR
 Learning multiple layers of features from tiny images,2009, TechReport
 Towards deeplearning models resistant to adversarial attacks,2018, ArXiv
 Bag of tricks for adversarial training,2021, InICLR
 Adversarial robustness through local lineariza-tion,2019, In NeurIPS
 Defense-gan: Protecting classifiers againstadversarial attacks using generative models,2018, ArXiv
 Attacks on state-of-the-art face recognition using attentionaladversarial attack generative network,2021, Multimedia Tools and Applications
 Confidence-calibrated adversarial training: Generaliz-ing to unseen attacks,2020, In ICML
 Bilateral adversarial training: Towards fast training of more robust models againstadversarial attacks,2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
 Improving adversarialrobustness requires revisiting misclassified examples,2020, In ICLR
 Fast is better than free: Revisiting adversarial training,2020, InICLR
 Feature squeezing: Detecting adversarial examples in deepneural networks,2018, ArXiv
