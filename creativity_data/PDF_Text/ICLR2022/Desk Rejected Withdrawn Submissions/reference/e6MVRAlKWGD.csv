title,year,conference
 Comparing automatic and human evaluation of nlg systems,2006, In11th conference of the european chapter of the association for computational linguistics
 Pitfalls in machine learning research: Reexaminingthe development cycle,2020, In ”I Can’t Believe It’s Not Better!”NeurIPS 2020 workshop
 Lan-guage models are few-shot learners,2020, arXiv preprint arXiv:2005
 Fabula entropyindexing: Objective measures of story coherence,2021, In Proceedings of the Third Workshopon Narrative Understanding
 Revisiting summarization evaluation for scientific arti-cles,2016, arXiv preprint arXiv:1604
 Bert: Pre-training of deep bidirectional transformers for language understanding,2018, arXiv preprintarXiv:1810
 Scare-crow: A framework for scrutinizing machine text,2021, arXiv preprint arXiv:2107
 Memorization vs,2021, generalization:Quantifying data leakage in NLP performance evaluation
 Question answering as an automatic eval-uation metric for news article summarization,2019, arXiv preprint arXiv:1906
 Hierarchical neural story generation,2018, InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers)
 Automatictreatment planning based on three-dimensional dose distribution predicted from deeplearning technique,2019, Medical physics
 Why verbs are harder to learn than nouns: Initial insightsfrom a computational model of intention recognition in situated word learning,2005, In 27thAnnual Meeting of the Cognitive Science Society
 Affectgan: Affect-basedgenerative art driven by semantics,2021, arXiv preprint arXiv:2109
 Zero-shot controlledgeneration with encoder-decoder transformers,2021, arXiv preprint arXiv:2106
 Qmdp-net: Deep learning for planning underpartial observability,2017, arXiv preprint arXiv:1703
 A style-based generator architecture for gen-erative adversarial networks,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Baleen: Robust multi-hop reasoningat scale via condensed retrieval,2021, arXiv preprint arXiv:2101
 Rouge: A package for automatic evaluation of summaries,2004, In Text summa-rization branches out
 Roberta: A robustly optimizedbert pretraining approach,2019, arXiv preprint arXiv:1907
 Collaborative storytelling with large-scale neurallanguage models,2020, arXiv preprint arXiv:2011
 Styleclip:Text-driven manipulation of stylegan imagery,2021, arXiv preprint arXiv:2103
 Inferring the reader: Guiding au-tomated story generation with commonsense reasoning,2021, arXiv preprint arXiv:2105
 Predicting generated storyquality with quantitative measures,2018, In Fourteenth Artificial Intel ligence and InteractiveDigital Entertainment Conference
 Exploring the limits of transfer learn-ing with a unified text-to-text transformer,2019, arXiv preprint arXiv:1910
 The limits of automatic summarisation according to rouge,2017, In Proceedingsof the 15th Conference of the European Chapter of the Association for ComputationalLinguistics: Volume 2
 Approximate unsupervised summary optimi-sation for selections of rouge,2016, In Actes de la conference conjointe JEP-TALN-RECITAL2016
 Mastering the game of go with deep neural networks and tree search,2016, Na-ture
 GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-guage Model,2021, https://github
 Megatron-cntrl: Controllable story generation withexternal knowledge using large-scale language models,2020, arXiv preprint arXiv:2010
 Plan-and-write: Towards better automatic storytelling,2019, In Proceedings of the AAAI Conferenceon Artificial Intel ligence
 Pegasus: Pre-training withextracted gap-sentences for abstractive summarization,2020, In International Conference onMachine Learning
 Aligning books and movies: Towards story-like visual explana-tions by watching movies and reading books,2015, In The IEEE International Conference onComputer Vision (ICCV)
