title,year,conference
 Efficientlifelong learning with a-gem,2018, arXiv preprint arXiv:1812
 On tiny episodic memories in continuallearning,2019, arXiv preprint arXiv:1902
 A continual learning survey: Defying forgetting in classificationtasks,2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
 Animage is worth 16x16 words: Transformers for image recognition at scale,2021, ICLR
 Adver-sarial continual learning,2020, In ECCV
 Hybrid computing using a neural network with dynamic external memory,2016, Nature
 A survey of deep learningtechniques for autonomous driving,2020, Journal of Field Robotics
 Memory efficient experience replay forstreaming learning,2019, In ICRA
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcom-ing catastrophic forgetting in neural networks,2017, PNAS
 Ask me anything: Dynamic memory networks fornatural language processing,2016, In ICML
 The mnist database of handwritten digits,1998, http://yann
 The power of scale for parameter-efficient prompttuning,2021, arXiv preprint arXiv:2104
 Prefix-tuning: Optimizing continuous prompts for generation,2021, arXivpreprint arXiv:2101
 Learning without forgetting,2017, TPAMI
 Core50: a new dataset and benchmark for continuousobject recognition,2017, In Conference on Robot Learning
 The natural languagedecathlon: Multitask learning as question answering,2018, arXiv preprint arXiv:1806
 A surveyon bias and fairness in machine learning,2021, ACM Computing Surveys (CSUR)
 An empirical investiga-tion of the role of pre-training in lifelong learning,2021, ICML Workshop on Theory and Foundation ofContinual Learning
 Readingdigits in natural images with unsupervised feature learning,2011, In NIPS
 Neural discrete representation learn-ing,2017, arXiv preprint arXiv:1711
 Continuallifelong learning with neural networks: A review,2019, Neural Networks
 Gdumb: A simple approach that questionsour progress in continual learning,2020, In ECCV
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, JMLR
 Progressive neural networks,2016, arXiv preprintarXiv:1606
 Exploiting cloze questions for few shot text classification andnatural language inference,2020, arXiv preprint arXiv:2001
 Encoders and ensembles for task-freecontinual learning,2021, arXiv preprint arXiv:2105
 Autoprompt:Eliciting knowledge from language models with automatically generated prompts,2020, arXiv preprintarXiv:2010
 Privacy-preserving deep learning,2015, In Proc SIGSAC conferenceon computer and communications security
 Three scenarios for continual learning,2019, arXiv preprintarXiv:1904
 Attention is all you need,2017, NeurIPS
 Fashion-mnist: a novel image dataset for benchmark-ing machine learning algorithms,2017, arXiv preprint arXiv:1708
 Lifelong learning with dynamicallyexpandable networks,2017, arXiv preprint arXiv:1708
 Task agnostic continual learning usingonline variational bayes,2018, arXiv preprint arXiv:1803
