title,year,conference
 Quantifying attention flow in transformers,2020, In Proceedings ofthe 58th Annual Meeting of the Association for Computational Linguistics (ACL)
 Understanding robustness of transformers for image classification,2021, arXiv preprintarXiv:2103
 Adversarialrobustness: From self-supervised pre-training to fine-tuning,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Deep residual learning for image recog-nition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In ICLR
 Using pre-training can improve model robustnessand uncertainty,2019, In International Conference on Machine Learning
 Intriguing properties of vision transformers,2021, arXiv preprintarXiv:2105
 The limitations of deep learning in adversarial settings,2016, In 2016 IEEE European sympo-sium on security and privacy (EuroS&P)
 Vision transformers are robust learners,2021, arXiv preprintarXiv:2105
 Grad-cam: Visual explanations from deep networks via gradient-based local-ization,2017, In Proceedings of the IEEE international conference on computer vision
 On the adversarial robust-ness of visual transformers,2021, arXiv preprint arXiv:2103
 Intriguing properties of neural networks,2014, ICLR
 Mlp-mixer: Anall-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Visualizing and understanding convolutional networks,2014, InEuropean conference on computer vision
