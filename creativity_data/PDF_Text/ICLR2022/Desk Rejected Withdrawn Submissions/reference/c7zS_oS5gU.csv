title,year,conference
 Variationalinformation distillation for knowledge transfer,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition
 Network flows,1988, 1988
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Rademacher and gaussian complexities: Risk bounds andstructural results,2002, Journal of Machine Learning Research
 Iterativebregman projections for regularized transportation problems,2015, SIAM Journal on Scientific Com-puting
 A large anno-tated corpus for learning natural language inference,2015, arXiv preprint arXiv:1508
 Iemocap: Interactive emotional dyadicmotion captUre database,2008, Language resources and evaluation
 Optimal transport for do-main adaptation,2016, IEEE transactions on pattern analysis and machine intelligence
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, Advances in neuralinformation processing systems
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Knowledge distillation: Asurvey,2021, International Journal ofComputer Vision
 Ups and downs: Modeling the visual evolution of fashion trendswith one-class collaborative filtering,2016, In proceedings of the 25th international conference onworld wide web
 Scaling federated learning for fine-tuning of large language models,2021, arXiv preprintarXiv:2102
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Algorithms and theory for multiple-sourceadaptation,2018, arXiv preprint arXiv:1805
 First analysis of local gd on hetero-geneous data,2019, arXiv preprint arXiv:1909
 Federated optimization:Distributed machine learning for on-device intelligence,2016, arXiv preprint arXiv:1610
 Fair resource allocation in federatedlearning,2019, arXiv preprint arXiv:1905
 On the convergence offedavg on non-iid data,2019, arXiv preprint arXiv:1907
 Dailydialog: A manuallylabelled multi-turn dialogue dataset,2017, In Proceedings of The 8th International Joint Conference onNatural Language Processing (IJCNLP 2017)
 Ensemble distillation for robust modelfusion in federated learning,2020, arXiv preprint arXiv:2006
 Federated pretraining and fine tuning of bert using clinical notes frommultiple silos,2020, arXiv preprint arXiv:2002
 Differential properties ofsinkhorn approximation for learning with wasserstein distance,2018, Advances in Neural InformationProcessing Systems
 Modeling semantic containment and exclusion innatural language inference,2008, In Proceedings of the 22nd International Conference on Compu-tational Linguistics (CoIing 2008)
 A surveyon bias and fairness in machine learning,2019, arXiv preprint arXiv:1908
 Agnostic federated learning,2019, In Interna-tional Conference on Machine Learning
 Machine learning: a probabilistic perspective,2012, MIT press
 Distributed gradient methods for convex machine learning problems in networks:Distributed optimization,2020, IEEE Signal Processing Magazine
 Ad-versarial nli: A new benchmark for natural language understanding,2020, In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics
 Feed: Feature-level ensemble for knowledge distillation,2019, arXivpreprint arXiv:1909
 Computational optimal transport: With applications to datascience,2019, Foundations and TrendsÂ® in Machine Learning
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 Contrastive representation distillation,2019, arXivpreprint arXiv:1910
 Effective federated adaptive gradient methods withnon-iid decentralized data,2020, arXiv preprint arXiv:2009
 Similarity-preserving knowledge distillation,2019, In Proceedings of theIEEE/CVF International Conference on Computer Vision
 Well-read students learn better:On the importance of pre-training compact models,2019, arXiv preprint arXiv:1908
 Adaptive federated learning in resource constrained edge computing systems,2019, IEEEJournal on Selected Areas in Communications
 The effect of class distribution on classifier learning: an empiricalstudy,2001, 2001
 A broad-coverage challenge corpus forsentence understanding through inference,2017, arXiv preprint arXiv:1704
 Distilled person re-identification:Towards a more scalable system,2019, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition
 Learning from multiple teacher networks,2017, InProceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery andData Mining
 It is important to note that con-textualization can be task-specific,5000, We randomly sample 5000 review-label pairs for each sentimentclass
