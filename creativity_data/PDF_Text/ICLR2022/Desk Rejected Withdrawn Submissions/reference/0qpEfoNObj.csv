title,year,conference
 Information dropout: Learning optimal representationsthrough noisy computation,2018, TPAMI
 On the properties of variational approximationsof gibbs posteriors,2016, JMLR
 Implicit regularization in deep matrixfactorization,2019, NeurIPS
 Understanding dropout,2013, NIPS
 First-and second-order methods for learning: between steepest descent and newtonâ€™smethod,1992, Neural computation
 Pattern recognition and machine learning,2006,2006
 Weight uncertainty inneural network,2015, In ICML
 Enriching word vectorswith subword information,2017, TACL
 Practical gauss-newton optimisation for deeplearning,2017, ICML
 The intriguing role of module criticalityin the generalization of deep networks,2020, ICLR
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Preserving statistical validity in adaptive data analysis,2015, In STOC
 Data-dependent pac-bayes priors via differentialprivacy,2018, NeurIPS
 In search of robust measures of generaliza-tion,2020, NeurIPS
 Reducing transformer depth on demand withstructured dropout,2020, ICLR
 Contextualdropout: An efficient sample-dependent dropout module,2021, ICLR
 Generalizable adversarial training via spectral nor-malization,2018, ICLR
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2016, ICML
 Concrete dropout,2017, NIPS
 Demystifying dropout,2019, International Conference onMachine Learning
 Pac-bayesian learn-ing of linear classifiers,2009, In ICML
 Pac-bayes un-leashed: generalisation bounds with unbounded losses,2021, Entropy
 On the inductive bias of dropout,2015, JMLR
 Surprising properties of dropout in deep networks,2017, JMLR
 Improving neural networks by preventing co-adaptation of feature detectors,2012, arXiv preprintarXiv:1207
 Improving generalization bycontrolling label-noise information in neural network weights,2020, ICML
 Three factors influencing minima in sgd,2017, ICANN
 Fantasticgeneralization measures and where to find them,2020, ICLR
 How doesweight correlation affect the generalisation ability of deep neural networks,2020, NeurIPS
 Bag of tricks for efficienttext classification,2017, In EACL
 Improving generalization performance by sWitching fromadam to sgd,2017, arXiv preprint arXiv:1712
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, ICLR
 Convolutional neural netWorks for sentence classification,2014, In EMNLP
 Variational dropout and the local reparameteri-zation trick,2015, NIPS
 Generalized variance,2004, Encyclopedia of Statistical Sciences
 Imagenet classification With deep convo-lutional neural netWorks,2012, NIPS
 Pac-bayes & margins,2003, NIPS
 Meta dropout: Learning toperturb latent features for generalization,2020, ICLR
 Dichotomize and general-ize: Pac-bayesian binary activated deep neural netWorks,2019, NeurIPS
 ToWards explaining the regularization effect of initial largelearning rate in training neural netWorks,2019, NeurIPS
 Recurrent neural netWork for text classification Withmulti-task learning,2016, arXiv preprint arXiv:1605
 Optimizing neural netWorks With kronecker-factored approximatecurvature,2015, ICML
 A note on the pac bayesian theorem,2004, arXiv preprint cs/0411099
 Pac-bayesian model averaging,1999, In COLT
 On dropout and nuclear norm regularization,2019, ICML
 On convergence and generalization of dropout training,2020, NeurIPS
 On the implicit bias of dropout,2018, ICML
 All you need is a good init,2015, arXiv preprint arXiv:1511
 Generalization in deep networks: The role of distance frominitialization,2019, arXiv preprint arXiv:1901
 Dropout as a structuredshrinkage prior,2019, ICML
 Representation based complexity measures for predicting gener-alization in deep learning,2020, arXiv preprint arXiv:2012
 Readingdigits in natural images with unsupervised feature learning,2011,2011
 Path-sgd: Path-normalized optimiza-tion in deep neural networks,2015, NIPS
 Norm-based capacity control in neuralnetworks,2015, COLT
 Exploring general-ization in deep learning,2017, NIPS
 A pac-bayesian approach tospectrally-normalized margin bounds for neural networks,2018, ICLR
 To-wards understanding the role of over-parametrization in generalization of neural networks,2018, arXivpreprint arXiv:1805
 Pac-bayesbounds with data dependent priors,2012, JMLR
 Learning pac-bayes priors for probabilisticneural networks,2021, arXiv preprint arXiv:2109
 Tighter risk certifi-cates for neural networks,2021, JMLR
 Pac-bayesian margin bounds for con-volutional neural networks,2017, arXiv preprint arXiv:1801
 A scalable laplace approximation for neuralnetworks,2018, ICLR
 Pac-bayes with backprop,2019, arXivpreprint arXiv:1908
 Pac-bayes analysisbeyond the usual bounds,2020, In NeurIPS
 Exact solutions to the nonlinear dynam-ics of learning in deep linear neural netWorks,2013, arXiv preprint arXiv:1312
 Pac-bayesian generalisation error bounds for gaussian process classification,2002, Jour-nal of machine learning research
 On the generalization effects oflinear transformations in data augmentation,2020, ICML
 Second-order methods for neural networks: Fast and reliable training methodsfor multi-layer perceptrons,2012, Springer Science & Business Media
 Very deep convolutional netWorks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 A bayesian perspective on generalization and stochastic gradientdescent,2018, ICLR
 Thuctc: an efficientchinese text classifier,2016, GitHub Repository
 A strongly quasiconvexpac-bayesian bound,2017, In ALT
 Dropout training as adaptive regularization,2013, NIPS
 Fast dropout training,2013, ICML
 The implicit and explicit regularization effects ofdropout,2020, ICML
 Bayesian learning via stochastic gradient langevin dynamics,2011, InICML
 A Walk With sgd,2018, arXiv preprintarXiv:1802
 Spectral norm regularization for improving the generalizabilityof deep learning,2017, arXiv preprint arXiv:1705
 ToWards under-standing and improving dropout in game theory,2021, ICLR
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2019, ICML
 Rethinking bias-variancetrade-off for generalization of neural netWorks,2020, ICML
