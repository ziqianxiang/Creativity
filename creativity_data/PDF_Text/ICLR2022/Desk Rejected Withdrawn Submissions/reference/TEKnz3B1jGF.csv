title,year,conference
 Beit: Bert pre-training of image transformers,2021, arXiv preprintarXiv:2106
 A map of object space in primateinferotemporal cortex,2020, Nature
 Assessing the similarity of cortical object and scene representationsthrough cross-validated voxel encoding models,2019, Journal of Vision
 Deep neural networks rival the representation of primate itcortex for core visual object recognition,2014, PLoS computational biology
 The algonauts project:A platform for communication between the sciences of biological and artificial intelligence,2019, arXive-prints
 Sentence complexity and input modality effects in sentence compre-hension: an fmri study,2004, NeuroImage
 Imagenet: A large-scale hi-erarchical image database,2009, In 2009 IEEE conference on computer vision and pattern recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 An im-age is worth 16x16 words: Transformers for image recognition at scale,2020, In International Confer-ence on Learning Representations
 Seeing it all:Convolutional network layers map the function of the human visual system,2017, NeuroImage
 Linking artificial and human neural representations of language,2019, InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Deep neural networks reveal a gradient in the complexity ofneural representations across the ventral stream,2015, Journal of Neuroscience
 Identifying natural imagesfrom human brain activity,2008, Nature
 Recurrence is required to capture the representational dynamics of thehuman visual system,2019, Proceedings of the National Academy of Sciences
 Brain-like object recognition withhigh-performing shallow recurrent anns,2019, Advances in Neural Information Processing Systems
 Visualbert: A simpleand performant baseline for vision and language,2019, arXiv preprint arXiv:1908
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Vilbert: pretraining task-agnostic visiolin-guistic representations for vision-and-language tasks,2019, In Proceedings of the 33rd InternationalConference on Neural Information Processing Systems
 Effects of language onvisual perception,2020, Trends in cognitive sciences
 Stepencog: A convolutionallstm autoencoder for near-perfect fmri encoding,2019, In 2019 International Joint Conference on Neu-ral Networks (IJCNN)
 Toward a universal decoder of linguisticmeaning from brain activation,2018, Nature communications
 Languagemodels are unsupervised multitask learners,2019, 2019
 Learning transferable visualmodels from natural language supervision,2021, Image
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, Advances in neural information processing systems
 Imagenet large scale visualrecognition challenge,2015, International journal of computer vision
 The neural architecture of language: Integrativereverse-engineering converges on a model for predictive processing,2020, BioRxiv
 Inducing brain-relevant bias in natural languageprocessing models,2019, Advances in Neural Information Processing Systems
 Very deep convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Lxmert: Learning cross-modality encoder representations from trans-formers,2019, In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Inverse retinotopy: inferring the visual content of imagesfrom brain activation patterns,2006, Neuroimage
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Neural taskonomy: Inferring the similarity of task-derived representations from brain activity,2019, Advances in Neural Information Processing Systems
 Hierarchical modular optimiza-tion of convolutional networks achieves representations similar to macaque it and human ventralstream,2013, 2013
 Using goal-driven deep learning models to understandsensory cortex,2016, Nature neuroscience
 Performance-optimized hierarchical models predict neural responses in higher visualcortex,2014, Proceedings of the national academy of sciences
