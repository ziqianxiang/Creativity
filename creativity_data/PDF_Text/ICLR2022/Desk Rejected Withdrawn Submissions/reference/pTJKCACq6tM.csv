title,year,conference
 QSGD: Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in Neural Information Process-ing Systems 
 Byzantine stochastic gradient descent,2018, In Advances inNeural Information Processing Systems
 The convergence of sparsified gradient methods,2018, In Advances in Neural InformationProcessing Systems
 Robust solutions of linear programming problems contami-nated with uncertain data,2000, Mathematical programming
 Nonlinear programming,1997, Journal of the Operational Research Society
 Theory and applications of robustoptimization,2011, SIAM review
 Machine learning with adversaries: Byzantinetolerant gradient descent,2017, In Advances in Neural Information Processing Systems
 Large-scale machine learning with stochastic gradient descent,2010, In Proceedings ofCOMPSTAT’2010
 On distributedstochastic gradient descent for nonconvex functions in the presence of byzantines,2020, In ICASSP2020-2020 IEEE International Conference on Acoustics
 Sample size selection in optimiza-tion methods for machine learning,2012, Mathematical programming
 Open questions concerning weiszfeld’s algorithm forthe fermat-weber location problem,1989, Mathematical Programming
 Combi-natorial feature selection problems,2000, In Proceedings 41st Annual Symposium on Foundations ofComputer Science
 Entropy-sgd: Biasing gradient descentinto wide valleys,2019, Journal of Statistical Mechanics: Theory and Experiment
 Draco: Byzantine-resilient distributed training via redundant gradients,2018, In International Conference on MachineLearning
 Targeted backdoor attacks on deeplearning systems using data poisoning,2017, arXiv preprint arXiv:1712
 Distributed statistical machine learning in adversarial settings:Byzantine gradient descent,2017, Proceedings of the ACM on Measurement and Analysis of ComputingSystems
 Runtime guarantees forregression problems,2013, In Proceedings of the 4th conference on Innovations in Theoretical ComputerScience
 Geometric medianin nearly linear time,2016, In Proceedings of the forty-eighth annual ACM symposium on Theory ofComputing
 Byzantine-resilient SGD in high dimensions on heterogeneousdata,2020, arXiv preprint arXiv:2005
 Optimal distributed online predictionusing mini-batches,2012, The Journal of Machine Learning Research
 Nearest neighbor based greedycoordinate descent,2011, In Advances in Neural Information Processing Systems
 Recent advances in algorithmic high-dimensional robuststatistics,2019, arXiv preprint arXiv:1911
 Feedback control theory,2013, CourierCorporation
 Distributed robust learning,2014, arXiv preprint arXiv:1409
 Penalized regressions: the bridge versus the lasso,1998, Journal of computational andgraphical statistics
 Communication-efficient and byzantine-robust distributed learning,2019, arXiv preprintarXiv:1911
 Badnets: Evaluating backdooringattacks on deep neural networks,2019, IEEEAccess
 When cycliccoordinate descent outperforms randomized coordinate descent,2017, 2017
 Note on the median of a multivariate distribution,1948, Biometrika
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Benchmarking neural network robustness to commoncorruptions and perturbations,2018, In International Conference on Learning Representations
 Fast coordinate descent methods with variable selectionfor non-negative matrix factorization,2011, In Proceedings of the 17th ACM SIGKDD internationalconference on Knowledge discovery and data mining
 Making large-scale svm learning practical,1998, Technical report
 Linear convergence of gradient and proximal-gradient methods under the Polyak-Eojasiewicz condition,2016, In European Conference on MachineLearning and Knowledge Discovery in Databases-Volume 9851
 Efficientgreedy coordinate descent for composite problems,2019, In The 22nd International Conference onArtificial Intelligence and Statistics
 Error feedbackfixes signsgd and other gradient compression schemes,2019, In International Conference on MachineLearning
 The median of a finite measure on a banach space,1987, Statistical data analysis basedon the Ll-norm and related methods (NeuChatel
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Imagenet classification with deep con-volutional neural networks,2012, Advances in neural information processing systems
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Principled approaches to robust machine learning and beyond,2018, PhD thesis
 Rsa: Byzantine-robuststochastic aggregation methods for distributed learning from heterogeneous datasets,2019, In Proceed-ings of the AAAI Conference on Artificial Intelligence
 Efficient mini-batch training forstochastic optimization,2014, In Proceedings of the 20th ACM SIGKDD international conference onKnowledge discovery and data mining
 Backdoor embedding inconvolutional neural network models via invisible perturbation,2018, arXiv preprint arXiv:1808
 Breakdown points of affine equivariant estimators ofmultivariate location and covariance matrices,1991, The Annals of Statistics
 The hidden vulnerability ofdistributed learning in byzantium,2018, arXiv preprint arXiv:1802
 Geometric median and robust estimation in banach spaces,2015, Bernoulli
 Non-asymptotic analysis of stochastic approximation algorithmsfor machine learning,2011, In Advances in Neural Information Processing Systems
 Paved with good intentions: analysis of a randomized blockkaczmarz method,2014, Linear Algebra and its Applications
 Efficiency of coordinate descent methods on huge-scale optimization problems,2012, SIAMJournal on Optimization
 Coordinatedescent converges faster with the gauss-southwell rule than random selection,2015, In InternationalConference on Machine Learning
 A fast algorithm for training support vector machines,1998, CiteSeerX
 Iteration complexity of randomized block-coordinate descentmethods for minimizing a composite function,2014, Mathematical Programming
 A stochastic approximation method,1951, The annals of mathematicalstatistics
 On the nonasymptotic convergence of cyclic coordinate descentmethods,2013, SIAM Journal on Optimization
 Block coordinate relaxation methods for non-parametric wavelet denoising,2000, Journal of computational and graphical statistics
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 Accelerated mini-batch stochastic dual coordinate ascent,2013, InAdvances in Neural Information Processing Systems
 Learning with bad training data via iterative trimmed lossminimization,2019, In International Conference on Machine Learning
 Understanding top-k sparsificationin distributed deep learning,2019, arXiv preprint arXiv:1911
 Resilience: A criterion for learning in thepresence of arbitrary outliers,2017, arXiv preprint arXiv:1703
 Local sgd converges fast and communicates little,2018, arXiv preprint arXiv:1805
 The error-feedback framework: Better rates for sgdwith delayed gradients and compressed communication,2019, arXiv preprint arXiv:1909
 Approximate steepest coordinate descent,2017, InInternational Conference on Machine Learning
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 Scalable distributed dnn training using commodity gpu cloud computing,2015, In SixteenthAnnual Conference of the International Speech Communication Association
 Spectral signatures in backdoor attacks,2018, In Advancesin Neural Information Processing Systems
 Block-coordinate gradient descent method for linearly constrainednonsmooth separable optimization,2009, Journal of optimization theory and applications
 A coordinate gradient descent method for nonsmooth separableminimization,2009, Mathematical Programming
 Distributed asynchronous deterministicand stochastic gradient optimization algorithms,1986, IEEE transactions on automatic control
 The multivariate l1-median and associated data depth,2000, Proceedingsof the National Academy of Sciences
 Probabilistic logics and the synthesis of reliable organisms from unreliablecomponents,1956, Automata studies
 Provably correct algorithms for matrix column subset selection withselectively sampled data,2017, The Journal of Machine Learning Research
 Alfred Weber’s theory of the location of industries,1929, TheUniversity of Chicago Press
 Federated variance-reducedstochastic gradient descent with robustness to byzantine attacks,2020, IEEE Transactions on SignalProcessing
 Fashion-mnist: a novel image dataset for benchmarkingmachine learning algorithms,2017, arXiv preprint arXiv:1708
 Zeno: Distributed stochastic gradient descent withsuspicion-based fault-tolerance,6893, In International Conference on Machine Learning
 Byzantine-resilient stochastic gradient descentfor distributed learning: A lipschitz-inspired coordinate-wise median approach,2019, In 2019 IEEE 58thConference on Decision and Control (CDC)
 Asynchronous parallel greedy coordinate descent,2016, In NIPS
 Large linear classification whendata cannot fit in memory,2012, ACM Transactions on Knowledge Discovery from Data (TKDD)
 New analysis of linear convergence of gradient-type methods via unifying error boundconditions,2020, Mathematical Programming
