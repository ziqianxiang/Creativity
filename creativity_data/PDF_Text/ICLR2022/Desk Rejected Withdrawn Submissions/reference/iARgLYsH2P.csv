title,year,conference
 Variational attention forsequence-to-sequence models,2018, In Proc
 On attention redun-dancy: A comprehensive study,2021, In Proc
 Isolating sources of disentangle-ment in vaes,2018, In Proc
 Generating long sequences with sparsetransformers,2019, arXiv preprint arXiv:1904
 Adaptively sparse transformers,2019, InProc
 Unsupervised learning of disentangled representations fromvideo,2017, In Proc
 Gaussian transformer: A lightweight approach for naturallanguage inference,2019, Proc
 Î²-VAE: Learning basic visual concepts with a con-strained variational framework,2017, In Proc
 Attention is not explanation,2019, In Proc
 Variational deepembedding: an unsupervised and generative approach to clustering,2017, In Proc
 Disentangled representationlearning for non-parallel text style transfer,2019, In Proc
 T-GSA: Transformer with gaussian-weightedself-attention for speech enhancement,2020, In Proc
 Auto-encoding variational Bayes,2014, In Proc
 A structured self-attentive sentence embedding,2017, In Proc
 Challenging common assumptions in the unsupervised learn-ing of disentangled representations,2019, In Proc
 In Proc,2021, of International Conference on LearningRepresentations
 BLEU: a method for automaticevaluation of machine translation,2002, In Proc
 On variationalbounds of mutual information,2019, In Proc
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguis-tics
 On mu-tual information maximization for representation learning,2020, In Proc
 Attention is all you need,2017, In Proc
 Pay less attention withlightweight and dynamic convolutions,2019, In Proc
 Adversarialsparse transformer for time series forecasting,2020, In Proc
 Disentangled sequential autoencoder,2018, In Proc
 Improvingzero-shot voice style transfer via disentangled representation learning,2021, In Proc
