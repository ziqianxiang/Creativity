title,year,conference
 Low-rank bottleneck in multi-head attention models,2020, In International Conference on Machine Learn-ing
 On the relationship between self-attention and convolutional layers,2020, In International Conference on Learning Representations
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXivpreprint arXiv:1901
 Convit: Improving vision transformers with soft convolutional inductive biases,2021, In MarinaMeila and Tong Zhang (eds
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Attention is not all you need: Pureattention loses rank doubly exponentially with depth,2021, arXiv preprint arXiv:2103
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Rethinking positional encoding in language pre-training,2020, InInternational Conference on Learning Representations
 Learning multiple layers of features from tiny images,2009, 2009
 Layer normalization,2016, arXiv preprintarXiv:1607
 The depth-to-width interplayin self-attention,2020, arXiv preprint arXiv:2006
 Learnable fourier features for multi-dimensional spatial positional encoding,2021, In NeurIPS
 On the expressive power ofself-attention matrices,2021, arXiv preprint arXiv:2106
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, arXiv preprintarXiv:2103
 On the turing completeness of modern neuralnetwork architectures,2019, In International Conference on Learning Representations
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Training data-efficient image transformers & distillation through attention,2021, InMarina Meila and Tong Zhang (eds
 Attention is all you need,2017, In I
 Which transformer architecture fitsmy data? a vocabulary bottleneck in self-attention,2021, arXiv preprint arXiv:2105
 Pytorch image models,2019, https://github
 Multi-scale context aggregation by dilated convolutions,2015, arXivpreprint arXiv:1511
 Big bird: Transformers forlonger sequences,2020, In NeurIPS
