title,year,conference
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Training deeper neuralmachine translation models with transparent attention,2018, arXiv preprint arXiv:1808
 Faster transformer decoding: N-grammasked self-attention,2020, arXiv preprint arXiv:2001
 Meshed-memory Trans-former for image caPtioning,1057, In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition
 Language modeling with gatedconvolutional networks,2017, In International conference on machine learning
 Latent alignment andvariational attention,2018, In Advances in Neural Information Processing Systems
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Animage is worth 16x16 words: Transformers for image recognition at scale,2020, arXiv preprintarXiv:2010
 Understanding back-translation atscale,2018, arXiv preprint arXiv:1808
 Bayesian attention modules,2020, Advances inNeural Information Processing Systems
 Convolutionalsequence to sequence learning,1243, In International Conference on Machine Learning
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Making the Vin VQA matter: Elevating the role of image understanding in visual question answering,2017, InProceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Deep residual learning for imagerecognition,2016, In Proceedings of the IEEE conference on computer vision and pattern recognition
 Transformers are rnns:Fast autoregressive transformers with linear attention,2020, In International Conference on MachineLearning
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Revealing the dark secrets ofbert,2019, arXiv preprint arXiv:1908
 A simple weight decay can improve generalization,1992, In Advances inneural information processing systems
 Albert: A lite bert for self-supervised learning of language representations,2019, arXiv preprintarXiv:1909
 An empiricalevaluation of deep architectures on problems with many factors of variation,2007, In Proceedings of the24th international conference on Machine learning
 Parameter effi-cient multimodal transformers for video representation learning,2020, arXiv preprint arXiv:2012
 Microsoft COCO: Common objects in context,2014, In Europeanconference on computer vision
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 LinguisticknoWledge and transferability of contextual representations,2019, arXiv preprint arXiv:1903
 Very deep transformers for neural machinetranslation,2020, arXiv preprint arXiv:2008
 BLEU: A method for automaticevaluation of machine translation,2002, In Proceedings of the 40th annual meeting on association forcomputational linguistics
 Fine-tuned transformers show clusters of similarrepresentations across layers,2021, arXiv preprint arXiv:2109
 A call for clarity in reporting bleu scores,2018, arXiv preprint arXiv:1804
 Sequence level trainingwith recurrent neural networks,2015, arXiv preprint arXiv:1511
 Improving neural machine translation modelswith monolingual data,2015, arXiv preprint arXiv:1511
 The evolved transformer,2019, In International Conference onMachine Learning
 Mobilebert: acompact task-agnostic bert for resource-limited devices,2020, arXiv preprint arXiv:2004
 Rethinking theinception architecture for computer vision,2016, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Synthesizer: Re-thinking self-attention for transformer models,2021, In International Conference on Machine Learning
 Bert rediscovers the classical nlp pipeline,2019, arXivpreprint arXiv:1905
 What do you learn fromcontext? probing for sentence structure in contextualized word representations,2019, arXiv preprintarXiv:1905
 Training data-efficient image transformers & distillation through attention,2021, In InternationalConference on Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Graph attention networks,2017, arXiv preprint arXiv:1710
 Pay less attention withlightweight and dynamic convolutions,2019, arXiv preprint arXiv:1901
 Revisiting semi-supervised learning withgraph embeddings,2016, arXiv preprint arXiv:1603
 Deep modular co-attention networks forvisual question answering,2019, In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition
 Graph transformernetworks,2019, Advances in Neural Information Processing Systems
