title,year,conference
 Convolutionalsequence to sequence learning,2017, In International Conference on Machine Learning
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Bert: Pre-training ofdeep bidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Nerf: Representing scenes as neural radiance fields for view synthesis,2020, In EuropeanConference on Computer Vision
 Reconstructing continuousdistributions of 3d protein structure from cryo-em images,2019, arXiv preprint arXiv:1909
 Deformable neural radiance fields,2020, arXiv preprintarXiv:2011
 Neural scene flow fields forspace-time view synthesis of dynamic scenes,2020, arXiv preprint arXiv:2011
 Neural scene graphsfor dynamic scenes,2020, arXiv preprint arXiv:2011
 Dynamic neural radiancefields for monocular 4d facial avatar reconstruction,2020, arXiv preprint arXiv:2012
 Nerf in the wild: Neural radiance fields for unconstrained photocollections,2020, arXiv preprint arXiv:2008
 Mip-nerf: A multiscale representation for anti-aliasing neural radiancefields,2021, arXiv preprint arXiv:2103
 On the spectral bias of neural networks,2019, In InternationalConference on Machine Learning
 Fourier featureslet networks learn high frequency functions in low dimensional domains,2020, arXiv preprintarXiv:2006
 Sampling from large matrices: An approach throughgeometric functional analysis,2007, Journal ofthe ACM (JACM)
 Self-attention with relative position repre-sentations,2018, arXiv preprint arXiv:1803
 Attentive language models beyond a fixed-length context,2019, arxiv 2019
 Exploring the limits of transfer learning with a unifiedtext-to-text transformer,2019, arXiv preprint arXiv:1910
 Deberta: Decoding-enhancedbert with disentangled attention,2020, arXiv preprint arXiv:2006
 Constituency parsing with a self-attentive encoder,2018, arXiv preprintarXiv:1805
 Encoding word order in complex embeddings,2019, arXiv preprint arXiv:1912
 Learning to encode positionfor transformer with continuous dynamical model,2020, In International Conference on MachineLearning
 Deep neural networks are easily fooled: Highconfidence predictions for unrecognizable images,2015, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Compositional pattern producing networks: A novel abstraction of develop-ment,2007, Genetic programming and evolvable machines
 Differentiable volu-metric rendering: Learning implicit 3d representations without 3d supervision,2020, In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization,2019, InProceedings of the IEEE/CVF International Conference on Computer Vision
 Scene representation networks:Continuous 3d-structure-aware neural scene representations,2019, arXiv preprint arXiv:1906
 Random features for large-scale kernel machines,2007, In NIPS
 Fine-grained analysis of op-timization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 On the inductive bias of neural tangent kernels,2019, arXiv preprintarXiv:1905
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Wide neural networks of any depth evolve as linearmodels under gradient descent,2019, arXiv preprint arXiv:1902
