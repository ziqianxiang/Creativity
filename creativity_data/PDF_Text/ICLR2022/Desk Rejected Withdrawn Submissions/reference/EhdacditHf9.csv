title,year,conference
 Optimization methods for large-scale machinelearning,2018, SIAMReview
 Stochastic gradient descent in cor-related settings: A study on Gaussian processes,2020, In Advances in Neural Information ProcessingSystems
 On the convergence of a class of Adam-type algorithms for non-convex optimization,2019, In Proceedings of The International Conference onLearning Representations
 Adaptive subgradient methods for online learning andstochastic optimization,2011, Journal of Machine Learning Research
 SPIDER: Near-optimal non-convexoptimization via stochastic path-integrated differential estimator,2018, In Advances in Neural Informa-tion Processing Systems
 Convergence rates for the stochastic gradi-ent descent method for non-convex objective functions,2020, Journal of Machine Learning Research
 Optimal stochastic approximation algorithms for strongly con-vex stochastic composite optimization I: A generic algorithmic framework,2012, SIAM Journal onOptimization
 Optimal stochastic approximation algorithms for strongly con-vex stochastic composite optimization II: Shrinking procedures and optimal algorithms,2013, SIAMJournal on Optimization
 Matrix Analysis,1985, Cambridge University Press
 Appropriate learning rates of adaptive learning rate optimization algo-rithms for training deep neural networks,2021, IEEE Transactions on Cybernetics
 Adam: A method for stochastic optimization,2015, In Proceedingsof The International Conference on Learning Representations
 Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence,2021, In Proceedings of the 24th Interna-tional Conference on Artificial Intelligence and Statistics
 Adaptive gradient methods with dynamicbound of learning rate,2019, In Proceedings of The International Conference on Learning Representa-tions
 Optimizing neural networks with Kronecker-factored approxi-mate curvature,2015, In Proceedings ofMachine Learning Research
 Stochastic opti-mization for performative prediction,2020, In Advances in Neural Information Processing Systems
 Robust stochasticapproximation approach to stochastic programming,2009, SIAM Journal on Optimization
 Some methods of speeding up the convergence of iteration methods,1964, USSR Com-putational Mathematics and Mathematical Physics
 On the convergence of Adam and beyond,2018, InProceedings of The International Conference on Learning Representations
 A stochastic approximation method,1951, The Annals of Mathe-matical Statistics
 Learning representations byback-propagating errors,1986, Nature
 Robustness analysis of non-convex stochastic gradient descentusing biased expectations,2020, In Advances in Neural Information Processing Systems
 Descending through a crowded valley-Benchmarking deep learning optimizers,2021, In Proceedings of the 38th International Conference onMachine Learning
 Measuring the effects of data parallelism on neural network training,2019, Journal ofMachine Learning Research
 On the importance of initial-ization and momentum in deep learning,2013, In Proceedings of the 30th International Conference onMachine Learning
 RMSProp: Divide the gradient by a running average of itsrecent magnitude,2012, COURSERA: Neural networks for machine learning
 Attention is All you Need,2017, In Advances in Neural Infor-mation Processing Systems
 Lipschitz regularity of deep neural networks: analysis andefficient estimation,2018, In Advances in Neural Information Processing Systems
 Which algorithmic choices matter at which batchsizes? Insights from a noisy quadratic model,2019, In Advances in Neural Information ProcessingSystems
 AdaBelief optimizer: Adapting stepsizes by the belief inobserved gradients,2020, In Advances in Neural Information Processing Systems
 Online convex programming and generalized infinitesimal gradient ascent,2003, InProceedings of the 20th International Conference on Machine Learning
 Parallelized stochastic gradientdescent,2010, In Advances in Neural Information Processing Systems
