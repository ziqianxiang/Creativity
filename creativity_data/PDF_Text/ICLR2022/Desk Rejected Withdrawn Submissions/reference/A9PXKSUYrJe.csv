title,year,conference
 Unsupervisedlabel noise modeling and loss correction,2019, In Proc
 A closerlook at memorization in deep netWorks,2017, In Proc
 Mixmatch: A holistic approach to semi-supervised learning,2019, Proc
 Active Bias: Training moreaccurate neural netWorks by emphasizing high variance samples,2017, In Proc
 Avoiding your teacher’smistakes: Training neural netWorks With controlled Weak supervision,2017, arXiv preprintarXiv:1711
 Learning to learn from Weaksupervision by full supervision,2017, In Proc
 Robust loss functions under label noise for deepneural netWorks,2017, In Proc
 Training deep neural-netWorks using a noise adaptationlayer,2017, In Proc
 In Proc,2018, Advances in Neural InformationProcessing Systems (NeurIPS)
 Co-teaching: Robust training of deep neural netWorks With extremely noisy labels,2018, InProc
 Deep self-learning from noisy labels,2019, In Proc
 Using trusted data to traindeep netWorks on labels corrupted by severe noise,2018, In Proc
 Local intrinsic dimensionality I: An extreme-value-theoretic foundation for sim-ilarity applications,2017, In Proc
 MentorNet: Learning data-driven curriculum for very deep neural netWorks on corrupted labels,2018, In Proc
 Dividemix: Learning With noisy labels as semi-supervised learning,2020, arXiv preprint arXiv:2002
 Learningfrom noisy labels with distillation,2017, In Proc
 Soseleto: A unified approach to transfer learning and training withnoisy labels,2018, arXiv preprint arXiv:1805
 Classification with noisy labels by importance reweighting,2015, IEEETransactions on Pattern Analysis and Machine Intelligence (TPAMI)
 Curriculum loss: Robust learning and generalization against labelcorruption,2020, In Proc
 Decoupling” when to update” from” how to update”,2017, InProc
 SELF: Learning to filter noisy labels with self-ensembling,2020, In Proc
 Makingdeep neural networks robust to label noise: A loss correction approach,2017, In Proc
 A study of gaussian mixture models of color and texturefeatures for image classification and segmentation,2006, Pattern Recognition (PR)
 Training deep neural networks on noisy labels with bootstrapping,2015, In Proc
 Learning to reweight examples forrobust deep learning,2018, In Proc
 Meta-Weight-Net: Learning an explicit mapping for sample weighting,2019, In Proc
 SELFIE: Refurbishing unclean samples for robustdeep learning,2019, In Proc
 Joint optimization frame-work for learning with noisy labels,2018, In Proc
 Learningfrom noisy large-scale datasets with minimal supervision,2017, In Proc
 Symmetric cross en-tropy for robust learning with noisy labels,2019, In Proc
 Combating noisy labels by agreement: Ajoint training method with co-regularization,2020, In Proc
 Learning from massive noisylabeled data for image classification,2015, In Proc
 Learning from massive noisylabeled data for image classification,2015, In Proc
 Searching to exploit memorizationeffect in learning with noisy labels,2020, In Proc
 Understanding deeplearning requires rethinking generalization,2017, In Proc
 mixup: Beyond empir-ical risk minimization,2018, In Proc
 Learning withfeature-dependent label noise: A progressive approach,2021, In Proc
 Distilling effectivesupervision from severe label noise,2020, In Proc
 Distilling effectivesupervision from severe label noise,2020, In Proc
 Con-trast to divide: Self-supervised pre-training for learning with noisy labels,2021, arXiv preprintarXiv:2103
