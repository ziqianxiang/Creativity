title,year,conference
 Onlineknowledge distillation via collaborative learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR)
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Paraphrasing complex network: Network compressionvia factor transfer,2018, In S
 On information and sufficiency,1951, The annals of mathemati-cal statistics
 Pruning filters forefficient convnets,2016, arXiv preprint arXiv:1608
 Hrank: Filter pruning using high-rank feature map,2020, In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition
 Learn-ing efficient convolutional networks through network slimming,2017, In Proceedings of the IEEEInternational Conference on Computer Vision
 Distilling knowledge via knowledgereview,2021, In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 ImageNetLarge Scale Visual Recognition Challenge,2015, International Journal of Computer Vision (IJCV)
 Mo-bilenetv2: Inverted residuals and linear bottlenecks,2018, In Proceedings of the IEEE conference oncomputer vision and pattern recognition
 Using small proxy datasets to accelerate hyperparameter search,2019, arXivpreprint arXiv:1906
 Efficientnet: Rethinking model scaling for convolutional neuralnetworks,2019, arXiv preprint arXiv:1905
 Fbnetv2: Differentiableneural architecture search for spatial and channel dimensions,2020, In CVPR
 Gate decorator: Global filter pruningmethod for accelerating deep convolutional neural networks,2019, arXiv preprint arXiv:1909
 Deep mutual learning,2018, InProceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition
 Discrimination-aware channel pruning for deep neural networks,2018, arXiv preprintarXiv:1810
