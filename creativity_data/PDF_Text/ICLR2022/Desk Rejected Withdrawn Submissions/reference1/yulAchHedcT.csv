title,year,conference
 Knapsack pruningwith inner distillation,2020, arXiv:2002
 Survey and critique of techniques forextracting rules from trained artificial neural networks,1995, Knowledge-Based Systems
 Random forests,2001, Machine Learning
 Bagging predictors,1996, Machine Learning
 Classification and Re-gression Trees,1984, Wadsworth
 XGBoost: A scalable tree boosting system,2016, In Proc
 Using sampling and qUeries to extract rUles from trained neUralnetworks,1994, In Proc
 Model compression and hardwareacceleration for neUral networks: A comprehensive sUrvey,2020, 108(4):485-532
 Knowledge discovery via mUltiple models,1998, Intelligent Data Analysis
 LIBLINEAR: Alibrary for large linear classification,2008, J
 A decision-theoretic generalization of on-line learning and an applicationto boosting,1997, J
 Greedy fUnction approximation: A gradient boosting machine,2001, Annals ofStatistics
 Knowledge distillation: AsUrvey,2021, Int
 The treeensemble layer: Differentiability meets conditional computation,2020, In Hal DaUme In and AartiSingh (eds
 Deep residual learning for image recogni-tion,2016, In Proc
 Optimal selection of matrix shape and decompo-sition scheme for neural network compression,2021, In Proc
 Deep neuraldecision forests,2015, In Proc
 Gradient-based learning applied todocument recognition,1998, Proc
 Pruning and quantization for deepneural network acceleration: A survey,2021, arXiv:2101
 HRank: Filter pruning using high-rank feature map,2020, In Proc
 Blockout: Dynamic model selection forhierarchical deep networks,2016, In Proc
 Scikit-learn: Machine learning in Python,2011, J
 Model compression via distillation and quanti-zation,2018, In Proc
 of the 5th Int,2017, Conf
 Very deep convolutional networks for large-scale imagerecognition,2015, In Proc
 Convolutional networks with adaptive inference graphs,2018, In Vit-torio Ferrari
 NBDT: Neural-backed decision tree,2021, In Proc
 Two-stepquantization for low-bit neural networks,4376, In Proc
 PocketFlow: An automated framework for compressing and accelerating deepneural networks,2018, In NIPS Workshop on Compact Deep Neural Network Representation withIndustrial Applications (CDNNRIA)
 Multi-class AdaBoost,2009, Statistics and ItsInterface
 Neural network distiller: aPython package for DNN compression research,2019, arXiv:1910
