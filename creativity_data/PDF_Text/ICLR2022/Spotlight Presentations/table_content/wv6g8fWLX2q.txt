Table 1: Forecasting performance on PeMSD3, PeMSD4 and PeMSD8, along with results on sta-tistical significance. Standard deviations are suppressed for the sake of room and are reported inTable 8 in Appendix E.
Table 2: 15-day ahead forecasting results (RMSE) onCOVID-19 hospitalizations in CA and TX.
Table 3: Forecasting performance (MAPE in %) on Ethereum networks (left panel) and the TAMP-S2GCNets ablation study on PeMSD4 and PeMSD8 (right panel).
Table 4: MAPE (%) (standard deviation) of TAMP-S2GCNets with single- and multifiltrations(left panel) and MAPE (%) and computation time for TAMP-S2GCNets with our DEPS, MP-Iof Carriere & BlUmberg (2020) and MP-L of ViPond (2020) (right panel).
Table 5: The main symbols and definitions in this paper.
Table 6: Summary of datasets used in time series forecasting task.
Table 7: The output dimensions of spatial graph convolutional layer (QSpa), supragraph diffusionconvolutional layer (QSup), and spatio-temporal feature transformation (QFT) on different datasets.
Table 8: Forecasting performance and standard deviations on PeMSD3, PeMSD4 and PeMSD8datasets. All models are re-run using the original authorsâ€™ code.
Table 9: The TAMP-S2GCNets ablation study on Golem (MAPE) and CA. Here *, **, *** denotesignificant, statistically significant, highly statistically significant results.
Table 10: MAPE (in%) (standard deviation) of TAMP-S2GCNets with single filtration, filtrationensemble, and multifiltration persistence.
Table 11: MAPE (%) and computational time (in second) on Decentraland and Golem for TAMP-S2GCNets with our DEPS and MP-I of Carriere & BlUmberg (2020) and MP-L of ViPond (2020),as MP summaries.
Table 12: Types of multifiltration for all datasets.
Table 13: Ablation study of the normalized self-adaptive adjacency matrix on PeMSD4.
Table 14: Ablation study of the normalized self-adaptive adjacency matrix on PeMSD8.
Table 15: Ablation study on global average pooling and global max pooling. Here * denotes signif-icant results.
Table 16: Average training time (per epoch in seconds) for our TAMP-S2GCNets and 4 baselineson PeMSD4 dataset, where ** denotes statistically significant results.
Table 17: Average training time (per epoch in seconds) for our TAMP-S2GCNets and 4 baselineson Decentraland dataset, where *** denotes highly statistically significant results.
Table 18: Computational complexity and running time (in seconds) comparison on Decentralanddataset.
