Table 1: Test accuracies on sMNIST, psMNIST and nCIFAR-10, where M denotes the total numberof parameters of the corresponding model. Results of other models are taken from the respectiveoriginal paper referenced in the main text, except that the results for LSTM are taken from Helfrichet al. (2018), for GRU from Chang et al. (2017) and the results indicated by * are added by us.
Table 2: Test accuracies on EigenWorms using 5 re-trainings of each best performing network (basedon the validation set), where all other results are taken from Rusch & Mishra (2021b) except that theNRDE result is taken from Morrill et al. (2021) and the results indicated by * are added by us.
Table 3: Test L2 error on heart-rate prediction using PPG data. All results are obtained by runningthe same code and using the same fine-tuning protocol.
Table 4: Test L2 error on FitzHugh-Nagumo system prediction. All results are obtained by runningthe same code and using the same fine-tuning protocol.
Table 5: Test accuracies on Google12. All results are obtained by running the same code and usingthe same fine-tuning protocol.
Table 6: Test bits-per-character (bpc) on PTB character-level for single layer LEM and other singlelayer RNN architectures. Other results are taken from the papers cited accordingly in the table,while the results for coRNN are added by us.
Table 7: Test perplexity on PTB word-level for single layer LEM and other single layer RNN archi-tectures. ___________________________________________________________________________________Model	test PerPlexity	# units	# ParamsLiPschitz RNN (Erichson et al., 2021)	115.4	160	76kFastRNN (Kag & Saligrama, 2021)	115.9	256	131kLSTM (Kag & Saligrama, 2021)	116.9	256	524kSkiPLSTM (Kag & Saligrama, 2021)	114.2	256	524kTARNN (Kag & Saligrama, 2021)	94.6	256	524kLEM	72.8	256	524kvery challenging. In this paper, we have proposed Long Expressive Memory (LEM), a novel re-current architecture, with a suitable time-discretization of a specific multiscale system of ODEs (2)serving as the circuit to the model. By a combination of theoretical arguments and extensive empiri-cal evaluations on a diverse set of learning tasks, we demonstrate that LEM is able to learn long-termdependencies while retaining sufficient expressivity for efficiently solving realistic learning tasks.
Table 8: Rounded hyperparameters of the best performing LEM architecture for each experiment.
Table 9: Test L2 error on FitzHugh-Nagumo system prediction.
Table 10: Test accuracies on EigenWorms using 5 re-trainings of each best performing network(based on the validation set), where we train LSTM and LEM with and without chrono intialization,as well as LEM WithOUt chrono initialization but With tuned ∆t.__________________________________Model	test accuracy	# units	# params	chrono	tuning ∆tLSTM	38.5% ± 10.1%	32	5.3k	NO	/LSTM	82.6 % ± 6.4%	32	5.3k	YES	/LEM	57.9% ± 7.7%	32	5.3k	NO	NOLEM	88.2% ± 6.9%	32	5.3k	YES	NOLEM	92.3% ± 1.8%	32	5.3k	NO	YESWe test the chrono initialization for LEM on the EigenWorms dataset, where we train LEM (withouttuning ∆t, i.e., setting ∆t = 1), with and without chrono initialization. We provide the results inTable 10, where we show again the results of LSTM with and without chrono initialization as wellas the LEM result with tuned ∆t and without chrono initialization from Table 2 for comparison. We17Published as a conference paper at ICLR 2022see from Table 10 that when ∆t is fixed to 1, the chrono initialization significantly improves theresult of LEM. However, if we tune ∆t, but do not use the chrono initialization, we significantlyimprove the performance of LEM again. We further remark that tuning ∆t as well as using chronoinitialization for LEM does not improve the results obtained with simply tuning ∆t in LEM. Thus,we conclude that chrono initialization can successfully be adapted to LEM. However, tuning ∆t
Table 11: Test accuracies on EigenWorms using 5 re-trainings of each best performing network(based on the validation set) for LSTMs with ∆t-scaled input and forget gates, as well as LSTMswith sub-sampling routines, baseline LSTM and LEM.
