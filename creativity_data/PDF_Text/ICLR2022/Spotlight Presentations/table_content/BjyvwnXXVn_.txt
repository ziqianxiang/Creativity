Table 1: ImageNet classification accuracy ofa straightforward inattentive token removal for a trainedDeiT-S (Touvron et al., 2021a). The inattentive tokens are directly removed based on the attentionfrom the class token to other tokens at the 4th, 7th and 10th layers.
Table 2: Comparison on the two variants of EViT on DeiT-S (Touvron et al., 2021a). The values ofthe Top-1 and Top-5 accuracy are averaged over three independent trials. The number behind ± isthe standard deviation of the three trials. The number in blue is the gap of the corresponding valuew.r.t. the baseline DeiT-S.
Table 3: Results of EViT on LV-ViT-S	Table 4: Results of training EViT-DeiT-S with a keep rate of 0.7 for different epochs.
Table 5: Results of training/finetuning on high resolution images. EViT-DeiT-S and EViT-LV-ViT-Shave a throughput/MACs comparable to the baselines while achieving better recognition accuracyon ImageNet. The numbers before and after ↑ indicate the image size in the 300-epoch training and100-epoch finetuning, respectively.
Table 6: Results of training EViT-DeiT-S and EViT-DeiT-B using DeiT-S as an oracle. Training forlonger epochs continues to benefits the EViT in efficiency regime.
Table 7: Comparisons on EViT and DynamicViT (Rao et al., 2021). X indicates the model isinitialized with a pre-trained DeiT-S. For fair comparison, the throughput is measured on the samemachine with the same setting using a maximum batch size.
Table 8: Results of EViT on DeiT-BKeep rate	Top-1 Acc (%)	Top-5 Acc (%)	Throughput (images/s)	MACs (G)DeiT-B	81.8	95.6	1295	17.60.9	81.8 (-0.0)	95.6(-0.0)	1441 (+11%)	15.3 (-13%)0.8	81.7 (-0.1)	95.4 (-0.2)	1637 (+26%)	13.2 (-25%)0.7	81.3 (-0.5)	95.3 (-0.3)	2053 (+59%)	11.5 (-35%)0.6	80.9 (-0.9)	95.1 (-0.5)	2177 (+68%)	10.0 (-43%)0.5	80.0 (-1.8)	94.5 (-1.1)	2482 (+92%)	8.7 (-51%)Table 9: Comparisons of the two variants (i.e., vanilla inattentive token removal and inattentivetoken fusion) of EViT-DeiT-S on higher resolution images.
Table 9: Comparisons of the two variants (i.e., vanilla inattentive token removal and inattentivetoken fusion) of EViT-DeiT-S on higher resolution images.
Table 10: The performance of EViT-DeiT-S with different attentive token identification strategies.
Table 11: Training time for 300-epoch training of DeiT-S and EViT-DeiT-S with different keep rates.
Table 12: Results of training/finetuning on high resolution images. The numbers before and after ↑indicate the image size in the 300-epoch training and 50-epoch finetuning, respectively.
Table 13: The performances of EViT-DeiT-S with different of keep rates and reorganization loca-tions (R.L.). For fair comparison, we keep the variants having the same level of computational cost(i.e., MACs) as our EViT baseline in the first row by tuning the keep rates.
Table 14: The performance of EViT-DeiT-S equipped with the DINO attention. The “MACs” in thetable is the computational cost of the EViT-DeiT-S model and “Combined MACs” is the sum of theMACs of both EViT-DeiT-S and the DINO model (i.e., a DeiT-S) which is used to obtain the DINOattention.
Table 15: An ablation of token removal strategy in EViT-DeiT-S. EViT-topk is the method of keepingthe tokens with the largest [CLS]-to-token attention values. EViT-random denotes the method ofrandomly keeping the tokens, while EViT-mink is the method of keeping the tokens with the smallest[CLS]-to-token attention values.
