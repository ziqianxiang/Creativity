Table 1: Perceiver IO on language: results on the GLUE benchmark (Avg. = average performance, higheris better). Following Devlin et al. (2019) we exclude the WNLI task. We use Pearson correlation on STS-B,Matthews correlation on CoLa and accuracy on the remaining tasks. BERT Base (test) performance is reportedfrom Devlin et al. (2019). SPS = train-time steps per second. M = # inputs and N = # latents.
Table 2: Multitask Perceiver IO. Re-sults use the same metric as Tab. 1(higher is better).
Table 3: Optical Flow evaluated on Sintel (Butler et al.,2012) and KITTI with average end-point error (EPE) (loweris better). Baselines are reported from Sun et al. (2021).
Table 4: Multimodal autoencoding results. Higheris better for accuracy and PSNR.
Table 5: Details of each of the tasks we use to evaluate Perceiver IO here. The positional and task embeddingsappended to inputs for each case are listed in Tab. 6.
Table 6: Table best viewed on a screen. The structure and size of the positional and task embeddings used toconstruct Perceiver IO’s encoder key-value inputs and decoder query inputs, for each domain described in themain text. “[x, y]” indicates that x’s and y’s features are concatenated, while “x + y” indicates that x’s and y’sfeatures are added to produce the full featurization. “FF” = Fourier features as in Jaegle et al. (2021).
Table 7: Results on ImageNet image classification (top-1 accuracy, higher is better). “-” indicates a valuewe could not find reported in the literature. We did not extensively tune our models for efficiency on imageclassification - the primary focus of this work is generality, rather than speed on images - Perceiver IO usescomparable FLOPs to attention-based image classification models, especially for the more compact configurationB pretrained on JFT. The positional encoding does not significantly change model FLOPs.
Table 8: ImageNet model training speed. The model used for pretraining is faster because it uses only 16process modules. We did not reimplement baselines, so we report only the training speed of Perceiver andPerceiver IO models.
Table 9: We evaluate Perceiver IO on StarCraft II by using it to replace the well-tuned Transformer entityencoder. Perceiver IO matches the performance of the original Transformer despite using fewer FLOPs andparameters and requiring essentially no tuning. Note that the training steps/sec of the overall system does notchange because the entity encoder is not the speed bottleneck.
Table 10: Perceiver IO on multimodal (audio + video) AudioSet classification (mAP = mean average precision,higher is better). All models have similar runtimes despite FLOPs differences because the bottleneck is dataloading and preprocessing rather than model forward/backward passes.
Table 11: Perceiver IO architecture details for language experiments.
Table 12: Hyperparameters for masked language modelling (MLM) pre-training experimentsF.4 GLUE FinetuningFollowing Devlin et al. (2019), we specify a fixed-size hyperparameter grid and select the best devperformance across that grid for each task independently (Tab. 12). The full GLUE results are shownin Tab. 14. Following Devlin et al. (2019) we exclude the WNLI task. We use accuracy for all tasksexpect STS-B and CoLA where we use Pearson correlation and Matthews correlation respectively.
Table 13: Hyperparameters for GLUE finetuning experiments. We sweep over the values in brackets.
Table 14: Full GLUE results (higher is better). The first 3 models use SentencePiece tokens, the latter 3 useUTF-8 bytes directly.
Table 15: Ablation on the UTF-8 Bytes Perceiver IO latent width versus depth.
Table 16: Ablated Optical Flow results (end-point error, lower is better). The top Perceiver IO results show theconfiguration from the main paper. We ablate 1) patch size for the context surrounding each pixel, 2) whetherthe two frames are concatenated or input separately to the Perceiver, 3) whether the inputs and queries aredownsampled by a factor of 4 using a convolution, and then subsequently upsampled with RAFT, and finally a thenumber of self-attention modules (depth) and number of elements in the latent array, resulting in a bottom-rownetwork which is substantially less expensive than the original model.
Table 17: Additional details of the model used for Multimodal autoencoding.
