Table 1: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weightsare learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) andthe dynamic variant (D-DW-Conv.) in terms of the patterns of sparse connectivity, weight sharing, and dynamicweight. Please refer to Figure 1 for the connectivity pattern illustration.
Table 2: ImageNet classification comparison for ResNet, Mixer and ResMLP, ViT and DeiT, Swin (SwinTransformer), DW-Conv. (depth-wise convolution), and D-DW-Conv. (dynamic depth-wise convolution).
Table 3: Comparison results on COCO object detection and ADE semantic segmentation.
Table 4: Effects of weight sharing across channels and positions. The results are reported on theImageNet top-1 accuracy. SC = Sharing across channels. SP = sharing across positions.
Table 5: Comparison of different dynamic weight manners. The results are reported on the ImageNet top-1accuracy. Shifted window sampling (Win. samp.) means the way in Swin Transformer and sliding meansthe densely-sampling manner. The result of Sliding local MLP is from (Liu et al., 2021b). homo. dyna. =homogeneous dynamic weight. inhomo. dyna. = inhomogeneous dynamic weight.
Table 6: Comparison with concurrent works on ImageNet classification with tiny models.				#param.	FLOPs	top-1 acc.
Table 7: The comparison of attention, local MLP (non-dynamic version of local attention, the attention weightsare learned as static model parameters), local attention, convolution, depth-wise convolution (DW-Conv.) and thedynamic variant (D-DW-Conv.), as well as MLP and MLP variants in terms of the patterns of sparse connectivity,weight sharing, and dynamic weight. ^Spatial-mixing MLP (channel-separable MLP) corresponds to token-mixerMLP. Q X 1 Conv. is also called point-wise (spatial-separable) MLP. [The weights might be shared within eachgroup of channels. Please refer to Figure 1 for the connectivity pattern illustration.
Table 8: Architectures details of Swin Transformer and depth-wise convolution-based network (DWConv.) for the tiny model. The architectures for the base model can be easily obtained.
Table 9: ImageNet classification comparison for ResNet, HRNet, Mixer and ResMLP and gMLP, ViT andDeiT, Swin (Swin Transformer), DW-Conv. (depth-wise convolution), and D-DW-Conv. (dynamic depth-wiseconvolution). â†‘ means that ResNet is built by using two 3 X 3 convolutions to form the residual units. Table 7presents the comparison for representative modules in terms of spare connectivity, weight sharing and dynamicweight.
Table 10: Comparison on ImageNet-1K classification with ImageNet-22K pre-training.
Table 11: Comparison results on COCO object detection and ADE semantic segmentation withImageNet-22k pre-training.
Table 12: Exploring normalization schemes of Swin Transformer and depth-wise convolution basednetworks (DW Conv.) for the tiny model. The results are reported on the ImageNet top-1 accuracy.
Table 13: Comparison between local attention and depth-wise convolution in VOLO (Yuan et al.,2021c) and SVT (Chu et al., 2021a) architecture. Results are reported on ImageNet classificationwith tiny model.
Table 14: Retrain on larger images.
Table 15: Cooperate with SE.
