Table 1: Results for the Bouncing Balls (Top) and Reacher (Bottom) environments. Center: Foreach reward model, we compute each distance metric×1000 from GROUND TRUTH (GT). Thecoverage distribution is indicated by the policy (πuni corresponding to the distribution resulting fromuniformly random actions, ∏* to that from expert actions). Right: The average episode return and itsstandard error of the policy trained on the corresponding reward model. Values are averaged over 5executions of all steps (data collection, reward learning, reward evaluation) across different randomseeds (see Appendix A.1.7 for standard errors of distances). Distance values that are inversely corre-lated with episode return are desirable (i.e., increasing distance within a column should correspondwith decreasing episode return). Additionally, for hand-designed models that are equivalent to GT(SHAPING, FEASIBILITY), and for well-fit learned models (REGRESS), lower is better. Learnedmodels are fit to SHAPED, and may produce higher episode return than those fit to GT as a result.
Table 2: PPO parameters. Superscripts indicate Bouncing Balls (BB) or Reacher (R) environments.
Table 3: The sizes of the datasets used for reward learning and evaluation for each environment.
Table 4: Parameters for learning transition models. For information on datasets used for trainingsee section A.1.3. Superscripts indicate parameters specific to Bouncing Balls environment (BB) orReacher environment (R).
Table 5: Parameters for reward learning algorithms. For information on datasets used for training seesection A.1.3. Superscripts indicate parameters specific to preference learning (p), OOD regression(OOD), Bouncing Balls environment (BB), Reacher environment (R).
Table 6: EPIC parameters.
Table 7: DARD parameters. Superscripts indicate parameters specific to the Bouncing Balls (BB)or Reacher (R) environments.
Table 8: Standard error values for Bouncing Balls environment.
Table 9: Standard error values for Reacher environment.
Table 10: Top: Results for the original Point Maze environment. Bottom: Results for the PointMaze environment when using a larger timestep. See the caption of table 1 for how to interpret theseresults. DARD distances are much closer to those of EPIC in the large-timestep case, indicatingthat the differences in rewards assigned to nearby transitions has a significant impact on DARDperformance. Parameters used for reward learning match those of Gleave et al. (2020). We wereunable to exactly reproduce the episode returns of the policies trained on learned reward models.
