Table 1: Comparison of this work and existing work on theoretical analyses of deep learning for highdimensional data. a = (ai)∞=ι is a smoothness parameter, a := (P∞=ι a-1)-1, V = Qip — 1/2)+,S = aι =…=ad and D is the dimensionality of low dimensional structure.
Table 2: Notation listnotation	definitionn (Xi yi) Dn = (Xi ,yi)n=ι fo a = (ai)∞=1 S = (si)i=1 ψl(X) = Q∞=1 ψli (Xi) δs(f) k∙k2 k∙kPχ Fp,θ([0,1]∞) Y(S) Y(S) = ha, si γ(s) = max{aiSi : i ∈ N} η Ψ(L,W,S,B) P (L0,B0,W 0,C 0,L,W,S,B) I (T,Y) dmax(T, Y) fmax(T, Y) V α(Y) G(T,Y)		sample size i-th observation (Xi： input, yi： output) training data the true function smoothness with respect to each coordinate frequency with respect to each coordinate trigonometric orthonormal basis functions basis function expansion of f for l ∈ Z∞ such that b2si-1C ≤ |li | < 2si L2-norm with respect to the uniform distribution (kf k2 := P∫f(X )2dλ∞(X))		 L2-norm with respect to PX (kfk PX := √Eχ^Pχ [f (X)2]) Y-smooth function class penalty on each frequency component mixed smoothness anisotropic smoothness ReLU activation function set of fully connected networks with depth L, width W, sparsity level S, norm bound B set of dilated CNNs with depth L0, filter width W0, channel size C0 accompanied with an FNN in Ψ(L,W,S,B) the set of features contributing a frequency component S with Y(S) < t |I(T, y) |: axial complexity maXs∈N∞: γ(s)≤τ maxi∈N s，frequency direction complexity iP - 1 1 P∞ si suPs∈N∞ ^=Γ Es∈N∞: γ(s)<T 2s	A Connection to finite dimensional input settingWe can easily see that the analysis in our paper can be directly applied to a setting where the inputis finite dimensional (say, d-dimensional). LetJd := {s ∈ N0∞ : si = 0 (i = d + 1, . . . )} ,and suppose that γ(s) : N∞ → R>0 ∪ {∞} satisfies γ(s) < ∞ (s ∈ Jd) and γ(s) = ∞ (s ∈/ Jd).
