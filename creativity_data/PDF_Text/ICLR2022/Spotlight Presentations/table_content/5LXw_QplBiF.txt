Table 1: Language modeling results on PTB, measured by perplexity and SG score. The setting∣Q| = 1, ∣Γ∣ = 2 represents minimal capacity in the (R)NS-RNN models and is meant to serve asa baseline for the other settings. The other two settings are meant to test the upper limits of modelcapacity before computational cost becomes too great. The setting |Q| = 1, ∣Γ∣ = 11 represents thegreatest number of stack symbol types we can afford to use, using only one PDA state. We selected thesetting ∣Q∣ = 3, ∣Γ∣ = 4 by increasing the number of PDA states, and then the number of stack symboltypes, until computational cost became too great (recall that the time complexity is O(∣Q∣4∣Γ∣3), soadding states is more expensive than adding stack symbol types).
Table 2: Wall-clock execution time for each model on the marked reversal task, measured in secondsper epoch of training (averaged over all epochs). The speed of the NS models is roughly the same;there is some variation here due to differences in training data and GPU model.
Table 3: Language modeling results on PTB, measured by perplexity and SG score, with additionalexperiments included.
Table 4: SG scores broken down by circuit. Agr. = Agreement, Lic. = Licensing, GPE = Garden-PathEffects, GSE = Gross Syntactic Expectation, CE = Center Embedding, LDD = Long-DistanceDependencies.
