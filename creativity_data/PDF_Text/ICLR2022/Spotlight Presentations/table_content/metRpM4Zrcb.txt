Table 1: Class-incremental results on CIFAR100 and ImageNet-Subset with different number ofincremental tasks (N). We report average incremental accuracy for all methods.
Table 2: Results on 20-SPUt CIFAR100 and 20-SPUt miniImageNet. (*) We re-implement PNN andAPD with our network architecture. Analysis on memory is provided in Appendix C.10.
Table A: Statistics of 10-Split CIFAR100.
Table B: Statistics of 20-Split CIFAR100and 20-Split miniImageNet.
Table C: Statistics of 5-Split MNIST.
Table D: Results on 5-Split MNIST.
Table E: Class-incremental results on 10-Split CIFAR100.
Table F:	Class-incremental results on 10-Split ImageNet-Subset.
Table G:	Average incremental accuracy on CUBS and Flowers datsetsMethod	CUBS	FlowersLwF-E* (Li & Hoiem, 2017)	69.8	87.2EWC-E* (Kirkpatricketal., 2017a)	69.7	85.9MAS-E* (Aljundi et al., 2018b)	68.5	84.7SDC (Yu et al., 2020)	70.0	86.8Ours (Base)	69.3	86.9w/ Ew = 2	72.1	89.3Under the class-incremental setting, we compare the average incremental accuracy with other benchmarks. Onboth datasets, as shown in the table above, our method produces the best results.
Table H:	Ablation study on different metrics in inter-task ensemble.
Table I:	Inference time comparisons on CIFAR100.
