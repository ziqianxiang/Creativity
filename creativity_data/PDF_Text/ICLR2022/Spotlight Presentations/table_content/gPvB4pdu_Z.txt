Table 1: Testing performance on benchmark datasets and medical datasets. The percentage numberis the Second row denotes the imbalanced ratio (Cf the text).______________________Datasets		For AUC Maximization			Datasets	For AUC Maximization		-imratio	1%	10%	30%		Method	AUC	CE	0.742±0.003	0.917±0.006	0.957±0.001		CE	0.879±0.008	AUCsc	0.753±0.003	0.915±0.002	0.964±0.003		AUCsc	0.868±0.006	AUC-CE	0.770±0.007	0.939±0.004	0.974±0.003		AUC-CE	0.880±0.005CAT vs DOG	TS-DRW	0.750±0.009	0.914±0.003	0.961±0.001	Melanoma	TS-DRW	0.878±0.007	TS-DEC	0.754±0.010	0.918±0.003	0.963±0.001		TS-DEC	0.877±0.005	CT (AUC)	0.789±0.008	0.946±0.002	0.977±0.001		CT (AUC)	0.900±0.002	CE	0.689±0.003	0.901±0.002	0.944±0.001		Ce	0.892±0.001	AUCsc	0.728±0.002	0.905±0.002	0.946±0.001		AUCsc	0.899±0.002f^'ττ7Λ pin	AUC-CE	0.735±0.003	0.928±0.001	0.957±0.001		AUC-CE	0.902±0.002CIFAR10	TS-DRW	0.708±0.002	0.896±0.002	0.946±0.003	CheXpert	TS-DRW	0.900±0.002	TS-DEC	0.707±0.002	0.897±0.002	0.944±0.001		TS-DEC	0.897±0.001	CT (AUC)	0.739±0.004	0.935±0.001	0.964±0.001		CT (AUC)	0.909±0.003	Ce	0.655±0.005	0.819±0.004	0.885±0.004		CE	0.949±0.001	AUCsc	0.665±0.005	0.805±0.017	0.887±0.007		AUCsc	0.929±0.001CTT 1 ∩	AUC-CE	0.668±0.007	0.836±0.006	0.905±0.001	TrYrΛV"∖ Λ"-l	AUC-CE	0.957±0.001STL10	TS-DRW	0.655±0.004	0.803±0.013	0.887±0.002	DDSM+	TS-DRW	0.942±0.003
Table 2: Left: S1 6= S2 vs S1 = S2, right: β0 = 1 vs β0 < 1 in Algorithm 1. Note that we tunek ∈ {1, 2, 3} for for the left table and fix k = 1 for the right table. left (S1 6= S2) vs right (β0 ≤ 1)verifies that tuning k is helpful.
Table 3: Description of datasets for classification tasks.
Table 4: Testing performance on medical datasets.
Table 5: Achieved testing AUC for each method after training ResNet20 for 1000 seconds.
Table 6: Different Adam-style methods and their satisfactions of Assumption 1					method	update for ht	Additional assumption		cl and cu	SHB	ht(∙) = 1,G = O	-		cl = 1, cu =	1Adam	Z2,t+1 = (1 - βt0)Z2,t + βt0Ot2	∣Qt∣∞≤ G	cl	=g+g0 , cu	_	1 一用AMSGrad	Z02,t+1 = (1 -βt0)Z02,t +βt0Ot2 Z2,t+1 = max(Z2,t, Z02,t+1)	kOtk∞ ≤ G	cl	=G+1G0 , cu	_ ɪ 一G0AdaFom (AdaGrad)	z2,t+1 = t⅛ Ptt=O Ot	kOtk∞ ≤ G	cl	=g+go , cu	_ ɪ 一G0Adabound	Z02,t+1 = (1 - βt0)Z02,t + βt0 Ot2 z2,t+1 = π[1∕cU,1∕c2] [z2,t+1],	GO 二	0-		cl = cl , cu =	cumapping. Then we haveEtkZt+ι - h(xt)k2 ≤ (1 - βt)∣% - h(xt-1)k2 + 2β2Et∣Qh(xt) - h(xt)k2 + L2kxt - XtTk2,βt(8)where Et denotes the expectation conditioned on all randomness before Oh(xt).
Table 7: Testing performance on benchmark datasets and ImageNet-LT. The percentage number isthe second row denotes the imbalanced ratio (cf the text). All experiments on benchmark datasets areaveraged over three runs with different random seeds. The network structure used in all experimentsisResNet32. __________________________________________________________Datasets	For Accuracy Maximization		Method	1%	10%CIFAR10 (LT)	CE LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.713±0.001	0.876±0.002 0.744±0.003	0.872±0.002 0.780±0.003	0.879±0.000 0.758±0.016	0.842±0.004 0.787±0.001	0.883±0.001CIFAR100 (LT)	Ce LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.396±0.002	0.572±0.000 0.407±0.004	0.559±0.003 0.427±0.006	0.579±0.001 0.403±0.003	0.536±0.001 0.430±0.005 0.585±0.002STL10 (LT)	Ce LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.441±0.017	0.639±0.009 0.440±0.010 0.641±0.008 0.458±0.006	0.651±0.017 0.457±0.013	0.629±0.009 0.488±0.012 0.662±0.005ImageNet (LT)	-CE [Jamal et al. (2020)] CB-CE [Cui et al. (2019)] 	CT (CB-CE)	0.2526 0.2659 0.2661D Compositional Training with Class Weighted lossIn this section, we extend the compositional training method to deep learning with class weightedloss. Let LCW(w) denote a class weighted loss written as:1nLCW(W) = n Ipyi'(w; Xi, yi),	(23)where pyi ∈ (0, 1) denotes a weight assigned to the i-th example that depends on the class the databelongs to. There are different methods for determining the class-level weight. A simple methodis to set pi according to the reciprocal of its corresponding class size, i.e., pyi = 1∕nyi . Recently,Cui et al. (2019) proposed an improved variant of class-weighted loss by using an effective numberof samples per-class instead of the class size to compute the individual weight, i.e., pyi
