Table 1: Main Inefficiency Results, comparing (Bellotti, 2021) (Bel, trained with THRL) and Con-fTr (trained with THRLP) using THR or APS at test time (with α=0.01). We also report improve-ments relative to the baseline, i.e., standard cross-entropy training, in percentage in parentheses.
Table 2: Ensemble Results and Lower Confidence Levels α: Left: “Conformalization” of ensem-bles using a 2-layer MLP trained on logits, either normally or using ConfTr. The ensemble contains18 models with accuracies in between 75.10 and 82.72%. Training a model on top of the ensembleclearly outperforms the best model of the ensemble; using ConfTr further boosts Ineff. Right: Theinefficiency improvements of Tab. 1 generalize to lower confidence levels α on EMNIST, althoughConfTr is trained with α=0.01.
Table A: Used Datasets: Summary of train/calibration/test splits, epochs and models used on alldatasets in our experiments. The calibration set is usually less than 10% of the training set. Onmost datasets, the test set is roughly two times larger than the calibration set. When computingrandom calibration/test splits for evaluation, see text, the number of calibration and test examplesstays constant. * On Camelyon, we use features provided by Wilder et al. (2020) instead of theoriginal images. ** For EMNIST, we use a custom subset of the “byClass” split.
Table B: Used ConfTr Hyper-Parameters with and without Lclass for THRLP during training andThr at test time. The hyper-parameters for APS at test time might vary slightly from those reportedhere. The exact grid search performed to obtained these hyper-parameters can be found in the text.
Table C: Importance of Random Trials: We report coverage and inefficiency with the correspond-ing standard deviation across 10 test (left) and 10 training trials (right). ConfTr was trained usingThrLP if not stated otherwise. For test trials, a fixed model is used. Results for training trials ad-ditionally include 10 test trials, but the standard deviation is reported only across the training trials.
Table D: Hyper-Parameter Ablation on MNIST: For ConfTr without Lclass, we report inefficiencyand accuracy when varying hyper-parameters individually: batch size/learning rate, size weight λ,temperature T and confidence level α. While size weight λ and temperature T have insignificantimpact, too small batch size can prevent ConfTr from converging. Furthermore, the chosen hyper-parameters do not generalize well to higher confidence levels α ∈ {0.1, 0.05}.
Table E: Ablation for CoverTr and ConfTr on MNIST and Fashion-MNIST: We report ineffi-ciency and accuracy for (Bellotti, 2021) (Bel), CoverTr and ConfTr considering various CP methodsfor training and testing. Bel outperforms the baseline when using ThrL, but does not do so for Thron MNIST. CoverTr with Thr or APS during training is challenging, resulting in high inefficiency(mainly due to large variation among training trials, c.f . Tab. C), justifying our choice of THRLPfor ConfTr. Also CoverTr is unable to improve over the Thr baseline. Similar observations hold onFashion-MNIST where, however, CoverTr with Thr or APS was not possible.
Table F: Inefficiency and Accuracy on Multiclass Datasets: Complementing Tab. 1 in the mainpaper, we include results for CoverTr on CIFAR10. Furthermore, we consider training a non-linear2-layer MLP on the ResNet features on CIFAR10, c.f . Sec. F, alongside the ensemble results fromthe main paper. We report inefficiency and accuracy in all cases, focusing on ConfTr in comparisonto Bel. On EMNIST, we additionally consider α = 0.005, 0.001 (for the baseline and ConfTr only).
Table G: Inefficiency and Accuracy on Binary Datasets. Experimental results on the binarydatasets WineQuality, GermanCredit and Camelyon. While we include APS on WineQuality, wefocus on Thr on GermanCredit and Camelyon due to slightly lower inefficiency. However, ThrL,Thr and APS perform very similarly on all tested binary datasets. Generally, ConfTr does not im-prove significantly over the baseline. * On Camelyon, we report the best results without trainingtrials as sub-sampling the 280 training examples is prohibitively expensive.
Table H: Mis-Coverage on MNIST, Fashion-MNIST and CIFAR10: We present inefficiency andmis-coverage for various cases: On MNIST, we consider 2 vs. other classes as well as even vs. oddclasses. In both cases, mis-coverage can be reduced significantly. As in the main paper, however,reducing MisCover0→1 usually increases MisCover1→0 and vice-versa. On Fashion-MNIST, weconsider 6 (“shirt”) vs. other classes. Only on CIFAR10, considering “vehicles” vs. “animals”, mis-coverage cannot be reduced significantly. In particular, we were unable to reduce MisCover1→0 .
