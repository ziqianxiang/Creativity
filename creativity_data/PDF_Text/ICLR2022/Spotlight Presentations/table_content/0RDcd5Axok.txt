Table 1: Parameter-efficient tuning methods decomposed along the defined design dimensions. Here, for clarity,we directly write the adapter nonlinear function as ReLU which is commonly used. The bottom part of the tableexemplifies new variants by transferring design choices of existing approaches.
Table 2: Accuracy on the dev set ofMNLI and SST2. MAM Adapter isproposed in §4.6. Bitfit numbers arefrom Ben Zaken et al. (2021).
Table 4: Results on en-ro dataset.
Table 3: Comparison of different insertion forms for adapters,i.e. sequential adapter (SA) and parallel adapter (PA). We in-clude the results of prefix tuning as a reference point.
Table 6: Comparison of various parameter-efficient tuning methods and the proposed variants. “f” are resultscopied from Lewis et al. (2020) and Liu et al. (2020b). We could not reproduce exactly the same full fine-tuning numbers with the same hyperparameters or even searching them. The reason may be the differentlibraries which the training code is based on - full fine-tuning is very sensitive to training hyperparameters. Forthe most performant methods we run with 3 random seeds and report mean and standard deviation.
Table 5: Results on XSum when using differentcomposition functions. The modified representa-tion is FFN. The bottleneck dimension r = 512for (Scaled) PA and r = 102 for LoRA.
Table 7: Dataset Statistics of the four tasks.
Table 8: Training hyperparameters of parameter-efficient tuning methods on the four tasks. lr and ls representslearning rate and label smoothing respectively.
Table 9: Number of attention or FFN sub- layers in each layer of the pre-trained mod- els.	Table 10: Number of parameters used at each sub-layer for dif- ferent methods.			NWtn		NWffnBART/mBARTLARGE ROBERTaBASE	Prefix Tuning	2ld	-Nttn	3	1	Adapter variants	2rd	2rdNffn	2	1	LoRA	2 X 2rd = 4rd	2 X (rd + 4dr) = 10rdWe compute the number of tunable parameters based on where the tunable module is inserted intoand how it is parameterized. The pretrained-models for summarization or MT have an encoder-decoder structure and each has L layers, whereas RoBERTaBASE for classification tasks only hasL encoder layers. To simplify the computation of tunable parameters, we compute the sum ofparameter used in one encoder layer and one decoder layer as the parameter overhead of one singlelayer of the pre-trained encoder-decoder model. Each layer has Nattn sub-layers and Nffn sub-layers. For the encoder-decoder models, Nattn = 3: the encoder self-attention, the decoder self-attention and the decoder cross-attention. For the classification tasks, RoBERTaBASE only has theencoder self-attention, thus Nattn = 1. We present the number of attention and ffn sub-layersfor different pre-trained models in Table 10. For modifications applied at the attention sub-layers,the number of tunable parameters is computed by ∣θ∣attn = NWWtn X Nattn X L, where NWtndenotes the number of parameters (Wdown or Wup) used for one attention sub-layer. Similarly,the number of tunable parameters for the FFN sub-layers is computed by ∣Θffn = NWffn X Nffn XL. In Table 10, we show the number of parameters for one sub-layer. As we have explained in§4.4, LoRA approximates the update of each weight matrix with a pair of Wdown and Wup, thus
Table 11: Number of tunable parameters of various parameter-efficient tuning methods with BART/MBARTmodels (L = 12) as an example.
Table 12: Performance on the test sets of abstractive summarization (XSum) and WMT EN-RO translation.
