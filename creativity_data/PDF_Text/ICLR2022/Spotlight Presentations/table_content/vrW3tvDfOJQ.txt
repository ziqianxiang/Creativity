Table 1: 25th - 50th - 75th percentiles of the number of episodes necessary for the return averagedwith a 100-episode window to reach the solved score on different environments. IV-DQN showssignificant improvements in sample efficiency when the environment is not exploration-based.
Table 2: Performance at 100K and 200K time steps (100 and 200 episodes) for several roboticsenvironments in OpenAI Gym. The results show the mean and standard error over 25 runs.
Table 3: List and set of hyperparameters that were tuned when testing the algorithm.
Table 4:	Average training time for 1000 steps on walker2d environment.
Table 5:	Average training time per episode on LunarLander-v2 environmentDQN BootstraPDQN SunriseDQN IV-DQN (ours)Runtime/ episode (s)	0.51	1.5	1.53	1.87Figure 8: Mean target variance Predictions for discrete environmentsE	Variance estimationThe difference of performance between IV-RL with sampled ensembles and variance ensembles issometimes significant. Looking at the variance prediction can allow us to better understand thisresult.
