Table 1: Examples of Exponential Family Distributions where ψ(x) and B(x) denote Digamma andBeta function, respectively.
Table 2: Classification results on Sensorless Drive with Categorical target distribution. Best scoresamong all single-pass models are in bold. Best scores among all models are starred.
Table 3: Classification results on CIFAR-10 with Categorical target distribution. Best scores amongall single-pass models are in bold. Best scores among all models are starred. Gray numbers indicatethat R-PriorNet has seen samples from the SVHN dataset during training.
Table 4: Results on the Bike Sharing Dataset with Normal N and Poison Poi target distributions.
Table 5: Regression results on models trained on different UCI datasets with Normal target distribution.
Table 6: Regression results on NYU Depth v2 with Normal target distribution. RMSE is in cm. OODscores on LSUN are reported on the held-out classes ‘classrooms’ (left) and ‘churches’ (right).
Table 7: Batched Inference Time (in ms),NVIDIA GTX 1080 Ti	CIFAR-10 (batch size 4,096)	NYU Depth v2 (batch size 4)Dropout	407.91 ± 5.65	650.96 ± 0.22Ensemble	361.61 ± 5.41	649.78 ± 0.18R-PriorNet	61.83 ± 2.57	-EnD2	61.83 ± 2.57	-PostNet	88.56 ± 0.06	-EvReg	-	129.88 ± 0.75NatPN	75.64 ± 0.04	137.13 ± 0.18NatPE	370.17 ± 0.09	676.74 ± 0.38PostNet - which scales linearly w.r.t. the number of classes since it evaluates one normalizing flowper class. Lastly, NatPN is the only single-pass model that can be used for both tasks.
Table 8: Results on MNIST (classification with Categorical target distribution). Best scores among allsingle-pass models are in bold. Best scores among all models are starred. Gray numbers indicate thatR-PriorNet has seen samples from the FMNIST dataset during training.
Table 9: Results on FMNIST (classification with Categorical target distribution). Best scores amongall single-pass models are in bold. Best scores among all models are starred. Gray numbers indicatethat R-PriorNet has seen samples from the KMNIST dataset during training.
Table 10: Classification results on CIFAR-10 with Categorical target distribution. Best scores amongall single-pass models are in bold. Best scores among all models are starred. Gray numbers indicatethat R-PriorNet has seen samples from the SVHN dataset during training.
Table 11: Results on the Bike Sharing Dataset with Normal N and Poison Poi target distributions.
Table 12: MNIST comparison (<latent dim> 一 <certainty budget> 一 <radial IayerS>∕<MAF layers>).
Table 13: CIFAR10 comparison (<latent dim> - <certainty budget> - <radiallayers>∕<MAF layers>).
Table 14: Bike Sharing (Normal N) comparison (<latent dim> - <certainty budget> - <radiallayers>/<MAF layers>). Bold and starred number indicate best score among all models.
Table 15: Bike Sharing (Poisson Poi) comparison (<latent dim> - <certainty budget> - <radiallayers>/<MAF layers>). Bold and starred number indicate best score among all models.
Table 16: MNIST - OOD detection with AUC-ROC scores. Bold numbers indicate best score amongsingle-pass models. Starred numbers indicate best scores among all models. Gray numbers indicatethat R-PriorNet has seen samples from the Fashion-MNIST dataset during training.
Table 17: CIFAR-10 - OOD detection with AUC-ROC scores. Bold numbers indicate best scoreamong single-pass models. Starred numbers indicate best scores among all models. Gray numbersindicate that R-PriorNet has seen samples from the SVHN dataset during training.
Table 18: Bike Sharing - OOD detection with AUC-ROC scores. Bold numbers indicate best scoreamong single-pass models. Starred numbers indicate best scores among all models. Normal andPoisson Regression are treated separately.
