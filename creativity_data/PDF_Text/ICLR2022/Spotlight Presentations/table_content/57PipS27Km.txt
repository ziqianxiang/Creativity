Table 1: Memory required to compute meta-gradients for different algorithms. Exact: the methodreturns the exact meta-gradients. Full net.: the whole network is adapted, with a number of meta-parameters ∣θ∣. The requirements for checkpointing are taken from (Shaban et al., 2019). Note thattypically M d in few-shot learning.
Table 2: Few-shot classification on miniImageNet & tieredImageNet. The average accuracy (%) on1,000 held-out meta-test tasks is reported with 95% confidence interval. ✓ denotes gradient-basedmeta-learning algorithms. ? denotes baseline results we executed using the official implementations.
Table 3: References for the results provided in Table 2: O (LiU et al., 2019), O (Oh et al., 2021), O(Aimen et al., 2021), O (Arnold et al., 2021), and O are reported in their respective references (underModel). Recall that ? denotes baseline results we executed using the official implementations.
Table 4: The effect of the numerical solver on the performance of COMLN. The average accuracy(%) on 1, 000 held-out meta-test tasks is reported with 95% confidence interval. Note that for a givensetting, the same 1, 000 tasks are used for evaluation, making both methods directly comparable. RK:4th-order Runge-Kutta with Dormand Prince adaptive step size. Euler: explicit Euler scheme.
Table 5: miniImageNet results using LEO embeddings and a single linear classifier layer. The averageaccuracy (%) on 1,000 held-out meta-test tasks is reported with 95% confidence interval. * Resultsreported in (Rusu et al., 2018). ** Note that LEO uses more than a single linear classifier layer, butwe add the numbers for completeness.
