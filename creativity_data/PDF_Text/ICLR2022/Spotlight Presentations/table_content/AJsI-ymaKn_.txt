Table 1: Related work. Comparison to policy learning methods in terms of interpretability.
Table 2: Comparison of the performance of policy learning algorithms2on medical datasets. Interpretabil-ity scores out of ten were obtained through our clinician survey. Lower is better for Brier calibration. Standarderrors for MIMIC and SYNTH were ≤ 0.04.
Table 3: Comparison of related work in light of our key policy learning goals. DM-IL stands fordistribution-matching imitation learning. f stands for a representation learning step; bt is the agent’s beliefover the true state st .
Table 4: Comparison of time-series methods with actions.
Table 5: Performance of different history extraction models for decision tree policies on ADNI. Unlessshown, standard errors were all ≤ 0.02. History-extraction and action-prediction trees have respective depthsdH and dA .
Table 6: Performance of different leaf models for history recurrence on ADNI. Standard error on tree depthd is omitted for brevity.
Table 7: Comparison of our proposed architecture with traditional, soft and cascaded decision trees(SDT) as in Breiman et al. (1984), Frosst & Hinton (2017) and Ding et al. (2021).
Table 8: Performance of multidimensional and different axis-aligned decision tree policy structures. Stan-dard errors all ≤ 1%.
Table 9: Performance of multidimensional and axis-aligned decision tree policy structures on ADNI. Thestatic setting corresponds to learning a mapping zt to at .
Table 10: Hyperparameter grid for Poetree optimisation, with values optimised for the ADNI dataset.
Table 11: Performance of decision tree policies optimised with different objective functions on ADNI.
Table 12: Complexity analysis of different policy learning algorithms on ADNI. Runtime is measured ascomputation time for the prediction of all test actions (10% of demonstrations trajectories).
Table 13: Patient trajectories under a policy learned by PO-BC-IL. History embeddings are obtainedthrough a recurrent neural network.
