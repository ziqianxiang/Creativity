Table 1: Configuration of the Cifar10 experiments.
Table 2: Configuration of the Cifar100 experimentsWe report more details on the models, including number of parameters and FLOPs, in Table 4.
Table 3: Configuration of the ImageNet experimentTable 4: The performance of Pixelfly and ViT or MLP-Mixer on the ImageNet benchmarks, including thenumber of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet)compared to the dense model.
Table 4: The performance of Pixelfly and ViT or MLP-Mixer on the ImageNet benchmarks, including thenumber of parameters and FLOPs. We measure the accuracy and the training time speedup (on ImageNet)compared to the dense model.
Table 5: The performance of Pixelfly, BigBird and GPT-2-Small on WikiText-103, including the number ofparameters and FLOPs. We measure the perplexity and the training speed up.
Table 6: Configuration of the WikiText103 experimentsModel	Optimizer Weight Decay Learning Rate Dropout Warmup/EpochGPT-2-Small	Adam	0.1	0.0001	0.1	5/100Pixelfly	Adam	0.1	0.0001	0.1	5/100L.3 Measuring Empirical NTKThe Empirical NTK is a rough estimation of the real NTK, in which the width of the neural net goes to infinity.
Table 7: Microbenchmarking of different patterns. Given GPU processes the matrix block by block of size 32× 32, random block pattern’s latency increases as the block size shrinks, while Pixelfly remains efficient. Wemeasure the latency by averaging 100 runs of batch size 4096 for each configuration.
Table 8: The performance of Pixelfly and original Butterfly on MLP-Mixer on the ImageNet benchmarks.
