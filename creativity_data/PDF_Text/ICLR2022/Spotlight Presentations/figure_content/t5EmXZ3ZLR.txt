Figure 1: Comparison of SOSP to local, i.e. layer-wise, pruning methods on Cifar10 (see Suppl.
Figure 2: Runtime to calculate thepruning masks for ResNet-56 on Ci-far10 over the width of the networkfor SOSP-I and SOSP-H. We varythe width of the network by increas-ing the width of each layer by a mul-tiplicative factor.
Figure 3: We remove architectural bottlenecks found by SOSP using the expand-pruning scheme(a) on Cifar10. The width of blocks and layers having low pruning ratios according to the standardpruning scheme (“train-pruning” in Suppl. Fig. 8d and g) are expanded by a width multiplier of 2 (c,f). As a baseline, we uniformly expand all layers in the network by a multiplier of 1.1 (d, g). Thelayer-wise pruning ratios of the enlarged network models are shown as bar plots in (c, d, f, g). Theaverage test accuracy and standard deviation are shown over the number of model parameters (b, e).
Figure 4: Comparison of the accuracies achieved by SOSP and a first-order method, where allsecond-order terms are set to zero (see end of Sec. 2.2), for ResNet-56 before and after the fine-tuningstep on Cifar10 and Cifar100. All shown experiments prune a full ResNet-56 model with 855 × 103parameters; the parameter count of the pruned network is shown on the horizontal axis. The resultsclearly show that adding second-order information increases the performance, before as well as afterfine-tuning (FT). The advantage of using second-order information increases as the size of the prunednetwork decreases.
Figure 5:	Comparison of the accuracies achieved by vanilla SOSP-I and a variation of SOSP-I,where all off-diagonal terms of the Hessian are set to zero, for ResNet-56 and VGG on Cifar10. Theresults on both networks suggest that cross-structure correlations can significantly improve pruningperformance. The gap in performance is even larger before the fine-tuning step (not shown).
Figure 6:	Comparison of the accuracies achieved by SOSP and its competing methods on ResNet-18/50 on ImageNet. The plots visualize the data of Tab. 2.
Figure 9: Comparison of pruning and fine-tuning a randomly initialized (left) and a pretrained (right)network for ResNet-56 on Cifar10. Random pruning is a baseline which selects uniformly at randomstructures s and adds them to the mask M until the predefined pruning ratio is reached.
Figure 7:	Comparison of the accuracies for Cifar10 achieved by SOSP-H and SOSP-I vs. CCP (Penget al., 2019) over the alternative MAC-count used by (Peng et al., 2019) (see also App. D).
Figure 8: Comparison between pruning after training and at initialization on Cifar10. Both pruningschemes, train-pruning (a) and init-pruning (b), train the network for the same overall number ofepochs, but generate and apply the pruning masks at different point in times. The average and standarddeviation of the test accuracy across 3 trials is plotted against the number of model parameters forResNet-56 (top row; c) and VGG (bottom row; f). For a single trial, in which overall 50% of thestructures are pruned, we visualize the pruning masks of train-pruning and init-pruning by showingthe layer-wise pruning ratios in (d, g) and (e, h), respectively.
Figure 10:	We widen architectural bottlenecks found by SOSP starting with a randomly initalizednetwork. The width of blocks and layers with low17pruning ratios in the train-pruning scheme (Fig. 8eand h) are expanded by a width multiplier of 2 (b, e). As a baseline, we again uniformly expand alllayers in the network by a factor 1.1 (c, f). The layer-wise pruning ratios of the enlarged networkmodels are shown as bar plots in (b c e f). The average and standard deviation of the test accuracyPublished as a conference paper at ICLR 2022A.12 Shuffle ExperimentsSee Table 8 and its caption.
Figure 11:	Mean and standard deviation plots of the accuracies after fine-tuning of SOSP forResNet-56 and DenseNet-40 on Cifar10.
