Figure 1: Illustration of the transformer architectureand several state-of-the-art parameter-efficient tuningmethods. We use blocks with dashed borderlines torepresent the added modules by those methods.
Figure 2: Performance of different methods on theXSUm (Narayan et al., 2018) summarization task.
Figure 3: Graphical illustration of existing methods and the proposed variants. "PLM module” represents acertain sublayer of the PLM (e.g. attention or FFN) that is frozen. “Scaled PA” denotes scaled parallel adapter.
Figure 4: Performance of previous state-of-the-art parameter-efficient tuning methods on XSum (left) and en-ro (right).
Figure 5: Results on XSum (left) and en-ro (right). PA represents parallel adapter. Blue and red markers applymodifications at attention and FFN sub-layers respectively (best viewed in color).
