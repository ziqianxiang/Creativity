Figure 1: The NODE-GAM architecture. Here we show 4 features with 4 different colors. Each layerconsists of I differentiable oblivious decision trees that outputs h1...hI, where each hi only dependson 1 feature. We only connect trees between layers if two trees depend on the same features. And weconcatenate all outputs from all layers as inputs to the last linear layer WL to produce outputs.
Figure 2: The shape plots of 4 (out of 11) features of 4 models (NODE-GA2M, NODE-GAM, EBM,and Spline) trained on the Bikeshare dataset.
Figure 3: The shape plots of 4 interactions of NODE-GA2M trained on the Bikeshare dataset.
Figure 4: The shape plots of 4 GAMs trained on MIMIC-II dataset (4 of the 17 features are shown).
Figure 5: The shape plots of 4 interactions of NODE-GA2M trained on the MIMIC2 dataset.
Figure 6: The relative improvement (%) over NODE-GAM without self-supervision (No-SS) on 6datasets with various labeled data ratio. The higher number is better.
Figure 7: The NODE-GA2M architecture. Here we show 4 features with 4 different colors. Eachlayer consists of I differentiable oblivious decision trees that outputs h1 ...hI, where each hi dependson at most 2 features. We only connect trees between layers if two trees depend on the same twofeatures. And we concatenate all outputs from all layers as inputs to the last linear layer WL toproduce outputs.
Figure 8: The shape plotsimportance (Imp).
Figure 9: The shape plots of top 16 interactions in Bikeshare. We also show the feature importance(Imp).
Figure 10: The shape plots of all features (main effects) in MIMIC2. We also show the featureimportance (Imp).
Figure 11: The shape plots of top 16 interactions in MIMIC2. We also show the feature importance(Imp).
