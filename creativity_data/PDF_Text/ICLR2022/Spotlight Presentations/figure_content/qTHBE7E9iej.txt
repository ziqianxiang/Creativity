Figure 1: (a) Graphical model for HeLMS, with solid lines indicating the underlying generativemodel (prior) and dashed lines indicating dependencies introduced by the inference model (poste-rior). (b) Network architecture, showing the high-, mid-, and low-level networks from left to right,respectively. As indicated by superscripts, different subsets of the input state x can be provided tothe high level (H L), mid level (M L), and low level (LL) (information-asymmetry).
Figure 2: The object sets (triplets) used for our experiments, introduced by (Lee et al., 2021).
Figure 3: (a) Image sequences showing example rollouts when fixing the discrete skill (different foreach row) and running the mid- and low-level controllers in the environment. Each skill executesa different behaviour, such as lifting (top row), reach-to-red (middle row), or grasping (bottom) (b)The learned skill transition prior p(yt | yt-1). (c) Histogram showing the use of different skills.
Figure 4: Performance when transferring to the red-on-blue stacking task using staged sparse rewardwith every unseen object set.
Figure 5: (a) Performance on pyramid task; and (b)image sequence showing episode rollout from a learnedsolution on this task (left-to-right, top-to-bottom).
Figure 6: Performance forvision-based stacking.
Figure 7: Transfer performance on a stacking task with different reward sparsities. Left: densereward, Centre: staged sparse reward, Right: sparse rewardfor each stage or sub-task: reaching, grasping, lifting, and placing the red object, and subsequentlythe blue object. In Figure 5(a), we plot the performance of both variants of our approach, as well asNPMP and MPO; we omit the BC baselines as this involves transferring to a fundamentally differenttask. Both HeLMS-mix and HeLMS-cat reach a higher asymptotic performance than both NPMPand MPO, indicating that the learned skills can be better transferred to a different task. We show anepisode rollout in Figure 5(b) in which the learned agent can successfully solve the task.
Figure 8: Ablation for continuous and dis-crete components during offline learning,when transferring to the (a) easy case (ob-ject set 4) and (b) hard case (object set 3).
Figure 9:	Ablations for KL-regularisation, showing downstream performance with different degreesof online KL-regularisation. Performance is evaluated for easy (object set 4) and hard (object set3) transfer cases, with sparse staged rewards; when using different offline regularisation coefficient(Î²z) values for the mid-level components.
Figure 10:	Ablation for NPMP (Merel et al., 2019) using staged sparse reward, with all object sets.
Figure 11: The five object sets (triplets) used in the paper. This image has been taken directly from(Lee et al., 2021) for clarity.
