Figure 1: The overall AdaRL framework. We learn a Dynamic Bayesian Network (DBN) over theobservations, latent states, reward, actions and domain-specific change factors that is shared acrossthe domains. We then characterize a minimal set of representations that suffice for policy transfer, sothat we can quickly adapt the optimal source policy with only a few samples from the target domain.
Figure 2: Illustrations of the change factors on modified Pong game.
Figure A1: Graphical representation of the generative model in Eq. 1. In the top of the figure, thesquare boxes are the domain-specific parameters θk, which are constant in time, while the rectangularboxes represent the binary masks that encode the edges. R represents the cumulative rewardLemma A1. Under the assumption that the graph G is Markov and faithful to the measured data, astate dimension si,t ∈ st is part of stmin iff:si,t -⊥ at | Rt+1, st ∀st ⊆ {st \ si,t},Similarly, a change factor dimension θi,k ∈ θk is part of θkmin iff:θi,k 其 at∣Rt+ι, lt, ∀lt ⊆ {st,{θk \ θi,k}}.
Figure A2: Example model in which s1,t and s3,t are compact domain-specific representations forpolicy learning, while s2,t is not. This does not mean that s2,t and θ2,k are not useful in the modelestimation part, especially in estimating θko .
Figure A3: Diagram of MiSS-VAE neural network architecture. The "sequential VAE" compo-nent, "multi-model" component, and "structure" component are marked with black, red, and blue,respectively.
Figure A4: Visual examples of Cartpole game and change factors. (a) Cartpole game; (b) ModifiedCartpole game with Gaussian noise on the image. The light blue arrows are added to show thedirection in which the agent can move.
Figure A5: The estimated θmin for the three change factors in Cartpole (POMDP): gravity (A), mass(B), and noise level (C) for Ntarget = 50..
Figure A6: Learning curves for modified Cartpole experiments (POMDP version) with change factors.
Figure A7: Learning curves for modified Cartpole experiments (MDP version) with change factors.
Figure A8: Visual example of the original Pong game and the various change factors. The light bluearrows are added to show the direction in which the agent can move.
Figure A9: Learned θθ for the two change factors, size and noise, in modified Pong.
