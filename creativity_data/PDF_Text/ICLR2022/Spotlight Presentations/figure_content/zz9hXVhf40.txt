Figure 1: a) The variation of different uncertainty penalties against true dynamics error during a model rolloutof Hopper Medium-Expert. The canonical ensemble variance penalty most closely fits the true dynamics error.
Figure 2: Box Plots showing D4RL Medium transferred to Random. We show IQR limits and the median valuedenoted by the black vertical line. Green = HalfCheetah, Blue = Hopper. Max Aleatoric, Max Pairw. Diff. andLOO KL are unstable w.r.t. ensemble member count. In contrast, ensemble variance and std. are far more stable.
Figure 4: MOPO performance on theHopper medium-expert environment.
Figure 5: Scatter Plots showing HalfCheetah D4RL transfer tasks.
Figure 6:	Scatter Plots showing Hopper D4RL transfer tasks.
Figure 7:	Scatter Plots showing HalfCheetah D4RL true model-based error calibration.
Figure 8:	Scatter Plots showing Hopper D4RL true model-based error calibration.
Figure 9: Box Plots showing HalfCheetah D4RL transfer tasks.
Figure 10: Box Plots showing Hopper D4RL transfer tasks.
Figure 11: Boxplots showing HalfCheetah D4RL true model-based error penalty distributions.
Figure 12: Boxplots showing Hopper D4RL true model-based error penalty distributions.
Figure 13: HalfCheetah Spearman Statistics23Published as a conference paper at ICLR 2022Pearson： rMed .-Exp. → Random Med.-Exp. → Mixed Med.-Exp. →Med. Med.-Exp. → Med.-Exp. Med.-Exp.→Exp.
Figure 14: HalfCheetah Pearson Statistics24Published as a conference paper at ICLR 2022B.2.2	Hopper D4RL: TransferraEL-<σφdSMed.-Exp. → Random	Med.-Exp.→ Mixed Med.-Exp. →Med. Med.-Exp. → Med.-Exp.	Med.-Exp. → Exp.
Figure 15: Hopper Spearman Statistics25Published as a conference paper at ICLR 2022Pearson： r0.750.250.750.500.250.000.750.500.250.000.750.500.250.000.00Med. → Random
Figure 16: Hopper Pearson Statistics15	20Max Aleatoric -------- Max Pairwise Diff. -------- Ensemble Std. ---------- Ensemble Van15	20 5	10	15	20 5	10	15	20 5	10	15	20 5Number of Models0.50							—	:—r																											Med. → Med.
Figure 17:	HalfCheetah Spearman StatisticsRandom	Mixed	Med.	Med.-Exp.	Exp.
Figure 18:	HalfCheetah Pearson StatisticsB.2.4 Hopper D4RL: True Model-Based ErrorRandom	Mixed	Med.	Med.-Exp.	Exp.
Figure 19:	Hopper Spearman StatisticsRandom	Mixed	Med.	Med.-Exp.	Exp.
Figure 20:	Hopper Pearson Statistics27Published as a conference paper at ICLR 2022B.2.5 All AggregatedHaIfCheetahHoppera :ueuxle°dst 0.4CΦ 0.2d5	10	15	20Number of Models」:UOSJe0d5	10	15	20Number of Models5	10	15	20	5	10	15	20Number of Models Number of Modelsa :ueE」eods---- Max Aleatoric ----- Max Pairwise Diff. ---- Ensemble Std. ----- EnsembIeVar. — LL Var. ----------- KL LOO
Figure 21:	Aggregated True Model-Based correlation statistics over all datasets (i.e., Random through toExpert); Left: HalfCheetah; Right: HopperC	Skewness and Kurtosis ComparisonsC.1 S kewnes s and Kurtosis OverallHere we present the 3rd and 4th order statistics (skew and kurtosis respectively) of each penalty,illustrating that even with identical model counts, the shape statistics between penalties are vastlydifferent.
Figure 22:	HalfCheetah Transfer.
Figure 23:	Hopper Transfer.
Figure 24: HalfCheetah True Model-Based.
Figure 25: Hopper True Model-Based.
Figure 26: Median True Model-Based Errors as a function of rollout timestepWe observe indeed that both median dynamics and distribution errors increase with increasing timestep in the model. The only real exception is HalfCheetah Medium-Expert, which we believe to bedue to our trained policy not being able to successfully exploit this environment.
Figure 27: Several Individual Ground Truth Rollouts in Hopper Medium-ExpertWe observe that errors along any single trajectory tend to manifest as ‘spikes’, and that it is entirelypossible to recover from these, returning to either admissible dynamics, or parts of the state-actionspace that have been seen in the data. This speaks to the nature of how we ought to penalize policiesfor accessing regions of inaccuracy/uncertainty, and may justify a hybrid MOPO/MOReL approach,whereby we penalize individual transitions along a trajectory, but do not stop rollouts early. Indeed,this is similar to the approach taken in M2AC (non-stop), albeit they choose to ‘mask’ uncertaintransitions, not penalize them. We leave the design of such an algorithm to future work.
Figure 28: Comparing OOD dynamics and inputs on a Hopper Medium-Expert trajectoryWe first speak to the inset annotated ‘1’. Here we observe that the transitions generated in fact closelyresemble the data that our model was trained on, however the predicted dynamics are incorrect, andcause an aforementioned ‘spike’. This is the opposite of what is observed in the inset annotated‘2’; where we actually predict accurate dynamics, however the resultant state-action tuples do notclosely resemble the data that our model was trained on. We generally observe that regions of highDistribution Error tend to be preceded by ‘spikes’ pertaining to high Dynamics Error, and this presentan exciting avenue for future work understanding how these quantities are related.
Figure 29: t-SNE projection of 10,000 Hopper D4RL Medium-Replay and 10,000 exploitative Imagined policystate-action tuplesWe observe that the induced policies inside the model displays some overlap with the D4RL data,but also resides in parts of the manifold where there is little coverage from the D4RL data, likelyrepresenting regions of exploitation. Importantly, the exploitative WM trajectories display strongstate-action tuple diversity, comparable to that of the offline data it was trained on.
Figure 30: Hopper Medium-Expert True Model-Based Experiments; Left: Precision v.s. Recall against GroundTruth; Middle: Higher Performing Penalties v.s. Ground Truth MSE in Imagined Rollout; Right: LowerPerforming Penalties v.s. Ground Truth MSE in Imagined RolloutAs noted previously, different penalties have varying scales and distribution profiles, so we need away of standardizing the method of assessment. Using our observation that errors manifest as ‘spikes’during a rollout, we propose treating each penalty as a classifier. Concretely, our test set consists ofthe ground truth data labeled by whether or not they exceed a certain percentile at a particular timestep. Each penalty may be then be treated as a ‘classifier’ by normalizing its range to lie in [0, 1]. Wecan then use standard classification quality measures, such as AUC, to determine the effectiveness ofthese penalties at capturing these spikes, whilst sidestepping the issue of the different distributionalprofiles identified previously.
Figure 31: Precision Recall curves on ground truth data.
