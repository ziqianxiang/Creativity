Figure 1: Overview of the proposed approach. When a new incremented task data I emerge, thememory recovery paradigm R(L) is called to generate transfer set S, including synthesized imagesfor the classes that the learner network L has learned before. In the proposed recovery engine,we first generate preliminary model outputs {V} for each learned class and then form a refined set{V}* by applying a constraint F({V}, Y) using a dynamic recommender vector Y to adjust due tounderlying distribution. Then, a random noise input is initialized to be optimized conditioned onthe refined output {V}*. This procedure is performed for each learned class several times to formthe transfer set S. Finally, the learner network is retrained on the combined dataset D, includingthe transfer set S and new incremented task data I using a two-part cost function composing aclassification (CE) and knowledge distillation (KD) terms, respectively.
Figure 2: Visualization of the sampled recovered data from the learner network using the proposedmemory recovery paradigm, while training on CIFAR10 dataset.
Figure 3: Experimental results on CIFAR-10 dataset. (a) classification accuracy curves for CF-ILwith various transfer set size. Performance comparison of the CF-IL with various hyperparametersλ (c) and η (d).
Figure 4: Visualization of happened False Memory When retrieving data from the memory recoverystep (Right) and a correct sample which passed the second step in our introduced network outputmodeling procedure (Left).
