Figure 1: Generalized Decision Transformer (GDT), where the figure is a minor generalization of the DTarchitecture (Chen et al., 2021a) and the table summarizes how it leads to different classes of algorithmswith only small architectural changes. If the feature function Φ(s, a) is reward r(s, a) and the anti-causalaggregator is γ-discounted summation, we recover DT for offline RL. If the aggregator is binning, we getCategorical DT (CDT) for offline multi-task state-marginal matching. If the aggregator is a second transformer,we get Bi-directional DT (BDT) for offline multi-task imitation learning (IL), or equivalently one-shot IL. Thechoices of Φ(s, a) and the aggregator together decide IΦ(τ) in Hindsight Information Matching (HIM) objectivediscussed in Section 4 and Table 1, where conversely GDT can essentially solve any HIM problem with properchoices of Φ and aggregator.
Figure 2: (a) Z-Velocity and (b) Unseen Cheetah-Velocity results. Blue histograms represent target distributions.
Figure 3: Distributions of the features (reward, and x-velocity) in the D4RL medium-expert datasets.
Figure 4: (a) Reward and (b) state-feature (x-velocity) distribution matching in halfcheetah (top), hopper(middle), and walker2d (bottom). The left two examples are the distributions from the best trajectories, and righttwo are the distributions from the middle trajectories in the held-out test set. The rollout distributions of CDT(red) match the target distributions (blue) very well in all cases.
Figure 5: Visualization of 2D state-feature (xy-velocities) distribution matching, binning each dimensionseparately. Top row shows the results from an expert target trajectory, and bottom row from a medium one.
Figure 6: (a) Reward and (b) state-feature distribution matching by Bi-directional Decision Transformer(m = 16) in halfcheetah (top), hopper (middle), and walker2d (bottom). The left two examples are thedistributions from the best trajectories, and right two are the distributions from the middle trajectories.
Figure 7: Reward distribution matching in halfcheetah (top two rows; best and middle), hopper (middle tworows; best and middle), and walker2d (bottom two rows; best and middle). We shift the target distribution with(from left to right column); bin_size ×{-3.0, -2.0, -1.0, 0.0, +1.0, +2.0, +3.0} (Table 14). CategoricalDT (red) can match the rollouts to the shifted target distributions (blue) when the shifted targets are withinthe support of dataset distributions. For DT (yellow, captioned as Deterministic), we only visualize the deltafunction at the means of rollouts.
Figure 8: State-feature distribution matching, especially x-velocity, in halfcheetah (top two rows; bestand middle), hopper (middle two rows; best and middle), and walker2d (bottom two rows; best andmiddle). We shift the target distribution with constant offset (from left to right column); bin_size×{-3.0, -2.0, -1.0, 0.0, +1.0, +2.0, +3.0} (Table 15). For DT (yellow, captioned as Deterministic), weonly visualize the delta function at the means of rollouts.
Figure 9: State-feature distribution matching in halfcheetah (top), hopper (middle), and walker2d (bottom).
