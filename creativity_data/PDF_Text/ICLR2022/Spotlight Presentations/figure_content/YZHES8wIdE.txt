Figure 1: Generative Planning Method. GPM has three main components. i) Plan generation. AtTactualΠeach time step, the plan generator Π generates a plan Tnew based on the state st. ii) Plan update. Toldis obtained by “time-forwarding” the previous actual plan P(Tactual) one step. iii) Plan switch. Thereplanning signal is generated based on the values of the previous plan Told and the new one Tnew,and only switches if certain criteria is satisfied. The plan enclosed with a black box indicates the onebeen selected at that time step. In the case of driving, the primitive action to be executed is derivedfrom the plan as ω(Tactual), which is sent to the environment for execution as shown on the right.
Figure 2: Network Structures of (a) plan generator (generative actor) and (b) plan value function(critic). (a) The plan generator generates a sequence of actions in an autoregressive way to form the fullplan. For each step start fromthe second one, a skip-connection is added fromthe input previous actionto the output action. (b) The plan value function takes state and plan as input, and outputs a sequenceof values along the plan. In this example, the plan value function takes St and T = [at, at+1,at+2, •一]as input and outputs values [Q(st,at), Q(st,at, at+ι), Q(st, at, at+ι, at+2),…].Q(st,at) uses aseparate decoder different from the rest of steps t + 1,t + 2, •一，whose values are computed as thesum of the value from the previous step and the output from the decoder.
Figure 3:	Return Curves on benchmark continuous control tasks for different methods.
Figure 4:	Exploration Trajectory Visualization and Return Curves. (a) Each figure shows collec-tively the 50 rollout trajectories during training for the range depicted above. Trajectories are plottedrelative its start position for easier visualization (unit:meter). The chronological order of positionsin the trajectory is encoded in the same way as Figure 5. Visually, an effective agent should pushthe position of the red points as far as possible along its route. All plots including thumbnails arerendered at the same scale relative to their axes. (b) Return curves of different methods.
Figure 5: Evolution of GPM Plans: (a) initially the plans are nearly uniformly distributed acrossall the directions; (b) forward-moving plans appear to take a larger portion afterwards; (c) furthertraining shows the emergence of plans with more adjustments on speed, indicated by shorter lengthplans; (d) later, more curved turning plans and plans with increased length appear; (e) at the end oftraining, the agent can drive smoothly, therefore forward moving and turning plans take the majority.
Figure 6: Plan Visualization within an Episode. Note that the agent is trained with a frontal cameracapturing 64×64 RGB images. The top-down view at a higher resolution is only used for visualization.
Figure 7: Network structures for CARLA task.
Figure 8: CARLA environment and action space.
Figure 9:	Map of and Example Routes. (a) Map of Town01 (Dosovitskiy et al., 2017). (b) Mapwith some example routes overlaid. Different routes are rendered with different colors. The beginningof each route is denoted with a 4 symbol and the end of the route is denoted with a O symbol.
Figure 10:	Ablations on (a) plan length and (b) target commitment length. For (a) the targetcommitment length is set as 1.5. For (b) the plan length is set as 5.
Figure 11:	Observational Dropout for (a) FAR (b) GPM-COmmit. Here both approaches generateplans of fixed length and execute them to the end before generating the next plan.
Figure 12: Visualization of State Visitation and Evolution during training on Pendulum. Eachfigure shows collectively all the rollout trajectories within the range of steps depicted on the top.
Figure 13: Exploration Trajectory Visualization. Trajectory are plotted relative its start position.
