Figure 1: Over-smoothing in BERT models.
Figure 2: Illustration of self-attention and the corresponding graph G . For simplicity, we drop theself-loops in G .
Figure 3: Consine similarity between the attentionmatrices A's at layer i and its next higher layer.
Figure 4: The illustration of over-smoothing problem. Recursively, Hl will converge to subspace Mwhere representation of each token is identical.
Figure 5: The estimated distribution of σ1σ2 for different fine-tuned models.
Figure 7: Visualization of importance weights of gate fusion on different layers.
Figure 6: The token-wise similarity comparison between BERT and BERT with gate fusion. Here Fmeans the final output, which is the fusion results for our approach.
