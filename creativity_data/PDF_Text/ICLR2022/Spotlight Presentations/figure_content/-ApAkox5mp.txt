Figure 1: Bi-level optimization: Convergence of held-out test loss for different hyperparameter opti-mization methods on the '2-regularized logistic regression problem for the 2 datasets (20news (Lang,1995) and real-sim (lib)) SHINE achieves the best performances for both problems while theJacobian-Free method is much slower, in particular on 20news. Note that the kink for HOAG onreal-sim does not mean it is better as the optimization stops once the validation loss has convergedand not the test one. The typical loss order of magnitude is 102. An extended figure with moremethods is provided in Section E.1.
Figure 2: Bi-level optimization with OPA: (left) Convergence of different hyperparameter opti-mization methods on the '2-regularized LR problem for the 2θnews dataset (Lang, 1995) on held-outtest data. SHINE with OPA achieves similar performance as SHINE without OPA but with better con-vergence guarantees. (right) Evaluation of the inversion quality in direction v using OPA b = Bn-1vcompared to the exact inverse a = Jgθ (z?)-1v for 3 different directions: the prescribed direction,the Krylov direction and a random direction. The points represent the cosine similarity between aand b as a function of the ratio of their norm and the closer to (1, 1) the better. The inverse in theprescribed direction is better than in random direction.
Figure 3: DEQ: Top-1 accuracy function of backward pass runtime for the different methodsconsidered to train DEQs, on CIFAR (Krizhevsky, 2009) and ImageNet (Deng et al., 2009). Theoriginal DEQ training method corresponds to the Full backward pass points and the vanilla SHINEand Jacobian-Free methods correspond to direct use of the inverse approximation without furtherrefinement. The other points correspond to further refinements of the different methods with differentnumber of iterations used to invert Jg§ (z*) in the direction of NzL(z?). This highlights the trade-offbetween computations and performances driving the refinement choice.
