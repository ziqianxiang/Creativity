Figure 1: Visualization of discussed loss function with regards to a reference point marked withthe white cross and the corresponding value function on the Pendulum environment. For the valuefunction, darker color indicates a lower value. In the loss figures, darker color indicates how large.
Figure 2:	Evolution of the VAML loss over changing value functions on the Pendulum domain.
Figure 3:	Performance of VaGraM and MLE models with reduced model size. The dotted linescorrespond to the final performance reported for model-free SAC (grey, approx. 3200). Shaded arearepresents standard error over 16 repeated runs. VaGraM continues to solve the task almost unim-peded, while MLE is unable to even stabilize the Hopper when using a two layer neural network.
Figure 4: Performance of VaGraM and MLE models with distracting state dimensions. Thedotted lines correspond to the final performance achieved by both algorithms on the Hopper taskwithout distraction (grey, approx. 3200). Shaded area represents standard error over 16 repeatedruns. VaGraM achieves significantly higher returns than the MLE baseline, especially in the mostchallenging setting with 15 distracting dimensions.
Figure 5: Comparison of VaGraM, MLE (MBPO baseline), IterVAML Farahmand (2018) and avalue-weighing ablation on two simple continuous control tasks. The shaded area represents stan-dard error estimated over 8 runs. While VaGraM, IterVAML and MBPO are able to achieve satis-factory performance on the Pendulum Swingup task, IterVAML fails to stabilize the more difficultCartpole balancing task.
Figure 6: Comparison of VaGraM and MBPO on the Mujoco tasks presented in Janner et al. (2019).
Figure 7: Comparison of the empirical performance of deterministic and probabilistic MLEmodels vs VaGraM. Thick lines denote the mean and shaded area the standard error over 8 runs.
