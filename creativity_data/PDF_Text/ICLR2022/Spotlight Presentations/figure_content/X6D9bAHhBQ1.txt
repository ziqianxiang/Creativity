Figure 1: Stochastic MuZero. (A) Monte Carlo Tree Search used in Stochastic MuZero, wherediamond nodes represent chance nodes and circular nodes represent decision nodes. During theselection phase edges are selected by applying the pUCT formula in the case of decision nodes, andby sampling the prior σ in the case of chance nodes. (B) Training of stochastic model in StochasticMuZero. Here for a given trajectory of length 2 with observations 0≤t∙.t+2, actions at：t+2, valuetargets zt：t+2, policy targets ∏t±+2 and rewards 设力+上力+长,the model is unrolled for 2 steps. Duringthe unroll, the encoder e receives the observation o≤t+k as an input and generates a chance code ct+kdeterministically. The policy, value and reward outputs of the model are trained towards the targetsπt+k, zt+k and ut+k respectively. The distributions σk over future codes are trained to predict thecode produced by the encoder.
Figure 2: Planning in 2048. a) Stochastic MuZero, trained using 100 simulations of planning with alearned stochastic model, matched the performance of AlphaZero, using 100 simulations of a perfectstochastic simulator, while a deterministic learned model (MuZero) performed poorly. b) Evaluationof final agent using different levels of search. Stochastic MuZero scales well during evaluation tointermediate levels of search (roughly comparable to 3-ply lookahead), exceeding the playing strengthof the state-of-the-art baseline (ja´kowski, 2016). However, as the number of simulations increaseswe observe diminishing returns due to imperfections of the learned model.
Figure 3: Stochastic MuZero in Backgammon. a) Stochastic MuZero, trained using 1600 simula-tions of planning with a learned stochastic model, matched the performance of AlphaZero, trainedusing 1600 simulations of a perfect stochastic simulator, as well as matching the superhuman-levelprogram GNUbg Grandmaster. A deterministic learned model (MuZero) performed poorly. b)Stochastic MuZero’s model scaled well to large searches, and exceeded the playing strength ofGNUbg Grandmaster when using more than 103 simulations.
Figure 4: Stochastic MuZero in Go. Comparison of Stochastic MuZero and MuZero in the game ofGo. a) Stochastic MuZero and MuZero when compared in 9x9 Go. MuZero has a search budget of 200simulations during training of 800 during evaluation, while Stochastic MuZero uses 400 simulationsduring training and 1600 during evaluation. The Elo scale was anchored so that the performance ofthe final MuZero baseline corresponded to an Elo of 2000. b) Stochastic MuZero and MuZero whencompared in 19x19 Go. MuZero has a search budget of 400 simulations during training of 800 duringevaluation, while Stochastic MuZero uses 800 simulations during training and 1600 during evaluation.
Figure 5: Stochastic MuZero reproducibility across all domains. We ran our method StochasticMuZero in all environments using 9 different seeds to measure its robustness to random initialization.
Figure 6: Average distribution of learned chance outcomes. The average distribution of learnedchance outcomes over all chance nodes after running Stochastic MuZero at each game for 5 episodes.
