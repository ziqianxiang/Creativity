Figure 1: Realistic learning settings With neural networks and logistic regression. Left: Variance of training lossof a neural network with width d and tanh activation on the MNIST dataset. We see that the variance explodesafter d ≥ 200. In contrast, rescaling the learning rate by 1/d results in a constant noise level in training. Thissuggests that the stability condition we derived for high-dimension regression is also useful for understandingdeep learning. Middle: Stability of Adam with the same setting. Adam also experiences a similar stabilityproblem when the model width increases. Right: Logistic regression on MNIST trained with SGD; with λ = 1.5,S = 32. We see that the optimal performance is also achieved at negative weight decay strength γ, suggestingthat a large learning rate can indeed introduce effective regularization.
Figure 2: Comparison of the pro-posed theory with the continuous-time theory on the SGD station-ary distribution for aλ = 1. Theproposed theory agrees with the―	experiment exactly.
Figure 3: Left: 1d experiments with label noise. The parameters are set to be a = 1.5 and λ = 1.
Figure 4: Comparison between theoretical predictions and experiments. (a) 1d experiment. We plotΣ as an increasing function of λ. We see that the continuous-time approximation fails to predict thedivergence at a learning rate and the prediction in Liu et al. (2021) severely underestimates the modelfluctuation. In contrast, our result is accurate throughout the entire range of learning rates. (b)-(c) 2dexperiments. The straight line shows where the proposed theory predicts a divergence in the variance,which agrees with experiment exactly. The Hessian has eigenvalues 1 and 0.5, and λ = 1.5. For alarge batch size, the discrete-time Hessian approximation is quite accurate; for a small S, the Hessianapproximation underestimates the overall strength of the fluctuation. In contrast, the continuous-timeresult is both inaccurate in shape and in strength.
Figure 5: 1d experiments with L2 regularization with weight decay strength γ. The parameters areset to be a = 4, λ = 1, S = 64. This shows a case where the optimal γ is negative. The vertical linesshow where our theory predicts a divergence.
Figure 6: High-dimensional linear regression. We see that the predicted fluctuation coefficient agreeswith the experiment well. The slight deviation is due to a finite training time and finite N and D. Onthe other hand, a naive Hessian approximation results in a qualitatively wrong result.
Figure 7: Tail index β of the stationary distribution of SGD in a 1d linear regression problem. Left toRight: aλ = 0.2, 1.0, 1.8.
