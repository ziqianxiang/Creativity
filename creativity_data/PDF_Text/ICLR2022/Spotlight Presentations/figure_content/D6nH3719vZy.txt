Figure 1: Left: Conventional adversarial attacks view ViT as a single classifier and maximize the predictionloss (e.g., cross entropy) to fool the model based on the last classification token only. This leads to sub-optimalresults as class tokens in previous ViT blocks only indirectly influence adversarial perturbations. In contrast, ourapproach (right) effectively utilizes the underlying ViT architecture to create a self-ensemble using class tokensproduced by all blocks within ViT to design the adversarial attack. Our self-ensemble enables to use hierarchicaldiscriminative information learned by all class tokens. Consequently, an attack based on our self-ensemblegenerates transferable adversaries that generalize well across different model types and vision tasks.
Figure 2: Adversarial examples for ViTs have onlymoderate transferability. In fact transferabililty (%) ofMIM (Dong et al., 2018) perturbations to target mod-els goes down as the source model size increases suchas from DeiT-T (Touvron et al., 2020) (5M parameters)to DeiT-B (Touvron et al., 2020) (86M parameters).
Figure 3: Distribution of discriminative infor-mation across blocks of DeiT models. Notehow multiple intermediate blocks contain fea-tures with considerable discriminative infor-mation as measured by top-1 accuracy on theImageNet val. set. These are standard mod-els pretrained on ImageNet with no furthertraining. Each block (x-axis) corresponds toa classifier Fk as defined in Equation 1.
Figure 4: Recent ViTs process 196 imagepatches, leading to 196 patch tokens. Werearranged these to create a 14x14 featuregrid which is processed by a convolutionalblock to extract structural information, fol-IstBlockShared ClassifierShared Local Norm2st Blocknth Blocknth BlockPretrained &Fixed5â–¡Jlowed by average pooling to create a singlepatch token. Class token is refined via aMLP layer before feeding to the classifier.
Figure 5: Self-Ensemble for DeiT (Touvron et al.,2020): We measure the top-1 accuracy on ImageNetusing the class-token of each block and compare toour refined tokens. These results show that fine-tuninghelps align tokens from intermediate blocks with the fi-nal classifier enhancing their classification performance.
Figure 6: Ablative Study:Fooling rate of intermediate lay-ers under MIM (white-box) at-tack using our self-ensemble ap-proach. We obtain favorable im-provements for our method.
Figure 7: Visualization ofDETR failure cases for ourproposed DIMRE attack gener-ated from DeiT-S source model.
Figure 8:	Our proposed refinement module (Sec. 3.2)processes patch tokens using a convolutional block(He et al., 2016) while the class token is processed bya linear layer. Class and patch tokens are the outputs ofan intermediate ViT block. The convolutional layershave a filter size of 3x3xd. We rearrange the numberof patch tokens into 14x14 grid before feeding themto convolutional block. The embedding dimension(d) of the class token and each patch token dictatesthe number of parameters and inference compute costwithin convolutional and MLP layers of the refinementmodule.
Figure 9:	Class Tokens t-SNE visualization: We extracted class tokens from the intermediate blocks (used forcreating individual models within self-ensemble) and visualize these in 2D space via t-SNE (Van der Maaten &Hinton, 2008). Our refined tokens have lower intraclass variations i.e., feature representations of the same classsamples are clustered together. Further refined tokens have better interclass separation than original tokens. Weused sklearn (Pedregosa et al., 2011) and perplexity is set to 30 for all the experiments. (best viewed in zoom)21Published as a conference paper at ICLR 2022Block Number (CVT-S)Figure 10: Self-Ensemble for CvT (Wu et al., 2021): We measure the top-1 (%) accuracy on ImageNetvalidation set using the class-token and average of patch tokens of each block for CvT and MLP-Mixer andcompare to our refined tokens. These results show that fine-tuning helps align tokens from intermediate blockswith the final classifier enhancing their classification performance. An interesting observation is that pretrainedMLP-Mixer has lower final accuracy than CvT models, however, its early blocks show more discriminabilitythan CvT. This allows a powerful self-ensemble which in turn boost adversarial transferability (Table 15).
Figure 10: Self-Ensemble for CvT (Wu et al., 2021): We measure the top-1 (%) accuracy on ImageNetvalidation set using the class-token and average of patch tokens of each block for CvT and MLP-Mixer andcompare to our refined tokens. These results show that fine-tuning helps align tokens from intermediate blockswith the final classifier enhancing their classification performance. An interesting observation is that pretrainedMLP-Mixer has lower final accuracy than CvT models, however, its early blocks show more discriminabilitythan CvT. This allows a powerful self-ensemble which in turn boost adversarial transferability (Table 15).
Figure 11: Self-Ensemble for ResNet50 (He et al., 2016): We report relative improvement after addingrefinement module for Resnet50 and Deit-S mdoels. Note that the basic version of self-ensemble can not appliedto ResNet50 due to varying channel dimension across different layers. Feature refinement improves adversarialtransfer from ResNet50, however relative gains are significant for vision transformer, Deit-S.
