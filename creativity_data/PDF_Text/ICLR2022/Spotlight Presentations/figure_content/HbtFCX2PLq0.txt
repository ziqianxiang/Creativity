Figure 1: Illustration of our proposal: We propose using knowledge distillation with the basemodel as the teacher (with some mixing parameter λ) and then training on the distilled label. Weshow theoretically that training our loss with the distilled label yields approximately the same solutionas the original churn constrained optimization problem for some slack that depends on λ (or viceversa). The significance is that the simple and popular distillation procedure yields the same solutionas the original churn problem, without having to deal with the additional complexity that comes withsolving constrained optimization problems.
Figure 2: IMDB dataset with Transformer-1, Transformer-4 and Transformer-16. We show the Paretofrontier for each of the baselines. We see that distillation is able to obtain solutions that dominate theother baselines in both churn and accuracy.
Figure 3: Distillation vs Anchor Ablation: We provide an ablation study further showing that usingthe true labels for wrongly predicted examples by the base model (as done in anchor method) isworse than using distillation for all the examples. We show the performance as we vary the numberof wrongly predicted examples that we use the true label instead of the distilled label. The x-axis isthe fraction of the most (sorted by softmax score) wrongly predicted examples (i.e. 0 is distillationand 1 is anchor method) and y-axis is the churn at cold accuracy metric. We show the results forphishing dataset using fcn-1000 and celebA dataset predicting attractiveness using convnet-1, wherethe average accuracies across the runs of the base model were 93.3% and 69.2%, respectively.
Figure 4: IMDB dataset with transformer. Pareto frontier for each baseline and costs of each method,where the cost is a convex combination between the error and the churn, as we vary the weightbetween churn and accuracy. Top two: Initial batch size 100. Middle: Initial batch size 1000.
