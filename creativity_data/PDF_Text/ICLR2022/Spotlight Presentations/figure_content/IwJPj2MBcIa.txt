Figure 1: Motivation behind Compositional Attention. In a visual question answering setting, wesee that the “rigid” mapping between search (query-key) and retrieval (value) in multi-head attentionleads to redundant parameters being learned (middle row; (c)). In contrast, when the search andretrieval mechanisms are disentangled and have a pairing set dynamically, these can be composedefficiently without learning of redundant parameters (bottom row; (c)). For details, refer to Section 2.3Addressing these shortcomings, there are several recent attention-enabled systems developed to allowbetter decomposition and re-composition of knowledge (Goyal et al., 2019; 2021a;b), some of whichwe discuss in Appendix A. However, most of these efforts hinge on purpose-built architecturalcomponents that remain niche and often are difficult to implement at scale. To complement theseefforts and build on the proven efficacy of Transformers, our goal is to develop minimal modificationsto key-value attention to enable flexible decomposition of computations found in attention heads, andeliminate some parameter redundancy. Crucially, we aim for a mechanism that is easily implementedand plug-and-play for existing Transformers (and all the models based on them).
Figure 2: Computation graph for Compositional Attention. We show computations for one searchand two retrievals. Multiple searches operate in parallel with different search but shared retrievalparameters. The outputs are then fed to a linear network to give the final output as in Equation 14Wq ∈ Rd×dk , Wk ∈ Rd×dk and Wv ∈ Rd×dv respectively. This is given byQ = XWq K = XWk V = XWv.	(1)For each query, a similarity score is computed with each key using a scaled cosine similarity (calledscaled dot-product) to give the attention weights which are used to soft-combine the values asAttention(Q, K, V) = Softmax (Q|_ , axis = 'keys') V	(2)where √= is the scaling factor.
Figure 3: Left: Contextual Retrieval Task Illustration. Dynamic search and retrieval based onsearch, retrieval and retrieval context features. Each element has a corresponding output but weshow it only for xi for brevity. Right: Performance on Contextual Retrieval Task. Here, wecompare our proposed model against standard Multi-Head attention model (lower is better) on varioussetups of the task. Our proposed model outperforms the baseline in both in-distribution as well asout-of-distribution settings.
Figure 4: Efficient Composition in Contextual Retrieval Task. We plot the average search-retrievalactivation statistics across data with Left: All possible value combinations, Middle: Subset of valuecombinations used for training, and Right: Remaining subset of value combinations used for OoDtesting. The activation statistics switch distinctly between OoD training and testing and stay aroundthe average when all possible subsets are shown, thus highlighting good specialization.
Figure 5: Retrieval Specialization onContextual Retrieval Task. The pro-posed model learns to specialize its ownretrieval (X-axis) based on ground truthvalues (Y-axis).
Figure 6: Performance on ESBN Tasks. Our proposed model outperforms the baseline acrossdifferent tasks especially in the extreme OoD setting.
Figure 7: Performance on LanguageModeling (WikiText103). We illustratethat our proposed mechanism outper-forms the standard multi-head attention.
Figure 8: Performance on Contextual Retrieval Task. We compare our proposed model againststandard Multi-Head attention (lower loss is better) on various setups of the task. Our proposed modeloutperforms the baseline across various model capacities (low and high) and number of heads.
Figure 9: Performance on OoD Contextual Retrieval Task. We showcase that our proposedmechanism outperforms standard Multi-Head attention (lower is better) on out of distribution (OoD)variant of the various setups across various model capacities (low and high) and number of heads.
Figure 10: Convergence on Contextual Retrieval Task. We see that the proposed mechanismconverges faster and works well even in low data regime (low iterations).
Figure 11: Specialization plots for the Contextual Retrieval Task. We plot the attention scores forground truth retrieval Vs learned retrieval for different task setups - left: 1 search 2 retrieval, middle:1 search 4 retrieval, and right: 2 search 4 retrieval.
Figure 12: Contextual Retrieval Task.
Figure 13: Sort-of-CLEVR. Samples from the dataset. Non-relational refers to unary type questionsand Relational refers to binary and ternary type questions. Source: Santoro et al. (2017)Algorithm	Dimensions	Heads	Unary Accuracy	Binary Accuracy Ternary Accuracy	Transformer	32	2	66.2±8.8	72.8±o.8	54.5±53.6Compositional Transformer		-	74.1±13.2	[73.7±2∙o	53.6±0.8Transformer	256	4	98.6±o.2	84.4±5.3	64.9±3.3		8	98.5±o.2	84.5±6.o	65.4±4.7Compositional Transformer		-	98.8±0.1	[88.2±3.2	66.9±1.8Transformer	512	4	98.5±o.6	84.2±4.7	61.5±4.8		8	98.5±o.4	81.5±5.o	∣62.2±4.6Compositional Transformer		-	∣98.9±o.3	∣84.5±5.o	62.1 ±4.3Table 7: Dimensions and Heads Ablation on Sort of CLEVR. We perform ablations with increasednumber of dimensions and heads. For proposed model, We use 2 searches - 2 retrievals for 32dimensional model and 4 searches - 2 retrievals for other dimensions.
Figure 14: Sort-of-CLEVR Re-trieval Activation. Activationstatistics against the different typesof questions in the dataset.
Figure 15: Search-Retrieval Pair-ing in Equilateral Triangle Detec-tion. We visualize the activationstatistics of learned retrievals (Xaxis) against learned searches show-ing that Compositional Attentioncan also learn tight pairing betweensearch and retrieval.
Figure 16: Logical Reasoning in ESBN Tasks. Illustration of the four tasks in the suite. (a)Same/Different. Predict whether the two objects are identical or not; they are not in this case, (b)RMTS. Match the relation in the context image to the two choices; here option 2 is the right answersince similar to context objects, it also has two identical objects, (c) Distribution of 3. Find themissing object with permutations rule; option 2 is the right answer since the square is in the contextobjects and missing from the second row, and (d) Identity Rules. Find the missing object with ABArule; option 1 is the correct answer since it follows the same rule of identical objects on the edges.
Figure 17: Logical Reasoning in ESBN Tasks. We see that compositional attention outperformsmulti-head attention baseline over different number of retrievals, especially on Same/Different andRMTS. Compositional-r refers to the proposed model with r retrievals.
