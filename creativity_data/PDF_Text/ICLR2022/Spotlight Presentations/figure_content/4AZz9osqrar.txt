Figure 1: Relative performance gap (lower is better) between imbalanced and balanced represen-tation learning. The gap is much smaller for self-supervised (MoCo v2) representations (∆SSL inblue) vs. supervised ones (∆SL in red) on long-tailed ImageNet with various number of examples n,across both ID (a) and OOD (b) evaluations. See Equation (1) for the precise definition of the relativeperformance gap and and Figure 2 for the absolute performance.
Figure 2: Representation quality on balanced and imbalanced datasets. Left: CIFAR-10, SL vs.
Figure 3: Explaining SSL’s robustness in a toy setting. e1 and e2 are two orthogonal directionsin the d-dimensional Euclidean space that decides the labels, and e3:d represents the other d - 2dimensions. Classes 1 and 2 are frequent classes and the third class is rare. To classify the threeclasses, the representations need to contain both e1 and e2 directions. Supervised learning learnsdirection e1 from the frequent classes (which is necessary and sufficient to identify classes 1 and 2)and some overfitting direction v from the rare class which has insufficient data. Note that v might bemostly in the e3:d directions due to overfitting. In contrast, SSL learns both e1 and e2 directions fromthe frequent classes because they capture the intrinsic structures of the inputs (e.g., e1 and e2 are thedirections with the largest variances), even though e2 does not help distinguish the frequent classes.
Figure 4: Visualization of SSL features in semi-synthetic settings. Left: The right halves of therare examples decide the labels, while the left are blank. The left halves of the frequent examplesdecide the labels, while the right halves are random half images, which contain label-irrelevant-but-transferable features. Middle: Visualization of features with Grad-CAM (Selvaraju et al., 2017).
Figure 5: Visualization of the label distributions. We visualize the label distributions of the imbal-anced CIFAR-10 and ImageNet. We consider two imbalance ratios r for each dataset.ImbalancedCIFAR-10 follows the exponential distribution, while imbalanced ImageNet follows Pareto distribu-tion.
Figure 6:	OOD Results of SimSiam on ImageNet. SimSiam also demonstrates more robustnessto class imbalance compared to supervised learning. The relative gap to balanced dataset is muchsmaller than supervised learning across different imbalance ratios.
Figure 7:	Examples of the semi-synthetic Datasets and Grad-CAM visualizations. SimCLRlearns features from both left and right sides, whereas SL mainly learns label-relevant features fromthe left side of frequent data and ignore label-irrelevant features on the right side. In Figure 4, weprovide results the high-resolution images of the 10 CIFAR classes to make the results easier tointerpret. Here we further visualize the results on original CIFAR-10 images.
