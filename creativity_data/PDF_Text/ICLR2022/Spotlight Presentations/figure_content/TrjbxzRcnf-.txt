Figure 1: Adding a memory of 8K tokens improves perplexity across different model sizes.
Figure 2: We extend Transformers with access to (key, value) pairs of previously seen subsequences.
Figure 3: Our data pipeline splits documents into subsequences and packs subsequences into batches.
Figure 6: Finetuning a 1B vanilla Transformer model to use external memory of size 65K.
Figure 7: Difference in loss for each token in a randomly chosen paper, using the same model oncewith a memory size of 8K and once with 32K. Higher numbers mean the longer memory helped incomparison to the shorter memory. This paper is 22K tokens long.
Figure 9: Histogram of the number of tokens in arXiv math papers dataset. We tuncated the histogramat 500k tokens. The maximum paper had almost 1.6M tokens.
Figure 10: Histogram of the number of tokens in Github repositories dataset. We cut off the long tailof this plot. The repository with the maximum length has just over 9M tokens.
Figure 11: Histogram of the number of tokens in Isabelle proof scripts dataset.
Figure 12: Histogram of the number of tokens in PG19 books dataset.
Figure 13: Histogram of the number of tokens in C4 documents filtered by documents that have lessthan 4096 tokens.
Figure 19:	Definition of Fourier_sum_limit_pair.
Figure 20:	Definition of orthonormal_system_trigonometric_set.
Figure 21:	Definition of Fourier_series_square_summable.
Figure 22:	Definition of orthonormal_system.
