Figure 1: Illustration of how an element g ∈ Cn acts on the feature map by rotating the pixels andtransforming the channel space through ρ0, ρ1, and ρreg. Left: Cn acts on the channel space of a1-channel feature map by identical mapping. Middle: Cn acts on the channel space of a vector fieldby rotating the vector at each pixel through ρ1 . Right: Cn acts on the channel space of a 4-channelfeature map by permuting the order of the channels by ρreg .
Figure 2: The manipulation scene (a)and the visual state space (b).
Figure 3: Illustration of Q-map equivariance. The outputQ-map rotates with the inputimage.
Figure 4: Illustration of the equivari-ant actor network (top) and the invariantcritic network (bottom).
Figure 5: The experimental environments implemented in PyBullet (Coumans & Bai, 2016). Theleft image in each sub figure shows an initial state of the environment; the right image shows thegoal state. The poses of the objects are randomly initialized.
Figure 6: Comparison of Equivariant DQN (blue) with baselines. The plots show the evaluationperformance of the greedy policy in terms of the discounted reward. The evaluation is performedevery 500 training steps. Results are averaged over four runs. Shading denotes standard error.
Figure 7: Comparison of Equivariant SAC (blue) with baselines. The plots show the evaluationperformance of the greedy policy in terms of the discounted reward. The evaluation is performedevery 500 training steps. Results are averaged over four runs. Shading denotes standard error.
Figure 8: Comparison of Equivariant SACfD (blue) with baselines. The plots show the evaluationperformance of the greedy policy in terms of the discounted reward. The evaluation is performedevery 500 training steps. Results are averaged over four runs. Shading denotes standard error.
Figure 9: Comparison of Equivariant SACfD (blue) with baselines. The plots show the evaluationperformance of the greedy policy in terms of the discounted reward. The evaluation is performedevery 500 training steps. Results are averaged over four runs. Shading denotes standard error.
Figure 10: Comparison of Equivariant SACfD withbaselines. Results are averaged over four runs.
Figure 11: The object set forObject Picking environmentThe Block Pulling requires the robot to pull one block to make contact with the other block. TheObject Picking requires the robot the pick up an object randomly sampled from a set of 11 objects(Figure 11). The Drawer Opening requires the robot to pull open a drawer. The Block Stackingrequires the robot to stack one block on top of another. The House Building requires the robot tostack a triangle roof on top of a block. The Corner Picking requires the robot to slide the block fromthe corner and then pick it up.
Figure 12: The architecture of the Equivariant DQN (a) and the Equivariant SAC (b). ReLU nonlin-earity is omitted in the figure. A convolutional layer with a suffix of R indicates a regular represen-tation layer (e.g., 16R is a 16-channel regular representation layer); a convolution layer with a suffixof T indicates a trivial representation layer (e.g., 1T is a 1-channel trivial representation layer).
Figure 13: The architecture of the baseline conventional CNN DQN (a) and the baseline conven-tional CNN SAC (b). The baseline CNN architectures have similar amount of trainable parametersas the equivariant architectures. Specifically, Equivariant DQN has 2.6M parameters, and baselineDQN has 3.9M parameters; Equivariant SAC has 2.3M parameters, and baseline SAC has 2.6Mparameters. ReLU nonlinearity is omitted in the figure.
Figure 14: (a)-(b): additional results for Section 6.3. (c)-(d): additional results for Section 6.4.
Figure 15: Ablation of using equivariant network solely in actor network or critic network. The plotsshow the evaluation performance in terms of discounted reward during training. The evaluation isperformed every 500 training steps. Results are averaged over four runs. Shading denotes standarderror.
Figure 16: Ablation of using different symmetry groups (C8 , C4, or C2), in Equivariant SACfD.
Figure 17: Ablation of using equivariant architecture in non-symmetric tasks. The plots show theevaluation performance of the greedy policy in terms of the discounted reward. The evaluation isperformed every 500 training steps. Results are averaged over four runs. Shading denotes standarderror.
Figure 18: Ablation of comparing against rotational augmentation baselines applied with rotationalbuffer augmentation. The plots show the evaluation performance of the greedy policy in terms ofthe discounted reward. The evaluation is performed every 500 training steps. Results are averagedover four runs. Shading denotes standard error.
