Figure 1: Pixelfly targets GEMM-based networks (networks whose computation is dominated by matrix multiply), whichit views as a series of matrix multiplication. For each matrix multiply from Model Schema, it (1) allocates computebudget based on dimension and layer type, (2) the budget decides a mapping (hyper-parameter) to our proposed flat blockbutterfly sparsity patterns, (3) outputs a hardware-aware sparse mask. Note since the hardware is a block device, onememory access to an element in a block leads to the access to the full block.
Figure 2: Visualization ofmemory access for a hardwarewith block size 4: accessingthe one (red) location meansaccessing the full 4 × 4 block(blue).
Figure 3: Visualization of Flat, Block, and Flat Block butterfly.
Figure 4: NTK Comparison withDense Model.
Figure 5: The performance of Pixelfly and ViT or MLP-Mixer on CIFAR10,CIFAR100 and ImageNet benchmarks. We measure the accuracy and the trainingtime speedup (on ImageNet) compared to the dense model.
Figure 6: Comparison with a representative sparsetraining baseline RigL (Evci et al., 2020).
Figure 7: Comparison with representative sparse attention base-lines.
Figure 8: The performance of Pixelfly, BigBird and GPT-2-Small, Medium on WikiText-103. We measure the perplexityand the training speed up.
Figure 9: The performance of Pixelfly, Reformer and vanilla transformer on Long-Range-Arena benchmarks. We measure the accuracy and training speed.
Figure 10: Sparsity Mask for Rectangular Matrices.
Figure 11: Speedup of multiplying Mflatx compared to multiplying Mx. Flattening the products yields up3 × speedup.
Figure 12: Sparsity pattern candidate components: Local corresponds to local interaction of neighboringelements; Global (low-rank) involves the interaction between all elements and a small subset of elements;Butterfly captures the interaction between elements that are some fixed distance apart; Random is common inthe pruning literature.
Figure 13: Speed-accuracy tradeoff of Pixelfly on ImageNet classification, with Mixer-B/16 as the densemodel. Pixelfly maintains or exceeds the accuracy of the dense model, up to around 2.3× speedup (or around30% of the number of parameters). Performance degrades when the Pixelfly model has fewer than 30% of thenumber of parameters.
