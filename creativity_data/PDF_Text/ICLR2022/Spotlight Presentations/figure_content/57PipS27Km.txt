Figure 1: Illustration of the adaptation process in (a) a gradient-based meta-learning algorithm, suchas ANIL (Raghu et al., 2019), where the adapted parameters WT are given after T steps of gradientdescent, and in (b) Continuous-Time Meta-Learning (COMLN), where the adapted parameters W (T)are the result of following the dynamics of a gradient vector field up to time T .
Figure 2: Numerical instability of the adjoint method appliedto the gradient vector field of a quadratic loss function. Thetrajectory in green starting at W (0) corresponds to the inte-gration of the dynamical system in (8) forward in time up toT , and the trajectory in red starting at W (T ) corresponds toits integration backward in time. Note that T was chosen sothat W (T) does not reach the equilibrium/minimum of theloss W?.
Figure 3: Empirical efficiency of COMLN on a single 5-shot 5-way task, with a Conv-4 backbone.
Figure 4: Empirical efficiency of COMLN on a single 5-shot 5-way task, with a ResNet-12 backbone;this figure is similar to Figure 3. (Left) Memory usage for computing the meta-gradients as a functionof the number of inner-gradient steps. The extrapolated dashed lines correspond to the methodreaching the memory capacity of a Tesla V100 GPU with 32Gb of memory. (Right) Average timetaken (in ms) to compute the exact meta-gradients. The extrapolated dashed lines correspond to themethod taking over 3 seconds.
Figure 5: (Left) Evolution of the meta-parameter T controlling the amount of adaptation necessaryfor all tasks during meta-training. Here, the backbone is a ResNet-12. We normalized the duration ofmeta-training in [0, 1] to account for early stopping; typically the model for miniImageNet requires anorder of magnitude fewer iterations. (Right) Comparison of COMLN (where T is learned, in red) tometa-learning with a fixed length of adaptation T (in blue), on a 5-shot 5-way classification problemon the miniImageNet dataset (with a Conv-4 backbone).
