Figure 2: Left: Architecture diagram for complete HAL method. Q-Values of the meta-COntrOller are maskedby the output of the affordance classifier. The e operator represents the standard Ggreedy action selectionprocedure used in Q-learning, while 2 represents our affordance aware version. Right: For an optimal policy(top), the mask will have no effect since Q values will naturally be low for unafforded subtasks. However,a suboptimal policy (bottom) will benefit from a mask since it can be efficiently learned and used to pruneirrelevant subtasks before TD errors can propagate.
Figure 3: Screenshots of crafting (left) and treasure (right) environments. Displayed to theright of the environments are each item’s ground-truth affordance indicator and inventory count.
Figure 4:	Success rate over the course of training for crafting iron task (left) and treasure(center). Sub-policy success for treasure (right). Success rate is the proportion of episode wherethe agent receives the target milestone, and sub-policy success is how often sub-policies, on average,receive the correct milestone when called, before a timing out or collecting incorrect milestones.
Figure 5:	Comparing the robustness of HAL and HR+H to varying milestone sets (left) and variousedge stochasticity frequencies (center and right, respectively) in crafting iron task.
Figure 6: Percentage of episodes whereeach milestone is achieved in craft-ing environment task-agnostic setting.
Figure 7: Abstract representation of crafting subtask hierarchy, where milestones are circled andarrows indicate to explicit subtask dependencies between milestones and numbers indicate the depthof each item in the hierarchy (used in Figure 6). Numerical preconditions for crafting recipes are notshown, as well as other possible implicit environmental dependencies (e.g. mining X to reach y).
Figure 8: Abstract representation of one possible treasure subtask hierarchy, where milestonesare circled and arrows indicate to explicit subtask dependencies between milestones.
Figure 9: Walk-through of a successful treasure task episode. Items currently in the agent’spossession are indicated by the numbers on the right hand side and ground-truth affordances areindicated by green circles if the milestone is afforded and red if not.
Figure 10: Walk-through of successful crafting environment diamond task episode.
Figure 11: Training curves for component-wise ablations of HAL on treasure task. HAL refersto the full method, while the rest of the items displayed in the legend indicate which componentsare removed. -RAI no longer uses the context representation as input to the affordance classifier(the classifier is trained directly over the state). -RT no longer uses affordance classifier loss toadditionally tune the context representation weights. -CL removes the contrastive loss altogether,allowing the weights to be trained solely with affordance classifier gradients. Lastly, -FNF nolonger filters false negatives using the learned context representation.
Figure 12:	Plots comparing HAL’s final success rate with that of HR+H’s on treasure task withalternative hyperparameter settings. Left: percentile values used for defining the upper confidencebound ρ for false negative filtering. Center: standard deviation values, σ, used for sampling positivepoints from the (truncated) normal distribution in the contrastive loss. Right: threshold value overwhich the affordance classifier output is discretized.
Figure 13:	Plots evaluating the efficacy of various σ values used for the contrastive loss in a stochas-tic CRAFTING environment on the iron task, with item disappearance rate 1/50 steps. Left: finalcontrastive loss values for HAL. Right: final HAL success rates compared to HR+H.
Figure 14: Episode length over the course of training for all baselines on crafting iron (topleft) and treasure (top right), corresponding to Figure 4. Episode length with variable levels ofstochasticity crafting iron (bottom), corresponding to Figure 5.
