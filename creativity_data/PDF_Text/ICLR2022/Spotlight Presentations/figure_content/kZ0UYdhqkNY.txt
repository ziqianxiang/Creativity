Figure 1: Illustration of SNVL We first learn the likelihoodp(x∣θ) for any θ. We then use variationalinference to learn the posterior distribution by minimizing a general divergence measure D. Theobtained posterior distribution is sampled with sampling importance resampling (SIR) to run newsimulations and refine the likelihood estimator.
Figure 2: A Posterior approximations of SNLE, SNPLA, and SNVI+fKL for the two moons bench-mark example. B Runtime of all algorithms.
Figure 3: C2ST benchmark results for SNVI with likelihood-estimation (SNLVI) for four mod-els, Bernoulli GLM (A), Lotka volterra (B), Two moons (C) and SLCP (D). Each point representsthe average metric value for ten different observations, as well as the confidence intervals. Barson the right indicate the average runtime. Two reference methods: SNLE with MCMC sampling,and SNPLA (which uses rKL), as well as three variants of SNVI, with forward KL (SNVI+fKL),importance-weighted ELBO (SNVI+IW) and α-divergence (SNVI+α). Dotted lines: Performancewhen not using SIR.
Figure 4: (A) Empirical observation, arrows indicate some of the summary statistics. Scale bar is onesecond. (B) Cornerplot showing a subset of the marginal and pairwise marginal distributions of the31-dimensional posterior (full posterior in Appendix Fig. 12). Red dot: MAP. Black dot: Posteriormean. (C) Conditional distributions p(θi,j |x, θ6=i,j). Green dot shows the sample on which wecondition. (D) Simulated traces from the posterior mean and MAP. Scale bar is one second. (E)Simulated traces of three posterior samples. (F) Posterior predictive and prior predictive median(z-scored) distances from the observation. (G) Time required to obtain 10k samples: SNVI takes 11minutes and SNLE with 100-chain MCMC 808 minutes, i.e. over 13 hours.
Figure 5: A Left: Gradient estimation on the Gaussian example. For values of μ around μ* = 4/5,all estimators provide good gradients. As μ is farther from μ*, the forward variational bound (fVB)(grey) vanishes, whereas the self-normalized fVB approaches a constant. Right: SNR for the fVBand the self-normalized fVB. B Theoretical and empirical densities pκ(r) for μ = 6 and μ = 12.
Figure 6: Visualization of SIR. A Toy example with a Gaussian proposal density. Left: Two toy ex-amples with a Gaussian (top) or bimodal (bottom) target density. SIR (with K = 32) can extend theGaussian density and refine the approximation if the proposal is overdispersed (middle), but helpsless when it is too narrow (right). B SIR improvements on two moons example. We plot the jointdensity as learned by the likelihood-model p(xo, θ) = 'ψ(xo∣θ)p(θ) against the variational Poste-rior qφ (blue, obtained with the fKL), as well as the SIR-corrected density with K = 2 (orange) andK = 32 (green). DesPite using an exPressive normalizing flow as qφ, SIR imProves the accuracy.
Figure 7: A neural sPline flow (NSF) estimating a bimodal likelihood with 104 simulations with PriorN(0, 2). ToP: Ground truth. Bottom: Likelihood-aPProximation with NSF. The learned likelihoodclosely matches the true likelihood.
Figure 8: Comparison between SNPLA implementation of (Wiqvist et al., 2021) and SNVI withrKL.
Figure 9: Samples from the posterior distributions for SNLE with MCMC, SNVI + fKL, SNVI +rKL. First row: results for SLCP. Second row: Lotka-Volterra. Third row: Bernoulli GLM.
Figure 10: C2ST benchmark results for SNVI with ratio estimation (SNRVI) for four models,Bernoulli GLM (A), Lotka Volterra (B), Two moons (C) and SLCP (D). Each point represents theaverage metric value for ten different observations, as well as the confidence intervals. Bars on theright indicate the average runtime. Two reference methods: SNRE with MCMC sampling and therKL, as well as three variants of SNVI, with forward KL (SNVI+fKL), importance-weighted ELBO(SNVI+IW) and α-divergence (SNVI+α). Dotted lines: performance when not using SIR.
Figure 11:	Evaluation of further variational objectives for the two moons (top) and the SLCP (bot-tom) task. Left: Variations of the forward KL (with and without self-normalized weights). Middle:Variations of the IW-ELBO (with and without STL). Right: Variations of the α-divergence (withand without STL as well as for different values of α.
Figure 12:	Posterior distribution for the neuroscience model of the pyloric network. In Fig. 4B weshow a subset. The black point is a mean estimate using 107 samples. The red point is a maximuma-posterior estimate, obtained by gradient ascent.
Figure 13: Runtime of the classifier cζ (θ) in the model of the pyloric network (90% of simulationsare invalid). Training the classifier is approximately three times cheaper than training the likelihood-model (compare left bar to second left) and thus increases the computational cost only modestly.
