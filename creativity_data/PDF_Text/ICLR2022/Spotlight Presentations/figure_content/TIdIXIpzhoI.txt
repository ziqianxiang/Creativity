Figure 1: A visualization of two iterations of our proposed progressive distillation algorithm. Asampler f (z; η), mapping random noise to samples x in 4 deterministic steps, is distilled into anew sampler f (z; θ) taking only a single step. The original sampler is derived by approximatelyintegrating the probability flow ODE for a learned diffusion model, and distillation can thus beunderstood as learning to integrate in fewer steps, or amortizing this integration into the new sampler.
Figure 2: Left: Log weight assigned to reconstruction loss ∣∣x -攵入 k 2 as a function of the log-SNRλ = log[α2/σ2], for each of our considered training loss weightings, excluding the influence ofthe αt , σt schedule. Right: Weights assigned to the reconstruction loss including the effect of thecosine schedule at = cos(0.5∏t), with t 〜U[0,1]. The weights are only defined UP to a constant,and we have adjusted these constants to fit this graph.
Figure 3: Random samples from our distilled 64 × 64 ImageNet models, conditioned on the ‘mala-mute’ class, for fixed random seed and for varying number of sampling steps. The mapping frominput noise to output image is well preserved as the number of sampling steps is reduced.
Figure 4: Sample quality results as measured by FID for our distilled model on unconditionalCIFAR-10, class-conditional 64x64 ImageNet, 128x128 LSUN bedrooms, and 128x128 LSUNchurch-outdoor. We compare against the DDIM sampler and against an optimized stochastic sam-pler, each evaluated using the same models that were used to initialize the progressive distillationprocedure. For CIFAR-10 we report an average over 4 random seeds. For the other data sets weonly use a single run because of their computational demand. For the stochastic sampler we set thevariance as a log-scale interpolation between an upper and lower bound on the variance, follow-ing Nichol & Dhariwal (2021), but we use a single interpolation coefficient rather than a learnedcoefficient. We then tune this interpolation coefficient separately for each number of sampling stepsand report only the best result for that number of steps: this way we obtained better results than withthe learned interpolation.
Figure 5: Visualization of reparameterizing the diffusion process in terms of φ and vφ .
Figure 6: FID of generated samples from distilled and undistilled models, using DDIM or stochasticsampling. For the stochastic sampling results we present the best FID obtained by a grid-search over11 possible noise levels, spaced log-uniformly between the upper and lower bound on the varianceas derived by Ho et al. (2020). The performance of the distilled model with stochastic samplingis found to lie in between the undistilled original model with stochastic sampling and the distilledDDIM sampler: For small numbers of sampling steps the DDIM sampler performs better with thedistilled model, for large numbers of steps the stochastic sampler performs better.
Figure 7: Random samples from our distilled CIFAR-10 models, for fixed random seed and forvarying number of sampling steps.
Figure 8: Random samples from our distilled 64 × 64 ImageNet models, conditioned on the ‘coralreef’ class, for fixed random seed and for varying number of sampling steps.
Figure 9:	Random samples from our distilled 64 × 64 ImageNet models, conditioned on the ‘sportscar’ class, for fixed random seed and for varying number of sampling steps.
Figure 10:	Random samples from our distilled LSUN bedrooms models, for fixed random seed andfor varying number of sampling steps.
Figure 11:	Random samples from our distilled LSUN church-outdoor models, for fixed random seedand for varying number of sampling steps.
Figure 12: Comparing our proposed schedule for progressive distillation taking 50k parameter up-dates to train a new student every time the number of steps is halved, versus fast sampling schedulestaking fewer parameter updates (25k, 10k, 5k), and a fast schedule dividing the number of steps by4 for every new student instead of by 2. All reported numbers are averages over 4 random seeds.
Figure 13: Comparing our proposed schedule for progressive distillation taking 50k parameter up-dates to train a new student every time the number of steps is halved, versus a fast sampling scheduletaking 10k parameter updates. For each reported number of steps we selected the optimal learningrate from [5e-5, 1e-4, 2e-4, 3e-4]. Results are for a single random seed.
