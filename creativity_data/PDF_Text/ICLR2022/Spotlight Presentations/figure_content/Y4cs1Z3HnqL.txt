Figure 1: (a) Illustration of the uncertainty estimations in the regression task. The white dots rep-resent data points, and the color scale represents the bootstrapped-uncertainty values in the wholeinput space. (b) Illustration of the workflow of PBRL. PBRL splits the loss function into two com-ponents. The TD-error (in) represents the regular TD-error for in-distribution data (i.e., from theoffline dataset), and pseudo TD-error (ood) represent the loss function for OOD actions. In theupdate of Q-functions, both losses are computed and summed up for the gradient update.
Figure 2: Average training curve in Gym.
Figure 3: The uncertainty and Q-value for different state-action sets in the training process.
Figure 4:	The Ablation on the number of bootstrapped Q-functions.
Figure 5:	The Ablation on penalty strategy for the in-distribution data.
Figure 6:	The Ablation on the tuning parameter βin in the in-distribution target.
Figure 7:	The Ablation on the tuning parameter βood in the OOD target.
Figure 8: With different settings of βood , we show the Q-value for state-action pairs sampled fromDin in the training process.
Figure 9: The Ablation on action selection scheme of the actor.
Figure 10:	The Ablation on different number of OOD actions. the performance becomes better evenwith very small amout of OOD actions.
Figure 11:	The Ablation on OOD target with yood = 0 (normalized scores).
Figure 12:	The Ablation on OOD target with yood = 0. Q-offline is the Q-value for (s, a) pairssampled from the offline dataset, where a follows the behavior policy.
Figure 13:	The Ablation on OOD target with yood = 0. Q-CurrPolicy is the Q-value for (s, aπ)pairs, where a∏ 〜π(a∣s) follows the training policy π.
Figure 14: Comparision of different regularizers (normalized score)PI-Small ∙ SN[-1,-2]	—⅛— SN[-1]	* L 2(1e-4)	∙ L 2(1e-2)	▼ None	—⅛— PBRLPI-IargeFigure 15: Comparision of different regularizers (Q-value along trajectories of the training policy)hopper-medium-replay-v22004006008001000-i— PI-Iarge -PI-Small	—SN[-1,-2]	—SN[-1]	—L2(1e-4)	—L2(1e-2)	—ψ- NoneT- PBRLwalker2d-medium-v2Ay°d 3uo」」no ə-JO-U一s-JΦOU∩∞O325-20-15-
Figure 15: Comparision of different regularizers (Q-value along trajectories of the training policy)hopper-medium-replay-v22004006008001000-i— PI-Iarge -PI-Small	—SN[-1,-2]	—SN[-1]	—L2(1e-4)	—L2(1e-2)	—ψ- NoneT- PBRLwalker2d-medium-v2Ay°d 3uo」」no ə-JO-U一s-JΦOU∩∞O325-20-15-10-5-0-
Figure 16: Comparision of different regularizers (Uncertainty along trajectories of the trainingpolicy)27Published as a conference paper at ICLR 2022E Experiments in Adroit DomainIn Adroit, the agent controls a 24-DoF robotic hand to hammer a nail, open a door, twirl a pen,and move a ball, as shown in Fig. 17. The Adroit domain includes three dataset types, namely,demonstration data from a human (‘human’), expert data from an RL policy (‘expert’), and fifty-fiftymixed data from human demonstrations and an imitation policy (‘cloned’). The adroit tasks are morechallenging than the Gym domain in task complexity. In addition, the use of human demonstrationin the dataset also makes the task more challenging. We present the normalized scores in Table 4.
Figure 17: Illustration of tasks in Adroit domain.
Figure 18: Aggregate metrics on D4RL with 95% CIs based on 15 tasks and 5 seeds for each task.
Figure 19: Performance profiles on D4RL based on score distributions (left), and average scoredistributions (right). Shaded regions show pointwise 95% confidence bands based on percentilebootstrap with stratified sampling. The T value where the profiles intersect y = 0.5 shows themedian, and the area under the performance profile corresponds to the mean.
