Figure 1: COBERL. A) General Architecture. We use a residual network to encode observations intoembeddings Yt . We feed Yt through a causally masked GTrXL transformer, which computes the predictedmasked inputs Xt and passes those together with Yt to a learnt gate. The output of the gate is passed through asingle LSTM layer to produce the values that we use for computing the RL loss. B) Contrastive loss. We alsocompute a contrastive loss using predicted masked inputs Xt and Yt as targets. For this, we do not use the causalmask of the Transfomer. For details about the contrastive loss, please see Section 2.1. C) Computation of qxt andqyt (from Equation 1) with the round brackets denoting the computation of the similarity between the entries D)Regularization terms from Eq. 3 which explicitly enforce self-attention consistency.
Figure 2: Area under thecurve (AUC) for Atari on all57 levels (higher is better).
Figure 3: Area under the curve(AUC) for DmControl (higher isbetter).
Figure 4: DMLab-30 experiments. a) Human normalised returns in DMLab-30 across all the 30 levels (higheris better). b) Area under the curve for all the 30 DMLab levels. Results are over 5 seeds and the final 5% oftraining.(lower) c) Area under the curve (AUC) for DMLab-30 across all the 30 levels. (higher is better)the setup of GTrXl (Parisotto et al., 2020), which we use as our baseline. In Figure 4A we showthe final results on the DMLab-30 domain. If we look at all the 30 games, CoBERL reaches asubstantially higher score than GTrXL (COBERL=115.47% ± 4.21%, GTrXL=101.54% ± 0.50%,t(60)=4.37, p=1.14e-5, Figure 4A). In Figure 4B we analysed data efficiency by computing the AUCand associated average statistics. In DMLab the difference between models is even more significantthan Atari, t(60)=6.097, p=9.39e-09, with COBERL (M=1193.28, SD=383.29) having better averageAUC than GTrXL (M=764.09, SD=224.51), hence showing that our methods scales well to morecomplex domains. (see Appendix F for learning curves)4.2	AblationsIn Sec. 2, we explained contributions that are essential to CoBERL. We now disentangle the addedbenefit of these two separate contributions. Moreover, we run a set of ablations to understand therole of model size on the results. Ablations are run on 7 Atari games chosen to match the ones in theoriginal DQN publication (Mnih et al., 2013), and on all the 30 DMLab games.
Figure 5: Learning Curves for Atari. The x-axis represents number of environment steps in millions. The y-axisrepresent the Human Normalised score. The error represents the 95% confidence interval. Red is CoBERL,BLUE is GTrXL29Published as a conference paper at ICLR 2022F.2 DMControl Learning curvesFigure 6: Learning Curves for DMControl. The x-axis represents number of environment steps in millions. They-axis represent the Episode return. The error represents the 95% confidence interval. Red is CoBERL, BLUEis GTrXL30Published as a conference paper at ICLR 2022F.3 DMLab Learning curvesconcept task Oi trainconcept task 03 trainemstm non matrh1.21.5emstm watermaMkeys d8rs randomIanaJeaSy shapes
Figure 6: Learning Curves for DMControl. The x-axis represents number of environment steps in millions. They-axis represent the Episode return. The error represents the 95% confidence interval. Red is CoBERL, BLUEis GTrXL30Published as a conference paper at ICLR 2022F.3 DMLab Learning curvesconcept task Oi trainconcept task 03 trainemstm non matrh1.21.5emstm watermaMkeys d8rs randomIanaJeaSy shapes0.50.&1.50.7⅛0.5G0.2⅛
Figure 7:	Learning Curves for DMLab. The x-axis represents number of environment steps in billions. The y-axis represent the Human Normalised score. The error represents the 95% confidence interval. Red is CoBERL,BLUE is GTrXL31Published as a conference paper at ICLR 2022G LicensesThe The Arcade Learning Environment Bellemare et al. (2013) is released as free, open-sourcesoftware under the terms of the GNU General Public License. The latest version of the source code ispublicly available at: http://arcadelearningenvironment.orgDeepMind Control Suite Tassa et al. (2018) is released as free, open-source software un-der the terms of Apache-2.0 License. The latest version of the source code is pub-licly available at: https://github.com/deepmind/dm_control/blob/master/dm_control/suite/README.mdDmLab Beattie et al. (2016) is released as free, open-source software under the terms of Apache-2.0License. The latest version of the source code is publicly available at: https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/dmlab30H Pseudo-codeAlgorithm 1 Pseudo-code for CoBERLtraining_iterations J 0initialise_weights(VisualEncoder, TransformerStack, Gate, ValueNetwork)
Figure 8:	Human normalised score as a function of the masking percentage on the input sequence.
Figure 9: AUC calculated at 100% human score.
