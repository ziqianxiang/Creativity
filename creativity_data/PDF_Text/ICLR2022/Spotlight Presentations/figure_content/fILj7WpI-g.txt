Figure 1: The PerceiVer IO architecture can be used on domains with a wide variety of input andoutput spaces, including multi-task language understanding, dense visual tasks like optical flow,hybrid dense/sparse multimodal tasks such as video+audio+class autoencoding, and tasks withsymbolic outputs like StarCraft IL See Tables 5 and 6 for details of all domains considered here.
Figure 2: The Perceiver IO architecture. Perceiver IO maps arbitrary input arrays to arbitrary outputarrays in a domain agnostic process. The bulk of the computation happens in a latent space whose sizeis typically smaller than the inputs and outputs, which makes the process computationally tractableeven for very large inputs & outputs. See Fig. 5 for a more detailed look at encode, process, anddecode attention.
Figure 3: We construct queries with output-specific features to produce outputs with differentsemantics. For settings where each output point differs only in its position, like language, a positionembedding can be used. Input features for the target output can also be used to query, either alone(as for StarCraft II) or alongside position features (as for flow). For multi-{task, modal} settings weuse one embedding for each {task, modality} instead of each position. A single learned embeddingsuffices for simple classification tasks, like ImageNet. For tasks with heterogeneous outputs likemultimodal autoencoding, features that are specific to some queries (like xy position) can be combinedwith modality embeddings, which also pad embeddings to fixed length.
Figure 4: Multimodal audio-video-label autoencoding with 88x compression. Side-by-side: inputson left, reconstructions right. See the supplemental material for example output video and audio.
Figure 5: Schematic depiction of encode, process, and decode attention. Each attention module usesthe same operations, but differs in which inputs are used to generate key/values or queries and inthe output shape. Encode attention can be viewed as mapping an input to a latent space, typicallywith a smaller index dimension (fewer elements). Decode attention can be viewed as mapping alatent to an output space, often with a larger index dimension (more elements). Both of these areforms of cross-attention. Process attention (self-attention) preserves the input index dimension (sameelements). Red and blue dashed lines are used to highlight the two matrix multiplications used inQKV attention, as described in the text.
Figure 6: Single-query attention decoder (left), as used in Perceiver IO for classification tasks anda standard average + project decoder (right), as used in Jaegle et al. (2021). Both modules can beseen as first aggregating latents by weighted averaging (learned, data-dependent weighting for theattention decoder; uniform weights for the average + project decoder) and then projecting to an outputchannel dimension (linear value projection + MLP for the attention decoder; simple linear projectionby the average + project decoder). Attentional decoding is more expressive than average + projectdecoding and follows the same architectural template as encoder and processor modules.
Figure 7: Visualization of attention weights for a few queries in the initial cross-attention layer. Weuse the color to convey the weight of the attention and normalize by the maximum weight to makethem easier to visualize. Best viewed in color.
Figure 8: Qualitative examples of optical flow. For each image pair, we show the two frames (top),and then the estimated flow (bottom left) and the ground-truth flow (bottom right). In the left example,we see one person under heavy occlusion where the correct flow is propagated into a region with fewdetails. Another person in the foreground has clothes with little texture and substantial blur, and yetthe algorithm can propagate the flow across the entire region. In the center example, we see verylarge motions from both the dragon and the person, yet many fine structures are preserved like thepole. On the right, we see a forest scene with a few extremely small objects with very subtle motions(circled) which our algorithm is able to detect and segment correctly.
