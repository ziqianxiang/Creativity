Figure 1: Illustration of conformal training (ConfTr): We develop differentiable prediction andcalibration steps for conformal prediction (CP), SMOOTHCAL and SMOOTHPRED. During training,this allows ConfTr to “simulate” CP on each mini-batch B by calibrating on the first half Bcal andpredicting confidence sets on the other half Bpred (c.f . a ). ConfTr can optimize arbitrary losses onthe predicted confidence sets, e.g., reducing average confidence set size (inefficiency) using a sizeloss Ω or penalizing specific classes from being included using a classification loss L (Cf. (Jb). Aftertraining using our method, any existing CP method can be used to obtain a coverage guarantee.
Figure 2: Baseline CP Results on CIFAR: Left: Inefficiency (Ineff, lower is better) for the CPmethods discussed in Sec. 2. Coverage (Cover), omitted here, is empirically close to 1 - α. THRclearly outperforms all approaches w.r.t. inefficiency. Right: Inefficiency distribution across CI-FAR10 classes (for α=0.01) is plotted, with more difficult classes yielding higher inefficiency.
Figure 3: Shaping Class-Conditional Inefficiency on CIFAR: Possible inefficiency reductions,in percentage change, per class (blue) and the impact on the overall, average inefficiency acrossclasses (green). Left: Significant inefficiency reductions are possible for all classes on CIFAR10.
Figure 4: Controlling Coverage Confusion: Controlling coverage confusion using ConfTr withLclass and an increasing penalty Ly,k > 0 on Fashion-MNIST. For classes 4 and 6 (“coat” and“shirt”), coverage confusion Σy,k and Σk,y decreases significantly (blue and green). However, con-fusion of class 4 with class 2 (“pullover”) might increase (gray). ConfTr can also reduce coverageconfusion of multiple pairs of classes (e.g., additionally considering class 2). Instead, we can alsopenalize confusion for each pair (y, k), k ∈ [K], e.g., y = 6. Here, Ly,k > 0, but Ly,k = 0, i.e., Coverconfusion is not reduced symmetrically.
Figure 5: Left: Reducing Mis-Coverage: Following Sec. 3.3, ConfTr allows to reduce mis-coverage on CIFAR. We consider K0={3} (i.e., “cat”) vs. all other classes on CIFAR10 (left)and “human-made” vs. “natural” on CIFAR100 (|K0|=35, |K1 |=65, right). On CIFAR10, bothMisCover0→1 and MisCover1→1 can be reduced significantly without large impact on inefficiency.
Figure A: Reducing Class- and Group-Conditional Inefficiency on CIFAR. Results, ComPlemen-tary to Fig. 3, showing the impact of higher size weights ω in Eq. (3) for classes 0 and 3 (“airplaneand “cat”) on CIFAR10 and coarse classes 9 and 15 (“large man-made outdoor things” and “reP-tiles”) on CIFAR100. ConfTr allows to reduce inefficiency (blue) in all cases, irresPective of whetherinefficiency is generally above or below average (green).
Figure B: Relative Class and Group-Conditional Inefficiency Improvements: Complementingthe main PaPer, we Plot the Possible (relative) inefficiency reduction by class or grouP (“odd” vs“even”) on MNIST and Fashion-MNIST. On CIFAR100, we consider the first 10 classes for brevity.
Figure C: Full Coverage ConfusionMatrix on CIFAR10: We plot thefull coverage confusion matrices Σfrom Eq. (7) on CIFAR10 for theConfTr baseline (with LCIaSS, left) andConfTr with Ly,k = 1 in Eq. (5) forclasses y,k ∈ {4, 5, 7} (right, high-lighted in red).
Figure D: Coverage Confusion Changes on Fashion-MNIST and CIFAR10: Left: coverage con-fusion change when targeting classes 4 and 6 (“coat” and “shirt”) on Fashion-MNIST and 3 and 5(“cat” and “dog”) on CIFAR10. The separate cell on the left is the ConfTr baseline which is, up toslight variations, close to Ly,k = 0. Middle and right: coverage confusion for a whole row, i.e.,Σy,k with fixed class y and all k 6= y. We show row 6 on Fashion-MNIST and 3 on CIFAR10. Inboth cases, coverage confusion can be reduced significantly.
Figure E: Coverage Confusion Reduction on MNIST and CIFAR10: Controlling coverage con-fusion for various class pairs. On MNIST, coverage confusion reduction is usually more significantand the reduction scales roughly linear with the weight Ly,k. On CIFAR10, in contrast, coverageconfusion cannot always be reduced for multiple class pairs at the same time (see light gray).
Figure F: Manipulating Inefficiency and Coverage Confusion on WineQuality: ComplementingFig. 5 (right) in the main paper, we plot the possible inefficiency reduction for class 1 (“good wine”,left) and full coverage confusion matrices for increased L0,0 > 1 and L1,0 > 0 (right, top andbottom, respectively). While we can reduce inefficiency for class 0 (“bad wine”), this is not possiblefor class 1. However, class-conditional coverage for class 0 can be improved significantly and wecan reduce coverage confusion Σ0,1.
Figure G: Class-Conditional Inefficiency and Coverage Confusion: Comparison between base-line and ConfTr regarding class-conditional inefficiency and coverage confusion Σ, c.f . Sec. 3.3. Forthe inefficiency comparison, we consider ConfTr without Lclass, while for coverage confusion, Con-fTr was trained with Lclass. As ConfTr reduces overall inefficiency quite significantly on MNIST andFashion-MNIST, class-conditional inefficiency is also lower, on average. But the distribution acrossclasses remains similar. The same holds for coverage confusion, where lower overall inefficiencyreduces confusion across the matrix, but the “pattern” remains roughly the same. On CIFAR10,ConfTr does not improve average inefficiency significantly, such that the confusion matrix remainsmostly the same.
