Figure 1: Generative learning trilemma.
Figure 2: Top: The evolution of 1D data distribution q(x0) through the diffusion process. Bottom:,The visualization of the true denoising distribution for varying step sizes conditioned on a fixed x5.
Figure 3: The training process of denoising diffusion GAN.
Figure 4: Sample quality vs sampling time trade-off.
Figure 5: CIFAR-10 qualitative samples.
Figure 6: Qualitative results on the 25-Gaussians dataset.
Figure 7: Qualitative results on CelebA-HQ 256 and LSUN Church Outdoor 256.
Figure 8: Multi-modality of denoising dis-tribution given the same noisy observation.
Figure 9: Qualitative results on stroke-basedsynthesis. Top row: stroke paintings. Bottomtwo rows: generated samples corresponding tothe stroke painting (best seen when zoomed in).
Figure 10: The discriminator loss per denoising step during training.
Figure 11: Additional qualitative samples on CIFAR-10.
Figure 12: Additional qualitative samples on CelebA-HQ.
Figure 13: Additional qualitative samples on LSUN Church Outdoor.
Figure 14: Visualization of samples from pθ (xo∣ Xt) for different t on CeIebA-HQ. For each exam-ple, the top row contains xt from diffusion process steps, where x0 is a sample from the dataset.
Figure 15: Visualization of samples from pθ (xo∣Xt) for different t on LSUN Church. For eachexample, the top row contains xt from diffusion process steps, where x0 is a sample from thedataset. The bottom rows contain 3 samples from pθ (xo∣Xt) for different t's.
Figure 16: CIFAR-10 nearest neighbors in VGG feature space. Generated samples are in theleftmost column, and training set nearest neighbors are in the remaining columns.
Figure 17: CelebA-HQ nearest neighbors in the VGG feature space. Generated samples are in theleftmost column, and training set nearest neighbors are in the remaining columns.
