Figure 1: Feature dot-products φ(s, a)>φ(s0, a0) increase during training when backing up from out-of-sample but in-distribution actions (TD-learning: left, Q-learning: right), though the average Q-value con-verges and stays relatively constant. Using only seen state-action pairs for backups (offline SARSA) or notperforming Bellman backups (i.e., supervised regression) avoids this issue, with stable and relatively lowdot products. Left: TD-learning with high feature dot products eventually destabilizes and produces incorrectQ-values, Right: DQN attains extremely large feature dot products, despite a relatively stable trend in Q-values.
Figure 2: Even when current offline RL algorithms are initialized at a high-performing checkpoint that attainssmall feature dot products, feature dot products increase with further training and the performance degrades.
Figure 3: Performance of DR3 + COG ontwo manipulation tasks using only 5% and25% of the data used by Singh et al. [38]to make these more challenging. COG +DR3 outperforms COG in training and at-tains higher average and final performance.
Figure 4: Normalized performance across 17 Atari gamesfor REM + DR3 (top), CQL + DR3 (bottom). x-axis rep-resents gradient steps; no new data is collected. While naiveREM suffers from a degradation in performance with moretraining, REM + DR3 not only remains generally stable withmore training, but also attains higher final performance. CQL+ DR3 attains higher performance than CQL. We report IQMwith 95% stratified bootstrap CIs [3].
Figure 5: Trend of effective rank, srank(Φ) offeatures Φ learned by the Q-function When trainedWith TD error (red, “Without DR3”) and With TD er-ror + DR3 (blue, “With DR3”) on three Atari gamesusing the 5% dataset. Note that DR3 alleviates rankcollapse, Without explicitly aiming to.
Figure 6: Comparing DR3 regularizers for oursimplifying choice of M and M induced by la-bel noise, With base CQL and DQN algorithms.
