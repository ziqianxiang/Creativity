Figure 1: Distribution of the performance on GLUE dev sets (Wang et al., 2019), averaged acrossfine-tuning runs for each checkpoint. The dashed line indicates the performance of the originalBert release.
Figure 2: Performance distributionon the dev sets of SQuAD v1.1 andv2.0 (Rajpurkar et al., 2016; 2018).
Figure 3: Bias correlation on Winogender for each pre-training seed. Each box represents the distri-bution of the score over five training runs of the coreference model. Dark boxes represent each baseMultiB erts checkpoint, while lighter boxes (CDA-incr) are the corresponding checkpoints after50k steps of additional pretraining with CDA. Some seeds are better than others on this task (forexample, seed 23), but the CDA-incr consistently reduces the bias correlation for most seeds.
Figure 4:	Accuracy of MNLI models on the anti-stereotypical (non-entailment) examples fromHANS (McCoy et al., 2020), grouped by pre-training seed. Each column shows the distributionof five fine-tuning runs based on the same initial checkpoint.
Figure 5:	Bias correlation on Winogender for each pre-training seed. Each box represents the dis-tribution of the score over five training runs of the coreference model over each MultiBerts basecheckpoint. This is the same data as Figure 3, but showing only the base checkpoints.
Figure 6: Bias correlation on Winogender forfive pretraining seeds, with 25 coreference runsper seed.
Figure 7: Distribution of estimated performance on MNLI across bootstrap samples, for runswith 1M or 2M steps. Individual samples of L(S, (X, Y)) and L'(S, (X, Y)) on the left, deltasL'(S, (X, Y)) - L(S, (X, Y)) shown on the right. Bootstrap experiment is run as in Table 5, whichgives ฮด = 0.007 with p < 0.001.
Figure 8: Distribution of the performance on GLUE dev sets, showing only runs with the bestselected learning rate for each task. Each plot shows 25 points (5 fine-tuning x 5 pre-training)for each of the 1M and 2M-step versions of each of the pre-training runs for which we releaseintermediate checkpoints (ยง2).
