Figure 1: The common perception in physics-informed machine learning is that increased perfor-mance is the result of complex biases. We find, however, that simpler implicit biases (such assecond-order structure) often account for almost all of the improvement over baselines.
Figure 2: Left: The degree of energy violation (|H - H |/|H ||H |) on test rollouts as a functionof rollout relative error (∣∣Z - zk∕∣∣Z∣∣kzk) across different environments and random seeds. BothHNNs and NeuralODEs are scattered around the line x = y . Conditioned on the rollout perfor-mance, whether or not the model is Hamiltonian has little impact on the energy violation. Right:Energy violation on test trajectories is plotted as a function of the time T of the rollout, with theshaded regions showing 1 standard deviation in log space taken across 5 random seeds and the testtrajectories.
Figure 3: Left: Test rollout error as a function of the regularization weighting in the loss. EVenat an optimally chosen SymPlectic regularization strength, the benefit to model generalization isnegligible. Right: Test rollout error plotted against the final value of the symplecticity error for theregularized models. For systems with more than a couple degrees of freedom, symplecticity error isnegatively correlated with the quality of predictions.
Figure 4: Left: NODE model with and without second-order structure (encoding dq/dt = v).
Figure 5: Left: Log rollout error for NODEs with second order bias and HNNs trained chain pendu-lums, where the analytic form of the Hamiltonian is simpler than the vector field. Right: Mechanic-sNNs and HNNs trained on spring pendulums, which have Hamiltonians and vector fields of similarcomplexity. HNNs outperform NODE with second order bias on systems that use non-Cartesiancoordinates. Error bars show standard error across 5 seeds.
Figure 6: Comparing the performance on damped systems. The NODE + SO matches the perfor-mance of a SymODEN with a fraction of the parameters and compute. HNNs without forcing termsencode the wrong inductive biases and thus fit the data poorly. Error bars denote standard erroracross 5 seeds.
Figure 7: HNNs perform very poorly on complex dynamics like OPenAI Gym MUjoco controlsystems. Biasing the model towards Hamiltonian dynamics makes it difficult to fit the trainingdata. Simply imposing second-order structure on a NODE is much more effective. Error bars showstandard error across 4 seeds.
Figure 8:	Switching from l2 to l1 loss can improve rollout error slightly, but doesn’t impact theordering of the models. The other elements of the experimental setup are identical to above. Errorbars show one standard deviation.
Figure 9:	On the additional systems from Finzi et al. (2020), We can observe the effect of secondorder structure, compared with NODE and HNN baselines. As before, second order structure seemsto account for much of the difference between NODE and HNN models. Error bars show onestandard deviation.
