Figure 1: Convex Constraints	Figure 2: Non-Convex ConstraintsIn Figures 1 and 2, Y’s columns, i.e. the particles Y1 ,Y2, and Y3, are each illustrated by a • at theconstraint set (K) vertices. The bubble around each each Yi illustrates the predicted probability,for a given input, that f(x) is nearest to that Yi. The × is the transformer’s prediction which is,by construction, a convex combination of the Yi weighted by the aforementioned probabilities andtherefore they lie in the K if it is convex (Figure 1) but not if K is non-convex (Figure 2).
Figure 3: Encoder : ≈ f	Figure 4: Decoder : ≈ Random Projection to KTΓ-<∙	C .[F	,	,	,1	Y	1	/»	Π^>T)	CE	1	1	C	Z 1	∙	1∖Figure 3 illustrates the encoder network E : Rn → Rm , whose role is to perform a (classical)unconstrained approximation of the target function, f . Since E is a classical feedforward networkthen its approximation of the target function can be arbitrarily close to the constraint set K but itneed not lie in it. The next step is to “map the encoder network’s output onto K with low distortion.”The role of the decoder network D is to correct any constraint violation made by encoder network by“projecting them back on to K”. However, such a projection does not exist if K is not convex sincethere must be more than one closest point in K to some y ∈ Rm (Motzkin, 1935). Nevertheless,if the “projection” were capable of mapping any y ∈ Rm to multiple points on K, ranked by theirproximity to y, then there would be no trouble. The decoder network accomplishes precisely this, asillustrated in Figure 4, where the bubbles illustrate the probability of any particle in K being closestto y, illustrated by the size of the bubbles in Figure 4. Mathematically,6 D : Rm → Pι(K) approx-imates a (non-affine) random projection, in the sense of Ohta (2009); Ambrosio & Puglisi (2020);BrUe et al. (2021); i.e.: a 1-Lipschitz map Π : Rm → Pι(K) satisfying the random projectionproperty: for all y ∈ KΠy = δy.
