Figure 1: IIW (up), loss and accuracy (down) of different activation functions (linear, tanh,ReLU, and sigmoid) NNs. There is a clear boundary between the initial fitting and the compres-sion phases identified by IIW. Meanwhile, the train loss encounters the inflection point that keepsdecreasing with slower slope. Note that the learning rate is set small (1e-4) except for sigmoid-NNfor the better display of two phases.
Figure 2: Information compression with varyingnumber of layers (1, 2, 3, and 4) for ReLU MLPs.
Figure 3: Left: Training and test accuracyw.r.t. # units; Right: Complexity measure(IIW and `2 -norm) w.r.t. # units. Blue dash lineshows the gap between train/test acc (general-ization gap). We find `2 -norm keeps increas-ing with more hidden units. Instead, IIW keepspace with the generalization gap: the larger thegap, the larger the IIW.
Figure 4: Left: IIW, train, and test accuracywhen noise ratio in labels changes. IIW riseswhen noise ratio grows. Right: IIW with vary-ing size of random-label data. Test acc keepsconstant while train acc decays. Hence, moredata causes lower IIW because of the shrinkinggap between the train and test accuracy.
Figure 5: Left: Training and test accuracyw.r.t. # batch size; Right: IIW w.r.t. # batchsize. We find IIW keeps pace with the gener-alization gap: the larger the gap, the larger theIIW. From IIW we can specify that 16 is thebest which reaches the least generalization gapwithout the need of having the model tested.
Figure 6: The tracked IIW of the VGG net dur-ing the training by four ways: vanilla, '2-normregularization, dropout, and PIB training. Wecan identify that: first, all of them still follow afitting-compressing paradigm specified by IIW;second, vanilla VGG reaches the largest IIW farabove the others; third, PIB regularizes IIW di-rectly thus yielding the smallest IIW.
