Figure 1: GreaseLM Architecture. The textual context is appended with a special interactiontoken and passed through N LM-based unimodal encoding layers. Simultaneously, a local KG ofrelevant knowledge is extracted and connected to an interaction node. In the later GREASELM lay-ers, the language representation continues to be updated through LM layers and the KG is processedusing a GNN, simulating reasoning over its knowledge. In each layer, after each modality’s repre-sentation is updated, the representations of the interaction token and node are pulled, concatenated,and passed through a modality interaction (MInt) unit to mix their representations. In subsequentlayers, the mixed information from the interaction elements mixes with their respective modalities,allowing knowledge from the KG to affect the representations of individual tokens, and context fromlanguage to affect fine-grained entity knowledge representations in the GNN.
Figure 2: Qualitative analysis of GreaseLM’s graph attention weight changes across multiplelayers of message passing compared with QA-GNN. GreaseLM demonstrates attention changepatterns that more closely resemble the expected change in focus on the “bug” entity.
