Figure 1: The multi-round generic FL pipeline(top). The dashed arrow indicates that local mod-els or statistics may be carried to the next round.
Figure 2: Local training of FED-ROD. Yel-low/blue boxes are the models for G-FL/P-FL.
Figure 3: Comparison of thetraining and test accuracy in the P-FL setup. FedAvg’s local mod-els achieve lower training accu-racy but higher test accuracy.
Figure 4: Comparison of local training strategies and model architectures. G: generic; P: personalized.
Figure 5: Upper: G-FL test accuracyalong the training rounds before/after av-eraging the local models. Lower: vari-ances of Wm - W across clients.
Figure 6: The average P-FL accuracyon future clients, with local training.
Figure 7: Comparison of the empirical risk and regularization between personalized models of DITTO and localmodels of FEDAVG. The dataset is CIFAR-10, with Dir(0.3).
Figure 8: The G-FL accuracy by the local models wm of different generic methods. There are 100/20 clientsfor FMNIST/CIFAR-10, respectively. Both datasets use Dir(0.3).
Figure 9: Variances of local model updates w.r.t. the global model. For both datasets, we use Dir(0.3).
Figure 10: Training curves of different FL algorithms. We show the G-FL accuracy along the training process,using models before (i.e., local models) and after global aggregation. The dataset is CIFAR-10 Dir(0.3).
Figure 11: P-FL accuracy of G-head (left) and G-head + P-head (right) using the local models of FED-ROD,evaluated on each client’s test data. Here we use CIFAR-10 Dir(0.3) with 20 clients.
Figure 12: P-FL accuracy of hypernetwork before and after local training.
