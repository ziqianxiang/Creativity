Figure 1: Two different aspects consistently show that MSAs flatten loss landscapes. Left: Losslandscape visualizations show that ViT has a flatter loss (NLL + `2 regularization) than ResNet.
Figure 2: The Fourier analysis shows that MSAs do not act like Convs. Left: Relative log ampli-tudes of Fourier transformed feature map show that ViT tends to reduce high-frequency signals, whileResNet amplifies them. ∆ Log amplitude is the difference between the log amplitude at normalizedfrequency 0.0π (center) and at 1.0π (boundary). See Fig. 8 for more detailed analysis. Right: Wemeasure the decrease in accuracy against frequency-based random noise. ResNet is vulnerable tohigh-frequency noise, while ViT is robust against them. We use frequency window size of 0.1π.
Figure 3: Comparison of three different repeating patterns. Left: Spatial smoothings are locatedat the end of CNN stages. Middle: The stages of ViTs consist of repetitions of canonical Transformers.
Figure 4: Hessian max eigenvalue spec-tra show that MSAs have their advan-tages and disadvantages. The dotted lineis the spectrum of ViT using 6% datasetfor training. Left: ViT has a number of neg-ative Hessian eigenvalues, while ResNetonly has a few. Right: The magnitude ofViT’s positive Hessian eigenvalues is small.
Figure 5: ViT does not overfit training datasets. “R” is ResNet and “RX” is ResNeXt. Left: Weakinductive bias disturbs NN optimization. The lower the NLLtrain, the lower the error. Right: The lackof dataset also disturbs NN optimization.
Figure 6: GAP classifier suppresses neg-ative Hessian max eigenvalues in anearly phase of training. We present Hes-sian max eigenvalue spectrum of ViT withGAP classifier instead of CLS token.
Figure 7: Locality constraint improves the performance of ViT. We analyze the ViT with con-volutional MSAs. Convolutional MSA with 8 × 8 kernel is global MSA. Left: Local MSAs learnstronger representations than global MSA. Right: Locality inductive bias suppresses the negativeHessian eigenvalues, i.e., local MSAs have convex losses.
Figure 8: MSAs (gray area) generally re-duce the high-frequency component of fea-ture map, and MLPs (white area) amplifyit. This figure provides ∆ log amplitude ofViT at high-frequency (1.0π). See also Fig. 2aand Fig. D.2 for more results.
Figure 9: MSAs (gray area) reduce the variance of feature map points, but Convs (white area)increase the variance. The blue area is subsampling layer. This result implies that MSAs ensemblefeature maps, but Convs do not.
Figure 10: Multi-stageCNNs and ViTs behavelike a series connection ofsmall individual models.
Figure 11: Detailed architecture of Alter-ResNet-50 for CIFAR-100. White, gray, and blue blocksmean Conv, MSA, and subsampling blocks. All stages (except stage 1) end with MSA blocks. Thismodel is based on pre-activation ResNet-50. Following Swin, MSAs in stages 1 to 4 have 3, 6, 12,and 24 heads, respectively.
Figure 12: AlterNet outperforms CNNs and ViTs. Left: MSAs in the late of the stages improveaccuracy. We replace Convs of ResNet with MSAs one by one according to the build-up rules. c1to c4 stands for the stages. Several MSAs in c3 harm the accuracy, but the MSA at the end ofc2 improves it. Center: AlterNet outperforms CNNs even in a small data regime. Robustness ismean accuracy on CIFAR-100-C. “RX” is ResNeXt. Right: MSAs in AlterNet suppress the largeeigenvalues; i.e., AlterNet has a flatter loss landscape than ResNet in the early phase of training.
