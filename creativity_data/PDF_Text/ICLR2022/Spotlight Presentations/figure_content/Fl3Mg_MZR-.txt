Figure 1: Representation compression, disentangling baselines & lottery ticket effects in continuouscontrol tasks. Left. IMP successively prunes task-irrelevant outer rim pixels in a MNIST digit-classification task and for an IMP-masked agent solving a visual navigation task in DRL. The chan-nel encoding the patrolling enemy is pruned up to the point where only potential enemy locationsare considered. Middle. To disentangle the contributions of mask, initialization and layer-wisepruning ratio to the winning lottery ticket (mask/weights condition), We compare three baselines:After each IMP iteration, We permute either only the remaining initial weights (mask/permuted) oralso the sparsity mask (Permuted/permuted). The third baseline is created by randomly sampling asparse mask and random re-initialization of the weights (random/re-init). Right. Avg. normalizedperformance of policies at different sparsity levels and for different baseline conditions and fourPyBullet (Ellenberger, 2018) tasks. For both behavioral cloning and PPO agents most of the ticketeffect can be attributed to tasks IMP-derived mask as compared to the weight initialization. Agentstrained with supervision can be pruned to higher sparsity levels before performance deteriorates.
Figure 2: Comparing lottery tickets in DRL and supervised behavioral cloning. Networks trainedWith explicit supervision can be pruned to higher sparsity levels before performance starts to de-grade. Results are averaged over 15 runs for both the Cart-Pole and Acrobot and 10 runs for PyBulletenvironments. We plot mean best performance and one standard deviation.
Figure 3: Disentangling baselines for lottery tickets in supervised behavioral cloning. The gap be-tween the ticket (mask/weights) and weight-permuted baseline (mask/permuted) is small, indicatinga strong contribution of the mask. Results are averaged over 15 runs for both Cart-Pole and Acrobotand 10 runs for PyBullet environments. We plot mean best performance and one standard deviation.
Figure 4: Tickets in on- and off-policy deep reinforcement learning. The disentangling baselines fortickets in on-policy (PPO)and Off-PoliCy (DQN) DRL for a set of continuous control, a visual nav-igation and a subset of ATARI environments reveal the consistent importance of the IMP-extractedmask. Results are averaged over 5 independent runs on the Gridworld and ATARI and 10 runs onthe continuous control environments. We plot mean best performance and one standard deviation.
Figure 5: IMP eliminates task-irrelevant observation dimensions for a high-dimensional visual nav-igation task (ot ∈ R6× 10×20). Left. Channel-/pixel-wise cumulative weight magnitudes. IMPsuccessively prunes redundant input pixels which are not necessary to solve the navigation task. Allof the pruned enemy channel pixels encode locations which the patrolling enemy cannot access.
Figure 6: IMP eliminates task-irrelevant observation dimensions for a selection of MinAtar environ-ments (Young & Tian, 2019). The environment depicting figures were adapted from Young & Tian(2019). Left. Freeway. IMP provides an inductive bias by differentially pruning object channelswith different velocities (e.g. car speeds). Middle. SpaceInvaders. IMP only preserves pixels of en-emy objects which encode actionable proximity information (e.g. bullets being close to the agent).
Figure 7: IMP eliminates task-irrelevant observation dimensions for a set of continuous controltasks. For all three tasks there exists a moderate sparsity level (approximately 10% non-sparseweights) at which entire input units (columns of the first linear layer) are pruned while the agentsstill train to the performance level of their dense counterparts. The extent of this observation stronglydepends on the considered environment indicating a varying extent of observation over-specification.
Figure 8: IMP eliminates task-irrelevant observation dimensions for low-dimensional control tasks.
Figure 9: Lottery tickets in supervised policy distillation (MinAtar environments, Young & Tian,2019). We find evidence for a strong contribution of the IMP-identified mask to the overall ticketeffect. The qualitative baseline comparison generalizes from MLP- to CNN-based agents. Top.
Figure 10: Tickets in off-policy deep reinforcement learning (MinAtar environments, Young & Tian,2019). We find evidence for a strong contribution of the IMP-identified mask to the overall ticketeffect. The qualitative baseline comparison generalizes from MLP- to CNN-based agents. Top.
Figure 11: Effect of network size on lottery ticket effect in supervised behavioral cloning and deepreinforcement learning. Top. The initial network size has no influence on relative performance oftickets and Permuted/permuted baselines for supervised behavioral cloning. Larger networks do notyield tickets that outperform those generated from smaller networks for a given absolute number ofremaining weights. Bottom. Initial network size comparison for tickets in on- and off-policy DRL.
Figure 12: In the main text, agents were trained on GridMaze using the DQN algorithm and onCart-Pole using PPO. Here, We report the performance OfPPO-trained agents on the GridMaze task(left) and of DQN-trained agents on the cart-pole task (right) for different network sizes.
Figure 13: Performance of agents trained on an RGB-encoded GridMaze task (left) and on a ran-domly projected, entangled representation (right). The derived mask robustly contributes most tothe ticket. More information can be found in section B.
Figure 14: Left. Lottery ticket plot with and without L2 weight decay (λ = 0.1). Using weightdecay does not impair the ticket phenomenon for a DQN agent. Middle. Lottery ticket plot withand without dropout in all layers (P = 0.1). Dropout deteriorates overall performance at all levelsof sparsity, but does not impair the ticket effect for a DQN agent. Right. Late rewinding (Frankleet al., 2019) to different stages of training (0, 100k, 400k, 4000k environment steps).
Figure 15: Tickets in on-policy deep reinforcement learning and when pruning only the actor mod-ule. The random re-sampling baseline in this case outperforms permuting both the weights and themask. The results are averaged over 10 runs. We plot mean best performance and one standarddeviation.
