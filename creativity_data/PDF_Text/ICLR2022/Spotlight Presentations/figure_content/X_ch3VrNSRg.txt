Figure 1: Left figure: Structure of EE-Net. In the right figure, Case 1: "Upward" exploration shouldbe made when the learner underestimates the reward; Case 2: "Downward" exploration shouldbe chosen when the learner overestimates the reward. EE-Net has the ability to adaptively makeexploration according to different cases. In contrast, UCB-based strategy will always make upwardexploration, and TS-based strategy will randomly choose upward or downward exploration.
Figure 2: Regret comparison on Movielens and Yelp (mean of 10 runs with standard deviation(shadow)). With the same exploitation network fi, EE-Net outperforms all baselines.
Figure 3: Regret comparison on Mnist and Disin (mean of 10 runs with standard deviation (shadow)).
Figure 4: Ablation study on label function y for f2. EE-Net denotes yι = r - fι, EE-Net-absdenotes y = |r - fι|, and EE-Net-ReLU denotes y3 = ReLU(r - fι). EE-Net shows the bestperformance on these two datasets.
Figure 5: Ablation study on decision maker f3. EE-Net-Lin denotes f3 = fι + f2, EE-Net-NoLindenote the nonlinear one where f3 is a neural network (2 layer, width 20), EE-Net denotes the hybridone where f3 = f 1 + f2 if t ≤ 500 and f3 is the neural network if t > 500. EE-Net has the moststable and best performance.
Figure 6: Two types of exploration: Upward exploration and Downward exploration. fι is theexploitation network (estimated reward) and h is the expected reward.
