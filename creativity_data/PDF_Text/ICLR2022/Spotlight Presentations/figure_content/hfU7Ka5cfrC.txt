Figure 1: Summary of our derivation (Section 2): an example of online hypergradient descent usingexact hypergradients from the implicit function theorem (IFT) (―►), with our method's approximatehypergradients (―►) superimposed. We target optimal validation loss ( ), adjusting weights W basedon the training loss. Classical weight updates (for fixed hyperparameters λ) converge (—►, )to the best-response line w*(λ) (-); the IFT gives hyperparameter updates (—►) leading to aminimum of validation loss along w*(λ). Our approximate hyperparameter updates (—►) differ inmagnitude from these exact updates, but still give useful guidance.
Figure 2: Reinterpreting the role of learning rateη over a loss function L. (a) In the classicalsetting, η scales the gradient of some fixed L.
Figure 3: Sample hyperparameter trajectoriesfrom training UCI Energy under OurWD+LR con-figuration. Background shading gives a non-HPO baseline with hyperparameters fixed at thecorresponding initial point; these results are in-terpolated by a Gaussian process. Note the tra-jectories are generally attracted to the valley ofhigh performance at learning rates around 10-1and weight decays below 10-2.
Figure 4: Left: Pseudocode for our method. Right: Median final test loss on UCI Energy after 400hyperparameter updates from random initialisations, for various update intervals T and look-backdistances i, with (a) no hyper-parameter tuning, Random, and (b) our proposed method, OurswD+LR+M.
Figure 6:	Median final test loss over 100 repetitions, after optimising UCI Energy for 400 hyperpa-rameter update steps, from a variety of update intervals T and look-back distances i.
Figure 7:	Mean absolute error in our approximate hypergradients, based on the first hyperparameterupdates of50 repetitions for each cell, using UCI Energy. We vary the update interval T and look-backdistance i to investigate the sensitivity of the problem to these meta-hyperparameters. Hypergradientsare shown separately for each hyperparameter.
Figure 8: Evolution of test MSEs during training of UCI and Kin8nm datasets for 4000 full-batchepochs from each of 200 random initialisations of SGD learning rate, weight decay and momentum.
Figure 9: Empirical CDFs of final test losses, after training UCI and Kin8nm datasets for 4000full-batch epochs from each of 200 random initialisations of SGD learning rate, weight decay andmomentum. Runs ending with NaN losses account for the CDFs not reaching 1.0. Higher curves arebetter; note the logarithmic horizontal scales.
Figure 10: Sample learning rate evolutions after training UCI and Kin8nm datasets for 4000 full-batch epochs; one of 200 random initialisations is shown for each dataset, chosen randomly from ourresults.
Figure 12: Empirical CDFs of final test losses, trained on larger-scale datasets for 50(a)/72(b, c) epochsfrom 100(a)/50(b, c) random SGD hyperparameter initialisations. Key as in Figure 13; notes as inFigure 9.
Figure 13: Empirical CDFs of final test errors after training on larger-scale datasets for 50(a)/72(b)epochs from each of 100(a)/50(b) random initialisations of SGD hyperparameters. Notes as in Figure 9.
Figure 14: Single-pass runtime distributions for our larger-scale experiments. Worst-case complexitiesare dominated by second derivative computations, but remain competitive with conventional multi-pass HPO techniques, which scale our Random results by the number of configurations sampled.
Figure 16: CDFs of final test losses and evolutions of bootstrapped median losses from 60 randominitialisations on a shorter problem of 200 (UCI and Kin8nm) / 1000 (Fashion-MNIST) networkweight updates, featuring full-horizon Long Diff-through-Opt as described in Appendix B.7.1.
Figure 18: CDF of final test losses and evolution of bootstrapped median losses from 200 randominitialisations on UCI Energy, featuring 36 initialisations of Full Diff-through-Opt with a 4 000-steplook-back horizon as described in Appendix B.7.3.
