Figure 1: SGD converges to a local maximum for the example studied in Sec. 5.1. Left: Distribution of W onthe quadratic potential, L = xw2∕2; the histograms in different colors show the distribution of model parametersat different time steps. Middle: Distribution on the fourth-order potential, L = XWF/2 + w4/4. Right: Escapeprobability as a function of a and λ. The parameters space is divided into an absorbing phase where w isattracted to the local maximum (in dark blue) and an active phase where w successfully escapes the two centralbins (in white).
Figure 2: AMSGrad diverges to the local maximum with or without momentum in the example We studied inSec 5.4. Our result shows that AMSGrad is always attracted to the local maximum, while Adam has the potentialof escaping the local maximum with momentum. Left: without momentum. Right: with momentum.
Figure 3: Convergence of a two-layer one-neuron neural network to a saddle point. The blue region showslearning ratethe empirical density of converged parameter distribution. Left: λ = 0.001 at step 10000 converges to globalminima. Mid: λ = 0.1 at step 10000 converges to a saddle point. Right: Average loss in equilibrium as afunction of learning rate. The loss function diverges for learning rates larger than 0.108.
Figure 4: Escape probability from the local maximum as a function of a and λ with fourth-orderloss landscape. The parameters space is divided into an absorbing phase where w is attracted to thelocal maximum and an active phase where w escapes to infinity successfully. The orange line is thetheoretical phase transition line for the quadratic loss function. We see that when λ is small, the linebased on quadratic loss also gives good agreement with the fourth-order loss. This suggests that partof this result is universal and independent of the details of the loss function.
Figure 5: Escape rate γ as a function of time step t obtained when a = -1, showing that γ, defined inEquation (73), is indeed a well-defined quantity when t is large. γ becomes stable after roughly 40steps.
Figure 6: Escape rate Y as a function of learning rate λ with quadratic loss landscape, obtained by simulations.
Figure 8: Stationary distribution of W with additive noise of σ = 0.1 with quadratic loss landscape.
Figure 9: Escape probability as a function of a and λ. The parameters space is divided into anabsorbing phase where w is attracted to the local maximum (in dark blue) and an active phasewhere w successfully escapes the two central bins (in white). The orange line denotes the analyticalconvergence bound on λ as a function of a obtained in the discrete-time calculation, while the reddashed line denotes the same bound obtained by the continuous-time calculation. Here the batchsize S is set to 1. The result given by the continuous-time process agrees well with the discrete-timeprocess in the small λ small a limit.
