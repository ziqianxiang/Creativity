Figure 1: An illustration of (a) embedding distribution tuned by SCGM Via dynamic mixture prob-abilities πi, when yi = 1, and (b) when yi = 2, where (c) summarizes SCGM,s graphical model.
Figure 2: The tSNE visualization of the embeddings learnedby ANCOR and SCGM-G. Star represents superclass means.
Figure 3: (a) An illustration of the model personalization framework for fine-grained event predic-tion for hemodialysis patients. (b) The distributions of patient-wise prediction accuracy on the queryset by different methods. Each dot represents the accuracy of one patient,s personalized model.
Figure 4: An illustration the architecture of SCGM-AFig. 4 illustrates the architecture of SCGM-A, which was implemented on a generic encoder (e.g.,ResNet-50) with momentum encoders. The query V and positive key k+ are computed from tworandom augmentations of the input images through the encoder and embedder, and their momentumupdates. There is a superclass-wise dictionary maintains C queues for C different superclasses so thatsuperclass-wise negative keys k- can be generated for within-superclass contrastive learning, whichfacilitates preserving intra-class variation. At the SCGM classification head, v, k+ and k- are ap-plied with an angular normalization, followed by the InfoNCE loss, where the angular normalization16Published as a conference paper at ICLR 2022is defined by (Bukchin et al., 2021)MIA =(高一言)，■ 一 ■k	(18)which enables contrasts from an angular perspective, and induces better synergy with cross-entropyloss.
Figure 5: An illustration the architecture of DCCNFig. 5 illustrates the architecture of the Dual-Channel Combiner Network (DCCN). DCCN is effec-tive to process heterogeneous medical records data that usually consists of static profiles and timeseries (i.e., records) (Che et al., 2016). It has a static channel realized by MLPS to encode staticfeatures (e.g., demographic information, infrequent blood test results, etc.) and a temporal channelrealized by RNNs (e.g., LSTM) to encode temporal feature (e.g., blood flow, venous pressure, etc.).
Figure 6: The training loss w.r.t. the number of epochs of SCGM on BREEDS datasets(b) SCGM-A0	25	50	75	100 125 150 175 200epochcosine annealing with warm restarts schedule for the learning rate (as discussed in Sec. 4.1), whichhas 20 epochs per cycle. This is consistent with Fig. 6. In practice, we observed both E-step andM-step can decrease the loss function values since both steps are theoretically optimizing the sameloss function. There is a rounding step (as discussed in Sec. 3.2) for generating discrete code fromthe posterior, which follows the E-step. We found this rounding step is beneficial, since withoutit, too small values may be generated in the posterior inference, which is unstable in computation.
Figure 7: The tSNE visualization of the embeddings learned by different methods. Star representssuperclass means. “+” marker represents subclass means.
