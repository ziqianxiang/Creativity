Figure 1: An illustrative example of orthogonal classifier in a linear case. (a) is the data distributions intwo classes; (b,c,d) are the probabilities of the data from class 1 predicted by the full/principal/orthogonalclassifiers. Red and blue colors mean a probability close to 1 or 0. The white color indicates regions witha probability close to 0.5, which are classifiers’ decision boundaries. Clearly, w1 and w2 have orthogonaldecision boundaries.
Figure 2: (a,b): Test loss versus log expected divergence and learning rate. (c): The histogram ofpredicted probability of different methods. The two red lines are the ground-truth probabilities forP(Y = 1∣Brown background) and P(Y = 1∣Green background).
Figure 3: Style transfer samples on the domain A of CMNIST and CelebA-GH. We visualize theinputs (top row) and the corresponding transferred samples of different methods.
Figure 4: CMNIST and CelebA-GHD Extra ExperimentsD.1 Style transferTable 6: CelebA	Z1 acc.	Z2 acc.	FID JVanilla	75.3	87.0	36.2wX ∖ w1-MLDG	81.8	93.5	35.2wX ∖ w1-TRM	76.3	92.5	33.5wX ∖ w1 -Fish	74.2	93.3	33.1IS-Oracle	733	88.1	36.5wX ∖ w1 -Oracle	84.0	95.2	38.5We show transferred samples for both domains in Fig. 4 on CMNIST and CelebA-GH.
