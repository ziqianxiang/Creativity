Figure 1: Overview of the proposed ViTGAN framework. Both the generator and the discriminatorare designed based on the Vision Transformer (ViT). Discriminator score is derived from the clas-sification embedding (denoted as [*] in the Figure). The generator generates pixels patch-by-patchbased on patch embeddings.
Figure 2: Generator Architecture Variants. The diagram on the left shows three generatorarchitectures we consider: (A) adding intermediate latent embedding w to every positional embedding,(B) prepending w to the sequence, and (C) replacing normalization with self-modulated layernorm(SLN) computed by learned affine transform (denoted as A in the figure) from w. On the right, weshow the details of the self-modulation operation applied in the Transformer block.
Figure 3: Qualitative Comparison. We compare our ViTGAN with StyleGAN2, and our bestTransformer baseline, i.e., a vanilla pair of ViT generator and discriminator described in Section 5.1,on the CIFAR-10 32 × 32, CelebA 64 × 64 and LSUN Bedroom 64 × 64 datasets. Results on LSUNBedroom 128 × 128 and 256 × 256 can be found in the Appendix.
Figure 4: (a-c) Gradient magnitude (L2 norm over all parameters) of ViT discriminator and (d-f)FID score (lower the better) as a function of training iteration. Our ViTGAN are compared withtwo baselines of Vanilla ViT discriminators with R1 penalty and spectral norm (SN). The remainingarchitectures are the same for all methods. Our method overcomes the spikes of gradient magnitude,and achieve lower FIDs (on CIFAR and CelebA) or a comparable FID (on LSUN).
Figure 5: Samples from ViTGAN on 128 × 128 resolution. Our ViTGAN was trained on LSUNBedroom 128 × 128 dataset. We achieved the FID of 2.48.
Figure 6: Samples from ViTGAN on 256 × 256 resolution. Our ViTGAN was trained on LSUNBedroom 256 × 256 dataset. We achieved the FID of 4.67.
