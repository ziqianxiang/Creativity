Figure 1: These plots depict the canonicalization procedure of EPIC (left) and the transformation ofDARD (right) applied to a single transition from s = 1 to s0 = 2 in a simple MDP. The contoursdepict a reward function for a 1D, continuous-state environment with a continuous action space ofdisplacements in [-1, 1]. The axes show the state S and next state S0 (omitting the action becauseT is deterministic). The reward function is zero everywhere, but shaped by the potential functionΦ(s). The plotted points correspond to different terms in Eq. (1) or Eq. (3) as indicated by thelegends. EPIC canonicalizes reward functions by computing reward values for transitions that maynot be possible under the transition model of the environment. This is reflected in the visualizationby samples comprising the different expectations being dispersed globally over S and/or S0 . Ourapproach in contrast uses a transition model to transform the reward function using approximatelyin-distribution transitions as illustrated visually by its local sampling in computing the expectations.
Figure 2: Comparison of the sensitivity of DARD and EPIC to variations in the size of the dataset.
Figure 3: Comparison of DARD distance with policy return for randomized reward functions inthe Reacher environment. Each point in these scatter plots corresponds to a different randomlygenerated reward function. We train a policy on each of these reward functions, and then evaluatethat policy against the ground-truth reward function. The resulting mean policy return is plotted onthe y-axis. On the x-axis we plot the DARD distance computed with respect to the πuni (Left) and∏* (Right) coverage distributions. See Appendix A.2.3 for an analysis of the results.
Figure 4: This plot shows histograms of the ground truth and predicted next state displacementsbetween the end-effector and goal location in the Reacher environment. These displacements arecomputed on 200 transitions for which the learned transition model yields transformed reward valuesthat most differ from those of the ground truth transition model. The displacements predicted by thelearned transition model are consistently larger than those of the ground truth model, and are oftenbeyond the threshold at which the “goal reached” reward is assigned.
