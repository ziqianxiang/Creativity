Figure 1: (A) Illustration of D-CODE (compared with two-step symbolic regression). Both algorithmsinvolve two steps, but they (1) estimate different variables in step one and (2) optimize differentobjective functions in step two. (B) An example of closed-form function and its tree representation.
Figure 2: The success probability of the two growth models under different settings. The three columnscorrespond to various noise levels σR, step sizes ∆t, and numbers of trajectories N . The two rows correspondto the Gompertz model and the generalized logistic model. The shaded area is the 95% confidence interval.
Figure 3: Illustration of performance gain using a trajectory with moderate noise σR = 0.1. Panel A: thetrajectory xb(t) estimated from the measurements y(t) is very close to the true trajectory x(t); hence, the firststep of D-CODE tends to have small error. Panels B and C: even with moderate noise on x(t), the estimatedderivatives X(t) may suffer from very high variance (B: finite difference) or systematic bias (C: SR-T, SR-S).
Figure 4: Simulation results of the chaotic Lorenz system. First row: the success probabilities for the threeequations under different noise levels. Second row: simulated trajectories using true and estimated equations.
Figure 5: An observed trajectoryand the trajectories generated bythe two discovered ODEs. Valuesare indexed at t = 0.
Figure 6: A visual illustration of the proof of Fundamental lemma of calculus of variationsThenZ	h(t)g(t) dt = Z	h(t)g(t)dt > 00	t1 -δ(10)That contradicts R0T h(t)g(t) dt = 0 for all g ∈ C 1[0, T] SUCh that g(0) = g(T) = 0.	□Proof of Proposition 1. Observe thatX j ⑴=fj (Mt)) — fj (Mt)) - X j ⑴=0(11)14Published as a conference paper at ICLR 2022Now using Fundamental lemma of calculus of variations, we get that the following two statementsare equivalentfj(x(t)) - Xj(t)=0 Vt ∈ [0,T]T (fj (x(t)) - X j (t))g(t)dt = 0 Vg ∈C1 [0,T ], g(0) = g(T ) = 00(12)By linearity and integration by parts, we get/ (fj(x(t)) - Xj(t))g(t)dt = L fj(x(t))g(t)dt - L Xj(t)g(t)dt=ZT fj(x(t))g(t)dt + IT Xj (t)g(t)dt- Xj(T)g(T) + Xj(0)g(0)
Figure 7: The metric dx (Eq. 6) for the two growth models under different settings (smaller better). The threecolumns correspond to various noise levels σR, step sizes ∆t, and numbers of trajectories N . The two rowscorrespond to the Gompertz model and the generalized logistic model respectively.
Figure 8: The metric dx (Eq. 6) for the chaotic Lorenz system under different noise levels (smaller better).
Figure 9: Simulation results for the dynamical system in Eq 38 under various noise levels. Left: the successprobability (higher better) and Right: distance dx (lower better).
Figure 10: The success probability with different families and different numbers of testing functions g (sineand cubic spline).
Figure 11: Comparison of average training time.
Figure 12: Visualization of the observations y (t), the true trajectory x(t) and the smoothed trajectory X(t).
Figure 13: The computation time (in seconds) to evaluate a candidate function f . The data are generated fromGompertz and Logistic ODEs respectively. The results are calculated from 100 independent runs. The summarystatistics (mean, median and quartiles) are shown in the boxplot. DO is 30 - 50 times slower than D-CODE.
Figure 14: The probability of successfully recovering the unknown ODEs. Two versions of NODE are comparedwith D-CODE on the Gompertz ODE (first panel) and the Lorenz system (second to fourth panels). In therightmost panel, the results of NODE-L and NODE-S overlap and only one line is displayed.
Figure 15: The distance dx between the true and the learned ODE.
