title,year,conference
 Onlearnability wih computable learners,2020, In Aryeh Kontorovich and Gergely Neu (eds
 Computational Complexity: A Modern Approach,2006, Cambridge University Press
 Residualenergy-based models for text generation,2021, JMLR
 The page-fault weird machine: Lessons ininstruction-less computation,2013, In WOOT
 A neural probabilisticlanguage model,1532, J
 On the computational power of Transformers and itsimplications in sequence modeling,2020, In Proceedings of the 24th Conference on Computational NaturalLanguage Learning
 Applying probability measures to abstract languages,1973, IEEE Transactionson Computers
 In H,1877, Larochelle
 Recurrent neural networksas weighted language recognizers,2018, In Proceedings of the 2018 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies
 The complexity of theorem-proving procedures,1971, In Proceedings of the Third AnnualAcm Symposium on Theory of Computing
 Graphical models over multiple strings,2009, In Proceedings of the 2009Conference on Empirical Methods in Natural Language Processing
 Expectation semirings: Flexible EM for finite-state transducers,2001, In Gertjan van Noord (ed
 Inapproximability of the partition function for theantiferromagnetic Ising and hard-core models,2016, Combinatorics
 Probabilistic machine learning and artificial intelligence,2015, Nature
 Java generics are Turing complete,2016, CoRR
 Training products of experts by minimizing contrastive divergence,0899, NeuralComput
 Whole sentence neural language models,2018, InICASSP
 Polynomial-time approximation algorithms for the Ising model,1993, SIAMJournal on computing
 Estimation of probabilities from sparse data for the language model component of a speechrecognizer,1987, IEEE Transactions on Acoustics
 Ergodic hidden Markov models and polygrams forlanguage modeling,1994, In Proceedings of ICASSP ’94
 Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data,2001, In ICML
 On the learnability of the uncomputable,1996, In ICML
 A tutorial on energy-based learning,2006, In G
 Theory of point estimation,2006, Springer
 Limitations of autoregressivemodels and their alternatives,2021, In Proceedings of the 2021 Conference of the North American Chapter ofthe Associationfor Computational Linguistics: Human Language Technologies
 Noise contrastive estimation and negative sampling for conditionalmodels: Consistency and statistical efficiency,1405, In EMNLP
 Recurrent neural network basedlanguage model,2010, In INTERSPEECH
 Safe and effective importance sampling,2000, Journal of the American StatisticalAssociation
 Attention is Turing-complete,2021, Journal OfMaChineLearning Research
 Language models areunsupervised multitask learners,2019, Unpublished
 Whole-sentence exponential language models: Avehicle for linguistic-statistical integration,2001, Computer Speech & Language
 Refining generative language models using discriminative learning,1006, In EMNLP
 On the computational power of neural nets,0022, Journal of Computer andSystem Sciences
 Introduction to the Theory of Computation,1133, Course Technology
 Contrastive estimation: Training log-linear models on unlabeled data,2005, InProceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05)
 A theory of the learnable,1984, Commun
 C++ templates are Turing complete,2003, Technical report
 A relatively small Turing machine whose behavior is independent of settheory,2016, ArXiv
