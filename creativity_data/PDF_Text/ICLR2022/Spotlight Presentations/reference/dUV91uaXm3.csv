title,year,conference
 Layer normalization,2016, Preprint arXiv:1607
 The Fifth PASCAL RecognizingTextual Entailment Challenge,2009, In TAC 2009 Workshop
 Language Models are Few-Shot Learners,2020, Preprint arXiv:2005
 End-to-End ObjectDetection with Transformers,2020, In European Conference on Computer Vision
 Quora Question Pairs,2018, University of Waterloo
 Spectral Graph Theory,1997, American Mathematical Soc
 BERT: Pre-training of Deep Bidirectional Transform-ers for Language Understanding,2019, In North American Chapter of the Association for ComputationalLinguistics
 Automatically Constructing a Corpus of Sentential Paraphrases,2005, InInternational Workshop on Paraphrasing
 Attention is Not All You Need: Pure Attention Loses RankDoubly Exponentially with Depth,2021, In International Conference on Machine Learning
 An Image is Worth 16x16 Words: Transformers forImage Recognition at Scale,2021, In International Conference on Learning Representations
 Improve Vision Transformers Training bySuppressing Over-smoothing,2021, Preprint arXiv:2104
 Efficient Training of BERT by ProgressivelyStacking,2019, In International Conference on Machine Learning
 Realformer: Transformer likes residual attention,2021, InFindings of Annual Meeting of the Association for Computational Linguistics
 Tackling Over-Smoothing for General GraphConvolutional Networks,2020, Preprint arXiv:2008
 Shallow-Deep Networks: Understanding and Mitigating NetworkOverthinking,2019, In International Conference on Machine Learning
 Semi-Supervised Classification with Graph Convolutional Networks,2017, InInternational Conference on Learning Representations
 Revealing the Dark Secrets of BERT,2019, InEmpirical Methods in Natural Language Processing
 ALBERT: A Lite BERT forSelf-supervised Learning of Language Representations,2020, In International Conference on LearningRepresentations
 Neural Text Generation from Structured Data with Applicationto the Biography Domain,2016, In Empirical Methods in Natural Language Processing
 Deeper Insights into Graph Convolutional Networks for Semi-SupervisedLearning,2018, In AAAI conference on artificial intelligence
 RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019, Preprint arXiv:1907
 Graph Neural Networks Exponentially Lose Expressive Power for NodeClassification,2020, In International Conference on Learning Representations
 Scaling Neural Machine Translation,2018, In MachineTranslation
 SANVis: VisualAnalytics for Understanding Self-Attention Networks,2019, In IEEE Visualization Conference
 Know What You Don’t Know: Unanswerable Questions forSQuAD,2018, In Annual Meeting of the Association for Computational Linguistics
 DropEdge: Towards Deep Graph Convolutional Networkson Node Classification,2020, In International Conference on Learning Representations
 Remarks on Some Nonparametric Estimates of a Density Function,1956, Annals ofMathematical Statistics
 SparseBERT: Rethinking the ImportanceAnalysis in Self-attention,2021, In International Conference on Machine Learning
 Recursive Deep Modelsfor Semantic Compositionality Over a Sentiment Treebank,2013, In Empirical Methods in NaturalLanguage Processing
 A Tutorial on Spectral Clustering,2007, Statistics and computing
 GLUE: A Multi-Task Benchmarkand Analysis Platform for Natural Language Understanding,2018, In EMNLP Workshop BlackboxNLP
 Multi-Granularity Hierarchical Attention Fusion Networks for ReadingComprehension and Question Answering,2018, In Annual Meeting of the Association for ComputationalLinguistics
 Neural Network Acceptability Judgments,2019, Transactions ofthe Association for Computational Linguistics
 A Broad-Coverage Challenge Corpus for SentenceUnderstanding through Inference,2018, In North American Chapter of the Association for ComputationalLinguistics
 Transformers: State-of-the-Art Natural Language Processing,2020, In EmpiricalMethods in Natural Language Processing: System Demonstrations
 Google’s Neural Machine Translation System: Bridging the Gap betweenHuman and Machine Translation,2016, Preprint arXiv:1609
 Representation Learning on Graphswith Jumping Knowledge Networks,2018, In International Conference on Machine Learning
 SWAG: A Large-Scale Adversarial Dataset forGrounded Commonsense Inference,2018, In Empirical Methods in Natural Language Processing
 PairNorm: Tackling Oversmoothing in GNNs,2020, In International Conferenceon Learning Representations
 BERT Loses Patience: Fast and RobustInference with Early Exit,2020, In Neural Information Processing Systems
 Aligning Booksand Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books,2015, InInternational Conference on Computer Vision
