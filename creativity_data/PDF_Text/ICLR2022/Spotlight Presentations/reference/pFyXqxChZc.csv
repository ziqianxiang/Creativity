title,year,conference
 On theutility of gradient compression in distributed training systems,2021, arXiv preprint arXiv:2103
 QSGD:Communication-efficient SGD via gradient quantization and encoding,2017, In Advances in NeuralInformation Processing Systems
 Variance reduction for faster non-convex optimization,2016, In The33th International Conference on Machine Learning
 On biased ComPres-sion for distributed learning,2020, arXiv:2002
 MPI for Python,2005, Journal OfParallel and DistributedComputing
 BERT: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Variance-reduced methods formachine learning,2020, Proceedings of the IEEE
 SGD: General analysis and improved rates,2019, In Kamalika Chaudhuri and RuslanSalakhutdinov (eds
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Natural compression for distributed deep learning,2019, arXiv preprint arXiv:1905
 Densely connectedconvolutional networks,2017, In Proceedings of the IEEE conference on computer vision and patternrecognition
 Accelerating stochastic gradient descent using predictive variancereduction,2013, Advances in neural information processing Systems
 Error feedbackfixes SignSGD and other gradient compression schemes,2019, In International Conference on MachineLearning
 Distributed learning with com-pressed gradients,2018, arXiv preprint arXiv:1806
 Decentralized deep learningwith arbitrary communication compression,2019, arXiv preprint arXiv:1907
 Don'tjump through hoops and remove thoseloops: SVRG and Katyusha are better without the outer loop,2020, In Algorithmic Learning Theory
 Distributed learningwith compressed gradient differences,2019, arXiv preprint arXiv:1901
 Zero-shot text-to-image generation,2021, In Proceedings of the 38th InternationalConference on Machine Learning
 Uncertainty principle for communication com-pression in distributed and federated learning and the search for an optimal compressor,2020, arXivpreprint arXiv:2002
 Scaling distributed ma-chine learning with in-network aggregation,2021, To appear in 18th USENIX Symposium on NetworkedSystems Design and Implementation
 Sparsified SGD with memory,2018, InAdvances in Neural Information Processing Systems
 Powersgd: Practical low-rank gradientcompression for distributed optimization,2019, Advances In Neural Information Processing Systems32 (Nips 2019)
 ATOMO: Communication-efficient learning via atomic sparsification,2018, Advances in Neu-ral Information Processing Systems
 Compressed communication for distributed deeplearning: Survey and quantitative evaluation,2020, Technical report
