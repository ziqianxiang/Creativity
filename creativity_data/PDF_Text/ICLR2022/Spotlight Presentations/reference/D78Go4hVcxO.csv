title,year,conference
 Lambdanetworks: Modeling long-range interactions without attention,2021, In InternationalConference on Learning Representations
 Attention augmentedconvolutional networks,2019, In International Conference on Computer Vision
 Revisiting resnets: Improved training and scaling strategies,2021, InAdvances in Neural Information Processing Systems
 Understanding robustness of transformers for image classification,2021, In InternationalConference on Computer Vision
 End-to-end object detection with transformers,2020, In European Conference on ComputerVision
 When vision transformers outperform resnetswithout pretraining or strong data augmentations,2022, In International Conference on LearningRepresentations
 Twins: Revisiting the design of spatial attention in vision transformers,2021, In Advancesin Neural Information Processing Systems
 On the relationship between self-attention and convolutional layers,2020, In International Conference on Learning Representations
 Randaugment: Practical automateddata augmentation with a reduced search space,2020, In Advances in Neural Information ProcessingSystems
 Coatnet: Marrying convolution andattention for all data sizes,2021, In Advances in Neural Information Processing Systems
 Identifying and attacking the saddle Point Problem in high-dimensional non-convexoPtimization,2014, In Advances in Neural Information Processing Systems
 An imageis worth 16x16 words: Transformers for image recognition at scale,2021, In International Conferenceon Learning Representations
 Sharpness-aware minimizationfor efficiently improving generalization,2021, In International Conference on Learning Representations
 Imagenet-trained cnns are biased towards texture; increasing shape bias improvesaccuracy and robustness,2019, In International Conference on Learning Representations
 An investigation into neural net optimizationvia hessian eigenvalue density,2019, In International Conference on Machine Learning
 Levit: a vision transformer in convnet’s clothing for faster inference,2021, InInternational Conference on Computer Vision
 Cmt:Convolutional neural networks meet vision transformers,2021, arXiv preprint arXiv:2107
 DeeP residual learning for imagerecognition,2016, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Identity maPPings in deeP residualnetworks,2016, In European Conference on Computer Vision
 Benchmarking neural network robustness to commoncorruPtions and Perturbations,2019, In International Conference on Learning Representations
 Infinite attention: NngP andntk for deeP attention networks,2020, In International Conference on Machine Learning
 DeeP networks withstochastic dePth,2016, In European Conference on Computer Vision
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, In Advances in Neural Information Processing Systems
 The break-even Point on oPtimization trajectories of deeP neuralnetworks,2020, In International Conference on Learning Representations
 CatastroPhic fisher exPlosion: EarlyPhase fisher matrix imPacts generalization,2021, In International Conference on Machine Learning
 On large-batch training for deeP learning: Generalization gaP and sharP minima,2017, InInternational Conference on Learning Representations
 Learning multiPle layers of features from tiny images,2009, 2009
 Visualizing the loss landscapeof neural nets,2018, In Advances in Neural Information Processing Systems
 On the linearity of large non-linear models: when andwhy the tangent kernel is constant,2020, In Advances in Neural Information Processing Systems
 Swin transformer: Hierarchical vision transformer using shifted windows,2021, In InternationalConference on Computer Vision
 Sgdr: Stochastic gradient descent with warm restarts,2017, In Interna-tional Conference on Learning Representations
 Decoupled weight decay regularization,2019, In International Confer-ence on Learning Representations
 Towards robust vision transformer,2021, arXiv preprint arXiv:2105
 Revisiting the calibration of modern neural networks,2021, In Advancesin Neural Information Processing Systems
 Intriguing properties of vision transformers,2021, In Advances in Neural InformationProcessing Systems
 Do wide and deep networks learn the samethings? uncovering how neural network representations vary with width and depth,2021, In InternationalConference on Learning Representations
 Vector quantized bayesian neural network inferencefor data streams,2021, In Proceedings of the AAAI Conference on Artificial Intelligence
 Vision transformers are robust learners,2022, In Proceedings of the AAAIConference on Artificial Intelligence
 ImageNetLarge Scale Visual Recognition Challenge,2015, International Journal of Computer Vision (IJCV)
 On the adversarial robustnessof visual transformers,2021, arXiv preprint arXiv:2103
 Revisiting unreasonableeffectiveness of data in deep learning era,2017, In International Conference on Computer Vision
 Rethinking theinception architecture for computer vision,2016, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Mlp-mixer: An all-mlparchitecture for vision,2021, In Advances in Neural Information Processing Systems
 Training data-efficient image transformers & distillation through attention,2021, In InternationalConference on Machine Learning
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Scaling the depth of vi-sion transformers via the fourier domain analysis,2022, In International Conference on LearningRepresentations
 Implementation of vision transformer,2021, https://github
 Non-local neural networks,2018, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Combining ensembles and data augmentation can harm yourcalibration,2021, In International Conference on Learning Representations
 Pytorch image models,2019, https://github
 Earlyconvolutions help transformers see better,2021, In Advances in Neural Information Processing Systems
 Aggregated residualtransformations for deep neural networks,2017, In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition
 Convolutionalself-attention networks,2019, In Proceedings of the 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies
 Pyhessian: Neural networksthrough the lens of the hessian,2020, In 2020 IEEE International Conference on Big Data (Big Data)
 A fourierperspective on model robustness in computer vision,2019, In Advances in Neural Information ProcessingSystems
 Rethinking token-mixing mlp for mlp-basedvision backbone,2021, In BMVC
 Metaformer is actually what you need for vision,2021, arXiv preprint arXiv:2111
 Incorporatingconvolution designs into visual transformers,2021, In International Conference on Computer Vision
 mixup: Beyond empiricalrisk minimization,2018, In International Conference on Learning Representations
 Making convolutional networks shift-invariant again,2019, In International Conference onMachine Learning
 Random erasing data augmenta-tion,2020, In Proceedings of the AAAI Conference on Artificial Intelligence
25 × 10-4 and weight decay of 5 × 10-2,2016, We alsouse cosine annealing scheduler (Loshchilov & Hutter
 The Softmax term and V in Eq,2020, (1) exactly correspond to π(xiIxj) andp(zjIxi
 In Fig,2022, 5a
 Figure C,6789,7 analyzes the loss land-scapes of large models
