title,year,conference
 ETC: encoding long and structuredinputs in transformers,2020, In EMNLP
 Program synthesiswith large language models,2021, CoRR
 Language models are few-shot learners,2020, In NeurIPS
 Evaluatinglarge language models trained on code,2021, CoRR
 Rethinking attention with performers,2021, InICLR
 Training verifiers to solve math word problems,2021, CoRR
 BERT: pre-training of deepbidirectional transformers for language understanding,2019, In ACL
 Addressingsome limitations of transformers with feedback memory,2020, arXiv preprint arXiv:2002
 Augmenting transformers withKNN-based composite memory for dialog,2021, Transactions of the Association for ComputationalLinguistics
 Improving neural language models with acontinuous cache,2017, In ICLR
 Memory-efficienttransformers via top-k attention,2021, CoRR
 Retrieval augmentedlanguage model pre-training,2020, In ICML
 Query-keynormalization for transformers,2020, In EMNLP
 Billion-scale similarity search with GPUs,2021, IEEETransactions on Big Data
 Generalizationthrough memorization: Nearest neighbor language models,2020, In ICLR
 Reformer: The efficient transformer,2020, In ICLR
 Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing,2018, In EMNLP
 Large memory layers with product keys,2019, In NeurIPS
 Pre-training via paraphrasing,2020, In NeurIPS
 Retrieval-augmented genera-tion for knowledge-intensive NLP tasks,2020, In NeurIPS
 Isarstep: a benchmark for high-levelmathematical reasoning,2021, In ICLR
 Competition-levelcode generation with alphacode,2022, DeepMind
 Mathematical reasoningvia self-supervised skip-tree training,2021, In ICLR
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguistics
 Aug-menting self-attention with persistent memory,2019, arXiv preprint arXiv:1907
 Not all memories are created equal: Learning to forget by expiring,2021, In ICML
 Efficient transformers: A survey,2020, arXivpreprint arXiv:2009
 Long range arena: A benchmark for efficienttransformers,2021, In ICLR
 Attention is all you need,2017, In NeurIPS
 Exploration of neural machinetranslation in autoformalization of mathematics in mizar,2020, In International Conference on CertifiedPrograms and Proofs
 Linformer: Self-attention withlinear complexity,2020, arXiv preprint arXiv:2006
 Cross-batch memory for embeddinglearning,2020, In CVPR
 An efficient gradient-based algorithm for on-line training ofrecurrent network trajectories,1990, Neural Computation
 Adaptive semiparametriclanguage models,2021, ACL
 Big bird:Transformers for longer sequences,2020, In NeurIPS
 H-transformer-1d: Fast one-dimensional hierarchical attention forsequences,2021, In ACL
