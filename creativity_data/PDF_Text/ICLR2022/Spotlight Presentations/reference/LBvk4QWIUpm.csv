title,year,conference
 Breaking the curse of dimensionality with convex neural networks,2017, Journal of Ma-chine Learning Research
 Universal approximation bounds for superpositions of a sigmoidal function,1993, IEEETransactions on Information Theory
 For valid generalization the size of the weights is more important than the size of thenetwork,1997, In Advances in Neural Information Processing Systems
 On the global convergence of gradient descent for over-parameterized models using optimal transport,2018, In Advances in neural information processingsystems
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Control
 Essentially no barriersin neural network energy landscape,2018, In Proceedings of the 35th International Conference onMachine Learning
 The Universality of the Radon Transform,2003, Oxford
 Integral Geometry and Radon Transforms,2011, Springer
 Multilayer feedforward networks are uni-versal approximators,1989, Neural Networks
 Approximation by combinations of relu and squaredrelu ridge functions with `1 and `0 controls,2018, IEEE Transactions on Information Theory
 Explaining landscape connectivity of low-cost solutions for multilayer nets,2019, In Advancesin Neural Information Processing Systems
 A mean field view of the landscape of two-layer neural networks,2018, Proceedings ofthe National Academy ofSciences
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2015, In ICLR (Workshop)
 A function space view of boundednorm infinite width relu nets: The multivariate case,2019, In International Conference on LearningRepresentations (ICLR 2020)
 Banach space representer theorems for neural networks andridge splines,2021, Journal ofMachine Learning Research
 Functional Analysis,1991, International series in pure and applied mathematics
 Tempered distributions and schwartz functions on definable manifolds,2020, Journal ofFunctional Analysis
 Fourier features let net-works learn high frequency functions in low dimensional domains,2020, NeurIPS
 Regularization matters: Generalization andoptimization of neural nets v,2020,s
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
