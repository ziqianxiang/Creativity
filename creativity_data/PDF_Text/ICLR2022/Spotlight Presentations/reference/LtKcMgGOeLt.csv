title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Practical Gauss-Newton optimisation fordeep learning,2017, In Doina Precup and Yee Whye Teh (eds
 Entropy-sgd: Biasing gradientdescent into wide valleys,2017, In International Conference on Learning Representations
 A simple frameworkfor contrastive learning of visual representations,2020, In Hal DaUme In and Aarti Singh (eds
 Neural architecture search on imagenet infour GPU hours: A theoretically inspired perspective,2021, In International Conference on LearningRepresentations
 Stabilizing differentiable architecture search via perturbation-based regularization,2020, In Hal Daume III and Aarti Singh (eds
 Robustand accurate object detection via adversarial learning,2021, In Proceedings of the IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition (CVPR)
 Autoaugment:Learning augmentation strategies from data,2019, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR)
 Randaugment: Practical au-tomated data augmentation with a reduced search space,2020, In 2020 IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops (CVPRW)
 Imagenet: A large-scale hier-archical image database,2009, In 2009 IEEE Conference on Computer Vision and Pattern Recognition
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 An image is worth 16x16 words: Transformers for image recogni-tion at scale,2021, In International Conference on Learning Representations
 Multiscale vision transformers,2021, arXiv preprint arXiv:2104
 Sharpness-aware minimiza-tion for efficiently improving generalization,2021, In International Conference on Learning Represen-tations
 Deep residual learning for image recog-nition,2016, In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
 Momentum contrast forunsupervised visual representation learning,2020, In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR)
 Benchmarking neural network robustness to common cor-ruptions and perturbations,2019, In International Conference on Learning Representations
 On the relation between the sharpest directions of DNN loss and the SGD step length,2019, InInternational Conference on Learning Representations
 On large-batch training for deep learning: Generalization gap and sharp minima,2017, InInternational Conference on Learning Representations
 Supervised contrastive learning,2020, InH
 Adam: A method for stochastic optimization,2015, In InternationalConference on Learning Representations
 Learning multiple layers of features from tiny images,2009, Technical report
 Visualizing the loss land-scape of neural nets,2018, In S
 Pay attention to mlps,2021, arXiv preprintarXiv:2105
 To-wards deep learning models resistant to adversarial attacks,2018, In International Conference on Learn-ing Representations
 Do you even need attention? a stack of feed-forward layers does surprisinglywell on imagenet,2021, arXiv preprint arXiv:2105
 Automated flower classification over a large numberof classes,2008, In 2008 Sixth Indian Conference on Computer Vision
 Cats and dogs,2012, In2012 IEEE Conference on Computer Vision and Pattern Recognition
 Improving language under-standing by generative pre-training,2018,2018
 Adversarial training for free!In H,2019, Wallach
 Trainability of relu networks and data-dependent ini-tialization,2020, Journal of Machine Learning for Modeling and Computing
 A bayesian perspective on generalization and stochastic gradientdescent,2018, In International Conference on Learning Representations
 Revisiting unreasonable effec-tiveness of data in deep learning era,2017, In 2017 IEEE International Conference on Computer Vision(ICCV)
 Rethinkingthe inception architecture for computer vision,2016, In 2016 IEEE Conference on Computer Vision andPattern Recognition (CVPR)
 Lecture 6,2012,5¡ªRmsProp: Divide the gradient by a running average of itsrecent magnitude
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Fast is better than free: Revisiting adversarial training,2020, InInternational Conference on Learning Representations
 Adversarial weight perturbation helps robust gen-eralization,2020, In H
 Disentangling trainability and general-ization in deep neural networks,2020, In Hal Daume III and Aarti Singh (eds
 Adversarialexamples improve image recognition,2020, In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR)
 Mean field residual networks: On the edge of chaos,2017, InI
 Large batch optimization for deeplearning: Training bert in 76 minutes,2020, In International Conference on Learning Representations
 Understanding and robustifying differentiable architecture search,2020, In International Confer-ence on Learning Representations
 mixup: Beyond em-pirical risk minimization,2018, In International Conference on Learning Representations
