title,year,conference
 Relative entropy regularized policy iteration,2018, arXivpreprint arXiv:1812
 Contrastive behavioralsimilarity embeddings for generalization in reinforcement learning,2021, ArXiv
 Deepmind lab,2016, arXiv preprintarXiv:1612
 The arcade learning environ-ment: An evaluation platform for general agents,2013, Journal of Artificial Intelligence Research
 Language models are few-shot learners,2020, In NeurIPS
 End-to-end object detection with transformers,2020, In ECCV
 Unsupervised learningof visual features by contrasting cluster assignments,2020, ArXiv
 A simple framework forcontrastive learning of visual representations,2020, In Proceedings of the 37th International Conferenceon Machine Learning
 Transformer-xl: Attentive language models beyond a fixed-length context,2019, arXiv preprintarXiv:1901
 BERT: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of the 2019 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies
 Generalization of reinforcementlearners with working and episodic memory,2019, NeurIPS
 Noise-contrastive estimation: A new estimation principlefor unnormalized statistical models,2010, In Proceedings of the Thirteenth International Conference onArtificial Intelligence and Statistics
 World models,2018, arXiv preprint arXiv:1803
 Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor,2018, In ICML
 Dimensionality reduction by learning an invariantmapping,2006, In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition(CVPR
 Dream to control: Learningbehaviors by latent imagination,2020, In ICLR
 Mastering atari with discreteworld models,2020, arXiv preprint arXiv:2010
 Rainbow: Combining improvements indeep reinforcement learning,2018, In Proceedings of the AAAI Conference on Artificial Intelligence
 Long short-term memory,1997, Neural computation
 Reinforcement learning with unsupervised auxiliary tasks,2016, arXivpreprint arXiv:1611
 Model-basedreinforcement learning for atari,2019, arXiv preprint arXiv:1903
 Recurrentexperience replay in distributed reinforcement learning,2018, In International conference on learningrepresentations
 Image augmentation is all you need: Regularizingdeep reinforcement learning from pixels,2020, arXiv preprint arXiv:2004
 Buildingmachines that learn and think like people,2017, Behavioral and brain sciences
 Return-based contrastive representation learning for reinforcement learning,2021, In ICLR 2021
 Revisiting the arcade learning environment: Evaluation protocols and open problems forgeneral agents,2018, Journal of Artificial Intelligence Research
 An empirical model oflarge-batch training,2018, arXiv preprint arXiv:1812
 Repre-sentation learning via invariant causal mechanisms,2021, In International conference on learningrepresentations
 Playing atari with deep reinforcement learning,2013, arXiv preprintarXiv:1312
 Human-level controlthrough deep reinforcement learning,2015, nature
 Representation learning with contrastive predictivecoding,2018, arXiv preprint arXiv:1807
 Stabilizing transformersfor reinforcement learning,7487, In International Conference on Machine Learning
 Incremental multi-step q-learning,1994, In Machine Learning Proceed-ings 1994
 Data-efficient reinforcement learning with momentum predictive representations,2020, ArXiv
 The uncannysimilarity of recurrence and depth,2021, arXiv preprint arXiv:2102
 V-mpo: On-policy maximum aposteriori policy optimization for discrete and continuous control,2019, arXiv preprint arXiv:1909
 Curl: Contrastive unsupervised representations for reinforce-ment learning,2020, In ICML
 Curl: Contrastive unsupervised representationsfor reinforcement learning,2020, arXiv preprint arXiv:2004
 Deepmind control suite,2018, arXiv preprintarXiv:1801
 Learning valuesacross many orders of magnitude,2016, arXiv preprint arXiv:1602
 Attention is all you need,2017, In NeurIPS
 The bottom-up evolution of representations in thetransformer: A study with machine translation and language modeling objectives,2019, arXiv preprintarXiv:1909
 Xlnet: Generalized autoregressive pretraining for language understanding,2019, arXiv preprintarXiv:1906
 Masked contrastiverepresentation learning for reinforcement learning,2020, ArXiv
