title,year,conference
 Character-levellanguage modeling with deeper self-attention,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Adaptive input representations for neural language modeling,2018, InInternational Conference on Learning Representations
 A maximum likelihood approach to continuousspeech recognition,1983, IEEE transactions on pattern analysis and machine intelligence
 Segatron:Segment-aware transformer for language modeling and understanding,2021, 2021
 Graph con-volutional encoders for syntax-aware neural machine translation,2017, In Proceedings of the 2017Conference on Empirical Methods in Natural Language Processing
 An empirical study of smoothing techniques for languagemodeling,1999, Computer Speech & Language
 Electra: Pre-trainingtext encoders as discriminators rather than generators,2020, In International Conference on LearningRepresentations
 Question answering by reasoning across documentswith graph convolutional networks,2018, arXiv preprint arXiv:1808
 Bert: Pre-training of deepbidirectional transformers for language understanding,2018, arXiv preprint arXiv:1810
 Augmenting transformers withknn-based composite memory for dialogue,2020, arXiv preprint arXiv:2004
 Optimized product quantization,2013, IEEE transactionson pattern analysis and machine intelligence
 Realm: Retrieval-augmented language model pre-training,2020, arXiv preprint arXiv:2002
 Inductive representation learning on large graphs,2017, InAdvances in neural information processing Systems
 Heterogeneous graph transformer,2020, InProceedings of The Web Conference 2020
 Boosting neural machine translation with similartranslations,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Billion-scale similarity search with gpus,2019, IEEETransactions on Big Data
 Spanbert:Improving pre-training by representing and predicting spans,2020, Transactions of the Association forComputational Linguistics
 Exploring thelimits of language modeling,2016, arXiv preprint arXiv:1602
 Dense passage retrieval for open-domain question answering,2020, arXivpreprint arXiv:2004
 Generalizationthrough memorization: Nearest neighbor language models,2019, arXiv preprint arXiv:1911
 Nearest neighbormachine translation,2020, arXiv preprint arXiv:2010
 Dynamic evaluation of trans-former language models,2019, arXiv preprint arXiv:1904
 Learning dense representations ofphrases at scale,2020, arXiv preprint arXiv:2012
 Pre-training via paraphrasing,2020, arXiv preprint arXiv:2006
 Retrieval-augmented genera-tion for knowledge-intensive nlp tasks,2020, arXiv preprint arXiv:2005
 Graph enhanced dualattention network for document-level relation extraction,2020, In Proceedings of the 28th InternationalConference on Computational Linguistics
 Sac: Acceleratingand structuring self-attention via sparse adaptive connection,2020, arXiv preprint arXiv:2003
 Bertgcn:Transductive text classification by combining gcn and bert,2021, arXiv preprint arXiv:2105
 Roberta: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Fastnearest neighbor machine translation,2021, arXiv preprint arXiv:2105
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Regularizing and optimizing lstmlanguage models,2017, arXiv preprint arXiv:1708
 Statistical language models based on neural networks,2012, Presentation at Google
 Estimation of probabilities in the language model of the ibm speech recognitionsystem,1984, IEEE Transactions on Acoustics
 Improving transformer models by reordering theirsublayers,2020, In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics
 Shortformer: Better language modeling using shorterinputs,2020, arXiv preprint arXiv:2012
 Fast parametric learning with activationmemorization,2018, In International Conference on Machine Learning
 Efficient content-based sparseattention with routing transformers,2021, Transactions of the Association for Computational Linguistics
 Thegraph neural network model,2008, IEEE Transactions on Neural Networks
 Bidirectional recurrent neural networks,1997, IEEE transactions onSignal Processing
 A mathematical theory of communication,2001, ACM SIGMOBILE mobilecomputing and communications review
 Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,2016, 2016
 Mesh-tensorflow: Deeplearning for supercomputers,2018, In NeurIPS
 Exploringgraph-structured passage representation for multi-hop reading comprehension with graph neuralnetworks,2018, arXiv preprint arXiv:1809
 Adaptive attention spanin transformers,2019, In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics
 Efficient retrieval augmentedgeneration from unstructured knoWledge for task-oriented dialog,2021, arXiv preprint arXiv:2102
 Learning to remember translation historyWith a continuous cache,2018, Transactions of the Association for Computational Linguistics
 Attention is all you need,2017, In Advances in neural informationprocessing systems
 Pay less attention withlightweight and dynamic convolutions,2018, In International Conference on Learning Representations
 Session-basedrecommendation with graph neural networks,2019, In Proceedings of the AAAI Conference on ArtificialIntelligence
 Datanoising as smoothing in neural network language models,2017, arXiv preprint arXiv:1703
 Approximate nearest neighbor negative contrastive learning for dense textretrieval,2020, arXiv preprint arXiv:2007
 Graph convolutional networks for text classification,2019, InProceedings of the AAAI conference on artificial intelligence
 Bp-transformer: Modellinglong-range context via binary partitioning,2019, arXiv preprint arXiv:1911
 Guidingneural machine translation with retrieved translation pieces,2018, arXiv preprint arXiv:1804
