title,year,conference
 Surrogate regret bounds for bipartite ranking via strongly proper losses,2014, TheJournal of Machine Learning Research 
 Large scale distributed neural network training through online distillation,2018, arXiv preprintarXiv:1804
 Domainadaptation of dnn acoustic models using knowledge distillation,2017, In 2017 IEEE InternationalConference on Acoustics
 On warm-starting neural network training,2019, arXiv preprintarXiv:1910
 Locally adaptive label smoothing for predictive churn,2021, arXiv preprintarXiv:2102
 A comparativeanalysis of offline and online evaluations and discussion of research paper recommender systemevaluation,2013, In Proceedings of the international workshop on reproducibility and replication inrecommender systems evaluation
 On the reproducibility of neural network predictions,2021, arXiv preprintarXiv:2102
 Two-player games for efficient non-convexconstrained optimization,2019, In Algorithmic Learning Theory
 Data-driven metric development for online controlled experiments:Seven lessons learned,2016, In Proceedings of the 22nd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining
 Improving the sensitivity of online controlledexperiments by utilizing pre-experiment data,2013, In Proceedings of the sixth ACM internationalconference on Web search and data mining
 Distillation Â« early stopping? harvesting darkknowledge utilizing anisotropic information retrieval for overparameterized neural network,2019, arXivpreprint arXiv:1910
 Launch and iterate: Reducingprediction churn,2016, In Advances in Neural Information Processing Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Proceedings of the thirteenth international conference on artificial intelligence andstatistics
 Satisfying real-world goalswith dataset constraints,2016, In Advances in Neural Information Processing Systems
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Harnessing deep neuralnetworks with logic rules,2016, arXiv preprint arXiv:1603
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Learning fromnoisy labels with distillation,2017, In Proceedings of the IEEE International Conference on ComputerVision
 Unifying distillation andprivileged information,2015, arXiv preprint arXiv:1511
 Online batch selection for faster training of neural networks,2015, arXivpreprint arXiv:1511
 Self-distillation amplifies regularizationin hilbert space,2020, arXiv preprint arXiv:2002
 Distillation as adefense to adversarial perturbations against deep neural networks,2016, In 2016 IEEE symposium onsecurity and privacy (SP)
 Towards understanding knowledge distillation,2019, In InternationalConference on Machine Learning
 Model compression via distillation and quantiza-tion,2018, arXiv preprint arXiv:1802
 Rethinkingthe inception architecture for computer vision,2016, In Proceedings of the IEEE conference on computervision and pattern recognition
 Ad recommendationsystems for life-time value optimization,2015, In Proceedings of the 24th International Conference onWorld Wide Web
 Estimating numerical error in neural network simulations ongraphics processing units,2015, BMC Neuroscience
 Learning using privileged information: similarity control andknowledge transfer,2015, J
 Composite multiclass losses,2016, Journal ofMachine Learning Research
 Visual relationship detection with internaland external linguistic knowledge distillation,2017, In Proceedings of the IEEE international conferenceon computer vision
 mixup: Beyond empiricalrisk minimization,2017, arXiv preprint arXiv:1710
