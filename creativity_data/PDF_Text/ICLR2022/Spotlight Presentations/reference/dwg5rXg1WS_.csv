title,year,conference
 Image generators with conditionally-independent pixel synthesis,2021, In CVPR
 Towardtransformer-based object detection,2020, arXiv preprint arXiv:2012
 Large scale gan training for high fidelity naturalimage synthesis,2019, In ICLR
 Language models are few-shot learners,2020, In NeurIPS
 Pre-trained image processing transformer,2020, In ICML
 Generative pretraining from pixels,2020, In ICML
 On self modulation for generativeadversarial networks,2019, In ICLR
 An empirical study of training self-supervised visiontransformers,2021, CoRR
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In ACL
 Attention is not all you need: Pureattention loses rank doubly exponentially with depth,2021, arXiv preprint arXiv:2103
 Taming transformers for high-resolution imagesynthesis,2021, In CVPR
 Many paths to equilibrium: Gans do not need to decrease a divergence at everystep,2018, In ICLR
 Generative adversarial nets,2014, In NeurIPS
 Improvedtraining of wasserstein gans,2017, In NeurIPS
 Ganstrained by a two time-scale update rule converge to a local nash equilibrium,2017, In NeurIPS
 Generative adversarial transformers,2021, arXivpreprint:2103
 Transgan: Two transformers can make one stronggan,2021, arXiv preprint arXiv:2102
 A style-based generator architecture for generativeadversarial networks,2019, In CVPR
 Traininggenerative adversarial networks with limited data,2020, In NeurIPS
 Analyzingand improving the image quality of StyleGAN,2020, In CVPR
 The lipschitz constant of self-attention,2021, InICML
 Fuseformer: Fusing fine-grained information in transformers for videoinpainting,2021, In ICCV
 Deep learning face attributes in the wild,2015, InICCV
 Spectral normalization forgenerative adversarial networks,2018, In ICLR
 Integral probability metrics and their generating classes of functions,1997, Advances inApplied Probability
 f-gan: Training generative neural samplersusing variational divergence minimization,2016, In NeurIPS
 Deepsdf:Learning continuous signed distance functions for shape representation,2019, In CVPR
 Unsupervised representation learning with deepconvolutional generative adversarial networks,2016, In ICLR
 Zero-shot text-to-image generation,2021, arXiv preprint arXiv:2102
 A u-net based discriminator for generativeadversarial networks,2020, In CVPR
 Implicit neural representations with periodic activation functions,2020, In NeurIPS
 Bridging the gap between f -gans and wasserstein gans,2020, In ICML
 Segmenter: Transformer forsemantic segmentation,2021, arXiv preprint arXiv:2105
 Fourier features let networks learnhigh frequency functions in low dimensional domains,2020, In NeurIPS
 Mlp-mixer: An all-mlp architecture for vision,2021, arXiv preprint arXiv:2105
 Training data-effiCient image transformers & distillation through attention,2020, arXiv preprintarXiv:2012
 Regularizing generativeadversarial networks under limited data,2021, In CVPR
 Attention is all you need,2017, In NeurIPS
 Cvt:Introducing convolutions to vision transformers,2021, arXiv preprint arXiv:2103
 Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop,2015, arXiv preprint arXiv:1506
 Self-attention generativeadversarial networks,2019, In ICML
 Consistency regularization forgenerative adversarial networks,2020, In ICLR
 Differentiable augmentation fordata-efficient gan training,2020, In NeurIPS
 Image augmentations forGAN training,2020, arXiv preprint:2006
 Lipschitz generative adversarial nets,2019, In ICML
