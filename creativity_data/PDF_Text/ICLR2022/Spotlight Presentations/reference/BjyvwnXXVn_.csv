title,year,conference
 Conditional computationin neural networks for faster models,2015, arXiv preprint arXiv:1511
 End-to-end object detection with transformers,2020, In European Conferenceon Computer Vision
 Emerging properties in self-supervised vision transformers,2021, arXiv preprintarXiv:2104
 Net2net: Accelerating learning via knoWledgetransfer,2015, arXiv preprint arXiv:1511
 Compressingneural netWorks With the hashing trick,2015, In International Conference on Machine Learning
 Visformer: Thevision-friendly transformer,2021, arXiv preprint arXiv:2104
 Rethinking attentionWith performers,2020, arXiv preprint arXiv:2009
 TWins: Revisiting the design of spatial attention in vision transformers,2021, Arxivpreprint 2104
 Imagenet: A large-scale hier-archical image database,2009, In IEEE/CVF Conference on Computer Vision and Pattern Recognition
 An im-age is worth 16x16 words: Transformers for image recognition at scale,2021, In International Confer-ence on Learning Representations
 Multiscale vision transformers,2021, arXiv:2104
 RevitalizingCNN attention via transformers in self-suPervised visual rePresentation learning,2021, In Advances inNeural Information Processing Systems
 Video action transformer net-work,2019, In IEEE/CVF Conference on Computer Vision and Pattern Recognition
 Levit: a vision transformer in convnet¡¯s clothing for faster inference,2021, arXivpreprint arXiv:2104
 Adaptive computation time for recurrent neural networks,2016, arXiv preprintarXiv:1603
 Transformer intransformer,2021, arXiv preprint arXiv:2103
 Learning both weights and connections forefficient neural networks,2015, arXiv preprint arXiv:1506
 Dynamic neuralnetworks: A survey,2021, arXiv preprint arXiv:2102
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 All tokens matter: Token labeling for training better vision transformers,2021, arXiv preprintarXiv:2104
 Reformer: The efficient transformer,2020, arXivpreprint arXiv:2001
 Swin transformer: Hierarchical vision transformer Using shifted windows,2021, arXiv preprintarXiv:2103
 Vilbert: Pretraining task-agnostic visiolin-gUistic representations for vision-and-langUage tasks,2019, arXiv preprint arXiv:1908
 Face model compressionby distilling knowledge from neUrons,2016, In AAAI Conference on Artificial Intelligence
 IntrigUing properties of vision transformers,2021, Advances inNeural Information Processing Systems
 Dynamicvit:Efficient vision transformers with dynamic token sparsification,2021, arXiv preprint arXiv:2106
 Fitnets: Hints for thin deep nets,2014, arXiv preprint arXiv:1412
 To-kenlearner: Adaptive space-time tokenization for videos,2021, In Advances in Neural InformationProcessing Systems
 Data-free parameter pruning for deep neural networks,2015, arXivpreprint arXiv:1507
 Revisiting unreasonable effec-tiveness of data in deep learning era,2017, In IEEE/CVF International Conference on Computer Vision
 Videobert: A jointmodel for video and language representation learning,2019, In IEEE/CVF International Conference onComputer Vision
 Efficientnet: Rethinking model scaling for convolutional neural net-works,2019, In International Conference on Machine Learning
 Training data-effiCient image transformers & distillation through attention,2021, InInternational Conference on Machine Learning
 Goingdeeper with image transformers,2021, arXiv preprint arXiv:2103
 Attention is all you need,2017, In Advances in Neural Infor-mation Processing Systems
 Linformer: Self-attentionwith linear complexity,2020, arXiv preprint arXiv:2006
 Pyramid vision transformer: A versatile backbone for dense prediction withoutconvolutions,2021, In IEEE/CVF International Conference on Computer Vision
 Skipnet: Learning dy-namic routing in convolutional networks,2018, In European Conference on Computer Vision
 Not all images are worth16x16 words: Dynamic vision transformers with adaptive sequence length,2021, arXiv preprintarXiv:2105
 End-to-end video instance segmentation with transformers,2021, In IEEE/CVF Con-ference on Computer Vision and Pattern Recognition
 Dynamixer: A visionmlp architecture with dynamic mixing,2022, arXiv preprint arXiv:2201
 Seg-former: Simple and efficient design for semantic segmentation with transformers,2021, In Advances inNeural Information Processing Systems
 Co-scale conv-attentional image transform-ers,2021, arXiv preprint arXiv:2104
 Tokens-to-token vit: Training vision transformers from scratch onimagenet,2021, arXiv preprint arXiv:2101
 Deformable detr:Deformable transformers for end-to-end object detection,2020, arXiv preprint arXiv:2010
