title,year,conference
 Linear extension operators between spaces of Lipschitz mapsand optimal transport,2020, Journalfur die Reine UndAngewandte Mathematik
 Low-rank plus sparse decompo-sition of covariance matrices using neural network parametrization,2021, IEEE Transaction on NeuralNetworks and Learning Systems
 Neural machine translation by jointlylearning to align and translate,2015, In Proceedings of the International Conference on Learning Rep-resentations (ICLR)
 On approximating arbitrary metrices by tree metrics,1999, In Proceedings of the ThirtiethAnnual ACM Symposium on the Theory of Computing
 Convex analysis and monotone operator theory inHilbert spaces,2011, CMS Books in MathematiCS/Ouvrages de MathematiqUeS de la SMC
 Large sample theory of intrinsic and extrinsic samplemeans on manifolds,2003, The Annals of Statistics
 Riemannian metric and geometric mean for positivesemidefinite matrices of fixed rank,2009, SIAM Journal on Matrix Analysis and Applications
 Rank-preserving geometric means ofpositive semi-definite matrices,2013, Linear Algebra and its Applications
 Geomet-ric deep learning: going beyond Euclidean data,2017, IEEE Signal Processing Magazine
 Distances de Levy et extensions desthe´oreme`s de la limite centrale et de Glivenko-Cantelli,1993, Publ
 Methods of geometric analysis in extension and trace prob-lems,2012, Volume 1
 Methods of geometric analysis in extension and trace prob-lems,2012, Volume 2
 Linear Lipschitz and C1 extension operatorsthrough random projection,2021, Journal of Functional Analysis
 Probably approximately correct constrained learning,2020, InProceedings of Advances in Neural Information Processing Systems (NeurIPS)
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, In Proceedings ofAdvances in Neural Information Processing Systems (NeurIPS)
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of Con-trol
 Sigmoid-weighted linear units for neural networkfunction approximation in reinforcement learning,2018, Neural Networks
 A sharp form of Whitney’s extension theorem,2005, Annals of Mathematics
 Les elements aleatoires de nature quelconque dans un espace distancie,1948, AnnaleSde l
 Risk bounds for reservoir comput-ing,2020, Journal of Machine Learning Research
 Approximation bounds for randomneural networks and reservoir systems,2020, arXiv preprint arXiv:2002
 Delving deep into rectifiers: Surpassinghuman-level performance on ImageNet classification,2015, In Proceedings of the IEEE internationalconference on comPuter vision
 Lectures on analysis on metric sPaces,2001, Universitext
 Playing Stackelberg games forminimal cost for production and utilities,2018, In ECOS 2018-Proceedings of the 31st InternationalConference on Efficiency
 Multilayer feedforward networks are uni-versal approximators,1989, Neural Network
 Riemannian geometry and geometric analysis,2017, Universitext
 Uber die Cremonasche Transformation der Ebene,0075, J
 Riemannian center of mass and mollifier smoothing,1977, Communications on Pureand APPlied Mathematics
 Universal approximation with deep narrow networks,2020, In JacobAbernethy and Shivani Agarwal (eds
 Probability theory: A comPrehensive course,2014, Universitext
 Generalizedsliced Wasserstein distances,2019, In Proceedings of Advances in Neural Information Processing Sys-tems (NeurIPS)
 Universal regular conditional distributions,2021, arXiv PrePrint:2105
 Non-Euclidean universal approximation,2020, In Proceed-ings of Advances in Neural Information Processing Systems (NeurIPS)
 Extending Lipschitz functions via random metric partitions,2005, Inven-tiones Mathematicae
 Complexity lower bounds for nonconvex-strongly-concave min-max optimization,2021, arXiv:2104
 Accelerated first-order methods for geodesically convex optimization on riemannian manifolds,2017, In Proceedings ofAdvances in Neural Information Processing Systems (NeurIPS)
 Reservoir computing approaches to recurrent neural net-work training,1574, Computer Science Review
 Geomstats: A python package for riemannian geometry inmachine learning,2020, Journal of Machine Learning Research
 Sur quelques Proprietes CaraCteristiques des ensembles bornes nonconvexes,1935, Bardi
 Extending Lipschitz and Holder maps between metric spaces,2009, Positivity
 Fast and robust Earth Mover’s distances,2009, In Proceedings of the 12thIEEE International Conference on Computer Vision (ICCV)
 Equivalence of approximation by convolutional neuralnetworks and fully-connected networks,2020, Proceedings of the American Mathematical Society
 Approximation theory of the MLP model in neural networks,1999, Acta Numerica
 Most convex functions have unique minimizers,2016, Journal ofConvex Analysis
 On the relation of max-flow to min-cut for generalized networks,1989, EuropeanJournal of Operational Research
 Glob-ally injective ReLU networks,2020, arXiv:2006
 Searching for activation functions,2018, In Proceedingsof the International Conference of Learning Representations (ICLR)
 Understanding Machine Learning: From Theory toAlgorithms,2014, Cambridge University Press
 Neural network approximation: Three hiddenlayers are enough,2021, Neural Networks
 Optimal transport: Fast Proba-bilistic approximation with exact solvers,2019, Journal of Machine Learning Research
 Probability measures on metric spaces of nonpositive curvature,2003, In Heatkernels and analysis on manifolds
 Attention is all you need,2017, In Proceedings of Advancesin Neural Information Processing Systems
 Implications of dynamical symmetry breaking,1976, Physical Review D
 Analytic extensions of differentiable functions defined in closed sets,1934, Transactionsof the American Mathematical Society
 Elementary superexpressive activations,2021, In Proceedings of the 38th InternationalConference on Machine Learning (ICML)
 The phase diagram of approximation rates for deep neuralnetworks,2020, In Proceedings of Advances in Neural Information Processing Systems (NeurIPS)
 First-order methods for geodesically convex optimization,2016, In Pro-ceedings of the 29th Conference on Learning Theory (COLT)
 Universality of deep convolutional neural networks,2020, Applied and ComputationalHarmonic Analysis
