title,year,conference
 A convergence theory for deep learning via over-parameterization,2019, In International Conference on Machine Learning
 Implicit regularization in deep matrixfactorization,2019, arXiv preprint arXiv:1905
 Fine-grained analysis ofoptimization and generalization for overparameterized two-layer neural networks,2019, In InternationalConference on Machine Learning
 On the implicit bias of initialization shape: Beyond infinitesimalmirror descent,2021, arXiv preprint arXiv:2102
 Convergence of probability measures,2013, John Wiley & Sons
 Stochastic gradient and langevinprocesses,2020, In International Conference on Machine Learning
 The critical locus of overparameterized neural networks,2020, arXiv preprint arXiv:2005
 The loss landscape of overparameterized neural networks,2018, arXiv preprintarXiv:1804
 Sgd learns the conjugate kernel class of the network,2017, arXiv preprint arXiv:1702
 Riemannian geometry,2013, Springer Science & Business Media
 Gradient descent finds globalminima of deep neural networks,1675, In International Conference on Machine Learning
 Gradient descent provably optimizesover-parameterized neural networks,2018, arXiv preprint arXiv:1810
 Differentiation of the limit mapping in a dynamical system,0024, Journal of the LondonMathematical Society
 Understanding implicit regularization in over-parameterized nonlinear statistical model,2020, arXiv preprint arXiv:2007
 Convergence rates for the stochastic gradientdescent method for non-convex objective functions,2020, Journal of Machine Learning Research
 Characterizing implicit bias interms of optimization geometry,1832, In International Conference on Machine Learning
 Shape matters: Understanding the implicitbias of the noise covariance,2020, arXiv preprint arXiv:2006
 Differentiating the pseudo determinant,2018, Linear Algebra and its Applications
 Stochastic analysis on manifolds,2002, Number 38
 Neural tangent kernel: Convergence andgeneralization in neural networks,2018, arXiv preprint arXiv:1806
 Three factors influencing minima in sgd,2017, arXiv preprint arXiv:1711
 On the probability that a random± 1-matrix issingular,1995, Journal of the American Mathematical Society
 Solutions of a stochastic differential equation forced onto a manifold by alarge drift,1991, TheAnnalsofProbability
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 One weird trick for parallelizing convolutional neural networks,2014, arXiv preprintarXiv:1404
 Explaining landscape connectivity of low-cost solutions for multilayer nets,2019, arXivpreprint arXiv:1906
 Efficient backprop,2012, InNeural networks: Tricks of the trade
 Gradient descent onlyconverges to minimizers,2016, In Conference on learning theory
 First-order methods almost always avoid saddle points,2017, arXiv preprint arXiv:1710
 On generalization error bounds of noisy gradient methodsfor non-convex learning,2019, arXiv preprint arXiv:1902
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, In International Conference on Machine Learning
 Learning overparameterized neural networks via stochastic gradientdescent on structured data,2018, arXiv preprint arXiv:1808
 Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations,2018, In Conference On Learning Theory
 Towards explaining the regularization effect of initial largelearning rate in training neural networks,2019, arXiv preprint arXiv:1907
 Towards resolving the implicit bias of gradient descentfor matrix factorization: Greedy low-rank learning,2020, In International Conference on LearningRepresentations
 Reconciling modern deep learning with traditionaloptimization analyses: The intrinsic learning rate,2020, Advances in Neural Information ProcessingSystems
 A topological property of real analytic subsets,1963, Coll
 On connected sublevel sets in deep learning,2019, In International Conference on MachineLearning
 On the loss landscape of a classof deep neural networks with no bad local valleys,2018, arXiv preprint arXiv:1809
 Differential equations and dynamical systems,2001, 2001
 Implicit bias of sgd for diagonal linearnetworks: a provable benefit of stochasticity,2021, arXiv preprint arXiv:2106
 Convergence of stochastic processes,2012, Springer Science & Business Media
 Gradient methods for solving equations and inequalities,1964, USSR ComputationalMathematics and Mathematical Physics
 Minimax-optimal rates for sparse additivemodels over kernel classes via convex programming,2012, Journal of machine learning research
 On learning rates and schr\” odinger operators,2020, arXivpreprint arXiv:2004
 The implicitbias of gradient descent on separable data,2018, The Journal of Machine Learning Research
 Implicit regularization for optimal sparserecovery,2019, Advances in Neural Information Processing Systems
 Spurious valleys in two-layer neural networkoptimization landscapes,2018, arXiv preprint arXiv:1802
 Stochastic-process limits: an introduction to stochastic-process limits and their applica-tion to queues,2002, Springer Science & Business Media
 Stochastic gradient descent with noise of machine learning type,2021, part ii:Continuous time analysis
 Kernel and rich regimes in overparametrized models,2020, InConference on Learning Theory
 On thenoisy gradient descent that generalizes as sgd,2020, In International Conference on Machine Learning
 A diffusion theory for deep learning dynamics:Stochastic gradient descent escapes from sharp minima exponentially fast,2020, arXiv preprintarXiv:2002
 Implicit regularization via hadamard product over-parametrization in high-dimensional linear regression,2019, arXiv preprint arXiv:1903
 The anisotropic noise in stochasticgradient descent: Its behavior of escaping from sharp minima and regularization effects,2018, arXivpreprint arXiv:1803
 Gradient descent optimizes over-parameterized deep relu networks,2020, Machine Learning
