title,year,conference
 Layer normalization,2016, arXiv preprintarXiv:1607
 Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models,2021, arXiv e-prints
 Identifying and reducing gender bias in word-level languagemodels,2019, In Proceedings of the 2019 NAACL: Student Research Workshop
 Language models arefew-shot learners,2020, arXiv preprint arXiv:2005
 Extracting training datafrom large language models,2020, arXiv preprint arXiv:2012
 Bert: Pre-training of deepbidirectional transformers for language understanding,2019, In Proceedings of NAACL
 Switch transformers: Scaling to trillion parametermodels with simple and efficient sparsity,2021, arXiv preprint arXiv:2101
 Transformer feed-forward layers arekey-value memories,2021, In Proceedings of EMNLP
 Parameter-efficient transfer learning with diff prun-ing,2021, In Proceedings of ACL
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In Proceedings of ICCV
 LoRA: Low-rank adaptation of large language models,2021, arXiv preprint arXiv:2106
 Adam: A method for stochastic optimization,2015, In Proceedings ofICLR
 The power of scale for parameter-efficient prompttuning,2021, In Proceedings of EMNLP
 Prefix-tuning: Optimizing continuous prompts for generation,2021, InProceedings of ACL
 ROUGE: A package for automatic evaluation of summaries,2004, In Text SummarizationBranches Out
 RoBERTa: A robustly optimized bert pretrainingapproach,2019, arXiv preprint arXiv:1907
 Multilingual denoising pre-training for neural machine translation,2020, Trans-actions of the Association for Computational Linguistics
 Multilingual denoising pre-training for neural machine transla-tion,2020, Transactions of the Association for Computational Linguistics
 Compacter: Efficient low-rankhypercomplex adapter layers,2021, In Proceedings of NeurIPS
 The E2E dataset: New challenges forend-to-end generation,2017, In Proceedings of the 18th Annual SIGdial Meeting on Discourse andDialogue
 Bleu: a method for automaticevaluation of machine translation,2002, In Proceedings of ACL
 Deep contextualized word representations,2018, In Proceedings of NAACL
 Adapter-Fusion: Non-destructive task composition for transfer learning,2021, In Proceedings of EACL
 Pre-trainedmodels for natural language processing: A survey,2020, Science China Technological Sciences
 Languagemodels are unsupervised multitask learners,2019, OpenAI blog
 Exploring the limits of transfer learning with a unified text-to-texttransformer,2020, Journal of Machine Learning Research
 Recursive deep models for semantic compositionality over a sentimenttreebank,2013, In Proceedings of EMNLP
 Energy and policy considerations for deeplearning in NLP,2019, In Proceedings of ACL
 Attention is all you need,2017, In Proceedings of NeurIPS
 A broad-coverage challenge corpus for sen-tence understanding through inference,2018, In Proceedings of NAACL
 Transformers: State-of-the-art naturallanguage processing,2020, In Proceedings of EMNLP: System Demonstrations
 Serial or parallel? plug-able adapter for multilingual machine translation,2021, arXiv preprint arXiv:2104
