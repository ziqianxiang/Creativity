title,year,conference
 Maximum a posteriori policy optimisation,2018, In International Conference on Learn-ing Representations (ICLR)
 Constrained policy optimization,2017, InProceedings of the 34th International Conference on Machine Learning
 Constrained Markov Decision Processes,1999, Chapman and Hall
 Mixture density networks,1994, Technical report
 An actor-critic algorithm for constrained markov decision processes,2005, Systems & ControlLetters
 Risk-sensitive and robust decision-making: a cvar optimization approach,2015, In C
 An empirical investigation of the challenges of real-world reinforcement learn-ing,2020, 2020
 An Introduction to the Bootstrap,1993, Number 57 in Monographson Statistics and Applied Probability
 Off-policy deep reinforcement learning withoutexploration,2019, In Proceedings of the 36th International Conference on Machine Learning (ICML)
 Im-proved training of Wasserstein GANs,2017, In I
 Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor,2018, In Proceedings ofthe 35thInternational Conference on Machine Learning (ICML)
 Bootstrapping with models: Confidence intervalsfor off-policy evaluation,2017, In Proceedings of the 16th Conference on Autonomous Agents andMultiAgent Systems
 Acme: A research framework for distributedreinforcement learning,2020, arXiv preprint arXiv:2006
 Robust dynamic programming,2005, Mathematics of Operations Research
 MOReL : Model-based offline reinforcement learning,2020, In Advances in Neural Information Processing Systems(NeurIPS)
 Imitation learning via off-policy distribu-tion matching,2019, In Proceedings of the 7th International Conference on Learning Representations(ICLR)
 Offline reinforcement learningwith fisher divergence critic regularization,2021, In Marina Meila and Tong Zhang (eds
 Stabilizing off-policy Q-learning via bootstrapping error redUction,2019, In Advances in Neural Information Processing Systems(NeurIPS)
 Conservative Q-learning for offlinereinforcement learning,2020, In Advances in Neural Information Processing Systems (NeurIPS)
 Safe policy improvement withbaseline bootstrapping,2019, In Proceedings of the 36th International Conference on Machine Learning(ICML)
 Batch policy learning Under constraints,2019, In Pro-ceedings of the 36th International Conference on Machine Learning
 Batch reinforcementlearning with hyperparameter gradients,2020, In Hal DaUme In and Aarti Singh (eds
 OptiDICE: Offlinepolicy optimization via stationary distribUtion correction estimation,2021, In Proceedings of the 38thInternational Conference on Machine Learning
 ContinUoUs control with deep reinforcement learning,2016, In 4thInternational Conference on Learning Representations
 DualDICE: Behavior-agnostic estimationof discounted stationary distribution corrections,2019, In Advances in Neural Information ProcessingSystems (NeurIPS)
 AlgaeDICE:Policy gradient from arbitrary experience,2019, arXiv preprint arXiv:1912
 Benchmarking Safe Exploration in Deep Reinforce-ment Learning,2019, 2019
 Constrained Markov decision processes via back-ward value functions,8502, In Proceedings of the 37th International Conference on Machine Learning(ICML)
 Masteringthe game of Go without human knowledge,2017, Nature
 Reward constrained policy optimization,2019, InInternational Conference on Learning Representations (ICLR)
 Risk-averse offline reinforcement learn-ing,2021, In International Conference on Learning Representations (ICLR)
 Off-policy evaluation viathe regularized lagrangian,2020, In Advances in Neural Information Processing Systems (NeurIPS)
 Wcsac: WorSt-case soft actor critic for safety-constrained reinforcement learning,2021, Proceedings of the AAAIConference on Artificial Intelligence
 MOPO: Model-based offline policy optimization,2020, In Advances in Neural Infor-mation Processing Systems (NeurIPS)
 GenDICE: Generalized offline estimationof stationary values,2020, In Proceedings of the 8th International Conference on Learning Represen-tations (ICLR)
 GradientDICE: Rethinking generalized offlineestimation of stationary values,2020, In Proceedings of the 35th International Conference on MachineLearning (ICML)
