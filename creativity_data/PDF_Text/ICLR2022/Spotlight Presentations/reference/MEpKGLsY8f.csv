title,year,conference
 UnsUPervised learning,1989, Neural computation
 Combining labeled and unlabeled data with co-training,1998, In COLT
 A closer lookat the training strategy for modern meta-learning,2020, In NeurIPS
 MATE: PlUgging in modelawareness to task embedding for meta learning,2020, In NeurIPS
 TOHAN: A one-steP aPProach towards few-shot hyPothesis adaPtation,2021, InNeurIPS
 Imagenet: A large-scalehierarchical image database,2009, In CVPR
 Learning classifiers from only Positive and Unlabeled data,2008, In KDD
 Rethinking imPortance weighting fordeeP learning Under distribUtion shift,2020, In NeurIPS
 Learning boUnds for oPen-setlearning,2021, In ICML
 Open set domain adaptation:Theoretical bound and algorithm,2021, IEEE Transactions on Neural Networks and Learning Systems
 Aunified objective for novel class discovery,2021, In ICCV
 Model-agnostic meta-learning for fast adaptation ofdeep networks,2017, In ICML
 Unsupervised representation learning bypredicting image rotations,2018, In ICLR
 Sigua:Forgetting may make learning with noisy labels more robust,2020, In ICML
 Learning to discover novel visual categories viadeep transfer clustering,2019, In ICCV
 Deep residual learning for imagerecognition,2016, In CVPR
 Meta-learning in neuralnetworks: A survey,2020, arXiv:2004
 Learning to cluster in order to transfer across domainsand tasks,2018, In ICLR
 Multi-class classificationwithout multi-class labels,2019, In ICLR
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Joint representation learning and novel categorydiscovery on single- and multi-modal data,2021, In ICCV
 Meta-learning to cluster,2019, arXiv:1910
 Adam: A method for stochastic optimization,2015, In ICLR
 Positive-unlabeledlearning with non-negative risk estimator,2017, NeurIPS
 Learning multiple layers of features from tiny images,2009,2009
 Stability and hypothesis transfer learning,2013, In ICML
 Human-level concept learningthrough probabilistic program induction,2015, Science
 Few-shot learning with globalclass representations,2019, In ICCV
 Learning from positive and unlabeled examples with different datadistributions,2005, In ECML
 Do we really need to access the source data? sourcehypothesis transfer for unsupervised domain adaptation,2020, In ICML
 Adaptivetask sampling for meta-learning,2020, In ECCV
 Butterfly: Apanacea for all difficulties in wildly unsupervised domain adaptation,2019, In NeurIPS Workshop onLearning Transferable Skills
 Learningdeep kernels for non-parametric two-sample tests,2020, In ICML
 Heterogeneous domain adaptation: An unsupervisedapproach,2020, IEEE Transactions on Neural Networks and Learning Systems
 Attribute propagation networkfor graph zero-shot learning,2020, In AAAI
 Isometricpropagation network for generalized zero-shot learning,2021, In ICLR
 Classification with noisy labels by importance reweighting,2015, IEEETransactions on pattern analysis and machine intelligence
 A multi-modemodulator for multi-domain few-shot classification,2021, In ICCV
 Conditional adversarialdomain adaptation,2018, In NeurIPS
 Some methods for classification and analysis of multivariate observations,1967, InProceedings of the fifth Berkeley symposium on mathematical statistics and probability
 Algorithmic stability and meta-learning,2005, Journal of Machine Learning Research
 Readingdigits in natural images with unsupervised feature learning,2011, In NeurIPS Workshop on DeepLearning and Unsupervised Feature Learning
 Theo-retical comparisons of positive-unlabeled learning against positive-negative learning,2016, NeurIPS
 Taeml: Task-adaptiveensemble of meta-learners,2018, In NeurIPS Workshop on Meta learning
 Adaptive posterior learning: few-shot learning with a surprise-based memory module,2019, In ICLR
 Optimization as a model for few-shot learning,2017, In ICLR
 Asymmetric tri-training for unsuperviseddomain adaptation,2017, In ICML
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Prototypical networks for few-shot learning,2017, InNeurIPS
 Deep model transferabilityfrom attribution maps,2019, In NeurIPS
 Transfer learning via minimizing theperformance gap between domains,2019, In NeurIPS
 Generalizing from a few examples:A survey on few-shot learning,2020, ACM Computing Surveys
 Meta-learning hyperparameter performance predictionwith neural processes,2021, In ICML
 Part-dependent label noise: Towards instance-dependentlabel noise,2020, NeurIPS
 Unsupervised deep embedding for clusteringanalysis,2016, In ICML
 Hierarchically structured meta-learning,2019, InICML
 Improving generalization in meta-learning via task augmentation,2021, In ICML
 Neighborhoodcontrastive learning for novel class discovery,2021, In CVPR
 Neighborhoodcontrastive learning,2021, In CVPR
 Openmix: Revivingknown knowledge for discovering novel visual categories in an open world,2021, In CVPR
 Multi-class heterogeneousdomain adaptation,2019, Journal of Machine Learning Research
 Laplacian regularized few-shotlearning,2020, In ICML
