title,year,conference
 Neural network learning: Theoretical foundations,2009, cambridgeuniversity press
 Almost linear vc-dimension bounds for piecewise polynomialnetworks,1998, Neural computation
 Nearly-tight vc-dimension and pseudodimen-sion bounds for piecewise linear neural networks,2019, The Journal of Machine Learning Research
 On the capabilities of multilayer perceptrons,1988, Journal of complexity
 Reconciling modern machine-learning practice and theclassical bias-variance trade-off,2019, Proceedings of the National Academy of Sciences
 Sharp representation theorems for relu networks with precise dependenceon depth,2020, arXiv preprint arXiv:2006
 A universal law of robustness via isoperimetry,2021, arXiv preprintarXiv:2105
 Network size and weights size for memorizationwith two-layers neural networks,2020, arXiv preprint arXiv:2006
 A law of robustness for two-layers neural networks,2021, InConference on Learning Theory
 Depth-width trade-offs for relu net-works via sharkovskyâ€™s theorem,2019, arXiv preprint arXiv:1912
 Approximation by superpositions of a sigmoidal function,1989, Mathematics of control
 Depth separation for neural networks,2017, In Conference on Learning Theory
 Memorizing gaussians with no over-parameterizaion via gradient decent on neuralnetworks,2020, arXiv preprint arXiv:2003
 The power of depth for feedforward neural networks,2016, In Conference onlearning theory
 Bounding the vapnik-chervonenkis dimension of concept classesparameterized by real numbers,1995, Machine Learning
 Identity matters in deep learning,2016, arXiv preprint arXiv:1611
 Upper bounds on the number of hidden neurons in feedforwardnetworks with arbitrary bounded nonlinear activation functions,1998, IEEE transactions on neuralnetworks
 Multilayer feedforward networks with a non-polynomial activation function can approximate any function,1993, Neural networks
 On the representational efficiency of re-stricted boltzmann machines,2013, In Advances in Neural Information Processing Systems
 Deep double descent: Wherebigger models and more data hurt,2019, arXiv preprint arXiv:1912
 Optimization landscape and expressivity of deep cnns,2018, In Internationalconference on machine learning
 Provable memorization via deep neural networks using sub-linear parameters,2020, arXiv preprint arXiv:2010
 An exponential improvement on thememorization capacity of deep threshold networks,2021, arXiv preprint arXiv:2106
 Depth-width tradeoffs in approximating natural functions with neuralnetworks,2017, In International Conference on Machine Learning
 A simple method to derive bounds on the size and to train multilayerneural networks,1991, IEEE transactions on neural networks
 Benefits of depth in neural networks,2016, In Conference on learning theory
 Neural networks with small weights and depth-separation barriers,2020, arXivpreprint arXiv:2006
 Size and depth separation in approximating benignfunctions with neural networks,2021, In Conference on Learning Theory
 Depth separation beyond radial functions,2021, arXivpreprint arXiv:2102
 Memory capacity of neural networks with threshold and relu activations,2020, arXivpreprint arXiv:2001
 Error bounds for approximations with deep relu networks,2017, Neural Networks
 Small relu networks are powerful memorizers: a tight analysisof memorization capacity,2019, In Advances in Neural Information Processing Systems
