Published as a conference paper at ICLR 2022
The Inductive Bias of In-Context Learning:
Rethinking Pretraining Example Design
Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen & Amnon Shashua
The Hebrew University of Jerusalem
{yoav.levine, noam.wies, daniel.jannai, dan.nav}@mail.huji.ac.il
Ab stract
Pretraining Neural Language Models (NLMs) over a large corpus involves chunk-
ing the text into training examples, which are contiguous text segments of sizes
processable by the neural architecture. We highlight a bias introduced by this
common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples. This
intuitive result has a twofold role. First, it formalizes the motivation behind a broad
line of recent successful NLM training heuristics, proposed for the pretraining and
fine-tuning stages, which do not necessarily appear related at first glance. Second,
our result clearly indicates further improvements to be made in NLM pretraining for
the benefit of Natural Language Understanding tasks. As an example, we propose
“kNN-Pretraining": we show that including semantically related non-neighboring
sentences in the same pretraining example yields improved sentence representa-
tions and open domain question answering abilities. This theoretically motivated
degree of freedom for pretraining example design indicates new training schemes
for self-improving representations.
1	Introduction
Beyond excelling in their core task of pure language modeling, modern Neural Language Models
(NLMs) show impressive zero- and few-shot abilities in more general Natural Language Understand-
ing (NLU) tasks (Brown et al., 2020). This implies that the training corpus contains the information
required for performing such tasks, and moreover it implies that the common pretraining process
grants the trained NLM some access to these higher level capabilities. In this paper, we highlight a
connection between the quality of the emergent NLU capabilities and a basic component in the NLM
training scheme: the process of segmenting the corpus into training examples.
Specifically, NLMs self-train over huge training corpora (typically, billions to trillions of words). A
basic, automatic, operation in the training pipeline is to segment these corpora into training examples:
contiguous text chunks of sizes processable by the neural architecture (typically, up to thousands of
words). We formalize an expressivity bias that this segmentation process introduces, to be referred to
as the in-context bias, which directly affects the NLM’s ability to integrate cross-corpus information.
We show that the NLM can model much stronger dependencies between sentences that were shown
together at least once in-context, i.e., in the same training example, than between sentences that were
never shown together in the same input. This inductive bias may be good for language modeling,
but it implies that NLU capabilities that involve integrating information from different examples
across the corpus (see, e.g., figure 1), are under-favored by design in the current setting. Thus, if one
sentence in the corpus can elucidate the meaning of another sentence (e.g., defines a hard concept or
provides auxiliary information), our result implies that a model that saw them in different training
examples will enjoy this elucidation less than a model that saw them in the same training example.
While standard approximation results examine the expressivity of an architecture over a single input,
our theoretical approach pertains to the entire training process, and examines the expressive capacity
of the resultant NLM with respect to the training set. Therefore, our approximation result ties an
optimization parameter (the learning-rate) to the regular NLM architecture expressivity parameters
(depth, width). Intuitively, sentences that were never shown in the same input can only access each
other via the weights of the network during training. The mechanism for “storing" information in
1
Published as a conference paper at ICLR 2022
the network involves a very small learning-rate term η ; our analysis formalizes and quantifies an
“expressivity toll" that the model pays when making use of such harder-to-access stored information.
We employ the tool of a function’s separation rank with respect to subsets of its variables, which
quantifies its ability to model input dependencies between these subsets. The separation rank was
employed for analyzing the dependencies modeled by convolutional (Cohen & Shashua, 2017),
recurrent (Levine et al., 2018), and self-attention (Levine et al., 2020) networks with respect to
a single input example. In order to analyze an NLM’s ability to model dependencies between
different training examples, we refine the usage of this measure in two manners: (1) we introduce
the ε-separation rank, which measures the effective ability of a function to model dependencies in a
finite precision setting, and (2) we modify the separation rank such that it can account for the more
intricate mechanism of mixing between variables that occurs in the sequential case.
Specifically, we upper bound the log of the separation rank of a depth L width dx self-attention based
NLM, with respect to two sentences that are shown in its input, by O(dxL), and prove that this bound
is tight. On the other hand, we upper bound this measure with respect to two sentences that were
never shown in the same input by O(dχ[L - 0.5iog3(η-1)]). Given common learning-rate values of
η ∈ [10-6, 10-4], this implies a guaranteed “depth deficit" of 〜6 layers for modeling dependencies
between sentences that are not seen in the same training example. After the presentation of our
results, we point at empirical evidence that imply that this depth deficit is more significant, and may
behave like a fraction of L. We leave attempts to tighten the depth deficit estimates to future work.
1.1	The in-context bias drives a variety of existing approaches
Several recent works intuitively rely on the above formalized in-context expressivity bias in different
manners, and significantly improve both task-specific training and pretraining of NLMs. Gao et al.
(2020) advance the frontier in k-shot learning via finetuning. They show that by concatenating several
related training examples per input, instead of using standard fine-tuning practice of one example per
input, the k-shot performance on sentence similarity tasks is considerably boosted. Another example
was pointed out in Humeau et al. (2020); Thakur et al. (2020): when training for sentence similarity
tasks, including both sentences in the same input leads to a performance gain of around 10 points
relative to separately encoding each sentence. In the challenging setting of open-domain question
answering, Izacard & Grave (2020) jointly attend to all documents that may contain the answer, and
show large gains relative to prior methods that consider these documents in separate forward passes.
Turning our focus to methods that leverage the in-context bias for improved pretraining, the most
straightforward effort is a body of work aimed at reducing the quadratic dependence of the Trans-
former computation on input sequence length (Tay et al., 2020). While allowing for more text
in-context during training, this does not improve the model’s ability to integrate text across different
documents in the corpus. The following approaches take a further step and enable direct cross-corpus
connections during pretraining. Lewis et al. (2020) attend to related documents when maximizing
the likelihood of a target document. The scope of related documents is restricted by meta-data:
taken from the same Wikipedia entry as the input, or published on the same date. Guu et al. (2020)
expand the scope of the related documents, by training a Knowledge-Retrieval model that has access
to the entire Wikipedia corpus. They retrieve several related documents per target document, but
condition on each related document independently. Outside of the natural language domain, Rao
et al. (2021) train a Transformer based protein-LM that receives multiple related protein sequences
in-context. Their protein-LM surpasses previous methods which process one sequence per input by a
wide margin, with significant parameter efficiency.
1.2	Leveraging the in-context bias for NLU oriented training
Though the in-context bias is intuitive, the above subsection surveys recent advances that leverage
it in non-trivial manners. Having formalized the theoretical advantage for in-context integration of
related text, the roots of the above successes can be unified, and importantly, new methods for tilting
the pretraining bias towards NLU tasks are indicated. Following the presentation of our theoretical
results in section 2, we detail in section 3 two controlled setting exemplifications of new methods that
directly leverage the in-context bias.
Our first experiment augments the Task Adaptive PreTraining (TAPT) setting of Gururangan et al.
(2020), in which an NLM that was pretrained on a general corpus continues pretraining (with
its original objective) on the training set of an NLU task. We perform TAPT on the SentEval
sentence similarity benchmark (Conneau & Kiela, 2018), and during TAPT introduce the following
2
Published as a conference paper at ICLR 2022
augmentation: along with SentEval sentences, we simultaneously pretrain on related sentences from
Wikipedia, the general pretraining corpus. The related sentences are found via k-Nearest Neighbors
(kNN) search between the embeddings of SentEval examples and all Wikipedia sentences; we thus
dub this approach kNN-TAPT. Importantly, during kNN-TAPT, each input includes a training example
from the task, appended in-context by its Wikipedia neighbors. We demonstrate significant gains of
the kNN-TAPT over regular TAPT on SentEval sentence similarity tasks. A dedicated ablation study
shows the significance of adding the general corpus neighbors in-context, versus in separate training
examples, during kNN-TAPT.
Our second experiment introduces a
task-independent pretraining phase,
dubbed kNN-Pretraining. As in
kNN-TAPT, we group together sen-
tences with similar sentence repre-
sentations in the same training ex-
ample, but in kNN-Pretraining we
use only sentences from the gen-
eral pretraining corpus. This can be
viewed as a sentence-focused varia-
tion of the above surveyed pretrain-
ing schemes in Lewis et al. (2020)
and Guu et al. (2020), who oper-
ate on full documents (up to 512
each), and is very similar to RETRO
by Borgeaud et al. (2021) (Deep-
Mind), who show the benefits of this
FΓ..a 460-page book is about 1 pound, 2 ounces..."
"...pens can weigh roughly 20-50 grams..." "Which is heavier, a pen or a book?"
Figure 1: A 10% addition of kNN-Pretraining boosts zero-shot
closed book QA score by〜5X (evaluation set size is 20,000).
approach given much larger resources. Figure 1 shows that after regular pretraining for 200K steps
on Wikipedia, the zero-shot closed book performance of 3 different randomly initialized GPT2-
medium models (345M parameters) on open domain questions from Wikipedia (Kwiatkowski et al.,
2019) is very low (correct on less than 50 questions out of 20,000 in the evaluation set). Adding
kNN-Pretraining for 20K steps raises performance significantly (correct on roughly 250 questions
in the evaluation set), reflecting the enhanced ability to integrate knowledge from related sentences,
acquired via the in-context bias.
In summary, our main contributions are:
•	We formally establish the in-context bias: information within pretraining examples is better
represented than information integrated across pretraining examples.
•	We ask and answer a new type of network expressivity question: how expressive is a network
with respect to examples seen during its training process?
•	We demonstrate that in-context bias motivated “pretraining example design" elicits better
representations from the same data: kNN-Pretraining improves on several NLU tasks.
2 Theoretical analysis: The in-context bias of self-attention
In this section, we consider the entire NLM training procedure as a functional that receives an
unlabeled training corpus and outputs a trained NLM. Our analysis focuses on the corpus segmentation
into training examples as a hyper-parameter of this functional. We reduce the high-level notion
of representing “cross-corpus correlations" to a quintessential case study: we quantify the NLM’s
ability to model dependencies between two sentences that appear in the same training example (the
in-context representation) and in different training examples (the sequential representation).
We believe that the in-context bias can be shown to exist in a broad range of architectures, but we
focus on self-attention since almost all modern NLMs are based on the Transformer architecture
of Vaswani et al. (2017). Our theoretical framework is based on that of Levine et al. (2020); Wies
et al. (2021), who analyze a simplified, theoretically accessible, self-attention network. They study
the expressivity of this self-attention architecture with respect to its input, and use a measure of a
multivariate function’s ability to correlate two subsets of its variable set, referred to as the separation
rank. The analyzed framework captures the connectivity of self-attention but omits its softmax and
ReLU non-linearities (see eq. 1 below). We refer the reader to Levine et al. (2020); Wies et al. (2021)
for a discussion on the impact of these relaxations. Essentially, they are shown to weaken the overall
network power but still allow a meaningful comparison of the self-attention integration abilities.
3
Published as a conference paper at ICLR 2022
Importantly, both works derive unforeseen theoretical conclusions from analyses of the separation
rank measure for this architecture class, and then provide extensive empirical corroboration for
their manifestation in common Transformer architectures, reinforcing the relevance of this setting.
In the following, we describe in section 2.1 the analyzed in-context and sequential self-attention
representations of two sentences. Then, in section 2.2, we present the separation rank, which we use
in section 2.3 for quantifying the advantage of in-context representations versus sequential ones.
2.1	The analyzed in-context and sequential representations
For an input sequence of N embedding vectors {xj ∈ Rdx }jN=1, denote the function realized by
the analyzed H-headed depth-L width-dx Transformer architecture at output location i ∈ [N] by:
gWWL,dx (x1,…，XN) ∈ Rdx, where W stands for learned parameters, recursively defined:
HN
gWi,l+1,dx gW1,l,dx,...,gWN,l,dx =XWO,l,hXaihjWV,l,hgjW,l,dx	(1)
h=1	j=1
aihj :=DWQ,l,hgiW,l,dx,WK,l,hgWj,l,dxE ; gWi,0,dx =Xi
where W is composed of Key, Query, Value and Output matrices: ∀l ∈ [L], h ∈ [H],
W K,l,h, W Q,l,h, W V,l,h, (WO,l,h)> ∈ Rda×dx, where we assume the standard choice da = dx/H.
For a word from vocabulary of size V , w ∈ [V], the translation into the Transformer dimension is
done via a mapping EMV : [V] → Rdx :
EMV (W) = (MV)W	⑵
where MV ∈ Rdx ×V is the learned vocabulary embedding matrix, and MV w is its wth column, also
referred to as the learned word embedding for w. Overall, the function of the analyzed Transformer
over a sequence of N words {wj ∈ [V ]}jN=1 can be written by composing eqs. 1 and 2:
yWLMV (w1,…,wN) = gW,L,dx (EmV (w1),…,EMV (WN))	⑶
For simplicity of presentation, we examine two sentences S1 and S2 of equal length N : S1 =
{W1j}jN=1 and S2 = {W2j}jN=1. The in-context representation simply concatenates both in the input:
yinLonxext (S1,S2) : = ywWLMV (w1,…,WN, w1,…,WN) .	(4)
For the sequential approach, we consider a setup in which sentence S1 is inserted into the network
at training step t and sentence S2 is inserted into the network at training step t + 1. The output of
the network at training step t is therefore: yWi,L,,dMxV (S1), where Wt, MtV stand for all the learned
weights before training step t. Focusing on autoregressive NLMs for simplicity of presentation
(the analysis holds for bidirectional NLMs as well), the log-likelihood loss is given by L (S1) =
-PN=I log ∖^so,f-tmax {(MtV)> y^M (Si)})汁[,and the gradient update for any learned
weight θ ∈ {Wt,MV} is: θt+ι (Si; η) = θt -η∙ dL(Sι)∕∂θt, where η is the learning rate. Accordingly,
the analyzed sequential representation is the network output after training step t + 1:
yseqUeι5i,η (S1,S2):= yWt+：(Si；n)MV+i(Si；n) (S2).	(5)
In practice, two relevant non-neighboring sentences are not necessarily shown in consecutive pre-
training steps. In comparison to the realistic scenario of Si and S2 appearing at any training step,
this simplifications tilts the representation in favor of modeling high correlations between Si and
S2 . Thus, by upper bounding the ability to correlate Si and S2 in the setting of eq. 5 (as we do in
section 2.3), we establish an inherent limitation of the network to access information that was stored
in its weights via the gradient update mechanism. In the next subsection, we present our approach for
measuring a network’s ability to correlate two sentences seen during training, which we will use in
order to separate between the in-context and sequential settings.
2.2 A measure for modeling in-context and sequential dependencies
In this section, we refine the separation rank, used in prior work in order to analyze the dependencies
between two sentences appended in-context. In section 2.2.1 we present the separation rank and
introduce a finite precision refinement of it, referred to as the effective separation rank, which helps
to elucidate the degradation in integration ability caused by the gradient update mechanism. In
section 2.2.2 we point at a structural problem in employing the separation rank in the same manner in
which it was employed in prior work that analyzed only architecture expressivity, and introduce the
the sequential separation rank, meaningful for both the in-context and sequential cases.
4
Published as a conference paper at ICLR 2022
2.2.1	The effective separation rank
The separation rank has been established as a measure of dependencies modeled by deep convolutional,
recurrent, and self-attention networks (Cohen & Shashua, 2017; Levine et al., 2018; 2020). For a
function y(A, B) over variables A = {aj ∈ X}jM=1 and B = {bj ∈ X}jM=1, the separation rank
w.r.t. (A, B) is the minimal number of summands that together sum up to equal y(A, B), where each
summand is multiplicatively separable w.r.t. (A, B), i.e., is equal to a product of two functions - one
that intakes only A variables and another that intakes only B variables. Formally, the separation rank
of y : X2M → R w.r.t. (A, B) is defined as follows:	R
sep(A,B) (y) := min R ∈ N : ∃g1. . .gR, g10. . .gR0 : XM → R s.t. y(A, B) = Xgr(A)gr0 (B)
If the separation rank of a function w.r.t. (A, B) is 1, it is multiplicatively separable w.r.t. (A, B),
meaning it cannot take into account consistency between A and B. The higher sep(A,B) (y) is, the
farther y is from this situation, i.e., the more it models dependency between A and B. We will further
make use of the effective separation rank:
ε-seP(A B) (y) ：= min ∣R0 ≤ SeP(A B) (y) ∈ N : ∃y ： X2M → R St
(7)
keP(A,B) (y) = R0] ∧ [∀x ∈ X2M : |y (x) - y (x)| ≤ ε] }
In words, if a function has a high separation rank, but it can be approximated up to error ε by a
function with a low separation rank, then it has a low ε-separation rank.
Prior works compare two functions by establishing the differences between their separation ranks. In
principle, these differences could manifest only in irrelevant magnitudes (if many of the summands
in the separation rank definition are negligibly small for the function with the higher separation rank,
for example). The effective separation rank is key to our analysis because we rely on the fact that
information on past examples is stored in the network weights in a small magnitude (due to a small
learning-rate). We show in section 2.3 that much of the integration between text segments from
different training examples occurs in very small magnitudes due to high powers of the learning rate,
limiting the effective integration, as measured by the ε-separation rank. Our techniques for bounding
the ε-separation rank are extendable to prior works, and while these did not examine the gradient
update mechanism, their results can be reinforced due to the guarantees of this introduced measure.
2.2.2	The sequential separation rank
Levine et al. (2020), who were the first to apply the separation rank to functions realized by Trans-
former architectures, studied classical architecture expressivity questions which apply only to the
i,L,dx
in-context representation. Accordingly, they analyzed only the separation rank of g x , defined in
eq. 1, and the input variables considered for calculating the separation rank were the word embed-
ding vectors. A fundamental difficulty arises when attempting to directly apply this method to the
sequential representation: the word embedding vectors are learned parameters of the architecture. In
the sequential case, when the second sentence S2 is introduced after the calculation at time-step t, the
vectors used to describe it, if we were to follow prior practice, would already have depended on S1.
In order to meaningfully measure the integration ability of two sentences across examples in the
presence of the gradient update mechanism, we introduce an auxiliary sentence association layer
with new variables: a, b ∈ Rdx , which explicitly associates each employed vocabulary embedding
vector with the sentence index s ∈ {1, 2} that invoked its usage:
ZS (EMV(Ws)) = IEMV(Wj) ® a	if s = 1	(8)
s	EMV (wsj )	b	if s = 2
where denotes element-wise multiplication. We define the sentence association operation over the
analyzed representations, denoted Zy (a, b) with y ∈ {yiin,-Lc,odnxtext(S1,S2),ysie,Lqu,ednxti,aηl(S1,S2)}(eqs.4
or 5), to be the application of the sentence association layer of eq. 8 to all uses of the input embedding
layer during the computation of y. Meaning, for both mechanisms, that chosen word embeddings
are marked with the identity of the sentence that invoked them. Finally, we define the following
specialization of the separation rank measure to our setting, referred to as the sequential separation
rank of y ∈ {yiin,L-c,odnxtext(S1,S2),ysie,Lqu,ednxti,aηl (S1,S2)}:
seq-seP(y) := seP(a,b) (Zy)	(9)
ε-seq-seP(y) := ε-seP(a,b) (Zy)
5
Published as a conference paper at ICLR 2022
Clearly, when the introduced variables are vectors of 1, the auxiliary layer in eq. 8 is the identity
operation and so Zy(1, 1) = y for both representations. More deeply, our expressivity questions
query the ability of the in-context and sequential mechanisms to integrate two sets of variables, and
Zy captures the essence of this ability by explicating where each set enters the computation.
In the next subsection, we show that for the in-context case, analyzed in prior work, the introduced
measure of the sequential separation rank is asymptotically equal to the previously employed measure
of separation w.r.t. a partition of the input word embeddings (Levine et al., 2020). Thus, the properties
of the existing framework are unchanged under the new definition. At the same time, for the sequential
case brought forth in this paper, the sequential separation rank considers both the effect of S1 on the
gradient-updated word embedding and the introduction of S2 into the computation.1 2 In the following
section, we make use of both extensions to the separation rank in eqs. 7 and 9 in order to establish the
in-context bias.
2.3 The expressive advantage of in-context learning
We show below that the function computed by a self-attention based NLM when inserting sentences
S1 and S2 together in its input (the in-context representation) can model more elaborate dependencies
between S1 and S2 than the function attained when showing S1 in the input, modifying the network’s
weights according to its loss, and then showing S2 in a subsequent input (the sequential representation).
We begin by stating the following corollary, following from theorem 2 in Levine et al. (2020) and
proposition 1 in appendix A, which upper bounds the sequential separation rank of the in-context
representation:
Corollary 1. Let yi(np-,cio)n,tLex,tdx be the p ∈ [dx] entry of the analyzed in-context representation defined in
eq. 4. Assume that L > log3 dx. Then (O notation omits log terms: log dx, log L, log H):
log hseq-Sep (yi(IpcontLd X)] = O(L ∙ dx)	(IO)
However, the ε-separation rank of the sequential representation is upper bounded by a lower term:
Theorem 1.	(See proof in appendix B). Let ys(epq,uie),nLtia,dl x ,η be the p ∈ [dx] entry of the analyzed
sequential representation defined in eq. 5. Assume that all learned parameters and all gradients are
bounded: ∀θ ∈ {W, MV} : 0 < Λmin ≤ ∣θ∣, ∣dL(SI)/∂θ∣ ≤ Λmaχ,2 N < dχ, and that L > log3 dχ.
Then, ∀ε > 0:
log hε-Seq-Sep (y(pqU)nLad x,η)]=OaL+0.5log3(η)] ∙ dx)	(II)
Therefore, a gap between upper bounds on the ability to model dependencies between S1 and S2 is
indicated. Since the learning rate η is a small term, its log is negative and the gap is in favor of the
in-context representation. The following theorem guarantees that this gap is meaningful, by showing
that the higher upper bound (of the in-context case) is tight in terms of effective rank:
Theorem 2.	(See proof in appendix C). For yi(np-,cio)n,tLex,tdx as defined in corollary 1, there exists an
assignment of the network weights for which the following holds:
( L	-ilog hε-Seq-Sep (yi(IpconLxdX)] =ω(L ∙ dx)	(12)
where ε = O ( (3L+Lx-1)).
Notably, corollary 1 and theorem 2 show that for the in-context case, the sequential separation rank
asymptotically equals the regular separation rank, validating the relevance of this measure.
We now provide a high level proof sketch that captures the manner in which the theoretical framework
of sections 2.1 and 2.2 is used for establishing the above gap (full proof in the appendix). For the
in-context case, notice that each self-attention layer, defined in eq. 1, is a degree 3 polynomial over
its 2N ∙ dχ inputs, rendering the whole network a degree 3L polynomial. We write this polynomial as
a sum over many monomials, and by definition, the separation rank of any monomial composing the
polynomial is 1. Since the separation rank of a sum of functions is upper bounded by the sum of their
separation ranks, we upper bound the separation rank by the number of these monomials, yielding
eq. 10. The main difference in the sequential representation case is that the S1 variables affect the
1To see this, note that for s = 2 the operation in eq. 8 includes both MtV+1 (a) and variables from b.
2The upper boundedness assumption resembles practices of gradient clipping and weight decay, and the
lower boundedness assumption resembles finite precision.
6
Published as a conference paper at ICLR 2022
computation only via the gradient, so their impact is expected to be limited. However, considering that
S? first encounters gradient updated vocabulary matrix entries (mV)古+]=(mV)古-ηdL(SI)/∂(m),
it appears that both S1 and S2 variables enter the self-attention stack via its input, similarly to the
in-context case. So the integration between S1 and S2 occurs right from the start, and indeed we
show that the separation rank of both representations is similar. However, since any function of S1 is
accompanied by the learning-rate η, the monomials for which there are many S1 variables will be
multiplied by high powers of η. This causes many monomials to be negligibly small, and accordingly
not to contribute to the ε-separation rank. By combinatorial considerations we show that the number
of monomials that are not attenuated by η (have sufficiently large magnitude) yields eq. 11. □
The above theorems establish that from an expressivity perspective, the small magnitude of commonly
employed learning-rates hinders the ability to integrate information across different training examples.
Specifically, the established gap implies that the power of the joint representation of two sentences
shown in different training examples is upper bounded by that of a network shallower by 0.5 log3(η-1)
layers that has seen them in the same context. Common learning-rate values are on the order of
η ∈ [10-6,10-4], implying a deficit of 〜6 layers in the sequential case. As shown in in Levine et al.
(2020); Tay et al. (2021), in many practical regimes of network size depth is crucial for expressivity,
reinforcing the implications of this gap.
The weaker upper bound, of the sequential case, is not guaranteed to be tight. This means that
theoretically, the sequential representation may in fact be much weaker than what we have proven,
e.g., that showing two sentences in the same context yields a representation that cannot be matched
merely by showing them in separate contexts and adding a realistic number of layers. However,
Roberts et al. (2020) show evidence supporting our indicated link between architectural parameters
and the in-context bias. They show that when performing open domain question answering tasks (their
defined “closed book" setting), a large T5 model that sees only the question performs comparably
to smaller models that are allowed to attend to the documents that contain the answer. This directly
implies a certain strength of the sequential mechanism, namely, that information which was seen
during training can be accessed via the weights when the model is realistically stronger, as implied
by our bounds. Notably, the large T5 model is 2-4 times the depth of the contrasted smaller models
(48 versus 12-24 layers), suggesting that the upper bound can be tightened to a fraction of L, or
that factors that are beyond expressivity also contribute to the in-context bias (e.g., optimization,
generalization). Investigation of these aspects is left for future work.
3	kNN based pretraining example design
Our theoretical analysis quantifies the relation between the small magnitude of the learning rate, and
the deficiency in the ability to model dependencies between different training examples. Clearly, small
learning-rates are critical for optimization purposes, so the formalized phenomenon should not be
solved via high learning-rates during training. Instead, our analysis makes it clear that if correlations
between specific sentences are important for a given task, appending them in-context yields better
representations for the task. Below, we describe two controlled experiments that demonstrate the
importance of this indicated “pretraining example design" degree of freedom. In both experiments,
correlated sentences are identified via kNN search in their RoBERTa-large sentence representation
space (Reimers & Gurevych, 2019), performed using the FAISS library (Johnson et al., 2019).
3.1	kNN Task Adaptive PreTraining
The Task Adaptive PreTraining (TAPT) method, in which an NLM pretrains on the training set of an
NLU task, leads to impressive gains (Gururangan et al., 2020). Notably, TAPT is most effective after
the regular pretraining stage on a general corpus. This implies that during TAPT, the model generates
improved representations by integrating the task related text with the knowledge stored in its weights
from the preceding general pretraining phase. Under this premise, we postulated that performance
will improve if we make relevant sentences from the general corpus more available to the model
during the TAPT phase. According to the above analysis, a simple and effective way to bias the
model towards representing desired correlations between sentences is to append them in context.
We thus propose the kNN-TAPT phase, in which the training examples are composed of task examples,
concatenated with their general corpus neighbors in embedding space. We applied kNN-TAPT on
the SentEval sentence similarity tasks. Showing similar sentences from Wikipedia is expected to be
particularly useful on these tasks, so this is a good experimentation ground to search for effects of
the in-context bias. For each SentEval example, we searched over 100M Wikipedia sentences and
appended in-context neighbors that have embeddings with over 0.8 cosine similarity to the SentEval
7
Published as a conference paper at ICLR 2022
	STS12	STS13	STS14	STS15	STS16	STS-B	SICK-R	Avg.
Basline Roberta Model	32.1	56.3	45.2	61.3	62.0	55.4	62.0	53.5
TAPT	43.0	62.2	51.6	70.6	64.9	63.0	63.5	59.8
kNN-TAPT (random, in-batch)	40.2	62.7	51.9	64.9	62.1	61.5	65.4	58.4
kNN-TAPT (neighbors, in-batch)	40.8	62.4	53.1	66.1	63.0	61.3	65.2	58.8
kNN-TAPT (random, in-context)	44.62	62.64	51.4	65.28	64.93	64.31	66.96	60.0
kNN-TAPT (neighbors, in-context)	44.9	63.4	52.1	66.2	65.3	66.5	68.3	61.0
Table 1: kNN-TAPT, which augments the Task Adaptive PreTraining (TAPT) setting of Gururangan
et al. (2020), harnesses the in-context bias and improves SentEval sentence similarity scores.
example embedding, with a special token inserted between different sentences. We continued until
finding no more neighbors or reaching a maximum of 256 tokens in the RoBERTa vocabulary (Liu
et al., 2019). This search yielded 170K examples, over which we continued training a pretrained
RoBERTa-base model for 5 epochs, using the first epoch for learning-rate warmup and examining
peak learning rates of {1, 3,5,7} ∙ 10-5. See appendix D for implementation details.
Table 1, shows zero-shot SentEval sentence similarity scores, attained by using the average word
embedding of an inserted sentence as its examined sentence representation (shown by Reimers &
Gurevych (2019) to be most meaningful in zero shot). All models were trained according to the
above prescription, besides the baseline RoBERTa which was simply evaluated. kNN-TAPT improves
over regular TAPT, by over 1 point on average, implying that the Wikipedia neighbors are indeed
useful to the TAPT stage. We compared 4 kNN-TAPT variants as an ablations study. Importantly all
variants labeled with kNN-TAPT train on the same training data during the TAPT stage - the SentEval
sentence similarity tasks training sets and their Wikipedia nearest neighbors, and differ only in the
arrangement of the data into training examples. The “neighbors" flag relates a SentEval example to
its actual neighbors from the kNN search, while the “random" flag relates it to random Wikipedia
sentences from the overall neighbors pool attained in the search. The “in batch" flag implies that
related sentences were shown in the same batch, where every training example includes only one
sentence from either SentEval or Wikipedia. In contrast, the “in context" flag implies that related
sentences were shown in the same training example.
The weakness of “neighbors, in-batch" implies that the a-priori plausible approach of biasing the
model to learn from these Wikipedia neighbors via placing them in the same batch is not nearly
as effective as the theoretically motivated in-context approach. Leading sentence representations
employ in-batch techniques (see for example the contrasive setting of Gao et al. (2021b)), and this
signal strongly suggests developing in-context parallels. The fact that the original TAPT scheme
outperforms the in-batch approaches implies that including the Wikipedia sentences in separate
training examples is harmful. We postulate that this is because training examples that have only
Wikipedia sentences actually dilute the original TAPT signal. Indeed, by this view, the reason that
“random, in-context" performs comparably to TAPT, is that it does not dilute the original TAPT signal
- every training example includes a SentEval example. Overall, the clear advantage of the “neighbors,
in-context" kNN-TAPT variant encourages leveraging the in-context bias for TAPT in further tasks.
3.2 kNN Pretraining
We extended the above to more general kNN-Pretraining, designing pretraining examples with
related non-neighboring sentences given only the general pretraining corpus. kNN-Pretraining is also
motivated by the kNN-LM results of Khandelwal et al. (2019), who show significant benefits of using
nearest neighbors in representation space at inference time. Their results exemplify the potential
impact of integrating cross-corpus related examples; our kNN-Pretraining approach provably biases
the model to learn these correlations at pretraining time, via the in-context bias.
Specifically, we performed kNN search over Wikipedia sentences for every sentence in Wikipedia,
and created each training example similarly to the protocol in the previous subsection. During
kNN-Pretraining, half of the batch contained regular pretraining examples and half contained the
prepared kNN examples, in order to retain longer ranged LM abilities. To examine the effect of kNN-
Pretraining, we pretrained GPT-base and GPT-medium (110M and 345M parameters) architectures
from scratch over Wikipedia in the regular pretraining scheme, and switched to kNN-Pretraining at
two different points during pretraining (200K and 400K). The training examples were of maximal
size 256, and the batch size was 128 for the GPT-medium models and 256 for the GPT-base models.
In order to directly probe the acquired ability to integrate non-neighboring sentences, we evaluated
the resultant models on the very challenging setup of zero-shot closed-book open domain question
8
Published as a conference paper at ICLR 2022
answering. In this setup, the unidirectional pretrained model decodes an answer conditioned on the
given open ended question. We evaluated the models on questions from the Natural Questions (NQ)
benchmark (Kwiatkowski et al., 2019), using the same phrasing employed in Brown et al. (2020),
and employing the standard “open-domain” version as used e.g. by Lee et al. (2019); Asai et al.
(2019); Roberts et al. (2020). NQ is composed of questions that have answers within Wikipedia, our
pretraining corpus. kNN pretraining can imtuitively improve in cases where the passage containing
the answer has elucidating nearest neighbors from across wikipedia that would help the model to
better internalize the answer, such that it is more accessible to the model in zero shot. As figure 1
demonstrates, 3 baseline models, pretrained with the regular scheme, achieve very low F1< 10-3
scores on this task. In contrast, kNN-Pretraining shows a low-scoring but significant improvement.
To increase the credibility of the signal, we evaluated our models on the first 20K examples from the
NQ training set (we tested zero-shot performance, so the training set was not used earlier). Indeed,
the attained F1 scores are low, but they correspond to 100s of correct answers that the kNN-Pretrained
model provide after roughly 10% of the overall training time, versus much less in the 3 randomly
initialized baseline models. Finally, we include in appendix E NQ scores of models of different sizes
when starting kNN-Pretraining at different checkpoints, and in appendix F zero-shot scores on several
GLUE tasks, which demonstrate clear gains of kNN-Pretraining over the baselines.
4	Discussion
Modern NLM pretraining schemes have tremendously advanced the natural language landscape,
since they allowed powerful models to train on huge amounts of unlabeled text. But NLMs are now
challenged with tasks which require deeper and more nuanced understanding of text, and means of
improving the basic pretraining process should be considered. For a given architecture, pretraining
can be improved by adding more data or finding more sophisticated training objectives to apply
over existing data. In this paper we highlight a parallel path for improvement, which employs the
same data and objective, but redistributes the available strength of the Transformer architecture such
that important connections within the pretraining corpus are learned more effectively. Specifically,
we highlight the bias of the trained NLM towards modeling dependencies between chunks of text
that appeared within the same training examples. In current pretraining schemes, this means that
dependencies between non-neighboring chunks of text are under-favored. If such dependencies matter
for the task at hand, we suggest rearranging the data into corresponding training examples.
We formalize the above notion. Our theoretical setup asks expressivity questions that pertain to the
training set rather than to a single example. We thus tie the construction of the training example with
the available expressivity of the architecture: we prove that the connections that can be modeled
between different training examples are bounded by the connections that can learned by a shallower
and weaker architecture, if these examples were inserted within the same input.
The advantage in including related text in the input of the NLM is noticed and leveraged in the
empirical landscape. With that, it is clear that showing the model related data is meaningful even if it
is in different training examples, and many leading methods elect to do just that. Our quantification
of this trade-off is intended to aid informed decisions and highlight the expressivity advantage
to be gained by smarter training example designs. We follow up on these recommendations and
demonstrate the immediately available gains to be achieved by designing training examples that
include nearest neighbors in embedding space. This method can be enhanced, and other more explicit
biases can be introduced. For example, multiple mentions of the same entity, event, or concept can be
concatenated within the same training example.
The gains achieved by using similarity in representation space indicate a path for self-improving
representations, left for future work. After a first cycle of kNN-Pretraining, the representation is
refined and applying a new kNN search over it can lead to more informative next round of kNN-
Pretraining. This way, deeper insight can be elicited from a given pretraining corpus.
Lastly, while this paper focused on leveraging the identified in-context bias for pretraining, it can also
be tied to recent successes of in-context inference methods. From the in-context few-shot prompts
of Brown et al. (2020), to in context augmentations such as in Gao et al. (2020); Schick & Schutze
(2020) and many others, the benefits of biasing the prediction by appending text in-context are now
widely established. The tools brought forth here can assist in clarifying the theoretical advantages of
such practices. Overall, our work aims to provide timely theoretical interpretations, to help guide the
rapid empirical advances of our field.
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank Or Sharir, Kevin Leyton-Brown, and Ori Ram for useful discussions. This research
was supported by the ERC (European Research Council) and the ISF (Israel Science Foundation).
Experiments were performed with Cloud TPUs and supported by Google’s TensorFlow Research
Cloud (TFRC). Yoav Levine was supported by the Israel Academy of Sciences Adams fellowship.
Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix
and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM
symposium on Theory ofcomputing, pp. 675-684, 2013.
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learn-
ing to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint
arXiv:1911.10470, 2019.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving
language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. In 5th International Conference on Learning Representations (ICLR), 2017.
Alexis Conneau and Douwe Kiela. Senteval: An evaluation toolkit for universal sentence representa-
tions. arXiv preprint arXiv:1803.05449, 2018.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. Recognizing textual entailment: Rational,
evaluation and approaches-erratum. Natural Language Engineering, 16(1):105-105, 2010.
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Gold-
ing, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang,
Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model
evaluation, September 2021a. URL https://doi.org/10.5281/zenodo.5371628.
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723, 2020.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
embeddings. arXiv preprint arXiv:2104.08821, 2021b.
Jake Gipple. The volume of n-balls. Rose-Hulman Undergraduate Mathematics Journal, 15(1):14,
2014.
SUchin Gururangan, Ana Marasovic, SWabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv
preprint arXiv:2004.10964, 2020.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-
augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.
Wolfgang Hackbusch. Tensor spaces and numerical tensor calculus, volume 42. Springer Science &
Business Media, 2012.
Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Archi-
tectures and pre-training strategies for fast and accurate multi-sentence scoring. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=SkxgnnNFvH.
Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open
domain question answering. arXiv preprint arXiv:2007.01282, 2020.
10
Published as a conference paper at ICLR 2022
Jeff Johnson, Matthijs Douze, and Herve J6gou. Billion-scale similarity search with gpus. IEEE
Transactions on Big Data, 2019.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,
2019.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics, 7:453-466, 2019.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open
domain question answering. arXiv preprint arXiv:1906.00300, 2019.
Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In
Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning,
2012.
Yoav Levine, Or Sharir, Alon Ziv, and Amnon Shashua. Benefits of depth for long-term memory of
recurrent networks. (ICLR 2018) International Conference on Learning Representations workshop,
2018.
Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. The
depth-to-width interplay in self-attention. In Advances in Neural Information Pro-
cessing Systems, 2020. URL https://papers.nips.cc/paper/2020/file/
ff4dfdf5904e920ce52b48c1cef97829- Paper.pdf.
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke
Zettlemoyer. Pre-training via paraphrasing. arXiv preprint arXiv:2006.15020, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F Canny, Pieter Abbeel, Tom Sercu, and
Alexander Rives. Msa transformer. bioRxiv, 2021.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.
arXiv preprint arXiv:1908.10084, 2019.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.
Timo Schick and Hinrich Schutze. It's not just size that matters: Small language models are also
few-shot learners. arXiv preprint arXiv:2009.07118, 2020.
David J. Smith and Mavina K. Vamanamurthy. How small is a unit ball? Mathematics Magazine, 62
(2):101-107, 1989. doi: 10.1080/0025570X.1989.11977419. URL https://doi.org/10.
1080/0025570X.1989.11977419.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006, 2020.
Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan
Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from
pre-training and fine-tuning transformers. arXiv preprint arXiv:2109.10686, 2021.
Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented sbert: Data
augmentation method for improving bi-encoders for pairwise sentence scoring tasks. arXiv preprint
arXiv:2010.08240, 2020.
11
Published as a conference paper at ICLR 2022
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing Systems,pp. 5998-6008, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:
A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint
arXiv:1804.07461, 2018.
Noam Wies, Yoav Levine, Daniel Jannai, and Amnon Shashua. Which transformer architecture
fits my data? a vocabulary bottleneck in self-attention. In Marina Meila and Tong Zhang (eds.),
Proceedings of the 38th International Conference on Machine Learning, volume 139 of Pro-
ceedings of Machine Learning Research, pp. 11170-11181. PMLR, 18-24 Jul 2021. URL
https://proceedings.mlr.press/v139/wies21a.html.
Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.
A Proof of Corollary 1
jN
Proposition 1. Let S1 = w1j	and S2
nw2jojN=1
be two sentences, giW,L,dx
the Transformer
operation of yiin,L-c,odnxtext (S1, S2) and MV be a vocabulary embedding matrix. Then:
sep-seq yiin,-Lc,odnxtext(S1,S2) ≤ sep([N],[2N]\[N]) gWi,L,dx
Proof. Assume that sep([N],[2N]\[N]) giW,L,dx	= R, then by definition there exist g1, . . . , gR :
(Rdx)N → R and g1 ,...,gR : (Rdx)N → R such that for any {xj }：：,
R
gWWL,dx (x1,..., x2N) = X gr (x1,..., XN) g!r (xN +1,..., x2N)
r=1
Now, given a, b ∈ Rdx , we can write:
R
Zyi,L,dχ(S1,S2) (a, b) =〉： gr (EMV (w1) ® a,..., EMV (WT) Θ a)
r=1
• g； (Emv (w1) Θ b,..., EMV (WN) Θ b)
Clearly, this form of presenting Zyi,L,dx (S ,S ) (a, b) is separable with respect to (a, b), and since it
has R summands, we can concluden-tcohnaext:
sep-seq yin,-c,onxtext (S1, S2) = sep(a,b) Zyiin,-Lco,ndtexxt(S1,S2) ≤ R
□
Corollary 1 now follows from an upper bound on sep([N],[2N]\[N]) gWi,L,dx given in Levine et al.
(2020).
B UPPER BOUND FOR THE ε-SEPARATION RANK
Definition 1. For an expression that can be represented as a sum of some terms,
12
Published as a conference paper at ICLR 2022
denote by f+ the corresponding sum, but with each term replaced by its absolute value, that is:
N
f+ := X |an|
n=1
and note that by the triangle inequality it holds that:
Ifl ≤ f+
Theorem 3. Let ys(epq,uie),nLtia,dl x,η be be the p ∈ [dx] entry of the analyzed sequential represen-
tation defined in eq. 5. Assume that all learned parameters and all gradients are bounded:
∀θ ∈ {W,MV} : Λmin ≤ ∣θ∣ , IdL(SI)/∂θ∣ ≤ Λmaχ for SOme 0 < Λmin ≤ Λmaχ, N < dχ,
η ∈ (0,1], 2(I+η"x < 3L, 2(1+ η) dχ < 3L, In addition, assume that there exists M ≥ 0 for which
itholdsthatZ+p,i,H,L,dx,η(S S ) < M on its domain. Then:
ysequential	(S1 ,S2 )
log hε-Seq-Sep (y(p,uenLad x,η)] = OaL+0.5log3 ㈤]∙ dX)
Proof. Denote:
ZS+)i(a,Si；n) (b) := ZypqUeHaL,dχ,η(S1,S2) (a, b)
The proof outline is as follows:
We start by finding a representation of ZΘ(S2)) g Sl .哈 as a sum of terms, where each term is separable
with respect to (a, b). We then turn to finding a subset of these terms, denoted G, such that the sum
of all terms in G is an ε-approximation of zΘ2)(a s1 ;啥.Lastly, since it follows from the definition
of the ε-separation rank and the construction of G that ε-sep(a,b) Zyp,i,H,L,dx,η(S ,S ) is upper
bounded by the cardinality of G (which is the number of summands in the approximation), we find an
upper bound to IGI, which is therefore an upper bound to ε-sep(a,b) Zyp,i,H,L,dx,η(S ,S ) as well,
which by definition is equal to ε-seq-sep yspe,qiu,Hen,tLial,dx ,η .
STEP 1 — A SEPARABLE REPRESENTATION OF Z p,i,H,L,dx,η S S
ySEQUENTIAL	(S1 ,S2)
Following Levine et al. (2020); Wies et al. (2021), Z^Sj^g Sl刁)(b) Can be written as:
需+i(a,Si；n) (b)
N
=X
j1 ,...,jC(L)=1 h∈[H][C(L)]
C(L)+1
da
X	Q(0,h)
Qr1,p
r1,...,rC(L)+1=1
∖ ∕C(L),
(13)
Y P P (c,h) Wjc)	Y /Q(c,h) Wjc
Prc	, w	Qrc+1 , w
c=1	c=1
Σ
jj	j	j
where Wj := g (b) + ηf (a)	b (this form follows from g (b) := EMV W2j	b is the entry-
wise product of b with the embedding of W2j prior to the tth training step, and f (a)j
dL(Si；a)
d(MV)W2
is the gradient update performed to W2j ’s embedding at time t), the Pr(cc,h) and Q(rcc,+h1) terms are sums
of products of the networks inner (i.e., non-embedding) weights which were also updated with respect
13
Published as a conference paper at ICLR 2022
to L (S1; a), and for convenience we denote jC(L)+1 := i and P (C(L)+1,h) := P (0,h).
N	da
= X	X	X	Q(r01,,hp)
j1,...,jC(L)=1 h∈[H][C(L)] r1,...,rC(L)+1=1
∕C(L) + 1	∖
•	( Y (Pp，,g (b)jc + ηf(ajc Q b〉J
∕C(L)	∖
•	(Y Q(rcc,+h1),g(b)jc + ηf (a)jc Qb J
Separating to vocab-gradient terms and vocab terms:
N	da
X	X	X	X	Q(r01,,hp)
j1,...,jC(L)=1 h∈[H][C (L)] r1,...,rC(L)+1=1
IP⊆[C(L)+1]
IQ⊆[C(L)]
X------V-----}
Indices involving both a and b
•	YDPr(cc,h),ηf(a)jc QbE (YDQ(rcc,+h1),ηf(a)jc QbE
c∈IP	c∈IQ
X-------------------------------------------------------------：
}
{^^^^^^^^^^^^^^
Terms involving both a and b
(Y
∖c∈[C(L) + 1]∖Ip
、
Pr(cc,h),g(b)jc	Y	Q(rcc ,+h1),g(b)jc
)∖c∈[C(L)]∖Iq	)
_______________________ - /
{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Terms involving just b
Opening to indices:
N	da
= X	X X	X	Q(r01,,hp)
IP ⊆[C(L)+1] j1,...,jC(L)=1 h∈[H][C(L)] r1,...,rC (L)+1 =1
IQ ⊆[C(L)]
Xdx	Y	Pr(cc,,αhc)ηf(a)jαcc	bαc	!(Y	Q(rcc,+h1),βc	ηf (a)jβcc	bβc
α1,...,αC(L)+1	c∈IP	c∈IQ
β1,...,βC(L) =1
( Y	Pr(cc,,αhc) g (b)jαcc J( Y	Q(rcc,+h1),βc g (b)jβcc
∖c∈[C(L) + 1]∖Ip	) V∈[C(L)]∖IQ
Separating to weights and variables:
dx	N
τα1,...,βC(L)
α1,...,αC(L)+1=1	IP ⊆[C(L)+1] j1,...,jC(L)=1
β1,...,βC(L) =1 IQ ⊆[C(L)]
ηf (a)jαcc bαc	(	ηf (a)jβcc bβc
c∈IP	c∈IQ
g (b)jαcc J(	g (b)jβcc
c∈[C(L) + 1]∖Ip	C	∖c∈[C(L)]∖Iq
14
Published as a conference paper at ICLR 2022
where:
Ta1,...,βc(L) := E
da
X	Q*)
h∈[H]Q(L)I r1,...,rC(L) + 1 = 1
∕C(L) + 1	∖ (C(D
Y	Peh)	Y Q(CM
I U	PrCQC 111 1 QrC+ι,βc
Compressing summation to count variable powers:
2C(L) + 1
X	X
NA=O	P1+----+pdx =NA
XV/	n1+…+ndx =2C(LHI-NA
Total power of f (a)、	一〜一	J
How many indices
are equal to each α∈[dχ]
Σ	Σ
Z1+--HZN = NA
m1 H——HmN = 2C(L) + 1-Na
∀j∈[N]∖{i}:	Zj +mj≡0 mod 2
Zi+mi≡1 mod 2
How many indices
are equal to each j∈[N]
O≤P1,1,…,Pdx,N ≤Na
Vα∈[dχ] Pj=1 pα,j=pα
Vj∈ [N] Pαx=1 pα,j = zj
S----------V-----------'
How to distribute the powers of f (a)
Σ
λNA,p,n
O≤n1,1,...,ndx,N ≤2C(L) + 1-Na
∀ɑ∈[dx] PPj = 1 nα,j =na
∀j∈[N] Pa=I nα,j=mj
'---------------------V---------------------'
How to distribute the powers of g(b)
Ndx	∖ / N dχ
∏∏(ηf(a)ab.)pα[ (∏∏(g(b)a)n'
j=1α=1	j ∖j=1α=1
where:
λNA,p,n
Σ
Ip ⊆[C(L) + 1]
Iq⊆[c(l)]
∣ip ∣+∣iq∣=na
dx
X
Ta1,…,βC(L)
α1,…,αC(L) + 1
β1 ,…,βc(L) =1
Vκ∈[dx] ∣{c∈Ip∣αc = κ }∣ + ∣{c∈IQβc = κ }∣=pκ
κ∈[dx] ∣{c∈[C(L)+1]∖Ip∣ɑc = κ } | +1 {c∈[C(L)]∖Iq | βc = κ } | =nκ
15
Published as a conference paper at ICLR 2022
Pushing in summations on N, only the parity matters:
2C(L) + 1
X产	X
Na=0	pι+--+Pdx =NA
n1 +-+ndχ =2C(L)+1 —NA
vɔ N
j=1 = l
Σ
e∈{0,l}N
EN=I ej ≡Na mod 2
ej ≤m∖n{NA,2C(L) — Na+2ei}
λNA,p,n
X~^
Network’s weights, function of a
/ ∖
N	dχ
X ∏∏(f(a)α 厂
0≤Pι,ι,…,Pdx,N ≤Na j=1 a=l
Z1+--HZN =NA
∀α∈ [dχ] PP j=ι pα,j =pα
∀j∈[N] Pa=I Pα,j = Zj
∖ ∀j∈[N] Zj≡ej	mod 2	/
----------------------V-------------------'
= *∙φA,p,e, function of a
/	∖
/ dx
• ∏ CMPa
∖ɑ=1
Y
N dχ
X	∏∏(g (b)a)nαj
0≤n1,1,…,ndχ ,N ≤2C(L)+ 1 —NA j=1 α=1
mι+——HmN=2C(L) + 1-Na
Vα∈[dχ] PPj = 1 na,j=na
∀j∈[N] Pa=I na,j=mj
Vj∈ [N]∖{i} mj ≡ej	mod 2
∖	mi≡(1-ei)	mod 2	/
_ - /
∖^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
= ：^B,p,n,e,function of b
2C(L) + 1
X ηNA	X
NA=0	P1H--HPdX =Na
n1 +-HndX =2 C(L)+1 一NA
PN
j=1
Σ
λNA,p,n
• φA,p,e • ψB,p,n,e
e∈{0,1}N
PN=I ej≡Na mod 2
ej≤min{NA,2C(L)-NA+2ei}
where each summand is separable with respect to (a, b).
STEP 2 ——AN ^-APPROXIMATION OF Z p,i,H,L,dx,η« Cf、
USe½UenTiaL x, (S1,S2)
Now that We have a representation of Zyp,i,H,L,dx,η(^ɪ 52)
turn to finding a subsum that can approximate Z p,i,H,L,dχ,η
ysequential
as a sum of (a, b)-separable terms, We
(Sl s2 ) up to an ε-precision.
First, let us define the set of all legal indices in the last sum:
D = D2C(L) + 1,dx,N := < (NA,p,n,e)∈N2dχ +N+1
0≤Na≤2C(L) + 1
∑1=1 Pa=NA
∑1=1 nα=2C(L)+1-Na
Vj∈[N], ej∈{0,1}
PN=I ej≡Na mod 2
PN=I ej≤min{NA,2C(L)-NA+ 2ei}
16
Published as a conference paper at ICLR 2022
And for G ⊆ D, denote:
ZG (a, b) :=	〉:	η AλNA,p,n ∙ φA,p,e ∙ ψB,p,n,e
(NA,p,n,e)∈G
which is the sum of all terms with indices in G. Clearly, summing over all possible indices gives us
the original expression:
ZD (a, b) = Z p,i,H,L,dx,η
ysequential
(a, b)
(S1,S2)
Given ε > 0, we wish to find a subset of the indices, G ⊆ D, such that the sum of all terms whose
indices are in G is an ε-approximation of Zyp,i,H,L,dx,η(S ,S )
G ⊆ D such that for all a, b:
(a, b). That is, we are looking for
|ZD (a, b) - ZG (a, b)| =
ZD\G (a, b) ≤ ε
Note that since we assume that ZD+ (a, b) = Z+p,i,H,L,dx,η	< M, we can get:
ysequential	(S1 ,S2 )
I 7	( N IZD∖G (a, b)| 7+，k、 ZD\G (a, b) . ʃ
IZD∖G (a, b)I = Z+ (a, b) ZG (a, b) ≤ Z+ (a, b) M
and it follows that it is enough for us to show that:
Z+\G (a,b) V ε
Z+ (a, b) ≤ M
which is equivalent showing that:
ZD\G (a,b) ≤ 三	(14)
ZG (a, b)	— M
under the assumption:
∂L (S )
∀θ ∈ Θ	θ,-----而」，f (a)α , bα,g (b)α ∈ [Λmin, Λmaχ]	(15)
∂θ
which we make going forward.
Zyspe,qiu,eHntia,lL,dx,η(S1,S2)
(a, b).
This will ensure that ZG is an ε-approximation of
Now, We assume that ∀θ ∈ Θ : θ, - dL∂S1) ∈ [Am®, Λmaχ], and by Levine et al. (2020), the PS and
Qs in eq. 13 are products ofup to L matrices, so each of their coordinates is bounded in ΛLmin, ΛLmax
and we assume without loss of generality that Λmin ≤ 1 ≤ Λmax (otherwise we could have picked a
smaller Λmin and a larger Λmax). Then for each (NA, p, n, e) ∈ D the following inequalities hold:
τα1 ,...,βC(L) ≤
da
X	X Λmax
h∈[H][C(L)] r1,...,rC(L)+1=1
C(L)+1
Λmax
c=1
C(L)
YΛ
c=1
dxC(L)daΛ
2C(L)+2
max
λNA,p,n
dx
≤ X	X	dxC(L)daΛ2mCa(xL)+2
IP ⊆[C (L)+1]	α1,...,αC(L)+1 1
IQ⊆[C(L)]	β1,...,βC(L) =1
|Ip ∣ + ∣Iq∣=NA	∀κ∈[dχ] ∣{c∈Ip ∣αc = κ }| + ∣{c∈Iq | βc = κ }∣=Pκ
κ∈[dx] l{c∈[c(L)+1]∖IP lac = κ }l + l{c∈[c(L)]∖1Qlβc = κ }l=nκ
2C (L) + 1	NA	2C (L) + 1 - NAdC(L)d Λ2C(L)+2
NA	p1,...,pdx	n1,...,ndx	dx	daΛmax
17
Published as a conference paper at ICLR 2022
N dχ
φA,pe ≤	X	∏∏ Λmaχ
O≤P1,1,…,Pdx,N ≤Na j=1 a=l
zι H----HZN=NA
∀α∈ [dχ] Pj=ι pα,j=pα
∀j∈[N] Pa=I Pα,j = Zj
∀j∈[N] Zj≡ej mod 2
/ ∖
E ι
0≤P1,1,…,Pdχ,N ≤nA
zιH----HZN=NA
Vα∈[dχ] Pj=1 Paj=Pa
,∀j∈[N] Pα=1 Pa,j =Zj	,
∖ Vj∈[N ] Zj ≡ e j	mod 2 /
ΛNA
max
/ dæ	∖	Ndx
ΨB,p,n,e ≤	∏ Λmaχ	X	∏ ∏ Λm
∖α=1	) 0≤n1,1 ,…,ndx,N ≤2C(L)+1-NA j = 1 α=1
m1H——HmN = 2C(L) + 1-Na
∀ɑ∈[dx] spj=1 na,j =na
∀j∈[N] Pa=I na,j =mj
Vj∈[N]∖{i} mj≡ej mod 2
mi≡(1-ei) mod 2
/ ∖
E ι
O≤n1,1,...,ndx,N ≤2C(L) + 1-Na
m1H——HmN = 2C(L) + 1-Na
∀ɑ∈[dx] PP j = 1 na,j =na
∀j∈[N] Pa=I na,j=mj
Vj∈[N]∖{i} mj≡ej	mod 2
∖	mi≡(1-ei)	mod 2	)
λ2C(L)H1
max
and therefore:
λNA,p,n ∙ φA,p,e , ψB ,p,n,e
/	∖
/ ∖
≤ E 1
0≤P1,1,…,Pdx,N ≤nA
Z1H-----HZN = NA
∀α∈[dx] Pj=1 Pa,j =Pa
V Vj∈[N ] Pa=1 Paj =Zj	,
∖ Vj∈ [N] Zj ≡ej mod 2	/
E ι
O≤n1,1,...,nd>N ≤2C(L)H1-Na
m1 H----HmN=2C (L)H1-Na
va∈ [dx ] PP j = 1 na,j =na
Vj∈[N] Pa=I na,j=mj
Vj∈[N]∖{i} mj≡ej	mod 2
∖	mi≡(1-ei)	mod 2	)
∕2C (L) + 1W	Na	λ (2C (L) + 1 - NA
k	NA	P ∖P1,.. .,Pdx) V n1,...,ndx
dC(L)d ΛL(2C(L) + 2) + 2C(L) + 1+NA
Xχ	tia 八 max
18
Published as a conference paper at ICLR 2022
Relaxing the parity constraints inside the brackets and recalling that Λmax ≥ 1 gives us an upper
bound:
∕2C (L) + 1W	NA	λ ∕2C (L) + 1 - NA
k	NA	J∖pι,...,pdxJ∖	nι,...,ndx
dC(L')daΛ
(L+2)(2C(L) + 2)
max
which We can further bound using lemmas 3 and 4 in Levine et al. (2020) until We are left with:
< ( e (2dχN + 2C(L) + 1) YdxN
dxN
(2。(L)+ 1)(	NA	)(2。(L)+ 1 - NA
k	NA	八pι,∙∙∙,pdj∖ n1,...,ndχ
dC(L)d0.Λ
(L+2)(2C(L) + 2)
max
On the other hand:
λNA,p,n ∙ φA,p,e , ψB,p,n,e
≥ (2C(L) + 1](	NA	W2C (L) + 1 - NA
N V	NA	八pι,∙∙∙,pdj∖ n1,...,ndχ
E 1
0≤P1,1,…,Pdx,N ≤Na
Zl+--HZN = NA
∀α∈[dx] pj=ι pα,j=pα
Cj∈[N] Pa=I Paj =Zj ,
∖ ∀j∈[N ] Zj ≡ e j mod 2 /
/ \
E 1
0≤n1,1 ,…,ndx,N ≤2 C(L)+1—NA
mιH——HmN = 2C(L) + 1-Na
Vɑ∈[dx] ^Pj=I na,j =na
∀j∈[N] Pa=I na,j =mj
Vj∈[N]∖{i} mj≡ej	mod 2
∖	mi≡(1-ei) mod 2	/
λ2C(L) + 1
八min
∕2C (L) + 1W Na λ [2。(L) + 1 - Na]
k NA 八pι,∙∙∙,pdj ∖ n1,...,ndx )
dC(L)d0,Λ
L(2C(L) + 2) + 2C(L) + 1+Na
min
22C (L) + 1W	Na	] [2。(L) + 1 - NA
k	NA	八pι,∙∙∙,pdj∖	n1,...,ndx
dC(L)d0,Λ
(L+2)(2C(L) + 2)
min
Combining the upper and lower bound for λNA,p,n ∙ ΦA,p,e ∙ ΨB,p,n,e, we get that for all G ⊆ D:
Zd∖g (a, b)
ZG (a, b)
< (e (2dχN + 2C(L) + 1) ∖2dxN (心 YL+2)(2C(L)+2)
一〈	dXN	)	∙ VΛ~J
P	ηNA (2C(L) + 1∖(	Na	∖(2C(L) + 1-Na∖
乙(NA,p,n,e)∈D∖g / V	Na	八P1,…,PdJ1 n1,…,ndx )
,---------------------：—：--------------：—：--------
P	ηNA (2C(L) + 1∖(	Na	∖(2C(L) + 1-Na∖
乙(NA,p,n,e)∈G /	∖	Na	八P1,…,Pdx 八 九1,…，ndx )
- dX5V…
/
≥
≥
\
19
Published as a conference paper at ICLR 2022
so in order to show that (14) holds, it suffices to show that:
P	InNA (2C(L) + 1)( Na )(2C(L) + 1-NA)
乙(NA,P,n,e)∈D∖G / ∖ NA 八Pl,…,PdJ( nι,---,ndx )
P(NA,p,n,e)∈G 〃NA CCNA)
dxN
e (2dxN + 2C (L) + 1)
)(	Na	)(2C(L) + 1-Na)
八 pι,...,pdj' nι,…,ndx )
∖ 2dχN	λ Amin、5+2)(2。(工)+2)	ɛ
J •(小J	^ M
Now, We can limit ourselves to subsets G ⊆ D of the form:
G(T )= {(NA,P,n,e)∈D InNA (2CNA+ 1 )J ,?%)(号1L)*NA)”}
and in this case we get that:
P	IINA (2C(L) + 1)(	Na	)(2C(L) + 1-Na)
乙(NA,p,n,e)∈D∖G(T )〃	∖	Na	)∖pι,...,pdj' nι,---,ndx )
P	NA (2C(L) +1) ( Na )(2C(L) + 1-NA)
乙(NA,p,n,e)∈G(T )〃 V NA 八 P1,…,Pdx 八 nι,...,ndx)
≤ E(NA,p,n,e)∈D∖G(T) T
E(NA,p,n,e)∈G(T) T
=|D|-|G (T )|
=|G (T )|
Let us define:
〜
D-
∣2C(L) + 1,dχ := {
G(T )：= {(
(NA,p,n)∈N2dx + 1
0≤Na≤2C(L) + 1	)
Pa=I Pa=NA	z
Pa=I nα = 2C(L) + 1-NA J
(Na ,P,n)∈DInNA(2CNA + 1)(pι,NApdJ(2n(IL)+n-NA)≥t}
and note that:
G(T) ≤ |G(T)| ≤ 2n ∙ IG(T)
D ≤ |D| ≤ 2n ∙ ∣D
and also:
2dx
2C (L) +1))
≤ ( e (2dx + 2C(L) + 1) λ 2dχ
where the inequality is due to lemma 3 in Levine et al. (2020).
Hence:
|D|-|G(T)| ≤ 乜
∣G (T)|	≤
e(2dχ + 2C(L) + 1)
∖2dχ	I ~ I
)-∣ G(T) I
~ ,.
G(T)
And after rearranging we get that for each T ≥ 0 such that:
G(T) ∣ ≥ --
1+
2n ∙ (e (2dx + 2C (L) + 1))2dx
dχN
e(2dχN+2C(L) + 1)
2dxN	/	∖(L+2)(2C(L) + 2)	∖
∙ 4	∙ M	dxdx
∖ 八max /	lyi /
〜
D
≤
〜
〜
D
In order for ZG(T) (a, b) to be an ε-approximation of Z p,i,∏,L,dx,η,s S ʌ (a, b).
' y	ysequential	(Sl ,S2)
(16)
20
Published as a conference paper at ICLR 2022
STEP 3 — AN UPPER BOUND TO ε-SEP(a b) Z p,i,H,L,dx,η
,	ySEQUENTIAL	(S1 ,S2 )
In the last step we have found a condition on subsets of indices, such that summing over
any subset who meets this condition will yield an ε-approximation of Z p,i,H,L,dx,η
ysequential
We will now find a specific subset who meets this condition, and use it in order to
ε-sep(a,b) Zyspe,qiu,enHtia,lL,dx,η(S1,S2) from above.
We will focus our attention on T s of the form:
(S1,S2).
bound
η(2C(L) + 1) ∕2C (L) + 1
T (S) = S • η 1+η I η(2C(L)+1)
'	ι+η
/	η(2C(L) + 1)
2C(L) + 1
1+η	1 /	1+η
η(2C(L) + 1)	η(2C(L) + 1)	( 2C(L) + 1	2C(L) + 1
(i+η)dχ ,...,	(1+η)dχ
(i+η)dχ ,..., (i+η)dχ
for some s ∈ 0, e-1.5 which we’ll determine later.
By lemma 9 we have that:
dx -1
G(T(s))∣≥ ɪ (∏⅛U)丁
dx π	2d2x (1 + η)
dx -1
(ln (ST))F
so we can choose:
s*=exp
22N (e(2dχ + 2C(L) + 1))4dχπ(2(1 + η))dχ-1
1
dx - 1
I U1+(
dχ N
e(2dχN + 2C(L) + 1)
)2dxN ( ΛmX )(L+ 2)(2C(L)+ 2) M !2dXdx(∏e(2C(L) + 1))dχ-1
/

、
/
and since this upholds s* < e-1∙5, We get that (16) indeed holds for G (T (s*)), and therefore that
ZG(T(s*)) is an ε-approximation of ZyP,i,H,L,dχ,η但】,?).
Now, from lemma 8 we get:
∣G(T (s5 2f¾a •( T 厂
(dx - 1)	(dx - 1)
1+
and therefore:
22n ∙ (e (2dχ + 2C(L) + 1))4dx
dxN
e(2dχN +2C(L) + 1)
2dχN	(N	、(L+2)(2C(L) + 2)	∖ 2	„ ,
•	Λmax	•备∙ dxdx
p,i,H,L,dx,η
ε-seq-sep ysequential
ε-sep(a,b) Zyspe,qiu,eHntia,lL,dx,η(S1,S2)
|G (T (s* ))1
2n ∙ ∣GG(T (s*))
2(2C(L) + 1) J 25√η )dx-1
(dx - 1)2	∖(dχ - 1)/
23N • (e (2dx + 2C (L) + 1))4dx
1+
dxN
e(2dχN+2C(L) + 1)
2dχN	∖(L+2)(2C(L)+2)	\ 2
• (M)	•帚• dxdx
≤
≤
≤
□
21
Published as a conference paper at ICLR 2022
B.1 Lemmas for estimating the number of coefficients
Remark 1. For brevity and clarity we will use expressions of the form (K K K), regardless of
'm，…，M
whether K is divisible by M or not. For the latter case, this expression should actually be:
K
,∙.∙,[ M _l, l M l +1,..., L M l + U
X-----------------V----------------}
(K mod M) times
expressions oftheform (K KK十)should be read as:
Lemma 1. For all K, M ∈ N, the maximal value of a
holds that |aj1 - aj2 | ≤ 1.
K..,a	is achieved when ∀j1 , j2 ∈ [M] it
Proof. Let a1 , . . . , aM be a sequence of non-negative integers such that a1 + . . . + aM = K
and K is maximal. Assume towards a contradiction there exist j1 , j2 ∈ [M] such that
a1,...,aM
ajι - aj2 > 1 ^⇒ aj2 + 1 < 1, then:
K
a1 , . . . , aM
K!
K!
(Qi∈[M]∖{j1,j2} ai!) ∙ (aji -I) ! Yaj2 + I)!
K!
aj2 + 1
ajι
(Qi∈[M ]∖{j1,j2} ai!) ∙ (aji -I)! Yaj2 + 1)!
= K
a1 , . . . , aj1 - 1, . . . , aj2 + 1, . . . , aM
in contrary to the maximality of a,.K..,a	. Therefore, ∀j1,j2 ∈ [M], |aj1 - aj2 | ≤ 1.
Lemma 2. Let K, M be two fixed natural numbers, η ∈ (0, 1] and denote
□
S(n) = (K)ηn(2 n 旦
∖ n /	∖ M ,..., M
—
Then for all n ∈ [K] ∪ {0}:
bMMMc ɪ /
S (n)= Y
j=0
-1
η(K - jM) AM
(j + 1) M )
η(K -3 M)
(L M ∖ + 1)M
n mod M
K
K	K
M,..., M
(17)
22
Published as a conference paper at ICLR 2022
Proof. We will prove by induction.
•	Base case: n = 0.
S (0) =
∖尸。
(η (K -jM)\M
I (j + 1) M )
(η (K -[M∙1M) !0 mod Ml κ ∖
I (LMM_l+1)M J	∖M^,…,M)
where the second equality is due to the fact that:
∖尸0
(η (K -jM)\M
I (j + 1) M )
1
as an empty product, and:
(η (K - 0 ∙ M) Y mod M
V 1 ∙ M )
Therefore, (17) is true for n = 0.
•	Induction step: Let n ≥ 0 such that (17) holds for n.
S (n +1)=(n：1)JY +
η (K - n)
(M∙ + 1)M
-S (n)
η (K - n)
η (K -jM八 M
Λ知T /
∏	(
∖ j=0	∖
(M∙ + 1)M
(j +1) M
η(κ -⅛d M)
(L M ∖ +1)M
n mod M
Λ 陪 C-i∕
∏	(
∖ j=0	\
η (K -jM八 M
(j + 1) M
K
η (K -[ nM」M)
(L nM J +1)M
(n+1) mod M
K
M , . .. , M
Thus, (17) holds for n + 1, and the proof of the induction step is complete. Hence, by
induction, (17) is correct for all n ∈ [K] U {0}.
□
Lemma 3. Let K, M be two fixed natural numbers, and η ∈ (0,1]. Then the maximum of:
23
Published as a conference paper at ICLR 2022
is achieved when:
([MKι+MjI) m ` 舟
n
and forall i ∈ [M] Jai —寻 ≤ 1 and Ibi — Kj-n ∣ ≤ L
Proof. From lemma 1 we know that a multinumial coefficient reaches its maximum when the sum
is evenly distributed between all indices. Since the ai s and bi can be chosen independently of each
other given K and n, we may assume without loss of generality that no matter the value of n, for all
i ∈ [M], it holds that ∣ai — Mn∣ ≤ 1 and ∣bi — K-n∣ ≤ 1.
Define S as in lemma 2.
Note that the function:
is Gaussian-shaped and therefore unimodal and has a unique maximum. ηx for η ∈(0, 1] is
monotonically decreasing, and therefore their product is also unimodal.
Since we are only interested in solutions for n ∈ N, we can reduce our problem to finding the
maximal n such that:
S (n — 1) ≤ S (n) o
』≥ 1
and from lemma 2 we have:
S (n)	= η (K - [nM11M) ≥ 1
S(n — 1)	([nM1 ∖ +1) M -
^⇒
n 一 1 m ≤ ηκ — M
M _	1+ η
So We get that S (n) is monotonically increasing as long as [n-ɪ1 M ≤ ηK+-j, and the largest
integer for which this condition holds is:
n
ηK — M
M (1 + η)
+1 M'
ηK
1 + η
□
Lemma 4. Denote by N BRd the number of integer lattice points in BRd (the d-dimensional zero-
centered ball of radius R). Then:
d
d
Vznd
∏ πe∖ 2
≤N(BR) ≤i√⅛
πd
+1d
Proof. Let I BRd be the set of all integer lattice points in BRd . For x ∈ I BRd , define:
Cx
d	11
×i=1 Xi — x,xi + 大
y ∈ Rd ∀i ∈ [d] ,xi — 2 ≤ y ≤ Xi + 2
Let y ∈ Sx∈I(Bd ) Cx, so there exists x ∈ I BRd such that y ∈ Cx, and from the triangle
inequality We get:
and therefore y ∈ B
d
R+ √2d
kyk ≤ kxk + ky—xk ≤ R+
Since y Was chosen arbitrarily, We get that:
Cx ⊆
x∈I(BdR)
√d
24
Published as a conference paper at ICLR 2022
Note that Vol (Cx) = 1 for all x ∈ I (BR) and that for x, x! ∈I (BR) such that x = x0, Cx ∩ Cχ
is a set of measure zero, hence:
N(Bd) = ∣I (Bd)∣
/
Vol
< Vol
d
π 2
r (d + 1)
R+√2d !d
Assume for convenience that d mod 2 ≡ 0, so Γ (d + 1) = (d)!, and Stirling,s approximation
yields:
d
π 2
Vnd ∙ ( .de )
On the other hand, note that Bd 亭
小+√2d !d=与
+ 1Y
⊆ Ux∈i(b⅛) Cχ, and therefore:
N (BR) = Vol
f
U	Cx
∖x∈z(BR)
Lemma 5. Let K, M be two fixed natural numbers, S ∈ (0,1) a constant sensitivity parameter, and
let”
Tk,m ：= a ∈ NM
Qi,..., QM
K
≥ S * I K_	K_
∖M,..., M
Then:
{α ∈ NM
aT1 = K,
a - K∙1
M
≤ K ln (ST)}
⊆ Tk,m
⊆ {a ∈ NM
aT1 = K,
. 1	≤ 4Kln (ST)
K
2
□
K
a - M
2
)
Proof. By Stirling,s approximation, it holds that:
and a slightly more accurate version is:
x! ` j2π (X+1) ∙( e )x
25
Published as a conference paper at ICLR 2022
and by plugging this approximation to the definition of a multinomial coefficient we get after some
rearranging:
(K ]`(2∏) 1-2M∙ ^KΞL.—^M_____________,
Vι,...,aM7	QM=I Jai + 6 exp (PMI a ln (M a,)
Note that using this approximation and some rearrangements, we get that the following are equivalent:
QKJ ≥ S ∙( K K K)
∖a1, . . . , aM)	∖ M , . . . , M )
1-M
^⇒ (2π) -ɪ
Kκ + 6	M K
QMd Jai + 6 exp (PMI ai ln (Ma，)
^⇒
M K
exp (PM=1 M ln (M ∙ K))
1-M
≥ s∙(2π) 2
M
Y
i=1
(18)
(19)
so we can characterize the set:
TK,M := {a ∈ NM ∣aτ1 = K,(QM=ι √ai+⅛ )∙exp(PM=ι ailn( K ɑ≈))≤s-1∙( K +1)粤}
as TK,M ' TK,M.
We’ll start by finding a condition that will assure us that (a1, . . . , aM) ∈ TK,M (i.e., we will
characterize a subset of TK,M).
First, note that by the AM-GM inequality,
and therefore:
PM1 (ai + 1) = K + 6 ∙M
M = M
K1
M1
Y 卜i + 6)≤
i=1
M
Y
i=1
i=1
Since we’re interested in a subset of TK,M, we can show that the last inequality holds when we
replace the first term in the left-hand side (QMi Jai + 1) with (M + 61) 2 (if the new inequality
holds, (19) must hold as well), and we’re left with:
M
K 1∖ ɪ
M + 6)	∙ exp
xx ai ln (Kai) ≤ ln (S
i=1
s-1
M
(M+6)
(20)
M
t
M
t
M
M + 6
M
^⇒
Now, for i ∈ [M], let hi := Mai - 1, so:
M M KM
Nai In(Mai)= M 工(1 + hi)ln(1 + hi)
and the last inequality becomes:
KM
M ΣS (1 + hi) ln (I + hi) ≤ ln (S-I)
M i=1
MM
^⇒ ɪ2 (I + hi)ln (I + hi) ≤ K ln (S )
i=1
26
Published as a conference paper at ICLR 2022
Observe that for all x ≥ -1, it holds that:
ln (1 + x) ≤ x
and therefore:
(1 + x) ln (1 + x) ≤ x2 + x
so it suffices (again, we’re only interested in a subset of TK,M) to show that:
MM
X (hi + hi) ≤ K ln (s- )
i=1
(21)
Now, observe that:
X hi = X (Kai- 1
i=1	i=1
0
so (21) becomes:
MM
X h2 ≤ Mln (s-1)
i=1
M	K 2 K	-1
O X Qi- M) ≤ M ln (S)
i=1
and we get that:
{a ∈ NMaT~ = k, a - M ∙ ~	≤ M ln (s-1) } ⊆ TK,M
Let us now turn to finding a condition that will assure us that (a1, . . . , aM) ∈/ TK,M (i.e., we will
characterize a subset of the complement of TK,M). Note that:
MX i=1 MX i=
pp
ex ex
so if (a1, . . . , aM) ∈/ TK,M, we must have that:
K lʌ MM
M + 6)	∙exp
ai ln (MaA +1]n(6Mai + M
i KK + + 2 V 6K + M
-1	(K 1、MM
>s IM + 6)
M
Q⇒): ai ln
i=1
M 1	6Mai + M
Kai) + 2ln(-6K⅛M-) > ln (S )
27
Published as a conference paper at ICLR 2022
and using the same definition of hi as before, we get:
M
ai ln
i=1
M
=X
i=1
M
=X
i=1
M 1	6Mai + M
KaiJ +2lnl 6K + M
K (1 + hi)ln(1 + hi) + 1ln(6K (1 +hi)+ M
M ( + i)	( + i)+2 k	6K + M
K
M (I + hi) ln (I + hi)
+1	pκ(1+hi)+M]-(K +	3κ	hh
+ 2l V	6K + M )	(M + 6K + M 广
where the last equality is due to the fact that PiM=1 hi = 0.
Observing the first order Taylor polynomial of the function:
K	1	6K(1 +x) +M K	3K
f (X)= M (1+ x)ln(1+ x) + 2ln(	6K + M )- (M +6K+m xx
at x = 0 with the remainder in the Lagrange form, we get:
f (hi) = (K + 6KKM) hi+ f00 (ξi) ∙ hi - (K + 6KKM) hi
h2 * *
=f00 (ξi) ∙ h
_ K K	18K2	! hi2
=IM (1+ ξi) - (6K (1 + ξi) + M)2) 工
for some ξi between 0 and hi .
(22)
Note that f00 (ξi) is monotonically decreasing with ξi for ξi ∈ (-1, M - 1], and using this fact and
the fact that K ≥ 1, (22) is lower bounded by:
旦. h2
4M2 i
So we can limit ourselves to looking at the cases where:
MK
X 4M2 ∙ h2 > ln (S-1)
i=1
M
0 X (ai
i=1
2
>
—
K
M
4K ln (ST)
and we get that:
a ∈ NM
aT ~1 = K,
K
a———
M
• ~
> 4Kln (ST) }
⊆ a∈NMaT~1
^⇒ Tk,m ⊆ {a ∈ NM
Ko \TK,M
aT ~1 = K,
K
a - M
• 1	≤ 4K ln (ST)}
Combining the two results together we get:
a ∈ NM
aT ~1 = K,
• ~1
≤ K ln (ST)}
⊆ TK,M
⊆ a ∈ NM
aT ~1 = K,
a - K ∙1
M
≤ 4Kln (ST) }
K
a - M
2
28
Published as a conference paper at ICLR 2022
□
Lemma 6. Let K, M be two fixed natural numbers, and s ∈ (0, 1) a constant sensitivity parameter.
Then the number of multinomial coefficients, a ,.K..,a , which uphold:
KK
aι,. .., aM J Vm,. .., M
is upper bounded by:
M-1
(Πe) F
/ M--- ∖ M -1
OrKMRi)
and lower bounded by:
M-1
(Πe) F
M √∏
- Z--- ∖ M-1
2PKIP - l)
Proof. Let (ai,.KaM)be a multinomial COeffiCient for which it holds that:
KK
a1, . . . , aM J ∖M, . . ., M
and denote:
TK,M :	= a∈NM		a1 , .	. . , aM		≥ S ∙	(K,..		.,K/ ʃ	
L TK,M :	=(a	∈ NM	aT~1=	K,	a	K -M	~	2 ≤	M ln (s-	1))
U TK,M :	=(a	∈ NM	aT~1=	K,	a	K -M	~	2 ≤	4K ln (S-	1))
LU
TK,M ⊆ TK,M ⊆ TK,M
then by lemma 5,
and therefore:
TKL,M ≤ |TK,M | ≤ TKU,M
so in order to bound the cardinality of TK,M (which is the quantity we are interested in), we can find
an upper bound on the cardinality of TKU,M and a lower bound on the cardinality of TKL,M.
LetB ∈ {L, U} anda ∈ TKB,M , and denote:
R (B) = (，K ln (ST)	if B = L
'	[p4K ln(s-1) if B = U
For i ∈ [M], denote Xi := a - K. So the problem has changed to finding the number of integer
M -tuples x1 , . . . , xM suCh that PiM=1 xi = 0 and kxk ≤ R (B), whiCh is the number of integer
lattiCe points x in the zero-Centered M -dimensional ball of radius R (B) that uphold PiM=1 xi = 0.
Note that the interseCtion of a d-dimensional ball of radius R with the hyperplane H =
y ∈ Rd y, ~1 = 0 is a (d - 1)-dimensional ball, so we Can assume that the number of in-
teger lattiCe points in the zero-Centered d-dimensional ball of radius R whose Coordinates add-up to
zero is 〜 册∙N (Bd-1), where 我 is the cosine of the angle between H and one of the axis-aligned
(d - 1)-dimensional hyperplanes in Rd. In our Case, R = R (B) and d = M, so by lemma 4, the
29
Published as a conference paper at ICLR 2022
cardinality of TKM is upper bounded by:
/ 、M T	/ Z_______ ∖ M-1
ɪ ∙N (βM-1_ʌ ‹ ɪ ∙(笏)2	22√4KIn(s-1)+1!
√M	V √4K In(ST)/ - √M v∕π (M - 1)〈	√M - 1	)
M-1	M —1
<(节)2	h/KIn(FTM
-(M - 1) √π ʊ M - 1	+ )
and the cardinality of TKM is lower bounded by:
1	N (BM - 1 ʌ ≥ ɪ (喻 MfI ( 2JK In(ST) - 1∖M 1
√M k √K ln(s-1)J √ √M √∏ (M - 1) ∖	√M~-1
M-1
≥ (贷)丁
M	M √π
, t__________ ∖ M — 1
2 √k In(ST) - Λ
M
□
Lemma 7. Let K be a fixed natural number, η ∈ (0,1], and S ∈ (0,1) a constant sensitivity
parameter.Then number ofinteger ns such that (K)ηn ≥ S ∙ (K)η中 is upper bounded by:
K J(1 + 2ln(s))2 (1 + η)2 — 4η
2(1+ η)(ln(s-1)-1)
Proof. Denote n := 1K + x, so:
ηn ≥ s ∙
K K∖ ηκ
ηK η1 + n
×1+n∕
O G/+ x)ηx ≥ S (K
1+η	1+η
Recall that by Stirling,s approximation We know that:
K
2πn (K — n)
So the number xs which uphold:
K K
n (K - n)K-n
KK
K
K
1 + η
—ηx
-x
^⇒
≥ S ∙
一 t
2∏造
K K
ηK
ηK、1+n
1+η )
备-x+2
η
(23)
-x
approximates the number we are trying to quantify.
30
Published as a conference paper at ICLR 2022
After some rearranging, one can observe that (23) is equivalent to:
ln (ST) ≥
+X+"、
-X +2)ln(l-?
(24)

and since for all t > -1, ɪ^ ≤ ln(1+1), the number of Xs which uphold (24) is upper bounded by
the number of integer Xs for which it holds that:
ln (ST)
(1 - η2) Kx - 2(1 + η)2 x2
2 2ηK2 + 2(1 — η2) Kx — 2(1 + η)2 x2
Q⇒ 2(1+ η)2 (ln (ST) - 1)x2
+(1 — 2ln (S-I))(1 — η2) Kx — 2ln (S-I) ηK2 ≤ 0
(25)
Recall that for inequalities of the form ax2 + bx + c ≤ 0 where a > 0, the set of all values of x
which satisfy this inequality is (-b-√b2-4ac, -b+√b2-4ac) and the number of integer values of X
which satisfy this condition is approximately Vb2-4ac (the interval,s length).
In our case, (25) gives us:
a = 2(1+ η)2 (ln (ST) - 1)
b = (1 - 2ln (ST))(I - η2) K
C = -2ln (ST) ηK2
and therefore:
√b2 - 4ac _ K，(2In(ST) - I)2 (1 + η) - 4η
a~ =	2(1+ η) (ln(S-1) - 1)
□
Lemma 8. Let K, M be two fixed natural numbers, η ∈ (0, 1], and S ∈ 0, e-1.5 a constant
sensitivity parameter. Denote:
DKM := n(n, a, b) ∈ N2M+1	a1+0.≤..n+≤aMK=n	o
K,M	b1+...+bM=K-n
define F : DK,M -→ R as:
F(n, a, b) = K ηn	n	b K-nb
n	a1, . . ., aM	b1, . . ., bM
and let x? := argmaxF (x). If IK ≥ (M — 1), then the number of X ∈ Dk,m which uphold
x∈DK,M
F (x) ≥ S ∙ F (x?) is boundedfrom above by:
(,1.	∖ M-1
25πeKln (ST) √η∖
2(M - 1)(1+ η) J
Proof. ByIemma3,x? ` (MnKn),M‰一…，M‰,MKn),...,MKn))∙ Bylemma7,
the number of ns between 0 ≤ n ≤ K such that (K)ηn ≥ S ∙ (K)η⅛n is upper bounded by
Kvzqln(Sn)(l-(S-(1+n) -4n, by lemma 6 the number of non-negative integer M-tuples aι,..., aM
such that aι + ... + aM = InK and (α ⅛ξ ) ≥ S ∙( 小 1+η 标 )is bounded from above by
十n	1,..., M	M(i+η),…,M(i + η)
31
Published as a conference paper at ICLR 2022
M-1
(∏ ) F
(M-1)√∏
，	1~…/ _i\	∖ M-I
4 J (M-n(Sι+η)) +1)	, and the number of non-negative integer M-tuples bι,...,bM
1,1	. 7	.	.	7	K	1 K VK ∖、	K	T⅛	∖ .	1	IlC	1	1
such that bi + ... + 6m = ικ+- and & 1+工)≥ S ∙ ( K 1+η K ) is bounded from above by
+η	b1,…,bM	' M(i+η),…,M(i+η)
M-1
(∏ ) F
(M-1)√∏
，	1_	、M-1
4 J(M—；(Is1+匕)+1)	. In total, without taking into consideration the interactions
between the three multiplicands (so our bound is not tight), the number of x ∈ DK,M which uphold
F (x) ≥ S ∙ F (x?) is upper bounded by:
K J(2ln(s-1) - 1)2 (1 + η)2 - 4η
2(1+ η)(ln(s-1) - 1)
I
binomial + η-exponent
}
M-1
(∏e) F
(M - 1) √∏
(4 / ηKIn(ST) — 1 1
I V(M- 1)(1 + η) +
M-1
}
^^^≡^^^{^^^^^^^≡
a-multinomial
M-1
(∏e) F
(M - 1) √∏
K	I_K ln(s-i)-
VV (M - 1)(1 + η) +1
M-1
}
b-multinomial
and since (M JlKi+# ≥ 1 and S ≤ e-1.5 (and hence ；(；；(：—1)—1)≤ 2), this can be further bounded
by:
2K
/	1.	∖ M-1
25πeKln (s-1) √η∖
(M — 1)2 ∏ y 2 (M -I) (1 + η)
□
Lemma 9. Let K, M be two fixed natural numbers, η ∈ (0, 1], and s ∈ 0, e-1.5 a constant
sensitivity parameter. Denote:
DKM := n(n, a, b) ∈N2M+1	a1+0.≤..n+≤aMK=n o
K,M	b1+...+bM=K-n
define F : DK,M -→ R as:
F(n,a,b)= Kn ηn
a1 , . . . , aM
K-
b1 , . . . ,
n
and let x? := arg maxF (x). If ɪ+^ ≥ M2, then the number of X ∈ Dk,m which uphold F (x) ≥
x∈DK,M
s ∙ F (x?) is boundedfrom below by:
M-1
-2-
1	∏πeK ln (s-1)
M√π 2 2M2 (1 + η)
Proof. By lemma 6, the number of non-negative integer M -tuples b1, . . . , bM such that b1 +
K	K-	K
...+ bM = iK and Q 1+工)≥ S ∙ ( K 1+η k ) is bounded from below by
+η	b1 ,...,bM	M(1+η)，...，M(1+η)
/	∖ MT
(π) 2
M √∏
,---- ∖ M —1
q≡ηM≡ -1!
Since these bs are only a subset of the elements of DK,M
which F takes into consideration, this is also a (quite loose) lower bound on the number of x ∈ DK,M
which uphold F (x) ≥ S ∙ F (x?).
Now, we know that:
K
1+η
≥M
2
32
Published as a conference paper at ICLR 2022
and since s ≤ e-1.5
, it holds that ln s-1 > 1, we get:
K
1+η
ln (ST) > TKn ≥ M2
^⇒ — 1 > 一
M
and therefore:
M-1
1
>
M-1
M1
—
M
M
M-1
-2-
M-1
(πe) F
1	( πeK ln (s-1)
M√ 2 2M2 (1+ n)
□
C LOWER BOUNDS ON THE ε-SEPARATION RANK
C.1 Preliminaries
C.1.1 Tensors and their matricization
We begin by laying out basic concepts in tensor theory required for the upcoming analysis. The core
concept of a tensor may be thought of as a multi-dimensional array. The order of a tensor is defined
to be the number of indexing entries in the array, referred to as modes. The dimension of a tensor in a
particular mode is defined as the number of values taken by the index in that mode. If A is a tensor of
order N and dimension Mi in each mode i ∈ [N], its entries are denoted Ad1...dN, where the index
in each mode takes values di ∈ [Mi].
We will make use of the concept of the matricization of A w.r.t. the balanced partition (P, Q),
denoted JAKp,q ∈ RM /2×M /2, which is essentially the arrangement of the tensor elements as
a matrix whose rows correspond to P and columns to Q. Suppose A ∈ RM× ×M is a tensor of
order N, and let (P, Q) be a balanced partition of [N], i.e. P and Q are disjoint size N/2 subsets
of [N] whose union gives [N]. The matricization of A w.r.t. the partition (P, Q), denoted JAKP,Q,
is the M N/2-by-M N/2 matrix holding the entries of A such that Ad1...dN is placed in row index
1 +PtN=/21(dpt - 1)M N/2-t and column index 1 +PtN=/21(dqt - 1)M N/2-t.
We now present the concept of grid tensors, which are a form of function discretization (Hackbusch,
2012). Essentially, the function is evaluated for a set of points on an exponentially large grid in
the input space and the outcomes are stored in a tensor. Formally, fixing a set of template vectors
x(1), . . . , x(Z) ∈ [V], the points on the grid are the set {(x(d1), . . . , x(dN))}dZ ,...,d =1. Given a
function y(x1, . . . , xN), the set of its values on the grid arranged in the form of a tensor are called
the grid tensor induced by y, denoted A(y)d1,...,dN ≡ y(x1 = x(d1), . . . , xN = x(dN)).
C.1.2 ε-RANK
We will make use of the concept of ε-rank Alon et al. (2013) of a matrix A defined for any ε > 0 as
the minimum rank over matrices that approximate every entry of A to within an additive ε. We will
prove lower bounds on the εs for which the ε-rank a matrix remain high by the following lemma:
Lemma 10. Let M ∈ Rn×n be symmetric matrix and ε > 0, then:
∀k ≤ n λk (M) ≥ ε =⇒ 三-rank (M) ≥ k	(26)
2n
33
Published as a conference paper at ICLR 2022
n×n
Proof. Let E ∈ [-ε, 2εn] , We need to prove that rank (M + E) ≥ k. Since M is symmetric, M
is diagonalizable with eigenvalues λι ≥ λ2 ≥ ∙∙∙ ≥ λn. Denote by v1,v2 ,...,vn the eigenvectors
that are normalized according to the l1 norm, then for any i ≤ k We have that:
k(M+E) vikι ≥ IiMviki - kEvi kι = λi — kEviki ≥ λi- n 2εn =ε > 0	(27)
In particular for any i ≤ k we have that (M + E) vi 6= 0 and since v1, v2, . . ., vk are linearly
independent we conclude that rank (M + E) ≥ k.	□
Finally, we will use the following lemma for lower bounding the amount of small eigenvalues of
symmetric matrices:
Lemma 11. Let M ∈ Rn×n be a symmetric matrix with diagonal entries that equal to 1, and denote
its eigenvalues by λi ≥ λ2, . . ., λn . Then:
r := max [ k : λk ≥ 1∖ ≥ In-J	(28)
n	kMkF
Proof. Since the trace of a matrix equals to both the sum of its eigenvalues and its diagonal entries,
we get:
nn
n = ^X [M]ii = trace (M) = ^X λi ≤ rλι + ———-≤ rλι + 1	(29)
i=i	i=i	n
Eq. 28 follows from the fact that:
kMIf = Jλ2 + …λ ≥ λι	(30)
□
C.1.3 High-Dimensional Spheres
We will use the Lebesgue measure on the sphere for taking expectations on Sd . For any measurable
subset A ⊆ Sd this measure is defined as the d + 1 dimensional volume of the "wedge" in the ball
Bd+i:
λd+1 ({tx IX ∈ At ∈ [0,1]})
μ (A) :=	λd+1 (Bd+1)	(31)
Where λd+i denotes the Lebesgue measure on Rd+i. We will also use an unnormalized version of
this measure:
μunnormalized (A)：= μ (A) ∙ λd+1(Bd+1)	(32)
Smith & Vamanamurthy (1989) showed that μu∏∏ormalized (Sd) is monotonically decreasing for d > 5.
The following lemma bounds the rate of this decrease:
Lemma 12. For any d > 5 the following holds:
2π
μunnormalized (S ) ≤ d + 1
μunnormalized (Sd)
(33)
Proof. Since d > 5 we know that μunnormalized (Sd)
μunnormalized (S" ɪ) ≤
μunnormalized (Sd)
Finally, Gipple (2014) showed that μunnormalized(Sd+ι)
μunnormaliZed(S '	)
> Munnormalized (Sd+1) and therefore:
μunnormalized (S" ɪ)	(34)
μunnormalized (Sd+1)
=d+∏1, completing the proof.	□
Finally, we will use a well known fact regarding the variation of the sphere volume for different radii
(see for example Smith & Vamanamurthy (1989)):
Fact 1. For any d ∈ N, R > 0 the following holds:
μunnormalized (RS )
μunnormalized (Sd )
Rd+i
(35)
34
Published as a conference paper at ICLR 2022
C.2 Proof of the lower bound
In this subsection, we prove theorem 2 of the main text. We will follow the proofs of Levine et al.
(2020); Wies et al. (2021), with important adjustments to the ε-sequential-separation rank definition.
We begin by showing that high ε-rank Alon et al. (2013) of the grid tensor matricization implies
high ε-sequential-separation rank (see section 2.2 of the main text) of the function. Essentially, we
apply claim 1 from Levine et al. (2020) to ε-approximations obtained from the ε-separation-rank
definition. This relation, which holds for all functions, is formulated below for functions realized by
the analyzed Transformer network:
Lemma 13. For yi(np-,cio)n,tLex,dt x as defined in theorem 1 of the main text. Let ε-seq-sep yi(np-,cio)n,tLex,dt x
denote its ε-sequential-separation rank. Then, for any integer Z, any ε > 0, any set of template
vectors x(1), . . . , x(Z) ∈ Rdx and any sub-matrix M of JA(Zy(p,i),L,dx )Ka,b it holds that:
ε-seq-sep yi(np-,cio)n,tLex,dtx ≥ ε-rank (M) ,	(36)
where A(Z (p,i),L,dx ) is the grid tensor of Z (p,i),L,dx with respect to the above template vectors.
yin-context	yin-context
Proof. If ε-seq-sep yi(np-,cio)n,tLex,tdx = ∞ then the inequality is trivially satisfied. Otherwise, as-
SUme that ε-seq-sep (y(PRnLdx) = K ∈ N, and let y be an ε-approximation for Z (p,i),L,dx with
yin-context
Seq-Sep (y) = K. By claim 1 of Levine et al. (2020) We have that rank(JA(y)Ka,b) ≤ K. De-
note by M the sub-matrix of JA(y)Ka,b that corresponds to the rows and columns in M. Now,
since y is an ε-approximation for Zy(p,i),L,dχ we have that ∣∣M - MI ≤ ε. Finally, by definition
ε-rank(M) ≤ rank (M) ≤ rank(JA(y)Ka,b) ≤ K.	□
Relying on lemma 13, we will bound the ε-sequential-separation rank from below via the ε-rank
of sub-matrices of JA(Z (p,i),L,dx )Ka b. Denote d :=(dx-H)/2, λ := 3L-2, lemmas 10, 11 assure
y p, ,, x a,
yin-context
US that for n :=《：) = Ω (2L∙dx) it is enough to prove that there exists an assignment to
the network’s weights, as well as choice of templates vectors, for which there exists sub-matrix
M ∈ Rn×n of JA(Z (p,i),L,dx )Ka,b that is symmetric with diagonal entries that equals to 1 and with
yin-context
kMk F ≤ (Pm+1)) n 3 ,in order toshow that ε-seq-sep (y (pc Rn Lxd") ≥ 2√nd+1 forε ≤ 5⅛.
Now we will use a corollary that is direct results of the proof in Levine et al. (2020). This corollary
shows that if Z (p,i),L,dx is able to produce vectors that do not change the analysis in Levine et al.
yin-context
(2020), then for any matrix B ∈ Rn×d with rows that are l2 normalized, there exists an assignment to
the networks weights, as well as choice of templates vectors, for which there exists a sub-matrix of the
grid tensor matricization that is equal to3 M = (BBT)θλ. Importantly M is symmetric. In addition,
since the rows of B are l2 normalized, the diagonal entries of M equals to 1. Therefore, M upholds
the assumptions of lemmas 10, 11, 13 and it is enough to find B for which kMkF ≤ (Pdny) n 3.
Corollary 2. Let d, λ > 0, assume that for any matrix A ∈ R ( 3Ld-2 )×d with rows that are l2
normalized, there exists a choice of template vectors x(1) , . . . , x(Z), an assignment to the embedding
layer and the first self-attention layer key and query weights, such that for any j1 , j2 ∈
the output of the first self-attention layer on j1, j2 +	3Ld-2	is:
3Ld-2
y(1,j) =	X W O,1,hW V,1,h	u
3We ignored the constant that appear in eq 28 of Levine et al. (2020) since we can get rid of this constant
by dividing the last layer output matrices. Importantly, this constant is larger than 1 and therefore the network
weights boundedness assumption is not violated
35
Published as a conference paper at ICLR 2022
for
∀α ∈ [dx]
uα
Aj1,φ(α)
Aj2,φ(α-da-1 )
2N
0
(a — 1) mod da < ^a2^^~ ∧ φ(a) ≤ d
da-1 ≤ (α — 1) mod d& < da _ 1 ∧ φ(α — da-1) ≤ d
(α - 1) mod da = da - 1
Otherwise
where Φ(j) ≡ bj-1∕daC ∙ (da — 1) + (j — 1 mod da) + L
Then for any matrix B ∈ Rn×d with rows that are l2 normalized, there exists an assignment to the
networks weights, as well as choice of templates vectors for which there exists sub-matrix of the grid
tensor matriCization that equal to M = (BBT)θλ.
Now we will shows that indeed Zy(p,i),L,dx is able to produce vectors that do not change the analysis
yin-context
in Levine et al. (2020) and the assumptions of corollary 2 holds.
Lemma 14. Let A ∈ R ( 3Ld-2 )
template vectors x(1) , . . . , x(Z),
×d with rows that are l2 normalized, then there exists a choice of
an assignment to the embedding layer and the first self-attention
layer key and query weights, such that for any j1 , j2 ∈
layer on j1 , j2 +	3Ld-2	is:
3Ld-2
the output of the first self-attention
for
y(1,j) =	X WO,1,hWV,1,h	u
{Ajι,Φ(α)
AN2,φ(α-d-)
(a — 1) mod da < "a2~^■■ ∧ φ(a) ≤ d
da-1 ≤ (α — 1) mod da < da — 1 ∧ φ(α — da-1) ≤ d
(α — 1) mod da = da — 1
Otherwise
where φ(j) ≡ bj-1∕daC ∙ (da - 1) + (j - 1 mod da) + 1.
Proof. We will ignore Z (p,i),L,dx ’s element-wise multiplication with vocabulary embedding matrix
yin-context
by choosing ∀i, j MiV,j = 1 (by the terms of corollary 2 it suffices to find any assignment of the
learned weights).
For any i ∈ 2	3Ld-2	+ 1 our templates vectors will be:
Ai,φ(j)
x(i)=N< Ai-(3L-2 )+1,Φ(j-⅛1)
N
0
i≤(3Ld-2)
∧(j-1) mod da< da-1 ∧φ(α)≤d
(3Ld-2)<i≤2(3Ld-2)
∧da2-1 ≤(j-1) mod dα<dα-1∧φ(α-da- )≤d
(j — 1) mod da = da — 1
Otherwise
We will implement summation of the inputs embedding in the first self-attention layer, we will follow
Levine et al. (2020) and set the first layer self-attention key and query weights to:
WiK,j,1,h = WiQ,j,1,h = 1i=1∧j=da
36
Published as a conference paper at ICLR 2022
This assignment implements summation of the inputs embedding in the first self-attention layer since:
NH
y(1,i)(x(d1),..., x(d2))α = XX W Q,1,h MVw1j	x(d1) , W K,1,h MVw1j	x(d1)
	j=1 h=1 (37) W O,1,hW V,1,h MVw1j	x(d1)	(38) NH +XX W Q,1,h MVw2j	x(d2) , W K,1,h MVw2j	x(d2) j=1 h=1 (39) W O,1,hW V,1,h MVw2j	x(d2)	(40) =1	=1 N H z}l{ z}l{ =xx χdαι) ∙ χddι) W O,1,hW V,1,hx(d1)	(41) j=1 h=1 a	a =1	=1 N H z}l{ z}l{ + XX xdd2) ∙ xdd2) WO,1,hWV,1,hχ(d2)	(42) j=1 h=1 =2	X W O,1,hW V,1,h N x(d1) + x(d2)	(43)
where (1) is because W Q,1,h = W K,1,h are matrices that are zero everywhere except for entry (1, da)
and that all the entries in the vocabulary embedding matrix equals to 1, and (2) because of linearity.
Therefore, for any j1 , j2 ∈	3Ld-2 the output of the first self-attention layer on j1 , j2 is:
y(1,j)
X W O,1,hW V,1,h
h=1
N	x(d1) +x(d2)
(44)
Finally, we need to show that indeed for any j1, j2 ∈
3Ld-2
eq 44 give the desired u:
∀α ∈ [dx]
{Aj1,φ(α)
AjWa-da-1)
2N
0
(a — 1) mod	da	< "a?~^■■	∧ φ(a) ≤ d
da-1 ≤ (α —	1)	mod da	< da — 1 ∧ φ(α	— da-1) ≤ d
(α - 1) mod da = da - 1
Otherwise
The third and forth cases are clear from x0 s definition, so it remain to prove the first and second cases.
For this we will examine d1 , d2 . d1 = j1 ≤	3Ld-2	and therefore:
(d1)	Aj1,φ(α)
xα	= 0
(a — 1) mod da < ^a^^^~ ∧ φ(a) ≤ d
da-1 ≤ (α — 1) mod d& < da _ 1 ∧ φ(α — da-1) ≤ d
x(d2) =
xα =
d2 = j2 +	3Ld-2	∈ 1 +	3Ld-2	, 2 3Ld-2	and therefore:
0	(α — 1) mod da < "a2~^■■ ∧ φ(a) ≤ d
Aj2,φ(α- da-1 )	da2 1 ≤ (α — 1) mod da < da — 1 ∧ φ(α — da- 1 ) ≤ d
So it clear that also the first and second cases upholds.	口
Returning to finding B for which ∣∣M∣∣f ≤ (p(d + 1)) n3, we will use the probabilistic method
for proving the existence of such B, i.e.we will show that for random B the expectation of kMkF ≤
(ʌ/(d + 1)) n44 and therefore in particular there exists such B.
37
Published as a conference paper at ICLR 2022
Lemma 15. For any d, λ ∈ N such that λ ≥ d the following holds:
EB~(Sd)n
h∣∣(BBT )°λ∣U≤ pd+^
(45)
Proof. We start by bounding the expectation of the squared norm:
2n
EB~(sd)n	JI(BBT)	||尸=X EB~(sd)n
i,j=1
[(bbt)q,j
n
=X
Eu,v〜Sd [hu,vi2λ] = n2Eu,v~Sd [hu,vi2λ]
i,j=1
-1
Now, Eu,v~sd [hu,v)2λ] ≤ (d + 1)《d) 2 by lemma 16 and therefore We got that:
EB~(Sd)n
3
I∣(BBT )°1F]≤ (d+1) ((d))2
(46)
(47)
(48)
Finally, by Jensen inequality we have that:
EB~(Sd)n h|| (BBT)θ
EB~(sd)n	|| (BBT 产
3
≤ p(d+1) ((d))4
(49)
2
F
□
C.3 Technical lemmas
Lemma 16. For any d, λ ∈ N such that λ ≥ d the following holds:
1
2
Eu,v〜Sd
hhu,vi2λi ≤ (d+ 1)	λd
(50)
Proof. We will use conditional expectation to make reduction for simpler expectation. From rotational
invariance of the uniform measure on Sd we know that for every rotation matrix R ∈ SO (d) and
unit vector v ∈ Sd we have that:
Eu~Sd [hu, vi2λlv] = Eu~Sd [hRu, Rvi21|v] = Eu~sd [hu,Rvi21|v|	(51)
where the first equality holds because R is orthogonal. Therefore, by choosing R such that Rv = e1
we will get that:
Eu,v〜Sd hu, vi2λ] = Ev~Sd [Eu^Sd [hu,vi2λlv]]
=Ev~Sd [Eu~Sd [ul"|v]] = Eu~Sd [u2λ]
Now we can calculate the last expectation directly:
2λ	「2λ1	2	2λ j ( ∖	μu μunnormalized ((√1 - x2) S^ ɪ) 2λ 3
Eu-Sd [u1 ] = / u1 dμ (U) = /	d^d∖	X dx
Ju∈Sd	J-1	μunnormalized (S )
Now, by lemma 12 and fact 1 we have that:
0<
μunnormalized ((√1 - x2) Sd ɪ)
μunnormalized
d + 1
2π
1-
Therefore we have that:
EU,V~Sd hhu,vi2λi ≤
d + 1
2
x2λdx = (d + 1)
∕1 (1 -
0
x2) 2 x2λdx
-1
Finally, by lemma 17 each term in the integral is upper bounded by	λd	2 and thus:
Eu,v〜Sd
hu,vi2λ ≤ (d+ 1)
1
(52)
(53)
(54)
(55)
(56)
(57)
≤
d
2
□
38
Published as a conference paper at ICLR 2022
Lemma 17. For any x ∈ [0, 1] and d, λ ∈ N such that λ ≥ d the following holds:
χ2λ(l - χ2) 2 ≤
1
2
(58)
Proof. Note that since χ2λ(1 - x2)2 = 0 in the boundaries (X ∈ {0,1}), it is enough to prove the
inequality for critical points.
0 = (x2λ (1 - x2) d) = (2λx2λ-1) (1 - x2) d - 2dχ2λ+1 (1 - x2) 2-
Therefore, x2
^⇒ 0 = 2λ(1 - x2) - dx2 ^⇒ x2 = ,I
2λ + d
= 22+d is the only critical point and:
/"。…)d ≤ (£ Y (1 - £
1	2 2 +d + d )<	2λ + d
1 - 2λ+d"a)d
≤ (1 - 3dλ ) (1.5(λd + d))
d
d ) 2
e (λ + d))
1
2
e(λ+d)
where the last inequality follow from the fact that:
λ+d-1
d
e (λ + d - 1) ) d ≤
(e (λ + d) )d
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
□
d
d
2
D	Experimental Details
D.1 kNN-TAPT
We conducted the network training described in section 3.1 of the main text with AdamW optimizer
(with the parameters suggested in the original RoBERTa paper: β1 = 0.9, β2 = 0.98, ε = 10-6 and
weight decay of 0.01), with batch sizes of 128 or 256 (depending on model size) and sequences of 256
tokens each. We started with pretrained RoBERTA-base weights from the HuggingFace Transformers
repository 4, and continued training them on the MLM task with masking probability of 15%, where
each masked token had a probability of 80% of being replaced with the special [MASK] token, 10% of
being replaced with a random token and 10% of being kept the same. The data used for this phase of
training was created using the four different procedures described in section 3.1. After the training
was finished, we evaluated the models’ performance using the SentEval kit.
D.2 kNN-Pretraining
We conducted the network training described in section 3.2 of the main text with AdamW optimizer
(with the parameters suggested in the original GPT-2 paper: β1 = 0.9, β2 = 0.95, ε = 10-8 and
weight decay of 0.1), with batch size of 512 and sequences of 256 tokens each. We pretrained a
HuggingFace Transformers implementation of GPT-2 from scratch on Wikipedia with the standard
LM objective, and switched to a mixture of the standard data and our generated kNN data in two
different points during training. After the training was finished, we evaluated the models’ performance
on the Natural Questions benchmark.
4https://huggingface.co/transformers/
39
Published as a conference paper at ICLR 2022
E	kNN-Pretraining at different checkpoints
The following table includes F1 evaluation scores of zero shoe closed book Natural Questions
examples for different model sizes at different training checkpoints. Overall, further pretraining
seems to improve the effectiveness of kNN-Pretraining.
Model Size	Reg.+kNN Steps	NQ F1	
110M	200/400+0	<	10-3
110M	200+40	6.2	• 10-3
110M	400+40	7.9	• 10-3
345M	200 / 400+0	<	10-3
345M	200+10	9.6	• 10-3
345M	400+10	1.4	• 10-2
Table 2: Impact of model size and regular pretraining steps on the Natural Questions F1 score of
kNN-Pretraining.
F	kNN-Pretraining on additional benchmarks
The main text describes experiments on the Natural Questions dataset. We test how kNN-Pretraining
affects other NLU tasks, by examining several tasks from the GLUE benchmark (Wang et al.,
2018) - Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2017), Recognizing
Textual Entailment (RTE) (Dagan et al. (2010) and others), and the The Winograd Schema Challenge
(WNLI) (Levesque et al., 2012). As in the case of Natural Questions, we evaluate the zero-shot
performance of our models since it is a direct probe to the abilities of the model straight after
the process of pretraining. In contrast to Natural Questions, the GLUE tasks we examined are
classification tasks and not generation tasks, so assessing zero shot performance on them is not
straightforward. We therefore follow the template-based method of Gao et al. (2021a) for converting
the tasks’ data into a format processable by unidirectional language models.
Notably, the examined GLUE classification tasks are not easy for the examined unidirectional models
in zero shot. Table 3 includes the zero-shot scores of the 345M parameter model that trained regularly
for 200K steps and then continued training for 20K steps of kNN-Pretraining, versus the average
of 3 baselines that trained regularly for the same number of overall steps (the same models used in
figure 1). Similarly to the results on Natural Questions (figure 1), all examined models score only
slightly better than random guess on the examined GLUE tasks. However (and again similarly to
the case of Natural Questions), we get a clear signal that kNN-Pretraining significantly moves the
needle when applied for just 10% of the regular pretraining time. We conjecture that when using
stronger models (that train for longer and over more data), the positive effect of kNN-Pretraining will
be enhanced, since as the model improves, it can better understand and utilize the various in-context
hints that kNN-Pretraining provides.
GLUE Task	MNLI	RTE	WNLI
Random guess	33.3	50.0	50.0
3 baselines - Average score	35.1	52.0	51.1
3 baselines - Max score	35.3	52.3	54.9
kNN-Pretraining	35.5	53.0	56.3
Table 3: Zero-shot accuracy scores on several GLUE tasks of a kNN-Pretrained model versus 3
baselines that trained regularly on the same data.
40