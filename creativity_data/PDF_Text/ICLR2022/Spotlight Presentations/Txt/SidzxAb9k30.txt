Published as a conference paper at ICLR 2022
Near-Optimal Reward-Free Exploration for
Linear Mixture MDPs with Plug-in Solver
Xiaoyu Chen & Jiachen Hu
Key Laboratory of Machine Perception, MOE,
School of Artificial Intelligence, Peking University
{cxy30, NickH}@pku.edu.cn
Lin F. Yang *
Electrical and Computer Engineering Department,
University of California, Los Angeles
linyang@ee.ucla.edu
Liwei Wang *
Key Laboratory of Machine Perception, MOE,
School of Artificial Intelligence, Peking University
International Center for Machine Learning Research, Peking University
wanglw@cis.pku.edu.cn
Ab stract
Although model-based reinforcement learning (RL) approaches are considered
more sample efficient, existing algorithms are usually relying on sophisticated
planning algorithm to couple tightly with the model-learning procedure. Hence
the learned models may lack the ability of being re-used with more specialized
planners. In this paper we address this issue and provide approaches to learn an
RL model efficiently without the guidance of a reward signal. In particular, we
take a plug-in solver approach, where we focus on learning a model in the explo-
ration phase and demand that any planning algorithm on the learned model can
give a near-optimal policy. specifically, we focus on the linear mixture MDP set-
ting, where the probability transition matrix is a (unknown) convex combination
ofa set of existing models. We show that, by establishing a novel exploration algo-
rithm, the plug-in approach learns a model by taking O(d2H3/e2) episodes with
the environment and any -optimal planner on the model gives an O()-optimal
policy on the original model. This sample complexity matches our lower bound
for non-plug-in approaches and is statistically optimal. We achieve this result
by leveraging a careful maximum total-variance bound using Bernstein inequality
and properties specified to linear mixture MDPs.
1	Introduction
In reinforcement learning, an agent repeatedly interacts with the unknown environment in order to
maximize the cumulative reward. To achieve this goal, an RL algorithm must be equipped with
effective exploration mechanisms to learn the unknown environment and find a near-optimal policy.
Efficient exploration is critical to the success of reinforcement learning algorithms, which has been
widely investigated from both the empirical and the theoretical perspectives (e.g. Stadie et al. (2015);
Pathak et al. (2017); Azar et al. (2017); Jin et al. (2018)). Model-based RL is one of the important
approaches to solve for the RL environment. In model-based RL, the agent learns the model of
the environment and then performs planning in the estimated model. It has been widely applied in
many RL scenarios, including both online setting (Kaiser et al., 2019; Luo et al., 2019; Azar et al.,
2017) and offline setting (Yu et al., 2020; Kidambi et al., 2020). It is also believed that model-
based RL is significantly more sample-efficient than model-free RL, which has been justified by
many recent empirical results (e.g. Kaiser et al. (2019); Wang et al. (2019)). Though the theoretical
model-based learning in small scale problems has been studied extensively (Azar et al., 2017; Zhou
et al., 2020a; Jin et al., 2020a), it is still far from complete, especially with the presence ofa function
approximator.
* Corresponding author.
1
Published as a conference paper at ICLR 2022
As an important implication of model-based approaches, the power of plug-in approach have been
studied in several works (Cui & Yang, 2020; Agarwal et al., 2020). The idea of plug-in approach
is rather simple: We construct an empirical Markov Decision Process (MDP) using maximum like-
lihood estimate, then return the (approximate) optimal policy with efficient planning algorithm in
this empirical model. The significance of plug-in approaches is two-folded. For one thing, it pre-
serves an empirical model that keeps the value of the policies, which is of independent interests. For
another, the empirical model can be used for any down-stream tasks, which makes the application
much more flexible. It is shown that the plug-in approach achieves the minimax sample complexity
to compute the -optimal policies with a generative model in the tabular (Agarwal et al., 2020) and
linear settings (Cui & Yang, 2020).
In this paper, we aim to understand the power of plug-in approach in the reward-free exploration with
linear function approximation. We study the linear mixture MDPs, where the transition probability
kernel is a linear mixture ofa number of basis kernels (Ayoub et al., 2020; Zhou et al., 2020a;b). We
first build an empirical model with an estimation of the transition dynamics in the exploration phase,
and then find a near-optimal policy by planning with the empirical model via arbitrary plug-in solver
in the planning phase. Our setting is different from the reward-free exploration with linear function
approximation without plug-in model (Wang et al., 2020; Zanette et al., 2020b), in which the agent
can directly observe all history samples and design specialized model-free algorithm in the planning
phase.
Our results show that the plug-in approach can achieve near-optimal sample complexity in the
reward-free setting. In particular, we proposed a statistically efficient algorithm for reward-free
exploration. Our algorithm samples O(d2H4/e2) trajectories during the exploration phase, which
suffices to obtain O()-optimal policies for an arbitrary reward function with an -optimal pluging
solver in the planning phase. Here dis the feature dimension, and H is the planning horizon. Fur-
thermore, with a more refined trajectory-wise uncertainty estimation, we further improve the sample
complexity bound to O (d2H3∕e2) in the regime where d > H and E ≤ H∕√d. This matches
our lower bound Ω(d2H3∕e2) for reward-free exploration in linear mixture MDPs, which indicates
that our upper bound is near-optimal except for logarithmic factors. To the best of our knowledge,
this is the first work that obtains minimax sample complexity bounds for the plug-in approach in
reward-free exploration with linear function approximation.
2	Related Work
RL with Linear Function Approximation Reinforcement learning with linear function approxi-
mation has been widely studied in the recent few years (e.g. Jiang et al. (2017); Yang & Wang (2019;
2020); Jin et al. (2020b); Modi et al. (2020); Du et al. (2019); Zanette et al. (2020a); Cai et al. (2020);
Ayoub et al. (2020); Weisz et al. (2021); Zhou et al. (2020a;b)). The linear mixture MDPs model
studied in our work assumes the transition probability function is parameterized as a linear func-
tion of a given feature mapping over state-action-next-state triple (Ayoub et al., 2020; Zhou et al.,
2020b;a). Based on the Bernstein inequality for vector-valued martingales, Zhou et al. (2020a) pro-
posed an efficient algorithm that obtains minimax regret in the regime where d > H. Besides linear
mixture MDPs, linear MDPs is another category of RL with linear function approximation, which
assumes both the transition probability function and reward function are parameterized as a linear
function of a given feature mapping over state-action pairs. The algorithms with best regret bounds
were proposed by Jin et al. (2020b) and Yang & Wang (2020), which studied model-free algorithm
and model-based algorithm respectively. The minimax regret bound for linear MDPs is still unclear.
Reward-Free Reinforcement Learning In contrast to the standard RL setting, reward-free re-
inforcement learning separates the exploration problem and the planning problem, which allows
one to handle them in a theoretically principled way. For tabular setting, reward-free reinforcement
learning has been well-exploited in many previous results (Jin et al., 2020a; Kaufmann et al., 2021;
Menard et al., 2020; Zhang et al., 2020; 2021b; WU et al., 2021a; Bai & Jin, 2020; Liu et al., 2021),
where the minimax rate is obtained by Menard et al. (2020). For reward-free exploration with lin-
ear function approximation, Wang et al. (2020) proposed the first efficient algorithm that obtains
O(d3 H6 /E2 ) sample complexity for linear MDPs. However, their algorithm is model-free in nature
and cannot guarantee good performance with any plug-in solver. Further, Qiu et al. (2021) proposed
the first provably efficient reward-free algorithm with kernel and neural function approximation.
2
Published as a conference paper at ICLR 2022
We also noticed that there is a concurrent work which also studied reward-free exploration for linear
mixture MDPs (Zhang et al., 2021a). Compared with their results, we focus on the setting of the
reward-free exploration with plug-in solver, which covers the standard reward-free setting studied in
the previous results. Furthermore, our sample complexity bounds are tighter than theirs by a factor
of H2 1. The above two differences introduce new challenges in both the algorithmic design and the
complexity analysis in this work, which makes our algorithms much more complicated than theirs.
Besides, our lower bound is tighter than theirs in the dependence on d.
Plug-in Approach The plug-in approach has been studied in tabular/linear case in restrictive set-
tings. E.g., Agarwal et al. (2020) and Cui & Yang (2020) studied the standard plug-in approach
with a generative model, where the algorithm is allowed to query the outcome of any state action
pair from an oracle. They showed the plug-in approach also achieved the minimax optimal sample
complexity to find an -optimal policy in both tabular MDPs and linear MDPs. The reward-free al-
gorithms proposed by Jin et al. (2020a) are model-based in nature, thus can be regarded as a solution
in the plug-in solver setting. However, their algorithms are restricted to the tabular case and cannot
be applied to the setting with linear function approximation.
3	Preliminaries
3.1	Episodic MDPs
We consider the setting of episodic Markov decision processes (MDPs), which can be denoted by
a six-tuple (S, A, P, R, H, ν), where S is the set of states, A is the action set, P is the transition
probability matrix so that Ph(∙∣s, a) gives the distribution over states if action a is taken on state S at
step h, Rh(s, a) is the deterministic reward function of taking action a on state s with support [0, 1]
in step h, H is the number of steps in each episode, and ν is the distribution of the initial state.
In episode k, the agent starts from an initial state sk,1 sampled from the distribution ν. At each
step h ∈ [H], the agent observes the current state sk,h ∈ S, takes action ak,h ∈ A, receives reward
Rh(sk,h, ak,h), and transits to state sk,h+1 with probability Ph(sk,h+1|sk,h, ak,h). The episode ends
when sH+1 is reached.
A deterministic policy π is a collection of H policy functions {πh : S → A}h∈[H] . We
use Π to denote the set of all deterministic policies. For a specific reward function R, we
use Vhπ : S × R → R to denote the value function at step h under policy π w.r.t. re-
ward R, which gives the expected sum of the remaining rewards received under policy π
starting from sh = s, i.e. Vhπ(s, R) = E PhH0=h R(sh0,πh0 (sh0)) | sh = s, P . Accord-
ingly, we define Qπh (s, a, R) as the expected Q-value function at step h: Qπh (s, a, R) =
E R (sh, ah) + PhH0=h+1 R(sh0, πh0 (sh0)) | sh = s, ah = a, P .
We use ∏R to denote the optimal policy w.r.t. reward R, and we use Vj^(∙, R) and Qh(∙, ∙, R) to
denote the optimal value and Q-function under optimal policy ∏R at step h. We say a policy ∏ is
-optimal w.r.t. reward RifE
[PH=1 Rh (sh,ah) I ∏]
≥ E IPH=I Rh (Sh, ah) | πr] -e.
For the convenience of explanation, we assume the agent always starts from the same state S1 in
each episode. It is straightforward to extend to the case with stochastic initialization, by adding a
initial state S0 with no rewards and only one action a0, and the transition probability of (S0, a0) is
the initial distribution μ. We use PhV(s, a, R) as a shorthand of £§, Ph(s0∣s, a)V(s0, R).
3.2	Linear Mixture MDPs
We study a special class of MDPs called linear mixture MDPs, where the transition probability
kernel is a linear mixture of a number of basis kernels (Ayoub et al., 2020; Zhou et al., 2020b;a).
This model is defined as follows in the previous literature.
1 When transformed to the time-homogeneous MDPs setting studied in Zhang et al. (2021a), our algorithms
can achieve sample complexity bounds O(d2H3/e2) and O((d2H2 + dH3)∕e2), respectively.
3
Published as a conference paper at ICLR 2022
Definition 1. (Ayoub et al., 2020) Let φ(s, a, s0) : S × A × S → Rd be a feature mapping satisfying
that for any bounded function V : S → [0, 1] and any tuple (s, a) ∈ S × A, we have kφV (s, a)k2 ≤
1, where φV (s, a) = s0∈S φ (s, a, s0) V (s0) . An MDP is called a linear mixture MDP if there
exists a parameter vector θh ∈ Rd with ∣∣θh∣∣2 ≤ B for a constant B and a feature vector φ(∙, ∙, ∙),
such that Ph (s0 |s, a) = θh＞φ(s, a, s0) for any state-action-next-state triplet (s, a, s0) ∈ S × A × S
and step h ∈ [H].
3.3	Reward-Free Reinforcement Learning
We study the problem of reward-free exploration with plug-in solver. Our setting is different from
the reward-free exploration setting studied in the previous literature. Formally, there are two phases
in this setting: exploration phase and planning phase.
During the exploration phase, the agent interacts with the environment for K episodes. In episode k,
the agent chooses a policy πk which induces a trajectory. The agent observes the states and actions
sk,ι, ak,ι,…,Sk,H, ak,H as usual, but does not observe any rewards. After K episodes, the agent
calculates the estimated model {Ph = 京＞φ}h∈[H], which will be used in the planning phase to
calculate the optimal policy.
During the planning phase, the agent is no longer allowed to interact with the MDP. Also, it cannot
directly observe the history samples obtained in the exploration phase. Instead, the agent is given a
set of reward function {Rh}h∈[H], where Rh : S × A → [0, 1] is the deterministic reward in step h.
For notation convenience, we occasionally use R as a shorthand of {Rh}h∈[H] during the analysis.
We define VV∏,P(s,R) as the value function of transition P and reward R, i.e. ^V∏,P(s, R)=
E [PH=h Rh (sh,∏h0 (shf | Sh = s,Pi .
In the planning phase, the agent calculates the optimal policy Πr with respect to the reward function
R in the estimated model P using any opt-optimal model-based solver. That is, the returned policy
ʌ.ɪ- D ,	_、	一己.	_、
∏r satisfies: % , (si, R) 一 % r, (sι,R) ≤ eopt.
The agent’s goal is to output an accurate model estimation P after the exploration phase, so that
the policy Πr calculated in planning phase can be E + EoPt-OPtimaI w.r.t. any reward function R.
Compared with the reward-free setting studied in the previous literature (Jin et al., 2020a; Wang
et al., 2020; Menard et al., 2020; Kaufmann et al., 2021), the main difference is that we require that
the algorithm maintains a model estimation instead of all the history samples after the exploration
phase, and can use any model-based solver to calculate the near-optimal policy in the planning
phase.
4	Reward-Free RL with Plug-in S olver
4.1	Algorithm
The exploration phase of the algorithm is presented in Algorithm 1. Recall that for a given value
function &,h+i, we have PhVk,h+ι(sk,h,ak,h) = θ＞。％,h+1 (sk,h,ak,h) for any k,h. Therefore,
Vk,h+1(sk,h+1) and φ% h+1 (sk,h, ak,h) can be regarded as the stochastic reward and the linear fea-
ture of a linear bandits problem with linear parameter θh . We employ the standard least-square
regression to learn the underlying parameter θh In each episode, we first update the estimation θk
based on history samples till episode k 一 1. We define the auxiliary rewards Rk,h to guide explo-
ration. We calculate the optimistic Q-function using the parameter estimation θk, and then execute
the greedy policy with respect to the updated Q-function to collect new samples.
The main problem is how to define the exploration-driven reward Rk,h (s, a), which measures the
uncertainty for state-action pair (s, a) at the current step. In the setting of linear mixture MDPs,
the linear feature φV (s, a) is a function of both state-action pair (s, a) and the next-step value func-
tion V. This is not a big deal in the standard exploration setting (Ayoub et al., 2020; Zhou et al.,
2020a). However, in the reward-free setting, since the reward function is not given beforehand, we
need to upper bound the estimation error of value functions for any possible rewards. To tackle
4
Published as a conference paper at ICLR 2022
Algorithm 1 Reward-free Exploration: Exploration Phase
Input: Failure probability δ > 0 and target accuracy e > 0
λ 一 B-2, β 一 HPlog(4H3Kλ-1δ-1) + √λB, V 一 {V : S → [0, H]}
for episode k = 1,2,…，K do
Qk,H+1(∙, ∙) = * 0, VkH+1(∙) = 0
5:	for step h = H, H - 1,…，1 do
Ak,h J Pk=I φt,h(St,h, at,h)φt,h(st,h, at,h)	+ λI
θk,h J (Λk,h )	Pt=1 φt,h (st,h , at,h)Vt,h+1,s,a (st,h+1 )
Vk,h+ι,s,a J argmaxv∈v ||£§, φ(s,a, s0)V(s0)k(Λk,h)-ι ,∀s, a
00
φk,h(s, a) j- ∑^o φ(s, a, S )Vk,h+1,s,a(S ), ∀s, a
10:	uk,h (s, a) J β φk,h (s, a)> (Λk,h)-1 φk,h (s, a), ∀s, a
Define the exploration-driven reward function Rk,h(S, a) = uk,h(S, a), ∀S, a
Qk,h(s, a) J min {θ>,h (Pso Φ(s, a, s0)Vk,h+ι(s0)) + Rk,h(s, a) + uk,h(s, a), h}
Vk,h(S) J maxa∈A Qk,h(S, a), πk,h(S) = arg maxa∈A Qk,h(S, a)
end for
15:	for step h = 1, 2,…，H do
Take action ak,h = ∏k,h(sk,h) and observe sk,h+ι ~ Ph(sk,h, ak,h)
end for
end for
Find Ph such that the transition Ph(∙∣∙, ∙)=石>φ(∙, ∙, ∙) is well-defined and 11θ*h - θκ,h 卜	≤
β for h ∈ [H]	,
20: Output: {Pfh}H=ι
this problem, we use maxV ∈V βkφV(S, a)kΛ-1 = maxV ∈V β φV (S, a)> (Λk h)-1 φV (S, a) as a
k,h
measure of the maximum uncertainty for the state-action pair (S, a), where V = {V : S → [0, H]}
is the set of all possible value functions, and Λk,h is the summation of all the history sam-
ples {St,h, at,h, St,h+1}tk=-11 with feature φt,h(St,h, at,h) = arg maxφV βkφV(S, a)kΛ-1 . In each
episode, we define Rk h(S, a) = uk h(S, a) = maxV ∈V βkφV(S, a)kΛ-1 , where Rk h(S, a) is the
k,h
exploration-driven reward used to guide exploration, and uk,h (S, a) is the additional bonus term
which helps to guarantee that Qk,h (S, a) is an optimistic estimation. Finally, the algorithm re-
turns the model estimation {Ph}h that is well defined (i.e. s0 Ph(S0|S, a) = 1, Ph(S0|S, a) ≥
0, ∀s, a, s0), and satisfy the constraints Mh 一 0&h卜 ≤ β∙
4.2 Implementation Details
Algorithm 1 involves two optimization problems in line 8 and line 19. These problems can be
formulated as the standard convex optimization problem with a slight modification. Specifically, the
optimization problem in line 8 of Algorithm 1 can be formulated in the following way:
max	φ(S, a, S0)V(S0)	S.t. 0 ≤ V(S) ≤ H, ∀S ∈ S	(1)
V	s0	(Λk,h)-1
In general, solving this optimization problem is hard. For the case of finite state space (S ≤ ∞),
a recent work of Zhang et al. (2021a) relaxed the problem to the following linear programming
problem:
max 卜-,k∕2Φ(s, a)f∣L s.t. kfk∞ ≤ H,
(2)
where Φ(s,a) = (φ (s, a,Sι),…，φ (s,a,S∣s∣)) and f = (f(Sι), ∙∙∙ ,f (S∣s∣))>. As dis-
cussed in Zhang et al. (2021a), the sample complexity will be worse by a factor of d if we solve the
5
Published as a conference paper at ICLR 2022
linear programming problem as an approximation. For the case where the state apace is infinite, we
can use state aggregation methods (Ren & Krogh, 2002; Singh et al., 1995) to reduce the infinite
state space to finite state space and apply the approximation approaches to solve it.
EI	. ∙	Fl	∙ 1 ∙	-t ∣-∖ ∕' * 1	JI	-t ∙ . 1' 1	. P⅛	. ∙ i~ ∙	1
The optimization problem in line 19 of Algorithm 1 is to find parameter θh satisfying several con-
straints. For the case where the state space is finite, we can solve this problem in the following
way:
min J J θh — θκ,h U	s.t.	^X ”0(s, a, s0) = 1, θ>φ(s, a, s0) ≥ 0,∀s,a ∈ S × A
The above problem can be regarded as a quadratic programming problem and can be solved ef-
ficiently by the standard optimization methods. By Lemma 4, we know that the true parameter
θh satisfies ∣∣θκ,h — θh∣∣Λκ,h ≤ β With high probability. Therefore, the solution θ)h satisfies the
Constraint Mh — θκ,h 卜 ≤ β with high probability.
For the case Where the state space is infinite, We can also solve the above problem using state ag-
gregation methods. In particular, if the linear mixture MDP model can be regarded as a linear com-
bination of several base MDP models (i.e. φ(s, a, s0) = (Pι(s0∣s, a), P2(s0∣s, a),…，Pd(s0∣s, a))>
where Pi(s0|s, a) is the transition probability of certain MDP model), then we can formulate the
optimization problem in the following way:
min Jθh — θκ,hJ∣A	s.t. ”1 = 1,θ 占 0,
which can also be solved efficiently in the case of infinite state space.
4.3 Regret
Theorem 1. With probability at least 1 — δ, after collecting K = O (ddιH ) trajectories , Algo-
rithm 1 returns a transition model P, then for any given reward in the planning phase, a policy
returned by any opt -optimal plug-in solver on (S, A, P, R, H, ν) is O(+opt)-optimalfor the true
MDP, (S, A, P, R, H, ν).
We also propose a lower bound for reward-free exploration in Appendix C. Our lower bound indi-
cates that Ω(d2H3/e2) episodes are necessary to find an c-oPtimal policy with constant probability.
This lower bound is achieved by connecting the sample complexity lower bound with the regret
lower bound of certain constructed learning algorithms in the standard online exploration setting.
Compared with this bound, our result matches the lower bound w.r.t. the dimension d and the preci-
sion c except for logarithmic factors.
There is also a recent paper of Wang et al. (2020) studying reward-free exploration in the setting
of linear MDPs. Their sample complexity is Oθ (d3H5 6/c2), thus our bound is better than theirs by a
factor ofH2. Though the setting is different, we find that our parameter choice and the more refined
analysis is applicable to their setting, which can help to further improve their bound by a factor of
H2. Please see Appendix D for the detailed discussion.
5 IMPROVING THE DEPENDENCE ON H
In this section, we close the gap on H with a maximum total-variance bound using Bernstein in-
equality. In the previous results studying regert minimization in online RL setting (Azar et al.,
2017; Zhou et al., 2020a; Wu et al., 2021b), one commonly-used approach to obtain the minimax
rate is to upper bound the regret using the total variance of the value function by the Bernstein’s
concentration inequalities, and finally bound the summation of the one-step transition variance by
the law of total variance (Lattimore & Hutter, 2012; Azar et al., 2013; 2017). However, the situa-
tion becomes much more complicated in the reward-free setting with linear function approximation.
Recall that Vθk,h+1,s,a(s0) defined in Line 9 of Algorithm 1 is the next-step value function that max-
imizes the uncertainty kφV (s, a)kΛ-1 for state-action pair (s, a). One naive approach is to still use
k,h
6
Published as a conference paper at ICLR 2022
maxV kφV (s, a)kΛ-1 as the uncertainty measure for (s, a) and upper bound the error rate by the
k,h
summation of the one-step transition variance of %,h+1/忆,九,@忆,h(s0). However, We can not upper
bound the variance summation of Vk”+1/忆,九,@忆,h(s0) in H steps by O(H2) similarly by the law of
total variance, since {Vk,h,sk,h,ak,h}h∈[H] is not the value functions induced from the same policy
and transition dynamics. To tackle the above problem, we need to define the exploration-driven
reward and the confidence bonus for each state-action pair in a more refined way. Our basic idea is
to carefully measure the expected total uncertainty along the whole trajectories w.r.t. each MDPs in
the confidence set. We will explain the detail in the following subsections.
5.1 Algorithm
Algorithm 2 Reward-free Exploration: Exploration Phase
Input: Failure probability δ > 0 and target accuracy e > 0
λ 一 B-2, β - 16,dlog(1 + KH2∕(dλ)) log(32K2H∕δ) + √λB
β - 16dpiog(1 + KH2∕(dλ)) log(32K2H∕δ) + √λB
β ― 16H 2 Pd log(1 + KH 4∕(dλ))log(32K 2H∕δ) + √λB
5: Set Λi,k,h 一 λI, θi,k,h 一 0 for k =1,h ∈ [H],i = 1,2,3,4, 5
Set U1,h to be the set containing all the θh that makes Ph well-defined, h ∈ [H].
for episode k = 1,2,…，K do
Calculate ∏k,京k, Rk = argmax∏ ∑ ∈u	R VknIP(si, R), where V is defined in Eqn 7.
π,θh ∈Uk,h ,R k,1
for step h = 1, 2,…，H do
10:	Take action according to the policy ∏k,h and observe sk,h+ι 〜Ph(∙∣sk,h, ak,h)
end for
for step h = 1, 2,…，H do
Update {Λi,k+1,h}i5=1 using Eqn 67, 68, 71, 72 and 77
Update the model estimation {θi,k+1,h }i5=1 using Eqn 69, 70, 73, 74 and 78
15:	Add the constraints (Eqn 3) to the confidence set Uk,h, and obtain Uk+1,h
end for
end for
Output: {Pκ,h(∙∣∙, ∙)=京K,hΦ(∙, ∙, •)}%_].
Our algorithm is described in Algorithm 2. Ata high level, Algorithm 2 maintains a high-confidence
set Uk,h for the real parameter θh in each episode k, and calculates an optimistic value function
kB,
VkIP(si, R) for any reward function R and transition Ph = θ>φ with θh ∈ Uk,h. Roughly speak-
∏ P
ing, the value function Vkπ,i,P(si, R) measures the expected uncertainty along the whole trajectory
induced by policy π in the MDP with transition P and reward R. To collect more “informative”
samples and minimize the worst-case uncertainty over all possible transition dynamics and reward
P
function, we calculate πk = arg maxπ maxθP ∈U ,R Vk,i, (si, R), and execute the policy πk to
collect more data in episode k. To ensure that the model estimation θk+i,h is close to the true model
θh w.r.t. features φV (s, a) of different value functions V, we use the samples collected so far to cal-
culate five model estimation {θi,k,h}i5=i and the corresponding constraints at the end of the episode
k. Each constraint is an ellipsoid in the parameter space centered at the parameter estimation θi,k,h
with covariance matrix Λi,k,h and radius βi, i.e.
IM- θi,k,hhi,k,h ≤ βi.	(3)
We update Uk+i,h by adding these constraints to the the confidence set Uk,h.
P
The remaining problems are how to define the value function Vkπ,i,P(si, R) that represents the ex-
pected uncertainty for policy ∏, transition P and reward R, and how to update the model estimation
θi,k,h and the confidence set Uk,h .
7
Published as a conference paper at ICLR 2022
Uncertainty Measure Instead of using maxV kφV (s, a)kΛ-1 to measure the maximum uncer-
k,h
tainty for the state-action pair (s, a) in Algorithm 1, we separately define the uncertainty along the
trajectories induced by different policy ∏, reward function R and transition dynamics Ph = θ>φ.
Specifically, Recall that VV∏,P (s, R) is the value function of policy ∏ in the MDP model with transi-
tion P and reward R. We define the following exploration-driven reward for transition P, reward R
and policy π:
UtPh(S,a® = β Xφ(s,a, s'')Vh+P(s0,R)	.	⑷
s0	(Λ1,k,h)-1
Suppose VVnHP+ι(s,R) = O, ∀s ∈ S, we define the corresponding value function recursively from
step H + 1 to step 1. That is,
, ∀s ∈ S .
(5)
VnhP(s,R) can be regarded as the expected uncertainty along the trajectories induced by policy ∏
for the value function Vhr,P(s, R). In each episode k, we wish to collect samples by executing the
6
policy π that maximizes the uncertainty measure max6∈ufc 九 R Vk ,iP(s, R). However, the definition
6
of V∕k IP(s,R) explicitly depends on the real transition dynamics P, which is unknown to the agent.
66
To solve this problem, we construct an optimistic estimation of VkhP(s, R). We define VkhP(s, R)
6
from step H + 1 to step 1 recursively. Suppose Vkn,H,P+1 (s, R) = 0. We calculate
Uπ,k,h(s,a,R1 = β Xφ(s,a,s'0)vk'h+ι(s,R)	,	⑹
s0	(Λ2,k,h)-1
VnhP(S,R) = min nun；ph(S,nh(s),R) + UwPh(S,πhis)® + PhVnP+ι(s,πh(s), R),H0 ,
(7)
n P6	n P6	n P6
where U2nhhkPhh is the confidence bonus which ensures that the optimism VknhhhP (S, R) ≥ VnhP (s,R)
6
holds with high probability. After calculating VkhP (s,R),we take maximization over all θh, ∈ Uk,h
6
and reward function R, and calculate πk = arg maxn maxθ6 ∈U hR Vknh1hP (S1 , R). We execute πk
to collect more samples in episode k.
Weighted Ridge Regression and Confidence Set In Algorithm 2, we maintain five model estima-
tion {θihkhh}i5=1 and add five constraints to the confidence set Ukhh in each episode. These constraints
can be roughly classified into three categories. The first and the third constraints are applied to en-
n P6	n P6
sure that PkhhVhn+hP1 (S, a, R) is an accurate estimation of PkhhVhn+hP1 (S, a, R) for any reward function
R and Pkhh = θk>hhφ satisfying θkhh ∈ Ukhh, while the second and the forth constraints are used
n P6	n P6
to guarantee that Pkhh Vhn+hP1 (S, a, R) is an accurate estimation of Ph Vhn+hP1 (S, a, R) for any reward
function R and Pkhh = θk>hhφ satisfying θkhh ∈ Ukhh. The last constraint is applied due to technical
issue and will be explained in Appendix B.2. In this subsection, we introduce the basic idea behind
the construction of the first and the third constraints. The second and the forth constraints follow
6
the same idea but consider the different value function Vhn+hP1 (S, a, R). The formal definition of the
parameters {θihkhh}i5=1 and the constraints are deferred to Appendix B.2.
n P6
For notation convenience, we use Vkhh(S) as a shorthand of VknhkhhPk (S, Rk) in this part. The con-
struction of our confidence sets is inspired by a recent algorithm called UCRL-VTR+ proposed
by Zhou et al. (2020a). Recall that we use (Skh, akh) to denote the state-action pair that the agent
8
Published as a conference paper at ICLR 2022
encounters at step h in episode k. We use the following ridge regression estimator to calculate the
corresponding θ1,k,h in episode k:
k-1
θ1,k,h =argmin λkθk2 + V [θ>φVt,h + ι∕σι,t,h (st,h,at,h) - Vt,h+1(St,h+1,Rt "σ1,t,h],⑻
θ∈Rd	t=1
where σ2,k,h = max {H2∕d, [Vk,hVk,h+ι](sk,h, ak,h) + Eι,k,h} is an optimistic estimation of the
one-step transition variance:
NhVk,h+L(sk,h, ak,h) = Es0~Ph (∙∣sk,h ,ak,h) |^(Vk,h+1 (s ) - PhVk,h(s k,h, ak,h)) ] .
In the definition of σ2 k h, Vk,hVk,h+ι(sk,h, ak,h ) is an empirical estimation for the variance
VhVk,h+ι(sk,h, ak,h), and Eι,k,h is a bonus term defined in Eqn. 61 which ensures that σ%h is
an optimistic estimation of VhVk,h+1 (Sk,h, ak,h). One technical issue here is how to estimate the
variance VhVk,h+1 (sk,h, ak,h). Recall that by definition,
VhVk,h+1 (sk,h, ak,h ) =PhVk,h+1 (sk,h , ak,h) - [PhVk,h+1 (sk,h , ak,h)]
=θh φvk2,h+1 (sk,h, ak,h) - [θh φvk,h+1 (sk,h, ak,h)] .
We use 优hφV2,h+1 (Sk,h,ak,h) - hθ>hφVk,h+ι (Sk,h,ak,h 彳	as our VarianCe
Vk,hVk,h+ι(sk,h,ak,h), where θk,h ∈ Uk,h is the parameter which maximizes
~
-rD,-、.^ 一一	一 二 一 ,	..
Vki (sι,R) in episode k. To ensure that V k,hVk,h+ι(sk,h, ak,h) is an accurate estimator, we main-
tain another parameter estimation θ3,k,h using history samples w.r.t. the feature φv2 九十 ι(sk,h, ak,h).
k-	2
θ3,k,h = argmin λkθk2 + X [θ>φVt2h+1 (st,h,at,h) - V2h+1(st,h+1)].
(9)
(10)
estimator
the value
(11)
After calculating θ1,k,h and θ3,k,h, we add the first and the third constraints (Eqn. 3) to the
confidence set Uk,h, where Λ1,k,h and Λ3,k,h is the corresponding covariance matrix of all
history samples, i.e. Λ1,k,h = Pk-IL σ-2,hφvt,h+ι (st,h, at,h)ΦVt,h+1 (St,h,at,h) and Λ3,k,h =
Ptk=-11 φvt2,h+1(st,h,at,h)φv>2 (st,h, at,h).
5.2 Regret
We present the regret upper bound of Algorithm 2 in Theorem 2. In the regime where d ≥ H and
E ≤ H∕√d, we can obtain O(d2H3∕e2) sample complexity upper bound, which matches the sample
complexity lower bound except logarithmic factors.
Theorem 2. With probability at least 1 - δ, after collecting K = O
~
d2H 3+dH4
d2∙5H2+d2H3
+
T2

trajectories, Algorithm 2 returns a transition model PK, then for any given reward in the planning
phase, a policy returned by any Eopt-optimal plug-in solver on (S, A, PK, R, H, ν) is O(E + Eopt)-
optimal for the true MDP, (S, A, P, R, H, ν).
6	Conclusion
This paper studies the sample complexity of plug-in solver approach for reward-free reinforcement
learning. We propose a statistically efficient algorithm with sample complexity O (d2H4∕e2).
We further refine the complexity by providing an another algorithm with sample complexity
O (d2H3∕e2) in certain parameter regimes. To the best of our knowledge, this is the first mini-
max sample complexity bound for reward-free exploration with linear function approximation. As
a side note, our approaches provide an efficient learning method for the RL model representation,
which preserves values and policies for any other down-stream tasks (specified by different rewards).
Our sample complexity bound matches the lower bound only when d ≥ H and E ≤ H∕√d. It is
unclear whether minimax rate could be obtained in a more broader parameter regimes. We plan to
address this issue in the future work.
9
Published as a conference paper at ICLR 2022
7	Acknowledgments
Liwei Wang was supported by National Key R&D Program of China (2018YFB1402600), Ex-
ploratory Research Project of Zhejiang Lab (No. 2022RC0AN02), BJNSF (L172037), Project
2020BD006 supported by PKUBaidu Fund.
References
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic
bandits. In NIPS, volume 11,pp. 2312-2320, 2011.
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory, pp. 67-83. PMLR, 2020.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463-474. PMLR, 2020.
Mohammad GheShIaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272. PMLR, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In Inter-
national Conference on Machine Learning, pp. 551-560. PMLR, 2020.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy opti-
mization. In International Conference on Machine Learning, pp. 1283-1294. PMLR, 2020.
Qiwen Cui and Lin F Yang. Is plug-in solver sample-efficient for feature-based reinforcement learn-
ing? arXiv preprint arXiv:2010.05673, 2020.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient
for sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-
textual decision processes with low bellman rank are pac-learnable. In International Conference
on Machine Learning, pp. 1704-1713. PMLR, 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably effi-
cient? arXiv preprint arXiv:1807.03765, 2018.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration
for reinforcement learning. In International Conference on Machine Learning, pp. 4870-4879.
PMLR, 2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020b.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Emilie Kaufmann, Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Edouard Leurent,
and Michal Valko. Adaptive reward-free exploration. In Algorithmic Learning Theory, pp. 865-
891. PMLR, 2021.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-
based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.
10
Published as a conference paper at ICLR 2022
Tor Lattimore and Marcus Hutter. Pac bounds for discounted mdps. In International Conference on
Algorithmic Learning Theory, pp. 320-334. Springer, 2012.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforce-
ment learning with self-play. In International Conference on Machine Learning, pp. 7001-7010.
PMLR, 2021.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=BJe1E2R5KX.
Pierre Menard, Omar Darwiche Domingues, Anders Jonsson, Emilie Kaufmann, EdoUard Leurent,
and Michal Valko. Fast active learning for pure exploration in reinforcement learning. arXiv
preprint arXiv:2007.13442, 2020.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pp. 2010-2020. PMLR, 2020.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In International Conference on Machine Learning, pp. 2778-2787.
PMLR, 2017.
Shuang Qiu, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. On reward-free rl with kernel and neural
function approximations: Single-agent mdp and markov game. In International Conference on
Machine Learning, pp. 8737-8747. PMLR, 2021.
Zhiyuan Ren and Bruce H Krogh. State aggregation in markov decision processes. In Proceedings
of the 41st IEEE Conference on Decision and Control, 2002., volume 4, pp. 3819-3824. IEEE,
2002.
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state
aggregation. Advances in neural information processing systems, pp. 361-368, 1995.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2006.11274, 2020.
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi
Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforce-
ment learning. arXiv preprint arXiv:1907.02057, 2019.
Gellert Weisz, Philip Amortila, and Csaba Szepesvari. Exponential lower bounds for planning in
mdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory,
pp. 1237-1264. PMLR, 2021.
Jingfeng Wu, Vladimir Braverman, and Lin F Yang. Gap-dependent unsupervised exploration for
reinforcement learning. arXiv preprint arXiv:2108.05439, 2021a.
Yue Wu, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal regret for learning
infinite-horizon average-reward mdps with linear function approximation. arXiv preprint
arXiv:2102.07301, 2021b.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995-7004. PMLR, 2019.
Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. In International Conference on Machine Learning, pp. 10746-10756. PMLR, 2020.
11
Published as a conference paper at ICLR 2022
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint
arXiv:2005.13239, 2020.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp.10978-10989. PMLR, 2020a.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Provably efficient
reward-agnostic navigation with linear value iteration. arXiv preprint arXiv:2008.07737, 2020b.
Weitong Zhang, Dongruo Zhou, and Quanquan Gu. Reward-free model-based reinforcement learn-
ing with linear function approximation. arXiv preprint arXiv:2110.06394, 2021a.
Xuezhou Zhang, Adish Singla, et al. Task-agnostic exploration in reinforcement learning. arXiv
preprint arXiv:2006.09497, 2020.
Zihan Zhang, Simon Du, and Xiangyang Ji. Near optimal reward-free reinforcement learning. In
International Conference on Machine Learning, pp. 12402-12412. PMLR, 2021b.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture markov decision processes. arXiv preprint arXiv:2012.08507, 2020a.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for dis-
counted mdps with feature mapping. arXiv preprint arXiv:2006.13165, 2020b.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for dis-
counted mdps with feature mapping. In International Conference on Machine Learning, pp.
12793-12802. PMLR, 2021.
12
Published as a conference paper at ICLR 2022
A Omitted Details in Section 4
A. 1 Notations
In this subsection, we summarize the notations used in Section A.
Symbol	Explanation
E1 Sk,h , ak,h S .. Vk,h+1,s,a φk,h (S, a) θk,h Λk,h uk,h (S, a) Rk,h (S, a) Qk,h (S, a) Vk,h(S) VSh*(S,R) QS*h (S, R) VSh (S, R) θSh Pk,h(s0∣s,a) 0 Ph (S0|S, a)	The high-probability event for Theorem 1 The state and action that the agent encounters in episode k and step h The value function with maximum uncertainty: arg maxV ∈V kPs0 φ(S, a, S0)V(S0)k(Λ )-1 Ps0 φ(S, a, S0)VSk,h+1,s,a(S0) The estimation of θh in episode k: (Λk,h )-1 Ptk=-11 φt,h (St,h , at,h )VSt,h+1,s,a(St,h+1) The covariance matrix in (k, h): Pk-IL φt,h(st,h, at,h)Φt,h(st,h, at,h)> + λI The uncertainty measure: β φk,h (S, a)> (Λk,h )-1 φk,h (S, a) The exploration-driven reward which equals uk,h (S, a) The Q function defined in line 12 of Algorithm 1 The value function defined in line 13 of Algorithm 1 The value function defined in Eqn 12 The Q value similarly defined as VSh* (S, R) The value function of policy π similarly defined as Vh* (S, R) The parameter estimation returned at the end of the exploration phase θ>,hφ(s, a, SO) θ>φ(s,a, SO)	
A.2 Proof Overview
■‰ τ	i ∙ ∕ι	1 ∙ .t	♦ ♦ ∙> ♦ ,ι	i' τ-,∙ . 1	∙ . 1	.1	ι i'	-r'r⅛ / C∖
Now We briefly explain the main idea m the proof. Firstly, We introduce the value function 以(s, R),
which is recursively defined from step H + 1 to step 1:
~ ,
VH +ι(s,R)=0,∀s ∈S
VhnS,R) =maχ{min ∣Rh(s,a) + PhV+1(s, a, R), h}} ,∀s ∈ S,h ∈ [H]
(12)
(13)
Compared with the definition of Vhc(s, R), the main difference is that we take minimization over
the value and H at each step. We state the folloWing lemma, Which gives an upper bound on the
sub-optimality gap of ∏ in the planning phase.
Lemma 1. With probability at least 1 一 δ, the sub-optimality gap of the policy Πr for any reward
∏	.^	∏^	1	-	1	. ■ π^	TTnE
function R in the planning phase satisfies V1*(s1, R) — % R (sι,R) ≤ 4VJ(sι, RK) + EoPt, where
RK is the exploration-driven reward used for episode K in the exploration phase.
This lemma connects the sub-optimality gap with the value function of the auxiliary reward in
episode K. So the remaining problem is how to upper bound %*(sι, RK). Since the exploration-
driven reward Rk is non-increasing w.r.t. k, it is not hard to prove that K%*(si,Rk ) ≤
PK=I Vι* (sι, Rk). We use the following two lemmas to upper bound PK=I Vl* (s 1, Rk).
Lemma 2. With probability at least 1 一 δ, Vh*(S, Rk) ≤ Vk,h (S) holds for any (S, a) ∈ S × A, h ∈
[H] andk ∈ [K].
Lemma 3. With probability at least 1 一 δ,
K
X Vkj(SI) ≤ 6H2dPKlog(4H3KB2/δ)log(1 + KH2B2/d).
k=1
With the help of the optimistic bonus term uk,h , we can prove that the estimation value Vk,h is always
optimistic w.r.t VV*, which is illustrated in Lemma 2. Therefore, we have PK=I V1*(s1,Rk) ≤
PkK=1 Vk,1(S1). By adapting the regret analysis in the standard RL setting to the reward-free setting,
we can upper bound the summation of Vk,1(S1) in Lemma 3. Combining the above two lemmas, we
derive the upper bound of V* (si, RK), then we bound the sub-optimality gap of ∏ by Lemma 1.
13
Published as a conference paper at ICLR 2022
A.3 High-probability Events
We firstly state the following high-probability events.
Lemma 4. With probability at least 1-δ∕2, thefollowing inequalityholdsforany k ∈ [K ],h ∈ [H ]:
kθk,h - θhkΛk,h ≤ β.	(14)
Proof. For some step h ∈ [H], the agent selects an action with feature xk,h = φk,h(sk,h, ak,h). The
noise ηk,h = 五,九风,九心,九(s0) - θ>Φk,h(sk,h, ak,h) satisfies H-SUb-GaUssian. By Lemma 20, the
following inequality holds with probability 1 -翕：
kθk,h - θhkΛk,h ≤ HJdlog( ʌʌ ) + λ1∕2B∙
λδ
By taking Union boUnd over all h ∈ [H], we can prove the lemma.
(15)
□
Lemma 5. With probability at least 1 — δ∕2, we have
K H
XX
(PhVk,h+ι(Sk,h,ak,G - Vk,h，+i(sk,h，+i)) ≤ p2H3Klog(4∕δ).
k=1 h=1
(16)
Proof. This lemma follows directly by Azuma,s inequality.	□
DUring the following analysis, we denote the high-probability events defined in Lemma 4 and
Lemma 5 as E1 .
A.4 Proof of Lemma 1
Lemma 6. (Restatement of Lemma 1) Under event Ei, the sub-optimality gap of the policy Πr for
any reward function R in the planning phase can be bounded by
%*(S1, R)- VjnR(si, R) ≤ 4%*(sι, RK) +	(17)
Proof.
V*(sι,R)- VnR(Si,R)	(18)
k* D	* D
=(½*(S1,R) - ViπR,P(sι,Rη + (¼πR,P(S1,R) - ViπR(sι,R)) + (%πR,p(sι,R) - ¼πR,P(sι,R))
(19)
≤ (%*(sι, R) -印R,P(si, R)) + (VnR,P(sι,R) - VnR(Sι, R)) + e0pt.	(20)
The inequality is because that the policy Πr is the EoPt-OPtimaI policy in the estimated MDP M, i.e.
VinR,P(S1,R) ≤ VinR,P(si, R)+ Eopt .
For the notation convenience, for a certain sequence {Rh}hH=i, we define the function Wh({Rh})
recursively from step H+1 to step 1. Firstly, we define WH+i({Rh}) = 0. Wh({Rh}) is calculated
recursively from Wh+i ({Rh})：
Wh({Rh}) = min {H, Rh + Wh+i({Rh})}.	(21)
Similarly with the definition of Vh= (s, R), We introduce the value function Vn (s, R), which is recur-
sively defined from step H + 1 to step 1：
~__ _, _
VH+i(s,R)=0,∀s ∈S	(22)
Vn(s, R)	= {min	{rh(s,π(s)) +	PhVh+i(s, π(s),	R),	H}}	,∀s	∈S, h ∈	[H].	(23)
14
Published as a conference paper at ICLR 2022
We use traj 〜(π, P) to indicate that the trajectory {sh, ah}H=ι is sampled from transition P with
policy π . For any policy π , we have
Esi〜μ (y∏,P(sι,R)- V1π(si))∣
Etraj〜(π,P)Wi ({(Ph - Ph)Vh+PK (sh,ah,R)})∣
Etraj〜(π,P) Wi ({(θh - θh) X φ(sh, ah, s0)V+P (s0, R)})
≤Etraj〜(π,P)Wl K Wh — θh∣k…
X~Λ	,.八 L 己.，
Eφ(sh,ah,s )Vh+1(S ,R)
s0
≤Etraj 〜(π,P )Wi I <2β X φ(sh ,ah,s0)V+P (s0,R)
∖ [	s0	(ΛK,h)-i
≤2Etraj〜(π,P)W 1 ({uΚ,h(sh, ah)})
~ 一. .
=2Vn (si,Rk )
≤2V1 (si,Rk ).
(ΛK,h)-1
(24)
(25)
(26)
(27)
(28)
(29)
(30)
(31)
〜
The second inequality is due to lemma 4 and the definition of θ. Plugging this inequality back to
Inq. 18, we can prove the lemma.	□
A.5 Proof of Lemma 2
Lemma 7. (Restatement ofLemma2) Under event Ei, Vh(s,Rk) ≤ Vkh(S) holds for any (s,a) ∈
S × A,h ∈ [H] andk ∈ [K]. .
Proof. We prove the lemma by induction. Suppose Vh+ι(s,Rk) ≤ Vk,h+ι(s),
K*
Qh (S, a, Rk) - Qk,h (S, a)
≤ - uk,h(s,	a)	+	(θk,h - θk,h)	∙ ^X φ(S, a,	S0)Vk,h (SO)	+ Ph(V*+i	- Vk,h+1)(s, a)
≤ - Uk,h(S,	a)	+	(θk,h - θk,h)	∙ X Φ(s, a,	S0)Vk,h (s)
≤ - uk,h(S, a) + Uθk,h - θk,h llʌ
φ(S, a, S0)Vk,h (S0)
s0	l(Λk,h)-1
≤ - Uk,h(S,a) + ∣θk,h - θk,h∣∖	max
φ(S, a, S0)V(S0)ll
s0	l(Λk,h)-1
≤ - uk,h(S, a) + βmax ll φ(S, a, S0)V(S0)ll
∈ l s0	l(Λk,h)-1
=0.
(32)
(33)
(34)
(35)
(36)
(37)
(38)
The first inequality is due to induction condition Vh*+i (S, Rk) ≤ Vk,h+i(S). The last inequality
is due to Lemma 4. Since Q*h(S, a) ≤ Qk,h(S, a, Rk) for any a ∈ A, we have Vh* (S, Rk) ≤
Vk,h(S).	口
A.6 Proof of Lemma 3
Lemma 8. (Restatement of Lemma 3) Under event Ei,	PkK=i Vk,i(Sk,i)	≤
6H2 dp K log(4H3 KB2/δ) log(1 + KH 2B2/d).	，，
15
Published as a conference paper at ICLR 2022
Proof.
Qk,h (sk,h, ak,h)
(39)
min {h, Rk,h(sk,a, ak,h) + uk,h(sk,a, ak,h) + Pyk,hVk,h+ι(sk,h, ak,h)}
≤ min
≤ min
{h, 2β kφk,h(Sk,h, ak,h)l∣Λ-1 + Pk,hVk,h+l(sk,h,。3%)}
{h, 2β kφk,h(Sk,h, ak,h)l∣Λ-1 } + min {h, (Pk,h - Ph) Vkh+l(sk,h, ak,h)}
(40)
(41)
(42)
+ (PhVk,h+1(sk,h, ak,h) - Vk,h+1 (sk,h+1)) + Vk,h+1 (sk,h+1)
(43)
≤3β min 1, kφk,h(sk,h, ak,h)kΛ-1 + (PhVk,h+1(sk,h, ak,h) - Vk,h+1(sk,h+1)) + Vk,h+1(sk,h+1).
k,h
(44)
The last inequality is from
—
Ph Vk,h+1 (sk,h, ak,h) =(θk,h - θh)φVk,h+1 (sk,h , ak,h )
≤ llθk,h - θh∣l	llφVk,h+ι (sk,h,ak,h)∣∣Λ-ι
Λk,h	k,h
≤β lφVk,h+1 (sk,h, ak,h)lΛ-1
k,h
≤β kφk,h(sk,h, ak,h)kΛ-1 .
k,h
(45)
(46)
(47)
(48)
From Inq 39, we have
K	KH
XVk,1(s1)=XX3βmin 1, kφk,h(sk,h, ak,h)kΛ-1	(49)
KH
+ ΣΣ(PhVk,h+1(sk,h, ak,h) - Vk,h+1(sk,h+1)) .	(50)
k=1 h=1
For the first term, by Lemma 21, we have for any h ∈ [H],
Xmin 1, kφk,h(sk,h, ak,h)kΛ-1 ≤tuK X min 1, kφk,h(sk,h, ak,h)k2Λ-1
≤P2dK log(1 + KH 2∕(dλ)).
(51)
(52)
Since λ = B-2 , we have
KH,	、
XX 3β min{l,kφk,h(sk,h ,ak,h)h-j ≤ 3H 2d,2K log(4H 3KB2∕δ) log(1 + KH 2B2∕d).
k=1 h=1	,
(53)
For the second term, by Lemma 5,
K H
XX
(PhVk,h+ι(sk,h,ak,h) — Vk,h+1(sk,h+1)) ≤ p2H3Klog(4∕δ).	(54)
k=1 h=1
Therefore, we have
K
X Vk,1(sk,1) ≤ 6H2dPKlog(4H3KB2∕δ)log(1 + KH2B2∕d).	(55)
k=1
□
Lemma 9. Underevent Ei, V>ι*(sι,Rκ) ≤ 6H 12dqlog(4H3KB2/6Kg(1+KH2B2^.
16
Published as a conference paper at ICLR 2022
>Λ Z- 1~t ∙	τV⅛ /	C ∖∕T7^	/	∖ 1	1	r	1
Proof. Since VJ(sk,ι,Rk) ≤ Vk,1(sk,1) by lemma 7, we have
K	K
X Vι(sι,Rk) ≤ X Vk,1(s1) ≤ 6H2dPKlog(4H3KB2∕δ) log(1 + KH2B2∕d).	(56)
By the definition of Rk, we know that Rk(s, a) is non-increasing w.r.t k. Therefore,
K	K
X Vl*(si,Rk) ≤ X V*(sι,Rk) ≤ 6HHZKlog(4H3KB2∕δ) log(1 + KH2B2∕d). (57)
The lemma is proved by dividing both sides by K.	□
A.7 Proof of Theorem 1
Combining the results in Lemma 9 and Lemma 6, we know that for any reward function R,
l∕" R∏ v^l D	2zJlog(4H3KB2∕δ) log(1 + KH2B2∕d)
Vι (sι,R) - Vι (sι,R) ≤ 24H dy-------------------K------------------+ eopt.	(58)
Choosing K = C1H4"2 log(4H3KB：/6log(1+KH2B2/d)Some constant Cl suffices to guarantee
that Vl* (sl, R) - VlπR (sl, R) ≤ + opt.
B Omitted Details in Section 5
B.1	Notations
In this subsection, we summarize the notations used in Section B.
Symbol	Explanation
The high-probability event for Theorem 2
The state and action that the agent encounters in episode k and step h
Ps0 φ(s, a, s0)V π,P (s0, R) for certain value function V
The value function of policy π in the MDP model with transition P and reward R
The expected uncertainty along the trajectories induced by policy ∏ for V∏,P (s, R) (Defined in Eqn 5)
D
The optimistic estimation of Vkj(s, R) (Defined in Eqn 7)
The exploration-driven reward for transition P , reward R and policy π (Defined in Eqn 4)
The confidence bonus ensuring the optimism VnrhP(s, R) ≥ VnhP(s,R) (Defined in Eqn 6)
The one-step transition variance w.r.t. certain value function V
The empirical variance estimation w.r.t. the value VV∏k,Pk (s, RQ (Defined in Eqn 59)
The empirical variance estimation w.r.t. the value Vhπk,Pk (s, Rk) (Defined in Eqn 60)
The confidence bonus for the variance estimation Vι,k,h (s, a) and VH,k,h (s, a), respectively
The optimistic variance estimation for Vι,k,h (s, a) and VH,k,h (s, a), respectively
The parameter estimation w.r.t. certain value function (Defined in Section B.2)
The empirical covariance matrix w.r.t. certain value function (Defined in Section B.2)
The confidence set containing θh with high probability
D
arg maxπ,θDh∈Uk,h,R Vk,l, (sl,R)
The “value function” for the MDP with transition Pk and reward Vι,k,h
log(32K2H∕δ) log2(1 + KH4B2)	''
B.2 Omitted Details of Algorithm 2
In algorithm 2, We maintain five different parameter estimation θι,k,h, θH,k,h, θ3,k,h, θ4,k,h
and θ5,k,h, and the corresponding covariance matrix Λι,k,h, ΛH,k,h, Λ3,k,h Λ4,k,h and Λ5,k,h
E2
sk,h, ak,h
φV (s, a, R)
Vhπ,P (s,R)
VnhP (s,R)
Vk,h,PD(s,R)
uln,,kP,h(s,a,R)
u2P,,kn,h(s,a,R)
VhV(s, a)
Vι,k,h (s,a)
VH,k,h (s, a)
El,k,h, E2,k,h
σ2,k,h, σ2,k,h
θi,k,h
Λi,k,h
Uk,h
πk , θk , Rk
~ ,.
K,h(s)
τ
17
Published as a conference paper at ICLR 2022
θ1,k,h, θ2,k,h are the parameter estimation using history samples w.r.t the features of variance-
normalized value function Vn普/σ1,fcjh and V∏h,P/σ2,k,h, respectively. θ3,k,h,θ4,k,h are the
22
VV∏h,Pk ) and (Vknh+k).
θ5,k,h is somewhat technical and will be explained later. for any i ∈ [5] and h ∈ [H],百」,h are
initialized as 0, and Λi,1,h are initialized as λI in Algorithm 2. U1,h is the set containing all the θh
that makes Ph well-defined, i.e. £§,京>φ(s0∣s, a) = 1 and 京>φ(s0∣s, a) ≥ 0,∀s, a.
After observing {sk,h, ak,h}H=1 in episode k, we calculate Vι,k,h (s,a) and V2,k,h (s, a) as the
corresponding variance in the empirical MDP with transition dynamics Pk.
VW(S,a) =θ>h Xφ(S,a,s0) (Vnh+k(S0,Rk)) -
s0
V2,k,h(s,a) =θ>h X φ(s,a,s0)(琮h⅛ (s0,Rk))2 -
s0
2
优h X Φ(s,a,s0)%h +k (s0,Rk),
s0
(59)
优 h X Φ(s,a,s0)啜h +k (s0,Rk).
s0
(60)
To guarantee the variance estimation is optimistic, we calculate the confidence bonus E1,k,h and
E2,k,h for variance estimation V 1,k,h (sk,h, ak,h) and V2,k,h (sk,h, ak,h):
Eι,k,h =min ∣H2, 4Hβk
+ min IH2, 2βk
E2,k,h =min|H2, 4Hβk
+ min I H2, 2βk
X Φ(sk,h,ak,h,s'Nk1hP (s0,Rk)
s0
X φ(sk,h,ak,h,s0)(琛h号(s0,Rk))2]
s0	(Λ3,k,h)-1
X φ(sk,h, ak,h, s^)Vk,h+l (S0,Rk)	;
s0	(Λ2,k,h)-1 
X φ(sk,h,ak,h,s0)(啜h号(S,Rk))2]
s0	(Λ4,k,h)-1
(61)
(62)
(63)
(64)
We add the bonuses to V ι,k,h(sk,h, ak,h) and V 2,k,h(sk,h, ak,h) and maintain the optimistic variance
estimation σ2 k h and σ^ k h
σ1,k,h = max {H /d, V 1,k,h(sk,h, ak,h ) + E 1,k,h },
σ2,k,h = max {H2/d, V2,k,h(sk,h, ak,h) + E2,k,h}.
(65)
(66)
We use σi,k,h to normalize the value obtained in each step. By setting λ = B-2 and solving the
ridge regression defined in Eqn. 8, we know that θi,k+1,h and Λi,k+1,h are updated in the following
18
Published as a conference paper at ICLR 2022
way:
a” + (σ1,k,h)-2 (X φ(sk,h,ak,h '"f0Rk )a Mss2a2sE (s0,Rk))
(67)
…h=Ai,k,h+…-2(X φ(sk,h,ak,h hd'Rk)) (XX……)啜陪⑸ R)
(68)
θ1,k+1,h = (Λι,k+ι,h)-1 XX(σι,t,h)-2 (XΦ(st,h,at,h,s0)V∏h+Pt(s0,Rt)) Vnh£(Sg,Rt),
t=1	s	(69)
θ2,k + 1,h = (A2,k+1,h厂1 X(σ2,t,h)-2 (X φ(st,h, at,h, s'”h+l(s', Rt' vf,hP1(st,h+ι, Rt)
t=1	s	(70)
Similarly, we update the estimation w.r.t (VV∏h+k(s0,Rk))2 and (以h+Pk(s0,Rk))2
H'k'h + (X ……)(Vknh+k (S0,Rk )
Λ4,k+1,h = Λ4,k,h +	X φ(sk,h, ak,h, s0) Vkπ,kh,+P1k (s0, Rk)
θ3,k+1,h = (Λ3,k+1,h)-1Xk	X
φ(st,h,at,h,s0) (Vnh+Pt(s0,Rt))
t=1	s0
φ(sk,h, ak,h, s0) (Vnh+Pk (S, Rk )))
(71)
φ(sk,h,ak,h,s') (VnhP (s0,Rk)))
(72)
I (Vnh 查(st,h+1,Rt)f ,
θ4,k+1,h = (Λ4,k+1,h)	φ(st,h, at,h, s )
t=1	s0
Pt(S0,Rt)) 2
(73)
(Vtnh+1 (st,h+ι,Rt))2.
(74)
~
~
We define Yk,h to be the “value function" with transition Pk and reward Vι,k,h.
^Yk,H+1(s) = 0,
Yk,h(s) = V1,k,h(s, πk(s)) + Pk,hYk,h+1(s, πk(s)).
(75)
(76)
~
ʌ
θ5,k,h is the parameter estimation using samples w.r.t. Yk,h(s)
>
Λ5,k+1,h = Λ5,k,h + X φ(Sk,h, ak,h , S )Yk,h+1
00
φ(sk,h, ak,h, s )Yk,h+1(s )
(77)
θ5,k+1,h = (Λ5,k+1,h)	X X φ(St,h , at,h , S )Yt,h+1 (S )	Yt,h+1 (Sk,h+1 ).
(78)
19
Published as a conference paper at ICLR 2022
We update the high-confidence set Uk,h by adding following five constraints:
..~	O	,.	ʌ
kθh - θ1,k,hkΛ1,k,h ≤ β,	(79)
..~	O	,.	ʌ
kθh - θ2,k,hkΛ2,k,h ≤ β,	(80)
..~	O	,.	≈
kθh - θ3,k,hkΛ3,k,h ≤ β,	(81)
..~	O	,.	≈
kθh - θ4,k,hkΛ4,k,h ≤ β,	(82)
..~	O	,.	≈
kθh - θ5,k,hkΛ5,k,h ≤ β.	(83)
Note that we add new constraints to this confidence set Uk,h in each episode, instead of updating Uk,h
as the intersection of the new constraints. This operation is designed to ensure that the cardinality
6
of the confidence set Uk,h is non-increasing w.r.t. the episode k, so that Vkπ,1,P (s1, R) is always
non-increasing w.r.t. k.
During the proof, we use φV π,P (s, a, R) as a shorthand of Ps0 φ(s, a, s0)V π,P (s0, R).
B.3 High-probability events
For notation convenience, we use V1,k,h(s, a) and V2,k,h(s, a) to denote the one-step transition
variance with regard to VVnh+Pk and Vknh+Pk，i.e.
Vι,k,h(s,a) =	Es0 〜Ph(∙∣s,a)	](%h+Pk	(s0,Rk)- PhVn,Pk (s,a,Rk)) j ,	(84)
V2,k,h(s,a) =	Es，〜Ph(∙∣s,a)](啜h+Pk	(s0,Rk)	- PhVn,Pk (s,a, Rk))I .	(85)
(86)
Lemma 10. With probability at least 1 - δ, the following event holds for any h ∈ [H], k ∈ [K]:
M，h - θh∣∣λ	≤	2β,	(87)
∣∣0k,h - θh∣h 上	≤	2β,	(88)
IV 1,k,h(Sk,h, ak,h)	-	V1,k,h(Sk,h, ak,h)∣	≤	E1,k,h,	(89)
∖V2,k,h(sk,h, ak,h)	-	V2,k,h(sk,h, ak,h)∣	≤	E2,k,h,	(90)
∣∣%h - θh∣h 上	≤	2β.	(91)
Proof. We firstly prove the first and the third inequality, the second and the fourth inequality can be
proved following the same idea. By the definition of Vι,k,h(sk,h, ak,h) and Vι,k,h(sk,h, ak,h):
∣V1,k,h (Sk,h , ak,h ) - V1,k,h (Sk,h , ak,h )∣
≤ min {h2,2H ∣∣θk,h - θh
+ min {h2, ∣∣θik,h - θh∣∣
Λ1,k,h
Λ3,k,h
φV^πk,pk (sk,h, ak,h, Rk )
(Λ1,k,h)-1
φ(V ∏k,Pk )2 (Sk,h,ak,h,Rk)
.
(Λ3,k,h)-1
(92)
(93)
(94)
Let Xk = (σι,k,h)-1Φv∏k,Pk (sk,h,ak,h),andthenoise
k,h+1
ηk = (σ1,k,h)	Vk,h+1 (sk,h+1, Rk ) - (σ1,k,h)	φ φ-^；k，Pk (Sk,h, ak,h, Rk ) ,θh) .
Since σι,k,h ≥，H2/d, We have ∣∣Xk ∣∣2 ≤ √d, E [η2 | Gk] ≤ d. By Lemma 22, We have with Prob
at least 1 - δ∕(8H), for all k ≤ K,
∣∣θh - θι,k,h∣k 石九 ≤ 16dPlog(1 + KH2∕(dλ))log(32K2H∕δ) + √λB = β. (95)
20
Published as a conference paper at ICLR 2022
Similarly, we can prove that
θh - θ3,k,h∣k	≤ 16H2√dlog(1 + KH4∕(dλ)) log(32K2H∕δ) + √λB = β. (96)
Since Rk,h - θι,k,h ∣λ
since | 氏,h - θ3,k,h∣ A
≤ β ≤ β by Inq 79, We have ∣∣θh -京k,h∣1	≤ 2β∙ Similarly,
≤ β by Inq 81, we have ∣∣θh -京k,h∣∖	≤ 2β. Plugging the above
inequalities back to Inq. 92, We have
V1,k,h (sk,h , ak,h ) - V1,k,h (sk,h , ak,h ) ≤ E1,k,h .
(97)
The above inequality indicates that σι,k,h ≥ Vι,k,h, which is an optimistic estimation.
As a result, the variance of the single-step noise ηk = (σι,k,h)-1V∏h+Pk (sk,h+ι,Rk)-
(σι,k,h)-1 φ φV∏k,pk (sk,h, ak,h, Rk) ,θh } satisfies E [r∣2 | Gk] ≤ L By Lemma 22, we can prove
k,h+1
a tighter confidence guarantee for θι,k,h:
∣∣θh - θι,k,h∣∣A ≤ 16,dlog(1 + KH2∕(dλ))log(32K2H∕δ) + √λB = β.	(98)
Combining with ∣θι,k,h - θι,k,h∣∣λ	≤ B by Inq 79, we have ∣∣θh - θι,k,h∣[	≤ 2,β.
Now we prove the last inequality in this lemma. Recall that we define
K,H+ι(s) = 0,	(99)
K,h(s) = V 1,k,h(s,∏k(s)) + Pk,hYk,h+l(s,∏k(s)) .	(100)
Note that Yk,h(s) ≤ H2 by the law of total variance (Lattimore & Hutter, 2012; Azar et al., 2013).
Therefore, the variance on the single-step noise in at most H2 . By Lemma 22, we have
∣∣θh -θ5,k,h∣∣A fc ≤ B.	(101)
COmbiningWith ∣∣θh - 05人八卜	≤ B by Inq 83, we have ∣∣θh - θ5,k,h∣1	≤ 2β.	□
Lemma 11. With probability at least 1 - δ∕2, we have
KH	~	~
XX (PhVnh+Pk(sk,h,ak,h,Rk) - Vnh+Pk(sk,h+ι,ak,h+ι,Rk)) ≤ p2H3Klog(8∕δ), (102)
k=1 h=1
K H
XX Ph
^k,h+ι(sk,h,ak,h,Rk) - Y⅛h+ι(sk,h+ι,ak,h+ι,Rk)) ≤ p2H5Klog(8∕δ). (103)
k=1 h=1
Proof. This lemma follows directly by Azuma’s inequality for martingale difference sequence and
union bound.	□
During the following analysis, we denote the high-probability events defined in Lemma 10 and
Lemma 11 as E2 .
B.4 Proof of Theorem 2
Lemma 12. Under event E2, we have
K
E 喉严(sι,Rk)	(104)
k=1
≤O Q(dH4 + d2H3)Klog(KH∕δ) log2(KH4B2)) + (d2.5H2 + d2H3) log(KH∕δ) log2(KH4B2)
(105)
21
Published as a conference paper at ICLR 2022
This lemma can be proved with the technique for regret analysis. We will explain itin Appendix B.5.
Lemma 13. (Optimism) Under ^vent E2, for any h ∈ [H],k ∈ [K], S ∈ S, π ∈ Π, R and P
satisfying Ph = θ>φ, we have
V∏P(s,R) ≥ Vkh(s,R).	(106)
一 .一	一一 ._	__k D ,	_、	D ,	_、	一
Proof. This lemma is proved by induction. Suppose Vk h+ι(s, R) ≥ Vk h+ι(s, R), We have
Vk∏hp (s,R) - v∏hP (s,R)	(107)
≥u∏,Ph(s,π(s),R) + (Pk,h- Ph)Vk⅛(s,∏(s),R)+ph (Vk⅛ - Vk∏h+ι) (s, π(s), R)
(108)
≥u∏,Ph(s, ∏(s), R) + (Pk,h - Ph)VriP+ι(s, ∏(s), R)	(109)
≥0.
(110)
(111)
一. .一.	_	_ _k D , _、	r- D , _、一 一_-	-	,__.
This indicates that VkhP(s, R) ≥ VkhP(s, R) holds for step h.	口
Lemma 14. under event E?, we have VKKiPK (sι,Rκ) ≤ PkT Vk,K^~("'Rk).
D
Proof. Firstly, we prove that VkIP(s, R) is non-increasing w.r.t k for any fixed π,P, s and R. ThiS
π PD	π PD
can be proved by induction. Suppose for any k1 ≤ k2, Vkπ,,Ph+1(s, R) ≥ Vkπ,,Ph+1(s, R) for any s.
Recall that
VknhP(s, R) = min {un，Ph(s,nh(s),R) + Uπ,Ph(s,πh(S),R) + PhVknh+1(S,πh(S),R),HO .
(112)
π PD	π PD
Since Λ1,k1,h 4 Λ1,k2,h, Λ2,k1,h 4 Λ2,k2,h and Vkπ1,,Ph+1(S,R) ≥ Vkπ2,,Ph+1(S,R) for any S, we can
prove that
DD
u1,,k1,h(S,πh(S),R) ≥ u1,,k2,h(S, πh(S), R),	(113)
DD
u2,,k1,h(S,πh(S),R) ≥ u2,,k2,h(S, πh(S), R),	(114)
PhVk∏'P+ι(s,∏h(s),R) ≥ PhVk2P+1 (s,∏h(s),R).	(115)
DD
Therefore, Vkπ,,Ph (S, R) ≥ Vkπ,,Ph (S, R) holds for step h and any S.
Since the cardinality of the transition set Uk,h is non-increasing (We add more constraints in each
episode), we know that θk2,h ∈ Ukι,h. By the optimality of ∏kι, θkι and Rki in episode kι, we have
DD
Vkπ1k,11,Pk1(S1,Rk1) ≥Vkπ1k,12,Pk2(S1,Rk2).
Combining the above two inequalities, we have
D	DD
Vkπ1k,11,Pk1 (S1, Rk,1) ≥Vkπ1k,21,Pk2(S1,Rk2) ≥Vkπ2k,12,Pk2(S1,Rk2).	(116)
This indicates that the value Vkπ,k1,Pk (S1, Rk) is non-increasing w.r.t. k. Therefore,
K
KVKπ,K1,PDK(S1, RK) ≤XVkπ,1k,PDk(S1,Rk).	(117)
k=1
□
22
Published as a conference paper at ICLR 2022
Lemma 15. Under event E?, the sub-optimality gap ofthe policy Πr returned in the planning phase
for any reward function R can be bounded by
V*(sι, R)- VnR (si, R) ≤ 4%'Pκ (si, RK) + e°pt∙
(118)
>Λ	Γ∙ T-I	11.1	TAT^F∙ . 1	. . 1	1	∕'	CI ♦	. 1	. ∙	. I	1 1 六
Proof. Recall that we use Vhπ to denote the value function of policy π on the estimated model θK .
V*(sι,R)- VnR(Si,R)	(119)
=(v*(sι,R) - VrR,PK(sι,R)) + (VfR,PK(sι,R) - VInR(Si)) + (v∏R,Pκ(si,R) - V∏R,Pκ(sι,R))
(120)
≤V*(sι, R) - VnR,Pκ(sι, R) + VnR,Pκ (sι,R) - VnR(Sι, R) + e°pt∙	(121)
The inequality is because that the policy Πr is the ∈opt-optimal policy in the estimated MDP MM.
For notation convenience, We use traj 〜 (∏,P) to denote that the trajectory ({sh,ah}H=ι) is
sampled with transition P and policy π. For a certain sequence {Rh}hH=i, we define the function
Wh({Rh}) recursively from step H + 1 to step 1. Firstly, We define WH+i({Rh}) = 0. Wh(R) is
calculated from Wh+i(R):
Wh({Rh}) = min {H, Rh + Wh+i({Rh})} .	(122)
With the above notation, We can prove that for any policy π ∈ Π,
Vn,Pκ(si, R)- Vn(sι)∣
Etraj〜(n,P)Wi ({(Pκ,h - Ph)V国,。％,冗)})|
Etraj 〜(n,P )卬1
〜
-θh)E φ(sh,ah,s0)V+κ
s0
(s0, R)
≤Etraj 〜(n,p) WI	< l∣θK,h - θh
Λ1,κ,h
fφ(sh,ah,sNh+κ (s0,R)
s0
(Λ1,κ,h)-1
≤Etraj〜(n,P) Wi
2β X φ(sh,ah ,s0)*PK (s0,R)
s0
=2Etraj〜(n,P)Wi ({uΠ,,KKh(Sh, ah, R)O)
=2VK,PK (sι,R)
≤2VK;PK (sι,R)
二2呢『PK (si ,Rk ).
(Λ1,κ,h)-1
(123)
(124)
(125)
(126)
(127)
(128)
(129)
(130)
(131)
≤
~
The second inequality is due to lemma 10 . The third inequality is due to Lemma 13. The last
inequality is due to the optimality of πK and RK. Plugging this inequality back to Inq. 119, We can
prove the lemma.	□
Proof. (Proof of Theorem 2) Combining Lemma 12, Lemma 14 and Lemma 15, We knoW that
K = C2(d2H3 + dH4)log(dH∕(δe)) log2(KHB∕(δe)) + C2(d2.5H2+d2H3) log(dH∕(δe))log2(KHB∕(δe))
=	e2	+	ʌ	e
some constant C2 suffices to guarantee that V*(si, R) - Vin(si, R) ≤ E + ^opt.
for
□
23
Published as a conference paper at ICLR 2022
B.5 Proof of Lemma 12
Proof. From the standard value decomposition technique (Azar et al., 2017; Jin et al., 2018), we can
decompose Vkn,1k,Pk (S1 , Rk) into following terms:
Vy (Si,Rk)
HH
≤ Xmin H, u1n,kk,,Phk (Sk,h, ak,h, Rk) + Xmin H, u2n,kk,,Phk (Sk,h, ak,h, Rk)
(132)
(133)
h=1
H
+ X min {H,(Pk,h- PhMh+
h=1
(sk,h , ak,h , Rk )
(134)
h=
H
+ X PhVkn,hk,+P1k(Sk,h,ak,h,Rk) - Vkn,hk,+P1k (Sk,h+1 , ak,h+1 ,Rk)
h=1
H
≤ X min H, u1n,kk,,Phk (Sk,h, ak,h, Rk)
h=1
H
+ 2 X min H, u2n,kk,,Phk (Sk,h, ak,h, Rk)
h=1
H
+ X PhVkn,1k,Pk(Sk,h, ak,h, Rk) - Vkn,1k,Pk (Sk,h+1 , ak,h+1 , Rk) ,
(135)
(136)
(137)
(138)
where the second inequality is derived from
(Pk,h - Ph)Vfh+l (Sk,h, ak,h, Rk ) =(Gk,h - θh) ^X φ(Sk,h, ak,h, SO)Vnh+k (S0, Rk)
s0
(139)
≤ ∣∣θ^k,h - θh
Λ2,k,h
X φ(sk,h, ak,h, SO)Vnh+k(S0,
s0
Rk)
Λ2,k,h
(140)
id,	__.
≤u2,kk,,hk (Sk,h, ak,h, Rk).
(141)
Eqn 138 is a martingale difference sequence. By Lemma 11, the summation over all k ∈ [K] is at
most p2H3K log(4∕δ). We mainly focus on Eqn 136 and Eqn 137.
Upper bound of Eqn 136 Firstly, we bound the summation of min H, u1π,kk,,Phk (sk,h, ak,h, Rk) .
24
Published as a conference paper at ICLR 2022
KH
XXmin H, u1π,kk,,Phk (sk,h, ak,h,
k=1 h=1
Rk)
KH
XX min H,β
k=1h=1 I
X Φ(sk,h,ak,h,s0)V∏h+Pk (s0,Rk)
s0
KH
≤E Eeσι,k,h min
k=1 h=1
φ(sk,h, ak,h, s )
s0
σι,k,h
≤β Eσ2,k,h Emin 41,
∖ k,h	k,h ∣
≤β∖ IX σ2,k,h
k,h
VS+Pk (s0,Rk)
X 皿	八 Vnh+k(S0, Rk)
φ φ(sk,h, ak,h, S ) Σ
七	σ1,k,h
. 2dH log(1 + KH∕λ).
(142)
(143)
(144)
(145)
(146)
The first inequality is due to Cauchy-Schwarz inequality. The second inequality is due to Lemma 21.
By the definition of σ2,k,h, We have
σ2,k,h ≤ H2∕d + Eι,k,h + Vι,k,h(sk,h, ak,h).	(147)
Now we bound Pkh Eι,k,h and Pkh Vι,k,h(sk,h ak,h) respectively. For Pkh Eι,k,h, We have
HK
XE1,k,h ≤XXmin I H2,4Hβ
HK
+ XXmin H2, 2β
h=1 k=1	I
X Φ(sk,h,ak,h,s0)V∏h +k (s0,Rk)	(	(148)
s0	(Λ1,k,h)-1 
X Φ(sk,h,ak,h,s0) (Vknh+Pk (s0,Rk))2]	}
s0	(Λ3,k,h)-1 
(149)
For the first part,
HK
XX min H H2, 4Hβ X φ(sk,h, ak,h,s0)VV∏hP (S, Rk)
h=1 k=1
HK
≤4H XX βσι,k,h min
h=1 k=1
s0
(Λ1,k,h)-1
1, X φ(Sk,h , ak,h , S )VV∏h +Pk (s0,Rk )∕σι,k,h
s0
≤4Hβ Eσ2,k,h Emin (1, Eφ(s.
k,h	k,h
s0
一 .0、Vnh+Pk (s0,Rk)
k,h, ak,h, S )	_
σ1,k,h
≤4Hβ X σ2,k,h ∙ 2dH log(1 + KH∕λ),
k,h
(Λ1,k,h)-1
2	、
Λ1-,1k,h
(150)
(151)
(152)
(153)
∖
where the last inequality is due to Lemma 21. Similarly, for the second part, by Lemma 21, we have
HK
XXmin H2,2β
h=1 k=1	I
X Φ(sk,h,ak,h,s0) (Vnh+Pk(s0,Rk))21
s0	(Λ3,k,h)-1
≤2∕3√2dH 2K log(1 + KH∕λ).
(154)
(155)
25
Published as a conference paper at ICLR 2022
.τ .	...	√-y	/	X .	. ♦	∙.	C £小.江/ lc∖	...	...
Note that V1,k,h(sk,h, ak,h) is the empirical variance of Vkπ,hk,+P1k (s0, Rk) with transition
Pk(s0|sk,h, ak,h). We bound the summation of V1,k,h(sk,h, ak,h) by the law of total variance (Lat-
timore & Hutter, 2012; Azar et al., 2013).
Recall that we define
Y^k,H+ι(s) = 0,	(156)
Yk,h(s) = V ι,k,h(s,∏k(s) + Pk,hYk,h+ι(s,∏k(s)).	(157)
By the law of total variance, we have Yk,1(s) ≤ H2 holds for any s, a and k. We now bound the
difference between Pk=I Ykj(Sk,1) and PK=I PH=I Vι,k,h(sk,h,ak,h).
H
Yk,1(sk,1) — EVι,k,h(sk,h, ak,h)	(158)
h=1
H
=Pk,1Yk,2(sk,1,ak,1) — EV ι,k,h(sk,h, ak,h)	(159)
h=2
≤ min{h 2,(PkJ- Pi) Yk,2(sk,1,ak,ι)} + (PIYk,2(sk,1, ak,i) — Yk,2(sk,2))	(160)
+ I Yk,2(sk,2) — EVi,k,h(sk,h, ak,h) I .	(161)
h=2
Therefore, we have
	K	KH Eh(Skj)-E EV i,k,h(sk,h,ak,h)	(162) k=1	k=1 h=1 KH ≤ XX min {h2, (Pk,h — Ph) Yk,h+ι(sk,h,ak,h)}	(163) k=1 h=1 HK + XX PhYk,h+i(sk,h, ak,h) — "Yk,h+1(sk,h+lf) .	(164) h=1 k=1
For Eqn 164, this term can be regarded as a martingale difference sequence, thus can be bounded by
H2 √KH by Lemma 11. For Eqn 163, We can bound this term in the following way:
	KH XXmin {h2, (Pk,h — Ph) Yk,h+i(sk,h,ak,h)}	(165) k=1 h=1 KH ≤ XXmin ( H2, ∣R,h — θh卜	kΦγ(sk,h, ak,h, R)∣∣λ-1 J	(166) k=1 h=1	5,k,h	, , KH ≤β XX min{l, kφγ (sk,h,ak,h,R)∣∣Λ-1 J	(167) k=1 h=1	, , H	K ≤β Xt K X min{l, kΦγ(sk,h,ak,h,R)∣∣Λ-1 J	(168) ≤∕3p2dH 2 K log(1 + KH∕λ).	(169)
The second inequality is due to Lemma 10. The third inequality is due to Cauchy-Schwarz inequal-
ity. The last inequality is due to Lemma 21.
From the above analysis, we have
	EVι,k,h(sk,h,ak,h) ≤ H2K + β,2dH2Klog(1 + KH∕λ).	(170) k,h
26
Published as a conference paper at ICLR 2022
Therefore, the summation of 31,2 can be bounded as
X σ2,k,h	(171)
k,h
≤H3K/d + ɪ2 E1,k,h +	V 1,k,h
(172)
VXσ2
≤H 3K/d + 4Hβ
,k,h
• 2dH log(1 + KH∕λ) + 3β VZdH 2K log(1 + KH∕λ) + H 2K.
(173)
We define T = log(32K2H∕δ) log2(1 + KH4B2). Solving for Pk h σ2 k h, we have
X σ2,k,h ≤c1 (H3K∕d + H2K + H3d3τ + √H2d3Kτ)	(174)
k,h
≤2cι (H3K∕d + H2K + H3d3τ) ,	(175)
where c1 denote a certain constant. The last inequality is due to 2√ab ≤ a + b for a, b ≥ 0. Plugging
the above inequality back to Inq 142, we have for a constant c2,
K H
XX UnkkPk(sk,h, ak,h, Rk) ≤ C2 P(dH4 + d2H3) Kτ + c2d2 5H2τ. (176)
k=1 h=1
R
Upper bound of Eqn 137 Now we focus on the summation of u2π,kk,,Phk (sk,h, ak,h, Rk). The proof
follows almost the same arguments as the summation of u1π,kk,,Phk (sk,h, ak,h, Rk), though the upper
bound of Pk=I PH=I V2,k,h(sk,h, ak,h) is derived in a different way. Following the same proof
idea, we can show that
KH
XXu2π,kk,,PRhk(sk,h,ak,h,Rk) ≤
k=1 h=1
ʌ
β

]Tσ∣,k,h2dH log(1 + KHN
k,h
σ2,k,h ≤ H2/d + E2,k,h + V2,k,h(sk,h, ak,h),
(177)
(178)
K H	1--------------------
XX
E2,k,h ≤ 4Hβ Xσ2,k,h ∙ 2dHlog(1 + KH∕λ) + 2β,2dH2Klog(1 + KH∕λ).
k=1 h=1	k,h
(179)
Combining Inq 178 and Inq 179 and solving for Pk 九 σ2 k九，We have
X σ2,k,h	(180)
k,h
≤2H3K∕d + 16dH3β2 log(1 + KH∕λ) + 4βPdH2K log(1 + KH∕λ) + 2 X %必2,a®,h)
k,h
(181)
≤6H3K∕d + 16dH3β2 log(1 + KH∕λ) + 4,β2d2∕Hlog(1 + KH∕λ) + 2 £ V2,k,h(sk,h, ak,h)
k,h
(182)
Plugging this inequality back to Inq 177, we have
KH
XX
u2π,kk,,PRhk (sk,h, ak,h, Rk)	(183)
k=1 h=1
≤βP6H4Klog(1 + KH∕λ) + 4ββdH2 log(1 + KH∕λ) + 2,β,βd3/2 log(1 + KH∕λ)	(184)
+ 卜2d2Hlog(32K2H∕δ) log2(1 + KH4B2) X匹,皿心 ak,h.
k,h
(185)
27
Published as a conference paper at ICLR 2022
WenoW bound the last term in the above equation. Define T = log(32K* 1 2H∕δ) log2(1 + KH4B2),
we have:
32d2HτXV2,k,h(sk,h, ak,h)
k,h
≤t
≤t
KH
32d2HτXX Esθ~Pk,h(∙∣Sk,h,ak,h)[(啜h +k )2(s0,Rk)]
k=1 h=1
KH
32d2H2T X X Es，~Pk,h(.Isk,h,ak,h JVk∏h+k (s0, Rk)]
k=1 h=1
1 K H	〜
≤4√2⅞d2H 3τ + 前 XX Esθ~Pk,h(∙∣Sk,h,ak,h) Kh +k (s0,Rk )]
3 k=1 h=1
KH
I-	C C	S r a r _ π
=4√2≡3d2H 3τ+γh XX v∏h +k (sk,h+ι, Rk)
c
3	k=1 h=1
KH
+ YH XX (PhVnh+k (sk,h, ak,h, Rk ) - Vnh+k (sk,h+1, Rkf)
3	k=1 h=1
KH
+ YH XX (Pk,h - Ph) Vnh+k (sk,h,ak,h,Rk),
3	k=1 h=1
(186)
(187)
(188)
(189)
(190)
(191)
(192)
where Y3 ≥ 1 is a constant to be defined later. The last inequality is due to 2√ob ≤
a + b for any a, b ≥ 0.	Note that by Lemma 10, Eqn 192 is upper bounded by
氏，
CfH Pk h U k h (sk,h, ak,h, Rk). Eqn 191 is a martingale difference sequence, and can be bounded
by，HK log(1∕δ) with high probability. Plugging the above inequality back to Inq 183 and solving
for PkK=1 PhH=1 u2n,kk,,Phk (sk,h, ak,h, Rk), we have
KH
iB.,
u2n,kk,,Phk(sk,h, ak,h, Rk)	(193)
k=1 h=1
(	1 KH 〜	∖
≤c3 YdH4Kτ + (d25H2 + d2H3)τ +，H £ £ V林普(sk,h+ι, Rk) ,	(194)
c3H k=1 h=1
where c3 ≥ 1 is a constant here. We set cY3 = c3 , then we have
KH
V一Λ V一Λ k.E>.,	_
u2,k,h (sk,h, ak,h, Rk)
k=1 h=1
K H
≤C3 (√dH4KT + (d2-5H2 + d2H3)τ) + HXXVπh+Pk(sk,h+1,Rk),
k=1 h=1
(195)
(196)
Bounding the summation of Vkn,1k,Pk (s1 , Rk) In the above analysis, we derive the upper bound
of Eqn 136 and Eqn 137 (Inq 176 and Inq 195). Now we can finally bound PkK=1 Vkn,1k,Pk (sk,1 , Rk).
For a constant c4 ,
K
<-— f⅛
£需,Pk (sk,ι,Rk)	(197)
k=1
KH
L---：---：7-Z---.	.clc cc.	1	¾-¾--— £5-.
≤C4 √(dH4 + d2H 3)Kτ) + c4(d2∙5H2 + d2H 3)τ + 百工 匚 Vnh 臂(sk,h+ι,Rk).	(198)
28
Published as a conference paper at ICLR 2022
Following the same analysis, the above inequality actually also holds for any step h ∈ [H]:
K
X Vknh,Pk (Sk,h,Rk)	(199)
k=1
IKH
≤c4P(dH4 + d2H3)Kτ) + c4(d2∙5H2 + d2H3)τ + H XX V∏hPkι(sk,hι + ι, Rk). (200)
k=1 h1=h
Define G =	c4 ^p(dH4 + d2H3)Kτ) + c4(d2∙5H2 + d2H3)τ)	and ah =
PkK=1 Vkπ,kh,Pk (sk,h, Rk). The above inequality can be simplified into the following form:
1H
ah ≤ G + H ɪ2 ah ,	QOI)
h1=h
and we have aH+1 = 0. From the elementary calculation, we can prove that aι ≤ (1+H )hG ≤ eG.
Therefore, we have
K
X v∏1,Pk (sk,ι,Rk)	(202)
k=1
≤C5 (y(dH4 + d2H3)Klog(KH∕δ) log2(KH4B2)) + (d2-5H2 + d2H3)log(KH∕δ)log2(KH4B2)
(203)
for a constant c5 = ec4.	□
C	The Lower B ound for Reward-free Exploration
In this section, we prove that even in the setting of non-plug-in reward-free exploration, the sample
complexity in the exploration phase to obtain an E-OPtimaI policy is at least Ω(d-H-). We say an
algorithm can (, δ)-learns the linear mixture MDP M if this algorithm returns an -optimal policy
for M with probability at least 1 - δ in the planning phase after receiving samples for K episodes
in the exploration phase. Our theorem is stated as follows.
Theorem 3. Suppose E ≤ min (Cι√H, C2√dH4B) , B > 1, H > 4,d> 3 for certain positive
constants C1 and C2. Then for any algorithm ALG there exists an linear mixture MDP instance
M = (S, A, P, R, H, ν) such that if ALG (E, δ)-learns the problem M, ALG needs to collect at
least K = Cd2H3∕E2 episodes during the exploration phase, where C is an absolute constant, and
0 < δ < 1 is a positive constant that has no dependence on E, H, d, K.
Compared with the lower bound proposed by Zhang et al. (2021a), we further improve their result
by a factor of d. This lower bound also indicates that our sample complexity bound in Theorem 2 is
statistically optimal. We prove Theorem 3 in Appendix C.1.
C.1 Proof of Theorem 3
Our basic idea to prove Theorem 3 is to connect the sample complexity lower bound with the regret
lower bound of another constructed learning algorithm in the standard online exploration setting.
To start with, we notice that the reward-free exploration is strictly harder than the standard non-
reward-free online exploration setting (e.g. Jin et al. (2018); Zhou et al. (2021; 2020a)), where the
reward function is deterministic and known during the exploration. Therefore, if we can prove the
sample complexity lower bound in the standard online exploration setting, this bound can also be
applied to the reward-free setting.
For readers who are not familiar with the formulation of online exploration setting studied in this
section, we firstly introduce the preliminaries. Compared with the reward-free exploration, the only
29
Published as a conference paper at ICLR 2022
difference is that the reward R(s, a) is fixed and known to the agent. In each episode, the agent starts
from an initial state sk,1 sampled from the distribution ν. At each step h ∈ [H], the agent observes
the current state sk,h ∈ S, takes action ak,h ∈ A, receives the deterministic reward Rh(sk,h, ak,h),
and transits to state sk,h+1 with probability Ph(sk,h+1 |sk,h, ak,h). The episode ends when sH+1
is reached. The agent’s goal is to find a -optimal policy π after K episodes. We say a policy π is
-optimal if
H
E	Rh (sh, ah) | π
h=1
H
≥ E X Rh (sh,ah) | ∏* — e,
h=1
where π* is the optimal policy for the MDP (S, A, P, R, H, V).
Theorem 4. Suppose E ≤ min (Cι√H, C2√dH4B) , B > 1, H > 4,d > 3. Then for any
algorithm ALG 1 solving the non-reward-free online exploration problem, there exists an linear
mixture MDP M = (S, A, P, R, H, ν) such that ALG1 needs to collect at least K = Cd2H3/E2
episodes to output an E-optimal policy for the linear mixture MDP M with probability at least 1 - δ,
where C is an absolute constant, and 0 < δ < 1 is a positive constant that has no dependence on
E, H, d, K.
From the above discussion, Theorem 3 can be directly proved by reduction if Theorem 4 is true.
Proof. (Proof of Theorem 3) The theorem can be proved by contradiction. Suppose there is an algo-
rithm ALG which can (E, δ)-learns any linear mixture MDP instance M with only K0 ≤ Cd2H3/E2
episodes. Then we can use this algorithm to solve the online exploration problem by simply ignor-
ing the information about the reward function and directly calling the exploration algorithm of ALG
during the exploration phase. Then in the planning phase, we use the planning algorithm of ALG to
output a policy based on the reward function as well as samples collected in the exploration phase.
Therefore, this indicates that ALG can output an E-optimal policy for the non-reward-free online
exploration problem with probability at least 1 - δ after only K0 ≤ Cd2H3/E2 episodes. This
contradicts the sample complexity lower bound in Theorem 4.	□
Now we discuss on how to prove Theorem 4.
Proof. (Proof of Theorem 4) Set K1 = cK for a positive constant c ≥ 2. We construct another
algorithm ALG2 for any possible ALG1 in the following way: In ALG2, the agent firstly runs
ALG1 for Ki/c = K episodes. After K episodes, suppose ALG1 outputs a policy ∏ according
to certain policy distribution ν∏. ALG2 executes the policy ∏ in the following c-1 Ki episodes.
The interaction ends after K1 episodes. ALG2 can be regarded as an algorithm which firstly runs
the online exploration algorithm ALG 1 for K episodes, and then evaluates the performance of the
policy ∏ in the following episodes. We study the total regret of ALG2 from episode K +1 to episode
K1.
Recently, Zhou et al. (2020a) proposed the following regret lower bound for linear mixture MDPs.
In order to avoid confusion, we use the notation K2 instead of K to denote the number of total
episodes of the regret minimization problem.
Lemma 16. (Theorem 5.6 in Zhou et al. (2020a)) Let B >	1 and suppose K2	≥
max {(d — 1)2H∕2, (d — 1)∕(32H (B - 1))}, d ≥ 4, H ≥ 3. Then for any algorithm there ex-
ists an episodic, B-bounded linear mixture MDPparameterized by Θ = (θi, ∙∙∙ , Θh) such that the
expected regret is at least Ω(dH√HK2).
To prove the above regret lower bound, Zhou et al. (2020a) construct a class of hard instances
which is an extension of the hard instance class for linear bandits (cf. Theorem 24.1 in Latti-
more & Szepesvari (2020)). They show that for any algorithm, there exists a hard instance in
the instance class such that the regret is at least Ω(dH√HK2). By Theorem 16 and setting
K2 = K1 , we know that for any possible algorithm ALG2, there exists a hard instance M such
that PK=I E(V*(si)- Vπk(sι)) ≥ c2dH√HK1 for a positive constant c2, where ∏k denotes
the policy used in episode k for algorithm ALG2 . The expectation is over all randomness of the
algorithm and the environment.
30
Published as a conference paper at ICLR 2022
Note that in the hard instance constructed in ZhoU et al. (2020a), the Per-SteP regret is at most
E (V*(sι) - Vπk(sι)) ≤ dHi^JKH[ for any episode k ≤ Ki. By choosing C = max { √J^c , 2},
we know that for the instance M,
k=K1 /c+1
K1
E	E(V*(sι)- Vπk(si)) ≥
(204)
By the definition of ALG2, We have ∏k = ∏ if k > Ki/c. Therefore, We have
(C - 1)KE∏”,sι〜μ (V*(si) - Vπ(si)) ≥ C2dHPHK =?H√HK.	(205)
Dividing both sides by (C - 1)K, We have
En〜μ∏,sι〜μ (V*(si) - Vπ(si)) ≥ JcC dHPH/K.	(206)
2(C - 1)
The above inequality indicates that, for any algorithm ALGi, there exists an instance M such that
the expected sub-optimality gap of the policy returned by ALGi is at least 书& dH，H/K after
collecting samPles for K ePisodes.
Suppose ALGi returns an E-OPtimaI policy ∏ with probability at least 1 - δ. Recall that the per-step
sub-optimality gap is at most E (V*(si) - Vπk (si)) ≤ 4H∖[1H in the constructed hard instance.
We have
(I - δ) ∙ C + δ ∙ —J=\ ≥E ≥ En 〜μ∏,sι 〜μ (V *(SI) - V K (SI)) ≥ ,乂 CldH PH∕K∙ (207)
4 2 CK	2(C - 1)
We set δ to be a constant satisfying 0 < δ < min {1, 2√cf.5c2 }. Solving the above inequality, we
Cd2 H 3^
have K ≥ 膏 for a positive constant C.	□
D Improved B ound for Reward-free Exploration in Linear MDPs
In this section, we explain how our choice of the exploration-driven reward can be used to improve
the sample complexity bound for reward-free exploration in linear MDPs (Wang et al., 2020). We
study the same reward-free setting as that in Wang et al. (2020), which is briefly explained in Ap-
pendix D.1. We describe our algorithm and bound in Appendix D.2, and prove our theorem in
Appendix D.3.
D.1 Preliminaries
For the completeness of explanation, we briefly restate the reward-free setting studied in Wang et al.
(2020). Compared with the setting in this work, the main differences are twofold: Firstly, they
study the linear MDPs setting instead of linear mixture MDPs in this work. Secondly, they study the
standard reward-free exploration setting without the constraints of the plug-in solver.
The linear MDP assumption, which was first introduced in Yang & Wang (2019); Jin et al. (2020b),
states that the model of the MDP can be represented by linear functions of given features.
Assumption 1. (Linear MDP) an MDP M = (S, A, P, R, H, ν) is said to be a linear MDP if the
following hold:
• There are d unknown signed measures μh = (μhi), μhc), ∙∙∙ , μhd)) such that for any
(s, a, s0) ∈ S ×A×S, Ph (s0 | s, a) = <μ% (s0), φ(s, a)i.
• There exists H unknown vectors ηi, ηc, ∙∙∙ , η∏ ∈ Rd such that for any (s, a) ∈ S ×A,
Rh(S, a) = hφ(S, a), ηhi.
31
Published as a conference paper at ICLR 2022
We assumefor all (s, a) ∈ S ×A and h ∈ [H], kφ(s, a)k ≤ 1, kμh(s)k2 ≤ √d and kηk2 ≤ √d.
For the reward-free exploration studied in Wang et al. (2020), the agent can collects a dataset of vis-
ited state-action pairs D = skh, akh (k,h)∈[K]×[H] which will be used in the planning phase. Then
during the planning phase, the agent can follow a certain designed learning algorithm to calculate
an -optimal policy w.r.t. the reward function R using the dataset D.
D.2 Algorithm and Theorem
Our algorithm can be divided into two parts. The exploration phase of the algorithm is presented in
Algorithm 3, and the planning phase is presented in Algorithm 4.
Algorithm 3 Reward-free Exploration for Linear MDPs: Exploration Phase
Input: Failure probability δ > 0 and target accuracy e > 0
β J cβ ∙ dHʌ/log (dHδ-1ε-1) for some c§ > 0
K J CK ∙ d3H4 log (dHδ-1ε-1) /ε2 for some CK > 0
for episode k = 1,2,…，K do
5:	Qk,H+1(∙, j J 0, Vk,H+1(∙) J 0
for step h = H, H 一 1,…，1 do
Ak,h J- PPt=ι φ(st,h, at,h)φ(st,h, at,h)ɪ + I
uk,h(∙, ∙) J β {φ(∙, l> (Ak,h) 1 φ(∙, ∙
Define the exploration-driven reward function Rk,h(∙, ∙) = uk,h(∙, ∙)
10:	wk,h j- (Ak,Q	PPt=ι φ(st,h, at,h)Vt,h+ι(St,h+ι)
Qk,h(∙, ∙) J min {w>,hφ(∙, ∙) + Rk,h(∙, ∙) + uk,h(∙, ∙), H}
Vk,h(s) J maxa∈A Qk,h(s, a), πk,h (s) = arg maxa∈A Qk,h(s, a)
end for
for step h = 1, 2,…，H do
15:	Take action ak,h = ∏k,h(sk,h) and observe Sk,h+ι 〜Ph(Sk,h,ak,h)
end for
end for
Output: D J {(Sk,h, ak,h)}(k,h)∈[K]×[H].
Algorithm 4 Reward-free Exploration for Linear MDPs: Planning Phase
Input: Dataset DJ {(sk,h, ak,h)}(k,h)∈[κ]×[H],rewardfunctions R = {Rh}h∈[H]
Qk,H+1(∙, ∙) = 0, Vk,H+l(∙)= 0
for step h = H,H 一 1,…，1 do
Λh J Pt=1 φ(St,h, at,h)φ(St,h, at,h)> + I
5:	uh(∙, ∙) J min {β,Φ(∙, ∙)> (Λk,h)-1 φ(∙, ∙), H
Wh J (Ah) 1 Pt=I φ(st,h, at,h)Vt,h+l(st,h+1)
Qh(∙, ∙) J min {w>φ(∙, •) + Rh(∙, ∙) + uh(∙, ∙), H}
Vh(S) J maxa∈AQh(S, a),πh(S) = argmaxa∈AQh(S, a)
end for
10: Output: π = {πh}h∈[H]
Compared with the algorithm in Wang et al. (2020), the main difference is that we set Rk,h = uk,h
in the exploration phase, instead of Rk,h = uk,h/H. The following theorem states the complexity
bound of our algorithms.
Theorem 5. With probability at least 1 - δ, after collecting O(d3H4 log(dHδ-1e-1)∕e2) trajecto-
ries during the exploration phase, our algorithm outputs an -optimal policy for any given reward
during the planning phase.
32
Published as a conference paper at ICLR 2022
D.3 Proof of Theorem 5
The proof of Theorem 5 follows the proof framework in Wang et al. (2020) with a slight modifica-
tion. Therefore, we only sketch the proof and mainly focus on explaining the differences. Firstly,
We introduce the value function 以(s, R), which is recursively defined from step H + 1 to step 1:
~ , .
VH +ι(s,R)=0,∀s ∈S
Vns,R) =maχ{min {Rh(s,a) + PhV+ι(s,a,R),H}} ,∀s ∈S,h ∈ [H ]
(208)
(209)
Compared with the definition of Vj^ (s, R), the main difference is that we take minimization over the
value and H at each step. We can similarly define Qh(s, a, R), V∏ (s, R) and Qn (s, a, R).
Lemma 17. With probability 1 一 δ∕2,forall k ∈ [K],
, , _ ________ , 、
VI (Sι,k,Rk) ≤ Vk,ι(sι,k)	QIO)
and
K
X Vkj(SkJ) ≤ c√d3H4Klog(dKH∕δ)	(211)
k=1
for some constant c > 0 where Vk,1 is as defined in Algorithm 3.
This lemma corresponds to Lemma 3.1 in Wang et al. (2020). The main difference is that we replace
%* with V*. Note that Vhc(s, R) ≤ H by the definition of Vhc(s, R). Lemma 17 can be similarly
proved following the proof of Lemma 3.1 in Wang et al. (2020) and replacing Vh* by Vh* during the
proof.
Lemma 18. With probability 1 — δ/4, for thefunCtion uh(∙, ∙) defined in Line 5 in Algorithm 4, we
have
Es〜μ [V* (s, Uh)i ≤ c0√d3H4 ∙ log(dKH∕δ)∕K	(212)
Compared with Lemma 3.2 in Wang et al. (2020), we replace the term V1* (s, uh∕H) with
V1* (s, uh). Note that with our choice of exploration-driven reward in the exploration phase (i.e.
Rk,h = uk,h), we replace the term uh∕H with uh in the expectation. This lemma can be proved by
following the proof of Lemma 3.2 in Wang et al. (2020).
Lemma 19. With probability 1 一 δ∕2, for any reward function satisfying Assumption 1 and all
h ∈ [H], we have
Qh(∙, ∙,r) ≤ Qh(∙, ∙) ≤ Rh(∙, ∙) + X Ph (S0 | ∙, ∙) Vh+1 (SO) + 2uh(∙, ∙).	(213)
Since our algorithm in the planning phase is exactly the same with that of Wang et al. (2020), this
lemma shares the same idea of Lemma 3.3 in Wang et al. (2020).
Now we can prove Theorem 5 with the help of Lemma 18 and Lemma 19.
Proof. (Proof of Theorem 5) We condition on the events defined in Lemma 18 and Lemma 19,
which hold with probability at least 1 一 δ. By Lemma 19, we have for any s ∈ S,
V1(s) = max Q1 (s, a) ≥ max Q1* (s, a, R) = V1* (s, R),	(214)
sa
which implies
Esi〜μ [V* (sι,R) — V∏ (sι,r)] ≤ Esi〜μ [V1 (si) — V∏ (sι,R)] .	(215)
Note that 0 ≤ Vh(s) ≤ H and 0 ≤ Vhπ(s, R) ≤ H since 0 ≤ R(s, a) ≤ 1. Therefore, we always
have Vh(s) — Vhπ(s, R) ≤ H. For any sh, we have
Vh (sh) — Vhπ (sh, R)	(216)
≤ min {H,PhVh+ι(sh,∏h(sh)) + 2uh(s%,∏h(sh)) — PhVh+1(sh+1,R)}	(217)
33
Published as a conference paper at ICLR 2022
By recursively decomposing Vh(sh) - Vhπ(sh, R) from step H to 1, we have
EsI〜μ [V1 (SI)- Vn(S 1, r)] ≤ Es〜μ
By definition of V1* (s, u), We have Es〜μ [VVπ (s, u)] ≤ 2Es〜μ
[V∏ (s,u)].
[VV*(s,u)] . By Lemma 18,
(218)
i [V*(s,u)] ≤ C0Pd3H4 ∙ log(dKH∕δ)∕K.
(219)
By taking K = cKd3H4 log(dHδ-1-1)∕2 for a sufficiently large constant cK > 0, we have
Esi〜μ [V* (S1,R) — Vn (sι,r)] ≤ C√d3H4 ∙ log(dKH∕δ)∕K ≤ e.
(220)
□
E Auxiliary Lemmas
Lemma 20. (Self-Normalized Bound for Vector-Valued Martingales, Theorem 1 and 2 in Abbasi-
Yadkori et al. (2011)) Let {Ft }t∞=0 be a filtration. Let {ηt}t∞=1 be a real-valued stochatic process
such that ηt is Ft-measurable and ηt is conditionally R-sub-Gaussian for some R ≥ 0, i.e.
λ2R2
∀λ ∈ R, E[eληt ∣Ft-i] ≤ exp(-^-
(221)
Let {Xt}t∞=1 be an Rd -valued stochastic process such that Xt is Ft-1-measurable. Assume that V
is a d × d positive definite matrix. For any t ≥ 0, define
tt
Vt = V + £ XsX>,St = EnsXs.
(222)
s=1
s=1
Then, for any δ > 0, with probability at least 1 - δ, for all t ≥ 0,
kStkV-i ≤ 2R2 log
det (跖1/2 det(V)-1/2
(223)
δ
Further, let V = Iλ, λ > 0. Define Yt =hXt, θ*> + n and assume that ∣∣θ*∣∣2 ≤ S, kXtk2 ≤
L, ∀t ≥ 1. Thenfor any δ > 0,with probability at least 1 — δ, for all t ≥ 0, θ* satisfies
O	_
θt — θ*
Vt
≤ R d log
1+ tL2∕λ
+ λ"S,
(224)
δ
where θt is the l2-regularized least-squares estimation of θ* with regularization parameter λ > 0
based on history samples till step t.
Lemma 21. (Lemma 11 in Abbasi-Yadkori et al. (2011)) Let {Xt}t∞=1 be a sequence in Rd, V a
d X dpositive definite matrix and define Vt = V + Ps=ι XsX>. Then, we have that
log(d⅛Vnθ≤ X kXtkJ.
(225)
Further , if kXt k2 ≤ L for all t, then
n
V-
det (Vn) — log det V) ≤ 2 Idlog ((traCe(V) + nL2) ∕d) — log det V),
t=1
(226)
and finally, if λmin(V) ≥ max(1, L2), then
Su „2	det (Vn)
X kXtkV-i ≤ 2log 诉Vy.
(227)
34
Published as a conference paper at ICLR 2022
Lemma 22. (Bernstein inequality for vector-valued martingales, Theorem 4.1 in Zhou et al.
(2020a)) Let {Gt}t∞=1 be a filtration, {xt, ηt}t≥1 a stochastic process so that xt ∈ Rd is Gt-
measurable and η ∈ R is Gt+ι -measurable. Fix R,L,σ,λ > 0, μ* ∈ Rd. For t ≥ 1, let
yt = hμ*, Xti + ηt and suppose that ηt, Xt also satisfy
∣ηt| ≤ R, E[ηt | Gt] = 0,E [η2 | Gt] ≤ σ2, kxtlb ≤ L.	(228)
Then, for any 0 < δ < 1, with probability at least 1 - δ, we have
t
∀t > 0, XXini	≤ βt, kμt - μ*kzt ≤ β + √λI∣μ*k2,
i=1	zt-1
where for t ≥ 1, μt = Z-1 bt,Zt = λI + Pi=I XiX>,bt = Pt=IyiXi and
(229)
βt = 8σ∙∖∕dlog (1 + tL2/(dλ)) log (4t2/δ) + 4Rlog (4t2∕δ) .	(230)
35