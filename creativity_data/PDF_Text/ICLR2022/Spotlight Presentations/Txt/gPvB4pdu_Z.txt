Published as a conference paper at ICLR 2022
Compositional Training for End-to-End Deep
AUC Maximization
Zhuoning Yuan1 , Zhishuai Guo1, Nitesh V. Chawla2, Tianbao Yang1
1 Department of Computer Science, University of Iowa
2Computer Science & Engineering, University of Notre Dame
{zhuoning-yuan, zhishuai-guo, tianbao-yang}@uiowa.edu, nchawla@nd.edu
Ab stract
Recently, deep AUC maximization (DAM) has achieved great success in differ-
ent domains (e.g., medical image classification). However, the end-to-end train-
ing for deep AUC maximization still remains a challenging problem. Previous
studies employ an ad-hoc two-stage approach that first trains the network by op-
timizing a traditional loss (e.g., cross-entropy loss) and then finetunes the net-
work by optimizing an AUC loss. This is because that training a deep neural
network from scratch by maximizing an AUC loss usually does not yield a satis-
factory performance. This phenomenon can be attributed to the degraded feature
representations learned by maximizing the AUC loss from scratch. To address
this issue, we propose a novel compositional training framework for end-to-end
DAM, namely compositional DAM. The key idea of compositional training is
to minimize a compositional objective function, where the outer function corre-
sponds to an AUC loss and the inner function represents a gradient descent step
for minimizing a traditional loss, e.g., the cross-entropy (CE) loss. To optimize
the non-standard compositional objective, we propose an efficient and provable
stochastic optimization algorithm. The proposed algorithm enhances the capabil-
ities of both robust feature learning and robust classifier learning by alternatively
taking a gradient descent step for the CE loss and for the AUC loss in a systematic
way. We conduct extensive empirical studies on imbalanced benchmark and med-
ical image datasets, which unanimously verify the effectiveness of the proposed
method. Our results show that the compositional training approach dramatically
improves both the feature representations and the testing AUC score compared
with traditional deep learning approaches, and yields better performance than the
two-stage approaches for DAM as well. The proposed method is implemented in
our open-sourced library LibAUC (www.libauc.org) and code is available at
https://github.com/Optimization- AI/LibAUC.
1	Introduction
Deep AUC maximization (DAM) represents a new learning paradigm for deep learning, which max-
imizes the area under ROC curve (AUC) on a training dataset for learning a deep neural network.
It has received increasing attention recently due to the advancement in large-scale non-convex opti-
mization algorithms for AUC maximization (Liu et al., 2019a; Yuan et al., 2021; Guo et al., 2020a;b).
Recently, DAM has been successfully applied to different domains with imbalanced data (Yuan
et al., 2020; Wang et al., 2021b). For example, Yuan et al. (2020) has employed DAM for a vari-
ety of medical image classification tasks, e.g., classification of X-ray images, skin lesion images,
mammograms, and microscopic images, and they observed great improvements with about 1%〜5%
AUC increase over traditional deep learning approaches by optimizing a standard loss function, e.g.,
the cross-entropy (CE) loss. These pioneering studies on DAM open a new direction for deep learn-
ing in the presence of imbalanced data but also raise many questions yet to be solved. A particular
question relates to how the network is trained by DAM. Existing studies employ a two-stage ap-
proach for DAM, in which the first stage pretrains the network on the training data by optimizing a
traditional loss function (e.g. the CE loss) and the second stage finetunes the network by optimiz-
ing an AUC loss. It was conjectured in (Yuan et al., 2020) the feature extraction layers learned by
directly optimizing AUC from scratch are not as good as optimizing the standard CE loss, similarly
as optimizing a class-weighted loss for deep learning with imbalanced data (Cao et al., 2019).
1
Published as a conference paper at ICLR 2022
CE (scratch)	AUC (scratch)	AUC-CE	CT
Figure 1: t-SNE visualization of feature representations of an imbalanced training set for the CatVs-
Dog visualized by tSNE learned by different methods (from left to right): optimizing CE loss, an
AUC loss, a linear combination of CE and AUC loss, and a compositional objective by our method.
Although the ad-hoc two-stage method of DAM has achieved some success, this approach leads
to several undesirable consequences increasing the engineering costs in practice: (i) which layers
should we finetune in the second stage? Fine-tuning all layers increases the training costs but not
necessarily improves the final performance (Jamal et al., 2020; Qi et al., 2020); (ii) when do we stop
the training for the first-stage? A long training time for the first-stage increases the overall training
costs but not necessarily increases the final prediction performance, while a short training time for
the first-stage could harm the prediction performance (Kang et al., 2019). Hence, the literature has
suggested different tricks for the two-stage approach, including the decoupling method that simply
optimizes the classifier layer in the second-stage (Kang et al., 2019) and deferred re-weighting that
only dedicates the iterations with the largest step size to the CE loss (Cao et al., 2019), which could
be borrowed to DAM as well. However, an important question remains open regarding DAM:
How can we conduct end-to-end training for deep AUC maximization?	I
To answer this question, we have examined the learned feature representations by optimizing an
AUC loss directly from scratch and confirmed the conjecture in (Yuan et al., 2020) that the learned
feature representations exhibit no advantage over optimizing the CE loss directly. In Figure 1 we
visualize the feature representations on an imbalanced training set for the CatvsDog classification
learned by different methods and visualized by t-SNE (van der Maaten & Hinton, 2008). We
can see that optimizing an AUC loss from scratch (2nd column) does not yield a cleaner feature
representations for the two classes of data than optimizing the CE loss. What makes end-to-end
deep learning successful is its superb feature learning capability, i.e., the lower layers capture the
low-level features and higher layers capture the high-level features. In terms of feature learning,
different examples roughly have equal weights regardless which classes they belong to. From this
perspective, we could understand why optimizing AUC loss alone bears worse feature learning
capability. The AUC loss assigns different weights to different examples from different classes for
more robust classifier learning. In particular, the data in the positive class has a higher weight than
data in the negative class. These non-equal weights are important for learning a robust classifier
given that feature representations have been learned well but are not readily helpful for learning
feature representations in an end-to-end fashion. Can we achieve both effects in a unified and
end-to-end learning framework, i.e., optimizing the CE loss with equal weights for robust feature
learning and optimizing an AUC loss with uneven weights for robust classifier learning? A naive
approach is to simply optimize a linear combination of the CE loss and an AUC loss. However, this
approach has a trade-off, meaning that AUC is not necessarily maximized due to the presence of
the CE loss in the objective and the learned feature representations could be degraded by the AUC
loss (Figure 1, 3rd column).
In this paper, we propose a better and novel end-to-end training method that not only achieves both
benefits of minimizing the CE loss for robust feature learning and minimizing an AUC loss for
robust classifier learning, but also achieves the effect of “1+1>2", i.e., achieves better performance
than the naive linear combination approach. The novel synthesis lies in how we composite the two
training steps corresponding to the CE loss and the AUC loss. The central idea is to minimize a
two-level compositional objective, where the outer function is an AUC loss, and the inner function
is a gradient descent step towards minimizing the CE loss, which represents a quick adaptation to the
solution to minimizing the CE loss. We propose a novel efficient stochastic algorithm with provable
convergence for minimizing the compositional objective, which performs alternating gradient-based
updates that are first based on the gradient of CE loss and then based on the gradient of an AUC loss
at the point obtained in the first step. We summarize our contributions below.
• We propose a novel training framework for end-to-end deep AUC maximization, namely compo-
sitional DAM. The novel compositional objective enables not only the robust feature learning of
2
Published as a conference paper at ICLR 2022
lower layers from minimizing the standard loss function but also the robust learning of a classifier
from minimizing an AUC loss.
• Theoretically, we propose an efficient stochastic optimization algorithm for solving compositional
DAM, and establish the same convergence rate as standard SGD for optimizing a standard aver-
aged loss. Empirically, we conduct extensive studies on benchmark and medical image datasets,
and observe that the proposed method not only improves the baseline methods including the
naive linear combination approach but also improves ad-hoc two-stage approaches for DAM. The
learned feature representations of compositional DAM (e.g., Figure 1 right) are much better than
those learned by minimizing the CE loss or an AUC loss alone and their combination.
2	Related Work
Deep AUC Maximization. AUC maximization has a history of two decades. Most of the existing
studies revolve around the design of efficient optimization algorithms. Earlier papers have focused
on full batch methods (Herbrich et al., 1999; Yan et al., 2003; Ferri et al., 2002; Freund et al., 2003;
Joachims, 2005; Herschtal & Raskutti, 2004; Rakotomamonjy, 2004; Zhang et al., 2012) and online
optimization methods (Zhao et al., 2011; Kar et al., 2014; 2013; Gao et al., 2013). Recently, stochas-
tic optimization for AUC maximization has become the dominating approach (Ying et al., 2016; Liu
et al., 2018; Natole et al., 2018; 2019). Ying et al. (2016) propose a milestone work for stochastic
optimization of AUC. They consider optimizing the pairwise square loss and propose an equivalent
min-max formulation that transforms the original non-decomposable objective into a decomposable
one, which enables the design of efficient stochastic methods based on mini-batch of data without
explicitly constructing the pairs. The min-max formulation also serves as the basis for recent works
on DAM (Liu et al., 2019a; Yuan et al., 2021; Guo et al., 2020a;b). (Liu et al., 2019a) is the first work
that explicitly considers DAM and develops the first practical and provable stochastic algorithms for
DAM based on the min-max formulation of the pairwise square loss function. However, this work
only focuses on optimization and experiments are done on simple benchmark datasets. Later, Yuan
et al. (2020) propose a new robust loss in the min-max form for DAM and evaluates the performance
of DAM on various medical image classification tasks, which demonstrates great success of DAM.
However, none of these works have addressed the problem of end-to-end training for DAM.
Deep Learning with Imbalanced data. Deep learning in the presence of imbalanced data has
recently attracted tremendous attention (Cui et al., 2019; Johnson & Khoshgoftaar, 2019; Masko &
Hensman, 2015; Lee et al., 2016; Khan et al., 2017; Dablain et al., 2021; Ren et al., 2018; Jamal
et al., 2020; Qi et al., 2020; Lin et al., 2017; Cao et al., 2019; Kang et al., 2019; Liu et al., 2019b;
Zhu & Yang, 2020; Zhou et al., 2020; Xiang et al., 2020; Wang et al., 2021a; 2020; Menon et al.,
2021). Among these studies that are closely related to our work include (Lin et al., 2017; Cui
et al., 2019; Cao et al., 2019; Qi et al., 2020; Kang et al., 2019; Jamal et al., 2020), which focus on
optimizing different objectives from the standard CE loss, including class-weighted loss, focal loss,
individually weighted loss functions, etc. Nevertheless, these works are not directly comparable to
our method for maximizing AUC.
Two-stage Approaches. However, directly optimizing a weighted loss for training a deep neural
network from scratch does not work well (Cao et al., 2019; Kang et al., 2019; Jamal et al., 2020; Qi
et al., 2020; Yuan et al., 2020). This phenomenon was first observed in (Cao et al., 2019), which
is attributed to the degraded feature representations. To tackle this issue, Cao et al. (2019) propose
a deferred re-weighting/re-sampling trick. It minimizes the standard average loss for the first stage
and switches to minimizing the class-weighted loss or re-sampling method in the second stage. In
their paper, the first stage is defined as the training period from the beginning to the iteration that
the step size was reduced for the first time in SGD or momentum methods. Kang et al. (2019)
investigate a decoupling approach, where the first stage learns the feature representations (i.e., a
feature extraction network) by optimizing a standard loss with a large number of iterations, and the
second stage learns a robust classifier (i.e. the classifier layer). The authors show that the decoupling
approach can achieve better performance than the deferred re-weighting trick in (Cao et al., 2019).
However, the decoupling approaches do not necessarily yield the best performance. In particular,
some studies have found that fine-tuning some higher layers besides the classifier layer in the second
stage is beneficial (Jamal et al., 2020; Qi et al., 2020). Recently, Yang & Xu (2020) propose to use
self-supervised learning for learning the feature representations for the first stage and to switch to
re-weighting method in the second stage. Different from these studies, our work is to design an
elegant end-to-end training framework for deep learning with an AUC loss.
3
Published as a conference paper at ICLR 2022
3	Compositional Training for Deep AUC Maximization
Notations. We use (x, y) to denote an example, where x ∈ Rd0 denotes the input and y ∈ Y denotes
its corresponding label. Let k ∙ k denote the Euclidean norm of a vector, and let W ∈ Rd denote
the weight parameters of a deep neural network. A function F (w) is called L-smooth if its gradient
is L-LiPschitz continuous, i.e., ∣∣VF(w) - VF(w0)k ≤ LkW - w0∣∣. Let f(w;x) denote the
prediction scores of a deep neural network parameterized by W on an input x, where f(W; x) ∈ R
for binary classification with Y = {1, -1}. Denote by D = {(x1, y1), . . . , (xn, yn)} a set of n
training examples. Let `(w; x, y) denote a loss function on an individual data, e.g., cross-entropy
loss. We let L(w; S) denote an aggregate loss function defined on a set of samples S ⊆ D. When
S = D, we simply use the notation L(w) = L(w; D). Let ∏ω[Θ] denotes an Euclidean projection
on the set Ω. Denote by n+(n-) the number of positive (negative) examples.
A standard approach of deep learning is to minimize an averaged loss on training examples, i.e.,
1n
min LAVG(W) = — V''(w; Xi,yi).	(1)
w∈Rd	n
i=1
AUC losses. AUC (area under the ROC curve) is a commonly used measure for evaluating classi-
fiers for binary classification with imbalanced data. Recently, there emerge voluminous studies on
optimizing AUC score for learning a predictive model (e.g., a deep neural network). The idea is to
optimize a surrogate loss for the AUC score. A special surrogate loss is the AUC square loss (Gao
& Zhou, 2015), which is defined as:
min^^ X X (C-(f(w；Xi)-f(W；Xj)))2,
w n+n- yi=1 yj=-1
where c is a margin parameter (e.g., 1). Since directly optimizing the above pairwise loss is com-
putationally expensive, existing works transform the above problem into an equivalent min-max
optimization (Liu et al., 2019a), which is decomposable over individual examples:
1n
min max Φ (w, a, b, θ) := 一 φ( φ (w, a, b, θ; Xi, yi), where
w,a,b θ∈Ω	n
i=1
(2)
φ(w, a, b,	θ; Xi,	yi)	= (1 -p) (f(w; Xi)	-	a)2 I[yi=1] +	p(f(w; Xi)	- b)2I[yi=-1]	(3)
-p(1 - p)θ2 +2θ (p(1 - P)C + pf(w; Xi)I[yi=-i] - (1 - p)f(w; Xi)I[yi=i]),
Ω = R and P = n+/n. From the above objective function φ, we can see that each example Xi also
has a class-level weight for their contributed loss, i.e., the data from the positive class is weighted
by 1 - P and the data from the negative class is weighted by P.
It was recently shown that the AUC square loss is sensitive to noisy data and also has adverse
effect when trained with easy data. Hence, Yuan et al. (2020) proposed the min-max AUC margin
(AUCM) loss, whose optimization problem is (2) with Ω = {0 ≤ θ ≤ u} for some u. Let us define
Lauc(w) = mi□a,b maxθ∈Ω Φ (w, a, b, θ) as the AUC loss function.
3.1	Compositional DAM: A Compositional Training method for DAM
In this section, we present the proposed compositional training for DAM. Our proposed objective
for end-to-end deep learning is given by
min LAUC(w - αVLAVG(w)),	(4)
w∈Rd
where α is hyper-parameter. Different from LAUC(w), the above objective is a compositional func-
tion, where the inner component w - αVLAVG(w) is another function of w. We refer to the above
objective as the compositional objective and a method for minimizing the above compositional ob-
jective as compositional training.
To understand the compositional objective (4), we take the second-order Taylor expansion of the
compositional objective, which include three terms:
LAUC(W - oVLavg(w)) ≈ Lauc(w) - oVLauc(w)>VLavg(w) + Cα2∕2∣VLAVG(w)k2, (5)
where C represents the Lipchitz continuity constant of VLauc(∙). In order to understand how the
three terms play their roles and evolve in the optimization process by our proposed algorithm pre-
sented in next subsection (Algorithm 1), we conduct some empirical studies on several benchmark
4
Published as a conference paper at ICLR 2022
Figure 2: Evolution of different terms in (5) computed in the process of our optimization algorithm
(Algorithm 1) on the CatvsDog data. LAVG is the averaged CE loss. Please refer to Appendix A.6
for more details of the calculations.
datasets reported in Appendix A.6. Here, we explain the result of the CatvsDog classification shown
in Figure 2. Initially, the first term LAUC (w) dominates the objective and the algorithm will fo-
cus on pushing this term to be smaller (1st column), once it reaches the same level of the third
term the algorithm Win shift its focus to push ∣∣VLAVG(W)Il smaller (2nd column) while keeping
▽LAUC(w)> VLavg(W) to be positive (3rd column). This process will continue by alternating be-
tween the efforts of pushing LAUC(W) smaller and of pushing ∣∣VLavg(w)∣2 to be smaller while
keeping VLAUC(W)>VLAVG(W) to be non-negative. We also notice that the final AUC loss is close
to zero. The same phenomena are also observed on other datasets.
To further understand the compositional training intuitively, let us take a thought experiment by
using a gradient descent method to optimize the compositional objective. First, we evaluate the
inner function by u = W - αVLAVG(W). We can see that u is computed by a gradient descent step
for minimizing the averaged loss LAVG(W), which facilitates the learning of lower layers for feature
extraction due to equal weights of all examples. Then, we take a gradient descent step to update W
for minimizing the outer function LAUC(∙) by using the gradient VLAUC(U) instead of VLAUC(w).
Because u is better than W in terms of feature extraction layers, taking a gradient descent step using
VLAUC (u) would be better than using VLAUC (W). In addition, taking a gradient descent step for
the outer function LAUC(∙) will make the classifier more robust to the minority class due to the
higher weights of examples from the minority class. Overall, we have two alternating conceptual
steps, i.e., the inner gradient descent step u = W - αVLAVG(W) acts as a feature purification
step, and the outer gradient descent step W - η(I - αV2LAVG(W))VLAUC(u) acts as a classifier
robustification step, where η is a step size.
VS. linear combination approach. It is notable that minimizing the compositional objective
LAUC (W - αVLAVG(W)) is different from minimizing a linear combination of an AUC loss and
the averaged loss, i.e., LAUC (W) + cLAVG (W), where c > 0 is a combination weight. First, minimiz-
ing the latter objective will push VLAUC (W) + cVLAVG(W) = 0. This makes VLAUC (W) to have
an opposite direction from VLAVG(W) at the optimal solution, which is different from minimizing
the compositional objective in light of the three terms in (5). Second, if we take a gradient descent
method for minimizing this objective, the update ofW is given by W-η(VLAUC(W)+cVLAVG(W)).
This update is fundamentally different from the two alternating steps of compositional training that
use gradients of the AUC loss and the average loss at different points. Third, minimizing the linear
combination has a trade-off, meaning that the AUC score is not necessarily maximized due to the
presence of the CE loss in the objective and the learned feature representations are degraded due to
the presence of AUC loss.
More discussions. Finally, we note that the inner gradient descent step W - αVLAVG(W) is sim-
ilar to the idea used in model-agnostic meta learning (MAML) (Finn et al., 2017). However, our
compositional objective works fundamentally different from that for MAML. In MAML, the outer
loss function and the loss function for the inner gradient descent step is the same. In contrast, the
two loss functions in our objective are different. But similar to MAML approaches, we can also run
multiple gradient descent steps for the inner function, i.e, the inner function W - αVLAVG(W) can
be replaced by multiple gradient descent steps. In our experiments, we also found this trick to be
helpful for improving the performance.
3.2	Stochastic Optimization Algorithms
In this subsection, we develop efficient stochastic optimization algorithms for optimizing the compo-
sitional objective (4) for DAM. First, we argue the necessity for such development. (i) the problem is
a min-max form and the objective is a compositional function, which makes computing an unbiased
stochastic gradient of the objective LAUC (W - αVLAVG(W)) impossible. Existing algorithms of
5
Published as a conference paper at ICLR 2022
Algorithm 1 Primal-Dual Stochastic Compositional Adaptive (PDSCA) method for solving (6)
1:	Require Parameters: β0,β1,α, G0,η1,η2
2:	Initialization: W0 = (w0; a0; b0) ∈ Rd+2, θo, uo ∈ Rd+2
3:	for t = 0, 1, ..., T do
4:	Sample two sets of examples denoted by S1, S2
5:	ut+ι = (1 — βo)ut + βoh(W t； Si)
6:	Ot = Vwh(Wt； S1)τ[Vug1(ut+1; S2) + θtVug2(ut+1; S2)]
7:	zt+1 = (1 — β1)zt + β1Ot
8:	z2,t+1 = ht({Oj, j = 0, . . . , t)}	ht can be implemented by that in Appendix B
9:	Wt+i = Wt — ηι √z2zt+* 1+Gj	owith the simplest form ht = 1
10:	θt+i = ∏Ω[θt + η2(g2(ut+1; Si ∪ S2) — Vg3(θt))]
11:	end for
non-convex min-max optimization for DAM that focus on minimizing LAUC (W) (Liu et al., 2019a;
Yuan et al., 2021; Guo et al., 2020a;b) are not applicable due to the presence of inner function
W - gVLavg(w). (ii) Our objective is also different from that of MAML due to that Lauc(∙) is a
min-max form, which renders existing algorithms for MAML (Finn et al., 2017; Fallah et al., 2020)
not applicable. Hence, below we propose an efficient stochastic algorithm for solving the compo-
sitional training for DAM whose objective is of the min-max compositional form, and establish its
convergence rate similar to that of standard SGD for minimizing the standard averaged loss.
In particular, for the considered AUC loss, the compositional objective becomes:
1n
min maxΦ (w - KLAVG(w), a, b, θ) = — φ (w - gVLavg(w), a, b, θ; xi1, yi) .	(6)
w,a,b θ∈Ω	n
i=i
We denote by a tuple W= (w; a; b). For simplicity of presentation, We write φ(w, a, b, θ, Xi,yi) as
φ(W, θ;Xi,yi) = gi(W;Xi,yi) + θg2(w;Xi,yi) - g3(θ),
where
g1(W； xi,yi) =(1 - p) (f (w； Xi) - a) I[yi = 1] + p(f(w; Xi) - b)2I[yi = -1]
+ 2pf (W； xi)I[yi=i] — 2(1 — p)f (W； xi)I[yi=-i] ,
and g2(W; Xi,yi) = 2 (Pf (w; x,Iq-i] - (1 -p)f (w; x,Iqi]) and g3(θ) = p(1 - p)θ2.
(7)
Denote by gi(W; S)= * pi∈5 gi(W; Xi,yi), g2(W; S)= 看 pi∈5 g2(W; Xi,y) Let
h(W)	=	(w	-	gVLavg(w); a;	b),	VWh(W)	=	(I	-	aVWLAVG(w); 1; 1), and	h(W; S)=
(W - αVLAVG (W; S); a; b).
We propose a primal-dual stochastic algorithm shown in Algorithm 1, which is referred to as
PDSCA. We provide some explanations of our algorithmic design. First, the step 5 of updating
ut+i corresponds to feature purification step. We use a moving average technique to update ut+i
that takes all historical updates into account, which is inspired by existing stochastic algorithms for
optimizing compositional functions (Wang et al., 2017). This is important for US to prove the con-
vergence rate of O(1∕√T) without using a large batch size at each iteration. If we simply using
ut+i = h(Wt; Si) (i.e., setting βo = 1) to estimate h(Wt), there will be a large error in estimating
the gradient Vugi(ut+i; S2) and Vug2(ut+i; S2) in step 6. Second, the step 6 is to estimate the
gradient of the outer function. We use two independent mini-batches Si, S2 to ensure that Ot is an
unbiased estimator of Vh(Wt)τVwφ(ut+i, θ). Using two independent mini-batches is also help-
ful for improving generalization as demonstrated in experiments. Third, the steps 7 - 9 are similar
to the momentum and adaptive methods for updating the model parameter. The step 8 is used for
computing the adaptive step size 1∕√z2,t+i + €0, which is similar to adaptive methods used for
deep learning, such as Adam, AMSGrad, AdaBound (Kingma & Ba, 2015; Reddi et al., 2018; Luo
et al., 2019). We use a general function ht in the algorithm, which can be implemented by different
methods corresponding to different adaptive step size choices. We present different ht in the Ap-
pendix B. The simplest one ht = 1 corresponds to that we do not use adaptive step size and only
use the momentum update. Fourth, the step 10 is for updating the dual variable θ using a stochastic
gradient ascent method. Finally, we point out that PDSCA is similar to some existing non-convex
strongly-concave min-max optimization algorithms (Guo et al., 2021) but with additional care on
the inner gradient descent step W - αVLAVG (W). We present an informal convergence of PDSCA
below.
6
Published as a conference paper at ICLR 2022
Theorem 1. (Informal) Under appropriate conditions on the functions LAVG, g1, g2 anda boundness
condition on θt, wt, ▽'(Wt; x,y), V2'(wt; x,y), with β0,β1 = O(1∕√T ),η1,η2 = O(1∕√T)
and a small constant τ, Algorithm 1 ensures that E
F(W) = maxθ∈Ω Φ (w — aVLavg(w), a, b, θ).
[t+t PT=0 kVF(Wt)k2]
≤ O(√T). where
Remark: We will present the detailed conditions in the supplement when proving the above the-
orem due to limit of space. The above theorem indicates that we can optimize the compositional
objective (6) with the same convergence rate as optimizing the averaged loss (1) for deep learning.
Practical Implementations. It is notable that Vh(W,Si) = (I — αV2L(w;Si); 1,1) (SteP 6)
involves the Hessian matrix V2L(w; S1). Indeed, we only need to compute the Hessian vector
Product involving in steP 6. Similar comPutation occurs in the meta learning algorithms (Finn et al.,
2017; Fallah et al., 2020). InsPired by Practical imPlementations of MAML (Finn et al., 2017) that
simPly ignore the second-order term, we use the same trick in our exPeriments. An additional useful
trick insPired by MAML is that we can take k ≥ 1 gradient descent stePs for the inner function,
correspondingly We maintain and update several U variables similar to step 5, i.e., using h(wt, Si)
for uPdating the first ut(+i)i, and using h(ut(+i)i; Si) for uPdating second ut(+2)i, and so on so forth. In
our experiments, We found that tuning k ∈ {1, 2, 3} is useful.
Finally, it is notable that although We focus on optimizing AUC loss for binary classification in this
Work, our compositional training method can be also extended to optimize other Weighted losses in
an end-to-end fashion, and We include some discussion and results in the Appendix D.
4 Experiments
In this section, We present some experimental results. We choose five baselines: optimizing the
AUC loss from scratch (AUCsc), optimizing the CE loss (CE), optimizing a linear combination of
the AUC loss and the CE loss With a tuned Weight (AUC-CE), the tWo-stage method With deferred
re-Weighting trick (Cao et al., 2019) (TS-DRW), the tWo-stage method by decoupling the learning of
feature netWork by minimizing CE loss and the learning of a classifier by minimizing the AUC loss
(TS-DEC) (Kang et al., 2019). We denote our method by CT (AUC). For AUC loss, We use AUCM
loss With the margin parameter fixed to be 1 (Yuan et al., 2020). We conduct experiments on four
benchmark datasetes and four medical image datasets. The statistics of these datasets are included
in the Appendix A.1. More training configurations can be found in Appendix A.2.
Benchmark datasets. We choose four benchmark image classification datasets, namely CatvsDog,
CIFAR10 (C10), CIFAR100 (C100), and STL10 (S10). For AUC maximization, We construct im-
balanced binary versions of these datasets by varying the imbalanced ratios (the ratio of positive
examples to the total number of training examples) similar to (Yuan et al., 2020). We use ResNet20
as the prediction netWork. The Weight decay is set to 1e-4 for all experiments. For algorithms to
maximize AUC, We use a batch size = 128 and train a total of 100 epochs, and We use step size 0.1
and decrease it by 10 times at 50% and 75% of total training time. We tune the beta parameters of
our method in a range [0.1, 0.99] With a grid search and find that good values are around 0.9. For
linear combination methods, We tune the Weight c of tWo losses in {0.25, 0.5, 0.75}. We tune the
number of inner gradient steps for CT in k ∈ {1, 2, 3} With α = 0.1. For all benchmark data, We
run three times for different random seeds and compute the mean and standard deviations.
Medical image datasets. We also conduct experiments on naturally imbalanced medical datasets.
We choose four medical image datasets, namely Melanoma data, CheXpert, DDSM+, and PatchCam
data. The Melanoma dataset is from the Kaggle 2020 competition (Rotemberg et al., 2021), Which
contains 33,126 labeled images in training set, including 584 positive samples and 32,542 negative
samples. We manually construct training, validation and testing datasets folloWing 70/10/20 split.
For this dataset, We use the images With 256x256 resolution in the experiments. CheXpert is a large-
scale chest X-ray dataset (Irvin et al., 2019), Which has 224,316 images With 224 x 224 resolution.
The dataset contains 5 binary classification tasks corresponding to 5 diseases, i.e., Cardiomegaly
(C0), Edema , Consolidation, Atelectasis, Pleural Effusion. We evaluate the performance on the
official validation set consisting of 200 patient studies and report the averaged AUC scores of all
5 diseases. The DDSM+ data is a combination of tWo datasets namely DDSM and CBIS-DDSM
(Lee et al., 2017; BoWyer et al., 1996; Heath et al., 1998), Which consists of 55,890 mammographic
training images (224×224) With an imratio of 13% and 15,364 images for testing With an imra-
tio of 13%. The PatchCamelyon dataset consists of 294,912 color images (96×96) extracted from
7
Published as a conference paper at ICLR 2022
Table 1: Testing performance on benchmark datasets and medical datasets. The percentage number
is the Second row denotes the imbalanced ratio (Cf the text).______________________
Datasets		For AUC Maximization			Datasets	For AUC Maximization	
	-imratio	1%	10%	30%		Method	AUC
	CE	0.742±0.003	0.917±0.006	0.957±0.001		CE	0.879±0.008
	AUCsc	0.753±0.003	0.915±0.002	0.964±0.003		AUCsc	0.868±0.006
	AUC-CE	0.770±0.007	0.939±0.004	0.974±0.003		AUC-CE	0.880±0.005
CAT vs DOG	TS-DRW	0.750±0.009	0.914±0.003	0.961±0.001	Melanoma	TS-DRW	0.878±0.007
	TS-DEC	0.754±0.010	0.918±0.003	0.963±0.001		TS-DEC	0.877±0.005
	CT (AUC)	0.789±0.008	0.946±0.002	0.977±0.001		CT (AUC)	0.900±0.002
	CE	0.689±0.003	0.901±0.002	0.944±0.001		Ce	0.892±0.001
	AUCsc	0.728±0.002	0.905±0.002	0.946±0.001		AUCsc	0.899±0.002
f^'ττ7Λ pin	AUC-CE	0.735±0.003	0.928±0.001	0.957±0.001		AUC-CE	0.902±0.002
CIFAR10	TS-DRW	0.708±0.002	0.896±0.002	0.946±0.003	CheXpert	TS-DRW	0.900±0.002
	TS-DEC	0.707±0.002	0.897±0.002	0.944±0.001		TS-DEC	0.897±0.001
	CT (AUC)	0.739±0.004	0.935±0.001	0.964±0.001		CT (AUC)	0.909±0.003
	Ce	0.655±0.005	0.819±0.004	0.885±0.004		CE	0.949±0.001
	AUCsc	0.665±0.005	0.805±0.017	0.887±0.007		AUCsc	0.929±0.001
CTT 1 ∩	AUC-CE	0.668±0.007	0.836±0.006	0.905±0.001	TrYrΛV"∖ Λ"-l	AUC-CE	0.957±0.001
STL10	TS-DRW	0.655±0.004	0.803±0.013	0.887±0.002	DDSM+	TS-DRW	0.942±0.003
	TS-DEC	0.661±0.002	0.816±0.007	0.882±0.007		TS-DEC	0.941±0.001
	CT (AUC)	0.673±0.010	0.837±0.006	0.906±0.001		CT (AUC)	0.981±0.001
	Ce	0.586±0.001	0.691±0.005	0.758±0.004		CE	0.869±0.007
	AUCsc	0.606±0.004	0.705±0.003	0.779±0.003		AUCsc	0.868±0.006
f^'TT7Λ P 1 ∩∩	AUC-CE	0.605±0.004	0.716±0.003	0.795±0.001		AUC-CE	0.868±0.005
CIFAR100	TS-DRW	0.588±0.003	0.691±0.004	0.762±0.001	PatchCam	TS-DRW	0.867±0.006
	TS-DEC	0.587±0.001	0.692±0.003	0.762±0.002		TS-DEC	0.869±0.009
	CT (AUC)	0.609±0.002	0.725±0.001	0.809±0.002		CT (AUC)	0.891±0.003
histopathologic scans of lymph node section for training and 32,768 images for testing with bal-
anced class ratio (Veeling et al., 2018; Bejnordi et al., 2017). For PathCamelyon, we manually
construct an imbalanced training dataset with an imratio of 1% and keep the testing set balanced.
For Melanoma data, we adopt a EfficientNetV2-S (Tan & Le, 2021) as the network structure, and for
CheXpert, DDSM+, PatchCam, we use DenseNet121 (Huang et al., 2017). We tune the number of
inner gradient steps for CT in k ∈ {1, 2, 3} and also tune α in {0.1, 0.05, 0.01} for the inner steps.
Results. The testing AUC results are reported in Table 1. We can see that the proposed composition
training method outperforms all baselines on all datasets for maximizing AUC. In addition, we have
the following observations: (i) optimizing an AUC loss from scratch does not necessarily yield a
better performance than minimizing the standard CE loss; (ii) the CT (AUC) method dramatically
improves the performance of optimizing the AUC loss from scratch, with about 2%〜5% improve-
ment on difficult medical classification tasks; (iii) the CT (AUC) method is generally better than the
linear combination approach (AUC-CE), especially on the more difficult medical image datasets. It
is notable that the reported results on some medical datasets are not comparable with that in (Yuan
et al., 2020) because (i) we report the performance on official CheXpert validation data instead of
the official testing data; (ii) we use a smaller resolution on Melanoma data; (ii) we do not tune the
margin parameter in the AUCM loss. We also plot the learned feature representations of training
data of different datasets visualized by t-SNE in Figure 3. We can see that the proposed CT (AUC)
method obtains better feature representations than the baseline approaches of optimizing CE or AUC
alone and the naive linear combination approach.
4.1 Ablation Study
We conduct some ablation study including (i) the comparison of convergence curves and the running
time analysis of different methods; (ii) the verification of our algorithmic design.
Convergence Curve. Below, we compare the convergence speed of our CT (AUC) approach
with other end-to-end learning baselines, i.e., CE, AUC, AUC-CE, with results on four benchmark
datasets plotted in Figure 4. The results indicate that our algorithm enjoys even faster convergence in
terms of number of epochs. The convergence curves of testing AUC are included in Appendix A.4.
Runtime analysis. We notice our method has larger running time per-iteration than minimizing the
CE and the AUC loss alone because of several backpropagations, but with a reward of faster con-
vergence in epochs and better testing performance. For fair comparison, we have run the baselines
with the same amount of time as our method and observed they are still worse than our method (cf
Appendix A.5). For example, on CIFAR10 (10%), CT (AUC) can achieve an AUC of 0.944 with a
running time of 1000s, in contrast, the baselines CE, AUC, AUC-CE, TS-DRW, and TS-DEC use
same running time and achieve AUC scores of 0.925, 0.915, 0.943, 0.917, 0.917, respectively.
8
Published as a conference paper at ICLR 2022
Figure 3: t-SNE visualization of training data (• is positive and • is negative) by (from top to bottom)
optimizing the CE loss, an AUC loss from scratch, a linear combined loss, and our CT method.
Figure 4: Convergence curves on four benchmark datasets with an imbalance ratio of 10%.
Verification of Algorithmic Design. We validate three algorithmic choices. (i) Using two inde-
pendent mini-batches S1 6= S2 is generally better than using the same mini-batch. (ii) Using the
momentum update for ut+1 (i.e., β0 < 1) is better than without using momentum update (β0 = 1).
(iii) tuning the number of inner gradient steps k ∈ {1, 2, 3} is helpful for improving the perfor-
mance. The results are demonstrated in Table 2, where all results are averaged over three trials.
Table 2: Left: S1 6= S2 vs S1 = S2, right: β0 = 1 vs β0 < 1 in Algorithm 1. Note that we tune
k ∈ {1, 2, 3} for for the left table and fix k = 1 for the right table. left (S1 6= S2) vs right (β0 ≤ 1)
verifies that tuning k is helpful.
Dataset	Method		1%	Imbalance Ratio 10%	I 30%	Method		1%	Imbalance Ratio 10%	30%
CATvsDOG	S1	S2	0.784±0.007	0.941±0.002	0.975±0.001	β0	1	0.765±0.005	0.937±0.004	0.971±0.002
	S1 6= S2		0.789±0.008	0.946±0.002	0.977±0.001	β0 < 1		0.769±0.007	0.939±0.004	0.975±0.006
CIFAR10	S1	S2	0.738±0.005	0.931±0.002	0.959±0.000	β0	1	0.724±0.006	0.928±0.004	0.957±0.001
	S1 6= S2		0.740±0.004	0.935±0.001	0.964±0.001	β0 < 1		0.725±0.011	0.929±0.002	0.960±0.001
STL10	S1	S2	0.671±0.007	0.820±0.025	0.902±0.003	β0	1	0.663±0.012	0.819±0.018	0.895±0.004
	S1 6= S2		0.681±0.005	0.839±0.004	0.907±0.001	β0 < 1		0.666±0.006	0.832±0.008	0.900±0.002
CIFAR100	S1	S2	0.608±0.004	0.709±0.002	0.788±0.005	β0	1	0.590±0.014	0.714±0.004	0.791±0.004
	S1 6= S2		0.609±0.002	0.725±0.000	0.809±0.002	β0 < 1		0.598±0.002	0.714±0.003	0.801±0.001
5	Conclusions
In this paper, we have proposed a novel end-to-end compositional training framework for deep AUC
maximization by optimizing a compositional objective. We also proposed an efficient stochastic
optimization method for compositional training of deep AUC maximization. We demonstrated the
effectiveness of compositional training on multiple benchmark datasets and medical datasets for
maximizing AUC. In future work, we will investigate compositional training for other imbalanced
loss functions more extensively.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank anonymous reviewers for their valuable comments. This work was partially supported by
NSF Career Award #1844403, NSF Award #2110545, NSF Award #1933212.
References
Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico
Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson,
Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of
lymph node metastases in women with breast cancer. Jama, 318(22):2199-2210, 2017.
K Bowyer, D Kopans, WP Kegelmeyer, R Moore, M Sallam, K Chang, and K Woods. The digital
database for screening mammography. In Third international workshop on digital mammography,
volume 58, pp. 27, 1996.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems, pp. 1567-1578, 2019.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 9268-9277, 2019.
Damien Dablain, Bartosz Krawczyk, and Nitesh V. Chawla. Deepsmote: Fusing deep learning and
SMOTE for imbalanced data. CoRR, abs/2105.02340, 2021. URL https://arxiv.org/
abs/2105.02340.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In International Conference on Artificial Intelli-
gence and Statistics, pp. 1082-1092. PMLR, 2020.
C Ferri, PA Flach, and J Herngndez-Orallo. Learning decision trees using the area under the roc
curve. In Claude Sammut and Achim Hoffmann (eds.), Proceedings of the 19th International
Conference on Machine Learning, pp. 139 - 146. Morgan Kaufmann, 2002. ISBN 1558608737.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th Interna-
tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
volume 70 of Proceedings of Machine Learning Research, pp. 1126-1135. PMLR, 2017. URL
http://proceedings.mlr.press/v70/finn17a.html.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for
combining preferences. J. Mach. Learn. Res., 4(null):933-969, December 2003. ISSN 1532-
4435.
Lawrence Fulton, Alex McLeod, Diane Dolezel, Nathaniel Bastian, and Christopher P Fulton. Deep
vision for breast cancer classification and segmentation. Cancers, 13(21):5384, 2021.
Wei Gao and Zhi-Hua Zhou. On the consistency of AUC pairwise optimization. In Qiang Yang and
Michael J. Wooldridge (eds.), Proceedings of the Twenty-Fourth International Joint Conference
on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 939-945.
AAAI Press, 2015. URL http://ijcai.org/Abstract/15/137.
Wei Gao, Rong Jin, Shenghuo Zhu, and Zhi-Hua Zhou. One-pass auc optimization. In International
conference on machine learning, pp. 906-914, 2013.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approxi-
mation method for nested stochastic optimization. SIAM J. Optim., 30(1):960-979, 2020. doi:
10.1137/18M1230542. URL https://doi.org/10.1137/18M1230542.
10
Published as a conference paper at ICLR 2022
Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, and Tianbao Yang. Communication-
efficient distributed stochastic AUC maximization with deep neural networks. In Proceedings of
the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual
Event, volume 119 of Proceedings ofMachine Learning Research,pp. 3864-3874. PMLR, 2020a.
URL http://proceedings.mlr.press/v119/guo20f.html.
Zhishuai Guo, Zhuoning Yuan, Yan Yan, and Tianbao Yang. Fast objective and duality gap conver-
gence for non-convex strongly-concave min-max problems. CoRR, abs/2006.06889, 2020b. URL
https://arxiv.org/abs/2006.06889.
Zhishuai Guo, Yi Xu, Wotao Yin, Rong Jin, and Tianbao Yang. On stochastic moving-average
estimators for non-convex optimization. CoRR, abs/2104.14840, 2021. URL https://arxiv.
org/abs/2104.14840.
Michael Heath, Kevin Bowyer, Daniel Kopans, Philip Kegelmeyer, Richard Moore, Kyong Chang,
and S Munishkumaran. Current status of the digital database for screening mammography. In
Digital mammography, pp. 457-460. Springer, 1998.
Ralf Herbrich, Thore Graepel, and Klause Obermayer. Large Margin Rank Boundaries for Ordinal
Regression. In Advances in Large Margin Classifiers, chapter 7, pp. 115-132. The MIT Press,
1999. URL http://www.herbrich.me/papers/nips98_ordinal.pdf.
Alan Herschtal and Bhavani Raskutti. Optimising area under the ROC curve using gradient de-
scent. In Carla E. Brodley (ed.), Machine Learning, Proceedings of the Twenty-first International
Conference (ICML 2004), Banff, Alberta, Canada, July 4-8, 2004, volume 69 of ACM Inter-
national Conference Proceeding Series. ACM, 2004. doi: 10.1145/1015330.1015366. URL
https://doi.org/10.1145/1015330.1015366.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest
radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 590-597, 2019.
Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong.
Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation
perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 7610-7619, 2020.
Thorsten Joachims. A support vector method for multivariate performance measures. In Pro-
ceedings of the 22nd International Conference on Machine Learning, ICML ’05, pp. 377-384,
New York, NY, USA, 2005. Association for Computing Machinery. ISBN 1595931805. doi:
10.1145/1102351.1102399. URL https://doi.org/10.1145/1102351.1102399.
Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal
of Big Data, 6(1):27, 2019.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint
arXiv:1910.09217, 2019.
Purushottam Kar, Bharath Sriperumbudur, Prateek Jain, and Harish Karnick. On the generalization
ability of online learning algorithms for pairwise loss functions. In Sanjoy Dasgupta and David
McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, vol-
ume 28 of Proceedings of Machine Learning Research, pp. 441-449, Atlanta, Georgia, USA, 17-
19 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/kar13.html.
Purushottam Kar, Harikrishna Narasimhan, and Prateek Jain. Online and stochastic gradient meth-
ods for non-decomposable loss functions. In Proceedings of the 27th International Conference
on Neural Information Processing Systems - Volume 1, NIPS’14, pp. 694-702, Cambridge, MA,
USA, 2014. MIT Press.
11
Published as a conference paper at ICLR 2022
Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, and Roberto Togneri.
Cost-sensitive learning of deep feature representations from imbalanced data. IEEE transactions
on neural networks and learning Systems, 29(8):3573-3587, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Hansang Lee, Minseok Park, and Junmo Kim. Plankton classification on imbalanced large scale
database via convolutional neural networks with transfer learning. In 2016 IEEE international
conference on image processing (ICIP), pp. 3713-3717. IEEE, 2016.
Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai Miyake, Mia Gorovoy, and
Daniel L Rubin. A curated mammography data set for use in computer-aided detection and
diagnosis research. Scientific data, 4(1):1-9, 2017.
Tianyi Lin, Chi Jin, and Michael I Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. arXiv preprint arXiv:1906.00331, 2019.
TsUng-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dolldr. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision, pp.
2980-2988, 2017.
MingrUi LiU, XiaoxUan Zhang, Zaiyi Chen, XiaoyU Wang, and Tianbao Yang. Fast stochastic aUc
maximization with o (1/n)-convergence rate. In International Conference on Machine Learning,
pp. 3195-3203, 2018.
MingrUi LiU, ZhUoning YUan, Yiming Ying, and Tianbao Yang. Stochastic aUc maximization with
deep neUral networks. arXiv preprint arXiv:1908.10831, 2019a.
Ziwei LiU, Zhongqi Miao, Xiaohang Zhan, JiayUn Wang, Boqing Gong, and Stella X. YU. Large-
scale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2537-2546.
CompUter Vision FoUndation / IEEE, 2019b. doi: 10.1109/CVPR.2019.00264. URL http:
//openaccess.thecvf.com/content_CVPR_2019/html/Liu_Large-Scale_
Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.html.
Liangchen LUo, YUanhao Xiong, Yan LiU, and XU SUn. Adaptive gradient methods with dynamic
boUnd of learning rate. In 7th International Conference on Learning Representations (ICLR),
2019.
David Masko and PaUlina Hensman. The impact of imbalanced training data for convolUtional
neUral networks, 2015.
Aditya Krishna Menon, Sadeep JayasUmana, Ankit Singh Rawat, HimanshU Jain, Andreas Veit, and
Sanjiv KUmar. Long-tail learning via logit adjUstment. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=37nvvqkCo5.
Michael Natole, Yiming Ying, and Siwei LyU. Stochastic proximal algorithms for aUc maximization.
In International Conference on Machine Learning, pp. 3707-3716, 2018.
Michael Natole, Yiming Ying, and Siwei LyU. Stochastic aUc optimization algorithms with linear
convergence. Frontiers in Applied Mathematics and Statistics, 5, 2019.
Qi Qi, Yi XU, Rong Jin, Wotao Yin, and Tianbao Yang. Attentional biased stochastic gradient for
imbalanced classification. CoRR, abs/2012.06951, 2020. URL https://arxiv.org/abs/
2012.06951.
Alain Rakotomamonjy. SUpport vector machines and area Under roc cUrves. Technical report, 2004.
Sashank J. Reddi, Satyen Kale, and Sanjiv KUmar. On the convergence of adam and beyond. In 6th
International Conference on Learning Representations (ICLR), 2018.
Mengye Ren, WenyUan Zeng, Bin Yang, and RaqUel UrtasUn. Learning to reweight examples for
robUst deep learning. arXiv preprint arXiv:1803.09050, 2018.
12
Published as a conference paper at ICLR 2022
Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil
Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al.
A patient-centric dataset of images and metadata for identifying melanomas using clinical context.
Scientific data, 8(1):1-8, 2021.
Eric Scuccimarra. Ddsm mammography: tfrecords files of scans from the ddsm dataset. Online,
2021. URL https://www.kaggle.com/skooch/ddsm-mammography.
Mingxing Tan and Quoc V Le. Efficientnetv2: Smaller models and faster training. arXiv preprint
arXiv:2104.00298, 2021.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9:2579-2605, 2008.
Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv-
ariant cnns for digital pathology. In International Conference on Medical image computing and
computer-assisted intervention, pp. 210-218. Springer, 2018.
Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):
419-449, 2017.
Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X. Yu. Long-tailed recognition
by routing diverse distribution-aware experts. CoRR, abs/2010.01809, 2020. URL https:
//arxiv.org/abs/2010.01809.
Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu. Long-tailed recognition by
routing diverse distribution-aware experts. In International Conference on Learning Representa-
tions, 2021a. URL https://openreview.net/forum?id=D9I3drBz4UC.
Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi,
Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural networks
for molecular property prediction and drug discovery, 2021b.
Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced
knowledge distillation for long-tailed classification. In Andrea Vedaldi, Horst Bischof, Thomas
Brox, and Jan-Michael Frahm (eds.), Computer Vision - ECCV 2020 - 16th European Confer-
ence, Glasgow, UK, August 23-28, 2020, Proceedings, Part V, volume 12350 of Lecture Notes
in Computer Science, pp. 247-263. Springer, 2020. doi: 10.1007/978-3-030-58558-7\_15. URL
https://doi.org/10.1007/978-3-030-58558-7_15.
Lian Yan, Robert Dodier, Michael C. Mozer, and Richard Wolniewicz. Optimizing classifier perfor-
mance via an approximation to the wilcoxon-mann-whitney statistic. In Proceedings of the Twen-
tieth International Conference on International Conference on Machine Learning, ICML’03, pp.
848-855. AAAI Press, 2003. ISBN 1577351894.
Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learn-
ing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 19290-19301. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
e025b6279c1b88d3ec0eca6fcb6e6280- Paper.pdf.
Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In Advances in
neural information processing systems, pp. 451-459, 2016.
Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Robust deep AUC maximization: A
new surrogate loss and empirical studies on medical image classification. CoRR, abs/2012.03173,
2020. URL https://arxiv.org/abs/2012.03173.
Zhuoning Yuan, Zhishuai Guo, Yi Xu, Yiming Ying, and Tianbao Yang. Federated deep AUC max-
imization for hetergeneous data with a constant communication complexity. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Re-
search, pp. 12219-12229. PMLR, 2021. URL http://proceedings.mlr.press/v139/
yuan21a.html.
13
Published as a conference paper at ICLR 2022
Xinhua Zhang, Ankan Saha, and S. V. N. Vishwanathan. Smoothing multivariate performance mea-
SUres. J. Mach. Learn. Res.,13(1):3623-3680, December 2012. ISSN 1532-4435.
Peilin Zhao, Steven C. H. Hoi, Rong Jin, and Tianbao Yang. Online auc maximization. In ICML,
pp. 233-240, 2011.
Yan Zheng, Yuchen Zheng, Daiki Suehiro, and Seiichi Uchida. Top-rank convolutional neural net-
work and its application to medical image-based diagnosis. Pattern Recognition, 120:108138,
2021.
Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: bilateral-branch network with
cumulative learning for long-tailed visual recognition. In 2020 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp.
9716-9725. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00974. URL https://doi.org/
10.1109/CVPR42600.2020.00974.
Linchao Zhu and Yi Yang. Inflated episodic memory with region self-attention for long-tailed
visual recognition. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 4343-4352. IEEE, 2020. doi: 10.
1109/CVPR42600.2020.00440. URL https://doi.org/10.1109/CVPR42600.2020.
00440.
14
Published as a conference paper at ICLR 2022
A Supplement of Experimental Section
A. 1 Dataset Descriptions
The detailed statistics of different datasetse are reported in Table 3. Note that ”# of images” refers to
the number of samples for the original training set. "LT" denote long-tailed version of the datasets
for multi-class tasks. Imbalance ratio (imratio) of XXX (Binary) data means the ratio of number of
positive examples to number of all examples. Imbalance ratio (imratio) of XXX (LT) data means
ration of the size of smallest class to the size of largest class. The DDSM+ is slightly different from
the standard version of CBIS-DDSM or DDSM (Lee et al. (2017); Bowyer et al. (1996); Heath et al.
(1998)). We actually use the dataset from (Scuccimarra (2021)) constructed by Eric A. Scuccimarra,
which consists of 55k images for training and 15k images for testing. The details about the dataset
construction can be found here (Scuccimarra (2021); Zheng et al. (2021); Fulton et al. (2021)).
For constructing DDSM+, the positive samples (cancer cases) are from CBIS-DDSM and negative
samples (normal cases) are from DDSM. To increase the size of the training data, the author applies
offline data augmentation and adds multiple augmented copies to the dataset. In particular, each
image (ROI) is randomly cropped three times into 598x598 images, with random flips and rotations,
and then the images are resized down to 299x299. For the testing set, the same augmentation is also
applied and thus the imbalance ratios remain the same as the training set. The imbalance ratio is
about 13% in both training and testing sets. The train/test split in terms of patient ID follows the
CBIS-DDSM split, which do not have any overlap. We will add these references in the revision and
also correct the citation for CBIS-DDSM. Please also note that this dataset (DDSM+) has also been
used for some recent works (Zheng et al. (2021); Fulton et al. (2021)).
Table 3: Description of datasets for classification tasks.
Dataset	# of images	# of classes	Imbalance Ratio
CATvsDOG (binary)	20,000	2	1%, 10%, 30%
CIFAR10 (binary)	50,000	2	1%, 10%, 30%
CIFAR100 (binary)	50,000	2	1%, 10%, 30%
STL10 (binary)	5,000	2	1%, 10%, 30%
CheXpert	223,416	2	12.2%, 32.2%, 6.8%, 31.2%, 40.3%
Melanoma	33,126	2	1.76%
DDSM+	55,000	2	13%
PatchCam	294,912	2	1%
CIFAR10 (LT)	50,000	10	1%, 10%
CIFAR100 (LT)	50,000	100	1%, 10%
STL10 (LT)	5,000	10	1%, 10%
ImageNet (LT)	115,800	1000	0.39%
A.2 Training Configurations
All benchmark datasets are experimented by NVIDIA GTX-2080Ti and four medical datasets,
i.e., CheXpert, Melanoma, DDSM+ and PatchCam, are experimented by NVIDIA V100. For the
datasets of binary classification in Table 1, we use the dataloaders from (Yuan et al., 2020). For the
long-tailed datasets of multi-class classification as in Section D, we uses the dataloaders from (Cui
et al., 2019). For TS-DRW, we train the models using cross-entropy loss at the first stage and then
switch to imbalanced losses at the later stages. For TS-DEC, we first conduct the regular training
using cross-entropy loss and same settings and then we discard the trained classifiers and finetune
the new classifier for additional 10 epochs using imbalanced loss with the learning rate of 0.01. For
the proposed compositional training methods, We consider the non-adaptive version with ht(∙) = 1
in all experiments. For all datasets, we use the train/val split to do cross-validation for parameter
tuning, except CheXpert as explained below. For the benchmark datasets, we use 19k/1k, 45k/5k,
45k/5k. 4k/1k training/validation split on CatvsDog, CIFAR10, CIFAR100, STL10, respectively.
For melanoma dataset, we use 70/10/20 split for train/val/test. For PatchCam, we use their official
validation set for tuning parameters, which includes about 37k images with balanced positive and
negative samples. For DDSM+, we tune the parameters on 10% data sampled from the training set.
For CheXpert, since the official testing set is not released and it will take a long time to evaluate all
methods on the official testing data, hence, we evaluate different methods only based on the official
validation set with parameters tuned according to this set. To make the experiment on Chexpert con-
15
Published as a conference paper at ICLR 2022
sistent with other datasets, we run experiments for all methods on CheXpert by following the same
cross-validation procedure, i.e., by sampling 10% training data based on patient ID as the validation
set to tune parameters and then we report the average scores of five diseases on the testing set (i.e.,
the official validation set as a testing set).
A.3 Evaluations on Medical Dataset with multiple runs
For medical datasets, we run all experiments and report the average performance over three runs.
We use batch size of 32 except for PatchCam that is 64, initial learning rate of 0.1 and weight decay
of 1e-5. We train Melanoma for 12 epochs, CheXpert for 2 epochs, DDSM+ for 5 epochs and
PatchCam for 5 epochs. The learning rate is decayed at 50%, 75% of total training iterations by
10 times. For compositional training, we tune the number of inner gradient steps in k ∈ {1, 2, 3}
and also tune α ∈ {0.1, 0.05, 0.01} for the inner steps. The results are summarized in the following
table.
Table 4: Testing performance on medical datasets.
Method eo	Melanoma	CheXPert	DDSM+	PatchCam
		AUC		
CE	0.879±0.008	0.892±0.001	0.949±0.001	0.869±0.007
AUC	0.868±0.006	0.899±0.002	0.929±0.001	0.868±0.006
AUC-CE	0.880±0.005	0.902±0.002	0.957±0.001	0.868±0.005
TS-DRW	0.878±0.007	0.900±0.002	0.942±0.003	0.867±0.006
TS-DEC	0.877±0.005	0.897±0.001	0.941±0.001	0.869±0.009
CT (AUC)	0.900±0.002	0.909±0.003	0.981±0.001	0.891±0.003
A.4 Testing Convergence Curves.
The convergence curves of testing AUC on the benchmark datasets of different methods are plotted
in Figure 5.
Figure 5: Convergence curves of testing AUC on four benchmark datasets with imbalance ratio of
10%.
A.5 Comparison with the same Runtime
To compare the performance with the same running time, we train ResNet20 on four benchmark
datasets with an imbalance ratio of 10% on a GTX-2080Ti. For each method, we train 1000 seconds
and report the best achieved testing AUC. The results are summarized in Table 5.
Table 5: Achieved testing AUC for each method after training ResNet20 for 1000 seconds.
Dataset	CT (AUC)	CE	AUC	AUC-CE	TS-DRW	TS-DEC
CATvsDOG (10%)	0.944	0.925	0.915	0.943	0.917	0.917
CIFAR10 (10%)	0.936	0.900	0.905	0.928	0.897	0.898
STL10 (10%)	0.837	0.815	0.783	0.829	0.816	0.815
CIFAR100 (10%)	0.724	0.691	0.701	0.718	0.696	0.696
A.6 Evolution of different terms of the compositional objective.
To better understand the proposed compositional objective, we plot the evolution curves of each term
in the decomposition equation (5) and compare them with that of naive linear combination approach
16
Published as a conference paper at ICLR 2022
(Linear Comb.). We conduct experiments on CATvsDOG, CIFAR10, CIFAR100 and STL10 with
imbalance ratio of 10% using ResNet20. We start with initial learning rate of 0.1 and decay it at 50th,
75th epoch by 0.1. In Figure 6, we plot the values of LAUC(w), LAVG(w), VLauc(w)>NLAVG(w),
∣∣VLavg(w)k2 v.s. the number of epochs on training set. For the calculations of LAUC(w), We
compute its values based on the optimal values of a, b, α according to (Yuan et al., 2020) for each
epoch. Regarding the calculations of w , we compute the mean values of all layers of models. We
defer results on other datasets to appendix. From the results, we observe that initially LAUC (w)
dominate the objective and keep decreasing at earlier iterations. When it reaches similar level of
the third term ∣VLAVG(w)∣, the objective shifts its focus on third term and pushes it to smaller
while maintaining second term positive. Eventually, we can see that both AUC and CE losses of
CT method reach to a level close to zero. In addition, comparing CT with the linear combination
method, it is amazing to see that CT drives both the CE loss and the AUC loss decrease faster and
to a smaller level than the linear combination method.
Figure 6: Evolution of each term of compositional objective function on CATvsDog, CIFAR10,
STL10 and CIFAR100 datasets (from top to bottom).
B	IMPLEMENTATION OF z2,t+1 = ht (O0 , O1 , . . . , Ot)
The ht is usually implemented by a recursion, where examples are given in Table 6. Similar to (Guo
et al., 2021), we make the following assumption for our analysis.
Assumption 1. For the Adam-Style algorithms in Table 6, we assume that St = 1∕(√z2,t+1 + G0)
is upper bounded and lower bounded, i.e., there exists 0 < cl < cu such that ∀i, cl ≤ ∣st,i ∣ ≤ cu,
where st,i denotes the i-th element of st.
Remark: Different implementations of ht that satisfy this assumption is presented in Table 6.
We need the following lemma to tackle the variance recursion.
Lemma 1 (Lemma 2, Ghadimi & Wang (2018)). Consider a moving average sequence zt+1 = (1 -
β)zt + βtOh(xt) for tracking h(xt), where E[Oh(xt)] = h(xt) and h is a L-Lipschitz continuous
17
Published as a conference paper at ICLR 2022
Table 6: Different Adam-style methods and their satisfactions of Assumption 1					
method	update for ht	Additional assumption		cl and cu	
SHB	ht(∙) = 1,G = O	-		cl = 1, cu =	1
Adam	Z2,t+1 = (1 - βt0)Z2,t + βt0Ot2	∣Qt∣∞≤ G	cl	=g+g0 , cu	_	1 一用
AMSGrad	Z02,t+1 = (1 -βt0)Z02,t +βt0Ot2 Z2,t+1 = max(Z2,t, Z02,t+1)	kOtk∞ ≤ G	cl	=G+1G0 , cu	_ ɪ 一G0
AdaFom (AdaGrad)	z2,t+1 = t⅛ Ptt=O Ot	kOtk∞ ≤ G	cl	=g+go , cu	_ ɪ 一G0
Adabound	Z02,t+1 = (1 - βt0)Z02,t + βt0 Ot2 z2,t+1 = π[1∕cU,1∕c2] [z2,t+1],	GO 二	0-		cl = cl , cu =	cu
mapping. Then we have
EtkZt+ι - h(xt)k2 ≤ (1 - βt)∣% - h(xt-1)k2 + 2β2Et∣Qh(xt) - h(xt)k2 + L2kxt - XtTk2,
βt
(8)
where Et denotes the expectation conditioned on all randomness before Oh(xt).
C Proof of Theorem 1
Denote η = ηιst, where St = 1∕(√z2t+ι + G0). We make the following assumptions regarding
problem 6.
Assumption 2.
•	VLAVG(W) is CLAVG-Lipschitz continuous, gι(W) is CgI-Lipschitz continuous and g2(w) is Cg2-
Lipschitz continuous.
•	V2LAVG(w) is LLAyG-LiPSchitz continuous, VgI(W) is LgI-LipSchitz continuous, and Vg2(W) is
Lg2 -Lipschitz continuous.
•	ElIaVLAVG(w) — αVLAVG(w; S)k2 ≤ σ2, E∣∣αV2LAVG(W) — αV2LAVG(w; S)k2 ≤ σ2,
EkVg1 (W) - Vg1(W; S)k2 ≤ σ2, Ekg1(W) - g1(W; S)k2 ≤ σ2, EkVg2(W) - Vg2(W; S)k2 ≤
σ2, Ekg2 (w) - g2(w; S)k2 ≤ σ2.
•	Ω is a bounded convex set with radius D.
It is notable that the last assumption can be replaced by a condition that the dual variables θt are
bounded.
We also know that g3(θ) is λ := 2p(1 - p)-strongly convex and also Lg3 = λ-smooth. De-
note θ*(w) = argmaxΦ(w,θ). Based on Assumption 2, We have that h(w) is (Ch := 1 +
θ∈Ω
aCLAVG)-Lipschitz continuous, Vh(W) is (Lh :=1 +。刀工惭)-Lipschitz continuous, EkVh(W)-
Vh(W; S)k2 ≤ (1 + D2)σ2, andEkh(W) — h(W; S)∣2 ≤ (1 + D2)σ2. We also know that F(W) is
LF-smooth, where LF := (Lgι + DLg2 + λ) + (LgI +DLg2 +λ) (Lemma 4.3 of (Lin et al., 2019)).
We present Theorem 1 formally in the following Theorem.
Theorem 2. Assume F(Wo) 一 F* ≤ Δf where F = min F(W). Suppose Assumptions 1
W
and 2 hold. With βo = O(1∕√T), βι = O(1∕√T), βo = O(1∕√T), βι = O(1∕√T),
ηι ≤ min{ ∖Jc3C 2, c3(C1λ2+512%) 2Cλ^, 2cf⅛F}, and η2 = O(1∕√T) where Ci and C3 are
proper constants specified in the proof, Algorithm 1 can ensure that
E [t⅛-γ X ∣vf (W t)k2l ≤ O( √T).
+ t=0	T
To prove this theorem, we first need a couple of lemmas.
18
Published as a conference paper at ICLR 2022
Lemma 2. Suppose Assumption 1 and Assumption 2 hold. Considering the update in Algorithm 1,
we have
F (Wt+1) ≤ F (W t) + η12u ∣∣Vh9ι(h(W t)) + θ*(h(W t))V%92(h(W t))] - zt+ι∣∣2
-与kVF(Wt)k2-与∣∣zt+ι∣∣2.
(9)
ProofofLemma 2. Due to the smoothness of F, We can prove that under η∖Lp ≤ CII@小,
F(Wt+1) ≤ F(Wt) + VF(Wt)>(Wt+1 - Wt) + lfIlWt+1 - Wtk2
=F(Wt) - VF(Wt)τ(ηt。zt+ι) + lFIlnt。zt+ιk2
=F (W t) +1 ∣∣√nt ◦ (VF (W t) - zt+ι)∣∣2 -1 ∣∣√nt OVF (W t)∣2 + ( Lf knt。zt+ιk2 - 2 ∣∣√nt。zt+j∣2)
≤ F(Wt) + 等∣∣[V%gι(h(Wt)) + θ*(h(Wt))Vw(h(Wt))] - zt+ιk2
-等IVF(Wt)∣2 + n2cu"…Izt川2
≤ F (W t)+ n12u k[Vhgi(h(W t))+ θ(h(W t))Vhg2(h(W t))] - zt+ιk2 - 'n^~ i∣vf (W t )∣∣2 - n41 ∣∣zt+ιk2∙
□
Lemma3. Let %+ι = ∏ω[θt + n2(g2(ut+1; Si US2) - Vg3(%))], we have
∣∣θt+ι - θ*(h(Wt+i))∣∣2 ≤(1 - n2-)Ekθt - θ*(h(Wt))∣∣2 + 2niσ2
16n -	C 4L2	C(10)
+--『Cg2E∣ut+1 - h(Wt)ll + n~λEkh(Wt) - h(Wt+i)∣∣
where Lq := C2 is the Lipschitz continuous constant of θ*(∙).
ProofofLemma3. Since θ*(h(Wt)) = ∏n[θ*(h(Wt)) + n2(g2(h(Wt)) - Vg3(θ*(h(Wt))))], we
have
E∣θt+1- θ*(h(Wt))∣∣2
=E∣∏Ω[θt + n2(g2(ut+1; Si US2) -Vg3(θt))] - ∏Ω[θ*(h(Wt))+ n2(g2(h(Wt)) - Vg3(θ*(h(Wt))))]∣2
≤ E∣[θt + n2(g2(ut+i; Si US2)-Vg3(θt))] -[θ*(h(Wt)) + n2(g2(h(Wt))-Vg3(θ*(h(Wt))))]∣2
=E∣∣[θt + n2(g2(ut+i; Si U S2) - Vg3(θt)) - n2g2(ut+i) + n2g2(ut+i)]
-[θ*(h(Wt)) + n2(g2(h(Wt)) - Vg3(θ*(h(Wt))))]∣
≤ Ell [θt+ n2S2(Ut+i) - vg3(θt))] - W*(h(Wt))+ n2(g2(h(Wt)) - Vg3(o*(h(Wt))))]∣∣2
+ n2E∣∣g2(ut+i;Si US2) - g2(ut+i)∣∣2
≤ E∣∣[θt + n2(g2(ut+i)-Vg3(θt))]-[θ*(h(Wt)) + n2(g2(h(Wt))-Vg3(θ*(h(Wt))))]∣2 + n2σ2,
(11)
where
Ell[θt+ n2(g2(Ut+i) - vg3(θt))] - [θ*(h(Wt)) + n2(g2(h(Wt))- Vg3(o*(h(Wt))))]∣∣2
=E∣∣θt - θ*(h(Wt))∣2 + n2E∣∣[g2(ut+i) - Vg3(θt)] - [g2(h(Wt)) - Vg3(θ*(h(Wt)))]∣2
+ 2n2hθt - θ*(h(Wt)), [g2(ut+i) - Vg3(θt)] - [g2(h(Wt)) - Vg3(θ*(h(Wt)))]i
≤ E∣∣θt - θ*(h(Wt))∣2 + 2n2Cg2E∣ut+i - h(Wt)∣∣2 + 2n2Lg3E∣∣θt - θ*(h(Wt))∣2	(12)
+ n^ Ekθt- θxh(Wt))I∣2 +■ -λ C22EkUt+i- h(W t )∣∣2 - 2n2λEIlθt- θ*(h(W t))∣ι2
≤ (I- n2λ)EIIθt- θYh(Wt))Il2 + -ɪC22EIlUt+i - h(Wt)Il2,
19
Published as a conference paper at ICLR 2022
where the first inequality uses strong monotone inequality as gɜ(-) is λ-strongly convex and the
second inequality uses 小 ≤ min{或L, 2}. Then,
L93
ElI仇+1- θ*(h(wt+ι))∣∣2
≤ (1 + 竽)E∣θt+ι - θ*(h(诙))k2 + (1 + 鲁)Ew(h(诙))-θ*(h(诙+ι))k2
2	小人
≤(I + η∣^)(I-小入)EIl 仇一 θ*(h(Wt))I∣2 + (1 + ^|^)(n2^2 + ɪ "2EkUt+1- h(W t)|2)
2	C
+ (ι + W)E∣∣θ*(h(Wt))- θ*(h(wt+1))∣∣2
η2λ
≤ (1 + η2λ)(1 - η2λ)E∣∣θt - θ*(h(Wt))∣∣2 + 2η2σ2 + 16η2C12E∣∣ut+ι - h(Wt)∣∣2
+ 4LθEkh(Wt) - h(Wt+ι)∣∣2
〃2人
≤ (1 - η2λ)E∣∣θt - θ*(h(Wt))∣∣2 +2η2σ2 + 16η2C^E∣∣ut+ι - h(Wt)∣∣2 + 4LθEkh(Wt) - h(Wt+ι)∣∣2,
2	人	〃2人
where the third inequality is because that θ*(∙) is Lq = Cg2-Lipschitz Lin et al. (2019).
□
Proof ofTheorem 2. Denote by gι(h(wt)) = Eχ.,y. [gi(h(ʌvt); Xi,yi)] and g2(h(Wt))=
Eχ-,y-[g2(h(wt);Xi,yi)]. Note that VhΦ(h(wvt),θt) = Vhgι(h(wt)) + θt^hg2(h(wt)). De-
note by ∆u,t = ∣∣ut+ι - h(wt)k2, ∆z,t = ∣∣zt+ι - Vwh(wt)τVhΦ(h(wt),θt)∣∣2 = I∣zt+1 -
Vh(Wt)τ[Vhgι(h(wt)) + θtVhg2(h(wt))]∣2, and δ = ∣% -θ*(h(wt))∣2.
Applying Lemma 1 to ut, we have
C 2
E[δu√+1] ≤ (I -e04％t + 2β0σ + -∣- IIW t+1 - W t∣∣2.
βθ
Hence we have
E "X∆u,J ≤ E "X ZL "+1 +2β0σ2(T + 1) + £ Ch/2”+1"2
_t=0	」	Lt=0	β0	t=0 β0
and
-T +1	-
E E ∆u,t
_t=1	.
≤ E [XF △”"- "+1 + 2β0σ2(τ+1)+E Chn 呵 2zt+112
_t=0	β0	t=0	β0
(13)
(14)
(15)
Define
et = (1 - β1)(Vwh(Wt)τVhΦ(h(Wt),θ*(h(Wt))) - Vwh(Wt-1)τVhΦ(h(Wt-1),θ*(Wt-1)))∙
We have
ketk2 ≤ 2(1 - β1)2 [(C21 + DY2 )Ch + Ch (l2i+ DLg2 )](kw t -诙-1k2 + W侬诙))-θ*(h(w t-ι))k2)
and
E∣zt+ι - Nah(wt)τVhΦ(h(wt),θ*(h(wt))) + et∣2
≤ E∣(1 - βι)[zt - Vwh(wt-ι)τVhΦ(h(wt-ι),θ*(h(wt-1)))]
+ βι[Vwh(wt; S1)VuΦ(ut+1,θt; S2) -Vwh(wt)τVhΦ(ut+ι,θt)]
+ βι[Vwh(wt)τVhΦ(ut+ι,θt) - Vwh(wt)τVhΦ(h(wt),θ*(h(wt)))]k2
≤ E∣(1 - βι)[zt - Vwh(wt-ι)τVhΦ(h(wt-ι),θ*(h(wt-1)))]
+ βι[Vwh(wt)τVhΦ(ut+ι,θt) - Vwh(wt)τVhΦ(h(wt),θ*(h(wt)))]k2
+ β2E∣Vwh(wt; S1)VuΦ(ut+1 ,θt; S2) - Vwh(Wt)τVhΦ(ut+ι,θt)k2
(16)
≤ (1 + 2-)(1 - β1)2E∣∣zt - Vwh(wt-ι)τVhΦ(h(wt-ι),θ*(h(wt-ι)))∣2
2
+ (1 + 工)2β2E∣∣Vwh(wt)τVhΦ(ut+ι,θt) - Vwh(wt)τVhΦ(h(wt),θt)k2
P-
+ (1 + ;2)2P2EkVwh(wt)τVhΦ(h(wt),θt) - Vwh(wt)τV⅛Φ(h(wt),θ*(h(wt)))∣∣2
P1
+ P2EkVwh(wt； SI)TVUΦ(ut+ι,θt; S2) - Vwh(Wt)τVhΦ(ut+ι,θt)∣∣2,
20
Published as a conference paper at ICLR 2022
where the last three terms can be bounded as below. First,
EilVwh(wt)τVhΦ(∏t+1,θt) -Vλwh(wt)τVhΦ(k(wt),θt)k2
≤2Ch(L2ι + L2 )Ekut+ι- k(w t)∣ι2∙
Second,
EkVWk(wt)τVhΦ(k(Wt),θt) -Vwk(wt)τVhΦ(k(Wt),θ*(k(wt)))k2
≤ ChGWt-θ*(k(w t))k2.
Third,
EkVWk(wt；SI)TVUΦ(ut+ι,θt;S2)-Vwk(wt)τVhΦ(ut+1,θt)k2
≤ 8σ2(Ch + D2Ch + σ2) +4Ch(σ2 + D2σ2) = 4σ2(3Ch + 3D2Ch + 2σ2).
(17)
(18)
(19)
It follows that
I∣zt+1 - Vλvh(wt)τVhΦ(h(^vt),θ*(h(Wt)))∣∣2
≤ (1 + β)E∣zt+ι - Vwh(Wt)TVhΦ(h(wt),θ*(h(Wt)))+ et∣2 + (1 + m)IletII2
2	βι
≤ (1 + β)2(1 - β1)2E∣∣zt - Vwh(Wt-ι)VhΦ(h(Wt-1),θt-1))∣∣2
+ 32βιCh(Lg1 + Lg2)E∣∣ut+ι - h(Wt)∣∣2 + 16%E∣∣θt - θ*(h(wt))∣∣2	(20)
+ 8βgσ2(3Cg + 3D2 Ch + 2σ2) + 宗 ∣∣et∣2
≤ (1 - βι)E∣zt - Vwh(Wt-1)VhΦ(h(Wt-1),θt-1))∣∣2 + β1C1E∣∣ut+1 - h(wt)∣∣2
+ 16βιE∣θt - θ*(h(Wt))∣∣2 + β1C2 + C3Ekwt- Wt-1∣∣2.
Pi
where C： =32Ch(Lg1+ Lg2), C2 =8σ2(3Ch + 3D2Ch + 2σ2) and C3 =8[(Cg1+ D2Cj2 )Ch +
Ch(LgI + D2Lg2)]. Thus,
「T
E X ∆z,t
_t = 0	.
≤e[苦+C1 Xi △“"+16 Xiδt+βιCg (T+1)+乎XkZt+ι k2∙
Using Lemma 3, we have
Xδt ≤ -2λE∣θo - θ*(h(W0))I2 + Ma +1) +∣∣% XE[∆u,t] + WC* XE∣zt+112,
t = 0	2	t = 0	η2	t=0
and
Xδt ≤ -2λE∣∣θ0 - θ*(h(W0))k2 + 和喂 +1) + ∣2% XE[∆u,t] + 8L麓2cu XE∣∣zt+1k2
t = 1	η2	t = 0	η2	t=0
Combining the upper bound of Et ∆z,t, ∑t ∆u,t, ∑t δ and Lemma 2, we have
gr	T	T	一 T
IIvf(Wt)II2 ≤ ηc XE(F(Wt)- F(Wt+1)) + C X △" - 2 XEIlZt+1∣∣2
≤ 2E(F(WO)	- F*)- 2	X E∣∣Zt+112	+ CuE [y	+ C1 £	∆u,t + 16 £ δt	+ β1C2	+ 中 X Izt+112
η1cl	2	t=0	Cl L β1	t=1	t=1	β1 t=0
≤ 2E(F(WC)- F*)+ CUe[∆β,0 + 芸δ0 + β1C2(τ + 1) + 64η"2f + 1) + C1 £ ∆u,t + 学第2 X∆u,t
+ Xe[(⅜(平)-1 )*1『
≤ 2E(F(W0)- F) + CuE [∆z^ + 出 + (C + 512盘4 1
一	η1Cl	Cl	β1	η2λ	λ2	β0 J
+ 出卫E [β1C2 + 牛 + 2(C1 + 51等氏局
Cl	λ	λ2
I X y[( CU 户3〃湾 Z T 512c!2)就〃渴)1 λ II	||2
+⅛E[匕(-Jr+ (CI + -^γ~ ) -yr)- ""zt+1 i J
21
Published as a conference paper at ICLR 2022
Due to the setting
η1 ≤ min{
~c~ βι
cl
β0λ
we have
c3uC3 2 ,c3u(C1λ2+512Cg22)2Ch
},
(21)
CU
Cl
C3η12c2u
β2
+ (C1 +
512cg2) Ch⅛
λ2
β2
(22)
Hence, we have
kVF (xt)k2
With βo = O(1∕√T), βι
≤ 2E(F(W0) - F*)
η1clT
+ cu E[β1C2+
64n2 σ2
λ
∆z,0
~W
32δo	512Cg2 A”,。
+ 声 + (C1 +	) 丁
+ 2(Cι + 51λcg2 )β0σ2
O(1∕√T), and η = O(1∕√T), we have
kVF(Xt)k2 ≤ O
Z)-1) ≤0.

+蚩E
□
Table 7: Testing performance on benchmark datasets and ImageNet-LT. The percentage number is
the second row denotes the imbalanced ratio (cf the text). All experiments on benchmark datasets are
averaged over three runs with different random seeds. The network structure used in all experiments
isResNet32. __________________________________________________________
Datasets	For Accuracy Maximization	
	Method	1%	10%
CIFAR10 (LT)	CE LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.713±0.001	0.876±0.002 0.744±0.003	0.872±0.002 0.780±0.003	0.879±0.000 0.758±0.016	0.842±0.004 0.787±0.001	0.883±0.001
CIFAR100 (LT)	Ce LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.396±0.002	0.572±0.000 0.407±0.004	0.559±0.003 0.427±0.006	0.579±0.001 0.403±0.003	0.536±0.001 0.430±0.005 0.585±0.002
STL10 (LT)	Ce LDAM [Cao et al. (2019)] TS-DRW [Cao et al. (2019)] TS-DEC [Kang et al. (2019)] CT (CB-LDAM)	0.441±0.017	0.639±0.009 0.440±0.010 0.641±0.008 0.458±0.006	0.651±0.017 0.457±0.013	0.629±0.009 0.488±0.012 0.662±0.005
ImageNet (LT)	-CE [Jamal et al. (2020)] CB-CE [Cui et al. (2019)] 	CT (CB-CE)	0.2526 0.2659 0.2661
D Compositional Training with Class Weighted loss
In this section, we extend the compositional training method to deep learning with class weighted
loss. Let LCW(w) denote a class weighted loss written as:
1n
LCW(W) = n Ipyi'(w; Xi, yi),	(23)
where pyi ∈ (0, 1) denotes a weight assigned to the i-th example that depends on the class the data
belongs to. There are different methods for determining the class-level weight. A simple method
is to set pi according to the reciprocal of its corresponding class size, i.e., pyi = 1∕nyi . Recently,
Cui et al. (2019) proposed an improved variant of class-weighted loss by using an effective number
of samples per-class instead of the class size to compute the individual weight, i.e., pyi
1-γ
l-γnyi ,
where γ ∈ (0, 1) is a hyper-parameter. We refer to the class weighted loss using these individual
weights as LCB(W) = 1/n Pn=I r--⅜'(w； Xi,y).
22
Published as a conference paper at ICLR 2022
With Class-Weighted Losses. The corresponding compositional objective is a standard two-level
compositional function, i.e.,
min F(W) = LCW(W — aVLAVG(w)).	(24)
w∈Rd
Assuming that the stochastic gradient of LCW can be easily computed, the optimization of the above
problem is easier than that for AUC loss.
We present a simplified stochastic adaptive algorithm with an Adam-style adaptive step size in Algo-
rithm 2 referred to as SCA, where ht (O0, . . . , Ot) denotes an appropriate mapping function, which
can be implemented by using different methods, including Adam, AMSGrad, Adabound, etc. Notice
that when ht(∙) = 1 Algorithm 2 becomes the NASA algorithm (Ghadimi et al., 2020) to stochastic
compositional optimization. We present an informal convergence of Algorithm 2 below.
Theorem 3. (Informal) Under appropriate conditions on the loss functions LAVG, LCW and
'(w;χ,y), with βo,βι,η
O(1∕√T), Algorithm 2 ensures that E [τ++χ PT=O ∣∣VF(Wt)Il2] ≤
O( √T).
Remark: The appropriate conditions include the Lipschitz continuous conditions on LAVG and
LCW and bounded variance conditions of V'(w; Xi,yi) and V2'(w; Xi,yi). We will present the
detailed conditions below when proving the above theorem. The above theorem indicates that we
can optimize the compositional objective (24) with the same convergence rate as optimizing the
averaged loss (1) for deep learning.
D. 1 Experiments with Multi-class Datasets
We conduct experiments on three benchmark multi-class image classification datasets, namely CI-
FAR10, CIFAR100, and STL10. We construct imbalanced versions of these datasets by keep their
classes but making the class sizes follow a long-tailed (LT) distribution with two imbalanced ratios
(the ratio of the size of minority class to the size of majority class) similar to (Cui et al., 2019).
We use ResNet32 as the network stucture. The weight decay is set to 2e-4 for all experiments. For
algorithms, we train a total of 200 epochs with a batch size 128 and we use step size 0.1 and de-
crease it by 10 times at at 80% and 90% of total training time. We tune the beta parameters of our
methods in a range [0.1, 0.99] with a grid search and find that good values are around 0.9. For the
class-weighted loss, we choose the class-weighted version of the LDAM loss (Cao et al., 2019). For
baselines, we compare with optimizing the CE loss (CE), optimizing the LDAM loss (LDAM), the
two-stage method with the deferred re-weighting that optimizes the class balanced LDAM loss in
the second stage (TS-DRW) (Cao et al., 2019), the two-stage method with the decoupling trick that
optimizes the class balanced LDAM loss in the second stage (TS-DEC) (Kang et al., 2019). The
results are shown in Table 7. We can see that the proposed CT method achieves the best accuracy
on all datasets. In addition, we conduct a large-scale experiment by following Jamal et al. (2020) on
ImageNet-LT with ResNet32. We use class-balanced loss (Cui et al., 2019) as outer loss function
of our compositional objective. We use an initial learning rate of 0.1 and run a total of 90 epochs
decaying learning rate every 35 epochs by a factor of 10. Eventually, we achieve the top1 accuracy
of 26.61%, which is better than two baselines, i.e., CE(25.26%) and CBCE(26.59%).
D.2 Analysis of Theorem 3
In the section we analyze Algorithm 2. An algorithm utilizing the adaptive step size and moving
average for nonconvex optimization has been studied in (Guo et al., 2021). But here we have to
tailor the algorithm and analysis to the considered formulation with composition. Denote st =
1/(z2,t+1 + G0 ), ηt = ηst . We make the following assumptions regarding the problem 24.
Assumption 3.
•	LCW(u) is CLCW -Lipschitz continuous, VLAVG(w) is CLAVG -Lipschitz continuous.
•	VLCW(u) is LLCW -Lipschitz continuous, V2LAVG(w) is LLAVG -Lipschitz continuous.
•	The stochastic oracle satisfies E∣αVLAVG (w) — αVLAVG(w; S)∣2 ≤ σ2, E∣αV2LAVG(w) —
αV2LAVG(w; S)∣2 ≤ σ2, EkVLCW(u) — VLCW(u; S)∣∣2 ≤ σ2.
We formally present Theorem 3 as follows
23
Published as a conference paper at ICLR 2022
Algorithm 2 Stochastic Compositional Adaptive (SCA) method for solving (24)
1:	Require Parameters: β0,β1,α, Go,η
2:	Initialization: w0 ∈ Rd , z0 , u0
3:	for t = 0, 1, ..., T do
4:	Sample three sets of examples denoted by S1, S2
5：	ut+ι = (1 — βo)ut + βo(wt — αVLAVG(wt; Sι))
6：	Ot = (I — αV2LAVG(wt; SI))VLCW(ut+i； S2)
7:	zt+1 = (1 — β1)zt + β1Ot
8:	z2,t+1 = ht(O0, . . . , Ot)	ht can be implemented by that in Appendix B,
9:	wt+i = wt — η √z2zt+1+G~	owith the simplest form ht = 1
10:	end for	,
Theorem 4. Assume F(x0) — F ≤ Δf where F = min F(x). Suppose Assumptions 1 and 3
x
hold. With η ≤ { 4L√clβ=u, 4(i+aCLC∣β0√C5cu, 2⅛ }, β0 = O(√T),β ≤ O(√T),andconstants
LF=2LLCW(1+αCLAVG)2+2CLCWαLLAVG,C5=(4L2LAVGCL2CW+2(1+αL2LAVG)LLCW),Algorithm
2 can ensure that
E
1T
TzlE kVF(wt)k2
T+ 1 t=0
≤O
Proof of Theorem 4. By Assumption 2, we know that F(w) is smooth with coefficient LF :=
2LLCW (1 + αCLAVG)2 + 2CLCW αLLAVG. We can prove that under ηLF ≤ cl/(2c2u),
F (wt+i) ≤ F (Wt) + VF (wt)>(wt+ι - Wt) + LF ∣∣wt+ι - wt∣∣2
=F(Wt) - VF(wt)>(ηt ◦ zt+i) + L2Fkηt ◦ zt+1k2
=F(Wt) + 2k√ηt◦(VF(Wt)-zt+I)II2 - 2k√ηtONF(Wt)II2 +(L2FIlnt ◦ zt+ιk2 - * 1 k√ηt◦ zt+ιk2)
≤ F(Wt) + 等∣VF(Wt) - zt+1k2 -等∣VF(Wt)∣2 +	- ncl ∣zt+ιk2
≤F (Wt)+ncu ∣∣vf (Wt)-Zt+1∣∣2 - ml ∣∣vf (Wt )k2- ncl ∣∣zt+ιk2.
(25)
Denote ∆z,t = kzt+i — VF(wt)k = kzt+i — (I — αV2 LAVG (wt))VLCW (wt — αVLAVG(wt))k2
and ∆u,t = kut+i — (wt — αVLAVG(wt))k2.
Applying Lemma 1 to ut , we have
E[∆u,t+ι] ≤ (1 - βo)∆u,t +2β2σ2 + (1 + £LAVG) ∣∣Wt+ι - Wt∣2.	(26)
Hence we have
E XT； ∆u,t ≤ E Xx △"" -；t+1 +2βoσ2(T + 1) + XX (1 + αCLAVG)2n2cu IE+1『.(27)
Defining
et =(1 - β1)(VF(Wt) - VF (Wt-1))
=(1 - β1)[(I - αV2LAVG(Wt))VLCW(Wt - VLAVG(Wt))	(28)
- (I - αV2LAVG(Wt-1))VLCW(Wt-1 - VLAVG (Wt-1))],
we get
ket k2 ≤ (1 — βi)2L2Fkwt — wt-i k2.	(29)
24
Published as a conference paper at ICLR 2022
It holds that
EllZt+ι - VF(Wt) + et∣∣2 ≤ E∣∣(1 - βι)(zt - VF(Wt-I)) + βι((I - αV2LAVG(wt; Sι))Vf (ut+ι; S2) - F(wt))∣∣2
=E∣∣(1 - βι)[zt - VF(wt-ι)]
+ βι[(I — aV2LAVG(wt; SI))VLCW(Ut+1; S2) — (I - (OVILAVG(WtyyVLCW(Ut+ι)
+ (I — aV2LAVG(wt))VLCW(ut+ι) - (I - OVLavg(w))VLCW(Wt - VLAVG(Wt))]∣2
=E[(1 - β1)2∣∣zt-VF(Wt-I)II2]
+ β2E∣∣ (I - αV2LAVG(wt; SI))VLCW(Ut+1； S2) — (I — αV2LAVG(Wt))VLcw(Ut+1)
+ (I — aV2LAVG(wt))VLcw(ut+ι) — (I — αV2LAVG(wt))VLCW(Wt- VLAVG(Wt))∣2
+ 2(1 - β1)β1E[(zt - VF(wt-ι))>((I - αV2LAVG(wt; SI))VLCW(Ut+1； S2) - (I - αV2LAVG(wt))VLcw(ut+ι))]
+ 2(1 - β1)β1E[(zt - VF(wt-ι))>((I - αV2LAVG(wt))VLCW(ut+1) - (I - αV2Lavg(wt))VLcw(wt — αVLAVG(wt)))]
≤ E[(1 - βι)2∣∣zt-VF(Wt-I)II2]
+ 2β2E∣(I — aV2LAvG(wt; SI))VLCW(Ut+1； S2) — (I — 0 + 2β2E∣(I — aV2LAvG(wt))VLCW(ut+1) — (I — αV2LAv + (1 - β1)2βEkZt-VF(wt-1 )k2 + 2β1E∣(I — aV2LAvG(wt))VLCW(ut+1) — (I — αV2LAv( ≤ (1 - β1)2(1 + V)EllZt- VF (Wt-I)∣∣2 + 4((1 + αLLAVG )	V2LAvG(wt))VLCW(ut+1)∣2 G(Wt))VLCW(Wt- αVLΑVG(wt))∣∣2 G(Wt))VLCW(Wt- aVLAVG(Wt)) k 2 ∣2 + σ2 + CL CW)β2σ2
+ 4β1(1 + oLlavg)2LLcwl∣Ut+1 - (wt - αLCW(wt))k2 ≤ (1 - β1)(1 - β)E∣zt - VF(Wt-I)k2 + β2C4σ2 + β1C5∆u,t. where C4 := 4((1 + αL∕)2 + σ2 + CLcw),。5 ：= 4(1 + aL/)2 It follows that ∣∣zt+1 - VF(Wt)II2 ≤ (1 + β)E∣∣zt+1 - VF(Wt) + et∣∣2 ≤ (1 + β)(1 -%)(1 - β)EIlZt- vf(Wt-I)∣∣2 + (1 + + (1 + β1 )C5β1∆u,t + ^4(1 - β1)2LF∣∣wt - Wt-Ik2 ≤ (1 - β1)E∣∣zt - VF(Wt-I)Il2 + 2C4β2σ2 + 2C5 % ∆%t + ^z-Lf I Wt — Wt-1 12. P1 Thus, E X ∆z,t ≤ E X ", -4"1 +2C4β1σ2(T +1) + 4LFη _t=0	∖	[t=0	β1	β1 Γ T Λ	A"	LLcw. 2 + (1 + f)IθtI2 P1 β)C4β2σ2 (30) T	- 2用zt+ιk2 + 2C5 X∆u,t t=0	_
≤ E E z't — z't+1 +2C4β1σ2(T +1) + y η _t=0	β1	β1 T +2C5E X u't % u,t+1 +2β0σ2(T +1)	72 CukZt+1k2 + X (I + aCLAVG)2η2cukzt+1k21] e (31)
25
Published as a conference paper at ICLR 2022
Combining this with (25), we obtain
T
号E
T
T
X kVF(wt)k2 ≤ F(W0)- F* - X 竽kzt+1k2 + X η2u∆z,t
t=0
T
≤ Δf + 等 X
t=0
∆z,t - ∆z,t+1	∆u,t - ∆u,t+1
一β一 +2C5 —β一
t=0
(32)
+η2u [2。4员+4c5β0]σ2(τ+1)
+
4LF η2cU + 2C5(1 + αCLAVG )2η2cU
ηc
彳
T
Xkzt+1k2.
t=0
Due to the setting
η≤
4LfPu, 4(1 + oClavg)√C高｝ ,
√cβι
√Clβo
(33)
we have
Thus,
4LF η2cU + 2。5(1 + aCLAVG )2η2cU
β2
ηc
ɪ
≤ 0.
(34)
1T
TTl EkVF (wt)k2
T+ 1 t=0
2∆F	cu
≤--ɪ + -
~ηTT	Cl
E[∆z,o]
β1T
+ 2C5
E[∆u,o]
β0T
+ CcL I2"1 +4C5β0]σ2.
(35)
虏
确
E
虏
With βo = O(√T), βι = O(√t) and η = O(√T),
E τ+1 X kVF(wt)k2 ≤ O(√=).	(36)
T+ 1 t=0	T
□
26