Published as a conference paper at ICLR 2022
COptiDICE: Offline Constrained Reinforce-
ment Learning via Stationary Distribution
Correction Estimation
Jongmin Lee1； Cosmin Paduraru2, Daniel J. Mankowitz2, Nicolas Heess2, Doina Precup2,
Kee-Eung Kim1 , Arthur Guez2
1KAIST, 2DeepMind
Ab stract
We consider the offline constrained reinforcement learning (RL) problem, in
which the agent aims to compute a policy that maximizes expected return while
satisfying given cost constraints, learning only from a pre-collected dataset. This
problem setting is appealing in many real-world scenarios, where direct interac-
tion with the environment is costly or risky, and where the resulting policy should
comply with safety constraints. However, itis challenging to compute a policy that
guarantees satisfying the cost constraints in the offline RL setting, since the off-
policy evaluation inherently has an estimation error. In this paper, we present an
offline constrained RL algorithm that optimizes the policy in the space of the sta-
tionary distribution. Our algorithm, COptiDICE, directly estimates the stationary
distribution corrections of the optimal policy with respect to returns, while con-
straining the cost upper bound, with the goal of yielding a cost-conservative policy
for actual constraint satisfaction. Experimental results show that COptiDICE at-
tains better policies in terms of constraint satisfaction and return-maximization,
outperforming baseline algorithms.
1 Introduction
Reinforcement learning (RL) has shown great promise in a wide range of domains, such as com-
plex games (Mnih et al., 2015; Silver et al., 2017) and robotic control (Lillicrap et al., 2016;
Haarnoja et al., 2018). However, the need to interact with the environment during learning hinders
its widespread application to many real-world problems for which executing exploratory behavior
in the environment is costly or dangerous. Offline RL (also known as batch RL) (Lange et al., 2012;
Levine et al., 2020) algorithms sidestep this problem and perform policy optimization solely from a
set of pre-collected data. The use of existing (offline) data can make offline reinforcement learning
applicable to real world systems and and has led to a sharp increase in interest in this paradigm.
Recent works on offline RL, however, mostly assume that the environment is modeled as a Markov
decision process (MDP), and standard offline RL algorithms focus on reward-maximization only
(Fujimoto et al., 2019; Wu et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Wang et al., 2020;
Kumar et al., 2020). In contrast, in real-world domains it is common that the behavior of the agent
is subject to additional constraints beyond the reward. Consider, for example, autonomous driving
or an industrial robot in a factory. Some behaviors may damage the agent itself or its surroundings,
and safety constraints should thus be considered as part of the objective of a suitable reinforce-
ment learning system. One of the ways to mathematically characterize a constrained RL problem
is through the formalism of constrained Markov Decision Processes (CMDP) (Altman, 1999). In
CMDPs taking an action incurs a cost as well as a reward, and the goal is to maximize the expected
long-term reward while satisfying a bound on the expected long-term cost. In this work, we aim to
solve the constrained decision making problem in the offline RL setting, to enable deployment in
various safety-critical domains where direct learning interactions are infeasible.
Offline constrained RL inherits the difficulties of offline unconstrained RL, while introducing ad-
ditional challenges. First, since the target policy being optimized deviates from the data-collection
*Work done during an internship at DeePMind.
1
Published as a conference paper at ICLR 2022
policy with no further data collection, distribution shift becomes the central difficulty. To mitigate
the distributional shift, existing offline RL methods frequently adopt the pessimism principle: either
by explicit policy (and critic) regularization that penalizes deviation from the data-collection policy
(Jaques et al., 2019; Wu et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Wang et al., 2020; Lee
et al., 2020; Kostrikov et al., 2021) orby reward penalty to the uncertain state-action regions (Kumar
et al., 2020; Kidambi et al., 2020; Yu et al., 2020). Second, in offline constrained RL, the computed
policy should satisfy the given cost constraints when it is deployed to the real environment. Unfor-
tunately off-policy policy evaluation inherently has estimation errors, and it is therefore difficult to
ensure that a policy estimated from a finite dataset will satisfy the constraint when executed in the en-
vironment. In addition, constrained policy optimization usually involves an additional optimization
for the Lagrange multiplier associated with the cost constraints. Actor-critic-based constrained RL
algorithms thus have to solve triple (i.e. critic, actor, Lagrange multiplier) intertwined optimization
problems (Borkar, 2005; Tessler et al., 2019), which can be very unstable in practice.
In this paper, we present an offline constrained RL algorithm that optimizes the state-action sta-
tionary distribution directly, rather than the Q-function or the policy. We show that such treatment
obviates the need for multiple estimators for the value and the policy, yielding a single optimiza-
tion objective that is practically solvable. Still, naively constraining the cost value may result in
severe constraint violation in the real environment, as we demonstrate empirically. We thus propose
a method to constrain the upper bound of the cost value, aiming to compute a policy more robust
in constraint violation, where the upper bound is computed in a way motivated by a recent advance
in off-policy confidence interval estimation (Dai et al., 2020). Our algorithm, Offline Constrained
Policy Optimization via stationary DIstribution Correction Estimation (COptiDICE), estimates the
stationary distribution corrections of the optimal policy that maximizes rewards while constraining
the cost upper bound, with the goal of yielding a cost-conservative policy for better actual constraint
satisfaction. COptiDICE computes the upper bound of the cost efficiently by solving an additional
minimization problem. Experimental results show that COptiDICE attains a better policy in terms
of constraint satisfaction and reward-maximization, outperforming several baseline algorithms.
2 Background
A Constrained Markov Decision Process (CMDP) (Altman, 1999) is an extension of an MDP, for-
mally defined by a tuple M =(S, A,T,R,C = {Ck}i..k, C = {^k}i..k,po, γi, where S is the set
of states, A is the set of actions, T : S × A → ∆(S) is a transition probability, R : S × A → R
is the reward function, Ck : S × A → R is the k-th cost function with its corresponding threshold
Ck ∈ R, po ∈ ∆(S) is the initial state distribution, and Y ∈ (0,1] is the discount factor. A policy
π : S → ∆(A) is a mapping from state to distribution over actions. We will express solving CMDPs
in terms of stationary distribution. For a given policy π, the stationary distribution dπ is defined by:
{∞
(1 -γ) P γtPr(st = s,at = a) ifγ < 1,
t=0T	(1)
lim t++t P Pr(St = s,a，t = a) if Y = 1,
T→∞ T+1 t=0
where so 〜po, at 〜 ∏(st), st+ι 〜 T(st, at) for all timesteps t ≥ 0. We define the value of the
policy as VR(π) := E(s,a)〜d∏ [R(s, a)] ∈ R and VC(∏) ：= E(s,a)〜d∏ [C(s, a)] ∈ RK. Constrained
RL aims to learn an optimal policy that maximizes the reward while bounding the costs up to the
thresholds by interactions with the environment:
max Vr(∏) s.t. VCk(∏) ≤ Ck ∀k =1,...,K	(2)
π
Lagrangian relaxation is typically employed to solve Eq. (2), leading to the unconstrained problem:
min max VR(π) — λ>(VC (π) — C)	(3)
λ≥o π
where λ ∈ RK is the Lagrange multiplier and C ∈ RK is the vector-valued cost threshold. The inner
maximization in Eq. (3) corresponds to computing an optimal policy that maximizes scalarized
rewards R(s, a) — λ>C(s, a), due to the linearity of the value function with respect to the reward
function. The outer minimization corresponds to balancing the cost penalty in the scalarized reward
2
Published as a conference paper at ICLR 2022
function: if the current policy is violating the k-th cost constraint, λk increases so that the cost is
penalized more in the scalarized reward, and vice versa.
In the offline RL setting, online interaction with the environment is not allowed, and the policy is
optimized using the fixed offline dataset D = {(s0 , s, a, r, c, s0)i }iN=1 collected with one or more
(unknown) data-collection policies. The empirical distribution of the dataset is denoted as dD, and
We will abuse the notation dD for S 〜dD, (s, a)〜dD, (s, a, s0)〜dD. We abuse (so, s, a, s0)〜
dD for so 〜po, (s, a, s0)〜dD. We denote the space of data samples (so, s, a, s0) as X.
A naive way to solve (3) in an offline manner is to adopt an actor-critic based offline RL algorithm for
maxπ VR-λ>C (π) while jointly optimizing λ. However, the intertwined training procedure of off-
policy actor-critic algorithms often suffers from instability due to the compounding error incurred
by bootstrapping out-of-distribution action values in an offline RL setting (Kumar et al., 2019). The
instability would be exacerbated when the nested optimization for λ is added.
3 Offline Constrained RL via Stationary Distribution
Correction Estimation
In this section, we present our offline constrained RL algorithm, Constrained Policy Optimization
via stationary DIstribution Correction Estimation (COptiDICE). The derivation of our algorithm
starts by augmenting the standard linear program for CMDP (Altman, 1999) with an additional
f -divergence regularization:
maxE(s,a)〜d[R(s,a)] - αDf(d∣∣dD)	(4)
s.t. E(s,a)〜d[Ck(s, a)] ≤ Ck	Vk =1,...,K	(5)
Pd(s0, a0) = (1 - γ)po(s0) + γPd(s, a)T(s0|s, a)	∀s0	(6)
d(s, a) ≥ 0	∀s, a, (7)
where Df(d∣∣dD ):= E(s,。)〜dD [f (筑：；))]is the f -divergence between the distribution d and the
dataset distribution dD, and α > 0 is the hyperparameter that controls the degree of pessimism, i.e.
how much we penalize the distribution shift, a commonly adopted principle for offline RL (Nachum
et al., 2019b; Kidambi et al., 2020; Yu et al., 2020; Lee et al., 2021). We assume that dD > 0
and f is a strictly convex and continuously differentiable function with f(1) = 0. Note that when
α = 0, the optimization (4-7) reduces to the standard linear program for CMDPs. The Bellman-flow
constraints (6-7) ensure that d is the stationary distribution of a some policy, where d(s, a) can be
interpreted as a normalized discounted occupancy measure of (s, a). Thus, we seek the stationary
distribution ofan optimal policy that maximizes the reward value (4) while bounding the cost values
(5). Once the optimal solution d* has been estimated, its corresponding optimal policy is obtained
by π*(a∣s) = Pd ,s,aɔ ,、.
y	a0 d*(S,aO)
Now, consider the Lagrangian for the constrained optimization problem (4-7):
K
min maxE(S a)〜d[R(s,a)] - αDf(d∣∣dD) - E λk(E(sja)^d[Ck(s,a)] - Ck)
λ≥o,ν d≥o	k=1
-P V (s0) [P d(s0, a0) - (1 - γ )po(s0) - γ P d(s, a)T (s0∣s, a)]	(8)
where λ ∈ R+K is the Lagrange multiplier for the cost constraints (5), and ν(s) ∈ R is the Lagrange
multiplier for the Bellman flow constraints (6). Solving (8) in its current form requires evaluation of
T(s0∣s,a) for (s, a)〜d, which is not accessible in the offline RL setting. To make the optimization
tractable, we rearrange the terms so that the direct dependence on d is eliminated, while introducing
new optimization variables w that represent stationary distribution corrections:
min max E (s,a)〜d [R(S,a) - λ>C(S,a)+ YV(SO) - V(s)] - a%。)〜dD [f (dD(⅛)]
λ≥0,ν d≥0 ，;T(s,a)	L ∖d (S，a)，」
+ (I - Y)ES0〜po [V(S0)] + λ>c
=min max E(Sa)〜dD [w(s, a)eλ,ν (s, a) - αf (w(s, a))] + (1 - Y)ESO 〜po [v (So)] + λ>c	⑼
λ≥o,ν w≥o	,
3
Published as a conference paper at ICLR 2022
where eλ,ν(s, a) ：= R(s, a) - λ>C(s, a) + YE§o〜T(s,a)[ν(s0)] - V(S) is the advantage function by
regarding V as a state value function, and w(s, a) := &£?)is the stationary distribution correction.
Every term in Eq. (9) can be estimated from samples in the offline dataset D :
min max E
ν,λ≥0 w≥0
(s0,s,a,s
)〜dD [w(s, a)eλ,ν(s, a, s0) - αf (w(s, a)) + (1 - γ)ν(so)] + λ>c
(10)
where eλ,ν (s,a,s0) ：= R(s,a) - λ>C(s,a)+ YV(s0) - V(s) is the advantage estimate using a single
sample. As a consequence, (10) can be optimized in a fully offline manner. Moreover, exploiting
the strict convexity of f, we can further derive a closed-form solution for the inner maximization in
(9) as follows. All the proofs can be found in Appendix B.
Proposition 1. For any V and λ, the closed-form solution of the inner maximization problem in (9)
is given by:
Wχ,ν(s,a) = (f 0)T(α1 巳λ,ν(s,a))十 where x+ = max(0,x)
(11)
Finally, by plugging the closed-form solution (11) into (9), we obtain the following convex mini-
mization problem:
min L(λ, V) = E(s,a)〜dD [wɪ,ν (s, a)eλ,ν(s, a) - αf(wλ,ν (s, a))] + (1 - Y)Eso〜po [ν(s0)] + λ>c
λ≥0,ν
(12)
To sum up, by operating in the space of stationary distributions, constrained (offline) RL can in
principle be solved by solving a single convex minimization (12) problem. This is in contrast to
existing constrained RL algorithms that manipulate both Q-function and policy, and thus require
solving triple optimization problems for the actor, the critic, and the cost Lagrange multiplier with
three different objective functions. Note also that when λ is fixed and treated as a constant, (12)
reduces to OptiDICE (Lee et al., 2021) for unconstrained RL with the scalarized rewards R(s, a) -
λ>C(s, a), without considering the cost constraints. In order to meet the constraints, λ should also
be optimized, and the procedure of (12) can be understood as joint optimization of:
V — arg min L(λ, v)	(OPtiDICEfor R — λ> C)	(13)
ν
λ J arg min λ> (C - E(s,Ο)〜dD [w'” (s, a)C(s, a)]) (Cost Lagrange multiplier)
λ≥0	×------------{z-----------}
≈VC (π)
*
Once the optimal solution of (12), (λ',ν"),is computed, w'*,ν* (s, a) = :D((：；)) is also derived by
(11), which is the stationary distribution correction between the stationary distribution of the optimal
policy for the CMDP and the dataset distribution.
3.1 Cost-conservative Constrained Policy Optimization
Our first method based on (12) relies on off-policy evaluation (OPE) using DICE to ensure cost
constraint satisfaction, i.e. E(s,Ο)〜dD [wλ,ν(s,a)C(s, a)] ≈ E(s,a)〜d∏ [C(s, a)] ≤ C. However, as
we will see later, constraining the cost value estimate naively can result in constraint violation when
deployed to the real environment. This is due to the fact that an off-policy value estimate based on
a finite dataset inevitably has estimation error. Reward estimation error may be tolerated as long as
the value estimates are useful as policy improvement signals: it may be sufficient to maintain the
relative order of action values, while the absolute values matter less. For the cost value constraint,
we instead rely on the estimated value directly.
To make a policy robust against cost constraint violation in an offline setting, we consider the con-
strained policy optimization scheme that exploits the upper bound of the cost value estimate:
max VR(π) s.t. UPPerBound(VCk(∏)) ≤ Ck ∀k	(14)
π
Then, the key question is how to estimate the upper bound of the policy value. One natural way
is to exploit bootstrap confidence interval (Efron & Tibshirani, 1993; Hanna et al., 2017). We can
4
Published as a conference paper at ICLR 2022
construct bootstrap datasets Di by resampling from D and run an OPE algorithm on each Di , which
yields population statistics for confidence interval estimation {VC(π)i}im=1. However, this procedure
is computationally very expensive since it requires solving m OPE tasks. Instead, we take a different
approach in a more computationally efficient way motivated by CoinDICE (Dai et al., 2020), a
recently proposed DICE-family algorithm for off-policy confidence interval estimation. Specifically,
given that our method estimates the stationary distribution corrections w(s, a) ≈ :>：；：])
target policy π, we consider the following optimization problem for each cost function Ck:
maχ E(s0,s,a,s0)〜p[w(s, a)Ck (s, a)]
P∈∆(X)
s.t. DKL(P(S0, s, a, s0)∣∣dD(s0, s, a, s0)) ≤ e
Pp(s0, a0)w(s0, a0) = (1 — Y)p0(s0) + Y Pp(s, a)w(s, a)p(s0∣s, a) ∀s0
a0	s,a
of the
(15)
(16)
(17)
wherep(so,s,a,s0) = po(s0)p(s, a)p(s0∣s, a) is the distribution over data samples (s0,s,a,s0) ∈ X
which lies in the simplex ∆(X), and e > 0 is the hyperparameter. In essence, we want to adversari-
ally optimize the distribution over data samples P so that it overestimates the cost value by (15). At
the same time, We enforce that the distribution P should not be perturbed too much from the empirical
data distribution dD by the KL constraint (16). Lastly, the perturbation of distribution should be done
in a way that maintains compatibility with the Bellman flow constraint. The constraint (17) is anal-
ogous to the Bellman flow constraint (6) by noting thatp(s, a)w(s, a) = p(s, a)加：：)≈ dπ(s, a).
In this optimization, when e = 0, the optimal solution is simply given by p* = dD, which yields
the vanilla OPE result via DICE, i.e.旧甲⑷〜,d [w(s, a)Ck(s, a)]. For e > 0, the cost value will be
overestimated more as e increases. Through a derivation similar to that for obtaining (12), we can
simplify the constrained optimization into a single unconstrained minimization problem as follows.
We denote (s0, s, a, s0) as x for notational brevity.
Proposition 2. The constrained optimization problem (15-17) can be reduced to solving the follow-
ing unconstrained minimization problem:
7min 'k(τ,χ; W) = τ log Ex 〜dD [exp (1 (w(s, a)(Ck(s, a) + γχ(s0) - χ(s)) + (1 - γ)χ(so))) ] + τe
,	(18)
where τ ∈ R+ corresponds to the Lagrange multiplier for the constraint (16), and χ(s) ∈ R
corresponds to the Lagrange multiplier for the constraint (17). In other words, mior ≥0,χ '(τ, χ)=
E(s,a)〜p* [w(s, a)Ck (s, a)] where p* is the optimal perturbed distribution ofthe problem (15-17).
Also, for the optimal solution (T*, χ*), p* is given by:
P* (x) Z dD(x)exp (. (w(s, a)(ck(s,a) + YX*(s0) — X*(s)) + (1 — γ)x*(s0))
X---------------------------------------------------------------------------
(19)
}
: ω* (x) (unnormalized weight for x = (s0, s, a, s0))
Note that every term in (18) can be estimated only using samples of the offline dataset D, thus it can
be optimized in a fully offline manner. This procedure can be understood as computing the weights
for each sample while adopting reweighting in the DICE-based OPE, i.e. UpperBound(VC (π)) =
Ex〜dD [ω*(x)w(s, a)C(s, a)] where ω*(x) = (normalized ω*(x) of (19)). The weights are given
non-uniformly so that the cost value is overestimated to the extent controlled by .
Remark. CoinDICE (Dai et al., 2020) solves the similar optimization problem to estimate an upper
cost value of the target policy π as follows:
max min T log E X 〜dD
w≥0τ≥0,ν	。0 〜∏(S0)
a 〜π(s0)
[exp (1 (w(s,a)(Ck(s,a) + γν(s0,a0) - ν(s0,a0)) + (1 - Y)ν(so,ao)), 十 τe
(20)
It is proven that (20) provides an asymptotic (1 - α)-upper-confidence-interval of the policy value if
e := ξα where ξa is the (1 一 α)-quantile ofthe χ2-distribution with 1 degree offreedom (Dai et al.,
2020). Compared to our optimization problem (18), CoinDICE’s (20) involves the additional outer
maximization, which is for estimating w(s,a) = [[：：), the stationary distribution corrections
of the target policy π. In contrast, we consider the case when w is given, thus solving the inner
minimization alone is enough.
5
Published as a conference paper at ICLR 2022
Finally, we are ready to present our final algorithm COptiDICE, an offline constrained RL algorithm
that maximizes rewards while bounding the upper cost value, with the goal of computing a policy
robust against cost violation. COptiDICE addresses (14) by solving the following joint optimization.
V — arg min L(λ,ν)	(OPtiDICEfor R 一 λ>C)	(21)
ν
τ,χ — arg min P,ι 'k (Tk ,Xk ； WSV)	(Upper cost value estimation)
τ ≥0,χ	,
λ — arg min λ> (^ 一 '(τ, χ; wɪ V) )	(Cost Lagrange multiplier)
λ≥0	、-----{Z」
≈UpperBοund(VC (π))
Compared to (13), the additional minimization for (τ, χ) is introduced to estimate the upper bound
of cost value.
3.2	Policy Extraction
Our algorithm estimates the stationary distribution corrections of the optimal policy, rather than
directly obtaining the policy itself. Since the stationary distribution corrections do not provide a
*
direct way to sample an action, We need to extract the optimal policy ∏ from w*(s, a) =，ð((：：)),
in order to select actions when deployed. For finite CMDPs, it is straightforward to obtain πS
by ∏*(a∣s)	=「d	T]s,a) 八	= Pd 皋：，\*;，.	However,	the same method cannot directly be
a0	dπ* (s,a0)	a0 dD(s,a)w*(s,a)	,
applied to continuous CMDPs due to the intractability of computing the normalization constant. For
continuous CMDPs, we instead extract the policy using importance-weighted behavioral cloning:
maxE(s,α)〜d∏* [log∏(a∣s)] = E(s,α)〜d。[w*(s, a)log∏(a∣s)]	(22)
which maximizes the log-likelihood of actions to be selected by the optimal policy πS .
3.3	Practical Algorithm with Function Approximation
For continuous or large CMDPs, we represent our optimization variables using neural networks.
The Lagrange multipliers ν and χ are networks parameterized by θ and φ respectively: νθ : S → R
is a feedforward neural network that takes a state as an input and outputs a scalar value, and χφ :
S → RK is defined similarly. λ ∈ R+K and τ ∈ R+K are represented by K-dimensional vectors. For
the policy πψ, we use a mixture density network (Bishop, 1994) where the parameters ofa Gaussian
mixture model are output by the neural network. The parameters of the νθ network are trained by
minimizing the loss:
min JV(θ) =Ex〜d。[W(s, a, s0)(R(s, a) — λ>C(s, a) + γνθ(s0) — vθ(S))	(23)
θ
—αf (W(s, a, Sy) + (1 — γ)νθ(so)] + λ>c
where W(s,a, s0) := (f0)-1( 1 (R(s,a) - λ>C(s,a)+ γνθ(s0) — vθ($)))十.While JV can be a
biased estimate of L(λ, V) in (12) in general due to (f 0)-1(E[∙]) = E[(f 0)-1(∙)], we can show that
JVλ is an upper bound of L(ν, λ) (i.e. we minimize the upper bound), and JVλ = L(ν, λ) holds if
transition dynamics are deterministic (e.g. Mujoco control tasks) (Lee et al., 2021). The parameters
of the χφ network and τ can be trained by:
K
min Σ τ log Ex* exp (1(W(s, a, s0)(Ck (s, a) + YXφ,k(s0) — Xφ,k (S))	(24)
τ ≥0,φ k=1	τ
+ (1 — γ)χφ,k(S0)	+ τk
This involves a logarithm outside of the expectation, which implies that mini-batch approximations
would introduce a bias. Still, we adopt the simple mini-batch approximation for computational
efficiency, with a moderately large batch size (e.g. 1024), which worked well in practice. The
empirical form of the loss we use is given by:
K
min Jτ,χ(τ,φ)=Ebatch(D)〜D ∑ T log Esbatch(D) [ exP (1 (w(s, a, s0) ∙
τ≥0,φ	k=1
(Ck(S, a) +γχφ,k(S0) — χφ,k(S)) + (1 — γ)χφ,k(S0)] + Tk
(25)
6
Published as a conference paper at ICLR 2022
Lastly, λ and the policy parameter ψ are optimized by:
min Jλ(λ) = λ>(C- Jτ,χ(τ, φ))
λ≥0
min Jn(ψ) = -Ex〜dD [W(s,a, s0)log∏ψ(a|s)]
ψ
(26)
(27)
The complete pseudo-code is described in Appendix C, where every parameter is optimized jointly.
4 Experiments
4.1	Tabular CMDPs (Randomly generated CMDPs)
We first probe how COptiDICE can improve the reward performance beyond the data-collection
policy while satisfying the given cost constraint via repeated experiments. We follow a proto-
col similar to the random MDP experiment in (Laroche et al., 2019; Lee et al., 2021) but with
cost as an additional consideration. We conduct repeated experiments for 10K runs, and for
each run, a CMDP M is generated randomly with the cost threshold C = 0.1. We test with
two types of data-collection policy πD, a constraint-satisfying policy (i.e. VC(πD) = 0.09)
and a constraint-violating policy (i.e. VC(πD) = 0.11). Then, a varying number of trajecto-
ries N ∈ {10, 20, 50, 100, 200, 500, 1000, 2000} are collected from the sampled CMDP using the
constructed data-collection policy πD, which constitutes the offline dataset D. Finally, the of-
fline dataset D is given to each offline constrained RL algorithm, and its reward and cost perfor-
mance is evaluated. For the reward, we evaluate the normalized performance of the policy π by
vVR(∏)-VR(£))∈ (-∞, 1] to see the performance improvement of ∏ over ∏d intuitively, where ∏
is the optimal policy of the underlying CMDP. More details can be found in Appendix E.
Offline constrained RL has been mostly unexplored, thus lacks published baseline algorithms. We
consider the following three baselines. First, BC denotes the simple behavior cloning algorithm to
see whether the proposed method is just remembering the dataset. Second, Baseline denotes
the algorithm that constructs a maximum-likelihood estimation (MLE) CMDP M using D and then
solves the MLE CMDP using a tabular CMDP solver (LP solver) (Altman, 1999). Third, C-SPIBB
is the variant of SPIBB (an offline RL method for tabular MDPs) (Laroche et al., 2019), where we
modified SPIBB to deal with the cost constraint by Lagrange relaxation (Appendix E).
Figure 1a presents the result when the data-collection policy is constraint-satisfying. The perfor-
mance ofBC approaches the performance of the πD as the size of the dataset increases, as expected.
When the size of the offline dataset is very small, Baseline severely violates the cost constraint when
its computed policy is deployed to the real environment (cost red curve in Figure 1a), and it even
fails to improve the reward performance over πD (reward red curve in Figure 1a). This result is ex-
pected since Baseline overfits to the MLE CMDP, exploiting the highly uncertain state-actions. This
can cause a significant gap between the stationary distribution of the optimized policy computed in
M and the one computed in M, leading to failure in both reward performance improvement and cost
constraint satisfaction. To prevent such distributional shift, offline RL algorithms commonly adopt
the pessimism principle, encouraging staying close to the data support. We can observe that such
pessimism principle is also effective in offline constrained RL: both C-SPIBB (orange) and the naive
version of COptiDICE (green) show consistent policy improvement over the data-collection policy
while showing better constraint satisfaction. Still, the pessimism principle alone is not sufficient
to ensure constraint satisfaction, raising the need for additional treatment for cost-conservativeness.
Finally, our COptiDICE (blue) shows much stricter cost satisfaction than other baseline algorithms
while outperforming Baseline and C-SPIBB in terms of reward performance.
Figure 1b presents the result when the data-collection policy is constraint-violating, where all the
baseline algorithms exhibit severe cost violation. This result shows that if the agent is encouraged
to stay close to the constraint-violating policy, it may negatively affect the constraint satisfaction,
although the pessimism principle was beneficial in terms of reward maximization. In this situation,
only COPtiDICE (blue) could meet the given constraint in general, which demonstrates the effective-
ness of our proposed method of constraining the upper bound of cost value. Although COptiDICE
(blue) shows reward performance degradation when the size of dataset is very small, this is natural in
that it sacrifices the reward performance to lower the cost value to meet the constraint conservatively,
and it still outperforms the baseline in this low-data regime.
7
Published as a conference paper at ICLR 2022
Cost threshold c ....... Data-collection policy -------Optimal policy	—⅛- BC
-*- Baseline	+ C-SPIBB	Y - COptiDICE (by (12))	-⅛- COptiDICE
(a) when the d ata-coll ection policy is cost-satisfying
(b) when the d ata-coll ection policy is cost-violating
Figure 1: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs for the
varying number of trajectories and two types of data-collection policies. Plots of (a) correspond to
the case when the data-collection policy is constraint-satisfying, while the data-collection policy is
constraint-violating in the last two plots of (b). The mean of normalized reward performance and
the mean of cost value are reported for 10K runs, where the error bar denotes the standard error.
4.2	Continuous control tasks (RWRL Suite)
We also evaluate COptiDICE on domains from the Real-World RL (RWRL) suite (Dulac-Arnold
et al., 2020), where the safety constraints are employed. The cost of 1 is given if the task-specific
safety constraint is violated at each time step, and the goal is to compute a policy that maximizes re-
wards while bounding the average cost UP to c. Per-domain safety constraints and the cost constraint
thresholds are given in Appendix E. Due to lack of an algorithm that addresses offline constrained
RL in continuous action sPace, we comPare COPtiDICE with simPle baseline algorithms, i.e. BC:
A simPle behavior-cloning agent, CRR (Wang et al., 2020): a state-of-the-art (unconstrained) offline
RL algorithm, and C-CRR: the constrained variant of CRR with Lagrangian relaxation where a cost
critic and a Lagrange multiPlier for the cost constraint are introduced (APPendix E).
Since there is no standard dataset for offline constrained RL, we collected data using online con-
strained RL agents (C-DMPO; the constrained variant of Distributional MPO) (Abdolmaleki et al.,
2018; MankoWitz et al., 2021). We trained the online C-DMPO with various cost thresholds C and
saved checkPoints at regular intervals, which constitutes the Pool of Policy checkPoints. Then, we
generated datasets, where each of them consists of β × 100% constraint-satisfying trajectories and
the rest constraint-violating trajectories. The trajectories were samPled by Policies in the Pool of
Policy checkPoints. Since there is a trade-off between reward and cost in general, the dataset is a
mixture of low-reward-low-cost trajectories and high-reward-high-cost trajectories.
Figure 2 Presents our results in RWRL tasks, where the dataset contains mostly constraint-satisfying
trajectories (β = 0.8), with some cost-violating trajectories. This tyPe of dataset is rePresentative
of many Practical scenarios: the data-collecting agent behaves safely in most cases, but sometimes
exhibit exPloratory behaviors which can be leveraged for Potential Performance imProvement. Due
to the characteristics of these datasets, BC (orange) generally yields constraint-satisfying Policy, but
its reward Performance is also very low. CRR (red) significantly imProves reward Performance over
BC, but it does not ensure that the constraints are satisfied. C-CRR (brown) incurs relatively lower
cost value than BC in Walker, QuadruPed, and Humanoid, but its reward Performance is clearly
worse than BC. The naive COPtiDICE algorithm (green) takes the cost constraint into account but
nevertheless frequently violates the constraint due to limitations of OPE. Finally, COPtiDICE (blue)
comPutes a Policy that is more robust to cost violation than other algorithms, while significantly
outPerforming BC in terms of reward Performance. We observe that the hyPerParameter in (16)
controls the degree of cost-conservativeness as exPected: larger values of lead to overestimates of
the cost value, yielding a more conservative Policy.
To study the dePendence on the characteristics of the dataset we exPeriment with different values
of β. Figure 3a show the result for β = 0.8 (low-reward-low-cost data), Figure 3b for β = 0.5,
and Figure 3c for β = 0.2 (high-reward-high-cost data). These results show the exPected trend:
more high-reward-high-cost data leads to a joint increase in rewards and costs of all agents. A
simPle modification of the unconstrained CRR to the constrained one (brown) was not effective
enough to satisfy the constraint. Our vanilla offline constrained RL algorithm (green), encouraged
to stay close to the data, also suffers from severe constraint violation when most of the trajectories
are given as the constraint-violating ones, which is similar to the result of Figure 1c-1d. Finally,
8
Published as a conference paper at ICLR 2022
—- Cost threshold c ------------ BC	CRR	----- C-CRR
----COptiDICE (by (12))	---- COptiDICE (e = 0.01)	----- COptiDICE (e = 0.05)	---- COptiDICE (e = 0.1)
iterations 1e6	iterations 1e6	iterations 1e6	iterations 1e6
(c) Quadruped	(d) Humanoid
Figure 2: Result of RWRL control tasks. For each task, we report the reward return and the average
cost. The results are averaged over 5 runs, and the shaded area represents the standard error.
— — Cost threshold c --------------- BC	CRR
---C-CRR
∩ q. AVeragecOSt . 1∩∩∩ Reward
U" J	IUUU
----COptiDICE (by (12))	---- COptiDICE (e = 0.01)
iterations 1e5	iterations 1e5
(a) Walker (/3= 0.8)
---- COptiDICE (e = 0.05)	---- COptiDICE (e = 0.1)
0.3
0.2
0.1
0.0
Average ∞st
O 5	10
iterations 1β5
(b)	Walker (/3=0.5)
(c)	Walker (/3= 0.2)
Figure 3: Result on RWRL walker using three dataset configurations with different levels of con-
straint satisfaction β: for (a) the data is obtained with β = 0.8 (low-reward-low-cost data), (b)
with β = 0.5, and (c) with β = 0.2 (high-reward-high-cost data).
COptiDICE (blue) demonstrates more robust behavior to avoid constraint violations across dataset
configurations, highlighting the effectiveness of our method constraining the cost upper bound.
5 Discussion and Conclusion
The notion of safety in RL has been captured in various forms such as risk-sensitivity (Chow et al.,
2015; Urpl et al., 2021; Yang et al., 2021), Robust MDP (Iyengar, 2005; Tamar et al., 2014), and
Constrained MDP (Altman, 1999), among which we focus on CMDP as it provides a natural for-
malism to encode safety specifications (Ray et al., 2019). Most of the existing constrained RL
algorithms (Achiam et al., 2017; Tessler et al., 2019; Satija et al., 2020) are on-policy algorithms,
which cannot be applied to the offline setting directly. A recent exception is the work by Le et al.
(2019) that also aims to solve constrained RL in an offline setting, though its method is limited to
discrete action spaces and relies on solving an MDP completely as an inner optimization, which is
inefficient. It also relies on the vanilla OPE estimate of the policy cost, which could result in severe
constraint violation when deployed. Lastly, in a work done concurrently to ours, Xu et al. (2021)
also exploits overestimated cost value to deal with the cost constraint, but their approach relies on
an actor-critic algorithm, while ours relies on stationary distribution optimization.
In this work, we have presented a DICE-based offline constrained RL algorithm, COptiDICE. DICE-
family algorithms have been proposed for off-policy evaluation (Nachum et al., 2019a; Zhang et al.,
2020a;b; Yang et al., 2020b; Dai et al., 2020), imitation learning (Kostrikov et al., 2019), offline
policy selection (Yang et al., 2020a), and RL (Nachum et al., 2019b; Lee et al., 2021), but none of
them is for constrained RL. Our first contribution was a derivation that constrained offline RL can be
tackled by solving a single minimization problem. We demonstrated that such approach, in its sim-
plest form, suffers from constraint violation in practice. To mitigate the issue, COptiDICE instead
constrains the cost upper bound, which is estimated in a way that exploits the distribution correc-
tion w obtained by solving the RL problem. Such reuse of w eliminates the nested optimization in
CoinDICE (Dai et al., 2020), and COptiDICE can be optimized efficiently as a result. Experimental
results demonstrated that our algorithm achieved better trade-off between reward maximization and
constraint satisfaction than several baselines, across domains and conditions.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors would like to thank Rui Zhu for technical support and Sandy Huang for paper feed-
back. Kee-Eung Kim was supported by the National Research Foundation (NRF) of Korea (NRF-
2019R1A2C1087634, NRF-2021M3I1A1097938) and the Ministry of Science and Information
communication Technology (MSIT) of Korea (IITP No.2019-0-00075, IITP No.2020-0-00940, IITP
No.2021-0-02068).
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learn-
ing Representations (ICLR), 2018.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research ,pp. 22-31. PMLR, 06-11 Aug 2017.
Eitan Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.
Christopher M. Bishop. Mixture density networks. Technical report, 1994.
V.S. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems & Control
Letters, 54(3):207-213, 2005.
Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-
making: a cvar optimization approach. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28, 2015.
Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans.
CoinDICE: Off-policy confidence interval estimation. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), volume 33, pp. 9398-9411, 2020.
Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,
and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learn-
ing. 2020.
Bradley Efron and Robert J. Tibshirani. An Introduction to the Bootstrap. Number 57 in Monographs
on Statistics and Applied Probability. Chapman & Hall/CRC, Boca Raton, Florida, USA, 1993.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In Proceedings of the 36th International Conference on Machine Learning (ICML),
2019.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-
gus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems
(NeurIPS), volume 30, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings ofthe 35th
International Conference on Machine Learning (ICML), 2018.
Josiah P. Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals
for off-policy evaluation. In Proceedings of the 16th Conference on Autonomous Agents and
MultiAgent Systems, AAMAS, pp. 538-546. ACM, 2017.
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara
Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex
Novikov, Sergio Gomez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew
Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed
reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. URL https://arxiv.org/
abs/2006.00979.
10
Published as a conference paper at ICLR 2022
Garud N. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 2005.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog, 2019.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL : Model-
based offline reinforcement learning. In Advances in Neural Information Processing Systems
(NeurIPS), 2020.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu-
tion matching. In Proceedings of the 7th International Conference on Learning Representations
(ICLR), 2019.
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning
with fisher divergence critic regularization. In Marina Meila and Tong Zhang (eds.), Proceed-
ings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of
Machine Learning Research,pp. 5774-5783. PMLR, 18-24 JUl 2021.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-
learning via bootstrapping error redUction. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Aviral KUmar, AUrick ZhoU, George TUcker, and Sergey Levine. Conservative Q-learning for offline
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Reinforcement learning: State-of-the-art.
Springer Berlin Heidelberg, 2012.
Romain Laroche, PaUl Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning
(ICML), 2019.
Hoang Le, Cameron Voloshin, and Yisong YUe. Batch policy learning Under constraints. In Pro-
ceedings of the 36th International Conference on Machine Learning, volUme 97 of Proceedings
of Machine Learning Research, pp. 3703-3712. PMLR, 09-15 JUn 2019.
ByUngjUn Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-EUng Kim. Batch reinforcement
learning with hyperparameter gradients. In Hal DaUme In and Aarti Singh (eds.), Proceedings of
the 37th International Conference on Machine Learning, volUme 119 of Proceedings of Machine
Learning Research, pp. 5725-5735. PMLR, 13-18 JUl 2020.
Jongmin Lee, Wonseok Jeon, ByUngjUn Lee, Joelle PineaU, and Kee-EUng Kim. OptiDICE: Offline
policy optimization via stationary distribUtion correction estimation. In Proceedings of the 38th
International Conference on Machine Learning, volUme 139 of Proceedings of Machine Learning
Research, pp. 6120-6130. PMLR, 18-24 JUl 2021.
Sergey Levine, Aviral KUmar, George TUcker, and JUstin FU. Offline reinforcement learning: TUto-
rial, review, and perspectives on open problems, 2020.
Timothy P. Lillicrap, Jonathan J. HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa,
David Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. In 4th
International Conference on Learning Representations, ICLR, 2016.
Daniel J. Mankowitz, Dan A. Calian, Rae Jeong, Cosmin PadUrarU, Nicolas Heess, SUmanth
Dathathri, Martin Riedmiller, and Timothy Mann. RobUst constrained reinforcement learning
for continUoUs control with model misspecification, 2021.
Volodymyr Mnih, Koray KavUkcUoglU, David Silver, Andrei A. RUsU, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis AntonogloU, Helen King, Dharshan KUmaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. HUman-level control throUgh deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
11
Published as a conference paper at ICLR 2022
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation
of discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems (NeurIPS), 2019a.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. AlgaeDICE:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking Safe Exploration in Deep Reinforce-
ment Learning. 2019.
Harsh Satija, Philip Amortila, and Joelle Pineau. Constrained Markov decision processes via back-
ward value functions. In Proceedings of the 37th International Conference on Machine Learning
(ICML), volume 119 of Proceedings of Machine Learning Research, pp. 8502-8511, 13-18 JUl
2020.
Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavioral modelling priors for offline reinforcement learning, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan
Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of Go without human knowledge. Nature, 550:354-359, 2017.
Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approximation.
In Proceedings of the 31st International Conference on International Conference on Machine
Learning (ICML), 2014.
Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
International Conference on Learning Representations (ICLR), 2019.
NUria Armengol Urpi, Sebastian Curi, and Andreas Krause. Risk-averse offline reinforcement learn-
ing. In International Conference on Learning Representations (ICLR), 2021.
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas.
Critic regularized regression. In Advances in Neural Information Processing Systems (NeurIPS),
volume 33, pp. 7768-7778, 2020.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning,
2019.
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline
reinforcement learning, 2021.
Mengjiao Yang, Bo Dai, Ofir Nachum, George Tucker, and Dale Schuurmans. Offline policy selec-
tion under uncertainty, 2020a.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized lagrangian. In Advances in Neural Information Processing Systems (NeurIPS),
2020b.
Qisong Yang, Thiago D. Simao, Simon H Tindemans, and MatthijS T. J. Spaan. Wcsac: WorSt-
case soft actor critic for safety-constrained reinforcement learning. Proceedings of the AAAI
Conference on Artificial Intelligence, 35(12):10639-10646, May 2021.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. MOPO: Model-based offline policy optimization. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2020.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized offline estimation
of stationary values. In Proceedings of the 8th International Conference on Learning Represen-
tations (ICLR), 2020a.
12
Published as a conference paper at ICLR 2022
Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized offline
estimation of stationary values. In Proceedings of the 35th International Conference on Machine
Learning (ICML), 2020b.
13
Published as a conference paper at ICLR 2022
A Algorithm for Undiscounted CMDP
For γ = 1, the optimization problem (4-7) should be modified by adding an additional normalization
constraint Ps,a d(s, a) = 1.
maxE(s,a)〜d[R(s,a)] - αDf(d∣∣dD)
s.t. E(s,a)〜d[Ck(S,a)] ≤ Ck
P d(s0, a0) = (1 - Y)po(s0) + Y P d(s, a)T(s0∣s, a)
a0	s,a
d(s, a) ≥ 0
d(s, a) = 1
s,a
(28)
∀k =	1, .	. . , K	(29)
∀s0	(30)
∀s, a	(31)
(32)
Then, we consider the Lagrangian:
K
、min maxE(s,a)〜d[R(s,a)] - αDf (d||dD) - E λk侬⑶。)〜d[C®(s, a)] - Ck)
λ≥0,ν,μ d≥0	k = 1
-P ν(s0) [P d(S, a0) - (1 - Y)p0(s0) - Y P d(s, a)T(s0∣s, a)] - μ[ P d(s, a) - l]
(33)
where μ ∈ R is the Lagrange multiplier for the normalization constraint (32). Then, We rearrange the
terms so that the direct dependence on d is eliminated, while introducing new optimization variables
w(s, a) = »；：；)that represent stationary distribution corrections:
、min maxE (s,a)〜d [R(s,a) - λ>C(S,a)+ YV(SO) - V(S) - μ] - αE(s,a)〜dD f (d¾⅛)^∣
λ≥0,ν,μ d≥0 J;T"a) L	L Vd (s,a)J∖
+ (1 - Y)EsO 〜po [v (so)] + λ> 金 + μ
、min max /so)〜dD [w(s,a)(eλ,ν(s,a) - μ) - αf (w(s,a))]
λ≥0,ν,μ w≥0 v , !
+ (1 - Y)EsO 〜po [v (so)] + λ> C + μ
(34)
which yields the minmax optimization problem that can be optimized in a fully offline manner. Due
to the strict convexity of f, we can also derive the closed form solution of the inner maximization
problem:
wλ,ν,μ (s,a) = (fl - 1 (1 (eλ,ν (s,a)- 〃))十	(35)
which simplify the the minmax problem (34) into the following single minimization problem:
、min L(λ,ν,μ) = E(s,a)〜dD [wS,ν,μ(s, a)(eλ,ν(s, a) - μ) - αf(wS,ν,μ(s,a))]
λ≥0,ν,μ
+ (1 - Y)Eso〜po[V(SO)] + λ>c + μ
(36)
Once the optimal solution (λ*, ν*,μ*) are obtained, we can also compute the stationary distribution
corrections of the optimal policy using (35). Still, naively using (36) would similarly result in
cost violation when the resulting policy is deployed to the real environment. We therefore adopt
constraining the upper bound of cost value:
ν, μ — arg min L(λ, ν, μ)	(RL for R — λ>C using (36))	(37)
ν,μ
τ,χ — arg min PK=I 'k(Tk,χk; WS νw) (Upper cost value estimation using (18))
τ ≥0,χ	, ,
λ — arg min λ> (C -'(τ, χ; wλ νμ))	(Cost Lagrange multiplier)
λ≥0	×------{Z ' , /
≈UpperBound(VC (π))
which completes the brief description of COptiDICE for Y = 1.
14
Published as a conference paper at ICLR 2022
B Proofs
Proposition 1. For any ν and λ, the closed-form solution of
max L(w,λ, ν) := E(s,a)〜dD IwGa)eλ,ν (S, a) - αf (WGa))] + (I - Y)ES0〜P0 [V(SO)] + λ>c
(38)
is given by:
Wχ,ν(s,a) = (f0)-1 (1 eλ,ν(s,a)) + where x+ = max(0, x)
(39)
Proof. For a fixed λ and ν, we write the dual of the maximization problem maxw≥0 L(w, λ, ν):
max min L(w, λ, ν) +	κ(S, a)w(S, a)
s,a
⇔ max min L(w, λ, V) + ɪ2 κ(s, a)dD(s, a)w(s, a)	(： dD > 0).
w κ≥0
s,a
By the strong duality, it is sufficient to consider KKT condition for (w*, κ*).
Condition 1 (PrimalfeasibiUty) w*(s, a) ≥ 0 ∀s, a
Condition 2 (Dualfeasibility) κ*(s, a) ≥ 0 ∀s, a
Condition 3 (Stationarity) dD(s, a)(eλ,ν(s, a) 一 αf0(w*(s, a)) + κ*(s, a)) = 0 ∀s, a
⇔ f(w*(s, a)) = 1 (eλ,ν (s, a) + κ*(s, a))
⇔ w*(S,a)=(尸)-1 (α(eλ,ν(s,a) + κ*(S, a)))	(4O)
Condition 4 (Complementary slackness) w* (S, a)κ* (S, a) = 0 ∀S, a
Then, we will show that:
wλ,ν(S, a) = fl-1 (α1 eλ,ν(s, a))+	(41)
satisfies the KKT conditions for all (S, a). First, Primal feasibility is always satisfied by definition
of w*,ν(s, a). Then, We consider either 1 eλ,ν(s, a) > f 0(0) or 1 eλ,ν(s, a) ≤ f0(0) for each (s, a).
(Case 1: 1 eλ,ν (s, a) > f 0(0)): In this case, k*(s, a) = 0, where Dual feasibility and Complemen-
tary slackness is satisfied. Stationarity also holds by:
wλ,ν (S,a) = (f0)T (1 eλ,ν(S,a)) +
=(f 0)-1 (1 eλ,ν(s, a))	(by assumption)
=(f')-1 (1 (eλ,ν(s, a) + κ*(s, a)))	⇔ (40)
Therefore, KKT conditions (Conditions 1-4) are satisfied.
(Case 2: 1 eλ,ν(s, a) ≤ f 0(0)): In this case, k*(s, a) = αf0(0) — eχ,ν(s, a), where Dualfeasibility
holds by assumption. Also, wλ*,ν(S, a) = 0 by assumption, which implies Complementary slackness
also holds. Finally, Stationarity also holds by:
wλ,ν (S,a) = (f')T (1 eλ,ν(s,a)) +
= 0	(by assumption)
= (f0)-1(f0(0))
=(f0)-1 (1 (eλ,ν(s, a) + k*(s, a)))	⇔ (40)
As a consequence, KKT conditions (Conditions 1-4) are always satisfied with wλ*,ν(S, a) =
(f0)-1 (1 eλ,ν(s, a)) + , which concludes the proof.	□
15
Published as a conference paper at ICLR 2022
Proposition 2. The constrained optimization problem (15-17) can be reduced to solving the follow-
ing unconstrained minimization problem:
^min 'k(τ,χ; W) = T log Ex 〜“d [exp (1 (w(s, a)(Ck(s, a)+ γχ(s0) - χ(s)) + (1 - γ)χ(so))) ] + τe
,	(18)
where τ ∈ R+ corresponds to the Lagrange multiplier for the constraint (16), and χ(s) ∈ R
corresponds to the Lagrange multiplier for the constraint (17). In other words, minr ≥o,χ '(τ, χ)=
E(s,a)〜p* [w(s, a)Ck (s, a)] where p* is the optimal perturbed distribution of the problem (15-17).
Also, for the optimal solution (τ*, χ*), p* is given by:
p*(x) H dD(x)exp (T* (w(s, a)(Ck(s,a) + YX*(s0) — χ*(s)) + (1 - γ)χ*(s°)))	(19)
、--------------------------------V-------------------------------}
=: ω* (x) (unnormalized weight for x = (s0, s, a, s0))
Proof. For the given constrained optimization problem:
max E(s0,s,a,s0)〜p[w(s, a)Ck (s, a)]	(42)
P∈∆(X)'
S.t. DKL(P(so, S, a, s0)∣∣dD(so, s, a, s0)) ≤ E	(43)
Pp(s0, a0)w(s0,a0) = (1 - Y)po(s0) + Y Pp(s, a)w(s, a)p(s0∣s, a) ∀s0	(44)
a0	s,a
We consider the Lagrangian:
min max Pp(so, s, a, s0)[w(s, a)Ck(s, a)] — T (Pp(so, s, a, s0) [log /；；：；：；：))]
T≥0,χ,ζ p≥0 X	x	,,,
-P x(s0)[PP(S0, a0)w(s0,a0) - (I - Y)po(SO) - γ PP(S,a)w(s,a)p(SlS,a)i
s0	a0	s,a
-Z[PP(So,s, a, s0) - 1i
-E
(45)
where τ ∈ R+ is the Lagrange multiplier for KL constraint (43), χ(S0) ∈ R is the Lagrange mul-
tiplier for (44), and ζ ∈ R is the Lagrange multiplier for the normalization constraint that ensures
Px P(X) = 1. Then, We rearrange (45) by:
min,maXEP(X)Iw(s, a) (Ck(s, a) + γχ(s0) — X(S)) + (1 — γ)χ(so) — Tlog dp(x)]
τ ≥o,χ,ζ P≥o
x
+ Te- Z[PP(X)- 1] =: g(τ,χ,Z,P)	(46)
Then, We can compute the non-parametric closed form solution for each sample X = (so, s, a, s0) for
the inner-maximization problem. Thanks to convexity of KL-divergence, it is sufficient to consider
dg∂pχxZ,P) = 0 for each x. Then,
dg∂⅜ζ,P) = w(s,a)(Ck(s,a) + γχ(s0) — χ(s)) + (1 — γ)χ(so) — Tlog d(x) + T — Z = 0
⇒ P(X) H dD(x)exp (1 (w(s, a)(Ck(s, a) + γχ(s0) - χ(s)) +(1- γ)χ(so)))	(47)
with some normalization constant that ensures Px P(X) = 1, which is described with respect to Z.
Then, by plugging (47) into (46), We obtain the result. Also, (19) is the direct result of (47).
□
16
Published as a conference paper at ICLR 2022
C Pseudocode of COptiDICE
Algorithm 1 COptiDICE
Input: An offline dataset D = {(s0 , s, a, r,c,s0)i}iN=1, a learning rate η .
1:	Initialize parameter vectors θ, φ, λ, τ, ψ .
2:	for each gradient step do
3:	Sample mini-batches from D.
4:	Compute gradients and perform SGD update:
5：	θ J θ - ηVθ JV(θ)	(Eq.(23))	φ	J	φ - NφJτ,χ(τ	φ)	(Eq. (25))
6：	T J [τ - ηVτJτ,χ(τ,φ)↑+	(Eq. (25))	λ	J	[λ -")入(刈十	(Eq. (26))
7:	ψ	J	ψ - ηVψJπ (ψ)	(Eq. (27))
8： end for
D Comparison with CoinDICE
CoinDICE (Dai et al., 2020) is a DICE-family algorithm for off-policy confidence interval estima-
tion. For a given policy π, CoinDICE essentially solves the following constrained optimization
problem to estimate the upper confidence interval of the cost value：
max maxE(so⑼。^/)〜p[w(s,a)Ck(s,a)]	(48)
P∈∆(X) w≥0
s.t. DKL(P(S0,s,a,s0)∣∣dD(s0,s,a,s0)) ≤ e	(49)
p(s0, a0)w(s0, a0) = (1 — γ)po(s0)π(a0∣s0) + Y Pp(s, a)w(s, a)p(s0∣s, a)π(a0∣s0) ∀s0,a0	(50)
s,a
The constraint (50) is analogous to the π-dependent Bellman flow constraint：
dπ(s0, a0) = (1 — Y)po(s0)∏(a0∣s0) + Y P dπ(s, a)T(s0∣s, a)π(a0∣s0) ∀s0, a0
s,a
(51)
It is well known that the transposed Bellman equation (51) always has a unique solution dπ, i.e. the
stationary distribution of the given policy ∏. Note that for a fixed p, the constrained optimization
problem (48-50) is over-constrained for w(s, a) by (50), and therefore the optimal solution will sim-
Ply be given by w*(s, a) = Cp(Ssa)) to satisfy the transposed Bellman equation (51) on the empirical
MDP defined by p. Then, We want to adversarially optimize the distribution P so that it overestimates
the cost value by (48). At the same time, we enforce that the distribution P should not be perturbed
too much from the empirical dsta distribution dD by the KL constraint (49). Finally, following the
similar derivation in Proposition 2, we can reduce the constrained optimization problem (48-50) to
solving the following unconstrained max-min optimization.
max min T log E	&d	exp ( 1 (w(s, a)(Ck(s, a) + γν(s0,a0) — ν(s,a)) + (1 — Y)ν(so,ao))	+ Te
w≥0τ≥0,ν	a。〜∏(so)L '	/-i
a0 〜π(s0)
where maxw≥o min”(∙) is to estimate w(s, a) = Cp(SsO)).
In contrast, we consider the case when w is given and aim to solve the following constrained opti-
mization problem.
max E(s0,s,α,s0)〜p[w(s, a)Ck (s, a)]	(15)
P∈∆(X)	.........
s.t. DKL(P(so, s, a, s0)∣∣dD(so, s, a, s0)) ≤ e	(16)
Pp(s0, a0)w(s0, a0) = (1 — Y)po(s0) + Y Pp(s, a)w(s, a)p(s0∣s, a) Vs	(17)
a0	S,a
This can be reduced to unconstrained minimization problem, without requiring nested optimization
to estimate w：
7min TlogEx〜dD 卜XP (1(w(s, a)(Ck(s, a) + γχ(s0) 一 χ(s)) + (1 — Y)χ(so)))] + Te
17
Published as a conference paper at ICLR 2022
E Experimental Settings
E.1 Random CMDPs
For random CMDP experiments, We follow a similar experimental protocol as (Laroche et al., 2019;
Lee et al., 2021) with additional consideration of cost constraint.
Random CMDP generation For each run, we constructed a random CMDP with |S | = 50, |A| =
4, γ = 0.95, and a fixed initial state s0. The transition probability is constructed randomly with con-
nectivity of4, i.e. for each (s, a), we sample 4 states uniformly, and then, the transition probabilities
to those states are determined by Dirichlet(1, 1, 1, 1). The reward of 1 is given to a single state
that minimizes the optimal policy’s reward value at s0, and 0 is given anywhere else. This reward
design can be roughly understood as choosing a goal state that is most difficult to reach from s0 .
The cost function is generated randomly, i.e. C(s, a)〜Beta(0.2,0.2) for ∀s ∈ S, a ∈ {a2, a3, a4}
and C(s, a1) = 0 to ensure existence of a feasible policy of the CMDP. Lastly, the cost threshold
c = 0.1 is used.
Data-collection policy construction The pseudo-code for the data-collection policy construction
is presented in Algorithm 2, where M is the underlying true CMDP, and CD ∈ {0.09,0.11} is the
hyperparameter that determines the cost value of ∏d. Starting from ∏d = ∏*, the policy is softened
Algorithm 2 Data-collection policy construction
Input: CMDP M =(S, A, T, R, C, c,po, γi, target cost value of the data-collection policy CD
Compute the optimal policy π* and its reward value function QR* (s, a) on the given CMDP M.
Initialize ∏soft J π* and ∏d J π*
Initialize a temperature parameter T J 10-6
# Compute 0.9-optimal behavior policy in terms of reward performance.
whileVRπD(s0) > 0.9VRπ*(s0) +0.1VRπunif(s0) do
Set ∏soft to ∏soft(a∣s) H exp (1 QR(S, a) ∀s,α
∏d J argmin∏ Df (dπ∣∣dπsoft) s.t. E(s,a)〜d∏ [C(s,a)] ≤ CD
τ J τ /0.9
end while
Output: The data-collection policy πD
via ∏soft(a∣s) h exp(QR(s, a)∕τ) while projecting it into the cost-satisfying one by solving ∏d J
argmin∏ Df (dπ∣∣dπsoft) s.t. E(s,a)〜d∏ [C(s, a)] ≤ CD. This process is repeated until the reward
performance of πD reaches 0.9-optimality while increasing the temperature τ, i.e. VRπD (s0) =
0.9VRπ* (s0) + 0.1VRπunif (s0).
After the data-collection policy πD is constructed, we sample trajectories using πD, which con-
stitutes the offline dataset D. We conducted experiments for a varying number of trajectories, i.e.
(the number of trajectories) ∈ {10, 20, 50, 100, 200, 500, 1000, 2000}. Each episode is terminated
either when 50 time steps have reached or the agent reached the goal state that yields a non-zero
reward.
Hyperparameters For tabular COPtiDICE, We used a =焉 and E = 0N, where N denotes the
number of trajectories in D. We also used f (x) = 1 (X - 1)2, which corresponds to χ2-divergence.
C-SPIBB: Constrained variant of SPIBB C-SPIBB solves the offline RL problem with respect
to R 一 λC Via SPIBB while updating λ in the direction of (VC(∏spibb) 一 ^), where VC(∏spibb) is
evaluated using the MLE CMDP. For C-SPIBB, we used N∧ = 5 as the hyperparameter of SPIBB.
18
Published as a conference paper at ICLR 2022
E.2 RWRL CONTROL TASKS
Network architecture and hyperparameters We used the ACME framework (Hoffman et al.,
2020). For the νθ network and the χφ network, we used LayerNormMLP with hidden sizes of
[512, 512, 256]. For the policy network πψ, we used the network architecture used in CRR (Wang
et al., 2020). Specifically, we the πψ network consists of4 ResidualMLP blocks with hidden size
of 1024 and a mixture of Gaussians policy head with 5 mixture components. We used the batch size
of 1024. We use Adam optimizer with learning rate 3e-4. Similar to ValueDICE (Kostrikov et al.,
2021), we additionally adopt gradient penalties (Gulrajani et al., 2017) for the νθ network and the
χφ network with coefficient 10e-5. We performed grid search for α ∈ {0.01, 0.05, 0.1} for each
domain. We used the following f as in OptiDICE (Lee et al., 2021):
x log x - x + 1 if 0 < x < 1
f (x)= I1 (X- 1)2 if X ≥ 1
Task specification We conduct experiments on domains using the RWRL suite with safety-spec.
The safety coefficient is a flag in the RWRL suite with safety-spec, and its value can be between 0.0
and 1.0. Lowering the value of the flag incurs more safety-constraint violation (cost of 1). Originally,
each domain has multiple types of safety constraints, but we use only one of them, which was the
hardest safety constraint to be satisfied by an online constrained RL agent. In summary, we used the
following task specifications (safety coefficients, name of the used safety constraint, cost threshold
^).
•	Cartpole (realworld-swingup): Safety-Coeff=0.3, Slider_pos, C = 0.1,
•	Walker (realworld-walk): Safety-Coeff=0.3, joint_velocity_constraint C = 0.1.
•	Quadruped (realworld-walk): Safety-Coeff=0.5, joint_angle_constraint C = 0.3.
•	Humanoid (realworld-walk): Safety-Coeff=0.5, joint_angle_constraint, C = 0.3.
The offline dataSet ConSiStS trajeCtorieS of 1000 epiSodeS for Cartpole and Walker, and 5000 epiSodeS
for Quadruped and Humanoid.
C-CRR: Constrained variant of CRR C-CRR additionally introduCeS the CoSt CritiC QC and the
Lagrange multiplier λ for the CoSt ConStraint. Then, the reward CritiC and the CoSt CritiC are trained
by minimizing TD-error for reward and CoSt reSpeCtively. The aCtor iS trained by weighted behavior-
cloning: max∏ E(s,Ο)〜dD [exp(A(s, a)∕β) logπ(a∣s)] where A(s,a) = (QR(S,a) — λQc(s,a))一
ml Pjm=I(QR(s, aj) - λQc(s,aj)), with aj 〜 π(∙∣s). This corresponds to optimizing the policy
with reSpeCt to the SCalarized reward R - λC . The Lagrange multiplier λ iS updated in the direCtion
of (E(s,a)〜dD [Qc(s,a)] - ^).
19
Published as a conference paper at ICLR 2022
F Discussion on the Dataset Coverage Assumption
Although we have made an assumption dD > 0 in Section 3, this is not strictly required for the
offline RL algorithm to work in practice. We adopted this assumption same as OptiDICE (Lee
et al., 2021) for the simplicity of describing the algorithm derivation from Eq (4-7) to Eq (10): the
assumption makes Eq. (10) correspond to solving the underlying true CMDP of Eq. (4-7). If we
take this assumption off, Eq. (10) then becomes equivalent to solving the reduced CMDP where the
state and action spaces are limited to the support of dD .
To see this, we consider the following constrained optimization problem, where the optimization
variables d(s, a) are defined only for the (s, a) Who are within the support of dD. We denotep^o and
T as the empirical initial state distribution and the empirical transition function respectively.
max P	d(s, a)R(s, a) - αDf (d||dD)	(52)
d (s,a)∈Supp(dD)
s.t. P	d(s,a)[Ck(s,a)] ≤ Ck	∀k = 1,...,K
(s,a)∈Supp(dD)
P	d(s0,a0) = (1 - γ)p^o(s0) + Y P	d(s,a)T(Sls,a)	∀s0 ∈ Supp(dD)
a0 ∈Supp(dD)	(s,a)∈Supp(dD)
d(s, a) ≥ 0	∀s, a ∈ Supp(dD)
Then, by considering the Lagrangian for (52) and following the similar derivation of Eq. (8-9),
we can finally arrive at Eq. (10) without any assumption on the coverage of dD, due to the fact
that P(s,a)∈Supp(dD)[d(s,a)(∙)] = P(s,a)∈Supp(dD) d" (s，a)壮£^) S = E(s,a)〜dD [壮£^) 3]
always holds. In other words, our offline RL algorithms that rely on Eq. (10) essentially solve the
reduced CMDP that is limited to the support of dD.
20
Published as a conference paper at ICLR 2022
G Ablation Experiments on different cost thresholds
In order to see the sensitivity of the proposed method to different cost thresholds, we conduct abla-
tion experiments on different cost thresholds using randomly generated CMDPs. The experimental
setup is identical to the one described in Section 4.1.
Cost threshold c
—⅛- Baseline
.....Data-Collection policy
C-SPIBB
----Optimal policy
-Φ- COptiDICE (by (12))
+ BC
-4- COptiDICE
(a) when Vb(TrD) = 0.09
Figure 4: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs. Plots for
the first two columns in (a) correspond to the case when the cost value of the data-collection policy
is 0.09 (corresponding to Figure 1a). Plots for the last two columns in (b) correspond to the case
when the cost value of the data-collection policy is 0.11 (corresponding to Figure 1b). The mean
of normalized reward performance and the mean of cost value are reported for 10K runs, where the
error bar denotes the standard error.
(b) when	= 0.11
On the same data-collection policy (whose cost value is either 0.09 or 0.11) and the same of-
fline dataset as in Figure 1, we tested each algorithm against different target cost thresholds
c ∈ {0.08,0.09,0.10,0.11,0.12}. Each row in Figure 4 presents the result for each target cost
threshold, ranging from C = 0.08 to c = 0.12. Our COPtiDICE (blue) shows consistent robustness
to constraint violation on varying cost thresholds, while other algorithms fail to meet the constraints
especially when the threshold is set to low values (e.g. rows 1 and 2).
21
Published as a conference paper at ICLR 2022
H Additional Experiments using Mixture Dataset
In Section 4.1, we demonstrated the results when the offline dataset was collected by a single data-
collection policy. However, in real-world situations, it would be common that data-collecting agents
act safely in most cases but have some unsafe attempts. To simulate this scenario, we conduct addi-
tional experiments on the use of a mixture dataset, where the dataset is collected by both constraint-
satisfying and constraint-violating policy.
Cost threshold c -----Optimal policy T- BC	TI¢- Baseline
Hr- C-SPIBB	-φ- COptiDICE (by (12))	-⅛- COptiDICE
(a) 80% of trajectories are by ∞st-satisfying policy,
20% of trajectories are by ∞st-violating policy
(b) 20% of trajectories are by cost-satisfying policy,
80% of trajectories are by cost-violating policy
Figure 5: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs, using
mixture dataset. Plots (a) correspond to the case when the dataset is collected generally by the
cost-satisfying policy, i.e. 80% of trajectories are by the cost-satisfying policy and 20% are by the
cost-violating policy. Plots (b) correspond to the case when the dataset is collected generally by
the cost-violating policy, i.e. 20% of trajectories are by the cost-satisfying policy and 80% are by the
cost-violating policy. The mean (unnormalized) reward value and the mean cost value are reported
for 10K runs, where the error bar denotes the standard error.
Figure 5 presents the result, where the dataset is collected by two policies, the cost-satisfying one
(i.e. its cost value is 0.09) and the cost-violating one (i.e. its cost value is 0.11). The overall trend
remains the same as in Figure 1 (Figure 5a ≈ Figure 1a, Figure 5b ≈ Figure 1b): when the dataset
consists of mostly cost-satisfying trajectories, baseline algorithms exhibits less constraint violation,
while they show more constraint violation when the dataset consists of mostly cost-violating ones.
Our COptiDICE (blue) still shows much stricter cost satisfaction than other baseline algorithms,
demonstrating a better trade-off between reward maximization and constraint satisfaction than base-
lines.
Furthermore, solving the reduced CMDP is a valid method in the offline RL setting since we may
want to optimize the policy only within the dataset support in order to prevent unexpected perfor-
mance degradation by out-of-distribution actions.
22
Published as a conference paper at ICLR 2022
I Additional Experiments on Data-collection Policy with
Limited Exploration Power
For the random CMDP experiment, we conducted additional experiments to see the results when
the data-collection policy πD does not cover the entire state-action space well. Specifically, we limit
the support of ∏d(∙∣s): ∏d(a|s) Will have non-zero probabilities only for a ∈ A where A ⊂ A
is the subset of A = {a1, a2, a3, a4}. By doing so, we ensure that πD covers only a part of the
entire state-action space. The detailed construction procedure of πD is described in Algorithm 3.
We conduct experiments for A = {a1}, A = {a1, a2}, and A = {a1, a2, a3}, and the results are
presented in Figure 6.
Algorithm 3 Constructing a data-collection policy with limited action support
Input: CMDP M =(S, A, T, R, C, c,po, γ), the subset of the entire actions, A ⊂ A, to be consid-
ered by the data-collection policy, CD: the target threshold of the data-collection policy.
1:	for each s ∈ S and a ∈ A do
2:
,τ.,	、
C(s, a) J
∏unif (a|s) J
C(s, a)
∞
∫ι∕∣A∣
0
if a ∈ A
if a ∈ A
# Eliminate action a ∈ A by assigning a large cost.
if a ∈ A
if a ∈ A
3:	end for
4:	ρ J 1
5:	while true do
6：	# Artificially lower the cost threshold (CD ∙ P) until ∏d meets the target constraint.
7:	π* J SolveCMDP(S, A, T, R, C, CD ∙ p,po, Y)
8:	∏d J 0.9 ∙ π* + 0.1 ∙ ∏unif # To make a stochastic policy within A.
9:	if VC (∏d ) > ^d then
10:	ρ J ρ * 0.99
11:	else
12:	break
13:	end if
14:	end while
Output: πD : the data-collection policy with limited action support.
23
Published as a conference paper at ICLR 2022
Cost threshold c ....... Data-collection policy -------Optimal policy	—⅛- BC
-⅜- Baseline	-⅛- C-SPIBB	-φ- COptiDICE (by (12))	-⅜- COptiDICE
Reward
1.0∣---------------------------
0.5
o.ok 西 ¾⅝ gj a eι 离T
101	102	103
number of trajectories in D
number of trajectories in D
Cost
0.05
0.00⅛-∙ 屯屯ts 屯西4
101	102	103
number of trajectories in D
Reward
1.0∣-----------------------
0.5
。心 a • ⅝⅝ e⅝ -ad
ιo1 ιo2 ιo3
number of trajectories in D
number of trajectories in D
Cost
0.05
o.ooF∙ a a∣ 星白屯 4
ιo1 ιo2 ιo3
number of trajectories in D
number of trajectories in D
number of trajectories in D
number of trajectories in D
number of trajectories in D
(a)	when the data-collection policy is cost-satisfying
(b)	when the data-collection policy is cost-violating
Figure 6: Result of tabular COptiDICE and baseline algorithms in random tabular CMDPs when
the data-collection policy has limited exploration power. Plots for the first two columns in (a)
correspond to the case when the data-collection policy is cost-satisfying. Plots for the last two
columns in (b) correspond to the case when the data-collection policy is cost-violating (except for
the first row). The first row denotes the case When A = {aj, the second row denotes the case
when A = {a1, a2}, and the third row denotes the case when A = {a1, a2, a3}. The mean of
normalized reward performance and the mean of cost value are reported for 10K runs, where the
error bar denotes the standard error.
The first row of Figure 6 shows the result when A = {aι}. Since aι is the zero-cost action as
described in Appendix E, the cost value of πD is always 0. Besides, since the dataset contains
only a single action a1 for each state due to the deterministic data-collection policy, no offline RL
algorithms can improve the performance beyond the data-collection policy: there must be more than
one action in some states in the dataset, in order for offline RL algorithms to have room to improve
the performance in general.
El	F	1,1	, 1 ∙ 1	C∙ TΓ-<∙	X- 1	. 1	1 , 1	7	C	1	l7
The second row and the third row of Figure 6 shows the result when A = {a1, a2} and A =
{a1, a2, a3} respectively. As the size of A increases from 2 to 3, offline RL algorithms further im-
prove the performance over the data-COlleCtiOn policy. This is natural since the larger A implies that
there is more room for offline RL algorithms to optimize. Finally, even when the exploration power
of the data-collection policy is limited, our COptiDICE (blue) shows a consistent advantage over
baseline algorithms, in terms of the better trade-off between reward maximization and constraint
satisfaction.
24