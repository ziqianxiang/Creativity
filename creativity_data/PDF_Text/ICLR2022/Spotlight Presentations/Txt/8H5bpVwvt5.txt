Published as a conference paper at ICLR 2022
AdaRL: What, Where, and How to Adapt in
Transfer Reinforcement Learning
Biwei Huang	Fan Feng
Carnegie Mellon University	City University of Hong Kong
biweih@andrew.cmu.edu	ffeng1017@gmail.com
Chaochao Lu
University of Cambridge & Max Planck Institute for Intelligent Systems
cl641@cam.ac.uk
Sara Magliacane	Kun Zhang
University of Amsterdam & MIT-IBM Watson AI Lab Carnegie Mellon University &
sara.magliacane@gmail.com	Mohamed bin Zayed University of Artificial Intelligence
kunz1@cmu.edu
Ab stract
One practical challenge in reinforcement learning (RL) is how to make quick
adaptations when faced with new environments. In this paper, we propose a
principled framework for adaptive RL, called AdaRL, that adapts reliably and
efficiently to changes across domains with a few samples from the target domain,
even in partially observable environments. Specifically, we leverage a parsimonious
graphical representation that characterizes structural relationships over variables
in the RL system. Such graphical representations provide a compact way to
encode what and where the changes across domains are, and furthermore inform
us with a minimal set of changes that one has to consider for the purpose of policy
adaptation. We show that by explicitly leveraging this compact representation to
encode changes, we can efficiently adapt the policy to the target domain, in which
only a few samples are needed and further policy optimization is avoided. We
illustrate the efficacy of AdaRL through a series of experiments that vary factors in
the observation, transition and reward functions for Cartpole and Atari games 1.
1	Introduction and Related Work
Over the last decades, reinforcement learning (RL) (Sutton and Barto, 1998) has been successful in
many tasks (Mnih et al., 2013; Silver et al., 2016). Most of these early successes focus on a fixed task
in a fixed environment. However, in real applications we often have changing environments, and it
has been demonstrated that the optimal policy learned in a specific domain may not be generalized to
other domains (Taylor and Stone, 2009). In contrast, humans are usually good at transferring acquired
knowledge to new environments and tasks both efficiently and effectively (Pearl and Mackenzie,
2018), thanks to the ability to understand the environments. Generally speaking, to achieve reliable,
low-cost, and interpretable transfer, it is essential to understand the underlying process—which
decision-making factors have changes, where the changes are, and how they change, instead of
transferring blindly (e.g., transferring the distribution of high-dimensional images directly).
There are roughly two research lines in transfer RL (Taylor and Stone, 2009; Zhu et al., 2020): (1)
finding policies that are robust to environment variations, and (2) adapting policies from the source
domain to the target domain as efficiently as possible. For the first line, the focus is on learning
policies that are robust to environment variations, e.g., by maximizing a risk-sensitive objective over
a distribution of environments (Tamar et al., 2015) or by extracting a set of invariant states (Zhang
et al., 2020a; 2021a; Tomar et al., 2021). A more recent method encodes task-relevant invariances
by putting behaviorally equivalent states together, which helps better generalization (Agarwal et al.,
2021). On the other hand, with the increase of the number of domains, the common part may get
1Code link: https://github.com/Adaptive-RL/AdaRL-code
1
Published as a conference paper at ICLR 2022
Source domains
Domain 1
Policy learning on
source domains
λ∙*(^λ)
Optimal parametrised policy
Identify compact
domain-
generalisable
representations
domain-specific
parameters
Am加
Utarget
Optimal target policy
Figure 1: The overall AdaRL framework. We learn a Dynamic Bayesian Network (DBN) over the
observations, latent states, reward, actions and domain-specific change factors that is shared across
the domains. We then characterize a minimal set of representations that suffice for policy transfer, so
that we can quickly adapt the optimal source policy with only a few samples from the target domain.
even smaller, running counter to the intention of collecting more information with more domains.
Moreover, focusing only on the invariant part and disregarding domain-specific information may
not be optimal; for instance, in the context of domain adaptation, it has been demonstrated that the
variable part also contains information helpful to improve prediction accuracy (Zhang et al., 2020b).
In this paper, we propose a method along the second line, adapting source policies to the target.
Approaches along this line adapt knowledge from source domains and reuse it in the target domain to
improve data efficiency, i.e., in order for the agent to require fewer explorations to learn the target
policy. For example, an agent could use importance reweighting on samples hs, a, r, s0i from sources
(Tirinzoni et al., 2018; 2019) or start from the optimal source policy to initialize a learner in the
target domain, as a near-optimal initializer (Taylor et al., 2007; Femgndez et al., 2010). Another
widely-used technique is finetuning: a model is pretrained on a source domain and the output layers
are finetuned via backpropagation in the target domain (Hinton and Salakhutdinov, 2006; Mesnil
et al., 2012). PNNs (Rusu et al., 2016), instead, retain a pool of pretrained models and learn lateral
connections from them to extract useful features for a new task. Moreover, a set of approaches focus
on sim2real transfer by adapting the parameters (Yu et al., 2017; Peng et al., 2020). However, many
of these approaches still require a large amount of explorations and optimization in the target domain.
Recently, meta-RL approches such as MAML (Finn et al., 2017), PEARL (Rakelly et al., 2019),
CAVIA (Zintgraf et al., 2019), Meta-Q learning (Fakoor et al., 2020), and others (Mendonca et al.,
2019; Clavera et al., 2019; Duan et al., 2017) have been successfully applied to learn an inductive
bias that accelerates the learning of a new task by training on a large number of tasks. Some of these
methods (e.g., CAVIA and PEARL), as well as some prior work (e.g., HiMDPs (Doshi-Velez and
Konidaris, 2016)) and recent follow-ups (Zhang et al., 2021b), have a similar motivation to our work:
in a new environment not all parameters need to be updated, so we can force the model to only adapt
a set of context parameters. However, these methods mostly focus on MDPs (except the Block MDP
assumption in Zhang et al. (2021b)) and model all changes as a black-box, which may be less efficient
for adaptation, as opposed to a factorized representation of change factors.
Considering these limitations, we propose AdaRL, a transfer RL approach that achieves low-cost,
reliable, and interpretable transfer for partially observable Markov decision processes (POMDPs),
with MDPs as a special case. In contrast to state-of-the-art approaches, we learn a parsimonious
graphical representation that is able to characterize structural relationships among different dimensions
of states, change factors, the perception, the reward variable, and the action variable. It allows us
to model changes in transition, observation, and reward functions in a component-wise way. This
representation is related to Factored MDPs (Kearns and Koller, 1999; Boutilier et al., 2000; Strehl
et al., 2007) and Factored POMDPs (Katt et al., 2019), but augmented with change factors that
represent a low-dimensional embedding of the changes across domains. Our main motivation is that
distribution shifts are usually localized - they are often due to the changes of only a few variables in
the generative processes, so we can just adapt the distribution of a small portion of variables (Huang
2
Published as a conference paper at ICLR 2022
et al., 2020; SchOlkoPf et al., 2021) and, furthermore, factorized according to the graph structure,
each distribution module can be adapted separately (Scholkopf, 2019; Zhang et al., 2020b).
In Fig. 1 we give a motivating example and a general description of AdaRL. In this example, we
consider learning policies for Pong (Bellemare et al., 2013) that can easily generalize to different
rotations ω and to images corrupted with white noise. Specifically, given data from n source domains
with different rotations and noise variances, we learn a parsimonious latent state representation shared
by all domains, denoted by st , and characterize the changes across domains by a two-dimensional
factor θk. We identify a set of minimal sufficient representations (stmin, θkmin) for policy transfer.
For instance, here only the rotation factor ω needs adapting (i.e., θkmin = ωk), since the noise factor
does not affect the optimal policy. Similarly, as we will show formally in the rest of the paper, not all
components si,t of the state vector st are necessary for policy transfer. For example, s2,t 6∈ stmin ,
since it never affects the future reward. We learn an optimal policy ∏*(∙∣θmin) on source domains. In
the target domain, we only need a few samples to quickly estimate the value of the low-dimensional
θmgnt, and then we can apply ∏* (∙∣θmrnget) directly. Our main contributions are summarized below:
•	We assume a generative environment model, which explicitly takes into account the structural
relationships among variables in the RL system. Such graphical representations provide a compact
way to encode what and where the changes across domains are.
•	Based on this model, we characterize a minimal set of representations that suffice for policy
learning across domains, including the domain-specific change factors and domain-shared state
representations. With this characterization, we adapt the policy with only a few target samples and
without policy optimization in the target domain, achieving low-cost and reliable policy transfer.
•	By leveraging a compact way to encode the changes, we also benefit from multi-task learning in
model estimation. In particular, we propose the Multi-model Structured Sequential Variational
Auto-Encoder (MiSS-VAE) for reliable model estimation in general cases.
2	A Compact Representation of Environmental Shifts
Suppose there are n source domains and n0 target domains. In each source domain, we observe
sequences {hot, at, rti}tT=1, where ot ∈ O are the perceived signals at time t (e.g., images), at ∈ A
is the executed action, and rt ∈ R is the reward signal. We denote the underlying latent states by
St = (sι,t, ∙ ∙ ∙ , sd,t)>, where d is the dimensionality of latent states. We assume that the generative
process of the environment in the k-th domain (with k = 1, . . . n + n0) can be described in terms of
the transition function for each dimension of s and the observation and reward functions as
f	Si,t	=	fi(c 广 © St-I,ca - ∙ at-i,cθk >s © θk,eSt),	for i = 1,…，d,
O	ot	=	g(cs>° © st,cθk-° ∙ θθ,eθ),	(1)
I	Tt	=	h(cs* © st-i,ca-r∙ at-i,cθk*∙ &k总),
where © denotes the element-wise product, the eis,t, eto, etr terms are i.i.d. random noises. As explained
below, c∙» are masks (binary vectors or scalars that represent structural relationships from one variable
to the other), and θk = (θks, θko, θkr) are the change factors that have a constant value in each domain,
but vary across domains in the transition, observation, and reward function, respectively. The latent
states st+1 form an MDP: given st and at, st+1 is independent of previous states and actions. The
perceived signals ot are generated from the underlying states st . The actions at directly influence the
latent states st+1, instead of the observed signalsot, and the reward is determined by the latent states
and the action. Eq. 1 can also represent MDPs as a special case if states st are directly observed, in
which case the observation function ofot is not needed.
Structural relationships and graphs. Often the action variable at-1 does not influence every
dimension of st, and similarly, the reward rt may not be influenced by every dimension of st-1.
Furthermore, there are structural relationships between different dimensions of st-1 and st. To
characterize these constraints, we explicitly take into account the graph structure G over the variables
in the system characterized by a Dynamic Bayesian Network (Murphy, 2002) and encode the edges
with masks C>. In the first equation in Eq. 1 the transition function for the state component s%, where
the jth entry of CS.s ∈ {0,1}d is 1 if and only if sj,t influences si,t+ι (graphically represented by an
edge), while Ca -s ∈ {0,1} is 1 if and only if the action at has any effect on si,t+ι. Similarly, the binary
vector cθk 4S ∈ {0,1}p encodes which components of the change factor θk = (θf,fc,..., θp,fc)> affect
si,t+1. The masks in the observation function g and reward function h have similar functions. The
3
Published as a conference paper at ICLR 2022
masks and the parameters of the functions f, g, and h, are invariant; all changes are encoded in θk.
For simplicity of notation, We collect all the transition mask vectors in the matrices Csfs := [cS∙s]d=ι
and Cθk - := [cθk"s]d=ι and the scalars in the vector Ca-s ：=虏"s]?=：
Characterization of change factors in a compact way. In practical scenarios, the environment
model may change across domains. Moreover, it is often the case that given a high-dimensional input,
only a feW factors may change, Which is knoWn as minimal change principle (Ghassami et al., 2018)
or sparse mechanism shift assumption (SChGlkOPf et al., 2021). In such a case, instead of learning the
distribution shift over the high-dimensional input, thanks to the parsimonious graphical representation,
We introduce a loW-dimensional vector θk to characterize the domain-specific information in a
compact Way (Zhang et al., 2020b). Specifically, θko, θkr, and θsk capture the change factors in the
observation function, reWard function, and transition dynamics, respectively; each of them can be
multi-dimensional and that they are constant Within each domain. In general, θk can capture both the
changes in the influencing strength and those in the graph structure, e.g., some edges may appear
only in some domains. Since We assume that the structural relationships in Eq. 1 are invariant across
domains, this means that the masks C> have to encode an edge even if it presents only in one domain,
and furthermore, since θk encodes the changes, it can sWitch the edge off in other domains. Fig. 1
shoWs an example of the graphical representation of the (estimated) environment model. Specifically,
in this example, θks only influences s1,t, at-1 does not have an edge to s1,t, and among the states,
only sd,t-1 has an edge to rt. In this example, We consider the case When the control signals are
random, so there is no edge betWeen st and at .
3	What, Where, and How to Adapt in RL
We first assume that the environment model in Eq. 1 is knoWn (We Will explain hoW to learn it in
Sec. 3.1), and characterize Which changes have an effect on the policy transfer to the target domain.
In Eq. 1, We alloW the model to change across domains, including all involved functions, and We
leverage θk to capture the changes in a compact Way. The varying model implies that the optimal
policy function may also vary across domains. HoW can We then characterize the changes in the
optimal policy function in a compact Way, as We did in the model? Interestingly, We find that the
change factor θk and the latent state st are sufficient for policy learning, but not every dimension
of θk or st is necessary, since they may not ever have an effect on the reWard, even in future steps.
We first give the definitions of compact domain-shared representations and compact domain-specific
representations, according to the graph structure, and We further shoW that they are the minimal set
of dimensions that suffice for policy learning across domains (proof in Appendix).
Definition 1. Given the graphical representation of an environment model G that is encoded in the
binary masks C f, we define recursively the representations that affect the reward in thefuture as:
• compact domain-shared representations stmin : the latent state components si,t ∈ st that either
-	have an edge to the reward in the next time-step rt+ι, i.e., Cs4r = 1, or
-	have an edge to another state component in the next time-step sj,t+ι, i.e., cj；S = 1, such that
the same component at time t is a compact domain-shared representation, i.e., sj,t ∈ stmin;
• compact domain-specific representations θkmin : the latent change factors θi,k ∈ θk that either:
-	have an edge to the reward in the next time-step rt+ι, i.e., θi,k = θkr and cθk.r = 1, or
-	have an edge to a state component sj,t ∈ Smin, i.e., cθk-S = 1.
Proposition 1. Under the assumption that the graph G is Markov and faithful to the measured data,
the union of compact domain-specific θkmin and compact shared representations stmin are the minimal
and sufficient dimensions for policy learning across domains.
For the example in Fig. 1, stmin = (s1,t, sd,t) and θkmin = {θks , θkr}. Note that θko is never in θkmin,
and thus if only the observation function changes, the optimal policy function ∏k remains the same
across domains. For example in Cartpole a change of color does not affect the optimal policy.
Moreover, if cθk"r = 1, then θ1r ∈ θmmin, which is the case for multi-task learning.
3.1 Simultaneous Estimation of Domain-Varying Models
In this section, we give the estimation procedure of the environment model in Eq. 1 from observed
sequences {{hyt,k, at,ki}tT=1}kn=1 from each source domain k, where yt,k = (ot>,k, rt>,k)> are the
observations and reward at time t in domain k. Instead of estimating the model in each domain
4
Published as a conference paper at ICLR 2022
separately, we estimate models from different domains simultaneously, by exploiting commonalities
across domains while at the same time preserving specific information for each domain. In particular,
we propose the Multi-model Structured Sequential Variational Auto-Encoder (MiSS-VAE), which
contains the following three essential components. (1) "Sequential VAE" component handles the
sequential data, with the underlying latent states satisfying an MDP. It is implemented by adding an
LSTM (Hochreiter and Schmidhuber, 1997) to encode the sequential information in the encoder to
learn the inference model qφ(st,k∣st-ι,k, yi：t,k, a±t-ι,k； θk). (2) πMulti-modelπ component handles
models from different domains at the same time, using the domain index k as an input and learning
the domain-specific factors θk. (3) "Structured" component: exploits the structural information that is
explicitly encoded with the binary masks, i.e., Ciin Eq. 1. Here, the joint distribution of latent states
are factorized according to structures, instead of being marginally independent as in traditional VAEs
(Kingma and Welling, 2014). Fig. A3 in Appendix gives the diagram of neural network architecture
in model training. Let y11::Tn = {{yt,k}tT=1}kn=1. By taking into account the above three components,
we maximize the following objective function L:
L(y1：T/β1,β2,φ,γ,c) = Lrec(y1：:T"βι,φ,c))+Lpred(y1:T:⑸⑷)-LKL(y1：:T;(。〃, c))-Lreg.
In particular, Lrec is the reconstruction loss for both observed images and rewards, to learn the
observation and the reward function, respectively. We also consider the one-step prediction loss Lpred.
n T-2
Lrec = P P EStk~qφ(∙∣θk)0θg Ρβι (0t,k |st,k； θk,cθk 予任O) +log Pβι (rt+1,k|st,k ,at,k; θr,cθk T Csfr )},
k=1 t=1
n T-2
Lpre = P P Est,k ~qφZθk){⅛ Pβ2 (Ot+1,kMk,θO,θS) +log Pβ2 (rt+2,k lst,k,at + 1,k ； θk ,θk )},
k=1 t=1
where pβ1 and pβ2 denote the generative models with parameters β1 and β2, respectively, that are
shared across domains, and qφ the inference model with shared parameters φ. We also use the
following KL-divergence loss to constrain the latent space:
nT
LKL = λo P P KL(qφ(st,k∣st-i,k, yi：t,k,ai:t-i,k； θk)kpY(st,k∣st-i,k,at-i,k； θk, Cs产 Ca-s,Cθk÷s)),
k=1 t=2
where we explicitly model the transition dynamics pγ with the parameters γ shared across domains;
this is essential for establishing a Markov chain in latent space and learning a representation for
long-term predictions. Moreover, the KL loss helps to constrain the latent space to (1) ensure
that the disentanglement between the inferred latent factors q(si,t∣∙) and q(sj,t∣∙) for i = j, since
we do not consider the instantaneous connections among state dimensions, and (2) ensure that
the latent representations st are maximally compressive about the observed high-dimensional data.
Furthermore, according to the edge-minimality property (Zhang and Spirtes, 2011) and the minimal
change principle (Ghassami et al., 2018), we add sparsity constraints on structural matrices and on
the change of domain-specific factors across domains, respectively, to achieve better identifiability:
Lreg = λikcs>°ki + λ2kcs+kki + λ3∣∣Cɑ"ki + λ4∣∣Cs>ski + λ5∣∣Ca/ski + λ6kCθk+ski + λ7 P ∣θj- θk∣.
i≤j,k≤n
Note that besides the shared parameters {βi, β2, φ, γ}, the structural relationships (encoded in binary
masks C) are also involved in the shared parameters. Each factor in pφ, pβi, and pγ is modeled
with a mixture of Gaussians, because with a suitable number of Gaussians, it can approximate a
wide class of continuous distributions. Moreover, in model estimation, the domain-specific factors
θk = {θko, θkk, θkr} are treated as parameters; they are constant within the same domain, but may differ
in different domains. We explicitly consider θk not only in the generative models pβi and pγ , but
also in the inference model qφ. In this way, except for θk, all other parameters in MiSS-VAE are
shared across domains, so that all we need to update in the target domain is the low-dimensional θk,
which greatly improves the sample efficiency and the statistical efficiency in the target domain.
3.2	Low-Cost and Interpretable Policy Transfer
After identifying what and where to transfer, we show how to adapt. Instead of learning the optimal
policy in each domain separately, which is time and sample inefficient, we leverage a multi-task
learning strategy: policies in different domains are optimized at the same time exploiting both
commonalities and differences across domains. Given the compact domain-shared stmin and domain-
specific representations θkmin, we represent the optimal policies across domains in a unified way:
at = ∏*(smin, θmin),	(2)
where θkmin explicitly and compactly encodes the changes in the policy function in each domain
k, and all other parameters in the optimal policy function ∏* are shared across domains. In other
5
Published as a conference paper at ICLR 2022
words, by learning ∏* in the source domains, and estimating the value of the change factor。凿^ and
inferring latent states stmarignet from the target domain, we can immediately derive the optimal policy in
the target domain without further policy optimization by just applying Eq. 2.
The AdaRL framework answers what and where the change factors are and which change factors
need to adapt across domains in an interpretable way. Moreover, AdaRL only requires a few samples
to update the low-dimensional domain-specific parameters θtmarignet to achieve the optimal policy in
the target domain, without further policy optimization, achieving the low cost. We provide the
pseudocode for the AdaRL algorithm in Alg. 1. The algorithm has three parts: (1) data collection
with a random policy or any initial policy from n source domains (line 2), (2) model estimation from
the n source domains with multi-task learning (lines 2-3, see Sec. 3.1 for details), and (3) learning the
optimal policy ∏* with deep Q-learning, by making use of domain-specific factors and the inferred
domain-shared state representations (lines 4-21). Specifically, because we do not directly observe the
states st, we infer q(smnk ∣o≤t+1,k, r≤t+1,k, a≤t,k, θ/in) and sample Smink from its posterior, for
the kth domain (lines 7 and 13). Moreover, the action-value function Q is learned by considering the
averaged error over the n source domains (line 18). AdaRL can be implemented with a wide class of
policy-learning algorithms, e.g., DDPG (Lillicrap et al., 2015), Q-learning (Mnih et al., 2015), and
Actor-Critic methods (Schulman et al., 2016; Mnih et al., 2016). Then, in the target domain, we only
need to collect a few rollouts to estimate the low-dimensional domain-specific representations θtmarignet,
with all other parameters being fixed (lines 22-23).
Algorithm 1 (AdaRL with Domains Shifts)
1:	Initialize action-value function Q, target action-value function Q0 , and replay buffer B.
2:	Record multiple rollouts for each source domain k(k = 1, ∙∙∙, n) and estimate the model in Eq.1.
3:	Identify the dimension indices of stmin and the values of θkmin according to the learned model.
4:	for episode = 1, . . . , M do
5:	for source domain k = 1, . . . , n do
6:	Receive initial observations o1,k and r1,k for the k-th domain.
7:	Infer the posterior q(s1m,kin |o1,k, r1,k, θkmin ) and sample initial inferred state s1m,kin .
8:	end for
9:	for timestep t = 1, . . . , T do
10:	for source domain k = 1, . . . , n do
11:	Select at,k randomly with probability ; otherwise at,k = arg maxa Q(stm,kin ,a,θkmin ).
12:	Execute action at,k, and receive reward rt+1,k and observation ot+1,k in the kth domain.
13:	Infer the posterior q(smin,k ∣o≤t+ι,k, r≤t+ι,k, a≤t,k, θmin) and sample Smin,k.
14:	Store transition (stm,kin , at,k, rt+1,k, stm+i1n,k, θkmin ) in reply buffer B.
15:	end for
16:	Randomly sample a minibatch of N transitions (Sim,jin,ai,j,ri+1,j,Sim+i1n,j,θjmin)fromB.
17:	Set yi,j = ri+1,j + λ maxa0 Q0(sim+i1n,j, a0, θjmin).
18:	Update action-value function Q by minimizing the loss:
L=ɪ X(yi,j - Q(smin, ai,j, θmn)))∙
n不IV
i,j
19:	end for
20:	Update the target network Q0 : Q0 = Q.
21:	end for
22:	Record a few rollouts from the target domain.
23:	Estimate the values of θtmarignet for the target domain, with all other parameters fixed.
3.3	Theoretical Properties
Below we show the conditions under which we can identify the true graph G from observational data,
even when the model in Eq. 1 is unknown. Furthermore, we derive a generalization bound of the
state-value function under the PAC-Bayes framework (McAllester, 1999).
Theorem 1 (Structural Identifiability). Suppose the underlying states St are observed, i.e., Eq. (1)
is an MDP. Then under the Markov condition and faithfulness assumption, the structural matrices
Cs"s, Ca-÷s, CsTr, CaTr, Cθk今S, and cθk今r are identifiable.
6
Published as a conference paper at ICLR 2022
This theorem shows that in the MDP scenario, where the underlying states are observed and cθk'o
and Cs40 are not considered by definition, We can uniquely determine the structural relationships
over {st-1, st, at-1, rt, θk}, i.e., the Dynamic Bayesian network G, from observed data under mild
conditions, without knowing the generative environment model. Even if θk is not directly observed,
we can identify which state dimension changes and if there is a change in the reward function.
Suppose there are n source domains, and for the kth domain, we have Sk =
((sι,k,v*(sι,k)), ∙∙∙ , (Smk,k,v*(smk,k))), Where mk is the number of samples from the kth do-
main, s.,k is a state sampled from the kth domain, and v*(s.,k) is its corresponding optimal
state-value. For any value function he*in(∙) parameterized by θmmin, we define the loss function
'(hθmin, (Sk,i,v*(si,k))) = Ddist(hθmin(si,k),v*(s*k)), where Ddist is a distance function that
measures the discrepancy between the learned value and the optimal value. The following theorem
gives a generalization bound of the state-value function under the PAC-Bayes framework.
Theorem 2 (Generalization Bound). Let Q be an arbitrary distribution over θkmin and P the prior
distribution over θkmin. Then for any δ ∈ (0, 1], with probability at least 1 - δ, the following
inequality holds uniformly for all Q,
er(Q) ≤ 1 P {eT(Q, Sk) + q2(mk-Γ) (,Dkl(Q∣∣P)+log 2nmk) + J八(DKL(Q||P)+log 竿)},
where er(Q) and er(Q, Sk) are the generalization error and the training error between the estimated
value and the optimal value, respectively.
Theorem 2 states that with high probability the generalization error er(Q) is upper bounded by
the empirical error plus two complexity terms. Specifically, the first one is the average of the task-
complexity terms from the observed domains, which converges to zero in the limit of samples in each
domain, i.e., mk → ∞. The second is an environment-complexity term, which converges to zero if
infinite domains are observed, i.e., n → ∞. Moreover, if assuming different dimensions of θkmin are
∣θminι .
independent, then DKL(Q||P) =	i=k1	DKL(Qi||Pi), which indicates that a low-dimensional
θkmin usually has a smaller KL divergence, so does the upper bound of the generalization error.
4	Evaluation
We modify the Cartpole and Atari Pong environments in OpenAI Gym (Brockman et al., 2016). Here,
we present a subset of the results; see Appendix for the complete results and the detailed settings. We
consider changes in the state dynamics (e.g., the change of gravity or cart mass in Cartpole, change
of orientation in Pong), changes in observations (e.g., different noise levels in images or different
colors in Pong), and changes in reward functions (e.g., different reward functions in Pong based on
the contact point of the ball), as shown in Fig. 2 for Pong. For each of these factors, we take into
account both interpolation (where the factor value in the target domain is in the support of that in
source domains), and extrapolation (where it is out of the support w.r.t. the source domains). We
train on n source domains based on the trajectory data generated by a random policy. In Cartpole, for
each domain we collect 10000 trials with 40 steps. For Pong experiments, each domain contains 40
episodes data and each of them takes a maximum of 10000 steps. In the target domain we consider
different sample sizes with Ntarget = {20, 50, 10000} to estimate θtmarignet. For both games, we evaluate
the POMDP case, where the inputs are high-dimensional images; note that we did not stack multiple
frames, so some properties (e.g., velocity) are not observed, resulting in a POMDP. For Cartpole, we
also consider the MDP case, where the true states (cart position and velocity, pole angle and angular
velocity) are used as the input to the model. In Cartpole, we also experiment with multiple factors
changing at the same time (e.g., gravity and mass change concurrently in the target domain).
Modified Cartpole setting The Cartpole problem consists of a cart and a vertical pendulum
attached to the cart using a passive pivot joint. The task is to prevent the vertical pendulum from
falling by putting a force on the cart to move it left or right. We introduce two change factors for
the state dynamics θks: varying gravity and varying mass of the cart. In terms of changes on the
observation function θko, we add Gaussian noise on the images. Since θko does not influence the
optimal policy (as shown in Prop. 1), we need it only for the model estimation, but not for policy
optimization. Moreover, if θk = {θko }, the optimal policy is shared across domains.
7
Published as a conference paper at ICLR 2022
Figure 2: Illustrations of the change factors on modified Pong game.
	Oracle Upper bound	Non-t lower bound	CAVIA (Zintgraf et al.,2019)	PEARL (Rakelly etal.,2019)	AdaRL* Ours w/o masks	AdaRL Ours
G_in	2486.1 (±369.7)	~1098.5 ∙ ~ (±472.1)	1603.0 (±877.4)	1647.4 (±617.2)	1940.5 (±841.7)	~2217.6~ (±981.5)
G_out	693.9 (±100.6)	~204.6 • (±39.8)	392.0 • (±125.8)	434.5 • (±102.4)	439.5 • (±157.8)	508.3 (±138.2)
M_in	2678.2 (±630.5)	~748.5 • (±342.8)	2139.7 (±859.6)	1784.0 (±845.3)	1946.2 • (±496.5)	~2260.2~ (±682.8)
M_out	1405.6 (±368.0)	~371.0 • (±92.5)	972.6 • (±401.4)	793.9 • (±394.2)	874.5 • (±290.8)	1001.7 (±273.3)
G_in & M_in	1984.2 (±871.3)	~365.0 • (±144.5)	1012.5 • (±664.9)	1260.8 • (±792.0)	1157.4 • (±578.5)	1428.4 (±495.6)
G_out & M_out	939.4 (±270.5)	336.9 • (±139.6)	648.2 • (±481.5)	544.32 • (±175.2)	596.0 • (±184.3)	689.4 (±272.5)
Table 1: Average final scores on modified Cartpole (MDP) with Ntarget = 50. The best non-oracle
results w.r.t. the mean are marked in red, while bold indicates a statistically significant result w.r.t. all
the baselines, and "•" indicates the baseline for which the improvements of AdaRL are statistically
significant (via Wilcoxon signed-rank test at 5% significance level). G and M denote the gravity and
mass respectively, and “*in" and “*out" denote the interpolation and extrapolation, respectively.
Modified Pong setting In Pong, one of the established Atari benchmarks (Bellemare et al., 2013),
the agent controls a paddle moving up and down vertically, aiming at hitting the ball. We consider
changes in observation function θko, state dynamics θks, and reward function θkr, as shown in Fig. 2.
We consider three change factors on perceived signals θko : different image sizes, different image
colors, and different noise levels. For the setting with different image colors, we use RGB images
as inputs and consider source domains with varying RGB colors {original, green, red} and target
domains with colors {yellow, white}, but for other settings, we convert the images to grayscale as
input. To change the state dynamics, we rotate the images ω degrees clockwise. To test the changes
in the reward function, we modify the reward as a function of the distance between contact point and
the central point of the paddle, denoted by d, as opposed to the original Pong in which it is constant
(-1 or +1) when the agent or the opponent misses the pong. We denote by L the half-length of the
paddle and formulate two groups of reward functions: (1) Linear reward: Tt =等; and (2) Non-linear
reward: r = d^, where α varies across domains.
Baselines In the MDP setting, we compare AdaRL with CAVIA (Zintgraf et al., 2019) and
PEARL (Rakelly et al., 2019). In the POMDP setting, we compare with PNN (Rusu et al., 2016),
PSM (Agarwal et al., 2021) and MQL (Fakoor et al., 2020). We also compare with AdaRL*, a
version of AdaRL that does not learn the binary masks Cf and therefore does not use any structural
information. All of these methods use the same number of samples Ntarget from the target domain.
We also compare with: 1) Non-t, a vanilla non-transfer baseline that pools data from all source
domains and learns a fixed model; and 2) an oracle baseline, which is completely trained on the
target domain with model-free exploration. For a fair comparison, we use the same policy learning
algorithm, Double DQN (Van Hasselt et al., 2016), for all methods. As opposed to MAML and PNN,
AdaRL only uses the Ntarget samples to estimate θtmairnget, without any policy optimization.
Results We measure performance by the mean and standard deviation of the final scores over 30
trials with different random seeds, as well as testing the significance with the Wilcoxon signed-
rank test (Conover, 1999). As shown in Tables 1, 2 and 3 2, AdaRL consistently outperforms the
baselines across most change factors in the MDP and POMDP case for modified Cartpole, and
2In Table 1-3, "•" indicates the baselines for which the improvements of AdaRL are statistically significant
(via Wilcoxon signed-rank test at 5% significance level).
8
Published as a conference paper at ICLR 2022
	Oracle	Non-t	PNN	PSM	MTQ	AdaRL*	AdaRL Upper bound lower bound (Rusu et al., 2016) (Agarwal et al., 2021) (Fakoor et al., 2020) Ours w/o masks	Ours
G_in	1930.5	1031.5 •	1268.5 •	1439.8 •	1517.9	1460.6	1697.4 (±1042.6)	(±837.9)	(±699.0)	(±427.6)	(±883.6)	(±497.5)	(±1002.3)
G_out	408.6	69.7 •	307.9 •	273.8 •	330.6	298.6 •	353.4 (±67.2)	(±19.4)	(±100.4)	(±92.6)	(±109.8)	(±69.3)	(±79.6)
M_in	2004.9	608.5 •	1600.8 •	1891.5	1735.6 •	1884.5	1912.8 (±404.3)	(±222.8)	(±463.5)	(±638.4)	(±398.7)	(±429.7)	(±378.5)
M_out	1498.6	216.4 •	987.6 •	1032.7 •	862.2 •	1219.5	1467.5 (±625.4)	(±77.3)	(±368.5)	(±634.0)	(±300.4)	(±1014.3)	(±837.2)
N_in	8640.5	942.0 •	3952.4 •	5279.6 •	6927.3 •	5540.8 •	7817.4 (±3086.1) (±207.5)	(±1024.9)	(±1969.7)	(±2464.8)	(±2013.6)	(±3009.5)
N_out	4465.2	1002.8 •	1137.1 •	2740.9 •	3298.5 •	2018.9 •	3640.9 (±667.3)	(±335.2)	(±384.6)	(±511.5)	(±537.8)	(±685.4)	(±841.0)
Table 2: Average final scores on modified Cartpole (POMDP) with Ntarget = 50. The best non-oracle
results are marked in red. G, M, and N denote the gravity, mass, and noise respectively.
	Oracle	Non-t	PNN	PSM	MTQ	AdaRL*	AdaRL Upper bound lower bound (Rusu et al., 2016) (Agarwal et al., 2021) (Fakoor et al., 2020) Ours w/o masks	Ours
O_in	18.65	6.18 •	9.70 •	11.61 •	15.79 •	14.27 •	18.97 (±2.43)	(±2.43)	(±2.09)	(±3.85)	(±3.26)	(±1.93)	(±2.00)
O_out	19.86	6.40 •	9.54 •	10.82 •	10.82 •	12.67 •	15.75 (±1.09)	(±3.17)	(±2.78)	(±3.29)	(±4.13)	(±2.49)	(±3.80)
C_in	19.35	8.53 •	14.44 •	19.02	16.97 •	18.52 •	19.14 (±0.45)	(±2.08)	(±2.37)	(±1.17)	(±2.02)	(±1.41)	(±1.05)
C_out	19.78	8.26 •	14.84 •	17.66 •	15.45 •	17.92	19.03 (±0.25)	(±3.45)	(±1.98)	(±2.46)	(±3.30)	(±1.83)	(±0.97)
S_in	18.32	6.91 •	11.80 •	12.65 •	13.68 •	14.23 •	16.65 (±1.18)	(±2.02)	(±3.25)	(±3.72)	(±3.49)	(±3.19)	(±1.72)
S_out	19.01	6.60 •	9.07 •	8.45 •	11.45 •	12.80 •	17.82 (±1.04)	(±3.11)	(±4.58)	(±4.51)	(±2.46)	(±2.62)	(±2.35)
N_in	18.48	5.51 •	12.73 •	11.30 •	12.67 •	13.78 •	16.84 (±1.25)	(±3.88)	(±3.67)	(±2.58)	(±3.84)	(±2.15)	(±3.13)
N_out	18.26	6.02 •	13.24 •	11.26 •	15.77 •	14.65 •	18.30 (±1.11)	(±3.19)	(±2.55)	(±3.15)	(±2.12)	(±3.01)	(±2.24)
Table 3: Average final scores on modified Pong (POMDP) with Ntarget = 50. The best non-oracle
are marked in red. O, C, S, and N denote the orientation, color, size, and noise factors, respectively.
in the POMDP case for Pong for Ntarget = 50. As ablation studies, to see the effect of learning
the graphical structure, We also compare with AdaRL*, which does not learn the binary masks
c∙, but just assumes everything is fully connected. Learning the graphical structure improves the
performances significantly, and without it AdaRL* is generally comparable to baselines. We provide
results with Ntarget = {20, 50, 10000} in Appendix, showing that the performance gains are larger
at smaller sample sizes. Furthermore, we consider the change of reward functions (see Table A12-14
in Appendix). More detailed experimental results are provided in Appendix, including the average
score across different Ntarget, policy learning curves and an analysis of the estimated θk w.r.t. real
change factor. Interestingly, in the Cartpole case, the estimated θk matches the physical quantities
that are being changed across the domains. In particular, the estimated θk for gravity and noise are
linear mappings of the gravity and noise level values. For the mass-varying case, the learned θsk is a
nonlinear monotonic function of the mass, which matches the influence of the mass on the dynamics.
5	Conclusions and future work
In this paper, we proposed AdaRL, a principled framework for transfer RL. AdaRL learns a latent
representation with domain-shared and domain-specific components across source domains, uses
it to learn an optimal policy parameterized by the domain-specific parameters, and applies it to a
new target domain. It is achieved without any further policy optimization, but just by estimating
the values of the domain-specific parameters in the target domain, which can be accomplished
with a few target-domain data. As opposed to previous work, AdaRL can model changes in the
state dynamics, observation function and reward function in an unified manner, and exploit the
factorization to improve the data efficiency and adapt faster with fewer samples. Further directions
include exploiting the target domain to fine-tune the policy and handling the out-of-distribution data.
Moreover, exploring an alternative to the reconstruction loss, e.g., using the contrastive loss (Laskin
et al., 2020), might also improve the training efficiency. Finally, an exciting next step is to transfer
knowledge across different tasks, e.g., different Atari games.
9
Published as a conference paper at ICLR 2022
Reproducibility S tatement
Proofs of all our theoretical results are given in Appendix A1, A2, and A3 with disclosure of all
assumptions. More details about the model estimation are given in Appendix A4. Appendix A5
provides the complete experimental details and results. All datasets used are publicly available
or instructions are provided on how to generate them in Appendix 5 and 6. A description of
the hyperparameters and network architectures used is included in Appendix 6. Source code is
given at https://github.com/Adaptive-RL/AdaRL-code, providing also a complete description of our
experimental environment, configuration files and instructions on the reproduction of our experiments.
Acknowledgement
KZ would like to acknowledge the support by the National Institutes of Health (NIH) under Contract
R01HL159805, by the NSF-Convergence Accelerator Track-D award #2134901, by the United States
Air Force under Contract No. FA8650-17-C7715, and by a grant from Apple. BH would like to
acknowledge the support by Apple PhD fellowship in AI/ML. Part of the work was performed while
BH was interning at the MIT-IBM Watson AI Lab.
References
R.	Agarwal, M. C. Machado, P. S. Castro, and M. G. Bellemare. Contrastive behavioral similar-
ity embeddings for generalization in reinforcement learning. In International Conference on
Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=
qda7-sVg84.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal OfArtificial Intelligence Research, 47:253-279,
2013.
C. Boutilier, R. Dearden, and M. Goldszmidt. Stochastic dynamic programming with factored
representations. Artificial Intelligence, 121(1-2):49-107, 2000.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym. arXiv preprint arXiv:1606.01540, 2016.
I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn. Learning to
adapt in dynamic, real-world environments through meta-reinforcement learning. In International
Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/
forum?id=HyztsoC5Y7.
W. J. Conover. Practical nonparametric statistics (3rd ed.). John Wiley & Sons, Inc., 1999.
F. Doshi-Velez and G. Konidaris. Hidden parameter markov decision processes: A semiparametric
regression approach for discovering latent task parametrizations. In Proceedings of the Twenty-Fifth
International Joint Conference on Artificial Intelligence (IJCAI), volume 2016, page 1432. NIH
Public Access, 2016.
Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl 2 : Fast reinforcement
learning via slow reinforcement learning. In International Conference on Learning Representations
(ICLR), 2017. URL https://openreview.net/forum?id=HkLXCE9lx.
R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola. Meta-q-learning. In International Conference
on Learning Representations (ICLR), 2020. URL https://openreview.net/forum?id=
SJeD3CEFPH.
F. Ferndndez, J. Garcia, and M. Veloso. Probabilistic policy reuse for inter-task transfer learning.
Robotics and Autonomous Systems, 58(7):866-871, 2010.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning (ICML), pages 1126-1135. PMLR, 2017.
10
Published as a conference paper at ICLR 2022
A. Ghassami, N. Kiyavash, B. Huang, and K. Zhang. Multi-domain causal structure learning in linear
systems. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504-507, 2006.
S.	Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
B. Huang, K. Zhang, J. Zhang, J. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Scholkopf.
Causal discovery from heterogeneous/nonstationary data. Journal of Machine Learning Research,
21(89):612-634, 2020.
S. Katt, F. A. Oliehoek, and C. Amato. Bayesian reinforcement learning in factored pomdps. In
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems
(AAMAS), AAMAS ’19, page 7-15, Richland, SC, 2019. ISBN 9781450363099.
M. Kearns and D. Koller. Efficient reinforcement learning in factored mdps. In Proceedings of
the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI), volume 16, pages
740-747, 1999.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on
Learning Representations (ICLR), 2014. URL https://openreview.net/forum?id=
33X9fd2-9FyZd.
M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for rein-
forcement learning. In International Conference on Machine Learning (ICML), pages 5639-5650.
PMLR, 2020.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
D. A. McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual Conference
on Computational Learning Theory (COLT), pages 164-170, 1999.
R. Mendonca, A. Gupta, R. Kralev, P. Abbeel, S. Levine, and C. Finn. Guided meta-policy search.
Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.
G. Mesnil, Y. Dauphin, X. Glorot, S. Rifai, Y. Bengio, I. Goodfellow, E. Lavoie, X. Muller, G. Des-
jardins, D. Warde-Farley, P. Vincent, A. Courville, and J. Bergstra. Unsupervised and transfer
learning challenge: a deep learning approach. JMLR W and CP: Proc. of the Unsupervised and
Transfer Learning challenge and workshop, 27, 2012.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015.
V.	Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine
Learning (ICML), pages 1928-1937. PMLR, 2016.
K. Murphy. Dynamic bayesian networks: Representation, inference and learning. UC Berkeley,
Computer Science Division, 2002.
J.	Pearl and D. Mackenzie. The Book of Why. Basic Books, New York, 2018. ISBN 978-0-465-
09760-9.
X. B. Peng, E. Coumans, T. Zhang, T.-W. E. Lee, J. Tan, and S. Levine. Learning agile robotic
locomotion skills by imitating animals. In Proceedings of Robotics: Science and Systems (RSS),
07 2020. doi: 10.15607/RSS.2020.XVI.064.
11
Published as a conference paper at ICLR 2022
K.	Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement
learning via probabilistic context variables. In International Conference on Machine Learning
(ICML), pages 5331-5340. PMLR, 2019.
A.	A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu,
and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
B.	Scholkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
B. Scholkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward
causal representation learning. Proceedings of the IEEE, 109(5):612-634, 2021.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. In International Conference on Learning Representations
(ICLR), 2016.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,
I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering
the game of go with deep neural networks and tree search. Nature, 529:484-489, 2016.
A. L. Strehl, C. Diuk, and M. L. Littman. Efficient structure learning in factored-state mdps. In
Proceedings of the Twenty-second AAAI Conference on Artificial Intelligence (AAAI), volume 7,
pages 645-650, 2007.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,
MA, 1998.
A. Tamar, Y. Glassner, and S. Mannor. Optimizing the cvar via sampling. In Proceedings of the
Twenty-ninth AAAI Conference on Artificial Intelligence (AAAI), 2015.
M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal
of Machine Learning Research, 10(7), 2009.
M. E. Taylor, P. Stone, and Y. Liu. Transfer learning via inter-task mappings for temporal difference
learning. Journal of Machine Learning Research, 8(1):2125-2167, 2007.
A. Tirinzoni, A. Sessa, M. Pirotta, and M. Restelli. Importance weighted transfer of samples
in reinforcement learning. In International Conference on Machine Learning (ICML), pages
4936-4945. PMLR, 2018.
A. Tirinzoni, M. Salvini, and M. Restelli. Transfer of samples in policy search via multiple importance
sampling. In International conference on machine learning (ICML), pages 6264-6274. PMLR,
2019.
M. Tomar, A. Zhang, R. Calandra, M. E. Taylor, and J. Pineau. Model-invariant state abstractions for
model-based reinforcement learning. In Self-Supervision for Reinforcement Learning Workshop -
ICLR 2021, 2021. URL https://openreview.net/forum?id=O1w48WvlmiC.
H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In
Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), volume 30, 2016.
W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with
online system identification. In Proceedings of Robotics: Science and Systems (RSS), Cambridge,
Massachusetts, July 2017. doi: 10.15607/RSS.2017.XIII.048.
A. Zhang, C. Lyle, S. Sodhani, A. Filos, M. Kwiatkowska, J. Pineau, Y. Gal, and D. Precup. Invariant
causal prediction for block mdps. In International Conference on Machine Learning (ICML),
pages 11214-11224. PMLR, 2020a.
A. Zhang, R. T. McAllister, R. Calandra, Y. Gal, and S. Levine. Learning invariant representations
for reinforcement learning without reconstruction. In International Conference on Learning Repre-
sentations (ICLR), 2021a. URL https://openreview.net/forum?id=-2FCwDKRREu.
12
Published as a conference paper at ICLR 2022
A. Zhang, S. Sodhani, K. Khetarpal, and J. Pineau. Learning robust state abstractions for hidden-
parameter block {mdp}s. In International Conference on Learning Representations (ICLR), 2021b.
URL https://openreview.net/forum?id=fmOOI2a3tQP.
J.	Zhang and P. Spirtes. Intervention, determinism, and the causal minimality condition. Synthese,
182(3):335-347, 2011.
K.	Zhang, M. Gong, P. Stojanov, B. Huang, Q. Liu, and C. Glymour. Domain adaptation as a problem
of inference on graphical models. Advances in Neural Information Processing Systems (NeurIPS),
33:4965-4976, 2020b.
Z. Zhu, K. Lin, and J. Zhou. Transfer learning in deep reinforcement learning: A survey. arXiv
preprint arXiv:2009.07888, 2020.
L.	Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson. Fast context adaptation via meta-
learning. In International Conference on Machine Learning (ICML), pages 7693-7702. PMLR,
2019.
13
Published as a conference paper at ICLR 2022
Appendix for
“AdaRL: What, Where, and How to Adapt in
Transfer Reinforcement Learning"
Appendix organization:
•	Appendix A1: Proof of Proposition 1
•	Appendix A2: Proof of Theorem 1
•	Appendix A3: Proof of Theorem 2
•	Appendix A4: More details for model estimation
•	Appendix A5: Complete experimental results
•	Appendix A6: Experimental details
A1 Proof of Proposition 1
We first review the definitions of d-separation, the Markov condition, and the faithfulness assumption
(Spirtes et al., 1993; Pearl, 2000), which will be used in the proof.
Given a directed acyclic graph G = (V, E), where V is the set of nodes and E is the set of directed
edges, we can define a graphical criterion that expresses a set of conditions on the paths.
Definition A1 (d-separation (Pearl, 2000)). A path p is said to be d-separated by a set of nodes
Z ⊆ V if and only if(1) P contains a chain i → m → j or afork i J m → j such that the middle
node m is in Z, or(2) P contains a collider i → m J j such that the middle node m is not in Z and
such that no descendant of m is in Z.
Let X, Y, andZ be disjunct sets of nodes. Z is said to d-separate X from Y (denoted as X ⊥d Y|Z) if
and only if Z blocks every path from a node in X to a node in Y.
Definition A2 (Global Markov Condition (Spirtes et al., 1993; Pearl, 2000)). A distribution P over
V satisfies the global Markov condition on graph G if for any partition (X, Z, Y) such that X ⊥d Y|Z
P(X, Y|Z) = P (X|Z)P (Y|Z).
In other words, X is conditionally independent of Y given Z, which we denote as X ⊥ Y|Z.
Definition A3 (Faithfulness Assumption (Spirtes et al., 1993; Pearl, 2000)). There are no indepen-
dencies between variables that are not entailed by the Markov Condition.
If we assume both of these assumptions, then we can use d-separation as a criterion to read all of
the conditional independences from a given DAG G. In particular, for any disjoint subset of nodes
X,Y,Z ⊆ V: X ⊥ Y∣Z ^⇒ X ⊥d Y|Z.
In our case we can represent the generative model in Eq. 1 as a Dynamic Bayesian Network(DBN)
G (Murphy, 2002) over the variables {st-ι,at-ι, 0t-1,rt, st, θk}, where the binary masks C4
represent edges or sets of edges, as shown in Fig. A1. As is typical in DBN we assume that the graph
is invariant across different timesteps. We assume θk are constant across the different timesteps. We
add to the image also the cumulative reward R. In practice we will focus instead on the cumulative
future reward Rt+1 = PτT=t+1 γτ-t-1rτ, which only considers the contributions of the rτ in the
future with respect to the current timestep t.
In order to prove Proposition 1, we first need to prove that the compact shared representations stmin
and compact shared representations θ kmin are all the state and change factors dimensions, respectively,
that are conditionally independent of at given the future cumulative reward Rt+1 , even given all
other variables:
14
Published as a conference paper at ICLR 2022
Figure A1: Graphical representation of the generative model in Eq. 1. In the top of the figure, the
square boxes are the domain-specific parameters θk, which are constant in time, while the rectangular
boxes represent the binary masks that encode the edges. R represents the cumulative reward
Lemma A1. Under the assumption that the graph G is Markov and faithful to the measured data, a
state dimension si,t ∈ st is part of stmin iff:
si,t -⊥ at | Rt+1, st ∀st ⊆ {st \ si,t},
Similarly, a change factor dimension θi,k ∈ θk is part of θkmin iff:
θi,k 其 at∣Rt+ι, lt, ∀lt ⊆ {st,{θk \ θi,k}}.
Proof. We split the proof in two parts, the "only if" and the "if" part:
“If conditionally dependent on at given Rt+1 then in compact representation ":
We first show that if si,t satisfies the conditional dependence si,t ⊥ at ∣Rt+1, st, ∀st ⊆ {st \ si,t},
then it is part of stmin, i.e. si,t either has an edge to the reward in the next time-step rt+1, or,
recursively, it has an edge to another state component in the next time-step sj,t+1, such that the same
component at time step t, sj,t+1 ∈ stmin . Note that this recursive definition collects all of the states
si,t that have an effect on future reward rt+τ , τ = {1, . . . , T - t}, either directly, or through the
influence on other state components. Since all of these rt+τ are influencing the cumulative future
reward Rt+1 , all of these components have an edge to Rt+1 as well. We prove it by contradiction.
Suppose that si,t -6- at |Rt+1 does not have a direct or indirect path to rt+τ , i.e. si,t ∈ smin. By
assumption at only affects future state st+1, so si,t - at. Then, according to the Markov and
faithfulness conditions, si,t is independent of at conditioning on Rt+1, since there is no path that
connects si,t → …→ rt+τ → Rt+1 - rt+1 - at on which Rt+1 is a collider (i.e. a variable with
two incoming edges), which is the only path which could introduce a conditional dependence. This
contradicts the assumption.
Similarly we show that if ∀lt ⊆ {st, {θk \ θi,k}} the change factor dimension θi,k -6- at|Rt+1, lt,
then θi,k ∈ θkmin, which similarly to previous case means it has a direct or indirect effect on rt+τ
and therefore Rt+1. By contradiction suppose that θi,k -6- at|Rt+1, lt for all previously defined lt,
but it is not a change parameter for the reward function θi,k ∈ θk with cθk"r = L nor it is a change
parameter for the state dynamics θi,k ∈ θks with a direct or indirect path to rt+τ for τ = 1, . . . , T - t.
By assumption of our model, θi,k is never connected to at directly, nor they might have a common
cause, so θi,k - at . Then, according to the Markov condition, θi,k is independent of at conditioning
on Rt+1 , which contradicts the assumption, since:
•	if θi,k ∈ θk or cθk>r = 0, then it means there is no path θi,k → si,t+τ → rt+τ → Rt+1 -
rt+1 - at for any T ∈ N that would be open by conditioning on Rt+1;
15
Published as a conference paper at ICLR 2022
•	if θi,k ∈ θks but there is no directed path to rt+τ for any τ ≥ 1, i.e. then there is also no
directed path θi,k → ∙∙∙ → rt+τ → Rt+ι J rt+ι J at that would be open when We
condition on Rt+1 .
“If in compact representation then conditionally dependent on at given Rt+1":
We next show that if si,t ∈ stmin, which mean si,t has a direct or indirect edge to rt+τ , τ =
{1,...,T 一 t}, then si,t satisfies the conditional dependence si,t -⊥ at∣Rt+ι, St, ∀st ⊆ {st \ s*t}.
We prove it by contradiction. Suppose si,t has a directed path to rt+τ and si,t is independent on
at given Rt+1 and a subset of other variables sst ⊆ st \ si,t. Since we assume that there are no
instantaneous causal relations across the state dimensions, if si,t ⊥6⊥ at|Rt+1 there can never be an
sj,t such that si,t ⊥ at|Rt+1, sj,t. In this case, this means that si,t ⊥ at|Rt+1 has to hold. Then
according to the Markov and faithfulness assumptions, si,t cannot have any directed path to any
rt+τ ∀τ ≥ 1, because that any such path create a v-structure in the collider Rt+1, which would be
open if we condition on Rt+1 , contradicting the assumption.
Similarly, suppose θi,k is a dimension in θk that has a directed path to rt+τ. We distinguish two cases,
and show in neither can θi,k be independent of at given Rt+1 and a subset of the other variables:
•	if θi,k ∈ θkr, then it cannot be independent of at when we condition on Rt+1, which is a
descendant of rt+1 and therefore opens the collider path θkr J rt+1 J at ;
•	if θi,k ∈ θks, then at timestep t it is always only connected to the corresponding si,t. So if
there is a directed path π to rt+τ, it has to go through si,t . While π cannot be blocked by
any subset of {θk \ θi,k}, it can be blocked by conditioning on si,t, there are infinite future
paths with the same structure, e.g. through si,t+1 that will not be blocked by conditioning
only on variables at timestep t. Under the faithfulness and Markov assumption this means
that θi,k cannot be independent from at by conditioning on any subset of state variables at
timestep t or any other change parameters, which is a contradiction.
□
We can now prove our main proposition:
Proposition 1. Under the assumption that the graph G is Markov and faithful to the measured data,
the union of compact domain-specific θkmin and compact shared representations stmin are the minimal
and sufficient dimensions for policy learning across domains.
Proof. As shown in the previous lemma, in compact domain-generalizable representations stmin
every dimension is dependent on at given Rt+1 and any other variables, and every other dimension is
independent of at given Rt+1 and some other variables. Furthermore, because every dimension that
is dependent on at is necessary for the policy learning and every dimension that is (conditionally)
independent of at for at least a subset of other variables is not necessary for the policy learning, com-
pact domain-specific θkmin and compact shared representations stmin contain minimal and sufficient
dimensions for policy learning across domains. Note that the agents determine the action under the
condition of maximizing cumulative reward, which policy learning aims to achieve, so we always
consider the situation when the discounted cumulative future reward Rt+ι is given.	□
A2 Proof of Theorem 1
Theorem 1 (Structural Identifiability). Suppose the underlying states st are observed, i.e., Eq. (1)
is an MDP. Then under the Markov condition and faithfulness assumption, the structural matrices
Cs今s, Ca∙s, Cs今r, Ca今T, Cθk"S, and cθk"r are identifiable .
Proof. We concatenate data from different domains and denote by k be the variable that takes the
domain index 1, ∙∙∙ ,n. Since the data distribution changes across domains and the change is due to
the unobserved change factors θk that influence the observed variables, we can represent the change
factors as a function of k. In other words, we use the domain index k as a surrogate variable to
characterize the unobserved change factors.
16
Published as a conference paper at ICLR 2022
(a) An example of a ground Bayesian network (unrolled DBN over time).
(b)	s2,t is not a compact domain-specific represen-
tation, since there is no directed path to any rt+τ ,
i.e. s2,t ⊥ at|R. Similarly, there is no directed
path from θ1,k to R.
(c)	s1,t is a compact domain-specific represen-
tation, since there exists a path to at that is d-
connected when we condition on the collider R.
Similarly θ1,k is d-connected to at when condi-
tioning on R.
Figure A2: Example model in which s1,t and s3,t are compact domain-specific representations for
policy learning, while s2,t is not. This does not mean that s2,t and θ2,k are not useful in the model
estimation part, especially in estimating θko .
17
Published as a conference paper at ICLR 2022
We denote the variable set in the system by V, with V = {s1,t-1, . . . , sd,t-1, s1,t, . . . , sd,t, at-1, rt},
and the variables form a dynamic Bayesian network G . Note that in our particular problem, according
to the generative environment model in Eq. (1), the possible edges in G are only those from
si,t-1 ∈ st-1 to sj,t ∈ st, from at-1 to sj,t ∈ st, from si,t-1 ∈ st-1 to rt, and from at-1 to rt. We
further include the domain index k into the system to characterize the unobserved change factors.
It has been shown that under the Markov condition and faithfulness assumption, for every Vi , Vj ∈ V,
Vi and Vj are not adjacent in G if and only if they are independent conditional on some subset of
{Vl |l 6= i, l 6= j} ∪ k (Huang et al., 2020). Thus, we can asymptotically identity the correct graph
skeleton over V.
Moreover, since we assume a dynamic Bayesian network, there the direction of an edge between a
variable at time t to one at time t + 1 is fixed. Therefore, the structural matrices Cs→s, Ca今s, Cs今r,
and CaT which are parts of the graph G over V, are identifiable.
Furthermore, we want to show the identifiability of Cθk今s, and cθk今r; that is, to identify which
distribution modules have changes. Whether a variable Vi has a changing module is decided by
whether Vi and k are independent conditional on some subset of other variables. The justification for
one side of this decision is trivial. If Vi’s module does not change, that means P(Vi | PAi) remains the
same for every value of k, and so Vi ⊥ k | PAi . Thus, if Vi and k are not independent conditional on
any subset of other variables, Vi ’s module changes with k, which is represented by an edge between
Vi and k. Conversely, we assume that if Vi ’s module changes, which entails that Vi and k are not
independent given PAi, then Vi and k are not independent given any other subset of V\{Vi }. If this
assumption does not hold, then we only claim to detect some (but not necessarily all) variables with
changing modules.	□
A3 Proof of Theorem 2
In this section, we derive the generalization bound under the PAC-Bayes framework (McAllester,
1999; Shalev-Shwartz and Ben-David, 2014), and our formulation follows (Pentina and Lampert,
2014) and (Amit and Meir, 2018). We assume that all domains share the sample space Z, hypothesis
space H, and loss function ` : H × Z → [0, 1]. All domains differ in the unknown sample distribution
Ek parameterized by θkmin associated with each domain k. We observe the training sets S1, . . . , Sn
corresponding to n different domains. The number of samples in domain k is denoted by mk .
Each dataset Sk is assumed to be generated from an unknown sample distribution Ekmk . We also
assume that the sample distribution Ek are generated i.i.d. from an unknown domain distribution
T. More specifically, we have Sk = (zι,k,...,zi,k,…，zmk,k), where zi,k =回,"回前.Note
that, Si,k is the i-th state sampled from k-th domain and v*(s*k) is its corresponding optimal
state-value. For any value function h@mn(∙) parameterized by。严，we define the loss function
'(hθmn, zi,k) = Ddist(hθmn(si,k),v*(s*k)), where DdiSt is a distance function that measures the
discrepancy between the learned value and the optimal state-value. We also let P be the prior
distribution over H and Q the posterior over H.
Theorem 1. Let Q be an arbitrary distribution over θkmin and P the prior distribution over θkmin.
Then for any δ ∈ (0, 1], with probability at least 1 - δ, the following inequality holds uniformly for
all Q,
er(Q) ≤ n XX er(Q, Sk)+1XX :2mk-1 (DKL(Q"P)+'+)
+√⅞=
Dkl(Q∖∖P)+log k
δ
where er(Q) and er(Q, Sk) are the generalization error and the training error between the estimated
value and the optimal value, respectively.
Proof. This proof consists of two steps, both using the classical PAC-Bayes bound (McAllester, 1999;
Shalev-Shwartz and Ben-David, 2014). Therefore, we start by restating the classical PCA-Bayes
bound.
18
Published as a conference paper at ICLR 2022
Theorem A1 (Classical PAC-Bayes Bound, General Notations). Let X be a sample space, P (X)
a distribution over X, Θ a hypothesis space. Given a loss function `(θ, X) : Θ × X → [0, 1]
and a collection of M i.i.d random variables (X1, . . . , XM) sampled from P (X), let π be a prior
distribution over hypothesis in Θ. Then, for any δ ∈ (0, 1], the following bound holds uniformly for
all posterior distributions ρ over Θ,
P (Xi〜P(X),θ J'GXi)] ≤ MM XdP做θ,Xm)] + j√-i] (DKL(P11n)+log [)，∀ρ)
≥ - δ.
Between-domain Generalization Bound First, we bound the between-domain generalization, i.e.,
relating er(Q) to er(Q, Ek).
We first expand the generalization error as below,
er(Q) =	E
(E,m)〜T
=E
(E,m)〜T
=E
(E,m)〜T
E
S〜Em
E
S〜Em
EE
θ〜Q	h 〜Q(S,θ)
E `(θ, E)
θ〜Q
SJEmer(Q，E).
E `(h, z)
(A1)
Then we compute the error across the training domains,
1n 1n
—X E E E '(h,z) = - X er(Q,Ek).
n 七Θ〜。 h^Q(Sk,θ) z~Ek '	n 七'
(A2)
Then Theorem A1 Says that for any δo 〜(0,1], We have
P I(Q) ≤ - X er(Q, Ek) + ^^ɪ (DKL(QHP )+lθg δ0)!
≥	-δ0,
(A3)
Where P is a prior distribution over θ.
Within-domain Generalization Bound Then, We bound the the Within-domain generalization,
i.e., relating er(Q, Ek) to er(Q, Sk).
We first have
er(Q, Ek) = E E E `(h,z).	(A4)
θ〜Q h〜Q(Sk ,θ) Z 〜Ek
Then We compute the empirical error across the training domains,
1 mk
er(Q, Sk)=——X E E '(h,zij).	(A5)
mk — h〜Q(Sk ,θ) ZJEk	,j
j=1
According to Theorem A1, for any δ0 〜(0,1], we have
可-------7Γ (DKL(p||n) + log -ξ-k)) ≥ 1 - δk .	(A6)
2(mk -1)	δk
P(r(Q,Ek) ≤ er(Q,Sk) + :
With the choice of π = P (θ)Q(SD, θ)dθ and ρ = Q(θ)Q(SD, θ)dθ, we have that
Dkl(ρ∖∖∏) ≤ Dkl(Q∖∖P) (Yin et al., 2019). Thus, the above inequality can be further written as,
P
er(Q,Ek) ≤ er(Q,Sk) +
j2m⅛ (DKL(Q∖∖p )+log mk
≥	-δk .
(A7)
19
Published as a conference paper at ICLR 2022
Overall Generalization Bound Combining Eq. (A3) and (A7) using the union bound and choosing
that for any δ > 0, set δo = 2 and δk = ɪ for k = 1,...,n, then We finally obtain,
P er(Q) ≤
n X e,r(Q,Sk)+1 X f2m⅛ (DKL(QIP "iog27k)
+√⅞-i
DKL(QIIP) + log ɪ ) I ≥ 1 - δ∙
(A8)
□
A4	More details for model estimation
A4.1 Locating model changes
In real-World scenarios, it is often the case that changes to the environment are sparse and localized.
Instead of assuming every function to change arbitrarily, Which is inefficient and unnecessarily
complex, We first identify possible locations of the changes. To this end, We concatenate data
from different domains and denote by k the variable that takes distinct values 1,…，n to represent
the domain index. Then, We exploit (conditional) independencies/dependencies to locate model
changes. These (conditional) independencies/dependencies can be tested by kernel-based conditional
independence tests (Zhang et al., 2011), Which alloWs for both linear or nonlinear relationships
betWeen variables. BeloW We shoW that in some cases, We can identify the location of θk, by using
the conditional independence relationships from concatenated data.
Proposition A1. In POMDP, where the underlying states are latent, we can localize the changes by
conditional independence relationships from concatenated observed data D in the following cases:
C1 : if ot ⊥ k, then there is neither a change in the observation function nor in the state dynamics
for any state that is an input to the observation function;
C2: if ot ⊥ k and at ⊥6⊥ kIrt+1, then there is only a change in the reward function;
C3: if at ⊥ k Irt+1, then there is neither a change in the reward function nor in the state dynamics
for any state in smin;
C4 : if at ⊥ kIrt+1 and ot ⊥6⊥ k, then there is a change in the observation function, or there exists
a state that is not in smin but is an input to the observation function, whose dynamics has a
change.
Proof. We first formulate the problem as folloWs. We concatenate data from different domains and
use domain-index variable k to indicate Whether the corresponding distribution module has changes
across domains. Specifically, by assuming the Markov condition and faithfulness assumption, si,t
has an edge with k if and only if p(si,t∣PA(si,t)) changes across domains, where PA(∙) denotes its
parents. Similarly, rt has an edge With k if and only ifp(rtIPA(rt)) changes across domains, and ot
has an edge with k if and only ifp(otIPA(ot)) changes across domains. Under this setting, locating
changes is equivalent to identify which variables have an edge with k from the data.
We consider the scenario of POMDP, where we only observe {ot , rt , at } and the underlying states st
are not observed. Below, we consider each case separately.
Case 1:	Show that if ot ⊥ k, then there is neither a change in the observation function nor a change
in the state dynamics for any state that is an input to the observation function.
We prove it by contradiction. Suppose that there is a change in the observation function and a change
in the state dynamics for any state that is an input to the observation function. That is, ot has an edge
with k, and si,t that has a direct edge to ot also connects with k. Based on faithfulness assumption,
ot ⊥6⊥ k, which contradicts to the assumption. Since we have a contradiction, it must be that there is
neither a change in the observation function nor a change in the state dynamics for any state that is an
input to the observation function.
Case 2:	Show that if ot ⊥ k and at ⊥6⊥ kIrt+1, then there is only a change in the reward function.
20
Published as a conference paper at ICLR 2022
If ot ⊥ k and at-1 ⊥6⊥ k|rt, based on the Markov condition and faithfulness assumption, rt has an
edge with k, and si,t and ot do not have edges with k; that is, there are only changes in the reward
function.
Case 3:	Show that if at ⊥ k|rt+1, then there is neither a change in the reward function nor a change
in the state dynamics for any state in smin .
By contradiction, suppose that there is a change in the reward function or there exists a state
sj,t ∈ sjmin that has a change in its dynamics. That is, rt has an edge with k or corresponding sj,t has
an edge with k. Based on faithfulness assumption, at ⊥6⊥ k|rt+1 , which contradicts to the assumption.
Case 4:	Show that if at ⊥ k|rt+1 and ot ⊥6⊥ k, then there is a change in the observation function, or
there exists a state that is not in smin but is an input to the observation function, whose dynamics has
a change.
According to Case 3, if at ⊥ k|rt+1, then there is neither a change in the reward function nor a
change in the state dynamics for any state in smin . Furthermore, since ot ⊥6⊥ k, then based on the
Markov condition, either ot has an edge with k, or there exists a state that is not in smin but is an
input to the observation function, whose dynamics has an edge with k . That is, there is a change in
the observation function, or there exists a state that is not in smin but is an input to the observation
function, whose dynamics has a change.
□
Based on Theorem 1 and Proposition A1, in MDP, we can fully determine where the changes are, so
We only need to consider the corresponding θR to capture the changes. In POMDR in Case 1, We
only need to involve θks and θkr in model estimation, that is, θk = {θks , θkr}; in Case 2, θk = {θkr}; and
in Case 3 & 4, θk = {θko, θks}. For other cases, We involve θk = {θko, θks , θkr} in model estimation.
A4.2 More details for estimation of domain-varying models in one step
We use MiSS-VAE to learn the environment model, Which contains three components: the "sequential
VAE" component, the "multi-model" component, and the "structure" component. Figure A3 gives the
diagram of neural netWork architecture in model training.
Specifically, for the "sequential VAE" component, We include a Long Short-Term Memory (LSTM
(Hochreiter and Schmidhuber, 1997)) to encode the sequential information With output ht and a
Mixture Density NetWork (MDN (Bishop, 1994)) to output the parameters of MoGs, and thus to
learn the inference model q°(st,k∣st-ι,k, yi：t,k, ai：t—i,k； θk) and infer a sample of s^ from q° as
the output. The generated sample further acts as an input to the decoder, and the decoder outputs
^t+ι and rt+2. Moreover, the state dynamics which satisfies a Markov process is modeled with an
MLP and MDN.
For the "multi-model" component, we include the domain index k as an input to LSTM and involve
θk as free parameters in the inference model qφ , by assuming that θk also characterizes the changes
in the inference model. Moreover, we embed θks in state dynamics pγ , θko in observation function and
θkr in reward function in the decoder. With such a characterization, except θk , all other parameters are
shared across domains, so that all we need to update in the target domain is the low-dimensional θk,
which greatly improves the sample efficiency and the statistical efficiency in the target domain-usually
few samples are needed.
For the "structure" component, the latent states are organized with structures, captured by the mask
Cs今s. Also, the structural relationships among perceived signals, latent states, the action variable, the
reward variable, and the domain-specific factors are embedded as free parameters (structural vectors
and scalars Cs>s, CaTs, cθk-, cs", and Ca+r) into MiSS-VAE.
A5 Complete experimental results
In this section we provide the complete experimental results on both of our settings, modified Cartpole
and modified Pong in the OpenAI Gym (Brockman et al., 2016). In the POMDP setting, we use
images as input, which for Cartpole look like Fig. A4(a) and for Pong look like Fig. A8(a). For
21
Published as a conference paper at ICLR 2022
（眼,既）
O1+2,∕+3
Ot+1,ft+2
Figure A3: Diagram of MiSS-VAE neural network architecture. The "sequential VAE" compo-
nent, "multi-model" component, and "structure" component are marked with black, red, and blue,
respectively.
Cartpole, we also consider the MDP case, where the true states (cart position and velocity, pole angle
and angular velocity) are used as the input to the model.
We consider changes in the state dynamics (e.g., the change of gravity or cart mass in Cartpole, or the
change of orientation in Atari), changes in perceived signals (e.g., different noise levels on observed
images in Cartpole, as shown in Fig. A4 or colors in Pong) and changes in reward functions (e.g.,
different reward functions in Pong based on the contact point of the ball), as shown in Fig. A8 for
Pong. For each of these factors, we take into account both interpolation (where the factor value in the
target domain is in the support of that in source domains), and extrapolation (where it is out of the
support w.r.t. the source domains).
We train on n source domains based on the trajectory data generated by a random policy. In Cartpole
experiments, for each domain we collect 10000 trials with 40 steps. For Pong experiments, each
domain contains 40 episodes data and each of them takes a maximum of 10000 steps.
A5.1 Complete results of modified Cartpole experiment
The Cartpole problem consists of a cart and a vertical pendulum attached to the cart using a passive
pivot joint. The cart can move left or right. The task is to prevent the vertical pendulum from falling
by putting a force on the cart to move it left or right. The action space consists of two actions: moving
left or right.
We introduce two change factors for the state dynamics θks: varying gravity and varying mass of the
cart, and a change factor in the observation function θko that is the image noise level. Fig. A4 gives a
visual example of Cartpole game, and the image with Gaussian noise. The images of the varying
gravity and mass look exactly like the original image. Specifically, in the gravity case, we consider
source domains with gravity g = {5, 10, 20, 30, 40}. We take into account both interpolation (where
the gravity in the target domain is in the support of that in source domains) with g = {15}, and
extrapolation (where it is out of the support w.r.t. the source domains) with g = {55}. Similarly,
we consider source domains where the mass of the cart is m = {0.5, 1.5, 2.5, 3.5, 4.5}, while in
target domains it is m = {1.0, 5.5}. In terms of changes on the observation function θko, we add
22
Published as a conference paper at ICLR 2022
Figure A4: Visual examples of Cartpole game and change factors. (a) Cartpole game; (b) Modified
Cartpole game with Gaussian noise on the image. The light blue arrows are added to show the
direction in which the agent can move.
Gravity	Mass
Figure A5: The estimated θmin for the three change factors in Cartpole (POMDP): gravity (A), mass
(B), and noise level (C) for Ntarget = 50..
Gaussian noise on the images with variance σ = {0.25, 0.75,1.25,1.75, 2.25} in source domains,
and σ = {0.5, 2.75} in target domains.
We summarize the detailed settings in both source and target domains in Table A1. In particular in
each experiment we use all source domains for each change factor and one of the target domains at a
time in either the interpolation and extrapolation set.
	Gravity	Mass	Noise
Source domains	{5,10, 20, 30,40}	{0.5,1.5, 2.5, 3.5,4.5}	{0.25, 0.75,1.25,1.75, 2.25}
Interpolation set	{15}一	{1.0}	{0.5}
Extrapolation set	{55}一	{5.5}	{2.75}
Table A1: The settings of source and target domains for modified Cartpole experiments.
A5.1.1 LEARNED θkIN CARTPOLE EXPERIMENTS
Fig. A5 shows the estimated θk in the modified Cartpole experiments. For the gravity and mass
scenarios, the learned parameters with different sample sizes are close with each other. This phe-
nomenon indicates that even with only a few samples (Ntarget = 50), AdaRL can estimate these
change parameters very well. For the noise level factor, the learned curves with different sample sizes
have a similar behaviour, but the distance is larger. We can see that the θk we learn is approximately
a linear function of the actual perturbation in gravity, while for the mass and noise are monotonic
functions.
A5.1.2 AVERAGE FINAL SCORES FOR MULTIPLE Ntarget IN CARTPOLE EXPERIMENTS
Tables A2 and A3 show the complete results of the modified Cartpole experiments (POMDP settings)
for Ntarget = 20 and Ntarget = 10000. Table A4 and A5 give the complete results of the modified
23
Published as a conference paper at ICLR 2022
	Oracle Upper bound	Non-t lower bound	PNN (Rusu et al., 2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours w/o masks	AdaRL Ours
G_in	1930.5 (±1042.6)	828.5 • (±509.4)	1113.4 • (±719.2)	1008.5 • (±453.6)	1257.2 (±503.5)	1290.6 (±589.1)	1302.7 (±874.0)
G_out	408.6 (土67.2)	54.0 • (±13.6)	109.7 • (±24.2)	156.8 • (±49.5)	120.5 • (±87.4)	173.8 (±39.3)	198.4 (±54.5)
M_in	2004.8 (±404.3)	447.2 • (±39.6)	1120.6 • (±348.1)	982.5 • (±363.2)	1245.7 • (±274.0)	1095.8 • (±521.3)	1361.0 (±327.3)
M_out	1498.6 (±625.4)	130.6 • (±39.8)	528.5 • (±251.4)	830.6 • (±317.2)	875.2 • (±262.5)	764.2 • (±320.9)	1082.5 (±236.3)
N_in	8640.5 (±3086.1)	679.4 • (±283.5)	4170.6 • (±2202.2)	4936.5 • (±1604.9)	3985.7 • (±2387.4)	4954.3 • (±2627.8)	5761.2 (±2341.5)
N_out	4465.2 (±667.3)	584.0 • (±429.2)	2841.5 • (±385.2)	2650.2 • (±453.6)	2654.0 • (±277.9)	1785.2 • (±470.3)	3318.7 (±293.5)
Table A2: Average final scores in modified Cartpole (POMDP) with Ntarget = 20. The best
non-oracle results are marked in red. G, M, and N denote the gravity, mass, and noise respectively.
	Oracle Upper bound	Non-t lower bound	PNN (Rusu et al., 2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours w/o masks	AdaRL Ours
G_in	1930.5 (±1042.6)	1115.2 • (±341.8)	1637.4 • (±378.2)	1838.4 (±358.1)	1459.2 • (±688.5)	1864.5 (±694.1)	1924.6 (±874.0)
G_out	408.6 (±67.2)	161.3 • (±65.9)	329.6 • (±48.9)	457.3 (±138.5)	393.2 • (±76.5)	384.2 • (±103.7)	410.6 (±92.3)
M_in	2004.8 (±404.3)	596.0• (±373.4)	1672.3 • (±642.9)	1798.5 • (±493.0)	1905.4 (±378.2)	1864.2 (±309.5)	1898.5 (±683.4)
M_out	1498.6 (±625.4)	325.6 • (±146.3)	1206.8 • (±394.7)	1339.4 • (±520.5)	1296.2 • (±773.1)	1297.4 • (±411.2)	1486.3 (±598.2)
N_in	8640.5 (±3086.1)	1239.6 • (±380.5)	6476.2 • (±3132.9)	7493.4 • (±1981.5)	7932.9 (±2389.0)	7382.4 • (±2915.3)	8179.8 (±2356.0)
N_out	4465.2 (±667.3)	962.5 • (±341.8)	3043.9 • (±1098.6)	2987.2 • (±1172.3)	3892.4 • (±763.0)	4183.6 (±782.2)	4235.2 (±532.4)
Table A3: Average final scores in modified Cartpole (POMDP) with Ntarget = 10000. The best
non-oracle results are marked in red. G, M, and N denote the gravity, mass, and noise respectively.
	Oracle Upper bound	Non-t lower bound	CAVIA (Zintgraf et al., 2019)	PEARL (Rakelly et al., 2019)	AdaRL* Ours w/o masks	AdaRL Ours
G_in	2486.1 (±369.7)	~972.6 • (±368.5)	1651.5 • (±623.8)	1720.3 • (±589.4)	1602.7 • (±393.6)	1943.2 (±765.4)
G_out	693.9 (±100.6)	~243.8 • (±45.2)	356.2 (±76.5)	362.1 (±57.3)	292.4 • (±91.8)	395.6 (±101.7)
M_in	2678.2 (±630.5)	~480.3 • (±136.2)	1306.8 • (±376.5)	1589.4 • (±682.3)	1624.8 • (±531.6)	1962.0 (±652.8)
M_out	1405.6 (±368.0)	~306.5 • (±162.4)	853.2 • (±317.6)	969.4 • (±238.5)	984.6 • (±209.8)	1113.5 (±394.2)
G_in & M_in	1984.2 (±871.3)	374.9 • (±126.8)	1174.3 • (±298.2)	964.3 • (±370.5)	1209.6 • (±425.7)	1392.7 (±392.6)
G_out & M_out	939.4 (±270.5)	~292.4 • (±127.6)	494.6 • (±201.3)	368.4 • (±259.8)	399.8 • (±242.5)	531.2 (±272.5)
Table A4: Average final scores in modified Cartpole (MDP) with Ntarget = 20. The best non-oracle
results are marked in red, while bold indicates a statistically significant result w.r.t. all the baselines.
G, M, and N denote the gravity, mass, and noise respectively. "•" indicates the baselines for which the
improvements of AdaRL are statistically significant (via Wilcoxon signed-rank test at 5% significance
level).
24
Published as a conference paper at ICLR 2022
	oracle Upper bound	Non-t lower bound	CAviA (Zintgraf et al., 2019)	PEARL (Rakelly et al., 2019)	AdaRL* ours w/o masks	AdaRL ours
G_in	2486.1 (±369.7)	~986.3 • (±392.5)	1907.4 • (±526.8)	2102.3 • (±398.5)	1864.0 • (±369.2)	2365.1 (±403.5)
G_out	693.9 (±100.6)	349.2 • (±72.0)	502.9 • (±133.2)	585.7 • (±98.6)	494.7 • (±151.4)	604.8 (±117.6)
M_in	2678.2 (±630.5)	643.9 • (±281.3)	2008.6 • (±436.2)	2106.2 • (±436.7)	2148.9 • (±387.2)	2415.2 (±591.4)
M_out	1405.6 (±368.0)	~617.4 • (±145.3)	1182.7 • (±255.8)	1294.5 (±210.6)	1207.5 (±251.3)	1263.5 (±362.9)
G_in & M_in	1984.2 (±871.3)	~452.6 • (±178.3)	1275.0 • (±432.5)	1468.7 • (±697.2)	1395.4 • (±387.2)	1589.4 (±379.5)
G_out & M_out	939.4 (±270.5)	596.2 • (±137.5)	709.5 • (±386.0)	743.8 • (±200.9)	724.7 (±283.8)	769.3 (±208.4)
Table A5: Average final scores in modified Cartpole (MDP) with Ntarget = 10000. The best non-
oracle results are marked in red, while bold indicates a statistically significant result w.r.t. all the
baselines. G, M, and N denote the gravity, mass, and noise respectively.
Cartpole experiments (MDP settings with symbolic input) for Ntarget = 20 and Ntarget = 10000.
We average the scores across 30 trials from different random seeds during the policy learning stage.
The results suggest that, in most cases, AdaRL can outperform other baselines.
A5.1.3 Average policy learning curves in terms of steps
sojoɔs
Episodes
—AdaRL 一— Oracle + PNN * PSM + MTQ + Non-t
Figure A6: Learning curves for modified Cartpole experiments (POMDP version) with change factors.
The reported scores are averaged across 30 trials.
Fig. A7 and A6 provide the learning curves for modified Cartpole experiments (MDP and POMDP
versions) with multiple change factors. In most cases, AdaRL can converge faster than other baselines.
25
Published as a conference paper at ICLR 2022
Gravity: 15
Gravity: 55
Mass: 1.0
Gravity: 15; Mass: 1.0
3000
2500
2000
1500
1000
500
0
100	200	300	400	500
Gravity: 55; Mass: 5.5
Episodes
. AdaRL-------- Oracle -C- CAVIA pE PEARL ψ Non-t
Figure A7: Learning curves for modified Cartpole experiments (MDP version) with change factors.
The reported scores are averaged across 30 trials.
Figure A8: Visual example of the original Pong game and the various change factors. The light blue
arrows are added to show the direction in which the agent can move.
A5.2 Complete results of the modified PONG experiment with changing
DYNAMICS AND OBSERVATIONS
Atari Pong is a two-dimensional game that simulates table tennis. The agent controls a paddle moving
up and down vertically, aiming at hitting the ball. The goal for the agent is to reach higher scores,
which are earned when the other agent (hard-coded) fails to hit back the ball. We show the example
of the original visual inputs and how it appears after we have changed each of the change factors in
Fig. A8.
In source domains, the degrees are chosen from ω = {0°, 180°}, and in target domains, they are
chosen from ω = {90°, 270°}. For the image size, We reduce the original image by a factor of
{2, 4, 6, 8} in source domains and by a factor of {3, 9} in target domains.
We summarize the detailed settings in both source and target domains in Table A6. in particular, in
each experiment we use all source domains for each change factor and one of the target domains at a
time in either the interpolation and extrapolation set.
26
Published as a conference paper at ICLR 2022
07
6
Size
(a)	Source vs. target domains for Ntarget = 10000.
(b)	Different Ntarget in target domains.
Figure A9: Learned θθ for the two change factors, size and noise, in modified Pong.
	Size	Orientations	Noise	Background colors
Source domains	{2,46 8}	-0°,180°-	{0.25, 0.75,1.25,1.75, 2.25}	original, green, red
Interpolation set	{3}	90s	{1.0}	yellow
Extrapolation set	{9}	270s	{2.75}	white
Table A6: The settings of source and target domains for modified Pong experiments.
A5.2.1 LEARNED θk IN MODIFIED PONG EXPERIMENTS
Fig. A9a, Table A7, and Table A8 show the learned θk in modified Pong experiments across different
change factors.
Table A7: The learned θks across different orientation angles with different Ntarget in modified Pong.
The bold columns represent the target domains.
Ntarget	Orientations	0。	90。	180。	270。
10000	θs1	-2.32	-1.78	1.69	0.44
	θs2	-2.94	-1.86	1.47	1.59
50	θs1	-2.01	-0.87	1.85	0.69
	θs2	-2.59	-1.84	1.42	1.07
20	θs1	-1.69	-1.23	0.79	1.38
	θs2	-1.98	-0.56	0.82	1.20
We can find that each dimension of the learned θsk is a nonlinear monotonic function of the change
factors. Table A7, Table A8 and Fig. A9b also give the learned θk with different sample sizes Ntarget
in target domains. Similarly, the learned curves with different sample sizes are homologous. Even
with a few samples, AdaRL can still capture the model changes well.
27
Published as a conference paper at ICLR 2022
Table A8: The learned θko across different colors with different Ntarget in modified Pong. The bold
columns represent the target domains.
Ntarget	Colors	Original	Red	Green	Yellow	White
	θko1	-136-	1.47	1.04	1.58	-0.91
10000	θko2	0.72	-1.15	1.17	0.96	-1.33
	θko3	0.93	-1.28	-1.31	-0.65	-1.09
	θko1	-096-	1.13	0.82	1.26	-0.73
50	θko2	0.59	-0.46	0.75	1.32	-0.59
	θko3	0.61	-1.02	-0.91	-0.18	-0.49
	θko1	-109-	1.38	0.65	1.30	-0.46
20	θko2	0.58	-0.72	0.39	1.60	-0.27
	θko3	0.38	-0.59	-0.63	-0.24	-0.33
A5.2.2 AVERAGE FINAL SCORES FOR MULTIPLE Ntarget
Table A2 and A3 provides the complete results of the modified Pong experiments with Ntarget = 20
and 10000, respectively. The details of both source and target domains are listed in Table A6. Similar
to the results of Cartpole, AdaRL can perform the best among all baselines in Pong experiments. As
shown in the results of the main paper, AdaRL consistently outperforms the other methods.
A5.2.3 Average policy learning curves in terms of steps
Fig. A12 gives the learning curves for modified Pong experiments with multiple change factors. From
the results, we can find that AdaRL can converge faster than other baselines.
A5.3 Complete results of the modified Pong experiment with changing reward
FUNCTIONS
Table A9 summarizes the detailed change factors in both linear and non-linear reward groups.
	Linear reward (kι)	Non-linear reward (k2)
Source domains	{0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8}	{2.0, 3.0, 5.0, 6.0,7.0, 8.0, 9.0}
Interpolation set	{0.5}	{4.0}
Extrapolation set	{0.9}	{1.0}
Table A9: The settings of source and target domains for modified Pong experiments.
We denote with L the half-length of the paddle and then formulate the two groups of reward functions
as: (1) Linear reward functions: r = kLd ,where kι ∈ {0.1,0.2,0.3,0.4,0.6,0.7,0.8} in source
domains and k1 ∈ {0.5, 0.9} in target domains; and (2) Non-linear reward functions: rt
k2L
d+3L ,
where k2 ∈ {2.0, 3.0, 5.0, 6.0, 7.0, 8.0, 9.0} in source domains and k2 ∈ {1.0, 4.0} in target domains.
A5.3.1 LEARNED θr IN MODIFIED PONG EXPERIMENTS
Fig. A10 and A11 give the learned θr with both linear and non-linear rewards. In both groups, the
learned θr is linearly or monotonically correlated with the change factor k1 and there is no significant
gap between the learned θr with different Ntarget .
A5.3.2 AVERAGE FINAL SCORES FOR MULTIPLE Ntarget
Table A12, A13 and A14 shows the average final scores for Ntarget = 50, 10000 and 50000 in
modified Pong experiments with changing rewards. AdaRL consistently outperforms the other
methods across different Ntarget .
28
Published as a conference paper at ICLR 2022
(a) Learnt θr in each domain.
(b) Different Ntarget in target domains.
Figure A10: Learned θ^ for the linear changing rewards in modified Pong.
k2	k2
(a) Learnt θr in each domain.
k 2	k 2
(b) Different Ntarget in target domains.
Figure A11: Learned θ^ for the non-linear changing rewards in modified Pong.
A5.3.3 Average policy learning curves in terms of steps
Fig. A12 (last two rows) gives the learning curves for modified Pong experiments with changing
rewards.
A5.4 RESULTS ON MUJOCO BENCHMARKS
We also apply AdaRL on MuJoCo benchmarks (Todorov et al., 2012), including Cheetah and Ant
with a set of target velocities. We follow the setup in MAML (Finn et al., 2017) and CAVIA (Zintgraf
et al., 2019). In model estimation stage, we choose 20 tasks for each of the game. The goal velocity of
each task is sampled between 0.0 and 2.0 for the cheetah and between 0.0 and 3.0 for ant. The reward
is the negative absolute value between agent,s current and the goal velocity. In policy optimization
stage, we utilize Trust Region Policy Optimization (TRPO Schulman et al. (2015)). The results on
29
Published as a conference paper at ICLR 2022
	Oracle Upper bound	Non-t lower bound	PNN (Rusu et al., 2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours w/o masks	AdaRL Ours
O_in	18.65 (±2.43)	4.30 • (±2.95)	8.68 • (±5.78)	9.65 • (±3.19)	14.80 • (±2.02)	15.08 • (±3.19)	16.79 (±1.84)
O_out	19.86 (±1.09)	5.09 • (±2.41)	10.61 • (±5.26)	9.94 • (±6.23)	11.82 (±2.46)	11.92 (±3.09)	12.70 (±4.38)
C_in	19.35 (±0.45)	7.72 • (±2.63)	13.75 • (±4.16)	10.87 • (±5.15)	14.80 • (±3.07)	16.07 (±2.86)	16.29 (±3.35)
C_out	19.78 (±0.25)	7.09 • (±3.21)	13.37 • (±4.42)	12.59 • ±3.80)	15.34 (±3.22)	15.84 (±3.10)	16.55 (±2.09)
S_in	18.32 (±1.18)	6.25 • (±3.42)	12.93 • (±2.72)	10.67 • (±1.85)	12.78 • (±3.46)	13.86 (±2.95)	14.92 (±4.48)
S_out	19.01 (±1.04)	5.45 • (±2.75)	9.69 • (±6.27)	13.80 • (±3.15)	12.62 • (±2.41)	15.31 (±2.13)	15.88 (±3.72)
N_in	18.48 (±1.25)	4.29 • (±2.22)	13.85 • (2.83)	13.69 • (±2.21)	10.96 • (±3.27)	13.51 • (±3.07)	15.57 (±2.95)
N_out	18.26 (±1.11)	5.19 • (±2.47)	11.83 • (±3.82)	14.07 • (±2.56)	12.75 • (±3.18)	14.29 • (±3.10)	16.38 (±2.72)
Table A10: Average final scores on modified Pong (POMDP) with Ntargets = 20. The best non-
oracle results are marked in red, while bold indicates a statistically significant result w.r.t. all the
baselines. O, C, S, and N denote the orientation, color, size, and noise factors, respectively.
	Oracle Upper bound	Non-t lower bound	PNN (Rusu et al., 2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours w/o masks	AdaRL Ours
O_in	18.65	8.04 •	12.19 •	12.37 •	14.64 •	17.42 •	18.85
	(±2.43)	(±1.78)	(±3.07)	(±2.92)	(±3.01)	(±2.20)	(±1.63)
O_out	19.86	6.97 •	16.48	15.79 •	12.75 •	17.25	17.93
	(±1.09)	(±1.88)	(±3.10)	(±2.29)	(±4.93)	(±1.85)	(±2.41)
C_in	19.35	8.09 •	15.89 •	16.70 •	17.85	17.73 •	18.93
	(±0.45)	(±3.11)	(±3.49)	(±2.38)	(±2.16)	(±2.01)	(±1.37)
C_out	19.78	7.48 •	16.85 •	16.29 •	17.93 •	18.49	19.28
	(±0.25)	(±2.09)	(±3.17)	(±2.64)	(±2.35)	(±2.04)	(±1.36)
S_in	18.32	7.45 •	12.89 •	13.84 •	15.33 •	15.79 •	17.49
	(±1.18)	(±3.15)	(±2.04)	(±3.27)	(±2.03)	(±2.62)	(±2.18)
S_out	19.01	7.04 •	14.69 •	17.25 •	18.48	17.82 •	19.21
	(±1.04)	(±2.36)	(±2.03)	(±2.30)	(±1.36)	(±1.98)	(±0.63)
N_in	18.48	6.82 •	13.84 •	16.80 •	17.58	15.93 •	18.25
	(±1.25)	(±2.09)	(±2.82)	(±1.73)	(±2.19)	(±3.68)	(±1.81)
N_out	18.26	7.82 •	14.89 •	16.85	17.03	16.49 •	17.85
	(±1.11)	(±2.46)	(±2.98)	(±3.94)	(±2.36)	(±3.25)	(±2.16)
Table A11: Average final scores on modified Pong (POMDP) with Ntargets = 10000. The best
non-oracle results are marked in red, while bold indicates a statistically significant result w.r.t. all the
baselines. O, C, S, and N denote the orientation, color, size, and noise factors, respectively.
	Oracle Upper bound	Non-t lower bound	PNN (Rusu et al., 2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours w/o masks	AdaRL Ours
Rl_in	7.98	3.19 •	4.95	5.04	4.78	3.49 •	5.81
	(±3.81)	(±2.27)	(±1.08)	(±2.11)	(±2.10)	(±1.97)	(±2.06)
Rl_out	9.61	5.19 •	5.89	6.03	6.21	5.64	6.12
	(±4.78)	(±2.80)	(±1.93)	(±2.71)	(±3.14)	(±2.59)	(±3.45)
Rn_in	7.62	2.85 •	5.31	5.06	5.52	5.79	5.84
	(±2.16)	(±1.71)	(±2.78)	(±3.89)	(±3.47)	(±3.03)	(±3.17)
Rn_out	41.36	21.73 •	27.19	23.27 •	25.49 •	26.33 •	29.92
	(±5.70)	(±8.54)	(±5.82)	(±8.01)	(±6.18)	(±7.94)	(±6.39)
Table A12: Results on modified Pong game with Ntargets = 50. The best non-oracle results are
marked in red, while bold indicates a statistically significant result w.r.t. all the baselines. Rl and Ro
denote the linear reward and nonlinear reward-changing cases, respectively.
30
Published as a conference paper at ICLR 2022
SalOOS
Orientation: 90°
0	500	1000	1500	2000	2500
Episodes
.AdaRL
Oracle T- PNN T- PSM T- MTQ Non-t
Figure A12: Learning curves for modified Pong experiments with change factors. The reported scores
are averaged across 3θ trials.
	Oracle Upper bound	Non-t lower bound	PNN (Rusu etal.,2016)	PSM (Agarwal et al., 2021a)	MTQ (Fakooretal.,2020)	AdaRL* Ours w/o masks	AdaRL Ours
Rl_in	7.98	4.65 •-	—5^-	6.45 •	6.62 •	6.88 •	7.69
	(±3.81)	(±1.70)	(±1.98)	(±1.82)	(±2.45)	(±3.19)	(±2.04)
Rl_out	9.61	5.82 •	6.15 •	7.30 •	8.42	7.04 •	8.41
	(±4.78)	(±2.01)	(±2.79)	(±1.98)	(±2.14)	(±2.52)	(±2.36)
Rn_in	7.62	3.13 •	5.68 •	6.42	6.30	5.52 •	6.57
	(±2.16)	(±2.47)	(±1.42)	(±3.31)	(±3.19)	(±1.09)	(±1.24)
Rn_out	41.36	27.70 •	31.28 •	33.60 •	29.77 •	33.83 •	36.52
	(±5.70)	(±3.45)	(±4.09)	(±5.52)	(±3.85)	(±5.02)	(±4.18)
Table A13: Average final scores on modified Pong (POMDP) with Ntarget = 10000. The best
non-oracle results are marked in red, while bold indicates a statistically significant result w.r.t. all the
baselines. Rl and Ro denote the linear and nonlinear reward changes, respectively.
the target domains are specified in Table A15, suggesting that AdaRL can achieve better performance
than the meta-learning approaches (i.e., MAML and CAVIA).
31
Published as a conference paper at ICLR 2022
	Oracle Upper bound	Non-t loWer bound	PNN (Rusu et al., 2016)	PSM (AgarWal et al., 2021a)	MTQ (Fakoor et al., 2020)	AdaRL* Ours W/o masks	AdaRL Ours
Rl_in	7.98	4.81 •	5.94 •	6.90	7.34	6.46 •	7.93
	(±3.81)	(±2.03)	(±4.87)	(±3.47)	(±3.18)	(±3.12)	(±2.09)
Rl_out	9.61	3.89 •	7.85	7.37	8.77	7.78	8.94
	(±4.78)	(±2.16)	(±2.88)	(±3.75)	(±2.61)	(±3.10)	(±2.02)
Rn_in	7.62	3.58 •	6.91	7.01	6.28 •	6.30 •	7.57
	(±2.16)	(±1.09)	(±2.85)	(±2.46)	(±3.14)	(±2.63)	(±1.94)
Rn_out	41.36	29.98 •	36.08 •	37.26	38.48	34.19 •	41.25
	(±5.70)	(±3.02)	(±10.35)	(±11.25)	(±12.59)	(±9.36)	(±6.92)
Table A14: Results on modified Pong game with Ntargets = 50000. The best non-oracle results are
marked in red, while bold indicates a statistically significant result w.r.t. all the baselines. Rl and Ro
denote the linear reward and nonlinear reward-changing cases, respectively.
	MAML (Finn et al., 2017)	CAVIA (Zintgraf et al., 2019)	AdaRL Ours
Cheetah (vel)	-89. 8	-86.5	-81.7
	(±4.1)	(±2.0)	(±3.2)
Ant (vel)	100.4	95.7	106.8
	(±10.9)	(±6.92)	(±8.4)
Table A15: Results on MuJoCo benchmarks (Cheetah and Ant experiments with different target
velocities, with 30 trials each) with Ntargets = 50, 000. The best results are marked in red.
A5.5 Effect of the policy used for data collection during model estimation
In our framework, we use the random policy to generate trajectories for each domain. The generated
trajectories are further used to conduct the model estimation. To study whether the random policy
will affect the effectiveness of model estimation, we compare the learnt parameters of latent space in
model estimation with the trajectories generated via (1) the random policy in our framework, and
(2) the optimal policies learnt on the source domains. Here we show the case with changing gravity
in the modified Cartpole game. In Fig. A13, We give the learnt parameters (μ and log σ) of the first
and second components in the Gaussian mixtures of the 10-th latent state at 20-th epoch. The results
demonstrate that the difference betWeen the tWo sets of learnt parameters is limited.
A5.6 More statistical evaluation protocols on the performance
In this section, We provide a more detailed and comprehensive comparison on the performances of
AdaRL and other baseline methods. FolloWing the recent published Work on reliable evaluation for
RL (AgarWal et al., 2021b), We utilize 4 evaluation metrics: median, interquartile mean (IQM), mean,
and optimality gap. Median and mean are the sample median and mean, respectively. IQM discards
the top and bottom 25% samples and then computes the mean value of the remaining ones. Thus, this
factor is insensitive to outliers. Optimality gap is quantified via the gap betWeen the performance of
each method and the mean score obtained by the oracle agent. Therefore, higher mean, median and
IQM and loWer optimality are the indications of better methods. All metrics are With 95% bootstrap
confidence intervals (CIs) (Efron, 1992).
Fig. A14, A15, A16, and A17 give the comparison on AdaRL and baselines using the set of evaluation
protocols in different games and settings. The results suggest that in most cases, AdaRL performs
consistently better than the baselines across all evaluation metrics.
32
Published as a conference paper at ICLR 2022
Learnt mean (μ)
log 3:
Optimal policy
Random policy
Learnt std (log σ)
log σ


Figure A13: Learnt μ and log σ in model estimation with both random policy (left), and optimal
policy (right).
33
Published as a conference paper at ICLR 2022

GJn
Median	IQM	Mean	Optimality Gap
PEARL
CAVIA
AdaRL*
AdaRL
1500 1750 2000—	1500 1750 2000—	1500 1750 2000- -600^^800 IOOO 1200
Median
PEARL	■
CAVIA
AdaRL*
AdaRL
1750 2000 2250 2500—1750 2000 2250
Mean	Optimality Gap
1750 2000 2250 2500 400 600 800 1000
Median
PEARL
CAVIA
AdaRL*
AdaRL
^750	9001050
Optlmallty Gap
450	600	750
Median
PEARL
CAVIA
AdaRL*
AdaRL
-900~1200 1500
Optimality GaP
500 750 1000 1250
160 240 320 400
Optimality Gap
Figure A14: Evaluation on the results of modified CartPole game under MDP settings for Ntarget
50.
34
Published as a conference paper at ICLR 2022
Median
GJn
Mean
PNN
MTQ
AdaRL*
AdaRL
1200 1600 2000
1200 1600 2000
1200 1600 2000
Optimality Gap
400 600 800 1000
Median
PSM
PNN
MTQ
AdaRL*
AdaRL
280 320 360
40	80	120	160
Optimality G 叩
Median
M_in
IQM
Mean
PSM
PNN
MTQ
AdaRL*
AdaRL

1600 1800 2000
1400 1600 1800 2000	1600 1800 2000
Optimality Gap
150 300 450 600
Median
PSM
PNN
Optimality Gap
900 1200 1500 1800 200	400	600
900 1200 1500 1800 800	1200 1600
N in
Median
AdaRL*
AdaRL
Mean
4000 6000 8000
4000 6000 8000
4000 6000 8000
Optimality Gap
1500 3000 4500
Median
Nout
PSM
PNN I
Mean
MTQ
AdaRL*
AdaRL
1600 2400 3200 4000
2000 3000 4000
1600 2400 3200 4000
Optimality Gap
800 1600 2400 3200

Figure A15: Evaluation on the results of modified CartPole game under POMDP settings for
Ntarget = 50.
35
Published as a conference paper at ICLR 2022
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
MTQ
AdaRL*
AdaRL
PSM
PNN
I4ΓQ
AdaRL*
AdaRL
Median
IQM
9	12~15~18^
Median
l∑
O in
Mean
Optimality Gap
^16^
20 9	12~~15~~18^
O out
Mean
3^
6^
^9^
Optimality Gap
10.0 12.5 15.0 17.5-10.0 12.5 15.0
10.0 12.5 15.0 17.5 5.0—73—10.0
Median
15.0 16.5 18.0 19.5-14^
Median
^16
IQM
IT
^16^
^18^
^14^
^16^
l∑
Median
^16^
Median
8	12	16^
Median
10.0 12.5~15ɪ0~LΛ5
Median
PSM
PNN
MTQ
AdaRL*
AdaRL
1∑5~15^0^^l7T
C in
Mean
Optimality Gap
^18^
C out
Iβ^
S in
15.0 16.5 18.0 19.5	L5~ɪθ~43^
Mean
Optlmallty Gap
IT
16^
^18^
2^
T
^6
IQM
Mean
Optimality Gap
12	14	16^
S out
8	12~~16^
N in
l∑
^16^

^8
Mean
Optimality Gap
8	12	16^
T
ɪ
^12^
IQM
Mean
Optimality Gap
10.0 12.5 15.0 17.510~I∑5~15^0~17?5
N_OUt
IQM
Mean
2^
^6
Optimality Gap
ι∑5^^IΓδ^^ITT
36
1∑5^^15Λ~173
T
T
~6
8
Figure A16: Evaluation on the results of modified Atari Pong game under POMDP settings with
changing orientation, color, size, and noise levels for Ntarget = 50.
Published as a conference paper at ICLR 2022
Median
Mean
Optimality Gap
AdaRL
AdaRL*
AdaRL
AdaRL*
AdaRL
Rl OUt
Optimality Gap
Mean
9.0 4.5
Median
Mean
Median
Optimality Gap
Mean
AdaRL*
AdaRL
30.0 32.5 35.0 37.5
30.0 32.5 35.0 37.5
7.5 10.0 12.5
AdaRL*
Median
MTQ
MTQ
Optimality Gap
Figure A17: Evaluation on the results of modified Atari Pong game under POMDP settings with
changing reward functions for Ntarget = 10000.
37
Published as a conference paper at ICLR 2022
A5.7 Pseudo code of Miss-VAE
Algorithm A1 Pseudo code of Miss-VAE.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
#	s: latent state; theta_o, theta_s, theta_r: changing factors;
#	C so, C sr, C ar, C ss, C as, C theta oo, C theta rr, C theta ss: structure masks;
#	o: observation; a: action; r: reward;
#	num_mix: the number of components in mixture Gaussian model
#	###################################### Encoder ########################################
for X in loader: # load a batch of data from K domains
ho = conv(o)
ha = conv(a)
hr = conv(r)
hd = fc(theta_o, theta_s, theta_r)
h = concat[ho, ha, hr, hd]
output, last_state = LSTM(h) # output the logmix, mean, and logstd of state
S = sample(output) # sample the state
#	###################################### Decoder ########################################
# Reconstruct o_{t}
s_o = multiply(s, C_so)
theta_o_o = multiply(theta_o, C_theta_oo)
h_s_theta = COnCat(fc(s_o), fc(theta_o_o))
o_t = conv_transpose(h_s_theta) # observation of current step
#	Reconstruct r_{t+1}
s_r = multiply(s, C_sr)
a_r = multiply(a, C_ar)
theta_r_r = multiply(theta_r, C_theta_rr)
h_as = fc(concat[s_r, a_r])
h_theta_r = fc(theta_r_r)
r_t+1 = fc(concat[h_as, h_theta_r]) # reward of current step
#	Predict O_{t+1}
o_t+1 = conv_transpose(s, theta_o, theta_s) # reward of next step
#	Predict r_{t+2}
r_t+2 = fc(concat[fc(s), fc(a), fc(theta_o), fc(theta_s)]) # reward in the next step
#	Markovian transition
s_s = multiply(s, C_ss)
theta_s_s = multiply(theta_s, C_theta_ss)
a_s = multiply(a, C_as)
h_dyn = fc(concat[s_s, theta_s_s, a_s])
s_output = Iinear(h_dyn)
s_logmix, s_mean, log_logstd = s_output.split # state in next step
####################################### Loss ###########################################
# Reconstruction loss
Rec_loss = mean(MSE(o - o_t) + MSE(r - r_t+1))
#	Prediction loss
Pred_loss = mean(MSE(o[:,:-1,:] - o_t+1) + MSE(r[:,:-1,:] - r_t+2))
#	KL loss
for idx in range(num_mixture):
g_logmix, g_mean, g_logstd = s_logmix[:,idx], s_mean[:,idx], log_logstd[:,idx]
KL_Loss += KL(g_logmix, g_mean, g_lOgStd)
KL_loss = mean(log(1 / (KL_Loss + 1e-10) + 1e-10))
#	Sparsity constraints
Reg_loss = mean(C_so) + mean(C_sr) + mean(C_ar) + mean(C_ss) + mean(C_theta_oo) +
mean(C_theta_rr) + mean(C_theta_ss) + mean(theta_o) + mean(theta_s) + mean(
theta_r)
#	Optimization step
loss = Rec_loss + Pred_loss + KL_loss + Reg_loss
loss.backward()
optimizer.step()
38
Published as a conference paper at ICLR 2022
A6 Experimental details
A6.1 Hyperparameters selections
Model estimation We use a random policy to collect sequence data from source domains. For both
modified Cartpole and Pong experiments, the sequence length is 40 and the number of sequence is
10000 for each domain. The sampling resolution is set to be 0.02. Other details are summarized
in Table A16. Parts of the implementation of model estimation modules follows the open-source
framework in the world model (Ha and Schmidhuber, 2018).
Settings	Cartpole	Pong
# Dimensions of latent space	20	25
# Dimensions of θ	1	Size & noise: 1, orientation: 2, color: 3, Reward: 1 (linear), 2 (non-linear)
# Epochs	1000	reward-varying: 4000, others: 1500
Batch size	20	80
# RNN cells	256	256
Initial learning rate	0.01	0.01
Learning rate decay rate	0.999	0.999
Dropout	0.90	0.90
KL-tolerance	0.50		0.50
Table A16: Experimental details on the model estimation part.
Policy learning We adopt Double DQN (Van Hasselt et al., 2016) during policy learning stage.
The detailed hyper-parameters are summarized in Table A17. For a fair comparison, we use the same
set of hyperparameters for training other baseline methods.
Settings	Cartpole	Pong
Discount factor	-0:99-	0.99
Exploration rate	1.0	1.0
Initial learning rate	0.01	0.01
Learning rate decay rate	0.999	0.999
Dropout	0.10	0.10
Table A17: Experimental details on the policy learning part.
A6.2 Experimental platforms
For the model estimation, Cartpole and Pong experiments are implemented on 1 NVIDIA P100
GPUs and 4 NVidia V100 GPUs, respectively. The policy learning stages in both experiments are
implemented on 8 Nvidia RTX 1080Ti GPUs.
A6.3 Licenses
In our code, we have used the following libraries which are covered by the corresponding licenses:
•	Tensorflow (Apache License 2.0),
•	Pytorch (BSD 3-Clause "New" or "Revised" License),
•	OpenAI Gym (MIT License),
•	OpenCV (Apache 2 License),
•	Numpy (BSD 3-Clause "New" or "Revised" License)
•	Keras (Apache License).
We released our code under the MIT License.
39
Published as a conference paper at ICLR 2022
References
Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive
behavioral similarity embeddings for generalization in reinforcement learning. In International
Conference on Learning Representations (ICLR), 2021a. URL https://openreview.net/
forum?id=qda7-sVg84.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information
Processing Systems (NeurIPS), 2021b.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory. In
International Conference on Machine Learning (ICML), pages 205-214, 2018.
C. M. Bishop. Mixture density networks. In Technical Report NCRG/4288, Aston University,
Birmingham, UK, 1994.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics,
pages 569-593. Springer, 1992.
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In Inter-
national Conference on Learning Representations (ICLR), 2020. URL https://openreview.
net/forum?id=SJeD3CEFPH.
C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning (ICML), pages 1126-1135. PMLR, 2017.
David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in
Neural Information Processing Systems (NeurIPS), 31, 2018.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780,
1997.
B. Huang, K. Zhang, J. Zhang, J. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Scholkopf.
Causal discovery from heterogeneous/nonstationary data. Journal of Machine Learning Research,
21(89):612-634, 2020.
David A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual Conference
on Computational Learning Theory (COLT), pages 164-170, 1999.
Kevin Murphy. Dynamic bayesian networks: Representation, inference and learning. UC Berkeley,
Computer Science Division, 2002.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, Cambridge,
2000.
Anastasia Pentina and Christoph Lampert. A pac-bayesian bound for lifelong learning. In Interna-
tional Conference on Machine Learning (ICML), pages 991-999, 2014.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In International Conference on
Machine Learning (ICML), pages 5331-5340. PMLR, 2019.
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), pages 1889-1897.
PMLR, 2015.
40
Published as a conference paper at ICLR 2022
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. Spring-Verlag Lectures
in Statistics, 1993.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
5026-5033. IEEE, 2012.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI),
volume 30, 2016.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. arXiv preprint arXiv:1912.03820, 2019.
KUn Zhang, Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Kernel-based conditional
independence test and application in causal discovery. In Proceedings of the Twenty-Seventh
Conference on Uncertainty in Artificial Intelligence, UAI’11, pages 804-813, Arlington, Virginia,
USA, 2011. AUAI Press. ISBN 9780974903972.
LUisa Zintgraf, Kyriacos Shiarli, Vitaly KUrin, Katja Hofmann, and Shimon Whiteson. Fast context
adaptation via meta-learning. In International Conference on Machine Learning (ICML), pages
7693-7702. PMLR, 2019.
41