Published as a conference paper at ICLR 2022
Adversarial Support Alignment
ShangyUan Tong*
MIT CSAIL
TimUr Garipov*
MIT CSAIL
Yang Zhang
MIT-IBM Watson AI Lab
ShiyU Chang
UC Santa Barbara
Tommi Jaakkola
MIT CSAIL
Ab stract
We study the problem of aligning the supports of distributions. Compared to the
existing work on distribution alignment, support alignment does not require the
densities to be matched. We propose symmetric support difference as a divergence
measure to quantify the mismatch between supports. We show that select discrimi-
nators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map
support differences as support differences in their one-dimensional output space.
Following this result, our method aligns supports by minimizing a symmetrized
relaxed optimal transport cost in the discriminator 1D space via an adversarial
process. Furthermore, we show that our approach can be viewed as a limit of
existing notions of alignment by increasing transportation assignment tolerance.
We quantitatively evaluate the method across domain adaptation tasks with shifts
in label distributions. Our experiments* 1 show that the proposed method is more
robust against these shifts than other alignment-based baselines.
1	Introduction
Learning tasks often involve estimating properties of distributions from samples or aligning such
characteristics across domains. We can align full distributions (adversarial domain alignment), certain
statistics (canonical correlation analysis), or the support of distributions (this paper). Much of the
recent work has focused on full distributional alignment, for good reasons. In domain adaptation,
motivated by theoretical results (Ben-David et al., 2007; 2010), a series of papers (Ajakan et al., 2014;
Ganin & Lempitsky, 2015; Ganin et al., 2016; Tzeng et al., 2017; Shen et al., 2018; Pei et al., 2018;
Zhao et al., 2018; Li et al., 2018a; Wang et al., 2021; Kumar et al., 2018) seek to align distributions of
representations between domains, and utilize a shared classifier on the aligned representation space.
Alignment in distributions implies alignment in supports. However, when there are additional objec-
tives/constraints to satisfy, the minimizer for a distribution alignment objective does not necessarily
minimize a support alignment objective. Example in Figure 1 demonstrates the qualitative distinction
between two minimizers when distribution alignment is not achievable. The distribution alignment
objective prefers to keep supports unaligned even if support alignment is achievable. Recent works
(Zhao et al., 2019; Li et al., 2020; Tan et al., 2020; Wu et al., 2019b; Tachet des Combes et al.,
2020) have demonstrated that a shift in label distributions between source and target leads to a
characterizable performance drop when the representations are forced into a distribution alignment.
The error bound in Johansson et al. (2019) suggests aligning the supports of representations instead.
In this paper, we focus on distribution support as the key characteristic to align. We introduce a
support divergence to measure the support mismatch and algorithms to optimize such alignment. We
also position our approach in the spectrum of other alignment methods. Our contributions are as
follows (all proofs can be found in Appendix A):
1.	In Section 2.1, we measure the differences between supports of distributions. Building on the
Hausdorff distance, we introduce a novel support divergence better suited for optimization,
which we refer to as symmetric support difference (SSD) divergence.
* First two authors contributed equally. Correspondence to Shangyuan Tong (sytong@csail.mit.edu).
1We provide the code reproducing experiment results at https://github.com/timgaripov/asa.
1
Published as a conference paper at ICLR 2022
2.	In Section 2.2, we identify an important property of the discriminator trained for
Jensen-Shannon divergence: support differences in the original space of interest are "pre-
served” as support differences in the one-dimensional discriminator output space.
3.	In Section 3, we present our practical algorithm for support alignment, Adversarial Support
Alignment (ASA). Essentially, based on the analysis presented in Section 2.2, our solution is
to align supports in the discriminator 1D space, which is computationally efficient.
4.	In Section 4, we place different notions of alignment - distribution alignment, relaxed
distribution alignment and support alignment - within a coherent spectrum from the point of
view of optimal transport, characterizing their relationships, both theoretically in terms of
their objectives and practically in terms of their algorithms.
5.	In Section 5, we demonstrate the effectiveness of support alignment in practice for domain
adaptation setting. Compared to other alignment-based baselines, our proposed method is
more robust against shifts in label distributions.
(a) Initialization
DW (p, qθ) = 11.12
D4 (p, qθ) = 14.9
3.0
2.5
2.0
1.5
1.0
0.5
0.0 0.2 0.4 0.6 0.8 1.0 1.2
X
3.0
2.5
(b) Distribution alignment
DW (p, qθ) = 2 ∙ io-3
D4 (p,qθ) = 6 ∙ 10-4
2.0
1.5
1.0
0.0 L
0.0	0.2	0.4 0.6 0.8 1.0
X
(c) Support alignment
DW (p, qθ) = 5 ∙ io-2
D4(p,qθ) < 1 ∙ 10-6
Figure 1: Illustration of differences between the final configurations of distribution alignment and
support alignment procedures. p(x) is a fixed Beta distribution p(x) = Beta(x | 4, 2) with support
[0, 1]; qθ(x) is a “shifted” Beta distribution qθ(x) = Beta(x - θ | 2, 4) parameterized by θ with
support [θ, θ + 1]. Panel (a) shows the initial configuration with θinit = -3. Panel (b) shows the result
by distribution alignment. Panel (c) shows the result by support alignment. We report Wasserstein
distance DW (p, qθ) (7) and SSD divergence D4(p, qθ) (1).
2	SSD divergence and support alignment
Notation. We consider an Euclidean space X = Rn equipped with Borel sigma algebra B and a
metric d : X × X → R (e.g. Euclidean distance). Let P be the set of probability measures on (X, B).
For p ∈ P, the support of p is denoted by supp(p) and is defined as the smallest closed set X ⊆ X
such thatp(X) = 1. f]p denotes the pushforward measure ofp induced by a measurable mapping f.
With a slight abuse of notation, we use p(x) and [f]p](t) to denote the densities of measures p and
f]p evaluated at x and t respectively, implicitly assuming that the measures are absolutely continuous.
The distance between a point x ∈ X and a subset Y ⊆ X is defined as d(x, Y ) = infy∈Y d(x, y).
The symmetric difference of two sets A and B is defined as A 4 B = (A \ B) ∪ (B \ A).
2.1	Difference between supports
To align the supports of distributions, we first need to evaluate how different they are. Similar to
distribution divergences like Jensen-Shannon divergence, we introduce a notion of support divergence.
A support divergence2 between two distributions in P is a function DS (∙, ∙): P×P→ R satisfying:
1) DS (p, q) ≥ 0 for all p, q ∈ P; 2) DS (p, q) = 0 iff supp(p) = supp(q).
While a distribution divergence is sensitive to both density and support differences, a support
divergence only needs to detect mismatches in supports, which are subsets of the metric space X .
2It is not technically a divergence on the space of distributions, since DS (p, q) = o does not imply p = q.
2
Published as a conference paper at ICLR 2022
An example of a distance between subsets of a metric space is the Hausdorff distance: dH (X, Y ) =
max{supx∈X d(x, Y ), supy∈Y d(y, X)}. Since it depends only on the greatest distance between a
point and a set, minimizing this objective for alignment only provides signal to a single point. To
make the optimization less sparse, we consider all points that violate the support alignment criterion
and introduce symmetric support difference (SSD) divergence:
D4 (p, q)
=Ex 〜P[d(x, supp(q))]+ Ex~q [d(x, SuPP(P))] .	(1)
Proposition 2.1. SSD divergence D4(p, q) is a support divergence.
We note that our proposed SSD divergence is closely related to Chamfer distance/divergence (CD)
(Fan et al., 2017; Nguyen et al., 2021) and Relaxed Word Mover’s Distance (RWMD) (Kusner
et al., 2015). While both CD and RWMD are stated for discrete points (see Section 6 for further
comments), SSD divergence is a general difference measure between arbitrary (discrete or continuous)
distributions. This distinction, albeit small, is important in our theoretical analysis (Sections 2.2, 4.1).
2.2	Support alignment in one-dimensional space
Goodfellow et al. (2014) showed that the log-loss discriminator f : X → [0, 1], trained to distinguish
samples from distributions P and q (SuPf Ex〜P [log f (x)] + Ex〜q [log(1 - f (X))D can be used to
estimate the Jensen-Shannon divergence between P and q. The closed form maximizer f * is
f*(x)
P(x)
P(x) + q(x)
∀x ∈ SuPP(P) ∪ SuPP(q).
(2)
Note that for a point x ∈/ SuPP(P) ∪ SuPP(q) the value of f* (x) can be set to an arbitrary value in
[0, 1], since the log-loss does not depend on f(x) for such x. The form of the optimal discriminator
(2) gives rise to our main theorem below, which characterizes the ability of the log-loss discriminator
to identify support misalignment.
Theorem 2.1. LetP and q be the distributions with densities satisfying
C <p(x) < C, ∀x ∈ SuPP(P);	C < q(x) < C, ∀X ∈ supp(q).
(3)
Let f* be the optimal discriminator (2). Then, D4(P, q) = 0 if and only if D4(f*]P, f *]q) = 0.
The idea of the proof is to show that the extreme values (0 and 1) of f* (x) can only be attained in
x ∈ SuPP(P)4 SuPP(q). Assumption (3) guarantees that f* (x) cannot approach neither 0 nor 1 in
the intersection of the supports SuPP(P) ∩ SuPP(q), i.e. the values {f* (x) | x ∈ SuPP(P) ∩ SuPP(q)}
are separated from the extreme values 0 and 1.
We conclude this section with two technical remarks on Theorem 2.1.
Remark 2.1.1. The result of Theorem 2.1 does not necessarily hold for other types of discriminators.
For instance, the dual Wasserstein discriminator (Arjovsky et al., 2017; Gulrajani et al., 2017) does
not always highlight the support difference in the original space as a support difference in the
discriminator output space. This observation is formaly stated in the following proposition.
Proposition 2.2. Let fW be the maximizer of SuPf:Lf)<[ Ex〜p[f (x)] — Ex〜q[f (x)], where L(∙) is
the Lipschitz constant. There exist P and q with SuPP(P) 6= SuPP(q) but SuPP(fW? ]P) = SuPP(fW? ]q).
Remark 2.1.2. In practice the discriminator is typically parameterized as f(x) = σ(g(x)), where
g : X → R is realized by a deep neural network and σ(x) = (1 + e-x)-1 is the sigmoid function.
The optimization problem for g is
inf Ex〜p [log(1 + e-g(x))i + Ex〜q [log(1 + eg3)] ,	(4)
and the optimal solution is g* (x) = log P(x) - log q(x). Naturally the result of Theorem 2.1 holds
for g*, since g*(x) = σ-1(f*(x)) and σ is a bijective mapping from R ∪ {-∞, ∞} to [0, 1].
3
Published as a conference paper at ICLR 2022
3	Adversarial support alignment
We consider distributions p and q parameterized by θ: pθ, qθ. The log-loss discriminator g optimized
for (4) is parameterized by ψ: gψ. Our analysis in Section 2.2 already suggests an algorithm. Namely,
we can optimize θ by minimizing D4 (gψ]pθ , gψ]qθ) while optimizing ψ by (4). This adversarial
game is analogous to the setup of the existing distribution alignment algorithms* 3 .
In practice, rather than having direct access to pθ , qθ , which is unavailable, we are often given i.i.d.
samples {χf}N=ι, {x?}Mι. They form discrete distributionspθ(x) = N PN=I δ(x - χp),qθ(χ)=
M PMι δ(χ -Xq), and [gψ]Pθ ](t) = NN PL δ(t-gψ (χp)), [gψ]^θ ](t) = M PM δ(t -gψ (Xq)).
Since gψ]pθ and gψ*qθ are discrete distributions, they have supports {gψ (χP)}N=ι and {gψ (Xq)}M=ι
respectively. SSD divergence between discrete distributions gψ]pθ and gψ]qθ is
1N	1M
D4(gψ]Pθ,gψ]qθ) = NN X d (gψ(xP),{gψ(Xj)}M=ι) + MM X d (gψ(Xq), {gψ(xj)}j=ι) ∙⑸
Effect of mini-batch training. When training on large datasets, we need to rely on stochastic opti-
mization with mini-batches. We denote the mini-batches (of same size, as in common practice) from
Pθ and qθ as Xp = {Xip}im=1 and Xq = {Xiq}im=1 respectively. By minimizing D4 (gψ (Xp), gψ (Xq)),
we only consider the mini-batch support distance rather than the population support distance (5). We
observe that in practice the described algorithm brings the distributions to a state closer to distribution
alignment rather than support alignment (see Appendix D.5 for details). The problem is in the
typically small batch size. The algorithm actually tries to enforce support alignment for all possible
pairs of mini-batches, which is a much stricter constraint than population support alignment.
To address the issue mentioned above, without working with a much larger batch size, we create
two “history buffers”: hp , storing the previous 1D discriminator outputs of (at most) n samples
from Pθ, and a similar buffer hq for qθ. Specifically, h = {gψold,i (Xold,i)}in=1 stores the values of the
previous n samples Xold,i mapped by their corresponding past “versions” of the discriminator gψold,i.
We minimize D4 (vp, vq), where vp = concat(hp, gψ (Xp)), vq = concat(hq, gψ (Xq)):
n+m	n+m
D4(vp,vq) = n+m I X d(vp,vq)+ X d(vq,vp) I .	(6)
i=1	j=1
Note that D 4 (∙, ∙) between two sets of 1D samples can be efficiently calculated since d(vf, Vq) and
d(vjq, vp) are simply 1-nearest neighbor distances in 1D. Moreover the history buffers store only
the scalar values from the previous batches. These values are only considered in nearest neighbor
assignment but do not directly provide gradient signal for optimization. Thus, the computation
overhead of including a long history buffer is very light. We present our full algorithm, Adversarial
Support Alignment (ASA), in Algorithm 1.
4	Spectrum of notions of alignment
In this section, we take a closer look into our work and different existing notions of alignment
that have been proposed in the literature, especially their formulations from the optimal transport
perspective. We show that our proposed support alignment framework is a limit of existing notions of
alignment, both in terms of theory and algorithm, by increasing transportation assignment tolerance.
4.1	Theoretical connections
Distribution alignment. Wasserstein distance is a commonly used objective for distribution align-
ment. In our analysis, we focus on the Wasserstein-1 distance:
DW(p, q)= 磔、E(x,y)〜Y[d(X,y)],	(7)
γ∈Γ(p,q)
3Following existing adversarial distribution alignment methods, e.g. (Goodfellow et al., 2014), we use single
update of ψ per 1 update of θ. While theoretical analysis for both distribution alignment and support alignment
(ours) assume optimal discriminators, training with single update of ψ is computationally cheap and effective.
4
Published as a conference paper at ICLR 2022
Algorithm 1 Our proposed ASA algorithm. n (maximum history buffer size), we use n = 1000.
1:
2:
3:
4:
5:
6:
7:
for number of training steps do
Sample mini-batches {χf}m=ι 〜pθ, {x?}m=ι 〜qθ.
Perform optimization step on ψ using stochastic gradient
Vψ (m1 PIhlog(1 + exp(-gψ(Xp))) +log(1 + exp(gψ(Xq)))J.
Vp J Concat(hp, {gψ(xp)}m=J, vq J Concat(hq, {gψ(Xq)}m=J.
∏p→q J argmin∙ d(vp,v?), ∏j→p J argmi% d(vp,vq).
Perform optimization step on θ using stochastic gradient
口 (KP hd(vp ,v∏ P →q)+d5 ,v∏q→ρ)])∙
UPDATEHISTORY(hp, {gψ(Xip)}im=1), UPDATEHISTORY(hq, {gψ(Xiq)}im=1).
8: end for
where Γ(p, q) is the set of all measures on X × X with marginals ofp and q, respectively. The value
of DW (p, q) is the minimal transportation cost for transporting probability mass from p to q. The
transportation cost is zero if and only if p = q, meaning the distributions are aligned.
Relaxed distribution alignment. Wu et al. (2019b) proposed a modified Wasserstein distance to
achieve asymmetrically-relaxed distribution alignment, namely β-admissible Wasserstein distance:
DW(p, q) =	inf 、E(x,y)〜Y[d(x,y)],	⑻
γ∈Γβ (p,q)
where Γβ (p, q) is the set of all measures γ on X × X such that	γ(X, y)dy = p(X), ∀X and
γ(X, y)dX ≤ (1 + β)q(y), ∀y. With the relaxed marginal constraints, one could choose a trans-
portation plan γ which transports probability mass from p to a modified distribution q0 rather than
the original distribution q as long as q0 satisfies the constraint q0(X) ≤ (1 + β)q(X), ∀X. Therefore,
DWβ (p, q) is zero if and only if p(X) ≤ (1 + β)q(X), ∀X. In (Wu et al., 2019b), β is normally set
to a positive finite number to achieve the asymmetric-relaxation of distribution alignment, and it is
shown that DW0 (p, q) = DW (p, q). We can extend DWβ (p, q) to a symmetric version, which we term
β1 , β2-admissible Wasserstein distance:
DWβ1,β2(p,q)=DWβ1(p,q)+DWβ2(q,p).	(9)
The aforementioned property of β-admissible Wasserstein distance implies that DWβ1,β2 (p, q) = 0 if
and only if p(X) ≤ (1 + β1)q(X), ∀X and q(X) ≤ (1 + β2)p(X), ∀X, in which case we call p and q
“(βι, β2)-aligned”, with βι and β2 controlling the transportation assignment tolerances.
Support alignment. The term Ep[d(X, supp(q))] in (1) represents the average distance from samples
inp to the support of q. From the optimal transport perspective, this value is the minimal transportation
cost of transporting the probability mass ofp into the support of q. We show that SSD divergence can
be considered as a transportation cost in the limit of infinite assignment tolerance.
Proposition 4.1. DW∞,∞(p, q) := limβ1,β2→∞ DWβ1,β2(p, q) = D4(p, q).
We now have completed the spectrum of alignment objectives defined within the optimal transport
framework. The following proposition establishes the relationship within the spectrum.
Proposition 4.2. Letp and q be two distributions in P. Then,
1.	DW (p, q) = 0 implies DWβ1,β2 (p, q) = 0 for all finite β1, β2 > 0.
2.	DWβ1,β2 (p, q) = 0 for some finite β1, β2 > 0 implies D4(p, q) = 0.
3.	The converse of statements 1 and 2 are false.
In addition to the result presented in Theorem 2.1, we can show that the log-loss discriminator can
also “preserve” the existing notions of alignment.
Proposition 4.3. Let f * be the optimal discriminator (2) for given distributions P and q. Then,
1.	DW (P, q) = O iffDW (f *]P,f*M = 0；	2 DW1,β2 (p, q) = 0 iffDW1,β2 (f *]P,f *附)=。.
5
Published as a conference paper at ICLR 2022
4.2	Algorithmic connections
The result of Proposition 4.3 suggests methods similar to our ASA algorithm presented in Section 3
can achieve different notions of alignment by minimizing objectives discussed in Section 4.1 between
the 1D pushforward distributions. We consider the setup used in Section 3 but without history buffers
to simplify the analysis, as their usage is orthogonal to our discussion in this section.
Recall that we work with a mini-batch setting, where {xip}im=1 and {xiq}im=1 are sampled from p and q
respectively, and g is the adversarial log-loss discriminator. We denote the corresponding 1D outputs
from the log-loss discriminator by op = {oip}im=1 = {g(xip)}im=1 and oq (defined similarly).
Distribution alignment. We adapt (7) for {oip}im=1 and {oiq}im=1:
mm
DW (op,oq )=	inf	- XX Yij d(oP,oj),	(⑼
γ∈Γ(op,oq) m	i j
i=1 j=1
where Γ(op, oq) is the set of m × m doubly stochastic matrices. Since op and oq are sets of 1D
samples with the same size, it can be shown (Rabin et al., 2011) that the optimal γ* corresponds to
an assignment ∏*, which pairs points in the sorting order and can be computed efficiently by sorting
both sets op and oq. The transportation cost is zero if and only if there exists an invertible 1-to-1
assignment ∏* such that op = o∏*(向.GAN training algorithms proposed in (Deshpande et al., 2018;
2019) utilize the above sorting procedure to estimate the maximum sliced Wasserstein distance.
Relaxed distribution alignment. Similarly, we can adapt (8):
mm
DW (op,oq )= inf - XX Yij d(oP ,oj),	(11)
W	γ∈Γβ(op,oq) m i=1 j=1	i j
where Γβ(op, oq) is the set of m × m matrices with non-negative real entries, such that Pjm=1 Yij =
1, ∀i and Pim=1 Yij ≤ 1 + β, ∀j. The optimization goal in (11) is to find a “soft-assignment” Y which
describes the transportation of probability mass from points oip in op to points oiq in oq . The parameter
β controls the set of admissible assignments Γβ, which is similar to its role discussed in Section 4.1:
with transportation assignment tolerance β, the total mass of points in op transported to each of
the points oq cannot exceed 1 + β. We refer to such assignments as (β + 1)-to-1 assignment. The
transportation cost is zero if and only if there exists such an assignment between op and oq.
It can be shown (see Appendix C) that for integer value of β, the set of minimizers of (11) must
contain a “hard-assignment” transportation plan, which assigns each point oip to exactly one point
ojq. Then (1 + β) gives the upper bound on the number of points oip that can be transported to given
point ojq . This hard assignment problem can be solved quasi-linearly with worst case time complexity
O (β + 1)m2 (Bonneel & Coeurjolly, 2019), which, combined with Proposition 4.3, can lead to
new algorithms for relaxed distribution alignment besides those proposed in Wu et al. (2019b).
Support alignment. When β = ∞, the sum Pim=1 Yij is unconstrained for all j, and each point oip
can be assigned to any of the points ojq. The optimal solution is simply 1-nearest neighbor assignment,
or to follow the above terminology, ∞-to-1 assignment.
5	Experiments
Problem setting. We evaluate our proposed ASA method in the setting of unsupervised domain
adaptation (UDA). The goal of UDA algorithms is to train and “adapt” a classification model
M : X → Y from source domain distribution pX,Y to target domain distribution qX,Y given the
access to a labeled source dataset {χf, yp}Nι 〜px,γ and an unlabeled target dataset {χ} }Nι 〜qχ.
A common approach for UDA is to represent M as Cφ ◦ Fθ: a classifier Cφ : Z → Y and a feature
extractor Fθ : X → Z, and train Cφ and Fθ by minimizing: 1) classification loss 'c∣s on source
examples; 2) alignment loss Dalign measuring discrepancy between pθZ = Fθ]pX and qZθ = Fθ]qX:
1 Np
Imin Np X'cls(Cφ(Fθ(xp)),yp) + λ∙Dalign ({Fθ(χP)}Nι, {Fθ(Xq)}Nι) ,	(12)
6
Published as a conference paper at ICLR 2022
-1.5 -1.0 -0.5 0.0	0.5 1.0 1.5 2.0 2.5
(a) No DA (avg acc: 63%)
DW(pθZ,qZθ) = 0.78
D4(pθZ,qZθ) = 0.10
(b) DANN (avg acc: 75%)
DW (pθZ, qZθ ) = 0.07
D4(pθZ,qZθ) = 0.02
(c) ASA-abs (avg acc: 94%)
DW (pθZ, qZθ ) = 0.59
D4(pθZ,qZθ) = 0.03
Figure 2: Visualization of learned 2D embeddings on 3-class USPS→MNIST with label distribution
shift. In source domain, all classes have equal probability 1. The target probabilities of classes '3’,
‘5’, ‘9’ are [23%, 65%, 12%]. Each panel shows 2 level sets (outer one approximates the support) of
the kernel density estimates of embeddings in source (filled regions) and target domains (solid/dashed
lines). We report the average class accuracy of the target domain, DW and D4 between embeddings.
In practice Dalign is an estimate of a divergence measure via an adversarial discriminator gψ . Choices
of Dalign include f-divergences (Ganin et al., 2016; Nowozin et al., 2016) and Wasserstein distance
(Arjovsky et al., 2017) to enforce distribution alignment and versions of re-weighted/relaxed distribu-
tion divergences (Wu et al., 2019b; Tachet des Combes et al., 2020) to enforce relaxed distribution
alignment. For support alignment, we apply the proposed ASA method as the alignment subroutine
in (12) with log-loss discriminator gψ (4) and Dalign computed as (6).
Task specifications. We consider 3 UDA tasks: USPS→MNIST, STL→CIFAR, and VisDA-2017,
and 2 versions of ASA: ASA-sq, ASA-abs corresponding to squared and absolute distances respec-
tively for d(∙, ∙) in (6). We compare ASA with: No DA (no domain adaptation), DANN (Ganin et al.,
2016) (distribution alignment with JS divergence), VADA (Shu et al., 2018) (distribution alignment
with virtual adversarial training), IWDAN, IWCDAN (Tachet des Combes et al., 2020) (relaxed
distribution alignment via importance weighting) SDANN-β (WU et al., 2019b) (relaxed/e-admissible
JS divergence via re-weighting). Please refer to Appendix D for full experimental details.
To evaluate the robustness of the methods, we simulate label distribution shift by subsampling source
and target dataset, so that source has balanced label distribution and target label distribution follows
the power law qγ(y) a σ(y)-α, where σ is a random permutation of class labels {1,..., K} and ɑ
controls the severity of the shift (α = 0 means balanced label distribution). For each task, we generate
5 random permutations σ for 4 different shift levels α ∈ {0, 1, 1.5, 2}. Essentially we transform each
(source, target) dataset pair to 5 × 4 = 20 tasks of different difficulty levels, since classes are not
equally difficult and different permutations can give them different weights.
Evaluation metrics. We choose the average (per-)class accuracy and minimum (per-)class accuracy
on the target test set as evaluation metrics. Under the average class accuracy metric, all classes are
treated as equally important (despite the unequal representation during training for α > 0), and the
minimum class accuracy focuses on model’s worst within-class performance. In order to account
for the variability of task difficulties across random permutations of target labels, we report robust
statistics, median and a 25-75 percentile interval, across 5 runs.
Illustrative example. First we consider a simplified setting to intuitively understand and directly
analyze the behavior of our proposed support alignment method in domain adaptation under label
distribution shift. We consider a 3-class USPS→MNIST problem by selecting a subset of examples
corresponding to digits ‘3’, ‘5’, and ‘9’, and use a feature extractor network with 2D output space. We
introduce label distribution shift as described above with α = 1.5, i.e. the probabilities of classes in
the target domain are 12%, 23%, and 65%. We compare No DA, DANN, ASA-abs by their average
target classification accuracy, Wasserstein distance DW (pθZ, qZθ ) and SSD divergence D4(pθZ, qZθ )
between the learned embeddings of source and target domain. We apply a global affine transformation
to each embedding space in order to have comparable distances between different spaces: we center
the embeddings so that their average is 0 and re-scale them so that their average norm is 1. The
7
Published as a conference paper at ICLR 2022
Table 1: Average and minimum class accuracy (%) on USPS→MNIST with different levels of shifts
in label distributions (higher α implies more severe imbalance). We report median (the main number),
and 25 (subscript) and 75 (superscript) percentiles across 5 runs.
	α=	0.0	α=	：1.0	α=	1.5	α=	2.0
Algorithm	average	min	average	min	average	min	average	min
No DA	71.9 7720..94	20.3 1227..96	72 9 74.7 72.9 72.0	25.8 313 . 18.3	71.3 7721..52	37.3 27.5 24.2	71 3 73.0 71.3 70.6	16.6 1260..88
DANN	97.8 9977..86	96.0 9965..18	83 5 84.6 83.5 76.7	36. 3 36.9 25.1 08.4	70.0 6713..29	01 1 01.5 01.1 01.0	57.8 5602..40	00.9 0010..65
VADA	98.0 9987..09	96.2 9965..39	89.9 88.2 88.1	48.9 57.8	83.1 78.2 70.7	06.6 0232..54	61.9 5656..43	01 4 01.5 01.4 00.8
IWDAN	97.5 9977..54	95.9 95.7 95.7	95.8 95.7 92.6	Ql 8 82.3 81.3 67.1	86 5 87.8 86.5 80.2	55.0 15.2 04.2	74 4 78.6 74.4 70.0	07 3 22.4 07.3 06.3
IWCDAN	98.0 9987..19	96.9 96.6 96.4	96.7 9973..53	85.1 6935..93 87.7	91 3 93.8 91.3 90.5	66 5 74.5 66.5 64.1	82.3 77.5 77.3	45.4 22.2 02.7
sDANN-4	87.4 8957..72	05.6 0905..06	94.9 9944..97	85.7 84.4	86.8 8895..15	21 6 50.3 21.6 15.4	81 5 83.1 81.5 81.3	39.3 3567..29
ASA-sq	93.7 9933..93	89.2 8898..44	92.3 9931..65	83.5 80.7	90 9 92.1 90.9 89.6	69 9 82.0 69.9 66.6	89.3 87.2 85.8	62.5 4696..34
ASA-abs	94.1 9943..58	88.9 8917..20	92.8 8939..23	78.9 85.1	925 92.9 92.5 90.9	824 85.4 82.4 74.5	904 90.7 90.4 89.2	68.4 6737..05
Table 2: Results on STL→CIFAR. Same setup and reporting metrics as Table 1.
	α=	0.0	α=	：1.0	α=	1.5	α=	2.0
Algorithm	average	min	average	min	average	min	average	min
No DA	69.9 6709..08	49.8 4505..63	68.8 6698..33	47.2 45.3	66.8 6676..24	46.0 4475..08	65 8 66.7 65.8 64.8	43 7 44.6 43.7 41.6
DANN	75.3 7754..49 76.7	54.6 5564..62	69.9 6708..16	44.8 40.7	64.9 6673..17	34.9 3363..89	63 3 64.8 63.3 57.4	27.0 2281..52
VADA	76.7 76.6	56.9 5583..35	70.6 7710..00	47 7 48.8 47.7 44.0	66.1 6665..54	39.3 35.7 33.3	63 2 64.7 63.2 60.2	28.0 25.5 25.2
IWDAN	69.9 6709..79	50.5 4507..69	68.7 6698..16	45 8 50.5 45.8 44.8	67 1 67.3 67.1 65.9	44 7 44.8 44.7 40.4	64.4 6643..96	36.8 3374..95
IWCDAN	70 1 70.2 70.1 70.1	49.3 47.8 42.4	69.4 6699..41	47 1 51.3 47.1 46.3	66.1 6675..20	39.9 3407..87 49.0	64.5 6653..19	37.0 3405..25
sDANN-4	71.8 7721..17	52.1 5522..81	71.1 7710..74	49 9 51.8 49.9 48.1	69.4 6708..07	48.6 43.5	66 4 67.9 66.4 66.2	39.0 3473..16
ASA-sq	71 7 71.9 71.7 71.7	52.9 4536..47	70.7 7710..04	52.7 51.6 46.8	69.2 6699..32	45 6 52.0 45.6 43.3	68.2 68.1 67.2	447 45.9 44.7 39.8
ASA-abs	71 6 71.7 71.6 71.2	49.0 4538..54	70.9 7710..08	49.2 57.3	69.6 6699..96	43 2 49.5 43.2 42.1	68.2 67.8 66.6	40.9 3495..04
results are shown in Figure 2 and Table D.5. Compared to No DA, both DANN and ASA achieve
support alignment. DANN enforces distribution alignment, and thus places some target embeddings
into regions corresponding to the wrong class. In comparison, ASA does not enforce distribution
alignment and maintains good class correspondence across the source and target embeddings.
Main results. The results of the main experimental evaluations are shown in Tables 1, 2, 3. Without
any alignment, source only training struggles relatively to adapt to the target domain. Nonetheless, its
performance across the imbalance levels remains robust, since the training procedure is the same.
Agreeing with the observation and theoretical results from previous work (Zhao et al., 2019; Li et al.,
2020; Tan et al., 2020; Wu et al., 2019b; Tachet des Combes et al., 2020), distribution alignment
methods (DANN and VADA) perform well when there is no shift but suffer otherwise, whereas
relaxed distribution alignment methods (IWDAN, IWCDAN and sDANN-β) show more resilience to
shifts. On all tasks with positive α, we observe that it is common for the existing methods to achieve
good class average accuracies while suffering significantly on some individual classes. These results
suggest that the often-ignored but important min-accuracy metric can be very challenging. Finally,
our support alignment methods (ASA-sq and ASA-abs) are the most robust ones against the shifts,
while still being competitive in the more balanced settings (α = 0 or 1). We achieve best results
in the more imbalanced and difficult tasks (α = 1.5 or 2) for almost all categories on all datasets.
Please refer to Appendix D for ablation studies and additional comparisons.
6 Related work
Distribution alignment. Apart from the works, e.g. (Ajakan et al., 2014; Ganin et al., 2016; Ganin
& Lempitsky, 2015; Pei et al., 2018; Zhao et al., 2018; Long et al., 2018; Tachet des Combes et al.,
8
Published as a conference paper at ICLR 2022
Table 3: Results on VisDA17. Same setup and reporting metrics as Table 1.
	α=	0.0	α=	, 1.0	α=	1.5	α=	2.0
Algorithm	average	min	average	min	average	min	average	min
No DA	49.5 4509..54	24.6 22.2 22.2	50 2 50.8 50.2 49.2	91 2 21.3 21.2 20.7	47 1 47.6 47.1 46.6	18.6 1228..26	45.3 4465..52	19.5 1194..84
DANN	75.4 7764..24	36.7 3405..96	64 1 65.3 64.1 62.8	25 0 29.3 25.0 24.8	52 1 52.3 52.1 51.4	11.5 1121..44	43.1 3449..31	03.6 0143..36
VADA	75 3 76.0 75.3 74.8	40.5 3419..87	64.6 6651..12	28.2 22.8 21.7	53 0 54.2 53.0 51.6	14.8 1213..77	43.9 4440..79	08.5 0115..10
IWDAN	73.2 7732..39	34.8 31.7 22.8	64.4 6641..61	12 1 24.7 12.1 05.0	51 3 56.6 51.3 51.0	04 6 10.4 04.6 02.1	45 1 48.0 45.1 41.7	04 6 13.6 04.6 01.2
IWCDAN	71.6 7750..26	27.6 28.0 . 22.8	60 6 61.0 60.6 60.2	01 1 11.3 01.1 00.7	49.7 4515..96	02.2 0050..72	46.2 38.3 37.3	00.6 0010..73
sDANN-4	72.4 7731..38	40.8 37.8 32.3	68.7 68.4 66.2	26 6 29.4 26.6 26.2	57.8 57.2 56.8	18.6 1236..97	50 7 51.7 50.7 49.8	18.6 1207..01
ASA-sq	64.9 6653..07	35.8 35.7 32.1	63.2 61.8 60.6	31 4 34.4 31.4 20.4	58.3 57.8 55.5	32.1 26.7 17.3	52.0 51.9 50.8	18.3 1216..29
ASA-abs	64.8 6654..05	41.9 40.6 36.0	62 0 62.3 62.0 60.5	29.7 27.3 16.7	58.4 57.1 56.2	26.0 1313..29	56.6 52.5 51.9	197 22.2 19.7 17.7
2020; Li et al., 2018b; Tzeng et al., 2017; Shen et al., 2018; Kumar et al., 2018; Li et al., 2018a;
Wang et al., 2021; Goodfellow et al., 2014; Arjovsky et al., 2017; Gulrajani et al., 2017; Mao et al.,
2017; Radford et al., 2015; Salimans et al., 2018; Genevay et al., 2018; Wu et al., 2019a; Deshpande
et al., 2018; 2019), that do distribution alignment, there are also papers (Long et al., 2015; 2017;
Peng et al., 2019; Sun et al., 2016; Sun & Saenko, 2016) focusing on aligning some characteristics of
the distribution, such as first or second moments. Our work is concerned with a different problem,
support alignment, which is a novel objective in this line of work. In terms of methodology, our use
of the discriminator output space to work with easier optimization in 1D is inspired by a line of work
(Salimans et al., 2018; Genevay et al., 2018; Wu et al., 2019a; Deshpande et al., 2018; 2019) on sliced
Wasserstein distance based models. Our result in Proposition 4.3 also provides theoretical insight on
the practical effectiveness of 1D OT in (Deshpande et al., 2019).
Relaxed distribution alignment. In Section 4, we have already covered in detail the connections
between our work and (Wu et al., 2019b). Balaji et al. (2020) introduced relaxed distribution
alignment with a different focus, aiming to be insensitive to outliers. Chamfer distance/divergence
(CD) is used to compute similarity between images/3D point clouds (Fan et al., 2017; Nguyen et al.,
2021). For text data, Kusner et al. (2015) presented Relaxed Word Mover’s Distance (RWMD) to
prune candidates of similar documents. CD and RWMD are essentially the same as (5) with d(∙, ∙)
being the Euclidean distance. They are computed by finding the nearest neighbor assignments. Our
subroutine of calculating the support distance in the 1D discriminator output space is done similarly
by finding nearest neighbors within the current batch and history buffers.
Support estimation. There exists a series of work, e.g. (SchOlkoPf et al., 2001; Hoffmann, 2007;
Tax & Duin, 2004; Knorr et al., 2000; Chalapathy et al., 2017; Ruff et al., 2018; Perera et al., 2019;
Deecke et al., 2018; Zenati et al., 2018), on novelty/anomaly detection problem, which can be casted
as support estimation. We consider a fundamentally different problem setting. Our goal is to align
the supports and our approach does not directly estimate the supports. Instead, we implicitly learn
the relationships between supports (density ratio to be specific) via a discriminator.
7 Conclusion and future work
In this paper, we studied the problem of aligning the supports of distributions. We formalized its
theoretical connections with existing alignment notions and demonstrated the effectiveness of the
approach in domain adaptation. We believe that our methodology opens possibilities for the design
of more nuanced and structured alignment constraints, suitable for various use cases. One natural
extension is support containment, achievable with only one term in (1). This approach is fitting for
partial domain adaptation, where some source domain classes do not appear in the target domain.
Another interesting direction is unsupervised domain transfer, where support alignment is more
desired than existing distribution alignment methods due to mode imbalance (Binkowski et al., 2019).
9
Published as a conference paper at ICLR 2022
Acknowledgments
The computational experiments presented in this paper were performed on “Satori” cluster developed
as a collaboration between MIT and IBM. We used Weights & Biases (Biewald, 2020) for experiment
tracking and visualizations to develop insights for this paper.
TJ acknowledges support from MIT-IBM Watson AI Lab, from Singapore DSO, and MIT-DSTA
Singapore collaboration. We thank Xiang Fu and all anonymous reviewers for their helpful comments
regarding the paper’s writing and presentation.
References
Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois Laviolette, and Mario Marchand. Domain-
adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014. 1, 8
Martin Arjovsky, Soumith Chintala, and L6on Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214-223. PMLR, 2017. 3, 7,9
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in
generative modeling and domain adaptation. Advances in Neural Information Processing Systems
Foundation (NeurIPS), 2020. 9
Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007. 1
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1):151-175, 2010. 1
Dimitris Bertsimas and John N Tsitsiklis. Introduction to linear optimization, volume 6. Athena
Scientific Belmont, MA, 1997. 21
Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.
com/. Software available from wandb.com. 10
Mikolaj Binkowski, Devon Hjelm, and Aaron Courville. Batch weight for domain adaptation with
mass shift. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
1844-1853, 2019. 9
Nicolas Bonneel and David Coeurjolly. Spot: sliced partial optimal transport. ACM Transactions on
Graphics (TOG), 38(4):1-13, 2019. 6
Eric Budish, Yeon-Koo Che, Fuhito Kojima, and Paul Milgrom. Implementing random assignments:
A generalization of the birkhoff-von neumann theorem. In 2009 Cowles Summer Conference, 2009.
21
Raghavendra Chalapathy, Aditya Krishna Menon, and Sanjay Chawla. Robust, deep and inductive
anomaly detection. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pp. 36-51. Springer, 2017. 9
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223. JMLR Workshop and Conference Proceedings, 2011. 22
Lucas Deecke, Robert Vandermeulen, Lukas Ruff, Stephan Mandt, and Marius Kloft. Image anomaly
detection with generative adversarial networks. In Joint european conference on machine learning
and knowledge discovery in databases, pp. 3-17. Springer, 2018. 9
Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
wasserstein distance. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3483-3491, 2018. 6,9, 20
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for
gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 10648-10656, 2019. 6, 9, 20, 23
10
Published as a conference paper at ICLR 2022
Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object
reconstruction from a single image. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 605-613, 2017. 3, 9
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International conference on machine learning, pp. 1180-1189. PMLR, 2015. 1, 8
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The journal of machine learning research, 17(1):2096-2030, 2016. 1, 7, 8
Aude Genevay, Gabriel Peyr6, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617.
PMLR, 2018. 9
Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
arXiv:1406.2661, 2014. 3,4, 9
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved
training of wasserstein gans. In Proceedings of the 31st International Conference on Neural
Information Processing Systems, pp. 5769-5779, 2017. 3, 9, 23
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016. 25
Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40(3):863-874, 2007. 9
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern
analysis and machine intelligence, 16(5):550-554, 1994. 22
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 527-536. PMLR, 2019. 1
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 22
Edwin M Knorr, Raymond T Ng, and Vladimir Tucakov. Distance-based outliers: algorithms and
applications. The VLDB Journal, 8(3):237-253, 2000. 9
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. 22
Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill
Freeman, and Gregory Wornell. Co-regularized alignment for unsupervised domain adaptation.
Advances in Neural Information Processing Systems, 31:9345-9356, 2018. 1, 9
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document
distances. In International conference on machine learning, pp. 957-966. PMLR, 2015. 3, 9
Yann LeCun, L6on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 22
Bo Li, Yezhen Wang, Tong Che, Shanghang Zhang, Sicheng Zhao, Pengfei Xu, Wei Zhou, Yoshua
Bengio, and Kurt Keutzer. Rethinking distributional matching based domain adaptation. arXiv
preprint arXiv:2006.13352, 2020. 1, 8
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5400-5409, 2018a. 1,9
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 624-639, 2018b. 9
11
Published as a conference paper at ICLR 2022
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97-105. PMLR,
2015. 9
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International conference on machine learning, pp. 2208-2217. PMLR,
2017. 9
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
8
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least
squares generative adversarial networks. In Proceedings of the IEEE international conference on
computer vision, pp. 2794-2802, 2017. 9
Trung Nguyen, Quang-Hieu Pham, Tam Le, Tung Pham, Nhat Ho, and Binh-Son Hua. Point-set
distances for learning representations of 3d point clouds. arXiv preprint arXiv:2102.04014, 2021.
3, 9
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Proceedings of the 30th International Conference on
Neural Information Processing Systems, pp. 271-279, 2016. 7
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adap-
tation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. 1,
8
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda:
The visual domain adaptation challenge, 2017. 25
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pp. 1406-1415, 2019. 9
Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Ocgan: One-class novelty detection using
gans with constrained latent representations. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 2898-2906, 2019. 9
Gabriel Peyra Marco Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355-607, 2019. 21
Julien Rabin, Gabriel Peyra Julie Delon, and Marc Bernot. Wasserstein barycenter and its application
to texture mixing. In International Conference on Scale Space and Variational Methods in
Computer Vision, pp. 435-446. Springer, 2011. 6, 20
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 9
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander
Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In International
conference on machine learning, pp. 4393-4402. PMLR, 2018. 9
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=rkQkBnJAb. 9
Bernhard Scholkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.
Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443-1471,
2001. 9
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018. 1, 9
12
Published as a conference paper at ICLR 2022
Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-t approach to unsupervised
domain adaptation. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1q-TM-AW. 7, 22, 24
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443-450. Springer, 2016. 9
Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. 9
Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation
with conditional distribution matching and generalized label shift. Advances in Neural Information
Processing Systems, 33, 2020. 1, 7, 8, 22
Shuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical
odyssey. In European Conference on Computer Vision, pp. 585-602. Springer, 2020. 1, 8
David MJ Tax and Robert PW Duin. Support vector data description. Machine learning, 54(1):45-66,
2004. 9
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
7167-7176, 2017. 1,9
Jing Wang, Jiahong Chen, Jianzhe Lin, Leonid Sigal, and Clarence W de Silva. Discriminative feature
alignment: Improving transferability of unsupervised domain adaptation by gaussian-guided latent
alignment. Pattern Recognition, 116:107943, 2021. 1, 9
Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, and Luc Van
Gool. Sliced wasserstein generative models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019a. 9, 20
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learning,
pp. 6872-6881. PMLR, 2019b. 1, 5, 6, 7, 8, 9,23
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar.
Adversarially learned anomaly detection. In 2018 IEEE International conference on data mining
(ICDM), pp. 727-736. IEEE, 2018. 9
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in Neural Information
Processing Systems, pp. 8568-8579, 2018. 1, 8
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532. PMLR, 2019. 1, 8
13
Published as a conference paper at ICLR 2022
A Proofs of the theoretical results
A.1 Proof of Proposition 2.1
1)	D4 (p, q) ≥ 0 for all p, q ∈ P :
Since d(∙, ∙) ≥ 0, for allp, q,
SD(p, q) ：= Ex〜p[d(x, supp(q))] = Ex〜P	inf	d(x, y) ≥ 0,	(13)
Ly 〜SUPP⑷	_
which makes D4 (p, q) = SD(p, q) + SD(q, p) ≥ 0.
2)	D4 (p, q) = 0 if and only if supp(p) = supp(q):
With statement 1, D4(p, q) = 0 if and only if SD(p, q) = 0 and SD(q,p) = 0.
Then,
SD(p,q)=0 =⇒ Ex〜p[d(x, supp(q))] = 0 =⇒ P ({x∣d(x, supp(q)) > 0}) = 0.
This is equivalent to
∀x ∈ supp(p), d(x, supp(q )) = 0.
Thus, supp(p) ⊆ supp(q), and similarly, supp(q) ⊆ supp(p), which makes supp(p) = supp(q).
A.2 Assumption and proof of Theorem 2.1
A.2. 1 Comments on Assumption (3)
Assumption (3) is not restrictive. Indeed, distributions satisfying Assumption (3) include:
•	uniform p(x) = U (x; [a, b]);
•	truncated normal;
• p(x) of the form
1-e-Ep(X)
P(X) =	Zp	,
x ∈ supp(P),
x ∈/ supp(P),
with non-negative energy (unnormalized log-density) function Ep : X → [0, ∞);
•	mixture of any distributions satisfying Assumption (3), for instance the distributions shown
in Figure A.1 top-left are mixtures of truncated normal distributions on [-2, 2].
Starting from arbitrary density P0 (x) with bounded support we can derive a density P(x) satisfying
Assumption (3) via density clipping and re-normalization
P(X) Z clip (po(x),仁C
for some C0 > 1.
A.2.2 Proof of Theorem 2.1
First, We show that D4(p, q) = 0 implies D4(f *]P,f *]q) = 0.
D4(P, q) = 0 implies supp(P) = supp(q). Then for any mapping f : X → R, we have supp(f]P)
supp(f]q), which implies supp(f1p) = supp(f %q). Thus, D4(f *]P,f [q) = 0.
Now, we prove that D4(f *]P,f 'q) = 0 implies D 4 (p,q) = 0 by contradiction.
14
Published as a conference paper at ICLR 2022
D4(f *]P, f 'q) = 0 implies the following:
Et〜f*]p[d(t, supp(f*]q)] = 0,	Et〜
f*]q[d(t, SUppf *]P)] = 0.
1)	Suppose Ex〜p[d(x, supp(q))] > 0. This is only possible if p({x | X ∈ SUpp(P) \ supp(q)}) > 0.
Since X ∈ SUpp(P) \ SUpp(q) implies P(X) > 0, q(X) = 0, and for any X ∈ SUpp(P) ∪ SUpp(q),
p(x) > 0, q(X) = 0 if and only if f *(x) = p(xp+q(x)= 1, we have:
Pf*]P({1}) = PP({X | X ∈ SUpp(P) \ SUpp(q)}) > 0,
and therefore 1 ∈ supp(f */).
For a real number α : 0 < α < c+, consider the probability of the event (1 - α, 1] ⊂ [0,1] under
distribution f *附：
Pf*]q((1 - α,1])= Pq({x ∣f*(X) ∈ (1 - α,1]}).
By assumption (3), P(X) < C and q(v) > 0 implies q(v) > C, therefore for X : q(v) > 0 we have
f*(x)
P(X)— <	P(X)	< C = 1___________J < 1 - α.
P(X) + q(χ)	P(X) + cl	C + -C	C2 + 1
This means that Pf*/((1 - α, 1]) = 0, i.e. supp(f 'q) ∩ (1 - α, 1] = 0.
To summarize, starting from the assumption that Ex〜P [d(χ, supp(q))] > 0 We showed that
•	1 ∈ supp(f* ]P), Pf *]P({1}) > 0；
•	supp(f *]q) ∩ (1 - α, 1] = 0.
Because D4(f“,f*附)≥ Et〜f*]ρ[d(t,supp(f*附川 ≥ Pf*]ρ({1}) ∙ d(1,supp(f*1q)) ≥
Pf*]ρ({1}) ∙ a > 0, which contradicts with the given D4(f∖p,f'q) = 0, We have
Ex〜P [d(x, SUpp(q))] = 0.
2)	Similarly, it can be shown Ex〜q [d(χ, SUpp(P))] = 0.
Thus, D4(f *]P,f %q) = 0 implies D4(p, q) = 0.
A.3 Proof of Proposition 2.2
Consider a 1-dimensional Euclidean space R. Let SUpp(P) = [-1,1 ] ∪ [1, 2] withp([-2,1 ]) = 4
and p([1, 2]) =1 ∙Let supp(q) =[-2, -1] ∪[- 2,1 ] ∪[1,2] with q([-2, -1]) =1, q([-1,2]) = 4
and q([1, 2]) = 1. The supports of P and q consist of disjoint closed intervals, and we assume uniform
distribution within each of these intervals, i.e. P has density P(X) = 44, ∀x ∈ [- 2, 2]; P(X) = 1, ∀x ∈
[1, 2] and q has densitiy q(X) = 1, ∀X ∈ [-2, -1]; q(X) = 4, ∀X ∈ [-2, 1 ]; q(X) = 2, ∀x ∈ [1, 2].
Clearly, SUpp(P) 6= SUpp(q).
The optimal dual Wasserstein discriminator fW is the maximizer of
sup	Ex〜p[f(X)] - Ey〜q[f(y)].
f:Lip(f )≤1
Thus, fW is the maximizer of
sup 3	f (x)Qx +	f (x)Qx —	f (x)Qx —	f (x)Qx — 2	f (x)Qx
f:Lip(f )≤1 4∖ J-1	1	-2	-1	1
which simplifies to
sup —	f (x)"x + 2 /	f (x)"x —	f (x)"x ).
f ：Lip(f )≤1 4∖ J-2	- 2	1	J
Since the optimization objective and the constraint are invariant to replacing the function f(X) with
its symmetric reflection g(X) = f (-X), if f0 is a optimal solution, then there exists a symmetric
15
Published as a conference paper at ICLR 2022
maximizer fW(x) = 2f0(x) + 2f0(-x), since fW(x) = fW(-x) and Lip(fW) ≤ Lip(f0) ≤ 1.
Thus, supp(fW]P) = supp(fW]q) as fW(x) = fW(-x) for X ∈ [1, 2].
Note that one can easily “extend” the above proof to discrete distributions, by replacing the disjoint
segments [-2, -1], [-2, 2], [1,2] with points {-1}, {0}, {1}.
A.4 Proof of Proposition 4.1
From (8), we have
D∞ (P,q) := βl→∞ DW (P,q) = β→∞Y∈Γn fp,q) E(x,yZ [d(x,y)],
where limβ→∞ Γβ(p, q) is the set of all measures γ on X × X such that γ(x, y)dy = p(x), ∀x and
γ(x, y)dx ≤ limβ→∞(1 + β)q(y), ∀y.
The set of inequalities
γ(x, y)dx ≤ lim (1 + β)q(y),	∀y
β→∞
can be simplified to
γ(x, y)dx = 0,	∀y such that q(y) = 0.
To put it together, we have
D∞(p, q) =	inf、E(χ,y)〜Y[d(x, y)],
γ∈Γ∞ (p,q)
where Γ∞(p, q) is the set of all measures γ on X × X such that γ(x, y)dy = p(x), ∀x and
γ(x, y)dx = 0, ∀y such that q(y) = 0. In other words, we seek the coupling γ(x, y) which defines
a transportation plan such that the total mass transported from given point x is equal to p(x), and the
only constraint on the destination points y is that no probability mass can be transported to points y
where q(y) = 0, i.e. y ∈/ supp(q).
Let y* (x) denote a function such that
y*(x) ∈ supp(q), ∀x;	d(x,y* (X))=	inf	d(x,y).
y∈supp(q)
We can see that γ * given by
γ*(x, y) = p(x)δ(y - y*(x)),
is the optimal coupling. Indeed, γ * satisfies the constraints γ* ∈ Γ∞, and the cost of any other
transportation cost γ ∈ Γ∞ is at least that ofγ* (since y*(x) is defined as a closest point y in supp(q)
to a given point x).
Thus,
D∞ (P,q) =	inf	E(x,y)~γ [d(X,y)] = E(x,y)~γ* [d(X,y)] = Ex~p	inf	d(x,y) ∙
γ∈Γ∞ (p,q)	y∈supp(q)
The last equation implies D∞(p, q) = SD(p, q) (SD(∙, ∙) is defined in (13)). Then,
DW∞,∞(p, q) := lim	DWβ1,β2(p, q) = DW∞(p, q)+DW∞(q, p) = SD(p, q)+SD(q, p) = D4(p, q).
β1 ,β2 →∞
A.5 Proof of Proposition 4.2
1. DW (p, q ) = 0 implies p = q, which is equivalent to
p(X)
= 1,	∀x ∈ SUPP(P) ∪ supp(q).
q(X)
Then clearly, for all finite β1 , β2 > 0 it satisfies
1	P(X)
ι । Q ≤	≤ 1 + βι,	∀χ ∈ supp(p) ∪ supp(q).
1 + β2	q(X)
Thus, DWβ1,β2 (P, q) = 0 for all finite β1, β2 > 0.
(14)
16
Published as a conference paper at ICLR 2022
2.	DWβ1,β2 (p, q) = 0 for some finite β1, β2 > 0 means that (14) is satisfied. This implies that
∀x ∈ supp(p), x ∈ supp(q) and ∀x ∈ supp(q), x ∈ supp(p), which makes supp(p) =
supp(q). Thus, D4 (p, q) = 0.
3.	The converse of statements 1 and 2 are false:
(a)	For all finite β1, β2 > 0, let supp(p) = supp(q) = {x1, x2}. Let p(x1) = p(x2) =
1/2 and q(xi) = (1 + β0)/2 and q(χ2) = (1 - β0)∕2 where
β0 = min (β2,l — 二).
1 + β1
Then, it can be easily checked that (14) is satisfied, which makes DWβ1,β2 (p, q) = 0.
However, since β0 6= 0, p 6= q and thus DW (p, q) 6= 0.
(b)	Similar to (a), let supp(p) = supp(q) = {x1, x2}. Let p(x1) = q(x2) = ε and
p(x2) = q(x1) = 1 - ε for some ε > 0. Since supp(p) = supp(q), D4(p, q) = 0.
However,
p(x1)	ε
lim ———-=lim---------= 0,
εψo q(x 1 )	εψ0 1 — ε
and, thus, for any finite β2 > 0 we can choose ε > 0 such that
P(XI) =	ε <	1
q(xι)	1 — ε 1 + β2
Therefore, (14) is not satisfied and DWβ1,β2 (p, q) 6= 0.
A.6 Proof of Proposition 4.3
Using (2), We first establish a connection between the pushforward distributions f hP and f hq.
Proposition A.1. Let f * be the optimal log-loss discriminator (2) between P and q. Then,
[T*J⅛∏t⅛) = t,	Vt ∈ supp(f*]p) ∪ supp(f*]q).
(15)
Proof. For any point t ∈ supp(f*]P) ∪ supp(f*]q), the values of the densities
[f * n](t) = Hm Pp ({x lt-ε<f*(x) <t + ε})=	几 ∣t-ε<f *(x)<t+ε} P(X) dx
]P ]P()*	2ε	阴	2ε
[f *]q](t) = lim Pq ({χlt-ε<f*(X) <t+明
εψ0	2ε
lim ∕{x∣t-ε<f*(x)<t+ε}q(X) d
εψo	2ε
Note that for all X : t - ε < f* (X) < t + ε we have
t - ε <	P(X)_
P(X) + q(X)
< t + ε,
which implies
(t - ε)(P(X) + q(X)) < P(X) < (t + ε)(P(X) + q(X)).
Since these inequalities hold for all X : t - ε < f* (X) < t + ε, the similar relationship holds for the
integrals:
(t - ε)	(P(X) + q (X)) dX
J {x∖t-ε<f *(x)<t+ε}
<	P(X) dX <
{x∖t-ε<f * (x)<t+ε}
(t + ε)	(P(X) + q (X)) dX.
{x∖t-ε<f * (x)<t+ε}
17
Published as a conference paper at ICLR 2022
The ratio [f∖p](t)∕([f hp](t) + [f %q](t)) Canbe expressed as
[f*]P](t)	= lim	几∣t-ε<f*(x)<t+ε}P(X) d
[f*]P]⑴ + [f* 附]⑴—阴儿 ∣t-ε<f*(x)<t+ε} (P(X) + 9(X)) dx
Using the inequality above we observe that
t-ε <
八x∣t-ε<f*(x)<t+ε} P(X) dx
八 x∣t-ε<f*(x)<t+ε} (P(X) + 9(X)) dx
< t + ε,
for all ε > 0, and taking the limit ε 1 0 We obtain
[f *]P](t)
[f*]p](t) + [f*]q](t)=.
Figure A.1: Visual illustration of the statement of Proposition A.1. The top-left panel shoWs tWo
example PDFs P(X), 9(X) on closed interval [-2, 2]. The bottom-left panel shoWs the optimal
discriminator function f *(x) = p(x)∕(p(x) + q(X)) as a function of X on [-2, 2]. The top-right
panel shows the PDFs [f *]p](t), [f ?q](t) of the pushforward distributions f *]P, f %q induced by
the discriminator mapping f *. f * maps [-2, 2] to [0,1] and [f *]p], [f 'q] are defined on [0,1].
Consider point xi ∈ [-2,2]. The value f *(xi) characterizes the ratio of densities p(xi)/(P(XI) +
q(X1)) at X1. For another point X2 mapped to the same value f*(X2) = f* (X1) = t1,2, the ratio
of densities P(X2)∕(P(X2) + q(X2)) is the same as P(X1)∕(P(X1) + q(X1)). All points X mapped to
t1,2 share the same ratio of the densities P(X)∕(P(X) + q(X)). This fact implies that the ratio of the
pushforward densities [f*]P](t1,2)∕([f*]P](t1,2) + [f *]q](t1,2)) at t1,2 must be the same as the ratio
of densities P(X1)∕(P(X1) + q(X1)) = t1,2 at X1 (or X2). The pushforward PDFs [f*]q](t), [f*]q](t)
satisfy property [f*]P](t)∕([f*]P](t) + [f*]q](t)) = tforallt ∈ supp(f*]P) ∪ supp(f*]q).
Comment. Intuitively this proposition states the following. If for some X ∈ X we have f* (X) = t ∈
[0, 1], t directly corresponds to the ratio of densities not only in the original space t = P(X)∕(P(X) +
q(X)), but also in the 1D discriminator output space t = [f*]P](t)∕([f*]P](t) + [f*]q](t)).
We also provide an intuitive example in Figure A.1.
Proof of Proposition 4.3, statement #1.
=⇒: If DW (P, q) = 0 then P = q, then f*]P = f*]q. Thus, DW(f*]P, f*]q) = 0.
18
Published as a conference paper at ICLR 2022
b：If DW(f *]P,f *M= 0, then f *超=f *必
Consider probability of event {t 11 > 2} under distribution f * ]p.
t>
f*(x) > 1 p(x) dx,
where IH is the indicator function (I[c] equal to 1 when the condition C is satisfied, and equal to 0
otherwise). For all X : p(χ) > 0, We have that f * (x) = .(/+1⑺ and p(χ) + q(χ) > 0. Therefore,
the expression above can be re-written as
1
> 2
t > 1B = / J	P(X)
2∖) =J	[p(x)+ q(x)
p(X) dX =
I[p(X) - q(X) > 0]p(X) dX.
Similarly, the probability of event {t 11 > ∣} under distribution f *]q is
t>
1
2
I[p(X) - q(X) > 0]q(X) dX.
f*]p = f*]qimplies that
pf*]P ({t t > 2})=Pf*]q ({t t > 2
or equivalently
I[p(X) - q(X) > 0](p(X) - q(X)) dX = 0.
Note, that the function I[p(X) - q(X) > 0](p(X) - q(X)) is non-negative for any X. This means
that the integral can be zero only if the function is zero everywhere implying that for any X either
I[p(X) - q(X) > 0] = 0 or p(X) - q(X) = 0. In other words,
p(X) ≤ q(X),	∀ X.
Using the fact the both densities p(X) and q(X) must sum up to 1, we conclude that p = q and
DW (p, q) = 0.
Proof of Proposition 4.3, statement #2.
Note that by (2) and (15), we have
∕*/、	P(X)	+	[f*]p](t)
f(X) = PnTqxy=t=W≡ww,	∀x ∈ Supp(P) ∪ supp(q).
=⇒: Suppose DWβ1,β2 (P, q) = 0. Then supp(P) = supp(q) = S (by Proposition 4.2) and
supp(f*]P) = supp(f*]q) = T by (Theorem 2.1). Moreover, DWβ1,β2 (P, q) = 0 implies
τ⅛ ≤ pX) ≤1+β1,
∀X ∈ S.
Since
PM
f*(x)= (P(X)()= -q(⅛y,	∀X ∈ S,
p(x) + q(X)	1 + %
q(x)
the inequalities above are equivalent to
1
2 + β2
≤ f*(X) ≤
1 + β1
2 + β1
∀X ∈ S.
19
Published as a conference paper at ICLR 2022
Combined with Proposition A.1, the above implies that
1	≤	[f *]p](t)	≤ 1 + βι
2 + β2 ≤ [f*]P](t) + [f*]q](t) ≤ 2 + βι
∀t ∈ T,
or equivalently
1	≤ [f*]P](t)
1 + β2 — [f*]q](t)
Therefore,。歌色(f *]P,f *]q) = 0.
≤ 1 + β1 ,
∀t ∈ T.
u: similarly, When。歌色(f*]P,f *用)=0,
supp(f*]p) = supp(f*]q) = T	=⇒	supp(p) = supp(q) = S.
Moreover,
1
1+12
≤ [f*]P](t)
≤ [f*]q](t)
≤ 1 + β1 ,
养
∀t ∈ T,
1	<________[f *]p] ⑴_______ < 1+ βι
2 + β2 ≤ [f*]P](t) + [f*]q](t) ≤ 2 + β2
∀t ∈ T
养
占 ≤ f *(x) ≤ ∣+f,	∀x ∈ s,
2 + β2	2 + β1
养
ɪ ≤ 里 ≤ 1 + βι,	∀x ∈ S.
1 + β2	q (x)
Therefore, DWβ1,β2 (p, q) = 0.
B A comment on “sliced” SSD divergence
Recent Works (Deshpande et al., 2018; 2019) have proposed to perform optimal transport (OT)-based
distribution alignment by reducing the OT problem (7) in the original, potentially high-dimensional
space, to that betWeen 1D distributions. Specifically, Deshpande et al. (2018) consider the sliced
Wasserstein distance (Rabin et al., 2011):
DSW (p, q)
/
Sn-1
DW (fθ]p,fθ]q)dθ,
(16)
Where Sn-1 = {θ ∈ Rn | kθk = 1} is a unit sphere in Rn, and fθ is a 1D linear projection
fθ (x) = hθ, xi. It is knoWn that DSW is a valid distribution divergence: for anyp 6= q there exists a
linear slicing function fθ , θ ∈ Sn-1 Which identifies difference in the distributions, i.e. fθ]p 6= fθ]q
(Cramer-WOld theorem). By reducing the original OT problem to that in a 1D space, WU et al.
(2019a) and Deshpande et al. (2019) develop efficient practical methods for distribution alignment
based on fast algorithms for the 1D OT problem.
Unfortunately, the straight-forWard extension of SSD divergence (1) to a 1D linearly sliced version
does not provide a valid support divergence.
Proposition B.1. There exist two distributions p and q in P, such that supp(p) 6= supp(q) but
supp(fθ]p) = supp(fθ]q), ∀fθ (x) = hθ, xi with θ ∈ Sn-1.
Proof. Consider a 2-dimensional Euclidean space R2 and let supp(p) = {(x, y)|x2 + y2 ≤ 2} and
supp(q) = {(x, y)|1 ≤ x2 + y2 ≤ 2}. Then, ∀fθ(x) = hθ, xi With θ ∈ S1,
supp(fθ]p) = supp(fθ]q) = [-2, 2].
This counterexample is shown in Figure B.1.	□
20
Published as a conference paper at ICLR 2022
Figure B.1: Visualization of example distributions for Proposition B.1
C	Discussion of “soft” and “hard” assignments with 1D discrete
DISTRIBUTIONS
In Section 4.2 we considered the “soft-assignment” relaxed OT problem (11) and claimed that for
integer β, the set of minimizers of (11) must contain a “hard-assignment” transportation plan, meaning
γij ∈ {0, 1}, ∀i, j. Below we justify this claim.
Note that for β = 0 the OT problem (11) is the standard OT problem for Wasserstein-1 distance (10),
since the inequality constraints Pim=1 γij ≤ 1, ∀j can only be satisfied as equalities. For this problem,
it is known (e.g. see Peyre et al. (2019) Proposition 2.1) that the set of optimal “soft-assignment”
contains a “hard-assignment” represented by a normalized permutation matrix. This fact can be
proven using the Birkhoff-von Neumann theorem. The Birkhoff-von Neumann theorem states that
the set of doubly stochastic matrices
nn
P ∈ Rn×n :	Pij ≥0,∀i,j,	XPij = 1,∀i,	XPij = 1,∀j
is exactly the set of all finite convex combinations of permutation matrices. In the context of the
linear program (11) with β = 0, the Birkhoff-von Neumann theorem means that all extreme points
of the polyhedron Γβ (op, oq) are hard-assignment matrices. Therefore, by the fundamental theorem
of linear programming (Bertsimas & Tsitsiklis, 1997), the minimum of the objective is reached at a
“hard-assignment” matrix.
We argue that a similar result holds for the case of integer β > 0. In this case, the matrices in
Γβ (op, oq) can not be associated with the doubly stochastic matrices, since constraints on of the
marginals of γ are relaxed to inequality constraints. Because of that, the Birkhoff-von Neumann the-
orem can not be applied. However, Budish et al. (2009) provide a generalization of the Birkhoff-von
Neumann theorem (Theorem 1 in (Budish et al., 2009)) which applies to the cases where the equality
constraints are replaced with integer-valued inequality constraints (recall that we consider integer β).
Using this generalized result, our claim can be proven by performing the following steps.
Clearly, the polyhedron Γβ(op, oq) contains all “hard-assignment” matrices and all their finite convex
combinations. The result proven in (Budish et al., 2009) implies that each element of Γβ(op, oq) can
be represented as a finite convex combination of “hard-assignment” matrices. Thus, the polyhedron
Γβ (op, oq) is exactly the set of all finite convex combinations of “hard-assignment” matrices and
all extreme points of the polyhedron are “hard-assignment” matrices. Finally, by analogy with the
case of β = 0, we invoke the fundamental theorem of the linear programming and conclude that the
minimum of the objective (11) is reached at γ corresponding to a “hard-assignment” matrix.
21
Published as a conference paper at ICLR 2022
D Experiment details
D. 1 USPS to MNIST experiment specifications
We use USPS (Hull, 1994) and MNIST (LeCun et al., 1998) datasets for this adaptation problem.
Following Tachet des Combes et al. (2020) we use LeNet-like (LeCun et al., 1998) architecture for
the feature extractor with the 500-dimensional feature representation. The classifier consists of a
single linear layer. The discriminator is implemented by a 3-layer MLP with 512 hidden units and
leaky-ReLU activation.
We train all methods for 65 000 steps with batch size 64. We train the feature extractor, the classifier,
and the discriminator with SGD (learning rate 0.02, momentum 0.9, weight decay 5 ∙ 10-4). We
perform a single discriminator update per 1 update of the feature extractor and the classifier. After the
first 30 000 steps we linearly anneal the feature extractor’s and classifier’s learning rates for 30 000
steps to the final value 2 ∙ 10-5.
The feature extractor’s loss is given by a weighted combination of the cross-entropy classification
loss on the labeled source example and a domain alignment loss computed from the discriminator’s
signal (recall that different method use different forms of the alignment loss). The weight for the
classification term is constant and set to λcls = 1. We introduce schedule for the alignment weight
λalign. For all alignment methods we linearly increase λalign from 0 to 1.0 during the first 10000 steps.
For ASA we use history buffers of size 1000.
D.2 STL to CIFAR experiment specifications
We use STL (Coates et al., 2011) and CIFAR-10 (Krizhevsky, 2009) for this adaptation task. STL
and CIFAR-10 are both 10-class classification problems. There are 9 common classes between the
two datasets. Following Shu et al. (2018) we create a 9-class classification problem by selecting the
subsets of examples of the 9 common classes.
For the feature extractor, we adapt the deep CNN architecture of Shu et al. (2018). The feature
representation is a 192-dimensional vector. The classifier consists of a single linear layer. The
discriminator is implemented by a 3-layer MLP with 512 hidden units and leaky-ReLU activation.
We train all methods for 40 000 steps with batch size 64. We train the feature extractor, the classifier,
and the discriminator with ADAM (Kingma & Ba, 2014) (learning rate 0.001, β1 = 0.5, β2 = 0.999,
no weight decay). We perform a single discriminator update per 1 update of the feature extractor and
the classifier.
The weight for the classification loss term is constant and set to λcls = 1. For all alignment methods
we use constant alignment weight λalign = 0.1.
For ASA we use history buffers of size 1000.
Conditional entropy loss. Following (Shu et al., 2018) we use auxiliary conditional entropy loss on
target examples for domain adaptation methods. For a classifier Cφ : Z → Y and a feature extractor
Fθ : X → Z where classifier outputs the distribution over class labels {1, . . . , K}
K
Cφ(z)∈RK:	Cφ(z)k≥0, XCφ(z)k=1,
k=1
the conditional entropy loss on target examples {xiq}iN=q1 is given by
1 Nq	K
Lent = λent ∙ Nq X - X [C"Fθ (Xq))] k log [C"Fθ (Xq))] k ∙	(17)
i=1	k=1
λent is the weight of the conditional entropy loss in the total training objective. This loss acts as an
additional regularization of the embeddings of the unlabeled target examples: minimization of the
conditional entropy pushes target embeddings away from the classifier’s decision boundary.
For all domain adapation methods we use the conditional entropy loss (17) on target examples with
the weight λent = 0.1.
22
Published as a conference paper at ICLR 2022
D.3 Extended experimental results on STL to CIFAR
Effect of conditional entropy loss. In order to quantify the improvements of the support alignment
objective and the conditional entropy objective in separation, we conduct an ablation study. In
addition to the results reported in Section 5, we evaluate all domain adaptation methods (except
VADA which uses the conditional entropy in the original implementation) on STL→CIFAR task
without the conditional entropy loss (λent = 0). The results of the ablation study are presented in
Table D.1. We observe that the effect of the auxiliary conditional entropy is essentially the same for
all methods across all imbalance levels: with λent = 0.1 the accuracy either improves (especially
the average class accuracy) or roughly stays on the same level. The relative ranking of distribution
alignment, relaxed distribution alignment, and support alignment methods is the same with both
λent = 0 and λent = 0.1. The results demonstrate that the benefits of support alignment approach and
conditional entropy are orthogonal.
Table D.1: Results of ablation experiments of the effect of auxiliary conditional entropy loss on
STL→CIFAR data. Same setup and reporting metrics as Table 1.
α = 0.0
α = 1.0
α = 1.5
α = 2.0
Algorithm	λent	average	min	average	min	average	min	average	min
DANN	0.0	74 6 75.1 74.6 74.1	55.0 51.5 49.9	68.4 6697..20	43.2 41.7	65.7 6652..98	36.2 35.5 29.6	62 5 64.6 62.5 60.0	27.5 2275..57
DANN	0.1	75 3 75.4 75.3 74.9	54 6 56.6 54.6 54.2	69.9 6708..16	44.8 40.7	64.9 6673..17	34.9 3363..89	63 3 64.8 63.3 57.4	27.0 2218.52 21.2
IWDAN	0.0	70.4 7700..72	47.2 4486..08	68.6 6688..84	43.6 43.2	66.7 6676..90	44.7 4463..23	63.9 6662..19	36.5 3372..37
IWDAN	0.1	69.9 6709..79	50.5 4507..69	68.7 6698..16	45.8 4504..58	67 1 67.3 67.1 65.9	44 7 44.8 44.7 40.4	64.4 6643..96	36.8 3374..95
IWCDAN	0.0	70 1 70.8 70.1 70.0	50.5 4509..81	68.6 6698..42	44 2 45.8 44.2 41.2	66 0 66.0 66.0 65.9	45 0 47.8 45.0 43.7	63.8 6642..13	37 3 37.7 37.3 33.6
IWCDAN	0.1	70 1 70.2 70.1 70.1	49.3 47.8 42.4	69.4 6699..41	47.1 4516..33	66.1 6675..20	39.9 3407..87	64 5 65.1 64.5 63.9	37.0 3405..25
sDANN-4	0.0	69.4 6708..08	46.5 4495..71	69.6 6699..73	49.1 4497..24	68.0 6687..68	48.2 4482..86	66 3 66.4 66.3 64.2	40.7 3426..96
sDANN-4	0.1	71.8 7721..17	52.1 5522..81	71.1 7710..74	49 9 51.8 49.9 48.ι	69.4 6708..07	48.6 4493..05	66 4 67.9 66.4 66.2	39.0 3473..16
ASA-sq	0.0	69.9 6709..39	48.0 4506..16	68.8 6688..96	47 3 49.3 47.3 45.3	68.1 6687..72	45.4 4475..82	65.7 6665..46	43.6 4451..03
ASA-sq	0.1	71.7 7711..97	52.9 4536..47	70.7 7710..04	51 6 52.7 51.6 46.8	69.2 6699..32	45 6 52.0 45.6 43.3	68.1 6687..22	45.9 44.7 39.8
ASA-abs	0.0	69.8 6708..09	45.7 4485..04	68.4 6688..64	44.3 4464..80	67.9 6687..10	48.4 46.6 40.4	66.3 6665..97	41.6 4440..93
ASA-abs	0.1	71.6 7711..72	49.0 4538..54	70.9 7710..08	49.2 4507..03	69.6 6699..96	43 2 49.5 43.2 42.1	67.8 6686..26	40.9 3495..04
Comparison with optimal transport based baselines.
We provide additional experimental results comparing our method with OT-based methods for domain
adaptation. We implement two OT-based methods which we describe below.
• The first method is a variant of the max-sliced Wasserstein distance (which was proposed for
GAN training by Deshpande et al. (2019)) for domain adaptation. In the table below we refer
to this method as DANN-OT. In our implementation DANN-OT minimizes the Wasserstein
distance between the pushforward distributions g；p, g；q induced by the optimal log-loss
discriminator g* * (4). As discussed in Section 4.2 (paragraph “Distribution alignment”)
the computation of the Wasserstein distance between 1D distributions can be implemented
efficiently via sorting.
• The second method is an OT-based variant of DANN which uses a dual Wasserstein dis-
criminator instead of the log-loss discriminator. In the table below we refer to this method
as DANN-WGP. This method minimizes the Wasserstein distance in its dual Kantorovich
form. We train the discriminator with the Wasserstein dual objective and a gradient penalty
proposed to enforce Lipshitz-norm constraint (Gulrajani et al., 2017).
We present the evaluation results of the OT-based methods on STL→CIFAR domain adaptation task
in the Table D.2. Note that the OT-based methods aim to enforce distribution alignment constraints.
We observe that the OT-based methods follow the same trend as DANN: they deliver improved
accuracy compared to No DA in the balanced setting, but suffer in the imbalanced settings (α > 0)
due to their distribution alignment nature.
We would also like to make a comment on OT-based relaxed distribution alignment. Wu et al.
(2019b) propose method WDANN-β which minimizes the dual form of the asymmetrically-relaxed
23
Published as a conference paper at ICLR 2022
Table D.2: Results of comparison with optimal transport based methods on STL→CIFAR data. Same
setup and reporting metrics as Table 1.
α = 0.0
α = 1.0
α = 1.5
α = 2.0
Algorithm	average	min	average	min	average	min	average	min
No DA	69 9 70.0 69.9 69.8	49.8 4505..63	68.8 6689.33 . 68.3	47.2 45.2	66.8 6676..24	46.0 4475..08	65 8 66.7 65.8 64.8	43 7 44.6 43.7 41.6
DANN	75 3 75.4 75.3 74.9	54.6 5564..62	69 9 70.1 69.9 68.6	44.8 40.7	64.9 6673..17	34.9 3363..89	63 3 64.8 63.3 57.4	27.0 2281..52
DANN-OT	76.0 7765..08	55.5 55.2 54.3	67 7 68.9 67.7 67.1	43.0 3436..75	64.5 6650..19	34.4 2349..63	61 3 62.0 61.3 54.4	24.3 2253..52
DANN-WGP	74.8 7754..17	54.4 53.5 53.3	67 7 67.9 67.7 65.3	38.6 44.4	63.3 5637..41	27.0 2326..43	59 0 61.8 59.0 54.3	21.9 1228..56
sDANN-4	71.8 7721..17	52.1 5522..81	71 1 71.7 71.1 70.4	49.9 58.8	69.4 6708..07	48.6 4493..05	66.4 6676..92	39.0 3473..16
ASA-sq	71 7 71.9 71.7 71.7	52.9 4536..47	70.7 7710..04	51.6 56.8	69.2 6699..32	45 6 52.0 45.6 43.3	68.1 6687..22	44.7 3459..98
ASA-abs	71.7 71.6 71.2	49.0 4538..54	70.9 7710..08	49.2 57.3	69.6 6699..96	43 2 49.5 43.2 42.1	67.8 6686..26	40.9 3495..04
Wasserstein distance. However, they observe that sDANN-β outperforms WDANN-β in experiments.
Hence, we use sDANN-β as a relaxed distribution alignment baseline in our experiments.
Effect of alignment weight. We provide additional experimental results comparing ASA with
DANN and VADA across different values of the alignment loss weight λalign on STL→CIFAR task.
The results are shown in Table D.3. DANN with a higher alignment weight λalign = 1.0 performs
better in the balanced (α = 0) setting and worse in the imbalanced (α > 0) setting compared to
a lower weight λalign = 0.1, as the distribution alignment constraint is enforced stricter. VADA
optimizes a combination of distribution alignment + VAT (virtual adversarial training) objectives
(Shu et al., 2018), and we observe the same trend: with lower alignment weight λalign = 0.01, VADA
performs worse in the balanced setting and better in the imbalanced setting compared to a higher
weight λalign = 0.1. Weight λalign = 0.1 is a middle ground between having poor performance in the
imbalanced setting (λalign = 1.0) and not sufficiently enforcing distribution alignment (λalign = 0.01).
The role of VAT (similarly to that of conditional entropy loss) is orthogonal to alignment objectives.
Thus, we provide additional evaluations of combining support alignment and VAT (the “ASA-sq
+ VAT” entry in Table D.3) with alignment weight λalign = 1.0. The good performance of such
combination shows that:
•	One could improve our current support alignment performance by using auxiliary objectives.
•	Support alignment based method performs qualitatively different from distribution alignment
based method, since the performance holds with stricter support alignment while distri-
bution alignment needs to loosen the constraints considerably to reduce the performance
degradation in the imbalanced setting.
Table D.3: Results of comparison of ASA with DANN and VADA across different values of the
alignment loss weight λalign on STL→CIFAR data. Same setup and reporting metrics as Table 1.
Algorithm	λalign	α=	0.0	α=	1.0	α=	1.5	α=	2.0
		average	min	average	min	average	min	average	min
DANN	0.01	72.3 7722..72	49 5 50.8 49.5 48.8	70.6 79.7	48.9 4511..25	68.5 6687..72	46.1 3506..02	65 9 66.0 65.9 64.1	36 7 39.4 36.7 29.9
DANN	0.1	75.3 7754..49	54 6 56.6 54.6 54.2	69.9 6708..16	44.8 4450..17	64.9 6673..17	34.9 3363..89	63 3 64.8 63.3 57.4	27.0 2281..52
DANN	1.0	77.2 7776..38	58 5 59.4 58.5 56.7	66 3 66.8 66.3 64.5	37.9 3417..65	62.8 5636..31	28.9 27.5 24.6	58.7 5592..73	18.5 1207..52
VADA	0.01	74 4 74.4 74.4 74.2	54.2 5552..46	71 7 71.7 71.7 71.7	51.6 4525..00	69 5 69.7 69.5 68.4	47 5 49.8 47.5 40.0	65 9 66.1 65.9 64.8	39.4 37.2 35.3
VADA	0.1	76 7 76.7 76.7 76.6	56.9 5583..35	706 71.0 70.6 70.0	47 7 48.8 47.7 44.0	66.1 6665..54	39.3 35.7 33.3	63.2 6640..72	28.0 25.5 25.2
ASA-sq	0.1	71 7 71.9 71.7 71.7	52 9 53.4 52.9 46.7	7∩ 7 71.0 70.7 70.4	51 6 52.7 51.6 46.8	69.2 6699..32	45.6 4523..03	68.1 6687..22	44 7 45.9 44.7 39.8
ASA-sq + VAT	1.0	74 2 74.5 74.2 74.0	52.5 52.2 51.9	72.2 72.2 7i.9	53.6 53.5 45.4	70.6 7700..84	48.9 4525..36	67.4 6676..78	43 0 46.0 43.0 39.4
24
Published as a conference paper at ICLR 2022
D.4 VisDA- 1 7 experiment specifications
We use train and validation sets of the VisDA-17 challenge (Peng et al., 2017).
For the feature extractor we use ResNet-50 He et al. (2016) architecture with modified output size of
the final linear layer. The feature representation is 256-dimensional vector. We use the weights from
pre-trained ResNet-50 model (torchvision model hub) for all layers except the final linear layer.
The classifier consists of a single linear layer. The discriminator is implemented by a 3-layer MLP
with 1024 hidden units and leaky-ReLU activation.
We train all methods for 50 000 steps with batch size 36. We train the feature extractor, the classifier,
and the discriminator with SGD. For the feature extractor we use learning rate 0.001, momentum 0.9,
weight decay 0.001. For the classifier we use learning rate 0.01, momentum 0.9, weight decay 0.001.
For the discriminator we use learning rate 0.005, momentum 0.9, weight decay 0.001. We perform a
single discriminator update per 1 update of the feature extractor and the classifier. We linearly anneal
the feature extractor’s and classifier’s learning rate throughout the training (50 000) steps. By the end
of the training the learning rates of the feature extractor and the classifier are decreased by a factor of
0.05.
The weight for the classification term is constant and set to λcls = 1. We introduce schedule for the
alignment weight λalign. For all alignment methods we linearly increase λalign from 0 to 0.1 during
the first 10000 steps. For all methods we use auxiliary conditional entropy loss on target examples
with the weight λent = 0.1.
For ASA we use history buffers of size 1000.
D.5 History size effect and evaluation of support distance
History size effect. To quantify the effects of mini-batch training mentioned in Section 3, we explore
different sizes of history buffers on USPS→MNIST task with the label distribution shift α = 1.5.
The results are presented in Figure D.1 and Table D.4. Figure D.2 shows the distributions of outputs
of the learned discriminator at the end of the training. While without any alignment objectives
neither the densities nor the supports of gψ]pθZ and gψ]qZθ are aligned, both alignment methods
approximately satisfy their respective alignment constraints. Compared with DANN results, ASA
with small history size performs similarly to distribution alignment, while all history sizes are enough
for support alignment. We also observe the correlation between distribution distance and target
accuracy: under label distribution shifts, the better distribution alignment is achieved, the more target
accuracy suffers. Note that with too big history buffers (e.g. n = 5000), we observe a sudden drop
in performance and increases in distances. We hypothesize that this could be caused by the fact
that the history buffer stores discriminator output values from the past steps while the discriminator
parameters constantly evolve during training. As a result, for a large history buffer, the older items
might no longer accurately represent the current pushforward distribution as they become outdated.
——ASA
OOOOO
8 6 4 2
(氏)Kοτ3,mοοτ3 ss-ɔ IUniUl≡
100 500 1000 2000 5000
ASA history size
Q 10-1
——DANN ……NoDA
102
0	100 500 1000 2000 5000
ASA history size
əɔu亚-P-Oddns
100-
10-1-
5 ∙ 10-2∙
10-2
100 500 1000 2000 5000
ASA history size
Figure D.1: Evaluation of history size effect for ASA on MNIST→USPS with the label distribu-
tion shift (α = 1.5). The panels show (left to right): minimum class accuracy on target test set;
Wasserstein distance DW(gψ]pθZ, gψ]qZθ ) between the pushforward distributions of source and target
representations induced by the discriminator; SSD divergence D4 (gψ]pθZ , gψ]qZθ ) between the push-
forward distributions. In each panel the dashed lines show the respective quantities for “No DA” and
DANN methods.
25
Published as a conference paper at ICLR 2022
Direct evaluation of support distance. In order to directly evaluate the ability of ASA (with history
buffers) to enforce support alignment, we consider the setting of the illustrative experiment described
in Section 5 (3-class USPS→MNIST adaptation with 2D feature extractor, α = 1.5). We compare
methods No DA, DANN, and ASA-abs (with different history buffer sizes). For each method we
consider the embedding space of the learned feature extractor at the end of training and compute
Wasserstein distance DW (pθZ, qZθ ) and SSD divergence D4(pθZ, qZθ ) between the embeddings of
source and target domain (note that we compute the distances in the original embedding space directly
without projecting data to 1D with the discriminator). To ensure meaningful comparison of the
distances between different embedding spaces, we apply a global affine transformation for each
embedding space: we center the embeddings so that their average is 0 and re-scale them so that their
average norm is 1. The results of this evaluation are shown in Table D.5. We observe that, compared
to no alignment and distribution alignment (DANN) methods, ASA aligns the supports without
necessarily aligning the distributions (in this imbalanced setting, distribution alignment implies low
adaptation accuracy).
Table D.4: Analysis of effect history size parameter for ASA on USPS→MNIST with class label
distribution shift corresponding to α = 1.5. We report distribution and support distances between the
pushforward distributions gψ]pθZ and gψ]qZθ , as well as the value of discriminator’s log-loss.
Method	History size	Target accuracy (%)		Distribution distances		Log-loss
		average	min	DW (gψ]pZ ,gψ]qZ)	D4(gψ]pθZ, gψ]qZθ)	
No DA	—	71.28 7721..2515	27 46 37.26 27.46 24.21	307.56 272.03	40.35 4362..1060	00 05 00.07 00.05 00.04
DANN	—	69.96 7613..8259	01.11 0010..9539	0O ιι 00.11 00.11 00.10	00 00 00.00 00.00 00.00	00 65 00.65 00.65 00.65
ASA-abs	0	62 75 64.35 62.75 61.78	19.36 2137..9630	01 07 01∙15 01.07 00.99	00 01 00.01 00.01 00.00	00 57 00.58 00.57 00.56
ASA-abs	100	80.58 8718..2732	35.09 4342..1370	02 64 02.70 02.64 02.15	00 00 00.00 00.00 00.00	00 53 00.53 00.53 00.52
ASA-abs	500	92.02 9920..5766	76.96 8730..9724	06.21 0065..6489	00 00 00.01 00.00 00.00	00 45 00.45 00.45 00.45
ASA-abs	1000	92.54 9920..9930	82.41 8754..5433	08.06 0087..9197	00 01 00.02 00.01 00.01	00.41 0000..4410
ASA-abs	5000	86.03 8874..8506	62.19 7416..9628	29.23 巡	00 05 00.08 00.05 00.05	00.29 0000..2309
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
口 gttp (source)
口 g&q (target)
-30 -20-10 0 10 20 30
(a) No DA
(b) DANN
(c) ASA (n = 0)
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
(d) ASA (n = 1000)
Figure D.2: Kernel density estimates (in the discriminator output space) of gψ]pθZ, gψ]qZθ at the end
of the training on USPS→MNIST task with α = 1.5. n is the size of ASA history buffers.
26
Published as a conference paper at ICLR 2022
Table D.5: Results of No DA, DANN, and ASA-abs (with different history sizes) on 3-class
USPS→MNIST adaptation with 2D feature extractor and label distribution shift corresponding
to α = 1.5. We report average and minimum target class accuracy, as well as Wasserstein distance
DW and support divergence D4 between source pθZ and target qZθ 2D embedding distributions. We
report median (the main number), and 25 (subscript) and 75 (superscript) percentiles across 5 runs.
Algorithm	History size	Accuracy (avg)	Accuracy (min)	DW (pθZ , qZθ )	D4 (pθZ , qZθ )
No DA	—	63 0 69.6 63.0 62.3	45.3 37.9	-	0.78 0000..8745	0.10 00..1100
DANN	—	75.6 7832..74	54.8 45.6	0 07 0.08 0.07 0.06	0 02 0.02 0.02 0.02
ASA-abs	0	73.9 7843..14	61.8 54.4	0 23 0.47 0.23 0.22	0 03 0.03 0.03 0.03
ASA-abs	100	88.5 8956..18	71 4 93.3 71.4 70.6	0.54 00..3566	0 03 0.03 0.03 0.03
ASA-abs	500	94.5 8948..77	89.0 90.3 . 83.1	0.59 00..6545	0 03 0.03 0.03 0.03
ASA-abs	1000	91.1 9931..01	85.6 8860..27	0.59 00..6525	0 03 0.03 0.03 0.03
ASA-abs	2000	94.0 9941..72	88.6 8890..42	0 62 0.66 0.62 0.58	0 03 0.03 0.03 0.03
ASA-abs	5000	83.9 82.1 81.8	68.9 75.9	0 64 0.67 0.64 0.63	0 04 0.04 0.04 0.04
27