Published as a conference paper at ICLR 2022
Learning more skills through
OPTIMISTIC EXPLORATION
DJ Strouse； Kate Baumli, David Warde-Farley, Vlad Mnih, Steven Hansen*
DeepMind
{strouse, baumli, dwf, vmnih, stevenhansen}@google.com
Ab stract
Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al.,
2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic
rewards. They work by simultaneously training a policy to produce distinguishable
latent-conditioned trajectories, and a discriminator to evaluate distinguishability
by trying to infer latents from trajectories. The hope is for the agent to explore
and master the environment by encouraging each skill (latent) to reliably reach
different states. However, an inherent exploration problem lingers: when a novel
state is actually encountered, the discriminator will necessarily not have seen
enough training data to produce accurate and confident skill classifications, leading
to low intrinsic reward for the agent and effective penalization of the sort of
exploration needed to actually maximize the objective. To combat this inherent
pessimism towards exploration, we derive an information gain auxiliary objective
that involves training an ensemble of discriminators and rewarding the policy for
their disagreement. Our objective directly estimates the epistemic uncertainty
that comes from the discriminator not having seen enough training examples,
thus providing an intrinsic reward more tailored to the true objective compared
to pseudocount-based methods (Burda et al., 2019). We call this exploration
bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate
empirically that DISDAIN improves skill learning both in a tabular grid world (Four
Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage
researchers to treat pessimism with DISDAIN.
1	Introduction
Reinforcement learning (RL) has proven itself capable of learning useful skills when clear task-
specific rewards are available (OpenAI et al., 2019a;b; Vinyals et al., 2019). However, truly intelligent
agents should, like humans, be able to learn even in the absence of supervision in order to acquire
repurposable task-agnostic knowledge. Such unsupervised pre-training has seen recent success in
language (Radford et al., 2019; Brown et al., 2020) and vision (Chen et al., 2020a;b), but its potential
has yet to be fully realized in the learning of behavior.
The most promising class of algorithms for unsupervised skill discovery is based upon maximizing the
discriminability of skills represented by latent variables on which a policy is conditioned. Objectives
are typically derived from variational approximations to the mutual information between latent
variables and states visited (Gregor et al., 2016; Eysenbach et al., 2019; Warde-Farley et al., 2019;
Hansen et al., 2020; Baumli et al., 2021), employing a learned parametric skill discriminator. Both
the policy and discriminator have the objective of strong predictive performance by the discriminator,
though the discriminator is trained with supervised learning rather than RL. The end result is a policy
capable of producing consistently distinguishable behavioral motifs, or “skills.” These skills can be
evaluated zero-shot, fine-tuned, or composed in a hierarchical RL setup to maximize task reward
when one is introduced (Eysenbach et al., 2019; Hansen et al., 2020). Unsupervised skill learning
objectives can also be maximized in conjunction with task reward, in order to promote robustness of
learned behavior to environment perturbations (Mahajan et al., 2019; Kumar et al., 2020).
* equal contribution
1
Published as a conference paper at ICLR 2022
The degree to which unsupervised skill discovery methods are useful in such downstream applications
depends on how many skills they are able to learn. Indeed, Eysenbach et al. (2019) showed that
as more skills are learned, the performance obtained using the learned skills in a hierarchical
reinforcement learning setup improves. In follow up work, Achiam et al. (2018) showed that methods
like DIAYN can struggle to learn large numbers of skills and proposed gradually increasing the
number of skills according to a curriculum to make skill discovery easier.
The aim of this work is to improve the ability of skill discovery methods to learn more skills. We
highlight an exploration problem intrinsic to the entire class of variational skill discovery algorithms
which can inhibit discovery of new skills. During skill learning, the policy will necessarily need to
explore new states of the environment. The discriminator must then make latent predictions for states
it has never seen before, resulting in incorrect and/or low-confidence predictions. The policy will in
turn be penalized for this poor discriminator performance, and discouraged from seeking out new
states. We refer to this problem as “pessimistic exploration” and describe it further in section 2.
To motivate our solution, we argue that it is important for the policy to distinguish between two
kinds of uncertainty in the discriminator - aleatoric uncertainty that comes from the policy producing
similar trajectories for different skills, and epistemic uncertainty that comes from a lack of training
data. The former indicates poor policy performance, but the latter is in fact desirable, and serves as a
signal for potential discriminator learning. Unfortunately, skill discovery algorithms treat both types
of uncertainty the same and thus ignore this important signal. We propose to capture it.
Our primary contribution, which we present in section 3, is an exploration bonus tailored to skill
discovery algorithms, designed to overcome pessimistic exploration. We identify states of high
epistemic uncertainty in the discriminator by training an ensemble of discriminators and measuring
their disagreement. We call this exploration bonus discriminator disagreement intrinsic reward, or
DISDAIN. Intuitively, the ensemble members may disagree in novel states, but must come to agree
in frequently visited ones. More formally, we derive DISDAIN from a Bayesian perspective that 1)
represents the posterior over discriminator parameters using an ensemble, and 2) encourages the policy
to maximize information gain (i.e. reduce uncertainty) about the parameters of the discriminator.
In section 4, we demonstrate empirically that DISDAIN improves skill learning over unbonused
skill discovery algorithms in both an illustrative grid world (Four Rooms, Sutton et al. (1999)) and
the Atari 2600 learning environment (ALE, Bellemare et al. (2013)) more so than augmenting with
popular exploration bonuses not tailored to skill discovery.
2	Unsupervised skill learning through variational infomax
2.1	Introduction
We now formalize our setting of interest: unsupervised skill learning through variational information
maximization (Gregor et al., 2016; Eysenbach et al., 2019; Warde-Farley et al., 2019; Hansen et al.,
2020; Baumli et al., 2021). We consider a Markov decision process (MDP), defined by the tuple
M = (S , A, pE , ρ, r, γ), where S and A are state and action spaces, respectively, the environment
dynamics pE(s0 | s, a) specifies the probability of transitioning to state s0 ∈ S when taking action
a ∈ A in state s ∈ S, ρ(s) denotes the probability of starting an episode in state s, and γ ∈ [0, 1) is a
discount factor. Since we focus on unsupervised training, we ignore the environmental reward r.
Our agents seek to learn a repertoire of skills, indexed by the latent variable Z and represented
by the policy πθ (a | s, z), which is parameterized by θ and maps from states and latent variables
to distributions over actions. The latent variables are sampled Z 〜P(Z) at the beginning of each
trajectory and then fixed, so each z represents a temporally extended behavior. The skill trajectory
length T may differ from the episode length, thus a new skill might be resampled within an episode.
For conciseness, we will denote trajectories sampled from the policy by T 〜∏(z) when conditioning
on a particular skill z, and T 〜 ∏ when collecting trajectories across skills. To simplify our
discussion, and because it is the most common case in practice, we will assume that Z is categorical
with cardinality NZ, although much of the discussion carries over to continuous Z.
A large and growing class of objectives for unsupervised skill discovery are derived from maximizing
the mutual information between the latent skill Z and some feature of the resulting trajectories O(T):
F(θ) ≡	I(Z,O)	=	H(Z)- H(Z |	O)=Ez〜p(z),τ〜∏(z) [logp(z	|	o(τ))	- logp(z)] .	(1)
2
Published as a conference paper at ICLR 2022
For example, variational intrinsic control (VIC, Gregor et al. (2016)) maximizes I(Z; S0 , ST ) - the
mutual information between the skill and initial and final states (oT = (s0, sT )).1 Diversity is all
you need (DIAYN, Eysenbach et al. (2019)), on the other hand, maximizes I(Z, S) - the mutual
information between the skill and each state along the trajectory (ot = st for t ∈ 1 : T). Intuitively,
VIC produces skills that vary in the destination reached (without regard for the path taken), while
DIAYN produces skills that vary in the path taken (with less emphasis on the destination reached).
In practice, maximizing equation 1 is not straightforward, because it requires calculating the condi-
tional distribution p(Z | O). In general, it is necessary to estimate it with a learned parametric model
qφ(Z | O). We refer to this model as the discriminator, since it is trained to discriminate between
skills. Fortunately, replacing p with q still yields a lower bound on F(θ) (Barber and Agakov, 2004),
and we may instead maximize the proxy objective F(θ):
F(θ) ≥ F(θ) = Ez〜p(z),τ〜∏(z) [log qφ(Z | o(τ)) - logp(z)].	(2)
Optimizing F(θ) with respect to the policy parameters θ corresponds to RL on the reward:
rskill = logqφ(z | o) - logp(z) .	(3)
Since the intent is for the agent to learn a full repertoire of skills, the skill prior p(z) is typically
fixed to be uniform (Eysenbach et al., 2019; Achiam et al., 2018; Baumli et al., 2021), in which
case - logp(z) = log NZ. If the discriminator simply ignores the trajectory and guesses skills
uniformly as well, then log qφ(z | o) = - log NZ and the reward will be zero. If the discriminator
instead guesses perfectly, then log qφ(z | o) = 0, and the reward will be log NZ. More generally, the
expected reward is an estimate of the logarithm of the effective number of skills. Thus, measuring the
reward in bits (i.e. using log2 in equation 3), we can estimate the number of skills learnt as:
nskills = 2E[rskill] .	(4)
We adopt this quantity as our primary performance metric for our experiments.
To make sure the bound in equation 2 is as tight as possible, the discriminator qφ(Z | O) must also
be fit to its target p(Z | O) through supervised learning (SL) on the negative log likelihood loss:
L(φ) ≡ -Ez〜p(z),τ〜∏(z) [log qφ(Z 1 O(T))] .	(5)
The loss is minimized, and the bound in equation 2 tight, when qφ(Z | O) = p(Z | O).
The joint optimization of F(θ) by RL and L(φ) by SL forms a cooperative communication game
between policy and discriminator. The agent samples a skill Z 〜P(Z) and generates the “message”
τ 〜∏(z). The discriminator receives the message T and attempts to decode the original skill z.
When the policy produces trajectories for different skills that do not overlap in the features O(τ),
the discriminator will easily learn to label trajectories, and when the discriminator makes accurate
and confident predictions, the reward in equation 3 will be high. Ideally, the end result is a policy
exhibiting a maximally diverse set of skills. This joint-training system is depicted in Figure 2a.
2.2	Pessimistic exploration in unsupervised s kill discovery
A conflict arises between the exploration necessary for skill diversification and rewards supplied
by an imperfect discriminator, trained only on past policy-generated experience. Without data (and
in the absence of perfect generalization), the discriminator is likely to make poor predictions when
presented with trajectories containing previously unseen states, resulting in low reward for the policy.
Importantly, this penalization occurs regardless of whether the policy produces distinguishable
skill-conditioned trajectories if the current discriminator is a locally poor approximation to p(Z | O)
for a region of state space represented in O. We note that this is distinct from issues of pessimism
in exploration that arise more generally, including with stationary reward functions (Osband et al.,
2019), wherein naive exploration strategies fail to adequately position an agent for further acquisition
of information. In the scenario we examine here, the agent’s sole source of supervision directly
sabotages the learning process when the discriminator extrapolates poorly.
1More accurately, Gregor et al. (2016) conditioned on initial state and used I(Z, ST | S0) = H(Z | S0) -
H(Z | S0, ST). However, it has subsequently become common not to condition the skill sampling distribution
on s0 (Eysenbach et al., 2019), in which case H(Z | S0) = H(Z) and I(Z, ST | S0) = I(Z; S0, ST).
3
Published as a conference paper at ICLR 2022
(a) Past trajectories.
(b) AIeatoriC uncertainty arises when
different skills visit similar states.
Figure 1: The pessimistic exploration problem. Because skill discovery objectives do not distin-
guish between aleatoric and epistemic uncertainty, they penalize exploration.
T 〜π (z1)
q(Z | τ) = zι vs z2?
(C) Epistemic uncertainty arises
from exploring novel states.
We argue that in order to overcome this inherent pessimism, We must distinguish between two kinds
of uncertainty in the discriminator: aleatoric uncertainty (figure 1b) that is due to the policy producing
overlapping skills (i.e. high H(Z | O)), and epistemic uncertainty (figure 1c) that is due to a lack of
training data (i.e. poor match between qφ(Z | O) and p(Z | O)). Naively, both kinds of uncertainty
contribute to low reward for the policy (equation 3), but while reduction of aleatoric uncertainty
requires changes by the policy, epistemic uncertainty can be reduced (and thus reward increased)
simply by creating more data (i.e. visiting the same states again). Thus, we argue that we should
“reimburse” the policy for epistemic uncertainty in the discriminator, and in fact encourage the policy
to visits states of high epistemic uncertainty.
Put another way, when the policy only maximizes the reward of equation 3, it maximizes the lower
bound F(θ) ≤ F(θ) without regard for its tightness, despite that a looser bound means a more
pessimistic reward. The job of keeping the bound tight is left entirely to the discriminator (equation 5).
By incentivizing the policy to visit states of high epistemic uncertainty, valuable training data is
provided to the discriminator that allows it to better approximate its target and close the gap between
F(θ) and F(θ). In this way, we encourage the policy to help keep the bound F(θ) ≤ F(θ) tight.
In the next section, we formalize the intuitions outlined in the last two paragraphs. The result is an
exploration bonus measured as the disagreement among an ensemble of discriminators.
3	DISDAIN: Discriminator Disagreement Intrinsic Reward
To formalize the notion of discriminator uncertainty, we take a Bayesian approach and replace the
point estimate of discriminator parameters φ with a posterior p(φ). Maintaining a posterior allows us
to quantify the information gained about those parameters. Specifically, we will incentivize the policy
to produce trajectories in which observing the paired skill label z provides maximal information
about the discriminator parameters φ:
I(Z; Φ | O) = H(Z | O) - H(Z | O, Φ) .	(6)
Rewriting the entropies as expectations over trajectories, we have:
I(Z; Φ | O) = Ein H
where we have used that p(φ) does not depend on the present trajectory and so p(φ | o(τ)) = p(φ).
Note that unlike in equations 2 and 5, the expectation over trajectories is not skill-conditioned. The
marginalization over Z happens in the entropy and is over the discriminator’s posterior qφ (z | s)
rather than the agent’s prior p(z). This is because the discriminator parameters Φ are now part of the
probabilistic model and do not just enter through a variational approximation.
How should we represent the posterior over discriminator parameters p(φ)? There is consider-
able work in Bayesian deep learning that offers possible answers, but here we take an ensemble
approach (Seung et al., 1992; Lakshminarayanan et al., 2017). We train N discriminators with
parameters φi for the ith discriminator. The discriminators are independently initialized and in theory
could also be trained on different mini-batches, though in practice, we found it both sufficient and
p(φ)qφ(Z | o(τ)) dφ -	p(φ)H[qφ(Z | o(τ))] dφ (7)
4
Published as a conference paper at ICLR 2022
(a) Skill discovery algorithms.
Figure 2: Methods. (a) The skill discovery process, where joint optimization of a skill-conditioned
policy and skill discriminator ensure reliable and distinct behavior for each skill. (b) DISDAIN:
disagreement between an ensemble of skill discriminators informs exploration.
(b) DISDAIN.
simpler to train them on the same mini-batches, as others have also found (Osband et al., 2016). The
posterior p(φ) is then represented as a mixture of point masses at the φi:
1N
p(φ) = Nf δ(φ - φi).
N i=1
(8)
Substituting the posterior in equation 8 into equation 7, we have:
1
I(Z;中 | O)= Eτ~∏ H N 5>φi(Z∣ o(τ))
i=1
1
-N ∑h[qφi(Z I o(τ))]
i=1
(9)
Maximizing equation 9 with RL corresponds to adding the following auxiliary reward to the policy:
1N
rDISDAIN⑴ ≡ HN ɪ2 qΦi (Z 1 Ot)
N i=1
1N
-N EH[qΦi(Z | Ot)].
N i=1
(10)
In words, this is the entropy of the mean discriminator minus the mean of the entropies of the
discriminators. By Jensen’s inequality, entropy increases under averaging, and thus rDISDAIN ≥ 0.
For trajectories on which there has been ample training data for the discriminators, the ensemble
members should agree and qφi (z | O) ≈ qφj (z | O) for all i, j. Thus the two terms in equation 10 will
be equal and this reward will vanish. For states of high discriminator disagreement, however, this
reward will be positive, encouraging exploration. Therefore, we call equation 10 the Discriminator
Disagreement Intrinsic reward, or DISDAIN (figure 2b).
DISDAIN is simple to calculate for discrete Z, as is common in the relevant literature (Gregor et al.,
2016; Eysenbach et al., 2019; Achiam et al., 2018; Baumli et al., 2021). DISDAIN augments any
discriminator-based unsupervised skill learning algorithm with two changes. First, an ensemble
of discriminators is trained instead of just one, and rskill should be calculated using the ensemble-
averaged prediction qφ(Z | O)=1 PN=I qψi (Z | O). Second, the DISDAIN reward is combined
with rskill, which we do through simple addition with a tunable multiplier λ. Pseudocode for DISDAIN
is provided in Algorithm 1. With N = 1 and λ = 0, this is standard unbonused skill discovery.
4 Experiments
We validate DISDAIN by testing its ability to increase skill learning in an illustrative grid world
(Four Rooms) as well as a more challenging pixel-based setting requiring function approximation
(the 57 Atari games of the Arcade Learning Environment (Bellemare et al., 2013)). In addition to
comparing performance to unbonused skill learning, we also compare to using popular off-the-shelf
exploration bonuses that are not tailored to skill discovery. In Four Rooms, we compare to using
count-based bonuses, which are known to perform well in these settings (Brafman and Tennenholtz,
2002). In Atari, where count-based bonuses become untenable due to the enormous state space, we
compare to random network distillation (RND; Burda et al. (2019)), one of the most commonly used
5
Published as a conference paper at ICLR 2022
Algorithm 1: Skill discovery with DISDAIN
Input: policy πθ, discriminator ensemble {qφi}iN=1, skill features O(τ), skill distribution p(Z),
skill trajectory length T , DISDAIN reward weight λ
while not converged do
Reset environment, sampling initial state s0
while episode not ended do
Sample skill, Z 〜P(Z)
Sample trajectory of length T from so, T 〜 ∏(z)
Form average discriminator from ensemble, qφ = Nn PN=ι qφi
rskill = log qφ(z | O(τ)) - logp(z)
rDISDAIN = H[qφ(∙ | O(T))] - N pi=1 H[qφi(∙ | O(T))]
r = rskill + λrDISDAIN
Update θ with RL to maximize r
Update {qφi}iN=1with SL to maximize log qφi (z | O(T))
s0 = sT
pseudo-count based methods (Bellemare et al., 2016). In addition, we validate that any advantages of
DISDAIN are not purely due to using an ensemble of discriminators by evaluating an ablation that
includes the same ensemble but removes the DISDAIN reward (i.e. Algorithm 1 with λ = 0). In all
cases, our primary metric for comparison is the effective number of skills learnt, nskills (equation 4).
The effective number of skills is just an interpretable transformation of the mutual information
objective shared by a large number of unsupervised skill learning algorithms (e.g. Gregor et al.
(2016); Achiam et al. (2018); Eysenbach et al. (2019); Hansen et al. (2020)). As such, the fact that
DISDAIN helps better maximize this objective should be worthwhile in its own right, considering the
various use cases that motivated this objective in the existing literature. That said, we recognize that
it is not obvious what utility comes with increasing the number of effective skills. To address this, we
also measure three surrogates of skill utility: downstream goal achievement, unsupervised reward
attainment, and unsupervised state coverage.
Distributed training We use a distributed actor-learner setup similar to R2D2 (Kapturowski et al.,
2019), except we do not use replay prioritization or burn-in, and Q-value targets are computed with
Peng’s Q(λ) (Peng and Williams, 1994) rather than n-step double Q-learning. In all cases, we found
it important to train separate Q-functions for the skill learning rewards and exploration bonuses
(DISDAIN, RND, or count-based). This follows from Burda et al. (2019), who also found that
this setup helped stabilize learning. Unlike that work, we need to specify a value target for each
Q-function, which we take to be the value of the action that maximizes the composite value function,
where the two values are added with a tunable weight λ. The discriminator is trained on the same
mini-batches sampled from replay to train the Q-functions. For Atari experiments, the Q-networks
process batches of state, skill, and action tuples to produce scalar Q-values for each, and the ResNet
state embedding network used in Espeholt et al. (2018) is shared by both of the Q-networks and
discriminator. For the Four Rooms grid world, we use tabular Q-functions and discriminators. Further
implementation details can be found in appendix A.
Skill discovery Our skill discovery baseline deviates slightly from prior work in order to make
it representative of the skill discovery literature as a whole. We utilize a simple discriminator that
receives only the final state of a trajectory (so O(T) = sT), omitting the state-conditional prior and
action entropy bonuses used in specific algorithms (Gregor et al., 2016; Eysenbach et al., 2019).
Hyperparameters Most of the RL hyperparameters were not tuned, but rather taken from standard
values known to be reasonable for Peng’s Q(λ). The skill discovery specific hyperparameters were
tuned for the basic algorithm without exploration bonuses, and then reused in all conditions. Our
RND implementation was first tuned without skill learning to achieve Atari performance competitive
with the original paper. For all exploration bonuses (DISDAIN, RND, count), we tuned their reward
weighting (e.g. λ in algorithm 1), while for DISDAIN we additionally swept the ensemble size (N).
Baselines As with the skill discovery reward, we apply exploration bonuses only to the terminal
states of each skill trajectory. For the count-based bonus, we track the number of times an agent ends
6
Published as a conference paper at ICLR 2022
a skill trajectory in each state n(s) and apply the exploration bonus rτ = 1/dn(sτ). For RND, We
follow the details of Burda et al. (2019) as closely as possible. For our target and predictor networks,
We use the same ResNet architecture as the policy observation embedding described above, and then
project to a latent dimension of 128. Rather than normalizing observations based on running statistics,
we found it more reliable to use the standard ɪ normalization of Atari observations.
255
DISDAIN Our ensemble-based uncertainty estimator required many design choices, including the
size of ensembles, and to what extent the ensemble members shared training data and parameters.
In all of the domains we tested, we found training ensemble members on different batches to be
unnecessary, similar to Osband et al. (2016). In the tabular case (Four Rooms), parameter-sharing is
not a concern, and we found an ensemble size of N = 2 to be sufficient. For Atari, the ensemble of
discriminators reuse the single ResNet state embedding network which is shared by the value function.
The input to the ensemble will inevitably drift, even if the data distribution remains constant, since
the ResNet representations evolve according to a combination of the value function and discriminator
updates. Expressive ensembles (e.g. with hidden layers) never converged in practice. By contrast,
large linear ensembles (N = 40) were reliably convergent, with convergence time increasing with
ensemble size. We follow Osband et al. (2016) in scaling the gradients passed backward through the
embedding network by 奈 to account for the increase in effective learning rate.
4.1	Four Rooms
First, we evaluate skill learning in the illustrative grid world seen in figure 3. There are 4 rooms and
104 states. The agent begins each episode in the top left corner and at each step chooses an action
from the set: left, right, up, down, or no-op. Episodes are 20 steps and we sample one skill
per episode (i.e. T = 20). The episodes are long enough to reach all but one state, allowing for a
maximum of 103 skills. For each method, we set NZ = 128 to make this theoretically possible.2
Results As seen in figure 3, even in this simple task, unbonused agents are unable to exceed 30
skills and barely leave the first room (see figures 18 and 19 for example skill rollouts). With both
DISDAIN and a count bonus, agents explore all four rooms and learn approximately triple the number
of skills, with the best seeds learning approximately 90 skills. Both bonuses do slow learning and
add variance due to the addition of a separate learned Q-function (see figure 20 for individual seeds).
The ensemble-only ablation provides a small benefit to skill learning, but far less than DISDAIN,
demonstrating that the DISDAIN exploration bonus (rDISDAIN), rather than the ensembling, drives
the increased performance. As a simple demonstration of the usefulness of our learnt skills for
downstream tasks, we also evaluate each methods’ skills on the original Four Rooms reward (i.e.
reaching specified goal states) without additional finetuning. Specifically, for each method, we sample
each accessible state of the environment as a goal, pass it through the trained discriminator qφ, choose
the highest probability skill z, rollout the policy for one episode conditioned on z, and track the
fraction of goal states successfully reached (similar to the imitation learning evaluation of Eysenbach
et al. (2019)). As seen in Figure 3b, more learnt skills leads to better downstream task performance.
4.2	Atari
Next we consider skill learning on a standard suite of 57 Atari games, where prior work has shown
that learning discrete skills is quite difficult (Hansen et al., 2020). In addition, it is a non-trivial test
for our ensemble uncertainty based method to work in the function approximation setting, since this
depends on how each ensemble member generalizes to unseen data. Here we use NZ = 64 and
T = 20. Since Atari episodes vary in length, skills may be resampled within an episode.
The count-based baseline used in the Four Rooms experiments cannot be directly applied to a non-
tabular environment like Atari. While pseudo-count methods have been used here (Bellemare et al.,
2016), Random Network Distillation (RND) similarly induces long term exploration (Burda et al.,
2019) and has been used for this purpose in a state-of-the-art Atari agent (Badia et al., 2020). While
newer approaches surpass RND in some domains (Raileanu and Rocktaschel, 2020; Seo et al., 2021),
it is unclear if this is the case across the full Atari suite. So, at present, we believe RND is the most
compelling baseline to compare against DISDAIN.
2An open source reimplementation of DISDAIN on a smaller version of Four Rooms is available at
http://github.com/deepmind/disdain.
7
Published as a conference paper at ICLR 2022
(a) Learning curves.
IOO-
(C) States reached without DISDAIN.
,5Io
7 5
①二
25
At initialization (~0 steps)
Mid-training (~3M steps)
(d) States reached with DISDAIN.
Unbonused Count
■ Ensemble ≡DISDAIN
0.0e+00 2.5e+08 5.0e+08 7.5e+08 1.0e+09
environment steps
(e) DISDAIN bonuses.
Figure 3: Four Rooms results. (a) Skills learnt for top 10 of 20 seeds for each method. Mean ±
std over seeds. (b) Performance on the downstream task of reaching a target state, averaged across
all possible target states. Mean ± std over seeds. (c-d) Example states reached with and without
DISDAIN. Plots depict counts of final states reached after one rollout per skill. Columns correspond
to different points during training. With DISDAIN, agents learn to reach all states, while without,
they barely make it out of the first room. (e) Per-state DISDAIN bonuses for the policy depicted in
(d). In the beginning, all exploration is encouraged. In the middle, the checkerboard pattern emerges
because agents try to space out their skills. By the end of training, DISDAIN gracefully fades away.
training time
∙*jEe 旦 S=PlS
Unbonused
Learning curves
ɪʒ
5
2
3
O
Numberofframes (in millions)
RND	Ensemble	DISDAIN
10	20	30	40
Skills learnt (τ)
_ L
Figure 4: Number of skills learnt on Atari. Left: Effective skills (see equation 4) over training,
measured by interquartile mean (IQM). Center: Distribution of skills learnt across seeds and games.
Right: Distribution of skill boosts over Unbonused skill learning across tasks and games. Shaded
regions show pointwise 95% confidence bands based on percentile bootstrap with stratified sampling.
Results As pointed out in Agarwal et al. (2021), making statistically sound conclusions can be
challenging in the few-seed, many-task setting of Atari. Thus, we follow their recommendations
and focus our results primarily on statistically robust distributional claims here. Individual learning
curves and game-by-game results are available in the appendix.
As shown in figure 4, DISDAIN increases the effective number of skills learnt across the Atari suite,
with only modest damage to sample effiency. Additionally, we found that DISDAIN’s performance
8
Published as a conference paper at ICLR 2022
boosts are robust to its key hyperparameters, namely the bonus weight λ and ensemble size N (see
figure 8), with significant gains over unbonused skill learning maintained over more than an order of
magnitude of variation in both.
Notably, our results show that RND fails to significantly aid in skill learning. A sweep over the RND
bonus weight is shown in figure 11, but the summary is that as the RND bonus becomes similar in
magnitude to the skill learning reward, it damages skill learning rather than helping, and so the “best"
RND bonus weight for skill learning is approximately zero. This is perhaps unsurprising: RND was
designed in the context of stationary task rewards (as are most other exploration methods, such as
pseudo-counts), whereas skill learning objectives produce a highly non-stationary reward function.
This highlights the importance of using an exploration bonus tailored to the skill discovery setting.
Additionally, the failure of the “ensemble-only" baseline to significantly increase skill learning
demonstrates that is the rDISDAIN exploration bonus that is crucial to DISDAIN’s success, and not just
the ensembling of the discriminator.
To further probe the utility of our learnt skills,
we measure the unsupervised reward attainment
of a policy that randomly switches between
them. First reported in Hansen et al. (2020),
the idea is that while this policy is certainly far
from optimal, this metric indicates whether or
not the skill space is sufficient to perform the
various reward collecting behaviors involved in
each game. Additionally, we measure lifetime
state coverage as an indication of exploration
throughout learning (on a subset of games sup-
porting this metric; see Appendix B for details).
Figure 5 confirms that DISDAIN leads to both
increased reward attainment and lifetime cov-
erage (see figures 14, 15, and 16 for additional
lifetime and episodic state coverage results).
RND Ensemble
Zero-shot reward improvement
Figure 5: Qualitative analysis of skills learnt on
Atari. Both plots depict improvement over Un-
bonused skill learning aggregated across seeds and
games. Left: Probability of improvement on zero-
shot reward evaluation. Right: Distribution of per-
centile improvements on lifetime coverage. Error
bars depict 95% stratified boostrap CIs.
DISDAIN
-50%	δ%~+50% +100% +150%
Lifetime coverage boost (τ)
5 Discussion & Limitations
We introduced DISDAIN, an enhancement for unsupervised skill discovery algorithms which in-
creases the effective number of skills learnt across a diverse range of tasks, by using ensemble-based
uncertainty estimation to counteract a bias towards pessimistic exploration.
The connection between ensemble estimates of uncertainty and infomax exploration dates back to
at least Seung et al. (1992), who use it to select examples in an active supervised learning setting.
These ideas have more recently found use in the RL literature, with recent work using disagreement
among ensembles of value functions (Chen et al., 2017; Flennerhag et al., 2020; Zhang et al., 2020)
and forward models of the environment (Pathak et al., 2019; Shyam et al., 2019; Sekar et al., 2020) to
drive exploration. In a closely related line of work, ensembles of Q-functions have been used to drive
exploration without an explicit bonus based on disagreement (Osband et al., 2016; 2018). To our
knowledge, DISDAIN represents the first application of these ideas to unsupervised skill discovery.
We focused on discrete skill learning methods due to their relative prevalence (Gregor et al., 2016;
Eysenbach et al., 2019; Achiam et al., 2018; Baumli et al., 2021). In some cases, continuous or
structured skill spaces might make more sense (Hansen et al., 2020; Warde-Farley et al., 2019). While
the principles behind DISDAIN should still apply, further approximations may be necessary, e.g. for
the entropy of the ensemble-averaged discriminator, which may be unavailable in closed form.
Designing agents that explore and master their environment in the absence of task reward remains
an open problem, for which there exist many different families of approaches (e.g. reward-free
exploration (Jin et al., 2020; Zhang et al., 2021)). In this paper, we focus on improving one such
family - unsupervised skill learning through variational infomax (Section 2). By treating agents with
DISDAIN, we empower them to better maximize their objective and learn more skills. Leveraging
these skills for rapid task reward maximization remains an important direction for future research.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The authors would like to thank Stephen Spencer for engineering and technical support, Ian Osband
for feedback on an early draft, Rishabh Agarwal for suggestions on statistical analysis and plotting,
Phil Bachman for correcting the error discussed in Appendix C, and David Schwab for pointing us to
the original Query by Committee work (Seung et al., 1992).
References
Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option discovery
algorithms. arXiv preprint arXiv:1807.10299, 2018.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare.
Deep reinforcement learning at the edge of the statistical precipice. In Neural Information
Processing Systems (NeurIPS), 2021.
Ankesh Anand, Evan Racah, Sherjil Ozair, YoshUa Bengio, Marc-Alexandre C6t6, and R Devon
Hjelm. Unsupervised state representation learning in Atari. In Neural Information Processing
Systems (NeurIPS), 2019.
Adri鱼 Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Zhaohan Daniel GUo, and Charles BlUndell. Agent57: OUtperforming the Atari hUman benchmark.
In International Conference on Machine Learning (ICML), 2020.
David Barber and Felix Agakov. Information maximization in noisy channels: A variational approach.
In Neural Information Processing Systems (NIPS), 2004.
Kate Baumli, David Warde-Farley, Steven Hansen, and Volodymyr Mnih. Relative variational
intrinsic control. In AAAI Conference on Artificial Intelligence, 2021.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An
evaluation platform for general agents. Journal OfArtificial Intelligence Research, 47:253-279, 06
2013.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In Neural Information Processing
Systems (NIPS), 2016.
Ronen I Brafman and Moshe Tennenholtz. R-max - a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research (JMLR), 3(Oct):213-231,
2002.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In Neural Information Processing
Systems (NeurIPS), 2020.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations (ICLR), 2019.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. UCB exploration via Q-
ensembles. arXiv preprint arXiv:1706.01502, 2017.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning
(ICML), 2020a.
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervised models are strong semi-supervised learners. In Neural Information Processing
Systems (NeurIPS), 2020b.
10
Published as a conference paper at ICLR 2022
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:
Scalable distributed deep-RL with importance weighted actor-learner architectures. In International
Conference on Machine Learning (ICML), 2018.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. In International Conference on Learning Representations
(ICLR), 2019.
Sebastian Flennerhag, Jane X Wang, Pablo Sprechmann, Francesco Visin, Alexandre Galashov, Steven
Kapturowski, Diana L Borsa, Nicolas Heess, Andre Barreto, and Razvan Pascanu. Temporal
difference uncertainties as a signal for exploration. arXiv preprint arXiv:2010.02255, 2020.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr
Mnih. Fast task inference with variational intrinsic successor features. In International Conference
on Learning Representations (ICLR), 2020.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. In International Conference on Machine Learning (ICML), 2020.
Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent
experience replay in distributed reinforcement learning. In International Conference on Learning
Representations (ICLR), 2019.
Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need:
Few-shot extrapolation via structured maxent RL. In Neural Information Processing Systems
(NeurIPS), 2020.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Neural Information Processing Systems (NeurIPS),
2017.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: Multi-agent
variational exploration. In Neural Information Processing Systems (NeurIPS), 2019.
OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur
Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas
Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei
Zhang. Solving Rubik’s Cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019a.
OPenAL Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemysIaW Debiak,
Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jdzefowicz,
Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan
Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie
Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. arXiv
preprint arXiv:1912.06680, 2019b.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. In Neural Information Processing Systems (NIPS), 2016.
Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement
learning. In Neural Information Processing Systems (NeurIPS), 2018.
Ian Osband, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. Deep exploration via randomized
value functions. Journal of Machine Learning Research (JMLR), 20(124):1-62, 2019.
Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
In International Conference on Machine Learning (ICML), 2019.
Jing Peng and Ronald J Williams. Incremental multi-step Q-learning. In International Conference on
Machine Learning (ICML), 1994.
11
Published as a conference paper at ICLR 2022
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. Technical report, OpenAI, 2019.
Roberta Raileanu and Tim Rocktaschel. RIDE: Rewarding impact-driven exploration for Procedurally-
generated environments. In International Conference on Learning Representations (ICLR), 2020.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. In International Conference on Machine
Learning (ICML), 2020.
Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy
maximization with random encoders for efficient exploration. arXiv preprint arXiv:2102.09430,
2021.
H. Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Conference
on Learning Theory (COLT), 1992.
Pranav Shyam, Wojciech JaSkowski, and Faustino Gomez. Model-based active exploration. In
International Conference on Machine Learning (ICML), 2019.
Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1—2):181—211,
1999.
Oriol Vinyals, Igor Babuschkin, WojciechM. Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,
Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max
Jaderberg, Alexander S. Vezhnevets, Remi Leblond, Tobias Pohlen, Valentin Dalibard, David
Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff,
Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch, Katrina McKinney, Oliver Smith, Tom
Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver.
Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):
350-354, 2019.
David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and
Volodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In
International Conference on Learning Representations (ICLR), 2019.
Chuheng Zhang, Yuanying Cai, and Longbo Huang Jian Li. Exploration by maximizing Renyi
entropy for reward-free RL framework. In AAAI Conference on Artificial Intelligence, 2021.
Yunzhi Zhang, Pieter Abbeel, and Lerrel Pinto. Automatic curriculum learning through value
disagreement. In Neural Information Processing Systems (NeurIPS), 2020.
12
Published as a conference paper at ICLR 2022
A Compute Requirements and Hyperparameters
The compute cluster we performed experiments on is rather heterogeneous, and has features such
as host-sharing, adaptive load-balancing, etc. It is therefore hard to give precise details regarding
compute resources - however, the following is a best-guess estimate.
A full experimental training run for Atari lasted 4 days on average. Our distributed reinforcement
learning setup (Espeholt et al., 2018) used 100 CPU actors and a single V100 GPU learner. Thus,
we required approximately 9600 CPU hours and 96 V100 GPU hours per seed, with 3 seeds and 3
conditions per game.
Tuning required approximately 10 different hyper-parameters combinations on 6 games, amounting
to 864,000 CPU hours and 8,640 V100 GPU hours. The results on the full suite of 57 Atari games
required 4,924,800 CPU hours and 49,248 V100 GPU hours. Combining these, we get a total compute
budget of 5,788,800 CPU hours and 57,888 V100 GPU hours.
It is worth remembering that the above is likely quite a loose upper-bound, as this estimate assumes
100 percent up time, which is far from the truth given the host-sharing and load-balancing involved in
our setup. Additionally, V100 GPUs were chosen based on what was on hand; our models are small
enough to fit on much cheaper cards without much slowdown.
B C overage Metrics
We calculate two related notions of coverage: lifetime and episodic. Lifetime coverage corresponds
to the number of unique states encountered during an agent’s lifetime, whereas episodic coverage
corresponds to the number of unique states encountered during each episode. Both rely on the notion
of a unique state. The subset of games chosen for these metrics were those where a good notion of
a unique state is simple: they all involve a controllable avatar that moves in a coordinate system,
so unique avatar coordinates are used. This information is exposed in the RAM state of the Atari
emulator, as shown in Anand et al. (2019).
C Additional implementation details
For our skill learning reward, one generally helpful change that we had not previously encountered
was to clip negative skill rewards to 0 (rskill = max(rskill, 0)). A previous version of this manuscript
stated that: “This yields a strictly tighter lower bound on the mutual information (equation 2),
since negative rewards imply the discriminator’s performance is worse than chance.” However, that
argument isn’t quite correct. Clipping the expected reward (i.e. clipping outside the expectation)
would produce a tighter bound, but this argument does not hold on a sample-by-sample basis (i.e.
clipping inside the expectation, as we did). Thus, this should only be viewed as a heuristic.
References
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
13
Published as a conference paper at ICLR 2022
Hyperparameter	Atari	Four Rooms
Torso	IMPALA Torso (Espeholt et al., 2018)	tabular
Head hidden size	256	-
Number of actors	100	64
Batch size	128	16
Skill trajectory length (T)	20	same
Unroll length	20	same
Actor update period	100	same
Number of skill latents (NZ)	64	128
Replay buffer size	106 unrolls	same
Optimizer	Adam (Kingma and Ba, 2015)	SGD
learning rate	2 * 10-4	2 * 10-3
Adam E	10-3	-
Adam βι	0.0	-
Adam β2	0.95	-
RL algorithm	Q(λ) (Peng andWilliams, 1994)	same
λ	0.7	same
discount Y	0.99	same
Target update period	100	-
DISDAIN ensemble size (N)	40	2
DISDAIN reward Weight (λ)	180.0	10.0
RND reward weight	03	-
Count bonus weight	-	10.0
Table 1: Hyperparameters. Atari hyperparameters were tuned on a subset of 6 games
(beam_rider, breakout, pong, qbert, seaquest, and space_invaders). In both envi-
ronments, all RL and skill discovery hyperparameters were tuned for unbonused skill learning and
then held fixed when adding exploration bonuses.
14
Published as a conference paper at ICLR 2022
0e+00 5e+08 le+09 Oe+OO 5e+08
le+09 Oe+OO 5e+08 le+09
Oe+OO
le+09
5e+08 le+09 Oe+OO 5e+08
Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09
25
20
15
10
skills learnt
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09
Unbonused
BdRND
一Ensemble
DISDAIN
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09 Oe+OO
5e+08 le+09 Oe+OO 5e+08 le+09
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09
zaxxon
environment steps
Figure 6: Skills learnt per-seed and per-game on all 57 Atari games.
15
Published as a conference paper at ICLR 2022
skills learnt
Figure 7: Per-game boosts for each method. (b-d) Boosts in skills learnt on each game over
Unbonused skill learning. Mean ± standard deviation over 3 seeds. All 3 plots use the same y-axis
range magnitude so that bar heights are comparable. DISDAIN improves skill learning on 52/57
(91%) of games, with boosts of >5 skills on 18/57 (32%) and >10 on 6/57 (11%) of games. RND and
the Ensemble-only ablation perform closer to chance with improvements on 28/57 (49%) and 35/57
(61%), with boosts of >5 skills on 2/57 (4%) and 9/57 (16%) and >10 skills on 1/57 (2%) and 0/57
(0%) of games, respectively. a) Skills learnt on each game for each method for reference. Sorted by
magnitude of DISDAIN boost, as in (b).
16
Published as a conference paper at ICLR 2022
(a) Robustness to bonus weight (λ).
151——
一Unbonused
≡Ensemble
♦ DISDAIN (weight = 45)
≡DISDAIN (weight = 180)
DISDAIN (weight = 720)
0.0e+00 2.5e+08 5.0e+08 7.5e+08 1.0e+09
environment steps
(b) Robustness to ensemble size (N).
16h—
4U.Ja S=DIS
≡Unbonused
sDISDAIN (ensemble size = 20)
■DISDAIN (ensemble size = 40)
■DISDAIN (ensemble size = 80)
DISDAIN (ensemble size = 160)
DISDAIN (ensemble size = 320)
0.0e+00 2.5e+08 5.0e+08 7.5e+08 1.0e+09
environment steps
Figure 8: DISDAIN robustness to key hyperparameters. (a) Sweeping bonus weight (λ) with
fixed ensemble size N = 160 (see Algorithm 1). (b) Sweeping ensemble size (N) with fixed bonus
weight λ = 180. Curves averaged over 57 games and 3 seeds (for results broken out by game and
seed, see figures 9 and 10). For both hyperparameters, DISDAIN’s improvements over baselines are
robust over more than an order of magnitude of variation. All other experiments use λ = 180 and
N = 40 unless otherwise stated.
17
Published as a conference paper at ICLR 2022
7.5-
5-
2.5-
amidar
assault
asterix
asteroids
at antis
3
2
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
bank heist
battle zone
beam-rider
0-
6-
4-
2-
12.5-
0e+00 5e-i-08 le-i-09
OeiOO 5e-i-08 le+09
0e+00 5e+08 le+09
0
berzerk
30
20
10
0e+00
5e+08 le+09
bowling
2.5-
2-
1.5-
0e+00 5e+08 le+09
0
boxing
30
20
10
V'T---
0e+00
5e+08 le+09
breadUt
15-
10-
5-
0e+00 5e+08 le+09
20
10
O
Oe+OO 5e+08 le+09
ChoPPer^command
CraZy-dimber
30-
10
20-
10-
5
0e+00 5e+08 le+09
O-L
Oe+OO 5e+08 le+09
15 d
10-
5-1
defender
0e+00 5e+08 le+09
demon attack
20
10
0e+00 5e+08 le+09
15
12-
10
8-
4-
enduro
5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
fishing^derby
30-
20-
10-
O-L
Oe+OO 5e+08 le+09
freeway
12.5
10
7.5
5
2.5
0e+00 5e+08 le+09
frostbite
gopher
15
12.5-
10-
10
7.5-
5-
2.5-
5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
skills learnt
10-
5-
0e+00 5e+08 le+09
0e+00 5e+08 le+09
montezum a-revenge
20-
kung fU master
15-
10-
5-
15
10
O
Oe+OO 5e+08 le+09
5
V
0e+00 5e+08 le+09
ice hockey
15-
10-
5-
0e+00 5e+08 le+09
Jamesbond
20
15
10
5
O
Oe+OO 5e+08 le+09
12-
8-
4-
m⅜jpacman
5-
4-
3-
2-
0e+00 5e+08 le+09
20
15
10
5
O
Oe+OO 5e+08 le+09
15
10
5
phoenix
12.5-
15
10-
7.5-
10
5-
2.5-
0e+00 5e+08 le+09
Unbonused
Ensemble
5
0e+00 5e+08 le+09
Oe+OO 5e+08 le+09
0e+00 5e+08 le+09
DISDAIN (weight = 45)
DISDAIN (weight = 180)
DISDAIN (weight = 720)
private eye
pong
10-
5-
4-
0e+00 5e+08 le-i-09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
3-
2-
riverraid
15
10
5
0e+00 5e+08 le+09
road一runner
25-
20-
15-
10-
5-
rαbαtank
Oe+OO 5e+08 le+09
0』	.	.
Oe+OO 5e+08 le+09
0e+00 5e+08 le+09
solaris
9-
6-
3-
tutankham
15-
10-
5-
20-
Wizardjrfjwor
15-
10-
5-
yans.re&丽 W
30
20
10
0
12.5
space invaders
10
7.5
5
2.5
40-
0e+00 5e+08 le+09
up n down
2.5
2
1.5
surround
Starjunner
5
4
30-
3
20-
10-
2
Oe+OO 5e+08 le+09
o-L
Oe+OO 5e+08 le+09
venture
20-
15-
10-
5-
0
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09
20^
10-
Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09 Oe+OO 5e+08 le+09
environment steps
Video-Pin⅛⅞∏^
8
6
4
2
Oe+OO 5e+08 le+09
Figure 9: Per-game bonus weight sweep for DISDAIN across all 57 Atari games.
18
Published as a conference paper at ICLR 2022
skills learnt
atlantis
H
0e+00 5e+08 le+09
IO
7.5
5
2.5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
30
20
10
O
0e+00 5e+08 le+09
gravltar
10
5
0e+00 5e+08 le+09
20
15
10
5
0e+00 5e+08 le+09
kung-fu-m aster
20
15
10
5
0e+00 5e+08 le+09
O
monte zum a-revenge
20
15
10
5
u∖---
0e+00
5e+08 le+09
skiing
4∙ f=AF
3-
2-
1, ,
Oe+OQ 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
boxing
Oe+OO 5e+08 le+09
0e+00 5e+08 le+09 Oe+OO
5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
gopher
Oe+OO 5e+08 le+09
20
15
10
5
0e+00 5e+08 le+09
Jamesbond
20
15
10
5
0e+00 5e+08 le+09
15
10
5
0e+00 5e+08 le+09
15
10
5
Oe+OO 5e+08 le+09
msjɔaeman
6
5
4
3
2
0e+00 5e+08 le+09
O
na me-thls-game
20
15
10
5
U -,
0e+00
15
5e+08 le+09
phoenix
10
5
0e+00 5e+08 le+09
15
10
5
Oe+OO 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09 0e+00 5e+08 le+09
0e+00 5e+08 le+09 Oe+OO 5e+08 le+09
environment steps
Unbonused
a DISDAIN (ensemble size
DISDAIN (ensemble size
DISDAIN (ensemble size
DISDAIN (ensemble size
DISDAIN (ensemble size
20)
40)
80)
160)
320)
Figure 10: Per-game ensemble size sweep for DISDAIN across all 57 Atari games.
19
Published as a conference paper at ICLR 2022
^ES- ω≡v-ω
mUnbonused
—RND (weight = 0.1)
RND (weight = 0.3)
TRND (weight = 1)
RND (weight = 3)
0.0e+00 2.5e+08 5.0e+08 7.5e+08 1.0e+09
environment steps
Figure 11: RND bonus weight sweep. RND fails to significantly improve skill learning as the
weighting on its contribution to the reward is increased. As soon as the RND bonus becomes of
a similar order of magnitude as the skill learning reward (bonus weight λ ≈ 1), skill learning
performance begins to decay, suggesting that the kind of exploration encouraged by RND is not
conducive to skill learning. Results averaged over 57 games and 3 seeds (see figure 12 for results
broken out by game and seed).
20
Published as a conference paper at ICLR 2022
7.5
5
2.5
3
2
5
4
25
20
15
10
5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
assault
asteroids
Γ
atl≡ntis
4
3
2Q
3
10
2
0e÷00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
O
0e+00 5e+08 le+09
bank-heist
battle一zoι⅞^
8
12
6
4
2
8
4
0e+00 5e+08 le÷09
0e+00 5e+08 le÷09
breakout
10
5
0e÷00 5e+08 le+09
centipede
20
15
10
5
0e+00 5e+08 le+09
double dunk
10
5
enduro
16
12
8
4
0e÷00 5e+08 le+09
0e÷00 5e+08 le+09
beam rider
10
7.5
5
2.5
0e+00 5e+08 le÷09
chopper-comma nd
25
20
15
10
5
O
0e+00 5e+08 le÷09
fishinα derby
20
10
O
0e+00 5e+08 le+09
bowling
30
2.5
20
10
20
2
1.5
0e+00 5e+08 le÷09
O
0e+00 5e+08 le÷09
boxing
15
10
5
0e+00 5e+08 le÷09
α¾zy一CHmber
demon-attack
15
9
10
6
3
15
5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
Freeway
10
7.5
5
2.5
0e+00 5e+08 le+09
10
5
frostbite
gopher
7.5
5
2.5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
skills learnt
iamesbond
kangaroo
16
15
12
10
8
5
5
2.5
10
7.5
gravitar
2Q
15
10
4
5
0e+00 5e+08 le÷09
0e+00 5e+08 le÷09
0e+00 5e+08 le+09
0e+00 5e+08 le÷09
0e+00 5e+08 le÷09
16
ku ng-MmaSter
12
8
4
0e+00 5e+08 le+09
16
montezuma revenge
5
4
12
3
8
2
4
0e+00 5e+08 le+09
12.5
10
7.5
5
2.5
pitfail
0e÷00 5e+08 le+09
name this game
phoenix
ms pacman
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
PriVate一eyQ
qbert
rive Ed
10
7.5
5
2.5
20
4
15
15
3
10
10
2
5
5
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le÷09
0e+00 5e+08 le÷09
skiing
seaquest
20
4
15
3
10
5
10
2
0e+00 5e+08 le+09
0e+00 5e+08 le+09
Solaris
7.5
5
2.5
0e÷00 5e+08 le+09
10
7.5
5
2.5
0e+00 5e+08 le+09
10
7.5
5
2.5
tennis
0e÷00 5e+08 le+09
time-pilot
up-nj⅛而
10
15
2.5
7.5
2
10
5
2.5
tutankham
1.5
5
0e+00 5e+08 le+09
0e÷00 5e+08 le+09 0e÷00 5e+08 le+09
20
15
10
5
10
7.5
2.5
0e+00 5e+08 le÷09
star,gunner
30
20
10
O-L1--------1-------r
Oe÷OQ 5e+08 le+09
20
15
10
5
venture
O
0e+00 5e+08 le+09
FObotank
5
0e÷00 5e+08 le÷09
surround
4
3
2
0e+00 5e+08 le+09
6
5
4
3
VitteO-Pinball
2-L1-------1--------r
Oe÷OQ 5e+08 le+09
Unbonused
RND (weight = 0.1)
RND (weight = 0.3)
RND (weight = 1)
RND (weight = 3)
wizard of wor
15
10
5
15
10
5
yars-revenge
30
20
10
0
0e+00 5e+08 le÷09 0e+00 5e+08 le÷09 0e+00 5e+08 le÷09
environment steps
Figure 12: Per-game bonus weight sweep for RND across all 57 Atari games.
21
Published as a conference paper at ICLR 2022
450-
400-
350-
300-
250-
200-
150-I,	, J
0e+00 5e+08 le+09
alien
40
30
20
10
50
amldar
0e+00 5e+08 le+09
assault
600
400
200
0e+00 5e+08 le+09
asteιIX
500-
400-
300-
200-1____
0e+00 5e+08 le+09
800
600
400
asteroids
200 tι______I_______
0e+00 5e+08 le+09
at∣a∏t∣s
20000-
15000-
IOOOO-
5000-
0e+00 5e+08 le+09
50-
bank-helst
battle-zone
40-
30-
20-
10-
O-
IOOO
900
800
700
600
500
beam rider
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
berzerk
500-
400-
300-
200-
IOO
0e+00 5e+08 le+09
boxing
30
20
10
bowling
O
0e+00 5e+08 le+09
0e+00 5e+08 le+09
breakout
1.6-
1.4-
1.2-
0.8
0e+00 5e+08 le+09
2400
2000
1600
1200
centipede
7000-
800
600
400
0e+00 5e+08 le+09
0e+00 5e+08 le+09
climber
6000-
5000-
4000-
3000-
0e+00 5e+08 le+09
demon attack
IOOQ
0e+00 5e+08 le+09
0e+00 5e+08 le+09
double.dιii⅞^
freeway
enduro
125
3-
2-
Q
U-
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
0e+00 5e+08 le+09
frostbite
IOO
75
50
25
0e+00 5e+08 le+09
gopher
600-
400-
200-
0e+00 5e+08 le+09
kung-fu-master
1500-
3 4 5 6 7
11-IlI
-----
0e+00 5e+08 le+09
PhoentX
PItfaU
O-
1000-
500-
0-
0.075
J-0.025
montezu mareve nge
0.050
0.025
0
0e+00 5e+08 le+09 0e+00 5e+08 le+09
-1000
PrlVate eye
2000
1000
0
0e+00 5e+08 le+09 0e+00 5e+08 le+09
seaquest
15000
skiing
250-
200-
150-
100-
0e+00 5e+08 le+09
0e+00 5e+08 le+09
-22.75-
tennis
tlme-pllot
-23-
-23.25-
-23.50-
-23.75-
0e+00 5e+08 le+09
0e+00 5e+08 le+09
800-
wlzard-ofwor
600-
400-
200-
0e+00 5e+08 le+09
4000
2000
0
0e+00 5e+08 le+09
ms-pacman
500
400
300
0e+00 5e+08 le+09
750
500
250
0e+00 5e+08 le+09
SoIaι⅛
name-tħls-game
2000-
1500-
1000-
500-
0e+00 5e+08 le+09
0e+00 5e+08 le+09
15
tutankham
1500
1000
500
-100-
-200-
-300-
DISDAIN
0e+00 5e+08 le+09
0e+00 5e+08 le+09
riverraid
2000-
1500-
1000-
500-
1500
0e+00 5e+08 le+09
road-runner
1000
500
O
0e+00 5e+08 le+09
robotank
5-
4-
3-
2-
0e+00 5e+08 le+09
350-
300-
250-
200-
1200
1000
150-lι ι ι
0e+00 5e-t-08 le-t-09
surround
Stajgunner
800
600
0e+00 5e+08le+09
0e+005e+08 le+09
2500--
up-n-down^
15
venture
vldeoj)lnball
15000-
10000-
5000-
0e+00 5e+08 le+09 0e+00 5e+08 le+09
0e+00 5e+08 le+09
O-L
0e+00 5e+08 le+09
zaxxon
300
200
100
O
0e+00 5e+08 le+09
environment steps
O

0
0
Figure 13: Per-game task reward attainment curves throughout training. We emphasize that
agents are trained only to maximize the skill learning objective and any associated exploration bonus
(i.e. DISDAIN or RND), and not the task reward. Thus, these plots depict zero-shot reward attainment
while uniformly randomly switching between skills.
22
Published as a conference paper at ICLR 2022
①①>0u①IUQ①lt-
ms pacman
80000-
70000-
60000-
50000 -
40000-
10750 -
10500-
10250 -
IOOOO-
IlOOO
seaquest
5000-
4000-
3000-
1720-
1700-
1680-
private eye
Unbonused
RND
—Ensemble
DISDAIN
0e+00	5e+08	le+09	0e+00	5e+08	le+09	0e+00	5e+08	le+09
environment steps
Figure 14: Lifetime coverage for all games, methods, and seeds. All policies quickly achieve the
same score on ms_pacman and seaquest, so those levels are removed from the analysis in the
main text.
4jSooq ①①>03
RND
Ensemble
DISDAIN
ypo-d ①
Figure 15: Episodic coverage boosts over unbonused skill learning. DISDAIN provides significant
boosts over unbonused skill learning on hero and montezuma_revenge, while all methods
perform similarly on the other four levels analyzed. Since state coverage metrics are particularly well
suited to montezuma_revenge, the boost there is especially interesting. Results averaged over 3
seeds. For results over training for each seed, see figure 16.
0e+00	5e+08
le+09	0e+00	5e+08
montezuma revenge
900-
600-
300-
le+09	0e+00	5e+08	le+09
0e+00	5e+08
le+09	0e+00	5e+08
environment steps
Figure 16: Episodic coverage for all games, methods, and seeds.
le+09	0e+00	5e+08	le+09
Unbonused
RND
一Ensemble
■ DISDAIN
23
Published as a conference paper at ICLR 2022
SnUOq z-<Qω-Q
Skill learning reward
Figure 17:	rskill vs rDISDAIN during learning for all seeds per-game on all 57 Atari games. Each
panel includes data from 3 seeds. DISDAIN reward tends to dominate early but fades away as skill
learning converges.
24
Published as a conference paper at ICLR 2022
Skill 52 (0.99)
Skill 31 (0.99)
Skill 117 (0.99) Skill 63 (0.99) Skill 77 (0.99)
Skill 94 (0.99)
Skill 86 (0.99)
Skill 95 (0.99)
Skill 56 (0.99)
Skill 76 (0.99)
Skill 99 (0.99)
Skill 15 (0.99)
Skill 106 (0.99) Skill 101 (0.99) Skill 73 (0.99)
Skill B (0.99)
Skill 84 (0.99)
Skill 51 (0.99)
Skm 109 (050)
Skill 49 (049)
Skill 44 (0.49)
Skill 120 (0.50)
Skill 57 (0.99) Skill 103 (0.49) Skill 30 (0.50)
Skill 20 (0.99)
Skul 61 (0.99)
Skill 21 (0.99)
Skill 114 (0.50)
Skill 96 (049)
Skill 24 (0.98)
Skill 23 (0.49)
Skill B5 (0.50) Skill IB (0.50) Skill 64 (0.50)
Skill 41 (0.99)
Skill 127 (0.50)
Skill 25 (0.49)
Skill 26 (0.99)
Skill 56 (0.99)
Skill 105 (0.99)
Skill 121 (0.99)
Skill 102 (0.51) Skill 75 (0.48) Skill BO (0.99)
Skill 104 (0.51)
Skill 32 (0.49)
Skill 89 (0.99)
Skill 116 (1.00)
Skill Bl (0.99)
Skill 107 (0.99)
Skill 45 (0.99)
Skill 50 (0.96) Skill 111 (0.49) Skill 90 (0.50)
Skill 59 (0.99)
Skill 11 (0.99)
SklU 72 (0.99)
Skill 22 (0.99)
Skill 1 (0.99)
Skul 27 (0.99)
Skill 65 (0.49)
Skill B2 (0.50) Skill 74 (0.99) Skill 40 (0.99)
Skill 36 (0.99)
Skill 10 (0.99)
Skill 42 (0.99)
Skill IlB (0.50)
Skm 106 (0.50)
Skill 9 (0.50)
Skill 39 (0.50)
Skill IlO (0.99) Skill 123 (0.99) Skill 115 (0.99)
Skill 66 (0.99)
Skill 48 (0.99)
Skill 55 (0.49)
Skill 34 (0.51)
Skill 5 (0.99)
Skill 125 (0.99)
Skill 3 (0.99)
Skill 4 (0.99) Skill 6 (0.99) Skill 12 (0.49)
Skill 19 (0.50)
Skill 93 (0.99)
Skill 46 (0.50)
Skill 112 (049)
Skill 113 (0.99)
Skill 17 (0.49)
Skill 53 (0.50)
Skill 98 (0.99) Skill 16 (0.99) Skill 71 (0.50)
Skill 70 (0.49)
Skill 2 (0.99)
Skill 91 (0.99)
Skill 47 (0.99)
Skill O (0.33)
Skill 86 (0.34)
Skill 54 (0.32)
Skill B3 (0.99) Skill 13 (0.99) Skill 60 (0.99)
Skill 92 (0.99)
Skill 119 (0.99)
Skill 7 (0.48)
Skill 69 (0.51)
Skill 122 (0.99)
Skill 100 (0.99)
Skill 67 (0.99)
Skill 7B (0.50) Skill 97 (0.49) Skill 126 (0.99)
Skill 33 (0.99)
Skul 37 (0.99)
Skill 66 (0.99)
Skill 14 (0.99)
Skill 79 (0.50)
Skill 35 (0.49)
Skill 29 (0.50)
Skill 43 (0.50) Skill 62 (0.99) Skill 87 (0.51)
Skill 38 (0.49)
Skill 124 (0.99)
Skill 28 (0.99)
Figure 18:	Example DISDAIN skill rollouts on Four Rooms. Each panel shows rollout for one
skill. Panels are labeled with skill index and the probability the discriminator assigns to the correct
skill label. Color indicates time within episode, moving from green (beginning) to yellow (end).
Skills are sorted by final state. DISDAIN learns to visit nearly every accessible state.
25
Published as a conference paper at ICLR 2022
Skill 125 (0.11)
Skill 2 (0.11) Skill B9 (0.11) Skill 91 (0.11) Skill 55 (0.12)
Skill 20 (0.09) Skul 17 (0.08) Skul 113 (0.08) Skill 74 (0.07)
Skill 25 (0.10) Skill 22 (0.10) Skill 77 (0.12) Skill 105 (0.11) Skill 127 (0.09)
Skill 96 (0.08)
Skill 13 (0.08) Skill 124 (0.09) Skill 123 (0.0B) Skill B (0.0B) Skill 36 (0.08)
Skul 60 (OQ8)
Skill 48 (0.26) Skill 3B (0.25) Skill 70 (0.24) Skill 59 (0.25)
Skill 101 (0.17) Skill 86 (0.17) Skill 72 (0.16) Skill 84 (0.17) Skill 3 (0.16)
Skill 24 (0.16)
Skill 41 (0.35) Skill 12 (0.35) Skill 33 (0.30) Skill 79 (1.00)
Skill B3 (0.07) Skill 49 (0.08) Skill 53 (0.0B) Skill 90 (0.0B) Skill 92 (0.07)
Skill 102 (O-OB)
Skill IB (0.08) Skill 110 (0.08) Skill 14 (0.08) Skill 114 (0.08)
Skill 10 (0.08) Skill 117 (0.07) Skill 32 (0.0B) Skill 26 (0.97) Skill 95 (0.11)
Skill 54 (0.11)
Skill 65 (0.12) Skill 115 (0.11) Skill 119 (0.11) Skill 63 (0.11)
Skill 51 (0.11) Skill 34 (0.11) Skill 40 (0.11) Skill 122 (0.96) Skill 52 (0.22)
Skill 99 (0.21)
Skill 73 (0.19) Skill 7 (0.19) Skill 67 (0.19) Skill 5 (1.00)
Skill 21 (1.00) Skill 16 (1.00) Skill 66 (0.25) Skill B5 (0.24) Skill 46 (0.25)
Skill 50 (0.25)
Skill 19 (0.20) Skill 15 (0.19) Skill 61 (O.IB) Skill 28 (0.22)
Skill 66 (0.21) Skill 47 (0.49) Skill 57 (0.50) Skill 97 (0.26) Skill 76 (0.24)
Skill 81 (0.26)
Skill 111 (0.24) Skill 1 (0.99) Skill 35 (0.33) Skill 30 (0.34)
Skill 43 (0.33) Skill 5B (1.00) Skill 86 (0.25) Skill IlB (0.25) Skill 29 (0.24)
Skill 71 (0.26)
Skill 106 (0.49) Skill 27 (0.50) Skill 56 (0.14) Skill 6 (0.14)
Skill 116 (0.15) Skill 64 (0.14) Skill IOB (0.15) Skill 104 (0.14) Skill 23 (0.14)
Skill 39 (0.99)
Skill 126 (0.46) Skill 109 (0.54) Skill 120 (0.49) Skill 37 (0.51)
Skill 11 (0.24) Skill 4 (0.26) Skill 121 (0.25) Skill 0 (0.25) Skill 107 (0.20)
Skill B2 (0.21)
Skill 42 (0.19) Skill 62 (0.20) Skill B7 (0.19) Skill 93 (0.32)
Skill 7B (0.36) Skill 94 (0.31) Skill 45 (0.4β) Skill 75 (0.51) Skill 103 (1.00)
Skill 69 (1.00)
Skill 44 (1.00) Skill 112 (1.00) Skill 9 (1.00) Skill 9B (1.00)
Figure 19: Example Unbonused skill rollouts on Four Rooms. Each panel shows rollout for one
skill. Panels are labeled with skill index and the probability the discriminator assigns to the correct
skill label. Color indicates time within episode, moving from green (beginning) to yellow (end).
Skills are sorted by final state. Unbonused skill learning fails to learn to reach many states beyond
the first (upper left) room.
Skill 80 (0.50) Skill 31 (0.49) Skill IOO (1.00)
26
Published as a conference paper at ICLR 2022
。 5 Q 5
0 7 5 2
1
-Eeg S=DlS
Unbonused
一Ensemble
Count
DISDAIN
0.0e+00 2.5e+08 5.0e+08 7.5e+08 1.0e+09
environment steps
Figure 20: Four Rooms training curves broken out by seed. The exploration bonuses dramatically
improve skill learning in most cases, though they also slow learning and add variance due to the
training of an additional separate value function (and for DISDAIN, an ensemble of discriminators).
27