Published as a conference paper at ICLR 2022
Poetree: Interpretable Policy Learning with
Adaptive Decision Trees
Alizee Pace
ETH AI Center, Switzerland
ETH Zurich, Switzerland
MPI for Intelligent Systems, Tubingen, Germany
alizee.pace@ai.ethz.ch
Alex J. Chan
University of Cambridge, UK
ajc340@cam.ac.uk
Mihaela van der Schaar
University of Cambridge, UK
Cambridge Centre for AI in Medicine, UK
The Alan Turing Institute, UK
mv472@cam.ac.uk
Ab stract
Building models of human decision-making from observed behaviour is critical to
better understand, diagnose and support real-world policies such as clinical care.
As established policy learning approaches remain focused on imitation perfor-
mance, they fall short of explaining the demonstrated decision-making process.
Policy Extraction through decision Trees (Poetree) is a novel framework for in-
terpretable policy learning, compatible with fully-offline and partially-observable
clinical decision environments - and builds probabilistic tree policies determin-
ing physician actions based on patients’ observations and medical history. Fully-
differentiable tree architectures are grown incrementally during optimization to
adapt their complexity to the modelling task, and learn a representation of pa-
tient history through recurrence, resulting in decision tree policies that adapt over
time with patient information. This policy learning method outperforms the state-
of-the-art on real and synthetic medical datasets, both in terms of understanding,
quantifying and evaluating observed behaviour as well as in accurately replicating
it - with potential to improve future decision support systems.
1	Introduction
Different approaches to integrating, analysing and acting upon clinical information leads to wide,
unwarranted variation in medical practice across regions and institutions (McKinlay et al., 2007;
Westert et al., 2018). Algorithms designed to support the clinical decision-making process can
help overcome this issue, with successful examples in oncology prognosis (Beck et al., 2011; Es-
teva et al., 2017), retinopathy detection (Gulshan et al., 2016) and radiotherapy planning (Valdes
et al., 2017). Still, these support systems focus on effectively replacing physicians with autonomous
agents, trained to optimise patient outcomes or diagnosis accuracy. With limited design concern for
end-users, these algorithms lack interpretability, leading to mistrust from the medical community
(Lai et al., 2020; The Royal Society, 2017).
Instead, our work aims to better describe and understand the decision-making process by observing
physician behaviour, in a form of epistemology. These transparent models of behaviour can clearly
synthesise domain expertise from current practice and could thus form the building blocks of future
decision support systems (Li et al., 2015), designed in partnership with clinicians for more reliable
validation, better trust, and widespread acceptance. In addition, such policy models will enable
quantitative comparisons of different strategies of care, and may in turn lead to novel guidelines and
educational materials to combat variability of practice.
The associated machine learning challenge, therefore, is to explain how physicians choose clinical
actions, given new patient observations and their prior medical history - and thus describe an inter-
pretable decision-making policy. Modelling clinical diagnostic and treatment policies is particularly
challenging, with observational data involving confounding factors and unknown evolution dynam-
1
Published as a conference paper at ICLR 2022
Histories H
I Observations Z
Actions A
Figure 1: Schematic representation of behavioural policies as trees, adapting over time.
ics, which we wish to then translate into a transparent representation of decision-making. State-of-
the-art solutions for uncovering demonstrated policies, such as inverse reinforcement learning (Ng
& Russell, 2000) or imitation learning (Bain & Sammut, 1996; Piot et al., 2014; Ho & Ermon, 2016),
fall short of interpretability - mainly proposing black-box models as behaviour representations.
In contrast, transparent policy parametrisations based on visual decision boundaries (Huyuk et al.,
2021), high-level programming syntax (Verma et al., 2018) or outcome preferences (Yau et al., 2020)
can explain learned behaviour, at the cost of restrictive modelling constraints and obscure dynamics.
Among possible interpretable representations, decision trees offer the flexibility to capture high-
dimensional environments and, as we demonstrate, can model time-series through recurrence. In
addition, decision trees are highly familiar to the medical community, with many clinical guidelines
having this form (Chou et al., 2007; McCreery & Truelove, 1991; Burch et al., 2012). Cognitive re-
search suggests human decision-making does follow a hierarchical, branching process (Zylberberg
et al., 2017), making these models straightforward to understand. As a result, our algorithm Pol-
icy Extraction through decision Trees (Poetree) represents clinical policies as decision trees,
evolving over time with patient information - as illustrated in Figure 1.
Contributions The main contributions of this work are as follows: (i) A novel framework for
policy learning in the form of incrementally grown, probabilistic decision trees, which adapt their
complexity to the task at hand. (ii) An interpretable model for representation learning over time-
series called recurrent decision trees. (iii) Illustrative analyses of our algorithm’s expressive power
in disambiguating policies otherwise unidentifiable with related benchmarks; integration of domain
knowledge through inductive bias; and formalisation and quantification of abstract behavioural con-
cepts (e.g. uncertain, anomalous or low-value actions) - thus demonstrating its effectiveness in both
understanding and replicating behaviour.
2	Problem Formalism
Our goal is to learn an interpretable representation of a demonstrated decision-making process to
understand agent behaviour. The clinical context requires offline learning using observational data
only, as experimenting with policies would be both unethical and impractical. Finally, as previ-
ous observations and actions affect treatment choice, we are concerned with a partially-observable
decision-making environment.
We assume access to a dataset of m demonstrated patient trajectories in discrete time D =
{(z1i , ai1, . . . , zτii, aiτi)}im=1, where τi is the length of trajectory i. We drop index i to denote a generic
draw from the population. At timestep t, the physician-agent observes patient features, denoted by
random variable Zt ∈ Z, and chooses treatment or diagnostic action At ∈ A, where A is a finite
set of actions. Let zt and at denote realisations of these random variables. In contrast to traditional
Markovian environments, the agent’s reward function R, state space S and transition dynamics are
all inaccessible.
Let f denote a representation function, mapping the history of patient observations and actions to
a representation space H, and ht = f(z1, a1, ...zt-1 , at-1) = f(z1:t-1, a1:t-1) ∈ H represent
the patient history prior to observing a new set of features zt . Following the partially-observable
Markov Decision Process (POMDP) formalism, information in {ht , zt} can be combined to form a
belief over the inaccessible patient state st. In contrast to recent policy learning work on POMDPs,
which uncover mappings from belief to action space (Sun et al., 2017; Choi & Kim, 2011; Makino
& Takeuchi, 2012; Huyuk et al., 2021), however, our aim is to highlight how newly-acquired obser-
vations zt conditions action at, to ensure decisions are based on interpretable, meaningful variables.
As a result, agent behaviour must be represented as a mapping from observation to action space.
An adaptive decision-making policy is a stationary mapping π : Z × H × A → [0, 1], where
π(at∣zt, ht) encodes the probability of choosing action at given patient history ht and latest obser-
2
Published as a conference paper at ICLR 2022
Vation Zt - thus ∀t, £。,∈/ ∏(at∣zt, ht) = 1. We assume trajectories in D are generated by an agent
following a policy ∏e, such that at 〜∏e(∙∣zt, ht). The goal of our work is to find an interpretable
policy representation π which matches and explains the behaViour of πE .
3	Interpretable policy learning with decision trees
3.1	Soft decision trees
inner node π,q
softmax(0⅛)I ^lγ I
∖αt I
»
£
(a) DT policy.
MotiVated by the interpretability and perVasiVeness of trees in the medical literature (Chou et al.,
2007), Policy Extraction through decision Trees (Poetree) represents obserVed decision-making
policies as trees. Our model architecture is inspired by soft
decision trees - structures with binary nodes, probabilistic
decision boundaries and fixed leaf outputs (Irsoy et al., 2012)
- for their transparency and modelling performance (Frosst
& Hinton, 2017). This superVised learning framework can
be adapted for policy learning by setting input Variables as
x = zt or as {ht, zt} (for fully- or partially-obserVable enVi-
ronments), and targets as y = at (Bain & Sammut, 1996).
Inner nodes Illustrated in Figure 2a, the path followed by
normalised input x ∈ RD is determined by gating functions
encoding the probability of taking the rightmost branch at
each non-leaf node n: pgnate(x) = σ xTwn + bn , where
{wn ∈ RD ; bn ∈ R}n∈Inner are trainable parameters
and σ is the sigmoid function. This design choice is moti-
Vated by its demonstrated flexibility and expressiVity (SilVa
et al., 2020), and can be made interpretable by retaining the
largest component of wn to form an unidimensional axis-
aligned threshold at each node, as illustrated in Figure 1 and
detailed in Appendix C. Model simplification can be made
more robust through L1 regularisation, encouraging sparsity
in {wn}n∈Inner. The path probability of each node, Pn(x),
is therefore giVen by the product of gating functions leading
to it from the tree root.
time delay
--------1----
T
C
(b) RDT policy.
Figure 2: Poetree policy structures.
Leaf nodes In our tree architecture, leaVes define a fixed
probability distribution oVer K categorical output classes.
Leaf l encodes relative activation values ^t = Softmax M),
where θal ∈ RK is a learnable parameter. For ease of com-
putation and interpretation, the output of the maximum-probability leaf αltmax, where lmaχ
arg maxl Pl(x), is taken as overall classification decision, following Frosst & Hinton (2017).
3.2	Recurrent decision trees
To account for previously acquired information, our algorithm must jointly extract and depend on
a representation of patient history ht . As in Figure 2b, RDTs take history ht and new observation
zt as inputs, and output both predicted action at and subsequent history embeddings ht+1 from
decision leaves. These are computed as hlt+1 = tanh(θhl ), where θhl ∈ RM is an additional leaf
parameter - alternative leaf models are discussed in Appendix B. Finally, as third leaf output with
parameters θzl ∈ RD, our model also predicts patient evolution, or observations at the next timestep
Zt+ι, effectively capturing the expected effects of a treatment on the patient trajectory as a policy
explanation (Yau et al., 2020). A single tree structure is used at all timesteps: unfolded over time,
our recurrent tree can be viewed as a sequence of cascaded trees (Ding et al., 2021) with shared
parameters Θ = {{wn, bn}n∈Inner; {θal , θhl θzl }l∈Leaf} and topology T.
3
Published as a conference paper at ICLR 2022
3.3	Tree growth and optimisation
Optimisation objective The policy learning objective is to recover the tree topology T and param-
eters Θ which best describe the observation-to-action mapping demonstrated by physician agents.
Our loss function combines the cross-entropy between one-hot encoded target at ∈ {0, 1}K and
each leaf’s action output, weighted by its respective path probability under input {ht , zt}:
L({ht,zt ,at}； T, Θ) = - X P l(ht,zt) X at,k log [&晨]	(1)
l∈Leaf	k
where ^t k is the output probability for action class k in leaf l. Additional objectives are necessary
for the multiple outputs to match their intended target: regularisation term Lz is designed to learn
patient evolution Zt+ι as expected by the acting physician. It minimises prediction error on zt+ι and
ensures the policy is consistent between timesteps by constraining predicted observations to lead to
similar action choices as true ones, under the new history ht+1 :
Lz({ht,zt,zt+ι}; T, Θ) = δι ∣∣zt+ι - ≡t+ιk2 + δ2 DKL (∏(∙∣ht+1,zt+1 k∏(∙∣ht+1,Zt+1) (2)
where{δ1, δ2} are tunable hyperparameters, weighting fidelity to true evolution and to demonstrated
behaviour. For a set of training trajectories D, and an overall loss L = L + Lzz, our policy learning
optimisation objective becomes:
arg min L(D)	(3)
T,Θ
The probabilistic nature of our model choice makes it fully-differentiable, allowing optimisation of
parameters Θ through stochastic gradient descent and backpropagation through the structure for a
fixed topology T. Optimising with respect to T, however, requires a more involved algorithm.
Algorithm 1: Tree growth optimisation.
1. Initialise T to inner node with two suboptimal leaves;
Optimise Θ via gradient descent of loss L;
while ∃ suboptimal leaves in T do
1. Initialisation
inne
Split suboptimal leaf into inner node with two leaves;
2a. Locally optimise Θ via gradient descent of L;	SuboPtimaɪ leaf	「
if validation performance improves then retain split;	Local opt'mieation
else 2b. Retain leaf as optimal;
3. Globally optimise Θ via gradient descent of L;
Prune branches in T with low path probability Pl (Dval).
3. Global optimisation
and path pruning
2. For each suboptimal leaf:
a. Split node or b—abe IaPOmtimal
<0.05
Keep best-performing
in validation
Tree growth Following the work of Irsoy et al. (2012) and Tanno et al. (2019), our decision tree
architecture is incrementally grown and optimised during the training process, allowing the model
to independently adapt its complexity to the task at hand. Algorithm 1 summarises the growth
and optimisation procedure. Starting from an optimised shallow structure, tree growth is carried
out by sequentially splitting each leaf node and locally optimising new parameters - the split is
accepted if validation performance is improved. After a final global optimisation, branches with a
path probability below a certain threshold on the validation set are pruned. Further details on the
tree growth procedure are given in Appendix B.
Complexity Policy training can be computationally complex due to the multiple local tree optimi-
sations involved and significant number of parameters in comparison to other interpretable models
(HUyUk et al., 2021). Still, at test time, computing tree path probabilities for a given input is inex-
pensive, which is advantageous to allow fast decision support. A detailed comparison of runtimes
involved with our method and related work is proposed in Appendix D.
4 Related Work
In this section, we compare related work on interpretable policy learning, and contrast them in terms
of key desiderata outlined in Section 2. Table 1 summarises our findings.
Policy learning While sequential decision-making problems are traditionally addressed through
reinforcement learning, this framework is inapplicable to our policy learning goal due to the unavail-
ability of a reward signal R. Instead, we focus on the inverse task of replicating the demonstrated
behaviour of an agent, known as imitation learning (IL), which assumes no access to R. Com-
mon approaches include behavioural cloning (BC) where this task is reduced to supervised learning,
4
Published as a conference paper at ICLR 2022
Table 1: Related work. Comparison to policy learning methods in terms of interpretability.
Related work	Offline Learning	Partial Observability	Interpretable Policy	Modelling Assumptions
BC-IL (Bain & Sammut, 1996)	✓	X	✓	-
PO-BC-IL (Sun et al., 2017)	✓	✓	X	-
Interpretable BC-IL (Huyuk et al., 2021)	✓	✓	✓	S known; low-dim. env.
DM-IL (Ho & Ermon, 2016)	X	X	X	-
MB-IL (Englert et al., 2013)	✓	X	X	Simple dynamics
IRL (Ng & Russell, 2000)	X	X	X	πE optimal wrt. R
PO-IRL (Choi & Kim, 2011)	X	✓	X	πE optimal wrt. R
Offline PO-IRL (Makino & Takeuchi, 2012)	✓	✓	X	πE optimal wrt. R
Interpretable RL (Silva et al., 2020)	X	X	✓	R known
POETREE (Ours)	✓	✓	✓	-
mapping states to actions (Bain & Sammut, 1996; Piot et al., 2014); and distribution-matching (DM)
methods, where state-action distributions between demonstrator and learned policies are matched
through adversarial learning (Ho & Ermon, 2016; Jeon et al., 2018; Kostrikov et al., 2020). Ap-
prenticeship learning (AL) aims to reach or surpass expert performance on a given task - a closely
related but distinct problem. The state-of-the-art solution is inverse reinforcement learning (IRL),
which recovers R from demonstration trajectories (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart
et al., 2008); followed by a forward reinforcement learning algorithm to obtain the policy.
Towards interpretability for understanding decisions Imitation and apprenticeship learning so-
lutions have been developed to tackle the challenges of fully-offline learning in partially-observable
decision environments, as required by the clinical context: in IL, Sun et al. (2017) learn belief states
through RNNs, and IRL was adapted for partial-observability (Choi & Kim, 2011) and offline learn-
ing (Makino & Takeuchi, 2012). However, while black-box neural networks typically provide best
results, a transparent description of behaviour requires parametrising the learned policy to be in-
formative and understandable. This often involves sacrificing action-matching performance (Lage
et al., 2018), and introduces assumptions which limit the applicability of the proposed method, such
as Markovianity (Silva et al., 2020)or low-dimensional state spaces (Huyuk et al., 2021). The latter
work, for instance, parametrises action choice through decision boundaries within the agent’s belief
space. In addition to leaving the observations-to-action mapping obscure, this imposes constraints
on modelled states and dynamics - which limit predictive performance, scalability and policy iden-
tifiability (Biasutti et al., 2012).
One approach to achieve interpretability in decision models is to implement interpretable reward
functions for IRL, providing insight into demonstrating agents’ inner objectives. Behaviour has thus
been explained as preferences over counterfactual outcomes (Bica et al., 2021), or over information
value in a time-pressured context (Jarrett & van der Schaar, 2020). Still, the need for black-box RL
algorithms to extract a policy obscures how observations affect action choice. Instead, the agent’s
policy function can be directly parametrised as an interpretable structure. RL policies have been
represented through a high-level programming syntax (Verma et al., 2018), or through explanations
in terms of intended outcomes (Yau et al., 2020). Related work on building interpretable models for
time-series in general is also discussed in Appendix B.
Adaptive decision trees for time-series Related soft and grown tree architectures have been pro-
posed for image-based tasks (Frosst & Hinton, 2017; Tanno et al., 2019) - little research has been
reported on probabilistic decision trees for the dynamic, heterogeneous and sparse nature of time-
series data. Tree structures have been proposed for representation learning, relying for instance on
projecting inputs to more informative features in between decision nodes (Tanno et al., 2019) or on
cascaded trees, sequentially using their outputs as input to subsequent ones (Ding et al., 2021). In
all cases, however, models lose interpretability, as decisions are based on unintuitive variables: in
contrast, we build thresholds over native observation variables, meaningful to human experts. Our
work is further contrasted with established tree models in Appendix B.
5	Illustrative Examples
5.1	Experimental Setup
Decision-Making Environments Three medical datasets were studied to evaluate our work. First,
as ground-truth policies πE are inaccessible in real data, a synthetic dataset of 1000 patient trajec-
5
Published as a conference paper at ICLR 2022
tories over 9 timesteps, denoted SYNTH, was simulated. In a POMDP with binary disease state
space, observations include one-dimensional diagnostic tests {z+, z-} as well as 10 noisy dimen-
sions disregarded by the agent (e.g. irrelevant measurements); and actions {a+, a-} correspond
to treatment and lack thereof. Assuming treatments require exposure beyond the end of symp-
toms (Entsuah et al., 1996), the expert policy treats patients who tested positive over the last three
timesteps: at = πE(ht, zt) = a+ if z+ ∈ {zt-2, zt-1, zt}; a- else. Next, a real medical dataset was
explored, following 1,625 patients from the Alzheimer’s Disease Neuroimaging Initiative (ADNI),
as in Huyuk et al. (2021) for benchmarking. We consider the task of predicting, at each visit,
whether a Magnetic Resonance Imaging (MRI) scan is ordered for cognitive disorder diagnosis
(NICE, 2018). Patient observations consist of the Clinical Dementia Rating (CDR-SB) on a severity
scale (normal; questionable impairment; severe dementia) following O’Bryant et al. (2008); and the
MRI outcome of the previous visit, categorised into four possibilities (no MRI scan; and below-
average, average and above-average hippocampal volume Vh). Finally, we also consider a dataset of
4,222 ICU patients over up to 6 timesteps extracted from the third Medical Information Mart for In-
tensive Care (MIMIC-III) database (Johnson et al., 2016) and predict antibiotic prescription based
on 8 observations - temperature, white blood cell count, heart rate, hematocrit, hemoglobin, blood
pressure, creatinine and potassium; as in Bica et al. (2021). By dataset design or by the nature of
real-world clinical practice, observation history must be considered by the acting policies - making
our decision-making environments partially-observable.
Success Metrics Reflecting our unconventional priority for policy interpretability over imitation
performance, we provide illustrative examples evidencing greater insight into observed behaviour
through decision tree representations. For a more quantitative assessment of interpretability, we also
surveyed five practising UK clinicians of different seniority level, asking them to score models out
of ten on how well they could understand the decision-making process - with details in Appendix
G. Action-matching performance was also evaluated through the areas under the receiver-operating-
characteristic curve (AUROC) and precision-recall curve (AUPRC), and through Brier calibration.
Benchmark Algorithms For benchmarking, we implemented different behavioural cloning mod-
els, mapping observations or belief states to actions: (i) decision tree (Tree BC-IL) and logistic
regression models with no patient history; (ii) a partially-observable BC algorithm (PO-BC-IL)
extracting history embeddings through an RNN (Sun et al., 2017), on which a feature importance
analysis is carried out (Lundberg et al., 2017); (iii) and an interpretable PO-BC-IL model, inter-
pole (Huyuk et al., 2021), described in Section 4. Benchmarks also include (iv) model-based
imitation learning PO-MB-IL (Englert et al., 2013), adapted for partial-observability with a learned
environment model; (v) Bayesian Inverse Reinforcement Learning for POMDPs (PO-IRL), based
on Jarrett & van der Schaar (2020); and (vi) a Offline PO-IRL version of this algorithm as in
Makino & Takeuchi (2012). Implementation details are provided in Appendix E.
5.2	Interpretability
In light of our priority on recovering an interpretable policy, we first propose several clinical exam-
ples and insights from the ADNI dataset to highlight the usefulness of our tree representation.
Explaining patient trajectories Our first example studies decision-making within typical ADNI
trajectories. Let Patient A be healthy with normal CDR score and average Vh on initial scan. Let
Patient B initially have mild cognitive impairment (MCI), progressing towards dementia: their CDR
degrades from questionable to severe at the last timestep and a below-average Vh is measured at each
visit. Patient C is diagnosed with dementia at the first visit, with severe CDR and low Vh : no further
scans are ordered as they would be uninformative (NICE, 2018). Figure 3a illustrates our learned
tree policy, both varying with and determining patient history at each timestep (see Appendix F
for the distinct policy of each patient). Patient A is not ordered any further scan, as they never
show low Vh or abnormal CDR, but may be ordered one if they develop cognitive impairment. For
Patient B with low Vh but a CDR score not yet severe, another MRI is ordered to monitor disease
progression. At the following visit, patient history conditions the physician to be more cautious of
scan results: even if Vh is found to be acceptable, another scan may be ordered if the CDR changes.
Finally, Patient C’s observations already give a certain diagnosis of dementia: no scan is necessary.
At following timesteps, unless the patient unexpectedly recovers, their history suggests to not carry
out any further scans. We must highlight the similar strategy between our decision tree policy and
published guidelines for Alzheimer’s diagnosis in Biasutti et al. (2012), reproduced in Appendix F:
our policy correctly learns that investigations are only needed until diagnosis is confirmed.
6
Published as a conference paper at ICLR 2022
Healthy
Vh low
---Patient A
---Patient B
—Patient C
CDR-SB normal
CDR-SB severe
MRI X
Vh low
I CDR-SB normal ∣ MRI
No previous MRI X
MRI ×
I______________________________________________________________________________________
(a) Adaptive decision tree policy
Figure 3: Comparison of interpretable policy learning methods on ADNI. Crosses stand for no action. Fig.
(b) follows a method from Huyuk et al. (2021).
Dementia
C ,DR-SB severe
MRI ×
(b) Decision
boundary over belief
(d) Logistic
regression
0.0	0.2	0.4	0.6	0.8	1.
Probabiliiy of low 1⅞
(c) Static decision
tree policy
High %-
Average I⅛-
No Prev. MRI-
Severe CDR-
Quest. CDR-
NonXIaI CDR
(e) Feature
importance
.00	.05	.10	.15	.2 0	.25
F⅛ature importance
Vh low
CDR-SB normal
29%	64%	\
MRI]	[MRI	X
63%
76%
Jr(Oe = MRI∣hι,zχ) = 0.29 X 0.63 + 0.64 X 0.76 + …
= 0.67
CDR-SB questionable
CDR-SB severe
---A (Heattby) ------ B (Degmding)	- C (Demsntia)
11%
MRI ×
67%
Vh low
CDR-SB severe
81%
MRI ×
83%
ιr(μt =A0G∣h4,¾) = ŋ,ll X 0.67 + 0.81 X 0.83 +...
= 0.75
i + l
Figure 4: Policy confidence on ADNI. Maximum probability paths are highlighted in blue, with action proba-
bilities in green/red.
In contrast, the black-box history extraction procedure required by most benchmarks for partial-
observability (PO-BC-IL, PO-IRL, Offline PO-IRL, PO-MB-IL) is difficult to interpret, with no
clear decision-making process. The policy learned by INTERPOLE (Huyuk et al., 2021) in Figure
3b, defined by decision boundaries over a subjective disease belief space, provides more insight:
this model suggests respectively few and frequent scans for healthy Patient A and MCI Patient B, at
different vertices of the belief space, but cannot account for scan reduction in diagnosed Patient C as
this requires discontinuous action regions. This policy representation only relies on disease beliefs
and thus cannot identify the effect of different observations on treatment strategies. Finally, com-
mon interpretable models proposed in Figures 3c_3e fail to convey an understanding of behavioural
strategy and evolution.
Decision-making uncertainty Our behaviour model also inherently captures the useful notion of
decision confidence thanks to its probabilistic nature:
π(at = k∖ht,zt) = X : P((ht,zt) ∙ at,k	(4)
l∈Leaf
where, for each leaf node l, Pl(ht, zt) and alt,k are the path and output probability for action k.
Figure 4 highlights how policy confidence varies over time for each typical patient. Uncertainty for
Patient A can be attributed to variability in healthy patient care in the demonstrations dataset, as
scans may provide information about their state, but must be balanced with cost and time considera-
tions (NICE, 2018). For Patient C with a dementia diagnosis, further investigations are not required
(Biasutti et al., 2012), reflected in confidence increase. As a result, for Patient B with degrading
symptoms, scans are initially ordered with low confidence to monitor progression, but as symptoms
worsen and the patient is diagnosed, decision confidence increases. Figure 4 also evidences both
greater inter-path and intra-leaf uncertainty1 for MRI prediction: our policy model identifies clear
conditions not warranting a scan after diagnosis, whereas conditions warranting one are more am-
biguous. This example illustrates the value of uncertainty estimation afforded by our framework,
modulating probability values within tree paths and leaf outputs.
1 Inter-path uncertainty can be measured as the entropy within leaf path probabilities - Pl Pl(x) log Pl(x),
while intra-leaf uncertainty can be measured as the average leaf output entropy: —1/1 Pl Pk at k log ^t 卜.
7
Published as a conference paper at ICLR 2022
92%	60%
π(αt = MRIlM ¾) = 0.95 × 0.92 + 0.04 × (1 — 0.60) + …
=0.90	y
π(at	∙≡⅛) = 0∙10 收 .-----------I
Ons
CTeCtl亨，
4 3 2 1 0
(第 SlHW 8A□EUU0JUIUn
(a) Anomalous behaviour per cohort.	(b) Uninformative actions.
Figure 5: Anomalous and low-value actions in various patient cohorts: patients over 75 years old, patients
carrying the 4 allele of the apolipoprotein E gene (ApoE4), patients with signs of MCI or dementia at their
first visit, female and male patients.
Anomalous behaviour detection Learned models of behaviour are also valuable to flag actions
incompatible with observed policies. Visits where an MRI is predicted with 90% certainty, yet the
physician agent does not order it - as in Figure 5a - make UP 8.4% of ADNL This may highlight
demonstration flaws, as the learned policy is confident and has well-calibrated probability estimates
(low Brier score in Table 2). Anomalous behaviour followed by an MRI ‘correcting’ the off-policy
action can also be detected, suggesting 6.7% of patients are thus investigated late - comparable to
belated diagnoses found in 6.5% of patients by HUyUk et al. (2021). As shown in Figure 5a, high-
risk cohorts due to age, female gender or ApoE4 allele (Launer et al., 1999; Lindsay et al., 2002)
are more often subject to late actions. Most anomalous actions on patients with ApoE4 or female
patients end up corrected, in contrast to older patients or ones with early symptoms - imaging is less
paramount for the latter, as they may be diagnosed clinically (NICE, 2018).
Action value quantification through counterfactual evolution Our joint expected evolution and
policy learning model allows action value to be quantified (Yau et al., 2020; Bica et al., 2021). Low-
value actions correspond to an ordered scan showing similar observations to the previous timestep,
when this could already be foreseen from expected evolution Z (less than average variation minus one
standard deviation). As shown in Figure 5b, ill patients are more often monitored with uninformative
MRIs than healthy ones. This analysis explains the high rate of uninformative actions for patients
with early signs of disease, found in previous work (Huyuk et al., 2021): the policy recommends
investigation for diagnosis, yet their expected evolution is typically unambiguous. Overall, our
decision tree architecture enables straightforward assessment of counterfactual patient evolution
under different observation values.
Overall, our decision tree policy parametrisation explains demonstrated decision-making behaviour
in terms of how actions are chosen based on observations and prior history, and, in the above
illustrative analyses, allows a formalisation of abstract notions - such as uncertain, anomalous or
low-value choices - for quantitative policy evaluation. Next, we show that these insights are not
gained at the cost of imitation performance.
5.3 Policy fidelity
Action-matching performance As shown in Table 2, our approach outperforms or comes second-
best on all success metrics compared to benchmark algorithms. This confirms the expressive power
of our recurrent tree policies over decision boundaries (Huyuk et al., 2021) and even over neural-
network-based methods (Sun et al., 2017), specifically optimised for action-matching. The poor
action-matching performance of static decision trees (Tree BC-IL) highlights the importance of his-
tory representation learning in overcoming the partial-observability of the decision-making environ-
ment. Mean relative evolution error between Zt and Zt was measured as 13 ± 4% and 26 ± 9% on
ADNI and MIMIC respectively, giving reasonable and insightful values. Our model was scored as
the second most understandable in our user survey, following the overly-simplistic static decision
tree (Fig. 3c). Collected feedback is provided in Appendix G.
Policy identifiability Figure 6 illustrates policies learned from our high-dimensional SYNTH
dataset with POETREE and the closest related work (HUyUk et al., 2021). Despite the apparent
simplicity of the decision-making problem, Interpole fails to identify the outlined treatment strat-
egy, as evidenced by its inferior performance in Table 2. This illustrates greater flexibility of our
8
Published as a conference paper at ICLR 2022
Table 2: Comparison of the performance of policy learning algorithms2on medical datasets. Interpretabil-
ity scores out of ten were obtained through our clinician survey. Lower is better for Brier calibration. Standard
errors for MIMIC and SYNTH were ≤ 0.04.
Task Algorithm	ADNI MRI scans				MIMIC antibiotics		SYNTH	
	Interpretability	AUROC	AUPRC	Brier	AUROC	Brier	AUROC	Brier
Tree BC-IL	9.3 ± 0.3	0.53 ± 0.01	0.72 ± 0.01	0.25 ± 0.01	0.50	0.23	-	-
PO-BC-IL [47]	0.3 ± 0.3	0.59 ± 0.04	0.80 ± 0.08	0.18 ± 0.05	0.67	0.16	0.98	0.02
Interpole [23]	7.3 ± 0.5	0.44 ± 0.04	0.75 ± 0.09	0.19 ± 0.07	0.65	0.21	0.84	0.12
PO-MB-IL [13]	-	0.54 ± 0.08	0.81 ± 0.07	0.19 ± 0.03	-	-	-	-
PO-IRL [25]	-	0.50 ± 0.08	0.82 ± 0.04	0.23 ± 0.01	-	-	-	-
Offline PO-IRL [37]	-	0.54 ± 0.06	0.83 ± 0.04	0.23 ± 0.01	-	-	-	-
Poetree	8.3 ± 0.3	0.62 ± 0.01	0.82 ± 0.01	0.18 ± 0.01	0.68	0.19	0.99	0.01
tree representation in disambiguating decision-making factors
which simply cannot be captured by previous work; and, due to not
relying on HMM-like dynamics, can be credited to fewer modelling
assumptions and scalability to high-dimensional environments.
Inductive bias Having demonstrated that our policy learning
framework allows us to recover domain expertise, we investigated
the effect of incorporating prior policy knowledge on learning per-
formance. In practice, our model’s action prediction structure can
be initialised based on a published guideline (Biasutti et al., 2012)
as in Appendix F: training time was reduced by a factor of 1.4
and action-matching AUPRC was improved to 0.84 ± 0.01. This
‘warm-start’ is a promising strategy to learn from physician obser-
vation while building on established clinical practice standards.
Accuracy-interpretability trade-off: complexity analysis In-
crementally grown trees outperform fixed structures by eliminating
unnecessary partitions, with 〜40% fewer parameters for the same
depth, greater readability (Lage et al., 2018) and reasonable training
time. Grown structures adapt their depth and complexity to avoid
overfitting while capturing the modelling task, in a form of Occam’s
razor - jointly optimising policy accuracy and interpretability. In
particular, sample complexity modulates learned policies in train-
ing. Detailed experimental results are provided in Appendix D.
(a) Decision-boundary policy
(Huyuk et al., 2021).
t + l
(b) Adaptive decision tree policy.
Figure 6: Recovered policies for
SYNTH.
6 Conclusion
Established paradigms to replicate demonstrated behaviour fall short of explaining learned decision
policies, whereas the state-of-the-art in interpretable policy learning was criticised by end-users for
restrictive modelling assumptions (Huyuk et al., 2021) - thus not fully capturing diagnostic strate-
gies. Policy Extraction with Decision Trees (Poetree) overcomes these challenges, outputting
most likely actions and uncertainty estimates, given patients’ observations and medical history. Our
algorithm carries out joint policy learning, history representation learning and evolution prediction
tasks through a recurrent, multi-output tree model - justifying agent decisions by comparing coun-
terfactual patient evolutions. Optimised structures outperform the state-of-the-art in interpretability,
policy fidelity and action-matching, thus providing an understanding of demonstrated behaviour
without compromising accuracy. In particular, we can formalise and quantify concepts of practice
variation with time; uncertain, anomalous, or low-value behaviour detection; and domain knowledge
integration through inductive bias. Finally, we validate clinical insights through a survey of intended
users.
Beyond policy learning, our novel interpretable framework for time-series modelling shows promise
for alternative tasks relevant to clinical decision support, such as patient trajectory prediction, out-
come estimation or human-in-the-loop learning for treatment recommendation -a promising avenue
of research for further work.
2References: [47] Sun et al. (2017); [23] Huyuk et al. (2021); [13] Englert et al. (2013); [25] Jarrett &
van der Schaar (2020); [37] Makino & Takeuchi (2012).
9
Published as a conference paper at ICLR 2022
Acknowledgments
We thank all NHS clinicians who participated in our survey and the Alzheimer’s Disease Neuroimag-
ing Initiative and Medical Information Mart for Intensive Care for access to datasets. This publica-
tion was partially made possible by an ETH AI Center doctoral fellowship to AP. AJC would like
to acknowledge and thank Microsoft Research for its support through its PhD Scholarship Program
with the EPSRC. Many thanks to group members of the van der Schaar Lab and to our reviewers for
their valuable feedback.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In-
ternational Conference on Machine Learning (ICML), 2004.
Michael Bain and Claude Sammut. A framework for behavioural cloning. MACHINE INTELLI-
GENCE 15, pp. 103-129, 1996. URL http://citeseerx.ist.psu.edu/viewdoc/
summary?doi=10.1.1.25.1759.
AH Beck, AR Sangoi, S Leung, RJ Marinelli, TO Nielsen, MJ van de Vijver, RB West, M van de
Rijn, and D Koller. Systematic analysis of breast cancer morphology uncovers stromal fea-
tures associated with survival. Science translational medicine, 3, 11 2011. ISSN 1946-6242.
doi: 10.1126/SCITRANSLMED.3002564. URL https://pubmed.ncbi.nlm.nih.gov/
22072638/.
Yoshua Bengio and Paolo Frasconi. An input output hmm architecture. Adv. Neural Inf. Process.
Syst., 7:427-434, 1995.
Maria Biasutti, Natacha Dufour, Clotilde Ferroud, William Dab, and Laura Temime. Cost-
effectiveness of magnetic resonance imaging with a new contrast agent for the early diagnosis of
alzheimer’s disease. PLOS ONE, 7:e35559, 4 2012. ISSN 1932-6203. doi: 10.1371/JOURNAL.
PONE.0035559. URL https://journals.plos.org/plosone/article?id=10.
1371/journal.pone.0035559.
Ioana Bica, Daniel Jarrett, Alihan HUyUk, and Mihaela van der Schaar. Learning "what-if” explana-
tions for sequential decision-making. Proc. 9th International Conference on Learning Represen-
tations (ICLR 2021), 2021. URL http://arxiv.org/abs/2007.13531.
Leo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and regression trees.
Wadsworth & Brooks/Cole, 1984.
J BUrch, S Hinde, S Palmer, F Beyer, J Minton, A Marson, U Wieshmann, N Woolacott, and
M Soares. The clinical effectiveness and cost-effectiveness of technologies Used to visUalise
the seizUre focUs in people with refractory epilepsy being considered for sUrgery: a systematic
review and decision-analytical model. Health Technology Assessment, 16:1-163, 9 2012. ISSN
ISSN: 2046-4924. doi: 10.3310/HTA16340. URL www.hta.ac.uk.
Edward Choi, Mohammad Taha Bahadori, JoshUa A. KUlas, Andy SchUetz, Walter F. Stewart, and
Jimeng SUn. Retain: An interpretable predictive model for healthcare Using reverse time attention
mechanism. Avances in Neural Information Processing Systems (NIPS), 29, 8 2016. URL http:
//arxiv.org/abs/1608.05745.
JaedeUg Choi and Kee-EUng Kim. Inverse reinforcement learning in partially observable environ-
ments. Journal of Machine Learning Research, 12:691-730, 2011. ISSN 1533-7928. URL
http://jmlr.org/papers/v12/choi11a.html.
Roger ChoU, Amir Qaseem, Vincenza Snow, Donald Casey, Thomas J. Cross, PaUl Shekelle, and
DoUglas K. Owens. Diagnosis and treatment of low back pain: A joint clinical practice gUide-
line from the american college of physicians and the american pain society. Annals of Internal
Medicine, 147:478-491, 10 2007. doi: 10.7326/0003-4819-147-7-200710020-00006.
Zihan Ding, Pablo Hernandez-Leal, Gavin WeigUang Ding, Changjian Li, and RUitong HUang.
Cdt: Cascading decision trees for explainable reinforcement learning. arXiv preprint
arXiv:2011.07553, 2021. URL https://github.com/openai/gym/blob/master/
gym/envs/box2d/lunar.
10
Published as a conference paper at ICLR 2022
Peter Englert, Alexandros Paraschos, Marc Peter Deisenroth, and Jan Peters. Probabilistic model-
based imitation learning. Adaptive Behavior,21:388-403, 10 2013. ISSN 10597123. doi: 10.
1177/1059712313491614. URL https://journals.sagepub.com/doi/full/10.
1177/1059712313491614.
A. R. Entsuah, R. L. Rudolph, D. Hackett, and S. Miska. Efficacy of venlafaxine and placebo
during long-term treatment of depression: A pooled analysis of relapse rates. International Clin-
ical Psychopharmacology, 11:137-145, 1996. doi: 10.1097/00004850-199611020-00008. URL
/record/1996-05005-007.
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
Nature 2017 542:7639, 542:115-118, 1 2017. ISSN 1476-4687. doi: 10.1038/nature21056. URL
https://www.nature.com/articles/nature21056.
Wael Farag. Safe-driving cloning by deep learning for autonomous cars. International Journal of
Advanced Mechatronic Systems, 7:390-397, 2017. doi: 10.1504/IJAMECHS.2017.099318.
Nicholas Frosst and Geoffrey Hinton. Distilling a neural network into a soft decision tree. CEX
workshop at AI*IA 2017 conference, 11 2017. URL http://arxiv.org/abs/1711.
09784.
Matthew Golub, Steven Chase, and Byron Yu. Learning an internal dynamics model from control
demonstration. pp. 606-614, 2 2013. ISSN 1938-7228. URL http://proceedings.mlr.
press/v28/golub13.html.
V Gulshan, L Peng, M Coram, MC Stumpe, D Wu, A Narayanaswamy, S Venugopalan, K Widner,
T Madams, J Cuadros, R Kim, R Raman, PC Nelson, JL Mega, and DR Webster. Development
and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus
photographs. JAMA, 316:2402-2410, 12 2016. ISSN 1538-3598. doi: 10.1001/JAMA.2016.
17216. URL https://pubmed.ncbi.nlm.nih.gov/27898976/.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. 30th Conference on
Neural Information Processing Systems, 2016.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-
1780, 11 1997. ISSN 08997667. doi: 10.1162/neco.1997.9.8.1735. URL http://direct.
mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf.
Stephanie L. Hyland, Martin Faltys, Matthias Huser, Xinrui Lyu, Thomas Gumbsch, Cristobal Es-
teban, Christian Bock, Max Horn, Michael Moor, Bastian Rieck, Marc Zimmermann, Dean Bo-
denham, Karsten Borgwardt, Gunnar Ratsch, and Tobias M. Merz. Early prediction of circu-
latory failure in the intensive care unit using machine learning. Nature Medicine 2020 26:3,
26:364-373, 3 2020. ISSN 1546-170X. doi: 10.1038/s41591-020-0789-4. URL https:
//www.nature.com/articles/s41591- 020-0789-4.
Alihan Huyuk, Daniel Jarrett, Cem Tekin, and Mihaela Van Der Schaar. Explaining by imitating:
Understanding decisions by interpretable policy learning. ICLR, 2021.
Ozan Irsoy, Olcay Taner Yildiz, and Ethem Alpaydin. Soft decision trees. 21st International Con-
ference on Pattern Recognition (ICPR), 2012.
Daniel Jarrett and Mihaela van der Schaar. Inverse active sensing: Modeling and understanding
timely decision-making. Proceedings of the 37th International Conference on Machine Learning,
2020.
Wonseok Jeon, Seokin Seo, and Kee-Eung Kim. A bayesian approach to generative adversarial
imitation learning. Advances in Neural Information Processing Systems, 31, 2018.
Alistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li Wei H. Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. Mimic-iii,
a freely accessible critical care database. Scientific Data, 3:1-9, 5 2016. ISSN 20524463. doi:
10.1038/sdata.2016.35. URL www.nature.com/sdata/.
11
Published as a conference paper at ICLR 2022
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 3rd International
Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, 12 2015.
URL https://arxiv.org/abs/1412.6980v9.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. ICLR, 12 2020. URL http://arxiv.org/abs/1912.05032.
Isaac Lage, Andrew Slavin Ross, Been Kim Google Brain, Samuel J Gershman, and Finale Doshi-
Velez. Human-in-the-loop interpretability prior. 32nd Conference on Neural Information Pro-
cessing Systems (NIPS 2018), 2018.
LJ Launer, K Andersen, ME Dewey, L Letenneur, A Ott, LA Amaducci, C Brayne, JR Copeland,
JF Dartigues, P Kragh-Sorensen, Lobo A, Martinez-Lage JM, T Stijnen, and A Hofman. Rates
and risk factors for dementia and alzheimer’s disease: results from eurodem pooled analyses.
eurodem incidence research group and work groups. european studies of dementia. Neurology,
52:78-84, 1 1999. ISSN 0028-3878. doi: 10.1212/WNL.52.1.78. URL https://pubmed.
ncbi.nlm.nih.gov/9921852/.
M.-C Lai, M Brian, and M.-F Mamzer. Perceptions of artificial intelligence in healthcare: findings
from a qualitative survey study among actors in france. J Transl Med, 18:14, 2020. doi: 10.1186/
s12967-019-02204-y. URL https://doi.org/10.1186/s12967-019-02204-y.
A Li, S Jin, L Zhang, and Y Jia. A sequential decision-theoretic model for medical diagnostic
system. Technology and health care : official journal of the European Society for Engineering
and Medicine, 23 Suppl 1:S37-S42, 5 2015. ISSN 1878-7401. doi: 10.3233/THC-150926. URL
https://pubmed.ncbi.nlm.nih.gov/26410326/.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from vi-
sual demonstrations. Proceedings of the 31st International Conference on Neural Information
Processing Systems, pp. 3815-3825, 2017. URL https://github.com/ermongroup/
InfoGAIL.
J Lindsay, D Laurin, R Verreault, R Hebert, B Helliwell, GB Hill, and I McDowell. Risk factors for
alzheimer’s disease: a prospective analysis from the canadian study of health and aging. American
journal of epidemiology, 156:445-453, 9 2002. ISSN 0002-9262. doi: 10.1093/AJE/KWF074.
URL https://pubmed.ncbi.nlm.nih.gov/12196314/.
Scott M Lundberg, Paul G Allen, and Su-In Lee. A unified approach to interpreting model pre-
dictions. 31st Conference on Neural Information Processing Systems (NIPS 2017), 2017. URL
https://github.com/slundberg/shap.
Takaki Makino and Johane Takeuchi. Apprenticeship learning for model parameters of partially ob-
servable environments. Proceedings of the 29 th International Conference on Machine Learning,
pp. 891-898, 2012.
R. G. Masterton, A. Galloway, G. French, M. Street, J. Armstrong, E. Brown, J. Cleverley, P. Dil-
worth, C. Fry, A. D. Gascoigne, Alan Knox, Dilip Nathwani, Robert Spencer, and Mark Wilcox.
Guidelines for the management of hospital-acquired pneumonia in the uk: Report of the work-
ing party on hospital-acquired pneumonia of the british society for antimicrobial chemotherapy.
Journal of Antimicrobial Chemotherapy, 62:5-34, 7 2008. ISSN 0305-7453. doi: 10.1093/JAC/
DKN162. URL https://academic.oup.com/jac/article/62/1/5/844812.
Ann M. McCreery and Edmond Truelove. Decision making in dentistry. part i: A historical and
methodological overview. The Journal of Prosthetic Dentistry, 65:447-451, 3 1991. ISSN 0022-
3913. doi: 10.1016/0022-3913(91)90241-N.
J. B. McKinlay, C. L. Link, K. M. Freund, L. D. Marceau, A. B. O’Donnell, and K. L.
Lutfey. Sources of variation in physician adherence with clinical guidelines: Results
from a factorial experiment. Journal of General Internal Medicine, 22:289, 3 2007. doi:
10.1007/S11606-006-0075-2. URL /pmc/articles/PMC1824760//pmc/articles/
PMC1824760/?report=abstracthttps://www.ncbi.nlm.nih.gov/pmc/
articles/PMC1824760/.
12
Published as a conference paper at ICLR 2022
Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. Proc. 17th
International Conf. on Machine Learning, pp. 663-670, 2000.
NICE. Dementia: Assessment, management and support for people living with dementia and their
carers. National Institute for Health and Care Excellence (NICE) Guideline No. 97, 2018. URL
http://www.nice.org.uk/guidance/ng97.
SE O’Bryant, SC Waring, CM Cullum, J Hall, L Lacritz, PJ Massman, PJ Lupo, JS Reisch, and
R Doody. Staging dementia using clinical dementia rating scale sum of boxes scores: a texas
alzheimer’s research consortium study. Archives of neurology, 65:1091-1095, 8 2008. ISSN
1538-3687. doi: 10.1001/ARCHNEUR.65.8.1091. URL https://pubmed.ncbi.nlm.
nih.gov/18695059/.
Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, and Jan Peters.
An algorithmic perspective on imitation learning. Foundations and Trends ® in Robotics, 7:
1-179, 2018. doi: 10.1561/2300000053.
Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classification
for apprenticeship learning. Proceedings of the 13th Inter-national Conference on Autonomous
Agents and MultiagentSystems (AAMAS 2014), 2014. URL www.ifaamas.org.
Andrew Silva, Taylor Killian, Ivan Rodriguez Jimenez, Sung-Hyun Son, and Matthew Gombolay.
Optimization methods for interpretable differentiable decision trees in reinforcement learning.
Proceedings of the 23rdInternational Conference on Artificial Intelligence and Statistics (AIS-
TATS), 2020.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply
aggrevated: Differentiable imitation learning for sequential prediction. Proceedings of the 34th
International Conference on Machine Learning, pp. 3309-3318, 7 2017. ISSN 2640-3498. URL
http://proceedings.mlr.press/v70/sun17d.html.
Ryutaro Tanno, Kai Arulkumaran, Daniel C Alexander, Antonio Criminisi, and Aditya Nori. Adap-
tive neural trees. Proceedings of the 36th International Conference on Machine Learning, pp.
6166-6175, 5 2019. ISSN 2640-3498. URL http://proceedings.mlr.press/v97/
tanno19a.html.
The Royal Society. Machine learning: The power and promise of computers that learn by example.
The Royal Society, 2017.
Gilmer Valdes, Charles B. Simone, Josephine Chen, Alexander Lin, Sue S. Yom, Adam J. Pat-
tison, Colin M. Carpenter, and Timothy D. Solberg. Clinical decision support of radiotherapy
treatment planning: A data-driven machine learning strategy for patient-specific dosimetric de-
cision making. Radiotherapy and Oncology, 125:392-397, 12 2017. ISSN 0167-8140. doi:
10.1016/J.RADONC.2017.10.014.
Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri.
Programmatically interpretable reinforcement learning. 35th International Conference on Ma-
chine Learning, ICML 2018, 11:8024-8033, 4 2018. URL https://arxiv.org/abs/
1804.02477v3.
Gert P. Westert, Stef Groenewoud, John E. Wennberg, Catherine Gerard, Phil Dasilva,
Femke Atsma, and David C. Goodman. Medical practice variation: Public report-
ing a first necessary step to spark change. International Journal for Quality in Health
Care, 30:731-735, 11 2018. ISSN 14643677. doi: 10.1093/intqhc/mzy092. URL
/pmc/articles/PMC6307331//pmc/articles/PMC6307331/?report=
abstracthttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6307331/.
Herman Yau, Chris Russell, and Simon Hadfield. What did you think would happen? explaining
agent behaviour through intended outcomes. Advances in Neural Information Processing Systems,
33:18375-18386, 2020. URL https://github.com/hmhyau/rl-intention.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. Proceedings of the Twenty-Third AAAI Conference on Artificial Intelli-
gence, 2008. URL www.aaai.org.
13
Published as a conference paper at ICLR 2022
Ariel Zylberberg, Jeannette AM Lorteije, Brian G Ouellette, Chris I De Zeeuw, Mariano
Sigman, and Pieter Roelfsema. Serial, parallel and hierarchical decision making in
primates.	eLife, 6, 6 2017. doi: 10.7554/ELIFE.17331. URL /pmc/articles/
PMC5484613//pmc/articles/PMC5484613/?report=abstracthttps:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC5484613/.
A	Additional Related Work
A. 1 Imitation learning
A.1.1 Behavioural cloning
Behavioural cloning (BC) is the simplest form imitation learning, as it reduces policy learning
to supervised learning of a direct mapping from states to actions, assuming a fully-observable
setting where S = Z (Bain & Sammut, 1996; Piot et al., 2014). For a trajectories dataset
D = {(z1:T, a1:T)}, the objective is to recover the deterministic policy mapping π : Z → A,
such that π(zt) is the action following observation zt :
∏ = arg min Ezt,at~D [L (∏(zt), at)]	(5)
π
where L(∙, y) is a supervised learning loss function for target value y.
This approach is typically compatible with offline learning, as it only requires access to demonstrated
trajectories of system states and actions (zt, at) at each timestep t. Its excellent learning efficiency
has motivated its use and deployment, including in safety-critical applications such as autonomous
driving (Farag, 2017).
With no explicit handling of system dynamics, however, this approach is highly susceptible to dis-
tributional shift (Ho & Ermon, 2016): at test time, the agent might enter states unseen in training
trajectories and thus drift away from its training domain and perform poorly. Likewise, BC is not
designed for non-Markovian or partially-observable environments, where the latest observation zt
is insufficient to determine action choice. Representation learning of POMDP belief states through
recurrent neural networks (Sun et al., 2017) can help tackle these issues at the cost of interpretability,
as it introduces black-box elements in the model.
A.1.2 State-action distribution matching
More recently, an alternative approach to dealing with covariate shift was proposed, based on match-
ing state-action distributions between the demonstrator and learned policies. Defining occupancy
measures μ∏ : S ×A → R for each policy π as μ∏(s, a) = π(a∣s) P∞=0 YtP(St = s∣π), this can
be achieved by minimising the Jensen-Shannon divergence in occupancy measures for each policy,
while maximising the entropy of the learned one H(π) through generative adversarial training (Ho
& Ermon, 2016; Jeon et al., 2018; Kostrikov et al., 2020):
arg min DJS (μ∏ ∣∣μ∏E)	and	arg max H(π)	(6)
ππ
In contrast to our descriptive goal, this approach therefore focuses on matching feature expectations
rather than explicitely recovering the demonstrated policy mapping from observations or belief states
to actions, based on the assumption that this behaviour is optimal - and thus aims to maximise
performance on a given task. In addition, it suffers from the same issue as BC-IL in providing
interpretable representations of belief states in partially-observable settings, despite Li et al. (2017)
improving explainability by clustering similar demonstration trajectories.
In all cases, online roll-outs of the learned policy are required to train a dynamics model of the
environment, as P(St = s∣∏) is needed to estimate occupancy, making this work incompatible
with purely observational clinical data. To overcome this, model-based imitation learning (MB-
IL) proposes to train autoregressive exogenous models of environment dynamics (Englert et al.,
2013) for offline feature expectation. Encouraging results were obtained for fully-observable robotic
applications, but these dynamics remain excessively simple for the healthcare context.
A comprehensive review of imitation learning methods can be found in Osa et al. (2018).
14
Published as a conference paper at ICLR 2022
A.2 Apprenticeship learning
Apprenticeship learning is another approach to modelling expert behaviour, concerned with match-
ing the performance of the demonstrating expert in terms of some unknown utility measure R. The
state-of-the-art solution to this task is inverse reinforcement learning (IRL), which involves recov-
ering this reward function from demonstration trajectories (Ng & Russell, 2000) - identifying the
expert’s hidden objectives. A forward reinforcement learning (RL) algorithm is subsequently ap-
plied to obtain the policy.
The inverse task of uncovering the reward function is fundamentally ill-posed, as the observed be-
haviour can be optimal under a multitude of reward signals. Additional constraints must thus be
specified to obtain a unique solution, for instance by maximising policy entropy (Ziebart et al.,
2008; Choi & Kim, 2011) as described above for distribution-matching imitation learning, or by
maximising the margin between induced policies in terms of feature expectations (Ng & Russell,
2000; Abbeel & Ng, 2004):
argminEs〜μ∏E [Vπ(S)- VπE (s)]	⑺
π
where Vπ(S) is a parametrisation of the value function for policy π, the expected sum of total dis-
counted rewards following this policy, assuming a start in state s: Vπ (S) = En [P∞=0 Y tR(st)∣so =
S].
IRL can deal with partially-observable environments (Choi & Kim, 2011) and even learn dynam-
ics models fully offline (Makino & Takeuchi, 2012). Still, in terms of providing insight into the
decision-making process, the indirect nature of the IRL ◦ RL framework results in uninterpretable
policies - despite attempts to parametrise reward functions for explainability (Bica et al., 2021).
Table 3: Comparison of related work in light of our key policy learning goals. DM-IL stands for
distribution-matching imitation learning. f stands for a representation learning step; bt is the agent’s belief
over the true state st .
Related work		Optimisation		Offline	Partial	Interpretable
		Objective		Learning	Observability	Policy
BC-IL (Bain & Sammut, 1996)		L(argmaxa π(a∣zt),ai)		✓	X	X
PO-BC-IL (Sun et al., 2017) Interpretable BC-IL (HUyUk et al., 2021)		∣L(argmaXa π(a∣bi),ai); bt:	= f (z1:t)	✓ ✓	✓ ✓	X ✓
DM-IL (Ho & Ermon, 2016)		DKL (π(at∣zt)kπE(at∣zt))		X	X	X
Interpretable DM-IL (Li et al., 2017)				X	X	✓
MB-IL (Englert et al., 2013)				✓	X	X
IRL (Ng & Russell, 2000)	π	=argmin∏ Ezt〜μ∏E [Vπ (Zt)-	V πE (zt)]	X	X	X
PO-IRL (Choi & Kim, 2011) Offline PO-IRL (Makino & Takeuchi, 2012)	)	π = argminn Ebt 〜μ∏E [V " (Bt) bt = f (z1:t)	- VπE (bt)];	X ✓	✓ ✓	X X
Subjective PO-IRL (Golub et al., 2013)				X	✓	X
Table 3 compares how related research addresses the key goals outlined for our work. Our policy
learning algorithm must be operable offline on observational data; must accommodate a partially-
observable environment to allow adaptive behaviour over time; and must result in an interpretable
representation of the policy. In comparison to HUyuk et al. (2021) which addresses all three goals,
we propose to implement a policy learning algorithm with fewer assumptions and restrictions on
the belief state and dynamics model structure, as well as a more interpretable and expressive policy
parametrisation.
B Soft Decision Tree Optimisation
In this section, additional details are provided on our decision tree structure and optimisation proce-
dure.
To improve optimisation, Frosst & Hinton (2017) encourage soft decision tree models to split data
evenly across tree inner nodes through an additional regularisation term Lsplit :
Lsplit = -λ X	2-dn [0.5 log αn + 0.5 log(1 - αn)]
n∈I nner
(8)
where
ExP n(x)Pnate(x)
-PxP n(χ)-
n
αn
(9)
and λ is a splitting penalty weight. This additional loss is essentially a cross-entropy between the
discrete binary distribution (0.5; 0.5) and the inner node’s data split (αn; 1 - αn); and decays with
15
Published as a conference paper at ICLR 2022
depth dn to allow uneven splits as the tree learns more complex hierarchies. This regularisation was
not found to significantly affect optimisation but was nevertheless included in our objective in light
of Frosst & Hinton (2017)’s favourable results.
The path probability Pn(x) of node n, conditioned on tree input x, is formally defined as follows:
P n(x)=	Y	Pmate(X)ln∈Sright(m) ∙ (1- Pmate(X))T “2^⑺	(⑼
m∈Spath (n)
where Spath (n) are the set of nodes on the path from the tree root to node n; Ix is the indicator
function (returns 1 ifx is true, 0 else); and Sright(m) are the set of nodes descending from m’s right
branch.
Non-categorical action types can also be modelled by passing leaf parameters θal through a hy-
perbolic tangent function, and replacing the cross-entropy in equation 1 with the mean-squared
error. Validation performance in Algorithm 1 can be determined with traditional supervised learn-
ing metrics, such as target prediction accuracy, as well as with alternative success measures such as
interpretability ratings or domain-expert approval.
A particular implementation challenge is to minimise the loss of information from previous opti-
misations during growth of the history tree, as new leaves are created (step 2a in Figure 9b). To
overcome this, split leaves are initialised to the previous value of the parent node, with added Gaus-
sian noise of variance σ2 = 1/d where d is the leaf depth. This facilitates local optimisation as the
tree grows, by ensuring new leaves still capture similar distributions to optimised parent nodes.
Table 4 contrasts mappings learned by our recurrent decision trees (RDTs) and RNNs for different
time-series tasks. Note that for action or evolution estimation beyond the input timestep, i.e. for
τ > 0 in this table, state-action distribution matching methods become important in training to
overcome compounding errors at test time. Our differentiable recurrent tree structures can also be
optimised by backpropagation through time as for RNNs.
B.1	Interpretable representation learning for time-series
Related work. The state-of-the-art in interpretable time-series modelling or representation learn-
ing is to highlight the relative importance of features in contributing to changes in model predictions.
This can be achieved with attention mechanisms as in Choi et al. (2016), or with Shapley explana-
tions (Lundberg et al., 2017). Overall, these methods remain tools to gain approximate insight into
complex models, rather than a transparent description of their inner workings. In addition, patient
observations and medical actions are all treated as independent variables as illustrated in Table 4 -
a significant assumption for policy learning, making models susceptible to confounding in obser-
vational data. In contrast, our work aims to distill decision-making pathways from observations to
actions in a clear format, comparable to the human thought process.
Recurrent decision trees This section demonstrates the gains achieved by our choice of history
representation learning model to tackle the challenges of partially-observable environments. In par-
ticular, Recurrent Decision Tree (RDT) models address our requirement for interpretable policies.
While the history embedding ht may not be interpretable, we experimentally find that the tree-based
model is more understandable than an RNN. Different structures shown in Figures 7b and 7c were
Table 4: Comparison of time-series methods with actions.
Framework	Recurrent Neural Network (RNN)	Recurrent Decision Tree (RDT)
Tasks & targets (τ ∈ N)	•	Policy learning (explored in this work): at+τ |z1:t, a1:t-1 •	Evolution prediction (decision support): zt+τ |z1:t-1, a1:t-1 •	Outcome prediction (decision support): yt+τ |z1:t-1, a1:t-1	
Mapping 	task 	recurrence	{ht; xt := {zt , at}} → ht+1 λ~λ target ʌʌ 一 lθ—匹—'"	{ht; zt} → {at; ht+1} (+ other target) —*O	, ,,
Typical model	ht+1 = σ(Wrht + Wxxt + bh)	at = altmax ; ht+1 = hltm+a1x ; lmax = arg maxl Pl (ht, zt)
Merits	• Modelling performance & flexibility	•	Interpretable model; ht modulates mapping from zt to target •	Disentanglement of ht and at via multiple outputs •	Joint recurrence and target prediction
16
Published as a conference paper at ICLR 2022
also studied, in which ht is first extracted from z1:t-1 through an RNN or RDT, and is concatenated
to latest observation zt as input to a distinct action-decision tree. Results in Table 5 highlight that our
RDT architecture performs as well as the RNN history extractor in action-matching performance,
and, as expected, is preferred by surveyed physicians for its interpretability. Performance remained
acceptable as history dimensionality was decreased to 1 (< 5% loss in accuracy), which is valuable
for visualisation purposes.
time delay
(a) DT policy.	(b) RNN+DT policy. (c) RDT+DT policy.	(d) RDT policy.
Figure 7: Decision tree policies with different history extraction models.
Table 5: Performance of different history extraction models for decision tree policies on ADNI. Unless
shown, standard errors were all ≤ 0.02. History-extraction and action-prediction trees have respective depths
dH and dA .
History extraction model	dH	dA	# Parameters	Interpretability	AUPRC	Brier
None (DT)	—	3.7 ± 0.4	36 ± 4	9.3 ± 0.3	0.72	0.25
RNN+DT	—	4.8 ± 0.2	253 ± 6	5.0 ± 0.5	0.83	0.17
RDT+DT	3.3 ± 0.4	2.5 ± 0.3	167 ± 12	6.3 ± 0.3	0.80	0.21
RDT	3.3 ± 0.7		122 ± 18	8.3 ± 0.3	0.82	0.18
The unexpected result that our decision trees perform history representation nearly as well as neural
networks can be attributed to the sparse and heterogeneous nature of our time-series data, often
better captured through models based on decision trees (Frosst & Hinton, 2017). It is however not
expected that RDTs would be applicable to complex, structured history embeddings as in speech or
language modelling.
17
Published as a conference paper at ICLR 2022
Leaf recurrence models An important step in model design consists of choosing leaf parameters
for history prediction. In increasing order of complexity in terms of numbers of parameters per
leaf, Table 6 highlight a number of possibilities, where {θh ∈ RM, θr ∈ RM, θr ∈ RM×M, θf ∈
RM×D} are trainable parameters for each leaf (for history and observation dimensionality M and D
respectively) and denotes element-wise multiplication. As for action prediction, the simplest solu-
tion is for leaves to be independent of inputs, outputting a fixed history vector - either a distribution
over categorical values with equation 11, or a vector of continuous values through equation 12. De-
pendence on previous history ht-1 and on the newest observation zt-1 can be incorporated through
element-wise or matrix multiplications (equations 13-17). In particular, equation 17 corresponds to
a simple RNN unit.
Table 6: Performance of different leaf models for history recurrence on ADNI. Standard error on tree depth
d is omitted for brevity.
Recurrence equation	d	# Parameters	Accuracy	Warm Start Acc.
ht+1 = softmax(θh)	(H)^^27	114±11	0.72 ± 0.02	-
ht+1 = tanh(θh)	(12)	3.0	130± 23	0.76 ± 0.03	-
ht+1 = tanh(θh + θr	ht)	(13)	2.7	154± 26	0.68 ± 0.01	-
ht+1 = tanh(θh + θfzt)	(14)	3.0	335 ± 46	0.76 ± 0.01	0.73 ± 0.01
ht+1 = tanh(θh + θr	ht + θfzt)	(15)	2.3	306 ± 19	0.75 ± 0.02	0.73 ± 0.01
ht+1 = tanh(θh + θrht)	(16)	3.3	522± 110	0.73 ± 0.02	0.77 ± 0.02
ht+1 = tanh(θh + θrht + θfzt)	(17)	3.7	847 ± 152	0.76 ± 0.02	0.77 ± 0.01
More complex leaf models tend perform best on ADNI, at the cost of tree readability and computa-
tion cost. Still, using fixed leaf history parameters through a hyperbolic tangent activation (equation
12) result in second-best performance with a minimal number of parameters, and intuitive readabil-
ity - justifying the use of this parsimonious model for all other investigations. Initialising parameters
with values previously optimised for the RNN+DT recurrence model, in a warm start, was found to
achieve similar performance, but did not systematically improve results. A caveat should be added,
as some decision-making environments require more complex recurrence models to capture sub-
tleties in patient history. For instance, policy optimisation for our proof-of-concept SYNTH dataset
was challenging with history-independent models, and recurrence equation 16 was used instead.
Overall, this architecture choice depends on the complexity of the modelling task and on the nature
of demonstrated trajectories.
B.2	Improvement on established decision tree models
We conclude this section with a comparison of our method with the traditional Classification And
Regression Tree (CART) and soft decision tree (SDT) algorithms in Table 7, to highlight our con-
tributions. In particular, our work combines soft and grown tree architectures (Frosst & Hinton,
2017; Tanno et al., 2019) and pioneers their application to time-series data instead of images. We
formalise and study the conversion of multidimensional gating functions to axis-aligned ones for in-
terpretability in Section C. Our final contribution is an entirely novel approach to model time-series
through a multi-output recurrent tree structure, extending the representation learning model of Ding
et al. (2021) to the sequential setting and improving its interpretability by marginalising out obscure
latent variables.
Table 7: Comparison of our proposed architecture with traditional, soft and cascaded decision trees
(SDT) as in Breiman et al. (1984), Frosst & Hinton (2017) and Ding et al. (2021).
	CART	SDT	CDT	Poetree
MODELLING TASKS				
Discrete, categorical, continuous outputs	✓	✓	✓	✓
Multiple outputs (e.g., {at, ht, 2±})	X	X	X	✓
Multidimensional decision boundaries	X	✓	✓	✓
Interpretable decision boundaries	✓	X	X	✓
Time-dependence handling via recurrence	X	X	X	✓
OPTIMISATION				
Optimisation objective	Gini impurity	Prediction error	Prediction error	Prediction error
Probabilistic decision boundaries	X	✓	✓	✓
Gradient-descent optimisation	X	✓	✓	✓
Tree depth growth & path pruning	X	X	X	✓
18
Published as a conference paper at ICLR 2022
While gradient-boosted decision tree structures have been reported to obtain good performance on
time-series forecasting tasks (Hyland et al., 2020), such ensemble methods result in multiple tree
structures to navigate which negatively affects interpretability (Lage et al., 2018). In addition, man-
ual pre-processing of the time-series data is required to include history information, which our model
avoids by independently learning history embeddings and marginalising them at interpretation time.
C	Axis-aligned decision tree models
For easier human interpretation, our decision tree policies should be represented with deterministic,
axis-aligned thresholds at each inner node. We propose a post-processing method to achieve this
from multidimensional trees, as well as a variant of our proposed architecture to directly learn axis-
aligned decision boundaries during training, and optimise a policy closer to the structure used at
inference time.
C.1 Multidimensional tree post-processing
A final technical consideration required by the model architecture proposed in Section 3.1 is policy
post-processing for human interpretation. After optimisation, our decision trees consist of multidi-
mensional gating functions over the history and observations variables which are difficult to under-
stand - as illustrated in Figure 8a, and must be converted to axis-aligned, unidimensional thresholds.
Each inner node thus consists of learned parameters {b, w} and computes the probability of taking
the right-most branch as follows:
MD
b + Ewi ∙ ht,i + E Wm+i ∙ zt,i	(18)
where M and D are history and observation space dimensionalities. Superscripts denoting node
indices are omitted for clarity.
The history vector is first marginalised out into the bias term, to condition the resulting observation-
to-action policy on the history as desired. This results in a specific tree model for each timepoint,
which only depends on observation zt for easier human interpretation. Gating functions are re-
expressed as follows:
Pgate(Zt) = σ (b/ + X : wm+i ∙ zt,i^	(19)
where b0 = b + PM=I wi ∙ ht,i is now a history-dependent bias term.
As in Silva et al. (2020), these time-dependent multidimensional policies are then converted into
axis-aligned unidimensional trees by taking the largest component of the remaining gating parame-
ters at each node, wmax :
Pgate(Zt) > 0.5	比 b + wmax ∙ zt,max > 0, where wmax = max{wm+i}i=1	(20)
=⇒ zt,max > -b0/wm
ax	(21)
(or	Zt,max < -b0/wm
ax if wmax <0)	(22)
Figures 8b and 8c are examples of axis-aligned policies at different timesteps for typical patient B
(with degrading MCI symptoms), much easier to follow than the multidimensional tree in Figure 8a.
Table 8: Performance of multidimensional and different axis-aligned decision tree policy structures. Stan-
dard errors all ≤ 1%.
Adaptive decision tree policy	ADNI Accuracy (%)		MIMIC Accuracy (%)
Multidimensional		77.6	79.9
Axis-aligned; bias b0		74.7	75.9
Axis-aligned; bias b0 + Pi6=max wi	• zt,i	76.9	77.8
Axis-aligned; bias b0 + Pi6=max wi	• zt,i	75.4	76.0
Axis-aligned thresholds were also improved by exploiting our framework to predict expected obser-
vations at the subsequent timestep Zt+ι. Thresholds were adjusted by incorporating the predictions
19
Published as a conference paper at ICLR 2022
(a) Multidimensional policy.
Vh low
Go right	Vh not low
I Vh low I I CDR-SB normal ∣ ∣ No previous MRI ∣	∣ CDR-SB severe
MRI MRI MRI X	MRI X CDR-SB normal ∣ ×
MRI MRI
(b) Axis-aligned policy, t = 1.
Vh low
CDR-SB normal No previous MRI I
MRI]	[mri ×
CDR-SB severe
MRI X
(d)	Axis-aligned policy after pruning, t = 1.
(e)	Axis-aligned policy after pruning, t = 4.
Figure 8: Transformation of multidimensional tree policy into axis-aligned and pruned structure. History
and expected evolution outputs are not represented for clarity.
20
Published as a conference paper at ICLR 2022
for all components except <⅞,maχ in the bias term, transforming equation 21 into:
b0 + Pi=max wi ∙ ZM
Ztmax >
wmax
(23)
As shown in Table 8, incorporating true and expected observation values in the thresholds improved
the axis-aligned model performance. Naturally, true test observations for the corresponding timestep
are unavailable, and can therefore not be used to adjust the threshold; only expected observations
can be used for this. This investigation highlights another benefit of the expected patient evolution
model, in addition to explanatory insights.
Further post-processing of axis-aligned policies in Figures 8b and 8c is required in the form of node
pruning, setting a minimum path probability threshold on the validation set, to address the following
concerns:
•	Axis-aligned thresholds are often sensible and typically in the [0, 1] range for categorical
variables, converting straightforwardly to meaningful decisions. Negative or out-of-scope
thresholds, however, exclude all data from a child path (e.g., left branch of the ”Go right”
node in Figure 8b), which can be pruned from the tree.
•	Learning interdependencies between variables within the hierarchical structure is also chal-
lenging. For example, in Figure 8b, the first right branch (low Vh on previous scan) is
incompatible with the following right branch (not low Vh). In practice, we obtained this
structure due to (1) soft partitions, with examples shared across incompatible branches;
and as (2) multi-dimensional gating affects thresholds in training. Again, the solution is to
eliminate non-sensical children nodes in post-processing.
Accuracy loss was found to be virtually null (< 2%) with a pruning probability threshold of 0.05 on
the validation set Dval. Formally, average path probability for node l was estimated as:
P l(Dval)
1
|Dval|
Pl(ht,zt)
{ht ,zt } ∈ Dval
(24)
and nodes with P l(Dval) < 0.05 were eliminated from the structure. Indistinguishable performance
of the multidimensional and axis-aligned tree mode was even obtained on ADNI by training the
multidimensional structure with an increasing L1 regularisation weight, to allow greater flexibility
at initial stages and induce a more axis-aligned structure at the end of training. With an L1 weight
from 10-5 to 10-1, this resulted in an action-matching accuracy of 0.75 ± 0.01 and AUROC of 0.52
± 0.02 with both multidimensional and post-processed models.
Overall, axis-aligned trees therefore offered excellent performance, comparable to multidimensional
ones. As the observation space dimensionality d increases, however, approximations become less
accurate. To overcome this, trees can be trained closer to their axis-aligned version during opti-
misation, for instance with L1 regularisation to induce sparsity in the gating functions. On a 15-
dimensional observation space with the SYNTH simulated policy, an L1-regularisation weight of
0.01 on gating parameters decreased multidimensional action-matching accuracy by 0.7% overall,
but improved axis-aligned performance by 5%. This suggests a promising avenue to ensure the
scalability of our model.
C.2 Axis-aligned tree training
In this section, we propose a variant to our optimisation algorithm in order to directly train a structure
that is closer to the one used in inference time.
The following gating function was designed to allow for axis-aligned training while keeping the tree
structure differentiable:
d
pgate(x) =	σ (xiwi + bi)	(25)
i=1
for an input x ∈ Rd . This function defines a soft AND gate over d axis-aligned thresholds, where
a threshold value -bi/wi is learned for each input dimension i. At inference time, each inner node
consists of d axis-aligned thresholds, one for each input dimension, which must all be satisfied to
proceed to the right child branch.
21
Published as a conference paper at ICLR 2022
In a recurrent architecture, multidimensional gating functions on the history embedding ht can be
kept for greater model flexibility, since these are marginalised at interpretation time. Axis-aligned
gating functions can be applied to the observation values zt only, such that:
d
pgate (ht , zt) = σ (hTw0 + b0) × "σ (zt,iWi + bi)	(26)
where σ is the sigmoid function and {w0, b0, {wi, bi}id=1} are leaf parameters.
Action-matching performance. Overall, as shown in Table 9, slightly superior performance was
still obtained from training and post-processing multidimensional soft architectures in comparison to
axis-aligned ones. This greater performance may be explained by greater flexibility during training
time, particularly as tree structures are grown from low-depth trees with poor discrimination if only
considering unidimensional partitions.
Table 9: Performance of multidimensional and axis-aligned decision tree policy structures on ADNI. The
static setting corresponds to learning a mapping zt to at .
Tree structure	Static setting		Recurrent setting	
	Accuracy	AUROC	Accuracy	AUROC
Multidimensional	0.75 ± 0.01	0.53 ± 0.01	0.78 ± 0.02	0.62 ± 0.01
Axis-aligned (post-processed from multi.)	0.73 ± 0.01	0.52 ± 0.02	0.76 ± 0.02	0.59 ± 0.01
Trained axis-aligned (SDT)	0.72 ± 0.02	0.52 ± 0.02	0.75 ± 0.02	0.56 ± 0.02
Trained axis-aligned (CART)	0.69 ± 0.01	0.50 ± 0.04	-	-
Better performance over a deterministic tree structure (CART) was also obtained. On our relatively
small datasets of a few thousand patients, sharing information from each patient across every node
through probabilistic gating functions ensures no part of the tree is trained on excessively few exam-
Ples - this may help combat overfitting, which decision trees are often subject to (Frosst & Hinton,
2017). An additional source of difference in performance at test time is that our differentiable struc-
ture is trained by gradient descent, while traditional trees are built by Gini impurity splitting.
D	Algorithm implementation details
Our Poetree algorithm was implemented as follows for our experimental investigations3. Models
were built using the open-source automatic differentiation framework JAX4.
All observation inputs were normalised prior to modelling. Hyperbolic tangent outputs for zt+ι
and axis-aligned thresholds were mapped back to the observation space after training for human-
readability and interpretation. History representation learning was integrated in the decision, through
the recurrent decision tree structure described in Section 3.2. Leaf models for history prediction were
independent of previous history: ht+1 = tanh(b), where b ∈ Rm is a trainable parameter for each
leaf and m is the history dimension.
Following Algorithm 1, tree structures were initialised from depth 2, to avoid excessively simple
local optima. For tree growth, limited to depth 5, the AUROC score was used as a measure of
validation performance, to determine whether to split leaves into new inner nodes. All global and
local optimisations were carried out until convergence - no improvements on the validation set for
50 update iterations. We used the Adam optimiser (Kingma & Ba, 2015) with a step size of 0.001
and a batch size of 32. Model hyperparameters in Table 10 were optimised through grid search on
validation datasets - random subsets of 10% of the training data.
All experiments were run with 5 different model initialisations and data splits and the average results
and standard errors were reported. Optimisation of tree structures was found to be temperamental,
with some initialisations not improving in performance with training. A restarting procedure was
therefore implemented if loss did not decrease by more than 5% over the first five epochs; and was
also applied to benchmark algorithms if necessary. Experiments were performed on a Microsoft
Azure virtual machine with 6 cores and powered by a Tesla K80 GPU.
3Code is made available at https://github.com/alizeepace/poetree and https://
github.com/vanderschaarlab/mlforhealthlabpub.
4https://github.com/google/jax
22
Published as a conference paper at ICLR 2022
Table 10: Hyperparameter grid for Poetree optimisation, with values optimised for the ADNI dataset.
Hyperparameter	Search range	Optimal values
History dimension m	1-20	8
Splitting penalty λ	10-4 - 101	10-1
Evolution prediction δ1	10-3 - 10-1	10-2
Evolution prediction δ2	10-4 - 10-1	10-3
Ablation Study The results of a brief study of the impact of different loss terms is provided in
Table 11. The choice for the loss of the tree structure itself L was first studied, comparing the
cross-entropy between targets and each leaf's output distribution ^l, weighted by their respective
path probability Pl (z), the cross-entropy with the weighted average of all leaf outputs (L0), and
the cross-entropy with the output of the maximum-probability leaf lmax (L00). The first objective
function, proposed by Frosst & Hinton (2017), returned marginally better results, as it may better
capture the different contribution from each element of the structure.
Table 11: Performance of decision tree policies optimised with different objective functions on ADNI.
CE(∙,a) is to the categorical cross-entropy loss with respect to target a.
Objective function		Action-matching accuracy	Relative MSE on Zt+1 (%)
L = PlPl(Z)∙CE(^l,a) L = CE(Pl Pl(z)∙ al,α) L00 = CE(almax ,a) where 1m&X 二	maxl Pl (Z)	0.78 ± 0.01 0.76 ± 0.01 0.77 ± 0.01	60 ± 10 - -
L + MSE(Zt+1) L + Lz L + LZ + Lsplit		0.75 ± 0.02 0.77 ± 0.02 0.77 ± 0.01	7±4 16±6 13±4
The performance of our model was also evaluated under the addition of the expected evolu-
tion regulariser Lz, itself composed of a mean-squared error term on the predicted observation,
MSE(Zt+ι) = δι ∣∣zt+ι - Zt+ι∣∣2, and a term penalising inconsistent action choices based on this
prediction, δ2 DKL (∏(∙∣ht+ι, zt+1k∏(∙∣ht+1, ≡t+ι)). Results suggest that the additional evolution
loss term Lz allows to balance consistency to the policy (by restoring the action-matching perfor-
mance degraded with only the MSE term) and fidelity to the observation evolution. Finally, the
splitting regularisation term Lsplit improved consistency between runs but did not largely improve
performance.
Complexity Analysis This section highlights the benefit of our tree growth procedure for both
interpretability, complexity and performance optimisation. Figure 9a compares results obtained
from trees of fixed depth and trees grown during training. Incrementally grown trees outperformed
all fixed structures with a minimal number of parameters (with, on average, 40% fewer parameters
than a fixed tree of the same depth) and reasonable training time (faster than fixed trees of depth
4 or 5, despite multiple local optimisations). Trees grown during the training process thus learn to
capture general hierarchies in the data while eliminating unnecessary partitions, which otherwise
make the tree more difficult to read as depth increases (Lage et al., 2018).
A comparison of performance against tree depth after optimisation is also proposed in Figure 9b,
demonstrating that our incrementally grown structure adapts its complexity to the the task at hand.
Indeed, while performance on the training set continuously increases with tree depth, test accuracy
is optimal at intermediate model complexities which avoid overfitting while capturing subtleties in
the data. With an average tree depth of 3.3 ± 0.7, our optimisation procedure therefore success-
fully implements this Occam’s razor. Tree depth was also found to adapt to sample complexity, as
optimised depth decreased to 2.4 ± 0.3 when input dataset size was halved.
Table 12: Complexity analysis of different policy learning algorithms on ADNI. Runtime is measured as
computation time for the prediction of all test actions (10% of demonstrations trajectories).
Model	Optimisation time (s)	Runtime (s)
Poetree	205 ± 32	<1
Poetree with inductive bias (Figure 12)	141 ± 24	<1
INTERPOLE (HUyUk et al., 2021)	752 ± 63	26 ± 4
23
Published as a conference paper at ICLR 2022
Average optimized depth
Grown tree
086420864
877777666
(兴)AOBInOOV
Fixed tree depth
X4≡qg ① JdJsUl
086420864
877777666
(％) AOEJnOOV
Test
Training
2	3	4	5
Grown tree depth
(a) Fixed trees	(b) Grown trees
Figure 9: Performance on ADNI as a function of tree depth, for fixed and grown trees. Our growth
optimisation procedure adapts model complexity to the task at hand to (a) facilitate interpretation and (b) avoid
overfitting.
Finally, Table 12 highlights the superior efficiency of our algorithm in training and runtime over its
closest related work, an interpretable PO-BC-IL model. This can be understood from the expensive
computation of Input-Output Hidden Markov Model (IOHMM) parameters to represent the POMDP
environment in the latter (Bengio & Frasconi, 1995). Model optimisation was accelerated when
initialising the tree structure with a sensible clinical policy as detailed in Section F.
E Benchmarks implementation details
All learned POMDP environment models are implemented as an Input-Output Hidden Markov
Model (IOHMM) (Bengio & Frasconi, 1995) and initialised uniformly at random. Benchmark im-
Plementation is similar to Huyuk et al. (2021) as this work shares our goal of interpretable policy
learning. Online-learning algorithms such as explainable DM-IL (Li et al., 2017) could not be used
for benchmarking, as our medical datasets do not allow interaction with the decision-making envi-
ronment.
PO-BC-IL The recurrent neural network is implemented as an LSTM unit (Hochreiter & Schmid-
huber, 1997) and a fully-connected layer, both of size 64; and trained through optimisation of the
categorical cross-entropy over actions using the Adam optimiser (Kingma & Ba, 2015) with learning
rate 0.001 and batch size 32 until convergence.
PO-IRL IOHMM parameters are first trained (Bengio & Frasconi, 1995) and freezed. The reward
function is initialised as R(s, a)〜N(0,0.0012) and is estimated as the average of 50 Markov Chain
Monte Carlo (MCMC) samples, retaining every tenth drawn value after burning-in 500 samples. In
Offline PO-IRL, IOHMM parameters and reward signal are jointly learned by MCMC sampling.
Q-values are then recovered with an off-the-shelf POMDP solver5.
PO-MB-IL After training and freezing IOHMM parameters, policies are parametrised in terms of
decision boundaries as in Huyuk et al. (2021), and initialised randomly. Training is carried out by
maximising the likelihood of actions in the demonstrations dataset by Expectation-Maximisation.
Maximisation is carried out with the Adam optimizer (Kingma & Ba, 2015) with learning rate 0.001
until convergence.
Interpretability benchmarks Our fully-observable behavioural cloning benchmarks include lo-
gistic regression6 and a static version of Poetree illustrated in Figure 2a. Feature importance
analysis extracts each input dimension’s contribution to the predictions of the PO-BC-IL model
through Shapley values (Lundberg et al., 2017).
5https://www.pomdp.org/code/index.html.
6 https://scikit- learn.org/
24
Published as a conference paper at ICLR 2022
F	Dataset details
F.1 Proof-of-concept policy
For the SYNTH dataset, treatment efficacy on diseased patients was set to 40%, with no spontaneous
state changes and an initial 80% diseased population. Diagnostic tests were given a 99% precision
and 5% false alarms.
F.2 The Alzheimer’ s Disease Neuroimaging Initiative (ADNI)
Rare visits without a CDR-SB measurement were discarded, as well as visits separated by more
than 6 months from the previous one - leaving 1,626 patients with at least two visits and a median
of three visits. Average MRI hippocampal volumes were taken as within half a standard deviation
from the population mean.
Published guidelines for Alzheimer’s disease investigation are reproduced in Figure 11. With the
help of one of the surveyed physicians, this was distilled as a static tree policy in Figure 12 to
initialise Poetree and assess the effect of inductive bias on optimisation performance. Gaussian
noise of variance σ2 = 0.1 was added to initialisation parameters. Table 12 evidences the complexity
gains afforded by this incorporation of prior knowledge, with action-matching performance reaching
0.62 ± 0.01 AUROC, 0.84 ± 0.01 AUPRC, and 0.18 ± 0.01 Brier score.
F.3 Medical Information Mart for Intensive Care (MIMIC-III)
Patients trajectories from the MIMIC dataset with missing values in one of the observation fea-
tures, or with non-consecutive measurements, were eliminated. In total, 4,222 ICU patients were
considered over up to 6 timesteps.
Policies recovered for the MIMIC antibiotic prediction task by our algorithm and the decision-
boundary framework (Huyuk et al., 2021) are given in Figure 13. The latter again falls short of
highlighting how patients are mapped onto the belief state, giving our decision tree greater expres-
sive power. Our results are consistent with policy learning work on the same task and dataset (Bica
et al., 2021), which identifies similar discriminative features: fever and abnormally high or low
white blood cell counts are known clinical criteria for infection and antibiotic treatment, if a bacte-
rial source is suspected (Masterton et al., 2008). Continuous variable thresholds evolve over time
through their dependence on history: the temperature criterion, for instance, adjusts to whether the
patient has a prior for infection.
G	Discussion with clinicians
To evaluate the interpretability of our decision tree policies, we consulted five practising physicians
for their feedback on our work in comparison to the state-of-the-art. Each physician was given
a short presentation introducing (1) our goal of representing the decision-making behaviour in an
intelligible way, (2) the ADNI dataset and the studied clinical environment, (3) how policies can
be represented in terms of decision boundaries over belief simplices (HUyUk et al., 2021), black-
box mappings from beliefs to actions (Sun et al., 2017), and direct mappings from observations to
actions (as in our work). All participants were unaware of what method we proposed, to minimise
any implicit bias in results.
Ethics Statement Our survey does not require formal ethics oversight as the identity of our clin-
icians cannot be obtained from published information (see U.S. Department of Health and Human
Services 45 CFR 46.104(d)(2)(i)), and as no sensitive or private data was shared with nor obtained
from participants. We followed the same procedure as in HUyUk et al. (2021) to ensure a fair com-
parison to this related work.
All five clinicians expressed the following preference in terms of interpretability of the decision-
making process:
RNN < Decision Boundaries < Decision Trees
where the RNN policies, learned by PO-BC-IL (Sun et al., 2017), correspond to black-box mappings
from observation to history, and from history to actions. This was illustrated in our survey through
Table 13. Decision-boundary policies correspond to the interpole benchmark in Figure 3b (HUyUk
et al., 2021). Finally, decision tree policies learned through our Poetree framework were illustrated
by Figure 3a.
25
Published as a conference paper at ICLR 2022
(a) Patient A: Healthy
(b) Patient B: Degrading
e + ι
(c) Patient C: Dementia
Figure 10: Adaptive decision tree policy for three typical ADNI patients, with each path highlighted in blue.
Figure 11: Published clinical diagnostic policy for patient with MCI symptoms. Figure reproduced from
Biasutti et al. (2012).
26
Published as a conference paper at ICLR 2022
MRI X
Figure 12: Policy initialisation for inductive bias analysis, based on Figure 11.
Healthy
Infected
(a) Decision-boundary policy
(Huyuk et al., 2021).
(b) Adaptive decision tree policy
(only 2 timesteps visualised).
Figure 13: Recovered policies for MIMIC. Units for white blood cell count (WBC) are 1000/mm3.
27
Published as a conference paper at ICLR 2022
Although more participants would be required for greater reliability, this survey was useful to con-
firm end-user’s preference for our representation of behaviour through trees, and to validate the pri-
orities of our work. Additional comments on the belief space and decision-boundary policy (Huyuk
et al., 2021) are summarised below:
•	Classifying patients within discrete disease states aligns with the cognitive process followed
in clinical practice (O’Bryant et al., 2008), but is challenging to achieve in practice, as
patients often fall on a continuum of symptoms and illness severity.
•	Visual representations of belief states can be misleading. In particular, the triangular rep-
resentation in Huyuk et al. (2021) does not evidence the linear progression from normal to
MCI to dementia, considered the traditional patient evolution (NICE, 2018). In addition,
higher-dimensional, hierarchical or overlapping belief states cannot be readily visualised.
•	The mapping from patient observations Z to belief space S, and the evolution of patient
trajectories in this space as new information is acquired, is unclear.
Feedback on our decision tree policies was also collected, highlighting important advantages and
limitations:
•	The flow chart captured by the decision tree was described by clinicians as most similar
to their own approach to care, hierarchically eliminating possible diagnoses, treatments or
investigations - in agreement with cognitive research (Zylberberg et al., 2017) and estab-
lished guidelines illustrated in Figure 11.
•	The concept of action value (for diagnostic tests, value of information; for treatments,
patient improvement) is captured in an easily understandable way in terms of expected
patient evolution. In contrast, they noted, reward functions and belief updates proposed by
related work (Makino & Takeuchi, 2012; Huyuk et al., 2021) do not represent the medical
thought-process as faithfully.
•	Although the recurrent decision tree mapping from leaves to a subsequent tree policy was
appreciated, there remain limitations in terms of the interpretability of the history embed-
ding, particularly with complex leaf recurrence models. In further work, we could visu-
alise history embeddings over a belief simplex following Huyuk et al. (2021), conditioning
vertices to encode distinct beliefs about the patient condition (e.g., diagnoses). Different
regions of belief space would thus be associated with different tree policies, rather than a
single most-likely action.
28
Published as a conference paper at ICLR 2022
Table 13: Patient trajectories under a policy learned by PO-BC-IL. History embeddings are obtained
through a recurrent neural network.
Timestep	Observation zt		History ht	Action at
	CDR-SB P	revious MRI		
t=1	Normal	Average Vh	[+0.5, -0.5, +0.5, +1.0, +0.2, -0.1, +0.9, -0.9]	X
t=2	Normal	X	[-0.4, +1.0, -0.7, +0.9, +0.8, +1.0, -0.9, +0.6]	X
t=3	Normal	X	[-0.2, +0.1, -0.1, +0.6, +0.9, +0.8, +0.6, -0.4]	X
t=4	Normal	X	[-0.2, +0.8, -0.6, +0.3, +0.8, +1.0, +0.0, +0.4]	X
(a) Patient A (Healthy)				
Timestep	Observation zt		History ht	Action at
	CDR-SB	Previous MRI		
t=1	Questionable	Low Vh	[+0.8, +0.3, +0.3, +0.2, +0.1, +0.7, -0.5, -0.7]	MRI
t=2	Questionable	Low Vh	[+0.9, -0.1, -0.2, +0.6, -0.2, +0.1, -0.5, -0.8]	MRI
t=3	Questionable	Low Vh	[+0.9, +0.6, -0.4, +0.9, -0.1, +0.6, -0.7, -0.7]	MRI
t=4	Severe	Low Vh	[+0.8, +0.2, -0.9, +0.9, -0.5, -0.6, -0.8, -0.8]	X
(b) Patient B (Degrading)				
Timestep	Observation zt		History ht	Action at
	CDR-SB_Previous MRI			
t=1	Severe	Low Vh	[-0.5, +0.9, +0.2, +0.5, +0.3, +0.5, -0.9, -0.9]	X
t=2	Severe	X	[-0.7, -0.5, +0.8, -0.4, +0.3, -0.1, -0.5, +0.7]	X
t=3	Severe	X	[-0.8, +0.7, +0.9, -0.8, +0.6, -0.1, -0.9, +0.9]	X
t=4	Severe	X	[-0.8, +0.4, +1.0, -0.9, +0.7, -0.4, -0.9, +0.9]	X
(c) Patient C (Dementia)
29