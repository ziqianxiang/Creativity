Published as a conference paper at ICLR 2022
Controlling directions orthogonal
TO A CLASSIFIER
Yilun Xu, Hao He, Tianxiao Shen, Tommi Jaakkola
Computer Science and Artificial Intelligence Lab,
Massachusetts Institute of Technology
{ylxu, haohe, tianxiao}@mit.edu; tommi@csail.mit.edu
Ab stract
We propose to identify directions invariant to a given classifier so that these direc-
tions can be controlled in tasks such as style transfer. While orthogonal decom-
position is directly identifiable when the given classifier is linear, we formally de-
fine a notion of orthogonality in the non-linear case. We also provide a surpris-
ingly simple method for constructing the orthogonal classifier (a classifier utiliz-
ing directions other than those of the given classifier). Empirically, we present
three use cases where controlling orthogonal variation is important: style transfer,
domain adaptation, and fairness. The orthogonal classifier enables desired style
transfer when domains vary in multiple aspects, improves domain adaptation with
label shifts and mitigates the unfairness as a predictor. The code is available at
https://github.com/Newbeeer/orthogonal_classifier.
1 Introduction
Many machine learning applications require explicit control of directions that are orthogonal to a
predefined one. For example, to ensure fairness, we can learn a classifier that is orthogonal to sensitive
attributes such as gender or race (Zemel et al., 2013; Madras et al., 2018). Similar, if we transfer images
from one style to another, content other than style should remain untouched. Therefore images before
and after transfer should align in directions orthogonal to style. Common to these problems is the task
of finding an orthogonal classifier. Given any principal classifier operating on the basis of principal
variables, our goal is to find a classifier, termed orthogonal classifier, that predicts the label on the basis
of orthogonal variables, defined formally later.
The notion of orthogonality is clear in the linear case. Consider a joint distribution PXY over X ∈ Rd and
binary label Y . Suppose the label distribution is Bernoulli, i.e., PY = B(Y ; 0.5) and class-conditional
distributions are Gaussian, Pχ∣γ=y = N(X; μy,σjl), where the means and variances depend on the
label. If the principal classifier is linear, wι = Pr(Y = 1∣θ[χ), any classifier w2, in the set W2 = {Pr(Y =
1∣θjχ) ∣ θ^[θ2 = 0}, is considered orthogonal to wι. Thus the two classifiers w1,w2, with orthogonal
decision boundaries (Fig. 1) focus on distinct but complementary attributes for predicting the same label.
Finding the orthogonal classifier is no longer straightforward in the non-linear case. To rigorously define
what we mean by the orthogonal classifier, we first introduce the notion of mutually orthogonal random
variables that correspond to (conditinally) independent latent variables mapped to observations through a
diffeomorphism (or bijection if discrete). Each r.v. is predictive of the label but represents complementary
information. Indeed, we show that the orthogonal random variable maximizes the conditional mutual
information with the label given the principal counterpart, subject to an independence constraint that
ensures complementarity.
Our search for the orthogonal classifier can be framed as follows: given a principal classifier w1 using
some unknown principal r.v. for prediction, how do we find its orthogonal classifier w2 relying solely on
orthogonal random variables? The solution to this problem, which we call classifier orthogonalization,
turns out to be surprisingly simple. In addition to the principal classifier, we assume access to a full
classifier wx that predicts the same label based on all the available information, implicitly relying on
both principal and orthogonal latent variables. The full classifier can be trained normally, absent of
constraints1. We can then effectively “subtract” the contribution of w1 from the full classifier to obtain
the orthogonal classifier w2 which we denote as w2 = wx ∖ w1 . The advantage of this construction is that
1The full classifier may fail to depend on all the features, e.g., due to simplicity bias (Shah et al., 2020).
1
Published as a conference paper at ICLR 2022
O
5
5-
4
3
2
O
(a) Distribution pX∣Y
5-
(b)	wx(x)
(c)	w1(x)
0	1	2	3	4	5
Xl
(d)	w2(x)
0	1	2	3	4	5
4
4
3
3
2
2
1
O
O 1
2
3	4	5
O
Figure 1: An illustrative example of orthogonal classifier in a linear case. (a) is the data distributions in
two classes; (b,c,d) are the probabilities of the data from class 1 predicted by the full/principal/orthogonal
classifiers. Red and blue colors mean a probability close to 1 or 0. The white color indicates regions with
a probability close to 0.5, which are classifiers’ decision boundaries. Clearly, w1 and w2 have orthogonal
decision boundaries.
we do not need to explicitly identify the underlying orthogonal variables. It suffices to operate only on
the level of classifier predictions.
We provide several use cases for the orthogonal classifier, either as a predictor or as a discriminator. As a
predictor, the orthogonal classifier predictions are invariant to the principal sensitive r.v., thus ensuring
fairness. As a discriminator, the orthogonal classifier enforces a partial alignment of distributions,
allowing changes in the principal direction. We demonstrate the value of such discriminators in 1)
controlled style transfer where the source and target domains differ in multiple aspects, but we only wish
to align domain A’s style to domain B, leaving other aspects intact; 2) domain adaptation with label
shift where we align feature distributions between the source and target domains, allowing shifts in label
proportions. Our results show that the simple method is on par with the state-of-the-art methods in each
task.
2	Notations and Definition
Symbols. We use the uppercase to denote random variable (e.g., data X, label Y ), the lowercase to
denote the corresponding samples and the calligraphic letter to denote the sample spaces of r.v., e.g., data
sample space X. We focus on the setting where label space Y is discrete, i.e., Y = {1,…，C}, and denote
the C - 1 dimensional probability simplex as ∆c. A classifier W : X → ∆c is a mapping from sample
space to the simplex. Its y-th dimension w(x)y denotes the predicted probability of label y for sample x.
Distributions. For random variables A, B, we use the notation pA, pA∣B , pAB to denote the
marginal/conditional/joint distribution, i.e.,pA(a) = p(A = a), pA∣B (a∣b) = p(A = a∣B = b),pAB (a, b) =
p(A = a, B = b). Sometimes, for simplicity, we may ignore the subscript if there is no ambiguity, e.g.,
p(a∣b) is an abbreviation forpA∣B(a∣b).
We begin by defining the notion of an orthogonal random variable. We consider continuous X, Z1, Z2
and assume their supports are manifolds diffeomorphic to the Euclidean space. The probability density
functions (PDF) are in C1. Given a joint distribution pXY , we define the orthogonal random variable as
follows:
Definition 1 (Orthogonal random variables). We say Z1 and Z2 are orthogonal random variables w.r.t
pXY if they satisfy the following properties:
(i)	There exists a diffeomorphism f : Z1 × Z2 → X such that f(Z1, Z2) = X.
(ii)	Z1 and Z2 are statistically independent given Y, i.e., Z1 ⊥ Z2∣Y .
The orthogonality relation is symmetric by definition. Note that the orthogonal pair perfectly reconstructs
the observations via the diffeomorphism f ; as random variables they are also sampled independently
from class conditional distributions p(Z1∣Y ) and p(Z2∣Y ). For example, we can regard foreground
objects and background scenes in natural images as being mutually orthogonal random variables.
Remark. The definition of orthogonality can be similarly developed for discrete variables and discrete-
continuous mixtures. For discrete variables, for example, we can replace the requirement of diffeomor-
phism with bijection.
2
Published as a conference paper at ICLR 2022
Since the diffeomorphism f is invertible, We can use zι ： X → Zi and z2 : X → Z2 to denote the two
parts of the inverse mapping so that Z1 = z1 (X) and Z2 = z2(X). Note that, for a given joint distribution
pXY , the decomposition into orthogonal random variables is not unique. There are multiple pairs of
random variables that represent valid mutually orthogonal latents of the data. We can further justify our
definition of orthogonality from an information theoretic perspective by showing that the choice of z2
attains the maximum of the following constrained optimization problem.
Proposition 1. Suppose the orthogonal r.v. of z1(X) w.r.tpXY exists and is denoted as z2(X). Then
z(X) = z2(X) is a maximizer of I(z(X); Y ∣z1 (X)) subject to I(z(X); z1 (X)∣Y ) = 0.
We defer the proof to Appendix B.1. Proposition 1 shows that the orthogonal random variable maxi-
mizes the additional information about the label we can obtain from X while remaining conditionally
independent of the principal random variable. This ensures complementary in predicting the label.
3	Constructing the Orthogonal Classifier
Let Z1 = z1(X) and Z2 = z2(X) be mutually orthogonal random variables w.r.t pXY . We call Z1 the
principal variable and Z2 the orthogonal variable. In this section, we describe how we can construct the
Bayes optimal classifier operating on features Z2 from the Bayes optimal classifier relying on Z1. We
formally refer to the classifiers of interests as: (1) principal classifier w1(x)y = p(Y = y∣Z1 = z1(x));
(2) orthogonal classifier w2(x)y = p(Y = y∣Z2 = z2(x)); (3) full classifier wx(x)y = p(Y = y∣X = x).
3.1	Classifier orthogonalization
Our key idea relies on the bijection between the density ratio and the Bayes optimal classifier (Sugiyama
et al., 2012). Specifically, the ratio of densities pX∣Y (x∣i) andpX∣Y (x∣j), assigned to an arbitrary point
x, can be represented by the Bayes optimal classifier w(χ)i = Pr(Y = i∣χ) as PXIY(Xj) = PYj)W(X)i ∙
Similar, the principal classifier w1 gives us associated density ratios of class-conditional distributions
over Zi. For any i,j ∈ Y, we have pZ1∣Y(z1(x)∣i) = PYg)w1(x)i. These can be combined to obtain
1	PZ1 ∣Y (z1 (X)∣j)	PY (i)w1 (X)j
density ratios of class-conditional distribution pZ2∣Y and subsequently calculate the orthogonal classifier
w2 . We additionally rely on the fact that the diffeomorphism f permits us to change variables between
X and z1,z2: pχ∣γ(x∣i) = Pzι∣γ(zι∣i) * Pz2∣γ(z2∣i) * volJf (z1,z2), where volJf is volume of the
Jacobian (Berger et al., 1987) of the diffeomorphism mapping. Taken together,
pZ2∣Y(z2∣i)	pZ1 ∣Y (zi ∣i)	*pZ2∣Y(z2∣i) * volJf(zi, z2)	pZ1 ∣Y (zi ∣i)	wX(x)i	wi(x)i
=	=	(1)
pZ2∣Y(z2∣j)	pZ1∣Y(zi∣j)	*pZ2∣Y(z2∣j) *volJf(zi,z2)	pZ1∣Y(zi∣j)	wX(x)j	wi(x)j
Note that since the diffeomorphism f is shared with all classes, the Jacobian is the same for all label-
conditioned distributions on Zi, Z2. Hence the Jacobian volume terms cancel each other in the above
equation. We can then finally work backwards from the density ratios of pZ2 ∣Y to the orthogonal classifier:
Pr(Y = i∣Z2 = Z2(χ)) = Pr(Y = i) W^xi / ∑ (Pr(Y= j) W^xj)	⑵
wi (x)i	j	wi (x)j
We call this procedure classifier orthogonalization since it adjusts the full classifier WX to be orthogonal
to the principal classifier Wi . The validity of this procedure requires overlapping supports of the class-
conditional distributions, which ensures the classifier outputs WX(x)i, Wi(x)i to remain non-zero for all
x∈X,i∈Y.
Empirically, we usually have access to a dataset D = {(xt, yt)}tn=i with n iid samples from the joint
distribution PXY. To obtain the orthogonal classifier, we need to first train the full classifier Wx based
on the dataset D. We can then follow the classifier orthogonalization to get an empirical orthogonal
classifier, denoted as w2 = WX ∖ wi. We use symbol ∖ to emphasize that the orthogonal classifier uses
complementary information relative to zi . Algorithm 1 summarizes the construction of the orthogonal
classifier.
Generalization bound. Since WX is trained on a finite dataset, we consider the generalization bound of
the constructed orthogonal classifier. We denote the population risk as R(W) = -EPXY (X,y) [log W(x)y]
and the empirical risk as R(W) = 一IDDi E® 加^D log w(Xi)yi. For a function family W whose elements
map X to the simplex ∆C, we define WX = infwx∈w R(WX),Wi = infwxew R(wx). We further denote
the Rademacher complexity of function family W with sample size ∣D∣ as R∣D∣(W).
3
Published as a conference paper at ICLR 2022
Algorithm 1 Classifier Orthogonalization
Input: principal classifier w1, dataset D.
Train an empirical full classifier Wx on D by empirical risk minimization.
Construct an orthogonal classifier Wx ∖ wι via classifier orthogonalization (Eq. (2)).
return the empirical orthogonal classifier W2 = Wx ∖ wι
Theorem 1. AssumePy is uniform distribution, ∀wx ∈ W takes values in (m, 1 - m)C with m ∈ (0, 2),
and 1/pX ∣Y (x∣y) ∈ (0, γ) ⊂ (0, +∞) holds for ∀x ∈ X, y ∈ Y. Then for any δ ∈ (0, 1) with probability
at least 1 - δ, we have:
∣R(Wx ∖ wι) - R(wx ∖ wι)∣ ≤ (1 + Y)
2R∣d∣(W ) + 2log -m ∖
2log 1
∣D∣
Theorem 1 shows that the population risk of the empirical orthogonal classifier in Algorithm 1 would
be close to the optimal risk if the maximum value of the reciprocal of class-conditioned distributions
1∕pχ∣γ (x∣y) and the Rademacher term are small. Typically, the Rademacher complexity term satisfies
R∣d∣(W) = O(∣D∣-2) (Bartlett & Mendelson, 2001; Gao & Zhou, 2016). We note that the empirical full
classifier may fail in specific ways that are harmful for our purposes. For example, the classifier may
not rely on all the key features due to simplicity bias as demonstrated by (Shah et al., 2020). There are
several ways that this effect can be mitigated, including Ross et al. (2020); Teney et al. (2021).
3.2	Alternative method: importance sampling
An alternative way to get the orthogonal classifier is via importance sampling. For each sample point
(x, y), We construct its importance φ(χ, y) ：= PpZ1(Z：(：))y)∙ Via Bayes' rule, the importance φ(χ, y) can
be calculated from the principal classifier by φ(χ, y) = AY(X))) . We define the importance sampling (IS)
objective as LIS(W) ：= Ex,y~pχY [φ(x, y) log w(x)y]. It can be shown that the orthogonal classifier w2
maximizes the IS objective, i.e., w2 = argmaxLIS. HoWever, the importance sampling method has an
Achilles’ heel: variance. A few bad samples with large weights can drastically deviate the estimation,
which is problematic for mini-batch learning in practice. We provide an analysis of the variance of
the IS objective. It shows the variance increases with the divergences between Z1’s marginal and its
label-conditioned marginals, {Df (pZ1 ∣∣pZ1 ∣Y=y)}yC=1 even when the learned classifier w is approaching
the optimal classifier, i.e., W ≈ w2. Let LnS(W) ：= 1 ∑η=ι Φ(χt, yt) log W(Xt)号七 be the empirical IS
1
objective estimated with n iid samples. Clearly, Var(LnS(W)) = nVar(LIS(w)). While Var(LIS(W))
at w = w2 is the following,
Var(LIS(W2)) = EY [(Df(PZ1∣∣PZ1∣ Y=y ) + I) EZ2∣Y=y log2 PY ∣Z2 (y∣z2)] - L(W2)2
where Df is the Pearson χ2-divergence. The expression indicates that the variance grows linearly with
the expected divergence. In contrast, the divergences have little effect when training the empirical full
classifier in classifier orthogonalization. We provide further experimental results in section 4.1.2 to
demonstrate that classifier orthogonalization has better stability than the IS objective with increasing
divergences and learning rates.
4	Orthogonal Classifier for Controlled Style Transfer
In style transfer, we have two domains A, B with distributions PXY , QXY . We use binary label Y to
indicate the domain of data X (0 for domain A, 1 for domain B). Assume Z1 and Z2 are mutually
orthogonal variables w.r.t both PXY and QXY. The goal is to conduct controlled style transfer between
the two domains. By controlled style transfer, we mean transferring domain A’s data to domain B only
via making changes on partial latent (Z1) while not touching the other latent (Z2). Mathematically,
we aim to transfer the latent distribution from PZ1 PZ2 to QZ1 PZ2 . This task cannot be achieved by
traditional style transfer algorithms such as CycleGAN (Zhu et al., 2017), since they directly transfer data
distributions from PX to QX, or equivalently from the perspective of latent distributions, from PZ1 PZ2
to QZ1 QZ2. Below we show that the orthogonal classifier Wx ∖ W1 enables such controlled style transfer.
Our strategy is to plug the orthogonal classifier into the objective of CycleGAN. The original Cycle-
GAN objective defines a min-max game between two generators/discriminator pairs GAB /DAB and
4
Published as a conference paper at ICLR 2022
-3.0	-2.5	-2.0	-1.5	-1.0
Learning rate (log-scale)
(a)	(b)	(c)
Figure 2: (a,b): Test loss versus log expected divergence and learning rate. (c): The histogram of
predicted probability of different methods. The two red lines are the ground-truth probabilities for
P(Y = 1∣Brown background) and P(Y = 1∣Green background).
Gba/Dba. GAB transforms the images from domain A to domain B. We use P to denote the generative
distribution, Le., GAB (X)〜P for X 〜P. Similarly, Q denotes the generative distribution of generator
GBA. The minimax game of CycleGAN is given by
min max LGAABN(GAB, DAB) + LGBAAN(GBA, DBA) + Lcyc(GAB, GBA)	(3)
GAB,GBA DAB,DBA
where GAN losses LGAABN , LGBAAN minimize the divergence between the generative distributions and the
targets, and the cycle consistency loss Lcyc is used to regularize the generators (Zhu et al., 2017).
Concretely, the GAN loss LGAABN is defined as follows,
LGAAN(GAB,Dab) ：= Eχ~Q[logDAB(x)] + Eχ~ρ[log(1 — DAB(x))]
Now we tilt the GAN losses in CycleGAN objective to guide the generators to perform controlled transfer.
Specially, we replace LGAABN in Eq. (3) with the orthogonal GAN loss LOAGBAN when updating the generator
GAB :
LabAn(Gab,Dab) ：= Eχ~p[log(1 — Φ(Dab(x),r(x)))]
where Φ(Dab(χ),r(χ))=" Y DAB(Xi(X)	and r(x) = wχ∖w1(x)0. Consider an extreme case
AB ,	(1-DAB (x))+DAB (x)r(x)	wx∖w1 (x)1
where we allow the model to change all latents, i.e., z1 (x) = x. As a result, LOGAN degenerates to
the original LGAN since Wx ∖ wι ≡ ɪ and Φ(Dab (x),r(x)) ≡ DAB (x). The other orthogonal GAN
loss LOBGAAN can be similarly derived. For a given generator GAB , the discriminator’s optimum is
achieved at DAB(x) = Q(χ)∕(P(x) + Q(χ)). Assuming the discriminator is always trained to be
optimal, the generator GAB Can be viewed as optimizing the following virtual objective: LOABAN(GAB)：=
LABAN(GAB, DAb ) = Ex~P log P(X)PQxx)r(x). The OPtimal generator in the new objective SatiSfieS the
following property.
Proposition 2. The global minimum of LAGAN(GAB) is achieved if and only if Pz1,Z2(z1,z2)=
QZ1 (z1)PZ2 (z2 ).
We defer the proof to Appendix B.4. The proposition states that the new objective LOAGBAN converts the
original global minimum QZ1 QZ2 in LGAABN to the desired one QZ1 PZ2. The symmetric statement holds
for LOBGAAN(GBA).
4.1	Experiments
4.1.1	Setup
Datasets. (a) CMNIST : We construct C(olors)MNIST dataset based on MNIST digits (LeCun & Cortes,
2005). CMNIST has two domains and 60000 images with size (3, 28, 28). The majority of the images
in each domain will have a digit color and a background color correspond to the domain: domain A
C “red digit/green background” and domain B C “blue digit/brown background”. We define two bias
degrees λd, λb to control the probability of the images having domain-associated colors, e.g., for an
image in domain A, with λd probability, its digit is set to be red; otherwise, its digit color is randomly
red or blue. The background colors are determined similarly with parameter λb . In our experiments,
5
Published as a conference paper at ICLR 2022
Table 1: CMNIST		Table 2: CelebA-GH				
	Z1 acc.	Z2 acc.	Z1 acc.		Z2 acc.	FID J
Vanilla	90.2	14.3	Vanilla	88.4	16.8	38.5
wx ∖ w1 (MLDG)	94.9	91.1	wx ∖ w1 (MLDG)	90.9	53.0	46.2
wx ∖ w1 (TRM)	89.2	96.2	wx ∖ w1 (TRM)	92.2	56.5	39.8
wx ∖ w1 (Fish)	93.9	98.0	wx ∖ w1 (Fish)	88.8	42.9	43.4
IS (Oracle)	90.0	100	IS (Oracle)	89.1	40.6	44.3
wx ∖ w1 (Oracle)	94.9	100	wx ∖ w1 (Oracle)	93.7	57.4	39.7
we set λd = 0.9 and λb = 0.8. Our goal is to transfer the digit color (Z1) while leaving the background
color (Z2) invariant. (b) CelebA-GH: We construct the CelebA-G(ender)H(air) dataset based on the
gender and hair color attributes in CelebA (Liu et al., 2015). CelebA-GH consists of 110919 images
resized to (3, 128, 128). In CelebA-GH, domain A is non-blond-haired males, and the domain B is
blond-haired females. Our goal is to transfer the facial characteristic of gender (Z1) while keeping the
hair color (Z2) intact.
Models. We compare variants of orthogonal classifiers to the vanilla CycleGAN (Vanilla) and the
importance sampling objective (IS). We consider two ways of obtaining the w1 classifier: (a) Oracle:
The principal classifier w1 is trained on an oracle dataset where Z1 is the only discriminative direction,
e.g., setting bias degrees λd = 0.9 and λb = 0 in CMNIST such that only digit colors vary across domains.
(b) Domain generalization: Domain generalization algorithms aim to learn the prediction mechanism that
is invariant across environments and thus generalizable to unseen environments (Blanchard et al., 2011;
Arjovsky et al., 2019). The variability of environments is considered as nuisance variation. We construct
environments such that only Y ∣Z1 is invariant. We defer details of the environments to Appendix E.1.
We use three domain generalization algorithms, Fish (Shi et al., 2021), TRM (Xu & Jaakkola, 2021) and
MLDG (Li et al., 2018), to obtain w1. We indicate different approaches by the parentheses after wx ∖ w1
and IS, e.g., wx ∖ w1 (Oracle) is the orthogonal classifier with w1 trained on Oracle datasets.
Metric. We use three metrics for quantitative evaluation: 1) Z1 accuracy: the success rate of transferring
an image’s latent z1 from domain A to domain B; 2) Z2 accuracy: percentage of transferred images
whose latents z2 are unchanged; 3) FID scores: a standard metric of image quality (Heusel et al., 2017).
We only report FID scores on the CelebA-GH dataset since it is not common to compute FID on MNIST
images. Z1, Z2 accuracies are measured by two oracle classifiers that output an image’s latents z1 and
z2 (Appendix E.1.3).
For more details of datasets, models and training procedure, please refer to Appendix E.
4.1.2	Results
Comparison to IS. In section 3.2, we demonstrate that IS suffers from high variance when the di-
vergences of the marginal and the label-conditioned latent distributions are large. We provide further
empirical evidence that our classifier orthogonalization method is more robust than IS. Fig. 2(a) shows
that the test loss of IS increases dramatically with the divergences. Fig. 2(b) displays that IS’s test loss
grows rapidly when enlarging learning rate, which corroborates the observation that gradient variances
are more detrimental to the model’s generalization with larger learning rates (Wang et al., 2013). In
contrast, the test loss of wx ∖ w1 remains stable with varying divergences and learning rates. In Fig. 2(c),
we visualize the histograms of predicted probabilities of wx ∖ w1 (light color) and IS (dark color).
We observe that the predicted probabilities of wx ∖ w1 better concentrates around the ground-truth
probabilities (red lines).
Main result. As shown in Table 1 and 2, adding an orthogonal classifier to the vanilla CycleGAN
significantly improves its z2 accuracy (from 14 to 90+ in CMNIST, from 16 to 40+ in CelebA-GH)
while incurring a slight loss of the image quality. We observe that the oracle version of orthogonal classi-
fier (wx ∖ w1 (Oracle)) achieves best z2 accuracy on both datasets. In addition, class orthogonalization is
compatible with domain generalization algorithms when the prediction mechanism Z1∣Y is invariant
across collected environments.
In Fig. 3, we visualize the transferred samples from domain A. We observe that vanilla CycleGAN
models change the background colors/hair colors along with digit colors/facial characteristics in CMNIST
and CelebA. In contrast, orthogonal classifiers better preserve the orthogonal aspects Z2 in the input
images. We also provide visualizations for domain B in Appendix C.
6
Published as a conference paper at ICLR 2022
(a) CMNIST: Domain A	(b) CelebA-GH: Domain A
Figure 3: Style transfer samples on the domain A of CMNIST and CelebA-GH. We visualize the
inputs (top row) and the corresponding transferred samples of different methods.
5	Orthogonal Classifier for Domain adaptation
In unsupervised domain adaptation, we have two domains, source and target with data distribution ps,
pt . Each sample comes with three variables, data X, task label Y and domain label U . We assume U is
uniformly distributed in {0 (source), 1 (target)}. The task label is only accessible in the source domain.
Consider learning a model h = g。f that is the composite of the encoder f : X → Z and the predictor
g : Z → Y. A common practice of domain adaptation is to match the two domain,s feature distributions
ps(f(X)),pt(f(X)) via domain adversarial training (Ganin & Lempitsky, 2015; Long et al., 2018; Shu
et al., 2018). The encoder f is optimized to extract useful feature for the predictor while simultaneously
bridging the domain gap via the following domain adversarial objective:
minmaxL(f,D) ：= Eχ~ps [logD(f (x))] + Eχ~pt [log(1 - D(f (x)))]	⑷
where discriminator D : Z → [0,1] distinguishes features from two domains. The equilibrium of the
objective is achieved when the encoder perfectly aligns the two domain, i.e., ps (f (x)) = pt (f (x)). We
highlight that such equilibrium is not desirable when the target domain has shifted label distribution (Az-
izzadenesheli et al., 2019; des Combes et al., 2020). Instead, when the label shift appears, it is more
preferred to align the conditioned feature distribution,i.e., ps(f(x)∣y) = pt(f(x)∣y), ∀y ∈ Y.
Now we show that our classifier orthogonalization technique can be applied to the discriminator for
making it “orthogonal to” the task label. Specifically, consider the original discriminator as full classifier
Wx, i.e., for a given f, Wx (x)o = arg maXD L(f,D) = PS (f p)fxf∙(x))∙ We then construct the principle
classifier w1 that discriminates the domain purely via the task label Y , i.e., w1 (x)0 = Pr(U = 0∣Y =
y(x)). Note that here we assume the task label Y can be determined by the data X, i.e., Y = y(X). We
focus on the case that Y is discrete so W1 could be directly computed via frequency count. We propose
to train the encoder f with the orthogonalized discriminator Wx ∖ W1 ,
min fL(f,(wx ∖ wι)(∙)o) = Ex~ps[log(wx ∖ wι)(x)0] + Ex~p』log(1 - (Wx ∖ wι)(x)0)]	(5)
Proposition 3. Suppose there exists random variable Z2 = z2 (X) orthogonal to y(X) = Y w.r.t
p(f(X), U). Then f achieves global optimum if and only if it aligns all label-conditioned feature
distributions, i.e., ps (f(x)∣y) = pt(f(x)∣y), ∀y ∈ Y.
Note that in practice, we have no access to the target domain label prior pt(Y ) and the label y(x) for
target domain data X ~ Pt. Thus we use the pseudo label y as a surrogate to construct the principle
classifier, where y(x) = arg maXy h(x)y is generated by our model h.
5.1	Experiments
Models. We take a well-known domain adaptation algorithm VADA (Shu et al., 2018) as our baseline.
VADA is based on domain adversarial training and combined with virtual adversarial training and entropy
regularization. We show that utilizing orthogonal classifier can improve its robustness to label shift. We
compare it with two improvements: (1) VADA+Wx ∖ W1 which is our method that plugs in the orthogonal
classifier as a discriminator in VADA; (2) VADA+IW: We tailor the SOTA method for label shifted
domain adaptation, importance-weighted domain adaptation (des Combes et al., 2020), and apply it to
VADA. We also incorporate two recent domain adaptation algorithms for images— CyCADA (Hoffman
et al., 2018) and ADDA (Tzeng et al., 2017)—into comparisons.
7
Published as a conference paper at ICLR 2022
Setup. We focus on visual domain adaptation and directly borrow the datasets, architectures and domain
setups from Shu et al. (2018). To add label shift between domains, we control the label distribution in
the two domains. For source domain, we sub-sample 70% data points from the first half of the classes
and 30% from the second half. We reverse the sub-sampling ratios on the target domain. The label
distribution remains the same across the target domain train and test set. Please see Appendix E.2 for
more details.
Results. Table 3 reports the test accuracy on seven domain adaptation tasks. We observe that VADA+wx ∖
w1 improves over VADA across all tasks and outperforms VADA+IW on five out of seven tasks. We find
VADA+IW performs worse than VADA in two tasks, MNIST→SVHN and MNIST→MNIST-M. Our
hypothesis is that the domain gap is large between these datasets, hindering the estimation of importance
weight. Hence, VADA-IW is unable to adapt the label shift appropriately. In addition, the results show
that VADA+wx ∖ w1 outperforms ADDA on six out of seven tasks.
Table 3: Test accuracy on visual domain adaptation benchmarks
Source Target	MNIST MNIST-M	SVHN MNIST	MNIST SVHN	DIGITS SVHN	SIGNS GTSRB	CIFAR STL	STL CIFAR
Source-Only	51.8	75.7	34.5	85.0	74.7	68.7	47.7
ADDA	89.7	78.2	38.4	86.0	90.6	66.8	50.4
CyCADA	-	82.8	39.6	-	-	-	-
VADA	77.8	79.0	35.7	90.3	93.6	72.4	53.1
VADA + IW	71.2；	87.1↑	34.5 ；	90.7 ↑	95.4↑	74.0↑	53.8↑
VADA + wx ∖ w1	79.1	88.0↑	40.5↑	90.6↑	95.2↑	74.5 ↑	54.1↑
6	Orthogonal Classifier for Fairnes s
We are given a dataset D = {(xt, yt, ut)}tn=1 that is sampled iid from the distribution pXYU, which
contains the observations x ∈ X, the sensitive attributes u ∈ U and the labels y ∈ Y. Our goal is to learn a
classifier that is accurate w.r.t y and fair w.r.t u. We frame the fairness problem as finding the orthogonal
classifier of an “totally unfair” classifier w1 that only uses the sensitive attributes u for prediction. We
can directly get the unfair classifier w1 from the dataset statistics, i.e., w1 (x)y = p(Y = y∣U = u(x)).
Below we show that the orthogonal classifier of unfair classifier meets equalized odds, one metric for
fairness evaluation.
Proposition 4. If the orthogonal random variable of U = u(X) w.r.t pXY exists, then the orthogonal
classifier wx ∖ w1 satisfies the criterion of equalized odds.
We emphasize that, unlike existing algorithms for learning fairness classifier, our method does not require
additional training. We obtain a fair classifier via orthogonalizing an existing model wx which is simply
the vanilla model trained by ERM on the dataset D.
6.1	Experiments
Setup. We experiment on the UCI Adult dataset, which has gender as the sensitive attribute and the UCI
German credit dataset, which has age as the sensitive attribute (Zemel et al., 2013; Madras et al., 2018;
Song et al., 2019). We compare the orthogonal classifier wx ∖ w1 to three baselines: LAFTR (Madras
et al., 2018), which proposes adversarial objective functions that upper bounds the unfairness metrics;
L-MIFR (Song et al., 2019), which uses mutual information objectives to control the expressiveness-
fairness trade-off; ReBias (Bahng et al., 2020), which minimizes the Hilbert-Schmidt Independence
Criterion between the model representations and biased representations; as well as the Vanilla (wx)
trained by ERM. We employ two fairness metrics - demographic parity distance (Δdp) and equalized
odds distance (Δeo) - defined in Madras et al. (2018). We denote the penalty coefficient of the adversarial
or de-biasing objective as γ, whose values govern a trade-off between prediction accuracy and fairness, in
all baselines. We borrow experiment configurations, such as CNN architecture, from Song et al. (2019).
Please refer to Appendix E.3 for more details.
Results. Tables 4, 5 report the test accuracy, ∆DP and ∆EO on Adults and German datasets. We observe
that the orthogonal classifier decreases the unfairness degree of the Vanilla model and has competitive
performances to existing baselines. Especially in the German dataset, compared to LAFTR, our method
8
Published as a conference paper at ICLR 2022
has the same ∆EO but better ∆DP and test accuracy. It is surprising that our method outperforms LAFTR
even though it is not specially designed for fairness. Further, our method has benefit of being training-free
which allows it to be applied to improve any off-the-shelf classifier’s fairness without additional training.
Table 4: Accuracy v.s. Fairness (Adults)
	Acc.↑	△dp 1	△EO 1
Vanilla	84.5	0.19	0.20
LAFTR (γ = 0.1)	84.2	0.14	0.09
LAFTR (γ = 0.5)	84.0	0.12	0.07
L-MIFR (γ = 0.05)	81.6	0.04	0.15
L-MIFR (γ = 0.1)	82.0	0.06	0.16
ReBias (γ = 100)	84.3	0.15	0.11
ReBias (γ = 50)	84.4	0.17	0.16
wx ∖ w1	81.6	0.12	0.12
Table 5: Accuracy v.s. Fairness (German)
	Acc.↑	△DP 1	△EO 1
Vanilla	76.0	0.19	0.33
LAFTR (γ = 0.1)	73.0	0.11	0.17
LAFTR (γ = 0.5)	72.7	0.11	0.19
L-MIFR (γ = 0.1)	75.8	0.10	0.21
L-MIFR (γ = 0.05)	75.6	0.08	0.18
ReBias (γ = 100)	73.0	0.07	0.17
ReBias (γ = 50)	75.0	0.10	0.20
wx ∖ w1	75.4	0.09	0.18
7	Related Works
Disentangled representation learning Similar to orthogonal random variables, disentangled represen-
tations are also based on specific notions of feature independence. For example, Higgins et al. (2018)
defines disentangled representations via equivalence and independence of group transformations, Kim &
Mnih (2018) relates disentanglement to distribution factorization, and Shu et al. (2020) characterizes
disentanglement through generator consistency and a notion of restrictiveness. Our work differs in two
key respects. First, most definitions of disentanglement rely primarily on the bijection between latents
and inputs (Shu et al., 2020) absent labels. In contrast, our orthogonal features must be conditionally
independent given labels. Further, in our work orthogonal features remain implicit and they are used
discriminatively in predictors. Several approaches aim to learn disentangled representations in an un-
supervised manner (Chen et al., 2016; Higgins et al., 2017; Chen et al., 2018). However, Locatello
et al. (2019) argues that unsupervised disentangled representation learning is impossible without a proper
inductive bias.
Model de-biasing A line of works focuses on preventing model replying on the dataset biases. Bahng
et al. (2020) learns the de-biased representation by imposing HSIC penalty with biased representation,
and Nam et al. (2020) trains an unbiased model by up-weighting the high loss samples in the biased
model. Li et al. (2021) de-bias training data through data augmentation. However, these works lack
theoretical definition for the de-biased model in general cases and often require explicit dataset biases.
Density ratio estimation using a classifier Using a binary classifier for estimating the density ra-
tio (Sugiyama et al., 2012) enjoys widespread attention in machine learning. The density ratio estimated
by classifier has been applied to Monte Carlo inference (Grover et al., 2019; Azadi et al., 2019), class
imbalance (Byrd & Lipton, 2019) and domain adaptation (Azizzadenesheli et al., 2019).
Learning a set of diverse classifiers Another line of work related to ours is learning a collection of
diverse classifiers through imposing penalties that relate to input gradients. Diversity here means that the
classifiers rely on different sets of features. To encourage such diversity, Ross et al. (2018; 2020) propose
a notion of local independence, which uses cosine similarity between input gradients of classifiers as
the regularizer, while in Teney et al. (2021) the regularizer pertains to dot products. Ross et al. (2017)
sequentially trains multiple classifiers to obtain qualitatively different decision boundaries. We defer
more discussions of the advantages of classifier orthogonalization over existing methods to Appendix F.
8	Conclusion
We consider finding a discriminative direction that is orthogonal to a given principal classifier. The
solution in the linear case is straightforward but does not generalize to the non-linear case. We define
and investigate orthogonal random variables, and propose a simple but effective algorithm (classifier
orthogonalization) to construct the orthogonal classifier with both theoretical and empirical support.
Empirically, we demonstrate that the orthogonal classifier enables controlled style transfer, improves
existing alignment methods for domain adaptation, and has a lower degree of unfairness.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The work was partially supported by an MIT-DSTA Singapore project and by an MIT-IBM Grand
Challenge grant. YX was partially supported by the HDTV Grand Alliance Fellowship. We would like
to thank the anonymous reviewers for their valuable feedback.
References
Martin Arjovsky, L. Bottou,Ishaan Gulrajani, and David LoPez-Paz. Invariant risk minimization. ArXiv,
abs/1907.02893, 2019.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian J. Goodfellow, and Augustus Odena. Discriminator
rejection samPling. ArXiv, abs/1810.06758, 2019.
K. Azizzadenesheli, Anqi Liu, Fanny Yang, and Anima Anandkumar. Regularized learning for domain
adaPtation under label shifts. ArXiv, abs/1903.09734, 2019.
Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased
rePresentations with biased rePresentations. In ICML, 2020.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian comPlexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, 2001.
M. Berger, Bernard Gostiaux, and S. Levy. Differential geometry: Manifolds, curves, and surfaces. 1987.
G. Blanchard, Gyemin Lee, and C. Scott. Generalizing from several related classification tasks to a new
unlabeled samPle. In NIPS, 2011.
Jonathon Byrd and Zachary Chase LiPton. What is the effect of imPortance weighting in deeP learning?
In ICML, 2019.
Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David Kristjanson Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In NeurIPS, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and P. Abbeel. Infogan: Inter-
Pretable rePresentation learning by information maximizing generative adversarial nets. In NIPS,
2016.
Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon. Domain adaptation with
conditional distribution matching and generalized label shift. ArXiv, abs/2003.04475, 2020.
Yaroslav Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. ArXiv,
abs/1409.7495, 2015.
Wei Gao and Zhi-Hua Zhou. Dropout rademacher complexity of deep neural networks. Science China
Information Sciences, 59(7):072104, 2016.
Aditya Grover, Jiaming Song, Alekh Agarwal, Kenneth Tran, Ashish Kapoor, E. Horvitz, and S. Er-
mon. Bias correction of learned generative models using likelihood-free importance weighting. In
DGS@ICLR, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and S. Hochreiter. Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.
Irina Higgins, Lolc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained
variational framework. In ICLR, 2017.
Irina Higgins, David Amos, David Pfau, SebaStien RaCaniere, Loic Matthey, Danilo Jimenez Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. ArXiv, abs/1812.02230,
2018.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, and
Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.
10
Published as a conference paper at ICLR 2022
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.
Y. LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning
for domain generalization. ArXiv, abs/1710.03463, 2018.
Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Loddon Yuille, and Cihang
Xie. Shape-texture debiased neural network training. ArXiv, abs/2010.05981, 2021.
Z. Liu, Ping Luo, Xiaogang Wang, and X. Tang. Deep learning face attributes in the wild. 2015 IEEE
International Conference on Computer Vision (ICCV), pp. 3730-3738, 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard SchOlkopf, and Olivier Bachem.
Challenging common assumptions in the unsupervised learning of disentangled representations. ArXiv,
abs/1811.12359, 2019.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial domain
adaptation. In NeurIPS, 2018.
David Madras, Elliot Creager, T. Pitassi, and R. Zemel. Learning adversarially fair and transferable
representations. In ICML, 2018.
Jun Hyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure:
Training debiased classifier from biased classifier. ArXiv, abs/2007.02561, 2020.
Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training
differentiable models by constraining their explanations. In IJCAI, 2017.
Andrew Slavin Ross, Weiwei Pan, and Finale Doshi-Velez. Learning qualitatively diverse and inter-
pretable rules for classification. ArXiv, abs/1806.08716, 2018.
Andrew Slavin Ross, Weiwei Pan, Leo Anthony Celi, and Finale Doshi-Velez. Ensembles of locally
independent prediction models. In AAAI, 2020.
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls
of simplicity bias in neural networks. ArXiv, abs/2006.07710, 2020.
Yuge Shi, Jeffrey S. Seely, Philip H. S. Torr, N. Siddharth, Awni Y. Hannun, Nicolas Usunier, and Gabriel
Synnaeve. Gradient matching for domain generalization. ArXiv, abs/2104.09937, 2021.
Rui Shu, H. Bui, H. Narui, and S. Ermon. A dirt-t approach to unsupervised domain adaptation. ArXiv,
abs/1802.08735, 2018.
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly supervised disentan-
glement with guarantees. ArXiv, abs/1910.09772, 2020.
Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and S. Ermon. Learning controllable
fair representations. In AISTATS, 2019.
Masashi Sugiyama, Taiji Suzuki, and T. Kanamori. Density ratio estimation in machine learning. 2012.
Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity
bias: Training a diverse set of models discovers solutions with superior ood generalization. ArXiv,
abs/2105.05612, 2021.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. 2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2962-
2971, 2017.
Chong Wang, X. Chen, Alex Smola, and E. Xing. Variance reduction for stochastic gradient optimization.
In NIPS, 2013.
Yilun Xu and T. Jaakkola. Learning representations that support robust transfer of predictors. ArXiv,
abs/2110.09940, 2021.
11
Published as a conference paper at ICLR 2022
R. Zemel, Ledell Yu Wu, Kevin Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In
ICML, 2013.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision
(ICCV),pp. 2242-2251, 2017.
12
Published as a conference paper at ICLR 2022
A	More properties of orthogonal random variable
In this section we demonstrate the properties of orthogonal random variables. We also discuss the
existence and uniqueness of orthogonal classifier when given the principal classifier. Below we list some
useful properties of orthogonal random variables.
Proposition 5. Let Z1 and Z2 be any mutually orthogonal w.r.t PXY, then we have
(i)	Invariance: For any diffeomorphism g, g(Z1) and g(Z2) are also mutually orthogonal.
(ii)	Distribution-free: Z1,Z2 are mutually orthogonal w.r.t Pχ'γ if and only if there exists a diffeo-
morphism mapping between X and X'.
(iii)	Existence: Suppose the label-conditioned distributions py ∈ C1, supp(py) = X, ∀y ∈ Y. For z1 ∈ C1
and z1(X) is diffeomorphism to Euclidean space, then ∀i ∈ Y, there exists a diffeomorphism map-
ping (z1 (X), z2,i (X)) such that z1(X) ⊥ z2,i (X)∣Y = i. Further if there exists diffeomorphism
mappings between z2,is, then the orthogonal r.v. of z1(X) exists.
In the following theorem, we justify the validity of the classifier orthogonalization when z1(x) satisfies
some regularity conditions.
Theorem 2. Suppose w1(x)i = p(Y = i∣z1(x)). If the label-condition distributions pys, and z1 satisfy
the conditions in Proposition 5.(iii), then wx ∖ w1 is the unique orthogonal classifier of w1.
Note that our construction of wx ∖ w1 does not require the exact expression of z1(X), its orthogonal r.v
or data generation process. The theorem states that the orthogonal classifier constructed by classifier
orthogonalization is the Bayesian classifier for any orthogonal random variables of z1 (X).
B Proofs
B.1 Proof for Proposition 1
Proposition 1. Suppose the orthogonal r.v. ofz1(X) w.r.tpXY exists and is denoted as z2(X). Then
z(X) = z2(X) is a maximizer of I(z(X); Y ∣z1 (X)) subject to I(z(X); z1 (X)∣Y ) = 0.
Proof. For any function z, by chain rule we have I(z1(X), z2(X); Y ) = I(z1(X); Y ) +
I(z2(X); Y∣zι(X)) and I(zι(X),z(X); Y) = I(zι(X); Y) +1(z(x); YE(X)). Further, the defini-
tion of the orthogonal r.v. ensures that there exists a differmorphism f between X and z1(X), z2(X),
whichimpliesI(z1(X),z(f(z1(X),z2(X)));Y)=I(z1(X),z(X);Y).
Hence by data processing inequality we have:
I(zι(X ),z2(X); Y )≥I(zι(X ),z(f (zι(X ),z2(X))); Y) = I(zι(X ),z(X); Y)
⇔ I (zι(X); Y) +1 (z2(X); Y E(X ))≥i(zι(X); Y) +1 (z(X); Y E(X))
⇔ I(z2(X); Y∣zι(X)) ≥ I(z(X); Y∣zι(X))
The above inequality shows that z = z2 maximize the mutual information I(z(X); Y∣z1(X)). Further,
by definition of the orthogonal r.v., z = z2 is independent of z1 conditioned on Y, i.e. z(X) ⊥ z1(X)∣Y
which is equivalent to I(z(X); zι(X)∣Y) = 0.	口
B.2 Proof for Theorem 1
Theorem 1. Assume Py is uniform distribution, ∀Wχ ∈ W takes values in (m, 1 - m)C with m ∈ (0, 2),
and 1∕pχ∣γ (x∣y) ∈ (0,γ) ⊂ (0, +∞) holds for ∀x ∈ X,y ∈ Y. Thenfor any δ ∈ (0,1) with probability
at least 1 - δ, we have:
∣R(Wχ ∖ wι) - R(w^ ∖ wι)∣ ≤ (1 + γ)
2R∣d∣(W ) + 2log : ∖
2log 1
∣D∣
Proof. Denote the empirical risk of w as
一
R(W) = R(W)
IDɪ	∑	log W(Xi) yi
∣D∣ (xi ,yi )∈D
13
Published as a conference paper at ICLR 2022
and the population risk as
R(W) = -Ep(x,y) [lɑgw(x)y]
Step 1: bounding excess risk IR(Wx)- R(w"∣.
By the classical result in theorem 8 in Bartlett & Mendelson (2001), We know that for any W ∈ W,
δ ∈ (0,1), since - log w(x)y ∈ (0, log A), with probability at least 1 - δ,
-	1 Λ	1	/ 2log 1
IR(W)- R(W)I= |— Σ lθg(W(Xi))yi - Ep [log(W(X)y )]| ≤ 2Rn(W) + 2lθg — V ------------
n i=1	m n
Since	Wx	=	Infg€wR(WX),Wx	= infg€w	R(WX),	and	IR(Wx)	- R(Wx)I	≤	2Rn(W)	+
生”，IR(Wx)- R(Wx)I ≤ 2Rn(W) + 2log A√2logi, we have
IR(Wx)- R(Wx)I ≤ 2%(W) + 2log -∖ 2⅛
m n
(6)
Step 2: bounding IR(Wx ∖ wi) - R(Wx ∖ wi)I by IR(Wx) - R(Wx)I.
R(w ∖ w1 ) = -Ep(x,y)
log
W(X) y
W1(x)y
W(X) y'
W WI(X)y'
R(W) + Ep(x,y)
logWl(X)y∑ * I
Then we have:
IR(Wx ∖ wι) - R(wx ∖ wι)I
≤ IR(Wx)- R(Wx)I + Ep(x,y)
IR(Wx)- R(wx)I + Ep(x,y)
log WMX)y∑ ≡⅛ _
log - V ]
log	Wt (x)y'
.∑y' E7 j
「	Wx (X)y' 】
-Ep(x,y)
log WI(X)y∑ ⅛⅛ I
≤ IR(Wx) - R(wx)I + max
Ep(x,y) logmax
y
WI(X)y'
WC (X)y'
WI(X) y' _
,EP(X，y)logmin
Wx(X)y'
WI(X)y'
Wt (X) y'
WI(X)y' J
We define the ratio γ(x, y) = Wx(x)y. We define ⅛(x) ：= argminy γ(x, y), y(X) ：= argmaχy r(X,y).
We have r(x,y(x)) ≤ ∑y'Wx(X)y'/WI(X)y' ≤ r(x,y(x)). Let assume -ʌʒ ≤ Y for all x,y. For any
,Σ	∑y' Wx(x)y' /W1 (x)y'	,	p(y∣X)	,
function y/ ： X → Y, we have the following bound, where we have importance weight φ(x, y) ：= ./)
if y = y'(x) otherwise 0,
IExlogr(x,y'(x))I = IEx,yφ(x,y)bgr(x,y)I ≤ YIEx,y logr(x,y)I = YIR(Wx)- R(Wx))I
As a result, we have
E log ∑y' Wx(X)y /W1 (X)y ≤ max (IE log r(x,y(x))I, IE log r(x,y(x))I) ≤ YIR(Wx) - R(wx)I
∑y' Wx(X)y'/WI(X)y'
Thus
IR(Wx ∖ wι) - R(wx ∖ wι)I ≤ IR(Wx) - R(wx)I
/	「	Wx(X)y' J
+ maχ卜p(x,y) l°gmax⅛⅛
∖	L	W1(x)y,一
≤ (1 + Y)IR(Wx)- R(Wx)I
log min
y'
Wx(X)y' ^j∣∖
WI(X)y' I I
Wx(X)y' I I
WI(X)y, Jl∕
⑺
14
Published as a conference paper at ICLR 2022
Combining Eq. (6) and Eq. (7), we know that with probability 1 - δ, we have:
∣R(Wχ ∖ wι) - R(Wx ∖ wι)∣ ≤ (1 + Y) I 2Rn(W) + 2log∖∕2bg δ ∣
∖	m n J
□
B.3	Proof for Proposition 5 and Theorem 2
Proposition 5. Let Z1 and Z2 be any mutually orthogonal w.r.t PXY, then we have
(i)	Invariance: For any diffeomorphism g, g(Z1) and g(Z2) are also mutually orthogonal.
(ii)	Distribution-free: Z1,Z2 are mutually orthogonal w.r.t Pχ'γ if and only if there exists a diffeo-
morphism mapping between X and X'.
(iii)	Existence: Suppose the label-conditioned distributions py ∈ C1, supp(py) = X, ∀y ∈ Y. For z1 ∈ C1
and z1(X) is diffeomorphism to Euclidean space, then ∀i ∈ Y, there exists a diffeomorphism map-
ping (z1 (X), z2,i (X)) such that z1(X) ⊥ z2,i (X)∣Y = i. Further if there exists diffeomorphism
mappings between z2,is, then the orthogonal r.v. of z1 (X) exists.
Proof. (i) Since Z1 and Z2 are independent, the independence also holds for g(Z1) and g(Z2). In
addition, we construct the diffeomorphism between (g(Z1), g(Z2)) and X as f(g(Z1), g(Z2)) =
f (g-1(g(Z1)), g-1(g(Z2))) = X. Apparently f(g-1(Z1), g-1(Z2)) is a diffeomorphism.
(ii)	We denote the diffeomorphism between X' and X as f. Then the diffeomorphism mapping between
(Zι, Z2) and X is f o f. Thus Z1,Z2 are mutually orthogonal w.r.t X'.
(ii	i) We will use a constructive proof.
Step 1: Denote dim(X) = d, dim(z1(X)) = k, then we can prove that the manifold[z1(X), Rd-k] is
diffeomorphism of X. We denote the diffeomorphism as f, and denote f(x) = [z1(x), t(x)] where
zι ： X → zι(X),t: X → Rd-k.
Step 2: For X∣Y = y, let Fj(t(x)j ∣ t(x)ι,…,t(x)j-ι, zι(x), 1) : R → [0,1],j ∈ {1,...,d — k} be
the CDF corresponding to the distribution of t(x)j ∣ t(X)1 = t(x)ι,…,t(X)j-ι = t(x)j-1,z1(X)=
z1(x),Y = y.
h(t(x); z1(x))1 = F1(t(x)1 ∣ z1(x), y)
h(t(x); z1(x))i = Fi(t(x)i ∣ t(x)1, …, t(x)i-1, z1(x), 1),i ∈ {2, . . . , d}
The conditions p ∈ C1, p(x) > 0, ∀x ∈ X ensure that the PDF of f(X) is also in C1 and takes values
in (0, ∞). Thus the above CDFs are all diffeomorphism mapping. By the inverse CDF theorem, for
any z1(x), h(t(X); z1(X) = z1(x)) is a uniform distribution in (0, 1)d-k. Hence the random variable
defined by the mapping h, i.e., z2,y(X) = h(t(X); z1(X)), is independent of z1(X) given Y = 1.
In addition, by the construction of z2,y (x) we know that there exists a diffeomorphism f between
(zι(X),t(X)) and (zι(X),z2,y(X)) such that f(zι(X),t(X)) = (z1(X),z2,y(X)). Then f-1 o
∕-1(z1(X), z2,y (X)) = X is also a diffeomorphsim mapping.
Step 3: From the condition we know that for every y ∈ Y, there exists a diffeomorphism function my such
thatmyoz2,y(x) = z2,1(x). Apparently ∀y, my oz2,y (X) ⊥ z2,1(X)∣Y = y by z2,y (X) ⊥ z1(X)∣Y = y.
Further, (z1, z2,1) is a diffeomorphism. Hence z2,1(X) is the orthogonal r.v. of z1(X) w.r.t PXY.
□
Theorem 2. Suppose w1(x)i = p(Y = i∣z1(x)). If the label-condition distributions pys, and z1 satisfy
the conditions in Proposition 5.(iii), then wx ∖ w1 is the unique orthogonal classifier of w1.
Proof. By the existence property in Proposition 5, we know that for X, there exists a orthogonal r.v. of
z1(X) and denote it as z2(X1). By the proposition we know that there exists a common diffeomorphism
f mapping (z1(X∣y), z2(X∣y)) to X∣y, ∀y ∈ Y.
15
Published as a conference paper at ICLR 2022
Further, by change of variables and the definition of orthogonal r.v., we have
Pi,2(Z2)
Pj,2 (Z2)
Pi,1(ZI)Pi,2(Z2)VOJf(Z1,z2) / Pi,1 (ZI)	Pi(X) / Pi,1 (ZI)	Wx(X)i / WI(X)i Th 逅小 卜代 f 1二
Pj,1(z1)Pj,2(z2)volJf(z1,z2) / j∏Z∏ = E / j∏Z∏ = WxW / WW. ThUs Via the bijecti0n of clas-
sifier and density ratiOs we knOw that wX ∖ w1 (x)i = p(Y = i∣z2(x)).
□
B.4	Proof for Proposition 2
Proposition 2. The global minimum of LAGAN(GAB) is achieved if and only if P⅛1,Z2(z1,z2)=
QZ1 (z1)PZ2(z2).
Proof. By definition, r(x) = Iww(XX) = Q(Z2(X)). ThUs We can reformulate the criterion LOABAN as the
following:
LAGAn(Gab ) = Ex~p log
P(X)
P (x) + Q(x)r(x)
EZ1,Z2~P log
P(Z1,Z2)
P(z1,z2)+ Q(ZI)Q(Z2) Q(Z2)
≥ - log Ej D ] = - log2
where we get the lower bound by Jensen's inequality. The equality holds when P(z1,z2) = Q(zι)P(z2).
□
B.5	Proof for Proposition 3
Proposition 3. Suppose there exists random variable Z2 = Z2 (X) orthogonal to y(X) = Y w.r.t
p(f (X), U). Then f achieves global optimum if and only ifit aligns all label-conditioned feature
distributions, i.e., ps(f (x)∣y) = pt(f (x)∣y), ∀y ∈ Y.
Proof. By definition, optimal classifier WX(X)O = P (fpsfXf(x)), principle classifier w1(x)0 =
P (y(¾()yI(Px)()y(X)). By the definition of orthogonal r.v., (Y, U) and f (X) have a bijection between
them. It suggests, conditioned on Y, the supports of f(X)IY = y is non-overlapped for different y in
both domains Ps andpt. Itmeans ifps(f (X)Iy) > 0 then ∀y' ≠ y, ps(f (∕)∣y') = 0.
Thus the orthogonal classifier wX ∖ w1 satisfies:
(“，、“，)(T) - Psf(X)) /(Psf(X)) , pt(f(X)))
X 1	°- Ps(y(X)) Ps(y(X))	Pt(y(X))
∑ Ps(f(X)∣y')Ps(y') /(
Ps(y(X))	/ ∖
pt(y(x))
∑y, Ps(f(X)∣y')Ps(y') + ∑ Pt(f(X)∣y')Pt(y')
Ps(y(X))
)
Psf(X)Iy(X))Ps(y(X)) / (
Ps(y(X))	/ ∖
Ps (f (X)Iy (X))
Psf(X)Iy(X))Ps(y(x)) + pt(f (X)Iy(X))Pt(y(X))
ps(y(x))
pt(y(x))
)
ps(f (x)∣y(x)) + pt(f(x)∣y(x))
Thus the objective in Eq. (5) can be reformulated as,
L(f, (wx ∖ w1)(∙)0)
= EX~ps [log
ps(f(x)∣y(x))
ps(f (x)∣y(x)) + pt(f (x)∣y(x))
] + E [log________Ptf(X)Iy(X))________]
] X-Pt[ gPs(f(X)Iy(X)) + Pt(f(X)Iy(X))]
Ey-Ps Ex-Ps (X∣y)[- log
ps(f (x)∣y) +pt(f(x)∣y)
ps(f (x)∣y)
]+ Ey-Pt EX-Pt(X∣y)[- ■ Pff⅜≡ ]
≥ - Ey~Ps
log EX~Ps (X∣y) [
ps(f (x)∣y) +pt(f(x)∣y)
ps(f (x)∣y)
] - Ey-Pt
Ps(f (X)|y)+ Pt(f (X)Iy)I
log EX-Pt(XIy) [ -Pf)W-]
= - 2 log 2.
16
Published as a conference paper at ICLR 2022
The lower-bound holds due to Jensen’s inequality. The equality is aChieved if and only if
invariant to x, for all y, i.e., ∀y ∈ Y, ps (f(x)∣y) = pt(f (x)∣y).
Ps(f(X)Iy) iq
Ptf(XW) is
□
B.6 Proof for Proposition 4
Proposition 4. If the orthogonal random variable of U = u(X) w.r.t pXY exists, then the orthogonal
classifier wX ∖ w1 satisfies the criterion of equalized odds.
Proof. We denote the orthogonal random variables as z2 (X), and wX ∖ w1 (x) = Pr[y∣z2(x)]. SinCe
z2(X) ⊥ U∣Y , we know that wX ∖ w1(X) ⊥ U∣Y . HenCe the prediCtion of the Classifier wX ∖ w1 is
Conditional independent of sensitive attribute U on the ground-truth label Y , whiCh meets the equalized
odds metric.	口
C Extra Samples

əl。巴O
oCnn
-书>
oCnn
-WA
(a) CMNIST: Domain A
(C) CelebA-GH: Domain A
-I 召AoCn≡əpEJO
(d) CelebA-GH: Domain B

Figure 4: CMNIST and CelebA-GH
D Extra Experiments
D.1 Style transfer
Table 6: CelebA
	Z1 acc.	Z2 acc.	FID J
Vanilla	75.3	87.0	36.2
wX ∖ w1-MLDG	81.8	93.5	35.2
wX ∖ w1-TRM	76.3	92.5	33.5
wX ∖ w1 -Fish	74.2	93.3	33.1
IS-Oracle	733	88.1	36.5
wX ∖ w1 -Oracle	84.0	95.2	38.5
We show transferred samples for both domains in Fig. 4 on CMNIST and CelebA-GH.
17
Published as a conference paper at ICLR 2022
D.2 Style transfer on CelebA
Unlike the split in CelebA-GH dataset, domain A is males, and domain B is females. The two domains
together form the full CelebA (Liu et al., 2015) with 202599 images. Note that there exists large
imbalance of hair colors within the male group, with only around 2% males having blond hair. Table 6
reports the Z1/Z2 accuracies and FID scores on the full CelebA datasets. We find that our method
improves the style transfer on the original CelebA dataset, especially the Z2 accuracy. It shows that our
method can help style transfer in real-world datasets with multiple varying factors.
E	Extra Experiment Details
E.1 Style transfer
E.1.1 Dataset details
We randomly sampled two digits colors and two background colors for CMNIST. For CelebA-GH, we
extract two subsets—male with non-blond hair and female with blond hair—by the attributes provided in
CelebA dataset. For all datasets, we use 0.8/0.2 proportions to split the train/test set.
For CelebA dataset, following the implementation of Zhu et al. (2017) (https://github.com/
junyanz/CycleGAN), we use 128-layer UNet as the generator and the discriminator in PatchGAN.
For CMNIST dataset, we use a 6-layer CNN as the generator and a 5-layer CNN as the discriminator.
We adopt Adam with learning rate 2e-4 as the optimizer and batch size 128/32 for CMNIST/CelebA.
E.1.2 Details of Models
We craft datasets for training the oracle w1(x) = Pr(Y ∣z1(x)). For CMNIST, oracle dataset has bias
degrees 0.9/0. For CelebA-GH, oracle dataset has {male, female} as classes and the hair colors under
each class are balanced.
For each dataset, we construct two environments to train the domain generalization models. For CMNIST,
the two environments are datasets with bias degrees 0.9/0.4 and 0.9/0.8. For CelebA-GH, the two
environments consist of different groups of equal size: one is {non-blond-haired males, blond-haired
females} and the other is {non-blond-haired males, blond-haired females, non-blond-haired females,
blond-haired males}. Note that the facial characteristic of gender (Z1) is the invariant prediction
mechanism across environments, instead of the hair color.
E.1.3 EVALUATION FOR Z1/Z2 ACCURACY
To evaluate the accuracies on Zι, Z2山tents, We train two oracle classifiers w； (x) =
Pr(Y|zi(x)),w；(x) = Pr(Y∣z2(χ)) on two designed datasets. Specifically, w1∕w2 are trained on
datasets D1 /D2 with Z1/Z2 as the only discriminative direction. For CMNIST, D1 is the dataset with
bias degrees 0.9/0 and D2 is the dataset with bias degrees 0/0.8. For CelebA-GH, D1 is the dataset
with {male, female} as classes and the hair colors under each class are balanced. D2 is the dataset with
{blond hair, non-blond hair} as classes and the males/females under each class are balanced.
The Z1 accuracy on dataset D for transfer model G is:
Zi accuracy = -ɪ- ∑ I ( arg max w； (x) = arg max w； (G(X)))
∣D∣ x∈D	y	y
And Z2 accuracy is:
Zi accuracy = -ɪ- ∑ I ( arg max w；(x) = arg max w； (G(X)))
∣D∣ x∈D	y	y
where I is the indicator function. Zi accuracy measures the success rate of transferring the Zi and Z2
accuracy measures the success rate of keeping Z2 aspects unchanged.
E.2 Domain adaptation
The full names for the seven datasets are MNIST, MNIST-M, SVHN, GTSRB, SYN-DIGITS (DIGITS),
SYN-SIGNS (SIGNS), CIFAR-10 (CIFAR) and STL-10 (STL). We sub-sample each dataset by the
designated imbalance ratio to construct the pairs.
18
Published as a conference paper at ICLR 2022
We use the 18-layer neural network in Shu et al. (2018) with pre-process via instance-normalization.
Following Shu et al. (2018), smaller CNN is used for digits (MNIST, MNIST-M, SVHN, DIGITS) and
traffic signs (SIGNS, GTSRB) and larger CNN is used for CIFAR-10 and STL-10. We use the Adam
optimizer and hyper-parameters in Appendix B in Shu et al. (2018) except for MNIST → MNIST-M,
where we find that setting λs = 1, λd = 1e - 1 largely improves the VADA performance over label
shifts. Furthermore, we set λd = 1e - 2, i.e., the default value suggested by Shu et al. (2018), to enable
adversarial feature alignment.
E.3 Fairness
The two datasets are: the UCI Adult dataset2 which has gender as the sensitive attribute; the UCI German
credit dataset3 which has age as the sensitive attribute. We borrow the encoder, decoder and the final fc
layer from Song et al. (2019). For fair comparison, we use the same encoder+fc layer as the function
family W of our classifier since our method do not need decoder for reconstruction. We use Adam with
learning rate 1e-3 as the optimizer, and a batch size of 64.
The original ReBias method trains a set of biased models to learn the de-biased representations (Bahng
et al., 2020), such as designing CNNs with small receptive fields to capture the texture bias. However, in
the fairness experiments, the bias is explicitly given, i.e., the sensitive attribute. For fair comparison, we
set the HSIC between the sensitive attribute and the representations as the de-bias regularizer.
F Learning a diverse set of classifiers
In this section we discuss the difference and advantages of the classifier orthogonalization, compared
to methods that learn a diverse set of classifiers. We also provide a counter-example to show that the
orthogonal input gradient does not lead to a pair of orthogonal classifiers. We show that orthogonal
classifiers might not in optimal solution set of minimizing input gradient penalty.
The goals of learning diverse classifiers include interpretability (Ross et al., 2017), overcoming simplicity
bias (Teney et al., 2021), improving ensemble performance (Ross et al., 2020), and recovering confound-
ing decision rules (Ross et al., 2018). There is a direct trade-off between diversity and accuracy and
this is controlled by the regularization parameter. In contrast, in our work, given the principal classifier,
our method directly constructs a classifier that uses only the orthogonal variables for prediction. There
is no trade-off to set in this sense. We also focus on novel applications of controlling the orthogonal
directions, such as style transfer, domain adaptation and fairness. Further, our notion of orthogonality is
defined via latent orthogonal variables that relate to inputs via a diffeomorphism (Definition 1) as opposed
to directly in the input space. Although learning diverse classifiers through input gradient penalties
is efficient, it does not guarantee orthogonality in the sense we define it. We provide an illustrative
counter-example in Appendix F, where the input space is not disentangled nor has orthogonal input
gradients but corresponds to a pair of orthogonal classifiers in our sense. We also prove that, given the
principal classifier, minimizing the loss by introducing an input gradient penalty would not necessarily
lead to an orthogonal classifier.
We note that one clear limitation of our classifier orthogonalization procedure is that we need access to
the full classifier. Learning this full classifier can be impacted by simplicity bias (Shah et al., 2020) that
could prevent it from relying on all the relevant features. We can address this limitation by leveraging
previous efforts (Ross et al., 2020; Teney et al., 2021) to mitigate simplicity bias when training the full
classifier.
F.1 Relationship to orthogonal input gradients
The orthogonality constraints on input gradients demonstrate good performance in learning a set of
diverse classifiers (Ross et al., 2017; 2018; 2020; Teney et al., 2021). They typically use the dot product
of input gradients between pairs of classifiers as the regularizer. We highlight that when facing the latent
orthogonal random variables, the orthogonal gradient constraints on input space no longer guarantees to
learn the orthogonal classifier. We demonstrate it on a simple non-linear example below.
Consider a binary classification problem with the following data generating process. Label Y is uniformly
distributions in {-1, 1}. Conditioned on the label, we have two independent k-dimensional latents
2https://archive.ics.uci.edu/ml/datasets/adult
3https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29
19
Published as a conference paper at ICLR 2022
Z1,Z2 SUCh that Zι∣Y = y ~ N(yμι,Ik), Z2∣Y = y ~ N(yμ2,Ik). μι ∈ Rk,μ2 ∈ Rm are the
means of the latent variables, and k > m. The data X is generated from latents via the following
diffeomorphism X = g(Z1 - (0(k-m))) where g is an arbitrary non-linear diffeomorphism on Rk.
∖ Z )
We Can see that Z1 , Z2 are mUtUally orthogonal w.r.t the distribUtion pXY . Now Consider the Bayes
optimal classifiers w1,w2 using variable Z1,Z2. We have wι(x) = PY∣Z1 (1∣zι) = σ(2μTzι) and
w2(x) = Py∣Z2(I∣z2) = σ(2μTz2) , where σ is the sigmoid function. Apparently, they are a pair of
orthogonal classifiers. Besides, we denote the classifier based on the first m dimensions in z1 for
prediction as w3, i.e., w3(x) = Pγ∣(Z1)m (1∣(zι)m) = σ(2(μι)Tm(zι)m)∙
Given the principal classifier w1, we consider using the loss function in Ross et al. (2018; 2020) to learn
the orthogonal classifier of w1, by adding a input gradient penalty term to the standard classification loss:
Lwi (w) = Lc(W) + λE[cos2(VχW1(x), Vχw(x))]
Lc is the expected cross-entropy loss, cos is the cosine-similarity and λ is the hyper-parameter for the
gradient penalty term (Ross et al., 2018; 2020). Below we show that (1) the orthogonal classifier w2
does not necessarily satisfy E[cos2(Vxw1(x), Vxw2(x))] = 0. (2) the orthogonal classifier w2 is not
necessarily the minimizer of Lw1 (w).
Proposition 6. For the above (Z1, Z2), their corresponding Bayes classifiers w1, w2 satisfy
E[VχWι(x)τ VχW2(x)] = 0 if and only if (μι)Tmμ2 = 0.
Proof. Let X1	=	g(Z1	-	(0	Z2	)),	X2 =	Z2	are the two components of X. Note that z1	= g-1 (x1) +
(0 x2 ). By chain rule, the input gradient of w1 is
VχWι(x) = z1VVzi Pr(Y = 1∣Zι = zι(x))
dx
=(Vxig-1(xι))	e-2μTZI(X)	, 2
Im×k	(k+m)×k (1 + e-2μTZI(X))2
where Im×k is the submatrix ofIk×k. Similarly we get VXw2(x) = ( 0 )
m×m (k+m)×m
Together, the dot product between input gradients is
VxWV(X)TVχW2(x) = ；e-』X22T(XI))T (I 0 )
(1 + e 2μ1 z1(x))2	Im×k	(k+m)×k ∖Im×m∕ (k+m)×m
4e-2μTZI(X)-2μT z2 (X) (μ1)Tmμ2
(1 + e-2μT ZI(X))2(1 + e-2μT z2(X))2
e-2μTz2(x)
(1+e-2μTz2(X))2	μ2,
2e-2μT z2(X)μ2
(1 + e-2μT z2(X))2
SinCe (1+益；；(：；)二1；：；：(X))2 > 0, E[VXWI(X)TVXw2(x)] ifandonlyif (μι)Tmμ2 = 0.
Proposition 6 first shows that the expected input gradient product of the two Bayes classifiers of underlying
latents is non-zero, unless the linear decision boundaries on zV, z2 are orthogonal. Hence, given the
principal classifier WV, the expected dot product of input gradients between WV and its orthogonal
classifier W2 does not have to be zero.
Proposition 7. The orthogonal Classifer w2 does not minimize Lwi (W) if (μι)m = μ2 and
μT VXig-1(x1)T VXig-1(x1)k×m(μ1)m < 0. Particularly, we sh^w that Lwi (w3) < Lwi (w2) in this
case.
Proof. Let w3(x) = pγ∖(Zi)m (1∣(zι)m). The input gradient of w3(x) is
VXW3(x) = (VXWI(X))：m = dχ- V(Zi)：m Pr(Y = 1∣(Zl)m
= ((VXig-V(XV))k×m)
Im×m	(k+m)×m
(Zl(x))：m)
e-2(μi ):mzi (X):m
(1 + e-2(μi)mzi(X)m )2
,2(μl)m
20
Published as a conference paper at ICLR 2022
When (μι1m = μ2, the dot-product between Vxw3(χ), Vxwι(χ) is
T / 、	2e-2μTzI(X)μτ
VXWI(X) VxW3(X) = (I + e-2μT ZI(X))2
((Vxi g-1(χ1))
Im×m
(Vxi g-1(χ1))
1m×k
k×m ∖
(k+m) ×m
(k+m)×k
e-2 (μi)imzi (X)：m
(1 + e-2(μι)mzι(x)m )2
4e-2μi ZI(X)-2 (μi ):mzi (x)：m
(1 + e-2μT ZI(X))2(1 + e-2(*i)T>I(X)m )2
(I μ2 Il2 +μ1 VXIg (XI) VXIg (X1)kxm(M1)：m)
T
・ 2(μ1)ιm
When (μ1)m = μ2, we know that Z1(x), z2(x) are equally predictive of the label and
thus Lc(w3) = LC(W2). Note that if μ1 NXIg-1(x1)τNXIg-1(x1)k×m(μ11m < 0,
E[cos2(VχW1(x), VχW2(x))] > E[cos2(VχW1(x), VχW3(x))]:
E[cos2(VxW1(x), VχW2(x))]
we have
∖21
En ∣ (Vxig-1(X1))
I 阿 ∣2
μ1 II2II (I)	μ2 II2
(k+m)×k	(k+m)×m
∖21
> E
(H μ2 ∣∣2 +μτVxig-1(χ1)τ(v%g-1(χ1))k×mμ2)
IlI(VXigI1(XI))	μ112∣ (VXyT(XI))
L∖	1	(k+m)×k	Im×k
(k+m)×k
μ2 ∣2
=E[cos2(VχW1(x), Vχw3(x))]
The strict inequality holds by μTVxig-1(χ1)τVxig-1(xι)k×m(μι)-.m < 0
(Vxi g-1(χ1))
1m×k
μ212 =I (vxig01(XI))	μ2 + (I 0 J
(k+m)×k	(k+m)×k	m×k / (k+m)×k
μ2 II2
=(∣ (vxig01(XI))	μ212 + I (I0J μ2
(k+m)×k	m×kJ (k+m)×k
≥∣ (I°)	μ2 12
m×k ( (k+m)×k
To verify that μf Vxig-1(X1)τVxig-1(X1)k×m(μ1)m < 0 does exist, pick k = 3,m = 2,
i
∣2)2
g-1(χ)
2	-3	3∖	∕1∖
1	-1	5	X,	and μ1 =	1	.	We have μf Vxig-1(χ1)τVxig-1(χ1)k×m(μ1)m =	-2 in this case.
1	-1	1	∖1∕
Together, we have £皿(w3) < Lw 1 (w2).
□
Proposition 7 shows that the orthogonal classifier W2 is not the minimizer of the loss with input gradient
penalty. The results above also hold for un-normalized version of input gradient penalty in Teney et al.
(2021) by similar analysis.
21