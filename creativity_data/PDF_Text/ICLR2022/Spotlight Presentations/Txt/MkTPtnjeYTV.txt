Published as a conference paper at ICLR 2022
On the Optimal Memorization Power of ReLU
Neural Networks
Gal Vardi* Gilad Yehudai* & Ohad Shamir
Department of Computer Science
Weizmann Institute of Science
{gal.vardi,gilad.yehudai,ohad.shamir}@weizmann.ac.il
Ab stract
We study the memorization power of feedforward ReLU neural networks. We
show that such networks can memorize any N points that satisfy a mild separabil-
ity assumption using O (√N) parameters. Known VC-dimension upper bounds
imply that memorizing N samples requires Ω(√N) parameters, and hence our
construction is optimal up to logarithmic factors. We also give a generalized con-
struction for networks with depth bounded by 1 ≤ L ≤ Nn , for memorizing N
samples using O(N/L) parameters. This bound is also optimal up to logarithmic
factors. Our construction uses weights with large bit complexity. We prove that
having such a large bit complexity is both necessary and sufficient for memoriza-
tion with a sub-linear number of parameters.
1	Introduction
The expressive power of neural networks has been widely studied in many previous works. These
works study different aspects of expressiveness, such as the universal approximation property (Cy-
benko, 1989; Leshno et al., 1993), and the benefits of depth in neural networks (Telgarsky, 2016;
Eldan and Shamir, 2016; Safran and Shamir, 2017; Daniely, 2017; Chatziafratis et al., 2019). An-
other central and well studied question is about their memorization power.
The problem of memorization in neural networks can be viewed in the following way: For every
dataset of N labeled samples (x1, y1), . . . , (xN, yN) ∈ X × Y, construct a network N : X → Y
such that N (xi) = yi for every i = 1, . . . , N. Many works have shown results regarding the
memorization power of neural networks, using different assumptions on the activation function and
data samples (see e.g. Huang and Babri (1998); Huang (2003); Baum (1988); Vershynin (2020);
Daniely (2019; 2020); Bubeck et al. (2020); Park et al. (2020); Hardt and Ma (2016); Yun et al.
(2019); Zhang et al. (2021); Nguyen and Hein (2018); Rajput et al. (2021); Sontag (1997)). The
question of memorization also have practical implications on phenomenons such as ”double descent”
(Belkin et al. (2019); Nakkiran et al. (2019)) which connects the memorization power of neural
networks with their generalization capabilities.
A trivial lower bound on the required size of the network for memorizing N labeled points is implied
by the VC dimension of the network (cf. Shalev-Shwartz and Ben-David (2014) ). That is, if a
network with a certain size cannot shatter any specific set of N points, then it certainly cannot
memorize all sets of N points. Known VC dimension bounds for networks with W parameters is
on the order of O(W 2) (Goldberg and Jerrum (1995); Bartlett et al. (1998; 2019)). Hence, it follows
that memorizing N samples would require at least Ω (N1/2) parameters. The best known upper
bound is given in Park et al. (2020), where it is shown that memorizing N data samples can be done
using a neural network with O N2/3 parameters. Thus, there is a clear gap between the lower
and upper bounds, although we note that the upper bound is for memorization of any set of data
samples, while the lower bound is for shattering a single set of data samples. In this paper we ask
the following questions:
* Equal contribution
1
Published as a conference paper at ICLR 2022
What is the minimal number of parameters that are required to memorize N la-
beled data samples? Is the task of memorizing any set of N data samples more
difficult than shattering a single set of N samples?
We answer these questions by providing a construction of a ReLU feedforward neural network which
achieves the lower bound up to logarithmic factors. In this construction we use a very deep neural
network, but with a constant width of 12. In more details, our main result is the following:
Theorem 1.1 (informal statement). Let (x1, y1), . . . , (xN, yN) ∈ Rd × {1, . . . , C} be a set of N
labeled samples of a constant dimension d, with kxi k ≤ r for every i and kxi - xj k ≥ δ for every
i = j. Then, there exists a ReLU neural network F : Rd → R with width 12, depth O √N^,
and O (√N) parameters, such that F(Xi) = yi for every i ∈ [N], where the notation O(∙) hides
logarithmic factors inN, C, r, δ-1.
Comparing this result to the known VC bounds, we show that, up to logarithmic factors, our con-
struction is optimal. This also shows, quite surprisingly, that up to logarithmic factors, the task of
memorizing any set of N points is not more difficult than shattering a single set of N points, under
the mild separability assumption of the data samples. We note that this result can also be extended
to regression tasks (see Remark 3.3).
In our construction, the depth of the network is Θ (√N). We also give a generalized construction
where the depth of the network is limited to some 1 ≤ L ≤ √N. In this case, the number of param-
eters in our construction is O (L) (see Theorem 5.1). We compare this result to the VC-dimension
bound from Bartlett et al. (2019), and show that our construction is optimal up to logarithmic factors.
Our construction uses a bit extraction technique, inspired by Telgarsky’s triangle function (Telgarsky
(2016)), and by Safran and Shamir (2017). Using this technique, We are able to use weights with
bit complexity Θ(√N), and deep neural networks to “extract” the bits of information from the
specially crafted weights of the network. We also generalize our results to the case of having a
bounded bit complexity restriction on the weights. We show both lower (Theorem 6.1) and upper
(Theorem 6.2) bounds, proving that memorizing N points using a network with N1- parameters,
for some ∈ [0, 0.5] can be done if the bit complexity of each weight is Θ (N). Hence, our
construction is also optimal, up to logarithmic factors, w.r.t the bit complexity of the network. We
emphasize that also in previous works showing non-trivial VC bounds (e.g. Bartlett et al. (1998;
2019)) weights with large bit complexity are used. We note that increasing the bit complexity beyond
N1/2 cannot be used to further reduce the number of parameters (see the discussion in section 4).
Related work
Memorization — upper bounds
The problem of memorizing arbitrary data points with neural networks has a rich history. Baum
(1988) studied memorization in single-hidden-layer neural networks with the threshold activation,
and showed that d Nd ] neurons suffice to memorize N arbitrary points in general position in Rd
with binary labels. Bubeck et al. (2020) extended the construction of Baum (1988) and showed that
single-hidden-layer ReLU networks with 4 ∙ d N e hidden neurons can memorize N points in general
position with arbitrary real labels. In Huang et al. (1991) and Sartori and Antsaklis (1991) itis shown
that single-hidden-layer networks with the threshold activation can memorize any arbitrary set of N
points, even if they are not in general position, using N - 1 neurons. Huang and Babri (1998)
proved a similar result for any bounded non-linear activation function σ where either limz→∞ σ(z)
or limz→-∞ σ(z) exists. Zhang et al. (2021) proved that single-hidden-layer ReLU networks can
memorize arbitrary N points inRd with arbitrary real labels using N neurons and 2N +d parameters.
Huang (2003) showed that two-hidden-layers networks with the sigmoid activation can memorize
N points with O( √N) neurons, but the number of parameters is still linear in N. Yun et al. (2019)
proved a similar result for ReLU (and hard-tanh) networks. Vershynin (2020) showed that threshold
and ReLU networks can memorize N binary-labeled unit vectors in Rd separated by a distance of
δ > 0, using O (e1∕δ2 + √Nj neurons and O (e1∕δ2 (d + √N) + N) parameters. Rajput et al.
2
Published as a conference paper at ICLR 2022
(2021) improved the dependence on δ by giving a construction with O (1 + √NJ neurons and
O (d + N) parameters. This result holds only for threshold networks, but does not assume that the
inputs are on the unit sphere.
The memorization power of more specific architectures was also studied. Hardt and Ma (2016)
proved that residual ReLU networks with O(N ) neurons can memorize N points on the unit sphere
separated by a constant distance. Nguyen and Hein (2018) considered convolutional neural networks
and showed, under certain assumptions, memorization using O(N ) neurons.
Note that in all the results mentioned above the number of parameters is at least linear inN . Our
work is inspired by Park et al. (2020), that established a first memorization result with a sub-linear
number of parameters. They showed that neural networks with sigmoidal or ReLU activations can
memorize N points in Rd separated by a normalized distance of δ, using O (N2/3 + log(1∕δ))
parameters (where the dimension d is constant). Thus, in this work We improve the dependence on
N from N2/3 to √N (up to logarithmic factors), which is optimal. We also note that the first stage
in our construction is similar to the first stage in theirs. In Park et al. (2020) there is also a discussion
about the bit complexity of the network, and the necessity that the bit complexity depends on N to
achieve memorization with a sub-linear number of parameters.
Finally, optimization aspects of memorization were studied in Bubeck et al. (2020); Daniely (2019;
2020).
Memorization — lower bounds
Lower bounds on the number of parameters required for memorization are implied by bounds on
the VC dimension of neural networks. indeed, ifW parameters are not sufficient for shattering even
a single set of size N, then they are clearly not sufficient for memorizing all sets of size N. The
VC dimension of neural networks has been extensively studied in recent decades (cf. Anthony and
Bartlett (2009); Bartlett et al. (2019)). The most relevant results for our work are by Goldberg and
Jerrum (1995) and Bartlett et al. (2019). We discuss these results and their implications in Sections 4
and 5.
Trade-offs between the number of parameters of the network and the Lipschitz parameter of the
prediction function in memorizing a given dataset are studies in Bubeck et al. (2021); Bubeck and
Sellke (2021).
The benefits of depth
in this work we show that deep networks have significantly more memorization power. Quite a
few theoretical works in recent years have explored the beneficial effect of depth on increasing the
expressiveness of neural networks (e.g., Martens et al. (2013); Eldan and Shamir (2016); Telgarsky
(2016); Liang and Srikant (2016); Daniely (2017); Safran and Shamir (2017); Yarotsky (2017);
Safran et al. (2019); Chatziafratis et al. (2019); Vardi and Shamir (2020); Bresler and Nagaraj (2020);
Venturi et al. (2021); Vardi et al. (2021)). The benefits of depth in the context of the VC dimension
is implied by, e.g., Bartlett et al. (2019). Finally, Park et al. (2020) already demonstrated that deep
networks have more memorization power than shallow ones, albeit with a weaker bound than ours.
2 Preliminaries
Notations
For n ∈ N and i ≤ j we denote by BiNi:j (n) the string of bits in places i until j inclusive, in the
binary representation of n and treat it as an integer (in binary basis). For example, BiN1:3(32) =
4. We denote by LEN(n) the number of bits in its binary representation. We denote BiNi (n) :=
BiNi:i (n). For a function f and i ∈ N we denote by f(i) the composition of f with itself i times. We
denote vectors in bold face. We use the O(N) notation to hide logarithmic factors, and use O(N)
to hide constant factors. For n ∈ N we denote [n] := {1, . . . , n}. We say that a hypothesis class H
shatters the points x1, . . . , xN ∈ X if for every y1, . . . , yN ∈ {±1} there is h ∈ H s.t. h(xi) = yi
for every i = 1, . . . , N .
3
Published as a conference paper at ICLR 2022
Neural Networks
We denote by σ(z) := max{0, z} the ReLU function. In this paper we only consider neural net-
works with the ReLU activation.
Let d ∈ N be the data input dimension. We define a neural network of depth L as N : Rd → R
where N (x) is computed recursively by
•	h(I) = σ (W(I)X + b(I)) for W(I) ∈ Rmi ×d, b(I) ∈ Rmi
•	h(i) = σ (W⑴MiT) + b⑴)for W(i) ∈ Rmi×mi-1, b(i) ∈ Rmi for i = 2,...,L — 1
•	N(X) = h(L) = W(L)h(L-1) + b(L) for W (L) ∈ R1×mL-i, b(L) ∈ R1
The width of the network is max{m1, . . . , mL-1}. We define the number of parameters of the
network as the number of weights of N which are non-zero. It corresponds to the number of edges
when we view the neural network as a directed acyclic graph. We note that this definition is standard
in the literature on VC dimension bounds for neural networks (cf. Bartlett et al. (2019)).
Bit Complexity
We refer to the bit complexity of a weight as the number of bits required to represent the weight.
Throughout the paper we use only weights which have a finite bit complexity. Specifically, for
n ∈ N, its bit complexity is dlog ne. The bit complexity of the network is the maximal bit complexity
of its weights.
Input Data Samples
In this work we assume that we are given N labeled data samples (X1, y1), . . . , (XN, yN) ∈ Rd×[C],
and our goal is to construct a network F : Rd → R such that F(Xi) = yi for every i ∈ [N]. We will
assume that there is a separation parameter δ > 0 such that for every i 6= j we have kXi — Xj k ≥ δ.
We will also assume that the data samples have a bounded norm, i.e. there is r > 0 such that
kXi k ≤ r for every i ∈ [N].
Note that any neural network that does not ignore input neurons must have at least d parameters.
Existing memorization results (e.g., Park et al. (2020)) assume that d is constant. As We discuss
in Remark 3.2, in our work we assume that d is at most O(√N). We also note that since our
bounds depend logarithmically on δ , then even if δ depends polynomially on N it Will not change
our asymptotic bounds.
3
一
MEMORIZATION USING O
PARAMETERS
In this section we prove that given a finite set of N labeled data points, there is a network with
O (√N) parameters which memorizes them. Formally, we have the following:
Theorem 3.1. Let N, d, C ∈ N, and r, δ > 0, and let (X1, y1), . . . , (XN, yN) ∈ Rd × [C] be a
set of N labeled samples with kXi k ≤ r for every i and kXi — Xj k ≥ δ for every i 6= j. Denote
R := 10rN2δ-1 √∏d. Then, there exists a neural network F : Rd → R with width 12 and depth
O
卜 N Iog(N) + 帚Ny
• max {log(R), log(C)}
such that F(Xi) = yi for every i ∈ [N].
From the above theorem, we get that the total number of parameters is O (d + √N). For d =
O(√N) (see Remark 3.2 below) we get the lower bound presented in Theorem 1.1. Note that except
for the number of data points N and the dimension d, the number of parameters of the network
depends only logarithmically on all the other parameters of the problem (namely, on C, r, δ-1). We
also note that the construction can be improved by some constant factors, but we preferred simplicity
4
Published as a conference paper at ICLR 2022
over minor improvements. Specifically, we hypothesize that it is possible to give a construction
using width 3 (instead of 12) similar to the result in Park et al. (2020). To simplify the terms in the
theorem, we assume that r ≥ 1, otherwise (i.e., if all data points have a norm smaller than 1) we just
fix r := 1 and get the same result. In the same manner, we assume that δ ≤ 1, otherwise we just fix
δ := 1.
Remark 3.2 (Dependence on d). Note that any neural network that does not ignore input neurons
must have at least d parameters. In our construction the first layer consists of a single neuron
(i.e width 1), which means that the number of parameters in the first layer is d + 1. Hence, this
dependence on d is unavoidable. Previous works (e.g., Park et al. (2020)) assumed that d is constant.
In our work, to achieve the bound of O (√N) parameters we can assume that either d is Constant
or it may depend on N with d = O (√N).
Remark 3.3 (From classification to regression). Although the theorem considers multi-class clas-
sification, it is possible to use the method suggested in Park et al. (2020) to extend the result to
regression. Namely, if the output is in some bounded interval, then we can partition it into smaller
intervals of length E each. We define a set of output classes C with O (ɪ) classes, such that each
class corresponds to an interval. Now, we can use Theorem 3.1 to get an approximation of the
output, while the number ofparameters is linear in log (ɪ).
3.1 Proof Intuition
Below we describe the main ideas of the proof. The full proof can be found in Appendix A. The
proof is divided into three stages, where at each stage we construct a network Fi for i ∈ {1, 2, 3},
and the final network is F = F3 ◦ F2 ◦ F1 . Each subnetwork Fi has width O(1), but the depth varies
for each such subnetwork.
Stage I: We project the data points from Rd to R. This stage is similar to the one used in the proof
of the main result from Park et al. (2020). We use a network with 2 layers for the projection. With
the correct scaling, the output of this network on x1 , . . . , xN ∈ Rd are points x1 , . . . , xN ∈ R with
|xi — Xj | ≥ 2 for i = j, and |xi| ≤ R for every i where R = O (N2drδ-1).
Stage II: Following the previous stage, our goal is now to memorize (x1 , y1 ), . . . , (xN, yN) ∈
R × [C], where the xi ’s are separated by a distance of at least 2. We can also reorder the indices
to assume w.l.o.g that xi < X2 < •… < xn. We split the data points into，N log(N) intervals,
each containing Jlog(N)data points. We also construct crafted integers wi..., w√N0g(N) and
ui,..., U√N /N)in the following way: Note that by the previous stage, if we round Xi to [xj,
it can be represented using log(R) bits. Also, each yi can be represented by at most log(C) bits.
Suppose that Xi is the k-th data point in the j-th interval (where j ∈ ['Nlog(N)] and k ∈
{θ,..., J logNN)- 1}), then we define uj-, Wj such that
BINk∙log(R)+L(k+1)∙log(R)(Uj ) = bxic
BINk∙log(C) + L(k + 1)∙log(C)(Wj) = Ji ∙
That is, for each interval j, the number Uj has log(R) bits which represent the integral value of the
k-th data point in this interval. In the same manner, Wj has log(C) bits which represent the label of
the k-th data point in this interval. This is true for all the k data points in the j -th interval, hence Wj
and Uj are represented with Iog(R) ∙
VZ TlogNN and log(C) ∙ ^NN bits respectively.
xi
We construct a network F2 : R → R3 such that F2(xi) = Wji	for each xi, where the i-th data
Uji
point is in the ji-th interval. The integers Wj and Uj are used as parameters of the network F2 .
Stage III: In this stage we construct a network which uses a bit extraction technique adapting Tel-
garsky’s triangle function (Telgarsky, 2016) to extract the relevant information out of the crafted
5
Published as a conference paper at ICLR 2022
integers from the previous stage. In more details, given the input
, we sequentially extract
from U the bits in places k ∙ log(R) + 1 until (k + 1) ∙ log(R) for k ∈
{0,1,..., q IoN)- 1},
and check whether x is at distance at most 1 from the integer represented by those bits. If it is, then
We extract the bits in places k ∙ log(C) + 1 until (k + 1) ∙ log(C) from w, and output the integer
represented by those bits. By the previous stage, for each xi we know that u includes the encoding
of bxic, and since |xi - xj | ≥ 2 for every j 6= i (by the first stage), there is exactly one such xi.
Hence, the output of this netWork is the correct label for each xi .
The construction is inspired by the Works of Bartlett et al. (2019); Park et al. (2020). Specifically,
the first stage uses similar results from Park et al. (2020) on projection onto a 1-dimensional space.
The main difference from previous constructions is in stages II and III Where in our construction We
encode the input data points in the Weights of the netWork. This is the main technique Which alloWs
us to reduce the required number of parameters from the O (N2/3) bound in Park et al. (2020) to
O (N1/2). We also note, that although we use the convention of counting the number of parameters
Without zero-Weights, in this section We do count zero Weights, hence this result is comparable to
Park et al. (2020) where they do count zero weights.
4 On the Optimal Number of Parameters
In section 3 we showed that a network with width O(1) and depth O (√N) can perfectly memorize
N data points, hence only O (√N) parameters are required for memorization. In this section, we
compare our results with the known lower bounds.
First, note that our problem contains additional parameters besides N. Namely, d, C, r and δ. As we
discussed in Remark 3.2, we assume that d is either constant or depends on N with d = O(√N).
In this comparison we also assume that r and δ-1 are either constants or depend polynomially on
N. We note that for a constant d, either r or δ-1 must depend on N. For example, assume d = 1
and r = 1/2, then to have N point in Rd with norm at most r and distance at least δ from one
another we must have that δ-1 ≥ N - 1. Hence, we can bound R ≤ poly(N) (where R is defined
in Theorem 3.1), which implies log(R) = O(log(N)). Moreover, we assume that C ≤ poly(N),
because it is reasonable to expect that the number of output classes is not larger than the number of
data points. Hence, also log(C) = O(log(N)). In regression tasks, as discussed in Remark 3.3, we
can choose C to consist of ∣"∣"∣ classes where E is either a constant or depends at most polynomially
on the other parameters.
Using that R, C ≤ poly(N), and tracing back the bounds given in Theorem 3.1, we get that the
number of parameters of the network is O (PNIog(N)). Note that here the O(∙) notation only
hides constant factors, which can also be exactly calculated using Theorem 3.1.
By Goldberg and Jerrum (1995), the VC dimension of the class of ReLU networks with W param-
eters is O(W 2). Thus, the maximal number of points that can be shattered is O(W 2). Hence, if
we can shatter N points then W = Ω(√N). In particular, it gives a Ω(√N) lower bound for the
number of parameters required to memorize N inputs. Thus, the gap between our upper bound and
the above lower bound is only O (Plog(N)), which is sub-logarithmic.
Moreover, it implies that the number of parameters required to shatter one size-N set is roughly
equal (up to a sub-logarithmic factor) to the number of parameters required to memorize all size-N
sets (that satisfy some mild assumptions). Thus, perhaps surprisingly, the task of memorizing all
size-N sets is not significantly harder than shattering a single size-N set.
If we further assume that C depends on N, i.e., the number of classes depends on the number of
samples (or in regression tasks, as we discussed in Remark 3.3, the accuracy depends on the number
of samples), then we can show that our bound is tight up to constant terms. Thus, in this case the
,log(N) factor is unavoidable. Formally, we have the following lemma:
6
Published as a conference paper at ICLR 2022
Lemma 4.1. Let C, N ∈ N, and assume that C = N for some constant > 0. If we can
express all the functions of the form g : [N] → [C] using neural networks with W parameters, then
W = Ω (pPNlog(N)).
Proof. Let F be the class of all the functions f : [N] × [ log(N)] → {0, 1}. The VC-dimension
bound from Goldberg and Jerrum (1995) implies that expressing all the functions from F with
neural networks requires W = Ω (PN log(N)) parameters, since it is equivalent to shattering a
set of size N log(N). Assume that we can express all the functions of the form g : [N] → [C]
using networks with W0 parameters. Given some function f ∈ F we can express it with a neural
network as follows: define a function g : [N] → [C] such that for every n ∈ [N] and i ∈ [ log(N)],
the i-th bit of g(n) is f(n, i). We construct a neural network for f, such that for an input (n, i)
it computes g(n) (using W0 parameters) and outputs its i-th bit. The extraction of the i-th bit can
be implemented using the bit extraction technique from the proof of Theorem 3.1. Overall, this
network requires O(W0 + log(N)) parameters, and in this manner we can express all the functions
in F. This shows that W =O(W0 + log(N)), but since We also have W = Ω PNIog(N)) then
W0 = Ω (PNlog(Ny).	□
5 Limiting the Depth
In this section we generalize Theorem 3.1 to the case of having a bounded depth. We will then
compare our upper bound on memorization to the VC-dimension bound from Bartlett et al. (2019).
We have the following:
Theorem 5.1. Assume the same setting as in Theorem 3.1, and denote R :=
[√N]. Then, there exists a network F : Rd → R with width O (Lr), depth O
dNCrδ-1. Let L ∈
and a total of O ( -F ,
J	∖L Vlog(L)
• log(R) + d I parameters such that F(Xi) = yi for every i ∈ [N].
Note that for L = √N we get a similar bound to Theorem 3.1. In the proof we partition the dataset
into L2 subsets, each containing L2 data points. We then use Theorem 3.1 on each such subset
to construct a subnetwork of depth O (L • log(R)) and width O(1) to memorize the data points in
the subset. We construct these subnetworks such that their output on each data point outside of
their corresponding subset is zero. Finally we stack these subnetworks to construct a wide network,
whose output is the sum of the outputs of the subnetworks.
By Bartlett et al. (2019), the VC dimension of the class of ReLU networks with W parameters and
depth L is O(W L log(W)). It implies that if we can shatter N points with networks of depth L,
then W = Ω (L l短N)). AS in section 4, assume that d is either constant or at most O(√N), and
that r, δ-1 and C are bounded by some poly(N). Theorem 5.1 implies that we can memorize any
N points (under the mild separability assumption) using networks of depth L and W = O (L)
parameters. Therefore, the gap between our upper bound and the above lower bound is logarithmic.
It also implies that, up to logarithmic factors, the task of memorizing any set of N points using
depth-L neural networks is not more difficult than shattering a single set of N points with depth-L
networks.
Proof of Theorem 5.1. We first construct a network H : Rd → R in the same manner as the con-
struction of F1 is the first stage of Theorem 3.1 (this construction is detailed in Lemma A.2 in the
appendix). This is a 2-layer network with width 1. We denote the output of H on the samples
X1, . . . , XN as x1, . . . , xN . Note that by the construction |xi| ≤ O(R) for every i ∈ [N] and
|xi - xj | ≥ 2 for every i 6= j.
We split the inputs to N subsets of size L2 each, we denote these subsets as Iι,..., IN (assume
L2
w.l.o.g. that L is an integer, otherwise replace it with[备]).For each subset Ik we use stages II
and III from Theorem 3.1 to construct a network Fk such that Fk(xi) = yi if xi ∈ Ik. Note that we
7
Published as a conference paper at ICLR 2022
only need to use the last two stages since we have already done the projection stage, hence are data
samples are already one-dimensional. We construct a network F : R → RN/L2 such that:
(Fι(χ) ∖
F(X)=	. I .
FN/L2 (x)
We also define a network G : RN/L2 → R which adds up all its inputs. Finally, we construct the
network F : Rd → R as F := G ◦ F ◦ H.
By the construction of each Fk from Theorem 3.1, for every x ∈ R, if |x - xi| ≥ 2 for every i ∈ Ik,
then Fk (x) = 0. For every i ∈ [N], there is exactly one k ∈ [N/L2] such that i ∈ Ik. Using the
projection H, for this k We get that Fk (Xi) = y% and for every ' = k We get that F'(xi) = 0. This
means that F(xi) = yi for every i ∈ [N].
The depth of F is the sum of the depths of its subnetwork. Both H and G have depth O(1), and by
Theorem 3.1, the depth ofFk is at most O
(√⅛) ∙log(R))
since each such network memorizes
L2
samples. Hence, the total depth of F is O
(√⅛) •"))
Finally, the width of F is the
maximal width of its subnetworks. The width of G and H is 1. The width of each Fk is also O(1)
by Theorem 3.1, hence the width of F is O(N), which is also the width of F.
Recall that the number of parameters of a network is the number of non-zero weights. In our
case, the network consists of N subnetworks, where the weights between these subnetworks are
zero.
Each such subnetwork has depth O
(√⅛ ∙log(R))
and width O(1).
Also, the pro-
jection phase requires O(d) parameters. In total, the number of parameters in the network is
o (L?N”R)+d).	口
6 Bit Complexity - Lower and Upper Bounds
In the proof of Theorem 3.1 the bit complexity of the network is roughly，N log(N) (See Theo-
rem A.1 in the appendix). On one hand, having such large bit complexity allows us to ”store” and
”extract” information from the weights of the network using bit extraction techniques. This enable
us to memorize N data points using significantly less than N parameters. On the other hand, having
large bit complexity makes the network difficult to implement on finite precision machines.
In this section we argue that having large bit complexity is necessary and sufficient if we want to
construct a network that memorizes N data points with less than N parameters. We show both
upper and lower bounds on the required bit complexity, and prove that, up to logarithmic factors,
our construction is optimal w.r.t. the bit complexity.
First, we show a simple upper bound on the VC dimension of neural networks with bounded bit
complexity. For this upper bound, we view the neural network as a directed graph with U neurons,
and where each weight is represented by B bits. Note that representing each parameter requires
log(B) +2 log(U) bits, that is log(B) bits for the weights magnitude, and 2 log(U) bits for the input
and output neuron of each edge.
Theorem 6.1.	Let H be the hypothesis class of ReLU neural networks with W parameters, where
each parameter is represented by B bits. Then, the VC dimension of H is O(WB + W log(W)).
Proof. We claim that H is a finite hypothesis class with at most 2O(W B+W log(W)) different func-
tions. Let f be a neural network in H, and suppose that it has at most W parameters and U ≤ W
neurons. Then, each weight off can be represented by at most B + 2 log(U) bits, namely, 2 log(U)
bits for the indices of the input and output neurons of the edge, and B bits for the weight magnitude.
Hence, f is represented by at most O(W (B + log(U))) bits. Since U ≤ W we get that every
function in H can be represented by at most O(W (B + log(W))) bits, which gives the bound on the
8
Published as a conference paper at ICLR 2022
size of H. An upper bound on the VC dimension of finite classes is the log of their size (cf. Shalev-
ShWartz and Ben-David (2014)), and hence the VC dimension of H is O(WB + W log(w)).	□
This lemma can be interpreted in the following way: Assume that we want to shatter a single set
of N points, and we use ReLU neural networks with N1-e parameters for some E ∈ [0, ɪ]. Then,
the bit complexity of each weight in the network must be at least Ω(Ne). Thus, to shatter N points
with less than N parameters, we must have a neural network with bit complexity that depends on N.
Also, for E = 2, our construction in Theorem 3.1 (which memorizes any set of N points) is optimal,
UP to logarithmic terms, since having bit complexity of Ω(√N) is unavoidable. We emphasize that
existing works that show non-trivial VC bounds also use large bit complexity (e.g. Bartlett et al.
(1998; 2019)).
We now show an upper bound on the number of parameters of a network for memorizing N data
points, assuming that the bit complexity of the network is bounded:
Theorem 6.2.	Assume the same setting as in Theorem 3.1, and denote R := dNCrδ-1. Let B ∈
[√N]. Then, there exists a network F : Rd → R with bit Complexity O (√ B(B)
, depth
O N NSog(B)
and width O(1) such that F(xi) = yi for every i ∈ [N].
The full proof can be found in Appendix B. The proof idea is to partition the dataset into B subsets,
each containing B2 data points. For each such subset we construct a subnetwork using Theorem 3.1
to memorize the points in this subset. We concatenate all these subnetwork to create one deep
network, such that the output of each subnetwork is added to the output of the previous one. Using
a specific projection from Lemma A.2 in the appendix, this enables each subnetwork to output 0
for each data point which is not in the corresponding subset. We get that the concatenated network
successfully memorizes the N given data points.
Assume, as in the previous sections, that r, δ-1 and C are bounded by some poly(N) and d is
bounded by O(√N). Theorem 6.2 implies that we can memorize any N points (under mild as-
sumptions), using networks with bit complexity B and O (B) parameters. More specifically, we
can memorize N points using networks with bit complexity O (Ne) and O (N 1-e) parameters, for
E ∈ [0, ∣]. Up to logarithmic factors in N, this matches the bound implied by Theorem 6.1.
7 Conclusions
In this work we showed that memorization of N separated points can be done using a feedforward
ReLU neural network with O (√N) parameters. We also showed that this construction is optimal
up to logarithmic terms. This result is generalized for the cases of having a bounded depth network,
and a network with bounded bit complexity. In both cases, our constructions are optimal up to
logarithmic terms.
An interesting future direction is to understand the connection between our results and the optimiza-
tion process of neural networks. In more details, it would be interesting to study whether training
neural networks with standard algorithms (e.g. GD or SGD) can converge to a solution which mem-
orizes N data samples while the network have significantly less than N parameters.
Another future direction is to study the connection between the bounds from this paper and the
generalization capacity of neural networks. The double descent phenomenon (Belkin et al. (2019))
suggests that after a network crosses the ”interpolation threshold”, it is able to generalize well. Our
results suggest that this threshold may be much smaller than N for a dataset with N samples, and it
would be interesting to study if this is true also in practice.
Finally, it would be interesting to understand whether the logarithmic terms on our upper bounds
are indeed necessary for the construction, or these are artifacts of our proof techniques. If these
logarithmic terms are not necessary, then it will show that for neural network, the tasks of shattering
a single set of N points, and memorizing any set of N points are exactly as difficult (maybe up to
constant factors).
9
Published as a conference paper at ICLR 2022
References
M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
P. L. Bartlett, V. Maiorov, and R. Meir. Almost linear vc-dimension bounds for piecewise polynomial
networks. Neural computation,10(8):2159-2173,1998.
P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension and pseudodimen-
sion bounds for piecewise linear neural networks. The Journal of Machine Learning Research,
20(1):2285-2301, 2019.
E. B. Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193-215,
1988.
M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the
classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 116(32):
15849-15854, 2019.
G. Bresler and D. Nagaraj. Sharp representation theorems for relu networks with precise dependence
on depth. arXiv preprint arXiv:2006.04048, 2020.
S. Bubeck and M. Sellke. A universal law of robustness via isoperimetry. arXiv preprint
arXiv:2105.12806, 2021.
S. Bubeck, R. Eldan, Y. T. Lee, and D. Mikulincer. Network size and weights size for memorization
with two-layers neural networks. arXiv preprint arXiv:2006.02855, 2020.
S. Bubeck, Y. Li, and D. M. Nagaraj. A law of robustness for two-layers neural networks. In
Conference on Learning Theory, pages 804-820. PMLR, 2021.
V. Chatziafratis, S. G. Nagarajan, I. Panageas, and X. Wang. Depth-width trade-offs for relu net-
works via sharkovsky’s theorem. arXiv preprint arXiv:1912.04378, 2019.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
A. Daniely. Depth separation for neural networks. In Conference on Learning Theory, pages 690-
696. PMLR, 2017.
A. Daniely. Neural networks learning and memorization with (almost) no over-parameterization.
arXiv preprint arXiv:1911.09873, 2019.
A. Daniely. Memorizing gaussians with no over-parameterizaion via gradient decent on neural
networks. arXiv preprint arXiv:2003.12895, 2020.
R.	Eldan and O. Shamir. The power of depth for feedforward neural networks. In Conference on
learning theory, pages 907-940. PMLR, 2016.
P. W. Goldberg and M. R. Jerrum. Bounding the vapnik-chervonenkis dimension of concept classes
parameterized by real numbers. Machine Learning, 18(2-3):131-148, 1995.
M. Hardt and T. Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
G.-B. Huang. Learning capability and storage capacity of two-hidden-layer feedforward networks.
IEEE transactions on neural networks, 14(2):274-281, 2003.
G.-B. Huang and H. A. Babri. Upper bounds on the number of hidden neurons in feedforward
networks with arbitrary bounded nonlinear activation functions. IEEE transactions on neural
networks, 9(1):224-229, 1998.
S.-C. Huang, Y.-F. Huang, et al. Bounds on the number of hidden neurons in multilayer perceptrons.
IEEE transactions on neural networks, 2(1):47-55, 1991.
10
Published as a conference paper at ICLR 2022
M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a non-
polynomial activation function can approximate any function. Neural networks, 6(6):861-867,
1993.
S.	Liang and R. Srikant. Why deep neural networks for function approximation? arXiv preprint
arXiv:1610.04161, 2016.
J. Martens, A. Chattopadhya, T. Pitassi, and R. Zemel. On the representational efficiency of re-
stricted boltzmann machines. In Advances in Neural Information Processing Systems, pages
2877-2885, 2013.
P.	Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever. Deep double descent: Where
bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019.
Q.	Nguyen and M. Hein. Optimization landscape and expressivity of deep cnns. In International
conference on machine learning, pages 3730-3739. PMLR, 2018.
S.	Park, J. Lee, C. Yun, and J. Shin. Provable memorization via deep neural networks using sub-
linear parameters. arXiv preprint arXiv:2010.13363, 2020.
S.	Rajput, K. Sreenivasan, D. Papailiopoulos, and A. Karbasi. An exponential improvement on the
memorization capacity of deep threshold networks. arXiv preprint arXiv:2106.07724, 2021.
I. Safran and O. Shamir. Depth-width tradeoffs in approximating natural functions with neural
networks. In International Conference on Machine Learning, pages 2979-2987. PMLR, 2017.
I. Safran, R. Eldan, and O. Shamir. Depth separations in neural networks: What is actually being
separated? arXiv preprint arXiv:1904.06984, 2019.
M. A. Sartori and P. J. Antsaklis. A simple method to derive bounds on the size and to train multilayer
neural networks. IEEE transactions on neural networks, 2(4):467-471, 1991.
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms.
Cambridge university press, 2014.
E. D. Sontag. Shattering all sets of ‘k’points in “general position” requires (k—1)/2 parameters.
Neural Computation, 9(2):337-348, 1997.
M. Telgarsky. Benefits of depth in neural networks. In Conference on learning theory, pages 1517-
1539. PMLR, 2016.
G. Vardi and O. Shamir. Neural networks with small weights and depth-separation barriers. arXiv
preprint arXiv:2006.00625, 2020.
G. Vardi, D. Reichman, T. Pitassi, and O. Shamir. Size and depth separation in approximating benign
functions with neural networks. In Conference on Learning Theory, pages 4195-4223. PMLR,
2021.
L. Venturi, S. Jelassi, T. Ozuch, and J. Bruna. Depth separation beyond radial functions. arXiv
preprint arXiv:2102.01621, 2021.
R. Vershynin. Memory capacity of neural networks with threshold and relu activations. arXiv
preprint arXiv:2001.06938, 2020.
D. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
C. Yun, S. Sra, and A. Jadbabaie. Small relu networks are powerful memorizers: a tight analysis
of memorization capacity. In Advances in Neural Information Processing Systems, pages 15558-
15569, 2019.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still)
requires rethinking generalization. Communications of the ACM, 64(3):107-115, 2021.
11
Published as a conference paper at ICLR 2022
A Proof from section 3
We first give the proofs for each of the three stages of the construction, then we combine all the
stages to prove Theorem 3.1. For convenience, we rewrite the theorem, and also state the bound on
the bit complexity of the network:
Theorem A.1. Let N, d, C ∈ N, r, δ > 0, and let (x1, y1), . . . , (xN, yN) ∈ Rd × [C] be a
set of N labeled samples with kxi k ≤ r for every i and kxi - xj k ≥ δ for every i 6= j.
Denote R := 10rN2δ-1 ∖fπd.. Then, there exists a neural network F : Rd → R with width
12, depth O (，Nlog(N) + NXogN ∙ max {log(R), log(C)})
and bit complexity bounded by
O (IOgm) + JloNNi ∙ max{IOg(R), lOg(C)})
such that F(xi) = yi for every i ∈ [N].
A.1 S tage I: Projecting onto a one-dimensional subspace
Lemma A.2. Let x1 , . . . , xN ∈ Rd with kxi k ≤ r for every i and kxi - xj k ≥ δ for every
i = j. Then, there exists a neural network F : Rd → R with width 1, depth 2, and bit complexity
log (3drN2√πδ-1), such that 0 ≤ F(Xi) ≤ 10rN2δ-1√πd for every i ∈ [N] and ∣F(xi) —
F(xj)| ≥ 2 for every i 6= j.
To show this lemma we use a similar argument to the projection phase from Park et al. (2020),
where the main difference is that we use weights with bounded bit complexity. Specifically, we use
the following:
Lemma A.3 (Lemma 8 from Park et al. (2020)). Let N, d ∈ N, then for any distinct X1 , . . . , XN∈
Rd there exists a unit vector u ∈ Rd such that for all i 6= j :
/Πd ∙ N kxi	-	Xjk	≤ |u> (Xi	-	Xj )|	≤	kxi	-	Xjk	⑴
Proof of Lemma A.2. We first use Lemma A.3 to find u ∈ Rd that satisfies Eq. (1). Note that
∣∣uk = 1, hence every coordinate of U is smaller than 1. We define U ∈ Rd such that each of its
coordinates is equal to the firstdlog (dN2 √π) ] bits of the corresponding coordinate of u. Note that
ku - Uk ≤ 2iog(√2√∏) ≤ N√d. For every i = j we have that:
|u>(Xi - Xj)| ≥ |u>(Xi - Xj)| T(U - u)>(Xi - Xj)|
≥ ∖^8dNkXi -Xjk - ku - uk ∙ kXi -Xjk
≥ r∏dN2 kXi -Xj k - N2√πd' kXi -Xj k
≥ N2√∏d kXi - Xjk.	⑵
We also have for every i ∈ [N]:
|u>Xi| ≤ ∣u>Xi∣ + |(u - u)>Xi∣
≤ k"k + N2√∏dk"k ≤ 2kXik .	⑶
Let b := - min{0, mi□i{[U>x∕}} + 1, and note that by Eq. (3) We have b ≤ 2r + 1. We define
the network F : Rd → R in the following way:
F(x) = (2N2δ-1√πd) ∙ σ (U>x + b).
We shoW the correctness of the construction. Let i 6= j , then We have that:
|F(xi) - F(xj)| = (2N2δ-1√πd) ∣σ (U>Xi + b) - σ (U>Xj + b) ∣
=(2N2δ-1√πd) ∣U>Xi + b - (U>Xj + b) ∣
=(2N2δ-1√πd) ∣U>(xi - xj-)∣ ≥ 2 ,
12
Published as a conference paper at ICLR 2022
where the second equality is because by the definition of b We have that U>xi + b ≥ 0 for every i,
and the last inequality is by Eq. (2) and the assumption that kxi - xj k ≥ δ for every i 6= j . Now,
let i ∈ [N], then we have:
|F(xi)| = (2N2δ-1√πd) ∙ σ(U>Xi + b)
=(2N2δ-1√πd) ∙ (U>Xi + b)
≤ (2N2δ-1√πd) ∙ (4r + 1) ≤ 10rN2δ-1√πd ,
where the second to last inequality is since b ≤ 2r + 1 and Eq. (3).
The network F has depth 2 and width 1. The bit complexity of the network can be bounded by
log (3drN2√πδ-1). This is because each coordinate of U can be represented using dlog(dN2√∏)]
bits, the bias b can be represented using at most dlog(2r + 1)~∖ bits and the weight in the second layer
can be represented using dlog(2N2√πdδ-1)e bits.	□
A.2 S tage II: Finding the right subset
Lemma A.4. Let xι < …< XN < R with R > 0, Xi ∈ R for every i ∈ [N] and |xi — xj| ≥ 2
for every i 6= j. Let m ∈ N with m < N and let w1, . . . , wm ∈ N where LEN(wj) ≤ bfor every
j ∈ [m]. Let k := ∣"m"∣. Then, there exists a neural network F : R → R with width 4, depth 3m + 2
and bit complexity b +dlog(R)] such that for every i ∈ [N ] we have that F (Xi) = Wd 春].
Proof. Let j ∈	[m].	We define network blocks	Fj	: R → R and	Fj	:	R2	→	R2	in
the following way: First, we use Lemma A.6 to construct Fj such that Fj (X) = 1 for every
X ∈ [bxj∙k-k+ιC, bxj∙k + 1C], and Fj(x) = 0 for x < [xj∙∙k-k+ιC — 2 or x > [xj/ + 11 + 1. In
particular, Fj(Xi) = I if i ∈ [j ∙ k — k +1,j ∙ k], and Fj(Xi) = 0 otherwise. Note that since k is
defined with ceil, it is possible that j ∙ k ≥ N, if this is the case we replace j ∙ k with N. Next, we
define:
Fj((X)) = (y + w： Fj (j .
Finally we define the network F(X)
Fm ◦・•.◦ Fl
X0	(we can use one extra layer to
augment the input with an extra coordinate of 0).
We show that this construction is correct. Note that for every i ∈ [N], if we denote j = dk], then
Fj(Xi) = 1 and for every j0 = j we have Fjo(Xi) = 0. By the construction of F we get that
F (Xi) = Wd i e.
The width of F at every layer j is at most the width of Fj + 2, since we also keep copies of both
X and y, hence the width is at most 4. The depth of Fj is 2, and F is a composition of the Fj's
with an extra layer for the input to get X 7→ X0 , and an extra layer in the output to extract the
last coordinate. Hence, its depth is 3m + 2. The bit complexity is bounded by the sum of the bit
complexity of Fj and the weights Wj, hence it is bounded by b +dlog(R)].	□
A.3 S tage III: Bit Extraction from the Crafted Weights
Lemma A.5. Let ρ,n,c ∈ N. Let U ∈ N with LEN(u) = P ∙ n and let W ∈ N with
len(w) = C ∙ n. Assume that for any ', k ∈ {0,1,...,n — 1} with ' = k we have that
∣BINρ,'+Lρ∙('+i)(u) — BINρ∙k+Lρ,(k+i)(u)∣ ≥ 2. Then, there exists a network F : R3 → R with
width 12, depth 3n ∙ max{ρ, c} + 2n + 2 and bit complexity n max{ρ, c} + 2, such that for eve^y
X > 0, ifthere exist j ∈ {0,1,...,n — 1} where [x] = BINp.j+i：p,(j+i)(u), then:
13
Published as a conference paper at ICLR 2022
Proof. Let i ∈ {0,1,...,n -1}, and denote by 夕(z) := σ(σ(2z) - σ(4z - 2)) the triangle function
due to Telgarsky (2016). We construct the following neural network Fi :
Fi :
x
…(2nuρ +
…(2nuρ +
d'" 2Wc +
八(⑹(2Wc +
y
∖
1	)
2n∙p+1 )
1	)
2n∙ρ+2 )
1	)
2n∙c + 1 )
1	)
2n∙c + 2 J
/
7→
x
*1)∙ρ) (2nuρ +
*1)∙ρ)(2nuρ +
d(i+1),C)(养C +
*1)© ( 2Wc +
y+yi
1	)
2n∙p+1 )
1	)
2n"+2 )
1	)
2n∙c + 1 )
1	)
2n∙c + 2 J
/
where we define y，：= 81由.C+1@+1)./*) if X ∈ 但1由.°+1e+1).0("), BlNi.p+1：(i+1).p(u) + 1], and
yi = 0 if x > BINi∙ρ+L(i+1)∙ρ(U) + 3 or χ < BINi.ρ+L(i+1)∙ρ(u) - 1.
The construction uses two basic building blocks: The first is using Lemma A.7 twice, for u and w.
This way we construct two smaller networks Fiw , Fiu, such that:
1
2n∙ p+1
1
2n∙ p+2
7→
Fiu :
1
2n∙ c+1
1
2n∙ c+2
7→
夕(Ci+1>P) ( 2u- + 2n∙p+1 )∖
夕(Ci+1> P) ( 2uu- + 2n∙P+2 ) I
,BINi∙ρ+L(i+1)∙P(U))
夕((i+1)∙c)(露+4八
g((i+1)∙c) (2w-c + 2n.：十2 ) I
、BINi∙c+L(i+1)∙c(W))
The second is to construct y%. We use Lemma A.8 with inputs X and BINi∙ρ+1<i+1)∙ρ(u), and denote
the output of this network by y%. We use the following 1-layer network:
(BINi∙c+1yi+1)∙c(w)) → σ (yi ∙ 2c+1 - 2c+1 + BINi∙c+L(i+1)∙c(W))
∖	∙ C+-L：(<I+-L) C ∖
Note that if yi = 1 then the output of the above network is BINi∙c+1‹i+1)∙c(w), and if y% = 0 then
the output of the network is 0 since BINi∙c+1‹i+1)∙c(w) ≤ 2c.
The last layer of Fi is just adding the output yi to y, and using the identity on the other coordinates
which are being kept as the output, while the other coordinates (namely, the last output coordinates
of Fiw and Fiu) do not appear in the output of Fi . Also note that all the inputs and outputs of the
networks and basic building blocks are positive, hence σ acts as the identity on them. This means
that if Fiu is deeper than Fiw (or the other way around), then we can just add identity layers to Fiw
which do not change the output, but make the depths of both networks equal. Finally we define:
F := G ◦ Fn-1 ◦ ∙ ∙ ∙ ◦ F0 ◦ H ,
where: (1) G : R5 → R is a 1-layer network that outputs the last coordinate of the input, and; (2)
H : R3 → R6 is a 1-layer network such that:
X
ɪ + -
2n∙p 1 '
-ɪ + :
2n∙ p 1 '
-W- + :
2n∙c I ,
-W- + :
2n∙c I ,
0
2n∙ p+1
2n∙ p+2
2n∙ c+1
2n∙ c+2
1
∖
/
where we assume that X, W, U > 0.
We show the correctness of the construction.
We have that F
As-
sume there exists j ∈ {0,1...,n - 1} such that [x] = BINPj+1p(j+1)(u). Then, by the
construction of yi we have that y7- = BINCj+±c,(j+1)(w), and for every other ` = j, since
∣BINρ∙'+Lρ∙('+1)(u) - BINPj+Lρ∙(j+1)(u)∣ ≥ 2, we must have that X > BINp.'+1：p.('+1)(u) + 3 or
x < BINP∙'+1p('+1)(u) - 2, which means that y' = 0. In total we get the the output of F is equal
to Pn-I yi = yj = BINCj+1：c.(j+1)(w) as required.
14
Published as a conference paper at ICLR 2022
We will now calculate the width, depth and bit complexity of the network F . The width of each
Fiw and Fiu is equal to 5 by Lemma A.7, and we need two more neurons for x and y. In total, the
width is bounded by 12. Note that all other parts of the network (i.e. H, G and the network from
Lemma A.8) require less width than this bound. For the depth, by Lemma A.7, the depth of each
FiU and Fw is at most 3 ∙ max{ρ, c}, adding the construction from Lemma A.8,the depth of each Fi
is at most 3 ∙ max{ρ, c} + 2. Summing over all i, and adding the depth of G and H We get that the
depth of F is bounded by 3n ∙ max{ρ, c} + 2n + 2. Finally, the bit complexity of Fu, Fiw and H is
bounded by n max{ρ, c} + 2, and all other parts of the netWork require less bit complexity. Hence,
the bit complexity of F is bounded by n ∙ max{ρ, c} + 2.	□
A.4 Proof of Theorem 3.1
The construction is done in three phases. We first project the points onto a 1-dimensional subspace,
where the projection preserves distances UP to some error. The second step is to split the points into
，N log(N) subsets, and to extract two weights containing “hints” about the points and their labels.
The third step is to parse the hints using an efficient bit extraction method, and to output the correct
label for each point.
Throughout the proof we assume w.l.o.g. that the following terms are integers since we use them
as indices: PNlog(N), Jlog(N), log(R),
log(C). If any of them is not an integer we can just
replace it with its ceil (i.e. dlog(R)e instead of log(R)). This replacement changes these numbers
by at most 1, which in turn can only increase the number of parameters or bit complexity by at most
a constant factor. Since we give the result using the O(∙) notation, this does not change it. Also,
note that the width of the network is independent of such terms.
For the first stage, we use Lemma A.2 to construct a network F1 : Rd → R such that F1 (xi) ≤
10rN2δ-1√πd for every i ∈ [N] and ∣Fι(xi) — FI(Xj)| ≥ 2 for every i = j.
We denote R := 10rN2δ-1 √∏d, and we denote the output of Fi on the samples xι,..., XN ∈ Rd
as x1 , . . . , xN ∈ R for simplicity. We also assume that the xi’s are in increasing order, otherwise
we reorder the indices. This concludes the first stage.
For the second stage, we define two sets of integers w1 , . . . , w
√N log(N)
each represented by
N
log(N)
• log(C) bits, and ui,..., u√N0g(N) each represented by
N
log(N)
• log(R) bits, in the


following way: For every i ∈ [N] let j :
i • JloNN) and k := i (m°d qIogNN)). We set:
BINk∙log(C) + L(k + 1)∙log(C)(Wj ) = Ji
BINk∙log(R)+L(k+1)∙log(R)(Uj) = bxic .
We now use Lemma A.4 twice to construct two networks F2w : R → R and F2u : R → R such that
Fw(Xi) = Wji and FU(xi) = Uji for ji = i • JIOgNN) . We construct the network F? : R → R to
be a concatenation of the two networks, with an additional coordinate which outputs σ(x) (since the
inputs xi are positive, this coordinate just output the exact input). Namely, we construct a network
such that for every i ∈ [N] we have
F2(xi) = Wji	,
Uji
where ji = i • qog(N) .
0, if there exist j ∈ < 0, 1,..., JIOg(N)
For the third stage, we use Lemma A.5 to construct a network F3 : R3 → R such that for every x >
—1} such that bxc = BINIOg(R)∙j + Llog(R)∙(j+1)(U),
then:
F3
BINIOg(C)∙j+Llog(C)∙(j + 1) (W).
15
Published as a conference paper at ICLR 2022
Finally, we construct the network F : Rd → R as F(x) = F3 ◦ F2 ◦ F1(x).
We show the correctness of the construction. Let i ∈ [N], and let y := F (xi). By the con-
StrUction of F? and the numbers w1,..., w√Nog(N), uι,..., u√Nθg(N), if We denote j :
i ∙"
F	r--ʌ	(FI(Xi)∖
and k := i (mod JlogNN)b then We have that: (1) F2 ◦ FI(Xi) =	Wj	； (2)
BINlog(R)∙k + Llog(R)∙(k+1) (Uj) = bFI(XiH ; and ⑶ BINlog(C)∙k+Llog(C)∙(k + 1) (Wj) = Ji∙ Finally,
by the construction of F3 we get that y = F3 ◦ F2 ◦ FI(Xi) = BINlog(C).k+i：log(c).(k+i)(wj) = y
as required.
The Width of the netWork F, namely the maximal Width of its subnetWorks, is the Width
of F3, Which is 12. The depth of F is the sum of the depths of each of its subnet-
works. The depth of Fi is 2, the depth of F? is O (PNIog(N7) and the depth of F3
is O( Jlog(N) ∙ max {log(R), log(C)}). Hence, the total depth of F can be bounded by
O (PN Iog(N)+q logNN) ∙mχ {Iog(R), lOg(C)}).
The bit complexity of F is the maximal bit complexity of its subnetworks. The
bit complexity of Fi is log (3drN2√πδ-1)	=	log(d) + log(R∕3), the bit complex-
ity of F2 is O( Jlog(N) ∙ max{log(R), log(C)}) and the bit complexity of F3 is also
O (Jlog(N) ∙ max{log(R), log(C)}). In total, the bit complexity of F can be bounded by
O (log®+qIog(N7 ∙max{Iog(R), lOg(C)}).
A.5 Auxiliary Lemmas
Lemma A.6. Let a, b ∈ N with a < b. Then, there exists a neural network F with depth 2, width
2 and bit complexity LEN(b) such that F(x) = 1 for X ∈ [a, b] and F(x) = 0 for x > b + 1 or
x < a — 2.
Proof. Consider the following neural network:
F(x) = σ(1 - σ(2a - 2x)) + σ(1 - σ(2x - 2b)) - 1 .
It is easy to see that this networks satisfies the requirements. Also, its bit complexity is at most
LEN(b), since a < b, hence a can be represented by at most LEN(b) bits.	□
Lemma A.7. Let n ∈ N and let i,j ∈ N with i < j ≤ n. Denote Telgarsky’s triangle function by
夕(z) := σ(σ(2z) 一 σ(4z — 2)). Then, there exists a neural network F : R2 → R3 with width 5,
depth 3(j - i + 1), and bit complexity n + 2, such that for any x ∈ N with LEN(x) ≤ n, if the input
…G-I) (2t+⅛
(2(j) (2χn + 2n+ι)
,then it outputs:	夕(j)(告 + 2⅛)
BINi:j(x)
Proof. We use Telgarsky’s function to extract bits. Let x ∈ N with LEN(x) = n, and let i ∈ N with
i ≤ n. Then, we have that:
BINi(X) = 2n+2-iσ /i)
(4)
The intuition behind Eq. (4) is the following: The function 夕(i) is a piecewise linear function with
2i-i ”bumps”. Each such ”bump” consists of two linear parts with a slope of 2i, the first linear part
goes from 0 to 1, and the second goes from 1 to 0. Let x ∈ N with at most n bits in its binary
representation. It can be seen that the i-th bit of X is 1 if 夕(i) (ɪ + 2⅛) is on the second linear
part (i.e. descending from 1 to 0) and its i-th bit is 0 otherwise. This shows that 夕(i)(告 + 2⅛∙)—
夕(i) (2χn + 2n⅛ι) is equal to 2i-n-2 if the i-th bit of X is 1, and this expression is negative otherwise.
The correctness of Eq. (4) follows.
16
Published as a conference paper at ICLR 2022
Let j, i ∈ N with i < j and denote c := j - i. Using the construction above as a building block,
we construct a network which outputs BINi:j (x) in the following way: For ` ∈ {0, 1, . . . , c} define
(fi-1+')( 2n + 2n+1 )∖
f` : R3 → R3 to be the neural network, such that for an input 夕(i-1+')(2n + 2n+2), itoutPuts
y
(2(i+') (2n + 2n+1 )ʌ
^(i+') (2n + 2⅛) . The network f` is obtained by composing the first two coordinates with
∖y + 2c-' BINi+'(x)/
TeIgarsky's function 夕,and then the last coordinate is calculated using Eq. (4). Finally, we define:
F := Fc ◦…。F0, and we augment the input of F0, such that its last coordinate is zero. The output
of F is as required since
c
X 2c-'BINi+'(x) = BINi：i+c(x) = BINij(x).
'=0
Finally, we compute the width, depth and bit complexity of F . Its width is bounded by twice the
width of 夕(which is 2), plus an extra neuron for computing the output y, hence its width is bounded
by 5. Its depth is equal to 3(j - i + 1), since we need two layers to compute 夕 and an extra layer
to compute the output y . Finally, the bit complexity is bounded by n + 2, since the largest weight in
the network is at most 2n+2.	□
Lemma A.8. There exists a network F : R2 → R with width 2 depth 2 and bit complexity 2 such
that F	xy
1 ifx ∈ [y, y + 1] and F
0 if X > y + 3 or x<y — 11.
Proof. We consider the following neural network:
F y = σ(1 - σ(2y - 2x)) + σ(1 - σ(2x - 2y - 2)) - 1 .
Itis easy to see that this network satisfies the requirements. It has width 2, depth 2 and bit complexity
2.	□
B Proof from section 6
Proof of Theorem 6.2. We first use Lemma A.2 to construct a network H : Rd → R in the same
manner of the construction of F1 is the first stage of Theorem 3.1. This is a 2-layer network with
width 1 and bit complexity of O(log(R)). We denote the output of H on the samples x1, . . . , xN
as x1, . . . , xN. Note that by the construction |xi| ≤ O(R) for every i ∈ [N] and |xi - xj | ≥ 2 for
every i 6= j .
We now split the inputs to B subsets of size B2 each, we denote these subsets as Iι,..., IN (We
B	B2
assume w.l.o.g. that B is an integer, otherwise we can replace it with ∣-B2]). For each subset Ik we
use Theorem 3.1 to construct a network Fk which memorizes the points in Ik, with the following
changes: (1) There is an additional coordinate which memorizes the input and output as is; and
(2) The output of the network is added to the output of the network for the previous subset k - 1.
For k = 1, we set this coordinate to be zero. That is, if xi ∈ Ik then Fk
otherwise Fk	xyi	= xyi.
Finally, we construct the network F : Rd → R as:
F = G ◦ F N ◦•••◦ Fi ◦ H,
B2
where G	xy	= y.
By the construction of each Fk from Theorem 3.1, for every x ∈ R, if |x - xi| ≥ 2 for every i ∈ Ik,
then Fk (x) = 0. For every i ∈ [N], there is exactly one k ∈ [N/B2] such that i ∈ Ik. Using the
17
Published as a conference paper at ICLR 2022
projection H, for this k we get that Fk
y +xiy , and for every ` 6= k we get that
F'
xyi . This means that F(xi) = yi for every i ∈ [N].
By Lemma A.4 and Lemma A.5, since each Fk is used to memorize B2 samples, then its bit com-
Plexity is bounded by O (BPlog(B) ∙ log(R)) bits, hence this is also the bit complexity of F.
The depth of each component Fk is O (BPlog(B) ∙ log(R)), and there are B such components.
Hence, the depth of the network F is O (N√Bg(B) log(R)). The width of F is bounded by the
width of each component, which is O ⑴.	□
18