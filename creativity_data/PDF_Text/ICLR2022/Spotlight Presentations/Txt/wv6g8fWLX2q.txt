Published as a conference paper at ICLR 2022
TAMP-S2GCNets: Coupling Time-Aware Mul-
tipersistence Knowledge Representation with
Spatio-Supra Graph Convolutional Networks
for Time-Series Forecasting
Yuzhou Chen1,3 Ignacio Segovia-Dominguez2,4 Baris Coskunuzer2 Yulia R. Gel2,5
1 Department of Electrical and Computer Engineering, Princeton University
2Department of Mathematical Sciences, University of Texas at Dallas
3Lawrence Berkeley National Laboratory
4Jet Propulsion Laboratory, Caltech
5National Science Foundation
yc0774@princeton.edu
{Ignacio.SegoviaDominguez, coskunuz, ygl}@utdallas.edu
Ab stract
Graph Neural Networks (GNNs) are proven to be a powerful machinery for learn-
ing complex dependencies in multivariate spatio-temporal processes. However,
most existing GNNs have inherently static architectures, and as a result, do not ex-
plicitly account for time dependencies of the encoded knowledge and are limited
in their ability to simultaneously infer latent time-conditioned relations among en-
tities. We postulate that such hidden time-conditioned properties may be captured
by the tools of multipersistence, i.e., a emerging machinery in topological data
analysis which allows us to quantify dynamics of the data shape along multiple
geometric dimensions. We make the first step toward integrating the two rising
research directions, that is, time-aware deep learning and multipersistence, and
propose a new model, Time-Aware Multipersistence Spatio-Supra Graph Convo-
lutional Network (TAMP-S2GCNets). We summarize inherent time-conditioned
topological properties of the data as time-aware multipersistence EUler-Poincare
surface and prove its stability. We then construct a supragraph convolution mod-
ule which simultaneously accounts for the extracted intra- and inter-dependencies
in the data. Our extensive experiments on highway traffic flow, Ethereum token
prices, and COVID-19 hospitalizations demonstrate that TAMP-S2GCNets out-
performs the state-of-the-art tools in multivariate time series forecasting tasks.
1	Introduction
Multivariate time series forecasting plays an integral role in virtually every aspect of societal func-
tioning, from biosurveillance to financial analytics to intelligent transportation solutions. In the last
few years, Graph Convolutional Networks (GCNs) have emerged as a powerful alternative to more
conventional time series predictive models. Despite their proven success, GCNs tend to be limited
in their ability to simultaneously infer latent temporal relations among entities (such as correlations
both within a time series and in-between time series or the joint spatio-temporal dependencies).
More generally, most existing GCNs architectures are inherently static and as such, do not explic-
itly account for time-conditioned properties of the encoded knowledge about the complex dynamic
phenomena.
At the same time, recent studies on integrating shape properties of the complex data into deep learn-
ing (DL) models indicate that topological representations, obtained using the tools of single param-
eter persistence, can bring an invaluable insight into the system organization and enhance the result-
ing graph learning mechanisms (Hofer et al., 2019; Carriere et al., 2020; Carlsson & Gabrielsson,
2020; Horn et al., 2021). (By shape here we broadly understand data properties which are invariant
under continuous deformations, e.g., stretching, bending, and twisting). However, in many appli-
1
Published as a conference paper at ICLR 2022
cations, particularly, involving spatio-temporal processes, the data exhibit richer structures which
cannot be well encoded with a single parameter persistence. We postulate that many critical hid-
den time-conditioned interrelations which are inaccessible with other methods can be captured by
the emerging machinery of multipersistence. Multipersistence, or multiparameter persistence (MP)
generalizes the notion of single parameter persistence to a case when dynamics of the inherent data
shape is discerned along multiple geometric dimensions (Carlsson & Zomorodian, 2009). Despite
its premise, applications of MP in any discipline remain nascent at best (Riess & Hansen, 2020;
Kerber, 2020).
We make the first step on a path of bridging the two emerging directions, namely, time-aware DL
with time-conditioned MP representations of complex dynamic phenomena. By time-conditioned
MP representations, we mean the most salient topological properties of the data that manifest them-
selves over time. To summarize such time-conditioned topological properties, we first introduce a
dynamic EUler-Poincare surface as a new MP summary. We then propose a directed multilayer supra
graph abstraction to represent a sequence of time-varying objects and develop a supragraph convolu-
tion module which allows us to simultaneously learn co-evolving intra- and inter-dependencies (i.e.,
spatial and temporal correlations) in the complex high-dimensional data.
The key novelty of this paper are summarized as follows:
•	This is the first work to bridge the concepts of MP with the time-aware learning paradigm. Appli-
cations of MP in any field of study are currently nascent.
•	We introduce a new time-aware multipersistence invariant, a dynamic Euler-Poincare surface. We
prove its stability and show its substantial computational gains and high utility for encoding the
time-conditioned knowledge.
•	We propose a mathematical abstraction of directed multilayer supra graph for time-conditioned
knowledge representation and construct a new Time-Aware Multipersistence Spatio-Supra Graph
Convolutional Network (TAMP-S2GCNets) which simultaneously learns latent temporal inter-
and intra-relations among entities in the complex high-dimensional data.
•	We perform expansive forecasting experiments, in application to highway traffic flow, Ethereum
token prices, and COVID-19 hospitalizations. Our findings demonstrate superior predictive per-
formance, versatility and computational efficiency of TAMP-S2GCNets, compared to the state-
of-the-art methods in multivariate time series forecasting.
2	Related Works
Multipersistence Despite that MP demonstrates very promising results in terms of improving ac-
curacy, tractability and robustness, applications of MP in ML are virtually non-existent (Wright &
Zheng, 2020; Riess & Hansen, 2020; Kerber, 2020). Some notable efforts in the direction to develop
MP summaries which are suitable for integration with ML models include Multiparameter Persis-
tence Kernel of Corbet et al. (2019), Multiparameter Persistence Landscapes (MP-L) of Vipond
(2020), and Multiparameter Persistence Images (MP-I) of Carriere & Blumberg (2020) which are
based on the concept of slicing, that is, restricting the MP module to an affine line (or single parame-
ter persistence) (Cerri et al., 2013; Landi, 2014). Such slicing methods enjoy a number of important
stability guarantees but tend to be computationally expensive even in static scenarios, which makes
them infeasible for time series forecasting tasks. In turn, the most recent results of Beltramo et al.
(2021) (i.e., Euler Characteristic Surfaces) and Coskunuzer et al. (2021) (i.e., Multiparameter Persis-
tence Grids) introduce pointwise MP representations, in application to static point clouds and graphs.
Such pointwise representations are weaker invariants but are substantially more computationally ef-
ficient. Integration of pointwise representation with ML models has not been yet investigated. Here
we propose the first time-aware pointwise MP representation, a dynamic Euler-Poincare surface,
derive its theoretical properties and integrate it with GCN in time series forecasting tasks.
Spatial-Temporal Graph Models and Forecasting Recent studies (Li et al., 2018; Yu et al., 2018;
Yao et al., 2018) introduce graph convolution methods into spatio-temporal networks for multivari-
ate time series forecasting which, as a result, allows for better modeling of dependencies among
entities (Wu et al., 2019; Bai et al., 2020; Cao et al., 2020) and handling data heterogeneity. Despite
the GCN successes, designs of the existing spatial-temporal GCNs largely rely on the pre-defined
graph structures. As such, GCNs are restricted in their ability to explicitly integrate time dimen-
sion into the knowledge representation and learning mechanisms, thereby, limiting model adaptivity
2
Published as a conference paper at ICLR 2022
to the dynamic environments and requiring more frequent retraining. Most recently, Chen et al.
(2021) propose a time-aware GCN, Z-GCNETs, for time series forecasting which integrates zigzag
persistence images based on a single filtration, as the primary time-conditioned topological repre-
sentation. In general, the zigzag concept can be combined with MP, but it requires more fundamental
advances in the theory of algebraic topology. As such, our time-aware MP learning approach may
be viewed as complementary to zigzag persistence, while considering time-changing connections of
graph structures in dynamic networks.
3	Time-Aware Multipersistence Euler Characteristic Surfaces
Spatio-Temporal Graph Construction We define a spatial network at time step t as Gt =
(Vt, Et, At, Xt), where Vt is a set of nodes and Et is a set of edges. We let |Vt| = N and |Et| = Mt.
The adjacency matrix At ∈ RN×N, and Xt = {xt,1, xt,2, . . . , xt,N}> ∈ RN×FN is the node
feature matrix with feature dimension FN . To construct the spatial network Gt, we can build the
adjacency matrix At based on (i) the prior knowledge of graph structure: first-order neighbours, i.e.,
At,uv = 1 if the node u and node v have a connection in the dynamic graph at the time step t; and
(ii) the Radial Basis Function (RBF): degrees of similarity between instances (i.e., nodes) in Xt,
i.e., At,uv = 1eχp (-∣∣χt,u-χt,v∣∣2∕γ)≤e, where Y denotes the length scale parameter and E denotes the
threshold parameter filte, rs no, isy edges. Let T be the total number of time steps. Given a sequence of
observations on a multivariate variable, X = {X1, X2, . . . , XT} ∈ RN ×FN ×T with T timestamps
and FN node attributes, we construct spatio-temporal networks G = {G1 , G2, . . . , GT} via either
prior knowledge of network structure or applying RBF to the node feature matrix.
Single Parameter Persistence Persistent homology (PH) based on one parameter discerns shape
of the complex data along a single geometric dimension. The goal is to select some suitable pa-
rameter of interest and then to study a graph Gt not as a single object, but as a sequence of nested
subgraphs, or a graph filtration Gt1 ⊆ Gt2 . . . ⊆ Gtm = Gt, induced by this evolving scale pa-
rameter. Armed with such filtration, we can then assess which structural patterns (e.g., loops and
cavities) appear/disappear and record their lifespans. To make the counting process more efficient
and systematic, we build a simplicial complex Cit from each subgraph Gti , resulting in a filtration
Ct1 ⊆ Ct2 . . . ⊆ Ctm (e.g., clique complexes). For example, to construct such filtration, a common
method is to consider a filtering function f : Vt 7→ R and the corresponding increasing set of thresh-
olds {αi }1m such that Cit = {∆ ∈ Ct : maxv∈∆ f (v) ≤ αi }. The resulting construction is called a
sublevel set filtration of f, and f can be selected, for instance, as degree, centrality, or eccentricity
function (Hofer et al., 2020; Cai & Wang, 2020). Similarly, f can be defined on the set of edges Et.
More details on a single filtration PH can be found in Appendix B.
Time-Aware Multiparameter Persistence (TAMP) Data in many applications, particularly, in-
volving spatio-temporal modeling, might be naturally indexed by multiple parameters, e.g., real
time traffic flow and optimal route in urban transportation analytics. Alternatively, the primary fo-
cus might be on discerning shape properties of the complex data along multiple dimensions. For
instance, to better predict cryptocurrency prices and manage cryptomarket investment performance,
we may need to evaluate structural patterns in cryptoasset dynamics not along one dimension but
simultaneously along the volume of transactions and transaction graph betweenness, as a measure
of joint perception of the cryptomarket volatility among the key investors. Such multidimensional
analysis of topological and geometric properties can be addressed using generalization of PH based
on a single filtration to a multifiltration case. That is, the MP idea is to simultaneously assess shape
characteristics of Gt based on a multivariate filtering function F : Vt 7→ Rd. As a result, e.g.,
for d = 2 and a set of nondecreasing thresholds {αi}1m and {βj }1n, instead of a single filtration
of complexes, we get a bifiltration of complexes {Ctαi ,βj | 1 ≤ i ≤ m, 1 ≤ j ≤ n} such that if
βk < βl, then Ctαi,βk ,→ Ctαi,βl and if αi < αj, then Ctαi,βk ,→ Ctαj,βk. Finally, this bifiltra-
tion induces a bigraded MP module {Hk(Ctαi,βk)}, where Hk is the kth homology group. Inspired
by Beltramo et al. (2021); Coskunuzer et al. (2021), we propose a new time-aware MP summary,
namely, a Dynamic Euler-Poincare Surface.
Definition 3.1 (Dynamic EUIer-Poincare Surface). Let {Gt}T=ι be a series of time-varying graphs.
Let F = (f, g) be a multivariate filtering function F : Vt 7→ R2 with thresholds I = {(αi , βj) |
1 ≤ i ≤ m, 1 ≤ j ≤ n}. Let Ctαi ,βj be the clique complex of the induced subgraph Gtαi ,βj =
F-1((-∞, ai] X (-∞, βj]), t = 1,2,... ,T and X be the Euler-Poincare characteristic. Then, a
3
Published as a conference paper at ICLR 2022
sequence of time-evolving m × n-matrices {Et}tT=1 such that Eitj = χ(Ctαi,βj ) for 1 ≤ i ≤ m,
1 ≤ j ≤ n is called Dynamic Euler-Poincare Surface (DEPS).(FigUre 2 in Appendix C shows a toy
example how DEPS is computed.)
Theoretical Guarantees of DEPS Consider two graphs G+ and G-, where time index t is sUp-
pressed for brevity. Let F : V ± 7→ R2 be a mUltivariate filtering fUnction with thresholds
I = {(αi, βj) | 1 ≤ i ≤ m, 1 ≤ j ≤ n}. Let C± be the cliqUe complexes of G±, and let
Cb ± = {Ci+j } be the bifiltration indUced by (C± , F, I) as before. Let E± be the corresponding
Euler-Poincare Surfaces (i.e., m X nmatrices). Then, SetkE+ -E-k1,1 = Pm=I P；=i 旧+ - EjI
as the distance between E+ and E-, where ∣∣ ∙ ∣∣1,1 is the vectorized Li matrix norm.
We now introduce an L1-based MP metric instead of using L∞ -based metrics like, e.g., matching or
interleaving, due to the nature of our summaries E± (see Remark C.1 in Appendix C). Let Dkf (C±)
and Dkg(C±) be the kth single parameter persistence diagrams (PDs) of C± for filtrations induced
by functions f,g : V± → R, respectively (see Appendix B). Let C±c and C±∙ be clique complexes
corresponding to G± = f-1((-∞, αi]) and G± = g-1((-∞,βj]). Define the ith column dis-
tance for the kth PDs as D,*(C+, C-) = Wi(Dg(C±), Dg(C-J), where Wi is the Wasserstein-I
distance. Similarly, the jth row distance for kth PDs is Dkj(C+, C-) = WI(Df (C+j), Df (C-∙)).
Definition 3.2 (Weak Li -metric for Multipersistence). The weak Li -metric between Cb± is
D(Cb+,Cb-) = max Dc(Cb+,Cb-),Dr(Cb+,Cb-) ,
s	.t. Dc(C+, C-)= PM=o PZi Dg.(C+, C-) and Dr(C+, C-) = PM P；= Dkj(C+, C-).
Now, We can state our stability result for Euler-Poincare Surfaces.
Theorem 3.1. Let G±, F, C±, E± be as defined above. Then, the Euler-Poincare Surfaces are stable
with respect to the weak Li -metric, i.e., ∣E+ — E-ki,i ≤ C ∙ D(C+, C-) for some C > 0.
The proof of the theorem is given in Appendix C. This stability result implies that the distances
between multiparameter PDs control the distance between the resulting Euler-Poincare Surfaces.
By combining with the stability result for PDs (Cohen-Steiner et al., 2007), one can conclude that
the small changes in the MP filtering function F : Vt 7→ Rd or the small changes in the input data
can result only in a small change in DEPS surfaces. For further discussion on implications of the
stability result, please refer to Remark C.2.
4 Time-Aware Multipersistence Spatio-Supra Graph
Convolutional Networks
Given T historical observations XT = {Xt-T , Xt-T+i, . . . , Xt-i}, the multivariate forecast-
ing model F(∙) is learned to predict future observations in the next H + 1 time steps, i.e.,
{Xt, Xt+i, . . . , Xt+H} = F(Xt-T , Xt-T+i, . . . , Xt-i).
Graph Learning Architecture The graph representation learning of our MPS2GCNets is build
upon GCN. To learn node representation from graph topology and node features, the input of GCN-
based approaches contains the adjacency matrix At of the original input graph Gt and the node
feature matrix Xt. However, the prior knowledge of graph structure (1) might restrict the modeling
capacity (i.e., graph edges cannot encode the complex relationships between nodes) and (2) leads to
the neglect of neighboring information with high diversity. To avoid these limitations, inspired by
the adaptive dependency matrix (Wu et al., 2019; Chen et al., 2021), we learn the normalized self-
adaptive adjacency matrix S = {suv}N ×N with the pre-defined “cost” staying in the same node
based on the learnable node embedding Eφ = (ei,φ, e2,φ, . . . , eN,φ)> ∈ RN×dc as
suv
__________exp(ReLU(eu,6e>,6))__________
Pv∈V∖{u} exp(ReLU(eu,φe>,φ))+eχp(duu)
__________________duu__________________
Pv∈V∖{u} exp(ReLU(eu,φe>φ)) + exp(duu)
u 6= v,
u = v.
4
Published as a conference paper at ICLR 2022
Here hyperparameter duu is the “cost” of staying in the same node U and ReLU(∙) = max(0, ∙)
is an activation function, which guarantees suv ≥ 0. Since Pv suv = 1 and suv ≥ 0, we can
use S as the normalized Laplacian. Towards more effective and robust learning of both spatial and
spectral characteristics, we represent the graph diffusion as a matrix power series. That is, let S
be an N × N × K Laplacian tensor containing the power series {I, S, . . . , SK-1} of S, where
I ∈ RN×N is the identity matrix and K ≥ 2.
Spatio-Temporal Feature Transformation Note that spatial and temporal domains contain inter-
dependent but highly heterogeneous types of information. Inspired by statistical factor analysis, we
facilitate signal extraction from these disparate informational sources, by mapping the original input
feature space into the high-level latent feature space, which can be written as follows
H('+1) = (XT EφΘ* )>U,	⑴
where Θ* ∈ Rdc × P×QFT and U ∈ RT represent the learnable parameters.
4.1 Supragraph Convolutional Module in Multilayer Supra Graph
We propose a novel supragraph convolutional module to simultaneously capture spatio-temporal
dependencies in dynamic networks. Our key approach is (i) to represent a sequence of time-varying
graphs recorded over a sliding time window, as a multilayer supra graph, instead of treating each
graph snapshot individually, and (ii) to learn the resulting multilayer supra graph with the random
walk exploration which allows us to encode the key details of the time-conditioned relationships
among nodes and to boost graph convolutions over multiple edge sets.
Sliding Window Historical Data as Multilayer Supra Graph Network To increase expres-
sive capability of spatio-temporal representation learning, we propose a novel directed mul-
tilayer supra graph. Particularly, we treat each graph within a sliding window GT =
{Gt-T, Gt-T+1, . . . , Gt-1} ∈ RN×N×T as a layer within a directed T -layered network. (Here
N is the number of nodes in each graph.) Since information in real world spatio-temporal pro-
cesses can propagate only from past to present to future, we consider directed multilayer supra
graph as abstraction for dynamic knowledge representation. That is, we assume that the informa-
tion (e.g., spatial features) is shared between layers ta and tb, whenever tb > ta, and time stamps
ta, tb ∈ {t - T, . . . , t - 1}. More specifically, we propose a strategy to create such kind of infor-
mation propagation channels by adding directed virtual edges (i.e., interlayer edges) between every
node in the layer ta and its counterparts in other “future” layers tb (tb > ta).
Definition 4.1. A directed multilayer supra graph (DMSG) is a tuple, defined as: DMSG =
(Gt-T,…，Gt-ι, IMtatb),where Gta = (Vta, Eta), ta ∈ {t -T,...,t - 1} are network layers and
IMtatb (Identity Mapping) is an N × N matrix of node mappings, with IMitjatb : vita × vjtb 7→ [0, 1].
That is, IMitiatb is the identity mapping between node vi in the “past” layer ta and the ”future” layer
tb (tb > ta): IMitiatb = 1.
The corresponding (r-th power) supra-adjacency matrix W = {wuv}NT×NT for DMSG is then
((SUva )r,	ta = tb and |u - v|	mod N = 0,
wuabv =	(dtuauta)r,	ta = tb and |u - v|	mod N = 0,	(2)
IdUUb,	ta = tb and |u — v|	mod N = 0,
where (stuavta )r is the r-th power of Stata which encodes the r-th step of a random walk among
nodes u and v	in layer ta, (dtuauta )r is the “cost” of staying in the	same node u and in the same layer
ta after r random walk steps, and dtuautb is	the “cost” of jumping	from the current	node u in	layer ta
to node u in layer tb. Finally, the generalized (r-th power) supra-Laplacian LSup ∈ RNT×NT for
DMSG is then
/ D11I+(S∖∖)r	D12I	…	DITI	∖
0	D22I+(S22)r	…	D2TI	∖
LSup =	.	.	.	.	I	.	⑶
..	..	..	..
∖	0	0	…DTTI+(STT)r)
Supragraph Diffusion Convolutional Layer Armed with the (r-th power) supra-Laplacian in (3),
we now build a supragraph diffusion convolutional layer to encode both the graph structure and node
5
Published as a conference paper at ICLR 2022
features from the temporal domain. That is, embedded node features are updated by message passing
and aggregation via intralayer and interlayer diffusion. The supragraph diffusion convolutional layer
is formulated as
rr('+1)	rʃ(') ∖>	o(')
Hi,Sup = (LSupHi,Sup) EφΘSup,	(4)
where H(?up ∈ RNT×P and Hi'+P ∈ RNT×QSup are the input and output activations for layer ',
H(Sup = XT, and Θ(SUp ∈ Rdc × P×QSup are the learnable parameters. In this case, the diffusion
on layer ta can extend to a fraction of nodes and propagate information of layer tb by interaction.
4.2	Spatial Graph Convolutional Layer
To aggregate the features of each node with its multi-hops neighbourhoods to generate node embed-
dings, we now turn to constructing the spatial graph convolution via
where θSPa
and HSpa)
τr('+1)	(G ττ(') ∖>	Q(Z)
Hi,Spa = (SHi,Spa) EφΘSpa ,
∈ Rdc×K×P×QSpa is the trainable weight, S is the Laplacian tensor, HySpa
(5)
∈ RN ×P
∈ RN×QSpa
are the node representations at the `-th layer and (` + 1)-th layer, respec-
tively, and Hi(,0S)pa = Xi, i.e., the node features at time step i. To reduce computational costs, we use
weight sharing matrix factorization instead of matrix entry assignments. As such, we timely update
the latest state of variables with back propagation algorithm and reduce the risk of over-fitting.
4.3	Learning Time-Aware Multipersistence Representation of Topological
Features
To fully utilize information delivered by the MP representation mechanism, we propose a CNN-
inspired Time-Aware Multipersistence EUler-Poincare Surface Representation Learning (DEPSRL)
module which employs the CNN base model fθ (including convolutional and pooling layers) and
readout layer. In summary, the proposed DEPSRL module offers the following unique innova-
tions: (i) extracting features from the global information, (ii) learning the relationship from DEPS
sequences pre and post (i.e., treating the fixed-size sliding window as multi-channels), and (iii) ag-
gregating topological features to make a fixed size representation. The summarized output feature
of DEPSRL module is given by
Φi,TAMP =㊉(fGAP(fθι ({Ei}TJ),fGMP(fθ2 ({Ei}iT=1))),	(6)
where ∕gap(∙), ∕gmp(∙), and f⅛, j = {1, 2}, are global average pooling, global max pooling, and
j-th CNN based model, respectively, and ㊉ denotes concatenation, where the ∕gap(∙) generates
summarized feature for each channel. We find that ∕gmp(∙) strengthens the representation learning
of the time-conditioned multipersistence features. The matrix Φ('+AMP ∈ RQTAMP is the output
of the DEPSRL module. To further learn and fuse multiple latent representations form different
views (i.e., spatial information, spatio-temporal correlations, and persistent topological features),
we combine these embeddings to obtain the final embedding
Z(Zout) = F(HSIl), Υi'+1), H(,FT)).	⑺
Here Υ('+1) = (1/T P H(SUp) )Φ(T^AMP integrates global topological and global spatio-temporal
information (i.e., we reshape H (ZS+1) ∈ RNT×QSup to HG (ZS+1) ∈ RN ×T×QSup and then average
i,S up	i,S up
H('+P) over temporal dimension), F(∙, ∙, ∙) is a dimension-wise concatenation function on embed-
dings along the output dimension, and Pi∈{Spa,Sup,FT} Qi= Qout. DEPSRL also allows for input
ofE from different bifiltrations. For more details, please refer to Appendix D.2.
4.4	Modeling Spatio-Temporal Dynamics
To encode spatio-temporal correlations among time series and get hidden state of nodes at a future
timestamp, we put the final embedding Zi(,Zo+u1t ) into Gated Recurrent Units (GRU). The forward
6
Published as a conference paper at ICLR 2022
propagation equations of GRU are as follows
<i = ς (W< [Ξi-1, Zi,out] + b<),
ξ) = tanh (^WΞ [=i ® ξ)-1 , Zi,out] + bΞ),
=i = ς (W= [Ξi-1, Zi,out] +b=),
(8)
Ξi = <i Θ Ξi-ι + (1 - <i) Θ Ξi,
where ς(∙) is an activation function which is ReLU in our case, Θ is the elementwise product,
<i is the update gate and =i is the reset gate. b<, b=, bΞ, W<, W=, and Wξ are learnable
parameters. [Ξi-1, Zi,out] and Ξi are the input and output of GRU model, respectively. Now we
get Ξi containing the structure, spatio-temporal, and topological information.
Figure 1: TAMP-S2GCNets consists of 5 components: (I) Spatial graph convolutional layer on Gt
extracts spatial information at time t (see Eq. 5). (II) Feature transformation (FT) learns representa-
tion of the spatio-temporal data XT over a sliding window of size T (see Eq. 1). (III) Supragraph
convolutional module captures joint spatio-temporal dependencies in XT (see Eq. 4). (IV) Detailed
architecture of the DEPSRL module (see Eqs. 6 and 7), where the DEPS with different types of mul-
tifiltrations can be learned by a CNN base model and global pooling mechanism. Here DEPS allows
us to learn the intrinsic graph structure both across multiple geometric dimensions and across time.
(V) The learned spatio-temporal dependencies are passed into GRU layer (see Eq. 8) for multi-step
forecasting. Note that the whole model can be trained in an end-to-end fashion.
5	Experiments
Data Description We use three different types of datasets to examine the performance of the pro-
posed TAMP-S2GCNets on dynamic networks: (i) The transportation and traffic data of California
state from the freeway Performance Measurement System (PeMS). We use three well-studied traffic
networks from literature: PeMSD3, PeMSD4 and PeMSD8 (Chen et al., 2001), (ii) Digital transac-
tions between users in the Ethereum blockchain network. We extract dynamic networks from three
token assets: Bytom, Decentraland and Golem (Li et al., 2020), and (iii) The spread of coronavirus
disease COVID-19 at county-level in states of California (CA) and Texas (TX). More details on each
dataset and parameter settings are provided in Appendix D.1.
Experimental Settings We compare the presented TAMP-S2GCNets with 13 state-of-the-art base-
lines (see more details of baselines in Appendix D.2) on three evaluation metrics, i.e, Mean Absolute
Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE).
For transportation networks, following Cao et al. (2020), we (i) split PeMSD3, PeMSD4, and
PeMSD8 datasets into a training set (60%), validation set (20%), and test set (20%) in chronologi-
cal order and (ii) use the one hour historical data to the next 15 minutes data. Following Chen et al.
(2021), we (i) split Bytom, Decentraland, and Golem token networks into training set (80%) and test
set (20%) in chronological order and (ii) use 7 days historical data to predict future 7 days data. For
COVID-19 datasets, in our experiments, we use daily data of 11 months of 2020, from February 1 to
December 31, and split the graph signals into training set, first 80% of days (268 days), and test set,
last 20% of days (67 days). Further specifics on experimental setup are provided in Appendix D.2.
The best results are in bold font and the results with dotted underline are the best performance
achieved by the runner-ups. We also perform a one-sided two-sample t-test between the best result
7
Published as a conference paper at ICLR 2022
and the best performance achieved by the runner-up, where *, **, *** denote significant, statisti-
cally significant, highly statistically significant results, respectively. More detailed experiments on
all datasets are in Appendix E. The data and code implementation are available at https://www.
dropbox.com/sh/n0ajd5l0tdeyb80/AABGn-ejfV1YtRwjf_L0AOsNa?dl=0.
Computational Complexity Although, in general, MP is computationally costly, in our case, to
get the DEPS summary, we do not need to compute PDs as in other MP approaches, but only
Betti numbers of each filtration cell. In particular, while a computational cost for obtaining kth-
PD for a graph is O(M3k), where Mk is the number k-simplices (Otter et al., 2017), obtaining
Euler Characteristics by sparse matrix methods has computational complexity of O(M0 + M1 +
M2) (Edelsbrunner & Parsa, 2014).
5.1	Experimental Results
Transportation Traffic Flow Table 1 shows the performance comparison among 13 state-of-the-art
baselines on PeMSD3, PeMSD4, and PeMSD8 for multi-step traffic flow forecasting. Our TAMP-
S2GCNets consistently outperforms baseline models on all 3 datasets except for PeMSD3, which
underscore effectiveness of TAMP-S2GCNets in time series forecasting tasks. On PeMSD3, our
TAMP-S2GCNets achieves the best MAE and MAPE, and has an average 2.75% relative gain, com-
pared to the runner-up. The average relative gains of TAMP-S2GCNets over the runner-ups in MAE
and MAPE are 2.78% and 2.19% on the 3 datasets, respectively.
Ethereum Blockchain Prices Table 3 (left) summarizes the comparison results in MAPE on 3
Ethereum token networks (i.e., Bytom, Decentrland, and Golen). Table 3 indicates that TAMP-
S2GCNets is always better than baselines for all dynamic Ethereum networks. We find that, even
compared to the baseline which also integrates topological information (i.e., Z-GCNETs), TAMP-
S2GCNets is highly competitive, implying that the time-aware MP representation with DEPS is
capable of better encoding time-conditioned information than the zigzag idea based on one filtration.
COVID-19 Hospitalizations Table 2 shows results on 15-day ahead forecasting of COVID-19 hos-
pitalizations in the U.S. states of California (CA) and Texas (TX) at a county level basis (RMSE
is aggregated over each state). For the sake of room, here we only display the top runner models.
TAMP-S2GCNets yields significantly better forecasting performance, with relative gains of 1.5%-
24.5% (in CA) and 5.5%-46.7% (in TX).
Table 1: Forecasting performance on PeMSD3, PeMSD4 and PeMSD8, along with results on sta-
tistical significance. Standard deviations are suppressed for the sake of room and are reported in
Table 8 in Appendix E.
Model	PeMSD3			PeMSD4			PeMSD8		
	MAE	RMSE MAPE (%		) MAE	RMSE MAPE (%)		MAE	RMSE MAPE (%)	
FC-LSTM (Sutskever et al., 2014)	21.33	35.11	23.33	27.14	41.59	18.20	22.20	34.06	14.20
SFM (Zhang et al., 2017)	17.67	30.01	18.33	24.36	37.10	17.20	16.01	27.41	10.40
N-BEATS (Oreshkin et al., 2019)	18.45	31.23	18.35	25.56	39.90	17.18	19.48	28.32	13.50
DCRNN (Li et al., 2018)	18.18	30.31	18.91	24.70	38.12	17.12	17.86	27.83	11.45
LSTNet (Lai et al., 2018)	19.07	29.67	17.73	24.04	37.38	17.01	20.26	31.96	11.30
STGCN (Yu et al., 2018)	17.49	30.12	17.15	22.70	35.50	14.59	18.02	27.83	11.40
TCN (Bai et al., 2018)	18.23	25.04	19.44	26.31	36.11	15.62	15.93	25.69	16.50
DeepState (Rangapuram et al., 2018)	15.59	*20.21	18.69	26.50	33.00	15.40	19.34	27.18	16.00
GraphWaveNet (Wu et al., 2019)	19.85	32.94	19.31	26.85	39.70	17.29	19.13	28.16	12.68
DeepGLO (Sen et al., 2019)	17.25	23.25	19.27	25.45	35.90	12.20	15.12	25.22	13.20
AGCRN (Bai et al., 2020)	14.22	24.03	13.89	1.7...7..8.	29.17	11.79 		14.59	23.06	9.29
Z-GCNETs (Chen et al., 2021)	1.4	2.0.	25.29	1. .3...8.8.	1.8...0..5.	2. .9...0.8.	11.79 		14.52 		2. .3...0.0.	9.28
StemGNN (Cao et al., 2020)	1.4	3.2.	2. .1...6.4.	1. .6...2.4.	20.20	3. .1...8.3.	12.00	15.83	2. .4...9.3.	9...2.6..
TAMP-S2GCNets (ours)	13.91	23.77	13.40	17.58	**28.56	*11.01	13.77	**21.70	8.99
5.2	Ablation Studies
Contribution of TAMP-S2GCNets components We now conduct the ablation stud-
ies on PeMSD4 and PeMSD8 to evaluate contribution of different components of our
framework. (Ablation results on Golem and CA are in Table 9 in Appendix E.)
8
Published as a conference paper at ICLR 2022
We compare our TAMP-S2GCNets with 4
ablated variants, i.e., (i) TAMP-S2GCNets
without DEPSRL module (w/o DEPSRL
module), (ii) TAMP-S2GCNets without
spatial graph convolutional layer (w/o
GCNSpa), (iii) TAMP-S2GCNets with-
out supragraph convolutional module (w/o
GCNSup), and (iv) TAMP-S2GCNets with-
out FT (w/o FT). As Table 3 (right panel)
shows, ablating each of above causes the
Table 2: 15-day ahead forecasting results (RMSE) on
COVID-19 hospitalizations in CA and TX.
Model
CA
TX
DCRNN (Li et al., 2018)	492.10±2.96
AGCRN (Bai et al., 2020)	448.27±2.78
90.47±2.28
52.96±3.92
StemGNN (Cao et al., 2020) .3.7..7...2.5..±..3...9. 1. .5.1...0..0.±.. 2.6.0.
TAMP-S2GCNets(ours)	*371.60±2.68 *48.21±3.17
performance drops sharply in comparison with our full TAMP-S2GCNets model. Especially, on
PeMSD4, the supragraph convolutional module significantly improves the results as it simultane-
ously captures the spatial and temporal information. In addition, it is evident that DEPSRL mod-
ule enhances the topological information learning ability in spatio-temporal domain, i.e., TAMP-
S2GCNets outperforms TAMP-S2GCNets w/o DEPSRL module with an average relative gain
3.92% on RMSE over PeMSD4 and PeMSD8. Besides, as expected, taking the advantages of
GCNSpa and FT, we improve the performance via capturing the graph structure in the spatial di-
mension and consolidating the processing of spatio-temporal correlations between node attributes,
respectively.
Table 3: Forecasting performance (MAPE in %) on Ethereum networks (left panel) and the TAMP-
S2GCNets ablation study on PeMSD4 and PeMSD8 (right panel).
Model	Bytom	Decentraland	Golem	Architecture	MAE	RMSE	MAPE
	—			TAMP-S2GCNets	17.58	28.56	11.01
DCRNN (Li et al., 2018)	35.36±1.18	27.69±1.77	23.15±1.91	W/o DEPSRL	17.89	***29.99	*11.07
STGCN (Yu et al., 2018)	37.33±1.06	28.22±1.69	23.68±2.31	PeMSD4 W/o GCNSpa	**19.41	***30.90	***11.21
GraphWaveNet (Wu et al., 2019) AGCRN (Bai et al., 2020)	39.18±0.96 34.46±1.37	37.67±1.76 26.75±1.51	28.89±2.34 22.83±1.91	W/o GCNSup W/o FT	***20.61 *18.30	***31.82 ***29.65	***12.64 ***12.29
				TAMP-S2GCNets	13.77	21.70	8.99
							
Z-GCNETs (Chen et al., 2021)	.3.1...0..4.±..0...7..8.	2..3...8.1..±..2...4..3.	2..2...3.2. .±. .1...4.2.	W/o DEPSRL	***14.28	**22.39	9.32
StemGNN (Cao et al., 2020)	34.91±1.04	28.37±1.96	22.50±2.01	PeMSD8 W/o GCNSpa W/o GCNSup	14.16 13.99	**22.29 *21.91	9.40 9.07
							
TAMP-S2GCNets (ours)	*29.26±1.06	***19.89±1.49	**20.10±2.30	W/o FT	***14.36	***22.60	9.26
Table 4: MAPE (%) (standard deviation) of TAMP-S2GCNets with single- and multifiltrations
(left panel) and MAPE (%) and computation time for TAMP-S2GCNets with our DEPS, MP-I
of Carriere & BlUmberg (2020) and MP-L of ViPond (2020) (right panel).
TAMP-S2GCNets on Bytom (Left)			TAMP-S2GCNets on Bytom (Right)		
Single filtration	Multifiltration		MP summary	MAPE(%)	Running time (s)
-	Deg & Btwns: 30.02±1.05		MP-I (Carriere & Blumberg, 2020)	~33.13	401.80
Deg: 30.56±1.08	Btwns & PowerTrns: 29.26±0.96		MP-L (Vipond, 2020)	32.19	517.11
Btwns: 30.80±1.70	Btwns & PowerVol: 29.27±0.78		DEPSDeg & Btwns	30.02	47.67
PowerTrns: 31.04±1.90	Deg & PowerTrns: 29.86±1.05		DEPSDeg& PowerTrns	2. .9...8.6.	3. .9...4.6.
PowerVol: 30.79±1.61	Deg & PowerVol: 29.41±1.06		DEPSBtWnS & POWerTrns	29.26	29.84
Contribution of Single vs. Multifiltration Persistence We consider sublevel filtrations based on
node degree (Deg), betweenness (Btwns), and graPh diameter (Power filtration) with edge weights
indUced by nUmber of transactions (PowerTrns) and volUme of transactions (PowerVol). As Table 4
(left) shows, MP always oUtPerforms single filtrations by a significant margin, both in terms of
Prediction accUracy and variability. The resUlts confirm oUr Premise that MP is able (i) to caPtUre
hidden time dePendencies of high dimensional time-varying objects which are inaccessible with
one-Parameter PH and (ii) to yield sUbstantially more stable featUre maPs in dynamic scenarios.
Contribution of DEPS vs. the Existing MP Summaries Finally, we evalUate Performance of the
new time-aware MP sUmmary, i.e., DEPS, in comParison to the MP sUmmaries based on the slic-
ing argument, namely, MP-I of Carriere & Blumberg (2020) and MP-L of Vipond (2020) as rep-
resentation inPUt to TAMP-S2GCNets. As Table 4 (right panel) shows, comParing to MP-I and
MP-L, DEPS yields 9%-12% improvement in MAPE, and at least a 10 times decrease in com-
putational time (29.84s for DEPS vs. 401.80s for MP-I and 517.11s for MP-L). Such substantial
differences in computational costs are explained by the need of the slicing-based MP summaries to
search for the most suitable one-parameter PH representation of MP. In turn, DEPS is based on a
9
Published as a conference paper at ICLR 2022
point-wise representation argument in linear algebra, and its high computational efficiency makes
DEPS the preferred choice for time-conditioned topological representation learning (see the running
time in Table 4). Since computational costs remain the major roadblock for MP applications, our
DEPS approach shows that MP tools based on scalable sparse matrix algorithms appear to be the
most promising direction for integration of MP into machine learning tasks.
6	Conclusion
We have explored utility of MP to enhance knowledge representation mechanisms within the time-
aware DL paradigm. The developed TAMP-S2GCNets model is shown to yield highly competitive
forecasting performance on a wide range of datasets, with much lower computational costs. In
the future we plan to explore combination of MP with the zigzag persistence and to investigate
asymptotic distributional properties of point-wise MP invariants.
10
Published as a conference paper at ICLR 2022
Ethics S tatement
We do not anticipate any negative ethical implications of the proposed methodology. In turn, we
believe that introduction of GCNs tools, coupled with multiparameter topological approaches, into
biosurveillance opens a new path for more accurate, timely, and robust tracking of infectious dis-
eases with high virulence such as COVID-19. In particular, multiparameter persistence allows for
extracting the most salient topological features along multiple geometric dimensions and, as such,
can be especially valuable to address disease dynamics as a function of multiple highly heteroge-
neous variables, e.g., socio-demographic, socio-environmental, and socio-economic factors. In turn,
GCNs and, more generally, geometric deep learning allow for capturing sophisticated nonlinear
spatio-temporal dependencies among factors which contribute to the disease spread. Hence, we pos-
tulate that in the next few years we can expect to see a new set of spatio-temporal biosurveillance
artificial intelligence algorithms, based on a combination of geometric deep learning, multiparame-
ter persistence, and more generally, tools of topological data analysis.
Reproducibility S tatement
The source code for the experiments can be accessed under https://github.com/
tamps2gcnets/TAMP_S2GCNets.git.
Acknowledgements
This work is sponsored by the National Science Foundation under award numbers ECCS 2039701,
INTERN supplement for ECCS 1824716, DMS 1925346 and the Department of the Navy, Office of
Naval Research under ONR award number N00014-21-1-2530. Part of this material is also based
upon work supported by (while serving at) the National Science Foundation. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of the author(s) and do not
necessarily reflect the views of the National Science Foundation and/or the Office of Naval Research.
The authors are also grateful to the ICLR anonymous reviewers for many insightful suggestions and
engaging discussion which improved clarity of the manuscript and highlighted new open research
directions.
References
U.S. Census Bureau: county adjacency file record layout. https://www.census.
gov/programs-surveys/geography/technical-documentation/
records-layout/county-adjacency-record-layout.html, a. Accessed:
2021-05-26.
U.S. Census Bureau: county population totals and components of change. https:
//www.census.gov/data/datasets/time-series/demo/popest/
2010s-counties-total.html, b. Accessed: 2021-05-26.
CovidActNow: u.s. covid risk & vaccine tracker. https://covidactnow.org, c. Accessed:
2021-05-26.
Etherscan: block explorer and analytics platform for ethereum, a decentralized smart contracts plat-
form. https://etherscan.io, d. Accessed: 2021-05-26.
Ethereum: community-run technology powering the cryptocurrency ether (ETH) and thousands of
decentralized applications. https://ethereum.org, e. Accessed: 2021-05-26.
JHU: covid-19 data from the johns hopkins university. https://github.com/
CSSEGISandData/COVID-19, f. Accessed: 2021-05-26.
Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson, Patrick Shipman,
Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori Ziegelmeier. Persistence images: A
stable vector representation of persistent homology. Journal of Machine Learning Research, 18
(8):1-35, 2017. URL http://jmlr.org/papers/v18/16-337.html.
11
Published as a conference paper at ICLR 2022
Robert J Adler, Omer Bobrowski, Matthew S Borman, Eliran Subag, Shmuel Weinberger, et al.
Persistent homology for random fields and complexes. In Borrowing strength: theory powering
appliCations-a FestschriftforLawrence D. Brown, pp. 124-143. 2010.
Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. Adaptive graph convolutional recurrent
network for traffic forecasting. Advances in Neural Information Processing Systems, 33, 2020.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional
and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.
Yuliy Baryshnikov and Robert Ghrist. Euler integration over definable functions. Proceedings of
the National Academy of Sciences, 107(21):9525-9530, 2010.
Gabriele Beltramo, Rayna Andreeva, Ylenia Giarratano, Miguel O Bernabeu, Rik Sarkar, and Pri-
moz Skraba. Euler characteristic surfaces. arXiv:2102.08260, 2021.
Chen Cai and Yusu Wang. Understanding the power of persistence pairing via permutation test.
arXiv preprint arXiv:2001.06058, 2020.
Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bix-
iong Xu, Jing Bai, Jie Tong, and Qi Zhang. Spectral temporal graph neural network for multivari-
ate time-series forecasting. In Advances in Neural Information Processing Systems, volume 33,
pp. 17766-17778, 2020.
Gunnar Carlsson and Rickard Bruel Gabrielsson. Topological approaches to deep learning. In
Topological Data Analysis, pp. 119-146. Springer, 2020.
Gunnar Carlsson and Afra Zomorodian. The theory of multidimensional persistence. Discrete &
Computational Geometry, 42(1):71-93, 2009.
Mathieu Carriere and Andrew Blumberg. Multiparameter persistence image for topological machine
learning. Advances in Neural Information Processing Systems, 33, 2020.
Mathieu Carriere, Frederic Chazal, Yuichi Ike, Theo Lacombe, Martin Royer, and Yuhei Umeda.
Perslay: A neural network layer for persistence diagrams and new graph topological signatures.
In AISTATS, pp. 2786-2796, 2020.
Andrea Cerri, Barbara Di Fabio, Massimo Ferri, Patrizio Frosini, and Claudia Landi. Betti num-
bers in multidimensional persistent homology are stable functions. Mathematical Methods in the
Applied Sciences, 36(12):1543-1557, 2013.
Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia. Freeway perfor-
mance measurement system: mining loop detector data. Transportation Research Record, 1748
(1):96-102, 2001.
Yuzhou Chen, Ignacio Segovia-Dominguez, and Yulia R Gel. Z-gcnets: Time zigzags at graph con-
volutional networks for time series forecasting. International Conference on Machine Learning,
2021.
David Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence diagrams.
Discrete & computational geometry, 37(1):103-120, 2007.
Rene Corbet, Ulderico Fugacci, Michael Kerber, Claudia Landi, and Bei Wang. A kernel for multi-
parameter persistent homology. Computers & graphics: X, 2:100005, 2019.
Baris Coskunuzer, Cuneyt Gurcan Akcora, Ignacio Segovia Dominguez, Zhiwei Zhen, Murat
Kantarcioglu, and Yulia R Gel. Smart vectorizations for single and multiparameter persistence.
arXiv:2104.04787, 2021.
M. di Angelo and G. Salzer. Tokens, types, and standards: Identification and utilization in
ethereum. In 2020 IEEE International Conference on Decentralized Applications and Infras-
tructures (DAPPS), pp. 1-10, 2020. doi: 10.1109/DAPPS49028.2020.00001.
Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track
covid-19 in real time. The Lancet Infectious Diseases, 20(5), May 2020. doi: 10.1016/
S1473-3099(20)30120-1.
12
Published as a conference paper at ICLR 2022
Herbert Edelsbrunner and John Harer. Computational topology: an introduction. American Mathe-
matical Soc., 2010.
Herbert Edelsbrunner and Salman Parsa. On the computational complexity of betti numbers: re-
ductions from matrix rank. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on
discrete algorithms,pp. 152-160. SIAM, 2014.
Robert Ghrist. Homological algebra and data. Math. Data, 25:273, 2018.
Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002.
Christoph Hofer, Florian Graf, Bastian Rieck, Marc Niethammer, and Roland Kwitt. Graph filtration
learning. In International Conference on Machine Learning, pp. 4314-4323. PMLR, 2020.
Christoph D Hofer, Roland Kwitt, and Marc Niethammer. Learning representations of persistence
barcodes. JMLR, 20(126):1-45, 2019.
Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and Karsten Borg-
wardt. Topological graph neural networks. arXiv:2102.07835, 2021.
Michael Kerber. Multi-parameter persistent homology is practical. 2020.
Michael Kerber and Alexander Rolle. Fast minimal presentations of bi-graded persistence modules.
In 2021 Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX), pp.
207-220. SIAM, 2021.
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term
temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference
on Research & Development in Information Retrieval, pp. 95-104, 2018.
Claudia Landi. The rank invariant stability via interleavings. arXiv:1412.3374, 2014.
Michael Lesnick and Matthew Wright. Computing minimal presentations and bigraded betti num-
bers of 2-parameter persistent homology. arXiv preprint arXiv:1902.05708, 2019.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. In International Conference on Learning Representations,
2018.
Yitao Li, Umar Islambekov, Cuneyt Akcora, Ekaterina Smirnova, Yulia R. Gel, and Murat Kantar-
cioglu. Dissecting ethereum blockchain analytics: What we learn from topology and geometry of
the ethereum graph? In Proceedings of the 2020 SIAM International Conference on Data Mining,
SDM 2020, pp. 523-531, 2020.
Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural ba-
sis expansion analysis for interpretable time series forecasting. In International Conference on
Learning Representations, 2019.
Nina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindrod, and Heather A Harrington. A roadmap
for the computation of persistent homology. EPJ Data Science, 6:1-38, 2017.
Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and
Tim Januschowski. Deep state space models for time series forecasting. Advances in neural
information processing systems, 31:7785-7794, 2018.
Hans Riess and Jakob Hansen. Multidimensional persistence module classification via lattice-
theoretic convolutions. 2020.
Rajat Sen, Hsiang-Fu Yu, and Inderjit S Dhillon. Think globally, act locally: A deep neural net-
work approach to high-dimensional time series forecasting. In Advances in Neural Information
Processing Systems, volume 32, 2019.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems, volume 27, 2014.
13
Published as a conference paper at ICLR 2022
Ashleigh Linnea Thomas. Invariants and Metrics for Multiparameter Persistent Homology. PhD
thesis, Duke University, 2019.
Gugan C Thoppe, D Yogeshwaran, Robert J Adler, et al. On the evolution of topology in dynamic
clique complexes. Advances inApplied Probability, 48(4):989-1014, 2016.
Virginia Vassilevska, Ryan Williams, and Raphael Yuster. Finding the smallest h-subgraph in real
weighted graphs and related problems. In Automata, Languages and Programming, pp. 262-273.
Springer Berlin Heidelberg, 2006.
Oliver Vipond. Multiparameter persistence landscapes. Journal of Machine Learning Research, 21
(61):1-38, 2020.
Matthew Wright and Xiaojun Zheng. Topological data analysis on simple english wikipedia articles.
arXiv:2007.00063, 2020.
Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. Graph wavenet for deep
spatial-temporal graph modeling. In Proceedings of the 28th International Joint Conference on
Artificial Intelligence, 2019.
Huaxiu Yao, Fei Wu, Jintao Ke, Xianfeng Tang, Yitian Jia, Siyu Lu, Pinghua Gong, Jieping Ye, and
Zhenhui Li. Deep multi-view spatial-temporal network for taxi demand prediction. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: a deep
learning framework for traffic forecasting. In Proceedings of the 27th International Joint Confer-
ence on Artificial Intelligence, pp. 3634-3640, 2018.
Liheng Zhang, Charu Aggarwal, and Guo-Jun Qi. Stock price prediction via discovering multi-
frequency trading patterns. In Proceedings of the 23rd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 2141-2149, 2017.
14
Published as a conference paper at ICLR 2022
tt
GtNMXXCf
A Notation
Frequently used notation is summarized in Table 5.
Table 5: The main symbols and definitions in this paper.
Notation Definition
the spatial network at timestamp t
the number of nodes
the number of edges at timestamp t
a sequence of observations on a multivariate variable
node feature matrix at timestamp t
simplicial complex
filtering function for sublevel filtration
Mk	number of k-simplices
Dk (C)	k-dimensional persistence diagram
X(C)	EUler-Poincare Characteristics
Hk(C)	kth homology group
F(∙, ∙)	multivariate filtering function
E±	Euler-Poincare Surfaces
{Et}tT=1	Dynamic EUler-PoinCare Surface
(Σ, A, P)	a probability space
F	a filtration of σ-fields
W1	Wasserstein-1 distance
D (∙, ∙)	the distance function for MP
B (∙)	the Betti function
Mk	the number k-simplices
FN	the feature dimension
T	the sliding window size
F(∙)	the multivariate forecasting model
S	the normalized self-adaptive adjacency matrix
Eφ	the learnable node embedding
S	Laplacian tensor
K	the length of Laplacian tensor	S
L	supra-Laplacian
Φ	the output of DEPSRL module
F(∙, ∙, ∙)	the dimension-wise concatenation function
B S ingle Parameter Persistent Homology and Its Summaries
All extracted topological features can be then summarized as a multiset in R2, called persistence
diagram (PD): Dk(C) = {(bi, di) ∈ R2 | bi < di}, where birth bi and death di mark the filtration
indexes at which the k-dimensional topological feature ρi appears and disappears, respectively. The
farther (bi , di ) from the diagonal is (that is, the more persistent ρi is), the likelier ρi is to contain
salient information on Gt. Multiplicity of (xb, yd) ∈ D is the number of p-dimensional topological
features (p-holes) that are born and die at xb and yb , respectively. Features on the diagonal of D
have infinite multiplicity.
Another summary which may be particularly suitable in our context of modeling noisy time-evolving
objects is the Euler-Poincare Characteristics (see Adler et al. (2010); Baryshnikov & Ghrist (2010)
on the Euler calculus and its applications, particular, in conjunction with sensor networks).
Definition B.1 (EUler-POinCare Characteristics). For a given simplicial complex C, Euler-Poincare
Characteristics χ(C) is defined as the alternating sum of the number of k-simplices ofC. That is, if
nk is the number of k-simplices in C, then χ(C) = PkM=0 (-1)knk, where M is the dimension of
highest dimensional simplex in C. Note that Euler-Poincare Characteristics is homotopy invariant,
15
Published as a conference paper at ICLR 2022
and an alternative definition is χ(C) = PkM=0 (-1)kBk (C) where Bk (C) is the kth Betti number
of C, i.e. rank of the kth homology group Hk(C) (Ghrist, 2018; Hatcher, 2002).
C	Proof of Theorem 3.1
In this section, we provide the proof of the stability theorem (Theorem 3.1). First, let us recall the
key notations we use. Let G+ and G - be two graphs and let C± be the clique complexes of G±
(Ghrist, 2018). Let F = (f, g) be a multivariate filter function, i.e. F : V± → R2, where V± is the
set of nodes of G±, and letI = {(αi, βj) | 1 ≤ i ≤ m, 1 ≤ j ≤ n} be the corresponding thresholds
for F = (f, g).
Next, we define Wasserstein-p distance among PDs, i.e., the important concept we borrow from the
theory of single parameter persistence (Edelsbrunner & Harer, 2010).
Definition C.1 (Wasserstein-p distance). Let (C±, f±, I±) be two single parameter filtrations, and
Dk(C±) be the corresponding PDs for k-cycles (i.e, k-dimensional topological features). Let qj+ =
(bj+, dj+) ∈ Dk(C+) be the birth and death times of a k-dimensional topological feature ρj. Then,
Wasserstein-p distance between Dk(C+) and Dk(C-) is defined as
1
Wp(Dk(C+), Dk(C-))=hφf(	X	kq+ - φ(q+)k∞)p, P ∈ Z+,	(9)
φ j∈D(C+)∪∆
where φ : Dk(C+) ∪ ∆ → Dk(C-) ∪ ∆ is a bijection (matching) and ∆ is the diagonal set, i.e.,
∆ = {(b, b)|b ∈ R}, which contains k-cycles of infinite multiplicity. With ∆ in both sides, we
ensure the existence of these bijections even if the cardinalities |{qj+}| and |{ql-}| are different.
When p = ∞, (9) corresponds to the bottleneck distance, i.e. W∞ . In the proof below, we use
p = 1, i.e. W1 -distance.
Armed the PD and Wasserstein-p distance concepts, we now turn to the main stability result for the
new MP summary. Let Cb± = {Ci±j } be the bifiltration associated with (C± , F, I), i.e. Ci±j is the
clique complex of the induced subgraph Gi±j = F-1((-∞, αi] × (-∞, βj]). (Figure 2 illustrates
such bifiltration on graphs with degree and eccentricity functions.) Let E± be the the corresponding
EUler-Poincare Surface, i.e. E± is an m X n-matrix with entries E± = χ(C±,), where χ(.) is
the Euler Characteristics (See Figure 2), and D(C+, C-) be the weak L1-distance as defined in
Section 3 of the paper (see also Remark C.1 below for the motivation to use this new metric).
Proof of Theorem 3.1: We start from providing the outline of the proof. Notice that Euler-Poincare
Surfaces is the alternating sum of Betti numbers {Bk}. Hence, the distance kE+-E- k1,1 is bounded
by the difference of the corresponding Betti numbers for Cb±, i.e. Eqs. (10) and (11). By fixing i
and k, and by taking column distance Dc into consideration, our main assertion
kE+ — E-k1,1 ≤ C∙D(C+, C-),	C > 0
then reduces to Eqs. (12) and (13). To relate the difference |B(Cj+) - B(Cj-)| to
W1(D(C+), D(C-)), we observe that the Betti number B(Cj±) is the sum of the number of bar-
codes in D(C±), containing βj, i.e., #{r | βj ∈ [br±, dr±)} (Eqs. (13) and (14)). Yet, the differences
of the number of barcodes containing βj for C+ and C- can be bounded by the Wasserstein-1 dis-
tances of D(C+) and D(C-), i.e. Eqs. (15) and (16). Aggregating these terms on indices i and k
finishes the proof.
Now, we turn to derivations in details. Recall that χ(C) = PkM=0(-1)kBk (C), where Bk(C)
is the kth Betti number of C and M is the maximum homological dimension of C. Then, since
Ei±j = χ(Ci±j), we have Ei±j = PkM=0Bk(Ci±j). Hence, for any i,j,
MM
∣E+ — Ejl = I X(-1)k[Bk(C+) -Bk(Cj)]| ≤ X ∣Bk(Cj) - Bk(Cj)|.	(10)
k=0	k=0
16
Published as a conference paper at ICLR 2022
Since kE+ -E-k1,1 = Pim=1 Pjn=1 |Ei+j - Ei-j|,wehave
mnM
kE+ - E-k1,1 ≤ XXX ∣Bk(Cj)- Bk(Cj)∣.
(11)
+
Now, to complete the proof, we need to bound the latter sum by the MP distance D(C+, C-).
Recall that by Definition 3.3
Mm
Dc(C+, C-) = XX Wι(Dk(C+), Dg(C-)) ≤ D(C+, C-).
k=0 i=1
Then, by Eq. 11, to prove the theorem, it is enough to show that for any 0 ≤ k0 ≤ M and for any
1 ≤ i0 ≤ m,
n
X∣Bko (C+j) -Bko (C-j )∣ ≤ C∙Wι(Dg0 (C+ *),Dk0 (C-*)),	C> 0.	(12)
j=1
For simplicity, without loss of generality, we fix 1 ≤ i0 ≤ m and 0 ≤ k0 ≤ N . Also, for sake
of notations, we drop the subscripts and superscripts, i.e C± = Ci±* and Cj± = Ci±j, D(C± ) =
Dkg (Ci±*) and B(.) = Bk0 (.). With these simplified notations, Eq. 12 is equivalent to
n
X∣B(C+) -B(C-)∣ ≤ C ∙Wι(D(C+), D(C-))	C > 0.	(13)
j=1
To show (13), we need to evaluate the relation between the Betti numbers B(Cj±) and the persistence
diagrams D(C±). Consider D(C) = {(br, dr) | 1 ≤ r ≤ Q}, where br represents the birth
time of a k0-cycle ρr in C, while dr represents the death time of ρr. In particular, ρr induces a
barcode [br, dr). For each such ρr, define indicator function ψr(t) = I[br,dr) (t), i.e. ψr(t) = 1 for
t ∈ [br, dr) and ψr(t) = 0 otherwise. Then, the Betti function of C is defined as B : [β1, ∞) → N
such that B(t) = PrQ=1 ψr(t) for t ∈ [β1, ∞). (Here, β1 is the lowest threshold for the filtering
function g in the set of thresholds I = {(αi, βj) | 1 ≤ i ≤ m, 1 ≤ j ≤ n}. Similarly, ifwe consider
row sum Dr instead of column sum Dc, then we would use the other filtering function f , and the
Betti functions would be defined as B : [α1, ∞) → N.) Hence,
Q±
B(Cj±) =	ψr±(βj), j∈Z+.
r=1
As a result, we get
n	n ∣ Q+	Q-	∣
X∣∣B(Cj+) - B(Cj-)∣∣ = X∣∣Xψr+(βj) -Xψr-(βj)∣∣	(14)
j=1	j=1 r=1	r=1
n Qb
≤ XX lψ+(βj ) - ψ-(r)(βj )|,
j=1 r=1
where φ : D(C+) → D(C-) is the best matching in the definition of W1(D(C+), D(C-)) and
Qb = max{Q+, Q-}.
Notice that the contribution of each ∣ψ+(βj) - ψ-r)(βj )| to the SUm at βj is less than the
number (#r) of intervals [br+, dr+)4[bφ+(r), bφ-(r)) containing the interval [βj, βj+1), where 4 is
the symmetric difference of the sets. Let K = minj (βj+1 - βj ). By dividing the length
k[br+, dr+)4[bφ+(r),bφ-(r))k by K, we ensure that every such interval contributes at least 1 to the
Betti number count. Hence, we arrive at the following key inequality:
a
nQ	1
XX lψr (βj ) - ψφ(r)(βj )l ≤ K ∙ X |br - bφ(r)1 + ldr - dφ(r) L	(15)
j=1 r=1	r
17
Published as a conference paper at ICLR 2022
Finally, since
Qb
X |b+ - b-(r)l + |d+ - d-(r)l ≤ 2Wι(D(C+),D(C-)),
r=1
we obtain
n2
∑∣B+(βj) -B-(βj)∣≤ KK ∙ WI(D(C+),D(C-)).	(16)
j=1
Since (16) holds for any 1 ≤ i0 ≤ m and 0 ≤ k0 ≤ N, for C = 2/K we get
mnM
X X XlBk(Cj)-Bk (C- )∣≤ CD。©+, C-).
i=1 j=1 k=0
By Eq. (11), this implies
kE+ — E-k1,1 ≤ C∙D(C+, C-),
which completes the proof.
Remark C.1 (On the Choice of the Metric). While interleaving and matching distances are very
common and useful for multipersistence in various cases (Thomas, 2019), they are not suitable to
study the EUler-Poincare Surfaces. The reason for this is that matching and interleaving distances
are based on L∞ metrics, and do not see the quantity of generators of small interleaving distances.
However, Euler-Poincare Surfaces by definition depends on the rank of the homology groups, and
the quantity of the generators (Betti numbers) are crucial in their definition even if they exhibit a
very short lifespan. In particular, L∞-based metrics find the maximum distance between the essen-
tial generators, but such metrics fail to capture the quantity of small generators. That is, while the
interleaving or matching distance between the multipersistence modules of the bifiltrations {C+ }
and {C } is small, there might be a very large distance between the induced Euler-Poincare Sur-
faces E+ and E- . In turn, Lp-based distances are successful to capture the rank even if they are
small (noise) generators, while interleaving and matching distances are not very sensitive to these.
(See Thomas (2019, Section 4.2) for further discussion.) This is why we introduce the weak L1
metric above which naturally suits to study the Euler-Poincare Surfaces and similar vectorizations
in the theory. Furthermore, such weak L1 metric can be considered a straightforward generalization
of L1 -based norm for multiparameter persistence.
Remark C.2 (Implications of the Stability Result). Note that our stability result (Theorem 3.1)
directly implies that the distances between multiparameter PDs control the distance between the
resulting Euler-Poincare Surfaces. By combining with the stability result for PDs (Cohen-Steiner
et al., 2007), one can conclude that the small changes in the MP filtering function F : Vt 7→ Rd or
the small changes in the input data can result only in a small change in DEPS surfaces. In particular,
by Cohen-Steiner et al. (2007), for single parameter persistence, we have
W(Df (C),Dg(C)) ≤ Cd(f,g)
where Df is the persistence diagram for filtering function f, and d(f, g) is the distance between the
functions f and g. The way we define our weak L1 -metric enables us to use this single persistence
result in this context as follows. The column distance Dc is the sum of single PD for each column.
Each column, (say jth column) is represented by a single variable filtering function F(t, βj). Then,
the changes in F(t, βj) reflects in the column distance for PDs corresponding to jth-column. This
means if we have a small change in F(t, βj) for each j, then the column distance for MP will be
small. So is the row distance induced by F (αi, s).
Similarly, ifwe have two MP filtering functions F± : V → R2, they will induce two filtrations C±.
When d(F+, F-) is small, then by Cohen-Steiner et al. (2007), D(Cb+, Cb-) is small. When we
apply our stability theorem to these filtrations, we see that the distance between the induced DEPS
surfaces kE+ - E- k1,1 will be small, too. Since small change in the data can be converted to small
change in the filtering functions on the same data, the result can also be interpreted as the small
change in the data results in small change in the induced DEPS surfaces.
18
Published as a conference paper at ICLR 2022
Remark C.3 (DEPS through the Probabilistic Perspective). Furthermore, the new time-aware
MP summary can also be assessed from a probabilistic perspective, ifwe view a multivariate time se-
ries X as real-valued random process indexed by time t and defined on a probability space (Σ, A, P).
For 1 ≤ J ≤ L ≤ ∞, define the σ-field zJL ⊂ A such that zJL = σ(Xk , J ≤ k ≤ L (k ∈ Z)), i.e.
zJL is a σ-field generated by XJ, XJ+1, . . . , XL. Then, Et is a real-valued zt1-measurable random
matrix and DEPS {Et}tT=1 is F-measurable random matrix, where F = (z1k)k>0 is a filtration of
σ-fields indexed over time. Given the results of Thoppe et al. (2016) on weak convergence of the
Euler characteristic X(C) for dynamic Erdos-Renyi graphs to the Ornstein-Uhlenbeck process, this
opens a potential path to establish asymptotic distribution of Et .
Remark C.4 (DEPS for Higher Dimensions). DEPS can be naturally expanded to a case when we
consider filtrations along more than two geometric dimensions. In such a case, instead of a m × n-
matrix, DEPS will be a tensor. Our stability result (i.e., Theorem 3.1) verbatim extends to such a
case.
BO = O
Bi = 0
尻=o
5ι = 0
Ho = I
B1=O
B0 = 1
Bi = 0
degree
0
0
2
3
4
Figure 2: Illustration of multidimensional persistence (i.e., multipersistence). In the original graph
(the top left panel), red numbers represent the eccentricity of the node, while blue numbers are the
node degrees. The top right panel depicts the corresponding Euler-PoinCare Surface. Shape ProP-
erties of the top graph are evaluated along two geometric dimensions (i.e., degree and eccentricity).
In particular, for each row, the rightmost graph is filtered by degree function. For each column, the
top graph is filtered by eccentricity function. Each cell includes Betti numbers B0 and B1 . Since B1
and B2 are 0 for each cell, we get Eij = B0 .
D Reproducibility
D.1 Datasets
We use three different datasets, from transportation, Ethereum blockchain, and biosurveillance do-
mains, to examine the performance of the proposed TAMP-S2GCNets methodology:
19
Published as a conference paper at ICLR 2022
1.	Transportation: The transportation and traffic data for the U.S. State of California, from
the freeway Performance Measurement System (PeMS) data sources (i.e., PeMSD3, PeMSD4,
PeMSD8) (Chen et al., 2001), naturally produces a dynamic network where each node is a loop
detector and each edge represents a freeway between two nodes. We extract aggregated data to
5 minutes, thus generating dynamic networks with 26,208, 16,992 and 17,856 nets in PeMSD3,
PeMSD4 and PeMSD8 datasets, respectively. To capture spatio-temporal dependencies, we re-
build the traffic graph structure, at time t, and compute edge weights wt,uv, between pair nodes
(u, v), via the Radial Basis Function wt,uv = e-||xt，u-Xt，v|| /γ; where Y = 1.0 and each node
χt,∙ has features speed, occupancy and flow rate. To investigate the dynamic of the traffic graph
topology, we restrict the final graphs to only keep edges whose are below or equal to threshold
α = 0.01.
2.	Blockchain: Ethereum blockchain data contain token assets which naturally represents network
layers of the Ethereum network (di Angelo & Salzer, 2020; Web, e), in which nodes and edges are
addresses of users and digital transactions, respectively. Using the publicly available blockchain,
we extract dynamic networks from three tokens with market value above $100,000,000: Bytom,
Decentraland and Golem. In our experiments, we extract dynamic networks by daily transactions
and use daily closed prices (Web, d), hence, creating a graph/net for each day. Since each token
is created at different date, in all cases last day is 05/07/2018, the dynamic networks vary in
the number of nets: Bytom (285 nets), Decentraland (206 nets) and Golem (443 nets). For the
sake of reasonable computation time, we reduce the original net size using the maximum weight
subgraph approximation method of Vassilevska et al. (2006) from more than 400,000 nodes,
average of 120,000 edges, to 100 nodes using number of transactions and price-volume as edge
weights.
3.	Biosurveillance: SARS-CoV-2, which is the virus that causes COVID-19, has spread around
every place in the world. To analyze the progression of coronavirus disease we create a dynamic
network based on daily records on COVID-19 cases and hospitalizations from the CovidActNow
project (Web, c) and Johns Hopkins University (Web, f; Dong et al., 2020), and population num-
bers from the U.S. Census Bureau (Web, b). We choose data at county level from California and
Texas states as our cases of study, and focus on forecasting the number of hospitalized patients.
Similarly to (i) we rebuild the coronavirus-spread graph structure, at day-time t, based on the
county adjacency graph (Web, a) and Radial Basis Function between pair of nodes. We build a
dynamic network for each state, each dynamic network contains 335 nets. The number of con-
firmed COVID-19 cases and county population size serves as node features, and final graphs only
keep edges whose are below or equal to the threshold α = 0.01.
More details on experimental settings and filtering functions are provided in Section D.2. Table 6
summarize the characteristics of all datasets used in our experiments.
Table 6: Summary of datasets used in time series forecasting task.
Type	Dataset	No. Nodes	No. avg Edges	Time interval
Traffic	PeMSD3	358	55.91	09/01/2018 - 11/30/2018
Traffic	PeMSD4	307	26.32	01/01/2018 - 28/02/2018
Traffic	PeMSD8	170	18.51	01/07/2016 - 31/08/2016
Ethereum	Bytom	100	9.98	27/07/2017 - 07/05/2018
Ethereum	Decentraland	100	16.94	14/10/2017 - 07/05/2018
Ethereum	Golem	100	20.58	18/02/2017 - 07/05/2018
COVID-19	CA	55	56.15	01/02/2020 - 31/12/2020
COVID-19	TX	251	929.80	01/02/2020 - 31/12/2020
D.2 Experimental Settings
We implement our TAMP-S2GCNets with Pytorch framework on NVIDIA GeForce RTX 3090
GPU. Further, for all datasets, TAMP-S2GCNets is trained end-to-end by using the Adam opti-
mizer with a L1 loss function. The tuning of our proposed TAMP-S2GCNets on each dataset is
done via grid search over a fixed set of choices and the same cross validation setup is used to
tune baselines. We compare TAMP-S2GCNets with 13 types of state-of-the-art methods, including
20
Published as a conference paper at ICLR 2022
FC-LSTM (Sutskever et al., 2014), SFM (Zhang et al., 2017), N-BEATS (Oreshkin et al., 2019),
DCRNN (Li et al., 2018), LSTNet (Lai et al., 2018), STGCN (Yu et al., 2018), TCN (Bai et al.,
2018), DeepState (Rangapuram et al., 2018), GraphWaveNet (Wu et al., 2019), DeepGLP (Sen
et al., 2019), AGCRN (Bai et al., 2020), Z-GCNETs (Chen et al., 2021), and StemGNN (Cao et al.,
2020).
MODEL SPECIFICATIONS For PeMSD3, TAMP-S2GCNets consists of 2 layers whose hidden
feature dimension is 64. The learning rate is 5e-3 and the batch size is set as 64. The “cost” dtuauta of
staying in the same node u (in the layer ta) and the “cost” dtuautb of jumping from the current node u
in layer ta to node u in layer tb are set to be 0.1 and 0.2, respectively.
For PeMSD4, TAMP-S2GCNets consists of 2 layers whose hidden feature dimension is 128. The
learning rate is 5e-3 and the batch size is set as 64. The “cost” dtuauta of staying in the same node
u (in the layer ta) and the “cost” dtuautb of jumping from the current node u in layer ta to node u in
layer tb are set to be 0.1 and 0.2, respectively.
For PeMSD8, TAMP-S2GCNets consists of 2 layers whose hidden feature dimension is 64. The
learning rate is 2e-3 and the batch size is set to 16. The “cost” dtuauta of staying in the same node
u (in the layer ta) and the “cost” dtuautb of jumping from the current node u in layer ta to node u in
layer tb are set to be 0.1 and 0, respectively. The dimension of node embedding dc (in Eφ) are 10
(PeMSD3), 10 (PeMSD4), and 20 (PeMSD8), respectively. We set the dimension K of Laplacian
tensor S as 2 for all transportation networks.
For all Ethereum token networks (i.e., Bytom, Decentraland, and Golem), TAMP-S2GCNets
consists of 2 layers whose hidden feature dimension is 32. The final hyperparameter setting is
learning rate of 1e-2 and batch size of 8. The “cost” dtuauta of staying in the same node u (in the layer
ta) and the “cost” dtuautb of jumping from the current node u in layer ta to node u in layer tb are set
to be 0.01 and 0.01, respectively. In addition, we set the embedding dimension dc in Eφ as 1 for
all Ethereum token networks. We set the dimension K of Laplacian tensor S as 2, 3, 3 for Bytom,
Decentraland, and Golem, respectively.
For COVID-19 hospitalizations in CA and TX, TAMP-S2GCNets consists of 2 layers whose hid-
den feature dimension is 128. The learning rate is set to 1e-1 and 1e-2 for CA and TX, respectively.
The batch size is set to 8, and the embedding dimension dc in Eφ is 10. The “cost” dtuauta of staying
in the same node u (in the layer ta) and the “cost” dtuautb of jumping from the current node u in layer
ta to node u in layer tb are set to be 0.1 and 0.2, respectively. We set the dimension K of Laplacian
tensor S as 2 for both CA and TX.
ALTERNATIVE OPTIONS FOR DEPS WITHIN DEPSRL MODULE In our experiments,
for all datasets, the CNN based model (i.e., fθj where j = {1, 2} in Time-Aware Multipersistence
EUler-Poincare Surface Representation Learning (DEPSRL) module) consists of 2 CNN layers. For
transportation networks and COVID, The filter size, kernel size, and stride is set to be 8, 2, 2 re-
spectively. For Ethereum token networks (i.e., Bytom, Decentraland, and Golem), the filter size,
kernel size, and stride is set to be 8, 1, 2 respectively. We set the size of global average pooling and
global max pooling as 3 × 3. Table 7 shows the optimal output dimensions of spatial graph convo-
lutional layer (QSpa), supragraph diffusion convolutional layer (QSup), and spatio-temporal feature
transformation (QFT) on all datasets.
Table 7: The output dimensions of spatial graph convolutional layer (QSpa), supragraph diffusion
convolutional layer (QSup), and spatio-temporal feature transformation (QFT) on different datasets.
Output dimension	PeMSD3	PeMSD4	PeMSD8	Bytom	Decentraland	Golem	CA	TX
QSpa	Qout 4	Qout 4	-^3Qout ^^ 4	Qout 2	Qout 2	3Qout 4	Qout 4	Qout 4
QSup	Qout	Qout	Qout	Qout	Qout	Qout	Qout	Qout
	R	R	TF	ɪ	ɪ	百	ɪ	R
QFT	QoUt -2-	QOUt -2-	QOUt -8~	QOUt	QOUt 		QOUt -8-	QOUt -2-	QOUt -2-
21
Published as a conference paper at ICLR 2022
Furthermore, in addition to the Time-Aware MUltiPersistence EUler-Poincare Surface Representation
Learning (DEPSRL) module (see Section 4.3 in our main paper):
Φi,T AM P = ㊉(fGAP(fθι ({Ei}T=ι)),∕GMP(∕θ2 ({Ei}T=ι))),
We also have considered passing the aggregated DEPS as input to DEPSRL, that is, fθ, (E), j = 1, 2,
whereE= 1/TPi=I ∈ Rm×nEi.
As expected, DEPS {Ei}iT=1 tends to yield better performance within the DEPSRL module than
its aggregated counterpart E, which can be explained by lower loss of time-conditioned topological
information within DEPS. However, in the case of PeMSD4 dataset, we find that E slightly improves
the performance of TAMP-S2GCNets (i.e., the MAE of TAMP-S2GCNets based on the aggregated
DEPS is 17.58 and the MAE of TAMP-S2GCNets based on DEPS is 17.69), which is likely due
to higher data heterogeneity of PeMSD4 vs. other transportation datasets. In all other datasets, we
report results based on DEPS.
COMPARISON TO OTHER MP SUMMARIES For fair comparison, computation of all MP
representations (as shown in Table 11 and Table 4 (right panel) in our main paper) have been run on
an 8-cores DO droplet machine with Intel Xeon Scalable processors at base frequency of 2.5 Ghz.
Our source codes are available for revision1. Source codes for computation of MP-I and MP-L are
freely available in Github2. We follow the filtering function recommendations for graphs, on both
MP-I and MP-L, from supplementary material of Carriere & Blumberg (2020). Further technical
details on the 8-core machine are publicly available3.
ON TYPES OF MULTIFILTRATIONS Good filtrations should have rich range to get a fine
resolution, and it should reflect an important property relevant to the question at hand. For mul-
tipersistence, for a good combination of two such filtering functions, one should choose functions
uncorrelated. One approach is to choose one function from the domain of the question, and the other
from strictly graph properties (e.g. degree, betweenness, etc.). We assess shape characteristics of
Gt via filtering functions based on node degree (Deg), betweenness centrality (Btwns), and edge
weights (Power). Edge weights for each dataset are computed as described in D.1; i.e using transac-
tions between addresses (Trns), volume of transactions (Vol), traffic measurements (Traffic)
and COVID-19 incidence (Incidence). Our analysis focuses on multivariate filtering function
F : Vt 7→ Rd with d = 2. For Deg and Btwns the set of nondecreasing thresholds runs on values
over nodes, whilst for filtering via power filtration (Power) of Gt nondecreasing thresholds runs on
weighted paths between nodes. Notice that each pair-combination of filtering functions produces
a specific MP representation in a form of the Euler-Poincare surface, which impacts the algorithm
performance, as shown in our multiple experiments. Table 12 presents the optimal multifiltration for
each dataset.
E	Additional Experiments and Running Time
Table 8: Forecasting performance and standard deviations on PeMSD3, PeMSD4 and PeMSD8
datasets. All models are re-run using the original authors’ code.
Model	PeMSD3		PeMSD4		PeMSD8	
	MAE	RMSE	MAPE (%) MAE	RMSE	MAPE (%)	MAE	RMSE	MAPE (%)
DCRNN (Li et al., 2018)	18.18±0.15	30.31±0.25	18.91±0.82 24.70±0.22 38.12±0.26	17.12±0.37	17.86±0.03 27.83±0.05	11.45±0.03
STGCN (Yu et al., 2018)	17.49±0.46 30.12±0.70		17.15±0.45 22.70±0.64 35.50±0.75	14.59±0.21	18.02±0.14 27.83±0.20	11.40±0.10
DeepState (Rangapuram et al., 2018)	15.59±0.43 20.21±0.60		18.69±0.22 26.50±0.25 33.00±0.67	15.40±0.23	19.34±0.11 27.18±0.22 16.00±0.10	
GraphWaveNet (Wu et al., 2019)	19.85±0.03 32.94±0.18		19.31±0.49 26.85±0.03 39.70±0.04 17.29±0.24 19.13±0.08 28.16±0.07			12.68±0.57
AGCRN (Bai et al., 2020)	14.22±0.31	24.03±0.36	13.89±0.29 17.78±0.15 29.17±0.09 		11.79±0.11 		14.59±0.40 23.06±0.33	9.29±0.11
Z-GCNETs (Chen et al., 2021)	1.4...2..0.±...0...3.3. 25.29±0.53		1.3	8.8.±...0...2.3. 18.05±0.20 2.9	0.8.±...0...1.9.	1.1...7..9.±...0...0.8.	1.4	5.2.±...0...1.5. 2.3	0.0.±...0...2.0.	9...2..8.±...0...3.1.
TAMP-S2GCNets (us)	13.91±0.16	2.3	7.7.±...0...3.2.	13.40±0∙39 17.58±0∙20 28.56±0.28 11.01±0.10 13.77±0.08 21.70±0.25			8.99±0.15
Why MP Helps? More Experiments To validate gains (if any) delivered by multifiltration com-
pared to the single filtration persistence and ensemble of multiple stacked together single parameter
1https://www.dropbox.com/sh/n0ajd5l0tdeyb80/AABGn-ejfV1YtRwjf_L0AOsNa?
dl=0
2https://github.com/MathieuCarriere/multipers
3https://www.digitalocean.com/blog/premium-droplets-intel-cascade-lake-amd-epyc-rome/
22
Published as a conference paper at ICLR 2022
Table 9: The TAMP-S2GCNets ablation study on Golem (MAPE) and CA. Here *, **, *** denote
significant, statistically significant, highly statistically significant results.
Architecture	Dataset	
	Golem (MAPE)	CA (RMSE)
TAMP-S2GCNets	20.10%	371.60
W/o DEPSRL	20.37%	*380.18
W/o GCNSPa	**21.51%	*378.87
W/o GCNSuP	*20.72%	**382.63
W/o FT	**21.08%	***383.37
filtrations, we also report the results of single filtration, filtration ensemble, and multifiltration on
Decentraland (see Table 10). (In case of a filtration ensemble, we aggregate two single filtrations
and feed the ensemble representation into the model.) We find that MP substantially outperforms
both the single filtration persistence and the filtration ensemble. In addition, the MP results are
generally more stable than ones based on the single filtration and filtration ensemble. This result
can be expected, as by construction of DEPS, each individual single parameter persistence corre-
sponds either to a row or a column of DEPS, that is, we have only a one-dimensional fingerprint
of the data. In contrast, in MP and its DEPS summary, we obtain a two-dimensional fingerprint of
the data, meaning any cell in the output simultaneously related to the all 8 neighboring cells (East,
West, Northeast, etc.). As such, MP provides a 2D resolution of the data space and delivers much
finer information on the underlyng data topology.
Table 10: MAPE (in%) (standard deviation) of TAMP-S2GCNets with single filtration, filtration
ensemble, and multifiltration persistence.
TAMP-S2GCNets on Decentraland
Single filtration	Filtration ensemble	Multifiltration
- Deg: 25.18±2.00 Btwns: 24.93±1.88 PowerTrns: 23.81±1.68 PowerVol: 22.15±2.70	[Deg, Btwns]: 24.35±1.78 [Btwns, PowerTrns]: 23.00±2.13 [Btwns, PowerVol]: 22.70±2.90 [Deg, PowerTrns]: 21.81±1.65 [Deg, PowerVol]: 21.58±3.03	Deg & Btwns: 21.13±1.57 Btwns & PowerTrns: 2. .0...0.0. .±. .1...5. .1. Btwns & PowerVol: 21.59±1.76 Deg & PowerTrns: 19.89±1.69 Deg & PowerVol: 20.83±1.86
Table 11 shows the computational costs and performance of different MP summaries on Decen-
traland and Golem datasets. We observe that our new time-aware MP summary DEPS not only
outperforms the existing MP summaries (i.e., MP-I and MP-L) but also induces substantially lower
computational costs. These findings demonstrate that our DEPS can effectively capture salient time-
aware topological information with lower time and memory cost.
Table 11: MAPE (%) and computational time (in second) on Decentraland and Golem for TAMP-
S2GCNets with our DEPS and MP-I of Carriere & BlUmberg (2020) and MP-L of ViPond (2020),
as MP summaries.
MP summary	Decentraland	Golem
MP-I (Carriere & Blumberg, 2020) MP-L (ViPond, 2020)	22.58 (366.71s) 23.10(489.50s)	22.00 (850.20s) 22.05 (1137.09s)
DEPSDeg & Btwns DEPSDeg & PowerTrns DEPSBtwns & PowerTrns	21.13 (43.25s) 19.89 (36.90s) 20.00 (25.34s)	21.05 (126.15s) 20.10 (112.61s) 21.00 (85.87s)
COVID-19 Biosurveillance: More Experiments Finally, Figure 3 dePicts the results on 15-day
ahead forecasting of hosPitalizations in California on a county level basis, using the toP 3 models
(i.e., AGCRN (Bai et al., 2020), StemGNN (Cao et al., 2020), and our TAMP-S2GCNets). (Here,
to differentiate among RMSEs on a county level basis, we omit the maP for DCRNN (Li et al.,
23
Published as a conference paper at ICLR 2022
(a) TAMP-S2GCNets.
(b) StemGNN.
(c) AGCRN.
(d) California counties.
Figure 3: Maps of RMSE values for 15-day ahead forecasts of COVID-19 hospitalizations on a
county level in California and counties in California. (a) TAMP-S2GCNets. (b) StemGNN. (c)
AGCRN. (d) California counties (from California Department of Education).
2018) due to its lower predictive performance, relative to RMSE scale of AGCRN, StemGNN,
and our TAMP-S2GCNets.) Overall, TAMP-S2GCNets and StemGNN tend to perform similarly
for less populated counties in Northern California, with TAMP-S2GCNets being slightly better
than StemGNN over the Pacific Coast and StemGNN yeilding some edge in the North East. In
turn, AGCRN tends to deliver more competitive performance in North-Eastern California, except of
Shasta county.
However, performance among models starts to differ drastically in more populated counties and, in
particular, in Southern California where itis much harder to forecast COVID-19 hospitalizations due
to higher population density and more disparate socio-economic factors. Here, TAMP-S2GCNets
24
Published as a conference paper at ICLR 2022
Table 12: Types of multifiltration for all datasets.
Dataset	Multifiltration
PeMSD3	Deg & Btwns
PeMSD4	Deg & Btwns
PeMSD8	Btwns & PowerTraffic
Bytom	Btwns & PowerTrns
Decentraland	Deg & PowerTrns
Golem	Deg & PowerTrns
COVID-19 CA	Btwns & PowerIncidence
COVID-19 TX	Deg & PowerInCidenCe
substantially outperforms both StemGNN and AGCRN almost in all counties of Southern Califor-
nia, yielding predictive gains from 3.8% vs. StemGNN and 29.8% vs. AGCRN in San Bernardino to
57.7% vs. StemGNN and 55.7% vs. AGCRN in San Diego. While all models perform very poorly in
Los Angeles county (i.e., one of the most populated counties), TAMP-S2GCNets still yields the low-
est RMSE. One important phenomenon we observe is that TAMP-S2GCNets systematically tends
to substantially outperform StemGNN and AGCRN in more highly populated counties, including
both Southern and Northern California. For instance, the predictive gains of TAMP-S2GCNets in
Santa Clara are 39.2% vs. StemGNN and 67.9% vs. AGCRN. These findings indicate that time-
aware MP topological information contained in TAMP-S2GCNets can address hidden relationships
among spatio-temporal factors contributing to COVID-19 dynamics which other models cannot.
F	Additional Details of TAMP-S2GCNets Architecture and
Experiments
Remark F.1 (Spatio-Temporal Feature Transformation). We utilize and share the learnable node
embedding Eφ to (i) construct the normalized self-adaptive adjacency matrix S and (ii) project the
target node from the input feature space (in spatial and temporal dimensions) into a high-level latent
space (together with ΘFT). Hence, this shared-update weights can not only mitigate the risk of over-
fitting but allow us to capture shared patterns among spatial-temporal features and graph structures
over the time axis. The learnable node embedding Eφ in TAMP-S2GCNets can be obtained from
the backward propagation through time.
We use Einstein summation convention in Eq. 1 for the multiplication of XT ∈ Rb×T ×N×P (where
(`)
b is the batch size) and EφΘFT. The implementation of matrix multiplication in Eq. 1 is as follows:
(i) We first perform the matrix multiplication between Eφ ∈ RN×dc and θF'T ∈ Rdc ×P×QFT to get
an intermediate weight term denoted by Θ£T ∈ RN×P×QFT; (ii) then we compute the matrix mul-
T	(`)
tiplication between XT and ΘF T through Einstein summation convention, i.e., in Pytorch, output
=torch.einsum(‘btni, nio → btno’，X, intermediate_weight_term) (where X is XT and intermedi-
ate_weight_term is ΘIFT).
Remark F.2 (Normalized Self-Adaptive Adjacency Matrix). We utilize the node embedding dic-
tionaries Eφ ∈ RN ×dc to construct the normalized self-adaptive adjacency matrix S via multiplying
Eφ and Eφ>. As such, construction of the normalized self-adaptive adjacency matrix S does not im-
pose any requirements on density of the input graph. That is, our graph learning architecture (i.e.,
the self-adaptive adjacency matrix construction mechanism) overcomes this problem of the dense
input graph met by GAT.
To explore more computationally efficient mechanisms of learning the normalized self-adaptive ad-
jacency matrix, in the future work, we will (i) apply the sparse sampling strategies to the graph
learning architecture; (ii) learn the graph based on the original graph structure information, i.e., we
only infer the dependencies between the adjacent nodes (i.e., there exists an edge between them in
the original graph) via multiplying eu,φ and ev>,φ; (iii) construct hypergraph representation instead of
graph which not only alleviate the computational complexity but capture heterogeneous higher-order
structures information.
25
Published as a conference paper at ICLR 2022
For S, in our experiments, we performed an extensive grid search for the hyperparameter K (i.e.,
truncation order). We find the optimal K of 2 for the transportation network, the optimal K of 2
for COVID-19 network dataset, and the optimal K of 3 for Ethereum blockchain network. That
is, for larger networks the truncation order K tends to be lower so that the computational costs are
mitigated.
Furthermore, we have performed ablation studies on PeMSD4 and PeMSD8 to assess utility of the
normalized self-adaptive adjacency matrix S in TAMP-S2GCNets. The results in Tables 13 and 14
indicate that TAMP-S2GCNets outperforms TAMP-S2GCNets with the plain adjacency matrix (in-
stead of the normalized self-adaptive adjacency matrix) for both PeMSD4 and PeMSD8 across
all the evaluation metrics. The results illustrate the importance of learning the node-specific pat-
terns/embeddings over the spatial-temporal time series dataset.
Table 13: Ablation study of the normalized self-adaptive adjacency matrix on PeMSD4.
Architecture	MAE	RMSE	MAPE (%)
TAMP-S2GCNets	17.58	28.56	11.01
TAMP-S2GCNets with plain adjacency matrix	17.69	28.66	11.74
Table 14: Ablation study of the normalized self-adaptive adjacency matrix on PeMSD8.
Architecture	MAE	RMSE	MAPE (%)
TAMP-S2GCNets	13.77	21.70	8.99
TAMP-S2GCNets with plain adjacency matrix	13.94	21.98	9.09
Remark F.3 (Global Average Pooling and Global Max Pooling). We utilize the global average
pooling (GAP) to preserve information about the EUler-Poincare surfaces in a fixed-size representa-
tion through averaging of the learnt topological embeddings. In our experiments, we find that adding
global max pooling (GMP) tends to strengthen the Euler-Poincare surfaces representation and to im-
prove the overall performance. Before GAP and GMP, We first use CNN base models (i.e., fθ` (∙)
and fθ2(∙)) to learn the topological features of Euler-Poincare surfaces (i.e., fθi({Ei}T=ι), where
i = {1, 2}); then We employ GAP and GMP to the outputs of CNN base models, respectively. We
have conducted the ablation study to evaluate the performance of TAMP-S2GCNets without (w/o)
GAP or GMP on Golem. From Table 15, we find that applying both GAP and GMP enhances
prediction performance.
Table 15: Ablation study on global average pooling and global max pooling. Here * denotes signif-
icant results.
Architecture	TAMP-S2GCNets	TAMP-S2GCNets w/o GAP	TAMP-S2GCNets w/o GMP
MAPE (%)	*20.10	20.83	20.57
Remark F.4 (Supra-Laplacian). In our experiments, for large-scale networks, we apply sparse
sampling strategy for the supra-Laplacian construction. For example, on transportation network
such as PeMSD3, we randomly select τ = 3 timestamps out of the T = 12 timestamps in the slid-
ing window; as a result, we feed the sparse supra-Laplacian (i.e., 3N × 3N; where N is the number
of nodes) into the supragraph diffusion convolutional layer. For smaller networks such as Ethereum
token networks, we utilize the T timestamps (i.e., the whole sliding window information) to con-
struct the supra-Laplacian. In the future we will explore various graph sparsification techniques,
e.g., subsampling, to reduce computational costs for larger networks.
Remark F.5 (More Details on the Framework of TAMP-S2GCNets in Figure 1). In Figure 1,
the input is the target network over past 3 time slices ({Gt-2, Gt-1, Gt}) and our TAMP-S2GCNets
consists of 5 components. That is, (i) equipped with node feature matrix Xt , we apply spatial graph
convolutional layer (i.e., GCN Layer in Figure 1) on Gt extracts spatial information at time t; (ii)
feature transformation (i.e., FT in Figure 1) learns representation of the spatio-temporal data XT
over a sliding window of size T (in Figure 1, for better understanding the operation procedure, we
26
Published as a conference paper at ICLR 2022
set T = 3, i.e., XT = {Xt-2, Xt-1, Xt}); (iii) we create a multiplex network based on the in-
put networks (i.e., {Gt-2, Gt-1 , Gt}) and construct the corresponding supra-Laplacian; lastly, we
feed the supra-Laplacian into supragraph convolutional module (i.e., Supra GCN Layer in Figure
1); (iv) in Figure 1, We consider two types of Euler-Poincare Surface Representation (DEPS), i.e.,
DEPSf,g and DEPSf,h are EUler-Poincare surface representations under different multifiltrations,
where f, h, g represent different types of filtrations; in (iv), we feed EUler-PoinCare surface repre-
sentations into CNN base models and employ global average/max pooling to the outputs of CNN
base models. Finally, we combine these embeddings to obtain the final embedding, where is fed into
GRU module for multi-step forecasting.
G Additional Computational Complexity and Training Time
Comparison
Remark G.1 (Training Time Comparison). We report the average training time (per epoch in sec-
onds) for our TAMP-S2GCNets and 4 baselines on PeMSD4 and Decentraland datasets in Tables 16
and 17. In terms of training time, TAMP-S2GCNets runs slightly slower than the baselines but the
demonstrated running time is comparable to the baselines. Although the time per epoch is slightly
higher for TAMP-S2GCNets, we find that the performance of TAMP-S2GCNets is significantly
better than the state-of-the-art baselines. As such, this extra time appears to be worth it.
Table 16: Average training time (per epoch in seconds) for our TAMP-S2GCNets and 4 baselines
on PeMSD4 dataset, where ** denotes statistically significant results.
	TAMP-S2GCNets	StemGNN	Z-GCNets	AGCRN	DCRNN
Average time	40.30 s	30.12 s	37.53 s	28.05 s	28.79 s
RMSE	** 28.56	31.83	29.08	29.17	38.12
Table 17: Average training time (per epoch in seconds) for our TAMP-S2GCNets and 4 baselines
on Decentraland dataset, where *** denotes highly statistically significant results.
	TAMP-S2GCNets	StemGNN	Z-GCNets	AGCRN	DCRNN
Average time	3.10 s	2.55 s	2.09 s	2.03 s	2.21 s
MAPE (%)	*** 19.89	28.37	23.81	26.75	27.69
Remark G.2 (More Details on Computational Complexity of Topological Summaries). Com-
putational complexity of Dynamic Euler-Poincare Surfaces (DEPS) is much lower than that of per-
sistence diagrams. This is because Euler Characteristics is an alternating sum of the Betti numbers,
and one does not need to compute the persistence diagrams to compute the Betti numbers. The com-
putational complexity of a single persistence diagram PDk is O(N 3), where N is the number of
k-simplices (Otter et al., 2017). On the other hand, by using sparse matrix methods, computational
complexity for the Euler Characteristics ofa simplicial complex with M simplices is O(M) (Edels-
brunner & Parsa, 2014). Ifp is the resolution size of the multipersistence grid, the resulting complex-
ity for DEPS is O(p2M). However, since one does not need to compute the persistence diagrams
to obtain the Betti numbers, faster algorithms are possible. For instance, Lesnick & Wright (2019)
developed a much faster algorithm (RIVET software) to compute Betti numbers in multipersistence
setting. This reduction approach leads to computational complexity of O(R3), where R is the size
of the filtered complex giving the minimal representation for the bipersistence module and R is far
less than the original number of simplices. Furthermore, by utilizing sparse matrix reductions, re-
cent results of Kerber & Rolle (2021) significantly improved the RIVET approach. In particular,
computational complexity for multipersistence landscapes with RIVET would be O(R5), where R
is again the size of the reduced filtered complex (Vipond, 2020).
We have provided both computational complexity and running time of 8 topological summaries on
Decentraland and Golem (see Table 18).
The computational complexity of the overall approach is: O(N2 + NTFNQsup + NT 2Qsup/2 +
Qsup P'=t-τ M(') + R3 + Wgru). That is, we have: (i) spatial graph convolution module: O(N2)
27
Published as a conference paper at ICLR 2022
Table 18: Computational complexity and running time (in seconds) comparison on Decentraland
dataset.
Persistence summarya	Running time	MAPE	Computational complexity
MP-I(Carriere & Blumberg, 2020)	366.71 S	22.58	O(p2QB) b
MP-L (Vipond, 2020)	489.50 S	23.10	O(R5) c
DEPSDeg & Btwns	43.25 S	21.13	O(p2M)
DEPSDeg & PowerTrns	36.90 s	19.89	or
DEPSBtwns & PowerTrns	25.34 S	20.00	O(R3) d
PIDeg (Adams et al., 2017)	11.43 s	23.81	
PIBtwns (Adams et al., 2017)	10.01 s	24.93	O(p2 ) e
PIPowerTrns (Adams et al., 2017)	12.79 S	25.18	
aDeg: node degree; Btwns: node betweenness centrality; Power: edge weight; Trns: transaction be-
tween addresses. Please refer to on types of filtrations (Appendix D.2) for a more detailed description of types
of multifiltration.
bWhere p is the resolution of MP-I, Q is the number of lines, B is the maximum number of bars of the
fibered barcodes.
cWhere R is the size of the reduced filtered complex (see previous paragraph). M is the number of simplices
in the simplicial complex.
dThe reported time for DEPS is given with the O(p2 × M) algorithm.
eThis computational complexity is for the time after the computation of persistence diagrams.
(where N is the number of nodes); (ii) supragraph diffusion convolution module: O(NTFNQsup +
N T 2Qsup∕2 + Qsup P'=1-τ M (')) (where FN is the number of node attribute features, T denotes
the sliding window size, Qsup is the output dimension of the supragraph diffusion convolutional
layer, and M(') is the number of edges at the '-th layer); (iii) DEPS: O(R3), where R is the size of
the reduced filtered complex; (iv) GRU: O(WGRU), where WGRU is the total number of parameters
in the GRU.
28