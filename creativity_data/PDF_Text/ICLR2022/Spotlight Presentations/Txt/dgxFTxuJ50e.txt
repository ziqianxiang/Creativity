Published as a conference paper at ICLR 2022
Learnability of convolutional neural net-
WORKS FOR INFINITE DIMENSIONAL INPUT VIA MIXED
AND ANISOTROPIC SMOOTHNESS
Sho Okumoto1:，Taiji Suzuki1,2#
1	Graduate School of Information Science and Technology, the University of Tokyo
2	RIKEN Center for Advanced Intelligence Project
*lebesgue0118@gmail.com, ^taiji@mist.i.u-tokyo.ac.jp
Ab stract
Among a wide range of success of deep learning, convolutional neural networks
have been extensively utilized in several tasks such as speech recognition, im-
age processing, and natural language processing, which require inputs with large
dimensions. Several studies have investigated function estimation capability of
deep learning, but most of them have assumed that the dimensionality of the input
is much smaller than the sample size. However, for typical data in applications
such as those handled by the convolutional neural networks described above, the
dimensionality of inputs is relatively high or even infinite. In this paper, we in-
vestigate the approximation and estimation errors of the (dilated) convolutional
neural networks when the input is infinite dimensional. Although the approxima-
tion and estimation errors of neural networks are affected by the curse of dimen-
Sionanty in the existing analyses for typical function spaces such as the Holder
and Besov spaces, we show that, by considering anisotropic smoothness, they can
alleviate exponential dependency on the dimensionality but they only depend on
the smoothness of the target functions. Our theoretical analysis supports the great
practical success of convolutional networks. Furthermore, we show that the di-
lated convolution is advantageous when the smoothness of the target function has
a sparse structure.
1	Introduction
Deep learning has shown high performance in several tasks such as image recognition, speech recog-
nition, and natural language processing. In particular, convolutional neural networks (CNNs) and
dilated CNNs have been quite effective in tasks involving high-dimensional data (van den Oord
et al., 2016; He et al., 2016; Simonyan & Zisserman, 2015; Yoon, 2014). However, many aspects
of its theoretical nature are still unclear while related theoretical studies have attracted much atten-
tion. Aside from the analysis of CNNs, one of the most fundamental issues in deep learning theories
is its function approximation and estimation capabilities. For example, it is well known that any
continuous function with compact support can be approximated with arbitrary accuracy by a two-
layer fully connected neural network (Cybenko, 1989; Hornik, 1991). Moreover, the representation
ability of deep learning to approximate a function in some function classes such as Holder classes
has also been extensively analyzed (Mhaskar & Micchelli, 1992; Mhaskar, 1993; Chui et al., 1994;
Mhaskar, 1996; Pinkus, 1999; Yarotsky, 2017; Petersen & Voigtlaender, 2017). In addition to the
approximation ability, the estimation ability of deep learning for estimating a function by a finite
sample has also been extensively studied. For example, Schmidt-Hieber (2020) derived the estima-
tion error bound of deep learning with ReLU activation (Nair & Hinton, 2010; Glorot et al., 2011) to
estimate functions in the Holder space and showed the rate of convergence achieves the (near) min-
imax optimal rate. Suzuki (2019) derived approximation and estimation error rates of deep learning
with ReLU activation for the Besov spaces, which were also shown to be (near) minimax optimal.
Although the derived rates of convergence are near optimal, these studies assumed that the dimen-
sionality of inputs is fixed and much smaller than the sample size. Indeed, the derived rates suffer
from the curse of dimensionality. However, in practice, we often encounter settings where the input
1
Published as a conference paper at ICLR 2022
dimensionality is larger than the sample size or even infinite. For example, in image recognition and
natural language processing, the dimensionality of inputs (images or texts) is very large, and they
could be seen as almost infinite dimensional.
To address this issue, some researches considered a setting where the dimensionality of the support
of the data distribution is low dimensional. Chen et al. (2019b;a) considered a setting where data
can be embedded in a low dimensional sub-manifold and derived the approximation error of func-
tions that depends merely on the dimensionality of the sub-manifold instead of that of the entire
space. Nakada & Imaizumi (2020) also considered a similar setting, and showed that the estimation
error is characterized by the Minkowski dimension of the support of the data distribution. Suzuki
(2019) showed that, even if the data cannot be embedded in a low dimensional manifold, anisotropic
smoothness of the target function can mitigate the curse of dimensionality. Although these studies
revealed that deep learning can avoid curse of dimensionality by utilizing some low dimensional
structures of data and the target functions, it still remains unclear how deep learning performs for
very high dimensional settings including an infinite dimensional setting. See Table 1 for a summa-
rized comparison to existing studies.
In terms of infinite dimensional inputs, there have been already several studies on approximation and
estimation errors for non-deep-learning methods. For example, so called hyperbolic cross approx-
imation has been considered to approximate a function in a tensor product space with support on
[0,1]∞ (Dung & GriebeL 2016) and a polynomial order approximation is possible for functions with
mixed smoothness, that is, specific summability properties of the smoothness indices are fulfilled.
Ingster & Stepanova (2011) analyzed a Gaussian white noise model with an infinite dimensional in-
put and showed that the estimation accuracy for signals on infinite dimensional anisotropic Sobolev
spaces depends on the reciprocal sum of the smoothness per axis (see also Ingster & Stepanova
(2006); Ingster & Suslina (2007); Ingster & Stepanova (2009)). Oliva et al. (2013; 2015) proposed
methods to estimate a map where the input and output are functions or distributions, and derive the
rate of convergence. Ferraty et al. (2007) analyzed the Nadaraya-Watson estimator when the inputs
are functions, derived the convergence rate of the estimator, and gave the asymptotic confidence
band in the context of functional data analysis (see Ling & Vieu (2018) as a comprehensive survey
of the nonlinear functional data analysis literature). However, these researches are not for the deep
learning and the benefit of deep learning for such situation has not been well characterized in the
literature.
In this study, we analyze the approximation and estimation accuracy in a setting where the input
is infinite dimensional, and derive their convergence rates. We assume that the true function has
mixed and anisotropic smoothness, that is, the function has different smoothness toward different
coordinate similarly to Dung & Griebel (2016); Ingster & Stepanova (2011). The intuition behind
this setting is as follows: Considering a function which takes an image as an input, an image can
be decomposed into different frequency components and usually a function of images has less sen-
sitivity on the high frequency components and more dependent on the low frequency components,
which can be formulated as non-uniform smoothness toward each coordinate direction. By consid-
ering such a setting, we can show that the rate of convergence can avoid the curse of dimensionality
and be of polynomial order. Our contribution can be summarized as follows:
1. We consider a learning problem in which the target function to be approximated or estimated
can take an infinite dimensional input and has mixed or anisotropic smoothness. We show that
deep learning by fully connected neural networks can achieve approximation and estimation
2. eWrreoraslsdoecpoennsdiednetroanlsyetotinngsmwohoetrhentehses osmf tohoethtanregsest ofufntchteiotnaragnedt fi 2unndcetpioenndheanst aofstphaersdeimsteruncsitounre.,
and then we show that dilated CNNs can find appropriate variables and improve the rate of
convergence. This indicates that CNNs can capture a long range dependence among the input.
These results show that even when the dimension d of the data is very large compared to the number
of observations n, or even when the input is infinite dimensional, itis possible to derive a polynomial
order estimation error bound that depends only on the smoothness of the function class. This analysis
partially explains the great success of CNNs in various applications with high dimensional inputs.
2 Problem Setting and notations
In this section, we prepare the notations and introduce the problem setting. Throughout this pa-
per, we use the following notations. Let R>0	:=	{s ∈ R :	s > 0}, and for	a set D, let D∞	:=
{(s1, . . . , si, . . . ) : si ∈ D} (for example, R∞	:=	{(si)i∞=1	: si ∈ R (∀i	= 1, 2, . . . )}).	For
2
Published as a conference paper at ICLR 2022
Table 1: Comparison of this work and existing work on theoretical analyses of deep learning for high
dimensional data. a = (ai)∞=ι is a smoothness parameter, a := (P∞=ι a-1)-1, V = Qip — 1/2)+,
S = aι =…=ad and D is the dimensionality of low dimensional structure.
Function class	mixed smooth (d《n)	anisotropic smooth (d《n)	low-dim data
Author	Suzuki (2019)	Suzuki & Nitanda (2021)	Nakada & Imaizumi (2020); Schmidt-Hieber	(2019); Chen et al. (2019b;a)
Rate	(n/ log(n)d-1)- 2s+1	2a —	 n 2a+1	2s n 2s+D
Function class	mixed smooth (d = ∞)	anisotropic smooth (d = ∞)	
Author	This work	This work	
Rate	2(ai-vΓ~	2(a-V)-	
	n 2(aι -v)+1	n 2(a-v)+1	
s ∈ R∞ , let supp(s) = {i ∈ N : si 6= 0}. Let N0∞ := {l ∈ (N ∪ {0})∞ : supp(l) < ∞} and
define Z0∞ and R0∞ in the same way. Furthermore, for s ∈ R0∞ , let 2s := 2Pi∞=1 si. For L ∈ N, let
[L] = {1, . . . , L}. For a ∈ R, let bac be the largest integer less than or equal to a.
2.1	Regression problem with infinite dimensional predictor
In this paper, we consider a regression problem where the predictor (input) is infinite dimensional.
Let λ be the uniform probability measure on ([0, 1], B([0, 1])) where B([0, 1]) is the Borel σ-field
on [0, 1], and let λ∞ be the product measure of λ on ([0, 1]∞ , B([0, 1]∞ )) where B([0, 1]∞ ) is
the product σ-algebra generated by the cylindric sets ∩j≤d{x ∈ [0, 1]∞ : xj ∈ Bj} for d =
1, 2, . . . and Bj ∈ B([0, 1]). Let PX be a probability measure defined on the measurable space
([0, 1]∞ , B([0, 1]∞ )) that is absolutely continuous to λ∞ and its Radon-Nikodym derivative satisfies
kddpX∣∣L∞([o,i]∞) < ∞1. Then, suppose that there exists a true function fo : [0,1]∞ → R, and
consider the following nonparametric regression problem with an infinite dimensional input:
Y =fo(X)+ξ,	(1)
where X is a random variable taking its value on [0, 1]∞ and obeys the distribution PX introduced
above, and ξ is a observation noise generated from N(0, σ2) (a normal distribution with mean 0 and
variance σ2 > 0). Let P be the joint distribution of X and Y obeying the regression model.
What we investigate in the following is (i) how efficiently we can approximate the true function fo
by a neural network, and (ii) how accurately deep learning can estimate the true function fo from
n observations Dn = (Xi, yi)in=1 where (Xi, yi)in=1 are i.i.d. observations from the model. As a
performance measure, we employ the mean squared error ∣f — fo∣2P := EP [(f(X) — fo(X))2],
which can be seen as the excess risk of the predictive error E(χ,γ)〜P [(f (X) — Y )2] associated with
the squared loss (i.e., ∣∣f — fo∣∣pχ = E(χ,γ)〜P[(f(X) — Y)2] — E(χ,γ)〜P[(fo(X) — Y)2]=
E(X,Y)〜P [(f(X) — Y)2] — inff :measurable E(X,Y)〜P [(f(X) — Y)2]).
2.2	Mixed and anisotropic smoothness on infinite dimensional variables
Here, We introduce a function class in which We suppose the true function fo is included. For a given
(√2cos(2π∣li∣x)	(li < 0),
l ∈ Z∞,	define	ψii	: [0,1] → R as ψii (x)	= <	√2sin(2π∣l∕x)	(li	>	0),	for X ∈ [0,1], and
∖ 1	(Ii= 0),
define ψl(X) := Qi∞=1 ψli (xi) forX = (xi)i∞=1 ∈ [0, 1]∞ . Let L2([0, 1]∞ ) := {f : [0, 1]∞ → R :
R[0,1]∞ f2 (x)dλ∞ (x) < ∞} equipped with the inner product hf, gi := R[0,1]∞ f(x)g(x)dλ∞ (x)
for f, g ∈ L2([0, 1]∞ ). Then, (ψl)l∈Z∞ forms a complete orthonormal system of L2([0, 1]∞ ), that
is, f ∈ L2([0, 1]∞ ) can be expanded as f(X) = Pl∈Z∞ hf, ψliψl (X) (see Ingster & Stepanova
(2011) for example). For s ∈ N0∞ , let δs(f) : R∞ → R be
δs(f )(∙)=	X	hf,ψliψl(∙),
l∈Z∞：b2si-1C≤|li ∣<2si
1This is a rather strong assumption. We can omit this if we take θ = 1 and p = ∞ for Fpγ,θ . However, we
don’t pursue this direction in this study.
3
Published as a conference paper at ICLR 2022
which can be seen as the frequency component of f of frequency |li | ' 2si toward each coordinate.
1/p
We also define ∣∣f kp := (/0 χ∞ |f ∣pdλ∞)	for P ≥ 1. Then, We define a function space with a
general smoothness configuration as follows.
Definition 1 (Function class with γ-smoothness). For a given γ : N0∞ → R>0 which is monoton-
ically non-decreasing with respect to each coordinate. For p ≥ 1, θ ≥ 1, we define the γ-smooth
space as
Fpγ,θ([0,1]∞) := f= = X hf,Ψ1iΨ1 : ( X 2θγ(s)kδs(f)kP)"θ < ∞
l∈Z0∞	s∈N0∞
equipped with the norm ∣f ∣Fγ := (Ps∈N0∞ 2θγ(s) ∣δs(f)∣θp	.
In the following, Fpγ,θ([0, 1]∞) is abbreviated to Fpγ,θ, and its unit ball is denoted by U (Fpγ,θ).
Remind that δs (f) represents the frequency component associated with the frequency (2si )i∞=1, and
then the norm of the γ-smooth space imposes weight 2θγ(s) on each frequency component associated
with s. In that sense, γ(s) controls the weight of each frequency component and accordingly a
function in the space can have different smoothness toward different coordinates. As a special case
ofγ(s), we investigate the following ones in this paper. We can see that a finite dimensional analysis
can be easily reduced to a special case of the infinite dimensional analysis (see Appendix A). In that
sense, our analysis generalizes existing finite dimensional analyses.
Definition 2 (Mixed smoothness and anisotropic smoothness). Given a monotonically non-
decreasing sequence a = (ai)i∞=1 ∈ R>∞0, we define the mixed smoothness as
(mixed smoothness)	γ(s) = ha, si,
where ha, si :=	i∞=1 aisi2, and define the anisotropic smoothness as
(anisotropic smoothness)	γ(s) = max{aisi : i ∈ N}.
Each component ai of a = (ai)i∞=1 represents the smoothness of the function with respect to the
variable xi. Since we assumed (ai)i∞=1 is monotonically non-decreasing, a function in the space has
higher smoothness toward the coordinate xi with higher index i. In other words, the function f in the
space is less sensitive to the variable xi with a larger index i. For example, in computer vision tasks,
we may suppose xi with a large index i corresponds to a higher frequency component of the input
image, and then the function is less sensitive to such high frequency components and more sensitive
to a low-frequency “global” information. This can be seen as an infinite dimensional variant of the
mixed smooth Besov space (Schmeisser, 1987; Sickel & Ullrich, 2009) and the anisotropic Besov
space (Nikol’skii, 1975; Vybiral, 2006; Triebel, 2011) (see Appendix C for detailed discussions). In
our theoretical analysis, we will assume that the true target function fo is included in the γ-smooth
function space.
Assumption 3. The target function satisfies fo ∈ U (Fpγ,θ) with p ≥ 1 and θ ≥ 1, and ∣fo∣∞ ≤
Bf for a fixed constant Bf > 0, where the smoothness γ is either the mixed smoothness or the
anisotropic smoothness.
3 Relation to existing work
A function space with the mixed smoothness in a finite dimensional setting can be found in
Schmeisser (1987); Sickel & Ullrich (2009), in which the mixed smooth Besov space is defined.
The approximation and estimation errors of deep neural networks for the mixed smooth Besov space
were analyzed by Suzuki (2019) for a special setting of aι =…=ad, and an approximation error
analysis for aι =…=a& = 2 was given by Montanelli & Du (2019) using a SParse-grid technique.
The mathematical properties of the anisotropic Besov space with finite dimensional input were ana-
lyzed in Nikol’skii (1975); Vybiral (2006); Triebel (2011). The statistical analysis on the anisotropic
Besov space can be dated back to Ibragimov & Khas’minskii (1984) and they derived the minimax
2Note that, since the number of nonzero components of s ∈ N0∞ is finite, the summation always converges.
For the same reason, the maximum in the anisotropic smoothness is also attained by some finite index i.
4
Published as a conference paper at ICLR 2022
optimal rate for density estimation where the density is in an anisotropic Besov space. Nyssbaum
(1983; 1987) also analyzed a nonparametric regression problem on an anisotropic Besov space. The
approximation and estimation error bounds by deep neural networks for composition functions in
anisotropic Besove spaces and superiority of deep learning compared to the kernel methods are
shown by Suzuki & Nitanda (2021). However, all of these studies are about finite dimensional input
and it is far from trivial to generalize it to the infinite dimensional setting.
Our analysis for the γ-smooth function space is closely related to Ingster & Stepanova (2011) in
which the anisotropic Sobolev space defined by Fc = Wa := {f ∈ L2 ([0,1]∞) : P∞=ι ∣∣ ∂aai∣∣2 <
xi
∞ is analyzed. They also derived a similar convergence rate to ours for non-deep learning estimator
for a Gaussian white noise model. In the literature of the functional data analysis, the Nadaraya-
Watson estimator for functional input has been extensively studied (Ferraty et al. (2007) and Ling
& Vieu (2018) for a comprehensive survey). If we apply the bound given in the literature to our
setting, the learning rate can be 1/poly- log(n) which is much slower than our analysis. This is
because their analysis does not make use of γ-smoothness. See Appendix C for more details.
Kohler & Langer (2020) analyzed CNNs in a setting where the target function has a hierarchical
max-pooling structure each layer of which is sufficiently smooth. On the other hand, our γ-smooth
function class imposes smoothness more directly on the target function. Liu et al. (2021) analyzed
learning ability of CNNs with a ResNet structure in a classification task where the data are distributed
on a low-dimensional manifold and established a rate which only depends on the dimensionality of
the low dimensional manifold. However, the input should be distributed on a low dimensional
manifold, while our analysis allows its support to be infinite dimensional. Estimation errors on a
low dimensional structure also have been studied in Yang & Dunson (2016); Bickel & Li (2007);
Nakada & Imaizumi (2020); Schmidt-Hieber (2019); Chen et al. (2019b).
4 Definition of a dilated convolutional neural network
In this section, we introduce the neural network model that we investigate in this paper. Let L ∈
N be the depth of the network and di (i = 1, . . . , L + 1) be the width of the i-th layer in the
network where we set dL+1 = 1. Then, the fully connected neural network (FNN) can be given
by (ALη(∙) + bL) ◦…O (Ain(∙) + bi) ◦…Q (AiX + bi) where Ai ∈ Rdi+1×di, b ∈ Rdi+1 and
η(x) = max{x, 0} is the ReLU activation function that is applied element-wise. The set of FNN
with depth L ∈ N, maximum width W ∈ N, norm bound B > 0, and sparsity level S ∈ N is defined
by
Φ(L, W, S, B):= {f (x) = (ALn(∙) + bL) O…。(Ain(∙) + bi) ◦…O (Ai X + bi):
L
max kAik∞ ∨ kbik∞ ≤ B, X kAik0 + kbik0 ≤ S, max di ≤ W ,
i=i,...,L	i=i,...L
i=i
where ∣∣∙∣∣∞ is the maximum absolute value among the elements of a vector or matrix3, and ∣∣ ∙ ko
is the number of non-zero elements of a vector or matrix.
Next, we define the (dilated) CNNs. Let C ∈ N be the number of channels and RC×∞ :=
{(xi,...Xi,...) : Xi ∈ RC}. Suppose that W ∈ Rc×w0 is a filter with a width W0 ∈ N, channel
size C ∈ N and an interval h ∈ N, then define the dilated convolution w?h X0 ∈ R∞ for an infinite-
sequence of vectors X0 = (X0i,j)iC=,∞i,j=i ∈ RC×∞ as (w?h X0)k = PiC=i PjW=i wi,jX0i,h(j-i)+k.
When h = 1, it is called a normal convolution. Moreover, given a filter F ∈ RC0 ×C×W0 with (C0)-
multiple channel outputs, we define its corresponding convlution Convh,F : RC×∞ → RC0×∞ as
(Fl,：,： ?h X0
Convh,F (X0)=	.
FC0,：,： ?h X0
Then, the dilated CNN can be defined as follows (its illustration can be found in Figure 1).
3We define a ∨ b := max{a, b} and a ∧ b := min{a, b} for a, b ∈ R.
5
Published as a conference paper at ICLR 2022
Definition 4 (Dilated CNN). For a given L0 , W0 ∈ N, suppose that we are given fil-
ters Fl ∈ RCl+1 ×Cl ×W0 with the number of channels Cl ∈ N (l ∈ [L0]) with
C1 = 1 and an FNN gFNN ∈ Φ(L, W, B, S), then a neural network given by f(X) =
(gFNN ◦ ConvW0L0-ι FLo ◦•••◦ ConvWtl-i ,Fi ◦…。Convι,Fι ◦ X)is called a dilated CNN4,
where gFNN is assumed to be applied in an element-wise manner to the infinite sequence. The set of
dilated CNNs with the same number of channels Cl = C0 (2 ≤ ∀l ≤ L0) in all layers but C1 = 1 is
denoted by
P(L0, B0, W0,C0,L, W, S,B) = { (gFNN ◦ ConvW，工，一,土 ◦…。Convι,Fι ◦ X)ɪ
Fl ∈RC0×C0×W0 (l ≥2), F1 ∈ RC0×1×W0, kFlk∞ ≤B0, gFNN ∈Φ(L,W,B,S
For simplicity, the set of dilated CNNs is abbreviated to P when there is no ambiguity about the
parameter configuration. When L0 = 1, it coincides with a set of regular CNNs. In our anal-
ysis, it is sufficient to consider an dilated CNN with a constant number of channels throughout
all layers (Cl = C (∀l ∈ [L0])). To evaluate the estimation accuracy, it is important to as-
sume the functions in the set is bounded in terms of the L∞-norm. For that purpose, we con-
Sider an dilated CNN clipped by a bound Bf > 0 defined as P(Bf ,L0, B0, W0, C, L, W, S, B):=
{f(X) = (-Bf ∨ (Bf ∧ f(X))) : f ∈ P (L0, B0, W 0, C, L, W,S,B)}.
Remark 5. In the definition of the dilated CNN, we do not impose ReLU activation. However, since
ReLU activation can realize a linear function for a bounded input and thus our analysis can be
straightforwardly applied even if there is nonlinear ReLU activation. Moreover, this paper mainly
focuses on 1D-convolution, but it can be generalized to 2D-convolution. See Appendix I for the
detailed discussions in which it is shown that γ-smoothness over a wavelet decomposition of an
input image achieves the same rate of convergence as in 1D-convolution.
5 Approximation and estimation errors of deep learning
In this section, we give our main result about the approximation and estimation errors of FNNs and
dilated CNNs when the true function fo is in the γ-smooth function class.
5.1	Approximation Error Analysis by Fully Connected Neural Networks
Here, we present the approximation error analysis of FNNs for a general smoothness Y not restricted
to the mixed/anisotropic smoothness. For a given T > 0 and the smoothness Y : N0∞ → R>0, define
I(T,Y) := {i ∈ N : 玉 ∈ N∞, si =0, Y(S) <T},
and then the following quantities play an important role in our approximation error analysis.
Definition 6 (Axial complexity and frequency direction complexity). The axial complexity is de-
fined by dmax(T, Y) := |I(T, Y)|. Moreover, the frequency direction complexity is defined by
fmax(T,Y) := maxs∈N0∞: γ(s)≤T maxi∈N si.
The axial complexity is used to evaluate how many components need to be extracted from a given
infinite-dimensional sequence X ∈ R∞ to achieve a particular approximation error, and the fre-
quency complexity characterizes up to which frequency we require to approximate a target function
with a particular error. Let
V := (P - 2) + , α(Y) := suPs∈N∞ Pγ=Ti, G(T, Y) := Ps∈N∞: γ(s)<T 2s,
where (x)+ := max{x, 0}. Then, a general approximation error theory for FNNs can be obtained
as follows.
Theorem 7 (Approximation error for the Y-smooth space by FNNs). Assume that Y, Y0 : N0∞ →
R>0 satisfy
γ0(s) < γ(s), vα(γ) < 1, vα(γ0) < 1,
4Here, we employ h = W 0k-1 for the k-th layer convolution. This structure is useful to show its feature
extraction ability in Section 5.3.
6
Published as a conference paper at ICLR 2022
and the target function f ∈ Fpγ,θ (p ≥ 1, θ ≥ 1) to be approximated satisfies kf k∞ ≤ Bf for a
constant Bf ∈ R>0. For arbitrary T > 0, we let a tuple (dmax, fmax, G) be
(dmax(γ),fmax(γ),G(T,γ))	(1≤θ≤2),
(dmax(γ0),fmax(γ0),G(T,γ0))	(2<θ),
and with some positive constants K, K0 depending only on Bf, we let
L = 2K max {dMax，T2, (log G)2, log fmax} ,	W = 21dmaχG,
S = 1764Kdmax max {dMax"2, (log G)2, log fmaχ} G,	B = (√2)dmaxK:
Then, there exists an FNN RT ∈ Φ(L, W, S, B) with dmax-dimensional input that takes
(xi)i∈I(T,γ) ∈ [0, 1]dmax as an input such that f0 : [0, 1]∞ → R given by f0(X) :=
RT ((Xi)i∈i(τ,γ)) for X = (xi)∞=ι ∈ [0,1]∞ satisfies
2-(1-vα(γ))T kf kFpγ,θ	(1 ≤θ≤2),
2-(Ia(YO))T (PT≤γ0(s) 2θ-θ2(YO(S)-Y(S))) "2τ∕θ kf kFγ,θ	(2 <θ).
According to this theorem, the derived approximation error can be achieved by FNNs if the required
dmax components of the input X is extracted. This theorem clarifies how the decay rate of the
frequency components of the target function affects the approximation accuracy. Since the approxi-
mation accuracy is determined by (dmax, fmax, G), it is not directly affected by the dimensionality
but is characterized merely by the smoothness parameter γ. Intuitively, T > 0 controls the approxi-
mation accuracy and simultaneously controls up to which frequency is used for the approximation.
Specifically, the difficulty of the approximation is determined by the number of bases required that
is characterized by the number of s ∈ N0∞ with γ(s) < T, and the maximum frequency required for
the approximation is also important for the analysis. The bound is proven by evaluating an approx-
imation error of a trigonometric polynomial approximation of f ∈ FpY,θ and showing that we can
construct a neural network that approximates a trigonometric polynomial with a certain accuracy.
5.2 Smoothness with polynomial order increase
Here, we derive a concrete convergence rate for CNNs in a setting where γ is mixed or anisotropic
smoothness and the smoothness parameter a = (ai)i∞=1 is polynomially increasing. In this setting,
we just need to use only one layer CNN. Deeper CNN layers will be used in the next section (Section
5.3) to perform adaptive feature extraction from wide range of inputs.
Assumption 8. There exists 0 < q < ∞ such that the smoothness parameter a = (ai)i∞=1 satisfies
ai = Ω(iq). We also assume aι < a2 for the mixed smoothness setting.
This assumption impose that the target function should be sufficiently smooth with respect to higher
order indices. Under this setting, we show the approximation and estimation errors as follows. First,
the approximation error by the CNNs can be evaluated as follows.
Theorem 9 (Approximation error bound under smoothness with polynomial order increase). Sup-
pose that Assumptions 3 and 8 hold, then we have the following approximation error bounds:
1.	Mixed smoothness (γ (s) = ha, si): Suppose that v/a1 < 1. Then, for arbitrary T > 0, there
exists a configuration ofthe network structure, L0 = 1, B0 = 1, W0 〜Tq, C0 〜Tq and
Lι(T)〜max {t q ,T 2}, Wι(T)〜(q∞=2(1 - 2 -⅛a1) )-1) T12 T,
SI(T)〜(Q∞=2(1 — 2 -(aa-a1) )-1)T 2 max {t 2 ,T 2} 2 T, B1(T)〜(√2)t 1,
such that there exists an dilated CNN f0 ∈ P(L0, B0, W0, C0, L1(T), W1(T), S1(T), B1(T)) satis-
fying the following approximation error:
kf0- fok2 . 2-(1-a1 )T.
2.	AnisotroPic smoothness (γ(s) = maxi{aiSi}): Let α := (P∞=1 a-1)-1 and suppose 0 ‹ α and
v < a, then there exists a network structure setting L0 = 1, B0 = 1, W0 〜T1, C0 〜T1 and
L2(T)〜max {T2,T2}, W2(T)〜Tq2T/a, S2(T)〜T2 max {T∣,T2}2T/a, B2(T)〜(√2)t 1,
7
Published as a conference paper at ICLR 2022
such that there exists an dilated CNN f0 ∈ P(L0, B0, W0, C0, L2 (T), W2 (T), S2 (T), B2 (T)) satis-
fying the following approximation error: kf 0 一 fo ∣∣2 . 2-(1-v/a)T.
The proof can be found in Appendix E. From this theorem, we can see that the number of layers, the
width, the number of parameters, and the size of the parameters are both determined by T and the
smoothness parameter a. Moreover, in Theorem 7, the approximation error was derived assuming
that the appropriate index set I(T, γ) was provided. On the other hand, in Theorem 9, we do not
make such an assumption because the CNNs can automatically extract the required index I(T, γ).
Next, we consider the estimation error of these models in the regression problem (Eq. (1)). Suppose
that we are given n observations Dn = (Xi, yi)in=1 following the model (1). We consider the em-
Pirical risk minimization estimator (ERM estimator) in the model P that is given by any minimizer
of the empirical risk:
1n
f ∈ argmin	(f (Xi) ― yi)2.
f ∈p n i=1
As we have stated above, we employ the mean squared error ∣∣f ― f o∣pχ as a performance measure.
Since f depends on the training data Dn, we take expectation with respect to Dn: EPn [∣f ―
fokPX ] := E(Xi,yi)i=1 〜Pn [kf - fokPX ]. Then, the following theorem holds.
Theorem 10 (Estimation error under smoothness with polynomial order increase). Suppose that
Assumptions 3 and 8 hold, then we have the following estimation error bounds:
1.	Mixed smoothness (γ(s) = ha, si): If v/a1 < 1, then by setting the network structure as L0 =
1, B0 = 1, W0 〜(log n)1, C0 〜(log n)1 and (L,W,S,B) = (LI(T ),Wι(T ),Sι(T ),Bι (T))
for T = 2(aι-1v)+ι log2(n) ,the ERM estimator f in P(Bf, L , B , W 0, C 0, L, W,S,B) achieves
-(ai-aI)	_,、	2(a1 - V)	2 ,0	4
EPn [kf ― f °kPχ] . (Q∞=2(1 ― 2a1	)T)n 2(a1 -v)+1 (log n)q +2 max{(log n)q, (log n)4}.
2.	Anisotropic smoothness (γ(S) = maXi{aiSi}): Under the same setting, if V < a, by
setting the network structure as L0 = 1, B0 = 1, W0 〜(log n) 1, C0 〜(log n) 1 and
(L,W,S,B) = (L2(T),W2(T),S2(T),B2(T)) for T = 2(G-V)+1 log2(n), the ERM estimator
ʌ 一. . .
f in P(Bf ,L0, B0, W0, C0,L, W, S, B) achieves
2	C	2(α-v)	2 , o	4	Λ
EPn [kf - f°kPχ].n- 2(a-v)+1 (log n)q+2 max{(log n) 4, (log n)4}.
The proof can be found in Appendix F. This theorem shows that even if the dimension of the in-
put data is infinite, for a function with a particular smoothness, CNNs can achieve a dimension-
independent convergence rate which is a polynomial order, that is, it can avoid the curse of dimen-
sionality by utilizing the increasing smoothness. We can see that the derived convergence rate is a
direct extension of finite dimensional one. Actually, ifv = 0, the rate for the anisotropic smoothness
matches that of the finite dimensional one (Suzuki & Nitanda, 2021) up to poly-log order which is
known as minimax optimal. Therefore, CNNs can achieve the optimal rate up to poly-log order at
least when v = 0. As for the mixed smoothness, a finite dimensional version was analyzed (Suzuki,
2019) and a similar rate was derived. However, our analysis assumes aι < a? and ai = Ω(iq)
and thus obtained completely dimensionality independent bound while the bound by Suzuki (2019)
depends on d in the exponent of the poly-log order.
5.3 Smoothness with sparsity
Next, We relax the assumption a% = Ω(iq) and consider a situation where there is a kind of sparse
structure in a. As we have seen in the previous section, under the assumption that the coordi-
nates with large indices are not important, polynomial-order convergence rate depending only on
the smoothness can be achieved by the ordinary CNNs. In this section, we show that similar rates
can be achieved by using dilated CNNs even when a does not satisfy the polynomial order increase
if a has sparsity. For that purpose, we first define the sparsity of the smoothness.
Definition 11 (Weak 'q-norm of smoothness). Given a = (ai)∞=ι ∈ R∞0 which is not necessarily
monotonically increasing, consider the SOrted SequenCe 0 < a^ ≤ a.2 ≤ ∙∙∙ in the ascending
order Then, define its weak 'q-normfor 0 < q < ∞ as ka∣Wlq := SuPj jqa-1.
8
Published as a conference paper at ICLR 2022
This kind of sparsity inducing norm were introduced and discussed previously in Donoho (1993);
Donoho et al. (1996); Yang & Barron (1999) to quantify sparsity of coefficients of basis expansions.
We notice that, if kakwlq is small, almost all ai s are very large and there are only few indices that
are small, which means sparseness. If the smoothness parameter a has a small weak 'q-norm, then
we can say that the functions with such smoothness has a small number of important coordinate
directions. Therefore, it is expected that we can approximate such a function efficiently by a neural
network. In this section, we analyze the approximation and estimation errors under the condition of
sparse smoothness.
Assumption 12. a = (ai)∞=ι satisfies ∣∣a∣∣ Wlq ≤ 1 for 0 ‹ q < ∞ and a，i = Ω(log i).
Note that this assumption relaxes the condition ai = Ω(iq) in Assumption 8 to ai = Ω(logi).
Instead, it imposes the sparsity kakwlq ≤ 1. Under this assumption, we obtain the following ap-
proximation error bound.
Theorem 13 (Approximation error bound for sparse smoothness). Suppose that Assumptions 3 and
12 hold, then we have the following approximation error bounds for any T > 1:
1.	Mixed smoothness (γ (s) = ha, si): Suppose that v/ai1 < 1, then there exist a set of network
structure parameters satisfying5
L0 〜T, B0 = 1, W0 = 3, C0 〜T1,
such that there exists an dilated CNN f0 ∈ P(L0, B0, W0, C0, L1(T), W1(T), S1(T), B1(T)) satis-
fying kf0- f0k2 . 2—(1—a111,
2.	Anisotropic smoothness (Y(S) = maXi{aiSi}): Suppose that 0 < a and v < a, then there
exist a set of network structure parameters satisfying L0 〜T, B0 〜1, W0 = 3, C0 〜Tq such
that there exists an dilated CNN f0 ∈ P(L0, B0, W0, C0, L2 (T), W2 (T), S2 (T), B2 (T)) satisfying
kf0- f0k2 . 2—(1—v/a)T.
We can see that this approximation error bound gives the same bound as Theorem 9 under a re-
laxed condition Assumption 12 with sparse smoothness. The only difference is the setting of the
convolution part (L0 , W0 , C) and other parts are same as Theorem 9. This difference is required
to find the important indices that are relatively non-smooth compared with other indices. Thanks
to the structure of dilated convolution, it can find such indices from a long range of index set:
{i ∈ N : i = O(3L0 )} (see Figure 1 for illustration of feature extraction by CNNs). Accordingly,
we also have the following estimation error bound.
Theorem 14 (Estimation error for sparse smoothness). Suppose that Assumptions 3 and 12 hold,
then by setting L0 〜log n, B0 〜1, W0 = 3, C0 〜(log n)i and (L, W,S,B) as in Theorem
10, the ERM estimator f in the class of dilated CNNs can achieve the same convergence rate of the
estimation error as Theorem 10.
The proof can be found in Appendix G. These theorems show that for polynomially increasing
smoothness, ordinary CNNs can perform optimal coordinate selection, while for sparse smoothness,
dilated CNNs play an important role in coordinate selection. This theorem shows that, when extract-
ing data with long-term dependence, convergence rates that avoid dependence on dimensionality can
be achieved by using dilated CNNs.
6 Conclusion
In this study, we gave a condition on the smoothness of the function space as one of the situations in
which the curse of dimensionality can be avoided when the input is ultra-high dimensional (n d)
or infinite dimensional (d = ∞). This study showed that the smoothness of the target function plays
an essential role in characterizing the estimation error bound. Especially, when the smoothness pa-
rameter (ai)i∞=1 grows up as the index i increase, we can obtain a polynomial order convergence rate
even if the input is infinite dimensional. Future plans include, for example, considering a situation
where the smoothness depends on each input location, and extending the definition of Fpγ,θ so that
it captures more realistic situations.
5The choice W0 = 3 is not a strict requirement. It can be replaced by an arbitrary integer H with H ≥ e.
9
Published as a conference paper at ICLR 2022
Acknowledgment
This study was partially supported by JSPS KAKENHI (18H03201), Japan Digital Design and JST
CREST.
References
P. J. Bickel and B. Li. Local polynomial regression on unknown manifolds. In Complex datasets
and inverse problems, pp. 177-186. Institute of Mathematical Statistics, 2007.
M. Chen, H. Jiang, W. Liao, and T. Zhao. Nonparametric regression on low-dimensional manifolds
using deep ReLU networks. arXiv e-prints, art. arXiv:1908.01842, Aug 2019a.
M. Chen, H. Jiang, W. Liao, and T. Zhao. Efficient approximation of deep ReLU networks for
functions on low dimensional manifolds. In Advances in Neural Information Processing Systems,
volume 32, pp. 8174-8184, 2019b.
C.	Chui, X. Li, and H. Mhaskar. Neural networks for localized approximation. Mathematics of
Computation, 63(208):607-623, 1994.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems, 2(4):303-314, 1989.
I. Daubechies. Ten lectures on wavelets. Society for Industrial and Applied Mathematics, 1992.
D. L. Donoho. Unconditional bases are optimal bases for data compression and for statistical esti-
mation. Applied and computational harmonic analysis, 1(1):100-115, 1993.
D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet
thresholding. The Annals of Statistics, 24(2):508-539, 1996.
D. Dung and M. GriebeL Hyperbolic cross approximation in infinite dimensions. Journal of Com-
plexity, 33:55-88, 2016.
D.	Dung, V. Temlyakov, and T. Ullrich. Hyperbolic Cross Approximation. Springer International
Publishing, 2018.
F.	Ferraty, A. Mas, and P. Vieu. Nonparametric regression on functional data: inference and practical
aspects. Australian & New Zealand Journal of Statistics, 49(3):267-286, 2007.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In Proceedings of the
14th International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings
of Machine Learning Research, pp. 315-323, 2011.
S. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network
learning over sparse parameter spaces. Neural Networks, 123:343-361, 2020.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):
251-257, 1991.
I. Ibragimov and R. Khas’minskii. More on the estimation of distribution densities. Journal of Soviet
Mathematics, 25(3):1155-1165, 1984.
Y. Ingster and N. Stepanova. On estimation and detection of infinite-variable function. Journal of
Mathematical Sciences, 139(3):6548-6561, 2006.
Y. Ingster and N. Stepanova. Estimation and detection of functions from weighted tensor product
spaces. Mathematical Methods of Statistics, 18:310-340, 2009.
Y. Ingster and N. Stepanova. Estimation and detection of functions from anisotropic sobolev classes.
Electronic Journal of Statistics, 5:484-506, 2011.
10
Published as a conference paper at ICLR 2022
Y. Ingster and I. Suslina. Estimation and detection of high-variable functions from sloan-
WozniakoWski space. Mathematical Methods ofStatistics,16:318-353, 2007.
M.	Kohler and S. Langer. Statistical theory for image classification using deep convolutional neural
netWorks With cross-entropy loss. arXiv preprint arXiv:2011.13602, 2020.
N.	Ling and P. Vieu. Nonparametric modelling for functional data: selected survey and tracks for
future. Statistics, 52(4):934-949, 2018.
H. Liu, M. Chen, T. Zhao, and W. Liao. Besov function approximation and binary classification
on loW-dimensional manifolds using convolutional residual netWorks. In Proceedings of the 38th
International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning
Research, pp. 6770-6780. PMLR, 2021.
P. Lizorkin and S. Nikol’skii. Function spaces of mixed smoothness from the decomposition point
of vieW. Proceedings of the Steklov Institute of Mathematics, 187:163-18, 1990.
H. N. Mhaskar. Neural netWorks for optimal approximation of smooth and analytic functions. Neural
Computation, 8(1):164-177, 1996.
H. N. Mhaskar and C. A. Micchelli. Approximation by superposition of sigmoidal and radial basis
functions. Advances in Applied mathematics, 13(3):350-373, 1992.
H. N. Mhaskar. Approximation properties of a multilayered feedforWard artificial neural netWork.
Advances in Computational Mathematics, 1(1):61-80, 1993.
H. Montanelli and Q. Du. NeW error bounds for deep relu netWorks using sparse grids. SIAM
Journal on Mathematics of Data Science, 1(1):78-92, 2019.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceed-
ings of the 27th International Conference on Machine Learning, pp. 807-814, 2010.
R. Nakada and M. Imaizumi. Adaptive approximation and generalization of deep neural netWork
With intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1-38, 2020.
R.	Nessel and G. Wilmes. Nikolskii-type inequalities for trigonometric polynomials and entire
functions of exponential type. Journal of the Australian Mathematical Society, 25(1):7-18, 1978.
S.	M. Nikol’skii. Approximation of functions of several variables and imbedding theorems, volume
205. Springer-Verlag Berlin Heidelberg, 1975.
M. Nyssbaum. Optimal filtration of a function of many variables in White gaussian noise. Problems
of Information Transmission, 19:23-29, 1983.
M. Nyssbaum. Nonparametric estimation of a regression function that is smooth in a domain in Rk .
Theory of Probability & Its Applications, 31(1):108-115, 1987.
J. Oliva, B. Poczos, and J. Schneider. Distribution to distribution regression. In Proceedings of the
International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning
Research, pp. 1049-1057, 2013.
J. Oliva, W. Neiswanger, B. Poczos, E. Xing, H. Trac, S. Ho, and J. Schneider. Fast function
to function regression. In Proceedings of the Eighteenth International Conference on Artificial
Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pp. 717-
725, 2015.
D. Perekrestenko, P. Grohs, D. Elbrachter, and H. Bolcskei. The universal approximation power of
finite-Width deep ReLU netWorks. CoRR, abs/1806.01528, 2018.
P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep
ReLU neural networks. arXiv preprint arXiv:1709.05289, 2017.
A. Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica, 8:143-195,
1999.
11
Published as a conference paper at ICLR 2022
H.-J. Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness
properties. Analysis Mathematica, 13(2):153-168,19*7.
J.	Schmidt-Hieber. Deep ReLU network approximation of functions on a manifold. arXiv preprint
arXiv:1908.00695, 2019.
J.	Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics, 48(4):1875-1897, 2020.
W. Sickel and T. Ullrich. Tensor products of Sobolev-Besov spaces and applications to approxima-
tion from the hyperbolic cross. Journal of Approximation Theory, 161(2):748-786, 2009.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In International Conference on Learning Representations, 2015.
T. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representa-
tions, 2019.
T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic Besov space. In Advances in Neural Information Processing Systems, volume 34.
Curran Associates, Inc., 2021. to appear.
V. Temlyakov. Approximation of functions with a bounded mixed difference by trigonometric poly-
nomials, and the widths of some classes of functions. Mathematics of the USSR-Izvestiya, 20(1):
173-187, 1983.
H. Triebel. Entropy numbers in function spaces with mixed integrability. Revista matema´ tica com-
plutense, 24(1):169-188, 2011.
A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. W. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR,
abs/1609.03499, 2016.
A. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-
tions to Statistics. Springer, New York, 1996.
J. Vybiral. Function spaces with dominating mixed smoothness. Dissertationes Math. (Rozprawy
Mat.), 436:3-73, 2006.
S. Yanchenko. Approximation of the Nikol’skii-Besov functional classes by entire functions of a
special form. Carpathian Mathematical Publications, 12(1):148-156, 2020.
Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The
Annals of Statistics, 27(5):1564-1599, 1999.
Y. Yang and D. B. Dunson. Bayesian manifold regression. The Annals of Statistics, 44(2):876-905,
2016.
D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:
103-114, 2017.
K. Yoon. Convolutional neural networks for sentence classification. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing, pp. 1746-1751, 2014.
12
Published as a conference paper at ICLR 2022
-----Appendix
Notation lists
Table 2: Notation list
notation	definition
n (Xi yi) Dn = (Xi ,yi)n=ι fo a = (ai)∞=1 S = (si)i=1 ψl(X) = Q∞=1 ψli (Xi) δs(f) k∙k2 k∙kPχ Fp,θ([0,1]∞) Y(S) Y(S) = ha, si γ(s) = max{aiSi : i ∈ N} η Ψ(L,W,S,B) P (L0,B0,W 0,C 0,L,W,S,B) I (T,Y) dmax(T, Y) fmax(T, Y) V α(Y) G(T,Y)		sample size i-th observation (Xi： input, yi： output) training data the true function smoothness with respect to each coordinate frequency with respect to each coordinate trigonometric orthonormal basis functions basis function expansion of f for l ∈ Z∞ such that b2si-1C ≤ |li | < 2si L2-norm with respect to the uniform distribution (kf k2 := P∫f(X )2dλ∞(X))		 L2-norm with respect to PX (kfk PX := √Eχ^Pχ [f (X)2]) Y-smooth function class penalty on each frequency component mixed smoothness anisotropic smoothness ReLU activation function set of fully connected networks with depth L, width W, sparsity level S, norm bound B set of dilated CNNs with depth L0, filter width W0, channel size C0 accompanied with an FNN in Ψ(L,W,S,B) the set of features contributing a frequency component S with Y(S) < t |I(T, y) |: axial complexity maXs∈N∞: γ(s)≤τ maxi∈N s，frequency direction complexity iP - 1 1 P∞ si suPs∈N∞ ^=Γ Es∈N∞: γ(s)<T 2s	
A Connection to finite dimensional input setting
We can easily see that the analysis in our paper can be directly applied to a setting where the input
is finite dimensional (say, d-dimensional). Let
Jd := {s ∈ N0∞ : si = 0 (i = d + 1, . . . )} ,
and suppose that γ(s) : N∞ → R>0 ∪ {∞} satisfies γ(s) < ∞ (s ∈ Jd) and γ(s) = ∞ (s ∈/ Jd).
If we set
Fp,θ,d([0,1]∞) := {f = X hf,ψιiψι : (X 2θγ(s)kδs(f)kP)/ < ∞, δs(f)=0(∀s /Jd)
l∈Z0∞	s∈Jd
then by the condition δs(f) = 0 (∀s ∈/ Jd), f ∈ Fpγ,θ,d([0, 1]∞) is independent of (xi)i>d:
f(x1, . . . ,xd, xd+1, . . . , xi, . . . ) = f(x1, . . . ,xd, x0d+1, . . . ,x0i . . . ),
for any (xi)i∞=1 ∈ [0, 1]∞ and x0i ∈ [0, 1] (i = d + 1, d + 2, . . . ). Then, for f ∈ Fpγ,θ,d([0, 1]∞), we
may define
fd(x1, . . . xd) := f(x1, . . . , xd, 0, . . . ),
13
Published as a conference paper at ICLR 2022
and it holds that kf - f0k2 = kfd - fd0 k2. Hence, by the same argument as the proof of Theorem
7, we have the same bound for Fpγ,θ,d([0, 1]∞). Thus, the same statement as Theorem 7 holds for
the γ-smooth functions on [0, 1]d. This shows that the difficulty of the approximation depends only
on the smoothness γ(s) and independent of the dimensionality d. In particular, we can establish the
bound even for d n.
B Analysis of estimation error by convolutional neural
NETWORK
In this section, we discuss approximation and estimation errors by CNNs and dilated CNNs. In
Theorem 7, we considered approximating a function in Fpγ,θ by FNNs. According to the analysis,
the index required to achieve the derived approximation error bound is determined by I(T, γ), and
only the coordinates corresponding to the index set I(T, γ) in X should be taken as input. However,
in practice, it is not given which index is required as input, and it should be estimated from the data.
In this section, we show that, under certain conditions, it is possible to find the required indices from
the data by using CNN and dilated CNN type architectures. We also show that these architectures
can achieve a favorable approximation and estimation errors for functions in mixed and anisotropic
smooth spaces that depends on the smoothness of the function classes.
C Relationship to existing function s paces
In this section, we discuss the relationship between the space Fpγ,θ, its finite dimensional counter-part
and some related function classes.
C.1 Mixed smoothness
First, we introduce a finite dimensional counter part of or function space with mixed smoothness
(Schmeisser, 1987; Sickel & Ullrich, 2009), where the domain of the input is [0, 1]d.
Definition 15 (Mixed smooth modulus of smoothness). For r ∈ N and h ∈ R>0, let
∆h(f )(χ) = {Pr=0 (r)(-1Lf(χ+加
(x ∈ [0, 1], x + rh ∈ [0, 1]),
(otherwise),
be the r-th order discrete differentiation for a function f : [0, 1] → R. By applying this discrete
differential operator to each coordinate of a subset e ⊂ {1, . . . , d}, the mixed discrete differential
operator for a step length h ∈ Rd>0 and the order parameter r ∈ Nd is defined as
△h；i(f)(x)= ∆hi(f(χι,...,∙,...,Xd)(Xi))), ∆h,e(f)：= (YS;) (f),
for X ∈ [0, 1]d. Then, the modulus of mixed smoothness is defined by
wr,p(f,t) :=	sup	k∆h,e(f)kp,
∣hi∣≤ti,i∈e
for t ∈ R>o and 1 ≤ P, where ∣∣ ∙ ∣∣p is the LP-norm with respect to the Lebesgue measure on [0,1]d.
Then, based on this modulus of mixed smoothness, we can define the mixed smooth Besov space as
follows.
Definition 16 (Mixed smooth Besov space). Let 1 ≤ p, q ≤ ∞. For a given smoothness parameter
a ∈ R>o, let ri = [a/ +1(i ∈ [d]) and define the seminorm | ∙ |河石3]as
nRx∈[0,1]d [(Qi∈e t-ai) wr,p(f,t)]q Q∈-il O"	(1 ≤ q< ∞),
supt∈[0,1]d	i∈e i	r,P(f, t)	(q = ∞).
Then we define the norm of the mixed smooth Besov space as
∣f ∣M Bpa,q := ∣f∣P +	X	|f|M B pa,,qe ,
e⊂{1,...,d}
14
Published as a conference paper at ICLR 2022
so that the mixed smooth Besov space is given by M B pα,q ([0, 1]d) := {f ∈ Lp([0, 1]d) :
kf kMBpa,q < ∞}.
The mixed smooth Besov space defined above can be seen as the finite dimensional counter part of
our space with mixed smoothness γ. Here, for s ∈ Nd, let
δs(f) =	X	hf, ψliψl,
l∈Z0 [2si-1C≤∣li∣<2si
where ψl : [0, 1]d → R is defined in the same way as in the infinite dimensional setting and f ∈
L2([0, 1]d). Then, it is known that
∣f∣MBa,q 〜X X(2ha,sikδs(f)kp)q!
s∈N0d
holds for 1 < p < ∞ (see Section 3.3 of Dung et al. (2018), Yanchenko (2020); Lizorkin &
Nikol’skii (1990) and references therein). Therefore, the mixed frequency space can be viewed
as an extension of the finite-dimensional mixed smooth Besov space to the infinite dimensional
one. Approximation and estimation abilities of deep learning with ReLU activation for the true
function in the finite-dimensional mixed smooth Besov space was investigated by Suzuki (2019)
when aι =…=a&. They showed that deep learning can achieve the (near) minimax optimal rate
in this setting and the mixed smoothness alleviates the curse of dimensionality.
C.2 Anisotropic smoothness
Ingster & Stepanova (2006) considered a general function class representated by
Fc :=	f ∈ L2([0, 1]∞) :	cl2 hf, φli2 < ∞
I	1∈Z∞
for a given sequence c = (cl)l∈Z∞ with cl ∈ R. This class includes our γ-smooth space with p =
θ = 2. In particular, Ingster & Stepanova (2011) analyzed an estimation problem when (cl)l∈Z∞ is
given by
∞
cl2 =X(2πli)2ai,
i=1
for monotonically increasing sequence a = (ai)i∞=1 ∈ R>∞0. It can be shown that, in this setting, Fc
is specifically given by the following anisotropic Sobolev classe:
(∞
f ∈ L2([0,1]∞) : X
i=1
∂ai f
∂Xαi
2<∞ .
2
This characterization highlights the intuition of the anisotropic smoothness, i.e., we assume the
functions f ∈ Fc have different smoothness (ai) with respect to each coordinate xi . Here, by
noticing that kδs(f )k2 = pι∈z∞QSi-ιc≤∣ιi∣<2Sihf, φli2, Wehave that
kfk2W2a =	cl2hf,ψli2=	cl2hf,ψli2
l∈Z∞	s∈N∞ l∈Z∞ :2si-1≤∣li∣<2si
≥	max{(2π2si-1)2ai}	hf,ψli2
i∈N
s∈N∞	l∈Z∞=2si-1 ≤∣li∣<2si
≥	22 max{ai si }i∞=1	hf, ψli2
s∈N∞	l∈Z∞∙.2si-1 ≤∣li∣<2si
22max{aisi}i∞=1kδs(f)k22= kfk2F2γ2,
s∈N0∞	,
15
Published as a conference paper at ICLR 2022
for γ(s) = maxi {aisi }. Therefore, we see that the unit ball of the anisotropic Sobolev space W2a
is included in the unit ball of F2γ,2 . Hence, our bounds in the following also give bounds for the
anisotropic Sobolev space.
Suzuki & Nitanda (2021) also analyzed approximation and estimation error bounds of deep learning
with the ReLU activation for the anisotropic Besov spaces defined on a finite dimensional space
[0, 1]d (Nikol’skii, 1975; Vybiral, 2006; Triebel, 2011).
D Proof of Theorem 7
We can see that δs can be decomposed as
δs(f)(x) =	ck exp(2πihk, xi),
k∈Z∞[2si-1C≤∣ki∣<2si
for ck ∈ C and the imaginary number i. Thus, when 1 ≤ p ≤ 2, from Theorem 1 of Nessel &
Wilmes (1978), we have that
kδs(f)k2 ≤ 2vskδs(f)kp,	(4)
where V = ( 1 一 1)+ and 2vs = 2v E∞=ιSi. Furthermore, for 2 < p, from the CaUchy-SchWarz
inequality, we obtain
kδs (f)k22 = Z	δs (f)2dλ∞
[0,1]∞
≤ Z	δs(f)pdλ∞! p Z	1dλ∞!	p
[0,1]∞	[0,1]∞
= kδs(f)k2p.
Therefore, Eq. (4) holds all over the range of 1 ≤ p < ∞.
To show the approximation error in the assertion of the theorem, we consider approximating f by
RT(f) defined as
RT(f) :
{Σs∈N∞:7(s)<T δs(f )
Σs∈N∞:70(s)<T δs(f )
(1≤θ≤2),
(2 < θ).
Then, we further approximate RT(f) by a fully connected neural network. For that purpose, we first
analyze the approximation error kf 一 RT (f)k2 by RT(f). This can be evaluated in the following
lemma.
Lemma 17. Under the same setting as Theorem 7, we have that
2-(1-vα)T kf kFpγ,θ
kf 一 RT(f )k2 ≤ [ 2-(I-VaO)ThPT≤γ0(s) 2θ-2(YO(S)-Y(S))] I" kf kFγ,θ
(1 ≤ θ ≤ 2),
(θ>2).
Proof. We show the inequality for the settings 1 ≤ θ ≤ 2 and 2 < θ separately.
1.	Approximation error kf 一 RT (f)k2 by RT(f) for 1 ≤ θ ≤ 2:
Using the orthogonality ofδS between different s, we have
kf 一 Rτ(f)k2 = (kf 一 Rτ(f)k2)θ/2
≤ ( X	kδs(f)k2)	⑸
s三N∞:T ≤γ(s)
≤	X	(2vskδs(f)kp)θ (∙.∙ θ∕2 ≤ 1)
s三N∞:T ≤γ(s)
=	X	(2Y(S)2vS-Y(S)kδS(f)kp)θ,	(6)
s三N∞:T ≤γ(s)
16
Published as a conference paper at ICLR 2022
where Eq. (4) is used to show Eq. (5). Then, for s ∈ N0∞ with T ≤ γ(s), using the assumption
V EY=S)Si ≤ va(γ) < 1, We have that
2vs-γ(s) ≤ 2(vα-1)γ(s) ≤ 2(vα-1)T .
Then, applying this inequality to Eq. (6), We obtain
X (2γ(s)2vs-γ(s)kδs(f)kp)θ ≤ 2-θ(1-vα)T X	2γ(s)kδs(f)kpθ
T ≤γ(s)
T ≤γ(s)
≤ 2-θ(1-vα)TkfkθFγ .
2.	Approximation error kf - RT (f)k2 by RT(f) for 2 < θ:
Using again the orthogonality of δs(f ) betWeen different s, We obtain
kf-RT(f)k22=	kδs(f)k22,
T≤γ0(s)
and
X kδs(f)k22 ≤ X (2vskδs(f)kp)2
T≤γ0(s)	T≤γ0(s)
= X	2vs-γ0(s)2γ0(s)-γ(s)2γ(s)kδs(f)kp2 .	(7)
T≤γ0(s)
Then, using the assumption vα0 < 1 for α0 = α(γ0), We obtain 2vs-γ0(s) ≤ 2(vα0-1)γ0(s) ≤
2(vα0-1)T. Applying this inequality to Eq. (7), We obtain
X	2vs-γ0(s)2γ0(s)-γ(s)2γ(s)kδs(f)kp2
T ≤γ0(s)
≤ 2-2(1-vα0)T X	2γ0(s)-γ(s)2γ(s)kδs(f)kp2 .
T ≤γ0(s)
Then, by using Cauchy-SchWarz inequality, We obtain
2-2(1-vα0)T X	2γ0(s)-γ(s)2γ(s)kδs(f)kp2
T ≤γ0(s)
2∕θ
≤ 2-2(IaO)T X(2Y(S)kδs(f)kp)
T≤γ0(s)
X	(2Y0(s)-Y(s))2/(I-2")
T≤γ0(s)
(1-2∕θ)
θ
×
≤ 2-2(1-vα0)T
X	2 θ-θ2 (γ0(s)-γ(s))
T ≤γ0(s)
1-2∕θ
kf k2Fpγ,θ .
□
Next, consider approximating RT by a neural netWork. We shoW it only for 1 ≤ θ ≤ 2, but the same
argument can be applied for 2 < θ. Theorem 4.1 in Perekrestenko et al. (2018) asserts that there
exist constants C1 , C2 > 0 such that, for
Lψ = Cl (log 1) + log (fmax),
.1	∙ ,	1	,	1 7 _ ɪ / T rʌ 1 C rʌ -1 9 T ∖ ,1	∙	, I
there exists a neural network ψii ∈ Φ(Lψ, 21, C2, 212Lψ) that can approximate ψii as
H /	7 Ii	/
kψii - ψii kL∞ ([0,1]) ≤ .
17
Published as a conference paper at ICLR 2022
Moreover, since ψli ∈ [-√2, √2],
k max{-√2, min{√2,Ψ1J} - ΨiiIIl∞([0,i]) ≤ e.
Since, we can write
max{-√2, min{√2, x}} = [η(x) - η(x - √2)] + [-η(-x) + η(-x + √2)]
for X ∈ R, it holds that max{-√2min{√2, x}} ∈ Φ(2,4, √2,16) and then we can see that
max{-√2, min{√2, ψιj} ∈ Φ(Lψ + 2, 21, max{C2, √2}, 212(Lψ + 2)).
(8)
By using these facts, We can construct the neural network that takes value in [-√2, √2]. In the
following, let
ψii =max{-√2, min{√2, ψ¾}}, Lψ
C1
2
+ log (fmax ) + 2.
Since we need to approximate a trigonometric polynomial, we need to (approximately) realize mul-
tiplications by neural networks. By Proposition 3 of Yarotsky (2017), using
L×
dmax
+ 5
dlog dmaxe , W× = 6dmax, S× = L×W×2
and a constant B× > 0, there exists a neural network φ× ∈ Φ(L× , W× , B× , S× ) that satisfies
dmax
φ× -	xi
i=1
≤ e.
L∞ ([-1,1]dmax)
Since ψli ≤ √2 is satisfied, we know that
⇒
φ×
dmax
Y ψii
一 √2
i=1	2
(√2)dmax φ
dmax
-Y ψii
i=1
≤e
L∞([0,1]∞)
≤ √2dmaxe.
L∞([0,1]∞)
(9)
Here, note that Eq. (8) yields that
dmax
dmax
∏ψ^li - ∏ ψli
i=1
i=1
dmax -1
X
j=0
L∞([0,1]∞)
dmax
j+1	dmax
∏ψ^li ∏ ψli- ∏ψ^li ∏ ψli
i=1	i=j+1
i=1
i=j+2
L∞([0,1]∞)
dmax -1
X
j=0
j	dmax
Y Ψii Y Ψii(ψj+1
i=1	i=j+2
- ψlj+1
L∞([0,1]∞)
≤ √2dmax
t1Rι- Lit.)
j=0
j
ʌ
≤ √2dmaxdmaxe.
Then, by using triangle inequality and Eq. (9), we know that
≤
(√2)dmax φ
(√2)dmax φ×
-∏ Ψii
i=1
dmax
- ψli
i=1
dmax
L∞([0,1]∞)
dmax
dmax
+
L∞([0,1]∞)
∏ψ^li - ∏ ψli
i=1
i=1
L∞([0,1]∞)
≤ (√2)dmax (dmaχ + 1)e.
18
Published as a conference paper at ICLR 2022
Therefore, if we set
RMf )：= Σ £ (√2)dmax hf,Ψl)Φ×
γ(s)<T l∈J (s)
where J(s) := {l ∈ Z0∞ : b2si-1c ≤ |li| < 2si}, we obtain that
—
ψl	)∣∣
√2dma ) L(T 1]dmax )
where we used the fact hf, ψli ≤ kfk2 ≤ Bf in the last inequality. Hence, ifwe put
2-T
€ =———----------------------------------------------------------,
(√2)dmax Bf (dmax + 1)G(T,γ),
(10)
then we have that
kf - RRT k2 ≤ kf - RT k2 + kRT - RT l∣L∞([-1,1]dmax )
≤ (2-T + Q(T))kf kFpγ,θ,
where Q(T)
2-(1-vα)T	(1 ≤ θ ≤ 2),
LO	2θ o o	ι-2∕θ	Noting that
2-(I-Va )T [PtRs) 21(Y (S)-Y(S))I	(θ > 2).	S
m .	. 、	..	^	.	......
2-T . Q(T), We have that kf - RTk2 . Q(T)kf |卜。.
Finally, we evaluate the network size to achieve this approximation error. Since RT is the linear
combination of neural network φ× ( ψ√
, by putting
L
W
S
B
Lφ + L× + 1,
21dmaxG(T, γ),
(212dmaχLφ + L×W× +1)G(T,γ),
max{(√2)dmax Bf ,B× ,C2},
We have that RT ∈ Φ(L, W, S, B). Substituting Eq. (10) to L× and L^, we can evaluate as
lψ
C1
2
+ log(fmax ) + 2
Cl (T log 2 + dmax log √2 + lθg S(γ,T) + log Bf + log dmax +1)+ log fmax
≤ C1 (6 max{log Bf, log 2})2	max{d2max,T2, (log S(γ, T))2 ,logfmax,
and
L×
3dmax
-------+5 ) dlog dmaxe
≤ max
3dma
log5 + log2 dlog dmaxe
≤	d6 max{log Bf, log 5}e dmax{dmax, T, log G(T, γ)}e dlog dmaxe .
Therefore, if we set K = 2 max d6 max {log Bf, log 5}e , C1 (6 max {log Bf, log 2})2	, then
it holds that
L ≤ 2Kmax {”2^,丁2, (logG(T,γ))2, log fmax},
19
Published as a conference paper at ICLR 2022
and thus we also have
S = (212dmaχLφ + L×W× + 1)G(T,Y)
≤ 2K(212dmaχ + 36dmaχ) max 心乂产2, (log G(T, γ))2, log fmax} G(T, Y)
≤ 4 X 212Kdmaxmax /乂产2, (log G(T,γ))2, logfmaχ} G(T,γ).
For the setting of 2 < θ, we can prove the bound by the same procedure. Thus, we obtain the
assertion.
E Proof of Theorem 9
Before we prove Theorem 9, we show the following lemma. The following lemma is inspired by
Lemma 1.2 of Temlyakov (1983).
Lemma 18. Suppose that a = (ai)i∞=1 and a0 = (a0i)i∞=1 are positive monotonically non-decreasing
sequences such that 1 ≤ a1 = a01 and
∞1	∞	1
< ∞,	< ∞,
1 - 2-(ai-a1)	1 2-β(ai-ai)
i=2	i=2 -
for a positive constant β > 0. Then we have the following inequalities:
∞
1
and
s∈N∞Xa0,si≥T
2-βha,si ≤ (1 - 2-β)-1
i=2
1 - 2-β(ai-ai)
2-βT
(21)
∞
0	iY=2
1 - 2-(ai-aI)
(22)
1
Proof. First, note that
Σ
s∈N∞<a0,si≥T
2-βha,si
2-βs1	X	2-βPi∞=2aisi
(	∖(Si)能2∈N∞:P直2 aiSi≥T
+	2-βPi∞=2aisi	2-βs1
(Si)能2∈N∞Wn2 aiSi<T	∖sι∈N∪{0}∖sι≥T-P∞ *s,
(23)
If T satisfies Pi∞=2 a0i si < T, then we have that
2-βs1
sι∈N∪{0}*ι≥T-Pi=2 aiSi
2-βT2β Pi∞=2 a0iSi
1 - 2-β
Thus, the second term of the right hand side of Eq. (23) can be evaluated as
2-βT(1 - 2-β)-1	X	2-β Pi∞=2(ai-a0i)si
(Si)能2∈N∞:P2 aiSi<T
Next, we evaluate the first term of the right hand side of Eq. (23). We see that
∞
X 2-βs1 = (1 - 2-β)-1,
s1=0
20
Published as a conference paper at ICLR 2022
and
Σ
2T P22 OiSi
(Si )建2∈N∞:P 晨2 Oi Si≥T
≤	E	2- P二2(θi-ai)Si 2- P22 aiSi
(Si)建2∈N∞:P互2 aiSi≥T
Σ
≤2-βT
2-β P二2(θi-ai)Si
(Si)能2∈N∞:P互2 aiSi≥T
Thus the first term of the right hand side of Eq. (23) can be bounded by
2-βτ (1 — 2-β )-1	〉：	2-β P 二2(ai-ai)Si
(Si)能2∈N∞W!=2 aiSi≥T
Hence, by noticing that
∞
E 2-β P二2(ai-ai)Si = Y
s∈N∞	i=2
2-β(ai-ai)Si	= ɪɪ
i=2
1
1 — 2-β(ai-ai),
and combining the evaluations above, Eq. (23) yields that
∞
X	2-βha,s) ≤ 2-βτ(1 ―2-β)-1Y
s∈N∞<a0,s>≥T
which yields the first inequality.
i=2
1 _ 2-β(αi-αi),
Next, We show the second inequality. We decompose ∑S∈N∞-<a s)<t
defined as
2s as PT=0 It where It is
It=	E	2P≡=1 Si.
s∈N∞<-1≤<a,S><t
Here, for 1 ≤ t ≤ T, we can evaluate It as
It ≤ 22t
2P 晨1 ZhaS
S∈N∞*-1≤<a,S) <t
2-P 鼠ι(2ai-1)Si
S∈N∞*-1≤ha,S> <t
Since ai ≥ 1, we have ai ≤ 2ai — 1 (i ≥ 1) and thus by Eq. (21) with ai = aι (∀i ∈ N) and β = 1,
we have
E	2-P鼠ι(2ai-1)Si ≤
s∈N∞ :t-1≤(a,S)<t
E	2-ha,Si
s∈N∞ :t-1≤(a,S)<t
E 2-ha,Si
s∈N∞ :t-1≤ha0,S)
1 — 2-(ai-aι)
2-t
∖i=2	/
where we used the first assertion (21) in the last inequality. Therefore, we can obtain
It ≤ 22t	E	2- P落(2aiT)Si≤ 4 ɪɪ
S∈N∞ιt-1≤(a,S)<t	∖i=2
Now using this bound, we can verify that
1 — 2-(ai-aι)
2t.
(∞
一	Y
1 — 2-(ai-aι)
T
E 2t
t=0
(∞
Y
i=2
1 — 2-(ai-aι)
Σ
∞
1
≤
Σ
≤
≤
/ ∞
4 Y
∞
1
1
1
1
which yields the second assertion (22).
□
21
Published as a conference paper at ICLR 2022
Now, we are ready to show Theorem 9. First, we give the proof in the setting of γ(s) = ha, si.
Proof for mixed smoothness (γ(s) = ha, si). First, we consider the setting 1 ≤ θ ≤ 2. Lemma
18 yields
G(T,γ)=	X	2s = X	2s ≤ 8 (YY -J)) 2a1.	(24)
s∈N∞<a,si<T	s∈N∞<OL,si<aL	∖i=2 1- 2 a1	)
Moreover, we can easily see that
P∞ s
i=1 si
α = SUP 1
s∈N0∞ ha, si
1
a1
because a is a positive monotonically non-decreasing sequence. By the assumption ai = Ω(iq),
dmax 〜TIAq, fmax 〜T
are satisfied. Now, by using the filter w ∈ RC×1×W0 with the width W0 = dmax, the number of
output channels C = dmax and the number of input channels C0 = 1 given by
1 (i = j),
wi,1,j =	0 (i 6= j),
for i, j ∈ [dmax], we can see that
x1
(Convι,w(X))ι =	. I .
xdmax
By Theorem 7, if we set
L = 2K max {t2 ,T2 },
W = 21 (YY -JT) T12a1,
1 - 2 ʒl
S = 1764K (YY ——JT) T2 max nT2,T2} 2a1,
i=2 1 - 2- aτ-
B = (√2)dmax K 0,
where K, K0 > 0 are constants, for any function f ∈ U (Fph,aθ,si ), there exists a neural network
RT ∈ Φ(L, W, S, B) such that
0
f (X ) := RT (x1 , . . . , xdmax )
satisfies
kf0 - fk2 ≤ 2-(1-a1 )T.
Since	f(X)	=	(RT	◦ Convι,w(X))	, We can see that	f 0	is a dilated CNN:	f0	∈
P(L0,B0,W0,C,L,W,B,S) where L0 = 11,B0 = 1 andW0 = C = dmax.
Next, we consider the setting 2 < θ. Let a； = a1, and for δ = a2 - ai (which is positive by the
assumption a2 > a；) and a constant U that satisfies 2 < u < 2+ 券,we set ai =筹 for i ≥ 1. Then,
a01 < a02 ≤ . . . is satisfied, that is, a0 = (a0i)i∞=1 is a positive monotonically increasing sequence.
Moreover, by the assumption a% = Ω(iq), it holds that, for all c > 0,
∞
Y
i=2
-2
-c
ai
a1
∞
iY=21-2-c
< ∞.
(25)
1
1
1
22
Published as a conference paper at ICLR 2022
By noticing that 2(aa-ai) = 2 (1 - 1)0i ≤- a∙ where We used u > 2, we also have that
2 θ2θ2 ha0-a,si
s∈N∞ <a0,s>≥T
Σ
s∈N0∞ :h2a0 /a1 ,si≥2T /a1
2-a-12 ha/ai,si
Σ
≤
Moreover, since we have verified Eq. (25), we can apply Lemma 18 to the right hand side of this
inequality and it can be further bounded as
Σ
s∈N0∞ :h2a0 /ai ,si≥2T /ai
2-a⅛ ha/ai,si
≤ 2-
2θ T
θ-2 T
By setting γ0(s) = ha0, si and using Theorem 7, we can see that if we define the set of CNN P by
the same argument as the case of 1 ≤ θ ≤ 2,forallf ∈ U(Fpγ,θ), there exists a dilated CNN f0 ∈ P
such that
kf0 - f∣∣2 . 2-(I-Va(YO))T(2-θ2-θ2T)1∕2-1∕θ = 2-2(1-avi)T
is satisfied, where we used α(γ0) = A by the definition of a0. Here, as in Eq. (24), we have that
GK Y0) ≤ 8 (Y	'-al,卜 ≤ 8 (YY	lai)卜粉 .
∖i=2 1 - 2 al /	∖i=2 1 — 2 al	)
Therefore, by resetting T J 2T, we obtain the same result as in 1 ≤ θ ≤ 2.
Proof for anisotropic smoothness (γ(s) = maxi{aisi}i). Here, we consider the setting of mixed
smoothness γ(s) = maxi{aisi}i. We note that
∞∞
i=1 si	i=1 si
α(γ) = sup —i=7 ≤ sup -i=~~7
s∈N0∞ supi {ai si }	s∈R>∞0 supi {aisi }
sup
sup
T>0 {s∈R>0 :maxi {ai si}=T}
P∞=1 S
T
The condition supi{aiSi} ≤ T is equivalent to the condition that Si ≤ T for all i ∈ N, the right
hand side can be further bounded as
∞
α(γ) ≤ X-.
i=1 ai
Therefore, in the setting of 1 ≤ θ ≤ 2, Theorem 7 with α = P∞=1 十 yields that
kRT(f)-fk2 ≤2-(1-δα)TkfkFpγ,θ.
Since it holds that
∞
X	2s ≤ Y
s∈N∞ ∙∙γ(s)<T	i=1
八a e
X 2si
si=0
G(T, Y) . 2^i=1 daie is satisfied. By noticing that a isa positive monotonically increasing sequence
with polynomial order growth, the same argument as in the setting of γ(S) = h-, Si can be applied.
Then, we obtain the assertion.
F	Proof of Theorem 10
The estimation error can be derived by evaluating the bias and variance trade-off. The bias can
be evaluated by the approximation error which has been analyzed in the previous sections, and the
variance can be characterized by the complexity of the model. As a measure of complexity of the
model, we utilize the covering number of the model (van der Vaart & Wellner, 1996).
23
Published as a conference paper at ICLR 2022
Definition 19 (Covering number). For a norm space F equipped with a norm ∣∣ ∙ ∣∣, the e-covering
number N (F, e, ∣ ∙ ∣) the minimum number of balls with radius e (measured by the norm ∣∣ ∙ ∣∣) to
cover the norm space F:
N(F, δ,1HI) = inf {n ∈ N ： ∃(fι,…，fn) ∈ F n, ∀f ∈F, ∃ ∈ [n], kfi - f k ≤ e}.
The covering number of the model of the dilated CNNs can be evaluated as in the following lemma.
Lemma 20. The log-covering number of the set of the dilated CNNs P(L0, B0, W0, C0, L, W, B, S)
can be bounded as
logN(P, δ, k∙∣∞) . (S + W0C0)(L + L0) log (LLO(B0∨ I)(B ∨ I)C'W'W).
Proof. The assertion can be shown by evaluating how strongly a small perturbation of parameters
can deviate the function. For w ∈ RC0×W0 and h ∈ N, we have that
kw?h X — w0 ?h Xk∞ ≤ k(w — w0) ?h Xk∞ ≤ W0C∣w — w0k∞∣Xk∞,
kw?h X — w?h X0k∞ ≤ W0C0kwk∞kX — X0k∞.
Therefore, for F, F0 ∈ RC0×C0×W0, we have
∣Convh,F ◦ X — Convh,F 0 ◦ X ∣∞
max0 ∣(Fi,:,: ?h X) — (Fi0,:,: ?h X)∣∞
∣Convh,F ◦ X — Convh,F ◦ X0 ∣∞
≤ W0C0	max0 ∣Fi,:,: — Fi0,:,: ∣∞ ∣X∣∞
i∈[C ]
= W0C0∣F — F0∣∞∣X∣∞,
≤ max ∣(Fi,:,: ?h X) — (Fi,:,: ?h X0)∣
i∈[C0]
≤ W0C0 max ∣Fi,"J∣∞ ∣X — X0k∞
i∈[C0]
≤ W0C0∣Fk∞∣X — X0k∞.
(26)
(27)
Here, for CNNs f, g such that
f (X) = ConvwoL0 FLo ◦••• ◦ Convwoi,f1 ◦••• ◦ Conv 1,F1 ◦ X,
g(X) = ConvwoLθ fo ◦•••◦ COnvW，\f0 ◦••• ◦ Convi,f； ◦ X,
we define
Al (f )(X ):= Convw oi-ι,Fι-1。…。Conv ι,Fι ◦ X,
Bi (g)(X0) := Convw ol0,f0,。…。Convw oi+ι ,f∖1。X 0,
where A1 (f)(X) = BL0(f)(X) = X. By using these notation and the triangle inequality, we can
see that it holds that
kf (X) — g(X )k∞
L0
≤ X ∣Bi(g)。Convw01,F1。Al(f)(X) — Bi(g)。Convw〃,耳。Ai(f )(X)k∞.
l=1
Hence, by applying the inequalities (27) and (26), we can see that
kf (X) - g(X)k∞ ≤ L0(W0C0)L0kXk∞ (Y kFikJ l理a^kFi- Fi0k∞
≤L0(W0C0B0)L0 max 0 ∣Fi — Fi0∣∞.
i=1,...L
Now, for fFNN ∈ Φ(L, W, B, S), we have
|fFNN(x) — fFNN(x0)| ≤ (BW)L∣x — x0∣∞,
24
Published as a conference paper at ICLR 2022
and thus we also have
kfFNN ◦ f(X) — fFNN ◦ g(X)k∞ ≤ L(BW)L(TC0B0)L0 max |田-F0k∞.
l=1,...L0
On the other hands, fFNN, gFNN ∈ Φ(L, W, B, S) can be represented as
fFNN(X) = (ALns + bL)。…。(Alη(∙) + bl)。…。…(AIx + bI),
gFNN0(X) = (ALnG) + bL)。…。(Alna) + bl) ◦•••◦... (AIx + bI).
Then, by noticing that kg(X)k∞ ≤ (W0C0)L0, the same argument as Lemma 3 of Suzuki (2019)
yields that, if
max max {kAl - A0lk∞, kbl - b0lk∞} ≤ δ,	(28)
l=1,...,L
then the L∞ distance between the composite functions fFNN ◦ g and gFNN0 ◦ g can be bounded as
kfFNN ◦ g(X) — gFNN0 ◦ g(X )k∞ ≤ δL(W 0C 0)L0 {(B + 1)(W + 1)}l.
Since the triangle inequality yields
IIFNNof(X)-FNN0og(X)k∞ ≤ IIFNNof(X)-FNNog(X)∣∣∞+∣∣FNNog(X)-FNN0og(X)k∞,
by combining these inequalities, under the condition (28) and maxl=1,...L0 kFl — Fl0k∞ ≤ δ, we
obtain
∣FNN ◦ f (X) — FNN0 ◦ g(X)k∞
≤ δL0(BW)L(W 0C0B0)L0 + δL(W0C0)L0{(B + 1)(W + 1)}L.
Therefore, by noticing that the number of combinations of non-zero parameter configurations in
Φ(L, W, B, S) is bounded by (W + 1)LS, we can again utilize the same argument as Lemma 3 of
Suzuki (2019) to we obtain
logNP δ, k∙k∞) . (S + W0C0)(L + L0) log (LLO(BOV I)(B ∨ I)CWW).
□
Using a covering number bound of a model, a standard analysis of the ERM estimator gives the
following bound on its estimation error.
Lemma 21 (Theorem 2.6 of Hayakawa & Suzuki (2020); Schmidt-Hieber (2020)). Let fb be any
ERM estimator that takes its value in a model F ⊂ L∞([0, 1]∞). Suppose that there exists a
constant F > 0 such that ∣∣f°k∞ ≤ F and ∣∣f k ≤ F for any f ∈ F. Thenfor any 0 < δ < 1
satisfying N (F, δ, ∣ ∙ ∣∞) ≥ 3 ,it holds that
EPn [kfb — f ◦kPx] ≤ 4 inf kf - f ◦kPx + C ((F + σ)N(F,δ,k∙k∞) + δ(F + σ)),
f∈F	n
where C is a universal constant.
Since we have assumed PX is absolutely continuous to the Lebesgue measure λ∞ and k dPχ k∞ <
∞, k ∙ ∣Pχ inthe right hand side can be replaced by ∣∙∣ 2 with a constant factor multiplication. Now,
we are ready to show Theorem 10.
25
Published as a conference paper at ICLR 2022
Proof of Theorem 10 First, we consider the setting of γ(s) = ha, si. By Theorem 9, by setting
L0 = 1,
B0 = 1,
w 0 〜τ q,
C 0 〜T 1,
L 〜max {t 2, T 2},
W 〜(YY -J)) T12a1,
∖i=ι 1 - 2
S 〜(YY —「)! T2 max nTq,T20 2a1,
J 1 - 2
1
B 〜(√2)t q,
We have that, for any f ° ∈ U(号6), there exists a dilated CNN f ∈ P(L0, B0, W0, C0,L, W, B, S)
such that
kf -f°k2 ≤ 2-2(I-V/aI)T
Moreover, by Lemma 20, the covering number of P can be bounded as
log(N(P ,δ,k∙k∞)) . (Y ]/(—J 2 a1 T 2+1 max {t 4 ,T [log (T)
Here, under the assumption ∣∣f°k∞ ≤ Bf, we can also obtain that there exists f ∈
P(Bf ,L0, B0, W0, C0,L, W, B, S) such that ∣∣f — f°∣2 ≤ 2-2(1-v/a1)T, and a covering number
evaluation log (N(P, δ, ∣ ∙ ∣∣∞)) ≤ log (N(P, δ, ∣ ∙ ∣∣∞)). Therefore, by Lemma 21, the ERM esti-
mator f taking its value in P(Bf, L0, B0, W0, C, L, W, B, S) satisfies
EPn [∣fb- fo∣2PX]
(Q∞=2	-(Ji) J2 a1 T q + 1 max(T 4，T 4 卜og( δ )
.2-2(I-a1 )T + (B2 + σ2)ʌ——V^1_n----------------------------+ δ(Bf + σ).
T	1^	1
Thus, by taking T to satisfy 2aι = n2(ai-v)+1 and letting δ = 1, we obtain that
∞	1	2(a1-v)	2	4
Π ------(ai-a1)	n- 2(a1-V)+ 1 (IOg n)q + max{(IOg n)q , (IOg n)4 },
i=2 1 — 2 ʒr-
which yields the first assertion.
Next, we consider the setting ofγ = maxi{aisi}i. We again apply Theorem 9 so that, by setting
L0 = 1,
B0 = 1,
W 0 〜T 1,
C 0 〜T q,
L 〜max {t 2, T2},
W 〜T 1 2T∕a,
S 〜T2 max {Tq,T2} 2T/a,
1
B 〜(√2)t q,
26
Published as a conference paper at ICLR 2022
We have that, for any f ° ∈
such that
U (Fpγ,θ), there exists a dilated CNN f ∈ P(L0, B0, W0, C0, L, W, B, S)
kf -f°k2 . 2-2(1-v㈤T
Therefore, by the same argument as the setting of mix smoothness, We can verify
EPn[kF- fokpx] .-2(a-v)+1 (logn)2 +2 max{(logn)4, (logn)4},
Which yields the second assertion.
G	Proofs of Theorems 13 and 14
First, We prove the folloWing lemma.
Lemma 22. Let X = (xi)i∞=1 ∈ R∞. For any sequence (xi1 , . . . , xiN) satisfying ij ≤ HL0 with
arbitrary integer H > 1 and iι < i2 < •一 <，n, there exists a dilated CNN where the number
of layers is L0, the width of filters is H, the number of channels of each layer is N and the filters
Wl ∈ RN×N×H (2 ≤ l ≤ T) and W1 ∈ RN×H in each layer satisfy kWl k∞ ≤ 1 (l = 1, . . . , L0)
such that
(xiι,...,XiN)> = (ConvHL0-1,WL0 ◦…◦ ConvHl-1,W1 ◦…◦ Convι,Wι ◦ X，.
Proof. Let
Al(X) = ConvHl-ι,wl ◦•••◦ Conv1,W1 ◦ X.
First, We let (W1)k,j = 1 if (ik - 1 mod T) = j - 1 and (W1)k,j = 0 otherWise, for k ∈ [N] and
j ∈ [T]. Then, for each k ∈ [N], there exists i(k1) ∈ [HL0-1] such that A1(X)k,(i(1)-1)H+1 = xik,
indeed, i(k1) can be obtained as i(k1) = b(ik - 1)/Hc + 1. In the folloWing, We recursively define Wl
and i(kl) for l = 2, . . . , L0 so that
Al(X)k,(i(l)-1)Hl+1 = xik (k∈ [N])	(29)
is satisfied. We remark that this is satisfied for l = 2. Suppose that, for l, We have Wl and i(kl) that
satisfy the condition (29). If We define
A0l,i: (X) := (Al (X):,(i-1)Hl + 1 + (j-1)Hl + l)∞=ι ∈ RN×∞
for i ∈ N, then We have that
Al+1(X):,(i-1)Hl+1+1 = (ConvHl,Wl+1 ◦ Al(X)):,(i-1)Hl+1+1
I (Wl+1)1,:,: ?1 Al,i: (X) ∖
∖(Wl +I)N,:,: ?1 A0l,i:(X J 1
Therefore, by letting i(kl+1) as i(kl+1) = b(ii(l) - 1)/Hc + 1 (i.e., the integer such that (i(kl+1) -
1)H + 1 ≤ i(kl) < i(kl+1)H) and setting Wl+1 so that (Wl+1)k,j = 1 if (i(kl) - 1 mod T) =
j - 1 and (Wl+1)k,j = 0 otherWise (k ∈ [N] and j ∈ [T]), We can see that the condition (29)
(Al+1(X)k,(i(l+1)-1)T l+1+1 = xik for all k ∈ [N]) holds for l + 1. Therefore, by the inductive
argument, We have that
AL0 (X):,1 = (xik)kN=1,
because i(kL0) satisfies 0 ≤ i(kL0) - 1 ≤ (ik - 1)/HL0 Which yields i(kL0) = 1 for all k ∈ [N] by the
assumption ik ≤ H L0.	□
A pictorial illustration of the proof of Lemma 22 With H = W0, N = C0 and L0 = 2 is given in
Figure 1.
27
Published as a conference paper at ICLR 2022
FNN
Figure 1: Illustration of how important features are extracted by the dilated convolution. The colored
circles indicate the important features (xi1 , . . . , xiC0 ).
Proofs of Theorems 13 and 14.
Let (ij)∞=ι be the sequence of indices that yields a” < a&2 < •…for the given positive sequence
a = (ai)i∞=1. We only give the proof for the mixed smoothness setting γ(s) = ha, si. As for the
anisotropic smoothness setting, the same reasoning can be applied.
Since kakwlq ≤ 1 is satisfied by the assumption, we have aij ≥ jq for all j ∈ N. Therefore, we can
verify that for all T > 0, it holds that
I(T,γ) = {i : ∃s ∈N0∞ , si 6=0, ha, si ≤T}
={ij：1 ≤ j ≤ T10 .
Hence, We can see that dmax = T 1, fmax = Tq by their definitions. Since a% = Ω(log i), there
exists a constant Q > 0 such that
ai ≥ Q log i
for all i ∈ [N]. Then, if a% ≤ T, i ≤ exp (T) is satisfied. By using Lemma 22 with N = dmax, we
can construct the dilated convolutional structure
Conv d τ e -ι	◦••• ◦ Conv 1,W1 ◦ X
that extracts all of the elements in I(T, γ). In particular, for a fully connected neural network RT
with dmax dimensional input, we have that
RT ◦ Conv d τe -ι	◦•••◦ Convι,Wι ◦ X
^ ,, _, 一 ...
=RT((Xi ： i ∈ I(T,γ))),	(32)
1
that is, the convolutional neural network in the right hand side depends only on the elements in
I(T,γ).
28
Published as a conference paper at ICLR 2022
On the other hand, by Lemma 18, we have that
2s
s∈N0∞ :ha,si<T
Σ
s∈N∞E∞=ι a? Sj<T
2s
.in—
i6=i1 1 - 2
1	匚
2 aii
(ai-ai1 )
- - ail	)

Therefore, by Theorem 7, if we set
L 〜max{Tq,T2}, W 〜21 Y ---------------------------
i6=i1 1 - 2
1	1工
t q 2 αii
(ai-aiι )
'-aι)

S ~∣π-
i6=i1 1 - 2
1
(ai-ai1 )
-aι-
T	1
maxT2{T2,T2}2ai1, B 〜√2Tq,

there exists a neural network RT ∈ Φ(L, W, B, S) such that f0(X) = RT ((xi : i ∈ I(T, γ))
satisfies
kf0 - fok2 ≤ 2-(1-av1 )T
(35)
Therefore, by combining Eq. (32) and Eq. (35), we can set
T
Q
L0
1
〜T, W0 = 3, C0 = Tq, B0 = 1,
and (L, W, S, B) as above so that there exists f0 ∈ P(L0, B0, W0, C0, L, W, B, S) such that
kf0 - fk2 ≤ 2-(1-av1 )T.
This yields the proof of Theorem 13.
The remaining proof for the estimation error (Theorem 14) can be done in the same manner as that
of Theorem 10, which yields Theorem 14.
□
H Numerical experiments
H.1 The experiment of Dimension Independence
Figure 2: Smoothness with polynomial order in-	Figure 3: Smoothness with sparsity
crease
In this section, we verify the dimension independence of CNNs by numerical experiments using
functions with anisotropic smoothness. The experiments were conducted in the following settings:
29
Published as a conference paper at ICLR 2022
1.	Smoothness with Polynomial order increase:
We consider the following function as the true function:
d √τ k	k
f °(x) = X Pk 2ip Y Cos (2πxi),
k=1	i=1 2 i=1
where p,d ∈ N are parameters. Data generation model: yi = f °(χi) (i = 1,...,n).
Parameter settings: n = 128, p = 1.2, 2.0, 3.0, 4.0, 5.0, d= 100, 200, 500, 1000.
The trained model: CNN with the kernel width W0 = 10, the number of channels C0 = 10,
and the depth L0 = 1.
2.	Smoothness with sparse structure:
We consider the following function as the true function:
[d/10] f k
f°(X)= X Pk 2ip Y cos(2πχιoi),
k=1	i=1 2 i=1
where p,d ∈ N are parameters. Data generation model: yi = f ° (xi) (i = 1,...,n).
Parameter setting: n = 128, p = 1.2, 2.0, 3.0, 4.0, 5.0, d= 100, 200, 500, 1000.
The trained model: Dilated CNN with the depth L0 = 3 layers, the kernel width W0 = 3,
and the number of channels C0 = 10.
In each experiment, we estimated f° from n observations (xi, yi)in=1 using a CNN or dilated CNN.
The results are shown in Figure 2 and Figure 3. These results show that for the same p, we don’t
see any deterioration of the estimation error by increasing the dimensionality d of the input. These
results are consistent with the fact that the theoretical upper bound is dimension independent and
depends only on smoothness. Moreover, in the setting of sparse smoothness, we again observe that
the estimation error is dimension-independent by using dilated CNN, which is also consistent with
our theoretical findings.
I Extension of 2D convolution
Here, we present that a similar argument can be applied to 2D convolution for image in-
puts. For that purpose, we consider a quite simple setting where the true function fo has a
mixed/anisotropic smoothness with respect to the wavelet coefficients of the input image. More
precisely, suppose that the input image X has size N × N where N = 2K for an integer
K (X ∈ [0,1]N×n) and let (αk,j)j(N/2 ) for k ∈ K be the wavelet coefficient at the level
k. Then, we assume the true function is γ-smooth with respect to the wavelet coefficients
(ai,i,ai,2,...,ai,3(N/2)2,a2,i,...,a2,3(N/22),...,aK,i) ∈ RN . Usually, N2 is large and a
standard analysis suffers from the curse of dimensionality.
The construction of wavelet coefficients is as follows (see Daubechies (1992) for more details). The
wavelet transform is obtained based on a pair of a wavelet filter 夕 ∈ RN and scaling filter ψ ∈ RN.
We assume this pair of filters are given. For u ∈ RM1 and v ∈ RM2, let
u1 v1	. . .	u1 vM2
U % V :=	.	...	. I ∈ RMι×M2,
uM1 v1 . . .	uM1 vM2
and for x, y ∈ RM1 ×M2 , let
M1 M2
hx, yi := ΣΣxi,j yi,j.
i=1 j=1
The level k wavelet and scaling bases are obtained by
2k-1	2k-1
夕(k)= £ 夕i+(j-1)N∕2k-1, ψ(k) = E 夕i+(j-1)N∕2k-1 (i = 1,...,N/2kT).
j=1	j =1
30
Published as a conference paper at ICLR 2022
We apply a shift operation on these basis functions as
ik,j) =(夕((L+2(j-1)) mod (N∕2k-1)] + l)i=ι	∈ RN/2	Cj =1…，N/2k),
and we define ψ(k,j) in the same way. Using these filter, we can calculate the wavelet coefficients in
an inductive manner. At the first level, we obtain
α(,jJ) = (∕1,i)㊈ ∕1j),Xi
αij2) = hd1,i) Q ψ(1,j) ,X〉
α(,j,3) = hψ(1,i) Q d1,j) ,Xi
αi(,1j,4) = hψ(1,i) Q ψ(1,j), Xi
(i,j ∈ [N/2]),
(i,j ∈ [N/2]),
(i,j ∈ [N/2]),
(i,j ∈ [N/2]).
Let A(1,q) = (αi(,1j,q))i,j for q = 1, 2, 3, 4. At the k-th level, we assume that we have already have
A(k-1,q) ∈ RN/2k-1 (q ∈ [4]). Then, the k-th level coefficients can be obtained by
α(,kJ) =	hdk,i)	Q dk,j),A*τ,1)i	(i,j	∈	[N∕2k]),
ɑj =	hdk,i)	Q @(k，j),A(IJ) i	(i,j	∈	[N∕2k]),
α(k,3) =	(砂(3	Q *j), A(IJ)i	(i,j	∈	[N∕2k]),
αi(,kj,4) =	hψ(k,i)	Q ψ(k,j), A(k-1,1)i	(i,j	∈	[N/2k]).
We again let A(k,q) = (αi(,kj,q))i,j for q = 1, 2, 3, 4. Then, we obtain the wavelet coefficient as
A(X) = (A(k,2), A(k,3), A(k,4))kK=-11 in a recursive manner that represents strength of each fre-
quency component k of the input image X at each location j . We assume that the true function fo
is γ-smooth with respect to this coefficient (more precisely its vectorization vec(A(X)) ∈ RN2).
Assumption 23. The true function fo : RN×N → N is γ-smooth with respect to vec(A(X)).
The smoothness satisfies the sparsity (Assumption 12). Moreover, we also assume Assumptions 3 is
satisfied.
From now on, we construct a 2D-CNN structure to extract variables in I(T, γ) from vec(A(X)).
Suppose that the support of 夕 and ψ are included in the first H components, then the inner product
appeared above are executable just in a local H × H region. For example, the Haar wavelet has
H = 2. Therefore, the inner product to extract the coefficients can be realized by the standard
convolution operation in CNNs.
From now on, we employ the same notations used in the proof of Theorems 13 and 14. We T > 0
be any positive real. We define (dmax, fmax, G) as in Theorem 7. If we can show that a CNN can
extract variables included in I(T, γ), then we can apply the same argument to Theorems 13 and 14.
Let the number of channels of of 2D convolution be Ck = dmax + 4 for k = 1, . . . , K and C0 = 1.
For each layer k ∈ [K - 1], we will define a filter W(k) ∈ RCk ×Ck-1 ×Hk×Hk where Hk =
min{H, N∕2k}. Let Ak(X) ∈ RCk×NSk,×N呼 be the output from the k-th layer of the CNN.
Then, Ak (X) is updated by the following convolution with the interval h = 2: A0(X) = X and
Hk Hk Ck-1
Ak(X)q,i,j = X X X Ak-1(X)q1,2(i-1)+i1,2(j-1)+j1 Wq(,q)1,i1,j1 (q ∈ Ck, i,j ∈ [N∕2k]),
i1=1 j1=1 q1=1
where, by convention, we let Ak-1 (X)i,j = Ak-1(X)(i-1 mod N/2k-1)+1,(j-1 mod N/2k-1)+1 for
i and j which are out of the range {1, . . . , N∕2k-1}.
Next, we show that an appropriately defined filter can extract I(T, γ). First, we let a part of W(k) as
follows to extract (A(k,1), A(k,2), A(k,3), A(k,4)):
2 1: Hk Q 2 1: Hk ,
2 1: Hk Q ψ1: Hk ,
ψ1: Hk Q 41： Hk ,
ψ1: Hk Q ψ1: Hk ,
31
Published as a conference paper at ICLR 2022
where xi:i0 = (xi, xi+1, . . . , xi0)> for a vector x.
Second, for k ≥ 2, we extract the “important coefficients” included in I(T, γ)from
(A(k-1,2), A(k-1,3), A(k-1,4)) that are already extracted in the last layer. For that purpose, let
Wj(,kq),i ,j = 1 for 2 ≤ q ≤ 4, j ≥ 5 and i1 , j1 ∈ [2] if A(k-1,q) contains the (j - 4)-th ele-
ment of I(T, γ) at its (i1, j1)-th component. Otherwise, let Wj(,kq),i ,j = 0.
Finally, we extract the important coefficients included in Ak-1(X)q,:,: for q ≥ 5. For 5 ≤ q ≤ N,
let Wj,q,i1,i2 = 1 for j ≥ 5 and i1, j1 ∈ [2] if Ak-1 (X)q,:,: contains the (j - 4)-th element of
I(T, γ) at its (i1,j1)-th component. Otherwise, we let Wj,q,i1,i2 = 0.
Since the interval of convolution is h = 2, the size of AK (X) is CK × 1 × 1 which can be seen as
a “vector.” By the construction above, it contains all coefficients included in I(T, γ). Therefore, by
feeding X = vec(AK (X)) to an FNN RT as considered in the proof of Theorems 13 and 14, we
obtain the same approximation and estimation error bounds as those theorems under Assumption
23. Here, note that the number of parameters in the CNN constructed above has the same order as
the 1D-situation thanks. Thus, we obtain the same convergence rate as Theorems 13 and 14.
32