Published as a conference paper at ICLR 2022
Learning Optimal Conformal Classifiers
David Stutz1,2, Krishnamurthy (Dj) Dvijotham1, Ali Taylan Cemgil1, Arnaud Doucet1
1 DeepMind, 2 Max Planck Institute for Informatics, Saarland Informatics Campus
Ab stract
Modern deep learning based classifiers show very high accuracy on test data but
this does not provide sufficient guarantees for safe deployment, especially in high-
stake AI applications such as medical diagnosis. Usually, predictions are obtained
without a reliable uncertainty estimate or a formal guarantee. Conformal pre-
diction (CP) addresses these issues by using the classifier’s predictions, e.g., its
probability estimates, to predict confidence sets containing the true class with a
user-specified probability. However, using CP as a separate processing step after
training prevents the underlying model from adapting to the prediction of confi-
dence sets. Thus, this paper explores strategies to differentiate through CP during
training with the goal of training model with the conformal wrapper end-to-end.
In our approach, conformal training (ConfTr), we specifically “simulate” con-
formalization on mini-batches during training. Compared to standard training,
ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art
CP methods applied after training. Moreover, it allows to “shape” the confidence
sets predicted at test time, which is difficult for standard CP. On experiments with
several datasets, we show ConfTr can influence how inefficiency is distributed
across classes, or guide the composition of confidence sets in terms of the in-
cluded classes, while retaining the guarantees offered by CP.
1	Introduction
In classification tasks, for input x, we approximate the posterior distribution over classes y ∈ [K] :=
{1, . . . , K}, denoted πy(x) ≈ P(Y = y|X = x). Following Bayes’ decision rule, the single class
with highest posterior probability is predicted for optimizing a0-1 classification loss. This way, deep
networks πθ,y(x) with parameters θ achieve impressive accuracy on held-out test sets. However, this
does not guarantee safe deployment. Conformal prediction (CP) (Vovk et al., 2005) uses a post-
training calibration step to guarantee a user-specified coverage: by allowing to predict confidence
sets C(X) ⊆ [K], CP guarantees the true class Y to be included with confidence level α, i.e.
P(Y ∈ C(X)) ≥ 1 - α when the calibration examples (Xi, Yi), i ∈ Ical are drawn exchangeably
from the test distribution. This is usually achieved in two steps: In the prediction step, so-called
conformity scores (w.r.t. to a class k ∈ [K]) are computed to construct the confidence sets C(X).
During the calibration step, these conformity scores on the calibration set w.r.t. the true class Yi are
ranked to determine a cut-off threshold τ for the predicted probabilities πθ(x) guaranteeing coverage
1 - α. This is called marginal coverage as it holds only unconditionally, i.e., the expectation is being
taken not only w.r.t. (X, Y ) but also over the distribution of all possible calibration sets, rather than
w.r.t. the conditional distribution p(Y |X).
CP also outputs intuitive uncertainty estimates: larger confidence sets |C(X)| generally convey
higher uncertainty. Although CP is agnostic to details of the underlying model πθ (x), the obtained
uncertainty estimates depend strongly on the model’s performance. If the underlying classifier is
poor, CP results in too large and thus uninformative confidence sets. “Uneven” coverage is also a
common issue, where lower coverage is achieved on more difficult classes. To address such prob-
lems, the threshold CP method of (Sadinle et al., 2019) explicitly minimizes inefficiency. Romano
et al. (2020) and Cauchois et al. (2020) propose methods that perform favorably in terms of (approx-
imate) conditional coverage. The adaptive prediction sets (APS) method of Romano et al. (2020)
is further extended by Angelopoulos et al. (2021) to return smaller confidence sets. These various
objectives are typically achieved by changing the definition of the conformity scores. In all cases,
CP is used as a post-training calibration step. In contrast, our work does not focus on advancing CP
1
Published as a conference paper at ICLR 2022
Figure 1: Illustration of conformal training (ConfTr): We develop differentiable prediction and
calibration steps for conformal prediction (CP), SMOOTHCAL and SMOOTHPRED. During training,
this allows ConfTr to “simulate” CP on each mini-batch B by calibrating on the first half Bcal and
predicting confidence sets on the other half Bpred (c.f . a ). ConfTr can optimize arbitrary losses on
the predicted confidence sets, e.g., reducing average confidence set size (inefficiency) using a size
loss Ω or penalizing specific classes from being included using a classification loss L (Cf. (Jb). After
training using our method, any existing CP method can be used to obtain a coverage guarantee.
itself, e.g., through new conformity scores, but develops a novel training procedure for the classifier
πθ . After training, any of the above CP methods can readily be applied.
Indeed, while the flexibility of CP regarding the underlying model appears attractive, it is also a se-
vere limitation: Learning the model parameters θ is not informed about the post-hoc “conformaliza-
tion”, i.e., they are are not tuned towards any specific objective such as reducing expected confidence
set size (inefficiency). During training, the model will typically be trained to minimize cross-entropy
loss. At test time, in contrast, it is used to obtain a set predictor C(X) with specific properties such
as low inefficiency. In concurrent work, Bellotti (2021) addresses this issue by learning a set pre-
dictor C(X) through thresholding logits: Classes with logits exceeding 1 are included in C(X) and
training aims to minimize inefficiency while targeting coverage 1 - α. In experiments using linear
models only, this approach is shown to decrease inefficiency. However, (Bellotti, 2021) ignores the
crucial calibration step ofCP during training and does not allow to optimize losses beyond marginal
coverage or inefficiency. In contrast, our work subsumes (Bellotti, 2021), but additionally considers
the calibration step during training, which is crucial for further decreasing inefficiency. Furthermore,
we aim to allow fine-grained control over class-conditional inefficiency or the composition of the
confidence sets by allowing to optimize arbitrary losses defined on confidence sets.
Our contributions can be summarized as follows:
1.	We propose conformal training (ConfTr), a procedure allowing to train model and conformal
wrapper end-to-end. This is achieved by developing smooth implementations of recent CP
methods for use during training. On each mini-batch, ConfTr “simulates” conformalization,
using half of the batch for calibration, and the other half for prediction and loss computation,
c.f. Fig. 1 ba . After training, any existing CP method can provide a coverage guarantee.
2.	In experiments, using ConfTr for training consistently reduces the inefficiency of conformal
predictors such as threshold CP (THR) (Sadinle et al., 2019) or APS (Romano et al., 2020)
applied after training. We further improve over (Bellotti, 2021), illustrating the importance of
the calibration step during training.
3.	Using carefully constructed losses, ConfTr allows to “shape” the confidence sets obtained at test
time: We can reduce class-conditional inefficiency or “coverage confusion”, i.e., the likelihood
of two or more classes being included in the same confidence sets, c.f. Fig. 1 bb . Generally, in
contrast to (Bellotti, 2021), ConfTr allows to optimize arbitrary losses on the confidence sets.
Because ConfTr is agnostic to the CP method used at test time, our work is complementary to most
related work, i.e., any advancement in terms of CP is directly applicable to ConfTr. For example,
this might include conditional or application-specific guarantees as in (Sadinle et al., 2016; Bates
et al., 2021). Most importantly, ConfTr preserves the coverage guarantee obtained through CP.
2
Published as a conference paper at ICLR 2022
CP Baseline Comparison by Ineff				
Dataset, α	THRL	THR	RpSI	RAPS
CIFAR10, 005 CIFAR10, 0.01	2.22 3.92	1.64 2.93	2.06 3.30	1.74 3.06
CIFAR100, 0QΓ	19.22	10.63	16.62	14.25
>4
u
U
ω
髀
φ
o .
0
CIFAR10: Inefficiency by Class for Baseline+Thr
Figure 2: Baseline CP Results on CIFAR: Left: Inefficiency (Ineff, lower is better) for the CP
methods discussed in Sec. 2. Coverage (Cover), omitted here, is empirically close to 1 - α. THR
clearly outperforms all approaches w.r.t. inefficiency. Right: Inefficiency distribution across CI-
FAR10 classes (for α=0.01) is plotted, with more difficult classes yielding higher inefficiency.
2	Differentiable Conformal Predictors
We are interested in training the model πθ end-to-end with the conformal wrapper in order to allow
fine-grained control over the confidence sets C(X). Before developing differentiable CP methods
for training in Sec. 2.2, we review two recently proposed conformal predictors that we use at test
time. These consist of two steps, see Sec. 2.1: for prediction (on the test set) we need to define the
confidence sets Cθ (X; τ) which depend on the model parameters θ through the predictions πθ and
where the threshold τ is determined during calibration on a held-out calibration set (Xi, Yi), i ∈ Ical
in order to obtain coverage.
2.1	Conformal Predictors
The threshold conformal predictor (Thr) (Sadinle et al., 2019) constructs the confidence sets
by thresholding probabilities: Cθ(x; τ) := {k : πθ,k (x) =: Eθ(x, k) ≥ τ}. Here, the subscript
Cθ makes the dependence on the model πθ and its parameters θ explicit. During calibration, τ is
computed as the α(1 + 1/|Ical|)-quantile of the so-called conformity scores Eθ(xi, yi) = πθ,yi (xi).
The conformity scores indicate, for each example, the threshold that ensures coverage. Marginal
coverage of (1 - α) is guaranteed on a test example (X, Y ). In practice, THR can also be applied
on logits (ThrL) or log-probabilities (ThrLP) instead of probabilities.
Adaptive Prediction Sets (APS) (Romano et al., 2020) constructs confidence sets based on the
ordered probabilities. Specifically, Cθ(x; τ) := {k : Eθ(x, k) ≤ τ} with:
Eθ(x,k) := πθ,y(1) (x) + . . . + πθ,y(k-1) (x) + U πθ,y(k) (x),	(1)
where πθ,y(1) (x) ≥ . . . ≥ πθ,y(K) (x) and U is a uniform random variable in [0, 1] to break ties.
Similar to THR, the conformity scores Eθ (xi , yi) w.r.t. the true classes yi are used for calibration,
but the (1 - α)(1 + 1/|Ical|)-quantile is required to ensure marginal coverage on test examples.
Performance of CP is then measured using two metrics: (empirical and marginal) coverage (Cover)
as well as inefficiency (Ineff). Letting Itest be a test set of size |Itest |, these metrics are computed as
CoVer= -.......j- X δ[yi ∈ C(Xi)] and Ineff := -7-------j- X ： IC(Xi) |,	(2)
|Itest| i∈Itest	|Itest| i∈Itest
where δ denotes an indicator function that is 1 when its argument is true and 0 otherwise. Due to the
marginal coverage guarantee provided by CP (c.f. (Romano et al., 2020) or App. C), the empirical
coverage, when averaged across several calibration/test splits, is Cover ≈ 1 - α. Thus, we concen-
trate on inefficiency as the main metric to compare across CP methods and models. With accuracy,
we refer to the (top-1) accuracy with respect to the arg max-predictions, i.e., arg maxk πθ,k(X), ob-
tained by the underlying model π. As shown in Fig. 2 (left), THR clearly outperforms THRL and
APS w.r.t. inefficiency (lower is better) averaged across random Ical/Itest splits (details in Sec. F).
CP is intended to be used as a “wrapper” around πθ . “Better” CP methods generally result in lower
inefficiency for a fixed model πθ . For example, following Fig. 2 (left), regularized APS (RAPS)
(Angelopoulos et al., 2021) recently showed how to improve inefficiency compared to APS by
modifying the conformity score - without outperforming THR, however. Fine-grained control over
inefficiency, e.g., conditioned on the class or the composition of the C(X) is generally not possible.
Integrating CP into the training procedure promises a higher degree of control, however, requires
differentiable CP implementations, e.g., for THR or APS.
3
Published as a conference paper at ICLR 2022
1:	function PREDICt(∏θ(x), T)
2:	compute Eθ(x, k), k∈[K]
3:	return Cθ(x; T) = {k : Eθ(x, k) ≥ τ}
1:	function CALIBRATE({(∏θ(xi),yi}‰ι, ɑ)
2:	compute Eθ (xi,yi), i=1,...,n
3:	return QUANTILe({Eθ(xi,yi)}, α(1 + 1/n))
1: function SMOOTHPRED(∏θ(x), T, T=1)
2: return Cθ,k(x; τ) = σ( (Eθ(XTk)-T)), k ∈ [K]
3: function SMOOTHCAL({(∏θ(xi),yi}n=ι, a)
4: return SMOOTHQUANt({Eθ(xi, yi)}, a(1 + 1 ))
1:	function CONFORMALTRAINING(α, λ=1)
2:	for mini-batch B do
3:	randomly split batch Bca ] BPred = B
4:	{“On-the-fly” calibration on Bm：}
5:	T = SMOOTHCAL({(∏θ(xi),yi)}i∈Bcal, α)
6:	{Prediction only on i ∈ BPred:}
7:	Cθ(Xi； T) = SMOOTHPRED(∏Θ(Xi), T)
8:	{Optional classification loss:}
9： LB =0 or Pi∈Bpred L(Cθ(Xi； T),yi)
10： ωb = Pi∈Bpred 0(Cθ (Xi； T))
11：	∆ = Vθ 1/1BPredI(LB + λΩB )
12: update parameters θ using ∆
Algorithm 1: Smooth CP and Conformal Training (ConfTr): Top left: At test time, for THR,
PREDICT computes the conformity scores Eθ(x, k) for each k∈[K] and constructs the confidence
sets Cθ (x; τ) by thresholding with τ . CALIBRATE determines the threshold τ as the α(1 + 1 2 3 4/n)-
quantile of the conformity scores w.r.t. the true classes yi on a calibration set {(xi, yi)} of size
n:=|Ical|. THR and APS use different conformity scores. Right and bottom left: ConfTr calibrates
on a part of each mini-batch, Bcal. Thereby, we obtain guaranteed coverage on the other part, Bpred
(in expectation across batches). Then, the inefficiency on Bpred is minimized to update the model
parameters θ. Smooth implementations of calibration and prediction are used.
2.2 Differentiable Prediction and Calibration Steps
Differentiating through CP involves differentiable prediction and calibration steps: We want
Cθ(x; τ) to be differentiable w.r.t. the predictions πθ(x), and τ to be differentiable w.r.t. to the
predictions πθ (xi), i ∈ Ical used for calibration. We emphasize that, ultimately, this allows to
differentiate through both calibration and prediction w.r.t. the model parameters θ, on which the
predictions πθ (x) and thus the conformity scores Eθ (x, k) depend. For brevity, we focus on THR,
see Alg. 1 and discuss APS in App. D.
Prediction involves thresholding the conformity scores Eθ(x, k), which can be smoothed using
the sigmoid function σ(z) = 1/1+exp(-z) and a temperature hyper-parameter T: Cθ,k(x; τ) :=
σ ((Eθ(χ,k)-τ)∕τ). Essentially, Cρ,k(x; T) ∈ [0,1] represents a soft assignment of class k to the
confidence set, i.e., can be interpreted as the probability of k being included. For T → 0, the “hard”
confidence set will be recovered, i.e., Cθ,k(x; T) = 1 for k ∈ Cθ(x; T) and0 otherwise. For THR, the
conformity scores are naturally differentiable w.r.t. to the parameters θ because E(x, k) = πθ,k (x).
As the conformity scores are already differentiable, calibration merely involves a differentiable
quantile computation. This can be accomplished using any smooth sorting approach (Blondel et al.,
2020; Cuturi et al., 2019; Williamson, 2020). These often come with a “dispersion” hyper-parameter
such that smooth sorting approximates “hard” sorting for → 0. Overall, this results in the
threshold T being differentiable w.r.t. the predictions of the calibration examples {(πθ (xi), yi}i∈Ical
and the model’s parameters θ.
As this approximation is using smooth operations, the coverage guarantee seems lost. However,
in the limit of T, → 0 we recover the original non-smooth computations and the corresponding
coverage guarantee. Thus, itis reasonable to assume that, in practice, we empirically obtain coverage
close to (1 - α). We found that this is sufficient because these smooth variants are only used during
training. At test time, we use the original (non-smooth) implementations and the coverage guarantee
follows directly from (Romano et al., 2020; Sadinle et al., 2019).
3	CONFORMAL TRAINING (CONFTR): Learning CONFORMAL PREDICTION
The key idea of conformal training (ConfTr) is to “simulate” CP during training, i.e., performing
both calibration and prediction steps on each mini-batch. This is accomplished using the differ-
entiable conformal predictors as introduced in Sec. 2.2. ConfTr can be viewed as a generalization
of (Bellotti, 2021) that just differentiates through the prediction step with a fixed threshold, with-
out considering the crucial calibration step, see App. E. In both cases, only the training procedure
changes. After training, standard (non-smooth) conformal predictors are applied.
4
Published as a conference paper at ICLR 2022
3.1	ConfTr by Optimizing Inefficiency
ConfTr performs (differentiable) CP on each mini-batch during stochastic gradient descent (SGD)
training. In particular, as illustrated in Fig. 1 a , we split each mini-batch B in half: the first half
is used for calibration, Bcal, and the second one for prediction and loss computation, Bpred. That
is, on Bcal, we calibrate τ by computing the α(1 + 1/|Bcal|)-quantile of the conformity scores in a
differentiable manner. It is important to note that we compute Cθ (xi ; τ) only for i ∈ Bpred and not
for i ∈ Bcal. Then, in expectation across mini-batches and large enough |Bcal |, for T, → 0, CP
guarantees coverage 1 - α on Bpred. Assuming empirical coverage to be close to (1 - α) in practice,
we only need to minimize inefficiency during training:
min log E [Ω(Cθ(X; T))] with Ω(Cθ (x; T)) = max (θ, ^X Cθ,k (x; T) - κ) .	(3)
We emphasize that ConfTr optimizes the model parameters θ on which the confidence sets Cθ de-
Pend through the model predictions ∏θ . Here, Ω is a “smooth" size loss intended to minimize the
expected inefficiency, i.e., E[∣Cθ(X;T)|], not to be confused with the statistic in Eq. (2) used for
evaluation. Remember that Cπ,k(x; T) can be understood as a soft assignment of class k to the
confidence set Cθ (x; T). By default, we use κ = 1 in order to not penalize singletons. However,
κ ∈ {0, 1} can generally be treated as hyper-parameter. After training, any CP method can be ap-
plied to re-calibrate T on a held-out calibration set Ical as usual, i.e., the thresholds T obtained during
training are not kept. This ensures that we obtain a coverage guarantee of CP.
3.2	ConfTr with Classification Loss
In order to obtain more control over the composition of confidence sets Cθ(X; T) at test time, ConfTr
can be complemented using a generic loss L:
minlog (E [L(Cθ(X； τ), Y) + λΩ(Cθ(X; T))]).	(4)
θ
While L can be any arbitrary loss defined directly on the confidence sets Cθ, we propose to use
a “configurable” classification loss Lclass. This classification loss is intended to explicitly enforce
coverage, i.e., make sure the true label Y is included in Cθ(X; T), and optionally penalize other
classes k not to be included in Cθ, as illustrated in Fig. 1 b . To this end, we define
Lclass(Cθ (x; T), y)
-Cθ,k (x； T)) ∙ δ[y = k] +
{^^^^^^"^^^^^^^
enforce y to be in C
Cθ,k"∙δ[y=kJ ].
penalize class k6=y not to be in C
(5)
As above, Cθ,k(x; T) ∈ [0, 1] such that 1 - Cθ(x; T) can be understood as the likelihood of k not
being in Cθ(x; T). In Eq. (5), the first term is used to encourage coverage, while the second term can
be used to avoid predicting other classes. This is governed by the loss matrix L: For L = IK, i.e.,
the identity matrix with K rows and columns, this loss simply enforces coverage (perfect coverage
if Lclass = 0). However, setting any Ly,k > 0 for y 6= k penalizes the model from including class
k in confidence sets with ground truth y. Thus, cleverly defining L allows to define rather complex
objectives, as we will explore next. ConfTr with (optional) classification loss is summarized in
Alg. 1 (right) and Python code can be found in App. P.
3.3	ConfTr with General and Application-Specific Losses
We consider several use cases motivated by medical diagnosis, e.g., breast cancer screening (McK-
inney et al., 2020) or classification of dermatological conditions (Liu et al., 2020; Roy et al., 2021;
Jain et al., 2021). In skin condition classification, for example, predicting sets of classes, e.g., the
top-k conditions, is already a common strategy for handling uncertainty. In these cases, we not
only care about coverage guarantees but also desirable characteristics of the confidence sets. These
constraints in terms of the predicted confidence sets can, however, be rather complicated and pose
difficulties for standard CP. We explore several exemplary use cases to demonstrate the applicability
of ConfTr, that are also relevant beyond the considered use cases in medical diagnosis.
5
Published as a conference paper at ICLR 2022
First, we consider “shaping” class-conditional inefficiency, formally defined as
Ineff[Y = y] :
P--------1---------τ X δ[yi = y]|C(Xi)|.
i∈Itest δ[yi = y] i∈Itest
(6)
Similarly, we can define inefficiency conditional on a group of classes. For example, we could
reduce inefficiency, i.e., uncertainty, on “low-risk” diseases at the expense of higher uncertainty
on “high-risk” conditions. This can be thought of as re-allocating time spent by a doctor towards
high-risk cases. Using ConfTr, we can manipulate group- or class-conditional inefficiency using a
weighted size loss ω ∙ Ω(C(X; T)) with ω := ω(Y) depending on the ground truth Y in Eq.(3).
Next, we consider which classes are actually included in the confidence sets. CP itself does not
enforce any constraints on the composition of the confidence sets. However, with ConfTr, we can
penalize the “confusion” between pairs of classes: for example if two diseases are frequently con-
fused by doctors, it makes sense to train models that avoid confidence sets that contain both diseases.
To control such cases, we define the coverage confusion matrix as
Σy,k
：=击X」yi
y ∧ k ∈ C(xi)].
(7)
The off-diagonals, i.e., Σy,k for y 6= k, quantify how often class k is included in confidence sets
with true class y. Reducing Σy,k can be accomplished using a positive entry Ly,k > 0 in Eq. (5).
Finally, we explicitly want to penalize “overlap” between groups of classes in confidence sets. For
example, we may not want to concurrently include very high-risk conditions among low-risk ones
in confidence sets, to avoid unwanted anxiety or tests for the patient. Letting K0 ] K1 being two
disjoint sets of classes, we define mis-coverage as
MisCover0→1 = P ,	；y	∈ K]	X	δ[yi	∈	Ko	∧	(∃k	∈	Ki	: k ∈ C(Xi))].	(8)
Reducing MisCover0→1 means avoiding classes K1 being included in confidence sets of classes K0.
Again, we use Ly,k > 0 for y ∈ K0 , k ∈ K1 to approach this problem. MisCover1→0 is defined
analogously and measures the opposite, i.e., classes K0 being included in confidence sets of K1.
4	Experiments
We present experiments in two parts: First, in Sec. 4.1, we demonstrate that ConfTr can reduce
inefficiency of Thr and APS compared to CP applied to a baseline model trained using cross-
entropy loss separately (see Tab. 1 for the main results). Thereby, we outperform concurrent work
of Bellotti (2021). Second, in Sec. 4.2, we show how ConfTr can be used to “shape” confidence sets,
i.e., reduce class-conditional inefficiency for specific (groups of) classes or coverage confusion of
two or more classes, while maintaining the marginal coverage guarantee. This is impossible using
(Bellotti, 2021) and rather difficult for standard CP.
We consider several benchmark datasets as well as architectures, c.f . Tab. A, and report metrics
averaged across 10 random calibration/test splits for 10 trained models for each method. We focus
on (non-differentiable) THR and APS as CP methods used after training and, thus, obtain the cor-
responding coverage guarantee. Thr, in particular, consistently achieves lower inefficiency for a
fixed confidence level α than, e.g., THRL (i.e., THR on logits) or RAPS, see Fig. 2 (left). We set
α = 0.01 and use the same α during training using ConfTr. Hyper-parameters are optimized for
Thr or APS individually. We refer to App. F for further details on datasets, models, evaluation
protocol and hyper-parameter optimization.
4.1	Reducing Inefficiency with ConfTr
In the first part, we focus on the inefficiency reductions of ConfTr in comparison to a standard cross-
entropy training baseline and (Bellotti, 2021) (Bel). After summarizing the possible inefficiency
reductions, we also discuss which CP method to use during training and how ConfTr can be used
for ensembles and generalizes to lower α.
6
Published as a conference paper at ICLR 2022
Table 1: Main Inefficiency Results, comparing (Bellotti, 2021) (Bel, trained with THRL) and Con-
fTr (trained with THRLP) using THR or APS at test time (with α=0.01). We also report improve-
ments relative to the baseline, i.e., standard cross-entropy training, in percentage in parentheses.
ConfTr results in a consistent improvement of inefficiency for both Thr and APS. Training with
Lclass, using L = IK , generally works slightly better. On CIFAR, the inefficiency reduction is
smaller compared to other datasets as ConfTr is trained on pre-trained ResNet features, see text.
More results can be found in App. J.
Inefficiency，ConfTr (trained w/ ThrLP), α = 0.01							
		Thr						ApS			
Dataset	Basel.	Bel	Confrr	+Lclass	Basel.	ConfTr	+Lclass
MNIST-	2.23	2.70	2.18	2.11 (-5.4%)	2.50	2.16	2.14 (-14.4%)
F-MNIST	2.05	1.90	1.69	1.67 (-18.5%)	2.36	1.82	1.72 (-27.1%)
EMNIST	2.66	3.48	2.66	2.49 (-6.4%)	4.23	2.86	2.87 (-32.2%)
CIFAR10	2.93	2.93	2.88	2.84 (-3.1%)	3.30	3.05	2.93(-11.2%)
CIFAR100	10.63	10.91	10.78	10.44 (-1.8%)	16.62	12.99	12.73 (-23.4%)
Main Results: In Tab. 1, we summarize the inefficiency reductions possible through ConfTr (trained
with THRLP) in comparison to Bel (trained with THRL) and the baseline. Bel does not consistently
improve inefficiency on all datasets. Specifically, on MNIST, EMNIST or CIFAR100, inefficiency
actually worsens. Our ConfTr, in contrast, reduces inefficiency consistently, not only for THR but
also for APS. Here, improvements on CIFAR for Thr are generally less pronounced. This is likely
because we train linear models on top of a pre-trained ResNet (He et al., 2016) where features are not
taking into account conformalization at test time, see App. J. For APS, in contrast, improvements are
still significant. Across all datasets, training with Lclass generally performs slightly better, especially
for datasets with many classes such as EMNIST (K=52) or CIFAR100 (K=100). Overall, ConfTr
yields significant inefficiency reductions, independent of the CP method used at test time.
Conformal Predictors for Training: In Tab. 1, we use THRLP during training, irrespective of the
CP method used at test time. This is counter-intuitive when using, e.g., APS at test time. However,
training with Thr and APS is rather difficult, as discussed in App. I. This is likely caused by
limited gradient flow as both Thr and APS are defined on the predicted probabilities instead of
log-probabilities as used for ThrLP or in cross-entropy training. Moreover, re-formulating the
conformity scores of APS in Eq. (11) to use log-probabilities is non-trivial. In contrast, Bel has to
be trained using THRL as a fixed threshold τ is used during training. This is because the calibration
step is ignored during training. Also, fixing τ is not straightforward for THR due to the limited
range of the predicted probabilities πθ,k (x) ∈ [0, 1], see App. E. We believe that this contributes
to the poor performance of Bel on several datasets. Finally, we found that Bel or ConfTr do not
necessarily recover the accuracy of the baseline. Remember that we refer to the accuracy in terms
of the arg max-prediction of πθ . When training from scratch, accuracy can be 2-6% lower while
still reducing inefficiency. This is interesting because ConfTr is still able to improve inefficiency,
highlighting that cross-entropy training is not appropriate for CP.
Further Results: Tab. 2 includes additional results for ConfTr to “conformalize” ensembles on
CIFAR10 (left) and with lower confidence levels α on EMNIST (right). In the first example, we
consider applying CP to an ensemble of models. Ensemble CP methods such as (Yang & Kuchib-
hotla, 2021) cannot improve Ineff over the best model of the ensemble, i.e., 3.10 for THR. Instead,
Table 2: Ensemble Results and Lower Confidence Levels α: Left: “Conformalization” of ensem-
bles using a 2-layer MLP trained on logits, either normally or using ConfTr. The ensemble contains
18 models with accuracies in between 75.10 and 82.72%. Training a model on top of the ensemble
clearly outperforms the best model of the ensemble; using ConfTr further boosts Ineff. Right: The
inefficiency improvements of Tab. 1 generalize to lower confidence levels α on EMNIST, although
ConfTr is trained with α=0.01.
CIFAR10: Ensemble Results				EMNIST: ConfidenCe LevelS		
Test		Thr				Method	Basel.	ConfTr
Method	(Models)	+MLP	+ConfTr	Test		Thr		
Avg. Ineff Best Ineff	3.10 2.84	2.40 2.33	2.35 2.30	Ineff, α=0.005 Ineff, α=0.001	4.10 15.73	3.37 (-17.8%) 13.65(-13.2%)
7
Published as a conference paper at ICLR 2022
CIFAR10： Inefr Improvement by Class
(求)86UeIo 8>⅛而 α
CIFAR10: Ineff Reduction by Group
InefT improvement Group 0
Avg. Ineff Increase Group Q
(求)α>Bueqυ8>⅛a3
Class
(求)86UeIo 8>⅛⑥ H
Figure 3: Shaping Class-Conditional Inefficiency on CIFAR: Possible inefficiency reductions,
in percentage change, per class (blue) and the impact on the overall, average inefficiency across
classes (green). Left: Significant inefficiency reductions are possible for all classes on CIFAR10.
Middle: The same strategy applies to groups of classes, e.g., “vehicles” vs “animals”, as well.
Right: Similarly, on CIFAR100, we group classes by their coarse class (20 groups a` 5 classes), see
(Krizhevsky, 2009), allowing inefficiency improvements of more than 30% per individual group.
training an MLP on top of the ensemble’s logits can improve Ineff to 2.40 and additionally using
ConfTr to 2.35. The second example shows that ConfTr, trained for α=0.01, generalizes very well
to significantly smaller confidence levels, e.g., α=0.001 on EMNIST. In fact, the improvement of
ConfTr (without Lclass) in terms of inefficiency is actually more significant for lower confidence
levels. We also found ConfTr to be very stable regarding hyper-parameters, see App. H. Only too
small batch sizes (e.g., |B|=100 on MNIST) prevents convergence. This is likely because of too
few examples (|Bcal|=50) for calibration with α=0.01 during training. More results, e.g., on binary
datasets or including additional hyper-parameter ablation can be found in App. J.
4.2	Conformal Training for Applications: Case Studies
For the second part, we focus on ConfTr trained with ThrLP and evaluated using Thr. We follow
Sec. 3.3 and start by reducing class- or group-conditional inefficiency using ConfTr (without Lclass),
before demonstrating reductions in coverage confusion of two or more classes and avoiding mis-
coverage between groups of classes (with Lclass). Because this level of control over the confidence
sets is not easily possible using Bel or standard CP, we concentrate on ConfTr only:
Shaping Conditional Inefficiency: We use ConfTr to reduce class-conditional inefficiency for spe-
cific classes or a group of classes, as defined in Eq. (6). In Fig. 2, inefficiency is shown to vary
widely across classes: On CIFAR10, the more difficult class 3 (“cat”) obtains higher inefficiency
than the easier class 1 (“automobile”). Thus, in Fig. 3, we use ω=10 as described in Sec. 3.3 to
reduce class- or group-conditional inefficiency. We report the relative change in percentage, show-
ing that inefficiency reductions of 20% or more are possible for many classes, including “cat” on
CIFAR10 (left, blue). This is also possible for two groups of classes, “vehicles” vs. “animals” (mid-
dle). However, these reductions usually come at the cost of a slight increase in average inefficiency
across all classes (green). On CIFAR100, we consider 20 coarse classes, each containing 5 of the
100 classes (right). Again, significant inefficiency reductions per coarse class are possible. These
observations generalize to all other considered datasets and different class groups, see App. L.
Avoiding Coverage Confusion: Next, we use ConfTr to manipulate the coverage confusion matrix
as defined in Eq. (7). Specifically, we intend to reduce coverage confusion of selected sets of classes.
F
O
求 F-MNIST: Reducing Cov Confusion 2-4-6 洪 F-MNIST: Reducing Cov Confusion Class 6
φ 1	A。
6
ω
6
Off-Diagonal Weight
ra 0.5
O
S 0.0
⅞-0.5
O
I -	-
—— 6-0 ——6-3 ——2-4
---6-2 ---6-4
-1.0
0.0	0.2	0.4	0.6	0.8	1.0
Off-Diagonal Weight
Figure 4: Controlling Coverage Confusion: Controlling coverage confusion using ConfTr with
Lclass and an increasing penalty Ly,k > 0 on Fashion-MNIST. For classes 4 and 6 (“coat” and
“shirt”), coverage confusion Σy,k and Σk,y decreases significantly (blue and green). However, con-
fusion of class 4 with class 2 (“pullover”) might increase (gray). ConfTr can also reduce coverage
confusion of multiple pairs of classes (e.g., additionally considering class 2). Instead, we can also
penalize confusion for each pair (y, k), k ∈ [K], e.g., y = 6. Here, Ly,k > 0, but Ly,k = 0, i.e., Cover
confusion is not reduced symmetrically.
8
Published as a conference paper at ICLR 2022
CIFAR10: Ko= 3 (“cat”) vs. Ki= Others CIFAR100: Ko= “human-made vs. Ki= “natural”						
	CIFAR10			CIFAR100		
		MisCover ]			MisCover ]	
Method	Ineff	0→1	1→0	Inefr	0→1	1→0
ConfTr LKo ,Kι =1 LKι,Ko =1	2.84 2.89 2.92	98.92 91.60 97.36	36.52 34.74 26.43	!044 16.50 11.35	40.09 15.77 45.37	29.6 70.26 17.56
WIneQuaIIty: Reducing Ineff of Class 0
δl2.0	：
l.5
——Size O	——Size 1
0	2	4	6	8	10
Size Weight Class 0
WIneQuaIIty: Importance Class 0
1.0	1.5	2.0	2.5	3.0	3.5	4.0
On-Diagonal Weight
Figure 5: Left: Reducing Mis-Coverage: Following Sec. 3.3, ConfTr allows to reduce mis-
coverage on CIFAR. We consider K0={3} (i.e., “cat”) vs. all other classes on CIFAR10 (left)
and “human-made” vs. “natural” on CIFAR100 (|K0|=35, |K1 |=65, right). On CIFAR10, both
MisCover0→1 and MisCover1→1 can be reduced significantly without large impact on inefficiency.
For CIFAR100, in contrast, Ineff increases more significantly. Right: Binary Class-Conditional
Inefficiency and Coverage: We plot inefficiency by class (top) and coverage confusion (bottom)
on WineQuality. We can reduce inefficiency for class 0 (“bad”), the minority class, at the expense
of higher inefficiency for class 1 (“good”) and boost class-conditional coverage for class 0.
Using a non-zero entry Ly,k > 0, y 6= k in Lclass, as described in Sec. 3.3, Fig. 4 (left) shows that cov-
erage confusion can be reduced significantly for large enough Ly,k on Fashion-MNIST: Considering
classes 4 and 6 (“coat” and “shirt”) confusion can be reduced by roughly 1%. However, as accuracy
stays roughly the same and coverage is guaranteed, this comes at the cost of increasing coverage
confusion for other class pairs, e.g., 2 (“pullover”) and 4. ConfTr can also be used to reduce cover-
age confusion of multiple class pairs (middle) or a whole row in the coverage confusion matrix Σy,k
with fixed y and y6=k∈[K]. Fig. 4 (right) shows the results for class 6: coverage confusion with,
e.g., classes 0 (“t-shirt”), 2 or 4 (blue, green and violet) is reduced roughly 0.5% each at the cost
of increased confusion of classes 2 and 4 (in gray). These experiments can be reproduced on other
datasets, e.g., MNIST or CIFAR10 in App. M.
Reducing Mis-Coverage: We can also address unwanted “overlap” of two groups of classes using
ConfTr and Lclass. In Fig. 5 (left) we explicitly measure mis-coverage as defined in Eq. (8). First, on
CIFAR10, we consider a singleton group K0={3} (“cat”) and K1=[K] \ {3}: The ConfTr baseline
MisCover0→1 tells us that 98.92% of confidence sets with true class 3 also contain other classes.
Given an average inefficiency of 2.84 this is reasonable. Using L3,k = 1, k 6= 3, this can be reduced
to 91.6%. Vice-versa, the fraction of confidence sets of class y 6=3 containing class 3 can be reduced
from 36.52% to 26.43%. On CIFAR100, this also allows to reduce overlap between “human-made”
(35 classes) and “natural” (65 classes) things, e.g., MisCover0→1 reduces from 40.09% to 15.77%,
at the cost of a slight increase in inefficiency. See App. N for additional results.
Binary Datasets: Finally, in Fig. 5 (right), we illustrate that the above conclusions generalize to
the binary case: On WineQuality, we can control inefficiency of class 0 (“bad wine”, minority
class with 〜37% of examples) at the expense of increased inefficiency for class 1 ("good wine”,
top). Similarly, we can (empirically) improve class-conditional coverage for class 0 (bottom) or
manipulate coverage confusion of both classes, see App. O. 5
5 Conclusion
We introduced conformal training (ConfTr), a novel method to train conformal predictors end-
to-end with the underlying model. This addresses a major limitation of conformal prediction (CP)
in practice: The model is fixed, leaving CP little to no control over the predicted confidence sets.
In thorough experiments, we demonstrated that ConfTr can improve inefficiency of state-of-the-art
CP methods such as Thr (Sadinle et al., 2019) or APS (Romano et al., 2020). More importantly,
motivated by medical diagnosis, we highlighted the ability of ConfTr to manipulate the predicted
confidence sets in various ways. First, ConfTr can “shape” the class-conditional inefficiency dis-
tribution, i.e., reduce inefficiency on specific classes at the cost of higher inefficiency for others.
Second, ConfTr allows to control the coverage-confusion matrix by, e.g., reducing the probability
of including classes other than the ground truth in confidence sets. Finally, this can be extended to
explicitly reduce “overlap” between groups of classes in the predicted confidence sets. In all cases,
ConfTr does not lose the (marginal) coverage guarantee provided by CP.
9
Published as a conference paper at ICLR 2022
Ethics S tatement
Recent deep learning based classifiers, as used in many high-stakes applications, achieve impressive
accuracies on held-out test examples. However, this does not provide sufficient guarantees for safe
deployment. Conformal prediction (CP), instead, predicts confidence sets equipped with a guaran-
tee that the true class is included with specific, user-specified probability. These confidence sets
also provide intuitive uncertainty estimates. We specifically expect CP to be beneficial in the medi-
cal domain, improving trustworthiness among doctors and patients alike by providing performance
guarantees and reliable uncertainty estimates. Yet, the current work does not contain experiments
with personal/sensitive medical data. The presented results are on standard benchmark datasets only.
However, these benefits of CP may not materialize in many applications unless CP can be better
integrated into existing classifiers. These are predominantly deep networks, trained end-to-end to,
e.g., optimize classification performance. CP, in contrast, is agnostic to the underlying model, being
applied as “wrapper” post-training, such that the obtained confidence sets may not be optimal, e.g.,
in terms of size (inefficiency) or composition (i.e., the included classes). Especially in the medical
domain, constraints on the confidence sets can be rather complex. Our conformal training (ConfTr)
integrates CP into the training procedure, allowing to optimize very specific objectives defined on
the predicted confidence sets - without losing the guarantees. In medical diagnosis, smaller Confi-
dence sets may avoid confusion or anxiety among doctors or patients, ultimately leading to better
diagnoses. For example, we can reduce inefficiency (i.e., the ambiguity of predicted conditions)
for conditions that are particularly difficult for doctors to diagnose. Alternatively, ConfTr allows to
avoid confusion between low- and high-risk conditions within the confidence sets.
Generally, beyond medical diagnosis, we believe ConfTr to have positive impact in settings where
additional constraints on confidence sets are relevant in addition to the guarantees and uncertainty
estimates provided by CP.
Reproducibility S tatement
In order to ensure reproducibility, we include a detailed description of our experimental setup
in App. F. We discuss all necessary information for conformal training (ConfTr) as well as our
baselines. This includes architectures, training procedure and hyper-parameters, as well as pre-
processing/data augmentation if applicable. Furthermore, we describe our evaluation procedure
which includes multiple calibration/test splits for conformal prediction (CP) at test time as well as
multiple training runs to capture randomness in the used calibration examples and during training.
To this end, Tab. A reports the training/calibration/test splits of all used datasets and Tab. B the used
hyper-parameters for ConfTr. While Alg. 1 already summarizes the used (smooth) threshold CP
methods and our ConfTr, App. P (specifically Alg. B) lists the corresponding Python implementa-
tion of these key components.
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,
Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wat-
tenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learn-
ing on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software
available from tensorflow.org.
Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra Malik. Uncertainty
sets for image classifiers using conformal prediction. In Proc. of the International Conference on
Learning Representations (ICLR), 2021.
Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. Predictive
inference with the jackknife+. arXiv.org, abs/1905.02928, 2019a.
10
Published as a conference paper at ICLR 2022
Rina Foygel Barber, Emmanuel J. Candes, Aaditya Ramdas, and Ryan J. Tibshirani. The limits of
distribution-free conditional predictive inference. arXiv.org, abs/1903.04684, 2019b.
Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I. Jordan.
Distribution-free, risk-controlling prediction sets. arXiv.org, abs/2101.02703, 2021.
Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes van Diest, Bram van Ginneken, Nico
Karssemeijer, Geert J. S. Litjens, Jeroen A. van der Laak, Meyke Hermsen, Quirine F Manson,
Maschenka C. A. Balkenhol, Oscar G. F. Geessink, Nikolaos Stathonikos, Marcory Crf van Dijk,
Peter Bult, Francisco Beca, Andrew H. Beck, Dayong Wang, Aditya Khosla, Rishab Gargeya, Hu-
mayun Irshad, Aoxiao Zhong, Qi Dou, Quanzheng Li, Hao Chen, Huang Lin, Pheng-Ann Heng,
Christian Hass, Elia Bruni, Quincy Wong, Ugur Halici, Mustafa Umit Oner, Rengul Cetin-Atalay,
Matt Berseth, Vitali Khvatkov, A F Vylegzhanin, Oren Z. Kraus, Muhammad Shaban, Nasir M.
Rajpoot, Ruqayya Awan, Korsuk Sirinukunwattana, Talha Qaiser, Yee-Wah Tsang, David Tellez,
Jonas Annuscheit, Peter Hufnagl, Mira Valkonen, Kimmo Kartasalo, Leena Latonen, Pekka Ru-
usuvuori, Kaisa Liimatainen, Shadi Albarqouni, Bharti Mungal, Ami George, Stefanie Demirci,
Nassir Navab, Seiryo Watanabe, Shigeto Seno, Yoichi Takenaka, Hideo Matsuda, Hady Ahmady
Phoulady, Vassili A. Kovalev, Alexander Kalinovsky, Vitali Liauchuk, Gloria Bueno, M. del Mila-
gro Femandez-CarrobleS,Ismael Serrano, Oscar Deniz, Daniel Racoceanu, and Rui Venancio. Di-
agnostic assessment of deep learning algorithms for detection of lymph node metastases in women
with breast cancer. Journal ofthe American Medical Association (JAMA), 318:2199-2210, 2017.
Anthony Bellotti. Optimized conformal classification using gradient descent approximation.
arXiv.org, abs/2105.11255, 2021.
Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable sorting
and ranking. In Proc. of the International Conference on Machine Learning (ICML), 2020.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
Maxime Cauchois, Suyash Gupta, and John Duchi. Knowing what you know: valid and validated
confidence sets in multiclass and multilabel prediction. arXiv.org, abs/2004.10181, 2020.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. EMNIST: an extension of
MNIST to handwritten letters. arXiv.org, abs/1702.05373, 2017.
Paulo Cortez, Antonio Cerdeira, Fernando Almeida, Telmo Matos, and JoSe Reis. Modeling wine
preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):
547-553, 2009.
Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment:
Learning augmentation policies from data. arXiv.org, abs/1805.09501, 2018.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), 2019.
Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv.org, abs/1708.04552, 2017.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Proc. of the International Conference on Machine Learning (ICML), 2017.
Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple choice learning: Learning
to produce multiple structured outputs. In Advances in Neural Information Processing Systems
(NeurIPS), 2012.
Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
11
Published as a conference paper at ICLR 2022
Yotam Hechtlinger, Barnabas Poczos, and Larry A. Wasserman. Cautious deep learning. arXiv.org,
abs/1805.09460, 2018.
Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.
URL http://github.com/deepmind/dm-haiku.
Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan.
Optax: composable gradient transformation and optimisation, in jax!, 2020. URL http://
github.com/deepmind/optax.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proc. of the International Conference on Machine Learning
(ICML), 2015.
Ayush Jain, David H. Way, Vishakha Gupta, Yi Gao, Guilherme de Oliveira Marinho, Jay Hart-
ford, R. Sayres, K. Kanada, C. Eng, Kunal Nagpal, K. Desalvo, Greg S Corrado, Lily H. Peng,
Dale R. Webster, R. C. Dunn, David Coz, Susan J. Huang, Yun Liu, Peggy Bui, and Yuan Liu.
Development and assessment of an artificial intelligence-based tool for skin condition diagnosis
by primary care physicians and nurse practitioners in teledermatology practices. Journal of the
American Medical Association (JAMA), 4 4, 2021.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, volume 86, pp. 2278-2324, 1998.
Jing Lei, Alessandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore
functional data. Annals of Mathematics and Artificial Intelligence, 74:29-43, 2013.
Yuan Liu, Ayush Jain, Clara Eng, David H. Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guil-
herme de Oliveira Marinho, Jessica Gallegos, Sara Gabriele, Vishakha Gupta, Nalini Singh, Vivek
Natarajan, Rainer Hofmann-Wellenhof, Gregory S. Corrado, Lily H. Peng, Dale R. Webster, Den-
nis Ai, Susan Huang, Yun Liu, R. Carter Dunn, and David Coz. A deep learning system for
differential diagnosis of skin diseases. Nature Medicine, 26:900-908, 2020.
Scott Mayer McKinney, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova,
Hutan Ashrafian, Trevor Back, Mary Chesus, Greg Corrado, Ara Darzi, Mozziyar Etemadi, Flo-
rencia Garcia-Vicente, Fiona J. Gilbert, Mark D. Halling-Brown, Demis Hassabis, Sunny Jansen,
Alan Karthikesalingam, Christopher J. Kelly, Dominic King, Joseph R. Ledsam, David S. Mel-
nick, Hormuz Mostofi, Lily H. Peng, Joshua Jay Reicher, Bernardino Romera-Paredes, Richard
Sidebottom, Mustafa Suleyman, Daniel Tse, Kenneth C. Young, Jeffrey De Fauw, and Shravya
Shetty. International evaluation of an ai system for breast cancer screening. Nature, 577:89-94,
2020.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In Proc. of the International Conference on Machine Learning (ICML).
Yaniv Romano, Evan Patterson, and Emmanuel J. Candes. Conformalized quantile regression. In
Advances in Neural Information Processing Systems (NeurIPS), 2019.
Yaniv Romano, Matteo Sesia, and Emmanuel J. Candes. Classification with valid and adaptive
coverage. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick
Pawlowski, J. Freyberg, Yuan Liu, Zach Beaver, Nam S. Vo, Peggy Bui, Samantha Winter, Patricia
MacWilliams, Greg S Corrado, Umesh Telang, Yun Liu, Taylan Cemgil, A. Karthikesalingam,
Balaji Lakshminarayanan, and Jim Winkens. Does your dermatology classifier know what it
doesn’t know? detecting the long-tail of unseen conditions. arXiv.org, abs/2104.03829, 2021.
Mauricio Sadinle, Jing Lei, and Larry A. Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. arXiv.org, abs/1609.00451, 2016.
12
Published as a conference paper at ICLR 2022
Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classifiers with
bounded error levels. Journal OftheAmerican Statistical Association (JASA), 114(525):223-234,
2019.
Vladimir Vovk. Conditional validity of inductive conformal predictors. In Proc. of the Asian Con-
ference on Machine Learning (ACML), 2012.
Vladimir Vovk. Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence, 74:
9-28, 2013.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World.
Springer-Verlag, Berlin, Heidelberg, 2005.
Bryan Wilder, Eric Horvitz, and Ece Kamar. Learning to complement humans. In Proc. of the
International Joint Conference on Artificial Intelligence (IJCAI), 2020.
John H Williamson. Differentiable parallel approximate sorting networks, 2020. URL https:
//johnhw.github.io/differentiable_sorting/index.md.html.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv.org, abs/1708.07747, 2017.
Yachong Yang and A. Kuchibhotla. Finite-sample efficient conformal prediction. arXiv.org,
abs/2104.13871, 2021.
13
Published as a conference paper at ICLR 2022
A Overview and Outline
In the appendix, we discuss an additional baseline, called coverage training (CoverTr), provide
additional details on our experimental setup and include complementary results. Specifically, the
appendix includes:
•	Additional discussion of related work in Sec. B;
•	Formal statement of the coverage guarantee obtained through conformal prediction (CP) in
Sec. C;
•	Differentiable implementation of APS in Sec. D;
•	Discussion of coverage training (CoverTr) and (Bellotti, 2021) in Sec. E;
•	Details on our experimental setup, including dataset splits, model details and used hyper-
parameters for ConfTr, in Sec. F;
•	Experiments regarding random training and test trials in Sec. G;
•	Hyper-parameter ablation on MNIST in Sec. H;
•	CoverTr and ConfTr ablation on MNIST and Fashion-MNIST in Sec. I;
•	Complete inefficiency (Ineff) results on all datasets in Sec. J;
•	Effect of (standard) ConfTr on class-conditional inefficiency and coverage (Cover) confu-
sion in Sec. K;
•	Additional results for ConfTr shaping the class-conditional inefficiency distribution in
Sec. L;
•	More experiments for ConfTr manipulating coverage confusion in Sec. M;
•	Complementary results for ConfTr reducing mis-coverage (MisCover) in Sec. N;
•	Class-conditional inefficiency and coverage confusion on binary datasets in Sec. O;
•	Python- and Jax (Bradbury et al., 2018) code for ConfTr in Sec. P.
B	Related Work
Conformal prediction (CP) builds on early work by Vovk et al. (2005) considering both regression,
see e.g., (Romano et al., 2019) for references, and classification settings, e.g. (Romano et al., 2020;
Angelopoulos et al., 2021; Cauchois et al., 2020; Hechtlinger et al., 2018). Most of these approaches
follow a split CP approach (Lei et al., 2013) where a held-out calibration set is used, as in the main
paper, however, other variants based on cross-validation (Vovk, 2013) or jackknife (i.e., leave-one-
out) (Barber et al., 2019a) are available. These approaches mostly provide marginal coverage. Vovk
(2012); Barber et al. (2019b) suggest that it is generally difficult or impossible to obtain conditional
coverage. However, Romano et al. (2020) work towards empirically better conditional coverage
and Sadinle et al. (2019) show that efficient class-conditional coverage is possible. Angelopoulos
et al. (2021) extend the work by Romano et al. (2020) to obtain smaller confidence sets at the
expense of the obtained empirical conditional coverage. CP has also been studied in the context of
ensembles (Yang & Kuchibhotla, 2021), allowing to perform model selection based on inefficiency
while keeping coverage guarantees. The work of Bates et al. (2021) can be seen as a CP extension in
which a guarantee on an arbitrary, user-specified risk can be obtained, using a conformal predictor
similar to (Sadinle et al., 2019). Our conformal training (ConfTr) follows the split CP approach
and is specifically targeted towards classification problems. Nevertheless, extensions to regression,
or other CP formulations such as (Bates et al., 2021) during training, are possible. Beyond that,
ConfTr is agnostic to the CP method used at test time and can thus be seen as complementary to the
CP methods discussed above. This means that ConfTr can easily be combined with approaches such
as (Bates et al., 2021) or class-conditional conformal predictors (Sadinle et al., 2019) at test time.
In terms of learning to predict confidence sets, our approach has similarities to the multiple choice
learning of GUzman-Rivera et al. (2012) which yields multiple possible outputs in StrUctUredPredic-
tion settings (e.g., image segmentation). However, the obtained prediction sets are fixed size and no
coverage guarantee is provided. Concurrent work by Bellotti (2021) is discussed in detail in App. E.
14
Published as a conference paper at ICLR 2022
C C overage Guarantee
Following Romano et al. (2020), we briefly state the coverage guarantee obtained by CP in formal
terms: Given that the learning algorithm used is invariant to permutations of the training examples,
and the calibration examples {(Xi , Yi)}i∈Ical are exchangeably drawn from the same distribution
encountered at test time, the discussed CP methods satisfy
P(Y ∈ C(X)) ≥ 1 - α.	(9)
As highlighted in (Romano et al., 2020), this bound is near tight if the scores E(xi) are almost surely
distinct:
P(Y ∈ C(X)) ≤ 1 - α +	1	.	(10)
|Ical | + 1
Note that this is the case for APS due to the uniform random variable U in Eq. (11). (Romano
et al., 2020) notes that there is generally no guarantee on conditional coverage, as this requires addi-
tional assumptions. However, class-conditional coverage can be obtained using Thr as outlined in
(Sadinle et al., 2019). Moreover, Sadinle et al. (2019) show that Thr is the most efficient conformal
predictor given a fixed model πθ, i.e., minimizes inefficiency. We refer to (Sadinle et al., 2019) for
exact statements of the latter two findings.
D Differentiable APS
Our differentiable implementation closely follows the one for Thr outlined in Sec. 2.2. The main
difference is the conformity score E(x, k) computation, i.e.,
Eθ(x,k) := πθ,y(1) (x) + . . . + πθ,y(k-1) (x) + U πθ,y(k) (x),	(11)
where πθ,y(1) (x) ≥ . . . ≥ πθ,y(K) (x) and U is a uniform random variable in [0, 1] to break ties. As
in the calibration step, we use an arbitrary smooth sorting approach for this. This implementation
could easily be extended to include the regularizer of Angelopoulos et al. (2021), as well.
E Coverage Training
As intermediate step towards conformal training (ConfTr), we can also ignore the calibration step
and just differentiate through the prediction step, i.e., Cθ(X; τ). This can be accomplished by fixing
the threshold τ . Then, πθ essentially learns to produce probabilities that yield “good” confidence
sets Cθ(X; τ) for the chosen threshold τ. Following Alg. A, coverage training (CoverTr) computes
Cθ(X; τ) on each mini-batch using a fixed τ. The model’s parameters θ are obtained by solving
minlog (E [L(Cθ(X； τ), Y) + λΩ(Cθ(X; T))]).	(12)
θ
Again, L is the classification loss from Eq. (5) and Ω the size loss from Eq. (3). The classification
loss has to ensure that the true label y is in the predicted confidence set Cθ(X; τ) as the calibration
step is missing. In contrast to ConfTr, CoverTr strictly requires both classification and size loss dur-
ing training. This is because using a fixed threshold τ yields trivial solutions for both classification
and size loss when used in isolation (i.e., L is minimized for Cθ(X; T) = [K] and Ω is minimized
for Cθ (X; τ) = 0). Thus, balancing both terms in Eq. (12) using λ is crucial during training. As
with ConfTr, the threshold T is re-calibrated at test time to obtain a coverage guarantee. Choosing
T for training, in contrast, can be difficult: First, T will likely evolve during training (when πθ gets
1	function COVERAGETRAINING(T, λ)
2	for mini-batch B do
3	C(xi； T) := SMOOTHPRED(∏θ(Xi), T), i∈B
4	LB := Pi∈B L(Cθ(xi; T),yi)
5	Ωb := Pi∈B Ω(Cθ(Xi； T))
6	∆:= Vθ 1∕∣b∣(Lb + λΩB)
7	update parameters θ using ∆
Algorithm A: Coverage Training (CoverTr):
Compared to Alg. 1 for ConfTr, CoverTr sim-
plifies training by not differentiating through
the calibration step and avoiding splitting the
batch B in half. However, fixing the threshold
T can be a problem and training requires both
coverage and size loss.
15
Published as a conference paper at ICLR 2022
Table A: Used Datasets: Summary of train/calibration/test splits, epochs and models used on all
datasets in our experiments. The calibration set is usually less than 10% of the training set. On
most datasets, the test set is roughly two times larger than the calibration set. When computing
random calibration/test splits for evaluation, see text, the number of calibration and test examples
stays constant. * On Camelyon, we use features provided by Wilder et al. (2020) instead of the
original images. ** For EMNIST, we use a custom subset of the “byClass” split.
Dataset Statistics							
Dataset	Train	Cal	Test	Dimensions	Classes	Epochs	Model
CameIyon2016* (Bejnordi et al., 2017)	^^80^	To0^	~~∏~	31	-2-	100	1-layer MLP
GermanCredit (Dua & Graff, 2017)	700	100	200	24	2	100	Linear
WineQuality (Cortez et al., 2009)	4500	500	898	11	2	100	2-layer MLP
MNIST (LeCun et al., 1998)	55k	5k	10k	28 × 28	10	50	Linear
EMNIST** (Cohen et al., 2017)	98.8k	5.2k	18.8k	28 × 28	52	75	2-layer MLP
Fashion-MNIST (Xiao et al., 2017)	55k	5k	10k	28 × 28	10	150	2-layer MLP
CIFAR10 (Krizhevsky, 2009)	45k	5k	10k	32 × 32 × 3	10	150	ResNet-34
CIFAR100 (Krizhevsky, 2009)		45k	5k	10k	32 × 32 × 3	100	150	ReSNet-50
more and more accurate) and, second, the general ballpark of reasonable thresholds τ depends on
the dataset as well as model and is difficult to predict in advance.
In concurrent work by Bellotti (2021) (referred to as Bel), the problem with fixing a threshold τ is
circumvented by using THRL during training, i.e., THR on logits. As the logits are unbounded, the
threshold can be chosen arbitrarily, e.g., τ = 1. As Bel also follows the formulation of Eq. (12), the
approach can be seen as a special case of CoverTr. However, a less flexible coverage loss is used
during training: Instead of Lclass, the loss is meant to enforce a specific coverage level (1 - α) on
each mini-batch. This is done using a squared loss on coverage:
-(1-α)	(13)
for a mini-batch B of examples. In contrast to Eq. (12), Lcov is applied per batch and not per
example. For the size loss, Bellotti (2021) uses κ = 0 in Eq. (3). Besides not providing much
control over the confidence sets, Lcov also encourages coverage (1 - α) instead of perfect coverage.
Nevertheless, this approach is shown to improve inefficiency of ThrL on various UCI datasets (Dua
& Graff, 2017) using linear logistic regression models. The experiments in the main paper show that
this generalizes to non-linear models and more complex datasets. Nevertheless, Bel is restricted
to ThrL which is outperformed significantly by both Thr and APS. Thus, Bel is consistently
outperformed by ConfTr in terms of inefficiency improvements. Moreover, the approach cannot be
used for any of the studied use cases in Sec. 3.3.
Using CoverTr with THR and APS remains problematic. While we found τ ∈ [0.9, 0.99] (or
[-0.1, -0.01 for THRLP) to work reasonably on some datasets, we had difficulties on others, as
highlighted in Sec. I. Moreover, as CoverTr requires balancing coverage L and size loss Ω, hyper-
parameter optimization is more complex compared to ConfTr. By extension, these problems also
limit the applicability of Bel. Thus, we would ideally want to re-calibrate the threshold τ after each
model update. Doing calibration on a larger, held-out calibration set, however, wastes valuable train-
ing examples and compute resources. Thus, ConfTr directly calibrates on each mini-batch and also
differentiates through the calibration step itself to obtain meaningful gradients.
F Experimental Setup
Datasets and Splits: We consider Camelyon2016 (Bejnordi et al., 2017), GermanCredit (Dua &
Graff, 2017), WineQuality (Cortez et al., 2009), MNIST (LeCun et al., 1998), EMNIST (Cohen
et al., 2017), Fashion-MNIST (Cohen et al., 2017) and CIFAR (Krizhevsky, 2009) with a fixed split
of training, calibration and test examples. Tab. A summarizes key statistics of the used datasets
which we elaborate on in the following. Except Camelyon, all datasets are provided by Tensorflow
(Abadi et al., 2015)1. For Camelyon, we use the pre-computed features of Wilder et al. (2020) which
1https://www.tensorflow.org/datasets
16
Published as a conference paper at ICLR 2022
are based on open source code from the Camelyon2016 challenge2 . For datasets providing a default
training/test split, we take the last 10% of training examples as calibration set. On Camelyon, we use
the original training set, but split test examples into 100 validation and 17 test examples. This is be-
cause less than 100 calibration examples are not meaningful for α=0.05. As we evaluate 10 random
calibration/test splits, the few test examples are not problematic in practice. On GermanCredit and
WineQuality, we manually created training/calibration/test splits, roughly matching 70%/10%/20%.
We use the “white wine” subset for WineQuality; to create a binary classification problem, wine
with quality 6 or higher is categorized as “good wine” (class 1), following (Bellotti, 2021). Finally,
for EMNIST, We consider a subset of the “byClass” split that contains 52 = 2 ∙ 26 classes comprised
of all lower and upper case letters. We take the first 122.8k examples, split as in Tab. A.
Models and Training: We consider linear models, multi-layer perceptrons (MLPs) and ResNets
(He et al., 2016) as shoWn in Tab. A. Specifically, We use a linear model on MNIST and German-
Credit, 1- or 2-layer MLPs on Camelyon2016, WineQuality and Fashion-MNIST, and ResNet-34/50
(He et al., 2016) on CIFAR10/100. Models and training are implemented in Jax (Bradbury et al.,
2018)3 and the ResNets folloW the implementation and architecture provided by Haiku (Hennigan
et al., 2020)4. Our l-layer MLPs comprise l hidden layers. We use 32, 256, 128, 64 units per hidden
layer on Camelyon, WineQuality, EMNIST and Fashion-MNIST, respectively. These Were chosen
by grid search over {16, 32, 64, 128, 256}. In all cases, We use ReLU activations (Nair & Hinton)
and batch normalization (Ioffe & Szegedy, 2015). We train using stochastic gradient descent (SGD)
With momentum 0.0005 and Nesterov gradients. The baseline models are trained With cross-entropy
loss, While ConfTr folloWs Alg. 1 and CoverTr folloWs Alg. A. Learning rate and batch size are op-
timized alongside the ConfTr hyper-parameters using grid search, see beloW. The number of epochs
are listed in Tab. A and We folloW a multi-step learning rate schedule, multiplying the initial learn-
ing rate by 0.1 after 2/5, 3/5 and 4/5 of the epochs. We use Haiku’s default initializer. On CIFAR,
We apply Whitening using the per-channel mean and standard deviation computed on the training
set. On the non-image datasets (Camelyon, GermanCredit, WineQuality), We Whiten each feature
individually. On MNIST, EMNIST and Fashion-MNIST, the input pixels are just scaled to [-1, 1].
Except on CIFAR, see next paragraph, We do not use any data augmentation. Finally, We do not use
Platt scaling (Guo et al., 2017) as used in (Angelopoulos et al., 2021).
Fine-Tuning on CIFAR: On CIFAR10 and CIFAR100, We train base ResNet-34/ResNet-50 models
Which are then fine-tuned using Bel, CoverTr or ConfTr. We specifically use a ResNet-34 With only
4 base channels to obtain an accuracy of 82.6%, using only random flips and crops as data augmen-
tation. The rationale is to focus on the results for CP at test time, Without optimizing accuracy of
the base model. On CIFAR100, We use 64 base channels for the ResNet-50 and additionally employ
AutoAugment (Cubuk et al., 2018) and Cutout (Devries & Taylor, 2017) as data augmentation. This
model obtains 73.64% accuracy. These base models are trained on 100% of the training examples
(Without calibration examples). For fine-tuning, the last layer (i.e., logit layer) is re-initialized and
trained using the same data augmentation as applied for the base model, subject to the random train-
ing trials described beloW. We also consider “extending” the ResNet by training a 2-layer MLP
With 128 units per hidden layer on top of the features (instead of re-initializing and fine-tuning the
logit layer). All reported results either correspond to fine-tuned (i.e., linear model on features) or
extended models (i.e., 2-layer MLP on features) trained on these base models.
Hyper-Parameters: The final hyper-parameters selected for ConfTr (for THR at test time) on
all datasets are summarized in Tab. B. These Were obtained using grid search over the fol-
loWing hyper-parameters: batch size in {1000, 500, 100} for WineQuality, MNIST, EMNIST,
Fashion-MNIST and CIFAR, {300, 200, 100, 50} on GermanCredit and {80, 40, 20, 10} on Came-
lyon; learning rate in {0.05, 0.01, 0.005}; temperature T ∈ {0.01, 0.1, 0.5, 1}; size Weight λ ∈
{0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1,5, 10} (c.f. Eq. (1), right); andκ ∈ {0, 1} (c.f.
Eq. (3)). Grid search Was done for each dataset individually on 100% of the training examples
(c.f . Tab. A). That is, for hyper-parameter optimization We did not perform random training trials
as described next. The best hyper-parameters according to inefficiency after evaluating 3 random
calibration/test splits Were selected, both for THR and APS at test time, With and Without Lclass.
2https://github.com/arjunvekariyagithub/camelyon16-grand-challenge
3https://github.com/google/jax
4https://github.com/deepmind/dm-haiku
17
Published as a conference paper at ICLR 2022
Table B: Used ConfTr Hyper-Parameters with and without Lclass for THRLP during training and
Thr at test time. The hyper-parameters for APS at test time might vary slightly from those reported
here. The exact grid search performed to obtained these hyper-parameters can be found in the text.
Note that, while hyper-parameters fluctuate slightly, λ needs to be chosen higher when training with
Lclass. Additionally, and in contrast to Bel, κ = 1 in Eq. (3) performs better, especially combined
with Lclass. Note that dispersion for smooth sorting is fixed to = 0.1.
ConfTr Hyper-Parameters (for THRLP during training and THR at test time)					
Dataset, Method	Batch Size	Learning rate	Temp. T	Size weight λ	K in Eq. (3)
Camelyon, ConfTr	20	0.005	0.1	5	1
Camelyon, ConfTr +Lclass	10	0.01	0.01	5	1
GermanCredit, ConfTr	^^00-	0.05	-1-	5	1
GermanCredit, ConfTr +Lclass	400	0.05	0.1	5	1
WineQuality, ConfTr	∏00-	0.005	0.5	0.05	1
WineQuality, ConfTr +Lclass	100	0.005	0.1	0.5	1
MNIST, COnfTr	"3θ0-	0.05	0.5	0.01	1
MNIST, ConfTr +Lclass	100	0.01	1	0.5	1
EMNIST, ConfTr	∏00-	0.01-	-1-	0.01	1
EMNIST, ConfTr +Lclass	100	0.01	1	5	1
FaShiOn-MNIST, COnfTr	∏00-	0.01-	0.1	0.01	0
Fashion-MNIST, ConfTr +Lclass	100	0.01	0.1	0.5	1
CIFAR10, fine-tune COnfTr	500	0.01-	-1-	0.05	0
CIFAR10, fine-tune ConfTr +Lclass	500	0.05	0.1	1	1
CIFAR10, “extend” ConfTr	100	0.01	1	0.005	0
CIFAR10, “extend” ConfTr +Lclass	500	0.05	0.1	0.1	1
CIFAR100, fine-tune COnfTr	∏00-	0.005	-1-	0.005	0
CIFAR100, fine-tune COnfTr +Lclass	100	0.005	1	0.01	1
Table C: Importance of Random Trials: We report coverage and inefficiency with the correspond-
ing standard deviation across 10 test (left) and 10 training trials (right). ConfTr was trained using
ThrLP if not stated otherwise. For test trials, a fixed model is used. Results for training trials ad-
ditionally include 10 test trials, but the standard deviation is reported only across the training trials.
These results help to disentangle the impact of test and training trials. For example, while ConfTr
with APS (during training) works in the best case, the standard deviation of 3.1 across multiple
training trials indicates that training is not stable.
MNIST:水山 trials, Cover/Ineff for THR				MNIST: Training trials, Cover/InefffOr THR			
Method	Acc	Cover	Ineff	Method	Acc	Cover	Ineff
Baseline ConfTr COnfTr +Lclass	92.45 90.38 91.14	99.09±0.2 99.05±0.2 99.03±0.19	2.23±0.15 2.14±0.13 2.09±0.12	Baseline ConfTr ConfTr +Lclass	92.4±O06 90.2±0.12 91.2±0.05	99.09±0.8 99.03±0.22 99.05±0.21	2.23±0.01 2.18±0.025 2.11±0.028
				COnfTr With APS	87.9±4.81	99.09±0.29	5.79±3.1
Tab. B allows to make several observations. First, on the comparably small (and binary) datasets
Camelyon and GermanCredit, the size weight λ = 5 is rather high. For ConfTr without Lclass, this
just indicates that a higher learning rate could be used. Then using Lclass, however, this shows that
the size loss is rather important for ConfTr, especially on binary datasets. Second, we found the
temperature T to have low impact on results, also see Sec. H. On multiclass datasets, the size weight
λ is usually higher when employing Lclass. Finally, especially with Lclass, using “valid” size loss,
i.e., κ = 1, to not penalize confidence sets of size 1, works better than κ = 0.
Random Training and Test Trials: For statistically meaningful results, we perform random test
and training trials. Following common practice (Angelopoulos et al., 2021), we evaluate CP meth-
ods at test time using 10 random calibration/test splits. To this end, we throw all calibration and test
examples together and sample a new calibration/test split for each trial, preserving the original cali-
bration/test composition which is summarized in Tab. A. Metrics such as coverage and inefficiency
are then empirically evaluated as the average across all test trials. Additionally, and in contrast
to (Bellotti, 2021), we consider random training trials: After hyper-parameters optimization on all
training examples, we train 10 models with the final hyper-parameters on a new training set obtained
by sampling the original one with up to 5 replacements. For example, on MNIST, with 55k training
18
Published as a conference paper at ICLR 2022
Table D: Hyper-Parameter Ablation on MNIST: For ConfTr without Lclass, we report inefficiency
and accuracy when varying hyper-parameters individually: batch size/learning rate, size weight λ,
temperature T and confidence level α. While size weight λ and temperature T have insignificant
impact, too small batch size can prevent ConfTr from converging. Furthermore, the chosen hyper-
parameters do not generalize well to higher confidence levels α ∈ {0.1, 0.05}.
Batch Size and Learning Rate									
Batch Size Learning Rate	1000 0.05	1000 0.01	1000 0.005	500 0.05	500 0.01	500 0.005	100 0.05	100 0.01	100 0.005
Ineff Acc	227Γ 89.05	224a 89.18	4∑4a 89.06	-2Γ18^ 90.23	2.18 90.22	2∏T 90.27	8?04 11.5	^732^ 22.46	^966^ 12.13
Size Weight λ							
λ	0.001	0.005	0.01	0.05	0.1	1	10
Ineff	^T8^	2.18		2?19	^Γ9^	"2.19-	2∏Γ
Acc	90.2	20.23	90.23	90.2	90.25	90.23	90.26
Temperature T
T	0.01	0.05	0.1	0.5	1	5	10
Ineff Acc	2.39 88.54	2.23 89.94	2.2 90.02	2.19 90.24	2.18 90.28	2.2 90.05	2.29 89.63
Confidence Level α (during training)				
ɑ	0.1	0.05	0.01	0.005
Ineff	^807^	^72T	-2Γ18^	2.17
Acc	12.88	39.82	90.23	89.47
examples, we randomly sample 10 training sets of same size with each, on average, containing only
〜68% unique examples from the original training set. Overall, this means that We report, e.g., inef-
ficiency as average over a total of 10 ∙ 10 = 100 random training and test trials. As a consequence,
our evaluation protocol accounts for randomness at test time (i.e., regarding the calibration set) and
at training time (i.e., regarding the training set, model initialization, etc.).
G Importance of Random Trials
In Tab. C We highlight the importance of random training and test trials for evaluation. On the left,
We shoW the impact of trials at test time, i.e., 10 random calibration/test splits, for a fixed model on
MNIST. While the standard deviation of coverage is comparably small, usually ≤ 0.2%, standard
deviation of inefficiency is higher in relative terms. This makes sense as coverage is guaranteed,
While inefficiency depends more strongly on the sampled calibration set. The right table, in contrast,
shoWs that training trials exhibit loWer standard deviation in terms of inefficiency. HoWever, training
With, e.g., APS Will mainly result in high inefficiency, on average, because of large standard devia-
tion. In fact, ConfTr With APS or Thr at training time results in Worse inefficiency mainly because
training is less stable. This supports the importance of running multiple training trials for ConfTr.
H Impact of Hyper-Parameters
In Tab. D, We conduct ablation for individual hyper-parameters of ConfTr With ThrLP and Without
Lclass on MNIST. The hyper-parameters used in the main paper, c.f . Tab. B, are highlighted in bold.
As outlined in Sec. F, hyper-parameter optimization Was conducted on 100% training examples With
only 3 random test trials, While Tab. D shoWs results using random training and test trials. We found
batch size and learning rate to be most impactful. While batch sizes 1000 and 500 both Work, batch
size 100 prevents ConfTr from converging properly. This might be due to the used α = 0.01 Which
might be too loW for batch size 100 Where only 50 examples are available for calibration during
training. Without Lclass, the size Weight λ merely scales the learning rate and, thus, has little to
no impact. For ConfTr with Lclass, We generally found the size Weight λ to be more important for
balancing classification loss L and size loss Ω in Eq. (4). Temperature has no significant impact,
although a temperature of 0.5 or 1 Works best. Finally, the hyper-parameters do generalize to a
loWer confidence level α = 0.005. Significantly loWer values, e.g., α = 0.001, are, hoWever,
19
Published as a conference paper at ICLR 2022
Table E: Ablation for CoverTr and ConfTr on MNIST and Fashion-MNIST: We report ineffi-
ciency and accuracy for (Bellotti, 2021) (Bel), CoverTr and ConfTr considering various CP methods
for training and testing. Bel outperforms the baseline when using ThrL, but does not do so for Thr
on MNIST. CoverTr with Thr or APS during training is challenging, resulting in high inefficiency
(mainly due to large variation among training trials, c.f . Tab. C), justifying our choice of THRLP
for ConfTr. Also CoverTr is unable to improve over the Thr baseline. Similar observations hold on
Fashion-MNIST where, however, CoverTr with Thr or APS was not possible.
MNIST: Ablation for CoverTr and ConfTr													
Method	Baseline			Bel		CoverTr					COnfTr				
Train				ThrL		Thr	APS	THRLP		ThrLP	ThrLP	+Lclass	
Test	THRL	THR	APS	ThrL	Thr	THR	^APS	Thr	APS	THR	APS	Thr	APS
Avg. Ineff Avg. ACC	3.57 92.39	223Γ 92.39	2.5 92.39	2.73 81.41	2^7TΓ 90.01	^63^ 83.85	88.53	T5" 92.63	^76^ 92.63	2.18 90.24	2.16 90.21	ɪɪl" 91.18	ɪrr 91.35
Fashion-MNIST: AblatiOn for COVerTr and COnfTr											
Method	Baseline			Bel		CoverTr			COnfTr				
Train				THRL	-		Thr	ThrLP	ThrLP	ThrLP	_+L	class
Test	ThrL	Thr	APS	ThrL	Thr	THR	THR	THR	APS	THR	"APS^
Ineff-	2.52	ɪor	^36^	1.83	~T9~	"4.03-	2.69	1.69	1.82	T67^	TTT
Acc	89.16	89.16	89.16	84.29	84.61	89.23	87.48	88.86	87.43	89.23	88.69
not meaningful due to the batch size of 500. However, significantly higher confidence levels, e.g.,
α = 0.1 or α = 0.05, require re-optimizing the other hyper-parameters.
I	CoverTr and ConfTr Ablation on MNIST and Fashion-MNIST
In Tab. E, we present an ablation for CoverTr, see Sec. E, and ConfTr on MNIST, using a linear
model, and Fashion-MNIST, using a 2-layer MLP. Bel is generally able to improve inefficiency of
ThrL. Using Thr, however, Bel worsens inefficiency on MNIST significantly, while improving
slightly over the baseline on Fashion-MNIST. As a result, the improvement of ConfTr over Bel is
also less significant on Fashion-MNIST. Using CoverTr with Thr or APS during training works
poorly. As described in Tab. C, this is mainly due to a high variation across training runs, i.e.,
individual models might work well, but training is not stable enough to get consistent improvements.
Thus, on MNIST, inefficiency for CoverTr with Thr and APS is very high. Moreover, on Fashion-
MNIST, we were unable to train CoverTr with Thr and APS. Using ThrLP, training with CoverTr
works and is reasonably stable, but does not improve over the baseline. It does improve over Bel on
MNIST though. As described in the main paper, we suspect the fixed threshold τ to be problematic.
Overall, however, only ConfTr is able to outperform the Thr baseline on both datasets. Here,
ConfTr with Lclass works slightly better than without.
J	All Inefficiency Results
Tab. F shows complementary results for ConfTr on CIFAR10, EMNIST and CIFAR100. For results
on MNIST and Fashion-MNIST, see Tab. D. On CIFAR10, we also include ConfTr using a 2-layer
MLP on top OfResNet features - instead of the linear model used in the main paper. In Tab. F, this is
referred to as “extending”. However, inefficiency increases slightly compared to re-initializing and
training just the (linear) logit layer. This shows that the smaller inefficiency improvements on CIFAR
shown in the main paper are not due to the linear model used, but rather caused by the features
themselves. We suspect that this is because the features are trained to optimize cross-entropy loss,
leaving ConfTr less flexibility to optimize inefficiency. In Tab. G, we consider three binary datasets,
i.e., WineQuality, GermanCredit and Camelyon. On binary datasets, THRL, THR and APS perform
very similar. This already suggests that there is little room for inefficiency improvements. Indeed,
ConfTr is not able to improve inefficiency significantly. However, this is partly due to our thorough
evaluation scheme: On Camelyon (using α=0.05), we do not report averages across all training
trials, but the results corresponding to the best model. This is because sub-sampling the training
examples is unreasonable given that there are only 280 of them. Thus, Camelyon shows that ConfTr
can improve inefficiency. On WineQuality or GermanCredit, however, this is “hidden” in reporting
averages across 10 training runs.
20
Published as a conference paper at ICLR 2022
Table F: Inefficiency and Accuracy on Multiclass Datasets: Complementing Tab. 1 in the main
paper, we include results for CoverTr on CIFAR10. Furthermore, we consider training a non-linear
2-layer MLP on the ResNet features on CIFAR10, c.f . Sec. F, alongside the ensemble results from
the main paper. We report inefficiency and accuracy in all cases, focusing on ConfTr in comparison
to Bel. On EMNIST, we additionally consider α = 0.005, 0.001 (for the baseline and ConfTr only).
As in the main paper, ConfTr consistently improves inefficiency of Thr and APS.
CIFAR10: Fine-Tuning and “Extending”											
				Fine-tuning						“Extend”	
Method	Baselines			Bel	CoverTr		ConfTr					ConfTr 一	
Train				THRL	THRLP	ThrLP	ThrLP	+Lclass		ThrLP	+Lclass
Test	THRL	Thr	APS	Thr	Thr	Thr	APS	Thr	APS	Thr	Thr
Ineff-	3.92	T93"	^3^	2.93	2.84	2.88	3.05	T84	2.93	2.89	2.96
ACC	82.6	82.6	82.6	82.18	82.36	82.32	82.34	82.4	82.4	82.3	82.23
CIFAR10: EnSemble ReSultS							
Method	(Ensemble Models)			Ensemble+MLP			Ensemble +ConfTr
Train Test	ThrL	Thr	APS	ThrL	Thr	APS	THRLP Thr
Avg. Ineff	4.19		37T	3.12	~1Λ~	2.77	-235-
Best Ineff	3.74	2.84	3.17	3.0	2.33	2.71	2.3
Avg. Acc	80.65	80.65	80.65	85.88	85.88	85.88	85.88
Best Acc	82.58	82.58	82.58	86.01	86.01	86.01	86.02
EMNIST
Method	Baselines			Bel 一		Confrr			
Train				ThrL	ThrL	ThrLP	ThrLP	+L	class I
Test	THRL	THR	APS	THRL	THR	THR	APS	Thr	APS
Ineff	5.07	^66^	4^33~	3.95	3.48	2.66	2.86	2.49	^87^
Ineff, α=0.005	9.23	4.1	6.04	—	一	3.37	—	一	—
Ineff, α=0.001	23.89	15.73	19.33	—	一	13.65	—	一	—
Acc	83.79	83.79	83.79	80.69	80.69	77.1	77.43	77.49	78.09
CIFAR100	一								
Method	Baselines			Bel	ConfTr			
Train				THRL	ThrLP	ThrLP	_+L	class
Test	THRL	THR	"APS	THR	THR	APS	THR	"APS
Ineff-	19.22	10.63	16.62	10.91	10.78	12.99	10.44	12.73
Acc	73.36	73.36	73.36	72.65	72.02	72.78	73.27	72.99
K Effect of ConfTr on Class-Conditional Inefficiency and
Coverage Confusion
Fig. G shows that standard ConfTr (without Lclass) does not have a significant influence on the class-
conditional inefficiency distribution compared to the baseline. Similarly, ConfTr with Lclass and
identity loss matrix L = IK does not influence coverage confusion besides reducing overall inef-
ficiency. Specifically, on MNIST, Fashion-MNIST and CIFAR10, we show the class-conditional
inefficiency distribution (left) as well as the coverage confusion matrices (middle and right) for the
baseline and ConfTr. On the left, we consider ConfTr without Lclass, and on the right with Lclass.
As can be seen, only an overall reduction of inefficiency is visible, the distribution of Ineff[y], c.f .
Eq. (6), across classes y remains roughly the same. For coverage confusion Σ from Eq. (7), the
same observation can be made, i.e., an overall reduction of inefficiency also reduces confusion, but
the spatial pattern remains the same. Thus, in the main paper and the following experiments, we al-
ways highlight the improvement over standard ConfTr, without Lclass for reducing class-conditional
inefficiency and with Lclass for changing coverage confusion or improving MisCover.
21
Published as a conference paper at ICLR 2022
Table G: Inefficiency and Accuracy on Binary Datasets. Experimental results on the binary
datasets WineQuality, GermanCredit and Camelyon. While we include APS on WineQuality, we
focus on Thr on GermanCredit and Camelyon due to slightly lower inefficiency. However, ThrL,
Thr and APS perform very similarly on all tested binary datasets. Generally, ConfTr does not im-
prove significantly over the baseline. * On Camelyon, we report the best results without training
trials as sub-sampling the 280 training examples is prohibitively expensive.
WineQuality									
Method	Baselines			Bel	CoverTr	COnfTr			
Train				THRL	THRLP	ThrLP	ThrLP	_+L	class
Test	THRL	THR	APS	THR	THR	THR	APS	THR	"APS
Ineff, α=0.01	1.76	T76^	1.79	1.77	1.81	1.75	1.82	1.74	1.77
Ineff, α=0.05	1.48	1.49	1.53	1.57	1.50	1.51	—	1.52	一
Acc	82.82	82.82	82.82	71.3	81.5	73.8	74.24	73.91	73.91
GermanCredit						
Method	Baselines			Bel	COnfTr 一	
Train				THRL	ThrLP	+Lclass
Test	THRL	THR	APS	THR	THR	THR
Ineff- Acc	1.89 74.4	1.86 74.4	τ≡z 74.4	1.85 72.35	1.88 72.81	1.77 69.5
Camelyon* α=0.05						
Method	Baselines			Bel	COnfTr 一	
Train				THRL	ThrLP	+Lclass
Test	THRL	THR	APS	THR	THR	THR
Best Ineff	1.41	1.47	1.59	1.25	-1.2	1.25
Best Acc	88	88	88	92	91.5	85
L S haping Class-Conditional Inefficiency on Other Datasets
Fig. A and B provide complementary results demonstrating the ability of ConfTr to shape the class-
or group-conditional inefficiency distribution. First, Fig. A plots inefficiency for individual classes
on CIFAR10 and coarse classes on CIFAR100. In both cases, significant inefficiency reductions
are possible for high weights ω in Eq. (3), irrespective or whether the corresponding (coarse) class
has above-average inefficiency to begin with. This means that inefficiency reduction is possible for
easier and harder classes alike. Second, Fig. B plots the relative inefficiency changes, in percentage,
possible per-class or group on MNIST, Fashion-MNIST and CIFAR100. For CIFAR100, we show
only the first 10 classes for brevity. In all cases, significant inefficiency reductions are possible, at the
expense of a slight increases in average inefficiency across all classes. Here, MNIST is considerably
>ucυ-uευc-
CIFAR10: Reducing InefT of Class 0
----- Ineff Class 0 Avg. Ineff
0	2	4	6 B
SlzewelghtCiassO
>ucυ-uευc-
CIFAR10: Reducing InefT of Class 3
0	2	4	6 B 10
Size Weight Class 3
CIFAR100: Reducing Ineff of Group 9
0	2	4 β e 10
Size Weight Group 9
----InefFGroup9 ----Avg. Ineff
CIFAR100: Reducing Ineff of Group 15
>15	___________一
g	：
一
te	：
2	5	;	 IneffGrcup	15	--Avg. Ineff
0	2	4 β e 10
Size Weight Group 15
Figure A: Reducing Class- and Group-Conditional Inefficiency on CIFAR. Results, ComPlemen-
tary to Fig. 3, showing the impact of higher size weights ω in Eq. (3) for classes 0 and 3 (“airplane
and “cat”) on CIFAR10 and coarse classes 9 and 15 (“large man-made outdoor things” and “reP-
tiles”) on CIFAR100. ConfTr allows to reduce inefficiency (blue) in all cases, irresPective of whether
inefficiency is generally above or below average (green).
10
MNIST: Ineff Improvement by Class
MNlST: InefT Reduction by Group
F-MNlST: Ineff Improvement by Class
CIFAR1OO: Ineff Improvement by Class
0	2	4	6	8
Class
0	2	4	6	8
Class
0	2	4	6	8
Class
0	2	4	6	8
Class
Figure B: Relative Class and Group-Conditional Inefficiency Improvements: Complementing
the main PaPer, we Plot the Possible (relative) inefficiency reduction by class or grouP (“odd” vs
“even”) on MNIST and Fashion-MNIST. On CIFAR100, we consider the first 10 classes for brevity.
In all cases, significant per-class or -group inefficiency reductions are possible.
22
Published as a conference paper at ICLR 2022
SS<ODφml
2	3	4	5	6	7
Predicted Class
4.32	4.25	4.21	4.18	3.44	3.10
3.49	3.50	3.48	3.33	2.87	2.62
9.11	8.82	8.81	8.80	8.61	8.39
9.24	8.26	8.20	8.11	7.77	7.56
0123456789
Predicted Class
Figure C: Full Coverage Confusion
Matrix on CIFAR10: We plot the
full coverage confusion matrices Σ
from Eq. (7) on CIFAR10 for the
ConfTr baseline (with LCIaSS, left) and
ConfTr with Ly,k = 1 in Eq. (5) for
classes y,k ∈ {4, 5, 7} (right, high-
lighted in red).
¥9lnπ⅛TIn
ISINWH 。-NVHlD
4.00
Fashlon-MNlST: Reducing Cov Confusion for Class/Row 6
—
1⅛ΦM -λcoctλ-q⅛
0123456789
Class
CIFAR10: Reducing Cov Confusion for Class/Row 3
5.05
4.52
5.19
5.13
5.13
4.98
5.09
4.94
0.01
M 0.05
4.89
4.65
0123456789
Class
0	0.01 0.05	0.1	0.5
Off-Diagonal Weight





Figure D: Coverage Confusion Changes on Fashion-MNIST and CIFAR10: Left: coverage con-
fusion change when targeting classes 4 and 6 (“coat” and “shirt”) on Fashion-MNIST and 3 and 5
(“cat” and “dog”) on CIFAR10. The separate cell on the left is the ConfTr baseline which is, up to
slight variations, close to Ly,k = 0. Middle and right: coverage confusion for a whole row, i.e.,
Σy,k with fixed class y and all k 6= y. We show row 6 on Fashion-MNIST and 3 on CIFAR10. In
both cases, coverage confusion can be reduced significantly.
easier than Fashion-MNIST: higher inefficiency reductions are possible per class and the cost in
terms of average inefficiency increase is smaller. On CIFAR100, inefficiency reductions of 40% or
more are possible. This is likely because of the high number of classes, i.e., ConfTr has a lot of
flexibility to find suitable trade-offs during training.
M Manipulating Coverage Confusion on Other Datasets
Fig. C to E provide additional results for reducing coverage confusion using ConfTr. First, in Fig. C
we show the full coverage confusion matrices for the ConfTr baseline (with Lclass, left) and ConfTr
with Ly,k = 1, y 6= k ∈ {4, 5, 7} (right, marked in red) on CIFAR10. This allows to get the
complete picture of how coverage confusion changes and the involved trade-offs. As demonstrated
in the main paper, coverage confusion for, e.g., classes 4 and 5 (“deer” and “dog”) reduces. However,
coverage confusion for other class pairs might increase slightly. Then, supplementary to Fig. 4 in
the main paper, we provide the actual numbers in Fig. D. In particular, we visualize how the actual
coverage confusion entries (left) or rows (right) change depending on the off-diagonal weights Ly,k .
Finally, Fig. E presents additional results on MNIST and CIFAR10. From these examples it can be
seen that reducing coverage confusion is easier on MNIST, reducing linearly with the corresponding
penalty Ly,k. Moreover, the achieved reductions are more significant. On CIFAR10, in contrast,
coverage confusion reduces very quickly for small Ly,k before stagnating for larger Ly,k . At the
same time, not all targeted class pairs might yield significant coverage confusion reductions.
0.0	0.2	0.4	0.6	0.8	1.0
Off-Diagonal Weight
MNIST: Reducing Cov Confusion Class 9
(％)ω6UB5 uowauoɔ >0u
0.0	0.2	0.4	0.6	0.8	1.0
Off-Diagonal Weight
0.0	0.2	0.4	0.6	0.8	1.0
Off-Diagonal Weight
(％)ω6UB5 uowauoɔ >0u
Figure E: Coverage Confusion Reduction on MNIST and CIFAR10: Controlling coverage con-
fusion for various class pairs. On MNIST, coverage confusion reduction is usually more significant
and the reduction scales roughly linear with the weight Ly,k. On CIFAR10, in contrast, coverage
confusion cannot always be reduced for multiple class pairs at the same time (see light gray).
23
Published as a conference paper at ICLR 2022
Table H: Mis-Coverage on MNIST, Fashion-MNIST and CIFAR10: We present inefficiency and
mis-coverage for various cases: On MNIST, we consider 2 vs. other classes as well as even vs. odd
classes. In both cases, mis-coverage can be reduced significantly. As in the main paper, however,
reducing MisCover0→1 usually increases MisCover1→0 and vice-versa. On Fashion-MNIST, we
consider 6 (“shirt”) vs. other classes. Only on CIFAR10, considering “vehicles” vs. “animals”, mis-
coverage cannot be reduced significantly. In particular, we were unable to reduce MisCover1→0 .
Ko= 2、	vs. Ki= Others				Ko= Even vs.		Ki=Odd	
MNIST		MisCover ]			MNIST		MisCover ]	
Method	Ineff	0→1	1→0		Method	Inef	0→1	1→0
ConfTr	2.11	49.68	14.74		ConfTr	2.11	38.84	38.69
LKo ,Kι =1	2.15	36.63	17.42		LKo ,Kι =1	2.16	29.36	49.08
LKι,Ko =1	2.09	51.54	7.62		LK1,K0 =1	2.09	44.3	26.08
Ko = 6 ("shirt") vs. Ki = Others			
F-MNIST	MisCover ]		
Method	Ineff	0→1	1→0
ConfTr	T67"	80:28	20.93
LKo Ki =1	1.70	72.58	25.81
LKi ,Ko =1	1.72	81.18	17.66
Ko= “vehicles” vs. Ki= “animals”			
CIFAR10	MisCover ]		
Method	Ineff	0→1	1→0
ConfTr	2.84	1222	-16.45-
LKo Ki =1	2.92	20.00	22.69
LK1,K0 =1	2.87	24.76	16.73
N MisCover Results on Additional Datasets
Tab. H provides mis-coverage results for different settings on MNIST, Fashion-MNIST and CI-
FAR10. As in the main paper, we are able to reduce mis-coverage significantly on MNIST and
Fashion-MNIST. Only on CIFAR10, considering “vehicles” vs. “animals” as on CIFAR100 in
the main paper, we are unable to obtain significant reductions. While, we are able to reduce
MisCover0→1 slightly from 22.22% to 20%, MisCover1→0 increases slightly from 16.45% to
16.73% even for high off-diagonal weights used in L. Compared to CIFAR100, this might be due to
less flexibility to find suitable trade-offs as CIFAR10 has only 10 classes. Moreover, mis-coverages
on CIFAR10 are rather small to begin with, indicating that vehicles and animals do not overlap much
by default.
O Additional Results on B inary Datasets
Fig. F shows results complementing Fig. 5 (right) in the main paper. Specifically, we show that
reducing inefficiency for class 1 (“good wine”) is unfortunately not possible. This might also be
due to the fact that class 1 is the majority class, with 〜63% of examples. However, in addition
to improving coverage conditioned on class 0, we are able to reduce coverage confusion Σ0,1, c.f.
Sec. 3.3. We found that these results generalize to GermanCredit, however, being less pronounced,
presumably because of significantly fewer training and calibration examples.
WIneQuaIIty: Reducing Ineff of Class 1
2.α
AUUa-yy=au-
Size Weight Class 1
---Sizel ------SizeO
SSeO SrUL SS8O3el
Cov Confusion
Predicted Class
SS3 31MF
O 1
Predicted Class
0-1 Weight 0.10
O 1
Predicted Class
0-1 W⅛lflht 0.50
O 1
O 1
Predicted Class
0-1 weight 0.05
1 33.83
O 1
Pred ⅛tE Class
SS8_。eel-
PreiIiCtE C∣ess	PEictE CIesS
SSeo 户 «s_u
j
O 1	O
Figure F: Manipulating Inefficiency and Coverage Confusion on WineQuality: Complementing
Fig. 5 (right) in the main paper, we plot the possible inefficiency reduction for class 1 (“good wine”,
left) and full coverage confusion matrices for increased L0,0 > 1 and L1,0 > 0 (right, top and
bottom, respectively). While we can reduce inefficiency for class 0 (“bad wine”), this is not possible
for class 1. However, class-conditional coverage for class 0 can be improved significantly and we
can reduce coverage confusion Σ0,1.
24
Published as a conference paper at ICLR 2022
P Pseudo Code
Alg. B presents code in Python, using Jax (Bradbury et al., 2018), Haiku (Hennigan et al., 2020)
and Optax (Hessel et al., 2020). We assume access to a smooth sorting routine that allows to
compute quantiles in a differentiable way: smooth_quantile. Specifically, Alg. B provides
an exemplary implementation of ConfTr with (smooth) THR and Lclass as outlined in Alg. 1 in
the main paper. smooth_predict_threshold and smooth_calibrate_threshold
implement differentiable prediction and calibration steps for Thr. These implementations
are used in compute_loss_and_error to “simulate” CP on mini-batches during train-
ing. Size loss Ω from Eq. (3) and classification loss from Eq. (5) are implemented in
compute_size_loss and compute_general_classification_loss. Note that the
definition of compute_loss_and_error distinguishes between trainable_params and
fixed_params, allowing to fine-tune a pre-trained model.
25
Published as a conference paper at ICLR 2022
MNlST: Coverage Confusion Matrix for Baseline+Thr
MNIST: Coverage Confusion Matrix for ConfTr+Lclass+Thr
AUUəptəu-
MNlST: Baseline+Thr
0	2	4	6	8
Class
MNIST: ConfΓr+Thr
SSeOəruj.
0.01	0.93	0.46	0.11	2.34	0.60	0.19	0.51	0.20
11.20	2.83	1.70	0.07	0.75	0.19	0.85	3.38	0.19
9.67
0.02
sse□φrul
0.69
9.68
3.36
0123456789
Predicted Class
F-MNIST: Coverage Confusion Matrix for Baseline+Thr
9
0	12
4.92
4.73
7	8	9
Predicted Class
0	2	4	6	8
Class
F-MNIST: Baseline+Thr
>ocφ-o⅝c-
F-MNIST: Coverage Confusion Matrix for ConfTr+Lclass+Thr
o
SSeOφnJJ.
ssn>□Orul
F-MNIST: ConfΓr+Thr
CIFAR10: Baseline+Thr
0	2	4	6	8
Class
CIFAR10: ConfΓr+Thr
0123456789
Predicted Class
CIFAR10: Coverage Confusion Matrix for Baseline+Thr
SSeOφnJJ.
8	9
0	12
3	4	5	6
Predicted Class
3.98
3.81
4.06
3.81
D.∣
D.54
4.96
4.72
4.38
7
8
9
0123456789
Predicted Class
CIFAR1O: Coverage Confusion Matrix for ConfTr+Lclass+Thr
o
1
2
ssn>0① ruɪ
7
8
9
0123456789
Predicted Class
4	6
Class
Auuapwau-
AUUəp 柜 əu- AUUəp 柜 əu- AUUap⅛pu 一

3	4	5	6
Figure G: Class-Conditional Inefficiency and Coverage Confusion: Comparison between base-
line and ConfTr regarding class-conditional inefficiency and coverage confusion Σ, c.f . Sec. 3.3. For
the inefficiency comparison, we consider ConfTr without Lclass, while for coverage confusion, Con-
fTr was trained with Lclass. As ConfTr reduces overall inefficiency quite significantly on MNIST and
Fashion-MNIST, class-conditional inefficiency is also lower, on average. But the distribution across
classes remains similar. The same holds for coverage confusion, where lower overall inefficiency
reduces confusion across the matrix, but the “pattern” remains roughly the same. On CIFAR10,
ConfTr does not improve average inefficiency significantly, such that the confusion matrix remains
mostly the same.
26
Published as a conference paper at ICLR 2022
Algorithm B Python Pseudo-Code for ConfTr: We present code based on our Python and Jax
implementation of ConfTr. In particular, we include smooth calibration and prediction steps for
THR as well as the classification loss LCIaSS and the Size loss Ω. Instead of including a full train-
ing loop, compute_loss_and_error shows how to compute the loss which can then be called
using jax.value_and_grad(compute_loss_and_error, has_aux=True) and used
for training using Optax. Hyper-parameters, including alpha, dispersion, size_weight,
temperature, loss_matrix, size_weights and weight_decay, are not defined ex-
plicitly for brevity. smooth_quantile is assumed to be a provided differentiable quantile com-
putation method. Finally, model can be any Jax/Haiku model.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
import jax
import jax.numpy as jnp
import haiku as hk
def smooth_predict_threshold(
probabilities: jnp.ndarray, tau: float, temperature: float) -> jnp.ndarray:
"""Smooth implementation of prediction step for Thr."""
return jax.nn.sigmoid((probabilities - tau) / temperature)
def smooth_calibrate_threshold(
probabilities: jnp.ndarray, labels: jnp.ndarray,
alpha: float, dispersion: float) -> float:
"""Smooth implementation of the calibration step for Thr."""
conformity_scores = probabilities[jnp.arange(probabilities.shape[0]), labels.astype(int)]
return smooth_quantile(array, dispersion, (1 + 1./array.shape[0]) * alpha)
def compute_general_classification_loss(
confidence_sets: jnp.ndarray, labels: jnp.ndarray,
loss_matrix: jnp.ndarray) -> jnp.ndarray:
"""Compute the classification loss Lclass on the given confidence sets."""
one_hot_labels = jax.nn.one_hot(labels, confidence_sets.shape[1])
l1	= (1 - confidence_sets) * one_hot_labels * loss_matrix[labels]
l2	= confidence_sets * (1 - one_hot_labels) * loss_matrix[labels]
loss = jnp.sum(jnp.maximum(l1 + l2, jnp.zeros_like(l1)), axis=1)
return jnp.mean(loss)
def compute_size_loss(
confidence_sets: jnp.ndarray, target_size: int, weights: jnp.ndarray) -> jnp.ndarray:
"""Compute size loss."""
return jnp.mean(weights * jnp.maximum(jnp.sum(confidence_sets, axis=1) - target_size, 0))
FlatMapping = Union[hk.Params, hk.State]
def compute_loss_and_error(
trainable_params: FlatMapping, fixed_params: FlatMapping, inputs: jnp.ndarray,
labels: jnp.ndarray, model_state: FlatMapping, training: bool, rng: jnp.ndarray,
) -> Tuple[jnp.ndarray, FlatMapping]:
"""Compute classification and size loss through calibration/prediction."""
params = hk.data_structures.merge(trainable_params, fixed_params)
#	Model is a Haiku model, e.g., ResNet or MLP.
logits, new_model_state = model.apply(params, model_state, rng, inputs, training=training)
probabilities = jax.nn.softmax(logits, axis=1)
val_split = int(0.5 * probabilities.shape[0])
val_probabilities = probabilities[:val_split]
val_labels = labels[:val_split]
test_probabilities = probabilities[val_split:]
test_labels = labels[val_split:]
#	Calibrate on the calibration probabilities with ground truth labels:
val_tau = smooth_calibrate_threshold(val_probabilities, val_labels, alpha, dispersion)
#	Predict on the test probabilities:
test_confidence_sets = smooth_predict_threshold(test_probabilities, val_tau, rng)
#	Compute the classification loss Lclass with a fixed loss matrix L:
classification_loss = compute_general_classification_loss(
test_confidence_sets, test_labels, loss_matrix)
#	Optionally set size weights determined by ground truth labels:
weights = size_weights[test_labels]
#	Compute size loss multiplied by size weight:
size_loss = size_weight * compute_size_loss(confidence_sets, weights)
#	Compute the log of classification and size loss:
loss = jnp.log(classification_loss + size_loss + 1e-8)
loss += weight_decay * sum(jnp.sum(jnp.square(param)) for param in jax.tree_leaves(params))
return loss, new_model_state
27