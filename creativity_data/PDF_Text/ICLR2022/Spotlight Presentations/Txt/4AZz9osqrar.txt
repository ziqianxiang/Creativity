Published as a conference paper at ICLR 2022
Self-supervised Learning is More Robust to
Dataset Imbalance
Hong Liu
Stanford University
hliu99@stanford.edu
Jeff Z. HaoChen
Stanford University
jhaochen@stanford.edu
Adrien Gaidon
Toyota Research Institute
adrien.gaidon@tri.global
Tengyu Ma
Stanford University
tengyuma@stanford.edu
Ab stract
Self-supervised learning (SSL) is a scalable way to learn general visual represen-
tations since it learns without labels. However, large-scale unlabeled datasets in
the wild often have long-tailed label distributions, where we know little about
the behavior of SSL. In this work, we systematically investigate self-supervised
learning under dataset imbalance. First, we find out via extensive experiments
that off-the-shelf self-supervised representations are already more robust to class
imbalance than supervised representations. The performance gap between bal-
anced and imbalanced pre-training with SSL is significantly smaller than the gap
with supervised learning, across sample sizes, for both in-domain and, especially,
out-of-domain evaluation. Second, towards understanding the robustness of SSL,
we hypothesize that SSL learns richer features from frequent data: it may learn
label-irrelevant-but-transferable features that help classify the rare classes and
downstream tasks. In contrast, supervised learning has no incentive to learn fea-
tures irrelevant to the labels from frequent examples. We validate this hypothesis
with semi-synthetic experiments and theoretical analyses on a simplified setting.
Third, inspired by the theoretical insights, we devise a re-weighted regularization
technique that consistently improves the SSL representation quality on imbalanced
datasets with several evaluation criteria, closing the small gap between balanced
and imbalanced datasets with the same number of examples.
1 Introduction
Self-supervised learning (SSL) is an important paradigm of machine learning, because it can lever-
age the availability of large-scale unlabeled datasets to learn representations for a wide range of
downstream tasks and datasets (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al.,
2020; Chen & He, 2021). Current SSL algorithms are mostly trained on curated, balanced datasets,
but large-scale unlabeled datasets in the wild are inevitably imbalanced with a long-tailed label
distribution (Reed, 2001; Liu et al., 2019). Curating a class-balanced unlabeled dataset requires the
knowledge of labels, which defeats the purpose of leveraging unlabeled data by SSL.
The behavior of SSL algorithms under dataset imbalance remains largely underexplored in the
literature, but extensive studies do not bode well for supervised learning (SL) with imbalanced
datasets. The performance of vanilla supervised methods degrades significantly on class-imbalanced
datasets (Cui et al., 2019; Cao et al., 2019; Buda et al., 2018), posing challenges to practical
applications such as instance segmentation (Tang et al., 2020) and depth estimation (Yang et al.,
2021). Many recent works address this issue with various regularization and re-weighting/re-sampling
techniques (Ando & Huang, 2017; Wang et al., 2017b; Jamal et al., 2020; Cui et al., 2019; Cao et al.,
2019; 2021; Tian et al., 2020; Hong et al., 2021; Wang et al., 2021).
In this work, we systematically investigate the representation quality of SSL algorithms under class
imbalance. Perhaps surprisingly, we find out that off-the-shelf SSL representations are already more
robust to dataset imbalance than the representations learned by supervised pre-training. We evaluate
1
Published as a conference paper at ICLR 2022
Imbalanced Ratio r = 0.004	： Imbalanced Ratio r = 0.0025
252015105 O
%¾sseα p3uu∙5rq304 φ> de。
14K 29K 58K 87K 116K IOK 20K 40K 60K 80K
Total Number of Pre-training Examples n
876543210
% WSSRa P3uuew8 E 8>% ⑥U dee
Imbalanced Ratio r = 0.004
(a) In Domain (ID).	(b) Out of Domain (OOD).
Figure 1: Relative performance gap (lower is better) between imbalanced and balanced represen-
tation learning. The gap is much smaller for self-supervised (MoCo v2) representations (∆SSL in
blue) vs. supervised ones (∆SL in red) on long-tailed ImageNet with various number of examples n,
across both ID (a) and OOD (b) evaluations. See Equation (1) for the precise definition of the relative
performance gap and and Figure 2 for the absolute performance.
the representation quality by linear probe on in-domain (ID) data and finetuning on out-of-domain
(OOD) data. We compare the robustness of SL and SSL representations by computing the gap
between the performance of the representations pre-trained on balanced and imbalanced datasets of
the same sizes. We observe that the balance-imbalance gap for SSL is much smaller than SL, under a
variety of configurations with varying dataset sizes and imbalance ratios and with both ID and OOD
evaluations (see Figure 1 and Section 2 for more details). This robustness holds even with the same
number of samples for SL and SSL, although SSL does not require labels and hence can be more
easily applied to larger datasets than SL.
Why is SSL more robust to dataset imbalance? We identify the following underlying cause to
answer this fundamental question: SSL learns richer features from the frequent classes than SL does.
These features may help classify the rare classes under ID evaluation and are transferable to the
downstream tasks under OOD evaluation. For simplicity, consider the situation where rare classes
have so limited data that both SL and SSL models overfit to the rare data. In this case, it is important
for the models to learn diverse features from the frequent classes which can help classify the rare
classes. Supervised learning is only incentivized to learn those features relevant to predicting frequent
classes and may ignore other features. In contrast, SSL may learn the structures within the frequent
classes better—because it is not supervised or incentivized by any labels, it can learn not only the
label-relevant features but also other interesting features capturing the intrinsic properties of the input
distribution, which may generalize/transfer better to rare classes and downstream tasks.
We empirically validate this intuition by visualizing the features on a semi-synthetic dataset where
the label-relevant features and label-irrelevant-but-transferable features are prominently seen by
design (cf. Section 3.2). In addition, we construct a toy example where we can rigorously prove the
difference between self-supervised and supervised features in Section 3.1.
Finally, given our theoretical insights, we take a step towards further improving SSL algorithms, clos-
ing the small gap between SSL on balanced and imbalanced datasets. We identify the generalization
gap between the empirical and population pre-training losses on rare data as the key to improvements.
To this end, we design a simple algorithm that first roughly estimates the density of examples with
kernel density estimation and then applies a larger sharpness-based regularization (Foret et al., 2020)
to the estimated rare examples. Our algorithm consistently improves the representation quality under
several evaluation protocols.
We sum up our contributions as follows. (1) We are the first to systematically investigate the
robustness of self-supervised representation learning to dataset imbalance. (2) We propose and
validate an explanation of this robustness of SSL, empirically and theoretically. (3) We propose a
principled method to improve SSL under unknown dataset imbalance.
2	Exploring the Effect of Class Imbalance on SSL
Dataset class imbalance can pose challenge to self-supervised learning in the wild. Without access to
labels, we cannot know in advance whether a large-scale unlabeled dataset is imbalanced. We need to
2
Published as a conference paper at ICLR 2022
study how SSL will behave under dataset imbalance to deploy SSL in the wild safely. In this section,
we systematically investigate the effect of class imbalance on self-supervised representations.
2.1	Problem Formulation
Class-imbalanced pre-training datasets. We assume the datapoints / inputs are in Rd and come
from C underlying classes. Let x denote the input and y denote the corresponding label. Supervised
pre-training algorithms have access to the inputs and corresponding labels, whereas self-supervised
pre-training only observes the inputs. Given a pre-training distribution P over over Rd × [C], let r
denote the ratio of class imbalance. That is, r is the ratio between the probability of the rarest class
and the most frequent class: r =甫：?* [C] Pyj) ≤ 1. We will construct distributions with varying
maxj∈[C] P(y=j)
imbalance ratios and use Pr to denote the distribution with ratio r. We also use Pbal for the case where
r = 1, i.e. the dataset is balanced. Large-scale data in the wild often follow heavily long-tailed label
distributions where r is small. We assume that for any class j ∈ [C], the class-conditional distribution
Pr(x|y = j) is the same across balanced and imbalanced datasets for all r. The pre-training dataset
Pnr consists of n i.i.d. samples from Pr .
Pre-trained models. A feature extractor is a function fφ : Rd → Rm parameterized by neural
network parameters φ, which maps inputs to representations. A linear head is a linear function
gθ : Rm → RC , which can be composed with fφ to produce the predictions. SSL algorithms learn φ
from unlabeled data. Supervised pre-training learns the feature extractor and the linear head from
labeled data. We drop the head and only evaluate the quality of feature extractor φ.1
Following the standard evaluation protocol in prior works (He et al., 2020; Chen et al., 2020), we
measure the quality of learned representations on both in-domain and out-of-domain datasets with
either linear probe or fine-tuning, as detailed below.
In-domain (ID) evaluation tests the performance of representations on the balanced in-domain
distribution Pbal with linear probe. Given a feature extractor fφ pre-trained on a pre-training dataset
Pnr with n data points and imbalance ratio r, we train a C-way linear classifier θ on top of fφ on a
balanced dataset2 * sampled i.i.d. from Pbal. We evaluate the representation quality with the top-1
accuracy of the learned linear head on Pbal. We denote the ID accuracy of supervised pre-trained
representations by AISDL(n, r). Note that AISDL(n, 1) stands for the result with balanced pre-training
dataset. For SSL representations, we denote the accuracy by AISDSL(n, r).
Out-of-domain (OOD) evaluation tests the performance of representations by fine-tuning the feature
extractor and the head on a (or multiple) downstream target distribution Pt . Starting from a feature
extractor fφ (pre-trained on a dataset of size n and imbalance ratio r) and a randomly initialized
classifier θ, we fine-tune φ and θ on the target dataset Pt , and evaluate the representation quality
by the expected top-1 accuracy on Pt . We use ASOLOD (n, r) and ASOSOLD (n, r) to denote the resulting
accuracies of supervised and self-supervised representations, respectively.
Summary of varying factors. We aim to study the effect of class imbalance to feature qualities on
a diverse set of configurations with the following varying factors: (1) the number of examples in
pre-training n, (2) the imbalance ratio of the pre-training dataset r, (3) ID or OOD evaluation, and (4)
self-supervised learning algorithms: MoCo v2 (He et al., 2020), or SimSiam (Chen & He, 2021).
2.2	Experimental Setup
Datasets. We pre-train the representations on variants of ImageNet (Russakovsky et al., 2015) or
CIFAR-10 (Krizhevsky & Hinton, 2009) with a wide range of numbers of examples and ratios of im-
balance. Following Liu et al. (2019), we consider exponential and Pareto distributions, which closely
simulate the natural long-tailed distributions. We consider imbalance ratio in {1, 0.004, 0.0025} for
ImageNet and {1, 0.1, 0.01} for CIFAR-10. For each imbalance ratio, we further downsample the
dataset with a sampling ratio in {0.75, 0.5, 0.25, 0.125} to form datasets with varying sizes. We
1It is well-known that the composition of the head and features learned from supervised learning is more
sensitive to imbalanced dataset than feature extractor φ itself (Cao et al., 2019; Kang et al., 2020). Please also
see Table 3 in Appendix C for a comparison between CRT (Kang et al., 2020) and Supervised.
2We essentially use the largest balanced labeled ID dataset for this evaluation, which oftentimes means the
entire curated training dataset, such as CIFAR-10 with 50,000 examples and ImageNet with 1,281,167 examples.
3
Published as a conference paper at ICLR 2022
Ooooo
9 8 7 6 5
e< ¾, OlC≤4L□UO ›UE3WU<
A^l(π, 1)
—A^l(π, 0.1)
——喑 S, 0.01)
A⅛(n,l)
一A⅛(n,0.1)
——A⅛(π,0.01)
O 5
5 4
α<%4->QN<υ6EUJ-UOAUEJmUq
0 5 0 5 0 5
4 3 3 2 2 1
20000	40000	60000	80000	100000
Total Number of Pre-training Examples n
Ooooo
7 6 5 4 3
ag4 % Oi:,11-S co >UE≈UU<
(a) CIFAR-10, ID
0	2500 5000 7500 10000 12500 15000 17500 20000
Total Number of Pre-training Examples n
^oɑb(ŋɪ ɪ)
^⅛(n,0.1)
——Λ≡¾(n,0.01)
^OOD(n» ɪ)
一Λ‰(n,0.1)
一Λ‰(n,0.01)
O 2500 5000 7500 IOOOO 12500 15000 17500 20000
Total Number of Pre-training Examples n
(b) ImageNet, ID
0 5 0 5 0 5
8 7 7 6 6 5
a§< 求 slυs3Eα 1(U6」EL co AUE-JnUUV
(c) CIFAR-10, OOD	(d) ImageNet, OOD
Figure 2: Representation quality on balanced and imbalanced datasets. Left: CIFAR-10, SL vs.
SSL (SimSiam); Right: ImageNet, SL vs. SSL (MoCo v2). For both ID and OOD, the gap between
balanced and imbalanced datasets with the same n is larger for supervised learning. The accuracy of
supervised representations is better with reasonably large n in ID evaluation, while self-supervised
representations perform better in OOD evaluation.4
fix the variant of the dataset when comparing different algorithms. For ID evaluation, we use the
original CIFAR-10 or ImageNet training set for the training phase of linear probe and use the original
validation set for the final evaluation. For OOD evaluation of representations learned on CIFAR-10,
we use STL-10 (Coates et al., 2011) as the target /downstream dataset. For OOD evaluation of repre-
sentations learned on ImageNet, we fine-tune the pre-trained feature extractors on CUB-200 (Wah
et al., 2011), Stanford Cars (Krause et al., 2013), Oxford Pets (Parkhi et al., 2012), and Aircrafts (Maji
et al., 2013), and measure the representation quality with average accuracy on the downstream tasks.
Models. We use ResNet-18 on CIFAR-10 and ResNet-50 on ImageNet as backbones. For supervised
pre-training, we follow the standard protocol of He et al. (2016) and Kang et al. (2020). For self-
supervised pre-training, we consider MoCo v2 (He et al., 2020) and SimSiam (Chen & He, 2021).
We run each evaluation experiment with 3 seeds and report the average and standard deviation in the
figures. Further implementation details and additional results are deferred to Section A.
2.3 Results: Self-supervised Learning is More Robust than Supervised
Learning to Dataset Imbalance
In Figure 2, we plot the results of ID and OOD evaluations, respectively. For both ID and OOD
evaluations, the gap between SSL representations learned on balanced and imbalanced datasets
with the same number of pre-training examples, i.e., ASSL(n, 1) - ASSL(n, r), is smaller than the
gap of supervised representations, i.e., ASL(n, 1) - ASL(n, r), consistently in all configurations.
Furthermore, we compute the relative accuracy gap to balanced dataset ∆SSL(n, r) , (ASSL(n, 1) -
ASSL(n, r))/ASSL(n, 1) in Figure 1. We observe that with the same number of pre-training examples,
the relative gap of SSL representations between balanced and imbalanced datasets is smaller than
that of SL representations across the board,
ASSL , ASSLSJ)- ASSL(Mr)	ASL	, ASLSJ)- ASL(n,r)
δ	(n,r) =-------ASSLKl---------W δ (n,r) = ~ALKI-.	⑴
4The maximum n is smaller for extreme imbalance. The standard deviation comes only from the randomness
of evaluation. We do not include the stddev for ImageNet ID due to limitation of computation resources.
4
Published as a conference paper at ICLR 2022
Figure 3: Explaining SSL’s robustness in a toy setting. e1 and e2 are two orthogonal directions
in the d-dimensional Euclidean space that decides the labels, and e3:d represents the other d - 2
dimensions. Classes 1 and 2 are frequent classes and the third class is rare. To classify the three
classes, the representations need to contain both e1 and e2 directions. Supervised learning learns
direction e1 from the frequent classes (which is necessary and sufficient to identify classes 1 and 2)
and some overfitting direction v from the rare class which has insufficient data. Note that v might be
mostly in the e3:d directions due to overfitting. In contrast, SSL learns both e1 and e2 directions from
the frequent classes because they capture the intrinsic structures of the inputs (e.g., e1 and e2 are the
directions with the largest variances), even though e2 does not help distinguish the frequent classes.
The direction e2 learned from frequent data by SSL can help classify the rare class.
Also note that comparing the robustness with the same number of data is actually in favor of SL,
because SSL is more easily applied to larger datasets without the need of collecting labels.
ID vs. OOD. As shown in Figure 2, we observe that representations from supervised pre-training
perform better than self-supervised pre-training in ID evaluation with reasonably large n, while
self-supervised pre-training is better in OOD evaluation. This phenomenon is orthogonal to our
observation that SSL is more robust to dataset imbalance, and is consistent with recent works (e.g.,
Chen et al. (2020); He et al. (2020)) which also observed that SSL performs slightly worse than
supervised learning on balanced ID evaluation but better on OOD tasks.
3	Analysis
We have found out with extensive experiments that self-supervised representations are more robust to
class imbalance than supervised representations. A natural and fundamental question arises: where
does the robustness stem from? In this section, we propose a possible reason and justify it with
theoretical and empirical analyses.
SSL learns richer features from frequent data that are transferable to rare data. The rare classes
of the imbalanced dataset can contain only a few examples, making it hard to learn proper features to
classify the rare classes. In this case, one may want to resort to the features learned from the frequent
classes for help. However, due to the supervised nature of classification tasks, the supervised model
mainly learns the features that help classify the frequent classes and may neglect other features which
can transfer to the rare classes and potentially the downstream tasks. Partly because of this, Jamal
et al. (2020) explicitly encourage the model to learn features transferable from the frequent to the rare
classes with meta-learning. In contrast, in self-supervised learning, without the bias or incentive from
the labels, the models can learn richer features that capture the intrinsic structures of the inputs—both
features useful for classifying the frequent classes and features transferable to the rare classes.
3.1	Rigorous Analysis on A Toy Setting
To justify the above conjecture, we instantiate supervised and self-supervised learning in a setting
where the features helpful to classify the frequent classes and features transferable to the rare classes
can be clearly separated. In this case, we prove that self-supervised learning learns better features
than supervised learning.
Data distribution. Let e1, e2 be two orthogonal unit-norm vectors in the d-dimensional Euclidean
space. Consider the following pre-training distribution P of a 3-way classification problem, where
the class label y ∈ [3]. The input x is generated as follows. Let τ > 0 and ρ > 0 be hyperparameters
of the distribution. First sample q uniformly from {0,1} and ξ 〜N(0, I) from Gaussian distribution.
For the first class (y = 1), set x = e1 - qτe2 + ρξ. For the second class (y = 2), set x =
5
Published as a conference paper at ICLR 2022
-e1 - qτe2 + ρξ. For the third class (y = 3), set x = e2 + ρξ. Classes 1 and 2 are frequent classes,
while class 3 is the rare class, i.e., P(y=3), P(y=2) = o(1). See Figure 3 for an illustration of this
data distribution. In this case, both e1 and e2 are features from the frequent classes 1 and 2. However,
only e1 helps classify the frequent classes and only e2 can be transferred to the rare classes.
Algorithm formulations. For supervised learning, we train a two-layer linear network fW1,W2 (x) ,
W2W1 x with weight matrices W1 ∈ Rm×d and W2 ∈ R3×m for some m ≥ 3, and then use the
first layer WSL = W1 as the feature for downstream tasks. Given a linearly separable labeled
dataset, we learn such a network with minimal norm kW1>W1 k2F + kW2>W2k2F subject to the margin
constraint fW1,W2 (x)y ≥ fW1,W2 (x)y0 + 1 for all data (x, y) in the dataset and y0 6= y.5 * For self-
supervised learned, similar to SimSiam (Chen et al., 2020), we construct positive pairs (x + ξ, x + ξ0)
where x is from the empirical dataset, ξ and ξ0 are independent random perturbations. We learn a
matrix WSSL ∈ Rm×d which minimizes —E[(W(X + ξ))T(W(X + ξ0))] + 21∣ W>W∣∣F, where the
expectation E is over the empirical dataset and the randomness of ξ and ξ0 . The regularization term
1 ∣∣W>WkF is introduced only to make the learned features more mathematically tractable. We use
WSSLX as the feature of data X in the downstream task.
Main intuitions. We compare the features learned by SSL and supervised learning on an imbalanced
dataset that contains an abundant (poly in d) number of data from the frequent classes but only a
small (sublinear in d) number of data from the rare class. The key intuition behind our analysis is
that supervised learning learns only the e1 direction (which helps classify class 1 vs. class 2) and
some random direction that overfits to the rare class. In contrast, self-supervised learning learns both
e1 and e2 directions from the frequent classes. Since how well the feature helps classify the rare
class (in ID evaluation) depends on how much it correlates with the e2 direction, SSL provably learns
features that help classify the rare class, while supervised learning fails. This intuition is formalized
by the following theorem.
Theorem 3.1. Let n1,n2,n3 be the number ofdatafrom the three classes respectively. Let P = d- 1
and T = d⅛ in the data generative model. For n1,n2 = Θ(poly(d)) and n3 ≤ d⅛, with probability
1
at least 1 — O(e-d 10), the following statements holdfor anyfeature dimension m ≥ 3:
•	Let WSL = [w1,w2,…，wm]> be the feature learned by SL, then Pm=Ihe2,wii2 ≤ O(d- 10).
•	Let WSSL = [W1,W2,…，Wm]> be the feature learned by SSL, then ∣∣Πe2k2 ≥ 1 一 O(d-1),
where Π projects e2 onto the row span of WSSL.
Supervised learning results in features WSL whose rows have small correlation with the transferable
feature e2, indicating that supervised learning only learns features for classifying the frequent classes
and ignore the transferable features. In contrast, self-supervised learning recovers e2 well, even
though e2 is not relevant to classifying the frequent classes. The proofs are deferred to Section E.
3.2 Illustrative Semi-synthetic Experiments
In the previous subsection, we have shown that self-supervised learning provably learns label-
irrelevant-but-transferable features from the frequent classes which can help classify the rare class in
the toy case, while supervised learning mainly focuses on the label-relevant features. However, in
real-world datasets, it is intractable to distinguish the two groups of features. To amplify this effect in
a real-world dataset and highlight the insight of the theoretical analysis, we design a semi-synthetic
experiment on SimCLR (Chen et al., 2020) to validate our conclusion.
Dataset. In the theoretical analysis above, the frequent classes contain both features related to the
classification of frequent classes and features transferable to the the rare classes. Similarly, we
consider an imbalanced pre-training dataset with two groups of features modified from CIFAR-10 as
shown in Figure 4 (Left). We construct classes 1-5 as the frequent classes, where each class contains
5000 examples. Classes 6-10 are the rare classes, where each class has 10 examples. In this case, the
ratio of imbalance r = 0.002. Each image from classes 1-5 consists of a left half and a right half.
The left half of an example is from classes 1-5 of the original CIFAR-10 and corresponds to the label
of that example. The right half is from a random image of CIFAR-10, which is label-irrelevant. In
5Previous work shows that deep linear networks trained with gradient descent using logistic loss converge to
this min norm solution in direction (Ji & Telgarsky, 2018).
6
Published as a conference paper at ICLR 2022
Figure 4: Visualization of SSL features in semi-synthetic settings. Left: The right halves of the
rare examples decide the labels, while the left are blank. The left halves of the frequent examples
decide the labels, while the right halves are random half images, which contain label-irrelevant-but-
transferable features. Middle: Visualization of features with Grad-CAM (Selvaraju et al., 2017).
SimCLR learns features from both left and right sides, whereas SL mainly learns label-relevant
features from the left and ignore label-irrelevant features on the right. Right: Accuracies evaluated
on rare classes. The head classifiers are trained on 25000 examples from the 5 rare classes. SimCLR
learns much better features for rare classes than SL. We include random feature (feature extractor
with random weights) and supervised-rare (model trained with only the rare examples) for references.
contrast, the left half of an example from classes 6-10 is blank, whereas the right half is label-relevant
and from classes 6-10 of the original CIFAR-10. In this setting, features from the left halves of
the images are correlated to the classification of the frequent classes, while features from the right
halves are label-irrelevant for the frequent classes, but can help classify the rare classes. Note that
features from the right halves cannot be directly learned from the rare classes since they have only 10
examples per class. This is consistent with the setting of Theorem 3.1.
Pre-training. We pre-train the representations on the semi-synthetic imbalanced dataset. For
supervised learning, we use ResNet-50 on this 10-way classification task. For self-supervised
learning, we use SimCLR with ResNet-50. To avoid confusing the left and right parts, we disable the
random horizontal flip in the data augmentation. After pre-training, we fix the representations and
train a linear classifier on top of the representations with balanced data from the 5 rare classes (25000
examples in total) to test if the model learns proper features for the rare classes during pre-training. In
Figure 4 (Right), we test the classifier on the rare classes. In Figure 4 (Middle), we further visualize
the Grad-CAM (Selvaraju et al., 2017) of the representations on the held-out set6.
Results. As a sanity check, we first pre-train a supervised model with only the 50 rare examples and
train the linear head classifier with 25000 examples from the rare classes (5-way classification) to see
if the model can learn proper features for the rare classes with only rare examples (Supervised-rare
in Figure 4 (Right)). As expected, the accuracy is 36.5%, which is almost the same as randomly
initialized representations with trained head classifier, indicating that the model cannot learn the
features for the rare classes with only rare examples due to the limited number of examples. We then
compare supervised learning with self-supervised learning on the whole semi-synthetic dataset. In
Figure 4 (Right), self-supervised representations perform much better than supervised representations
on the rare classes (70.1% vs 44.3%). We further visualize the activation maps of representations
with Grad-CAM. Supervised learning mostly activate the left halves of the examples for both frequent
and rare classes, indicating that it mainly learn features on the left. In sharp contrast, self-supervised
learning activates the whole image on the frequent examples and the right part on the rare examples,
indicating that it learns features from both parts.
4	Improving SSL on Imbalanced Datasets with Regularization
In this section, we aim to further improve the performance of SSL to close the gap between imbalanced
and balanced datasets. Many prior works on imbalanced supervised learning regularize the rare classes
more strongly, motivated by the observation that the rare classes suffer from more overfitting (Cao
et al., 2019; 2021). Inspired by these works, we compute the generalization gaps (i.e., the differences
between empirical and validation pre-training losses) on frequent and rare classes for the step-
6CIFAR images are of low resolution. For visualization, we use high resolution version of the CIFAR-10
images in Figure 4 (Middle). We also provide the visualization on original CIFAR-10 images in Figure 7.
7
Published as a conference paper at ICLR 2022
imbalance CIFAR-10 datasets (where 5 classes are frequent class with 5000 examples per class and
the rest are rare with 50 examples per class). As shown in Table 1 (a), we still observe a similar
phenomenon—the frequent classes have much smaller pre-training generalization gap than the rare
classes (0.035 vs. 0.081), which indicates the necessity of more regularization on the rare classes.
We need a data-dependent regularizer that can have different effects on rare and frequent examples.
Thus, weight decay or dropout (Srivastava et al., 2014) are not suitable. The prior work of Cao et al.
(2019) regularizes the rare classes more strongly with larger margin, but it does not apply to SSL
where no labels are available. Inspired by Cao et al. (2021), we adapt sharpness-aware minimization
(SAM) (Foret et al., 2021) to imbalanced SSL.
Reweighted SAM (rwSAM). SAM improves model generalization by penalizing loss sharpness.
n
Suppose the training loss of the representation fφ is L(φ), i.e. L(φ) = 1 Ej=I '(xj, φ). SAM seeks
parameters where the loss is uniformly low in the neighboring area,
>
minL(φ + e(φ)),	where e(φ) = argmaxe > VφL(φ).	(2)
φ	kk<ρ
To take the weight of different examples into account, we add reweighting to the inner maximization
step of SAM. Intuitively, we wish the optimization landscape to be flatter for rare examples, which is
in effect regularizing the model more on rare examples. Concretely, consider the reweighted training
loss associated with weight vector w ∈ Rn, Lw(φ) = 1 P；=i Wj'(χj,φ). The reweighted SAM
objective re-weights the regularization-related terms (e.g., ew) but not the training loss L:
>
min L(φ + ew(φ)), where ew (φ) = arg max e> Vφ Lw (φ).	(3)
φ	kk<ρ
Assigning Weight with Kernel Density Estimation. The weight wj of an example xj should be
inversely correlated with the frequency of the corresponding class yj . However, we have no access
to the labels. In order to approximate the frequency, we use kernel density estimation on top of
the representations fφ. Concretely, denote by K(∙,h) the Gaussian density with bandwidth h. We
assign wi to be inversely correlated with the estimated density, i.e., wi = (1 pn=1 K(fφ(Xi)-
fφ(xj ), h) -α where h and α > 0 are hyperparameters selected by cross validation.
4.1	Experiments
We test the proposed rwSAM on CIFAR-10 with step or exponential imbalance and ImageNet-
LT (Liu et al., 2019). After self-supervised pre-training on the long-tailed dataset, we evaluate
the representations by (1) linear probing on the balanced in-domain dataset and (2) fine-tuning on
downstream target datasets. For (1) and (2), we compare with SSL, SSL+SAM (w/o reweighting),
and SSL balanced, which learns the representations on the balanced dataset with the same number of
examples. Implementation details and additional results are deferred to Section C. Code is available
at https://github.com/Liuhong99/Imbalanced-SSL.
Results. Table 1 (a) summarizes results on long tailed CIFAR-10. With both step and exponential
imbalance, rwSAM improves the performance of SimSiam over 1%, and even surpasses the perfor-
mance of SimSiam on balanced CIFAR-10 with the same number of examples. Note that compared
to SimSiam, rwSAM closes the generalization gap of pre-training loss on rare examples from 0.081
to 0.066, which verifies the effect of re-weighted regularization. In Table 1 (b), we provide the result
of fine-tuning on downstream tasks with representations pre-trained on ImageNet-LT. The proposed
method improves the transferability of representations to downstream tasks consistently.
5	Related Work
Supervised Learning with Dataset Imbalance. There exists a line of works studying supervised
imbalanced classification. Ando & Huang (2017); Buda et al. (2018) proposed to re-sample the
data to make the frequent and rare classes appear with equal frequency in training. Re-weighting
assigns different weights for head and tail classes and eases the optimization difficulty under class
imbalance (Cui et al., 2019; Tang et al., 2020; Wang et al., 2017b). Byrd & Lipton (2019); Xu et al.
(2021) studied the effect of importance weighting and found out that importance weighting does not
change the solution without regularization. Cao et al. (2019) studied reweighted regularization based
8
Published as a conference paper at ICLR 2022
(a) CIFAR, ID	r	=0.01, step		r = 0.01, exp
Method	Acc. (%)	Gap Freq.	Gap Rare	Acc. (%)
SimSiam	84.3 ± 0.2	0.035	0.081	81.4 ± 0.3
SimSiam+SAM	84.7 ± 0.3	0.044	0.075	82.1 ± 0.4
SimSiam, balanced	85.8 ± 0.2	0.038	0.037	82.0 ± 0.4
SimSiam+rwSAM ∣ 85.6 ± 0.4	0.037	0.066	∣	82.7 ± 0.5
(b) ImageNet, OOD ∣	Target dataset
Method	CUB	Cars	Aircrafts	Pets	Avg.
MoCo v2	69.9 ± 0.7	88.4 ± 0.4	82.9 ± 0.6	80.1 ± 0.6	80.3
MoCo v2+SAM	69.9 ± 0.5	88.8 ± 0.5	83.4 ± 0.4	81.5 ± 0.8	80.9
MoCo v2, balanced	69.8 ± 0.5	88.6 ± 0.4	82.7 ± 0.5	80.0 ± 0.4	80.2
MoCo v2+rwSAM	70.3 ± 0.7	88.7 ± 0.3	84.9 ± 0.6	81.7 ± 0.4	81.4
SimSiam	70.0 ± 0.3	87.0 ± 0.6	81.5 ±0.7	83.8 ± 0.5	80.6
SimSiam, balanced	70.5 ± 0.8	87.9 ± 0.7	81.8 ± 0.7	82.7 ± 0.4	80.7
SimSiam+rwSAM	70.7 ± 0.8	88.4 ± 0.6	82.6 ± 0.6	84.0 ± 0.4	81.4
Table 1: Results of the proposed rwSAM. (a) Results on CIFAR-10-LT with linear probe and
ID evaluation. SimSiam+rwSAM on imbalanced datasets performs even better than SimSiam on
balanced datasets with the same number of examples. Note that rwSAM closes the generalization
gap on the rare examples (0.081 vs. 0.066). (b) Results on ImageNet-LT with fine-tuning and OOD
evaluation. rwSAM improves the performance of MoCo v2 and SimSiam on the target datasets.
on classifier margin, but these techniques are limited to supervised imbalanced recognition. Cao et al.
(2021) proposed to regularize the local curvature of loss on imbalanced and noisy datasets. Recent
works also designed specific losses or training pipelines for imbalanced recognition (Jamal et al.,
2020; Hong et al., 2021; Wang et al., 2021; Zhang et al., 2021).
Several works also studied the supervised representations under dataset imbalance. Kang et al. (2020);
Wang et al. (2020) found out that the representations of supervised learning perform better than
the classifier itself with class imbalance. Yang & Xu (2020) studied the effect of self-training and
self-supervised pre-training on supervised imbalanced recognition classifiers. In contrast, the focus
of our paper is the effect of class imbalance on self-supervised representations.
Self-supervised Learning. Recent works on self-supervised learning successfully learn representa-
tions that approach the supervised baseline on ImageNet and various downstream tasks. Contrastive
learning methods attract positive pairs and drive apart negative pairs (He et al., 2020; Chen et al.,
2020). Siamese networks predict the output of the other branch, and use stop-gradient to avoid
collapsing (Grill et al., 2020; Chen & He, 2021). Clustering methods learn representations by per-
forming clustering on the representations and improve the representations with cluster index (Caron
et al., 2020). Cole et al. (2021) investigated the effect of data quantity and task granularity on
self-supervised representations. Goyal et al. (2021) studied self-supervised methods on large scale
datasets in the wild. Kotar et al. (2021) studied whether dataset imbalance can have a significant
impact on contrastive learning representations. Several works have also theoretically studied the
success of self-supervised learning (Arora et al., 2019; Lee et al., 2020b; HaoChen et al., 2021; Wei
et al., 2021).
6	Conclusion
Our paper is the first to study the problem of robustness to imbalanced training of self-supervised
representations. We discover that self-supervised representations are more robust to class imbalance
than supervised representations and explore the underlying cause of this phenomenon. Our experi-
ments mainly focus on vision datasets. Future works can study the effect of dataset imbalance on
NLP datasets, where self-supervised pre-training is a dominant approach. We hope our study can
inspire analysis of self-supervised learning in broader environments in the wild such as domain shift,
and provide insights for the design of future unsupervised learning methods.
9
Published as a conference paper at ICLR 2022
Acknowledgements
We thank Colin Wei, Margalit Glasgow, and Shibani Santurkar for helpful discussions. TM ac-
knowledges support of Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, and JD.com.
Toyota Research Institute provided funds to support this work.
Reproducibility S tatement
To ensure reproducibility, we describe the implementation details of the algorithms and the con-
struction of the datasets in Section A.1 and Section C. The code of the experiments in Section 2,
Section 3.2 and Section 4 is provided in the supplementary material. We describe the setting and data
assumptions of the toy case in Section 3.1 and provide the proof in Section E.
Ethics S tatement
Our paper studies the problem of robustness to imbalanced training of self-supervised representations.
This setting is important to AI Ethics, as large real-world datasets tend to be imbalanced in practice,
for instance including less examples from under-represented minorities. Furthermore, pre-training is
a standard practice in deep learning, especially for quickly adapting models to new domains, which
corresponds to our OOD evaluation scenario.
Our experiments and theoretical analysis show that SSL is more robust than supervised pre-training,
especially in the OOD scenario. As supervised learning is still the de facto standard for pre-training,
our work should have a wide impact, encouraging practitioners to use SSL for pre-training instead, or
at least consider evaluating the impact of imbalanced pre-training on their downstream task.
We also remark that the paper does not imply at all that the algorithms proposed or studied can
guarantee any form of fairness, and they in fact should still suffer from biases. The paper should be
considered as a step towards studying the important technical issue of dataset imbalance, which is
related to the fairness or biases questions.
References
Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying imbalanced data.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
770-785, 2017.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. In International
Conference on Machine Learning, 2019.
Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.
Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In
International Conference on Machine Learning,pp. 872-881. PMLR, 2019.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems, volume 32, pp. 1565-1576. Curran Associates, Inc., June 2019.
Kaidi Cao, Yining Chen, Junwei Lu, Nikos Arechiga, Adrien Gaidon, and Tengyu Ma. Heteroskedas-
tic and imbalanced deep learning with adaptive regularization. In International Conference on
Learning Representations, 2021.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 33:9912-9924, 2020.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.
10
Published as a conference paper at ICLR 2022
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
volume 119 of Proceedings ofMachine Learning Research, pp. 1597-1607. PMLR, PMLR, 13-18
Jul 2020.
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15750-15758,
June 2021.
Adam Coates, Andrew Y Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In International Conference on Artificial Intelligence and Statistics, pp. 215-223,
2011.
Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge Belongie. When does
contrastive visual representation learning work? arXiv preprint arXiv:2105.05837, 2021.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, pp. 9268-9277, 2019.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by
context prediction. In Proceedings of the IEEE international conference on computer vision, pp.
1422-1430, 2015.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. In International Conference on Learning Representations,
2021.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.
Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat
Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual
features in the wild. arXiv preprint arXiv:2103.01988, 2021.
Jean-Bastien Grill, Florian Strub, Florent Altche, COrentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 33:21271-21284, 2020.
Hongyu Guo and Herna L Viktor. Learning from imbalanced data sets with boosting and data
generation: the databoost-im approach. ACM Sigkdd Explorations Newsletter, 6(1):30-39, 2004.
Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.
Peter Hart. The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory,
14(3):515-516, 1968.
Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263-1284, 2009.
Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. Adasyn: Adaptive synthetic sampling
approach for imbalanced learning. In 2008 IEEE international joint conference on neural networks
(IEEE world congress on computational intelligence), pp. 1322-1328. IEEE, 2008.
11
Published as a conference paper at ICLR 2022
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, June 2020.
Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang.
Disentangling label distribution for long-tailed visual recognition. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 6626-6636, 2021.
Xinting Hu, Yi Jiang, Kaihua Tang, Jingyuan Chen, Chunyan Miao, and Hanwang Zhang. Learning
to segment the tail. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 14045-14054, 2020.
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Deep imbalanced learning for
face recognition and attribute prediction. IEEE transactions on pattern analysis and machine
intelligence, 42(11):2781-2794, 2019.
Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong.
Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation
perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032, 2018.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classifier for long-tailed recognition. In International
Conference on Learning Representations, 2020.
Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting
contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pp. 9949-9959, 2021.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In Proceedings of the IEEE international conference on computer vision workshops,
pp. 554-561, 2013.
Bartosz Krawczyk. Learning from imbalanced data: open challenges and future directions. Progress
in Artificial Intelligence, 5(4):221-232, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Miroslav Kubat, Stan Matwin, et al. Addressing the curse of imbalanced training sets: one-sided
selection. In Icml, volume 97, pp. 179-186. Citeseer, 1997.
Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and Sung Ju
Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks.
In International Conference on Learning Representations, 2020a.
Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020b.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988,
2017.
Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, and Wenhui Li. Deep representation learning
on long-tailed data: A learnable embedding augmentation perspective. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2970-2979, 2020.
12
Published as a conference paper at ICLR 2022
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-
scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2019.
Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representa-
tional continuity for unsupervised continual learning. In International Conference on Learning
Representations, 2022.
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on Learning
Representations, 2021.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw
puzzles. In European conference on computer vision, pp. 69-84. Springer, 2016.
Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012
IEEE conference on computer vision and pattern recognition, pp. 3498-3505, 2012.
William J Reed. The pareto, zipf and other power laws. Economics letters, 74(1):15-19, 2001.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. In International Conference on Machine Learning, pp. 4334-4343, 2018.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):
211-252, 2015.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-
net: Learning an explicit mapping for sample weighting. In Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good
and removing the bad momentum causal effect. In Advances in Neural Information Processing
Systems, volume 33, pp. 1513-1524. Curran Associates, Inc., 2020.
Junjiao Tian, Yen-Cheng Liu, Nathan Glaser, Yen-Chang Hsu, and Zsolt Kira. Posterior re-calibration
for imbalanced datasets. arXiv preprint arXiv:2010.11820, 2020.
Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic
posterior information to linear models. arXiv:2003.02234, 2020.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redun-
dancy, and linear models. In Algorithmic Learning Theory, pp. 1179-1206. PMLR, 2021.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011.
13
Published as a conference paper at ICLR 2022
Shuo Wang and Xin Yao. Diversity analysis on imbalanced data sets by using ensemble models. In
2009 IEEE symposium on computational intelligence and data mining, pp. 324-331. IEEE, 2009.
Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew, Sheng Tang, Steven Hoi, and Jiashi Feng.
The devil is in classification: A simple framework for long-tail instance segmentation. In European
Conference on computer vision, pp. 728-744. Springer, 2020.
Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive invariance for self-supervised visual
representation learning. In Proceedings of the IEEE international conference on computer vision,
pp. 1329-1338, 2017a.
Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu. Long-tailed recognition by rout-
ing diverse distribution-aware experts. In International Conference on Learning Representations,
2021.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Proceedings
of the 31st International Conference on Neural Information Processing Systems, pp. 7032-7042,
2017b.
Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in
downstream tasks? an analysis of head and prompt tuning. arXiv preprint arXiv:2106.09226, 2021.
Da Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep
learning. In International Conference on Learning Representations, 2021.
Yuzhe Yang and Zhi Xu. Rethinking the value of labels for improving class-imbalanced learning.
In Advances in Neural Information Processing Systems, volume 33, pp. 19290-19301. Curran
Associates, Inc., 2020.
Yuzhe Yang, Kaiwen Zha, Yingcong Chen, Hao Wang, and Dina Katabi. Delving into deep imbal-
anced regression. In Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 11842-11851, 18-24 Jul 2021.
Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A
unified framework for long-tail visual recognition. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 2361-2370, 2021.
14
Published as a conference paper at ICLR 2022
Figure 5: Visualization of the label distributions. We visualize the label distributions of the imbal-
anced CIFAR-10 and ImageNet. We consider two imbalance ratios r for each dataset.Imbalanced
CIFAR-10 follows the exponential distribution, while imbalanced ImageNet follows Pareto distribu-
tion.
A Details of Section 2
A. 1 Implementation Details
Generating Pre-training Datasets. CIFAR-10 (Krizhevsky & Hinton, 2009) contains 10 classes with
5000 examples per class. We use exponential imbalance, i.e. for class c, the number of examples is
5000 × eβ(c-1). we consider imbalance ratio r ∈ {0.1, 0.01}, i.e. the number of examples belonging
to the rarest class is 500 or 50. The total ns is therefore 20431 or 12406. ImageNet-LT is constructed
by Liu et al. (2019), which follows the Pareto distribution with the power value 6. The number
of examples from the rarest class is 5. We construct a long tailed ImageNet following the Pareto
distribution with more imbalance, where the number of examples from the rarest class is 3. The total
number of examples ns is 115846 and 80218 respectively. For each ratio of imbalance, we further
downsample the dataset with the sampling ratio in {0.75, 0.5, 0.25, 0.125} to formulate different
number of examples. To compare with the balanced setting fairly, we also sample balanced versions
of datasets with the same number of examples. Note that each variant of the dataset is fixed after
construction for all algorithms. See the visualization of label distributions of dataset variants in
Figure 5.
Training Procedure. For supervised pre-training, we follow the standard protocol of He et al. (2016)
and Kang et al. (2020). On the standard ImageNet-LT, we train the models for 90 epochs with step
learning rate decay. For down-sampled variants, the training epochs are selected with cross validation.
Fo self-supervised learning, the initial learning rate on the standard ImageNet-LT is set to 0.025
with batch-size 256. We train the model for 300 epochs on the standard ImageNet-LT and adopt
cosine learning rate decay following (He et al., 2020; Chen & He, 2021). We train the models for
more epochs on the down sampled variants to ensure the same number of total iterations. The code
on CIFAR-10 LT is adapted from https://github.com/Reza-Safdari/SimSiam-91.
9-top1-acc-on-CIFAR10.
Evavluation. For in-domain evaluation (ID), we first train the the representations on the aforemen-
tioned dataset variants, and then train the linear head classifier on the full balanced CIFAR10 or
ImageNet. We set the initial learning rate to 30 when training the linear head with batch-size 4096
and train for 100 epochs in total. For in-domain out-of-domain evaluation (OOD) on ImageNet, we
first train the the representations on the aforementioned dataset variants, and then fine-tune the model
to CUB-200 (Wah et al., 2011), Stanford Cars (Krause et al., 2013), Oxford Pets (Parkhi et al., 2012),
and Aircrafts (Maji et al., 2013). The number of examples of these target datasets ranges from 2k to
10k, which is a reasonable scale as the number of examples of the pre-training dataset variants ranges
from 10k to 110k. The representation quality is evaluated with the average performance on the four
tasks. We set the initial learning rate to 0.1 in fine-tuning train for 150 epochs in total. For in-domain
out-of-domain evaluation (OOD) on CIFAR-10, we use STL-10 as the downstream target tasks and
perform linear probe.
15
Published as a conference paper at ICLR 2022
(a) OOD Accuracy.
洪8
S 7
S6
ω 5
∣4
Imabalanced Ratio r = 0.004
■溪
■ %
Imabalanced Ratio r = 0.0025
J]JJJ
2
14K 29K 58K 87K 116K IOK 20K 40K 60K 80K
Total Number of Pre-training Examples n
(b) Relative Gap.
Figure 6:	OOD Results of SimSiam on ImageNet. SimSiam also demonstrates more robustness
to class imbalance compared to supervised learning. The relative gap to balanced dataset is much
smaller than supervised learning across different imbalance ratios.
Table 2: Numbers in Figure 2 and Figure 6.
Imbalanced Ratio r	—	r = 1, balanced			—	—	r=	0.004		—	—	r=	0.0025		
Data Quantity n	116K	87K	58K	29K	14K	116K	87K	58K	29K	14K	80K	60K	40K	20K	10K
MoCo V2, ID	50.4	43.5	40.9	37.0	30.8	49.5	43.2	39.5	36.6	30.5	40.6	38.8	35.5	31.9	27.2
MoCo V2, OOD	80.3	79.8	79.7	77.4	77.0	80.2	80.1	79.5	77.8	77.3	79.2	78.8	77.7	75.6	74.4
Supervised, ID	54.3	51.6	46.1	40.5	26.3	52.9	49.6	44.0	37.3	24.9	46.1	42.0	36.3	27.5	20.3
Supervised, OOD	76.6	74.7	71.9	67.4	59.1	75.5	73.3	70.4	65.8	57.8	71.8	69.1	65.9	60.3	54.3
SimSiam, OOD	80.7	80.4	79.9	78.7	77.2	80.6	79.9	79.6	78.8	76.9	79.8	79.3	78.8	77.5	76.0
A.2 Additional Results
To validate the phenomenon observed in Section 2 is consistent for different self-supervised learning
algorithms, we provide the OOD evaluation results of SimSiam trained on ImageNet variants and
relative performance gap with balanced datasets in Figure 6. SimSiam representations are also less
sensitive to class imbalance than supervised representations.
We also provide the numbers of Figure 2 and Figure 6 in Table 2.
B Details of Section 3.2
We first generate the balanced semi-synthetic dataset with 5000 examples per class. The left halves of
images from classes 1-5 correspond to the labels, while the right halves are random. The left halves of
images from class 6-10 are blank, whereas the right halves correspond to the labels. We then generate
the imbalanced dataset, which consists of the 5000 examples per class from classes 1-5 (frequent
classes), and 10 examples per class from classes 6-10 (rare classes). We use Grad-CAM imple-
mentation based on https://github.com/meliketoy/gradcam.pytorch and SimCLR
implementation from https://github.com/leftthomas/SimCLR. We provide examples
and Grad-CAM of the semi-synthetic datasets in Figure 7.
C Details of Section 4
C.1 Implementation Details
We use the same implementation as Section 2 for supervised and self-supervised learning
baselines. We implement sharpness-aware minimization following (Foret et al., 2021). In
each step of update, we first compute the reweighted loss Lw (φ)
compute its gradient w.r.t. φ, i.e. VφLw (φ).	Then We can
q-1	1/p
PSgn(VφLw(φ)) ∣VφLw(φ)∣	/ (∣∣VφLw(φ)kq)	, where P + 1
=n Pn=I Wj'(Xj ,φ) and
compute (φ) as (φ) =
= 1. Finally, we update the
16
Published as a conference paper at ICLR 2022
sφsseDi≡∙lnb∙li
Supervised
Representations
Self-supervised
Representations
Supervised
Representations
sφssecs∙,,!eH
Self-supervised
Representations
Figure 7:	Examples of the semi-synthetic Datasets and Grad-CAM visualizations. SimCLR
learns features from both left and right sides, whereas SL mainly learns label-relevant features from
the left side of frequent data and ignore label-irrelevant features on the right side. In Figure 4, we
provide results the high-resolution images of the 10 CIFAR classes to make the results easier to
interpret. Here we further visualize the results on original CIFAR-10 images.
model on the loss without reweighting L(φ) by φ = φ - ηVφL(φ + e(φ)). A detailed algorithm can
be viewed in Algorithm 1.
Algorithm 1 Reweighted Sharpness-Aware Minimization (rwSAM)
1:
2:
3:
4:
5:
6:
Input: the pre-training dataset Dbs .
Output: learned representations φ.
Stage 1: compute the weight w.
for i = 0 to MaxIter do
Randomly sample a batch of examples {xi}ib=1 from Dbs.
Update the representations φ on {xi}ib=1 to minimize the loss.
7:	end for
I	I	V—7	^τ~ / I ∖
Φ J Φ - ηVφL(φ).
8:	Generate the weight with kernel density estimation:
1n
Wi =(n∑SK(fΦ(xi)- fΦ(xjIh)) ,
j=1
9:	Stage 1: reweighted SAM.
10:	for i = 0 to MaxIter do
11:	Randomly sample a batch of examples {xi}ib=1 from Dbs.
12:	Calculate e(φ) based on the reweighted loss Lw(φ).
q-1	1/p
eφ = ρsgn(VφLbw(φ)) VφLbw(φ)	/ kVφLbw(φ)kqq
13:	Update the representations φ on {xi}ib=1 to minimize the loss and penalize the sharpness,
φ J φ - ηVφL(φ + e(φ)).
14:	end for
We select the hyperparameters ρ and α with cross validation. On ImageNet-LT and iNaturalist, ρ = 2
andα = 0.5. On CIFAR-10-LT, ρ = 5 andα = 1.2.
C.2 Additional Results
We further introduce another evaluation protocol
of the representations learned on imbalanced Ima-
geNet: following the protocol of Kang et al. (2020);
Yang & Xu (2020), we fine-tune the representa-
tions on imbalanced ImageNet dataset with super-
vision, and then re-train the linear classifier with
class-aware resampling, to compare with super-
vised imbalanced recognition methods. In MoCo
V2 pre-training, we use the standard data augmen-
Table 3: ImageNet-LT with Supervision.
Method	Backbone	Acc.
Supervised	ResNet-50	49.3
CRT (Kang et al., 2020)	ResNet-50	52.0
LADE (Hong et al., 2021)	ResNeXt-50	53.0
RIDE (Wang et al., 2021)	ResNet-50	54.9
RIDE (Wang et al., 2021)	ResNeXt-50	56.4
MoCo V2	ResNet-50	55.0
MoCo V2+rwSAM	ResNet-50	55.5
17
Published as a conference paper at ICLR 2022
tation following He et al. (2020). In fine-tuning, we use RandAugment (Cubuk et al., 2020). For
this evaluation, we further compare with CRT (Kang et al., 2020), LADE (Hong et al., 2021), and
RIDE (Wang et al., 2021), which are strong methods tailored to supervised imbalanced recognition.
Results are provided in Table 3. Supervised here refers to training the feature extractor and linear
classifier with supervision on the imbalanced dataset directly. CRT first trains the feature extractor
with supervision, and then re-trains the classifier with class-aware resampled loss. Note that CRT is
performing better than Supervised, indicating that the composition of the head and features learned
from supervised learning is more sensitive to imbalanced dataset than the quality of feature extractor
itself.
Even with a simple pre-training and fine-tuning pipeline, MoCo V2 representations can be comparable
with much more complicated state-of-the-arts tailored to supervised imbalanced recognition, further
corroborating the power of SSL under class imbalance. With rwSAM, we can further improve the
result of MoCo V2.
D	Additional Related Work
D. 1 Supervised Learning with Dataset Imbalance
There exists a long line of works studying supervised imbalanced classification (He & Garcia, 2009;
Krawczyk, 2016). Early works on ensemble learning adjusted the boosting and bagging algorithms
with resampling in the imbalanced setting (Guo & Viktor, 2004; Wang & Yao, 2009). Classical
methods include resampling and reweighting. Hart (1968); Kubat et al. (1997); Chawla et al. (2002);
He et al. (2008); Ando & Huang (2017); Buda et al. (2018); Hu et al. (2020) proposed to re-sample
the data to make the frequent and rare classes appear with equal frequency in training. Re-weighting
assigns different weights for head and tail classes and eases the optimization difficulty under class
imbalance (Cui et al., 2019; Tang et al., 2020; Wang et al., 2017b; Huang et al., 2019). Byrd &
Lipton (2019) empirically studied the effect of importance weighting and found out that importance
weighting does not change the solution without regularization. Xu et al. (2021) justified this finding
with theoretical analysis based on the implicit bias of gradient descend on separable data.
Cao et al. (2019) initiated the idea of using re-weighted regularization and proposed the principle
of regularizing rare classes more heavily. Re-weighted regularizaton is shown to be typically more
effective than re-weighting or re-sampling the losses. Cao et al. (2021) proposed to regularize the
local curvature of loss on imbalanced and noisy datasets.
Works in the modern deep learning era also designed specific losses or training pipelines for imbal-
anced recognition (Tang et al., 2020; Hong et al., 2021; Wang et al., 2021; Zhang et al., 2021). Lin
et al. (2017) proposed to focus on hard examples to prevents easy examples from overwhelming the
models during training. Meta-learning approaches meta-learned the weight or the ensemble (Wang
et al., 2017b; Ren et al., 2018; Shu et al., 2019; Lee et al., 2020a). Liu et al. (2019); Jamal et al.
(2020); Liu et al. (2020) improved the performance on the rare examples by explicitly encourages
transfer learning. Re-calibration methods adjust the logits of the outputs with re-weighting (Tian
et al., 2020; Menon et al., 2021).
Several works also studied the supervised representations under dataset imbalance. Kang et al. (2020);
Wang et al. (2020) found out that the representations of supervised learning perform better than
the classifier itself with class imbalance. Yang & Xu (2020) studied the effect of self-training and
self-supervised pre-training on supervised imbalanced recognition classifiers. In contrast, the focus
of our paper is the effect of class imbalance on self-supervised representations.
D.2 Self-supervised Learning
Earlier works on self-supervised learning learned visual representations by context prediction (Do-
ersch et al., 2015; Wang et al., 2017a), solving puzzles (Noroozi & Favaro, 2016), and rotation
prediction (Gidaris et al., 2018). Recent works on self-supervised learning successfully learn repre-
sentations that approach the supervised baseline on ImageNet and various downstream tasks, and
closed the gap with supervised pre-training. Contrastive learning methods attract positive pairs
and drive apart negative pairs (He et al., 2020; Chen et al., 2020). Siamese networks predict the
output of the other branch, and use stop-gradient to avoid collapsing (Grill et al., 2020; Chen & He,
2021). Clustering methods learn representations by performing clustering on the representations and
18
Published as a conference paper at ICLR 2022
improve the representations with cluster index (Caron et al., 2020). Cole et al. (2021) investigated the
effect of data quantity and task granularity on self-supervised representations. Goyal et al. (2021)
studied self-supervised methods on large scale datasets in the wild, but they do not consider dataset
imbalance explicitly. Kotar et al. (2021) studied whether dataset imbalance can have a significant
impact on contrastive learning representations. Madaan et al. (2022) found out that self-supervised
representations are better at continual learning than supervised representations. Several works have
also theoretically studied the success of self-supervised learning (Arora et al., 2019; HaoChen et al.,
2021; Wei et al., 2021; Lee et al., 2020b; Tian et al., 2021; Tosh et al., 2020; 2021). Our analysis in
Section 3.1 is partially inspired by the work HaoChen et al. (2020).
19
Published as a conference paper at ICLR 2022
E Proof of Theorem 3.1
We notate data from the first class as xi(1) = e1 - qi(1) τ e2 + ρξi(1) where i ∈ [n1] and qi(1) ∈ {0, 1}.
Similarly, we notate data from the second class as xi(2) = -e1 - qi(2)τe2 + ρξi(2) where i ∈ [n2] and
qi(1) ∈ {0, 1}. We notate data from the third class as xi(3) = e2 + ρξi(3) where i ∈ [n3]. Notice that
all ξi(k) are independently sampled from N(0, I).
We first introduce the following lemma, which gives some high probability properties of independent
Gaussian random variables.
Lemma E.1. Let ξi 〜N(0, I) for i ∈ [n]. Then, for any n ≤ poly(d), with probability at least
1
1	一 e-d 10 and large enough d, we have:
•	∣hξi,eιi∣ ≤ d10, ∣hξi,e2i∣ ≤ d 110 and ∣∣∣ξi∣∣2 - d| ≤ 4d3 forall i ∈ [n].
•	∣hξi,ξj i∣ ≤ 3d3 forall i = j.
ProofofLemma E.1. Let ξ, ξ 〜N(0, I) be two independent random variables. By the tail bound
of normal distribution, we have
1
Pr (∣hξ,eιi∣ ≥ d 1⅛) ≤ d-110 ∙ e-d5.	(4)
By the tail bound of χ2d distribution, we have
Pr (lkξk2 - d| ≥ 4d3) ≤ 2e-√d.	(5)
Since the directions of ξ and ξ0 are independent, we can bound their correlation with the norm of ξ
times the projection of ξ 0 onto ξ:
Pr (lhξ,ξ0il ≥ 3d3) ≤ Pr (kξk2 ≥ √d + 2d8) +Pr (lhξ',卷il ≥ d 110) ≤ T + 2e-√d
(6)
Since every ξi and ξj are independent when i 6= j , by the union bound, we know that with probability
-d5	11
at least 1 一(n2 + 2n)(^ɪ- + 2e-Vd),wehave ∣hξi,e>i∣ ≤ d 10, ∣hξi,e2i∣ ≤ d而 and ||£k2 — d| ≤
d 10
4d 4 for all i ∈ [n], and also ∣hξi, ξj)| ≤ 3d 5 for all i 6= j . Since the error probability is exponential
1
in d, for large enough d, the error probability is smaller than e-d 10, which finishes the proof. □
Using the above lemma, we can prove the following lemma which constructs a linear classifier of the
empirical dataset with relatively large margin and small norm.
Lemma E.2. In the Setting of Theorem 3.1, let Wi = eι, Wg = —eι, Wl = Pd Pn= 1 ξ(3). Apply
Lemma E.1 to the set of all ξi(k) where k ∈ [3] and i ∈ [nk]. When the high probability outcome of
Lemma E.1 happens, the margin ofclassifier {wg,W2,W3} is at least 1 — O(d- 10). Furthermore,
we have ∣∣w3∣∣2 ≤ O(d-5).
Proof of Lemma E.2. When the high probability outcome of Lemma E.1 happens, we give a lower
bound on the margin for all data in the dataset. For data x = xi(1) in class 1, we have
wι>x = 1+ hξ(1),eιiρ ≥ 1 - Pd 1o,	G)
w2>x = -1 + hξ(1),eιiρ ≤ -1 + Pda,	⑻
W3>x = Pd 卜…⑴τe2 + ρξ(1))> (Xξj3) ≤ Fd± + 等d3.	(9)
20
Published as a conference paper at ICLR 2022
So the margin on data (xi(1) , 1) is
w*>x - w3>x ≥ 1 - ρd 110 - n3(τ + 1) d告-3n3d5 ≥ 1 - O(d-10).	(10)
Similarly, for data x(2) in class 2, the margin is at least 1 - O(d-io).
For data x = xi(3) in class 3, we have
w3>x = Pd(Xl ξj) >卜2 + ρξ(3)) ≥ d kξ(3)k2 - 3n3 d3 -哈 ≥ 1 - OW5). (11)
On the other hand,
wι>x = hρξ(3, eιi ≤ Pd10,	(12)
w2>x = hρξ(3), -e1i ≤ Pd10 .	(13)
So the margin is
w3>x — max{wj>x, w；>x} ≥ w3>X — ρd10 ≥ 1 — O(d-10).	(14)
Finally, noticing that ∣∣w]∣2 ≤ 2n3√d ≤ 2d- 1o finishes the proof.	□
We also introduce the following helper lemma:
Lemma E.3. Let W ∈ R3×d be an arbitrary matrix, m ≥ 3. Then, we have
kW kF = 2 Wmi=W (kw> W2kF + kW1W> kF),	(15)
where W1 ∈ Rm×d and W2 ∈ R3×m. Furthermore, the minimum is achieved when W1 W1> =
W2>W2.
Proof. On one hand, we have
kW k2F = Tr(WW>)	(16)
=	min Tr(W2W1W1>W2>)	(17)
W2 W1=W	1	2
=	min Tr(W1W1>W2>W2)	(18)
W2 W1=W	1	2
≤ IwminW(kW1W>kF + kW>W2kF),	(19)
2 W2 W1=W
where the inequality becomes equality if and only if W1 W1> = W2> W2 .
On the other hand, let W = UΣV be the SVD decomposition of W, where Σ ∈ R3×d is a diagonal
1
matrix with σ1,σ2, σ3 on its diagonal. For integers p,q ≥ 3, We use Σp×q to denote the P X q matrix
_________	__ _____ 1
with √σ1, √σ2, √σ3 at its first 3 diagonal positions and 0 otherwise. If we set Wi = Σ2n×dV and
W2 = U Σ∙f ×m, then it can be verified that W = W2W1 and ∣∣ W ∣∣F = 2 (∣∣W1W> kF + ∣∣W>W2∣∣F).
Therefore, the equality holds in Equation 19, which finishes the proof.	□
Now we are ready to prove the supervised learning part of Theorem 3.1:
Proof of Theorem 3.1 (supervised learning part). Let {W1,W2,W3} be three vectors in Rd that mini-
mize kwi k22 + kw2 k22 + kw3k22 subject to the margin constraint wy>x ≥ wy>0 x + 1 for all empirical
data (x, y) and y0 6= y. To prove the supervised learning part of Theorem 3.1, we will first prove
that hW1,e1i2 + hw^2, ej2 + hW3, ei)2 ≤ O(d-10) with high probability, and then use this result to
prove the correlation between e2 and WSL.
21
Published as a conference paper at ICLR 2022
We frist apply Lemma E.1 to the set of all ξi(k) where k ∈ [3] and i ∈ [nk]. We consider the situation
when the high probability outcome of Lemma E.1 holds (which happens with probability at least
1	1
1 - e-d 10). ByLemmaE.2, the constructed classifier {wj,w2,w3} has margin a ≥ 1 - O(d-ɪθ)
in this case. As a result, {1 w↑, 1 w2,1 w3} is a classifier with margin 1 and norm bounded by
k 1w；k2 + k1 W2k2 + k 1w3k2 = 2+ kW3k2 ≤ 2 + O(d-10).	(20)
α	α	α	α2
Let {W1,W2,W3} be min-norm linear classifier of the empirical dataset. Since its norm cannot be
larger than the constructed one, we have ∣∣Wι∣∣2 + ∣∣W2∣∣2 + IW∣∣2 ≤ 2 + O(d-而).By standard
-1
concentration inequality, when nι ≥ poly(d), with probability at least 1 - ed 10, we have
lEi∈[n1],q(1)=0[X(1) ]-eil≤ d-上,	(21)
where the expectation is over all the data from class 1 that satisfies qi(1) = 0. By the definition of
{W1,W2,W3} we know (WI - W3)>χ(I) ≥ 1 for all i ∈ [n1 ], hence averaging over all the class 1
data with qi(1) = 0 and using the above inequality gives us
(W1 — W3)>e1 ≥ 1 — ∣Wι — ι^3∣2 ∙ d-10 ≥ 1 — O(d-10).	(22)
A similar analysis for class 2 data gives us
(W2 - t^3)>(-e1) ≥ 1 - O(d-10).	(23)
Now we prove that W1,W2, W3 all have small correlation with e2. Without loss of generality, we
assume W>eι，t ≥ 0. If t ≥ 2, we have
〈W1, e。2 + h^^2, e。2 + hW3, e02 ≥ (t + 1 — O(d 10)) > 2.25 — O(d 10),	(24)
which contradicts with ∣Wιk2 + ∣∣W2∣∣2 + ∣∣W3∣∣2 ≤ 2 + O(d-击).Therefore, there must be t ≤ ɪ,
hence
hW1,e1i2 + ®2, eιi2 + hW3, e1i2	(25)
≥ (1 +1 - O(d-110 ))2 + (1 - t - O(d-110 ))2 + t2	(26)
≥ 2 + 3t2 - O(d- 110)	(27)
≥ 2 - O(d-110).	(28)
As a result,
hW1,e2i2 + hW2, e2i2 + hW3, e2i2	(29)
≤ kw1k2 + kw2k2 + kw3k2 -hw1,e1i2 -hw2, e1 i2 -hw3, e1i2	(30)
≤(2 + O(d- 1⅛)) -(2 - O(d- 110))	(31)
≤ O(d- 110).	(32)
Now we turn to the analysis of WSL. Recall that we learn two matrices W1 ∈ Rm×d and W2 ∈
R3×m that minimize ∣W1>W1∣2F + ∣W2>W2 ∣2F subject to the margin constraint (W2W1x)y ≥
(W2W1x)y0 + 1, and the supervised representation is WSL = W1. According to Lemma E.3, we
know that the solution W1 and W2 satisfy W2W1 = [W1,W2,W3]> and W>W2 = W1W>. Let
W2> W2 = W1W1> = U>ΣU be the SVD decomposition, where Σ ∈ Rm×m is a non-negative
diagonal matrix and U is a unitary matrix. Since W2 has rank at most 3, there are at most 3 entries in
Σ that are non-zero. Without loss of generality, we assume that all the non-zero entries of Σ are in
the first 3 rows.
Let Σm×d and Σ3×m be the matrices by reshaping Σ (deleting or padding all-0 rows/columns) to
the corresponding dimensions. We can write Wi as Wi = U>∑mm×dV1 for some unitary matrix
22
Published as a conference paper at ICLR 2022
1
V1 ∈ Rd×d, where ∑m乂& is the element-wise square root of ∑m×d. Similarly, W2
some unitary matrix V2 ∈ R3×3. Taking the product gives W2W1 = V2Σ3×dV1.
1
V2∑3×mU for
Let WSL = Wi = [w1,w2, ∙∙∙ , Wm]>. Now we finishe the proof with
m
Xhwi,e2i2 = kW1e2∣∣2 = ku >∑mm ×d%e2k2 ≤ kvie2 k2 ∙k∑d×dVie2k2 = 1 陷 ∑3×dVie2k2
i=i
(33)
=∣∣W2W1e2∣∣2 = hW1,e2i2 + hW2,e2i2 + hW3,e2i2 ≤ O(d-10).	(34)
□
To prove the self-supervised learning part of Theorem 3.1, we first introduce the following lemma
which gives some helpful properties of the empirical data matrix.
Lemma E.4. In the setting of Theorem 3.1, let M , Ex [xx>] where the expectation is over
1
empirical data. Then, when n1,n2 ≥ poly(d), with probability at least 1 一 e-d 10, we have: (1)
e> Me2 ≥ Ω(d2), and (2) u>Mu ≤ O(1) forall U ∈ Rd such that UYe2 = 0 and ∣∣uk2 = 1.
Proof of Lemma E.4. Let n = ni + n2 + n3 . We abuse notation and let ξi (i ∈ [n]) be the set of all
ξ(k) that appears in the empirical data. Let matrix M0 = n PZi ξiξ7. By standard concentration
1
inequalities and union bound, for n ≥ poly(d), with probability at least 1 一 2e-d 10, we have that
| M0,j | ≤ d for all i = j and | Mii 一 11 ≤ d for all i ∈ [d]. In this case, for any vector U ∈ Rd such
that kUk2 = 1 and U>e2 = 0, we have
U>MU ≤ 2kUk22 +2U>(ρ2M0)U ≤ 2 +2ρ2 + 2ρ2kM0 一 IkF ≤ O(1).	(35)
On the other hand, by the definition of data distirbution and standard concentration inequalities, for
1
n ≥ poly(d), with probability at least 1 一 1 e-d 10 we have that: at least 1 of all data either is class 1
with q(1) = 1 or class 2 with q(2) = 1, and ∣∣n PZi ξi∣2 ≤ O(d). In this case,
e>Me2= Eχ[(e>x)2] ≥ (Eχ[e>x])2 ≥ (IT - e> (1 X ξ)) ≥ Ω(τ2) = Ω(d5).	(36)
□
Using the above lemma, we can prove the self-supervised learning part of Theorem 3.1.
Proof of Theorem3.1(self-supervised learning part). Let M = Ex[xx>] be the empirical data ma-
trix, where the expectation is over the dataset. Notice that self-supervised learning objective has the
same minimizer as the matrix factorization objective ∣∣M — W>W∣∣F, by Eckart-Young-Mirsky theo-
rem we know that the span of W1,W2,…，Wm is exactly the span of the top m eigenvectors of matrix
M. Let M = Pid=1 λi vi vi> where λi is the i-th largest eigenvalue of M with the corresponding
eigenvector vi . We decompose e2 in the eigenvector basis ase2 = Pid=1 ζi vi .
We first note that λ1 ≥ Ω(d2) and maxi=ι λi ≤ O(1). Indeed, we know that E[M] = diag(1 +
2 ,2	2	2	_2、
d- 5, d5 + d- 5, d- 5,…，d- 5). By standard matrix concentration bounds (e.g. Theorem 4.6.1 of
Vershynin (2018)), we know that with probability at least 1 — e-d 10, ∣M — E[M]∣ ≤ O(d-2). By
Weyl,s inequality we know that maxɪ ∖λi(T) — λi(S)| ≤ ∣∣S — T∣∣, so λι ≥ Ω(d5) and maxi=ι λi ≤
O(1).
1	2
By Lemma E.4, we know that with probability at least 1 — e-d 10, we have e>Me2 ≥ Ω(d5) and
UkUMu ≤ O(1) for all u orthogonal to e2. To prove the result regarding self-supervised learning, we
only need to prove that Z2 ≥ 1 — O(d- 1) in this case.
23
Published as a conference paper at ICLR 2022
We first show that Z2 ≥ 11. For contradiction, first assume Z2 ≤ 2. Define vector
U = q - Z2vι - Z1p=j2Vv.	(37)
1 - Z1
which satisfies u>e2 = 0 and kuk22 = 1. Notice that
U>MU = (1 - Z2)λ -	Z2 Pvd=2 Z2λv	(38)
H2 =(1	Z1 )λ1	1 - Z2	
、λι	ʌ ≥ ———max λv		(39)
≥ Ω(d5),		(40)
which contradicts to u>Mu ≤ O(1). Therefore, We have Z2 ≥ 1.
To prove that Z2 is close to 1, we let scalar t =吉-1 and define vector
d
u = -tZ1V1 +	Zi Vi ,	(41)
i=2
which satisfies u>e2 = 0. Since Z2 ≥ 1, we have t ≤ 1 and ∣∣uk2 ≤ 1. As a result, we have
u>Mu
"FT
d
≥ t2λιZ2 + X λvZ2 ≥ t2e>Me2 ≥ Ω(t2d2).
i=2
(42)
On the other hand, we know that u>Mu ≤ O(1). Comparing these two bounds gives us t2 ≤ O(d-5),
kuk2
which means Z2 ≥ 1 — O(d- 5).	口
24