Published as a conference paper at ICLR 2022
Fairness in Representation for Multilingual
NLP: Insights from Controlled Experiments on
Conditional Language Modeling
Ada Wan
University of Zurich
ada.wan@uzh.ch
Ab stract
We perform systematically and fairly controlled experiments with the 6-layer Trans-
former to investigate the hardness in conditional-language-modeling languages
which have been traditionally considered morphologically rich (AR and RU) and
poor (ZH). We evaluate through statistical comparisons across 30 possible language
directions from the 6 languages of the United Nations Parallel Corpus across 5 data
sizes on 3 representation levels — character, byte, and word. Results show that
performance is relative to the representational granularity of each of the languages,
not to the language as a whole. On the character and byte levels, we are able to
eliminate statistically significant performance disparity, hence demonstrating that a
language cannot be intrinsically hard. The disparity that mirrors the morphological
complexity hierarchy is shown to be a byproduct of word segmentation. Evidence
from data statistics, along with the fact that word segmentation is qualitatively inde-
terminate, renders a decades-long debate on morphological complexity (unless it is
being intentionally modeled in a word-based, meaning-driven context) irrelevant in
the context of computing. The intent of our work is to help effect more objectivity
and adequacy in evaluation as well as fairness and inclusivity in experimental
setup in the area of language and computing so to uphold diversity in Machine
Learning and Artificial Intelligence research. Multilinguality is real and relevant in
computing not due to canonical, structural linguistic concepts such as morphology
or “words” in our minds, but rather standards related to internationalization and
localization, such as character encoding — something which has thus far been
sorely overlooked in our discourse and curricula.
1	Introduction
1.1	Background and motivation
Most current work on fairness in Machine Learning (ML) and Natural Language Processing (NLP)
focuses on the societal biases encoded in natural language data that are propagated and amplified
when they are used at scale for/as Artificial Intelligence (AI) solutions1. But little has been said or
questioned about the bias, as in, the favoring of certain outcomes, implicit in our theoretical/scientific
assumptions that results in the varying performance of different languages in computing.
Disparity in machine translation results For instance, results reported in Junczys-Dowmunt
et al. (2016) for Phrase-Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) and
Neural MT (Bahdanau et al., 2014) on the 6 official languages2 of the United Nations (UN) Parallel
Corpus (Ziemski et al., 2016) indicate a disparity between EN/ES/FR and AR/RU/ZH in BLEU
(Papineni et al., 2002) — translation performance in the latter group is generally worse, regardless
of the MT algorithm used. AR and RU are traditionally considered morphologically complex (see
e.g. Minkov et al. (2007), Seddah et al. (2010) and proceedings of related workshops in subsequent
1see e.g. work from conference (https://facctconference.org) and workshops in previous years
on “Fairness, Accountability, and Transparency” (FAccT)
2Arabic (AR), English (EN), Spanish (ES), French (FR), Russian (RU), and Chinese (ZH)
1
Published as a conference paper at ICLR 2022
years for Statistical Parsing of Morphologically Rich Languages), and ZH morphologically frugal
(for its lacking determiners and plural or tense markers) (Koehn, 2005). While Koehn (2005) found
translating into EN to be easier than into morphologically rich languages based on word-level BLEU
scores from PBSMT systems of 110 language directions from the 11 Europarl languages then,
Bugliarello et al. (2020) found it is easier to translate out of EN than into it based on 21 Europarl
languages in BPEs (Byte Pair Encodings) (Sennrich et al., 2016) with the Transformer (Vaswani et al.,
2017) in a new metric, cross-mutual information.
Disparity in language modeling results Disparate performances across different languages seem
to have been implicitly accepted in that it is often believed that some languages are harder to model
than others. Bender (2009) advocated the relevance of linguistic typology for the design of language-
universal NLP systems due to differences based on crosslinguistic structural notions, such as parts
of speech and morphological complexity. Cotterell et al. (2018) studied (monolingual) language
models (LMs) on the 21 Europarl languages using a word-level 7-gram standard Kneser & Ney
(1995) model and LSTM-LMs (Sundermeyer et al., 2012) with characters and lemmatized forms
in information-theoretic terms, and found morphological complexity to be the primary culprit for
the differences in performance. Mielke et al. (2019) extended the coverage to 69 languages with the
multilingual Bible corpus (Mayer & Cysouw, 2014), tested on RNN-LMs (an implementation of
LSTM (Hochreiter & Schmidhuber, 1997)) with characters and BPEs, but concluded that basic data
statistics in vocabulary size (|V |) and sequence length were the most predictive performance features.
We noticed, however, a discrepancy in the results from Mielke et al. (2019) for ZH — it came out
as the least difficult for the character model, but it is the 6th most difficult language for the BPE
model. As different input representations have been tested with different architectures with divergent
results in different metrics in previous studies, each of them only testing with one data size, we
decided to investigate the matter more systematically once again with statistical comparisons of score
distributions between languages.
1.2	Research questions and contributions
Research questions Are there any statistically significant differences in hardness when it comes
to Conditional-Language-Modeling (CLMing) languages which have been traditionally considered
morphologically rich (AR and RU) and poor (ZH) with the 6-layer Transformer? Is morphological
complexity inherent in language? When is the notion of morphological complexity relevant in
computing?
Summary of findings and insights Based on our bilingual CLMing setup with the UN Parallel
Corpus in the data size range from 102 to 106 lines on the character, byte, and word levels, we find:
1.	Language has many finer-grained dimensions with different representations and learning patterns.
Hardness in modeling is relative to its representational granularity (representation relativity).
2.	There is neutralization of source language instances, i.e. there are no statistically significant
differences between source language pairs. Only pairs of target languages differ significantly.
3.	On the character and byte levels, hardness is correlated with statistical properties concerning
sequence length and |V | of a language, regardless of its morphological profile. As itis possible to
eliminate performance disparity by decomposing sequences into finer-grained units in characters
and bytes, we show that morphological complexity is not an intrinsic property of language.
Unless word-based methods are used, or unless we implement/model it explicitly, the notion of
morphological complexity is irrelevant in computing.
4.	On the word level, hardness is correlated with |V |, and a complexity hierarchy arises through
the manual preprocessing step of word tokenization. This complexity/disparity effected by word
segmentation can be improved by subword tokenization but cannot be eliminated due to the
fundamental qualitative differences in the definition of a “word” being one that neither holds
universally nor is suitable/consistent for fair crosslinguistic comparisons.
5.	Representational units of finer granularity can help close the gap in performance disparity.
Orthogonal to our main research questions, we also observed 2 types of sample-wise non-monotonicity
— Double Descent (Belkin et al., 2019; Nakkiran et al., 2020) and erraticity. For reasons due to length
and scope for this paper, we will defer discussions and analyses of these beyond what is addressed in
§ 5 to future work.
2
Published as a conference paper at ICLR 2022
Outline of the paper In § 2, we define our method and experimental setup. We present our results
and analysis on the primary representations in § 3 and those from the secondary set of controls in § 4
in a progressive manner to ease understanding. Meta analysis on performance disparity and other
discussions are in § 5.
2	Method and definitions
Conditional language modeling CLMing is the modeling of the probability of the next token,
given the history of the preceding tokens and conditioning context. In our case, such conditioning
context is a line from the source language. To explicitly focus on modeling the complexities that may
or may not be intrinsic to the languages, we study the more fundamental process of CLMing without
performing any translation. This allows us to eliminate confounds associated with generation and
other evaluation metrics. One could think of our setup as estimating conditional probabilities with
the Transformer, with a bilingual (one-to-one) setup where the perplexity (PP) of one target language
(ltrg) is estimated given the parallel data in one source language (lsrc), where lsrc 6= ltrg. We focus on
the very basics and examine the first step in our pipeline — input representations, holding everything
else constant. Instead of measuring absolute cross-entropy scores at one data size, we evaluate the
relative differences between development (dev) set score distributions between languages.
Controlled experiments as basic research for scientific understanding of language data Using
the UN Parallel Corpus, the data from which the MT results in Junczys-Dowmunt et al. (2016) stem,
we perform a series of controlled experiments with the Transformer, holding the hyperparameter
settings for all 30 one-to-one language directions from the 6 languages constant. We control for size
(from 102 to 106 lines) and language with respect to representational granularity. We examine 3
primary representation types/levels — character, byte (UTF-8), and word, and upon encountering
some unusual phenomena, we perform a secondary set of controls with 5 alternate representations —
on the character level: Pinyin and Wubi (ASCII representations for ZH phones and character strokes,
respectively), on the byte level: code page 1256 (for AR) and code page 1251 (for RU), and on the
word level: BPE. These symbolic variants allow us to manipulate the statistical properties of the
representations, while staying as “faithful” to the language as possible. We adopt this symbolic data-
centric3 * approach because we would like to more directly interpret the confounds, if any, that make
language data different from other data types. We operate on a smaller data size range as this is more
common in traditional language sciences and one of our higher goals is to bridge an understanding
between language sciences and engineering (the latter being the dominant focus in NLP), and between
traditional symbolic sciences and ML. We run statistical tests to identify the strongest correlates
of performance and to assess whether the differences between the mean performance of different
groups are indeed significant. We are concerned not with the absolute scores, but with the differences
between score distributions from different languages.
Fair evaluation with multitexts Multitexts are multiway parallel corpora. The UN Parallel Corpus
is a 6-way parallel corpus consisting of manually translated UN documents from the 25-year period
between 1990 and 2014. We use the UN Parallel Corpus because it contains languages conventionally
regarded as morphologically rich and poor, has quality and size sufficient for evaluation, and more
importantly, it comes as raw texts (untokenized), unlike both of the corpora that Mielke et al. (2019)
used. Detokenization (esp. the evaluation thereof) is not a trivial task.
Fair information-theoretic evaluation metric Most sequence-to-sequence models are optimized
using a cross-entropy loss, defined as:
N
H(t, s) = - X log2 p(ti | t<i,s)	(1)
i=1
where t is the sequence of tokens to be predicted, ti refers to the ith token in that sequence, s is
the sequence of tokens conditioned on, and N = |t|. It is customary to report scores as PP, which
is 2~NH(t,s), i.e. 2 to the power of the cross-entropy averaged by the number of tokens in the dev
3Two testing/evaluation approaches — data-centric: hold the algorithm constant and tweak data, vs. algorithm-
centric: hold data constant and tweak the algorithm.
3
Published as a conference paper at ICLR 2022
data. Cotterell et al. (2018) proposed to use “renormalized” PP to evaluate LMs tokenwise fairly
by dividing the overall bits per utterance/sequence by one constant token count in any one arbitrary
language (e.g. so to arrive at “bits per character” in one language to evaluate all languages). But we
find that it is not necessary to assign a perspective that is centered on any one particular language,
when we can evaluate simply by the total number of bits for a larger portion of texts/sequences.
This can be a fairer, more general and flexible way of evaluating data that has not been or cannot be
perfectly segmented or aligned line by line. We hence used instead unnormalized PP, i.e. the total
number of bits needed to encode the dev set (3,077 lines per language, after length filtering, in our
case). As the implementation we used only reports PP, we transformed it back to entropy as defined
above via H(t, s) = log2 P P (t|s) × N.
Disparity/Inequality In the context of our CLMing experiments, we consider there to be “dis-
parity” or “inequality” between languages l1 and l2 if there are significant differences between the
performance distributions of these two languages with respect to each representation. Here, by
performance we mean the number of bits required to encode the held-out data using a trained CLM.
With 30 directions, there are 15 pairs of source languages (lsrc1, lsrc2) and 15 pairs of target languages
(ltrg1, ltrg2) possible. We compare the source languages among each other, and the target languages
among each other. Each lsrc or each ltrg consists of scores from all models trained across various sizes
and directions. To assess whether the differences are significant, we perform unpaired two-sided
significance tests with the null hypothesis that the score distributions for the two languages are not
different. Upon testing for normality with the Shapiro-Wilk test (Shapiro & Wilk, 1965; Royston,
1995), we use the parametric unpaired two-sample Welch’s t-test (Welch, 1947) (when normal) or the
non-parametric unpaired Wilcoxon test (Wilcoxon, 1945) (when not normal) for the comparisons.
We use the implementation in R (R Core Team, 2014) for these 3 tests. To account for the multiple
comparisons we are performing, we correct all p-values using Bonferroni correction (Benjamini &
Heller, 2008; Dror et al., 2017) and follow Holm’s procedure4 (Holm, 1979; Dror et al., 2017) to
identify the pairs of l1 and l2 with significant differences after correction. We report all 3 levels of
significance (α ≤ 0.05, 0.01, 0.001) for a more comprehensive overview. In contrast to Dror et al.
(2017), which aimed to compare the performance of different algorithms, we compare languages (in
the context of computing).
Experimental setup The systematic, identical treatment we give to our data is described as follows
with further preprocessing and hyperparameter details in Appendices A and B, respectively.
After filtering length to 300 characters maximum per line in parallel for the 6 languages, we made 3
subsets of the data with 1 million lines each — one having lines in the order of the original corpus
(dataset A) and two other randomly sampled (without replacement) from the full corpus (datasets
B & C). Lines in all datasets are extracted in parallel and remain fully aligned for the 6 languages.
For each run and each representation, there are 30 pairwise directions (i.e. one lsrc to one ltrg) that
result from the 6 languages. We trained all 150 (for 5 sizes) 6-layer Transformer models for each run
using the SOCKEYE Toolkit (Hieber et al., 2018). We optimize using PP and use early stopping if no
PP improvement occurs after 3 checkpoints up to 50 epochs maximum, taking the best checkpoint.
Characters and bytes are supposed to mitigate the out-of-vocabulary (OOV) problem on the word
level. In order to assess the effect of modeling with finer granularity more precisely, all vocabulary
items appearing once in the train set are accounted for (i.e. full vocabulary on train, as in Gerz
et al. (2018a;b)). But we allow our system to categorize all unknown items in the dev set to be
unknown (UNK) so to measure OOVs (open vocabulary on dev (Jurafsky & Martin, 2009)). To
identify correlates of performance, we compute Spearman’s correlation (Spearman, 1904) with some
basic statistical properties of the data (e.g. length, |V |, type-token-ratio, OOV rate) as metrics — a
complete list is provided in App. C. See App. D for sample construction for statistical comparisons.
3	Experimental results of primary representations
Subfigures 1a, 1b, and 1c show the mean results across 12 runs of the 3 primary representations
— character, byte, and word, respectively. The x-axis represents data size in number of lines and
the y-axis the total conditional cross-entropy, measured in bits (Eq. 1). Each line connects 5 data
points corresponding to the number of bits the CLMs (trained with training data of 102, 103, 104,
4using implementation from https://github.com/rtmdrr/replicability-analysis-NLP
4
Published as a conference paper at ICLR 2022
1e+03	1e+05
number of lines
(a) CHAR
' ‘
二二一二,=二二
1e+03
number of lines
1e+05
(b) BYTE
EEI EEI ^^^1 EEI EEI ‘
一二二二,=二二
500000-
TRG
-AR
三
---RU
□ ZH
SRC
AR
一EN
ES
一FR
RU
ZH
1e+03
number of lines
1e+□5
(c) WORD
!
-三三g三司
(g)	(h)


ie+03 1e+05 1e+03 ie+05 ιe+03 *+05	*+B ιe+05 ιe+03 ιe+05 ie+03 ie+05	ie+03 1e+05 ιe+03 ιe+05 ιe+03 ie+05
number of lies	number of lies	number of lines
(d) CHAR by target	(e) BYTE by target	(f) WORD by target
Figure 1: Number of bits (the lower the better) across data size from 102 to 106 lines plotted for all 30 directions.
Subfigures 1a, 1b, and 1c show mean scores across 12 runs. Subfigures 1d, 1e, and 1f depict the corresponding
information respectively sorted in 6 facets by target language and with error bars. Legend in Subfigure 1g
shows the correspondence between colors and source languages, in Subfigure 1h between line types and target
languages. (These figures are also shown enlarged in Appendix F. Please note that results pertinent to our first
research question of this paper concerning statistically significant differences are summarized in Table 1, figures
are a visual aid only. We are not concerned with the absolute scores but the distances between scores, i.e. spaces
between the sets of lines by ltrg. The point here is to show the differences in Transformer’s overall learning
patterns relative to the representational granularity.)
105, and 106 lines) needed to encode the target language dev set given the corresponding text in
the source language. These are the same data in the same 30 language directions and 5 sizes with
the same training regime, just preprocessed/segmented differently. This confirms representation
relativity — hardness in modeling is relative to its representational granularity. Languages (or any
objects being modeled) need to be evaluated relative to their representation. “One size does not fit all”
(Durrani et al., 2019). Our conventional way of referring to “language” (as a socio-cultural product
or with traditional word-based approaches, or even for most multilingual tasks and competitions) is
too coarse-grained for computing (see also Fisch et al. (2019) and Ponti et al. (2020)).
Subfigures 1d, 1e, and 1f display the corresponding information sorted into facets by target language,
source languages represented as line types. Through these we see more clearly that results can be
grouped rather neatly by target language — as implicit in the Transformer’s architecture, the decoder
is unaware of the source language in the encoder. As shown in Table 1 in § 5 summarizing the number
of source and target language pairs with significant differences, there are no significant differences
across any source language pairs. The Transformer neutralizes source language instances. This
could explain why transfer learning or multilingual/zero-shot translation (Johnson et al., 2017) is
possible at all on a conceptual level.
In general, for character and byte models, most language directions do seem to converge at 104 lines
to similar values across all target languages, with few notable exceptions. There are some fluctuations
5
Published as a conference paper at ICLR 2022
past 104, indicating further tuning of hyperparameters would be beneficial due to our present setting
possibly working most favorably at 104. On the character level, target language ZH (ZHtrg) shows a
different learning pattern throughout. And on the byte level, ARtrg and RUtrg display highly unstable
behavior, which we refer to as erratic. Word models exhibit Double Descent across the board (note
the spike at 103), but overall, difficult/easy languages stay consistent, with AR and RU being the
hardest, followed by ES and FR, then EN and ZH. A practical takeaway from this set of experiments:
in order to obtain more robust training results, use bytes for ZH (as suggested in Li et al. (2019a)) and
characters for AR and RU (e.g. Lee et al. (2017)) — also if one wanted to avoid any “class” problems
in performance disparity with words. Performance disparity for these representations is reported in
Table 1 under “CHAR”, “BYTE”, and “WORD”. Do note, however, that the intrinsic performance
of ZH with word segmentation is not particularly subpar. But this often does not correlate with its
poorer downstream tasks results (recall results from Junczys-Dowmunt et al. (2016)). Since the
notion of word in ZH is highly contested and ambiguous — i) it is often aimed to align with that in
other languages so to accommodate academic theories and manual feature engineering5, ii) there is
great variation among different conventions, and iii) native ZH speakers identify characters as words
— there are reasons to rethink this procedure now that fairer and language-independent processing in
finer granularity is possible. Li et al. (2019b) questioned the necessity of CWS in Deep Learning
(DL)-based ZH NLP and presented evidence in favor of character-based processing, including results
from downstream NLP tasks. In Linguistics, Duanmu (2017) presented a summary on the contested
nature of wordhood in (Mandarin) ZH in relation to EN. A more native account of ZH, however,
despite a couple of dialects/varieties of it being considered a high-resource language, has not yet been
fully recognized and accepted in NLP.
4	Understanding the phenomena with alternate representations
To understand why some languages show different results than others, we carried out a secondary set
of controlled experiments with representations targeting the problematic statistical properties of the
corresponding target languages.
!
,5saBl=NZH。-RaBu=,∙uz"
二二-
-i5rττm.
TRG
二 AR
二
…AU
-ZH
SRC
AA
—FR
AU
ZH
2"""一
S 鲁宾晨.∙-κa≡u=mz"
l,......----
RG
-AR
RC
-AR
≡
ZH
ι"00"'-
number of lies
n umber of lines
er of i
(a) Wubi	(b) Wubi by target	(c) Pinyin	(d) Pinyin by target

Figure 2: Character-level remedies for ZH: Wubi vs. Pinyin.
Character level We reduced the high |V | in ZH with representations in ASCII characters — Pinyin
and Wubi. We replaced the ZH data in these formats only on the target side and reran the experiments
involving ZHtrg on the character level. Results in Figure 2 and Table 1 show that the elimination of
5It is a “legacy interpretation” which stemmed from a practical compromise from the early days in ZH NLP
when the goal was to align with EN words for MT. Chinese word segmentation (CWS) has been a decades-long
issue in text processing. But even in EN, for computing, the variability in “word” counts (from the trivial
convention of whitespace tokenization) results in different bit counts, affecting file sizes. In NLP, such method of
“word” counting brings about a high |V |, hence different tokenization schemes have been designed to mitigate
this problem. For humans, there is no consensus about the definition of “words”. Even for a purely academic
account, it is held to be indeterminate (see Haspelmath (2011) and references therein from the past century).
Kilgarriff (1997; 2014) pointed out that “words” and “word senses” and the number thereof, in terms of lexical
entries for dictionaries, are contextual and arbitrary.
6
Published as a conference paper at ICLR 2022
!
ιe+∣3	e+05
number of lies
(a) Code page 1256 & 1251
ie+03	1e+05	ιe+03	ιe+05	ιe+03	ie+05
number of lines
(b) Code page by target
ιe+
n umber of lines
(c) BPE
!
Sam.∙am
.........................---
1e+03 1e+05	ie+03 *+05	*+B 1e+05
number of ines
(d) BPE by target

Figure 3: Byte-level (Subfigures 3a & 3b) remedies with code page 1256 for target AR and 1251 for
target RU, and word-level (Subfigures 3c & 3d) remedy with BPE for all languages.
disparity on the character level is possible if ZH is represented through Pinyin (transliteration), as in
Subfigure 2c, though at the cost of the native script information. Models represented through Wubi,
an input algorithm that decomposes character-internal information into stroke shape and ordering
and matches these to 5 classes of radicals (Lunde, 2008), display a behavioral tendency unlike those
with other (phonetic) alphabetic scripts6 (Subfigure 2a), suggesting that this script/stroke pattern
decomposes differently. But ZH the language is not an outlier all around.
Byte level Length is the most salient statistical attribute that makes AR and RU outliers. To shorten
their sequence lengths, we tested with alternate encodings on ARtrg and RUtrg — code page 1256
and 1251, which provide 1-byte encodings specific to AR and RU, respectively. Results are shown in
Subfigures 3a and 3b. Not only is erraticity resolved, the number of 15 possible target language pairs
with significant differences reduces from 8 with the UTF-8 byte representation to 0 (Table 1 under
“ARRUt”), indicating that we eliminated disparity with this optimization heuristic. Since our heuristic
is a lossless and reversible transform, it shows that a complexity that is intrinsic and necessary
in language7 does not exist in computing, however diverse they may be, as our 6 are, from the
conventional linguistic typological, phylogenetic, historical, or geographical perspectives.
Word level The main difference between word and character/byte models is length not being a top
contributing factor correlating with performance, but instead |V | is. This is understandable as word
segmentation neutralizes sequence lengths. To remedy the OOV problem, we use BPE, which learns
a fixed vocabulary of variable-length character sequences (on word level, as it presupposes word
segmentation) from the training data. It is more fine-grained than word segmentation and is known to
better model subword units for morphologically complex languages. We use the same vocabulary of
30,000 as specified in Junczys-Dowmunt et al. (2016). This reduced our averaged OOV token rate by
89-100% across the 5 sizes. The number of language pairs with significant differences reduced to 7
from 8 for word models. While BPEs are still not as effective as our character/byte variants, their
results show how finer-grained modeling contributes positively to closing the disparity gap.
5	Meta results, analyses, and discussion
Performance disparity Table 1 lists the number of language pairs with significant differences
under the representations studied. Since it is possible for our character and byte models to effect no
performance disparity for the same languages on the same data, a complexity intrinsic to language
does not exist. In fact, the customary expectation that languages ought to perform differently is
created through our word segmentation practice. Furthermore, the order of AR/RU > ES/FR > EN/ZH
(Figure 1c) resembles the idea of morphological complexity. Considering there are character-internal
6which are sequences with an implicit/explicit pattern made up of consonants and vowels
7 aside from its statistical properties related to length and vocabulary. To show something is not necessarily
true, only 1 counter observation is needed.
7
Published as a conference paper at ICLR 2022
Table 1: Disparity Table Number of language pairs out of 15 with significant differences, with respective
p-values. ARRUt refers to AR & RU being optimized only on the target side; whereas ARRUs,t denotes
optimization on both source and target sides (relevant for directions AR-RU and RU-AR).
	p-value	CHAR		Pinyin		Wubi		BYTE		ARRUt		ARRUs,t		WORD		BPE	
		src	trg	src	trg	src	trg	src	trg	src	trg	src	trg	src	trg	src	trg
	0.05	0	7	0	4	0	8	0	9	0	4	0	4	0	11	0	10
	0.01	0	5	0	2	0	6	0	8	0	3	0	4	0	8	0	8
	0.001	0	3	0	0	0	5	0	8	0	0	0	2	0	8	0	7
Table 2: Target language pairs with significant differences indicate that the 2 languages are not equally/similarly
good or equally/similarly bad. 15 (non-directional) language pairs total possible from 30 language directions,
p=0.001.
LANGtrg PAIR CHAR Pinyin Wubi BYTE ARRUt ARRUs,t WORD BPE
AR-EN	X	XX
AR-ES
EN-ES	X
AR-FR	X
EN-FR	X	X
ES-FR
AR-RU	X
EN-RU	X X	XX
ES-RU	X
FR-RU	X
AR-ZH	X	X X	X X
EN-ZH	X	X
ES-ZH	X	X X
FR-ZH	X	X	X X
RU-ZH	X X	X	XX
meaningful units in languages with logographic script such as ZH (cf. Zhang & Komachi (2018))
that are rarely captured, studied, or referred to as “morphemes”, this goes to show that linguistic
morphology, along with its complexity, as it is practiced today8 and that which has occurred in the
NLP discourse thus far, has only been relevant on the “word” level, conceptually constrained by
unstandardizable units such as “words” (and “sentences”). The definition of word, however, has been
recognized as problematic for a very long time in the language sciences (cf. Footnote 5).
While the lack of significant differences between pairs of source languages would signify neutraliza-
tion of source language instances, it does not mean that source languages have no effect on the target.
For our byte solutions with code pages, we experimented also with source side optimization in the
directions that involve AR/RU as source. This affected the distribution of the disparity results for that
representation — with 2 pairs being significantly different (see Table 1 under “ARRUs,t”). We defer
further investigation on the nature of source language neutralization to future work.
Target language pairs with significant differences are summarized in Table 2. We show that mor-
phological complexity can be empirically eliminated in this one-setting-for-all configuration with a
6-layer network, no hyperparameter tuning, and a maximum line length of 300 characters (and its
corresponding equivalence in other representations) as constrained by our hardware and compute
time listed in App. A and current data availability. A more analytical solution can be obtained through
data statistics (see App. E). A conceptual solution lies in the definition of “words” and morphology.
Sample-wise Double Descent (DD) Sample-wise non-monotonicity/DD (Nakkiran et al., 2020)
denotes a degradation followed by an improvement in performance with increasing data size. We
notice word models and character models with ZHtrg, i.e. models with high target |V |, are prone
8But there are no reasons why we cannot adopt a statistical science of language in finer granularities
beyond/without “words”, with standardized units (characters/bytes) and/or continuous representations. Resources,
e.g. quality parallel data or contrast sets, can serve both data science and ML interpretation and evaluation well.
8
Published as a conference paper at ICLR 2022
to exhibit a spike at 103. A common pattern for these is the ratio of target training token count to
number of parameters falls into O(10-4) for 102 lines, O(10-3) at 103, O(10-2) at 104, and O(10-1)
for 105 lines and so on. But for more atomic units such as alphabetic (not logographic) characters
(may it be Latin, Cyrillic, or Abjad) and for bytes, this progression instead begins at O(10-3) at
102 lines. Instead of considering this spike of 103 as irregular, we may instead want to think of this
learning curve as shifted by 1 order of magnitude to the right for characters and bytes and/or the
performance at 102 lines for words and ZH-characters due to being overparameterized and hence
abnormal. This would also fit in with the findings by Belkin et al. (2019) and Nakkiran et al. (2020)
attributing DD to overparameterization. While almost all work attribute DD to algorithmic reasons,
findings from Chen et al. (2020) corroborate our observation and confirms that DD arises due to “the
interaction between the properties of the data and the inductive biases of learning algorithms”. Other
related work on the DD phenomenon and its development can also be found in their work.
Erraticity We observe another type of sample-wise non-monotonicity, one that signals irregular
and unstable performance across data sizes and runs. Within one run, erraticity can be observed
directly as changes in direction on the y-axis. Across runs, large variance can be observed, even with
the same dataset. Erraticity can also be observed indirectly through a negative correlation between
data size and performance. Much work on length bias in NMT have focused on solutions related to
search, e.g. Murray & Chiang (2018). Our experiments show that a kind of length bias can surface
already with CLMing, without generation taking place.
Additional related work That basic data statistics are the driver of success in performance in
multilingual modeling has so far only been explicitly argued for in Mielke et al. (2019). We go
beyond their work in monolingual LMs to study CLMs and evaluate also in relation to data size,
representational granularity, and quantitative and qualitative fairness. To the best of our knowledge,
there has been no prior work on demonstrating the neutralization of source language instances through
statistical comparisons, a numerical analysis on DD for sequence-to-sequence models, the meta
phenomenon of a sample-wise non-monotonicity (erraticity) being related to length.
6	Conclusion
Summary We investigate whether the performance disparity between languages which have been
traditionally considered morphologically rich (AR and RU) and poor (ZH) in the 6-layer Transformer
CLM due to morphological complexity is justified and find that it is not. Performance disparity can
be explained by data statistics and in the context of computing, it can be eliminated by optimization
on length and |V | through character/byte representations. In fact, morphological complexity is not
a necessary concept in computing because “word” is not a necessary concept in computing, unless
we make it so through word segmentation. A morphological complexity hierarchy can result simply
through word segmentation. Furthermore, there are many possible interpretations to “words” for
humans and since morphology is defined with the concept of “word”, there is no stable ground for
assessing this complexity. Representational units of finer granularity were shown to help eliminate
performance disparity though at the cost of longer sequence length, which can have a negative impact
on robustness. In addition, we found all word models and character models with ZHtrg to behave
similarly in their being prone to exhibit a peak (as sample-wise DD) around 103 lines in our setting.
Outlook ML has enabled greater diversity in NLP (Joshi et al., 2020). Fairness, in the elimination
of disparity, does not require big data. This paper made a pioneering attempt to bridge research in
NNs/DL, language sciences, and language engineering through a data-centric perspective. Multilin-
guality is real and relevant in computing not due to canonical, structural linguistic concepts such
as morphology or “words” in our minds, but rather standards related to internationalization and
localization, such as character encoding — something which has thus far been sorely overlooked in
our discourse and curricula. We also believe that a more fine-grained statistical data science can well
complement algorithmic analyses with a view that is more empirically robust (i.e. experimentally
verifiable) and more relevant to machine processing, contributing to a more generalizable and inter-
pretable pool of knowledge for ML/NNs/DL. A more comprehensive study can lead us not only to
new scientific frontiers, but also to better designs and evaluation, benefitting the development of a
more general, diverse and inclusive AI.
9
Published as a conference paper at ICLR 2022
Ethics Statement: Fairness concerns for multilinguality
Clearer nomenclature If/When the intent is not to explicitly model linguistic morphology in
computing, one can simply describe languages and their statistical profiles with respect to their
representational granularity in characters or bytes (which are and/or can be exhaustively standardized
in computing), or refer to sequences as longer/shorter or having a higher/lower vocabulary size when
comparing them with each other, rather than “richer”/“poorer” based on concepts (e.g. “words”,
“sentences”) that can be ambiguous, contested, and inaccessible to many.
Accessibility Language communities who are unfamiliar with languages similar to dominant
languages or those who are reluctant to conform to one structurally similar interpretation should
not have to feel inadequate in processing their own language “from scratch”, if they so choose.
As technologists, we can help take equitable measures to make fairer data representations and
infrastructure available. A “word”-free view of language preempts linguistic/cultural hegemony and
such an interpretation would also help make the analyses of language data more objective and clearer.
Scarcity of quality and/or multiway data for science, evaluation, and documentation With
the rise of multilingual models comes an alleged decrease in reliance on parallel corpora for MT.
But data, esp. high-fidelity/quality9 textual and multimodal multiway parallel data, play not only
an important role in scientific research, but also one for historical/cultural documentation. And as
this paper shows, they can also serve as evaluation data for ML models for better understanding and
interpretation. As challenge sets, data for machine processing would need to be statistically diverse
and challenging. Parallel data from previous years have often come in the form of bitexts (2-way
parallel text data), usually “word”-tokenized where real length information has been compromised.
(The Bible data from Mayer & Cysouw (2014) came with another confound — the ZH numeral ‘一’
(“one”) is recognized as a dash (punctuation) and hence tokenized with surrounding whitespaces.) At
the time of our present study (2019-present), the UN Parallel Corpus was the only unperturbed, fully
multiway parallel data sufficient for reliable evaluation for our size range. Data for science, evaluation,
and documentation require long-term, stable platform(s) and support. There are many forms of data
science. But in terms of having a sustainable practice to collect and curate good data, and exercising
enough of a science with that data so to improve collective intelligence and mutual understanding,
instead of looking at data from an utilitarian point of view only for consumer application purposes,
there seems to be room for improvement still. We hope our work could help effect a positive change
in this direction.10
Acknowledgments
The author thanks all anonymous reviewers and PCs as well as the following individuals for their
informative comments on previous version(s) of this paper (names in alphabetic order by last name):
Benjamin Borschinger, Jordan Boyd-Graber, Christian Buck, KyUnghyUn Cho, Kenneth Church,
Miryam de Lhoneux, Rotem Dror, San Duanmu, Chris Dyer, Roman Flury, John Goldsmith, Yannic
Kilcher, Sandra Kubler, Thomas McColgan, Jason Naradowsky, Ryokan Ri, Anna Rogers, Mark
Rowan, Nikunj Saunshi, Rico Sennrich, Ekaterina Vylomova, Jason Weston, and Kie Zuraw.
We also thank Timothy Baldwin, Kevin Duh, Daniela Gerz, Kenneth Heafield, Felix Hieber, Marcin
Junczys-Dowmunt, Gal Kaplun, Dan Klein, Sabrina Mielke, Mathias Muller, Nakkiran Preetum, and
Annette Rios for their kind responses in correspondence concerning related work and software. We
thank Ismail Moukadiri for his help with the Arabic language.
Some initial experiments were supported by the computing resources at the Department of Informatics
and Computational Linguistics at UZH and the Institute of Neuroinformatics at UZH and ETH Zurich
(for which we especially thank Richard H.R. Hahnloser, Nikola I. Nikolov, Yuhuang Hu, and Pawel
Pyk).
9Note that “high-quality” does not necessarily mean “clean(ed)” data — it would depend on the situation/task.
10Please see further discussion in Wan (2022).
10
Published as a conference paper at ICLR 2022
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv e-prints, abs/1409.0473, September 2014. URL https:
//arxiv.org/abs/1409.0473.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of
Sciences,116(32):15849-15854,2019. ISSN0027-8424. doi: 10.1073∕pnas.1903070116. URL
https://www.pnas.org/content/116/32/15849.
Emily M. Bender. Linguistically naive != language independent: Why NLP needs linguistic ty-
pology. In Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics
and Computational Linguistics: Virtuous, Vicious or Vacuous?, pp. 26-32, Athens, Greece,
March 2009. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W09-0106.
Yoav Benjamini and Ruth Heller. Screening for partial conjunction hypotheses. Biometrics, 64(4):
1215-1222, 2008. ISSN 0006341X, 15410420. URL http://www.jstor.org/stable/
25502204.
Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki
Okazaki. It’s easier to translate out of English than into it: Measuring neural translation difficulty
by cross-mutual information. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 1640-1649, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.149. URL https://www.aclweb.org/
anthology/2020.acl-main.149.
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve, 2020.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. Revisiting
character-based neural machine translation with capacity and compression. In Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4295-4305.
Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/
D18-1461.
Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. Are all languages equally hard
to language-model? In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 2
(Short Papers), pp. 536-541, New Orleans, Louisiana, June 2018. Association for Computational
Linguistics. doi: 10.18653/v1/N18- 2085. URL https://www.aclweb.org/anthology/
N18-2085.
Rotem Dror, Gili Baumer, Marina Bogomolov, and Roi Reichart. Replicability analysis for natural
language processing: Testing significance with multiple datasets. Transactions of the Association
for Computational Linguistics, 5:471-486, 2017. URL http://aclweb.org/anthology/
Q17-1033.
San Duanmu. Word and wordhood, modern. In Rint Sybesma (ed.), Encyclopedia of Chinese
Language and Linguistics, pp. 543-549. Brill, 2017.
Nadir Durrani, Fahim Dalvi, Hassan Sajjad, Yonatan Belinkov, and Preslav Nakov. One size does
not fit all: Comparing NMT representations of different granularities. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1504-1516, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1154.
URL https://www.aclweb.org/anthology/N19- 1154.
11
Published as a conference paper at ICLR 2022
Adam Fisch, Jiang Guo, and Regina Barzilay. Working hard or hardly working: Challenges of
integrating typology into neural dependency parsers. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),pp. 5713-5719, Hong Kong, China, November
2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1574. URL https:
//www.aclweb.org/anthology/D19-1574.
Daniela Gerz, Ivan VuliC, Edoardo Ponti, Jason Naradowsky, Roi Reichart, and Anna Korhonen.
Language modeling for morphologically rich languages: Character-aware modeling for word-level
prediction. Transactions of the Association for Computational Linguistics, 6:451-465, 2018a. doi:
10.1162/tacl_a_00032. URL https://www.aclweb.org/anthology/Q18-1032.
Daniela Gerz, Ivan Vulic, Edoardo Maria Ponti, Roi Reichart, and Anna Korhonen. On the relation
between linguistic typology and (limitations of) multilingual language modeling. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 316-327,
Brussels, Belgium, October-November 2018b. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/D18-1029.
Martin Haspelmath. The indeterminacy of word segmentation and the nature of morphology and
syntax. Folia Linguistica, 2011.
Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and
Matt Post. The Sockeye neural machine translation toolkit at AMTA 2018. In Proceedings
of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1:
Research Papers), pp. 200-207. Association for Machine Translation in the Americas, 2018. URL
http://aclweb.org/anthology/W18-1820.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of
Statistics, 6(2):65-70, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/
stable/4615733.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viegas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Google’s multilingual neural machine translation system: Enabling zero-shot transla-
tion. Transactions of the Association for Computational Linguistics, 5:339-351, 2017. doi:
10.1162/tacl_a_00065. URL https://www.aclweb.org/anthology/Q17-1024.
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state
and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th
Annual Meeting of the Association for Computational Linguistics, pp. 6282-6293, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL
https://www.aclweb.org/anthology/2020.acl-main.560.
Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. Is neural machine translation ready
for deployment? A case study on 30 translation directions. In IWSLT 2016, Seattle, October
2016. URL https://www.microsoft.com/en-us/research/publication/
neural-machine-translation-ready-deployment-case-study-30-\
translation-directions/.
Daniel Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition. Pearson Prentice Hall,
second edition, 2009.
Adam Kilgarriff. "I don’t believe in word senses". CoRR, cmp-lg/9712006, 1997. URL http:
//arxiv.org/abs/cmp-lg/9712006.
Adam Kilgarriff. "How many words are there?". 2014. URL https://www.sketchengine.
eu/wp-content/uploads/How_Many_Words_2014.pdf.
12
Published as a conference paper at ICLR 2022
Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In 1995
International Conference on Acoustics, Speech, and Signal Processing, volume 1, pp. 181-184.
IEEE, 1995.
Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of
Machine Translation Summit X: Papers, pp. 79-86, Phuket, Thailand, September 13-15 2005. URL
https://aclanthology.org/2005.mtsummit-papers.11.
Philipp Koehn, Franz J. Och, and Daniel Marcu. Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 127-133, 2003. URL https://www.aclweb.
org/anthology/N03-1017.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177-180.
Association for Computational Linguistics, 2007. URL http://aclweb.org/anthology/
P07-2045.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine translation
without explicit segmentation. Transactions of the Association for Computational Linguistics, 5:
365-378, December 2017. doi: 10.1162/tacl_a_00067. URL https://www.aclweb.org/
anthology/Q17-1026.
Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, and William Chan. Bytes are all you need: End-
to-end multilingual speech recognition and synthesis with bytes. In ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5621-5625.
IEEE, 2019a.
Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. Is word
segmentation necessary for deep learning of Chinese representations? In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 3242-3252, Florence,
Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1314. URL
https://www.aclweb.org/anthology/P19-1314.
Ken Lunde. CJKV Information Processing. O’Reilly Media, Inc., 2nd edition, 2008. ISBN
0596514476, 9780596514471.
Thomas Mayer and Michael Cysouw. Creating a massively parallel Bible corpus. In Proceedings
of the Ninth International Conference on Language Resources and Evaluation (LREC-2014), pp.
3158-3163, Reykjavik, Iceland, May 2014. European Languages Resources Association (ELRA).
Sabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian Roark, and Jason Eisner. What kind
of language is hard to language-model? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pp. 4975-4989, Florence, Italy, July 2019. Association
for Computational Linguistics. doi: 10.18653/v1/P19-1491. URL https://www.aclweb.
org/anthology/P19-1491.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. Generating complex morphology for machine
translation. In Proceedings of the 45th Annual Meeting of the Association of Computational
Linguistics, pp. 128-135, Prague, Czech Republic, June 2007. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/P07-1017.
Kenton Murray and David Chiang. Correcting length bias in neural machine translation. In Proceed-
ings of the Third Conference on Machine Translation: Research Papers, pp. 212-223, Belgium,
Brussels, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6322.
URL https://www.aclweb.org/anthology/W18- 6322.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
13
Published as a conference paper at ICLR 2022
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association
for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://www.aclweb.
org/anthology/P02-1040.
Edoardo Maria Ponti, Helen O'Horan, Yevgeni Berzak, Ivan Vulic, Roi Reichart, Thierry Poibeau,
Ekaterina Shutova, and Anna Korhonen. Modeling language variation and universals: A survey on
typological linguistics for natural language processing, 2020.
R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria, 2014. URL http://www.R-project.org/.
Patrick Royston. Remark as r94: A remark on algorithm as 181: The w-test for normality. Journal of
the Royal Statistical Society. Series C (Applied Statistics), 44(4):547-551, 1995. ISSN 00359254,
14679876. URL http://www.jstor.org/stable/2986146.
Djame Seddah, Sandra Kuebler, and Reut Tsarfaty (eds.). Proceedings of the NAACL HLT 2010
First Workshop on Statistical Parsing of Morphologically Rich Languages, Los Angeles, CA, USA,
June 2010. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/W10-1400.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.
org/anthology/P16-1162.
S. S. Shapiro and M. B. Wilk. An analysis of variance test for normality (complete SamPIeS)中.
Biometrika, 52(3-4):591-611, 12 1965. ISSN 0006-3444. doi: 10.1093/biomet/52.3-4.591. URL
https://doi.org/10.1093/biomet/52.3-4.591.
C. Spearman. The proof and measurement of association between two things. The American
Journal of Psychology, 15(1):72-101, 1904. ISSN 00029556. URL http://www.jstor.
org/stable/1412159.
Martin Sundermeyer, Ralf Schluter, and Hermann Ney. Lstm neural networks for language modeling.
In INTERSPEECH, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. URL http:
//papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.
Ada Wan. A statistical typology of language in finer granularity with parallel data. 2022.
B. L. Welch. The Generalization of ‘Student’s’ Problem when Several Different Population Variances
are Involved. Biometrika, 34(1-2):28-35, 01 1947. ISSN 0006-3444. doi: 10.1093/biomet/34.1-2.
28. URL https://doi.org/10.1093/biomet/34.1-2.28.
Frank Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80-83, 1945.
ISSN 00994987. URL http://www.jstor.org/stable/3001968.
Longtu Zhang and Mamoru Komachi. Neural machine translation of logographic language using
sub-character level information. In Proceedings of the Third Conference on Machine Translation:
Research Papers, pp. 17-25, Belgium, Brussels, October 2018. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/W18-6303.
MiChaI Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. The United Nations Parallel
Corpus v1.0. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Sara
Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis (eds.), Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC 2016), Paris, France, may 2016. European Language Resources
Association (ELRA). ISBN 978-2-9517408-9-1.
14
Published as a conference paper at ICLR 2022
Appendices
A	Data selection and preprocessing details	16
B	Hyperparameter setting	17
C	Correlation statistics	18
D	Statistical comparisons	19
E	Data statistics	20
F Enlarged figures for all 30 language directions (aggregate results
from all runs)	24
15
Published as a conference paper at ICLR 2022
A Data selection and preprocessing details
The UN Parallel Corpus v1.0 (Ziemski et al., 2016) consists of manually translated UN documents
from 1990 to 2014 in the 6 official UN languages. Therein is a subcorpus that is fully aligned
by line, comprising the 6-way parallel corpus we use. We tried to have as little preprocessing or
filtering as necessary to eliminate possible confounds. But as the initial runs of our experiment failed
due to insufficient memory on a single GPU with 12 GB VRAM11, we filtered out lines with more
than 300 characters in any language in lockstep with one another for all the 6 languages such that
the subcorpora would remain parallel, thereby keeping the material of each language semantically
equivalent to one another. 8,944,859 lines for each language were retained as our training data which
cover up to the 75th percentile in line length for all 6 languages. In order to monitor the effect of data
size, we made subcorpora of each language in 5 sizes by heading the first 102, 103, 104, 105, 106
lines12. We refer to this as dataset A. In addition, to better understand and verify the consistency
of the phenomena observed, we made 2 supplemental datasets by shuffling the 8,944,859 lines two
different times randomly and heading the number of lines in our 5 sizes for each language, again in
lockstep with one another (datasets B and C).
The systematic training regime that we gave to our language directions is identical for all and we
controlled also for seeds. For each of the 3 primary representations — character, byte, and word, we
performed:
•	5runsin5 sizes (102 - 106 lines): A0 (seed=13), B0 (13), C0 (9948), A1 (9948), A2 (265), and
•	7 more runs in 4 sizes (102 - 105 lines): A3 (777), A4 (42), A5 (340589), A6 (1000), A7
(83146), B1 (9948), &C1 (13).
Figure 1 shows results from all 12 runs in all sizes for the primary representations.
For the alternate/secondary representations, we performed 3 runs each in 5 sizes (102-106 lines) (A0,
B0, & C0). Due to limitations in computing resources, we were not able to perform as many runs as
the primary representations. But important for our statistical comparisons is that we evaluate based
on an equal number of runs and on the same data for all candidates. Tables 1 & 2 are the results.
For each run and each size, there are 30 pairwise directions (i.e. 1 source language to 1 target language,
e.g. AR-EN for Arabic to English) that result from the 6 languages. We trained all 150 jobs (30
directions x 5 sizes) for each run and representation using the Transformer model (Vaswani et al.,
2017) as supported by the SOCKEYE Toolkit (Hieber et al., 2018) (version 1.18.85), based on MXNet
(Chen et al., 2015). A detailed description of the architecture of the Transformer can be found in
(Vaswani et al., 2017). The same set of hyperparameters applies to all and its values are listed in
Appendix B.
For character modeling, we used a dummy symbol to denote each whitespace. For byte, we turned
each UTF-8-encoded character into a byte string in decimal value, such that each token is a number
between 0 and 255, inclusive. For word, we followed (Junczys-Dowmunt et al., 2016) and used
the Moses tokenizer (Koehn et al., 2007) as is standard in NMT practice when word tokenization is
applied and Jieba13 for segmentation in ZH.
Pinyin is a romanization of ZH characters based on their pronunciations and Wubi an input algorithm
that decomposes character-internal information into stroke shape and ordering and matches these
to 5 classes of radicals (Lunde, 2008). For Pinyin, we used the implementation from https:
//github.com/lxyu/pinyin in the numerical format such that each character/syllable is
11GPUs used for experiments in this paper range from a NVIDIA TITAN RTX (24 GB), NVIDIA GeForce
RTX 2080 Ti (11 GB), a GTX Titan X (12 GB), to a GTX 1080 (8 GB). All jobs were run on a single GPU
setting. Some word-level experiments involving ARtrg or RUtrg at 106 had to be run on a CPU as 24 GB VRAM
were not sufficient. Models with higher maximum sequence lengths (e.g. byte models) were trained with 24 GB
VRAM. Difference in equipment does not necessarily lead to degradation/improvement in scores.
12The terms “line” and “sentence” have been used interchangeably in the NLP literature. We use “line” to
denote a sequence that ends with a newline character and “sentence” as one with an ending punctuation. Most
parallel corpora, such as ours, are aligned by line, as a line may be part of a sentence or without an ending
punctuation (e.g. a header/title). Using a standardized unit such as “line” would also be a fairer measure to
linguae/scriptiones continuae (languages/scripts with no explicit punctuation).
13https://github.com/fxsjy/jieba
16
Published as a conference paper at ICLR 2022
followed by a single digit indicating its lexical tone in Mandarin. For Wubi, we used the dictionary
from the implementation from https://github.com/arcsecw/wubi.
We have implemented all representations such that they would be reversible even when the se-
quence contains code-mixing. Additional code will be made available at https://github.com/
dadasci.
We used the official dev set as provided in (Ziemski et al., 2016), 3,077 lines per language remained
from 4,000 after filtering line length to 300 characters. Data statistics is provided in Appendix E for
reference.
Notes on training time Each run of 30 directions in 5 sizes took approximately 8-12 days for
character and byte models. Byte models generally took longer — hence training time is positively
correlated with length (concurring with observations by Cherry et al. (2018) as they compared
character with BPE models). A maximum length of 300 characters entails a maximum length of
at least 300 bytes in UTF-8. Each run of word models (30 directions, 5 sizes) took about 6 days
(excluding the training of some 7-9 directions out of 30 per run involving ARtrg or RUtrg at 106 on
word level which took about 12-18 hours each direction to train on a CPU as these required more
space and would run out of memory (OOM) on our GPUs otherwise). These figures do not include
the additional probing experiments described in § 4.
B	Hyperparameter setting
•	encoder transformer;
•	decoder transformer;
•	num-layers 6:6;
•	num-embed 512:512;
•	transformer-model-size 512;
•	transformer-attention-heads 8;
•	transformer-feed-forward-num-hidden 2048;
•	transformer-activation-type relu;
•	transformer-positional-embedding-type fixed;
•	transformer-preprocess d; transformer-postprocess drn;
•	transformer-dropout-attention 0.1;
•	transformer-dropout-act 0.1;
•	transformer-dropout-prepost 0.1;
•	batch-size 15;
•	batch-type sentence;
•	max-num-checkpoint-not-improved 3;
•	max-num-epochs 50;
•	optimizer adam;
•	optimized-metric perplexity;
•	optimizer-params epsilon: 0.000000001, beta1: 0.9, beta2: 0.98;
•	label-smoothing 0.0;
•	learning-rate-reduce-num-not-improved 4;
•	learning-rate-reduce-factor 0.001;
•	loss-normalization-type valid;
•	max-seq-len 300 for character, word, and BPE, 672 for all bytes, 688 for Wubi, 680 for Pinyin;
•	checkpoint-frequency/interval 4000.
(For smaller datasets, the end of 50 epochs is often reached before the first checkpoint. Since SOCKEYE
only outputs scores at checkpoints, we adjusted the checkpoint frequency as follows to get a score outputted
by the end of 50 epochs: 1000 for 100 lines for all character & byte instances, 400 for 100 lines for word
and 500 for 100 lines BPE, 3450 for 1000 lines for word & BPE. For the very few cases that this default
does not suffice due to bucketing of similar length sequences, we manually set the checkpoint frequency to
the last batch.)
17
Published as a conference paper at ICLR 2022
C Correlation statistics
Best correlating metrics, i.e. the union of top 3 metrics for all representations.
For each representation, the top 3 metrics are boldfaced.
All correlations are highly significant (p < 10-30), except for min source length for WORD (p ≈ 0.0001) and
min target length for WORD (p ≈ 0.3861).
Metric	CHAR	Pinyin	Wubi	BYTE	ARRUt	ARRUs,t	WORD	BPE
minimum length (target)	0.84	0.85	0.86	0.60	0.84	0.84	-0.02	0.65
minimum length (source)	0.82	0.84	0.85	0.57	0.84	0.84	0.10	0.64
number of tokens (source)	-0.78	-0.81	-0.82	-0.60	-0.81	-0.81	-0.59	-0.83
TTR (target)	0.83	0.83	0.84	0.48	0.81	0.81	0.61	0.83
|V | (source)	-0.54	-0.51	-0.51	-0.50	-0.67	-0.68	-0.63	-0.86
data size in lines	-0.80	-0.83	-0.83	-0.59	-0.81	-0.81	-0.62	-0.86
OOV token rate (target)	0.69	0.66	0.66	0.47	0.67	0.68	0.66	0.62
OOV type rate (target)	0.70	0.71	0.72	0.47	0.69	0.70	0.65	0.62
TTR (source)	0.67	0.71	0.71	0.60	0.81	0.81	0.56	0.82
The full list of metrics used for the correlation analysis is:
1.	minimum length (source),
2.	minimum length (target),
3.	maximum length (source),
4.	maximum length (target),
5.	median length (source),
6.	median length (target),
7.	mean length (source),
8.	mean length (target),
9.	length std (source),
10.	length std (target),
11.	data size in lines,
12.	number of parameters,
13.	number of types (|V |) (source),
14.	number of types (|V |) (target),
15.	number of tokens (source),
16.	number of tokens (target),
17.	type-token-ratio (TTR) (source),
18.	type-token-ratio (TTR) (target),
19.	OOV type rate (source),
20.	OOV type rate (target),
21.	OOV token rate (source),
22.	OOV token rate (target),
23.	token ratio,
24.	target type-to-parameter ratio,
25.	target token-to-parameter ratio,
26.	distance between the TTRs of source and target = (1 - TTRsrc/TTRtrg)2,
27.	token-to-parameter ratio (i) = (median length source * median length target * num_lines) / num_parameters,
28.	token-to-parameter ratio (ii) = (num_source_tokens * num_target_tokens) / num_parameters.
18
Published as a conference paper at ICLR 2022
D Statistical comparisons
Recall the definition and method for our Disparity/Inequality assessment from § 2:
In the context of our CLMing experiments, we consider there to be “disparity”
or “inequality” between languages l1 and l2 if there are significant differences
between the performance distributions of these two languages with respect to each
representation. Here, by performance we mean the number of bits required to
encode the held-out data using a trained CLM. With 30 directions, there are 15
pairs of source languages (lsrc1, lsrc2) and 15 pairs of target languages (ltrg1, ltrg2)
possible. We compare the source languages among each other, and the target
languages among each other. Each lsrc or each ltrg consists of scores from all models
trained across various sizes and directions. To assess whether the differences
are significant, we perform unpaired two-sided significance tests with the null
hypothesis that the score distributions for the two languages are not different. Upon
testing for normality with the Shapiro-Wilk test (Shapiro & Wilk, 1965; Royston,
1995), we use the parametric unpaired two-sample Welch’s t-test (Welch, 1947)
(when normal) or the non-parametric unpaired Wilcoxon test (Wilcoxon, 1945)
(when not normal) for the comparisons. We use the implementation in R (R Core
Team, 2014) for these 3 tests. To account for the multiple comparisons we are
performing, we correct all p-values using Bonferroni’s correction (Benjamini &
Heller, 2008; Dror et al., 2017) and follow Holm’s procedure14 (Holm, 1979; Dror
et al., 2017) to identify the pairs of l1 and l2 with significant differences after
correction. We report all 3 levels of significance (α ≤ 0.05, 0.01, 0.001) for a more
comprehensive overview. In contrast to Dror et al. (2017), which aimed to compare
the performance of different algorithms, we compare languages (in the context of
computing).
To get samples for the statistical comparison results for the Disparity Table (Table 1):
For each representation, we used 3 runs (A0, B0, C0) in 5 sizes (102-106 lines) for each lsrc and each
ltrg. There are:
6 lsrc (ARsrc, ENsrc, ESsrc, FRsrc, RUsrc, ZHsrc) and
6 ltrg (ARtrg, ENtrg, EStrg, FRtrg, RUtrg, ZHtrg).
We compare pairwise among the lsrc. The 15 pairs are:
ARsrc-ENsrc, ARsrc-ESsrc, ARsrc-FRsrc, ARsrc-RUsrc, ARsrc-ZHsrc,
ENsrc-ESsrc, ENsrc-FRsrc, ENsrc-RUsrc, ENsrc-ZHsrc,
ESsrc-FRsrc, ESsrc-RUsrc, ESsrc-ZHsr
c,
FRsrc-RUsrc, FRsrc-ZHsrc,
RUsrc-ZHsrc.
Likewise 15 pairs among the ltrg.
For example, for the character (primary) representation to compare between ARsrc and ENsrc, we
construct the sample for ARsrc (sampleARsrc) and the sample for ENsrc (sampleENsrc) as follows:
Out of the 30 CHAR directions, there are 5 directions involving ARsrc trained for each run and data
size (i.e. the directions: AR-EN, AR-ES, AR-FR, AR-RU, AR-ZH).
For each direction, there are 15 models trained (3 runs x 5 sizes). We take all 75 CHAR models (15
models x 5 directions) involving ARsrc as sampleARsrc. That’s a sample of size 75.
Likewise, for ENsrc (5 directions: EN-AR, EN-ES, EN-FR, EN-RU, EN-ZH), we also have 75 data
points for sampleENsrc. (Likewise also for all 6 lsrc and all 6 ltrg.)
For the comparisons, we compare pairwise, i.e. with two samples each time, but with unpaired
two-sample Welch’s t-test (when normal) or the non-parametric unpaired Wilcoxon test (when not
normal) because sampleARsrc and sampleENsrc have one direction that is not paired: AR-EN and
EN-AR. Other directions can be seen as paired, e.g. AR-ES and EN-ES as both having the same ltrg.
14using implementation from https://github.com/rtmdrr/replicability-analysis-NLP
19
E Data statistics
2。
•	Number of types, i.e. vocabulary size (∣ V∣). Note that Sockeye adds for its calculation 4 additional types: <pad>, <s>, <∕s>, <unk>.
•	Number of tokens. This excludes the 1 EOS/BOS (eπd-∕begiπniπg-of-seπteπce) marker added by Sockeye to each line.
•	Out-of-vocabulary (OOV) type rate (in %), i.e. the fraction of the types in the dev data that is not covered by the types in the training data.
•	OOV token rate (in %), i.e. the fraction of tokens in the dev data that is treated as UNKnowns.
•	Type-token-ratio (in %), i.e. the ratio between the number of types and tokens in the data. This is a rough proxy for lexical diversity in that a value of 1 would indicate that no type is ever seen twice, and a value very close to O would indicate that very few
distinct types account for almost all of the data.
•	Line length (excl. EOS/BOS marker): meaπ±standard deviation, and the 0/25/50/75/1 OO-th percentile.
Statistics for dataset A
s,'ss4s∙∙s∙∙,,∙u∙
"u'rarau,r"
∙ss,"∙∙u∙us
*,■常RIJzh--
Msfe-
1•■，电*t*t*t3-u'"^u'"^
ss∙∙sss
i≈,'∙sl∙∙∙s∙
一g≡s≡
腌≈5is≈
:::湍蹒:::
::::"::;:::
::::"::::::
湍!!;谭
u∙≈s≈s∙'∙,
潞滥!;:
::!::!:s
::::::湍
:::::::::
:s:s;s
滥滥:1;
*»瑞1»»⑸
us聘国1«
酬需,4,≡
s≡a≡!i"
Ii
""s∙∙≈s4,s∙∙
™鬻%s4∙
S⅛i,',i,'i∙'k
l∙∙l∙∙≈≈
::!::!歌
颂-4∙∙u黑
,,,,,
Bs尴!:=
;：=懒愣
-缴镯
Ss
∙ss,s4ij∙iji
s-islus4
S's44ai
t"u∙ls一
:s:s1"
歌≈i≡∙∙∙,
::::s:!:
PUbliShed as a ConferenCe PaPersICLR 2022
≡s≡s≡
sss≡≡
,'uisl"is
懈媵-
瑞媵黑
≈"≈s≈s
::::s:"
5≈S≡s≡
≈s≡∙
tsais
i≡≈sii≡
sI≡uu
∙i≈is≡
!?:!::!£
-1(*■■BS»,"
,豁宾黑-5,
*r取常黑
"<豁说更
•'"•'"懈ξ
u,案财-益
晒黑嚷»≡,
11
Ssia
睿-畿


I
I
≡≡
I

泮


≈≈≈
≈≈≈
≈≈≈


™
≡,≡,
I
S≡s,
绘S55s

s≡≈
≡s≡SE

°一
AR EN ES FR RU ZH	S/49/115/170/^ 温娥镰 湍4潞秘陶 瑞蠲图褊 泰精雕	5郴报黑瀚瑞慨湍需慨廊靛嬴 渭描蹄8…挪搦嬴林藐%斶跪骗 6摆揣陶阖虢搞瀚㈱慨“瑞⅛ 可雕泪（B报混隈峭踹嬴蔚踹 1⅛ √≈‰ M擢假怖擢魏一瑞疏⅛ 战渴昵谭帮郎面蜀踹播防踹	局湍嬴隔滤院战搬龈揭僦假陶就擒					跪罚渴，跪献6搞球微湍潮温制虢篙 瑞甯湍茄粽黑调露瑞溪辘选播球捣 卷跳微储黯，躅就『混揭源播球搞 流程瑞就跣瀚疆温湍那瑞就端黑 1S.54±1O.65 54.75 ±9.53	51.66±1O.S1	51.51 ±11.15	50.45 ± 11.44 1/8/18/27/39 1/19/25/31/58 1/14/22/29/104 1/13/22/30/1" 1/11/20/29/136 17.51±10.6S 53.57±9.55	50.77±10.4S	50.39 ± 10.60	19.36 ±10.ST 2/7/17/25/46 1/18/23/29/76 1/13/20/28/108 1/13/20/28/111 1/11/19/27/167	湍蒜法推漏/踹潞瑞「湍搬工混帚混 凋靛洗施崎雕潞帮搞溜揭猿摇防跪 4瑞薪湍斶茄愣溜帮瑞林瑞乳㈱嘉微 弑瑞"牒调"瑞踹贯强⅛摘牖犒 31.10±17.45 59.11 ±11.51 53.13 ± 11.71 53.0S±15.17	55.07±15.57 4/18/29/45/69 1/21/29/37/68 1/15/23/31/104 1/14/23/32/110 1/12/22/31/166 55.53±15.33 55.46 ± 10.59 51.55 ±10.94 50.S4±10.95	19.S9±11.53 2/13/20/31/58 1/19/25/32/79 1/14/11/19/108 1/13/11/18/111 1/11/20/28/167
			瑞踞;微城揭靛嘉就需勰。。 7揶端微7湍蠲球。6斶露捣7			1∕⅛≡oo 1∕⅛≡11 描制赫加播瀛瑞			
			5/朋谪陶 517.51 ±134.51 20/77/215/332/489 8559±5L18 7/35/81/127/192	5/掇温祸捻2 309.5S±114.fe 10/231/318/392/554 116.67±43.S5 6/88/117/1宓/243	建隘瑞嬴 105.00 ±50.61 1/66/103/139/297				
						559.90 ±137.10 1/155/264/365/566 99.90 ±51.49 1/61/101/138/337	547.10±141.14 1/127/249/357/567 94.57±55.S0 1/51/95/134/594		
ZH_pinyin ZH-WUbl AR-CPl256	7瑞跪踹5 燃捣曲	14S.0S± 55.57	127.96 ±63.96	125.31±65.25	11S.47±66.SS 7/109/149/188/312 1/82/129/175/369 1/76/156/174/407 1/63/119/169/645 157.19 ± 4S.00	110.45 ± 54.71	10&86 ± 56.04	105.91 ± 57.41 7/94/129/158/266 1/71/111/150/327 1/67/110/150/364 1/55/103/146/627	描院黑嬴	5搦源踹3		僦瑞哈	微弱除。。		
调蠲跳6。+踹靛最温温孤揄段湍僦）一/胡猊歌。
PUbliShed as a ConferenCe PaPersICLR 2022
Statistics for dataset B
Number of Iinss	100	1,000	CHAR 10,000	100,000	1,	00,	00	100	1,000	BYTE 10,000	1	0,0	0	1,	00,000	100	1,000	粉	100,000	1,000,000	100	1,000	EPE 10,000		00,000	1, Ooo,00。
Number of TYPES 1 AR,pl256 RiLCPI251	87 76 86 89 109 761 71 87	134 99 104 112 142 1,575 104 124	2,402	251 189 189 202 504 3,465 241 246	431 343 366 352 320 4,705 430 421				124 103 IOS 120 141 153	147 135 133 142 159 165		159 163 161 162 161 175			181 181 185 185 ISO 188	常 1,142 SfiT	6,719 4,1宓 4,775 强	:代： 16⅛5 展	97,467 普,舞 方:348 46： 788	311,355 140,912 154,482 132,881 251,355 140,188	SlO 784 814 812 993 1,0把	4,408 球 3,670 3：717	黑； 12,529 12,258 普端	29,680 l≡ 29,261 27,409	29,959 29,178 29,471 29,370 29,811 29,107
								87 109	134 142	168		251 504			431 320										
Number Cf TOKENS ⅛ AR,pl256 RU,pl251	9,798 11,816 13,543 13,173 13,106 3,329 11,391 10,035	100,599 119,796 135,106 134,705 130,289 33,TES 115,577 101,049	1,019,696 1,201,839 1,363,454 1,358,567 1,325,793 340,692 1,166,227 1,019,113	10,248,976 12,087,922 13,709,493 13,662,024 13,356,717 3,419,803 11,732,264 10,242,540	120 137 136 133 34 11'	481 969 180 811 699 206, 36T	163 542 870 473 06 349	17,656 11,820 ⅛61° 24,059 9,122	181,866 119,839 137,613 139,250 2*嗑5	1,842,138 1, 202,217 1,幽315 1,404,001 2,433,154 935,172	18 12 13 14 24 9,	517 091 967 121 512 02,	669 075 629 949 743	151 139 141 245 94	172,618 006,803 753,263 403,257 396,332 064,353	戢 ιjβo 1,866	牌 :消 19,642 18,697	熠:假 241,789 247,027 200,188 189,773	1,915,746 2,122,365 2,431,579 2,485,203 2,014,889 1,906,704	19,138,724 21, 534,437 54, 326,055 24,874,355 50,153,124 19, "3,133	ɪ 3：760 2,563	25,234 24,634 28,179 28,541 26,136 21,630	210,622 220,390 253,075 257,305 218,325 199,028	090,403 159,566 485,292 527,889 170,410 946,543	北髓需 24,933,犯7 25,374, 629 21,792, 216 19,574,572
								调	秘黑	1, 019,696 1, 325,793	10 13	248 356	976 717	Iof	481,816										
OoVt等ate(%) ⅛ AR,pl256 RiLCPI251			o.π &25 11.71 7.96 5.74 6.ES 5.04 0.S3					,28.46 56.47 2455 2458 53.61 2026	488 11.76 10.91 7.63 486 5.61							93.61 S9.5S 90.59 90.53 93.50 8973	75.07 61.04 64.57 65.61 6975 64.10	35.51 2628 57.73 2579 35.50 59.54	13.07 10.55 9.61 &71 11.10 10.44		7.86 4.00 4.OS 3.S7 4.39 6739	0.30 0.44 0.44 0.47 0.53 57.00			
								麟	5.3S 6.16	0.77 5.74		0.0 0.0			0.00 0.00										
OoVtOtenrate(%) zz11h⅛ 能耦																53.94 34.11 31.55 30.96 49.05 37.81	2539 15.IS 15.57 11.91 2275 13.74				0.S5 0.30 0.19 0.55 0.71 55.05				
								0.31 0.55	0.01 0.00	0.00 0.00		0.0 0.0			0.00 0.00										
ttr7%> ⅛ AR,pl256 RiLCPI251																61.S6 45.79 40.59 40.43 5& 27 4S,07	3532 19.70 50.01 1&97 33.19 2226	1482 6.47 6.95 6.33 15.97 7.75			53.91 54.03 51.99 22.16 56.41 41.67	17.47 13.S3 13.15 12.86 1S.4S 17. IS			
								0.S9 0.S3	宵	0.05 0.01		0.0 0.0			0.00 0.00										
21
Meanline leι⅞th± std 0/25 后 0/75/10(Mh										
AR	9∕⅛⅛ i∕≡⅛ 1∕≡^oo 1∕⅛≡oo 1∕⅛≡oo	勰睛牌2	2斯热温	瑞/瑞斯常	湍/㈱繇	摆寇尴焉	撷挪"嘏相湍温版勒篇牌揭漪犒	2匐就能济;凝贰		潞糕6潞谭5
EN	溜常揭 潞露揄 商常慨 糊就幅 糊防黑	7∕64∕114∕m∕556	漏混靛蕊	湍靠假	V⅛⅛≈oo	糊踢髓3	潞糊瑞调瑞女就揣"跋踹7潞斶源	施筋;输搬漪4摆	湍溪选	潞帮捣那帮热
ES	谓渴菰温湍湖就…糕初瑞 阖踹踹一加混褊编	137一90 ±7&93 11/81/134/202/288	湍湍瑞温	斶跟翁瑞	湍陶秘版	湍箭蜀金	凋靛孤摆播报端泰瑞岗撮捣J躅雕。	猬靛搐徜帚牒	战椽犒	潞揭小徵提搞
FR	131.73 ±75.95	134.70 ±S0.4S	135S6±79.51	136一62±7&71	136.S1±7S.9O 8/74/128/196/289 2/64/135/202/300 1/65/137/199/300 1/69/138/199/300 1/69/138/200/300	湍麻踹	湍需需扁	斶斯籍瓢	湍资揣揭	湍髓展	潞罐湍瑞一卷踹滞翅曲潞出松	笳睛踹温节隘	报简踹	潞帛捣溜就温
RU	131.06 ±76.S9	130.59 ±77S0	135.5S±77S0	133.57± 77.09	133.70 ±77.19 10/71/123/195/271 2/62/130/193/299 1/65/133/195/300 1/67/134/194/300 1/67/134/195/300	540.59 ±144.55 12/125/229/363/505	539.15±144.35 2/113/2犯/352/558	543.35 ±144.14 1/117/245/358/565	245.13 ±14239 1/122/246/357/566	545.40 ±143. OS 1/122/247/3犯/569	19.60±10.5S	19.64 ± 11.5S 50.05±11.63	50.15±11.50	50.15 ±11.51 1/12/20/28/43 1/10/20/29/72 1/10/20/29/85 1/11/20/29/117 1/11/20/29/1双	2陶巍揭涡转/献9		盘IE 端指 0 lfll^l/161
ZH ZH-Pnym ZH-WUbl AR,pl256 RU,pl251	施茄瑞然谒般溜瑞温溜就输溜耨图 113.91±64.46	115.5S ± 69.1S	11662 ± 6&13	117.35 ±67.41	117.37±67.50 8/66/109/174/265 2/55/115/167/325 1/58/117/169/357 1/60/118/169/394 1/60/118/169/680 100.35 ±55.70	101.05 ±60.13	10L91 ± 5&94	10243 ± 5&21	10246 ± 5&25 8/59/99/150/226	2/48/100/146/325 1/52/102/147/318 1/53/102/147/381 1/54/103/146/688	搦施编 蔚瑟法 湍标拨	92一69±55.28 2/45/92/133/290 摘瑞跳4	93.55 ±54.59 1∕48∕93∕135∕2S3 1/51/101/147/300 135.5S±77.S0 1/65/133/195/300	94.03 ±53.67 1/49/94/135/369 斶版瑞 V⅛≈oo	94.06 ±53.73 1/49/94/135/667 斶版瑞 漏掇献M)	1S.66±1O.5S	1S.7O±11.1S	1S.9S±11.O5	19.07±10.SS	19.07±10.90 2/11/18/26/48 1/10/18/27/68 1/10/19/27/99 1/10/19/27/116 1/10/19/27/167	醯端品混铸魏湍跪捣指瑞犒描蔺除		
PUbliShed as a ConferenCe PaPersICLR 2022
Statistics for dataset C
黑 1,8M	院 24, 600 24, 950 20,474 19,120	报滥 242,672 248,488 201,618 190,485	1,912,669 2,123,153 2,430,586 2,485,118 2,016,451 1,907,643	19, 21, 24, 24, 20, 19,	137, 231, 319, 864, 156, 073,	540 465 760 449 467 463	3,51« 3：813 2,559	25,601 25,111 28,726 28,907 26,772 22,067	210,738 221,264 慧徵 219：794 199,706	2,087,431 2,160,162 2,483,995 2,527,957 2,172,616 1,947,300	H 21： 19,	蠹 925： 362, 787, 573,	760 482 209 754 539 372
9372 8947 90.71 S9.95 93.11 90.34	71.13 60.56 63.96 65.57 69.66 6348	35.70 2627 27.18 55.96 32.15 2939	13.44 10.43 971 &89 10.91 10.31				6.06 3.55 4.44 3.96 4S5 67.9S	0.34 0.49 0.53 0.49 0.59 2622					
IIIll0'»
滥
懈
密
髓温
i
懈:!;黑
≈≡-
湍
播黑懈
潞
Zs-
≈≈≈Z Zzz Il
≈≈≈
:::::;:::
::::":::
≈≈≈
::!::!歌
温黑膜
≡≡
22
23
MeanlinekI⅞th± std
0∕25∕50∕75Λ00-th
AR
EN
ES
FR
RU
ZH
ZH-Pnym
ZH-WUbl
AR,pl256
RU,pl251
Statistics for development (dev) set
As a different set of vocabulary is learned from each training dataset and data size, BPE has a distinct dev set for each.
334,358 391,222 443,958 43S,083 431,538 107,990 376,979 330,734	605,516 391,260 452,190 452,556 793,214 301,085 334,3空	61, 371 67, 629 78, 087 78,745 64, ISO 60, 013	167,574 155,826 170,433 166,280 177,818 96,745	115,693 101,782 113,犯 3 114,694 113,628 80,231	S3, 001 77, (E9 88, 634 88,726 79,763 68,129	76,284 71,276 82,281 82,448 72,480 63,614	70,527 70,339 *魁 以整	149,第9 140, 256 155, ES7 156, 256 163, 319 93,775	97,843 韶g 104,071 黑翁	潟馈 84：914 85,115 需播	Si	68,270 69,348 SO Jti 北,源	149,623 140,377 154,746 153,745 163,806 94,127	i≡ 104,091 7Vis	73,541 73,468 84,870 85,125 75,096 65,203	68,579 69,633 80,579 80,912 70,196 61,916	68,278 69,341 80, 371 80, 654 69,犯2 61,823
		55.54 10.64 10.95 10.56 19.97 12.35	0.45 0.45 0.40 0.45 0.49 3.33		10. 84 778 7S1 7S0 15.64 9.37	15.04 10.47 10.64 10.34 1725 11.S5	17.65 10.74 10.91 10.63 17.99 15.19			14.64 &86 &87 S.S0 14.73 10.33	IS.90 10.85 11.01 10.75 IS.35 1248	19.17 10.93 11.15 10.79 IS.51 15.46			1452 &92 &87 S.73 14.66 10.41	1&86 IOSl 11.01 10.73 IS.35 15.36	19.IS 10.95 11.15 10.SO 1&52 15.49
尚磊徐渊帮狼湍拓镶混挪混拢法
溜靛温潞端沟瀛擢篇叙挪洸溜懿瑞
湍舒描工踹加湍都5匐薪洗溜部用
搞牖需…繇揄窑湍崎麒湍游却调薪洗
圄露/黑部僦…潞靛蕊溜泰搞湍榔5
拣假挪混薪撮湍薪捣潞转瑞战楙跟
就竭搞摇㈱捣贰蓊虢2田强/勰凋泰提
播球微调就魄混转瑞潞燃…徜蒜疆
J露徜选拣防:诵。湍筋热7湍筋劲3提;彘跪
相温搬禽陶虢湍燃3徜扇跳潞挪6
盛端疆"踹斯5调懿瑞提牖踹湍t/踹
糊靛油湍随瑞调魂踹j踹福湍帮楹
漓椅"怖跪而防蹑温溜"涓踹
淌麻温战需魏瑞懿提粽牒…那掷
PUbliShed as a ConferenCe PaPersICLR 2022
Published as a conference paper at ICLR 2022
F Enlarged figures for all 30 language directions (aggregate results
from all runs)
number of bits
Figure 4: CHAR: character models
24
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 4: CHAR: character models (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
25
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 5: CHAR with Pinyin for ZHtrg
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
26
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 5: CHAR with Pinyin for ZHtrg (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC

AR

EN

ES

FR

RU

ZH
27
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 6: CHAR with Wubi for ZHtrg
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
28
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 6: CHAR with Wubi for ZHtrg (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC

AR

EN

ES

FR

RU

ZH
29
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
2500000 -
2000000 -
1500000-
1000000-
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
1e+03
number of lines
1e+05
Figure 7:	BYTE models with UTF-8 encoding
30
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 7: BYTE models with UTF-8 encoding (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
31
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
number of lines
Figure 8: BYTE with ARtrg & RUtrg optimized with code pages 1256 & 1251 (ARRUtrg)
32
Published as a conference paper at ICLR 2022
2000000 -
1500000-
1000000-
2000000 -
1500000-
1000000-
sl_q Jo -JeqEnU
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
1e+03	1e+05	1e+03	1e+05	1e+03	1e+05
number of lines
Figure 8:	BYTE with ARtrg & RUtrg optimized with code pages 1256 & 1251 (target language as
facet)
33
Published as a conference paper at ICLR 2022
sl_q Jo -JeqEnU
1800000-
1500000-
1200000-
900000 -
1e+03	1e+05
number of lines
Figure 9: BYTE with directions AR-RU & RU-AR optimized on both source and target sides
(ARRUsrc,trg)
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
34
Published as a conference paper at ICLR 2022
1500000-
1000000-
1500000-
1000000-
1e+03	1e+05	1e+03	1e+05	1e+03	1e+05
number of lines
sl_q Jo -JeqEnU
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
Figure 9:	BYTE with directions AR-RU & RU-AR optimized on both source and target sides (target
language as facet)
35
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 10: WORD models
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
36
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 10: WORD models (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
37
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 11: BPE models
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
38
Published as a conference paper at ICLR 2022
sl_q jo -JeqEnU
Figure 11: BPE models (target language as facet)
TRG
AR
EN
ES
FR
RU
ZH
SRC
AR
EN
ES
FR
RU
ZH
39
Published as a conference paper at ICLR 2022
Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Condi-
tional Language Modeling
Version 1.1
ICLR 2022 camera-ready copy (20220510)
40