Published as a conference paper at ICLR 2022
EE-Net:	Exploitation-Exploration Neural
Networks in Contextual Bandits
Yikun Ban, Yuchen Yan, Arindam Banerjee, Jingrui He
University of Illinois at Urbana-Champaign
{yikunb2, yucheny5, arindamb, jingrui}@illinois.edu
Ab stract
In this paper, we propose a novel neural exploration strategy in contextual bandits,
EE-Net, distinct from the standard UCB-based and TS-based approaches. Contex-
tual multi-armed bandits have been studied for decades with various applications.
To solve the exploitation-exploration tradeoff in bandits, there are three main tech-
niques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound
(UCB). In recent literature, linear contextual bandits have adopted ridge regression
to estimate the reward function and combine it with TS or UCB strategies for
exploration. However, this line of works explicitly assumes the reward is based on
a linear function of arm vectors, which may not be true in real-world datasets. To
overcome this challenge, a series of neural bandit algorithms have been proposed,
where a neural network is used to learn the underlying reward function and TS or
UCB are adapted for exploration. Instead of calculating a large-deviation based
statistical bound for exploration like previous methods, we propose "EE-Net", a
novel neural-based exploration strategy. In addition to using a neural network
(Exploitation network) to learn the reward function, EE-Net uses another neural
network (Exploration network) to adaptively learn potential gains compared to the
currently estimated reward for exploration. Then, a decision-maker is constructed
to combine the outputs from the Exploitation and Exploration networks. We prove
that EE-Net can achieve O(√TTogT) regret and show that EE-Net outperforms
existing linear and neural contextual bandit baselines on real-world datasets.
1	Introduction
The stochastic contextual multi-armed bandit (MAB) (Lattimore and Szepesvdri, 2020) has been
studied for decades in machine learning community to solve sequential decision making, with
applications in online advertising (Li et al., 2010), personal recommendation (Wu et al., 2016; Ban
and He, 2021b), etc. In the standard contextual bandit setting, a set of n arms are presented to a
learner in each round, where each arm is represented by a context vector. Then by certain strategy,
the learner selects and plays one arm, receiving a reward. The goal of this problem is to maximize
the cumulative rewards of T rounds.
MAB algorithms have principled approaches to address the trade-off between Exploitation and
Exploration (EE), as the collected data from past rounds should be exploited to get good rewards but
also under-explored arms need to be explored with the hope of getting even better rewards. The most
widely-used approaches for EE trade-off can be classified into three main techniques: Epsilon-greedy
(Langford and Zhang, 2008), Thompson Sampling (TS) (Thompson, 1933), and Upper Confidence
Bound (UCB) (Auer, 2002; Ban and He, 2020).
Linear bandits (Li et al., 2010; Dani et al., 2008; Abbasi-Yadkori et al., 2011), where the reward is
assumed to be a linear function with respect to arm vectors, have been well studied and succeeded
both empirically and theoretically. Given an arm, ridge regression is usually adapted to estimate its
reward based on collected data from past rounds. UCB-based algorithms (Li et al., 2010; Chu et al.,
2011; Wu et al., 2016; Ban and He, 2021b) calculate an upper bound for the confidence ellipsoid
of estimated reward and determine the arm according to the sum of estimated reward and UCB.
TS-based algorithms (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017) formulate each arm as
a posterior distribution where mean is the estimated reward and choose the one with the maximal
1
Published as a conference paper at ICLR 2022
!"Wi;'")-
Case 1: Upward Exploration
Expected Reward ∙ h(!)
Estimated Reward• &!(!；(!)
Estimated Reward
!"(Vfi;'")-
d∙
!
&!（!；（!）
Expected Reward ∙ h(!)
Case 2: Downward Exploration
Adaptive Exploitation-Exploration: ! + f"
Figure 1: Left figure: Structure of EE-Net. In the right figure, Case 1: "Upward" exploration should
be made when the learner underestimates the reward; Case 2: "Downward" exploration should
be chosen when the learner overestimates the reward. EE-Net has the ability to adaptively make
exploration according to different cases. In contrast, UCB-based strategy will always make upward
exploration, and TS-based strategy will randomly choose upward or downward exploration.
sampled reward. However, the linear assumption regarding the reward may not be true in real-world
applications (Valko et al., 2013).
To learn non-linear reward functions, recent works have utilized deep neural networks to learn the
underlying reward functions, thanks to its powerful representation ability. Considering the past
selected arms and received rewards as training samples, a neural network f1 is built for exploitation.
Zhou et al. (2020) computes a gradient-based upper confidence bound with respect to f1 and uses
UCB strategy to select arms. Zhang et al. (2021) formulates each arm as a normal distribution where
the mean is f1 and deviation is calculated based on gradient of f1, and then uses the TS strategy to
choose arms. Both Zhou et al. (2020) and Zhang et al. (2021) achieve the near-optimal regret bound
of O(√T log T).
In this paper, we propose a novel neural exploration strategy, named "EE-Net". Similar to other
neural bandits, EE-Net has another exploitation network f1 to estimate rewards for each arm. The
crucial difference from existing works is that EE-Net has an exploration network f2 to predict the
potential gain for each arm compared to current reward estimate. The input to the exploration network
is the gradient of f1 and the ground-truth is residual difference between the true received reward
and the estimated reward from f1 . The strategy is inspired by recent advances in the neural UCB
strategies (Zhou et al., 2020; Ban et al., 2021). Finally, a decision-maker f3 is constructed to select
arms. f3 has two modes: linear or nonlinear. In linear mode, f3 is a linear combination of f1 and f2,
inspired by the UCB strategy. In the nonlinear mode, f3 is formulated as a neural network with input
(f1, f2) and the goal is to learn the probability of being an optimal arm for each arm. Figure 1 depicts
the workflow of EE-Net and its advantages for exploration compared to UCB or TS-based methods
(see more details in Appendix D). To sum up, the contributions of this paper can be summarized as
follows:
1.	We propose a novel neural exploration strategy, EE-Net, where another neural network is
assigned to learn the potential gain compared to the current reward estimate.
2.	Under standard assumptions of over-parameterized neural networks, we prove that EE-Net
can achieve the regret upper bound of O(√T log T), which improves a multiplicative factor
of √log T and is independent of either input or effective dimension, compared to existing
state-of-the-art neural bandit algorithms.
3.	We conduct extensive experiments on four real-world datasets, showing that EE-Net outper-
forms baselines including linear and neural versions of -greedy, TS, and UCB.
Next, we discuss the problem definition in Sec.3, elaborate on the proposed EE-Net in Sec.4, and
present our theoretical analysis in Sec.5. In the end, we provide the empirical evaluation (Sec.6) and
conclusion.
2
Published as a conference paper at ICLR 2022
2	Related Work
Constrained Contextual bandits. The common constrain placed on the reward function is the linear
assumption, usually calculated by ridge regression (Li et al., 2010; Abbasi-Yadkori et al., 2011; Valko
et al., 2013; Dani et al., 2008). The linear UCB-based bandit algorithms (Abbasi-Yadkori et al.,
2011; Li et al., 2016) and the linear Thompson Sampling (Agrawal and Goyal, 2013; Abeille and
Lazaric, 2017) can achieve successful performance and the near-optimal regret bound of O(√T). To
break the linear assumption, Filippi et al. (2010) generalizes the reward function to a composition of
linear and non-linear functions and then adopt a UCB-based algorithm to deal with it; Bubeck et al.
(2011) imposes the Lipschitz property on reward metric space and constructs a hierarchical optimistic
optimization to make selections; Valko et al. (2013) embeds the reward function into Reproducing
Kernel Hilbert Space and proposes the kernelized TS/UCB bandit algorithms.
Neural Bandits. To learn non-linear reward functions, deep neural networks have been adapted to
bandits with various variants. Riquelme et al. (2018); Lu and Van Roy (2017) build L-layer DNN to
learn the arm embeddings and apply Thompson Sampling on the last layer for exploration. Zhou et al.
(2020) first introduces a provable neural-based contextual bandit algorithm with a UCB exploration
strategy and then Zhang et al. (2021) extends the neural network to Thompson sampling framework.
Their regret analysis is built on recent advances on the convergence theory in over-parameterized
neural networks(Du et al., 2019; Allen-Zhu et al., 2019) and utilizes Neural Tangent Kernel (Jacot
et al., 2018; Arora et al., 2019) to construct connections with linear contextual bandits (Abbasi-
Yadkori et al., 2011). Ban and He (2021a) further adopts convolutional neural networks with UCB
exploration aiming for visual-aware applications. Xu et al. (2020) performs UCB-based exploration
on the last layer of neural networks to reduce the computational cost brought by gradient-based UCB.
Different from the above existing works, EE-Net keeps the powerful representation ability of neural
networks to learn reward function and first assigns another neural network to determine exploration.
3	Problem definition
We consider the standard contextual multi-armed bandit with the known number of rounds T (Zhou
et al., 2020; Zhang et al., 2021). In each round t ∈ [T], where the sequence [T] = [1, 2, . . . ,T],
the learner is presented with n arms, Xt = {xt,1, . . . , xt,n}, in which each arm is represented by a
feature vector xt,i ∈ Rd for each i ∈ [n]. After playing one arm xt,i, its reward rt,i is assumed to be
generated by the function:
rt,i = h(xt,i) + ηt,i ,	(3.1)
where the unknown reward function h(xt,i) can be either linear or non-linear and the noise ηt,i is
drawn from certain distribution with expectation E[ηt,i] = 0. Following many existing works (Zhou
et al., 2020; Ban et al., 2021; Zhang et al., 2021), we consider bounded rewards, rt,i ∈ [a, b]. For the
brevity, we denote the selected arm in round t by xt and the reward received in t by rt . The pseudo
regret ofT rounds is defined as:
T
RT = E Xa-rt) ,	(3.2)
t=1
where E[rJ= | Xt] = maxi∈[n] h(xt,i) is the maximal expected reward in the round t. The goal of this
problem is to minimize RT by certain selection strategy.
Notation. We denote by {xi}it=1 the sequence (x1, . . . , xt). We use kvk2 to denote the Euclidean
norm for a vector v , and kWk2 and kWkF to denote the spectral and Frobenius norm for a matrix
W. We use〈•，•〉to denote the standard inner product between two vectors or two matrices. We may
use Oθ1 f1(xt,i) or Oθ1 f1 to represent the gradient Oθ1 f1(xt,i; θt1) for brevity. We use {xτ, rτ}tτ =1
to represent the collected data up to round t.
4	Proposed Method: EE-Net
EE-Net is composed of three components. The first component is the exploitation network, fι(∙; θ1),
which focuses on learning the unknown reward function h based on the data collected in past rounds.
3
Published as a conference paper at ICLR 2022
Table 1: Structure of EE-Net (Round t).
Input	Network	Label
{xτ}tτ =1	fι(∙;θ1) (Exploitation)	{rτ}tτ =1
{Oθ1	f1(xτ; θτ1-1)}tτ =1 τ-1	f2(∙;θ2) (Exploration)	{(rτ - f1(xτ； θT-1))}： =1
{(f1(xτ; θτ1-1), f2(Oθτ1-1f1; θτ2-1))}tτ=1	f3(∙; θ3)	(Decision- maker with non-linear function)	{pτ}tτ =1
The second component is the exploration network, f2(∙; θ2), which focuses on characterizing the level
of exploration needed for each arm in the present round. The third component is the decision-maker,
f3, which focuses on suitably combining the outputs of the exploitation and exploration networks
leading to the arm selection.
1)	Exploitation Net. The exploitation net f1 is a neural network which learns the mapping from
arms to rewards. In round t, denote the network by fι(∙; θ1-J, where the superscript of θ1-ι is the
index of network and the subscript represents the round where the parameters of f1 finished the last
update. Given an arm xt,i, i ∈ [n], f1(xt,i; θt1-1) is considered the "exploitation score" for xt,i. By
some criterion, after playing arm xt, we receive a reward rt. Therefore, we can conduct gradient
descent to update θ1 based on the collected training samples {xτ , rτ }tτ=1 and denote the updated
parameters by θt1 .
2)	Exploration Net. Our exploration strategy is inspired by existing UCB-based neural bandits
(Zhou et al., 2020; Ban et al., 2021). Based on the Lemma 5.2 in (Ban et al., 2021), given an arm xt,i,
with probability at least 1 - δ, we have the following UCB form:
Ih(XtG- f1 (Xt,i； θ1-1)1 ≤ ψ(Oθ1-1f1 (Xt,i； θt-1)),	(4.1)
where h is defined in Eq. (3.1) and Ψ is an upper confidence bound represented by a function with
respect to the gradient Oθ1 f1 (see more details and discussions in Appendix D). Then we have the
following definition.
Definition 4.1. In round t, given an arm Xt,i, we define h(Xt,i) - f1(Xt,i; θt1-1) as the "expected
potential gain"for Xt,i and rt,i - f1(Xt,i; θt1-1) as the "potential gain"for Xt,i.
Let yt,i = rt,i - f1(Xt,i; θt1-1). When yt,i > 0, the arm Xt,i has positive potential gain compared to
the estimated reward f1(Xt,i; θt1-1). A large positive yt,i makes the arm more suitable for exploration,
whereas a small (or negative) yt,i makes the arm unsuitable for exploration. Recall that traditional
approaches such as UCB intend to estimate such potential gain yt,i using standard tools, e.g., Markov
inequality, Hoeffding bounds, etc., from large deviation bounds.
Instead of calculating a large-deviation based statistical bound for yt,i, we use a neural network
f2(∙; θ2) to represent Ψ, where the input is Oθi ɪ fι(xt,i) and the ground truth is rt,i - fι (xt,i; θ1-1).
Adopting gradient Oθ1 f1(Xt,i) as the input also is due to the fact that it incorporates two aspects of
information: the feature of the arm and the discriminative information of f1 .
Moreover, in the upper bound of NeuralUCB or the variance of NeuralTS, there is a recursive term
At-ι = I + PT=I Vθ^ ιfι(xτ )Vθ^ ιfι(xτ )> which is a function of past gradients up to (t - 1)
and incorporates relevant historical information. On the contrary, in EE-Net, the recursive term
which depends on past gradients is θt2-1 in the exploration network f2 because we have conducted
gradient descent for θt2-1 based on {Vθ1 f1(xτ)}tτ-=11. Therefore, this form θt2-1 is similar to At-1
in neuralUCB/TS, but EE-net does not (need to) make a specific assumption about the functional
form of past gradients, and is also more memory-efficient.
To sum up, in round t, we consider f2 (Oθ1 f1(xt,i); θt2-1) as the "exploration score" of xt,i,
because it indicates the potential gain of xt,i compared to our current exploitation score f1(xt,i; θt1-1).
Therefore, after receiving the reward rt, we can use gradient descent to update θ2 based on collected
training samples {Oθ1 f1(xτ), rτ - f1(xτ; θτ1-1}tτ=1. We also provide other two heuristic forms
for f2s ground-truth label: ∣rt,i - fι(xt,i; θ1-ι)∣ and ReLU(rt,i - fι(xt,i; θ1-ι)). We compare
them in an ablation study in Appendix B.
4
Published as a conference paper at ICLR 2022
Algorithm 1 EE-Net
Input: f1, f2, f3, T (number of rounds), η1 (learning rate for f1), η2 (learning rate for f2), η3
1: 2: 3: 4: 5: 6: 7: 8: 9:	(learning rate for f3), K1 (number of iterations for f1), K2 (number of iterations for f2) , K3 (number of iterations for f3), φ (normalization operator) 123 Initialize θ0 , θ0 , θ0 ; θ0 = θ0 , θ0 = θ0 , θ0 = θ0 for t = 1, 2, . . . , T do Observe n arms {xt,1 , . . . , xt,n} for each i ∈ [n] do Compute f1(xt,i; θt1-1), f2(φ(Oθt1-1 f1(xt,i)); θt2-1), f3((f1, f2); θt3-1) end for xt = arg maxxt,i,i∈[n] f3 f1(xt,i; θt1-1), f2(φ(Oθt1-1 f1(xt,i)); θt2-1); θt3-1 Play xt and observe reward rt θt1, θt2, θt3 = GRADIENTDESCENT(θ0, {xτ}tτ=1, {rτ}tτ=1)
10:	end for
11: 12:	procedure GRADIENTDESCENT(θ0, {xτ}tτ=1, {rτ}tτ=1)
13:	Li = 1 PT =1 (fι(xτ； θ1)-rτ)2
14:	θ1,(0) = θ10
15:	for k ∈ {1, . . . , K1} do
16:	θ1,(k) = θ1,(k-1) - η1Oθ1,(k-1)L1
17:	end for
18:	θbt1 = θ1,(K1)
19:	L2 = 1 PT =1 (f2(Φ(θeτ 1 fι(xτ))； a?) - (rτ - fι(xτ； θT-i)))2
20:	θ2,(0) = θ20
21:	for k ∈ {1, . . . , K2} do
22:	θ2,(k) = θ2,(k-1) -η2Oθ2,(k-1)L2
23:	end for
24:	θbt2 = θ2,(K2)
25:	Determine label pt
26:	L3 = - 1 Pt=i [Pt log f3((f1, f2 )； θ3) + (I- Pt)Iog(I- f3((f1,f2); θ3川
27:	θ3,(0) = θ30
28:	for k ∈ {1, . . . , K3} do
29:	θ3,(k) = θ3,(k-1) -η3Oθ3,(k-1)L3
30:	end for
31: 32:	θbt3 = θ3,(K3) 12	12	12 Randomly choose (θt1, θt2) uniformly from {(bθ0, θb0), (θb1, θb1), . . . , (θbt , bθt )}
33:	3	33	3 Randomly choose θt3 uniformly from {θ0, θ1, . . . , θt }
34:	Return θt1, θt2, θt3
35:	end procedure
3)	Decision-maker. In round t, given an arm xt,i , i ∈ [n], with the computed exploitation score
f1 (xt,i; θt1-1) and exploration score f2 (Oθ1 f1 ; θt2-1), we use a function f3 f1 , f2; θ3 to trade
off between exploitation and exploration and compute the final score for xt,i. The selection criterion
is defined as
xt = arg max f3 f1(xt,i; θt1-1), f2 Oθ1 f1(xt,i); θt2-1 ; θt3-1 .
xt,i,i∈[n]	t-1
Note that f3 can be either linear or non-linear functions. We provide the following two forms.
(1)	Linear function. f3 can be formulated as a linear function with respect to f1 and f2 :
f3(f1, f2; θ3) = w1f1(xt,i; θ1) + w2f2(Oθ1f1; θ2)
5
Published as a conference paper at ICLR 2022
where w1 , w2 are two weights preset by the learner. When w1 = w2 = 1, f3 can be thought of as
UCB-type policy, where the estimated reward f1 and potential gain f2 are simply added together. In
experiments, we report its empirical performance in ablation study (Appendix B).
(2)	Non-linear function. f3 also can be formulated as a neural network to learn the mapping from
(f1, f2) to the optimal arm. We transform the bandit problem into a binary classification problem.
Given an arm xt,i, we define pt,i as the probability of being the optimal arm for xt,i in round t. For
brevity, we denote by pt the probability of being the optimal arm for the selected arm xt in round t.
According to different reward distributions, we have different approaches to determine pt .
1.	Binary reward. ∀t ∈ [T], suppose rt is a binary variable over a, b(a < b), it is straightfor-
ward to set: pt = 1.0 if rt = b; pt = 0.0, otherwise.
2.	Continuous reward. ∀t ∈ [T], suppose rt is a continuous variable over the range [a, b], we
provide two ways to determine pt. (1) Pt can be directly set as rb--a .(2) The learner can set
a threshold θ, (a < θ < b). Then pt = 1.0 if rt > θ; pt = 0.0, otherwise.
Therefore, with the collected training samples nf1 (xτ; θτ1-1), f2(Oθ1 f1; θτ2-1) ,pτo	in
round t, we can conduct gradient descent to update parameters of f3(∙; θ3).
Table 1 details the working structure of EE-Net. Algorithm 1 depicts the workflow of EE-Net, where
the input of f2 is normalized, i.e., φ(Oθ1 f1(xt,i)). Algorithm 1 provides a version of gradient
descent (GD) to update EE-Net, where drawing (θt1 , θt2 ) uniformly from their stored historical
parameters is for the sake of analysis. One can easily extend EE-Net to stochastic GD to update the
parameters incrementally.
Remark 4.1 (Network structure). The networks f1 , f2 , f3 can be different structures according
to different applications. For example, in the vision tasks, f1 can be set up as convolutional layers
(LeCun et al., 1995). For the exploration network f2, the input Oθ1 f1 may have exploding dimensions
when the exploitation network f1 becomes wide and deep, which may cause huge computation cost
for f2. To address this challenge, we can apply dimensionality reduction techniques to obtain low-
dimensional vectors of Oθ1 f1. In the experiments, we use Roweis and Saul (2000) to acquire a
10-dimensional vector for O®i f 1 and achieve the best performance among all baselines.	□
Remark 4.2 (Exploration direction). EE-Net has the ability to determine exploration direction.
Given an arm xt,i, when the estimation f1(xt,i) is lower than the expected reward h(xt,i), the learner
should make the "upward" exploration, i.e., increase the chance of xt,i being explored; When f1(xt,i)
is higher than h(xt,i), the learner should do the "downward" exploration, i.e., decrease the chance
of xt,i being explored. EE-Net uses the neural network f2 to learn h(xt,i) - f1(xt,i) (which has
positive and negative scores) and has the ability to determine the exploration direction. In contrast,
NeuralUCB will always make "upward" exploration and NeuralTS will randomly choose between
"upward" exploration and "downward" exploration (see selection criteria in Table 2 and more details
in Appendix D).	□
Remark 4.3 (Space complexity). NeuralUCB and NeuralTS have to maintain the gradient outer
product matrix (e.g., At = Ptτ=1 Oθ1 f1(xτ; θτ1 )Oθ1 f1(xτ; θτ1)> ∈ Rp×p) and, for θ1 ∈ Rp, have
a space complexity of O(p2) to store the outer product. On the contrary, EE-Net does not have this
matrix and only regards Oθ1 f1 as the input of f2. Thus, EE-Net reduces the space complexity from
O(p2) to O(p).	□
5 Regret Analysis
In this section, we provide the regret analysis of EE-Net when f3 is set as the linear function
f3 = f1+f2, which can be thought ofas the UCB-type trade-off between exploitation and exploration.
For the sake of simplicity, we conduct the regret analysis on some unknown but fixed data distribution
D. In each round t, n samples {(xt,1, rt,1), (xt,2, rt,2), . . . , (xt,n, rt,n)} are drawn i.i.d. from D.
This is standard distribution assumption in over-parameterized neural networks (Cao and Gu, 2019).
Then, for the analysis, we have the following assumption, which is a standard input assumption in
neural bandits and over-parameterized neural networks(Zhou et al., 2020; Allen-Zhu et al., 2019).
6
Published as a conference paper at ICLR 2022
Assumption 5.1 (ρ-Separability). For any t ∈ [T], i ∈ [n], kxt,ik2 = 1, and rt,i ∈ [0, 1]. Then, for
every pairxt,i,xt0,i0, t0 ∈ [T],i0 ∈ [k], and (t, i) 6= (t0, i0), kxt,i - xt0,i0 k2 > ρ, and suppose there
exists an operator such that kφ(∙)k2 = 1 and ∣∣φ(θθi fι(xt,i)) - Φ(θθi f1(xt0,i0))k2 ≥ P
For example, the operator can be designed as φ(θθi fι^xt,i)) = (√2OθIf(Xx,"斗,x√i). The
analysis will focus on over-parameterized neural networks (Jacot et al., 2018; Du et al., 2019; Allen-
Zhu et al., 2019). Given an input x ∈ Rd, without loss of generality, we define the fully-connected
network f with depth L ≥ 2 and width m:
f(x; θ) = WLσ(WL-1σ(WL-2... σ(W1x)))	(5.1)
where σ is the ReLU activation function, W1 ∈ Rm×d, Wl ∈ Rm×m, for 2 ≤ l ≤ L - 1,
WL ∈ R1×m, and θ = [vec(W1)|,vec(W2)|,...,vec(WL)|]|.
Initialization. For any l ∈ [L - 1], each entry of Wl is drawn from the normal distribution N(0, )
and WL is drawn from the normal distribution N(0,*).Note that EE-Net at most has three networks
f1, f2, f3. We define them following the definition of f for brevity, although they may have different
depth or width. Then, we have the following theorem for EE-Net. Recall that η1 , η2 are the learning
rates for f1, f2; K1 is the number of iterations of gradient descent for f1 in each round; and K2 is the
number of iterations for f2 .
Theorem 1. Let f1, f2 follow the setting of f (Eq. (5.1) ) with the same width m and depth L. Let
L1, L2 be loss functions defined in Algorithm 1. Set f3 as f3 = f1 + f2. For any δ ∈ (0, 1), ∈
(0,O(T)],P ∈ (0, O(L)], suppose
m ≥ Ω (poly(T, n, L, ρ-1) ∙ log(1∕δ) ∙ e√log(Tn㈤)),
η1 = η2 = min (θ (√fem) , θ IPOly(TPn,L) ∙ m )),	⑸幻
Ki =」(吗r∙ log(「)).
Then, with probability at least 1 - δ over the initialization, the pseudo regret of EE-Net in T rounds
satisfies
RT ≤ O(1) + (2Vt - 1)3√2O(L) + O
((2√T -
(5.3)
Comparison with existing works. Under the similar assumptions in over-parameterized neural
networks, the regret bounds complexity of NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang
et al., 2021) both are
Rτ ≤ O (qdTQ)∙O (F), and J log;;+TnX)
where His the neural tangent kernel matrix (NTK) (Jacot et al., 2018; Arora et al., 2019) and λis a
regularization parameter. Similarly, in linear contextual bandits, Abbasi-Yadkori et al. (2011) achieve
O(d√Tlog T) and Li et al. (2017) achieve O(√dTlog T).
Remark 5.1. Compared to NeUralUCB/TS, EE-Net roughly improves by a multiplicative factor
of √log T, because our proof of EE-Net is directly built on recent advances in convergence theory
(Allen-Zhu et al., 2019) and generalization bound (Cao and Gu, 2019) of over-parameterized neural
networks. Instead, the analysis for NeuralUCB/TS contains three parts of approximation error by
calculating the distances between the expected reward and ridge regression, ridge regression and
NTK, and NTK and network function.	□
Remark 5.2. The regret bound of EE-Net does not have the effective dimension d or input dimension
d. d or d may cause significant error, when the determinant of H is extremely large or d > T.	□
The proof of Theorem 1 is in Appendix C and mainly based on the following generalization bound.
The bound results from an online-to-batch conversion while using convergence guarantees of deep
learning optimization.
7
Published as a conference paper at ICLR 2022
Lemma 5.1. For any δ ∈ (0,1), e ∈ (0,1), ρ ∈ (0, O(L)), suppose m, η1,η2, Ki, K2 satisfy the
conditions in Eq. (5.2) and (xτ,i, r*,i)〜D, ∀τ ∈ [t], i ∈ [n]. Let
xt = arg max
xt,i,i∈[n]
f2	φ(Oθt1-1fi(xt,i; θti-i)); θt2-i + fi(xt,i; θti-i) ,
and rt is the corresponding reward, given (xt,i, rt,i), i ∈ [n]. Then, with probability at least (1 - δ)
over the random of the initialization, it holds that
E	hf2 (O5: 1 fι(xt,i; θ1-i)); θ2-i) - (rt - fι(xt; θL))∣ | {xτ,r }T=l]
,i,rt,i),i∈[n]	-
≤ n+o( √t)+(1+2。产「,
(5.4)
where the expectation is also taken over (θti-i, θt2-i) that are uniformly drawn from (θbτ, θbτ), τ ∈
[t - 1].
Remark 5.3. Lemma 5.1 provides a fixed O( √^)-rate generalization bound for exploitation-
exploration networks fi, f2 in contrast with the relative bound w.r.t. the Neural Tangent Random
Feature (NTRF) benchmark (Cao and Gu, 2019). We achieve this by working in the regression rather
than classification setting and utilizing the convergence guarantees for square loss (Allen-Zhu et al.,
2019). Note that the bound in Lemma 5.1 holds in the setting of bounded (possibly random) rewards
r ∈ [0, 1] instead of a fixed function in the conventional classification setting.
6 Experiments
In this section, we evaluate EE-Net on four real-world datasets comparing with strong state-of-the-art
baselines. We first present the setup of experiments, then show regret comparison and report ablation
study. Codes are available at1.
We use four real-world datasets: Mnist, Yelp, Movielens, and Disin, the details and settings of
which are attached in Appendix A.
Figure 2: Regret comparison on Movielens and Yelp (mean of 10 runs with standard deviation
(shadow)). With the same exploitation network fi, EE-Net outperforms all baselines.
Baselines. To comprehensively evaluate EE-Net, we choose 3 neural-based bandit algorithms, one
linear and one kernelized bandit algorithms.
1.	LinUCB (Li et al., 2010) explicitly assumes the reward is a linear function of arm vector
and unknown user parameter and then applies the ridge regression and un upper confidence
bound to determine selected arm.
2.	KernelUCB (Valko et al., 2013) adopts a predefined kernel matrix on the reward space
combined with a UCB-based exploration strategy.
3.	Neural-Epsilon adapts the epsilon-greedy exploration strategy on exploitation network fi.
I.e., with probability 1 - e, the arm is selected by xt = arg maxi∈[n] fi(xt,i; θi) and with
probability e, the arm is chosen randomly.
IhttPS://github.com/banyikun/EE-Net-ICLR-2022
8
Published as a conference paper at ICLR 2022
Figure 3: Regret comparison on Mnist and Disin (mean of 10 runs with standard deviation (shadow)).
With the same exploitation network fι, EE-Net outperforms all baselines.
4. NeuralUCB (Zhou et al., 2020) uses the exploitation network fι to learn the reward function
coming with an UCB-based exploration strategy.
5. NeuralTS (Zhang et al., 2021) adopts the exploitation network fι to learn the reward function
coming with an Thompson Sampling exploration strategy.
Note that we do not report results of LinTS and KernelTS in experiments, because of the limited
space in figures, but LinTS and KernelTS have been significantly outperformed by NeuralTS (Zhang
et al., 2021).
Setup for EE-Net. To compare fairly, for all the neural-based methods including EE-Net, the
exploitation network f1 is built by a 2-layer fully-connected network with 100 width. For the
exploration network f2, we use a 2-layer fully-connected network with 100 width as well. For the
decision maker f3 , by comprehensively evaluate both linear and nonlinear functions, we found that
the most effective approach is combining them together, which we call " hybrid decision maker". In
detail, for rounds t ≤ 500, f3 is set as f3 = f2 + f1, and for t > 500, f3 is set as a neural network
with two 20-width fully-connected layers. Setting f3 in this way is because the linear decision maker
can maintain stable performance in each running (robustness) and the non-linear decision maker can
further improve the performance (see details in Appendix B). The hybrid decision maker can combine
these two advantages together. The configurations of all methods are attached in Appendix A.
Results. Figure 2 and Figure 3 show the regret comparison on these four datasets. EE-Net consistently
outperforms all baselines across all datasets. For LinUCB and KernelUCN, the simple linear reward
function or predefined kernel cannot properly formulate ground-truth reward function existed in
real-world datasets. In particular, on Mnist and Disin datasets, the correlations between rewards
and arm feature vectors are not linear or some simple mappings. Thus, LinUCB and KernelUCB
barely exploit the past collected data samples and fail to select correct arms. For neural-based
bandit algorithms, the exploration probability of Neural-Epsilon is fixed and difficult to be adjustable.
Thus it is usually hard to make effective exploration. To make exploration, NeuralUCB statistically
calculates a gradient-based upper confidence bound and NeuralTS draws each arm’s predicted reward
from a normal distribution where the standard deviation is computed by gradient. However, the
confidence bound or standard deviation they calculated only consider the worst cases and thus
may not be able represent the actual potential of each arm, and they cannot make "upward" and
"downward" exploration properly. Instead, EE-Net uses a neural network f2 to learn each arm’s
potential by neural network’s powerful representation ability. Therefore, EE-Net can outperform
these two state-of-the-art bandit algorithms. Note that NeuralUCB/TS does need two parameters to
tune UCB/TS according to different scenarios while EE-Net only needs to set up a neural network
and automatically learns it.
Ablation Study. In Appendix B, we conduct ablation study regarding the label function y of f2 and
the different setting of f3.
7 Conclusion
In this paper, we propose a novel exploration strategy, EE-Net. In addition to a neural network that
exploits collected data in past rounds , EE-Net has another neural network to learn the potential gain
compared to current estimation for exploration. Then, a decision maker is built to make selections
to further trade off between exploitation and exploration. We demonstrate that EE-Net outperforms
NeuralUCB and NeuralTS both theoretically and empirically, becoming the new state-of-the-art
exploration policy.
9
Published as a conference paper at ICLR 2022
Acknowledgements: We are grateful to Shiliang Zuo and Yunzhe Qi for the valuable discussions
in the revisions of EE-Net. This research work is supported by National Science Foundation
under Awards No. IIS-1947203, IIS-2002540, IIS-2137468, IIS-1908104, OAC-1934634, and DBI-
2021898, and a grant from C3.ai. The views and conclusions are those of the authors and should not
be interpreted as representing the official policies of the funding agencies or the government.
References
Y. Abbasi-Yadkori, D. Pdl, and C SzePesvðri. Improved algorithms for linear stochastic bandits. In
Advances in Neural Information Processing Systems, pages 2312-2320, 2011.
M. Abeille and A. Lazaric. Linear thompson sampling revisited. In Artificial Intelligence and
Statistics, pages 176-184. PMLR, 2017.
S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In
International Conference on Machine Learning, pages 127-135. PMLR, 2013.
H. Ahmed, I. Traore, and S. Saad. Detecting opinion spams and fake news using text classification.
Security and Privacy, 1(1):e9, 2018.
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In International Conference on Machine Learning, pages 242-252. PMLR, 2019.
S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with
an infinitely wide neural net. In Advances in Neural Information Processing Systems, pages
8141-8150, 2019.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397-422, 2002.
Y. Ban and J. He. Generic outlier detection in multi-armed bandit. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 913-923,
2020.
Y. Ban and J. He. Convolutional neural bandit: Provable algorithm for visual-aware advertising.
arXiv preprint arXiv:2107.07438, 2021a.
Y. Ban and J. He. Local clustering in contextual multi-armed bandits. In Proceedings of the Web
Conference 2021, pages 2335-2346, 2021b.
Y. Ban, J. He, and C. B. Cook. Multi-facet contextual bandits: A neural network perspective. In
The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,
Singapore, August 14-18, 2021, pages 35-45, 2021.
S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvdri. X-armed bandits. Journal of Machine Learning
Research, 12(5), 2011.
Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in Neural Information Processing Systems, 32:10836-10846, 2019.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. Advances in neural information processing systems, 14, 2001.
E. Chlebus. An approximate formula for a partial sum of the divergent p-series. Applied Mathematics
Letters, 22(5):732-737, 2009.
W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
pages 208-214, 2011.
V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. 2008.
S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. In International Conference on Machine Learning, pages 1675-1685. PMLR, 2019.
10
Published as a conference paper at ICLR 2022
S. Filippi, O. Cappe, A. Garivier, and C. SzePesv虹i. Parametric bandits: The generalized linear case.
In Advances in Neural Information Processing Systems, pages 586-594, 2010.
D. Fu and J. He. SDG: A simplified and dynamic graph neural network. In SIGIR ’21: The 44th
International ACM SIGIR Conference on Research and Development in Information Retrieval,
Virtual Event, Canada, July 11-15, 2021, pages 2273-2277. ACM, 2021.
F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Acm transactions on
interactive intelligent systems (tiis), 5(4):1-19, 2015.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in neural information processing systems, pages 8571-8580, 2018.
J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.
In Advances in neural information processing systems, pages 817-824, 2008.
T. Lattimore and C. SzePeSvðri. Bandit algorithms. Cambridge University Press, 2020.
Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The handbook
of brain theory and neural networks, 3361(10):1995, 1995.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the 19th international conference on World wide web,
pages 661-670, 2010.
L. Li, Y. Lu, and D. Zhou. Provably optimal algorithms for generalized linear contextual bandits. In
International Conference on Machine Learning, pages 2071-2080. PMLR, 2017.
S. Li, A. Karatzoglou, and C. Gentile. Collaborative filtering bandits. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information Retrieval,
pages 539-548, 2016.
X. Lu and B. Van Roy. Ensemble sampling. arXiv preprint arXiv:1705.07347, 2017.
C.	Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: An empirical comparison
of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127, 2018.
S.	T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. science,
290(5500):2323-2326, 2000.
W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of kernelised
contextual bandits. arXiv preprint arXiv:1309.6869, 2013.
Q. Wu, H. Wang, Q. Gu, and H. Wang. Contextual bandits in a collaborative environment. In
Proceedings of the 39th International ACM SIGIR conference on Research and Development in
Information Retrieval, pages 529-538, 2016.
P. Xu, Z. Wen, H. Zhao, and Q. Gu. Neural contextual bandits with deep representation and shallow
exploration. arXiv preprint arXiv:2012.01780, 2020.
W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International Conference on
Learning Representations, 2021.
D.	Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration. In International
Conference on Machine Learning, pages 11492-11502. PMLR, 2020.
11
Published as a conference paper at ICLR 2022
A	Datasets and Setup
MNIST dataset. MNIST is a well-known image dataset (LeCun et al., 1998) for the 10-class
classification problem. Following the evaluation setting of existing works (Valko et al., 2013; Zhou
et al., 2020; Zhang et al., 2021), we transform this classification problem into bandit problem.
Consider an image x ∈ Rd, we aim to classify it from 10 classes. First, in each round, the image
x is transformed into 10 arms and presented to the learner, represented by 10 vectors in sequence
x1 = (x, 0, . . . , 0), x2 = (0, x, . . . , 0), . . . , x10 = (0, 0, . . . , x) ∈ R10d. The reward is defined as 1
if the index of selected arm matches the index of x’s ground-truth class; Otherwise, the reward is 0.
Yelp2 and Movielens (Harper and Konstan, 2015) datasets. Yelp is a dataset released in the Yelp
dataset challenge, which consists of 4.7 million rating entries for 1.57 × 105 restaurants by 1.18
million users. MovieLens is a dataset consisting of 25 million ratings between 1.6 × 105 users
and 6 × 104 movies. We build the rating matrix by choosing the top 2000 users and top 10000
restaurants(movies) and use singular-value decomposition (SVD) to extract a 10-dimension feature
vector for each user and restaurant(movie). In these two datasets, the bandit algorithm is to choose the
restaurants(movies) with bad ratings. We generate the reward by using the restaurant(movie)’s gained
stars scored by the users. In each rating record, if the user scores a restaurant(movie) less than 2 stars
(5 stars totally), its reward is 1; Otherwise, its reward is 0. In each round, we set 10 arms as follows:
we randomly choose one with reward 1 and randomly pick the other 9 restaurants(movies) with 0
rewards; then, the representation of each arm is the concatenation of corresponding user feature
vector and restaurant(movie) feature vector.
Disin (Ahmed et al., 2018) dataset. Disin is a fake news dataset on kaggle3 including 12600 fake
news articles and 12600 truthful news articles, where each article is represented by the text. To
transform the text into vectors, we use the approach (Fu and He, 2021) to represent each article by a
300-dimension vector. Similarly, we form a 10-arm pool in each round, where 9 real news and 1 fake
news are randomly selected. If the fake news is selected, the reward is 1; Otherwise, the reward is 0.
Configurations. For LinUCB, following (Li et al., 2010), we do a grid search for the exploration
constant α over (0.01, 0.1, 1) which is to tune the scale of UCB. For KernelUCB (Valko et al., 2013),
we use the radial basis function kernel and stop adding contexts after 1000 rounds, following (Valko
et al., 2013; Zhou et al., 2020). For the regularization parameter λ and exploration parameter ν in
KernelUCB, we do the grid search for λ over (0.1, 1, 10) and for ν over (0.01, 0.1, 1). For NeuralUCB
and NeuralTS, following setting of (Zhou et al., 2020; Zhang et al., 2021), we use the exploiation
network f1 and conduct the grid search for the exploration parameter ν over (0.001, 0.01, 0.1, 1) and
for the regularization parameter λ over (0.01, 0.1, 1). For NeuralEpsilon, we use the same neural
network f1 and do the grid search for the exploration probability over (0.01, 0.1, 0.2). For the
neural bandits NeuralUCB/TS, following their setting, as they have expensive computation cost to
store and compute the whole gradient matrix, we use a diagonal matrix to make approximation. For
all neural networks, we conduct the grid search for learning rate over (0.01, 0.001, 0.0005, 0.0001).
For all grid-searched parameters, we choose the best of them for the comparison and report the
averaged results of 10 runs for all methods.
Create exploration samples for f2. When the selected arm is not optimal in a round, the optimal
arm must exist among the remaining arms, and thus the exploration consideration should be added
to the remaining arms. Based on this fact, we create additional samples for the exploration network
f2 in practice. For example, in setting of binary reward, e.g., 0 or 1 reward, if the received reward
rt = 0 while select xt, we add new train samples for f2, (xt,i, cr) for each i ∈ [i] ∩ xt,i 6= xt, where
cr ∈ (0, 1) usually is a small constant. This measure can further improve the performance of EE-Net
in our experiments.
B Ablation Study
In this section, we conduct ablation study regarding the label function y for exploration network f2
and seting of decision maker f3 on two representative datasets Movielens and Mnist.
2https://www.yelp.com/dataset
3https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset
12
Published as a conference paper at ICLR 2022
Movielens
Mmst
2000	4000	6000	8000	10000
Rounds
2000	4000	6000	8000	10000
Rounds
Figure 4: Ablation study on label function y for f2. EE-Net denotes yι = r - fι, EE-Net-abs
denotes y = |r - fι|, and EE-Net-ReLU denotes y3 = ReLU(r - fι). EE-Net shows the best
performance on these two datasets.
Movielens
2000	4000	6000	8000	10000
Rounds
Figure 5: Ablation study on decision maker f3. EE-Net-Lin denotes f3 = fι + f2, EE-Net-NoLin
denote the nonlinear one where f3 is a neural network (2 layer, width 20), EE-Net denotes the hybrid
one where f3 = f 1 + f2 if t ≤ 500 and f3 is the neural network if t > 500. EE-Net has the most
stable and best performance.
Mnist
2000	4000	6000	8000	10000
Rounds
Label function y . In this paper, we use y1 = r - f1 to measure the potential gain of an arm, as the
label of f2. Moreover, we provide other two intuitive form y2 = |r - f1 | and y3 = ReLU (r - f1).
Figure 4 shows the regret with different y, where "EE-Net" denotes our method with default y1 ,
"EE-Net-abs" represents the one with y2 and "EE-Net-ReLU" is with y3 . On Movielens and Mnist
datasets, EE-Net slightly outperforms EE-Net-abs and EE-Net-ReLU. In fact, y1 can effectively
represent the positive potential gain and negative potential gain, such that f2 intends to score the
arm with positive gain higher and score the arm with negative gain lower. However, y2 treats the
positive/negative potential gain evenly, weakening the discriminative ability. y3 can recognize the
positive gain while neglecting the difference of negative gain. Therefore, y1 usually is the most
effective one for empirical performance.
Setting of f3 . f3 can be set as an either linear function or non-linear function. In the experiment, we
test the simple linear function f3 = f1 + f2 , denoted by "EE-Net-Lin", and a non-linear function
represented by a 2-layer 20-width fully-connected neural network, denoted by "EE-Net-NoLin". For
the default hybrid setting, denoted by "EE-Net", when rounds t ≤ 500, f3 = f1 + f2 ; Otherwise, f3
is the neural network. Figure 5 reports the regret with these three different modes. EE-Net achieves
the best performance with small standard deviation. In contrast, EE-Net-NoLin obtains the worst
performance and largest standard deviation. However, notice that EE-Net-NoLin can achieve the
best performance in certain running (the green shallow) but it is erratic. Because in the begin phase,
without enough training samples, EE-Net-NoLin strongly relies on the quality of collected samples.
With appropriate training samples, gradient descent can lead f3 to global optimum. On the other hand,
with misleading training samples, gradient descent can deviate f3 from global optimum. Therefore,
EE-Net-NoLin shows very unstable performance. In contrast, EE-Net-Lin is inspired by the UCB
strategy, i.e., the exploitation plus the exploration, exhibiting stable performance. To combine their
13
Published as a conference paper at ICLR 2022
advantages together, we propose the hybrid approach, EE-Net, achieving the best performance with
strong stability.
C Proof of Theorem 1
In this section, we provide the proof of Theorem 1 and related lemmas.
Proof. For brevity, for the selected arm xt in round t, let h(xt) be its expected reward
and XJ= = argmaxχti,i∈[n] h(xt,i) be the optimal arm in round t. Let f3(x; θt-ι) =
f2 φ(Oθt1-1 f1(x; θt1-1)); θt2-1 + f1(x; θt1-1).
Note that (xt,i, rt,i)〜D, for each i ∈ [n]. Then, the expected regret of round t is given by
Rt = Ext,i,i∈[n] [h(xt=) - h(xt)]
= Ext,i,i∈[n] [h(xt=) - f3(xt) + f3(xt) -h(xt)]
≤ Eχt,i,i∈[n] [h(x;) — f3(X=) + f3(Xt) — f3(Xt), +f3(Xt) — h(Xt)]
'------------------}
I1
= Ext,i,i∈[n] [h(Xt=) —f3(Xt=)+f3(Xt) — h(Xt)]
= Ext,i,i∈[n][h(Xt=) — f3(Xt=; θt-1) + f3(Xt; θt-1) — h(Xt)]
(=a)Ext,i,i∈[n][h(Xt=) — f3(Xt=; θt=-1) + f3(Xt=; θt=-1) — f3(Xt=; θt-1) + f3(Xt; θt-1) — h(Xt)]
=Eχt,i,i∈[n] [h(X;) — f3(Xt； θ=-l)∣]+ Eχt,i,i∈[n] [f3(X=； θ=-l) — f3(Xt ； θt-l)]
+ Ext,i,i∈[n] [f3(Xt; θt-1) — h(Xt)]
≤ '	E、.,J∣f2 (φ(θθFfι(X= ； θ1-ι)); θ3) — (建 — fι(X=; θ1-υ)∣i
(xt,i,rt,i),i∈[n]	t-1
'----------------------------------------------------------------------------}
{z
I2
+ Eχt,i,i∈[n] h|f2 (φ(θθ1-*ιfl(x=; θt-ι)); θ2-ι) — f2 (φ(θθ1-1 fl(H； θ1-l)); θ2-l)∣i
'----------------------------------------------------------------------------------}
{Z
I3
+ Eχt,i,i∈[n] h|fl ㈤;θtl) — 九3；θL)∣i
X---------------------------------------------------/
∙•^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^≡""
I4
十 ,	E、	r 1 h∣f2 (Φ(θθl 1 fi(Xt； θ1-i)); θ2-i) -(rt-fi(Xt； θ1-i))∣i
(xt,i,rt,i),i∈[n]	t-1
'----------------------------------------------------------------------------------}
{z
I5
(C.1)
where I1 is because f3 (Xt) = maxi∈[n] f3(Xt,i) and f3 (Xt) - f3 (Xt= ) ≥ 0 and (a) introduces the
additional parameters θt=-1 = (θt1-,=1, θt2-,=1) which will be suitably chosen.
Because, for each i ∈ [n],(Xt,i, rt,i)〜 D, applying Lemma C.1 and Corollary C.1, with probability
at least (1 — δ) over the randomness of initialization, for I2, I5, we have
I2,I5 ≤ Π + O( √t )+(1 + 2ξ√W≡,
(C.2)
where
ξ =。⑴ + O () + O (心叫,3θg1166 m!	(C.3)
∖	pʌ/m	J ∖	ρ4/3 m1/6	I
and we apply the union bound over round τ, ∀τ ∈ [t] to make Lemma C.1 and Corollary C.1 hold for
each round τ, τ ∈ [t].
For I3 , I4, based on Lemma C.2, with probability at least 1 — δ, we have
M ≤ (1 + o (tRogH Oo
∖	∖ ρ1∕3m1∕t6 I I
logm)+O (t ：/；； 1/6m) :=ξι.(C⑷
14
Published as a conference paper at ICLR 2022
To sum up, with probability at least 1 - δ, we have
Rt ≤ 2 (r≡ + O( √t )+(1 + 2ξ≠W≡ + ξι
(C.5)
Then expected regret of T rounds is computed by
T
RT = XRt
t=1
≤ 2 X (Π + O( √t )+(1 + 2ξ√W≡ + ξj
≤ (2√T - 1)2√2^ + (2√T - 1)3√2O(L) + 2(1 + 2ξ)(2√T - 1)，2log(O(Tn)/S) +O(1)
'-----------------------------{----------------------------}
I2
=(2√T - 1)(2√2^ + 3√2O(L)) + 2(1 + 2ξ)(2√T - 1)p2∖og(O(Tn)∕δ) + O(I)
(C.6)
where I2 is because PT=I √t ≤ JT √1t dx + 1 = 2√T - 1 (Chlebus, 2009) and the bound of ξ1 is
due to the choice of m, i.e., since ξι = O(1∕m1/6) and m ≥ Ω(poly(T)), m can be chosen so that
Tξ1 = Oe(T/m1/6) ≤ O(1).
Then, when ≤ 1∕T, we have
RT ≤O(1)+ (2√T - 1)3√2O(L) + 2(1 + 2ξ)(2√T - 1)P2log(O(Tn)∕δ).	(C.7)
As the choice of m, we have ξ ≤ O(1). Therefore, we have
RT ≤ O⑴ + (2√T - 1)3√2O(L) + O ((2√T - 1),2log(O(Tn)∕δ)) .	(C.8)
The proof is completed.	□
LemmaC.1. [Lemma 5.1 restated] For any δ, e ∈ (0,1),ρ ∈ (0, O( L)), suppose m,η1,η2, Ki, K
satisfy the conditions in Eq. (5.2) and (xτ,i, 5i)〜D, ∀τ ∈ [t], i ∈ [n]. Let
xt = arg max hf2 φ(Oθt1-1 f1 (xt,i; θt-1 )); θt-1 + f1 (xt,i ; θt-1)i ,
and rt is the corresponding reward, given (xt,i, rt,i), i ∈ [n]. Then, with probability at least (1 - δ)
over the random of the initialization, it holds that
E	hf2 (φ(O毋 IfI(Xt； θ1-i)); θ2-i) -Wt- f1 (Xt； θ1-1))l | {xτ,rτ}T=1i
(xt,i,rt,i),i∈[n]	t-1
≤ r≡+o( √⅛ i+2,严平≡,
(C.9)
where the expectation is also taken over (θt1-1, θt2-1) that are uniformly drawn from (θbτ, θbτ), τ ∈
[t- 1].
Proof. In this proof, we consider the collected data ofup to round t - 1, {Xτ, rτ}tτ-=11, as the training
dataset and then obtain a generalization bound for it, inspired by Cao and Gu (2019).
For convenience, we use X := Xt,i, r := rt,i, noting that the same analysis holds for each i ∈ [n].
Consider the exploration network f2, applying Lemma C.3. With probability at least 1 - δ, for any
τ ∈ [t], we have
∣f2 Q(OII fi(x; bτ ))； bT )∣≤ ξ.	(C.10)
Similarly, applying Lemma C.3 again, with probability at least 1 - δ, for any t ∈ [T], we have
1
Ifi (x; θτ)1 ≤ ξ	(C.11)
15
Published as a conference paper at ICLR 2022
Because for any r 〜Dr, |r| ≤ 1, with Eq. (C.10) and (C.11), applying union bound, with probability
at least (1 - 2δ) over the random initialization, we have
f2 φ(Oθb1 f1(x; θbτ1)); θbτ2 - (r - f1(x; θbτ1)) ≤ 1+2ξ.	(C.12)
Noting that (C.12) is for x = xτ,i for a specific τ ∈ [t], i ∈ [n]. By union bound, (C.12)
2
holds ∀τ ∈ [t], i ∈ [n] with probability at least (1 - ntδ). For brevity, let f2(x; θτ) represent
f2 φ(Oθb1 f1(x; θbτ1)); θbτ2 .
12
Recall that, for each τ ∈ [t - 1], θτ and θτ are the parameters training on {xτ0 , rτ0 }ττ0=1 according
to Algorithm 1. In round τ ∈ [t], let xτ = arg maxxτ,i,i∈[n][f1(xτ,i; θτ1-1) + f2(xτ,i; θτ2-1)], given
(xτ,i, rr,i)〜 D,i ∈ [n]. Let r「be the corresponding reward. Let (x", r；,i)〜 D,i ∈ [n] be shadow
samples from the same distribution and let x0τ = arg maxx0 ,i∈[n] [f1(x0τ,i; θτ1-1) + f2(x0τ,i; θτ2-1)],
with rτ0 being the corresponding reward. Then, we define
Vτ:= (x0τ,i,rτ0E,i),i∈[n] hf2(x0τ; θbτ2-1) - rτ0 - f1(x0τ; θbτ1-1)i
-|f2(XT； bT-1)-卜T- f1(xτ； bT-1))l .
(C.13)
Then, as (x「,i, f)〜D,i ∈ [n], based on the definition of (x「r「), We have
EM∣Fτ-ι] = ,o	0E、r ι h∣f2 (XT； bT-ι) — WT- fι(xT； bT-ι))∣l Ft-J
(x0τ,i,rτ0 ,i),i∈[n]
-E	h∣f2 (xτ；bT-ι) - }τ-fι(xτ；bT-ι))∣∣Fτ-ι]	(C.14)
(xτ,i,rτ,i),i∈[n]
=0,
where FT-1 denotes the σ-algebra generated by the history HT-1 = {xT0, rT0}TT0-=11.
Moreover, we have
tt
7 X 咚=t X(X，，E)闫JJf2(xT； bτ-1) - (rT - fι(xT； bτ-i)) ∣i
t T=1	t T=1 (xτ,i,rτ,i),i∈[n]
t
-t X ∣f2 (xT； bT-l) - (rτ - f1(xτ； bT-1)) ∣
T=1
Since {VT}tT =1 is a martingale difference sequence, inspired by Lemma 1 in (Cesa-Bianchi et al.,
2001), applying the Hoeffding-Azuma inequality, with probability at least 1 - 3δ, we have
P
1X 咚-t X E[K |Ft] > ∣1+2ξ}d2⅛t1M I ≤ δ
τ=1	ιz⅜, F ∖
⇒	P t X Vt > (1 + 2ξ)J辿g* ≤ δ,
T=1
(C.15)
where I1 = 0 according to (C.14) and I2 is because of (C.12).
16
Published as a conference paper at ICLR 2022
According to Algorithm 1, (θt1-1, θt2-1) is uniformly drawn from {(θbτ1 , θbτ2)}tτ-=10. Thus, with proba-
bility 1 - 3δ, we have
E
(x0t,i ,rt0,i),i∈[n]
*%ι)f2 Gt; θ2-ι) Tr0-W; *讥
t
=t X E(Xf [f2 (XT； bT-ɔ - (rT - f1 (XT； bT-1)) ∣]
τ=1
≤ t X ∣f2 (XT； bT-1)- (rτ - fι(xτ; θT-ι))∣+(1 + 20产野.
T=1
'--------------------{--------------------}
I3
(C.16)
For I3, according to Lemma C.6, for any e satisfying ke? 一θ0 k2 ≤ O(ρ√3m log m), with probability
1 - δ, we have
t2	1
t X f2(Xt； θτ-1) - (rτ - f1(XT； θτ-1
T=1
t
≤ t Xlf2 (X
:T ； e2) - (rT - fl(XT ； bT-1 ))∣+O
(C.17)
I4
For I4, according to Lemma C.4 (1), these exists e satisfying ∣∣e2 - θ2∣∣2 ≤ O(ρ√3m log m), with
probability 1 - δ, such that
t
t Xf2(Xt； e ) - (rT - f1 (xt； θT-1
T=1
≤ t √t
t2
X(f2(Xt； e ) - (rτ - f1 (XT； θτ-1)))
T=1
(C.18)
{z
I5
1
≤√t
|
}
}
2
where I5 follows by a direct application of Lemma C.4 (1) by defining the loss L(θ )
2 PT=1 (f2(XT； e2) - (rT - f1 (XT； bT-1))) ≤ e.
Combining Eq.(C.16), Eq.(C.17) and Eq.(C.18), with probability (1 - 5δ) we have
.E.	. . .	[∣f2	(Xt；	θ2-J -(rt-	f1(Xt； θt-1 ))l	|{XT ,rT }T=11]
(xt,i,rt,i),i∈[n]
≤ ∖∣τ + O
+ (1 + 2ξ√2^0g≡
(C.19)
where the expectation over (θt1-1, θt2-1) that is uniformly drawn from {(θbT1,θbT2)}tT-=10.
Then, applying union bound to t, n and rescaling the δ complete the proof.
□
Corollary C.1. For any δ, e ∈ (0,1), ρ ∈ (0, O(L)), suppose m, η1, η2, K1,K2 satisfy the condi-
tions in Eq. (5.2) and (Xτ,i, rτ,i) ~ D, ∀τ ∈ [t], i ∈ [n]. Forany T ∈ [t], let
XT = arg max [h(Xτ,i)],
xτ,i,i∈[n]
17
Published as a conference paper at ICLR 2022
and rT is the corresponding reward, given (xτ,i, rτ,i), i ∈ [n]∙ Then, with probability at least (1 一 δ)
over the random of the initialization, there exist θ1-1, θ2-1, s.t., ∣∣θ1-1 — θ1∣∣2 ≤ O( -√m log m)
and ∣∣θ2-1 — θ0∣∣2 ≤ O(ρ√3m log m), Such that
E	E、r 1 h∣∕2	(φ(θθi,*fι(xt; θ1-t1)); θ2-tI)-G-	fι(xt;	θ1-1))∣	|	{xT ,rT }T=1ιi
(xt,i,rt,i),i∈[n]	t-1
≤ W + O(兽)+(1 + 2ξ)产平区,
(C.20)
where the expectation is also taken over (θ1-1, θ2-ι) that are uniformly drawnfrom (b「 , bT ), τ ∈
[t - 1].
Proof. This a direct corollary of Lemma C.1, given the optimal historical pairs {xT, "}；=；. For
^2,*	/	^1,*	^2,*∖
brevity, let f2(x; br ) represent f2 (0(031,* fι(x; br )); bτ )∙
1	1 ^1,*	^2,*	„	、
Suppose that, for each T ∈ [t — 1],。丁 and。丁 are the parameters training on {x；o, rTo }T o=ι
according to Algorithm 1. Note that these pairs {xTo, rMT，=1 are unknown to the algorithm We run,
^1,* ^2,*
and the parameters (θτ , θτ ) are not estimated. However, for the analysis, it is sufficient to show
that there exist such parameters so that the conditional expectation of the error can be bounded.
In round T ∈ [t], let XT = arg maxχτ,ii∈[n] [h(xτ,i)], given (x「,i, rT,i)〜D, i ∈ [n]. Let r； be the
corresponding reward. Let (XT i, r； i)〜D, i ∈ [n] be shadow samples from the same distribution
and let XT= = arg maxχ0 i,i∈[n] h(x；,i), with IrT being the corresponding reward. Then, we define
∣	^2 *	^1 *
VT:= 0 0E	∣∣f2(x0T=; θbT,-1) —	rT0= — f1(x0T=; θbT,-1) ∣∣
(x0t,i,rt0,i),i∈[n]
∣	2,*	1,*	∣
—∣f2(xT;θτ,-1) — TT- f1(x*;bτ'-1))∣ .
Then, as (x；,i, r；,i)〜D, i ∈ [n], we have
∣	2,*	1,*	∣
[VT|FT-1]= 0 0E	∣∣f2	x0T*;	θbT,-1 —	rT0*	—	f1(x0T*;	θbT,-1)	∣∣	|FT-1
(x0τ,i,rτ0 ,i),i∈[n]
∣	2 *	1*	∣
- E	∣f2 (xT ； bτ11) — (rT — fι(xT ； bτ--i))∣l Fτ-1
(xτ,i,rτ,i),i∈[n]
=0,
(C.21)
(C.22)
where FT -1 denotes the σ-algebra generated by the history {xT*0, rT*0}TT 0-=11.
Therefore, {VT}tT=1is a martingale difference sequence. Similarly, applying the Hoeffding-Azuma
inequality to VT , with probability 1 — 3δ, we have
EE
(x0t,i,rt0,i),i∈[n] (θt1-,*1,θt2-,*1)
h∣f2 (xt*; θ2-t1)—卜广一fι(xt*; θ1r1))∣i
t
X λ ip	h∣f (Y 0*∙b2,* )	("* f G 0*∙b1,*、) ∣i
t，E(XT,i,rT ,i),i∈[n] [f2 (xT； θT-1J - (rT - fl(xT ; θT-1)川
T=1
(C.23)
t
≤ t X∣f2 (x*; bT,-1) - (rT - fι(xT; θT-
T=1
X----------------------------------------
ι))∣+(1 + 2ξ)产斗δ)
{z
I3
}
18
Published as a conference paper at ICLR 2022
For I3, according to Lemma C.6, for any L satisfying ∣∣e2, * - θ0∣∣2 ≤ O(ρ√m logm) , with
probability 1 - δ, we have
t
ɪ XI f2(xT; E1)-g-f1(xT; bT':1))
τ =1
t
≤ t χ∣ f2(xτ ； e2,*)-卜T- f1 (xτ ； bT-1
τ =1
X-------------------------
+O
(C.24)
{z
I4
For I4, according to Lemma C.4 (1), these exists e， satisfying ∣∣e2, -。2心 ≤ O(ρ√m logm),
with probability 1 - δ, such that
1 t
1 ∑∣∕2 (xT; θ2,*)-B — ∕1X;织 1
t2
χ (f2(xτ； θ ,)-卜τ - f1(xτ bτ,-1)))
T =1
(C.25)
{z
I5
\\
≤
where I5 follows by a direct application of Lemma C.4 (1) by defining the loss L(e2'*) =
2 PT=1	(f2(xT；	e2，)-	卜*	-	f1(xT；	bT-1)))	≤ e.	Combining above inequalities, with proba-
bility (1 - 5δ) we have
E	h ∣ f2 (xt；熊-1) - S- f1(xt； ⅛Λ))∣∣{xT,rT}T=1]
(xt,i,rt,i),i∈[n] LI	'	/	∖	/ I	」
≤ ∖∣τ + o
+ (1 + 2ξ√^0gP
(C.26)
Then, applying union bound to t, n and rescaling the δ complete the proof.
Lemma C.2. Given δ, e ∈ (0,1), ρ ∈ (0,0(L)), suppose m, ηι, η2, K1, K satisfy the conditions
in Eq. (5.2). Then, with probability at least 1 一 δ, in each round t ∈ [T ] ,for any ∣∣x∣∣2
1, we have
(1)
If1(x；d 1) - f1 (x； θ1-1)∣
≤ (1 + O (1)) O
[t4，ioCt4L2 log11/6 m
lθg m) + O 的3加/6
(C.27)
≤1 √t
⑵
,
,
；
□
I /2 (φ3”1(x;胆 1))；对)-f2 (φ(θ*∕1(x;见 1))； *)∣
≤ (1+O C)卜
[t4，ioCt4L2 log11/6 m
lθg m) + O	ρ4∕3m1∕6
(C.28)
；
⑶
IweLIf1(x；见 1 )∣∣2,kθθ2-1f2(0(o*f1(x； BL))； θtτ) ∣∣2
≤ (1 + O (tL：/0g5；6m)) O(L).
∖	∖ ρ1∕3m1/6 I I
(C.29)
19
Published as a conference paper at ICLR 2022
Proof. AccordingtoLemmaC.4 (2), kb；-1 - θ1∣∣2 ≤ O(-√m log m),∀τ ∈ [t]. Thus, we have
kθ1-ι - θ11∣2 ≤ O(-√√m logm).
First, based on Triangle inequality, for any kxk2 = 1, we have
kOθt1-1 f1(x; θt1-1)k2 ≤ kOθ10 f1(x; θ01)k2 + kOθt1-1 f1(x; θt1-1) - Oθ01 f1(xi; θ01)k2
≤(1 + O (¾⅛!! O(L)	S)
∖	∖ ρ1∕3m1∕6	1 I
where the last inequality is because of Lemma C.4 (3) and Lemma C.7.
Applying Lemma C.5 (1), for any X 〜D, ∣∣x∣∣2 = 1 and kθ1-ι - θ1-ιk ≤ O(ρ√t3m log m) = w, We
have
∣fι(x; θ1-ι) - fι(x; Θ1-ι)∣
≤lhθθ1-1 fi(Xi； Θ1-ι), θ1-ι- Θ1-ιil + O(L2Pmlog(m))kθ}ι - θ1-ιk2w1/3
≤kθθ1-1 fi(Xi； θl-1)k2kθl-∖- θl-1k2 + O(L2Pmlog(m))kθ}ι - θ1-ιk2w1/3	(c∙31)
≤ (1 + O (tL31 ∣og5∕6 6m !! O( L√3= log m)+ O (t4L4 ∣og11∕6 m !
∖	∖ ρ1/3m1/6 I I ρ√m	∖ ρ4∕3m1∕6	)
Similarly, we can use the same way to prove the lemmas for f2 .
□
Lemma C.3. Let f (∙; θt) follow the stochastic gradient descent of fι or f2 in Algorithm 1. Suppose
m, η1, η2 satisfy the conditions in Eq. (5.2). With probability at least 1 - δ, for any x with kxk2 = 1
and t ∈ [T], it holds that
If (x； bt)∣≤o ⑴ + O(；)
tt4nL2 log11/6 m
十 ρ	ρ4∕3m1/6
Proof. Considering an inequality |a - b| ≤ c, we have |a| ≤ |b| + c. Let θ0 be randomly initialized.
Then applying Lemma C.5 (X), for any X 〜D, kx∣∣2 = 1 and ∣∣bt - θok ≤ w, We have
If (x； bt)| ≤ If (x； θo)l + ∣hθθo f (xi； θo), bt - θoi∣ + O(L2 pm log(m))kbt — θo∣2w1/3
≤ O⑴ + ∣∣Oθof (xi； Θ0)k2kbt - Θ0k2 +O(L2Pmlog(m))kbt - θo∣2w1/3
{z} |
I0
{z
I1
}
≤ O(1) + O(L) ∙O
{z
I2
log m) + O(L2 Pm log(m)) ∙ O
------'।--------------------
{z
I3
4∕3
log m
}
(t3L log m )	(t4L2 log11/6 m!
(ρ√m J +	( ρ4∕3m1/6 J
(C.32)
where: Io is based on the Lemma C.4 (3); I1 is an application of Cauchy-Schwarz inequality; I2 is
according to Lemma C.4 (2) and (3) in which θt can be considered as one step gradient descent; I3 is
due to Lemma C.4 (2).
Then, the proof is completed.
□
Lemma C.4. Given a constant 0 <	< 1, suppose m satisfies the conditions in Eq. (5.2), the
learning rate η = Ω( 口^ ^―二)团),the number ofiterations K = Ω( P°ly (-2n,L) ∙ log e-1). Then, with
probability at least 1 - δ, starting from random initialization θ0,
(1) (Theorem 1 in (Allen-Zhu et al., 2019)) In round t ∈ [T], given the collected data
{x；, r； }t=τ ,the loss function is defined as: L(θ) = 2 PT=1 (f (x； ； θ) — IrT )2. Then,
20
Published as a conference paper at ICLR 2022
there exists θ satisfying kθ - θ0 k2 ≤ O
K = Ω(Poypy ∙ log e-1) iterations;
log m , such that L(θe) ≤ in
(2)	(Theorem 1 in (Allen-Zhu et al., 2019)) For any k ∈ [K], it holds uniformly that kθt(k) -
θ0k2 ≤O(p√mlogm)；
(3)	Following the initialization, given kxk2 = 1, it holds that
kθθo f (x; Θ0)k2 ≤O(L), lf(x; Θ0)I≤O(I)
where θt(k) rePresents the Parameters of f after k ∈ [K] iterations of gradient descent in round t.
Proof. Note that the output dimension d in (Allen-Zhu et al., 2019) is removed because the output
of network function in this paper always is a scalar. For (1) and (2), the only different setting from
(Allen-Zhu et al., 2019) is that the initialization of last layer WL〜N(0, m) in this paper while
WL 〜N(0, d) in (Allen-Zhu et al., 2019). Because d = 1 and m > d here, the upper bound in
(Allen-Zhu et al., 2019) still holds for Wl: with probability at least 1 - exp(-Ω(m∕L)), kWL∣∣F ≤
,m/d. Therefore, (1) and (2) still hold for the initialization of this paper.
For (3), based on Lemma 7.1 in Allen-Zhu et al. (2019), we have |f (x; θo)∣ ≤ O(1). Denote by D
the ReLU function. For any l ∈ [L],
kOWif (x; θo)kF ≤ ∣∣WlDWl-1 …DWl + lkF ∙ ∣∣DW1+1 …XkF ≤ O(√L)
where the inequality is according to Lemma 7.2 in Allen-Zhu et al. (2019). Therefore, we have
kθθof(x; Θ0)k2 ≤O(L).	□
Lemma C.5 (Lemma 4.1, (Cao and Gu, 2019)). For any δ ∈ (0, 1), if w satisfies
O(m-3/2L-3/2[log(tnL2∕δ)]3/2) ≤ W ≤ O(L-6[logm]-3/2),
then, with Probability at least 1 - δ over randomness of θ0, for any t ∈ [T], kxk2 = 1, and θ, θ0
satisfying kθ - θ0k2 ≤ w and kθ0 - θ0k2 ≤ w , it holds uniformly that
|f (Xi； θ) - f (Xi； θ0) -hθθ0f(xi； θ0), θ - θ0i∣ ≤ O(w1/3L2Pmlog(m))kθ - θ0k2.	(C.33)
Lemma C.6. For any δ > 0, suPPose
m > O (POly(T, n, ρ-1, L, log(1∕δ) ∙ e√log '1/)).
t5	2
Then, with probability at least 1 一 δ, setting η = Θ(8号m)for algorithm 1, for any θ satisfying
Ile — θ0∣∣2 ≤ O(p√m log m), it holds that
t
X ∣f2 (φ(Oθ1∕l(xτ； bT-l)); bT-l) - (rτ - f1(xτ； bT-l))l
τ=1
≤ X ∣f2 (φ(Ob1∕l(xτ; bT-l)); e2) - (rτ - f1(xτ； bT-'))∣ + O (3√√t)
Proof. This is a direct application of Lemma 4.3 in (Cao and Gu, 2019) by setting R = t3 log m,e 二
-LR, and V = V0R2, where V is some small enough absolute constant. We set LT(bT-ι)二
∣∣f2(Oθb1 f1; θbτ2-1) - rτ - f1(xτ; θbτ1-1) ∣∣. Based on Lemma C.4 (2), for any τ ∈ [t], we have
2	2	2	2	t3
kbτ-bτ-1k2 ≤kbτ- θ0k2 + kθ0- bτ-1k2 ≤O( 而 log m
21
Published as a conference paper at ICLR 2022
Table 2: Selection Criterion Comparison (xt： selected arm in round t).
Methods	Selection Criterion
Neural Epsilon-greedy	With probability 1 — e, Xt = argmaxχt,i,i∈[n] fi(xt,i； θ1); Otherwise, select Xt randomly.
NeuralTS (Zhang et al., 2021)	For xt,i,∀i ∈ [n], draw rt,i from N(fi(xt,i； θ1),σt,i2), Then, select xt,i, i = arg maxi∈[n] τt∕
NeuralUCB (Zhou et al., 2020)	Xt = argmaxχt,i,i∈[n] (fi(xt,i； θ1) + UCBt,i).
EE-Net (Our approach)	∀i ∈ [n], compute f1(x5 θ1), f2 Qifi(xt,i； θ1); θ2) (Ex- ploration Net). Then Xt = argmaxχt,ii∈[n] f3(f1,f2; θ3).
Then, according to Lemma 4.3 in (Cao and Gu, 2019), then, for any θe2 satisfying kθe2 - θ20k2 ≤
O(ρ√√m log m), there exist a small enough absolute constant V0, such that
tt
22
Lτ(bθτ-1) ≤	Lτ (θe ) + 3t.	(C.34)
τ=1	τ=1
Then, replacing E completes the proof.	□
Lemma C.7 (Theorem 5, Allen-Zhu et al. (2019)). For any δ ∈ (0, 1), if w satisfies that
O(m-3/2L-3/2 max{log-3/2 m, log3/2(Tn∕δ)}) ≤ W ≤ O(L-9/2 log-3 m),	(C.35)
then, with probability at least 1 - δ, for all kθ - θ0 k2 ≤ w, we have
∣∣Oθf(x； θ) - Oθof(x； Θ0)k2 ≤ O(plog mw1∕3L3)kθθof(x; Θ0)k2.	(C.36)
D Motivation of Exploration Network
In this section, we list one gradient-based UCB from existing works (Ban et al., 2021; Zhou et al.,
2020), which motivates our design of exploration network f2. Let g(xt; θt) = Oθt f (xt; θt).
Lemma D.1. (Lemma 5.2 in (Ban et al., 2021)). Given a set of context vectors {xt}tT=1 and the
corresponding rewards {rt}tT=1 , E(rt) = h(xt) for any xt ∈ {xt}tT=1. Let f(xt; θ) be the L-layers
fully-connected neural network where the width is m, the learning rate is η, the number of iterations
of gradient descent is K. Then, there exist positive constants C1, C2, S, such that if
m ≥ poly(T,n, L, log(1∕δ) ∙ d ∙ e√log 1/6), η = O(TmL + mλ)-1, K ≥ O(TL∕λ),
then, with probability at least 1 - δ, for any xt ∈ {xt}tT=1, we have the following upper confidence
bound:
∣h(xt) - f (xt； θt)l ≤γι∣g(xt; θt)∕√m∣A-i + Y2 + Y1Y3 + Y4,	(D.1)
where
Yι(m,L) = (λ + tO(L)) ∙ ((1 — ηmλ)J/2p∕λ) + 1
Y2(m,L, δ) = ∣g(xt; θo)∕√mk A0-ι
(VmtiIF^+aS
γ3(m, L) = C2m-1/6Plog mt1∕6λ-7∕6L7/2, γ4(m, L) = Cim-1/6Plog mt2∕3λ-2∕3L3
tt
At = λI + X g(xt； θt)g(xt; θt)l∕m, At = λI + X g(xt； θo)g(xt; θo)l∕m.
i=1	i=1
Note that g(xt; θ0) is the gradient at initialization, which can be initialized as constants. Therefore,
the above UCB can be represented as the following form for exploitation network f1: |h(xt,i) -
fi(xt,i； Θ1)l ≤ Ψ(g(xt; θt)).
22
Published as a conference paper at ICLR 2022
Table 3: Exploration Direction Comparison.
MethOdS ∣	"Upward" Exploration "Downward" Exploration
NeuralUCB ∣	√	I	X	
NeuralTS ∣	Randomly	∣	Randomly
EE-Net I	√	I	√
EE-Net has smaller approximation error. Given an arm x, let f1 (x) be the estimated reward and
h(x) be the expected reward. The exploration network f2 in EE-Net is to learn h(x) - f1(x), i.e.,
the residual between expected reward and estimated reward, which is the ultimate goal of making
exploration. There are advantages of using a network f2 to learn h(x) - f1(x) in EE-Net, compared
to giving a statistical upper bound for it such as NeuralUCB, (Ban et al., 2021), and NeuralTS (in
NeuralTS, the variance ν can be thought of as the upper bound). For EE-Net, the approximation error
for h(x) - f1(x) is caused by the genenalization error of the neural network (Lemma B.1. in the
manuscript). In contrast, for NeuralUCB, (Ban et al., 2021), and NeuralTS, the approximation error
for h(χ) - fι (x) includes three parts. The first part is caused by ridge regression. The second part
of the approximation error is caused by the distance between ridge regression and Neural Tangent
Kernel (NTK). The third part of the approximation error is caused by the distance between NTK and
the network function. Because they use the upper bound to make selections, the errors inherently
exist in their algorithms. By reducing the three parts of the approximation errors to only the neural
network convergence error, EE-Net achieves tighter regret bound compared to them (improving by
roughly √logT).
一 ∙ h(%,t)
Gap T I
L ∙ f,a,i M)
CaSe 1: UPWard Exploration
「∙方-H)
Gap Tl
L ∙以F)
Case 2: DoWnWard Exploration
Figure 6: Two types of exploration: Upward exploration and Downward exploration. fι is the
exploitation network (estimated reward) and h is the expected reward.
EE-Net has the ability to determine exploration direction. The two types of exploration are
described by Figure 6. When the estimated reward is larger than the expected reward, i.e., h(x) -
f1(x) < 0, we need to do the ‘downward exploration’, i.e., lowering the exploration score of x to
reduce its chance of being explored; when h(x) - f1(x) > 0, we should do the ‘upward exploration’,
i.e., raising the exploration score of x to increase its chance of being explored. For EE-Net, f2 is
to learn h(x) - f1(x). When h(x) - f1(x) > 0, f2(x) will also be positive to make the upward
exploration. When h(x) - f1(x) < 0, f2(x) will be negative to make the downward exploration. In
contrast, NeuralUCB will always choose upward exploration, i.e., f1(x) + UCB(x) where UCB(x)
is always positive. In particular, when h(x) -f1(x) < 0, NeuralUCB will further amplify the mistake.
NeuralTS will randomly choose upward or downward exploration for all cases, because it draws a
sampled reward from a normal distribution where the mean is f1(x) and the variance ν is the upper
bound.
23