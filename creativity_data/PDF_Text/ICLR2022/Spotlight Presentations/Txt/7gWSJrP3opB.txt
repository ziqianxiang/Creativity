Published as a conference paper at ICLR 2022
A General Analysis of Example-Selection for
Stochastic Gradient Descent
Yucheng Lu； Si Yi Meng*, Christopher De Sa
Department of Computer Science
Cornell University
Ithaca, NY 14853, USA
{yl2967,sm2833,cmd353}@cornell.edu
Ab stract
Training example order in SGD has long been known to affect convergence rate.
Recent results show that accelerated rates are possible in a variety of cases for
permutation-based sample orders, in which each example from the training set
is used once before any example is reused. In this paper, we develop a broad
condition on the sequence of examples used by SGD that is sufficient to prove
tight convergence rates in both strongly convex and non-convex settings. We show
that our approach suffices to recover, and in some cases improve upon, previous
state-of-the-art analyses for four known example-selection schemes: (1) shuffle
once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4)
Markov Chain Gradient Descent. Motivated by our theory, we propose two new
example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve
unprecedented accelerated convergence rates for learning with data augmentation.
Second, we greedily choose a fixed scan-order to minimize the metric used in our
condition and show that we can obtain more accurate solutions from the same
number of epochs of SGD. We conclude by empirically demonstrating the utility
of our approach for both convex linear-model and deep learning tasks. Our code is
available at: https://github.com/EugeneLYC/qmc-ordering.
1	Introduction
To minimize a differentiable function f : Rd → R, stochastic gradient descent (SGD) iteratively
updates a parameter vector w ∈ Rd starting at some w0 by running
wt+1 = Wt - αNf(wt; xt),	(1)
where αt is the step size at iteration t, xt is a data example (often a minibatch of data) chosen
by some process—typically by subsampling a training dataset—for SGD to use at iteration t, and
Vf (wt； Xt) is an example gradient, which We hope will be a good approximation for the gradient
of the objective Vf (Wt). In the standard setup, the x's are drawn from a dataset D of size n, and
f (W) = n Px∈d f (w; x), which is often referred to as Empirical Risk Minimization (ERM). The
order in which the example sequence x0, x1, . . . are chosen is known to affect the convergence of SGD.
For instance, compare so-called “random reshuffling” to with-replacement sampling: optimizing a
strongly convex objective with T total iterations, random reshuffling samples the xt from D without
replacement and achieves an accelerated convergence rate of 1/T2 , while with-replacement sampling
yields a convergence rate of 1∕τ (Bottou, 2012; Recht & Re, 2012; Gurbuzbalaban et al., 2021).
Similar accelerated rates have been shown in other settings and for other example orders, such as
shuffling the dataset once (Nguyen et al., 2020; Ahn et al., 2020; Mishchenko et al., 2020). However,
these analyses have mostly focused only on specific example-selection schemes, and study only the
case of ERM-type finite-sum objectives. This does not help us understand how new example orders
(such as the data echoing method of Choi et al. (2019)) affect the convergence of SGD.
This paper develops a general condition on the example gradients themselves that is sufficient to
provide a convergence rate for SGD. Intuitively, our main result is: the convergence rate of SGD
* Equal Contribution.
1
Published as a conference paper at ICLR 2022
depends on howfast the averages ofconsecutive example gradients Vf (w; Xt) converge to thefull
objective gradient Vf (w). Using with-rePlacement sampling, the average of m consecutive gradient
examples starting at any timestep T,2 P；=+；-1 Vf (w; xt), converges to Vf (W) at a rate of O(1∕m)
in terms of the norm squared: we show SGD with any example sequence that fulfills this condition
will converge at the same asymptotic rate as with-replacement sampling. Alternatively, if that average
converges at the faster O(1/m2) rate typical of Quasi-Monte-Carlo (QMC) (Caflisch, 1998), then we
show SGD enjoys the accelerated rate that random reshuffling gets. Our contributions are as follows:
•	We propose a new condition on the example gradients—average gradient error—and provide
convergence analysis using this general condition for both non-convex and strongly-convex
problems. We justify the validity of this condition on synthetic experiments (Section 3).
•	We show that many commonly used example orderings—shuffle once, random reshuffling, data
echoing, Markov Chain Gradient Descent—can be analyzed as special cases under our theoretical
results, which match or improve upon their existing rates in the literature (Section 4).
•	We propose two new algorithms: (1) QMC-based data augmentation that transforms examples via
a low-discrepancy sequence, improving generalization; and (2) a greedy algorithm that sorts the
examples before each epoch based on our new average gradient error metric (Section 5).
•	Empirically, we evaluate our two algorithms on several image classification benchmarks including
MNIST, CIFAR10/100 and ImageNet. We show with QMC-based data augmentation, a higher
validation accuracy can be achieved without hyperparameter tuning—this suggests that QMC may
be a good default driver to use with data augmentation for deep learning in general. Meanwhile,
the greedy algorithm converges faster both in terms of iteration and wall-clock time (Section 6).
2	Related work
Example ordering in stochastic optimization. Traditional example ordering in SGD is carried
out in a with-replacement fashion, which is used to ensure unbiased estimation of the full gradient
(Robbins & Monro, 1951; Bach & Moulines, 2011; Zhang, 2004; Bottou et al., 2018; Drori & Shamir,
2020). Significant attention has been paid to importance sampling with respect to various measures,
such as Lipschitz constants (Schmidt et al., 2017; Needell et al., 2014), example gradient norms
and bounds (Zhao & Zhang, 2015; Alain et al., 2015; Papa et al., 2015; Lee et al., 2019), individual
losses (Kawaguchi & Lu, 2020; Loshchilov & Hutter, 2015), and data heterogeneity (Lu et al., 2021).
Without-replacement sampling, however, is more common in practice and empirically allows faster
convergence (Bottou, 2012). Among the most popular without-replacement approaches are shuffle
once (SO) (Bertsekas, 2011; GurbuzbaIaban et al., 2019) and random reshuffling (RR) (Ying et al.,
2017). In theory, Recht & Re (2012) undertook the first investigation on convergence of RR via
the noncommutative arithmetic-geometric mean conjecture, to which subsequent works provide
counter examples (Yun et al., 2021; De Sa, 2020). HaoChen & Sra (2019) performed an epoch-wise
acceleration analysis on RR while GUrbUzbalaban et al. (2021) considered its convergence over
infinite epochs. Safran & Shamir (2020) analyzed the lower bounds for both SO and RR methods,
and their results are further polished by Mishchenko et al. (2020) via the Bregman divergence bound.
Aside from manual ordering, another line of research focuses on perturbed example ordering from
data echoing (Choi et al., 2019; Agarwal et al., 2020).
Quasi-Monte Carlo. Quasi-Monte Carlo (QMC) is a variant of Monte Carlo (MC) methods that
uses a low-discrepancy sequence instead of a pseudorandom sequence. QMC has been successfully
applied in a wide variety of domains including computer graphics (Keller, 1995), finance (Joy et al.,
1996), and computational biology (Cieslak et al., 2008). In machine learning, using QMC in place
of MC can significantly improve many techniques including variational inference (Buchholz et al.,
2018; Liu & Owen, 2021), feature mapping (Yang et al., 2014; Avron et al., 2016), normalizing
flows (Wenzel et al., 2018), deep learning based PDE (Chen et al., 2019), and time series analysis
(Philipson et al., 2020). For stochastic optimization, early works like Homem-de Mello (2008) and
Pennanen (2005) established the asymptotic convergence of a QMC sequence in terms of the training
set size, while Jank (2005) proposed replacing MC with QMC in computing the E-step of the EM
algorithm. Similar to our motivation, Buchholz et al. (2018) analyzed the convergence of SGD when
samples are drawn using QMC. Their approach differs significantly from ours in that their method
draws an independent unbiased length-b QMC sequence for each minibatch, while our examples
come from contiguous subsequences of a length-T QMC sequence that is used across all iterations.
2
Published as a conference paper at ICLR 2022
3	Example-gradient averages and SGD convergence
In this section, we describe our setup, define the average gradient error condition we are proposing,
and state our main result. Our objective is to minimize a continuously differentiable function
f : Rd → R using examples xt from some set X retrieved at each iteration t. We make the usual
assumption that both the loss gradient and the example gradients are L-Lipschitz continuous.
Assumption 1 (L-Smoothness). For some L < ∞, for any u, v ∈ Rd and any example x ∈ X,
∣∣Vf (u;X)- Vf (v;x)k ≤ L ∙ ku - Vk and ∣∣Vf (u) - ▽/(V)Il ≤ L ∙ ∣∣u - v∣∣∙
We propose a new condition on the average gradient error. Informally, this condition bounds how
averages of consecutive example gradients approximate the objective gradient. Formally,
Assumption 2. In the context of Equation (1), we say that the example sequence x0, x1, . . . is
(γ, C, Φ)-concentrating for γ ∈ [1, 2], C > 0, Φ ≥ 0, if for any timestep τ ≥ 0 and any m > 0,
1 τ +m-1	2	1
mm X Vf (wτ ； Xt)-Vf(WT)	≤ 菽(C2 +Φ2kVf(wτ )『),	⑵
t=τ
where wτ is the weight parameter vector arrived at after τ SGD update steps in Equation (1).
The constants C and Φ here may depend on the total number of iterations T : specifically, they may
absorb logarithmic factors like log(τ + m)2s typical in a QMC error bound, where s is the dimension
of the sample space. In addition, when the {Xt }’s are random, we will show that this assumption
holds with high probability. Furthermore, it suffices to show that the inequality holds for any w:
our requirement that it holds only for the specific wτ arrived at by SGD is weaker. We can develop
intuition about Assumption 2 by considering familiar cases:
•	For general with-replacement sampling, the sum in (2) is a sum of independent random variables;
so, a concentration argument would yield Assumption 2 with γ = 1 with high probability.
•	For general without-replacement sampling from a dataset of size n, any whole-epoch subsequences
from the sum in (2) will cancel to zero, so we expect that sum to have magnitude O(n) independent
of m, and thus Assumption 2 should hold with γ = 2 (Propositions 1 to 4).
•	For low-discrepancy sampling, we would expect Assumption 2 to hold with γ = 2, along with a
multiplicative log(τ + m)2s term: this is the classic error rate for QMC (Proposition 6).
In Section 4, we will make this intuition rigorous with high probability under the bounded gradient
error assumption. This intuition illustrates both how Assumption 2 can cover previously analyzed
settings and how it can generalize to cases not previously studied, such as QMC data augmentation.
Also observe that Assumption 2 is easily adapted to minibatch SGD: if it holds for the single-example
case, it should also hold with modified constants for minibatches of size b (consisting of averages
of b consecutive example gradients) by substituting m 7→ mb.1 Since this is straightforward, for
simplicity of presentation our theory focuses on the batch-size-1 case. Our analysis is based on the
following key lemma, which bounds the evolution of SGD over an “analysis phase” of m steps.
Lemma 1. Suppose our setup satisfies Assumptions 1 and 2 and that we use a constant step size α.
For all timesteps T ≥ 0, let m > 0 be some integer such that 3am1-γ∕2Φ ≤ αm ≤ 焉.Then the
objective at timestep τ + m is bounded by
f(wτ+m) ≤ f (wτ) - 1 αm∣Vf(wτ)『+ 2αm1-γC2.
By applying this lemma inductively and choosing the step size appropriately, we can derive conver-
gence rates for SGD in a variety of settings. Note that while here for simplicity we state results for a
fixed step size, Lemma 1 can also be used to get essentially the same rates for diminishing step size
schemes, which We analyze in Appendices A.2 and A.3. In What follows, We let f * be the global
minimum of f, let ∆ := f (wo) - f *, and use O to hide logarithmic terms in the problem parameters
such as C, Φ, L, ∆, and , While treating γ as a constant.
Theorem 1 (Non-convex case). Suppose that our setup satisfies Assumptions 1 and 2, and let > 0
be any target error. Using SGD (1) with a constant step size α = 笠 ∣"(4C∕e + 3Φ)2∕γ] , the
numberof steps T needed to achieve mint=。,…，t-ι ∣∣Vf (wt)∣∣2 ≤ e2 is at most
T=
[中]J( 4C + 3Φ)2∕γ] = O (⅞⅛∆ +
Φ2∕γ L∆
+ 宗+Φ2∕γ).
1The only technical subtlety is that the iterates wτ of SGD Would vary With minibatch sizes.
3
Published as a conference paper at ICLR 2022
Online
Offline
06
10m
3 3 ] ] U
1
«-?«>1 FX33aAz
Online
ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ ɪ
-°Λ9≈A I (穴OSUaATa--
Offline
(a) Distance to optimum over iterations
(b) Average gradient error (LHS of Assumption 2)
Figure 1: Comparison of sampling schemes on a synthetic least squares problem.
Observe that when γ = 1, this gives the standard -4 rate expected for non-convex SGD, and for
γ = 2, this gives us the accelerated -3 rate of shuffling methods. We can get an even faster rate if f
satisfies the μ-Polyak-Eojasiewicz (PL) condition ∣∣Vf (w)k2 ≥ 2μ(f (W) - f *), which generalizes
strong convexity and has been applied before in the study of sample orders (Mishchenko et al., 2020;
Ahn et al., 2020): in particular the following theorem holds for μ-strongly convex functions.
Theorem 2. Suppose that f satisfies the μ-PL condition and our setup satisfies Assumptions 1 and 2.
Let e > 0 be any target error, and K = L∕μ be the condition number of the problem. Using SGD
(1) with a constant step size ɑ =笠 ∣"(8C2∕(μe2) + 9Φ2) 1/Y ]-1, the number of steps T needed to
guarantee f (WT) 一 f * ≤ e2 is at most
T=112K log (等)]∙[(普 + 9Φ2)1] = O (J + κΦ2∕γ + K).
Observe that when γ = 1, this recovers the ordinary T = κe-2 rate we usually get for strongly
convex SGD, and when γ = 2 we get a faster rate of Ke-1 . These theorems together show that our
Assumption 2 is sufficient to show the convergence of SGD, and the convergence-rate parameter γ
of the assumption translates to affect the convergence rate of SGD. This validates our intuition that
faster convergence of averages of consecutive example gradients to the full gradient Vf (W) leads to
faster convergence of SGD.
Synthetic experiments. We quickly validate these results on a synthetic 10-dimensional
strongly-convex problem. The first setting we consider is the expected risk minimization of
R(W) = E [(xτw — y)2], where X 〜 N(0, Id) and y | x 〜N(XTw*, 1) for some optimal value
W* . We run SGD in an online fashion—at iteration t we draw samples (xt, yt) from the underlying
distribution to compute VR(Wt; Xt) used in the update. We compare drawing these samples indepen-
dently at random against drawing using a QMC sequence (Sobol in [0, 1]d) via an inverse transform,
using for both cases the same diminishing step size scheme selected to minimize the expected risk
for the random-sampling case—the optimal step size scheme for vanilla SGD. The online plot of
Fig. 1(a) shows that the convergence rate is strictly superior with QMC, which achieves a O(1∕t2 ) rate
compared to the O(1∕t) rate of random sampling, which is what Theorem 2 predicts.
We also evaluate the offline setting, where we draw n independent examples from the same distribution
to form a training set, and minimize the empirical risk Rn(W) = ɪ Pn=I(XTW - yi)2. This
corresponds to a least squares problem with optimal solution Wn* . We run SGD epoch-wise for
K epochs, in which we compare sampling from the training set uniformly with replacement (IID
Uniform), random reshuffling (RR), shuffle once (SO), and sampling using one QMC sequence in
[0, 1] of length T = nK followed by a mapping to example indices (Sobol). In the offline plot of
Fig. 1(a), we see that the low-discrepancy methods all yield an accelerated rate compared to IID
Uniform, which again validates our theory. Additional details can be found in Appendix A.1.3.
With the same synthetic setup, we also verified our bound in Assumption 2 by measuring the average
gradient error over a sequence of examples. The fixed point Wτ is arbitrarily set to the origin. For the
online setting (left of Fig. 1(b)), we use the same set of examples as in the SGD experiments above
starting at t = 0. Similarly, in the offline setting, we go through the examples epoch-wise. As we can
see, the sample orderings given by QMC, RR and SO (offline only) indeed give us an accelerated rate
of decrease in the average gradient error as we increase m (γ ≈ 2), justifying our main assumption.
4
Published as a conference paper at ICLR 2022
4	Analysis of existing scan orders
In this section, we illustrate the power of our approach by proving convergence rates for example-
selection methods proposed and analyzed in previous literature. For each method, we show Assump-
tion 2 holds with high probability, and then by applying Theorems 1 and 2 to these results, we show
how existing rates for these methods can be recovered and in some ways improved upon. To the best
of our knowledge, these are the first high-probability results for shuffle once and random reshuffle for
general non-convex optimization. When using a finite training set of examples, we let n denote its
size, let the examples be indexed as x(0), x(1), . . . , x(n-1) (to avoid confusion with xt, the example
used by SGD at step t), and let T = nK denote the total number of iterations after K epochs. We
will also require the following standard assumption that bounds the error of a single example gradient.
Assumption 3. For all examples x ∈ X and points w ∈ Rd, there exists A, B ≥ 0 such that the
gradient errors satisfy ∣∣Vf (w; x) — Vf (w)k2 ≤ A2 + B2kVf (w)k2.
Shuffle once (SO). In the shuffle-once variant of SGD, a single permutation σ of {0, . . . , n — 1} is
chosen uniformly at random at the start, and the examples are used repeatedly in that order: explicitly,
xt = x(σ(t mod n)). One way to analyze shuffle-once is to prove Assumption 2 for permutation-based
methods generally.
Proposition 1. Let Assumptions 1 and 3 hold, and suppose that we are using a permutation-based
method of sampling, that is, any method such that xkn, xkn+1, . . . , xkn+n-1 is a permutation of
x(0) , x(1) , . . . , x(n-1) for all epochs k ≥ 0. Any such method satisfies Assumption 2 with γ = 2,
C2 = n2A2 and Φ2 = n2B2.
This immediately lets us recover previous rates up to constant factors, but we can do better. In the
case where we are learning over a bounded region, we can prove a stronger result for shuffle once.
Proposition 2. Suppose that we are using the shuffle once variant of SGD to learn over a region
B ∈ Rd of radius at most R, such that the iterates wt are guaranteed to remain within this region.
Assume that for all w ∈ B and all examples x in the training set of size n, Assumption 1 (L-
Smoothness) and Assumption 3 hold. Then with probability at least 1 — p, Assumption 2 holds with
Y = 2, C2 = O(dA2(n + B2)), and Φ2 = O(ndB2).2
In comparison to previous results and to our rate implied by Proposition 1, this improves the
dependence from n to √nd, which is a significant improvement over the best rates for shuffle once
available in the literature when the dimension is small relative the training set size. In particular, if
we set B = 0 and consider small , our rate in the non-convex case becomes
AL∆√nd
T = O
T3
which implies e2 ≤O ((AL**"3
This rate matches that for shuffle once achieved in Nguyen et al. (2020, Corollary 1) in terms of e,
up to logarithmic factors. In the μ-PL (or strongly convex) case for B = 0, we again obtain a rate
matching that of Nguyen et al. (2020), Ahn et al. (2020), and Mishchenko et al. (2020):
T = O (KA ynμd) , which implies e2 ≤ O (κ2*d
κ2A2d
μnK2
Random reshuffling (RR). Random reshuffling is similar to shuffle once, except that a new
ordering is chosen at each epoch: sampling the dataset without replacement. Concretely, if σk denotes
the permutation used by random reshuffling at the kth epoch, then Xt = x(σbt∕nc (t mod n)).
Proposition 3. Suppose that we are using the random reshuffling variant of SGD. Assume that for all
w ∈ Rd and all examples, Assumption 1 (L-Smoothness) and Assumption 3 hold. For some p ∈ (0, 1),
Set the constant step size to satisfy a ≤ (max {1460BnL ∙ log (4e2T∕p) , 2nL}) . Then with
probability at least 1 — P, Assumption 2 holds with Y = 2, C2 = O(nA2) and Φ2 = O(nB2).
Setting B = 0, our rate in the non-convex case now becomes
T = O ( AL∆√n) , which implies e2 ≤ O ((ALATxP" ) = O
2The probability P is only present in log terms in these expressions, so it does not appear in the O.
5
Published as a conference paper at ICLR 2022
Here we match the best rate obtained by Mishchenko et al. (2020, Corollary 3) in the small setting.
In the μ-PL (or Strongly-Convex) case, We get
T = O (KAqn) , Whichimplies e2 ≤ O(⅛A>) = O(藤)∙
As in shuffle once, here our rate for random reshuffling matches that obtained by Nguyen et al. (2020)
and Ahn et al. (2020), as Well as Mishchenko et al. (2020) albeit With a slightly Worse dependency
on κ. It’s Worth noting that for simple quadratics, RR and SO are only faster than With-replacement
SGD when K & 5 (Safran & Shamir, 2021).
Random reshuffling with data echoing. Data echoing is a technique that can be easily imple-
mented in a machine learning training pipeline to increase throughput and improve performance.
It Was first introduced and tested empirically by Choi et al. (2019) and analyzed by AgarWal et al.
(2020). The idea is to perform multiple SGD updates on each example xi (or minibatch) before
proceeding to the next. By “echoing” examples We alloW more time for upstream data loading and
preprocessing, and consequently decrease doWnstream GPU idle time for gradient computation. For
simplicity, We also use a fixed number of echos c as in AgarWal et al. (2020). Concretely, a c-echoed
version of a sample order X is given by Xt =企〔〃可.Data echoing can essentially be applied to
any example-ordering scheme, and here We provide one analysis under random reshuffling. The
justification for Assumption 2 under random reshuffling With data echoing folloWs essentially Without
modification from the c = 1 version in the previous subsection.
Proposition 4. Suppose that we are using the random reshuffling variant of SGD, where each example
is echoed c times using the same step size in RR. Under the same assumptions as in Proposition 3,
with probability at least 1 一 P, Assumption 2 holds with Y = 2, C2 =O(CnA2) and Φ2 = O(CnB2).
It immediately folloWs that data echoing should get the same convergence rates We shoWed in
Section 4 With A2 and B2 multiplied by C. Although AgarWal et al. (2020) also provided an analysis
for data echoing, they require that the examples are sampled independently, rather than the random
reshuffling setting that is more commonly-used: as a result, their analysis did not achieve the
accelerated e-3 rate that shuffled methods enjoy. Another advantage of our analysis is that the proof
folloWs exactly from that of vanilla random reshuffling, from Which the constant C simply propagates.
Markov chain gradient descent (MCGD). To illustrate the versatility of Assumption 2, We shoW
hoW it can be satisfied by a problem Where the objective is not a finite sum and Where γ 6= 2. Consider
f (W) = Eξ〜ξ[∕(w; ξ)] with some underlying distribution Ξ. Running SGD then requires that at
each iteration t, we draw Vf (wt； ξt) where ξt 〜Ξ; however, sampling from Ξ can be intractable.
The method of Markov Chain Gradient Descent addresses this problem by sampling the ξt from the
trajectory of a single Markov chain with stationary distribution Ξ (Sun et al., 2018). The intuition
is that although at early iterations the ξ's have not converged to their true distribution, the iterates
visited by SGD are also far from the optimum, thus larger approximation error in the early ξ's is
rather harmless. As we continue iterating, the Markov chain will mix as SGD converges. We show
that the convergence of MCGD can be bounded in terms of the mixing time of that Markov chain.
Proposition 5. Suppose that we use samples Xt from a Markov chain with mixing time tmix. Assume
that for all w ∈ R and all examples Xt, Assumption 3 holds. Then with probability at least 1 一 p,
Assumption 2 holds with Y = 1, C2 = O(A2tmx), and Φ2 =O(B2*齿).
It follows that for non-convex optimization in the B = 0 case, our convergence rate is given by
T = O (A2L∆mx) , which implies e2 ≤ O(Atmx1√lδ).
Sun et al. (2018, Theorem 2) use a diminishing step size O(1/tq) to obtain T = O(e-2/1-q), where
q ∈ (1/2, 1). In contrast, our rate is faster and holds with high probability instead of in expectation.
5	New example-selection methods for faster convergence
The analysis in Section 4 focused on recovering the convergence rates for SGD with known example-
ordering algorithms. In this section, we propose two new example-selection approaches that allow
faster convergence: QMC-based data augmentation and greedily minimizing the metric in (2).
6
Published as a conference paper at ICLR 2022
Algorithm 1 Example-Ordered SGD via Greedily Minimizing Average Gradient Error
Input: step size a, number of iterations T, random projection matrix Π, buffer for gradients estima-
tion: gi J 0, ∀i ∈ {0,…,n - 1}, g J 0, initial weights wo, initial permutation σ0.
1:	for t = 0, ∙∙∙ , T/n — 1 do
2:	Initialize : gi J 0, ∀i ∈ {0, ∙∙∙ ,n - 1}; g J 0; I J 0.
3:	for i = 0,…,n — 1 do
4:	Update the model parameters: wtn+i+ι J wtn+i — αVf (wtn+i； x(σt(i))).
5:	Update the buffers: gσt(i) J ∏ Vf(Wtn+i； XSMi))); g J g + gσt(i).
6:	end for
7:	for i = 0,…，n — 1 do
8:	σt+1(i) J arg min
i∈{ 0,…,n- 1 }\I
9:	end for
10:	end for
11:	return wT
P (gj — g/n)2 * * ; I J I ∪ {σt+1(i)}.
j∈I∪{i}
QMC-based data augmentation. In many scenarios where only limited examples are given, we
want to augment the dataset for better generalization. More formally, given a transform function
A that takes example x and a random variable ζ uniformly distributed in [0, 1]s as input, where s
denotes the dimension of augmentation space, the augmented objective for a dataset D of size n is
f (W) = n P Eζ~u[0,i]s f (w； A(x,Z)) = n PRRsf (w； A(x,Z)) dZ.	(3)
x∈D	x∈D
The rationale is that by performing some reasonable random transformation on a given example, we
assume the output would be another example that is identically distributed, and the expected value
models an infinitely-large training set consisting of such transformed examples. For example, in an
image classification task, we could set s = 1 and have A(x, ζ) output the image x rotated by an angle
of 20° (2Z — 1), modeling that a slight rotation of an image should preserve its label.
QMC can approximate this expectation with a low-discrepancy sequence of ζt drawn from the
s-dimensional unit cube [0, 1]s. Examples of such sequences include the Halton and Sobol sequences
(Drmota & Tichy, 2006). QMC is especially favorable for data augmentation because s is usually
small in most data augmentation methods. We propose to use QMC for data augmentation together
with random reshuffling. Concretely, if ζ0, ζ1, . . . is our low-discrepancy sequence and σk denotes
the permutation used by random reshuffling in the kth epoch, then we propose to use the example
Xt = A(x "」(t mod n)) ,Zbt∕nC+σbt∕nC (t mod n)). That is, when we sample example i in epoch k, we
use the (k + i)th element of the low-discrepancy sequence. This is not the only reasonable way of
combining QMC and RR: it is just one way we found to work well. In theory we would expect the
approximation error here to decay at the rate O(1/m2), instead of the O(1/m) of the random sampling
that is standard for data augmentation. To prove this rigorously, we make two additional assumptions
which are commonly used in analyzing QMC sequences (Aistleitner & Dick, 2014).
Assumption 4 (Bounded gradient variation). There exists a constant V > 0 such that for any fixed
w ∈ Rd andX ∈ X, the example gradients as a function of ζ under QMC data augmentation, F(ζ) =
Vf (w； A(X, ζ)), has Hardy-Krause variation (Tezuka, 2000) at most V, that is VHK(F) ≤ V .
Assumption 5. The QMC sequence {ζt}t≥0 has low star-discrepancy (Owen, 2003): for all m > 0,
m-1	s	s
SUP	m P 1{ζt ∈ [0, a)} — Qaj ≤ CQMC ∙ —m—,
a∈[0,1]s	t=0	j=1
where [0, a) = {X ∈ [0, 1]s | 0 ≤ Xj < aj, j = 1, . . . , d} for some constant CQMC.
Proposition 6. Suppose that we are using the random reshuffling variant of SGD with QMC data
augmentation as described. Assume that for all w ∈ Rd and all examples, Assumptions 1 , 3,
4 and 5 hold for Equation 3. For some p ∈ (0, 1), set the step size to be a constant such that
α ≤ (max{1460BnL ∙ log(4e2T∕p) , 2nL})-1. Then with probability at least 1 — P, Assumption 2
holds with Y = 2, C2 = O(n2V2CQMClog(T)2s + nA2) and Φ2 = O(nB2).
Comparing it with Proposition 3, this QMC variant enjoys the same O(1∕t2) rate we get for vanilla
random reshuffling: to our knowledge, this is the first accelerated rate for learning with data augmen-
7
Published as a conference paper at ICLR 2022
---- IID-Uniform (tuned)
QMC (untuned)
——QMC (tuned)
150	175	200
Epoch
>υ2⊃uυ< Uo⅛jPP=B>
9 8 7 6 5
6 6 6 6 6
Aue.Inuuq UOJ3BP=B>
(a) Validation of ResNet20 on CIFAR10	(b) Validation of ResNet20 on CIFAR100
Figure 2:	Data augmentation with IID-uniform (standard) and QMC-based methods on CIFAR.
tation. In addition to the theoretical advance, we demonstrate in Section 6 that our QMC variant can
achieve better validation performance in practice in multiple applications.
Better example ordering via greedy selection. Taking a closer look at Assumption 2, the
magnitude of the left-hand side plays a crucial role in the convergence. In the ERM setting, this
motivates us to select a permuted example order that minimizes this expression, following which
SGD would converge with minimized average gradient error. However, naively constructing such a
sequence is tedious as iterating over all the τ, m > 0 and all permutations can be computationally
intensive. In light of this, we apply several approximation techniques into the construction and
formulate it into Algorithm 1. The first technique is to use stale gradients, i.e., using the gradients
computed at each epoch to estimate the sequence used in the next epoch (line 5 in Algorithm 1).
The intuition is that based on the smoothness of loss function (Assumption 1), we would expect
the stale gradients to approximate the current gradients with tolerant approximation error as long as
the step size is reasonably small. The second technique is random projection (line 9): we search
a lower-dimensional space, which allows faster construction and reduces memory use (see similar
strategies of using smoothness and dimension reduction techniques in (Caflisch, 1998)). Because this
is a selection-by-permutation method, under Assumption 3 our greedy selection method will trivially
satisfy Assumption 2 with γ = 2, C = nA, and Φ = nB (see Proposition 1).
6	Experiments
In this section we evaluate our new algorithms
on several deeP learning benchmarks. First, we
comPare QMC-based data augmentation against
IID-uniform augmentation on CIFAR10/100 and
ImageNet datasets. Second, we comPare greedy
Table 1: Top1 validation accuracy (%) of
ResNet18 on ImageNet. The original one is the
standard benchmark provided by PyTorch.
Original Uniform (tuned) QMC (untuned)
69.76	70.19	70.48
ordering (Algorithm 1) with RR and SO, and show how to further accelerate it with randomly
projected sorting. Other details on the experimental setup can be found in Appendix A.1.1. Each
experiment is repeated 10 times with consistent seeds among the algorithms.
QMC-based data augmentation. We start by training ResNet20 on CIFAR10 and CIFAR100,
where discrete and continuous data augmentations are applied, respectively. Specifically, CIFAR10
uses random crop and random horizontal flip while CIFAR100 uses an additional random rotation
of 15 degrees. To apply the QMC-based data augmentation, we first generate a Sobol sequence of
appropriate dimension using the qmcpy package (Choi et al., 2020+), and then replace the pseudo-
random sequence used in the original random augmentation pipelines with that Sobol sequence.
We run the baseline IID-uniform method with finetuned hyperparameters (weight decay 10-4),
which reproduces the result from He et al. (2016) with an error rate 8.4%. Then we run QMC-base
augmentation with the same hyperparameter (untuned) and finetuned counterparts, with a grid search
over weight decay values in {r ∙ 10-4}4=1. From Figure 2 We observe the QMC-based augmentation
consistently outperforms the baseline methods, even without hyperparameter tuning. Comparing
Figure 2 with He et al. (2016), we observe the QMC-based augmentation allows ResNet20 to reach
comparable validation accuracy as ResNet44 while requiring only 40% as many parameters (0.27M
vs 0.66M). We run a t-test on these results (in Appendix A.1.2) to show the validation accuracy
from the two augmentation methods are different statistically significantly (P-Value P = 7 ∙ 10-7 on
8
Published as a conference paper at ICLR 2022
.5,O.5Q
NNLL
9 9 9 9
>U2⊃UU< Uo-⅛p 一-
Shuffle Once
Random Reshuffling
----Greedy w/ QR
— Greedy w/ sparse+QR
0	500	1000
Time(sec)
(a) Logistic Regression on MNIST. The greedy algorithm reuses the hyperparameters finetuned
1500
on RR.
(b) ResNet20 on CIFAR10. The greedy algorithm is able to converge faster when optimizing the same
loss function while achieving SOTA validation accuracy with mild tuning.
Figure 3:	Comparison between Algorithm 1 and RR/SO on MNIST and CIFAR10.
CIFAR10 and p=0.036 on CIFAR100). Importantly, this improvement comes essentially for free, as
generating low discrepancy sequences in low dimension has very little overhead.
We also evaluate our method on fine-tuning ResNet18 on ImageNet. We apply different augmentation
methods on a pre-trained model and train for 5 additional epochs with step size 10-4. We report their
Top1 accuracies in Table 6, which shows that our QMC method improves the validation accuracy by
0.3% compared to the same number of epochs of fine-tuning using random sampling.
Better example ordering via greedy selection. In this section we evaluate Algorithm 1 on two
benchmarks: Logistic Regression on MNIST and ResNet20 on CIFAR10. As discussed, sorting the
stale gradients naively could incur substantial overhead on memory and computation. To mitigate
this, we adopt two methods: random projection and QR decomposition. The former is mainly
to reduce storage: we obtain a gradient computed at some time in an epoch and project it into a
lower-dimensional space before storing it. Classic ways of projection include Gaussian projection or
random sparsification: we adopt the latter as it does not require storing the projection matrix, which
minimizes the storage cost. After we obtain all the stale gradients, we concatenate them into a matrix
and perform QR decomposition before sorting, which allows us to sort in a low-dimensional space
while preserving the order of gradients since the inner products between any two tensors will remain
the same Gander (1980). We set the target dimension to be of 10% size of the original space. In the
spirit of evaluating the applicability of Algorithm 1, we do not perform hyperparameter tuning in this
section but reuse the ones tuned in the literature on Random Reshuffling.
We plot the results in Figure 3. In Figure 3(a) we observe greedy algorithms can consistently converge
faster than RR and SO epoch-wise with optimizing the same loss function. When QR is used without
projection, the algorithm is able to reach higher validation accuracy but converges slower with respect
to the wall-clock time. On the other hand, when we use random sparsification additionally, the
algorithm converges faster with respect to both epoch and wall-clock time without compromising
the validation accuracy. For CIFAR10, we observe the greedy method can converge faster when
optimizing the same loss as other baselines, and achieves higher validation accuracy when fine-tuned.
7 Conclusion
We present a unified analysis on example orderings used in SGD, which generalizes several widely-
used orderings in the literature. We propose a greedy algorithm that allows faster convergence via
constructing a better example order with approximate sorting techniques, as well as QMC-based
augmentation that achieves higher validation accuracy on multiple benchmarks. One potential future
direction is designing example orderings to more efficiently minimize the average gradient errors.
9
Published as a conference paper at ICLR 2022
Acknowledgments
The authors would like to thank A. Feder Cooper and anonymous reviewers from ICLR 2022 for their
valuable feedbacks on earlier versions of this paper.
References
Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, and Cyril Zhang. Stochastic optimization
with laggard data pipelines. In Advances in Neural Information Processing Systems, 2020.
Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component
convexity and large epoch requirements. In Advances in Neural Information Processing Systems,
2020.
Christoph Aistleitner and Josef Dick. Functions of bounded variation, signed measures, and a general
Koksma-Hlawka inequality. arXiv:1406.0230, 2014.
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron C. Courville, and Yoshua Bengio. Variance
reduction in SGD by distributed importance sampling. arXiv:1511.06481, 2015.
Haim Avron, Vikas Sindhwani, Jiyan Yang, and Michael W. Mahoney. Quasi-Monte Carlo Feature
Maps for Shift-Invariant Kernels. The Journal ofMachine Learning Research,17:120:1-120:38,
2016.
Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms
for machine learning. In Advances in Neural Information Processing Systems, pp. 451-459, 2011.
Bernard Bercu, Bernard Delyon, and Emmanuel Rio. Concentration inequalities for sums and
martingales. SpringerBriefs in Mathematics. Springer, 2015.
Dimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Opti-
mization: A Survey. In Optimization for Machine Learning. The MIT Press, 2011.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks ofthe trade, pp. 421-436.
Springer, 2012.
Leon Bottou, Frank E. Curtis, and Jorge NoCedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Alexander Buchholz, Florian Wenzel, and Stephan Mandt. Quasi-Monte Carlo Variational Inference.
In Proceedings of the International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pp. 667-676. PMLR, 2018.
Russel E. Caflisch. Monte Carlo and quasi-Monte Carlo methods. Acta Numerica, 7:1-49, 1998.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
transactions on intelligent systems and technology (TIST), 2(3):27, 2011.
Jingrun Chen, Rui Du, Panchi Li, and Liyao Lyu. Quasi-Monte Carlo sampling for machine-learning
partial differential equations. arXiv:1911.01612, 2019.
Dami Choi, Alexandre Passos, Christopher J. Shallue, and George E. Dahl. Faster neural network
training with data echoing. arXiv:1907.05550, 2019.
S.-C. T. Choi, F. J. Hickernell, M. McCourt, and A. Sorokin. QMCPy: A quasi-Monte Carlo Python
library, 2020+. URL https://github.com/QMCSoftware/QMCSoftware.
Mikolaj Cieslak, Christiane Lemieux, Jim Hanan, and Przemyslaw Prusinkiewicz. Quasi-Monte
Carlo simulation of the light environment of plants. Functional Plant Biology, 35(10):837-849,
2008.
Christopher De Sa. Random reshuffling is not always better. In Advances in Neural Information
Processing Systems, 2020.
10
Published as a conference paper at ICLR 2022
Michael Drmota and Robert F/ Tichy. Sequences, discrepancies and applications. Springer, 2006.
Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient
descent. In Proceedings of the International Conference on Machine Learning, volume 119, pp.
2658-2667. PMLR, 2020.
Walter Gander. Algorithms for the QR decomposition. Seminar fur Angewandte Mathematik:
Research report, 80(02):1251-1268, 1980.
Mert Gurbuzbalaban, Asuman E. Ozdaglar, and Pablo A. Parrilo. Convergence rate of incremental
gradient and incremental Newton methods. SIAM Journal on Optimization, 29(4):2542-2565,
2019.
Mert Gurbuzbalaban, Asuman E. Ozdaglar, and Pablo A. Parrilo. Why random reshuffling beats
stochastic gradient descent. Mathematical Programming, 186(1):49-84, 2021.
Jeff Z. HaoChen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In Proceedings of
the International Conference on Machine Learning, volume 97, pp. 2624-2633, 2019.
Thomas P. Hayes. A large-deviation inequality for vector-valued martingales. Combinatorics,
Probability and Computing, 2005.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Tito Homem-de Mello. On rates of convergence for stochastic optimization problems under non-
independent and identically distributed sampling. SIAM Journal on Optimization, 19(2):524-551,
2008.
Wolfgang Jank. Quasi-Monte Carlo sampling to improve the efficiency of Monte Carlo EM. Compu-
tational statistics & data analysis, 48(4):685-701, 2005.
Corwin Joy, Phelim P. Boyle, and Ken Seng Tan. Quasi-Monte Carlo methods in numerical finance.
Management Science, 42(6):926-938, 1996.
Kenji Kawaguchi and Haihao Lu. Ordered SGD: A new stochastic optimization framework for
empirical risk minimization. In The 23rd International Conference on Artificial Intelligence and
Statistics, volume 108, pp. 669-679, 2020.
Alexander Keller. A Quasi-Monte Carlo Algorithm for the Global Illumination Problem in the
Radiosity Setting. In Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, pp.
239-251. Springer New York, 1995.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 10657-10665, 2019.
David A. Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Society, 2017.
Sifan Liu and Art B. Owen. Quasi-Newton Quasi-Monte Carlo for variational Bayes.
arXiv:2104.02865, 2021.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks.
arXiv:1511.06343, 2015.
Yucheng Lu, Youngsuk Park, Lifan Chen, Yuyang Wang, Christopher De Sa, and Dean Foster.
Variance reduced training with stratified sampling for forecasting models. In Proceedings of the
International Conference on Machine Learning, pp. 7145-7155. PMLR, 2021.
Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis
with vast improvements. In Advances in Neural Information Processing Systems, 2020.
11
Published as a conference paper at ICLR 2022
Deanna Needell, Rachel Ward, and Nathan Srebro. Stochastic Gradient Descent, Weighted Sampling,
and the Randomized Kaczmarz algorithm. In Advances in Neural Information Processing Systems,
pp.1017-1025, 2014.
Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten van Dijk. A
unified convergence analysis for shuffling-type gradient methods. arXiv:2002.08246, 2020.
Art B. Owen. Quasi-Monte Carlo sampling. Monte Carlo Ray Tracing: SIGGRAPH, 1:69-88, 2003.
Guillaume Papa, Pascal Bianchi, and StePhan Clemencon. Adaptive sampling for incremental
optimization using stochastic gradient descent. In Algorithmic Learning Theory, volume 9355 of
Lecture Notes in Computer Science, pp. 317-331. Springer, 2015.
Teemu Pennanen. Epi-convergent discretizations of multistage stochastic programs. Mathematics of
Operations Research, 30(1):245-256, 2005.
Pete Philipson, Graeme L. Hickey, Michael J. Crowther, and Ruwanthi Kolamunnage-Dona. Faster
Monte Carlo estimation of joint models for time-to-event and multivariate longitudinal data.
Computational Statistics & Data Analysis, 151:107010, 2020.
Benjamin Recht and Christopher Re. Toward a noncommutative arithmetic-geometric mean inequality:
Conjectures, case-studies, and consequences. In Conference on Learning Theory, volume 23, pp.
11.1-11.24, 2012.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathemati-
cal Statistics, 22(3):400 - 407, 1951.
Itay Safran and Ohad Shamir. How good is SGD with random shuffling? In Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research, pp. 3250-3284. PMLR, 2020.
Itay Safran and Ohad Shamir. Random shuffling beats SGD only after many epochs on ill-conditioned
problems. arXiv:2106.06880, 2021.
Mark Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. In Advances in Neural
Information Processing Systems, pp. 9918-9927, 2018.
Shu Tezuka. Discrepancy theory and its application to finance. In IFIP International Conference on
Theoretical Computer Science, pp. 243-256. Springer, 2000.
Florian Wenzel, Alexander Buchholz, and Stephan Mandt. Quasi-Monte Carlo Flows. In Proceedings
of the 3rd Workshop on Bayesian Deep Learning, 2018.
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael W. Mahoney. Quasi-Monte Carlo Feature
Maps for Shift-Invariant Kernels. In Proceedings of the International Conference on Machine
Learning, volume 32, pp. 485-493, 2014.
Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali H. Sayed. On the performance of random reshuffling
in stochastic learning. In 2017 Information Theory and Applications Workshop (ITA), pp. 1-5.
IEEE, 2017.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle SGD be better than
reshuffling SGD and GD? In Conference on Learning Theory, 2021.
Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the International Conference on Machine Learning, volume 69, pp.
116, 2004.
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized
loss minimization. In Proceedings of the 32nd International Conference on Machine Learning,
volume 37, pp. 1-9, 2015.
12
Published as a conference paper at ICLR 2022
A Appendix
Table of contents
A.1 Experiment details .......................................................... 14
A.1.1 Additional details for Section 6 ..................................... 14
A.1.2 t-test for data augmentation results ................................. 14
A.1.3 Small experiments .................................................... 14
A.1.4 Optimal step size derivation ......................................... 16
A.2 Convergence analysis: diminishing step size ................................. 18
A.2.1 Non-convex case ...................................................... 18
A.2.2 Strongly-convex case ................................................. 21
A.3 Proof for Lemma 1 ........................................................... 24
A.4 Convergence analysis: constant step size .................................... 28
A.4.1	Non-convex case .................................................... 28
A.4.2	Strongly-convex case ............................................... 29
A.5 Justifications for Assumption 2 under various example orderings ............. 30
A.5.1	Arbitrary permutation .............................................. 30
A.5.2	Shuffle once ....................................................... 30
A.5.3	Random reshuffling ................................................. 33
A.5.4	Random reshuffling with data echoing ............................... 37
A.5.5	Markov chain gradient descent ...................................... 38
A.5.6	QMC-based data augmentation with random reshuffling ................ 40
A.6 Miscellaneous lemmas ........................................................ 42
13
Published as a conference paper at ICLR 2022
A.1 Experiment details
A.1.1 Additional details for Section 6
In Section 6, all the training scripts are implemented via PyTorch1.6 and run on a single machine
configured With an 2.6GHz 4-core Intel (R) Xeon(R) CPU, 16GB memory and NVIDIA GeForce
GTX 1080Ti With CUDA 10.1.
In the example ordering comparison, We use the same seed among different algorithms in the same
run so as to guarantee every algorithm Works With the same loss function.
In the ImageNet experiment, the standard step size schedule for ImageNet training is starting at
0.1 and decaying by 10 every 30 epochs. The pre-trained model is the trained model at epoch 90.
Naturally, our learning rate should be 1e-4 by the same schedule. Other hyperparameters are adopted
by the open source implementation: https://github.com/pytorch/examples/tree/
master/imagenet.
In the data augmentation comparison, obviously other augmentation techniques can be used.
The strategies We used are taken from open source implementation https://github.com/
akamaster/pytorch_resnet_cifar10 and https://github.com/weiaicunzai/
pytorch-cifar100 that can reproduce the validation accuracy in He et al. (2016), so that our
comparison can be consistent With the correct benchmarks. BeloW We include the convergence plot
for the experiment on ImageNet With ResNet18, Where each algorithm is repeated three times With
seeds uniformly selected from [0, 1000].
SSo-I UOAeP=e>
/.08
50.9
/76
UOAeP=S I，dQL
Figure 4: Data augmentation with IID-uniform (standard) and QMC-based methods on ImageNet.
A.1.2 t-TEST FOR DATA AUGMENTATION RESULTS
We now perform t-test on the validation accuracy on two data augmentation results, to show the
improvement from QMC is statistically significant. We include validation accuracy at epoch 200
for CIFAR10 and best accuracy for CIFAR100 in Table A.1.2. We compute the p-value between
IID-Uniform and tuned QMC, and found the p-value on CIFAR10 and CIFAR100 to be 7e-7 and
0.036, respectively. Since they are both smaller than 0.05, we reject the null hypothesis which
concludes they are of the same mean.
A.1.3 Small experiments
For both experiments in Section 3, We generated w* from a standard Normal distribution. The
minibatch size is 1, and the total number of iterations is 107, with n = 104 over 1000 epochs for the
offline setting. For all variants, We use the theoretical step size optimized for SGD With replacement
(corresponding to IID Uniform) on this particular problem, given by
α = αt(I - αt)	With a =	kw*k2
t+1 — 1 - α2(d + 2)	0 — ∣∣w*k2(d + 2) + d,
Which are derived in Appendix A.1.4 beloW. To obtain loW-discrepancy samples from the Gaussian
distribution, We use the inverse transform method. First We obtain the QMC sequences (Sobol in our
14
Published as a conference paper at ICLR 2022
Table 2: Validation accuracy for different data augmentation methods on CIFAR datasets.
CIFAR10	CIFAR100
Runs IID-Uniform QMC QMC (tuned) IID-Uniform QMC QMC (tuned)
1	91.87	92.05	92.45	67.92	68.91	68.21
2	91.67	92.13	92.53	68.01	68.92	68.12
3	91.68	92.03	92.33	68.03	68.53	68.42
4	91.88	91.9	92.35	67.82	68.03	68.71
5	92.05	92.03	92.41	68.89	68.41	67.99
6	91.79	91.93	92.15	66.99	68.66	68.51
7	91.91	92.79	92.59	68.31	68.92	68.53
8	91.53	91.73	92.44	68.14	68.09	68.53
9	91.3	91.96	92.21	68.02	67.91	68.12
10	91.82	92.21	92.16	67.73	68.31	68.53
case) ζt ∈ [0, 1]s where s is the appropriate dimension, and then use that in the inverse CDF function
of a Gaussian distribution to obtain the corresponding Gaussian sample.
In addition to using synthetic data, we also performed an offline version of the experiment Figure 1(b)
on a real dataset, a6a from the LIBSVM repository (Chang & Lin, 2011). The dataset contains
n=11220 examples with d=124 features (including bias), and all labels are binary. We use logistic
regression with `2 regularization (with λ=1e-4) as the empirical risk:
1n	λ 2
Rn (w) = n∑log(1 + exp(-yi XTw)) + . ∣∣wk2.
i=1
To measure the left hand side of Assumption 2, we again fix wτ to be at the origin, and we take the
number of epochs to be 1000. The results are in Figure 5 for one run with an arbitrarily-set seed, and
we draw similar conclusions as observed with synthetic data (see Figure 1(b) in the main paper).
= (03UBA — (n。3UB>lo豆晶 __
Figure 5: Comparison of sampling schemes on '2-regularized logistic regression with real data.
15
Published as a conference paper at ICLR 2022
A.1.4 Optimal step size derivation
In this section, we derive the optimal step-size sequence used in our synthetic toy example from
Section 3. Recall our setup
X 〜N(0, Id), y = xTw* + e
where w* is the true model parameters that We are trying to recover, and e 〜N(0,1) is some intrinsic
error. The goal is to minimize the expected risk
f(w) = 2E[(xTw - y)2].
By using the tower rule of expectation conditioning on x, this objective can be written as
f(W) = 2(Hw - w*k2 + 1).
The full gradient and the example gradient given a pair (x, y) drawn from the above distribution are
Vf (W) = w — w*,
Vf (w; x, y) = (XTw — y)x = (XT(W — w*) — e)x.
Define the gradient error to be
∆g := Vf (w; X, y) — Vf (w) = (XXT — Id)(w — w*) — eX.	(4)
Let us first analyze the expected gradient error conditioned on any randomness in w that might arise
from the sampling history:
Ehk∆g k2 i = (w — w*)TE[(XXT — Id)2] — 2E[eXT(XXT — Id)](w — w*) + E[e2XTX].
The last term is simply E[e2XTX] = d since e is independent of X. The middle term cancels to 0 again
by independence and zero-mean of e. For the first term,
E[(XXT — Id)2] = E[XXTXXT — 2XXT + I]
= (d + 2)Id — 2Id + Id = (d + 1)Id .
Together, we have
Ehk∆gk2i = (d+1)kw—w*k2+d.	(5)
Now let us derive an suboptimality gap recursion for the SGD update step using the step size αt :
wt+1 — w* = wt — αtVf(wt; X, y) — w*
= (wt — w* ) + αtVf(wt) — αtVf(wt) — αtVf(wt; X, y)
= (1 — αt )(wt — w* ) — αt ∆tg ,
where ∆tg is the expression in Eq. (4) with wt as input. The expected suboptimality gap conditioned
on all randomness up to timestep t is then
Ehkwt+1 — w*k2i = (1 — αt)2Ehkwt — w*k2i — 2(1 — αt)E[((wt — w*)T∆tg] + αt2Ehk∆tgk2i.
Observe that under iid uniform sampling, E[∆tg] = 0, and using Eq. (5) gives us
Ehkwt+1 — w* k2i = (1 — αt)2 kwt — w* k2 + αt2(d + 1)kwt — w* k2 + αt2d.
Taking an expectation over the entire history to remove conditioning and let ρt := E kwt — w* k2 ,
ρt+1 = (1 — αt)2ρt + αt2 (d + 1)ρt + αt2d
= (1 — 2αt + αt2 (d + 2))ρt + αt2d.
(6)
Note that the RHS is convex in αt , we can differentiate to minimize the expected suboptimality at
every iteration ρt+1 = E kwt+1 — w* k2 :
0 = (—2 + 2αt (d + 2))ρt + 2αtd ⇒ αt
ρt
(d + 2)pt + d
16
Published as a conference paper at ICLR 2022
This also gives us an expression for ρt in terms of αt :
d
Pt = α-1 - (d +2),
which combined with Eq. (6) gives us
ρt+1 = (1 - αt)ρt = (1 - αt)
d
α-1 - (d + 2)
Finally, the optimal step-size sequence can be implemented via the following recursion
αt+1
1
α0
d
d + 2 +-----
ρt+1
d
d + 2 +---=
ρ0
d+2+
d+2+
αt-1 - (d+ 2)
1 - αt
d
∣∣W0 - w*k2
1
It is easy to verify that this is indeed a decreasing sequence. If we initialize at w0 = 0, then our step
sizes are given by
αt+1
αt(1 - αt)
1-α2(d + 2) with α0
kw*k2
k w*『(d + 2) + d
17
Published as a conference paper at ICLR 2022
A.2 Convergence analysis: diminishing step size
In the main text, our convergence results focused on the constant step size regime for simplicity of
presentation and ease of comparison to prior works, as they often lack diminishing step size results
for permutation-based SGD variants. Here we formally present the convergence rate under our main
Assumption 2 using a diminishing step size sequence. In Theorems 3 and 4 that we present below,
the rate is of the same order as what we obtained under a constant step size (Theorems 1 and 2) up to
a factor of log(T) that typically arises when using a diminishing step size.
Our analysis for the diminishing step size setting is accomplished by breaking the total number of
iterations into phases. Suppose we want to run a total of T iterations of SGD updates. We will break
the analysis into I number of phases. In each phase i = 0, . . . I - 1, we run K(i)m(i) number of
iterations with constant step size α(i). The step size is decayed at the beginning of each phase. For
this, we define the triplets (α(i), m(i), K(i)), where m(i) is an increasing sequence.
Recall that γ and Φ are constants in Assumption 2. In the non-convex case, the inner interval length
is chosen to be
m(i) = d3(Φ+1)%]2i,
where as for the strongly-convex setting it is
mCi) = d3(Φ+1产∙ eig
For both function classes the diminishing step-size is set to
α(i) = 一.
6Lm(i)
(7)
A.2.1 Diminishing step size: non-convex case
Theorem 3 (Diminishing step size: non-convex case). Suppose that our setup satisfies Assumptions 1
and 2, and let > 0 be any target error. After T iterations of SGD (Eq. (1)) with a diminishing
step-size (Eq. (7)), we can obtain
t=0minT-IkVf(Wt)『≤ 12log2 (^ɪ T)
L∆ + C 2(3(Φ+ 1))-2
TY/1+，(12(中 + l)2)-γ4+γ - 1
。(T
This implies the number of gradient evaluations to achieve mintkVf(wt)k2 ≤ 2 is
T
三-2(1+γ)
O(e -Y-).
Proof. To apply Lemma 1 to the iterations within a particular phase i, we need
3Φ ≤ mγ/2	and α ≤ —1—
6Lm
where we have temporarily dropped the index (i) for convenience. Lemma 1 gives us the following
bound on the objective value between any length-m number of iterations within that phase: for
τ ∈ {0, m - 1, 2m - 1, . . . , (K - 1)m - 1},
f(wτ+m) ≤ f(wτ) - ηkVf(wτ)k2 + 2C2α2m2-γ
4η
≤ f(wτ) - α4mkVf(wτ)k2 + 2C2αm1-γ,
since the step size is constant throughout one phase. Summing over K such intervals from τ = 0
followed by a telescope,
K-1
α4m X kVf (Wmk)k2 ≤ f (wo) - f(wmκ-i) + 2C2Kαm1-γ.
k=0
18
Published as a conference paper at ICLR 2022
We now restore our phase index,
α⑴m⑴K⑴1
4	K(T
K(i)-1
E kVf(Wmk)k2
k=0
≤ f(w0) - f(wm(i)K(i)-1) + 2C2K (i)α(i)(m(i))1-γ
C2
f (WO) - f (wm(i) K(i) -1) + 歹 K (m )	,
3L
where we have chosen the largest possible α(i) = 1/6Lm(i) for all i. Summing over I phases and
letting ∆ := f (wo) — f *,
I-1
X
i=0
α(i)m(i)K (i)	1
4	K (i)
K(i)-1	C 2 I-1
E kVf(wm(i)k)k2 ≤ I△+ 元汇 K⑴(m⑴)-
k=0	i=0
where the left hand side can be further lower bounded with
X a^mTK1 t=0minT-IkVf(Wt)k2 ≤ I δ+ CL2 X K(M))-Y,
i=0	,,	i=0
using the inclusion that 0, m(i) — 1,..., K (i)m(i) — 1}I=CI ⊂ {0,...,T — 1}. Now let us choose
m(i) = d(3(Φ + 1)产］2i
K(i) = d2iγe.
Here the choice of m(i) guarantees that
(m(i) )γ/2 =卜(3(Φ + 1))2勺2)”2
=d(3(Φ + 1)产eY/2(2i )γ/2
≥ (3(Φ+ 1))2iγ/2 ≥ 3Φ.
Next, observe that for the last term in the previous sum,
I-1	I-1
X K(i) (m(i))-Y = Xd2iγ e(d(3(Φ+ 1))2/y e2i)-γ
i=C	i=C
I-1
≤ X(2iγ + 1)((3(Φ + 1))2/y2i)-γ
i=C
I-1
≤ X2(2iγ)((3(Φ + 1))2/y2i)-γ
i=C
I-1
= X 2(3(Φ + 1))-22iγ2-iγ
i=C
= 2I(3(Φ + 1))-2.
Combining with the above,
I-1 α(i)m(i)K(i)	2C 2
X -4— 一严号 JVf(Wt)k2 ≤ I△+行∙ I(3(Φ +1))-2.
4	t=C,...,T -1	3L
i=C	,,
Furthermore, using our choice of α(i)
(8)
I-1
X
i=C
a(i)m(i)K(i)
4
1 I-1
61L X K⑴
i=C
19
Published as a conference paper at ICLR 2022
Re-arranging Eq.(8),
min	kV∕(wt)k2 ≤
t — 0 ,...,T	1
6IL∆+ § C2 ∙ I (3(Φ+ 1))-2
P— Ka)
(9)
The total number of iterations is given by
I-1
T = X K⑶m⑴
i—0
I-1
=Xd2iγ]∙d(3(Φ + 1))2∕2i
i—0
I-I
≥ (3(Φ + 1))2A X 2(γ+1)i
i—0
ɔ. 2(y+1)i — 1
= (3(φ + 1))2/' 2y+i - 1	≥
Solving for I and using lg to denote log§, we obtain
2()+1)i - 1
(3(Φ + 1))2∕γ	2γ+ι
i ≤ Ul)Ig
2y+1
(3(Φ + 1))2∕γ
T+1)≤ OIg
2y+2
(3(Φ+ 1))2∕γ
T .
Moreover, We can also upper bound T using similar arguments,
I-1
T = X∣^2iγ] ∙ ∣^(3(Φ + 1))2∕[2i
i—0
I-1
≤ X2(2iγ) ∙ 2(3(Φ+ 1))2/Y2i
i—0
I-1
4(3(Φ + 1))9 X 2(γ+1)i
i—0
4(3(Φ + 1)*γ
2(y+1)i
2y+1 - 1
which gives
I≥
/ ∖
——)lg —Ki - 1— T
Y +V g 4(3(Φ+1))2∕γ
V~—z~}
We are left to bound
I-l
X K⑶
i—0
I-l
X"
i—0
I-1
≥ X 2iγ
i—0
2γl - 1
2y - 1
2Y∕ι+γ lg(ΓT) - 1
≥----------------
一	2Y
(ΓT)γ∕1+γ - 1
=	2Y
(2Y+1-1	T 丫'+γ
= 3(3(φ+i))2% J
—	2Y
20
Published as a conference paper at ICLR 2022
using our lower bound for I . Substituting this and the upper bound for I into Eq. (9),
t=0minT-Iw(Wt)『≤
≤
6IL∆+ 32C2 ∙ I(3(Φ+1))-2
PI-0 K ⑶
(ɪʌ 1 (	2γ二2	八 6L∆+ ∖C2(3(Φ + 1))-2
G + " g1(3(Φ + 1)/)(2γ+i-2 τ["+Il
∖4(3(Φ + 1))∙γ )_
2y
≤
(16 T ∖ L∆ + C2 (3(Φ + 1))-2
3(Φ + 1) C 2y+1-1	TY/1+y _1
4(3(Φ+1)芦Y
≤
L∆ + C 2(3(Φ + 1))-2
Tγ∕1+γ(12(Φ + 1)2)-γ∕1+γ - 1
121g(丽” T
which yields our convergence rate in the nonconvex setting.
□
A.2.2 Diminishing step size: strongly-convex case
Theorem 4 (Diminishing step size: Strongly-ConveX case). Suppose f is μ-Polyak-Lojasiewicz (PL),
and that our setup satisfies Assumptions 1 and 2 Let e > 0 be any target error, and K = L∕μ be the
condition number of the problem. After T iterations of SGD (Eq. (1)) with a diminishing step-size
(Eq. (7)), we can obtain
f(wτ) - f * ≤
e1/ -1
2∣^12κ∣(3(Φ+ 1))%
4C2 (3(Φ+1))-2、= O(ɪ、
μ e(1 - ρ)d12κ1	TYJ
This implies the number of gradient evaluations to achieve f(wT) - f* ≤ e2 is
T = O(e-2/Y).
Proof. We will begin with the same analysis technique as used in the non-conveX case. To apply
Lemma 1 to the iterations within a particular phase i, we need
3Φ ≤ mγ/	and a ≤ —∣—
6Lm
dropping the indeX (i) for convenience and will re-introduce it later when appropriate. Lemma 1
gives us the following bound on the objective value between any length-m number of iterations
within that phase: for τ ∈ {0, m - 1, . . . , (K - 1)m - 1},
f(wτ+m) ≤ f(wτ) - ηkVf(wT)k2 + 2C2α2m2-γ
4η
≤ f(wτ) - α4mkVf(wτ)k2 + 2C2αm1-γ,
since the step size is constant throughout one phase. Strong-convexity (or the Polyak土OjaSieWiCZ
(PL) inequality) of f implies IlVf(W)『≥ 2μ(f(w) - f *) for all W ∈ Rd. Using this while
subtracting f * on both sides leads to
f(wτ+m) - f * ≤(1 - *) (f (wτ) - f *) + 2C2αm1-γ.
21
Published as a conference paper at ICLR 2022
Applying this recursively K times gives
K	K-1
f(wmK) - f * ≤(1 -詈)(f (wo) - f*) + 2C2ɑm1-γ X(1
k=0
αmμ∖ k
2 )
—
∞
≤ (1 — 詈)K(f (wo) — f *)+2C2ɑm1-Y X(I-等)k
k=0
= (1- ιμL) (f(wo)- f*) + 2C2αm1-γαmμ
= (l- ⅛) (f (wo) - f *)+4C2m-γμ-1,
where we have used
V 1	αmμ V μ ɪ
Q — 6Lm	>	2 - 12L <
as μ ≤ L.
We now restore the phase index. Letting Ti := Pii0-=1o m(i0)K(i0) be the total number of iterations
passed after i phases so that T = TI, and ∆t := f(wt) - f*,
*	1	K(i)
f(wTi+1)-f*≤ 11 -
4C2
∆Ti + — (m(i))-γ
μ
Now let us choose
m⑴=d(3(Φ + 1))2Α ∙ e"γ]
K⑴=K = d12κ] = d1	-~∖,--e ≥ 12.
1	1	1 lθg(1 - 1∕l2κ) 1 _
Here the choice of m(i) guarantees that
(m⑴)γ/2 = (d(3(Φ + 1))% ∙ e"γ])”2 ≥ ((3(Φ + 1))也)”2/2 ≥ 3Φ.
Using the constant K across all phases, our recursion can be simplified to
f (WTi+1) - f * ≤ (1 - :ɪ^ δT + -7—(m(i))-γ.
∖	12K I	μ
Since this holds for any i = 0, . . . , I - 1, applying recursion over I phases yields,
4C2 I-1
f (wτ) - f * ≤ (I-P)KI∆o + — X(1 -P)Ki(m(I-i))-γ
μ i=0
(10)
where We defined P := 1∕12κ. Observe that for our choice of m(i) and K,
I-1	I-1
T = X K⑴m⑴=KXd(3(Φ +1))2∕γ ∙ ei∕γ]
i=o	i=o
I-1
≤ KX 2(3(Φ + 1))% ∙ ei/Y
i=o
I-1
=2K(3(Φ+1))2∕γ X ei/Y
i=o
=2K(3(Φ+1))% I/, _ 1) ≤ 2K(3(Φ + 1)/ eι∕γ
e1∕γ - 1	l	e -	e1∕γ - 1
which implies
e1∕γ - 1
≥ Yog(2K(3(Φ + 1))2/，TJ,
22
Published as a conference paper at ICLR 2022
and so
(I-P)KI ≤ exp(~PKI) ≤ exp(-ρKγ ∙ log(2K(e(Φ + 1)产 T))
note that
-1
PKY = YPd ιogD e≥ γ∙
This gives us
(1 - P)KI ≤ (2K(e[Φ + 1))2/，) YT-y∙
(11)
Substituting this into Eq. (10),
e	e1∕γ _ 1	、-γ	4C2 上	，、
K(WT)-f* ≤δ0(2K(3(Φ+1))2∕γ)	T-Y + -μ-X(I-P产”尸.
(12)
For the last term,
I-1	I-1
X(1 -P)Ki(m(IT))-Y = X(1 -P)Kid(3(Φ+1))2/Y ∙ J-"1-Y
i=0	i=0
I-1
≤ (3(Φ + 1))-2 X(1 - P)KieiT
i=0
I-1
(3(Φ + 1))-2e
(3(Φ + 1))-2e
T £(e(1-P)K)i
i=0
-I (e(1-P)K)i
e(1 - P)K
=(3(Φ + 1))-2 (I - P)K
(3(卬 +I)) e(1 - p)k ∙
Substituting these into Eq. (12), and using Eq. (11) to bound (1 - P)KI gives us the convergence rate
for strongly-convex functions using a diminishing step size:
于(WT) - f * ≤ ∆(1 - P)KI + -y-(3(φ+ I))-2(：	K
μ	e(I - P)
≤ (1 - p)ki ∆ +
4C2∖ (3(Φ+1))-2
≤ (2∣^12κ](3(Φ+ 1))2∕γ
e(1 - P)
1△ +
K
4。2 (3(Φ+1))-2、T-Y
μ e(1 — p) d 12κ])
O
□
23
Published as a conference paper at ICLR 2022
A.3 Proof for Lemma 1
We now prove the lemma that bounds the evolution of SGD over a length-m analysis phase. This
lemma is the key to our convergence analyses in both the diminishing and constant step size regimes.
Before we proceed, we first state and prove the following bound on the gradient error when scaled by
a nonincreasing sequence.
Lemma 2. Suppose Assumption 2 holds. If {ρt} is a deterministic, nonincreasing, and nonnegative
sequence, then
τ+m-1	2
X Pt (Vf(WT； Xt)-Vf(WT))	≤ PT ∙ m2-γ ∙ (C2 + Φ2kVf(wτ川2).
t=τ
Proof. Let βt = Pt - Pt+1 for t ∈ {τ, τ + 1, . . . , τ + m - 2}, and let βT +m-1 = Pt+m-1. Observe
that these are all nonnegative, and
T +m-1	2
X Pt (Vf (WT; xt) - Vf (WT))
t=T
T +m-1 T +m-1
= XX βk (Vf(WT； Xt)-Vf(WT))
t=T k=t
T +m-1 k
X X βk (Vf(WT ； Xt) - Vf (Wt ))
k=T t=T
T +m-1 k
X 竺 X (Vf(WT； Xt)-Vf(WT))
k=T PT t=T
2
Applying Jensen’s inequality using PTk=+Tm-1 βk = PT,
T +m-1
X Pt (Vf (WT; Xt) - Vf (WT))
t=T
T +m-1
≤ PT2	X
k=T
βk
pT
(Vf(WT;Xt) - Vf (WT))
t=T
T +m-1
PT X βk (k - τ + 1)2
k=T PT
1k
(---7 £ (Vf(WT ； Xt) - Vf(WT ))
k-τ +1
t=T
2
Applying Assumption 2 on the squared norm,
T +m-1
X Pt (Vf (WT; Xt) - Vf (WT))
t=T
2
T +m-1
≤ pT X βk(k -T +1)2 • “—-- (C2 + Φ2kVf(Wτ)k2
T k=T PT	(k-τ+ 1)γ
T +m-1
=PT X β ∙ (k - τ +1)2-γ ∙ (C2 +Φ2kVf(Wτ)『)
k=T PT
T +m-1
≤ PT X β ∙ m2-γ ∙ (C2 + Φ2kVf(Wτ)『)
k=T PT
=pT ∙ m2-γ ∙ (C2 + Φ2kVf(Wτ)『),
which is what we wanted to show.
□
24
Published as a conference paper at ICLR 2022
We now recall the statement for Lemma 1. In the main paper, we state the lemma with constant step
size. In the following proof, we prove it holds for non-decreasing step size, where constant step size
will hold naturally as a special case. For this, we define η := Ptτ=+τm-1 αt .
Lemma 1. Suppose our setup satisfies Assumptions 1 and 2 and that we use a constant step size
a For all timesteps T ≥ 0, let m > 0 be some integer such that 3a「m1-γ/2 Φ ≤ η ≤ 笠.Then the
objective at timestep τ + m is bounded by
f(wτ+m) ≤ f (wτ) — 弓kVf(wT)k2 + 2αTm2-γC2.
4	ητ
Proof. Let η and g denote
τ +m-1
η :=	αt
t=τ
and
τ +m-1
αtVf(wt; xt).
t=τ
1
g —-
η
This means that wτ+m = wτ - ηg. From Assumption 1 (L-Smoothness),
f(wτ+m) = f(wτ - ηg)
≤ f(wτ)-ηhVf(wτ),gi + η2Lkgk2
=f (wτ) - 2kVf(wτ)『一2kgk2 + 2kg - Vf (wτ)k2 + η2Lkgk2
≤ f (wτ) - 2kVf(wτ)k2 + 2kg - Vf (wτ)k2,
where the last inequality follows because we assumed ηL ≤ 1/6 (< 1). Next, observe that
2 kg-Vf(wτ )k2
12
而 kηg-ηVf(WT)k
1
2η
T +m-1
X αt (Vf(wt; xt) - Vf (wT))
t=T
1 T +m-1	2	1
≤ n	^X αt (Vf(WT; Xt)-Vf(WT))	+ n
T +m-1
αt (Vf(Wt;xt) - Vf(WT;xt))
t=T
To bound the second term,
1
η
T +m-1
αt (Vf(Wt;xt) - Vf(WT;xt))
t=T
2
=η
T +m-1
X - (Vf(Wt； xt) - Vf (wτ； Xt))
t=T	η
2
T +m-1
≤ η X 也k(Vf(wt；xt) - Vf(wτ； χt))k2
t=T	η
T +m-1
≤ X αtL2kWt - WTk2.
t=T
Combining the above,
2 kg-Vf (wτ )k2 ≤1
T +m-1
X αt (Vf (WT； Xt) - Vf (WT))
t=T
T +m-1
+ X αt L2 kWt - WTk . (13)
t=T
Applying Lemma 2 on the first term in Eq. (13) gives
1
η
T +m-1
X αt (Vf(WT；Xt) - Vf (WT))
t=T
≤
2
1 αTm2-γ (C2 +Φ2kVf(wτ)『).
25
Published as a conference paper at ICLR 2022
For the second term in Eq. (13),
τ +m-1	τ +m-1
X αtL2kwt-wτk2= X αtL2
t=τ	t=τ
t-1
EaU▽/(wu； Xu)
=τ
2
τ +m-1
≤ 3	Xαt L2
t=τ
t-1
E au (Vf(Wu； XU)- V/(wτ； XU))
u=τ
2
τ +m-1
+ 3 X αtL2
t=τ
t-1
X au (Vf(WT ； XU)- Vf (Wτ))
u=τ
2
τ +m-1	t-1
+3	XαtL2XαuVf(wτ)
t=τ	u=τ
2
τ +m-1
≤ 3	Xαt L4
t=τ
wτ k2
τ +m-1
+ 3 X atL2 ∙ aTm2-γ (C2 + Φ2∣∣Vf(wτ)『)
t=τ
τ +m-1	t-1	2
+3	X αtL2	Xαu	kVf(wτ)k2,
t=τ	u=τ
where the second term in the last inequality follows from Lemma 2. Continuing,
τ +m-1	τ +m-1	t-1
X αtL2kwt - wτk2 ≤ 3 X αtL4η Xαukwu - wτk2
t=τ	t=τ	u=τ
τ +m-1
+3 X αtL2ατ2m2-γ C2 +Φ2kVf(wτ)k2
t=τ
+ 3η3L2kVf (wτ)k2
τ +m-1
= 3η2L2 X αtL2kwt - wτk2
t=τ
+3L2ηατ2m2-γ C2 +Φ2kVf(wτ)k2
+ 3η3L2kVf (wτ)k2.
This implies that
τ +m-1
(1 - 3η2L2) X αtL2 kwt - wτ k2 ≤ 3L2C2ηατ2 m2-γ
t=τ
+ (3η3L2 + 3L2ηaTm2-γΦ2) IlVf(WT)『.
And because by our assumption, Orm2-γΦ2 ≤ ¾-, it follows that
T +m-1
(1 - 3η2L2) X αtL2kwt-w"∣2 ≤ 3L2C2ηαTm2- + 10η3L2∣Vf (wτ)『.
t=T
Since We also assumed η ≤ 看,it follows that 3η2L2 ≤ *,and so
T +m-1
1-行)X OtL2Iwt - WTk2 ≤ 3L2C2ηaTm2-γ + —η∣Vf (wτ)『，
12	54
t=T
26
Published as a conference paper at ICLR 2022
which gives
τ +m-1
X atL2kwt - wτ∣∣2 ≤ ιι ∙ 3L2C2ηαTm2 Y + ιι ∙ 54ηkVf(WT)k2
t=τ
=36 L2C 2ηαT m2-γ + 10 ηkvf (wτ )k2.
11	99
So, putting this all together,
f(wτ+m) ≤ f(wτ) - η kVf (Wτ )k2 + 1 αT m2-γ (C2 + Φ2∣Vf (wτ )∣2
2	ηT
+ 36 L2C 2nαT m2-γ +10 ηkvf (WT )k2
11	99
=f(wτ) - η kVf (Wτ)k2 + 1 αTm2-γC2
2	ηT
+ 1 αT m2-γ Φ2∣Vf(wτ )k2
ηT
+ 36 L2C 2nαT m2-γ +10 nkVf (WT )k2
11	99
≤ f(wτ) — η ∣Vf(wτ )k2 + 1 αT m2-γ C2
2	ηT
+ 1 η∣Vf(wτ )k2
9
+ 4L2C 2ηαT m2-γ + 10 η∣Vf(wτ )∣2
99
=f(wτ) — η ∣Vf(wτ )k2 + 1 αTm2-γ C2
2	ηT
21
+ 4L2C 2ηαT m2-γ +-- η∣Vf(wτ )∣2
99
≤ f(wτ) — η∣Vf(wτ)k2 + 1 αTm2-γC2
-	ηT
+ ɪ C CaT m2-γ + 1 η∣Vf(wτ )k2
4η	T	4
≤ f(wτ ) - 4 kVf(wT )k2 + 2
η
ατ2m2-γC2.
This is what we wanted to show.
□
27
Published as a conference paper at ICLR 2022
A.4 Convergence analysis: constant step size
A.4. 1 Constant step size: non-convex case
Theorem 1 (Non-convex case). Suppose that our setup satisfies Assumptions 1 and 2, and let > 0
be any target error. Using SGD (1) with a constant step size α = 笠 ∣"(4C∕e + 3Φ)2∕γ] ,the
numberof steps T needed to achieve mint=。,…，t-ι ∣∣Vf (Wt)II2 ≤ e2 is at most
T = J4Lδ] ∙ l(4C + 3Φ)2∕[ = O (C+L∆ + φ⅛δ + 亭 + Φ2∕γ).
Proof. To satisfy the requirements of Lemma 1 in the constant step size, constant m case, we need
3αm1-γ∕2Φ ≤
mα ≤ ɪ-.
This breaks apart into first a constraint on only m:
3Φ ≤ mγ∕2,	(14)
and then a constraint on α in terms of m:
ɑ ≤ --—.
6Lm
In the nonconvex setting, we invoke Lemma 1 followed by summing up over K phases and telescop-
ing,
αm EkVf(Wmk)I2 ≤ f(wo) - f* + 2Kα2m2-γC2
4	αm
k=0
= f(W0) - f* +2Kαm1-γC2,
which gives a rate of
1 KX	2 / 4(f (WO)- f * )	-Y厂2
K UVf(Wmk )k ≤ αmK +8m Y C.
k=0
If we want to minimize the right side, observe that we will always want to set α as large as possible,
i.e. set α = 1/6Lm. This gives
1 K-1
K EkVf(Wmk)k2 ≤
k=0
24L(f(W0)- f *) +8m-YC2
K
Now, for this to all be less than e2, it suffices for
—*)≤ W and 8m-YC2 ≤ W.
K	2	2
The former occurs when
48L(f(Wo)- f *)
K ≥---------e2-------，
while the latter happens when
4C
4C ≤ mγ∕2.	(15)
e
If we let ∆ = f(W0) - f*, using Eqs. (14) and (15) and taking the minimum m and K required, we
can bound the number of iterations as
48L∆
C2∕γ	2
+ K+φ∕γ
T=mK≥
So,
T _ O (C2∕γL∆ + L∆Φ2∕γ
( e2+2∕γ	e2
where we suppose that γ acts as a constant.
□
28
Published as a conference paper at ICLR 2022
A.4.2 Constant step size: strongly-convex case
Theorem 2. Suppose that f satisfies the μ-PL condition and our setup satisfies Assumptions 1 and 2.
Let e > 0 be any target error, and K = L∕μ be the Condition number of the problem. Using SGD
(1) with a constant step size a =笠 ∣"(8C 2∕(μe2) + 9Φ2)1∕γ "∣-1, the number of steps T needed to
guarantee f(WT) - f* ≤ e2is at most
T =[12K log (等)]∙
+ 9Φ2 /γ
O (μCγ⅛ + κΦ2∕γ + K
Proof. The strongly convex setting has the same constraints on m and α as the non-convex setting.
Recall that μ-strong convexity or μ-PL of f implies that for all x, kVf (x)k2 ≥ 2μ(f (x) - f*).
Applying this to the result of Lemma 1 gives
f (wτ+m) ≤ f (wτ) -
等(f(wτ) - f*)+2ɑm1-YC2,
which is equivalent to
f (WT+m) - f * ≤ ( 1 -
αmμ) (f(wτ) - f*) + 2αm1-γC2.
Applying this recursively over K intervals of length m starting from τ = 0,
αm K	K-1
f (WmK) - f * ≤ (1----2^^^^) (f (WO)- f *) + 2αm1-γC2 ^X (1 -
k=0
αmμ∖ k
2 )
≤ exp	α αmμK、 ---)	(f(WO) -f*)+2αm1-γC2		∞ X1- k=O	αmμ∖ 2 )	k
= exp	α αmμK、 ---2-)	(f(WO)	-f*)+2αm1-γC2	1-1	amμ -	 2	-1
= exp	f amμK、 、	2-)	(f(WO)	-f*)+2αm1-γC2	2 •	 αmμ		
= exp	α αmμK、 ---)	(f(WO)	4C2 -f *) + —m-γ. μ			
As in the non-convex case, it best advantages us to set α as large as possible. Setting
f (WmK ) - f * ≤ exp (- 12L ) (f (WO) - f *) +------------------------m-γ.
For this to be less than e2, it suffices for each of these two terms to be less than 2∕2.
we would need
α = 1/6Lm gives
In the latter case,
The former case holds when
8C2 ≤ mY.
(16)
12L	2∆	2∆
K ≥ 丁 logG)=12κ l°g( 百
where K = L∕μ. Applying a ceiling, using the minimum requirement on K and m from Eq. (16)
above and Eq. (14) required in Lemma 1, we can lower bound the total number of iterations as
T = mK ≥
Ignoring logarithmic terms gives
12K log
∖1∕γ-
+ 9Φ2
8C2
μe2
□
29
Published as a conference paper at ICLR 2022
A.5 Justifications for Assumption 2 under various example orderings
A.5.1 Proof for Proposition 1 (Arbitrary permutation)
Proposition 1. Let Assumptions 1 and 3 hold, and suppose that we are using a permutation-based
method of sampling, that is, any method such that xkn, xkn+1, . . . , xkn+n-1 is a permutation of
x(0) , x(1) , . . . , x(n-1) for all epochs k ≥ 0. Any such method satisfies Assumption 2 with γ = 2,
C2 = n2A2 and Φ2 = n2B2.
Proof. This result follows trivially from Assumption 3. Consider an arbitrary sum of gradient errors
going from τ to τ + m - 1:
τ +m-1
X Vf(Wt； xσ(t)) - Vf(WT),
t=τ
where σ(t) is the index into the training set given by the permutation σ used at time step t. Since the
interval {τ, τ + 1, . . . , τ + m - 1} is arbitrary, this m can potentially be greater than n, the epoch
size. We can split this interval up as follows. Let τ1 be the first epoch boundary in the interval, such
that all t going from τ to τ1 - 1 are within the same epoch as Wτ , or else τ1 = τ + m if there is no
epoch boundary in the interval. Let τ2 be the last epoch boundary in the interval, such that all t going
from τ2 to τ + m - 1 are within a later epoch than Wτ (it may be the case that τ1 = τ2). Then the sum
τ2 -1
X Vf(WT； Xσ(t)) — Vf(T)
t=τ1
must be zero, since this interval goes over full epochs. This leaves us with
T +m-1
X Vf(WT； xσ(t)) - Vf (WT)
t=T
T1 -1
≤ X Vf(WT； xσ(t)) - Vf(WT)
t=T
≤ n(A2 + B2kVf(wτ)『)1/2.
T +m-1
+ X Vf(WT； xσ(t)) - Vf(WT)
t=T2
where we have used the fact that the intervals {τ, . . . , τ1} and {τ2, τ + m - 1} are of length at
most bn/2c (if such interval is of length greater than bn/2c, then bounding its norm is equivalent to
bounding the sums of the remaining terms, for which there are at most bn/2c of them). Therefore
1 T +m-1
m X	Vf(Wt； xσ(t)) -Vf(WT )
t=T
2
≤ mn2(A2 + B2kVf(Wτ)k2),
and so Assumption 2 is satisfied with γ = 2, C2 = n2A2, and Φ2 = n2B2 .
□
A.5.2 Proof for Proposition 2 (Shuffle once)
Proposition 2. Suppose that we are using the shuffle once variant of SGD to learn over a region
B ∈ Rd of radius at most R, such that the iterates Wt are guaranteed to remain within this region.
Assume that for all W ∈ B and all examples x in the training set of size n, Assumption 1 (L-
Smoothness) and Assumption 3 hold. Then with probability at least 1 - p, Assumption 2 holds with
Y = 2, C2 = O(dA2(n + B2)), and Φ2 = O(ndB2).3
Proof. We begin by invoking Lemma 6 (Permuted vector Hoeffding inequality) with
Xi,j
Vf(W； Xij) — Vf(W)
(a2 + B2kVf(W)k2)1/2
3The probability P is only present in log terms in these expressions, so it does not appear in the O.
30
Published as a conference paper at ICLR 2022
where we use xi,j to denote the j-th example after time step i, for all i ∈ {1, . . . m}, j ∈ {1, . . . , n},
with n ≥ m > 0. Note that here we are using a fixed w ∈ B that does not depend on the permutation
σ used in shuffle once. Clearly, Pjn=1 Xi,j = 0 for all i due to the periodicity in shuffle once, and
kXi,j k ≤ 1 by our assumption. Therefore the requirements of Lemma 6 are satisfied, which gives
the following high probability bound for some > 0 on any sequence drawn without replacement:
m
X Vf(W； xσ(t)) - Vf(W)
t=1
P
≥ e2(A2 + B2||Vf(w)『))≤ 2e2exp (-嘉
Using the periodicity property of shuffle once, i.e. if σ is the drawn permutation at the beginning of
training then σ(t + n) = σ(t), which allows us to arbitrarily shift the starting time step in the above:
τ +m-1
Vf (W; xσ(t)) - Vf (W)
≥e2(A2 + B2kVf(w)k2)j ≤ 2e2exp (-嘉
Consider an arbitrary interval of length m out of the permutation. As all of the n(n-1)/2 intervals are
equivalent to an interval of size at most bn/2c, by the sum to 0 property, we only need to consider
intervals with m ≤ n/2. This gives us
τ +m-1
X Vf (W; xσ(t)) - Vf (W)
t=τ
P
≥ e2(A2 + B2kVf (w)k2)j ≤ 2e2 exp (- 16n
By a union bound across all intervals we have
P ∃τ,m ∈ Z,
τ +m-1
X Vf (W; xσ(t)) - Vf (W)
t=τ
2
≥ 2
+ B2kVf(W)k2
≤ e2n2 exp (——e—
16n
Unfortunately, we cannot take a union bound over Rn or B (or even just over {W0, . . . , WT }), since
the set may contain iterates visited by shuffle once that depend on σ, and thus Lemma 6 may not hold.
Instead, suppose that we cover the whole region B with balls of radius δ. We will be able to do this
with (1 + 2R∕δ)d balls by Lemma 7 (e-netIemma). So, if the centers of the balls form a set W ,then
P ∃τ, m ∈ Z, W ∈ W,
τ +m-1
X	Vf(W xσ(t)) - Vf(W)
t=τ
≥e2(A2 + B2kVf(W)k2)j
≤ e2n2 (1 + 2R)∖xp (— 与
δ	16n
Note that the centers we use to cover the region is completely independent of the running algorithm,
therefore the union bound over Lemma 6 can be applied. Next, consider some Wτ that is not
necessarily the center of a ball. The function
τ +m-1
X Vf(Wτ; xσ(t)) - Vf(Wτ)
t=τ
is Ln-Lipschitz continuous, because each of the components is L-Lipschitz, and we can sum up only
at most n/2 of them. Let W ∈ W be such that k WT 一 Wk ≤ δ. Adding and subtracting Vf (W;。⑴)
and Vf (W), applying triangle inequality followed by Lipschitz continuity, We have
τ +m-1
X	Vf(WT； xσ(t)) - Vf(WT)
t=τ
T +m-1
≤ 2Lnδ + X Vf (W; xσ(t)) — Vf(W)
t=T
≤ 2Lnδ + e/A + B2||Vf(W)『.	(w.h.p.)
31
Published as a conference paper at ICLR 2022
Adding and subtracting Vf (wT) from Vf (W) and bound using δ and LiPschitz continuity again,
τ +m-1
X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ
≤ 2Lnδ + e√A2 + B2kVf(t^ - Vf(wτ) + Vf(wτ))k2
≤ 2Lnδ + e∖JA + 2B2L2δ2 + 2B2∣∣Vf(wτ)『,
and consequently
τ +m-1	2
X Vf (wτ; xσ(t)) -Vf(wτ)	≤ 4L2n2δ2 + 2e2 A2+2B2L2δ2+2B2kVf(wτ)k2 .
t=τ
Note that this inequality will fail to hold with Probability at most
e2n2 (l + 2R)∖xp (-4
δ	16n
If we want this to fail with Probability less than some p, it suffices to set
e2 = 16n
+ dlog (l + 2R
This gives
τ +m-1
X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ
2
≤ 4L2n2δ2
+ 32n ∙
+ dlog (l + 2R
+ 2B2L2δ2 + 2B2kVf(wτ)k2
Ifwe set δ such that L2nδ2 = A2, then we can bound this with
τ +m-1
X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ
+ 32n ∙
+ d log(1 + 2RA也))
which gives
1
m
2
≤ 4nA2
+ 2A2B2 + 2B2kVf (wτ )k2
n
τ +m-1
X Vf(wτ; xσ(t)) - Vf(wτ)
2
t=τ
< 32n
m2
+ d iog(ι + *))
+ 2A2B2 + 2B2kVf (wτ )k2
n
Hiding logarithmic terms involving n, p, R, L, A, the shuffle once setuP satisfies the requirement of
AssumPtion 2 that with Probability at least 1 - p,
1
m
τ+m-1
X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ
O
O
2
=O
2A2B2 +2B2kVf (wτ )k2
ndA2 + 2dA2B2 + 2ndB2kVf(wτ)k2
dA2(n+2B2) + 2ndB2kVf(wτ)k2
and so the parameters are Y = 2, C2 = O(dA2(n + B2)), and Φ2 = O(ndB2).
□
32
Published as a conference paper at ICLR 2022
A.5.3 Proof for Proposition 3 (Random Reshuffling)
We first introduce a lemma that bounds the distance between iterates obtained from SGD with random
permutation and that obtained from deterministic gradient descent, over one epoch.
Lemma 3. Consider a single epoch of SGD with constant step size α > 0, where the examples come
from a random permutation over the training set. Let Assumption 1 (L-Smoothness) and the bounded
gradient error Assumption 3 be satisfied. Without loss of generality, assume that the epoch starts at
time t = 0 at w0. Let the sequence ut be defined by u0 = w0 and
ut+1 = Ut - αVf (ut),
while
wt+1 = wt - αVf(wt; xt)
for some xt chosen from the permutation. With probability at least (1 - δ) it will hold that for all
T ∈ {1, . . . , n}, ifwe set
< .ʃ
α ≤ min
^ I
1
_________________L)
64e(e2 + 1)BnL log (孕)’2nL J
then
∣∣wτ - UTI∣2 ≤ 128α2n(e2 + 1)2 ∙ log (；n) (a2 + B2e2kVf (wτ)『).
Proof. Given our setup, observe that we can write the difference between these sequences as
wt+1 - Ut+1 = wt - Ut - α (Vf(wt; xt) - Vf(Ut))
= wt - Ut - α (Vf(wt; xt) - Vf(Ut;xt)) - α (Vf(Ut; xt) - Vf(Ut)) ,
such that summing this up and using w0 = U0 gives
T-1	T-1
wT - UT = -α	(Vf (wt; xt) - Vf(Ut; xt)) - α	(Vf(Ut; xt) - Vf(Ut)) ,
t=0	t=0
and so
∣wT - UT ∣
T-1
≤ α X ∣Vf (wt; xt) - Vf(Ut; xt)∣ + α
t=0
T-1
X (Vf(Ut;xt) -Vf(Ut))
t=0
T-1
≤ αL ∣wt - Ut ∣ + α
t=0
T -1
X (Vf(Ut;xt) - Vf(Ut))
t=0
Recall the update rule of Ut, with Assumption 1 (L-Smoothness), we obtain
∣Vf(Ut+1)-Vf(Ut)∣ ≤αL∣Vf(Ut)∣.
By the reverse triangle inequality, this implies
∣Vf(Ut)∣ ≤(1 - αL)-1∣Vf(Ut+1)∣
∣Vf(Ut+1)∣ ≤(1+αL)∣Vf(Ut)∣,
which further implies for any T ∈ {1, ∙∙∙ ,n} and t ∈ {0,…，T — 1},
∣Vf(Ut)∣ ≤(1 - αL)-n∣Vf (UT)∣
=(1+	αLΓY ∣Vf(uτ)k
1 - αL
αLn
≤ eχp ;-----1 kVf (uT)k
1 - αL
≤ eχp (2αLn) ∣Vf (UT)∣
≤e∣Vf (UT)∣,
33
Published as a conference paper at ICLR 2022
while for any t ∈ {T + 1,…，n},
kVf(Ut)k ≤(1 + αL)nkVf (UT)k
≤ekVf (UT)k,
where we apply the condition that α ≤ 1/(2nL). For a given T, consider the following vector set
Xj= Vf2(Uj;R)2-.(uj; , ∃j ∈{0,…，n},
A2 + B2e2kVf(UT)k
it can be easily verified that they sum to zero.
Now we apply Lemma 6 (Permuted vector Hoeffding inequality) , for any γ ≥ 0,
P QX (Vf(ut; Xt)-Vf(Ut))
≥ γ,A2 + B2e2∣Nf(UT)k2	≤ 2e2 exp
≤ 2e2 exp
as T ≤ n. Now, this holds for just one T. By a union bound,
T 1
P ∃T ∈ {1, . . . ,n},	(Vf(Ut;xt) - Vf (Ut)) ≥ γQT ≤ 2e2nexp
Y2
32T
Y 2
32n
Y 2
32n
t=0
where QT = A2+ + B2e2kVf (ut)k2. Now, if We setY such that
2e2n exp (一ɪ-) = δ ⇒ Y2 = 32n log
then we get that
T-1
P ∃T ∈ {1, . . . , n},	(Vf(Ut; xt) - Vf (Ut)) ≥ γQT ≤ δ.
t=0
In this case, we will have that with probability at least (1 - δ),
T -1
kWT - UTk ≤ αL	kWt - Ut k + αγQT.
t=0
If we let ρ0 = 0 and
T -1
ρT = αL	ρt + αγQT ,
t=0
then kWT - UT k ≤ ρT . Here, if T > 0,
ρT+1 - ρT = αLρT + αY(QT +1 - QT),
on the other hand, obviously ρ1 = αYQ1 ,
T -1
ρT =αY X(1 + αL)T -k-1(Qk+1 - Qk)
k=0
αY
1 + αL)T -kQk - X(1 + αL)T -k-1Qk
k=0
≤αγ (1 + αL) X(1 + αL)T -k-1Qk - X(1 + αL)T -k-1Qk + QT
k=0	k=0
≤αγ (aLe1/2 X Qk + QT
k=0
≤αγ(e2 + 1)QT
=αγ(e2 + 1) NA + B2e2∣Nf (WT )『
≤αγ(e2 + 1)(A + Bek▽/(WT )k).
34
Published as a conference paper at ICLR 2022
Put it back we obtain
∣∣WT - UTk ≤αγ(e2 + 1)(A + Bek▽/(UT)k)
≤αγ(e2 + 1)(A + BeLkwT — UT|| + Bek▽/(WT)k),
which gives
(1 — αγ(e2 + 1)BeL)kwT — UT∣∣ ≤ αγ(e2 + 1)(A + Bek▽/(wT)k) .
If we require
1
ɑ ≤ --------------------rrʒ-T
—64e(e2 + 1)BnL log (孕)
Squaring and substituting the value of γ gives
kwT — UTk2 ≤ 128α2n(e2 + 1)2 ∙ log (2e2n)(A2 + B2e2kVf (wT)『)
With probability at least (1 — δ).	□
We now provide justifications to Assumption 2 for the random reshuffling scheme.
Proposition 3. Suppose that we are using the random reshuffling variant of SGD. Assume that for all
w ∈ Rd and all examples, Assumption 1 (L-Smoothness) and Assumption 3 hold. For some p ∈ (0, 1),
set the constant step size to satisfy a ≤ (max {1460BnL ∙ log (4e2T∕p) , 2nL}) 1. Then with
probability at least 1 — P, Assumption 2 holds with Y = 2, C2 = O(nA2) and Φ2 = O(nB2).
Proof. For some γ > 0 (but different from the γ of Lemma 3, consider the event that for some
specific epoch k, for some wτ ∈ {w0, w1, . . . , wn(k-1)} ∪ {U0, U1, . . . , Un}, Where the Ui are the U
from Lemma 3 for epoch k, and for some τ and mk such that n(k — 1) ≤ τ < τ + mk ≤ nk,
τ+mk-1
X	Vf(wτ; xσ(t)) — Vf(wτ)
t=τ
≤ TA + B2kVf(wτ)k2.
Since all the xσ(t) are from the k-th epoch, but wτ is independent of any randomness in the k-th
epoch (as it is either a point visited in a previous epoch, or a value from the U sequence Which
depends only on the position at the start of the k-th epoch and not on any k-th epoch randomness), it
folloWs that We can apply Lemma 6 (Permuted vector Hoeffding inequality) on either this sum or,
alternatively, the terms from epoch k but not in the sum (the terms left out) to get that
P
τ+mk-1
X	Vf(wτ; xσ(t)) — Vf(wτ)
t=τ
≥ γ√A2 + B2kVf(wτ)k2
≤ 2e2 exp
________γ2_______
32 min(mk, n — mk)
As mk ≤ n, it folloWs that
τ +mk -1
X	Vf(wτ; xσ(t) ) — Vf(wτ)
≥ γ√A2 + B2kVf(wτ)k2
≤ 2e2exp (- 16n
NoW, by a union bound the probability that there exists some τ , wτ , and mk such that the average
gradient error is large is bounded by
τ+m-1
∃ X Vf(wτ; xσ(t) ) — Vf (w)
t=τ
≥ Y√A2 + B2kVf(wτ)k2
≤ 2e2 nT2 exp
35
Published as a conference paper at ICLR 2022
where T is the total number of iterations across all epochs (and we assume that we finish all epochs
so n divides T ). This follows from the fact that there are at most T such τ that we could take on, at
most T values of wτ that can be adopted for each, and at most n values mk can take on. If we set γ
such that
2e2nT2 exp (-ɪ) = P ⇒ Y = 16nlog (4e nT ) ,	(17)
16n	2	p
then with probability at least (1 - p/2), it will hold that for all epochs, wτ, τ, and mk,
T+mk-1	------------------
X Vf(wτ; Xσ(t)) -Vf (wτ) ≤ WA + B2kVf (wτ )k2.	(18)
t=τ
Additionally, set the δ in Lemma 3 to be pn/2T . By a union bound on the result of Lemma 3, across
all T/n epochs, with probability at least (1 - p/2), it must follow that
kwτ - UTk2 ≤ 128α2n(e2 + 1)2 ∙ log (4^)(A2 + B2e2∣∣Vf(wτ)『),	(19)
for the corresponding ut sequence for all epochs. Therefore, both of these inequalities Eqs. (18)
and (19) hold together with probability at least (1 - p).
Now, consider an arbitrary sum of gradient errors going from τ to τ + m - 1. Note since the interval
here is arbitrary, this m can be different from mk and potentially greater than n. We can split this
interval up as follows. Let τ1 be the first epoch boundary in the interval, such that all t going from τ
to τ1 - 1 are within the same epoch as wτ, or else τ1 = τ + m if there is no epoch boundary in the
interval. Let τ2 be the last epoch boundary in the interval, such that all t going from τ2 to τ + m - 1
are within a later epoch than wτ (it may be the case that τ1 = τ2). It follows that
τ +m-1
X Vf(wτ; xσ(t)) - Vf (wτ)
t=τ
τ1 -1
≤ X Vf(wτ; xσ(t)) - Vf (wτ)
t=τ
τ2 -1
+	Vf(wτ; xσ(t)) - Vf(wτ)
t=τ1
τ +m-1
+ X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ2
Observe that since the second of these sums must go over some number of full epochs, its value must
be 0. Therefore,
τ+m-1
X Vf(wτ; xσ(t)) - Vf (wτ)
t=τ
τ1 -1
≤ X Vf(wτ; xσ(t)) - Vf(wτ)
t=τ
τ +m-1
+ X Vf (wτ; xσ(t)) - Vf(wτ)
t=τ2
Observe that the term in the first sum must be nL/2-Lipschitz continuous in w, because it can be
written as the sum of at most bn/2c terms each of which is L-Lipschitz (either as the actual terms of
the sum, or else the terms left out of the sum). So, add and subtract Vf(uτ; xσ(t)) and Vf (uτ),
τ +m-1
X Vf(wτ; xσ(t)) - Vf (wτ)
t=τ
≤ nLkwτ - uτ k
τ1 -1
+ X Vf(uτ; xσ(t)) - Vf (uτ) +
t=τ
τ +m-1
X
t=τ2
Vf(wτ; xσ(t)) - Vf(wτ)
36
Published as a conference paper at ICLR 2022
Now applying Eq. (19) on the first term and Eq. (18) on the last two terms gives
τ +m-1
X Vf(Wt； xσ(t)) - Vf(WT)
t=τ
≤ nL ∙ jl28α2n(e2 + 1)2 ∙ log (42T)(A2 + B2e2∣∣Vf(wτ)『)
+ 2γ,A2 + B2kVf(wτ )『.
Squaring both sides for simplicity, we can bound this with
τ +m-1	2
X Vf(Wt； xσ(t)) - Vf(WT )
t=τ
≤ 256α2n3L2(e2 + 1)2 ∙ log (4e T)(A2 + B2e2∣∣Vf (w「)『)
+ 8γ2(A2 + B2kVf(Wτ)k2),
where we have also used Eq. (17) for γ. If we apply our requirement that αLn ≤ 1/2, we get
τ +m-1
X	Vf(WT ； xσ(t)) - Vf(WT )
t=τ
2
≤ 64n(e2 + 1)2 ∙ log (41T)(A2 + B2e2kVf (wt)『
+ 128n(A2 + B2kVf(Wτ)『) ∙ log
(4e2nT 2
k P
It follows that for random reshuffling with probability 1 - p, if we set
1
α ≤ min ----------------
I 64(e2 + 1)BenL log
1
,2nL
max 11460BnLlog (4e T) , 2nL}]
random reshuffling satisfies the requirement of Assumption 2 with Y = 2, C2 = O(nA2), and
Φ2 = O(nB2), where O(∙) hides logarithmic terms in n, T,p.	□
A.5.4 Proof for Proposition 4 (Random reshuffling with data echoing)
The proof for Proposition 4 is nearly a repeat of the random reshuffling proof in Proposition 3. The
trick is to re-define an epoch when data echoing is used. Instead of referring to an epoch as a random
permutation of the n examples in the training set as in vanilla random reshuffling, here we define the
cn samples as an epoch, where each example σ(i) is repeated c times. For instance, let c = 3, then
the k-th epoch with permutation σk is given by the sequence
. . , xσk (1) , xσk (1) , xσk (1) , xσk (2) , . . . , xσk (n-1) , xσk (n) , xσk (n) , xσk (n) , . . .
|
{^^^^^^^^^^^^
examples used in epoch k
}
As a result of this redefinition, Lemma 3 needs to be modified such that wherever n appears, we now
need cn. We omit the proof for the following lemma as it is a trivial adaptation from that of Lemma 3.
Lemma 4. Consider a single epoch of SGD with constant step size α > 0, where the examples come
from a random permutation over the training set, each echoed c times. Without loss of generality,
assume that the epoch starts at time t = 0 at W0. Let the sequence ut be defined by u0 = W0 and
ut+1 = ut - αVf(ut),
37
Published as a conference paper at ICLR 2022
while
Wt+1 = Wt - αVf (wt； Xt)
for some xt chosen from the permutation. Under the same assumptions as Lemma 3, with probability
at least (1 - δ) it will hold that for all T ∈ {1, . . . , cn},
∣∣WT — UTk2 ≤ 128α2cn(e2 + 1)2 ∙ log (2e^cn)(A2 + B2e2kVf (wτ)『).
The justifications for Assumption 2 for the random reshuffling with data echoing scheme can also be
obtained from Appendix A.5.3 similarly. At appropriate places one should invoke Lemma 4 instead
of Lemma 3, and replace n with cn. We omit the proof as it is again a trivial modification.
A.5.5 Proof for Proposition 5 (Markov Chain Gradient Descent (MCGD))
To justify Assumption 2 for MCGD, we will first need the following lemma.
Lemma 5. Let F be any vector-valued measurable function, and let x0, x1, . . . be a sequence of
samples from a Markov chain with mixing time tmix and stationary distribution π starting from an
arbitrary initial distribution. If the function is constrained such that ∣F (x)∣ ≤ 1 for all x, and if we
also have Eχ~∏ [F (X)] = 0, then for any δ ∈ (0,1),
PmXi=-01F(xi)
≥ 5tmix
≤ δ,
Proof. Consider the Doob martingale
m-1
Wk = E X F(xi) | Fk
i=0
where Fk contains all randomness up to timestep k, i.e. x0, x1, . . . , xk, and so (as usual for a Doob
martingale) the martingale property is trivially satisfied using repeated conditioning:
m-1
E [Wk+1 | Fk+1] = E E X F(xi) | Fk+1
i=0
m-1
| Fk	= E X F(xi) | Fk = Wk .
i=0
Observe that the sum we want is Wm
has increments
E Pim=-01 F(xi) | Fm = Pim=-01 F(xi), and that this sum
m-1
Wk+1 - Wk = E X F(xi) | Fk+1
i=0
m-1
- E X F(xi) | Fk
i=0
k+1
E X F(xi) | Fk+1
i=0
m-1
+ E X F(xi) | Fk+1
i=k+2
k	m-1
-E XF(xi) | Fk -E X F(xi) |Fk
i=0	i=k+1
k+1	k	m-1	m-1
XF(xi) - XF(xi) + X E[F(xi) | Fk+1] - X E[F(xi) | Fk]
i=0	i=0	i=k+2	i=k+1
m-1	m-1
F (xk+1) + X E [F(xi) | Fk+1] - X E[F(xi) | Fk].
i=k+2	i=k+1
Now, observe that since the mixing time of the Markov chain is tmiχ, for any i ≥ k, if μ denotes the
distribution of xi conditioned on Fk, then using results from Levin & Peres (2017, Section 4.5)
∣μ — π∣τv ≤ 2-b(i-k)/tmxc.
38
Published as a conference paper at ICLR 2022
It follows from this and the fact that F is bounded that
kE[F(xi) | Fk]k ≤ 2-b(i-k)/tmixc.
Therefore,
m-1
X
i=k+2
E [F (xi) | Fk+1]
m-1
≤ X kE[F(xi) | Fk+1]k
i=k+2
m-1
≤2
-b(i-k-1)/tmixc
i=k+2
∞
≤ X 2-b(i-k-1)/tmixc = 2tmix - 1,
i=k+2
and similarly for the last term. It follows that
kWk+1 - Wk k ≤ 4tmix.
Therefore, by the vector Azuma’s inequality of Hayes (2005, Theorem 1.8), for any a > 0,
P (IlWm - WOIl ≥ 4tmixa) ≤ 2e2 exp (- --).
2m
On the other hand, by the same reasoning as before,
kW0k
E
m-1
X F (xi) | F0
i=0
So,
≤ kF(x0)k + E
≤ 1 + 2tmix - 1
m-1
X F (xi) | F0
i=1
2tmix.
Now, setting a such that
and noting that this makes a ≥ 2, we get
PmXi=-01F(xi)
≤ δ,
which is what we wanted to show.
□
The justification for Assumption 2 then follows straightforwardly.
Proposition 5. Suppose that we use samples xt from a Markov chain with mixing time tmix. Assume
that for all w ∈ R and all examples xt, Assumption 3 holds. Then with probability at least 1 - p,
Assumption 2 holds with Y = 1, C2 = O)(Aλ~tmiχ), and Φ2 = O(Bi2场伙).
Proof. Observe that we need Lemma 5 to hold for all subintervals of examples, of which there are
only at most T2. So, setting δ to be p/T2 in Lemma 5, we can show that for all τ, m, the probability
τ +m-1
X (Nf(Wτ, xt) - Vf(WT))
t=τ
2
2e2T2
≥ 25(A2 + B2kVf(wτ)『)t‰i2mlog (—p-
39
Published as a conference paper at ICLR 2022
is at least 1 - p. Equivalently,
2
τ +m-1
一 X (Vf(WT； Xt)-Vf(WT))
m
t=T
≤ m 蔡(A2+B2kvf (wτ )『)∙ log( 2epT2)
It follows that MCGD satisfies the requirements of Assumption 2 with γ = 1,
C2=50A2tmix log (2eρT-)
and Φ2 = 50B2tmiχ log (2e T
mx	p
r-ɪ-ɪl 1 ∙ /pʌ	∙	∙	, 1	∙ , ∙	,	, ∙	T , 1 l' 11
The big-O expressions in the proposition statement immediately follow.
□
A.5.6 Justification for QMC-based data augmentation with random reshuffling
Before we begin with the proof, let us first present the introductory material necessary on quasi-Monte
Carlo methods. Similar to Monte Carlo integration, QMC is also used for numerical integration
but using low-discrepancy sequences instead of pseudorandom number sequences. Concretely, the
problem is to approximate the integral of a function f over some s-dimensional hypercube [0, 1]s,
I(f):=	[0,1]s
f (x)dx
using the average of the function evaluated at a sequence of points x1 , . . . , xm,
m
Im(f )：= — X f(Xi).
m
i=1
The approximation error rate is defined as = |I(f) - Im (f)|, and it is well-known that in the case
of Monte Carlo integration where the xi’s are drawn uniformly at random from [0, 1]s, the error rate
is 2 = O(1/m). If the sequence of xi ’s has low star-discrepancy, which is defined as
Dm := sup
U
m
X l{xi ∈ U}- VolUme(U)
m
i=1
where U = Qjs=1[0, bi] for bi ∈ [0, 1), and the volume is measured using the s-dimensional Lebesgue
measure. Intuitively, the smaller this quantity is the more evenly the sequence of points covers the
space. Some popular low-discrepancy sequences include the Halton sequence, Sobol sequence, van
der Corput sequence, etc., which are essentially deterministic sequences that are cleverly constructed
to mimic random numbers but in fact have low star-discrepancy. For instance, the Halton sequence
satisfies Dm = O ((log m)s/m).
To ensure fast convergence of Im to I as we increase m, in addition to using low discrepency
sequences we also need f to be relatively well-behaved. For this, the Hardy-Krause variation VHK is
often used, for which we refer the reader to Drmota & Tichy (2006, Definition 1.13) for its detailed
characterization. Most importantly, if f has finite VHK on [0, 1]s, then the Koksma-Hlawka inequality
guarantees that
E ≤ VHk Dm ∙
This implies that using quasi-Monte Carlo with, for instance, the Halton sequence, to integrate
a function f with bounded Hardy-Krause variation, the error rate would be E2 = O((log m)2s/m2).
In comparison to the Monte Carlo error rate, the QMC error rate can be much faster when the
dimensionality s is relatively small. For an in-depth exposition of the related materials we recommend
Owen (2003) and Drmota & Tichy (2006).
We are now ready to restate our proposition from Section 5, in which we describe our QMC data
augmentation setup.
Proposition 6. Suppose that we are using the random reshuffling variant of SGD with QMC data
augmentation as described. Assume that for all W ∈ Rd and all examples, Assumptions 1 , 3,
4 and 5 hold for Equation 3. For some p ∈ (0, 1), set the step size to be a constant such that
α ≤ (max{1460BnL ∙ log(4e2T/p) , 2nL})-1. Then with probability at least 1 一 P, Assumption 2
holds with Y = 2, C2 = O(n2V2CQMClog(T)2s + nA2) and Φ2 = O(nB2).
40
Published as a conference paper at ICLR 2022
Proof. Recall that the example used in the t-th iteration is being transformed as
xt
人(xSbt/nc (t mod 叫 Zb"+，1”"」(t mod n)).
We start from the average gradient error term, that is,
1
m
τ +m-1
X Vf (wτ; A (Xt, Zt)) - Vf(wτ)
t=τ
2
≤2
τ +m-1	1 n
X Vf(wτ; A (χt,Zt)) - n X
t=τ
τ +m-1
X Vf (w.
t=τ
i=1
Z&Vf(WT; A (PZ))|
1 τ +m-1
,τ；A(Xt,Zt))----X El Vf(wτ;A(xt；Z))
m z—ζ Z 〜[0,1]S
t=τ
1
m
1
m
2
1
+2
m
τ +m-1	n	2
X E 1	Vf(wτ;	A(Xt; Z))- - X E ι Vf(wτ;	A (X(AZ))Il	,
卜	Z〜[0,i]s	n 弋	Z〜[0,i]s	Il
t=τ	i=1
Where the first step folloWs from the definition of f in the data augmentation setup (Eq. (3)). In the
second step, the first norm relates to the QMC approximation error While the second norm relates
to the RR analysis. From the analysis in Section A.5.3 We knoW the second norm can be bounded
asymptotically (and probabilistically) by O(nA2) + O(nB2)∣∣Vf (w「)『/m2. Now We analyze the
first term. Since We use a contiguous QMC subsequence on each individual example, for the period
of [T, T + m - 1], We define Tj and mj as the starting point and length of example X(j) being chosen
during this period, such that Pjn=1 mj = m. With this notation, We can noW reWrite the first norm as
1
m
τ +m-1
X Vf(WA(Xt,Zt))- ζ~E,i]sVf(WA(Xte)
2
2
n Tj +mj -1
JX XVfe
j=1
t=τj
Wτ； A (Xej), Zt))-(EjSVf(WT; A(X(j); Z))
j=1
τj +mj -1
WT; A X(j), Zt
t=τj
ζJE,^svf^wτ ； A (x(j)；Z))
2n(τj + mj)2
m2
n
X
j=1
τj +mj -1
τj + mj
X Vf Wττ;A (Xj),Zt)) - Z〜E,1]SVf Wττ; A U); Z))
2nTj2
+ B
τj-1
∑ τ1 ∑›f
wτ; A(X(j),Zt
j=1
t=0
Z&sVf(WT ; A N; Z))
n
≤ m χ χ f
≤
n
1
2
2
1
2
Now We can use the Koksma-HgWka inequality (Aistleitner & Dick, 2014) to bound these norms,
which gives us
1
m
τ +m-1
X Vf(M A(Xt,Zt))- ζJE^f (wτ; A (xt; ζ))
2
≤O
nCQ2MC V 2
m2
n
X ∙ (log(Tj + mj )2s + Iog(Tj 产)
j=1
≤O
n2CQ2MCV 2 log(T)2s
m2
Putting it together, we have shown that random reshuffling with QMC data augmentation satisfies
Assumption 2 with Y = 2, C2 = O ("2CQmcV2 log(T)2s + nA2), and Φ2 = O (nB2).
□
41
Published as a conference paper at ICLR 2022
A.6 Miscellaneous lemmas
We now collect some technical lemmas used in our analyses.
The following lemma is a Hoeffding-type concentration bound on the sums of random permutations
of vectors. This lemma is particularly useful for simplifying the logic of the proofs of our shuffling
propositions above, as it frees us from having to use Doob-martingale-type arguments throughout.
Lemma 6 (Permuted vector Hoeffding inequality). Let n ≥ m > 0 be some integers, and let Xi,j
for i ∈ {1, . . . , m} and j ∈ {1, . . . , n} be vectors in Rd. We also require for all i,j, kXi,jk ≤ 1,
and that for all i,
n
X Xi,j = 0.
j=1
Then, if σ : {1, . . . , n} → {1, . . . , n} is a random permutation and that the individual Xi,j ’s do not
depend on σ,
P	Xi=m1Xi,σ(i)
Note that this lemma trivially also applies to random subsamples of vector sums, by just letting
Xi,j = Yj for the desired vector sequence Yj.
Proof. This proof is adapted from the proof of Theorem 4.3 in Bercu et al. (2015). Their approach
is more sophisticated and tends to a Bernstein-type inequality, but is specialized to only scalars.
Consider sampling the elements of the permutation one at a time, and let Fj be the filtration containing
the random variables σ(1), σ(2), . . . , σ(j) but not σ(j + 1), . . . , σn. For k ≤ n, define the process
m
Wk =E XXi,σ(i) |Fk
i=1
Observe that this must be a (vector) martingale process, as it is a Doob martingale. Explicitly, we can
write it as
k	1m
Wk = E X Xi,σ(i) + n - k X	X XijI Fk
_i=1	i = k+1 j∕{σ(1),…,σ(k)}
k	1 mk
E £ Xi,σ(i) - n-k £ £ Xi,σ(j) | Fk	,
i=1	i=k+1 j=1
42
Published as a conference paper at ICLR 2022
where here we used the fact that the Xi,j’s sum to 0 over j = [n]. Thus, the increments of this process
will have
—
Wk-1
k
E X Xi
i=1
1 mk
W(i) - n—k ΣS EXi,σ(j) I Fk
i=k+1 j =1
k-1
- E X Xi
i=1
1 m k-1
,σ(i) - n - k +1 E EXi,σ(j) * 1 Fk-I
i=k j=1
XkHk)- n-kE
mk
X X Xi,σ(j) | Fk
i=k+1 j=1
+——1——E
n-k+1
XkHk)- n-k
m k-1
Σ∑Xi.σ(j) IFk-I
m
Xi,σ(k)
i=k+1
—
n-k n-k+ 1
m k-1
iX+i X Xi") + ；
k-1
Xk,σ(j),
j=1
1
1
—
where we have repeatedly applied the martingale property. Now to bound all of these, first by our
assumption
Xk,σ(k) ≤ 1.
Also,
1
n-k
m
Xi,σ(k)
i=k+1
m-k
≤ ---r ≤ 1.
n-k
Next, again using the sum to 0 assumption, we have
1
n-k+1
k-1
Xk,σ(j	)
=1
1
n-k+1
n
-	Xk,σ(j)
j=k
n-k+1
≤ n-k + 1 = ,
and finally, by a combination of these bounds,
1
1
—
n-k n-k+1
m k-1
X XXi,σ(j)
i=k+1 j=1
1
(n - k)(n - k + 1)
m k-1
Xi,σ(j)
i=k+1 j=1
≤
1
(n - k)(n - k + 1)
1.
≤
∙ (m — k) ∙ (n — k + 1)
It follows that the increment satisfies
kWk - Wk-1 k ≤ 4
with probability 1. We can now apply the vector Azuma’s inequality of Hayes (2005, Theorem 1.8).
Applying this to Wm gives the result that, for any a,
P (kWmk ≥ a ≤ 2e2exp (-3am
This proves the lemma.
□
The next lemma we state is a standard result showing that we can cover a region of radius R with a
bounded number of balls of radius .
43
Published as a conference paper at ICLR 2022
Lemma 7 (-net lemma). For any region D of radius R in d-dimensional space, and any > 0, there
exists a subset S ⊆ D such that S is of size at most
|S| ≤
1+
and for every point X ∈ D, there exists a X ∈ S such that ∣∣x 一 Xk ≤ 匕
Proof. Consider the following procedure. As long as there are points in D that are not within a
distance of an existing point in S, choose one such point arbitrarily and add it to S. Observe that
with this construction, any two points in S must be at a distance greater than from each other. This
means that if we center a ball with radius /2 at each of the points in S, these balls will be disjoint.
The total volume of these balls will be |S| ∙ V1 ∙ (e∕2)d, where V1 is the volume of the unit ball
in d-dimensional space. However, the centers of all these balls must lie in D, and so the balls
themselves must all lie within a slightly larger region of radius R+ /2. This region will have volume
V1 ∙ (R + "2)d. This shows that our process must eventually stop, implying that S must exist, and
gives us a bound on the size of S as
∣S∣≤「=(1+2R )d
and the proof is complete.
□
44