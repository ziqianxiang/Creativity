Published as a conference paper at ICLR 2022
Understanding Latent Correlation-Based
Multiview Learning and Self-Supervision: An
Identifiability Perspective
Qi Lyu
School of EECS
Oregon State Univ.
Xiao Fu*
School of EECS
Oregon State Univ.
Weiran Wang
Google Inc.
Mountain View
Songtao Lu
IBM Research
Yorktown Heights
Ab stract
Multiple views of data, both naturally acquired (e.g., image and audio) and arti-
ficially produced (e.g., via adding different noise to data samples), have proven
useful in enhancing representation learning. Natural views are often handled by
multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA],
while the artificial ones are frequently used in self-supervised learning (SSL)
paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often
involve learning neural feature extractors such that the embeddings of data exhibit
high cross-view correlations. Although intuitive, the effectiveness of correlation-
based neural embedding is mostly empirically validated. This work aims to under-
stand latent correlation maximization-based deep multiview learning from a latent
component identification viewpoint. An intuitive generative model of multiview
data is adopted, where the views are different nonlinear mixtures of shared and
private components. Since the shared components are view/distortion-invariant,
representing the data using such components is believed to reveal the identity of
the samples effectively and robustly. Under this model, latent correlation max-
imization is shown to guarantee the extraction of the shared components across
views (up to certain ambiguities). In addition, it is further shown that the pri-
vate information in each view can be provably disentangled from the shared using
proper regularization design. A finite sample analysis, which has been rare in non-
linear mixture identifiability study, is also presented. The theoretical results and
newly designed regularization are tested on a series of tasks.
1	Introduction
One pillar of unsupervised representation learning is multiview learning. Extracting shared infor-
mation from multiple “views” (e.g., image and audio) of data entities has been considered a major
means to fend against noise and data scarcity. A key computational tool for multiview learning is
canonical correlation analysis (CCA) (Hotelling, 1936). The classic CCA seeks linear transforma-
tion matrices such that transformed views are maximally correlated. A number of works studied
nonlinear extensions of CCA; see kernel CCA in (Lai & Fyfe, 2000) and deep learning-based CCA
(DCCA) in (Andrew et al., 2013; Wang et al., 2015). DCCA and its variants were shown to largely
outperform the classical linear CCA in many tasks.
In recent years, a series of self-supervised learning (SSL) paradigms were proposed. These SSL
approaches exhibit a lot of similarities with DCCA approaches, except that the “views” are noisy
data “augmenented” from the original clean data. To be specific, different views are generated by
distorting data—e.g., using rotating, cropping, and/or adding noise to data samples (Dosovitskiy
et al., 2015; Gidaris et al., 2018; Chen et al., 2020; Grill et al., 2020). Then, neural encoders are
employed to map these artificial views to embeddings that are highly correlated across views. This
genre—which will be referred to as artificial multiview SSL (AM-SSL)—includes some empirically
successful frameworks, e.g., BYOL (Grill et al., 2020) and Barlow Twins (Zbontar et al., 2021).
* Contact information: Q. Lyu and X. Fu: {lyuqi,xiao.fu}@oregonstate.edu. W. Wang:
weiranwang@ttic.edu. S. Lu: songtao@ibm.com.
1
Published as a conference paper at ICLR 2022
Notably, many DCCA and AM-SSL approaches involve (explicitly or implicitly) searching for
highly correlated representations from multiple views, using neural feature extractors (encoders).
The empirical success of DCCA and AM-SSL bears an important research question: How to under-
stand the role of cross-view correlation in deep multiview learning? Furthermore, how to use such
understanding to design theory-backed learning criteria to serve various purposes?
Intuitively, it makes sense that many DCCA and AM-SSL paradigms involve latent correlation max-
imization in their loss functions, as such loss functions lead to similar/identical representations
from different views—which identifies view-invariant essential information that is often identity-
revealing. However, beyond intuition, theoretical support of latent correlation-based deep multiview
learning had been less studied, until recent works started exploring this direction in both nonlinear
CCA and AM-SSL (see, e.g., (LyU & Fu, 2020; Von Kugelgen et al., 2021; Zimmermann et al.,
2021; Tian et al., 2021; Saunshi et al., 2019; Tosh et al., 2021)), but more insights and theoretical
underpinnings remain to be discovered under more realistic and challenging settings. In this work,
we offer an understanding to the role of latent correlation maximization that is seen in a number of
DCCA and AM-SSL systems from a nonlinear mixture learning viewpoint—and use such under-
standing to assist various learning tasks, e.g., clustering, cross-view translation, and cross-sample
generation. Our detailed contributions are:
(i)	Understanding Latent Correlation Maximization - Shared Component Identification. We
start with a concept that has been advocated in many multiview learning works. In particular, the
views are nonlinear mixtures of shared and private latent components; see, e.g., (Huang et al.,
2018; Lee et al., 2018; Wang et al., 2016). The shared components are distortion/view invariant
and identity-revealing. The private components and view-specific nonlinear mixing processes de-
termine the different appearances of the views. By assuming independence between the shared
and private components and invertibility of the data generating process, we show that maximizing
the correlation of latent representations extracted from different views leads to identification of the
ground-truth shared components up to invertible transformations.
(ii)	Imposing Additional Constraints - Private Component Identification. Using the understand-
ing to latent correlation maximization-type loss functions in DCCA and AM-SSL, we take a step
further. We show that with carefully imposed constraints, the private components in the views can
also be identified, under reasonable assumptions. Learning private components can facilitate tasks
such as cross-view and cross-sample data generation (Huang et al., 2018; Lee et al., 2018).
(iii)	Finite-Sample Analysis. Most existing unsupervised nonlinear mixture identification works,
e.g., those from the nonlinear independent component analysis (ICA) literature (Hyvarinen &
Morioka, 2016; 2017; Hyvarinen et al., 2019; Khemakhem et al., 2020; Locatello et al., 2020; Gre-
sele et al., 2020), are based on infinite data. This is perhaps because finite sample analysis for
unsupervised learning is generally much more challenging relative to supervised cases—and there
is no existing “universal” analytical tools. In this work, we provide sample complexity analysis for
the proposed unsupervised multiview learning criterion. We come up with a success metric for char-
acterizing the performance of latent component extraction, and integrate generalization analysis and
numerical differentiation to quantify this metric. To our best knowledge, this is the first finite-sample
analysis of nonlinear mixture model-based multiview unsupervised learning.
(iv)	Practical Implementation. Based on the theoretical understanding, we propose a latent
correlation-maximization based multiview learning criterion for extracting both the shared com-
ponents and private components. To realize the criterion, a notable innovation is a minimax neural
regularizer that serves for extracting the private components. The regularizer shares the same pur-
pose of some known independence promoters (e.g., Hilbert-Schmidt Independence Criterion (HSIC)
(Gretton et al., 2007)) but is arguably easier to implement using stochastic gradient algorithms.
Notation. The notations used in this work are summarized in the supplementary material.
2 Background: Latent Correlation in DCCA and AM-SSL
In this section, we briefly review some deep multiview learning paradigms that use latent correlation
maximization and its close relatives.
2
Published as a conference paper at ICLR 2022
2.1	Latent Correlation Maximization in DCCA
DCCA methods aim at extracting common information from multiple views of data samples. Such
information is expected to be informative and essential in representing the data.
•	DCCA. The objective of DCCA can be summarized as follows (Andrew et al., 2013):
maximize Tr E f (1) x(1) f(2) x(2)>	, s.t. E f(q) x(q) f(q) x(q)> = I, (1)
where x(q) ∈ RMq 〜Dq is a data sample from view q for q = 1, 2, Dq is the underlying distribution
of the qth view, f(1) : RM1 → RD and f(2) : RM2 → RD are two neural networks. CCA was found
particularly useful in fending against unknown and strong view-specific (private) interference (see
theoretical supports in (Bach & Jordan, 2005; Ibrahim & Sidiropoulos, 2020)). Such properties were
also observed in DCCA research (Wang et al., 2015), while theoretical analysis is mostly elusive.
An equivalent representation of (1) is as follows
minimize E	f (1) (x(1)) - f(2)(x(2))	, s.t. E f(q) x(q) f(q) x(q)	= I,	(2)
which is expressed from latent component matching perspective. Both the correlation maximization
form in (1) and the component matching form in (2) are widely used in the literature. As we will
see in our proofs, although the former is popular in the literature (Andrew et al., 2013; Wang et al.,
2015; Chen et al., 2020), the latter is handier for theoretical analysis.
•	Slack Variable-Based DCCA. In (Benton et al., 2017) and (Lyu & Fu, 2020), a deep multiview
learning criterion is used:
2
minimize
f(q)
E
q=1
s.t. E[|gi|2] = 1, E[gigj] = 0.
(3)
The slack variable g represents the common latent embedding learned from the two views. Con-
ceptually, this is also latent correlation maximization (or latent component matching). To see
this, assume that there exists f(q) (x(q)) = g for all x(q). The criterion amounts to learning
[f(1)(x(1))]k = [f(2)(x(2))]k—which has the maximally attainable correlation.
2.2	Latent Correlation Maximization/Component Matching in AM-SSL
Similar to DCCA, the goal of AM-SSL is also to find identity-revealing embeddings of data samples
without using labels. The idea is often realized via intentionally distorting the data to create multiple
artificial views. Then, the encoders are require to produce highly correlated (or closely matched)
embeddings from such views. In AM-SSL, the views x(1) and x(2) are different augmentations
(e.g., by adding noise, cropping, and rotation) of the sample x.
•	Barlow Twins. The most recent development, namely, the Barlow Twins network (Zbon-
tar et al., 2021) is appealing since it entails a succinct implementation. Specifically, the Barlow
Twins network aims to learn a single encoder f : RM → RD for two distorted views. The cost
function is as follows:
D	DD
minimize X (1 - Cii)2 + λXXCi2j, where Cij
f	i=1	i=1 j 6=i
E [[f (X⑴)]i[f (X⑵)j]
PE[[f (X ⑴)]2 ] ,E[[f (X ⑵)j ]
When the learned embeddings are constrained to have zero mean, i.e., E f (X(q)) = 0, Cij is the
cross-correlation betweenf(X(1)) andf(X(2)). Note that the normalized representation of cross-
correlation in Cij is equivalent to the objective in (1) with the orthogonality constraints.
•	BYOL. The BYOL method (Grill et al., 2020) uses a cross-view matching criterion that can be
distilled as follows:
minim(iZ)e E IIf⑴(X(I))-f⑵(X(2))|口	⑷
3
Published as a conference paper at ICLR 2022
where f (∙) means that the output of the network is normalized. In BYOL, the networks are
constructed in a special way (e.g., part of f(2)’s weights are moving averages of the correspond part
of f(1)’s weights). Nonetheless, the cross-view matching perspective is still very similar to that in
latent component matching in (2).
•	SimSiam. The loss function of SimSiam (Chen & He, 2021) has a similar structure as that of
BYOL, but with a Siamese network, which, essentially, is also latent component matching.
3 Understanding Latent Correlation Maximization
In this section, we offer understandings to latent correlation maximization (and latent component
matching) from an unsupervised nonlinear multiview mixture identification viewpoint. We will
also show that such understanding can help improve multiview learning criteria to serve different
purposes, e.g., cross-view and cross-sample data generation.
3.1	Multiview as Nonlinear Mixtures of Private and Shared Components
We consider the following multiview generative model:
where x'q) ∈ RMq is the 'th sample of the qth view for q = 1,2, z` ∈ RD is the shared component
across views, and c'q) ∈ RDq represents the private information of the qth view—which are the 'th
samples of continuous random variables denoted by z ∈ RD, c(q) ∈ RDq, respectively. In addition,
g(q)(∙) : RD+Dq → RMq is an invertible and smooth nonlinear transformation, which is unknown.
Additional notes on (5) and the shared-private component-based modeling idea in the literature can
be found in the supplementary materials (Appendix H). We will use the following assumption:
Assumption 1 (Group Independence) Under ⑸,the samples z` and c'q) are realizations ofcon-
tinuous latent random variables z, c(q) for q = 1, 2, whose joint distributions satisfy the following:
Z 〜p(z), C⑷〜p(c⑷)，Z ∈ Z, C⑷ ∈ Cq,	p(z, C⑴，C⑵)=p(z)p(c(1))p(c(2)),	(6)
where Z ⊆ RD, Cq ⊆ RDq are the continuous supports of p(z) and p(c(q)), respectively.
Assumption 1 is considered reasonable under both AM-SSL and DCCA settings. For AM-SSL,
the private information can be understood as random data augmentation noise-induced components,
and thus it makes sense to assume that such noise is independent with the shared information (which
corresponds to the identity-revealing components of the data sample). In DCCA problems, the pri-
vate style information can change drastically from view to view (e.g., audio, text, video) without
changing the shared content information (e.g., identity of the entity)—which also shows indepen-
dence between the two parts. In our analysis, we will assume that D and Dq are known to facilitate
exposition. In practice, these parameters are often selected using a validation set.
Learning Goals. Our interest lies in extracting z` and c'q) (UP to certain ambiguities) from the
views in an unsupervised manner. In particular, we hope to answer under what conditions these
latent components can be identified—and to what extent. As mentioned, z` is view/distortion-
invariant and thus should be identity-revealing. The ability of extracting it may explain DCCA and
AM-SSL's effectiveness. In addition, the identification of c'q) and the mixing processes may help
generate data in different views.
3.2	A Latent Correlation-Based Learning Criterion
Given observations from both views {x'1), x'2)}N=ι generated from (5), we aim to understand how
latent correlation maximization (or latent component matching) helps with our learning goals. To
4
Published as a conference paper at ICLR 2022
this end, we consider the following problem criterion:
mfaximize Tr (N ∑ fS1) (x'1)) fS2) (χ'2))>)	Ga)
subject tof (q) for q = 1, 2 are invertible,	(7b)
NN X fSq) (χ'q)) fSq) (χ'q)) > = I, N X fSq) (χ'q)) = 0,q = 1,2,	(7c)
'=1	' '=1
fSq) (x'q)) ⊥⊥ fPq) (x'q)) , q=1, 2,	(7d)
where f(q) : RMq → RD+Dq for q = 1, 2 are the feature extractors of view q. We use the notations
fSq) (MG) = hf ⑷ WLd , fPq) (MG) = hf ⑷(MG )〕D+1:D+Dq , q=1, 2,
to denote the encoder-extracted shared and private components for each view, respectively. Note
that designating the first D dimensions of the encoder outputs to represent the shared information is
without loss of generality, since the permutation ambiguity is intrinsic.
The correlation maximization objective is reminiscent of the criteria of learning paradigms such as
DCCA and Barlow Twins. In addition, under the constraints in (7), the objective function is also
equivalent to shared component matching that is similar to those used by BYOL and SimSiam, i.e.,
NN
max「K X fS1) (χ'1)) fS2) (x'2))) O mn N X 帆1) (χ'1)) - fS2) (x'2)) IL.
To explain the criterion, note that we have a couple of goals that we hope to achieve with fS(q ) and
fP(q) . First, the objective function aims to maximize the latent correlation of the learned shared
components, i.e., fS(q). This is similar to those in DCCA and AM-SSL, and is based on the belief
that the shared information should be identical across views. Second, in (7d), we ask fS(q) and fP(q)
to be statistically independent for q = 1, 2. This promotes the disentanglement of the shared and
private parts of each encoder, following in Assumption 1. Third, the invertibility constraint in (7b)
is to ensure that the latent and the ambient data can be constructed from each other—which is often
important in unsupervised learning, for avoiding trivial solutions; see, e.g., (Hyvarinen et al., 2019;
Von Kugelgen et al., 2021). The orthogonality and zero-mean constraints in (7c) are used to make
the correlation metric meaningful. In particular, if the learned components are not zero-mean, the
learned embeddings may not capture “co-variations” but dominated by some constant terms.
3.3	Theoretical Und erstanding
We have the following theorem in terms of learning the shared components:
Theorem 1 (Shared Component Extraction) Under the generative model in (5) and Assumption 1,
consider the population form in (7) (i.e., N = ∞). Assume that the considered constraints hold over
all x(q) ∈ Xq for q = 1, 2, where Xq = {x(q) |x(q) = g(q) ([z>, (c(q))>]>), ∀z ∈ Z, ∀c(q) ∈ Cq}.
Denote fb(q) as any solution of (7). Also assume that the first-order derivative of fb(q) ◦ g(q) exists.
Then, we have b = fSq) (X⑷)= γ(z) no matter if (7d) is enforced ornot, where Y(∙) : RD → RD
is an unknown invertible function.
A remark is that b = Y(Z) has all the information of Z due to the invertibility of Y(∙). Theo-
rem 1 clearly indicates that latent correlation maximization/latent component matching can identify
the view/distortion-invariant information contained in multiple views under unknown nonlinear dis-
tortions. This result may explain the reason why many DCCA and AM-SSL schemes use latent
correlation maximization/latent component matching as part of their objectives. Theorem 1 also in-
dicates that if one only aims to extract Z, the constraint in (7d) is not needed. In the next theorem, we
show that our designed constraint in (7d) can help disentangle the shared and private components:
5
Published as a conference paper at ICLR 2022
Theorem 2 (Private Component Extraction) Under the same conditions as in Theorem 1, also
assume that (7d) is enforced. Then, we further have b(q) = fq) (X⑷)=δ⑷(C⑷),where
δS) (∙) : RDq → RDq is an unknown invertiblefUnction.
Note that separating z and C(q) may be used for other tasks such as cross-view translation (Huang
et al., 2018; Lee et al., 2018) and content/style disentanglement.
The above theorems are based on the so-called population case (with N = ∞ and the Xq observed).
This is similar to the vast majority of provable nonlinear ICA/factor disentanglement literature; see
(Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019; Locatello et al., 2020; Khemakhem et al.,
2020). It is of interest to study the finite sample case. In addition, most of these works assumed that
the learning function f(q) is a universal function approximator. In practice, considering f(q) ∈ F ,
where F is a certain restricted function class that may have mismatches with g(q) ’s function class,
is meaningful. To proceed, we assume D1 = D2 and M = M1 = M2 for notation simplicity and:
Assumption 2 Assume the following conditions hold:
(a)	We have g(q) ∈ G and learn f(q) from F, where the function classes F and G are third-order
differentiable and bounded.
(b)	The Rademacher complexity (Bartlett & Mendelson, 2002) ofF0 = {fd : RM → R|fd(x) =
[f (x)]d, f∈ F} is bounded by RN given N samples.
(c)	Define GT = {u : RM → RD+D1 ∣us(x) = γ(z), us(x) = [u(x)]i:D} ∀x ∈ Xq and any
invertible Y(∙). There exists f ∈ F such that suPχ∈Xq IIfS(X) — US(X)112 ≤ V.
(d)	Any third-order partial derivative of [h(q)(x)]d = [f (q) ◦ g(q)(x)]d resides in [-Cd, Cd] for all
X ∈ Xq. In addition, [C(q)]j ∈ [—Cp, Cp] with 0 < Cp < ∞forj ∈ [Dq].
Assumption 2 specifies some conditions of the function class F where the learning functions are
chosen from. Specifically, (a) and (d) mean that the learning function is sufficiently smooth (i.e.,
with bounded third-order derivatives); (b) means that the learning function is not overly complex
(i.e., with a bounded Rademacher complexity); and (c) means that the learning function should be
expressive enough to approximate the inverse of the generative function .
Theorem 3 (Sample Complexity) Under the generative model in (5), Assumption 1 and the suite
of ConditionS in Assumption 2, assume that (x'1), x'2)) for ' = 1,...,N are i.i.d. SampleS of
(X(1), X(2)). Denote fb(q) as any solution of (7) with the invertibility constraint satisfied. Then, we
have the following holds with probability of at least 1 — δ:
E XXXX (a[f⅞(χ(q))]i∕∂Cjq))2 = O ((DRN + Plog(1∕δ)∕N + V2)2/3)	(8)
i=1 j=1
for any C(q) ∈ Cq such that —Cp + κj ≤ c(jq) ≤ Cp — κj for all j ∈ [Dq] and all i ∈ [D], where
Kj = Ω((3∕Cd)1∕3(4Cf (2DRn + Cf √iog(1∕δ)∕2N) + 4ν2 )1/6).
If the metric on the left hand side of (8) is zero, then fs (x(q)) is disentangled from c(q). The
theorem indicates that with N samples, the metric is bounded by O(N-1∕3). In addition, RN de-
creases when N increases; e.g., a fully connected neural network with bounded weights satisfies
RN = O(N -1∕2) (Shalev-Shwartz & Ben-David, 2014). When RN increases (e.g., by using a more
complex neural network), the function mismatch V often decreases (since F can be more expressive
with a higher RN). In other words, Theorem 3 indicates a tradeoff between the expressiveness of
the function class F and the sample complexity. If F comprises neural networks, the expressive-
ness is increased (or equivalently, the modeling error is reduced) by increasing the width or depth
of networks. But this in turn increases RN and requires more samples to reduce (8). This makes
sense—one hopes to use a sufficiently expressive learning function, but does not hope to use an
excessively expressive one, which is similar to the case in supervised learning.
6
Published as a conference paper at ICLR 2022
4 Implementation
Enforcing Group Statistical Independence. A notable challenge is the statistical independence
constraint in (7d), whose enforcement is often an art. Early methods such as (Taleb & Jutten, 1999;
HyVarinen & Oja, 2000) may be costly. The HSIC method in (Gretton et al., 2007) which measures
the correlation of two variables in a kernel space can be used in our framework, but kernels some-
times induce large memory oVerheads and are sensitiVe to parameter (e.g., kernel width) selection.
In this work, we proVide a simple alternatiVe. Note that if two Variables X and Y are statistically
independent, then we have p(X,Y) = P(X)p(Y) ^⇒ E[φ(X)τ(Y)] = E[φ(X)]E[τ(Y)] for all
measurable functions φ(∙) : R → R and T(∙) : R → R (Gretton et al., 2005). Hence, to enforce group
independence between variables zb(q) and cb(q), we propose to exhaust the space of all measurable
functions φ(q) : RD → R and τ (q) : RDq → R, such that
sup R(q) = sup ∣Cov[Φ(q)(b(q)),τ (q)(b(q))]∕( qv[φ(q) (b(q))] qv[τ (q)(b(q))])	(9)
φ(q),τ (q)	φ(q),τ (q)
is minimized. It is not hard to show the following (see the proof in the supplementary material):
Proposition 1 In (9), if supφ(q),τ (q) R(q) = 0 over all measurable functions φ(q) and τ (q), then,
any zb(q) i and cb(q) j for i ∈ [D] andj ∈ [Dq] are statistically independent.
In practice, we use two neural networks to represent φ(q) and τ (q), respectively, which blends well
with the neural encoders for algorithm design.
Reformulation and Optimization. We use deep neural networks to serve as f(q). We introduce a
slack variable u` and change the objective to minimizing l` = P；=i ku` - fSq)(x'q))k2 Iike in
(3). The slack variable can also make orthogonality and zero-mean constraints easier to enforce. A
reconstruction loss v` = P；=i ∣∣x'q) - r(q)(f ⑷(x'q)))k2 is employed to promote invertibility of
f(q), where r(q) is a reconstruction network. Let θ collect the parameters of f(q) and r(q), and η
the parameters of φ(q), τ(q). The overall formulation is:
min max L + βV + λR,	s.t. ɪ	u`u> = I, ɪ	u` = 0,	(10)
U,θ η	N	N
'=1	'=1
where L = 1∕n PN=IL`, V = 1∕n PN=1 V', U = [uι,…，UN] ∈ Rd×n, R = P2=1 R(q),
and β, λ ≥ 0. We design an algorithm for handling (10) with scalable updates; see the supple-
mentary materials for detailed implementation and complexity analysis. We should mention that
using reconstruction to encourage invertibility is often seen in multiview learning; see, e.g., (Wang
et al., 2015). Nonetheless, this needs not to be the only invertibility-encouraging method. Other
realizations such as flow-based (e.g., (Kingma & Dhariwal, 2018)) and entropy regularization-based
approaches (e.g., (Von Kugelgen et al., 2021)) are also viable•一see more in Appendix I.
5	Related Work - Nonlinear ICA and Latent Disentanglement
Other than the DCCA and AM-SSL works, our design for private and shared information separation
also draws insights from two topics in unsupervised representation learning, namely, nonlinear ICA
(nICA) (Hyvarinen & Morioka, 2016; 2017; Hyvarinen et al., 2019; Khemakhem et al., 2020) and
latent factor disentanglement (Higgins et al., 2017; Kim & Mnih, 2018; Chen et al., 2018; Zhao
et al., 2019; Lopez et al., 2018), which are recently offered a unifying perspective in (Khemakhem
et al., 2020). The nICA works aim to separate nonlinearly mixed latent components to an individual
component level, which is in general impossible unless additional information associated with each
sample (e.g., time frame labels (Hyvarinen & Morioka, 2016) and class labels (Hyvarinen et al.,
2019; Khemakhem et al., 2020)) is used. Multiple views are less studied in the context of nICA,
with the recent exception in (Locatello et al., 2020) and (Gresele et al., 2020). Nonetheless, their
models are different from ours and the approaches cannot extract the private information from views
with different nonlinear models. The concurrent work in (Von Kugelgen et al., 2021) worked on
7
Published as a conference paper at ICLR 2022
Figure 1: t-SNE of the results on multiview MNIST from (Wang et al., 2015). Baselines: DCCA
(Wang et al., 2015), BarlOw Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020).
content-style disentanglement under data augmented SSL settings and considered a similar genera-
tive model where both shared and private components are explicitly used. A key difference is that
their model uses an identical nonlinear generative function across the views (i.e., g(1) = g(2)), while
We consider two possibly different g(q),s. In addition, our learning criterion is able to extract the
private information, while the work in (Von Kugelgen et al., 2021) did not consider this aspect.
None of the aforementioned works offered finite sample analysis. Our independence promoter is
reminiscent of (Gretton et al., 2005), with the extension to handle group variables.
6	Experiments
Synthetic Data. We first use synthetic data for theory validation; see the supplementary materials.
6.1	Validating Theorem 1 - Shared Component Learning
In this subsection, we show that latent correlation maximization (or latent component matching)
leads to shared component extraction under the model in (5)—which can be used to explain the
effectiveness of a number of DCCA and AM-SSL formulations. This is also the objective of our
formulation (7) if the constraint in (7d) is not enforced.
Multiview MNIST Data. For proof-of-concept, we adopt a multiview MNIST dataset that was
used in (Wang et al., 2015). There, the “augmented” view of MNIST contains randomly rotated
digits and the other with additive Gaussian white noise (Wang et al., 2015); see Fig. 1. This is
similar to the data augmentation ideas in AM-SSL. Using this multiview data, we apply different
multiview learning paradigms that match the latent representations (or maximize the correlations
of learned representations) across views. The dataset has 70,000 samples that are 28×28 images
of handwritten digits. Note that without (7d), our method can be understood as a slight variant of
(3). To benchmark our method, we use a number of DCCA and AM-SSL approaches mentioned in
Sec. 2, namely, DCCA (Wang et al., 2015), Barlow Twins (Zbontar et al., 2021) and BYOL (Grill
et al., 2020). We set D = 10, D1 = 20 and D2 = 50 through a validation set. The detailed settings
of our neural networks can be found in the supplementary material.
Since we do not have ground-truth to evaluate the effectiveness of shared information extraction,
we follow the evaluation method in (Wang et al., 2015) and apply k-means to all the embeddings
b(1) = fS1)(x'1)) and compute the clustering accuracy on the test set (see the visualization of b(2)
in the supplementary materials). In Fig. 1, we show the t-SNE (Van der Maaten & Hinton, 2008)
visualizations of z'1)'s on a test set of 10,000 samples together with the clustering accuracy. All
results are averaged over 5 random initializations.
By Theorem 1, all the methods under test should output identity-revealing representations of the
data samples. The reason is that the two views share the same identity information of a sample.
Indeed, from Fig. 1, one can see that all latent correlation-maximization-based DCCA and AM-SSL
methods learn informative representations that are sufficiently distinguishable. The “shape” of the
clusters are different, which can be explained by the existence of the invertible function Y(∙). These
results corroborate our analysis in Theorem 1.
CIFAR10 Data. We also observe similar results using the CIFAR10 data (Krizhevsky et al., 2009).
The results can be found in the supplementary materials, due to page limitations.
8
Published as a conference paper at ICLR 2022
Figure 2: Evaluation on Cars3D; rows in blue boxes are w/ R; rows in green boxes are w/o R.
6.2	Validating Theorem 2 - Shared and Private Component Disentanglement
We use data generation examples to support our claim in Theorem 2—i.e., with our designed regu-
larizer R in (10), one can provably disentangle the shared and private latent components.
Cars3D Data for Cross-sample Data Generation. We use the Cars3D dataset (Reed et al., 2015)
that contains different car CAD models. For each car image, there are three defining aspects (namely,
‘type’, ‘elevation, and 'azimuth').
We create two views as follows. We assume that given the car type that is captured by shared vari-
ables Z and the azimuths that are captured by c(q), the generation mappings g(1) and g⑵ produce
car images with low elevations and high elevations, respectively (so they must be different map-
pings). Under our setting, each view has N = 8, 784 car images. We model z with D = 10. For
c(q), we set D1 = D2 = 2. More details about our settings are in the supplementary material.
We use the idea of cross-sample data generation to evaluate the effectiveness of our method. To
be precise, we evaluate the learned fb(q)'s by combining b(q) = fSq)(XP)) and cjq) = fbPq)(Xjq))
and generating X'qj = b(q)([(z'q))>, (Cjq))>]>), where r(q) is the learned reconstruction network [cf
Eq. (10)]. Under our model, if the shared components and private components are truly disentangled
in the latent domain, this generated sample Xyj should exhibit the same 'type' and 'elevation' as
those of x'q) and the 'azimuth' of xjq).
Fig. 2 shows our experiment results. The proposed method’s outputs are as expected. For example,
on the left of Fig. 2, the z'1) is extracted from the red convertible, and the cj-1)'s are extracted from
the top row. One can see that the generated X'?'s under the proposed method with R are all red
convertibles with the same elevation, but using the azimuths of the corresponding cars from the top
row. Note that if R is not used, then the learned cbj(q) may still contain the ‘type’ or ‘elevation’
information; see the example highlighted with yellow background on the right of Fig. 2. More
results are in the supplementary material.
dSprites Data and MNIST Data for Cross-sample/Cross-view Data Generation. We offer two
extra sets of examples to validate our claim in Theorem 2. Please see the supplementary materials.
7	Conclusion
in this work, we provided theoretical understandings to the role of latent correlation maximization
(latent component matching) that is often used in DCCA and AM-SSL methods from an unsuper-
vised nonlinear mixture learning viewpoint. in particular, we modeled multiview data as nonlinear
mixtures of shared and private components, and showed that latent correlation maximization en-
sures to extract the shared components—which are believed to be identity-revealing. in addition, we
showed that, with a carefully designed constraint (which is approximated by a neural regularizer),
one can further disentangle the shared and private information, under reasonable conditions. We also
analyzed the sample complexity for extracting the shared information, which has not been addressed
in nonlinear component analysis works, to our best knowledge. To realize our learning criterion, we
proposed a slack variable-assisted latent correlation maximization approach, with a novel minimax
neural regularizer for promoting group independence. We tested our method over synthetic and real
data. The results corroborated our design goals and theoretical analyses.
9
Published as a conference paper at ICLR 2022
Acknowledgement. This work is supported in part by the National Science Foundation (NSF) under
Project NSF ECCS-1808159, and in part by the Army Research Office (ARO) under Project ARO
W911NF-21-1-0227.
Ethics Statement. This paper focuses on theoretical analysis and provable guarantees of learning
criteria. It does not involve human subjects or other ethics-related concerns.
Reproducibility Statement. The authors strive to make the research in this work reproducible.
The supplementary materials contain rich details of the algorithm implementation and experiment
settings. The source code of our Python-implemented algorithm and two demos with real data are
uploaded as supplementary materials. The details of the proofs of our theoretical claims are also
included in the supplementary materials.
References
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis.
In International Conference on Machine Learning, pp. 1247-1255. PMLR, 2013.
Raman Arora and Karen Livescu. Multi-view cca-based acoustic features for phonetic recognition
across speakers and domains. In 2013 IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 7135-7139. IEEE, 2013.
Francis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation analysis.
2005.
Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. MINE: Mutual information neural estimation. In International
Conference on Machine Learning, volume 80, pp. 531-540. PMLR, 10-15 Jul 2018.
Adrian Benton, Huda Khayrallah, Biman Gujral, Drew Reisinger, Shenmin Zhang, and Raman
Arora. Deep generalized canonical correlation analysis. In RepL4NLP at ACL, 2017.
J Douglas Carroll. Generalization of canonical correlation analysis to three or more sets of vari-
ables. In Proceedings of the 76th annual convention of the American Psychological Association,
volume 3, pp. 227-228, 1968.
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen-
tanglement in variational autoencoders. In Advances in Neural Information Processing Systems,
pp. 2610-2620, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
pp. 1597-1607. PMLR, 2020.
Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In Proceedings of
Conference on Computer Vision and Pattern Recognition, pp. 15750-15758, 2021.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
G Darmois. Analyse des liaisons de probabilite´. In Proc. Int. Stat. Conferences 1947, pp. 231, 1951.
Richard A Davis, Keh-Shin Lii, and Dimitris N Politis. Remarks on some nonparametric estimates
of a density function. In Selected Works of Murray Rosenblatt, pp. 95-100. Springer, 2011.
Paramveer Dhillon, Jordan Rodu, Dean Foster, and Lyle Ungar. Two Step CCA: A new spec-
tral method for estimating vector models of words. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12), pp. 1551-1558, New York, NY, USA, July 2012.
Omnipress.
Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas
Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks.
IEEE Trans. Pattern Anal. Mach. Intell., 38(9):1734-1747, 2015.
10
Published as a conference paper at ICLR 2022
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal ofMachine Learning Research, 12(Jul):2121-2159, 2011.
Ronald A Fisher. Frequency distribution of the values of the correlation coefficient in samples from
an indefinitely large population. Biometrika, 10(4):507-521, 1915.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018.
Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. A multi-view embedding space for
modeling internet images, tags, and their semantics. International Journal of Computer Vision,
106(2):210-233, 2014.
Luigi Gresele, Paul K Rubenstein, Arash Mehrjou, Francesco Locatello, and Bernhard Scholkopf.
The incomplete Rosetta stone problem: Identifiability results for multi-view nonlinear ICA. In
Proceedings of UAI 2020, pp. 217-227, 2020.
Arthur Gretton, Alexander J Smola, Olivier Bousquet, Ralf Herbrich, Andrei Belitski, Mark Augath,
Yusuke Murayama, Jon Pauls, Bernhard Scholkopf, and Nikos K Logothetis. Kernel constrained
covariance for dependence measurement. In International Conference on Artificial Intelligence
and Statistics, volume 10, pp. 112-119. Citeseer, 2005.
Arthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard Scholkopf, and Alex Smola. A
kernel statistical test of independence. Advances in Neural Information Processing Systems, 20,
2007.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a
new approach to self-supervised learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21271-21284, 2020.
Gregory Gundersen, Bianca Dumitrascu, Jordan T Ash, and Barbara E Engelhardt. End-to-end
training of deep probabilistic CCA on paired biomedical observations. In Uncertainty in Artificial
Intelligence, 2019.
Michael Gutmann and Aapo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the 13th International Conference on
Artificial Intelligence and Statistics, pp. 297-304. JMLR Workshop and Conference Proceedings,
2010.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.
Harold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321-377, 1936.
Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. In Proceedings of the European Conference on Computer Vision, pp. 172-189,
2018.
Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ICA. In Advances in Neural Information Processing Systems, volume 29, 2016.
Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of temporally dependent stationary sources.
In International Conference on Artificial Intelligence and Statistics, volume 54, pp. 460-469,
20-22 Apr 2017.
Aapo Hyvarinen and Erkki Oja. Independent component analysis: Algorithms and applications.
Neural Networks, 13(4-5):411-430, 2000.
11
Published as a conference paper at ICLR 2022
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ICA using auxiliary variables
and generalized contrastive learning. In International Conference on Artificial Intelligence and
Statistics,pp. 859-868, 2019.
Mohamed Salah Ibrahim and Nicholas D Sidiropoulos. Reliable detection of unknown cell-edge
users via canonical correlation analysis. IEEE Trans. Wireless Commun., 19(6):4170-4182, 2020.
Jon R Kettenring. Canonical analysis of several sets of variables. Biometrika, 58(3):433-451, 1971.
Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoen-
coders and nonlinear ICA: A unifying framework. In International Conference on Artificial In-
telligence and Statistics, pp. 2207-2217. PMLR, 2020.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning, volume 80, pp. 2649-2658. PMLR, 10-15 Jul 2018.
Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
Advances in Neural Information Processing Systems, 31, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Pei Ling Lai and Colin Fyfe. Kernel and nonlinear canonical correlation analysis. International
Journal of Neural Systems, 10(05):365-377, 2000.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 4681-4690, 2017.
Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse
image-to-image translation via disentangled representations. In Proceedings of the European
Conference on Computer Vision, pp. 35-51, 2018.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Confer-
ence on Machine Learning, pp. 6348-6359. PMLR, 2020.
Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on auto-
encoding variational Bayes. Advances in Neural Information Processing Systems, 31:6114-6125,
2018.
Qi Lyu and Xiao Fu. Nonlinear multiview analysis: Identifiability and neural network-assisted
implementation. IEEE Trans. Signal Process., 68:2697-2712, 2020.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT press, 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Pushpendre Rastogi, Benjamin Van Durme, and Raman Arora. Multiview LSA: Representation
learning via generalized CCA. In Proceedings of the 2015 conference of the North American
chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
556-566, 2015.
Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances
in Neural Information Processing Systems, 28:1252-1260, 2015.
12
Published as a conference paper at ICLR 2022
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research,pp. 5628-5637. PMLR, 09-15 JUn 2019.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge University press, 2014.
Richard Socher and Li Fei-Fei. Connecting modalities: Semi-sUpervised segmentation and anno-
tation of images Using Unaligned text corpora. In 2010 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 966-973. IEEE, 2010.
Anisse Taleb and Christian JUtten. SoUrce separation in post-nonlinear mixtUres. IEEE Trans. Signal
Process., 47(10):2807-2820, 1999.
YUandong Tian, Xinlei Chen, and SUrya GangUli. Understanding self-sUpervised learning dynam-
ics withoUt contrastive pairs. In Proceedings of the 38th International Conference on Machine
Learning, volUme 139 of Proceedings of Machine Learning Research, pp. 10268-10278. PMLR,
18-24 JUl 2021.
Christopher Tosh, Akshay KrishnamUrthy, and Daniel HsU. Contrastive learning, mUlti-view redUn-
dancy, and linear models. In Algorithmic Learning Theory, pp. 1179-1206. PMLR, 2021.
LaUrens Van der Maaten and Geoffrey Hinton. VisUalizing data Using t-SNE. Journal of Machine
Learning Research, 9(11), 2008.
Julius Von KUgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel
Besserve, and Francesco Locatello. Self-sUpervised learning with data aUgmentations provably
isolates content from style. Advances in Neural Information Processing Systems, 34, 2021.
Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through align-
ment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929-9939. PMLR, 2020.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation
learning. In International Conference on Machine Learning, pp. 1083-1092, 2015.
Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correla-
tion analysis. arXiv preprint arXiv:1610.03454, 2016.
Ka Yee Yeung and Walter L Ruzzo. Details of the Adjusted Rand Index and clustering algorithms,
supplement to the paper an empirical study on Principal Component Analysis for clustering gene
expression data. Bioinformatics, 17(9):763-774, 2001.
Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, and Stephane Deny. Barlow Twins: Self-supervised
learning via redundancy reduction. In Proceedings of the 38th International Conference on Ma-
chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12310-12320.
PMLR, 18-24 Jul 2021.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing learning and inference in
variational autoencoders. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):
5885-5892, Jul. 2019.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 2223-2232, 2017.
Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 12979-12990. PMLR, 18-24 Jul 2021.
13
Published as a conference paper at ICLR 2022
Supplementary Materials
A Notation
The notations used in this work are summarized in Table A.1.
Table A.1: Definition of notations.
Notation	Definition
x, x, X	scalar, vector, and matrix
[x]i, xi	both represent the ith element of vector X
[X]ij	the ith row, jth column of matrix X
p(x)	probability density function of random variable X
x ⊥⊥ y	X and y are statistically independent, i.e., p(x, y) = p(x)p(y)
x ⊥⊥ y	Xi ⊥⊥ yj for all i, j
xτ, Xτ	transpose of x, X
Jf	Jocobian matrix of a vector-valued function f
I	identity matrix with a proper size
f ◦ g	function composition operation
detX	determinant of a square matrix X
EH	expectation
V[∙]	variance
Cov[∙, ∙]	covariance
[N]	the integer set {1, 2,..., N}
B	Proof of Theorem 1
Theorem L (Shared Component Extraction) Under the generative model in (5) and As-
sumption 1, consider the population form in (7) (i.e., N = ∞). Assume that the con-
sidered constraints hold over all x(q) ∈ Xq for q = 1,2, where Xq = {x(q)∣x(q) =
g(q)([z>, (c(q))>]τ), ∀z ∈ Z, ∀c(q) ∈ Cq}. Denote f(q) as any solution of (7). Also assume
that the first-order derivative of f(q) ◦ g(q) exists. Then, We have b = fSq) (Xs)) = Y(z) no
matter if (7d) is enforced or not, where Y(∙): RD → RD is a certain invertible functions.
We consider the formulation in (7) without (7d). When N = ∞ and x(q)〜Xq for q = 1, 2 are
all available, the sample average version of the formulation in (7) becomes the following expected
value version:
maximize Tr E
f(1),f(2)
subject to f(q) for q = 1, 2 are invertible,
E ]fSq) (χ'q)) fSg) (χ'q))τ[ = I, E [fs (χ'q))i = 0, q = 1,2
(B.1a)
(B.1b)
(B.1c)
First, note that under the generative model in (5), the maximum of the objective function in (7) is
(1)
D, which is obtained when every corresponding components of the learned solutions, i.e., fS	:
RM1 → RD and fbS(1) : RM2 → RD, are perfectly correlated, i.e.,
fbS(1) (x(1) = fbS(2) (x(2) .	(B.2)
14
Published as a conference paper at ICLR 2022
Indeed, one may rewrite (B.1a) as
+I
(B.3)
where the second equality holds because the constraint in (B.1c). Note that the criterion in (B.3)
admits the optimal solution in (B.2) under our generative model.
Note that one solution to attain zero cost of (B.3) is
x(q) = z, q = 1, 2.
However, the question lies in “uniqueness”, i.e., can enforcing (B.2) always yield fSq) (x(q)) = Z
(up to certain ambiguities)? This is central to learning criterion design, as the expressiveness of
function approximators like neural networks may attain zero cost of (B.3) with undesired solutions.
Assume that a solution that satisfies (B.2) is found. Denote fb(q) for q = 1, 2 as the solution.
Combine the solution with the generative model in (5). Then, following equality can be obtained:
h(S1)
z
c(1)
h(S2)
z
c(2)
(B.4)
where we have
in which
We hope to show that h(S1) and h(S2) are functions of only z—i.e., the functions fbS(q) for q = 1, 2
only extract the shared information.
To show that h(S1) is a function of only z but not a function of c(1), we consider the first-order
partial derivatives of h(S1) w.r.t. z and c(1), respectively. Namely, we hope to show that the matrix
consisting of all the partial derivatives of h(S1) w.r.t. z is full rank while any partial derivatives of
h(S1) w.r.t. c(1) is zero.
Therefore, we investigate the Jacobian of h(1) which fully characterizes all the first-order partial
derivatives of the function h(S1) and h(P1) w.r.t. z and c(1). Let us denote the outputs of h(1) =
fb(1) ◦ g(1)(ω(1)) as follows:
zb
cb(1)
z
c(1)
(B.5)
The Jacobian of h(1) can be expressed using the following block form
J(1)
J(1)
J11
J(1)
J21
J(1)
J12
J(1)
J22
15
Published as a conference paper at ICLR 2022
where J1(11) ∈ RD×D, J1(21) ∈ RD×D1, J2(11) ∈ RD1×D and J2(21) ∈ RD1×D1 are Jacobian matrices
defined as follows
∂zD
∂hhS1)(ω(1))i1
∂ZD
，[■耿川⑴]
∂ZD
.
.
.
∂[hP1)(ω(1))iD,
∂ZD
∂[hS1)(ωα))il]
∂hh(P1)(ω(1))iD
What we hope to show is that J1(21) is an all-zero matrix while the determinant of J1(11) is non-zero.
We first show that J1(21) = 0. Note that (B.4) holds over the entire domain. Hence, we consider any
fixed Z and C(2). Then for all c(1),the following equation holds:
Z
C⑵
Z
c(1)
(B.6)
for all C(I) ∈ Ci with any fixed Z and C(2).
Let us define matrices HS(1) and HS(2), where
∂
hHS(q)ii,j =
Mq) (ω(q))] i “
MD
1,…，D,j = 1,…，Dι.
By taking partial derivatives of Eq. (B.6) w.r.t. c(j1) for j
Jacobian:
1, . . . , D1, we have the following
一∂Zι
∂c11)
∂Zι 一
HSIL) = HS2LJ=)
z,c(2)
∂Zd
∂c11)
∂c12)
∂c11)
∂Zd
∂c(1)
∂cD1
∂c12)
(b)
z,c(2)
0D×D1
0D2 ×D1
0D×D1 ,

温
∂c11)
∂c(2)
d2
∂c⑴
∂cD1
(B.7)
where Jh(2) ∈ RD×(D+D2) is the Jacobian of h(S2), (a) is by the chain rules and (b) is because
We take derivatives of constants. The equation above holds for any Z and C(2). Hence, the same
derivation holds for all Z and C(2), which leads to the conclusion that the learned h(Sq)(ω(q)) is not
a function of C(1). Note that another possibility that could lead to (B.7) is that h(Sq) (ω(q)) always
outputs a constant. However, this is not possible because each h(q) = fb(q) ◦ g(q) is an invertible
function, which implies that any dimension of h(q) (ω(q)) cannot be a constant if ω(q) is not a
constant. To be more precise, note that x(q) is generated from ω(q) that has D + Dq dimensions. If
there are D dimensions (i.e., h(Sq) (ω(q)) ∈ RD) that are constants (and thus no information) in the
learned generative domain, then x(q) cannot be reconstructed from that domain, which contradicts
invertibility.
16
Published as a conference paper at ICLR 2022
Then, the Jacobian of h(1) can be re-expressed by (B.7)
J(1)
H(1)
HS
J(1)
J22
0
D×D1
J2(21)
Next, we show that the determinant of J1(11) is non-zero. By the structure of the Jacobian J(1), one
can see that zb is a function of only z but not determined by c(1), where we denote as zb = γ(z).
Besides, since h(1) is invertible, we have
detJ(1) = detJ1(11) detJ2(21) 6= 0
by the property of determinant for block matrix. It further indicates that detJ1(11) 6= 0 (so does
IdetJ2I) I), which implies that γ(∙) is an invertible function. This proves Theorem 1.	□
C Proof Of Theorem 2
Theorem 2.	(Private Component Extraction) Under the same conditions as in Theorem 1,
also assume that (7d) is enforced, We further have b(q) = fq) (x(q)) = δ(q) (c(q)), where
δ(q)(∙) : RDq → RDq is a certain invertible function.
Following the proof of Theorem 1, we further consider the expected value version with (7d), i.e.,
mfa(1x)imf(i2z)eTr E fS(1) x(1) fS(2) x(2)>	(C.1a)
subject to f(q) for q = 1, 2 are invertible,	(C.1b)
E fS(q) x(q) fS(q)	x(q)> =I, E	hfS(q)	x(q)i =0,	q= 1,2,	(C.1c)
fS(q) x(q) ⊥⊥ fP(q)	x(q)	, q= 1,2,	(C.1d)
Again, under our generative model, any optimal solution (fb(1), fb(2)) satisfies
We hope to further show that
cb(1) = fbP x(1) = h(P1)	c(z1)
is an invertible function-transformed version of c(1), where
Recall that we have the following Jacobian matrix for function h(1)
where the second row corresponding to function h(P1).
Since we have shown that both IIdetJ1(11) II 6= 0 and IIdetJ2(21) II 6= 0 in Section B, we only need to show
that J2(11) is an all-zero matrix. To show this, we will use the condition (C.1d). First, it is not hard
to see that
∙ ∙ ∙
∂Z1	∂ZD
T(1) —	.	.	.	T(1)
J21 =	..	..	..	J11 ,
西
∂Zd
17
Published as a conference paper at ICLR 2022
by chain rules Where the first matrix on the right hand side is the Jacobian of cb(1) W.r.t. zb. By (C.1d),
we have c(1) ⊥⊥ Z which means that we can observe fixed C⑴
(b1),…，Ci,…，cD)) for any
fixed Ci with any possible z. Therefore, at any specific point of C(I), the following always holds
d [c(1)]i	dci	0
==O
,
∂zj	∂zj
since the numerator is a constant. Note that the above holds for any C(I) with different Ci for
i ∈ [D1], which further means that we actually have
西
∂zd
0
西
∂zd
for all hP1) (ω(1)) and Z
Therefore, J2(11) = 0D1×DJ1(11) = 0D1×D.
Consequently, we have the following block diagonal
form for J(1) by combing with Theorem 1, i.e.,
J1(11)	0D×D1
0D1×D
By invertibility of fZ(q) and g(q), we have
IIIdetJ(1)III = IIIdetJ1(11)III IIIdetJ2(21)III 6=0,
Which implies that zb
respectively. The same
γ(z) and c(1) = δ⑴(C⑴)With invertible functions γ(∙) and δ⑴(∙),
proof technique applies to δ⑵(∙).

D Proof of Theorem 3
Theorem 3.	Under the generative model in (5) and Assumptions 1 and 2, assume that
(x'1),x'2)) for ' = 1,...,N are i.i.d. samples of (X⑴,X⑵).Denote Fq) ∈ F as any
solution of (7) with the invertibility constraint satisfied. Then, we have the following holds
with probability of at least 1 - δ:
E
D Dq
XX(∂ 侨(x(q))]i∕∂cjq))2
i=1j=1
O
Z_________ C'2/3
+ √l0g(l∕δ)∕N + V2 )
(D.1)
for any C⑷ ∈ Cq such that -Cp + Kj ≤ Cjq) ≤ Cp — Kj for all j ∈ [Dq] and all i ∈ [D], where
Kj = Ω((3/Cd)1/3(4Cf (2DRn + Cf plog(1∕δ)∕2N) + 4ν2)1/6).
D.1 A Lemma on Rademacher complexity
To derive the Rademacher complexity of the loss function, We have the folloWing lemma
Lemma 1 Consider the following function class
H = l X(1),X(2)
—
where fd(1),fd(2) ∈ F0 are as defined in Assumption 2. Assume that
fd(2)X(2)2
Ifdq) (x(q))| ≤ Cf for all
fd(q) ∈ F0, where Cf > 0. Then, the Rademacher complexity of class H is bounded by
RN (H) ≤ 4DCf RN.
18
Published as a conference paper at ICLR 2022
Proof: First, We have the function IfdI) (x(1)) - f，2 (x(2)) ∣ bounded Within [0, 2Cf]. According
to the Lipschitz composition property of Rademacher complexity (Bartlett & Mendelson, 2002), we
have
Rn (φ OF) ≤ LφRN (F)
Where Lφ is the Lipschitz constant of φ. Here φ(x) = x2 * * * * * * and Lφ = 4Cf.
Combining With the linearity property of the Rademacher complexity (Bartlett & Mendelson, 2002),
We have
RN (H) ≤ 4DCf RN	(D.2)
Which completes the proof of the lemma. Note that (D.2) is derived by treating fd(q) for d ∈ [D]
as individual functions. HoWever, fd(q) for d ∈ [D] are the first D outputs of the same function
f (q)—Which means that many parameters of these D functions are constrained to be identical.
Nonetheless, this fact does not affect the inequality in (D.2) since adding confining constraints to
the function class H only reduces the Rademacher complexity (Bartlett & Mendelson, 2002).
D.2 Proof of Theorem 3
First, We bound the true risk on the vieW matching loss. Consider the regression problem given
(x'1), x'2)) as samples, we have
minimize
f(1),f(2)
By Assumption 2(c), the first term on the right hand side can be bounded as
x(2)
—
2
2
1N
≤ N X(V + V)2 = 4ν2.
where the first equality is because there exist U⑴ ∈ GT and U⑵ ∈ GT such that USI) (x(1))
uS2) (x(2)) = γ(z) and the second inequality is by the triangle inequality.
Therefore, by plugging in RN (H) we have:
E ]∣∣fS1) (x'1))-fS2) W: ≤ e
with the definition
e := 4v2 + 8DCfRn + 4Cf JlogNδ)
= 4Cf(2DRn + Cf Jl0gNδ)! +4ν2.
19
Published as a conference paper at ICLR 2022
Next, we use the bound of true risk to bound the energy of the entries of the Jacobian matrix on the
left hand side ofEq. (D.1). Denote h(SI) = ∕,(q) 0g⑷.We define the error for any individual sample
pair (x'D, x'2))〜P (X(I), x(2)) as
with E[εg] ≤ e.
Define another two pairs of samples, such that
z`
华)
忌I)
z`
c'I) - △6一
-hS2)
2
=εb
2
2
=ε',
2
z`
d + + △%_
—
z`
华)
where ∆ > 0 and e7- is the unit vector in the CjI) direction. Then by triangle inequality we have
z`
c` ) + △%_
z`
c'I) - △/
Define
忌I)
≤ √εb + √εJ∙
2
ψij GI)):
-忌I)
•・" ∙∙f )]τ
i
which is a scalar function of C(I) with fixed Z and c(1') for k = j.
∂C⅞(χ⑴儿
Then the element
can be estimated using the central difference formula as
d [f⅛(X(I))] i
∂CFr^
可
M (	一	z`	一 d)+ Xj	)-h"	一	z`	一 c'I) - Xj	i
∆2
2∆
12
where ξι ∈
忌I)
-忌I)
2∆
∆2
㈤-∆,*+∆).
Since ∣χ∕ ≤ Ilxh ≤ ∣∣x∣∣2,wehave
d [f⅛ (X(I))]i
j +△), ξ2 ∈ (c'? - ∆, c'?) and by intermediate value theorem W ∈
de?
By taking expectation, We have
d [f⅛ (X(I))] i
∂CFr^
2∆
E[√⅞] + E[√εJ]
2∆
∆2
≤
E
—
z`
d + + △%
z`
c'I) - △/
i
≤
≤
≤
√e+ɪ ∣ψij(ξ')∣,
where the second inequality is by Jensen,s inequality
E[√ε∑] ≤ pE%] ≤ √ε,
20
Published as a conference paper at ICLR 2022
due to the concavity of √X.
We aim to find the smallest upper bound, i.e.,
inf	√ + '
0<∆<min{Cp + c'1j,Cp-c'1j O δ 6
(D.3)
Note that the function in (D.3) is convex and smooth. We have the minimizer
∆ ∈
IΨij0(ξ0)l
1/3
,min {Cp + c'1), Cp
—
which gives us the minimum
i∆f ∆ + 今 lψ0j0(ξ0)1 ≤ min
3 八Ψij0(ξ0)l
1/3	/—
e1∕3,必
Kj
2
3
2
.
where Kj = min Cp + c'1j, Cp
—
If κj ≥ (∣ψ3√√∣0)∣) / , then we can bound
E
d fS (X(I))] i
dcjI)
≤ 3 (用(ξ0)∣)1/3 e1∕3
2
3
With fixed N, one can choose e = 4Cf(2DRn + Cf Jlɔg(ɪ/")) +4ν2, which gives the following
bound
∂
E
[fS (x(1))[
∂cj1)
3
≤ —
-2
1/3 ,f(2DRN + Cf")+4ν)/3
i
if Kj ≥ (C)1/3(4Cf(2DRn + Cf qlog(≡) +4ν2)i"
Considering all i, j pairs, we have
D D1
E XX
[fs (X ⑴)]J
∂cj1)
3
≤ 2 DDi
1/3 (Cf(2DRN + Cfrl0g2≡)+4ν) 1/3
∂
i
Since ∣∣ ∙ k2 is upper bounded by ∣∣ ∙ k1,we have
D D1
EXX
d hfS (X(I))ii:
∂cj1)	)
≤ 4 D2D2
2/3 1°f(2DRN + Cf")+4ν)2/3
which completes the proof. The same holds for q = 2 by role symmetry.
E Proof of Proposition 1
The claim can be proved by contradiction. Take q = 1 for example. Suppose that there exists two
elements zbi(1) and cb(j1) that are dependent but (9) is 0. Let
φ1 = e>i ,	τ1 = e>j ,
21
Published as a conference paper at ICLR 2022
which are valid choices. Then, we have
Cov[φ1(zb(1)),τ1(cb(1))] = E[zbi(1)cb(j1)] -E[zbi(1)]E[cb(j1)].
Note that by definition of independence, we have
E[b(1)cj.1)] = E[b(1)]E[b(.1)] 0 b(1) ⊥⊥ cj.1).
Hence, zbi(1) and cb(j1) being dependent means that
E[zbi(1)bc(j1)] -E[zbi(1)]E[cb(j1)] 6=0.
Consequently, one can see that
sup Cov[φι(b⑴),τι(C(D)] ≥ Cov[φι(b⑴),τι(b(D)]∣φι=e>	e> = Cov[b(DbD] = 0.
φ1,τ1	i j
The above is a contradiction to our assumption that holds.
On the other hand, if zbi(1) and cb(j1) are independent for all i ∈ [D] and j ∈ [D1], then we have
E[φ(zbi(1))τ(bc(j1))] -E[φ(zbi)]E[τ(cb(j1))] =0,
for all i ∈ [D] and j ∈ [D1], for any φ : R → R and τ : R → R.
F Detailed Algorithm Implementation
Recall that the proposed criterion is
ma⅛⅛ze Tr (N X fS1) (χ'1)) fS2) (x'2))>
subject to (q) for q = 1, 2 are invertible,
'=1	'=1
(F.1c)
(F.1a)
(F.1b)
fSq) (x'q)) ⊥⊥ fPq (x'q)) , q=1, 2,	(F.1d)
We will use neural networks to parameterize the functions that we aim to learn. To move forward,
first, as we have shown in the proof of Theorem 1, Eq. (F.1) is equivalent to the following:
minimize
f (1) ,f (2)
'=1
(F.2a)
2
2
1N	>	1N
SUbjeCt to N X fSq) W)) fSq) (χ'q))	= I, N X fSq) (χ'q)) = 0, q = 1,2
v '=1	v '=1
f(q) for q = 1, 2 are invertible,
(F.2b)
(F.2C)
fSq) (x'q)) ⊥⊥ fPq) (x'q)) , q = 1, 2,	(F.2d)
Note that we have manifold Constraints on both neUral networks fS(1) and fS(2). DireCtly optimizing
over sUCh manifold Constraints may be Costly and Challenging. To redUCe the diffiCUlty of this Con-
strained problem, we introdUCe a slaCk variable u` and reCast the formUlation in (F.2) as follows:
N	N2	2
f⅛nfmizU' L=N X L'=N XXM- fSq)(X"IL
f ,f , '	'=1	'=1 q=1
1N	1N
SUbjeCt to N A u'u> = I, N ∑u' = 0.
f(q) for q = 1, 2 are invertible,
(F.3a)
(F.3b)
(F.3C)
(F.3d)
22
Published as a conference paper at ICLR 2022
Ideally, We hope that u` = Y(z`). Introducing u` makes the f ⑴ and f ⑵ subproblems Uncon-
strained. This is a commonly used reformulation in neural network based multiview matching (see
(Benton et al., 2017; Lyu & Fu, 2020)), Which is reminiscent of the MAX-VAR formulation of
CCA (Kettenring, 1971; Carroll, 1968; Rastogi et al., 2015). Such reformulations oftentimes make
algorithm design easier, since the constraints are simplified.
The inveritibility and independence constraints in (F.3c) and (F.3d) are also not straightforWard to
enforce. Instead of directly enforcing the invertibility constraint in (F.3c), We design a regularization
term. Specifically, We use the idea of autoencoder that reconstructs the samples from their latent
representations f (q)(x'q)). We define aregularizer
N	N2	2
v=N x v`=N XXM)-r(q)(f (q)(x'q)))∣∣2 ,
'=1	'=1 q=1
(F.4)
as the reconstruction loss, Where r(q)’s are the reconstruction neural netWorks. Note that the above
term being zero does not necessarily indicate that the function f(q) is invertible, since this term
is only imposed on limited number of samples. But in practice, this idea is effective in learning
invertible transformations—also see (Wang et al., 2015; Lyu & Fu, 2020).
To promote the statistical independence constraint in (F.3d), We use the designed independence
regularizer, i.e.,
SUP R(q) = SUP	ICov [φ(q) (b(q)) LT⑷(gfa)川
Sup R	Sup	,,
Φ(q),τ⑷	φ(q),τ⑷(Jv[φ⑷(b⑷)]JV [τ(q) (c(q))]j
(F.5)
Where φ(q) and τ(q) are again represented by neural netWorks.
Let θ collect the parameters of f(q) and r(q), and η the parameters of φ(q) and τ(q). Putting all the
terms together, our Working cost function is summarized as folloWs:
minmax ɪ X L'(θ, U) + βɪ X V'(θ) + λR(θ, η),
U ,θ η N A~~/	N Aβ/
'=1	'=1
1 ET 1
subject to N NUU> = I, N U1 = O,
(F.6a)
(F.6b)
where U = [uι,…，un] ∈ Rd×n, and β and λ are nonnegative and R = P；=i R(q).
In terms of algorithm design, We propose to handle U, θ and η cyclically When the other tWo are
fixed, i.e., alternating optimization (AO).
First, we use stochastic gradient descent and ascent for the unconstrained θ and η subproblems. To
proceed, we sample a batch of data indexed by B ⊆ [N]. Then, θ and η can be updated by any
stochastic gradient based optimizers, e.g., the plain-vanilla stochastic gradient descent/ascent,
θ 一 θ - Y (|B| X(VθL'(θ, U) + βVθV'(θ)) + λVθR(θ, η))
η - η + δ (λVηR(θ, η)),
where γ and δ are the step sizes for the updates of θ and η, respectively. The stochastic gradients of
L, V are defined as follows:
VΘL ：=焉 X VθL'(θ, U)
|B| '∈B
VθV := IBi X vθv'(θ).
'∈B
(F.7a)
(F.7b)
23
Published as a conference paper at ICLR 2022
^ _ _ ^ _
Vθ R and V η R
In addition, the terms
are defined similarly. Taking the latter as an example. We
have VbηR = Pq2=1 Vb ηR(q), and Vb η R(q) is estimated by taking gradient w.r.t. η of the following
batch-estimated R(q) (the same holds for VbθR):
R(Bq) :=	(F.8a)
k总(φ⑷(钓-1⅜ IBφ⑷(裂)),⑷(毅)-由`pb T⑷(铲力 _
S由IPB G (W-由区北 E S-IPB(⑷(铲)):山区T⑷钟力2
Vb η R(q) := VηR(Bq)	(F.8b)
Vb θ R(q) := VθR(Bq).	(F.8c)
It was shown in (Fisher, 1915) that the correlation coefficient is computed using random samples
of Gaussian variables, the estimator in (F.8a) for R(q) is asymptotically unbiased. For other distri-
butions, the estimation also works well in practice; see, e.g., DCCA based works in (Wang et al.,
2015).
Consider more general stochastic optimizers, e.g., Adam (Kingma & Ba, 2015) and Adagrad (Duchi
et al., 2011). Then, the updates can be summarized as follows:
(
θ, V θ L + βV θ V + λV θ R)	(F.9)
η J SGD-Optimizer (η, -VηR).	(F.10)
where (F.10) uses the negative stochastic gradient since it is an ascending step, while stochastic
optimizers are by default descending the objective function.
The U subproblem consists of (F.3a) and (F.3b). It can be re-expressed as follows by expanding
(F.3a):
N	>2	>
N XTr 2u'u> - 2u' (fS1) (x'1)) + fS2) "))) + X fSq) ,州 f* (，州
'=1	∖	q=1
Note that the first term is a constant by (F.3b) and the last term does not involve u`. Then, we rewrite
the u` subproblem as
maximize Tr U F (1) + F (2)	,
subject to Nn UU> = I, NU1 = 0,
(F.11)
where F(G= IfSq)(x[q)),…，fSq)(xN))]
This is an orthogonal projection onto the set of row
zero-mean and orthogonal matrices. It is shown in (Lyu & Fu, 2020, Lemma 1) that such a projec-
tion problem, although nonconvex, can be solved to optimality via a mean-removed singular value
decomposition (SVD) procedure, i.e.,
U J NS ST >, With SDT > = SVD ((F(I) + F⑵)W) ,	(F.12)
where W = IN - N 11> removes the mean of F(I) + F⑵，D ∈ Rd×d holds the singular values,
S ∈ RD×D andT ∈ RN×D are the left and right orthogonal matrices in the SVD, respectively.
To summarize, an alternating optimization algorithm is summarized in Algorithm 1. Notice that
we use two different batch sizes (denoted as B1 and B2 in line 4 of Algorithm 1) to construct the
gradient estimations for VθL, VθV and VθR, VηR, respectively. The reason is that accurately
24
Published as a conference paper at ICLR 2022
Algorithm 1: Proposed Algorithm.
Data: x'q) for ' = 1, •…,N and q = 1, 2
Result: f(q) , r(q)
1 while stopping criterion is not reached do		
2	U J √NST>with SDT> = SVD ((F⑴ + F(2)) W);	
3	while stopping criterion is not reached do	
4		draw a random batch B1 and B2;	// use |B2 | > |B1 |
5		▽θL J |B1| P'∈B1 vθL';
6		▽θ V J 百 P'∈B1 vθ V';
7		▽ ηRJ P2=ι ▽nRB)	// using B2 and (F.8a), (F.8b)
8		▽ θRJ P2=ι ▽ RB) ;	// using B and (F.8a), (F,8c)
9		θ J SGDqptimizer ( θ, NeL + β^θV + λ▽θRj ;	// descent
10		η J SGDqptimizer (η, -WbηR) ;	// ascent
11	end	
12 end		
Table F.1: Computational complexity of the proposed algorithm in each iteration, where dθ and
dη denote the parameter dimensions of the encoder/reconstruction and the independence-promoting
networks, respectively.
	Complexity (flops)
Line 2	O(ND2)
Line 5	O(∣B1 ∣dθ)
Line 6	O(∣Bι∣dθ)
Line 7	O(∣B2∣dη)
Line 8	O(∣B2 ∣dθ)
Line 9	O(dθ)
Line 10	,`	O(dη)
Overall	O (ND +Wi + ∣B2l)dθ + WK)
estimating of R using (F.8a) often requires a relatively large batch size, while small batches may
suffice for the gradient estimations of L and V, according to our extensive simulations.
Computational Complexity. Tab. F.1 summarizes the computational complexity of each step.
Specifically, line 2 requires computing a thin SVD, which requires O(N D2 ) flops. Note that this is
linear in the number of samples N , and D is the dimension of the shared component, which is often
relatively small in practice.
Inside the inner loop line 4-10, lines 5 and 6 construct the gradient estimations w.r.t. θ and η.
These two steps cost O(∣Bι∣dθ) flops. Similarly, lines 7 and 8 use O(∣B2∣dη) and O(∣B2∣dθ) flops,
respectively. Note that we have used dθ and dη to denote the parameter dimensions of the en-
coder/reconstruction and the independence-promoting networks, respectively. Typically, |B1 | and
|B2| are small numbers compared to N (e.g., |B1 | = 128, |B2 | = 512 while N could easily exceed
105). For line 9 and 10, when a first-order stochastic optimizer (e.g., plain-vanilla SGD, ADAM,
Adagrad) is used, this step has a computational complexity that is linear in terms of the network
size.
One can see that all the steps scale linearly with size of the neural networks or the sample size, which
makes the algorithm easy to run with large-scale data sets and large-size neural feature extractors.
25
Published as a conference paper at ICLR 2022
Figure G.1: Left: z; middle: t-SNE of X⑴;right: t-SNE of X⑵.
G	Experiments： More Details and Additional Validations
In this section, We show all the experiment results with greater details, e.g., results under more
metrics, more setting details, and more demonstrations. We also include more real data experiments
using the CIFAR10 (Krizhevsky et al., 2009) and dSprites data (Higgins et al., 2017).
G.1 S ynthetic Data - Validating Main Theorems
in this subsection, we describe the synthetic data experiments. For synthetic data, we generate the
shared z ∈ R2 that is uniformly drawn from the unit circle, with noise N (0, 0.022) added to each
dimension. The private components are scalars C(I) ~ N(0, 2.0) and c(2) ~ Laplace(0,4.0). The
shared-to-private energy ratios for the two views are approximately -6 dB and -18 dB. The sample
size is N = 5, 000. And we use two different one-hidden-layer neural networks with 3 neurons
and softplus activation to represent the invertible g(q) ’s. The network parameters are drawn from
standard normal distribution.
The shared component z and the t-SNE of X(1) and X(2) are shown in Fig. G.1. one can see
that by incorporating strong noise and nonlinear transformations, the shape of circle is hardly to be
identified in both views.
in our simulations, f(q) is represented by a three-hidden-layer multi-layer perceptrons (MLPs) with
256 neurons in each layer with ReLU activations. in addition, φ(q) and τ(q) are represented by two-
hidden-layer MLPs with 128 neurons in each layer. We set batch size to be 1000, β = 1.0, λ = 0.1.
We use the Adam optimizer (Kingma & Ba, 2015) with initial learning rate 0.001 for all the parame-
ters. Besides, we also regularize the network parameters using kη k22 with a regularization parameter
0.1. This often helps improve numerical stability when optimizing cost functions involving neural
networks. We run lines 4-10 of Algorithm 1 for 10 epochs to update θ and η .
For ablation study, we test the methods with different combinations of L, R and V, i.e.,
(i)	the proposed method (L + V + R);
(ii)	the proposed without independence regularization (L + V);
(iii)	the proposed without reconstruction (L + R);
(iv)	the proposed with only latent correlation maximization (L);
(v)	we also test the performance with HSiC (Gretton et al., 2007) as the independence regular-
izer (L + V + HISC);
All methods stop when the average matching loss L reaches 0.01. The learned components by the
proposed method are shown in Fig. G.2. one can see that the estimated shared components are
matched, while the second view exhibits relatively large noise level as expected. For the estimated
private components, one can see that both δ⑴(∙) and δ⑵(∙) are approximately invertible functions.
To evaluate the performance of the synthetic experiment, we compute mutual information (Mi) be-
tween groups of random variables of interest (measured by the mutual information neural estimation
26
Published as a conference paper at ICLR 2022
Figure G.2: (a) Scatter plot of zb(1); (b) bc(1) as a function of c(1); (c) Scatter plot of zb(2); (d) cb(2) as
a function of c(2) .
(MINE) (Belghazi et al., 2018) and Gaussian kernel density estimation (KDE) (Davis et al., 2011)).
The results are averaged from 10 random trials.
One can see that all methods successfully extract the information about z in the sense that both
zb(q) = fbS(q) (x(q)) for q = 1, 2 have similarly high MIs with z. Besides, all methods output zb(q) and
bc(q) (c(q)) that have small MIs—meaning that they are not dependent.
Although most loss functions using latent correlation maximization extract the shared z’s informa-
tion well, the difference is articulated in extracting the private information. The proposed L+V+R
objective has the best performance on that regard. The method L+V+HSIC also works reasonably
well since HSIC serves the same purpose as R does—but with a kernel-based implementation.
Moreover, by looking at the last two columns, one can see that the methods with R and V perform
the best in removing the information of z from c(q) . This corroborates our analysis that both (7b)
(invertibility) and (7d) (independence) are vital to achieve private-shared information disentangle-
ment.
Tab. G.1 shows the results, with all the entries averaged over 10 random trials. One can see that
the results via the Gaussian KDE are consistent with those under MINE. That is, the proposed
L + V + R exhibits the best performance in terms of extracting and disentangling the shared and
private information.
G.2 Synthetic Data - Robustness to Strong Private Interference
In this subsection, we demonstrate the performance of the proposed method under different levels of
private component energy (which are often considered interference) (Ibrahim & Sidiropoulos, 2020;
Bach & Jordan, 2005; Lyu & Fu, 2020). First, we define the shared-to-private ratio (SPR) for the
q-th view as
SPR = 10 log10
DN PN=ikz`k2 ʌ
DN PN=1 kc'q) k2 )
dB.
For the experiment, we make both views have identical SPRs. We test the performance under SPR=-
10 dB, -20 dB and -30 dB, respectively.
27
Published as a conference paper at ICLR 2022
Table G.1: Mutual information (MI) between groups of variables. “f"： high score Preferred; “1”:
low score preferred; “n/a”: not applicable; zb(q) = fbS(q) for q = 1, 2.
b1),z (↑) I幺2Lz (↑) I b⑴,c⑴ 创 幺2屋⑵⑷忸I),c⑴(↑)忸2防⑵(T)I刖,z⑷I潟,z⑷
Metric	MINE-based MI Estimation (BeIghaZi et al., 2018)							
L	2.32±0.06	2.36±0.10	0.01±0.00	0.00±0.00	0.45±0.13	0.33±0.07	0.39±0.14	0.43±0.04
L+V	2.37±0.05	2.38±0.08	0.01±0.00	0.01±0.00	0.55±0.09	0.33±0.08	0.31±0.06	0.43±0.05
L+R	2.32±0.05	2.33±0.07	0.02±0.01	0.00±0.00	0.90±0.42	0.22±0.12	0.09±0.06	0.22±0.12
L+V+R	2.43±0.04	2.39±0.09	0.01±0.01	0.01±0.00	1.22± 0.33	0.85±0.23	0.04±0.02	0.11±0.03
L + V+HSIC	2.48±0.09	2.43±0.08	0.01±0.00	0.01±0.01	0.68±0.25	0.52±0.14	0.04±0.01	0.07±0.02
Metric	Gaussian kernel density estimate (KDE)-based MI Estimation
L	2.73±0.22	2.88±0.22	0.00±0.00	0.00	0.33±0.13	0.01±0.02	0.34±0.13	0.37±0.06
L+V	2.95±0.21	2.91±0.26	0.00±0.01	0.00	0.38±0.11	0.01±0.02	0.24±0.04	0.39±0.07
L+R	2.83±0.18	2.86±0.15	0.00	0.00	0.72±0.40	0.10±0.10	0.08±0.07	0.28±0.16
L+V+R	3.27±0.07	2.88±0.25	0.00±0.01	0.00	0.99±0.30	0.35±0.11	0.03±0.04	0.08±0.03
L + V+HSIC	3.35±0.16	2.95±0.33	0.00	0.00	0.78±0.22	0.06±0.06	0.05±0.02	0.02±0.01
Table G.2: Mutual information (MI) between groups of variables under different SPR. “f"： high
score preferred; “^”: low score preferred; b(q) = fbSq) for q = 1,2.
冢1),z ⑴ I 冢” (↑) I 冢I),c⑴卬 I 冢”⑵(i) I UD (↑) I b(”⑵(↑) I m 卬 I b(2),z 卬
SPR	MINE-based MI Estimation (Belghazi et al., 2018)
-10 dB	2.41±0.07	2.43 ±0.05	0.01±0.01	0.01±0.01	1.72±0.41	0.52±0.10	0.02±0.01	0.19±0.03
-20 dB	1.81±0.10	2.15±0.09	0.10±0.06	0.01±0.01	1.41±0.15	1.15±0.07	0.08±0.04	0.06±0.02
-30 dB	1.16±0.07	1.55±0.10	0.11±0.09	0.07±0.04	1.77±0.42	0.73±0.14	0.03±0.02	0.14±0.11
Table G.3: Mutual information (MI) between groups of variables with different β (i.e. reconstruction
term). "f”: high score preferred; “^”: low score preferred; b⑷=fSq) for q = 1, 2.
I b⑴,z (↑) I Z⑵,z (↑) I 冢I),c(I) (D I b⑵,C⑵(()I c(I),c(I) (↑) I b(2),c⑵(↑) ∣ b"Z!)∣ 省2,z Q)
λ = 1e-1	MINE-based MI Estimation (Belghazi et al., 2018)
β=	1e-2	2.13±0.26	2.11±0.33	0.01±0.01	0.01±0.00	0.77±0.03	0.48±0.18	0.13±0.02	0.17±0.07
β=	1e-1	2.16±0.23	2.12±0.21	0.01±0.01	0.01±0.00	1.09±0.37	0.70±0.20	0.10±0.06	0.16±0.07
β=	1e0	2.41±0.13	2.38±0.12	0.01±0.00	0.01±0.00	1.48±0.08	0.93±0.17	0.02±0.00	0.10±0.04
β=	1e1	2.34±0.19	2.28±0.08	0.04±0.04	0.02±0.02	0.86±0.40	0.41±0.21	0.15±0.10	0.52±0.26
β=	1e2	2.32±0.01	1.77±0.07	0.07±0.02	0.40±0.05	0.58±0.04	0.21±0.03	0.31±0.01	0.69±0.05
Tab. G.2 shows the evaluation results averaged from 10 trials. One can see that as SPR decreases, the
MI between the extracted zb and z decreases—but only gracefully. The slight decline of performance
is because the matching of two views becomes harder when the private components get stronger.
However, even if SPR=-30 dB, the extraction and disentanglement of private and shared information
are still clearly achieved. Such robustness to strong private interference is considered a key feature
of linear CCA (Ibrahim & Sidiropoulos, 2020) and post-nonlinear CCA (Lyu & Fu, 2020). Our
analysis and evaluation in this work show that such resilience is also inherited by the proposed
approach.
G.3 Synthetic Data - Sensitivity to Hyperparameters
In this subsection, we investigate the sensitivity to the key hyperparameters β and λ. The results are
shown in Tab. G.3 and Tab. G.4, respectively. One can see that for the reconstruction regularization
parameter (i.e., β) does not affect the results too much unless it was set to be overly large (i.e., β = 1
or 100 in our simulation). This makes sense, since reconstruction is for preventing trivial degenerate
solutions, and giving this part too much attention may not really help the learning goals (e.g., shared
and private information extraction) reflected in the other parts of the loss function. From Tab. G.4,
one can see that the choice of λ affects the performance slightly more than β . It makes sense, since
λ reflects the attention that the algorithm puts on the private component extraction part. We should
mention that, for real data analysis with a downstream task (e.g., classification), one can choose
these hyperparameters using a validation set.
28
Published as a conference paper at ICLR 2022
Table G.4: Mutual information (MI) between groups of variables with different λ (i.e. independence
regularize]). “f”: high score preferred; “^”: low score preferred; b(q) = fbSq) for q = 1, 2.
b⑴,z (↑)	I 冢2Zz	(↑)	I 冢I),c(I)(D	I 冢。,c⑵(D	I	b(D,c⑴(↑)	I	b(2),c⑵(↑)	∣	b(D,z	())∣	b(2),z Q)
β = 1e0 I	MINE-basedMIEstimation(Belghazietal.,2018)	一
λ = 1e-2
λ = 1e-1
λ = 1e0
λ = 1e1
λ = 1e2
2.36±0.09
2.41±0.13
2.04±0.12
1.49±0.18
0.80±0.23
2.34±0.05
2.38±0.12
2.00±0.11
1.55±0.11
0.96±0.36
0.01±0.00
0.01±0.00
0.04±0.01
0.17±0.07
0.46±0.13
0.01±0.00
0.01±0.00
0.02±0.00
0.17±0.10
0.28±0.07
0.56±0.18
1.48±0.08
0.71±0.17
0.61±0.04
0.18±0.08
0.51±0.14
0.93±0.17
0.37±0.10
0.17±0.07
0.22±0.06
0.25±0.07
0.02±0.01
0.15±0.06
0.23±0.02
0.75±0.37
0.36±0.08
0.10±0.04
0.22±0.03
0.16±0.05
0.57±0.18
Table G.5: Network structures for the MNIST experiment.
Encoders	Decoders
input: x'q) ∈ R28×28×1 4 X 4 Conv, 64 ReLU, stride 2 4 × 4 Conv, 32 ReLU, stride 2 FC 256, ReLU FC D + Dq	input: f (q)(x'q)) ∈ RD+Dq FC 256, ReLU FC 7 × 7 × 32, ReLU 4 × 4 Conv_Trans, 64 ReLU, stride 2 4 × 4 Conv_Trans, 1, stride 2
G.4 Real Data - More on Validating Theorem 1
Multiview MNIST Data. In this subsection, we provide more details and evaluation results on the
MNIST experiment. The way of generating such two views (as shown in Fig. 1 of the main text) of
MNIST data is similar to the data augmentation ideas used in AM-SSL, e.g., rotation, adding noise,
cropping, flipping (Chen et al., 2020; Grill et al., 2020). We aim to match the two views and try
to learn the shared representations, which should encode the class label information. The dataset
has 70,000 samples that are 28×28 images of handwritten digits. For the latent dimension, we set
D = 10, D1 = 20 and D2 = 50. In particular, since the second view consists of large random noise
that are not of interest, we only add the independence regularizer R(1) on the first view (i.e., the
rotated digits) to learn the private component of x'1). The learned c'1) was used to generate new
samples; see the illustration in our main text.
The network structure used for all methods is shown in Tab. G.5. The network structure for φ(q) and
τ(q) are MLPs with three hidden layers of 64 neurons. For hyperparameters, we set batch size to be
|B1| = 100 and |B2| = 1000, β = 1.0, λ = 100.0. For optimizer, we also use Adam (Kingma &
Ba, 2015) with initial learning rate 0.001 for θ and 1.0 for η. And we add squared `2 regularization
for both θ and η, with different regularization parameters that are 0.0001 and 0.1, respectively. We
also run the SGD optimizer for 10 epochs to update θ and η.
In Tab. G.6, we show more evaluation results on the learned shared information across views. To
be specific, We feed the learned z'1) ′s to a classifier and the k-means algorithm, to observe if the
learned representations improve the performance of supervised and unsupervised learning tasks. For
the classification task, we split the data as 50,000/10,000/10,000 for training/validation/test sets. We
train a linear support vector machine (SVM) using the training data. The performance is measured by
classification error (ERR). For the clustering task, we use the standard k-means to cluster on all the
data samples. After clustering, we report the performance on the test set. We use a number of metrics
to measure performance, namely, clustering accuracy (ACC), normalized mutual information (NMI),
and adjusted Rand index (ARI) (Yeung & Ruzzo, 2001). Among these metrics, ARI ranges from
-1 to +1, with 1 being the best and -1 the worst and NMI range from 0 to 1 with 1 being the best.
The “Baseline” denotes the results of simply applying SVM and k-means onto the raw data of
the first view, i.e., x('1) for ` = 1, . . . , N. All the results of algorithms that involve stochastic
methods are averaged over 5 random initializations. One can see that all methods have comparably
good results in terms of learning informative representations across views. The results empirically
validate our Theorem 1 that latent correlation maximization is a useful criterion to extract the shared
information with guarantees. In particular, Barlow Twins performs slightly better in terms of
benefiting downstream classification and clustering tasks. Nonetheless, the proposed method can
29
Published as a conference paper at ICLR 2022
Raw data	Proposed	DCCA	Barlow Twins	BYOL
Clust. Acc= 54.8% Clust. ACC= 95.7% Clust. ACC= 96.0% Clust. ACC= 97.1% Clust. ACC= 94.5%
Figure G.3: t-SNE of the results on multiview MNIST of the second view. Baselines: DCCA (Wang
et al., 2015), BarloW Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020).
Table G.6: The classification error (first row) and clustering results (rows 2 to 4) of the two-view
MNIST dataset. “↑”: high score preferred; “^”: low score preferred.
Baseline L + V + R	L + V	DCCA	DCCAE Barlow Twins BYOL
ERR (Q
ACC (↑)
NMI(↑)
ARI (↑)
13.40%	2.81%±0.21%	2.95%±0.23%	2.87%±0.16%	2.85%±0.05%	2.05%±0.07%	2.67%±0.22%
37.35%~97.03%±0.13%~96.80%±0.40%~97.02%%±0.12%~96.95%±0.12%~98.06%±0.06%~95.56%±0.41%
0.337	0.922±0.003	0.923±0.007	0.922±0.003	0.920±0.003	0.947±0.002	0.895±0.023
0.216	0.936±0.003	0.931±0.009	0.935±0.003	0.934±0.002	0.958±0.001	0.904±0.047
Table G.7: Augmentation Used for CIFAR10 to Generate Multiple Views.
Transformation	Value	Probability
ColorJitter	brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2	0.8
GrayScale	-	0.2
RandomResizedCrop	SCaIe=(0.2,1.0), ratio=(0.75, 4/3)	-
HorizontalFlip	-	05
GaussianBlur	σ 〜U[0.1, 2]	-	05
Solarization	-	0.4
Normalization	-	-
also guarantee extracting private information and facilitate cross-view image generation (see the
experiments in the main text), which is out the reach of Barlow Twins and BYOL.
In addition to showing the visualization of the learned embedding of the firs view in Fig. 1 of the
main paper, we also plot the t-SNE of the learned representation z`2) of the second view (i.e., the
noisy digits). The results are shown in Fig. G.3. One can see that the visualization and clustering
accuracy are similar to those obtained from 苗I).
Augmented CIFAR10 Data for SSL. We also use the CIFAR10 dataset (Krizhevsky et al., 2009)
to validate Theorem 1. The CIFAR10 dataset contains 50,000 and 10,000 images of size 32 × 32 for
training and testing, respectively. There are 10 different classes. We use ResNet18 as the backbone
network for learning the representation. Since CIFAR10 images are small, we replace the first 7x7
Conv layer of stride 2 with 3x3 Conv layer of stride 1. We also remove the max pooling layer. We
follow the evaluation method in (Chen & He, 2021) to stop the algorithms after one hundred epochs.
In terms of data augmentation, we use the pipeline of different transformations in Tab. G.7. Note
that for the proposed method, we only impose constraint (7c) since our goal here in this task is to
extract essential shared information.
We evaluate the proposed method and two AM-SSL baselines as mentioned in the main text, namely,
Barlow Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020) by feeding the learned rep-
resentations to a linear classifier. We report both the Top-1 linear classification accuracy and the
KNN (with k = 5) accuracy. The results are shown in Tab. G.8. One can see that different methods
achieve comparable results. The proposed method and BYOL attain essentially the same accuracy.
The t-SNE (Van der Maaten & Hinton, 2008) visualizations of the test set is plotted in Fig. G.4.
One can see that all methods extract “identity-revealing” information to a certain extent, as in the
MNIST case.
30
Published as a conference paper at ICLR 2022
Table G.8: Evaluation using CIFAR10.
BYOL BarloW Twins Proposed
Classification Acc. (%)	84.2	82.8	84.2-
KNN (k = 5) Acc. (%)	80.5	78.7	81.0
BYOL	Barlow Twins	Proposed
Figure G.4: t-SNE of learned representations for CIFAR10.
samples of viewl
samples of view2
Figure G.5: Samples of the lower elevations (VieWI) and higher elevations (view2).
Remark 1 We should remark that the experiment results suggest that latent correlation maximiza-
tion (or latent component matching) used in many AM-SSL and DCCA methods works towards the
same ultimate goal under our generative model in (5). However, this does not suggest that different
SSL and DCCA methods are essentially the same in practice—one should not expect that. In fact,
there are many factors that affect DCCA and AM-SSl methods, results, e.g., model mismatches,
optimization procedure, network construction, and the detailed designs in their loss functions. The
difference between the methods normally are more articulated with larger data sets or more complex
problems. Nonetheless, our interest lies in theoretical understanding of their common properties,
other than the differences in practical implementations. From this perspective, the results in this
section support our theoretical analysis in Theorem 1.
G.5 Real Data - More on Validating Theorem 2
Cars3D Data for Cross-sample Generation. In this subsection, we provide more detailed settings
and results of the Cars3D experiment. To create a multiview dataset, we assume that given the car
type that is captured by shared variables Z and the azimuths that are captured by C⑷,the generation
mappings g(1) and g(2) produce car images with low elevations and high elevations, respectively (so
they must be different mappings).
We split the car images as follows. We treat the same car model (e.g., a red convertible) with
lower and higher elevations as x'I) and x'2), respectively. The azimuths are randomly shuffled
with different pairs of x'I) and x'2). This way, if our generative model holds, z, c(q)and g(q) are
responsible for ‘type’, ‘azimuth’ and ‘elevation’, respectively. Some samples are shown in Fig. G.5.
Under our splitting, each view has 2×183×24 = 8784 RGB images that all have a size of 64×64×3.
We model the ‘type’ information z using D = 10 latent dimensions since many different factors
(e.g., color and shape) together give rise to a ‘type’. on the other hand, we set D1 = D2 = 2 to
model the ‘azimuth’ information.
Tab. G.9 shows network structures for the encoders and decoders of our formulation in (F.6). In the
table, FC denotes fully connected layer, Conv denotes convolutional layer and Conv_Trans denotes
2D transposed convolutional layer. As before, the structures of φ(q) and τ (q) are MLPs with two-
31
Published as a conference paper at ICLR 2022
Table G.9: Network structures for the Cars3D experiment.
Encoders	Decoders
input: x'q) ∈ r64×64×3-
4 × 4 Conv, 32 ReLU, stride 2
4 × 4 Conv, 32 ReLU, stride 2
4 × 4 Conv, 64 ReLU, stride 2
4 × 4 Conv, 64 ReLU, stride 2
FC 256, ReLU
FC 10+2
input: f (q)(x'q)) ∈ R10+2
FC 256, ReLU
FC 4 × 4 × 64, ReLU
4 X 4 Conv-Trans, 64 ReLU, stride 2
4 × 4 Conv-Trans, 32 ReLU, stride 2
4 × 4 Conv-Trans, 32 ReLU, stride 2
4 × 4 Conv-Trans, 3, stride 2
Figure G.6: Generated samples by fixing Z and varying bjq); rows in blue boxes are w/ R; rows in
green boxes are w/o R.
hidden-layer and 256 neurons for each layer. For hyperparameters, we set batch size to be |Bi| =
100 and ∣B21 = 800, β = 0.1, λ = 1.0. For this real data experiment, we also use Adam (Kingma
& Ba, 2015) as the optimizer with initial learning rate 0.001 for θ and 1.0 for η. We add ∣∣ηk2 for
regularization with parameter 0.0001. We limit the inner loops for solving the θ and η subproblems
to 10 epochs as well.
Figs. G.6 and G.7 show more results under the same setting as in Fig. 2 in the main text.
dSprites Data for Cross-sample data Generation. We present the results on an additional dataset,
i.e., dSprites (Higgins et al., 2017). In the dSprites dataset, 64 × 64 images are generated based on
five factors: 3 shapes (square, ellipse, heart), 6 scales, 40 orientations, and 32 different horizontal
and vertical coordinates.
In particular, we take a subset that contains squares and hearts as the two views of data. In this
subset, all the data samples are with the same scale. We assume the generating functions g(1)
and g(2) are responsible for the shape of square and heart, respectively. We treat the orientation
and horizontal positions as the shared information, i.e., z, and the vertical position as the private
information c(q). The vertical coordinates are random and not matched between different pairs of
x'1) and x'2). Overall, we have 40,960 samples for each view. We set D = 2 and Di = D2 = 1.
We use the same neural network structure as in Tab. G.9. The only differences lie in the input and
latent dimensions. The network structure for φ(q) and τ(q) are MLPs with two hidden layers of 128
neurons, and we set |B1 | = 100, |B2| = 500, β = 0.1, and λ = 100.0. Similar as before, we use
the Adam (Kingma & Ba, 2015) optimizer with initial learning rate 0.001 for θ. As before, we add
a squared `2 norm regularization on the network parameters η, and set the regularization parameter
to 0.1. We let the inner loop stochastic optimizers run for 10 epochs.
32
Published as a conference paper at ICLR 2022
Figure G.7: Generated samples by fixing b(q) and varying Z; rows in blue boxes are w/ R; rows in
green boxes are w/o R.
We conduct the same cross-sample data generation experiment as in the Cars3D case. Fig. G.8
shows the results. To be specific, we extract Z'q) = fs(x'q)) that represents the rotation and
horizontal coordinate information and cjq) = fp(x'q)). And we combine this information to-
gether to generate synthetic samples X'qj with the learned reconstruction network r(q), i.e., X'q) =
b(q) ([(b'q))>,(Cq))T).	'
The observations are similar to that in the Cars3D experiments. Ideally, the generated samples
should have the rotation and horizontal position of x(q) (contained in zZ(q)) while the vertical po-
sition of x(jq) (contained in cb(jq)). One can see that without using the independence regularizer R,
the generated samples may have rotation change, shape deformation compared to x'q) or simply the
vertical position is not exactly replicated from the sample x(jq). However, with the R regularization,
the results are exactly what one expects to see. This again verifies our claim in Theorem 2.
Multiview MNIST Data for Cross-view Generation. Using the multiview MNIST data, we also
show the cross-view generation results in Fig. G.9. Here, we extract z(2) = f(2)(x'2)) from the
second view and cj-1) = fp1)(xj1)) from the first. Then, we generate b(j) = b(1)([(zb(2) )>, (cj1))>]τ)
shown in the blue and green boxes. Ideally, the generated samples should have the digit information
of x'2) (contained in z'2)) while the style information of XjI) (contained in CjI)). Clearly, using R
attains the desired results. Note that this dataset is challenging as the noise in view 2 is very high,
making it hard to achieve perfect matching and reconstruction—but our result is still plausible.
Remark 2 We would like to mention that multiview data and pertinent learning tasks are perva-
sive in the real world. For example, acoustic features and articulatory recordings are two views of
speech signals, and multiview based representation learning can be used to enhance speech recog-
nition (Arora & Livescu, 2013; Wang et al., 2015). Another example is cross-media information
retrieval (Gong et al., 2014). There, a data entity has a text view and an image view, and the task
is to retrieve one view from another. This task can be efficiently done in the learned shared do-
main. In natural language processing, multilingual word embedding can also be formulated as a
CCA-type shared information learning problem; see (Socher & Fei-Fei, 2010; Dhillon et al., 2012).
In computer vision, there are a number of important tasks such as image style translation (e.g.,
sketch to picture and picture to cartoon) (Zhu et al., 2017; Huang et al., 2018; Lee et al., 2018) and
super-resolution (Ledig et al., 2017) can be considered as multiview learning problems. In particu-
lar, image style translation will benefit from our method’s guaranteed shared (content) and private
(style) disentanglement.
33
Published as a conference paper at ICLR 2022
Figure G.8: Generated samples by fixing z` (rotation and horizontal position) and varying cjq)
(vertical position). Top: the square view; bottom: the heart view; rows in blue boxes are w/ R; rows
in green boxes are w/o R.

w/ R
ΞBQE1□Ξ□E]□□
QΠHB□B□DBQ
qπħħqħbhħq
BlEl0喀QQe。囱
1染
咪
□ B□Ξ□□E1□□EI
QΠ□HDDB□□□
HUΠΠΠHΠΠΠΠ
ElBmEEIPImEl
E3K9E9QE1BE9E9K9Ξ
Figure G.9: Cross-view generation from x'2) to x'I).
H Additional Notes on S hared - Private Modeling
Regarding the generative model in (5), some remarks are as follows. The intuition that multiview
data consists of shared and private components are widely used; see, e.g., (Huang et al., 2018; Lee
et al., 2018; Wang et al., 2016; Gundersen et al., 2019). However, explicit generative models were
only considered in limited theory-oriented works.
34
Published as a conference paper at ICLR 2022
The model in (5) can be understood as a nonlinear generalization of the linear CCA model in
(Ibrahim & Sidiropoulos, 2020), where the views are modeled as
x'q) = A⑷[z>, (c'q))>>
for q = 1, 2. In (Lyu & Fu, 2020), a special type of nonlinear model, namely, the post-nonlinear
mixture model, was analyzed. There, the model is
x'q) = g⑷(A⑷[z>, (c'q))>]τ),
where g(q) (y) applies a nonlinear distortion to each element of y individually. However, post-
nonlinear models are much less general compared to our model in (5)—where g(q) (y) nonlinearly
distorts all elements of y jointly in an unknown way. More recently, under the context of AM-SSL,
the work in (Von Kugelgen et al., 2021) considered a multiview generative model that is similar to
our model, but the views share the same generative nonlinear function, i.e.,
x'q) = g([zτ, (c'q))τ]τ).
This assumption restricts the applicability of the model to scenarios where the two views are gen-
erated using exactly the same nonlinear distortions, which may be less flexible. Our model in (5)
subsumes the models in (Ibrahim & Sidiropoulos, 2020; Lyu & Fu, 2020; Von Kugelgen et al., 2021)
as its special cases.
I Avoiding Reconstruction Using Entropy Regularization
I.1 Entropy Regularization and Shared Component Identifiability
If we ignore private information extraction, our formulation for shared information extraction is as
follows:
mfi(n1)i,mf(i2z)eE	fS(1) x(1) - fS(2) x(2)2	(I.1a)
subject to f(q) for q = 1, 2 are invertible,	(I.1b)
E	fS(q)	x(q)	fS(q)	x(q)τ =I, E	hfS	x(q)i =0, q= 1,2,	(I.1c)
with the latent variables satisfying:
p(z, c(1), c(2)) = p(z)p(c(1))p(c(2)).
We hope to encourage invertibility of f(q) without using a decoder reconstruction network. To this
end, We generalize the idea in Theorem 4.4 in (Von Kugelgen et al., 2021). Note that (Von Kugelgen
et al., 2021) deals with the case where only one f is learned (i.e., f(1) = f(2)). Here, we show that
this idea can be used under our case as well. To see this, let us consider the following formulation:
mi(n1)im(i2z)e E	fS(1) x(1) - fS(2) x(2)2 -H fS(1) x(1)	(I.2a)
subject to fS(q) : RMq → (0, 1)D.	(I.2b)
where H(∙) computes the differential entropy of its argument. The formulation still aims to match
the latent representations of the two views, but at the same time maximizes the entropy of the learned
features of a the first view. The proof of this case consists of three major steps.
Step 1. It is straightforward to see that the optimal solution of (I.2) is
b = fS1)(x(1)) = fS2)(x(2)), b ~ Uniform(0,1)D
since the first term has optimal value 0 when two view are perfectly matched, and the differential en-
tropy of a random variable is maximized when the distribution on (0, 1)D is uniform (Cover, 1999).
Next, following the idea in (Von Kugelgen et al., 2021), by the Darmois construction (Darmois,
35
Published as a conference paper at ICLR 2022
1951), there exists d(∙) : Z → (0, I)D which maps the ground-truth Z toa uniform random variable
on (0, 1)D. Thus, one can construct an optimal solution of (I.2) as:
fS(q) = d ◦
1:D
where the first D dimensions of the output of (g(q))	are fed to d(∙).
Step 2. By using our proof technique in Theorem 1, employing the equation zb = fS(1)(x(1)) =
fS(2)(x(2)), it can be shown that zb only depends on the shared component z but does not depend
on either c(1) or c(2), which we denote as zb = γ(z). Note that the proof of this part holds since
it only uses the latent correlation maximization (or fS(q) matching). Using the same derivation as in
Theorem 1, we have the Jacobian of h(1) as follows:
J(1)
HS(1)
J2(21)
(1)
J11	0D×D1
(1)	(1)
J21 J22
which indicates that zb only depends on z but not c(1). The above also holds for the second view.
Note that the possibility of fS(q) being a trivial constant solution (and thus making HS(1) = 0) is
ruled out since fS(q)’s entropy is maximized.
Step 3. The last step in Theorem 1 is to use rank(J(1)) = D + D1 to show that J1(11) ∈ RD×D
has full rank. There, rank(J(1)) = D + D1 is natural since f(1) is constructed to be invertible
using an autoencoder (and thus f (1) ◦ g(1) is also invertible). Here, we could not use this argument.
However, similar to Theorem 4.4 in (Von KUgelgen et al., 2021), by applying Proposition 5 of
(Zimmermann et al., 2021), one can show that b = γ(z) where Y(∙) is an invertible function, if
p(z) is a regular density, i.e., 0 < p(z) < ∞ everywhere. Note that under our generative model,
f(1) (x(1)) = f(2) (x(2)) for all x(q). Hence, the above derivations can be repeated for f(2). This
concludes the proof.
I.2 Realization and Connection to Contrastive SSL
To implement the formulation (I.2), following the idea in (Von Kugelgen et al., 2021), one can use
the idea of InfoNCE (Gutmann & Hyvarinen, 2010; Oord et al., 2018), which it has interesting
connections to contrastive SSL (Wang & Isola, 2020). In particular, the formulation of InfoNCE is
as follows:
E{x'1) ,x'2) }K=1 〜p(x⑴,x(2))
- XX lo	exp{sim(bi, bi)∕τ}
,⅛ § Pj=ι eχp{sim(bi, bj)/t上
(I.3)
where b` and Z' are the learned representations of the two corresponding samples x'1) and x'2),
respectively, sim(a, b) computes the similarity of its arguments, τ is a temperature hyperparameter
and there are K samples of each batch where K - 1 of them are negative.
Note that in (Von Kugelgen et al., 2021), only one generative function g(∙) is considered. In their
implementation, given sample pairs {x'1), x'2)}j=ι, the above InfoNCE objective can be rewritten
with τ = 1 and sim(a, b) = -ka - bk22 as
K
X
i=1
[¢'?¢'2))3"(X ⑴,X(2) )
f(xi(1)) - f(xi(2))2 +logX exp - f(xi(1)) - f(x(j2))2
j=	(I.4)
The second term is a non-parametric entropy estimator of the representation as K → ∞ (Wang &
Isola, 2020). The above nicely connects AM-SSL with contrastive learning when g(1) = g(2) and
only one encoder is used, i.e., f(1) = f(2).
36
Published as a conference paper at ICLR 2022
However, in our problem the generative functions are different in each view. Hence, the formulation
above is not directly applicable. Nonetheless, one can use the slack variable based design as in (10).
Then, the problem can be reformulated as
E{x'1),x'2)}K=ι~P(x ⑴,x(2))
K 2	2	K
X X ui - f(q)(xi(q))	+ log X exp{-kui - ujk22}	.
i=ι [q=ι	j=ιJ
(I.5)
Note that the entropy regularization is imposed on the slack variable u—which indirectly promotes
high entropy of f(q)’s. This way, one can handle Q views with different generative functions g(q)’s.
37