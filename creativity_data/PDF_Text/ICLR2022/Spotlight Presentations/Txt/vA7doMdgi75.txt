Published as a conference paper at ICLR 2022
Implicit Bias of Projected Subgradient
Method Gives Provable Robust Recovery of
Subspaces of Unknown Codimension
Paris Giampouras, Benjamin D. Haeffele and Rene Vidal
Mathematical Institute for Data Science
Johns Hopkins University
Baltimore, MD, USA
{parisg,bhaeffele,rvidal}@jhu.edu
Ab stract
Robust subspace recovery (RSR) is the problem of learning a subspace from sample
data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is
a robust subspace recovery method that aims to find a basis for the orthogonal
complement of the subspace by minimizing the sum of the distances of the points
to the subspaces subject to orthogonality constraints on the basis. Prior work has
shown that DPCP can provably recover the correct subspace in the presence of
outliers as long as the true dimension of the subspace is known. In this paper, we
show that if the orthogonality constraints -adopted in previous DPCP formulations-
are relaxed and random initialization is used instead of spectral one, DPCP can
provably recover a subspace of unknown dimension. Specifically, we propose a very
simple algorithm based on running multiple instances of a projected sub-gradient
descent method (PSGM), with each problem instance seeking to find one vector in
the null space of the subspace. We theoretically prove that under mild conditions
this approach succeeds with high probability. In particular, we show that 1) all of
the problem instances will converge to a vector in the nullspace of the subspace
and 2) the ensemble of problem instance solutions will be sufficiently diverse
to fully span the nullspace of the subspace thus also revealing its true unknown
codimension. We provide empirical results that corroborate our theoretical results
and showcase the remarkable implicit rank regularization behavior of the PSGM
algorithm that allows us to perform RSR without knowing the subspace dimension.
1	Introduction
Robust subspace recovery (RSR) refers to the problem of identifying an underlying linear subspace
(with dimension less than the ambient data dimension) from sample data points that are potentially
corrupted with outliers (i.e., points that do not lie in the linear subspace). Many methods for RSR have
been proposed in the literature over the past several years (Xu et al., 2012; You et al., 2017a; Lerman
& Maunu, 2018). Formulations based on convex relaxations and decompositions of the data matrix
into a low-rank matrix plus a matrix of sparse corruptions - either entrywise-sparse corruptions as in
Candes et al. (2011) or columnwise-sparse corruptions as in Xu et al. (2012); McCoy & TroPP (2011)
- can, in certain situations, be shown to provably recover the true subspace when the dimension is
unknown. However, these theoretical guarantees often require the dimension of the subspace, d, to be
significantly less than the ambient dimension of the data, D, and these methods are not suitable for
the more challenging regime of subspaces of high relative dimension (i.e., when Dd ≈ 1).
Dual Principal Component Pursuit. Recently, progress has been made towards solving the RSR
problem in the high relative dimension regime by a formulation termed Dual Principal Component
Pursuit (DPCP). As implied by its name, DPCP follows a dual perspective of RSR by aiming to
recover a basis for the orthogonal complement of the inliers’ subspace. As shown in (Tsakiris &
Vidal, 2018), DPCP is provably robust in recovering subspaces of high relative dimension. However,
a key limitation of DPCP is that it requires a priori knowledge of the true subspace dimension.
1
Published as a conference paper at ICLR 2022
DPCP for C = 1. Let X ∈ RD×(N+M) denote the data matrix defined as X = [X O]Γ, where
X ∈ RD×N is a matrix containing N inliers as its columns, O ∈ RD×M is a matrix containing M
outliers, and Γ is an unknown permutation matrix. DPCP was first formulated by Tsakiris & Vidal
(2018) for handling subspaces of codimension c = D - d equal to 1 (i.e., the subspace is a hyperplane
with dimension d = D - 1). In this case, DPCP is formulated as the optimization problem
min ∣∣X>bkι s.t. ∣∣b∣∣2 = 1,	(1)
b∈RD
which is nonconvex due to the spherical constraint imposed on the normal vector b ∈ SD-1 of the
D - 1 dimensional hyperplane. Tsakiris & Vidal (2018) showed that the global minimizer of (1) is a
normal vector of the underlying true hyperplane when both inliers and outliers are well-distributed
or the ratio between the number of inliers and number of outliers is sufficiently small. Following a
probabilistic point of view, (Zhu et al., 2018) presented an improved theoretical analysis of DPCP
giving further insights on the remarkable robustness of DPCP in recovering the true underlying
subspaces even in datasets heavily corrupted by outliers. Moreover, the authors introduced a projected
subgradient method which converges to a normal vector of the true subspace at a linear rate.
Recursive DPCP for known c > 1. Zhu et al. (2018) also proposed an extension to DPCP to
subspaces with codimension c > 1 via a projected subgradient algorithm that tries to learn c
normal vectors to the subspace in a recursive manner. Specifically, after convergence to a normal
vector, the projected subgradient algorithm is initialized with a vector orthogonal to the previously
estimated normal vector. However, for that approach to be successful, knowledge of the true subspace
codimension c becomes critical. Specifically, ifan underestimate of the true codimension c is assumed
the recovered basis for the null space, B, will fail to span the whole null space, S⊥. On the other
hand, an overestimate of C will lead to columns of B corresponding to vectors that lie in S.
Orthogonal DPCP for known c. Zhu et al. (2019) proposed an alternative to (1) which attempts to
solve for C normal vectors to the subspace at once by minimizing the sum of the distances from the
points to the subspace:
min ∣∣X >B∣∣12	s.t. BB = I.	(2)
B∈RD×c	,
The authors also proposed an optimization algorithm based on the projected Riemannian subgradient
method (RSGM), which builds on similar ideas as the projected subgradient method of Zhu et al.
(2018) and enjoys a linear converge rate when the step size is selected based on a geometrically
diminishing rule. Ding et al. (2021) provided a geometric analysis of (2) which shows the merits of
DPCP in handling a) datasets highly contaminated by outliers (in the order of M = O(N2)) and b)
subspaces of high relative dimension. However, a key shortcoming of this approach is that, because
all minimizers of (2) are orthogonal matrices, a prerequisite for recovering the correct orthogonal
complement of the inliers subspace is the a priori knowledge of the true codimension C (see Fig. 1).
Contributions. In this work, we address this key limitation by proposing a framework that allows
us to perform robust subspace recovery in the high relative subspace dimension regime without
requiring a priori knowledge of the true subspace dimension. In particular, our proposed approach
is based on the simple idea of solving multiple, parallel instances of the DPCP formulation for
subspaces of codimension one,
0
c
min	X IlX>bikι s.t. IIbiIl2 = 1, i = 1, 2,...,c0	(3)
B∈RD×c0
i=1
where C0 is assumed to be an upper bound of C, i.e., C0 ≥ C. Contrary to (2), the objective function in
(3) decouples over the columns b of matrix B = [bi b2 •…bco ] and thus can be solved in a parallel
manner by independently applying a projected subgradient algorithm (referred to as PSGM) from C0
different random initializations. Moreover, we observe that with random initialization we can get
vectors sufficiently spread on the sphere that lead PSGM (initialized with those vectors) to return
normal vectors of S . These are all linearly independent when C0 ≤ C and thus can span S⊥ when
C0 = C. If C0 > C then PSGM will return C0 - C redundant vectors that will still lie in S⊥ yet they will
be linearly dependent (see Figure 1). That being said, we show that this simple strategy permits us to
robustly recover the true subspace even without knowledge of the true codimension C.
As is detailed in Sections 3 and 4, this remarkable behavior of PSGM originates from the implicit bias
that is induced in the optimization problem due to a) the relaxation of orthogonality constraints in (3)
and b) the random initialization scheme that is adopted. Our specific contributions are as follows:
2
Published as a conference paper at ICLR 2022
Figure 1: Graphical illustration of the recovered normal vectors of S by (left) the proposed DPCP-
PSGM approach and (right) methods that use spectral initialization and impose orthogonality con-
straints . Initial vectors b01 , b20 , b30 are randomly initialized and are non-orthogonal in (left) and spec-
trally initialized (hence orthogonal) in (right). Note that in (left) rank(B*) (where B* = [bɪ,与,bɜD
equals to the true codimension C = 2 of S and SPan(B *) ≡ S⊥ while in (right) B * is orthogonal
hence rank(B*) = 3 with b2* ∈ S.
1.	First, we study a continuous version of (3) where the inliers and outliers are drawn from continuous
measures. We show that this induces a benign landscape on the DPCP objective, which can be
analyzed more easily. Specifically, we prove that the DPCP problem in (3) can be solved via
a projected subgradient algorithm that implicitly biases solutions towards low-rank matrices
B ∈ RD×c0 whose columns are the projections of the randomly initialized columns of B0 onto
S⊥. As a result, B almost surely spans S⊥ as long as it is randomly initialized with c0 ≥ c.
2.	Second, we analyze the discrete version which is more challenging, yet of more practical interest,
showing that iterates of DPCP-PSGM converge to a scaled and perturbed version of the initial
matrix B0 . This compelling feature of DPCP-PSGM allows to derive a sufficient condition and a
probabilistic bound guaranteeing when the matrix B ∈ RD X c spans S⊥.
3.	We provide empirical results both on simulated and a real datasets, corroborating our theory and
showing the robustness of our approach even without knowledge of the true subspace codimension.
2	Related work
Subspace Recovery. Learning underlying low-dimensional subspace representations of data has been
a central topic of interest in machine learning research. Principal Component Analysis (PCA) has
been the most celebrated method of this kind and is based on the minimization of the perpendicular
distances of the data points from the estimates linear subspace, Jolliffe & Cadima (2016). Albeit, it is
originally formulated as nonconvex optimization problem, PCA can be easily solved in closed form
using a singular value decomposition (SVD) operation (see e.g. Vidal et al. (2016)). Despite its great
success, PCA is prone to failure when handling datasets that contain outliers i.e., data points whose
deviation from the inliers’ subspace is “large” in the `2 norm sense.
Robust Subspace Recovery (RSR). To remedy this weakness of PCA robust subspace recovery
(RSR) methods attempt to identify the outliers in the dataset an recover the true underlying low-
dimensional subspace of the inliers Lerman & Maunu (2018); Maunu et al. (2019). A classical
approach to this problem is RANSAC Fischler & Bolles (1981), which is given a time budget and
randomly chooses per iteration d points and then fits a d-dimensional subspace to those points and
checks how the proposed subspace fits the remaining data points. RANSAC then outputs the subspace
that agrees with the largest number of points. However, RANSAC’s reliance on randomly sampling
points to propose subspaces can be highly inefficient when the number of outliers is high (as well
as the fact that RANSAC also needs knowledge of the true subspace dimension, d). The need to
tackle inherent shortcomings of RANSAC pertaining to computational complexity issues inspired
alternative convex formulations of RSR, Xu et al. (2012); You et al. (2017b); Rahmani & Atia (2017);
Zhang & Lerman (2014). In Xu et al. (2012) the authors decompose the data matrix as a sum of
a low-rank and a column-sparse component. However, theoretical guarantees obtained for convex
3
Published as a conference paper at ICLR 2022
formulations only hold for subspaces of relatively low-dimensional subspaces i.e., for d D where
d and D denote the subspace and the ambient dimension, respectively. To the best of our knowledge,
existing RSR algorithms rely heavily on one of two key assumptions. 1) The subspace is very
low-dimensional relative to the ambient dimension (d D) or 2) The subspace dimension is a priori
known. Undoubtedly, the second hypothesis is rather strong in real-world applications, and many
applications also do not satisfy the first assumption. Moreover, heuristic strategies for selecting the
dimension of the subspace are hard to be applied in the RSR setting since they incur computationally
prohibitive procedures, Lerman & Maunu (2018).
Relation to Orthogonal Dictionary Learning (ODL). Note that objective functions in the form
of (3) show up beyond RSR problems i.e., in orthogonal dictionary learning (ODL), sparse blind
deconvolution, etc., Qu et al. (2020). Specifically, based on a similar formulation the authors in Bai
et al. (2019) proved that c0 = O(c log c) independent random initial vectors suffice in order to recover
with high probability a dictionary of size D × c with high accuracy. In this paper we aim to recover
a basis of the orthogonal complement of a subspace of unknown dimension instead of accurately
estimating a dictionary hence our goal differs from that in Bai et al. (2019).
Implicit bias in Robust Recovery Problems. The notions of implicit bias and implicit regularization
have been used interchangeably in the nonconvex optimization literature for describing the tendency
of optimization algorithms to converge to global minima of minimal complexity with favorable
generalization properties in overparameterized models, Gunasekar et al. (2018). In the context
of robust recovery, the authors in You et al. (2020) showed that Robust PCA can be suitably re-
parametrized in such a way to favor low-rank and sparse solutions without using any explicit
regularization. In this work, we use the term implicit bias for describing the convergence of DPCP-
PSGM to low-rank solutions, which are not necessarily global minimizers, that span the orthogonal
complement of the subspace when a) orthogonality constraints in DPCP formulation are relaxed b)
DPCP is overparameterized i.e., c0 ≥ c and c) PSGM randomly initialized.
3	Dual Principal Component Pursuit and the Projected
Subgradient Method
We re-write the DPCP formulation given in (1) as
min	∣∣X>bkι = ∣∣X>bkι + kO>bkι s.t. ∣∣b∣∣2 = 1	(4)
B∈RD×c0
In Zhu et al. (2018), the authors proposed a projected subgradient descent algorithm for addressing
(4) that consists of a subgradient step followed by a projection onto the sphere i.e.,
bk+1 =	bk	-	μk	(XSgn(X>bk)	+	OSgn(O>bk))	and Bk+1 = Psd- (bk+1),	(5)
where μk is the -adaptively updated per iteration- step size and bk is the unit '2 norm vector
corresponding to the kth iteration.
The convergence properties of the projected subgradient algorithm described above depend on specific
quantities denoted as cX,min and cX,max that reflect the geometry of the problem and are defined as
cχ,min = NN minb∈sd-ι∩s ∣∣X>bkι and CX,max = N maXb∈sD-ι∩s ∣∣X>b∣∣ι. Note that the more
well distributed the inliers are in the subspace S the higher the value of the quantity cX,min (called
as permeance statistic which first appeared in Lerman et al. (2015)) as it becomes harder to find a
vector b in the subspace S that is orthogonal to the inliers. Moreover, cX,min and cX,max converge
to the same value as N → ∞ provided the inliers are uniformly distributed in the subspace i.e.,
cX,min → cd, cX,max → cd, where cd is given as the average height of the unit hemisphere on Rd,
2, if d is even,
π	where k!!
1, if d is odd
Similarly to cX,min, cX,max, we will also be interested in quantities cO,min, cO,max which indicate
how well-distributed the outliers are in the ambient space. These quantities are defined as cO,min =
minb∈sD-ι M∣∣O>b∣∣ι and co,max = maXb∈sD-ι M∣∣O>b∣∣ι∙ co,max can be viewed as the dual
permeance statistic and is bounded away from small values while its difference from cO,min tends to
kk(k 一 2)(k — 4)…4 ∙ 2, k is even,
[k(k - 2)(k - 4) ∙∙∙ 3 ∙ 1, k is odd ⑹
_ (d - 2)!!∫
Cd := (d - 1)!! ɪ
4
Published as a conference paper at ICLR 2022
zero as M → ∞. Further, if the outliers are uniformly distributed on the sphere, then cO,max → cD
and cO,min → cD where cD is defined as in (6), (Zhu et al., 2018).
Finally, We also define the quantities no = M maXb∈sD-ι ∣∣(I - bb>)OSgn(O>b)k2 and ηχ =
M maXb∈sD-ι ∣∣(Ps - bb>)XSgn(X>b)∣∣2. As M → ∞ and assuming outliers in O are Well-
distributed We get OSgn(O>b) → cDb thus ηO → 0 (Tsakiris & Vidal, 2018). LikeWise, ηX → 0
as N → ∞ provided that inliers are uniformly distributed in the d-dimensional subspace. The
folloWing theorem (see full version in Appendix) provides convergence guarantees of the projected
subgradient method that Was proposed in Zhu et al. (2018) for addressing problem (1).
Theorem 1 (Informal Theorem 3 of Zhu et al. (2018)) Let {bk } the sequence generated by the
projected subgradient algorithm in Zhu et al. (2018), with initialization b° such that
θ0
< arctan (NNX,Mno)
and N cX ,mi ≥ NηX +MηO
(7)
where θo denotes the principal angle of b0 from S⊥. Ifthe step size μk is updated according to a
piecewise geometrically diminishing rule given as
k _ μ μ0,	k<Ko
μ =[ μ0βb(k-K0"K*c + 1,	k ≥ Ko
(8)
where β < 1, [∙] is the floor function, then the iterates bk converge to a normal vector of S.
4	Dual Principal Component Pursuit in subspaces of unknown
CODIMENSION
Current theoretical results provide guarantees for recovering the true inlier subspace, When the pro-
posed algorithms knoW a priori of the subspace codimension c, Which is a rather strong requirement
and is far from being true in real Word applications. Here We describe our proposed approach, Which
consists of removing the orthogonality constraint on B , along With a theoretical analysis that gives
guarantees of recovering the true underlying subspace even When the true codimension cis unknoWn.
First We analyze a continuous version of DPCP, Which arises When the number of inliers and outliers
are distributed according to continuous measures and their number tends to ∞. The continuous
DPCP incurs an optimization problem With a benign landscape that alloWs us to better illustrate the
favorable properties of DPCP-PSGM When it comes to the convergence of its iterates. Then We
extend the results to the discrete case that deals With a finite number of inliers and outliers yielding a
more challenging optimization landscape.
4.1	PSGM’s iterates convergence in the continuous version of DPCP
The folloWing lemma provides the continuous version of the discrete objective function given in (3).
Lemma 2 In the continuous case, the discrete DPCP problem given in (3) is reformulated as,
c0	c0
min 0 E (pEμSD-1 [fbi] + (I- P)EμSD-ι∩s [fbi]) = E kbik2 (PcD + (I- p)cdcos(φi))
B∈RD×c0
i=1	i=1
s	.t. ∣bi∣2 = 1, i = 1, 2, . . . , c0
(9)
where fb : SD-1 → R, fb(z) = |z>b|, φi is the principal angle of bi from the inliers subspace S
and P is the probability of occurrence of an outlier.
Note that μsD-ι, μsD-ι∩s are the continuous measures associated with the outliers and inliers,
respectively. Evidently, (9) attains its global minimum for vectors bi s that are orthogonal to the
inliers’ subspace. Based on (3) and due to Lemma 2, We can noW minimize the objective function
of the “continuous version” of DPCP by employing a projected subgradient methods (PSGM) that
5
Published as a conference paper at ICLR 2022
performs the following steps per iteration 1
bk+1 =	bk- μk (PcD bk	+ (1- p)cdSk)	and	bk+1	=	Ps⊥ (b"1),i = 1,2,...,c0	(10)
Lemma 3 A projected subgradient algorithm consisting of the steps described in (10) using a piece-
wise geometrically diminishing step size rule (see (8) in Theorem 1) will almost surely asymptotically
converge to a matrix B* ∈ RD×c0 whose columns b*, i = 1, 2,...,c0 will be normal vectors of
the inliers’ subspace when randomly initialized with vectors bi0 ∈ SD-1, i = 1, 2, . . . , c0 uniformly
distributed over the sphere SD-1.
Lemma 3 allows us to claim that we can always recover c0 ≥ c normal vectors to the inliers’ subspace
using a PSGM algorithm consisting of steps given in (10). However, this does not tell the whole story
yet, since our ultimate objective is to recover a matrix B that spans S⊥. Thus, it remains to show
that the rank of B is equal to the true and unknown codimension of the inliers, subspace c. NeXt we
prove that by initializing with a B0 such that rank(B0) = c0 (i.e., B0 is initialized to be full-rank),
we can guarantee that we can solve the continuous version of DPCP using PSGM and converge to
a B such that rank(B) = c thus getting span(B) ≡ S⊥ (along with recovering the true subspace
dimension). By projecting the PSGM iterates given in (10) onto S⊥ we have,
PS⊥(bk+1 ) = (1-μkPcD)Ps⊥ (Bk) and Ps⊥ (b"1) = Ps⊥(PSD-I (b”1))	(11)
We hence observe that the projections of successive iterates of PSGM are scaled versions of the
corresponding projections of the previous iterates. We can now state Lemma 4.
Lemma 4 The PSGM iterates 讨，i = 1,2,..., c0, k = 1, 2, . . . given in (10), when randomly
initialized with b0s, i = 1,2,...,c0 that are independently drawn from a spherical distribution
with unit `2 norm converge almost surely to c0 normal vectors of the inliers subspace S denoted as
b*, i = 1, 2,...,c0 that are given by b* = kPs ⊥(bi)k2, i = 1,2,...,c0∙
Lemma 4 shows that the initialization of PSGM plays a pivotal role since it determines the direction
0
of the recovered normal vectors {bi*}ic=1. Lemmas 3 and 4 pave the way for Theorem 5.
Theorem 5 Let B0 ∈ RD×c0 where C ≥ c with c denoting the true codimension of the inliers
subspace S, consisting of unit '2 norm column vectors b0 ∈ SD-1,i = 1, 2,...,c0 that are indepen-
dently drawn from uniform distribution over the sphere SD-1. A PSGM algorithm initialized with
B0 will almost surely converge to a matrix B* such that span(B*) ≡ S⊥.
From Theorem 5 we observe that in the benign scenario where inliers and outliers are distributed under
continuous measures, we can recover the correct orthogonal complement of the inlier’s subspace
even when we are oblivious to its true codimension. Remarkably, this is achieved by exploiting the
implicit bias induced by multiple random initializations of the PSGM algorithm for solving the DPCP
formulation given in (3), which is free of orthogonality constraints.
4.2 PSGM’s iterates convergence in the discrete version of DPCP
From this analysis of the continuous version of DPCP we now extend to the the discrete version,
which is of more practical relevance for finite data, yet also presents more challenges. To begin, we
reformulate the DPCP objective as follows
c0	c0	c0	c0
X kX >bi kι = X kX >bikι + kO>bikι = M X b>o b + N X b> X b	(12)
where xbi and obi are called as average inliers and average outliers terms, defined as xbi =
N PlN=I Sgn⑤Xj)xj and obi =焉 PM=I Sgn(b>oj)oj.
In Algorithm 1, we give the projected subgradient method (DPCP-PSGM) applied on the DPCP
problem given in (3).
1Note that ∂∣∣bk2 = ^^ for b = 0 and ∣∣bi∣∣2cos(φi) = b>Si where Si = |费密)心.
6
Published as a conference paper at ICLR 2022
Algorithm 1: DPCP-PSGM algorithm for solving (3)
Result: B = [bk, bk,..., bCc]]
Initialize: Randomly sample b0, b0,..., b00 from a uniform distribution on SDT
for k = 1, 2, . . . do
for i = 1, 2, . . . , c0 do
Update the step-size according to a specific rule;
bkk+1 =讨-μkk(Mok + NXk );
bk+1= Psd-1 (bk+1)J
end
end
Note that the average outliers and inliers terms are discrete versions of the corresponding continuous
average terms CDbi and Cdsi where Si = PS(bi), respectively, Tsakiris & Vidal (2018). We now
express the sub-gradient step of Algorithm 1 as
bk+1 =铸-μk(M(CDb + eO) + N(CDsk + eX))	(13)
where the quantities eO = o bk - CD bk - and eX = X bk - CDsk account for the error between the
ii
continuous and discrete versions of the average outliers and the average inliers terms, respectively.
Following a similar path as in the continuous case we next project the iterates of (13) onto S⊥,
PS⊥ (bk+1) = Ps⊥ (铸)-μk (MPs⊥ (CDbk + eO) + NjPs⊥Wħ+eXT
=(1 - μkMCD)Ps⊥(Bk)- μkMPs⊥(eO)
(14)
Remark. Eq. (14) reveals that DPCP-PSGM, applied on the discrete problem, gives rise to updates
whose projections to S⊥ are scaled and perturbed versions of the previous estimates. The magnitude
of perturbation depends on the discrepancy between the continuous and the discrete problem.
Recall that Ps⊥ (Bk) = PS",), and we can rewrite the update of the 2nd iteration of DPCP-PSGM,
Ps⊥ (b-) = (1 -μ1MCD)((1 - μ? MCD )Ps⊥ (b0) - μ0M Ps⊥ (eO0)) - 〃1μ Ps⊥ (e O* 1)
kbi k2
—(1 - "McD)(1 - M0McD) p (b0) _ (1 - "McD) 0Mp ( i,0) _ 1 Mp ( i,1)
一	IlbIIl2∣∣b0∣∣2	P S⊥ (bi)	kb1 k2	μi M P S⊥ (eO ) μi M P S⊥ (eO )
i i	i	(15)
where we have assumed that kbi0k2 = 1. By repeatedly applying the same steps, we can reach to the
following recursive expression for PS⊥ (biK),
K 小 K (KTI	(I - μk MCD) ! P +0、 JX-I	( KYY	(1 - μj MCD)] kʌʃp	(Pig
ps⊥ M=(口	产 ⊥(bi)-S	(j=k+ι	；	μiM ps⊥ (e o)
(16)
Where for j > K - 1 We Set QKfc+ι (I-记CD) = L
By dividing (16) with Qk=QI。-优'^) and by projecting onto the sphere SDT we get
k ik2
PSD-1(PS⊥(biK ))= PSD-1(PS⊥(bi0)-PS⊥(δiK ))	(17)
where δiis defined as δK = PKo1 (Qk=0 (1-%MCD)) μkM e Ok.
Assumption 1. We assume that the principal angles θ0i for all bi0s satisfy the inequality θ0i <
arctan (NnXXMnnO ) ∀i,i = 1，2，...，C∙
7
Published as a conference paper at ICLR 2022
Assumption 1 essentially assumes that the sufficient condition given in eq. (7) required by PSGM
algorithm for converging to a normal vector is satisfied which is the same condition for success in
Zhu et al. (2018). Under Assumption 1 we can invoke the convergence properties of PSGM given
in Theorem 1 and get as K → ∞, Psd-i (Ps⊥ (bK)) → b* ∈ S⊥ ∩ SD-1. That being said, We
denote b* = Psd-i (Ps⊥ (b0) - Ps⊥ 位))，where δi = limκ→∞ δK2. Following the same steps
as in Section 4.1 and by defining matrices B* = [b；, b2,..., b", B0 = [b0, b§,..., b0o], ∆∆ =
[δι, δ2,..., δc0] we can express the matrix B* as B* = Psd-i (Ps⊥ (b0 - ∆)) where B* will
now consist of normal vectors of the inliers’ subspace. In order to guarantee that span(B*) ≡ S⊥ it
thus suffices to ensure that rank (B*) = c. Here we show that a sufficient condition for this to hold
is that the matrix A = B0 — ∆ is full-rank.
Lemma 6 If Oc0(B0) > ∣∣∆ ∣∣2 then matrix A = B0 — ∆∆ is full-rank.
From Lemma 6 we can see that the success of DPCP-PSGM hinges on how well-conditioned the
matrix B0 is. Specifically, it says that if a lower-bound on the smallest singular is satisfied then
DPCP-PSGM is guaranteed to converge to the correct complement of the inlier without knowledge of
the correct codimension c. From this, we can prove the following Theorem.
Theorem 7 Let B0 ∈ RD×c0 with columns randomly sampled from a unit `2 norm spherical
distribution where c0 ≥ c with c denoting the true codimension of the inliers subspace S that satisfies
Assumption 1. If
> √c0κ(ηo + CO
,max -cd )
(18)
“小〃 一 iV.	M40	"-(I+，0(N("X+cχ,max)+M (ηO +cO
,max)))	1/K
where K = maxi §“久：()and r = ʌ--------------ι-μ0McD------------β1/ then With
probability at least 1-2 exp(-2C2) (where C1, C2 are absolute constants), Algorithm 1 with a piece-
wise geometrically diminishing step size rule will converge to a matrix B* such thatspan(B*) ≡ S⊥.
Note that quantities β, K* , K0 are used in the step-size update rule that is used as defined in (8) (See
also full version of Theorem 1 in Appendix)). Theorem 7 shows that we can randomly initialize DPCP-
PSGM, with a matrix B0 whose number of columns c0 is an overestimate of the true codimension
of the inliers’ subspace and with columns sampled independently by a uniform distribution over
the unit sphere and recover a matrix that will span the orthogonal complement of S . The probability
of success depends on the geometry of the problem since condition (18) is trivially satisfied (RHS
of (18) tends to 0 since ηO → 0 and cO,max →cd) in the continuous case which incurs a benign
geometry. Moreover, the a less benign geometry would increase the value of (ηO+	cO,max -cd)
thus requiring a smaller initial codimension c0 that would lead to larger values the LHS of (18).
5 Numerical Simulations
In this section we demonstrate the effectiveness of the proposed DPCP formulation and the derived
DPCP-PSGM algorithm in recovering orthogonal complements of subspace of unknown codimension.
We compare the proposed algorithm with previously developed methods i.e., DPCP-IRLS Tsakiris
& Vidal (2018) and the Riemannian Subgradient Method (RSGM) Zhu et al. (2019). Recall that
both DPCP-IRLS and RSGM address DPCP problem by enforcing orthogonality constraints, and
thus both algorithms are quite sensitive if the true codimension of S is not known a priori. Further,
they are both initialized using of spectral initialization i.e., B0 ∈ RD×c0 which contains the first C
>
eigenvectors of matrix XX as its columns, as proposed in Tsakiris & Vidal (2018).
Robustness to outliers in the unknown codimension regime. In this experiment we set the dimen-
sion of the ambient space to D = 200. We randomly generate N inliers uniformly distributed with
unit `2 norm in a d = 195 dimensional subspace (hence for its codimension we have c = D - d = 5).
Following a similar process we generate M outliers that live in the ambient space and are sampled
2For the sake of brevity we assume that the step size has been selected such that existence of the limit is
guaranteed. We refer the reader to the proof of Lemma 8 for further details.
8
Published as a conference paper at ICLR 2022
Figure 2: Distances of the recovered B from the true orthogonal complements S⊥ as recovered by
the proposed DPCP-PSGM algorithm provided an overestimated of the true c i.e., c0 = 10 (left),
RSGM provided the true c (middle) and RSGM provided c0 = 10 (right). Darker colors reflect higher
values of distances while lighter colors indicate successful recoveries of B .
from a uniform distribution over the unit sphere. Fig. 2, illustrates the distances (see Appendix)
of the recovered matrix B as obtained by the proposed DPCP-PSGM algorithm initialized with an
overestimate c0 = 10 of the true codimension c codimension and two versions of RSGM i.e., RSGM
when it is given as input true c = 5 and RSGM when being incognizant of c and hence it initialized
with a c0 = 10 of cAs is shown in Fig. 2 (right), RSGM fails to recover the correct orthogonal
complement of S when it is provided with an overestimate of the true c which is attributed to spectral
initialization and the imposed orthogonality constraints. On the contrary, DPCP-PSGM displays a
remarkably robust behavior (Fig. 2(middle)) even without knowing the true value of c, performing
similarly to RSGM when the latter knows beforehand the correct codimension (Fig. 2(left)).
Recovery of the true codimension. Here we test DPCP-PSGM on the recovery of the true codimen-
sion c of the inliers’ subspace S. Again, we set D = 200 and generate N = 1500 inliers as before.
We vary the true codimension of S from c = 10 to 20 and consider two different outlier’s ratios
r, defined as r = MMN, namely r = 0.6 and r = 0.7. In both cases, DPCP-PSGM is initialized
with the same overestimate of c i.e., c0 = 30. In Fig. 3 we report the estimated codimensions
obtained by DPCP-PSGM for 10 independent trials of the experiments. It can be observed that
DPCP-PSGM achieves 100% for all different codimensions for r = 0.6. Moreover, it shows a
remarkable performance in estimating the correct c’s even in the more challenging case corresponding
to outliers’ ratios equal to 0.7. with the estimated codimensions being close to the true values even
in the cases that it fails to exactly compute c. The results corroborate the theory showing that the
DPCP-PSGM with random initialization biases the solutions of B towards matrices with rank c.
6 Conclusions
We proposed a simple frame-
work which allows us to per-
form robust subspace recovery
without requiring a priori knowl-
edge of the subspace codimen-
sion. This is based on Dual Prin-
cipal Component Pursuit (DPCP)
and thus is amenable to han-
dling subspaces of high relative
dimensions. We observed that
a projected subgradient method
Figure 3: Estimated by DPCP-PSGM codimensions for two dif-
ferent outliers, ratios r = MMN (a) r = 0.6 (left) and (b) r = 0.7
(right)
(PSGM) induces implicit bias and converges to a matrix that spans a basis of the orthogonal comple-
ment of the inliers subspace even as long as a) we overestimate it codimension, b) lift orthogonality
constraints enforced in previous DPCP formulations and c) use random initialization. Empirical
results that corroborate the developed theory and showcase the merits of our approach.
9
Published as a conference paper at ICLR 2022
Ethics Statement This work focuses on theoretical aspects of robust subspace recovery problem
which is a well-established topic in machine learning research. The research conducted in the
framework of this work raises no ethical issues or any violations vis-a-vis the ICLR Code of Ethics.
Acknowledgments
We would like to thank Christian KUmmerle for helpful discussions on the probabilistic theorem
that is used in Theorem 7. This work is partially supported by by the European Union under the
Horizon 2020 Marie-SklodoWSka- Curie Global Fellowship program: HyPPOCRATES— H2020-
MSCA-IF-2018, Grant Agreement Number: 844290, and the NSF Grants 1704458, 2031985 and
1934979.
References
Yu Bai, Qijia Jiang, and Ju Sun. Subgradient descent learns orthogonal dictionaries. In International
Conference on Learning Representations, 2019. URL https://openreview.net/forum?
id=HklSf3CqKm.
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
Journal ofthe ACM (JACM), 58(3):1-37, 2011.
Tianyu Ding, Zhihui Zhu, Rene Vidal, and Daniel P Robinson. Dual principal component pursuit
for robust subspace learning: Theory and algorithms for a holistic approach. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 2739-2748. PMLR, 18-24 Jul
2021. URL https://proceedings.mlr.press/v139/ding21b.html.
Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting
with applications to image analysis and automated cartography. Communications of the ACM, 24
(6):381-395, 1981.
Paris V. Giampouras, Athanasios A. Rontogiannis, and Konstantinos D. Koutroumbas. Alternat-
ing iteratively reweighted least squares minimization for low-rank matrix factorization. IEEE
Transactions on Signal Processing, 67(2):490-503, 2019. doi: 10.1109/TSP.2018.2883921.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias
in terms of optimization geometry. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pp. 1832-1841. PMLR, 10-15 Jul 2018. URL https://proceedings.
mlr.press/v80/gunasekar18a.html.
Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments.
Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering
Sciences, 374(2065):20150202, 2016.
Gilad Lerman and Tyler Maunu. An overview of robust subspace recovery. Proceedings of the IEEE,
106(8):1380-1410, 2018.
Gilad Lerman, Michael B McCoy, Joel A Tropp, and Teng Zhang. Robust computation of linear
models by convex relaxation. Foundations of Computational Mathematics, 15(2):363-410, 2015.
Tyler Maunu, Teng Zhang, and Gilad Lerman. A well-tempered landscape for non-convex robust
subspace recovery. Journal of Machine Learning Research, 20(37), 2019.
Michael McCoy and Joel A Tropp. Two proposals for robust PCA using semidefinite programming.
Electronic Journal of Statistics, 5:1123-1160, 2011.
Qing Qu, Zhihui Zhu, Xiao Li, Manolis C Tsakiris, John Wright, and Rene Vidal. Finding the sparsest
vectors in a subspace: Theory, algorithms, and applications. arXiv preprint arXiv:2001.06970,
2020.
10
Published as a conference paper at ICLR 2022
Mostafa Rahmani and George Atia. Coherence pursuit: Fast, simple, and robust subspace recovery. In
Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Ma-
chine Learning, volume 70 of Proceedings ofMachine Learning Research, pp. 2864-2873. PMLR,
06-11 Aug 2017. URL https://Proceedings .mlr.press∕v70∕rahmani17a.html.
Manolis C. Tsakiris and Rene Vidal. Dual principal component pursuit. Journal ofMachine Learning
Research, 19(18):1-50, 2018.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Rene Vidal, Yi Ma, and S Shankar Sastry. Generalized principal component analysis, volume 5.
Springer, 2016.
Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit. IEEE
Transactions on Information Theory, 58(5):3047-3064, 2012.
C. You, D. Robinson, and R. Vidal. Provable self-representation based outlier detection in a union
of subspaces. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 4323-4332,
2017a.
Chong You, Daniel P Robinson, and Rene Vidal. Provable self-representation based outlier detection
in a union of subspaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3395-3404, 2017b.
Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant
learning rates for double over-parameterization. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
17733-17744. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/file/cd42c963390a9cd025d007dacfa99351-Paper.pdf.
Teng Zhang and Gilad Lerman. A novel m-estimator for robust PCA. The Journal of Machine
Learning Research, 15(1):749-808, 2014.
Z. Zhu, T. Ding, M. C. Tsakiris, D. P. Robinson, and R. Vidal. A linearly convergent method for
non-smooth non-convex optimization on the grassmannian with applications to robust subspace
and dictionary learning. In Neural Information Processing Systems (NIPS), 2019.
Zhihui Zhu, Yifan Wang, Daniel Robinson, Daniel Naiman, Rene Vidal, and Manolis
Tsakiris. Dual principal component pursuit: Improved analysis and efficient algorithms.
In Advances in Neural Information Processing Systems 2018, volume 31. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
af21d0c97db2e27e13572cbf59eb343d- Paper.pdf.
11
Published as a conference paper at ICLR 2022
A Appendix
Theorem 1 (Theorem 3 of Zhu et al. (2018)) Let {bk} the sequence generated by the projected
subgradient method in Zhu et al. (2018), with initialization bo such that
θ0
< arctan (NNXMO)
(19)
where θ0 denotes the principal angle of b0 from S⊥, and
N cX,min ≥ NηX+MηO	(20)
Let μ := 4maχ{Ncjc	1	Mco-}.	If μ0	≤	μ0	and the step size	μk	is updated according to a
piece-wise geometrically diminishing rule given as
k __ μ μ0,	k < Ko
μ = μ μoβb(k-K0"K*c + 1,	k ≥ Ko
where β < 1, [∙] is the floor function, and Ko,K* ∈ N are chosen such that
Ko ≥ K ♦心
K* ≥ f√2βμ0 (Ncχ - (Nηχ + Mηo)))-1
where,
KO (μ) :=___________________tan(0O)___________________
μ (Ncχ,min - max{1, tan(θo} (Nηχ + Mηo))
then for the angle θk between ^k and S⊥ it holds
max{tan(θo), √2μo}，k < Ko
tan(θ0) ≤ I 岛ebgEK",	k ≥ Ko
(21)
(22)
(23)
A.1 Proof of Lemma 2
Lemma 2 In the continuous case, the discrete DPCP problem given in (3) is reformulated as,
c0	c0
min 0 E (pEμSD-1 [fbi] + (I- P)EμSD-ι∩s [fbi]) = E kbik2 (PCD + (I- p)cdcos(φi))
B∈RD×c0
i=1	i=1
s.t. kbik2 = 1, i = 1, 2, . . . , c0
(24)
where fb : SD-1 → R, fb(z) = |z>b|, φi is the principal angle of bi from the inliers subspace S
and p is the probability of occurrence of an outlier.
Proof We define the discrete measures μχ, μo associated with the inliers and outliers, respectively
as,
1N	1N
μχ(Z) = N∑^δ(Z - Oj), μo(Z) = M∑δ(z - Xj)	(25)
where δ(∙) is the Dirac function. Recall that,
/	g(z)δ(z - zo)dμsD-ι = g(zo)
z∈SD-1
(26)
where g : SD-1 → R and μsD-ι is the uniform measure on SD-1
12
Published as a conference paper at ICLR 2022
The DPCP objective for the discrete version of the problem divided by M + N can be written as,
c0	c0	c0	N	M
M+NXkX>bikι = M+NX(kX>bikι + kO>bikι) = m+nX(X 吗bi| + X|o>bi|
i=1	i=1	i	j=1	j=1
1	c0 ( N r	Mf	∖
=M + N IΣJ *_i lz>bilδ(z - Xj)dμsD-ι + Σ2J *τ lz>biM(Z- oj)dμsD-ι J
1	c0 (	N	M	∖
=M + N (J s°.i lz>b∕∑δ(z - Xj)dμsD-ι+ J sd i |z>bi|Eδ(Z - oj)dμsD-ι I
0
c
X (pEμx [fbi] + (1- P)Eμo [fbi])
i=1
Note that μχ, μο arise by discretizing the continuous uniform measures μsD-ι and μsD-ι∩s
respectively (μsD-ι∩s denotes the uniform measure on SD-I ∩ S) and P is the probability of
occurrence of an outlier i.e., MMN → P as M, N →∞ (1 - P corresponds to the probability of
occurrence of an inlier). That being said, the continuous version of DPCP can be simply stated by
replacing μχ, μο with μsD-ι and μsD-ι∩s in equation 27 as follows,
c0
mBnX (pE"sd-i [fbi] + (1 — p)E"sd-i∩s %])	(27)
i=1
The RHS of (9) immediately shows up by invoking Proposition 4 in Tsakiris & Vidal (2018).
A.2 Proof of Lemma 3
Lemma 3: A projected subgradient algorithm consisting of the steps described in (10) using a piece-
wise geometrically diminishing step size rule (see (8) in Theorem 1) will almost surely asymptotically
converge to a matrix B* ∈ RD×c0 whose columns b>*, i = 1,2,...,c0 will be normal vectors of
the inliers’ subspace when randomly initialized with vectors bi0 ∈ SD-1, i = 1, 2, . . . , c0 uniformly
distributed over the sphere SD-1.
Proof:
The proof can be trivially obtained by noticing a) that the condition for convergence i.e., inequality
(7) of the projected subgradient algorithm given in Theorem 1 becomes θ0 < ∏2 in the continuous
case (since ηχ → 0, no → 0, CX4由 → Cd > 0) and b) the set of unit '2-norm vectors b0s,
i = 1, 2, . . . , c0 sampled independently by a uniform distribution over the sphere and whose principal
angle θ0 is π∏ form the inliers, subspace has measure 0.	■
A.3 Proof of Theorem 5
Lemma 4 The PSGM iterates 讨，i = 1,2,..., C0, k = 1, 2, . . . given in (10), when randomly
initialized with b0s, i = 1,2,...,c0 that are independently drawn from a spherical distribution
with unit `2 norm converge almost surely to C0 normal vectors of the inliers subspace S denoted as
bi* , i = 1, 2, . . . , C0 that are given by
b* = kP⅛⊥(⅞∣∣2,	i = 1,2,...,c0
(28)
13
Published as a conference paper at ICLR 2022
Proof Let Us assume b0 = b0. The iterates of subgradients steps of PSGM can be written in the
following form,
b1 = (I - μ0pcD)b0 - μ0(I -P)Cds
b2 = (I - μ1pcD)b1 - μ1(I -P)Cds
(29)
..
..
..
bK = (I- μK-1pcD)bK-1 - μK-1(1 - P)Cds
By projecting each update of PSGM onto S⊥ and since Ps⊥ (bk) = IlbkkPs⊥ (bk) We have,
Ps⊥ (bk+1) = (1 -ιμkpcD) Ps⊥ (bk)
(30)
We can thus easily derive the following form for PS⊥ (biK),
PS⊥ (bK) = (KYI (I -bμkpcD)) Ps⊥ (b0)	(31)
We know from Theorem 1 and Lemma 3 when DPCP-PSGM is initialized with bi0, i = 1, 2, . . . , C0s
randomly drawn according according to a spherical distribution then it will almost surely converge
as K → ∞ to vectors b*, i = 1,2,...,c0 i.e., bK → b where b； ∈ S⊥. Hence Ps⊥ (bK) → b
as K → ∞. Note that from Theorem 1 we have that μk = 焉 ∀k = {1, 2,..., K} hence
QK-LI (1-μkpcD) = O. From 31 and after projecting on the unit sphere and we thus have
k i k2
^* — PS⊥ (b0)	■
i = kPs⊥ (b0)k2.
Theorem 5 Let B0 ∈ RD×c0 where C ≥ C with C denoting the true codimension of the inliers
subspace S, consisting of unit '2 norm column vectors b0 ∈ SD-1,i = 1, 2,...,c0 that are indepen-
dently drawn from uniform distribution over the sphere SD-1. A PSGM algorithm initialized with
B 0 will almost surely converge to a matrix B ； such that span(B ；) ≡ S⊥.
Proof From Lemma 4 we have that for each initial unit norm vector bi0 which corresponds to the ith
column of B0 will almost surely converge to b； = js⊥ (bi ) . We can thus write B； = Ps⊥ (B0)Γ
kPS⊥ (bi )k2
where γ isa full-rankdiagonal matrix given be γ = diag( 花工的心,kPS,2b0)k2 ,…，花^ )k2)∙
Note that PS⊥ is a linear projection and thus we can write PS⊥ (B0) = BS⊥ BS> where BS⊥ RD×c
is an orthonormal matrix which spans S⊥. Note that the probability of sampling a low-rank matrix
B0 = [b10, b02, . . . , bc00] when columns bi0s are randomly and independently drawn from a spherical
distribution is zero. We thus have B； = Bs⊥ B> B0Γ with rank(B*) = c.	■
Lemma 6 If Oc0(B0) > ∣∆ ∣∣2 then the rank of matrix B0 一 ∆ equals C.
Proof Let A = B0 一 ∆. From singular value perturbation inequalities we have ∣σi(A) 一σi(B0)| ≤
∣∆∣∣2, for i = 1, 2,...,c0. Hence it holds,
-∣∣∆∣∣2 ≤ σi(A) - σi(B0)	(32)
If σc0(B0) > ∣∆∣∣2 then from equation 32 we get
σc0(A) > 0	(33)
hence the matrix B0 — ∆ will be full-rank.	■
14
Published as a conference paper at ICLR 2022
A.4 Proof of Theorem 7
We first give the following Lemmas:
Lemma 7 For the `2 norm of e iO,k for any k = 1, 2, . . . , K and i = 1, 2, . . . , c0 it holds,
keiO,k k2 ≤ ηO + cO,max - cd	(34)
Proof
keOkk2 = kob - cdbk2 = kMMOSgn(O>b) - cdbk2
=k fl — bb>)OSgn(O>b) + M bb>OSgn(O>b) - cdb∣∣2 (35)
≤ k fl - bb>)OSgn(O>b)k2 + (E ∣Q>bkι - cd)kb∣∣2
≤ ηO + cO
,max - cd
where We have used the fact that kb∣∣2 = 1.	■
Lemma 8 Let the step size ofAlgorithm 1 (DPCP-PSGM) μk being Updatedfollowing the piecewise
geometrically diminishing step size rule with
β < (______________1 - μ0McD_______________
V + μ0 (N (ηx + CX,max) + M(ηO +
cO,max))
K
For the spectral norm of∆ it holds k∆k2 ≤	c0κ(ηO +cO,max -cd) where κ = maxi
and ri
(1 + μ0 (N(ηx + cX,maX)+ M(no + co,maX)))
1-μ0McD
β1K
Mμ0
βK0∕K*(1-ri)
Proof We first bound the `2 norm of vectors bij s. We have that ∀i = 1, 2, . . . , c0 andj = 1, 2, . . . , K
it holds
bj+1 = Bj- μj (XSgn(X>^) + OSgn(OTBj))	(36)
We define the quantities
ηχ := max1 -1 ll(Ps - bjbj,T)XSgn(XTbj)k2	(37)
b∈SD-1 N
cχ,max := max IkX Tb j kι	(38)
b∈SD-1 N
kb j∣∣2 = 1 hence
kbj+1k2 ≤ 1+ μjkXSgn(XTbj) + OSgn(OTbjk2
≤ 1+ μj (kXSgn(XTBj)k2 + kOSgn(OTbj)k2)
≤ 1+ μj(k(Ps - bjR,t)XSgn(XTBj)k2 + k(bjbj，T)XSgn(XTBj)k2
+ k (1 - ^jbj,>) OSgn(OTbj)k2 + kbjbj,TOSgn(OTbj)k2)
≤ 1 + μj (N(ηx + CX,max) + M(ηO +
cO,max))
Due to equation 39 and since μj follows a non-increasing path as j → k, the scalar
Qk=0 (Jjk)is bounded above as follows,
YkΓ	kbjk2	≤ ( (1 + μ0 (N(nX + CX,max) + M(nO + cO,maX))))
j=0 (1 - μjMcd) ^ ∖	1 - μ0McD	J
term
(39)
15
Published as a conference paper at ICLR 2022
We now focus on the geometrically diminishing step size rule given in equation 8. We have μk =
μ0βbk-κ√κ*c + 1 < M0e(k-K0)/K* for k ≥ K and μ0β^-K0"K* > μ0 for k < K0. Hence we
can get the following upper bound
Kl→∞XY o⅛ μk m<
M iim KX ( (1 + μ0 (N (nX + CX ,maX) + M (nO + cO,maX))) !	Oβ(k-K0)∕κ*
k→∞ k=o ∖	1 - μ0McD	μ μi
—Iim	X-	M 1 l 0	( (1+	μ0 (N(nX +	CX,max) + M(nO +	cO,maX))) n1/K*\
≡ K→∞	L	M JK0/K*μi	1	1 - μoMcD	β
k=0	i
The series S = PKOI ((1+μ0(N(nX+cχmaMM(ηo+co,maX))) e1/K*) " is geometric and if
β < (________________1 - μ0McD_______________
V(1 + μ0 (N (ηχ + CX,max) + M(ηO +
cO,maX)))
K*
(40)
it converges as K →∞ to 士 where r = (I+μ0(N (nx+”立：(no+cO，maX))) β1∕K*.
Let US now bound the '2 norms of the columns of ∆. From Lemma 7 we have ∣∣e Ok ∣∣2 ≤ no +
cO,maX - Cd. We can easily thus derive that ∣δi∣2 ≤ κ(ηO + cO,maX - Cd). For the spectral norm
of ∆ we thus have
00
∣∣δ∣∣ _	sud∣δ X ∣2	—	SUD∣	PC=I Mi ∣2 <	SUDPc=I Mik2^
"△k2 =	X=P ^RT	=	X=P ∣χ k2	≤	X=P 一⅛一
≤ max kδi∣2sup∣x∣1 ≤ κ(ηo + cO,max - cd)√c"
i	X6=0 ∣X∣2
(41)
Where the last inequality arises since ∣X ∣1 ≤ √C0kX k2.
We then give the Theorem.
Theorem 9 (Theorem 5.58 of Vershynin (2010)) Let B be a D X d matrix (D ≥ d) whose columns
bi are independent sub-gaussian isotropic random vectors in RD with kb∕∣2 = DD almost surely.
Then for every t ≥ 0 the inequality
√D — C√d — t ≤ σmin(B) ≤ σmaχ(B) ≤ √D + C√d + t	(42)
with probability at least 1-2 exP(-Ct2), where C = Ck0, C = C0K > 0 depend only on the subgaussian
norm K = maxj ∣bi∣ψ2 of the columns.
The proof of Theorem 7 follows next.
Theorem 7 Let B0 ∈ RD×c0 with columns randomly sampled from a unit `2 norm spherical
distribution where C0 ≥ C with C denoting the true codimension of the inliers subspace S that satisfies
Assumption 1. If
> √C0κ(no + cO,maX - Cd)
(43)
where K = maxi 万勺/署1_%) and r = (1+μi(N(ηX+cl-OMM(ηO + co,maX))) β1∕K* then with
probability at least 1 - 2 exP(-2C2) (where C1, C2 are absolute constants), Algorithm 1 with a
geometrically diminishing SteP size rule will COnverge to a matrix B such that SPan(B*) ≡ S⊥.
16
Published as a conference paper at ICLR 2022
Table 1: Results on Washinghton DC AVIRIS hyperspectral image
Methods	F1-scores
r=80%^^r = 90%
DPCP-PSGM (unknown c)	0.994	0.993
RSGM (unknown c)	0	0
DPCP-IRLS (unknown c)	0	0
RSGM (c = 5)	0.999	0.993
DPCP-IRLS c = 5	1	0.995
Proof By Assumption 1 we have that all columns of B0 will satisfy the sufficient condition for
convergence of DPCP-PSGM (Algorithm 1) to a normal vector of S. From Lemma 6 and we use the
inequality q。，(Bo) > ∣∣∆∣∣2 which ensures full-rankness of B*, which is the key ingredient in order
to prove that SPan(B*) = S⊥. We can then Use Theorem 9 for matrix B°. Note that columns of Bo
are drawn independently and are uniformly distributed on the unit sphere. Hence, columns of B0 are
sampled by subgaussian distribution and the LHS of the inequality of the theorem appears if we scale
with √√d so that to create unit-norm columns and use LHS of the inequality of Theorem 7. The RHS
of the inequality is due to the upper bound of ∣∣∆∣∣2 as stated in Lemma 8. The absolute constants
C1, C2 depend only the subgaussian norm of the uniform distribution (they is no dependency on the
dimensions of the problem).
B Experimental details and additional material
All experiments were conducted on a MacBook Pro 2.6GhZ 6-Core Intel Core i7, memory 16GB 2667
Mhz DDR using Matlab2019B. For computational purposes and in order to avoid fine-tuning of the
piecewise geometrically diminishing (PGD) step size, the modified backtracking line-search (MBLS)
step-size rule was adopted for DPCP-PSGM as proposed in Zhu et al. (2018). We define the distance
between two subspaces spanned by matrices B and A as dist(B, A) = minQ∈O(D,c) ∣B - AQ∣F
where O(D, c) denotes the Stiefel manifold of orthogonal matrices of rank c. Note that dist(B, A) =
0 ^⇒ SPan(B) ≡ SPan(A) (SeeZhUetal.(2019)).
B.1 Outliers Pursuit in Washington DC Mall AVIRIS HSI
Hyperspectral images (HSIs) provide rich spectral information as compared to RGB images capturing
a wide range of the electromagnetic spectrum. Washington DC Mall AVIRIS HSI contains contiguous
spectral bands captured at 0.4 to 2.4μm region of visible and infrared spectrum, Giampouras et al.
(2019). In this experiments we randomly choose 10 out of its 210 spectral bands. Due to high
coherence in the both the spectral and the spatial domain, pixels of HSIs admit representations in
low-dimensional subspaces. Here, we use a 100×100 segment of the hyperspectral image selecting
randomly 10 out of its D = 210 spectral bands. We form a matrix X of size 10 × 10000 whose
columns correspond to different points in the 10-dimensional ambient space. Then we corrupt
columns of X by replacing them with outliers that are generated uniformly at random with unit `2
norm for two different outliers’ ratios i.e., r = 0.8 and r = 0.9. In the corrupted X, the remaining
clear pixels are considered as the inliers. Table 1 displays the F1 scores obtained by DPCP-PSGM,
RSGM and DPCP-IRLS algorithm. The latter two algorithms are evaluated in two scenarios: a)
codimension is initialized c0 = 5 and b) c0 = 10. Given the singular value distribution of the initial
image, we infer that the dimension d of the inliers’ subspace is less or equal than 5.
Hence, c0 = 5 (recall c = D - d) is close to the true codimension value while c0 = 10 is an
overestimate thereof. From Table 1, we can see that the proposed DPCP-PSGM succeeds in both
outliers’ ratios regardless its unawareness of the true codimension value. On the other hand, DPCP-
IRLS and RSGM fail when initialized with c = 10 and this is attributed to the restrictions induced
17
Published as a conference paper at ICLR 2022
(a)	(b)
(c)
(d)	(e)
Figure 4: (a) False RGB color image of the clean version of Washington Mall AVIRIS HSI, (b)
corrupted by outliers depicted with red and inliers correpsonding the non-red pixels (c) annotated
outliers as recoverd by the proposed DPCP-PSGM method initialized with c0 = 10 (d) RSGM with
c0 = 10, (e) RSGM with c0 = 5 and (f) DPCP-IRLS with c0 = 5.
(f)
due to the orthogonality constraints they both impose. In Fig. 4 we provide annotated versions of the
clean HSI, its corrupted by outliers version for outliers’ ratio r = 90%, and the annotated outliers as
recovered by the proposed DPCP-PSGM, RSGM, RSGM with c0 = 5 and DPCP-IRLS with c0 = 5.
18