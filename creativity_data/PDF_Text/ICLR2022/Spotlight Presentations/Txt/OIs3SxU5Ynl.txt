Published as a conference paper at ICLR 2022
VAE Approximation Error:
ELBO and Exponential Families
Alexander Shekhovtsov
Czech Technical University in Prague
shekhole@fel.cvut.cz
Dmitrij Schlesinger
Dresden University of Technology
Dmytro.Shlezinger@tu-dresden.de
Boris Flach
Czech Technical University in Prague
flachbor@fel.cvut.cz
Ab stract
The importance of Variational Autoencoders reaches far beyond standalone gen-
erative models — the approach is also used for learning latent representations and
can be generalized to semi-supervised learning. This requires a thorough analy-
sis of their commonly known shortcomings: posterior collapse and approximation
errors. This paper analyzes VAE approximation errors caused by the combination
of the ELBO objective and encoder models from conditional exponential families,
including, but not limited to, commonly used conditionally independent discrete
and continuous models. We characterize subclasses of generative models consis-
tent with these encoder families. We show that the ELBO optimizer is pulled away
from the likelihood optimizer towards the consistent subset and study this effect
experimentally. Importantly, this subset can not be enlarged, and the respective
error cannot be decreased, by considering deeper encoder/decoder networks.
1	Introduction
Variational autoencoders (VAE, Kingma & Welling, 2014; Rezende et al., 2014) strive at learning
complex data distributions pd(x), x ∈ X in a generative way. They introduce latent variables
Z ∈ Z and model the joint distribution as pθ(X | z)p(z), where P(Z) is a simple distribution which
is usually assumed to be known. The conditional distribution pθ (x | z), called decoder, is modeled
in terms of a deep network parametrized by θ ∈ Θ. Models defined in this way allow to sample
from pθ (x) = Ep(z)pθ (x | Z) easily, however at the price that computing the posterior pθ (Z | x) =
Pθ (x| Z)P(Z)/pθ (χ) is usually intractable. To handle this problem, VAE approximates the posterior
pθ (Z | x) by an amortized inference encoder qφ(Z | x) parametrized by φ ∈ Φ. Given the empirical
data distribution Pd(x), the model is learned by maximizing the evidence lower bound (ELBO) of
the data log-likelihood L(θ) = Epd logPθ(x). It can be expressed in the following two equivalent
forms:
LB (θ,φ) = EPd[Eqφ log Pθ (XI Z)- DKL (qφ(z | x) k P(Z))]
=L⑹-Epd[DKL-φ(Z | X) k Pθ(Z | x))].
(1a)
(1b)
The first form allows for stochastic optimization of ELBO while the second form shows that the gap
between log-likelihood and ELBO is exactly the mismatch between the encoder and the posterior.
VAEs constitute a powerful deep learning extension of the expectation-maximization (EM) approach
to handle latent variables. They are useful not only as generative models but also, e.g., in semi-
supervised learning (Kingma et al., 2014; Mattei & Frellsen, 2019). Furthermore the encoder part
constructs an efficient embedding of the data in the latent space, useful in many applications. The
outreach of the VAE approach requires therefore a careful empirical and theoretical analysis of the
problems and trade offs involved. The most important ones are (i) posterior collapse (He et al., 2019;
Lucas et al., 2019; Dai et al., 2018; Dai & Wipf, 2019; Dai et al., 2020) and (ii) approximation errors
caused by an inappropriate choice of the encoder family.
1
Published as a conference paper at ICLR 2022
F (θ)
Figure 1: Diagram of the VAE trade-off. The optimal solution Θvae is “in between” the maximum
likelihood solution Θml and the best solution in the class Θφ of consistent VAEs, where the posterior
approximation error function F(θ) vanishes. We give an explicit characterization of this consistent
set. At Θvae there is a balance between the gradient of -F (blue arrow) and the gradient of the data
log-likelihood (black arrow).
The VAE approximation error has been studied (e.g., Cremer et al. 2018; Hjelm et al. 2016; Kim
et al. 2018) so far mainly empirically. The problem also occurs and is well-recognized in the context
of variational inference and variational Bayesian inference, where the target posterior distribution is
expected to be complex. It is commonly understood, that the mean field approximation of pθ (Z | x)
by qφ(z | x) in (1b) significantly limits variational Bayesian inference. In contrast, in VAEs, the
decoder may adopt to compensate for the chosen encoder family. The effect of this coupling, we
believe, is not fully understood. The phenomenon of decoder adopting to the posterior was experi-
mentally observed, e.g., by Cremer et al. (2018, Section 5.4), noting that the approximation error is
often dominated by the amortization error. Turner & Sahani (2011, Sec. 1.4) analytically show for
linear state space models that simpler variational approximations (such a mean-field) can lead to less
bias in parameter estimation than more complicated structured approximations. Similarly, Shu et al.
(2018) view the VAE objective as providing a regularization and show that making the amortized
inference model smoother, while increasing the amortization gap, leads to a better generalization.
The common (empirical) understanding of the importance of the gap between the approximate and
the true posterior has led to many generalizations of standard VAEs, which achieve impressive practi-
cal results, notably, tighter bounds using importance weighting (Burda et al., 2016; Nowozin, 2018),
encoders employing normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016), hierar-
chical and autoregressive encoders (Vahdat & Kautz, 2020; S0nderby et al., 2016; Ranganath et al.,
2016), MRF encoders (Vahdat et al., 2020) and more. While these extensions mitigate the posterior
mismatch problem, they often come at a price of a more difficult training and more expensive in-
ference. Furthermore, simpler encoders may be of practical interest. Burda et al. (2016, Appendix
C) illustrates that IWAE approximate posteriors are less regular and more spread out. In contrast,
factorized encoders provide simple embeddings useful for downstream tasks such as semantic hash-
ing (Chaidaroon & Fang, 2017).
The aim of this paper is to study the approximation error of VAEs and its impact on the learned
decoder. We consider a setting that generalizes many common VAEs, in particular popular models
where encoder and decoder are conditionally independent Bernoulli or Gaussian distributions: we
assume that both decoder and encoder are conditional exponential families. We identify the subclass
of generative models where the encoder can model the posterior exactly, referred to as consistent
VAEs. We give a characterization of consistent VAEs revealing that this set in fact does not depend
on the complexity of the involved neural networks. We further show that the ELBO optimizer is
pulled towards this set away from the likelihood optimizer. Specializing the characterization to
several common VAE models, we show that the respective consistent models turn out to be RBM-
like in many cases. We experimentally investigate the detrimental effect in one case and show that a
simpler but more consistent VAE can perform better in the other.
2	Problem Statement
We adopt the following notion of approximation error. Consider a generative model class PΘ =
{pθ(χ,z) | θ ∈ Θ}, the encoder class Qφ = {qφ(z| x) | φ ∈ Φ} and the data distribution Pd(x).
The maximum likelihood generative model is given by θML ∈ argmaxθ∈Θ Epd(x) logpθ(x). For a
decoder with parameters θ we define its approximation error as the likelihood difference L(θML) -
2
Published as a conference paper at ICLR 2022
L(θ). Respectively, the VAE approximation error is defined for a given θ as:
L(θML) - maxφ LB (θ, φ) ≥ L(θML) - L(θ).	(2)
In order for this error to become zero, two conditions are necessary and sufficient:
•	Parameters (θ, φ) must be optimal for the ELBO objective.
•	ELBO must be tight at (θ, φ), i.e., LB (θ, φ) = L(θ).
Assuming that the optimality can be achieved, we study the non-tightness gap L(θ) - LB (θ, φ).
From (1b) it expresses as Epd [DκL(qφ(z | x) k pθ(Z | x))]. It follows that ELBO is tight at (θ, φ) iff
qφ(z | x) ≡ pθ(z | x). Hence, we define the consistent set ΘΦ ⊆ Θ as the subset of distributions
pθ(x, z) whose posteriors are in QΦ, i.e.,
Θφ = {θ ∈ Θ I ∃φ ∈ Φ: qφ(z | x) ≡ Pθ(Z | x)}.	(3)
The KL-divergence in the ELBO objective (1b) can vanish only if θ ∈ ΘΦ . If the likelihood max-
imizer θML is not contained in ΘΦ , then this KL-divergence pulls the optimizer towards ΘΦ and
away from θML as illustrated in Fig. 1.
We characterize the consistent set ΘΦ , on which the bound is tight, and show that this set is quite
narrow and does not depend on the complexity of the encoder and decoder networks beyond simple
1-layer linear mappings of sufficient statistics.
3	Theoretical Analysis
We consider a general class of VAEs, where both encoder and decoder are defined as exponential
families. This class includes many common models, in particular Gaussian VAEs and Bernoulli
VAEs with conditional independence assumptions, but also more complex ones, e.g., where the
encoder is a conditional random field (Vahdat et al., 2020)1.
Assumption 1 (Exponential family VAE). LetX andZ be sets of observations and latent variables,
respectively. We consider VAE models defined by
Pθ(x| Z) = h(x)exp[(ν(x),fθ(z)i - A(fθ(z))]	(4a)
qφ(z | x) = h'(z)exp[hψ(z),gφ(x)i — B(gφ(x))],	(4b)
where ν : X → Rn and ψ : Z → Rm are fixed sufficient statistics of dimensionality n and m;
fθ : Z → Rn and gφ : X → Rm are the decoder, resp., encoder, networks with learnable parameters
θ, resp. φ; h : X → R+, h0 : Z → R+ are strictly positive base measures and A, B denote the
respective log-partition functions.
Notice that this assumption imposes no restrictions on the nature of random variables x and Z . They
can be discrete or continuous, univariate or multivariate. Similarly, it imposes no restrictions on the
complexity of the decoder and encoder networks fθ (Z) and gφ(x).
Characterization of the consistent set. In the first step of our analysis, we investigate the conditions
under which the approximation error of an exponential family VAE can be made exactly zero. As
discussed above, a tight VAE (θ, φ) must satisfy ∀(x, Z) qφ(Z | x) = pθ (Z | x), which leads to the
following theorem.
Theorem 1. The consistent set ΘΦ of an exponential family VAE is given by decoders of the form
p(x | Z) = h(x) exp hν (x), W ψ(Z)i + hν (x), ui - A(Z) ,	(5)
where W is a n × m matrix and u ∈ Rn. Moreover, the corresponding encoders have the form
q(Z| x) = h0(Z) expψ(Z), WTν(x) + hψ(Z), vi - B(x)],	(6)
where v ∈ Rm.
1Notice, however, that this class does not include VAEs with advanced encoder families like normalizing
flows, hierarchical and autoregressive encoders.
3
Published as a conference paper at ICLR 2022
This is a direct consequence of a theorem by Arnold & Strauss (1991) (see Appendix A.1 for more
details). For a tight VAE, Theorem 1 states that the decoder and encoder are generalized linear
models (GLMs) (5) and (6) with the interaction between x and z parametrized by a matrix W and
two vectors u, v instead of the (complex) neural networks with parameters θ, φ. The corresponding
joint probability distribution takes the form of an EF Harmonium (Welling et al., 2005):
p(x,z) = h(x)h0(z) exp((ν(x), Wψ(z)i + W(x),u) + hψ(z), Vi — A).	(7)
Corollary 1. The subset ΘΦ of consistent models can not be enlarged by considering more complex
encoder networks g(x), provided that the affine family WTν(x) can already be represented.
Corollary 2. Let the decoder network family be affine in ψ(z), i.e., f(z) = Wψ (z) + a and let the
encoder network family g(x) include at least all affine maps V ν(x) + b. Then any global optimum
of ELBO attains a zero approximation error.
VAE can escape consistency when it degenerates to a flow. In practice, VAE models with rich de-
coders are almost never tight. It is therefore natural to ask, whether a small VAE posterior mismatch
error implies closeness of the optimal decoder to some decoder in the consistent set.
Definition 1. A VAE (pθ, q@) is ε-tight for some ε > 0 if Epd(X)DKL (qφ(z | x) ∣∣ pθ(z | x))] ≤ ε.
It turns out that this definition allows a VAE to approach tightness while not approaching consis-
tency. In the continuous case an example satisfying ε-tightness with non-linear decoder follows
from Dai & Wipf (2019, Theorem 2). They show, for a class of Gaussian VAEs with general neural
networks fθ, gφ, that it is possible to build a sequence of network parameters θt, φt with the follow-
ing properties: i) the target distribution is approximated arbitrary well, ii) the posterior mismatch
DKL(qφt (z| x) ∣ pθt (z| x)) approaches zero and iii) both the encoder and decoder approach deter-
ministic mappings. The VAE thus approaches a flow model (or invertible neural network) between
the data manifold and a subspace of the latent space (Dai & Wipf, 2019). Clearly, in a general case
the flow must be non-linear. A similar case can be made for discrete variables, see Example A.1.
Non-deterministic nearly-tight VAEs approach consistency. We would however argue that the
mode where the decoder and encoder are nearly-deterministic is not a natural VAE solution. By
making additional assumptions, excluding such deterministic solutions, and restricting ourselves to
the finite space in order to simplify the analysis, we can show that an ε-tight VAE does indeed
approach an EF-Harmonium.
Theorem 2. Let (pθ, qφ) be an exponential family VAE (Assumption 1) on a discrete space X × Z
with encoder qφ(z | x) and decoder posterior pθ (z | x) both bounded from below by α > 0. If the
VAE is ε-tight, then there exists a matrix W ∈ Rn,m and vectors u∈ Rn, v ∈ Rm such that the joint
model implied by the decoderpθ (x, z) = pθ (x| z)p(z) can be approximated by an unnormalized EF
Harmonium
p(x, Z) = h(x)h0(z) exp((ν(x), Wψ(z)i + W(x), U + hv, ψ(z)i + C)	(8)
with the error bound
Epd(X) [(log Pθ (x,z) — log P(x,z))2] ≤ ⅛ + o(ε)	VZ ∈Z.	(9)
The proof is given in Appendix A.3. In this theorem the function p(x, z) is non-negative but does
not necessarily satisfy the normalization constraint of a density. Re-normalizing it by adjusting c
in (8) may break the approximation guarantee. Nevertheless, if ε is small enough and, e.g., the
data distribution is non-negative on the whole X, we expect it to approach a density, in particular to
recover the result in Theorem 1 in the limit. Note that the theorem does not make any assumptions
about optimality of (θ, φ), i.e., it describes all models in the vicinity of the consistent set in Fig. 1.
3.1 Cases Analysis
This subsection gives a detailed analysis of consistent VAE models in several concrete cases of
practical interest.
Diagonal Gaussian VAE Let us consider a Gaussian VAE, as commonly applied to image gen-
eration (e.g., Dai & WiPf (2019)). Let X = Rn, Z = Rm, p(x| z) = N(x|μd(z),σ2I),
4
Published as a conference paper at ICLR 2022
q(z | x) = N(Z | μe(x), diag(σ2(x))), where μd, μe and σe are neural networks and σd is a common
pixel observation noise parameter. The decoder has minimal sufficient statistics ν(x) = x and base
measure h(x) = N(x| 0, σd2I). The encoder has minimal sufficient statistics ψ(z) = (z, z2), where
the square is coordinate-wise. Theorem 1 implies that a tight optimal VAE has the joint model
p(x,z) H h(x) exp[〈x, Wz + Vz2 + a)+ ® Zi +〈c, z2)]	(10)
for some matrices W, V and vectors a, b, c. Furthermore, the integral over z must be finite for all
x and therefore xTV + c < 0 must hold for all x ∈ Rn . This is possible only if V = 0 and
c < 0. The joint distribution is therefore a multivariate Gaussian and the same holds for its marginal
p(x). The neural network μd(z) must degenerate to μd(z) = σ2 ∙ (Wz + a) and the two neural
networks for the encoder to σ2(x) = -1/2c and μe(x) = -(WTx + b)∕2c, where divisions are
coordinate-wise. VAEs with such simplified, linear Gaussian encoder-decoder pairs, called "linear
VAEs" (Lucas et al., 2019) are known tobe consistent and to match the probabilistic PCA model (Dai
et al., 2018; Lucas et al., 2019). In this context, our Corollary 2 is a generalization of (Lucas et al.,
2019, Lemma 1) showing consistency of linear VAEs, to decoders in any exponential family with
natural parameters being a linear mapping of any fixed lifted latent representation ψ(z).
We argue that a joint Gaussian model is too simplistic to generate complex data such as realistic
images and that in this case the VAE error is detrimental. In Section 4.2 we experimentally confirm
that optimizing ELBO for a general decoder network μd causes qualitative and quantitative degrada-
tion relative to the ML decoder. Note that if we allowed σd to be dependent on z, the resulting joint
statistics in V 0 ψ would include terms x2z, x2z2. The joint distribution would not be Gaussian and
may be in fact multi-modal (see Anil Bhattacharayya’s distribution in Arnold et al. 2001).
Bernoulli-MRF VAE Vahdat et al. (2020) proposed to consider encoders in the Markov Random
Field (MRF) family, in particular encoders of the form q(z| x) H exp(hz1, V(x)z2i + hb1(x), z1i +
hb2(x), z2i), where z1, z2 are two groups of latent variables and interaction weights V, b1, b2 are
computed by the encoder network. In this case q(z| x) is itself a (conditional) RBM. While evalu-
ating q(z| x) is difficult, MCMC sampling is efficient. We assume binary observations x and a con-
ditionally independent Bernoulli decoder family as above. The decoder thus has sufficient statistics
ν = x and the encoder has ψ = (z1, z2, z1 0 z2). Introducing homogeneous constant components
x0 = z01 = z02 = 1, the family of consistent joint distributions can be compactly described as
p(x, z) = expPi,j,k Wi,j,k xi zj1 zk2],	(11)
where the summation in all indices starts from 0 and -W0,0,0 is the log-partition function. This
joint model is a higher order MRF with the highest order potentials given by cubic monomials.
Standard Bernoulli VAEs are a special case of the Bernoulli-MRF model, obtained when the inter-
action weights V are zero. The joint distribution of such tight optimal VAEs takes the form
p(x, z) = C exp(xτWz + UTx + vTz),	(12)
which is a restricted Boltzmann machine (RBM). Since RBMs are well known for being useful in
many applications (dimensionality reduction, collaborative filtering, feature learning, topic model-
ing), we hypothesize that they can make a good baseline for Bernoulli VAEs and furthermore that
the effect of pulling the VAE solution towards an RBM may be benign in case of insufficient data.
For example IWAE test likelihood in (Burda et al., 2016) is worse than that of an RBM (Burda et al.,
2015) on the Omniglot dataset. Furthermore, debiasing of IWAE (Nowozin, 2018) does not improve
test likelihood in many cases.
Bernoulli VAE for Semantic Hashing One important application of Bernoulli VAEs is the seman-
tic hashing problem, initially proposed and modeled with RBMs (Salakhutdinov & Hinton, 2009).
The problem is to assign to each document / image a compact binary latent code that can be used
for quick retrieval by the nearest neighbor search. We will detail now a more recent VAE model for
text documents (Chaidaroon & Fang, 2017; Shen et al., 2018) and show that it can be tight only in a
full posterior collapse. We correct the encoder so as to allow a larger consistent set and observe that
the resulting consistent joint distribution forms a multinomial-Bernoulli RBM.
Let x ∈ NK be word counts in a document with words from a dictionary of size K. Let z ∈ {0, 1}m
be a binary latent code. Let l = Pk xk denote the document’s length. We assume that the document
5
Published as a conference paper at ICLR 2022
length is independent of the latent topic and its distribution p(l) can be learned separately (e.g., a
log-normal distribution is a good fit). The decoder is defined using the multinomial distribution
model (words in the document are drawn from the same categorical distribution corresponding to its
topic):
p(x,l| z) = p(l)h(x| l) exp(f (z)Tx - lA(f(z))),	(13)
where f (z) is a neural network mapping the latent code to the logits of word occurrence probabil-
ities, A(η) = log Pk exp(ηk) and h(x| l) = [[Pkxk=l]] l!/ Qkxk ! is the base measure2. The
sufficient statistics are the word counts x. The prior p(z) is assumed uniform Bernoulli.
The encoder is the conditionally independent Bernoulli model, expressed as
q(z| x,l) Y exp(g(x)Tz),	(14)
where g(x) is the encoder network. Chaidaroon & Fang (2017) experimented with the encoder
and decoder design and recommended using TFIDF features instead of raw counts. First, we note
that the inverse document frequency (IDF) is not relevant, since it can be learned by the first linear
transform in the encoder. Effectively, the term frequency (TF), given by x/l, is used. This choice
is adopted in later works (Shen et al., 2018; Zamani Dadaneh et al., 2020; Nanculef et al., 2020). It
might seem reasonable that the latent code modeling the document topic should not depend on the
document length, only on the distribution of words in the document. However, we will argue that
this rationale is misleading for stochastic encoders.
We apply Theorem 1 to two groups of variables: observed (x, l) and latent z with h(x, l) =
h(x| l)p(l), ν(x, l) = x and ψ(z) = z. It follows that the consistent joint family is
p(x, l, z) = h(x, l) exp(xTW z + aTx + bTz + c).	(15)
This however implies that g(x) = Wx + b, i.e. the encoder network must be linear in x. Conse-
quently, it cannot match a function of word frequencies x/l (as chosen by design) unless W = 0, i.e.
a completely trivial model with an encoder not depending on x. Such an encoder would imply full
posterior collapse. The corresponding consistent set ΘΦ coincides with the set of collapsed VAEs
where the decoder does not depend on the latent variable z in Fig. 1. We conjecture that the inherent
inconsistency of this VAE has a detrimental effect on learning.
If instead, we let the encoder network to access word counts x directly, we obtain that g(x) =
W x + b can form a consistent VAE. Inspecting this encoder model in more detail, we see that
it builds up topic confidence in proportion to the evidence (total word counts), as the true pos-
terior would. Indeed, the true posterior p(z | x, l) satisfies the factorization by Bayes’s theorem:
p(z| x, l) = p(x| z, l)p(z)/p(x| l). The priorp(z) is constant by design, p(x| l) does not vary with z
and p(x| z, l) factors over all word instances according to (13). In other words, the coupling between
x and z in log p(z | x, l) is linear in x.
In Section 4 we study the proposed correction experimentally and show that it enables learning better
models under a variety of settings.
4	Experiments
4.1	Artificial Example
To start with, we illustrate our findings on a toy example. We consider a simple Gaussian mixture
model for which we can easily generate samples and compute all necessary quantities including
the ELBO objective. We define the ground truth model to be p* (χ,z) = p*(z)p*(χ∣ z), with Z ∈
{1... 4}, p*(z) ≡ 0.25, X ∈ R2, p*(x∣ Z) = N (x| μ(z), σ2I), i.e. a mixture of four 2D Gaussians.
Fig. 2(a) shows the color-coded posterior distribution p*(z | x). We assign a color to each component
and represent p* (Z | x) for each pixel X ∈ R2 by the corresponding mixture of the component colors.
For better interpretability, we illustrate further results by decision maps arg maxz p(Z | x). Fig. 2(b)
shows the decision map for p* (Z| x).
2Prior work (Chaidaroon & Fang, 2017; Shen et al., 2018) omits the base measure as it has no trainable
parameters.
6
Published as a conference paper at ICLR 2022
Figure 2: Artificial example. (a) Color-coded posterior distribution p* (z | x). (b-e): Decision maps
(arg maXz) of: (b) true posterior p*(z | x), (c) factorized encoder q°(Z | x) after joint learning, (d)
model posterior pθ (z | x) after joint learning, (e) RBM trained on the same data. Gaussian centers
μ(z) are shown as black dots. (f) Probability simplex of distributions over the four binary config-
urations. The vertices correspond to pure (deterministic) binary states represented by the code and
its respective color. The surface shows the manifold of factorized distributions realizable by q(z | x).
Notice that the two edges (00, 11) and (01, 10) are not in the manifold because they correspond
to switching of two bits simultaneously in a correlated way. The factorized approximation cannot
model transitions between these states. Hence, when learning VAE, these pairs of states are repulsed
in the decision maps (c), (d).
The aim of the experiment is to learn a VAE and to study the influence of the factorization assump-
tion on the results. We use the decoder architecture as in the ground truth model — a Gaussian
distributionpθ(X| Z) = N(X| θzoh, σ21), where Zoh is the one-hot (categorical) representation of z,
and θ is a 2×4 matrix that maps the four latent codes to 2D centers at general locations. Note that
the ground truth model is contained in the chosen decoder family. Hence, the ML solution is the
ground truth decoderp*(x| z). We restrict the encoder to factor over the binary representation of the
code zb ∈ {0,1}2 and define it as qφ(Zb | x) (X exp (gφ(x),zb〉，where gφ(x) is implemented as a
feed-forward network with two hidden layers, each with 64 units and ReLU activations.
First, we pre-train our factorized encoder by optimizing ELBO and keeping the ground truth decoder
fixed. The next step is to jointly train the encoder and decoder by maximizing ELBO. Since we are
interested how the ELBO objective distorts the likelihood solution, we start with the ground truth
decoder and the pre-trained encoder from the previous step. The ELBO-optimal decoder has to
match not only the training data, but also the inexact, factorizing encoder. The resulting qφ (z | x) is
shown in Fig. 2(c) and the learned model posterior pθ (z | x) X p(z)pθ(x | z) in Fig. 2(d). Note that
they match each other pretty well, but differ substantially from the ground truth posterior shown in
Fig. 2(b). The impact of the factorization is clearly visible — one can see two decision boundaries
(one for each bit of zb), which together partition the x-space into four regions, approximating the
true posterior. For comparison, Fig. 2(e) shows the posterior of an RBM trained on the same data.
It is clearly seen that the ELBO optimizer is pulled away from the likelihood optimizer towards
an RBM solution. Notice also the explanation given in Fig. 2(f). Summarizing, this simple toy
example clearly shows the VAE approximation error caused by the combination of ELBO objective
and the factorization assumption for the encoder. While the numerical difference between ELBO
and log-likelihood is small (see details in Appendix C.1), the qualitative difference in Fig. 2 appears
substantial.
4.2	Gaussian VAEs for CelebA Images
The goal and design of this experiment is similar to the previous one. We first define a ground truth
decoder which is used to generate training images. Then we pre-train an encoder by ELBO keeping
the ground truth decoder fixed. Finally, we train both model parts starting from the ground truth
decoder and the pre-trained encoder.
The ground truth generative model is obtained by training a convolutional Generative Adversarial
network (GAN) using code of Inkawhich (2017) on the CelebA dataset Liu et al. (2015). We scale
and crop all images to 64×64 pixels. In order to get a stochastic decoder, we equip the GAN gener-
ator x = d(z), z ∈ R100, x ∈ R64×64×3 with image noise σd. The ground truth generative model is
thus defined as p* (x, z) = p*(z)p* (x| Z), where p*(z) = N(z| 0, I), p* (x| Z) = N(x| μd(z), σdI),
and σd2 is a common noise variance for all pixels and color channels. We chose σd = 0.05 (the color
values are normalized to [-1, 1]). This corresponds to an image noise level, which is just visible,
but does not disturb visual perception essentially.
7
Published as a conference paper at ICLR 2022
Figure 3: Results for the encoder learned by su-
pervised conditional likelihood. Top row: train-
ing samples X 〜p*(x). Second row: the Cor-
responding reconstructions from mean values of
z, i.e. x 〜p*(x∣ μe (X)). Third row: reconstruc-
tions from sampled z, i.e. Z 〜N(μe(X),σ2(X))
followed by X 〜p*(x| Z).
Figure 4: Visual comparison - images drawn
from the original/learned models. Each col-
umn corresponds to a particular value of Z 〜
N(0, I). Top row: the ground truth model, mid-
dle row: learned decoder with fixed σd , bottom
row: decoder with learned σd .
The decoder family of the considered VAE consists of networks with the same architecture as d(Z).
This ensures that the ground truth decoder is a likelihood maximizer of the VAE model. The encoder
is defined as qΦ(z| x) = N(x| μe(x), diag(σ2(x))), where μe,Qe ∈ R100 are two outputs of a
convolutional neural network with an architecture similar to the architecture of the discriminator
used for training the GAN (except the output layer), i.e., qφ(Z | X) is a multivariate Gaussian with
diagonal covariance matrix whose parameters depend on X.
We pre-train the encoder fully supervised by maximizing its conditional log-likelihood
Ep*(x,z) log qφ(z| x) on examples drawn from the ground truth generating model p*(x, z).3 The
results of pre-training are shown in Fig. 3. Then we jointly learn the encoder and decoder by max-
imizing ELBO on x-samples drawn from the ground truth model. We start the learning with the
ground truth decoder p* (x | Z) and the encoder obtained in the previous step. Two variants are con-
sidered for this training: (i) keeping the image noise σd fixed and (ii) learning it along with other
model parameters. We evaluate the results quantitatively by computing the Frechet Inception Dis-
tances (FID) between the ground truth model p*(x∣ Z) and the obtained decoders pθ(x | Z) using the
code of Seitzer (2020). For this we generate 200k images from each model. The obtained values
are given in Tab. 1. Fig. 4 shows images generated by the ground truth model and the two learned
models.
To conclude, ELBO optimization harms the decoder considerably as clearly seen both from FID-
scores and the generated images. Models with higher ELBO values have worse FID-scores and
produce less realistic images.
Table 1: Optimizing the ELBO starting from the ML solution degrades the FID-score. The first
row corresponds to the pre-trained encoder for the ground truth decoder, its FID-score therefore
compares two image sets, both generated by the ground truth model.
Experiment	ELBO	FID
optimize encoder (conditional likelihood)	-364513.78	0.13
optimize encoder and decoder (ELBO, fixed σd)	-5898.94	77.10
optimize encoder and decoder (ELBO, learned σd)	9035.69	117.87
4.3	Bernoulli VAE for Text Documents
This experiment compares training of the VAE model for semantic hashing discussed in Section 3.1
with and without our proposed correction on the 20Newsgroups dataset (Lang & Rennie, 2008). We
describe the dataset, preprocessing and optimization details in Appendix C.2.
We compare three encoders: e1: linear encoder on word counts (the proposed correction), e2: deep
(2 hidden layers) encoder using word frequencies and e3: a linear encoder on frequencies. The
3In contrast to the previous experiment we optimize the “forward” KL-divergence, i.e. the conditional like-
lihood, instead of the “reverse” KL-divergence used in ELBO for simplicity.
8
Published as a conference paper at ICLR 2022
encoders are compared across different numbers of latent Bernoulli variables (bits) and different
decoder depths. The decoder depth denotes the number of fully connected hidden ReLU layers (0-
2). In both the encoder and decoder we use 512 units in hidden layers. The prior work mainly used
linear decoders following the ablation study of Shen et al. (2018). Our experiments also suggest that
using deep decoders in combination with longer bit-length leads to a significant overfitting. When
the decoder is linear, the posterior distribution is tractable and is linear as well, i.e., the VAE model
is equivalent to a Multinomial-Bernoulli RBM. We experimentally verify that a linear encoder on
word counts e1 indeed works better in this case. However, perhaps more surprisingly, we also find
out that it works better even for non-linear decoders.
Table 2 show the achieved training and test Negative ELBO (NELBO) values. We observe across all
settings that the simple encoder e1 is consistently better than the more complex encoder e2 which
in turn is significantly better than the linear encoder on frequencies e3. We conclude that the use
of VAEs with deep encoders based on word frequencies (Chaidaroon & Fang, 2017; Shen et al.,
2018; Zamani Dadaneh et al., 2020; Nanculef et al., 2020) is sub-optimal for this dataset. We also
observe that linear decoders generalize better under 32 and 64 bits compared to more complex
decoders, which suffer from overfitting. This implies that in these cases the best encoder-decoder
combination is linear, i.e. the basic RBM model. This evidence agrees with previously observed
worse reconstruction error with deep architecture (Dai et al., 2020), however we did not observe (a
more severe) posterior collapse with deeper models amongst the depths we report.
Table 2: Training and test NELBO values for Text-VAE with different configurations of bits, de-
coder and encoder. Bold highlights the best encoder choice and underlined bold values are the best
decoder-encoder combinations for each bit-length.
Training
Bits	dhidden=0			dhidden=1			dhidden=2		
	e1	e2	e3	e1	e2	e3	e1	e2	e3
8	419	429	439	321	390	415	325	370	421
16	382	398	419	201	329	407	164	269	413
32	337	358	412	165	189	403	132	159	411
64	296	324	407	171	189	398	134	149	409
Test
Bits	dhidden=0			dhidden=1			dhidden=2		
	e1	e2	e3	e1	e2	e3	e1	e2	e3
8	423	429	435	413	421	424	418	423	427
16	409	417	421	404	422	420	410	416	422
32	396	413	416	399	413	418	406	416	421
64	392	411	414	398	413	417	406	417	422
5	Conclusions
We have analyzed the approximation error of VAEs in a general setting, when both the decoder and
encoder are exponential families. This includes commonly used VAE variants as, e.g., Gaussian
VAEs and Bernoulli VAEs. We have shown that the subset of generative models consistent with the
encoder class is quite restricted: it coincides with the set of log-bilinear models on the sufficient
statistics of both decoder and encoder, i.e., RBM-like models. This consistent subset can not be
enlarged by using more complex encoder networks as long as encoder’s sufficient statistics remain
unchanged. In combination with the ELBO objective, this causes an approximation error — the
ELBO optimizer is pulled away from the data likelihood optimizer towards this subset. Moreover,
we proved theoretically that close-to-tight EF VAEs must be close to RBMs in a certain sense.
We have shown that the error is detrimental when the consistent subset is too restrictive. In the
cases where a lot of data is available and a high quality generative model is of the primary interest,
such as in the CelebA experiment, more expressive encoder families are required in addition to large
networks. On the other hand the VAE approximation error may result in a useful regularization when
the respective RBM is a good baseline model. In this case we can speak of a binning inductive bias
towards RBM, such as in our text-VAE experiment. Furthermore, simple encoders can be desired
when the learned representations are of interest, in particular they appear to facilitate similarity in
Hamming distance, useful in the semantic hashing problem.
Further connections to related work and discussion can be found in Appendix B.
9
Published as a conference paper at ICLR 2022
Acknowledgment
D.S. was supported by the German Federal Ministry of Education and Research (BMBF,
01/S18026A-F) by funding the competence center for Big Data and AI “ScaDS.AI Dres-
den/Leipzig”. A.S and B.F gratefully acknowledge support by the Czech OP VVV project ”Re-
search Center for Informatics” (CZ.02.1.01/0.0/0.0/16019/0000765)". B.F. was also supported by
the Czech Science Foundation, grant 19-09967S. The authors gratefully acknowledge the Center for
Information Services and HPC (ZIH) at TU Dresden for providing computing time. We thank the
anonymous reviewers for many helpful links and suggestions.
References
B. C. Arnold and D. J. Strauss. Bivariate distributions with conditionals in prescribed exponential
families. Journal ofthe Royal Statistical Society SeriesB(Methodological), 53(2):365-375,1991.
Barry C. Arnold, Enrique Castillo, and Jose Maria Sarabia. Conditionally specified distributions:
An introduction. Statistical Science, 16(3):249 - 274, 2001.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of MRF
log-likelihood using reverse annealing. In AISTATS, volume 38, pp. 102-110, 09-12 May 2015.
Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
ICLR, 2016.
Suthee Chaidaroon and Yi Fang. Variational deep semantic hashing for text documents. In SIGIR
Conference on Research and Development in Information Retrieval, pp. 75-84, 2017.
Chris Cremer, Xuechen Li, and David Duvenaud. Inference suboptimality in variational autoen-
coders. In ICML, volume 80, pp. 1078-1086, 2018.
Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In ICLR, 2019.
Bin Dai, Yu Wang, John Aston, Gang Hua, and David Wipf. Connections with robust PCA and
the role of emergent sparsity in variational autoencoder models. Journal of Machine Learning
Research, 19(41):1-42, 2018.
Bin Dai, Ziyu Wang, and David Wipf. The usual suspects? Reassessing blame for VAE posterior
collapse. In ICML, 2020.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. In ICLR, 2019.
Devon Hjelm, Russ R Salakhutdinov, Kyunghyun Cho, Nebojsa Jojic, Vince Calhoun, and Junyoung
Chung. Iterative refinement of the approximate posterior for directed belief networks. In Advances
in Neural Information Processing Systems, volume 29, pp. 4691-4699, 2016.
Nathan Inkawhich. DCGAN tutorial, 2017. URL https://pytorch.org/tutorials/
beginner/dcgan_faces_tutorial.html.
Yoon Kim, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. Semi-amortized
variational autoencoders. In ICML, volume 80, pp. 2678-2687, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-supervised
learning with deep generative models. In Proceedings of the 27th International Conference on
Neural Information Processing Systems - Volume 2, NIPS’14, pp. 3581-3589, Cambridge, MA,
USA, 2014. MIT Press.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In NeurIPS, volume 29, pp.
4743-4751, 2016.
10
Published as a conference paper at ICLR 2022
Ken Lang and Jason Rennie. The 20 newsgroups dataset, 2008. http://qwone.com/~jason/
20Newsgroups/.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the ELBO! a
linear VAE perspective on posterior collapse. In NeurIPS, volume 32,pp. 9408-9418, 2019.
Lars Maal0 e, Marco Fraccaro, Valentin LieVin, and Ole Winther. BIVA: A very deep hierarchy of
latent variables for generative modeling. In NeurIPS, volume 32, 2019.
Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: Deep generative modelling and imputation of
incomplete data sets. In ICML, volume 97, pp. 4413-4423, 09-15 Jun 2019.
Ricardo Nanculef, Francisco Alejandro Mena, Antonio Macaluso, Stefano Lodi, and Clau-
dio Sartori. Self-supervised Bernoulli autoencoders for semi-supervised hashing.	CoRR,
abs/2007.08799, 2020.
Sebastian Nowozin. Debiasing evidence approximations: On importance-weighted autoencoders
and jackknife variational inference. In International Conference on Learning Representations,
2018.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In ICML, vol-
ume 48, pp. 324-333, 20-22 Jun 2016.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In ICML,
volume 37, pp. 1530-1538, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, 2014.
Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. Int. J. Approx. Reasoning, 50(7):
969-978, July 2009.
Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/
pytorch-fid, August 2020. Version 0.1.1.
Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang, Guoyin Wang, Ricardo Henao,
and Lawrence Carin. NASH: Toward end-to-end neural architecture for generative semantic hash-
ing. In Annual Meeting of the Association for Computational Linguistics, pp. 2041-2050, jul
2018.
Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized
inference regularization. In NeurIPS, volume 31, 2018.
Robert Sicks, Ralf Korn, and Stefanie Schwaar. A generalised linear model framework for β-
variational autoencoders based on exponential dispersion families. CoRR, 2021.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in Neural Information Processing Systems, volume 29, pp.
3738-3746, 2016.
Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Student-
t variational autoencoder for robust density estimation. In IJCAI, pp. 2696-2702, 7 2018.
R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series
models. In Bayesian Time series models, chapter 5, pp. 109-130. 2011.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. In NeurIPS,
volume 33, pp. 19667-19679, 2020.
Arash Vahdat, Evgeny Andriyash, and William Macready. Undirected graphical models as approxi-
mate posteriors. In ICML, volume 119, pp. 9680-9689, 13-18 Jul 2020.
11
Published as a conference paper at ICLR 2022
Max Welling, Michal Rosen-Zvi, and Geoffrey E Hinton. Exponential family harmoniums with an
application to information retrieval. In NeurIPS, volume 17, 2005.
Mingzhang Yin and Mingyuan Zhou. ARM: Augment-REINFORCE-merge gradient for stochastic
binary networks. In ICLR, 2019.
Siamak Zamani Dadaneh, Shahin Boluki, Mingzhang Yin, Mingyuan Zhou, and Xiaoning Qian.
Pairwise supervised hashing with Bernoulli variational auto-encoder and self-control gradient
estimator. In UAI, volume 124,pp. 540-549, 2020.
12
Published as a conference paper at ICLR 2022
Appendix
A	Proofs
A.1 Proof of Theorem 1
The proof directly follows from the characterization of conditionally specified joint distributions in
the exponential family given by Arnold & Strauss (1991), see (Arnold et al., 2001, Theorem 3):
Theorem A.1 (Arnold & Strauss 1991). Let x ∈ X and z ∈ Z be random variables with a strictly
positive joint distribution such that both conditional distributions are exponential families with den-
sities
p(x| z) = h(x) exphν (x), f(z)i - A(z)]	(16a)
p(z|x) = h0(z) exphψ(z), g(x)i - B(x)],	(16b)
where ν : X → Rn and ψ : Z → Rm are minimal sufficient statistics, f : Z → Rn and
g : X → Rm are any mappings, h(x) and h0 (z) are base measures, and A and B denote the
respective log-partition functions4.
Then there exists a matrix W ∈ M(n + 1, m + 1), such that the density of the joint distribution can
be represented as
p(x, z) = h(x)h0(z) exp hνe(x), Wψe(z)i ,	(17)
where νe, ψe denote the statistics vectors extended with an additional component 1.
A.2 Example of a discrete VAE approaching a flow
Example A.1. In this example we construct a VAE that can be arbitrary close to tight one, but where
the decoder network does not approach a linear map. Let X = {-1, 1}2, Z = {-1, 1}2 and let
p(z) be uniform. Let the decoder be conditionally independent
p(x| z) = expβ hx, π(z)i - A(z)	(18)
where π(z) denotes the invertible mapping of Z to X given by
x1 = z1	(19)
x2 = z1z2 .	(20)
If the parameter β is sufficiently large, the distribution p(x| z) approaches the deterministic dis-
tribution δx=π(z) . Its posterior therefore also approaches the deterministic distribution δz=π-1 (x)
(notice that π-1 = π). Let the encoder be the conditionally independent model q(z | x) =
exp βhz, π-1 (x)i - A(x) . This decoder by design approaches δx=π(z) as well and thus the VAE
(p, q) achieves ε-tightness for sufficiently large β. At the same time the deviation between loga-
rithms of probabilities log q(z| x) and logp(z| x) grows with β.
A.3 Proof of Theorem 2
The idea of the proof is to bound the difference between log p(z | x) and log q(z| x), which is done
by Proposition A.2 and then in the space of log-probabilities to approximate the non-linear mapping
fe (z) by a linear one as detailed in Proposition A.1. By carefully choosing the norms and the
approximation we obtain a bound on the error for the joint model, which despite the discreteness
assumption of the observation space X in Theorem 2 does not depend on its cardinality.
For a finite set X ⊂ X let H be the |X |-dimensional vector space with the inner product hu, vipd =
Px∈X pd(x)u(x)v(x), assuming that pd (x) > 0 for all x ∈ X. The respective norm will be denoted
as k ∙ kPd.
4The log-partition function is usually defined as a function of the (natural) parameter. In this context we
consider h,h0, f, g to be fixed and consider the dependence on x, z only.
13
Published as a conference paper at ICLR 2022
Proposition A.1. Under model Assumption 1, for any finite X ⊆ X there exists a matrix W ∈
M(n + 1, m + 1) such that joint distribution implied by the decoder p(x, z) = p(x| z)p(z) can be
approximated by an unnormalized EF Harmonium
p(x, Z) = h(x)h0(z) exp(hνe(x),Wψe(z)i)	(21)
with the error bound
(Vz) Pχ∈χ Pd(X)Ilog p(x,z)- log p(x,z)∣2 ≤ Pχ∈χ Pd(X)IlOg q(z | X)- log p(z | x)∣2. (22)
The function p(χ, z) is non-negative but does not necessarily satisfy the normalization constraint of
a density.
Proof. For clarity, we will omit the dependence of the decoder and encoder on their parameters θ,
resp. φ. Throughout the proof we will also assume that a single z ∈ Z is fixed.
First, we expand
logq(z|X) = hψ(z), g(X)i - B(X) +logh0(z);	(23a)
log P(z | X) = log P(X | z) + logP(z) - log P(X)
= hν(X), f (z)i - A(z) + log h(X) + logP(z) - log P(X),	(23b)
where A(z) = A(f(z)) and B(X) = B(g(X)). We can therefore represent
log q(z | X) - log P(z | X) = hψ(z), g(X)i - B(X) + log P(X) - log h(X)	(24)
- hν(X), f(z)i + logP(z) -logh0(z) - A(z)	(25)
= hψe(z), ge(X)i - hνe(X), fe(z)i ,	(26)
where
ψe(z) = (ψ(z), 1);	(27)
νe(X) = (ν(X), 1);	(28)
ge(X) = (g(X), log P(X) - B(X) - log h(X));	(29)
fe(z) = (f(z), logP(z) - A(z) - log h0(z)).	(30)
With this representation we have:
Px∈X Pd(X)II hνe(X), fe(z)i - hψe(z), ge(X)i II2	(31)
= Px∈X Pd(X)II logq(z|X) - log P(z | X)II2 =: ∆2.	(32)
Let V be the matrix with rows νe(X) for all X ∈ X. Let G be the matrix with rows ge(X) for all
X ∈ X. We can rewrite the condition (31) in the form
ξ = V fe(z) - Gψe(z),	(33a)
kξ k2pd = ∆2 ,	(33b)
where ξ ∈ R|X| is the vector of residuals. Let P be the orthogonal projection onto the range of V in
the space H. Multiplying (33a) by P on the left, we obtain
PVfe(z)-PGψe(z)=Pξ.	(34)
Because PV = V we obtain
Vfe(z)-PGψe(z)=Pξ.	(35)
We therefore can consider the decoder network approximation fe(z) = Wψe(z), where W is the
solution to the consistent system of linear equations VW = PG and is independent of z. We can
therefore express
kVfe(z) — Vfe(z)kpd = kPξkpd ≤ kξkpd = ∆,	(36)
where the inequality holds because P is an orthogonal projection in H.
14
Published as a conference paper at ICLR 2022
We obtained that the decoder network fe (z) can be approximated by a linear mapping Wψe(z) such
that
Px∈X Pd(X)IhVe(X),fe(Z)i - hνe(X) , Wψe(z)i∣ ≤ 想.	(37)
Expressing back
hνe(X), fe(z)i = hν(X), f(z)i + logp(z) - A(z) - log h0(z)	(38a)
= logp(X| z) + logp(z) - log h(X) - log h0(z)	(38b)
= log p(X, z) - log h(X) - log h0(z)	(38c)
we obtain that
Px∈xPd(x)∣ logp(x,z) - logp(x,z)∣2 ≤ ∆2,	(39)
where
logp(x,z) = log h(x) + log h0(z) + hνe(x), Wψe(z)i.	(40)
□
Proposition A.2. Under model Assumption 1, let X and Z be discrete (finite) sets and let z ∈ Z be
chosen. If q(z| X) ≥ α andp(z| X) ≥ α for all X ∈ X, where X ⊆ X and
Epd(x) [DKL(q(z| X) k p(z| X))] ≤ ε,	(41)
then
Px∈χ Pd(x)( logp(z| X)- log q(z| x))2 ≤ 旨 + o(ε).	(42)
Proof. Let us denote ε(X) = DKL (q(z | X) k p(z| X)). Pinsker’s inequality assures for each X
sup ∣Pq(z| X)(S)- Pp(Z| x)(S)∣2 ≤ ε(x)∕2.	(43)
S⊂Z
Substituting S = {z} we obtain
|p(z | X) - q(z | X)|2 ≤ ε(X)∕2.	(44)
By taking expectation in pd(X) on both sides we obtain a variant of Pinsker’s inequality:
Epd(X)Ip(Z|x) - q(Z|X)F ≤ Epd(X)ε(χ"2 = 1 Epd(X)[DKL(q(z|X) IlP(Z|x))] ≤ ε/2.	(45)
Notice that the LHS depends on the given z. Because all summands are non-negative it follows that
∀X0 ⊆ X PX∈X0 pd (X)|p(Z | X) - q(Z| X)|2 ≤ ε∕2.	(46)
This inequality will be used in several places below.
Consider X ∈ X such that p(Z | X) > q(Z | X). Then
| log p(Z | X) - log q(Z | X)|2 = log2 ⅛⅛)	(47a)
= log2(1 + p(z1x()-X()Z1 X))	(47b)
≤ log2(1 + P(Z1 X)aq(z1 X)),	(47c)
where the inequality holds because log2 is monotonously increasing for arguments greater equal
than 1, which is ensured. Let us now consider X ∈ X such that p(Z | X) < q(Z | X). Then
I logp(z| X) - log q(zI x)∣2 = log2 Pz^	(48a)
= log2(1+ ”澄 X))	(48b)
≤ log2(1 + q(z1 X)ap(z1 X)).	(48c)
In total, we obtain
I logp(zI x) - log q(zI x)∣2 ≤ log2(1+ Ip(Z1 X)-q(Z1 X)I).	(49)
15
Published as a conference paper at ICLR 2022
Let Us denote u(χ) = lp(z| x)[q(z| x)| and partition the set X into two parts:
X1 = {x ∈ X | u(x) < u0 }	(50a)
X2 = {x ∈ X | u(x) ≥ u0},	(50b)
where we chose u° ∈ [√e - 1, 5] for reasons to be clarified below. For both parts, i.e. k = 1, 2, we
have
Px∈xkPd(X)Ilogq(Z|X)-logP(Z|X)F ≤ Px∈xkPd(X)log2(I + |p(z|x);q(z|x)1).	(51)
For X1 , (51) can be fUrther boUnded as
≤ Px∈x1 Pd(X) log(1 + …α…12)	(52a)
≤ log Pχ∈χ1 Pd(X) (ι + …aq(z|x)12)	(52b)
=log (Pd(XI) + a2 pχ∈χι Pd(X)IP(Z | x) - q(Z | X)F)	(52C)
≤ log (1 + 2⅛) = 2α2 + o(ε),	(52d)
where the first ineqUality holds for u0 ≤ 5, becaUse in this case log2(1 +u) ≤ log(1 +u2) holds, the
second ineqUality is the Jensen’s ineqUality for log and the last ineqUality Uses (46) Under monotone
log.
For X2 we have the following. Let V = Pd(X2) = Px∈X Pd(X). We can express
Px∈X2Pd(X)log2(1 + …aq(z|x)1)	(53a)
=V(Pχ∈χ2 Pd(X∣ X2)log2 (1+ lp(z1x)aq(z|x)1))	(53b)
Using that u < u2 on X2 and that log2 (1 + u) is monotone for a positive argUment, we can
boUnd (53b) as
≤ V(Pχ∈X2 Pd(X∣ X2)log2 (1 + …:产 x)l2)) .	(54)
FUrther, Using that log2(1 + v) is concave on X2 for v ≥ e - 1, we have
≤ V log2 (Pχ∈χ2 Pd(X∣ X2)(1+ …aq(z|x)12))	(55a)
=V log2 (1+Px∈x2 Pd(X∣ X2)…aq(z|x)12)	(55b)
≤ V log2 (1 + 2α2v ),	(55c)
where in the last step we Used (46).
Next we show that V itself is bounded above by 2U⅛ ∙ It follows from
u2V = p Pd(X)U2 ≤ p Pd(X) 1p(z|x)aq(z1x)12 ≤ p Pd(X) 1p(z|x)aq(z1x)12 ≤ 皋,(56)
x∈X2	x∈X2	x∈X
where the last inequality is again (46).
Now, letting r = 202v ≥ U ≥ 1 We can bound (55c) as follows
Vlog2 (1+ 2θ2v)=爵 I log2 (1 + r)	(57a)
≤ 蠢 supr≥1 1 log2 (1 + r) ≤ 20020∙64∙	(57b)
□
Theorem 2 follows by Proposition A.2 and Proposition A.1 with the choice X = X .
16
Published as a conference paper at ICLR 2022
B Discussion
In this section we discuss further connections to related work and some open questions.
Cremer et al. (2018) showed experimentally that using a more expressive class of models for the
encoder reduces not only the posterior family mismatch but also the amortization error. This obser-
vation is compatible with our results: increasing the expressive power of the encoder admits tight
VAEs with more complex dependence of z on x. Indeed, increasing the expressive power of the
encoder in our setting means extending its sufficient statistics ψ(z) by new components. While it
keeps the simple linear dependence g(x) = WTν(x) characterizing tight VAEs, it does lead to a
more expressive GLM p(z| x). Conversely, our results suggest that increasing the complexity of the
encoder network in order to reduce the amortization gap is only useful for models that are far from
the consistent set. Furthermore, there could be negative impact from increasing the model depth in
practice: (Dai et al., 2020) demonstrated that the risk of the learning converging to a suboptimal
solution (in particular leading to more collapsed latent dimensions) increases with decoder depth.
Lucas et al. (2019) showed that any spurious local minima in linear Gaussian VAEs are entirely due
to the marginal log likelihood and that the ELBO does not introduce any new local minima. It is
a good question5 whether something similar can be said about the EF VAE generalization. To our
best knowledge this is not straightforward in such a general setting. The result of Lucas et al. (2019)
is possible thanks to the fact that for linear Gaussian models ELBO is analytically tractable and
its stationary point conditions can be written down and analyzed. In the general EF setup, which
includes, e.g., MRF VAEs, this does not appear possible. On the other hand, for any consistent
EF VAE, the decoder posterior must be in the EF of the encoder. Therefore the optimal encoder
could be easier to find analytically or numerically using forward KL divergence and not the reverse
KL divergence used in ELBO, thus circumventing the question about local optima of ELBO. Since
ELBO at the optimal encoder is tight for a consistent VAE, this could be an alternative way to find
the global maximum.
A recent work by Sicks et al. (2021) extends the result of Lucas Lucas et al. (2019) in that they
develop an analytical local approximation to ELBO, which is exact in the Gaussian linear model
case and is a lower bound on ELBO for Binomial observation model. These results allow to ana-
lyze ELBO (and in particular the posterior collapse problem) locally under the assumption that the
decoder’s mapping f(z) is (locally) an affine mapping of z. Our Theorem 1 implies it must be so
globally for tight VAEs in several special cases (e.g., Bernoulli model), while in general itis an affine
mapping of ψ(z). These connections indicate that a better understanding of VAEs can be reached in
the setting where either decoder or encoder or both are consistent with the joint model (7).
We restricted this study to exponential families. While in richer models discussed in the introduction,
the approximation error still exists, it is made small by design, and it is less relevant and harder to
analyze it theoretically. One possible open direction where such analysis would make sense is to
consider fully factorized non-exponential cases, e.g., Student-t VAEs (Takahashi et al., 2018) or
models satisfying hierarchical or partial factorization (Maal0 e et al., 2019).
C Details of Experimental Setup
C.1 Artificial Example
We give here the achieved likelihood and ELBO values for this experiment. The negative entropy
Px p* (x) logp* (x) of the ground truth model, i.e., the best reachable data log-likelihood, is -3.65.
The ELBO of the pre-trained VAE is -3.74. During the second step of training, i.e., the joint
learning of the encoder and decoder by ELBO maximization, the ELBO value increases from -3.74
to -3.70. The data log-likelihood Px p*(x) logpθ (x) drops atthe same time from -3.65 to -3.68.
The approximation error (2) in the decoder caused by using the factorized encoder is only 0.03 nats,
but the qualitative difference between the ground truth model and the ELBO optimizer model shown
in Fig. 2 appears detrimental.
5pointed out by reviewers
17
Published as a conference paper at ICLR 2022
C.2 Bernoulli VAE for Text Documents
Dataset In this experiment we used the version of the 20Newsgroups data set (Lang & Rennie,
2008) denoted as “processed” by the authors. The dataset contains bag-of-words representations of
documents and is split into a training set with 11269 documents and a test set with 7505 documents.
We keep only the 10000 most frequent words in the training set, which is a common pre-processing
(each of the omitted words occurs not more than in 10 documents).
Optimization To train VAE we used the state-of-the-art unbiased gradient estimator ARM (Yin &
Zhou, 2019) and Adam optimizer with learning rate 0.001. We did not use the test set for parameter
selection. We train for 1000 epochs using 1-sample ARM and then for 500 more epochs using
10 samples for computing each gradient estimate with ARM. We report the lowest negative ELBO
(NELBO) values for the training set and test set during all epochs.
Models The decoder model (13) describes words as independent draws from a categorical distri-
bution specified by the neural network f(z). This network respectively has a structure
Linear → (ReLU → Linear) → Logsoftmax.
'------------------------------------------}
z
×dhidden
The input dimension equals to the number of latent bits, the output dimension equals the number of
words in the dictionary, 10000. For decoders with dhidden = 1, 2 the hidden layers contained 512
units.
The encoder networks e2, e3 take on the input word frequencies x/ Pk xk, the encoder network e1
takes on input word counts x. For the deep encoder e2 we used 2 hidden ReLU layers with 512 units
each.
18