Published as a conference paper at ICLR 2022
IntSGD: Adaptive Floatless
Compression of
Stochastic Gradients
Konstantin Mishchenko
CNRS, Ecole Normale SUPerieUre,Inria
konsta.mish@gmail.com
Dmitry Kovalev
KAUST
dakovalev1@gmail.com
Bokun Wang
KAUST*
bokunw.wang@gmail.com
Peter Richtarik
KAUST
peter.richtarik@kaust.edu.sa
Ab stract
We ProPose a family of adaPtive integer comPression oPerators for distribUted
Stochastic Gradient Descent (SGD) that do not commUnicate a single float. This
is achieved by mUltiPlying floating-Point vectors with a nUmber known to every
device and then roUnding to integers. In contrast to the Prior work on integer
comPression for SwitchML by SaPio et al. (2021), oUr IntSGD method is Prov-
ably convergent and comPUtationally cheaPer as it estimates the scaling of vectors
adaPtively. OUr theory shows that the iteration comPlexity of IntSGD matches that
of SGD UP to constant factors for both convex and non-convex, smooth and non-
smooth fUnctions, with and withoUt overParameterization. Moreover, oUr algo-
rithm can also be tailored for the PoPUlar all-redUce Primitive and shows Promising
emPirical Performance.
1 Introduction
Many recent breakthroUghs in machine learning were made Possible dUe to the introdUction of large,
soPhisticated and high caPacity sUPervised models whose training reqUires days or even weeks of
comPUtation (Hinton et al., 2015; He et al., 2016; HUang et al., 2017; Devlin et al., 2018). However,
it woUld not be Possible to train them withoUt corresPonding advances in Parallel and distribUted
algorithms caPable of taking advantage of modern hardware. Very large models are tyPically trained
on vast collections of training data stored in a distribUted fashion across a nUmber of comPUte nodes
that need to commUnicate throUghoUt the training Process. In this scenario, reliance on efficient
commUnication Protocols is of Utmost imPortance.
Communication in distributed systems. The training Process of large models relies on fast syn-
chronization of gradients comPUted in a Parallel fashion. Formally, to train a model, we want to
solve the Problem of Parallel/distribUted minimization of the average of n fUnctions:
f(x) d=f 1 P fi(x) ,	fi(x) d=f Eξ[fi(x; ξ)],	(1)
i=1
where we will comPUte the gradients of stochastic realizations fi (x; ξ). The two dominating Pro-
tocols for gradient synchronization are all-reduce and all-gather aggregation, which may Use either
Parameter Server or all-to-all commUnication Under the hood. The core difference between them
lies in that all-gather commUnicates all vectors, whereas all-redUce only oUtPUts their average. As
shown in PrevioUs works, cUrrent distribUted deeP learning algorithms Predominantly Use all-redUce
as it scales mUch better than all-gather (Vogels et al., 2019; Agarwal et al., 2021).
A PoPUlar way to redUce the commUnication cost of both all-redUce and all-gather Primitives is to
Use lossy comPression of gradients (Ramesh et al., 2021). To stUdy the benefit of lossy comPression,
large swaths of recent literatUre on distribUted training attribUte the cost of sending a single vector
* Work done when the author was a research intern at KAUST.
min
x∈Rd
1
Published as a conference paper at ICLR 2022
from a worker to the server to the number of bits needed to represent it. Based on this abstraction,
various elaborate vector compression techniques (see Table 1 in Beznosikov et al. 2020; Xu et al.
2020; Safaryan et al. 2020) and algorithms have been designed for higher and higher compression
ratios. However, in real systems, the efficiency of sending a vector is not fully characterized by the
number of bits alone, because:
•	First, many compressors with high compression ratio (e.g., natural compression (Horvgth
et al., 2019), quantization (Alistarh et al., 2017), top-k sparsification, sign (Bernstein et al.,
2018)) are not compatible with the efficient all-reduce primitive and require all-gather im-
plementation.
•	Secondly, some compressors rely on expensive operations such as low-rank decomposi-
tion (Wang et al., 2018; Vogels et al., 2019) or bit-level operations (Horvgth et al., 2019),
whose computation overhead may outweigh the benefits of reduced communication load.
•	Thirdly, algorithms with biased compressors such as Top-k SGD, SignSGD, PowerSGD
(Vogels et al., 2019), require the error-feedback (EF-SGD) mechanism (Stich et al., 2018;
Karimireddy et al., 2019) to ensure the convergence. Alas, error feedback needs extra
sequences that may not fit the low memory budget of GPUs. Moreover, to the best of our
knowledge, no convergence guarantee has been established for EF-SGD on the non-smooth
objectives with multiple workers.
SwitchML. Another approach to combating long communication times is to improve the hardware
itself. The recently proposed SwitchML is an alternative to the NVIDIA Collective Communications
Library (NCCL) on real-world hardware (Sapio et al., 2021). The first key component of SwitchML
is the in-network aggregation (INA) by a programmable switch. INA reduces the communication
cost and latency because the execution can be paralleled and pipelined. To be specific, it splits the
vector to aggregate into chunks and processes them individually by the switch pipeline. The advan-
tages of INA over parameter server and all-reduce in terms of latency and communication cost have
been theoretically and empirically justified by Sapio et al. (2021). The second key component of
SwitchML is stochastic gradient descent with integer rounding and aggregation. Instead of reducing
the data volume to exchange, the goal of integer rounding in SwitchML is to fit the limited com-
putation capability of the modern programmable switch, which only supports integer additions or
logic operations. To increase the rounding precision, the gradient gik on device i multiplies a positive
scaling factor αk known to every worker and then rounded to an integer number Int(αk ◦ gik). As
there is no additional scaling or decompression before aggregating the communicated vectors, their
sums can be computed on the fly. Then, each worker can divide the aggregated gradient by nαk to
update the model.
However, Sapio et al. (2021) remark that the choice of the scaling factor αk requires special care.
In their presentation1, one of the authors notes: “A bad choice of scaling factor can reduce the
performance.” To this end, they propose a heuristic-based profiling step that is executed before
the gradient aggregation and keeps the rounded integers small to fit in 32 bits. We refer to their
algorithm including the profiling step as Heuristic IntSGD. Unfortunately, no convergence guarantee
for that algorithm has been established. This is where our theory comes to the rescue. By rigorously
and exhaustively analyzing integer rounding based on scaling, we find adaptive rules for the scaling
factor αk that do not require the profiling employed by Sapio et al. (2021). As we will show in the
remainder of the paper, our algorithm is perfectly suited for both in-network aggregation (INA) of
SwitchML and for other efficient primitives such as all-reduce.
1.1	Contributions
We summarize the key differences of our algorithm and prior work in Table 1, and we also list our
main contributions below.
•	Adaptive IntSGD. We develop a family of computationally cheap adaptive scaling factors for
provably convergent IntSGD. It is a better alternative to the Heuristic IntSGD in Sapio et al. (2021)
that requires expensive operations and does not ensure convergence.
1https://youtu.be/gBPHFyBWVoM?t=606
2
Published as a conference paper at ICLR 2022
Table 1: Conceptual comparison of our method to the related literature. If all-reduce is supported,
the method does not need any decompression. If all-reduce is not supported, the expensive all-gather
operation is required and decompression is slow. See also Section 5 for numerical comparisons.
Algorithm	Supports all-reduce	Supports switch	Provably works	Fast compression	Works without error-feedback	Adaptive	Reference
IntSGD	✓	✓	✓	✓	✓	✓	Ours
Heuristic IntSGD	✓	✓	X	✓	✓	X	Sapio et al. (2021)
PowerSGD (theoretical)	✓	X	✓	X(1)	X	X(2)	Vogels et al. (2019)
PowerSGD (practical)	✓	X	X	✓(1)	X	X(2)	Vogels et al. (2019)
NatSGD	X	✓	✓	X	✓	N/A	Horvathet al. (2019)
QSGD	X	X	✓	✓	✓	N/A	Alistarh et al. (2017)
SignSGD	X	X	✓	✓	X	N/A	Karimireddy et al. (2019)
(1) In theory, PowerSGD requires computing low-rank decompositions. In practice, an approximation is found by power iteration, which
requires just a few matrix-vector multiplications but it is not analyzed theoretically and might be less stable.
(2) PowerSGD requires tuning the rank of the low-rank decomposition. Vogels et al. (2019) reported that rank-1 PowerSGD consistently
underperformed in their experiments, and, moreover, rank-2 was optimal for image classification while language modeling required
rank-4. Ramesh et al. (2021) reported that a much larger rank was needed to avoid a gap in the training loss.
•	Rates. We obtain the first analysis of the integer rounding and aggregation for distributed machine
learning. For all of the proposed variants, we prove convergence rates of IntSGD that match those of
full-precision SGD up to constant factors. Our results are tight and apply to both convex and non-
convex problems. Our analysis does not require any extra assumption compared to those typically
invoked for SGD. In contrast to other compression-based methods, IntSGD has the same rate as that
of full-precision SGD even on non-smooth problems.
•	IntDIANA. We observe empirically that IntSGD struggles when the devices have heterogeneous
(non-identical) data—an issue it shares with vanilla SGD—and propose an alternative method, IntDI-
ANA, that can provably alleviate this issue. We also show that our tools are useful for extending the
methodology beyond SGD methods, for example, to variance reduced methods (Johnson & Zhang,
2013; Allen-Zhu & Hazan, 2016; Kovalev et al., 2020; Gower et al., 2020) with integer rounding.
Please refer to Appendix A.2 for theoretical results and Appendix C.5 for the empirical verification.
2	Adaptive Integer Rounding and IntSGD
By randomized integer rounding we mean the mapping Int : R → Z defined by
Int(t) d=ef [t] + 1,
with probability pt d=ef t - [t],
with probability 1 - pt ,
where [t] denotes the floor of t ∈ R, i.e., [t] = k ∈ Z, where k is such that k ≤ t < k + 1. Note that
E [Int(t)] = (t - [t])([t] + 1) + ([t] + 1 - t)[t] = t.
We extend this mapping to vectors x ∈ Rd by applying in element-wise: Int(x)i d=ef Int(xi).
2.1	Adaptive integer rounding
Given a scaling vector α ∈ Rd with nonzero entries, we further define the adaptive integer rounding
operator Q : Rd → Rd by
Q(X) d=f 1 ◦ Int(α ◦ x),	(2)
where a ◦ b d=ef (a1b1, . . . , adbd) ∈ Rd denotes the Hadamard product of two vectors a =
(a1, . . . , ad) ∈ Rd and b = (b1, . . . , bd) ∈ Rd.
3
Published as a conference paper at ICLR 2022
Algorithm 1 IntSGD. Default setting for the tested problems: β = 0.9, ε = 10-8.
1:	Params: Stepsizes ηk, scaling vectors αk ∈ Rd
2:	Init: x0 ∈ Rd, x1 = x0 - no 1 Pn=ι g0
3:	for k = 1, 2, . . . do
4:	for each device i = 1, 2, . . . , n do
5:	Compute stochastic gradient gik (E gik | xk ∈ ∂fi(xk))
6:	Maintain the moving average: rk = βrk-ι + (1 - β)∣∣xk - XkTk2 * *
7:	Compute the adaptive scaling factor: α% = / VZd -
2nrk /ηk2 +ε2
8:	Scale and round the local gradient Q(gik) = Int(αk ◦ gik)
9:	end for
10:	Aggregate Q(gik) by either all-reduce or in-network aggregation (INA)
11:	for each device i = 1, 2, . . . , n do
12:	Compute the (sub)gradient estimator: gk = ^^ pn=ι Q(gk)
13:	Update the model parameter χk+1 = χk - ηkgk
14:	end for
15:	end for
As we show below, the adaptive integer rounding operator (2) has several properties which will be
useful in our analysis. In particular, the operator is unbiased, and its variance can be controlled by
choice of a possibly random scaling vector α ∈ Rd++.
Lemma 1. For any x ∈ Rd and α ∈ Rd++, we have
α ◦ E [Int(α ◦ x)] = x,
E U1 ◦Int(α ◦X)-X KjPt4⅛,
(3)
(4)
The expectations above are taken with respect to the randomness inherent in the rounding operator.
2.2 New algorithm: IntSGD
We are ready to present our algorithm, IntSGD. At iteration k, each device i computes a stochastic
(sub)gradient vector gik, i.e., a vector satisfying
E gik | Xk ∈ ∂fi(Xk).	(5)
Prior to communication, each worker i rescales its stochastic (sub)gradients gik using the same vector
αk ∈ Rd++, and applies the randomized rounding operator Int. The resulting vectors Int(αk ◦ gik)
are aggregated to obtain Pin=1 Int(αk ◦ gik), which is also an integer. Each device subsequently
performs division by n and inverse scaling to decode the message, obtaining the vector
nn	n
gk =f n⅛ ◦ P Int(αk ◦ gk = 1 P Ok ◦Int(αk ◦ gk) = n P Q(gk).
i=1	i=1	i=1
Here αk is a random adaptive scaling factor calculated based on the historical information. We
left the design of αk to Section 4. By combining (5) and (3), we observe that gk is a stochastic
(sub)gradient of f at Xk . Finally, all devices perform in parallel an SGD-type step of the form
χk+1 = χk - ηkgk and the process is repeated. Our IntSGD method is formally stated as Algorithm 1
with the suggested rule of α.
Relation to QSGD (Alistarh et al., 2017). QSGD bears some similarity to the IntSGD: Both of them
scale gik by a factor before the quantization (the scaling factor in QSGD is 1/kgik k for normalization).
However, some key difference makes the communication efficiency of IntSGD much better than that
of QSGD. Itis worth noting that the normalization factors 1/kgikk in QSGD are different for various
workers. Then, the quantized values of various workers need to be gathered and decompressed
before aggregation. On the contrary, the scaling factor αk in our IntSGD is the same for all workers
such that the sum of integers can be computed on the fly. Thus, IntSGD supports the efficient all-
reduce primitive while QSGD does not. As seen in the experimental results in Section 5 , this
4
Published as a conference paper at ICLR 2022
makes a big difference in empirical performance. Moreover, the proof technique for IntSGD is also
intrinsically different from that of QSGD. Please see the next section for the details.
3	Analysis of IntS GD2
To establish convergence of IntSGD, we introduce the following assumption on the scaling vector
αk = (αk,1, . . . , αk,d) ∈ R++.
d
Assumption 1. There exists β ∈ [0, 1) and a sufficiently small ε > 0 such that P E
j=1
k-1
bounded above by ηk2ε2 + 2n(1 - β) P βtE kxk-t - xk-t-1 k2 .
t=0
[α2j]
is
While this assumption may look exotic, it captures precisely what we need to establish the con-
vergence of IntSGD, and it holds for several practical choices of αk, including the one shown in
Section 4 and more choices in Appendix A.1.
Challenges in IntSGD analysis. Although the Int operation is unbiased and has finite variance
as shown in Lemma 1, we highlight that it is non-trivial to obtain the convergence analysis of IntSGD
and the analysis is different from that of QSGD and similar methods. Indeed, QSGD, Rank-k, and
NatSGD all use unbiased operators Q with variance satisfying E[kQ(g) - gk2] ≤ ωkgk2 for some
ω > 0. For them, the convergence theory is simply a plug-in of the analysis of Compressed SGD
(Khirirat et al., 2018). However, the integer rounding operator Int does not satisfy this property,
and the variance of the integer compressor will not decrease to zero when kgk2 → 0. Moreover, to
estimate αk adaptively, we use the values of past iterates, which makes its value itself random, so
the analysis of Compressed SGD cannot apply. As shown in our proofs, an extra trick is required:
we reserve an additional term Ptk=0 E[kxt+1 - xt k2] to control the variance of the rounding. Fur-
thermore, the analysis of moving-average estimation is particularly challenging since the value of
αk is affected by all past model updates, starting from the very first iteration.
3.1	Non-smooth analysis: generic result
Let us now show that IntSGD works well even on non-smooth functions.
Assumption 2. Stochastic (sub)gradients g1k , . . . , gnk sampled at iteration k satisfy the inequalities
Iln PpEk[gk]∣∣2 ≤ g2,	n PP Ek Ijlgk - e®/]『]≤⑹
where the former inequality corresponds to G-Lipschitzness of f and the latter to bounded variance
of stochastic (sub)gradients.
Theorem 1.	Let functions f1, . . . , fn be convex and Assumptions 1 and 2 be satisfied. Then
Ef(Xk) - f(x*)] ≤
kχ0-χ*k2+2(G2+σn2+εn )pk=o η
2 Pk-I η
where Xk = Pk1——Pk=O ηtxt is a weighted average of iterates.
t=0 ηt	t=0
3.2 Smooth analysis: generic result
We now develop a theory for smooth objectives.
Assumption 3. There exist constants L,σ* ≥ 0 such that the stochastic gradients gk ,...,gkk at
iteration k satisfy Ek[gf] = V∕i(xk) and
Il 1Pgk∣] ≤L(f(χk) -f(x*))+σ2
(7)
2In our analysis, we use the red color to highlight the extra terms coming from our integer compression, in
contrast to the blue error terms, which come from SGD itself.
5
Published as a conference paper at ICLR 2022
Assumption 3 is known as the expected smoothness assumption (Gower et al., 2019). In its formu-
lation, We divide the constant term σ2 by n, which is justified by the following proposition.
Proposition 1 (Section 3.3 in Gower et al. 2019). Let fi(x) = Eξ[fi(x; ξ)], gk = Pfi(Xk ξk), and
fi(∙; ξ) be convex and its gradient be Li-LiPSchitz for any ξ. Then, the second part of Assumption 3
is satisfied with σ2 def 2 Pn=1Eξ [kVfi(x*; ξ)∣∣2] and L def 4maxi=ι,...,n Li.
Gower et al. (2019) state and prove this result in a more general form, so for the reader’s convenience,
we provide a proof in the appendix.
Theorem 2.	Assume that f is convex and Assumption 3 holds. If ηk ≤ 芸 and Xk =
Pk1——Pk=o ηtxt is a weighted average of iterates, then
t=0 ηt	t=0
E[f(Xk) — f(x*)] ≤
kχ0-χ*k2+2( σ2+εn) Pt=。η
2 Pt=0 ηt
Corollary 1 (Overparameterized regime). When the model is overparameterized (i.e., the losses can
be minimized to optimality simultaneously: σ* = 0), we can set ε = 0 and obtain O (ɪ) rate.
3.3 Non-convex analysis: generic result
We now develop a theory for non-convex objectives.
Assumption 4. The gradient of f is L-Lipschitz and there exists finf ∈ R such that finf ≤ f(X)
for all X. Furthermore, for all i and k we have
E [kgik -Vfi(Xk)k2 ≤σ2.	(8)
Our main result in the non-convex regime follows.
Theorem 3.	Let f be L-smooth and let Assumption 1 hold. If 加 ≤ 2L for all k, then
E [kVf(Xk)k2] ≤ 2"""-finf+, + F)Pk=0η2L.
t=0 ηt
where Xk is SamPledfrom {x0,...,xk } with probabilities proportional to ηo,...,ηk.
3.4 Explicit complexity results
Having developed generic complexity results for IntSGD in the non-smooth (Section 3.1), smooth
(Section 3.2) and non-convex (Section 3.3) regimes, we now derive explicit convergence rates.
Corollary 2. For any sequence of scaling vectors αk satisfying Assumption 1, we recover the fol-
lowing complexities:
(i)	if f1 , . . . , fn are convex, Assumption 2 is satisfied and ηt = η
t = 0, . . . , k, then
I∣χ0-χ*k
√k(G2+σ2∕n)
for
E[f(xk )-f(x*)]=O(√⅛ + √k);
(ii)	if f is convex, Assumption 3 holds and η = min { 2L, 口篙；；J ∣, then
E [f (χk) - f (χ*)]=o (√nε+j⅛:1)；
(iii)	if f is non-convex, Assumption 4 holds and ηt = min { 2L, √"√k[σ+fεfn 卜hen
E [kVf(xk)k2] = o (√±n + f(XcVinf).
(9)
Based on Corollary 2, our IntSGD has linear speed-up in case (ii) and (iii).
6
Published as a conference paper at ICLR 2022
Comparison with error-feedback (EF-SGD). Distributed SGD with biased compressors (like
PowerSGD, SignSGD, Top-k SGD) requires the error-feedback modification to converge. In the
non-convex and smooth case, EF-SGD leads to the O (√^ + (GG)2/3) rate in KoloskoVa et al.
(2019) when assuming the second moment of stochastic gradient is bounded by G2 . Compared to
their result, our rate is never worse and does not require the second moment of stochastic gradi-
ent to be bounded, which is often violated in practice even for quadratic objectives and simplest
neural networks. The convergence guarantee of EF-SGD for the convex and non-smooth function
(Assumption 2) is even weaker: Karimireddy et al. (2019) show the O (√^) convergence rate of
EF-SGD only for the single-worker case (n = 1), which is %-times worse than IntSGD (δ could
be fairly small, e.g., δ = 1/d in Top-1 compression). To the best of our knowledge, there is no
convergence guarantee of EF-SGD for the non-smooth function when there are multiple workers. In
contrast, our IntSGD has the same rate as SGD under the same set of assumptions.
4 Design of S caling Factors
4.1	Adaptive scaling factor with the moving average and safeguard
We now present an effective rule of adaptive αk (presented in Algorithm 1) that satisfies Assump-
tion 1 for the convergence rates listed in previous section. In the appendix, we provide more options
that also satisfy Assumption 1 and the proof still goes through. For simplicity, we assume that the
first communication is exact, which allows us to estimate αk adaptively without worrying about α0 .
Proposition 2. Assumption 1 holds if We choose β ∈ [0,1),ε ≥ 0 and ɑk = -r=^===, where
2nrk /ηk2 +ε2
rk = βrk-1 + (1 - β)kxk - xk-1k2.
Remark 1. Here β ∈ [0, 1) is a constant factor to control the moving average update of rk, which
prevents the scaling factor αk from changing too rapidly. ε2 could be any sufficiently small number,
which serves as a safeguard to avoid the potential “divide by zero” error. We study the sensitivity of
our IntSGD to β and ε in Appendix C.4.
4.2	Compression efficiency
Let us now discuss the number of bits needed for the compressed vectors. Although the main
attraction of IntSGD is that it can perform efficient in-network communication, we may also hope to
gain from the smaller size of the updates.
Consider for simplicity the case where kxk - xk-1k ≈ kηkgik k with some i. The adaptive scheme
with β = 0, ε = 0 gives ak =√n∣X—Xk-1k ≈ √n∣ηk+1-xk k = √2⅛,sothat kakgk k∞ =
√√dn kggk∞ ≤ √√2n. Since we only use signed integers, we need at most 1 + log? √√∣ bits for each
coordinate. For instance, for d 〜 1010 and n 〜100, the upper bound is 1+log2 (√5 ∙ 105 * 7 * *) < 14
bits. The situation becomes even better when kgikk kgik k∞, i.e., when the stochastic gradients are
dense. This property has been observed in certain empirical evaluations for deep neural networks;
see for example the study in (Bernstein et al., 2018).
5 Experiments
5.1 Setup
We empirically compare our IntSGD algorithm with several representative and strong baselines:
SGD, Heuristic IntSGD (Sapio et al., 2021), SGD, PowerSGD + Error-feedback (EF) (Vogels et al.,
2019), NatSGD (HOrVgth et al., 2019), and QSGD (Alistarh et al., 2017). The experiments are
performed on 16 NVIDIA Tesla V100 GPUs located on 8 compute nodes of a cluster (2 GPUs per
node) following the PowerSGD paper. The compute nodes in the cluster utilize InfiniBand HDR-100
Director Switch at 100Gbps speed for network connection. The cluster also supports the NVIDIA
Collective Communications Library (NCCL).
7
Published as a conference paper at ICLR 2022
We consider two tasks: image classification by ResNet18 (He et al., 2016) on the CIFAR-10 dataset
and language modeling by a 3-layer LSTM on the Wikitext-2 dataset. The neural network architec-
tures and hyperparameters are from some public PyTorch implementations3. Our code is built on the
codebase of PowerSGD4. We also borrow their all-reduce-based implementations of SGD and Pow-
erSGD. It is worth noting that QSGD and NatSGD do not support all-reduce. Thus, we implement
their collective communications by all-gather. The implementations for compression and decom-
pression in QSGD and NatSGD are from the authors of NatSGD5. For the sake of comparison, we
also implement the all-gather-based SGD. We report the results of 3 repetitions with varying seeds.
Apart from the IntSGD with randomized integer rounding (IntSGD (Random)) analyzed in our the-
ory, we also consider the variant of IntSGD with deterministic integer rounding (IntSGD (Determ.))
which can use the PyTorch built-in function torch.round. For all IntSGD variants, we clip the
local stochastic gradients to ensure that each aggregated value fits in either 8 bits or 32 bits.
For more details of the experimental setup, please refer to Appendix C.1.
5.2	IntSGD vs. Heuristic IntSGD
First, we compare our IntSGD with the most related algorithm Heuristic IntSGD (Sapio et al., 2021).
For both algorithms, we consider two potential communication data types: int8 and int32. Note
that the rule of scaling factor in Heuristic IntSGD is ɑ = :募-工,where “nb” represents the number
of bits to encode each coordinate and “max_exp” is the rounded exponent of the largest absolute
value in the communicated package. Although this scaling rule is straightforward and avoids over-
flows, it cannot guarantee convergence, even with int32 as the communication data type. Indeed,
the Heuristic IntSGD may fail to match the testing performance of full-precision SGD according to
Figure 1. On the contrary, our IntSGD can perfectly match the performance of full-precision SGD
on both image classification and language modeling tasks, which is in accordance with our theory
that IntSGD is provably convergent.
Epochs
Figure 1: Comparison among IntSGD (8-bit or 32-bit), Heuristic IntSGD (8-bit or 32-bit), and full-
precision SGD on the tasks of training ResNet18 and LSTM.
5.3	IntSGD vs. other baselines
We also compare our IntSGD algorithms to the other baselines including the all-gather-based SGD,
QSGD, NatSGD and the all-reduce-based SGD, PowerSGD (EF) on the two tasks. See the test
performance and time breakdown in Table 2 and Table 3.
First, we can observe that the all-gather based SGD + compressors (e.g., QSGD, NatSGD) are indeed
faster than the all-gather based full-precision SGD, which shows the benefit of lossy compressions.
However, they are even much slower than the all-reduce-based full-precision SGD. Unfortunately,
QSGD and NatSGD) does not support the more efficient all-reduce primitive. Similar observation
can be seen in previous works (Vogels et al., 2019; Agarwal et al., 2021).
All of PowerSGD (EF), IntSGD (Random), and IntSGD (Determ.) are consistently faster than the all-
reduce-based full-precision SGD on both tasks. Compared to IntSGD (Determ.), IntSGD (Random)
leads to slightly more computation overhead due to the randomized rounding. However, IntSGD
3ResNet18: https://github.com/kuangliu/pytorch-cifar; LSTM: https://github.
com/pytorch/examples/tree/master/word_language_model
4https://github.com/epfml/powersgd
5https://github.com/sands-lab/grace
8
Published as a conference paper at ICLR 2022
Table 2: Test accuracy and time breakdown in one iteration (on average) of training ResNet18 on the CIFAR-10 dataset with 16 workers. All numbers of time are in millisecond (ms). In each column,				
the best one is highlighted in black and the second-best one is highlighted in gray.				
Algorithm	Test Accuracy (%)	Computation Overhead	Communication	Total Time
SGD (All-gather)	94.65 ± 0.08	-	261.29 ± 0.98	338.76 ± 0.76
QSGD	93.69 ± 0.03	129.25 ± 1.58	138.16 ± 1.29	320.49 ± 2.11
NatSGD	94.57 ± 0.13	36.01± 1.30	106.27 ± 1.43	197.18 ± 0.25
SGD (All-reduce)	94.67 ± 0.17	-	18.48 ± 0.09	74.32 ± 0.06
PowerSGD (EF)	94.33 ± 0.15	7.07 ± 0.03	5.03 ± 0.07	67.08 ± 0.06
IntSGD (Determ.)	94.43 ± 0.12	2.51 ± 0.04	6.92± 0.07	64.95 ± 0.15
IntSGD (Random)	94.55 ± 0.13	3.20 ± 0.02	6.21 ± 0.13	65.22 ± 0.08
(Determ.) fails to match the testing performance of SGD on the language modeling task. Compared
to PowerSGD (EF), our IntSGD variants are better on the task of training ResNet18 but inferior on
the task of training a 3-layer LSTM. Although IntSGD is not always better than PowerSGD (EF),
there are several scenarios where IntSGD is preferrable as explained in in Section 1 and Section 3.4.
In addition, as seen in Figure 3 of the Appendix C.3, PowerSGD (EF) converges much slower than
SGD and IntSGD in the first 150 epochs of the ResNet training (which has non-smooth activations).
Table 3: Test loss and time breakdown in one iteration (on average) of training a 3-layer LSTM
on the Wiki-text2 dataset with 16 workers. All numbers of time are in millisecond (ms). In each
column, the best one is highlighted in black and the second-best one is highlighted in gray.
Algorithm	Test Loss	Computation Overhead	Communication	Total Time
SGD (All-gather)	4.52 ± 0.01	-	733.07 ± 1.04	796.23 ± 1.03
QSGD	4.63 ± 0.01	43.67 ± 0.11	307.63 ± 1.16	399.10 ± 1.25
NatSGD	4.52 ± 0.01	64.63 ± 0.12	309.87 ± 1.32	422.49 ± 2.15
SGD (All-reduce)	4.54 ± 0.03	-	22.33 ± 0.02	70.46 ± 0.05
PowerSGD (EF)	4.52 ± 0.01	4.22 ± 0.01	2.10 ± 0.01	54.89 ± 0.02
IntSGD (Determ.)	4.70 ± 0.02	3.04 ± 0.01	6.94 ± 0.05	57.93 ± 0.03
IntSGD (Random)	4.54 ± 0.01	4.76 ± 0.01	7.14 ± 0.04	59.99 ± 0.01
6 Conclusion
In this paper, we propose the provably convergent and computationally cheap IntSGD algorithm for
efficient distributed machine learning. The core component of IntSGD is the adaptively estimated
scaling factor shared by all users, which makes it compatible with the widely used communication
primitive all-reduce and the recently proposed in-network aggregation (INA) (Sapio et al., 2021).
The convergence rates of IntSGD match that of SGD up to constant factors on a broad spectrum
of problems. Experimental results on two deep learning tasks show its promising empirical perfor-
mance. A limitation of our algorithm is that its compression ratio is bounded by 4, but we hope to
address this in a future work.
9
Published as a conference paper at ICLR 2022
Reproducibility statement. Regarding the theoretical results: We describe the mathematical set-
ting and algorithms in Section 1, 2, and Appendix A; Assumptions and the main theoretical results
are presented in Section 3; We provide the complete proof for those results in Appendix B. Regard-
ing the experimental results: We report the number of repetitions, the computing infrastructure used,
the range of hyper-parameters considered, and the evaluation metrics in Section 5 and Appendix C.1;
We attach our code in the supplementary material.
References
Saurabh Agarwal, Hongyi Wang, Shivaram Venkataraman, and Dimitris Papailiopoulos. On the
utility of gradient compression in distributed training systems. arXiv preprint arXiv:2103.00543,
2021.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural
Information Processing Systems,pp. 1709-1720, 2017.
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In The
33th International Conference on Machine Learning, pp. 699-707, 2016.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
SignSGD: Compressed optimisation for non-convex problems. In Jennifer Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80
of Proceedings of Machine Learning Research, pp. 560-569, Stockholmsmassan, Stockholm
Sweden, 10-15 Jul 2018. PMLR.
Aleksandr Beznosikov, Samuel Horvdth, Peter Rich电ik, and Mher Safaryan. On biased ComPres-
sion for distributed learning. arXiv:2002.12410, 2020.
Lisandro Dalcin, Rodrigo Paz, and Mario Storti. MPI for Python. Journal OfParallel and Distributed
Computing, 65(9):1108-1115, 2005.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Robert M. Gower, Mark Schmidt, Francis Bach, and Peter Richtdrik. Variance-reduced methods for
machine learning. Proceedings of the IEEE, 108(11):1968-1983, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtdrik. SGD: General analysis and improved rates. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 5200-5209, Long Beach, Califor-
nia, USA, 09-15 Jun 2019. PMLR.
Priya Goyal, Piotr Dolldr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Samuel Horvdth, Chen-Yu Ho, LUdOVit Horvdth, Atal Narayan Sahu, Marco Canini, and Peter
Richtdrik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988,
2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
10
Published as a conference paper at ICLR 2022
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. Advances in neural information processing Systems, 26:315-323, 2013.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback
fixes SignSGD and other gradient compression schemes. In International Conference on Machine
Learning, pp. 3252-3261, 2019.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with com-
pressed gradients. arXiv preprint arXiv:1806.06573, 2018.
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. Decentralized deep learning
with arbitrary communication compression. arXiv preprint arXiv:1907.09356, 2019.
Dmitry Kovalev, Samuel Horvdth, and Peter Richtdrik. Don'tjump through hoops and remove those
loops: SVRG and Katyusha are better without the outer loop. In Algorithmic Learning Theory,
pp. 451-467. PMLR, 2020.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the 38th International
Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,
pp. 8821-8831. PMLR, 18-24 Jul 2021.
Mher Safaryan, Egor Shulgin, and Peter Richtdrik. Uncertainty principle for communication com-
pression in distributed and federated learning and the search for an optimal compressor. arXiv
preprint arXiv:2002.08958, 2020.
Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind
Krishnamurthy, Masoud Moshref, Dan R. K. Ports, and Peter Richtdrik. Scaling distributed ma-
chine learning with in-network aggregation. To appear in 18th USENIX Symposium on Networked
Systems Design and Implementation, 2021.
Sebastian U. Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Advances in Neural Information Processing Systems, pp. 4447-4458, 2018.
Thijs Vogels, Sai Praneeth Karinireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient
compression for distributed optimization. Advances In Neural Information Processing Systems
32 (Nips 2019), 32(CONF), 2019.
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen
Wright. ATOMO: Communication-efficient learning via atomic sparsification. Advances in Neu-
ral Information Processing Systems, 31:9850-9861, 2018.
Hang Xu, Chen-Yu Ho, Ahmed M. Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos
Karatsenidis, Marco Canini, and Panos Kalnis. Compressed communication for distributed deep
learning: Survey and quantitative evaluation. Technical report, 2020.
11
Published as a conference paper at ICLR 2022
Appendix
A Other Variants of IntSGD
A.1 OTHER CHOICES OF SCALING FACTOR αk
In Section 4.1, we provide an effective scaling factor with the moving average and the safeguard.
However, there are other choices of scaling factor that also satisfy Assumption 1, and the conver-
gence proof still goes through.
Proposition 3 (Adaptive αk). If we choose
αk
ηk Vd
√2nkxk-xk-1 k
then Assumption 1 holds with ε = 0 and β = 0.
One can also consider applying an integer quantization with individual values of αt for each co-
ordinate or block, for instance, with an αt,l corresponding to the l-th layer in a neural network. It
is straightforward to see that this modification leads to the error PB1 di *,where B is the total
=1 αk,l
number of blocks and di is the dimension of the l-th block.
Proposition 4 (Adaptive block αk). Assume we are given a partition of all coordinates into B ≤ d
blocks with dimensions d1, . . . , dB, and denote by (xk)i the l-th block of coordinates of xk. Then
Assumption 1 holds with
αk,(l) = √2nk(xηk√dxk-1)lk,for l = 1,...,B.
There are two extreme cases in terms of how we can choose the blocks. One extreme is to set
B = 1, in which case we have a single scalar for the whole vector. The other extreme is to use
B = d, which means that αk = 2√n∣xη-χk-11 , where the division and absolute values are computed
coordinate-wise.
Algorithm 2 IntSGD: adaptive block quantization
1:	Input: x0 ∈ Rd, β ∈ [0,1), ε ≥ 0, x1 = x0 - no n Pi==r g0 a partitioning of Rd into B blocks
of sizes di, ...,dβ such that Rd = Rd1 ×∙∙∙× RdB
2:	for k = 1, 2, . . . do
3:	for each device i = 1, 2, . . . , n do
4:	Compute independent stochastic gradients gik (Ek [gik] ∈ ∂fi(xk))
5:	Maintain the exponetial moving average: rk,i = βrk-1,i + (1 - β)k(xk)i - (xk-1)i k2 {for
each block l = 1,...,B}
6:	Compute the adaptive scaling factors: αk,ι = / ηk VZdl d
√2nrk,ι+η2 diε ε2
7:	Scale and round the local gradient (Q(gik))i = Int(αk,i (gik)i)
8:	end for
9:	Aggregate Q(gik) by either all-reduce or in-network aggregation (INA)
10:	for each device i = 1, 2, . . . , n do
11:	Compute the (sub)gradient estimator: (gk)i = n^ Pn=I(Q(gk))i
12:	xk+1 = Xk - nkgk	，
13:	end for
14:	end for
Compression efficiency of IntSGD with adaptive block quantization. Our block-wise and
coordinate-wise compression can further benefit from reduced dimension factors in the upper
bounds, leading to the estimate of log2 2√dl bits for block with dimension di. However, for smaller
blocks it is less likely to happen that k(xk)i - (xk-1)i k ≈ kηk(gik)i k, so the estimate should be taken
12
Published as a conference paper at ICLR 2022
Algorithm 3 IntDIANA
1:
2:
3:
4:
5:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
6
Params: Stepsizes ηk, scaling vectors αk ∈ Rd
Init: x0 ∈ Rd, x1 = x0 - no 1 Pin=1 g0, h1 = 0, h1 = 0
for k = 1, 2, . . . do
for each device i = 1, 2, . . . , n do
Compute stochastic gradient gik (E gik | xk ∈ ∂fi(xk)).
Compute the adaptive scaling factor: α* = ^^黑，一∣∣
Scale and round the local gradient Q(gik) = Int(αk ◦ (gik - hik))
Update the local shift hik+1 = hik + Q(gik)
end for
Aggregate Q(gik) by either all-reduce or in-network aggregation (INA)
for each device i = 1, 2, . . . , n do
Compute the (sub)gradient estimator: gk = hk + η― £二〔 Q(gf)
Update the model parameter χk+1 = χk - ηkgk
Update global shift hk+1 = hk+ n⅛ Pn=ι Q(gk)
end for
end for
with a grain of salt. We hypothesize that using ε as in Proposition 2 is required to make block com-
pression robust. Notice that if stochastic gradients have bounded coordinates, i.e., kgik k∞ ≤ G∞
for all i, k, then We would need at most 1 + log? vG∞∞ bits to encode the integers. Since any
ε ≤ σ + √nG does not change the rate in the non-smooth case (see Equation (9)), we get for free
the upper bound of 1 + log2 √dGG∞ bits.
A.2 Handling heterogeneous data
IntSGD can be equipped with the full gradient or variance-reduced gradient estimator to enjoy faster
convergence than O (√⅛) shown in Corollary 2. For example, if we plug σ* = 0 (no variance) and
ε ≤ √n (sufficiently small safeguard) into item 2 of Corollary 2, the convergence rate OfIntSGD
is O (ɪ). However, when the data are heterogeneous (i.e., minimizing f (x) = ɪ £二 fi(x) will
not make ∣∣Vfi(x*)k = 0, ∀i ∈ [n]), the transmitted integer of IntSGD with σ* = 0, ε = 0 can
be gigantically large, which leads to very inefficient communications or even exception value error.
E.g., ifwe choose the adaptive αk and the full gradient gik = Vfi(xk), the largest integer to transmit
from worker i to the master is ∣∣αk Vfi(Xk )∣∞ ≈ ]f(χJ1∞, where the denominator is 0 while the
numerator is nonzero as the iterate converges to the optimum. To alleviate this issue, one needs to
compress gradient differences as is done for example by Mishchenko et al. (2019) in their DIANA
method. By marrying IntSGD with the DIANA trick, we obtain IntDIANA (Algorithm 3).
For IntDIANA with adaptive αk, the largest transmitted integer from worker to the master is ∣αk(gik -
hk)k∞ ≈ kgk-hk-∞∣. We will show that both the nominator and the denominator are infinitesimal
when xk converges to the optimum, such that the issue mentioned above can hopefully be solved.
Note that we can either use the full gradient gik = Vfi(xk) or the L-SVRG estimator Kovalev et al.
(2020)
gik = Vfi(xk; ξik) - Vfi(wik; ξik) + E Vfi(wik; ξ)
on the i-th worker. For the variance-reduced method, we further assume that fi has a finite-sum
structure, i.e.,
m
fi(x) = ml P fii(x)
l=1
such that Vfi(x; ξ) = Vfil(x), E[Vfi(x; ξ)]=2 Pm=I VfiU(x) and l is sampled from [m]
uniformly at random by ξ .
Our main convergence theorem describing the behavior of IntDIANA follows:
13
Published as a conference paper at ICLR 2022
Theorem 4. Assume that f is μ-strongly convex (μ ≥ 0) and f (∙; ξ) has Li-Lipschitz gradient for
any ξ, L d=ef 4 maxi Li.
1. If μ > 0,the iterates of IntDIANA with adaptive ak
n，d
√n∣∣xk —xk-1 k
satisfy
E Ψk ≤ θkΨ0.
•	For IntDIANA with the GD estimator gk = Pfi(Xk), we have θ =f max {1 一 ημ, 3} <
1 and Ψk d=f kxk — x*∣∣2 + ∣∣xk — XkTk2 + nL P ∣∣hk — Vfi(x*)k2, where
i=1
ηk=η ≤ 2(L+⅛) ；
•	For IntDIANA with the L-SVRG estimator, we have θ = max {1 — ημ, 4, 1 —急} < 1
and^Ψk =f ∣χk — χ*k2 + ∣χk -XkTk2 + 鬻 £乙 £陶 ∣Vfiι(Wk) — Vfil(X*)∣∣2 +
弹 Pn=I khk -Vfi(X*)k2, whereP = m,andηk = η ≤ 2E+2£加).
2.	If μ = 0, the iterates of IntDIANA with adaptive ak =佝,-Xk-Ik satisfy
E [f(Xk) - ft)] ≤ ηc½),
where Xk = ɪ Pk=0 Xi.
•	IntDIANA with the GD estimator requires that ηk = η ≤ —「1 L、,
4(L+ 32n)
•	IntDIANA with the L-SVRG estimator requires that ηk = η ≤ 4(L+2L∕n).
The above theorem establishes linear convergence of two versions of IntDIANA in the strongly con-
vex regime and sublinear convergence in the convex regime.
Compression efficiency of IntDIANA. If μ > 0, for IntDIANA with adaptive αk and either GD
or L-SVRG estimator, both ∣∣hk — Vfi(X*)∣2 and ∣∣Xk — XkTk2 converge to 0 linearly at the same
rate, while gk → Vfi(x*). Thus, the largest integer to transmit is ∣∣αk(gk — hk)k∞ ≈
is hopefully upper bounded.
kgk-hkk∞
∣∣xk-xk-1 k
B Proofs
B.1	Proofs for IntSGD
In the section, we provide the complete convergence proof of IntSGD.
B.1.1	Proofs for Lemma 1
Proof. Take y = α ◦ X and let py = y — [y], where [y] is the coordinate-wise floor, and py is the
vector of probabilities in the definition of Int(y). By definition it holds
E [Int(y)] =py([y] + 1) + (1 —py)[y] =py + [y] = y — [y] + [y] = y.
Plugging back y = α ◦ X, we obtain the first claim.
Similarly,
∣y — I nt(y)∣∞ = max yj — I nt(yj) ≤ max max z — [z], [z] + 1 — z = 1.
After substituting y = α ◦ X, it remains to mention
◦ 01nt(α 0 x) — X
α
≤ ∣Int(α 0 χ) — α 0 x∣∞ max ɪ
∞	j =1,...,d αj
14
Published as a conference paper at ICLR 2022
To obtain the last fact, notice that Int(y) - [y] is a vector of Bernoulli random variables. Since the
variance of any Bernoulli variable is bounded by 1, we have
E
1	2 d 1	d 1
α◦Int(α◦X)-x∣∣	=ΣαE[(Znt(yj)-yj) ] ≤∑4α2.
j=1 j	j=1 j
□
The staring point of our analysis is the following recursion. Let Pk =f ∣∣xk - x*k2, δk =f f (Xk) -
f(x*)and Zk =f k焉 P乙 gkk2.
Lemma 2. Assume that either i) functions f1 , . . . , fn are convex, or ii) f is convex and f1 , . . . , fn
are differentiable. Then
Ek Pk+1 ≤Pk -2ηkδk+Ak+Bk,
where Ak C=f 2流Ek [Zk] and Bk =f ɪ Pj=I Ok~ -∣∣χk+1 - χk ∣∣2 are the SGD and quantization
error terms, respectively.
B.1.2 Proof of Lemma 2
Proof. The last term in the expression that we want to prove is needed to be later used to compensate
quantization errors. For this reason, let us save one ∣Xk+1 -Xk ∣2 for later when expanding ∣Xk+1 -
x*k2. Consider the IntSGD step xk+1 - Xk = ηk1 Pn=I Q(gk), where Q(gk) = 01- ◦I nt(αk ◦ gk).
∣xk+1 — x*∣2 = IlXk — x*∣2 + 2(xk+1 — xk ,xk — x*i + ∣∣xk+1 — xk k2
IlXk — x*∣∣2 + 2(xk+1 — xk,xk — X*〉+ 2∣∣xk+1 — xkk2 - ∣xk+1 — xk∣2
n n
∣Xk - X*k2 - 2η XhQ(Ii), Xk - X*〉+2忧-X Q(gk)
n i=1	n i=1
2
- IXk+1 - XkI2.
(10)
Now let us forget about the first and last terms, which will directly go into the final bound, and
work with the other terms. By our assumptions, we either have Ek[Q(gik)] = Ek[gik] ∈ ∂fi (Xk) or
1 PNIEk [gk] = ▽/(Xk), so We obtain by convexity
n
Ek -2ηk XhQ(gk),Xk
n i=1
- X*〉
n
=-2ηk XhEk[gk],Xk - X*i ≤ -2ηk(f (Xk) - f (X*)).
n i=1
Moreover, using the tower property of expectation, we can decompose the penultimate term in (10)
as follows:
Ek
n
i=1
Ek
n	2
—X EQ[Q(gk)]	+ EQ
n
i=1
n	2
-X(Q(gk) - EQ[Q(gk)])
i=1
Ek
n
XEk [IQ(gik)-gikI2],
i=1
2
where in the last step we also used independence of the quantization errors (Q(g1k ) -
g1k),...,(Q(gnk) -gnk).
Next, we are going to deal with the quantization terms:
nn
XEk[IQ(gik)-gikI2] =XEk
i=1	i=1
Ok ◦Int(αk ◦ gk )-T ]≤) XX 七
i=1 j=1 k,j
d
n X ɪ.
4 乙 α2 -
j=1 k,j
Dividing both sides by n2 and plugging it into (10), we obtain the desired decomposition into SGD
and quantization terms.	□
15
Published as a conference paper at ICLR 2022
We now show how to control the quantization error by choosing the scaling vector αk in accordance
with Assumption 1.
Lemma 3. If the assumptions of Lemma 2 hold together with Assumption 1, then
kk	k
E [ρk+1 ] ≤ ρ0 - 2 P ηtE [δt] + 2 P η2E [Zt] + εn P η2
t=0	t=0	t=1
B.1.3 Proof of Lemma 3
Proof. Firstly, let us recur the bound in Lemma 2 from k to 0:
kk
E [kxk+1 - x*k2] ≤ kχ0 - χ*k2 - 2XηtE [f(xt) - f(x*)] + 2Xη2E
kd	2	k
0 X XE 图-X 叫―
)
1X H
i=1
Note that in the bound we do not have α0,j for any j as we assume that the first communication is
done without compression. Assumption 1 implies for the quantization error
k d	2	k	t-1
XXE [αηt	≤ X(η2ε2 + (1 - β) XβlE [kxt-l - XtTTk2])
k	k	k-t
=ε2Xηt2+(1-β)X E[kxt -xt-1k2]Xβl
t=1	t=1	l=0
kk	∞
≤ε2Xηt2+(1-β)X E[kxt -xt-1k2]Xβl
t=1	t=1	l=0
kk
=ε2Xηt2+XE[kxt -xt-1k2] .	(11)
It is clear that the latter terms get canceled When We plug this bound back into the first recursion. □
B.1.4 Proof of Theorem 1
Proof. Most of the derivation has been already obtained in Lemma 3 and We only need to take care
of the SGD terms. To do that, We decompose the gradient error into expectation and variance:
n
2
n
2
n
2
1n
Ek	n X gk
i=1
1n	2	1n
n XEk[gk]	+ Ek	n X(gk - Ek[gk])
i=1
1n
=-X Ek [gk ]
i=1
(6)	σ2
≤ G2 + -.
Thus, We arrive at the folloWing corollary of Lemma 3:
k
i=1
1n
n X Ek	gk - Ek[gk]
i=1
0 ≤ E [)χk+1 - X*k2] ≤ kx0 -X*k2 -2£ntE [f(Xt) - f(X*)] +2]Tn2
σ2	ε2
t=0
Furthermore, by convexity of f We have
t=0
+	+4n
2
+
n
k
2
n
1k
f(xk) - f(χ*) ≤ ʒ— Ent(f(χt) - f(χ*)).
t=0 ηt t=0
Plugging it back, rearranging the terms and dropping E [kxk+1 - x*k2] gives the result.
(12)
□
16
Published as a conference paper at ICLR 2022
B.1.5 Proof of Proposition 1
Proof. Fix any i. By Young’s inequality and independence of ξ1k, . . . , ξnk we have
E
n X gk ∏=E Il 1 X W ； ξk )∏
≤ 2E	1 X Vfi(x*; ξk)∣]+2E
2n
=n ΕE[kVfi(x*; ξk)k2]+2E
n i=1
Substituting the definition of σ2 and applying Jensen's inequality, We derive
E
n l2	2	n
n X gk|	≤ ? + n X E [kVfi(xk; ξk )-Vfi(x*; ξk )k2].
i=1	i=1
By our assumption, fi(∙; ξ) is convex and has Li-LiPschitz gradient, so we can use Equation (2.1.7)
in Theorem 2.1.5 in Nesterov (2013):
2E [kVfi(xk; ξk) - Vfi(X*; ξk )k2] ≤ 4LiE [fi(xk; ξk) - fi(x*; ξk) -hVfi(x*; ξ ),xk -x*〉]
=4LiE [fi(xk) - f(x*) - hVfi(x*), xk - x*i]
≤LE[fi(xk) -fi(x*) - hVfi(x*),xk - x*i] .
Taking the average over i = 1, . . . , n and noticing Pin=1 Vfi(x*) = 0 yields
E
1 n	l2	σ2 L n
n X gk∣	≤ n + L X E [fi(xk) - fi(x*) -hVfi(x*),Xk -X*i]
i=1	i=1
2
=-* + LE [f (xk) - f(x*)],
which is exactly our claim.
□
B.1.6 Proof of Theorem 2
Proof. The proof is almost identical to that of Theorem 1, but now we directly use Assumption 3
and plug it in inside Lemma 3 to get
E [kxk+1
kk
-x*k2] ≤ kx0 - x*k2 -2XηtE[f(xt)-f(x*)] +2Xηt2E
(7)	k
≤ kx0 - x*k2 -	2ηt(1-ηtL)E[f(xt) -f(x*)] +2
t=0
建+
n
1n
1 X
n i=1
k	2	2k
≤ kx0 - x*k2 - XηtE [f (Xt) - f (x*)] +2 -* + 4n Xη2.
t=0	n	4n t=1
Rearranging this inequality yields
k	2	2k
XηtE [f(Xt) - f(x*)] ≤ kx0 - X*k2 - E [kχk+1 - X*k2] +2 , + — Xη2
t=0	n	4n t=1
≤ kx0 -x*k2+2
-*2	ε2
+4n
To finish the proof, it remains to upper bound f (Xk) using convexity the same way as it was done in
Equation (12).	□
17
Published as a conference paper at ICLR 2022
B.1.7 Proof of Theorem 3
Proof. By L-smoothness of f we have
Ek[f(xk+1)] ≤ f(xk)+ Ek KVf(Xk ),xk+1 - xk i] + 2 Ek [kxk+1 - xk k2]
ηn	L
=f (xk)— 华 ∑EkKVf (xk), Q(gk)i] + 2Ek[kxk+1 — xkk2]
n i=1
=f (xk) — ηk kVf(xk )k2 + 2 Ek [kxk+1 — Xk k2]
=f (xk) — ηkkVf (xk )k2 + LEk [kχk+1 — Xk k2] — 2 Ek[kχk+1 — Xk k2]
1 n
=f (xk) — ηkkVf (xk)k2 + ηkLEk	- £Q(gk)
i=1
2 - 2 Ek [kxk+1 - xk k2 ].
Similarly to Lemma 2, we get a decomposition into SGD and quantization errors:
≤ Ek
n 2
EQ	- X Q(gk
i=1
-Xgk『# + n-2 XEk[kQ(gk) - gkk2]
i=1	i=1
-X T ] +4-n X αkj.
i=1	j=1 k,j
We proceed with the two terms separately. To begin with, we further decompose the SGD error into
its expectation and variance:
Ek
-n
n X gk
=1
n	2
-X Ek [gk ]	+ Ek
n i=1
n X 域- Ek[gk ])∣2
2
-n
kVf (xk)k2 + n NEk [kgk -Vfi(xk)k2]
≤ kVf(xk)k2 + σ2.
n
Moving on, We plug it back into the upper bound Ek [f (xk+1)]. Assuming ηk ≤ 2L, We get
σ2	L d η2	L
Ek [f(xk+1)] ≤ f(xk) — ηk(1 — ηkL)kVf(xk)k2 + ηkL- + ^∑22k- - ?Ek[kxk+1 — xkk2]
n	4n	α2	2
j=1 k,j
2	d2
≤ f(xk) - ηkVf(xk)k2 + ηkL- + L X 器 - LEk[kxk+1 - xkk2].
2	n	4n j=1 α2k,j	2
Finally, reusing Equation (11) produces the bound
k	2k	2k
E [f(xk+1)] ≤ f(x0) - X ηtE [kVf(xt)k2] + σn- Xη2L + 4n Xη2L.
t=0	t=0	t=0
Notice that by Assumption 4 finf ≤ f(xk+1), so We have
-k
ʒ— ∑ηtE [kVf(xt)k2] ≤ 2
t=0 ηt t=0
f (XO)- finf + (% + 4n) Pk=o η2L
Pk
t=0 ηt
The left-hand side is equal to E [kVf (Xk )k2] by definition of Xk, and we conclude the proof.
□
18
Published as a conference paper at ICLR 2022
B.1.8 Proof of Corollary 2
Proof. For the first part, we have
M-χ*k2 + 2 (G2 + σ2 + 42)Pk=0律=O (! = O (G + 货!
2 Pk=0 ηt	k∑k=0 ηt	√k	.
The other complexities follow similarly.	□
B.1.9	Proof of Proposition 2
Proof. By definition of αk
d	2	k-1
XE a∣~ =渭 + 2nE [rk] = η2ε2 + 2n(1 - β) Xβt^xk-t - xk-t-1k2.
□
B.1.10	Proof of Proposition 3
Proof. Indeed, we only need to plug in the values of αk,j :
d	2	k-1
X E 著	=2nE ∖∖∖xk - xk-1k2] β=0 2n(1-β) X βtE ∖∖∖xk-t - xk-t-1k2].
j=1	αk,j	t=0
□
B.1.11	Proof of Proposition 4
Proof. Since the l-th block has dl coordinates, we get
d
XE
j=1
B
X dlE
l=1
η2
a2,(l)
B
2nXE∖(xk)l - (xk-1)l∖2] = 2nE ∖xk - xk-1∖2] .
l=1
□
B.2 Proofs for IntDIANA
Assumption 5. fil(x) has Lil-Lipschitz gradient. We define L d=ef 4 maxi∈[n] maxl∈[m] Lil.
Proposition 5. Suppose that Assumption 5 holds. Then, we have the following for IntDIANA and
any x ∈ Rd :
nm
mnXXINfii(X)-Vfil(X*)k2 ≤ L(f(x) -f(x*))
i=1 l=1
(13)
Proof. Based on Assumption 5 and Theorem 2.1.5 Nesterov (2013), we have:
∖Vfii(x) - Vfil(x*)∣∣2 ≤ 2Lil (fil(x) - fil(x*')-Nfil(x*),x - x*i)
Thus, double averaging leads to:
nm
—XX ∖Vfil(x) -Vfil(x*)∖2
mn
i=1 l=1
nm
≤ m2n XXLil (fil(x) - fil(x*) -"fil(x*), X - x*〉)
mn i=1 l=1
1 nm
f (x) — f (x*) — h- XX
Vfil(x*),x-x*i
mn i=1 l=1
Considering that m1n P2ι Pm=I Vfil(X*) = 0 and defining 4maxi∈[n] maxl∈[m] Lil leads to the
□
claim in the proposition.
19
Published as a conference paper at ICLR 2022
Lemma 4. For IntDIANA (Algorithm 3) and gk
▽f (Xk) and:
def ι
Pin=1 (hik + Q(gik)), we have Ek gk
n
1d 1
Ek [kg k ] ≤ 4n X 项 + Ek
Un X gik『:
(14)
Proof. By definition, gk = 1 Pn=ι(hk + Q(gk)),so
nn	nn	n
Ek [gk] = - X hk + -XEk [Q(gk)] (=) - X hk + -XEk [gk] - - X hk = Vf(Xk).
nn	nn	n
i=1
i=1
i=1
i=1
i=1
Thus, we have shown that gk is an unbiased estimate of ▽f (xk). Let us proceed with the second
moment of gk:
2
Ek kgkk2 =Ek
(3)
Ek
1X (
i=1
:◦Int(αk。增-hk)) —(gik - hk)+gik
1X (
i=1
:◦Int(ak。(成-hi))-(gk - hi
+ Ek
(11X gk 口
-n
n ΣEk
i=1
:◦I nt(αk。增-hk))-(gik - hk)
2 + Ek
-X gk 口
(4)	1 d 1
≤ 4n X 项 + Ek
n X gk∣∣2
2
□
Lemma 5. If L-SVRG estimator gik
have Ek gik = Vfi (Xk) and
=Vfil(χk; ξik) — Vfil(Wk; ξi)+ Ukis used in IntDIANA, We
n
2
1n
Ek	n X O'
(15)
i=1
Ek [σk+1] ≤ (ɪ -p)σk + pL (f (Xk) - f (x*)),
(16)
where σ'=熹 Pn=I P乙加--V%(x*)『.
Proof. Recall that E kX - E [X] k2 ≤ E kXk2 for any random variable X. For the L-SVRG
estimator gk = Vfil(Xk; ξ) — Vfil(Wk; ξ) + Uk, we have:
n
2
n
2
n
2
n
Ek n X gk
i=1
n2	n
n X Vfi(Xk)	+ Ek - X(gk -Vfi(Xk))
i=1
≤ 2L (f (xk) — f(x*)) +
i=1
nm
覆 X m^ XVfil(Xk) -vfii(wk)-
i=1	l0=1
m
m X(Vfil0 (Xk) - Vfil0 (wk))
l0=1
n
m
2
nm
≤ 2L (f (Xk) - f (X*)) + n X m X ∣Vfil(Xk) - Vfil(Wk)∣∣2
i=1 l0=1
nm
≤ 2L (f (Xk) — f (x*)) + nm-n∑∑∣∣Vfil(Xk) — Vfil(X*)∣∣
nm
i=1 l=1
2 + n m1nΕΕ∣∣Vfil(Wk )-v%(x*)∣∣
i=1 l=1
2
20
Published as a conference paper at ICLR 2022
where σk =熹 Pn=I Pm=ι k▽力((Wk) - fii(x*)l∣2∙ Based on the update of control sequence in
L-SVRG, we have:
Ek [σk+1]
n m	n m
--p XXwfiI(Wk)- fii(x*)『+ ɪ XXwfiI(Xk)- fii(x*)『
i=1 i=1	i=1 i=1
(13)	L
≤ (1 - p)σ1 + pL (f(xk) - f(x*))
□
Lemma 6. Define σk =f 1 Pn=IIlhk - Vfi(x*)k2. For IntDIANA algorithm, we have:
n	d -.
Ek-1] ≤ - X Ek [∣gk-Vfi(x*)k2]+ X . ,	(17)
For the full gradient, we have n Pn=IEk [∣gk - Vfi(X*)∣∣2] ≤ L(f (xk) - f(x*)). For the L-
SVRG estimator, Wehaven Pn=IEk [∣∣gk - Vfi(X*)∣∣2] ≤ 4σk +3L(f (xk) - f(x*)).
Proof. We define σk C=f n Pn=I Ilhk - Vfi(X*)∣∣2. Consider the step hk+1 = hlk + Q(gf),
where Q(gk) = ±。Int(afc。(gk - hk)). Note that Ek [(gk - hk,gk - 2Vfi(X*) + hk〉]=
Ek [∣∣gf - Vfi(X*)∣∣2 -IIhk - Vfi(X*)∣∣2], which explains the last equality below:
n
1 n
Ek [σk+1] = Ek n E IIhk- Vfi(X*) + Q(glk)∣
2
i=1
1 n
σ+n ∑Efc
ɪ。1 nt (αk。(gk - hk))[1+2- XEk [hQ(gf),h-Vfi(x*)〉]
nn	d
(=)σk + — XEk [∣gk - hkk2] + 2- Xhgk- hf,博-Vfi(X*)〉+ X —
n	n	aL
i=ι	i=ι	j=ι kj
nd
≤ σ2 + n X Ek [hgk - hk,gk - 2Vfi(X") + hki] + X O-
nd
=-XEk [kgk -Vfi(x*)k2] + X-2 .
n i=1	j = 1 0k,j
For the full gradient gf = Vfi(xk), we have:
1工 -，	-1工. ,	2 L ,
-XEk [∣gk -Vfi(x*)k2] = - XEk [∣Vfi(xk) -Vfi(x*)∣∣2] ≤ -(f (xk) - f(x*)).
n i=1	n i=1
For the L-SVRG estimator, we have by Young,s inequality:
-n	2 n
n EEk [∣gk - Vfi(X*)『]≤ n EEk [∣gk - Vfi(Xk)『]
i=1
i=1
2 n
+ n NkVfi(Xk)-Vfi(X*)∣∣2
Onm
≤ mn XX IVfii(Xk) - Vfii(wk)k2 + L(f(Xk) - f(X*))
i=1 i = 1
n m	n m
≤ m XXIVfii (Xk )-Vfii(X*)∣∣2 +m4n XX ∣Vfii(wk) - Vfii(X*)∣∣2 + L(f (Xk) - f (x*))
i=1 i = 1	i=1 i = 1
(13)
≤ 4σk +3L(f (xk) - f(x*)).
□
21
Published as a conference paper at ICLR 2022
Lemma 7. Suppose that Assumption 5 holds. Besides, We assume that f (∙) is μ-strongly convex
(μ ≥ 0). For IntDIANA with adaptive ak = 下滞«_1口 and GD gradient estimator, we have:
Ek[W+1-x*∣∣2] + Ek [kxfc+1 - xk『]
≤ (1 - ηk μ)kxk - x*k2 + 2 kxk -XkTk2 - 2ηk (1 - 2ηk L)(f (xk)-f (x*)),
L
Ek [σk+1] ≤ L(f(xk) - f (x*)) + n∣∣xk - XkTk2.
For IntDIANA with adaptive ak and L-SVRG gradient estimator, we have:
Ek [kxk+1-x*k2]+Ek [∖∖xk+1 - xkk2]
≤ (1 - ηkμ)∖xk - x*k2 + 2kxk -XkTk2 - 2ηk (1 - 2ηk (L + 2∣) f (f (xk) - f(x*)) + 乎成,
Ek [σf+1] ≤ (1 -p)σk + pL(f (xk) - f(x*)),
Ek [σk+1] ≤ 4σf + 3L(f (xk) - f(x*)) + n∖xk - XkTk2.
Proof. By μ-strong convexity, we have:
Ek
n
i=1
n
=-2ηk XhEk[成],xk -x*)
i=1
≤ -2ηk(f (xk) - f (x*)) - ηkμkxk - x*k2.
BeSideS, ∣∣xk+1 - xk k2 = 2η2∣∣gk k2 -∣∣xk+1 - xk k2, so
Ek [kxk+1 - x*k2] + Ek [kxk+1 - xkk2]
=(1 - ηkμ)kxk - x*k2 - 2ηk(f (xk) - f(x*)) + 2η2Ek Ijlgkl ：
≤ (1 -ηk4)∣∣xk-x*∖2-2ηk(f(Xk)-f(x*))+2η2Ek	nXgk∣∣+ + 2nXOk
Applying Proposition 3 to the obtained bound results in the following recursion
Ek [kxk+1-x*k2]+Ek [kxk+1 - xkk2]
≤ (1 - ηkμ)kxk - x*k2 + 1 Ek [kxk - xk-1k2] - 2ηk(f (xk) - f (x*)) + 2ηkEk	ɪ X成『]
With the GD estimator, the produced bound simplifies to
Ek [kxk+1-x*k2] + Ek [kxk+1 - xk k2]
≤ (1 - ηkμ)kxk - x*k2 + 2kxk - xk-1k2 - 2ηk(1 - 2ηkL)(f (xk) - f(x*)).
Based on Lemma 6, the following is satisfied for IntDIANA with GD estimator and adaptive
α,, = 一Jk√d …:
k	√n∣∣xk-xk-1 k
L
Ek [σk+1] ≤ 2(f (xk) - f (x*)) + nkxk - xk-1k2∙
In turn, Lemma 5 gives for L-SVRG estimator	Ek [∣∣ ɪ Pn=I Ok ll[	≤
(2L + L) (f (xk) - f (x*)) + nσk, so we can derive that
Ek [kxk+1 - x*k2] + Ek [kxk+1 - xkk2]
≤ (1 - ηkμ)l∣xk - x*ll2 + 2l∣xk - XkTk2 - 2ηk (1 - 2ηk (L + 2n)) (f(Xk) - f(X*)) + ^kσk.
22
Published as a conference paper at ICLR 2022
Let us now combine Equation (16) and Lemma 6:
Ek [σk+1] ≤ (1-p)σk + PL(f(xk) - f(x*)),
Ek [σk+1] ≤ 4σk + 3L(f(xk) - f(x*)) + n∣∣xk - XkTk2.
□
Lemma 8. We define the Lyapunov function as Ψk =f ∣∣xk - x*k2 + ∣∣xk - XkTk2 + cιη2σk +
c2nkσk. Assume that the conditions of Lemma 7 hold. If μ > 0, We have:
E [Ψk+1] ≤ θE [Ψk] ,
where θ	d=f	max {(1	- ηkμ), 3},	ci	=	0,	c2	=	l2	and ηk ≤ 2也；二)for IntDIANA
def
with GD estimator. Alternatively, for IntDIANA with L-SVRG estimator, we have θ =
max {(I - ηkμ), 4, (1 - 8m)} and set ci = 8m, c2 = Ln, P = mm, and ηk ≤ m+⅛∕ny. If
μ = 0, we have
ηkE [f (xk) - f (x*)] ≤ E [Ψk] - E [Ψk+i],
where ηk ≤ 4(L J L)for the GD variant and ηk ≤ 4(l+2l/、)for the L-SVRG variant.
Proof. We define the Lyapunov function as Ψk d=f ∣∣xk - x*∣2 + ∣∣xk - XkTk2 + cjηkσk +
c2ηk2σ2k. For IntDIANA with GD estimator, we can set ci = 0 and derive the following inequality
from Lemma 7 and ηk+i ≤ ηk:
E [Ψk+i] ≤ (1-ηkμ)E [∣xk - x*k2] + Q + c2ηkn) E [∣xk - XkTk2]
-2ηk (1 - 2ηk (L + -8-)) E [f (Xk ) - f (x*)].
We first consider μ > 0 case. Let c? = L2, and ηk ≤	1 L、. We have E [Ψk+i] ≤
4n	2(L+ 32n)
max{(1 - ηkμ), 4} E [Ψk].
For IntDIANA with L-SVRG estimator, we have the following based on Lemma 7:
E [Ψk+i] ≤ (1 - ηkμ)E [kXk - X*k2] + (J + c2η2n) E [l" -XkTk2]
+ ηk (4 + 4c2 + (i - p)cι) E [σk]
-2ηk (1 - 2ηk (L + 2∣ + pcJL + 牛))E [f(Xk) - f(x*)]
Let ci = 8m, c2 = L2, P = mm, and ηk ≤ 2(L+2c/n). Plugging these values into the recursion, we
get E [Ψk+i] ≤ max {(1 - ηkμ), 4,(1 -熹)} E [Ψk ].
If μ = 0, we instead let ηk ≤ 乙① J L ) for the GD variant and ηk ≤4①十，'/.) for the L-SVRG
variant to obtain from the same recursions:
ηkE[f(Xk)-f(X*)] ≤E[Ψk] - E [Ψk+i] .
□
C Details and Additional Results of Experiments
C.1 More details
Here we provide more details of our experimental setting. We use the learning rate scaling technique
(Goyal et al., 2017; Vogels et al., 2019) with 5 warm-up epochs. As suggested in previous works
23
Published as a conference paper at ICLR 2022
(Vogels et al., 2019; Alistarh et al., 2017; HOrvgth et al., 2019), We tune the initial single-worker
learning rate on the full-precision SGD and then apply it to PowerSGD, QSGD, and NatSGD.
For the task of training ResNet18 on the CIFAR-10 dataset, we utilize momentum β = 0.9 and
weight decay with factor 10-4 (except the Batchnorm parameters) for all algorithms. All algorithms
run for 300 epochs. The learning rate decays by 10 times at epoch 150 and 250. The initial learning
rate is tuned in the range {0.05, 0.1, 0.2, 0.5} and we choose 0.1.
For the task of training a 3-layer LSTM, all algorithms run for 90 epochs. We set the size of word
embeddings to 650, the sequence length to 30, the number of hidden units per layer to 650, and the
dropout rate to 0.4. Besides, we tie the word embedding and softmax weights. We tune the initial
learning rate in the range of {0.6, 1.25, 2.5, 5} and we choose 1.25. For both tasks, we report the
results based on 3 repetitions with random seeds {0, 1, 2}. To measure the time of computation,
communication, and compression/decompression of the algorithms, we use the timer (a Python
context manager) implemented in the PowerSGD code6.
For PowerSGD, we use rank = 2 in the task of training ResNet18 on the CIFAR-10 dataset and rank
= 4 in the task of training LSTM on the Wikitext-2 dataset as suggested byVogels et al. (2019). For
QSGD, we use the gradient matrix of each layer as a bucket and set the number of quantization levels
to be 64 (6-bit).
C.2 Toy experiment on timings
8 Nodes X 2 GPUs/Node
Sw①ELL①ɔnp①千=V
101
100
106
107
#Coordinates
Figure 2: Time of communicating FP32 and Int8 messages based on all-reduce.
Figure 2 shows the different manners of PowerSGD and IntSGD to save the communication time
based on the all-reduce compared to full-precision SGD: 1) IntSGD (8-bit) communicates the data
with int8 data type but does not reduce the number of coordinates; 2) PowerSGD does not change
the data type but breaks one communication round of a big number of coordinates into three com-
munication rounds of much smaller numbers of coordinates.
C.3 Convergence curves of the deep learning tasks
Please see Figure 3 and Figure 4.
C.4 Sensitivity analysis of hyperparameters
We analyze the sensitivity of IntSGD to its hyperparameters β and ε. As shown in Figure 5,
the performance of IntSGD is quite stable across the choices of β ∈ {0.0, 0.3, 0.6, 0.9} and
ε ∈ {10-4, 10-6, 10-8} on the two considered tasks. Overall, β = 0.9 and ε = 10-8 is a good
default setting for our IntSGD algorithm.
6https://github.com/epfml/powersgd
24
Published as a conference paper at ICLR 2022
ReSNet18
ReSNet18
150	200	250
Epochs
A-x①-d」①d El
ssozsəh
0	50	100	150	200	250	300
101
100
0	50	100	150	200	250	300
→- SGD
→- QSGD
NatSGD
-→- IntSGD(Determ.)
-→- IntSGD(Random)
ReSNet18
Epochs
Epochs
Figure 3: Convergence curves of IntSGD (Random) and IntSGD (Determ.) and the baseline algo-
rithms on the task of training ResNet18 on the CIFAR-10 dataset.
101
9× 100
8× 100
o 7×100
-M
∣Ξ 6×100
5 X 100
0	20	40	60	80
Epochs
LSTM
4 X 102
3 X 102
2	X 102
102
Epochs
A-x①-d」①d El
4 X 102
3	X 102
2 X 102
102
→- SGD
qs- QSGD
NatSGD
—IntSGD(Determ.)
-→- IntSGD(Random)
0	20	40	60	80
Epochs
Figure 4:
rithms on
Convergence curves of IntSGD (Random) and IntSGD (Determ.)
the task of training a 3-layer LSTM on the Wikitext-2 dataset.
and the baseline algo-
25
Published as a conference paper at ICLR 2022
S Cj	Test Accuracy (%)"			-94.7	G O	4.57	4.58	4.57	-4.600 -4.595
	94.6	94.6	94.6						
	—	—		■	-94.6					
9 Cj	94.5	94.5	94.4		9 ɔ	4.59	4.59	4.59	-4.590
				-94.5	cr∖				-4.585
on	94.5	94.6	94.5	-94.4	on	4.59	4.59	4.59	-4.580
q Cj	94.7	94.7	94.3	-94.3	q	4.60	4.59	4.60	-4.575
	10-8	10-6	10 -4		d				-4.570
		"				10-8	10-6	10 -4	
Figure 5:	Test accuracy (on the image classification task) and test loss (on the language modeling
task) of IntSGD under different hyperparameters β and ε. "↑" or "]” denotes the larger, the better or
vice versa.
C.5 Logistic regression experiment
Setup: We run the experiments on the '2-regularized logistic regression problem with four datasets
(a5a, mushrooms, w8a, real-sim) from the LibSVM repository7, where
1n
f (X) = n X fi(X)
and
1m	λ
fi(X) = m Eiog(1+eχp(-Alx%)) + -2Iixk,
l=1
and x ∈ Rd, λ2 is chosen proportionally to 焉 and Ai,l ∈ Rd, bi,l ∈ {-1,1} are the feature
and label of l-th data point on the i-th worker. The experiments are performed on a machine with
24 Intel(R) Xeon(R) Gold 6246 CPU @ 3.30GHz cores, where 12 cores are connected to a socket
(there are two sockets in total). All experiments use 12 cpu cores and each core is utilized as a
worker. The communications are implemented based on the MPI4PY library DalCin et al. (2005).
The “optimum” x* is obtained by running GD with the whole data using one cpu core until there are
5000 iterations or ∣∣Vf(x) ∣2 ≤ 10-30.
Table 4: Information of the experiments on '2-regularized logistic regression.
Dataset	#Instances N	Dimension d	λ2
a5a	6414	123	5 × 10-4
mushrooms	8124	112	6 × 10-4
w8a	49749	300	10-4
real-sim	72309	20958	5 × 10-5
The whole dataset is split according to its original indices into n folds, and each fold is assigned to
a local worker, i.e., the data are heterogeneous. There are m = [NnC data points on each worker.
For each dataset, we run each algorithm multiples times with 20 random seeds for each worker. For
the stochastic algorithms, we randomly sample 5% of the local data as a minibatch (i.e., batch size
T = b20C) to estimate the stochastic gradient gk on each worker. We set P = m in VR-IntDIANA.
7https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html
26
Published as a conference paper at ICLR 2022
Apart from IntSGD with gk = Nfi(Xk) (IntGD), we also evaluate IntDIANA (Algorithm 3) with the
GD or L-SVRG estimator (called IntDIANA and VR-IntDIANA, respectively).
a5a, n=12
10-3
101
T- IntGD(Q = 0)
r- IntDIANA(Q = O)
T- VR-IntDIANA(Q = 0)
=10-7
S
I ∙c-ii
0	1000	2000	3000	4000	5000
#grad/mn
0	1000	2000	3000	4000
#grad/mn
0	1000	2000	3000	4000
#grad/mn
real-sim, n = 12
0	1000	2000	3000	4000
#grad/mn
a5a, n = 12
1000	2000	3000	4000
#grad/mn
→- IntGD(Q = O)
-→- IntDIANA(Q = O)
-*- VR-IntDIANA(Q = 0)
∙l36κlu-⅛3Ξ,e,
Figure 6:	Objective gaps and the max integer in the aggregated vector	in=1 Q(gik).
As shown in Figure 6, IntSGD with gik = Nfi (xk) (IntGD) suffers from low compression efficiency
issue (very large integer in the aggregated vector Pin=1 Q(gik)) and IntDIANA can solve this issue and
only requires less then 3 bits per coordinate in the communication. IntDIANA with SVRG gradient
estimator (VR-IntDIANA) further improves IntDIANA with GD estimator in terms of gradient oracles.
27