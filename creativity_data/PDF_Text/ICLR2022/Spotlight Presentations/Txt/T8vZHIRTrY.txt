Published as a conference paper at ICLR 2022
Understanding	Domain	Randomization	for
Sim-to-real Transfer
Xiaoyu Chen & Jiachen Hu *	Chi Jin
Key Laboratory of Machine Perception, MOE, Department of Electrical and Computer Engineering,
School of Artificial Intelligence, Peking University Princeton University
{cxy30, NickH}@pku.edu.cn	chij@princeton.edu
Lihong Li	Liwei Wang
Amazon	Key Laboratory of Machine Perception, MOE,
llh@amazon.com School of Artificial Intelligence, Peking University
International Center for Machine Learning Research, Peking University
wanglw@cis.pku.edu.cn
Ab stract
Reinforcement learning encounters many challenges when applied directly in the
real world. Sim-to-real transfer is widely used to transfer the knowledge learned
from simulation to the real world. Domain randomization—one of the most pop-
ular algorithms for sim-to-real transfer—has been demonstrated to be effective in
various tasks in robotics and autonomous driving. Despite its empirical successes,
theoretical understanding on why this simple algorithm works is limited. In this
paper, we propose a theoretical framework for sim-to-real transfers, in which the
simulator is modeled as a set of MDPs with tunable parameters (corresponding to
unknown physical parameters such as friction). We provide sharp bounds on the
sim-to-real gap—the difference between the value of policy returned by domain
randomization and the value of an optimal policy for the real world. We prove
that sim-to-real transfer can succeed under mild conditions without any real-world
training samples. Our theory also highlights the importance of using memory (i.e.,
history-dependent policies) in domain randomization. Our proof is based on novel
techniques that reduce the problem of bounding the sim-to-real gap to the prob-
lem of designing efficient learning algorithms for infinite-horizon MDPs, which
we believe are of independent interest.
1	Introduction
Reinforcement Learning (RL) is concerned with sequential decision making, in which the agent
interacts with the environment to maximize its cumulative rewards. This framework has achieved
tremendous empirical successes in various fields such as Atari games, Go and StarCraft (Mnih et al.,
2013; Silver et al., 2017; Vinyals et al., 2019). However, state-of-the-art algorithms often require a
large amount of training samples to achieve such a good performance. While feasible in applications
that have a good simulator such as the examples above, these methods are limited in applications
where interactions with the real environment are costly and risky, such as healthcare and robotics.
One solution to this challenge is sim-to-real transfer (Floreano et al., 2008; Kober et al., 2013). The
basic idea is to train an RL agent in a simulator that approximates the real world and then trans-
fer the trained agent to the real environment. This paradigm has been widely applied, especially
in robotics (Rusu et al., 2017; Peng et al., 2018; Chebotar et al., 2019) and autonomous driving
(Pouyanfar et al., 2019; Niu et al., 2021). Sim-to-real transfer is appealing as it provides an essen-
tially unlimited amount of data to the agent, and reduces the costs and risks in training.
However, sim-to-real transfer faces the fundamental challenge that the policy trained in the simulated
environment may have degenerated performance in the real world due to the sim-to-real gap—the
* These two authors contributed equally.
1
Published as a conference paper at ICLR 2022
mismatch between simulated and real environments. In addition to building higher-fidelity simula-
tors to alleviate this gap, domain randomization is another popular method (Sadeghi & Levine, 2016;
Tobin et al., 2017; Peng et al., 2018; OpenAI et al., 2018). Instead of training the agent in a single
simulated environment, domain randomization randomizes the dynamics of the environment, thus
exposes the agent to a diverse set of environments in the training phase. Policies learned entirely
in the simulated environment with domain randomization can be directly transferred to the physical
world with good performance (Sadeghi & Levine, 2016; Matas et al., 2018; OpenAI et al., 2018).
In this paper, we focus on understanding sim-to-real transfer and domain randomization from a
theoretical perspective. The empirical successes raise the question: can we provide guarantees
for the sub-optimality gap of the policy that is trained in a simulator with domain randomization
and directly transferred to the physical world? To do so, we formulate the simulator as a set of
MDPs with tunable latent variables, which corresponds to unknown parameters such as friction
coefficient or wind velocity in the real physical world. We model the training process with domain
randomization as finding an optimal history-dependent policy for a latent MDP, in which an MDP
is randomly drawn from a set of MDPs in the simulator at the beginning of each episode.
Our contributions can be summarized as follows:
•	We propose a novel formulation of sim-to-real transfer and establish the connection be-
tween domain randomization and the latent MDP model (Kwon et al., 2021). The latent
MDP model illustrates the uniform sampling nature of domain randomization, and helps to
analyze the sim-to-real gap for the policy obtained from domain randomization.
•	We study the optimality of domain randomization in three different settings. Our results
indicate that the sim-to-real gap of the policy trained in the simulation can be o(H) when
the randomized simulator class is finite or satisfies certain smoothness condition, where
H is the horizon of the real-world interaction. We also provide a lower bound showing
that such benign conditions are necessary for efficient learning. Our theory highlights the
importance of using memory (i.e., history-dependent policies) in domain randomization.
•	To analyze the optimality of domain randomization, we propose a novel proof framework
which reduces the problem of bounding the sim-to-real gap of domain randomization to
the problem of designing efficient learning algorithms for infinite-horizon MDPs, which
we believe are of independent interest.
•	As a byproduct of our proof, we provide the first provably efficient model-based algorithm
for learning infinite-horizon average-reward MDPs with general function approximation
(Algorithm 4 in Appendix C.3). Our algorithm achieves a regret bound of O(DydeT),
where T is the total timesteps and deis a complexity measure ofa certain function class F
that depends on the eluder dimension (Russo & Van Roy, 2013; Osband & Van Roy, 2014).
2	Related Work
Sim-to-Real and Domain Randomization The basic idea of sim-to-real is to first train an RL
agent in simulation, and then transfer it to the real environment. This idea has been widely applied
to problems such as robotics (e.g., Ng et al., 2006; Bousmalis et al., 2018; Tan et al., 2018; OpenAI
et al., 2018) and autonomous driving (e.g., Pouyanfar et al., 2019; Niu et al., 2021). To alleviate the
influence of reality gap, previous works have proposed different methods to help with sim-to-real
transfer, including progressive networks (Rusu et al., 2017), inverse dynamics models (Christiano
et al., 2016) and Bayesian methods (Cutler & How, 2015; Pautrat et al., 2018). Domain random-
ization is an alternative approach to making the learned policy to be more adaptive to different
environments (Sadeghi & Levine, 2016; Tobin et al., 2017; Peng et al., 2018; OpenAI et al., 2018),
thus greatly reducing the number of real-world interactions.
There are also theoretical works related to sim-to-real transfer. Jiang (2018) uses the number of
different state-action pairs as a measure of the gap between the simulator and the real environment.
Under the assumption that the number of different pairs is constant, they prove the hardness of
sim-to-real transfer and propose efficient adaptation algorithms with further conditions. Feng et al.
(2019) prove that an approximate simulator model can effectively reduce the sample complexity
in the real environment by eliminating sub-optimal actions from the policy search space. Zhong
2
Published as a conference paper at ICLR 2022
et al. (2019) formulate a theoretical sim-to-real framework using the rich observation Markov de-
cision processes (ROMDPs), and show that the transfer can result in a smaller real-world sample
complexity. None of these results study benefits of domain randomization in sim-to-real transfer.
Furthermore, all above works require real-world samples to fine-tune their policy during training,
while our work and the domain randomization algorithm do not.
POMDPs and Latent MDPs Partially observable Markov decision processes (POMDPs) are a
general framework for sequential decision-making problems when the state is not fully observ-
able (Smallwood & Sondik, 1973; Kaelbling et al., 1998; Vlassis et al., 2012; Jin et al., 2020a;
Xiong et al., 2021). Latent MDPs (Kwon et al., 2021), or LMDPs, are a special type of POMDPs,
in which the real environment is randomly sampled from a set of MDPs at the beginning of each
episode. This model has been widely investigated with different names such as hidden-model MDPs
and multi-model MDPs. There are also results studying the planning problem in LMDPs, when the
true parameters of the model is given (Chades et al., 2012; Buchholz & Scheftelowitsch, 2019;
Steimle et al., 2021) . Kwon et al. (2021) consider the regret minimization problem for LMDPs, and
provide efficient learning algorithms under different conditions. We remark that all works mentioned
above focus on the problems of finding the optimal policies for POMDPs or latent MDPs, which is
perpendicular to the central problem of this paper—bounding the performance gap of transferring
the optimal policies of latent MDPs from simulation to the real environment.
Infinite-horizon Average-Reward MDPs Recent theoretical progress has produced many prov-
ably sample-efficient algorithms for RL in infinite-horizon average-reward setting. Nearly matching
upper bounds and lower bounds are known for the tabular setting (Jaksch et al., 2010; Fruit et al.,
2018; Zhang & Ji, 2019; Wei et al., 2020). Beyond the tabular case, Wei et al. (2021) propose
efficient algorithms for infinite-horizon MDPs with linear function approximation. To the best of
our knowledge, our result (Algorithm 4) is the first efficient algorithm with near-optimal regret for
infinite-horizon average-reward MDPs with general function approximation.
3	Preliminaries
3.1	Episodic MDPs
We consider episodic RL problems where each MDP is specified by M = (S, A, P, R, H, s1). S
and A are the state and the action space with cardinality S and A respectively. We assume that S
and A are finite but can be extremely large. P : S × A → ∆(S) is the transition probability matrix
so that P (∙∣s, a) gives the distribution over states if action a is taken on state s, R : S ×A→ [0,1]
is the reward function. H is the number of steps in one episode.
For simplicity, we assume the agent always starts from the same state in each episode, and use s1
to denote the initial state at step h = 1. It is straight-forward to extend our results to the case with
random initialization. At step h ∈ [H], the agent observes the current state sh ∈ S, takes action
ah ∈ A, receives reward R(sh, ah), and transits to state sh+1 with probability P (sh+1 |sh, ah). The
episode ends when sH+1 is reached.
We consider the history-dependent policy class Π, where π ∈ Π is a collection of mappings
from the history observations to the distributions over actions. Specifically, we use trajh =
{(s1,a1,s2,a2,…，sh) | Si ∈ S,a% ∈ A,i ∈ [h]} to denote the set of all possible trajec-
tories of history till step h. We define a policy π ∈ Π to be a collection of H policy func-
tions {πh : trajh → ∆(A)}h∈[H]. We define VMπ ,h : S → R to be the value function at
step h under policy π on MDP M, i.e., VMπ ,h (s) = EM,π [PtH=h R(st, at) | sh = s]. Accord-
ingly, we define QπM,h : S × A → R to be the Q-value function at step h: QπM,h (s, a) =
EM,π [R(sh, ah) + Pt=h+1 R(st, at) | sh = s, ah = a].
We use ∏M to denote the optimal policy for a single MDP M. It can be shown that there exists ∏M
such that the policy at step h depends on only the state at step h but not any other prior history. That
is, ∏M can be expressed as a collection of H policy functions mapping from S to ∆(A). We use
VM,h and QM,h to denote the optimal value and Q-functions under the optimal policy ∏M at step
h.
3
Published as a conference paper at ICLR 2022
3.2	Practical Implementation of Domain Randomization
In this subsection, we briefly introduce how domain randomization works in practical applications.
Domain randomization is a popular technique for improving domain transfer (Tobin et al., 2017;
Peng et al., 2018; Matas et al., 2018), which is often used for zero-shot transfer when the target
domain is unknown or cannot be easily used for training. For example, by highly randomizing the
rendering settings for their simulated training set, Sadeghi & Levine (2016) trained vision-based
controllers for a quadrotor using only synthetically rendered scenes. OpenAI et al. (2018) studied
the problem of dexterous in-hand manipulation. The training is performed entirely in a simulated
environment in which they randomize the physical parameters of the system like friction coefficients
and vision properties such as object’s appearance.
To apply domain randomization in the simulation training, the first step before domain randomiza-
tion is usually to build a simulator that is close to the real environment. The simulated model is
further improved to match the physical system more closely through calibration. Though the simu-
lation is still a rough approximation of the physical setup after these engineering efforts, these steps
ensure that the randomized simulators generated by domain randomization can cover the real-world
variability. During the training phase, many aspects of the simulated environment are randomized
in each episode in order to help the agent learn a policy that generalizes to reality. The policy
trained with domain randomization can be represented using recurrent neural network with mem-
ory such as LSTM (Yu et al., 2018; OpenAI et al., 2018; Doersch & Zisserman, 2019). Such a
memory-augmented structure allows the policy to potentially identify the properties of the current
environment and adapt its behavior accordingly. With sufficient data sampled using the simulator,
the agent can find a near-optimal policy w.r.t. the average value function over a variety of simula-
tion environments. This policy has shown its great adaptivity in many previous results, and can be
directly applied to the physical world without any real-world fine-tuning (Sadeghi & Levine, 2016;
Matas et al., 2018; OpenAI et al., 2018).
4	Formulation
In this section, we propose our theoretical formulation of sim-to-real and domain randomization.
The corresponding models will be used to analyze the optimality of domain randomization in the
next section, which can also serve as a starting point for future research on sim-to-real.
4.1	Sim-to-real Transfer
In this paper, we model the simulator as a set of MDPs with tunable latent parameters. We consider
an MDP set U representing the simulator model with joint state space S and joint action space A.
Each MDP M = (S, A, PM, R, H, s1) in U has its own transition dynamics PM, which corre-
sponds to an MDP with certain choice of latent parameters. Our result can be easily extended to the
case where the rewards are also influenced by the latent parameters. We assume that there exists an
MDP M* ∈ U that represents the dynamics of the real environment.
We can now explain our general framework of sim-to-real. For simplicity, we assume that during
the simulation phase (or training phase), we are given the entire set U that represents MDPs under
different tunable latent parameter. Or equivalently, the learning agent is allowed to interact with any
MDP M ∈ U in arbitrary fashion, and sample arbitrary amount of trajectories. However, we do not
know which MDP M ∈ U represents the real environment. The objective of sim-to-real transfer is
to find a policy π purely based on U, which performs well in the real environment. In particular, we
measure the performance in terms of the sim-to-real gap, which is defined as the difference between
the value of learned policy π and the value of an optimal policy for the real world:
Gap(π) = VMM*,1(SI)- VM*,1(SI).	⑴
We remark that in our framework, the policy π is learned exclusively in simulation without the
use of any real world samples. We study this framework because (1) our primary interests—domain
randomization algorithm does not use any real-world samples for training; (2) we would like to focus
on the problem of knowledge transfer from simulation to the real world. The more general learning
paradigm that allows the fine-tuning of policy learned in simulation using real-world samples can
4
Published as a conference paper at ICLR 2022
be viewed as a combination of sim-to-real transfer and standard on-policy reinforcement learning,
which we left as an interesting topic for future research.
4.2	Domain Randomization and LMDPs
We first introduce Latent Markov decision processes (LMDPs) and then explain domain random-
ization in the viewpoint of LMDPs. A LMDP can be represented as (U, ν), where U is a set of
MDPs with joint state space S and joint action space A, and ν is a distribution over U . Each MDP
M = (S, A, PM , R, H, s1) in U has its own transition dynamics PM that may differs from other
MDPs. At the start of an episode, an MDP M ∈ U is randomly chosen according to the distribution
ν. The agent does not know explicitly which MDP is sampled, but she is allowed to interact with
this MDP M for one entire episode.
Domain randomization algorithm first specifies a distribution over tunable parameters, which equiv-
alently gives a distribution ν over MDPs in simulator U . This induces a LMDP with distribution
ν . The algorithm then samples trajectories from this LMDP, runs RL algorithms in order to find the
near-optimal policy of this LMDP. We consider the ideal scenario that the domain randomization
algorithm eventually find the globally optimal policy of this LMDP, which we formulate as domain
randomization oracle as follows:
Definition 1. (Domain Randomization Oracle) Let U be the set of MDPs generated by domain
randomization and ν be the uniform distribution over U . The domain randomization oracle returns
an optimal history-dependent policy πDR of the LMDP (U, V):
∏ D R = arg max EM〜V VM ι(sι).	(2)
π∈Π
Since LMDP is a special case of POMDPs, its optimal policy ∏DR in general will depend on his-
tory. This is in sharp contrast with the optimal policy of a MDP, which is history-independent. We
emphasize that both the memory-augmented policy and the randomization of the simulated environ-
ment are critical to the optimality guarantee of domain randomization. We also note that we don’t
restrict the learning algorithm used to find the policy ∏dr, which can be either in a model-based or
model-free style. Also, We don,t explicitly define the behavior of ∏dr. The only thing We know
about ∏DR is that it satisfies the optimality condition defined in Equation 2. In this paper, we aim to
bound the sim-to-real gap of ∏dr, i.e., Gap(∏DR, U) under different regimes.
5	Main Results
We are ready to present the sim-to-real gap of ∏DR in this section. We study the gap in three different
settings under our sim-to-real framework: finite simulator class (the cardinality |U| is finite) with the
separation condition (MDPs inU are distinct), finite simulator class without the separation condition,
and infinite simulator class. During our analysis, we mainly study the long-horizon setting where
H is relatively large compared with other parameters. This is a challenging setting that has been
widely-studied in recent years (Gupta et al., 2019; Mandlekar et al., 2020; Pirk et al., 2020). We
show that the sim-to-real gap of ∏dr is only O(log3(H)) for the finite simulator class with the
separation condition, and only O(√H) in the last two settings, matching the best possible lower
bound in terms of H .
In our analysis, we assume that the MDPs in U are communicating MDPs with a bounded diameter.
Assumption 1 (Communicating MDPs (Jaksch et al., 2010)). The diameter of any MDP M ∈ U is
bounded by D. That is, consider the stochastic process defined by a stationary policy π : S → A
on an MDP with initial state s. Let T (s0 |M, π, s) denote the random variable for the first time step
in which state s0 is reached in this process, then maxs=sθ∈s min∏=s→/ E [T (s0 | M, π, s)] ≤ D.
This is a natural assumption widely used in the literature (Jaksch et al., 2010; Agrawal & Jia, 2017;
Fruit et al., 2020). The communicating MDP model also covers many real-world tasks in robotics.
For example, transferring the position or angle ofa mechanical arm only costs constant time. More-
over, the diameter assumption is necessary under our framework.
Proposition 1. Without Assumption 1, there exists a hard instance U so that Gap(πDR) = Ω(H).
5
Published as a conference paper at ICLR 2022
We prove Proposition 1 in Appendix G.1. Note that the worst possible gap of any policy is H, so
∏DR becomes ineffective without Assumption 1.
5.1	Finite Simulator Class With Separation Condition
As a starting point, we will show the sim-to-real gap when the MDP set U is a finite set with cardi-
nality M. Intuitively, a desired property of ∏Dr is the ability to identify the environment the agent is
exploring within a few steps. ThiS is because ∏Dr is trained under uniform random environments, so
We hope it can learn to tell the differences between environments. As long as ∏DR has this property,
the agent is able to identify the environment dynamics quickly, and behave optimally afterwards
(note that the MDP set U is known to the agent).
Before presenting the general results, we first examine a simpler case where all MDPs in U are
distinct. Concretely, we assume that any two MDPs in U are well-separated on at least one state-
action pair. Note that this assumption is much weaker than the separation condition in Kwon et al.
(2021), which assumes strongly separated condition for each state-action pair.
Assumption 2 (δ-separated MDP set). For any M1 , M2 ∈ U, there exists a state-action pair
(s, a) ∈ S × A, such that the L1 distance between the probability of next state of the different MDPs
is at least δ, i.e. k(PMι 一 Pm2 )(∙ | s,a)∣k ≥ δ.
The following theorem shows the sim-to-real gap of ∏DR in δ-separated MDP sets.
Theorem 1. Under Assumption 1 andAssumption 2, for any M ∈ U, the sim-to-real gap of πDR is
at most
Gap(∏DR) = O ( DM"H4log2 (SMH0 ).	⑶
The proof of Theorem 1 is deferred to Appendix D. Though the dependence on M and δ may not be
tight, our bound has only poly-logarithmic dependence on the horizon H.
The main difficulty to prove Theorem 1 is that we do not know what ∏DR does exactly despite
knowing a simple and clean strategy in the real-world interaction with minimum sim-to-real gap.
That is, to firstly visit the state-action pairs that help the agent identify the environment quickly and
then follow the optimal policy in the real MDP M* after identifying M*. Therefore, we use a novel
constructive argument in the proof. We construct a base policy that implements the idea mentioned
above, and show that ∏Dr cannot be much worse than the base policy. The proof overview can be
found in Section 6.
5.2	Finite Simulator Class Without S eparation Condition
Now we generalize the setting and study the sim-to-real gap of ∏DR when U is finite but not necessary
a δ-separated MDP set. Surprisingly, we show that ∏Dr can achieve O(√H) sim-to-real gap when
|U | = M.
Theorem 2. Under Assumption 1, when the MDP set induced by domain randomization U is a finite
set with cardinality M, the sim-to-real gap of πDR is upper bounded by
Gap(∏DR) = O(DPM3Hlog(MH)) .	(4)
Theorem 2 is proved in Appendix E. This theorem implies the importance of randomization and
memory in the domain randomization algorithms (Sadeghi & Levine, 2016; Tobin et al., 2017; Peng
et al., 2018; OpenAI et al., 2018). With both of them, we successfully reduce the worst possible
gap of ∏DR from the order of H to the order of √H, so per step loss will be only O(HT/2).
Without randomization, it is not possible to reduce the worst possible gap (i.e., the sim-to-real gap)
because the policy is even not trained on all environments. Without memory, the policy is not able
to implicitly “identify” the environments, so it cannot achieve sublinear loss in the worst case.
We also use a constructive argument to prove Theorem 2. However, it is more difficult to construct
the base policy because we do not have any idea to minimize the gap without the well-separated
condition (Assumption 2). Fortunately, we observe that the base policy is also a memory-based
6
Published as a conference paper at ICLR 2022
policy, which basically can be viewed as an algorithm that seeks to minimize the sim-to-real gap in
an unknown underlying MDP in U . Therefore, we connect the sim-to-real gap of the base policy
with the regret bound of the algorithms in infinite-horizon average-reward MDPs (Bartlett & Tewari,
2012; Fruit et al., 2018; Zhang & Ji, 2019). The proof overview is deferred to Section 6.
To illustrate the hardness of minimizing the worst case gap, We prove the following lower bound for
Gap(∏, U) to show that any policy must suffer a gap at least Ω(√H).
Theorem 3. Under Assumption 1, suppose A ≥ 10, SA ≥ M ≥ 100, D ≥ 20 logA M, H ≥ DM,
for any history dependent policy π = {πh : trajh → A}hH=1, there exists a set of M MDPs
U = {Mm}M=ι and a choice of M* ∈ U such that Gap(π) is at least Ω(，DMH).
The proof of Theorem 3 follows the idea of the lower bound proof for tabular MDPS (JakSch et al.,
2010), which we defer to Appendix G.2. This lower bound implies that Ω(√H) sim-to-real gap is
unavoidable for the policy ∏DR when directly transferred to the real environment.
5.3	Infinite Simulator Class
In real-world scenarios, the MDP class is very likely to be extensively large. For instance, many
physical parameters such as surface friction coefficients and robot joint damping coefficients are
sampled uniformly from a continuous interval in the Dexterous Hand Manipulation algorithms (Ope-
nAI et al., 2018). In these cases, the induced MDP setU is large and even infinite. A natural question
is whether we can extend our analysis to the infinite simulator class case, and provide a correspond-
ing sim-to-real gap.
Intuitively, since the domain randomization approach returns the optimal policy in the average man-
ner, the policy ∏D R can perform bad in the real world M* if most MDPs in the randomized set differ
much with M*. In other words, U must be ”smooth” near M* for domain randomization to return
a nontrivial policy. By ”smoothness”, we mean that there is a positive probability that the uniform
distribution ν returns a MDP that is close to M*. This is because the probability that ν samples
exactly M* in a infinite simulator class is 0, so domain randomization cannot work at all if such
smoothness does not hold.
Formally, we assume there is a distance measure d(M1, M2) on U between two MDPs M1 and
M2. Define the ^-neighborhood Cm*,∈ of M* as Cm*,∈ = {M ∈ U : d(M, M*) ≤ e}. The
smoothness condition is formally stated as follows:
Assumption 3 (Smoothness near M*). There exists a positive real number 0, and a Lipchitz con-
Stant L, such that for the policy πD r, the value function of any two MDPs in Cm*® is L-Lipchitz
w.r.t the distance function d, i.e.
IVMDR,1(s1)- VMD2,1(s1)∣ ≤ L ∙ d(M1,M2),∀M1,M2 ∈ Cm*后.	(5)
For example, we can set d(M1, M2) = I[M1 6= M2] in the finite simulator class. For complicated
simulator class, we need to ensure there exists some d(∙, ∙) that L is not large.
With Assumption 3, it is possible to compute the sim-to-real gap ofπD*R. In the finite simulator class,
we have shown that the gap depends on M polynomially, which can be viewed as the complexity of
U. The question is, how do we measure the complexity of U when it is infinitely large?
Motivated by Ayoub et al. (2020), we consider the function class
F = {fM(s,a,λ) : S × A × Λ → R such that fM(s, a, λ) = PMλ(s, a) forM ∈U,λ ∈ Λ},
(6)
where Λ = {λ*M, M ∈ U} is the optimal bias functions of M ∈ U in the infinite-horizon average-
reward setting (Bartlett & Tewari (2012); Fruit et al. (2018); Zhang & Ji (2019)). We note this
function class is only used for analysis purposes to express our complexity measure; it does not
affect the domain randomization algorithm. We use the the -log-covering number and the -eluder
dimension of F to characterize the complexity of the simulator class U. In the setting of linear
combined models (Ayoub et al., 2020), the -log-covering number and the -eluder dimension are
7
Published as a conference paper at ICLR 2022
(7)
O (d log(1/)), where d is the dimension of the linear representation in linear combined models.
For readers not familiar with eluder dimension or infinite-horizon average-reward MDPs, please see
Appendix A for preliminary explanations.
Here comes our bound of sim-to-real gap for the infinite simulator class setting, which is proved in
Appendix F.
Theorem 4. Under Assumption 1 and 3, the sim-to-real gap of the domain randomization policy
πDR is at mostfor 0 ≤ e < ∈o
C / *、C( DPdeHiOgCH7N(FW)I ɪ τ
Gap(πDR) = O (-----------V(CMa-------------+
Here ν(CM*,e) is the probability of V sampling a MDP in cm*,& de = dim E (F, 1/H) is the 1/H -
eluder dimension F, and N(F, 1/H) is the 1/H -covering number ofF w.r.t. L∞ norm.
Theorem 4 is a generalization of Theorem 2, since we can reduce Theorem 4 to Theorem 2 by setting
d(M1, M2) = I[M1 6= M2] and = 0, in which case ν(CM*,) = 1/M and de ≤ M.
The proof overview can be found in Section 6. The main technique is still a reduction to the regret
minimization problem in infinite-horizon average-reward setting. We construct a base policy and
shows that the regret of it is only O(√H). A key point to note is that our construction of the base
policy also solves an open problem of designing efficient algorithms that achieve O( √T) regret in
the infinite-horizon average-reward setting with general function approximation. This base policy is
of independent interests.
To complement our positive results, we also provide a negative result that even if the MDPs in
U have nice low-rank properties (e.g., the linear low-rank property (Jin et al., 2020b; Zhou et al.,
2020)), the policy ∏Dr returned by the domain randomization oracle can still have Ω(H) sim-to-real
gap when the simulator class is large and the smoothness condition (Assumption 3) does not hold.
This explains the necessity of our preconditions. Please refer to Proposition 2 in Appendix G.3 for
details.
6	Proof Overview
In this section, we will give a short overview of our novel proof techniques for the results shown in
section 5. The main proof technique is based on reducing the problem of bounding the sim-to-real
gap to the problem of constructing base policies. In the settings without separation conditions, we
further connect the construction of the base policies to the design of efficient learning algorithms for
the infinite-horizon average-reward settings.
6.1	Reducing to Constructing Base Policies
Intuitively, if there exists a base policy ∏ ∈ Π with bounded sim-to-real gap, then the gap of ∏DR
will not be too large since πD*R defined in Eqn 2 is the policy with the maximum average value.
Lemma 1. Suppose there exists a policy π ∈ Π such that the sim-to-real gap of π for any MDP
M ∈ U satisfies VM 1(s1) 一 VM ι(sι) ≤ C,then we have Gap(πDR) ≤ MC when U is a finite
set with |U | = M. Furthermore, when U is an infinite set satisfying the smoothness condition
(assumption 3), we havefor any 0 < e < ∈o, Gap(πDR) ≤ C/ν (CM*,e) + Le.
We defer the proof to Appendix B.1. Now with this reduction lemma, the remaining problem is
defined as follows: Suppose the real MDP M* belongs to the MDP set U. We know the full
information (transition matrix) of any MDP in the MDP set U. How to design a history-dependent
policy π ∈ Π with minimum sim-to-real gap maxM∈u (VM,ι(sι) 一 VMM,ι(sι)).
6.2	The Construction of the Base Policies
With separation conditions With the help of Lemma 1, we can bound the sim-to-real gap in the
setting of finite simulator class with separation condition by constructing a history-dependent policy
8
Published as a conference paper at ICLR 2022
∏. The formal definition of the policy ∏ can be found in Appendix C.1. The idea of the construction
is based on elimination: the policy ∏ explicitly collects samples on the “informative” state-action
pairs and eliminates the MDP that is less likely to be the real MDP from the candidate set. Once the
agent identifies the real MDP representing the dynamics of the physical environment, it follows the
optimal policy of the real MDP until the end of the interactions.
Without separation conditions The main challenge in this setting is that, we can no longer con-
struct a policy ∏ that “identify” the real MDP using the approaches as in the settings with separation
conditions. In fact, we may not be able to even “identify” the real MDP since there can be MDPs in
U that is very close to real MDP. Here, we use a different approach, which reduces the minimization
of sim-to-real gap of ∏ to the regret minimization problem in the infinite-horizon average-reward
MDPs.
The infinite-horizon average-reward setting has been well-studied (e.g., Jaksch et al., 2010;
Agrawal & Jia, 2017; Fruit et al., 2018; Wei et al., 2020). The main difference compared
with the episodic setting is that the agent interacts with the environment for infinite steps.
The gain of a policy is defined in the average manner. The value of a policy π is defined
as ρπ(s) = E[limT →∞ PtT=1 R(st, π (st))/T | s1 = s]. The optimal gain is defined as
ρ* (S) = maxs∈s max∏ ρπ (S), which is shown to be state-independent in Agrawal & Jia (2017),
so We use ρ* for short. The regret in the infinite-horizon setting is defined as Reg(T) =
[Tρ*- PT=1 R(st,at)]
E
, where the expectation is over the randomness of the trajectories. A
more detailed explanation of infinite-horizon average-reward MDPs can be found in Appendix A.1.
For an MDP M ∈ U , we can view it as a finite-horizon MDP with horizon H ; or we can view it
as an infinite-horizon MDP. This is because Assumption 1 ensures that the agent can travel to any
state from any state SH encountered at the H-th step (this may not be the case in the standard finite-
horizon MDPs, since people often assume that the states at the H -th level are terminating state). The
following lemma shows the connection between these two views.
Lemma 2. For a MDP M, let PM and VM,1(s1) to be the optimal expected gain in the infinite-
horizon view and the optimal value function in the episodic view respectively. We have the following
inequality: HPM 一 D ≤ VM,ι(sι) ≤ HPM + D.
This lemma indicates that, if We can design an algorithm (i.e. the base policy) ∏ in the infinite-
horizon setting with regret Reg(H), then the sim-to-real gap of this algorithm in episodic setting
satisfies Gap(∏) = VM,1(s1) 一 VM,ι(sι) ≤ Reg(H) + D. This lemma connects the sim-to-real
gap of π in finite-horizon setting to the regret in the infinite-horizon setting.
With the help of Lemma 1 and 2, the remaining problem is to design an efficient exploration algo-
rithm for infinite-horizon average-reward MDPs with the knowledge that the real MDP M* belongs
to a known MDP set U . Therefore, we propose two optimistic-exploration algorithms (Algorithm 3
and Algorithm 4) for the setting of finite simulator class and infinite simulator class respectively.
The formal definition of the algorithms are deferred to Appendix C.2 and Appendix C.3. Note that
our Algorithm 4 is the first efficient algorithm with O( √T) regret in the infinite-horizon average-
reward MDPs with general function approximation, which is of independent interest for efficient
online exploration in reinforcement learning.
7	Conclusion
In this paper, we study the optimality of policies learned from domain randomization in sim-to-real
transfer without real-world samples. We propose a novel formulation of sim-to-real transfer and
view domain randomization as an oracle that returns the optimal policy of an LMDP with uniform
initialization distribution. Following this idea, we show that the policy πDR can suffer only o(H)
loss compared with the optimal value function of the real environment when the simulator class is
finite or satisfies certain smoothness condition, thus this policy can perform well in the long-horizon
cases. We hope our formulation and analysis can provide insight to design more efficient algorithms
for sim-to-real transfer in the future.
9
Published as a conference paper at ICLR 2022
8	Acknowledgments
Liwei Wang was supported by National Key R&D Program of China (2018YFB1402600), Ex-
ploratory Research Project of Zhejiang Lab (No. 2022RC0AN02), BJNSF (L172037), Project
2020BD006 supported by PKUBaidu Fund.
References
Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret
bounds. arXiv preprint arXiv:1705.07041, 2017.
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement
learning with value-targeted regression. In International Conference on Machine Learning, pp.
463-474. PMLR, 2020.
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient Q-learning with low
switching cost. arXiv preprint arXiv:1905.12849, 2019.
Peter L Bartlett and Ambuj Tewari. Regal: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. arXiv preprint arXiv:1205.2661, 2012.
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrish-
nan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain
adaptation to improve efficiency of deep robotic grasping. In 2018 IEEE international conference
on robotics and automation (ICRA), pp. 4243T250. IEEE, 2018.
Peter Buchholz and Dimitri Scheftelowitsch. Computation of weighted sums of rewards for concur-
rent MDPs. Mathematical Methods of Operations Research, 89(1):142, 2019.
Iadine Chades, Josie Carwardine, Tara G Martin, Samuel Nicol, Regis Sabbadin, and Olivier BUf-
fet. MOMDPs: a solution for modelling adaptive management problems. In Twenty-Sixth AAAI
Conference on Artificial Intelligence, 2012.
Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff,
and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world
experience. In 2019 International Conference on Robotics and Automation (ICRA), pp. 8973—
8979. IEEE, 2019.
Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model. arXiv preprint arXiv:1610.03518, 2016.
Mark Cutler and Jonathan P How. Efficient reinforcement learning for robots using informative
simulated priors. In 2015 IEEE International Conference on Robotics and Automation (ICRA),
pp. 2605-2612. IEEE, 2015.
Carl Doersch and Andrew Zisserman. Sim2real transfer learning for 3d human pose estimation:
motion to the rescue. Advances in Neural Information Processing Systems, 32:12949-12961,
2019.
Fei Feng, Wotao Yin, and Lin F Yang. How does an approximate model help in reinforcement
learning? arXiv preprint arXiv:1912.02986, 2019.
Dario Floreano, Phil Husbands, and Stefano Nolfi. Evolutionary robotics. Technical report, Springer
Verlag, 2008.
Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner. Efficient bias-span-constrained
exploration-exploitation in reinforcement learning. In International Conference on Machine
Learning, pp. 1578-1586. PMLR, 2018.
Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric. Improved analysis of UCRL2 with empirical
bernstein inequality. arXiv preprint arXiv:2007.05456, 2020.
10
Published as a conference paper at ICLR 2022
Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint
arXiv:1910.11956, 2019.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(4), 2010.
Nan Jiang. PAC reinforcement learning with an imperfect model. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 32, 2018.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? arXiv preprint arXiv:1807.03765, 2018.
Chi Jin, Sham M Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement
learning of undercomplete POMDPs. arXiv preprint arXiv:2006.12484, 2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143.
PMLR, 2020b.
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman Eluder dimension: New rich classes of rl
problems, and sample-efficient algorithms. arXiv preprint arXiv:2102.00815, 2021.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101(1-2):99-134, 1998.
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal of Robotics Research, 32(11):1238-1274, 2013.
Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for
reinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203,
2021.
Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. RL for latent MDPs:
Regret guarantees and a lower bound. arXiv preprint arXiv:2102.04939, 2021.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.
Ajay Mandlekar, Danfei Xu, Roberto Martln-Martin, Silvio Savarese, and Li Fei-Fei. Learn-
ing to generalize across long-horizon tasks from human demonstrations. arXiv preprint
arXiv:2003.06085, 2020.
Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement learning for de-
formable object manipulation. In Conference on Robot Learning, pp. 734-743. PMLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and
Eric Liang. Autonomous inverted helicopter flight via reinforcement learning. In Experimental
robotics IX, pp. 363-372. Springer, 2006.
Haoyi Niu, Jianming Hu, Zheyu Cui, and Yi Zhang. DR2L: Surfacing corner cases to robus-
tify autonomous driving via domain randomization reinforcement learning. arXiv preprint
arXiv:2107.11762, 2021.
OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, RafaI Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, Jonas Schneider, Szy-
mon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba. Learning dexterous
in-hand manipulation. CoRR, 2018. URL http://arxiv.org/abs/1808.00177.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the Eluder dimension.
arXiv preprint arXiv:1406.1853, 2014.
11
Published as a conference paper at ICLR 2022
Remi PaUtraL Konstantinos Chatzilygeroudis, and Jean-BaPtiste Mouret. Bayesian optimization
with automatic prior selection for data-efficient direct policy search. In 2018 IEEE International
Conference on Robotics andAutomation (ICRA), pp. 7571-7578. IEEE, 2018.
Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of
robotic control with dynamics randomization. In 2018 IEEE international conference on robotics
and automation (ICRA), pp. 3803-3810. IEEE, 2018.
Soren Pirk, Karol Hausman, Alexander Toshev, and Mohi Khansari. Modeling long-horizon tasks
as sequential interaction landscapes. arXiv preprint arXiv:2006.04843, 2020.
Samira Pouyanfar, Muneeb Saleem, Nikhil George, and Shu-Ching Chen. ROADS: Randomization
for obstacle avoidance and driving in simulation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 0-0, 2019.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic
exploration. In NIPS, pp. 2256-2264. Citeseer, 2013.
Andrei A Rusu, Matej Vecer´k, Thomas Rothorl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell.
Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot Learning,
pp. 262-270. PMLR, 2017.
Fereshteh Sadeghi and Sergey Levine. Cad2rl: Real single-image flight without a single real image.
arXiv preprint arXiv:1611.04201, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. nature, 550(7676):354-359, 2017.
Richard D Smallwood and Edward J Sondik. The optimal control of partially observable Markov
processes over a finite horizon. Operations research, 21(5):1071-1088, 1973.
Lauren N Steimle, David L Kaufman, and Brian T Denton. Multi-model Markov decision processes.
IISE Transactions, pp. 1-16, 2021.
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and
Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint
arXiv:1804.10332, 2018.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23-30.
IEEE, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Nikos Vlassis, Michael L Littman, and David Barber. On the computational complexity of stochastic
controller optimization in POMDPs. ACM Transactions on Computation Theory (TOCT), 4(4):
1-8, 2012.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Reinforcement learning with general
value function approximation: Provably efficient approach via bounded Eluder dimension. arXiv
preprint arXiv:2005.10804, 2020.
Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-
free reinforcement learning in infinite-horizon average-reward Markov decision processes. In
International Conference on Machine Learning, pp. 10170-10180. PMLR, 2020.
12
Published as a conference paper at ICLR 2022
Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, and Rahul Jain. Learning infinite-horizon
average-reward MDPs with linear function approximation. In International Conference on Artifi-
Cial Intelligence and Statistics,pp. 3007-3θ15. PMLR, 2021.
Yi Xiong, Ningyuan Chen, Xuefeng Gao, and Xiang Zhou. Sublinear regret for learning POMDPs.
arXiv preprint arXiv:2107.03635, 2021.
Wenhao Yu, C Karen Liu, and Greg Turk. Policy transfer with strategy optimization. arXiv preprint
arXiv:1810.05751, 2018.
Zihan Zhang and Xiangyang Ji. Regret minimization for reinforcement learning by evaluating the
optimal bias function. arXiv preprint arXiv:1906.05110, 2019.
Yuren Zhong, Aniket Anand Deshmukh, and Clayton Scott. PAC reinforcement learning without
real-world feedback. arXiv preprint arXiv:1909.10449, 2019.
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learn-
ing for linear mixture Markov decision processes. arXiv preprint arXiv:2012.08507, 2020.
A Additional Preliminaries
A. 1 Infinite-horizon Average-reward MDPs
The infinite-horizon average-reward setting has been well-explored in the recent few
years (e.g. Jaksch et al. (2010); Agrawal & Jia (2017); Fruit et al. (2018); Wei et al. (2020)). The
main difference compared with the episodic setting is that the agent interacts with the environment
for infinite steps instead of restarting every H steps. The gain of a policy is defined in the average
manner.
Definition 2. (Definition 4 in Agrawal & Jia (2017)) The gain ρπ (s) of a stationary policy π from
starting state s1 = s is defined as:
1T
ρπ(S) = E Iim 不 ER(St尸(St)) | si = S	⑻
T→∞ T
t=1
In this setting, a common assumption is that the MDP is communicating (Assumption 1). Under this
assumption, we have the following lemma.
Lemma 3. (Agrawal & Jia, 2017, Lemma 2.1) Fora communicating MDP M with diameter D: (a)
The optimal gain ρ* is state-independent and is achieved by a deterministic stationary policy πDr；
that is, there exists a deterministic policy π* such that
ρP := max max ρπ (s0) = ρπ (s),∀s ∈ S	(9)
s0∈S π
(b) The optimal gain ρ* satisfies the following equation:
ρ	* = min max [R(s, a) + Pλ(s, a) — λ(s)] = max [R(s, a) + Pλ*(s, a) — λ*(s)] Xs	(10)
λ∈RS s,a	a
where Pλ(s, a) = £§, P (s0∣s, a)λ(s0), and λ* is the bias vectorofthe optimal policy π D R satisfying
0 ≤ λ*(s) ≤ D.	(11)
The regret minimization problem has been widely studied in this setting, with regret to be defined
as Reg(T) = E [Tρ* — PT=I R(St) at)], where the expectation is over the randomness of the
trajectories. For example, JakSch et al. (2010) proposed an efficient algorithm called UCRL2, which
achieves regret upper bound O(DS,AT). For notation convenience, We use PV(s, a) or Pλ(S, a)
as a shorthand of Ps0∈S P(S0|S, a)V(S0) or Ps0∈S P(S0|S, a)λ(S0).
13
Published as a conference paper at ICLR 2022
A.2 Eluder Dimension
Proposed by Russo & Van Roy (2013), eluder dimension has become a widely-used concept to
characterize the complexity of different function classes in bandits and RL (Wang et al., 2020;
Ayoub et al., 2020; Jin et al., 2021; Kong et al., 2021). In this work, we define eluder dimension to
characterize the complexity of the function F:
F = {fM(s,a,λ) : S × A × Λ → R such that fM(s, a, λ) = PMλ(s, a) forM ∈ U,λ ∈ Λ},
(12)
where Λ = {λM, M ∈ U} is the optimal bias functions of M ∈ U in the infinite-horizon average-
reward setting (Bartlett & Tewari (2012); Fruit et al. (2018); Zhang & Ji (2019)).
Definition 3. (Eluder dimension). Let ≥ 0 and Z = {(si, ai, λi)}in=1 ⊂ S × A × Λ be a sequence
of history samples.
•	A history sample (s, a, λ) ∈ S × A × Λ is -dependent on Z with respect to F if any
f,f∈F satisfying, kf - f0kz ≤ e also satisfies kf (s, a) - f0(s, a)k ≤ e. Here ∣∣f - f0∣∣z
is a shorthand of P(s,a,λ)∈Z (f - f0)2(s, a, λ).
•	An (s, a, λ) is -independent ofZ with respect to F if (s, a, λ) is not -dependent on Z.
•	The -eluder dimension of a function class F is the length of the longest sequence of el-
ements in S × A × Λ such that, for some 0 ≥ , every element is 0-independent of its
predecessors.
B Omitted Proof in Section 6
B.1 Proof of Lemma 1
Proof. We firstly study the case where U is a finite set with |U| = M. For ∏, we have
	M X (VM,1 (SI)- VM,1(SI)) ≤ C.	(13) M M∈U
By the optimality of ∏Dr, We know that
	M X v⅛ (si) ≥ M X VM,i(si) .	(14) M∈U	M∈U
Therefore,	M X (VM,1 (si) - v⅛(si)) ≤ C.	(15) M M∈U
Since the gap VM,1(s1) — VM^(SI) ≥ 0 for any i ∈ [M],wehave 焉(VM*,ι(sι) - VMDRJ(Sι)) ≤
C. That is,	(VM*,i(si)- VMDRJ(S1)) ≤ MC.	(16)
For the case where U is an infinite set satisfying Assumption 3, by the optimality of ∏Dr, we have
EM~ν hVMDRL(SI)i ≥ EM~ν [VM,1(s1)] .	(17)
Therefore,
EM〜V(CMk) hVM*,1(s1) - VMDRι(sι)i ≤ Em”]KM*,1(s1) - V⅛(sι)] ≤ Em” [VM*,1(s1) - VM,1(s1)].
(18)
14
Published as a conference paper at ICLR 2022
By Assumption 3, for any M ∈ C(M*, e), We have
I V∏DRj(SI)- V⅛RI(Sι)∣≤ Le.	(19)
Therefore, We have
V (CM*,e) (VM*,1(s1)- V：MDR,i(si) - Le) ≤ Em~“(CmM [VM*∕ι) — V：MDRi(si)i	Q。)
Combining Inq 18 and Inq 20, We have
v (CM*,e)(VM*,1(s1)- VnDRJ(si) — Le) ≤ C,	(21)
The lemma can be proved by reordering the above inequality.	口
B.2 Proof of Lemma 2
Proof. For MDP M, denote π↑n as the optimal policy in the infinite-horizon setting and {n；P h}H=ι
as the optimal policy in the episodic setting. By the optimality of π%, we have VM,1(s1) =
V⅛(sι) ≥ V⅛(sι).
By the Bellman equation in the infinite-horizon setting, we know that
λM(s) + PM = R(s,∏tn(s))+ PMλM(s,∏in(s)), ∀S ∈ S	(22)
For notation simplicity, we use dh(S1, π) to denote the state distribution at step h after starting from
state S1 at step 1 following policy π. From the above equation, we have
H
λM(SI) + HρM = E Esh~dh(sι Fn)R(Sh, , πin(Sh)) + Es Hs+-ι^dH+-ι(,sι,∏'*n))λM(s H+1). (23)
h=1
That is,
H
|ESh~dh(sιΕn)R(Sh, Kn(Sh))- HρM | = DM (SI)- ESH+ι-dH+ι (s1,π^n)λM(sH+1 )| ≤ D,
h=1
(24)
where PH=IEsh~dh(sι,啧)R(s%, , ∏in(Sh)) = V⅛(si)∙ Therefore, we have HPM - D ≤
ττ*
VMi(Si) ≤ VM,i(si).
For the second inequality, by the Bellman equation in the infinite-horizon setting, we have
H
λM(SI) + HPM ≥〉: Esh~dh(sι Gp)R(Sh, , πep,h(Sh)) + ESH+1 ~dH+ι(siEp)λM(SH+1).
h=i
(25)
That is,
H
E E ESh~dh(siEp)R(Sh, , πep,h (Sh))- HpM ≤ λM (SI)- EsH + 1~dH+1 (s1 ,πep)λM(SH+1) ≤ D,
h=i
(26)
where PH=IEsh~dh(siEp)R(Sh，，Kp,h(Sh)) = VM,l(si).	口
C	The Construction of Learning Algorithms
C.1 Finite S imulator Class with Separation Condition
In this subsection, we explicitly define the base policy ∏ with sim-to-real gap guarantee under the
separation condition. Note that a history-dependent policy for LMDPs can also be regarded as an
15
Published as a conference paper at ICLR 2022
Algorithm 1 Optimistic Exploration Under Separation Condition
1:	Initialize: the MDP set D = U, no = c0 log2(SMH)log(MH) for a constant c0
2:	. Stage 1: Explore and find the real MDP M*
3:	while |D| ≥ 1 do
4:	Randomly select two MDPs M1 and M2 from the MDP set D
5:	Choose (so,。。) = argmaX(s,α)∈s×A	∣∣(Pmi - Pm?) (∙ | s,a)kι
6:	Call Subroutine 2 with parameter (s0,	a0) and n0 to collect history samples	HM1,M2
7:	if ∃s0 ∈HM1,M2,PM2 (SlSo,ao) =	0 or Qs0∈Hm1,m2 PM⅛⅛⅛	≥	1	then
8:	Eliminate M2 from the MDP set D
9:	else
10:	Eliminate M1 from the MDP set D
11:	end if
12:	end while
13:	. Stage 2: Run the optimal policy of M*
14:	Denote M as the remaining MDP in the MDP set D
15:	RUn the optimal policy of M for the remaining steps
Algorithm 2 Subroutine: collecting data for (so, ao)
InpUt: informative state-action pair (so, ao), reqUired visitation coUnt no
Initialize: counter N(s。, a。) = 0, history data H = 0
Denote πM(s, s0) as the policy with the minimUm expected travelling time E [T (s0 | M, π, s)]
for MDP M (Defined in Assumption 1)
while N(s。, a。) ≤ n。 do
5:	for i = 1,…，M do
Denote the current state as sinit
Run the policy πMi (sinit, s。) for 2D steps, breaking the loop immediately once the
agent enters state s。
end for
if the agent enters state s。 then
10:	Execute a。, enter the next state s0.
counter N(s。, a。) = N(s。, a。) + 1, H = H S{s0}
end if
end while
Output: history data H
16
Published as a conference paper at ICLR 2022
algorithm for finite-horizon MDPs. By deriving an upper bound of the sim-to-real gap for π, We can
upper bound Gap(πjDR, U) with Lemma 1.
The policy ∏ is formally defined in Algorithm 1. There are two stages in Algorithm 1. In the first
stage, the agent,s goal is to quickly explore the environment and find the real MDP M* from the
MDP set U . This stage contains at most M - 1 parts. In each part, the agent randomly selects two
MDPs M1 and M2 from the remaining MDP set D. Since the agent knows the transition dynamics
of M1 and M2, it can find the most informative state-action pair (s0, a0) with maximum total-
variation difference between Pm、(∙∣so,αo) and Pm2 (∙∣so, ao). The algorithm calls Subroutine 2
to collect enough samples from (s0, a0) pairs, and then eliminates the MDP with less likelihood.
At the end of the first stage, the MDP set D is ensured to contain only one MDP M* with high
probability. Therefore, the agent can directly execute the optimal policy for the real MDP till step
H + 1 in the second stage.
Subroutine 2 is designed to collect enough samples from the given state-action pair (s0, a0). The
basic idea in Subroutine 2 is to quickly enter the state s0 and take action a0, until the visitation
counter N(s0, a0) = n0. Denote πM(s, s0) as the policy with the minimum expected travelling
time E [T (s0 | M, π, s)] for MDP M. Suppose the agent is currently at state sinit and runs the
policy ∏M*(sinit, so) in the following steps. By Assumption 1 and Markov,s inequality, the agent
will enter state s0 in 2D steps with probability at least 1/2. Therefore, in Subroutine 2, we runs the
policy πMi (sinit, s0) for 2D steps for i ∈ [M] alternatively. This ensures that the agent can enter
state s0 in 2MD steps with probability at least 1/2.
Theorem 5 states an upper bound of the sim-to-real gap for Algorithm 1, which is proved in Ap-
pendix D.1.
Theorem 5. Suppose we use π to denote the history-dependent policy represented by Algorithm 1.
Under Assumption 1 and Assumption 2, for any M ∈ U, the sim-to-real gap of Algorithm 1 is at
most
VM* ,1(s1) - VMπ ,1(s1) ≤ O
DM2 log(MH)log2 (SMH∕δ)
δ4
(27)
C.2 Finite S imulator Class without S eparation Condition
In this subsection, we propose an efficient algorithm in the infinite-horizon average-reward setting
for finite simulator class. Our algorithm is described in Algorithm 3. In episode k, the agent executes
the optimal policy of the optimistic MDP Mk with the maximum expected gain ρ*Mk. Once the
agent collects enough data and realizes that the current MDP Mk is not M* that represents the
dynamics of the real environment, the agent eliminates Mk from the MDP set.
Algorithm 3 Optimistic Exploration
1:	Initialize: the MDP set U = U, the episode counter k = 1, ho = 1
2:	Calculate M1 = arg maxM∈U ρ*M
3:	for step h = 1,…，H do
4:	Take action ah = πM* k (sh), obtain the reward R(sh, ah), and observe the next state sh+1
5:	if Pt=h0 (PMkλMk(St, at)- λMk(St+1)) I > Dp2(h - hO)Iog(2HM) then
6:	Eliminate Mk from the MDP set Uk, denote the remaining set as Uk+1
7:	Calculate Mk+1 = arg maxM∈Uk+1 ρ*M
8:	Set ho = h+ 1,andk = k+ 1.
9:	end if
10:	end for
To indicate the basic idea of the elimination condition defined in Line 5 of Algorithm 3, we briefly
explain our regret analysis of Algorithm 3. Suppose the MDP Mk selected in episode k satisfies the
17
Published as a conference paper at ICLR 2022
optimistic condition PMk ≥ ρM*, then the regret in H steps can be bounded as:
H
hPM* -〉： R(Sh,ah)	(28)
h=1
K τ(k+1)-1
≤ X X	(PMk- R(Sh,ah))	(29)
k=1 h=τ (k)
K τ(k+1)-1
=X X	(PMkλMk(sh,ah)- λMk(Sh))	(30)
k=1 h=τ (k)
K τ(k+1)-1	K
=X X	(PMk λMk(sh, ah) - λMk (Sh+1)) + X (λMk (ST (k+1) ) - λMk (ST (k))) (31)
k=1 h=τ (k)	k=1
K T(k+1)-1
≤ X X	(PMk λMk (sh,ah) - λMk (Sh+1))+ KD.	(32)
k=1 h=T (k)
Here we use K to denote the total number of episodes, and we use τ (k) to denote the first step of
episode k. The first inequality is due to the optimistic condition PMk ≥ ρM*. The first equality
is due to the Bellman equation in the finite-horizon setting (Eqn 10). The last inequality is due to
0 ≤》M(s) ≤ D. From the above inequality, We know that the regret in episode k depends on
the summation Ph=+；)-1 (PMk 入黄七(s八, ah - λMk(Sh,+ι)). If this term is relatively small, we
can continue following the policy ∏Mk with little loss. Since λMk (sh+i) is an empirical sample
of Pm*λMk (Sh, ah), we can guarantee that Mk is not M* with high probability if this term is
relatively large.
Based on the above discussion, we can upper bound the regret of Algorithm 3. We defer the proof
of Theorem 6 to Appendix E.1.
Theorem 6.	Under Assumption 1, the regret of Algorithm 3 is upper bounded by
Reg(H) ≤ O (DpMHlogMH) .	(33)
C.3 Infinite S imulator Class
In this subsection, we propose a provably efficient model-based algorithm solving infinite-horizon
average-reward MDPs with general function approximation. To the best of our knowledge, our
result is the first efficient algorithm with near-optimal regret for infinite-horizon average-reward
MDPs with general function approximation.
Considering the model class U which covers the true MDP M *, i.e. M * ∈ U, we define the function
space Λ = {λM, M ∈ U}, and space X = S ×A× Λ. We define the function space
F = {fM(S, a, λ) : X → R such that fM(S, a, λ) = PMλ(S, a) for M ∈ U, λ ∈ Λ} .	(34)
Our algorithm, which is described in Algorithm 4, also follows the well-known principle of optimism
in the face of uncertainty. In each episode k, we calculate the optimistic MDP Mk with maximum
expected gain P*Mk . We execute the optimal policy of M* to interact with the environment and
collect more samples. Once we have collected enough samples in episode k, we update the model
class Uk and compute the optimistic MDP for episode k + 1.
Compared with the setting of episodic MDP with general function approximation (Ayoub et al.,
2020; Wang et al., 2020; Jin et al., 2021), the additional problem in the infinite-horizon setting is
that the regret technically has linear dependence on the number of total episodes, or the number of
steps that we update the optimistic model and the policy. This corresponds to the last term (KD)
in Inq 32. Therefore, to design efficient algorithm with near-optimal regret in the infinite-horizon
setting, the algorithm should maintain low-switching property (Bai et al., 2019; Kong et al., 2021).
Taking inspiration from the recent work that studies efficient exploration with low switching cost
18
Published as a conference paper at ICLR 2022
kf -f k2
in episodic setting (Kong et al., 2021), We define the importance score, supʌ,f ∈f 口九-fkZ+α，
as a measure of the importance for new samples collected in current episode, and only update the
optimistic model and the policy When the importance score is greater than 1. Here kf1 - f2k2Z is a
shorthand of P(s,a,s0,λ)∈Z (f1 (s, a, λ) - f2(s, a, λ))2.
Algorithm 4 General Optimistic Algorithm
1:	Initialize: the MDP set U = U, episode counter k = 1
2:	Initialize: the history data set Z = 0, Znew = 0
3:	α = 4D2 + 1, β = cD2 log (H ∙ N (F, 1/H)) fora constant c.
4:	Compute M1 = argmaxM*% PM
5:	for step h = 1,…，H do
6:	Take action ah = ∏Mk (Sh) in the current state sh, and transit to state sh+ι
7:	Add (sh,ah,sh+ι,入黄忆)to the set Znew
8:	if importance score supʌ,f ∈f kfl-fjkZneα ≥ 1 then
9:	Add the history data Znew to the set Z
10:	CalCUIate M k+1 = argminM∈u P(sh,ah,sh+1,λh)∈Z (pMλh(Sh,ah) - λh(Sh+1))2
11:	UpdateUk+1 = {M∈U ： P(sh,ah,sh+ι,λh)∈Z((PM - PMk+J λh(s%, a"))2 ≤ β
12:	Compute Mk+1 = arg maxM∈Uk+ι PM
13:	Episode counter k = k + 1
14:	end if
15:	end for
We state the regret upper bound of Algorithm 4 in Theorem 5, and defer the proof of the theorem to
Appendix F.1.
Theorem 7.	Under Assumption 1, the regret of Algorithm 4 is uppder bounded by
Reg(H) ≤ O (DpdeHlOg(H NF, 1/H))) ,	(35)
where de is the E-eluder dimension of function class F with E = H, and N (F, 1/H) is the H -
covering number ofF w.r.t L∞ norm.
For α > 0, We say the covering number N(F, α) of F W.r.t the L∞ norm equals m if there is m
functions in F such that any function in F is at most α aWay from one of these m functions in norm
k ∙ k∞. The k ∙ k∞ norm of function f is defined as ∣∣f k∞ =ef maxχ∈χ |f (x)|.
D Omitted Proof for finite simulator class with separation
CONDITION
D.1 Proof of Theorem 5
The formal definition of ∏ is given in Algorithm 1. To upper bound the sim-to-real gap of ∏, we
discuss on the folloWing tWo benign properties of Algorithm 1 in Lemma 4 and Lemma 7. Lemma 4
states that the true MDP M* will never be eliminated from the MDP set D. Therefore, in stage 2,
the agent will execute the optimal policy in the remaining steps with high probability. Lemma 7
states that the total number of steps in stage 1 is upper bounded by O(M). This is where the final
bound in Theorem 5 comes from.
Lemma 4. With probability at least 1 一 H, the true MDP M* will never be eliminated from the
MDP set D in stage 1.
The while-loop in stage 1 will last for M - 1 times. To prove Lemma 4, we need to prove that, if
the true MDP M* is selected in a certain loop, then Q^ 〃 八S PMEJs，a) ≥ 1 holds with
(s,a,s )∈HM1 ,M2 PM(s |s,a)
high probability. This is illustrated in the following lemma.
19
Published as a conference paper at ICLR 2022
Lemma 5. Suppose H = {si}n= 1 is a set of no = c0 log (SMHH/δ)log(MH) independent samples
from a given state-action pair (so, ao) and MDP M* for a large constant co. Let Mi denote
another MDP satisfying ∣∣(Pm* — Pmi )(∙∣sο, ao)ki ≥ δ, then the following inequality holds with
probability at least 1 一 MH:
π
PM*(s0∣sο,aο)
PMι (s0∣sο, ao)
> 1,
(36)
Proof. The proof of Lemma 5 is inspired by the analysis in Kwon et al. (2021). To prove Inq 36, it
is enough to show that
ln
Y
s0∈H
PM*(s0∣sο,aο)
PMι (s0∣sο, ao)
ln (pm* (s0lsο, ao)
S0∈H	IPMI (SOlso,aO)
> 0.
(37)
holds with probability at least 1 一 MH.
Note that pM*(S∕∣S0,a0) Can be unbounded since Pm、(s0∣so, ao) can be zero for certain s0. To tackle
this issue, We define PMI for a sufficiently small a ≤ 4δS:
Pmi (s0∣s,a) = α + (1 - aS)Pmi (s0∣s, a).	(38)
have ln
We have
1 一 PM1
1
•|s, a)∣∣ ≤ 2Sα ≤ δ, thus
1 — Pm*) (∙∣s,a)∣∣ ≥ 2. Also, we
Pmi (s0∣s0,a0)
≤ ln(1∕a) for any s0 ∈ S. With the above definition, we can decompose
Inq 37 into two terms:
ln
s0∈H
(S0|So, ao)) = in	*(S0|So, ao)) + in
(s0|so, ao)√	§々	1 (s0|so, ao)√	S∈H
J (39)
Taking expectation over s0 〜Pm* (∙∣s,a) for the first term, we have
E
X0^ln PpM* (Silso, ao)
七 ∖PMι (SiISo,ao)
n0
PM* (s0lso, ao) ln
i=1
noD
s0
PM* (s0∣so, ao)
PM1(S0ISo, ao)
(40)
KL(PM* (S0lso, aO)IPMI (S0lso, aO))
(41)
noδ2
≥ F,
where the last inequality is due to Pinsker’s inequality.
(42)
Lemma 6. (Lemma C.2 in Kwon et al. (2021)) Suppose X is an arbitrary discrete random variable
on a finite support X. Then, in(1/P (X)) is a sub-exponential random variable (Vershynin, 2010)
with Orcliz norm in(1/P (X))φ1 = 1/e.
From the above Lemma, we know that both Pm、(s0|so, ao) and Pm* (s0|so, ao) are sub-exponential
random variables. By AzUma-HOeffing's inequality, we have with probability at least 1 — δo∕2,
X in ( ；-------1------) ≥ E
s0∈H	PM1(S0|So, ao)
n0
X in
i=1
—---------)
(S0i|So,ao)
—log(1∕α) VZ2no log(2∕δo). (43)
By Proposition 5.16 in Vershynin (2010), with probability at least 1 — δo∕2,
__	no	____________
X ln(PM* (S0|So,ao)) ≥ E Xln(PM*(Si|So,ao)) — pno log(2∕δo)∕c,	(44)
s0∈H	i=1
20
Published as a conference paper at ICLR 2022
for a certain constant c > 0. Therefore, we can lower bound the first term in Eqn 39,
ln (PM*(S0|s0, aO)
s0∈H	∖PMι (so∣S0,aθ)
≥ nθδ——log(1∕α)Ρ2no log(2∕δ0) - Pn log(2∕δo)∕c,
(45)
with probability at least 1 - δ0.
For the second term in Eqn 39, by the definition of PMi,
XH ln !MITO 一
s ∈H
(46)
Combining Inq 45 and Inq 46, we have
X ln ( PM ； o| 0，0]) ≥ —0-------Iog(I/a)p2no log(2/6O) - pno log(2/Jo)/c - 2αSno
s0∈H	PM1(s0|s0,a0)	2
(47)
Setting α = £ δ0 = MH ,and n0 = c0 log2(S"log(MH), we have
X ln ( PM" (Sls 0,aO) A > O
s⅜H	XPMi(SIs。,。。“
s ∈H
holds with probability at least 1 - MH.
(48)
□
Lemma 7. Suppose Stage 1 in Algorithm 1 ends in h。 steps. We have E[h。]	≤
O(DM log (SMH∕δ)log(MH)), where the expectation is over all randomness in the algorithm and
the environment.
Proof. Recall that πM (S, S0) is the policy with the minimum expected travelling time
E [T (S0 | M, π, S)] for MDP M. By Assumption 1, we have
E[T (s0 | M*,∏m(s, s0), s)] ≤ D.	(49)
Given state S and s0, by Markov,s inequality, we know that with probability at least 11,
T (s0 | M*,∏m* (s, s0), s) ≤ 2D.	(50)
Consider the following stochastic process: In each episode k, the agent starts from a state Sk that
is arbitrarily selected, and run the policy ∏M*(sk, s0) for 2D steps on the MDP M*. The process
terminates once the agent enters a certain state S。. By Inq 50, the probability that the process
terminates within k episodes is at least 1 -壶.By the basic algebraic calculation, the expected
stopping episode can be bounded by a constant 4.
Now we return to the proof of Lemma 7. In Subroutine 2, we run policy πMi for each MDP
Mi ∈ D alternately. By Lemma 4, the true MDP M* is always contained in the MDP set D.
Therefore, the expected travelling time to enter state s0 for n。times is bounded by n。∙ M ∙ 8D.
In stage 1, we call Subroutine 2 for M - 1 times, which means that the expected steps in stage 1
satisfies E[h0] ≤ 8M2n0d = O(DMM log2(SMH^)log(MH)).	口
21
Published as a conference paper at ICLR 2022
Proof. (Proof of Theorem 5) Recall that we use h0 to denote the total steps in stage 1. Firstly, we
prove that Gap(∏, U) is upper bounded by O (E[ho] + D).
VMM*,1 (SI)- VMM*,1 (SI)	(51)
h0	
=Eh0 EM*,∏Μ* ɪ2 R(Sh, ah) | ho + EM*,∏M* [VMM*,h0 + 1 (Sh0 + 1) | ho]	(52)
h=1 h0	
-Eh0 EM*,π ɪ2 R(Sh, αh) | ho + EM* ,π [VM*,h0 + 1(Sh0 + 1) | ho]	(53)
h=1	
≤Eh0 EM*,∏Μ* X R(Sh,ah) | ho J + Eh0 [EM*,∏M* [VMM*,h0 + 1(sh0 + 1) | ho]-	EM*,Π [VM* ,h0 + 1(sh0 + 1) | ho]
	(54)
≤E[ho] + Eh。忸M*,∏Μ* [VMM*,h0 + 1(sh0 + 1) | ho] - EM*,^ [VM*,h0 + 1(sh0 + 1) | ho]	(55)
The outer expectation is over all possible h0, while the inner expectation is over the possible trajec-
tories given fixed h°. By Lemma 4, We know that ∏ = ∏DR after h° steps with probability at least
1 - H. If this high-probability event happens, the second part in the above inequality equals to
E [EM*,∏M* [VM*,ho + 1(Sh0 +1) | ho] - EM*,∏ [VM*,ho + 1(Sh0 +1) | hθ]].
We can prove that this term is upper bounded by 2D. Given fixed h0,
EM*,∏M* [VMM*,ho + 1(sho + 1) | ho] - EM*,∏ [VMM*,ho + 1(sho + 1) | ho] is the difference of the
value function in step h0 + 1 under two different distribution of Sh0+1. We use dh(S, π) to denote
the state distribution in step h after starting from state S in step ho + 1 following policy π.
H
VMM*,ho + 1(ShO +1)=	ESh~dh(sho + ι,∏Μ* )R(Sh,πDR(Sh))
h=h0 +1
(56)
Σ
h=h0 +1
ρM* + Esh〜dh(sh0+1,∏M* )λM* (Sh) - Esh + 1^dh+1(sh0 + 1 ,∏M* )λM* (Sh+1
(57)
=(H - h0)ρM* + λM* (Sh0 +1) - EsH+ι〜dH+1(sh0 + 1,∏M*)λM* (SH+1), (58)
where the second equality is due to the Bellman equation in infinite-horizon setting (Eqn 10). Note
that by the communicating property, we have 0 ≤ λM* (s) ≤ D for any S ∈ S. Therefore, we have
EM*,∏M* [VM*,ho + 1(Sho + 1) | hθ] - EM*,∏ [VM*,ho + 1(Sho + 1) |ho] ≤2D. (59)
If the high-probability event defined in Lemma 4 does not hold (This happens with probability at
most H), we stillhave EM*,∏M* [VMM*,h0 + 1(sho + 1) | ho] -EM*,∏ [VMM* ,h0 + 1(Sh0 + 1) | ho] ≤ H,
thus this does not influence the final bound. Taking expectation over all possible ho, and plugging
the result back to Inq 51, we have
VM*,1(S1) - VM*,1(S1) ≤ O (E[ho] + D) = O (DM2log2(SMδH0log(MH)) .	(60)
□
D.2 Proof of Theorem 1
Proof. The theorem can be proved by combining Theorem 5 and Lemma 1.
By Theorem 5, we prove that the constructed policy ∏ satisfies
VMM*,1(SI)- VMM*,1(SI) ≤ O
DM2 log2 (SMH)log(MH)
δ4
(61)
22
Published as a conference paper at ICLR 2022
By Lemma 1, the sim-to-real gap of policy ∏Dr is bounded by
Gap(∏DR, U) ≤ O
DM3 log2(SMH) log(MH)
δ4
(62)
□
E Omitted proof for finite simulator class without separation
CONDITION
E.1 Proof of Theorem 6
Lemma 8. (Optimism) With probability at least 1 一 MH, we have PMk ≥ ρM* for any k ∈ [K].
Proof. For any fixed M ∈ U, and fixed step h ∈ [H], by Azuma’s inequality, we have with proba-
bility at least 1 — MIH,
h
X (λM (st+1) — Pm*λM (st,at)) ≤ Dp2(h — ho)log(2HM).	(63)
t=h0
Taking union bounds over all possible M and h, we know that the above event holds for all possible
M and h with probability 1 — MH. Under the above event, the true MDP M* will never be
eliminated from the MDP set Uk. Therefore, We have PMk ≥ ρM*.	□
Proof. (Proof of Theorem 6) By Lemma 8, we know that PMk ≥ ρM* for any k ∈ [K]. We use
τ (h) to denote the episode that step h belongs to. We can upper bound the regret in H steps as
follows:
H
hPm* 一〉： R(Sh, ah)
h=1
H
≤ ΣS SMτ(h) 一 R(Sh, Oh))
h=1
H
E (PMMh) λMτ(h) (sh, ah) — λMτ(h) (Sh))
h=1
HH
E (PMτ(h) — PM* ) λMτ(h) (Sh, ah) + E (PM* λMτ(h)
h=1	h=1
(sh, ah) 一 λMτ(h) (Sh))
H
(PMMh) — PM* ) λMτ(h) (Sh, ah) + PM* λMτ(h) (ShO , aho ) — λMτ(h) (SI)
h=1
H-1
+ ∑ (PM* λMτ(h) (Sh,ah) 一 λMτ(h) (Sh+1))
h=1
(64)
(65)
(66)
(67)
(68)
(69)
By Azuma’s inequality, we know that
H-1
X (PM* λMτ(h) (Sh,ah) — λMτ(h) (Sh+1)) ≤ DpHlog(2MH),	(70)
h=1
holds with probability at least 1 — MH. Since 0 ≤ λM ≤ D, we have PM* λMτ(h)⑶他,aho) 一
λMτ(h) (si) ≤ D. Therefore, we have
H	H
HPM* 一 ^X R(Sh,ah) ≤ ^X (PMMh) 一 PM* ) λMτ (h) (Sh,ah) + DpH log(2HM) + D (71)
h=1	h=1
≤Dp2MH log(2MH) + M + DpH log(2HM) + D.	(72)
23
Published as a conference paper at ICLR 2022
The first term in (71) is bounded by the stopping condition (line 5 of Algorithm 3). By Lemma 2,
We have VM*,1(s1) ≤ HρM* + D. Therefore, We have VM*,1(s1) - PhH=I R(Sh,ah) ≤
O (DpMHIog(MH)) with probability at least 1 - MH. Therefore, we have
VM*,ι(sι) - VMDR1(s1) ≤ O (DpMHlOg(MH) + H ∙ MH)) = O (DpMHlOg(MH)).
(73)
□
E.2 Proof of Theorem 2
Theorem 2 can be proved by combining Theorem 6 , Lemma 1 and Lemma 2. By Theorem 6, for
any M ∈ U, the policy ∏ represented by Algorithm 3 has regret bound HPM - PhH=I R(sh, ah) ≤
O (DpMHlog(MH)). Taking expectation over {sh,ah}h∈[h] and combining the inequality
with Lemma 2, we have for any M ∈ U .
VM,1(s1) - VMJ(SI) ≤ O (DpMHlOg(MH)) .	(74)
Then the theorem can be proved by Lemma 1.
F Omitted Proof for Infinite Simulator Class
F.1 Proof of Theorem 7
Lemma 9. (Low Switching) The total number of episode K is bounded by
K ≤ O(dimE(F, 1/H) lOg(D2H) lOg(H))	(75)
Proof. By Lemma 5 of Kong et al. (2021), we know that
H
min
t=1
sup
f1,f2∈F
(fι(xt)- f2(xt))2
IIfI- f2kZt + 1
≤ CdimE(F, 1/H) lOg(D2H) lOg(H)
(76)
1
for some constant C > 0.
Our idea is to use this result to upper bound the number of total switching steps.
Let τ (k) be the first step of episode k. By the definition of the function class F, we have (f1 -
f2)2(xt) ≤ 4D2 for anyf1,f2 ∈ F and xt . By the switching rule, we know that, once the agent
starts a new episode after step τ(k + 1) - 1, we have
τ (k+1)-1
X	(f1 -f2 )2 (xt ) ≤
t=τ (k)
τ (k)-1
X (f1 -f2)2 (xt) + α + 4D2, ∀f1,f2, xt
t=1
(77)
Therefore, we have
τ (k+1)-1	τ (k)-1
X	(f1 -f2)2(xt) ≤2 X (f1 - f2)2(xt) + α + 4D2, ∀f1, f2, xt	(78)
t=1	t=1
24
Published as a conference paper at ICLR 2022
Now we upper bound the importance score in the switching step τ (k + 1) - 1.
min sup
f1,f2
Ptτ=(kτ+(k1))-1(f1(xt) -f2(xt))2
kf1 -f2kZτ (k) + α
τ (k+1)-1
,1	≤min X
t=τ (k)
τ (k+1)-1
≤ min
t=τ (k)
(f1(xt) - f2(xt))2
Su2 kf1-f2kZτ(k) +α
sup
f1,f2
,1
(79)
2(fι(xt) - f2(xt))2	1
kf1-f2kZτ(k+i) -4D2 + α,
(80)
τ (k+1)-1
≤ min
t=τ (k)
q,m	2(f1 (Xt) - f2(Xt))2	1
Jd kf1-f2kZt- 4D2 + α,
(81)
J (XT . f	2(fι (xt)-f2(xt))2	J
≤	min sup	, 1
≤ t⅛)	LIJ2 kfι- f2kZt - 4D2 +α, ∫
(82)
Suppose the number of episodes is at most K. If we set α = 4D2 + 1, we have
KP
min sup
k=1	∣f1J2
tτ=(kτ+(k1))-1(f1(Xt)-f2(Xt))2
kf1 -f2kZτ(k) + α
,11 ≤ X min [sup 口 2(九(X：? - 力(Xy ,1)
,≤ t=1	Vι,fp2 kfι- f2kZt- 4D2 +2α, /
(83)
≤CdimE(F, 1/H) log(D2H) log(H)	(84)
By the switching rule, we have supf ,f
PT=k+k)T(∕ι(χt)-∕2(χt))2
kf1-f2kZτ(k) +α
≥ 1. Therefore, the LHS of the
above inequality is exactly K . Thus we have
K ≤ CdimE(F, 1/H) log(D2H) log(H).	(85)
□
Lemma 10. (Optimism) With probability at least 1 一 H, M* ∈ Uk holdsfor any episode k ∈ [K].
Proof. This lemma comes directly from Theorem 6 of Ayoub et al. (2020). Define the Filtration
F = (Fh)h>o so that Fh-ι is generated by (sι,aι,λι,…,sh,ah,λh). Then we have E[λh(sh+ι) |
Fh-ι] = Pm*λh(sh,ah) = fM*(sh,ah,λh). Meanwhile, λh(sh+ι) — /m* (sh,ah,λh) is condi-
tionally DD-SUbgaUssian given Fh-1. By Theorem 6 of Ayoub et al. (2020), We can directly know
that /m* ∈ Uk for any k ∈ [K] with probability at least 1 — H.	口
25
Published as a conference paper at ICLR 2022
Proof. (Proof of Theorem 7) Let τ (k) be the first step of episode k. Under the high-probability
event defined in Lemma 10, we can decompose the regret using the same trick in previous sections.
K τ (k+1)-1
X X HρM*- R(Sh,ah	(86)
k=1 h=τ (k)
K τ (k+1)-1
≤ X X (PMk- R(sh,ah))	(87)
k=1 h=τ (k)
K τ (k+1)-1
=	(PMk λh(sh, ah) - λh (sh))	(88)
k=1 h=τ (k)
K τ (k+1)-1	K τ(k+1)-1
=ɪ2 ɪs (PMk - PM* )λh(sh, ah) + ɪ2 ɪs	(PM* λh(sh, ah) - λh(Sh))	(89)
k=1 h=τ (k)	k=1 h=τ (k)
K τ (k+1)-1	K τ(k+1)-2
≤	(PMk - PM* )λh(Sh, ah) +	(PM* λh(Sh, ah) - λh (Sh+1)) + DK,
k=1 h=τ (k)	k=1 h=τ (k)
(90)
where the first inequality is due to optimism condition in Lemma 10. The first equality is due to the
Bellman equation 10 and λh = λMMk for T(k) ≤ h ≤ T(k + 1) - 1. The last inequality is due to
0 ≤ λh(S) ≤ D for any S ∈ S.
Now we bound the first two terms in Eqn 90. The second term can be regarded as a martingale
difference sequence. By Azuma's inequality, with probability at least 1 - H,
K T(k + 1)-1-1
XX
(Pm* λh(sh,ah) - λh(sh+ι)) ≤ Dp2Hlog(H).	(91)
k=1 h=τ (k)
Now we focus on the upper bound of the first term in Eqn 90. Under the high-probability event
defined in Lemma 10, the true model P is always in the model class Uk. For episode k, from the
construction of Uk We know that any f1,f2 ∈ Uk satisfies ∣∣fι - f2kZ ≤ 2β. Since Mk, M* ∈
τ(k)
Uk , we have
T (k-1)
X ((PMk -PM*)λt(St,at))2 ≤2β	(92)
t=1
Moreover, by the if condition in Line 8 of Alg. 4, we have for any T(k) ≤ h ≤ T(k + 1) - 1,
h
X ((PMk - PM* ) λt(St, at))2 ≤ 2β + α + D2.	(93)
t=τ (k)
Summing up the above two equations, we have
h
X ((PMk - PM* ) λt (St, at))2 ≤ 4β + α + D2 .	(94)
t=1
We invoke Lemma 26 of Jin et al. (2021) by setting G = F-F, Π = {δχ(∙)∣x ∈ X} where
δχ(∙) is the dirac measure centered at x, gt = fMk - fM* , ω = 1/H, β = 4β + α + D2 and
μt = 1 {∙ = (st,at,λt)}, then we have
26
Published as a conference paper at ICLR 2022
K T(T)	/	1
E E I(Pmt - Pm* ) λt(st,at)∣ ≤O( VZdimE (F, 1/H )βH + min(H, dimE (F, 1/H)) D + H ∙ H
τ=1 t=S(τ)
(95)
=O (PdimE (F,1/H)βH)	(96)
Plugging the results back to Eqn 90, we have
K T(k + 1)-1	z	、
X X HP - R(sh,ah) ≤ O(DpHdmE(Fwlog(H^N(F∏7Hy)	(97)
k=1 h=T (k)
By Lemma 2, We have VM*,ι(sι) ≤ HρM* + D. Therefore, We have
H	z	、
VM* ,1(s1)- X R(sh,ah) ≤ O(DpHdmE(FITwogTHTN(FnTW) ,	(98)
h=1
with probability at least 1 - MH. If the high-probability event doesn't holds (This happens with
probability at most H), then the gap VMM * 1(s1) - VMDR 1(s1) still can be bounded by H. Taking
expectation over the trajectory {sh}h, we have
VM *,1(s1) - V⅛ι(sι) ≤O(DpHdmEFwnogTH7NFnTHI + H ∙ H))	(99)
=O(DpHdimE (F, 1/H)log (H ∙ N(F, 1/H))) .	(100)
□
F.2 Proof of Theorem 4
Proof. Theorem 4 can be proved by combining Theorem 7 , Lemma 1 and Lemma 2. By Theo-
rem 7, for any M ∈ U, the policy ∏ represented by Algorithm 4 can obtain regret bound HPM -
Ph=I R(sh, ah) ≤ O(DpdeHlog(H ∙N(F, 1/H))). Taking expectation over {sh, ah}h∈H]
and combining the inequality with Lemma 2, we have for any M ∈ U .
VM,1(s1)- VMI,1(s1) ≤ O(DpdeHlOg(H ∙ N(F, 1/H))) .	(101)
Then the theorem can be proved by Lemma 1.	口
G Lower Bounds
G. 1 Proof of Proposition 1
Proof. Consider the following construction of U. There are 3M + 1 states. There are M actions
in the initial state s0, which is denoted as {ai}iM=1. After taking action ai in state s0, the agent will
transit to state si,1 with probability 1. In state si,1 for i ∈ [M], the agent can only take action a0,
and then transits to state si,2 with probability pi, and transits to state si,3 with probability 1 - pi .
State {si,2}iM=1 and {si,3}iM=1 are all absorbing states. That is, the agent can only take one action a0
in these states, and transits back to the current state with probability 1. The agent can only obtain
reward 1 in state si,2 for i ∈ [M], and all the other states have zero rewards. Therefore, if the agent
knows the transition dynamics of the MDP, it should take action ai with i = arg maxi pi in state s0 .
Now we define the transition dynamics of each MDP Mi. For each MDP Mi ∈ U, we have pi = 1
and Pj = 0 for all j ∈ [M],j = i. Therefore, the agent cannot identify M* in state s°. The best
policy in state s0 for latent MDP U is to randomly take an action ai. In this case, the sim-to-real gap
can be at least Ω(H) since the agent takes the wrong action in state so with probability 1 - M.
□
27
Published as a conference paper at ICLR 2022
G.2 Proof of Theorem 3
Proof. We first show that Ω( VMH) holds with the hard instance for multi-armed bandit (Lattimore
& Szepesvari, 2020). Consider a class of K-armed bandits instances with K = M. For the bandit
instance Mi, the expected reward of arm i is 2 + G while the expected rewards of other arms are 1.
Note that this is exactly the hard instance for K-armed bandits. Following the proof idea of the lower
bound for multi-armed bandits, we know that the regret (sim-to-real gap) is at least Ω(√MH).
We restate the hard instance construction from Jaksch et al. (2010). This hard instance construction
has also been applied to prove the lower bound in episodic setting (Jin et al., 2018). We firstly
introduce the two-state MDP construction. In their construction, the reward does not depend on
actions but states. State 1 always has reward 1 and state 0 always has reward 0. From state 1, any
action takes the agent to state 0 with probability δ, and to state 1 with probability 1 - δ. In state 0,
there is one action a* takes the agent to state 1 with probability δ + e, and the other action a takes
the agent to state 1 with probability δ. A standard Markov chain analysis shows that the stationary
distribution of the optimal policy (that is, the one that takes action a* in state 0) has a probability of
being in state 1 of
δ	δ + ε 1 ε
ɪrɪ = 2δ+I ≥ 2 + 6δfor ε ≤ δ∙
δ	δ+ε
(102)
In contrast, acting sub-optimally (that is taking action a in state 0) leads to a uniform distribution
over the two states. The regret per time step of pulling a sub-optimal action is of order /δ.
Consider O(S) copies of this two-state MDP where only one of the copies has such a good action
a* . These copies are connected into a single MDP with an A-ary tree structure. In this construction,
the agent needs to identify the optimal state-action pair over totally SA different choices. Setting
δ = D and E = C J篝,Jaksch et al. (2010) prove that the regret in the infinite-horizon setting is
Ω(√DSAT).
Our analysis follows the same idea of Jaksch et al. (2010). For the MDP instance Mi, the opti-
mal state-action pair is (si, ai) ((si, ai) 6= (sj, aj), ∀i 6= j). With the knowledge of the transition
dynamics of each Mi, the agent needs to identify the optimal state-action pair over totally M dif-
ferent pairs in our setting. Therefore, we can similarly prove that the lower bound is Ω(√DMH)
following their analysis.	□
G.3 Lower Bound in the Large S imulator Class
Proposition 2. Suppose All MDPs in the MDP set U are linear mixture models (Zhou et al., 2020)
sharing a common low dimensional representation with dimension d = O(log(M)), there exists a
hard instance such that the sim-to-real gap of the policy πD*R returned by the domain randomization
oracle can be still Ω(H) when M ≥ H.
Proof. We can consider the following linear bandit instance as a special case. Suppose there are two
actions with feature φ(a1) = (1, 0) and φ(a2) = (1, 1). In the MDP set M, there are M - 1 MDPs
with parameter θ% = (2, -Pi) with 1 <pi < 1, i ∈ [M - 1], and one MDP with parameter Θm =
(1,1). Suppose M = 4H + 5, the optimal policy of such an LMDP with uniform initialization
will never pull the action a2, which can suffer Ω(H) sim-to-real gap in the MDP with parameter
θM.	口
28