Published as a conference paper at ICLR 2022
Optimal transport for causal discovery
Ruibo Tu	Kun Zhang
KTH Royal Institute of Technology Carnegie Mellon University
ruibo@kth.se	Mohamed bin Zayed University of Artificial Intelligence
kunz1@cmu.edu
Hedvig Kjellstrom	Cheng Zhang
KTH Royal Institute of Technology	Microsoft Research
Silo AI	Cheng.Zhang@microsoft.com
hedvig@kth.se
Ab stract
To determine causal relationships between two variables, approaches based on
Functional Causal Models (FCMs) have been proposed by properly restricting
model classes; however, the performance is sensitive to the model assumptions,
which makes it difficult to use. In this paper, we provide a novel dynamical-system
view of FCMs and propose a new framework for identifying causal direction in the
bivariate case. We first show the connection between FCMs and optimal transport,
and then study optimal transport under the constraints of FCMs. Furthermore,
by exploiting the dynamical interpretation of optimal transport under the FCM
constraints, we determine the corresponding underlying dynamical process of the
static cause-effect pair data. It provides a new dimension for describing static
causal discovery tasks while enjoying more freedom for modeling the quantitative
causal influences. In particular, we show that Additive Noise Models (ANMs) cor-
respond to volume-preserving pressureless flows. Consequently, based on their
velocity field divergence, we introduce a criterion for determining causal direc-
tion. With this criterion, we propose a novel optimal transport-based algorithm
for ANMs which is robust to the choice of models and extend it to post-nonlinear
models. Our method demonstrated state-of-the-art results on both synthetic and
causal discovery benchmark datasets.
1	Introduction
Determining causal relationships between two variables is a fundamental and challenging causal
discovery task (Janzing et al., 2012). Conventional constraint-based and score-based causal discov-
ery methods identify causal structures only up to Markov equivalent classes (Spirtes et al., 2001), in
which some causal relationships are undetermined. To address this challenge, properly constrained
functional causal models (FCMs) have been proposed. FCMs represent the effect as a function of
its cause and independent noise and can help identify the causal direction between two variables by
imposing substantial structural constraints on model classes, such as additive noise models (ANMS)
(ShimizU et al., 2006; Hoyer et al., 2008) and post-nonlinear models (PNLS) (Zhang and Hyvarinen,
2009b). While some of the models, such as PNLs, are highly flexible, the constraints are still re-
strictive and difficult to interpret and relax. Inevitably, the performance of these methods is sensitive
to model assumptions and optimization algorithms, especially in real-world applications.
To handle the mentioned issues, we consider FCMs from a dynamical-system view. By augmenting
a time dimension for the static causal discovery task, we interpret FCMs with dynamical causal
processes under the least action principle (Arnol’d, 2013). The new interpretation connects FCMs
with a large class of models in dynamical systems. It then provides more freedom to model causal
influences, possibilities to derive new causal discovery criteria, and a potential direction to generalize
causal models with identifiable causal direction.
1
Published as a conference paper at ICLR 2022
In particular, we exploit the above idea by leveraging the intrinsic connection between FCMs and
optimal transport. Optimal transport is originally introduced by Monge (1781), which has been
applied in a large range of applications, not only because it is a natural way to describe moving
particles (Ambrosio et al., 2012) but also because of its recent improvement in the computational
methods (Cuturi, 2013; Kolouri et al., 2019). Recently, it has also been largely applied to generative
models for measuring the distance of probability distributions (Arjovsky et al., 2017; Kolouri et al.,
2018; Genevay et al., 2018). Among different optimal transport definitions, the L2 Wasserstein
distance got extensive applications in statistics (Rachev and RUschendorf, 1998), functional analysis
(Barthe, 1998), et al. (McCann, 1997; Otto, 1997). The dynamical formulation of the L2 Wasserstein
distance is introduced by Benamou and Brenier (2000) for relaxing the computational costs. We find
that in the context of the dynamical formulation, FCMs can be connected with optimal transport.
Furthermore, with the dynamical interpretation of optimal transport, one can naturally understand
FCMs from a dynamical-system view, which makes it possible to derive new criteria to identify
causal direction. Moreover, it also enables us to develop practical algorithms with optimal transport
for static causal discovery tasks without learning a regression model. Our main contributions are:
1.	Dynamical interpretation of FCMs in the bivariate case. We provide dynamical interpretations
of optimal transport under the constraints of FCMs. Furthermore, we introduce a time variable,
determine the underlying dynamical process under the least action principle (Arnol’d, 2013) for the
static bivariate causal discovery task, and characterize properties of the corresponding dynamical
systems (Sec. 3.1 and Sec. 3.2).
2.	A criterion for determining causal relationships between two variables. We study the correspond-
ing dynamical systems of FCMs and prove that ANMs correspond to volume-preserving pressureless
flows. Moreover, based on the divergence of their velocity fields, we propose a criterion for deter-
mining causal relationships and show that under the identifiability conditions of ANMs it is a valid
criterion for ANMs, which can be extended to PNLs directly (Sec. 3.2).
3.	An optimal transport-based approach (DIVOT) for distinguishing cause from effect between two
variables. DIVOT inherits the advantages of one-dimensional optimal transport. It can be computed
efficiently and does not require independence tests, learning a regression model, or deriving likeli-
hood functions for complicated distributions. Experimental results show that our method is robust to
the choice of models and has a promising performance compared with the state-of-the-art methods
on both synthetic and real cause-effect pair datasets (Sec. 4 and Sec. 6).
2	Preliminaries
Optimal transport: the underdetermined Jacobian problem. We mainly follow the notations
and the definitions of (Benamou and Brenier, 2000). Suppose that two (probability) density func-
tions, p0 (x) and pT (x) where x ∈ Rd, are non-negative and bounded with total mass one. The
transfer of p0(x) to pT (x) is realized with a smooth one-to-one map M : Rd → Rd. The Jacobian
problem is to find M that satisfies the Jacobian equation, p0(x0) = PT(M(xo))∣det(VM(xo))∣,
where XT = M (x0), V is the gradient in vector calculus, and det(∙) denotes determinant. This is an
underdetermined problem as many maps can be the solutions. A natural way is to choose the optimal
one, e.g., the one with the lowest cost. A common cost function is the Lp Wasserstein distance.
Lp Wasserstein distance and its one-dimensional closed-form solution. The Lp Wasser-
stein distance between p0 and pT, denoted by Wp(p0,pT), is defined by Wp(p0,pT)p =
infM |M(X0) - X0|pp0(X0)dX0, where p ≥ 1 (Kantorovich, 1948). In this work, we mainly
use the square of the L2 Wasserstein distance, denoted by W22 . Moreover, the one-dimensional
(1D) Lp Wasserstein distance has a closed-form solution, e.g., the 1D optimal solution of W22 is
M * = PT1 ◦ Po, where Po and PT are the cumulative distribution functions for po and PT, and “◦”
represents the function composition. In practice, the 1D optimal solution can be computed with the
average square distance between the sorted samples from Po and PT (Kolouri et al., 2019).
Functional causal models. FCMS represent the effect Y as a function f (∙) of the direct cause X
and independent noise Ey, where function f describes the causal influence ofX on Y, and Ey is the
exogenous variable/noise. Without any additional assumption on the functional classes, the causal
direction is not identifiable (Hyvarinen and Pajunen, 1999; Zhang et al., 2015a). Roughly speaking,
2
Published as a conference paper at ICLR 2022
(a) Pressureless potential flow corresponding to
FCMs
Figure 1: Panel (a) illustrates the trajectories of the pressureless potential flow corresponding to gen-
eral FCMs , while panel (b) shows the trajectories of the volume-preserving pressureless potential
flow corresponding to ANMs. In panel (b), the trajectories have zero velocities on the Ex/X axis
and move along straight lines that are parallel to each other when having the same values of X/Ex .
(b) Volume-preserving pressureless potential flow
corresponding to ANMs
because given variable pair (X, Y ), one can always construct Y = f(X, Ey) and another different
FCM, X = f (Y, Eχ), SUch that both of them have independent noise (HyVannen and Pajunen,
1999; Zhang et al., 2015a). Several works further introduce proper assumptions on model classes,
which guarantees that the independence of cause and noise only holds in the causal direction, e.g.,
ANM: Y = g(X) + Ey;	(1) PNL: Y = h(g(X) +Ey),	(2)
where g and h are nonlinear functions and h is invertible.
3	Dynamical Interpretation of Functional Causal Models
We first show the connection between FCMs and optimal transport in Sec. 3.1. In Sec. 3.2, we
further elaborate the analogy between the optimal transport problem and the causal direction deter-
mination problem. We then study the optimal transport under the constraints of FCMs, show the
corresponding dynamical systems of FCMs, and characterize the properties of such systems.
3.1	A reformulation of Functional Causal Models
As introduced in Sec. 2, FCMs are used to approximate the true data generation process. Given the
FCM, Y = f(X, Ey), we rewrite it in the vector form,
xT = XY = f(XE,xEy) = M	EEyx	= M(x0),	(3)
where x0 , xT ∈ R2, their probability densities p0, pT ≥ 0, and M : R2 → R2 . As an analogy
to the mass transfer scenario (Monge, 1781), we consider the samples of independent noise Ex and
Ey as the particles of materials and regard the map M in Eqn. (3) as a special transformation of
the independent noise samples. As shown in Fig. 1, one can consider the data points are transferred
from the original positions (which are unmeasured) in the plane Eχ-Ey at time 0 to the observed
positions in the plane X-Y at time T. Such transformation considers the transfer as a dynamical
process which moves the unmeasured independent noise x0 = [Ex, Ey]0 1 and consequently leads to
the observations xT = [X, Y]0. From the perspective of FCM-based causal discovery approaches,
causal influences are represented by FCMs which represent the effect as a function of its direct cause
and an unmeasured noise satisfying the FCM constraints:
(i)	The map constraint: the values of X are determined by the values of its corresponding noise,
i.e., X = Ex, while the values of the effect depend on cause X and noise Ey;
(ii)	The independence constraint: the noise terms are independent, i.e, Ex is independent of Ey .
1“0” denotes the transpose of vectors or matrices.
3
Published as a conference paper at ICLR 2022
Note that the optimal transport M* with the minimal Lp Wasserstein distance is not necessary to
be the one in Eqn. (3), because it has no information about the FCM constraints or the true data
generation process. In other words, given two sample sets of x0 and xT , the couplings given by
optimal transport are not necessary to be the ones generated from the ground-truth FCM.
3.2	Dynamical interpretation of FCMs: Optimal transport under the FCM
CONSTRAINTS
In this section, we jointly consider the causality and the optimality of the maps in the Jacobian
problem. It provides both a causal sense of the transformation and a dynamical view of FCMs. We
first recap the dynamical formulation of the L2 Wasserstein distance, study such dynamical systems
under the FCM constraints, and then show their properties under the FCM and ANM constraints.
Dynamical L2 Wasserstein distance. Benamou and Brenier (2000) formulate the L2 Monge-
Kantorovich problem as a convex space-time minimization problem in a continuum mechanics
framework. Fixing a time interval [0, T], they introduce the concepts of the smooth time-dependent
density ρ(t, xt) ≥ 0 and the velocity field v(t, xt). When they are clear from context, we denote
them by ρ and v. Because we are considering the bivariate case, xt ∈ R2 and v ∈ R2 . Then, they
give the dynamical formulation of W22 :
W22(p0,pT)	= infT	ρ(t, xt)|v(t, xt)|2dxtdt,	(4)
ρ,v	R2 0
{initial and final conditions: ρ(0, ∙) = po, ρ(T, ∙) = PT
the continuity equation: ∂tP + V ∙ (ρv) = 0	,
where V∙ denotes the divergence in vector calculus. They show that minimizing the objective func-
tion in the optimization problem (4) is equivalent to finding the dynamical system with the least
action (Arnol’d, 2013) and prove that the solutions of (4) are pressureless potential flows, of which
the fluid particles are not subject to any pressure or force and the trajectories are determined given
their initial positions and velocities or given their initial and final positions. Suppose that M* is the
solution given by W22 . The corresponding flows follow the time evolution equation,
Xt = X0 + Tv(t,xt), where v(t,Xt) = v(0, xo) = M*(x°) — x0 andt ∈ [0,T].	(5)
The time evolution equation shows that xt is a convex combination of x0 and M*(x0) and that the
velocity fields do not depend on time.
As an analogy between the optimal transport problem and causal direction determination, the density
ρ and the velocity v of moving particles can be considered as the probability density and the velocity
of changing values of data points. Moreover, the dynamical interpretation of the L2 Wasserstein
distance introduces a time variable and provides a natural time interpolation ρ(t, Xt) of ρ0 and ρT
together with the velocity field v(t, Xt). Similarly, we can also have the natural time interpolation
p(t, Xt) between p0 and pT as well as the velocity field v(t, Xt) under the least action principle
(Arnol’d, 2013), which is the dynamical interpretation of FCMs.
Dynamical L2 Wasserstein distance under the FCMs constraints. First, we introduce FCM
constraints in the context of the dynamical L2 Wasserstein distance. According to the time evolution
equation (5), we know that the velocity is fully determined by the initial and final values of Xt .
We first consider FCM constraint (i). For the initial and final values of the cause, the value of its
observation X is equal to its noise value Ex in Eqn. (3). Consequently, denoting v = [vx , vy]0,
where vx and vy represent the velocities along the Ex/X-axis and Ey/Y -axis respectively as shown
in Fi.g 1, we know that ∀t ∈ [0, T], xt ∈ R, vx(t, xt) = 0. As for FCM constraint (ii), it implies
that the initial noise of cause and effect, corresponding to Ex and Ey at the time 0, are independent.
Therefore, we have the FCM constraints for the dynamical L2 Wasserstein distance,
(I)	The map constraints: v(t, Xt) = [vx(t, xt), vy (t, yt)]0, ∀xt, yt ∈ R, t ∈ [0, T], vx(t, xt) = 0;
(II)	The independence constraint: two random variables (cause and effect) at the initial time have
the joint probability density function p0 (X0) and they are independent.
4
Published as a conference paper at ICLR 2022
Second, we characterize the properties of the dynamical L2 Wasserstein distance under constraints
(I) and (II). According to Eqn. (3), the form of M* is determined as
E
M*(x0) = f(EEx,xEy) ;	(6)
otherwise, the FCM constraints will be violated. Moreover, the W22 under the FCM constraints can
be computed with the one-dimensional W22 as shown in Prop. 1 (the derivation is in App. D).
Proposition 1. Under constraints (I) and (II), the square of the L2 Wasserstein distance between p0
and pT is
W22(P0,PT) = EEx hw22 (p(Ey),p(Y|Ex))i .	⑺
Furthermore, we consider the constraints of ANMs and characterize the corresponding dynamical
systems for now, which can be directly extended to PNLs as mentioned in Sec. 4.2. Based on
constraints (I) and (II), we further introduce the ANM constraint,
(III)	the effect is the sum of noise and a nonlinear function of cause as defined in Eqn. (1).
Theorem 1 ( Zero divergence of the velocity field ). Under constraints (I) and (II), the dynamical
systems given by the L2 Wasserstein distance are pressureless flows. Further under ANM constraint
(III), they become volume-preserving pressureless flows, of which the divergence of the velocity field,
v(t, xt) = [vx(t, xt), vy(t, yt)]0, satisfies
div v(t, Xt) = dvx∂Xχ) + dv∂詈2 = 0,∀t ∈ [0,T], χt, y ∈ R,
where div is the divergence operator in vector calculus.
Thm. 1 determines the corresponding dynamical systems of FCMs and ANMs by analyzing their
densities and velocity fields (the details of the proof are in App. D) and shows an essential property
of the corresponding dynamical systems of ANMs. The property indicates a potential criterion for
causal direction determination, which the divergence of the velocity field is zero everywhere in the
causal direction, while it may not always hold in the reverse direction. Next, we will verify the
criterion rigorously, propose an algorithm based on it, and show the extension for the PNL cases.
4	Causal direction determination with optimal transport
In this section, we define a divergence measure as a criterion for determining causal direction be-
tween two variables for ANMs. Based on the criterion, we then provide an algorithm to identify the
causal direction, named by the divergence measure with optimal transport (DIVOT).
4.1	Divergence measure as a causal discovery criterion
We first define the divergence measure and then show that it is a valid criterion for identifying the
causal direction in the bivariate case under the identifiability conditions of ANMs, as shown in Prop.
2 (the proof is in App. D).
Proposition 2 (Divergence measure as a causal discovery criterion). Define the divergence measure,
D(v)
|div v|2p0(X0)dX0 = Ex0[ |div v|2],
R2
(8)
where v = M* (X0 ) - X0. Suppose that constraints (I), (II), (III), and the identifiability conditions
of ANMs (Hoyer et al., 2008) are satisfied. The divergence measure of the corresponding dynamical
system satisfies D(v) = 0 if and only if X is the direct cause of Y.
Nevertheless, there are some challenges to compute the divergence measure. For example, we need
to solve a two-dimensional optimal transport problem and compute the derivative of a velocity field
for all samples. Another challenge of computing M* is that we in general have no information about
X0 . Such issues are all solved with DIVOT.
5
Published as a conference paper at ICLR 2022
4.2	Proposed method: DIVOT
We first provide an overview of the algorithm, DIVOT, for determining the causal direction between
two variables and then introduce the four steps to compute the divergence measure.
Overview of DIVOT. Alg. 1 is based on the divergence-measure criterion for causal direction
determination and determines causal direction between two variables. Given data of X and Y , we
want to infer whether X causes Y (X → Y ) or Y causes X (Y → X). We compute the divergence
measure in both directions. In practice, noise can have different variance in different applications,
and the larger variance can lead to the larger measure value with finite samples. So we normalize the
variance-based measure value with the estimated noise variance. And then the one with the smaller
normalized measure value is the causal direction. In App. A, we provide the modified algorithms
for including the independent case and the significance of the results with bootstrapping.
Algorithm 1: DIVOT: divergence measure with optimal transport for causal direction determi-
nation.
Input: data {(xi, yi)} and noise distribution 6 if LY→X < LX→Y then
p(E; θ0)	7	return Y → X
Output: X → Y or Y → X .	8 else	
1 Def Divoτ({(χi,yi)}, P(E; θo)):	9	return X → Y
2 data, θ = {(xi, yi)}, θ0	10 Def Div( data, P(E; θ)):	
3	LX→Y = Div(data, p(E; θ))	11	noise = Sampling(P(E; θ))
4 data, θ = {(yi, xi)}, θ0	12	datas , noises = OT(data, noise)
5	LY→χ = Div(data, p(E; θ))	13	loss = minθ VarDiv(datas, noises)
	14	return loss/Var(P(E; θ*))
Noise data generation: the first step of computing the divergence measure. To compute the
divergence measure, we need to know the velocity field v as defined in the time evolution equation
(5). It requires the couplings of the data of x0 = [Ex, Ey]0 and xT = [X, Y]0. But in the bivariate
causal discovery task, only the data of xT are given. Therefore, as shown in Line 11 of Alg. 1, we
first deal with the issue due to the lack of the noise data of x0, denoted by {(eix , eiy)}. To obtain the
noise data, we may assume a multivariate probability distribution ofx0 with the density p0(Ex, Ey)
and then sample data from it, represented by (eχ, ey)〜po(Eχ, Ey). Fortunately, due to the FCM
constraints, we know that p0(Ex, Ey) = p0(X, Ey) = p(X)p(Ey). So we only need to assume
the probability distribution of Ey and parameterize it with θ, denoted by p(Ey; θ). Suppose that
the dataset of xT with N samples is given, denoted by {(xi, yi)}N. We first sample a data set of
Ey with the sample size N, denoted by {eiy }N, e.g., in the experiments of this work, we use the
simplified reparameterization trick,
ey = fnoise(esource) = θ X esource and esource 〜N(0,1)/U(0,1),	(9)
where fθnoise is a monotonic function, and eysource is sampled from a standard normal distribution
or a uniform distribution. As the monotonic fθnoise can be very flexible, we can represent flexible
noise distributions with monotonic neural networks as in (Huang et al., 2018). Next, we randomly
match the data ofX, denoted by {xi}N, with {eiy}N, which gives the {(xi, eiy)}N as the data ofx0.
Optimal transport finds the couplings of the observation and the generated noise. Com-
puting the divergence measure requires the couplings of the data of x0 and xT because of v =
M*(xo) 一 xo； in other words, given the data of x° and XT, We need to solve a two-dimensional
optimal transport problem, which gives M* and the couplings as in Line 12 of Alg. 1. According to
Prop. 1, solving the two-dimensional optimal transport problem under the FCM constraints is equiv-
alent to solving one-dimensional optimal transport problems. More specifically, the expectation in
Eqn. (7) can be computed with the Monte Carlo estimator by using N samples of Ex , denoted by
{eix }N, and then the two-dimensional W22 in Eqn. (7) is computed with N one-dimensional W22,
i.e., W2(po,pτ) ≈ Nn Pi W2(p(Ey),p(Y∣Eχ = eix)). Importantly, solving the one-dimensional
optimal transport problem or computing the one-dimensional W22 can be implemented with a sorting
operation as in (Kolouri et al., 2019). Therefore, the couplings are found by which given each value
of Ex, we find the corresponding values of Ey and Y from {(eix, eiy)}N and {(xi, yi)}N and then
match such sorted values of Ey and Y .
6
Published as a conference paper at ICLR 2022
Variance-based divergence measure. Suppose that the previous step has found the couplings, i.e.,
given any sample (eix, eiy), we have its corresponding (xi, yi). Then, we will compute the divergence
of a velocity field as in Line 13 of Alg. 1. According to the definition, |div v|2 = (dvy)2. Since
all the eiy are matched with yi, we have vyi = yi - eiy . A straightforward way to approximate
the derivative is using its nearest neighbour pair (eixb, eiyb) and (xbi, ybi) and then approximate it
(yi -ei )-(ybi-ei )
with dy (Vy)∣y=yi = -------；_#>——y~; however, it suffers the following issues (especially in the
few-sample scenario) : (a) the denominator is in general a small number, and the distance to the
nearest neighbour can be large in the few-sample case, which makes the computation unstable and
inaccurate; (b) the deviation on the X-axis makes the approximation a biased estimate especially
when the gradient of g(X) + Ey at X = x is large. Therefore, we propose the variance-based
divergence measure working better in practice. It is straightforward to see that under constraints
(I) and (II), the value of the divergence measure of an ANM is zero if and only if the value of the
variance-based divergence measure is zero, which is defined as
Dvar(v) = EX[V[Vy|X|X]],
(10)
where V[Vy|X |X] represents a conditional variance of the velocity field Vy (a random variable)
at position X at the initial time. For example, V[Vy|x |X = x] represents the variance of all the
velocities vy at the positions where X = x at the initial time. {(xi, yi)}N denotes the cause-effect
pair dataset with the sample size N. {vyi |x}Nx denotes all the velocities vyi at position X = x at the
initial time, where the sample size is Nχ, and their mean value is Vyχ. Then, V[Vy∣χ∣X = x]=
PNxI(Vy|x - vy∣χ)2/(Nx - 1)，and
1	sort(y-→x) - sort(-→ey) - ave(y-→x - -→ey)22
Var(V) ≈ N X	Nx - 1
x∈{xi}N
(11)
where {xi}N represents the set of all the values ofX and its sample size is N; y-→x is the vector of the
Y samples where X = x; → is the vector of the Ey samples where Ex = x; sort(∙) sorts a vector;
ave(∙) computes the vector mean; and∣∣∙k2 is the square of a '2 norm.
Minimization w.r.t θ. Given the data {(xi, yi)}N and the density p(Ey; θ), we can compute the
divergence measure with the variance-based method. Note that we only initialize θ with some ran-
dom value, and p(Ey ; θ) is not necessary to be the true distribution or even significantly different
from the true one, which can lead to the wrong result of the divergence measure. Therefore, as
shown in Line 13 of Alg. 1, we minimize the divergence measure w.r.t. θ. For the minimization, one
can derive the gradient w.r.t θ in a simple parameterization case as (9), while in the complex case one
can use auto-differentiation. According to Prop. 2, the divergence measure in the causal direction is
zero if and only if P(Ey; θ*) with the optimal parameter θ* is the true noise distribution a.e., implied
by the identifiability of ANMs. In this work, we used autograd and RMSProp (gradient descent)
of JAX for the minimization.
Extension to PNLs. The measure (11) can be directly extended to the PNL cases. Since h in PNL
(2) is an invertible function, by considering h-1(Y) as a new random variable, h-1(Y) = g(X)+Ey
is an ANM. Thus, for PNLs, under the identifiabililty conditions of PNLS (Zhang and Hyvarinen,
2009b), Prop. 2 still holds; and we only need replace → in (11) with yX = fωNL(→), where fωNL is
an invertible function, e.g., it can be the simplified version of (Zhang et al., 2015a),
→ = fωNL(→) = → + 3a ∙ tanh(ωb ∙ → + ωj,	(12)
where ωa , ωb, and ωc are positive scalars. Moreover, fωPNL can also be the monotonic neural net-
works, such as (Huang et al., 2018).
Extension to the multivariate case. For simplicity and clarity of the paper, we focus on elabo-
rating the connection between FCMs and dynamical systems and developing the theoretical basis
and the method in the bivariate case, which are essential for the further development of FCM-based
causal discovery methods. In the case of multiple variables, one can use a constraint-based method
to find the causal skeleton (the undirected causal graph) and then use the extension of our method
for the edge orientation similar as (Zhang and Hyvarinen, 2009b; Monti et al., 2020; Khemakhem
et al., 2021). See App. B for details.
7
Published as a conference paper at ICLR 2022
(a) Accuracy on different ANM datasets.
Sample size
Sample size
(b) Robustness to prior misspecification.
Figure 2: Performance on synthetic data generated from different ANMs with different sample sizes.
fpiece is a discontinuous function: fpiece(x) = 0.5x3 - x, x ≤ 0; fpiece(x) = 1 - 0.5x3 + x, x > 0.
In panel (a), each result represents the percentage of correct results on datasets with a sample size
generated from an ANM. Panel (b) shows the results of applying DIVOT using different types of
distributions, P(Ey; θ), to the datasets with uniform distribution noise Ey 〜U(0,1).
5	Related work
There are mainly two types of causal discovery methods for static causal direction determination
between two variables. The first one introduces model assumptions to achieve the identifiability of
causal direction. Most of such methods are based on LiNGAM (Shimizu et al., 2006) and ANMS
(Hoyer et al., 2008; Mooij et al., 2009). Some of them are based on the more general models,
e.g., PNLs using MLP for representing nonlinear functions (PNL-MLP) (Zhang and Hyvarinen,
2009b) and PNLs using warped Gaussian process and mixture of Gaussian noise (PNL-WGP-MOG)
(Zhang et al., 2015a). Recently, Khemakhem et al. (2021) propose an autoregressive flow-based
model (CAREFL), of which the assumption is more general than ANMs and stricter than PNLs.
They commonly apply (non)linear regression to learn the function g in (1) (or together with h in
(2)), and then test the independence between the independent noise Ey in (1) (the residual) and the
cause X. And the independence test is commonly Hilbert-Schmidt independence criterion (HSIC)
(Gretton et al., 2005); however, as argued by Yamada and Sugiyama (2010), the kernel width limits
its practical use and its common heuristic value limits the flexibility of the function approxima-
tion. Moreover, there are other criteria proposed in the first type of methods, such as the likelihood
ratio (LLR) (HyVarinen and Smith, 2013), maximum likelihood-based criterion (MML), regression
error-based causal inference (RECI) (Blobaum et al., 2018), and mutual information (Zhang and
Hyvarinen, 2009a; Yamada and Sugiyama, 2010). Nevertheless, the common issue of the first type
of methods is that they restrict the model classes and that their performance is sensitive to model
assumptions. The second type of methods achieves identifiability by proposing other principles
instead of restricting model classes, such as exogeneity (Zhang et al., 2015b), the randomness of
exogeneous variables (Entropic) (Compton et al., 2021), and independent causal mechanisms, e.g.,
GPI (Stegle et al., 2010) and IGCI (Janzing et al., 2012).
6	Experiments
We demonstrate and evaluate our method on the synthetic and real-world cause-effect pair data
(Mooij et al., 2016). Moreover, we also provide the experiments and the discussion of our method
in the presence of unknown confounding in App. C. The details about experiments are in App. F.
Synthetic data. We evaluate DIVOT with Gaussian noise as in (9) on the datasets with different
(non)linear functions and samples sizes. We generate synthetic data with the ANMs: 1) Y = X +
Ey; 2) Y = 0.1(2.5X)3 - 0.1X + Ey; 3) Y = sin(4X) + Ey; 4) ifx < 0, Y = 0.5X 3 -
X + Ey and if X ≥ 0,Y = 1 - 0.5X3 + X + Ey , where X 〜U(-1,1) and Ey 〜U(0,1)
are uniform distribution. For each ANM, we generate datasets with the sample sizes 10, 25, 50,
100, 200, and 500. For each sample size, 100 different datasets are generated. Fig. 2a shows that
DIVOT consistently recovers the causal direction for all the cases. Since DIVOT has no smoothness
constraint of functions, it can deal with the case 4) which is a discontinuous function. We compared
DIVOT with the results of CAREFL (Khemakhem et al., 2021), RECI (Blobaum et al., 2018), and
8
Published as a conference paper at ICLR 2022
Table 1: Percentage (%) of recovering the true causal direction on the Tubingen cause-effect pair
datasets (Mooij et al., 2016) compared with the reported results of (Stegle et al., 2010) in the upper
table and (Zhang et al., 2015a) in the lower table.
Ours (PNL)	Ours (ANM)		LiNGAM ANM-Gauss ANM-MML ANM-HSIC PNL GPI-HSIC GPI-MML IGCI	
78.9±3.9	71±4	一		62±3	45±3	68±1	68±3	67±4	62±4	72±2	76±1	
Ours (PNL)		OUrS (ANM)		ANM^^WGP-GaUSS^^WGP-MOG^^PNL-MLP^^GPI^^IGCI
76.6±4.5		67±3 一		63	67	73	70	72	73 .
other benchmark methods in Appendix. As shown in Fig. 7 of App. F, our method performs better
than the others. Moreover, we also show the robustness of DIVOT to the prior misspecification as the
experiments in CAREFL (Khemakhem et al., 2021). The synthetic data are generated with uniform
distribution noise Ey 〜 U(0,1), where theP(Ey; θ) OfDrVOT is either uniform distribution U(0,1),
beta distribution B(a = 0.5, b = 0.5), or standard normal distribution N(0, 1). As shown in Fig.
2b, DrVOT with the misspecified noise distributions has similar performance with the one using the
correct class of distributions.
Tubingen cause-effect pair dataset. We apply DIVOT to the Tubingen cause-effect pair dataset
(Mooij et al., 2016). This is a collection of real-world cause-effect pairs. We use the variance-
based divergence measure (11) and parameterize p(Ey; θ) as in (9). As for the PNL extension, we
implement (12) without the positivity constraint (in practice imposing no constraint on ω also per-
forms well in this simple formulation). Moreover, we found that given a value of X or Y of the
datasets, there are often few samples. Thus, we consider a range of values of X, which may intro-
duce bias of the divergence measure; hence we also use a linear debiasing function for reducing the
bias. Then, we minimize the variance-based divergence measure over parameters with autograd
of JAX. More details about the debiasing function and optimization can be found in App. F. To
compare with the results reported in other works, we use the maximum number of the sample size
N = 500 as (Stegle et al., 2010; Zhang et al., 2015a) and run all the experiments with 3 random
seeds as (Stegle et al., 2010), though DIVOT is efficient enough for datasets with larger sample
sizes as shown in Appendix. In addition, we normalize data and select the ones within 2 standard
deviation, because DIVOT is sensitive to the outliers as optimal transport. As shown in Tab. 1,
our proposed DIVOT, especially the extension for PNLs, outperforms than the other methods. The
reported results in Tab. 1 are taken from (Stegle et al., 2010) and (Zhang et al., 2015a). And our
results are based on the same datasets as them, i.e., with 68 and 77 cause-effect pairs respectively.
Moreover, the entropic causal inference (Compton et al., 2021) is reported with 64.21% accuracy;
CAREFL (Khemakhem et al., 2021) is reported with 73% accuracy on 108 pairs; RECI (Blobaum
et al., 2018) is reported with 76% weighted accuracy on the 100 pairs. From Tab. 1, we can also
see that the other ANM/PNL-based methods are more sensitive to the choice of noise distributions.
As shown in Tab. 1, the ANM/PNL with Gaussian noise performs worse than the one with a more
complex distribution (mixture of Gaussian) or the one combined with a more stable measure, such
as HSIC independence test. In contrast, the PNL extension of DIVOT with the Gaussian noise has a
state-of-the-art result.
7	Conclusion
In this paper, we provide a new dynamical-system perspective of FCMs in the context of identify-
ing causal relationships in the bivariate case. We first demonstrate the connection between FCMs
and optimal transport and study the dynamical systems of the optimal transport under constraints
of FCMs. We then show that FCMs correspond to pressureless potential flows and that ANMs as
a special case corresponding to the pressureless potential flows with the velocity field divergence
equal to zero. Based on such findings, we propose a divergence measure-based criterion for causal
discovery and provide an efficient optimal transport-based algorithm, DIVOT, for identifying causal
direction between two variables. The experimental results on both synthetic and real datasets show
that compared with the state-of-the-art methods, DIVOT has state-of-the-art results and promising
properties for flexibility, efficiency, and prior misspecification robustness. We hope that the con-
nection between FCMs and optimal transport has the potential of helping understand general FCMs
from the dynamical perspective and inspiring more generic causal discovery methods.
9
Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENTS
KZ would like to acknowledge the support by the National Institutes of Health (NIH) under Contract
R01HL159805, by the NSF-Convergence Accelerator Track-D award ]2134901, and by the United
States Air Force under Contract No. FA8650-17-C7715. RT would like to acknowledge the funding
support of the Swedish e-Science Research Centre.
References
L.	Ambrosio, M. Colombo, G. De Philippis, and A. Figalli. Existence of eulerian solutions to the
semigeostrophic equations in physical space: the 2-dimensional periodic case. Communications
in Partial Differential Equations, 37(12):2209-2227, 2012.
M.	Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proceed-
ings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 214-223. PMLR, 06-11 Aug 2017.
V.	I. Arnol’d. Mathematical methods of classical mechanics, volume 60. Springer Science & Busi-
ness Media, 2013.
F. Barthe. Optimal young’s inequality and its converse: a simple proof. Geometric & Functional
Analysis GAFA, 8(2):234-242, 1998.
J.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the monge-kantorovich
mass transfer problem. Numerische Mathematik, 84(3):375-393, 2000.
P. Blobaum, D. Janzing, T. Washio, S. Shimizu, and B. SchOlkopf. Cause-effect inference by Com-
paring regression errors. In International Conference on Artificial Intelligence and Statistics,
pages 900-909. PMLR, 2018.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,
J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of
Python+NumPy programs, 2018. URL http://github.com/google/jax.
R.	T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations.
arXiv preprint arXiv:1806.07366, 2018.
D. M. Chickering. Optimal structure identification with greedy search. Journal of machine learning
research, 3(Nov):507-554, 2002.
S.	Compton, M. Kocaoglu, K. Greenewald, and D. Katz. Entropic causal inference: Identifiability
and finite sample results. arXiv preprint arXiv:2101.03501, 2021.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural
Information Processing Systems, volume 26. Curran Associates, Inc., 2013.
A. Genevay, G. Peyre, and M. Cuturi. Learning generative models with sinkhorn divergences. In
International Conference on Artificial Intelligence and Statistics, pages 1608-1617. PMLR, 2018.
A. Gretton, O. Bousquet, A. Smola, and B. Scholkopf. Measuring statistical dependence with
hilbert-schmidt norms. In International conference on algorithmic learning theory, pages 63-
77. Springer, 2005.
P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Scholkopf. Nonlinear causal discovery with
additive noise models. Advances in neural information processing systems, 21:689-696, 2008.
C.-W. Huang, D. Krueger, A. Lacoste, and A. Courville. Neural autoregressive flows. In Interna-
tional Conference on Machine Learning, pages 2078-2087. PMLR, 2018.
A. Hyvarinen and P. Pajunen. Nonlinear independent component analysis: Existence and uniqueness
results. Neural networks, 12(3):429-439, 1999.
A. Hyvarinen and S. M. Smith. Pairwise likelihood ratios for estimation of non-gaussian structural
equation models. Journal of Machine Learning Research, 14(Jan):111-152, 2013.
10
Published as a conference paper at ICLR 2022
D.Janzing and B. SchOlkopf. Detecting non-causal artifacts in multivariate linear regression models.
In International Conference on Machine Learning, pages 2245-2253. PMLR, 2018.
D. Janzing, J. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniusis, B. SteudeL and
B. Scholkopf. Information-geometric approach to inferring causal directions. Artificial Intel-
ligence, 182:1-31, 2012.
L. V. Kantorovich. On a problem of monge. Uspekhi Mat. Nauk., 1948.
I. Khemakhem, R. Monti, R. Leech, and A. Hyvarinen. Causal autoregressive flows. In International
Conference on Artificial Intelligence and Statistics, pages 3520-3528. PMLR, 2021.
S. Kolouri, G. K. Rohde, and H. Hoffmann. Sliced wasserstein distance for learning gaussian mix-
ture models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 3427-3436, 2018.
S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced wasserstein
distances. In Advances in Neural Information Processing Systems, volume 32. Curran Associates,
Inc., 2019.
A. A. Mastakouri, B. Scholkopf, and D. Janzing. Necessary and sufficient conditions for causal fea-
ture selection in time series with latent common causes. In International Conference on Machine
Learning, pages 7502-7511. PMLR, 2021.
R. J. McCann. A convexity principle for interacting gases. Advances in mathematics, 128(1):153-
179, 1997.
G. Monge. MemOire sur la theorie des deblais et des remblais. Histoire de l,Academie Royale des
Sciences de Paris, 1781.
R. P. Monti, K. Zhang, and A. Hyvarinen. Causal discovery with general non-linear relationships
using non-linear ica. In Uncertainty in Artificial Intelligence, pages 186-195. PMLR, 2020.
J. Mooij, D. Janzing, J. Peters, and B. SchOlkopf. Regression by dependence minimization and
its application to causal inference in additive noise models. In Proceedings of the 26th annual
international conference on machine learning, pages 745-752, 2009.
J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Scholkopf. Distinguishing cause from
effect using observational data: methods and benchmarks. The Journal of Machine Learning
Research, 17(1):1103-1204, 2016.
F. Otto. Viscous fingering: an optimal bound on the growth rate of the mixing zone. SIAM journal
on applied mathematics, 57(4):982-990, 1997.
S. T. Rachev and L. Ruschendorf. Mass Transportation Problems: Volume I: Theory, volume 1.
Springer Science & Business Media, 1998.
S. Shimizu, P. O. Hoyer, A. Hyvarinen, A. Kerminen, andM. Jordan. A linear non-gaussian acyclic
model for causal discovery. Journal of Machine Learning Research, 7(10), 2006.
L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference
on applications of computer vision (WACV), pages 464-472. IEEE, 2017.
P. Spirtes, C. Glymour, and R. Scheines. Causation, prediction, and search, 2nd edn., vol. 1, 2001.
O. Stegle, D. Janzing, K. Zhang, J. M. Mooij, and B. Scholkopf. Probabilistic latent variable models
for distinguishing between cause and effect. Advances in neural information processing systems,
23:1687-1695, 2010.
T. Tao.	Matrix identities as derivatives of determinant identities,
2013.	URL https://terrytao.wordpress.com/2013/01/13/
matrix- identities- as- derivatives- of- determinant- identities/.
11
Published as a conference paper at ICLR 2022
M. Yamada and M. Sugiyama. Dependence minimizing regression with model selection for non-
linear causal inference under non-gaussian noise. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 24, 2010.
K. Zhang and A. Hyvarinen. Causality discovery with additive disturbances: An information-
theoretical perspective. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 570-585. Springer, 2009a.
K. Zhang and A. Hyvarinen. On the identifiability of the post-nonlinear causal model. In Pro-
ceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 647-655,
2009b.
K. Zhang, Z. Wang, J. Zhang, and B. Scholkopf. On estimation of functional causal models: gen-
eral results and application to the post-nonlinear causal model. ACM Transactions on Intelligent
Systems and Technology (TIST), 7(2):1-22, 2015a.
K. Zhang, J. Zhang, and B. Scholkopf. Distinguishing cause from effect based on exogeneity. arXiv
preprint arXiv:1504.05651, 2015b.
12
Published as a conference paper at ICLR 2022
Appendix
In App. A, we provide the modification of Alg. 1 for including the independent case and the
significance of the results. In App. B, we provide the outlook of the extension to the multivariate
case. In App. C, we provide the analysis, the discussion, and the experimental results of our method
in the presence of unknown confounding. In App. D, we provide the proofs of Prop. 1, the extension
of Prop. 1, Prop. 2, and Thm. 1. In App. E, we include the identifiability conditions of ANMs in
(Hoyer et al., 2008). In App. F, we introduce the details of the DIVOT implementation and the
experiments:
•	App. F.1: introduce the concept, position, for computing the divergence measure (11);
•	App. F.2: show a potential problem of computing the variance-based divergence measure
in the finite/limited-sample case and introduce batches of data to deal with the problem;
•	App. F.3: show that using batches can introduce bias and lead to wrong causal relationships
(especially in the few-sample case) and then introduce debiasing functions to reduce the
bias, which is used for the experiments on real-world data in Sec. 6;
•	App. F.4: introduce the optimization methods used in the experiments on synthetic and
real-world data in Sec. 6 and show the convexity of the objective function, the divergence
measure (11), under the construction in Sec. 6;
•	App. F.5: show the robustness of DIVOT to prior misspecification;
•	App. F.6: compare DIVOT with other benchmark methods;
•	App. F.7: show the efficiency of DIVOT with its running time.
A Modification of Alg. 1 for including the independent case and
THE SIGNIFICANCE OF THE RESULTS
The proposed measure is able to deal with the independent case without relying on other methods or
tests after adapting the output conditions of the algorithm. The values of our proposed measure in
the independent case are zero in the two directions, while the measure values of the causal case are a
zero value in one direction and a non-zero value in another direction. The modified output conditions
are shown in Tab. 2. To determine that the value of our measure is zero or not, one could follow a
Table 2: Modified output conditions for the independent case.
Output of DIVOT	Div(X → Y) = 0	Div(X — Y) = 0
X, Y independent	True	True
X→Y	True	False
X—Y		False	TrUe	
similar way as (BlobaUm et al., 2018) choosing a threshold for real-world applications practically.
However, different applications may have different thresholds. For example, in practice, when the
noise distributions have different variance in the finite/few sample scenario, the larger variance can
lead to the larger measure value. Although one can handle the problem well by normalizing the
variance-based measure value with the estimated noise variance, deriving a statistical test for the
finite/few sample case is still the ideal way, which is another nontrivial task without assuming the
type of noise distributions and will be the future work of our method.
Bootstrapping for the significance of the results. Instead of testing whether a measure value is
significantly zero or non-zero, we suggest using a bootstrapping method and testing whether the two
measure values are significantly different (one could also test whether the difference between the
two measure values are significantly zero or not):
Step 1. use bootstrapping (resampling with replacement) to get B (e.g., 50) bootstrapping datasets;
Step 2. compute two measure values in the two directions for each bootstrapping dataset;
Step 3. apply a two-sample test (T-test) to check whether the mean of one measure value in one
direction is significantly different from another one in the other direction.
Step 4. if two measure values are significantly different, we conclude that the smaller one is in the
causal direction; if they are not significantly different, we conclude that it is the independent case.
13
Published as a conference paper at ICLR 2022
Moreover, we did experiments for the case where the causal relationship is so weak that the FCM
with a causal relationship is similar to the independent case (e.g., when the coefficient of the direct
cause is extremely small in the linear case, it is close to the independent case where the measure
values are close). We generate four datasets with 1000 samples for the four FCMs in the synthetic
data experiments in Sec. 6 with a weighting factor w: Y = w × f (X) + Ey, and then use the
bootstrapping method and T-test. Tab. 3 shows the p-values, of which small values indicate the sig-
nificantly different measure values. If the significant level is 0.05, our method can tell the difference
between two measures in the causal case when w > 0.02.
Table 3: Experimental results of testing the significance of the results.
W	0.01	0.02	0.03	0.04		0.05
MΓΓ	0.155	0.038	0.0006	2.429e-08		3.091e-11
M2	0.251	0.070	0.0003	0.0001		1.935e-07
M3	0.390	0.119	0.0196	0.0003		9.578e-08
M4	0.408	0.098	0.0037	2.571e-05		2.346e-11
W	0.01	0.02	0.03		0.04	0.05
M1	indep	X cause	X cause		X cause	X cause
M2	indep	indep	X cause		X cause	X cause
M3	indep	indep	X cause		X cause	X cause
M4	indep	indep	X cause		X cause	X cause
B Extension to the multivariate case
The problem of the naive solution to the multivariate case. A direct extension to the multi-
variate case is as the extensions of other bivariate causal discovery methods, such as (Khemakhem
et al., 2021), (Zhang and Hyvarinen, 2009b), and (Monti et al., 2020). One can first apply Constraint-
/score-based methods to get a causal skeleton, an undirected causal graph, and then use the extension
of our measure for finding all the causal directions. A problem of the extension in some of the other
bivariate works assuming causal sufficiency is that they directly applied their bivariate methods to
each edge of the causal skeleton without considering the DAG structure. This can lead to the wrong
results, especially in the case where they disregard the common parent/confounder of an edge.
Extending our method directly to the multivariate case. We provided the extensions of Prop.
1 and the variance-based measure, which can be used for computing the measure value considering
the DAG structure and then orienting the edges based on the causal skeleton in the multivariate
case. Our extension to the multivariate case has the following properties: Given the causal skeleton,
1.	it distinguishes Markov equivalent classes under the identifiability conditions of ANMs (or
PNLs);
2.	it still benefits from the closed-form 1D optimal transport solution and doesn’t have the
computational issue as in the high-dimensional optimal transport methods because of the
FCM constraints;
3.	it provides a score of which the causal structure has the minimum value compared with all
DAGs of the causal skeleton; moreover, the measure value of a DAG is the summation of
all measure values of the causal modules/conditionals/mechanisms.
First, we show the derivation of the extension of Prop. 1 for ANMs, given which the derivation
in the PNL case is straightforward (one can consider what we did for Eqn. (12) in our paper). The
proposition shows how to efficiently compute the L2 Wasserstein distance between high dimensional
distributions under FCM constraints. Suppose that the general ANM is Xi = gi (PAi) + Ei, where
i = 1, ..., m, Ei is the noise term of Xi, and PAi denotes the parent variables of Xi. The square of
L2 Wasserstein distance is
m
W22(p0,pT) =XEPAi[W22(p(Ei),p(Xi|PAi)],
i=1
14
Published as a conference paper at ICLR 2022
of which the derivation is shown in App. D.2.
Second, as a direct implication of Thm. 1, given the couplings of Ei and Xi, the corresponding
dynamical system has zero divergence of its velocity field; in other words, the corresponding dy-
namical system which moves the samples of x0 to the samples of xT under ANM constraints has
zero divergence on each dimension. And the variance-based measure is
C ，、 G 1 L	∣∣sort(xi∣pAi=k) - sort(→) - ave(----→ - →)||2
Dvar(V) ≈ T N T --------------------------------------------N----------------------------，
i=1 k∈{samples of PAi }N
where x--i|P--A-i=→k is the data vector, of which the elements are the Xi values of the samples with P Ai
taking the value k; →-ei is the data vector, of which the elements are the generated noise samples; Nk
represents the length of the vector x--i|P--A-i=→k or -→ei .
Next, as the direct extension, one could enumerate all possible DAGs of the causal skeleton and
compute their measure values, of which the minimum value is corresponding to the causal graph.
Because the causal skeleton is given, it must be the case where one of the two variables of an edge
is the cause and the other one is the effect. So the enumerated graphs have two situations: 1) all
the edges are correctly oriented; 2) the causal direction of at least one edge is wrong such that the
measure value of at least one causal module is significantly larger than the correct one (note that
considering a child as the direct cause leads to increasing the measure value, while omitting a cause
is not necessary to increase the measure value of the causal module). Therefore, we can simply
choose the graph with the minimum measure value as the causal one. As mentioned in the paper, it
is also very important to develop practical algorithms for large-scale real-world problems, and there
are some points for future works to further explore:
1.	Testing the significance of the results in the multivariate case, i.e., whether the minimum one is
significantly smaller than the others. One could apply bootstrapping to the dataset and have a p-
value for the measure value. One could also apply bootstrapping for each causal module, however,
there may exist the problem of the multiple statistical test issue with family-wise errors in this way.
2.	Developing an efficient search algorithm in the multivariate case without relying on constraint-
/score-based methods. This requires to analysing the measure value of a causal module in more sit-
uations (e.g., omitting a parent, involving an independent variable/non-child descendant/non-parent
ancestor, or a case mixing the mentioned factors) and considering the characteristics to develop an
efficient search algorithm similar as the greedy search algorithm (Chickering, 2002).
C Unknown confounding
The unknown confounding has different influences on the results of our method in different situ-
ations: 1) independent case: when two variables are independent; 2) Causal case: when there is a
causal relationship between two variables. Depending on how the unknown confounding influences
the pair of variables, it can make our method
1.	reverse the direction of the result in the causal case;
2.	disregard the causal relationship in the causal case;
3.	introduce extraneous causal relationship in the independent case.
To understand the results, we could analyze some toy examples intuitively. One extreme
independent case is that X = U and Y = Ey + U, where U is the unobserved confounder; Ey
is the noise of Y; X has no noise. Then our method will show that X is the cause of Y, even though
there is no causal relationship. Similarly, suppose that U and the noise of X and Y have the same
distribution with variance equal to 1, and that X = Ex + 1000U and Y = Ey + U. Then, the
distribution of X can be dominated by U, which leads to the wrong result for the same reason as the
extreme case.
In the Causal Case, it follows the same reason. For example, X = Ex + U and Y = X+Ey + 1000U.
Then the distribution of Y can be dominated by U. Therefore, the FCM is close to X = Ex + U
and Y = 1000U, which leads to reversing the causal direction in the result. Moreover, when
X = Ex + 1000U and Y = X +Ey + 1000U, then both distributions ofX and Y can be dominated
15
Published as a conference paper at ICLR 2022
by U; hence, it is close to X = 1000U and Y = 1000U, which leads to disregarding the causal
relationship in the result.
Nevertheless, in practice, the impact of unknown confounding is more complex and all factors can
be mixed together with the impact of finite samples and the function properties. Therefore, we
provide the experiments based on the synthetic datasets. From the experimental results, we can also
find that the strength of the confounding and the difference of the confounding strength on the two
variables are two important factors resulting in the wrong results of our method.
Experimental results for the unknown confounder. We generate a dataset with 1000 samples
for each FCM:
FCM1) X = U andY = U;
FCM2) X =	Ex	+ wx	× U and Y = Ey + wy ×	U ;
FCM3) X =	Ex	+ wx	× U and Y = f (X) + Ey	+ wy	× U,
where Ex and Ey are the noise terms; U is the unknown confounder; wx and wy are the coefficients
of U representing the confounding strength; we used f(X) = X and f(X) = sin(4X) in the
experiments. As for the data generation, Ex, Ey, and U follow the uniform distribution, U(0, 1),
and we vary the coefficients wx and wy .
Instead of testing whether a result is significantly close to zero, we use bootstrapping to resample
50 datasets for each generated dataset and then apply a two-sample test to test whether the measure
values in the two different directions are significantly different. Because although the measure value,
in theory, is zero in the causal direction when there is no unknown confounder, with finite samples
the measure value will be larger than zero in practice. And to decide which measure value is close to
zero or not, it requires a threshold which can vary in different applications, or deriving a statistical
test which is nontrivial for the measure without assuming the type of noise distributions and can be
the future work of our method. Thus, we test whether two measure values are significantly different,
and if so, we then conclude that the smaller one is in the causal direction; if they are not significantly
different, we then conclude that it is the independent case. As for the experimental results, when
the p-value is close to zero, it means that the two measure values are significantly different. And
in general, one could take the significant level at 0.05 to make a conclusion. We used 0.05 for the
experiments.
The experimental result of FCM1 is that the p-value is equal to 1.0, which means that the measure
values in the two directions are not significantly different.
As for the experimental results of FCM2, we found that in the independent case, when the coeffi-
cients wx and wy are the same, we can get the correct results; when the coefficients are different, the
method will give the wrong results. In different applications/scenarios, there are different tolerance
ranges for our methods such that when the difference of the confounding coefficients is within the
range, even if the confounding coefficients are different, we can still get the correct results.
Table 4: The experimental results of FCM2 (the independent case) in the presence of unknown
confounding.
wx\wy	0.1	1.0	10	Wχ∖Wy	0.1	1.0	10
0.1	0.354	1.57e-39	9.20e-05	-0∏	indep	causal	causal
1.0	-	0.159	3.20e-49	1.0	-	indep	causal
10	-	-	0.451	10	-	-	indep
As for the experimental results of FCM3, we found that in the causal case, the confounding strength
is a factor influencing the results more, which is different from the independent case. Moreover,
we found that when the nonlinear function f(X) is non-monotonic, such as sin(4X), the number
of correct results is larger than the one in the linear case. Because the non-monotonic function
itself can introduce a type of asymmetry which can indicate the causal direction for our method, the
non-monotonic functions can be easier than the monotonic functions.
Future work for dealing with the unknown confounders. Causal discovery in the presence of
unknown confounding is still an open problem, but there are some promising results in recent years
such as (Janzing and SchOlkopf, 20l8; Mastakouri et al., 2021). Especially, (MastakoUri et al.,
16
Published as a conference paper at ICLR 2022
Table 5: The experimental results of FCM3 (the causal case) with f (X) = X in the presence of
unknown confounding.
wx∖wy	0.1		1.0	10	100
0~1Λ	8.56e-56		2.98e-36	0.157	1.42e-06
1.0		1.22e-49	0.00027	9.60e-49	3.26e-51
10	0.002		7.02e-05	0.0018	0.007
100	0.385		0.398	0.577	0.70
wx\wy		0.1	1.0	10	100
0.1		X cause	X cause	indep	X cause
1.0		X cause	X cause	Y cause	Y cause
10		Y cause	Y cause	Y cause	X cause
100		indep	indep	indep	indep
Table 6: The experimental results of FCM3 (the causal case) with f (X) = sin(4X) in the presence
of unknown confounding.
wx\wy	0.1		1.0	10	100
0.1		1.42e-06	1.66e-68	5.62e-16	2.91e-09
1.0	5.68e-76		4.70e-81	5.62e-16	6.94e-51
10	2.17e-55		2.70e-47	1.36e-51	0.50
100	2.93e-06		2.70e-47	9.81e-24	0.08
wx\wy		0.1	1.0	10	100
0.1		X cause	X cause	X cause	X cause
1.0		X cause	X cause	X cause	Y cause
10		Y cause	Y cause	X cause	indep
100		Y cause	Y cause	X cause	indep
2021) shows the identifiability results on time-series data in the presence of memoryless unknown
confounders where the confounder doesn’t have an autocorrelation effect. A potential research
direction with our framework is that by recovering the corresponding dynamical process of a static
causal discovery problem while considering the memoryless confounding, we can determine the
causal direction between two variables in the presence of memoryless unknown confounders.
D Proofs and derivation
D. 1 Derivation of Prop. 1
Under constraints (I) and (II),
W22 (p0 , pT )
=R |M*(xo)- X0∣2p0(x0)dx0
RR (f(Ex, Ey) - Ey)2p(Ey)dEyp(Ex)dEx
= EEx[W22(p(Ey),p(Y |Ex))]
definition of W22
Eqn. (6), and the independence of Ex and Ey
D.2 Proof of the extension of Prop. 1
Suppose that in the multivariate ANM, x0 = [E1, ..., Em]0, and xT = [X1, ..., Xm]0, and Xi
gi(PAi) + Ei, where PAi denotes the parent of Xi.
17
Published as a conference paper at ICLR 2022
W22 (p0, pT )
|Mi(x0) - x0|2p0(x0)dx0
|Mi ([E1, .., Em]0) - [E1, .., Em]0|2p(E1, ..., Em)dx0
m
X	|Mii ([E1, .., Em]0) - Ei|2p(E1, ..., Em)dx0,
i=1
(where Mi represents the i-th element of M ")
m
X	|Mii([E1, ..,Em]0) - Ei|2p(Ei)dEip(E1:m-i)dE1:m-i,
i=1
(where 1 : m - i represents 1, ..., i - 1, i + 1, ..., m,
and the independence of noise implies p(Ei)p(E1:m-i))
m
X	|f(Ei,E1:m-i) - Ei |2p(Ei)dEi p(E1:m-i)dE1:m-i
i=1
m
X |fe(Ei, M1i:m-i(E1:m-i)) - Ei|2p(Ei)dEip(E1:m-i)dE1:m-i
i=1
m
X |fe(Ei,X1:m-i)) - Ei|2p(Ei)dEip(E1:m-i)dE1:m-i,
i=1
(where applying the change of variable formula)
m
X	|fe(Ei,X1:m-i))-Ei|2p(Ei)dEip(X1:m-i)dX1:m-i|det(J)|-1
i=1
m
X	|fe(Ei,X1:m-i)) - Ei|2p(Ei)dEi p(X1:m-i)dX1:m-i,
i=1
(where |det(J)|-1 = 1 for ANMs)
m
XEPAi[ |fe(Ei,PAi))-Ei|2p(Ei)dEi]
i=1
m
XEPAi[W22(p(Ei),p(Xi|PAi)]
i=1
D.3 Proof of Thm. 1
Part I. We first derive the time interpolation p(t, xt) of p0 and pT under dynamical FCM con-
straints. Suppose that the FCMs are Y = f (X, Ey). As for the dynamical formulation of
the L2 Wasserstein distance under constraints (I) and (II), according to the Jacobian equation,
p(t, Xt) = po(xo)∕∣ det( J ∂χt )|, t ∈ (0, T), where xt, x° ∈ R2 and J』is the Jacobian matrix w.r.t
dx0
dt Idet(J ∂xt )1	—
χ0. Moreover,	g∂( , “ = - ∣det(J∂χt)∣ , where J∂Xt
∂χo	x0
∂χ0
1 + T,	0
东 ∂x, T (∂f - I) + 1
18
Published as a conference paper at ICLR 2022
According to Jacobi’s formula and (Tao, 2013),
-d I det(A(t))∣
dt
=± det(A⑴加卜⑴T ItA⑴)
=±det(A(t))tr (det(A(t))-1adj(A(t))∣^A(t))
=± XXX (adj(A(t))0 Θ dtA(t)).
where is element wise multiplication and adj(A) is adjugate matrix of A and the sign takes the
same sign as det( Jg).Furthermore, since We know that under the structural constraints (I) and
dx0
(II), M* is Eqn. (6). Thus, we replace A(t) with J区=I + T(VM* 一 I), and then we have
dx0
J ∂χt
dx0
adj(J ∂χt Y
1 + T,	0
tf t( ∂f n , 1
T ∂X , T ( ∂Ey - 1) + 1
T(∂Ey — 1) + 1， TdX ;
0,	1 + T
d
dt
J∂xt
dx0
T1(VM* - I)
0,
1 ∂f
T ∂X，
Therefore,
d|det(J短)| = ±T(∂f - 1)(1 + TT),
which takes the same sign as det( J纯)
∂X0
Part II. We then proof the property of the corresponding dynamical systems of ANMs in Thm. 1.
∂ । det (J ∂×t ) 1
| det(Ja∂x⅞~, under the conditions in Thm. 1, ∂-f- = 1 and it is obvious
dχt
Because 2PTxl
	
that dT∣ det( J区)∣ = 0 and ∣ det( J绰)∣ = 0, ∀t ∈ [0,T]. Furthermore, according to the theorem
dx0	dx0
of instantaneous change of variables (Chen et al., 2018), which is a variant of Fokker-Plank equation,
we know that
d log『) = -tr(Vv) = -div V
Therefore, div v = 0.
D.4 Proof of Prop. 2
Necessary direction. We prove the necessary direction by showing that given X is the direct
cause of Y in an ANM, D(v) = 0. In the causal direction, because of the time evolution equation
(5), we know that it is sufficient to check the divergence of the velocity field at time 0, and that
the divergence taking zero value everywhere at time 0 implies the divergence taking zero value
everywhere for t ∈ [0, T]. Because of Thm. 1, we know the div v = 0 at time 0. Therefore,
D(v) = 0.
Sufficient direction. We prove the sufficient direction by showing the contradiction with the iden-
tifiability of ANMs. Given that D(v) = 0 under the constraints (I), (II), (III), and the identifiability
conditions of ANMs, it tells us that there is an ANM in the form of Eqn. (1) which is consistent
with the data distribution and has independent noise. Because of the identifiability of ANMs shown
19
Published as a conference paper at ICLR 2022
by (Hoyer et al., 2008), there is no ANM with Y → X which is consistent with the data distribution
and has independent noise at the same time. Therefore, it can only be the case the X is the cause of
Y.
Moreover, there is a concern that it may happen that in the direction which is not the causal direction,
the model is not an ANM and its divergence measure is equal to zero. This may be problematic if
the divergence measure is applied to applications. However, we will show that in general, this will
not happen by proving that under weak assumptions D(v) = 0 implies the model is an ANM.
First, D(v) = 0 implies div v = 0 for all x0 with positive probability densities p(x0) > 0. In the
following We say div V = 0 in short. Second, div V = 0 implies "p∂X) = 0 a.e. according to Thm.
d| det(J∂χt )|
1. Consequently,-------壮广0 =0, which leads to ∂df = 1 where Y = f (X, Ey) a.e. according to
the proof of Thm. 1. Therefore, under the assumptions: 1) p(Ey) is positive in a continuous range of
Ey; 2) in the range, ∂f = 1 a.e. implies that it holds everywhere, we have Y = Ey + C, where C
is a quantity which doesn’t change with Ey, e.g., it can be a function ofX or a constant. Therefore,
D(V) = 0 implies Y = Ey + g(X) further under assumptions 1) and 2).
E Identifiability conditions of ANMs in (Hoyer et al., 2008)
Because the identifiability conditions of ANMs in (Hoyer et al., 2008) are important and necessary
for our Thm. 1, we include them here:
Let the joint probability density of x and y be given by p(x, y) = pn(y - f (x))px (x), where pn,
px are probability densities on R. If there is a backward model of the same form, i.e., p(x, y) =
Pn(X - g(y))py(y), then, denoting V := logPn and ξ := logpx, the triple (f,Pχ,Pn) must satisfy
the following differential equation for all x, y with ν00(y - f(x))f0(x) 6= 0:
ν0(f 〃/
f0	,
where we have skipped the arguments y - f (x), x, and x for ν, ξ, and f and their derivatives,
respectively. Moreover, if for a fixed pair (f, ν) there exists y ∈ R such that ν00(y - f (x))f 0 (x) 6= 0
for all but a countable set of points x ∈ R, the set of all Px for which P has a backward model is
contained in a 3-dimensional affine space.
ξ000 = ξ00(-S + f) - 2ν00f 00f 0 + v0f000 + -
ν00 f0	ν00
F	Experiments and details of DIVOT
For the synthetic data experiments in Sec. 6, we visualized the generated data with the sample size
500 in Fig. 3. For determining causal direction, we used the variance-based divergence measure in
Figure 3: Synthetic data generated from ANMs with different (non)linear functions. From left to
right, they are (1) Y = X + Ey; (2) Y = 0.1(2.5X)3 - 0.1X + Ey; (3) Y = sin(4X) + Ey; (4)
Y = 0.5X3 — X + Ey if x < 0, and Y = 1 — 0.5X3 + X + Ey, otherwise, where X 〜U(-1,1)
and Ey 〜U(0,1) are uniform distributions.
Eqn. (11). The experiment setup is the same for experiments with different ANMs. For each ANM,
we run experiments with different sample sizes, 10, 25, 50, 100, 200, and 500. For each sample size,
100 different datasets are generated. For all the synthetic data experiments in Sec. 6, we use batches
for computing Eqn. (11) without using debiasing functions. In the following, we introduce positions
(App. F.1), batches (App. F.2), debiasing functions (App. F.3), and optimization details (App. F.4)
of DIVOT; as well as show its robustness to prior misspecification (App. F.5), more comparison
with other benchmark methods (App. F.6), and its efficiency (App. F.7).
20
Published as a conference paper at ICLR 2022
(a) The influence of batch sizes on determining causal (b) The influence of batch sizes on estimating p(Ey; θ)
direction and the improvement with a linear debiasing and the improvement with a linear debiasing function.
function.	The ground-truth θ is 1.
Figure 4: The influence of the batch size and the debiasing function on causal direction determina-
tion in panel (a) and density estimation in panel (b). The synthetic data (of which the sample size is
100) are generated from the linear ANM, Y = X + Ey, where X ~ U(-1,1) and Ey ~ U(0,1)
are uniform distributions. In panels (a) and (b), the batch size is represented as the ratio of the
sample size, e.g., 0.1 represents that the batch size is 0.1 × 100; the red triangular represents the
experiments with only using the variance-based divergence measure; the blue star represents using
the the variance-based divergence measure and a linear debiasing function. The error bar in panel
(b) represents standard deviation.
F.1 Number of positions for computing the variance-based divergence measure
Suppose a dataset {(xi, yi)}N with sample size N. For computing the divergence measure, we call
x ∈ {xi}N as a position and for each x ∈ {xi}N we need to specify the corresponding vector y-→x,
of which the number of elements is Nx and generated samples of -→ey. In practice, it is not necessary
to use all the N positions for computing Eqn. (11). For the synthetic data experiments in Sec. 6,
when N > 50, we choose 50 positions outofN; otherwise, we choose all the N positions. To select
a position (when N > 50), we first find the maximal and minimal values of {xi }N, compute the
interval length, max({xi}N) - min({xi}N) /50, and then choose a position every such length
(or its nearest position available in the data). We can then compute Eqn. (11) for causal direction
determination.
F.2 Batches for the finite (or limited) sample scenario
In the finite sample scenario, it can be the case that there is no other data at a position x. Therefore,
we use the neighbors of x as a batch of data for computing Eqn. (11). We consider all the data in
a batch as having the same value x. We represent the batch size with the percentage of the total
number N of samples. For the synthetic data experiments in Sec. 6, we use the batch size 0.4 for the
datasets with sample size 10; 0.2 for the datasets with sample sizes 25 and 50; 0.15 for the datasets
with sample sizes 100 and 200; and 0.05 for the datasets with sample size 500.
Note that for the synthetic data experiments in Sec. 6, the batch size is not larger than 0.2 except
the extremely small datasets with 10 samples. Because using larger batch sizes introduces bias by
concatenating data at different positions in a batch. For the experiments in Fig. 4a, we generate
a synthetic dataset with sample size 100 and use all the 100 positions for computing Eqn. (11).
We can see that choosing a proper batch size can increase the accuracy of DIVOT by increasing
the number of samples in a position; however, if further increasing the batch size such that it is
larger than 0.2, the accuracy is decreased because concatenating data at different positions leads to a
different corresponding noise distribution compared with the ground-true p(Ey). Thus, as shown in
Fig. 4b, the estimation of the noise distribution becomes worse and worse with increasing the batch
size. Such bias can be problematic especially in the few-sample case where we have to choose a
larger batch size for computing the divergence measure. Therefore, we use a debiasing function such
21
Published as a conference paper at ICLR 2022
(a) The debiasing function and g(x) in the ANM.
(b) The samples of (xi, yidebias).
Figure 5: As a by-product, debiasing functions can be used for estimating the nonlinear function
of an ANM in Eqn (1). We optimize the parameters of a neural network representing a nonlinear
debiasing function and the parameter of a uniform distribution representing the hypothesized noise
distribution. As for panel (a), the ground-true noise distribution is corresponding to θ = 1 and the
estimated parameter θ is 1.066. As for panel (b), given a dataset {(xi, yi)}, yidebias = yi - gd(xi).
that we can use a larger sample size for the few-sample case without sacrificing the performance of
DIVOT.
F.3 Debiasing functions for the few-sample scenario
Because the mentioned bias is introduced by omitting the position information, we introduce it back
with a debiasing function gd(x) to reduce the bias by modifying Eqn. (11) as
-----→	-----→	2
sort(yxdebais) - sort(-→ey) - ave(yxdebais - -→ey)
Dvar(V) ≈ N X -----------------------------N-1---------------------2, ey 〜P(Ey ；外
x∈{xi}N	x
------→
yxdebais = y-→x - gd(x; w),	(13)
where gd(x; w) is a (non)linear function parameterized with w, e.g., in our experiments the linear
debiasing function is gd(x) = w × x. As shown in Fig. 4, using the linear debiasing function
guarantees not only the accuracy while using a larger batch size than 0.2, but also the correctness of
the density estimation.
For a more complicated scenario, e.g., the ANM uses a nonlinear function, one can apply a neural
network to the debiasing function. We found that as a by-product, a sufficient flexible debiasing
function can be used for estimating g(x) of ANMs in Eqn. (1). As shown in Fig. 5, we generate
synthetic 500 data ofan ANM with the nonlinear function Y = g(X) +Ey = 0.1(2.5X)3 - 0.1X +
Ey, where X 〜U(-1,1) and Ey 〜U(-0.5,0.5) satisfy uniform distribution. The gd(x) is close
to the ground-truth g(x). Because this is not the main focus of the work, we would like to refer the
readers to the Jupyter notebook in supplementary materials for the details of the implementation.
Nevertheless, the purpose of using debiasing functions is not to estimate the g(x) accurately but to
reduce the bias introduced by using a large batch size. This means that when we use a restrictive
class ofgd(x), it can lead to a noticeable difference ofgd(x) and g(x). But for the purpose of causal
direction determination, the divergence measure does not require to specify the nonlinear functional
form of ANMs. Moreover, DIVOT based on optimal transport does not require to estimate g(x)
either. Therefore, DIVOT can still determine causal direction correctly in such case.
22
Published as a conference paper at ICLR 2022
F.4 Optimization methods and the convexity of the objective functions of
DIVOT
In this section, we introduce the optimization method of the synthetic experiments in Sec. 6 and
the convexity of the objective function. Next, we introduce the optimization method of DIVOT in
the real-world data experiments in Sec. 6, of which the divergence measure uses a linear debiasing
function and standard Gaussian noise.
As for the synthetic data experiments, we minimize the variance-based divergence measure in Eqn.
(11) as an objective function, of which the parameter is θ. Because when θ > 0, the linear function
fθnoise(eysource) does not change the order of sorting results of e-ys-o-u-r→ce, we have
Dvar (v)
事X

x∈{xi}N
Σ
x∈{xi}N
sort(y-→x) - fθnoise(sort(e-ys-o-u-r→ce)) - ave(y-→x - fθnoise(e-sy-o-u-r→ce))2
Nx - 1
1
N
sort(y-→x) -θ × sort(e-ys-o-u-r→ce) - ave(y-→x -θ × e-ys-o-u-r→ce)22
Nx - 1
fnoise(eyource) = θ × eyou-ce, θ > 0, esource 〜U(0,1).
(14)
It is obvious that the divergence measure in Eqn. (14) is convex on θ > 0. The convexity is achieved
by parameterizing the noise distribution with a linear function fnoise, and there are many exited
toolboxes for solving the minimization problem.
For the synthetic data experiments, We used gradient descend for finding the optimal θ*. The gradi-
ent jax.grad(loss) is computed with the autograd in JAX (Bradbury et al., 2018). We update
θ by specifying a step size sz and θ := θ - jax.grad(loss) × sz. If after the update θ < 0 (Which
has never happened), We set the value of θ as a positive number close to zero. We used sz = 1 for
all the synthetic data experiments. Moreover, because of the convexity, one can also use an one-step
update method to directly get the best parameter θ* by finding the root of the gradient function of
Eqn. (14). We simply give a range of θ and find θ* by a binary search method such that the gradient
at θ* is equal to zero. The experiments OfDIVOT for ANMS in the real-world data experiments also
used the one-step update for updating θ and the range θ is specified as [0, 100]. In addition, We need
to optimize over the parameter w of the linear debiasing function. We use gradient descent to find
the best parameter, w*. We update θ every 10 updates of w. For the PNL extension of DIVOT, we
used the cyclic learning rate (Smith, 2017) with gradient descent. We update θ every 10 updates of
w andω in (12). The program of DIVOT is terminated when the divergence measure converges. Ifa
more complicated scenario requires fθnoise to be a nonlinear function, one may need to use the gra-
dient descend method instead of the one-step update. We find that it is sufficient to use the standard
Gaussian distribution with the linear fθnoise for DIVOT to have promising results in the experiments
on the Tubingen datasets, which indicates that DIVOT is robust to the choice of models. We then
investigate the robustness of DIVOT to prior misspecification.
F.5 Robustness to prior misspecification
We use different hypothesized noise distributions in DIVOT to test the robustness to the misspecifi-
cation of noise distribution. The synthetic data are generated as in Sec. 6 with uniform distrbution
noise Ey 〜U(0,1). In DIVOT, we use one of the three hypothesized distributions: uniform distri-
bution U(0, 1), beta distribution B(a = 0.5, b = 0.5), and standard normal distribution N(0, 1). As
shown in Fig. 6, DIVOT with the misspecified noise distributions has similar performance with the
one using the correct class of distributions, which shows the robustness of DIVOT.
F.6 Comparison with results of benchmark methods
We compare DIVOT with other benchmark methods, such as ANM (Hoyer et al., 2008), CAREFL
(Khemakhem et al., 2021), RECI (Blobaum et al., 2018), and LLR (Hyvarinen and Smith, 2013).
As in (Khemakhem et al., 2021), the synthetic data are generated with the Laplace distribution as
23
Published as a conference paper at ICLR 2022
Figure 6: Robustness to prior misspecification: The performance of DIVOT with different hypothe-
sized distributions of P(Ey; θ) on the datasets with uniform distribution noise Ey 〜U(0,1).
Y=fpiecem + Ey
IOO 200	300	400	500
Sample size
Linear 6(0
X 2 =X ι + z2
∙8-64
Ooo
100	200	300	400	500
Sample size
Additive noise 6(0
x 2 = X1 + 1X3 + z 2
100	200	300	400	500	100	200	300	400	500
SamPle size	SamPle size
Figure 7: Comparison of the results of DIVOT (the first row) with the reported results in (Khe-
makhem et al., 2021) (the second row). The figures in the second row are taken from the screen-
shot of (Khemakhem et al., 2021). The noise (Z2 and Ey) distribution of the synthetic data is
a Laplace distribution. CAREFL represents the causal autoregressive model (Khemakhem et al.,
2021); CAREFL-NS represents the causal autoregressive model without scaling; RECI represents
the method, regression error causal inference (BIobaUm et al., 2018); ANM (Hoyer et al., 2008)
uses Gaussian process while ANM-NN uses a neural network; linear LR is the linear likelihood
ratio method (HyVarinen and Smith, 2013).
the noise distribution. DIVOT uses batches without debiasing functions. And the data preprocessing
is the same as in the real-world data experiments in Sec. 6. We run experiments on each dataset for
100 times. As shown in Fig. 7, for the sample size larger than 100 the accuracy of DIVOT is 100%
which shows that DIVOT has promising results and performs better compared the other methods,
especially in the linear case.
24
Published as a conference paper at ICLR 2022
F.7 Efficiency of DIVOT
For common causal discovery tasks in the bivariate case, the sample size 10000 is a large and chal-
lengeable one. Causal discovery methods need to consider the efficiency of algorithms especially
in the large sample size scenario. We apply DIVOT with batches and no debiasing functions to the
synthetic data, of which the sample size is 10000 generated with Y = 0.1((2.5X)3 - X) + Ey
and Ey 〜 U(0,1). As shown in Table 7, We test the running time of DIVOT to determine causal
direction with different number of positions and different batch sizes. The experiments are based on
MacBook Pro (15-inch, 2018) with 2.9 GHz 6-Core Intel Core i9. Our implementation is based on
JAX (Bradbury et al., 2018) which uses Apache License and the running time is measured with the
command %timeit in JAX.
Table 7: Running time of DIVOT on synthetic data, of which the sample size is 10000. The DIVOT
with batches and no debiasing function is tested with different number of positions and batch sizes.
batch size	50 positions	100 positions
0.001	240 ms±3.48 ms	431 ms ± 4.33 ms
0.01	7.71 s ± 475 ms	14.3 s ± 262 ms
0.1	1min 7s ± 1.81 s	2min 19s ± 2.11 s
25