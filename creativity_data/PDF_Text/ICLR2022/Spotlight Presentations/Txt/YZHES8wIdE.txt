Published as a conference paper at ICLR 2022
Generative Planning for Temporally Coordi-
nated Exploration in Reinforcement Learning
Haichao Zhang Wei Xu Haonan Yu
Horizon Robotics, Cupertino CA 95014
{haichao.zhang, wei.xu, haonan.yu}@horizon.ai
Ab stract
Standard model-free reinforcement learning algorithms optimize a policy that gen-
erates the action to be taken in the current time step in order to maximize expected
future return. While flexible, it faces difficulties arising from the inefficient explo-
ration due to its single step nature. In this work, we present Generative Planning
method (GPM), which can generate actions not only for the current step, but also
for a number of future steps (thus termed as generative planning). This brings
several benefits to GPM. Firstly, since GPM is trained by maximizing value, the
plans generated from it can be regarded as intentional action sequences for reaching
high value regions. GPM can therefore leverage its generated multi-step plans for
temporally coordinated exploration towards high value regions, which is potentially
more effective than a sequence of actions generated by perturbing each action at
single step level, whose consistent movement decays exponentially with the number
of exploration steps. Secondly, starting from a crude initial plan generator, GPM
can refine it to be adaptive to the task, which, in return, benefits future explorations.
This is potentially more effective than commonly used action-repeat strategy,
which is non-adaptive in its form of plans. Additionally, since the multi-step plan
can be interpreted as the intent of the agent from now to a span of time period
into the future, it offers a more informative and intuitive signal for interpretation.
Experiments are conducted on several benchmark environments and the results
demonstrated its effectiveness compared with several baseline methods. Code is
available: https://github.com/Haichao-Zhang/generative_planning.
1	Introduction
Reinforcement learning (RL) has received great success recently (Mnih et al., 2015; Lillicrap et al.,
2016; Gu et al., 2017; Haarnoja et al., 2018), reaching or even surpassing human-level performances
on some tasks (e.g. Mnih et al., 2015; Silver et al., 2016). However, the number of samples required
for training is still quite large, rendering sample efficiency improvement a critical problem. Standard
model-free RL methods operate at the granularity of individual time step. Although flexible, one
prominent issue is inefficient exploration, as the consecutive one-step actions are not necessarily
temporally consistent, which makes the probability of consistent movement decays exponentially
with the exploration steps, rendering many of the exploration efforts wasted (Dabney et al., 2021).
Extensive efforts have been made on improving the temporal consistency of exploration from different
perspectives with different approaches, including parameter space perturbation (e.g. Plappert et al.,
2018b; Fortunato et al., 2018; Xie et al., 2019), enhancing correlation between policy steps (e.g.
Lillicrap et al., 2016; Korenkevych et al., 2019; Amin et al., 2021; Zhang & Van Hoof, 2021)
and skill/option-based exploration (e.g. Agarwal et al., 2018; Tirumala et al., 2020; Pertsch et al.,
2020). Among them, simple action repeat-based temporal persistency has been shown to be very
successful (e.g. Mnih et al., 2015; Sharma et al., 2017). Action repeat refers to the strategy of repeating
a previous action for a fixed or variable number of time steps. This simple strategy demonstrates
strong performance on a broad range of tasks and has been actively investigated recently (Neunert
et al., 2019; Dabney et al., 2021; Biedenkapp et al., 2021; Yu et al., 2021).
Planning is another effective approach for obtaining a control policy alternative to RL (e.g. Chua
et al., 2018; Srinivas et al., 2018; Hafner et al., 2019; Eysenbach et al., 2019). Typically, it is used to
1
Published as a conference paper at ICLR 2022
St
TneW
Figure 1: Generative Planning Method. GPM has three main components. i) Plan generation. At
Tactual
Π
each time step, the plan generator Π generates a plan Tnew based on the state st. ii) Plan update. Told
is obtained by “time-forwarding” the previous actual plan P(Tactual) one step. iii) Plan switch. The
replanning signal is generated based on the values of the previous plan Told and the new one Tnew,
and only switches if certain criteria is satisfied. The plan enclosed with a black box indicates the one
been selected at that time step. In the case of driving, the primitive action to be executed is derived
from the plan as ω(Tactual), which is sent to the environment for execution as shown on the right.
optimize a sequence of actions by online minimization of a cost function computed together with a
model (Chua et al., 2018; Srinivas et al., 2018; Hafner et al., 2019). The control loop is closed with
model-predictive control (MPC), i.e. by executing the first action only and re-run the optimization
again in the next time step. Planning excels at reasoning over long horizons and has a high sample
efficiency, but is more computationally demanding and requires access to a model (Levine & Koltun,
2013a;b; Lowrey et al., 2019; Srinivas et al., 2018; Chua et al., 2018; Hafner et al., 2019). A large
body of work in the literature has explored how to leverage the notable complementary strengths
of RL and planning from many perspectives, e.g., guiding policy learning with planning (Levine &
Koltun, 2013a;b), improving planning with a learned policy (Schrittwieser et al., 2019; Wang & Ba,
2020) and generating intrinsic reward for exploration (Sekar et al., 2020).
This work is motivated by recognizing both the apparent complementary strengths and less apparent
connections between these two streams of seemingly different approaches, i.e., simple but successful
action repeat strategy and flexible but more demanding planning-based control. We note that they
can be connected from the perspective of planning for exploration. On the one hand, we can go
beyond simply repeating the same actions by using planning to induce more flexible forms of action
sequences for temporally coordinated exploration. On the other hand, the empirical success of action
repeat suggests that the form of planning that is useful for exploration could be possibly done in a
way that is lightweight and retains the overall model-free nature of the whole algorithm.
Motivated by these observations, we propose a Generative Planing Method (GPM) as an attempt to-
wards the goal of model-free RL with temporally coordinated, planning-like behavior for exploration.
The overall framework is shown in Figure 1. At the beginning of an episode, a plan T is generated.
The plan is a temporally extended sequence of actions and the one-step action for the current time
step of execution is derived from it as ω(T). Upon entering the next time step, the old plan is adjusted
by “time-forwarding” one step for alignment as P(T). The old plan is committed to until certain
conditions are met when compared to a new one generated at each time step. By doing this, we have
enabled both flexible intentional action sequences and temporally extended exploration. We refer this
type of temporally extended and intentional exploration as temporally coordinated exploration.
The contributions of this work can be summarized as follows:
•	we provide a natural perspective on extending standard model-free RL algorithms to gen-
erate multi-step action sequences for intentional exploration via planning by leveraging a
connection between planning and temporally extended exploration.
•	we propose a generative planning method as a concrete instantiation and initial attempt in
this direction;
2
Published as a conference paper at ICLR 2022
•	we verify the effectiveness of the proposed method by comparing it with a number of
baseline methods;
•	we show that the proposed method has additionally improved interpretability on several
aspects, including both the evolvement of its exploration behavior and final policy behavior.
2	Background
We will focus on continuous control tasks in this work. We first review some basics on model-free
RL for continuous control, temporally extended exploration and model-based planning.
2.1	Model-free RL for Continuous Control
Standard RL methods like SAC (Haarnoja et al., 2018) learn a policy function which maps the current
state s to action a as π(s) → a. The policy is typically modelled with a neural network with learnable
parameters θ and the optimization of the policy parameters is achieved by maximizing the following
function w.r.t. θ:
∏θ = arg max Es〜DEa〜∏g[Q(s,a) - a log ∏θ(a|s)].
θ
(1)
where D denotes replay buffer and α is a temperature parameter. Q(s, a) denotes the soft state-action
value function and is defined as:
Q(S, a)	r(S, a) + YEsO〜T(s,a),a0〜∏θ(s0) Q(s0, a0) ,
where T(s, a) denotes the dynamics function and r(s, a) the policy entropy augmented reward
function (Haarnoja et al., 2018). γ ∈ (0, 1) is a discount factor. By definition, Q(S, a) represents the
accumulated discounted future entropy augmented reward starting from S, taking action a, and then
following policy πθ thereafter.
2.2	Temporally Extended Exploration Meets Model-based Planning
The policy of the form in Eqn.(1) generates a one-step action at a time and the exploration is typically
achieved by incorporating noise at single step level (Haarnoja et al., 2018). While this is simple
and flexible in representing the control policy, it might not be effective during exploration as the
consecutive one-step actions are not necessarily temporally consistent, limiting its ability to escape
local optima thus wasting many of the exploration efforts (Dabney et al., 2021).
Instead, a directed form of exploration can potentially mitigate this issue by generating consistent
actions for a certain period of time (Sutton et al., 1999; Mnih et al., 2015; Sharma et al., 2017; Dabney
et al., 2021). Build upon this philosophy, action repeat-based temporally extended exploration, despite
of its simplicity, has been proven to be very effective and has been actively explored recently (Sharma
et al., 2017; Dabney et al., 2021; Biedenkapp et al., 2021; Yu et al., 2021).
Apart from directly improving the temporal property of the exploration behavior, another set of
approaches guide policy optimization with model-based planning (e.g. Levine & Koltun, 2013a;b;
Lowrey et al., 2019; Hoppe et al., 2019). In model-based planning, the optimized action for a
particular time step is obtained by online minimization of a cost function. To better leverage the
temporal structure of the cost function, planning is typically done for a span of time period by
optimizing a sequence of actions T = [a。, aι, ∙∙∙ ] (e.g. Williams et al., 20l7; Srinivas et al., 20l8;
Chua et al., 2018; Hafner et al., 2019; Schrittwieser et al., 2019):
[ao,aι,…] = arg min	C(s, [a。，aι,…]).
T =[ao,aι,…]
(2)
This type of approaches has the potential to generate more intentional and flexible action sequences
than repeating actions, as they are optimized at the sequence level. The generated action sequences
is typically used with MPC by executing only the first action and discarding the rest. Also, the
prerequisite of an additional model for calculating C and the online optimization of it indicate a large
gap between this type of approaches and model-free RL. This makes the task of how to leverage
planning for exploration in model-free RL more challenging.
3
Published as a conference paper at ICLR 2022
3	Generative Planning Method for Model-free RL
Model-based planning can generate flexible plans, but requires online planning with a model which
is computationally intensive. On the other hand, action-repeat is more efficient, but is less adaptive in
its form of plans. This motivates us to present GPM, which supports flexible plans beyond repeating
for exploration without online optimization and resembles typical model-free RL algorithms.
We define a plan τ as a sequence of decisions that spans a number of time steps in the corresponding
space A. For example, if A is the space of primitive actions, τ is a sequence of primitive actions,
i.e. T = [ao,aι,…].If A is the spatial coordinate space as common in robotics control and driving,
then τ is a sequence of spatial positions and is essentially a spatial-temporal trajectory.
The framework of GPM is illustrated in Figure 1. At the core of GPM is a plan generator Πθ (s) 7→ τ,
which is a generative model that takes state s as input and outputs a plan τ (c.f. Section 3.1).
In essence, it is a form of policy that outputs multi-step actions. (therefore termed as generative
planning). The plan generator is trained by maximizing plan values, which shapes it to generate
plans that are more adaptive along the training process (c.f. Section 3.2). During rollout, at each time
step t, give the observation st, the plan generator Π generates a plan Tnew = [at, at+ι, at+2, ∙∙∙].
Also, the old plan (if available) from the previous step is “time-forwarded” one step to generate
Told = ρ(Ta0ctual), where Ta0ctual denotes the actual plan from the previous time step. The replanning
mechanism is based on the values of both plans Told and Tnew to determine which one is used as the
actual plan Tactual (c.f. Section 3.3).
(a)	T	(b)	Q(st,at)	Q(st,at,at+ι) Q(st,at,at+1,at+2)
Figure 2: Network Structures of (a) plan generator (generative actor) and (b) plan value function
(critic). (a) The plan generator generates a sequence of actions in an autoregressive way to form the full
plan. For each step start fromthe second one, a skip-connection is added fromthe input previous action
to the output action. (b) The plan value function takes state and plan as input, and outputs a sequence
of values along the plan. In this example, the plan value function takes St and T = [at, at+1,at+2, •一]
as input and outputs values [Q(st,at), Q(st,at, at+ι), Q(st, at, at+ι, at+2),…].Q(st,at) uses a
separate decoder different from the rest of steps t + 1,t + 2, •一，whose values are computed as the
sum of the value from the previous step and the output from the decoder.
3.1	Plan Generator Modeling
It is natural to model the plan generator Π with an autoregressive structure to capture the inherent
temporal relations between consecutive actions. In practice, we use a recurrent neural network (RNN)
to model Π, with its structure shown in Figure 2 (a). More concretely, the plan generator ∏θ(S) is
implemented as a stochastic recurrent network as follows:
Zt = encoder(st),	ht = h(zt)
at = μmean(zt) + σstd(zt) ∙ n	n 〜N(0,1)	(3)
at+i = f (ht+i, at+i—1), ht+i = RNN(ht+i—1, at+i-1)	i ∈ [1, n ― 1] ∙
z denotes the feature extracted from s. h(z) denotes a network for initializing the state for RNN. In
the special case when T only contains a single action, Eqn.(3) reduces to the standard stochastic actor
as used in Haarnoja et al. (2018), with μmean and σstd the network for predicting mean and standard
derivation of the action distribution for the current time step. f denotes the action decoder for future
time steps, which can be implemented with some prior incorporated when available. For example, we
can implement f with a skip-connection to leverage the temporal structure:
at+i = f(ht+i, at+i—1) = at+i—1 + g(ht+i, at+i—1) i ∈ [1, n - 1]	(4)
where g is implemented with neural networks. In this case, if we initialize g to generate small values,
then we actually incorporated an action repeat prior to start with for exploration and learning. The
4
Published as a conference paper at ICLR 2022
degrees of stochastic of Πθ (s) can be adjusted by matching a specified a target entropy in a way
similar to SAC (Haarnoja et al., 2018), contributing a term J(α) into the total cost (c.f. Appendix A.1).
3.2	Plan Value Learning and Plan Generator Training
We propose to optimize the plan generator by solving the following optimization problem:
∏θ — arg min J∏(θ) = arg min Es~DET~∏θ,i~u(i,l) [-Q(s, 〃)]	(5)
where τl denotes sub-plan of length l extracted from τ, starting from its beginning. U (1, L) denotes
a discrete uniform distribution and L denotes the maximum plan length. By taking expectation w.r.t.
l 〜U(1, L), We essentially train the plan generator by maximizing not only the value of the full plan,
but also the value of the sub-plans. The plan value Q(st, τl ) is defined as:
Q(St, Tl)= Q(St, at, at+1,…,at+l-1)= rt+1 + Yrt+2 +-------+ γlEs'~T(s,τι),τ0~∏θ(s0) [Q(S0, τO)] ∙
Here St denotes the current state, rt+ι = R(st, at), rt+2 = Est+ι~p(st,at)R(st+ι, at+ι), with R
the reWard function. P (St, at) and T (S, τ) denote the action-level and plan-level transition functions
respectively. This formulation has a number of potential benefits: i) it can generate flexible and
adaptive form of plans to be used for temporally extended exploration; ii) the generative approach
used for plan generation retains the resemblance to typical model-free RL algorithms while avoids
expensive online planning; iii) it gains additional interpretability because of its multi-step plan.
To train the plan generator, we use the re-parametrization trick as in SAC (Kingma & Welling, 2014;
Schulman et al., 2015; Haarnoja et al., 2018), and also leverage the differentiability of the plan
generator for gradient calculation: ∆θ =电⑶丁)~d,i~u(i,l)[缪 dQdT,Tl) ]. Q needs to be learned in
order to calculate ∆θ. In practice, we parametrize it with neural networks as Qφ, whose structure
shown in Figure 2(b). More concretely, the plan value function is implemented as a recurrent network,
taking the one-step action as input at each step, and outputting the plan value up to the current step.
To leverage the temporal structure, we compute the plan value as the sum of the plan value from the
previous step and the output from the decoder of the current step. Since there is no previous step
value for the first step of plan, the decoder of the first step is not shared with the rest of plan steps. To
train the plan value function Qφ, we use TD-learning as commonly used for value learning in RL
algorithms (e.g. Haarnoja et al., 2018; Fujimoto et al., 2018)
Qφ — argmi∏φ Jq(Φ) = argmi∏φ 旧⑶丁)~d,i~u(i,l) kQφ(s, τl) - T(s,τl, r)k2	(6)
where T(s,τl, r) = rt+ι +γrt+2 +---+ YlEs，~T(s,τl),to~∏θ(s') [Qφ0(s0, τ0)] denotes the target value
computed with a target critic network Qφ0, with φ0 denoting the parameter values periodically copied
from φ softly (Haarnoja et al., 2018). By taking expectation w.r.t. l 〜 U(1, L), we learn not only the
value of the full plan, but also the value of the sub-plans, which is used in replanning. In addition,
an entropy term (computed on the first step of the plan generator) is also added to the target value
following Haarnoja et al. (2018). This form of learning has the following potential benefits: i) it can
be used to train the plan generator as in Eqn.(5); ii) it can learn from trajectories sampled from a replay
buffer, which is not necessarily aligned with the plans used during rollout (c.f. Appendix A.6.1); iii) it
supports the evaluation of sub-plans, which is necessary for adaptive replanning, as detailed below.
3.3	Temporal Plan Update and Replanning
To encourage temporally coordinated exploration, we propose to commit to the current plan by
default and switch if a new plan is better according to certain criteria. More concretely, at the
current step, we have a plan candidate τold obtained from the plan τa0ctual of the previous step
as τold = ρ(τa0ctual). In the typical case where the plan is consisted of a sequence of actions,
The action to be executed at the current step is ω(τ) = τ [0], and ρ is simply a shifting operator
extracting the rest of the plan: ρ(τ) =τ [1 :], serving as a plan candidate for the current time step.
At the same time, we generate another new plan Tnew 〜Π(st). Now we can make a decision
on which of the two candidates we shall use. If the length of Told (denoted as Ioid = ∣Toid∣) is
zero, meaning the previous plan has been fully executed, then we just need to switch to Tnew .
Otherwise, we compare their plan values for determining a switch. Note that since Told is always
shorter than Tnew, the plan values need to be computed up to lold . Denoting the plan values under
this setting as Qlold (S, Told) and Qlold (S, Tnew), we then construct a categorical distribution for
generating replanning signals as repla∏ 〜Categorical(z), where Z denotes the logits and is defined
as z , [, Qlold (S, Tnew) - Qlold (S, Told)]. If replan=1, we switch to Tnew; otherwise, we commit to
5
Published as a conference paper at ICLR 2022
τold at the current time step. Intuitively, if the value of the new plan is much higher than that of the
old one, a plan switch is preferred. Otherwise, it prefers to commit to the old plan. To adjust , we
use an objective similar to the entropy-based temperature adjustment in SAC (Haarnoja et al., 2018):
(7)
J()
• (Icommit - lcommit_target),

where ≥ 0, lcommit_target is a target commit length and lcommit is the actual commitment length
computed as the exponential moving average of commitment length during rollout. By doing this, we
can better leverage the temporally coordinated plans from the plan generator for exploration, while
allowing to switch plans if necessary. The overall algorithm is summarized in Appendix A.1.
4	Related Work
Exploration with Temporally Correlated Action Sequences. Our work mainly lies in this category.
Methods in this category focus on improving the correlations of consecutive actions to improve over
simple but widely used exploration methods like -greedy (Dabney et al., 2021). Some methods focus
on making the stochastic process underlying the exploration policy more temporally correlated (e.g.
Lillicrap et al., 2016; Korenkevych et al., 2019). Action repeat strategy leads to a family of successful
methods in this category by fully discarding action space randomness for the repeating steps after
the first one (e.g. Mnih et al., 2015; Sharma et al., 2017; Neunert et al., 2019; Dabney et al., 2021;
Biedenkapp et al., 2021; Yu et al., 2021). The proposed method is connected with action repeat in the
sense that the source of stochasticity is also introduced in the first step of the action sequence, but it
supports more flexible form of action sequences beyond repeating, and achieves this through plan
generator training, rather than relying on certain type of specifically designed stochastic process as
in Lillicrap et al. (2016); Korenkevych et al. (2019). This category of methods mostly retain the flat
RL structure, in contrast to the more complicated hierarchical RL structures as reviewed below.
Hierarchical RL. There is a large body of work on improving exploration and learning efficiency by
operating at skill level with a higher level meta-controller (e.g. Dayan & Hinton, 1993; Sutton et al.,
1999; Randlov, 1999; Vezhnevets et al., 2016; Bacon et al., 2017; Florensa et al., 2017; Co-Reyes
et al., 2018; Konidaris et al., 2010; Pertsch et al., 2020). However, most hierarchical RL methods
are nontrivial to adapt because of some task-specific aspects, e.g. the proper design of skill space,
its form of representation and its way of acquisition (Dayan & Hinton, 1993; Sutton et al., 1999;
Florensa et al., 2017; Co-Reyes et al., 2018; Konidaris et al., 2010; Pertsch et al., 2020). In contrast,
the proposed method is a light-weight design that resembles standard model-free RL algorithms to a
great extend, thus lies in a space that is mostly complementary to hierarchical RL methods.
Goal-conditional RL. Goal conditional RL methods learn a goal-conditional policy and address the
efficiency of policy learning by using techniques such hindsight experience replay. There are some
recent efforts on generating goals beyond simply relabeling the experienced states (e.g. Ren et al.,
2019; Pong et al., 2020; Pitis et al., 2020). In this setting, it resembles the hierarchical RL structure,
where the goal generator plays the role of a high-level meta-controller and the goal-conditional policy
functions as a low-level worker. Differently, we focus on how to improve action sequence based
exploration for plain model-free RL algorithms without resorting to goal-oriented designs. Therefore,
we view the proposed approach as orthogonal to goal-conditional RL and they could potentially be
integrated together to bring further improvements.
Model-based RL and Plan-to-Explore. Model-based RL is another large family of methods that
has been explored from many perspectives (e.g. Sutton, 1991; Silver et al., 2016; Srinivas et al.,
2018; Chua et al., 2018; Agarwal et al., 2018; Lowrey et al., 2019; Hoppe et al., 2019). i) Learning
with latent dynamics. While some works may require a given model (Silver et al., 2016; Lowrey
et al., 2019), many recent methods work with a latent dynamics model (e.g. Oh et al., 2017; Silver
et al., 2017; Schrittwieser et al., 2019; Hafner et al., 2020). Our work is related to Oh et al. (2017);
Silver et al. (2017); Schrittwieser et al. (2019) in the sense that our plan value function is trained
by predicting future values, and related to Hafner et al. (2020) in the way of policy training by
maximizing values. However, these approaches do not leverage their implicit models for temporally
extended exploration, which is the focus of our work, thus are orthogonal to our work. Also, our
value function has a dedicated latent dynamics model that is not shared with the plan generator,
which offers more flexibilities in structural design (e.g. the usage of a structural prior Eqn.(4) in plan
generator) and learning (each is responsible for its own objective). Therefore, the overall algorithmic
structure resemble standard model-free RL methods, making the transition between them more
natural. ii) Integrating model into model-free RL. There are many possible ways to integrate model
into model-free RL methods (e.g. Sutton, 1991; Levine & Koltun, 2013a;b; Racaniere et al., 2017;
6
Published as a conference paper at ICLR 2022
EmE a6BJΘΛ4
Emea ΦOTW⅛><
Em电 ese-eA4
Pendulum
1.0
ie4
0.2	0.4	0.6	0∙8
Environment Steps
CartPoIeSwingUP
Emea ⅛rt⅛><
FetchPush
0.2	0.4	0.6	0.8	1.0
Environment Steps lβ7
MountainCarContinuouS
LunarLanderContinuouS
Environment Steps
Em电 &s££
Environment Steps le5
Emea eβBJΘΛ4
FetchPickAndPIace
----SAC
----ICM
EZ
FAR
—DAR
TAAC
Dreamer
GPM

Figure 3:	Return Curves on benchmark continuous control tasks for different methods.
Srinivas et al., 2018). Imagination-augmented agent (Racaniere et al., 2017) uses the predictions from
a model as additional contexts for a model-free policy. Srinivas et al. (2018) embeds differentiable
planning within a goal-directed policy and optimizes the action plan through gradient descent. Our
work is different in that it uses a plan generator to directly generate action sequences conditioned on
the current state, much like a model-free policy, without using additional context from a model-based
prediction (Racaniere et al., 2017) or requiring expensive gradient based plan generation (Srinivas
et al., 2018). iii) Plan-to-explore. The general idea of plan-to-explore is broadly applicable and has
appeared in many categories including hierarchical (e.g. Konidaris et al., 2010; Pertsch et al., 2020),
goal-conditional (e.g. Ren et al., 2019; Pong et al., 2020; Pitis et al., 2020) as well as model-based
RL (e.g. Pathak et al., 2017; Henaff, 2019; Shyam et al., 2019; Sekar et al., 2020). In this work, we
focus on improving standard model-free RL methods by generating coordinated action sequence for
temporally extended exploration, which has a different focus and complementary to these approaches.
5 Experiments
Baselines Details. We implement and compare the performance of the proposed method with seven
baseline methods (Xu et al.).: i) SAC: the Soft Actor Critic (Haarnoja et al., 2018) algorithm which
is a representative model-free RL algorithm; ii) EZ: temporally-extended -Greedy exploration
(z-Greedy) (Dabney et al., 2021), which repeat the sampled action for a random duration during
exploration; iii) FAR: Fixed Action Repeat (Mnih et al., 2015), which repeats the policy action
for a fixed number of steps; iv) DAR: Dynamic Action Repeat with TempoRL (Biedenkapp et al.,
2021), which improves over FAR with a action-conditional policy for predicting a duration to repeat,
outperforming non-conditional alternatives (Sharma et al., 2017); v) TAAC: Temporally Abstract
Actor Critic (Yu et al., 2021), which uses a switching policy for determining whether to repeat the
previous action, instead of generating a duration to repeat beforehand as in DAR (Biedenkapp et al.,
2021); vi) ICM: Intrinsic Curiosity Module (Pathak et al., 2017), which is a model-based method for
reward bonus based exploration. vii) Dreamer: a model-based RL method which trains the policy
by back-propagation through the learned latent dynamics (Hafner et al., 2020). SAC is used as the
backbone algorithm for ii)〜Vi). More results and resources are available on the project page.1
1 https://sites.google.com/site/hczhang1/projects/generative- planning
7
Published as a conference paper at ICLR 2022
50-99 episodes	100-149 episodes	150-199 episodes
E 400
0少
0.0
(b)
Φ 300
f 200
600
500
100
CARLA
SAC
EZ
FAR
DAR
TAAC
GPM
0.2	0.4	0.6	0.8	1.0
Environment Steps 1θ7
Figure 4:	Exploration Trajectory Visualization and Return Curves. (a) Each figure shows collec-
tively the 50 rollout trajectories during training for the range depicted above. Trajectories are plotted
relative its start position for easier visualization (unit:meter). The chronological order of positions
in the trajectory is encoded in the same way as Figure 5. Visually, an effective agent should push
the position of the red points as far as possible along its route. All plots including thumbnails are
rendered at the same scale relative to their axes. (b) Return curves of different methods.
5.1	Experiments on Continuous Control
Benchmark Results. We conduct experiments on 12 benchmark tasks and the results are shown
in Figure 3 (image-based CARLA results are deferred to Figure 4). It is observed that vanilla
SAC can achieve reasonable performance on most of the tasks, albeit with a large performance
variance and high sample complexity. FAR shows better performance than SAC on some tasks (e.g.
BipedalWalker and FetchPush), but no clear improvements (e.g. InvertedPendulum) or even
worse performance on some other tasks (e.g. CartpoleSwingUp). This is potentially because of
the relatively more restrictive policy with a fixed repetition scheme. EZ outperforms SAC on some
tasks (e.g. MountainCarContinuous and FetchPickAndPlace) and retain mostly comparable
performance to SAC on others, demonstrating the contribution of incorporating temporal consistency
to the exploration policy. ICM and Dreamer improves over SAC on some tasks but not always. Both
DAR and TAAC show consistent improvements over SAC due to their flexibility on leveraging action
repeat to the extend where it is useful. The proposed GPM demonstrates further improvements in
terms of sample efficiency on some of the tasks, with final performance matching or outperforming
those of the baseline methods. Apart from state-based control, GPM also delivers strong performance
in a more challenging image-based control task (c.f. Figure 4), as detailed in section 5.2.
State Trajectory Visualization. To understand the behavior of different algorithms, we further
visualize the state visitation distribution and its evolution during training (Figure 12). it shows
that during the initial phase of training, the state coverage of the methods with extended temporal
exploration is much higher than that of SAC, which are consistent with the results reported in (Dabney
et al., 2021). Furthermore, as training progresses, GPM is able to find the high rewarding state more
quickly, visually depicting its improved exploration performance and sample efficiency.
5.2	End-to-End Autonomous Driving with Emerged Interpretable Plans
Autonomous Driving (AD) is an important application for learning-based algorithms. Although
the end-to-end approaches have showed encouraging results (e.g. Bojarski et al., 2016; Toromanoff
et al., 2020), one great limitation is their limited interpretability. Although GPM is also an end-to-end
approach, it has improved interpretability, in addition to its better performance. it is important to
note that this ability of generating interpretable plans is emerged from pure reinforcement learning,
without direct supervisions. This is in clear contrast to the methods from the decomposed paradigm,
which requires additional labels as direct supervision for training (Phan-Minh et al., 2020; Rhinehart
et al., 2020). We use CARLA (Dosovitskiy et al., 2017) to setup a challenging image-based driving
task. We set the plan length as L = 20 to enable more perceivable results when visualized. More
details including observations, actions, rewards and network structure are presented in Appendix A.5.
Behavior and Performance Comparison. The exploration behavior comparison of GPM with SAC
and EZ are shown in Figure 4 (a). it can be observed that the effective range of exploration of a
SAC agent increases slowly and is quite limited, demonstrating its inefficiency in exploration. EZ
shows enhanced exploration ability over SAC, as depicted by the enlarged range of explored areas.
GPM shows a further improved exploration ability. More specifically, GPM shows some directional
movement ability during the initial phase of training. it further shows reasonable turning behavior
after about 100 training episodes and more complex behavior with multiple turns after about 150
training episodes. in terms of performance, it can be observerd from Figure 4 (b) that GPM learns
faster than other baseline methods, with final performance better than or competitive to other methods.
8
Published as a conference paper at ICLR 2022
Figure 5: Evolution of GPM Plans: (a) initially the plans are nearly uniformly distributed across
all the directions; (b) forward-moving plans appear to take a larger portion afterwards; (c) further
training shows the emergence of plans with more adjustments on speed, indicated by shorter length
plans; (d) later, more curved turning plans and plans with increased length appear; (e) at the end of
training, the agent can drive smoothly, therefore forward moving and turning plans take the majority.
Figure 6: Plan Visualization within an Episode. Note that the agent is trained with a frontal camera
capturing 64×64 RGB images. The top-down view at a higher resolution is only used for visualization.
The plan is color-coded as in Figure 5. Frames switching to new plans are enclosed with a black box.
At t1 , a roughly straight plan is generated, and it is committed to for multiple time steps till t2 . At the
next time step t3 , the agent switches to a new plan, passing a car in the front. This plan is committed
to by the agent (t3 → t4) and at t5 , another new plan curved towards the turning direction is adopted.
t6 and t7 show two frames while committing to the t5-plan for a right turn. After that, the old plan is
replaced by a new plan at t8, executed till t9 and is replaced by another one at t10 .
Emerged and Evolved Planning. The evolution of the plans from GPM is visually shown in Figure 5.
It shows the emergence of meaningful plans and the evolution of them along the training process.
This naturally depicts several advantages of GPM over standard end-to-end AD systems: i) it has more
interpretable plans, which can serve as a media for communicating the intent of the algorithm; ii) the
evolution of the algorithm behavior during training is more perceivable; iii) the plans, with grounded
meaning of spatial-temporal trajectories, offer the opportunity of further adjustment according
downstream requirements in the AD pipeline, which could potentially narrow down the gap between
end-to-end and decomposed approaches and provide an opportunity to integrate. The plans from
GPM together with the visual context images are provided in Figure 6. It can be observed that GPM
plans are interpretable and meaningful. Note that this a type of emerged planning ability as there is
no direct supervision on it but purely acquired through interactions with the environment.
6 Conclusion and Future Work
We explore a perspective on using planning for inducing temporally extended, flexible and intentional
action sequence for exploration in a generative fashion. We present a Generative Planning method as
an attempt towards this goal. The proposed approach features an inherent mechanism for generating
temporally coordinated plans for exploration, while retaining the major features and advantages of
typical model-free RL algorithms. We hope this work can serve as an initial encouraging step that can
inspire more developments in this direction. For future work, the quality of the plans can potentially
be further improved by learning a more refined cost with the help of additional data (Ziebart et al.,
2009; Wulfmeier et al., 2016; Choi et al., 2019; Rosbach et al., 2019). While we have mainly
focused on continuous control tasks in this work, it is possible to generalize GPM to handle discrete
actions (Tennenholtz & Mannor, 2019). The network architecture also provides another axis for
potential improvements. We leave the exploration of these interesting directions as our future work.
9
Published as a conference paper at ICLR 2022
Acknowledgements. We would like to thank Jingchu Liu, Lisen Mu, Jerry Bai, Le Zhao, Break
Yang, Lingfeng Sun and anonymous reviewers for discussions and feedback on the project and paper.
We would also like to thank the Horizon AI platform team for infrastructure support.
References
Arpit Agarwal, Katharina Muelling, and Katerina Fragkiadaki. Model learning for look-ahead
exploration in continuous control. CoRR, abs/1811.08086, 2018.
Susan Amin, Maziar Gomrokchi, Hossein Aboutalebi, Harsh Satija, and Doina Precup. Locally
persistent exploration in continuous control tasks with sparse rewards. In Marina Meila and Tong
Zhang (eds.), International Conference on Machine Learning, 2021.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
the Thirty-First AAAI Conference on Artificial Intelligence, 2017.
Andre Biedenkapp, RaghU Rajan, Frank Hutter, and Marius Lindauer. TempoRL: Learning when to
act. In International Conference on Machine Learning, 2021.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, 2016.
Dooseop Choi, Kyoungwook Min, and Jeongdan Choi. Regularizing neural networks for future
trajectory prediction via inverse reinforcement learning. CoRR, 2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems, 2018.
John D. Co-Reyes, Yuxuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, and Sergey
Levine. Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory
embeddings. In International Conference on Machine Learning, 2018.
Will Dabney, Georg Ostrovski, and Andre Barreto. Temporally-extended ε-greedy exploration. In
International Conference on Learning Representations, 2021.
Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in Neural Informa-
tion Processing Systems, 1993.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:
An open urban driving simulator. In Conference on Robot Learning, 2017.
Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging
planning and reinforcement learning. In Advances in Neural Information Processing Systems,
2019.
Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic neural networks for hierarchical reinforce-
ment learning. In International Conference on Machine Learning, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband,
Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell,
and Shane Legg. Noisy networks for exploration. In International Conference on Learning
Representations, 2018.
C. Daniel Freeman, David Ha, and Luke Metz. Learning to predict without looking ahead: World
models without forward prediction. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,
Florence d’Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information
Processing Systems, 2019.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In International Conference on Machine Learning, 2018.
10
Published as a conference paper at ICLR 2022
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In IEEE International Conference on
Robotics and Automation, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
on Machine Learning, 2018.
Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, 2019.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2020.
Mikael Henaff. Explicit explore-exploit algorithms in continuous state spaces. In NeurIPS, 2019.
Sabrina Hoppe, Zhongyu Lou, Daniel Hennes, and Marc Toussaint. Planning approximate exploration
trajectories for model-free reinforcement learning in contact-rich manipulation. IEEE Robotics
Autom. Lett., 4(4):4042-4047, 2019.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
on Learning Representations, 2014.
George Konidaris, Scott Kuindersma, Andrew Barto, and Roderic Grupen. Constructing skill trees
for reinforcement learning agents from demonstration trajectories. In International Conference on
Neural Information Processing Systems, 2010.
Dmytro Korenkevych, A. Rupam Mahmood, Gautham Vasan, and James Bergstra. Autoregressive
policies for continuous control deep reinforcement learning. In International Joint Conference on
Artificial Intelligence, 2019.
K.J. Kyriakopoulos and G.N. Saridis. Minimum jerk path generation. In IEEE International
Conference on Robotics and Automation, 1988.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine
Learning, 2013a.
Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Advances
in Neural Information Processing Systems, 2013b.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
International Conference on Learning Representations, 2016.
Kendall Lowrey, Aravind Rajeswaran, Sham Kakade, Emanuel Todorov, and Igor Mordatch. Plan
online, learn offline: Efficient learning and exploration via model-based control. In International
Conference on Learning Representations, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, February 2015.
Michael Neunert, Abbas Abdolmaleki, Markus Wulfmeier, Thomas Lampe, Jost Tobias Springenberg,
Roland Hafner, Francesco Romano, Jonas Buchli, Nicolas Heess, and Martin A. Riedmiller.
Continuous-discrete reinforcement learning for hybrid control in robotics. In Conference on Robot
Learning, 2019.
Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural
Information Processing Systems, 2017.
11
Published as a conference paper at ICLR 2022
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by
self-supervised prediction. In International Conference on Machine Learning, 2017.
Karl Pertsch, Youngwoon Lee, and Joseph J. Lim. Accelerating reinforcement learning with learned
skill priors. In Conference on Robot Learning, 2020.
Tung Phan-Minh, Elena Corina Grigore, Freddy A. Boulton, Oscar Beijbom, and Eric M. Wolff.
Covernet: Multimodal behavior prediction using trajectory sets. In IEEE Conference on Computer
Vision and Pattern Recognition, 2020.
Silviu Pitis, Harris Chan, Stephen Zhao, Bradly C. Stadie, and Jimmy Ba. Maximum entropy gain
exploration for long horizon multi-goal reinforcement learning. In International Conference on
Machine Learning, 2020.
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Joshua Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech
Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for
research. ArXiv, abs/1802.09464, 2018a.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
In International Conference on Learning Representations, 2018b.
Vitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-fit:
State-covering self-supervised reinforcement learning. In International Conference on Machine
Learning, 2020.
Sebastien Racani2re, Theophane Weber, David P Reichert, Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Raz-
van Pascanu, Peter W. Battaglia, Demis Hassabis, David Silver, and Daan Wierstra. Imagination-
augmented agents for deep reinforcement learning. In Advances in Neural Information Processing
Systems, 2017.
Jette Randlov. Learning macro-actions in reinforcement learning. In Advances in Neural Information
Processing Systems, 1999.
Zhizhou Ren, Kefan Dong, Yuan Zhou, Qiang Liu, and Jian Peng. Exploration via hindsight goal
generation. In Advances in Neural Information Processing Systems, 2019.
Nicholas Rhinehart, Rowan McAllister, and Sergey Levine. Deep imitative models for flexible
inference, planning, and control. In International Conference on Learning Representations, 2020.
Sascha Rosbach, Vinit James, Simon GroBjohann, Silviu Homoceanu, and Stefan Roth. Driving with
style: Inverse reinforcement learning in general-purpose planning for automated driving. CoRR,
abs/1905.00229, 2019.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap,
and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR,
abs/1911.08265, 2019.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, 2015.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. In International Conference on Machine
Learning, 2020.
Sahil Sharma, Aravind S. Lakshminarayanan, and Balaraman Ravindran. Learning to repeat: Fine
grained action repetition for deep reinforcement learning. In International Conference on Learning
Representations, 2017.
Pranav Shyam, Wojciech ja´kowski, and Faustino Gomez. Model-based active exploration. In
International Conference on Machine Learning, 2019.
12
Published as a conference paper at ICLR 2022
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with
deep neural networks and tree search. Nature, pp. 484-489, January 2016.
David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel
Dulac-Arnold, David P Reichert, Neil C. Rabinowitz, Andre Barreto, and Thomas Degris. The
predictron: End-to-end learning and planning. In International Conference on Machine Learning,
2017.
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal plan-
ning networks: Learning generalizable representations for visuomotor control. In International
Conference on Machine Learning, 2018.
Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART
Bull., 2(4):160-163, 1991.
R.S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial Intelligence, 112:181-211, 1999.
Guy Tennenholtz and Shie Mannor. The natural language of actions. In International Conference on
Machine Learning, 2019.
Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu,
Jonathan Schwarz, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh,
and Nicolas Heess. Behavior priors for efficient reinforcement learning. CoRR, abs/2010.14274,
2020.
Marin Toromanoff, E. Wirbel, and F. Moutarde. End-to-end model-free reinforcement learning for
urban driving using implicit affordances. IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2020.
Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John Agapiou,
and koray kavukcuoglu. Strategic attentive writer for learning macro-actions. In Advances in
Neural Information Processing Systems, 2016.
Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. In International
Conference on Learning Representations, 2020.
Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M. Rehg, Byron Boots, and
Evangelos A. Theodorou. Information theoretic MPC for model-based reinforcement learning. In
IEEE International Conference on Robotics and Automation, 2017.
Markus Wulfmeier, Dominic Zeng Wang, and Ingmar Posner. Watch this: Scalable cost-function
learning for path planning in urban environments. In International Conference on Intelligent
Robots and Systems, 2016.
Sirui Xie, Junning Huang, Lanxin Lei, Chunxiao Liu, Zheng Ma, Wei Zhang, and Liang Lin.
NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning.
In International Conference on Learning Representations, 2019.
Wei Xu, Haonan Yu, Haichao Zhang, Yingxiang Hong, Break Yang, Le Zhao, Jerry Bai, and
ALF contributors. ALF: Agent Learning Framework. URL https://github.com/
HorizonRobotics/alf.
Haonan Yu, Wei Xu, and Haichao Zhang. TAAC: Temporally abstract actor-critic for continuous
control. In Advances in Neural Information Processing Systems, 2021.
Yijie Zhang and Herke Van Hoof. Deep coherent exploration for continuous control. In International
Conference on Machine Learning, 2021.
Brian D. Ziebart, Nathan Ratliff, Garratt Gallagher, Christoph Mertz, Kevin Peterson, J. Andrew
Bagnell, Martial Hebert, Anind K. Dey, and Siddhartha Srinivasa. Planning-based prediction for
pedestrians. In International Conference on Intelligent Robots and Systems, 2009.
13
Published as a conference paper at ICLR 2022
A Appendix
A.1 GPM Algorithm
The overall procedure of the proposed method is presented in Algorithm 1.
Algorithm 1 Generative Planning Method (GPM)
Initialize: θ, φ
φ0 ― φ, D — 0
for each iteration do
for each environment step do
Told = 0 if ∣Told∣ is 0 else Told — P(Tt-I)	. time-forward the old plan
Tnew 〜 ∏(st)	. generate a new plan
m = 1 if lold = ∣Told∣ is 0 else m〜Categorical([g Qi。.(s, 丁…))—Qlold(s,τ0ld)])
Tt = (1 — m) ∙ Told + m ∙ Tnew	. select the actual plan from old and new plans
at — ω(Tt)	. extract the primitive action for the current step
st+ι 〜p(st+ι∣st,at)
D — D ∪ {(st, at, r(st, at), st+1 )}	. save transition to replay buffer
end for
for each train step do
Φ — Φ — λVφ Jq(φ)
θ ― θ — λVθ J∏(θ)
α — α — λVα J (α)
— — λV J ()
φ0 — ηφ + (1 — η)φ0
end for
end for
plan value update
plan generator update
temperature update
replan threshold update
target parameter soft-copy
J(α) = Eat=ω(τ),τ〜π [—α log ∏o(at ∣st,α) — αH], where ∏o denotes the policy corresponding to
the first step of the plan, which is similar to the one used in Haarnoja et al. (2018). H is the target
entropy set in the same way as SAC (Haarnoja et al., 2018) for controlling the degree of stochasticity
of the policy by adjusting the temperature α.
A.2 DETAILS ON ρ AND ω OPERATORS
Here we summarize in Table 1 the meaning and implementations of some entities as well as operators
that are used in this work to further help understanding.
Entity/Operator	Meaning/Implementation	
	Gym Task	CARLA Task
T = M …，θj,…]	a denotes a primitive action	a denotes a setpoint in ego-coordinate
	P(T)	P(T )= TShift(T ) = t[1:]	ρ(τ) = TworId2ego。丁shift。丁ego2world(τ)
	ω(τ)		ω(τ) = t[0]		
Table 1: Details on T, ρ and ω.
In Table 1, T ego2world is an operator for transforming the plan from ego to world coordinate and
T world2ego the reverse operator.
A.3 Network Structure and Details
Network details for different tasks are summarized in Table 2, where we use [K] × L to denote an
MLP with L FC layers and each with the output size of K . For baseline methods, they use the
network structure as shown in Table 2 for both actor and critic networks. For GPM, we implement
the plan generator and plan value network as in Figure 2. The plan generator first use an MLP that
has the same structure as baseline methods as shown in Table 2 for encoding the observation s to a
feature vector z. z is used as the input of the Normal Layer, which uses FCs for generating mean
and std vectors of a diagonal Gaussian distribution (Haarnoja et al., 2018). z is passed through a FC
layer to the initial state of a GRU network (plan RNN), with its hidden size the same as the single
14
Published as a conference paper at ICLR 2022
layer size in Table 2. Action is detached before inputting to the GRU to to enforce the training of
GRU and improve training stability. A shared FC layer is used for generating the actions after the
first step, taking the output from plan RNN as input. The plan value network is implemented with
LSTM (value RNN) with hidden size the same as the single layer size in Table 2. An MLP decoder
with the same structure as those used in other baseline methods (Table 2) are used for plan value
estimation, taking the output from value RNN as decoder input. The first step uses one such decoder
with parameters not shared with others. The rest of the steps uses a shared one. The reason for this
is that the conditioning state for the first step is different from the rest. ReLU activation is used for
hidden layers. Following standard practice, the minimum over the outputs of two value networks is
used to mitigate value overestimation (Fujimoto et al., 2018; Haarnoja et al., 2018).
Task	Network (actor/Critic)	learning rate	batch size	plan length	# parallel actors
Pendulum	[100] × 2	5e-4	64	3	1
InvertedPendulum	[256] × 2	ie-4	256	3	1
InvertedDoublePendulum	[256] × 2	ie-4	256	3	1
CartPOleSwingUP	[256] × 2	ie-4	256	3	1
LUnarLanderCOntinUOUs	[256] × 2	ie-4	256	3	1
MOUntainCarCOntinUOUs	[256] × 2	ie-4	256	10	1
BiPedalWalker	[256] × 2	5e-4	4096	3	32
Fetch	[256] × 3	1e-3	5000	3	38
CARLA	observation_encoder → [256]	1e-4	128	20	4
Table 2: Details on network structure and hyper-parameters. More details on the observation
encoder for CARLA is provided in Appendix A.5.2 and Figure 7.
A.4 Gym Task Details
The Gym tasks used in this paper and their corresponding Gym environment names are summarized
in Table 3. Fetch tasks are from Plappert et al. (2018a).
Task	Gym Environment Name
Pendulum	Pendulum-v0
InvertedPendulum	InVertedPendUIum-V2
InvertedDoublePendulum	InVertedDoublePendulum-v2
MountainCarContinuous	MOUntainCarCOntinUOUS-V0
CartPoleSwingUp	CartPoleSwingUp-v0 2
LunarLanderContinuous	LUnarLanderCOntinUOUS-V2
BipedalWalker	BiPedalWalker-v2
FetchReach	FetchReach-v1
FetchPush	FetchPush-v1
FetchSlide	FetchSlide-v1
FetchPickAndPlace	FetChPiCkAndPIaCe-VI
Table 3: Tasks and gym environment names.
A.5 CARLA Task Details
The CARLA driving task is implemented based on the open source Car Learning to Act (CARLA)
simulator (Dosovitskiy et al., 2017). More details on sensor/observation/action/reward, network
structure etc. are presented in the sequel.
A.5.1 Sensors and Observations
The following types of sensors are used in CARLA driving task (c.f. Figure 7):
•	Camera: frontal camera capturing RGB image of size 64×64×3;
•	IMU sensor: 7D vector containing acceleration (3D), gyroscope (3D), compass (1D),
providing some measurements on the status of the current vehicle; 1
1https://github.com/angelolovatto/gym-cartpole-swingup
15
Published as a conference paper at ICLR 2022
•	Velocity: 3D vector containing velocity relative to ego-coordinate in m/s;
•	Navigation: the collection of the positions of 8 waypoints d meters away from the ego
vehicle along the route, with d ∈ {1, 3, 10, 30, 100, 300, 1000, 3000} meters. The positions
of the waypoints are represented in 3D ego-coordinate. The sensory reading is a 24D vector;
•	Goal: 3D position in ego-coordinate representing the target position the ego car should
navigate to and stop at;
•	Previous action: 2D vector representing the previous action. More details on the action
space are provided in Appendix A.5.3.
The delta time between two time steps is ∆t = 0.05s, i.e., the perception and control rate is 20 Hz.
Actor Network
Critic Network
Figure 7: Network structures for CARLA task.
policy
value
plan/aCtion
A.5.2 CARLA Network Structure
The network structure used in CARLA task is shown in Figure 7. This is the common structured
shared by all the methods including GPM. Given the observation containing multi-modality signals, an
observation encoder is first used to fuse them into a low-dimensional feature vector. This is achieved
by processing each modality of signals with a dedicated encoder with the same dimensionality for the
outputs. More specifically, the visual encoder is a ResNet-18 based CNN. Each of other modalities
are encoded by a fully connected layer. The outputs from each of the encoder are fused together with
a sum operator followed by a FC layer to produce the feature vector as input to downstream networks.
The observation encoder is shared by both the actor and critic network. During training, the the
gradient from the actor is stopped and the observation encoder is only trained by value learning.
A.5.3 CARLA Controller and Action S pace
The CARLA environment with its controller interface is illustrated in Figure 8. It takes as input a
setpoint in ego-coordinate, specifying an immediate position to go to for the current step. For the
driving case, the setpoint can be represented in the 2D spatial ego-coordinate. Therefore there are two
dimensions (x: lateral, y: longitudinal) in the action space, with a = (x, y) ∈ [-20, 20] × [-20, 20].
The actor controls the movement of the car by generating 2D action as the setpoint. Target direction
and speed will be computed based on the setpoint and ego-position (origin), which will be passed to a
PID-controller to produce control signals including throttle, steer, brake and reverse. These signals
will be used by the simulator to generate the next state of the car. As part of the environment, a
shrinkage is applied to the setpoint before computing the control signals:
a = a ∙
max(kak - δ, 0)
kak
(8)
This is used to balance the portion of the braking behavior w.r.t. others in the control space. δ = 3 is
used in this work.
A.5.4 Reward
The reward in CARLA environment is consisted of the following components:
16
Published as a conference paper at ICLR 2022
CARLA Environment
Figure 8: CARLA environment and action space.
•	Route distance reward: this reward is computed as the distance traveled along route from
the previous to the current time step.
•	Collision reward: this is computed as min(collision_penalty, 0.5 * max(0, episode_reward)),
where ColliSion_penalty=20, episode_reward denotes the accumulated rewards up to now
within the current episode. Issued only once for a contiguous collision event.
•	Red-light reward: this is computed as min(red_light_penalty, 0.3 * max(0., episode_reward)),
where red_light_penalty=20. Issued only once for a contiguous red light violation event.
•	Success reward: a success reward of 100 will be received by the agent (ego-car) if its
position is within 5 meters to the goal position and its speed is less than 10-3 m/s.
A.5.5 Episode Termination Conditions
The episode terminates if either of the following conditions are satisfied:
•	success: the car’s position is within 5 meters to the goal position and its speed is less than
10-3 m/s.
•	the maximum number of allowed time steps K is reached: this number is computed as
K = route-length , where the min velocity = 5 m/s and ∆t = 0.05s. The state is
min_velocity X ∆t
marked as a non-terminal state, allowing bootstrapping for value learning at this state;
• stuck at collision: if the car is involved in the same collision event and does not move more
than 1 meter after 100 time steps. The state is marked as a non-terminal state, allowing
bootstrapping for value learning at this state.
A.5.6 Map and Example Routes
For CARLA driving task, we use Town01, which is a town with representative layout consisting a
number of T junctions and intersections. The layout of Town01 is shown in Figure 9 (a). The size
of the town is about 410m X 344m. We also spawn 20 other vehicles controlled by autopilot and 20
pedestrians to simulate typical driving conditions involving not only static objects but also dynamic
ones.
(a)	(b)
Figure 9:	Map of and Example Routes. (a) Map of Town01 (Dosovitskiy et al., 2017). (b) Map
with some example routes overlaid. Different routes are rendered with different colors. The beginning
of each route is denoted with a 4 symbol and the end of the route is denoted with a O symbol.
17
Published as a conference paper at ICLR 2022
The goal location is randomly selected from the set of waypoints. The agent is spawned to a random
valid waypoint location at the beginning of each episode. A waypoint is valid if it is on the road and is
at least 10 meters away from any other actor in the world (vehicles and pedestrians). Some examples
routes are shown in Figure 9(b). As can be observed from the figure, the routes constructed in this
way have a reasonable amount of diversity and cover representative driving scenarios. For example,
different routes could include different number of turns, directions for turning, static objects (as the
routes are different) and also different dynamic objects (e.g. other vehicles) at different space-time
locations. The task of the agent is to navigate to the goal position following the route and stay still
after arrived at the goal location.
A.6 More Implementation Details
A.6.1 Plan Saving, Sampling and Generation
In practice, for plan value learning, for each training iteration, we sample a batch of (state, plan,
rewards, next-plan-state) tuple {sit, τti, rit, sin}iN=1 from the replay buffer. More concretely, for each
state sit randomly sampled from the replay buffer, we sample an associated plan τti as the actions
starting from the current step of sit to a variable number of future steps uniformly sampled in the
range of 1 and maximum plan length L, i.e. Tt = [at,…,ai+?-/ where l 〜Uniform(1, L). The
reward vector is consisted of the rewards along the plan rtt =屏+匕 …，片+J The plan terminal
state sin is the state at the time step after the last step of τti, i.e. sin = sit+l . Therefore, in effect,
we use sampled plan for plan value learning. By constructing plans in this way, we have maximal
flexibility of leveraging the action sequences from replay buffer with minimal assumption on the plan
distribution. During training, lcommit_target is set to 0.5L.
Note that in the case of driving (and other similar scenarios), since the plan is a spatial trajectory
in the ego-coordinate, and the plan update operator in this case is ρ(τ) = T world2ego ◦ T shift ◦
Tego2world(τ) = TWorld2ego◦Tshift(τ0) (c.f. Table 1), To enable the construction ofplans from action
sequences sampled from the replay buffer (not necessarily aligned with the plan boundary during
rollout), we need to save the a0 = ω(τ0) into replay buffer, i.e. D — D ∪ {(st, at r(st, at), st+ι)}.
a0 = ω(τ0) actually denotes the setpoint in world coordinate, In this case, since the actions saved in
replay buffer are in world coordinate, we can construct plans in world coordinate by by sampling
sequences from the replay buffer, in the same way as described above, i.e. Tt = [at, ∙∙∙ , at+?-/
where l ~ Uniform(1, L). Then the plans in ego-coordinate can be obtained as TWorld2ego(τ∕). For
the standard Gym tasks, since both T world2ego and T world2ego are identity operators, we have T0 ≡ T,
meaning we can simply save the current primitive action a = ω(T) into replay buffer.
A scaling factor can be used for the replanning distribution Categorical(K). We simply use κ=1.
For testing the plan generator after training, the mode of the policy is used, by taking the mean of the
Gaussian policy used in the plan generator and taking the mode of Categorical(z) for replanning.
A.6.2 Plan Generator in Autonomous Driving
We implement f used in plan generator as follows
at+i = at+i-1 + ∆at+i-1 + g(ht+i, at+i-1, ∆at+i-1 )	i ∈ [1, n - 1]	(9)
where ∆at+i-1 = (at+i-1 - at+i-2). This actually incorporates a linear prior into f, which is useful
for tasks with position-based controls. A smoothing cost is also applied to the plan to minimize the
maximum absolute value of its third order derivative max | .T..| (Kyriakopoulos & Saridis, 1988).
In the most basic form, the recurrent plan generator can run at the scale of time-step in order to
generate the full plan. In practice, it can be adjusted according to the actual situation, and running at
a coarser scale, which has the advantage of lower-computational cost. For example, we can let the
policy only output a few number (k) of anchor points and interpolate between them to get the full
plan using a particular form of interpolation methods. Linear interpolation is used in this work.
18
Published as a conference paper at ICLR 2022
A.7 More Results
A.7.1 Ablation Results on Plan Length and Target Commitment Length
Impact of Plan Length. The results with different plan lengths are shown in Figure 10(a). It
is observed that the overall the performance is relatively insensitive to plan length, although the
performance could be further optimized by tuning this hyper-parameter.
Target Commitment Length. The results with different target commitment lengths are shown in
Figure 10(b). The results indicate that the performance of the proposed method is relatively stable
across different target commitment lengths, potentially due to its ability in generating plans with
adaptive shapes.

300
LunarLanderContinuouS
CartpoleSwingup
-----plaπ-leπ 3
-----plaπ-leπ 5
-----plaπ-leπ 7
0.S	1.0 IS
Environment Steps
20
ie5
o fl ------------ plaπ-leπ 9
∏	------ plaπ-leπ H
-100 I
0.0	0.5	1.0	1.5	2.0
Environment Steps	le5
EmeH ⅛sφ><
CartPoIeSwinguP
-----target-leπ 3
-----target-leπ 5
-----target-leπ 7
-----target-leπ 9
-----target-leπ 11
0.5	1.0	1.5	2.0
Environment Steps le5
EmeH ⅛sφ><
Figure 10:	Ablations on (a) plan length and (b) target commitment length. For (a) the target
commitment length is set as 1.5. For (b) the plan length is set as 5.
A.7.2 Observational Dropout: The Importance of Adaptive Plans
Here we compare the proposed method and action repeat under the setting where a fixed replanning
duration is used. For action repeat, this corresponds to the FAR algorithm. For GPM, we also execute
the full plan before switching (denoted as GPM-commit). This actually an instance of observational
dropout (Freeman et al., 2019), which requires the algorithm to extrapolate controls generated from
available observations to those time steps where observations are unavailable. The results are shown
in Figure 11. This results highlight the importance of adaptive form of plans on performance. Fixed
form of plan as used in FAR might be applicable to certain types of tasks (e.g. CartPoleSwingUp).
However, for other tasks, it might be less appropriate and leads to significant performance decreases
with increasing plan length (e.g., LunarLanderContinuous). In contrast, the proposed approach
shows stable performance across a wide range of plan lengths. By contrasting the performance of
FAR with GPM-commit, we highlight the importance on the adaptiveness of the plans.
CartPoIeSwinguP
LunarLanderContinuouS
Enjθa⅛sφ><
plaπ-leπ 3
plaπ-leπ 5
plaπ-leπ 7
CartpoleSwingup
-----plaπ-leπ 3
-----plaπ-leπ 5
-----plaπ-leπ 7
-----plaπ-leπ 9
-----plaπ-leπ 11
0.5	1.0	1.5	2.0
Environment Steps le5
Figure 11:	Observational Dropout for (a) FAR (b) GPM-COmmit. Here both approaches generate
plans of fixed length and execute them to the end before generating the next plan.
A.7.3 MPC v.s. GPM
MPC has been typically used together with planning for obtaining the
control policy (Lowrey et al., 2019). This strategy is appropriate when
a model is given (Lowrey et al., 2019). However, for exploration in RL,
MPC is less desirable as it essentially switches the plan at every step, not
leveraging the temporal property of the plans, as shown on the right, by
comparing with a variant of using MPC together with GPM (GPM-MPC).
19
Published as a conference paper at ICLR 2022
A.7.4 Exploration Trajectory Visualization and Comparison
0-2k steps
2k-4k steps
4k-6k steps
episode end
5 0 5 5 0 5
)s/dar( yticolev ralulgna
ZE RAD
W
CL
0
angle (rad)
angle (rad)
angle (rad)
episode
beginning
Figure 12: Visualization of State Visitation and Evolution during training on Pendulum. Each
figure shows collectively all the rollout trajectories within the range of steps depicted on the top.
Since the most rewarding state is (0, 0), an effective policy should quickly move close to this state.
Figure 13: Exploration Trajectory Visualization. Trajectory are plotted relative its start position.
20