Published as a conference paper at ICLR 2022
Training invariances and the low-rank phe-
nomenon: beyond linear networks
Thien Le & Stefanie Jegelka
Massachusetts Institute of Technology
{thienle,stefje}@mit.edu
Ab stract
The implicit bias induced by the training of neural networks has become a topic of
rigorous study. In the limit of gradient flow and gradient descent with appropriate
step size, it has been shown that when one trains a deep linear network with logis-
tic or exponential loss on linearly separable data, the weights converge to rank-1
matrices. In this paper, we extend this theoretical result to the last few linear layers
of the much wider class of nonlinear ReLU-activated feedforward networks con-
taining fully-connected layers and skip connections. Similar to the linear case, the
proof relies on specific local training invariances, sometimes referred to as align-
ment, which we show to hold for submatrices where neurons are stably-activated
in all training examples, and it reflects empirical results in the literature. We also
show this is not true in general for the full matrix of ReLU fully-connected lay-
ers. Our proof relies on a specific decomposition of the network into a multilinear
function and another ReLU network whose weights are constant under a certain
parameter directional convergence.
1 Introduction
Recently, great progress has been made in understanding the trajectory of gradient flow (GF) (Ji &
Telgarsky, 2019; 2020; Lyu & Li, 2020), gradient descent (GD) (Ji & Telgarsky, 2019; Arora et al.,
2018) and stochastic gradient descent (SGD) (Neyshabur et al., 2015; 2017) in the training of neural
networks. While good theory has been developed for deep linear networks (Zhou & Liang, 2018;
Arora et al., 2018), practical architectures such as ReLU fully-connected networks or ResNets are
highly non-linear. This causes the underlying optimization problem (usually empirical risk min-
imization) to be highly non-smooth (e.g. for ReLUs) and non-convex, necessitating special tools
such as the Clarke subdifferential (Clarke, 1983).
One of the many exciting results of this body of literature is that gradient-based algorithms exhibit
some form of implicit regularization: the optimization algorithm prefers some stationary points to
others. In particular, a wide range of implicit biases has been shown in practice (Huh et al., 2021)
and proven for deep linear networks (Arora et al., 2018; Ji & Telgarsky, 2019), convolutional neural
networks (Gunasekar et al., 2018) and homogeneous networks (Ji & Telgarsky, 2020). One well
known result for linearly separable data is that in various practical settings, linear networks converge
to the solution of the hard SVM problem, i.e., a max-margin classifier (Ji & Telgarsky, 2019), while
a relaxed version is true for CNNs (Gunasekar et al., 2018). This holds even when the margin does
not explicitly appear in the optimization objective - hence the name implicit regularization.
Another strong form of implicit regularization relates to the structure of weight matrices of fully
connected networks. In particular, Ji & Telgarsky (2019) prove that for deep linear networks for
binary classification, weight matrices tend to rank-1 matrices in Frobenius norm as a result of GF/GD
training, and that adjacent layers’ singular vectors align. The max-margin phenomenon follows as
a result. In practice, Huh et al. (2021) empirically document the low rank bias across different
non-linear architectures. However, their results include ReLU fully-connected networks, CNNs and
ResNet, which are not all covered by the existing theory. Beyond linear fully connected networks,
Du et al. (2018) show vertex-wise invariances for fully-connected ReLU networks and invariances
in Frobenius norm differences between layers for CNNs. Yet, despite the evidence in (Huh et al.,
2021), it has been an open theoretical question how more detailed structural relations between layers,
1
Published as a conference paper at ICLR 2022
which, e.g., imply the low-rank result, generalize to other, structured or local nonlinear and possibly
non-homogeneous architectures, and how to even characterize these.
Hence, in this work, we take steps to addressing a wider set of architectures and invariances. First,
we show a class of vertex and edge-wise quantities that remain invariant during gradient flow train-
ing. Applying these invariances to architectures containing fully connected, convolutional and resid-
ual blocks, arranged appropriately, we obtain invariances of the singular values in adjacent (weight)
matrices or submatrices. Second, we argue that a matrix-wise invariance is not always true for gen-
eral ReLU fully-connected layers. Third, we obtain low-rank results for arbitrary non-homogeneous
networks whose last few layers contain linear fully-connected and linear ResNet blocks. To the
best of our knowledge, this is the first time a low-rank phenomenon is proven rigorously for these
architectures.
Our theoretical results offer explanations for empirical observations on more general architectures,
and apply to the experiments in (Huh et al., 2021) for ResNet and CNNs. They also include the
squared loss used there, in addition to the exponential or logistic loss used in most theoretical
low-rank results. Moreover, our Theorem 2 gives an explanation for the “reduced alignment” phe-
nomenon observed by Ji & Telgarsky (2019), where in experiments on AlexNet over CIFAR-10
the ratio kW k2/kW kF converges to a value strictly less than 1 for some fully-connected layer W
towards the end of the network.
One challenge in the analysis is the non-smoothness of the networks and ensuring an “operational”
chain rule. To cope with this setting, we use a specific decomposition of an arbitrary ReLU archi-
tecture into a multilinear function and a ReLU network with +1/-1 weights. This reduction holds in
the stable sign regime, a certain convergence setting of the parameters. This regime is different from
stable activations, and is implied, e.g., by directional convergence of the parameters to a vector with
non-zero entries (Lemma 1). This construction may be of independent interest.
In short, we make the following contributions to analyzing implicit biases of general architectures:
•	We show vertex and edge-wise weight invariances during training with gradient flow. Via these
invariances, we prove that for architectures containing fully-connected layers, convolutional layers
and residual blocks, when appropriately organized into matrices, adjacent matrices or submatrices
of neurons with stable activation pattern have bounded singular value (Theorem 1).
•	In the stable sign regime, we show a low-rank bias for arbitrary nonhomogeneous feedforward
networks whose last few layers are a composition of linear fully-connected and linear ResNet
variant blocks (Theorem 2). In particular, if the Frobenius norms of these layers diverge, then the
ratio between their operator norm and Frobenius norm is bounded non-trivially by an expression
fully specified by the architecture. To the best of our knowledge, this is the first time this type of
bias is shown for nonlinear, nonhomogeneous networks.
•	We prove our results via a decomposition that reduces arbitrarily structured feedforward networks
with positively-homogeneous activation (e.g., ReLU) to a multilinear structure (Lemma 1).
1.1	Related works
Decomposition of fully-connected neural networks into a multilinear part and a 0-1 part has been
used by Choromanska et al. (2015); Kawaguchi (2016), but their formulation does not apply to
trajectory studies. In Section 3, we give a detailed comparison between their approach and ours.
Our decomposition makes use of the construction of a tree network that Khim & Loh (2019) use
to analyze generalization of fully-connected networks. We describe their approach in Section 2
and show how we extend their construction to arbitrary feedforward networks. This construction
makes up the first part of the proof of our decomposition lemma. The paper also makes use of path
enumeration of neural nets, which overlaps with the path-norm literature of Neyshabur et al. (2015;
2017). The distinction is that we are studying classical gradient flow, as opposed to SGD (Neyshabur
et al., 2015) or its variants (Neyshabur et al., 2017).
For linear networks, low-rank bias is proven for separable data and exponential-tailed loss in Ji &
Telgarsky (2019). Du et al. (2018) give certain vertex-wise invariances for fully-connected ReLU
networks and Frobenius norm difference invariances for CNNs. Compared to their results, ours are
slightly stronger since we prove that our invariances hold for almost every time t on the gradient
2
Published as a conference paper at ICLR 2022
flow trajectory, thus allowing for the use of a Fundamental Theorem of Calculus and downstream
analysis. Moreover, the set of invariances we show is strictly larger. Radhakrishnan et al. (2020)
show negative results when generalizing the low-rank bias from (Ji & Telgarsky, 2019) to vector-
valued neural networks. In our work, we only consider scalar-valued neural networks performing
binary classification. For linearly inseparable but rank-1 or whitened data, a recent line of work
from Ergen & Pilanci (2021) gives explicit close form optimal solution, which is both low rank and
aligned, for the regularized objective. This was done for both the linear and ReLU neural networks.
In our work, we focus on the properties of the network along the gradient flow trajectory.
2	Preliminaries and notation
For an integer k ∈ N, we write the set [k] := {1, 2, . . . , k}. For vectors, we extend the sign function
sgn : R → {-1, 1} coordinate-wise as sgn : (xi)i∈[d] 7→ (sgn(xi))i∈[d] . For some (usually the
canonical) basis (ei)i∈[n] in some vector space Rn, for all x ∈ Rn we use the notation [x]i = hx, eii
to denote the i-th coordinate of x.
Clarke subdifferential, definability and nonsmooth analysis. The analysis of non-smooth func-
tions is central to our results. For a locally Lipschitz function f : D → R with open domain D,
there exists a set Dc ⊆ D of full LebesgUe measure on which the derivative Vf exists everywhere
by Rademacher’s theorem. As a result, calculus can usually be done over the Clarke subdifferential
∂f (x) := CONV limi→∞ Vf(xi) | xi ∈ Dc, xi → x where CONV denotes the convex hull.
The Clarke subdifferential generalizes both the smooth derivative when f ∈ C 1 (continuously dif-
ferentiable) and the convex subdifferential when f is convex. However, it only admits a chain rule
with an inclusion and not equality, which is though necessary for backpropagation in deep learn-
ing. We do not delve in too much depth into Clarke subdifferentials in this paper, but use it when
we extend previous results that also use this framework. We refer to e.g. (Davis et al., 2020; Ji &
Telgarsky, 2020; Bolte & Pauwels, 2020) for more details.
Neural networks. Consider a neural network ν : Rd → R. The computation graph of ν is a
weighted directed graph G = (V, E, w) with weight function w : E → R. For each neuron v ∈ V ,
let INv := {u ∈ V : uv ∈ E} and OUTv := {w ∈ V : vw ∈ E} be the input and output neurons of
v. Let {i1, i2, . . . , id} =: I ⊂ V and O := {o} ⊂ V be the set of input and output neurons defined
as IN(i) = 0 = OUT(o), ∀i ∈ I. Each neuron V ∈ V\I is equipped with apositively 1-homogeneous
activation function σv (such as the ReLU x 7→ max(x, 0), leaky ReLU x 7→ max(x, αx) for some
small positive α , or the linear activation).
To avoid unnecessary brackets, We write We := w(e) for some e ∈ E. We will also write W ∈ RE,
where E is the set of learnable weights, as the vector of learnable parameters. Let P bea path in G,
i.e., a set of edges in E that forms a path. We write v ∈ P for some v ∈ V if there exists u ∈ V
such that uv ∈ P or vu ∈ P. Let ρ be the number of distinct paths from any i ∈ I to o ∈ O. Let
P := p1 , . . . , pρ be the enumeration of these paths. For a path p ∈ P and an input x to the neural
network, denote by xp the coordinate of x used in p.
Given a binary classification dataset (xi, yi) i∈[n] with xi ∈ Rd, kxi k ≤ 1 and yi ∈ {-1, 1}, we
minimize the empirical risk R(W) = 1 PZi '(y%ν(Xi)) = 1 PZi '(ν(yiXi)) with loss ' : R →
R, using gradient flow dwd(t) ∈ -∂R(w(t)).
As we detail the architectures used in this paper, we recall that the activation of each neuron is still
positively-homogeneous. The networks considered here are assumed to be bias-free.
Definition 1 (Feedforward networks). A neural net ν with graph G is a feedforward network if G
is a directed acyclic graph (DAG).
Definition 2 (Fully-connected networks). A feedforward network ν with graph G is a fully-
connected network if there exists a partition of V into V = (I ≡ V1) t V2 t . . . t (VL+1 ≡ O) such
that for all u, v ∈ V, uv ∈ E iff there exists i ∈ [L] such that u ∈ Vi and v ∈ Vi+1.
Definition 3 (Tree networks). A feedforward network ν with graph G is a tree network if the under-
lying undirected graph G is a tree (undirected acyclic graph).
3
Published as a conference paper at ICLR 2022
Examples of feedforwards networks include ResNet (He et al., 2016), DenseNet (Huang et al.,
2017), CNNs (Fukushima, 1980; LeCun et al., 2015) and other fully-connected ReLU architectures.
For a fully-connected network ν with layer partition V =: V1 t . . . t VL+1 where L is the number
of (hidden) layers, let ni := |Vi| be the number of neurons in the i-th layer and enumerate Vi =
{vi,j}j∈[ni]. Weights in this architecture can be organized into matrices W[1], W[2], . . . , W[L] where
Rni+1×ni 3W[i] = ((wvi,jvi+1,k))j∈[ni],k∈[ni+1], for all i ∈ [L].
Tree networks. Most practical architectures are not tree networks, but trees have
been used to prove generalization bounds for adversarial risk. In particular, for fully-
connected neural networks f whose activations are monotonically increasing and 1-
Lipschitz, Khim & Loh (2019) define the tree transform as the tree network T f (x; w) =
PpnLL=1 W1[L,p]L σ ... Ppn22=1 Wp[23],p2 σ wp2..pL +Ppn11=1Wp[12],p1xp1	for vectors w with
QjL=2 nj entries, indexed by an L-tuple (p2 , . . . , pL). We extend this idea in the next section.
3	Structural lemma: decomposition of deep networks
We begin with a decomposition of a neural network into a multilinear and a non-weighted nonlinear
part, which will greatly facilitate the chain rule that we need to apply in the analysis. Before stating
the decomposition, we need the following definition of a path enumeration function, which computes
the product of all weights and inputs on each path of a neural network.
Definition 4 (Path enumeration function). Let ν be a feedforward neural network with graph G
and paths P = p1 , . . . , pρ . The path enumeration function h is defined for this network as
h : (x1, x2, . . . , xd) 7→ xp Qe∈p we where xp := xk such that ik ∈ p.
We first state the main result of this section, proven in Appendix B.
Lemma 1 (Decomposition). Let ν : Rd → R be a feedforward network with computation graph G,
and P the number ofdistinct maximal paths in G. Then there exists a tree network μ : RP → R such
that V = μ ◦ h where h : Rd → RP is the path enumeration function of G. Furthermore, all weights
in μ are either —1 or +1 andfuUy determined by the signs of the weights in V.
Path activation of ReLU networks in the literature. The viewpoint that for every feedforward
network V there exists a tree network μ such that V = μ ◦ h is not new and our emphasis here is on
the fact that the description of μ : RP → R is fully determined by the signs of the weights. Indeed,
in analyses of the loss landscape (Choromanska et al., 2015; Kawaguchi, 2016), ReLU networks are
described as a sum over paths:
V(x; w) =	Zp(x; w)	we = (Zp(x; w))p∈P, h(x; w) Rρ ,	(1)
p∈P	e∈p
where Zp(x; w) = 1 iff all ReLUs on path p are active (have nonnegative preactivation) and 0
otherwise. One can then take μ as a tree network with no hidden layer, P input neurons all connected
to a single output neuron. However, this formulation complicates analyses of gradient trajectories,
because of the explicit dependence of Zp on numerical values of w. In our lemma, μ isa tree network
whose description depends only on the signs of the weights. If the weight signs (not necessarily the
ReLU activation pattern!) are constant, μ is fixed, allowing for a chain rule to differentiate through
it. That weight signs are constant is realistic, in the sense that it is implied by directional parameter
convergence (Section 4.1). To see this, compare the partial derivative with respect to some we (when
it exists) between the two approaches, in the limit where weight signs are constant:
(using Lemma 1)	∂ν∕∂we	=	ɪ2	Vwμ(x; w)]p ∙ Xp ɪɪ Wf,	(2)
p∈P∣e∈p	f ∈p,f=e
(using Zp in Eqn. 1)	∂ν∕∂we	=	ɪ2	(WedZp(x; w)∕∂we + Zp(x; W))	ɪɪ Wf.	(3)
p∈P∣e∈p	f ∈p,f=e
In particular, the dependence of Equation 2 on We is extremely simple. The utility of this fact will
be made precise in the next section when we study invariances.
4
Published as a conference paper at ICLR 2022
Proof sketch. The proof contains two main steps. First, we “unroll” the feedforward network into
a tree network that computes the same function by adding extra vertices, edges and enable weight
sharing. This step is part of the tree transform in Khim & Loh (2019) if the neural network is a
fully-connected network; we generalize it to work with arbitrary feedforward networks. Second,
we “pull back” the weights towards the input nodes using positive homogeneity of the activations:
a ∙ σ(x) = sgn(a) ∙ σ(x∣a∣). This operation is first done on vertices closest to the output vertex
(in number of edges on the unique path between any two vertices in a tree) and continues until all
vertices have been processed. Finally, all the residual signs can be subsumed into μ by subdividing
edges incident to input neurons. We give a quick illustration of the two steps described above for a
fully-connected ReLU-activated network with 1 hidden layer in Appendix A.
4 Main Theorem: Training invariances
In this section, we put the previous decomposition lemma to use in proving an implicit regularization
property of gradient flow when training deep neural networks.
4.1	Stable sign regime: a consequence of directional convergence
Recall the gradient flow curve {w(t)}t∈[o,∞) defined by the differential inclusion dwd(t) ∈
-∂R(w(t)). We first state the main assumption in this section.
Assumption 1 (Stable sign regime). For some t0 < tN ∈ [0, ∞], we assume that for all t ∈
[t0, tN), sgn(w(t)) = sgn(w(t0)). If this holds, we say that gradient flow is in a stable sign regime.
Without loss of generality, when using this assumption, we identify t0 with 0 and write ”for some
t ≥ 0” to mean ”for some t ∈ [t0, tN)”.
In fact, the following assumption - the existence and finiteness part of which has been proven in (Ji
& Telgarsky, 2020) for homogeneous networks, is sufficient.
Assumption 2 (Directional convergence to non-vanishing limit in each entry). We assume that
kWk2 t→∞> W exists, is finite in each entry and furthermore, for all e ∈ E, we = 0.
Motivation and justification. It is straightforward to see that Assumption 1 follows from As-
sumption 2 but we provide a proof in the Appendix (Claim 1). Directional convergence was proven
by Ji & Telgarsky (2020) for the exponential/logistic loss and the class of homogeneous networks,
under additional mild assumptions. This fact justifies the first part of Assumption 2 for these archi-
tectures. The second part of Assumption 2 is pathological for our case, in the sense that directional
convergence alone does not imply stable signs (for example, a weight that converges to 0 can change
sign an infinite number of times).
Pointwise convergence is too strong in general. Note also that assuming pointwise convergence
of the weights (i.e. limt→∞ w(t) exists and is finite) is a much stronger statement, which is not true
for the case of exponential/logistic loss and homogeneous networks (since kw(t)k2 diverges, see for
example Lyu & Li (2020), Ji & Telgarsky (2020), Ji & Telgarsky (2019)). Even when pointwise
convergence holds, it would immediately reduce statements on asymptotic properties of gradient
flow on ReLU activated architectures to that of linearly activated architectures. One may want
to assume that gradient flow starts in the final affine piece prior to its pointwise convergence and
thus activation patterns are fixed throughout training and the behavior is (multi)linear. In contrast,
directional convergence of the weights does not imply such a reduction from the ReLU-activation
to the linear case. Similarly, with stable signs, the parts of the input where the network is linear are
not convex, as opposed to the linearized case (Hanin & Rolnick, 2019) (see also Claim 2).
Stable sign implication. The motivation for Assumption 1 is that weights in the tree network μ
in Lemma 1 are fully determined by the signs of the weights in the original feedforward network ν.
Thus, under Assumption 1, one can completely fix the weights of μ - it has no learnable parameters.
Since we have the decomposition V = μ ◦ h where h is the path enumeration function, dynamics of
μ are fully determined by dynamics of h in the stable sign regime. To complete the picture, observe
that h is highly multilinear in structure: the degree of a particular edge weight we in each entry of h
is at most 1 by definition ofa path; and if ν is fully-connected, then h is a Rn1 ×n2×...×nL tensor.
5
Published as a conference paper at ICLR 2022
4.2	Training invariances
First, we state an assumption on the loss function that holds for most losses used in practice, such as
the logistic, exponential or squared loss.
Assumption 3 (Differentiable loss). The loss function ` : R → R is differentiable everywhere.
Lemma 2 (Vertex-wise invariance). Under Assumptions 1, and 3, for all v ∈ V \{I ∪ O} such that
all edges incident to v have learnable weights, for a.e. time t ≥ 0,
Xwu2v(t)- X wv2b(t) = Xwu2v(0)- X wv2b(0).	(4)
u∈INv	b∈OUTv	u∈INv	b∈OUTv
If we also have INu = INv = IN and OUTu = OUTv = OUT and u and v have the same activation
pattern (preactivation has the same sign) for each training example and for a.e time t ≥ 0, then for
a.e. time t ≥ 0,
wau (t)wav (t) -	wub (t)wv b (t) =	wau (0)wav (0) -	wub (0)wvb (0).	(5)
a∈IN	b∈OUT	a∈IN	b∈OUT
Comparison to Du et al. (2018) A closely related form of Equation 4 in Lemma 2 has appeared
in Du et al. (2018) (Theorem 2.1) for fully-connected ReLU/leaky-ReLU networks. In particular,
the authors showed that the difference between incoming and outgoing weights does not change. In-
voking the Fundamental Theorem of Calculus (FTC) over this statement will return ours. However,
their proof may not hold on a nonnegligible set of time t due to the use of the operational chain rule
that holds only for almost all we. Thus the FTC can fail. The stronger form we showed here is useful
in proving downstream algorithmic consequences, such as the low rank phenomenon. Furthermore,
our result also holds for arbitrary feedforward architectures and not just the fully-connected case.
Before we put Lemma 2 to use, we list definitions of some ResNet variants.
Definition 5. Denote ResNetIdentity, ResNetDiagonal and ResNetFree to be the version of ResNet
described in He et al. (2016) where the residual block is defined respectively as
1.	r(x; U, Y ) = σ(Uσ(Y x) + Ix) where x ∈ Ra, Y, U> ∈ Rb×a, and I is the identity,
2.	r(x; U, Y, D) = σ(Uσ(Y x) + Dx) where x ∈ Ra, Y, U> ∈ Rb×a, and D is diagonal,
3.	r(x; U, Y, Z) = σ(Uσ(Y x) + Zx) where x ∈ Ra,Y ∈ Rb×a,U ∈ Rc×b andZ ∈ Rc×a.
ResNetIdentity is the most common version of ResNet in practice. ResNetIdentity is a special
case of ResNetDiagonal, which is a special case of ResNetFree. Yet, theorems for ResNetFree do
not generalize trivially to the remaining variants, due to the restriction of Lemma 2 and Lemma 3 to
vertices adjacent to all learnable weights and layers containing all learnable weights. For readability,
we introduce the following notation:
Definition 6 (Submatrices of active neurons). Fix some time t, let W ∈ Ra×b be a weight matrix
from some set of a neurons to another set of b neurons. Let Iactive ⊆ [b] be the set of b neurons that
are active (linearor ReLU with nonnegative preactivation). We write [W>W]初证 ∈ R1Iactivel×lIactive|
for the submatrix of W> W with rows and columns from Iactive. Similarly, if W0 ∈ b × c is another
weight matrix from the same set of b neurons to another set of c neurons then [W0W0>]active is
defined as the submatrix with rows and columns from Iactive.
When applying Lemma 2 to specific architectures, we obtain the following:
Theorem 1 (Matrix-wise invariances). Recall that a convolutional layer with number of input ker-
nels a, kernel size b and number of output kernels c and is a tensor in Ra×b×c. Under Assumptions
1 and 3, we have the following matrix-wise invariance for a.e. time t ≥ 0:
W1(t)W1(t)>i	= 0, for:	(6)
1.	(Fully-connected layers) W1 ∈ Rb×a and W2 ∈ Rc×b consecutive fully-connected layers,
2.	(Convolutional layers) W1 is convolutional, viewed as a flattening to a matrix Rc×(a×b), and
W2 adjacent convolutional, viewed as a flattening to a matrix R(d×e)×c,
ddt (hw2aw2 ⑴ Ltive- h
6
Published as a conference paper at ICLR 2022
3.	(Within residual block of ResNet) W1 = Y and W2 = U where r(x; U, Y, Z) is a residual block
of ResNetIdentity, ResNetDiagonal or ResNetFree,
4.	(Between residual blocks of ResNet) W1 = U1
j ∈ {1, 2} are consecutive ResNetFree blocks,
Z1, W2 = ZY2 where r(x; Uj , Yj , Zj),
5.	(Convolutional-fully-connected layers) W1 convolutional, viewed as a flattening to a matrix
Rc×(a×b) and W2 adjacent fully-connected layer, viewed as an rearrangement to Rd×c,
6.	(Convolutional-ResNetFree block) W1 convolutional, viewed as a flattening to a matrix
Rc×(a×b) and W2 is a rearrangement of U Z into an element of Rd×c, where r(x; U, Y, Z)
is an adjacent ResNetFree block,
7.	(ResNetFree block-fully-connected layers and vice versa) W1 = U Z ∈ Rb×a and W2 ∈
Y
Rc×b adjacent fully-connected or W1 ∈ Rb×a fully-connected and W2 = Z ∈ Rc×b adjacent
ResNet block where r(x; U, Y, Z) is the ResNetFree block.
We emphasize that the above theorem only makes local requirements on the neural network, to
have local parts that are either fully-connected, convolutional or a residual block. The only global
architecture requirement is feedforward-ness. The first point of Theorem 1 admits an extremely
simple proof for the linear fully-connected network case in Arora et al. (2018) (Theorem 1).
Significance of Theorem 1. If we have a set of neurons that is active throughout training (which
is vacuously true for linear layers), we can invoke an FTC and get W2(t)>W2(t) - W1(t)W1(t)> =
W2(0)>W2(0)-W1(0)W1(0)> for the submatrix restricted to these neurons. Assume for simplicity
that the right hand side is 0, then the singular values of W1 and W2 are the same for each of the
cases listed in Theorem 1. If we can form a chain of matrices whose singular values are the same
by iteratively invoking Theorem 1, then all matrices considered have the same singular values as the
final fully-connected layer that connects to the output. Recall that our networks are scalar-valued, so
the final layer is a row vector, which is rank 1 and thus all layers considered in the chain have rank
1, which is useful in the next section.
Proof sketch of Theorem 1. Given Lemma 2, we demonstrate the proof for the first point. The
remaining points admit the exact same proof technique but on different matrices, which require some
bookkeeping. Let W1 ∈ Rb×a and W2 ∈ Rc×b be two consecutive fully-connected layers for some
a, b, c ∈ N number of vertices in these layers. Applying Equation 4 of Lemma 2 to each of the
b shared neurons between these two layers, one obtains the diagonal entries of Equation 6 of the
Theorem. Now, apply Equation 5 to each pair among the b shared neurons between these two layers
to get the off-diagonal entries of Equation 6.
Next, we define layers for architectures where weights are not necessarily organized into matrices,
e.g., ResNet or DenseNet.
Definition 7 (Layer). Let F ⊂ E be such that 1) for all e 6= f ∈ F, there is no path that contains
both e and f; and 2) the graph (V, E\F, w) is disconnected. Then F is called a layer of G.
For this definition, we have the following invariance:
Lemma 3 (Edge-wise invariance). Under Assumptions 1 and 3, for all layers F and F0 that contain
all learnable weights, it holds that for a.e. time t ≥ 0,
X we2(t) - Xwf2(t)=Xwe2(0)- Xwf2(0).	(7)
e∈F	f∈F0	e∈F	f∈F0
Significance of Lemma 3. A flattening of a convolutional parameter tensor and a stacking of
matrices in a ResNetDiagonal and ResNetFree block forms a layer. This lemma implies that the
squared Frobenius norm of these matrices in the same network differs by a value that is fixed at
initialization. The lemma also gives a direct implicit regularization for networks with biases, by
treating neurons with bias as having an extra in-edge whose weight is the bias, from an extra in-
vertex which is an input vertex with fixed input value 1.
7
Published as a conference paper at ICLR 2022
4.3	Proof sketch of Lemma 2 and Lemma 3
The proofs of Lemma 2, Lemma 3 and Theorem 1 share the technique of double counting paths,
which we explain next. For simplicity, we assume here that we are working with a network that is
differentiable everywhere in some domain that We are considering - We give a full general proof in
the Appendix. The main proof idea was used in (Arora et al., 2018) and involves simply writing
doWn the partial derivative of the risk. We have, for some particular Weight we , e ∈ E, via the
smooth chain rule
∂R(w)
∂we
1n
一£i0(yiv(xi； W)) ∙ yi ∙
n i=1
∂ν (W)
∂we
1n
n∑Y(IyiV(xi； W)) ∙ yi ∙ Σ [μ0(xi； w)]p ∙ (χi)p	ɪɪ Wf,
i=1	p∈P,p3e	f ∈p,f 6=e
(8)
(9)
where in the second line, we invoke the decomposition Lemma 1 and emphasize that μ has no
learnable parameters in the stable sign regime. NoW multiply We to the above expression to get
∂R(W)
We-^---
∂We
1n
x n xi,
p∈P,p3e	i=1
(10)
where Ai,p(W)= l0(yi,ν(xi; W)) ∙ yi ∙ "E; w)[? ∙ (xi)P Qf Ep ∣Wf |. Notice that Ai,p(W)does not
depend explicitly on the edge e, with respect to which we are differentiating (only through W). Thus,
we sum over in-edges and out-edges of a particular v satisfying the assumption of Lemma 2 to get
Σ
u∈INv
∂R(W)
Wuv -----
∂Wuv
1	∂R(W)
H n i=1 Ai,p(W) = b%Wvb F
(11)
Note that the only difference between Equations 10 and 11 is the set of paths that we are sum-
ming over, and we double count this set of paths. We use the definition of gradient flow to obtain
∂R(w)∕∂we = dWe(t)∕dt and integrate with respect to time using a FTC to get the first part of
Lemma 2. More work is needed to get the second part of Lemma 2, which is detailed in Appendix
C. Finally, to get Lemma 3, we double count the set of all paths P.
4.4	Noninvariance of general ReLU layers
The restriction of Theorem 1 to submatrices of active neurons may appear limiting, but does not
extend to the general case. With the same technique as above, we can write down the gradient for
the Gram matrix W1> W1 for ReLU layers and show that it is not equal to its counterpart W2W2>,
thus giving a negative result:
Lemma 4 (Noninvariance in ReLU layers). Even under Assumptions 3 and 1, for a.e. time t ≥ 0,
% (W2(t)>W2(t)- W1(t)W1(t)>) =0,	(12)
for the different pairs of W1, W2 detailed in Theorem 1.
Despite the negative result, the closed form of the gradient for the Gram matrices can be shown to
be low rank with another subgradient model. Details may be found in Appendix C.
5 Consequences: Low rank phenomenon for nonlinear
NONHOMOGENEOUS DEEP FEEDFORWARD NET
We apply the results from previous parts to prove a low-rank bias result for a large class of feedfor-
ward networks. To the best of our knowledge, this is the first time such a result is shown for this
class of deep networks, although the linear fully-connected network analogue has been known for
some time. In light of Theorem 1, we define a matrix representation ofa layer:
Definition 8 (Matrix representation of a layer). The matrix representation for a ResNetFree block
r(x； U, Y, Z) is U Z ; for a T a,b,c ∈ Ra×b×c convolutional tensor it is the flattening to an
element of Ra×(b×c); and for a fully-connected layer it is the weight matrix itself.
8
Published as a conference paper at ICLR 2022
kW；1 -Uu(t)v> ⑴
Theorem 2 (Reduced alignment for non-homogeneous networks). Under Assumptions 1 and 3, let ν
consist of an arbitrary feedforward neural network η, followed by K ≥ 0 linear convolutional layers
(Tak,bk,ck)k∈[K], followed by M ≥ 0 layers that are either linear ResNetFree blocks or linear fully-
connected layers; and finally ending with a linear fully-connected layer Fin. For j ∈ [K + M],
denote by W[j] the matrix representation of the j-th layer after η, Nr(j) the number of ResNetFree
blocks between j and Fin exclusively and Vc(j) ：= max dim W[M+1] ∙ Qj<k≤M min(ak, bk) if
j ≤ M and 1 otherwise. Then there exists a constant D ≥ 0 fixed at initialization such that for a.e.
time t > 0,
8Nr(j1V(j) kW[j](t)kF -kW[j](t)k2 ≤ D, ∀j ∈ [K + M].	(13)
Furthermore, assume that kW [j] kF → ∞ for some j ∈ [K + M], then we have, as t → ∞:
1/ min rank(W [k]), 8Nr(j)Vc(j) ≤ kW [k](t)k22/kW [k](t)k2F ≤ 1,	(14)
In particular, for the last few fully-connected layers j with Nr(j) = 0 and Vc(j) = 1, we have:
t→∞	t→∞
---→ 0, ∀k ∈ [M],	| vj+1, uj | ---→ 1,	(15)
F
where uj and vj are the left and right principal singular vectors ofW[j].
Corollary 1. For fully-connected networks with ReLU activations where the last K layers are lin-
ear layers, trained with linearly separable data under logistic loss `, under the assumptions that
R(w(0)) < '(0) and the limiting direction of weight vector (which exists (Ji & Telgarsky, 2020))
has no 0 entries, Equation 15 holds for the last K layers.
Significance Equation 13 and its limiting counterpart Equation 14 quantify a low-rank phe-
nomenon by providing a lower bound on the ratio of the largest squared singular value (the operator
norm) and the sum of all squared singular values (the Frobenius norm). This lower bound depends
on the number (not dimensions) of ResNetFree layers (Nr) and certain dimensions of convolutional
layers (Vc). When the dimensions of ResNetFree layers are large, maxdimW[M+1] is small and the
number of input channels of convolutional layers are large, this lower bound is strictly better than the
trivial lower bound of 1/rank(W [k]). This is a quantification of the reduced alignment observed in
(Ji & Telgarsky, 2019). In particular, for the last few fully connected layers (Equation 15, Corollary
1), the lower bound matches the upper bound of 1 in the limit of Frobenius norm tending to infinity
and the limiting weight matrices have rank 1 and adjacent layers align.
6 Concluding remarks and Future directions
In this paper, we extend the proof of the low rank phenomenon, which has been widely observed in
practice, beyond the linear network case. In particular, we address a variety of nonlinear architectural
structures, homogeneous and non-homogeneous, which in this context have not been addressed
theoretically before. To this end, we decomposed a feedforward ReLU/linear activated network into
a composition of a multilinear function with a tree network. If the weights converge in direction to a
vector with non-zero entries, the tree net is eventually fixed, allowing for chain rules to differentiate
through. This leads to various matrix-wise invariances between fully-connected, convolution layers
and ResNet blocks, enabling us to control the singular values of consecutive layers. In the end, we
obtain a low-rank theorem for said local architectures.
Proving convergence to the stable sign regime for a wider set of architectures will strengthen The-
orem 2. Another direction is to connect our low-rank bias results to the max-margin implicit
regularization literature, which has been shown for linear networks and, more recently, certain 2-
homogeneous architectures (Ji & Telgarsky, 2020).
Acknowledgments
This work was partially funded by NSF CAREER award 1553284 and NSF award 2134108. The
authors thank the anonymous reviewers for their insightful feedback. We would also like to thank
Matus Telgarsky for fruitful discussions on their related papers and on the Clarke subdifferential,
and Kaifeng Lyu for pointing out an error in an earlier version of this paper.
9
Published as a conference paper at ICLR 2022
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer G. Dy and Andreas Krause (eds.), ICML,
volume 80 of Proceedings ofMachine Learning Research, pp. 244-253. PMLR, 2018. URL
http://dblp.uni-trier.de/db/conf/icml/icml2018.html#AroraCH18.
Jerome Bolte and EdoUard Pauwels. A mathematical model for automatic differentiation in ma-
chine learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 10809-10819. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
7a674153c63cff1ad7f0e261c369ab2c- Paper.pdf.
Anna Choromanska, MIkael Henaff, Michael Mathieu, Gerard Ben Arous, and Yann LeCun. The
Loss Surfaces of Multilayer Networks. In Guy Lebanon and S. V. N. Vishwanathan (eds.), Pro-
ceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,
volume 38 of Proceedings of Machine Learning Research, pp. 192-204, San Diego, Califor-
nia, USA, 09-12 May 2015. PMLR. URL https://proceedings.mlr.press/v38/
choromanska15.html.
F.H. Clarke. Optimization and Nonsmooth Analysis. Wiley New York, 1983.
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics, 20(1):119-
154, 2020. doi: 10.1007/s10208-018-09409-5. URL https://doi.org/10.1007/
s10208-018-09409-5.
Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homoge-
neous models: Layers are automatically balanced, 2018.
Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality.
In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of
Machine Learning Research, pp. 3004-3014. PMLR, 2021. URL http://proceedings.
mlr.press/v139/ergen21b.html.
Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193-202, 1980.
doi: 10.1007/BF00344251. URL https://doi.org/10.1007/BF00344251.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient de-
scent on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/
paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf.
Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation pat-
terns. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
9766527f2b5d3e95d4a733fcfb77bd7e-Paper.pdf.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90.
Christopher Heil. Absolute continuity and the fundamental theorem of calculus, 2019. URL
https://doi.org/10.1007/978-3-030-26903-6_6.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017.
10
Published as a conference paper at ICLR 2022
Minyoung Huh, Hossein Mobahi, Richard Zhang, Pulkit Agrawal, and Phillip Isola. The low-rank
simplicity bias in deep networks. arXiv, 2021.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In In-
ternational Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=HJflg30qKX.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 17176-17186. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
c76e4b2fa54f8506719a5c0dc14c2eb9- Paper.pdf.
Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/
paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf.
Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation, 2019.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural net-
works. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=SJeLIgBKPS.
Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized op-
timization in deep neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso-
ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/
eaa32c96f620053cf442ad32258076b9- Paper.pdf.
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Geometry of
optimization and implicit regularization in deep learning, 2017.
Adityanarayanan Radhakrishnan, Eshaan Nichani, Daniel Bernstein, and Caroline Uhler. On align-
ment in deep linear neural networks, 2020.
Miaoyan Wang, Khanh Dao Duc, Jonathan Fischer, and Yun S. Song. Operator norm inequalities
between tensor unfoldings on the partition lattice, May 2017. ISSN 0024-3795. URL http:
//dx.doi.org/10.1016/j.laa.2017.01.017.
Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and
landscape properties. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=SysEexbRb.
A Illustration of Lemma 1
We give a quick illustration of the two steps described in Section 3 for a fully-connected ReLU-
activated network with 1 hidden layer. Figure 1 describes the unrolling of the neural network (Figure
1a) into a tree network (Figure 1b). Figure 2a describes the weight pull-back in the hidden layer and
Figure 2b describes the weight pull-back in the input layer. It is clear that the inputs of the tree net
in Figure 2b are coordinates of the path enumeration function h(x; w) in this example. Furthermore,
weights in the tree net depend entirely on the signs of the original weights. The rest of the proof
argues this intuition for general feed-forward neural nets. As a remark, in general, ρ is a very large
number - exponential in the number of layers for a fully-connected net with fixed width.
11
Published as a conference paper at ICLR 2022
T~~∖
Gl
(b)
(a)
Figure 1: Transformation of a feedforward network into a tree net. All nodes apart from the input
nodes use ReLU activation. The two neural nets drawn here compute the same function. This idea
has been used in Khim & Loh (2019) to prove generalization bounds for adversarial risk.
(b)
(a)
Figure 2: Pulling back of weights in a tree net. All nodes apart from the input nodes use the
ReLU activation. The original net was drawn in Figure 1a and 1b. This is possible due to the
positive-homogeneity of the activation function. From Figure 2b, one can recover the final tree net
in Theorem 1 with weights from {-1, +1} and input from the path enumeration function h of the
neural net by subdividing the edge incident to the input neurons and assign weights corresponding
to sgn(h(x)).
B Proof of Lemma 1
We first prove an absolute-valued version of Lemma 1 and show that the extension to Lemma 1 is
straightforward. In other words, we first prove
Lemma 5 (Absolute-valued decomposition). For an arbitrary feed-forward neural network ν :
Rd → R, there exists a tree network μ0 : RP → R such that V = μ0 ◦ h0 where h0 is the
12
Published as a conference paper at ICLR 2022
absolute-valued path enumeration function defined as h0(x) = xp e∈p |we|	. Furthermore,
the weights of μ0 is in {-1,1} and only depends on the sign of the weights of the original network
ν.
We first give some extra notation for this section. In general, we define:
•	夕(S) to be the finite power set of some finite set S.
•	|p| ∈ N≥0 to be the cardinality of a set p. When p is a path then it is viewed as the number
of edges.
To make it clear which graph we are referring to, define for an arbitrary feedforward neural network
ν : Rd → R with computation graph G = (G[V], G[E], G[w]):
•	IG = {i1G, . . . , idG} to be the set of input node of G and OG = {oG} to be the set of the
single output node of G.
•	PG to be the enumeration of all paths from any input node in IG to the output node oG .
•	HG to be the enumeration of all paths from any node v ∈ G[V] to the output node oG.
Note that if G has more than 2 vertices then HG ⊃ PG .
•	Each v ∈ G[V] to be equipped with a fix activation σv that is positively-1-homogeneous.
To be precise, G is enforced to be a DAG, is connected and is simple (no self-loop, at most
1 edge between any pair of vertices). Each node v of G[V] is equipped with: an activation
function G[σ](v) that is positively-1-homogeneous; a pre-activation function defined as:
(G[PRE](v))(x) = (xj	ifv≡ijGforsomej∈ [d]	(16)
(G[PRE](v))(x) = Pu∈INv POSTv (x)wuv otherwise;	(16)
and a post-activation function defined as
(G[POST](v))(x) = σv((G[PRE](v))(x))	(17)
Note that POSToG = ν.
•	vpG, epG to be the vertex and edge, respectively, furthest from oG on some path p of G.
•	xpG to be the unique xj such that ijG ∈ p.
Definition 9 (Hasse diagram of inclusion). Let S be a finite set and a set S ⊂ 夕(S) of elements
in S. A Hasse diagram ofS is a directed unweighted graph HASSE(S) = (S, E) where for any
p, q ∈ S, pq ∈ E iffp ⊆ q and |q| - |p| = 1.
Definition 10 (Unrolling of feedforward neural networks). The unrolled tree neural network τG of
G is a tree neural network with computation graph TG where
•	Unweighted graph (TG[V], TG[E]) = HASSE(HG). In particular TG[V] = HG and we
identify vertices of TG with paths in G.
•	Weight function TG[w] : TG [E] 3 pq 7→ G[w](ep).
•	Activation TG [σ](v) := G[σ](vp).
Lemma 6 (Unrolled network computes the same function). Fix an arbitrary feedforward neural
network ν : Rd → R with computation graph G. Let τG be the unrolled tree network of G with
computation graph TG. Then ν = τG.
Proof. We proceed with induction on αG := maxp∈P |p| the longest path between any input node
and the output node of G. In the base case, set αG = 0. Then VG = {oG} is a singleton and the
neural network ν computes the activation of the input and return it. HG = VT = {p0} then is
a singleton containing just the trivial path that has just the output vertex of G and no edges. The
activation function attached to p0 in T , by construction, is the activation of oG . Thus, τ also simply
returns the activation of the input.
Assume that τG = ν for any ν with graph G such that αG ≤ t - 1 for some t ≥ 1; and for the τ
constructed as described. We will show that the proposition is also true when ν has graph G with
αG = t. Fix such a ν and G that αG = t. We prove this induction step by:
13
Published as a conference paper at ICLR 2022
1.	First constructing G0 from G such that ν0 = ν where ν0 is the neural network computed by
G0.
2.	Then showing that TG = G0 by constructing an isomorphism π : G0[V] → TG[V] that
preserves E, w and σ.
The construction of G0 An illustration of the following steps can be found in Figure 3. Recall
that INoG = {v1, . . . , vm} is the set of in-vertices of oG.
1.	Create m distinct, identical copies of G: G1 , . . . , Gm .
2.	For each j ∈ [m], remove from Gj all vertices u (and their adjacent edges) such that there
are no directed path from u to vj .
3.	We now note that αGj ≤ t - 1 (to be argued) and invoke inductive hypothesis over Gj
to get an unrolled tree network τj with graph Tj such that τj = νj where νj is the neural
network computed by Gj .
4.	Finally, construct G0 by creating a new output vertex oG0 and connect it to the output
vertices oTj for all j ∈ [m]. As a sanity check, since each Tj is a tree network, so is G0.
More precisely,
(a)	G0[V] ={oG0}∪Sjm=1Tj[V];
(b)	G0[E] = {oGj oG0 | j ∈ [m]} ∪ Sjm=1 Tj [E]
(c) G0[w](e) = G[w](vjoG) if e ≡ oGj oG0 for some j ∈ [m] and Tj [w](e), where
e ∈ Tj[E], otherwise.
(d) G0 [σ](v) = G[σ](oG) if v ≡ oG and Tj [σ](v), where v ∈ Tj [V ], otherwise.
(a) G	(b) G1 ≡ TG1	(c) G2 = TG2
Figure 3: Construction of G0 . G1 and G2 are the modified copies of G in step 2. In step 3, the
transformation TGi happens to coincide with Gi for i = 1, 2 in this case. G0 is created in step 4 by
adding an extra vertex oG0 and connecting it to v1 and v2 with the appropriate weights and activation
and can be seen in Figure 1b.
G0 is well-defined We verify each steps in the above construction:
1.	This step is well-defined.
2.	For any Gj , as long as αG ≥ 2 (by definition), we always remove oG from each Gj in each
step; since vj is an in-vertex of oG and G is a DAG. Otherwise, this step is well-defined.
14
Published as a conference paper at ICLR 2022
3.	Fix a j ∈ [m]. By construction (and since we always remove oG from Gj in the previous
step),OGj = {vj}. If there is a path p* with length at least t in Gj, then since Gj [E] ⊆
G[E] and og ∈ G[V] \ Gj [V], the path p* ∪ {og} created by appending og to p* is a
valid path with length t + 1. This violates the assumption that αG = t and we conclude,
by contradiction, that αGj ≤ t - 1. This justifies the invocation of inductive hypothesis for
νj to get a tree neural net τj .
4.	The final step is well-defined.
ν0 computes the same function as ν Recall that ν0 is the neural network with graph G0 . We have
for any input x ∈ Rd
ν0(x) = (G0[POST](oG0))(x)	(18)
=G0[σ](θGo) (t G0[w](vjθGo) ∙ (G0[POST](vj))(x)j	(19)
=G[σ](θG) (t G[w](vj og) ∙ (Tj [POST](vj∙ ))(x)j	(20)
=G[σ](θG) (XG[w](vjog) ∙ (G[POST](vj-))(x)j = V(x)	(21)
where we invoked the inductive hypothesis in the last line to get
(Tj [POST](vj ))(x) = τj (x) = νj (x) = (G[POST](vj ))(x),	(22)
and the rest are definitions.
G0 is isomorphic to TG Although this should be straightforward from the construction, we give a
formal proof. Consider the isomorphism π : G0 [V] → TG[V] given as
π(v)
oTG = {oG} ∈ HG
p ∪ {oG}
if v ≡ oG0
if Tj [V] 3 v ≡ p ∈ HGj for some j ∈ [m],
(23)
where paths are viewed as set of vertices. The second case is well-defined since p ∪ {oG} ∈ HG for
anyp ∈ HGj for any j ∈ [m] by construction of Gj.
We can now verify that π is an isomorphism between the two structures. Fix pq ∈ G0 [E]. Consider
two separate cases: pq ∈ Tj [E] for some j and pq 6∈ Tj [E] for any j. In the first case, by definition
of Tj as a Hasse diagram, p = {vp} ∪ q are paths in HGj . Thus, π(p)π(q) = (p ∪ {oG})(q ∪
{oG}) satisfying π(p) = {vp} ∪ π(q). Thus, by definition of Hasse diagram, π(p)π(q) ∈ TG[E].
Furthermore, G0 [w](pq) = G[w](ep) = TG[w](π(p)π(q)) In the second case, we have pq = vjoG
for some j ∈ [m]. Thus, π(p)π(q) = ({vj , oG})({oG}) ∈ TG [E] also by definition of Hasse
diagram. At the same time, G0[w](pq) = G[w](vj oG) = TG[w](π(p)π(q)) by definition.
Fix v ∈ G0[V]. If v ≡ oG0 then G0[σ](v) = G[σ](oG). At the same time, TG [σ](π(v)) =
G[σ](v{oG} = G[σ](oG) = G0[σ](v). If v 6≡ oG0 then there is a j ∈ [m] and a p ∈ HGj such
that v = p ∈ Tj [V]. Then by definition,
G0[σ](v) = Tj [σ](v) = G[σ](vp) = G[σ](vp ∪ {oG0}) = TG[σ](π(v)).	(24)
This completes the inductive proof that shows τG = ν0 = ν for G with αG = t. By mathematical
induction, the claim holds for all neural network V.	□
Lemma 7 (Pull-back of numerical weights). Fix an arbitrary feedforward neural network ν : Rd →
R with computation graph G. Let v ∈ G[V] \ (IG ∪ OG) be an inner vertex of G with k in-edges
e1 , . . . , ek ∈ G[E] and a single out-edge f ∈ G[E]. Then the network V0 with computation graph
G0 = (G[V], G[E], G0[w]) defined as
fG[w](e)|G[w](f )|
G0 [w] : e 7→	sgn(G[w](f))
IGiw]®
if e ≡ ej for some j ∈ [k]
if e ≡ f
otherwise.
(25)
15
Published as a conference paper at ICLR 2022
computes the same function as ν. In other words, we can pull the numerical values of G[w](f)
through v, into its in-edges; leaving behind only its sign.
When fixing input x to G, one can extend this operation to ij ∈ IG forj ∈ [d]. By setting G0[w](f) =
sgn(G[w](f)) and update xj to xj |G[w](f)|.
Proof. Let the single out-vertex of v be b. If suffices to show that G[PRE](b) = G0 [PRE](b). Fix
an input x to ν. Let the k in-edges of v be a1, . . . , ak. Since we only change edges incident to v,
G[POST]aj = G0 [POST]aj for allj ∈ [k]. We have:
G0[PRE](b) = sgn(G[w](vb)) ∙ (G[σ](v))(G0[PRE](v))	(26)
=sgn(G[w](vb)) ∙ (G[σ](v)) (^X ∣G[w](vb)∣G[w](aj∙V) ∙ G[POST](aj∙) j	(27)
=sgn(G[w](vb))∣G[w](vb)∣ ∙ (G[σ](v)) (^XG[w](aj-V) ∙ G[POST](aj∙) j	(28)
=G[w](vb) ∙ (G[σ](v)) (XG[w](ajV) ∙ G[POST](aj)j = G[PRE](b),	(29)
where equation 28 is due to positive homogeneity of。竺 and the rest are just definitions.	□
We can now give the proof of Lemma 5.
Proof of Lemma 5. Given an arbitrary feedforward neural network ν with computation graph
G = (G[V], G[E], G[w]), we use Lemma 6 and get the unrolled tree network TG =
(TG[V],TG[E],TG[w]) such that ν = τG.
Let π = π1, π2, . . . , πξ be an ordering of TG[V] \ OTG (so ξ = |H| - 1) by a breadth first search
on TG starting from oTG. In other words, if dtopo(u, oTG) > dtopo(V, oTG) then u appears after V in
π, where dtopo(a, b) is the number of edges on the unique path from a to b for some a, b ∈ TG[V].
Iteratively apply Lemma 7 to vertex π1 , . . . , πξ in TG while maintaining the same function. After
ξ such applications, We arrive at a network μG with graph MG such that μG = TG = V. Recall
that the pull-back operation of Lemma 7 only changes the tree weights. Furthermore, the π ordering
is chosen so that subsequent weight pull-back does not affect edges closer to oTG . Therefore, at
iteration j ,
1.	MG[w](πkq) = sgn(G[w](eπk)) for all k ≤ j, for some q ∈ MG[V] such that (πk)q ∈
MG[E].
2.	if πj 6∈ IMG then MG[w](r(πj)) = G[w](er) Qf ∈πj |G[w](f)|, for some r ∈ MG[V]
such that r(πj) ∈ MG [E]; otherwise, πj is an input vertex corresponding to input xπj ,
then xπj is modified to xπj Qf∈π |G[w](f)| = h0G(xπj ) where h0G is the absolute-valued
path enumeration function.
This completes the proof.	□
Now we present the extension to Lemma 1:
ProofofLemma 1. Invoke Lemma 5 to get a tree network μ0 such that V = μ0 ◦ h0. Then one
can subdivide each input edges (edges that are incident to some input neuron ij) into two edges
connected by a neuron with linear activation. One of the resulting egde takes the weight of the old
input edge; and the other is used to remove the absolute value in the definition of the (basic) path
enumeration function.
More formally, for all P ∈ P, recall that ip is a particular input neuron of μ0 in the decomposition
lemma (Lemma 1). Since μ0 is a tree neural network, we there exists a distinct node Up in μ0 that is
16
Published as a conference paper at ICLR 2022
adjacent to ip . Remove the edge ipup , add a neuron u0p , connect ipu0p and u0pup , where the weight
of the former neuron is set to Sgn (Qe∈p We) and the latter to w[μ0](ipUp). It is straightforward to
see that with μ constructed from above, V = μ ◦ h where h is the path enumeration function. □
With slight modifications to the proof technique, one can show all the results for Theorem 1, The-
orem 2 and Corollary 1 to the same matrix representation as presented in the paper but with the
absolute signs around them.
C	Proof of Training Invariances
Claim 1 (Directional convergence to non-vanishing point implies stable sign). Assumption 2 implies
Assumption 1.
Proof. Let V be the direction that1赢色 converges to. Let O be the orthant that V lies in. Since V
does not have a 0 entry, the ball B with radius mini |vi|/2 and center v is a subset of the interior of
O. Since 口/孔 converges to v, there exists a time T such that for all s > T, 口:禺e ∈ B. Thus,
eventually, w(s) ∈ B where its signs stay constant.	□
Claim 2 (Directional convergence does not imply stable activation). There exists a function w : R →
Rd such that w(t) converges in direction to some vector V but for all u ∈ w(R), w-1 (u) has infinitely
many elements. This means that for some ReLU network empirical risk function R(w) whose set
of nondifferentiable points D has nonempty intersection with w(R), the trajectory w(t) t≥0 can
cross a boundary from one activation pattern to another an infinite number of times.
Proof. Fix a vector V ∈ Rd. Consider the function t → v|t sin(t)∣.	□
Lemma 8 (Clarke partial subderivatives of inputs with the same activation pattern is the same).
Assume stable sign regime (Assumption 1). Let p1, p2 ∈ P be paths of the same length L. Let
p1 = {V1, . . . , VL} ∈ V L and p2 = {u1, . . . , uL} ∈ V L. Assume that for each i ∈ [L], we have Vi
and ui having the same activation pattern for each input training example, where the activation ofa
neuron is 0 if it is ReLU activated and has negative preactivation; and is 1 if it is linearly activated
or has nonnegative preactivation. Then dp、μ(h) = ∂p2μ(h) where ∂ is the Clarke subdifferential.
Proof. In this proof, we use the absolute-valued version of the decomposition lemma. Fix a training
example j and some weight w0 and let the output of the path enumeration function be h := (hp =
h(xj; w0))p∈P. Denote X ⊆ R2 the input space of all possible pairs of values of (p1,p2) such that
Assumption 1 and the extra assumption that both paths have the same activation pattern on each
neuron hold. Let m : R2 → R be the same function as the tree network μ but with all but the two
inputs at p1 , p2 frozen. We will show that m is symmetric in its input. Once this is establish, it is
trivial to invoke the definition of the Clarke subdifferential to obtain the conclusion of the lemma.
Let (a, b) ∈ X ⊆ R2. We thus show that if (b, a) ∈ X then m(a, b) = m(b, a). Recall that μ is itself
a ReLU-activated (in places where the corresponding original neurons are ReLU-activated) neural
network. The fact that μ has a tree architecture means that for each input node, there is a unique
path going to the output node. Thus, the set of paths from some input node in μ to its output node
can be identified with the set of inputs itself: P . Now let us considered the product of weights on
some arbitrary path p. It is not hard to see that for each such path, the product is just Pp = Qe∈p we
since the input to p is |P| and going along the path collects all signs ofwe for all e ∈ p.
We now invoke the 0-1 form of ReLU neural network to get m(a,b) = μ(h) =
Pp∈P Zp(a, b)Pp(a, b) where Zp is 1 iff all neurons on path P is active in the μ network (recall that
we identify paths in μ to input nodes). Consider that what changes between m(a, b) and m(b, a):
since μ is a tree network, exchanging two inputs can only have effect on the activation pattern of
neurons along the two paths from these inputs. However, we restricted X to be the space where
activation pattern of each neuron in the two paths is identical to one another. Since both (a, b) and
(b, a) is in X, swapping one for another does not affect the activation pattern of each neuron in the
two paths! These neurons activation pattern is then identical to those in network μ by construction
17
Published as a conference paper at ICLR 2022
and hence Zp(a, b) = Zp(b, a) for all P ∈ P since activation pattern of each node of the μ network
stays the same. Thus We conclude that m(a, b) = m(b, a). This completes the proof.	□
Proof of Lemma 2. This proof uses the absolute-value-free version of the decomposition lemma.
This is just to declutter notations, as the same conclusion can also be reach using the other version,
with some keeping track of weight signs. Recall that our real weight vector w(t) is an arc Ji &
Telgarsky (2020) which, by definition, is absolutely continuous. It then follows from real analysis
that its component we(t) is also absolutely continuous for any e ∈ E. For absolutely continuous
functions we(t) for some e ∈ E, invoke the Fundamental Theorem of Calculus (FTC) (Chapter 6,
(Heil, 2019)), to get:
X Wuv⑴-X Wuv(O) =2 X w	Wuv(S)dwuv(S)ds	(3O)
u∈INv	u∈INv	u∈INv [0,t]
We now proceed to compute dt(s). Since w(t) is absolutely continuous and we are taking the
integral via FTC, we only need to compute dW(s) for a.e. time s. By chain rule (see for example,
the first line of the proof of Lemma C.8 in Lyu & Li (2020)), there exists functions (gj)jn=1 such that
gj ∈ ∂νxj (W) ⊆ R|E| for all j ∈ [n] where νxj (W) = ν(xj; W), and for a.e. time S ≥ 0,
dW	1 n
币(S) = n Xl (y V (Xj ；W(S))) ∙ yj ∙ gj	(31)
j=1
Fix j ∈ [n], by the inclusion chain rule, since Vxj = μ ◦ h^, with h and μ also locally Lipschitz,
we have by Theorem I.1 of Lyu & Li (2020),
∂νxj (w) = ∂(μ ◦ hxj)(w) ⊆ CONV < X[α]pβj,p | α ∈ ∂μ(h(w)),βj,p ∈ ∂[hxjp(w) ｝ . (32)
I P∈ρ
Thus there exists (Ya)A=I ≥ O, PA=I Ya = 1 and (αa ∈ ∂μ(h(w)),β% ∈ ∂[hxjp(w))A=ι such
that:
A
gj =XYaX[αa]pβja,p.
a=1	p∈ρ
Here we use Assumption 1 to deduce that eventually, all weights are non-zero in gradient flow
trajectory to compute:
∂2=｛一｝
Plug this back into gj to get:
gj = I EYa E [α]p ∙ (xj )p	∏ wf I .
∖a=1	p∈ρ∣e∈p	f∈p,f=e	e e∈E
Plug this back into 笔(s) and to get, coordinate-wise, for a.e. S ≥ O,
dW	1 n	A
而(S) = n X l	(yiV (xi； W(S)))	∙	yi	∙ X Ya	X	[αa]p ∙	(xj )p	Y	wf(S).	(33)
j=1	a=1	p∈ρ∣e∈p	f ∈P,f=e
Multiply both sides with We gives:
dW	1 n
We(S)市(S)= X n X dj,P(W(S)),	(34)
p∈P∣e∈p	j = 1
18
Published as a conference paper at ICLR 2022
where
A
dj,p(W) = '0(yiV(Xi；W)) ∙ yi ∙ EYa[α]p ∙ (Xj)p ∙ ɪɪ Iwf(S)|.	(35)
a=1	f∈p
Note that d does not depend on the specific edge e used in Equation 33 and also that the term given
by β does not depend on a and we can simply write αp for PaA=1 γa [α]pa .
Plugging back into the FTC to get:
Xwu2v(t)- X wu2v(0) u∈INv	u∈INv	1n = 2E/(M	Σ	n£dj,p(w(s))ds	(36) 1n = 2/[ot]	∑ n^dj,p(w(s))ds.	(37)
Finally, by an identical argument but applied to the set of edges vb for some b ∈ OUTv , we have:
1n
E wVb(t) - E wVb(0) = 2 E nfdj,p(w(s%ds	(38)
b∈OUTv	b∈OUTv	J[0,t] p∈P∣v∈p j = 1
= XWu2v(t)- X Wu2v(0),	(39)
u∈INv	u∈INv
which completes the proof of the first part of the lemma.
For the second part, recall that we have 2 vertices u, v such that INv = INu = IN and OUTv =
OUTu = OUT with stable activation pattern for each training example. To make it more readable, we
drop the explicit dependence on t in our notation and introduce some new ones: for some a ∈ IN, let
PI→a be the set of all paths from some input node in I to node a and for some b ∈ OUT, let Pb→o
be the set of all paths from b to the output node o. Then one can decompose the sum as
dtwau = £-'0(yV(Xj； W)) ∙ y ∙	E EE (Xj )P1 ∙ wub ∙	∏ wf ∙ ɑPl∪{u}∪P2 ,
j=1	p1 ∈PI→a b∈OUT p2∈Pb→o	f ∈p1 ∪p2
(40)
where αp is the partial Clarke subdifferential at input p.
Recall that the end goal is to derive
"77 wauwav = wau ~J7wav + wav _77wau.	(41)
dt	dt	dt
Using equation equation 40, the second term on the right hand side becomes
dn
wav dtwau = E-'0(yν (Xj; w)) ∙ y ∙ EEE
j=1	p1∈PI
→a b∈OUT p2∈Pb→o
(42)
(Xj)pι ∙ wav ∙ ɪɪ wf ∙ wub ∙ αp1∪{u}∪p2 (Xj; w)	(43)
f ∈p1 ∪p2
19
Published as a conference paper at ICLR 2022
where the product wav
tion to get
(Qf ∈pι∪p2 ∙Wf) Wub is ajagged path. Then one continues with the deriva-
ddt(XWauWav
a∈IN
X dt (WauWav )
a∈IN
(44)
n
X -'0(yν(Xj； W)) ∙ y ∙ X X X X
(45)
j=1
a∈IN b∈OUT p1∈PI→a p2∈Pb→o
(Xj )pι ∙ Y Wf	(Wav ∙ Wub ∙ αp1∪{u}∪p2 (xj ； w) + Wau ∙ Wvb ∙ apι∪{v}∪p? (Xj； W))
f ∈p1 ∪p2
(46)
On the other hand
dt ( X WubWvb
b∈OUT
X dt (WubWvb)
b∈OUT
(47)
n
X -'0(yν(Xj; W)) ∙ y ∙ XX X X
(48)
j=1
a∈IN b∈OUT p1∈PI→a p2∈Pb→o
(Xj )pι	∙ Y	Wf	(Wav	∙ Wub ∙ αp1∪{v}∪p2 (xj,w)+ Wau	∙	Wvb	∙	αpι∪{u}∪p? (xj ； W)).
f ∈p1 ∪p2
(49)
This is where the more restrictive assumption that for each training example, u and v have the
same activation pattern as each other (but the same activation pattern between u and v of, say
X1 , may differs from that on, say X2). Under this assumption, we can invoke Lemma 8 to get
αp1∪{u}∪p2 (Xj; W) = αp1 ∪{v}∪p2 (Xj; W) as a set. This identifies 46 with 49 and gives us:
d I X Wau(t)Wav (t) - X Wub(t)wvb(t) I =0.
a∈IN	b∈OUT
(50)
This holds for any time t where u and v has the same activation pattern. If further, they have the
same activation pattern throughout the training phase being considered, then one can invoke FTC to
get the second conclusion of Lemma 2. Note, however, that we will be using this differential version
in the proof of Theorem 1.
□
Proof of Lemma 3. The proof is identical to that of Lemma 2, with the only difference being the set
A that we double count. Here, set A to be P . Then by the definition of layer (see Definition 7), one
can double count P by counting paths that goes through any element of a particular layer. The proof
completes by considering (using the same notation as the proof of Lemma 2) for a.e. time s ≥ 0,
dW	1 n
X We(S) -d^ (S)= X n X dj,p(W(Sy) ∙ (Xj )p ∙ Y Wf(S),	(51)
e∈F	p∈P j=1	f∈p
for any layer F.	□
Before continuing with the proof of Theorem 1, we state a classification of neurons in a convolutional
layer. Recall that all convolutional layers in this paper is linear. Due to massive weight sharing within
a convolutional layer, there are a lot more neurons and edges than the number of free parameters in
this layer. Here, we formalize some concepts:
20
Published as a conference paper at ICLR 2022
Figure 4: Jagged path. Here the straight arrow denote a single edge in the graph while the snaked
arrow denote a path with possibly more than one edge. u and v are nodes in the statement of the
second part of Lemma 2, a ∈ IN = INv = INu , b ∈ OUT = OUTv = OUTu .
Definition 11 (Convolutional tensors). Free parameters in a (bias-free) convolutional layer can be
organized into a tensor T ∈ Ra×b×c where a is the number of input channels, b is the size of each
2D filter and c is the number of output channels.
For example, in the first convolutional layer of AlexNet, the input is an image with 3 image channels
(red, blue and green), so a = 3 in this layer; filters are of size 11 × 11 so b = 121 is the size of
each 2D filter; and finally, there are 96 output channels, so c = 96. Note that changing the stride
and padding does not affect the number of free parameters, but does change the number of neurons
in the computation graph of this layer. Thus, we need the following definition:
Definition 12 (Convolutional neuron organization). Fix a convolutional layer with free parame-
ters T ∈ Rc×b×a. Then neurons in this layer can be organized into a two dimensional array
{vj,k }j∈[l],k∈[c] for some l ∈ N such that
1.	For all j, j0 ∈ [l] and for all k ∈ [c], vj,k and vj0 ,k share all its input weights. More
formally, there exists a bijection φ : INvj,k → INvj0,k such that:
wuvj,k ≡ wφ(u)vj0,k
(52)
for all u ∈ INvj .
2.	For all j ∈ [l] and for all k, k0 ∈ [c], vj,k and vj,k0 has the same in-vertices and out-vertices.
In other words,
INvjk = INvj,k0 =: INvj and OUTvjk = O U Tvj,k0 =: OUTvj .
(53)
For example, in the first convolutional of AlexNet in R96×121×3, the input image dimension is
224 × 224 × 3 pixels and the stride of the 11 × 11 × 3 filters in the layer is 4, with no padding.
Thus a single 2D filter traverse the image (224-(11-4)) =： ι times, each corresponds to a different
neuron in the layer. This process is then repeated c times for each output channel, for a total of c × l
neurons in the convolutional layer.
Lemma 9 (Extension of Lemma 2 to weight sharing). Let V := {vj,k}j∈[l],k∈{1,2} be a convolu-
tional neuron organization. Recall that by definition 12, for all j ∈ [l], INvj,1 = INvj,2 =: INj and
= OUTvj,2 =: OUTj. We have for a.e. time t ≥ 0,
X	|w(t)w0(t)| - |w(0)w0(0)| = X	|z(t)z0(t)| - |z(0)z0(0)|,	(54)
(w,w0)∈wIN (V)	(z,z0)∈wOUT(V)
OUTvj,1
where
wIN(V) := {(w1,w2) | ∀j ∈ [l], ∃aj ∈ INj, wajvj,1 ≡ w1 andwajvj,2 ≡ w2},
and similarly,
wOUT(V) := {(w1, w2) | ∀j ∈ [l], ∃bj ∈ OUTj, wvj,1bj ≡ w1 and wvj,2bj ≡ w2}.
21
Published as a conference paper at ICLR 2022
Before we start the proof, some remarks are in order. Let T1 ∈ R2×b×a be a convolutional layer and
let V = {vj,k}j∈[l],k∈[2] be its convolutional neuron organization. Then
wIN(V) = {([T1]1,j2,j1, [T1]2,j2,j1) |j1 ∈ [a], j2 ∈ [b]}.	(55)
If furthermore T2 ∈ Re×d×2 is a convolutional layer immediately after T1, then
wOUT(V) = {([T2]k1,k2,1 , [T2]k1,k2,2) | k1 ∈ [e], k2 ∈ [d]}.	(56)
If instead of T2, the subsequent layer to T1 is a fully-connected layer W ∈ Rd×(l×2) (recall that
there are 2l neurons in T1 layer), then
wOUT(V) ={(Wl1,l2×1,[T2]l1,l2×2) | l1 ∈ [d], l2 ∈ [l]}.	(57)
Proof of Lemma 9. As before, the proof is identical to that of Lemma 2 with the exception being the
set of paths that we are double counting over. For all j ∈ [l], let
Aj := {p1 ∪ p2 | p1 is a path from I to vj,1 ,p2 is a path from vj,2 to o},	(58)
and
A0j := {p1 ∪ p2 | p1 is a path from I to vj,2 ,p2 is a path from vj,1 to o}.	(59)
Let A := Sj∈[l] Aj ∪ A0j . Then by an identical argument as that of Lemma 2, we can show that
1n
w(t)w0(t) - w(0)w0(0) = LEnfdj,p(w(s))ds	(60)
(w,w0)∈wIN(V)	[0,t] p∈A n j=1
= X	z(t)z0 (t) - z(0)z0(0).	(61)
(z,z0)∈wOUT(V)
Here the notation di,j is well-defined since we do not have to specify a path for the subgradient
αp . This is because we are working with linear convolutional layers and thus all subgradients are
gradients and is evaluated to 1.
□
Proof of Theorem 1. All points in this theorem admit the same proof technique: First, form the
matrices as instructed. Let X = {v1, v2, . . . , vm} ⊆ V be the active neurons shared between the
two layers. Check the conditions of the second part of Lemma 2 and invoke the lemma for each pair
u, v ∈ X . We now apply this to each points:
1.	Let W1 ∈ Rb×a be a fully-connected layer from neurons in V1 to neurons in V2 and W2 ∈
Rc×b be a fully-connected layer from V2 to V3 . Then we have INu = V and OUTu = W
for all u ∈ U . Furthermore, all weights around any u are learnable for all u in U. Invoke
the second part of Lemma 2 to get the conclusion.
2.	let T1 ∈ Rc×b×a and T2 ∈ Re×d×c be the convolutional tensors with convolutional neuron
organization of T1 (Definition 12) being {vj,k}j∈[l1],k∈[c]. Form the matrix representation
W1 ∈ Rc×(ab) and W2 ∈ R(de)×c as per the theorem statement. By Definition 12, for
k, k0 ∈ [c], for all j ∈ [l], INvj,k = INvj,k0 and OUTvj,k = OUTvj,k0 . Invoke Lemma 9 to get
the conclusion.
3.	Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNet-
Free. In all variants, skip connection affects neither the edges in U ∈ Rb×a and Y ∈ Rc×b .
Let Y be fully-connected from neurons V1 to neurons V2 and U be fully-connected from
neurons V2 to neurons V3. Then for each u ∈ V2 , INu = V1 and OUTu = V3. Furthermore,
all weights around any vertices in V2 are learnable. Invoke the second part of Lemma 2 to
get the conclusion.
22
Published as a conference paper at ICLR 2022
4.	Let ri(x; Ui, Yi, Zi), i ∈ {1, 2} be consecutive ResNetFree block. Let Y1 be fully con-
nected from neurons V1 to neurons V2, U1 be fully-connected from V2 to neurons V3, Y2 be
fully-connected from neurons V3 to V4 and U2 be fully-connected from V4 to neurons V5 .
Y2
Then U1 Z1 is fully-connected from V1 ∪ V2 to V3 and Z2 is fully-connected from
V3 to V4 ∪ V5 . Invoke the first point to get the conclusion.
5.	Let the convolutional tensor be T ∈ Rc×b×a with convolutional neuron organization
{vj,k}j∈[l],k∈[c] for some l ∈ N. Let the adjacent fully-connected layer be W ∈ Rd×(l×c).
Form the matrix representation W1 ∈ Rc×ab and W2 ∈ Rdl×c as per the theorem state-
ment. Then we have for any k, k0 ∈ [c] and for all j ∈ [l], INvj,k = INvj,k0 =: INj and
OUTvj,k = OUTvj,k0 =: OUTj. Invoke Lemma 9 to get the conclusion.
6.	Let r(x; U, Y, Z) be a residual block of either ResNetIdentity, ResNetDiagonal or ResNet-
Free. Let Y be fully-connected from neurons V1 to neurons V2 , U be fully-connected from
neurons V2 to neurons V3. Thus, Z is fully-connected from V1 to V3 . Then W2 = U Z
is fully connected from V1 to V2 ∪ V3 .We invoke the fifth point to get the conclusion.
7.
first consider the case where the ResNetFree block is followed by the fully-connected layer.
Let r(x; U, Y, Z) be the first ResNetFree block with input neurons where Y fully-connects
neurons V1 to V2 and U fully connects V2 to V3 . Then we have U Z fully-connects
V1 ∪V2 to V3. If the subsequent layer is a fully-connected layer then invoke the first point to
get the conclusion; otherwise if the subsequent layer is a ResNetFree block r(x; U0, Y0, Z0)
Y
with Y 0 fully-connects V3 to V4 and U0 fully-connects V4 to V5. Then Z fully-connects
V3 to V4 ∪ V5 and we one again invoke the first point to get the conclusion.
□
Proof of Lemma 4. This is the continuation of the proof of Lemma 2. To obtain noninvariance, one
only needs to show that when u is active and v inactive, the expression in 46 and 49 are not equal in
general. For the sake of notation, we pick the case where the preactivation of u is strictly positive,
that ofv is strictly negative, and further assume that the whole network is differentiable at the current
weights w for all training examples.
In this case, it is not hard to see that the Clarke subdifferential ∂wμ(h) is a singleton and contains
the gradient of the μ network. Furthermore, for any path P = (vι,..., vl), the partial derivative ∣p
is 1 if all neurons on p are active and 0 otherwise. Thus, we have
(62)
∑-'0(yν(χj; W)) ∙ y W E E	E ((Xj)p「	∏ Wf I wav ∙ Wub∙
j=1	a∈IN b∈OUT active p1 ∈PI→a activep2∈Pb→o	f ∈p1 ∪p2
(63)
We can actually factorize this even further by noticing that the term Wav does not depend on b and
Wub does not depend on a. Rearranging the sum and factorizes give:
ddt I X Wauwj
a∈IN
n
f-'0(yν(xj w)) ∙ y ∙
j=1
E (Xj )pι ∙ ∏wf) I E ∏ Wf
active p1 ∈PI→v	f∈p1	activep2∈Pu→o f∈p2
(64)
(65)
23
Published as a conference paper at ICLR 2022
On the other hand
d
dt
wubwvb
b∈OUT
n
-'0 -'0(yν(xj; W)) ∙ y ∙
j=1
E	(Xj )pι ∙ ∏ Wf )1 E ∏ Wf
active p1 ∈PI→u	f∈p1	active p2∈Pv→o f∈p2
(66)
(67)
Take, for example, an asymmetric case where the in-edges of v has much larger weights than that of
u while out-edges of v has much smaller weights than that of u, then 65 is much larger than 67 and
therefore the two expressions are not equal in the general case. A symmetric initialization scheme
that prevents the above asymmetry may prevent this from happens, but this requires additional as-
sumptions and is opened to future work.	□
Remark 1. Using the automatic differentiation framework while setting the gradient of ReLU to be
1 if the preactivation is nonnegative while 0 otherwise, the same derivation of 65 can be achieved.
Interestingly, if one only has a single example, then the final expression of 65 implies that the matrix
ddt (W> Wk) has rank at most 2, where Wk is a weight matrix in,for example, ReLUfully-Connected
neural network. Controlling the eigenvalues under low-rank updates may allow us to bound singular
values of the full weight matrices Wk. The resulting bound would not be uniform over time, but
improves with training and is thus a different kind of low rank result. This, however, is outside of the
scope of this paper.
D Proof of Theorem 2
First we state a helper lemma
Lemma 10 (Largest singular value of different flattening of the same tensor is close). LetT ∈ Ra,b,c
be an order 3 tensor (say a convolutional weight tensor). Let T1 , T2 be the standard flattening of
this tensor into an element in Rc×(a×b) and R(b×c)×a respectively. Then,
m⅛y kTik2 ≤ kT2k2.	(68)
Proof. Invoke Theorem 4.8 of Wang et al. (2017) and using the same notation in the same paper, we
have for π1 = {{c}, {a, b}} and π2 = {{b, c}, {a}},
dm『kT1k2 ≤kT2k2.
(69)
All that is left is to compute the left hand side in term ofa,b, c. By definition, dim(T) =abc and
dimT (π1, π2) = dimT ({{c}, {a, b}}, {{b, c}, {a}}) =[max (Dτ({c},{b,c}),Dτ({c},{α})] • [max (DT({a, b}, {b, c}), DT({a, b}, {a}))] =[max(c, 0)] ∙ [max(α, b)] = C max(a, b).	(70) (71) (72) (73)
Plug this in equation 69 to get the final result.
□
Lemma 11 (Shuffling layer in linear ResNetFree block preserves largest singular value up to multi-
ple by 8). Recall that for a ResNetFree block r(U, Y, Z) with Y ∈ Rb×a, U ∈ Rc×b and Z ∈ Rc×a,
Y
there are two possible rearrangement of the weights A = U Z and B = Z . We have
kBk2 ≥ 1 kAk2 — D0 where D0 ≥ 0 is fixed at inialization.
24
Published as a conference paper at ICLR 2022
Proof. Recall that by point three of Theorem 1, we have matrix invariance
U>(t)U(t) - Y (t)Y >(t) =U>(0)U(0) - Y (0)Y >(0).
Note that we can obtain this form since we are only considering linear ResNetFree blocks, so all
neurons are active at all time for all training examples. Thus, we can invoke a FTC to get this form
from the differential from in theorem 1.
By line B.2 in Ji & Telgarsky (2019),
kY k22 ≥ kUk22-D,	(74)
where D = kU>(0)U(0) - Y (0)Y >(0)k22 is fixed at initialization.
For positive semidefinite matrix X, denote λ(X) to be the function that returns the maximum eigen-
value of X . We have
kBk22 = (λ(B>B))2
Y + Z>Z
2
(75)
(76)
(77)
(78)
(79)
(80)
(81)
(82)
where 77 is by an application of Weyl’s inequality for Hermitian matrices which states that λ(C +
D) ≥ λ(C) + t where t is is the smallest eigenvalue of D, which is nonnegative since matrices here
are all positive semidefinite; 79 is a consequence of 74 and 81 is the application of the inequality
λ(C + D) ≤ λ(C) + λ(D) which is another of Weyl's inequality for Hermitian matrices. □
Proof of Theorem 2. Fix j ∈ [K + M], we first invoke Lemma 3 to bound the Frobenius norm of
each of the final K + M + 1 layers (counting the last layer Fin) via the last layer. Let Wj be the
matrix representation of the j -th layer among the last M + 1 layer as described in Theorem 1. Note
that even if a layer has more than one matrix representation in Theorem 1, their Frobenius norm is
still the same because different representation merely re-organize the weights. Thus, we can pick
an arbitrary representation in this step. However, the same is not true for the operator norm and we
have to be a lot more careful in the next step. For each j ∈ [K + M], we have
kWj(t)k2F - kF in(t)k2F =D0,	(83)
where D0 = kWj (0)k2F - kF in(0)k2F fixed at initialization.
Now, we bound the difference between the operator norm of Wj (t) and F in(t) by a telescoping
argument. By Lemma 11, switching from one matrix representation to the other for ResNet incurs
at most a multiplicative factor of 8 and an additive factor cost that depends only on the initialization.
In each adjacent layer, the maximum number of switch between representation is one (so that it fits
the form prescribed in Theorem 1). By matrix invariance between adjacent layers of the K + M
pairs of adjacent layers,
kWl(t)k22 ≥CkWl+1(t)k22-Dl,	(84)
25
Published as a conference paper at ICLR 2022
for C = 1/8 if the k + 1 layer is a ResNetFree block (Lemma 11), C = 1/ min(ak+1, bk+1) if the
k + 1 layer is a convolutional layer, C = max dim W [M+1] if k = M (Lemma 10) and C = 1
otherwise; for Dl = kWl>+1(0)Wl+1(0) - Wl(0)Wl>(0)k22.
Telescope the sum and subtract from 83 to get the statement of the theorem. When Frobenius norm
diverges, divide both sides by the Frobenius norm to get the ratio statement. Note that the bound
kW k2F /rank(W) ≤ kW k22 is trivial since the largest singular value squared is at least the average
singular value squared.
When the lower bound of 14 is 1, it matches the upper bound and thus the largest singular value
dominates all other singular values. Alignment follows from the proof of Lemma 2.6 second point
in Ji & Telgarsky (2019).	□
E	Proof of Corollary 1
Proof. Let L be the number of layers in the network. Under the conditions stated in Corollary 1,
Lyu & Li (2020) and Ji & Telgarsky (2020) showed that kw(t)k2 diverges. Invoke Lemma 3 for all
k ∈ [L - 1] and sum up the results, we have LkWL(t)k2F = D00 + PjL=1 kWj (t)k2F kWk k2F =
D00 + kw(t)k22 where D00 is constant in t. Thus kWj (t)k2F diverges for all j ∈ [L]. Since the sum of
all but the largest singular value is bounded, but the sum of all singular values diverge, we conclude
that the largest singular value eventually dominates the remaining singular values. Together with
convergence in direction for these architecture Ji & Telgarsky (2020), we have each matrix converges
to its rank-1 approximation.
That all the feedforward neural networks with ReLU/leaky ReLU/linear activations can be definable
in the same o-minimal structure that contains the exponential function follows from the work of Ji
& Telgarsky (2020) and that definability is closed under function composition.	□
26