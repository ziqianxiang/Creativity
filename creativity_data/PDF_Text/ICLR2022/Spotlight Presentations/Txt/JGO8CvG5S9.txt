Published as a conference paper at ICLR 2022
Universal Approximation Under Constraints
is Possible with Transformers
Anastasis Kratsios； Tianlin Liu & Ivan Dokmanic
Universitat Basel,
Departement Mathematik und Informatik
{firstname.lastname}@unibas.ch
Behnoosh Zamanlooy；
Universitat Zurich,
Department of Informatics
bzamanlooy@ifi.uzh.ch
Abstract
Many practical problems need the output of a machine learning model to satisfy
a set of constraints, K. There are, however, no known guarantees that classical
neural networks can exactly encode constraints while simultaneously achieving
universality. We provide a quantitative constrained universal approximation theo-
rem which guarantees that for any convex or non-convex compact set K and any
continuous function f : Rn → K, there is a probabilistic transformer 户 whose
randomized outputs all lie in K and whose expected output uniformly approxi-
mates f. Our second main result is a “deep neural version” of Berge (1963)’s
Maximum Theorem. The result guarantees that given an objective function L,
a constraint set K, and a family of soft constraint sets, there is a probabilistic
transformer F that approximately minimizes L and whose outputs belong to K;
moreover, F approximately satisfies the soft constraints. Our results imply the
first universal approximation theorem for classical transformers with exact convex
constraint satisfaction, and a chart-free universal approximation theorem for Rie-
mannian manifold-valued functions subject to geodesically-convex constraints.
Keywords: Constrained Universal Approximation, Probabilistic Attention, Transformer Networks,
Geometric Deep Learning, Measurable Maximum Theorem, Non-Affine Random Projections.
1	Introduction
In supervised learning, we select a parameterized model f : Rn → Rm by optimizing a real-valued
loss function* 1 L over training data from an input-output domain X×Y⊆Rn × Rm. A necessary
property for a model class to produce asymptotically optimal results, for any continuous loss L,
is the universal approximation property. However, often more structure (beyond vectorial Rm) is
present in a learning problem and this structure must be encoded into the trained model f to obtain
meaningful or feasible predictions. This additional structure is typically described by a constraint
set K ⊆ Rm and the condition f(X) ⊆ K. For example, in classification K = {y ∈ [0,1]m :
Pm=1 yi =1} (Shalev-Shwartz & Ben-David, 2014), in Stackelberg games (Holters et al., 2018;
Jin et al., 2020; Li et al., 2021) K is the set of utility-maximizing actions ofan opponent, in integer
programming K is the integer lattice Zm (Conforti et al., 2014), in financial risk-management K is
a set of positions meeting the minimum solvency requirements imposed by international regularity
bodies (Basel Committee on Banking Supervision, 2015; 2019; McNeil et al., 2015), in covariance
matrix prediction K ⊆ Rm×m is the set ofm × m matrices which are symmetric and positive semi-
definite (Bonnabel et al., 2013; Bonnabel & Sepulchre, 2009; Baes et al., 2021), in geometric deep
learning K is typically a manifold (e.g. a pose manifold in computer vision and robotics (Ding &
Fan, 2014) or a manifold of distance matrices (Dokmanic et al., 2015)), a graph, or an orbit of a
group action (Bronstein et al., 2017; 2021; Kratsios & Bilokopytov, 2020). Therefore, we ask:
Is exact constraint satisfaction possible with universal deep learning models?
* Corresponding authors.
1For example, in a regression problem one can set L(x, y)=kf(x) - yk for an unknown function f or in
regression problems one sets L(x, y)=Pm=1[C(x)]i log(yi) for an unknown classifier C.
1
Published as a conference paper at ICLR 2022
The answer to this question begins by examining the classical universal approximation theorems for
deep feedforward networks. If L and K are mildly regular, the universal approximation theorems of
Homik et al. (1989); Cybenko (1989); Pinkus (1999); GUhring et al. (2020); Kidger & Lyons (2020);
Park et al. (2021) guarantee that for any “good activation function σ” and for every tolerance level
e > 0, there is a deep feedforward network with activation function σ, such that infy∈κ L(x, y) and
L(x, f (x)) are uniformly at most e apart. Written in terms of the optimality set,
Ii P/ ∖	∙ τ /	∖ Ii
sup kf (x) - argmin L(x, y)k≤ e,
x∈X
y∈K
(1)
where the distance of a point y ∈ Rm to a set A ⊆ Rm is defined by ky - Ak , infa∈A ky - ak.
Since argminy∈K L(x, y) ⊆ K, then (1) only implies that kf (x) - Kk≤e and there is no reason
to believe that the constraint f(x) ∈ K is exactly satisfied, for every x ∈X.
This kind of approximate constraint satisfaction is not always appropriate. In the following examples
constraint violation causes either practical or theoretical concerns:
(i)	In post-financial crisis risk management, international regulatory bodies mandate that any
financial actor should maintain solubility proportional to the risk of their investments (Basel
Committee on Banking Supervision, 2015; 2019). To prevent future financial crises, any
violation of these risk constraints, no matter the size, incurs large and immediate fines.
(ii)	In geometric deep learning, we often need to encode complicated non-vectorial structure
present in a dataset, by viewing it as a K valued function (Fletcher, 2013; Bonnabel &
Sepulchre, 2009; Baes et al., 2021). However, if K is non-convex then Motzkin (1935)
confirms that there is no unique way to map predictions f (x) 6∈ K to a closest point in
K. Thus, we are faced with the dilemma: either make an ad-hoc choice of a k in K with
k ≈ f(x) (ex.: an arbitrary choice scheme when K = Zm) or have meaningless predictions
(ex: non-integer values to integer programs, or symmetry breaking (Weinberg, 1976)2).
Constrained learning was recognized as an effective framework for fairness and robustness by Cha-
mon & Ribeiro (2020) who study empirical risk minimization under constraints. Many emerging
topics in machine learning lead to constrained learning formulations. A case in point is model-
based domain generalization (Robey et al., 2021). Despite the importance of (deep) learning with
constraints, there are no related approximation-theoretic results to the best of our knowledge.
In this paper, we bridge this theoretical gap by showing that universal approximation with exact
constraint satisfaction is always possible for deep (probabilistic) transformer networks with a single
attention mechanism as output layer. Our contribution is three-fold:
1.
2.
3.
We derive the first universal approximation theorem with exact constraint satisfaction;
Our transformer network’s encoder and decoder adapt to the dimension of the constraint
set and thus beat the curse of dimensionality for low-dimensional constraints;
Our models leverage a probabilistic attention mechanism that can encode non-convex con-
straints. This probabilistic approach is key to bypass the topological obstructions to non-
Euclidean universal approximation (Kratsios & Papon, 2021).
Our analysis provides perspective on the empirical success of attention and adds to the recent line
of work on approximation theory for transformer networks, (Yun et al., 2020a;b), which roughly
considers the unconstrained case (with K in (1) replaced by Rm) in the special case of L(x, y)=
kf(x) -yk for a suitable target function f : Rn → Rm. Our probabilistic perspective on transformer
networks fits with the representations of Vuckovic et al. (2021) and of Kratsios (2021).
Our results can be regarded as an approximation-theoretic counterpart to the constrained statistical
learning theory of Chamon & Ribeiro (2020). Further, they put forward a perspective on random-
ness in neural networks that is complementary to the work of Louart et al. (2018); Gonon et al.
(2020a;b). We look at the same problem focusing on constraint satisfaction instead of training effi-
ciency. Finally, our proof methods are novel, and build on contemporary tools from metric geometry
(Ambrosio & Puglisi, 2020; Brue et al., 2021).
2As discussed in Rosset et al. (2021) this is problematic since respecting symmetries can often massively
reduce the computational burden of a learning task.
2
Published as a conference paper at ICLR 2022
1.1	The Probabilistic Attention Mechanism
We now give a high-level explanation of our results; the detailed formulations are in Section 2.
Introduced in (Bahdanau et al., 2015) and later used to define the transformer architecture (Vaswani
et al., 2017), in the NLP context, attention maps a matrix of queries Q, a matrix of keys K, and a
matrix of values V to the quantity Softmax(QK>)V , where the softmax function (defined below) is
applied row-wise to QK>. Just as the authors of (Petersen & Voigtlaender, 2020; Zhou, 2020) focus
on the simplified versions of practically implementable ConvNets in the study of approximation
theory of deep ConvNets (e.g. omitting pooling layers), we find it sufficient to study the following
simplified attention mechanism to obtain universal approximation results:
N
Attention (w, Y ) , SoftmaxN (w)> Y =	[SoftmaxN (w)n]Yn ,	(2)
n=1
where w ∈ RN, SoftmaxN : RN 3 w → (PNwwkew3 )N=1，and Y is an N × m matrix. The attention
mechanism (2) can be interpreted as “paying attention” to a set of particles Y1 ,...,YN ∈ Rm defined
by Y’s rows. This simplified form of attention is sufficient to demonstrate that transformer networks
can approximate a function while respecting a constraint set, K, whether convex or non-convex.
Informal Theorem 1.1 (Deep Maximum Theorem for Transformers). If K is convex and the quan-
titles defining (1) are regular then, for any e ∈ (0,1], there Is afeedforward network f, an Xe ⊂ Rn
of probability 1-e, and a matrix Y such that the transformer Attention(f (x), Y) satisfies:
(i)	Exact Constraint Satisfaction: For each x ∈ Rn, Attention(f(x), Y)∈ K,
(ii)	Universal Approximation: supχ∈χ k Attention(f (x),Y) - argmin L(x,y*)k≤ e
e	y*∈K
Informal Theorem 1.1 guarantees that simple transformer networks can minimize any loss function
while exactly satisfying the set of convex constraints. As illustrated by Figure 1 and Figure 2, K’s
convexity is critical here, since without it the transformer’s prediction may fail to lie in K. This is
because any transformer network’s output is a convex combinations of the particles Y1 ,Y2,Y3; thus,
any transformer network’s predictions must belong to these particles’ convex hull.
▲ Λ
Figure 1: Convex Constraints	Figure 2: Non-Convex Constraints
In Figures 1 and 2, Y’s columns, i.e. the particles Y1 ,Y2, and Y3, are each illustrated by a • at the
constraint set (K) vertices. The bubble around each each Yi illustrates the predicted probability,
for a given input, that f(x) is nearest to that Yi. The × is the transformer’s prediction which is,
by construction, a convex combination of the Yi weighted by the aforementioned probabilities and
therefore they lie in the K if it is convex (Figure 1) but not if K is non-convex (Figure 2).
Naturally, we arrive at the question: How can (i) and (ii) simultaneously hold when K is non-convex?
Returning to Vaswani et al. (2017) and using the introduced terminology, we note that the role of
the SoftmaxN layer is to rank the importance of the particles {Yn}N=1 when optimizing L, at any
given input: the weights [SoftmaxN (w)]n in (2) can be interpreted as charging their respective point
masses {δY }nN=1 with probabilities of being optimal for L (relative to the other particles)3. This
suggests the following probabilistic reinterpretation of attention (which we denote by p-attention):
N
P-attention(w, Y) ,	[SoftmaxN (w)]nδY .	(3)
n=1
3Following Villani (2009), δY is the Borel probability measure on Rm assigning full probability to any
Borel subset of Rm containing the particle Yn and 0 otherwise.
3
Published as a conference paper at ICLR 2022
Crudely put, P-attention(∙, Y) “pays relative attention to the particles” Yι,...,Yn ∈ Rm.
A simple computation shows that the mean prediction of our probabilistic attention mechanism,
exactly implements “classical” Attention of Vaswani et al. (2017), as defined in (2),
Attention(w,Y ) = EX 〜P-attention(w,γ )[X ],	(4)
where EX〜p-attention(w,γ)[X] denotes the (vector-valued) expectation of a random-vector X dis-
tributed according to P-attention(w, Y). Hence, (3) is no less general than (2). The advantage
of (3) is that, if each particle Yn belongs to K (even if K is non-convex) then, any sample drawn
from the probability measure P-attention(w, Y) necessarily belongs to K.
1.2 Qualitative Results: Deep Maximum Theorem
Probabilistic attention (3) yields the following non-convex generalization of Informal Theorem 1.1.
The result is a qualitative universal approximation theorem as well as a deep neural version of the
Maximum Theorem4 (Berge, 1963), which states that under mild regularity conditions, given any
well-behaved family of input dependent “soft constraint sets” {Cx }x∈Rn compatible with K, there
is a measurable function mapping each x ∈ Rn to a minimizer of L(x, y) on K ∩ Cx.
We use W1 to denote the Wasserstein-1 distance between probability measures on K . The re-
sults also give the flexibility to the user to enforce an input-dependent family of “soft constraints”
{Cx}x∈Rn which only need to hold approximately; definitions are provided in Section 1.4.
Informal Theorem 1.2 (Deep Maximum Theorem: Non-Convex Case). If the quantities defining 1
are regular, K is a compact set of “exact constraints”, and {Cx}x∈Rn a set of “soft constraints”,
then, for any approximation quality 0 < e ≤ 1, there is a deep feedforward network f and a matrix
Y satisfying:
(i)	Exact Constraint Satisfaction: For each x ∈ Rn, P-attention(f (x), Y) is supported in K,
(ii)	Universal Approximation: P(WI(P-attention(f(x),Y), argmin L(x,y?)) ≤ e) ≥ 1 - e;
y? ∈Cx ∩K
where for a probability measure P on Rm anda B ⊆ Rm we define W1(P, B) , inf b∈B W1(P,δb).
Example 1.3 (Reduction to Classical Point-to-Set Distance). In particular, when P is a point-mass
P = δy for some y ∈ Rm, then one recovers the familiar Euclidean distance to the set B via:
W1(δy,B)(d=ef) infW1(δy,δb)=infky-bk (d=ef) ky - Bk;
b∈B	b∈B
where the first and second equality follows from (Villani, 2009, (5) - page 99), and the last equality
is the definition of ky - B k (as in (Aubin & Frankowska, 2009, Definition 1.1.1)).
Another important class of non-convex constraints arising from geometric deep learning where K is
a non-Euclidean ball in a Riemannian submanifold of Rm . In this broad case, we may extract mean
predictions from P-attention(f, Y), by applying the Frechet mean introduced in Frechet (1948).
Such “geometric means” are well-understood theoretically (Bhattacharya & Patrangenaru, 2003)
and easily handled numerically Miolane et al. (2020); Lou et al. (2020).
1.3	Quantitative Results: Constrained Universal Approximation Theorem
In its current form, the objective function L is too general to derive quantitative approximation
rates5. Nevertheless, as with most universal approximation theorems (Hornik et al., 1989; Pinkus,
1999; Kidger & Lyons, 2020), if each soft constraint Cx is set to Rm and L quantifies the uniform
distance to an unknown continuous function f : Rn → K in the Euclidean sense,
L(x, y) , kf (x) -yk,
then, Informal Theorem 1.2 reduces to a (qualitative) universal approximation for transformer net-
works with exact constraint satisfaction. In fact, this additional structure is enough for us to derive
quantitative versions of the aforementioned results. We permit ourselves the general situation, where
4More precisely, our result is a deep neural version of the measure-theoretic counterpart to Berge’s Maxi-
mum Theorem; see (Aliprantis & Border, 2006, (Measurable Maximum Theorem) - Theorem 18.19).
5For instance, L can describe anything from a regression, to a clustering problem.
4
Published as a conference paper at ICLR 2022
K is contained in an unknown d-dimensional submanifold (where d ∈ Θ(mS) for some S > 0).
Our approximation rates scale favourably in the ratio S ≈ Ioogm); i.e., We avoid the curse of dimen-
sionality for low-dimensional constraint sets. This additional structure translates into the familiar
encoder-decoder structure deployed in most transformer network implementations.
Figure 3: Encoder : ≈ f	Figure 4: Decoder : ≈ Random Projection to K
TΓ-<∙	C .[F	,	,	,1	Y	1	/»	Π^>T)	CE	1	1	C	Z 1	∙	1∖
Figure 3 illustrates the encoder network E : Rn → Rm , whose role is to perform a (classical)
unconstrained approximation of the target function, f . Since E is a classical feedforward network
then its approximation of the target function can be arbitrarily close to the constraint set K but it
need not lie in it. The next step is to “map the encoder network’s output onto K with low distortion.”
The role of the decoder network D is to correct any constraint violation made by encoder network by
“projecting them back on to K”. However, such a projection does not exist if K is not convex since
there must be more than one closest point in K to some y ∈ Rm (Motzkin, 1935). Nevertheless,
if the “projection” were capable of mapping any y ∈ Rm to multiple points on K, ranked by their
proximity to y, then there would be no trouble. The decoder network accomplishes precisely this, as
illustrated in Figure 4, where the bubbles illustrate the probability of any particle in K being closest
to y, illustrated by the size of the bubbles in Figure 4. Mathematically,6 D : Rm → Pι(K) approx-
imates a (non-affine) random projection, in the sense of Ohta (2009); Ambrosio & Puglisi (2020);
BrUe et al. (2021); i.e.: a 1-Lipschitz map Π : Rm → Pι(K) satisfying the random projection
property: for all y ∈ K
Πy = δy.
Thus, Π's random projection property means that it fixes any output already satisfying the constraint
K, and its Lipschitz regularity implies that it is stable. Thus, sampling from Π(y1) is similar to
sampling from Π(y2) whenever the points y1,y2 ∈ Rm are near to one another.
Remark 1.4. Random projections are closely tied to the (random) partitions of unity of Lee & Naor
(2005) (see (Ambrosio & Puglisi, 2020, Theorem 2.8)). These random projections generalize the
random projections of Johnson & Lindenstrauss (1984), beyond the case where K is affine.
Remark 1.5. The special case of random projections onto affine spaces has recently been used when
constructing universal neural models (Cuchiero et al., 2021; Puthawala et al., 2020).
We record the complexity of both the decoder and encoder networks constructed in our quantitative
results in Table 1. Here A,B,C,D ≥ 0 are constants independent of e and k, where k ∈ N+ is
the number of continuous derivatives which f admits (when viewed as a function into Rm). From
Network	E	D
Depth	1	2n	2n O(ms (1 + e3(kn+1) kn+1)) O ((	N2 (A + 2e)(4 - e-1)2)等)
Width	m 1 (4n + 10)	m S + N + 2
N	-	O((e-1A + B)号)
Q	-	O(e -mm )
EKlYC	i	i`	i	i`	.	1 r zr>	∙	, ∙	c
Table 1: Complexity of simple transformer network f = D ◦ E approximating f.
Table 1, we see that if m1《m then, S > 0 is large; hence, emm, (1 - 4e-1) 2Sm, and Nmm are small.
1.4	Notation and Background
Optimal Transportd Given any non-empty subset K ⊆ Rm, the set of all Borel probability
measures P on K with a finite mean; i.e.: EX〜p[∣∣X∣∣] < ∞, is denoted by Pi(K). Wasserstein
6P1 (K) denotes the Wasserstein space on K, and is defined formally below.
5
Published as a conference paper at ICLR 2022
distance W1 is defined for any P, Q ∈P1(K) by the minimal energy needed to transport all mass
from P to Q. Following Villani (2009), W1 (P, Q) is defined by:
Wι(P,Q)，inf E(χ1,χ2)〜∏[∣∣Xι- X2k],
where the infimum is taken over all Borel probability measures π on K2 with marginals P and Q.
The metric space (P1 (K) , W1) is named the Wasserstein space overK; we abbreviate itby P1 (K).
Smooth Function Spaces The set of real-valued continuous functions on Rn is denoted by C(Rn).
Let k ∈ N+ and X⊆[0, 1]n be non-empty. The set of functions f : X→K for which there is
a k-times continuously differentiable f : Rn → Rm extending f; i.e.: f|X = f, is denoted by
Ctkr(X,K). Our interest in Ctkr(X,K) does not stem from the fact that it contains all smooth func-
tions mapping [0, 1]n to K, but rather that it allows us to speak about the uniform approximation of
discontinuous K-valued functions on regions in [0, 1]n where they are “regular”. This is noteworthy
for pathological constraint sets, such as integer constraints7. For details on Ctkr(X,K), see (Brudnyi
& Brudnyi, 2012a;b) and the extension theorems of Whitney (1934); Fefferman (2005).
Neural Networks It has recently been observed that deep feedforward networks with multiple
activation functions, or more generally parametric families of activation functions, achieved signif-
icantly more efficient approximation rates than classical feedforward networks with a single acti-
vation function (Yarotsky & Zhevnerchuk, 2020; Yarotsky, 2021; Shen et al., 2021a;b). Practically
deployed examples of parametric activation functions are the PReLU activation function of He et al.
(2015), the Sigmoid-weighted Linear Unit (SiLU) of Elfwing et al. (2018), and the Swish activation
function of Ramachandran et al. (2018). We also observe a similar phenomenon, and therefore our
quantitative results consider deep feedforward networks whose activation functions belongs to a 1-
parameter family σ? , {σt}t∈[0,1] ⊆ C(R). The set of all such networks is denoted byNNσ? and
it includes all f : Rn → RN with iterative representation:
f(x) , A(J)x(J),	x(j+1) , σ% ((Aj)X)ij+ b(j)),	X⑼,x,	(5)
where X ∈ Rn, j =1,...,J - 1, each A(j) is a dj × dj+1-matrix, each b(j) ∈ Rdj+1, dJ+1 = N,
max	dj is f’s width.
j=1,...,J+1
d1
0, t1,1,...,tJ,N ∈ [0, 1], for each j. The integer J is f’s depth and
Example 1.6 (Networks with Untrainable Nonlinearity). Denote σ , σ0. The subset of classical
σσ
feedforward networks consisting of all f ∈NNσ? with each σt = σ in (5) is denoted NNσn,N.
It is approximation theoretically advantageous to generalize the proposed definition of probabilistic
attention in the introduction (3) by replacing Y with a 3-dimensional array (elementary 3-tensor).
Definition 1.7 (Probabilistic Attention). Let N,Q,m ∈ N+, and Y be an N × Q × m-array with
Yn,q ∈ K for n =1,...,N, q =1,...,Q. Probabilistic attention is the function:
Rn 3 w 7→ P-attention(w, Y ) , A xx XXSoftmaxN (w)nδYn,q ∈P1 (K) .
n=1 q=1
IfY is an N × m-matrix, as in (3), then we identify Y as the N × m × 1-array in the obvious manner.
Set-Valued Analysis: A family of non-empty subsets {Cx }x∈Rn of K is said to be a weakly
measurable correspondence, denoted C : Rn ⇒ Rm, if for every open subset8 U ⊆ K, {X ∈ Rn :
Cx ∩ U = 0} is a non-empty Borel subset of Rn (AliPrantis & Border, 2006, pages 557, 592).
2 Main Results
We now present our main results in detail. All proofs are relegated to the paper’s appendix.
2.1	Qualitative Approximation: Deep Maximum Theorem
Our main qualitative result is the following deep neural version of Berge (1963)’s Maximum The-
orem where, the measurable selector is approximately implemented by a probabilistic transformer
7For example, there is no non-constant continuous function f : [0,1] → Z. However, for any λ ∈ (0, 2)
and any y1,y2 ∈ Z, f = y1I[0,λ] + y2I[λ+1,1] belongs to Ckr ([0, λ] ∪ [λ + 2,1], Z) for each k ∈ N+，
8Since K is equipped with its subspace topology, then an open subset U of K is any subset of Rm of the
form U = U1 ∩ K where U1 is an open subset of Rm (see (Munkres, 2000, Lemma 16.1) for further details).
6
Published as a conference paper at ICLR 2022
network. We first present the general qualitative result which gives a concrete description of a mea-
surable selector of (1), with high-probability, which has the key property that all its predictions
satisfy the required constraints defined by K.
Assumption 2.1 (Kidger & Lyons (2020)). σ : R → R is continuous, σ is differentiable at some
x0 ∈ R, and its derivative satisfies σ0(x0) 6=0.
Theorem 2.2 (Deep Maximum Theorem). Let σ satisfy Assumption 2.1. Let K ⊆ Rn be a non-
empty compact set, C : Rn ⇒ Rm be a weakly-measurable correspondence with closed values such
that CX ∩ K = 0 for each X ∈ Rn, L ∈ C(Rm), and P be a Borel probability measure on Rn.
σ
For each 0 < e ≤ 1, there ISan N ∈ N+, an f ∈ NNn N of width at most 2 + n + N, and an
N × m-matrix Y such that:
F : Rn 3 X → P-attention (f(x),Y) ∈ PI(Rm),	(6)
satisfies the following:
(i)	Exact Constrain Satisfaction: ∪χ∈Rn SuPP(F(X)) ⊆ K,
(ii)	Probably Approximately Optimality: There is a compact Xe ⊆ Rn satisfying:
(a)	max W1 (F (X), argmin L(X, y)) ≤ e,
x∈Xe	y∈Cχ∩K
(b)	1 - P(Xe) ≤ e.
Theorem 2.2 implies that for any random field (Y x)x∈Rn on Rm (i.e. a family of Rm-valued random
vectors indexed by Rn) with YX 〜 F(X): 1. samples drawn from YX are in K (by (i)) and 2.
samples drawn from each Y x are near to the optimality set argminy∈C ∩K L(X, y) (by (ii)).
Corollary 2.3 (F ’s Mean Prediction). Assume the setting of Theorem 2.2. Let {YX}X∈Rn be a
K-valued random field with YX 〜F (x) for each X ∈ Rn then, 1 一 P(Xe) ≤ e and
max E [kYX - argmin L(X, y?)k] ≤ e.
x∈X	y? ∈Cχ∩K
Appendix 8 contains additional consequences of the Deep Maximum Theorem, such as the special
case of classical transformers when K is convex. Next, we complement our qualitative results by
their quantitative analogues, within the context of universal approximation under constraints.
2.2 Quantitative Approximation: Constrained Universal Approximation
In order to derive a quantitative constrained universal approximation theorem, we require the loss
function to be tied to the Euclidean norm in the following manner.
Assumption 2.4 (Norm-Controllable Loss). There is a continuous f : Rn → Rm with f(Rn) ⊆ K
and a continuous l :[0, ∞) → [0, ∞) with l(0) = 0, satisfying: L(X, y) ≤ l(kf (X) 一 yk).
Just as with transformer networks, our “constrained universal approximation theorem” approximates
a suitably regular function f : Rn → K ⊆ Rm while exactly respecting the constraints K by
implementing an encoder-decoder network architecture. Thus, our model is a composition of an
encoder network E: Rn → Rd whose role is to approximate f ina classical “unconstrained fashion”
and a decoder network (with probabilistic attention layers at its output) DD : Rd → Pι(K) whose role
is to enforce the constraints K while preserving the approximation performed by E , where d ≪ m.
To take advantage of the encoder-decoder framework present in most transformer networks, we
formalize what is often called a “latent low-dimensional manifold” hypothesis. Briefly, this means
that, the hard constraints in set K are contained in a “low dimensional” subspace.
Assumption 2.5 (Low-Dimensional Manifold). There is an 0 <sand a smooth bijection Φ from
Rn to itselfwith smooth inverse9, such that Φ(K) ⊆ Rd; where 2 ≤ d and d ∈ Θ(m 1).
Assumption 2.5 does not postulate that K is itself a single-chart low-dimensional manifold, or even
a manifold. Rather, K need only be contained in a low-dimensional manifold. For the fast rates we
use activation functions generalizing the swish function (Ramachandran et al., 2018) as follows.
Assumption 2.6 (Swish-Like Activation Function). The map σ :[0, 1] × R 3 (α, t) 7→ σα(t) ∈ R
is continuous; σ0 is non-affine and piecewise-linear, and σ1 is smooth10 and non-polynomial.
9Here smooth means that Φ is continuously differentiable any number of times. NB, smooth bijections with
smooth inverses are called diffeomorphisms in the differential geometry and differential topology literature.
10Following Jost (2017), a function σ : R → R is called smooth (or class C∞) if ∂kσ exists for each k ∈ N+.
7
Published as a conference paper at ICLR 2022
Theorem 2.7 (Constrained Universal Approximation). Let k ∈ N+ and X⊆[0, 1]n be non-empty.
Suppose that σ satisfies 2.6, L satisfies Assumption 2.4, K ⊆ Rn is non-empty, compact and sat-
isfies Assumption 2.5. For any f ∈ Ckr (X, K), every constraining quality eκ > 0, and ^very
σ
approximation error ef > 0, there exist N,Q ∈ N+, an encoderE∈NNσn,d, and a decoder:
DD : Rd 3 x → ^X P-attention (DD(x), Y) ∈ Pι(K)	(7)
k=1
where DD ∈ NNσ,Nand Y is an N × Q × m-array with Y1,1,...,YN,Q ∈ K such that:
(i) Exact Constrain Satisfaction: For each x ∈ Rn: supp(D ◦ E(x)) ⊆ K,
(ii) Universal Approximation: The estimate holds 11 :
1
sup W1(D ◦ E(x), argmin L(x, y)) ≤ eK + k Lip(Φ-1)def;
x∈[0,1]n	y∈K
where, 0 <kis an absolute constant independent of n, m, d, f, and of e andLip(Φ-1) denotes
the Lipschitz constant of Φ-1 on the compact set {z ∈ Rd : kz - Φ(K)k ≤ ef }.
1	. 1 1	1	12∖	1 1	77∙E,τ12	F
Furthermore, the complexities of D and E are recorded in Table 1for F = ek = ef.
In practice, we can only sample from each measure D ◦E(x). In this case, we may ask how the typical
sample drawn from a random-vector Yx distributed according to our learned measure D ◦ E(x)
performs when minimizing L(x, y). The next result relates the estimates in Theorem 2.7 (ii) to the
typical (in Yx) worst-case (in x) gap between a sample from YX and f (x), as quantified by L(χ, ∙).
Corollary 2.8 (Average Worst-Case Loss). Assume the setting of Theorem 2.7 and suppose that the
“modulus” l in Assumption 2.4 is strictly increasing and concave. LetD andE be as in Theorem 2.7
and let {YX }χ∈χ be an Rm-valued random field with YX 〜D ◦ E (x) ,for each X ∈ Rn. Then:
mx EYχ~D°E(x) [L(x,YX)] ≤ l (eκ + kLip(φT)def).
X∈X
Corollary 2.8 quantifies the expected performance of a sample from our probabilistic transformer
model, as expressed by L, whereas Theorem 2.7 (ii) quantifies the difference from the transformer’s
prediction to the optimal prediction value. Next, we consider implications of our main results.
2.3 Applications
We apply our theory to obtain a universal approximation theorem for classical transformer networks
with exact convex constraint satisfaction and to derive a version of the non-Euclidean universal ap-
proximation theorems of Kratsios & Bilokopytov (2020); Kratsios & Papon (2021) for Riemannian-
manifold valued functions which does not need explicit charts. As with most quantitative (uniform)
universal approximation theorems (Guhring et al., 2020; Kidger & Lyons, 2020; Shen et al., 2021a),
we henceforth consider L(x, y) = kf(x) - yk. We also fix f ∈ Ck ([0, 1]n,K).
2.3.1	Transformers are Convex-Constrained Universal Approximators
We return to the familiar transformer networks of Vaswani et al. (2017). The next result shows that
transformer networks can balance universal approximation and exact convex constraint satisfaction.
This is because when K is convex, then the mean of the random field {YX}X∈Rn of Corollary 2.3
must belong to K. Consequently, the identity (4) implies that Attention(D ◦ E(∙), Y) ≈ f.
Corollary 2.9 (Constrained Universal Approximation: Convex Constraints). Consider the setting
and notation of Corollary 2.8. Suppose that K is convex and let L(x, y) = kf(x) - yk. Then:
.. 一一 . ^ ^..
Rn 3 x → E[Yx] = Attention(D ◦ E(x), Y) ∈ K;	(8)
(i)	Exact Constraint Satisfaction: EYx~d°E(,)[Yx] ∈ K,foreach X ∈ Rn,
(ii)	Universal Approximation: sup[o, i]n Ilf (x) - E Yx〜/空⑺丫 x]k < eκ + kdef.
The “complexities” Ofthe networks DD and E are recorded in Table 13 1for F = ek = ef.
11In fact, we actually prove that the slightly stronger statement: sup ∈[0 1]n W1
(D ◦ Ε(x), B.)
≤ €K +
kLip(Φ-1)df. Both formulations align when l has a unique minimum at 0, as is the case when L(x, y)
kf (x) — yk? and ∣∣ ∙ k? is any norm on Rm.
12Explicit constants are recorded in Table 2 within the paper,s appendix; there, eκ and ef may differ.
13Explicit constants are recorded in Table 2 within the paper,s appendix; there, eκ and ef may differ.
8
Published as a conference paper at ICLR 2022
2.3.2 Chart-Free Riemannian Manifold-Valued Universal Approximation
We explore how additional non-convex structure of the constraint set K can be encoded by the
probabilistic transformer networks of Theorems 2.2 and 2.7 and be used to build new types of (de-
terministic) transformer networks. These results highlight that the standard transformer networks
of (8) are specialized for convex constraints and that by instead using an intrinsic variant of expec-
tation, we build can new types of “geometric transformer networks” customized to K’s geometry.
This section makes use of Riemannian geometry; for an overview see Jost (2017).
Let (M, g) be a connected d-dimensional Riemannian submanifold of Rm with distance function
by dg. We only require the following mild assumption introduced in Afsari (2011). We recall that
the injectivity radius at y0, denoted by infg(y0), (see (Jost, 2017, Definition 1.4.6)) is the minimum
length of a geodesic (or minimal length curve) in M with starting point y0 . We also recall that the
sectional curvature (see (Jost, 2017, Definition 4.3.2) for a formal statement) quantifies the curvature
of (M, g) as compared the geometry of its flat counterpart Rd. We focus on a broad class of non-
convex constrains, namely geodesically convex constraints, which generalize convex constraint and
have received recent attention in the optimization literature (Zhang & Sra, 2016; Liu et al., 2017).
Assumption 2.10 (Geodesically Convex Constraints). The Riemannian manifold (M, g) is con-
nected, it is complete as a metric space, and all its sectional curvatures of (M, g) are all bounded
above by a constant C ≥ 0. The non-empty constrain set K satisfies:
1.	K is contained in the geodesic ball B(y0,ρ) , {y ∈ M : dg (y0, y) <ρ} for some point
yo ∈ M and some radius P satisfying 14: 0 < P < 2-1 min{injg (yo), √=},
2.	For each y0,y1 ∈ K there exists a unique geodesic γ :[0, 1] → K joining y0 to y1.
Our latent probabilistic representation grants us the flexibility of replacing the usual “extrinsic mean”
used in (8) to extract deterministic predictions from our probabilistic transformer networks via an
additional Frechet mean layer at their readout. This intrinsic notion of a mean, was introduced
independently in Frechet (1948) and in Karcher (1977), and is defined on any P ∈ P1(K) by:
P，argmin / d2(k, u) P(du).	(9)
k∈K
With this “geometric readout layer” added to our model, we obtain the following variants of our
main results in this non-convex, but geometrically regular, setting.
Corollary 2.11 (Constrained Universal Approximation: Riemannian Case). Consider the setting
and notation of Corollary 2.8. Let L(x, y)=kf(x) - yk. If Assumption 2.10 holds then:
___ ^ ^, . ____________
Rn 3 x → DD ◦ E(X) ∈ K,	(10)
is a well-defined Lipschitz-continuous function, and the following hold:
/ ∙ ∖	n ， c j ∙ j n √∙∕>	√∙	zr> G， ∖ _	τ√^	C	ι _ τ?
(i)	Exact Constraint Satisfaction:	D ◦E (x) ∈	K, for each	x ∈X,
(ii)	Universal Approximation: SuPX dg (f (x), D ◦ E (x)) < eκ + kde∕.
The complexities of D and E are recorded in Table14 15 1 for W = ek = ef.
3 Discussion
In this paper, we derived the first constrained universal approximation theorems using probabilistic
reformation of Vaswani et al. (2017)’s transformer networks. The results assumed both a quantitative
form (Theorem 2.7) and a qualitative form in the more general case of an arbitrary loss functions L
and additional compatible soft constraints in (Theorem 2.2). Our results provide (generic) direction
to end-users designing deep learning models processing non-vectorial structures and constraints.
As this is the first approximation theoretic result in this direction, there are naturally as many ques-
tions raised as have been answered. In particular, it is natural to ask: “Are the probabilistic trans-
former networks trainable in practice; especially when K is non-convex?”. In Appendix 5, we show
that the answer is indeed: “Yes!”, by proposing a training algorithm in that direction and showing
that we outperform an MLP model and a classical transformer network in terms of a joint MSE and
distance to the constraint set. The evaluation is performed on a large number of randomly generated
experiments, whose objective is to reduce the MSE to a randomly generated function mapping a
high-dimensional Euclidean space to there sphere R3 with outputs constrained to the sphere.
14Following Afsari (2011), We make the convention that if C ≤ 0 then, √U is interpreted as ∞.
15Explicit constants are recorded in Table 2 within the paper,s appendix; there, eκ and ef may differ.
9
Published as a conference paper at ICLR 2022
Acknowledgments
Anastasis Kratsios and Ivan Dokmanic were supported by the European Research Council (ERC)
Starting Grant 852821—SWING. The authors thank Wahid Khosrawi-Sardroudi, Phillip Casgrain,
and Hanna Sophia Wutte from ETH Zurich, Valentin Debarnot from the University of Basel for
their helpful feedback, and Sven Seuken from the University of Zurich for his helpful feedback in
the rebuttal phase.
References
Bijan Afsari. Riemannian Lp center of mass: existence, uniqueness, and convexity. Proceedings of
the American Mathematical Society, 139(2):655-673, 2011.
Charalambos D. Aliprantis and Kim C. Border. Infinite dimensional analysis: A hitchhiker’s guide.
Springer, Berlin, third edition, 2006.
Luigi Ambrosio and Daniele Puglisi. Linear extension operators between spaces of Lipschitz maps
and optimal transport. Journalfur die Reine UndAngewandte Mathematik, 764:1-21, 2020.
Anonymized. Pytorch implementation of attend-to-constraints, 2021. URL https://drive.
google.com/file/d/1vryYsUmHt0fok3Mrje6oN9Tjs2UmpgkA/view.
Jean-Pierre Aubin and HeIene Frankowska. Set-valued analysis. Modern Birkhauser Classics.
Birkhauser Boston, Inc., Boston, MA, 2009.
Michel Baes, Calypso Herrera, Ariel Neufeld, and Pierre Ruyssen. Low-rank plus sparse decompo-
sition of covariance matrices using neural network parametrization. IEEE Transaction on Neural
Networks and Learning Systems, 2021.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of the International Conference on Learning Rep-
resentations (ICLR), 2015.
Yair Bartal. On approximating arbitrary metrices by tree metrics. In Proceedings of the Thirtieth
Annual ACM Symposium on the Theory of Computing, pp. 161-168. ACM, New York, 1999.
Basel Committee on Banking Supervision. Fundamental review of the trading book: outstanding
issues, February 2015. https://www.bis.org/bcbs/publ/d305.pdf.
Basel Committee on Banking Supervision. Minimum capital requirements for market risk, February
2019. https://www.bis.org/bcbs/publ/d457.pdf.
Heinz H. Bauschke and Patrick L. Combettes. Convex analysis and monotone operator theory in
Hilbert spaces. CMS Books in MathematiCS/Ouvrages de MathematiqUeS de la SMC. Springer,
New York, 2011. ISBN 978-1-4419-9466-0. doi: 10.1007/978-1-4419-9467-7. URL https:
//doi.org/10.1007/978-1-4419-94 67-7. With a foreword by Hedy Attouch.
Claude Berge. ESpaCeS Topologiques (Topological Spaces). Dunod, 1963.
Rabi Bhattacharya and Vic Patrangenaru. Large sample theory of intrinsic and extrinsic sample
means on manifolds. The Annals of Statistics, 31(1):1-29, 2003.
SiIVere Bonnabel and Rodolphe Sepulchre. Riemannian metric and geometric mean for positive
semidefinite matrices of fixed rank. SIAM Journal on Matrix Analysis and Applications, 31(3):
1055-1070, 2009.
SiIVere Bonnabel, Anne Collard, and Rodolphe Sepulchre. Rank-preserving geometric means of
positive semi-definite matrices. Linear Algebra and its Applications, 438(8):3202-3216, 2013.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
10
Published as a conference paper at ICLR 2022
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar VeliCkovic. Geometric deep learn-
ing: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv:2104.08708, 2021. URL http:
//arxiv.org/abs/2104.13478.
Bernard Bru, Henri Heinich, and Jean-Claude Lootgieter. Distances de Levy et extensions des
the´oreme`s de la limite centrale et de Glivenko-Cantelli. Publ. Inst. Statist. Univ. Paris, 37(3-4):
29-42,1993.
Alexander Brudnyi and Yuri Brudnyi. Methods of geometric analysis in extension and trace prob-
lems. Volume 1, volume 102 of Monographs in Mathematics. BirkhaUSer/Springer Basel AG,
Basel, 2012a.
Alexander Brudnyi and Yuri Brudnyi. Methods of geometric analysis in extension and trace prob-
lems. Volume 2, volume 103 of Monographs in Mathematics. BirkhaUSer/Springer Basel AG,
Basel, 2012b.
Elia Brue, Simone Di Marino, and Federico Stra. Linear Lipschitz and C1 extension operators
through random projection. Journal of Functional Analysis, 280(4):108868, 2021.
Luiz Chamon and Alejandro Ribeiro. Probably approximately correct constrained learning. In
Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2020.
Michele Conforti, Gerard Cornuejols, and Giacomo Zambelli. Integer Programming, volume 271
of Graduate Texts in Mathematics. Springer, Cham, 2014.
Christa Cuchiero, Lukas Gonon, Lyudmila Grigoryeva, Juan-Pablo Ortega, and Josef Teichmann.
Discrete-time signatures and randomness in reservoir computing. IEEE Transactions on Neural
Networks and Learning Systems, pp. 1-10, 2021.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proceedings of
Advances in Neural Information Processing Systems (NeurIPS), pp. 2292-2300, 2013.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems, 2(4):303-314, 1989.
Meng Ding and Guoliang Fan. Multilayer joint gait-pose manifolds for human gait motion modeling.
IEEE Transactions on Cybernetics, 45(11):2413-2424, 2014.
Ivan Dokmanic, Reza Parhizkar, Juri Ranieri, and Martin Vetterli. Euclidean distance matrices:
Essential theory, algorithms, and applications. IEEE Signal Processing Magazine, 32(6):12-30,
2015. doi: 10.1109/MSP.2015.2398954.
Richard M. Dudley. Real analysis and probability, volume 74 of Cambridge Studies in Advanced
Mathematics. Cambridge University Press, Cambridge, 2002.
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning. Neural Networks, 107:3-11, 2018.
Charles L. Fefferman. A sharp form of Whitney’s extension theorem. Annals of Mathematics, 161
(1):509-577, 2005.
Thomas Fletcher. Geodesic regression and the theory of least squares on Riemannian manifolds.
International Journal of Computer Vision, 105(2):171-185, 2013.
Maurice Frechet. Les elements aleatoires de nature quelconque dans un espace distancie. AnnaleS
de l,Institut Henri Poincare, 10:215-310, 1948.
Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega. Risk bounds for reservoir comput-
ing. Journal of Machine Learning Research, 21(240):1-61, 2020a. URL http://jmlr.org/
papers/v21/19-902.html.
Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega. Approximation bounds for random
neural networks and reservoir systems. arXiv preprint arXiv:2002.05933, 2020b.
11
Published as a conference paper at ICLR 2022
Lyudmila Grigoryeva and Juan-Pablo Ortega. Universal discrete-time reservoir computers with
stochastic inputs and linear readouts using non-homogeneous state-affine systems. J. Mach.
Learn. Res., 19:Paper No. 24, 40, 2018.
Lyudmila Grigoryeva and Juan-Pablo Ortega. Differentiable reservoir computing. J. Mach. Learn.
Res., 20:Paper No. 179, 62, 2019.
Ingo Guhring, Gitta KUtyniok, and PhiliPP Petersen. Error bounds for approximations with deep
ReLU neural networks in Ws,p norms. AnalysisandAPPlications,18(5):803-859, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Proceedings of the IEEE international
conference on comPuter vision, pp. 1026-1034, 2015.
Juha Heinonen. Lectures on analysis on metric sPaces. Universitext. Springer-Verlag, New York,
2001.
Ludger Holters, Bjorn Bahl, Maike Hennen, and Andre Bardow. Playing Stackelberg games for
minimal cost for production and utilities. In ECOS 2018-Proceedings of the 31st International
Conference on Efficiency, Cost, OPtimisation, Simulation and Environmental ImPact of Energy
Systems, pp. 36-36. University of Minho, 2018.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural Network, 2(5):359-366, July 1989.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In Proceedings of the International Conference on Machine Learning
(ICML), 2020.
William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert
space. In Conference in modern analysis and Probability, volume 26 of ContemP. Math., pp.
189-206. American Mathematical Society, RI, 1984.
Jurgen Jost. Riemannian geometry and geometric analysis. Universitext. Springer, Heidelberg,
seventh edition, 2017.
Heinrich W. E. Jung. Uber die Cremonasche Transformation der Ebene. J. Reine Angew. Math.,
138:255-318, 1910. ISSN 0075-4102. doi: 10.1515/crll.1910.138.255.
Olav Kallenberg. Foundations of modern Probability, volume 99 of Probability Theory and Stochas-
tic Modelling. Springer, Cham, third edition, 2021.
Hermann Karcher. Riemannian center of mass and mollifier smoothing. Communications on Pure
and APPlied Mathematics, 30(5):509-541, 1977.
Patrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Jacob
Abernethy and Shivani Agarwal (eds.), Proceedings of Machine Learning Research, volume 125,
pp. 2306-2327. PMLR, 09-12 Jul 2020.
Achim Klenke. Probability theory: A comPrehensive course. Universitext. Springer, London, sec-
ond edition, 2014.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized
sliced Wasserstein distances. In Proceedings of Advances in Neural Information Processing Sys-
tems (NeurIPS), 2019.
Anastasis Kratsios. Universal regular conditional distributions. arXiv PrePrint:2105.07743, 2021.
URL https://arxiv.org/abs/2105.07743.
Anastasis Kratsios and Eugene Bilokopytov. Non-Euclidean universal approximation. In Proceed-
ings of Advances in Neural Information Processing Systems (NeurIPS), 2020.
Anastasis Kratsios and Leonie Papon. Universal approximation theorems for differentiable geomet-
ric deep learning, 2021. URL https://arxiv.org/abs/2101.05390.
12
Published as a conference paper at ICLR 2022
James R. Lee and Assaf Naor. Extending Lipschitz functions via random metric partitions. Inven-
tiones Mathematicae, 160(1):59-95, 2005.
Haochuan Li, Yi Tian, Jingzhao Zhang, and Ali Jadbabaie. Complexity lower bounds for nonconvex-
strongly-concave min-max optimization. arXiv:2104.08708, 2021. URL https://arxiv.
org/abs/2104.08708.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-
order methods for geodesically convex optimization on riemannian manifolds. In Proceedings of
Advances in Neural Information Processing Systems (NeurIPS), 2017.
Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa.
Differentiating through the Frechet mean. In Proceedings of the International Conference on
Machine Learning (ICML), 2020.
Cosme Louart, Zhenyu Liao, and Romain Couillet. A random matrix approach to neural networks.
Ann. Appl. Probab., 28(2):1190-1248, 2018. ISSN 1050-5164. doi: 10.1214/17-AAP1328. URL
https://doi.org/10.1214/17-AAP1328.
Mantas LUkoseViciUs and Herbert Jaeger. Reservoir computing approaches to recurrent neural net-
work training. Computer Science Review, 3(3):127-149, 2009. ISSN 1574-0137.
Alexander J. McNeil, Rudiger Frey, and Paul Embrechts. Quantitative Risk Management: Concepts,
Techniques and Tools. Princeton Series in Finance. Princeton University Press, Princeton, NJ,
2015.
Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin Hou, Yann Thanwerdas,
Stefan Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti, Hatem Hajri, Yann Cabanes, Thomas
Gerald, Paul Chauchat, Christian Shewmake, Daniel Brooks, Bernhard Kainz, Claire Donnat,
Susan Holmes, and Xavier Pennec. Geomstats: A python package for riemannian geometry in
machine learning. Journal of Machine Learning Research, 21(223):1-9, 2020.
Theodore Samuel Motzkin. Sur quelques Proprietes CaraCteristiques des ensembles bornes non
convexes. Bardi, 1935.
James R. Munkres. Topology. Prentice Hall, Inc., Upper Saddle River, NJ, 2000. Second edition.
Shin-ichi Ohta. Extending Lipschitz and Holder maps between metric spaces. Positivity, 13(2):
407-425, 2009.
Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation.
International Conference on Learning Representations (ICLR), 2021.
Ofir Pele and Michael Werman. Fast and robust Earth Mover’s distances. In Proceedings of the 12th
IEEE International Conference on Computer Vision (ICCV), pp. 460-467, 2009.
Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural
networks and fully-connected networks. Proceedings of the American Mathematical Society, 148
(4):1567-1581, 2020.
Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica, 1999,
8:143-195, 1999.
Chayne Planiden and Xianfu Wang. Most convex functions have unique minimizers. Journal of
Convex Analysis, 23(3):877-892, 2016.
Pakize Simin Pulat. On the relation of max-flow to min-cut for generalized networks. European
Journal of Operational Research, 39(1):103-107, 1989.
Michael Puthawala, Konik Kothari, Matti Lassas, Ivan Dokmanic, and Maarten de Hoop. Glob-
ally injective ReLU networks. arXiv:2006.08464, 2020. URL https://arxiv.org/abs/
2105.07743.
Prajit Ramachandran, Barret Zoph, and Quoc Le. Searching for activation functions. In Proceedings
of the International Conference of Learning Representations (ICLR), 2018.
13
Published as a conference paper at ICLR 2022
Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization.
arXiv:2102.11436, 2021. URL https://arxiv.org/abs/2102.11436.
James C. Robinson. Dimensions, embeddings, and attractors, volume 186 of Cambridge Tracts in
Mathematics. Cambridge University Press, Cambridge, 2011.
Denis Rosset, Felipe Montealegre-Mora, and Jean-Daniel Bancal. RepLAB: A computa-
tional/numerical approach to representation theory. In Quantum Theory and Symmetries, pp.
643-653. Springer, 2021.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, USA, 2014.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Neural network approximation: Three hidden
layers are enough. Neural Networks, 141:160-173, 2021a.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network with approximation error being
reciprocal of width to power of square root of depth. Neural Computation, 33(4):1005-1036, 03
2021b.
Max Sommerfeld, Jorn Schrieber, Yoav ZemeL and Axel Munk. Optimal transport: Fast Proba-
bilistic approximation with exact solvers. Journal of Machine Learning Research, 20(105):1-23,
2019.
Karl-Theodor Sturm. Probability measures on metric spaces of nonpositive curvature. In Heat
kernels and analysis on manifolds, graphs, and metric spaces, volume 338 of Contemp. Math.,
pp. 357-390. Amer. Math. Soc., Providence, RI, 2003.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
LUkasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of Advances
in Neural Information Processing Systems, pp. 5998-6008, 2017.
Cedric Villani. Optimal Transport: Old and New, volume 338. Springer, 2009.
James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. On the regularity of attention.
arXiv:2102.05628, 2021. URL https://arxiv.org/abs/2102.05628.
Steven Weinberg. Implications of dynamical symmetry breaking. Physical Review D, 13(4):974,
1976.
Hassler Whitney. Analytic extensions of differentiable functions defined in closed sets. Transactions
of the American Mathematical Society, 36(1):63-89, 1934.
Dmitry Yarotsky. Elementary superexpressive activations. In Proceedings of the 38th International
Conference on Machine Learning (ICML), 2021.
Dmitry Yarotsky and Anton Zhevnerchuk. The phase diagram of approximation rates for deep neural
networks. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS),
volume 33, 2020.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are
transformers universal approximators of sequence-to-sequence functions? In Proceedings of the
International Conference on Learning Representations (ICLR), 2020a.
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and
Sanjiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse
transformers. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS),
2020b.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In Pro-
ceedings of the 29th Conference on Learning Theory (COLT), 2016.
Ding-Xuan Zhou. Universality of deep convolutional neural networks. Applied and Computational
Harmonic Analysis, 48(2):787-794, 2020.
14