Published as a conference paper at ICLR 2022
On the approximation properties of recur-
rent encoder-decoder architectures
Zhong Li*
School of Mathematical Sciences
Peking University
li_zhong@pku.edu.cn
Haotian Jiang*
Department of Mathematics
National University of Singapore
e0012663@u.nus.edu
QianXiao Lit
Department of Mathematics
National University of Singapore
qianxiao@nus.edu.sg
Abstract
Encoder-decoder architectures have recently gained popularity in sequence
to sequence modelling, featuring in state-of-the-art models such as trans-
formers. However, a mathematical understanding of their working princi-
ples still remains limited. In this paper, we study the approximation prop-
erties of recurrent encoder-decoder architectures. Prior work established
theoretical results for RNNs in the linear setting, where approximation
capabilities can be related to smoothness and memory of target temporal
relationships. Here, we uncover that the encoder and decoder together form
a particular “temporal product structure” which determines the approxi-
mation efficiency. Moreover, the encoder-decoder architecture generalises
RNNs with the capability to learn time-inhomogeneous relationships. Our
results provide the theoretical understanding of approximation properties
of the recurrent encoder-decoder architecture, which precisely characterises,
in the considered setting, the types of temporal relationships that can be
efficiently learned.
1	Introduction
Encoder-decoder is an increasingly popular architecture for sequence to sequence modelling
problems (Sutskever et al., 2014; Chiu et al., 2018; Venugopalan et al., 2015). The core
of this architecture is to first encode the input sequence into a vector using the encoder
and then map the vector into the output sequence through the decoder. In particular,
such architecture forms the main component in the transformer network (Vaswani et al.,
2017), which has become a powerful method for modelling sequence to sequence relationships
(Parmar et al., 2018; Beltagy et al., 2020; Li et al., 2019).
The encoder-decoder family of structures differ significantly from direct application of recur-
rent neural networks (RNNs, Elman (1990)) and its generalisations (Hochreiter & Schmid-
huber, 1997; Cho et al., 2014b) for processing sequences. However, both architectures can
be considered as modelling mappings between sequences, albeit with different underlying
structures. Hence, a natural but unresolved question is: how are these approaches funda-
mentally different? Answering this question is not only of theoretical importance but also
of practical interest. Currently, architectural selection for different time series modelling
tasks is predominantly empirical. Thus, it is desirable to develop a concrete mathemati-
cal framework to understand the key differences between separate architectures in order to
guide practitioners in a principled way.
* Equal contribution
,Corresponding author
1
Published as a conference paper at ICLR 2022
In this paper, we investigate the approximation properties of encoder-decoder architectures.
Approximation is one of the most basic and important problems for supervised learning. It
considers to what extent the model can fit a target. In particular, we prove a general approx-
imation result in the linear setting, which characterises the types of temporal input-output
relationships that can be efficiently approximated by encoder-decoder architectures. These
results reveal that such architectures essentially generalise RNNs by lifting the requirement
of time-homogeneity (see Remark 3.2) in the target relationships. Hence, it can be used
to tackle a broader class of sequence to sequence problems. Furthermore, of particular in-
terest is the identification of a “temporal product structure” — a precise property of the
target temporal relationship that highlights another intrinsic difference between recurrent
encoder-decoders and RNNs.
Our main contributions can be summarised as follows.
1.	We prove a universal approximation result for recurrent encoder-decoder architec-
tures in the linear setting, including the approximation rates.
2.	We show that in the considered setting, the recurrent encoder-decoder generalises
the RNNs and can approximate time-inhomogeneous relationships, which further
adapt to additional temporal product structures in the target relationship. This
answers precisely how encoder-decoders are different from RNNs, at least in the
considered setting.
Organisation. In Section 2, we review the related work on encoder-decoder architectures
and general approximation theories of sequence modelling. The approximation problem is
formulated in Section 3. Our main results, their consequences and numerical illustrations
are presented in Section 4. All the proofs and numerical details are included in appendices.
Notations. For consistency, we adhere to the following notations. Boldfaced letters are
reserved for sequences or paths, which can be understood as functions of time. Lower case
letters can mean vectors or scalars. Matrices are denoted by capital letters. For α ∈ N, Cα
denotes the space of functions with continuous derivatives up to order-α.
2	Related work
We first review some previous works on sequence to sequence modelling. The encoder-
decoder architecture first appeared in Kalchbrenner & Blunsom (2013), where they map the
input sequence into a vector using convolutional neural networks (CNNs), and then using
a recurrent structure to map the vector to the output sequence. With the flexibility of
manipulating the underlying structure of encoder and decoder, numerous models based on
this architecture have come out thereafter. For instance, Cho et al. (2014b) used gated RNNs
as both the encoder and decoder, while in the later work (Cho et al., 2014a), they proposed
a CNN-based decoder. In Sutskever et al. (2014), they proposed a deep LSTM for both
the encoder and decoder. Bahdanau et al. (2015) first introduced the attention mechanism,
which was further developed in the well-known transformer networks (Vaswani et al., 2017).
However, most of the research on encoder-decoder architectures focused on applications. A
theoretical understanding is helpful for its further improvement and development.
From the theoretical point of view, Ye & Sung (2019) studied several theoretical properties of
CNN encoder-decoders, including expressiveness, generalisation capability and optimisation
landscape. Of particular relevance to the current work is expressiveness, which considers
the relationships that can be generated from the architecture. However, this is not approx-
imation. Yun et al. (2020) proved the universal approximation property of transformers for
certain classes of functions, for example, permutation equivariant functions, but they did not
consider the actual dynamical properties of target relationships that affect approximation.
Dynamical proprieties such as memory, smoothness and low rank structures are essential,
because they can precisely characterise different temporal relationships and affect the ap-
proximation capabilities of models. Assuming the target generated from a hidden dynamical
system is one approach, which is widely applied (Maass et al., 2007; Schafer & Zimmermann,
2007; Doya, 1993; Funahashi & Nakamura, 1993). In contrast, a functional-based approach is
2
Published as a conference paper at ICLR 2022
recently introduced, where the target temporal relationships are generated from functionals
satisfying specific properties such as linearity, continuity, regularity and time-homogeneity
(Li et al., 2021). In Li et al. (2021), the approximation properties of linear RNN models
are studied, and the results therein show that the approximation efficiency is related to the
memory structure. In Jiang et al. (2021), similar formulations are applied to investigate
convolutional architectures, where the results suggest that targets with certain spectrum
regularity can be well approximated by dilated CNNs. Under this framework, the target
temporal relationship that can be efficiently approximated is characterised by properties
such as memory, smoothness and sparsity. This enables us to make precise mathemati-
cal comparisons between different architectures. Our results in this work reveal that the
encoder-decoders have a special temporal product structure which is intrinsically different
from other sequence modelling architectures.
3	Problem formulation
In this section, we precisely define the input space, output space, concept space and hy-
pothesis space, respectively.
Functional formulation of temporal modelling. First, we define the input and output
space precisely. A temporal sequence can be viewed as a function of time t. The input space
is defined by X = C0((-∞, 0], Rd). This is the space of continuous functions from (-∞, 0]
to Rd vanishing at infinity, where d ∈ N+ is the dimension. Denote the element in X by
X = {Xt ∈ Rd : t ≤ 0}, We equip X with the supremum norm IIxlIX := supt≤° ∣∣x∕∣∞. We
take the outputs space as Y = Cb([0, ∞), R), the space of bounded continuous functions
from [0, ∞) to R. We consider real-valued outputs, since each dimension can be handled
individually for vector-valued outputs.
The mapping between input and output sequences can be formulated as a sequence of
functionals, i.e. yt = Ht(x), t ≥ 0. The output yt at the time step t depends on the input
sequence x. The ground truth relation between inputs and outputs is formulated by the
sequence of functionals H := {Ht : t ≥ 0}.
We provide an example to illustrate the above formulation. Given an input x, the output
y is a smoothed version of x, resulting from convolving x with the Gaussian kernel g(s) =
√= exp(-%). This relation can be formulated as yt = Ht(x) = f∞ g(t + S)x-sds.
The RNN encoder-decoder model. For the supervised learning problem, our goal is
to use a model to learn the target relationship H . First, we define the model. Among all
different variants of the encoder-decoder architectures, the RNN encoder-decoder introduced
in Cho et al. (2014b) can be considered as the most simple and representative model, where
the encoder and decoder are both RNNs. We study this particular model as we try to
eliminate other factors and only focus on the encoder-decoder architecture itself.
Under our setting, the simplified model of Cho et al. (2014b) with RNNs as both encoder
and decoder can be formulated as
hs = σE(WEhs-1+ UExs + bE), v = hτ,
gt = σD(WDgt-1+ bD),	g0 = v,	(1)
ot = WOgt + bO ,
where ht , gt are hidden states of the encoder and decoder respectively. Recurrent activation
functions are denoted by σE and σD . Here, τ denotes the terminating time step of the
encoder, and v is the summary of the input sequence, which is called as the coding vector.
The mo del prediction is denoted as ot ∈ R. All the other notations are model parameters.
Equation (1) describes the following model dynamics. First, the encoder reads the entire
input x, and then summarises the input into a fixed size coding vector v, which is also the
last hidden state of the encoder. Next, the coding vector is passed into the decoder as the
initial state, and then the decoder produces an output at each time step. Note that the
encoder has a terminating time, and the decoder has a starting time. This is the reason
why we take the input and output as semi-infinite sequences.
3
Published as a conference paper at ICLR 2022
We study a linear, residual and continuous-time idealisation of the model dynamics (1):
hs = Whs + Uxs,	v = Qh0, s ≤ 0
g t = Vgt,	g 0 = Pv,	(2)
ot = c gt,	t ≥ 0,
where W ∈ RmE ×mE, U ∈ RmE×d, Q ∈ RN×mE, V ∈ RmD ×mD, P ∈ RmD×N and c ∈ RmD
are parameters. mE and mD denote the width of encoder and decoder, respectively. The
coding vector v has dimension N , where we apply linear transformations to control it. We
assume h-∞ = 0, which is the usual choice for the initial condition of RNN hidden states.
Since our goal is to investigate approximation problems over large time horizons, we are
supposed to consider the stable RNN encoder-decoders, where
W ∈ WmE := {W ∈ RmE ×mE : eigenvalues of W have negative real parts},	(3)
V ∈ VmD := {V ∈ RmD ×mD : eigenvalues of V have negative real parts}.	(4)
The hypothesis space of RNN encoder-decoder models with arbitrary widths and coding
vector dimension is defined as H := UmE,m0N∈n+ HmE,mD,N, where
∞
^evtP /
0
^
HmE ,mD ,N
∣H:= {H : t ≥ 0} : Ht(x)
QeWsUx-sds, with
(W,U,Q,V,P,c) ∈ WmE × RmE×d × RN×mE
× VmD × RmD×N
× RmD .
(5)
The widths mE, mD and the coding vector dimension N together control the capac-
ity/complexity of the hypothesis space. Note that the assumptions on eigenvalues of W
and V ensure that the parameterized linear functionals are continuous.
Due to the mathematical form (5), not all functionals can be represented by RNN encoder-
decoders. To achieve a good approximation, the target functionals must possess certain
structures. We introduce the following definitions to clarify these structures.
Definition 3.1. Let H = {Ht : t ≥ 0} be a sequence of functionals.
1.	For any t ≥ 0, the functional Ht is linear and continuous if for any λ1, λ2 ∈ R
and xι, X2 ∈ X, We have Ht(λ 1 xι + λ2X2) = λ 1 Ht(xι) + λ2Ht(x2), and ∣∣Ht∣∣ :=
suPx∈X IlxIIA∙≤ι |Ht(x)| < ∞, where ∣∣Ht∣∣ denotes the induced functional norm.
2.	For any t ≥ 0, the functional Ht is regular if for any sequence {x(n) }n∞=1 ⊂ X
such that limn→∞ x(sn) = 0 for almost every s ≤ 0 (Lebesgue measure), we have
limn→∞ Ht(x(n) ) = 0.
For a sequence of functionals H, We define its norm by ∣H∣∣ :
∞
∣ IIHt Il dt.
0
Remark 3.1. The definitions of linear and continuous functionals are standard. One can
vieW regular functionals as those not determined by inputs on arbitrarily smal l time intervals,
e.g. an infinitely thin spike (i.e. δ-functions).
Given the above definitions, we immediately have the following observation.
Proposition 3.1. Let H ∈ H be a sequence of functionals in the RNN encoder-decoder
hypothesis space (see (5)). Then for any t ≥ 0, Ht ∈ H is a linear, continuous and regular
functional. Furthermore, ∣H11∣ decays exponentially as a function of t.
The proof is found in Appendix A. This proposition characterises properties of the encoder-
decoder hypothesis space. In particular, it is different from the RNN hypothesis space
discussed in Li et al. (2021), since the encoder-decoder is not necessarily time-homogeneous.
Remark 3.2. A sequence of functionals H is time-homogeneous iffor any t, τ ≥ 0, Ht (x) =
Ht+τ (x(τ)), With x(τ)s = xs-τ for all s ∈ R. That is, if the input is shifted to the right by
4
Published as a conference paper at ICLR 2022
τ , the output is also shifted by τ . Temporal convolution is an example of time-homogeneous
operation (recal l the Gaussian convolution discussed in Section 3. An example of time-
inhomogeneous relationship is video captioning: shifts in the sequence of input video frames
do not necessarily lead to corresponding shifts in the caption text sequence.
Relation with RNNs. Here, we emphasise the differences between the encoder-decoder
hypothesis space and the RNN hypothesis space discussed in Li et al. (2021), where
H!RNN)(x) = f∞ Cτew(t+S)Ux-Sds. A key difference is that the encoder-decoder has a
structure involving two temporal parameters t and s, while the RNN only has one depend-
ing on t + s, due to the time-homogeneity. Owing to this difference and the fact that
H(RNN) ⊂ H, the encoder-decoder hypothesis space (5) is more general, with the extra Ca-
pability to learn time-inhomogeneous relationships. Furthermore, eV t and eWS adapt to a
temporal product structure, which is an intrinsic difference between encoder-decoders and
other architectures. We will discuss this in detail in the next section.
4 Approximation results
One of the most fundamental problems for supervised learning is the approximation prob-
lem. It basically concerns the capacity of the hypothesis space to fit the concept space. In
general, there are two levels of approximation problems that can be discussed. The first is
known as the universal approximation, which considers the density of the hypothesis space
in the concept space. The second is the approximation rate, which aims to characterise
quantitatively the approximation accuracy concerning the capacity/complexity of the hy-
pothesis space (e.g. the number of trainable parameters). In this section, both of them are
developed for RNN encoder-decoders.
4.1	Universal approximation
We first present the most basic density result, which states that any linear, continuous,
and regular temporal relationship can be approximated by RNN encoder-decoders up to
arbitrary accuracy. The proof is found in Appendix B.
Theorem 4.1.	Let H be a sequence of linear, continuous and regular functionals defined
on X, and satisfy ∣∣H∣∣ < ∞. Then for any e > 0, there exists H ∈ H such that
∞
H∣≡ /
0
IlH -
IHt -H11dt
< 6.
(6)
Here, we highlight two important observations while deriving Theorem 4.1. First, one
can show that each sequence of functionals H ∈ H can be associated with a unique two-
parameter “representation” P(t, s), such that Ht(x) = f∞ xTSP(t, s)ds. Recall the model
form Ht(x) = f∞ x-sp(t,s)ds, where ρ(t, s):= [cτeVtPQeWsU]τ denotes the correspond-
ing representation. The functional approximation is then reduced to function approximation
in the sense of representations, i.e. IlH - HIl ≤ ∣∣P一PIlLι(Q∞)2). It turns out that P directly
affects the rate of approximation and gives rise to intrinsic properties. We will discuss this
in detail in Section 4.3. In addition, we again emphasise the differences between the present
work and Li et al. (2021). In Li et al. (2021), the target relationships are assumed to be
time-homogeneous with the representation Ht(x) = 0∞ P(t + s)x-Sds, which only depends
on t + s. However, the setting here does not assume time-homogeneity, hence implies a more
general representation P depending on the two temporal directions t and s simultaneously.
4.2 General approximation rates
While the density result (Theorem 4.1) ensures the universal approximation property of the
RNN encoder-decoder, it does not identify targets that can be efficiently approximated. To
achieve this, we focus on approximation rates next. We characterise the temporal structure
of a target relationship by observing its responses to “constant” input signals. Here, we
consider the approximation rates for the model with “large size” coding vector, where the
5
Published as a conference paper at ICLR 2022
dimension N ≥ rm := min{m’E, mD}. This is the scenario where We fix the widths but take
an oversized coding vector.
Theorem 4.2.	Let H be a sequence of linear, continuous and regular functionals defined
on X, and satisfy ∣∣H∣∣ < ∞. Consider the output of piece-wise constant signals yC(t,s)=
Ht(ei1(-∞,-s] ), t, s ≥ 0, i = 1, 2, . . . , d, where {ei}id=1 denotes the standard basis of Rd.
Assume that there exist α ∈ N+, β > 0 such that for any i = 1, 2, . . . , d,
yic ∈ Cα+1([0, ∞)2),	(7)
∂k+l
eβ(t+s) ∂tk∂sl yc(t,s) = o⑴ as ∣∣(t,s)∣→∞,	(kJ) ∈ N × N+, k + T ≤ α + 1.	(8)
rm	Ii	7∖ 7^ —	RT	i 7	∙ i TT 一 公I	7 ι 7	，
Then for any	mE, mD , N ∈	N+,	there	exists H ∈ HmE,mD ,N	such that
IlH -旬I ≤
C(α)γd
(9)
where C (α), γ > 0 are both universal constants with dependence only on α and (α, β),
respectively, and Y := ’ max^	max^	sup β-(k+1)eβ(t+s) ∣ ∂∂k+si yC(t, s) ∣ < ∞. Here,
the number of trainable parameters is dN(mE + mD) with N ≥ m.
The proof is found in Appendix C. First, note that the error bound does not depend on the
coding vector size N, as long as N ≥ m. This is because further increasing N beyond rm
only increases the number of trainable parameters, but does not increase the model capacity
(see Remark C.2). Only the mo del widths mE, mD affect the approximation capabilities.
Next, we focus on the classes of target relationships that can be well approximated. Here,
α characterises the smoothness of H, and β characterises the temporal decay rates of the
output responding to a constant signal under H. This is a notion of memory in the target
relationship. The error bound (9) indicates that a sequence of target functionals can be
efficiently approximated by the encoder-decoder if it is smooth (large α), and has fast
decayed memory (large β ).
The characterisation in smoothness and memory decay also appears in the approxima-
tion results of RNNs (Li et al., 2021), where the upper bound is C0)^. However, our
results for encoder-decoders suggest extra structures, where the bound involves two (in-
stead of one) temporal parameters together with smoothness and decay memories in both.
The two-parameter temporal dependence allows the encoder-decoder to approximate time-
inhomogeneous relationships, which generalises the RNN. This two-parameter structure
further leads to adaptation to a specific low rank type of target relationships, resulting in
finer approximation rates as we discuss next.
4.3 Approximation rates via temporal product structure
Motivation of temporal product structure. In contrast with Theorem 4.2, we next
consider the model with N < m = min{mE,mD}. In this situation, the model has fewer
parameters, and we aim to characterise the target relationships by further exploiting the
structure of the two-parameter representation ρ(t, s). This leads to a finer approximation
rate by considering mE, mD, N together.
We first motivate how the “temporal product structure” arises, and how it relates to the
approximation. Detailed discussions and proofs are found in Appendix D. For the illustration
purpose, we set the input dimension d = 1. Recall Q ∈ RN ×mE , P ∈ RmD ×N, then the
representation P of the encoder-decoder functional can be rewritten as
N	mD
ρ(t, S) = ɑTeVtP ∙ QeWsu =工(工 CiPjn [eVt] ij
n=1	i,j=1
N
=工5 n (t) φ n ( S ) .
n=1
mE
∑ 〃必“eW
i,j=1
(10)
6
Published as a conference paper at ICLR 2022
This is a tensor product structure over the (t, s) time domain (determined by the encoder
{φn} and decoder {今n} successively). We call it the temporal product structure. As is
shown later, this structure significantly affects approximation rates. When {φn} and {今n}
are selected as the “bases” along s, t direction, respectively, N is considered as the rank of
the temporal product. We also define N as the rank of the model, which is understood as
the maximum rank of temporal products that the encoder-decoder model can represent.
The rank concept of temporal relationships. Recall that the given number of train-
able parameters is dN (mE + mD). Hence, a low rank model may achieve fewer trainable
parameters. When investigating relationships that can be well approximated by low rank
models, a natural conjecture would be “low rank” targets.
What is the meaning of “low rank” for a temporal relationship? It is well-known that in linear
algebra, an operator is low rank means that its range space is low-dimensional. This idea
can be also applied to temporal relationships. For a “low rank” temporal relationship, the
output sequence is more “regular”, meaning that the output sequences (viewed as functions)
are in a low-dimensional function space. We provide an intuitive numerical illustration for
better understanding.
2.0
1.5
1.0
0.5
∩,0
-1.0
-1.5
(a) high rank relationship
t
t
40
30
20
10
0
-10
(b) low rank relationship
Figure 1: We construct a high rank and a low rank target from the temporal product. For
both (1a) and (1b), we plot the inputs xt together with the corresponding outputs Ht (x).
Detailed settings are found in Appendix E.1.
Figure 1 shows the outputs of a high rank (a) and a low rank (b) target relationship on
the same set of random input sequences. Different colours refer to different instances of
inputs. In the first case (high rank), the temporal structure of the outputs is very complex
and depends sensitively on the inputs. However, in the second case (low rank), the output
sequences are much more regular, and only macroscopic structures (e.g. scale/offset) appear.
Remark 4.1. In the research of approximation theories for temporal sequences, prior works
also related a notion of rank to approximation properties of the dilated convolutional structure
(Jiang et al., 2021). Here, we emphasise that the notion of rank considered in our work is
very different from that in Jiang et al. (2021), which mainly concerns the tensorisation of
a discrete-time sequence according to the width of convolution filters.
POD as an analogue of SVD. Now, we characterise low rank and high rank temporal
relationships in a mathematical way. We will introduce the concepts informally, and rigorous
definitions and arguments can be found in Appendix D.
For a matrix, we can assess its rank by performing the singular value decomposition
(SVD). This method can be extended to the temporal relationships using proper orthogo-
nal decomposition (POD; Liang et al. (2002), Berkooz et al. (1993), Chatterjee (2000)).
The basic insight is that the function ρ can be decomposed into the following form:
P(t,s) = EN= ι Qnψn(t)φn(S), where No ≤ ∞, {ψn} and {φn} are orthonormal bases, and
σι ≥ σ2 ≥ ∙∙∙ ≥ 0 denote the singular values. This procedure can be viewed as apply-
ing SVD to an infinite-dimensional space (when No = ∞). An analogue of Eckart-Young
theorem (Eckart & Young, 1936), which characterises the best low rank approximation,
also holds for POD. It roughly states that inf加卜(P)=N∣∣P — PIL2 = EN= N+]σn. That is,
7
Published as a conference paper at ICLR 2022
any target ρ has a rank-N best approximation, with error equalling to the tail sum of the
squared singular values. In other words, a target with fast decayed σn (low “effective rank”)
has smaller approximation errors. This forms the basis of our next result, which states that
if the target relationship possesses an effective low rank structure in terms of the decay
of singular values, then one can achieve an efficient approximation using encoder-decoder
structures by limiting the size of coding vectors. Detailed definitions for {σn} and proofs
are found in Appendix D.
Theorem 4.3.	Assume the same conditions as in Theorem 4.2. Then for any mE, mD, N ∈
N+ with N ≤ m, there exists H ∈ H皿后,mD ,n such that
IlH - ^∣. CaYd {(1 + √m=
(11)
where . hides universal positive constants, and Tm = min{mE, mD}. Here, the number of
trainable parameters is dN(mE + mD) with N ≤ Tm.
This is a finer approximation rate compared to Theorem 4.2, where both the widths mE, mD
and the coding vector size N affect the model capacity for approximation. Besides the
smoothness and memory decay, we have the additional rank structure of the target rela-
tionship, which is characterised by its singular values {σn}. We again focus on the class of
functionals that can be well approximated. Smoothness α and decay rate β is the same as
Theorem 4.2. The difference lies in the rank structure indicated by {σn }: the error bound is
small if {σn } has a small tail nm=N +1 σn2 . It suggests that a target with fast decayed {σn}
or low “effective rank” can be well approximated by the RNN encoder-decoder with fewer
parameters. Due to the Eckart-Young-like low rank approximation, We can appropriately
select N based on the decay rate of singular values.
Here, we emphasise that the temporal product is an intrinsic structure arising from the
encoder-decoder architecture. Recall the dynamics of the encoder-decoder: it first encodes
the input sequence into a coding vector, and then decodes an entire output sequence from
it. In this sense, the coding vector is the only interaction between the input and output.
Thus, the coding vector size N is an essential measure of the model capacity concerning
the dependence of outputs on inputs. Here, we show that this concept can be formalised
as a notion of rank, which can pinpoint the precise types of input-output relationships that
encoder-decoder architectures are well adapted to.
Numerical illustrations. Here, we utilise numerical examples to illustrate the above
discussions. We observe how the decay rate of singular values, the rank N0 of the target
relationships, and the model rank N affect the approximation error IlH - H∣∣. However,
it is not always possible to construct the best approximation. Instead, we perform some
training steps to achieve an upper bound of the approximation error, which is consistent
with our theoretical results.
In Figure 2, we train linear encoder-decoder models to learn three relationships of different
ranks determined by various decay patterns of singular values, given in (a), (b) and (c).
Different colours denote targets with different ranks. From Figure 2, we have the following
observations consistent with previous discussions. First, observe that increasing the model
rank N makes approximation errors smaller, as expected. Moreover, note that when increas-
ing N , the speeds of error decrements are different. If the singular values decay fast, the
approximation errors also decay fast. This implies that a target with fast decayed singular
values can be approximated efficiently with fewer parameters (smaller N). In addition, for
each experiment, we are able to achieve low approximation errors by choosing N《m. The
errors will remain unchanged or decrease much more slowly when further increasing N . This
suggests that in practice, one can choose N such that it covers the major singular values of
the target in order to improve the approximation efficiency.
8
Published as a conference paper at ICLR 2022
×10-2
1.1
0.9
§0.7
2 0.5
0.3
0.0
2	4	6	8	10
N
o
UJ
2.8
2.3
1.8
1.3
0.8
0.0
(b) σ -n: n 葬ɔ
(c) σn = n-2
Figure 2: In (a), (b), (c) we consider target relationships with different singular values
indicated in the respective caption. For (a), (b) we also consider targets with different rank,
where N0 = 2, 4, 6, 8. We use models with fixed width m = mE = mD = 128 and coding
vector size N. Detailed settings are found in Appendix E.2.
n n	fn-1, n ≤ Nɔ
(a)σn= 0,n	>N00
In Figure 3, we perform experiments on the forced Lorentz 96 system (Lorenz, 1996), which
parameterises a high-dimensional and nonlinear relationship between input forcing and
model states. The parameters K, J in the Lorenz 96 system control the overall complexity
of the target (see Appendix E.3 for details). We use the RNN encoder-decoder with tanh
activations to learn this target. Although our theories are developed in the linear regime,
the low rank approximation phenomenon also appears in this nonlinear setting. The error
decrements saturate when increasing the coding vector size N beyond a threshold, suggest-
ing the existence of some implicit notion of “rank” of the target nonlinear functional. This
“rank” increases with the target complexity (mainly K).
×10-4
8
2
6
δ
占4
LU 4
30
10	20
N
0	10	20	30
N
0
0
Figure 3: K, J are the parameters of the Lorenz 96 system. They describe the number
of independent and coupled variables in the system, which can be viewed as a complexity
measure. Detailed settings are found in Appendix E.3.
5 Conclusion
We theoretically study the approximation properties of the RNN encoder-decoder in a lin-
ear setting. We prove a universal approximation result for linear temporal relationships
utilising encoder-decoder architectures, and show that they generalise RNNs to the time-
inhomogeneous setting. Moreover, we discover an important temporal product structure
that characterises the types of input-output relationships especially suited for the efficient
approximation using encoder-decoders. This elucidates the key differences between these
novel architectures and classical methods for temporal modelling, and forms a basic step
towards understanding the intricacies of modern deep learning.
9
Published as a conference paper at ICLR 2022
Reproducibility statements. Detailed proofs for theoretical results, and complete set-
tings of numerical examples are found in the appendix. The source code for numerical tests
can be made available upon request.
Here is a quick reference:
Proposition 3.1 Properties of RNN encoder-decoder functionals Theorem 4.1	Universal approximation theorem Theorem 4.2	General approximation rates Theorem 4.3	Approximation rates concerning temporal product structure Figure 1	Illustration of high/low rank temporal relationships Figure 2	Numerical examples on singular values Figure 3	Numerical examples on Lorenz 96 systems	Appendix A Appendix B Appendix C Appendix D Appendix E.1 Appendix E.2 Appendix E.3
Acknowledgements
ZL is supported by Peking University under BICMR mathematical scholarship. HJ is sup-
ported by National University of Singapore under PGF scholarship. QL is supported by the
National Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-2021-
0005).
10
Published as a conference paper at ICLR 2022
References
Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Repre-
Sentations, pp. 1-15, 2015.
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150, 2020.
G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the
analysis of turbulent flows. Annual Review of Fluid Mechanics, 25(1):539-575, 1993.
doi: 10.1146/annurev.fl.25.010193.002543. URL https://doi.org/10.1146/annurev.
fl.25.010193.002543.
Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Ching-Hua Chang and Chung-Wei Ha. On eigenvalues of differentiable positive definite
kernels. Integral Equations and Operator Theory, 33:1-7, 1999. doi: 10.1007/BF01203078.
Anindya Chatterjee. An introduction to the proper orthogonal decomposition. Current
Science, 78(7):808-817, 2000. ISSN 00113891. URL http://www.jstor.org/stable/
24103957.
Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen,
Z. Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,
Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with
sequence-to-sequence models. In IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 4774-4778, 2018.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On
the properties of neural machine translation: Encoder-decoder approaches. In Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103-111.
Association for Computational Linguistics, 2014a. doi: 10.3115/v1/W14-4012. URL
https://aclanthology.org/W14-4012.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation. Conference on Empirical Meth-
ods in Natural Language Processing, pp. 1724-1734, 2014b. doi: 10.3115/v1/d14-1179.
Kenji Doya. Universality of fully-connected recurrent neural networks. IEEE Transactions
on Neural Networks, 1993.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
Psychometrika, 1(3):211-218, 1936.
Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990.
Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by con-
tinuous time recurrent neural networks. Neural Networks, 6(6):801 - 806, 1993. ISSN
0893-6080.
Sepp Hochreiter and Jiirgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735-1780, 1997.
Haotian Jiang, Zhong Li, and Qianxiao Li. Approximation theory of convolutional archi-
tectures for time series modelling. In Proceedings of the 38th International Conference on
Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4961-
4970. PMLR, 2021. URL https://proceedings.mlr.press/v139/jiang21d.html.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Confer-
ence on Empirical Methods in Natural Language Processing, pp. 1700-1709, 2013. ISBN
9781937284978.
11
Published as a conference paper at ICLR 2022
Peter D. Lax. Functional Analysis. John Wiley & Sons, Inc., 2002.
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis
with transformer network. In AAAI Conference on Artificial Intel ligence, volume 33, pp.
6706-6713, 2019.
Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neu-
ral networks: Approximation and optimization analysis. In International Conference on
Learning Representations, 2021. URL https://openreview.net/forum?id=8Sqhl-nF50.
Y. C. Liang, H. P. Lee, S. P. Lim, W. Z. Lin, K. H. Lee, and C. G. Wu. Proper orthogonal
decomposition and its applications—Part I: Theory. Journal of Sound and Vibration, 252
(3):527-544, 2002. ISSN 0022-460X. doi: https://doi.org/10.1006/jsvi.2001.4041. URL
https://www.sciencedirect.com/science/article/pii/S0022460X01940416.
G. G. Lorentz. Approximation of Functions. AMS Chelsea Publishing Series. Holt, Rinehart
and Winston, 2005. ISBN 9780821840504. URL https://books.google.com.sg/books?
id=8VMrOmTKSe0C.
Edward N. Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Pre-
dictability, volume 1, pp. 40-58, 1996.
Wolfgang Maass, Prashant Joshi, and Eduardo D. Sontag. Computational aspects of feed-
back in neural circuits. PLOS Computational Biology, 3(1):e165, 2007.
Charles Bradfield Morrey. Multiple Integrals in the Calculus of Variations. Springer-Verlag,
1966.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexan-
der Ku, and Dustin Tran. Image transformer. In International Conference on Machine
Learning, pp. 4055-4064. PMLR, 2018.
Walter Rudin. Real and Complex Analysis. Higher Mathematics Series. McGraw-Hill Edu-
cation, 1987. ISBN 9780070542341. URL https://books.google.com.sg/books?id=Z_
fuAAAAMAAJ.
Anton Maximilian Schafer and Hans-Georg Zimmermann. Recurrent neural networks are
universal approximators. International Journal of Neural Systems, 17(4):253-263, 2007.
Martin H. Schultz. L∞-multivariate approximation theory. SIAM Journal on Numerical
Analysis, 6(2):161-183, 1969. doi: 10.1137/0706017. URL https://doi.org/10.1137/
0706017.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, volume 4, pp. 3104-
3112, 2014.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances
in Neural Information Processing Systems, pp. 5999-6009, 2017.
Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the
IEEE International Conference on Computer Vision, pp. 4534-4542, 2015.
Jong Chul Ye and Woon Kyoung Sung. Understanding geometry of encoder-decoder
CNNs. In International Conference on Machine Learning, pp. 12245-12254, 2019. ISBN
9781510886988.
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=ByxRM0Ntvr.
12
Published as a conference paper at ICLR 2022
A Properties of model functionals
In this section, we prove observations of the hypothesis space reported in Proposition 3.1.
Proof of Proposition 3.1. Recall that
∞
0
Ht(x； θ)
CTeVtMeWsUx-sds.
(12)
Fix any θ = (W, V, U, M, c). The linearity is obvious. Since both W ∈ WmE and V ∈ VmD
have eigenvalues with negative real parts, there exist C1, c 2 ,c 1 ,c 2 > 0, such that IeVt ∣∣∞ ≤
c 1L21 and IIeWsI∣∞ ≤ C1 e-c2s for any t,s ≥ 0, hence
| c t evtMeWsUx-s | ≤ ∣ C ∣∣ι ∣ evtM^s Ux-s ∣∣∞
≤∣ c MeVt∣∞∣ M∣∞∣ews∣∞∣u∣∞∣ x-s ∣∞
.e-c2te-C2s∣x∣χ,	(13)
where . hides universal positive constants depending on parameters θ. Therefore
Ht(x； θ)| . e-C2t∣x∣χ ⇒ 恒t(•； θ)∣ . e-C21.	(14)
That is, the functional Ht(∙； θ) is bounded (i.e. continuous) with an exponentially-decayed
norm (as a function of t). Finally, by (13) and Lebesgue’s dominated convergence theorem,
the functional Ht(∙； θ) is also regular. The proof is completed.	□
B Universal approximation
In this section, we provide the proof of Theorem 4.1, i.e. the universal approximation
property of RNN encoder-decoders. As is stated in the main text, the key step is to utilise
the classical representation theorem, which helps us to reduce the approximation problem
of functionals to functions.
B.1 Preliminaries
First, we list the background definitions and notations used in the following theorems. Let
(X, A) be a measure space (with A as the σ-algebra of subsets of X).
•	The space X is called local ly compact if for any x ∈ X, x has a compact neighbour-
hood.
•	X is a Hausdorff space if all distinct points in X are pair-wisely separable by neigh-
bourhoods. That is, for any x, y ∈ X , there exists a neighbourhood ∆x of x and a
neighbourhood ∆y of y, such that ∆方 ∩ ∆y = 0.
•	The measure μ is called a finite measure, if it satisfies μ(X) < ∞. The measure
ν is called a σ-finite measure, if X can be covered with at most countably many
measurable sets with finite measure. That is, there are measurable sets {An }n∞=1 ⊂
A with ν(An) < ∞ for all n ∈ N+ , such that n∞=1 An = X . Obviously, a finite
measure is also σ-finite.
•	Let X = (-∞, 0]. For a measure μ on the measure space ((-∞, 0], A), μ is absolutely
continuous with respect to the Lebesgue measure ν, if for every measurable set A,
V (A) = 0 implies μ(A) = 0, which is written as μ《V.
Denote by C0(X) the linear space of continuous functions defined on X vanishing at infinity.
We have the following classical representation theorem.
Theorem B.1 (Riesz-Markov-Kakutani representation theorem). Let X be a local ly com-
pact Hausdorff space. For any continuous linear functional ψ on C0(X), there is a unique,
regular, countably additive and signed measure μ on X, such that
ψ(f) =
X
f (x) dμ(x),
∀f ∈ C0 (X),
(15)
13
Published as a conference paper at ICLR 2022
with IlψIl = |μ|(X). Here, |μ|(X) denotes the total variation of (the signed measure) μ,
which is defined as |μ|(X):= SupP £『1|μ(Ai)|, where P : X = Ui=I Ai is a Partition over
X with Ai ∈ A for all i = 1, 2,…，k.
Proof. Well-known, see e.g. Bogachev (2007) (CH 7.10.4).	□
Remark B.1. It is straightforward to verify that |μ|(X) = supA∈a(∣μ(A)| + |μ(Ac)|). Fur-
thermore, if μ has a density dμ∕dν with respect to a countably additive, nonnegative measure
ν, then we have |μ|(X) = ∣∣dμ∕dν∣Li(V).
To handle signed measures, the following Jordan decomposition theorem (Bogachev, 2007)
is necessary.
Theorem B.2. Let μ be a signed measure on the measure space (X, A). Then, there are two
mutually singular (non-negative) measures μ+ and μ- on (X, A), such that μ = μ+ 一 μ—.
Moreover, such a pair (μ+, μ-) is unique.
Based on this, we have the following proposition to characterise absolutely continuous signed
measures.
Proposition B.1. If μ and V are signed measures, then we have μ《V ⇔ μ+《V and
μ-《v.
We also need the following Radon-Nikodym theorem (Bogachev, 2007).
Theorem B.3. Let (X, A, v) be a σ-finite measure space, and let μ be a σ-finite signed
measure, such that μ 《V. Then there exists a unique measurable function f, such that
μ(A) = Af fdv for every measurable set A.
B.2 Proofs
Before we prove the universal approximation theorem (Theorem 4.1), we need some lemmas.
Lemma B.1. Let {Ht : t ≥ 0} be a family of linear, continuous and regular functionals
defined on X . Then there exists a integrable function ρ : [0, ∞)2 → Rd, i.e.
d
IlPIlLι([o,∞)2) := £ IlPiIlLι([o,∞)2) < ∞,	(16)
i=1
such that
Ht (x) = /	x-SP(t, s)ds,	∀x ∈ X.
0
(17)
In particular, we have ∣∣H∣∣ = ff∞ ∣∣Ht∣∣dt = ∣∣P∣∣Li([o,∞)2).
Proof. Obviously, (-∞, 0] is a locally compact Hausdorff space. For any t ≥ 0, since
Ht is linear continuous, according to the Riesz-Markov-Kakutani representation theorem
(Theorem B.1), there exists a unique, regular, countably additive and signed measure μt,
such that
Ht (x) = /	x；dμt (S), ∀x ∈ X,
-∞
(18)
with E d=ι | μt,i ∣((-∞, 0]) = I Ht ∣∣. We show that for any t ≥ 0, i = 1, 2, ∙∙∙ ,d, μt,i is
absolutely continuous with respect to V (the Lebesgue measure), i.e. μt,i《 V. According
to Theorem B.2 and Proposition B.1, one can assume μt,i to be non-negative without loss
of generality. Take a measurable set A ⊂ (-∞, 0] with V(A) = 0, the aim is to show
μt,i(A) = 0. Let AA = (-∞, 0] \ A. Since both A and AA are measurable, there exist
Kn ⊂ A, Kn ⊂ AA with Kn, Kn closed, such that μt,i(A \ Kn) ≤ 1 /n, μt,i(AA \ Kn) ≤ 1 /n
14
Published as a conference paper at ICLR 2022
and V(A/ \ Kn) ≤ 1 /n for any n ∈ N+. Fix any i ∈ {1, 2,…,d}, We construct the sequence
of input signals {x(n)}∞=1 as
0 0, s ≤ 0,j = i,
Xsn) =(0,	s ∈ Kn,j = i,
I1,	s ∈ Kn, j = i,
(19)
j = 1, 2,…，d,
which can then be continuously extended to (-∞, 0] by defining xSi := "(S K(S,KnS K,)∈
[0,1]. 1	…S…
We deduce that limn→∞ XSi = 0 for ν-a.e. s ≤ 0. In fact, let S := {s ≤ 0 : limn→∞ XSn =
0}, we have Kn U S since for any S ∈ Kn, x，? = 0. Hence, (-∞, 0] \ S U A U (A' \ Kn),
which gives V((-∞, 0] \ S) ≤ V(A) + V(A/ \ Kn) ≤ 1 /n → 0 as n → ∞. Due to the regularity
of Ht, we get limn→∞ Ht(x(n)) = 0. By (18) and (19), we have
d λ0
Ht(X(n)) =工 /	XSj dμt,j(S)
j =1 Jf
∕0
J -∞
χ Xn dμt,i(S)
++
JKn	JA \ Kn	Ja，\Kn
xS,i dμt,i ( S ) = μt,i ( Kn ) + 11 ,n + 12,n,
(20)
where	μt,i(Kn)	=	μt,i(A)	-	μt,i(A \	Kn)	∈	[μt,i(A)	-	1 /n,μt,i(A)],	and	111 ,n|	+
II2 ,n 1	≤	A\ \ Kn +	JA，\K ；	1 dμt,i ( S )	=	μt,i (A	\	Kn )	+	μt,i (A，\	Kn)	≤ 2/n, which gives
limn→∞ Ht(x(n)) = μt,i(A). Therefore, at,i(A) = 0.
Notice that ∣μt,i((-∞, 0])∣ ≤ ∣μt,i∣((-∞, 0]) ≤ ∣∣Ht∣∣ < ∞ for a.e. t ≥ 0 (since IlH∣∣ =
J0∞∣∣HtIdt < ∞), we get that ((-∞, 0], A, μt,i) is a finite measure space, and hence σ-finite.
Obviously, ((-∞, 0], A, ν) is a σ-finite measure space. According to the Radon-Nikodym
theorem (Theorem B.3), there exists a unique measurable function pt,i : (-∞, 0] → R, such
that μt,i (A) = JA pt,i (S)dν(S) for every measurable set A. Hence, we have
Z 0	∕∞
x；pt (S)dS = /	xZsp(t, S)dS,	Vx ∈ X,
(21)
with P(t, S) := pt(-S).	In addition, by Remark B.1, we have ∣μt,i∣((-∞, 0])
/-∞ ∣pt,i(S)∣dS, which gives
∣H∣
Λ∞	_ d	Λ∞
I	I Ht Il dt = f
0	i=1 0
d
∣ μt,i ∣((-∞, 0]) dt =	1 PiI L 1([0,∞)2)
i=1
Il P Il L 1([0,∞)2)∙
(22)
The proof is completed.
□
Based on this representation theorem, the problem of functional approximation is reduced
as function approximation. That is,
Λ∞	尸
IlH - H = Jo I Ht -H 11 dt = ^
sup IHt(x) - Ht(x; θ) Idt
1 UxuX ≤1
=
0
≤/
0
∙∞
∙∞
[
IIxllX ≤1 J0
sup
x—S (p(t, S) — p(t, S))dS dt
y*∞
sup /	Il x - S llæ! P (t,S) - p( t,S )I1 dSdt
IIxIlX ≤1 Jo
_ d r∞ r∞
≤y2 /	∣ pi (t,S) - P i (t,S )∣ dSdt,
i=1	0	0
(23)
1Here, d(s,B) := inf{∣S 一 a ∣ : a ∈ B} is the distance between a point S and a set B.
15
Published as a conference paper at ICLR 2022
i.e.
d
IlH -旬I ≤ IlP 一 ρ∖∖L 1([0,∞)2)：= 工 IlPi —P i Il L 1([0,∞)2)∙	(24)
i=1
Lemma B.2. Let P(t, S) : [0, ∞)2 → R with ∣∣P∣∣Li(Q∞)2) < ∞. Then for any e > 0, there
exists a polynomial p(u, v) =	jm=1,k=1 cjkujvk, such that
I I	| P (t, s) - p (e- ,e-S )| dtds < e.
00
(25)
Proof. Fix any t > 0. Consider the following transformation
R(Uv) = (u1vP(-lnu,-lnV), UN ∈ (0, 1]]	(26)
u, v = 0,	uv = 0.
This transformation preserves the norm with ∣∣P∣∣L 1(口∞)2) = ∣∣R∣∣L 1([0,1]2).
First, according to the density of continuous functions in Lp space (Rudin, 1987, Theorem
3.14), there exists R ∈ C([0, 1]2), such that ∣∣R - RIlLi([o,i]2) < t/2. Next, by the density of
polynomials in the space of continuous functions (Lorentz, 2005, Theorem 6), there exists
a polynomial q(u,v) = E；k=0 CjkUjvk, such that ∣∣R - q∣∣L∞([0,1]2) < t/2. Finally, let
p(u, v) = uvq(u, v), we have
R(u, v)-----—p(u, v) dudv
=IR — q Il L Yai]2)
≤ IR - RIL 1([0,1]2) + IlR - qIlL∞([0,1]2)
< t/2 + t/2 = t,	(27)
which completes the proof.	□
Γ Γ |P(t,s)-p(eτ,e-S)|dtds = 1 /
0	0	00
Now we are ready to prove the universal approximation theorem.
Proof of Theorem 4.1. According to the representation theorem (Lemma B.1), we have
Ht (X)= / x-sP(t, s)ds,	∀x ∈ X,
0
(28)
where ∣∣P∣∣Li(q∞)2) = ∣∣H∣∣ < ∞. Therefore, by Lemma B.2, there exists Pi(u, v)
E；k=1 cjkUjvk, i = 1, 2,...,d, where m is the maximal degree of {Pi}d=i, such that
d ∞∞
S / /
|Pi (t, s) - Pi (e-t
e-S) |dtds < t.
(29)
Let
c = U = 1m, V = W = - diag (1, 2, . . . , m),
W = diag( W, W,…，W) ∈ Rdm × dm, U = diag( u, u,…，u) ∈ Rdm × d,	(30)
M = PQ = (M1 ,M2,…，Md) ∈ Rm×dm, [Mi]jk = Cj?,
we get
P(t, S)τ = cτevtPQeWsU = cτevtMewsU
=cτevt ∙ (M 1, M2,…,Md) ∙ diag(ews, ews,…,ews) ∙ diag(u, u, ∙ ∙ ∙ ,u)
=(CTeVtM 1, cτevtM2,…,CTeVtMd) ∙ diag(ewsu, ewsu,…,ewsu)
=(CTeVtM1 ewsu, cτevtM22ewsu,…,cτevtMdewsu),	(31)
16
Published as a conference paper at ICLR 2022
with
ρi(t, S) = cɪeVtMieWsu = Pi(e-1, e-S),	i = 1, 2,...,d.
Therefore, by (24) and (29), we have
d
IlH -旬I ≤ ^ Ilρi - Pill L 1([0,∞)2)
d ∞∞
=4/ J IPi(t,s) — Pi(e-1, e-s)∣dsdt < e,
which completes the proof.
(32)
(33)
□
C General approximation rates
In this section, the proof of Theorem 4.2 is given. Again, by (24), the aim now is to
investigate the function approximation ∣∣P 一 ρ∣. Since one can handle each spatial dimension
separately (similarly with (30) and (31)), we firstly derive the estimates by assuming d = 1,
and then extend the obtained results to the case of multi-dimensional inputs (for general
d ∈ N+).
Conditions on representation. To characterise the accuracy of using the model
CTevtMeWsu (with M := PQ) to approximate the target P(t, S), the first stuff is to trans-
late the conditions on the output (of piece-wise constant signals) to the representation.
Recall that yc(t,s) = Ht(1(-∞,-s]), t,s ≥ 0, We get P(t,s) = — ∙dHt(1(-∞,-s]). Hence, the
assumptions on yc in Theorem 4.2 is equivalent to the following smoothness and exponential
decay conditions on P. That is, there exist α ∈ N+, β > 0 such that
P ∈ Cα([0, +∞)2),	(34)
∂k+l
eβ(t+S) ∂tk∂si P(t,s) = °(1) as 11(t,s)∣∣ → ∞, k,l ∈ N, k + l ≤ α∙	(35)
Note that the last decay condition implies
∂k+1
sup β-(k+1)eβ(t+s) ∂tk∂s1 P(t,s) ≤ Y, k,l ∈ N, k + l ≤ α	(36)
for some γ > 0.
C.1 Basics
Let Ω ∈ Rd be a bounded set. Define the spaces
Cα(Ω) :=	{f	∈	C(Ω) : Dif ∈ C(Ω) for all |i| ≤ α},	α ∈ N,	(37)
Cα,μ(Ω) :=	{f	∈	Cα(Ω) : |Dif (x) — Dif(y)| ≤ KIlx —	y∣∣μ for some	K > 0,
for all x, y ∈ Ω and |i| = α},	(38)
and the “norm”
I f Iα,μa := sup sup Dif (X)— IDfy)|, ∀f ∈ Cα,μ(Ω),	(39)
|i|= αx,y∈Ω	IlX ― y∣∣2
with the shorthand ∣∣ ∙ ∣∣ω := ∣∙∣o,o,ω∙
Theorem C.1 (Multivariate Jackson,s theorem (Schultz, 1969), Theorem 4.10). Let Ω ∈ Rd
be a regular, 2 bounded and open set, and f ∈ Cα,μ(Ω) for some α ∈ N, μ ∈ [0, 1]. Then for
any n ∈ N+, we have
网 I f - P Iω ≤ 4 I f I ɑ,μ,Ω,	(4。)
p∈Pn	n
2It is proved that every bounded, open and convex set is regular. See Morrey (1966) (Lemma
3.4.1).
17
Published as a conference paper at ICLR 2022
where Pd denotes the set of all polynomials with the degree of no more than n in each
variable, C(α,μ) > 0 is a universal constant only depending on α,μ and Ω.
A commonly used case is when μ = 0. That is, for f ∈ Cα(Ω), we get
I力α,0,Ω ≤ 2 max IlDi力1 L∞(Ω) < ∞.	(41)
|i|= α
For any x, x0 ∈ Ω, let p(x) := p(x) + f (x0) — p(x0), and e(x) := f (x) — p(x). Then P ∈ Pd,
and
If(x) — P(x)∣ ≤ Ie(x0)∣ + Ie(x)—2(X0)∣
≤ SUP_ e(X) ― e(y)l = ie10,0,Ω = IIf ―PllC, ∀X ∈ ω.	(42)
x,y ∈Ω
This gives the following convenient corollary.
Corollary C.1. Let Ω ∈ Rd be a regular, bounded and open set, and f ∈ Cα (Ω) for some
a ∈ N. Then for any n ∈ N+, there exists P ∈ Pd such that
C
Iif—pIiL∞(C) ≤ na max ∣∣Df lL∞m,	(43)
where Pd denotes the set of all polynomials with the degree of no more than n in each
variable, Ca > 0 is a universal constant only depending on a and Ω.
C.2 Proofs
Now we are ready to present the proof.
Proof of Theorem 4.2. Step 1: domain transform. Consider the transform from the infinite
domain [0, ∞)2 to the compact one [0, 1]2:
R(u V) = J UvP(-C0ln u, — C0ln V), u,v ∈ (0,1],
(,) = [0,	uv = 0,
(44)
where C0 := (α + 1)/β > 0 is a fixed constant. A straightforward computation by induction
shows that, for any k, l ∈ N, k + l ≤ a, and any u, v ∈ (0, 1],
∂k+1	(一 1)k+1 k l	. . ∂+j
k feθ l R(u, V) = 'k +L +1 工工C(k, i)C/(l,3)c0+ j-ξ7-g-P(—C0lnu, - C0ln V),	(45)
∂uk ∂vl	Uk +1 Vl+1J^2~W	∂tIdSj
where C(k, i), Cz(l,j) are some integer constants, and (t, s) = (—C0 ln u, — C0 ln V) is a one-
to-one mapping between (0, 1]2 and [0, ∞)2. By (36), we get
∂k +1	1 k l	. . ∂i + j
A kA IR(u,v) ≤ k+1 I+1 工£|C(k,i)||C'(l,j)lc0+j	jp(—C0lnu, -C0lnV)
∂uk ∂vl	uk +1 Vl+1J^2~W	∂tIdSj
∂k+1	t	S	(k +1), (l + 1) ʌ ʌ	∂i + j
dukdvR(eF,eF) ≤ e亏%FS ΣΣ ∣C(k,i)∣∣C'(l,j)∣端, 即P(t s)
i=0 j=0
k l	Ai + j
≤ EE IC (k,i )∣∣ C '(l,j)∣(α + 1)i+j ∙ β Ti+j) e∣β (t+S) ^^ P (t,s)
kl
≤ EE IC(k,i)∣∣Cz(l,j)∣(α + 1)i+jγ ≤ C(α)γ,	(46)
18
Published as a conference paper at ICLR 2022
where C(α) > 0 is a universal constant only depending on a. According to the decay
condition (35), we get
d+jρ(—C0 ln u, — Co ln v)	(卜+i) t (+i), ∂i+j	，	、
lim	dtds 八一0----,——0---'一 = lim eteS-----------------ρ(t s)
(%J→‰)	uk+1N+1	(t,s)→(tmo,+∞)e	e	∂ti∂sj P( t )
lim
(t,S )→( + ∞, + ∞)
0,
e¾2βte⅛≡iβs ∙ eβ(t+s)^^P(t S)
(47)
and similarly for Uo, Vo ∈ (0, 1],
∂⅞⅛ P(—C o ln S- oln Il * * V)
(u,v )→( U 0, o)	Uk +1 Vl + 1
(k +1) t (I + 1) S ∂i+j	/	、
lim	e C0 e C0	-——-p(t, s)
(t,s)→(-c0 ln u0, + ∞)	∂ti∂sj
Mt,s⅛→+∞ e阜％安βs ∙ eβ(t+S)"『人)
= 0,
∂∂j P (T o ln U,—C oln V ) = 0
(u,v)→(o,v0 )	Uk +1 Vl + 1	= .
This gives
∂k+1
ER(U)= °，(U) ∈ [0,1]×{0}u{0}× [0，1]，
and
Mo := max max
k,l∈N,k+1≤α (u,v)∈[o, 1]2
∂k+l
∂uk ∂v1 R (U,V)
≤ C(α)Y
(48)
(49)
(50)
(51)
by (46) and (50). Hence, R(u, v) ∈ Cα([0,1]2) with bounded derivatives.
Step 2: polynomial approximation. According to Corollary C.1 and (51), we obtain that
there exists Rn ∈ P2, such that
〜	Ca	∂k +1
IiR-RnIiL8([o，i]2)≤nakl∈ma+1=α 五砒7R(UN)
1 n k,l ∈n , k +1 ——a u u V V
≤ c≠,	Vn ∈ N+,
na
L ∞([o,1]2)
(52)
where C(ɑ) > 0 is a universal constant only related to a. Furthermore, let R2(u, v):=
R n (U, V ) — R n ( u, 0) — Rn (0, V ) + R 2 (0, 0), We get Rn ∈ P2 With Rn (U, 0) = Rn (0, V ) = 0 for
any u, v ∈ [0, 1]. By (50), we have R(u, V) = 0 for any (u, V) ∈ [0, 1] × {0} U {0} × [0, 1], then
Il R — Rn IlL∞ ([o,1]2) ≤ Il R — Rn Il L∞([o,1]2) + ∣∣ Rn — Rn ∣∣L∞([o,1]2)
≤ ^ R — Rn Il L∞([o,1]2) + Il Rn (u, 0) — R(u, 0)∣∣ L∞([o,1]2)
+ Il Rn (0, V) — R(0, V)∣∣ L∞([o,1]2) + Il Rn (0, 0) — R(0, 0)∣∣ L∞([o,1]2)
.Il R — Rn Il L∞([o,1]2),	(53)
i.e. we can further require the approximator satisfying the zero half-boundary condition
(i.e. vanishing on (u, v) ∈ [0, 1] × {0} U {0} × [0, 1]) without effecting the approximation
accuracy. Let Th := min{m^, ImD}, we get
Ilr — Rb∞([o,1]2) ≤ CaI ≤ c(α)Y (亲 + 亲),	(54)
m	∖mE	mD /
where
m m
R ：= Rm ∈ Pm,	R(u, v):= ΣΣrij UlVj.	(55)
i=1j=1
19
Published as a conference paper at ICLR 2022
Let
C =	1 m, U = 1 m, M = [rij]	∈ Rm×m,	(56)
V =	-diag(2, 3, ∙∙∙ ,τm + 1)/c o, W = -diag(2, 3,	∙∙∙ ,τm +	1)/c o,	(57)
then by (24) and (54), we have
IIH - B∣∣≤∣∣”^∣L 1([0,∞)2)	(58)
=Il P(t,S)- c t eVtMeWsu\\L 1([0,∞)2)
=ll P (ts )-e - C e - s 凤 e - C ,e - s )\L 1([0,∞)2)
=C0 \\R - Rl∖L 1([0,1]2)	(59)
≤C0wR -RIIL∞([o,i]2) ≤ C2α)γ ≤CaI(m+m)∙	(60)
That is, one can achieve an approximation accuracy scaling like (1 /m)α with Th2 parameters.
The proof is completed.	□
Remark C.1. The extension to multi-dimensional inputs (general d ∈ N+) is found in the
last paragraph of Appendix D.2.
Remark C.2. Recal l M = PQ ∈ RmD ×mE with P ∈ RmD ×N, Q ∈ RN ×mE , we only need
to investigate the case of N ≤ min{mE, mp} = Tm, since rank(M) ≤ Tm.
D Approximation rates via temporal product structure
In this section, the proof of Theorem 4.3 is provided.
D.1 Proper Orthogonal Decomposition
Proper orthogonal decomposition (POD; (Liang et al., 2002), (Berkooz et al., 1993), (Chat-
terjee, 2000)) is a method for model reduction, which is commonly applied to numerical
PDEs and fluids mechanics. It can be viewed as an extension of singular value decomposi-
tion (SVD) and principal component analysis (PCA) to infinite-dimensional spaces.
Fix any R ∈ L∞([0, 1]2). 3 Define the POD operator
K:
φ(v) →
R(u, V)φ(V)dv ∙ R(u, V)du,
φ(V) ∈ L2[0, 1]∙
(61)
Proposition D.1. The operator K is linear, bounded, compact, self-adjoint and non-
negative.
Proof. (i) The linearity is obvious.
(ii) Let
R : φ(v) → f R(u, v)φ(v)dv,	φ(v) ∈ L2 [0, 1],
0
then
(Kφ)(V) = / R(u,v)(Rφ)(u)du,	φ(V) ∈ L2[0,1]∙
0
By the Cauchy-Schwartz inequality, we get
IIRφIlL2[o,i] ≤ IlRlL2([0,i]2)1 φil2[o,i],
(62)
(63)
(64)
3Here, we use the same notation as (44), since the function defined there is also bounded.
20
Published as a conference paper at ICLR 2022
which gives
IIKφbL2[0,1] ≤ IlRnL2([0,1]2)HrφbL2[0,1] ≤ IlRbL2([0,1]2)HφHL2[0,1]∙	(65)
Note that R(u, v) ∈ L∞([0, 1]2) ⊂ L2([0, 1]2), hence both K and R are bounded operator
from L2 [0, 1] to itself.
(iii)	It is well-known that the Hilbert-Schmidt integral operator
(Cψ)(V) = [ R(u,v)ψ(U)du, ψ(U) ∈ L2[0, 1]
0
is a compact operator from L2 [0, 1] to itself. Therefore
Kφ =CRφ,	φ ∈ L2[0,1],
(66)
(67)
which gives K = CR. Since R is bounded and C is compact, we get K is also compact.
(iv)	By Fubini’s theorem, it is straightforward to verify that
R(u, w) (/ R(u, v)φ(v)dv) du ∙ ψ(W)dw
11[R(u, w)R(u, v)φ(V)ψ(w)dvdudw
R(U
1
R(u, W)ψ(W)dw du ∙ φ(v)dv
(φ, Kψ)L2[0,1],	φ,ψ ∈ L2[0,1]∙
(68)
(v)	By Fubini’s theorem, it is straightforward to verify that
The proof is completed.
(K φ,φφL 2[0,1]
(Rφ)(U)R(u, W)du ∙ φ(w)dw
(Rφ)(u)R(u, w)φ(w)dudw
(Rφ)(u)	R(u, w)φ(w)dw du
[(Rφ)2(U)du ≥ 0, φ ∈ L2[0, 1]∙
0
(69)
□
Combining (i)—(iv) and applying Hilbert-Schmidt’s expansion theorem, we obtain that
L2 [0, 1] has an orthonormal basis {φn}n∈N {ψξ}ξ∈Ξ, such that
•	Kφn = λnφn , λn = 0 for n ∈ N , and Kψξ = 0 for ξ ∈ Ξ, where N is a finite or
countable set. If N is not finite, we have limn→∞ λn = 0;
•	For any ψ ∈ L2 [0, 1], we have
ψ = £ (ψ, φn〉L2 [0,1]φn + £(ψ, ψξ)L 2[0,1] ψξ,	(70)
n∈N	ξ∈Ξ
where the second summation has at most countable non-zero terms, and
K ψ =	λn (ψ,φn〉L 2[0, 1] φn∙	(71)
n∈N
Here, all the series converge under the norm ∣∣ ∙ ∣∣L2[0,1]. Without loss of generality, N =
{1, 2,…,N0} for N0 ∈ N+ or N0 = +∞ (i.e. N = N+). By (69), We get
0 ≤ (Kφn, φn〉L2[0,1] = λn (φn, φnYL2[0, 1] = λn,	∀n ∈ N,	(72)
21
Published as a conference paper at ICLR 2022
i.e. all the eigenvalues of K are non-negative, and λn = 0 for n ∈ N implies λn > 0,
∀n ∈ N. In addition, limn→∞ λn = 0 implies that one can index all the eigenvalues in a
non-increasing sequence: λ 1 ≥ λ2 ≥ ∙∙∙ ≥ λn ≥ ∙∙∙ ≥ 0.
Then, we can present the POD estimate.
Theorem D.1. For any R ∈ L∞ ([0, 1]2), we have
/1
0
R ( u, V ) - £〈 R ( u, V ) ,φn ( V )〉L 2[0,1]φn ( V )
n=1
du =	λn,
L2[0,1]	n=N +1
∀N ∈ N.
(73)
Proof. Combining (69) and (72) gives
λn = (K φn, φn〉L 2[0,1] = / (R φn )2( U) du, ∀ n ∈ N.
0
(74)
Similarly,
L
1
(R ψξ )2( U ) du =(K ψξ ,ψξ〉L 2[0,1] = £ λn( ψξ ,φn〉L 2[0,1] =0,	∀ξ ∈ ξ ,
n∈N
(75)
which gives
(Rψξ)(U) =	R(U, V)ψξ(V)dV = 0, a.e. U ∈ [0, 1].
0
(76)
Notice that Ru(V) := R(u, V) ∈ Cα [0, 1] ⊂ L2 [0, 1] for any u ∈ [0, 1]. By (70) and (76), we
get
Il Ru IlL 2[0,1]=(
E (Ru, φn〉L2[0,1] φn + ∑( Ru,ψξ〉L2[0,1]ψξ,
n∈N
ξ∈Ξ
E (Ru, φn〉L2[0,1]φn + g( Ru,ψξ)L2[0,1]ψξ
=∑( Ru,φn〉L 2[0,1]+ ∑( Ru,ψξ〉L 2[0,1]
n∈N	ξ∈Ξ
=	(Rφn)2 (U),	a.e. U ∈ [0, 1].
n∈N
Hence for any N ∈ N, we have
(77)
N
Ru - Ru, Ru, φn〉L 2[0,1] φn
n=1
L2[0,1]
N
IlRuIlL2[0,1] -£(Ru, φn〉L2[0,1]
n=1
N
工(R φn )2( U )-工(R φn )2( U )
(78)
n∈N
N0
工(R Φn )2( U ),
n=N +1
n=1
(79)
2
where the summation is zero by convention if the subscript is larger than the superscript.
This by (74) implies
2
/1
0
R ( u,v ) - £( R ( u,v ) ,φn ( V )〉L 2[0,1] φn ( V )
n=1
L2[0,1]
N0
dU =	λn.
n=N+1
(80)
Here, the equality holds as a consequence of Beppo Levi’s monotone convergence lemma
and Lebesgue’s dominated convergence theorem, and one has n∈N λn < +∞. In fact, for
22
Published as a conference paper at ICLR 2022
N0 = +∞, let Sn = En=1 λk, we get Sn increasing (since λk ≥ 0 for all k ∈ N). By (74)
and (78), we have
n1
Sn =V / (Rφk)2(U)du
k=1 0
(Ru, φk〉L2[0, 1]du ≤ I	IlRUIlL2[0,1]du = Hr^L2([0, 1]2),
0
which gives that Sn converges as n → ∞. The proof is completed.
(81)
□
Remark D.1. Let ψn(U) := 〈R(u, v),φn(V)〉L2 [0, 1], then we have the POD estimate
R(u, V) ≈ En Ψn(U)φn(V), where the error is characterised by the tail sum of eigenvalues of
the POD operator.
Recall the POD operator defined in (61). We similarly define
K : φ (V) →
R(u, V)φ(V)dv ∙ R(u, V)du,	φ(V) ∈ L2[0, 1],
(82)
1	F> ∙	1 r> 1	/ r- r- ∖	」	1 ι ∙ j ι	ι	♦一
where R is defined as (55), i.e. the approximator constructed in the general approximation
theorem before. Obviously, as a polynomial, R ∈ L∞([0, 1]2). Hence, by Proposition D.1,
K is also linear, bounded, compact, self-adjoint and non-negative. In addition, according to
Theorem D.1, we have the following POD estimate
2
∕1
0
_	, ɔ- 、 K7
where {λn }N= 1
N
R( u, V ) - £〈R( u, V ), φ n ( V )〉L 2[0,1] φn ( V )
n=1
L 2[0,1]
N 〜
du = £ λ n,
n=N +1
are eigenvalues of K satisfying λ 1 ≥ λ2 ≥ ∙∙∙ ≥ λn ≥ ∙∙∙ ≥ 0, and {φn},
(83)
L2 [0, 1] are the corresponding orthonormal eigenfunctions, i.e. Kφn = λnφn, λn > 0 for
n ∈ {1, 2, ∙∙∙ , N0 }.
〜
Lemma D.1. K is a finite-rank operator. That is, No ≤ m = min{mE, m^} < ∞.
~
No L
n =1 ⊂
〜
Proof. Let 9 n (U) :=(R( u, v ) ,φ n (V ))L 2[0 1]. We first show that both 0 n (U) and φ n (V) are
polynomials. In fact, since R(u, V) = EZ1 Em=1 rijuivj, we have
∂k
τn	τn
为 R( u,v) = ɪɪ
∂uk
rij (i - k)!
Uii Vj
≤∑2∑>j∣
i=k j=1
i!
(i - k)!
,G(k,m, k =1, 2,…,
(84)
i = k j = 1
m m
〜
i!
with the convention that the summation is zero if the subscript is larger than the superscript,
i.e. C 1(k,τh) = 0 for any k > rh. Let C 1(rm) := max{∣RIlL∞([0,1]2), max1≤k≤m C 1(k, τn)},
then we have
〜
〜
∂ k =,	、〜一	,	. . -	...	— C - r	一 一
TT-T R( u, v ) φn ( V ) ≤ C 1(两)| φn ( V )∣ ∈ L [0, 1] ⊂ L [0, 1], k
∂uk
0, 1,…
According to Lebesgue,s dominated convergence theorem, we get by induction that
dk	dk 1	1 ∂k
duk 0 n ( U ) = d-k J R ( U,V ) φn ( V ) dv = J 巾 R ( U,V ) φn ( V ) dv, k = 0, 1,….
Similarly, we get
(85)
(86)
∂k
∂Vk R (u,v)
m m
ΣΣ r ij ui
m m
(j - k)!
Vbk
≤Σ2Σ> ij∣
i=1 j=k
j!
(j - k)!
=C2(k, rn),	k = 1, 2,…,
(87)
j!
23
Published as a conference paper at ICLR 2022
and C2(k,m) = 0 for any k > m. Let C2(m) := max{∣∣RllL∞([0,1]2), maxι≤k≤m C2(k, rmj)},
then for k = 0,1,…，we have
1 R( u,
0
〜
∂vk R(u,v)

≤ C2(m)|| Rll L ∞([0,1]2)ll φn Il L 1[0,1]
≤ C2(m)lφnIL2[0,1] = C2(m) ⊂ L 1[0, 1]∙	(88)
According to Lebesgue’s dominated convergence theorem, we get by induction that
dk
dvk φ n (V)
1	1 ∂ k	1
J- J ∂VkR(u, V) J R(u, V)φn (V)dvdu,
k = 0, 1, ∙∙∙.
(89)
That is,夕 n,φn ∈ C ∞[0,1] for any n = 1, 2,…，No. Since R( u, 0) = R(0, v) = 0 for
kk
u,v ∈ [0,1], We get 中n(0) = φn(0) = 0. Furthermore, We have 鼻中n(U) =嬴φn(v) = 0
C-	-	一 ~	—	—	、沁■-	一C「一一	一	一	一
for k > m, hence 夕n,φn ∈ Pm∙ Since {φn}N= 1 ⊂ L2[0, 1] are orthonormal, We must have
N ≤ Th < ∞. 4 The proof is completed.	□
D.2 Approximation rates
Perturbation of eigenvalues. First, We need to bound the gap betWeen the eigenvalues
,T 、仆C	-	、 NT~ ,	一.	一 C	- .C	一. ，..、、 一	…一
{λn }n=1 and {λn}nN=01 (corresponding to the function R defined in (44)). The folloWing
theorem is necessary.
Theorem D.2 (Courant-Fischer-Weyl min-max principle; Lax (2002) (Chapter 28, The-
orem 4)). Let B be a compact, self-adjoint operator on a Hilbert space H, whose positive
eigenvalues are listed in a decreasing order μι ≥ μ2 ≥ ∙∙∙ ≥ μk ≥ …> 0. Then
max min	(Bx, x∖h
Sk X∈Sk,11XIIh=i
μk,
(90)
where Sk ⊂ H is any k-dimensional linear subspace.
Based on it, We have the folloWing lemma to characterise the perturbation of singular values.
Lemma D.2. For any R1, R2 ∈ L∞([0, 1]2), we have the estimate
√λF - P
≤ IlR1 - r2∣∣L2([0,1]2).
(91)
Proof. According to Theorem D.2 and by (69), We have
λkR=
maX φ ∈s k ,∣m k,"=产 Rφ,φ φ L 2[0,1]
max min
S k φ ∈S k , 1 φ Il L 2[0, i] = 1
IIr Rφ lL 2[0,1].
(92)
Note that RR1 - RR2 = RR1 -R2 and by (64), We have
λkR1 = max	min
S	S k φ ∈S k , 1 φ Il L 2[0,1] = 1
≤ max	min
S k φ ∈S k , 1 φ Il L 2[0, i] = 1
≤ max	min
S k φ ∈S k , 1 φ Il L 2[0, i] = 1
≤ max	min
S k φ ∈S k , 1 φ Il L 2[0, i] = 1
IIR r 1φ l L 2[0,1]
(I(R R1 -R R 2 )φ Il L 2[0,1] + IIR R 2 φ l L 2[0,1])
(|R R1-R 2 1 + IIR R 2 φ 1 L 2[0,1])
IIR R 2 φ 1 L 2[0,1] + Il RI- R2∣∣ L 2([0,1]2)
λk2 + Ii r 1- R2∣ι L 2([0,1]2),
(93)
4In fact, for any P ∈ Pm with P(0) = 0, We have P ∈ span{v, v2, •…,vm}. Through a standard
Schmidt-orthogonalization, we can get ek (V) ∈ P k, k = 1,2, •…,m, such that 〈 ei,ej〉L 2[0,1]=
δij, and P ∈ span{e 1(V), e2(V),… ,em<j(V)}. That is, if〈p,q〉L2[0,1] = 0 for some p, q ∈ Pm with
p(0) = 0, q(0) = 0, then their coordinates under the basis {ek}m=ι are orthogonal. Hence, the NO
orthogonal m-dimenSiOnal coordinates here leads to at most m non-zeros.
24
Published as a conference paper at ICLR 2022
and similarly,
λR2k2 =max, lmin	R R 2φ b L 2[0,1]
Sk φ∈sk, IlφllL2[0,i] = l
≤ max'DUmin	_,("(R R 2—R R I) φ 11L 用。」]+ IlR R i φ∣ L 制。」〕)
S k φ ∈S k , Il 例 L 2[0,1] = 1
≤ max 一 Umin	I(IRR2-RJ + IIrr 1φil2[0,1])
S k φ ∈S k , Il 例 L 2[0,1] = 1
≤ maxa c lmin	JRR1φ 1L町。」1+ ιιr2 - RIIlL河。」〕2)
S k φ ∈S k , h φ h l 2[0,1] = 1
=J入R1 + IlR1 - R2lL2([0,1]2),
which completes the proof.
(94)
□
Proofs. Now We are ready to derive the final estimate.
Proof of Theorem 4.3. By Lemma D.2 and (54), we get
Vλk - ∖k^k I ≤ IlR - RIll2([0, 1]2) ≤ ιR - RllL∞([0, 1]2) ≤ a ^Y.
(95)
Combining (54), (83) and (95) gives that
1 Il	N
I	t	t	S	S
F Il P (t,s ) - 2_^e~ 访 0 n ( U c0 ) ∙ U 访 φn ( e c0 )
°0 Il	n =1
L 1([0, ∞)2)
N
R((U V ) - £ 0n ( U ) φn ( V )
n =1	L 1([0,1]2)
N
≤ Il R ( u,v ) - R( u,v )|| L 1([0,1]2) + R( u,v ) - £0 n ( U ) φ n ( V )
n =1	L 1([0,1]2)
≤ ll R ( u,v ) - R( u,v )l L ∞([0,1]2) +
N
R(U, V )-工 0n (U) Φn ( V )
n =1	L 2([0, 1]2)
≤ 叶 + J £ λ n ≤ 3 +
mα	∖	mα
n n=N +1
~ ~
N0	N0
£ | λn-λn | + £ λn
n = N +1	n=N +1
< C(α)Y
V C(α)Y
≤ -----
一 mα
V C(α)Y
≤ -----
一 mα
where < hides universal positive constants.
For the corresponding parameters, recall that 0n, φn
write
∈ Pm with 0n (0) = φn (0) = 0, we can
沅
0n (U) =工 Pinu1,
i =1
沅
Φn ( V ) = y^Qnj Vj
j = 1
(97)
25
Published as a conference paper at ICLR 2022
for any n = 1, 2,…，No. Let
c = 1 m,	U = 1 m,	(98)
V = -diag(2, 3, ∙∙∙ ,τm+ 1)/c o, W = -diag(2, 3, ∙∙∙ ,τm+ 1)/c o,	(99)
M = PQ with P = [Pin] ∈ Rm×N, Q = [Qnj] ∈ RN×m,	(100)
then we have
N rm	m
cτevtMewsu = cτevtP ∙ QeWsu =工工 e-i+1 tPn ∙工 Qnje-j+1 S
n=1 i=1	j=1
N
∑t	, t ,	s -~	, s ,	,
e~c0CPn(e~c0) ∙ e~c0φn(e~c0).	(101)
n=1
Plugging this into (96) gives
IlH -即∣≤∣∣P-PIL 1([0,∞)2)	(102)
=Il P (t, s ) - c τ eVtMeWsu∖∖ L ι([0, ∞)2)
.Cβα)γ[(1	+ JN0-N)	∙mα	+ [ 玄	λn	+ ∖ ∑ "•焉,
n=N +1	n=N +1
(103)
and the number of trainable parameters is 2Nm. Together with Lemma D.1, the proof is
completed.	口
Extension to multi-dimensional inputs. The above results can be naturally extended
to the general case where a d-dimensional input is given (∀d ∈ N+). In fact, let
Il Pi (t,s) - cτ evtMieWsu∖∖ L i([0, ∞)2) . e, i = 1, 2,…，d,	(104)
for some 0 < e《1, then We take
M = (M1 ,M2,…,Md) ∈ Rm×dm,	(105)
W = diag( W,W, ∙∙∙ ,W) ∈ R dm × dm, U = diag( u,u,…，u) ∈ R dm × d,	(106)
and have
cτ evtMewsU = CTeVt ∙ (M1 ,M2,…，Md) ∙ diag( eWs, eWs,…,ews) ∙ diag( u, u,…，u)
=(cτevtM 1 ,cτevtM22,…,cτevtMd) ∙ diag(ewsu, ewsu,…,ewsu)
=(CTeVtMIeWsu, cτeVtM2eWsu,…,cτeVtMdeWsu).	(107)
If the form PQ(= M) is required, it is sufficient to take Mi = PiQi, i = 1, 2, ∙∙∙ ,d, and
P = (Pι, P2,…，Pd) ∈ Rm×dN, Q = diag(Q1, Q2 ,…，Qd) ∈ RdN×dm.	(108)
Therefore, we obtain
d
∑ IlPi(t,s) - [cτeVtMeWsU]illL 1([0,∞)2) . de,	(109)
i=1
with the number of parameters increased by d-times compared to the corresponding one-
dimensional setting.
D.3 Case analysis
Bounds under different cases. Now we make the comparison between (102) and (58).
Recall that {λn}N= 1 is a positive decreased sequence (with limn→∞ λn = 0 and E∞=ι λn ≤
IlRIlL2([o 1]2) by (81), if No = +∞), and No ≤ m, we have the following cases.
26
Published as a conference paper at ICLR 2022
•	if No = o(m), we set N = No in (102) and get the same bound as (58), but the
number of parameters is only O(NQm) = o(Tn2);
•	if NQ = O(n), then (102) implies that
Il P (t, s ) - cτ eVtMeWSu\\L 1([Q,∞)2)
1
Irhα	十
m
工 λn + ∖
n=N +1	∖
Irh α1 2
.W ∣(1+B
m
工"∙
n=N +1
n = N +1
n=N +1
(110)
(111)
1
We give the following typical examples to illustrate sufficient conditions to guarantee
(111).
-If λn = O(n-r) with r > 2α + 1 ≥ 3(α ∈ N+), since
m	产	1/1	1	、
T n-r ‹	x-rdx =-- ------------r , ∀r> 1,	(112)
乙 一 Jn	r - 1 ∖Nr-1	Thr-1 广	, 人 /
n=N +1	JN	'	J
we get by (111) that
m	沅
Σ∖ 、	1	1	2 α — 1
Xn .工 nT . L . kɪ ⇔ N & nK,	(113)
n = N +1	n=N +1
m	m	1	1	α — 1
∑ " . ∑ n- r .4.W ⇔ N & n A.	(114)
Z__ J	Z__ J	N 2 1	mα 1
n=N +1	n=N +1
Assume that N 〜 mδ with δ ∈ [0, 1), then by (113) and (114), we require
δ ≥ max{2α-1, IBI}, i.e.
2 2 α - 1 fa - 1	2 α - 1	,、
r ≥ max < —内-----+ 1, 2 (—内--+1 H =—内-----+ 1.	(115)
Meanwhile, the POD-estimate (110) achieves an accuracy scaling like (1 /m)α-2
with O(m1++) parameters, while under the same capacity, the accuracy of
(58) scales like (1 /m)( 2~). The former beats the latter if a 一 ɪ > α(，δ),
i.e. δ < 1 一 1 /a (a ≥ 2). When δ = 1 一 1 /a, (115) becomes r ≥
max( 2α2-1,2(a + 1)} = 2α2-1. That is to say, r* := 2α2-1 can be viewed
α — 1 ,	α —1	,,	α — 1
as an upper bound of the critical point where the two estimates are compa-
rable. When r > r*, the POD-estimate outperforms the non-POD-estimate,
and this effect gets more notable with r increasing. In fact, when r > r*, we
2 α — 1	O-1.
take N = m r —1 > m 2—1 (hence satisfying (113) and (114)), which gives an
O((1 /m)α- 2) approximation error with O(m1+ r—1 ) parameters for the POD-
estimate, while O(m2-1) trainable parameters are needed to achieve the same
accuracy using the non-POD-estimate. Since 21-ɪ1 < 1 一 ∣, we get that the
POD-estimate outperforms the non-POD-estimate. When r → +∞, the num-
ber of trainable parameters for the POD-estimate is O(m), much better than
the non-POD-estimate.
27
Published as a conference paper at ICLR 2022
Remark D.2. Let
K(v, w) :=	R(u, v)R(u, w)du,	v, w ∈ [0, 1],
0
we get
(Kφ)(w) = f K(v, w)φ(v)dv,	φ ∈ L2[0, 1].
0
(116)
(117)
Recall thatR∈ Cα([0, 1]2) ⊂ L∞([0, 1]2), we getK ∈ L∞([0, 1]2) ⊂ L2([0, 1]2),
and hence K can be also viewed as a Hilbert-Schmidt integral operator with the
kernel K, where K is obviously symmetric as K(v, w) = K(w, v), and positive
definite since 0 ≤ (Kφ,φ)L2[0,1]= /1 /1 K(v, w)φ(v)φ(W)dvdw, φ ∈ L2[0, 1]
by (69). Since the derivatives of R (up to α-order) are continuous on [0, 1]2
(hence bounded and integrable), we get K ∈ Cα,α ([0, 1]2) (α-differentiable for
both arguments). According to Chang & Ha (1999) (Theorem 1), we have
∞
工 λn . N-α,	∀N ∈ N+ ⇒ λn = O(n-(α+1)), n → ∞.	(118)
n=N +2α+1
That is to say, this general setting (only assume smoothness of the kernel)
can not guarantee the sufficient condition provided here (λn = O(n-r) with
r > 2α-11), i.e. the point that the POD-estimate outperforms the non-POD-
estimate requires a faster decay of the eigenvalues.
-If λn = O(e-ωn) with ω > 0, we get by (111) that
m
λn.	e
n=N +1	n=N +1
-ωn
e-ω(N+1)
1 一 e -ω
1
m2 α-1
⇔ N & 1ln (E) - 1,
n=N +1
n=N +1
e-ω (N+1)
1 ― e - ω
m α-1
(119)
⇔ N & Im(二)- 1.
(120)
m
∞
∞
£ E . ∑ e -2 n
1
That is to say, for any a ∈ N+, ω > 0, we have N 〜(2a - 1) ln m. This
implies an O((1 /m)α- 1) approximation error with O(mlnm) parameters for
the POD-estimate, while O(m2-1) parameters are needed to achieve the same
accuracy using the non-POD-estimate.
- If N0 < +∞ (i.e. K is a finite rank operator by (71)), one can just take N = N0
and get by (110) an O((1 /m)α- 1) approximation error. For m ∈ N+ sufficiently
large such that m 〜 N0 for some K》1, the number of trainable parameters for
the POD-estimate is O(Nm) = O(N0+1), while O Nκ((2 α)) parameters are
needed to achieve the same accuracy using the non-POD-estimate. Obviously,
if a ≥ 2, we get K(2 一 1) ≥ ∣K》K + 1.
Eigenvalue approximation. One can apply (91) in Lemma D.2 to estimate the eigen-
values {λn}N= 1, where R1 = R and R2 is taken as some approximator of R, say R. Let
λ := λR, we recall (91) as
(121)
We provide a naive method here. That is, one can take Rasa piece-wise constant ap-
proximation of R. Fix any n ∈ N+. Let I := {0}, Ii := (i-1, i ] for i = 1, 2, ∙∙∙ ,n, then
{Ii ×Ijj} jo is the uniform partition over [0, 1]2. Let R(u,v):= Eh=° R(i, j)1i. ×iB (u, V).
28
Published as a conference paper at ICLR 2022
(n,j R
1ii ×I (u,v)φ(V)dv ∙ 1ii/ ×ij/ (u, w)du
gj Rl
1ii ×Ij (U, V)φ(V)dv ∙ 1ii ×Ik (u, W)du
Then for any φ ∈ L2 [0, 1], if W ∈ Ik, k = 0,1,…,n, we have
(KRΦ)(w)= / / R(u,v)Φ(V)dv ∙ R(u, w)du
nnn n
=ΣΣΣΣR
i=0 j=0 i/ = 0 j/=0
nn
=S三R
=S S R 1) R Gk)∙ 1 / φ(V)dv,	(122)
i=0 j=0	n n	n n n Ij
which is a constant only related to k. That is, KRφ is a piece-wise constant func-
tion, i.e. KRφ ∈ span{1iJn=0, or range(KR) ⊂ span{1iJn=0. Obviously, {1iJn=0
is an orthogonal set, which implies that the operator KR is of finite rank at most
n + 1. Let Rij := 1R(i-1, j-ɪ), i,j = 1, 2,…，n + 1, {σ^}忆1 be the singular value of
R := [RRij] ∈ R(n +1)×(n +1) and V ∈ R(n +1)×(n+1) with columns as the corresponding right
singular vectors. Set [e 1 ,e2,…,en, + 1] := [1或,1iι,…,1in] V, then by (122), we get for
l = 1, 2, ∙∙∙ ,n + 1,
(K R el)(W)=SSR( n,n)R( i, n) ∙ n2 Vj+1,
nn nn n
i=0 j=0
n+1 n+1
=SS RijR i,k+1 Vjl = [Rt R V-. ,^	,	(123)
i=1 j=1	k+1
which gives
nn
KRel = S [RTRV:,]k 11Ik = σl S Vk +1 ,l1Ik = σι el,	(124)
k=0	+	k=0
i.e. the set {σ2}n=1 collects the all the eigenvalues of KR, which can be obtained by the
SVD of R.
For the error estimate, it is straightforward to have
2
ʌ
R(u, v) — R(u, v)∣∣
L2([0,1]2)
n n i i j j
SSL L
i=1j=1	n n
R (u,v)- R^n^ dudv
n n pi ji.
≤ ss 广「
i=1 j=1 ∙⅛i j (u,v)∈[0,1]2
n n pi ji.
≤ SSL L 2C2(α)Y2
max
WR(u,v)112 ∣(u
• ±dudv
n2
n2
i
-,v
n
—U dudv
(125)
—
—
hence by (121), ∖√λk — σ^ ≤ ?C(Ia)Y for
any n ∈ N+ .
E Numerical settings
According to Lemma B.1, the target input-output temporal relationship has a Riesz repre-
sentation form. Under the discrete-time regime, we are supposed to set
T
Ht (x) = S P (t,s) %,	(126)
s=1
where T ∈ N+ is the path length.
29
Published as a conference paper at ICLR 2022
E.1 Settings of Figure 1
For the input, we generate 6 sequences using Gaussian i.i.d. random variables with the path
length T = 30. Hence, the output (target) is Ht(x) = E301 P(t,s)Xs.
The high rank target has the representation
P high( t,s ) = 1cos(t), t = s,	(127)
0,	t = s,
while the low rank target has the representation
99	1
ρlow(t, S) = ^^------- cos(nπt) cos(nπs).	(128)
n=0
E.2 Settings of Figure 2
Consider the target with the following representation
ts	t	s
ρ (t,s) = e -访 e -访 R (e -访,e - c0),	(129)
where
∞
R(u, V )=工 σnψn (U) φn ( V),	(130)
n=1
with both {ψk} and {φk} as orthonormal bases. In this way, We construct a target with
singular values {σk}. Under the discrete-time setting, we are supposed to use the following
linear, width-m RNN encoder-decoder
τ
Ht (x)=工 CTVtPQWST Ux(T - s),
s=1
(131)
where P ∈ Rm×N, Q ∈ RN×m with m = mD = mE.
Recall that the approximation error is derived as
,.__ .............. ʌ .. _ ʌ .. ,
IIH - Hn . IlR- ʃ?bLι([0,i]2) ≤ IlR- RiL2([0,i]2),	(132)
where R(u,v) := EN=I ⑦i(U)φi(V) ∈ Pm with ⑦n,φn ∈ Pm. In the numerical experiments,
we first construct an R(u, V), and then fit it with the polynomial R(u, V) using the method
of least squares. The norm ∣∣R 一 R∣∣L2([0,1]2) is used to evaluate the approximation error.
In the particular example reported in Figure 2, we set m = 128,4n (U) = ʌ/2sin(nπu)
and φn(V) = √2 sin(n∏V). The singular values are taken as: (a) σn = Jn , n ≤ F ;
0,	n > N0
(b) σn
n , n ≤ 0; (c) σn = n-2, with N0 = 2, 4, 6, 8. As an infinite sum, R is
0,	n > N0
constructed under a finite truncation with the first 50 terms.
E.3 Settings of Figure 3
We perform experiments on nonlinear targets to show that the insight of low rank approxi-
mation also holds in the nonlinear case.
Nonlinear target. We consider the forced Lorenz 96 system (Lorenz, 1996), which is an
important example of reduced order modelling for convection dynamics, with applications
in weather forecasting.
Mathematically, the system has K output variables {yk} and JK hidden variables {zj,k}
with k = 1, 2, . . . , K and j = 1, 2, . . . , J. The parameters K, J control the number of
30
Published as a conference paper at ICLR 2022
variables in the system, and can be viewed as a complexity measure. The input {xk } is an
external temporal forcing. The system satisfies the following dynamics
~t~ = -yk-1(yk-2 - yk +1) - yk + Xk - J ^^ zj,k,
j=1
dZj,k
dt
-zj+1,k(zj+2,k - zj-1,k) - zj,k + yk,
(133)
(134)
with cyclic indices yk+K = yk, zj,k+K = zj,k and zj+J,k = zj,k. Here, we take {xk} as
randomly generated input sequences with the path length 64. We have tested for several
cases with different parameters: i) J = 6 with K = 1, 5, 10, 20; ii) K = 5 with J =
5, 15, 25, 100.
Note that the forced Lorenz 96 system parameterizes a highly nonlinear functional.
Nonlinear model. We learn the ab ove system using RNN encoder-decoders with nonlin-
ear activations, i.e.
hs = σ(WEhs-1 + UExs + bE), v = σ(Qhτ + b1),
gt = σ(WDgt-1 + bD),	g0 = σ(Pv + b2),	(135)
ot =WOgt+bO,
where σ is the element-wise tanh activation. Let m = 128 be the hidden dimension, N =
1, 2, . . . , 32 be the size of the coding vector v, we have xs , ot, bO ∈ RK , hs , bE , bD , b2 ∈ Rm ,
WE, WD ∈ Rm×m, b1 ∈ RN, WO ∈ Rm×K, Q ∈ Rm×N, P ∈ RN×m. Note that we construct
the model with a fixed hidden dimension m but different N, thus only sizes of Q, P, b1, b2
are varying, while sizes of other parameters remain unchanged.
Training and initialisation. We denote the model with the coding vector size N as
EncDec(N) . We utilise the Adam optimiser and train from EncDec(1) to EncDec(32) . For
EncDec(1), we use a normal random initialisation, and train for 3000 epoches until a stable
error. For EncDec(N) with N > 1, we use the parameters trained from EncDec(N -1) as the
initialisation. For the parameters Q, P, b1 , b2 , we pad them to match the size of EncDec(N)
with normal distributions as initialisations.
It is shown that the low rank approximation phenomena discovered in the linear setting also
appears in this nonlinear case.
31