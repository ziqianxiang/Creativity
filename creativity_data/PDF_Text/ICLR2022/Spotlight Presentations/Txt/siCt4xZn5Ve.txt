Published as a conference paper at ICLR 2022
What Happens After SGD Reaches Zero Loss?
-A Mathematical Framework
Zhiyuan Li
Department of Computer Science
Princeton University
zhiyuanli@cs.princeton.edu
Sanjeev Arora
Department of Computer Science
Princeton University
arora@cs.princeton.edu
Tianhao Wang
Department of Statistics and Data Science
Yale University
tianhao.wang@yale.edu
Ab stract
Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the
key challenges in deep learning, especially for overparametrized models, where
the local minimizers of the loss function L can form a manifold. Intuitively, with a
sufficiently small learning rate η, SGD tracks Gradient Descent (GD) until it gets
close to such manifold, where the gradient noise prevents further convergence. In
such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases
a regularizer-like term, the sharpness of loss, tr[V2L]. The current paper gives a
general framework for such analysis by adapting ideas from Katzenberger (1991). It
allows in principle a complete characterization for the regularization effect of SGD
around such manifold—i.e., the ”implicit bias”—using a stochastic differential
equation (SDE) describing the limiting dynamics of the parameters, which is
determined jointly by the loss function and the noise covariance. This yields some
new results: (1) a global analysis of the implicit bias valid for η-2 steps, in contrast
to the local analysis of Blanc et al. (2020) that is only valid for η-1.6 steps and (2)
allowing arbitrary noise covariance. As an application, we show with arbitrary
large initialization, label noise SGD can always escape the kernel regime and only
requires O(κ ln d) samples for learning an κ-sparse overparametrized linear model
in Rd (Woodworth et al., 2020), while GD initialized in the kernel regime requires
Ω(d) samples. This upper bound is minimax optimal and improves the previous
O(κ2) upper bound (HaoChen et al., 2020).
1	Introduction
The implicit bias underlies the generalization ability of machine learning models trained by stochastic
gradient descent (SGD). But it still remains a mystery to mathematically characterize such bias. We
study SGD in the following formulation
Xη(k +1)= Xη(k) — η(VL(xη(k)) + √Ξ ∙ σξk (xη(k)))	(1)
where η is the learning rate (LR), L : RD → R is the training loss and σ(x) =
[σ1(x), σ2(x), . . . , σΞ(x)] ∈ RD×Ξ is a deterministic noise function. Here ξk is sampled uniformly
from {1, 2, . . . ,Ξ} and it satisfies Eξk [σξk (x)] = 0, ∀x ∈ Rd and k.
It is widely believed that large LR (or equivalently, small batch size) helps SGD find better minima.
For instance, some previous works argued that large noise enables SGD to select a flatter attraction
basin of the loss landscape which potentially benefits generalization (Li et al., 2019c; Jastrzebski et al.,
2017). However, there is also experimental evidence (Li et al., 2020b) that small LR also has equally
good implicit bias (albeit with higher training time), and that is the case studied here. Presumably
low LR precludes SGD jumping between different basins since under general conditions this should
require Ω(exp(1 /η)) steps (Shi et al., 2020). In other words, there should be a mechanism to reach
better generalization while staying within a single basin. For deterministic GD similar mechanisms
1
Published as a conference paper at ICLR 2022
(a) Taylor Expansion of VL
(b) Normal Space Dynamics
Figure 1: Illustration for limiting flow in R2. Γ is an 1D manifold of minimizers of loss L.
(c) Tangent Space Dynamics
have been demonstrated in simple cases (Soudry et al., 2018; Lyu & Li, 2019) and referred to as
implicit bias of gradient descent. The current paper can be seen as study of implicit bias of Stochastic
GD, which turns out to be quite different, mathematically.
Recent work (Blanc et al., 2020) shed light on this direction by analyzing effects of stochasticity in
the gradient. For sufficiently small LR, SGD will reach and be trapped around some manifold of local
minimizers, denoted by Γ (see Figure 2). The effect is shown to be an implicit deterministic drift in a
direction corresponding to lowering a regularizer-like term along the manifold. They showed SGD
with label noise locally decreases the sharpness of loss, tr[V2L], by Θ(η0.4) in η-1.6 steps. However,
such an analysis is actually local, since the natural time scale of analysis should be η-2, not η-1.6 .
The contribution of the current paper is a more general and global analysis of this type. We introduce
a more powerful framework inspired by the classic paper (Katzenberger, 1991).
1.1	Intuitive explanation of regularization effect due to SGD
We start with an intuitive description of the implicit regularization effect described in Blanc et al.
(2020). For simplification, we show it for the canonical SDE approximation (See Section B.1 for more
details) of SGD (1) (Li et al., 2017; Cheng et al., 2020). Here W(t) is the standard Ξ-dimensional
Brownian motion. The only property about label noise SGD we will use is that the noise covariance
σσ> (x) = V2L(X) for every X in the manifold Γ (See derivation in Section 5).
dX η (t) = -ηVL(Xη(t))dt + η∙σ (X η(t))dW (t)∙
(2)
C	-TZ- /rʌʌ ∙ -I	1	1	/	-I	1 ...	.,FZ■一 ɪɔ El	1 ∙	1	6 /
Suppose Xη (0) is already close to some local mιnιmιzer point X* ∈ Γ. The goal is to show Xη (t)
will move in the tangent space and steadily decrease tr[V2L]. At first glance, this seems impossible
as the gradient VL vanishes around Γ, and the noise has zero mean, implying SGD should be like
random walk instead of a deterministic drift. The key observation of Blanc et al. (2020) is that the
local dynamics of Xη (t) is completely different in tangent space and normal space — the fast random
walk in normal space causes Xη (t) to move slowly (with velocity Θ(η2)) but deterministically
in certain direction. To explain this, letting ∆(t) = Xη (t) - X*, Taylor expansion of (2) gives
d∆(t) ≈ -ηV2L(X*)∆dt + ησ(X*)dW(t), meaning ∆ is behaving like an Ornstein-Uhlenbeck
(OU) process locally in the normal space. Its mixing time is Θ(η-1) and the stationary distribution is
the standard multivariate gaussian in the normal space scaled by √η (see Figure 1b), because noise
covariance σσ> = V2L. Though this OU process itself doesn’t form any regularization, it activates
the second order Taylor expansion of VL(X* + ∆(t)), i.e., — 2∂2(VL)(X*)[∆(t), ∆(t)], creating
a Θ(η2) velocity in the tangent space. Since there is no push back force in the tangent space, the
small velocity accumulates over time, and in a longer time scale of Ω(η-1), the time average of the
stochastic velocity is roughly the same as the expected velocity when ∆ is sampled from its stationary
distribution. This simplifies the expression of the velocity in tangent space to
Figure 1c), where VT means the gradient is only taken in the tangent space.
η2VT tr[V2L] (see
However, the above approach only gives a local analysis for O(η-1.6) time, where the total movement
due to implicit regularization is O(η2-1.6) = O(η0.4) and thus is negligible when η → 0. In order
to get a non-trivial limiting dynamics when η → 0, a global analysis for Ω(η-2) steps is necessary
and it cannot be done by Taylor expansion with a single reference point. Recent work by Damian
et al. (2021) glues analyses of multiple local phases into a global guarantee that SGD finds a (, γ)-
stationary point for the regularized loss, but still doesn’t show convergence for trajectory when η → 0
and cannot deal with general noise types, e.g., noise lying in the tangent space of the manifold. The
2
Published as a conference paper at ICLR 2022
main technical difficulty here is that it’s not clear how to separate the slow and fast dynamics in
different spaces and how to only take limit for the slow dynamics, especially when shifting to a new
reference point in the Taylor series calculation.
1.2	Our Approach: Separating the Slow from the Fast
In this work, we tackle this problem via a different angle. First, since the anticipated limiting
dynamics is of speed Θ(η2), we change the time scaling to accelerate (2) by η-2 times, which yields
dXη (t) = -η-1VL(Xη (t))dt + σ(Xη (t))dW (t).	(3)
The key idea here is that we only need to track the slow dynamic, or equivalently, some projection of
X onto the manifold Γ, Φ(X). Here Φ : RD → Γ is some function to be specified and hopefully we
can simplify the dynamics (3) via choosing suitable Φ. To track the dynamics of Φ(Xη), we apply
Ito’s lemma (a.k.a. stochastic chain rule, see Lemma A.9) to Equation (3), which yields
dΦ(Xη(t)) = -η-1∂Φ(Xη(t))VL(Xη(t))dt + ∂Φ(Xη(t))σ(Xη(t))dW (t)
+ 1 XDTdijφ(Xn (t))(σ(xη ⑴》(XnD)T)ijdt.
2 i,j =1
Note the first term -η-1∂Φ(Xn)VL(Xn) is going to diverge to ∞ when η → 0, so a natural choice
for Φ is to kill the first term. Further note -∂Φ(X)VL(X) is indeed the directional derivative of Φ
at X towards -VL, killing the first term becomes equivalent to making Φ invariant under Gradient
Flow (GF) of -VL(X)! Thus it suffices to take Φ(X) to be the limit of GF starting at X. (Formally
defined in Section 3; see Lemma C.2 for a proof of ∂ Φ(X)VL(X) ≡ 0.)
Also intuitively Xn will be infinitely close to Γ, i.e., d(Xn(t), Γ) → 0 for any t > 0 as η → 0, so we
have Φ(Xn) ≈ Xn. Thus we can rewrite the above equation as
dXn(t) ≈ ∂Φ(Xn(t))σ(Xn(t))dW(t) + 1 XDTdijΦ(Xn(t))(σ(Xn(t))σ(Xn(t))>)jdt, (4)
2 i,j =1
and the solution of (4) shall converge to that of the following (in an intuitive sense):
dX (t) = ∂Φ(X (t))σ(X (t))dW (t) + 1 XD,	dij Φ(X (t))(σ(X (t))σ(X (t))>)jdt, (5)
2	i,j=1
The above argument for SDE was first formalized and rigorously proved by Katzenberger (1991). It
included an extension of the analysis to the case of asymptotic continuous dynamics (Theorem 4.1)
including SGD with infinitesimal LR, but the result is weaker in this case and no convergence is
shown. Another obstacle for applying this analysis is that 2nd order partial derivatives of Φ are
unknown. We solve these issues in Section 4 and our main result Theorem 4.6 gives a clean and
complete characterization for the implicit bias of SGD with infinitesimal LR in Θ(η-2) steps. Finally,
our Corollary 5.2 shows (5) gives exactly the same regularization as tr[V2L] for label noise SGD.
The main contributions of this paper are summarized as follows.
1.	In Section 4, we propose a mathematical framework to study the implicit bias of SGD with
infinitesimal LR. Our main theorem (Theorem 4.6) gives the limiting diffusion of SGD with LR η
for Θ(η-2) steps as η → 0 and allows any covariance structure.
2.	In Section 5, we give limiting dynamics of SGD with isotropic noise and label noise.
3.	In Section 6, we show for any initialization, SGD with label noise achieves O(κ lnd) sample
complexity for learning a κ-sparse overparametrized linear model (Woodworth et al., 2020). In
this case, the implicit regularizer is a data-dependent weighted `1 regularizer, meaning noise
can help reduce the norm and even escape the kernel regime. The O(κ lnd) rate is minimax
optimal (RaskUtti et al., 2012) and improves over O(κ2) upper bound by HaoChen et al. (2020).
In contrast, vanilla GD requires Ω(d) samples to generalize in the kernel regime.
For technical contributions, we rigorously prove the convergence of GF for OLM (Lemma 6.3),
unlike many existing implicit bias analyses which have to assume the convergence. We also prove
the convergence of limiting flow to the global minimizer of the regularizer (Lemma 6.5) by a
trajectory analysis via our framework. It cannot be proved by previous results (Blanc et al., 2020;
Damian et al., 2021), as they only assert convergence to stationary point in the best case.
3
Published as a conference paper at ICLR 2022
2	Related Works
Loss Landscape of Overparametrized Models A phenomenon known as mode connectivity has
been observed that local minimizers of the loss function of a neural network are connected by
simple paths (Freeman & Bruna, 2016; Garipov et al., 2018; Draxler et al., 2018), especially for
overparametrized models (Venturi et al., 2018; Liang et al., 2018; Nguyen et al., 2018; Nguyen, 2019).
Later this phenomanon is explained under generic assumptions by Kuditipudi et al. (2019). Moreover,
it has been proved that the local minimizers of an overparametrized network form a low-dimensional
manifold (Cooper, 2018; 2020) which possibly has many components. Fehrman et al. (2020) proved
the convergence rate of SGD to the manifold of local minimizers starting in a small neighborhood.
Implicit Bias in Overparametrized Models Algorithmic regularization has received great attention
in the community (Arora et al., 2018; 2019a; Gunasekar et al., 2018b;a;b; Soudry et al., 2018; Li
et al., 2018; 2020a). In particular, the SGD noise is widely believed to be a promising candidate for
explaining the generalization ability of modern neural networks (LeCun et al., 2012; Keskar et al.,
2016; Hoffer et al., 2017; Zhu et al., 2018; Li et al., 2019a). Beyond the size of noise (Li et al.,
2019c; Jastrzebski et al., 2017), the shape and class of the noise also play an important role (Wen
et al., 2019; Wu et al., 2020). It is shown by HaoChen et al. (2020) that parameter-dependent
noise will bias SGD towards a low-complexity local minimizer. Similar implicit bias has also been
studied for overparametrized nonlinear statistical models by Fan et al. (2020). Several existing
works (Vaskevicius et al., 2019; Woodworth et al., 2020; Zhao et al., 2019) have shown that for the
quadratically overparametrized linear model, i.e., w = u2 -v2 or w = uv, gradient descent/flow
from small initialization implicitly regularizes `1 norm and provides better generalization when the
groundtruth is sparse. This is in sharp contrast to the kernel regime, where neural networks trained by
gradient descent behaves like kernel methods (Daniely, 2017; Jacot et al., 2018; Yang, 2019). This
allows one to prove convergence to zero loss solutions in overparametrized settings (Li & Liang,
2018; Du et al., 2018; Allen-Zhu et al., 2019b;a; Du et al., 2019; Zou et al., 2020), where the learnt
function minimizes the corresponding RKHS norm (Arora et al., 2019b; Chizat et al., 2018).
Modelling Stochastic First-Order Methods with Ito SDE Apart from the discrete-time analysis,
another popular approach to study SGD is through the continuous-time lens using SDE (Li et al.,
2017; 2019b; Cheng et al., 2020). Such an approach is often more elegant and can provide fruitful
insights like the linear scaling rule (Krizhevsky, 2014; Goyal et al., 2017) and the intrinsic learning
rate (Li et al., 2020b). A recent work by Li et al. (2021) justifies such SDE approximation. Xie et al.
(2020) gave a heuristic derivation explaining why SGD favors flat minima with SDE approximation.
Wojtowytsch (2021) showed that the invariant distribution of the canonical SDE approximation
of SGD will collapse to some manifold of minimizers and in particular, favors flat minima. By
approximating SGD using a SDE with slightly modified covariance for the overparametrized linear
model, Pesme et al. (2021) relates the strength of implicit regularization to training speed.
3	Notation and Preliminaries
Given loss L, the GF governed by L can be described through a mapping φ : RD × [0, ∞) → RD
satisfying φ(x,t) = X — Rt VL(φ(x,s))ds. We further denote the limiting mapping Φ(x) =
limt→∞ φ(x, t) whenever the limit exists. We denote 1ξ ∈ RΞ as the one-hot vector where ξ-th
coordinate is 1, and 1 the all 1 vector. See Appendix A for a complete clarification of notations.
3.1	Manifold of local minimizers
Assumption 3.1. Assume that the loss L : RD → R is a C3 function, and that Γ is a (D - M)-
dimensional C2-submanifold of RD for some integer 0 ≤ M ≤ D, where for all x ∈ Γ, x is a local
minimizer of L and rank(V2L(x)) = M.
Assumption 3.2. Assume that U is an open neighborhood of Γ satisfying that gradient flow starting
in U converges to some point in Γ, i.e., ∀x ∈ U, Φ(x) ∈ Γ. (Then Φ is C2 on U by Falconer (1983).)
When does such a manifold exist? The vast overparametrization in modern deep learning is a major
reason for the set of global minimizers to appear as a Riemannian manifold (possibly with multiple
connected components), instead of isolated ones. Suppose all global minimizers interpolate the
4
Published as a conference paper at ICLR 2022
training dataset, i.e., ∀x ∈ RD, L(x) = minx0∈RD L(x0) implies fi(x) = yi for all i ∈ [n], then by
preimage theorem (Banyaga & Hurtubise, 2013), the manifold Γ := {x ∈ RD | fi (x) = yi , ∀i ∈ [n]}
is of dimension D - n if the Jacobian matrix [Vfι(x),..., Vfn(x)] has rank n for all X ∈ Γ. Note
this condition is equivalent to that NTK at x has full rank, which is very common in literature.
4	Limiting Diffusion of SGD
In Section 4.1 we first recap the main result of Katzenberger (1991). In Section 4.2 we derive the
closed-form expressions of ∂Φ and ∂2Φ. We present our main result in Section 4.3. We remark that
sometimes we omit the dependency on t to make things clearer.
4.1	Recap of Katzenb erger’ s Theorem
Let {An}n≥1 be a sequence of integrators, where each An : R → R is a non-decreasing function
with An(0) = 0. Let {Zn}n≥ι be a sequence of R旧l-valued stochastic processes defined on R.
Given loss function L and noise covariance function σ, we consider the following stochastic process:
Xn(t)
X(0)+Z t σ(Xn(s)dZn(s) + Z t
00
-VL(Xn(s))dAn(s)
(6)
In particular, when the integrator sequence {An}n≥1 increases infinitely fast, meaning that ∀ >
0, inf t≥0 (An (t + ) - An(t)) → ∞ as n → ∞, we call (6) a Katzenberger process.
One difficulty for directly studying the limiting dynamics of Xn (t) is that the point-wise limit as
n → ∞ become discontinuous at t = 0 if X(0) ∈/ Γ. The reason is that clearly limn→∞ Xn(0) =
X (0), but for any t > 0, since {An}n≥1 increases infinitely fast, one can prove limn→∞ Xn(t) ∈ Γ!
To circumvent this issue, we consider Yn(t) = Xn(t) - φ(X(0), An(t)) + Φ(X(0)). Then for each
n ≥ 1, we have Yn (0) = Φ(X (0)) and limn→∞ Yn (t) = limn→∞ Xn (t). Thus Yn(t) has the same
limit on (0, ∞) as Xn(t), but the limit of the former is further continuous at t = 0.
Theorem 4.1 (Informal version of Theorem B.7, Katzenberger 1991). Suppose the loss L, manifold Γ
and neighborhood U satisfies Assumptions 3.1 and 3.2. Let {Xn}n≥1 be a sequence of Katzenberger
process with {An}n≥1, {Zn}n≥1. Let Yn(t) = Xn(t) - φ(X (0), An(t)) + Φ(X0). Under technical
assumptions, it holds that if (Yn, Zn) converges to some (Y, W) in distribution, where {W (t)}t≥0 is
the standard Brownian motion, then Y stays on Γ and admits
Y(t)
Y(0)+
0
∂Φ(Y)σ(Y)dW (s)
1D t
+ 2 Xij=J djφ(Y)(σ(Y)σ(Y)>)".
(7)
Indeed, SGD (1) can be rewritten into a Katzenberger process as in the following lemma.
Lemma 4.2. Let {ηn}n∞=1 be any positive sequence with limn→∞ ηn = 0, An ⑴=ηnbt∕ηnC,
and Zn(t) = ηn Pk=?」√Ξ(1ξk — Ξξ 1), where ξ1,ξ2,…仪 Unif([Ξ]). Then with the Same
initialization Xn(0) = xηn (0) ≡ X (0), Xn(kηn2) defined by (6) is a Katzenberger process and is
equal to xηn (k) defined in (1) with LR equal to ηn for all k ≥ 1. Moreover, the counterpart of (7) is
Y(t)
Φ(X(0))+ / ∂Φ(Y)σ(Y)dW(s) + 1∕ ∂2Φ(Y)[Σ(Y)]ds,
(8)
where Σ ≡ σσ> and {W (t)}t≥0 is a Ξ-dimensional standard Brownian motion.
However, there are two obstacles preventing us from directly applying Theorem 4.1 to SGD. First,
the stochastic integral in (8) depends on the derivatives of Φ, ∂Φ and ∂ij Φ, but Katzenberger (1991)
did not give their dependency on loss L. To resolve this, we explicitly calculate the derivatives of Φ
on Γ in terms of the derivatives of L in Section 4.2.
The second difficulty comes from the convergence of (Yn , Zn ) which we assume as granted for
brevity in Theorem 4.1. In fact, the full version of Theorem 4.1 (see Theorem B.7) concerns the
stopped version of Yn with respect to some compact K ⊂ U, i.e., Yμn(K)(t) = Yn(t ∧ μn(K))
where μn(K) is the stopping time of Yn leaving K. As noted in Katzenberger (1991), we need the
5
Published as a conference paper at ICLR 2022
convergence of μn(K) for Yμn(K) to converge, which is a strong condition and difficult to prove in
our cases. We circumvent this issue by proving Theorem B.9, a user-friendly interface for the original
theorem in Katzenberger (1991), and it only requires the information about the limiting diffusion.
Building upon these, we present our final result as Theorem 4.6.
4.2	Closed-Form expression of the limiting diffusion
We can calculate the derivatives of Φ by relating to those of L. Here the key observation is the
invariance of Φ along the trajectory of GF.
Lemma 4.3. For any x ∈ Γ, ∂Φ(x) ∈ RD×D is the projection matrix onto tangent space Tx (Γ).
To express the second-order derivatives compactly, we introduce the notion of Lyapunov operator.
Definition 4.4 (Lyapunov Operator). For a symmetric matrix H, we define WH = {Σ ∈ RD×D |
Σ = Σ>,HHt∑ = Σ = ΣHH*} and Lyapunov Operator LH : WH → WH as LH(Σ)=
H>Σ + ΣH. It’s easy to verify L-H1 is well-defined on WH.
Lemma 4.5. Let x be any point in Γ and Σ = Σ(x) = σσ> (x) ∈ RD×D be the noise covariance
at x1. Then Σ can be decomposed as Σ = Σk + Σ⊥ + Σk,⊥ + Σ⊥,k, where Σk := ∂ΦΣ∂Φ,
Σ⊥ := (ID - ∂Φ)Σ(ID - ∂Φ) and Σk,⊥ = Σ>⊥,k = ∂ΦΣ(ID - ∂Φ) are the noise covariance in
tangent space, normal space and across both spaces, respectively. Then it holds that
∂2Φ[∑] =(V2L)S2(VL) [∑∣∣] -∂Φ∂2(VL) [Lv2l(Σ⊥)] +2∂Φ∂2(VL) [(V2L)t∑⊥j∣] . (9)
4.3	Main Result
Now we are ready to present our main result. It’s a direct combination of Theorem B.9 and Lemma 4.5.
Theorem 4.6. Suppose the loss function L, the manifold of local minimizer Γ and the open neighbor-
hood U satisfy Assumptions 3.1 and 3.2, and xη (0) = x(0) ∈ U for all η > 0. If SDE (10) has a
global solution Y with Y(0) = x(0) and Y never leaves U, i.e., P[Y (t) ∈ U, ∀t ≥ 0] = 1, then for
any T > 0, xη(bT /η2c) converges in distribution to Y(T) as η → 0.
dY(t) = ∑2(Y)dW(t) + 2V2L(Y户∂2(VL)(Y) [∑k (Y)] dt
'---------------} |________________________}
^^^^{^^^^^
Tangent Noise
1---------------------7-------------
Tangent Noise Compensation
+ 2 ∂ Φ(Y) ( ∂2(VL)(Y) [v2 L(Y 户 ∑⊥,k(Y)], - ∂2(VL)(Y) [lJ(Σ⊥(Y ))],) dt,
(10)
Mixed Regularization
where Σ ≡ σσ> and Σk , Σ⊥, Σ⊥,k are defined in Lemma 4.5.
^^^^^≡^^{^^^^^^^^≡
Normal Regularization
Based on the above theorem, the limiting dynamics of SGD can be understood as follows: (a) the
tangent noise, ∑j2(Y)dW(t), is preserved, and the second term of (10) can be viewed as the
necessary tangent noise compensation for the limiting dynamics to stay on Γ. Indeed, Lemma C.7
shows that the value of the second term only depends on Γ itself, i.e., it’s same for all loss L which
locally defines the same Γ. (b) The noise in the normal space is killed since the limiting dynamics
always stay on Γ. However, its second order effect (Ito correction term) takes place as a vector field
on Γ, which induces the Noise Regularization and Mixed Regularization term, corresponding to
the mixed and normal noise covariance respectively.
Remark 4.7. In Appendix B.4 we indeed prove a stronger version of Theorem 4.6 that the sample
paths of SGD converge in distribution, i.e., let xη (t) = xη([t∕η2C), then Xn weakly converges to
Y on [0, T]. Moreover, we only assume the existence ofa global solution for ease of presentation.
As long as there exists a compact K ⊆ Γ such that Y stays in K on [0, T] with high probability,
Theorem B.9 still provides the convergence of SGD iterates (stopped at the boundary of K) before
time T with high probability.
5	Implications and Examples
In this section, we derive the limiting dynamics for two notable noise types, where we fix the expected
loss L and the noise distribution, and only drive η to 0. The proofs are deferred into Appendix C.3.
1For notational convenience, we drop dependency on x.
6
Published as a conference paper at ICLR 2022
Type I: Isotropic Noise. Isotropic noise means Σ(x) ≡ ID for any x ∈ Γ (Shi et al., 2020). The
following theorem shows that the limiting diffusion with isotropic noise can be viewed as a Brownian
Motion plus Riemannian Gradient Flow with respect to the pseudo-determinant of V2L.
Corollary 5.1 (Limiting Diffusion for Isotropic Noise). If Σ ≡ ID on Γ, SDE (10) is then
dY(t) = ∂Φ(Y)dW + 1 V2L(Y)t∂2(VL)(Y)[∂Φ(Y)] dt — 1 ∂Φ(Y)V(ln ∣V2L(Y)∣ + )dt (11)
|
{^^^^^^^^^^^—
Brownian Motion on Manifold
/ I
}
^^^^^{^^^^^^^^™
Normal Regularization
where ∣V2L(Y)∣+ = limα→o 1[-,!1()+0!?)) is the pseudo-determinant of V2L(Y). ∣V2L(Y)∣+ is
also equal to the sum of log of non-zero eigenvalue values of V2 L(Y).
Type II: Label Noise. When doing SGD for '2-regression on dataset {(zi, yi)}n=ι, adding label
noise (Blanc et al., 2020; Damian et al., 2021) means replacing the true label at iteration k, yik,
by a fresh noisy label 勾彼卜:=yik + δk, where δk i% Unif{-δ, δ} for some constant δ > 0. Then
the corresponding loss becomes 11 (九(x) 一 yik)2, where 九卜(x) is the output of the model with
parameter x on data zik . So the label noise SGD update is
xk+1 = Xk 一 η/2 ∙ Vx (fik (Xk ) 一 yik + δik ) = Xk 一 η(fik (Xk ) 一 yik + δk ) Vxfik (Xk ). (12)
Suppose the model can achieve the global minimum of the loss L(X) := ɪE[(fi(x) 一 yi)2] at x*,
then the model must interpolate the whole dataset, i.e., fi (x*) = yi for all i ∈ [n], and thus here the
manifold Γ is a subset of {X ∈ RD | fi (X) = yi, ∀i ∈ [n]}. Here the key property of the label noise
used in previous works is Σ(x) = δ- pn=ι Vxfi(X)Vxfi(X)T = δ2 V2L(x). Lately, Damian et al.
(2021) further generalizes the analysis to other losses, e.g., logistic loss and exponential loss, as long
as they satisfy Σ(X) = cV2L(X) for some constant c > 0.
In sharp contrast to the delicate discrete-time analysis in Blanc et al. (2020) and Damian et al. (2021),
the following corollary recovers the same result but with much simpler analysis - taking derivatives
is all you need. Under our framework, we no longer need to do Taylor expansion manually nor
carefully control the infinitesimal variables of different orders together. It is also worth mentioning
that our framework immediately gives a global analysis of Θ(η-2) steps for SGD, far beyond the
local coupling analysis in previous works. In Section 6, we will see how such global analysis allows
us to prove a concrete generalization upper bound in a non-convex problem, the overparametrized
linear model (Woodworth et al., 2020; HaoChen et al., 2020).
Corollary 5.2 (Limiting Flow for Label Noise). If Σ ≡ cV2L on Γ for some constant c > 0,
SDE (10) can be simplified into (13) where the regularization is from the noise in the normal space.
dY(t) = -1/4 ∙ ∂Φ(Y(t))Vtr[cV2L(Y(t))]dt.	(13)
6	Provab le Generalization B enefit with Label Noise
In this section, we show provable benefit of label noise in generalization using our framework
(Theorem B.7) in a concrete setting, the overparametrized linear models (OLM) (Woodworth
et al., 2020). While the existing implicit regularization results for Gradient Flow often relates the
generalization quality to initialization, e.g., Woodworth et al. (2020) shows that for OLM, small
initialization corresponds to the rich regime and prefers solutions with small `1 norm while large
initialization corresponds to the kernel regime and prefers solutions with small `2 norm, our result
Theorem 6.1 surprisingly proves that even if an OLM is initialized in the kernel regime, label noise
SGD can still help it escape and then enter the rich regime by minimizing its weighted `1 norm. When
the groundtruth is κ-sparse, this provides a O(K ln d) vs Ω(d) sample complexity separation between
SGD with label noise and GD when both initialized in the kernel regime. Here d is the dimension of
the groundtruth. The lower bound for GD in the kernel regime is folklore, but for completeness, we
state the result as Theorem 6.7 in Section 6.3 and append its proof in Appendix D.6.
Theorem 6.1. In the setting ofOLM, suppose the groundtruth is K-sparse andn ≥ Ω(κ ln d) training
data are sampled from either i.i.d. Gaussian or Boolean distribution. Then for any initialization Xinit
(except a zero-measure set) and any > 0, there exist η0 , T > 0 such that for any η < η0, OLM
trained with label noise SGD (12) with LR equal to η for bT /η2c steps returns an -optimal solution,
with probability of 1 — e-Q(n) over the randomness of the training dataset.
7
Published as a conference paper at ICLR 2022
The proof roadmap of Theorem 6.1 is the following:
1.	Show Assumption 3.1 is satisfied, i.e., the set of local minimizers, Γ, is indeed a manifold
and the hessian V2L(χ) is non-degenerate on Γ (by Lemma 6.2);
2.	Show Assumption 3.2 is satisfied, i.e., Φ(U) ⊂ Γ (by Lemma 6.3);
3.	Show the limiting flow (13) converges to the minimizer of the regularizer (by Lemma 6.5);
4.	Show the minimizer of the regularizer recovers the groundtruth (by Lemma 6.6).
Our setting is more general than HaoChen et al. (2020), which assumes w* ∈ {0,1}d and their
reparametrization can only express positive linear functions, i.e., w = u2. Their Oe(κ2) rate is
achieved with a delicate three phase LR schedule, while our O(κ ln d) rate only uses a constant LR.
Setting: Let {(zi, yi)}i∈n be the training dataset where zι,...,zn i蚓 Unif({±1}d) or N(0, Id)
and each yi = hzi, w*i for some unknown w* ∈ Rd. We assume that w* is κ-sparse for some κ < d.
Denote x = uv ∈ RD = R2d, and we will use x and (u, v) exchangeably as the parameter of
functions defined on RD in the sequel. For each i ∈ [n], define fi(x) = fi(u, v) = zi>(u2 - v2).
Then we fit {(zi, yi)}i∈[n] with an overparametrized model through the following loss function:
L(X) = L(u,v) = n Ei=/(U,"), where 'i(u,v) = 2(fi(u,v) - y)).	(14)
It is straightforward to verify that V2L(x) = 4 PZi (：：CX) (：：CX)>, ∀x ∈ Γ. For simplicity, we
define Z = (z1, . . . , zn)> ∈ Rn×d and Y = (y1, . . . , yn)> ∈ Rn. Consider the following manifold:
Γ= {x = (u>,v>)> ∈ U : Z (uθ2 — vθ2) = Y} , where U =(R \ {0})D.	(15)
We verify that the above loss function L and manifold Γ satisfy Assumption 3.1 by Lemma 6.2, and
that the neighborhood U and Γ satisfy Assumption 3.2 by Lemma 6.3.
Lemma 6.2. Consider the loss L defined in (14) and manifold Γ defined in (15). If data is full
rank, i.e., rank(Z) = n, then it holds that (a). Γ is a smooth manifold of dimension D - n; (b).
rank(V2L(x)) = n for all x ∈ Γ. In particular, rank(Z) = n holds with probability 1 for Gaussian
distribution and with probability 1 - cd for Boolean distribution for some constant c ∈ (0, 1).
Lemma 6.3. Consider the loss function L defined in (14), manifold Γ and its open neighborhood
defined in (15). For gradient flow ddtt = -VL(Xt) starting at any xo ∈ U, it holds that Φ(x0) ∈ Γ.
Remark 6.4. In previous works (Woodworth et al., 2020; Azulay et al., 2021), the convergence of
gradient flow is only assumed. Recently Pesme et al. (2021) proved it for a specific initialization, i.e.,
uj = vj = α, ∀j ∈ [n] for some α > 0. Lemma 6.3 completely removes the technical assumption.
Therefore, by the result in the previous section, the implicit regularizer on the manifold is R(X) =
tr(Σ(X)) = tr(δ2V2L(X)). Without loss of generality, we take δ = 1. Hence, it follows that
R(X) = 4 χD=ι (Xn=IzRE + j	(W
The limiting behavior of label noise SGD is described by a Riemannian gradient flow on Γ as follows:
dxt = -1/4 ∙ ∂Φ(xt)VR(xt)dt, with xo = Φ(xinit) ∈ Γ.	(17)
*、
The goal is to show that the above limiting flow will converge to the underlying groundtruth x* = Q*)
where (u*,v*) = ([w*]+1/2, [-w*]Θ1/2).
6.1	Limiting Flow Converges to Minimizers of Regularizer
In this subsection we show limiting flow (13) starting from anywhere on Γ converges to the minimizer
of regularizer R (by Lemma 6.5). The proof contains two parts: (a) the limiting flow converges;
(b) the limit point of the flow cannot be sub-optimal stationary points. These are indeed the most
technical and difficult parts of proving the O(κ ln d) upper bound, where the difficulty comes from
the fact that the manifold Γ is not compact, and the stationary points of the limiting flow are in fact all
located on the boundary of Γ. However, the limiting flow itself is not even defined on the boundary
of the manifold Γ. Even if we can extend ∂Φ(∙)VR(∙) continuously to entire RD, the continuous
extension is not everywhere differentiable.
Thus the non-compactness of Γ brings challenges for both (a) and (b). For (a), the convergence
for standard gradient flow is often for free, as long as the trajectory is bounded and the objective is
8
Published as a conference paper at ICLR 2022
analytic or smooth and Semialgebraic. The latter ensures the so-called KUrdyka-LojasieWicz (KL)
inequality (Lojasiewicz, 1963), which implies finite trajectory length and thus the convergence.
HoWever, since our floW does not satisfy those nice properties, We have to shoW that the limiting floW
satisfies Polyak-LojasieWicz condition (a special case of KL condition) (Polyak, 1964) via careful
calculation (by Lemma D.16).
For (b), the standard analysis based on center stable manifold theorem shoWs that gradient descent/floW
converges to strict saddle (stationary point With at least one negative eigenvalue in hessian) only for a
zero-measure set of initialization (Lee et al., 2016; 2017). HoWever, such analyses cannot deal With
the case Where the floW is not differentiable at the sub-optimal stationary point. To circumvent this
issue, We prove the non-convergence to sub-optimal stationary points With a novel approach: We shoW
that for any stationary point x, Whenever there exists a descent direction of the regularizer R at x, We
can construct a potential function Which increases monotonically along the floW around x, While the
potential function is equal to -∞ at x, leading to a contradiction. (See proof of Lemma 6.5.)
Lemma 6.5. Let {xt}t≥0 ⊆ RD be generated by the flow defined in (17) with any initialization
xo ∈ Γ. Then x∞ = limt→∞ Xt exists. Moreover, x∞ = x* is the optimal solution of (18).
6.2	Minimizer of the Regularizer Recovers the Sparse Groundtruth
Note 1 Pn=1 z2,j = 1 when z%,j iid Unif{-1,1}, and we can show minimizing R(x) on Γ, (18),
is equivalent to finding the minimum `1 norm solution of Equation (14). Standard results in sparse
recovery imply that minimum `1 norm solution recovers with the sparse groundtruth. The gaussian
case is more complicated but still can be proved with techniques from Tropp (2015).
minimize R(X) = n Xd=1 (Xn=Iz2J (U2 + Vj)，
(18)
subject to Z(u2 - v2) = Zw*.
Lemma 6.6. Let zɪ ,...,zn ''吟 Unif({±1}d) or N(0, Id). Then there exist some constants C,c> 0
such that f n ≥ CK ln d, then with probability at least 1 — e-cn, the optimal solution of (18), (U, v),
is unique up to sign flips of each coordinate and recovers the groundtruth, i.e., Uθ2 — Vθ2 = w*.
6.3	Lower Bound for Gradient Descent in the Kernel Regime
In this subsection we show GD needs at least Ω(d) samples to learn OLM, when initialized in the
kernel regime. This lower bound holds for all learning rate schedules and numbers of steps. This is in
sharp contrast to the O(κ lnd) sample complexity upper bound of SGD with label noise. Following
the setting of kernel regime in (Woodworth et al., 2020), we consider the limit of U0 = v0 = α1,
with α → ∞. It holds that fi(uo, vo) = 0 and Vfi(U0,v0) = [αzi, -azi] for each i ∈ [n].
Standard convergence analysis for NTK (Neural Tangent Kernel, Jacot et al. (2018)) shows that upon
convergence, the distance traveled by parameter converges to 0, and thus the learned model shall
converge in function space, so is the generalization performance. For ease of illustration, we directly
consider the lower bound for test loss when the NTK is fixed throughout the training.
Theorem 6.7. Assume zι,...,zn ”吟 N(0, Id) and y% = z>w*, for all i ∈ [n]. Define the loss
with linearized model as L(x) = in=1 (fi (x0) + hVfi(x0), x - x0i - yi)2, where x = uv and
x0 = uv0 = α 11 . Then for any groundtruth w*, any learning rate schedule {ηt}t≥1, and any fixed
number of steps T, the expected '2 loss of x(T) is at least (1 — n) ∣∣w*kj, where x(T) is the T -th
iterate ofGD on L, i.e., x(t + 1) = x(t) - ηtVL(x(t)), for all t ≥ 0.
7 Conclusion and Future Work
We propose a mathematical framework to study the implicit bias of SGD with infinitesimal LR. We
show that with arbitrary noise covariance, Θ(η-2) steps of SGD converge to a limiting diffusion
on certain manifold of local minimizer, as the LR η → 0. For specific noise types, this allows
us to recover and strengthen results regarding implicit bias in previous works with much simpler
analysis. In particular, we show a sample complexity gap between label noise SGD and GD in the
kernel regime for a overparametrized linear model, justifying the generalization benefit of SGD. For
the future work, we believe our framework can be applied to analyze the implicit bias of SGD in
more complex models towards better understanding of the algorithmic regularization induced by
stochasticity. It will be valuable to extend our method to other stochastic optimization algorithms,
e.g., ADAM, SGD with momentum.
9
Published as a conference paper at ICLR 2022
Acknowledgement
We thank Yangyang Li for pointing us to Katzenberger (1991). We also thank Wei Zhan and Jason
Lee for helpful discussions.
The authors acknowledge support from NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla
Research, Amazon Research, DARPA and SRC. ZL is also supported by Microsoft Research PhD
Fellowship.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. Advances in neural information processing systems,
2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019b.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning, pp.
244-253. PMLR, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019a.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332. PMLR, 2019b.
Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake Woodworth, Nathan Srebro, Amir
Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal
mirror descent. arXiv preprint arXiv:2102.09769, 2021.
Augustin Banyaga and David Hurtubise. Lectures on Morse homology, volume 29. Springer Science
& Business Media, 2013.
Patrick Billingsley. Convergence of probability measures. John Wiley & Sons, 2013.
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory, pp.
483-513. PMLR, 2020.
Xiang Cheng, Dong Yin, Peter Bartlett, and Michael Jordan. Stochastic gradient and langevin
processes. In International Conference on Machine Learning, pp. 1810-1819. PMLR, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Y Cooper. The critical locus of overparameterized neural networks. arXiv preprint arXiv:2005.04210,
2020.
Yaim Cooper. The loss landscape of overparameterized neural networks. arXiv preprint
arXiv:1804.10200, 2018.
Alex Damian, Tengyu Ma, and Jason Lee. Label noise sgd provably prefers flat global minimizers.
arXiv preprint arXiv:2106.06530, 2021.
Amit Daniely. Sgd learns the conjugate kernel class of the network. arXiv preprint arXiv:1702.08503,
2017.
Manfredo P Do Carmo. Riemannian geometry. Springer Science & Business Media, 2013.
10
Published as a conference paper at ICLR 2022
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in
neural network energy landscape. In International conference on machine learning, pp.1309-1318.
PMLR, 2018.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-1685.
PMLR, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.
K. J. Falconer. Differentiation of the limit mapping in a dynamical system. Journal of the London
Mathematical Society, s2-27(2):356-372, 1983. ISSN 0024-6107. doi: 10.1112/jlms/s2-27.2.356.
Jianqing Fan, Zhuoran Yang, and Mengxin Yu. Understanding implicit regularization in over-
parameterized nonlinear statistical model. arXiv preprint arXiv:2007.08322, 2020.
Benjamin Fehrman, Benjamin Gess, and Arnulf Jentzen. Convergence rates for the stochastic gradient
descent method for non-convex objective functions. Journal of Machine Learning Research, 21,
2020.
C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540, 2016.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson.
Loss surfaces, mode connectivity, and fast ensembling of dnns. arXiv preprint arXiv:1802.10026,
2018.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832-1841.
PMLR, 2018a.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
Implicit regularization in matrix factorization. In 2018 Information Theory and Applications
Workshop (ITA), pp. 1-10. IEEE, 2018b.
Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the implicit
bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the gen-
eralization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741,
2017.
Andrew Holbrook. Differentiating the pseudo determinant. Linear Algebra and its Applications, 548:
293-304, 2018.
Elton P Hsu. Stochastic analysis on manifolds. Number 38. American Mathematical Soc., 2002.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Jeff Kahn, Janos Komlos, and Endre Szemeredi. On the probability that a random± 1-matrix is
singular. Journal of the American Mathematical Society, 8(1):223-240, 1995.
Ioannis Karatzas and Steven Shreve. Brownian motion and stochastic calculus, volume 113. springer,
2014.
11
Published as a conference paper at ICLR 2022
Gary Shon Katzenberger. Solutions of a stochastic differential equation forced onto a manifold by a
large drift. TheAnnalsofProbability,pp.1587-1628, 1991.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014.
Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, and
Rong Ge. Explaining landscape connectivity of low-cost solutions for multilayer nets. arXiv
preprint arXiv:1906.06247, 2019.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and KlaUs-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on learning theory, pp. 1246-1257. PMLR, 2016.
Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I Jordan, and Benjamin
Recht. First-order methods almost always avoid saddle points. arXiv preprint arXiv:1710.07406,
2017.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. arXiv preprint arXiv:1902.00621, 2019a.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pp. 2101-2110. PMLR,
2017.
Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic
gradient algorithms i: Mathematical foundations. The Journal of Machine Learning Research, 20
(1):1474-1520, 2019b.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory,
pp. 2-47. PMLR, 2018.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595, 2019c.
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent
for matrix factorization: Greedy low-rank learning. In International Conference on Learning
Representations, 2020a.
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020b.
Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? arXiv preprint arXiv:2010.08515, 2020c.
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic
differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021.
Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of
neural networks for binary classification. In International Conference on Machine Learning, pp.
2835-2843. PMLR, 2018.
Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les equations
aux de´ rive´ es partielles, 117(87-89):2, 1963.
12
Published as a conference paper at ICLR 2022
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
arXiv preprint arXiv:1906.05890, 2019.
Andrew Y Ng. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings
of the twenty-first international conference on Machine learning, pp. 78, 2004.
Quynh Nguyen. On connected sublevel sets in deep learning. In International Conference on Machine
Learning,pp. 4790-4799. PMLR, 2019.
Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class
of deep neural networks with no bad local valleys. arXiv preprint arXiv:1809.10749, 2018.
Lawrence M. Perko. Differential equations and dynamical systems. 2001.
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear
networks: a provable benefit of stochasticity. arXiv preprint arXiv:2106.09524, 2021.
David Pollard. Convergence of stochastic processes. Springer Science & Business Media, 2012.
Boris T Polyak. Gradient methods for solving equations and inequalities. USSR Computational
Mathematics and Mathematical Physics, 4(6):17-32, 1964.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax-optimal rates for sparse additive
models over kernel classes via convex programming. Journal of machine learning research, 13(2),
2012.
Bin Shi, Weijie J Su, and Michael I Jordan. On learning rates and schr\” odinger operators. arXiv
preprint arXiv:2004.06977, 2020.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822-2878, 2018.
Joel A Tropp. Convex recovery of a structured signal from independent random linear measurements.
In Sampling Theory, a Renaissance, pp. 67-101. Springer, 2015.
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse
recovery. Advances in Neural Information Processing Systems, 32:2972-2983, 2019.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in two-layer neural network
optimization landscapes. arXiv preprint arXiv:1802.06384, 2018.
Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba. Interplay
between optimization and generalization of stochastic gradient descent with covariance noise.
arXiv preprint arXiv:1902.08234, 2019.
Ward Whitt. Stochastic-process limits: an introduction to stochastic-process limits and their applica-
tion to queues. Springer Science & Business Media, 2002.
Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. part ii:
Continuous time analysis. arXiv preprint arXiv:2106.02588, 2021.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
Conference on Learning Theory, pp. 3635-3673. PMLR, 2020.
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the
noisy gradient descent that generalizes as sgd. In International Conference on Machine Learning,
pp. 10367-10376. PMLR, 2020.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics:
Stochastic gradient descent escapes from sharp minima exponentially fast. arXiv preprint
arXiv:2002.03495, 2020.
13
Published as a conference paper at ICLR 2022
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Peng Zhao, Yun Yang, and Qiao-Chu He. Implicit regularization via hadamard product over-
parametrization in high-dimensional linear regression. arXiv preprint arXiv:1903.09367, 2019.
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic
gradient descent: Its behavior of escaping from sharp minima and regularization effects. arXiv
preprint arXiv:1803.00195, 2018.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning,109(3):467-492, 2020.
14
Published as a conference paper at ICLR 2022
A	Preliminaries on Stochastic Processes
We first clarify the notations in this paper. For any integer k, we denote Ck as the set of the k
times continuously differentiable functions. We denote a ∧ b = min{a, b}. For any vector u, v
and α ∈ R, we define [u v]i = uivi and [vα]i = viα . For any matrix A, we denote its pseudo
inverse by AL For mapping F : RD → RD, We denote the Jacobian of F at X by ∂F(x) ∈ RD×D
where the (i, j)-th entry is ∂jFi(x). We also use ∂ F (x)[u] and ∂2F(x)[u, v] to denote the first
and second order directional derivative of F at x along the derivation of u (and v). We abuse the
notation of ∂2F by viewing it a linear mapping defined on RD 0 RD = RD , in the sense that
∂2 F(x)[Σ] = PiD,j=1 ∂2F(x)[ei, ej]Σij, for any Σ ∈ RD×D. For any submanifold Γ ⊂ RD and
x ∈ Γ, we denote by Tx(Γ) the tangent space of Γ at x and Tx⊥(Γ) the normal space of Γ at x.
Next, we review a few basics of stochastic processes that will be useful for proving our results, so
that our paper will be self-contained. We refer the reader to classics like Karatzas & Shreve (2014);
Billingsley (2013); Pollard (2012) for more systematic derivations.
Throughout the rest of this section, let E be a Banach space equipped with norm k ∙ k, e.g., (R, | ∙ |)
and (RD,∣∣∙∣∣2).
A.1 CADLAGFUNCTION AND METRIC
Definition A.1 (CadlagfUnCtion). Let T ∈ [0, ∞]. A function g : [0,T) → E is Cadlag if for all
t ∈ [0, T) it is right-continuous at t and its left limit g(t-) exists. Let DE [0, T) be the set of all
CadlagfUnCtiOn mapping [0, T) into E. We also use DE [0, T) to denote the set of all continuous
function mapping [0, T) into E. By definition, CE [0, T) ⊂ DE [0, T).
Definition A.2 (Continuity modulus). For any function f : [0, ∞) → E and any interval I ⊆ [0, ∞),
we define
ω(f; I) = sup kf (s) - f (t)k.
s,t∈I
For any N ∈ N and θ > 0, we further define the continuity modulus of continuous f as
ωN (f, θ) = sup	{ω(f; [t, t + θ])}.
0≤t≤t+θ≤N
Moreover, the continuity modulus of Cadlagf ∈ DE [0, ∞) is defined as
ωN(f, θ) = inf m maxω(f; [ti-i,ti) : 0 ≤ to < …<t = N, inf (ti - t-ι) ≥ θ 1 .
i≤r	i<r
Definition A.3 (Jump). For any g ∈ DE [0, T), we define the jump of g at t to be
∆g(t) = g(t) - g(t-).
For any δ > 0, we define hδ : [0, ∞) → [0, ∞) by
0	ifr≤δ
hδ(r) =	.
1 — δ∕r if r ≥ δ
We then further define Jδ : DRD [0, ∞) → DRD [0, ∞) (Katzenberger, 1991) as
Jδ(g)(t) = X hδ(k∆g(s)k)∆g(s).	(19)
0<s≤t
Definition A.4 (Skorohod metric on DE [0, ∞)). For each finite T > 0 and each pair of functions
f, g ∈ DE [0, ∞), define dT(f, g) as the infimum of all those values of δ for which there exist grids
0 ≤ to < tι < •一< tm and 0 < so < si < •一< •一< Sm, with tk, Sk ≥ T, such that
|ti - si| ≤ δ for i = 0, . . . , k, and
kf(t)-g(s)k ≤δ	if(t,s)∈[ti,ti+1)× [si,si+1)
for i = 0, . . . , k - 1. The Skorohod metric on DE [0, ∞) is defined to be
∞
d(f, g) = X 2-Tmin{1,dT(f,g)}.
T=1
15
Published as a conference paper at ICLR 2022
A.2 Stochastic Processes and Stochastic Integral
Let (Ω, F, {Ft}t≥o, P) be a filtered probability space.
Definition A.5 (Cross variation). Let X and Y be two {Ft}t≥0-adapted stochastic processes such
that X has sample paths in DRD×e [0, ∞) and Y has samples paths in DRe [0, ∞), then the cross
variation of X and Y on (0, t], denoted by [X, Y](t), is defined to be the limit of
m-1
X (X (ti+1) - X(ti))(Y (ti+1) - Y (ti))
i=0
in probability as the mesh size of 0 = to < tι < •一< tm = t goes to 0, if it exists. Moreover, for Y
itself, we write
e
[Y] = X[Yi,Yi]
i=1
Definition A.6 (Martingale). Let {X(t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If for all
0 ≤ s ≤ t, it holds that
E[X(t) | Fs] = X(s),
then X is called a martingale.
Definition A.7 (Local martingale). Let {X(t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If there
exists a sequence of {Ft}t≥0-stopping time, {τk}k≥0, such that
•	P[τk < τk+1] = 1, P[limk
→∞ τk = ∞] = 1,
•	and {Xτk (t)}t≥0 is a {Ft}t≥0-adapted martingale,
then X is called a local martingale.
Definition A.8 (Semimartingale). Let {X (t)}t≥0 be a {Ft}t≥0-adapted stochastic process. If there
exists a local martingale {M(t)}t≥o and a CadIag{Ft}t≥o-adapted process {A(t)}t≥o with bounded
total variation that X(t) = M(t) + A(t), then X is called a semimartingale.
Lemma A.9 (Ito’s Lemma). Let {X (t)}t≥o be defined through the following Ito drift-diffusion
process:
dX (t) = μ(t)dt + σ(t)dW (t).
where {W (t)}t≥0 is the standard Brownian motion. Then for any twice differentiable function f, it
holds that
df (t, X (t)) = ddf+ + (Vxf )>μt + 2 tr[σ> VXfσ]) dt + (Vxf )>σ(t)dW (t).
A.3 Weak Convergence for Stochastic Processes
Let (DE [0, ∞), A, d) be a metric space equipped with a σ-algebra A and the Skorohod metric defined
in the previous subsection.
Let {Xn }n≥0 be a sequence of stochastic processes on a sequence of probability spaces
{(Ωn, Fn, Pn)}n≥o SUCh that each Xn has sample paths in DE[0, ∞). Also, let X be a stochas-
tic process on (Ω, F, P) with sample paths on DE [0, ∞).
Definition A.10 (Weak convergence). A sequence of stochastic process{Xn}n≥0 is said to converge
in distribution or weakly converge to X (written as Xn ⇒ X) if and only if for all A-measurable,
bounded, and continuous function f : DE [0, ∞) → R, it holds that
lim E[f(Xn)] =E[f(X)].
n→∞
(20)
Though we define weak convergence for a countable sequence of stochastic processes, but it is still
valid if we index the stochastic processes by real numbers, e.g., {Xη}η≥0, and consider the weak
convergence of Xη as η → 0. This is because the convergence in (20) is for a sequence of real
numbers, which is also well-defined if we replace limn→∞ by limη→0 .
16
Published as a conference paper at ICLR 2022
Definition A.11 (δ-Prohorov distance). Let δ > 0. For any two probability measures P and Q on a
metric space with metric d, let (X, Y ) be a coupling such that P is the marginalized law of X and Q
that of Y . We define
ρδ(P,Q) = inf{ > 0 : ∃(X,Y),P[d(X,Y) ≥ ] ≤ δ}.
Note this distance is not a metric because it does not satisfy triangle inequality.
Definition A.12 (Prohorov metric). For any two probability measures P and Q on a metric space
with metric d, let (X, Y ) be a coupling such that P is the marginalized law of X and Q that of Y .
Denote the marginal laws of X and Y by L(X) and L(Y ) respectively. We define the Prohorov
metric as
ρ(P, Q) =inf{ > 0 : ∃(X,Y),L(X) = P,L(Y) = Q, P[d(X, Y) ≥ ] ≤ }.
It can be shown that Xn ⇒ X is equivalent to limn→∞ ρ(Xn , X) = 0.
Theorem A.13 (Skorohod Representation Theorem). Suppose Pn, n = 1, 2, . . . and P are proba-
bility measures on E such that Pn ⇒ P. Then there is a probability space (Ω, F, P) on which are
defined E -valued random variables Xn, n = 1, 2, . . . and X with distributions Pn and P respectively,
such that limn→∞ Xn = X a.s.
The main convergence result in Katzenberger (1991) (Theorem B.7) are in the sense of Skorohod
metric in Definition A.4, which is harder to understand and use compared to the more common
uniform metric (Definition A.14). However, convergence in Skorohod metric and uniform metric
indeed coincide with each other when the limit is in CRD [0, ∞), i.e., the continuous functions.
Definition A.14 (Uniform metric on DE [0, ∞)). For each finite T > 0 and each pair of functions
f, g ∈ DE [0, T), the uniform metric is defined to be
dU(f,g;T) = sup kf (t) -g(t)k.
t∈[0,T)
The uniform metric on DE [0, ∞) is defined to be
∞
dU(f,g) = X2-Tmin{1,dU(f,g;T)}.
T=1
Lemma A.15 (Problem 7, Section 5, Pollard (2012)). If Xn ⇒ X in the Skorohod sense, and X has
sample paths in CRD [0, ∞), then Xn ⇒ X in the uniform metric.
Remark A.16. We shall note the uniform metric defined above is weaker than supt∈[0,∞) kf (t) -
g(t)k. Convergence in the uniform metric on [0, ∞] defined in Definition A.14 is equivalent to
convergence in the uniform metric on each compact set [0, T] for T ∈ N+. The same holds for the
Skorohod topology.
B Limiting Diffusion of SGD
In this section, we give a complete derivation of the limiting diffusion of SGD. Here we use ⇒ to
denote the convergence in distribution. For any U ⊆ RD, We denote by U its interior. For linear
space S, we use S⊥ to denote its orthogonal complement.
First, as mentioned in Assumption 3.2, We verify that the mapping Φ is C2 in Lemma B.1. In
Appendix B.1 We discuss hoW different time scalings could affect the coefficients in SDE (2)
and (3). Then We check the necessary conditions for applying the results in Katzenberger (1991)
in Appendix B.2 and recap the corresponding theorem for the asymptotically continuous case
in Appendix B.3. Finally, We provide a user-friendly interface for Katzenberger’s theorem in
Appendix B.4.
Lemma B.1 (Implication of Falconer (1983)). Under Assumption 3.2, Φ is C2 on U.
ProofofLemma B.1. Applying Theorem 5.1 of Falconer (1983) with f (∙) = φ(∙, 1) suffices. □
17
Published as a conference paper at ICLR 2022
B.1 Approximating SGD by SDE
Let’s first clarify how we derive the SDEs, (2) and (3), that approximate SGD (1) under different
time scalings. Recall W(t) is Ξ-dimensional Brownian motion and that σ(X) : RD → RD×Ξ is a
deterministic noise function. As proposed by Li et al. (2017), one approach to approximate SGD (1)
by SDE is to consider the following SDE:
dX (t) = -VL(X (t))dt + √ησ(X (t))dW (t),
where the time correspondence is t = kη, i.e., X(kη) ≈ xη(k).
Now rescale the above SDE by considering X(t) = X(tη), which then yields
~ .. . .......................... 一 .. .
dX(t) = dX (tη) = -VL(X (tη))d(tη) + √ησ(X (tη))dW (tη)
=-ηVL(X (tη))dt + √ησ(X (tη))dW (tη).
Now We define W0(t) = √W(tη), and it's easy to verify that W0(t) is also a Ξ-dimensional
brownian motion, which means W0 =d W, i.e., W and W0 have the same sample paths in CRd [0, ∞).
Thus
dX(t) = -ηVL(X (tη))dt + ησ(X (tη))dW 0(t)
=-ηVL(X(t))dt + ησ(X(t))dW 0(t),
where the time correspondence is t = k, i.e., X(k) ≈ xη(k). The above SDE is exactly the same
as (2).
Then, to accelerate the above SDE by η-2 times, let's define X(t) = X(t∕η2). Then it follows that
dX(t) = dX(t∕η2) = —ηVL(X(t∕η2))dt∕η2 + ησ(X(t∕η2))dW (t∕η2)
=—1 VL(X(t))dt + σ(X(t))d (ηW(t∕η2))
Again note that ηW (t∕η2 ) =d W(t) in sample paths and thus is also a Ξ-Brownian motion. Here the
time correspondence is t = kη2 , i.e., evolving for constant time with the above SDE approximates
Ω(1∕η2) steps of SGD. In this way, we derive SDE (3) in the main context.
B.2 Necessary Conditions
Below we collect the necessary conditions imposed on {Zn}n≥1 and {An}n≥1 in Katzenberger
(1991). Recall that we consider the following stochastic process
Xn(t) =X(0)+ t σ(Xn(s))dZn(s) —	tVL(Xn(s))dAn(s).
For any stopping time τ, the stopped process is defined as Xnτ (t) = Xn(t ∧ τ). For any compact
K ⊂ U, we define the stopping time of Xn leaving K as λn(K) = inf{t ≥ 0 | Xn(t—) ∈∕
°.
°.
K or Xn(t) ∈ K}.
Condition B.2. The integrator sequence {An}n≥1 is asymptotically continuous: sup |An(t) —
t>0
An(t—)| ⇒ 0 where An(t—) = lims→t- An(s) is the left limit of An at t.
Condition B.3. The integrator sequence {An}n≥1 increases infinitely fast: ∀ > 0, inf(An(t + ) —
An (t)) ⇒ ∞.
Condition B.4 (Eq.(5.1), Katzenberger 1991). For every T > 0, as n → ∞, it holds that
sup	k∆Zn(t)k2 ⇒ 0.
0<t≤T∧λn (K)
Condition B.5 (Condition 4.2, Katzenberger 1991). For each n ≥ 1, let Yn be a {Ftn}-
semimartingale with sample paths in DRD [0, ∞). Assume that for some δ > 0 (allowing δ = ∞) and
every n ≥ 1 there exist stopping times {τnm | m ≥ 1} and a decomposition of Yn — Jδ (Yn)
18
Published as a conference paper at ICLR 2022
into a local martingale Mn plus a finite variation process Fn such that P[τnm ≤ m] ≤ 1/m,
{[Mn](t ∧ τnm) + Tt∧τm (Fn)}n≥1 is uniformly integrable for every t ≥ 0 and m ≥ 1, and
lim lim sup P sup (Tt+γ (Fn) - Tt (Fn)) >	= 0,
γ→0 n→∞	0≤t≤T
for every e > 0 and T > 0, where Tt(∙) denotes total variation on the interval [0,t].
Lemma B.6. For SGD iterates defined using the notation in Lemma 4.2, the sequences {An}n≥1
and {Zn}n≥1 satisfy Condition B.2, B.3, B.4 and B.5.
Proof of Lemma B.6. Condition B.2 is obvious from the definition of {An}n≥1.
Next, for any e > 0 and t ∈ [0, T], we have
An(t + e) - An(t) = ηn ∙
-ηn ∙
t
_nn_
≥ t + e - η2
一	ηn
t e - ηn
—=----------
ηn	ηn
which implies that info≤t≤τ(An(t + e) - An(t)) > e∕(2ηn) for small enough ηn. Then taking
n → ∞ yields the Condition B.3.
For Condition B.4, note that
∆zn(t) = ∣ηnVzξm% - 1I)
if t = k ∙ ηn,
otherwise.
Therefore, we have ∣∣∆Zn(t)k2 ≤ 2ηn√Ξ for all t > 0. ThiSimPIieSthatk ∆Zn(t)k 2 → 0 uniformly
over t > 0 as n → ∞, which verifies Condition B.4.
We proceed to verify Condition B.5. By the definition of Zn, we know that {Zn (t)}t≥0 is a
jump process with independent increments and thus is a martingale. Therefore, by decomposing
Zn = Mn + Fn with Mn being a local martingale and Fn a finite variation process, we must have
Fn = 0 and Mn is Zn itself. It then suffices to show that [Mn](t ∧ τnm) is uniformly integrable for
every t ≥ 0 and m ≥ 1. Since Mn is a pure jump process, we have
[Mn](t ∧ τnm) =	X	k∆Mn(s)k22 ≤ X k∆Mn(s)k22
0<s≤t∧τ m	0<s≤t
n
bt/nnC
X
k=1
ηn
2	bt/nnC
≤ 4Ξ X ηn2 ≤ 4Ξt.
2	k=1
This implies that [Mn](t ∧ Tm) is universally bounded by 4t, and thus [Mn](t ∧ Tm) is uniformly
integrable. This completes the proof.	□
Lemma 4.2. Let {ηn}n∞=1 be any positive sequence with limn→∞ ηn = 0, An(t) = ηnbt∕ηnJ,
and Zn(t) = ηn Pk=nnc √Ξ(1ξk — 11), where ξ1,ξ2,... i'i.d' Unf([Ξ]). Then with the Same
initialization Xn(0) = xnn (0) ≡ X (0), Xn(kηn2 ) defined by (6) is a Katzenberger process and is
equal to xnn (k) defined in (1) with LR equal to ηn for all k ≥ 1. Moreover, the counterpart of (7) iS
Y(t)
Φ(X(0)) + / ∂Φ(Y)σ(Y)dW(s) + 1 / ∂2Φ(Y)[Σ(Y)]ds,
(8)
where Σ ≡ σσ> and {W (t)}t≥0 iS a Ξ-dimenSional Standard Brownian motion.
Proof of Lemma 4.2. For any n ≥ 1, it suffices to show that given Xn(kηn2 ) = xnn (k), we further
have Xn((k + 1)ηn2 ) = xnn (k + 1). By the definition ofXn(t), we have
Xn((k+1)ηn2) - Xn(kηn2)
(k+1)nn2	(k+1)nn2
=-/	VL(Xn(t))dAn(t)+ /	σ(Xn(t))dZn(t)
knn2	knn2
=-VL(Xn(kηC ))(An ((k + 1)ηn ) - An(kηn )) + σ(xn(kηl ))(Zn((k + 1)ηn ) - Zn(kηn ))
=-ηnVL(Xn(kηn)) + ηn√Ξσξk (Xn(a))
=-ηnVL(Xnn(k)) + ηn施气片 (Xnn(k)) = Xnn(k + 1) - xnn(k)
19
Published as a conference paper at ICLR 2022
where the second equality is because An(t) and Zn(t) are constant on interval [kηn2 , (k + 1)ηn2). This
confirms the alignment between {Xn(kηn2)}k≥1 and {xηn(k)}k≥1.
For the second claim, note that σ(x)EZn(t) ≡ 0 for all x ∈ RD, t ≥ 0 (since the noise has zero-
expectation) and that {Zn(t) - EZn(t)}t≥0 will converge in distribution to a Brownian motion by
the classic functional central limit theorem (see, for example, Theorem 4.3.5 in Whitt (2002)). Thus,
the limiting diffusion of Xn as n → ∞ can be obtained by substituting Z with the standard Brownian
motion W in (22). This completes the proof.	□
B.3 Katzenberger’ s Theorem for Asymptotically Continuous Case
The full Katzenberger’s theorem deals with a more general case, which only requires the sequence of
intergrators to be asymptotically continuous, thus including SDE (3) and SGD (1) with η goes to 0.
To describe the results in Katzenberger (1991), we first introduce some definitions. For each n ≥ 1, let
(Ωn, Fn, {Ftn}t≥o, P) be a filtered probability space, Zn an Re-VaIued cadlag {Ff} -semimartingale
with Zn(0) = 0 and An a real-valued cadlag {Ftn}-adapted nondecreasing process with An(0) = 0.
Let σn : U → M(D, e) be continuous with σn → σ uniformly on compact subsets of U. Let Xn be
an RD-Valued cadlag {Ftn}-semimartingale satisfying, for all compact K ⊂ U,
Xn(t)
X(0)+ Zt σ(Xn)dZn + [t -VL(Xn)dAn
00
(21)
一 一一 、 ________ 一 、 _______ .一， 一， 一 ， , 一 ， 、 ， θ-ʌ .
for all t ≤ λn(K) where λn(K) = inf{t ≥ 0 | Xn(t-) ∈ K or Xn(t) ∈ K} is the stopping time
of Xn leaVing K .
Theorem B.7 (Theorem 6.3, Katzenberger 1991). Suppose X(0) ∈ U, Assumptions 3.1 and 3.2,
Condition B.2, B.3, B.4 and B.5 hold. For any compact K ⊂ U, define μn(K) = inf{t ≥
0 | Yn(t-) ∈ K or Yn(t) ∈ K}, then the sequence {(Ynμn(κ),Zμn(K),μn(K)} is relatively
compact in Drd ×e [0, ∞) X [0, ∞). If (Y,Z,μ) is a limit point ofthis sequence under the skorohod
metric (Definition A.4), then (Y, Z) is a continuous semimartingale, Y(t) ∈ Γ for every t ≥ 0 a.s.,
μ ≥ inf{t ≥ 0 | Y(t) ∈ K} a.s. and Y(t) admits
Y (t)= Y (0)+ t μ ∂Φ(Y (s))σ(Y (s))dZ (s)
0
1 D e	ft∧μ
+ 5 EE	∂ijΦ(Y(s))σ(Y(s))ikσ(Y(s)jιd[Zk,Zι](s).	(22)
i,j=1 k,l=1 0
We note that by Lemma A.15, conVergence in distribution under skorohod metric is equiValent to
conVergence in distribution under uniform metric Definition A.14, therefore in the rest of the paper
we will only use the uniform metric in the rest of the paper, e.g., wheneVer we mention ProhoroV
metric and δ-ProhoroV distance, the underlying metric is the uniform metric.
B.4 A User-friendly Interface for Katzenberger’s Theorem
Based on the Lemma B.6, we can immediately apply Theorem B.7 to obtain the following limiting
diffusion of SGD.
Theorem B.8. Let the manifold Γ and its open neighborhood U satisfy Assumptions 3.1 and 3.2. Let
K ⊂ U be any compact set and fix some x0 ∈ K. Consider the SGD formulated in Lemma 4.2 where
Xηn (0) ≡ x0. Define
Yηn(t) =Xηn(t)-φ(Xηn(0),Aηn(t))+Φ(Xηn(0))
and μηn (K) = min{t ∈ N | Ynn (t) ∈ K}. Then the sequence {(以而(K),Zηn,Nηn (K))}n≥ι is
relatively compact in Drd × Rn [0, ∞) × [0, ∞]. Moreover, if (Y, Z, μ) is a limit point ofthis sequence,
it holds that Y(t) ∈ Γ a.sfor all t ≥ 0, μ ≥ inf{t ≥ 0 | Y(t) ∈ K} and Y(t) admits
Y(t)
t∧μ
s=0
∂Φ(Y(s))σ(Y(s))dW(s) +
∕∙tΛμ D
L5 Xi
∂ijΦ(Y(s))(σ(Y(s))σ(Y(s))>)ijds
(23)
20
Published as a conference paper at ICLR 2022
where {W(s)}s≥o is the standard Brownian motion and σ(∙) is as defined in Lemma 4.2.
However, the above theorem is hard to parse and cannot be directly applied if we want to fur-
ther study the implicit bias of SGD through this limiting diffusion. Therefore, we develop a
user-friendly interface to it in below. In particular, Theorem 4.6 is the a special case of Theo-
1
rem B.9. In Theorem 4.6, We replace ∂Φ(Y(t))σ(Y(t)) with Σ22 (Y(t)) to simplify the equation,
since ∂Φ(Y (t))σ(Y (t)) (∂Φ(Y (t))σ(Y (t)))> = Σk (Y (t)) and thus this change doesn’t affect the
distribution of the sample paths of the solution.
Theorem B.9. Under the same setting as Theorem B.8, we change the integer index back to η > 0
with a slight abuse ofnotation. For any stopping time μ and stochastic process {Y(t)}t≥o Such that
μ ≥ inf{t ≥ 0 | Y(t) ∈ K}, Y(0) = Φ(xo) and that (Y, μ) satisfy Equation (23) for some standard
Brownian motion W. For any compact set K ⊆ U and T > 0, define μ(K) = inf{t ≥ 0 | Y(t) ∈
K} and δ = P(μ(K) ≤ T). Thenfor any e > 0, it holdsfor all sufficiently small LR η that:
ρ2δ(Yμ(K)∧t,y*(K)∧t) ≤ e,
(24)
which means there is a coupling between the distribution of the StOPPed processes Yμη (K)∧t and
Yμ(κ)∧τ, such that the uniform metric between them is smaller than e with probability at least 1 一 2δ.
In other words, limn→o ρ2δ (Yμ (K)∧t, Yμ(K)∧T) = 0.
Moreover, when {Y (t)}t≥0 is a global solution to the following limiting diffusion
t	t1D
丫⑴=I。"⑼。(Y(S))dW os)+L0 2 iX%@(Y(S))(«”"))3'
and Y never leaves U, i.e. P[∀t ≥ 0, Y(t) ∈ U] = 1, it holds that YηT converges in distribution to
YT as η → 0 for any fixed T > 0.
For clarity, we break the proof of Theorem B.9 into two parts, devoted to the two claims respectively.
Proof of the first claim of Theorem B.9. First, Theorem B.8 guarantees there exists a stopping time
μ and a stochastic process {Y(t)}t≥o such that
1.	(Y, μ) satisfies Equation (23);
2.	Y ∈ Γ a.s.;
.	.	-	.~ . 、	. O-
3.	μ ≥ μ(K):= inf{t ≥ 0 | Y(t) ∈ K}.
The above conditions imply that Y.(K) ∈ Γ a.s.. Since the coefficients in Equation (23) are locally
Lipschitz, we claim that (Yμ(K), μ(K)) = (Yμ(K),μ(K)). To see this, note that for any compact
K ⊆ U, the noise function σ, ∂Φ and ∂ 2Φ are all Lipschitz on K, thus we can extend their definitions
to RD such that the resulting functions are still locally Lipschitz. Based on this extension, applying
classic theorem on weak uniqueness (e.g., Theorem 1.1.10, Hsu 2002) to the extended version of
Equation (23) yields the equivalence in law. Thus we only need to prove the first claim for Y.
Let ET be the event such that μ(K) > T on ET. Then restricted on ET, we have Y(T ∧ μ)=
Y(T ∧ μ(K)) as μ ≥ μ(K) holds a.s. We first prove the claim for any convergent subsequence of
{Yη }η>0 .
Now, let {ηm}m≥ι be a sequence of LRs such that ηm → 0 and Yμηm(K) ⇒ Yμ as m → ∞.
By applying the Skorohod representation theorem, we can put {Yηm }m≥1 and Y under the same
probability space such that Yμm(K) → Yμ a.s. in the Skorohod metric, or equivalently the uniform
metric (since Yμ is continuous) i.e.,
du(Ynmm(K), Yμ) → 0,a.s.,
21
Published as a conference paper at ICLR 2022
which further implies that for any > 0, there exists some N > 0 such that for all m > N ,
p [du (Ynmnm (K)∧T ,y μ∧T)≥d ≤ δ.
Restricted on ET, we have du(Ynmnm(K)∧T,Yeμ∧T) = du(Ynmm(K)∧T,γμ(K)∧T), and it follows
that for all m > N,
PhdU (Ynmm(K)∧T ,γμ(K)∧T) ≥e] ≤ p[{du (Ynmm(K)∧T ,γ μ(K)∧T) ≥e}∩Eτ ] + P [ET ]
=p[{du(Ynmm(K)∧T,Yμ∧τ) ≥e}∩Eτi + P[ET]
≤ PhdU(Ynmm (K)∧τ, Yμ∧τ) ≥ e] +P[ET]
≤ 2δ.
By the definition of the Prohorov metric in Definition A.12, we then get
ρ2δ (Ynmnm (K)∧τ, Yμ(K)∧τ) ≤ e for all m > N. Therefore, We have
lim ρ2δ(Ynmm(K)∧T,Yμ(K)∧τ)=0.
m→∞
Now we claim that it indeed holds that limn→o ρ2δ (YnXn (K)∧τ,Yeμ(K)∧T) = 0. We prove this
by contradiction. Suppose otherwise, then there exists some e > 0 such that for all η0 > 0,
there exists some η < ηo with ρ2δ(Yμn(K)∧τ,Yμ(K)∧τ) > e. Consequently, there is a se-
quence {ηm}m≥ι satisfying limm→∞ ηm = 0 and ρ2δ(Ynmm(K), Yμ(K)∧τ) > e for all m. Since
{(Ynmnm(K)∧τ, Znm, μηm (K))}m≥ι is relatively compact, there exists a subsequence (WLOG, as-
sume it is the original sequence itself) converging to (Yμ∧τ, W, μ) in distribution. However, repeating
the exactly same argument as above, we would have ρ2δ (Ynmnm (K)∧τ, Yμ(K)∧τ) ≤ e for all suffi-
ciently large m, which is a contradiction. This completes the proof.	□
Proof of the second claim of Theorem B.9. We will first show there exists a sequence of compact
set {Km}m≥1 such that ∪m∞=1Km = U and Km ⊆ Km+1. For m ∈ N+, we define Hm =
U\ (Bι∕m(0) + RD \ U) and Km = Hm ∩ Bm(0). By definition it holds that ∀m < m0, Hm ⊆ Hm/
and Km ⊆ Kmo. Moreover, since Km is bounded and closed, Km is compact for every m. Now
we claim ∪∞=ιKm = U. Note that ∪∞=ιKm = Um=Hm ∩ Bm(0) = ∪∞=ιHm. ∀χ ∈ U,
since U is open, we know du (x, RD \ U) > 0, thus there exists m0 ∈ N+, such that ∀m ≥ m0,
X ∈ (Bι∕m(0) + RD \ U) and thus X ∈ Hm, which implies X ∈ ∪∞=ιHm. On the other hand,
∀x ∈ RD \ U, it holds that x ∈ (B1/m(0) + RD \ U) for all m ∈ N+, thus x ∈/ Hm ⊂ Km.
Therefore, since Y ∈ U and is continuous almost surely, random variables limm→∞ μ(Km,) = ∞
a.s., which implies μ(Km,) converges to ∞ in distribution, i,e,, ∀δ > 0,T > 0, ∃m ∈ N+, such that
∀K ⊇ Km, it holds P[μ(K) ≤ T] ≤ δ.
Now we will show for any T > 0 and e > 0, there exists η0 such that ρ (YT, YnT) ≤ e for all
η ≤ ηo. Fixing any T > 0, for any e > 0, let δ = ∣, then from above we know exists compact
set K, such that P(μ(K) ≤ T) ≤ δ. We further pick K0 = K + B2e0 (0), where e0 can be any real
number satisfying 0 < e0 < e and K0 ⊆ U. Such e0 exists since U is open. Note K ⊆ K0, we have
P(μ(K0) ≤ T) ≤ P(μ(K) ≤ T) ≤ δ. Thus by the first claim of Theorem B.9, there exists ηo > 0,
such that for all η ≤ηo, we have ρ2δ (YnXn(K)∧τ, Y*(KO)∧τ) ≤ 2-dτee0.
Note that ρδ (Y*(K)∧τ, Y*(KO)∧τ) = 0, so we have for all η ≤ η0,
ρ
3δ
(Yμ(K)∧τ Yμn(K0)∧T) ≤ 2
-dTee0
By the definition of δ-Prohorov distance in Definition A.11, we can assume (Yμ(K)∧τ, γμn(K )∧τ)
is already the coupling such that P [du(Y*(K)∧τ, YnXn(K )∧τ) ≥ 2-dTe e] ≤ 3δ. Below we want
to show ρ3δ (Yμ(K)∧τ, YnT) ≤ 2-dτee0. Note that for all t ≥ 0, Yμ(K)∧τ(t) ∈ K, thus we know if
22
Published as a conference paper at ICLR 2022
μη (K0) ≤ T, then
du (Y μ(K)∧T ,Yμη (K0)∧T) ≥ 2-dT e ∣∣Y”(K)∧t (μη (K 0)) - Yμη(KO)∧t (μ∏ (K 0)(
≥ 2-dTedU(K, Rd/K0)
≥ 2-dTe0.
On the other hand, if μη(K0) > T, then Yn = Y^n(K )∧T. Thus We can conclude that
du (Yμ(K)∧t , yT ) ≥ 2-dTee0 implies du (Yμ(K)∧t ,γμn (K )∧T) ≥ 2-dTee0. Therefore, we further
have
PidU(Y"(K)∧T,YT) ≥ 2-dTee] ≤ PIdU(Y"(K)∧T, Yμη(K0)∧T) ≥ 2-dTee] ≤ 3δ,
that is,
ρ3δ(Yμ(K)∧t,YT) ≤ 2-dTee0.
Finally, since ρδ (YT, Yμ(K)∧T) = 0, we have for all η ≤ η0 ,
Pe(YT,YT) = ρ4δ(YT,YT) ≤ ρ3δ(Yμ(K)∧T,YT) + ρδ(YT,Y"(K)∧T) ≤ 2-dTee0 + 0 ≤ e,
which completes the proof.	□
Now, we provide the proof of Theorem 4.6 as a direct application of Theorem B.9.
Proof of Theorem 4.6. We first prove that Y never leaves Γ, i.e., P[Y (t) ∈ Γ, ∀t ≥ 0] = 1. By the
result of Theorem B.8, We know that for each compact set K ⊂ Γ, Yμ(K) stays on Γ almost surely,
〜
°.
where μ(K) := inf{t ≥ 0 | Y(t) ∈ K} is the earliest time that Y leaves K. In other words, for
all compact set K ⊂ Γ, P[∃t ≥ 0, Y(t) ∈/ Γ, Y(t) ∈ K] = 0. Let {Km}m≥1 be any sequence of
compact sets such that ∪m≥1 Km = U and Km ⊂ U, e.g., the ones constructed in the proof of the
second claim of Theorem B.9. Therefore, we have
∞
P[∃t ≥0,Y(t) ∈/ Γ] = P[∃t ≥0,Y(t) ∈/ Γ, Y(t) ∈ U] ≤ X P[∃t ≥0,Y(t) ∈/ Γ, Y(t) ∈ Km] =0,
m=1
which means Y always stays on Γ.
Then recall the decomposition of Σ = Σk + Σ⊥ + Σk,⊥ + Σ⊥,k as defined in Lemma 4.5. Since Y
never leaves Γ, by Lemma 4.5, we can rewrite Equation (10) as
dY(t) = ∑j∕2dW (t) + ∂ 2Φ(Y (t))[Σ(Y (t))]dt
1D
=∂Φ(Y(t))σ(Y(t))dW(t) + 2 E ∂jΦ(Y(t))(σ(Y(t))σ(Y(t))>)jdt
i,j=1
where the second equality follows from the definition that Σk = ∂ΦΣ∂Φ = ∂Φσσ> ∂Φ. This
coincides with the formulation of the limiting diffusion in Theorem B.9. Therefore, further combining
Lemma 4.2 and the second part of Theorem B.9, we obtain the desired result.
□
Remark B.10. Our result suggests that for tiny LR η, SGD dynamics have two phases. In Phase I of
Θ(1∕η) steps, the SGD iterates move towards the manifold Γ of local minimizers along GF. Then
in Phase II which is of Θ(1∕η2) steps, the SGD iterates stay close to Γ and diffuse approximately
according to (10). See Figure 2for an illustration of this two-phase dynamics. However, since the
length of Phase I gets negligible compared to that of Phase II when η → 0, Theorem 4.6 only reflects
the time scaling of Phase II.
C	Explicit Formula of the Limiting Diffusion
In this section, we demonstrate how to compute the derivatives of Φ by relating to those of the loss
function L, and then present the explicit formula of the limiting diffusion.
23
PUbHShed as a COnferenCe PaPer-ICLR 2022
FigUre 2-πlL0n forrwo—phase dynamics Of SGD Wirh rhe Same example as in FigUre 1 ♦「is an
ID manifold OfminimiZerS ofoss L.
c∙ 1 EXPLlCrr EXPRESSIoN OF THE DERlVATlVES
FOr anyH∈ 「3 we choose an or-honomlal basis of 母(「)as-71 二：飞DIMJ∙ La
be an OrrhonormaI basis Of 片(「)Sorhar -WhQD 一 is an OrthonormaI basis Of
¾,
Lemma c∙l∙ FOr any e ∈「and any V ∈^(「L it holds thaf V2.LH)7 H 0∙
proof. For any H ∈ (Γ)3 Ier {h( Z) }rlv0 be a ParamerriZed Smoorh CUrVe On Γ SUCh rhaf 2(0) H H
and ⅛^--τo H C Then 4L(Xt} H O for all■ThUS O H dv^c) L——0 H ∖72L(-e∙ □
Lemma c∙2∙ FOr any H ∈ IRDhds that qφ() vl() H O Cmd
q2φ(vl(vl(" iqφ∖72lvl∙
proof. FiXing any H ∈ 屈 DiaU —V.L (ΛE)) be initialized afΛo) H 2∙ SinCe 9(23) H
θsfor all f ≥θwe have
∕Φ(HS) U iqφ(h3)vL (「§ H 0∙
EVaIUaring rhe above equation a〔 f H O yields Qθ(H)vh(H) H 0∙ MoreoVe Lrake rhe SeCond Order
derivative and We have
⅜I⅛ΦF) “4φλλ)崔 WaλLswλ)混"o∙
Evaluating a〔r H O COmPIaes rhe PrOOf∙ □
NOW We Can PrOVe Lemma 43 rest⅛ed in beow∙
Lemma 4・3・ FaranyH∈Qθ(H) ∈ IRDXD -S『he projecf ion ma 三 XonfOfangerlfsPaCeTXB).
PrOOfOfLemma 4∙3. FOr any V ∈ 母(「)“ Ier (7(r)1lvo) bo a ParamerriZed SmOOrh CUrVe On「
SUCh thafAo) H H and ¾^∙LU0 H W. SinCeAe) ∈「for all f ≥ O。 we have s(Ae)) M 7s" and
thus
ThiSimPHeS rh-Qφ(2vu e for all e ∈^(Γ)∙
NeXLfOr any It ∈ 品(Γ) and Z ≥ConSider expanding /X + ZV2L(tf)Z Uck
/L (2 + ZV2Ltw HL∙ -4XVU + 03
+ O
24
Published as a conference paper at ICLR 2022
where the second equality follows from the assumption that V2L(χ) is full-rank when restricted on
Tx⊥(Γ). Then since ∂Φ is continuous, it follows that
lim dφ(X + tv2L3tuVL(X + tv2L(X)tU) = iim ∂φ(χ + tv2L"(u + o⑴)
= ∂Φ(x)u.
By Lemma C.2, we have ∂Φ(X + t(v2L(X))tu))vL(X + t(v2L(X))tu) = 0 for all t> 0, which
then implies that ∂Φ(X)u = 0 for all u∈ Tx⊥(Γ).
Therefore, under the basis {vi, . . . , vN}, ∂Φ(X) is given by
∂Φ(X) = ID0-M 00 ∈ RD×D,
that is, the projection matrix onto TX(Γ).	口
Lemma C.3. For any X ∈ Γ, it holds that ∂ Φ(X)v2 L(X) = 0.
Proof. It directly follows from Lemma C.1 and Lemma 4.3.	口
Next, we proceed to compute the second-order derivatives.
Lemma C.4. For any X ∈ Γ, u∈ RD and v ∈ Tx (Γ), it holds that
∂2Φ(X)[v, u] = -∂Φ(X)∂2 (vL)(X)[v, v2L(X)tu] - v2L(X)t∂2 (vL)(X)[v, ∂Φ(X)u].
Proof of Lemma C.4. Consider a parametrized smooth curve {v(t)}t≥0 on Γ such that v(0) = X and
dvdtt)∣t=0 = v. We define P(t) = ∂Φ(v(t)), P⊥(t) = ID — P(t) and H(t) = V2L(v(t)) for all
t≥ 0. By Lemma C.1 and 4.3, we have
P⊥(t)H(t) = H(t)P⊥(t) = H(t),	(25)
Denote the derivative ofP(t), P⊥(t) and H(t) with respect to tas P0(t), (P⊥)0(t) and H0(t). Then
differentiating with respect to t, we have
(P⊥)0(t)H(t)	= H0(t)	-	P⊥(t)H0(t)	= P(t)H0(t).	(26)
Then combining (25) and (26) and evaluating at t= 0, we have
P0(0)H(0) = -(P⊥)0(0)H(0) = -P(0)H0(0)	(27)
We can decompose P0(0) and H(0) as follows
P0(0) = PP12011((00)) PP21022((00)) , H(0) = 00 H202(0) ,	(28)
where P101(0) ∈ R(D-M)×(D-M) and H22 is the hessian ofL restricted on Tx⊥(Γ). Also note that
P(0)H0(0)P⊥(0)= ID0-M 00 HH120011((00)) HH120022((00))	00 I0M
= 0 H102 (0) ,
and thus by (28) we have
P0 0 H 0	0	P102(0)H22(0)	0	-H102(0)
P (0)H(0) =	0	P202 (0)H22(0)	= 0	0	.
This implies that we must have P202 (0) = 0 and P102(0)H22 (0) = H102 (0). Similarly, by taking
transpose in (28), we also have H22(0)P201(0) = -H201(0).
25
Published as a conference paper at ICLR 2022
It then remains to determine the value of P101 (0). Note that since P (t)P (t) = P (t), we have
P0(t)P(t) + P(t)P0(t) = P0(t), evaluating at t = 0 yields
2P101 (0) = P101(0).
Therefore, we must have P101 (0) = 0. Combining the above results, we obtain
P0(0) = -P(0)H0(0)H(0)t - H(0)tH0(0)P(0).
Finally, recall that P(t) = ∂Φ(v(t)), and thus
po(0) = d- ∂φ(v(t))	= ∂ 2Φ(x)[v].
dt	t=0
Similarly, We have H0(0) = ∂2(VL)(χ)[v], and it follows that
∂2Φ(x)[v] = -∂Φ(x)∂2(VL)(x)[v]V2L(x)t - V2L(x)t∂2(VL)(x)[v]∂Φ(x).
□
Lemma C.5. For any x ∈ Γ and u ∈ Tx⊥(Γ), it holds that
∂ 2Φ(x)[uu> + V2L(x)tuu> V2L(x)] = -∂ Φ(x)∂2(VL)(x)[V2L(x)tuu>].
ProofofLemma C.5. For any U ∈ T⊥(Γ), we define u(t) = X + tV2L(x)*u for t ≥ 0. By Taylor
approximation, we have
VL(u(t)) = tV2L(x)V2L(x)tu + o(t) = tu + o(t)	(29)
and
V2L(u(t)) = V2L(x) + t∂2(VL)(x)[V2L(x)tu]+ o(t).	(30)
Combine (29) and (30) and apply Lemma C.2, and it follows that
0= ∂2Φ(u(t))[VL(u(t)),VL(u(t))]+∂Φ(u(t))V2L(u(t))VL(u(t))
=t2 ∂2 Φ(u(t))[u + o(1)](u + o(1)) + t2∂Φ(u(t))∂2(VL)(x)[V2L(x)'u](u + o(1))
+12 dφ(u(t)) V2L(x)(u + o(1))
=t2 ∂2 Φ(u(t))[u + o(1)](u + o(1)) + t2∂Φ(u(t))∂2(VL)(x)[V2L(x)'u](u + o(1))
∂Φ(u(t)) - ∂ Φ(x)
+12 —^^t--------L V2L(x)(u + o(1))
where the last equality follows from Lemma C.3. Dividing both sides by t2 and letting t → 0, we get
∂2Φ(x)[u]u + ∂Φ(x)∂2(VL)(x)[V2L(x)*u]u + ∂2Φ(x)[V2L(x)*u]V2L(x)u = 0.
Rearranging the above equation completes the proof.	□
With the notion of Lyapunov Operator in Definition 4.4, Lemma C.5 can be further simplified into
Lemma C.6.
Lemma C.6. For any x ∈ Γ and Σ ∈ span{uu> | u ∈ Tx⊥ (Γ)},
h∂2Φ(x), Σi = -∂Φ(x)∂2(VL)(x)[Lv2l(x)(Σ)].	(31)
Proof of Lemma C.6. Let A = uu> + V2L(x)^uu> V2 L(x) and B = V2L(x)*uu>. The key
observation is that A + A> = L^L(X)(B + B>). Therefore, by Lemma C.5, it holds that
∂2Φ(x)[LsL(X)(B+B>)] = ∂ 2Φ(x)[A+A>] = 2∂Φ(x)∂ 2(VL)(x)[B ] = ∂ Φ(x)∂2(VL)(x)[B+B>].
Since V2L(x)* is full-rank when restricted to T⊥(Γ), we have span{V2L(x)^uu> + uu>V2L(x)才 |
u ∈ TX⊥(Γ)} = span{uu> | u ∈ TX⊥(Γ)}. Thus by the linearity of above equation, we can replace
B + B> by any Σ ∈ span{uu> | U ∈ T⊥(Γ)}, resulting in the desired equation.	□
Then Lemma 4.5 directly follows from Lemma C.4 and C.5.
26
Published as a conference paper at ICLR 2022
C.2 Tangent Noise Compensation only Dependends on the Manifold Itself
Here we show that the second term of (10), i.e., the tangent noise compensation for the limiting
dynamics to stay on Γ, only depends on Γ itself.
Lemma C.7. For any x ∈ Γ, suppose there exist a neighborhood Ux of x and two loss func-
tions L and L0 that define the same manifold Γ locally in Uχ, i.e., Γ ∩ Ux = {x | VL(x)=
0} = {x | VL0(x) = 0}. Then for any V ∈ Tx(Γ), it holds that (VllL(Xy) ∂2(VL)(x) [v, v]=
(V2L0(x))t∂2(VL0)(x) [v,v].
ProofofLemma C.7. Let {v(t)}t≥o be a smooth curve on Γ with v(0) = X and ddt)∣t=o = v.
Since v(t) stays on Γ, we have VL(v(t)) = 0 for all t ≥ 0. Taking derivative for two times yields
∂2(VL)(v(t))[ddtt), ddtt)] + V2L(v(t)) ddvt2t) = 0. Evaluating it at t = 0 and multiplying both
sides by V2L(χ)t, we get
V2L(x)t∂2(VL)(x) [v,v] = -V2L(x)tV2L(x)d2* v3	= -∂Φ(x)d2v^	.
dt	t=0	dt	t=0
Since ∂Φ(X) is the projection matrix onto Tx(Γ) by Lemma 4.3, it does not depend on L, so
analogously we also have V2L0(x)^∂2(VL0)(χ)[v,v] = -∂Φ(x)号2" :=0 as well. The proof is
thus completed. Note that ∂Φ(x)d-v2t)l C is indeed the second fundamental form for V at x, and
dt2 t=0	,
the value won’t change if we choose another parametric smooth curve with a different second-order
time derivative. (See Chapter 6 in Do Carmo (2013) for a reference.)	□
C.3 Proof of results in Section 5
Now we are ready to give the missing proofs in Section 5 which yield explicit formula of the limiting
diffusion for label noise and isotropic noise.
Corollary 5.1 (Limiting Diffusion for Isotropic Noise). IfΣ ≡ ID on Γ, SDE (10) is then
dY (t)
∂Φ(Y)dW + 1 V2L(Y)t∂2(VL)(Y)[∂Φ(Y)] dt — 1 ∂Φ(Y)V(ln ∣V2L(Y)∣ + )dt (11)
/ I
Brownian Motion on Manifold
z
Normal Regularization
where ∣V2L(Y)∣+ = limα→o J-LanY()+LD))is the pseudo-determinant of V2L(Y). ∣V2L(Y)∣+ is
also equal to the sum of log of non-zero eigenvalue values of V2L(Y).
Proof of Corollary 5.1. Set Σk = ∂Φ, Σ⊥ = ID - ∂Φ and Σ⊥,k = Σk,⊥ = 0 in the decomposition
of Σ by Lemma 4.5, and we need to show ∂ΦV(ln ∣∑∣+) = ∂2(VL)[(V2L)^].
Holbrook (2018) shows that the gradient of pseudo-inverse determinant satisfies V∣A∣+ = ∣A∣+Al
Thus we have for any vector V ∈ RD,〈v, Vln ∣V2L∣ +〉= D l[ξL2L] L, ∂2(VL)[v]) =
(V2L,∂2(VL)[v]) = ∂2(VL)[v, V2L] = ^v, ∂2(VL)[(V2L)^]), which completes the proof. □
Corollary 5.2 (Limiting Flow for Label Noise). If Σ ≡ cV2L on Γ for some constant c > 0,
SDE (10) can be simplified into (13) where the regularization is from the noise in the normal space.
dY(t) = -1/4 ∙ ∂Φ(Y(t))Vtr[cV2L(Y(t))]dt.
(13)
Proof of Corollary 5.2. Since Σ = cV2L, here we have Σ⊥ = Σ and Σk , Σ⊥,k , Σk,⊥ = 0. Thus it
suffices to show that 2∂2(VL) [L7-l(∑⊥)] = V tr[V2L]. Note that for any V ∈ RD,
v>V tr[V2L] = 〈ID, ∂2(VL)[v] = 〈ID - ∂Φ, ∂2(VL)[v],	(32)
where the second equality is because the the tangent space of symmetric rank-n matrices at V2L is
{AV2L + V2LA> | A ∈ RD×D}, and every element in this tangent space has zero inner-product
with ∂Φ by Lemma 4.3. Also note that L^-l(V2L) = 1 (ID 一 ∂Φ), thus〈ID 一 ∂Φ, ∂2(VL)[v])=
2 <Lx72l(V2L),∂2(VL)[v]> = 2v>∂2(VL)[Lv-l(V2L)].	□
27
Published as a conference paper at ICLR 2022
C.4 EXAMPLE: k-PHASE MOTOR
We also give an example with rigorous proof where the implicit bias induced by noise in the normal
space cannot be characterized by a fixed regularizer, which was first discovered by Damian et al.
(2021) but was only verified via experiments.
Note the normal regularization in both cases of label noise and isotropic noise induces Riemmanian
gradient flow against some regularizer, it’s natural to wonder if the limiting flow induced by the
normal noise can always be characterized by certain regularizer. Interestingly, Damian et al. (2021)
answers this question negatively via experiments in their Section E.2. We adapt their example into
the following one, and rigorously prove the limiting flow moves around a cycle at a constant speed
and never stops using our framework.
Suppose dimension D = k + 2 ≥ 5. For each x ∈ RD , we decompose x = xx1:2 where x1:2 ∈ R2
and X3：D ∈ RD-2. Let Qθ ∈ R2×2 be the rotation matrix of angle θ, i.e., Qθ = (Sosθ [0S)θ) and
theloss L(X) = 1 (IIx1：2k2 - I)2 + 2 pD=3(2 + <Qα-3V,xi：2)x2, where α = D-2 and V is any
vector in R2 with unit norm. Here the manifold is given by Γ := {x | L(x) = 0} = {x ∈ RD |
x12 + x22 = 1, xj = 0, ∀j = 3, . . . , D}.
The basic idea is that we can add noise in the ‘auxiliary dimensions’ for j = 3, . . . , D to get the
regularization force on the circle {x21 + x22 = 1}, and the goal is to make the vector field induced
by the normal regularization always point to the same direction, say anti-clockwise. However, this
cannot be done with a single auxiliary dimension because from the analysis for label noise, we know
when L-71 l(∑⊥) is identity, the normal regularization term in Equation (10) has 0 path integral along
the unit circle and thus it must have both directions. The key observation here is that we can align the
magnitude of noise with the strength of the regularization to make the path integral positive. By using
k ≥ 3 auxiliary dimensions, we can further ensure the normal regularization force is anti-clockwise
and of constant magnitude, which is reminiscent of how a three-phase induction motor works.
Lemma C.8. Let Σ ∈ RD×D be given by Σj (x) = (1 + ^Qα^3 v, Q-n/2xi：2))(2+ (Q0-3v.xi2)),
if i = j ≥ 3 or 0 otherwise, then the solution of SDE (10) is the following (33) , which implies that
Y (t) moves anti-clockwise with a constant angular speed of (D - 2)/2.
Y1:2 (t) = Qt(D-2)/2Y1:2 (0)	and	Y3:D (t) ≡ 0.	(33)
Proof of Lemma C.8. Note that for any x ∈ Γ, it holds that
(2+〈Qa-3v,xi：2〉if i = j ≥ 3,
(V2L(x))ij =	xixj	if i,j ∈ {1,2},
10	otherwise.
(34)
Then clearly Σ only brings about noise in the normal space, and specifically, it holds that
Lv2L(x)(∑(x)) = diag(0,0,1 + (QaV,Q-∏∕2xL2),…，1 + <QD-3v, Q-∏∕2xL2))∙ Further note
that, by the special structure of the hessian in (34) and Lemma C.3, for any x ∈ Γ, we have
∂Φ(x) = (x2,-xι, 0,..., 0)>(x2,-xι, 0,..., 0) = (Q-n02Xi：2)(Q-n02xi：2)>. Combining these
28
Published as a conference paper at ICLR 2022
facts, the dynamics of the first two coordinates in SDE (10) can be simplified into
dxi：2(t)
dt
—
2 ∂Φ(x(t))∂2(VL)(x(t))[Lv2L(Σ(x(t))f∣
2	1:2
1	>	> D	j3
=-5Q-n/2x1：2x1：2Q-n/2 JS (1 +〈QI v, Q-n/2x1：2〉) V1:2(djjL)(X)
2	j=3
=-2Q-π∕2X1.2 卜-n/2Xl:2, X (1 + <Qα-3V, Q-n/2Xl:2〉)Qj--
=-2Q-n/2x1:2(Q-n/2x1:2, X Qa—Sv: + X (Qjo-v, Q-n/2x1:2〉2)
=-1Q-n/2x1:2 (0+ D - 2 IlQ-n/2x1:2 俏)=D 2 2Qn/2x1:2,
where the second to the last equality follows from the property of Qα and the last equality follows
from the fact that kx1:2 k22 = 1 for all x ∈ Γ. Note we require k ≥ 3 (or D ≥ 5) to allow
PD=3 (Qi-3v, Q-n/2xi:2〉2 = D-2 ||Q-n/2xi:2||2. On the other hand, we have dx3dD㈤
∂Φ kills the movement on that component.
The proof is completed by noting that the solution of x1:2 is
xi:2(t) = exp t∙ ∙ D - 2Qn/2)xi:2(0),
0 as
and by Lemma C.9,
D D — 2	∖	t(D-2)	t(D-2)
exp I t ∙-2— Qn/2)= (exp(Qn/2))	= Qi 2	= Q t(D-2).
□
T PiriiTiii ( ɑ p-γτ~ι ( ( 0 -1 ʌ ʌ — ( CoS 1 - Sin 1
Lemma C.9. exp( 1 0 ) = sin 1 cos1
Proof. By definition, for matrix A = (1 -j1), exp(A) = P∞=o A. Note A2 = -I, A3 = —A and
A4 = I, and by using this pattern, we can easily check that
∞ At
X A
乙t!
t=0
P∞=0(T)i (21)!
UO(T)i (2i+1)!
—
1
(2i+1)!
(2i)!
=csions11
- sin 1
cos 1
□
D	Proof of results in Section 6
In this section, we present the missing proofs in Section 6 regarding the overparametrized linear
model.
For convenience, for any p, r ≥ 0 and u ∈ RD, we denote by Brp(u) the `p norm ball of radius r
centered at u. We also denote vi:j = (vi, vi+1, . . . , vj)> for i, j ∈ [D].
D.1 Proof of Theorem 6.1
In this subsection, we provide the proof of Theorem 6.1.
Theorem 6.1. In the setting ofOLM, suppose the groundtruth is K-sparse andn ≥ Ω(κ ln d) training
data are sampled from either i.i.d. Gaussian or Boolean distribution. Then for any initialization xinit
(except a zero-measure set) and any > 0, there exist η0 , T > 0 such that for any η < η0, OLM
trained with label noise SGD (12) with LR equal to η for bT /η2c steps returns an -optimal solution,
with probability of 1 — e-Q(n) over the randomness of the training dataset.
29
Published as a conference paper at ICLR 2022
Proofof Theorem 6.1. First, by Lemma 6.6, it holds with probability at least 1 一 e-Q(n) that the
solution to (18), x*, is unique UP to and satisfies |x* | = ψ(w*). Then on this event, for any e > 0,
by Lemma 6.5, there exists some T > 0 such that xT given by the Riemannian gradient flow (17)
satisfies that xT is an e/2-optimal solution of the OLM. For this T , by Theorem 4.6, we know that
the [T∕η2C-th SGD iterate, Xη(∖T∕η2∖), satisfies ∣∣xη(∖T∕η2∖) 一 XT∣∣2 ≤ e/2 with probability at
least 1 一 e-。⑺ for all sufficiently small η > 0, and thus Xη(∖T∕η2∖) is an e-optimal solution of
the OLM. Finally, the validity of applying Theorem 4.6 is guaranteed by Lemma 6.2 and 6.3. This
completes the proof.	□
In the following subsections, we provide the proofs of all the components used in the above proof.
D.2 Proof of Lemma 6.2
Recall that for each i ∈ [n] fi(x) = f(u, V) = z>(uθ2 一 vθ2), Vfi(x) = 2(ZiθV), and K(x)=
(Kij (χ))i,j∈[n] whereeach Kij (x) = Wfi(x), Vfj (x)i. Then
V2'i(x) = 2 (z θu ) ((Zi Θ u)> -(Zi Θ v)>) + (fi(u,v) - y) ∙ diag(zi, z/.
一Zi θ V)
So for any x ∈ Γ, it holds that
V2L(X) = - XX GizΘΘUA((Zi Θ u)> -(zi Θ v)>) .	(35)
n i=1
Lemma D.1. For any fixed x ∈ RD, suppose {Vfi(x)}i∈[n] is linearly independent, then K(x) is
full-rank.
Proof of Lemma D.1. Suppose otherwise, then there exists some λ ∈ Rn such that λ 6= 0 and
λ>K(x)λ = 0. However, note that
n
λ>K(x)λ = X	λiλjKij(x)
i,j=1
n
= X	λiλjhVfi(x),Vfj(x)i
i,j=1
n	2
= X λiVfi (x)	,
i=1	2
which implies that Pin=1 λiVfi (x) = 0. This is a contradiction since by assumption {Vfi (x)}i∈[n]
is linearly independent.	□
Lemma 6.2. Consider the loss L defined in (14) and manifold Γ defined in (15). If data is full
rank, i.e., rank(Z) = -, then it holds that (a). Γ is a smooth manifold of dimension D 一 -; (b).
rank(V2L(x)) = -for all x ∈ Γ. In particular, rank(Z) = - holds with probability 1 for Gaussian
distribution and with probability 1 一 cd for Boolean distribution for some constant c ∈ (0, 1).
Proof of Lemma 6.2. (1) By preimage theorem (Banyaga & Hurtubise, 2013), it suffices to check the
jacobian [Vfι(χ),..., Vfn(x)] = 2[(-1 ^Uv),..., (-zn (^Uɔ] is full rank. Similarly, for the second
claim, due to (35). it is also equivalent to show that { -zzi uv }i∈[n] is of rank -.
Since Uv ∈ Γ ⊂ U, each coordinate is non-zero, thus we only need to show that {Zi}i∈[n] is of rank
-. This happens with probability 1 in the Gaussian case, and probability at least 1 一 cd for some
constant C ∈ (0,1) by Kahn et al. (1995). This completes the proof.	□
30
Published as a conference paper at ICLR 2022
D.3 Proof of Lemma 6.3
We first establish some auxiliary results. The following lemma shows the PL condition along the
trajectory of gradient flow.
Lemma D.2. Along the gradient flow generated by -VL, it holds that ∣∣VL(x(t))k2 ≥
16λmin(ZZ>) ∙ mini∈[d] ∣Ui(0)vi(0)∣L(x(t)),∀t ≥ 0.
To prove Lemma D.2, we need the following invariance along the gradient flow.
Lemma D.3. Along the gradient flow generated by -VL, uj (t)vj (t) stays constant for allj ∈ [d].
Thus, sign(uj (t)) = sign(uj (0)) and sign(vj (t)) = sign(vj (0)) for any j ∈ [d].
Proof of Lemma D.3.
∂ (Uj ⑴Vj ⑻=d∂(L ∙ Vj ⑴ + Uj ⑴∙ dj"
=VuL(u(t),v(t))j ∙ Vj(t) + Uj(t) ∙ VvL(u(t),v(t))j
=2 X(fi(u(t),v(t)) - yi)zi,juj(t)vj(t) - 2Uj(L X(fi(u(t),v(t)) - yi)zi,jVj(t)
n i=1	n	i=1
= 0.
Therefore, any sign change of uj (t), Vj (t) would enforce uj (t) = 0 or Vj (t) = 0 for some t > 0
since uj(t), Vj (t) are continuous in time t. This immediately leads to a contradiction to the invariance
of Uj (t)vj (t).	□
We then can prove Lemma D.2.
Proof of Lemma D.2. Note that
1n
∣VL(χ)k2 =滔 E (fi(χ) - yi)(fj(X)-y) hVfi(x), Vfj(x)i
i,j=1
1n
≥ j2 £(fi(x) — yi)2λmin(K(x))
i=1
2
=—L(X)λmin(K (x)),
j
where K(X) is a j × j p.s.d. matrix with Kij (X) = hVfi (X), Vfj (X)i. Below we lower bound
λmin(K(X)), the smallest eigenvalue of K(X). Note that Kij(X(t)) = 4 Pdh=1 zi,hzj,h((uh(t))2 +
(Vh (t))2 ), and we have
K (X(t)) = 4Z diag((u(t))2 + (V (t))2)Z >	8Zdiag(|u(t)	V(t)|)Z>
=) 8Zdiag(∣u(0) Θ v(0)|)Z> 占 8min ∣ui(0)vi(0)∣ZZT
i∈[d]
where (*) is by LemmaD.3. Thus λmin(K(x(t)) ≥ 8mi□i∈[d] ∣ui(0)vi(0)∣λmin(ZZT) for all t ≥ 0,
which completes the proof.	□
We also need the following characterization of the manifold Γ.
Lemma D.4. All the stationary points in U are global minimizers, i.e., Γ = {X ∈ U | VL(X) = 0}.
Proof of Lemma D.4. Since Γ is the set of local minimizers, each X in Γ must satisfy VL(X) = 0. The
other direction is proved by noting that rank({zi}i∈[n]) = j, which implies rank({Vfi(X)}i∈[n]) =
j.
Now, we are ready to prove Lemma 6.3 which is restated below.
31
Published as a conference paper at ICLR 2022
Lemma 6.3. Consider the loss function L defined in (14), manifold Γ and its open neighborhood
defined in (15). For gradient flow dχt = -VL(Xt) starting at any xo ∈ U, it holds that Φ(x0) ∈ Γ.
ProofofLemma 6.3. It suffices to prove gradient flow dχdtt) = -VL(χ(t)) converges when t → ∞,
as long as x(0) ∈ U. Whenever it converges, it must converge to a stationary point in U. The proof
will be completed by noting that all stationary point of L in U belongs to Γ (Lemma D.4).
Below We prove limt→∞x(t) exists. Denote C = 16mini∈[d] ∣Ui(0)vi(0)∣λmin(ZZ>), then it
follows from Lemma D.2 that
dx(t) = kVL(X(t))k ≤ kVL(x(t))k2 = -d⅞P = __1	dP∑W
dt	— PCL(x(t)	PL(x(t))	2√C	dt	.
Thus the total GF trajectory length is bounded by Rt∞o ∣∣dXdtt)∣∣ dt ≤ R∞o 一2√√c d√Ldt(t"dt ≤
L2x√^), where the last inequality uses that L is non-negative over RD. Therefore, the GF must
converge.	□
D.4 Proof of results in Section 6.2
Without loss of generality, we will assume Pi=1 zi2,j > 0 for all j ∈ [d], because otherwise we can
just delete the unused coordinate, since there won’t be any update in the parameter corresponding to
that coordinate. Moreover, in both gaussian and boolean setting, it can be shown that with probability
1, Pi=1 zi2,j > 0 for all j ∈ [d].
To study the optimal solution to (18), we consider the corresponding d-dimensional convex program
in terms of w ∈ Rd, which has been studied in Tropp (2015):
minimize R(W) = 4 X(X z2,j ) |Wjl,
n j=1 i=1
subject to Zw = Zw*.
(36)
Here we slightly abuse the notation of R and the parameter dimension will be clear from the context.
We can relate the optimal solution to (18) to that of (36) via a canonical parametrization defined as
follows.
Definition D.5 (Canonical Parametrization). For any w ∈ Rd, we define (V) = ψ(w) = ([w>]+1/2,
[-w>]+1/2)> as the canonical parametrization of w. Clearly, it holds that u2 - v2 = w.
Indeed, we can show that if (36) has a unique optimal solution, it immediately follows that the optimal
solution to (18) is also unique up to sign flips of each coordinate, as summarized in the lemma below.
Lemma D.6. Suppose the optimal solution to (36) is unique and equal to w*. Then the optimal
solution to (18) is also unique up to sign flips of each coordinate. In particular, one of them is given
by (U*,V*) = ψ(w*), that is, the CanoniCalparametrization of w*.
ProofofLemma D.6. Let (U, V) be any optimal solution of (18) and we define W = Uθ2 一 VGl2,
which is also feasible to (36). By the optimality of w*, we have
dn	dn	dn
X	Xz2,j	∣w*l	≤ X	Xz2,j	1Wj1	≤ X	X蜃(U2	+ Vj).	(37)
j=1	i=1	j=1	i=1	j=1	i=1
On the other hand, (U*,V*) = ψ(w*) is feasible to (18). Thus, it follows from the optimality of (U,V)
that
dn	dn	dn
X	Xz2,j	(U2	+ 有 ≤ X	Xz2,j	((Uj)2	+ (V*)2)	= X	Xz2,j	j (38)
j=1	i=1	j=1	i=1	j=1	i=1
32
Published as a conference paper at ICLR 2022
Combining (37) and (38) yields
dn	dn	dn
X	X z2j	(U2 +	率=X	X z2j	Iw； I = X	X Wj	烤-V ∣	⑸)
j =1	i=1	j=1	i=1	j =1	i=1
which implies that uθ2 - vθ2 is also an optimal solution of (36). Since w； is the unique optimal
solution to (36), We have Uθ2 - Vθ2 = w；. Moreover, by (39), We must have Uθ2 = [w；]+ and
Uθ2 = [w；]+, otherwise the equality would not hold. This completes the proof.	□
Therefore, the unique optimality of (18) can be reduced to that of (36). In the sequel, We shoW that the
latter holds for both Boolean and Gaussian random vectors. We divide Lemma 6.6 into to Lemma D.8
and D.7 for clarity.
Lemma D.7 (Boolean Case). Let zι,...,zn L吟 Unif({±1}d). There exist some constants C,c> 0
such that if the sample size n satisfies
n ≥ C[κ ln(d∕κ) + κ]
then with probability at least 1 — e-cn, the optimal solution of (18), (U, v), is unique up to sign flips
of each coordinate and recovers the groundtruth, i.e., Uθ2 — Vθ2 = w；.
ProofofLemma D.7. By the assumption that zι,...,zc i'ʃi;d' Unif({±1}d), we have PC=I ZIij = n
for all j ∈ [d]. Then (36) is equivalent to the following optimization problem:
minimize g (w) = kwk1 ,
subject to Zw = Z((u；)θ2 - (v*)θ2).
(40)
This model exactly fits the Example 6.2 in Tropp (2015) with σ = 1 and α = 1/ √2. Then applying
Equation (4.2) and Theorem 6.3 in Tropp (2015), (40) has a unique optimal solution equal to
(u*)θ2 - (v*)θ2 with probability at least 1 - e-ch2 for some constant c > 0, given that the sample
size satisfies
n ≥ C(K ln(d∕κ) + K + h)
for some absolute constant C > 0. Choosing h = ɪ and then adjusting the choices of C, C
appropriately yield the desired result. Finally, applying Lemma D.6 finishes the proof.	□
The Gaussian case requires more careful treatment.
Lemma D.8 (Gaussian Case). Let zι,...,Zc 'i吟 N (0, Id). There exist some constants C,c > 0
such that if the sample size satisfies
n ≥ CK ln d,
then with probability at least 1 — (2d + 1)e-cc, the optimal solution of (18), (U, V), is unique UP to
sign flips of each coordinate of U and V and recovers the groundtruth, i.e., Uθ2 — Vθ2 = w；.
ProofofLemma D.8. Since zι,...,Zc L吟 N(0, Id), we have
c
P X zi2,j ∈ [n/2, 3n/2], ∀j ∈ [d] ≥ 1 - 2de-cc
i=1
for some constant c > 0, and we denote this event by Ec . Therefore, on Ec , we have
DD
2X(Uj2+Vj2)≤R(x)≤6X(Uj2+Vj2)
j =1	j =1
or equivalently,
2(kUθ2k1 + kVθ2k1) ≤ R(x) ≤ 6(kUθ2k1 + Vθ2k1).
33
Published as a conference paper at ICLR 2022
Define w* = (u*)θ2 - (v*)θ2, and (36) is equivalent to the following convex optimization problem
minimize g(w) = n X (X z2,j) |wj+w* |,
subject to Zw = 0.
(41)
The point w = 0 is feasible for (41), and we claim that this is the unique optimal solution when n is
large enough. In detail, assume that there exists a non-zero feasible point w for (41) in the descent
cone (Tropp, 2015) D(g, w*) of g, then
λmin(Z ； D(g,W*)) ≤ kzWk2 =0
kwk2
where the equality follows from that w is feasible. Therefore, we only need to show that
λmin(Z; D(g, x*)) is bounded from below for sufficiently large n.
On En , it holds that g belongs to the following function class
G = {h : Rd → R | h(w) = ^XUj∣Wj|, U e y} with Y = {υ ∈ Rd : υj∙ ∈ [2, 6],∀j ∈ [d]}.
We identify gυ ∈ G with υ ∈ Υ, then D(g, w*) ⊆ ∪υ∈ΥD(gυ, w*)) := DΥ, which further implies
that
λmin (Z; D(g, w*)) ≥ λmin (Z; DΥ).
Recall the definition of minimum conic singular value (Tropp, 2015):
λmin (Z; DΥ) = inf sup hq, Zpi.
p∈DΥ∩Sd-1 q∈Sn-1
where Sn-1 denotes the unit sphere in Rn. Applying the same argument as in Tropp (2015) yields
P [λmin(Z; DY) ≥ √n-1 - W(DY) - h] ≥ 1 - e-h2/2.
Take the intersection of this event with En , and we obtain from a union bound that
λmin(Z； D(g, W*)) ≥ √n - 1 - W(DY)- h	(42)
with probability at least 1 - e-h2/2 - 2de-cn. It remains to determine w(DY), which is defined as
w(Dy) = Ez 〜N (0,id)	sup	hz,Pi = Ez 〜N (0,%) Sup Sup	hz,Pi .	(43)
p∈Dγ∩Sd-1	υ∈Y p∈D(gυ,x*)∩Sd-1
Without loss of generality, we assume that W* = (W1*, . . . , Wκ* , 0, . . . , 0)> with W1*, . . . , Wκ* > 0,
otherwise one only needs to specify the signs and the nonzero set of W* in the sequel. For any U ∈ Υ
and any P ∈ D(gυ, w*) ∩ Sd-1, there exists some τ > 0 such that gυ(w* + T ∙ P) ≤ gυ(w*), i.e.,
dd
X υj|w* + τPj | ≤ X υj |w*|
j=1	j=1
which further implies that
dκ	κ
T X Uj|Pj| ≤XUj(IW*|-|w*-τPjD ≤TXUj|Pj|
j=κ+1	j=1	j=1
where the second inequality follows from the triangle inequality. Then since each Uj ∈ [2, 6], it
follows that
d
κ
E ∣pjι≤ 3∑Ipj∣.
j=κ+1	j=1
34
Published as a conference paper at ICLR 2022
Note that this holds for all ξ ∈ Ξ simultaneously. Now let Us denote pi：K = (pι,...,Pκ) ∈ RK
andp(κ+i)-.d = (Pκ+ι,... ,Pd) ∈ Rd-κ, and similarly for other d-dimensional vectors. Then for all
p ∈ DΥ ∩ Sd-1, by Cauchy-Schwartz inequality, we have
kP(κ + 1)4k1 ≤ 3kPLκk1 ≤ 3√κkPl-.κk2.
Thus, for any z ∈ Rd and any p ∈ DΥ ∩ Sd-1, it follows that
hz, pi = hz1:K,p1:Ki + hz(K+1):d,p(K+1):di
≤	kzLK - kPLK- + ||P(K+1):dk1 ∙	max Jj|
j∈{K+1,...,d}
≤	kzLK - kPLK- + 3√κkPl-.κk2 • max	| Zj |
j∈{K+1,...,d}
≤ ∣∣ZLκ∣∣2 +3√κ • max	|zj|
j∈{K+1,...,d}
where the last inequality follows from the fact that p ∈ Sd-1. Therefore, combine the above inequality
with (43), and we obtain that
W(DY) ≤ E ∣∣ZLκ∣∣2 + 3√κ ∙ max	∣Zj|
j∈{K+1,...,d}
≤ √κ + 3√κ • E max |zj | .	(44)
j∈{K+1,...,d}
where the second inequality follows from the fact that E[|z1:K∣∣2] ≤ ∙√E[∣∣ZLκk2] = √κ. To bound
the second term in (44), applying Lemma D.9, it follows from (44) that
W(DY) ≤ √K + 3√2κ ln(2(d - K)).	(45)
Therefore, combining (45) and (42), we obtain
λmin(Z； D(g, w*)) ≥ √n — 1 — √K — 3√2κ ln(2(d — K)) — h.
Therefore, choosing h = √n — 1 /2, as long as n satisfies that n ≥ C(K ln d) for some constant
C > 0, we have λm∕(Z; D(g, w*)) > 0 with probability at least 1 — (2d + 1)e-cn. Finally, the
uniqueness of the optimal solution to (18) in this case follows from Lemma D.6.	□
Lemma D.9. Let Z 〜N(0, Id), then it holds that E Imaxi三⑷ |z/] ≤ '2 ln(2d).
Proof of Lemma D.9. Denote M = maxi∈[d] |zi|. For any λ > 0, by Jensen’s inequality, we have
eλ∙E[M] ≤ EdM] = E
max eλlzil
i∈[d]
d
≤ XE卜11却.
i=1
Note that E[eλlzil] ≤ 2 • E[eλzi]. Thus, by the expression of the Gaussian moment generating function,
we further have
d
eλ∙E[M] ≤ 2 X E 卜λz[ = 2d",
i=1
from which it follows that
E[M] ≤ ln≡ + |.
λ2
Choosing λ =，2 ln(2d) yields the desired result.	□
35
Published as a conference paper at ICLR 2022
D.5 Proof of Lemma 6.5
Instead of studying the convergence of the Riemannian gradient flow directly, it is more convenient
to consider it in the ambient space RD. To do so, we define a Lagrange function L(x; λ) =
R(x) + Pin=1 λi(fi(x) - yi) for λ ∈ Rn. Based on this Lagrangian, we can continuously extend
∂Φ(x)VR(χ) to the whole space RD. In specific, We can find a continuous function F : RD → RD
such that F(∙)∣r = ∂Φ(∙)VR(∙). Such an F can be implicitly constructed via the following lemma.
Lemma D.10. The `2 norm has a unique minimizer among {VxL(x; λ) | λ ∈ Rn} for any fixed
X ∈ RD. Thus we can define F : RD → RD by F(x) = argming∈τ,L3λ)∣λ∈Rn} ∣∣g∣∣2∙ Moreover,
it holds that hF (x), Vfi (x)i = 0 for all i ∈ [n].
Proof of Lemma D.10. Fix any x ∈ RD. Note that {VxL(x; λ) | λ ∈ Rn} is the subspace spanned
by {Vfi(x)}i∈[n] shifted by VR(x), thus there is unique minimizer of the `2 norm in this set. This
implies that F(x) = argming∈τxL3λ)∣λ∈Rn} I∣gk2 is well-defined.
To show the second claim, denote h(λ) = ∣∣VχL(χ; λ)∣2∕2, which is a quadratic function of λ ∈ Rn.
Then we have
hVR(x),Vf1(x)i	Pin=1λihVf1(x),Vfi(x)i
Vh(λ) =	.	I +	.
hVR(x), Vfn(x)i	Pin=1λihVfn(x),Vfi(x)i
hVR(x), Vf1 (x)i
.	I + K (x)λ.
hVR(x), Vfn(x)i
For any λ such that VxL(x; λ) = F (x), we must have Vh(λ) = 0 by the definition of F (x), which
by the above implies
(K(x)λ)i = -hVR(x), Vfi(x)i	for all i ∈ [n].
Therefore, we further have
n
hF(x),Vfi(x)i = hVR(x), Vfi(x)i + XλjhVfi(x),Vfj(x)i = hVR(x),Vfi(x)i + (K(x)λ)i = 0
j=1
for all i ∈ [n]. This finishes the proof.
□
Hence, with any initialization x(0) ∈ Γ, the limiting flow (17) is equivalent to the following dynamics
号=-4 F (x(t)).
(46)
Thus Lemma 6.5 can be proved by showing that the above χ(t) converges to x* as t → ∞. We first
present a series of auxiliary results in below.
Lemma D.11 (Implications for F(x) = 0). Let F : RD → RD be as defined in Lemma D.10. For
any x = uv ∈ RD such that F(x) = 0, it holds that for each j ∈ [d], either uj = 0 or vj = 0.
Proof. Since F(x) = 0, it holds for all j ∈ [d] that,
0
0
*(X) + X λi(x) f (X)
∂uj	i=1	∂uj
∂R	n
∂V^(X) + 2^λi(x)
2uj
2vj
nn
4 Xz2j + Xλi(X)Zij ,
n i=1	i=1
nn
n ∑>2,j-E λi(x)%j.
i=1	i=1
If there exists some j ∈ [d] such that uj 6= 0 and vj 6= 0, then it follows from the above two identities
that
n
Xzi2,j=0
i=1
which happens with probability 0 in both the Boolean and Gaussian case. Therefore, we must have
Uj = 0 or Vj = 0 for all j ∈ [d].	口
36
Published as a conference paper at ICLR 2022
Lemma D.12. Let F : RD → RD be as defined in Lemma D.10. Then F is continuous on RD.
Proof. Case I. We first consider the simpler case of any fixed x* ∈ U =(R \ {0})D, assuming that
K(x*) is full-rank. Lemma D.10 implies that for any λ ∈ Rn such that VχL(x*; λ) = F(x*), We
have
K(x*)λ= -[Vf1(x)...Vfn(x)]>VR(x).
Thus such λ is unique and given by
λ(x*)=-K(x*)-1[Vf1(x)...Vfn(x)]>VR(x).
Since K(x) is continuous around x*, there exists a sufficiently small δ > 0 such that for any
x ∈ Bδ(x*), K(x) is full-rank, Which further implies that K(x)-1 is also continuous in Bδ(x).
Therefore, by the above characterization of λ, We see that λ(x) is continuous for x ∈ Bδ(x*), and so
isF(x) = VR(x) +Pin=1λi(x)Vfi(x).
Case II. Next, We consider all general x* ∈ RD. Here for simplicity, We reorder the coordinates as
x = (u1, v1, u2, v2, . . . , ud, vd) With a slight abuse of notation. Without loss of generality, fix any
x* such that for some q ∈ [d], (ui(0))2 + (vi(0))2 > 0 for all i = 1, . . . , q and ui* = vi* = 0 for
all i = q + 1, . . . , d. Then VR(x*) and {Vfi(x*)}i∈[n] only depend on {zi,j}i∈[n],j∈[q], and for all
i ∈ [n], it holds that
(VR(x*))(2q+1):D = (Vfi(x*))(2q+1):D = 0.
Note that if We replace {Vfi(x)}i∈[n] by any fixed and invertible linear transform of itself, it Would
not affect the definition of F (x). In specific, We can choose an invertible matrix Q ∈ Rn×n
such that, for some q0 ∈ [q], (Zι, ...,zn) = (zι,..., Zn )Q satisfies that {Zi,i：q }i∈[q0] is linearly
independent andZi,i：q = 0 for all i = q0 + 1,...,n. We then consider [vfι(x),..., Vfn (x)]=
[Vf1(x), . . . , Vfn(x)] Q and the corresponding F (x). For notational simplicity, We assume that Q
can be chosen as the identity matrix, so that (Z1, . . . , Zn) itself satisfies the above property, and We
repeat it here for clarity
{zi,i：q}i∈[q0] is linearly independent and zi^-q = 0 for all i = q0 + 1,...,n.	(47)
This further implies that
(Vfi(x))1:(2q) = 0,	for all i ∈ {q0 + 1, . . . , n} and x ∈ RD.	(48)
In the sequel, we use λ for n-dimensional vectors and λ for q0-dimensional vectors. Denote2
λ(x) ∈ argmin VR(x) + X λiVfi(x)
λ∈Rn	i=1
2
λ(x) ∈ argmin
入 ∈Rq0
VR(x) + XXX λiV(fi(χ) I
i=1	1:(2q)
2
Then due to (47) and (48), we have
VR(x*) + XXX λi(x* )Vfi(x*)
n
VR(x*)+	Xλi(x)Vfi(x*)	= kF(x*)k2.
i=1	2
(49)
2We do not care about the specific choice of λ(x) or λ(x) when there are multiple candidates, and we only
need their properties according to Lemma D.10, so they can be arbitrary. Also, the minimum of `2 -norm of an
affine space can always be attained so argmin exists.
37
Published as a conference paper at ICLR 2022
On the other hand, for any x ∈ RD , by (48), we have
q0
VR(X) +	λXi(X)Vfi(X)
i=1
1:(2q)
= minn	VR(X) + Xλi(X)Vfi(X)
1:(2q)
n
VR(X) +Xλi(X)Vfi(X)
i=1
1:(2q)
= kF1:(2q)(X)k2
2
2
≤kF(x)k2≤
n
VR(x) + X λi(x*)Vfi(x)
i=1
(50)
2
where the first and third inequalities follow from the definition of F(x). Let X → x*, by the continuity
of VR(x) and {Vfi(χ)}i∈[n], we have
lim
x→x*
n
VR(x) + X λi(x*)Vfi(x)
i=1
n
VR(x*) + X λi(x*)Vfi(x*)
i=1
(51)
2
2
~
~
Denote K(x) = (Kij(x))(i,j)∈[q0]2 = (Nfi(X)i：(2q), Vfi(X)上⑶力了力三寸".By applying the
same argument as in Case I, since K(x*) is full-rank, it also holds that limχ→χ* λ(χ) = λ(x*), and
thus
χ→n* Il ^VR(x) + XXi(X)Vfi(X)i：(2q)
q0
VR(x) + EXi(X*) Vfi (x* )
i=1
1:(2q)
(52)
2
2
Combing (49), (50), (51) and (52) yields
lim kF1:(2q) (X)k2 = lim min	VR(X) + XλiVfi(X)
x→x*	x→x* λ∈Rn
i=1
1:(2q)
=kF (x*)k2.
2
(53)
Moreover, since kF(2q+1):D(X)k2 =	kF (X)k22 - kF1:(2q)(X)k22, we also have
xl→imx* kF(2q+1):D(X)k2 = 0.
It then remains to show that limχ→χ* Fi：(2q)(x) = Fi：(2q)(x*), which directly follows
(54)
from
limχ→x* 110 (x)=11q (x*) = λ(x*).
Now, for any e > 0, due to the convergence of λ(χ) and that K(x*) * 0, wecan pick a sufficiently
small δι such that for some constant a > 0 and all X ∈ Bδι (x*), it holds that ∣∣λ(x) 一 χ(x*)∣∣2 ≤ e/2
and
q0
VR(X) +	λXiVfi (X)
i=1
1:(2q)
2
≥	VR(X) +Xq λXi(X)Vfi(X)
2
1:(2q)
2
+ αkx - X(X)k2.
2
(55)
for all λX ∈ Rp , where the inequality follows from the strong convexity. Meanwhile, due to (48), we
have
lim	VR(X) + Xq λi(X)Vfi(X)
x→x*
i=1
1:(2q)
= lim	VR(X) +Xn λi(X)Vfi(X)
x→x*
2	I	i=1
1:(2q)
2
q0
VR(x)+E Xi(X*) Vfi(X*)
i=1
1:(2q)
2
lim
x→x*
q0
VR(X) +	λXi (X)Vfi (X)
i=1
1:(2q)
2
38
Published as a conference paper at ICLR 2022
where the second equality follows from (53) and the second equality is due to (52). Therefore, we
can pick a sufficiently small δ2 such that
q0
VR(X) +	λi(X)Vfi(X)
i=1
1:(2q)
≤ ∣∣∣VR(X) +Xq λXi(X)Vfi(X)
1:(2q)
α2
+ ɪ
2
(56)
2
for all X ∈ Bδ2 (X*). Setting δ = min(δ1, δ2), it follows from (55) and (56) that
∣∣λιq(x) - X(X)Il2 ≤ I，forall X ∈ Bδ(x*).
Recall that we already have ∣λX(X) - λX(X*)∣ ≤ /2, and thus
∣∣λiq (X)- λ(x*)ly, ∣∣2 = ∣∣λiq (X)- λ(x*)∣2 ≤ ∣∣λiq (X)- X(X)Il2 + ∣λ(x) -
for all X ∈ Bδ(x*). Therefore, we Seethatlimχ→χ* 11@(/) = λ(X*)iy/.
Finally, it follows from the triangle inequality that
X(x*)k2


≤
kF(X)- F(x*)k2 ≤
-F(x*)
1:(2q)
+ kF(2q+1):D (X)k2 + kF(2q+1):D (x*)k2
2
|
z
0
}
q0	q0
VR(X) +	λi(X)Vfi(X) - VR(X*) -	λi(X*)Vfi(X*)
i=1	i=1
1:(2q)
+ kF(2q+1):D(X)k2
2
≤ IIX λi(x)Vfi(x) - λi(x*)Vfi(x*)
+ ∣∣VR(χ) - VR(x*)k2 + kF(2q+1):D(X)Il2
2
where, as X → x*, the first term vanishes by the convergence of 1上勺，(x) and the continuity of each
Vfi(X), the second term converges to 0 by the continuity of VR(X) and the third term vanishes
by (54). Therefore, we conclude that
lim F(X) = F(X*),
x→x*
that is, F is continuous.
Lemma D.13. For any initialization X
(46)) is defined on [0, ∞).
□
,* ∈ Γ, the Riemmanian Gradient Flow (17) (or equivalently,
Proof of Lemma D.13. Let [0, T) be the right maximal interval of existence of the solution of
Riemannian gradient glow and suppose T 6= ∞. Since R(X(t)) is monotone decreasing, thus
R(X(t)) is upper bounded by R(X(0)) and therefore IVR(X(t))I is also upper bounded. Since
∣∣ ddtt)∣∣ ≤ ∣∣VR(X(t))∣2 for any t < T ,the left limit x(T-):= lim「→T - X(T) must exist. By
Corollary 1, Perko (2001), X(T -) belongs to boundary of U, i.e., uj(T-) = 0 or Vj(T-) = 0 for
some j ∈ [d] by Lemma D.11. By the definition of the Riemannian gradient flow in (17), we have
dt(Uj(t)vj(t))=(Vj(t)e> uj-(t)e>)ddt)
=-4 (Vj(t)e> uj-(t)e>)F(X(t)).
By the expression of F (X(t)) = VR(X(t)) + Pin=1 λi(X(t))Vfi(X(t)), we then have
n
n
-n X
dt (Uj(t) Vj(t))
i=1
n
4 X
n
i=1
1n
z2,j + 2 E%(χ(t))zi,j	uj(t)vj(t)-
i=1
2n	1n
n Σz2,j- 2 £%(x(t))zi,j uj(t)vj(t)
i=1	i=1
zi2,j	uj(t)vj(t).
—
Denote Sj = n4 Pn=Iz2j. It follows that |uj(t)vj(t)| = |uj(0)Vj(0)∣e-sjt for all t ∈ [0, T). Taking
the limit we have |uj(T—)Vj(T-)| ≥ |uj(0)Vj(0)∣e-sjT > 0. Contradiction with T = ∞!	□
39
Published as a conference paper at ICLR 2022
Before showing that F satisfies the PL condition, we need the following two intermediate results.
Given two points u and v in Rd, we say u weakly dominate v (written as u ≤ v) if and only if
ui ≤ vi, for all i ∈ [d]. Given two subsets A and B of RD, we say A weakly dominates B if and
only if for any point v in B , there exists a point u ∈ A such that u ≤ v.
Lemma D.14. For some q ∈ [D], let S be any q-dimensional subspace of RD and P = {u ∈ RD |
ui ≥ 0, ∀i ∈ [D]}. Let u? be an arbitrary point in P and Q = P ∩ (u? + S). Then there exists a
radius r > 0, such that Br1(0) ∩ Q weakly dominates Q, where Br1(0) is the `1 -norm ball of radius r
centered at 0.
As a direct implication, for any continuous function f : P → R, which is coordinate-wise non-
decreasing, minx∈U f (x) can always be achieved.
Proof of Lemma D.14. We will prove by induction on the environment dimension D. For the base
case of D = 1, either S = {0} or S = R, and it is straight-forward to verify the desired for both
scenarios.
Suppose the proposition holds for D - 1, below we show it holds for D. For each i ∈ [D], we apply
the proposition with D - 1 to Q ∩ {u ∈ P | ui = 0} (which can be seen as a subset of RD-1), and
let ri be the corresponding `1 radius. Set r = maxi∈[D] ri, and we show that choosing the radius to
be r suffices.
For any v ∈ Q, we take a random direction in S, denoted by ω. If ω ≥ 0 or ω ≤ 0, we denote by y
the first intersection (i.e., choosing the smallest λ) between the line {v - λ∣ω∣}λ≥o and the boundary
of U, i.e., ∪iD=1{z ∈ RD | zi = 0}. Clearly y ≤ v. By the induction hypothesis, there exists a
u ∈ Br1 (0) ∩ Q such that u ≤ y. Thus u ≤ v and meets our requirement.
If ω has different signs across its coordinates, we take y1, y2 to be the first intersections of the line
{v - λ∣ω∣}λ∈R and the boundary of U in directions of λ > 0 and λ < 0, respectively. Again by
the induction hypothesis, there exist u1, u2 ∈ Br1(0) ∩ Q such that u1 ≤ y1 and u2 ≤ y2. Since v
lies in the line connecting u1 and u2, there exists some h ∈ [0, 1] such that v = (1 - h)u1 + hu2.
It then follows that (1 - h)u1 + hu2 ≤ (1 - h)y1 + hy2 = v. Now since Q is convex, we have
(1 - h)u1 + hu2 ∈ Q, and by the triangle inequality it also holds that k(1 - h)u1 + hu2k1 ≤ r, so
(1 - h)u1 + hu2 ∈ Br1(0) ∩ Q. Therefore, we conclude that Br1(0) ∩ Q weakly dominates Q, and
thus the proposition holds for D. This completes the proof by induction.	□
Lemma D.15. For some q ∈ [D], let S be any q-dimensional subspace of RD and P = {u ∈ RD |
ui ≥ 0, ∀i ∈ [D]}. Let u? be an arbitrary point in P and Q = P ∩ (u? + S). Then there exists
a constant C ∈ (0,1] such that for any sufficiently small radius r > 0, C ∙ Q weakly dominates
P ∩ (u? + S + B/(0)), where Br(0) is the '2-norm ball ofradius r centered at 0.
Proof of Lemma D.15. We will prove by induction on the environment dimension D. For the base
case of D = 1, either S = {0} or S = R. S = R is straight-forward; for the case S = {0}, we just
need to ensure C|u?| ≤ |u?| - r, and it suffices to pick r = |u?| and C = 0.5.
Suppose the proposition holds for D - 1, below we show it holds for D. For each i ∈ [D], we
first consider the intersection between P ∩ (u? + S + Br2(0)) and Hi := {u ∈ RD | ui = 0}. Let
ui be an arbitrary point in P ∩ (u? + S) ∩ Hi, then P ∩ (u? + S) ∩ Hi = P ∩ (ui + S) ∩ Hi =
P ∩ (ui + S ∩ Hi). Furthermore, there exists {αi}i∈[D] which only depends on S and satisfies
P ∩ (u* + S + Br2(0)) ∩ Hi ⊂ P ∩ (Ui + S∩ Hi + Bair (0) ∩ Hi). Applying the induction hypothesis
to P ∩ (ui + S ∩ Hi + Bα2ir(0) ∩ Hi), we know there exists a C > 0 such that for sufficiently small r,
C(P∩ (u? + S) ∩ Hi) = C(P∩ (ui + S∩ Hi)) weakly dominates P∩ (ui + S∩ Hi + Bα2 r(0) ∩ Hi).
For any point v in Q and any z ∈ Br2(0), we take a random direction in S, denoted by ω. Ifω ≥ 0
or ω ≤ 0, we denote by y the first intersection between {v + Z — λ∣ω∣}λ≥o and the boundary of U.
Clearly y ≤ v. Since y ∈ P ∩ (u? + S + Br2 (0)) ∩ Hi ⊂ P ∩ (ui + S ∩ Hi + Bα2ir(0) ∩ Hi), by
the induction hypothesis, there exists a u ∈ C(P ∩ (u? + S) ∩ Hi) such that u ≤ y. Thus z ≤ v + z
and z ∈ C(P ∩ (u? + S)) = C ∙ Q.
If ω has different signs across its coordinates, we take y1, y2 to be the first intersections of the line
{v + z - λ∣ω∣}λ∈R and the boundary of U in directions of λ > 0 and λ < 0, respectively. By the
induction hypothesis, there exist u1,u2 ∈ C ∙ Q such that uι ≤ yι and u2 ≤ y2. Since V + Z lies
40
Published as a conference paper at ICLR 2022
in the line connecting u1 and u2, there exists some h ∈ [0, 1] such that v + z = (1 - h)y1 + hy2.
It then follows that (1 - h)u1 + hu2 ≤ (1 - h)y1 + hy2 = v + z. Since Q is convex, we have
(1 - h)u1 + hu2 ∈ cQ. Therefore, we conclude that cQ ∩ Q weakly dominates P ∩ (u? + S+Br2(0))
for all sufficiently small r, and thus the proposition holds for D. This completes the proof by
induction.	□
Lemma D.16. (PoIyak-Lojasiewicz Conditionfor F.) For any x* such that L(x*) = 0, i.e., x* ∈ Γ,
there exist a neighbourhood U0 of x* and a constant c > 0, such that kF(x)k2 ≥ C ∙ max(R(x) —
R(x*), 0) for all X ∈ U0 ∩ Γ. Note this requirement is only non-trivial when ∣∣F(x*)k2 = 0 since F
is continuous.
Proof of Lemma D.16. It suffices to show the PL condition for {x | F(x) = 0}. We need to show
for any x* satisfying F(x*) = 0, there exist some e > 0 and C > 0, such that for all X ∈ Γ ∩ B2(x*)
with R(x) > R(x*), it holds that kF (x)k22 ≥ C(R(x) —R(x*)).
Case I. We first prove the case where x = uv itself is a canonical parametrization of w =
uθ2 — vθ2,i.e., UjVj = 0 for all j ∈ [d]. Since x* satisfies VF(x*) = 0, by LemmaD.11,we have
x* = ψ(w*) where w* = (u*)2 — (v*)2. In this case, we can rewrite both R and F as functions of
w ∈ Rd. In detail, we define R0(w) = R(ψ(w)) and F 0(w) = F (ψ(w)) for all w ∈ Rd. For any w
in a sufficiently small neighbourhood of w*, it holds that sign(wj) = sign(wj*) for all j ∈ [q]. Below
we show that for each possible sign pattern of w(q+1):d, there exists some constant C which admits
the PL condition in the corresponding orthant. Then we take the minimum of all C from different
orthant and the proof is completed. W.L.O.G., we assume that wj ≥ 0, for all j = q + 1 . . . , d.
We temporarily reorder the coordinates as x = (u1, v1, u2, v2, . . . , ud, vd)>. Recall that Z =
[z1, . . . , zn]> is a n-by-d matrix, and we have
kF 0(w)k22 = min (a — sign(w)	Z>λ)θ2, |w| ,
where a = M PZi zΘ2 ∈ Rd. SinCe F(x*) = 0, there must exist λ* ∈ Rn, such that the first 2q
coordinates of VR(x*) + Pin=1 λi*Vfi(x*) are equal to 0. As argued in the proof of Lemma D.12,
we can assume the first q0 rows of Z are linear independent on the first q coordinates for some q0 ∈ [q].
In other words, Z can be written as 0
ZB
ZD
where ZA ∈ Rq0×q. We further denote λa := Xi©,
λb := X(q0+i)：n, Wa := wi：q and Wb := w(q+i)：d for convenience, then we have
kF 0(w)k22 = minn	(ai	+ sign(wa)	ZA>λa)θ2, |wa|	+	(a2 +	ZB>λa +	ZD>λb)θ2,	wb	. (57)
Since every W in Γ is a global minimizer, R(W) = R(W) + PNi λ*(z>W — yi) := g>w + R0(w*),
where g = sign(W) a + Z>λ*. Similarly we define ga := gi:q and gb := g(q+i):d. It holds that
ga = 0 and we assume ZDgb = 0 without loss of generality, because this can always be done by
picking suitable λ* for i = q0 + 1,..., n. (We have such freedom on X*o+i:n because they doesn't
affect the first 2q coordinates.)
We denote λa — λ*a by ∆λa, then since 0 = ga = sign(Wa) ai + ZA>λ*a, we further have
(ai +sign(Wa)	ZA>λa)θ2, |Wa| = (ai + sign(Wa)	ZA>λ*a +sign(Wa)	ZA>∆λa)θ2, |Wa|
= (sign(Wa)	ZA>∆λa)θ2, |Wa| .
On the other hand, we have gb = sign(Wb) a2 + ZB>λ*a + ZD>λb* = a2 + ZB>λ*a + ZD>λb* by the
assumption that each coordinate of Wb is non-negative. Combining this with the above identity, we
can rewrite Equation (57) as:
∣F0(w)∣2 = min <(Z>∆λa)θ2,心。|〉+ ((gb + Z>∆λa + Z1λb产,Qwb .	(58)
Now suppose R0(W) — R0(W*) = gb>Wb = δ for some sufficiently small δ (which can be controlled
by e). We will proceed in the following two cases separately.
41
Published as a conference paper at ICLR 2022
•	Case I.1: ∣∣∆λak2 = Ω(√δ). Since ZA has full row rank, [[(//^Aa)02'1=
(ZA>∆λa)22 ≥ k∆λak22 λ2min(ZA) is lower-bounded. On the other hand, we can choose
small enough such that ∀i ∈ [q]∣(wa)2∣ ≥ 11 (Way2. Thus the first term of Equation (58) is
Iowerboundedby ∣∆λak2 入舄正(。4)∙ mini∈[q] 2(Way = Ω(δ) = Ω(R0(w) - R0(wa)).
•	CaseL2: k∆λa∣2 = O(√δ). Let U = gb+Zj∆λa+ZDλb, then we have u ∈ S+B2√δ(°)
for some constant c > 0, where S = {gb + ZD>λb | λb ∈ Rn-q0}. By Lemma D.14, there
exists some constant co ≥ 1, such that * ∙ S weakly dominates S + B2√δ(°). ThUS We
have ∣F0(w)∣2 ≥ infu∈s+Bc√δ(o)(uθ2, w6) ≥ infu∈ɪ$ (sθ2,w6), where the last step
is because each coordinate of Wb is non-negative.
Let A be the orthogonal complement of span(ZD, gb), i.e., the spanned space of columns
of ZD	and gb,	we know	Wb	∈	⅛∙gb	+	A,	since	ZDWb	=	ZDw2 = ° and g> Wb	=	δ.
kgb k2
Therefore,
inf	kF0(W)k2
w:R0(w)-R0(w*) = δ>0 R0(w) - R0(Wa)
≥	inf	inf DUθ2, ?E
Wb：R0(w) —R0(w* ) = δ>0 u∈ -1 ∙S ∖	δ /
c0
≥ J	inf	(uθ2, Wb).
c0 wb∈	⅛∙gb + A,Wb≥0,u∈S
kgb k22
(59)
Note uθ2 , Wb is a monotone non-decreasing function in the first joint orthant, i.e.,
{(u, Wb) ∈ Rd × Rd-q0 | u ≥ °, Wb ≥ °}, thus by Lemma D.15 the infinimum can
be achieved by some finite (u, Wb) in the joint first orthant. Applying the same argument to
each other orthant of u ∈ Rd , we conclude that the right-hand-side of (59) can be achieved.
On the other hand, we have u>Wb = δ > ° for all Wb ∈	' gb + A and U ∈ S, by
kgb k2
ZDgb = ° and the definition of A. This implies there exists at least one i ∈ [d - q0]
such that W2,iui > °, which further implies uθ2, Wb > °. Therefore, we conclude that
∣F0(w)∣2 = Ω(R0(w)- R0(wo)).
Case II. Next, for any general x = uv , we define W = uθ2 - vθ2 and m = min{uθ2, vθ2 },
where min is taken coordinate-wise. Then we can rewrite ∣F (x)∣22 as
min
λ∈Rn
min
λ∈Rn
min
λ∈Rn
≥ min
λ∈Rn
+
+
+
+
+
Z
-Z
Z
-Z
Z
-Z
Z
-Z
Z
-Z
θ2
λ	ψ(W)θ2 + min
λ∈Rn
1
2
λ	ψ(W) + min
2 λ∈Rn
+
1
Z
-Z
1
a
a
a
a
42
Published as a conference paper at ICLR 2022
Then applying the result for the previous case yields the following for some constant C ∈ (0, 1):
kF(χ)k2 ≥ c(R(ψ(w)) - R(ψ(w*)) + min (『+ [-Z]λλj θ [√m
λ∈Rn	a -	m
=C(R(ψ(w)) — R(x*) + 2 (a02,m)
≥ C(R(ψ(w)) — R(x*) + 2 min ai〈a, m)
i∈[d]
=C (R(ψ(w)) — R(x*) + min ai(R(x) — R(ψ(w)))
i∈[d]
≥ min < C, min a J (R(X) — R(x*)),
i∈[d]
2
2
where the first equality follows from the fact that x* = ψ(w*) and the last inequality is due to the
fact that both R(ψ(w) — R(ψ(w*)) and R(X) — R(ψ(w)) are non-negative. This completes the
proof.	□
Now, based on the PL condition, we can show that (17) indeed converges.
Lemma D.17. The trajectory of the flow defined in (17) has finite length, i.e., f∞∞0o ∣∣ 箸 ∣∣2dt < ∞
for any X* ∈ Γ. Moreover, X(t) converges to some X(∞) when t → ∞ with F (X(∞)) = 0.
Proof of Lemma D.17. Note that along the Riemannian gradient flow, R(X(t)) is non-increasing,
thus ∣X(t)∣2 is bounded over time and {X(t)}t≥0 has at least one limit point, which we will call X*.
Therefore, R(X*) is a limit point of R(X(t)), and again since R(X(t)) is non-increasing, it follows
that R(X(t)) ≥ R(X*) and limt→∞ R(X(t)) = R(X*). Below we will show limt→∞ X(t) = X*.
Note that d¾≡ = <VR(x(t)),陪E = — <VR(x(t)), 1F(x(t)))= - 4 ∣F(x(t))∣2 where
the last equality applies Lemma D.10. By Lemma D.16, there exists a neighbourhood of X*, U0,
in which PL condition holds of F. Since X* is a limit point, there exists a time T0 , such that
XT0 ∈ U. Let T1 = inf t≥T0 {X(t) ∈/ U0} (which is equal to ∞ if X(t) ∈ U0 for all t ≥ T0 ).
Since X(t) is continuous in t and U is open, we know T1 > T0 and for all t ∈ [T0 , T1), we have
kF(χ(t))∣2 ≥ √C(R(χ(t)) - R(x*))1/2.
Thus it holds that for t ∈ [T0 , T1),
d(R(X(t)d - RE)) ≤ -占(R(χ(t)) - R(x*))1/2 kF (X(t))∣2 ,
that is,
d(R(x(t)) - R(x*))1/2
dt
≤-38c kF(x(t))∣2.
Therefore, we have
T1
kF (X(t))k2 dt ≤
t=T0
8
√c(R(X(T0))- R(x*))1/2.
(60)
Thus if we pick T0 such that R(X(T0)) - R(X*) is sufficiently small, R(T1) will remain in U, which
implies that T1 cannot be finite and has to be ∞. Therefore, Equation (60) shows that the trajectory
of X(t) is of finite length, so X(∞) := limt→∞ X(t) exists and is equal to X*. As a by-product, F(X*)
must be 0.
Finally, collecting all the above lemmas, we are able to prove Lemma 6.5. In Lemma D.17 we already
show the convergence of X(t) as t → ∞, the main part of the proof of Lemma 6.5 is to show the
x(∞) cannot be sub-optimal stationary points of R on Γ, the closure of Γ. The key idea here is that
we can construct a different potential φ for each such sub-optimal stationary point X*, such that (1)
φ(xt) is locally increasing in a sufficiently neighborhood of x* and (2) limx→x* φ(x) = -∞.
Lemma 6.5. Let {Xt }t≥0 ⊆ RD be generated by the flow defined in (17) with any initialization
X0 ∈ Γ. Then X∞ = limt→∞ Xt exists. Moreover, X∞ = X* is the optimal solution of (18).
43
Published as a conference paper at ICLR 2022
Proof of Lemma 6.5. We will prove by contradiction. Suppose x(∞) = uv((∞∞)) = limt→∞ x(t) is
not the optimal solution to (18). Denote w(t) = (u(t))2 - (v(t))2, then w(∞) = limt→∞ w(t)
is not the optimal solution to (36). Thus We have R(w(t)) > R(W) Without loss of generality,
suppose there is some q ∈ [d] such that (ui (∞))2 + (vi (∞))2 > 0 for all i = 1, . . . , q and
ui(∞) = vi(∞) = 0 for all i = q + 1, . . . , d. Again, as argued in the proof of Lemma D.12, We can
assume that, for some q0 ∈ [q],
{zi,i：q}i∈[q0] is linearly independent and Zi,i：q = 0 for all i = q0 + 1,...,n.	(61)
Since both w(∞) and w* satisfy the constraint that Zw(∞) = Zw* = Y, We further have
0 = hzi , w(∞)i = hzi , w i = hzi,(q+1):d, w(q+1):di, for all i = q + 1, . . . , n.	(62)
Consider a potential function φ : U → R defined as
d
夕(x)=夕(u, V)= X wj* ln(uj )21{wj* > 0} - ln(vj)2 1{wj* < 0} .
j=q+1
Clearly limt→∞ 夕(x(t)) = -∞ if limt→∞ x(t) = x(∞). Below we will show contradiction if
χ(∞) is SUbOPtimaL Consider the dynamics of 夕(x) along the Riemannian gradient flow:
ddt(X⑴)=(ViXx⑴),
t)), 1F (x(t))
(63)
where F is defined previously in Lemma D.10. Recall the definition of F, and we have
hVi(x(t)),F(x(t))i = (Vi(x(t)), 1 VR(x(t)) + 4 XX λi(x(t))Vfi(x(t)):
'------------------------V-------------------------}
I1
+ (V以x(t)), 14 XX λi(x(t))Vfi(x(t))∖ .
i=q0+1
X--------------------------------------}
(64)
"^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^""^
I2
To show(▽夕(x(t)), F(χ(t))i < 0, we analyze Ii and工2 separately. By the definition of 夕(x), we
have
d
Vi(x) = X	2wj*
j=q+1
^1{w* > 0}
1{wj* < 0}
ej	V	eD+j
where ej is the j-th canonical base ofRd. Recall that Vfi(x) = 2 -zzi uv , and we further have
1{wj* > 0}	1{wj* < 0}
------------〈ej, Zi Θ Ui +------------〈ej ,Zi Θ Vi
uj	vj
1{wj* > 0}	1{wj* < 0}
-	Zijuj +	-	ZijVj
uj	Vj
n
nd
I2 = X	λi (x(t)) X	wj*
i=q0+1	j=q+1
nd
= X	λi (x(t)) X	wj*
i=q0+1	j=q+1
nd
=	λi(x(t))	wj*zi,j = ∑ λi (x(t))hzi,(q+1):d, w(*q+1):di = 0	(65)
i=q0+1	j=q+1	i=q0+1
where the last equality follows from (62).
Next, we show that I1 < 0 by utilizing the fact that w* - w(∞) is a descent direction of R0(w). For
w ∈ Rd, define fi(w) = z>w and
q0
R(W) = R(W) + y^λi(x(∞))(fi(w) - yi).
i=1
44
Published as a conference paper at ICLR 2022
zɔl IC	_ TTh ∩	,♦ C ♦ Γ7	T Z^ ∙ . Λ 11 .Λ 1	7 /	∖	CC	1	∙	_ Γ T	1,1
Clearly, for any W ∈ RD satisfying Zw = Y, it holds that fi(w) — yi = 0 for each i ∈ [n], and thus
R(w) = R(w). In particular, We have R(w(∞)) = R(w(∞)) > R(w*) = R(w*). Since R(w) is a
~
~
~
~
convex function, it follows that R(w(∞) + s(w* — w(∞))) ≤ sR(w*)÷ (1 — s)R(∞) < R(w(∞))
for all 0 < s ≤ 1, which implies dR(w(∞) + s(w* — w(∞)))∣s=0 < —2c < 0+ for some constant
c > 0. Note that, for small enough s > 0, we have
R(W(∞) + S(W* — w(∞))) = 4X (Xz2,j) ∣wj(∞) + s(w； — wj(∞))∣
-X (X z2,jj sign(wj(∞))(Wj(∞) + s(w； — wj(∞)))
4d
+ -E
j=q+ι
(X ZjS∣w; ∣.
Therefore, we can compute the derivative with respect to s at s = 0 as
一	dR....................
—	2c > —^-(w(∞) + s(w* — w(∞)))
s=0
-	X (X Zj Sign(Wj (∞))(w*-wj 3))+-
j=1 ∖i=1
j=q+ι
(X Zj ∣wR
+ y^λj(x(∞))z> (w* — Wj (∞))
i=1
-	X (XZjSign(Wj(∞))(w* -w∞+4 X (XZjIW;∣
+ ΣS(W；— Wj(∞)) E%w∞))zi,j+ £ wj E%w∞))zi,j
j=i
i=1
j=q+1	i=1
(66)
d
X
q
q
/
q
d	q
where the second equality follows from the fact that wg+i)d(∞) = 0. Since χ(t)
x(∞), we must have F(χ(∞)) = 0, which implies that for each j ∈ {1,...,q},
converges to

n
q
0 = ∂∂R (χ(∞)) + X λi (x(∞
duj	W
q0
i , ,	..	,	,	4 二
-(x(∞)) = 2uj(∞) - X
i=1
z2,j+E%w∞))zi,j	,
0
∞)) + fλi(x(∞
i=1
.	. n
i(x(∞)) = 2vj(∞) - X
Z j
i,j
i=1
q0
i=1	i=1
—E%(χ(∞))%j .
n
Combining the above two equalities yields
I
4n	q0
-ΣS Zi,j = — sign(wj(∞)) fλi(x(∞))zi,j,	for all j ∈ [q].
i=1	i=1
Apply the above identity together with (66), and we obtain
q	q	4 d ( n
-2c>E-Sign(Wj(∞))2(w*- w(∞))E%(χ(∞))z⅛,j + - E (EZj∣wj∣
j=1	i=1	j=q+1 ∖i=1	)
q	q	d	q
+∑(w*- wj(∞))Eλi(χ(∞))%j+ E w*Eλi(χ(∞))%,j
j = 1	i=1	j=q+1	i=1
4d n	d	q
=-X XZj ∣w*∣+ X w* X%(χ(∞))%,j	(67)
j=q+1 ∖i=1	j=q+1	i=1
45
Published as a conference paper at ICLR 2022
On the other hand, by directly evaluating VR(χ(t)) and each Vfi(X(t)), We can computeIi as
Vd^	w*1{w* > 0}	2 Vn^ Q	1	Xqr
Ii = X	—UT^—	n X zi,j Uj⑴+	2	X	%(XD)Zij U ⑴
j =q+1	j	i=1	i=1
Vd^ w*1{w* <	0}	2	Vn^ Q	1	Xqr
-X -V^- n X	zi,jvj⑴-2	X λi(X(U)zi,jVj⑴
j=q+i	j	i=i	i=i
d	n	d	q0
=2 X	Xz2,j 同+2 X Wj X%(χ(t))Zi,j
j=q+i	i=i	j=q+i	i=i
2 d	n	1 d	q0
=n X	X z2,j	|wj| + 2 X Wj X	λi(X(S))%
j=q+i	i=i	j=q+i	i=i
1	d	q0
+ 2 X Wj X (λi(χ(t)) - λi(x(∞))) zi,j.
j=q+i	i=i
We already know that λiy,(x) is continuous at x(∞) by the proof of LemmaD.12, so the third term
converges to 0 as X(t) tends to X(∞). NoW, applying (67), We immediately see that there exists some
δ > 0 such that Ii < -c for X(t) ∈ Bδ(X(∞)). As we have shown in the above that I2 = 0, it then
follows from (63) and (64) that
苧(x(t)) > c, for all x(t) ∈ Bδ(x(∞)).	(68)
Since limt→∞ X(t) = X(∞), there exists some T > 0 such that X(t) ∈ Bδ(X(∞)) for all t > T. By
the proof ofLemma D.13, we know that 夕(X(T)) > 一∞, then it follows from (68) that
lim 夕(X(t))=夕(X(T)) + /	d,(X⑴)d力 > ^(x(t)) + / Cdt = ∞
t→∞	T dt	T
which is a contradiction. This finishes the proof.	□
D.6 Proof of Theorem 6.7
Here we present the lower bound on the sample complexity of GD in the kernel regime.
Theorem 6.7. Assume zι,...,zn ”吟 N(0, Id) and y% = z>wj, for all i ∈ [n]. Define the loss
with linearized model as L(X) = in=i (fi (X0) + hVfi(X0), X 一 X0i 一 yi)2, where X = uv and
X0 = uv0 = α 11 . Then for any groundtruth Wj, any learning rate schedule {ηt}t≥i, and any fixed
number of steps T, the expected '2 loss of X(T) is at least (1 — d) IlWjk 2, where X(T) is the T -th
iterate of GD on L, i.e., X(t + 1) = X(t) 一 ηtVL(X(t)), for all t ≥ 0.
Proof of Theorem 6.7. We first simplify the loss function by substituting X0 = X — X(0), so corre-
spondingly X00 = 0 and we consider L0(X0) := L(X) = (hVfi(X(0)), X0i — yi)2. We can think as if
GD is performed on L0(X0). For simplicity, we still use the X and L(X) notation in below.
In order to show test loss lower bound against a single fixed target function, we must take the
properties of the algorithm into account. The proof is based on the observation that GD is rotationally
equivariant (Ng, 2004; Li et al., 2020c) as an iterative algorithm, i.e., if one rotates the entire data
distribution (including both the training and test data), the expected loss of the learned function
remains the same. Since the data distribution and initialization are invariant under any rotation, it
means the expected loss of X(T) with ground truth being Wj is the same as the case where the ground
truth is uniformly randomly sampled from all vectors of `2 -norm kWj k2.
Thus the test loss of X(T) is
Ez	(hVfz(X(0)), X(T)i	— hz, Wji)2	=Ez	h(hz, Wj	—	(U(T)	— v(T))i)2i	=kWj—(U(T)—v(T))k22.
(69)
46
Published as a conference paper at ICLR 2022
Note X(T) ∈ span{Vfχ(χ(O))}, which is at most an n-dimenSional space spanned by the gradients
of model output at x(0), so is u(T) - v(T). We denote the corresponding space for u(T) - v(T)
by S, so dim(S) ≤ n and it holds that ∣∣w* 一 (U(T) 一 v(T))k2 ≥ ∣∣(Id 一 PS)w*∣∣2, where PS is
projection matrix onto space S.
The expected test loss is lower bounded by
Ew* 忸Zihkw* —(U(T) - v(T))k2ii = EzihEw* [kw*-(U(T) - v(T))|同]
≥	min Ew* hk(Id — PS)w*∣∣2]
{zi}i∈[n]
≥	(1- n) kw*k2.
□
47