Published as a conference paper at ICLR 2022
Independent SE(3)-Equivariant Models for
End-to-End Rigid Protein Docking
Octavian-Eugen Ganeat * *	Xinyuan Huang§*	Charlotte Bunne	Yatao Biant
MIT	ETH Zurich	ETH Zurich	Tencent AI Lab
Regina Barzilay	Tommi Jaakkola	Andreas Krause
MIT	MIT	ETH Zurich
Abstract
Protein complex formation is a central problem in biology, being involved in most
of the cell's processes, and essential for applications, e.g. drug design or protein
engineering. We tackle rigid body protein-protein docking, i.e., computationally
predicting the 3D structure of a protein-protein complex from the individual un-
bound structures, assuming no conformational change within the proteins happens
during binding. We design a novel pairwise-independent SE(3)-equivariant graph
matching network to predict the rotation and translation to place one of the proteins
at the right docked position relative to the second protein. We mathematically
guarantee a basic principle: the predicted complex is always identical regardless
of the initial locations and orientations of the two structures. Our model, named
EQUIDOCK, approximates the binding pockets and predicts the docking poses
using keypoint matching and alignment, achieved through optimal transport and a
differentiable Kabsch algorithm. Empirically, we achieve significant running time
improvements and often outperform existing docking software despite not relying
on heavy candidate sampling, structure refinement, or templates. 1
1	INTRODUCTION
In a recent breakthrough, ALPHAFOLD 2 (Jumper
et al., 2021; Senior et al., 2020) provides a solution
to a grand challenge in biology-inferring a pro-
tein's three-dimensional structure from its amino
acid sequence (Baek et al., 2021), following the
dogma sequence determines structure.
a.
protein Z b. protein Z-dependent »C
inhibitor
protein Z-dependent
inhibitor
Figure 1: Different views of the 3D structure
of a protein complex. a. Surface and b. cartoon
view of protein Z and its inhibitor.
protein Z
Besides their complex three-dimensional nature,
proteins dynamically alter their function and struc-
ture in response to cellular signals, changes in the
environment, or upon molecular docking. In par-
ticular, protein interactions are involved in various biological processes including signal transduction,
protein synthesis, DNA replication and repair. Molecular docking is key to understanding protein
interactions’ mechanisms and effects, and, subsequently, to developing therapeutic interventions.
We here address the problem of rigid body protein-protein docking which refers to computationally
predicting the 3D structure of a protein-protein complex given the 3D structures of the two proteins
in unbound state. Rigid body means no deformations occur within any protein during binding, which
is a realistic assumption in many biological settings.
Popular docking software (Chen et al., 2003; Venkatraman et al., 2009; De Vries et al., 2010; Torchala
et al., 2013; Schindler et al., 2017; Sunny and Jayaraj, 2021) are typically computationally expensive,
tCorrespondence to: Octavian Ganea (oct@mit.edu) and Yatao Bian (yatao.bian@gmail.com).
* Equal contribution.
§ Work done during an internship at Tencent AI Lab.
1Our code is publicly available: https://github.com/octavian-ganea/equidock_public.
1
Published as a conference paper at ICLR 2022
Figure 2: Same output guarantee of EQUIDOCK. We predict a rigid transformation to place the
ligand in the binding location w.r.t the receptor. We mathematically guarantee to output the same
complex structure — UP to an SE(3) transformation — independently of the initial unbound positions,
rotations, or roles of both constituents. (RMSD = Root-mean-square deviation of atomic positions)
taking between minutes and hours to solve a single example pair, while not being guaranteed to find
accurate complex structures. These methods largely follow the steps: i.) randomly sample a large
number (e.g., millions) of candidate initial complex structures, ii.) employ a scoring function to rank
the candidates, iii.) adjust and refine the top complex structures based on an energy model (e.g., force
field). We here take a first step towards tackling these issues by using deep learning models for direct
prediction of protein complex structures.
Contributions. We design EQUIDOCK, a fast, end-to-end method for rigid body docking that
directly predicts the SE(3) transformation to place one of the proteins (ligand) at the right location and
orientation with respect to the second protein (receptor). Our method is based on the principle that the
exact same complex structure should be predicted irrespectively of the initial 3D placements and roles
of both constituents (see Fig. 2). We achieve this desideratum by incorporating the inductive biases of
pairwise SE(3)-equivariance and commutativity, and deriving novel theoretical results for necessary
and sufficient model constraints (see Section 3). Next, we create EquiDock to satisfy these proper-
ties by design, being a combination of: i) a novel type of pairwise independent SE(3)-equivariant
graph matching networks, ii) an attention-based keypoint selection algorithm that discovers repre-
sentative points and aligns them with the binding pocket residues using optimal transport, and iii)
a differentiable superimposition model to recover the optimal global rigid transformation. Unlike
prior work, our method does not use heavy candidate sampling or ranking, templates, task-specific
geometric or chemical hand-crafted features, or pre-computed meshes. This enables us to achieve
plausible structures with a speed-up of 80-500x compared to popular docking software, offering a
promising competitive alternative to current solutions for this problem.
2	Related work
Geometric Deep Learning. Graph Neural Networks (GNNs) are becoming the de facto choice
for learning with graph data (Bruna et al., 2013; Defferrard et al., 2016; Kipf and Welling, 2016;
Gilmer et al., 2017; Xu et al., 2018; Li et al., 2019). Motivated by symmetries naturally occurring in
different data types, architectures are tailored to explicitly incorporate such properties (Cohen and
Welling, 2016a;b; Thomas et al., 2018; Fuchs et al., 2020; Finzi et al., 2020; Eismann et al., 2020;
Satorras et al., 2021). GNNs are validated in a variety of tasks such as particle system dynamics or
conformation-based energy estimation (Weiler and Cesa, 2019; Rezende et al., 2019).
Euclidean Neural Networks (E(3)-NNs). However, plain GNNs and other deep learning methods
do not understand data naturally lying in the 3D Euclidean space. For example, how should the
output deterministically change with the input, e.g. when it is rotated ? The recent Euclidean neural
networks address this problem, being designed from geometric first-principles. They make use of
SE(3)- equivariant and invariant neural layers, thus avoiding expensive data augmentation strategies.
Such constrained models ease optimization and have shown important improvements in biology or
chemistry 一 e.g. for molecular structures (Fuchs et al., 2020; Hutchinson et al., 2020; Wu et al., 2021;
2
Published as a conference paper at ICLR 2022
Jumper et al., 2021; Ganea et al., 2021) and different types of 3D point clouds (Thomas et al., 2018).
Different from prior work, we here derive constraints for pairs of 3D objects via pairwise independent
SE(3)-equivariances, and design a principled approach for modeling rigid body docking.
Protein Folding. Deep neural networks have been used to predict inter-residue contacts, distance
and/or orientations (Adhikari and Cheng, 2018; Yang et al., 2020; Senior et al., 2020; Ju et al., 2021),
that are subsequently transformed into additional constraints or differentiable energy terms for protein
structure optimization. AlphaFold 2 (Jumper et al., 2021) and Rosetta Fold (Baek et al., 2021)
are state-of-the-art approaches, and directly predict protein structures from co-evolution information
embedded in homologous sequences, using geometric deep learning and E(3)-NNs.
Protein-Protein Docking and Interaction. Experimentally determining structures of protein
complexes is often expensive and time-consuming, rendering a premium on computational methods
(Vakser, 2014). Protein docking methods (Chen et al., 2003; Venkatraman et al., 2009; De Vries et al.,
2010; Biesiada et al., 2011; Torchala et al., 2013; Schindler et al., 2017; Weng et al., 2019; Sunny and
Jayaraj, 2021; Christoffer et al., 2021; Yan et al., 2020) typically run several steps: first, they sample
thousands or millions of complex candidates; second, they use a scoring function for ranking (Moal
et al., 2013; Basu and Wallner, 2016; Launay et al., 2020; Eismann et al., 2020); finally, top-ranked
candidates undergo a structure refinement process using energy or geometric models (Verburgt and
Kihara, 2021). Relevant to protein-protein interaction (PPI) is the task of protein interface prediction
where GNNs have showed promise (Fout et al., 2017; Townshend et al., 2019; Liu et al., 2020; Xie
and Xu, 2021; Dai and Bailey-Kellogg, 2021). Recently, AlphaFold 2 and RoseTTAFold have
been utilized as subroutines to improve PPIs from different aspects (Humphreys et al., 2021; Pei et al.,
2021; Jovine), e.g., combining physics-based docking method Cluspro (Kozakov et al., 2017; Ghani
et al., 2021), or using extended multiple-sequence alignments to predict the structure of heterodimeric
protein complexes from the sequence information (Bryant et al., 2021). Concurrently to our work,
Evans et al. (2021) extend AlphaFold 2 to multiple chains during both training and inference.
Drug-Target Interaction (DTI). DTI aims to compute drug-target binding poses and affinity,
playing an essential role in understanding drugs’ mechanism of action. Prior methods (Wallach et al.,
2015; Li et al., 2021) predict binding affinity from protein-ligand co-crystal structures, but such data
is expensive to obtain experimentally. These models are typically based on heavy candidate sampling
and ranking (Trott and Olson, 2010; Koes et al., 2013; McNutt et al., 2021; Bao et al., 2021), being
tailored for small drug-like ligands and often assuming known binding pocket. Thus, they are not
immediately applicable to our use case. In contrast, our rigid docking approach is generic and could
be extended to accelerate DTI research as part of future work.
3	Mathematical constraints for rigid body docking
We start by introducing the rigid body docking problem and derive the geometric constraints for
enforcing same output complex prediction regardless of the initial unbound positions or roles (Fig. 2).
Rigid Protein-Protein Docking - Problem Setup. We are given as input a pair of proteins
forming a complex. They are (arbitrarily) denoted as the ligand and receptor, consisting of n and
m residues, respectively. These proteins are represented in their bound (docked) state as 3D point
clouds X； ∈ R3×n, Xg ∈ R3×m, where each residue's location is given by the coordinates of its
corresponding α-carbon atom. In the unbound state, the docked ligand is randomly rotated and
translated in space, resulting in a modified point cloud X1 ∈ R3×n. For simplicity and w.l.o.g., the
receptor remains in its bound location X2 = X2； .
The task is to predict a rotation R ∈ SO(3) and a translation t ∈ R3 such that RX1 + t = X1；,
using as input the proteins and their unbound positions X1 and X2.
Here, R = R(X1 |X2) and t = t(X1 |X2) are functions of the two proteins, where we omit residue
identity or other protein information in this notation, for brevity.
Note that we assume rigid backbone and side-chains for both proteins. We therefore do not tackle the
more challenging problem of flexible docking, but our approach offers an important step towards it.
3
Published as a conference paper at ICLR 2022
b.
• binding pocket points
X2
Y2 = RYi + t
binding pocket
keypoint alignment
• keypoints obtained via
multi-head attention from the IEGMN
prediction
error - ligand
RMSD
LMSE
RXi + t
undocked /
unbound input
L' LNI
auxiliary loss for avoiding
point cloud intersection
predicted docked structure
by superimposing
keypoints Y] and Y2
Figure 3: Details on EQUIDOCK's Architecture and Losses. a. The message passing operations in
IEGMN guarantee pairwise independent SE(3)-equivariance as in Eq. (4), b. We predict keypoints for
each protein that are aligned with the binding pocket location using an additional optimal transport
(OT) loss, c. After predicting the docked position, we compute an MSE loss on the ligand, as well as
a loss to discourage body intersections.
a.
receptor
X2 ∈ R3×
P1 = P2
true docked
COmPleX	true binding
pocket location
(train labels)
We desire that the predicted complex structure is independent of the initial locations and orientations
of the two proteins, as well as of their roles - see Fig. 2. Formally, we wish to guarantee that:
(R(Zι 4)Z1 + t(Z1 %))㊉ Z ≡ (R(Xι ∣X2)X1 + t(X1 ∣X2))㊉ X, (SE(3)-invariance)
(R(Xι∣X2)X1 + t(X1 ∣X2))㊉ X2 ≡ Xi ㊉(R(X2∣X1)X2 + t(X2∣Xι)), (commutativity)
∀Q1,Q2 ∈ SO(3),∀g1,g2 ∈R3,∀X1 ∈R3×n,X2 ∈ R3×m, and Zl =QlXl+gl,l ∈ {1, 2}.
(1)
for any rotations Q1, Q2 and translations g1, g2, where ㊉ is concatenation along columns, and ≡
denotes identity after superimposition, i.e. zero Root-Mean-Square Deviation (RMSD) between the
two 3D point sets after applying the Kabsch algorithm (Kabsch, 1976). An immediate question arises:
How do the constraints in Eq. (1) translate into constraints for R(∙∣∙) and t(∙∣∙) ?
The rotation R and translation t change in a systematic way when we apply SE(3) transformations
or swap proteins’ roles. These properties restrict our class of functions as derived below.
SE(3)-equivariance Constraints. If we apply any distinct SE(3) transformations on the unbound
ligand X1 and receptor X2, i.e. we dock Q1X1 + g1 onto Q2X2 + g2, then the rotation matrix
R(Q1X1 + g1 |Q2X2 + g2) and translation vector t(Q1X1 + g1 |Q2X2 + g2) can be derived from
the original R(X1 |X2) and t(X1 |X2) assuming that we always do rotations first. In this case,
R(Q1X1 + g1|Q2X2 + g2) can be decomposed into three rotations: i.) apply Q1> to undo the
rotation Q1 applied on X1, ii.) apply R(X1 |X2), iii.) apply Q2 to rotate the docked ligand together
with the receptor. This gives R(Q1X1 + g1 |Q2X2 + g2) = Q2R(X1|X2)Q1>, which in turn
constraints the translation vector. We provide a formal statement and prove it in Appendix B.1:
Proposition 1. For any Q1, Q2 ∈ SO(3), g1, g2 ∈ R3, SE(3)-invariance of the predicted docked
complex defined by Eq. (1) is guaranteed iff
R(Q1X1 + g1|Q2X2 + g2) =Q2R(X1|X2)Q1>
t(Q1X1 + g1|Q2X2 + g2) = Q2t(X1|X2) - Q2R(X1|X2)Q1>g1 + g2.
(2)
As a direct consequence of this proposition, we have the following statement.
Proposition 2. Any model satisfying Proposition 1 guarantees invariance of the predicted complex
w.r.t. any SE(3) transformation on X1, and equivariance w.r.t. any SE(3) transformation on X2:
R(Z1|X2)Z1 +t(Z1|X2) = R(X1|X2)X1 +t(X1|X2), where Z1 = Q1X1 + g1
R(X1|Z2)X1+t(X1|Z2) = Q2 [R(X1|X2)X1 + t(X1|X2)] + g2,	where Z2 =Q2X2+g2
∀Q1,Q2 ∈ SO(3),∀g1,g2 ∈R3,∀X1 ∈R3×n,∀X2 ∈ R3×m.
Commutativity. Instead of docking X1 with respect to X2, we can also dock X2 with respect to X1.
In this case, we require the final complex structures to be identical after superimposition, i.e., zero
RMSD. This property is named commutativity and it is satisfied as follows (proof in Appendix B.2).
4
Published as a conference paper at ICLR 2022
Proposition 3. Commutativity as defined by Eq. (1) is guaranteed iff
R(X2|X1) = R>(X1|X2);	t(X2|X1) = -R>(X1|X2)t(X1|X2),	(3)
Point Permutation Invariance. We also enforce residue permutation invariance. Formally, both
R(X1 |X2) and t(X1 |X2) should not depend on the order or columns of X1 and, resp., of X2.
4	EquiDock Model
Protein Representation. A protein is a sequence of amino acid residues that folds in a 3D structure.
Each residue has a general structure with a side-chain specifying its type, allowing us to define a
local frame and derive SE(3)-invariant features for any pair of residues —see Appendix A.
We represent a protein as a graph G = (V, E), similar to Fout et al. (2017); Townshend et al.
(2019); Liu et al. (2020). Each node i ∈ V represents one residue and has 3D coordinates xi ∈ R3
corresponding to the α-carbon atom’s location. Edges are given by a k-nearest-neighbor (k-NN)
graph using Euclidean distance of the original 3D node coordinates.
Overview of Our Approach. Our model is depicted in Fig. 3. We first build k-NN protein graphs
G1 = (V1, E1) and G2 = (V2, E2). We then design SE(3)-invariant node features F1 ∈ Rd×n, F2 ∈
Rd×m and edge features {fj→i : ∀(i, j) ∈ E1 ∪ E2 } (see Appendix A).
Next, we apply several layers consisting of functions Φ that jointly transform node coordinates and
features. Crucially, we guarantee, by design, pairwise independent SE(3)-equivariance for coordinate
embeddings, and invariance for feature embeddings. This double constraint is formally defined:
GivenZ1,H1,Z2,H2=Φ(X1,F1,X2,F2)
wehaveQ1Z1+g1,H1,Q2Z2+g2,H2 = Φ(Q1X1 + g1, F1, Q2X2 + g2, F2),	(4)
∀Q1,Q2 ∈ SO(3), ∀g1, g2 ∈R3.
We implement Φ as a novel type of message-passing neural network (MPNN). We then use the output
node coordinate and feature embeddings to compute R(X1 |X2) and t(X1 |X2). These functions
depend on pairwise interactions between the two proteins modeled as cross-messages, but also
incorporate the 3D structure in a pairwise-independent SE(3)-equivariant way to satisfy Eq. (1),
Proposition 1 and Proposition 3. We discover keypoints from each protein based on a neural attention
mechanism and softly guide them to represent the respective binding pocket locations via an optimal
transport based auxiliary loss. Finally, we obtain the SE(3) transformation by superimposing the
two keypoint sets via a differentiable version of the Kabsch algorithm. An additional soft-constraint
discourages point cloud intersections. We now detail each of these model components.
Independent E(3)-Equivariant Graph Matching Networks (IEGMNs). Our architecture for Φ
satisfying Eq. (4) is called Independent E(3)-Equivariant Graph Matching Network (IEGMN) - see
Fig. 3. It extends both Graph Matching Networks (GMN) (Li et al., 2019) and E(3)-Equivariant Graph
Neural Networks (E(3)-GNN) (Satorras et al., 2021). IEGMNs perform node coordinate and feature
embedding updates for an input pair of graphs G1 = (V1, E1), G2 = (V2, E2), and use inter- and intra-
node messages, as well as E(3)-equivariant coordinate updates. The l-th layer of IEGMNs transforms
node latent/feature embeddings {hi(l)}i∈V1∪V2 and node coordinate embeddings {xi(l)}i∈V1∪V2 as
mj→i	ie(h(l), hjl, exp(-kXil)- Xjl)k2∕σ), fj-i), Yejf ∈ Ei ∪ E2	(5)
μj→i 二	aj→iWh(jl),∀i∈ V1,j ∈ V2ori ∈V2,j ∈ V1	(6)
mi	= |N1h X mj→i,∀i ∈ V1 ∪V2 j∈N(i)	(7)
μi 二	二):μj→i,∀i ∈ Vι, and μi =): μj→i,∀i ∈ V2	(8)
	j∈V2	j∈V1	
(l+1) xi	=ηχ(0) + (1 - η)χ(l) + X (XTl- Xjl)Wx(mj∙→i),∀i ∈ Vi ∪V2	(9)
	j∈N(i)	
hi(l+1)	=(1 - β) ∙ h(ll + β ∙中hW, mi, μi, fi), ∀i ∈V1 ∪ V2,	(10)
5
Published as a conference paper at ICLR 2022
where N(i) are the neighbors of node i;夕X is a real-valued (scalar) parametric function; W is a
learnable matrix;夕h,夕e are parametric functions (MLPS) outputting a vector Rd; fj→ and f are the
original edge and node features (extracted SE(3)-invariantly from the residues). aj→i is an attention
based coefficient with trainable shallow neural networks ψq and ψk :
a = exp(hψq (h(l)),ψk (hjl))i)
j→i pjo exp( hψq (hiI)),ψk (hj0))i)，
(11)
Note that all parameters of W,夕x,夕h,夕e, ψq, ψk can be shared or different for different IEGMN
layers . The output of several IEGMN layers is then denoted as:
Z1 ∈ R3×n, H1 ∈ Rd×n, Z2 ∈ R3×m, H2 ∈ Rd×m = IEGMN(X1, F1, X2, F2).	(12)
It is then straightforward to prove the following (see Appendix B.3):
Proposition 4. IEGMNs satisfy the pairwise independent SE(3)-equivariance property in Eq. (4).
Keypoints for Differentiable Protein Superimposition. Next, we use multi-head attention to
obtain K points for each protein, Y1 , Y2 ∈ R3×K, which we name keypoints. We train them to
become representative points for the binding pocket of the respective protein pair (softly-enforced by
an additional loss described later). If this would holds perfectly, then the superimposition of Y1 and
Y2 would give the corresponding ground truth superimposition of X1 and X2. Our model is :
nm
y1k :=	αikz1i; y2k :=	βjkz2j,
i=1	j=1
where zii denotes the i-th column of matrix Zι, and α, = Softmax%(√h]Wkμ(夕(H2))) are
attention scores (similarly defined for βjk), with Wk0 ∈ Rd×d a parametric matrix (different for each
attention head),夕 a linear layer plus a LeakyReLU non-linearity, and μ(∙) is the mean vector.
Differentiable Kabsch Model. We design the rotation and translation that docks protein 1 into
protein 2 to be the same transformation used to superimpose Y1 and Y2 — see Fig. 3. For this,
we compute a differentiable version of the Kabsch algorithm (Kabsch, 1976) as follows. Let
A = Y2Y1 ∈ R3×3 computed using zero-mean keypoints. The singular value decomposition
(SVD) is A = U2SU1> , where U2, U1 ∈ O(3). Finally, we define the differentiable functions
100
R(X1|X2; θ) = U2 0 1 0 U1> ,	where d = sign(det(U2U1> ))
0 0 d	(13)
t(Xl∣X2; θ) = μ(Y2) - R(X1∣X2; θ)μ(Y1),
where μ(∙) is the mean vector of a point cloud. It is straightforward to prove that this model satisfies
all the equivariance properties in Eqs. (1) to (3). From a practical perspective, the gradient and
backpropagation through the SVD operation was analyzed by (Ionescu et al., 2015; Papadopoulo and
Lourakis, 2000) and implemented in the automatic differentiation frameworks such as PyTorch.
MSE Loss. During training, we randomly decide which protein is the receptor (say protein 2), keep
it in the docked position (i.e., X2 = Xg), predict the SE(3) transformation using Eq. (13) and use it
to compute the final position of the ligand as X1 = R(X1∣X2)X1 + t(X1∣X2). The mean squared
error (MSE) loss is then LMSE = 1 Pn=ι l∣xg - Xik2.
Optimal Transport and Binding Pocket Keypoint Alignment. As stated before, we desire that
Y1 and Y2 are representative points for the binding pocket location of the respective protein pair.
However, this needs to be encouraged explicitly, which we achieve using an additional loss.
We first define the binding pocket point sets, inspiring from previous PPI work (Section 2). Given
the residues’ α-carbon locations of the bound (docked) structures, X1g and X2g, we select all pairs
of residues at less than T Euclidean distance (τ = 8A in our experiments). We assume these
are all interacting residues. Denote these pairs as {(xg1s, xg2s), s ∈ 1, . . . , S}, where S is variable
across data pairs. We compute midpoints of these segments, denoted as P1g, P2g ∈ R3×S, where
6
Published as a conference paper at ICLR 2022
p↑s = P2s = 0.5 ∙ (x；S + X2s). We view P； and Pg as binding pocket points. In the unbound state,
these sets are randomly moved in space together with the respective protein residue coordinates X1
and X2. We denote them as P1, P2 ∈ R3×S. For clarity, if X1 = QX12 + g, then P1 = QP12 + g.
We desire that Y1 is a representative set for the 3D set P1 (and, similarly, Y2 for P2). However,
while at training time we know that every point p1s corresponds to the point p2s (and, similarly, y1k
aligns with y2k, by assumption), we unfortunately do not know the actual alignment between points
in Yl and Pl, for every l ∈ {1, 2}. This can be recovered using an additional optimal transport loss:
LOT = ∈mU(iSn,K)hT,Ci, where Cs,k = ky1k - p1sk2 + ky2k - p2sk2,	(14)
where U(S, K) is the set of S × K transport plans with uniform marginals. The optimal transport
plan is computed using an Earth Mover’s Distance and the POT library (Flamary et al., 2021), while
being kept fixed during back-propagation and optimization when only the cost matrix is trained.
Note that our approach assumes that y1k corresponds to y2k, for every k ∈ {1, . . . , K}. Intuitively,
each attention head k will identify a specific geometric/chemical local surface feature of protein 1 by
y1k, and match its complementary feature of protein 2 by y2k.
Avoiding Point Cloud Intersection. In practice, our model does not enforce a useful inductive
bias, namely that proteins forming complexes are never "intersecting" with each other. To address
this issue, we first state a notion of the "interior" of a protein point cloud. Following previous
work (Sverrisson et al., 2021; Venkatraman et al., 2009), we define the surface of a protein point
cloud X ∈ R3×n as {x ∈ R3 : G(X) = Y}, where G(X) = -σln(pn=1 exp(-∣∣x - Xi∣∣2∕σ)).
The parameters σ and γ are chosen such that there exist no "holes" inside a protein (we found
γ = 10, σ = 25 to work well, see Appendix E). As a consequence, the interior of the protein is given
by {x ∈ R3 : G(x) < γ}. Then, the condition for non-intersecting ligand and receptor can be written
as G1 (x2j ) > γ, ∀j ∈ 1, . . . , m and G2 (x1i) > γ, ∀i ∈ 1, . . . , n. As a loss function, this becomes
1n	1m
LNI = — £max(0,Y - G2(xii)) +——EmaX(0,γ - G1(x2j)).	(15)
ni=1	mj=1
Surface Aware Node Features. Surface contact modeling is important for protein docking. We
here design a novel surface feature type that differentiates residues closer to the surface of the protein
from those in the interior. Similar to Sverrisson et al. (2021), we prioritize efficiency and avoid
pre-computing meshes, but show that our new feature is a good proxy for residue’s depth (i.e. distance
to the protein surface). Intuitively, residues in the core of the protein are locally surrounded in all
directions by other residues. This is not true for residues on the surface, e.g., neighbors are in a
half-space if the surface is locally flat. Building on this intuition, for each node (residue) i in the
k-NN protein graph, we compute the norm of the weighted average of its neighbor forces, which can
be interpreted as the normalized gradient of the G(x) surface function. This SE(3)-invariant feature is
ρi(xi; λ)
k f"∈Ni wi,i0,λ(Xi - χi0)k
Pi0∈Ni wi,i0,λkxi - xi0 k
,	_	eXp(-∣∣Xi - Xio∣∣2∕λ)
where "I = Pj∈Ni exp(-∣∣Xi - Xj l∣2∕λ). (16)
Intuitively, as depicted in Fig. 8, residues in the interior of the protein have values close to 0 since they
are surrounded by vectors from all directions that cancel out, while residues near the surface have
neighbors only in a narrower cone, with aperture depending on the local curvature of the surface. We
show in Appendix C that this feature correlates well with more expensive residue depth estimation
methods, e.g. based on MSMS, thus offering a computationally appealing alternative. We also
compute an estimation of this feature for large dense point clouds based on the local surface angle.
5	Experiments
Datasets. We leverage the following datasets: Docking Benchmark 5.5 (DB5.5) (Vreven et al.,
2015) and Database of Interacting Protein Structures (DIPS) (Townshend et al., 2019). DB5.5 is
a gold standard dataset in terms of data quality, but contains only 253 structures. DIPS is a larger
protein complex structures dataset mined from the Protein Data Bank (Berman et al., 2000) and
tailored for rigid body docking. Datasets information is given in Appendix D. We filter DIPS to only
7
Published as a conference paper at ICLR 2022
Table 1: Complex Prediction Results. As in the main text, the proprietary baselines might internally
use parts of the test sets (e.g. to extract templates or features), thus their numbers might be optimistic.
DIPS Test Set	DB5.5 Test Set
Complex RMSD Interface RMSD Complex RMSD Interface RMSD
Methods Median Mean Std Median Mean Std Median Mean Std Median Mean Std
Attract	17.17	14.93 10.39		12.41	14.02 11.81		9.55	10.09	9.88	7.48	10.69 10.90	
HDock	6.23	10.77	11.39	3.90	8.88	10.95	0.30	5.34	12.04	0.24	4.76	10.83
ClusPro	15.76	14.47	10.24	12.54	13.62	11.11	3.38	8.25	7.92	2.31	8.71	9.89
PatchDock	15.24	13.58	10.30	11.44	12.15	10.50	18.26	18.00	10.12	18.88	18.75	10.06
EquiDock	13.29	14.52	7.13	10.18	11.92	7.01	14.13	14.72	5.31	11.97	13.23	4.93
Method
• AΠRACT
• HDOCK
• OUSPRO
• PATCHDOCK
• EQUIDOCK
C-RMSD distributions (DIPS test)
HDOCK
PATCHDOCK
I-RMSD distributions (DIPS test)
ATTRACT HDOCK CLUSPRO PArcHDoCK EO∪IDOCK
C-RMSD vs I-RMSD scatter plot (DIPS test)
Figure 4: a. Complex-RMSD distributions (DIPS test set); b. Interface-RMSD distributions (DIPS
test set); c. scatter plot for C-RMSD vs I-RMSD (DIPS test set).
keep proteins with at most 10K atoms. Datasets are then randomly partitioned in train/val/test splits
of sizes 203/25/25 (DB5.5) and 39,937/974/965 (DIPS). For DIPS, the split is based on protein family
to separate similar proteins. For the final evaluation in Table 1, we use the full DB5.5 test set, and
randomly sample 100 pairs from different protein families from the DIPS test set.
Baselines. We compare our EQUIDOCK method
with popular state-of-the-art docking software 2 Clus-
Pro (Piper) (Desta et al., 2020; Kozakov et al.,
2017),ATTRACT (Schindler et al., 2017; de Vries et al.,
2015), PatchDock (Mashiach et al., 2010; Schneidman-
Duhovny et al., 2005), and HDock (Yan et al., 2020;
2017b;a; Huang and Zou, 2014; 2008). These baselines
provide user-friendly local packages suitable for automatic
experiments or webservers for manual submissions.
Evaluation Metrics. To measure prediction’s quality, we
report Complex Root Mean Square Deviation (CRMSD)
and Interface Root Mean Square Deviation (IRMSD), de-
Figure 5: Inference running time distri-
butions (log10 scale).
fined below. Given the ground truth and predicted complex structures, Z* ∈ R3 ×(n+m) and
Z ∈ R3×(n+m), we first superimpose them using the Kabsch algorithm (Kabsch, 1976), and then
compute C-RMSD = y n11^ ∣∣Z* - ZkF∙ We COmPUte I-RMSD similarly, but using only the coor-
dinates of the interface residues with distance less than 8A to the other protein's residues. For a fair
comparison among baselines, we use only the α-carbon coordinates to compute both metrics.
Training Details. We train our models on the train part of DIPS first, using Adam (Kingma and
Ba, 2014) with learning rate 2e-4 and early stopping with patience of 30 epochs. We update the
best validation model only when it achieves a score of less than 98% of the previous best validation
score, where the score is the median of Ligand RMSD on the full DIPS validation set. The best DIPS
validated model is then tested on the DIPS test set. For DB5.5, we fine tune the DIPS pre-trained
2ClusPro: https://cluspro.bu.edu/, Attract: www.attract.ph.tum.de/services/
ATTRACT/ATTRACT.vdi.gz, PatchDock: https://bioinfo3d.cs.tau.ac.il/PatchDock/,
HDOCK: http://huanglab.phys.hust.edu.cn/software/HDOCK/
8
Published as a conference paper at ICLR 2022
EquiDock
PatchDock
HDOCK
ClUsPro	Ground Truth
CRMSD = 7.13	CRMSD = 17.37	CRMSD = 18.00	CRMSD = 18.56
PDB ID: 3DMP
Figure 6: Visualization of a protein complex successfully predicted by EquiDock. Note that all
other methods find the binding interface on the wrong side of the black protein.
model on the DB5.5 training set using learning rate 1e-4 and early stopping with 150 epochs patience.
The best DB5.5 validated model is finally tested on DB5.5 test set. During training, we randomly
assign the roles of ligand and receptor. Also, during both training and testing, we randomly rotate and
translate the ligand in space (even though our model is invariant to this operation) for all baselines.
Complex Prediction Results. Results are shown in Table 1, Fig. 4 and Appendix E. We note
that our method is competitive and often outperforms the baselines. However, we do not use heavy
candidate sampling and re-ranking, we do not rely on task-specific hand-crafted features, and we
currently do not perform structure fine-tuning, aiming to predict the SE(3) ligand transformation in
a direct shot. Moreover, we note that some of the baselines might have used part of our test set in
validating their models, for example to learn surface templates, thus, their reported scores might be
optimistic. Notably, HDock score function was validated on DB4 which overlaps with DB5.5. A
more appropriate comparison would require us to re-build these baselines without information from
our test sets, a task that is currently not possible without open-source implementations.
Computational Efficiency. We show inference times in Fig. 5 and Table 4. Note that EQUIDOCK
is between 80-500 times faster than the baselines. This is especially important for intensive screening
applications that aim to scan over vast search spaces, e.g. for drug discovery. In addition, it is also
relevant for de novo design of binding proteins (e.g. antibodies (Jin et al., 2021)) or for use cases
when protein docking models are just a component of significantly larger end-to-end architectures
targeting more involved biological scenarios, e.g., representing a drug’s mechanism of action or
modeling cellular processes with a single model as opposed to a multi-pipeline architecture.
Visualization. We show in Fig. 6 a successful example of a test DIPS protein pair for which our
model significantly outperforms all baselines.
6 Conclusion
We have presented an extremely fast, end-to-end rigid protein docking approach that does not rely on
candidate sampling, templates, task-specific features or pre-computed meshes. Our method smartly
incorporates useful rigid protein docking priors including commutativity and pairwise independent
SE(3)-equivariances, thus avoiding the computational burden of data augmentation.
We look forward to incorporating more domain knowledge in EquiDock and extend it for flexible
docking and docking molecular dynamics, as well as adapt it to other related tasks such as drug
binding prediction. On the long term, we envision that fast and accurate deep learning models
would allow us to tackle more complex and involved biological scenarios, for example to model the
mechanism of action of various drugs or to design de novo binding proteins and drugs to specific
targets (e.g. for antibody generation). Last, we hope that our architecture can inspire the design of
other types of biological 3D interactions.
Limitations. First, our presented model does not incorporate protein flexibility which is necessary
for various protein families, e.g., antibodies. Unfortunately, both DB5 and DIPS datasets are biased
towards rigid body docking . Second, we only prevent steric clashes using a soft constraint (Eq. (15))
which has limitations (see Table 6). Future extensions would hard-constrain the model to prevent
such artifacts.
9
Published as a conference paper at ICLR 2022
Acknowledgements
The authors thank Hannes Stark, Gabriele Corso, Patrick Walters, Tian Xie, Xiang Fu, Jacob Stern,
Jason Yim, Lewis Martin, Jeremy Wohlwend, Jiaxiang Wu, Wei Liu, and Ding Xue for insightful
and helpful discussions. OEG is funded by the Machine Learning for Pharmaceutical Discovery
and Synthesis (MLPDS) consortium, the Abdul Latif Jameel Clinic for Machine Learning in Health,
the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats
program, and the DARPA Accelerated Molecular Discovery program. This publication was created
as part of NCCR Catalysis (grant number 180544), a National Centres of Competence in Research
funded by the Swiss National Science Foundation. RB and TJ also acknowledge support from NSF
Expeditions grant (award 1918839): Collaborative Research: Understanding the World Through
Code.
References
B. Adhikari and J. Cheng. Confold2: Improved contact-driven ab initio protein structure modeling.
BMCbioinformatics,19(1):1-5, 2018. 3
M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong,
L. N. Kinch, R. D. Schaeffer, et al. Accurate prediction of protein structures and interactions using
a three-track neural network. Science, 373(6557):871-876, 2021. 1, 3
J. Bao, X. He, and J. Z. Zhang. Deepbsp—a machine learning method for accurate prediction of
protein-ligand docking structures. Journal of Chemical Information and Modeling, 2021. 3
S. Basu and B. Wallner. Dockq: a quality measure for protein-protein docking models. PloS one, 11
(8):e0161879, 2016. 3
H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and
P. E. Bourne. The protein data bank. Nucleic acids research, 28(1):235-242, 2000. 7
J. Biesiada, A. Porollo, P. Velayutham, M. Kouril, and J. Meller. Survey of public domain software
for docking simulations and virtual screening. Human genomics, 5(5):1-9, 2011. 3
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks
on graphs. arXiv preprint arXiv:1312.6203, 2013. 2
P. Bryant, G. Pozzati, and A. Elofsson. Improved prediction of protein-protein interactions using
alphafold2 and extended multiple-sequence alignments. bioRxiv, 2021. 3
R. Chen, L. Li, and Z. Weng. Zdock: an initial-stage protein-docking algorithm. Proteins: Structure,
Function, and Bioinformatics, 52(1):80-87, 2003. 1, 3
C. Christoffer, S. Chen, V. Bharadwaj, T. Aderinwale, V. Kumar, M. Hormati, and D. Kihara. Lzerd
webserver for pairwise and multiple protein-protein docking. Nucleic Acids Research, 2021. 3
T. Cohen and M. Welling. Group equivariant convolutional networks. In International conference on
machine learning, pages 2990-2999. PMLR, 2016a. 2
T. S. Cohen and M. Welling. Steerable cnns. arXiv preprint arXiv:1612.08498, 2016b. 2
B. Dai and C. Bailey-Kellogg. Protein interaction interface region prediction by geometric deep
learning. Bioinformatics, 2021. 3
S. J. De Vries, M. Van Dijk, and A. M. Bonvin. The haddock web server for data-driven biomolecular
docking. Nature protocols, 5(5):883-897, 2010. 1, 3
S. J. de Vries, C. E. Schindler, I. C. de Beauchene, and M. Zacharias. A Web interface for easy
flexible protein-protein docking with attract. Biophysical journal, 108(3):462-465, 2015. 8
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural netWorks on graphs With fast
localized spectral filtering. arXiv preprint arXiv:1606.09375, 2016. 2
10
Published as a conference paper at ICLR 2022
I. T. Desta, K. A. Porter, B. Xia, D. Kozakov, and S. Vajda. Performance and its limits in rigid body
protein-protein docking. Structure, 28(9):1071-1081, 2020. 8
S. Eismann, R. J. Townshend, N. Thomas, M. Jagota, B. Jing, and R. O. Dror. Hierarchical, rotation-
equivariant neural networks to select structural models of protein complexes. Proteins: Structure,
Function, and Bioinformatics, 2020. 2, 3
R. Evans, M. O’Neill, A. Pritzel, N. Antropova, A. W. Senior, T. Green, A.右dek, R. Bates,
S. Blackwell, J. Yim, O. Ronneberger, S. Bodenstein, M. Zielinski, A. Bridgland, A. Potapenko,
A. Cowie, K. Tunyasuvunakool, R. Jain, E. Clancy, P. Kohli, J. Jumper, and D. Hassabis. Protein
complex prediction with alphafold-multimer. bioRxiv, 2021. doi: 10.1101/2021.10.04.463034. 3
M. Finzi, S. Stanton, P. Izmailov, and A. G. Wilson. Generalizing convolutional neural networks for
equivariance to lie groups on arbitrary continuous data. In International Conference on Machine
Learning, pages 3165-3176. PMLR, 2020. 2
R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos,
K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko,
A. Rolet, A. Schutz, V. Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot:
Python optimal transport. Journal of Machine Learning Research, 22(78):1-8, 2021. URL
http://jmlr.org/papers/v22/20-451.html. 7
A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur. Protein interface prediction using graph convolutional
networks. In Proceedings of the 31st International Conference on Neural Information Processing
Systems, pages 6533-6542, 2017. 3, 5
F. B. Fuchs, D. E. Worrall, V. Fischer, and M. Welling. Se (3)-transformers: 3d roto-translation
equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020. 2
O.-E. Ganea, L. Pattanaik, C. W. Coley, R. Barzilay, K. F. Jensen, W. H. Green, and T. S. Jaakkola.
Geomol: Torsional geometric generation of molecular 3d conformer ensembles. arXiv preprint
arXiv:2106.07802, 2021. 3
U. Ghani, I. Desta, A. Jindal, O. Khan, G. Jones, S. Kotelnikov, D. Padhorny, S. Vajda, and D. Kozakov.
Improved docking of protein models by a combination of alphafold2 and cluspro. bioRxiv, 2021. 3
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for
quantum chemistry. In International Conference on Machine Learning, pages 1263-1272. PMLR,
2017. 2
S.-Y. Huang and X. Zou. An iterative knowledge-based scoring function for protein-protein recogni-
tion. Proteins: Structure, Function, and Bioinformatics, 72(2):557-579, 2008. 8
S.-Y. Huang and X. Zou. A knowledge-based scoring function for protein-rna interactions derived
from a statistical mechanics-based iterative method. Nucleic acids research, 42(7):e55-e55, 2014.
8
I.	R. Humphreys, J. Pei, M. Baek, A. Krishnakumar, I. Anishchenko, S. R. Ovchinnikov, J. Zheng,
T. Ness, S. Banjade, S. R. Bagde, V. Stancheva, X. Li, K. Liu, Z. Zheng, D. Barerro, U. Roy, I. S.
Fernandez, B. Szakal, D. Branzei, E. C. Greene, S. Biggins, S. Keeney, E. A. Miller, J. C. Fromme,
T. Hendrickson, Q. Cong, and D. Baker. Structures of core eukaryotic protein complexes. 2021.
doi: 10.1101/2021.09.30.462231. 3
M. Hutchinson, C. L. Lan, S. Zaidi, E. Dupont, Y. W. Teh, and H. Kim. Lietransformer: Equivariant
self-attention for lie groups. arXiv Preprint, 2012.10885, 2020. 2
J.	Ingraham, V. K. Garg, R. Barzilay, and T. Jaakkola. Generative models for graph-based protein
design. 2019. 15
C.	Ionescu, O. Vantzos, and C. Sminchisescu. Matrix backpropagation for deep networks with
structured layers. In Proceedings of the IEEE International Conference on Computer Vision, pages
2965-2973, 2015. 6
11
Published as a conference paper at ICLR 2022
W. Jin, J. Wohlwend, R. Barzilay, and T. Jaakkola. Iterative refinement graph neural network for
antibody sequence-structure co-design. arXiv preprint arXiv:2110.04624, 2021. 9
L. Jovine. Using machine learning to study protein-protein interactions: From the UromodUlin
polymer to egg zona pellucida filaments. Molecular Reproduction and Development, n/a(n/a). doi:
https://doi.org/10.1002/mrd.23538. URL https://onlinelibrary.wiley.com/doi/
abs/10.1002/mrd.23538. 3
F. JU, J. ZhU, B. Shao, L. Kong, T.-Y. LiU, W.-M. Zheng, and D. BU. CopUlanet: Learning residUe
co-evolUtion directly from mUltiple seqUence alignment for protein strUctUre prediction. Nature
Communications, 12(1):2535, May 2021. 3
J. JUmper, R. Evans, A. Pritzel, T. Green, M. FigUrnov, O. Ronneberger, K. TUnyasUvUnakool,
R. Bates, A.%idek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold.
Nature, 596(7873):583-589, 2021. 1, 3, 15
W. Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica
Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922-923,
1976. 4, 6, 8
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 8
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016. 2
D. R. Koes, M. P. Baumgartner, and C. J. Camacho. Lessons learned in empirical scoring with smina
from the csar 2011 benchmarking exercise. Journal of chemical information and modeling, 53(8):
1893-1904, 2013. 3
D. Kozakov, D. R. Hall, B. Xia, K. A. Porter, D. Padhorny, C. Yueh, D. Beglov, and S. Vajda. The
cluspro web server for protein-protein docking. Nature protocols, 12(2):255-278, 2017. 3, 8
G. Launay, M. Ohue, J. Prieto Santero, Y. Matsuzaki, C. Hilpert, N. Uchikoga, T. Hayashi, and
J. Martin. Evaluation of consrank-like scoring functions for rescoring ensembles of protein-protein
docking poses. Frontiers in molecular biosciences, 7:308, 2020. 3
S. Li, J. Zhou, T. Xu, L. Huang, F. Wang, H. Xiong, W. Huang, D. Dou, and H. Xiong. Structure-
aware interactive graph neural networks for the prediction of protein-ligand binding affinity. In
Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,
pages 975-985, 2021. 3
Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli. Graph matching networks for learning the similarity
of graph structured objects. In International Conference on Machine Learning, pages 3835-3845.
PMLR, 2019. 2, 5
Y. Liu, H. Yuan, L. Cai, and S. Ji. Deep learning of high-order interactions for protein interface
prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 679-687, 2020. 3, 5
E. Mashiach, D. Schneidman-Duhovny, A. Peri, Y. Shavit, R. Nussinov, and H. J. Wolfson. An
integrated suite of fast docking algorithms. Proteins: Structure, Function, and Bioinformatics, 78
(15):3197-3204, 2010. 8
A. T. McNutt, P. Francoeur, R. Aggarwal, T. Masuda, R. Meli, M. Ragoza, J. Sunseri, and D. R. Koes.
Gnina 1.0: molecular docking with deep learning. Journal of cheminformatics, 13(1):1-20, 2021.
3
I.	H. Moal, M. Torchala, P A. Bates, and J. Fernandez-Recio. The scoring of poses in protein-protein
docking: current capabilities and future directions. BMC bioinformatics, 14(1):1-15, 2013. 3
T. Papadopoulo and M. I. Lourakis. Estimating the jacobian of the singular value decomposition:
Theory and applications. In European Conference on Computer Vision, pages 554-570. Springer,
2000. 6
12
Published as a conference paper at ICLR 2022
J.	Pei, J. Zhang, and Q. Cong. Human mitochondrial protein complexes revealed by large-scale
coevolution analysis and deep learning-based structure modeling. bioRxiv, 2021. 3
D. J. Rezende, S. Racaniere, I. Higgins, and P. Toth. Equivariant hamiltonian flows. arXivpreprint
arXiv:1909.13739, 2019. 2
M. F. Sanner, A. J. Olson, and J.-C. Spehner. Reduced surface: an efficient way to compute molecular
surfaces. Biopolymers, 38(3):305-320,1996. 17
V. G. Satorras, E. Hoogeboom, and M. Welling. E(n)-equivariant graph neural networks. arXiv
preprint arXiv:2102.09844, 2021. 2, 5
C.	E. Schindler, I. Chauvot de Beauchene, S. J. de Vries, and M. Zacharias. Protein-protein and
peptide-protein docking and refinement using attract in capri. Proteins: Structure, Function, and
Bioinformatics, 85(3):391-398, 2017. 1, 3, 8
D.	Schneidman-Duhovny, Y. Inbar, R. Nussinov, and H. J. Wolfson. Patchdock and symmdock:
servers for rigid and symmetric docking. Nucleic acids research, 33(suppl_2):W363-W367, 2005.
8
A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A.力dek, A. W. Nelson,
A. Bridgland, et al. Improved protein structure prediction using potentials from deep learning.
Nature, 577(7792):706-710, 2020. 1, 3
S. Sunny and P. Jayaraj. Fpdock: Protein-protein docking using flower pollination algorithm.
Computational Biology and Chemistry, 93:107518, 2021. 1, 3
F. Sverrisson, J. Feydy, B. E. Correia, and M. M. Bronstein. Fast end-to-end learning on protein
surfaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 15272-15281, 2021. 7
N.	Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li, K. Kohlhoff, and P. Riley. Tensor field net-
works: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint
arXiv:1802.08219, 2018. 2, 3
M. Torchala, I. H. Moal, R. A. Chaleil, J. Fernandez-Recio, and P. A. Bates. Swarmdock: a server for
flexible protein-protein docking. Bioinformatics, 29(6):807-809, 2013. 1, 3
R. Townshend, R. Bedi, P. Suriana, and R. Dror. End-to-end learning on 3d protein structure for
interface prediction. Advances in Neural Information Processing Systems, 32:15642-15651, 2019.
3, 5, 7
O.	Trott and A. J. Olson. Autodock vina: improving the speed and accuracy of docking with a new
scoring function, efficient optimization, and multithreading. Journal of computational chemistry,
31(2):455-461, 2010. 3
I.	A. Vakser. Protein-protein docking: From interaction to interactome. Biophysical journal, 107(8):
1785-1793, 2014. 3
V. Venkatraman, Y. D. Yang, L. Sael, and D. Kihara. Protein-protein docking using region-based 3d
zernike descriptors. BMC bioinformatics, 10(1):1-21, 2009. 1, 3, 7
J.	Verburgt and D. Kihara. Benchmarking of structure refinement methods for protein complex
models. Proteins: Structure, Function, and Bioinformatics, 2021. 3
T.	Vreven, I. H. Moal, A. Vangone, B. G. Pierce, P. L. Kastritis, M. Torchala, R. Chaleil, B. Jim6nez-
GarCia, P. A. Bates, J. Fernandez-Recio, et al. Updates to the integrated protein-protein interaction
benchmarks: docking benchmark version 5 and affinity benchmark version 2. Journal of molecular
biology, 427(19):3031-3041, 2015. 7
I.	Wallach, M. Dzamba, and A. Heifets. Atomnet: a deep convolutional neural network for bioactivity
prediction in structure-based drug discovery. arXiv preprint arXiv:1510.02855, 2015. 3
M. Weiler and G. Cesa. General e(2)-equivariant steerable cnns. arXiv preprint arXiv:1911.08251,
2019. 2
13
Published as a conference paper at ICLR 2022
G. Weng, E. Wang, Z. Wang, H. Liu, F. Zhu, D. Li, and T. Hou. Hawkdock: a web server to predict
and analyze the protein-protein complex based on computational docking and mm/gbsa. Nucleic
acids research, 47(W1):W322-W330, 2019. 3
J.	Wu, T. Shen, H. Lan, Y. Bian, and J. Huang. Se (3)-equivariant energy-based models for end-to-end
protein folding. bioRxiv, 2021. 2
Z. Xie and J. Xu. Deep graph learning of inter-protein contacts. bioRxiv, 2021. 3
K.	Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In International
Conference on Learning Representations, 2018. 2
Y. Yan, Z. Wen, X. Wang, and S.-Y. Huang. Addressing recent docking challenges: A hybrid strategy
to integrate template-based and free protein-protein docking. Proteins: Structure, Function, and
Bioinformatics, 85(3):497-512, 2017a. 8
Y. Yan, D. Zhang, P. Zhou, B. Li, and S.-Y. Huang. Hdock: a web server for protein-protein and
protein-dna/rna docking based on a hybrid strategy. Nucleic acids research, 45(W1):W365-W373,
2017b. 8
Y. Yan, H. Tao, J. He, and S.-Y. Huang. The hdock server for integrated protein-protein docking.
Nature protocols, 15(5):1829-1852, 2020. 3, 8
J.	Yang, I. Anishchenko, H. Park, Z. Peng, S. Ovchinnikov, and D. Baker. Improved protein structure
prediction using predicted interresidue orientations. Proceedings of the National Academy of
Sciences, 117(3):1496-1503, 2020. 3
14
Published as a conference paper at ICLR 2022
Contents
Representing Proteins as Graphs
Appendix
15
B Proofs of the Main Propositions	16
B.1	Proof of Proposition 1............................................................ 16
B.2	Proof of Proposition 3............................................................ 17
B.3	Proof of Proposition 4............................................................ 17
Surface Features
17
Datasets
18
More Experimental Details and Results
20
A Representing Proteins as Graphs
A protein is comprised of amino acid residues. The structure of an amino acid residue is shown in
Figure Fig. 7. Generally, an amino acid residue contains amino (-NH-), α-carbon atom and carboxyl
(-CO-), along with a side chain (R) connected with the α-carbon atom. The side chain (R) is specific
to each type of amino acid residues.
Figure 7: Representation of an amino acid residue and its local coordinate system.
A
C
D
E
We work on residue level (our approaches can be extended to atom level as well). A protein is
represented by a set of nodes where each node is an amino acid residue in the protein. Each node i
has a 3D coordinate xi ∈ R3 which is the 3D coordinate of α-carbon atom of the residue.
The neighborhood of a node is the set of k (k = 10 in our experiments) nearest nodes where the
distance is the Euclidean distance between 3D coordinates.
Node feature is a one dimension indicator (one-hot encoding) of the type of amino acid residue. This
one dimension indicator will be passed into an embedding layer.
Local Coordinate System. Similar to Ingraham et al. (2019) and Jumper et al. (2021), we introduce
a local coordinate system for each residue which denotes the orientation of a residue. Based on
this, we can further design SE(3)-invariant edge features. As shown in Figure 7, for a residue i, we
15
Published as a conference paper at ICLR 2022
denote the unit vector pointing from α-carbon atom to nitrogen atom as ui . We denote the unit vector
pointing from α-carbon atom to carbon atom of the carboxyl (-CO-) as ti . ui and ti together define
a plane, and the normal of this plane is ni = 彦二：.Finally, We define Vi = ni X Ui. Then ni,
ui and vi together form the basis of residue i’s local coordinate system. They together encode the
orientation of residue i.
Then We introduce the edge features of an edge j → i ∈ E . These features describe the relative
position of j With respect to i, the relative orientation of j With respect to i and the distance betWeen
j and i.
Relative Position Edge Features First We introduce the edge features pj →i Which describe relative
position of j With respect to i:
>
ni>>
pj→i = ui	[xj - xi]
vi>
Relative Orientation Edge Features As We mention above, each residue has orientation Which
carries information. Here We introduce the edge features qj→i , kj→i and tj→i Which describe
relative orientation of j With respect to i:
niT		niT		niT	
uiT	[nj] ,	kj→i =	uiT	[uj] ,	tj→i =	uiT	[vj]
T		T		T	
viT		viT		viT	
Distance-Based Edge Features Distance also carries information. Here We use radial basis func-
tion of distance as edge features:
(kxj-Xik)2
f	= e	2σ2	r = 1 2	R
j →i,r = e r	, r = , , ...,
Where R and scale parameters {σr}1≤r≤R are hyperparameters. In experiments, the set of scale
parameters We used is {1.5x|x = 0, 1, 2, ..., 14}. So for each edge, there are 15 distance-based edge
features.
Surface Aware Node Features We additionally compute 5 surface aWare node features defined in
Eq. (16) using λ ∈ {1., 2., 5., 10., 30.}.
B Proofs of the Main Propositions
B.1	Proof of Proposition 1.
Proof. Denote the predicted ligand position by R(X1 ∣X2)X1 + t(X1 ∣X2) = X1.
Assume first that SE(3)-invariance of the predicted docked complex defined by Eq. (1) is satisfied.
Then the transformation to dock Q1X1 + g1 With respect to Q2X2 + g2 is the same as the transfor-
mation to change QiXi + gι into Q2X1 + g2. We use thenotation: R>(X1∣X2) = (R(Xι ∣X2))>.
Then, We have the folloWing derivation steps:
R(X1∣X2)X1 + t(X1∣X2) = X1
Xi + R>(X1∣X2)t(X1∣X2) = R>(X1∣X2)X 1
Xi + R>(X1∣X2)t(X1∣X2) = R>(X1∣X2)Q> (Q2X 1 + g2 - g2)
X1 + R>(X1∣X2)t(X1∣X2) = R>(X1∣X2)Q> (Q2X 1 + g2) - R>(X1∣X2)Q>g2
X1 + R>(X1∣X2)t(X1∣X2) + R>(X1∣X2)Q>g2 = R>(X1∣X2)Q> (Q2X 1 + g2)
Q> (QIX1 + g1 - gI) + RT(X1|X2)(t(X 1|X2) + Q>g2) = RT(X1 |X2)Q> (Q2X1 + g2)
Q> (QIX1 + gI)- q>g1 + RT(X1 |X2Xt(X1 |X2) + Q>g2 ) = RT(X1 |X2)Q> (Q2X 1 + g2 )
R(X1∣X2)Q> (Q1 X1 + g1) - R(X1∣X2)Q>g1 + t(X1∣X2) + Q>g2 = Q> (Q2X 1 + g2)
Q2R(X1∣X2)Q> (Q1X1 + g1) - Q2R(X1∣X2)Q>g1 + Q2t(X1∣X2) + g2 = Q2X1 + g2
16
Published as a conference paper at ICLR 2022
From the last equation above, one derives the transformation of Q1X1 + g1 into Q2X1 + g2, which
is, by definition of the functions R and t, the same as the transformation to dock Q1X1 + g1 with
respect to Q2X2 + g2 . This transformation is
R(Q1X1 + g1|Q2X2 + g2) =Q2R(X1|X2)Q1>
t(Q1X1 + g1 |Q2X2 + g2) = Q2t(X1 |X2) - Q2R(X1|X2)Q1>g1 + g2
which concludes the proof.
Conversely, assuming constraints in Eq. (2) hold, we derive that Q1X1 + g1 is transformed into
Q2X1 + g2 , which then is trivial to check that it satisfies SE(3)-invariance of the predicted docked
complex defined by Eq. (1).
□
B.2	Proof of Proposition 3.
Proof. We use the notation R>(X1 |X2) := (R(X1 |X2))>. As in Appendix B.1, we denote
R(X1 |X2)X1 + t(X1 |X2) = X1. Then the transformation to dock X2 with respect to X1 is the
same as the transformation to change X1 back to X1, which is
R(XIX2)X1 + t(Xg) = X1
Xi + R> (X1∣X2)t(Xg) = R>(X1∣X2)X 1
Xi = R>(X1∣X2)X 1 - R>(X1 ∣X2)t(X1∣X2)
From the last equation above, We derive the transformation to change XI back to X1, which is the
same as the transformation to dock X2 with respect to X1.	□
B.3	Proof of Proposition 4.
Proof. Let X(1l+1), H(1l+1), X(2l+1), H(2l+1) = IEGMN(X(1l), H(1l), X(2l), H(2l)) be the output of an
IEGMN layer. Then, for any matrices Q1, Q2 ∈ SO(3) and any translation vectors g1, g2 ∈ R3, we
want to prove that IEGMN satisfy the pairwise independent SE(3)-equivariance property:
Q1X(1l+1)+g1,H(1l+1),Q2X(2l+1)+g2,H(2l+1) =IEGMN(Q1X(1l)+g1,H(1l),Q2X(2l)+g2,H(2l))
where each column of X(1l) ∈ R3×n,H(1l) ∈ Rd×n,X(2l) ∈ R3×m and H(2l) ∈ Rd×m represent an
individual node’s coordinate embedding or feature embedding.
We first note that the equations of our proposed IEGMN layer that compute messages mj→i, μj→i,
mi and μ% are SE(3)-invariant. Indeed, they depend on the initial features which are SE(3)-invariant
by design, the current latent node embeddings {hi(l)}i∈V1∪V2, as well as the Euclidean distances on
the current node coordinates {xi(l)}i∈V1∪V2. Thus, we also derive that the equation that computes the
new latent node embeddings hi(l+1) is SE(3)-invariant. Last, the equation that updates the coordinates
xi(l+1) is SE(3)-equivariant with respect to the 3D coordinates of nodes from the same graph as i, but
SE(3)-invariant with respect to the 3D coordinates of nodes from the other graph since it only uses
invariant transformations of the latter.
□
C	S urface Features
Visualization. We further discuss our new surface features introduced in Eq. (16). We first visualize
their design intuition in Fig. 8. A synthetic experiment is shown in Fig. 9.
Correlation with MSMS features. Next, we analyze how accurate are these features compared
to established residue depth estimation methods, e.g. based on the MSMS software (Sanner et al.,
1996). We plot the Spearman rank-order correlation of the two methods in Fig. 10. We observe a
concentrated distribution with a mean of 0.68 and a median of 0.70, suggesting a strong correlation
with the MSMS depth estimation.
17
Published as a conference paper at ICLR 2022
Closed form expression. Finally, we prove that for points close to the protein surface and sur-
rounded by (infinitely) many equally-distanced and equally-spaced points, one can derive a closed
form expression of the surface features defined in Eq. (16). See Fig. 11. We work in 2 dimensions,
but extensions to 3 dimensions are straightforward. Assume that the local surface at point xi has
angle α. Further, assume that xi is surrounded by N equally-distanced and equally-spaced points
denoted by x0i. Then, all wi,i0,λ will be identical. Then, the summation vector in the numerator
of Eq. (16) will only have non-zero components on the direction that bisects the surface angle, as
the other components will cancel-out. Then, under the limit N → ∞, we derive the closed form
expression:
Pi(Xi; λ) = N
Xi - XiO
∣∣χi - XiO k
Σ
W X cos(焉)≈N→∞ —	cos(θ)dθ = 2 ( / )
Nj=0 N	α 0	α
(17)
Figure 8: Intuition behind surface features defined in Eq. (16). a. Residues in the core (interior) of a
protein are likely to have a small weighted average of directionally spread neighboring forces, while
b. residues close to the surface receive vector contributions from a narrower space subset and, thus,
have larger ρ feature values.
Figure 9: Distribution of our surface feature values defined in Eq. (16) for 500 points uniformly
distributed in the unit circle. One can notice a strong correlation with the depth (i.e. distance to
surface) which is further quantified in Fig. 10. Note that the scale for λ in this synthetic experiment
differs from that of real proteins.
D Datasets
The overview of datasets is in Table 2. DB5.5 is obtained from https://zlab.umassmed.edu/
benchmark/, while DIPS is downloaded from https://github.com/drorlab/DIPS.
While DIPS contains only the bound structures, thus currently being only suitable for rigid docking,
DB5.5 also includes unbound protein structures, however, mostly showing rigid structures - see
Fig. 12.
18
Published as a conference paper at ICLR 2022
Figure 10: Distribution of the Spearman rank-order coefficient computed per each protein as the
correlation between MSMS residues’ depths and our surface features defined in Eq. (16) (for λ = 30).
Histogram computed over the ligands in the DIPS test set (100 proteins).
Figure 11: For points close to the protein surface where the local surface angle is α we can derive
a closed form expression for the surface feature defined in Eq. (16) under the assumption of being
surrounded by infinitely many points at approximately equal distances and equally-spaced . A similar
derivation is possible in 3D.
RMSD (A) bound - unbound structures (Of the Ca atomic coordinates)
Figure 12: Distance (RMSD) between unbound and bound structures of the DB5.5 dataset reveals
that most of the proteins are relatively rigid. Thus, better datasets are needed to tackle the docking
conformational change problem.
19
Published as a conference paper at ICLR 2022
Table 2: Overview of Datasets. For DIPS, the statistics of number of residues and atoms per protein
is based on a subset consisting of 200 proteins.
Dataset # Pairs of Proteins # Residues per Protein # Atoms per Protein
DIPS	41876	276 (±189)	2159 (±1495)
DB5.5	253	268 (±215)	2089 (±1694)
E More Experimental Details and Results
Baseline Failures. On the test sets, ATTRACT fails for ’1N2C’ in DB5.5, ’oi_4oip.pdb1_8’,
’oi_4oip.pdb1_3’ and ’p7_4p7s.pdb1_2’ in DIPS. For such failure cases, we use the unbound input
structure as the prediction for metrics calculation.
Hyperparameters. We perform hyperparameter search over the choices listed in Table 3 and select
the best hyperparameters for DB5.5 and DIPS respectively based on their corresponding validation
sets.
Table 3: Hyperparameter choices. LN stands for layer normalization, BN stands for batch normaliza-
tion.
Hyperparameter	Choice
Node degree (for k-NN)	10
Weight of the intersection loss	0.0, 1.0
Normalization for hi of IEGMN layers	No, LN
Normalization for others	No, BN, LN
Number of attention heads (K)	25, 50, 100
Slope of leaky relu	0.1, 0.01
Dimension of hi of IEGMN layers	32, 64
Dimension of residue type embedding	32, 64
Number of IEGMN layers	5, 8
If IEGMN layers except the first one share parameters True, False	
η of coordinates skip connection	0.0, 0.25
Weight decay	0, 1e-5, 1e-4, 1e-3
Detailed Running Times. In addition to the main text, we show in Table 4 detailed running times
of all methods. Hardware specifications are as follows: Attract was run on a 6-Core Intel Core i7
2.2 GHz CPU; HDock was run on a single Intel Xeon Gold 6230 2.1 GHz CPU; EquiDock was
run on a single Intel Core i9-9880H 2.3 GHz CPU. ClusPro and PatchDock have been manually
run using their respective web servers.
Plots for DB5.5. We show the corresponding plots for DB5.5 results in Fig. 13.
Table 4: Inference time comparison (in seconds). Note: ClusPro and PatchDock were run manually
using the respective public webservers, thus their runtimes are influenced by their cluster load.
Runtime on DIPS Test Set Runtime on DB5.5 Test Set
Methods	Mean Median		Min	Max	Std	Mean Median		Min	Max	Std
Attract (local)	1285	793	62	8192	793	570	524	180	1708	373
HDock (local)	778	635	145	3177	570	615	461	210	2593	459
ClusPro (web)	10475	9831	2632 22654 4512			15507	14393	9207	28528 4126	
PatchDock (web)	7378	6900	600	16560 3979		3290	2820	1080	14520 2459	
EquiDock (local)	5	3	1	22	5	5	3	1	53	10
20
Published as a conference paper at ICLR 2022
Ablation Studies. To highlight contributions of different model components, we provide ablation
studies in Table 5. One can note that, as expected, removing the pocket loss results in lower interface
RMSD scores compared to removing other components.
Analysis of the Intersection Loss. We further analyze the intersection loss introduced in Eq. (15)
with parameters γ = 10 and σ = 25 (chosen on DB5 validation set). We show in Table 6 that this
loss achieves almost perfect values for the ground truth structures, being important to softly constrain
non-intersecting predicted proteins.
Figure 13: DB5.5 test results: a. Complex-RMSD distributions; b. Interface-RMSD distributions; c.
scatter plot for C-RMSD vs I-RMSD.
Table 5: Ablation studies. We show DIPS test median C-RMSD and I-RMSD values for the
corresponding best validation models. Abbreviations: “intersection loss” = intersection loss in
Eq. (15), “pocket loss” = pocket loss in Eq. (14), “surface feas” = surface features in Eq. (16).
Model	C-RMSD I-RMSD	
Full model	13.29	10.18
without pocket loss	15.91	12.01
without pocket loss, intersection loss	16.43	12.92
without pocket loss, surface feas	14.80	13.10
without pocket loss, intersection loss, surface feas 15.19		11.38
without surface feas	13.73	10.65
without intersection loss	15.49	11.09
without intersection loss, surface feas	15.04	10.94
Table 6: Values of the intersection loss defined in Eq. (15) and evaluated on the DIPS validation set
in different scenarios. “Centered structures” means that both ground truth ligand and receptor point
clouds have been centered (0-mean), without any other modifications.
Centered EquiDock trained
EquiDock trained
Ground truth
structures with intersection loss without intersection loss complexes
56.42
12.68
21.03
1.16
21