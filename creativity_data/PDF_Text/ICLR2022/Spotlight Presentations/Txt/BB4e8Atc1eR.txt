Published as a conference paper at ICLR 2022
Scalable Sampling for
Nonsymmetric Determinantal Point Processes
Insu Han
Yale University
insu.han@yale.edu
Mike Gartrell
Criteo AI Lab
m.gartrell@criteo.com
Jennifer Gillenwater
Google Research
jengi@google.com
Elvis Dohmatob
Facebook AI Research
dohmatob@fb.com
Amin Karbasi
Yale University
amin.karbasi@yale.edu
Ab stract
A determinantal point process (DPP) on a collection of M items is a model,
parameterized by a symmetric kernel matrix, that assigns a probability to every
subset of those items. Recent work shows that removing the kernel symmetry
constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive
performance gains for machine learning applications. However, existing work
leaves open the question of scalable NDPP sampling. There is only one known
DPP sampling algorithm, based on Cholesky decomposition, that can directly apply
to NDPPs as well. Unfortunately, its runtime is cubic in M , and thus does not
scale to large item collections. In this work, we first note that this algorithm can be
transformed into a linear-time one for kernels with low-rank structure. Furthermore,
we develop a scalable sublinear-time rejection sampling algorithm by constructing a
novel proposal distribution. Additionally, we show that imposing certain structural
constraints on the NDPP kernel enables us to bound the rejection rate in a way that
depends only on the kernel rank. In our experiments we compare the speed of all
of these samplers for a variety of real-world tasks.
1	Introduction
A determinantal point process (DPP) on M items is a model, parameterized by a symmetric kernel
matrix, that assigns a probability to every subset of those items. DPPs have been applied to a wide
range of machine learning tasks, including stochastic gradient descent (SGD) (Zhang et al., 2017),
reinforcement learning (Osogami & Raymond, 2019; Yang et al., 2020), text summarization (Dupuy
& Bach, 2018), coresets (Tremblay et al., 2019), and more. However, a symmetric kernel can only
capture negative correlations between items. Recent works (Brunel, 2018; Gartrell et al., 2019) have
shown that using a nonsymmetric DPP (NDPP) allows modeling of positive correlations as well,
which can lead to significant predictive performance gains. Gartrell et al. (2021) provides scalable
NDPP kernel learning and MAP inference algorithms, but leaves open the question of scalable
sampling. The only known sampling algorithm for NDPPs is the Cholesky-based approach described
in Poulson (2019), which has a runtime of O(M 3) and thus does not scale to large item collections.
There is a rich body of work on efficient sampling algorithms for (symmetric) DPPs, including recent
works such as Derezinski et al. (2019); Poulson (2019); Calandriello et al. (2020). Key distinctions
between existing sampling algorithms include whether they are for exact or approximate sampling,
whether they assume the DPP kernel has some low-rank K M , and whether they sample from
the space of all 2M subsets or from the restricted space of size-k subsets, so-called k-DPPs. In
the context of MAP inference, influential work, including Summa et al. (2014); Chen et al. (2018);
Hassani et al. (2019); Ebrahimi et al. (2017); Indyk et al. (2020), proposed efficient algorithms that
the approximate (sub)determinant maximization problem and provide rigorous guarantees. In this
work we focus on exact sampling for low-rank kernels, and provide scalable algorithms for NDPPs.
Our contributions are as follows, with runtime and memory details summarized in Table 1:
•	Linear-time sampling (Section 3): We show how to transform the O(M 3) Cholesky-
decomposition-based sampler from Poulson (2019) into an O(M K2) sampler for rank-K kernels.
1
Published as a conference paper at ICLR 2022
Table 1: Runtime and memory complexities for sampling algorithms developed in this work. M is
the size of the entire item set (ground set), and K is the rank of the kernel (often K M in practice).
We use k by the size of the sampled set (often k K in practice). ω ∈ [0, 1] is a data-dependent
constant (with our specific learning scheme, ω 1). The sublinear-time rejection algorithm includes
a one-time preprocessing step, after which each successive sample only requires “sampling time”.
Sampling algorithm	Preprocessing time	Sampling time	Memory
Linear-time Cholesky-based	-	O(MK2)	O(MK)
Sublinear-time rejection	O(MK2)	O((k3logM + k4 + K)(1 + ω)K) *	O(MK2)
* This assumes some orthogonality constraint on the kernel.
•	Sublinear-time sampling (Section 4): Using rejection sampling, we show how to leverage existing
sublinear-time samplers for symmetric DPPs to implement a sublinear-time sampler for a subclass of
NDPPs that we call orthogonal NDPPs (ONDPPs).
•	Learning with orthogonality constraints (Section 5): We show that the scalable NDPP kernel
learning of Gartrell et al. (2021) can be slightly modified to impose an orthogonality constraint,
yielding the ONDPP subclass. The constraint allows us to control the rejection sampling algorithm’s
rejection rate, ensuring its scalability. Experiments suggest that the predictive performance of the
kernels is not degraded by this change.
For a common large-scale setting where M is 1 million, our sublinear-time sampler results in runtime
that is hundreds of times faster than the linear-time sampler. In the same setting, our linear-time
sampler provides runtime that is millions of times faster than the only previously known NDPP
sampling algorithm, which has cubic time complexity and is thus impractical in this scenario.
2	Background
Notation. We use [M] := {1, . . . , M} to denote the set of items 1 through M. We use IK to denote
the K-by-K identity matrix, and often write I := IM when the dimensionality should be clear from
context. Given L ∈ RM ×M, we use Li,j to denote the entry in the i-th row and j-th column, and
La,b ∈ RlAl×lBl for the submatrix formed by taking rows A and columns B. We also slightly abuse
notation to denote principal submatrices with a single subscript, LA := LA,A.
Kernels. As discussed earlier, both (symmetric) DPPs and NDPPs define a probability distribution
over all 2M subsets of a ground set [M]. The distribution is parameterized by a kernel matrix
L ∈ RM×M and the probability of a subset Y ⊆ [M] is defined to be Pr(Y) Y det(Lγ). For this to
define a valid distribution, it must be the case that det(LY) ≥ 0 for all Y. For symmetric DPPs, the
non-negativity requirement is identical to a requirement that L be positive semi-definite (PSD). For
nonsymmetric DPPs, there is no such simple correspondence, but prior work such as Gartrell et al.
(2019; 2021) has focused on PSD matrices for simplicity.
Normalizing and marginalizing. The normalizer of a DPP or NDPP distribution can also be written
as a single determinant: PY⊆[M] det(LY) = det(L + I) (Kulesza & Taskar, 2012, Theorem 2.1).
Additionally, the marginal probability of a subset can be written as a determinant: Pr(A ⊆ Y) =
det(KA), for K := I - (L + I)-1 (Kulesza & Taskar, 2012, Theorem 2.2)*, where K is typically
called the marginal kernel.
Intuition. The diagonal element Ki,i is the probability that item i is included in a set sampled from
the model. The 2-by-2 determinant det(K{i,j}) = Ki,iKj,j - Ki,jKj,j is the probability that both
i and j are included in the sample. A symmetric DPP has a symmetric marginal kernel, meaning
Ki,j = Kj,i, and hence Ki,iKj,j - Ki,j Kj,i ≤ Ki,iKj,j . This implies that the probability of
including both i andj in the sampled set cannot be greater than the product of their individual inclusion
probabilities. Hence, symmetric DPPs can only encode negative correlations. In contrast, NDPPs can
have Ki,j and Kj,i with differing signs, allowing them to also capture positive correlations.
2.1	Related Work
Learning. Gartrell et al. (2021) proposes a low-rank kernel decomposition for NDPPs that admits
linear-time learning. The decomposition takes the form L := V V > + B(D - D>)B> for
*The proofs in Kulesza & Taskar (2012) typically assume a symmetric kernel, but this particular one does
not rely on the symmetry.
2
Published as a conference paper at ICLR 2022
Algorithm 1 Cholesky-based NDPP sampling (Poulson, 2019, Algorithm 1)
1	procedure SAMPLECHOLESKY(K)	. marginal kernel factorization Z, W
2	Y — 0	Q J W
3	:	for i = 1 to M do
4	:	pi — Ki,i	pi — zi>Qzi
5	:	u — uniform(0, 1)
6	
7	:	if u ≤ pi then Y — Y ∪ {i} :	else pi — pi - 1
8	KA — KA - κA,pKi,A for A := {i + 1,...,M}	Q — Q - Qziz>q
9	:	return Y
V , B ∈ RM×K, and D ∈ RK×K. The V V > component is a rank-K symmetric matrix, which can
model negative correlations between items. The B(D - D>)B> component is a rank-K skew-
symmetric matrix, which can model positive correlations between items. For compactness of notation,
we will write L = ZXZ>, where Z = V B ∈ RM×2K, andX = hI0K D-0D> i ∈ R2K×2K.
The marginal kernel in this case also has a rank-2K decomposition, as can be shown via application
of the Woodbury matrix identity:
K := I - (I + L)T = ZX (I2k + Z>ZX) — 1 Z>.	(1)
Note that the matrix to be inverted can be computed from Z and X in O(MK2) time, and the inverse
itself takes O(K3) time. Thus, K can be computed from L in time O(MK2). We will develop
sampling algorithms for this decomposition, as well as an orthogonality-constrained version of it. We
use W := X I2K + Z>ZX -1 in what follows so that we can compactly write K = ZW Z>.
Sampling. While there are a number of exact sampling algorithms for DPPs with symmetric kernels,
the only published algorithm that clearly can directly apply to NDPPs is from Poulson (2019) (see
Theorem 2 therein). This algorithm begins with an empty set Y = 0 and iterates through the M items,
deciding for each whether or not to include it in Y based on all of the previous inclusion/exclusion
decisions. Poulson (2019) shows, via the Cholesky decomposition, that the necessary conditional
probabilities can be computed as follows:
Prj ∈ Y | i ∈ Y )= Pr" YY) = Kj,j - (Kj,iKi,j) /K,,
Pr(i ∈ Y)
Pr(j ∈ Y | i ∈/ Y)
Pr(j ∈ Y)- Pr({i,j}⊆ Y)
Pr(i ∈ Y)
Kj,j - (Kj,iKi,j) / (Ki,i - 1) .
(2)
(3)
Algorithm 1 (left-hand side) gives pseudocode for this Cholesky-based sampling algorithm*.
There has also been some recent work on approximate sampling for fixed-size k-NDPPs: Alimoham-
madi et al. (2021) provide a Markov chain Monte Carlo (MCMC) algorithm and prove that the overall
runtime to approximate ε-close total variation distance is bounded by O(M2k3 log(1∕(ε Pr(Yo))),
where Pr(Y0) is probability of an initial state Y0. Improving this runtime is an interesting avenue for
future work, but for this paper we focus on exact sampling.
3	Linear-time Cholesky-Based Sampling
In this section, we show that the O(M3) runtime of the Cholesky-based sampler from Poulson
(2019) can be significantly improved when using the low-rank kernel decomposition of Gartrell et al.
(2021). First, note that Line 8 of Algorithm 1, where all marginal probabilities are updated via an
(M - i)-by-(M - i) matrix subtraction, is the most costly part of the algorithm, making overall time
and memory complexities O(M3) and O(M 2), respectively. However, when the DPP kernel is given
by a low-rank decomposition, we observe that marginal probabilities can be updated by matrix-vector
*Cholesky decomposition is defined only for a symmetric positive definite matrix. However, we use the
term “Cholesky” from Poulson (2019) to maintain consistency with this work, although Algorithm 1 is valid for
nonsymmetric matrices.
3
Published as a conference paper at ICLR 2022
Algorithm 2 Rejection NDPP sampling	(Tree-based sampling)
1 2 3 4 5	: procedure PREPRoCESS(V , B, D) {(σj, y2j-1, y2j)}K=/2 - YOULADECOMPOSE(B, D)* X — diag(Iκ,σι,σι,..., σκ∕2,σK∕2) Z — [V, yι,..., yκ ]	{(λi, Zi)}2KL — EIGENDECOMPOSE(Z^1/2) T — CONSTRUCTTREE(M, [Z1,..., Z2κ]>) return Z, X	return T, {(λi, Zi)}2Kl
6 7 8 9 10 11 12	procedure SAMPLEREJECT(V, B, D, Z, X)	. tree T, eigen pair {(λi, Zi)}2KI of ZXZ :	while true do Y — SAMPLEDPP(Z父Z>)	Y — SAMPLEDPP(T, {(%, Zi)}2Kι) U — Uniform(0,1) ,det([VV > + B(D-D>)B>]γ) P	&€可逐文2>]丫) :	if u ≤ p then break :	return Y
multiplications of dimension 2K, regardless of M . In more detail, suppose we have the marginal
kernel K = ZWZ> as in Eq. (1) and let zj be the j-th row vector in Z. Then, for i 6= j:
Prj ∈ Y | i ∈ Y) = Kj,j - (K"Ki,j"K" = z>(W - (WyW>W)) Zj,	(4)
zi Wzi
Pr(j ∈ Y I i ∈Y ) = z> (W - (WzffW)) Zj.	(5)
The conditional probabilities in Eqs. (4) and (5) are of bilinear form, and the zj do not change during
sampling. Hence, it is enough to update the 2K-by-2K inner matrix at each iteration, and obtain the
marginal probability by multiplying this matrix by Zi . The details are shown on the right-hand side of
Algorithm 1. The overall time and memory complexities are O(MK2) and O(MK), respectively.
4	Sublinear-time Rejection Sampling
Although the Cholesky-based sampler runs in time linear in M, even this is too expensive for the
large M that are often encountered in real-world datasets. To improve runtime, we consider rejection
sampling (Von Neumann, 1963). Let p be the target distribution that we aim to sample, and let
q be any distribution whose support corresponds to that of p; we call q the proposal distribution.
Assume that there is a universal constant U such that p(x) ≤ Uq(x) for all x. In this setting, rejection
sampling draws a sample x from q and accepts it with probability p(x)/(U q (x)), repeating until an
acceptance occurs. The distribution of the resulting samples is p. It is important to choose a good
proposal distribution q so that sampling is efficient and the number of rejections is small.
4.1	Proposal DPP Construction
Our first goal is to find a proposal DPP with symmetric kernel L that can upper-bound all probabilities
of samples from the NDPP with kernel L within a constant factor. To this end, we expand the
determinant of a principal submatrix, det(LY ), using the spectral decomposition of the NDPP kernel.
Such a decomposition essentially amounts to combining the eigendecomposition of the symmetric
part of L with the Youla decomposition (Youla, 1961) of the skew-symmetric part.
Specifically, suppose {(σj, y2j-1, y2j)}jK=/12 is the Youla decomposition of B(D - D>)B> (see
Appendix D for more details), that is,
K/2
B(D — d>)b> = Xσj (y2j-1y>∙ - y2jy>∙-ι).	⑹
j=1
^Pseudo-code of Youladecompose is provided in Algorithm 4. See Appendix D.
4
Published as a conference paper at ICLR 2022
Then we can simply write L = ZXZ>, for Z := [V , y1, . . . , yK] ∈ RM×2K, and
X := diag
0	σ1
-σ1	0
Now, consider defining a related but symmetric
0
..,
-σK∕2
PSD kernel
σK∕2
0
(7)
^	^	-I-	^
L := ZXZ> with X :
diag (IK, σ1,σ1,...,σκ∕2,σκ∕2). All determinants of the principal submatrices of L = ZXZ>
upper-bound those of L, as stated below.
Theorem 1. For every subset Y ⊆ [M], it holds that det(LY) ≤ det(LY). Moreover, equality holds
when the size of Y is equal to the rank of L.
Proof sketch: From the Cauchy-Binet formula, the determinants of LY and LY for all Y ⊆
[M], |Y | ≤ 2K can be represented as
Σ
det(LY)
det(LY)
I⊆[K],∣I∣ = ∣Y| J⊆[K],∣J| = |Y|
X	~€可父1 )det(Zγ,ι )2.
I⊆[2K],∣I∣ = ∣Y∣
det(XI,J) det(ZY,I) det(ZY,J),
(8)
(9)
Many of the terms in Eq. (8) are actually zero due to the block-diagonal structure of X. For example,
note that if 1 ∈ I but 1 ∈/ J, then there is an all-zeros row in XI,J, making det(XI,J) = 0. We show
that each XI,J with nonzero determinant is a block-diagonal matrix with diagonal entries among
±σj, or [; j . With this observation, We can prove that det(Xι,j) is upper-bounded by det(X∕)
or det(Xj). Then, through application of the rearrangement inequality, we can upper-bound the sum
of the det(Xι,j) det(Zγ,ι) det(Zγ,j) in Eq. (8) with a sum over det(^) det(Zγ,ι )2. Finally, we
show that the number of non-zero terms in Eq. (8) is identical to the number of non-zero terms in
Eq. (9). Combining these gives us the desired inequality det(LY) ≤ det(LY). The full proof of
Theorem 1 is in Appendix E.1.
Now, recall that the normalizer of a DPP (or NDPP) with kernel L is det(L+I). The ratio of
probability of the NDPP with kernel L to that of a DPP with kernel L is thus:
PrL(Y) _ det(Lγ)/det(L + I) det(L + I)
PrL(Y) = det(Lγ)/det(L + I) ≤ det(L + I),
where the inequality follows from Theorem 1. This gives us the necessary universal constant U
upper-bounding the ratio of the target distribution to the proposal distribution. Hence, given a sample
Y drawn from the DPP with kernel L, we can use acceptance probability PrL(Y)/(U PrLb (Y)) =
1	1 / T ∖∕1j∕T^	∖ TA	1	FC	1	, J	1	1 •	•	•
det(LY)/ det(LY). Pseudo-codes for proposal construction and rejection sampling are given in
Algorithm 2. Note that to derive L from L it suffices to run the Youla decomposition of B(D-
D>)B>, because the difference is only in the skew-symmetric part. This decomposition can run in
O(MK2) time; more details are provided in Appendix D. Since L is a symmetric PSD matrix, we
can apply existing fast DPP sampling algorithms to sample from it. In particular, in the next section
we combine a fast tree-based method with rejection sampling.
4.2	Sublinear-time Tree-based Sampling
There are several DPP sampling algorithms that run in sublinear time, such as tree-based (Gillenwater
et al., 2019) and intermediate (Derezinski et al., 2019) sampling algorithms. Here, we consider
applying the former, a tree-based approach, to sample from the proposal distribution defined by L.
We give some details of the sampling procedure, as in the course of applying it we discovered an
optimization that slightly improves on the runtime of prior work. Formally, let {(λ-, z-)}-2=K1 be the
eigendecomposition of Lb and Z := [z1, . . . , z2K] ∈ RM×2K. As shown in Kulesza & Taskar (2012,
Lemma 2.6), for every Y ⊆ [M], |Y | ≤ 2K, the probability of Y under DPP with L can be written:
PrLb (Y)
, Z 0	、
det(Lγ)
det(Lb+I)
E⊆[2K],∣E∣ = ∣Y∣
det(ZY,EZY>,E)
i∈E
λi+1 Y λ-+ι
i	i∈E -
(10)
5
Published as a conference paper at ICLR 2022
Algorithm 3 Tree-based DPP sampling (Gillenwater et al., 2019)
1	: procedure B RANCH(A, Z)	10:	procedure CONSTRUCTTREE(M, Z)
2	: if A = {j } then	11:	return BRANCH([M], Z)
3	T.A Tj},T.∑ - Z>：Zj,：		
4	:	return T		
5	a`, Ar J Split A in half		
6	T.left J BRANCH(A', Z)		
7	:	T.right J B RANCH(Ar , Z)		
8	: T.Σ J T.left.Σ + T.right.Σ		
9	: return T		
12	: procedure SAMPLEDPP(T, Z, {λi}iK=1)	21:	procedure S AMPLEITEM(T, QY , E)
13	:	EJ0, YJ0, QYJ0	22:	if T is a leaf then return T.A
14	: for i = 1, . . . , K do	23:	p` J (T.left.∑E, QY)
15	E J E ∪{i} w.p. λi∕(λi + 1)	24:	Pr J〈T.right.∑E, QY)
16	: for k = 1, . . . , |E| do	25:	u J uniform(0, 1)
17	:	j J SAMPLEITEM(T, QY, E)	26:	if U ≤ P then P'+Pr
18	:	YJY∪{j}	27:	return SAMPLEITEM(T.left, QY, E)
19	QY J I|E| -Z>,E (ZY,EzY,E) 1 ZY,E	28:	else
20	: return Y	29:	return SAMPLEITEM(T.right, QY, E)
A matrix of the form Z:,E Z:>,E can be a valid marginal kernel for a special type of DPP, called an
elementary DPP. Hence, Eq. (10) can be thought of as DPP probabilities expressed as a mixture of
elementary DPPs. Based on this mixture view, DPP sampling can be done in two steps: (1) choose
an elementary DPP according to its mixture weight, and then (2) sample a subset from the selected
elementary DPP. Step (1) can be performed by 2K independent random coin tossings, while step
(2) involves computational overhead. The key idea of tree-based sampling is that step (2) can be
accelerated by traversing a binary tree structure, which can be done in time logarithmic in M .
More specifically, given the marginal kernel K = Z:,E Z:>,E, where E is obtained from step (1), we
start from the empty set Y = 0 and repeatedly add an item j to Y with probability:
Pr(j ∈ S | Y ⊆ S) = Kj,j- Kj,Y(KY)-1Kγ,j = Zj,EQYZ>e =〈QY, (Z>:ZjUE〉, (11)
where S is some final selected subset, and QY := IE 一 ZYE (Zγ,EZYE) 1 ZγE. Consider a
binary tree whose root includes a ground set [M]. Every non-leaf node contains a subset A ⊆ [M] and
stores a 2K-by-2K matrix Pj ∈A Z>：Zj,：. A partition a` and Ar, such that a` ∪ Ar = A,A' ∩ Ar =
0, are passed to its left and right subtree, respectively. The resulting tree has M leaves and each has
exactly a single item. Then, one can sample a single item by recursively moving down to the left
node with probability:
hQY,Pj∈A' (Z>E,:)Ei
hQY,Pj∈A(ZjW,)E>'
(12)
or to the right node with probability 1 一 p` , until reaching a leaf node. An item in the leaf node is
chosen with probability according to Eq. (11). Since every subset in the support ofan elementary DPP
with a rank-k kernel has exactly k items, this process is repeated for |E | iterations. Full descriptions
of tree construction and sampling are provided in Algorithm 3. The proposed tree-based rejection
sampling for an NDPP is outlined on the right-side of Algorithm 2. The one-time pre-processing
step of constructing the tree (CONSTRUCTTREE) requires O(MK2) time. After pre-processing, the
procedure SAMPLEDPP involves |E| traversals of a tree of depth O(log M), where in each node a
O(|E|2) operation is required. The overall runtime is summarized in Proposition 1 and the proof can
be found in Appendix E.2.
Proposition 1. The tree-based sampling procedure SAMPLEDPP in Algorithm 3 runs in time
O(K + k3 log M + k4), where k is the size of the sampled set§.
§Computing p` via Eq. (12) improves on Gillenwater et al. (2019)’s O(k4 log M) runtime for this step.
6
Published as a conference paper at ICLR 2022
4.3	Average Number of Rejections
We now return to rejection sampling and focus on the expected number of rejections. The number
of rejections of Algorithm 2 is known to be a geometric random variable with mean equal to the
constant U used to upper-bound the ratio of the target distribution to the proposal distribution:
det(L + I)/ det(L + I). If all columns in V and B are orthogonal, which we denote V ⊥ B, then
the expected number of rejections depends only on the eigenvalues of the skew-symmetric part of the
NDPP kernel.
Theorem 2. Given an NDPP kernel L = V V > + B(D - D>)B> for V , B ∈ RM ×K, D ∈
RK ×K, consider the proposal kernel Lb as proposed in Section 4.1. Let {σj }jK=/12 be the positive
b
eigenvalues obtaιnedfrom the Youla decomposition of B(D 一 D>)B>. If V ⊥ B, then det(L；i)=
QK/2(1 + σjσ+1) ≤ (1+ ω)K∕2, where ω = K PX jl ∈ QD
Proof sketch: Orthogonality between V and B allows det(L + I) to be expressed just in terms of the
eigenvalues of VV> and B(D 一 D>)B>. Since both L and Lb share the symmetric part VV>,
the ratio of determinants only depends on the skew-symmetric part. A more formal proof appears in
Appendix E.3.
Assuming we have a kernel where V ⊥ B, we can combine Theorem 2 with the tree-based rejection
sampling algorithm (right-side in Algorithm 2) to sample in time O((K + k3 log M + k4)(1+ ω)K/2).
Hence, we have a sampling algorithm that is sublinear in M, and can be much faster than the Cholesky-
based algorithm when (1 + ω)K/2《M. In the next section, We introduce a learning scheme with
the V ⊥ B constraint, as well as regularization to ensure that ω is small.
5	Learning with Orthogonality Constraints
We aim to learn a NDPP that provides both good predictive performance and a low rejection rate. We
>	>>
parameterize our NDPP kernel matrix L = VV + B (D 一 D )B by
0
0
0
0
D = diag
σ1
0
σK∕2
0
(13)
for σj ≥ 0, B> B = I, and, motivated by Theorem 2, require V> B = 0* We call such
orthogonality-constrained NDPPs “ONDPPs”. Notice that if V ⊥ B, then L has the full rank
of 2K, since the intersection of the column spaces spanned by V and by B is empty, and thus the
full rank available for modeling can be used. Thus, this constraint can also be thought of as simply
ensuring that ONDPPs use the full rank available to them.
Given example subsets {Y1, . . . , Yn} as training data, learning is done by minimizing the regularized
negative log-likelihood:
mm	-1 XX log (曾(M
V,B,{σ,}K/ n a	det(L + I)
M
+αX
i=1
Mk2
μi
X kbik2
⅛1	μi
K∕2
+ γ log
j =1
(14)
where α,β,γ > 0 are hyperparameters, μi is the frequency of item i in the training data, and Vi
and bi represent the rows of V and B, respectively. This objective is very similar to that of Gartrell
et al. (2021), except for the orthogonality constraint and the final regularization term. Note that this
regularization term corresponds exactly to the logarithm of the average rejection rate, and therefore
should help to control the number of rejections.
6	Experiments
We first show that the orthogonality constraint from Section 5 does not degrade the predictive
performance of learned kernels. We then compare the speed of our proposed sampling algorithms.
‘Technical details: To learn NDPP models with the constraint V> B = 0, we project V according
to: V — V - B(B>B)T(BTV). For the B>B = I constraint, we apply QR decomposition on B.
Note that both operations require O(MK2) time. (Constrained learning and sampling code is provided at
https://github.com/insuhan/nonsymmetric-dpp-sampling. We use Pytorch’s linalg.solve to avoid the
expense of explicitly computing the (B>B)-1 inverse.) Hence, our learning time complexity is identical to
that of Gartrell et al. (2021).
7
Published as a conference paper at ICLR 2022
Table 2: Average MPR and AUC, with 95% confidence estimates obtained via bootstrapping, test
log-likelihood, and the number of rejections for NDPP models. Bold values indicate the best MPR,
outside of the confidence intervals of the two baseline methods.
Low-rank DPP Models	Metric	UK Retail M=3,941	Recipe M=7,993	Instacart M=49,677	Million Song M =371,410	Book M=1,059,437
Symmetric DPP (Gartrell et al., 2017)	MPR	76.42 ± 0.97	95.04 ± 0.69	93.06 ± 0.92	90.00 ± 1.18	72.54 ± 2.03
	AUC	0.74 ± 0.01	0.99 ± 0.01	0.86 ± 0.01	0.77 ± 0.01	0.70 ± 0.01
	Log-Likelihood	-104.89	-44.63	-73.22	-310.14	-149.76
	MPR	77.09 ± 1.10	95.17 ± 0.67	92.40 ± 1.05	89.00 ± 1.11	72.98 ± 1.46
NDPP	AUC	0.74 ± 0.01	0.99 ± 0.00	0.87 ± 0.01	0.80 ± 0.01	0.74 ± 0.01
(Gartrell et al., 2021)	Log-Likelihood	-99.09	-44.72	-74.94	-314.12	-149.93
	# of Rejections	4.136 ×1010	78.95	6.806 ×103	3.907 ×1010	9.245×106
	MPR	78.43 ± 0.95	95.40 ± 0.62	92.80 ± 0.99	93.02 ± 0.83	75.35 ± 1.83
ONDPP	AUC	0.71 ± 0.00	0.99 ± 0.01	0.83 ± 0.01	0.77 ± 0.01	0.64 ± 0.01
without regularization	Log-Likelihood	-99.45	-44.60	-72.69	-302.64	-140.53
	# of Rejections	1.818 ×109	103.81	128.96	5.563 ×107	682.22
	MPR	77.12 ± 0.98	95.50 ± 0.59	92.99 ± 0.95	92.86 ± 0.80	75.73 ± 1.84
ONDPP	AUC	0.72 ± 0.01	0.99 ± 0.01	0.83 ± 0.01	0.77 ± 0.01	0.64 ± 0.01
with regularization	Log-Likelihood	-103.83	-44.56	-72.72	-305.66	-140.67
	# of Rejections	26.09	21.59	79.74	45.42	61.10
6.1	Predictive Performance Results for NDPP Learning
We benchmark various DPP models, including symmetric (Gartrell et al., 2017), nonsymmetric for
scalable learning (Gartrell et al., 2021), as well as our ONDPP kernels with and without rejection rate
regularization. We use the scalable NDPP models (Gartrell et al., 2021) as a baseline||. The kernel
components of each model are learned using five real-world recommendation datasets, which have
ground set sizes that range from 3,941 to 1,059,437 items (see Appendix A for more details).
Our experimental setup and metrics mirror those of Gartrell et al. (2021). We report the mean
percentile rank (MPR) metric for a next-item prediction task, the AUC metric for subset discrimination,
and the log-likelihood of the test set; see Appendix B for more details on the experiments and metrics.
For all metrics, higher numbers are better. For NDPP models, we additionally report the average
rejection rates when they apply to rejection sampling.
In Table 2, we observe that the predictive performance of our ONDPP models generally match or
sometimes exceed the baseline. This is likely because the orthogonality constraint enables more
effective use of the full rank-2K feature space. Moreover, imposing the regularization on rejection
rate, as shown in Eq. (14), often leads to dramatically smaller rejection rates, while the impact on
predictive performance is generally marginal. These results justify the ONDPP and regularization
for fast sampling. Finally, we observe that the learning time of our ONDPP models is typically a bit
longer than that of the NDPP models, but still quite reasonable (e.g., the time per iteration for the
NDPP takes 27 seconds for the Book dataset, while our ONDPP takes 49.7 seconds).
Fig. 1 shows how the regularizer γ affects the test log-likelihood and the average number of rejections.
We see that γ degrades predictive performance and reduces the rejection rate when set above a certain
threshold; this behavior is seen for many datasets. However, for the Recipe dataset we observed that
the test log-likelihood is not very sensitive to γ, likely because all models in our experiments achieve
very high performance on this dataset. In general, we observe that γ can be set to a value that results
in a small rejection rate, while having minimal impact on predictive performance.
6.2	Sampling Time Comparison
We benchmark the Cholesky-based sampling algorithm (Algorithm 1) and tree-based rejection
sampling algorithm (Algorithm 2) on ONDPPs with both synthetic and real-world data.
||We use the code from https://github.com/cgartrel/scalable-nonsymmetric-DPPs for the NDPP
baseline, which is made available under the MIT license. To simplify learning and MAP inference, Gartrell
et al. (2021) set B = V in their experiments. However, since we have the V ⊥ B constraint in our ONDPP
approach, we cannot set B = V . Hence, for a fair comparison, we do not set B = V for the NDPP baseline in
our experiments, and thus the results in Table 2 differ slightly from those published in Gartrell et al. (2021).
8
Published as a conference paper at ICLR 2022
Figure 1: Average number of rejections and test Figure 2: Wall-clock time (sec) for synthetic data
log-likelihood with different values of the regu- for (a) NDPP sampling algorithms and (b) prepro-
larizer γ for ONDPPs trained on the UK Retail cessing steps for the rejection sampling. Shaded
dataset. Shaded regions are 95% confidence inter- regions are 95% confidence intervals from 100
vals of 10 independent trials.	independent trials.
Table 3: Wall-clock time (sec) for preprocessing and sampling ONDPPs trained on real-world data,
and speedup of the tree-based sampler over the Cholesky-based one. We set K = 100 and provide
average times with 95% confidence intervals from 10 independent trials for the Cholesky-based
algorithm and 100 trials for the rejection algorithm. Memory usage for the tree is also reported.
	UK Retail M =3,941	Recipe M=7,993	Instacart M=49,677	Million Song M=371,410	Book M=1,059,437
Spectral decomposition	0.209	0.226	0.505	2.639	7.482
Tree construction	0.997	1.998	12.65	119.0	340.1
Cholesky-based sampling	5.572 ± 0.056	11.36 ± 0.098	71.82 ± 1.087	545.8 ± 8.776	1,631 ± 11.84
Tree-based rejection sampling	2.463 ± 0.417	1.331 ± 0.241	5.962 ± 1.049	14.72 ± 2.620	6.627 ± 1.294
(Speedup)	(×2.262)	(×8.535)	(×12.05)	(× 37.08)	(×246.1)
Tree memory usage	630.5 MB	1.279 GB	7.948 GB	59.43 GB	169.5 GB
Synthetic datasets. We generate non-uniform random features for V , B as done by (Han &
Gillenwater, 2020). In particular, we first sample x1, . . . , x100 from N (0, I2K /(2K)), and integers
t1, . . . , t100 from Poisson distribution with mean 5, rescaling the integers such that Pi ti = M. Next,
we draw ti random vectors from N (xi , I2K), and assign the first K-dimensional vectors as the row
vectors of V and the latter vectors as those of B. Each entry of D is sampled from N(0, 1). We
choose K = 100 and vary M from 212 to 220.
Fig. 2(a) illustrates the runtimes of Algorithms 1 and 2. We verify that the rejection sampling
time tends to increase sub-linearly with the ground set size M , while the Cholesky-based sampler
runs in linear time. In Fig. 2(b), the runtimes of the preprocessing steps for Algorithm 2 (i.e.,
spectral decomposition and tree construction) are reported. Although the rejection sampler requires
these additional processes, they are one-time steps and run much faster than a single run of the
Choleksy-based method for M = 220 .
Real-world datasets. In Table 3, we report the runtimes and speedup of NDPP sampling algorithms
for real-world datasets. All NDPP kernels are obtained using learning with orthogonality constraints,
with rejection rate regularization as reported in Section 6.1. We observe that the tree-based rejection
sampling runs up to 246 times faster than the Cholesky-based algorithm. For larger datasets, we
expect that this gap would significantly increase. As with the synthetic experiments, we see that the
tree construction pre-processing time is comparable to the time required to draw a single sample via
the other methods, and thus the tree-based method is often the best choice for repeated sampling**.
7	Conclusion
In this work we developed scalable sampling methods for NDPPs. One limitation of our rejection
sampler is its practical restriction to the ONDPP subclass. Other opportunities for future work
include the extension of our rejection sampling approach to the generation of fixed-size samples
(from k-NDPPs), the development of approximate sampling techniques, and the extension of DPP
samplers along the lines of Derezinski et al. (2019); Calandriello et al. (2020) to NDPPs. Scalable
sampling also opens the door to using NDPPs as building blocks in probabilistic models.
**We note that the tree can consume substantial memory, e.g., 169.5 GB for the Book dataset with K = 100.
For settings where this scale of memory use is unacceptable, we suggest use of the intermediate sampling
algorithm (Calandriello et al., 2020) in place of tree-based sampling. The resulting sampling algorithm may be
slower, but the O(M + K) memory cost is substantially lower.
9
Published as a conference paper at ICLR 2022
8	Ethics Statement
In general, our work moves in a positive direction by substantially decreasing the computational
costs of NDPP sampling. When using our constrained learning method to learn kernels from user
data, we recommend employing a technique such as differentially-private SGD (Abadi et al., 2016)
to help prevent user data leaks, and adjusting the weights on training examples to balance the impact
of sub-groups of users so as to make the final kernel as fair as possible. As far as we are aware, the
datasets used in this work do not contain personally identifiable information or offensive content.
We were not able to determine if user consent was explicitly obtained by the organizations that
constructed these datasets.
9	Reproducibility Statement
We have made extensive effort to ensure that all algorithmic, theoretical, and experimental contribu-
tions described in this work are reproducible. All of the code implementing our constrained learning
and sampling algorithms is publicly available 廿.The proofs for our theoretical contributions are
available in Appendix E. For our experiments, all dataset processing steps, experimental procedures,
and hyperparameter settings are described in Appendices A, B, and C, respectively.
10	Acknowledgements
Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032) and
ONR (N00014-19-1-2406).
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep Learning with Differential Privacy. In Conference on Computer and
Communications Security, 2016.
Yeganeh Alimohammadi, Nima Anari, Kirankumar Shiragur, and Thuy-Duong Vuong. Fractionally
log-concave and sector-stable polynomials: counting planar matchings and more. In Symposium
on the Theory of Computing (STOC), 2021.
Victor-Emmanuel Brunel. Learning Signed Determinantal Point Processes through the Principal
Minor Assignment Problem. In Neural Information Processing Systems (NeurIPS), 2018.
Daniele Calandriello, Michal Derezinski, and Michal Valko. Sampling from a k-DPP without looking
at all items. In Neural Information Processing Systems (NeurIPS), 2020.
Daqing Chen, Sai Laing Sain, and Kun Guo. Data mining for the online retail industry: A case study
of RFM model-based customer segmentation using data mining. Journal of Database Marketing
& Customer Strategy Management, 2012.
Laming Chen, Guoxin Zhang, and Eric Zhou. Fast greedy MAP inference for Determinantal
Point Process to improve recommendation diversity. In Neural Information Processing Systems
(NeurIPS), 2018.
Michal Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point
processes with sublinear time preprocessing. Neural Information Processing Systems (NeurIPS),
2019.
Christophe Dupuy and Francis Bach. Learning determinantal point processes in sublinear time. In
Conference on Artificial Intelligence and Statistics (AISTATS), 2018.
Javad Ebrahimi, Damian Straszak, and Nisheeth Vishnoi. Subdeterminant maximization via noncon-
vex relaxations and anti-concentration. Foundations of Computer Science (FOCS), 2017.
"https://github.com/insuhan/nonsymmetric-dpp-sampling
10
Published as a conference paper at ICLR 2022
Mike Gartrell, Ulrich Paquet, and Noam Koenigstein. Low-Rank Factorization of Determinantal
Point Processes. In Conference on Artificial Intelligence (AAAI), 2017.
Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. Learning Non-
symmetric Determinantal Point Processes. In Neural Information Processing Systems (NeurIPS),
2019.
Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel.
Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes. In
International Conference on Learning Representations (ICLR), 2021.
Jennifer Gillenwater, Alex Kulesza, Zelda Mariet, and Sergei Vassilvtiskii. A Tree-Based Method
for Fast Repeated Sampling of Determinantal Point Processes. In International Conference on
Machine Learning (ICML), 2019.
Insu Han and Jennifer Gillenwater. MAP Inference for Customized Determinantal Point Processes via
Maximum Inner Product Search. In Conference on Artificial Intelligence and Statistics (AISTATS),
2020.
Hamed Hassani, Amin Karbasi, Aryan Mokhtari, and Zebang Shen. Stochastic conditional gradient++.
SIAM Journal on Optimization (SIOPT), 2019.
Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative Filtering for Implicit Feedback Datasets.
In International Conference on Data Mining (ICDM), 2008.
Piotr Indyk, Sepideh Mahabadi, Shayan Oveis Gharan, and Alireza Rezaei. Composable core-sets for
determinant maximization problems via spectral spanners. In Symposium on Discrete Algorithms
(SODA), 2020.
Instacart. The Instacart Online Grocery Shopping Dataset, 2017. URL https://www.instacart.
com/datasets/grocery-shopping-2017. Accessed May 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Alex Kulesza and Ben Taskar. Determinantal Point Processes for Machine Learning. Foundations
and Trends® in Machine Learning, 2012.
Yanen Li, Jia Hu, ChengXiang Zhai, and Ye Chen. Improving One-class Collaborative Filtering by
Incorporating Rich User Information. In Conference on Information and Knowledge Management
(CIKM), 2010.
Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, and Julian J McAuley. Generating Person-
alized Recipes from Historical User Preferences. In Empirical Methods in Natural Language
Processing (EMNLP), 2019.
Brian McFee, Thierry Bertin-Mahieux, Daniel PW Ellis, and Gert RG Lanckriet. The million song
dataset challenge. In International Conference on the World Wide Web (WWW), 2012.
Yuji Nakatsukasa. The low-rank eigenvalue problem. arXiv preprint arXiv:1905.11490, 2019.
Takayuki Osogami and Rudy Raymond. Determinantal reinforcement learning. In Conference on
Artificial Intelligence (AAAI), 2019.
Jack Poulson. High-performance sampling of generic Determinantal Point Processes. arXiv preprint
arXiv:1905.00165, 2019.
Jocelyn Quaintance. Combinatorial Identities: Table I: Intermediate Techniques for Summing Finite
Series, volume 3. 2010.
Marco Di Summa, Friedrich Eisenbrand, Yuri Faenza, and Carsten Moldenhauer. On largest volume
simplices and sub-determinants. In Symposium on Discrete Algorithms (SODA), 2014.
Nicolas Tremblay, Simon Barthelme, and Pierre-Olivier Amblard. Determinantal Point Processes for
Coresets. Journal of Machine Learning Research (JMLR), 2019.
11
Published as a conference paper at ICLR 2022
John Von Neumann. Various techniques used in connection with random digits. John von Neumann,
Collected Works, 1963.
Mengting Wan and Julian McAuley. Item recommendation on monotonic behavior chains. In
Conference on Recommender Systems (RecSys), 2018.
Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning (ICML),
2020.
DC Youla. A normal form for a matrix under the unitary congruence group. Canadian Journal of
Mathematics, 1961.
Cheng Zhang, HedVig Kjellstrom, and StePhan Mandt. Determinantal Point Processes for Mini-Batch
Diversification. In Conference on Uncertainty in Artificial Intelligence (UAI), 2017.
12
Published as a conference paper at ICLR 2022
A	Full details on datasets
We perform experiments on several real-world public datasets composed of subsets:
•	UK Retail: This dataset (Chen et al., 2012) contains baskets representing transactions from an
online retail company that sells all-occasion gifts. We omit baskets with more than 100 items, leaving
us with a dataset containing 19,762 baskets drawn from a catalog of M = 3,941 products. Baskets
containing more than 100 items are in the long tail of the basket-size distribution, so omitting these is
reasonable, and allows us to use a low-rank factorization of the NDPP with K = 100.
•	Recipe: This dataset (Majumder et al., 2019) contains recipes and food reviews from Food.com
(formerly Genius Kitchen产.Each recipe ("basket")is composed of a collection of ingredients,
resulting in 178,265 recipes and a catalog of 7,993 ingredients.
•	Instacart: This dataset (Instacart, 2017) contains baskets purchased by Instacart users* §§ ***. We omit
baskets with more than 100 items, resulting in 3.2 million baskets and a catalog of 49,677 products.
•	Million Song: This dataset (McFee et al., 2012) contains playlists (“baskets”) of songs from Echo
Nest USerS啊.We trim playlists with more than 100 items, leaving 968,674 playlists and a catalog of
371,410 songs.
•	Book: This dataset (Wan & McAuley, 2018) contains reviews from the Goodreads book review
website, including a variety of attributes describing the items*** . For each user we build a subset
(“basket”) containing the books reviewed by that user. We trim subsets with more than 100 books,
resulting in 430,563 subsets and a catalog of 1,059,437 books.
As far as we are aware, these datasets do not contain personally identifiable information or offensive
content. While the UK Retail dataset is publicly available, we were unable to find a license for it.
Also, we were not able to determine if user consent was explicitly obtained by the organizations that
constructed these datasets.
B Full details on experimental setup and metrics
We use 300 randomly-selected baskets as a held-out validation set, for tracking convergence during
training and for tuning hyperparameters. Another 2000 random baskets are used for testing, and
the rest are used for training. Convergence is reached during training when the relative change in
validation log-likelihood is below a predetermined threshold. We use PyTorch with Adam (Kingma &
Ba, 2015) for optimization. We initialize D from the standard Gaussian distribution N (0, 1), while
V and B are initialized from the uniform(0, 1) distribution.
Subset expansion task. We use greedy conditioning to do next-item prediction (Gartrell et al., 2021,
Section 4.2). We compare methods using a standard recommender system metric: mean percentile
rank (MPR) (Hu et al., 2008; Li et al., 2010). MPR of 50 is equivalent to random selection; MPR
of 100 means that the model perfectly predicts the next item. See Appendix B.1 for a complete
description of the MPR metric.
Subset discrimination task. We also test the ability of a model to discriminate observed subsets
from randomly generated ones. For each subset in the test set, we generate a subset of the same
length by drawing items uniformly at random (and we ensure that the same item is not drawn more
than once for a subset). We compute the AUC for the model on these observed and random subsets,
where the score for each subset is the log-likelihood that the model assigns to the subset.
“See https://www.kaggle.com/shuyangli94/food-com-recipes-and-user-interactions for the li-
cense for this public dataset.
§§This public dataset is available for non-commercial use; see https://www.instacart.com/datasets/
gro cery- shopping- 2017 for the license.
犯See http://millionsongdataset.com/faq/ for the license for this public dataset.
***This public dataset is available for academic use only; see https://sites.google.com/eng.ucsd.edu/
ucsdbookgraph/home for the license.
13
Published as a conference paper at ICLR 2022
B.1 Mean Percentile Rank
We begin our definition of MPR by defining percentile rank (PR). First, given a set J, let pi,J
Pr(J ∪ {i} | J). The percentile rank of an item i given a set J is defined as
i0 6∈J 1(pi,J ≥ pi0,J)
PRi,J =	∈	|YJ---------- X 10θ%
where Y\J indicates those elements in the ground set Y that are not found inJ.
For our evaluation, given a test set Y , we select a random element i ∈ Y and compute PRi,Y \{i}. We
then average over the set of all test instances T to compute the mean percentile rank (MPR):
MPR
∣τ X pr
Y∈T
i,Y \{i}.
C Hyperparameters for experiments
Preventing numerical instabilities: The det(LYi) in Eq. (14) will be zero whenever ∣Yi∣ > K,
where Yi is an observed subset. To address this in practice we set K to the size of the largest subset
observed in the data, K0, as in Gartrell et al. (2017). However, this does not entirely fix the issue,
as there is still a chance that the term will be zero even when ∣Yi ∣ ≤ K. In this case though, we
know that we are not at a maximum, since the value of the objective function is -∞. Numerically,
to prevent such singularities, in our implementation we add a small I correction to each LYi when
optimizing Eq. (14) ( = 10-5 in our experiments).
We perform a grid search using a held-out validation set to select the best-performing hyperparameters
for each model and dataset. The hyperparameter settings used for each model and dataset are described
below.
Symmetric low-rank DPP (Gartrell et al., 2017). For this model, we use K for the number of item
feature dimensions for the symmetric component V , and α for the regularization hyperparameter for
V . We use the following hyperparameter settings:
•	UK Retail dataset: K = 100, α = 1.
•	Recipe dataset: K = 100, α = 0.01
•	Instacart dataset: K = 100, α = 0.001.
•	Million Song dataset: K = 100, α = 0.0001.
•	Book dataset: K = 100, α = 0.001
Scalable NDPP (Gartrell et al., 2021). As described in Section 2.1, we use K to denote the number of
item feature dimensions for the symmetric component V and the dimensionality of the nonsymmetric
component D. α and β are the regularization hyperparameters. We use the following hyperparameter
settings:
•	UK dataset: K = 100, α = 0.01.
•	Recipe dataset: K = 100, α = β = 0.01.
•	Instacart dataset: K = 100, α = 0.001.
•	Million Song dataset: K = 100, α = 0.01.
•	Book dataset: K = 100, α = β = 0.1
ONDPP. As described in Section 5, we use K to denote the number of item feature dimensions for
the symmetric component V and the dimensionality of the nonsymmetric component C. α, β, and γ
are the regularization hyperparameters. We use the following hyperparameter settings:
•	UK dataset: K = 100, α = β = 0.01, γ = 0.5.
•	Recipe dataset: K = 100, α = β = 0.01, γ = 0.1.
•	Instacart dataset: K = 100, α = β = 0.001, γ = 0.001.
•	Million Song dataset: K = 100, α = β = 0.01, γ = 0.2.
14
Published as a conference paper at ICLR 2022
•	Book dataset: K = 100, α = β = 0.01, γ = 0.1.
For all of the above model configurations and datasets, we use a batch size of 800 during training.
D Youla Decomposition: S pectral Decomposition for
S kew- symmetric Matrix
We provide some basic facts on the spectral decomposition of a skew-symmetric matrix, and introduce
an efficient algorithm for this decomposition when it is given by a low-rank factorization. We write
i := √-1 and VH as the conjugate transpose of V ∈ CM, and denote Re(Z) and Im(Z) by the real
and imaginary parts of a complex number z, respectively.
Given B ∈ RM×K and D ∈ RK×K, consider a rank-K skew-symmetric matrix B(D- D>)B>.
Note that all nonzero eigenvalues of a real-valued skew-symmetric matrix are purely imaginary.
Denote iσι, -iσι,..., iσκ∕2, -iσκ∕2 by its nonzero eigenvalues where each of σj is real, and
a1 + ib1, a1 -ib1, . . . aK/2 + ibK/2, aK/2 - ibK/2 by the corresponding eigenvectors for aj, bj ∈
RM, which come in conjugate pairs. Then, we can write
K/2
B	(D - D>)B> = X iσj (aj +ibj)(aj +ibj)H -iσj(aj -ibj)(aj -ibj)H	(15)
j=1
K/2
= X 2σj(ajbj> - bjaj>)	(16)
j=1
K/2	>	>
= X	aj	-bj	aj +bj	-σj	σ0j	aajj>	+	bjj>	.	(17)
Note that a1 ± b1 , . . . , aK/2 ± bK/2 are real-valued orthonormal vectors, because
a1, b1, . . . , aK/2, bK/2 are orthogonal to each other and kaj ± bj k1 22 = kaj k22 + kbj k22 = 1 for
all j. The pair {(σj, aj - bj, aj + bj)}jK=/12 is often called the Youla decomposition (Youla, 1961) of
B(D - D>)B>. To efficiently compute the Youla decomposition of a rank-K matrix, we use the
following result.
Proposition 2 (Proposition 1, Nakatsukasa (2019)). Given A, B ∈ CM ×K, the nonzero eigenvalues
of AB> ∈ CM×M and B>A ∈ CK×K are identical. In addition, if (λ, V) is an eigenpair of B>A
with λ 6= 0, then (λ, AV/ kAVk2) is an eigenpair of AB>.
From the above proposition, one can first compute (D - D> )B>B and then apply the eigende-
composition to that K-by-K matrix. Taking the imaginary part of the obtained eigenvalues gives
us the σj ’s, and multiplying B by the eigenvectors gives us the eigenvectors of B(D - D> )B> .
In addition, this can be done in O(MK2 + K3 4 5 6 7 8) time; when M > K it runs much faster than the
eigendecomposition of B(D - D> )B>, which requires O(M 3) time. The pseudo-code of the
Youla decomposition is provided in Algorithm 4.
Algorithm 4 Youla decomposition of low-rank skew-symmetric matrix
1: procedure YOULADECOMPOSITIon(B, D)
2:	{(%, Zj), (η7-,Zj)}K=2 J eigendecomposition of (D - D>)B>B
3:	for j = 1, . . . , K/2 do
4:	σj J Im(ηj) for j = 1, . . . , K/2
5:	y2j-1 J B (Re(zj) - Im(zj))
6:	y2j J B (Re(zj) + Im(zj))
7:	yj J yj/ kyjk forj = 1, . . . ,K
8:	return {(σj, y2j-1, y2j)}jK=/12
15
Published as a conference paper at ICLR 2022
E Proofs
E.1 Proof of Theorem 1
mi. ....--Yl	ι	∙, ι ιι , ι j / τ ∖ ij∕t^ ∖ n<	ι∙. ι ι ι
Theorem 1. For every subset Y ⊆ [M], it holds that det(LY) ≤ det(LY). Moreover, equality holds
when the size of Y is equal to the rank of L.
Proof of Theorem 1. It is enough to fix Y ⊆ [M] such that 1 ≤ |Y | ≤ 2K, because the rank of both
L and L is UP to 2K. Denote k := |Y| and ([2K]) ：= {I ⊆ [2K]; |I| = k} for k ≤ 2K. We recall
the definition of Lb: given V , B, D such that L = V V > + B(D - D>)B>, let {(ρi, vi)}iK=1
be the eigendecomPosition of V V > and {(σj, y2j-1, y2j)}jK=/12 be the YoUla decomPosition of
B(D - D>)B>. Denote Z := [v1, . . . , vK, y1, . . . , yK] ∈ RM×2K and
X := diag ρ, . . . , ρK,
X ：= diag ρι,...,ρκ,
0
-σ1
σ1
0
σ1
0
0
σ1
0	σκ∕2
-σκ∕2	0
σK∕2
0
0
σK∕2
so that L = ZXZ> and L = ZXZ>. Applying the Cauchy-Binet formula twice, We can write the
determinant of the principal submatrices of both L and L:
det(LY)
det(LY)
?〈X
[( t
∈ ∈ e
J Jd
)))
]]]
[2Kk K[2k [2Kk
[( [( [(
∈∈∈
III
===
det(XI,J) det(ZY,I) det(ZY,J)
([2kK])
X det(j^ι,j )det(Zγ,ι )det(Zγ,j)
2
)
I
Y,
(18)
(19)
where Eq. (19) follows from the fact that X is diagonal, which means that det(XI,J) = 0 for I 6= J.
When the size of Y is equal to the rank of L (i.e., k = 2K), the summations in Eqs. (18) and (19)
•	1 ∙ /`	∙ 1	ι,∕τ∖ ι, / -<7- ∖ i t ∕, rτ ∖ 9	tι, /言 ∖	ι, / ɪz- ∖ ι ,/rɪ ∖ 9 ¾. τ
simplify to single terms: det(LY) = det(X) det(ZY,:)2 and det(LY) = det(X) det(ZY,:)2. Now,
observe that the determinants of the full X and X matrices are identical: det(X) = det(X) =
QiK=1 ρi QjK=/12 σj2. Hence, it holds that det(LY) = det(LbY). This proves the second statement of
the theorem.
To prove that det(LY) ≤ det(LY) for smaller subsets Y , we will use the following:
Claim 1. For every I,J ∈ ([2K]) SuCh that det(Xι,j) = 0, there exists a (nonempty) collection of
subset pairs S (I, J) ⊆ ([2K]) X ([2K]) such that
E	det(χι,j) det(Zγ,ι) det(Zγ,j) ≤ E	det(XI,i) det(ZY,i)2.	(20)
(I0,J0)∈S(I,J)	(I0,J0)∈S(I,J)
Claim 2. The number of nonzero terms in Eq. (18) is identical to that in Eq. (19).
Combining Claim 1 with Claim 2 yields
det(Lγ)=	X	det(XI,j)det(Zγ,I)det(Zγ,j) ≤ X det^I,I) det(Zγ,I)2 = det(LY).
I,J∈([2kK])	I∈([2kK])
We conclude the proof of Theorem 1. Below we provide proofs for Claim 1 and Claim 2.
Proof of Claim 1. Recall that X is a block-diagonal matrix, where each block is of size either
1-by-1, containing ρi, or 2-by-2, containing both σj and -σj in the form -0σ σ0j . A submatrix
XI,J ∈ Rk×k with rows I and columns J will only have a nonzero determinant if it contains no
16
Published as a conference paper at ICLR 2022
all-zero row or column. Hence, any XI,J with nonzero determinant will have the following form (or
some permutation of this block-diagonal):
PpI	…	0
..	..	..
..	.
0	. . .	Pp|P I,J |
±σqι	…	0
0
XI,J
0	♦一±σcι T T
qIQI,J I
0	σrι
-σrι 0
.
.
.
0	0	σr∣Rl,J∣
-σrIRI,JI	0
(21)
and we denote PI,J := {p1, . . . , p|PI,J |}, QI,J := {q1, . . . , q|QI,J | }, and RI,J := {r1, . . . , r|RI,J|}.
Indices p ∈ PI,J yield a diagonal matrix with entries Pp. For such p, both I and J must contain index
p. Indices r ∈ RI,J yield a block-diagonal matrix of the form -0σ σ0r . For such r, both I and J
must contain a pair of indices, (K + 2r - 1, K + 2r). Finally, indices q ∈ QI,J yield a diagonal
matrix with entries of ±σq (the sign can be + or -). For such q, I contains K + 2q - 1 or K + 2q,
and J must contain the other. Note that there is no intersection between QI,J and RI,J.
If Qi,j is an empty set (i.e., I = J), then det(Xι,j) = det(X∕,j) and
det(Xι,j) det(Zγ,ι) det(Zγ,j) = det(X∕) det(Zγ,ι)2.	(22)
Thus, the terms in Eq. (18) in this case appear in Eq. (19). Now assume that QI,J = 0 and consider
the following set of pairs:
S(I, J) := {(I0, J0) : PI,J =PI0,J0,QI,J =QI0,J0,RI,J =RI0,J0}.
In other words, for (I0, J0) ∈ S(I, J), the diagonal XI0,J0 contains Pp, -0σ σ0r exactly as in XI,J.
However, the signs of the σr ’s may differ from XI,J. Combining this observation with the definition
of父，
. . .. . .. .ʌ . ʌ . ʌ . ʌ
∣det(Xιo,j 0 )| = ∣det(Xι,j )| = det(Xι) = det(^r) = det(Xj) = det(Xj,).	(23)
Therefore,
det(XI0,J0)det(ZY,I0)det(ZY,J0)	(24)
(I0,J0)∈S(I,J)
≤ X	|det(XI0,J0 )| det(ZY,I0 ) det(ZY,J0 )	(25)
(I0,J0)∈S(I,J)
=det(X∕)	〉：	det(Zγ,∕o)det(Zγ,j0)	(26)
(I0,J0)∈S(I,J)
≤ det(XI)	X	det(ZY,I0)2	(27)
(I0,*)∈S(I,J)
=	^X	det(XIo)det(Zγ,Io )2	(28)
(I0,*)∈S(I,J)
where the third line comes from Eq. (23) and the fourth line follows from the rearrangement inequality.
Note that application of this inequality does not change the number of terms in the sum. This
completes the proof of Claim 1.
17
Published as a conference paper at ICLR 2022
Proof of Claim 2. In Eq. (19), observe that det(X∕) det(Zγ,ι )2 = 0 if and only if det(X∕) = 0.
Since all piis and σ∕s are positive, the number of I ⊆ [2K], |I| = k such that det(^) = 0 is
equal to 2kK . Similarly, the number of nonzero terms in Eq. (18) equals the number of possible
choices of I, J ∈ ([2K]) such that det(Xι,j) = 0. This can be counted as follows: first choose i
items in {pι,..., PK } for i = 0,...,k; then, choose j items in {[σ1 ] ,∙∙∙, [ -σ∖ 2，K/[ } for
j = 0,..., [k-iC; lastly, choose k - i - 2j of {±σq; q / Ri,j}, then choose the sign for each of
these (σq or -σq). Combining all of these choices, the total number of nonzero terms is:
choice of ρp
b k-i C
X
j=0
K/2	K/2-j	2k-i-2j
j	k-i-2j
| {Z }	、	7	}
choice of h -0σr σ0r i	choice of ±σq
(29)
(30)
(31)
where the second line comes from the fact that (2n) = Pbmc (n)(/-1) 2m-2j for any integers
n, m ∈ N such that m ≤ 2n (see (1.69) in Quaintance (2010)), and the third line follows from the fact
that Pr=0 (m)(r-i) = (n+m) forn, m, r ∈ N (Vandermonde’s identity). Hence, both the number
of nonzero terms in Eqs. (18) and (19) is equal to (2K). This completes the proof of Claim 2.	□
E.2 Proof of Proposition 1
Proposition 1. The tree-based sampling procedure SAMPLEDPP in Algorithm 3 runs in time
O(K + k3 log M + k4), where k is the size ofthe sampled set廿L
Proof of Proposition 1. Since computing p` takes O(k2) from Eq. (12), and since the binary tree
has depth O(log M), S AMPLEITEM in Algorithm 3 runs in O(k2 log M) time. Moreover, the query
matrix QY can be updated in O(k3) time as it only requires a k-by-k matrix inversion. Therefore,
the overall runtime of the tree-based elementary DPP sampling algorithm (after pre-processing) is
O(k3 log M + k4). This improves the previous O(k4 log M) runtime studied in Gillenwater et al.
(2019). Combining this with elementary DPP selection (Line 15 in Algorithm 3), we can sample a set
in O(K + k3 log M + k4) time. This completes the proof of Proposition 1.	□
E.3 Proof of Theorem 2
Theorem 2. Given an NDPP kernel L = VV> + B(D - D>)B>for V,B ∈ RM×K,D ∈
RK ×K, consider the proposal kernel Lb as proposed in Section 4.1. Let {σj }jK=/12 be the positive
b
eigenvalues obtainedfrom the Youla decomposition of B(D — D>)B>. If V ⊥ B, then de%+'=
QK/2(1 + σ2j) ≤ (1+ ω)K∕2, where ω = K PK/2 σ2j /(。，1.
jj
Proof of Theorem 2. Since the column spaces of V and B are orthogonal, the corresponding
eigenvectors are also orthogonal, i.e., Z> Z = I2K. Then,
det(L+I) = det(ZXZ> +I) = det(XZ>Z+I2K) = det(X +I2K)	(32)
= YK (pi+ 1)KY/2det	-1σj σ1j	(33)
i=1	j=1	j
K	K/2
=Y(pi+1)Y(σj2+1)	(34)
i=1	j=1
,"Computing p` via Eq. (12) improves on Gillenwater et al.(2019)‘s Ο(k4 log M) runtime for this step.
18
Published as a conference paper at ICLR 2022
and similarly
K	K/2
det(Lb + I) =Y(ρi+1) Y(σj+1)2.
i=1	j=1
Combining Eqs. (34) and (35), we have that
det(L + I) _ Y2 (σj + 1)2 — Y 1	2σj、	2 X 2σj
而E = H 运E = N C + σT+1) ≤ l + K j=ι σF1
where the inequality holds from the Jensen’s inequality. This completes the proof of Theorem 2.
(35)
(36)
□
19