Published as a conference paper at ICLR 2022
Path Auxiliary Proposal for MCMC in Dis-
crete Space
Haoran Sun	Hanjun Dai	Wei Xia*
Georgia Institute of Technology	Google Brain	Amazon
hsun349@gatech.edu	hadai@google.com	weixxia@amazon.com
Arun Ramamurthy
Siemens
arun.ramamurthy@siemens.com
Ab stract
Energy-based Models (EBMs) offer a powerful approach for modeling discrete
structure, but both inference and learning of EBM are hard as it involves sam-
pling from discrete distributions. Recent work shows Markov Chain Monte Carlo
(MCMC) with the informed proposal is a powerful tool for such sampling. How-
ever, an informed proposal only allows local updates as it requires evaluating all
energy changes in the neighborhood. In this work, we present a path auxiliary al-
gorithm that uses a composition of local moves to efficiently explore large neigh-
borhoods. We also give a fast version of our algorithm that only queries the eval-
uation of energy function twice for each proposal via linearization of the energy
function. Empirically, we show that our path auxiliary algorithms considerably
outperform other generic samplers on various discrete models for sampling, in-
ference, and learning. Our method can also be used to train deep EBMs for high
dimensional discrete data.
1	Introduction
Many real-world problems involve discrete structured data modeling, such as syntax trees for nat-
ural language processing(Tai et al., 2015), graphical model for molecules(Gilmer et al., 2017), etc.
A powerful approach for modelling the distribution over structured data is Energy Based Mod-
els(LeCun et al., 2006) (EBMs). EBMs define the distribution with an unnormalized energy func-
tion, which allows great flexibility to fit the target distribution. However, this flexibility also results
in the difficulties in inference and learning, as they require sampling from the EBM (Andrieu et al.,
2003; Hinton, 2002), where the partition function is intractable in many cases.
Markov Chain Monte Carlo (MCMC) algorithms are one of the most widely used methodologies to
sample from intractable distributions (Robert & Casella, 2013). The efficiency for MCMC depends
drastically on the proposal distribution. For example, in continuous space, Metropolis-adjusted
Langevin algorithm (MALA) exploits the gradient of the target in single step walk and biases the
proposal distribution toward high probability region (Rossky et al., 1978; Roberts & Rosenthal,
1998; Welling & Teh, 2011); Hamiltonian Monte Carlo (HMC) employs multi-step walk and ex-
plores the distribution more efficiently(Neal, 2004; Girolami & Calderhead, 2011; Hoffman et al.,
2014). These methods substantially improve the performance of the MCMC algorithm in theory
and in practice. However, their proposal distributions are derived as discretization of continuous
diffusion process and it is still not clear how to appropriately extend such methods into discrete
space(Zanella, 2020).
Recently, Zanella (2020) proposed a general framework called pointwise informed proposal (PIP)
that shows promising results on directly sampling from discrete distributions. PIP utilizes the energy
change in the neighborhood of current state to propose a new state. Following this work, Grathwohl
et al. (2021) propose a more efficient sampler that uses Taylor series to estimate the energy change
*Work done during the time at Siemens
1
Published as a conference paper at ICLR 2022
in the neighborhood. However, both methods only focus on proposing from a small neighborhood,
for example, a 1-Hamming ball. This is due to the computational expense of evaluating the energy
change or approximation error of Taylor series for a large neighborhood. As a result, the samples in
Markov chain will have strong correlation or even be trapped at local optimum, which deteriorates
the sampling efficiency.
In order to efficiently explore a larger neighborhood, we propose a path auxiliary sampler, which is
an auxiliary sampler (Liang et al., 2011) that uses auxiliary path to propose new states. In construc-
tion of the path, we employ a local proposal distribution to make a sequence of small movements
to reach a new state in long distance. A typical challenge for such multi-step proposal is that the
accept ratio could decrease very fast when the number of steps increases. In this work, we show that
the accept ratio of our path auxiliary proposal is independent of the path and only determined by the
property of the current state and the proposed state. As a result, our algorithm is able to maintain a
high accept ratio with multiple local steps. We can provably show that the locally balancing func-
tions are asymptotically optimal for our path auxiliary sampler. We also introduce a scalable version
of the algorithm that uses linearization and can be applied to smooth target distributions.
We empirically evaluate our methods in inference, sampling, and learning on various discrete EBMs.
We demonstrate that our methods significantly improve the sampling efficiency on parity model,
weighted permutation model, Ising model, Restricted Boltzmann Machine, and factorial Hidden
Markov Model. Our method can also learn competitive deep EBMs on discrete image data. The code
can be found at https://github.com/ha0ransun/Path-Auxiliary-Sampler.git.
2	Background
Energy Based Model: Let X be a finite state space. An EBM defines an energy function f :
X → R with target distribution π(x) = e-f(x)/Z, where Z = Pz∈X e-f(z) (LeCun et al., 2006;
Wainwright & Jordan, 2008; Du & Mordatch, 2019). The unnormalized energy function provides
great flexibility to characterize complex distribution. However, this can also make the partition
function Z intractable to calculate and exacerbate the difficulties of learning and inference.
Metropolis-Hastings: MCMC is a commonly used methodology to sample from an intractable
distribution. Metropolis-Hastings (MH) is one of the most commonly used framework for
MCMC(Metropolis et al., 1953; Hastings, 1970). Given the current state x, a proposal distribu-
tion Q(x, ∙) gives a new state y, then MH algorithm accept y with probability of min{ 1, πy)Qyx }
π(x)Q(x,y)
to guarantee the Markov chain is π-reversible. The efficiency of MH algorithm highly depends on
the selection of proposal distribution Q.
Peskun Ordering: Peskun ordering provides a method to compare the efficiency for two MCMC
algorithms (Peskun, 1973; Tierney, 1998). Let P1, P2 be π-reversible Markov transition kernels on
X such that P1(x, y) > cP2(x, y) for all x 6= y for a fixed c > 0, then we say P1 is c-times more
efficient than P2 as the following holds: 1) varπ(h, P1) ≤ CVarn(h, P2) + 1-cVarn(h), ∀h : X →
R; 2) Gap(P1) ≥ c Gap(P2). A smaller asymptotic variance varπ(h, P) means a better estimation
for the expectation of h and a larger spectral gap Gap(P) means a faster conVergence of the MarkoV
chain.
Pointwise Informed Proposals and Locally Balancing Function: A pointwise informed pro-
posal (PIP) is a MH algorithm in discrete space (Zanella, 2020). PIP uses proposal distribution
Qg(x,y) = g(∏(x))I(x,y)∕Zg (x), where the I(x,y) = 1{y∈N(x)} is the membership indicator
w.r.t a symmetric neighborhood N(∙), and the normalizer Zg(x) = Pz∈χ g(∏f))I(x, z). The
weight function g : R+ → R+ determines the efficiency of PIP. Zanella (2020) show that the family
of locally balancing functions G = {g : R+ → R+,g(t) = tg( t), ∀t > 0} is asymptotically opti-
mal for PIP w.r.t. Peskun ordering. Empirically, Zanella (2020) shows g(t) = ʌ/t and g(t)= 力
haVe the best performance.
Gibbs with Gradient GWG (Grathwohl et al., 2021) is a scalable Version of PIP, where the target
distribution ∏(∙) is approximated via Taylor,s series. Despite being powerful, PIP and GWG requires
the calculate the weight and sample from the neighborhood. Hence, only a small neighborhood,
usually a 1-Hamming ball, is used in existing methods.
2
Published as a conference paper at ICLR 2022
3	path auxiliary Proposal
Problems occur in point-wise informed proposal (PIP) when merely considering a small neighbor-
hood, especially for distributions with many local optima. For example, consider a parity distribution
where the state space X = {0, 1}p, and the energy function f(x) = U if the number of 1s in x is
odd, otherwise f(x) = 0. When a 1-Hamming ball neighborhood is used, by symmetry, a PIP will
have a uniform probability to propose a new state from neighborhood. Then, the expected time
to leave a low energy state is O(eU), which could be very inefficient when U is large. When a
r-Hamming ball larger neighborhood is used, a PIP can efficiently escape a low energy state. How-
ever, the neighborhood will contain O(nr) states, which could be computationally prohibitive when
n or r is large.
To address such problems with PIP, we propose a path auxiliary sampler. Instead of directly sam-
pling from a large neighborhood, path auxiliary sampler sequentially samples new state from a local
proposal distribution Q0 . The computation cost at each step is still manageable as Q0 still samples
locally. Then the composition of small movements forms a path that can lead to a new state that is
distant from the current state. The complexity of the sampling grows linearly instead of exponen-
tially w.r.t. r. In this section, we first present the framework of our algorithm. Then, we discuss the
choice of the weight function for path auxiliary sampler. And finally, we introduce a fast version of
path auxiliary sampler.
3.1	Proposal via auxiliary path
We first define an auxiliary path. Given the neighborhood function N, the set of auxiliary paths on
X is defined as
Σ(X, N ):= {(σ, L): σi ∈X ,i = 0,...,L; σι ∈ N (σi-i)),l = 1,…,L}	(1)
To obtain the auxiliary path, We employ a PIP Qo(∙, ∙) to make local movements. We also sample
path length from a prior α(∙) to assure our chain is aperiodic. The path auxiliary sampler is defined
as folloWs
1.	Sample a path length L from a prior α(L).
2.	Denote the current state Xt = σ0, sample σι 〜Qo(σι-ι, ∙) for l = 1,…，L.
3.	Accept xt+ι = σL in probability A(x,σ,L) = min {l, ；(：：) QLT Q0(；l：：1) },else xt+ι = xt.
Theorem 1. The path auxiliary proposal transition rule described above satisfies the detailed bal-
ance and induces a reversible Markov chain with π as its invariant distribution.
3.2	B enefit from Auxiliary Path
We first analyze the performance of path auxiliary sampler on parity distribution mentioned above.
Assume we use a uniform prior α(1) = α(2) = 2. If we sample L = 2, then we will transit back to
a neW state With same energy as the current state. As a result, We can alWays escape from a state in
O(1) steps in expectation.
Having a path auxiliary sampler to sample from L-Hamming ball at each step can be more efficient
than performing L steps of MH sampling in a 1-Hamming ball. To mathematically justify it, we
compare their accept ratio. For a path (σ, L):
π(σL) QlL=1 Q0(σl,σl-1)	L	π(σl)Q0(σl, σl-1)
min t1,∏(σo)Q3 Qo(σi-i,σι) }≥ g min V, n。一"“，)
(2)
We can notice that the accept ratio for path auxiliary sampler is the product of the probability for
single-step method before the minimum operator. As a result, an auxiliary path can have a high
accept ratio as long as the product is large. On the contrary, When performing L steps MH sampling,
the transition can probably be blocked at some steps in the path With loW accept ratios.
3
Published as a conference paper at ICLR 2022
3.3	Balanced Proposal
Although the product in equation 2 allows the Markov chain to escape from a local optimum when
some steps in the path have high accept ratio, it can also exponentially decrease the accept ratio
w.r.t. the path length L when every step has a low accept ratio. Hence, it is important to select a
good local proposal Q0 . In this section, we show that the locally balancing function Zanella (2020)
is asymptotically optimal for path auxiliary sampler. We first define a sub-class of weight function
named as ideal function:
Definition 1. F is the set of ideal function. A function f ∈ F if and only if following conditions
holds: 1) f : R+ → R+; 2) f(1) = 1; 3) f(t) is monotonic increasing，) f (t)f (1 )t ≤ 1, ∀t ≤ 1
The next theorem shows that locally balancing function G is asymptotically better than ideal function
F in Peskun Ordering.
Theorem 2. Consider the state space in Cartesian products X = ×in=1Xi, where each Xi is a finite
space with M elements, and the neighborhood is defined as 1-Hamming ball. Let dn be the maximum
degree in conditional independence graph. IfI) limn→∞ dn = 0; 2) the target distribution satisfies
∏∏(xy) ≤ C < ∞, ∀y ∈ N(x); 3) the path length prior α is bounded by U. Thenfor any f ∈ F, we
have a function g ∈ G = {g(∙) : g(t) = tg(1)} that is asymptotically more efficient than f, which
implies G is asymptotically optimal in F.
The idea to prove this theorem is to use
f(t) := √f(t)f(t)t
. By definition, we have:
f(t) = ft (t)f (ɪ )t = tjf(tf(1)1=tf(1)	(3)
which means f ∈ G. Then, the theorem is proved by showing the ∏-reversible Markov transition
kernel Pf(x, y) ≥ Pf(x, y) asymptotically holds. The proof is given in Sec A.5
Although Definition 1 has some constraints, it already contains almost all natural choices of weight
functions, such as f (t) = 1+^, f (t) = min{1,t}, f (t) = max{1,t}, and f (t) = tα,α ≥ 0. Hence,
the asymptotic optimality for locally balanced function G in ideal function F strongly suggest that
locally balanced function is a good choice for path auxiliary sampler. More discussion for ideal
function can be found in Sec A.3
Property 1. When using a locally balancing function g ∈ G as the weight function, the accept ratio
for path auxiliary sampler can be written as:
A(x, y, σ, L)
(4)
The path independent form in Property 1 shows that if the energy function is smooth enough, the
accept ratio in equation 4 will be high when y is close to x. By selecting g(t) = ʌ/t ∈ G, we name
our auxiliary sampler (Liang et al., 2011) as path auxiliary sampler (PAS) and give the algorithm in
Algorithm 1.
3.4	Scalable path auxiliary Algorithm
Although path auxiliary proposal reduces the complexity from exponential to linear, it still requires
O(nL) evaluations of the probability π(x) in total. In cases where the computational bottleneck is
evaluating π(x), this cost will be expensive. Fortunately, most distributions of interest have differ-
entiable energy functions, such as deep EBMs , and we can use the linearization as approximation
(Grathwohl et al., 2021). Given the current state σ0, the linearization is
~ , 、 ., 、 ____________.., .
fσo (z) = f (σo) + Ef (σo),z - σ°i
(5)
we can estimate ∏σ0(y)∕∏σ0(x) = e-hvf(σ0),y-xi, and define our proposal distribution
Qg,σo (X,y) = g( ∏π0(yX) )∕Zg,σ° (X), withthenormaIizer Zg,σo (X) = Pz∈N(x) g(	)∙ AfterSam-
pling σ, we use the linearization fσL at σL to compute the accept ratio w.r.t. the auxiliary path
)
A(X, σ, L) = min 1,
My QL=I Qgσ S,σl-1)
π(X)Q0=1 Qg,σo (σl-1,σl)
(6)
4
Published as a conference paper at ICLR 2022
1
2
3
4
5
6
Algorithm 1: Path Auxiliary Sampler (PAS) and the fast version (PAFS)
Input: Target Distribution π, Initial state xo, Path Length Prior a, Weight Function g(t) = √t
Output: Sample sequence (x1, x2, ...)
repeat
Sample path length L 〜α(L)
Denote σ0 = xt, sample σι 〜Qg(σi-ι, ∙) or σι 〜Qg,。。(σl-ι, ∙) for l = 1,..., L.
Accept xt+ι = ol in probability A(x, σ, L) = min{1, Zg(Xt))}
or A(x, σ, L)=min {1, ⅛)⅛⅛σ⅛σ⅛ o, else xt+1=xt.
until finish sampling;
By using approximation, We only need to evaluate ∏(y), ∏(χ), Vf (y), Vf (x) once in every pro-
posal, which can significantly reduce the computational cost. We name this algorithm as path auxil-
iary fast sampler (PAFS) (see Algorithm 1).
The PAS frameWork also alloWs different approximations. For example, GWG (GrathWohl et al.,
2021) can be seen as a special case where all indices are sampled from Qg,。。(σo, ∙) rather than
Qg,σo (σι, ∙). Besides, one way to further encourage long range movements is sampling sites without
replacement, that’s to say, the auxiliary path σ does not modify a site more than once. However,
such a sampler requires a decent choice of the path length L. When L is too large, the acceptance
rate drops exponentially fast. Hence, in this work, we focus on PAFS. Similar to GWG (Grathwohl
et al., 2021), the decrease of the proposal quality from PAFS can be bounded.
Theorem 3. Assume the energy function f(x) is differentiable, Vf (x) is K -Lipschitz. Consider
we use weight function g(t) = √t, Path length prior a bounded by U, and I-Hamming ball as
neighborhood. Denote P andP as the transition kernel by path auxiliary sampler and path auxiliary
faster sampler, respectively, we have
U(U+1)
P(x,y) ≥ e-K-2-P(x,y)
(7)
U U(U + 1)
Using the Peskun ordering, we know PAFS is at least e-K -2- times as efficient as PAS. When
K is small, theorem 3 justify PAFS is a good approximation for PAS. When K is large, a tighter
bound needs more assumptions for the target distribution, e.g. conditional independence. See the
proof and discussion in Sec A.7.
4	Related Works
Informed proposal for Metropolis-Hastings (MH) algorithm has been extensively studied in the
continuous space(Robert & Casella, 2013). The most famous algorithms are Metropolis-adjusted
Langevein algorithm (MALA) (Roberts & Rosenthal, 1998) and Hamiltonian Monte Carlo (HMC).
MALA, HMC, and their variants (Girolami & Calderhead, 2011; Hoffman et al., 2014; Welling &
Teh, 2011; Titsias & Papaspiliopoulos, 2018) exploit the gradient of the target distribution to bias
the proposal distribution towards high probability regions. Although gradient-based methods hav-
ing brought substantial improvements in continuous space, it is still unclear how to extend them to
discrete space.
A number of methods try to map the discrete space to a continuous space using relaxation, apply
informed methods in continuous space, and then map the new state back into discrete space(Zhang
et al., 2012; Pakman & Paninski, 2013; Nishimura et al., 2017; Han & Liu, 2018; Jaini et al., 2021)
via Gaussian Integral Trick, uniform dequantization, or VAE flow. Such methods work in some
scenarios, but a key challenge is the embedding of discrete space into continuous space can destroy
the inherent discrete structure, resulting in highly multi-modal and irregular target distribution in
continuous spaces.
Another group of methods directly work on discrete space. Dai et al. (2020) introduces the path as
latent variable in the variational distribution for initializing PCD, but still relies on slow Gibbs sam-
pling for improvement; Titsias & Yau (2017) augment the discrete space with auxiliary variable and
5
Published as a conference paper at ICLR 2022
perform Gibbs sampling in the augmented space based on informed proposal. Zanella (2020) shows
that a family of locally balancing function is asymptotically optimal for informed proposal. Follow-
ing Zanella (2020), Power & Goldman (2019) extends the framework to Markov jumping process,
and Sansone (2021) parameterize locally balanced function to tune it via mutual information to se-
lect good weight function from the locally balanced class. When the target distribution is smooth
enough, Grathwohl et al. (2021) employs a Taylor’s series to approximate the target distribution and
further improve the sampling efficiency. Though these informed proposal algorithms successfully
show orders of magnitude improvements when using 1-Hamming ball as neighborhood, they are
not able to explore a large neighborhood. An extension of GWG (Grathwohl et al., 2021) can par-
tially address this problem via sampling multiple dimensions to modify in one step. However, such
procedure can easily lead to backtracks thereby reducing its efficiency.
5	Experiments
5.1	Samplers under Consideration
In this section, we empirically evaluate the sampling efficiency of path auxiliary sampler (PAS) and
path auxiliary fast sampler (PAFS). We choose the path length prior α as a uniform distribution on
{1, ..., 2X - 1} as suggested in Hoffman et al. (2021) and denote the corresponding samplers as
PAS-X and PAFS-X.
We compare our methods with five types of baselines: random walk sampler (RW), Gibbs sampler
(Gibbs), Hamming ball sampler (HB), locally balanced sampler (LB), Gibbs with gradient sampler
(GWG). RW is an informed proposal with g(t) = 1 that uniformly propose new state from neighbor-
hood. Gibbs partitions the dimension of a state x into two groups xu and x-u, then updates the state
from conditional distribution p(xu|x-u). We denote it as Gibbs-X, where X refers to the dimension
of xu. HB is a two-stage Gibbs sampler on the extended state space (x, x0) (Titsias & Yau, 2017).
We use HB-10-1, where 10 is the block size, and 1 is the hamming ball size. LB (Zanella, 2020)
is implemented as a single-step PIP with g(t) = √7. GWG (GrathWohI et al., 2021) is a scalable
version of LB, which draw the index to flip via first order Taylor’s series of the target distribution.
Although the original paper focuses on flipping one site per step, it is possible for GWG to draw
multiple indices to flip in every step. We denote the algorithm as GWG-X, where X indicates that
the number of indices to modify per step is uniformly sampled from {1, ..., 2X - 1}. The difference
between PAFS and GWG is that GWG is more likely to sample the same index repeatedly thereby
reducing the efficiency especially after mixing, e.g in figure 6.
5.2	Inference on Energy Based Model
Parity Model: We first demonstrate the benefit of path auxiliary sampler versus single-step sampler
in a inference task that estimates the mean μ of a parity distribution. A parity distribution has state
space X = {0, 1}p and energy function
f(x) = ((Pip=1 xi) mod 2)U	(8)
i	.e. a state has energy U ifit has an odd number of 1s, otherwise 0. The neighborhood is defined as
1-Hamming ball. We run the simulation with p = 100 and U ={1, 3, 5}, and report the estimation
Figure 1: Estimation Error of Distribution Mean on Parity Model
error En := ∣∣∕^n - μ∣∣2, where μn = PZi Xi is the sample mean of the Markov chain at step n.
For each setting and method, we run 5 chains for 20,000 steps, and we plot the mean and standard
deviation of the estimation error in figure 1. We can observe that the efficiency of single step sampler
6
Published as a conference paper at ICLR 2022
Figure 2: Energy Trace on Weighted Permutation Model
Figure 3: PAS and PAFS in different lengths on 200 X 200 Ising Model
decreases exponentially when U increases. On the contrary, the path auxiliary sampler can always
escape from the local optima in parity distribution and estimate the distribution mean efficiently.
Weighted Permutation Model: We consider an optimization task on weighted permutation model.
The state space X = Sp is a symmetric group, i.e. for any P ∈ X, P is a permutation of 1,2,...,p.
The energy function is defined as:
f (P)= Pp=I wi,ρ(i)	⑼
Following Zanella (2020), the weight {wj}p,j=ι are i.i.d. sampled from GaUSSian(0,σ2). The
neighborhood is determined by one exchange of the permutation. We use P = 100, σ = 3, 5,10,
and run 5 simulations for each configurations. From figure 2, We can see that LB sampler is trapped
at local optimum, especially when σ is large and the distribution is sharp. On the contrary, PAS can
always efficiently find good solutions via auxiliary path.
5.3	Sampling on Energy Based Model
Lattice Ising Model: Consider the state space X = {-1,1}Vp, where (Vp,Ep) is the P × P square
lattice graph. For each X ∈ X, the energy function is defined as:
f (X) = - Σi∈Vp ɑixi - λ E(i,j)∈Epxixj
(10)
α ∈ R are bias terms representing the property of Xi and λ is a global interaction term. We
first compare PAS and PAFS in different path lengths. For each length, we run 100 chains with
100,000 steps. We report the burn-in stage as well as effective sample size (ESS) * in terms of both
MCMC steps and energy function evaluation times in figure 3. We can see that, 1) PAS has slight
improvement in terms of energy function evaluations when we increase the path length; 2) when
compared to PAS, PAFS has very similar proposal quality and significantly better efficiency. We
also compare our sampler with other competitors. For each sampler, we run 5 Markov chains with
1,000,000 steps and report the ESS to compare the proposal quality in figure 4. We can see our
sampler leads in both quality and efficiency. More results are given in Sec B.1.
factorial Hidden Markov Model: FHMM is a statistical model that use latent variables in X =
{0, 1}n×k to characterize time series data y ∈ RN. DenoteP(X) for hidden variables, and p(y|x)
for likelihood:
N	K	N
P(X) = ∏p(xn,ι) Y[ρ(xn,k∣Xn,k-ι), p(y∣x) = ɪɪ GaUSSian(yn； WXn + b,σ2)	(11)
n=1	k=2	n=1
Given data y, we use MCMC to sample X from the posterior p(x∣j) and compare the mixing
for different samplers. We choose parameters N = 1000, K = 10, P(XnJ = 1) = 0.05,
*Computed follows Tensorflow Probability
7
Published as a conference paper at ICLR 2022
Figure 4: ESS for Different Samplers on 200 × 200 Ising model
Figure 5: BurnIn in FHMM
P(Xn,k = xn,k-1) = 0.85, w ∈ Gaussian(0, IK), b ∈ Gaussian(0, 1), and σ2 = 0.25. We
run each chain 5 times and report the mean and std for the energy (negative log joint density)
E(X) = - log P(X)p(y∣X) and the reconstruction error ∣∣y - wX∣∣2 w.r.t. the number of evaluations of
energy function. From figure 5 we can see that both GWG and PAFS gain significant improvements
in mixing by using a larger neighborhood. More results are given in Sec B.2
Restricted Boltzmann Machine: RBM is bipartite latent-variable model, whose energy function is
defined as:
f(X) = log(1 + ewTx+c) + bTX	(12)
where {W, b, c} are parameters and X ∈ {0, 1}D. We follow Grathwohl et al. (2021) to train a
RBM with 500 hidden units on the MNIST dataset using contrastive divergence(Hinton, 2002).
We generate samples via various MCMC samplers on the trained RBM. Besides reporting ESS,
we also estimate the maximum mean discrepancy (MMD) (Gretton et al., 2012) w.r.t. a set of
“ground truth” samples generated by structure known Block-Gibbs sampler. Figure 7 shows PAFS
has large ESS and can efficiently match the “ground truth” samples. We also demonstrate an ablation
study on different the approximations of the auxiliary path. Specifically, we evaluate GWG, PAFS
with different lengths. For each sampler, we run 100 chains and report their ESS and average hop
distance. We can notice GWG suffers from a large path length L, while our PAFS obtains robust
Figure 6: Sampling on RBM with Different Path Length
improvements for all L by employing a soft no-replacement sampling. This result indicates that
our PAFS provides an efficient framework to sample from a large neighborhood. More results and
discussions, including a no-replacement sampler, are given in Sec B.3.
Figure 7: Sampling on RBM trained on MNIST
IO4
IO9
IO2
RBM ESS
012345β7
Figure 8: Learning Ising
8
Published as a conference paper at ICLR 2022
Dataset	VAE (MLP)	VAE (Conv)	EBM (GWG)	EBM (Gibbs)	EBM (PAFS)	RBM	DBN
-Static MNIST-	-86.05	-82.41	-80.01	-117.17	-79.58	-86.39	-85.67
Dynamic MNIST	-82.42	-80.40	-80.51	-121.19	-79.59	-	-
Omniglot	-103.52	-97.65	-94.72	-142.06	-90.75	-100.47	-100.78
Caltech Silhouettes	-112.08	-106.35	-96.20	-163.50	-84.56	-	-
Table 1: Evaluation of different discrete models on the held-out test set.
5.4	Learning on Energy Based Model
Learning an EBM is a challenge task. Consider the target distribution is π and our energy function
fθ is parameterized by θ. The gradient for the likelihood of ∏ (x) 8 e-fθ(x) is:
Vθ logP(X)= En [Vθfθ (x)] - E∏θ [Vθfθ (x)]	(13)
The first expectation can be estimated using the data from true distribution. The second expectation
requires samples from the current model, which is typically obtained via MCMC. Hence, the success
of training an EBM relies on efficient MCMC algorithms to get an accurate estimation of the second
expectation.
Ising model We first learn an Ising model following the setting in Grathwohl et al. (2021), where
the energy function f(x) = θxT Jx. Given a set of samples {xi}, the task is to learn an EBM via
recovering the connectivity matrix J. We generate a 25 × 25 2D cyclic lattice with θ = 0.25 and
sample training data with a long-run Gibbs chain. We train the models to maximize the likelihood
of samples using persistent contrastive divergence (PCD)(Tieleman & Hinton, 2009) and report the
root mean squared error (RMSE) between the inferred connectivity matrix J and the true matrix J.
For fair comparison, we also include GWG-eq, which runs GWG-1 with more steps such that it has
the same time cost as our PAFS-5. Figure 8 shows larger neighborhood can effectively help learn the
Ising model. GWG-5 having similar performance as PAFS-5 as, using PCD, the learning efficiency
is mainly determined by the mixing of the Markov chain.
Deep EBM We evaluate path auxiliary sampler by learning a deep EBM. The experiment follows
the setting in (Grathwohl et al., 2021). We train deep EBMs paramterized by Residual Networks(He
et al., 2016) on small binary image datasets using PCD(Tieleman & Hinton, 2009) with a replay
buffer(Du & Mordatch, 2019). We compare our methods with Variational Autoencoders Kingma &
Welling (2013), GWG (Grathwohl et al., 2021), RBM anda Deep Belief Network(Hinton, 2009).
We estimate the likelihoods using Annealed Importance Sampling(Neal, 2001) and report the results
in table.1. The results for VAE is taken from Tomczak & Welling (2018), RBM and DBN are taken
from Burda et al. (2015), GWG is taken from Grathwohl et al. (2021). We can see that our approach
improves the log-likelihoods for deep EBMs on all datasets.
6	Discussion and Conclusion
The problem for the selection of the path length remains open. In this work, we propose a soft
no-replacement method PAFS. PAFS is not sensitive to the path length larger than the optimal
length, hence has robust proposal quality for different choice of path length. We also considered
no-backtrack samplers that flip sites without replacement. Intuitively, such auxiliary path encour-
ages to propose states in larger distance. However, for no-backtrack samplers, a too large path length
is harmful for the proposal quality and significant reduce the efficiency of the sampler. As a result,
finding a principle way to decide the path length is very important. In continuous space, the optimal
step size can be characterized via acceptance rate (Gelman et al., 1997; Roberts & Rosenthal, 1998;
2001; Beskos et al., 2013). PAS is the gradient based sampler in discrete space, we believe it is
possible to derive its optimal path length in a similar manner as continuous case. We will investigate
it in our future work.
In summary, informed proposal has shown good results for inference, sampling, and learning EBMs
in discrete spaces. Our path auxiliary sampler provides an approach allowing informed proposal to
efficiently explore large neighborhoods. We believe there is considerable room for future works to
improve the sampling methods in discrete space.
9
Published as a conference paper at ICLR 2022
Acknowledgement: This research was supported in part by the Defense Advanced Research
Projects Agency (DARPA) under Contract FA8750-20-C-0542 (Systemic Generative Engineering).
The views, opinions, and/or findings expressed are those of the author(s) and should not be in-
terpreted as representing the official views or policies of the Department of Defense or the U.S.
Government.
References
Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to
mcmc for machine learning. Machine learning, 50(1):5-43, 2003.
Alexandros Beskos, Natesh Pillai, Gareth Roberts, Jesus-Maria Sanz-Serna, and Andrew Stuart.
Optimal tuning of the hybrid monte carlo algorithm. Bernoulli, 19(5A):1501-1534, 2013.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf
log-likelihood using reverse annealing. In Artificial Intelligence and Statistics, pp. 102-110.
PMLR, 2015.
Hanjun Dai, Rishabh Singh, Bo Dai, Charles Sutton, and Dale Schuurmans. Learning discrete
energy-based models via auxiliary-variable local exploration. arXiv preprint arXiv:2011.05363,
2020.
Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv
preprint arXiv:1903.08689, 2019.
Andrew Gelman, Walter R Gilks, and Gareth O Roberts. Weak convergence and optimal scaling of
random walk metropolis algorithms. The annals of applied probability, 7(1):110-120, 1997.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International conference on machine learning, pp.
1263-1272. PMLR, 2017.
Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):
123-214, 2011.
Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris J Maddison. Oops i
took a gradient: Scalable sampling for discrete distributions. arXiv preprint arXiv:2102.04509,
2021.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Jun Han and Qiang Liu. Stein variational gradient descent without gradient. In International Con-
ference on Machine Learning, pp. 1900-1908. PMLR, 2018.
W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Geoffrey E Hinton. Deep belief networks. Scholarpedia, 4(5):5947, 2009.
Matthew Hoffman, Alexey Radul, and Pavel Sountsov. An adaptive-mcmc scheme for setting tra-
jectory lengths in hamiltonian monte carlo. In International Conference on Artificial Intelligence
and Statistics, pp. 3907-3915. PMLR, 2021.
Matthew D Hoffman, Andrew Gelman, et al. The no-u-turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
10
Published as a conference paper at ICLR 2022
Priyank Jaini, Didrik Nielsen, and Max Welling. Sampling in combinatorial spaces with survae
flow augmented mcmc. In International Conference on Artificial Intelligence and Statistics, pp.
3349-3357. PMLR, 2021.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006.
Faming Liang, Chuanhai Liu, and Raymond Carroll. Advanced Markov chain Monte Carlo methods:
learning from past samples, volume 714. John Wiley & Sons, 2011.
Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward
Teller. Equation of state calculations by fast computing machines. The journal of chemical
physics, 21(6):1087-1092, 1953.
Radford M Neal. Annealed importance sampling. Statistics and computing, 11(2):125-139, 2001.
Radford M Neal. Improving asymptotic variance of mcmc estimators: Non-reversible chains are
better. arXiv preprint math/0407281, 2004.
Akihiko Nishimura, David Dunson, and Jianfeng Lu. Discontinuous hamiltonian monte carlo for
sampling discrete parameters. arXiv preprint arXiv:1705.08510, 853, 2017.
Ari Pakman and Liam Paninski. Auxiliary-variable exact hamiltonian monte carlo samplers for
binary distributions. arXiv preprint arXiv:1311.2166, 2013.
Peter H Peskun. Optimum monte-carlo sampling using markov chains. Biometrika, 60(3):607-612,
1973.
Samuel Power and Jacob Vorstrup Goldman. Accelerated sampling on discrete spaces with non-
reversible markov processes. arXiv preprint arXiv:1912.04681, 2019.
Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business
Media, 2013.
Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling of discrete approximations to langevin
diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):
255-268, 1998.
Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling for various metropolis-hastings algo-
rithms. Statistical science, 16(4):351-367, 2001.
Peter J Rossky, JD Doll, and HL Friedman. Brownian dynamics as smart monte carlo simulation.
The Journal of Chemical Physics, 69(10):4628-4633, 1978.
Emanuele Sansone. Lsb: Local self-balancing mcmc in discrete spaces. arXiv preprint
arXiv:2109.03867, 2021.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Tijmen Tieleman and Geoffrey Hinton. Using fast weights to improve persistent contrastive di-
vergence. In Proceedings of the 26th annual international conference on machine learning, pp.
1033-1040, 2009.
Luke Tierney. A note on metropolis-hastings kernels for general state spaces. Annals of applied
probability, pp. 1-9, 1998.
Michalis K Titsias and Omiros Papaspiliopoulos. Auxiliary gradient-based sampling algorithms.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(4):749-767, 2018.
Michalis K Titsias and Christopher Yau. The hamming ball sampler. Journal of the American
Statistical Association, 112(520):1598-1611, 2017.
11
Published as a conference paper at ICLR 2022
Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial
Intelligence and Statistics, pp.1214-1223. PMLR, 2018.
Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari-
ational inference. Now Publishers Inc, 2008.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688.
Citeseer, 2011.
Joe Whittaker. Graphical models in applied multivariate statistics. Wiley Publishing, 2009.
Giacomo Zanella. Informed proposals for local mcmc in discrete spaces. Journal of the American
Statistical Association, 115(530):852-865, 2020.
Yichuan Zhang, Zoubin Ghahramani, Amos J Storkey, and Charles Sutton. Continuous relaxations
for discrete hamiltonian monte carlo. Advances in Neural Information Processing Systems, 25:
3194-3202, 2012.
12
Published as a conference paper at ICLR 2022
A Proofs
A.1 Proof for Theorem 1
Our path auxiliary sampler is a proposal distribution augmented auxiliary sampler. Before diving
into the proof, we first introduce some background knowledge for auxiliary sampler, based on the
results from page 86 in Liang et al. (2011). Given a proposal distribution T (y|x), we can augment
it by an auxiliary variable u, such that T(y|x) = T1(u|x)T2(y|x, u)du. Using this proposal
distribution, we can define a marginal sampler and an auxiliary sampler.
1.	For a marginal sampler, the accept rate is
Amar(x,6=mm{1, ^) R	(uR^TZu } = mm{1, π≡x∣^}	(14)
π(x) T1 (u|x)T2 (y |x, u)du	π(x)T (y|x)
and for y 6= x, its transition kernel is:
Amar(x, y) = T (y|x)A
mar(x, y)
(15)
2.	For an auxiliary sampler, given x, We first sample auxiliary U 〜Ti(u|x), then We sample the
new state y 〜T2(y∣χ, u). The corresponding accept rate is:
Aaux(x, y, u) = min{1,
Wy)TI(UIy)T2(X|y,U)}
π(x)Tι(u∣x)T2(y∣x, U)
(16)
Since the proposal and the accept rate depend on the auxiliary U, We need to integrate the auxiliary
variable U to obtain the transition from x to y:
Kaux(x, y) =	T1(U|x)T2(y|x,U)A(x, y, U)dU
u
(17)
Though a marginal sampler is Peskun better than an auxiliary sampler (Titsias & Papaspiliopoulos,
2018), a marginal sampler is intractable in most of scenarios, as it requires to integrate over auxiliary
variables U in both proposal and accept rate calculation. Hence, our sampler is implemented in an
auxiliary fashion.
Going back our theorem, the auxiliary variable U is the auxiliary path (σ, L). Condition on L, We
have:
L
Ti(u∣x, L) = Tι(σ∣x) = Y Qo(σi-i,σι)	(18)
l=1
T2(y|x, U, L) = I{x, y are the tWo ends of path U(or sayσ)}	(19)
Given path length L, With a little abuse of notation, the transition kernel for our path auxiliary
sampler K(x, y|L) is:
Tι(σ∣x,L)
ʌ
X	YL Q0(σl-1,σl)
(σ,L)∈Σ(X,N):	l=1
σo = x	、
A(x, y, σlL)
{T2(y|x, σ, L) z	"	" 八	ξ{-l
)k6min (1, n(y)Ql=i Q0。1。-1)如0 = x} ) #
[π(x) Q1=1 Qo(σι-ι, σι)1{σL = y} ʃ |
{^^^^^^^^^^^^^^^^^≡
Tι(σ∣x, L)T2(y∣x, σ, L)A(x, y,σ∣L)dσ
z
|
}
{^^^^^^^^^^^^^^^^^^^^^^^^≡
K(x, y|L) = Rσ Tι(σlx, L)T2(y|x, σ, L)A(x, y,σ∖L)dσ
(20)
When σL 6= y , the accept rate above is not Well-defined. To avoid making extra definition, We
absorb T2, the indicator, as a constraint of the domain in the integration. Then, We have:
Tι(σ∣x,L)
ʌ
A(x, y,σlL)
人
Σ
(σ,L)∈Σ(X,N):
σ0=x,σL=y
z
π(y) QlL=1 Q0(σl, σl-1)
Q0(σi-1,σi)>inΓ,∏(x) QL=1 Qo(σi-i,σι)
)#
(21)
{Z
K(x, y|L) = Rσ Tι(σlx, L)T2(y|x, σ, L)A(x, y, σlL)dσ
With these knoWledge prepared, We begin to proof our theorem 1.
z
}
13
Published as a conference paper at ICLR 2022
Proof. Denote K(x, y) as the probability that x transit to a different state y, then we have:
π(x)K(x, y)
(小 「	IvTLrQ (	Λ ∙ π( π(y) QL=IQo(σι,σι-ι) W
π(X) Ta(L)	工	IlQO (σl-1,σl) minp, , EL Cf---------------------； (\
L	(σ,L)∈Σ(X,N):	l=1	π(x) l=1 Q0(σl-1, σl)
σ0 =x,σL =y
α(L)
L	(σ,L)∈Σ(X,N):
σ0=x,σL=y
Π(x) (YQo(σi-i ,σι))min 卜π(y) Q11 Q。：”]]
l=1	π(x) l=1Q0(σl-1,σl)
min π(x)	Q0(σl-1, σl), π(y)	Q0(σl, σl-1)
l=1	l=1
α(L)
L	(σ,L)∈Σ(X,N):
σ0=x,σL=y
α(L)
L	(σ,L)∈Σ(X,N):
σ0=x,σL=y
∏(y) (Y Q0 …ι))m∣n{ π(y) QL=1 Q0(σι-σ:)，1)j
π π V"^ 小 V"^ π (Tl n ( Λ ∙ Jn(X) QL=I Qo(σι,σι-ι) 1 Y∖
π(y) XX aRJXx N MYQ。(”1)1 " π(y)* Qo(σi-I,σι)用
σ0=y,σL=x
π(y)K(y, X)
(22)
(23)
(24)
(25)
(26)
(27)
(28)
The key idea for the proof is that (σ, L) is symmetric w.r.t. its two ends, when σ is a path from
X to y, σ is also a path from y to σ. Hence, we are able to exchange the orientation of the path in
equation 27.	□
A.2 Proof for Lemma 1
Lemma 1. X is a finite state space with distribution π and a neighborhood function N such that
every state has equal number of states. F is a class of weight functions, such that for any f ∈ F,
1) f : R+ → R+, 2) f (1) = 1, 3) f(t) is monotonically increasing, 4) f(t)f(t)t ≤ 1, ∀t ≤ L
∀f ∈ F, define f(t) = Jf (t)f (t)t, we have
mi* Zf(X) ≤ maχ Zf (y)	(29)
x∈X	y∈X
Proof. Since X is finite, we can always find state X1, X2 having highest and lowest probability,
respectively. Then, we have:
Zf(XI)= ∑ f(咨)≤∣N(xι)l = ∣N(X2)I≤ X f(-∏¾) = Zf(X2)	(30)
πX	πX
z∈N (x1 )	z∈N (x2 )
□
A.3 Discussion for Ideal Function
We define ideal function as a stepping stone to show the advantage of locally balanced function. We
name it ”ideal” as the its properties are ideal assumption for a weight function in PIP. Conditions
1) f : R+ → R+ is a natural requirement for weight function. Condition 2) f(1) = 1 can be
easily realized by substituting f(t) by f (t)/f (1), as PIP only depends on the ratio of weights.
condition 3) f (t) is monotonically increasing, and condition 4) f (t)f (1 )t ≤ 1, ∀t ≤ 1, though, are
technical requirements, it is reasonable for weight functions. Monotonic increasing in 3) indicates
the proposal distribution match the target distribution, where a point z has a higher probability in
target distribution should also has a higher probability in proposal distribution. Condition 4) is
weaker than the following condition: f(t) ≤ t, ∀t ≥ 1, hence condition 4) is easier to satisfy.
14
Published as a conference paper at ICLR 2022
Currently definition of ideal function class already contains most of commonly used weight function,
such as f (t) = ι+t,f(t) = max{1,t}, f (t) = min{1,t}, and f (t) = tα, where α ≥ 0. Also, We
can following properties
Property 2. The logarithm of ideal function is closed in convex combination. That’s to say,
∀f1, ..., fn ∈ F, and ∀λ1, ..., λn, s.t λi ≥ 0 and	in=1 λi = 1, we have
F(t) = ePin=1λilogfi(t) ∈ F	(31)
Proof. Let fi and λi be defined as above. Obviously, F is positive and F(1) = 1, hence satisfies
condition 1) and 2). For condition 3), we have:
F(t)F(1)t = ePn=1 λi log fi(t) ∙ ePn=ι λi log	) ∙ ePn=ι λ log t
=ep2ι λi(log fi(t)+log fi(t )+logt)
=ePn=ι λi log(fi(t)fi(t)t)
Hence, ∀t ≤ 1, we have:
F(t)f (ɪ)t ≤ epn=1 λi0 = 1	(35)
which implies F(t) ∈ F.	□
Property 3. Ideal function is a superset for normalized monotonically increasing locally balanced
function GI := {g ∈ G : g(1) = 1, g is monotonically increasing}.
Proof. ∀g ∈ GI, condition 1), 2) and 3) are obviously satisfied. For condition 4), since g(t) is locally
balanced, we have:
g(t)g(t)t = g2(t) ≤ g2(1) = 1	(36)
implies g ∈ F.	□
(32)
(33)
(34)
Considering the ideal function cover such a large number of functions, we believe the asymptotic
optimality over ideal function strongly suggests locally balanced function is a good choice for path
auxiliary sampler.
A.4 Proof For Lemma 2
The (undirected) conditional independence graph corresponds to Markov random fields:
Definition 2. The conditional independence graph ofX is the undirected graph G = (K, E), where
K = {1, ..., k} and (i, j) ∈/ E if and only if Xi ⊥ Xj |XK\{i,j}.
More details can be found in p.60 Whittaker (2009).
Lemma 2. Consider the state space in Cartesian products X = ×in=1 Xi, where each Xi is a finite
space with M elements, and the neighborhood is defined as 1-Hamming ball. Let dn be the maximum
degree in conditional independence graph. Define Cgn) := suPy∈N3 Zg(y). If1) limn→∞ dn = 0;
2)	the target distribution satisfies ∏(y) ≤ C < ∞, ∀y ∈ N (x) ,then we have:
1	≤ Cgn) ≤ 1 + O( dng(e))) as n → ∞,	∀g ∈F	(37)
To simplify the notation, we assume M = 2 in this proof. It is straightforward to extend the proof
to any finite M .
Proof. On one side, by definition, we can easily see that C(gn) ≥ 1. On the other side, given
x, we use index i represent that we select y ∈ N(x) by flipping index i for x. We de-
note gχ(j) = g(∏(z))∕g(∏(x)) where Z is obtained by flipping index j for x, and we denote
gi(j) = g(∏(z))∕g(∏(y)), where Z is obtained by flipping index j for y. We also denote B(i)
15
Published as a conference paper at ICLR 2022
as the Markov boundary for i, which means given B(i), i is independent with remaining nodes.
Then, we can write
sup sup pn≤⅛)
x∈X i∈[M] j=1 gx (j)
Σj∈B(i) gi(j ) + Σj∈B(i) gij )
Sup sup	C 小_LL	TTTT
x∈X i∈[M]乙j∈B(i) gXj)十 Z^j∈B(i) gXj)
j∈B(i) gi(j)
≤ SuP sup
X∈X i∈[M] j∈B (i) gX(j )
十	Σj∈B(i) gi(j)
Σj∈B(i) gxj ) 十 Σj∈B(i) gxj )
≤ sup sup 1 十
X∈X i∈[M]
Σj∈B(i) gij)
j∈B(i) gX (j) 十 j∈/ B(i) gX (j)
≤ sup sup 1 十
X∈X i∈[M]
dng(C
ng(C1)
dng(C)
一	而Iy
(38)
(39)
(40)
(41)
(42)
(43)
If B(i) is not empty, the first term in equation 40 equals to 1. If B(i) is empty, the first term in
equation 40 does not exist, hence it can still be bounded by 1.	□
Remark: For a fixed ideal function g ∈ F, Lemma 2 shows c(n) converges to 1 ata rate 1 + O( dn)
A.5 Proof for Theorem 2
We prove the theorem by using Lemma 1 and Lemma 2.
Proof. We first show that, for all g ∈ F, the transition probability:
Pg(χ,y) ≥ (^-)UPg(χ,y)	(44)
Cg Cg
where cg is defined in Lemma 2. We temporarily ignore the superscription and will add it back at the
end of the proof. Consider a path (σ, L), We denote t7-,fc = ；(?). Then the probability that X = σ0
transits to y = σL is:
Pg(X, y,σlL) = Y Qg(σι-ι,σι)min [1, T Ql=1 Qg(σl,σlT) ∖
l=1	π(X) l=1 Qg(σl-1, σl)
g(tl-1,l)	g(tl,l-1)
=min 1∏ Zg^, ∏ ti-i，l F)
By definition of Cg , we have:
Pg (χ,y,σlL)
pg(x,y,σ∖L')
≤ CgL
min {QL=1 g(tl-l,l), QL=1 tl-l,lg(tl,l-l)}
maxz∈X(n) ZL(Z)
1 min{QL=1 g(tl-1,l),QL=i tl-1,lg(tl,l-l)}
CL	minZ∈X (n) Zg (Z)
Use the property of g, we have
min	g(tl-1,l),	tl-1,lg(tl,l-1)	≤ t
l=1	l=1
LL
∏ g(tl-1,l)g(tl,l-1)tl-1,l = ∏g(tι-ι,ι)
l=1	l=1
(45)
(46)
(47)
(48)
(49)
≥
16
Published as a conference paper at ICLR 2022
Combining equation 47, equation 48, equation 49, and Lemma 1 we have:
Pg(x, y,σ∣L) ≥ ɪ QL=1 g(t-L7	(50)
CL minZ∈X(n) ZL(Z)
1 minnQlL=1 g(tl-1,l), QlL=1 tl-1,lg(tl,l-1)o
≥ ——L---------------------llγλ-----------(5	(51)
CL	maxz∈X(n) ZL (Z)
≥ ~jL~LPL(X,y,σlL)	(52)
Cg Cg
The inequality holds for arbitrary auxiliary path (σ, L) and the path length L ≤ U, hence we prove
(n)
the first step. Then ∀g∈ F , we use the estimation of Cg in Lemma 2, we have
PL(χ,y) ≥ Cgn)PL(χ,y)
where
C (n) = 1 -O(Udn g(C))
g	ng(专)
(53)
(54)
This indicates g is asymptotically better than g and proves the theorem.	□
Remark: In the proof, equation 54 shows the convergence rate depends on g, this factor prevents
a uniform convergence rate for the ideal function class F . We can obtain a uniform rate when we
constraint in smaller function class, for example, if we restrict to FM := {f ∈ F : f (C)/f (C) ≤
M}, the convergence rate is 1 -O(dn).
A.6 Proof for Property 1
Proof.
A(x, y, σ, L)=min∕1, n(y) Q=1 QS。-1)1
I ∏(X)QL=1 Q(σi-I,σl)j
min
min
1,	YL
l=1
1, (Yl
l=1
π(σl)
QL=1 g( ⅞f )∕Zg(σι)
π(σi-i)) QL=1 g(∏⅛⅛)∕Zg(σi-i)
π(σl) g(“Π(σR ) ∖ Y YY Zg (σl-1)
π(σl-1) g(πg⅛))卅 Zg(σL
min
Zg (χ)]
Zg (y) j
(55)
(56)
(57)
(58)
□
A.7 Proof for Theorem 3
The idea to proof theorem 3 is similar to Grathwohl et al. (2021), which uses the Lipschitz condition
to bound the estimation error. The different is, in PAFS, we use the linearization at each state σl to
propose the next state σl+1, hence to need to accumulate the error at each step to obtain our final
result.
17
Published as a conference paper at ICLR 2022
ɪʌ /` i -	/ τ 、	i` . i	1 .i	.∙	.∙	J- / ∖ J- /	∖ /`	_ * r/	∖
Proof. For any path (σ, L), we first bound the estimation error f0(z) - f0(σl-1), for z ∈ N(σl-1).
Since f is M -smooth, we have:
f(z) - f(σl-1)
≤gf(σi-i),z - σi-ii + K2-∣∣z - σi-i∣∣2
= hVf (σo), Z - σi-ii + hVf (σi-i) - Vf (σo), z - σi-ii + ɪ∣∣z - σi-ik2
≤hVf (σo),z - σi-ιi + K(I - 2)
Similarly, we also have:
f (Z)- f(σl-1) ≥ hVf (σ0),z - σl-1i + K(I + J
With this estimation, we have:
〜	e-2(▽/9O),σι-σι-ιi
Q0(σlT ,σl) = P W	、e- 2 Ef(σo),σι-σi-l'i
z∈N (σl-1) e
e- 1 [f (σι)-f (σι-ι)-K(l- 1)]
—P	e- 1 [f (Z)-"σι-1)+K(I+1)]
z∈N (σl-1) e 2	2
= Q(σl-1, σl)e-Kl
Similarly, we can also obtain
<QL(σι,σi-i) ≥ Q(σι,σi-i)eK(Lf
Now, consider the transition kernel
L
PP(x,y) = Ea(L)	E	UQ o(σi-i,σι)min
(σ,L)∈Σ(X,N): l=1
σ0 =x,σL =y
J1 π(y) QL=I QL(σl ,σl-1) ]
1 π(x) QL=I Qθ(σl-1,σl) /
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
(67)
(68)
α(L)
L	(σ,L)∈Σ(X,N):
σ0 =x,σL =y
min Y Y Q0(σl-1, σl),Y QL(σl,σl-1)]
l=1	π(x) l=1
(69)
≥ α(L)
L
X min 1口 Q(σl-1,σl)e-κl, ∏(χ) Y Q(σl, σl-1)e-KlLTl
(σ,L)∈Σ(X,N):	l=1	π x l=1
σ0 =x,σL =y
(70)
≥	α(L)
(σ,L)∈Σ(X,N):
σ0 =x,σL =y
e-KL(L+1) min (^∏Q(σi-i,σι), n|) YQ(σι,σ-ι)}
(71)
L
L
)
U(U +1)
≥ e-κ ―2 — P(x,y)
(72)
□
Remark: Though the theorem proved above is very loose, it provides a framework to prove ap-
proximation bound for path auxiliary sampler via single step approximation in equation 66. More
sophisticated bounds can be obtained by improving the single step estimation in equation 66 via the
property of the conditional independence. Specifically, a local modifying of the state usually does
not influence most of the indices, which for example can be characterized by the Markov boundary.
Then, with high probability, the auxiliary path can avoid manipulating correlated indices and hence
result in the approximation error in equation 66 be controlled by a constant, rather than e-Kl . For
example, in Ising model, an auxiliary path has high probability to avoid manipulating two adjacent
nodes, and in this case, the linearization is accurate and we loose nothing in approximation. In deep
EBMs, the conditional independence is usually not available, but we can still expect low correla-
tion between most of dimensions. Such analysis relies on more accurate assumption of conditional
independence and we will leave it to our future work.
18
Published as a conference paper at ICLR 2022
Figure 9: PAS and PAFS in different lengths on 50 X 50 Ising Model
Figure 10: PAS and PAFS in different lengths on 100 × 100 Ising Model
ESS in terms of Energy Evaluations
8
B Details for Experiments
B.1	Sampling on Ising
Following Zanella (2020), We set the interaction term λ = 1, and We set αi = μ + Zi if i in the
center of the square lattice, and αi = -μ + Zi otherwise, where μ = 2 and Zi 〜Unif(-3,3). We
generate the lattice Ising model in four sizes: P = 50,100,150, 200. We first run PAS and PAFS
with path lengh L = 1, 2,3,5,10 on Ising model with size P = 50,100,150, 200. For each path
length and each model size, we run 100 chains 100,000 steps and compute the ESS using the last
50,000 steps. The results for P = 200 is given in figure 3. We give the results for P = 50,100,150
in the following. From figure 9, figure 10, figure 11, and figure 3, we can see that the difference
between PAS and PAFS in terms of MCMC steps is decreasing when the model size is increasing.
The reason is that when the model becomes larger, most of the nodes are conditionally independent.
When the auxiliary path does not involve two adjacent nodes, the approximation has zero error. This
observation provides a possibility to further improve the approximation bound in Theorem 3. We
will study this in our future work.
For comparing with other samplers, we report the results for each problem sizeP = 50,100,150, 200
in figure 12, figure 13, figure 14, figure 4. Our path auxiliary samplers substantially outperform other
competitors.
B.2	SAMPLING ON FHMM
We compare PAS and PAFS with different path length on FHMM. We choose parameters K = 10,
P(xn,ι = 1) = 0.05, P(Xn,k = Xn,k-ι) = 0.85, W ∈ GaUSSian(0,Ik), b ∈ Gaussian(0,1),
and σ2 = 0.25. We simulate PAS, PAFS with path length L = 1, 2,3,5,10 on FHMM with
N = 500,1000,1500,2000. For each configuration, we run 100 chains and report the burn in
period, as well as the ESS in terms of MCMC steps and energy function evaluations.
10»
ESS in terms of Energy Evaluations
O 2	4 β 8
MCMCSamplers
Figure 11: PAS and PAFS in different lengths on 150 × 150 Ising Model
19
Published as a conference paper at ICLR 2022
ESS in terms of Energy Function Evaluations
Figure 12: Sampling on 50 × 50 Ising model
ESS in terms of MCMC steps
0	2	4	6	8
Figure 13: Sampling on 100 × 100 Ising model
We can observe that: 1) in terms of MCMC steps, PAFS has very similar burn in time in energy
compared to PAS, while the the gap in ESS is larger than the gap in Ising. The reason is FHMM
is not as close to linear model as Ising and PAFS has larger estimation error. 2) in terms of energy
function evaluations, PAFS still leads the performance as other models. PAS only obtains faster
mixing, and no improvements in ESS. The results show that whether PAS can help depends on the
property of the target distribution.
B.3	Sampling on RBM
We compare GWG, PAFS, and NB with different path lengths on RBM trained on MNIST dataset.
GWG (Grathwohl et al., 2021) samples all indices to manipulate based on the linearization at starting
state of the path σ0 , and we classify it as sampling with replacement. Our PAFS samples the indices
based on the linearization at current state σl and we classify it as sampling with soft no replacement.
Specifically, consider a binary case, when we flip index i at state σl, then in the remaining of the path,
the energy change for index i will be reversed. Hence, if we flip an index and reduce the system’s
energy, we will have small probability flip it back. A third type sampler is No Backtrack (NB)
sampler. It is also a path auxiliary sampler and it rules out the indices have been selected to assure
ESS in terms of Energy Function Evaluations
0
2	4	6	8
Figure 14: Sampling on 150 × 150 Ising model
ESS in terms of MCMC steps
0	2	4	6	8
20
Published as a conference paper at ICLR 2022
BurnIn in terms of MCMC steps	Reconstruction Error in terms of MCMC steps	BurnIn in terms of Energy Evaluations	Reconstruction Error in terms of Energy Evaluations
Figure 15: PAS and PAFS in different path lengths on FHMM N=500
Figure 16: PAS and PAFS in different path lengths on FHMM N=1000
it is sampling with hard no replacement. Specifically, NB first calculate the probability for indices
based on Vf (σ0) as GWG. Then, once selecting an index i at σι, NB manipulates the probability for
selecting index i in the remaining path. For each sampler with path length L = 1, ..., 20, we run 100
chains and report the MMD (Gretton et al., 2012) w.r.t. a set of “ground truth” samples generated
by structure known Block-Gibbs sampler. We can see that, when increasing the path length, PAFS
has steady fast mixing, GWG decreases the efficiency in mixing, and NB fails in mixing. For this
reason, we only compare the statistics for GWG and PAFS in our main text in figure 6.
B.4	Running Time in Learning Ising
We follow the experiments in Grathwohl et al. (2021). GWG runs faster 1.4x than our PAFS-5 as
the energy function computation is cheap in this experiment. To obtain a fair comparison, we add
GWG-equivalent-length (GWG-eq), which we allow GWG-1 to run 1.4x more sampling steps such
that its running time is the same as our PAFS. When our PAFS runs 5, 10, 25, 50 steps, GWG-eq
runs 6, 13 34, 69 steps. Hence, when we draw figure 8, the point for GWG-eq at step 5, 10, 25, 50
using the RMSE obtained at setp 6, 13, 34, 69.
B.5	Details in Learning Deep EBMs
DataSet	Static MNIST	Omniglot	Caltech
PAFS-3	0.685	0.885	0.893
PAFS-5	0.746	0.980	0.962
PAFS-7	0.775	1.030	1.010
GWG	0.877	1.130	1.140
Table 2: Run Time for One Batch for different discrete models.
Figure 17: PAS and PAFS in different path lengths on FHMM N=1500
21
Published as a conference paper at ICLR 2022
Bumln in terms Of MCMC steps
5000
0
-5000
-1∞∞
----pas-ɪ
pafs-ɪ
----pas-2
----pafs-2
----pas-3
----pafs-3
----pas-5
-pafs-5
----pas-10
pafs-ɪθ
O 500 IOOO 1500 2∞0 2500 3000 3500 40∞
MCMC Steps
Reconstruction Error in terms Of MCMC steps
O 500 IOOO 1500
—pas-ɪ
—pafs-ɪ
—pas-2
---pafs-2
---pas-3
---pafs-3
---pas-5
---pafs-5
k— pas-10
pafs-ɪθ
2000 2500 3000 3500 4000
MCMC Samplers
Reconstruction Error in terms of Energy Evaluations
----pas-ɪ
----pafs-ɪ
----pas-2
----pafs-2
----pas-3
----pafs-3
----pas-5
----pafs-5
£- pas-10
pafs-ɪθ
口
O l∞0 20∞ 30∞ 40∞ 50∞ 6000 7000 80∞
MCMC Samplers
Figure 18:	PAS and PAFS in different path lengths on FHMM N=2000
ESS in terms of MCMC steps on L=IQQQ
o
pas-1
pafs-1
pas-2
■ pafs-2
pas-3
pafs-3
pas-5
pafs-5
pas-10
pafs-10
ESS in terms of Energy Evaluations on L=IOOO
pas-1
pafs-1
pas-2
pafs-2
pas-3 I
pals-3 I
pas∙5 I
pafs-5 I
pas-10 I
1,1
πππι
ESS in terms Of MCMC StePS On L=I500
ESS in terms of Energy Evaluations on L= 1500
ioɪ-
β×ιoo
4×10°
3×100
ESS in terms of MCMC steps on L=2QQQ
ESS in terms of Energy Evaluations on L=2000

4
6
8
Figure 19:	ESSfor PAS and PAFS in different Path lengths on FHMM
We use the same setting as GrathWohl et al. (2021), including the batch size, number of iterations,
the PCD hyper parameters, etc. In principle, both PAFS and GWG requires two energy function
evaluations per MCMC step, despite that PAFS explores much larger neighborhood space via path
auxiliary. As the energy function evaluation and gradient calculation usually dominates the compu-
tation, we expect these two should have similar runtime.
In practice, we report the average running time per batch of 100 chains in table 2. We can see for
deep EBMs, our PAFS with different path lengths run slightly faster than GWG. Given that both of
the methods implemented using Pytorch, the minor speed up might be due to the implementation
issue. So overall we think it is fair to say the two methods run equally fast in most cases. For
this reason, we use the same number of steps when we train the deep EBMs. As is reported in
Grathwohl et al. (2021), we the same number of steps (40) to train the binary deep EBMs. For
our method, we also tune the expected path length in {3, 5, 7}, and report the result with the best
validation likelihood.
Figure 20: GWG Burn In
Figure 21: PAFS Burn In
Figure 22: NB Burn In
22