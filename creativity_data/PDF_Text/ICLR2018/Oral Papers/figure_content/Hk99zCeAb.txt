Figure 1: Our training starts with both the generator (G) and discriminator (D) having a low spa-tial resolution of 4×4 pixels. As the training advances, we incrementally add layers to G and D,thus increasing the spatial resolution of the generated images. An existing layers remain trainablethroughout the process. Here N × N refers to convolutional layers operating on N X N spatialresolution. This allows stable synthesis in high resolutions and also speeds UP training considerably.
Figure 2: When doubling the resolution of the generator (G) and discriminator (D) we fade in thenew layers smoothly. This example illustrates the transition from 16 × 16 images (a) to 32 × 32images (c). During the transition (b) We treat the layers that operate on the higher resolution like aresidual block, whose weight α increases linearly from 0 to 1. Here ∣ 2× ∣ and ∣ 0.5× ∣ refer to doublingand halving the image resolution using nearest neighbor filtering and average pooling, respectively.
Figure 3:	(a) - (g) CELEBA examples corresponding to rows in Table 1. These are intentionallynon-Converged. (h) Our Converged result. NotiCe that some images show aliasing and some are notsharp - this is a flaw of the dataset, which the model learns to replicate faithfully.
Figure 4:	Effect of progressive growing on training speed and convergence. The timings weremeasured on a single-GPU setup using NVIDIA Tesla P100. (a) Statistical similarity with respectto wall clock time for Gulrajani et al. (2017) using CELEBA at 128 × 128 resolution. Each graphrepresents sliced Wasserstein distance on one level of the Laplacian pyramid, and the vertical lineindicates the point where we stop the training in Table 1. (b) Same graph with progressive growingenabled. The dashed vertical lines indicate points where we double the resolution of G and D. (c)Effect of progressive growing on the raw training speed in 1024 × 1024 resolution.
Figure 5: 1024 × 1024 images generated using the CELEBA-HQ dataset. See Appendix F for alarger set of results, and the accompanying video for latent space interpolations.
Figure 6: Visual quality comparison in LSUN bedroom; pictures copied from the cited articles.
Figure 7: Selection of 256 × 256 images generated from different LSUN categories.
Figure 8: Creating the CelebA-HQ dataset. We start with a JPEG image (a) from the CelebA in-the-wild dataset. We improve the visual quality (b,top) through JPEG artifact removal (b,middle) and4x super-resolution (b,bottom). We then extend the image through mirror padding (c) and Gaussianfiltering (d) to produce a visually pleasing depth-of-field effect. Finally, we use the facial landmarklocations to select an appropriate crop region (e) and perform high-quality resampling to obtain thefinal image at 1024 × 1024 resolution (f).
Figure 9: CIFAR10 images generated using a network that was trained unsupervised (no label con-ditioning), and achieves a record 8.80 inception score.
Figure 10: Top: Our CelebA-HQ results. Next five rows: Nearest neighbors found from the train-ing data, based on feature-space distance. We used activations from five VGG layers, as suggestedby Chen & Koltun (2017). Only the crop highlighted in bottom right image was used for comparisonin order to exclude image background and focus the search on matching facial features.
Figure 11: Additional 1024× 1024 images generated using the CELEBA-HQ dataset. Sliced Wasser-stein Distance (SWD) ×103 for levels 1024, ..., 16: 7.48, 7.24, 6.08, 3.51, 3.55, 3.02, 7.22, forwhich the average is 5.44. FreChet Inception Distance (FID) computed from 50K images was 7.30.
Figure 13: Example images generated at 256 × 256 from LSUN categories. Sliced WassersteinDistance (SWD) ×103 is given for levels 256, 128, 64, 32 and 16, and the average is bolded. Wealso quote the Frechet Inception Distance (FID) computed from 50K images.
