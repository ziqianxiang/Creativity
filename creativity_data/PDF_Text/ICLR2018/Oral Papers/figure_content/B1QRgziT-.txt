Figure 1:	Inception scores on CIFAR-10 and STL-10 with different methods and hyperparameters(higher is better).
Figure 2:	FIDs on CIFAR-10 and STL-10 with different methods and hyperparameters (lower isbetter).
Figure 3: Squared singular values of weight matrices trained with different methods: Weight clip-ping (WC), Weight Normalization (WN) and Spectral Normalization (SN). We scaled the singularvalues so that the largest singular values is equal to 1. For WN and SN, we calculated singular valuesof the normalized weight matrices.
Figure 4: The effect on the performance on STL-10 induced by the change of the feature mapdimension of the final layer. The width of the highlighted region represents standard deviation of theresults over multiple seeds of weight initialization. The orthonormal regularization does not performwell with large feature map dimension, possibly because of its design that forces the discriminatorto use all dimensions including the ones that are unnecessary. For the setting of the optimizers’hyper-parameters, We used the setting C, which was optimal for “orthonormal regularization”Figure 5: Learning curves for conditional image generation in terms of Inception score for SN-GANs and GANs with orthonormal regularization on ImageNet.
Figure 5: Learning curves for conditional image generation in terms of Inception score for SN-GANs and GANs with orthonormal regularization on ImageNet.
Figure 6: Generated images on different methods: WGAN-GP, weight normalization, and spectralnormalization on CIFAR-10 and STL-10.
Figure 7: 128x128 pixel images generated by SN-GANs trained on ILSVRC2012 dataset. Theinception score is 21.1±.35.
Figure 8:	Res-Block architecture.
Figure 9: Spectral norms of all seven convolutional layers in the standard CNN during course of thetraining on CIFAR 10.
Figure 10: Computational time for 100 updates. We set ndis = 5C.3 THE EFFECT OF ndis ON SPECTRAL NORMALIZATION AND WEIGHT NORMALIZATIONFigure 11 shows the effect of ndis on the performance of weight normalization and spectral normal-ization. All results shown in Figure 11 follows setting D, except for the value of ndis. For WN,the performance deteriorates with larger ndis, which amounts to computing minimax with betteraccuracy. Our SN does not suffer from this unintended effect.
Figure 11: The effect of ndis on spectral normalization and weight normalization. The shadedregion represents the variance of the result over different seeds.
Figure 12: Generated images with GAN-GP, Layer Norm and Batch Norm on CIFAR-1020Published as a conference paper at ICLR 2018C.5 Image generation on Imagenet(a) Unconditional GANsd.o5 0 5 Oo2 2 11φbωω uo⅛θou-1.0	2.0	3.0	4.04.5iteration 1e5(b) Conditional GANs with projection discriminatorFigure 13: Learning curves in terms of Inception score for SN-GANs and GANs with orthonormalregularization on ImageNet. The figure (a) shows the results for the standard (unconditional) GANs,and the figure (b) shows the results for the conditional GANs trained with projection discrimina-tor (Miyato & Koyama, 2018)D Spectral Normalization vs Other Regularization TechniquesThis section is dedicated to the comparative study of spectral normalization and other regularizationmethods for discriminators. In particular, we will show that contemporary regularizations includingweight normalization and weight clipping implicitly impose constraints on weight matrices that
Figure 13: Learning curves in terms of Inception score for SN-GANs and GANs with orthonormalregularization on ImageNet. The figure (a) shows the results for the standard (unconditional) GANs,and the figure (b) shows the results for the conditional GANs trained with projection discrimina-tor (Miyato & Koyama, 2018)D Spectral Normalization vs Other Regularization TechniquesThis section is dedicated to the comparative study of spectral normalization and other regularizationmethods for discriminators. In particular, we will show that contemporary regularizations includingweight normalization and weight clipping implicitly impose constraints on weight matrices thatplaces unnecessary restriction on the search space of the discriminator. More specifically, we willshow that weight normalization and weight clipping unwittingly favor low-rank weight matrices.
Figure 15: Learning curves of (a) critic loss and (b) inception score on different reparametrizationmethod on CIFAR-10 ; weight normalization (WGAN-GP w/ WN), spectral normalization (WGAN-GP w/ SN), and parametrization free (WGAN-GP).
