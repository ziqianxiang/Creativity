Figure 1: The goal-conditioned skill policy (GSP) takes as input the current and goal observationsand outputs an action sequence that would lead to that goal. We compare the performance of thefollowing GSP models: (a) Simple inverse model; (b) Mutli-step GSP with previous action history;(c) Mutli-step GSP with previous action history and a forward model as regularizer, but no forwardconsistency; (d) Mutli-step GSP with forward consistency loss proposed in this work.
Figure 2: Qualitative visualization of results for rope manipulation task using Baxter robot. (a) Ourrobotics system setup. (b) The sequence of human demonstration images provided by the humanduring inference for the task of knot-tying (top row), and the sequences of observation states reachedby the robot while imitating the given demonstration (bottom rows). (c) The sequence of humandemonstration images and the ones reached by the robot for the task of manipulating rope into 'S’shape. Our agent is able to successfully imitate the demonstration.
Figure 3: GSP trained using forward consistency loss significantly outperforms the baselines at thetask of ⑶ manipulating rope into 'S’ ShaPe as measured by TPS-RPM error and ⑹ knot-tyingwhere We report success rate with bootstrap standard deviation.
Figure 4: Visualization of the TurtleBot trajectory to reach a goal image (right) from the initial image(top-left). Since the initial and goal image have no overlap, the robot first explores the environmentby turning in place. Once it detects overlap between its current image and goal image (i.e. step 42onward), it moves towards the goal. Note that We did not explicitly train the robot to explore andsuch exploratory behavior naturally emerged from the self-supervised learning.
Figure 5: The performance of TUrtleBot at following a visual demonstration given as a sequence ofimages (top row). The TurtleBot is positioned in a manner such that the first image in demonstrationhas no overlap with its current observation. Even under this condition the robot is able to move closeto the first demo image (shown as Robot WayPoint-1) and then follow the provided demonstrationuntil the end. This also exemplifies a failure case for classical methods; there are no possible key-point matches between WayPoint-1 and WayPoint-2, and the initial observation is even farther fromWayPoint-1.
