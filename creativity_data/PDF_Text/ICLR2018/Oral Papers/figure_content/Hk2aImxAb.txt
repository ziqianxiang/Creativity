Figure 1: Two images containinga horse. The left image is canon-ical and easy to detect even witha small model, whereas the rightimage requires a computational-ly more expensive network archi-tecture. (Copyright Pixel Addictand Doyle (CC BY-ND 2.0).)much smaller models would also obtain decent error, very large,computationally intensive models seem necessary to correctly clas-sify the hard examples that make up the bulk of the remaining mis-classifications of modern algorithms. To illustrate this point, Fig-ure 1 shows two images of horses. The left image depicts a horsein canonical pose and is easy to classify, whereas the right image istaken from a rare viewpoint and is likely in the tail of the data dis-tribution. Computationally intensive models are needed to classifysuch tail examples correctly, but are wasteful when applied to canonical images such as the left one.
Figure 2: Illustration of the first four layers of an MSDNet with three scales. The horizontal direction cor-responds to the layer direction (depth) of the network. The vertical direction corresponds to the scale of thefeature maps. Horizontal arrows indicate a regular convolution operation, whereas diagonal and vertical arrowsindicate a strided convolution operation. Classifiers only operate on feature maps at the coarsest scale. Connec-tions across more than one layer are not drawn explicitly: they are implicit through recursive concatenations.
Figure 3: Relative accuracy of the intermediate classifier (left) and the final classifier (right) when introducinga single intermediate classifier at different layers in a ResNet, DenseNet and MSDNet. All experiments wereperformed on the CIFAR-100 dataset. Higher is better.
Figure 4: The output x' of layer ' at the sth scale in a MSDNet. Herein, [... ] denotes the concatenationoperator, h'(∙) a regular convolution transformation, and hss (∙) a strided convolutional. Note that the outputsof hsS and h` have the same feature map size; their outputs are concatenated along the channel dimension.
Figure 5: Accuracy (top-1) of anytime prediction models as a function of computational budget on the ImageNet(left) and CIFAR-100 (right) datasets. Higher is better.
Figure 7: Accuracy (top-1) of budgeted batch classification models as a function of average computationalbudget per image the on ImageNet (left) and CIFAR-100 (right) datasets. Higher is better.
Figure 6: Sampled images from the ImageNet classesRed wine and Volcano. Top row: images exited fromthe first classifier of a MSDNet with correct predic-tion; Bottom row: images failed to be correctly clas-sified at the first classifier but were correctly predict-ed and exited at the last layer.
Figure 8: Test accuracy of DenseNet* on CIFAR-100 under the anytime learning setting (left) and the budgetedbatch setting (right).
Figure 9: Illustration of an MSDNet with network reduction. The network has S = 3 scales, and it is dividedinto three blocks, which maintain a decreasing number of scales. A transition layer is placed between twocontiguous blocks.
Figure 10: Ablation study (on CIFAR-100) of MS-DNets that shows the effect of dense connectivi-ty, multi-scale features, and intermediate classifiers.
Figure 11: Classification accuracies on the CIFAR-10 dataset in the anytime-prediction setting (left) and the89Batch computational learning on CIFAR-10(δξ) Aɔmnɔɔ010.5	1.0	1.5	2.0average budget (in MUL-ADD)DenseNet-88ResNet-110MSDNet with early-exitsResNetMC with early-exitsDenseNetMC with early-exitsResNets (He et al., 2015)DenseNets (Huang et al., 2016)Stochastic Depth-110 (Huang et al., 2016)WideResNet-40 (Zagoruyko et al., 2016)0.02.5×108budgeted batch setting (right).
