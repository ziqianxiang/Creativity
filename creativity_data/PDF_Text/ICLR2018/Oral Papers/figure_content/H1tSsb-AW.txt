Figure 1: Comparison between value function baseline and action-conditioned baseline on variouscontinuous control tasks. Action-dependent baseline performs consistently better across all the tasks.
Figure 2: Variants of the action-dependent baseline that use: (i) sampling from the Q-function toestimate the conditional expectation; (ii) Using the mean action to form a linear approximation tothe conditional expectation. We find that both variants perform comparably, with the latter beingmore computationally efficient.
Figure 3: Experiments with additional information in the baseline.
Figure 4: We study the influence of λ in GAE which allows us to trade off bias and variance asdesired. High bias gradient corresponding to smaller values of λ do not make progress after a while.
Figure 5: Shown is the learning curve for a synthetic high-dimensional target matching task (5seeds), for 12 to 2000 dimensional action spaces. At high dimensions, the linear feature action-dependent baseline provides notable and consistent variance reduction, as compared to a linear fea-ture state baseline. For 100, 400, and 2000 dimensions, our method converges 10% faster to theoptimal solution.
Figure 6: Reference training curves for an early and invalidated version of the synthetic high-dimensional target matching task (3 seeds).
