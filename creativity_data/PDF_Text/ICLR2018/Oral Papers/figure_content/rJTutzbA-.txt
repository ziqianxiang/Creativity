Figure 1: Plot of 1/rate (refer equation (1)) vs condition number (Îº) for various methods for thelinear regression problem. Discrete distribution in the left, Gaussian to the right.
Figure 2: Training loss (left) and test loss (right) while training deep autoencoder for mnist withminibatch size 8. Clearly, ASGD matches performance of NAG and outperforms SGD on the testdata. HB also outperforms SGD.
Figure 3: Training loss (left) and test loss (right) while training deep autoencoder for mnist withminibatch size 1. Interestingly, SGD, HB and NAG, all decrease the loss at a similar rate,while ASGD decays at a faster rate.
Figure 4: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function valuefor batch size 8 (right) for SGD, HB and NAG.
Figure 5: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function valuefor batch size 8 (right) for ASGD compared to HB. In the above plots, both ASGD and ASGD-Hb-Params refer to ASGD run with the learning rate and decay schedule of HB. ASGD-Fully-Optimizedrefers to ASGD where learning rate and decay schedule were also selected by grid search.
Figure 6: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function valuefor batch size 8 (right) for ASGD compared to NAG. In the above plots, ASGD was run with thelearning rate and decay schedule of NAG. Other parameters were selected by grid search.
Figure 7: Expected rate of error decay (equation 1) vs condition number for various methods for thelinear regression problem. Left is for discrete distribution and right is for Gaussian distribution.
Figure 8: Training loss for batch sizes 128 and 8 respectively for SGD, HB and NAG.
Figure 9: Training function value for ASGD compared to HB for batch sizes 128 and 8 respectively.
Figure 10: Training function value for ASGD compared to NAG for batch size 128 and 8 respec-tively.
