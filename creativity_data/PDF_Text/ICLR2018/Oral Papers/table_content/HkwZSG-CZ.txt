Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtainedfrom Merity et al. (2017) and Krause et al. (2017). f indicates using dynamic evaluation.
Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) andKrause et al. (2017). f indicates using dynamic evaluation.
Table 3: Perplexity comparison on 1B word dataset. Train perplexity is the average of the last 4,000 updates.
Table 4: Evaluation scores on Switchboard.
Table 5: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.
Table 6: Rank comparison on PTB.		To ensure com-	20	9981	56.17parable model sizes, the embedding sizes of Softmax,					MoC and MoS are 400, 280, 280 respectively. The			Table 7: Empirical rank and test perplexity on		vocabulary size, i.e., M, is 10,000 for all models.			PTB with different number of Softmaxes.		•	Secondly, we show that, before reaching full rank, increasing the number of mixture componentsin MoS also increases the rank of the log-probability matrix, which in turn leads to improvedperformance (lower perplexity). Specifically, on PTB, with other hyper-parameters fixed as usedin section 3.1, we vary the number of mixtures used in MoS and compare the corresponding em-pirical rank and test perplexity without finetuning. Table 7 summarizes the results. This clearpositive correlation between rank and performance strongly supports the our theoretical analysisin section 2. Moreover, note that after reaching almost full rank (i.e., using 15 mixture compo-nents), further increasing the number of components degrades the performance due to overfitting(as we inspected the training and test perplexities).
Table 8: Hyper-parameters used for MoS. V-dropout abbreviates variational dropout (Gal & Ghahramani,2016). See (Merity et al., 2017) for more detailed descriptions.
Table 9: Hyper-parameters used for dynamic evaluation of MoS. See (Krause et al., 2017) for more detaileddescriptions.
Table 10: Hyper-parameters used for Softmax and MoS in experiment on 1B word dataset.
Table 11: Empirical expected pairwise KLD on PTB.
Table 12: BPC comparison on text8. For MoS, “-n” indicates Using n mixtUres. “hid” and “emb” denote thehidden size and embedding size respectively.
Table 13: Training time slowdown compared to Softmax. MoS-K means using K mixture components. “bs”indicates Softmax and MoS use the same batch sizes on one GPU. “best-1” and “best-3” refer to the settingswhere Softmax and MoS obtain their own best perplexity, with 1 and 3 GPUs respectively.
Table 14: Compaison of next-token prediction on Penn Treebank test data. N stands for a number as the resultof preprocessing (Mikolov et al., 2010). The context shown only includes the previous sentence and the currentsentence the prediction step resides in.
