Table 1: ParametersParameter	Use	ValueS	max actor-critic episodes	4000β	critic learning rate	O(1/s)ξ	actor learning rate	O(1/s ln ln s)c	αij scaling factor	1e4	Adam optimizer learning rate for reward	1e-4dR	convergence threshold for reward iteration	1e-4θfinal	learned policy parameter	8.64E Maximum entropy distributionGiven a finite set of trajectories {τi }i, where each trajectory is a sequence of state-action pairsτi = (si1, ai1, . . . , ). Suppose each trajectory τi has an unknown probability pi. The entropy ofthe probability distribution is H = - Pi pi ln(pi). In the continuous case, we write the differentialentropy:Hp(τ ) ln(p(τ))dτwhere p(∙) is the probability density We want to derive. The constraints are:J r(τ)p(τ)dτ = E[r(τ)] = μrp(τ )dτ = 113
