Published as a conference paper at ICLR 2018
Boosting Dilated Convolutional Networks
with Mixed Tensor Decompositions
Nadav Cohen
Institute for Advanced Study
cohennadav@ias.edu
Ronen Tamari
The Hebrew University of Jerusalem
ronent@cs.huji.ac.il
Amnon Shashua
The Hebrew University of Jerusalem
shashua@cs.huji.ac.il
Ab stract
The driving force behind deep networks is their ability to compactly represent rich
classes of functions. The primary notion for formally reasoning about this phe-
nomenon is expressive efficiency, which refers to a situation where one network
must grow unfeasibly large in order to replicate functions of another. To date, ex-
pressive efficiency analyses focused on the architectural feature of depth, showing
that deep networks are representationally superior to shallow ones. In this paper
we study the expressive efficiency brought forth by connectivity, motivated by the
observation that modern networks interconnect their layers in elaborate ways. We
focus on dilated convolutional networks, a family of deep models delivering state
of the art performance in sequence processing tasks. By introducing and analyz-
ing the concept of mixed tensor decompositions, we prove that interconnecting
dilated convolutional networks can lead to expressive efficiency. In particular, we
show that even a single connection between intermediate layers can already lead
to an almost quadratic gap, which in large-scale settings typically makes the dif-
ference between a model that is practical and one that is not. Empirical evaluation
demonstrates how the expressive efficiency of connectivity, similarly to that of
depth, translates into gains in accuracy. This leads us to believe that expressive
efficiency may serve a key role in developing new tools for deep network design.
1	Introduction
One of the key attributes fueling the success of deep learning is the ability of deep networks to
compactly represent rich classes of functions. This phenomenon has drawn considerable attention
from the theoretical machine learning community in recent years. The primary notion for formally
reasoning about the representational abilities of different models is expressive efficiency. Given
two network architectures A and B, with size parameters (typically the width of layers across a
network) rA and rB, we say that architecture A is expressively efficient w.r.t. architecture B if
the following two conditions hold: (i) any function realized by B with size rB can be realized (or
approximated) by A with size rA ∈ O(rB); (ii) there exist functions realized by A with size rA that
cannot be realized (or approximated) by B unless its size meets rp ∈ Ω(f (ra)) for some SUPer-
linear function f . The nature of the function f in condition (ii) determines the type of efficiency
taking place -if f is exponential then architecture A is said to be exponentially expressively efficient
w.r.t. architecture B, and if f is polynomial so is the expressive efficiency of A over B.
To date, works studying expressive efficiency in the context of deep learning (e.g. Delalleau and
Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir
(2015); Poole et al. (2016); Raghu et al. (2016); Cohen et al. (2016b); Cohen and Shashua (2016);
Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth,
showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoreti-
cal focus is motivated by the vast empirical evidence supporting the importance of depth (cf. LeCun
et al. (2015)). However, it largely overlooks an additional architectural feature that in recent years
is proving to have great impact on the performance of deep networks - connectivity. Nearly all state
of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b;a))
1
Published as a conference paper at ICLR 2018
deviate from the simple feed-forward (chain) approach, running layers connected under various
schemes. Whether or not this relates to expressive efficiency remains to be an open question.
A specific family of deep networks gaining increased attention in the deep learning community is
that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den
Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the
art performance in audio and text processing tasks. Dilated convolutional networks are frequently
applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising
non-contiguous filters with a different dilation (distance between neighboring elements). The choice
of dilations directly affects the space of functions that may be realized by a network, and while no
choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks
with different dilations leads to expressive efficiency, and by this demonstrate that connectivity
indeed bears the potential to enhance the expressiveness of deep networks.
Our analysis follows several recent works utilizing tensor decompositions for theoretical studies
of deep learning (e.g. Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular,
builds on the equivalence between hierarchical tensor decompositions and convolutional networks
established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated con-
volutional networks, the choice of dilations throughout a network corresponds to determination of
the mode (dimension) tree underlying the respective decomposition. We then define the notion of a
mixed tensor decomposition, which blends together multiple mode trees, effectively creating a large
ensemble of hybrid trees formed from all possible combinations. Mixed tensor decompositions cor-
respond to mixed dilated convolutional networks, i.e. mixtures formed by connecting intermediate
layers of different dilated convolutional networks. This allows studying the expressive properties of
such mixtures using mathematical machinery from the field of tensor analysis. We fully analyze a
particular case of dilated convolutional arithmetic circuits, showing that a single connection between
intermediate layers already leads to an almost quadratic expressive efficiency, which in large-scale
settings typically makes the difference between a model that is practical and one that is not.
An experiment on TIMIT speech corpus (Garofolo et al. (1993)) evaluates the dilated convolutional
network architectures covered by our analysis. We find that interconnecting intermediate layers of
different networks improves accuracy, with no additional cost in terms of computation or model
capacity. This serves as an indication that with the architectural feature of connectivity, similarly to
the case of depth, expressive efficiency and improved accuracies go hand in hand. Accordingly, we
believe expressive efficiency may serve a key role in developing new tools for deep network design.
2	Summary of Our Analysis and Contributions
For the convenience of the reader, we summarize below the analysis and contributions of this paper.
The summarized material is delivered fully in sec. 3, 4, 5 and the appendices referenced therein.
To keep the manuscript at reasonable length, much of the material is located in the appendices. We
refer the reader to Cohen et al. (2017) for a longer, self-contained version of the text.
Our analysis begins in sec. 3, where we present the dilated convolutional network underlying
WaveNet (fig. 1). We consider this to be the baseline architecture and, following Cohen and Shashua
(2016), facilitate its study through tensor analysis. The key to introducing tensors into the frame-
work is a discretization of the network,s input-output mapping. Namely, f (x[t-N +1],..., x[t]) - a
function realized by the network (t here stands for a natural time index), is conceptually evaluated
on a finite (exponentially large) number of input points, generated from all possible assignments of
the variables x[t-N +1], . . . , x[t] to each hold one of M predetermined values. This gives rise to
an N -dimensional lookup table, with length M in each axis. We refer to this lookup table as a grid
tensor (eq. 1). It is shown (app. C) that grid tensors brought forth by the baseline dilated convolu-
tional network (fig. 1) can be expressed as a hierarchical tensor decomposition, referred to as the
baseline decomposition (eq. 2).
The baseline decomposition implicitly adheres to a particular tree over tensor modes (axes). This
calls for a generalization, and we indeed define a general mode tree (def. 1), followed by a corre-
sponding hierarchical tensor decomposition, referred to as the tree decomposition (eq. 3). Different
choices of mode trees lead to tree decompositions characterizing networks with different dilations.
We focus on the tree that corresponds to the baseline network (fig. 2(a)), and on those corresponding
to networks obtained by swapping dilations of different layers (fig. 2(b), for example).
2
Published as a conference paper at ICLR 2018
output
L-1
hidden
layers
input
Time
h ⑶[t ]产 g《a2, 咒 h(1)[t - 2]), K "I, h(1)[必	OLt ] y = g Ka L ,y,I,h(LT) [ — 2 L R,0 L, y ,II, h(LT)L殖)
∈
size-2 conv:
dilation-2L-1
size-2 conv:
dilation-2
size-2 conv:
dilation-1
t-2L	t-2L+1 t-2L+2 ・・・
•・・	t-3	t-2	t-1	t	t+1
TTTT------------------------- 。
N:=2 time points h1 [t]7 = g(〈a1，%I,x[t-1],(a1，%II,x[t]))
Figure 1: Baseline dilated convolutional network architecture (see description in app. B).
Armed with a framework for representing different dilated convolutional networks through hierar-
chical tensor decompositions of different mode trees, we head on in sec. 4 and introduce the notion
of a mixed tensor decomposition (eq. 4). The mixed decomposition of two mode trees T and T is
based on a preselected set of nodes present in both trees, referred to as mixture nodes. Individual
tree decompositions of T and T are run in parallel, where at each mixture node, tensors from the two
decompositions are swapped. If N and N are the dilated convolutional networks characterized by T
and T (respectively), the mixed decomposition characterizes a mixed (interconnected) network M,
formed by rewiring intermediate layers of N into N, and vice versa (see illustration in fig. 3).
The heart of our analysis is sec. 5, where we study the expressive efficiency of the mixed network M
over the individual networks N and N. Establishing expressive efficiency requires showing that any
function realized by N or N can be realized by M with no more than linear growth in size, whereas
the converse does not hold, i.e. there exist functions realizable by M that cannot be realized by N
or N unless their size is allowed to grow super-linearly. From a tensor decomposition perspective,
this translates to the following two propositions:
(i)	any tensor generated by a tree decomposition of T or T can be realized by their mixed
decomposition with no more than linear growth in size;
(ii)	there exist tensors realizable by the mixed decomposition ofT and T that cannot be realized
by their individual tree decompositions without a super-linear growth in size.
We address both propositions through the notion of hybrid mode trees (def. 2; fig. 4), which are
simply mode trees born from combinations of T and T . We prove (claim 1) that the mixed decom-
position of T and T can replicate, with no more than linear growth in size, the tree decomposition
of any hybrid tree H. Since T and T are in particular hybrid mode trees of themselves, we ob-
tain an affirmative answer to proposition (i). For addressing proposition (ii), we demonstrate a case
(with convolutional arithmetic circuits) where there exists a hybrid tree H whose tree decomposi-
tion generates tensors that require the tree decompositions of T and T to grow super-linearly. Since
the mixed decomposition of T and T can (by claim 1) replicate the tree decomposition of H with
no more than linear growth, proposition (ii) is established, and M is indeed expressively efficient
w.r.t. N and N (corollary 1).
The central tool for establishing proposition (ii), or more specifically, for demonstrating the existence
of a hybrid tree H whose tree decomposition requires those of T and T to grow super-linearly, is a
tight analysis of tensors generated by a tree decomposition in terms of their ranks when arranged as
matrices (theorem 1). Matricization ranks under hierarchical tensor decompositions are of interest
from a pure tensor analysis perspective (cf. Hackbusch (2012)), as well as in the context of deep
learning (cf. Cohen and Shashua (2017)). The bounds we provide are much tighter (exact in many
cases) and far more general than those existing in the literature, and we expect them to prove useful
in different applications. The key idea in deriving these bounds is to consider a matricized form of
the tree decomposition, and recursively propagate outwards various matrices (for details see proof
of theorem 1 in app. E.2).
3
Published as a conference paper at ICLR 2018
(a)
dilation-8
dilation-4
dilation-2
dilation-1
QOQQQaaqqqqaQqq.
ooooooo
0 0
V
O
DOOOOC
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
{1,2,3,4,5,6,7,8}	∣	|{9,10,11,12,13,14,15,16} J
05,6,7,8} J
OOOO
。。
{1,2,3,4}
{7,8}
dilation-8 ± ± I i '
OOOO
dilation-1	厂
.0..0,0
dilation-2
O
O
⅛
O
O
{1,2}
{3,4}
{5,6}
InJ □2L∣ 03L∣ □4L∣ 05L∣ □6L∣ □7L1 向
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
OOOOOOOOOOO
(5,6,7,8,13,14,15,16}
(13,14,15,16}
(b) OOOOOOOOOOOOOOO
dilation-4 斗
Q-O
OOO
(1,2,3,4)
□ C
□。
OOO
OOO
Figure 2: Best viewed in color. Dilated convolutional networks (left) and the mode trees underlying their
respective tensor decompositions (right). (a) Baseline architecture - dilation 2l-1 in layer l. (b) Architecture
obtained by swapping dilations of even and odd layers.
To conclude this section, we list below the main contributions of the paper:
•	We introduce the notion of a mixed tensor decomposition, and prove that it brings forth a rep-
resentational advantage compared to the individual hierarchical decompositions it comprises.
This development is of interest from a pure tensor analysis perspective, independently of con-
volutional networks, or machine learning in general.
•	We provide the first formal evidence for the fact that interconnectivity - an architectural feature
prevalent in state of the art deep learning, brings forth expressive efficiency.
•	Our central theorem (theorem 1) provides the most comprehensive characterization to date of
matricization ranks brought forth by hierarchical tensor decompositions.
3 Dilated Convolutional Networks
Dilated convolutional networks are a family of convolutional networks (LeCun and Bengio (1995))
gaining increased attention in the deep learning community. As opposed to more conventional con-
volutional architectures (e.g. Krizhevsky et al. (2012)), which are applied primarily to images (and
videos), dilated convolutional networks thrive in sequence processing tasks. For example, they un-
derlie Google’s WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016))
models, which provide state of the art performance in audio and text processing tasks.
3.1	Baseline Architecture
The dilated convolutional network architecture considered as baseline in this paper is the one under-
lying WaveNet, depicted in fig. 1. Due to lack of space, we defer its detailed description to app. B,
and merely note here that We use g(∙) to denote the function combining two size-1 convolutions into
a single size-2 convolution with non-linearity (e.g. g(a, b):= max{a + b, 0} for ReLU activation).
Our interest lies on the representational abilities of a network, i.e. on the properties of the input-
output mappings it can realize. For a fixed time point t, o[t] 一 network output at time t, is a function
of x[t-2L+1]... x[t] — network input over the last 2L time points. Taking into account temporal
stationarity, and denoting for brevity N:=2L, we may write o[t]y = fy (x[t-N +1], . . . , x[t]) for
every output coordinate y ∈ [.l]. We study the functions {fy (∙)}y, which obviously depend on the
convolution weights {al,γ,I, al,γ,II}l,γ, through the process of discretization. Namely, we choose a
collection of vectors v(1) . . . v(M), and for each output coordinate y, define the following tensor:
Ayd1...dN := fy(v(d1),..., v(dN)) ∀d1...dN ∈ [M]	(1)
V(I). V(M) are referred to as discretizers, and Ay is referred to as the grid tensor of fy(∙). The
size ofa grid tensor is exponential in N, thus treating it directly is intractable. However, the network
admits a compact parameterization of grid tensors in terms of its convolution weights (see app. C,
and the preliminaries in app. A):
4
Published as a conference paper at ICLR 2018
For j = 1. . .N:
φ0,j,γ = [vγ(1),...,vγ(M)]> ∀γ∈ [r0]
For l = 1. . .L , j = 1. . .N/2l :
φl,j,γ = (Xrl-I a” ∙ φlτ,2jT,α) Zg (Xrl-I aα,Y,II ∙ φlτ,2j,a) ∀γ ∈ [ri]
Ay = φL,1,y ∀y ∈ [rL]	(2)
This parameterization is in fact a hierarchical tensor decomposition. To highlight its correspondence
to the baseline dilated convolutional network (fig. 1), we refer to it as the baseline decomposition.
3.2	Dilations and Mode Trees
The baseline decomposition (eq. 2), corresponding to the baseline dilated convolutional network
(fig. 1), implicitly adheres to a tree structure1 In this subsection we generalize the underlying tree,
and show that the resulting decompositions capture networks with various dilations throughout their
convolutional layers. We begin by defining a general (binary) tree over tensor modes:
Definition 1. Let N ∈ N. A binary mode tree2 over [N] is a full binary tree3 in which:
•	Every node is labeled by a subset of [N]
•	There are exactly N leaves, labeled {1} . . . {N}
•	The label of an interior (non-leaf) node is the union of the labels of its children
If T is a binary mode tree, we identify its nodes with their labels, i.e. with the corresponding subsets
of [N]. The set of all interior nodes is denoted by int(T) ⊂ 2[N]; the children of an interior node
ν ⊂ [N] are denoted by CI(ν; T), CII(ν; T) ⊂ [N]; and the parent of a non-root node ν ⊂ [N] is
denoted by P(ν; T). Notice that by definition, the root node is labeled [N].
Recall the definition of grid tensors {Ay}y (eq. 1), and let T be a binary mode tree over [N].
T induces a hierarchical decomposition of the grid tensors, referred to as its tree decomposition:
For j = 1. . .N :
φ{j},γ = [vγ(1) , . . . , vγ(M)]> ∀γ ∈ [r]
For ν in int(T ) (depth-first order):
φν,γ =	σ(ν;T)	((XrTaa'',I-	ΦCI(ν;T )，a) Zg	(XrTaa，”•	ΦC11(";T )，"))	∀Y ∈	[r]
Ay = φ[N],y ∀y ∈ [r]	(3)
To conserve space We defer the annotation of the tree decomposition to app. D, noting that r∈N - the
number of tensors in each group {φν,γ}γ, is referred to as the size constant of the decomposition.4
Compare the general tree decomposition (eq. 3) to the baseline decomposition (eq. 2). It is not
difficult to see that the latter is a special case of the former, corresponding to a binary mode tree T
that is perfect,5 and Whose depth-l nodes are adjacent sets of size N/2l (fig. 2(a)-right). This implies
that such a mode tree, When plugged into the tree decomposition, provides a characterization of
the baseline dilated convolutional netWork (fig. 1), i.e. a netWork Whose dilation in layer l is 2l-1
(fig. 2(a)-left). If We Were to choose a different mode tree, the corresponding dilated convolutional
netWork Would change.6 For example, ifWe sWap connections in the mode tree (fig. 2(b)-right), We
obtain a decomposition that characterizes a netWork Whose dilations are sWapped (fig. 2(b)-left).
4 Mixed Tensor Decompositions
Let T and T be two binary mode trees over [N] (def. 1). We will now define mixed tensor decompo-
sitions ,blending together the tree decompositions of T and T (eq. 3). A mixed decomposition of T
and T is obtained by choosing a collection of mixture nodes mix(T, T). These are nodes (subsets
of [N]) that reside in the interior of both T and T, defining locations in the tree decompositions at
which tensors will be exchanged. If miχ(T, T) is chosen as the empty set, the mixed decomposition
simply sums the output tensors generated by the tree decompositions of T and T. Otherwise, the tree
decompositions of T and T progress in parallel, until reaching a mixture node μ∈mix(T, T), where
they exchange tensors between them. The process continues until all mixture nodes are visited and
the root node (of both trees) [N] is reached. At this point tensors are summed and returned as output.
The formal definition of the mixed decomposition, annotated in detail in app. D, is as follows:
5
9
A：suopTSodojd
OMI 既U!MonoJ Qψ gmSSQJppB 01 SlUnoUlB STtqi .N Pub N 丁1丛 iuəɪɔɪjjə ApAXSSQJdXQ ST W IBqI
Moqs 01 Q5∏[ pɪno/ʌ əM ∙^y Aq ψθM^u ∣Buoμn∣θΛuoo poiB∣τp poxτuι gmpuodsəuoɔ sjτ ə:ouəp PUB
'HH)x?Ui səpou Qjmxτuι Jo əɔɪoqɔ iB[nopjBd b UK)JJ gupɪnsəj Q7 'bə) uoμτsoduιooop poxτuι oψ
jəpɪsuoɔ ^ɪəʌpɔədsəj JJ puυ JJ səəɪi əpouɪ ψτM (ɛ `bə) uoμτsoduιooop əəj: oψ Aq pəzuəiɔRlBqɔ
əib sgmddBui IndJnOTndU! osoqM SWK)MIəu ∣Buoμn∣θΛuoo ρoq□[!p OMl oq N Pub N 汨1 /'∞s ul SV
SuVNV ADNHIDIHHn HAISSHHdXH g
•£ *⅛ uτ pəiRnSn口！ əɪe HK)Miəu pgχτuι gupɪnsəj oψ PUB 'səpou gjnιxτuι Jo əɔɪoqɔ o∣qτssod y
Z ∙⅛ m pəKRlXK)(I səəj: PUB SWK)MIəu oψ əɪe JJ PUB JJ ijγ ijγ γeψ əsoddns 'ə[duIBXə joj tsjəa əɔu
PUB ijγ uτ jqAb∣ ∣Buoμn∣θΛuoo B Jo Slndlno jγ o：m səiim Ajduiis uotpquuoojqiut UB əjəqm 'pəiɔəu
-uoɔjə:m əɪe N PuB N sψθM^u qoτqM W suoμBθθ∣ oψ səmuiɪə:əp K[əadəəjjə (Lz ∕)ι秋 səpou
Qjmxτuι jo əɔɪoqɔ əm `sjəknɪ oiBTpouuoim jpψ gupɔəuuoɔjə:m puυ，n PUB N J0 豆nd⅛no gψ gmuɪ
-uɪns Aq pəuuoj WWil ↑υuoιjn↑o^Aθ∂ p∂in↑ιp p∂xιιu b Jo guτddBuι IndJno-IndU! oψ səzuə]ɔRiBqɔ
(9'bə) JJ PUB JJ jo uoμτsoduιooop poxτuι əm θ[əA口ɔədsəi) JJ PUB JJ Jo suoμτsoduιooop əəu oψ Aq
pQz∏Qio⅛ιeqo əib sgmddBuι IndlnOTndU! əsoqm s^jom:əu ∣Buδμn∣θΛuoo poiB∣τp oψ oq N PUB N lə"
(。)	[j]ə/1a /[ʌd夕 + 万Dv]"=犷:6
亿/W 乂A Ur∣Φ V-> Ur∣φ 加以S : 8
((QQ ⑷ HQ 夕∙ I”或◎	.Z) % (QQ ⑷ 1。夕♦"啰	mM))(Ι⑷。=Ua	： L
:(ɪəpɪo uoɪsnɪɔuɪ) {po具SIA Xpeojqp jj Ul sapou} ∖ T佑 u (/)?〃？ Ul 〃 ɪoj : ə
同乂A ((江(山)％" . u/管 mZ) 6% (江(Z⑷I. ♦ l/管 mZ)) (Z⑷。=心,	：5
:(ɪəpɪo uoɪsnɪɔuɪ) {po具SIA Xpeojqp jj Ul sapou} ∖ r佑 u (/)?〃？ Ul 〃 ɪoj :什
:(ɪəpɪo uoɪsnɪɔuɪ) {[w]} ∩ (Z 汝 Ul WK)H ： £
Iubisuoo əzts uonτsoduιooop - J [[]BCA ±[(w)a ζ'''ζ([；0] = LqW =心{,}@	: ζ
:N…工=C1。H : T
,(uəəɪ§) ɪə^pɪ ∣BUoμn∣ΘΛUθo oiwpoui同Ul ub Jo suiiiməi PIre UolIeUluInS inʤno qgnoιqι pəuIqUIoɔ əib (ʧəɪ
忆 ?U) LL PUe LL φTM pəibioossb N PUU N sχ∙iθΛvpN 'uopɪsoduɪoɔəp POXlUI UoSOqɔ。1^uɪpuodsəɪɪoɔ 斗ɪoʌvpu
puoμn∣ΘΛUθo p。冲P POXIPM (q) G "bə) uopɪsoduɪoɔəp POXlUI 叫]ɪoj (LL ⑦和〃 səpou aι∏ιxιuι Jo oɔɪOqɔ
a∣qιssod b φiM §uop 'Q 用U JO ]q⅛μ oq] uo UoAIg) LL PUe LL SoO图 əpouɪ omjj (υ) ,ioɪoɔ Ul Po丛OlA OqQL ：£
SIO% mɔl IB iodBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2018
Figure 4: Best viewed in color. (a) Two mode trees T and T along With a possible choice of mixture nodes
(same as in fig. 3(a)). (b) Sample of the resulting hybrid mode trees (def. 2).
Proposition 1. Consider a tree decomposition (eq. 3) with underlying mode tree T or T and size
constant r = rtree. This decomposition can be realized by a mixed decomposition of T and T (eq. 4)
whose size constant r is linear in rtree.
Proposition 2. Consider a mixed decomposition of T and T (eq. 4) with size COnStant r = rmiχ.
This decomposition can generate grid tensors {Ay}y that cannot be generated by tree decomposi-
tions of T or T (eq. 3) unless their size COnStant r is SuPer-Unear in rmix.
As a first step in treating prop. 1 and 2, we define the notion of a hybrid mode tree:
Definition 2. Let T and T be binary mode trees over [N] (def. 1), and let mix(T, T) be a corre-
sponding collection of mixture nodes, i.e. a set of nodes (subsets of[N]) contained in the interior of
both T and T. We say that H is a hybrid mode tree of T and T w.r.t. mix(T, T), if it is a binary
mode tree over [N], whose interior may be generated by the following process:
int(H) = 0
For μ in mix(T, T) ∪ {[N]} (inclusion order):
S = int(T) ∩ 2μ \ {nodes in T already assigned to S}
S = int(T) ∩ 2μ \ {nodes in T already assigned to S}
int(H) = int(H)∪S or int(H) = int(H)∪S
In words, for every μ that is either a mixture node or the root node, int(H) includes a segment from
either int(T) or int(T), where the segment comprises all descendants of μ from which the path to μ
does not cross any other mixture node (see illustration in fig. 4).
Claim 1 below states that with proper weight setting, a mixed decomposition of T and T (eq. 4) with
size constant r=rmix, can realize any tree decomposition (eq. 3) with size constant r=rmix/2, if
the underlying mode tree is a hybrid of T and T. Since T and T are in particular hybrid mode trees
of themselves, we obtain an affirmative answer to prop. 1.
Claim 1 (proof in app. E.1). Let T and T be binary mode trees over [N] (def. 1), and let mix(T, T)
be a corresponding collection of mixture nodes. Consider a mixed decomposition of T and T
w.r.t. mix(T, T) (eq. 4), and denote its size constant r by rmix. Let H be a hybrid mode tree of T
and T w.r.t. mix(T,T) (def. 2), and consider the respective tree decomposition (eq. 3), with size
constant r=rmix /2. For any setting of weights {aν,γ,I, aν,γ,II}ν,γ leading to grid tensors {Ay }y in
this tree decomposition, there exists a setting ofweights {aν,γ,i, aν,γ,ii}ν,γ, {a",γ,I, aν,γ,ii}ν,γ in the
mixed decomposition, independent of discretizers {v(i)}i∈[M], that leads to the same grid tensors.8
Claim 1 not only addresses prop. 1, but also brings forth a strategy for treating prop. 2. The strategy
is to find a hybrid mode tree H distinct enough from T and T such that its tree decomposition, which
according to claim 1 is easily realized by the mixed decomposition, poses a significant challenge for
the individual tree decompositions of T and T. Hereinafter we pursue this line of reasoning, focusing
on the particular case of convolutional arithmetic circuits - g(a, b)=a∙b. We focus on this special
case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same
time corresponding to models showing promising results in practice (see for example Cohen et al.
(2016a); Sharir et al. (2016)).9
To crisply phrase our central theorem, we define the notion of an index set tiled by a mode tree:
Definition 3. Let T be a binary mode tree over [N] (def. 1), and let I ⊂ [N] be a non-empty set
of indexes. A tiling ofI by T is a collection of nodes in the tree, denoted Θ(I; T), which meets
7
Published as a conference paper at ICLR 2018
the following two requirements: (i) ∪v∈θ(I;T) V = I (ii) V∈Θ(I; T)⇒P(ν; T)⊂I. In words,
Θ(I; T) is a set of nodes in T whose disjoint union gives I, where each node is maximal, i.e. its
parent in the tree is not a subset ofI. See illustration in fig. 6 (supplementary material).
Theorem 1 below provides a tight characterization of grid tensors generated by a tree decomposition
in terms of their ranks when matricized (see app. A) w.r.t. an index set. This result is of general
interest from both tensor analysis and deep learning perspectives. We use it to establish prop. 2.
Theorem 1 (proof in app. E.2). Let T be a binary mode tree over [N] (def. 1), and consider the
corresponding tree decomposition (eq. 3) with discretizers v(1) . . . v(M) spanning Rr. Assume
that g(a, b)=a∙b (non-generalized decomposition - see app. A), and suppose the generated grid
tensors {Ay}y arematriCized(Seeapp. A)w.r.t. an index set I⊂[N], 0=Σ=[N], whose complement
we denote by Ic:=[N]\I. Then, the ranks of the grid tensor matricizations {JAy KI}y are:
• no greater than rmm{|®(I;TJJθ(I0;T)|}
at least r∣{(ν1,ν2)∈Θ(I;T)×Θ(Ic;T): νι and ν2 are siblings in T with depth>1}|
almost always, i.e. for
all configurations of weights {aν,γ,I, aν,γ,II}ν,γ buta set of Lebesgue measure zero
Given two mode trees T and T, with a corresponding collection of mixture nodes mix(T, T), the
bounds in theorem 1 can be used to find an index set I and a hybrid mode tree H, such that the tree
decomposition ofH generates grid tensors whose ranks under matricization w.r.t. I are much higher
than those brought forth by the tree decompositions of T and T. This fulfills the strategy described
above, thereby establishing prop. 2. In app. F we demonstrate this process with the exemplar setting
considered throughout the paper (fig. 2, 3, 4). The following corollary is reached:
Corollary 1. Let N be the baseline dilated convolutional network (fig. 1), and let N be a network
obtained by swapping dilations of groups of k layers (the case k=2 is illustrated in fig. 2(b)-left). De-
note by M the mixed network obtained by summing the outputs of N and N, while interconnecting
their k’th intermediate layer (and possibly additional layers). Assume the networks’ convolutional
operator g(∙) is a product. Then, besides a negligible set, all functions realized by M with r chan-
nels in the layers of each interconnected network, cannot be realized by N (or N) if the number of
channels in each of its layers is less than (r/2)2/(1+21-k).
Corollary 1 (along with claim 1) demonstrates that interconnecting intermediate layers of differ-
ent dilated convolutional networks can bring forth expressive efficiency. The lower bound in the
corollary 一 (r/2)2/(1+2	), is essentially quadratic when k ≥ 4. For example, if k = 4 and the
number of channels r in each interconnected network is 128, the lower bound implies that in order
to maintain representational abilities with an individual network, over 1500 channels in each layer
are required - far beyond acceptable practice in deep learning.
6	Experiment
To assess the practical implications of the expressive efficiency brought forth by mixing dilated
convolutional networks, a simple experiment was conducted. We trained a baseline dilated convo-
lutional network N (dilation 2l-1 in layer l - see sec. 3.1) with architectural parameters similar to
those used in WaveNet (van den Oord et al. (2016)), to classify individual phonemes in the TIMIT
acoustic speech corpus (Garofolo et al. (1993)). In addition to the baseline model, we also trained
a companion network N obtained by swapping dilations of even and odd layers. The mode trees
corresponding to these networks (illustrated in fig. 2) - T and T, share interior nodes of even depth,
thus any subset of those nodes may serve as mixture nodes for a mixed decomposition (eq. 4). We
evaluate mixed dilated convolutional networks M corresponding to different choices of mixture
nodes (see fig. 3 for illustration of a particular case). Specifically, we consider choices of the fol-
lowing form: mix(T, T) := {ν ∈ int(T')∩int(T') : depth of V (in T and T) ≥ threshold}. Varying
the threshold yields mixed networks with a varying number of interconnections. In the extreme
case miχ(T, T) = 0 (high threshold), M simply sums the outputs of N and N. As the threshold
decreases interconnections between hidden layers are added - starting from hidden layer 2, then
including hidden layer 4, and so on. The intuition from our analysis (sec. 5) is that additional in-
terconnections result in a larger ensemble of hybrid mode trees, which in turn boosts the expressive
power of the mixed network M. As fig. 5 shows, this intuition indeed complies with the results in
practice - classification accuracy improves as we increase the number of interconnections, without
any additional cost in terms of computation or model capacity.6 * * * 10
8
Published as a conference paper at ICLR 2018
Connections up to layer
Figure 5: Experimental results - increasing the number of interconnections between hidden layers of different
dilated convolutional networks improves accuracy, with no additional cost in computation or model capacity.
It is important to stress that our objective in the experiment was to evaluate, in the most controlled
setting possible, the exact models covered by our analysis. We did not compare to state of the art
results, as all phoneme recognition rates reported in the literature deviate from our basic setting -
they heavily rely on data pre-processing (e.g. Mel-Frequency Cepstral Coefficients), prediction post-
processing (e.g. Conditional Random Fields), or both. The recent DeepLab model (Chen et al.
(2016)) has demonstrated that when combined with other techniques, mixing dilated convolutions
can lead to state of the art image segmentation performance. We are currently pursuing similar
results in the context of sequence processing tasks.
To conclude this section, we briefly convey implementation details behind the experiment. TIMIT
dataset is an acoustic-phonetic corpus comprising 6300 sentences manually labeled at the phoneme
level. We split the data into train and validation sets in accordance with Halberstadt (1998), and
as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 plus an ad-
ditional “garbage” label. The task was then to classify individual phonemes into one of the latter
categories. In accordance with WaveNet, the baseline dilated convolutional network had ReLU ac-
tivation (g(a, b)= max{a+b, 0} - see sec. 3.1),11 32 channels per layer, and input vectors of dimen-
sion 256 holding one-hot quantizations of the audio signal. The number of layers L was set to 12,
corresponding to an input window of N=2L=4096 samples, spanning 250ms of audio signal -
standard practice with TIMIT dataset. The framework chosen for running the experiment was Caffe
toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with
default hyper-parameters: moment decay rates β1 = 0.9, β2 = 0.999; learning rate α = 0.001).
Weight decay and batch size were set to 10-5 and 128 respectively. Models were trained for 35000
iterations, with learning rate decreased by a factor of 10 after 80% of iterations took place.
7 Conclusion
Nearly all state of the art deep networks these days (e.g. Szegedy et al. (2015); He et al. (2015);
Huang et al. (2016b;a)) deviate from the simple feed-forward (chain) approach, employing various
connectivity schemes between their layers. In this paper we studied the representational implications
of connectivity in the context of dilated convolutional networks, a family of deep models deliver-
ing state of the art performance in audio and text processing tasks, underlying Google’s WaveNet
(van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)). We formulated our study
through the notion of expressive efficiency, which refers to a situation where one network must
grow unfeasibly large to realize (or approximate) functions of another. Our analysis shows that
interconnecting hidden layers of different dilated convolutional networks can bring forth a model
that is expressively efficient w.r.t. the individual networks it comprises. In particular, we show that
a single connection between hidden layers can already lead to an almost quadratic gap, which in
large-scale settings typically makes the difference between a model that is practical and one that is
not. We empirically evaluate the analyzed networks, and find that the expressive efficiency brought
forth by interconnectivity coincides with improved accuracies.
To date, formal analyses studying expressive efficiency have focused on the architectural feature of
depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. These
studies were motivated by the vast empirical evidence supporting the importance of depth. Our
work thus provides a second exemplar of an architectural feature for which expressive efficiency
and superior accuracies go hand in hand. This leads us to believe that expressive efficiency may
serve a key role in the development of new tools for deep network design.
9
Published as a conference paper at ICLR 2018
Acknowledgments
This work was supported by Intel grant ICRI-CI #9-2012-6133, by ISF Center grant 1790/12, and
by the European Research Council (TheoryDL project). Nadav Cohen was supported by a Google
Doctoral Fellowship in Machine Learning.
References
Richard Bellman. Introduction to matrix analysis, volume 960. SIAM, 1970.
Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report 05-02, 2005.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Se-
mantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. arXiv
preprint arXiv:1606.00915, 2016.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions.
International Conference on Machine Learning (ICML), 2016.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling geometry.
International Conference on Learning Representations (ICLR), 2017.
Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016a.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis.
Conference On Learning Theory (COLT), 2016b.
Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with mixed tensor
decompositions. arXiv preprint arXiv:1703.06846, 2017.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Infor-
mation Processing Systems, pages 666-674, 2011.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. arXiv preprint
arXiv:1512.03965, 2015.
John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett. Darpa timit acoustic-
phonetic continous speech corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon technical report n, 93,
1993.
Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Com-
putational Mathematics. Springer Science & Business Media, Berlin, Heidelberg, February 2012.
Andrew K Halberstadt. Heterogeneous acoustic measurements and multiple classifiers for speech recognition.
PhD thesis, Massachusetts Institute of Technology, 1998.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv
preprint arXiv:1512.03385, 2015.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional
networks. arXiv preprint arXiv:1608.06993, 2016a.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic
depth. In European Conference on Computer Vision, pages 646-661. Springer, 2016b.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the Perils of Non-Convexity: Guaranteed
Training of Neural Networks using Tensor Methods. CoRR abs/1506.08473, 2015.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar-
rama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of
the 22nd ACM international conference on Multimedia, pages 675-678. ACM, 2014.
Frank Jones. Lebesgue integration on Euclidean space. Jones & Bartlett Learning, 2001.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099, 2016.
10
Published as a conference paper at ICLR 2018
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional
Neural Networks. Advances in Neural Information Processing Systems, pages 1106-1114,2012.
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook
of brain theory and neural networks, 3361(10), 1995.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444, May 2015.
K-F Lee and H-W Hon. Speaker-independent phone recognition using hidden markov models. IEEE Transac-
tions on Acoustics, Speech, and Signal Processing, 37(11):1641-1648, 1989.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning real and boolean functions: When is deep
better than shallow. arXiv preprint arXiv:1603.00988, 2016.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of
deep neural networks. In Advances in Neural Information Processing Systems, pages 2924-2932, 2014.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceed-
ings of the 27th International Conference on Machine Learning (ICML-10), pages 807-814, 2010.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of inference regions of deep feed
forward networks with piece-wise linear activations. arXiv preprint arXiv, 1312, 2013.
Tomaso Poggio, Fabio Anselmi, and Lorenzo Rosasco. I-theory on depth vs width: hierarchical function
composition. Technical report, Center for Brains, Minds and Machines (CBMM), 2015.
Ben Poole, Subhaneil Lahiri, Maithreyi Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential ex-
pressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing
Systems, pages 3360-3368, 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power
of deep neural networks. arXiv preprint arXiv:1606.05336, 2016.
Hanie Sedghi and Anima Anandkumar. Training input-output recurrent neural networks through spectral meth-
ods. arXiv preprint arXiv:1603.00954, 2016.
Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tensorial mixture models. arXiv preprint
arXiv:1610.04167, 2016.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. CVPR, 2015.
Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101,
2015.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal KalCh-
brenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR
abs/1609.03499, 2016.
11
Published as a conference paper at ICLR 2018
Notes
1	For every (l, j), there exists a group of tensors {φl,j,γ }γ, formed through combinations of tensors from
its “child” groups {φl-1,2j-1,γ}γ and {φl-1,2j,γ}γ.
2	Binary mode trees lead to decompositions (eq. 3) that correspond to networks with size-2 convolutions.
We limit ourselves to this case for simplicity of presentation. Our formulation can easily be extended to
account for convolutions of arbitrary size by considering mode trees that are not necessarily binary, and by
modifying the decomposition in eq. 3 to take (generalized) tensor products between an arbitrary number
of tensors (not necessarily two).
3	A full binary tree is a tree in which all interior (non-leaf) nodes have exactly two children.
4	In general the number of tensors in the group {φν,γ}γ may vary across nodes ν, but for simplicity of
presentation we assume that all groups comprise exactly r tensors.
5	A perfect binary tree is a tree in which all interior (non-leaf) nodes have exactly two children and all leaves
have exactly the same depth.
6	Itis important to stress that not all choices of mode trees lead to networks resembling ones used in practice.
For example, if different leaves in a tree have different depths, different inputs in the corresponding network
pass through a different number of layers. Conversely, not every type of dilated convolutional network used
in practice corresponds to a mode tree - only ones in which an input is connected to the output through a
single path.
7	A few remarks are in order at this point:
•	The number of channels in each layer of N or N corresponds to the constant r in the respective
tree decomposition (eq. 3 with underlying mode tree T or T respectively). Similarly, the number of
channels in each layer of each interconnected network in M corresponds to r in the respective mixed
decomposition (eq. 4). In both the tree and mixed decompositions, r, referred to as the size constant,
stands for the number of tensors {φν,γ }γ (respectively {φν,γ }γ) held in each node V (respectively P).
We set this number uniformly across nodes, corresponding to uniformly sized layers across networks,
merely for simplicity of presentation. Our formulations and analysis can easily be adapted to account
for varying layer sizes, by allowing different nodes in a decomposition to hold a different number
of tensors. Note that an implication of our uniform setting is that a network’s input and output
dimensions vary along with the size of its hidden layers. When replicating a function realized by a
network using a larger network, we simply pad input vectors with zeros, and ignore the excess output
coordinates.
•	An additional simplification we made relates to weight sharing. In both the tree and mixed de-
compositions, each interior node ν (respectively νP) has a separate set of weights {aν,γ,I, aν,γ,II}γ
(respectively {a",γ,I, av,Y,II}Y). This implies that in the corresponding networks, convolution filters
may vary through time, i.e. different weights may be used against different portions of a convolved
sequence. The more commonplace setting of stationary filters (standard convolutions) is obtained by
restricting different nodes in a decomposition to possess the same weights. We do not introduce such
restrictions into our formulations, as they make little difference in terms of the analysis, but on the
other hand significantly burden presentation.
8	In accordance with the remark given at the beginning of this section, when using the (larger) mixed
decomposition, we pad discretizers with zeros, and ignore the excess output tensors.
9	Treatment of additional cases can be achieved by deriving a result analogous to theorem 1, i.e. by charac-
terizing matricization ranks brought forth by a tree decomposition (eq. 3) whose underlying operator g(∙)
corresponds to the architecture of interest (e.g. g(a, b) = max{a + b, 0} for networks with ReLU activa-
tion). Results along this line were established in Cohen and Shashua (2016). We defer their adoption and
development to future work.
10	We note that in addition to the mixed dilated convolutional network M, we also evaluated the individual
networks N and N - both reached accuracies comparable to M in the case of zero interconnections
(output summation only).
11	The case of convolutional arithmetic circuits (g(a, b)=a∙b) was also evaluated, leading to the exact same
trends as those observed with ReLU (fig. 5).
12
Published as a conference paper at ICLR 2018
mode tree T
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
{1,2,3,4,5,6,7,8}
{9,10,11,12,13,14,15,16}
{1,2,3,4}
{5,6,7,8}
{9,10,11,12}
{13,14,15,16}
{1,2}
{3,4}
{5,6}口	□{7,8}□匚{9,10}
{11,12}
{13,14}J	匚{15,16}
{1}J [⑵η 匚⑶Zl 匚⑷口 [{5d[{6口 [{7}η 匚⑻η ∏SjE{10}] [{11}] [{12}] [{13}] [{14}] [{15}] [{16}
index Set I = {3,4,5,6,7,8,9,11,12} T tiling Θ(I;T) = {{3,4},{5,6,7,8},{9},{11,12}}
Figure 6: Mode tree T along with a specific index set I and the resulting tiling Θ(I; T ) (def. 3).
A	Preliminaries
The constructions and analyses delivered in this paper rely on concepts from the field of tensor
analysis. Below we provide the minimal background required in order to follow our arguments.1
The core concept in tensor analysis is a tensor, which for our purposes may simply be thought of
as a multi-dimensional array. The order of a tensor is defined to be the number of indexing entries
in the array, which are referred to as modes. The dimension of a tensor in a particular mode is
defined as the number of values that may be taken by the index in that mode. For example, a 4-by-3
matrix is a tensor of order 2, i.e. it has two modes, with dimension 4 in mode 1 and dimension 3 in
mode 2. If A is a tensor of order N and dimension Mi in each mode i ∈ {1, . . . , N}, the space of
all configurations it can take is denoted, quite naturally, by RM1 × × MN.
A fundamental operator in tensor analysis is the tensor product (also known as outer prod-
uct), which We denote by 0. It is an operator that intakes two tensors A ∈ RM1× ×MP and
B ∈ RMP+1× ×MP+Q (orders P and Q respectively), and returns a tensor A0B ∈ RM1× ×MP+Q
(orderP+Q)definedby: (A0B)d1 …dP十口 = Adi…dP∙BdP十1…dP+q. InCohenandShashua(2016)
a generalization of the tensor product is defined, by replacing multiplication with a general opera-
tor g(∙). Specifically, for a function g : R X R → R that is commutative (g(a, b) = g(b, a) for all
a, b ∈ R), the generalized tensor product, denoted 0g, is defined to be the operator that for input ten-
sors A ∈ RM1× ×MP and B ∈ RMP+1× ×MP+Q (orders P and Q respectively), returns the tensor
A0g B ∈ RMi×…×MP+q (order P + Q) given by: (A0g B)dι...dP+q = g(Adι...dP, BdP +1 …dP+Q).
An additional operator we make use of is mode permutation. Let A be a tensor of order N, and
let σ(∙) be a permutation over N (bijective mapping from {1,...,Ν} to itself). The mode permu-
tation of A w.r.t. σ(∙), which by a slight abuse of notation is denoted σ(A), is the order-N tensor
defined by: σ(A)d1...dN = Adσ(1)...dσ(N) . In words, σ(A) is the tensor obtained by rearranging the
modes of A in accordance with σ(∙).
When studying tensors, it is oftentimes useful to arrange them as matrices, a procedure referred to as
matricization. Let Abe a tensor of order N and dimension Mi in each mode i ∈ {1, . . . , N}, and let
I ⊂ {1, . . . , N} be a set of mode indexes, whose complement {1, . . . , N} \I we denote byIc. We
may write I = {iι,...,i∣ι∣} where iι < .一 < i∣ι∣, and similarly IC = {jι,...,j∣ic∣} where jι <
•一< j∣ic∣. The matricization of A w.r.t. I, denoted JAKI, is the Qt=ι Mit-by-Q|tI=c1| Mjt matrix
holding the entries ofA such that Ad1...dN is placed in row index 1 + P|tI=|1(dit - 1) Q|tI0=| t+1 Mit0
and column index 1+ Ρt=1(djt — 1) QtI=t+ι Mjt0. If I = 0 or I = {1,..., N}, then by definition
JAKI is a row or column (respectively) vector of dimension QtN=1 Mt holding Ad1...dN in entry
1 + PtN=1(dt - 1) QtN0=t+1 Mt0.
To conclude this appendix, we hereinafter list notational conventions used throughout the paper.
We denote tensors with uppercase calligraphic letters, e.g. A, and in some cases, with the Greek
letters φ,夕 or ψ. Subscripts are used to refer to individual tensor entries, e.g. Adι...dN ∈ R, whereas
1 The viewpoint we adopt is actually a concrete special case of a more abstract algebraic viewpoint of tensor
analysis, as presented for example in Hackbusch (2012). We limit ourselves to this concrete viewpoint since it
suffices for our needs and is easier to grasp.
13
Published as a conference paper at ICLR 2018
superscripts indicate the location ofa tensor in some annotated collection, for example Ay stands for
the y’th tensor in the collection A1 . . . Ar . Vectors are typically denoted with boldface lowercase
letters, e.g. a, where again subscripts refer to an individual entry (e.g. aα ∈ R), and superscripts
to the identity of a vector within some annotated collection (e.g. al,j is the (l, j)’th vector in the
set {al,j}l=1...L,j=1...N). We use non-boldface lowercase or uppercase letters (e.g. l or L) to denote
scalars, and in this case, both subscripts and superscripts distinguish between objects in an annotated
set (e.g. li , li , Li , Li ∈ R). Finally, for a positive integer N ∈ N, we use [N] as shorthand for the
set {1, . . . , N}.
B Detailed Description of the Baseline Architecture
In this appendix we describe in detail the architecture of the baseline dilated convolutional net-
work (fig. 1). The input to the network is a sequence of vectors (x[t])t ⊂ Rr0 , where t is a nat-
ural time index. A size-2 convolutional layer with dilation-1, i.e. with contiguous filters, maps
this input into the hidden sequence (h(1) [t])t ⊂ Rr1 . Specifically, entry γ ∈ [r1] of h(1) [t] is
obtained by applying the filter formed by a1,γ,I, a1,γ,II ∈ Rr0 to time points t-1, t of the input:
h(1) [t]γ = g( a1,γ,I, x[t-1]), (a1,γ,π, x[t])). We use g(∙) here to denote the binary function Com-
bining two size-1 convolutions into a single size-2 convolution with non-linearity. Different choices
of g(∙) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to stan-
dard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas
g(a, b) = a∙b gives rise to What is known as a convolutional arithmetic circuit (Cohen et al. (2016b)).
Following the first hidden layer, L-1 size-2 convolutional layers with increasing dilations are ap-
plied. Specifically, for l = 2, . . ., L-1, hidden layer l maps the sequence (h(l-1) [t])t ⊂ Rrl-1
into (h(l) [t])t ⊂ Rrl using filters with dilation-2l-1, i.e. with an internal temporal gap of 2l-1-1
points: h(l) [t]γ = g(al,γ,I, h(l-1) [t-2l-1] , al,γ,II, h(l-1) [t]). The last convolutional layer
maps (h(L-1) [t])t into network output sequence (o[t])t ⊂ RrL using filters with dilation-2L-1:
o[t]y = g(aL,y,I, h(L-1)[t-2L-1] , aL,y,II, h(L-1)[t]).
Altogether, the architectural parameters of the network are the number of convolutional layers L,
the convolutional operator g(∙), the input dimension ro, the number of channels r for each hidden
layer l ∈ [L-1], and the output dimension rL. The learnable parameters are the convolution weights
al,γ,I, al,γ,II ∈ Rrl-1 for channel γ ∈ [rl] of layer l ∈ [L].
C Derivation of the Baseline Decomposition
In this appendix we derive the baseline decomposition (eq. 2) - a parameterization of grid ten-
sors (eq. 1) discretizing input-output mappings of the baseline dilated convolutional network
(fig. 1; app. B). As discussed in sec. 3.1, o[t] - the network output at time t, is a function of
x[t-N +1] . . . x[t] - its input over the last N := 2L time points. We would like to show that for
any d1. . .dN ∈ [M], entry (d1, . . . , dN) of a tensor Ay generated by eq. 2, is equal to coordinate y
of network output o[t] under the following input assignment: x[t-N +1] = v(d1), . . . , x[t] = v(dN).
To achieve this, we prove by induction that under the latter assignment, for every l ∈ [L] ∪ {0},
j ∈ [N/2l] and γ ∈ [rl], coordinate γ of the network’s depth-l sequence (input (x[t])t for l = 0;
hidden sequence (h(l)[t])t for l ∈ [L - 1]; output (o[t])t for l = L) at time t - N + j∙2l, is equal to
entry (d(j-1)2l+1, . . . , d(j-1)2l+2l ) of the tensor φl,j,γ in the baseline decomposition (eq. 2). The
desired result then follows from the case l = L, j = 1, γ = y.
When l = 0, the inductive hypothesis is trivial - coordinate γ of the input sequence at time t-N +j,
i.e. x[t - N + j]γ, is by definition of our assignment equal to vγ(dj ) - entry dj of the tensor φ0,j,γ
(see eq. 2). Assume now that the inductive hypothesis holds whenever l = k, and consider the
tensor φk+1,j,γ for some j ∈ [N/2k+1] and γ ∈ [rk+1]. From the baseline decomposition (eq. 2):
φk+1,j,γ = (χr: i aα+ι,γ,I ∙ ok，2-1。) 0g (χr: i aα+1,γ,π ∙ Φk,2j,α)
Focusing on entry (d(j-1)2k+1+1, . . . , d(j-1)2k+1+2k+1) of the left-hand side, while recalling the
definition of the generalized tensor product Xg (app. A), we may write:
φk+1,j,γ	=
d(j-1)2k+1+1，...，d(j-1)2k+1+2k+1
g Prk ak+1,γ,I φk,2j-1,α	Prk ak+1,γ,II φk,2j,α	)(5)
α=1 α	d(2j-2)2k+1,...,d(2j-2)2k+2k ,	α=1 α	d(2j-1)2k+1,...,d(2j-1)2k+2k
14
Published as a conference paper at ICLR 2018
By our inductive assumption:
Φk,2jτ,α	d	fc	fc	=	h(k)[t- N +(2j- 1)∙2k]α	∀α ∈	[rk]
d(2j-2)2k+1,...,d(2j-2)2k+2k
Φk,2j,α fc	d	fc	fc	=	h(k)[t	— N + 2j∙2k]α	∀α ∈	[rk]
d(2j-1)2k+1,...,d(2j-1)2k+2k
where we overload notation in the case k = 0, letting (h(0) [t])t stand for the input sequence (x[t])t.
Plugging the latter into eq. 5, we obtain:
φk+1,j,γ	=
d(j-1)2k+1+1,...,d(j-1)2k+1+2k+1
g ((ak+1,γ,I, h(k)[t - N + (2j - 1)∙2k]) ,〈ak+1，Y，n, h(k)[t - N + 2j∙2kD)
By the definition of the baseline dilated convolutional network (fig. 1; app. B), the latter expression
is precisely equal to coordinate γ of the sequence (h(k+1) [t])t (or (o[t])t if k = L - 1) at time t -
N + j∙2k+1. This proves that our inductive hypothesis holds when l = k + 1, and in general.
D	Annotations of the Tree and Mixed Decompositions
In this appendix we describe the tree and mixed decompositions (eq. 3 and 4 respectively), whose
annotations were omitted from the text due to lack of space.
Let T be a binary mode tree over [N] (def. 1). The tree decomposition of T (eq. 3) iteratively
assigns a group of (2|v|-order) tensors {φν,γ}γ∈[r] for each node V in T, based on weight Vec-
tors {aν,γ,I, aν,γ,II ∈ Rr}γ∈[r] defined for each interior node ν ∈int(T). Specifically, the de-
composition traverses through T in a depth-first fashion, and for each node ν, assigns the tensor
group {φν,γ}γ as follows:
•	If ν is a leaf, i.e. ν={j} for some j ∈ [N], its tensors ({φν,γ}γ) are set directly by the
discretizers v(1) . . . v(M) (vγ(i) in eq. 3 stands for coordinate γ of v(i)).
•	If ν is an interior node, i.e. ν ∈int(T), its tensors are set through combinations of the
tensors of its children ({φCI(ν;T),γ}γ and {φCπ(ν;T),γ}γ). These combinations are based
on the weight vectors {aν,γ,I}γ and {aν,γ,II}γ, as depicted in eq. 3 (aνα,γ,I and aνα,γ,II there
stand for coordinate a of aν,γ,I and aν,γ,II respectively). The permutation σ(ν;T)(∙) in the
assignment of φν,γ arranges the modes of the tensor (see app. A) such that these comply
with a sorted ordering of V. Namely, if We denote by iι< … <i∣a(ν;T)∣ the elements of
Ci(v; T )⊂[N ], andby jι< •…<j∣cπ(ν;T )∣ the elements of Cn(V; T )⊂[N ], the permutation
σ(ν;T) ： [2|v|] → [2|v| ] is the one that sorts the tuple (iι,..., iQ(ν;T )∣, ji,∙∙∙, j∣Cu(ν;T )∣)
in ascending order.
The final outcome of the tree decomposition, i.e. the generated grid tensors {Ay}y, are precisely the
tensor group {φ[N],γ}γ corresponding to the root ofT ([N]).
Heading on to the mixed decomposition (eq. 4), let T be an additional binary mode tree over [N]
(def. 1). When considering the tree decomposition of T, We use {φν,γ}γ∈[r] to denote the tensor
group of node V∈T, and {a",γ,I, aν,γ,π}γ∈m to denote the weights of interior node V∈int(T). For
a chosen collection of mixture nodes mix(T, ^Γ)⊂int(T)∩int(T), the mixed decomposition of T
and T blends together their tree decompositions by running these in parallel, while exchanging
tensors whenever a mixture node is reached. The procedure is formulated in eq. 4 - annotation
follows:
•	As in the basic tree decomposition (eq. 3), the first step (lines 1-2) is to assign tensors
corresponding to the leaf nodes ({1} . . . {N}) via discretizers v(1) . . . v(M).
•	The outer loop in line 3 traverses μ through mixture nodes and the root node in inclusion
order, i.e. such that a node (subset of [N]) is always reached after all nodes strictly contained
in it.
•	Lines 4-5 (respectively 6-7) are the same as in the tree decomposition (eq. 3), except that
instead of running through the entire interior of T (respectively T), they cover a segment
of it. This segment continues where the previous ones left off, and comprises only nodes
(subsets of [N]) contained in μ (including μ itself).
15
Published as a conference paper at ICLR 2018
•	Line 8 is where the mixing takes place - here half the tensors corresponding to node μ in
the decomposition of T ({φμ,γ}γ), are exchanged for half the tensors corresponding to μ
in the decomposition of T ({φμ,γ}γ).
•	Finally, after μ has reached the root node [N] and the decompositions of T and T have con-
cluded, line 9 sums the output tensors of these decompositions ({φ[N],y}y and {φ[N],y}y
respectively), producing the grid tensors {Ay}y.
E Deferred Proofs
E.1 Proof of Claim 1
We initiate the proof by introducing notations that will allow a more compact presentation. Here-
inafter, We let {aH,ν,γ,I, aH,ν,γ,π ∈ Rrmix/2}“三击跳切,7日丁小//2] stand for the weights in the tree
decomposition of the hybrid mode tree H (eq. 3 with size constant r = rmix/2 and underly-
ing mode tree given by def. 2). Similarly, we use {aT,ν,γ,I, aT,ν,γ,II ∈ Rrmix }ν∈int(T),γ∈[rmix]
and {aτ,ν,γ,I, aτ,ν,γ,π ∈ Rrmix }ν∈int(gr),γ∈[rmix] to denote the weights, corresponding to T and T
(respectively), in the mixed decomposition (eq. 4 with size constant r = rmix). Recall that by con-
struction (def. 2), int(H) - the interior of H, consists of different segments (collections of nodes),
each taken from either int(T) or int(T) We define t : int(H) → {T, T} to be the function indi-
cating which tree an interior node in H came from. Specifically, if the node ν∈int(H) originated
from T we have t(ν) = T, and on the other hand, if its source is T then t(ν) = T. By convention，
feeding t(∙) with an argument outside int(H) yields something that is different from both T and T.
For example, if ν∈int(H) is the root node, i.e. ν = [N], then P(ν; H) -its parent in H, is undefined
and we have t(P (ν; H))6=t(ν). Similarly, if the child CI(ν; H) of ν∈int(H) is a leaf, it is outside
the domain of t(∙) and thus t(ν)=t(Cι(ν; H)).
Given a setting of weights {aH,ν,γ,I, aH,ν,γ,II}ν,γ for the tree decomposition of H, we would like
to show that there exists a setting of weights {aT,ν,γ,I, aT,ν,γ,II}ν,γ and {aT,ν,γ,I, aT,ν,γ,II}ν,γ for
the mixed decomposition of T and T, such that the latter generates grid tensors identical to those
of the former. More precisely, for any collection of discretizers {v(i) ∈ Rrmix∕2}i∈[M] fed into the
tree decomposition of H, leading the latter to produce grid tensors {Ay}y∈[rmix∕2], we would like
the mixed decomposition to be such that when fed with the padded discretizers {[(v(i))> 0]> ∈
Rrmix }i∈[M], the first rmix/2 grid tensors it generates are equal to {Ay}y∈[rmix /2] . We prove ex-
istence of the sought after weight setting constructively, by presenting an explicit procedure for
assigning {aT,ν,γ,I, aT,ν,γ,II}ν,γ and {aT,ν,γ,I, aT,ν,γ,II}ν,γ based on {aH,ν,γ,I, aH,ν,γ,II}ν,γ:
Initialize:
a	T,ν,γ,I = aT,ν,γ,II = 0 ∀ν∈int(T),γ∈ [rmix]
a	T,ν,γ,I = aT,",γ,∏ = 0 ∀ν∈int(rΓ), Y ∈ [rmiχ]
For ν in int(H) (depth-first order):
at(ν),ν,γ+1 rmix,I = {	0> (aH,V,γ,I)>> (aH,V,γ,I)> 0>>	,t(ν )= t(CI(ν; H)) , t(ν) 6= t(CI(ν; H))	∀γ ∈ [rmix ∕2]
at(ν),ν,γ+1 rmix,II = {	0> (aH,V,γ,II)>> (aH,V,γ,II)> 0>>	,t(ν)= t(C∏(ν; H)) ,t(ν) = t(CII(ν; H))	∀γ ∈ [rmix ∕2]
If t(P(ν； H)) = t(ν):
Swap at(V),ν,γ,I J at(V),ν,γ+1 rmix,I ∀γ ∈ [rmix/2]
Swap at(V),ν,γ,II《__> at(V),ν,γ+ 2rmix,∏ ∀γ ∈ [rmiχ∕2]	(6)
The idea behind this assignment is as follows. The computation corresponding to a node in the tree
decomposition of H, is carried out, in the mixed decomposition of T and T, by the respective node
in the respective source tree. That is to say, the computation of ν∈int(H) in the tree decomposition
is carried out by ν∈int(t(ν)) in the mixed decomposition. ν∈int(t(ν)) uses half (rmix∕2) of its
16
Published as a conference paper at ICLR 2018
weight vectors, and in each used weight vector, half (rmix /2) of the coordinates hold actual (non-
zero) values - a copy of the respective weight from V∈int(H). The choice of which weight vectors
to use, and which coordinates to use in the active weight vectors, depends on the tree-transitioning
scheme. If the parent of ν in H came from the same tree as ν, i.e. t(P (ν; H)) = t(ν), ν∈int(t(ν)) in
the mixed decomposition uses weight vectors with higher indexes (γ∈rmiχ∕2 + [rmix/2]), as these
relate to tensors that are not exchanged (see eq. 4). On the other hand, if t(P (ν; H))6=t(ν), weight
vectors with lower indexes (γ ∈ [rmix/2]) are used, so that the computations (tensors) will be sent to
the opposite tree. The analogous rationale holds for the children ofν in H (CI(ν; H) and CII(ν; H)).
If a child came from the same tree as ν, upper coordinates of the appropriate weight vectors are used,
so that computations (tensors) coming from the present tree are collected. On the other hand, if the
child came from the opposite tree, lower coordinates are used and computations (tensors) from that
tree are fetched. Altogether, the assignment in eq. 6 meets our requirements, and thus concludes the
proof.
□
E.2 Proof of Theorem 1
E.2. 1 Sketch
The proof proceeds in three stages. In the first stage we matricize the tree decomposition of T ,
i.e. transform it from a tensor decomposition generating {Ay}y to a matrix decomposition generat-
ing {JAy KI}y. In this transformation, instances of the tensor product 0 Mg with g(a, b) = a∙b - see
app. A) convert to a Kronecker product . The second stage of the proof establishes the upper bound
stated in the theorem, by showing that for each y, JAyKI is equal to a product of matrices, one of
which has size rlθ(1;T X -by-rlθ(Ic;T )|. The key idea in this stage is the propagation of elements out
of the matrix decomposition, using the relation (AA0) (BB0) = (AA0)(BB0). The third and
final stage of the proof establishes the lower bound stated in the theorem. Here again, elements are
propagated out of the matrix decomposition, allowing the construction ofa concrete configuration of
weights ({aν,γ,I, aν,γ,II}ν,γ) for which the lower bound holds. The fact that the lower bound holds
almost always is then a direct corollary of app. G, where it is shown that the tree decomposition
admits maximal matricization ranks almost always when g(∙) is the product operator.
The study of matricization ranks under hierarchical tensor decompositions is of significant inter-
est, particularly in the context of deep learning. Cohen et al. (2016b) proved the lower bound in
the theorem for the specific case where T is the mode tree corresponding to the baseline dilated
convolutional network (see fig. 2(a)), and I = {1, 3, . . . , N -1}. The result was used to establish
exponential expressive efficiency of deep convolutional arithmetic circuits w.r.t. shallow ones. Co-
hen and Shashua (2017) later extended the analysis by deriving upper bounds for arbitrary index
sets I, using them to study the ability of deep convolutional arithmetic circuits to model correlations
among regions of their input. The bounds used in Cohen and Shashua (2017) were loose, and in
fact trivial for many choices of index sets I. We here treat arbitrary mode trees T and index sets I,
proving upper and lower bounds that are tight, oftentimes exact. Such tight bounds are necessary for
identifying expressive efficiency that is not exponential, as we do in this paper. The key to deriving
the bounds is the aforementioned idea of propagating elements out of a matrix decomposition.
E.2.2 Complete Proof
Since we are dealing with a single particular mode tree T, we omit it from our notations throughout
the proof. Specifically, we denote by CI(ν) and CII(ν) (instead of CI(ν; T) and CII(ν; T)) the
children of an interior node ν ∈int(T); by Θ(I) and Θ(Ic) (instead of Θ(I; T) and Θ(Ic; T)) the
tilings of I and IC (respectively) w.r.t. T (see def. 3); and by σ(ν)(∙) (instead of σ(ν;T)(∙)) the
permutation corresponding to ν ∈int(T) in the tree decomposition (eq. 3).
The first stage of the proof is to derive a matricized form of the tree decomposition, shedding light
into the manner in which grid tensor matricizations {JAy KI}y are generated. As a preparatory step
in this direction, we define the notion of an index set reduction. Let ν ⊂ [N] be a node in T, whose
elements we denote by iι < •…< i∣ν∣. The reduction of I onto V is defined as follows:
I∣ν ：= {j ∈ [∣V∣] : ij ∈I∩ V}	(7)
In words, it is the set of indexes corresponding to the intersection I ∩ V inside V. Besides index set
reduction, an additional tool we will be using is the Kronecker product - a matrix operator we denote
17
Published as a conference paper at ICLR 2018
by . For two matrices A ∈ RM1 ×M2 and B ∈ RN1 ×N2, AB is the matrix in RM1N1 ×M2N2
holding Aij Bkl in row index (i - 1)N1 + k and column index (j - 1)N2 + l.
Consider the central relation in the tree decomposition (eq. 3), while noticing that Zg ≡ 0 in our
setting (g(∙) is the product operator - see app. A):
φν,γ}	= σ(V) ((XrTaaAI ∙ ΦCI(ν),α) 0 (XrTaa，” ∙ 0CII(v)，a))
order 2|v|
(8)
Suppose We would like to matricize the tensor φν,γ w.r.t. the reduction I∣ν. If all elements of CI(V)
were smaller than those of Cn(V), the permutation σ(ν)(∙) would be the identity (see sec. 3.2), and
the following matrix relation would hold:
Jφν,γ Kilv	=	[(X[=1 aα,γ,I ∙ φccf 0 (Xa=1 aa,γ,II ∙ ΦCII(V),a)]IL
=rXa=I aa,γ,I ∙ φCI(V),azi∣c(v) θ rXa=I aa,γ,II ∙产(V),aLcπ(v)
=(Xa=Iaa" JΦCI(V),aKi∣cI(v)) Θ (Xa=Iaa,γ,II ∙ JφCU(V),aKi∣cII(v))
In general however, elements in CI(V) could be greater than ones in CII(V), and so eq. 8 includes a
tensor mode sorting via σ(V) (∙). In matricized form, this amounts to rearranging rows and columns
through appropriate permutation matrices Q(V) and Q(V) respectively:
JΦ",γKi∣v = Q(V) ((Xa=1 aa,γ,I ∙ JΦCI(V),aKi∣cI(v)) Θ(X：=1 aa,γ,II ∙ JφCII(V),aKilCII(V))) Q(V)
We thus arrive at the following matrix form of eq. 3, referred to as the matricized tree decomposition:
For j = 1. . .N :
Jφ{j},γKι∣{j} = MY1),…,vYM)]>]©「} ∀γ∈ [r]
For ν in int(T) (depth-first order):
Jφν,γ KiIv = Q(V)
∀γ ∈ [r]
JAyKI = Jφ[N],yKi∣[N] ∀y ∈ [r]
(9)
Next, we move on to the second stage of the proof, where we establish the upper bound stated in the
theorem:
rankJAyKi ≤ rmin{lΘ(i)l,lΘ(ic)l} ∀y	(10)
We begin by “propagating outwards” the permutation matrices Q(ND and QaND corresponding to
the root node [N] in the matricized tree decomposition (eq. 9). Namely, for every γ ∈ [r], we replace
the matrix Jφ[N],γKil[N] by:
B[N],γ =(Xa型叫ΦCI([N]),αKIIcI(Nj ® (X。"限想为叫”“)!
and accordingly move QaND and Q([ND to the assignments of {JAyKI}y. This gives rise to the
following decomposition:
18
Published as a conference paper at ICLR 2018
For j = 1. . .N :
Jφ{j},γ Kι∣{j}
,...,vγ(M)]>z	∀γ∈ [r]
I|{j}
For ν in int(T ) \ {[N]} (depth-first order):
Jφν,γ Kι∣ν = Q(V)
∀γ ∈ [r]
。(£a*，”JeCIIWKIIcIIaN])!	∀Y∈ [r]
B[N],γ= (XX。/铲(“叼1Ι5)
α=1
JAyKI = Q([N])B[N]，yQ([N]) ∀y ∈ [r]
Consider now CI ([N ]) -a child of the root node [N ], and suppose We would like to similarly prop-
agate outwards its permutation matrices Q(CIaN])) and Q(CIaND). We may define, for every γ∈[r]:
BCI([N]),γ ：= (XX aCI([M),γJ[。侬。»))，%"*“))!。(X aCI([M),γ,πJΦCII(CI([M)),αKIIcπ(cI(N]))
α=1	α=1
which in turn implies:
B[N],γ
(E。^，[，。加M))BCI([N])，aQ(CI(W)) Θ (XXaaNEWCIIaNDQKIICn(Nj
(Q(CI(W)) (XXaαN],γ,ιBCI(W),) Q(CI([M))) θ (E。器门叫°CII(N])，aKIICn([N])
Now, for any matrices A, A0 , B, B0 such that AA0 and BB0 are defined, the following equality
holds: (AA0) (BB0) = (AA0)(BB0) (see Bellman (1970) for proof). We may therefore write:
B[N],γ
Q(CI([N]))I
aaN]，Y，IBCI([N])，a! Θ (XXaaN],MjΦCII([M),aKI∣cπ(N])
(CI([N]))l0/
where I and I are identity matrices of appropriate sizes. Propagating outwards the matrices
Q(CIaN I))OI and Q(Cl(N ]))0j (while redefining B [N ],γ appropriately), We arrive at the following
decomposition:
For j = 1. . .N :
Jφ{j},γ KI%)
For ν in int(T ) \ {[N], CI ([N])} (depth-first order):
Jφν,γ KIIv = Q(V)	aα,γjjφCI(V),αKI∣cI(ν)
aα,MjφCII(V),ακI∣cIIJ) Q(V) ∀γ∈ [r]
BCI(W),γ = (E aCI([M)叫 ΦCI(CI(W)),αKIIcI(cI(N)J
ΘXr aαCI([N]),γ,IIJφCII(CI([N])),αKIICII(CI([N]))! ∀γ∈ [r]
a[αN],γ,IBCI([N]),α! ΘXr a[αN],γ,IIJφCII([N]),αKIICII([N])!	∀γ∈[r]
JAyKI=(Q([M)(Q(CI([M⅝I)) B[N],y ((Q(CI(N]⅝∕)Q(W)) ∀y ∈ [r]
19
Published as a conference paper at ICLR 2018
Continuing this process, We propagate outwards the permutation matrices Q(V) and Q(V) of all
nodes ν in the tree that are not members of the tilings Θ(I) or Θ(Ic) (see def. 3), and are not
descendants of such. This brings forth the following decomposition:
For j = 1. . .N :
Jφ{j},γKι∣{j} = MY1),…,vYM)]>]©「} ∀γ∈ [r]
For ν in int(T)∩{nodes in Θ(I) or Θ(Ic) or descendants of such} (depth-first order):
Jφν,γ Kι∣ν = Q(V)
∀γ ∈ [r]
For ν in Θ(I) ∪ Θ(Ic):
Bν,γ = JΦν,γKι∣ν ∀Y∈ [r]
For ν in int(T)\{nodes in Θ(I) or Θ(Ic) or descendants of such} (depth-first order):
Bν,γ = Xr aνα,γ,IBCI(ν),α
Xr aνα,γ,IIBCII(ν),α	∀γ∈ [r]
JAyKI = A∙B[N],y∙A ∀y ∈ [r], for appropriate matrices A and A
Consider now a node V∈int(T) whose child belongs to a tiling - without loss of generality CI(V)
belongs to Θ(I). Notice that in this case BCI(V),α is a column vector for every α ∈ [r]. We may
thus define BCI(V) to be the matrix whose a'th column is BCI(V),α, and get the following equalities:
Bν,γ = BCI(ν)aν,γ,I Xr aνα,γ,IIBCII(ν),α = BCI(ν)I aν,γ,I Xr aνα,γ,IIBCII(ν),α
where again, I is an appropriately sized identity matrix. This implies that we can propagate out-
wards BCI(V)I, just as we have done with permutation matrices. Applying this procedure to all
nodes in the tilings Θ(I) and Θ(Ic), we arrive at the decomposition below:
For ν in Θ(I):
Bν,γ = e(γ) ∀γ ∈ [r]
For ν in Θ(Ic):
Bν,γ = (e(γ))> ∀γ ∈ [r]
For ν in int(T)\{nodes in Θ(I) or Θ(Ic) or descendants of such} (depth-first order):
aνα,γ,IBCI(ν),α	Xr aνα,γ,IIBCII(ν),α	∀γ ∈ [r]
JAyKI = A∙B[N],y∙A ∀y ∈ [r], for appropriate matrices A and A
Notice that for compactness in writing we made use of the fact that aV,γ,I = Prα=1 aVα,γ,IIe(α),
where e(α), α ∈ [r], is the vector in Rr holding 1 in entry α and 0 in the rest. Note also that in this
decomposition, as opposed to the previous ones, the matrices A and A are not global constants that
depend only on T. Rather, they also depend on Jφν,γKIIV for tiling nodes V ∈ Θ(I) ∪ Θ(Ic),
and thus are ultimately determined through a hidden computation that is not specified above.
This hidden computation is outside our scope, as we are only interested in the size of the matri-
ces {B[N],y}y. It is not difficult to see that this size is precisely rlθ(I)I-by-rlθ(Ic)I meaning that
the ranks of {B[N],y}y are no more than rmm{|®(I)i,iθ(Ic)I}. Since these ranks are greater than or
equal to those of {JAyKI}y, the sought after upper bound (eq. 10) indeed holds.
In the third and final stage of the proof, we establish the lower bound stated in the theorem, namely,
that for all configurations of weights {aV,γ,I, aV,γ,II}V,γ but a set of Lebesgue measure zero:
rankJAy KI
≥ rI{(V1 ,V2 )∈Θ(I)×Θ(Ic ): V1 and V2 are siblings in T with depth>1}I
∀y
(11)
We reduce the problem in three successive steps:
20
Published as a conference paper at ICLR 2018
•	A tree decomposition (eq. 3) with a product operator g(∙) admits maximal matricization
ranks almost always (see app. G). Therefore, to prove that eq. 11 holds for all weight
settings but a set of Lebesgue measure zero, it suffices to find a particular weight setting
for which the inequality holds.
•	By assumption, the discretizers {v(i) }i∈[M] span Rr. Without loss of generality, assume
that {v(i)}i∈[r] are linearly independent, and consider the sub-tensors of {Ay}y formed by
restricting their indexes to the range 1. . .r (instead of 1. . .M). The matricizations of these
sub-tensors w.r.t. I are sub-matrices of {JAyKI}y, thus any lower bound on ranks of the
former matricizations immediately translates to a lower bound on ranks of the latter. Since
the sub-tensors are precisely the grid tensors that would have been generated by the tree
decomposition (eq. 3) had We omitted the trailing discretizers {v(i)}i∈[M]\[r], establishing
eq. 11 in the case M = r proves that it holds in general (M≥r).
•	Bearing in mind that we assume M = r (and linear independence of {v(i)}i∈[r]), denote
by V the r-by-r matrix holding v(i) in its i'th row, i.e. V := [v(1)… • v(r)]>. From the tree
decomposition (eq. 3) it is evident that the discretizers affect generated grid tensors only
through products of the form Vaν,γI or Vaν,γII, where ν is a parent of a leaf node in T .
Since V is invertible ({v(i)}i∈[r] are linearly independent), its exact value has no effect
on the class of representable grid tensors - any change it undergoes may be accounted for
by the weights aν,γI and aν,γII that multiply it (these weights do not appear elsewhere in
the decomposition). Accordingly, for establishing a lower bound on achievable grid tensor
matricization ranks, the value of V is irrelevant (so long as it is invertible), and we may
assume, without loss of generality, that V is the identity matrix, i.e. that v(i) = e(i) for
all i ∈ [r].
Taking into account the above reductions, our objective is to show that there exists a setting of
weights {aν,γ,I, aν,γ,II}ν,γ, such that the following special case of the matricized tree decomposition
(eq. 9) generates matricizations meeting the lower bound in eq. 11:
For j in I:
JΦ{j},γKi%) = e(γ) ∀Y ∈ [r]
For j in Ic :
JΦ{j},γKι∣{j) = (e(γ))> ∀Y ∈ [r]
For ν in int(T) (depth-first order):
Jφν,γ Kι∣ν = Q(V)
∀γ ∈ [r]
JAyKI = JΦ[N],yKι∣N] ∀y ∈ [r]
Similarly to the procedure carried out in the second stage of the proof (establishing the upper bound
in eq. 10), we now propagate outwards the permutation matrices Q(V) and Q(V) corresponding to all
interior nodes ν∈int(T ). This brings forth the following decomposition:
For j in I:
B{j},γ = e(γ)	∀γ ∈ [r]
For j in Ic :
B{j},γ = (e(γ))> ∀γ ∈ [r]
For ν in int(T) (depth-first order):
Bν,γ
∀γ ∈ [r]
JAyKI = A∙B[N],y∙A ∀y ∈ [r], for appropriate matrices A and A	(12)
The matrices A and A in the assignments of {JAyKI}y essentially collect all permutation matri-
ces {Q(ν)}ν and {Q(ν)}ν (respectively) that have been propagated outwards. Specifically, A (re-
spectively A) is a product of factors, each of the form IΘQ(ν)Θl0 (respectively IOQ(V)I0) for a
21
Published as a conference paper at ICLR 2018
different interior node ν and appropriately sized identity matrices I and I0 . Since permutation ma-
trices are invertible, and since the Kronecker product between two invertible matrices is invertible
as well (See Bellman (1970) for proof), We conclude that the matrices A and A are invertible. There-
fore, for every y ∈ [r], the rank of JAyKI is equal to that of B[N],y. It thus suffices to find a setting
of weights {aν,γ,I, aν,γ,II}ν,γ for which:
rank(B[N],γ) ≥
r∣{(νι ,ν2)∈Θ(I) ×Θ(Ic): νι and ν? are siblings in T with depth>1}∣
∀γ ∈ [r]
(13)
Disregard the trivial case where there exist siblings ν1 ∈ Θ(I) and ν2 ∈ Θ(Ic) of depth 1,2 and
consider the following weight setting:
•	ν is a node in Θ(I) or Θ(Ic), or a descendant of such:
aν,γ,I = aν,γ,II = e(γ) ∀γ ∈ [r]
•	ν has one child in Θ(I) and the other in Θ(Ic):
aν,γ,I = aν,γ,II = e(γ) ∀γ ∈ [r]
•	ν is the root node [N]:
aν,γ,I = aν,γ,II = e(1)	∀γ ∈ [r]
•	ν meets neither of the above (0 and 1 here denote the all-zero and all-one vectors in Rr,
respectively):
aν,1,I =	e1(1)
aν,1,II =	e1(1)
, CI(ν) has one child in Θ(I) and the other in Θ(Ic)
, otherwise
, CII(ν) has one child in Θ(I) and the other in Θ(Ic)
, otherwise
aν,γ,I = aν,γ,II = 0 ∀γ ∈ [r] \ {1}
Plugging this into the decomposition in eq. 12, one readily sees that:
•	For every ν ∈ Θ(I), {Bν,γ}γ∈[r] are indicator column vectors (one entry holds 1, the rest
hold 0) such that Bν,γ 6=Bν,γ0 if γ 6= γ0. The same holds for ν ∈ Θ(Ic), but with the
vectors being rows.
•	If ν has one child in Θ(I) and the other in Θ(Ic), {Bν,γ}γ∈[r] are indicator matrices,
where both the row and column indexes of the active entry do not repeat as γ varies.
•	The matrices {B [N],γ }γ∈[r] corresponding to the root node [N] are equal to one another,
given by a joint Kronecker product between all of the following:
-Bν,1 for every node V in either Θ(I) or Θ(IC) which does not have a sibling in the
other
- Prα=1 Bν,α for every node ν that has one child in Θ(I) and the other in Θ(Ic)
According to the first observation above, Bν,1 has rank 1 for every ν in Θ(I) or Θ(Ic). The second
observation implies that Prα=1 Bν,α has rank r for every node ν that has one child in Θ(I) and
the other in Θ(Ic). In turn, and while taking into account the rank-multiplicative property of the
Kronecker product (rank(AΘA0) = rank(A)∙rank(A0) - see Bellman (l970) for proof), the third
observation implies:
rɑnk(B[N],Y) = r|{(Vι,ν2^θ(I)×θ(Ic): VI and V are siblings inT}| YY ∈ [r]
We thus have found weights {aν,γ,I, aν,γ,II}ν,γ for which eq. 13 holds.3 This establishes the sought
after lower bound on matricization ranks (eq. 11), completing the proof of the theorem.
□
2 In this case I and Ic are the children of the root node [N], and the maximal rank of B[N],γ is 1 for
every γ ∈ [r].
3 This applies to all but the trivial case where I is such that there exist siblings ν1 ∈ Θ(I) and ν2 ∈ Θ(Ic)
of depth 1 (I and Ic are the children of the root node [N]). In the latter case the lower bound in eq. 13 can be
met trivially.
22
Published as a conference paper at ICLR 2018
mode tree T
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
index set
I = {1,3,5,7,9,10,13,14}
complement	Ic ={2,4,6,8,11,12,15,16}
mode tree T
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
{1,2,3,4}
{1,2,3,4,5,6,7,8}
{9,10,11,12,13,14,15,16}
{1,2,3,4,9,10,11,12}
{5,6,7,8,13,14,15,16}
{7,8}
tilings
®(I ； T ) = {{1},{3},{5},{7},{9,10},{13,14}}
®(IC ； T ) = {{2},{4},{6},{8},{11,12},{15,16}}
{5,6,7,8}
{9,10,11,12}
{13,14,15,16})
mixture
nodes
{1,2,3,4}
{5,6,7,8}
{9,10,11,12}
{13,14,15,16})
{11,12}
{13,14}
{15,16}
14}
~W
{9,11}
{10,12H	[{13,15H	匚{14,16}
{1}	{2}	{3}	{4}	{5}	{6}	{7}	{8}
∏{15∏ 砌
hybrid mode tree H
{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}
(5,6,7,8,13,14,15,16}
tilings
®(I ； T H{1,3},{5,7},{9},{10},{13},{14}}
®(IC ； T ) = {{2,4},{6,8},{11},{12},{15},{16}}
9	*9}	{10}	{11}	{12}	{13}	{14}	{15}	{16}
{13,14,15,16})

阚圆砌
tilings
®(I ；H H{1},{3},{5},{7},{9},{10},{13},{14}}	®(IC ； H ) =	{{2},{4},{6},{8},{11},{12},{15*16}}
Figure 7: Best viewed in color. Two mode trees T and T With a possible choice of mixture nodes (same as
in fig. 3(a) and 4(a)), along with a particular formed hybrid tree H . An index set I and its complement Ic are
tiled into more pieces by H than they are by T and T, leading the former to generate grid tensors with higher
matricization ranks (theorem 1).
F	Demonstration of Expres sive Efficiency
In this appendix, omitted from the text due to lack of space, we demonstrate the application of theo-
rem 1 for establishing expressive efficiency. In particular, we use the theorem to derive corollary 1.
Consider our exemplar mode trees illustrated in fig. 2. Specifically, let T be the mode tree corre-
sponding to the baseline dilated convolutional network (dilation 2l-1 in layer l∈[L]=[log2 N] 一 see
sec. 3.1), and let T be the mode tree corresponding to the network obtained by swapping dilations of
even and odd layers (such that layer l has dilation 2l-2 if l is even, and 2l if l is odd). T is a perfect
binary tree whose depth-l nodes, l ∈ {0,1,..., L}, are (k - 1)N∕2l + [N/2l] for k ∈ [2l].4 T is
also perfect and has the same even-depth nodes, but its odd-depth nodes differ - they are generated
by splitting parents into children holding non-contiguous quadrants. Suppose we choose mix(T, T)
to include the set of nodes in T and T whose depth is L-2, and consider the hybrid mode tree H
formed by taking the segments (see def. 2) of the first half of these nodes from T, and the rest of
the tree from T. An illustration of T, T and H in this setting, for the case L = 4, is given in fig. 7.
Now, let the index set I consist of every second index in [N/2], and every second pair of indexes
in N/2 + [N/2], i.e. I := {2k-1 : k∈[N∕4]}∪{N∕2+4k-k0 : k∈[N∕8], k0=2, 3}. As illustrated
in fig. 7, the mode tree T tiles (see def. 3) the lower half of I into singletons, and its upper half
into pairs. The same applies to T’s tiling of I’s complement Ic := [N] \ I. Moreover, for every
node in the tiling Θ(I; T), there exists a sibling in Θ(Ic; T) (and vice versa). By theorem 1, this
implies that the tree decomposition of T generates grid tensors whose matricizations w.r.t. I have
rank rN/4+N/8. A similar situation occurs with the mode tree T, under which I and IC are tiled
into pairs in their lower halves and into singletons in their top halves (see illustration in fig. 7). This
also leads to matricized grid tensors of rank rN/4+N/8 . On the other hand, the hybrid mode tree H
tiles I and Ic entirely into singletons (see illustration in fig. 7), leading (by theorem 1) to grid tensor
matricization ranks of rN/2. This means that if we were to replicate grid tensors generated by the
tree decomposition of H using those of T or T (or a summation thereof), we would need to increase
the size constant r super-linearly -bya power of 4/3 (at least).
The above example can be generalized, by considering swapping the dilations of more than two
layers at once. In particular, ifT is the mode tree corresponding to the baseline dilated convolutional
network (dilation 2l-1 in layer l), T is the mode tree corresponding to the network obtained by
swapping dilations of groups of k layers (dilation 2d“ke∙k-1-((l-1)mod k) in layer l), and the set of
mixture nodes includes all nodes of depth L-k, a hybrid mode tree H and an index set I can be
found, such that the tree decomposition of H generates grid tensors whose ranks when matricized
w.r.t. I can only be matched by the tree decompositions of T and T if their size constant r is
increased by a power of 2/(1 + 21-k). Since the mixed decomposition of T and T (eq. 4) can
realize the tree decomposition of H with double the size constant (claim 1), we conclude that it
can, with size constant 2r, generate grid tensors whose matricization ranks (w.r.t. I) require the tree
4 If c is a scalar and S is a set, c + S stands for the set obtained by adding c to each element in S.
23
Published as a conference paper at ICLR 2018
decompositions of T and T to have size constant r2/(1+21 fc) - SUPer-linearly larger. Therefore, in
this particular setting, prop. 2 holds and the mixed decomposition of T and T is indeed expressively
efficient w.r.t. their tree decompositions. Taking into accoUnt the fact that the mixed decomposition
admits maximal matricization ranks almost always when g(∙) is the product operator (see app. G),
we formalize the result in network terms and reach corollary 1.
G Maximality of Matricization Ranks
In the proof of theorem 1 (app. E.2), and in the derivation of corollary 1 (app. F), we made use of the
fact that a tree or mixed decomposition (eq. 3or4 respectively), with a product operator g(∙), admits
maximal matricization ranks almost always. That is to say, for any index set I ⊂ [N], the ranks of
generated grid tensors {Ay}y when matricized w.r.t. I, attain their maximum possible values (which
depend on both the decomposition and I) for all configurations of weights ({aν,γ,I, aν,γ,II}ν,γ for
the tree decomposition, {aν,γ,I, aν,γ,π}ν,γ and {a",γ,I, a”,γ,π}ν,γ for the mixed decomposition) but
a set of Lebesgue measure zero. Hereinafter we justify this assertion.
When equipped with the product operator (g(a, b) = a∙b), a tree or mixed decomposition generates
grid tensors {Ay}y whose entries are polynomials in the decomposition weights. Therefore, for
any index set I ⊂ [N], the entries of the matricizations {JAy KI}y are, too, polynomials in the
decomposition weights. Claim 2 below implies that for a particular index y, the rank of JAyKI is
maximal almost always, i.e. for all weight settings but a set of measure zero. Since the union of
finitely many zero measure sets is itself a zero measure set (see Jones (2001) for example), we
conclude that the ranks of {JAy KI}y are jointly maximal almost always, which is what we set out
to prove.
Claim 2. Let D, M1, M2 ∈ N, and consider a polynomial function mapping weights α ∈ RD to
matrices A(α) ∈ RM1 ×M2 (“polynomial” here means that all entries of A(α) are polynomials
in α). Denote R = maxα∈RD rank(A(α)), and consider the set S := {α ∈ RD : rank(A(α)) <
R}. This set has Lebesgue measure zero.
Proof. We disregard the trivial case where R = 0. Let α0 be a point at which R is at-
tained (rank(A(α0)) = R), and assume without loss of generality that the top-left R×R minor
of A(ao), i.e. the determinant of A(ao)i：R,i：R, is non-zero. The function P : RD → R defined
by p(α) = det(A(a)i：R,i：R) is a polynomial, which by construction does not vanish everywhere
(p(α0) 6= 0). The zero set of a polynomial is either the entire space, or a set of Lebesgue measure
zero (see Caron and Traynor (2005) for proof). Therefore, the zero set of p(∙) has Lebesgue measure
zero. Now, for every α∈S:
rank(A(α)) < R ⇒ rank(A(a) 1：R, 1：R) < R ⇒ p(α) := det(A(a) 1:R, 1:R) = 0
S is thus contained in the zero set of p(∙), and therefore too, has Lebesgue measure zero. □
24