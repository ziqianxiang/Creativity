Published as a conference paper at ICLR 2018
On the insufficiency of existing momentum
schemes for Stochastic Optimization
Rahul Kidamb产，Praneeth Netrapalli2, Prateek Jain2 and Sham M. Kakade* 1
1 University of Washington Seattle 2 Microsoft Research India
rkidambi@uw.edu, {praneeth, prajain}@microsoft.com,
sham@cs.washington.edu
Ab stract
Momentum based stochastic gradient methods such as heavy ball (HB) and Nes-
terov’s accelerated gradient descent (NAG) method are widely used in practice for
training deep networks and other supervised learning models, as they often pro-
vide significant improvements over stochastic gradient descent (SGD). Rigorously
speaking, “fast gradient” methods have provable improvements over gradient de-
scent only for the deterministic case, where the gradients are exact. In the stochas-
tic case, the popular explanations for their wide applicability is that when these
fast gradient methods are applied in the stochastic case, they partially mimic their
exact gradient counterparts, resulting in some practical gain. This work provides
a counterpoint to this belief by proving that there exist simple problem instances
where these methods cannot outperform SGD despite the best setting of its pa-
rameters. These negative problem instances are, in an informal sense, generic;
they do not look like carefully constructed pathological instances. These results
suggest (along with empirical evidence) that HB or NAG’s practical performance
gains are a by-product of mini-batching.
Furthermore, this work provides a viable (and provable) alternative, which, on the
same set of problem instances, significantly improves over HB, NAG, and SGD’s
performance. This algorithm, referred to as Accelerated Stochastic Gradient De-
scent (ASGD), is a simple to implement stochastic algorithm, based on a relatively
less popular variant of Nesterov’s Acceleration. Extensive empirical results in this
paper show that ASGD has performance gains over HB, NAG, and SGD. The code
implementing the ASGD Algorithm can be found here1.
1	Introduction
First order optimization methods, which access a function (to be optimized) through its gradient or
an unbiased approximation of its gradient, are the workhorses for modern large scale optimization
problems, which include training the current state-of-the-art deep neural networks. Gradient de-
scent (Cauchy, 1847) is the simplest first order method that is used heavily in practice. However,
it is known that for the class of smooth convex functions as well as some simple non-smooth prob-
lems (Nesterov, 2012a)), gradient descent is suboptimal (Nesterov, 2004) and there exists a class of
algorithms called fast gradient/momentum based methods which achieve optimal convergence guar-
antees. The heavy ball method (Polyak, 1964) and Nesterov’s accelerated gradient descent (Nes-
terov, 1983) are two of the most popular methods in this category.
On the other hand, training deep neural networks on large scale datasets have been possible through
the use of Stochastic Gradient Descent (SGD) (Robbins & Monro, 1951), which samples a random
subset of training data to compute gradient estimates that are then used to optimize the objective
function. The advantages of SGD for large scale optimization and the related issues of tradeoffs
between computational and statistical efficiency was highlighted in Bottou & Bousquet (2007).
*part of the work was done during an internship at Microsoft Research, India.
1link to the ASGD code: https://github.com/rahulkidambi/AccSGD
1
Published as a conference paper at ICLR 2018
The above mentioned theoretical advantages of fast gradient methods (Polyak, 1964; Nesterov,
1983) (albeit for smooth convex problems) coupled with cheap to compute stochastic gradient es-
timates led to the influential work of Sutskever et al. (2013), which demonstrated the empirical
advantages possessed by SGD when augmented with the momentum machinery. This work has led
to widespread adoption of momentum methods for training deep neural nets; so much so that, in the
context of neural network training, gradient descent often refers to momentum methods.
But, there is a subtle difference between classical momentum methods and their implementation in
practice - classical momentum methods work in the exact first order oracle model (Nesterov, 2004),
i.e., they employ exact gradients (computed on the full training dataset), while in practice (Sutskever
et al., 2013), they are implemented with stochastic gradients (estimated from a randomly sampled
mini-batch of training data). This leads to a natural question:
“Are momentum methods optimal even in the stochastic first order oracle (SFO) model, where we
access stochastic gradients computed on a small constant sized minibatches (or a batchsize of 1?)”
Even disregarding the question of optimality of momentum methods in the SFO model, it is not
even known if momentum methods (say, Polyak (1964); Nesterov (1983)) provide any provable
improvement over SGD in this model. While these are open questions, a recent effort of Jain et al.
(2017) showed that improving upon SGD (in the stochastic first order oracle) is rather subtle as
there exists problem instances in SFO model where it is not possible to improve upon SGD, even
information theoretically. Jain et al. (2017) studied a variant of Nesterov’s accelerated gradient
updates (Nesterov, 2012b) for stochastic linear regression and show that their method improves
upon SGD wherever it is information theoretically admissible. Through out this paper, we refer to
the algorithm of Jain et al. (2017) as Accelerated Stochastic Gradient Method (ASGD) while we
refer to a stochastic version of the most widespread form of Nesterov’s method (Nesterov, 1983) as
NAG; HB denotes a stochastic version of the heavy ball method (Polyak, 1964). Critically, while
Jain et al. (2017) shows that ASGD improves on SGD in any information-theoretically admissible
regime, it is still not known whether HB and NAG can achieve a similar performance gain.
A key contribution of this work is to show that HB does not provide similar performance gains
over SGD even when it is informationally-theoretically admissible. That is, we provide a problem
instance where it is indeed possible to improve upon SGD (and ASGD achieves this improvement),
but HB cannot achieve any improvement over SGD. We validate this claim empirically as well. In
fact, we provide empirical evidence to the claim that NAG also do not achieve any improvement
over SGD for several problems where ASGD can still achieve better rates of convergence.
This raises a question about why HB and NAG provide better performance than SGD in practice
(Sutskever et al., 2013), especially for training deep networks. Our conclusion (that is well supported
by our theoretical result) is that HB and NAG’s improved performance is attributed to mini-batching
and hence, these methods will often struggle to improve over SGD with small constant batch sizes.
This is in stark contrast to methods like ASGD, which is designed to improve over SGD across
both small or large mini-batch sizes. In fact, based on our experiments, we observe that on the task
of training deep residual networks (He et al., 2016a) on the cifar-10 dataset, we note that ASGD
offers noticeable improvements by achieving 5 - 7% better test error over HB and NAG even with
commonly used batch sizes like 128 during the initial stages of the optimization.
1.1	Contributions
The contributions of this paper are as follows.
1.	In Section 3, we prove that HB is not optimal in the SFO model. In particular, there
exist linear regression problems for which the performance of HB (with any step size and
momentum) is either the same or worse than that of SGD while ASGD improves upon
both of them.
2.	Experiments on several linear regression problems suggest that the suboptimality of HB in
the SFO model is not restricted to special cases - it is rather widespread. Empirically, the
same holds true for NAG as well (Section 5).
3.	The above observations suggest that the only reason for the superiority of momentum meth-
ods in practice is mini-batching, which reduces the variance in stochastic gradients and
moves the SFO closer to the exact first order oracle. This conclusion is supported by em-
2
Published as a conference paper at ICLR 2018
Algorithm 1 HB: Heavy ball with a SFO
Require: Initial wo, stepsize δ, momentum ɑ
1:	w-ι — wo; t - 0	/*Set w-ι to wo*/
2:	while wt not converged do
3:	Wt+1 - Wt-δ∙Vft(wt) + α∙ (Wt - wt-i)
/*Sum of stochastic gradient step and mo-
mentum*/
4:	t — t + 1
Ensure: wt	/*Return the last iterate*/
Algorithm 2 NAG: Nesterov's AGD with a SFO
Require: Initial wo, stepsize δ, momentum ɑ
1:	vo — wo； t - 0	/*Set vo to wo*/
2:	while wt not converged do
3:	vt+ι — Wt — δ ∙ V ft (w t) /*SGD step*/
4:	wt+1 = (1 + α)vt+1 - αvt /*Sum of SGD
step and previous iterate*/
5:	t — t + 1
Ensure: wt	/*Return the last iterate*/
pirical evidence through training deep residual networks on cifar-10, with a batch size of 8
(see Section 5.3).
4.	We present an intuitive and easier to tune version of ASGD (see Section 4) and show
that ASGD can provide significantly faster convergence to a reasonable accuracy than
SGD, HB, NAG, while still providing favorable or comparable asymptotic accuracy as
these methods, particularly on several deep learning problems.
Hence, the take-home message of this paper is: HB and NAG are not optimal in the SFO model. The
only reason for the superiority of momentum methods in practice is mini-batching. ASGD provides
a distinct advantage in training deep networks over SGD, HB and NAG.
2	Notation
We denote matrices by bold-face capital letters and vectors by lower-case letters. f(w) =
1/n Pi fi(w) denotes the function to optimize w.r.t. model parameters w. Vf (w) denotes exact
gradient of f at w while Vft(w) denotes a stochastic gradient of f. That is, Vft(wt) = Vfit (w)
where it is sampled uniformly at random from [1,...,n]. For linear regression, fi(w) = 0.5 ∙ (b —
hw, aii)2 where bi ∈ <is the target and ai ∈ <d is the covariate, and Vft(wt) = -(bt -hwt, ati)at.
In this case, H = E [aa>] denotes the Hessian of f and K = ：1(H) denotes it,s condition number.
Algorithm 1 provides a pseudo-code of HB method (Polyak, 1964). wt-wt-1 is the momentum term
and α denotes the momentum parameter. Next iterate wt+1 is obtained by a linear combination of
the SGD update and the momentum term. Algorithm 2 provides pseudo-code ofa stochastic version
of the most commonly used form of Nesterov’s accelerated gradient descent (Nesterov, 1983).
3	Suboptimality of Heavy Ball Method
In this section, we show that there exists linear regression problems where the performance
of HB (Algorithm 1) is no better than that of SGD, while ASGD significantly improves upon SGD’s
performance. Let us now describe the problem instance.
Fix w* ∈ R2 and let (a, b)〜D be a sample from the distribution such that:
∫σι ∙ Z ∙ eι w.p. 0.5
[σ2 ∙ z ∙ e2 w.p. 0.5,
and b = hw* , ai ,
where e1 , e2 ∈ R2 are canonical basis vectors, σ1 > σ2 > 0. Let z be a random variable such
that E [z2] = 2 and E [z4] = 2c ≥ 4. Hence, we have: E [(a(i))2] = σi2, E [(a(i))4] = cσi4, for
i = 1, 2. Now, our goal is to minimize:
f (w) def 0.5 ∙ E [(<w*,ai - b)2i , Hessian H def E [aa>] = σ2，2 .
Let κ and K denote the computational and statistical condition numbers - see Jain et al. (2017)
2
for definitions. For the problem above, we have K = T and K = c. Then we obtain following
σ2
convergence rates for SGD and ASGD when applied to the above given problem instance:
3
Published as a conference paper at ICLR 2018
Algorithm 3 Accelerated stochastic gradient descent - ASGD
Input: Initial wo, short step δ, long step parameter K ≥ 1, statistical advantage parameter ξ ≤ √κ
1:	Wo — wo; t - 0	/*Set running average to w0*/
2:	a J 1 - 0.72∙ξ	/*Set momentum value*/
κ
3:	while wt not converged do
4:	Wt+ι J α ∙ Wt + (1 - α) ∙ (Wt - 篙 ∙ ▽ ft(wt))	/*Update the running average as a
weighted average of previous running average and a long step gradient */
5:	wt+ι	J	0.7+(.7-α)	∙	(Wt	- δ ∙	Vft(wt))	+	0.7+-1α-α)	∙	Wt+ι	/*Update the iterate as
weighted average of current running average and short step gradient*/
6:	t J t + 1
Output: Wt	/*Return the last iterate*/ * 2
Corollary 1 (of Theorem 1 of Jain et al. (2016)). Let WtSGD be the tth iterate of SGD on the above
problem with starting point wo and stepsize ^ɪɪ. The error of WSGD can be bounded as,
E f (wSGD)] - f (w*) ≤ exp (-t) (f(Wo)- f(W*))
On the other hand, ASGD achieves the following superior rate.
Corollary 2 (of Theorem 1 of Jain et al. (2017)). Let WtASGD be the tth iterate of ASGD on the
above problem with starting point Wo and appropriate parameters. The error of WtASGD can be
bounded as,
E [f (_waSGD)] - f (w*) ≤ Poly(K) exp
(√^) (f (WO) - f (WJ ).
KK
2
Note that for a given problem/input distribution K = C is a constant while K = T can be arbitrarily
σ2
large. Note that κ > K = c. Hence, ASGD improves upon rate of SGD by a factor of √K. The
following proposition, which is the main result of this section, establishes that HB (Algorithm 1)
cannot provide a similar improvement over SGD as what ASGD offers. In fact, we show no matter
the choice of parameters of HB, its performance does not improve over SGD by more than a constant.
Proposition 3. Let WtHB be the tth iterate ofHB (Algorithm 1) on the above problem with starting
point Wo. For any choice of stepsize δ and momentum α ∈ [0, 1], ∃T large enough such that ∀t ≥ T,
we have,
E [f (wHB)] - f (w*) ≥ C(κ, δ, α) ∙ exp
-500t
(J(WO)- f (w*)),
K
where C(K, δ, α) depends on K, δ and α (but not on t).
Thus, to obtain W s.t. kwb - w*k ≤ e, HB requires Ω(κ log ɪ) samples and iterations. On the
other hand, ASGD can obtain e-approximation to w* in O(√κ log Klog ɪ) iterations. We note that
the gains offered by ASGD are meaningful when K > O(c) (Jain et al., 2017); otherwise, all the
algorithms including SGD achieve nearly the same rates (upto constant factors). While we do not
prove it theoretically, we observe empirically that for the same problem instance, NAG also obtains
nearly same rate as HB and SGD. We conjecture that a lower bound for NAG can be established
using a similar proof technique as that of HB (i.e. Proposition 3). We also believe that the constant
in the lower bound described in proposition 3 can be improved to some small number (≤ 5).
4 Algorithm
We will now present and explain an intuitive version of ASGD (pseudo code in Algorithm 3). The
algorithm takes three inputs: short step δ, long step parameter K and statistical advantage parameter
ξ. The short step δ is precisely the same as the step size in SGD, HB or NAG. For convex problems,
this scales inversely with the smoothness of the function. The long step parameter K is intended
4
Published as a conference paper at ICLR 2018
to give an estimate of the ratio of the largest and smallest curvatures of the function; for convex
functions, this is just the condition number. The statistical advantage parameter ξ captures trade
off between statistical and computational condition numbers - in the deterministic case, ξ = √K
and ASGD is equivalent to NAG, while in the high stochasticity regime, ξ is much smaller. The
algorithm maintains two iterates: descent iterate Wt and a running average Wt. The running average
is a weighted average of the previous average and a long gradient step from the descent iterate, while
the descent iterate is updated as a convex combination of short gradient step from the descent iterate
and the running average. The idea is that since the algorithm takes a long step as well as short step
and an appropriate average of both of them, it can make progress on different directions at a similar
pace. Appendix B shows the equivalence between Algorithm 3 and ASGD as proposed in Jain et al.
(2017). Note that the constant 0.7 appearing in Algorithm 3 has no special significance. Jain et al.
(2017) require it to be smaller than ,1/6 but any constant smaller than 1 seems to work in practice.
5 Experiments
We now present our experimental results exploring performance of SGD, HB, NAG and ASGD. Our
experiments are geared towards answering the following questions:
•	Even for linear regression, is the suboptimality of HB restricted to specific distributions in
Section 3 or does it hold for more general distributions as well? Is the same true of NAG?
•	What is the reason for the superiority of HB and NAG in practice? Is it because momen-
tum methods have better performance that SGD for stochastic gradients or due to mini-
batching? Does this superiority hold even for small minibatches?
•	How does the performance of ASGD compare to that of SGD, HB and NAG, when training
deep networks?
Section 5.1 and parts of Section 5.2 address the first two questions. Section 5.2 and 5.3 address
Question 2 partially and the last question. We use Matlab to conduct experiments presented in
Section 5.1 and use PyTorch (pytorch, 2017) for our deep networks related experiments. Pytorch
code implementing the ASGD algorithm can be found at https://github.com/rahulkidambi/AccSGD.
5.1 Linear Regression
In this section, we will present results on performance of the four optimization methods (SGD,
HB, NAG, and ASGD) for linear regression problems. We consider two different class of linear
regression problems, both of them in two dimensions. Given κ which stands for condition number,
we consider the following two distributions:
Discrete: a = eι w.p. 0.5 and a = K ∙ e2 With 0.5; ei is the ith standard basis vector.
Gaussian : a ∈ R2 is distributed as a Gaussian random vector with covariance matrix
10
0 1 .
We fix a randomly generated w* ∈ R2 and for both the distributions above, We let b =(w*,a).
We vary κ from {24, 25, ..., 212} and for each κ in this set, we run 100 independent runs of all four
methods, each for a total of t = 5κ iterations. We define that the algorithm converges if there is no
error in the second half (i.e. after 2.5κ updates) that exceeds the starting error - this is reasonable
since we expect geometric convergence of the initial error.
Unlike ASGD and SGD, we do not know optimal learning rate and momentum parameters for NAG
and HB in the stochastic gradient model. So, we perform a grid search over the values of the
learning rate and momentum parameters. In particular, we lay a 10 × 10 grid in [0, 1] × [0, 1] for
learning rate and momentum and run NAG and HB. Then, for each grid point, we consider the subset
of 100 trials that converged and computed the final error using these. Finally, the parameters that
yield the minimal error are chosen for NAG and HB, and these numbers are reported. We measure
convergence performance of a method using: *
log(f(w0)) - log(f (wt))
rate —
(1)
t
5
Published as a conference paper at ICLR 2018
Figure 1: Plot of 1/rate (refer equation (1)) vs condition number (κ) for various methods for the
linear regression problem. Discrete distribution in the left, Gaussian to the right.
Algorithm	Slope - discrete	Slope - Gaussian
SGD	0.9302 =	0.8745
HB	0.8522	0.8769
NAG	0.98	0.9494
ASGD	0.5480	0.5127
Table 1: Slopes (i.e. γ) obtained by fitting a line to the curves in Figure 1. A value of γ indicates
that the error decays at a rate of exp (-t). A smaller value of Y indicates a faster rate of error decay.
We compute the rate (1) for all the algorithms with varying condition number κ. Given a rate vs κ
plot for a method, we compute it’s slope (denoted as γ) using linear regression. Table 1 presents the
estimated slopes (i.e. γ) for various methods for both the discrete and the Gaussian case. The slope
values clearly show that the rate of SGD, HB and NAG have a nearly linear dependence on K while
that of ASGD seems to scale linearly with √κ.
5.2	Deep Autoencoders for MNIST
In this section, we present experimental results on training deep autoencoders for the mnist dataset,
and we closely follow the setup of Hinton & Salakhutdinov (2006). This problem is a standard
benchmark for evaluating the performance of different optimization algorithms e.g., Martens (2010);
Sutskever et al. (2013); Martens & Grosse (2015); Reddi et al. (2017). The network architecture
follows previous work (Hinton & Salakhutdinov, 2006) and is represented as 784 - 1000 - 500 -
250-30-250-500- 1000 - 784 with the first and last 784 nodes representing the input and output
respectively. All hidden/output nodes employ sigmoid activations except for the layer with 30 nodes
which employs linear activations and we use MSE loss. Initialization follows the scheme of Martens
(2010), also employed in Sutskever et al. (2013); Martens & Grosse (2015). We perform training
with two minibatch sizes -1 and 8. The runs with minibatch size of 1 were run for 30 epochs while
the runs with minibatch size of 8 were run for 50 epochs. For each of SGD, HB, NAG and ASGD,
a grid search over learning rate, momentum and long step parameter (whichever is applicable) was
done and best parameters were chosen based on achieving the smallest training error in the same
protocol followed by Sutskever et al. (2013). The grid was extended whenever the best parameter
fell at the edge of a grid. For the parameters chosen by grid search, we perform 10 runs with
different seeds and averaged the results. The results are presented in Figures 2 and 3. Note that the
final loss values reported are suboptimal compared to those in published literature e.g., Sutskever
et al. (2013); while Sutskever et al. (2013) report results after 750000 updates with a large batch size
of 200 (which implies a total of 750000 × 200 = 150M gradient evaluations), whereas, our results
are after 1.8M updates of SGD with a batch size 1 (which is just 1.8M gradient evaluations).
Effect of minibatch sizes: While HB and NAG decay the loss faster compared to SGD for a mini-
batch size of8 (Figure 2), this superior decay rate does not hold for a minibatch size of 1 (Figure 3).
This supports our intuitions from the stochastic linear regression setting, where we demonstrate
that HB and NAG are suboptimal in the stochastic first order oracle model.
6
Published as a conference paper at ICLR 2018
Figure 2: Training loss (left) and test loss (right) while training deep autoencoder for mnist with
minibatch size 8. Clearly, ASGD matches performance of NAG and outperforms SGD on the test
data. HB also outperforms SGD.
Figure 3: Training loss (left) and test loss (right) while training deep autoencoder for mnist with
minibatch size 1. Interestingly, SGD, HB and NAG, all decrease the loss at a similar rate,
while ASGD decays at a faster rate.
Comparison of ASGD with momentum methods: While ASGD performs slightly better than
NAG for batch size 8 in the training error (Figure 2), ASGD decays the error at a faster rate compared
to all the three other methods for a batch size of 1 (Figure 3).
5.3	Deep Residual Networks for CIFAR-10
We will now present experimental results on training deep residual networks (He et al., 2016b) with
pre-activation blocks He et al. (2016a) for classifying images in cifar-10 (Krizhevsky & Hinton,
2009); the network we use has 44 layers (dubbed preresnet-44). The code for this section was down-
loaded from preresnet (2017). One of the most distinct characteristics of this experiment compared
to our previous experiments is learning rate decay. We use a validation set based decay scheme,
wherein, after every 3 epochs, we decay the learning rate by a certain factor (which we grid search
on) if the validation zero one error does not decrease by at least a certain amount (precise numbers
are provided in the appendix since they vary across batch sizes). Due to space constraints, we present
only a subset of training error plots. Please see Appendix C.3 for some more plots on training errors.
Effect of minibatch sizes: Our first experiment tries to understand how the performance
of HB and NAG compare with that of SGD and how it varies with minibatch sizes. Figure 4 presents
the test zero one error for minibatch sizes of 8 and 128. While training with batch size 8 was done
for 40 epochs, with batch size 128, it was done for 120 epochs. We perform a grid search over all
parameters for each of these algorithms. See Appendix C.3 for details on the grid search parameters.
We observe that final error achieved by SGD, HB and NAG are all very close for both batch sizes.
While NAG exhibits a superior rate of convergence compared to SGD and HB for batch size 128,
this superior rate of convergence disappears for a batch size of 8.
Comparison of ASGD with momentum methods: The next experiment tries to understand
how ASGD compares with HB and NAG. The errors achieved by various methods when we do
7
Published as a conference paper at ICLR 2018
Figure 4: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value
for batch size 8 (right) for SGD, HB and NAG.
Figure 5: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value
for batch size 8 (right) for ASGD compared to HB. In the above plots, both ASGD and ASGD-Hb-
Params refer to ASGD run with the learning rate and decay schedule of HB. ASGD-Fully-Optimized
refers to ASGD where learning rate and decay schedule were also selected by grid search.
grid search over all parameters are presented in Table 2. Note that the final test errors for batch size
128 are better than those for batch size 8 since the former was run for 120 epochs while the latter
was run only for 40 epochs (due to time constraints).
Algorithm	Final test error - batch size 128	Final test error - batch size 8
SGD	8.32 ± 0.21	9.57 ± 0.18
HB	7.98 ± 0.19	9.28 ± 0.25
NAG	7.63 ± 0.18	9.07 ± 0.18
ASGD	7.23 ± 0.22	8.52 ± 0.16
Table 2: Final test errors achieved by various methods for batch sizes of 128 and 8. The hyperpa-
rameters have been chosen by grid search.
While the final error achieved by ASGD is similar/favorable compared to all other methods, we
are also interested in understanding whether ASGD has a superior convergence speed. For this
experiment, we need to address the issue of differing learning rates used by various algorithms and
different iterations where they decay learning rates. So, for each of HB and NAG, we choose the
learning rate and decay factors by grid search, use these values for ASGD and do grid search only
over long step parameter κ and momentum α for ASGD. The results are presented in Figures 5
and 6. For batch size 128, ASGD decays error at a faster rate compared to both HB and NAG.
For batch size 8, while we see a superior convergence of ASGD compared to NAG, we do not see
this superiority over HB. The reason for this turns out to be that the learning rate for HB, which
we also use for ASGD, turns out to be quite suboptimal for ASGD. So, for batch size 8, we also
compare fully optimized (i.e., grid search over learning rate as well) ASGD with HB. The superiority
of ASGD over HB is clear from this comparison. These results suggest that ASGD decays error at a
faster rate compared to HB and NAG across different batch sizes.
6	Related Work
First order oracle methods: The primary method in this family is Gradient Descent (GD) (Cauchy,
1847). As mentioned previously, GD is suboptimal for smooth convex optimization (Nesterov,
8
Published as a conference paper at ICLR 2018
Figure 6: Test zero one loss for batch size 128 (left), batch size 8 (center) and training function value
for batch size 8 (right) for ASGD compared to NAG. In the above plots, ASGD was run with the
learning rate and decay schedule of NAG. Other parameters were selected by grid search.
2004), and this is addressed using momentum methods such as the Heavy Ball method (Polyak,
1964) (for quadratics), and Nesterov’s Accelerated gradient descent (Nesterov, 1983).
Stochastic first order methods and noise stability: The simplest method employing the SFO is
SGD (Robbins & Monro, 1951); the effectiveness of SGD has been immense, and its applicability
goes well beyond optimizing convex objectives. Accelerating SGD is a tricky proposition given the
instability of fast gradient methods in dealing with noise, as evidenced by several negative results
which consider statistical (Proakis, 1974; Polyak, 1987; Roy & Shynk, 1990), numerical (Paige,
1971; Greenbaum, 1989) and adversarial errors (d’Aspremont, 2008; Devolder et al., 2014). A
result of Jain et al. (2017) developed the first provably accelerated SGD method for linear regression
which achieved minimax rates, inspired by a method of Nesterov (2012b). Schemes of Ghadimi
& Lan (2012; 2013); Dieuleveut et al. (2016), which indicate acceleration is possible with noisy
gradients do not hold in the SFO model satisfied by algorithms that are run in practice (see Jain et al.
(2017) for more details).
While HB (Polyak, 1964) and NAG (Nesterov, 1983) are known to be effective in case of exact first
order oracle, for the SFO, the theoretical performance of HB and NAG is not well understood.
Understanding Stochastic Heavy Ball: Understanding HB’s performance with inexact gradients
has been considered in efforts spanning several decades, in many communities like controls, opti-
mization and signal processing. Polyak (1987) considered HB with noisy gradients and concluded
that the improvements offered by HB with inexact gradients vanish unless strong assumptions on
the inexactness was considered; an instance of this is when the variance of inexactness decreased
as the iterates approach the minimizer. Proakis (1974); Roy & Shynk (1990); Sharma et al. (1998)
suggest that the improved non-asymptotic rates offered by stochastic HB arose at the cost of worse
asymptotic behavior. We resolve these unquantified improvements on rates as being just constant
factors over SGD, in stark contrast to the gains offered by ASGD. Loizou & RiChtarik (2017) state
their method as Stochastic HB but require stochastic gradients that nearly behave as exact gradients;
indeed, their rates match that of the standard HB method (Polyak, 1964). Such rates are not infor-
mation theoretically possible (see Jain et al. (2017)), especially with a batch size of 1 or even with
constant sized minibatches.
Accelerated and Fast Methods for finite-sums: There have been developments pertaining to faster
methods for finite-sums (also known as offline stochastic optimization): amongst these are methods
such as SDCA (Shalev-Shwartz & Zhang, 2012), SAG (Roux et al., 2012), SVRG (Johnson &
Zhang, 2013), SAGA (Defazio et al., 2014), which offer linear convergence rates for strongly con-
vex finite-sums, improving over SGD’s sub-linear rates (Rakhlin et al., 2012). These methods have
been improved using accelerated variants (Shalev-Shwartz & Zhang, 2014; Frostig et al., 2015a; Lin
et al., 2015; Defazio, 2016; Allen-Zhu, 2016). Note that these methods require storing the entire
training set in memory and taking multiple passes over the same for guaranteed progress. Further-
more, these methods require computing a batch gradient or require memory requirements (typically
Ω(∣ training data points|)). For deep learning problems, data augmentation is often deemed neces-
sary for achieving good performance; this implies computing quantities such as batch gradient (or
storage necessities) over this augmented dataset is often infeasible. Such requirements are miti-
gated by the use of simple streaming methods such as SGD, ASGD, HB, NAG. For other technical
distinctions between the offline and online stochastic methods refer to Frostig et al. (2015b).
9
Published as a conference paper at ICLR 2018
Practical methods for training deep networks: Momentum based methods employed with
stochastic gradients (Sutskever et al., 2013) have become standard and very popular in practice.
These schemes tend to outperform standard SGD on several important practical problems. As previ-
ously mentioned, we attribute this improvement to effect of mini-batching rather than improvement
offered by HB or NAG in the SFO model. Schemes such as Adagrad (Duchi et al., 2011), RM-
SProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) represent an important and useful
class of algorithms. The advantages offered by these methods are orthogonal to the advantages of-
fered by fast gradient methods; it is an important direction to explore augmenting these methods
with ASGD as opposed to standard HB or NAG based acceleration schemes.
Chaudhari et al. (2017) proposed Entropy-SGD, which is an altered objective that adds a local strong
convexity term to the actual empirical risk objective, with an aim to improve generalization. How-
ever, we do not understand convergence rates for convex problems or the generalization ability of
this technique in a rigorous manner. Chaudhari et al. (2017) propose to use SGD in their procedure
but mention that they employ the HB/NAG method in their implementation for achieving better per-
formance. Naturally, we can use ASGD in this context. Path normalized SGD (Neyshabur et al.,
2015) is a variant of SGD that alters the metric on which the weights are optimized. As noted in
their paper, path normalized SGD could be improved using HB/NAG (or even the ASGD method).
7	Conclusions and Future Directions
In this paper, we show that the performance gain of HB over SGD in stochastic setting is attributed to
mini-batching rather than the algorithm’s ability to accelerate with stochastic gradients. Concretely,
we provide a formal proof that for several easy problem instances, HB does not outperform SGD
despite large condition number of the problem; we observe this trend for NAG in our experiments.
In contrast, ASGD (Jain et al., 2017) provides significant improvement over SGD for these problem
instances. We observe similar trends when training a resnet on cifar-10 and an autoencoder on
mnist. This work motivates several directions such as understanding the behavior of ASGD on
domains such as NLP, and developing automatic momentum tuning schemes (Zhang et al., 2017).
Acknowledgments
Sham Kakade acknowledges funding from NSF Awards CCF-1703574 and CCF-1740551.
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. CoRR,
abs/1603.05953, 2016.
Leon BottoU and Olivier Bousquet. The tradeoffs of large scale learning. In NIPS 20, 2007.
Louis Augustin Cauchy. Methode generale pour la resolution des Systemes d'equations SimUltanees.
C. R. Acad. Sci. Paris, 1847.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. CoRR, abs/1611.01838, 2017.
Alexandre d’Aspremont. Smooth optimization with approximate gradient. SIAM Journal on Opti-
mization ,19(3):1171-1183,2008.
Aaron Defazio. A simple practical accelerated method for finite sums. Advances in Neural Infor-
mation Processing Systems 29 (NIPS 2016), 2016.
Aaron Defazio, Francis R. Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS 27, 2014.
Olivier Devolder, Franccois Glineur, and Yurii E. Nesterov. First-order methods of smooth convex
optimization with inexact oracle. Mathematical Programming, 146:37-75, 2014.
10
Published as a conference paper at ICLR 2018
Aymeric Dieuleveut, Nicolas Flammarion, and Francis R. Bach. Harder, better, faster, stronger
convergence rates for least-squares regression. CoRR, abs/1602.05419, 2016.
John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal ofMachine Learning Research, 12:2121-2159, 2011.
Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. Un-regularizing: approximate proximal
point and faster stochastic algorithms for empirical risk minimization. In ICML, 2015a.
Roy Frostig, Rong Ge, Sham M. Kakade, and Aaron Sidford. Competing with the empirical risk
minimizer in a single pass. In COLT, 2015b.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on
Optimization, 2012.
Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly con-
vex stochastic composite optimization, ii: shrinking procedures and optimal algorithms. SIAM
Journal on Optimization, 2013.
Anne Greenbaum. Behavior of slightly perturbed lanczos and conjugate-gradient recurrences. Lin-
ear Algebra and its Applications, 1989.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV (4), Lecture Notes in Computer Science, pp. 630-645. Springer, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016b.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Par-
allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint
arXiv:1610.03774, 2016.
Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerat-
ing stochastic gradient descent. arXiv preprint arXiv:1704.08227, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In NIPS 26, 2013.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
HongzhoU Lin, Julien Mairal, and Zald Harchaoui. A universal catalyst for first-order optimization.
In NIPS, 2015.
Nicolas Loizou and Peter Richtarik. Linearly convergent stochastic heavy ball method for minimiz-
ing generalization error. 2017.
James Martens. Deep learning via hessian-free optimization. In International conference on machine
learning, 2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, 2015.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2).
In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming
Series B, 2012a.
11
Published as a conference paper at ICLR 2018
Yurii E. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of
Applied Optimization. Kluwer Academic Publishers, 2004.
Yurii E. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems.
SIAMJournal on Optimization, 22(2):341-362, 2012b.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized opti-
mization in deep neural networks. CoRR, abs/1506.02617, 2015.
Christopher C. Paige. The computation of eigenvalues and eigenvectors of very large sparse matri-
ces. PhD Thesis, University of London, 1971.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Boris T. Polyak. Introduction to Optimization. Optimization Software, 1987.
preresnet. Preresnet-44 for cifar-10. https://github.com/D-X-Y/ResNeXt-DenseNet,
2017. Accessed: 2017-10-25.
John G. Proakis. Channel identification for high speed digital communications. IEEE Transactions
on Automatic Control, 1974.
pytorch. Pytorch. https://github.com/pytorch, 2017. Accessed: 2017-10-25.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In ICML, 2012.
Sashank Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdi-
nov, and Alexander Smola. A generic approach for escaping saddle points. arXiv preprint
arXiv:1709.01434, 2017.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, vol. 22, 1951.
Nicolas Le Roux, Mark Schmidt, and Francis R. Bach. A stochastic gradient method with an expo-
nential convergence rate for strongly-convex optimization with finite training sets. In NIPS 25,
2012.
Sumit Roy and John J. Shynk. Analysis of the momentum lms algorithm. IEEE Transactions on
Acoustics, Speech and Signal Processing, 1990.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. CoRR, abs/1209.1873, 2012.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. In ICML, 2014.
Rajesh Sharma, William A. Sethares, and James A. Bucklew. Analysis of momentum adaptive
filtering algorithms. IEEE Transactions on Signal Processing, 1998.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012.
Jian Zhang, Ioannis Mitliagkas, and Christopher R. Yellowfin and the art of momentum tuning.
CoRR, abs/1706.03471, 2017.
12
Published as a conference paper at ICLR 2018
A Suboptimality of HB: Proof of Proposition 3
Before proceeding to the proof, we introduce some additional notation. Let θt(+j)1 denote the con-
catenated and centered estimates in the jth direction for j = 1, 2.
θ(j) =f [w(+)ι -(W*0叫	∙	2
θt+1= Wtj)-(W*" j = 1, 2.
Since the distribution over x is such that the coordinates are decoupled, we see that θt(+j)1 can be
written in terms of θt(j) as:
θt(+j)1=Abt(+j)1θ(tj), withAbt(+j)1= 1 +α-1δ(at(+j)1)2	-0α .
denote the covariance matrix of θ(tj+)1. We have Φt(+j)1 = B(j)Φt(j)
Let Φt+)ι =f E [θt+)ι 乳 θt+)ι]
with, B(j) defined as
	E [(1 + α — δ(a(j))2)2]	E [-α(1 + α — δ(a(j))2)]	E [-α(1 + α - δ(a(j))2]	α2
B(j) d=ef	E [(1 + α - δ(a(j))2)]	0	-α	0
	E (1 + α — δ(a(j))2)	-α 10 (1 + α - δσ2)2 + (c - 1)(δσ2)2 -α(1 + α - δσ2) (1+α-δσj2)	0 (1 + α - δσj )	-α 10	0 0 —α(1 + α — δσj) Oj -ɑ	0 0	0 . 00	0 0
We prove Proposition 3 by showing that for any choice of stepsize and momentum, either of the two
holds:
•	B(1) has an eigenvalue larger than 1, or,
•	the largest eigenvalue of B(2) is greater than 1 - 500.
This is formalized in the following two lemmas.
Lemma 4. Ifthe stepsize δ is such that δσ2 ≥ 3；：)1, then B(I) has an eigenvalue ≥ 1.
2(1-α2)
Lemma 5. Ifthe StePSize δ is such that δσ2 < ：+(：_2)1, then B(2) has an eigenvalue ofmagnιtude
≥ 1 - 500.
κ
Given this notation, we can now consider the j th dimension without the superscripts; when needed,
1 + α - x, we have:
they will be made clear in the exposition. Denoting		x d=ef δσ2 and t d=ef
	-P + (c — 1)x2	——at ——at α2
B=	t	0	-α 0
	t	-a 0	0
	1	0	00
A.1 Proof
The analysis goes via computation of the characteristic polynomial ofB and evaluating it at different
values to obtain bounds on its roots.
Lemma 6. The characteristic polynomial of B is:
D(z) = z4 - (t2 + (c - 1)x2)z3 + (2αt2 - 2α2)z2 + (-t2 + (c - 1)x2)α2z + α4.
13
Published as a conference paper at ICLR 2018
Proof. We first begin by writing out the expression for the determinant:
Det(B — ZI) =	P + (c ——1)χ2 ——z ——at	——at	α2 t	—z	—a	0 t	—a	—Z	0 . 1	0	0	—z
expanding along the first column, we have:
Det(B — ZI)=(产	+	(C — 1)χ2	— z)(a2z	— Zɜ)	—	t(—atZ + O1 tz) + t(—αt(αz) +	Z ∙ 0tz) —	(Z ∙ O1 z	— a，)
=	(t2	+	(c - 1)x2	- z)(α2z	- z3)	-	2t(α2tz - αtz2) - (α2 z2 - α4).
Expanding the terms yields the expression in the lemma.	□
The next corollary follows by some simple arithmetic manipulations.
Corollary 7. Substituting z = 1 - τ in the characteristic equation of Lemma 6, we have:
D(1 - τ) = τ4 + τ3 (-4 + t2 + (c - 1)x2) + τ2(6 - 3t2 - 3(c - 1)x2 - 2α2 + 2αt2)
+ τ(-4 + 3t2 + 3(c - 1)x2 + 4α2 - 4αt2 - (c - 1)x2α2 + t2α2)
+ (1 - t2 - (c - 1)x2 - 2α2 + 2αt2 + (c - 1)x2α2 - t2α2 + α4)
= τ4 + τ3[-(3 + α)(1 - α) - 2x(1 + α) + cx2]
+ τ2 [(3 - 4α - α2 + 2α3) - 2x(1 + α)(2α - 3) + x2 (2α - 3c)]
+ τ [-(1 - α)2 (1 - α2) - 2x(3 - α)(1 - α2) + x2 (3c - 4α + (2 - c)α2)]
+ x(1 - α)[2(1 - α2) - x(c + (c - 2)α)].	(2)
Proof of Lemma 4. The first observation necessary to prove the lemma is that the characteristic poly-
nomial D(z) approaches ∞ as z → ∞, i.e., limz→∞ D(z) = +∞.
Next, we evaluate the characteristic polynomial at 1, i.e. compute D(1). This follows in a straight-
forward manner from corollary (7) by substituting τ = 0 in equation (2), and this yields,
D(1) = (1 — α)x ∙(2(1 — α2) — x(1 — α) — (C — 1)x(1 + α)).
As α < 1, x = δσ2 > 0, we have the following by setting D(1) ≤ 0 and solving for x:
X ≥ 2(1-ɑ2)
^x ≥r	.
c+ (c — 2)α
Since D(1) ≤ 0 and D(Z) ≥ 0 as Z → ∞, there exists a root of D(∙) which is ≥ 1.	□
Remark 8. The above characterization is striking in the sense that for any c > 1, increasing the
momentum parameter α naturally requires the reduction in the step size δ to permit the convergence
of the algorithm, which is not observed when fast gradient methods are employed in deterministic
optimization. For instance, in the case of deterministic optimization, setting c = 1 yields δσ12 <
2(1 + α). On the other hand, when employing the stochastic heavy ball method with x(j) = 2σj2,
we have the condition that C = 2, and this implies, δσ2 < 2(I-α ) = 1 一 a2.
We now prove Lemma 5. We first consider the large momentum setting.
Lemma 9. When the momentum parameter α is set such that 1 — 450∕κ ≤ α ≤ 1, B has an
eigenvalue of magnitude ≥ 1 — 4∣0.
Proof. This follows easily from the fact that det(B) = α4 = Qj4=1 λj (B) ≤ (λmax(B))4, thus
implying 1 — 450∕κ ≤ α ≤ ∣λmaχ(B)∣.	□
Remark 10. Note that the above lemma holds for any value of the learning rate δ, and holds for
every eigen direction ofH. Thus, for “large” values of momentum, the behavior of stochastic heavy
ball does degenerate to the behavior of stochastic gradient descent.
14
Published as a conference paper at ICLR 2018
We now consider the setting where momentum is bounded away from 1.
Corollary 11. Consider B(2) ,by substituting T = l∕κ, X = δλmin = c(δσ2)∕κ in equation (2) and
accumulating terms in varying powers of 1∕κ, we obtain:
+
def c3(δσ12)2l3	l4 - 2c(δσ12)l3(1 + α) + (2α - 3c)c2(δσ12)2l2
G(I) =---------------1-----------------------------------------------
κ5
κ4
-(3 + α)(1 — α)l3 — 2(1 + α)(2α — 3)c(δσ2 )l2 + (3c — 4α + (2 — c)α2)c2(δσ2)2l
+ --~~~..........................
(3 — 4α — α2 + 2α3)l2 — 2c(δσ2)l(3 — α)(1 — α2) — c2(δσ2)2(1 — α)(c + (c — 2)α)
+'----------一~K~~一~~—一
+ —(1 — α)2(1 — α2)l + 2c(δσ2)(1 — α)(1 — α2)	(,
κ
Lemma 12. Let 2 < c < 3000, 0 ≤ α ≤ 1 — 4K0,1 =1 + 2c(-σ1). Then, G(l) ≤ 0.
Proof. Since (δσ2) ≤。+式?，this implies ⅞⅛ ≤ °+(；-露 ≤ 4 ,thus implying, 1 ≤ l ≤ 9.
Substituting the value of l in equation (3), the coefficient of O(1∕κ) is —(1 — α)3(1 + α).
We will bound this term along with (3 — 4α — α2 + 2α3)l2∕κ2 = (1 — α)2 (3 + 2α)l2∕κ2 to obtain:
—(1 — α)3(1 + α)	(1 — α)2(3 + 2α)l2
K	+	K2	≤
≤
≤
—(1 — a)3(1 + a) + 405(1 — a)
K
(I—a产(蛆 ― (1 —
KK
(I—a产(蛆 — (1 —
KK
K2
α2)
、、	45 ∙ 4502
a)	≤ -——
K
2
where, we use the fact that a < 1, l ≤ 9. The natural implication of this bound is that the terms that
are lower order, such as O(1∕K4) and O(1∕K5) will be negative owing to the large constant above.
Let us verify that this is indeed the case by considering the terms having powers of O(1∕K4) and
O(1∕K5) from equation (3):
c3(δσ2)2l3	l4 — 2c(δσ2)l3(1 + a) + (2a — 3c)c2(δσ2)2l2
K5	+	K4
45 ∙ 4502
K4
≤
c3(δσ2)2l3	l4	45 ∙ 4502
κ5	K κ4	κ4
cl3	(94 — (45 ∙ 4502))
― κ5 十	κ4
93 c + 94 — (45 ∙ 4502)
K4
The expression above evaluates to ≤ 0 given an upperbound on the value of c. The expression above
follows from the fact that l ≤ 9, K ≥ 1.
Next, consider the terms involving O(1∕K3) and O(1∕K2), in particular,
(3c — 4a + (2 — c)a2 )c2 (δσ12 )2l	c2 (δσ12 )2 (1 — a)(c + (c — 2)a)
--------------Ξ--------------- -------------- 	Ξ--------
K3-------------------------------------------------------K2
≤ cWf (- (1 - a)(c +(c - 2)a))
KK
≤ cWf (码 — (1 — a)(c +(c - 2)a))
K2	K
≤ c2(δσ2)2 (码—(1 - a)c)
K2	K
< c3(δσ2)2(51	450)
K2	K	K
≤ c3∙σ2)2 • -405 ≤ 0
K2 K
15
Published as a conference paper at ICLR 2018
Next,
—2(1 + α)(2α — 3)c(δσ2)l2	2c(δσ2)l(3 — α)(1 — α2)
κ2
≤
2(1 + α)c(δσ2)l ( —(2ɑ — 3)1
κ2	κ
— (3 — α)(1 — α)
≤
≤
2(1 + α)c(δσ2)1 ( 31
κ2	κ
2(1 + α)c(δσ2)1 ( 31
κ2	κ
— 2(1 — α)
2 ・450 )
κ
≤
2(1 + α)c(δσ2)1 (3 . 27	2 . 450
κ2	κ κ
In both these cases, We used the fact that α ≤ 1 一 450 implying -(1 一 α) ≤ -45-. Finally, other
remaining terms are negative.	□
Before rounding up the proof of the proposition, We need the folloWing lemma to ensure that our
loWer bounds on the largest eigenvalue of B indeed affect the algorithm’s rates and are true irre-
spective of Where the algorithm is begun. Note that this alloWs our result to be much stronger than
typical optimization loWerbounds that rely on specific initializations to ensure a component along
the largest eigendirection of the update operator, for Which bounds are proven.
Lemma 13. For any starting iterate w0 = w*, the HB method produces a non-zero component
along the largest eigen direction of B.
Proof. We note that in a similar manner as other proofs, it suffices to argue for each dimension of
the problem separately. But before We start looking at each dimension separately, let us consider
the jth dimension, and detail the approach We use to prove the claim: the idea is to examine the
subspace spanned by covariance E [θ(j) 0 θ(j[ of the iterates θ0j), θj), θ”…,for every starting
iterate θ(0j) 6= [0, 0]> and prove that the largest eigenvector of the expected operator B(j ) is not
orthogonal to this subspace. This implies that there exists a non-zero component of E
in the largest eigen direction of B(j), and this decays at a rate that is at best λmax(B(j)).
Since B(j) ∈ R4×4, We begin by examining the expected covariance spanned by the iterates
θ(0j), θ(1j), θ(2j), θ(3j). Let w0(j) — (w*)(j) = w-(j1) — (w*)(j) = k(j). NoW, this implies θ(0j) =
k(jj ∙ [1,1]>. Then,
θj) = k(j)Aj)	∣"1"∣	, with	Aj)= 1 + α 一	'H?"	-α	, where Hj)	=	(aj))2.
This implies that k just appears as a scale factor. This in turn implies that in order to analyze
the subspace spanned by the covariance of iterates θ(0j) , θ(1j), ..., we can assume k(j) = 1 without
any loss in generality. This implies, θ(0j) = [1, 1]>. Note that with this in place, we see that we
can now drop the superscript j that represents the dimension, since the analysis decouples across
def
the dimensions j ∈ {1, 2}. Furthermore, let the entries of the vector θk be represented as θk =
[θki θk2]> Next, denote 1 + α 一 δHk =九.This implies,
θ2 = Ab 2θ1
Furthermore,
θ1 = Ab 1 θ0
—α
t2(t1 ——α)——α
t? 一 ɑ
■ ʌ
tι
1
t3(t2(tι — α) 一 α) 一 α(t] — α)
t2 (t1 一 α) 一 α
(4)
16
Published as a conference paper at ICLR 2018
Let Us consider the vectorized form of Φj = E [θj 0 θj], and We denote this as vec(Φj). Note that
vec(Φj) makes Φj become a column vector of size 4 × 1. Now, consider vec(Φj) forj = 0, 1, 2, 3
and concatenate these to form a matrix that We denote as D, i.e.
D = [vec(Φ0) vec(Φ1) vec(Φ2) vec(Φ3)] .
NoW, since We note that Φj is a symmetric 2 × 2 matrix, D shoUld contain tWo identical
roWs implying that it has an eigenvalUe that is zero and a corresponding eigenvector that is
[θ -1∕√2 1/y∕(2) θ] . It turns out that this is also an eigenvector of B with an eigenvalue
α. Note that det(B) = α4. This implies there are tWo cases that We need to consider: (i) When all
eigenvalues of B have the same magnitude (= α). In this case, we are already done, because there
exists at least one non zero eigenvalue of D and this should have some component along one of
the eigenvectors of B and we know that all eigenvectors have eigenvalues with a magnitude equal to
λmax(B). Thus, there exists an iterate which has a non-zero component along the largest eigendirec-
tion of B. (ii) the second case is the situation when we have eigenvalues with different magnitudes.
In this case, note that det(B) = α4 < (λmax(B))4 implying λmax(B) > α. In this case, we need
to prove that D spans a three-dimensional subspace; if it does, it contains a component along the
largest eigendirection of B which will round up the proof. Since we need to understand whether D
spans a three dimensional subspace, we can consider a different (yet related) matrix, which we call
R and this is defined as:
R
def
θ021	θ121	θ221
E	θ01θ02	θ11θ12 θ21θ22
θ022	θ122	θ222
Given the expressions for {θj }j3=0 (by definition of θ0 and using equation 4), we can substitute to
see that R has the following expression:
「1 E 3— α)2]
R = 1 E [t'ι — α]
11
E [(t2(t1 - α) - α)2]
E [((t2(tι - α) - a))(+i - α)]
Ed—ɑ)2]
If we compute and prove that det(R) 6= 0, we are done since that implies that R has three non-zero
eigenvalues.
This implies, we first define the following: let qγ = (t — γ)2 + (c — 1)x2. Then, R can be expressed
as:
qα	q0qα — 2αt(t — α) + α2
t — α tqα — α(t — α)
1	qα
qα	qα(q0 — qα) — 2αt(t — α) + α2
t — α	tqα — α(t — α) — (t — α)qα
10
qα — 1	qα (q0 — qα ) — 2αt(t — α) + α2
t — α — 1	tqα — α(t — α) — (t — α)qα
00
qα — 1	qα (q0 — qα ) — 2αt(t — α) + α2
t — α — 1	tqα — α(t — α) — (t — α)qα
00
Note: (i) qα — 1 = (t — α)2 — 1 + (c — 1)x2 = (1 — x)2 — 1 + (c — 1)x2 = —2x + x2 + (c — 1)x2 =
—2x + cx2.
(ii)	t — α — 1 = —x
(iii)	α(qα — (t —α)) = α((t-α)2 — (t—α) + (c-1)x2) = α((1-x)(—x) + (c-1)x2) = αx(-1+cx)
(iv) q0 — qα = t — (t — α) = α(2t — α) = 2tα — α .
Then,
(2αt — α )qα — 2αt(t — α) + α = 2tα(qα — (t — α)) + α (1 — qα )
=2ta(-x + cx2) — α2(-2x + cx2)
17
Published as a conference paper at ICLR 2018
= -2tαx + 2xα1 2 + 2tαcx2 - cα2x2
= 2αx(-t + α) + cαx (2t - α)
= -2αx(1 - x) + 2cαx2 (1 - x) + cα2x2
= 2αx(1 - x)(-1 + cx) + cα2x2.
Then,
det(R)= det(l0
x(cx - 2) 2αx(1 - x)(-1 + cx) + cα2x2
Then,
x2αdet
x3αdet
0
0
1
0
0
1
-x
0
(cx - 2)
-1
0
c
-1
0
αx(cx - 1)
0
cαx + 2(1 - x)(cx - 1)
cx - 1
0
cα - 2(cx - 1)
cx - 1
0
det(R) = x3α c(-1 + cx) - 2(-1 + cx) + cα
αx3 (c - 2)(-1 + cx) + cα
Note that this determinant can be zero when
(c - 2)(1 - cx)
α =-----------------
(5)
c
We show this is not possible by splitting our argument into two parts, one about the convergent
regime of the algorithm (where, δσ2 < c+1--2)0) and the other about the divergent regime.
Let us first provide a proof for the convergent regime of the algorithm. For this regime, let the chosen
δ be represented as δ+. Now, for the smaller eigen direction, X = δ+ λmin = cδ+σ2∕κ. Suppose ɑ
was chosen as per equation 5,
cα
C - 2
δ+ σ12
c2δ+σ12
1 ----------
κ
κα
c(c — 2).
We Will noW prove that δ+σ12
of the HB updates, i.e., δσ12 <
C (C - c02) is much larger than one allowed by the convergence
2(1-ɑ2)
2(1-α2) ≤
c+(c-2)α -
2(1-α2)
.In particular, if We prove that C (C
-ɪ) >
c-2 ) >
for any admissible value of α, We are done.
κ1
c
⇔κ
α
c ' c c — 2
κ κα
=C c — 2
κα κ κα
C-2 C C
- κα > 2C - 2Cα
2(1 - ɑ2)
c
2-2α2
2-2α2
—
C
—
C
—
>
>
K
>
⇔ 2cα - κα + (κ - 2c) > 0.
The two roots of this quadratic equation are α+ = 2C -
1 and α- = 1. Note that κ
~ .
κ = c;
≥
note that there is not much any method gains over SGD if κ = O(c). And, for any κ ≥ 4c, note,
α+ > α-, indicating that the above equation holds true if ɑ > α+ = 2C — 1 or if ɑ < α-= 1. The
latter condition is true and hence the proposition that δ+σ2 > c+；-02)、is true.
18
Published as a conference paper at ICLR 2018
We need to prove that the determinant does not vanish in the divergent regime for rounding up the
proof to the lemma.
Now, let Us consider the divergent regime of the algorithm, i.e., when, δσ2 > c+；-02)：. Further-
1Ca
more, for the larger eigendireCtion,the determinant is zero when δσ2 = c-2 = c -宣 (obtained
by substituting X = δσ, in equation 5). If we show that c+1--2)0 > c - c¾ for all admissible
values of c, we are done. We will explore this in greater detail:
2(1 — α2)	1 α
C + (c — 2)α〉C C — 2
⇔ 2(1 — α2) ≥ 1 +------α--------α — α2
C C-2
⇔ 1 — α2
⇔ C — 2C — α C + 2Cα
—4(c — 1)
C(C — 2)
≥ —4cq + 4α
≥
α
⇔ C2(1 — α2) — 2C(1 — α2 — 2α) — 4α ≥ 0.
considering the quadratic in the left hand size and solving it for C, we have:
	±	2(1 — α2 - 2α) ± p4(1 — α2 — 20)2 + 16α(1 — α2) C =	2(1 — α2) (1 — α2 - 2α) ± p(1 — α2 — 2α)2 + 4α(1 — a2) (1 — ɑ2) (1 — α2 - 2α) ± p1 + ɑ4 + 4α2 — 2α2 — 4α + 4a3 + 4α(1 — a2) (1 — ɑ2) (1 — α2 - 2α) 土 (1 + α2) (1 — ɑ2)
This holds true iff
or iff,	—	—2α(1 + α)	— 2α 1 — α	1 — α2	1 — a， 2(1 — α)	2 c ≥ c+ ==	= =	 C ≥ C	1 — a2	1 + a.
Which is true automatically since C > 2. This completes the proof of the lemma.
□
We are now ready to prove Lemma 5.
Proof of Lemma 5. Combining Lemmas 9 and 12, we see that no matter what stepsize and momen-
tum we choose, B(j) has an eigenvalue of magnitude at least 1 — 500 for some j ∈ {1,2}. This
proves the lemma.	□
B Equivalence of Algorithm 3 and ASGD
We begin by writing out the updates of ASGD as written out in Jain et al. (2017), which starts with
two iterates ab0 and d0, and from time t = 0, 1, ...T — 1 implements the following updates:
^
^
bt = α1abt + (1 — α1)dt
bt+1 = bt — δι57 ft+ι(bt)
~ . ^ , , , ^
Cbt = β1bbt + (1 — β1)dbt
^	ʌ	q ~	，个、
dt+1 = Ct — Yi V ft+ι(bt).
(6)
(7)
(8)
(9)
19
Published as a conference paper at ICLR 2018
Next, We specify the step sizes βι = c3∕√κe, αι = c3∕(c3 + β), γι = β∕(c3λmin) and δι = 1/R2,
where K = R2∕λmin. Note that the step sizes in the paper of Jain et al. (2017) with ci in their paper
set to 1 yields the step sizes above. NoW, substituting equation 8 in equation 9 and substituting the
value of γ1 , we have:
dt+1 = β1 bt -
β1 bt -
1
-ʌ----▽ ft+ι(bt) ) +(I - βι)bt
c3λmin
δκ
C-▽ ft+i(bt) } +(1 - β1)dt.
(10)
We see that dt+i is precisely the update of the running average w⅞+ι in the ASGD method employed
in this paper.
We now update bt to become bt+i and this can be done by writing out equation 6 at t + 1, i.e:
bt+i = αiabt+i + (1 - αi)dt+i
" C VV C ∕V^ ∖λ . ( Λ ∖ V
=αι (bt - διVft+ι(bt) I +(1 - αι)dt+ι.
(11)
By substituting the value of αi we note that this is indeed the update of the iterate as a convex
combination of the current running average and a short gradient step as written in this paper. In this
paper, we set c3 to be equal to 0.7, and any constant less than 1 works. In terms of variables, we
note that α in this paper’s algorithm description maps to 1 - βi.
C More details on experiments
In this section, we will present more details on our experimental setup.
C.1 Linear Regression
In this section, we will present some more results on our experiments on the linear regression prob-
lem. Just as in Appendix A, it is indeed possible to compute the expected error of all the algorithms
among SGD, HB, NAG and ASGD, by tracking certain covariance matrices which evolve as lin-
ear systems. For SGD, for instance, denoting ΦSGD = E [(wSGD - w*) 0 (WSGD - w*)], we
see that ΦtS+GiD = B ◦ ΦtSGD, where B is a linear operator acting on d × d matrices such that
B ◦ M d=ef M - δHM - δMH + δ2E hx, Mxi xx>]. Similarly, HB, NAG and ASGD also have
corresponding operators (see Appendix A for more details on the operator corresponding to HB).
The largest magnitude of the eigenvalues of these matrices indicate the rate of decay achieved by
the particular algorithm - smaller it is compared to 1, faster the decay.
We now detail the range of parameters explored for these results: the condition number κwas
varied from {24, 25, .., 228} for all the optimization methods and for both the discrete and gaussian
problem. For each of these experiments, we draw 1000 samples and compute the empirical estimate
of the fourth moment tensor. For NAG and HB, we did a very fine grid search by sampling 50
values in the interval (0, 1] for both the learning rate and the momentum parameter and chose the
parameter setting that yielded the smallest λmax (B) that is less than 1 (so that it falls in the range
of convergence of the algorithm). As for SGD and ASGD, we employed a learning rate of 1∕3 for
the Gaussian case and a step size of 0.9 for the discrete case. The statistical advantage parameter of
ASGD was chosen to be ,3κ∕2 for the Gaussian case and ,2κ∕3 for the Discrete case, and the
a long step parameters of 3κ and 2κ were chosen for the Gaussian and Discrete case respectively.
The reason it appears as if we choose a parameter above the theoretically maximal allowed value
of the advantage parameter is because the definition of κis different in this case. The κwe speak
about for this experiment is λmax∕λmin unlike the condition number for the stochastic optimization
problem. In a manner similar to actually running the algorithms (the results of whose are presented
in the main paper), we also note that we can compute the rate as in equation 1 and join all these rates
using a curve and estimate its slope (in the log scale). This result is indicated in table 3.
Figure 7 presents these results, where for each method, we did grid search over all parameters
and chose parameters that give smallest λmax. We see the same pattern as in Figure 1 from actual
20
Published as a conference paper at ICLR 2018
runs - SGD,HB and NAG all have linear dependence on condition number κ, while ASGD has a
dependence of √κ.
Figure 7: Expected rate of error decay (equation 1) vs condition number for various methods for the
linear regression problem. Left is for discrete distribution and right is for Gaussian distribution.
Algorithm	Slope - discrete	Slope - Gaussian
]SGD	0.9990 =	0.9995
HB	1.0340	0.9989
NAG	1.0627	1.0416
ASGD	0.4923	0.4906
Table 3: Slopes (i.e. γ) obtained by fitting a line to the curves in Figure 7. A value of γ indicates
that the error decays at a rate of exp (-Yt). A smaller value of Y indicates a faster rate of error decay.
C.2 Autoencoders for MNIST
We begin by noting that the learning rates tend to vary as we vary batch sizes, which is something
that is known in theory (Jain et al., 2016). Furthermore, we extend the grid especially whenever
our best parameters of a baseline method tends to land at the edge of a grid. The parameter ranges
explored by our grid search are:
Batch Size 1: (parameters chosen by running for 20 epochs)
•	SGD: learning rate: {0.01,0.01√10,0.1,0.1√10,1, √10,5,10,20,10√10,40,60,80,100.
•	NAG/HB: learning rate:	{0.01√10,0.1,0.1√10,1, √10,10},	momentum
{0, 0.5, 0.75, 0.9, 0.95, 0.97}.
•	ASGD: learning rate:	{2.5, 5}, long step {100.0, 1000.0}, advantage parameter
{2.5, 5.0, 10.0, 20.0}.
Batch Size 8: (parameters chosen by running for 50 epochs)
•	SGD: learning rate: {0.001,0.001√10,0.01,0.01√10,0.1,0.1√10,1, √10,5,10
,10√10,40,60,80,100,120,140}.
•	NAG/HB: learning rate:	{5.0,10.0,20.0,10√10,40,60},	momentum
{0, 0.25, 0.5, 0.75, 0.9, 0.95}.
•	ASGD: learning rate {40, 60}.	For a long step of 100, advantage parameters of
{	1.5, 2, 2.5, 5, 10, 20}. For a long step of 1000, we swept over advantage parameters of
{2.5, 5, 10}.
21
Published as a conference paper at ICLR 2018
C.3 Deep Residual Networks for CIFAR- 1 0
In this section, we will provide more details on our experiments on cifar-10, as well as present
some additional results. We used a weight decay of 0.0005 in all our experiments. The grid search
parameters we used for various algorithms are as follows. Note that the ranges in which parameters
such as learning rate need to be searched differ based on batch size (Jain et al., 2016). Furthermore,
we tend to extrapolate the grid search whenever a parameter (except for the learning rate decay
factor) at the edge of the grid has been chosen; this is done so that we always tend to lie in the
interior of the grid that we have searched on. Note that for the purposes of the grid search, we
choose a hold out set from the training data and add it in to the training data after the parameters are
chosen, for the final run.
Batch Size 8: Note: (i) parameters chosen by running for 40 epochs and picking the grid search
parameter that yields the smallest validation 0/1 error. (ii) The validation set decay scheme that we
use is that if the validation error does not decay by at least 1% every three passes over the data, we
cut the learning rate by a constant factor (which is grid searched as described below). The minimal
learning rate to use is fixed to be 6.25 × 10-5, so that we do not decay far too many times and curtail
progress prematurely.
•	SGD: learning rate: {0.0033, 0.01, 0.033, 0.1, 0.33}, learning rate decay factor {5, 10}.
•	NAG/HB: learning rate: {0.001, 0.0033, 0.01, 0.033}, momentum {0.8, 0.9, 0.95, 0.97},
learning rate decay factor {5, 10}.
•	ASGD: learning rate {0.01, 0.0330, 0.1}, long step {1000, 10000, 50000}, advantage pa-
rameter {5, 10}, learning rate decay factor {5, 10}.
Batch Size 128: Note: (i) parameters chosen by running for 120 epochs and picking the grid search
parameter that yields the smallest validation 0/1 error. (ii) The validation set decay scheme that we
use is that if the validation error does not decay by at least 0.2% every four passes over the data, we
cut the learning rate by a constant factor (which is grid searched as described below). The minimal
learning rate to use is fixed to be 1 × 10-3, so that we do not decay far too many times and curtail
progress prematurely.
•	SGD: learning rate: {0.01,0.03,0.09,0.27,0.81}, learning rate decay factor {2, √10, 5}.
•	NAG/HB: learning rate: {0.01,0.03,0.09,0.27}, momentum {0.5,0.8,0.9,0.95,0.97},
learning rate decay factor {2, √Iθ, 5}.
•	ASGD: learning rate {0.01,0.03,0.09,0.27}, long step {100,1000,10000}, advantage pa-
rameter {5,10, 20}, learning rate decay factor {2, √Iθ, 5}.
As a final remark, for any comparison across algorithms, such as, (i) ASGD vs. NAG, (ii) ASGD
vs HB, we fix the starting learning rate, learning rate decay factor and decay schedule chosen by
the best grid search run of NAG/HB respectively and perform a grid search over the long step and
advantage parameter of ASGD. In a similar manner, when we compare (iii) SGD vs NAG or, (iv)
SGD vs. HB, we choose the learning rate, learning rate decay factor and decay schedule of SGD and
simply sweep over the momentum parameter of NAG or HB and choose the momentum that offers
the best validation error.
We now present plots of training function value for different algorithms and batch sizes.
Effect of minibatch sizes: Figure 8 plots training function value for batch sizes of 128 and 8
for SGD, HB and NAG. We notice that in the initial stages of training, NAG obtains substantial
improvements compared to SGD and HB for batch size 128 but not for batch size 8. Towards the
end of training however, NAG starts decreasing the training function value rapidly for both the batch
sizes. The reason for this phenomenon is not clear. Note however, that at this point, the test error
has already stabilized and the algorithms are just overfitting to the data.
Comparison of ASGD with momentum methods: We now present the training error plots
for ASGD compared to HB and NAG in Figures 9 and 10 respectively. As mentioned earlier, in
order to see a clear trend, we constrain the learning rate and decay schedule of ASGD to be the
same as that ofHB and NAG respectively, which themselves were learned using grid search. We see
22
Published as a conference paper at ICLR 2018
AdoeU 山 ωωo∂
Figure 8: Training loss for batch sizes 128 and 8 respectively for SGD, HB and NAG.
similar trends as in the validation error plots from Figures 5 and 6. Please see the figures and their
captions for more details.
0.419
0.125
0.037
0.011
0.003
AdOBU 山 ωωo∂
Figure 9: Training function value for ASGD compared to HB for batch sizes 128 and 8 respectively.
Figure 10: Training function value for ASGD compared to NAG for batch size 128 and 8 respec-
tively.
AdOBU 山 ωωo∂
——ASGD
——NAG
23