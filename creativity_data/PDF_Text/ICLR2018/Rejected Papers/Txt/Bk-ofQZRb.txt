Under review as a conference paper at ICLR 2018
TD Learning with Constrained Gradients
Anonymous authors
Paper under double-blind review
Ab stract
Temporal Difference Learning with function approximation is known to be un-
stable. Previous work like Sutton et al. (2009b) and Sutton et al. (2009a) has
presented alternative objectives that are stable to minimize. However, in practice,
TD-learning with neural networks requires various tricks like using a target net-
work that updates slowly (Mnih et al., 2015). In this work we propose a constraint
on the TD update that minimizes change to the target values. This constraint can
be applied to the gradients of any TD objective, and can be easily applied to non-
linear function approximation. We validate this update by applying our technique
to deep Q-learning, and training without a target network. We also show that
adding this constraint on Baird’s counterexample keeps Q-learning from diverg-
ing.
1	Introduction
Temporal Difference learning is one of the most important paradigms in Reinforcement Learning
(Sutton & Barto). Techniques based on nonlinear function approximators and stochastic gradient
descent such as deep networks have led to significant breakthroughs in the class of problems that
these methods can be applied to (Mnih et al., 2013; 2015; Silver et al., 2016; Schulman et al., 2015).
However, the most popular methods, such as TD(λ), Q-learning and Sarsa, are not true gradient
descent techniques (Barnard, 1993) and do not converge on some simple examples (Baird et al.,
1995).
Baird et al. (1995) and Baird & Moore (1999) propose residual gradients as a way to overcome
this issue. Residual methods, also called backwards bootstrapping, work by splitting the TD error
over both the current state and the next state. These methods are substantially slower to converge,
however, and Sutton et al. (2009b) show that the fixed point that they converge to is not the desired
fixed point of TD-learning methods.
Sutton et al. (2009b) propose an alternative objective function formulated by projecting the TD
target onto the basis of the linear function approximator, and prove convergence to the fixed point of
this projected Bellman error is the ideal fixed point for TD methods. Bhatnagar et al. (2009) extend
this technique to nonlinear function approximators by projecting instead on the tangent space of
the function at that point. Subsequently, Scherrer (2010) has combined these techniques of residual
gradient and projected Bellman error by proposing an oblique projection, and Liu et al. (2015) has
shown that the projected Bellman objective is a saddle point formulation which allows a finite sample
analysis.
However, when using deep networks for approximating the value function, simpler techniques like
Q-learning and Sarsa are still used in practice with stabilizing techniques like a target network that
is updated more slowly than the actual parameters (Mnih et al., 2015; 2013).
In this work, we propose a constraint on the update to the parameters that minimizes the change to
target values, freezing the target that we are moving our current predictions towards. Subject to this
constraint, the update minimizes the TD-error as much as possible. We show that this constraint can
be easily added to existing techniques, and works with all the techniques mentioned above.
We validate our method by showing convergence on Baird’s counterexample and a gridworld do-
main. On the gridworld domain we parametrize the value function using a multi-layer perceptron,
and show that we do not need a target network.
1
Under review as a conference paper at ICLR 2018
2	Notation and Background
Reinforcement Learning problems are generally defined as a Markov Decision Process (MDP), (S ,
A, P, R, R, d0, γ). We use the definition and notation as defined in Sutton & Barto, second edition,
unless otherwise specified.
In case of a function approximation, we define the value and action value functions with parameters
by θ.
vπ(slθ) = Eπ [Rt + YRt+1 + Y2Rt+2 + ... |St = s]	(I)
q∏(s, alθ) = E∏ [Rt + γRt+ι + γ2Rt+2 + ... |St = s, At = a]	(2)
We focus on TD(0) methods, such as Sarsa, Expected Sarsa and Q-learning. The TD error that all
these methods minimize is as follows:
δτD = v∏(st∣θ) - (rt + γv∏0(st+ι∣θ))	(3)
The choice of π0 determines if the update is on-policy or off-policy. For Q-learning the target is
maxa q(st+1, a).
If we consider TD-learning using function approximation, the loss that is minimized is the squared
TD error. For example, in Q-learning
LTD = kq(st,at∣θ) - rt - Ymaxq(st+ι,a∣θ)k2
a
The gradient of this loss is the direction in which you update the parameters. We shall define the
gradient of the TD loss with respect to state st and parameters θt as gTD (st). The gradient of
some other function f (st∣θt) can similarly be defined as gf(st). The parameters are then updated
according to gradient descent with step size α as follows:
∂LTD ∂st
gTD (St) =	---而	(4)
∂st ∂θt
θt+1 = θt - αgTD(st)	(5)
3	TD Updates with Constrained Gradients
A key characteristic of TD-methods is bootstrapping, i.e. the update to the prediction at each step
uses the prediction at the next step as part of it’s target. This method is intuitive and works excep-
tionally well in a tabular setting (Sutton & Barto). In this setting, updates to the value of one state,
or state-action pair do not affect the values of any other state or state-action.
TD-learning using a function approximator is not so straightforward, however. When using a func-
tion approximator, states nearby will tend to share features, or have features that are very similar.
If we update the parameters associated with these features, we will update the value of not only
the current state, but also states nearby that use those features. In general, this is what we want to
happen. With prohibitively large state spaces, we want to generalize across states instead of learning
values separately for each one. However, if the value of the state visited on the next step, which
often does share features, is also updated, then the results of the update might not have the desired
effect on the TD-error.
Generally, methods for TD-learning using function approximation do not take into account that up-
dating θt in the direction that minimizes TD-error the most, might also change v(st+1∣θt+1).
Though they do not point out this insight as we have, previous works that aims to address conver-
gence of TD methods using function approximation do deal with this issue indirectly, like residual
gradients (Baird et al., 1995) and methods minimizing MSPBE (Sutton et al., 2009b). Residual gra-
dients does this by essentially updating the parameters of the next state in the opposite direction of
the update to the parameters of the current state. This splits the error between the current state and
the next state, and the fixed point we reach does not act as a predictive representation of the reward.
MSPBE methods act by removing the component of the error that is not in the span of the features
of the current state, by projecting the TD targets onto these features. The update for these methods
2
Under review as a conference paper at ICLR 2018
Figure 1: Modifying the gradient by projecting onto the direction orthogonal to direction of gradient
at st+1
involves the product of three expectations, which is handled by keeping a separate set of weights
that approximate two of these expectations, and is updated at a faster scale. The idea also does not
immediately scale to nonlinear function approximation. Bhatnagar et al. (2009) propose a solution
by projecting the error on the tangent plane to the function at the point at which it is evaluated.
3.1	Constraining the Update
We propose to instead constrain the update to the parameters such that the change to the values of
the next state is minimized, while also minimizing the TD-error. To do this, instead of modifying
the objective, we look at the gradients of the update.
gTD(st) is the gradient at st that minimizes the TD error. gv(st+1) is the gradient at st+1 that will
change the value the most. We update the parameters θt with gupdate(st) such that the update is
orthogonal to gv (st+1). That is, we update the parameters θt such that there is no change in the
direction that will affect v(st+1). Graphically, the update can be seen in figure 1. The actual updates
to the parameters are as given below.
gupdate(st) = gTD(st) - ΠgTD (st)
gv (St+1)
gv(St+I)
IlgTD (St+I)Il
∏gTD (St) = (gTD(sJ ∙ gv (St+l)) X gv (St+l)
θt+1 = θt - αgupdate (St)
(6)
(7)
(8)
(9)
As can be seen, the proposed update is orthogonal to the direction of the gradient at the next
state. Which means that it will minimize the impact on the next state. On the other hand,
∠(gupdate(st),gτD(St)) ≤ 90°. This implies that applying gupdate(st) to the parameters θ min-
imizes the TD error, unless it would change the values of the next state.
Furthermore, our technique can be applied on top of any of these techniques to improve their behav-
ior. We show this for residual gradients and Q-learning in the following experiments.
4	Experiments
To show that our method learns just as fast as TD while guaranteeing convergence similar to residual
methods, we show the behavior of our algorithm on the following 3 examples.
3
Under review as a conference paper at ICLR 2018
(a) Baird’s Counterexample.
Figure 2: Baird’s Counterexample is specified by 6 states and 7 parameters. The value of each state
is calculated as given inside the state. At each step, the agent is initialized at one of the 6 states
uniformly at random, and transitions to the state at the bottom, shown by the arrows.
(b) Comparison of the average values across states on
Baird’s counterexample over first 2000 iterations of
training
(a) Gridworld, goal in red
(b) DQN
(c) Constrained DQN
Figure 3: A 10 × 10 Gridworld with a goal at location (0, 4), which is midway between one of the
walls. Both DQN and Constrained DQN are used to approximate the value function for a softmax
policy.
4.1	Baird’s Counterexample
Baird’s counterexample is a problem introduced in Baird et al. (1995) to show that gradient descent
with function approximation using TD updates does not converge.
The comparison of our technique with Q-learning and Residual Gradients is shown in figure 2. We
compare the average performance for all tehcniques over 10 independent runs.
If we apply gradient projection while using the TD error, we show that both Q-learning (TD update)
and updates using residual gradients (Baird et al., 1995) converge, but not to the ideal values of 0.
In the figure, these values are almost overlapping. Our method constrains the gradient to not modify
the weights of the next state, which in this case means that w0 and w6 never get updated. This means
that the values do not converge to the true values (0), but they do not blow up as they do if using
regular TD updates. Residual gradients converge to the ideal values of 0 eventually. GTD2 (Sutton
et al., 2009b) also converges to 0, as was shown in the paper, but we have not included that in this
graph to avoid cluttering.
4.2	Gridworld
The Gridworld domain we use is a (10× 10) room with d0 = S, and R((0, 4)) = 1 and 0 everywhere
else. We have set the goal as (0, 4) arbitrarily and our results are similar for any goal on this grid.
The input to the function approximation is only the (x, y) coordinates of the agent. We use a deep
network with 2 hidden layers, each with 32 units, for approximating the Q-values. We execute a
SoftmaX policy, and the target values are also calculated as V(st+1) = Pa π(a∣st+1 )q(st+1,a),
4
Under review as a conference paper at ICLR 2018
SPosa 山-p-ieməz θ6e.iφλv
Figure 4: Comparison of DQN and Constrained on the Cartpole Problem, taken over 10 runs. The
shaded area specifies std deviation in the scores of the agent across independent runs. The agent is
cut off after it’s average performance exceeds 199 over a running window of 100 episodes
where the policy π is a softmax over the Q-values. The room can be seen in Figure 3 with the goal
in red, along with a comparison of the value functions learnt for the 2 methods we compare.
- Q-learning Constrained Q-learning
MSE 0.0335 ± 0.0017	0.0076 ± 0.0028-
Table 1: Comparison of the Mean Squared Error between the value function approximated by Q
learning and by Constrained Q learning with respect to the value function calculated by policy
evaluation on the Gridworld domain. Constrained Q-learning gets substantially lower error.
We see from the learned value function that the value function that DQN learns is sharper. This might
be because the next state values that it uses to update are from a target network that updates slowly
and thus provides stale targets. Constraining the update leads to a smoother value function, which is
encouraging since it shows that constraint does not dissuade generalization. This experiment shows
that constrained updates allow generalization that is useful, while not allowing the target to drift off
or values to explode.
The ground truth can be calculated for this domain using tabular policy evaluation. We calculate this
ground truth value function and compare Mean Squared Error with the value functions learned by
DQN and Constrained DQN over 10 independent runs. The results of this comparison can be seen
in table 1
4.3 Cartpole
As a way to compare against Q-learning with a deep network, we test on the cartpole domain (Barto
et al., 1983). We use implementations from OpenAI baselines (Hesse et al., 2017) for Deep Q-
learning to ensure that the code is reproducible and to ensure fairness. The network we use is with
2 hidden layers of [5, 32]. The only other difference compared to the implemented baseline is that
we use RMSProp (Tieleman & Hinton, 2012) as the particular machinary for optimization instead
of Adam (Kingma & Ba, 2014). This is just to stay close to the method used in Mnih et al. (2015)
and in practice, Adam works just as well and the comparison is similar.
The two methods are trained using exactly the same code except for the updates, and the fact that
Constrained DQN does not use a target network. We can also train COnstrained DQN with a larger
5
Under review as a conference paper at ICLR 2018
step size (10-3), while DQN requires a smaller step size (10-4) to learn. The comparison with
DQN is shown in 4. We see that constrained DQN learns much faster, with much less variance than
regular DQN.
5 Discussion and Conclusion
In this paper we introduce a constraint on the updates to the parameters for TD learning with function
approximation. This constraint forces the targets in the Bellman equation to not move when the
update is applied to the parameters. We enforce this constraint by projecting the gradient of the
TD error with respect to the parameters for state st onto the orthogonal space to the gradient with
respect to the parameters for state st+1.
We show in our experiments that this added constraint stops parameters in Baird’s counterexample
from exploding when we use TD-learning. But since we do not allow changes to target parameters,
this also keeps Residual Gradients from converging to the true values of the Markov Process.
On a Gridworld domain we demonstrate that we can perform TD-learning using a 2-layer neural
network, without the need for a target network that updates more slowly. We compare the solution
obtained with DQN and show that it is closer to the solution obtained by tabular policy evaluation.
Finally, we also show that constrained DQN can learn faster and with less variance on the classical
Cartpole domain.
For future work, we hope to scale this approach to larger problems such as the Atari domain (Belle-
mare et al., 2013). We would also like to prove convergence of TD-learning with this added con-
straint.
References
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation. In
Proceedings ofthe twelfth international conference on machine learning, pp. 30-37, 1995.
Leemon C Baird and Andrew W Moore. Gradient descent for general reinforcement learning. In
Advances in neural information processing systems, pp. 968-974, 1999.
Etienne Barnard. Temporal-difference methods and markov models. IEEE Transactions on Systems,
Man, and Cybernetics, 23(2):357-365, 1993.
Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can
solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics,
(5):834-846, 1983.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. 2013.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba
Szepesvari. Convergent temporal-difference learning with arbitrary smooth function approxi-
mation. In Advances in Neural Information Processing Systems, pp. 1204-1212, 2009.
Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai
Wu. Openai baselines. https://github.com/openai/baselines, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Bo Liu, Ji Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample
analysis of proximal gradient td algorithms. In UAI, pp. 504-513, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
6
Under review as a conference paper at ICLR 2018
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Bruno Scherrer. Should one compute the temporal difference fix point or minimize the bellman
residual? the unified oblique projection view. arXiv preprint arXiv:1011.4362, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15), pp. 1889-1897, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.
Richard S Sutton, Hamid R Maei, and Csaba Szepesvari. A convergent o(n) temporal-difference
algorithm for off-policy learning with linear function approximation. In Advances in neural in-
formation processing systems, pp. 1609-1616, 2009a.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 993-1000. ACM, 2009b.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
7