Under review as a conference paper at ICLR 2018
Learning to Optimize Neural Nets
Anonymous authors
Paper under double-blind review
Ab stract
Learning to Optimize (Li & Malik, 2016) is a recently proposed framework for
learning optimization algorithms using reinforcement learning. In this paper, we
explore learning an optimization algorithm for training shallow neural nets. Such
high-dimensional stochastic optimization problems present interesting challenges
for existing reinforcement learning algorithms. We develop an extension that is
suited to learning optimization algorithms in this setting and demonstrate that the
learned optimization algorithm consistently outperforms other known optimiza-
tion algorithms even on unseen tasks and is robust to changes in stochasticity of
gradients and the neural net architecture. More specifically, we show that an op-
timization algorithm trained with the proposed method on the problem of training
a neural net on MNIST generalizes to the problems of training neural nets on the
Toronto Faces Dataset, CIFAR-10 and CIFAR-100.
1	Introduction
Machine learning is centred on the philosophy that learning patterns automatically from data is
generally better than meticulously crafting rules by hand. This data-driven approach has delivered:
today, machine learning techniques can be found in a wide range of application areas, both in AI and
beyond. Yet, there is one domain that has conspicuously been left untouched by machine learning:
the design of tools that power machine learning itself.
One of the most widely used tools in machine learning is optimization algorithms. We have grown
accustomed to seeing an optimization algorithm as a black box that takes in a model that we design
and the data that we collect and outputs the optimal model parameters. The optimization algorithm
itself largely stays static: its design is reserved for human experts, who must toil through many
rounds of theoretical analysis and empirical validation to devise a better optimization algorithm.
Given this state of affairs, perhaps it is time for us to start practicing what we preach and learn how
to learn.
Recently, Li & Malik (2016) and Andrychowicz et al. (2016) introduced two different frameworks
for learning optimization algorithms. Whereas Andrychowicz et al. (2016) focuses on learning an
optimization algorithm for training models on a particular task, Li & Malik (2016) sets a more am-
bitious objective of learning an optimization algorithm for training models that is task-independent.
We study the latter paradigm in this paper and develop a method for learning an optimization algo-
rithm for high-dimensional stochastic optimization problems, like the problem of training shallow
neural nets.
Under the “Learning to Optimize” framework proposed by Li & Malik (2016), the problem of learn-
ing an optimization algorithm is formulated as a reinforcement learning problem. We consider the
general structure of an unconstrained continuous optimization algorithm, as shown in Algorithm 1.
In each iteration, the algorithm takes a step ∆x and uses it to update the current iterate x(i) . In
hand-engineered optimization algorithms, ∆x is computed using some fixed formula φ that depends
on the objective function, the current iterate and past iterates. Often, it is simply a function of the
current and past gradients.
Different choices of φ yield different optimization algorithms and so each optimization algorithm is
essentially characterized by its update formula φ. Hence, by learning φ, we can learn an optimization
algorithm. Li & Malik (2016) observed that an optimization algorithm can be viewed as a Markov
decision process (MDP), where the state includes the current iterate, the action is the step vector ∆x
1
Under review as a conference paper at ICLR 2018
Algorithm 1 General structure of optimization algorithms
Require: Objective function f
x(0) J random point in the domain of f
for i = 1, 2, . . . do
∆x J φ(f, {x(0), . . . , x(i-1)})
if stopping condition is met then
return x(i-1)
end if
x(i) J x(i-1) + ∆x
end for
and the policy is the update formula φ. Hence, the problem of learning φ simply reduces to a policy
search problem.
In this paper, we build on the method proposed in (Li & Malik, 2016) and develop an extension that
is suited to learning optimization algorithms for high-dimensional stochastic problems. We use it to
learn an optimization algorithm for training shallow neural nets and show that it outperforms popular
hand-engineered optimization algorithms like ADAM (Kingma & Ba, 2014), AdaGrad (Duchi et al.,
2011) and RMSprop (Tieleman & Hinton, 2012) and an optimization algorithm learned using the
supervised learning method proposed in (Andrychowicz et al., 2016). Furthermore, we demonstrate
that our optimization algorithm learned from the experience of training on MNIST generalizes to
training on other datasets that have very dissimilar statistics, like the Toronto Faces Dataset, CIFAR-
10 and CIFAR-100.
2	Related Work
The line of work on learning optimization algorithms is fairly recent. Li & Malik (2016) and
Andrychowicz et al. (2016) were the first to propose learning general optimization algorithms. Li &
Malik (2016) explored learning task-independent optimization algorithms and used reinforcement
learning to learn the optimization algorithm, while Andrychowicz et al. (2016) investigated learning
task-dependent optimization algorithms and used supervised learning.
In the special case where objective functions that the optimization algorithm is trained on are loss
functions for training other models, these methods can be used for “learning to learn” or “meta-
learning”. While these terms have appeared from time to time in the literature (Baxter et al., 1995;
Vilalta & Drissi, 2002; Brazdil et al., 2008; Thrun & Pratt, 2012), they have been used by different
authors to refer to disparate methods with different purposes. These methods all share the objective
of learning some form of meta-knowledge about learning, but differ in the type of meta-knowledge
they aim to learn. We can divide the various methods into the following three categories.
2.1	Learning What to Learn
Methods in this category Thrun & Pratt (2012) aim to learn what parameter values of the base-level
learner are useful across a family of related tasks. The meta-knowledge captures commonalities
shared by tasks in the family, which enables learning on a new task from the family to be performed
more quickly. Most early methods fall into this category; this line of work has blossomed into an
area that has later become known as transfer learning and multi-task learning.
2.2	Learning Which Model to Learn
Methods in this category Brazdil et al. (2008) aim to learn which base-level learner achieves the
best performance on a task. The meta-knowledge captures correlations between different tasks and
the performance of different base-level learners on those tasks. One challenge under this setting is
to decide on a parameterization of the space of base-level learners that is both rich enough to be
capable of representing disparate base-level learners and compact enough to permit tractable search
over this space. Brazdil et al. (2003) proposes a nonparametric representation and stores examples of
different base-level learners in a database, whereas Schmidhuber (2004) proposes representing base-
2
Under review as a conference paper at ICLR 2018
level learners as general-purpose programs. The former has limited representation power, while the
latter makes search and learning in the space of base-level learners intractable. Hochreiter et al.
(2001) views the (online) training procedure of any base-learner as a black box function that maps
a sequence of training examples to a sequence of predictions and models it as a recurrent neural
net. Under this formulation, meta-training reduces to training the recurrent net, and the base-level
learner is encoded in the memory state of the recurrent net.
Hyperparameter optimization can be seen as another example of methods in this category. The
space of base-level learners to search over is parameterized by a predefined set of hyperparameters.
Unlike the methods above, multiple trials with different hyperparameter settings on the same task are
permitted, and so generalization across tasks is not required. The discovered hyperparameters are
generally specific to the task at hand and hyperparameter optimization must be rerun for new tasks.
Various kinds of methods have been proposed, such those based on Bayesian optimization (Hutter
et al., 2011; Bergstra et al., 2011; Snoek et al., 2012; Swersky et al., 2013; Feurer et al., 2015),
random search (Bergstra & Bengio, 2012) and gradient-based optimization (Bengio, 2000; Domke,
2012; Maclaurin et al., 2015).
2.3	Learning How to Learn
Methods in this category aim to learn a good algorithm for training a base-level learner. Unlike
methods in the previous categories, the goal is not to learn about the outcome of learning, but rather
the process of learning. The meta-knowledge captures commonalities in the behaviours of learning
algorithms that achieve good performance. The base-level learner and the task are given by the
user, so the learned algorithm must generalize across base-level learners and tasks. Since learning in
most cases is equivalent to optimizing some objective function, learning a learning algorithm often
reduces to learning an optimization algorithm. This problem was explored in (Li & Malik, 2016)
and (Andrychowicz et al., 2016). Closely related is (Bengio et al., 1991), which learns a Hebb-
like synaptic learning rule that does not depend on the objective function, which does not allow for
generalization to different objective functions.
Various work has explored learning how to adjust the hyperparameters of hand-engineered optimiza-
tion algorithms, like the step size (Hansen, 2016; Daniel et al., 2016; Fu et al., 2016) or the damping
factor in the Levenberg-Marquardt algorithm (Ruvolo et al., 2009). Related to this line of work is
stochastic meta-descent (Bray et al., 2004), which derives a rule for adjusting the step size analyt-
ically. A different line of work (Gregor & LeCun, 2010; Sprechmann et al., 2013) parameterizes
intermediate operands of special-purpose solvers for a class of optimization problems that arise in
sparse coding and learns them using supervised learning.
3	Learning to Optimize
3.1	Setting
In the “Learning to Optimize” framework, we are given a set of training objective functions
f1 , . . . , fn drawn from some distribution F. An optimization algorithm P takes an objective func-
tion f and an initial iterate x(0) as input and produces a sequence of iterates x(1) , . . . , x(T), where
x(T ) is the solution found by the optimizer. We are also given a distribution D that generates
the initial iterate x(0) and a meta-loss L, which takes an objective function f and a sequence of
iterates x(1) , . . . , x(T) produced by an optimization algorithm as input and outputs a scalar that
measures the quality of the iterates. The goal is to learn an optimization algorithm P * such that
Ef〜F,χ(0)〜D [L(f, P*(f, x(0)))] is minimized. The meta-loss is chosen to penalize optimization
algorithms that exhibit behaviours we find undesirable, like slow convergence or excessive oscilla-
tions. Assuming we would like to learn an algorithm that minimizes the objective function itis given,
a good choice of meta-loss would then simply be PiT=1 f (x(i)), which is equivalent to cumulative
regret and can be interpreted as the area under the curve of objective values over time.
The objective functions f1, . . . , fn may correspond to loss functions for training base-level learners,
in which case the algorithm that learns the optimization algorithm can be viewed as a meta-learner.
In this setting, each objective function is the loss function for training a particular base-learner on
a particular task, and so the set of training objective functions can be loss functions for training a
3
Under review as a conference paper at ICLR 2018
base-learner or a family of base-learners on different tasks. At test time, the learned optimization
algorithm is evaluated on unseen objective functions, which correspond to loss functions for training
base-learners on new tasks, which may be completely unrelated to tasks used for training the op-
timization algorithm. Therefore, the learned optimization algorithm must not learn anything about
the tasks used for training. Instead, the goal is to learn an optimization algorithm that can exploit the
geometric structure of the error surface induced by the base-learners. For example, if the base-level
model is a neural net with ReLU activation units, the optimization algorithm should hopefully learn
to leverage the piecewise linearity of the model. Hence, there is a clear division of responsibilities
between the meta-learner and base-learners. The knowledge learned at the meta-level should be
pertinent for all tasks, whereas the knowledge learned at the base-level should be task-specific. The
meta-learner should therefore generalize across tasks, whereas the base-learner should generalize
across instances.
3.2	RL Preliminaries
The goal of reinforcement learning is to learn to interact with an environment in a way that min-
imizes cumulative costs that are expected to be incurred over time. The environment is formal-
ized as a partially observable Markov decision process (POMDP)1, which is defined by the tuple
(S, O, A, pi , p, po, c, T ), where S ⊆ RD is the set of states, O ⊆ RD0 is the set of observations,
A ⊆ Rd is the set of actions, pi (s0) is the probability density over initial states s0, p (st+1 |st, at )
is the probability density over the subsequent state st+1 given the current state st and action at ,
po (ot |st ) is the probability density over the current observation ot given the current state st,
c : S → R is a function that assigns a cost to each state and T is the time horizon. Often, the
probability densities p and po are unknown and not given to the learning algorithm.
A policy π (at |ot, t) is a conditional probability density over actions at given the current observation
ot and time step t. When a policy is independent of t, it is known as a stationary policy. The goal of
the reinforcement learning algorithm is to learn a policy ∏* that minimizes the total expected cost
over time. More precisely,
T
π* = argmin Eso,ao,sι,...,sτ	c(st) ,
t=0
where the expectation is taken with respect to the joint distribution over the sequence of states and
actions, often referred to as a trajectory, which has the density
q(s0, a0,s1, . . . , sT) =	pi (s0) po (o0| s0)
T-1
∏ π (at| ot,t)p (st+1| st, at)po (ot+1| st+1) .
t=0
To make learning tractable, π is often constrained to lie in a parameterized family. A common
assumption is that π (a/ ot,t) = N (μπ(0t), Σπ(ot)), where N(μ, Σ) denotes the density of a
Gaussian with mean μ and covariance Σ. The functions μπ(∙) and possibly Σπ(∙) are modelled
using function approximators, whose parameters are learned.
3.3	Formulation
In our setting, the state St consists of the current iterate χ(t) and features Φ(∙) that depend on the
history of iterates x(1),..., χ(t), (noisy) gradients Vf"(χ(1)),..., Vf"(χ(t)) and (noisy) objective
values f(χ(1)),..., f(χ(t)). The action at is the step ∆χ that will be used to update the iterate. The
observation ot excludes x(t) and consists of features Ψ(∙) that depend on the iterates, gradient and
objective values from recent iterations, and the previous memory state of the learned optimization
algorithm, which takes the form of a recurrent neural net. This memory state can be viewed as a
statistic of the previous observations that is learned jointly with the policy.
1What is described is an undiscounted finite-horizon POMDP with continuous state, observation and action
spaces.
4
Under review as a conference paper at ICLR 2018
Under this formulation, the initial probability density pi captures how the initial iterate, gradient
and objective value tend to be distributed. The transition probability density p captures the how the
gradient and objective value are likely to change given the step that is taken currently; in other words,
it encodes the local geometry of the training objective functions. Assuming the goal is to learn an
optimization algorithm that minimizes the objective function, the cost c ofa state st = (X⑶,Φ(∙))T
is simply the true objective value f(x(t)).
Any particular policy π (at |ot, t ), which generates at = ∆x at every time step, corresponds to
a particular (noisy) update formula φ, and therefore a particular (noisy) optimization algorithm.
Therefore, learning an optimization algorithm simply reduces to searching for the optimal policy.
The mean of the policy is modelled as a recurrent neural net fragment that corresponds to a single
time step, which takes the observation features Ψ(∙) and the previous memory state as input and
outputs the step to take.
3.4	Guided Policy Search
The reinforcement learning method we use is guided policy search (GPS) (Levine et al., 2015),
which is a policy search method designed for searching over large classes of expressive non-linear
policies in continuous state and action spaces. It maintains two policies, ψ and π, where the former
lies in a time-varying linear policy class in which the optimal policy can found in closed form, and
the latter lies in a stationary non-linear policy class in which policy optimization is challenging. In
each iteration, it performs policy optimization on ψ, and uses the resulting policy as supervision to
train π .
More precisely, GPS solves the following constrained optimization problem:
min Eψ
θ,η
T
X c(st)
t=0
s.t. ψ (at| st, t; η) = π (at| st; θ) ∀at, st, t
where η and θ denote the parameters of ψ and ∏ respectively, EP [∙] denotes the expectation taken
with respect to the trajectory induced by a policy ρ and π (at| st; θ) := o π ( at| ot; θ) po ( ot| st)2.
Since there are an infinite number of equality constraints, the problem is relaxed by enforcing equal-
ity on the mean actions taken by ψ and π at every time step3. So, the problem becomes:
T
X c(st) s.t. Eψ [at] = Eψ [Eπ [at| st]] ∀t
t=0
min Eψ
θ,η
This problem is solved using Bregman ADMM (Wang & Banerjee, 2014), which performs the fol-
lowing updates in each iteration:
η 一
T
arg min X Eψ c(st) - λtT at + νtDt (η, θ)
η t=0
T
θ — arg min X λTEψ [E∏ [at| s£]] + VtDt (θ,η)
θ t=0
λt ― λt + ανt (Eψ [E∏ [at| st]] - Eψ [at]) ∀t,
where	Dt (θ, η)	:= Eψ [DKL (π (at| st; θ)k ψ (at| st, t; η))]	and
Eψ [DKL (ψ (at| st, t; η)k π (at| st; θ))].
Dt (η, θ)
The algorithm assumes that ψ ( at | st, t; η) = N (Ktst + kt, Gt), where η := (Kt, kt, Gt)tT=1 and
π (at| ot； θ) = N (μ∏ (ot), Σπ), where θ := (ω, Σπ) and μ∏ (∙) can be an arbitrary function that is
typically modelled using a nonlinear function approximator like a neural net.
2In practice, the explicit form of the observation probability po is usually not known or the integral may be
intractable to compute. So, a linear Gaussian model is fitted to samples of st and at and used in place of the
true π ( at | st ; θ) where necessary.
3Though the Bregman divergence penalty is applied to the original probability distributions over at .
5
Under review as a conference paper at ICLR 2018
At the start of each iteration, the algorithm constructs a model of the transition probability density
P (st+11 St, at, t; Z) = N(AtSt + Btat + ct, Ft), where Z ：= (At, Bt, ct, Ft)T=I is fitted to samples
of st drawn from the trajectory induced by ψ, which essentially amounts to a local linearization of
the true transition probability P (st+ι | st, at, t). We will use Eψ [∙] to denote expectation taken with
respect to the trajectory induced by ψ under the modelled transition probability p. Additionally, the
algorithm fits local quadratic approximations to c(St) around samples of St drawn from the trajectory
induced by ψ so that C(St) ≈ c(st) ：= 11 STCtSt + dTSt + ht for s/s that are near the samples.
With these assumptions, the subproblem that needs to be solved to update η = (Kt, kt, Gt)tT=1
becomes:
T
minXEψ Ic(St)- λTat] + VtDt (η,θ)
η t=0
T
s.t. fEψ [Dkl (ψ (at| st,t η)k ψ (at| st,t η0))] ≤ 3
t=0
where η0 denotes the old η from the previous iteration. Because P and C are only valid locally around
the trajectory induced by ψ, the constraint is added to limit the amount by which η is updated. It
turns out that the unconstrained problem can be solved in closed form using a dynamic programming
algorithm known as linear-quadratic-Gaussian (LQG) regulator in time linear in the time horizon T
and cubic in the dimensionality of the state space D. The constrained problem is solved using dual
gradient descent, which uses LQG as a subroutine to solve for the primal variables in each iteration
and increments the dual variable on the constraint until it is satisfied.
Updating θ is straightforward, since expectations taken with respect to the trajectory induced by π
are always conditioned on St and all outer expectations over St are taken with respect to the trajectory
induced by ψ. Therefore, π is essentially decoupled from the transition probability P ( St+1 | St, at, t)
and so its parameters can be updated without affecting the distribution of St’s. The subproblem that
needs to be solved to update θ therefore amounts to a standard supervised learning problem.
Since ψ ( at| St, t; η) and π ( at| St; θ) are Gaussian, Dt (θ, η) can be computed analytically. More
concretely, if we assume Σπ to be fixed for simplicity, the subproblem that is solved for updating
θ = (ω, Σπ) is:
T
minEψ XλTμω(θt) + 葭(tr (G-1∑π) - log ∣∑π∣)
t=0
+~2tt (μω (Ot) - Eψ [ at| st, t])T Gt1 (μω (Ot) - Eψ [ at| st, t])
Note that the last term is the squared Mahalanobis distance between the mean actions of ψ and π at
time step t, which is intuitive as we would like to encourage π to match ψ.
3.5	Convolutional GPS
The problem of learning high-dimensional optimization algorithms presents challenges for rein-
forcement learning algorithms due to high dimensionality of the state and action spaces. For exam-
ple, in the case of GPS, because the running time of LQG is cubic in dimensionality of the state
space, performing policy search even in the simple class of linear-Gaussian policies would be pro-
hibitively expensive when the dimensionality of the optimization problem is high.
Fortunately, many high-dimensional optimization problems have underlying structure that can be
exploited. For example, the parameters of neural nets are equivalent up to permutation among
certain coordinates. More concretely, for fully connected neural nets, the dimensions of a hidden
layer and the corresponding weights can be permuted arbitrarily without changing the function they
compute. Because permuting the dimensions of two adjacent layers can permute the weight matrix
arbitrarily, an optimization algorithm should be invariant to permutations of the rows and columns
of a weight matrix. A reasonable prior to impose is that the algorithm should behave in the same
6
Under review as a conference paper at ICLR 2018

——Gr⅞dene Dwcene
Moπnn(um
Conjugate Oadlent
—LaroS
MaGrad
一ADAM
—RMSpTOp
—Uibgobgo
PrvdcM Snp Omccm

——(⅛⅜dler* θæænt
Momenhzn
ConjugKeGradIent
—LaroS
MaOad
—<OAM
—RMSprop
—ULBGOBGO
—⅛⅝⅜⅝⅛SWpP⅜⅝Wlt
Gradient Deewnt
Momenom
CoriJj 9atβGr¾d ent
L-BFCS
Ma&ad
UMM
KMSfrop
12LBO»«0
ABdCwd Shb Descent
(a)
(b)
(c)
Figure 1: Comparison of the various hand-engineered and learned algorithms on training neural nets
with 48 input and hidden units on (a) TFD, (b) CIFAR-10 and (c) CIFAR-100 with mini-batches of
size 64. The vertical axis is the true objective value and the horizontal axis represents the iteration.
Best viewed in colour.
manner on all coordinates that correspond to entries in the same matrix. That is, if the values
of two coordinates in all current and past gradients and iterates are identical, then the step vector
produced by the algorithm should have identical values in these two coordinates. We will refer to
the set of coordinates on which permutation invariance is enforced as a coordinate group. For the
purposes of learning an optimization algorithm for neural nets, a natural choice would be to make
each coordinate group correspond to a weight matrix or a bias vector. Hence, the total number of
coordinate groups is twice the number of layers, which is usually fairly small.
In the case of GPS, we impose this prior on both ψ and π. For the purposes of updating η, we first
impose a block-diagonal structure on the parameters At , Bt and Ft of the fitted transition proba-
bility density P (st+ι | st, at, t; Z) = N(Atst + Btat + ct, Ft), So that for each coordinate in the
optimization problem, the dimensions of st+1 that correspond to the coordinate only depend on the
dimensions of St and at that correspond to the same coordinate. As a result, p( st+ι∣ st,at,t; Z)
decomposes into multiple independent probability densities Pj (sj'+J sj,aj,t;Zj), one for each
coordinate j. Similarly, We also impose a block-diagonal structure on Ct for fitting C(St) and on
the parameter matrix of the fitted model for π ( at | st; θ). Under these assumptions, Kt and Gt are
guaranteed to be block-diagonal as Well. Hence, the Bregman divergence penalty term, D (η, θ)
decomposes into a sum of Bregman divergence terms, one for each coordinate.
We then further constrain dual variables λt, sub-vectors of parameter vectors and sub-matrices of
parameter matrices corresponding to each coordinate group to be identical across the group. Addi-
tionally, We replace the Weight νt on D (η, θ) With an individual Weight on each Bregman divergence
term for each coordinate group. The problem then decomposes into multiple independent subprob-
lems, one for each coordinate group. Because the dimensionality of the state subspace corresponding
to each coordinate is constant, LQG can be executed on each subproblem much more efficiently.
Similarly, for ∏, we choose a μ∏(∙) that shares parameters across different coordinates in the same
group. We also impose a block-diagonal structure on Σπ and constrain the appropriate sub-matrices
to share their entries.
3.6	Features
We describe the features Φ(∙) and Ψ(∙) at time step t, which define the state St and observation ot
respectively.
Because of the stochasticity of gradients and objective values, the state features Φ(∙) are defined
in terms of summary statistics of the history of iterates
t
{χ(i)}t=o, gradients {v∕(χ(i))}	and
{"i))}
objective values
We define the following statistics, which we will refer to as the
i=0
average recent iterate, gradient and objective value respectively:
X⑶
1	Pi	Xj)
min(i+1,3) -j = max(i-2,0)
7
Under review as a conference paper at ICLR 2018
festal	l⅛r⅛kw	ImHm
(a)
(b)
(c)
Figure 2: Comparison of the various hand-engineered and learned algorithms on training neural nets
with 100 input units and 200 hidden units on (a) TFD, (b) CIFAR-10 and (c) CIFAR-100 with mini-
batches of size 64. The vertical axis is the true objective value and the horizontal axis represents the
iteration. Best viewed in colour.
• Vf(X⑶)：=m⅛) Pj = max(i-2,0) Vff(x(j))
^ ..
f(x<i * * 4i):：
1
min(i+1,3)
Pj=max(i-2,0) f(xj
The state features Φ(∙) consist of the relative change in the average recent objective value, the
average recent gradient normalized by the magnitude of the a previous average recent gradient and
a previous change in average recent iterate relative to the current change in average recent iterate:
{(∕(χ(t-5i)) - /(χ(t-"i+1)))
//(χ(t-"i+1)))}
24
i=0
{ v∕(χ(t-5i))/ Q v/(x(max(t-5(i+1),tmod5)))| +1
25
i=0
X (max(t — 5(i+1) ,tmod5 + 5)) -χ(max(t — 5(i+2) ,tmod5))
χ(t-5i)-χ(t-5(i+1))∣ +0. 1
24
i=0
Note that all operations are applied element-wise. Also, whenever a feature becomes undefined (i.e.:
when the time step index becomes negative), it is replaced with the all-zeros vector.
Unlike state features, which are only used when training the optimization algorithm, observation
features Ψ(∙) are used both during training and at test time. Consequently, We use noisier observation
features that can be computed more efficiently and require less memory overhead. The observation
features consist of the folloWing:
• (/(x(t)) - /(x(tτ))) /f(χ(tτ))
• v/(x(t))/ (∣V∕(x(max(tτ,O)))I + 1)
|x(max(t—1,1)) -x(max(t—2,0)) |
•	|x(t)-x(t-1) ∣+0.1
4 Experiments
For clarity, We Will refer to training of the optimization algorithm as “meta-training” to differentiate
it from base-level training, Which Will simply be referred to as “training”.
We meta-trained an optimization algorithm on a single objective function, Which corresponds to the
problem of training a tWo-layer neural net With 48 input units, 48 hidden units and 10 output units
on a randomly projected and normalized version of the MNIST training set With dimensionality 48
and unit variance in each dimension. We modelled the optimization algorithm using an recurrent
neural net With a single layer of 128 LSTM (Hochreiter & Schmidhuber, 1997) cells. We used a
time horizon of 400 iterations and a mini-batch size of 64 for computing stochastic gradients and
objective values. We evaluate the optimization algorithm on its ability to generalize to unseen objec-
tive functions, Which correspond to the problems of training neural nets on different tasks/datasets.
8
Under review as a conference paper at ICLR 2018
.»
'∞
ID
Gr⅞deneO⅞βcene
Momentum
Conjugate Oadlent
LaRSS
MaGrad
ADAM
RMSPrŋp
Uibgobgo
⅛⅝⅜E SttP ∏wcw*
(a)
Iiwitai
ftadltnt Dteccnt
Momentwn
ConjugKeGradIent
LaRSS
MaOad
KUM
RMSprop
ULBGOBGO
⅛⅝⅜⅝⅛ Sttpnwcwt
(b)
so ioo 150 ao so ate 3»
ImHaI
(c)

Figure 3:	Comparison of the various hand-engineered and learned algorithms on training neural nets
with 48 input and hidden units on (a) TFD, (b) CIFAR-10 and (c) CIFAR-100 with mini-batches of
size 10. The vertical axis is the true objective value and the horizontal axis represents the iteration.
Best viewed in colour.
Vssv>-B5
GradtrtOeecene
Momcntuni
Conjugate Osdlent
LaRSS
MaGrad
ADAM
RMSPrŋp
Uibgobgo
PrMced Sttp OtKtrt
50 m ISO 200	2S0	300	360
k^d0ι
(a)
a in ιa Zm 25130 3sl
Iiwitai
(b)
s> ωo iso ao so am 3»
ImHaI
(c)
Figure 4:	Comparison of the various hand-engineered and learned algorithms on training neural nets
with 100 input units and 200 hidden units on (a) TFD, (b) CIFAR-10 and (c) CIFAR-100 with mini-
batches of size 10. The vertical axis is the true objective value and the horizontal axis represents the
iteration. Best viewed in colour.
We evaluate the learned optimization algorithm on three datasets, the Toronto Faces Dataset (TFD),
CIFAR-10 and CIFAR-100. These datasets are chosen for their very different characteristics from
MNIST and each other: TFD contains 3300 grayscale images that have relatively little variation and
has seven different categories, whereas CIFAR-100 contains 50,000 colour images that have varied
appearance and has 100 different categories.
All algorithms are tuned on the training objective function. For hand-engineered algorithms, this
entails choosing the best hyperparameters; for learned algorithms, this entails meta-training on the
objective function. We compare to the seven hand-engineered algorithms: stochastic gradient de-
scent, momentum, conjugate gradient, L-BFGS, ADAM, AdaGrad and RMSprop. In addition, we
compare to an optimization algorithm meta-trained using the method described in (Andrychowicz
et al., 2016) on the same training objective function (training two-layer neural net on randomly
projected and normalized MNIST) under the same setting (a time horizon of 400 iterations and a
mini-batch size of 64).
First, we examine the performance of various optimization algorithms on similar objective functions.
The optimization problems under consideration are those for training neural nets that have the same
number of input and hidden units (48 and 48) as those used during meta-training. The number of
output units varies with the number of categories in each dataset. We use the same mini-batch size
as that used during meta-training. As shown in Figure 1, the optimization algorithm meta-trained
using our method (which we will refer to as Predicted Step Descent) consistently descends to the
optimum the fastest across all datasets. On the other hand, other algorithms are not as consistent
and the relative ranking of other algorithms varies by dataset. This suggests that Predicted Step
Descent has learned to be robust to variations in the data distributions, despite being trained on only
one objective function, which is associated with a very specific data distribution that characterizes
MNIST. It is also interesting to note that while the algorithm meta-trained using (Andrychowicz
9
Under review as a conference paper at ICLR 2018
Iiwitai
(a)	(b)
(c)
Figure 5: Comparison of the various hand-engineered and learned algorithms on training neural
nets with 100 input units and 200 hidden units on (a) TFD, (b) CIFAR-10 and (c) CIFAR-100 for
800 iterations with mini-batches of size 64. The vertical axis is the true objective value and the
horizontal axis represents the iteration. Best viewed in colour.


et al., 2016) (which we will refer to as L2LBGDBGD) performs well on CIFAR, it is unable to
reach the optimum on TFD.
Next, we change the architecture of the neural nets and see if Predicted Step Descent generalizes to
the new architecture. We increase the number of input units to 100 and the number of hidden units
to 200, so that the number of parameters is roughly increased by a factor of 8. As shown in Figure 2,
Predicted Step Descent consistently outperforms other algorithms on each dataset, despite having
not been trained to optimize neural nets of this architecture. Interestingly, while it exhibited a bit
of oscillation initially on TFD and CIFAR-10, it quickly recovered and overtook other algorithms,
which is reminiscent of the phenomenon reported in (Li & Malik, 2016) for low-dimensional op-
timization problems. This suggests that it has learned to detect when it is performing poorly and
knows how to change tack accordingly. L2LBGDBGD experienced difficulties on TFD and CIFAR-
10 as well, but slowly diverged.
We now investigate how robust Predicted Step Descent is to stochasticity of the gradients. To this
end, we take a look at its performance when we reduce the mini-batch size from 64 to 10 on both
the original architecture with 48 input and hidden units and the enlarged architecture with 100 input
units and 200 hidden units. As shown in Figure 3, on the original architecture, Predicted Step
Descent still outperforms all other algorithms and is able to handle the increased stochasticity fairly
well. In contrast, conjugate gradient and L2LBGDBGD had some difficulty handling the increased
stochasticity on TFD and to a lesser extent, on CIFAR-10. In the former case, both diverged; in the
latter case, both were progressing slowly towards the optimum.
On the enlarged architecture, Predicted Step Descent experienced some significant oscillations on
TFD and CIFAR-10, but still managed to achieve a much better objective value than all the other
algorithms. Many hand-engineered algorithms also experienced much greater oscillations than pre-
viously, suggesting that the optimization problems are inherently harder. L2LBGDBGD diverged
fairly quickly on these two datasets.
Finally, we try doubling the number of iterations. As shown in Figure 5, despite being trained over
a time horizon of 400 iterations, Predicted Step Descent behaves reasonably beyond the number of
iterations it is trained for.
5 Conclusion
In this paper, we presented a new method for learning optimization algorithms for high-dimensional
stochastic problems. We applied the method to learning an optimization algorithm for training
shallow neural nets. We showed that the algorithm learned using our method on the problem of
training a neural net on MNIST generalizes to the problems of training neural nets on unrelated
tasks/datasets like the Toronto Faces Dataset, CIFAR-10 and CIFAR-100. We also demonstrated
that the learned optimization algorithm is robust to changes in the stochasticity of gradients and the
neural net architecture.
10
Under review as a conference paper at ICLR 2018
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
and Nando de Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint
arXiv:1606.04474, 2016.
Jonathan Baxter, Rich Caruana, Tom Mitchell, Lorien Y Pratt, Daniel L Silver, and Sebastian Thrun.
NIPS 1995 workshop on learning to learn: Knowledge consolidation and transfer in induc-
tive systems. https://web.archive.org/web/20000618135816/http://www.
cs.cmu.edu/afs/cs.cmu.edu/user/caruana/pub/transfer.html, 1995. Ac-
cessed: 2015-12-05.
Y Bengio, S Bengio, and J Cloutier. Learning a synaptic learning rule. In Neural Networks, 1991.,
IJCNN-91-Seattle International Joint Conference on, volume 2,pp. 969-VoL IEEE, 1991.
Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation, 12(8):1889-
1900, 2000.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. The Journal
of Machine Learning Research, 13(1):281-305, 2012.
James S Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs KegL Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems, pp. 2546-2554, 2011.
M Bray, E Koller-Meier, P Muller, L Van Gool, and NN Schraudolph. 3D hand tracking by rapid
stochastic gradient descent using a skinning model. In Visual Media Production, 2004.(CVMP).
1st European Conference on, pp. 59-68. IET, 2004.
Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: appli-
cations to data mining. Springer Science & Business Media, 2008.
Pavel B Brazdil, Carlos Soares, and Joaquim Pinto Da Costa. Ranking learning algorithms: Using
ibl and meta-learning on accuracy and time results. Machine Learning, 50(3):251-277, 2003.
Christian Daniel, Jonathan Taylor, and Sebastian Nowozin. Learning step size controllers for robust
neural network training. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.
Justin Domke. Generic methods for optimization-based modeling. In AISTATS, volume 22, pp.
318-326, 2012.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparameter
optimization via meta-learning. In AAAI, pp. 1128-1135, 2015.
Jie Fu, Zichuan Lin, Miao Liu, Nicholas Leonard, Jiashi Feng, and Tat-Seng Chua. Deep q-networks
for accelerating the training of deep neural networks. arXiv preprint arXiv:1606.01467, 2016.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of
the 27th International Conference on Machine Learning (ICML-10), pp. 399-406, 2010.
Samantha Hansen. Using deep q-learning to control optimization hyperparameters. arXiv preprint
arXiv:1602.04062, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for
general algorithm configuration. In Learning and Intelligent Optimization, pp. 507-523. Springer,
2011.
11
Under review as a conference paper at ICLR 2018
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. arXiv preprint arXiv:1504.00702, 2015.
Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. arXiv preprint arXiv:1502.03492, 2015.
Paul L Ruvolo, Ian Fasel, and Javier R Movellan. Optimization on a budget: A reinforcement
learning approach. In Advances in Neural Information Processing Systems, pp. 1385-1392, 2009.
Jurgen Schmidhuber. Optimal ordered problem solver. Machine Learning, 54(3):211-254, 2004.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Pablo Sprechmann, Roee Litman, Tal Ben Yakar, Alexander M Bronstein, and Guillermo Sapiro.
Supervised sparse analysis and synthesis operators. In Advances in Neural Information Process-
ing Systems, pp. 908-916, 2013.
Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances
in neural information processing systems, pp. 2004-2012, 2013.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18(2):77-95, 2002.
Huahua Wang and Arindam Banerjee. Bregman alternating direction method of multipliers. CoRR,
abs/1306.3203, 2014.
12