Under review as a conference paper at ICLR 2018
Hierarchical Adversarially Learned
Inference
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel hierarchical generative model with a simple Markovian struc-
ture and a corresponding inference model. Both the generative and inference model
are trained using the adversarial learning paradigm. We demonstrate that the hierar-
chical structure supports the learning of progressively more abstract representations
as well as providing semantically meaningful reconstructions with different levels
of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence
between the generative and inference network is enough to minimize the recon-
struction error. The resulting semantically meaningful hierarchical latent structure
discovery is exemplified on the CelebA dataset. There, we show that the features
learned by our model in an unsupervised way outperform the best handcrafted
features. Furthermore, the extracted features remain competitive when compared
to several recent deep supervised approaches on an attribute prediction task on
CelebA. Finally, we leverage the model’s inference network to achieve state-of-
the-art performance on a semi-supervised variant of the MNIST digit classification
task.
1	Introduction
Deep generative models represent powerful approaches to modeling highly complex high-dimensional
data. There has been a lot of recent research geared towards the advancement of deep generative
modeling strategies, including Variational Autoencoders (VAE) (Kingma & Welling, 2013), autore-
gressive models (Oord et al., 2016a;b) and hybrid models (Gulrajani et al., 2016; Nguyen et al., 2016).
However, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have emerged as the
learning paradigm of choice across a varied range of tasks, especially in computer vision Zhu et al.
(2017), simulation and robotics Finn et al. (2016) Shrivastava et al. (2016). GANs cast the learning of
a generative network in the form of a game between the generative and discriminator networks. While
the discriminator is trained to distinguish between the true and generated examples, the generative
model is trained to fool the discriminator. Using a discriminator network in GANs avoids the need
for an explicit reconstruction-based loss function. This allows this model class to generate visually
sharper images than VAEs while simultaneously enjoying faster sampling than autoregressive models.
Recent work, known as either ALI (Dumoulin et al., 2016) or BiGAN (Donahue et al., 2016), has
shown that the adversarial learning paradigm can be extended to incorporate the learning of an
inference network. While the inference network, or encoder, maps training examples x to a latent
space variable z, the decoder plays the role of the standard GAN generator mapping from space of the
latent variables (that is typically sampled from some factorial distribution) into the data space. In ALI,
the discriminator is trained to distinguish between the encoder and the decoder, while the encoder
and decoder are trained to conspire together to fool the discriminator. Unlike some approaches that
hybridize VAE-style inference with GAN-style generative learning (e.g. Larsen et al. (2015), Chen
et al. (2016)), the encoder and decoder in ALI use a purely adversarial approach. One big advantage
of adopting an adversarial-only formalism is demonstrated by the high-quality of the generated
samples. Additionally, we are given a mechanism to infer the latent code associated with a true data
example.
One interesting feature highlighted in the original ALI work (Dumoulin et al., 2016) is that even
though the encoder and decoder models are never explicitly trained to perform reconstruction, this can
nevertheless be easily done by projecting data samples via the encoder into the latent space, copying
1
Under review as a conference paper at ICLR 2018
these values across to the latent variable layer of the decoder and projecting them back to the data
space. Doing this yields reconstructions that often preserve some semantic features of the original
input data, but are perceptually relatively different from the original samples. These observations
naturally lead to the question of the source of the discrepancy between the data samples and their
ALI reconstructions. Is the discrepancy due to a failure of the adversarial training paradigm, or is
it due to the more standard challenge of compressing the information from the data into a rather
restrictive latent feature vector? Ulyanov et al. (2017) show that an improvement in reconstructions is
achievable when additional terms which explicitly minimize reconstruction error in the data space are
added to the training objective. Li et al. (2017b) palliates to the non-identifiability issues pertaining
to bidirectional adversarial training by augmenting the generator’s loss with an adversarial cycle
consistency loss.
In this paper we explore issues surrounding the representation of complex, richly-structured data, such
as natural images, in the context of a novel, hierarchical generative model, Hierarchical Adversarially
Learned Inference (HALI), which represents a hierarchical extension of ALI. We show that within
a purely adversarial training paradigm, and by exploiting the model’s hierarchical structure, we
can modulate the perceptual fidelity of the reconstructions. We provide theoretical arguments for
why HALI’s adversarial game should be sufficient to minimize the reconstruction cost and show
empirical evidence supporting this perspective. Finally, we evaluate the usefulness of the learned
representations on a semi-supervised task on MNIST and an attribution prediction task on the CelebA
dataset.
2	Related work
Our work fits into the general trend of hybrid approaches to generative modeling that combine aspects
of VAEs and GANs. For example, Adversarial Autoencoders (Makhzani et al., 2015) replace the
Kullback-Leibler divergence that appears in the training objective for VAEs with an adversarial
discriminator that learns to distinguish between samples from the approximate posterior and the
prior. A second line of research has been directed towards replacing the reconstruction penalty from
the VAE objective with GANs or other kinds of auxiliary losses. Examples of this include Larsen
et al. (2015) that combines the GAN generator and the VAE decoder into one network and Lamb
et al. (2016) that uses the loss of a pre-trained classifier as an additional reconstruction loss in the
VAE objective. Another research direction has been focused on augmenting GANs with inference
machinery. One particular approach is given by Dumoulin et al. (2016); Donahue et al. (2016),
where, like in our approach, there is a separate inference network that is jointly trained with the usual
GAN discriminator and generator. Karaletsos (2016) presents a theoretical framework to jointly
train inference networks and generators defined on directed acyclic graphs by leverage multiple
discriminators defined nodes and their parents. Another related work is that of Huang et al. (2016b)
which takes advantage of the representational information coming from a pre-trained discriminator.
Their model decomposes the data generating task into multiple subtasks, where each level outputs
an intermediate representation conditioned on the representations from higher level. A stack of
discriminators is employed to provide signals for these intermediate representations. The idea of
stacking discriminator can be traced back to Denton et al. (2015) which used used a succession
of convolutional networks within a Laplacian pyramid framework to progressively increase the
resolution of the generated images.
3	Hierachical Adversarially Learned Inference
The goal of generative modeling is to capture the data-generating process with a probabilistic model.
Most real-world data is highly complex and thus, the exact modeling of the underlying probability
density function is usually computationally intractable. Motivated by this fact, GANs (Goodfellow
et al., 2014) model the data-generating distribution as a transformation of some fixed distribution
over latent variables. In particular, the adversarial loss, through a discriminator network, forces the
generator network to produce samples that are close to those of the data-generating distribution.
While GANs are flexible and provide good approximations to the true data-generating mechanism,
their original formulation does not permit inference on the latent variables. In order to mitigate
this, Adversarially Learned Inference (ALI) (Dumoulin et al., 2016) extends the GAN framework to
include an inference network that encodes the data into the latent space. The discriminator is then
2
Under review as a conference paper at ICLR 2018
trained to discriminate between the joint distribution of the data and latent causes coming from the
generator and inference network. Thus, the ALI objective encourages a matching of the two joint
distributions, which also results in all the marginals and conditional distributions being matched. This
enables inference on the latent variables.
We endeavor to improve on ALI in two aspects. First, as reconstructions from ALI only loosely match
the input on a perceptual level, we want to achieve better perceptual matching in the reconstructions.
Second, we wish to be able to compress the observables, x, using a sequence of composed features
maps, leading to a distilled hierarchy of stochastic latent representations, denoted by z1 to zL. Note
that, as a consequence of the data processing inequality(Cover & Thomas, 2012), latent representations
higher up in the hierarchy cannot contain more information than those situated lower in the hierarchy.
In information-theoretic terms, the conditional entropy of the observables given a latent variable is
non-increasing as we ascend the hierarchy. This loss of information can be seen as responsible for
the perceptual discrepancy observed in ALI’s reconstructions. Thus, the question we seek to answer
becomes: How can we achieve high perceptual fidelity of the data reconstructions while also having
a compressed latent space that is strongly coupled with the observables? In this paper, we propose to
answer this using a novel model, Hierarchical Adversarially Learned Inference (HALI), that uses a
simple hierarchical Markovian inference network that is matched through adversarial training to a
similarly constructed generator network. Furthermore, we discuss the hierarchy of reconstructions
induced by the HALI’s hierarchical inference network and show that the resulting reconstruction
errors are implicitly minimized during adversarial training. Also, we leverage HALI’s hierarchial
inference network to offer a novel approach to semi-supervised learning in generative adversarial
models.
3.1	A Model for Hierarchical Features
Denote by P(S) the set of all probability measures on some set S. Let TZ|X be a Markov kernel
associating to each element x ∈ X a probability measure PZ|X=x ∈ P(Z). Given two Markov
kernels TW|V and TV |U, a further Markov kernel can be defined by composing these two and
then marginalizing over V , i.e. TW|V ◦ TV |U : U → P(W). Consider a set of random variables
x, z1, . . . , zL. Using the composition operation, we can construct a hierarchy of Markov kernels or
feature transitions as
TzlIx = TzlIzl-I。…。TziH	⑴
A desirable property for these feature transitions is to have some form of inverses. Motivated by this,
We define the adjoint feature transition as T^⑶ ɪ = Tzi_r\zi. From this, We see that
TzLIx = Tx∖zL = Tzi\z2。…。Tzl_i\zl .	⑵
This can be interpreted as the generative mechanism of the latent variables given the data being
the "inverse" of the data-generating mechanism given the latent variables. Let q(x) denote the
distribution of the data and p(zL) be the prior on the latent variables. Typically the prior Will be a
simple distribution, e.g. a standard Gaussian p(zL) = N(0 | I) .
The composition of Markov kernels in Eq. 1, mapping data samples x to samples of the latent
variables zL using z1 , . . . , zL-1 constitutes the encoder. Similarly, the composition of kernels in
Eq. 2 mapping prior samples of zL to data samples x through zL-1, . . . , z1 constitutes the decoder.
Thus, the joint distribution of the encoder can be Written as
L
q(x, . . . , zL) =	q(zl | zl-1) q(z1 | x) q(x),	(3)
l=2
While the joint distribution of the decoder is given by
L
p(x, . . . , zL) = p(x | z1)	p(zl-1 | zl)p(zL).	(4)
l=2
3
Under review as a conference paper at ICLR 2018
Algorithm 1 HALI training procedure.
θg ,θd J initialize network parameters
repeat
for m ∈ {1, . . . , M} do
z(m) 〜q* * * (X)
ZLm 〜P(Z)
for l ∈ {1, . . . , L} do
Zm 〜q(zι I Zι-ι)
end for
. Sample from the dataset
. Sample from the prior
. Sample from each level in the encoder’s hierarchy
for l ∈ {L . . . 1} do
z(mι) 〜p(zι-ι | zι)	. Sample from each level in the decoder's hierarchy
end for
Pqm) J D(ZOm),..., Z(m))	. Get discriminator predictions on encoder,s distribution
ρ(pm) J D(Z0(m), . . . , Zι(m))	. Get discriminator predictions on decoder’s distribution
end for
Ld J——焉 PM=IlOg(Pqi))-焉 PM=1 log(1 - Ppi))	. Compute discriminator loss
Lg J-MM PM=Ilog(I — Pqi)) — MM PMIlog(Ppi))	. ComPUte generator loss
θd J θd - Vθd Ld	. Gradient update on discriminator network
. Gradient update on generator networks
θgJθg-VθgLg
until convergence
The encoder and decoder distributions can be visualized graphically as
Tz1 |x	Tz2 |z1
一-------------------------------
X	____z1 J	一 z2
Tx|z1	Tz1 |z2
Tz3 |z2	TzL |zL-1
ZL
TzL-1 |zL
Having constructed the joint distributions of the encoder and decoder, we can now match these
distributions through adversarial training. It can be shown that, under an ideal (non-parametric)
discriminator, this is equivalent to minimizing the Jensen-Shanon divergence between the joint Eq. 3
and Eq. 4, see (Dumoulin et al., 2016). Algorithm 1 details the training procedure.
3.2 A hierarchy of reconstructions
The Markovian character of both the encoder and decoder implies a hierarchy of reconstructions in the
decoder. In particular, for a given observation X 〜p(x), the model yields L different reconstructions
Xι 〜Tχ∣zι ◦ Tzι∣x for l ∈ {1,...,L} with Xι the reconstruction of the X at the l-th level of the
hierarchy. Here, we can think of Tzl|x as projecting X to the l-th intermediate representation and
Tx|zl as projecting it back to the input space. Then, the reconstruction error for a given input X at
the l-th hierarchical level is given by
LI(X) = Ezl〜TzlIx [-log(P(X | zι))].	⑸
Contrary to models that try to merge autoencoders and adversarial models, e.g. Rosca et al. (2017);
Larsen et al. (2015), HALI does not require any additional terms in its loss function in order to
minimize the above reconstruction error. Indeed, the reconstruction errors at the different levels of
HALI are minimized down to the amount of information about X that a given level of the hierarchy
is able to encode as training proceeds. Furthermore, under an optimal discriminator, training in
HALI minimizes the Jensen-Shanon divergence between q(X, Z1, . . . , ZL) and P(X, Z1, . . . , ZL) as
formalized in Proposition 1 below. Furthermore, the interaction between the reconstruction error and
training dynamics is captured in Proposition 1.
Proposition 1. Assuming q(X, Zι) is bounded away for zero for all l ∈ {1, . . . , L}, we have that
Ex 〜q(x)[Lι (x)] - H (x I Zι) ≤ KDJS (p(x, zi,∙∙∙, ZL) II q(X, zι,∙∙∙, zl)),	(6)
where H(X | Zι) is computed under the encoder’s distribution and K is as defined in Lemma 2 in the
appendix.
4
Under review as a conference paper at ICLR 2018
On the other hand, proposition 2 below relates the intermediate representations in the hierarchy to
the corresponding induced reconstruction error.
Proposition 2. For any given latent variable zl,
Ex〜qx[Ez〜Tzl∣χ[-logP(X | Zi)]] ≥ H(X | Zl)	(7)
i.e. the reconstruction error is an upper bound on H(x | zl ).
In summary, Propositions 1 and 2 establish the dynamics between the hierarchical representation
learned by the inference network, the reconstruction errors and the adversarial matching of the joint
distributions Eq. 3 and Eq. 4. The proofs on the two propositions above are deferred to the appendix.
Having theoretically established the interplay between layer-wise reconstructions and the training
mechanics, we now move to the empirical evaluation of HALI.
4	Empirical Analysis: Setup
We designed our experiments with the objective of addressing the following questions: Is HALI
successful in improving the fidelity perceptual reconstructions? Does HALI induces a semanti-
cally meaningful representation of the observed data? Are the learned representations useful for
downstream classification tasks? All of these questions are considered in turn in the following
sections.
We evaluated HALI on four datasets, CIFAR10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al.,
2011), ImageNet 128x128 (Russakovsky et al., 2015) and CelebA (Liu et al., 2015). We used
two conditional hierarchies in all experiments with the Markov kernels parametrized by conditional
isotropic Gaussians. For SVHN, CIFAR10 and CelebA the resolutions of two level latent variables are
Z1 ∈ R64×16×16 and Z2 ∈ R256. For ImageNet, the resolutions is Z1 ∈ R64×32×32 and Z2 ∈ R256.
For both the encoder and decoder, we use residual blocks(He et al., 2015) with skip connections
between the blocks in conjunction with batch normalization(Ioffe & Szegedy, 2015). We use
convolution with stride 2 for downsampling in the encoder and bilinear upsampling in the decoder. In
the discriminator, we use consecutive stride 1 and stride 2 convolutions and weight normalization
(Salimans & Kingma, 2016). To regularize the discriminator, we apply dropout every 3 layers with
a probability of retention of 0.2. We also add Gaussian noise with standard deviation of 0.2 at the
inputs of the discriminator and the encoder.
5	Empirical Analysis I: Reconstructions
One of the desired objectives of a generative model is to reconstruct the input images from the
latent representation. We show that HALI offers improved perceptual reconstructions relative to the
(non-hierarchical) ALI model.
5.1	Qualitative analysis
First, we present reconstructions obtained on ImageNet. Reconstructions from SVHN and CIFAR10
can be seen in Fig. 7 in the appendix. Fig. 1 highlights HALI’s ability to reconstruct the input
samples with high fidelity. We observe that reconstructions from the first level of the hierarchy exhibit
local differences in the natural images, while reconstructions from the second level of the hierarchy
displays global change. Higher conditional reconstructions are more often than not reconstructed as a
different member of the same class. Moreover, we show in Fig. 2 that this increase in reconstruction
fidelity does not impact the quality of the generative samples from HALI’s decoder.
5.2	Quantitative analysis
We further investigate the quality of the reconstructions with a quantitative assessment of the preser-
vation of perceptual features in the input sample. For this evaluation task, we use the CelebA
dataset where each image comes with a 40 dimensional binary attributes vector. A VGG-16 classi-
fier(Simonyan & Zisserman, 2014) was trained on the CelebA training set to classify the individual
attributes. This trained model is then used to classify the attributes of the reconstructions from the
5
Under review as a conference paper at ICLR 2018
(a) ImageNet128 from z1
(b) ImageNet128 from z2
Figure 1: ImageNet128 reconstructions from z1 and z2 . Odd columns corresponds to examples from the
validation set while even columns are the model’s reconstructions
Figure 2: Samples from 128 × 128 CelebA and ImageNet128 datasets
(b) ImageNet128
6
Under review as a conference paper at ICLR 2018
	Mean	Std	# Best
Data	77.13	12.48	
VAE	81.28	10.50	5
ALI	84.60	5.73	3
HALI z1	91.35	5.62	27
HALI z2	86.28	5.64	3
Table 1: Summary of CelebA attributes classification from reconstructions for VAE, ALI and the two levels of
HALI. The data row is the summary for the VGG classifier and the other scores have been normalized by it.
Mean and standard deviation are expressed as percentages. # best represents the count of when a model has the
best score on a single attribute. Note that it does not sum to 40 as there were ties.
validation set. We consider a reconstruction as being good if it preserves - as measured by the trained
classifier - the attributes possessed by the original sample.
We report a summary of the statistics of the classifier’s accuracies in Table 1. We do this for three
different models, VAE, ALI and HALI. An inspection of the table reveals that the proportion of
attributes where HALI’s reconstructions outperforms the other models is clearly dominant. Therefore,
the encoder-decoder relationship of HALI better preserves the identifiable attributes compared to
other models leveraging such relationships. Please refer to Table 5 in the appendix for the full table
of attributes score.
5.3	Perceptual Reconstructions
In the same spirit as Larsen et al. (2015), we construct a metric by computing the Euclidean distance
between the input images and their various reconstructions in the discriminator’s feature space. More
precisely, let ∙ → D(∙) be the embedding of the input to the pen-ultimate layer of the discriminator.
We compute the discriminator embedded distance
dc(u, V)= ∖∖D(u, Ui, U2) - D(v, Vι, V2) ∣∣2 ,	(8)
where ∙ → ∣∣∙k2 is the Euclidean norm. We then compute the average distances dc(x,Xi) and
dc(x, X2) over the ImageNet validation set. Fig. 3a shows that under do the average reconstruction
errors for both Xi and X2 decrease steadily as training advances. Furthermore, the reconstruction
error under dc of the reconstructions from the first level of the hierarchy are uniformly bounded
by above by those of the second. We note that while the VAEGAN model of Larsen et al. (2015)
explicitly minimizes the perceptual reconstruction error by adding this term to their loss function,
HALI implicitly minimizes it during adversarial training, as shown in subsection 3.2.
(a)
Figure 3: Comparison of average reconstruction error over the validation set for each level of reconstructions
using the Euclidean (a) and discriminator embedded (b) distances. Using both distances, reconstructions errors
for X 〜TX∣zι are uniformly below those for X 〜TX.. The reconstruction error using the Euclidean distance
eventually stalls showing that the Euclidean metric poorly approximates the manifold of natural images.
(b)
6	Empirical Analysis II: Learned Representations
We now move on to assessing the quality of our learned representation through inpainting, visualizing
the hierarchy and innovation vectors.
7
Under review as a conference paper at ICLR 2018
6.1	Inpainting
Inpainting is the task of reconstructing the missing or lost parts of an image. It is a challenging task
since sufficient prior information is needed to meaningfully replace the missing parts of an image.
While it is common to incorporate inpainting-specific training Yeh et al. (2016); Perez et al. (20θ3);
Pathak et al. (2016), in our case we simply use the standard HALI adversarial loss during training
and reconstruct incomplete images during inference time.
Figure 4: Inpainting on center cropped images on CelebA, SVHN and MS-COCO datasets (left to right).
We first predict the missing portions from the higher level reconstructions followed by iteratively
using the lower level reconstructions that are pixel-wise closer to the original image. Fig. 4 shows the
inpaintings on center-cropped SVHN, CelebA and MS-COCO (Lin et al., 2014) datasets without any
blending post-processing or explicit supervision. The effectiveness of our model at this task is due
the hierarchy - We can extract semantically consistent reconstructions from the higher levels of the
hierarchy, then leverage pixel-wise reconstructions from the lower levels.
Figure 5: Real CelebA faces (right) and their corresponding innovation tensor (IT) updates (left). For instance,
the third roW in the figure features Christina Hendricks folloWed by hair-color IT updates. Similarly, the first tWo
roWs depicts usage of smile-IT and the 4th roW glasses-plus-hair-color-IT.
6.2	Hierarchical latent representations
To qualitatively shoW that higher levels of the hierarchy encode increasingly abstract representation
of the data, We individually vary the latent variables and observe the effect.
The process is as folloW: We sample a latent code from the prior distribution z2 . We then multiply
individual components of the vector by scalars ranging from -3 to 3. For z1, We fix z2 and multiply
each feature map independently by scalars ranging from -3 to 3. In all cases these modified latent
vectors are then decoded back to input data space. Fig. 6 (a) and (b) exhibit some of those decodings
for z2, While (c) and (d) do the same for the loWer conditional z1. The last column contain the
decodings obtained from the originally sampled latent codes. We see that the representations learned
in the z2 conditional are responsible for high level variations like gender, While z1 codes imply
local/pixel-Wise changes such as saturation or lip color.
8
Under review as a conference paper at ICLR 2018
(a) CelebA orientation variation
(b) CelebA gender variation
(c) CelebA lipstick feature map variation
Figure 6: (a) and (b) showcase z2 vector variation. We sample a set of z2 vectors from the prior. We repeatedly
replace a single relevant entry in each vector by a scalar ranging from -3 to 3 and decode. (c) and (d) follows
the same process using the z1 latent space.
(d) CelebA saturation feature map variation
6.3	Latent semantic Innovation
With HALI, we can exploit the jointly learned hierarchical inference mechanism to modify actual
data samples by manipulating their latent codes. We refer to these sorts of manipulations as latent
semantic innovations.
Consider a given instance from a dataset X 〜q(x). Encoding X yields Zi and Z?. We modify Z2
by multiplying a specific entry by a scalar a. We denote the resulting vector by Zf. We decode the
latter and get Za 〜Tzγ∖^. We decode the unmodified encoding vector and get Zi 〜 Tzι∣Z2. We
then form the innovation tensor ηα = Zi - Zf. Finally, We subtract the innovation vector from the
initial encoding, thus getting Za = Zi - ηa, and sample Xa 〜Tx∖^α, This method provides explicit
control and alloWs us to carry out these variations on real samples in a completely unsupervised Way.
The results are shoWn in Fig. 5. These Were done on the CelebA validation set and Were not used for
training.
7	Empirical Evaluations III: Learning Predictive Representations
We evaluate the usefulness of our learned representation for doWnstream tasks by quantifying the
performance of HALI on attribute classification in CelebA and on a semi-supervised variant of the
MNIST digit classification task.
7.1	Unsupervised classification
FolloWing the protocol established by Berg & Belhumeur (2013); Liu et al. (2015), We train 40
linear SVMs on HALI encoder representations (i.e. We utilize the inference netWork) on the CelebA
validation set and subsequently measure performance on the test set. As in Berg & Belhumeur
(2013); Huang et al. (2016a); Kalayeh et al. (2017), We report the balanced accuracy in order to
evaluate the attribute prediction performance. We emphasize that, for this experiment, the HALI
encoder and decoder Were trained in on entirely unsupervised data. Attribute labels Were only used to
train the linear SVM classifiers.
A summary of the results are reported in Table 2. HALI’s unsupervised features surpass those
of VAE and ALI, but more remarkably, they outperform the best handcrafted features by a Wide
margin (Zhang et al., 2014). Furthermore, our approach outperforms a number of supervised (Huang
et al., 2016a) and deeply supervised (Liu et al., 2015) features. Table 6 in the appendix arrays the
results per attribute.
9
Under review as a conference paper at ICLR 2018
	Mean	Std	# Best
TriPIet-kNN (SChrOff et al., 2015)	71.55	12.61	-0
PANDA (Zhang et al., 2014)	76.95	13.33	0
Anet (Liu et al., 2015)	79.56	12.17	0
LMLE-kNN (Huang et al., 2016a)	83.83	12.33	22
VAE	73.30	9.65	0
ALI	73.88	10.16	0
HALI		83.75	8.96	15
Table 2: Summary of statistics for CelebA attributes mean per-class balanced accuracy given in percentage
points. # best represents the count of when a model has the best score on a single attribute. Note that it does not
sum to 40 as there were ties.
	MNIST (# errors)	
VAE (M1+M2) (Kingma et al., 2014)	233 ± 14
VAT (Miyato et al., 2017)	136
CatGAN (Springenberg, 2015)	191 ± 10
Adversarial Autoencoder (Makhzani et al., 2015)	190 ± 10
PixelGAN (Makhzani & Frey, 2017)	108 ± 15
ADGM (Maal0e et al., 2016)	96 ± 2
Feature-Matching GAN (Salimans et al., 2016)	93 ± 6.5
Triple GAN (Li et al., 2017a)	91 ± 58
GSSLTRABG (Dai et al., 2017)		79.5 ± 9.8
HALI(OurS)	73
Table 3: Comparison on semi-supervised learning with state-of-the-art methods on MNIST with 100 labels
instance per class. Only methods without data augmentation are included.
7.2	Semi-supervised learning within HALI
The HALI hierarchy can also be used in a more integrated semi-supervised setting, where the
encoder also receives a training signal from the supervised objective. The currently most successful
approach to semi-supervised in adversarially trained generative models are built on the approach
introduced by Salimans et al. (2016). This formalism relies on exploiting the discriminator’s feature
to differentiate between the individual classes present in the labeled data as well as the generated
samples. Taking inspiration from (Makhzani et al., 2015; Makhzani & Frey, 2017), we adopt a
different approach that leverages the Markovian hierarchical inference network made available by
HALI,
x → z → y,
(9)
Where Z = enc(x + Q e), with e 〜N(0, I), and y is a categorical random variable. In practice, We
characterize the conditional distribution of y given z by a softmax. The cost of the generator is then
augmented by a supervised cost. Let us write Dsup as the set of pairs all labeled instance along with
their label, the supervised cost reads
Lsup =而_F	E Eyk l°g(yk).
| Dsup | (y,x)∈Dsup k
y~q(y|X)
(10)
We showcased this approach on a semi-supervised variant of MNIST(LeCun et al., 1998) digit
classification task with 100 labeled examples evenly distributed across classes.
Table 3 shows that HALI achieves a new state-of-the-art result for this setting. Note that unlike Dai
et al. (2017), HALI uses no additional regularization.
8	Conclusion and future work
In this paper, we introduced HALI, a novel adversarially trained generative model. HALI learns a
hierarchy of latent variables with a simple Markovian structure in both the generator and inference
10
Under review as a conference paper at ICLR 2018
networks. We have shown both theoretically and empirically the advantages gained by extending the
ALI framework to a hierarchy.
While there are many potential applications of HALI, one important future direction of research is to
explore ways to render the training process more stable and straightforward. GANs are well-known
to be challenging to train and the introduction of a hierarchy of latent variables only adds to this.
References
Thomas Berg and Peter N Belhumeur. Poof: Part-based one-vs.-one features for fine-grained
categorization, face verification, and attribute estimation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition,pp. 955-962, 2013.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. CoRR,
abs/1606.03657, 2016. URL http://arxiv.org/abs/1606.03657.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semi-
supervised learning that requires a bad gan. arXiv preprint arXiv:1705.09783, 2017.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a
laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems,
pp. 1486-1494, 2015.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Chelsea Finn, Ian J. Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction
through video prediction. CoRR, abs/1605.07157, 2016. URL http://arxiv.org/abs/
1605.07157.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David
Vdzquez, and Aaron C. Courville. Pixelvae: A latent variable model for natural images. CoRR,
abs/1611.05013, 2016. URL http://arxiv.org/abs/1611.05013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for
imbalanced classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5375-5384, 2016a.
Xun Huang, Yixuan Li, Omid Poursaeed, John E. Hopcroft, and Serge J. Belongie. Stacked generative
adversarial networks. CoRR, abs/1612.04357, 2016b. URL http://arxiv.org/abs/1612.
04357.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/
abs/1502.03167.
Mahdi M Kalayeh, Boqing Gong, and Mubarak Shah. Improving facial attribute prediction using
semantic segmentation. arXiv preprint arXiv:1704.08740, 2017.
Theofanis Karaletsos. Adversarial message passing for graphical models. NIPS workshop on
Advances in Approximate Bayesian Inference, 2016.
11
Under review as a conference paper at ICLR 2018
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems, pp.
3581-3589, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.
Alex Lamb, Vincent Dumoulin, and Aaron Courville. Discriminative regularization for generative
models. arXiv preprint arXiv:1602.03220, 2016.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. AU-
toencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300,
2015.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits,
1998.
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. arXiv preprint
arXiv:1703.02291, 2017a.
Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, and Lawrence
Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In
Advances in Neural Information Processing Systems (NIPS), 2017b.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolldr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
Lars Maal0e, Casper Kaae S0nderby, S0ren Kaae S0nderby, and Ole Winther. Auxiliary deep
generative models. arXiv preprint arXiv:1602.05473, 2016.
Alireza Makhzani and Brendan Frey. Pixelgan autoencoders. arXiv preprint arXiv:1706.00531, 2017.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.
arXiv preprint arXiv:1511.05644, 2015.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a reg-
ularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976,
2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 4. Granada, Spain, 2011.
Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. Plug & play gener-
ative networks: Conditional iterative generation of images in latent space. CoRR, abs/1612.00005,
2016. URL http://arxiv.org/abs/1612.00005.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems,
pp. 4790-4798, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 2016b.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context
encoders: Feature learning by inpainting. CoRR, abs/1604.07379, 2016. URL http://arxiv.
org/abs/1604.07379.
12
Under review as a conference paper at ICLR 2018
Patrick P6rez, Michel Gangnet, and Andrew Blake. Poisson image editing. ACM Trans. Graph., 22
(3):313-318, July 2003. ISSN 0730-0301. doi: 10.1145/882262.882269. URL http://doi.
acm.org/10.1145/882262.882269.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational
approaches for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987,
2017.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 115(3):211-252, 2015.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In Advances in Neural Information Processing
Systems, pp. 901-901, 2016.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering-1a_089. pdf. 2015.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learn-
ing from simulated and unsupervised images through adversarial training. CoRR, abs/1612.07828,
2016. URL http://arxiv.org/abs/1612.07828.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014. URL http://arxiv.org/abs/1409.1556.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Adversarial generator-encoder networks.
arXiv preprint arXiv:1704.02304, 2017.
Raymond Yeh, Chen Chen, Teck-Yian Lim, Mark Hasegawa-Johnson, and Minh N. Do. Semantic
image inpainting with perceptual and contextual losses. CoRR, abs/1607.07539, 2016. URL
http://arxiv.org/abs/1607.07539.
Ning Zhang, Manohar Paluri, Marc’Aurelio Ranzato, Trevor Darrell, and Lubomir Bourdev. Panda:
Pose aligned networks for deep attribute modeling. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1637-1644, 2014.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017. URL http://
arxiv.org/abs/1703.10593.
13
Under review as a conference paper at ICLR 2018
14
Under review as a conference paper at ICLR 2018
A Architecture Details
Operation	Kernel	Strides	Feature maps	BN/WN?	Dropout	Nonlinearity
Gzi (x) - 3 X 128 X 128 input						
Convolution	3x3	1x1	32	x	0.0	Leaky ReLU
Convolution	3x3	2x2	64	BN	0.0	Leaky ReLU
Resnet Block	3x3	1x1	64	BN	0.0	Leaky ReLU
Resnet Block	3x3	1x1	64	BN	0.0	Leaky ReLU
Convolution	3x3	2x2	128	BN	0.0	Leaky ReLU
Resnet Block	3x3	1x1	128	BN	0.0	Leaky ReLU
Convolution	3x3	1x1	128	x	0.0	Leaky ReLU
Gaussian Layer Gz2 (zι) - 64 x 32 x 32 input						
Convolution	3x3	2x2	256	BN	0.0	Leaky ReLU
Convolution	3x3	2x2	256	BN	0.0	Leaky ReLU
Convolution	3x3	2x2	512	BN	0.0	Leaky ReLU
Resnet Block	3x3	1x1	512	BN	0.0	Leaky ReLU
Convolution	4x4	valid	512	BN	0.0	Leaky ReLU
Convolution	1x1	1x1	512	x	0.0	Linear
Gzi (z2) - 128 x 1 x 1 input						
Convolution	1x1	1x1	4 * 4 * 256	BN	0.0	Leaky ReLU
Bilinear Upsampling						
Resnet Block	3x3	1x1	256	BN	0.0	Leaky ReLU
Bilinear Upsampling						
Convolution	3x3	1x1	256	BN	0.0	Leaky ReLU
Convolution	3x3	1x1	128	BN	0.0	Leaky ReLU
Bilinear Upsampling						
Convolution	3x3	1x1	128	x	0.0	Leaky ReLU
Gaussian Layer Gχ(zι) - 64 x 32 x 32 input						
Convolution	1x1	1x1	64	BN	0.0	Leaky ReLU
Resnet Block	3x3	1x1	64	BN	0.0	Leaky ReLU
Bilinear Upsampling						
Resnet Block	3x3	1x1	64	BN	0.0	Leaky ReLU
Bilinear Upsampling						
Convolution	3x3	1x1	64	BN	0.0	Leaky ReLU
Convolution	3x3	1x1	32	BN	0.0	Leaky ReLU
Convolution	1x1	1x1	3	x	0.0	Tanh
D(X) - 3 X 128 X 128 input						
Convolution	3x3	1x1	32	WN	0.2	Leaky ReLU
Convolution	3x3	2x2	64	WN	0.5	Leaky ReLU
Convolution	3x3	2x2	64	WN	0.5	Leaky ReLU
Convolution	3x3	1x1	64	WN	0.5	Leaky ReLU
D(x, z1) - 128 x 32 x 32 input						
Concatenate D(x) and z1 along the channel axis Convolution 3 x 3	1 x 1	128	WN					0.2	Leaky ReLU
Convolution	3x3	2x2	256	WN	0.5	Leaky ReLU
Convolution	3x3	2x2	256	WN	0.5	Leaky ReLU
Convolution	3x3	2x2	512	WN	0.5	Leaky ReLU
Convolution	4x4	valid	512	WN	0.2	Leaky ReLU
D(x, z1, z2) - 512 x 1 x 1 input						
Concatenate D(x, z1) and z2 along the channel axis Convolution 1 x 1	1 x 1	1024	x					0.5	Leaky ReLU
Convolution	1x1	1x1	1024	x	0.5	Leaky ReLU
Convolution	1x1	1x1	1	x	0.5	Sigmoid
Table 4: Architecture detail for HALI(unsupervised) on the Imagenet 128 and CelebA 128 Datasets.
15
Under review as a conference paper at ICLR 2018
B Proofs
Lemma 1. Let f be a valid f-divergence generator. Let p and q be joint distributions over a random
vector x. Let xA be any strict subset of x and x-A its complement, then
Df (p(x) || q(x)) ≥ Df (p(xA) || q(xA))
(11)
Proof. By definition, we have
Df(P(X) || q(X)) = Ex 〜q(X)[f ( q(X))] = ExA 〜q(XA)Ex-A 〜q(X-AIxA ) [f ( ^(^^j^(^ʒɪ^ɪ))]
Using that f is convex, Jensen’s inequality yields
Df(P(X) || q(x)) ≥ ExA 〜q(xA)[f(Ex-A 〜q(x-A∣xA) P(XA)片:禺)]
Simplifying the inner expectation on the right hand side, we conclude that
Df(P(X) Il q(x)) ≥ ExA〜q(xA)[f(p⅛Aτ)]
q(XA)
□
Lemma 2 (Kullback-Leibler’s upper bound by Jensen-Shannon). Assume that P and q are two
probability distribution absolutely continuous with respect to each other. Moreover, assume that q is
bounded away from zero. Then, there exist a positive scalar K such that
DKL(P || q) ≤ KDJS(P || q).	(12)
Proof. We start by bounding the Kullblack-Leibler divergence by the χ2-distance. We have
DKL(P || q) ≤
∕iog(Px))dχ ≤ iog(i + Dχ2(p Il q)) ≤ Dχ2(p Il q)
(13)
The first inequality follows by Jensen’s inequality. The third inequality follows by the Taylor
expansion. Recall that both the χ2 -distance and the Jensen-Shanon divergences are f-divergences
with generators given by fχ2 (t) = (t - 1)2 and fjs(t) = U log() + log(t+^), respectively. We
fx2(t)
fjs (t).
form the function t 7→ h(t)
h is strictly increasing on [0, ∞). Since we are assuming
q to be bounded away from zero, we know that there is a constant c1 such that q(X) > c1 for
all X. Subsequently for all x, we have that P(X) ≤ c2 := maxx Pc(X). Thus, for all X we have
h(怒)≤ K ：= h(c2) and hencefχ2 (P) ≤ KfJS(翳).Intergrating with respect to q, we
conclude
DKL(P II q) ≤ Dχ2 (P II q) ≤ KDJS(P II q)
□
Proposition 3. Assuming q(X, zl) and P(X, zl) are positive for any l ∈ {1, . . . , L}. We have
Ex 〜q(x)[Ll(X)] - H (x I Zl) ≤ KDJS (p(X, zι,∙∙∙, ZL) Il q(X, zι,∙∙∙, ZL)))	(14)
Where H(X I zl) is computed under the encoder’s distribution q(X, zl)
Proof. By elementary manipulations we have.
Ex〜q(x)[Ll(X)] = DKL(P(x, zl) II q(X, zl)) + H(Xl I zl) - DKL(P(Zl) II q(zl))
Where the conditional entropy H(Xl I Zl) is computed q(X, Zl). By the non-negativity of the
KL-divergence we obtain
Ex〜q(x)[Ll(X)] ≤ Dkl(p(x, zl) II q(X, zl)) + H(XlI Zl)
Using lemma 2, we have
16
Under review as a conference paper at ICLR 2018
(b) CIFAR10 from z1
Figure 7: Reconstructions for SVHN and CIFAR10 from z1 and reconstructions from z2 . Odd columns
corresponds to examples from the validation set while even columns are the model’s reconstructions
(d) CIFAR10 from z2
Ex〜Pd(x)[Ll(x)] - H(X | Zl) ≤ KDJS(p(x, Zl) || q(x, Zl)))
The Jensen-Shanon divergence being f-divergence, using Lemma 1, we conclude
Ex〜Pd(x) [L (X)] ― H(X | Zl) ≤ KDJS(P(X, z1, . . ., ZL) || q(X, z1, . . . , ZL))).
□
Proposition 4. For any given latent variable Zl, the reconstruction likelihood
Ex〜qχ[Ez〜τzι∣χ[- logP(X | Zl)]] is an upper bound on H(X | Zl).
Proof. By the non-negativity of the Kullback-Leibler divergence, we have that
DKL(q(X | Zl) || P(X | Zl)) ≥ 0 =⇒ H[q(X | Zl)] ≤ Ezl〜TzlIx[-log(P(X | zi))]
. Integrating over the marginal and applying Fubini’s theorem yields
H(X | Zl)
≤ Ex 〜P(X)Ezl〜TzlIx [- log(p(X | Zl))],
where the conditional entropy H(x | Zl) is computed under the encoder distribution.	□
17
Under review as a conference paper at ICLR 2018
8684929984
6980929587
7286799091
8469848887
6086879889
7789839488
8877728978
9283779283
7477769580
7073898188
6096839283
8093838990
8074779278
8183919592
8874819594
9079889189
9198949896
Data	86	81	69
VAE	80	96	60
ALI	72	90	83
HALI (z1)	92	93	93
HALI (z2)	80	78	91
7462838381
7264829185
9295859789
7173908287
5878858887
5482838388
9486889490
5492939399
6965879486
9473819075
9698929895
8591869689
8082778480
9793909790
8978769484
5683899190
7167827175
点
华
Q
69
69
89
90
89
I
-J
S
60
94
87
91
88
⅛Q
Data
VAE
ALI
HALI (z1)
HALI (z2)
Table 5: CelebA attributes accuracies of reconstructions by different models. The data row displays the raw
average of positive accuracies, predicting true 1, and negative accuracies, predicting a true 0 by our VGG
classifier. Other rows show the same average accuracies where each individual accuracy is normalized by its
corresponding data score. The numbers are all percentages.
86898992756485
8568599
8999778
2951893
7789889
3465092
7889879
2858156
8899889
60646874807886
64657079777885
8422212
6788778
76858387626477
43505659646578
81919099838892
82849092757585
68727880676974
55565760585762
81929098868793
5430794
7779889
63677073686878
83858788626980
73
77
76
79
65
70
Triplet-kNN	66
PANDA	76
Anet	81
LMLE-kNN	82
VAE	78
ALI	78
HALI(Unsup) 86	77
75788487697180
3590329
7879888
50515959676771
1759822
9999789
84909099849195
69778383656778
77788183697277
3693500
6667567
92989799816891
71817988797791
4868909
6677788
60677076797884
61676972596169
63697780868589
61666768515465
2736990
8899779
47515759606272
57636173818290
92939696675288
Triplet-kNN	91
PANDA	99
Anet	99
LMLE-kNN	99
VAE	78
ALI	83
HALI(Unsup)	96
Table 6: Mean per-class balanced accuracy in percentage points of each of the 40 face attributes on CelebA.
18