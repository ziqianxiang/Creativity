Under review as a conference paper at ICLR 2018
Characterizing Sparse Connectivity Patterns
in Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel way of reducing the number of parameters in the storage-
hungry fully connected layers of a neural network by using pre-defined sparsity,
where the majority of connections are absent prior to starting training. Our re-
sults indicate that convolutional neural networks can operate without any loss of
accuracy at less than 0.5% classification layer connection density, or less than 5%
overall network connection density. We also investigate the effects of pre-defining
the sparsity of networks with only fully connected layers. Based on our sparsi-
fying technique, we introduce the ‘scatter’ metric to characterize the quality of a
particular connection pattern. As proof of concept, we show results on CIFAR,
MNIST and a new dataset on classifying Morse code symbols, which highlights
some interesting trends and limits of sparse connection patterns.
1 Introduction
Neural networks (NNs) in machine learning systems are critical drivers of new technologies such
as image processing and speech recognition. Modern NNs are gigantic in size with millions of pa-
rameters, such as the ones described in Alexnet (Krizhevsky et al., 2012), Overfeat (Sermanet et al.,
2013) and ResNet (He et al., 2016). They therefore require an enormous amount of memory and
silicon processing during usage. Optimizing a network to improve performance typically involves
making it deeper and adding more parameters (Simonyan & Zisserman, 2015; Szegedy et al., 2015;
Huang et al., 2016), which further exacerbates the problem of large storage complexity. While the
convolutional (conv) layers in these networks do feature extraction, there are usually fully connected
layers at the end performing classification. We shall henceforth refer to these layers as connected
layers (CLs), of which fully connected layers (FCLs) are a special case. Owing to their high density
of connections, the majority of network parameters are concentrated in FCLs. For example, the
FCLs in Alexnet account for 95.7% of the network parameters (Zhang et al., 2016).
We shall refer to the spaces between CLs as CL junctions (or simply junctions), which are occupied
by connections, or weights. Given the trend in modern NNs, We raise the question - "How necessary
is it to have FCLs?” or, in other words, “What if most of the junction connections never existed?
Would the resulting sparsely connected layers (SCLs), when trained and tested, still give competitive
performance?” As an example, consider a network with 2 CLs of 100 neurons each and the junction
between them has 1000 weights instead of the expected 10,000. Then this is a sparse network with
connection density of 10%. Given such a sparse architecture, a natural question to ask is “How can
the existing 1000 weights be best distributed so that network performance is maximized?”
In this regard, the present work makes the following contributions. In Section 2, we formalize the
concept of sparsity, or its opposite measure density, and explore its effects on different network
types. We show that CL parameters are largely redundant and a network pre-defined to be sparse
before starting training does not result in any performance degradation. For certain network archi-
tectures, this leads to CL parameter reduction by a factor of more than 450, or an overall parameter
reduction by a factor of more than 20. In Section 2.4, we discuss techniques to distribute connec-
tions across junctions when given an overall network density. Finally, in Section 3, we formalize
pre-defined sparse connectivity patterns using adjacency matrices and introduce the scatter metric.
Our results show that scatter is a quick and useful indicator of how good a sparse network is.
1
Under review as a conference paper at ICLR 2018
2 Pre-defined Sparsity
As an example of the footprint of modern NNs, AlexNet has a weight size of 234 MB and requires
635 million arithmetic operations only for feedforward processing (Zhang et al., 2016). It has been
shown that NNs, particularly their FCLs, have an excess of parameters and tend to overfit to the
training data (Denil et al., 2013), resulting in inferior performance on test data. The following
paragraph describes several previous works that have attempted to reduce parameters in NNs.
Dropout (deletion) of random neurons (Srivastava et al., 2014) trains multiple differently configured
networks, which are finally combined to regain the original full size network. Chen et al. (2015)
randomly forced the same value on collections of weights, but acknowledged that “a significant
number of nodes [get] disconnected from neighboring layers.” Other sparsifying techniques such
as pruning and quantization (Han et al., 2016; 2015; Zhou et al., 2016; Gong et al., 2014) first train
the complete network, and then perform further computations to delete parameters. Sindhwani et al.
(2015) used low rank matrices to impose structure on network parameters. Srinivas et al. (2016)
proposed a regularizer to reduce parameters in the network, but acknowledged that this increased
training complexity. In general, all these architectures deal with FCLs at some point of time during
their usage cycle and therefore, do not permanently solve the parameter explosion problem of NNs.
2.1	Our Methodology
Our attempt to simplify NNs is to pre-define the level of sparsity, or connection density, in a network
prior to the start of training. This means that our network always has fewer connections than its FCL
counterpart; the weights which are absent never make an appearance during training or inference.
In our notation, a NN will have J junctions, i.e. J + 1 layers, with {N1,N2, •…，NJ +ι} being
the number of neurons in each layer. Ni and Ni+1 are respectively the number of neurons in the
earlier (left) and later (right) layers of junction i. Every left neuron has a fixed number of edges
going from it to the right, and every right neuron has a fixed number of edges coming into it from
the left. These numbers are defined as fan-out (foi) and fan-in (fii), respectively. For conventional
FCLs, foi = Ni+1 and fii = Ni . We propose SCLs where foi < Ni+1 and fii < Ni, such that
Ni × foi = Ni+1 × fii = Wi, the number of weights in junction i. Having a fixed foi and fii
ensures that all neurons in a junction contribute equally and none of them get disconnected, since that
would lead to a loss of information. The connection density in junction i is given as Wi/(NiNi+1)
and the overall CL connection density is defined as
PiJ=1 Wi	/	PiJ=1 NiNi+1 .
Note that earlier works such as Dey et al. (2017b;a) have proposed hardware architectures that
leverage pre-defined sparsity to speed up training. However, a complete analysis of methods to
pre-define connections, its possible gains on different kinds of modern deep NNs and a test of its
limits via a metric quantifying its goodness has been lacking. Bourely et al. (2017) introduced a
metric based on eigenvalues, but ran limited tests on MNIST. The following subsections analyze our
method of pre-defined sparsity in more detail. We experimented with networks operating on CIFAR,
MNIST and Morse code symbol classification - a new dataset described in Dey (2017)1.
2.2	Network Experiments
2.2	. 1 CIFAR
We used the original CIFAR10 and CIFAR100 datasets without data augmentation. Our network
has 6 conv layers with number of filters equal to [64, 64, 128, 128, 256, 256]. Each has window size
3x3. The outputs are batch-normalized before applying ReLU non-linearity. A max-pooling layer
of pool size 2x2 succeeds every pair of conv layers. This structure finally results in a layer of 4096
neurons, which is followed by the CLs. We used the Adam optimizer, ReLU-activated hidden layers
and softmax output layer - choices which we maintained for all networks unless otherwise specified.
Our results in Section 2.4 indicate that later CL junctions (i.e. closer to the outputs) should be
denser than earlier ones (i.e. closer to the inputs). Moreover, since most CL networks have a ta-
pering structure where Ni monotonically decreases as i increases, more parameter savings can be
1L2 regularization is used wherever applicable for the MNIST networks. For Morse, the difference with
and without regularization is negligible, while for CIFAR, the accuracy results differ by about 1%
2
Under review as a conference paper at ICLR 2018
achieved by making earlier layers less dense. Accordingly we did a grid search and picked CL junc-
tion densities as given in Table 1. The phrase ‘conv+2CLs’ denotes 2 CL junctions corresponding
to a CL neuron configuration of (4096, 512, 16) for CIFAR10, (4096, 512, 128) for CIFAR1002,
and (3136, 784, 10) for MNIST (see Section 2.2.2). For ‘conv+3CLs’, an additional 256-neuron
layer precedes the output. ‘MNIST CL’ and ‘Morse CL’ refer to the CL only networks described
subsequently, for which we have only shown some of the more important configurations in Table 1.
As an example, consider the first network in ‘CIFAR10 conv+2CLs’ which has fo1 = fo2 = 1.
This means that the individual junction densities are (4096 × 1)/(4096 × 512) = 0.2% and (512 ×
1)/(512 × 16) = 6.3%, to give an overall CL density of (4096 × 1+512× 1)/(4096 × 512 + 512 ×
16) = 0.22%. In other words, while FCLs would have been 100% dense with 2, 097, 152 + 8192 =
2, 105, 344 weights, the SCLs use 4096 + 512 = 4608 weights, which is 457 times less. Note that
weights in the sparse junction are distributed as fixed, but randomly generated patterns, with the
constraints of fixed fan-in and fan-out.
Table 1: Densities for some of our sparse networks
Net	Junction fan-outs	CL Junction Densities (%)	Overall CL Density (%)	Net	Junction fan-outs	CL Junction Densities (%)	Overall CL Density (%)
CIFAR10 二	1,1	0.2,6.3 =	0.22 二	CIFAR10 "	1,1,1	0.2,0.4,6.3 =	0.22 二
conv+2CLs	1,8	0.2,50	0.39	conv+3CLs	1,1,8 1,2,16	0.2,0.4,50 0.2,0.8,100	0.3 0.41
CIFAR100	11	0.2,0.8	0.21	CIFAR100	1,1,1	0.2,0.4,0.8	0.22
conv+2CLs	1,8 1,32	0.2,6.3 0.2,25	0.38 0.95	conv+3CLs	1,1,16 1,2,32	0.2,0.4,13 0.2,0.8,25	0.39 0.59
MNIST	15	0.150	0.29	MNISTCL	4,10	1.79,100	302
conv+2CLs	4,10	0.5,100	0.83	(x = 224)	112,10	50,100	50.63
	16,10	2,100	2.35	Morse CL —	512,32	50,50 —	50	一
(c) CIFAR10conv+3CLs (left),
co∩v+2CLs (right)
(a) CIFAR10 conv+2CLs
9070503010
(%) Λ□≡jn□□v UORep⅛>
Density-Sjunctions
86.4	86.6	86.4
100% 0.95% 0.38% 0.21%
Overall Connection
Density-Zjunctions
100%	0.39%	0.22%
Overall Connection
Density - 2 junctions
58.6 59.1 57	55.4
5 IO 15	20	25	30	5	10	15	20	25	30
(d) CIFAR100 conv+2CLs (e) CIFAR100 conv+3CLs ⑴ CIFAR100 conv+3CLs (left),
co∩v+2CLs (right)
Figure 1: Performance results of pre-defined sparsity for (a)-(C) CIFAR10, and (d)-(f) CIFAR100,
trained for 30 epochs using different network densities and varying number of CLs. (a),(b),(d),(e)
Validation accuracy across epochs. (c),(f) Best validation accuracies after 1, 5 and 30 epochs.
Figure 1 shows the results for CIFAR. Subfigures (a), (b), (d) and (e) show classification perfor-
mance on validation data as the network is trained for 30 epochs (note that the final accuracies
stayed almost constant after 20 epochs). The different lines correspond to different overall CL den-
sities. Subfigures (c) and (f) show the best validation accuracies after 1, 5 and 30 epochs for the
different CL densities. We see that the final accuracies (the numbers at the top of each column)
show negligible performance degradation for these extremely low levels of density, not to mention
some cases where SCLs outperform FCLs. These results point to the promise of sparsity. Also
2Powers of 2 are used for ease of testing sparsity. The extra output neurons have a ‘false’ ground truth
labeling and thus do not impact the final classification accuracy.
3
Under review as a conference paper at ICLR 2018
notice from subfigures (c) and (f) that SCLs generally start training quicker than FCLs, as evidenced
by their higher accuracies after 1 epoch of training. See Appendix Section 5.3 for more discussion.
2.2.2	MNIST
We used 2 different kinds of networks when experimenting on MNIST (no data augmentation). The
first was 'conv+2CLs' - 2 ConV layers having 32 and 64 filters of size 5x5 each, alternating With 2x2
max pooling layers. This results in a layer of 3136 neurons, which is followed by 2 CLs having 784
and 10 neurons, i.e. 2 junctions overall. Fig. 2(a) and (b) show the results. Due to the simplicity of
the overall network, performance starts degrading at higher densities compared to CIFAR. However,
a network with CL density 2.35% still matches FCLs in performance. Note that the total number of
weights (conv+SCLs) is 0.11M for this network, which is only 4.37% of the original (conv+FCLs).
The second was a family of networks with only CLs, either having a single junction with a neuron
configuration of (1024, 16), or 2 junctions configured as (784, x, 10), where x varies. The results are
shown in Fig. 2(c), which offers two insights. Firstly, performance drops off at higher densities for
CL only MNIST networks as compared to the one with conv layers. However, half the parameters
can still be dropped without appreciable performance degradation. This aspect is further discussed
in Section 2.3. Secondly, large SCLs perform better than small FCLs with similar number of param-
eters. Considering the black-circled points as an example, performance drops when switching from
224 hidden neurons at 12.5% density to 112 at 25% to 56 at 50% to 28 at 100%, even though all
these networks have similar number of parameters. So increasing the number of hidden neurons is
desirable, albeit with diminishing returns.
(a) MNIST conv+2CLs (b) MNIST conv+2CLs	(c) MNIST CL only
3 1 9 7 5 3
9.9.& & & &
9 9 9 9 9 9
(％) >u23υu< UoAeP=e>
Figure 2: (a)-(b) Performance results of pre-defined sparsity on an MNIST conv network with
different densities, each trained for 30 epochs. (c) Performance vs. connection density for different
MNIST CL only networks, each trained for 100 epochs.
2.2.3	MORSE
The Morse code dataset presents a harder challenge for sparsity. It only has 64-valued inputs (as
compared to 784 for MNIST and 3072 for CIFAR), so each input neuron encodes a significant
amount of information. The outputs are Morse codewords and there are 64 classes. Distinctions
between inputs belonging to different classes is small. For example, the input pattern for the Morse
codeword ‘.......’ can be easily confused with the codeword ‘... -’. As a result, performance
degrades quickly as connections are removed. Our network had 64 input and output neurons and
1024 hidden layer neurons, i.e. 3 CLs and 2 junctions, trained using stochastic gradient descent.
The results are shown in Fig. 3(a). As with MNIST CL only, 50% density can be achieved with
negligible degradation in accuracy.
2.3	Analyzing the Results of Pre-defined Sparsity
Our results indicate that for deep networks having several conv layers, there is severe redundancy in
the CLs. As a result, they can be made extremely sparse without hampering network performance,
which leads to significant memory savings. If the network only has CLs, the amount of density
reduction achievable without performance degradation is smaller. This can be explained using the
argument of relative importance. For a network which extensively extracts features and processes its
raw input data via conv filters, the input to the CLs can already substantially discriminate between
4
Under review as a conference paper at ICLR 2018
Figure 3: (a) Performance vs. connection density for a Morse CL only 2 junction network. (b) and
(c) Performance results by varying individual junction densities while overall density is fixed at (b)
25% (c) 50%. All cases trained for 30 epochs.
inputs belonging to different classes. As a result, the importance of the CLs’ functioning is less as
compared to a network where they process the raw inputs.
The computational savings by sparsifying CLs, however, are not as large because the conv layers
dominate the computational complexity. Other types of NNs, such as restricted Boltzmann ma-
chines, have higher prominence of CLs than CNNs and would thus benefit more from our approach.
Table 2 shows the overall memory and computational gains obtained from pre-defining CLs to be
sparse for our networks. The number of SCL parameters (params) are calculated by taking the mini-
mum overall CL density at which there is no accuracy loss. Note that the number of operations (ops)
for CLs is nearly the same as their number of parameters, hence are not explicitly shown.
Table 2: Savings in some of our NN architectures due to pre-defined sparsity
Net	CLs/ Total Layers	Conv Params (M)	ConV OPS (B)	FC CL Params (M)	Sparse CL Par- ams (M)	OVerall Param % Reduction	Overall Op % Reduction
Morse CL	2/2	0	0	0.131	0.066	50	50
MNIST CL (X = 224)	2/2	0	0	0.178	0.089	50	50
MNIST conv+2CLs	2/6	0.05	0.1	2.47	0.06	95.63	1829
CIFAR10 conv+2CLs	2/17	1.15	0.15	2.11	0.005	64.63	135
CIFAR100 conv+2CLs	2/17	1.15	0.15	2.16	002	64.76	138
CIFAR10 conv+3CLs	3/18	1.15	0.15	2.23	0.009	65.83	143
CIFAR100 conv+3CLs	3/18	1.15	0.15	2.26	0.013	65.99	1.45
2.4	Distributing Individual Junction Densities
Note that the Morse code network has symmetric junctions since each will have 64 × 1024 = 65, 536
weights to give a total of 131,072 FCL weights. Consider an example where overall density of 50%
(i.e. 65,536 total SCL weights) is desired. This can be achieved in multiple ways, such as making
both junctions 50% dense, i.e. 32,768 weights in each. Here we explore if individual junction
densities contribute equally to network performance.
Figures 3(b) and (c) sweep junction 1 and 2 connectivity densities on the x-axis such that the result-
ing overall density is fixed at 25% for (b) and 50% for (c). The black vertical line denotes where
the densities are equal. Note that peak performance in both cases is achieved to the left of the black
line, such as in (c) where junction 2 is 75% dense and junction 1 is 25% dense. This suggests that
later junctions need more connections than earlier ones. See Appendix Section 5.1 for more details.
3	Connectivity patterns
We now introduce adjacency matrices to describe junction connection patterns. Let Ai ∈
{0, 1}Ni+1 ×Ni be the (simplified) adjacency matrix of junction i, such that element [Ai]j,k indicates
whether there is a connection between the jth right neuron and kth left neuron. Ai will have fii 1’s
on each row and foi 1’s on each column. These adjacency matrices can be multiplied to yield the ef-
5
Under review as a conference paper at ICLR 2018
fective connection pattern between any 2 junctions X and Y , i.e. AX:Y = QiX=Y Ai ∈ Z≥N0Y+1×NX,
where element [AX:Y ]j,k denotes the number of paths from the kth neuron in layer X to the jth neu-
ron in layer (Y + 1). For the special case where X = 1 and Y = J (total number of junctions), we
obtain the input-output adjacency matrix A1:J. As a simple example, consider the (8, 4, 4) network
shown in Fig. 4 where fo1 = 1 and fo2 = 2, which implies that fi1 = fi2 = 2. A1 and A2 are
adjacency matrices of single junctions. We obtain the input-output adjacency matrix A1:2 = A2A1,
equivalent f o1:2 = fo1fo2 = 2, and equivalent f i1:2 = fi1fi2 = 4. Note that this equivalent
junction 1:2 is only an abstract concept that aids visualizing how neurons connect from the inputs to
the outputs. It has no relation to the overall network density.
Overall density = (8+8)/(32+16) = 33%
Junction 1
$ = 8" = 4
fo y = 1 , f/ y = 2
Density = 25%
ʌi =
00001010
00100001
10000100
01010000
100 1
00 11
1100
0 110
Equivalent Junction 1:2
N1 =8, N3 = 4
Equivalent fθy.2 = 1x2 = 2
Equivalent fi1.2 =2x2 = 4
01011010
11010100
00101011
10100101
Figure 4: An example of adjacency matrices and equivalent junctions.
We now attempt to characterize the quality of a sparse connection pattern, i.e. we try to find the best
possible way to connect neurons to optimize performance. Since sparsity gives good performance,
we hypothesize that there exists redundancy / correlated information between neurons. Intuitively,
we assume that left neurons of a junction can be grouped into windows depending on the dimen-
sionality of the left layer output. For example, the input layer in an MNIST CL only network would
have 2D windows, each of which might correspond to a fraction of the image, as shown in Fig. 5(a).
When outputs from a CL have an additional dimension for features, such as in CIFAR or the MNIST
conv network, each window is a cuboid capturing fractions of both spatial extent and features, as
shown in Fig. 5(b). Given such windows, we will try to maximize the number of left windows to
which each right neuron connects, the idea being that each right neuron should get some information
from all portions of the left layer in order to capture global view. To realize the importance of this,
consider the MNIST output neuron representing digit 2. Let’s say the sparse connection pattern is
such that when the connections to output 3 are traced back to the input layer, they all come from
the top half of the image. This would be undesirable since the top half of an image of a 2 can be
mistaken for a 3. A good sparse connection pattern will try to avoid such scenarios by spreading the
connections to any right neuron across as many input windows as possible. The problem can also
be mirrored so that every left neuron connects to as many different right windows as possible. This
ensures that local information from left neurons is spread to different parts of the right layer. The
grouping of right windows will depend on the dimensionality of the input to the right layer.
The window size is chosen to be the minimum possible such that the ideal number of connections
from or to it remains integral. The example from Fig. 4 is reproduced in Fig. 6. Since fi1 = 2,
the inputs must be grouped into 2 windows so that ideally 1 connection from each reaches every
hidden neuron. If instead the inputs are grouped into 4 windows, the ideal number would be half
of a connection, which is not achievable. In order to achieve the minimum window size, we let the
number of left windows be fi and the number of right windows be fo. So in junction i, the number
of neurons in each left and right window is Ni/fii and Ni+1/foi, respectively. Then we construct
left- and right-window adjacency matrices Aiwil ∈ Z≥N0i+1 ×fii and Aiwir ∈ Zf≥o0i×Ni by summing up
entries of Ai as shown in Fig. 5(c). The window adjacency matrices describe connectivity between
windows and neurons on the opposite side. Ideally, every window adjacency matrix for a single
junction should be the all 1s matrix, which signifies exactly 1 connection from every window to
6
Under review as a conference paper at ICLR 2018
Figure 5: (a) Example of 16 2D windows for an MNIST input image. (b) Example of 3D windows
when the output from a layer also includes features. (c) Construction of window adjacency matrices.
O 1 2
2 1 O
Figure 6: Window adjacency matrices and scatter. Green neurons indicate ideal connectivity. The
hidden layer is split into 2 to show separate constructions of A1w1r and A2w2l
every neuron on the opposite side. Note that these matrices can also be constructed for multiple
junctions, i.e. AwXX:Yl and AwXY:Yr , by multiplying matrices for individual junctions. See Appendix
Section 5.2 for more discussion.
3.1	Scatter
Scatter is a proxy for the performance of a NN. It is useful because it can be computed in a fraction
of a second and used to predict how good or bad a sparse network is without spending time training
it. To compute scatter, we count the number of entries greater than or equal to 1 in the window
adjacency matrix. If a particular window gets more than its fair share of connections to a neuron on
the opposite side, then it is depriving some other window from getting its fair share. This should not
be encouraged, so we treat entries greater than 1 the same as 1. Scatter is the average of the count,
i.e. for junction i:
1	Ni+1 fii
Sif = E X X IR- H
j=1 k=1
1	foi Ni
Sib = NfOi XX I ([Aw/ ≥ O ⑴
j=1 k=1
Subscripts f and b denote forward (left windows to right neurons) and backward (right neurons to
left windows), indicating the direction of data flow. As an example, we consider A1w1l in Fig. 6,
which has a scatter value S1f = 6/8 = 0.75. The other scatter values can be computed similarly
to form the scatter vector S = [S1f , S1b, S2f , S2b, Sf , Sb], where the final 2 values correspond to
junction 1:2. Notice that S Win be all 1s for FCLs, which is the ideal case. Incorporating sparsity
leads to reduced S values. The final scatter metric S ∈ [0,1] is the minimum value in S, i.e. 0.75
for Fig. 6. Our experiments indicate that any low value in S leads to bad performance, so we picked
the critical minimum value.
7
Under review as a conference paper at ICLR 2018
3.2	Analysis and Results of Scatter
We ran experiments to evaluate scatter using a) the Morse CL only network with fo = 128, 8,
b) an MNIST CL only network with (1024, 64, 16) neuron configuration and fo = 1, 4, and c)
the ‘conv+2CLs’ CIFAR10 network with fo = 1, 2. We found that high scatter indicates good
performance and the correlation is stronger for networks where CLs have more importance, i.e. CL
only networks as opposed to conv. This is shown in the performance vs. scatter plots in Fig. 7,
where (a) and (b) show the performance predicting ability of scatter better than (c). Note that the
random connection patterns used so far have the highest scatter and occur as the rightmost points in
each subfigure. The other points are obtained by specifically planning connections. We found that
When 1 junction was planned to give corresponding high values in S, it invariably led to low values
for another junction, leading to a low S . This explains why random patterns generally perform well.
75
(a) Morse CL only (b) MNIST CL only (c) CIFAR10 co∩v+2CLs
65
45
35
25
15
	[C 0	).68, 0.64, 0.66, l.62 0.91.0.911			∙
					
[1,1/8, 1, 1, 1, • 「1 1 "f∕α Y			1] or i 11		
c	>ame accuracy				
					
					
.L∣, ∣, ∣, • [1/8,1,		1, 1/0, 1/OJ 1,1,1/8,1/8]			
1 0.2 0.3 0.4 0.5 0.6 0.7
Scatter (S)
90 85 80 70				―[0.64 0.65, C		ɪo .68,	717
		[1,1	Tv ∙[	∣,1∕2, 1J 1,1,1/4,			1,1]
		,16.1	,1, ■	∣,1∕4, 1]			
							
	■	[1/1	6,1,	1/4,	1,1/	4,1]	
υ.0 0.1 0.2 0.3 0.4 0.5 0.6 0.							
Scatter (S)
Scatter (S)

Figure 7: Network performance vs. scatter for CL only networks of (a) Morse (b) MNIST, and
convolutional network with 2 CL junctions of (c) CIFAR10. All minimum values that need to be
considered to differentiate between connection patterns are bolded.
S is shown alongside each point. When S is equal for different connection patterns, the next min-
imum value in S needs to be considered to differentiate the networks, and so on. Considering the
Morse results, the leftmost 3 points all have S = 8, but the number of occurrences of 8 in S is 3 for
the lowest point (8% accuracy), 2 for the second lowest (12% accuracy) and 1 for the highest point
(46% accuracy). For the MNIST results, both the leftmost points have a single minimum value of
116 in S, but the lower has two occurrences of 4 while the upper has one.
We draw several insights from these results. Firstly, although we defined S as a single value for
convenience, there may arise cases when other (non-minimum) elements in S are important. Sec-
ondly, perhaps contrary to intuition, the concept of windows and scatter is important for all CLs,
not simply the first. As shown in Fig. 7a), a network with S1b = ɪ performs equally poorly as a
network with S2f = 8. Thirdly, scatter is a sufficient metric for performance, not necessary. A
network with a high S value will perform well, but a network with a slightly lower S than another
cannot be conclusively dismissed as being worse. But if a network has multiple low values in S, it
should be rejected. Finally, carefully choosing which neurons to group in a window will increase the
predictive power of scatter. A priori knowledge of the dataset will lead to better window choices.
4	Conclusion and Future Work
This paper discusses the merits of pre-defining sparsity in CLs of neural networks, which leads to
significant reduction in parameters without performance loss. In general, the smaller the fraction of
CLs in a network, the more redundancy there exists in their parameters. If we can achieve similar
results (i.e., 0.2% density) on Alexnet for example, we would obtain 95% reduction in overall pa-
rameters. Coupled with hardware acceleration designed for pre-defined sparse networks, we believe
our approach will lead to more aggressive exploration of network structure. Network connectivity
can be guided by the scatter metric, which is closely related to performance, and by optimally dis-
tributing connections across junctions. Future work would involve extension to conv layers, since
recent CNNs have lower values for the ratio of number of CLs to number of conv layers.
8
Under review as a conference paper at ICLR 2018
References
Alfred Bourely, John Patrick Boueri, and Krzysztof Choromonski. Sparse neural network topolo-
gies. In arXiv:1706.05683, 2017.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. In Proc. ICML,pp. 2285-2294.JMLR.org, 2015.
Misha Denil, Babak Shakibi, Laurent Dinh, Marc’aurelio Ranzato, and Nando D. Freitas. Predicting
parameters in deep learning. In Proc. NIPS, pp. 2148-2156, 2013.
Sourya Dey.	Morse code dataset for artificial neural networks, Oct
2017.	URL https://cobaltfolly.wordpress.com/2017/10/15/
morse-code-dataset-for-artificial-neural-networks/.
Sourya Dey, Peter A. Beerel, and Keith M. Chugg. Interleaver design for deep neural networks. In
Proc. Asilomar Conference on Signals, Systems and Computers. IEEE, 2017a.
Sourya Dey, Yinan Shao, Keith M. Chugg, and Peter A. Beerel. Accelerating training of deep neural
networks via sparse edge processing. In Proc. ICANN, pp. 273-280. Springer, 2017b.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev. Compressing deep convolutional
networks using vector quantization. In arXiv:1412.6115, 2014.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Proc. NIPS, pp. 1135-1143, 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. In Proc. ICLR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. CVPR, pp. 770-778, June 2016.
Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In arXiv:1608.06993, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Proc. NIPS, pp. 1097-1105, 2012.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun.
Overfeat: Integrated recognition, localization and detection using convolutional networks. In
arXiv:1312.6229, 2013.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proc. ICLR, 2015.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Proc. NIPS, pp. 3088-3096. Curran Associates, Inc., 2015.
Suraj Srinivas, Akshayvarun Subramanya, and R. Venkatesh Babu. Training sparse neural networks.
In arXiv:1611.06694, 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,
and A. Rabinovich. Going deeper with convolutions. In Proc. CVPR, pp. 1-9, 2015.
Chen Zhang, Di Wu, Jiayu Sun, Guangyu Sun, Guojie Luo, and Jason Cong. Energy-efficient CNN
implementation on a deeply pipelined FPGA cluster. In Proc. ISLPED, pp. 326-331. ACM, 2016.
Xichuan Zhou, Shengli Li, Kai Qin, Kunping Li, Fang Tang, Shengdong Hu, Shujun Liu, and Zhi
Lin. Deep adaptive network: An efficient deep neural network with sparse binary connections. In
arXiv:1604.06154, 2016.
9
Under review as a conference paper at ICLR 2018
5 Appendix
5.1	More on Distributing Individual Junction Densities
Section 2.4 showed that when overall CL density is fixed, it is desirable to make junction 2 denser
than junction 1. It is also interesting to note, however, that performance falls off more sharply when
junction 1 density is reduced to the bare minimum as compared to treating junction 2 similarly. This
is not shown in Fig. 3 due to space constraints. We found that when junction 1 had the minimum
possible density and junction 2 had the maximum possible while still satisfying the fixed overall,
the accuracy was about 36% for both subfigures (b) and (c). When the densities were flipped, the
accuracies were 67% for subfigure (b) and 75% for (c) in Figure 3.
5.2	Dense cases of Window Adjacency Matrices
As stated in Section 3.1, window output matrices for several junctions can be constructed by mul-
tiplying the individual matrices for each component junction. Consider the Morse network as de-
scribed in Section 3.2. Note that f o1:2 = 128 × 8 = 1024 and f i1:2 = 8 × 128 = 1024. Thus,
for the equivalent junction 1:2 which has N1 = 64 left neurons and N3 = 64 right neurons, we
have f o1:2 > N3 and f i1:2 > N1. So in this case the number of neurons in each window will
be rounded up to 1, and both the ideal window adjacency matrices A1w:12l and A1w:22r will be all 16’s
matrices since the ideal number of connections from each window to a neuron on the opposite side
is 1024/64 = 16. This is a result of the network having sufficient density so that several paths exist
from every input neuron to every output neuron.
5.3	Possible reasons for SCLs converging faster than FCLs
Training a neural network is essentially an exercise in finding the minimum of the cost function,
which is a function of all the network parameters. The graph for cost as a function of parameters may
have saddle points which masquerade as minima. It could also be poorly conditioned, wherein the
gradient of cost with respect to two different parameters have widely different magnitudes, making
simultaneous optimization difficult. These effects are non-idealities and training the network often
takes more time because of the length of the trajectory needed to overcome these and arrive at the
optimum point. The probability of encountering these non-idealities increases as the number of
network parameters increase, i.e. less parameters leads to a higher ratio of minima : saddle points,
which can make the network converge faster. We hypothesize that SCLs train faster than FCLs due
to the former having fewer parameters.
10