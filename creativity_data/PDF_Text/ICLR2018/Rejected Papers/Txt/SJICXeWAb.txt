Under review as a conference paper at ICLR 2018
Depth separation and weight-width trade-offs
FOR SIGMOIDAL NEURAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
Some recent work has shown separation between the expressive power of depth-2
and depth-3 neural networks. These separation results are shown by constructing
functions and input distributions, so that the function is well-approximable by a
depth-3 neural network of polynomial size but it cannot be well-approximated
under the chosen input distribution by any depth-2 neural network of polynomial
size. These results are not robust and require carefully chosen functions as well as
input distributions.
We show a similar separation between the expressive power of depth-2 and depth-
3 sigmoidal neural networks over a large class of input distributions, as long as
the weights are polynomially bounded. While doing so, we also show that depth-
2 sigmoidal neural networks with small width and small weights can be well-
approximated by low-degree multivariate polynomials.
1	Introduction
Understanding the remarkable success of deep neural networks in many domains is an important
problem at present (e.g., LeCun et al. (2015)). This problem has many facets such as understanding
generalization, expressive power, optimization algorithms in deep learning. In this paper, we focus
on the question of understanding the expressive power of neural networks. In other words, we study
what functions can and cannot be represented and approximated by neural networks of bounded
size, depth, width and weights.
The early results on the expressive power of neural networks showed that the depth-2 neural net-
works are universal approximators; that is to say, with only mild restrictions on the activation func-
tions or neurons, the depth-2 neural networks are powerful enough to uniformly approximate arbi-
trary continuous functions on bounded domains in Rd, e.g., Cybenko (1989); Hornik et al. (1989);
Barron (1994). However, the bounds that they provide on the size or width of these neural networks
are quite general, and therefore, weak. Understanding what functions can be represented or well-
approximated by neural networks with bounded parameters is a general direction in the study of
expressive power of neural networks. Here the parameters could mean the number of neurons, the
width of hidden layers, the depth, and the magnitude of its weights etc.
Natural signals (images, speech etc.) tend to be representable as compositional hierarchies LeCun
et al. (2015), and deeper networks can be thought of as representing deeper hierarchies. The power
of depth has been a subject of investigation in deep learning, e.g., He et al. (2016). We are interested
in understanding the effect of depth on the expressive power. In particular, one may ask whether
having more depth allows representation of more functions if the size bound remains the same.
Eldan & Shamir (2016) show a separation between depth-2 and depth-3 neural networks. More
precisely, they exhibit a function g : Rd → R and a probability distribution μ on Rd such that
g is bounded and supported on a ball of radius O(√d) and expressible by a depth-3 network of
size polynomially bounded in d. But any depth-2 network approximating g in L2-norm (or squared
error) within a small constant under the distribution μ must be of size exponentially large in d.
Their separation works for all reasonable activation functions including ReLUs (Rectified Linear
Units) and sigmoids. The function and the input distribution in Eldan & Shamir (2016) are carefully
constructed and their proof techniques seem to crucially rely on the specifics of these constructions.
Building upon this result, Safran & Shamir (2017) show that while the indicator function of the
1
Under review as a conference paper at ICLR 2018
L2-ball can be well-approximated by depth-3 networks of polynomial size, any good approximation
to it by depth-2 networks must require exponential size. Here, the notion of approximation in the
lower bound is the same as in Eldan & Shamir (2016) and a carefully constructed distribution that is
arguably not quite natural.
Daniely (2017) (see also Martens et al. (2013)) also gave a separation between depth-2 and depth-3
networks by exhibiting a function g : Sd-1 × Sd-1 → R which can be well-approximated by a
depth-3 ReLU neural network of polynomially bounded size and weights but cannot be approxi-
mated by any depth-2 (sigmoid, ReLU or more general) neural network of polynomial size with
(exponentially) bounded weights. This separation holds under uniform distribution on Sd-1 × Sd-1,
which is more natural than the previous distributions. However, the proof technique crucially uses
harmonic analysis on the unit sphere, and does not seems robust or applicable to other distributions.
Telgarsky (2016) shows a separation between depth-2k3 + 8 and depth-k ReLU neural networks,
for any positive integer k, when the input is uniformly distributed over [-1, 1]d. Liang & Srikant
(2017) (see also Safran & Shamir (2017); Yarotsky (2016)) show that there are univariate functions
on a bounded interval such that neural networks of constant depth require size at least Ω (poly(1∕e))
for a uniform -approximation over the interval, whereas deep networks (the depth can depend on )
can have size O (polylog(1/)).
The above separation results all fit the following template: certain carefully constructed functions
can be well approximated by deep networks, but are hard to approximate by shallow networks
using a notion of error that uses a carefully defined distribution. (Only Liang & Srikant (2017) is
distribution-independent as it deals with uniform approximation everywhere in the domain). Thus
these results do not tell us the extent to which deeper networks are more expressive than the shallow
ones. We would like to understand whether there are large classes of functions and distributions that
witness the separation between deep and shallow networks. An answer to this question is also more
likely to shed light on practical applications of neural networks. Shamir (2016); Shalev-Shwartz
et al. (2017); Song et al. (2017) show that even functions computed by a depth-2 neural network of
polynomial size can be hard to learn using gradient descent type of algorithms for a wide class of
distributions. These results address questions about learnability rather than the expressive power of
deep neural networks.
Hanin (2017) shows that piecewise affine functions on [0, 1]d with N pieces can be exactly repre-
sented by a width (d + 3) network of depth atmost N. Lower bound of Ω((N + d - 1)∕(d + 1)) on
the depth is proven for functions of the above type when the network has width at most (d + 1) and
very closely approximates the function.
Our depth separation results apply to neural networks with bounds on the magnitudes of the weights.
While we would prefer to prove our results without any weight restrictions, we now argue that small
weights are natural. In training neural networks, often weights are not allowed to be too large
to avoid overfitting. Weight decay is a commonly used regularization heuristic in deep learning
to control the weights. Early stopping can also achieve this effect. Another motivation to keep the
weights low is to keep the Lipschitz constant of the function computed by the network (w.r.t. changes
in the input, while keeping the network parameters fixed) small. Goodfellow et al. (2016) contains
many of these references. One of the surprising discoveries about neural networks has been the
existence of adversarial examples (Szegedy et al. (2013)). These are examples obtained by adding
a tiny perturbation to input from class so that the resulting input is misclassified by the network.
The perturbations are imperceptible to humans. Existence of such examples for a network suggests
that the Lipschitz constant of the network is high as noted in Szegedy et al. (2013). This lead them
to suggest regularizing training of neural nets by penalizing high Lipschitz constant to improve
the generalization error and, in particular, eliminate adversarial examples. This is carried out in
CiSSe et al. (2017), who find a way to control the Lipschitz constant by enforcing an orthonormality
constraint on the weight matrices along with other tricks. They report better resilience to adversarial
examples. On the other hand, Neyshabur et al. (2017) suggest that Lipschitz constant cannot tell the
full story about generalization.
2
Under review as a conference paper at ICLR 2018
2	Our results
We exhibit a simple function (derived from Daniely (2017)) over the unit ball Bd in d-dimensions can
be well-approximated by a depth-3 sigmoidal neural network with size and weights polynomially
bounded in d. However, its any reasonable approximation using a depth-2 sigmoidal neural network
with polynomially bounded weights must have size exponentially large in d.
Our separation is robust and works for a general class of input distributions, as long as their density
is at least 1/poly(d) on some small ball of radius 1/poly(d) in Bd. The function we use can also be
replaced by many other functions that are polynomially-Lipschitz but not close to any low-degree
polynomial.
As a by-product of our argument, we also show that constant-depth sigmoidal neural networks are
well-approximated by low-degree multivariate polynomials (with a degree bound that allows the
depth separation mentioned above).
3	Polynomial approximations to sigmoidal neural networks
In this section, we show that a sigmoid neuron can be well-approximated by a low-degree polyno-
mial. As a corollary, we show that depth-2 (and in genenral, small-depth) sigmoidal neural networks
can be well-approximated by low-degree multivariate polynomials. The main idea is to use Cheby-
shev polynomial approximation as in Shalev-Shwartz et al. (2011), which closely approximates the
minimax polynomial (or the polynomial that has the smallest maximum deviation) to a given func-
tion. For the simplicity of presentation and arguments, we drop the bias term b in the activation
function σ(hw, xi + b). This is without loss of generality, as explained at the end of the last section.
3.1	Polynomial approximation to a sigmoid neuron
The activation function of a sigmoid neuron σ : R → R is defined as
1
σ(t)=-------；——-.
1 +exp(-t)
Chebyshev polynomials of the first kind {Tj(t)}j≥0 are defined recursively as T0(t) = 1, T1(t) = t,
and Tj+ι(t) = 2t ∙ Tj(t) - Tj-ι(t). They form an orthonormal basis of polynomials over [-1,1]
with respect to the density 1/√1 -12. The coefficient Cj in the Chebyshev expansion of σ(wt)=
Pj∞=0cjTj(t) over [-1, 1] is given by
cj
1 + 1(j > 0)广 σ(wt) Tj ⑴ dt
∏	J-I √1 -12
Proposition 1 (see Lemma B.1 in Shalev-Shwartz et al. (2011)) bounds the magnitude of coefficients
cj in the Chebyshev expansion of σ(wt) = Pj∞=0 cj Tj (t).
Proposition 1. For any j > 1, the coefficient cj in the Chebyshev expansion of a sigmoid neuron
σ(wt) is bounded by
∣cj∣≤ (∏ + 2](1 + ∏) j.
|w|	π	|w|
Proposition 1 implies low-degree polynomial approximation to sigmoid neurons as follows. This
observation appeared in Shalev-Shwartz et al. (2011) (see equation (B.7) in their paper). For com-
pleteness, we give the proof in Appendix A.
Proposition 2. Given any w ∈ R with |w| ≤ B, there exists a polynomial p of degree
O (B log (B/e)) such that ∣σ(wt) — p(t)∣ ≤ e ,for all t ∈ [-1,1].
We use this O (log(1/)) dependence in the above bound crucially in some of our results, e.g., a
weaker version of Daniely’s separation result for depth-2 and depth-3 neural networks. Notice that
this logarithmic dependence does not hold for a ReLU neuron; it is O(1/e) instead.
3
Under review as a conference paper at ICLR 2018
3.2 Polynomials approximations to small-depth neural networks
A depth-2 sigmoidal neural network on input t ∈ [-1, 1] computes a linear combination of sig-
moidal neurons σ(w1t), σ(w2t), . . . , σ(wnt), for w1, w2, . . . , wn ∈ R, and computes a function
f : [-1, 1] → R given by
n
f(t) = X aiσ(wit)
i=1
Here are a few propositions on polynomial approximations to small-depth neural networks. For
completeness, their proofs are included in Appendix A.
Proposition 3 shows that a depth-2 sigmoidal neural network of bounded weights and width is close
to a low-degree polynomial.
Proposition 3. Let f : [-1, 1] → R be a function computed by a depth-2 sigmoidal neural network
of width n and weights bounded by B. Then f is δ-approximated (in L∞-norm) over [-1, 1] by a
polynomial of degree O (B log (nB2∕δ)).
Now consider a depth-2 sigmoidal neural network on input x ∈ Bd, where Bd = {x ∈ Rd :
kxk ≤ 1}. It is given by a linear combination of sigmoidal activations applied to linear functions
hw1, xi , hw2, xi , . . . , hwn, xi (or affine functions when we have biases), forw1, w2, . . . , wn ∈ Rd
and it computes a function F : Bd → R given by
n
F(x) =	aiσ(hwi, xi)
i=1
Proposition 4 below is a multivariate version of Proposition 3.
Proposition 4. Let F : Bd → R be a function computed by a depth-2 sigmoidal neural network
with width n and bounded weights, that is, |ai | ≤ B and kwi k ≤ B, for 1 ≤ i ≤ n. Then F is δ-
approximated (in L∞-norm) over Bd by a polynomial of degree O (B log (nB2/δ)) in d variables
given by the coordinates x = (x1, x2, . . . , xd).
Note that its proof crucially uses the fact that Proposition 2 guarantees a low-degree polynomial that
approximates a sigmoid neuron everywhere in [-1, 1].
A depth-k sigmoidal neural network can be thought of as a composition - a depth-2 sigmoidal neural
network on top, whose each input variable is a sigmoid applied to a depth-(k - 2) sigmoidal neural
network. In other words, it computes a function F : Bd → R given by
n
F(x) =	aiσ (hwi, yi) ,
i=1
where y = (y1 , y2 , . . . , ym) has each coordinate yj = σ(Fj (x)), for 1 ≤ j ≤ m, such that each
Fi : Bd → R is a function computed by a depth-(k - 2) sigmoidal neural network.
Now we show an interesting consequence, namely, any constant-depth sigmoidal neural network
with polynomial width and polynomially bounded weights can be well-approximated by a low-
degree multivariate polynomial. The bounds presented in Proposition 5 are not optimal but the
qualitative statement is interesting in contrast with the depth separation result. The growth of the
degree of polynomial approximation is dependent on the widths of hidden layers and it is also the
subtle reason why a depth separation result is still possible (when the weights are bounded).
Proposition 5. Let F : Bd → R be a function computed by a depth-k sigmoidal neural network
of width at most n in each layer and weights bounded by B, then F (x) can be δ-approximated (in
L∞-norm) over Bd bya d-variatepolynomial ofdegree O ((nB)k logk (nB∕δ)) in each coordinate
variable ofx = (x1 , x2 , . . . , xd).
Note that when n and B are polynomial in d and the depth k is constant, then this low-degree
polynomial approximation also has degree polynomial in d.
4
Under review as a conference paper at ICLR 2018
4 L∞-SEPARATION OF DEPTH-2 VS. DEPTH-3 SEPARATION FOR GENERAL
INPUT DISTRIBUTIONS
Daniely shows that if g : [-1, 1] → R cannot be approximated by a polynomial of degree O(d2),
then G : Sd-1 × Sd-1 → R defined as G(x, y) = g(hx, yi) cannot be approximated by any depth-
2 neural network of polynomial size and (exponentially) bounded weights. Daniely shows this
lower bound for a general neuron or activation function that includes sigmoids and ReLUs. Daniely
then uses G(x, y) = g(hx, yi) = sin(πd3 hx, yi) which, on the other hand, is approximable by a
depth-3 ReLU neural network with polynomial size and polynomially bounded weights. This gives
a separation between depth-2 and depth-3 ReLU neural networks w.r.t. uniform distribution over
Sd-1 × Sd-1. Daniely’s proof uses harmonic analysis on the unit sphere, and requires the uniform
distribution on Sd-1 × Sd-1 in a crucial way.
We show a simple proof of separation between depth-2 and depth-3 sigmoidal neural networks that
compute functions F : Bd → R. Our proof works for a large class of distributions onBd but requires
the weights to be polynomially bounded.
The following lemma appears in Debao (1993). Assumption 1 in Eldan & Shamir (2016) and their
version of this lemma for ReLU networks was used by Daniely (2017) in the proof of separation
between the expressive power of depth-2 and depth-3 ReLU networks.
Lemma 6. Let f : [-1, 1] → R be any L-Lipschitz function. Then there exists a function g :
[-1, 1] → R computed by a depth-2 sigmoidal neural network such that
n
g(t) = f(0) +Xaiσ(wit+bi),
i=1
the width n as well as the weights are bounded by poly(L, 1/), and |f (t) - g(t)| ≤ , for all
t ∈ [-1, 1].
Now we are ready to show the separation between depth-2 and depth-3 sigmoidal neural networks.
The main idea, similar to Daniely (2017), is to exhibit a function that is Lipschitz but far from any
low-degree polynomial. The Lipschitz property helps in showing that our function can be well-
approximated by a depth-3 neural network of small size and small weights. However, being far
from any low-degree polynomial, it cannot be approximated by any depth-2 neural network.
Theorem 7. Consider the function G : Bd → R given by G(x) = sin(πd5 kxk2). Then G can be
δ-approximated (in L∞-norm) by a depth-3 sigmoidal neural network of width and weights polyno-
mially bounded in d. However, any function F : Bd → R computed by a depth-2 sigmoidal neural
network with weights O(d2) cannot δ-approximate G even when its width n is 2O(d).
By modifying the function to G(x) = sin(πN kxk2), this lower bound with L∞-norm holds for any
distribution over Bd whose support contains a radial line segment of length at least 1/poly(d), by
making N = poly(d), for a large enough polynomial.
Remark: Given any distribution μ over Bd whose probability density is at least 1/poly (d) on some
small ball of radius 1/poly(d), the lower bound or inapproximability by any depth-2 sigmoidal
neural network can be made to work with L2-norm (squared error), fora large enough N = poly(d).
Proof. First, we will show that G(x) can be well-approximated by a depth-3 sigmoidal neural
network of polynomial size and weights. The idea is similar to Daniely’s construction for ReLU
networks in Daniely (2017). By Lemma 6, there exists a function f : [-1, 1] → R computed
by a depth-2 sigmoidal neural network of size and weights bounded by poly(d, 1/) such that
t2 - f (t) ≤ /10d6, for all t ∈ [-1, 1]. Thus, we can compute xi2 for each coordinate of x and
add them up to get an -approximation to kxk2 over Bd. That is, there exists a function S : Bd → R
computed by a depth-2 sigmoidal neural network of size and weights bounded by poly(d, 1/) such
that S(x) - kxk2 ≤ /10d5, for all x ∈ Bd. Again, by Lemma 6, we can approximate sin(πd3t)
over [0, 1] using f : [-1, 1] → R computed by another depth-2 sigmoidal neural network with size
and weights bounded by poly(d, 1/) such that sin(πd3t) - f (t) ≤ /2, for all t ∈ [0, 1]. Note
that the composition of these two depth-2 neural networks f (N (x)) gives a depth-3 neural network
5
Under review as a conference paper at ICLR 2018
as the output of the hidden layer of the bottom network can be fed into the top network as inputs.
|G(x)-f(S(x))| = sin(πd5kxk2)-f(S(x))
≤ sin(πd5kxk2)-f(kxk2)+f(kxk2)-f(S(x))
≤ /2 + 4d5 kxk2 - S(x)
using that f that approximates sin(πd5t) closely must also be 4d5-Lipschitz
≤ "2 + 4d5 ∙ "10d5 ≤ e.
Now we will show the lower bound. Consider any function F : Bd → R computed by a depth-2
sigmoidal neural network whose weights are bounded by B = O(d2 ) and width is n. Propo-
sition 4 shows that there exists a d-variate polynomial P(x) of degree O(B log(nB2∕δ)) =
O (d2 log(n∕δ) + d2 logd) in each variable such that |F(x) — P(x)| ≤ δ, for all X ∈ Bd. Let μ
be any measure on Bd whose support contains some radial line segment of length at least 1/poly(d)
in Bd. In other words, there exists a unit vector U such that the support of μ intersects the ra-
dial set {X ∈ Bd : X = tu, for some t ∈ [—1, 1]} in some line segment of length at least
1∕poly(d). Then P(tu) is a univariate polynomial of degree O(d3 log(n∕δ) + d3 log d) that δ-
approximates F(tu), for all t ∈ [一1,1]. By Lemma 8, using D = O(d3 log(n∕δ) + d3 logd),
l = 1∕poly(d) and N = d5∕l, we get that if n = 2O(d), then there exists a t0 ∈ [—1, 1] such
that sin(πN t20) — P (tu) ≥ 1. Therefore, by triangle inequality, sin(πN kt0uk2) — F (t0 u) ≥
sin(πN t20) — P(t0u) — |P (t0 u) — F(t0u)| ≥ 1 — δ > δ, for δ < 1∕2. This means that G(X) can-
not be well-approximated by any F(X) computed by a depth-2 neural network with polynomially
bounded weights even when it has width 2O(d).	□
Now we show that the candidate function proposed by Daniely g(t) = sin(πN t), for large enough
N, is far from any low-degree polynomial w.r.t. any measure μ on [—1,1] with a reasonable support.
Lemma 8. Let P be any polynomial of degree D and μ be any measure on [—1,1] whose Support
contains an interval of length at least l. Then, for N large enough to satisfy Nl > D + 3, there
exists to ∈ [一1,1] Such that μ(to) > 0 and ∣sin(πNt0) — p(to)∣ > L In other words, sin(πNt) is
1 -far(in L∞-norm)from any polynomial ofdegree D over interval [—1,1] with measure μ.
Proof. Let μ(t) > 0 for some interval [a, a + l] ⊆ [—1,1]. Consider S = {t ∈ [a, a + l] : t =
—1 + (i + 1∕2)∕N, for some integer i}. Then S contains at least Nl — 2 points where sin(π N t)
alternates as ±1. Any polynomial p of degree D cannot match the sign of sin(π N t) on all the
points in S. Otherwise, by intermediate value theorem, p must have at least Nl — 3 roots between
the points of S, which means D ≥ Nl — 3, a contradiction. Thus, there exists t0 ∈ S such that
p(t0) and sin(πN t0) have opposite signs. Since sin(πN t) = ±1, for any t ∈ S, the sign mismatch
implies ∣sin(πNto) — p(to)∣ > 1.	□
An important remark on biases: Even though we handled the case of sigmoid neurons without biases,
the proof technique carries over to the sigmoid neurons with biases σ(hw, Xi + b). The idea is to
consider a new (d + 1)-dimensional input Xnew = (X, xd+1) = (x1, x2, . . . , xd+1) with xd+1 = 1,
and consider the new weight vector wnew = (w, b). Thus, hwnew, Xnewi = hw, Xi + b. The new
input lies on a d-dimensional hyperplane slice of Bd+1, so we need to look at the restriction of the
input distribution μ to this slice. Most of the ideas in our proofs generalize without any technical
modifications. We defer the details to the full version.
5	L2-SEPARATION OF DEPTH-2 VS. DEPTH-3 SEPARATION FOR GENERAL
INPUT DISTRIBUTIONS
In this section we show lower bounds under the L2-norm. The theorem below gives a technical
condition on the class of densities μ on Bd for which our lower bound holds. Let's give an example
to illustrate that the condition on density is reasonable: Let K ⊂ Bd be a convex set such that
6
Under review as a conference paper at ICLR 2018
every point in K is at least r away from the boundary of Bd (where r = 1/poly(d) is a parameter).
Further assume that (1) the probability mass of K is at least a constant and (2) for every point in
K the probability density is within a constant factor of the uniform density on K. Then our lower
bound applies to μ.
Theorem 9. Consider the function G : Bd → R given by G(X) = sin(πN ∣∣xk2). Let μ be any
probability density over Bd such that there exists a subset C ⊆ Bd satisfying the following two
conditions:
•	The r-interior of C defined as C0 = {X ∈ C : B(X, r) ⊆ C} contains at least γ fraction
ofthe total probability massfor some γ > 0, i.e., JC μ(x)dx ≥ γ.
•	For any affine line `, the induced probability density on every segment of length at least r
in the intersection ' ∩ C is (α, β)-uniform, i.e., it is at least a times and at most β times the
uniform density on that segment.
Let F : Bd → R be any function computed by a depth-2 sigmoidal neural network with weights
bounded by B and width n. Thenfor any 0 < δ《αγ∕3β and N》(B/r2) log(nB2∕δ), the
function F cannot δ-approximate G on Bd under L2-norm (squared error) under the probability
density μ.
In particular, ifα, β, γ are constants, B = poly(d), n = 2d, and r = 1/poly(d), then it suffices to
choose N = poly(d) for a sufficiently large degree polynomial.
Proof. We show a lower bound on L2-error of approximating G(X) with any multivariate polyno-
mial P : Bd → R of degree D under the distribution given by μ on Bd. For any fixed unit vector v,
consider u ∈ Bd-1 orthogonal to v and let `u be the affine line going through u and parallel to the
direction v given by `u = {X = u + tv : t ∈ R}.
J (G(X) — P(x))2 μ(x)dx
Bd
≥ / (G(X)- P(x))2 μ(x)dx
C
= JJ (G(U + tv) — P(u + tv))2 μ(u + tv) IC(u,t) dt du
Bd-1 R
where IC (u, t)
1, if u ∈ Bd and u + tv ∈ C
0, otherwise
≥	(G(u + tv)
Bd-1 R
—P(u + tv))2 μ(u + tv) IC(u, t) dt du
where IC(u,t) = 1, if U ∈ BdT and U + tv ∈ {u + t0v ∈ C : t0 ∈ I} ⊆ C
for some interval I of length at least r, and IC (u, t) = 0, otherwise
_ %d-ι Rr (G(u + tv) — P(u + tv))2 μ(u + tv) IC(u,t) dt du
RBd-I RR μ(U + tv) IC(u,t) dtdu
≥ %d-ι Rr (G(u + tv) — P(u + tv))2 μ(u + tv) IC(u,t) dt du
—	Ad-1 Rr μ(u + tv) IC(u,t) dt du
■ J J μ(u + tv) IC(u, t) dt du
Bd-1 R
J μ(x)dx
C0
because for any X = U + tv ∈ C0 We have B(x, r) ⊆ C, therefore IC(u, t) = 1
≥ min
u∈Bd-1
≥ min
u∈Bd-1
α
β
JR μ(u + tv) IC(u,t) dt
Rr (G(u + tv) — P(u + tv))2 IC(u,t) dt
JR IC (u,t) dt
∙ J μ(x)dx
C0
∙ J μ(x)dx
C0
7
Under review as a conference paper at ICLR 2018
because for any line ', the distribution induced by μ(x) along any line segment
of length at least r in the intersection ` ∩ C is (α, β)-uniform, for any line `
≥ β ∙ Y.
The last inequality is using the condition /。0 μ(x)dx ≥ Y given in Theorem 9 and an adaptation
of the following idea from Lemma 5 of Daniely (2017). For any fixed u and v, G(u + tv) =
sin(πN (kuk2 + t2)) and P(u + tv) is a polynomial of degree at most D in t. The function
sin(πN (kuk2 +t2)) alternates its sign as kuk2 +t2 takes values that are successive integer multiples
of 1/N. Consider s = t2 ∈ [0, 1] and divide [0, 1] into N disjoint segments using integer grid of
step size 1/N. For any polynomial p(s) of degree at most D and any interval I ⊆ [0, 1] of length
r D/N, there exists at least Nr - D - 2 segments of length 1/N each on which sin(π N s) and
p(s) do not change signs and have opposite signs. Now using (sin(π N s) - p(s))2 ≥ sin2(πN s),
integrating we get that I (sin(πN s) - p(s))2ds ≥ r/2. Extending this proof to t instead of s = t2,
using sin2(πNt2)t ≤ sin2(πNt) for all t ∈ [0, 1], and incorporating the shift πN kuk2, we can
similarly show that RI sin2(πN (kuk2 + t2)) - P(u + tv))2dt ≥ r/3. Summing up over multiple
such intervals gives the final inequality.	□
The L2 separation between depth-2 and depth-3 neural networks under probability density μ now
follows by taking a small enough δ, and combining the following ingredients (i) Proposition 4 says
that any depth-2 sigmoid neural networks of width n = 2d and weights bounded by B = poly(d)
can be δ-approximated in L∞ (and hence, also L2) by a multivariate polynomials of degree
D = O(B log(nB2 /δ)) = poly(d), (ii) proof of Theorem 7 (initial part) says that G(x) can be
δ-approximated in L∞ (and hence, also L2) by a depth-3 sigmoid neural network of width and size
poly(d), but (iii) Theorem 9 says that, for N = poly(d) of large enough degree, G(x) cannot be
3δ-approximated in L2 by any multivariate polynomial of degree D, and (iv) triangle inequality.
References
Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning ,14(1):115-133,Jan 1994. ISSN 1573-0565. doi:10.1007/BF00993164. URL https:
//doi.org/10.1007/BF00993164.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp.
854-863, 2017. URL http://Proceedings.mlr.press∕v70∕cisse17a.html.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, Dec 1989. ISSN 1435-568X. doi: 10.1007/BF02551274.
URL https://doi.org/10.1007/BF02551274.
Amit Daniely. Depth separation for neural networks. In Proceedings of the 30th Conference on
Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pp. 690-696, 2017.
URL http://proceedings.mlr.press/v65/daniely17a.html.
Chen Debao. Degree of approximation by superpositions of a sigmoidal function. Approximation
Theory and its Applications, 9(3):17-28, Sep 1993. ISSN 1573-8175. doi: 10.1007/BF02836480.
URL https://doi.org/10.1007/BF02836480.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Proceedings
of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016,
pp. 907-940, 2016. URL http://jmlr.org/proceedings/papers/v49/eldan16.
html.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu
activations. CoRR, abs/1708.02691, 2017. URL http://arxiv.org/abs/1708.02691.
8
Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV USA, June 27-30, 2016, pp. 770-778, 2016. doi: 10.1109/CVPR.2016.90. URL
https://doi.org/10.1109/CVPR.2016.90.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks
are universal approximators. Neural Networks, 2(5):359 - 366, 1989. ISSN 0893-6080.
doi: https://doi.org/10.1016/0893-6080(89)90020-8. URL http://www.sciencedirect.
com/science/article/pii/0893608089900208.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.
Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? In ICLR, 2017.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational
efficiency of restricted boltzmann machines. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26,
pp. 2877-2885. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5020-on-the-representational-efficiency-of-restricted-boltzmann-machines.
pdf.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring gen-
eralization in deep learning. CoRR, abs/1706.08947, 2017. URL http://arxiv.org/abs/
1706.08947.
Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with
neural networks. In Proceedings of the 34th International Conference on Machine Learn-
ing, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2979-2987, 2017. URL
http://proceedings.mlr.press/v70/safran17a.html.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with
the 0-1 loss. SIAM J. Comput., 40(6):1623-1646, December 2011. ISSN 0097-5397. doi: 10.
1137/100806126. URL http://dx.doi.org/10.1137/100806126.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learn-
ing. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017, pp. 3067-3075, 2017. URL http://proceedings.
mlr.press/v70/shalev-shwartz17a.html.
Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037,
2016. URL http://arxiv.org/abs/1609.01037.
Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural net-
works. CoRR, abs/1707.04615, 2017. URL http://arxiv.org/abs/1707.04615.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL
http://arxiv.org/abs/1312.6199.
Matus Telgarsky. benefits of depth in neural networks. In Proceedings of the 29th Conference on
Learning Theory, COLT 2016, New York, USA, June 23-26, 2016, pp. 1517-1539, 2016. URL
http://jmlr.org/proceedings/papers/v49/telgarsky16.html.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. CoRR, abs/1610.01145,
2016. URL http://arxiv.org/abs/1610.01145.
9
Under review as a conference paper at ICLR 2018
A Proofs of polynomial approximations to neural networks
Proof of Proposition 2
Proof. Consider the degree-D approximation to σ(wt) given by the first D terms in its Chebyshev
expansion. The error of this approximation for any t ∈ [-1, 1] is bounded by
lσ(Wt)- P⑴ |= EcjTj ⑴
j>D
≤ X |cj|
j>D
≤ (M + 2 j (1 + 而)
≤ (⅛ + 2 1 + 高)(X (1 + 高)
=(± + 2	1 + 工厂D+1).回(1 + 4)
∖ |w|	π	|w| j	π 1	|w| j
≤ ,
using Proposition 1, |w| ≤ B, and D = O (B log (B/)).
□
Proof of Proposition 3
Proof. Let f be computed by a depth-2 sigmoidal neural network given by f(t) = Pin=1 aiσ(wit).
Define a parameter E = δ∕nB. Proposition 2 guarantees polynomial P1,P2,... ,Pn of degree
O (Blog(B∕e)) such that ∣σ(wit) -Pi(t)∣ ≤ e, for all t ∈ [-1,1]. Thus, the polynomial
p(t) = pn=ι aiPi(t) has degree O (B log(B∕e)) = O (B log(nB2∕δ)), and for any t ∈ [-1,1],
n
aiσ(wit) - p(t)
i=1
Proof of Proposition 4
nn
=	aiσ(wit) - Σ aipi(t)
i=1	i=1
n
≤	|ai| ∣σ(wit) - Pi(t)∣
i=1
≤ nBE
δ
using |ai | ≤ B
using E = δ∕nB.
□
Proof. Let F be computed by a depth-2 neural network given by F (x) = Pin=1 aiσ(hwi, xi),
where |ai| ≤ B and kwi k ≤ B, for 1 ≤ i ≤ n. Thus, F(x) = Pin=1 aiσ(kwi k ti), where
ti = hwi∕ kwik , xi ∈ [-1, 1] because wi∕ kwik ∈ Bd, for 1 ≤ i ≤ n, and x ∈ Bd.
Define a parameter E = δ∕nB. Proposition 2 guarantees polynomial p1,p2, . . . ,pn of degree
O (Blog(B∕e)) such that ∣σ(∣∣Wik t) -Pi(t)∣ ≤ e, for all t ∈ [-1,1]. Consider the following
polynomial P(x) = P(x1, x2, . . . , xd) = Pin=1 aipi(hwi∕ kwik , xi). P(x) is a d-variate poly-
nomial of degree O (B log(B∕E)) = O B log(nB2∕δ) in each variable x1, x2, . . . , xd. For any
x ∈ Bd,
n	n
|F (x) - P(x)| = X aiσ(hwi,xi) - X aipi(hwi∕ kwik , xi)
i=1	i=1
10
Under review as a conference paper at ICLR 2018
n
≤ 5S|ai| lσ(hwi, Xi) -pi(hwi/ kwik , xi)|
i=1
n
≤ BE lσ(kwik ti) -pi(ti)|	using ti = hwi/ kwik ,Xi and |ai| ≤ B
i=1
≤ EnB	using ∣σ(∣∣wik t) — pi(t)∣ ≤ e, for all t ∈ [—1,1]
= δ.
□
Proof of Proposition 5
Proof. We prove this by induction on the depth k. By induction hypothesis each
Fj (X) can be E1-approximated (in L∞-norm) by a d-variate polynomial Qj (X) of degree
O (nB)k-2 log(k-2) (nB/E1) in each variable. Thus, |Fj (X) — Qj (X)| = E1, for any X ∈ Bd
and 1 ≤ j ≤ m. Because a sigmoid neuron is Lipschitz,
∣y∙ — σ(Qj(x))| = ∣σ(Fj(x)) — σ(Q,(x))| ≤ |Fj-(x) — Qj(x)| ≤ J
for any X ∈ Bd and 1 ≤ j ≤ m.
Since Fj (x) is the output ofa depth-(k—2) sigmoidal neural network of width at most n and weights
at most B, we must have |Fj (x)| ≤ nB, for all x ∈ Bd. Thus, |Qj (x)| ≤ nB + E1 ≤ 2nB. By
Proposition 2, there exists a polynomial q(t) of degree at most O (nB log(nB/E2)) such that
HQ(Xy) - q(Qj(X))I ≤ a
for all x ∈ Bd and 1 ≤ j ≤ m.
Consider q ∈ Rm as q = (q(Q1(x)), q(Q2(x)), . . . , q(Qm(x))). Then, for any x ∈ Bd, we have
|hwi,yi — hwi,qi| = |hwi,y —qi|
≤ kwik ky—qk
(m	Y〃
≤ B l∑ (yj-q(Qj (X)))J
≤ B√m (ei + ⑴
≤ B√n (EI + E2) .
Again by Proposition 2, there is a polynomial p of degree at most O (nB log(nB/E)) such that
∣σ(hWi, qi) — p(hwi, qi)| ≤ e, for all X ∈ Bd and 1 ≤ i ≤ n. This is because |〈Wi, q〉| = O(nB).
Let’s define P(x) = Pin=1 aip(hwi, qi). Therefore, for any x ∈ Bd,
n	n
|F (x) — P(x)| =	aiσ(hWi, yi) —	aip(hWi,qi)
i=1	i=1
n
≤ 5S|ai| lσ(hwi, yi)— p(hwi, qi)|
i=1
n
≤ X |ai| (lσ(hwi, yi)— σ(hwi, qi)| + lσ(hwi, qi)— P(Wi qi)D
i=1
n
≤	|ai| (|hwi,yi — hwi,qi| +E)
i=1
≤ nB (B√n(eι + e?) + E)
≤ δ,
11
Under review as a conference paper at ICLR 2018
if We use 印=e2 = 6/3n3/2B2 and E = δ/3nB.
P(x) is a d-variate polynomial of degree
deg (P) ≤ deg(p)deg(q) ∙ deg (Qj) = O ((nB)k logk InB /),
in each variable.	□
12