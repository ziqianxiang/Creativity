Under review as a conference paper at ICLR 2018
Global Convergence of Policy Gradient
Methods for Linearized Control Problems
Anonymous authors
Paper under double-blind review
Ab stract
Direct policy gradient methods for reinforcement learning and continuous con-
trol problems are a popular approach for a variety of reasons: 1) they are easy
to implement without explicit knowledge of the underlying model 2) they are an
“end-to-end” approach, directly optimizing the performance metric of interest 3)
they inherently allow for richly parameterized policies. A notable drawback is that
even in the most basic continuous control problem (that of linear quadratic regu-
lators), these methods must solve a non-convex optimization problem, where little
is understood about their efficiency from both computational and statistical per-
spectives. In contrast, system identification and model based planning in optimal
control theory have a much more solid theoretical footing, where much is known
with regards to their computational and statistical properties. This work bridges
this gap showing that (model free) policy gradient methods globally converge to
the optimal solution and are efficient (polynomially so in relevant problem depen-
dent quantities) with regards to their sample and computational complexities.
1	Introduction
Recent years have seen major advances in the control of uncertain dynamical systems using rein-
forcement learning and data-driven approaches; examples range from allowing robots to perform
more sophisticated controls tasks such as robotic hand manipulation (Tassa et al., 2012; Al Borno
et al., 2013; Kumar et al., 2016; Levine et al., 2016; Tobin et al., 2017; Rajeswaran et al., 2017a),
to sequential decision making in game domains, e.g. AlphaGo (Silver et al., 2016) and Atari game
playing (Mnih et al., 2015). Deep reinforcement learning (DeepRL) are becoming increasingly pop-
ular for tackling such challenging sequential decision making problems.
Many of these successes have relied on sampling based reinforcement learning algorithms such as
policy gradient methods, including the DeepRL approaches; here, there is little theoretical under-
standing of their efficiency, either from a statistical or a computational perspective. In contrast,
control theory (optimal and adaptive control) has a rich body of tools, with provable guarantees,
for related sequential decision making problems, particularly those that involve continuous control.
These latter techniques are often model-based — they estimate an explicit dynamical model first
(e.g. system identification) and then design optimal controllers.
This work builds bridges between these two lines of work, namely, between optimal control theory
and sample based reinforcement learning methods, using ideas from mathematical optimization.
1.1	The optimal control problem
In the standard optimal control problem, the dynamics model ft, where ft is specified as
xt+1 = ft (xt, ut, wt) ,
maps a state xt ∈ Rd, a control (the action) ut ∈ Rk , and a disturbance wt, to the next state
xt+1 ∈ Rd . The objective is to find the control input ut which minimizes the long term cost,
T
minimize	ct(xt, ut)
t=1
such that xt+1 = ft (xt , ut , wt) .
1
Under review as a conference paper at ICLR 2018
Here the ut are allowed to depend on the history of observed states.
In practice, this is often solved by considering the linearized control (sub-)problem where the dy-
namics are approximated by
xt+1 = Atxt + Btut + wt ,
and the costs are approximated by a quadratic function in xt and ut , e.g. (Todorov & Li, 2004). This
work considers an important special case: the time homogenous, infinite horizon problem referred
to as the linear quadratic regulator (LQR) problem. The results herein can also be extended to the
finite horizon, time in-homogenous setting, discussed in Section 5.
In the LQR problem, the objective is
∞
minimize E	(xt> Qxt + ut>Rut)
t=0
such that xt+ι = Axt + But, xo 〜D.
where initial state xo 〜 D is assumed to be randomly distributed according to distribution D; the
matrices A ∈ Rd×d and B ∈ Rd×k are referred to as system (or transition) matrices; Q ∈ Rd×d
and R ∈ Rk×k are both positive definite matrices that parameterize the quadratic costs. For clarity,
this work does not consider a noise disturbance but only a random initial state. The importance of
(some) randomization for analyzing direct methods is discussed in Section 3.
Throughout, assume that A and B are such that the optimal cost is finite (for example, the control-
lability of the pair (A, B) would ensure this). Optimal control theory (Anderson & Moore, 1990;
Evans, 2005; Bertsekas, 2011; 2017) shows that the optimal control input can be written as a linear
function in the state,
ut = -K *xt
where K * ∈ Rk×d.
Planning with a known model. Planning can be achieved by solving the algebraic Riccati equation,
P =ATPA+Q-ATPB(BTPB+R)-1BTPA,	(1)
for a positive definite matrix P which parameterizes the “cost-to-go” (the optimal cost from a state
going forward). The optimal control gain is then given as:
K* = -(BTPB + R)-1BTPA.	(2)
There are both algebraic solution methods to find P and (convex) SDP formulations to solve for P .
More broadly, even though there are convex formulations for planning, these formulations: 1) do not
directly parameterize the policy 2) they are not “end-to-end” approaches in that they are not directly
optimizing the cost function of interest and 3) it is not immediately clear how to utilize these ap-
proaches in the model-free setting, where the agent only has simulation access. These formulations
are discussed in Section A, where there is a discussion of how the standard SDP formulation is not
a direct method that minizes the cost over the set of feasible policies.
1.2	Contributions of this work
Even in the most basic case of the standard linear quadratic regulator model, little is understood as
to how direct (model-free) policy gradient methods fare. This work provides rigorous guarantees,
showing that, while in fact the approach is a non-convex one, directly using (model free) local search
methods leads to finding the globally optimal policy. The main contributions are as follows:
• (Exact case) Even with access to exact gradient evaluation, little is understood about
whether or not convergence to the optimal policy occurs, even in the limit, due to the
non-convexity in the problem. This work shows that global convergence does indeed occur
(and does so efficiently) for local search based methods.
• (Model free case) Without a model, this work shows how one can use simulated trajectories
(as opposed to having knowledge of the model) in a stochastic policy gradient method
where provable convergence to a globally optimal policy is guaranteed, with (polynomially)
efficient computational and sample complexities.
2
Under review as a conference paper at ICLR 2018
• (The natural policy gradient) Natural policy gradient methods (Kakade, 2001) — and re-
lated algorithms such as Trust Region Policy Optimization (Schulman et al., 2015) and the
natural actor critic (Peters & Schaal, 2007) — are some of the most widely used and effec-
tive policy gradient methods (see Duan et al. (2016)). While many results argue in favor
of this method based on either information geometry (Kakade, 2001; Bagnell & Schneider,
2003) or based on connections to actor-critic methods (Deisenroth et al., 2013), these re-
sults do not provably show an improved convergence rate. This work is the first to provide
a guarantee that the natural gradient method enjoys a considerably improved convergence
rate over its naive gradient counterpart.
More broadly, the techniques in this work merge ideas from optimal control theory, mathematical
(and zeroth order) optimization, and sample based reinforcement learning methods. These tech-
niques may ultimately help in improving upon the existing set of algorithms, addressing issues such
as variance reduction or improving upon the natural policy gradient method (with, say, a Gauss-
Newton method). The Discussion touches upon some of these issues.
1.3	Related work
In the reinforcement learning setting, the model is unknown, and the agent must learn to act through
its interactions with the environment. Here, solution concepts are typically divided into: model-
based approaches, where the agent attempts to learn a model of the world, and model-free ap-
proaches, where the agent directly learns to act and does not explicitly learn a model of the world.
The related work on provably learning LQRs is reviewed from this perspective.
Model-based learning approaches. In the context of LQRs, the agent attempts to learn the dy-
namics of “the plant” (i.e. the model) and then plans, using this model, for control synthesis. Here,
the classical approach is to learn the model with subspace identification (Ljung, 1999). Fiechter
(1994) provides a provable learning (and non-asymptotic) result, where the quality of the policy
obtained is shown to be near optimal (efficiency is in terms of the persistence of the training data
and the controllability Gramian). Abbasi-Yadkori & Szepesvari (2011) also provides provable, non-
asymptotic learning results (in a regret context), using a bandit algorithm that achieves lower sample
complexity (by balancing exploration-exploitation more effectively); the computational efficiency
of this approach is less clear.
More recently, Dean et al. (2017) expands on an explicit system identification process, where a
robust control synthesis procedure is adopted that relies on a coarse model of the plant matrices
(A and B are estimated up to some accuracy level, naturally leading to a “robust control” setup).
Arguably, this is the most general (and non-asymptotic) result, that is efficient from both a statistical
perspective (computationally, the method works with a finite horizon to approximate the infinite
horizon). This result only needs the plant to be controllable; the work herein needs the stronger
assumption that the initial policy in the local search procedure is a stable controller (an assumption
which may be inherent to local search procedures, discussed in Section 5).
Model-free learning approaches. Model-free approaches that do not rely on an explicit system
identification step typically either: 1) estimate value functions (or state-action values) through Monte
Carlo simulation which are then used in some approximate dynamic programming variant (Bert-
sekas, 2011) or 2) directly optimize a (parameterized) policy, also through Monte Carlo simulation.
Model-free approaches for learning optimal controllers is not well understood, from a theoretical
perspective. Here, Bradtke et al. (1994) provides an asymptotic learnability result using a value
function approach, namely Q-learning.
2	Preliminaries and Background
2.1	Exact Gradient Descent
This work seeks to characterize the behavior of (direct) policy gradient methods, where the policy
is linearly parameterized, as specified by a matrix K ∈ Rk×d which generates the controls:
ut = -Kxt
3
Under review as a conference paper at ICLR 2018
for t ≥ 0. The cost of this K is denoted as:
∞
C(K)= Eχo~D X(x>Qxt + U>Rut)
t=0
where {xt, ut} is the trajectory induced by following K, starting with xo 〜D. The importance
of (some) randomization, either in x0 or noise through having a disturbance, for analyzing gradient
methods is discussed in Section 3. Here, K * is a minimizer of C (∙).
Gradient descent on C(K), with a fixed stepsize η, follows the update rule:
K J K - NC(K).
It is helpful to explicitly write out the functional form of the gradient. Define PK as the solution to:
PK = Q + K>RK+ (A - BK)>PK(A - BK) .
and, under this definition, it follows that C(K) can be written as:
C(K) = Eχo~D x>PKxo .
Also, define ΣK as the (un-normalized) state correlation matrix, i.e.
∞
∑K = Eχo~D E xtx> .
t=o
Lemma 1. (Policy Gradient Expression) The policy gradient is:
VC(K) = 2 ((R + B>PKB)K - B>PKA) ∑k
Proof. Observe:
x>Pkxo = x> (Q + K>RK) xo + x> (A — BK)>Pk (A — BK)xo .
This implies:
xo>VPKxo = 2RKxoxo> - 2B>PK(A - BK)xoxo> +xo>(A - BK)>VPK(A - BK)xo
=	2 ((R + B>PkB)K — B>PkA) xox> + x>VPKxι
∞
=	2 ((R + B>PkB)K — B>PkA)Xxt xt>
t=o
using recursion and that xι = (A 一 BK)xo. Taking expectations completes the proof.	口
2.2 Review: (Model free) sample based policy gradient methods
Sample based policy gradient methods introduce some randomization for estimating the gradient.
REINFORCE. Let ∏θ(u|x) be a parametric stochastic policy, where u 〜∏θ(∙∣x). The policy
gradient of the cost, C(θ), is:
∞
VC(θ) = E X Qπθt (xt, ut)V log πθ(ut |xt)
t=o
∞
where Qπθ (x, u) = E	ct|xo = x, uo = u
t=o
where the expectation is with respect to the trajectory {xt, ut} induced under the policy πθ and
where Qπθ (x, u) is referred to as the state-action value. The REINFORCE algorithm uses Monte
Carlo estimates of the gradient obtained by simulating πθ .
The natural policy gradient. The natural policy gradient (Kakade, 2001) follows the update:
∞
θ J θ — ηG-1VC (θ), where Gθ = E ^X V log ∏θ (ut∣xt)V log ∏θ (ut∣xt)>
t=o
4
Under review as a conference paper at ICLR 2018
where Gθ is the Fisher information matrix. There are numerous succesful related approaches (Peters
& Schaal, 2007; Schulman et al., 2015; Duan et al., 2016). An important special case is using a linear
policy with additive Gaussian noise (Rajeswaran et al., 2017b), i.e.
πK (x, u) = N (Kx, σ2I)	(3)
where K ∈ Rk×d and σ2 is the noise variance. Here, the natural policy gradient of K (when σ is
considered fixed) takes the form:
K 一 K - ηVC(κ)∑κ1	(4)
To see this, one can verify that the Fisher matrix of size kd × kd, which is indexed as [GK](i,j),(i0,j0)
where i, i0 ∈ {1, . . . k} and j, j0 ∈ {1, . . . d}, has a block diagonal form where the only non-zeros
blocks are [Gk](i,.),(i,.)= ∑k (this is the block corresponding to the i-th coordinate of the action,
as i ranges from 1 to k). This form holds more generally, for any diagonal noise.
Zeroth order optimization. Zeroth order optimization is a generic procedure (Conn et al., 2009;
Nesterov & Spokoiny, 2015) for optimizing a function f (x), using only query access to the function
values of f (∙) at input points X (and without explicit query access to the gradients of f). This is also
the approach in using “evolutionary strategies” (Salimans et al., 2017). The generic approach can
be described as follows: define the perturbed function as
fσ2 (X) = Eε〜N(0,σ2I) [f(X + ε)]
For small σ, the smooth function is a good approximation to the original function. Due to the
Gaussian smoothing, the gradient has the particularly simple functional form (see Conn et al. (2009);
Nesterov & Spokoiny (2015)):
Vfσ2 (X) = σEε〜N(0,σ2I) [f(x + ε)ε] .
This expression implies a straightforward method to obtain an unbiased estimate of the Vfσ2 (X),
through obtaining only the function values f(X + ε) for random ε.
3 The (non-convex) Optimization Landscape
This section provides a brief characterization of the optimization landscape, in order to help provide
intuition as to why global convergence is possible and as to where the analysis difficulties lie.
Lemma 2. (Non-convexity) Ifd ≥ 3, there exists an LQR optimization problem, minK C(K), which
is not convex, quasi-convex, and star-convex.
Section B provides a specific example. In general, for a non-convex optimization problem, gradient
descent may not even converge to the global optima in the limit. For the case of LQRs, the following
corollary (of Lemma 8) provides a characterization of the stationary points.
Corollary 3. (Stationary point characterization) If VC(K) = 0, then either K is an optimal policy
or ΣK is rank deficient.
This lemma is the motivation for using a distribution over X0 (as opposed to a deterministic starting
point): Ex。〜DX0X> being full rank guarantees that ∑k is full rank, which implies all stationary
points are a global optima. An additive disturbance in the dynamics model also suffices.
The concept of gradient domination is important in the non-convex
1963; Nesterov & Polyak, 2006; Karimi et al., 2016). A function f
dominated if there exists some constant λ, such that for all X,
optimization literature (Polyak,
d
: R → R is said to be gradient
f(X) - minf(X0) ≤ λkVf(X)k2.
x0
If a function is gradient dominated, this implies that if the magnitude of the gradient is small at some
X, then the function value at X will be close to that of the optimal function value.
The following corollary of Lemma 8 shows that C(K) is gradient dominated.
Corollary 4. (Gradient Domination) Suppose Ex。〜D xox> is full rank. Then C (K) is gradient
dominated, i.e.
C (K) — C (K *) ≤ λhVC(K), VC(K )i
where λ is a problem dependent constant (and〈•, •)denotes the trace inner product).
5
Under review as a conference paper at ICLR 2018
With gradient domination and no (spurious) local optima, one may hope that recent results on es-
caping saddle points (Nesterov & Polyak, 2006; Ge et al., 2015; Jin et al., 2017) immediately imply
that gradient descent converges quickly. This is not the case due to that it is not straightforward
to characterize the (local) smoothness properties of C(K); this is a difficulty well studied in the
optimal control theory literature, related to robustness and stability. In fact, if it were the case that
C(K) is a smooth function1 (in addition to being gradient dominated), then classical mathematical
optimization results (Polyak, 1963) would not only immediately imply global convergence, these
results would also imply convergence at a linear rate.
4 Main Results
First, results on exact gradient methods are provided. From an analysis perspective, this is the nat-
ural starting point; once global convergence is established for exact methods, the question of using
simulation-based, model-free methods can be approached with zeroth-order optimization methods.
Notation. kZk denotes the spectral norm of a matrix Z; Tr(Z) denotes the trace of a square matrix;
σmin(Z) denotes the minimal singular value of a square matrix Z. Also, it is helpful to define
μ := σmin(Exo 〜D X0X>)
4.1	Model-based optimization: exact gradient methods
The following three exact update rules are considered:
Gradient descent:	Kn+ι	=	Kn	-	ηVC(Kn)	(5)
Natural policy gradient descent:	Kn+ι	=	Kn	—	ηVC(Kn)∑Kl	(6)
Gauss-Newton:	Kn+1	=	Kn	-	η(R + B>PKnB)-1VC(Kn)Σ-K1	.	(7)
The natural policy gradient descent direction	is	defined	so that it is consistent with	the	stochastic
case, as per Equation 4. It is straightforward to verify that the policy iteration algorithm is a special
case of the Gauss-Newton method when η = 1 (for the case of policy iteration, convergence in the
limit is provided in Todorov &Li (2004); Ng et al. (2002); Liao & Shoemaker (1991), along with
local convergence rates.)
The Gauss-Newton method requires the most complex oracle to implement: it requires access to
VC(K), ΣK , and R+B> PK B; it also enjoys the strongest convergence rate guarantee. At the other
extreme, gradient descent requires oracle access to only VC(K) and has the slowest convergence
rate. The natural policy gradient sits in between, requiring oracle access to VC(K) and ΣK , and
having a convergence rate between the other two methods.
Theorem 5. (Global Convergence ofGradient Methods) Suppose C (Ko) is finite and and μ > 0.
•	Gauss-Newton case: For a stepsize η = 1 and for
N > k∑Klk log C(KO)- C(K*)
μ	ε	，
the Gauss-Newton algorithm (Equation 7) enjoys the following performance bound:
C(KN) - C (K *) ≤ ε
•	Natural policy gradient case: For a stepsize
_	1
η =闽 + kBk2Cg)
and for
N > k∑κ*k ( kRk + kBk2C(K0)ʌ log C(Ko)- C(K*)
μ	∖σmin(R)	μσmin(R) J	ε
natural policy gradient descent (Equation 6) enjoys the following performance bound:
____________________________________ C(KN) - C (K *) ≤ ε .
1A differentiable function f(x) is said to be smooth if the gradients of f are continuous.
6
Under review as a conference paper at ICLR 2018
Algorithm 1 Model-Free Policy Gradient (and Natural Policy Gradient) Estimation
1:
2:
3:
4:
Input: K, number of trajectories m, roll out length `, smoothing parameter r, dimension d
for i = 1,∙…m do
Sample a policy Kbi = K + Ui, where Ui is drawn uniformly at random over matrices whose
(Frobenius) norm is r.
Simulate Ki for ' steps starting from x° 〜 D. Let Ci and Σi be the empirical estimates:
``
Cbi = X ct ,	Σbi = X xtxt>
t=1	t=1
where ct and xt are the costs and states on this trajectory.
5:	end for
6:	Return the (biased) estimates:
1md
VC(K) = m Er2 CiUi,
i=1
m
-X ∑ i
m
i=1
• Gradient descent case: For an appropriate (constant) setting of the stepsize η,
η = poly
(μσmin (Q)	111
I^Kr, kAk, kBk,商,σmin
and for
N ≥k∑Klk log C(KG - C(K*)* poly (^K^
μ	ε	∖μσ min (Q)
MkjB k,∣∣R∣, -ɪ-),
σmin (R)
gradient descent (Equation 5) enjoys the following performance bound:
C(KN) - C (K *) ≤ ε.
In comparison to model-based approaches, these results require the (possibly) stronger assumption
that the initial policy is a stable controller, i.e. C(K0) is finite (an assumption which may be inherent
to local search procedures). The Discussion mentions this as direction of future work.
4.2 Model free optimization: sample based policy gradient methods
In the model free setting, the controller has only simulation access to the model; the model param-
eters, A, B, Q and R, are unknown. The standard optimal control theory approach is to use system
identification to learn the model, and then plan with this learned model This section proves that
model-free, policy gradient methods also lead to globally optimal policies, with both polynomial
computational and sample complexities (in the relevant quantities).
Using a zeroth-order optimization approach (see Section 2.2), Algorithm 1 provides a procedure to
find (controllably biased) estimates, VC(K) and ΣK, of both VC(K) and ΣK. These can then be
used in the policy gradient and natural policy gradient updates as follows:
Gradient descent:	Kn+1	=	Kn	- ηVC(Kn)	(8)
Natural policy gradient descent:	Kn+ι	=	Kn	一 ηVC(Kn)∑K：	,	(9)
where Algorithm 1 is called at every iteration to provide the estimates of VC(Kn) and ΣKn.
The choice of using zeroth order optimization vs using REINFORCE (with Gaussian additive noise,
as in Equation 3) is primarily for technical reasons2. It is plausible that the REINFORCE estimation
procedure has lower variance. One additional minor difference, again for technical reasons, is that
Algorithm 1 uses a perturbation from the surface ofa sphere (as opposed to a Gaussian perturbation).
2The correlations in the state-action value estimates in REINFORCE are more challenging to analyze.
7
Under review as a conference paper at ICLR 2018
Theorem 6. (Global Convergence in the Model Free Setting) Suppose C (Ko) is finite, μ > 0, and
that xo 〜 D has norm bounded by L almost surely. Also, for both the policy gradient method and
the natural policy gradient method, suppose Algorithm 1 is called with parameters:
m,', 1/r = poly (C(Ko),1, —^ɪ, ∣∣A∣∣, ∣∣B∣∣, ∣∣R∣∣, —1—^,d, 1∕e,L2∕μ).
∖	μ σmin(Q)	σmin(R)	J
•	Natural policy gradient case: For a stepsize
_	1
η = kRk + kBk2Cg)
and for
kRk	+ kBk2C(Ko)
σmin (R)	μσmin (R)
then, with high probability, i.e. with probability greater than 1 - exp(-d), the natural
policy gradient descent update (Equation 9) enjoys the following performance bound:
C(KN) - C (K *) ≤ ε.
log2(C(Ko)- C(K*))
N > "k
μ
ε
•	Gradient descent case: For an appropriate (constant) setting of the stepsize η,
η=poly
μσmin (Q)	111
^Kr, PK, Pk,画,σmin
and if N satisfies
N≥
k∑κ*k log C(KO)- C(K*)
μ	ε
Poly (μσ [in (Q) JAkJB k, kRk, σm~R))
then, with high probability, gradient descent (Equation 8) enjoys the following performance
bound:
C(KN) - C(K*) ≤ ε.
5	Conclusions and Discussion
This work has provided provable guarantees that model-based gradient methods and model-free
(sample based) policy gradient methods convergence to the globally optimal solution, with finite
polynomial computational and sample complexities. Taken together, the results herein place these
popular and practical policy gradient approaches on a firm theoretical footing, making them compa-
rable to other principled approaches (e.g. subspace ID methods and algebraic iterative approaches).
Finite C(Ko ) assumption, noisy case, and finite horizon case. These methods allow for exten-
sions to the noisy case and the finite horizon case. This work also made the assumption that C(Ko)
is finite, which may not be easy to achieve in some infinite horizon problems. The simplest way
to address this is to model the infinite horizon problem with a finite horizon one; the techniques
developed in Section D.1 shows this is possible. This is an important direction for future work.
Open Problems.
•	Variance reduction: This work only proved efficiency from a polynomial sample size per-
spective. An interesting future direction would be in how to rigorously combine variance
reduction methods and model-based methods to further decrease the sample size.
•	A sample based Gauss-Newton approach: This work showed how the Gauss-Newton algo-
rithm improves over even the natural policy gradient method, in the exact case. A practi-
cally relevant question for the Gauss-Newton method would be how to both: a) construct a
sample based estimator b) extend this scheme to deal with (non-linear) parametric policies.
•	Robust control: In model based approaches, optimal control theory provides efficient pro-
cedures to deal with (bounded) model mis-specification. An important question is how to
provably understand robustness in a model free setting.
8
Under review as a conference paper at ICLR 2018
References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Regret bounds for the adaptive control of linear
quadratic systems. Conference on Learning Theory, 2011. ISSN 15337928.
M. Al Borno, M. de Lasa, and A. Hertzmann. Trajectory Optimization for Full-Body Movements
with Complex Contacts. IEEE Transactions on Visualization and Computer Graphics, 2013.
Brian D. O. Anderson and John B. Moore. Optimal Control: Linear Quadratic Methods. Prentice-
Hall, Inc., Upper Saddle River, NJ, USA, 1990. ISBN 0-13-638560-5.
J. Andrew Bagnell and Jeff Schneider. Covariant policy search. In Proceedings of the 18th Interna-
tional Joint Conference on Artificial Intelligence, IJCAI'03, pp. 1019-1024, San Francisco, CA,
USA, 2003. Morgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.
cfm?id=1630659.1630805.
V. Balakrishnan and L. Vandenberghe. Semidefinite programming duality and linear time-invariant
systems. IEEE Transactions on Automatic Control, 48(1):30-41, 2003.
Dimitri P. Bertsekas. Approximate policy iteration: A survey and some new methods. Jour-
nal of Control Theory and Applications, 9(3):310-335, 2011. ISSN 16726340. doi: 10.1007/
s11768-011-1005-3.
Dimitri P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2017.
S.J. Bradtke, B.E. Ydstie, and a.G. Barto. Adaptive linear quadratic control using policy iteration.
Proceedings of American Control Conference, 3(2):3475-3479, 1994. doi: 10.1109/ACC.1994.
735224.
E.F. Camacho and C. Bordons. Model Predictive Control. Advanced Textbooks in Control and
Signal Processing. Springer London, 2004. ISBN 9781852336943.
A.R. Conn, K. Scheinberg, and L.N. Vicente. Introduction to derivative-free optimization, volume 8
of MPS/SIAM Series on Optimization. Society for Industrial and Applied Mathematics (SIAM),
Philadelphia, PA; Mathematical Programming Society (MPS), Philadelphia, PA, 2009.
S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu. On the sample complexity of the linear quadratic
regulator. ArXiv e-prints, 2017.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for
robotics. Found. Trends Robot, 2(1&#8211;2):1-142, August 2013. ISSN 1935-8253. doi:
10.1561/2300000021. URL http://dx.doi.org/10.1561/2300000021.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In ICML, 2016.
Lawrence C. Evans. An introduction to mathematical optimal control theory. University of
California, Department of Mathematics, pp. 126, 2005. ISSN 14712334. doi: 10.1186/
1471-2334-10-32.
Claude-Nicolas Fiechter. PAC adaptive control of liner systems. In Proceeding COLT ’94 Proceed-
ings of the seventh annual conference on Computational learning theory, pp. 88-97, 1994.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization
in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual
ACM-SIAM symposium on Discrete algorithms, pp. 385-394. Society for Industrial and Applied
Mathematics, 2005.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. Proceedings of The 28th Conference on Learning Theory,
COLT 2015, Paris, France, July 3-6, 2015, 2015.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape sad-
dle points efficiently. In Proceedings of the 34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 1724-1732, 2017.
9
Under review as a conference paper at ICLR 2018
S. Kakade. A natural policy gradient. In NIPS, 2001.
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In ICML,
2002.
S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis, Gatsby Computa-
tional Neuroscience Unit, University College, London, 2003.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-ICjasieWicz condition. Machine Learning and Knowledge Dis-
covery in Databases: European Conference, ECML PKDD 2016, Riva del Garda, Italy, Septem-
berl9-23, 2016, Proceedings, Part I, pp. 795-811, 2016.
D.	L. Kleinman. On an iterative technique for Riccati equation comPutations. IEEE Transactions on
Automatic Control, 13(1):114-115, 1968. ISSN 0018-9286. doi: 10.1109/TAC.1968.1098829.
V. Kumar, E. Todorov, and S. Levine. Optimal control With learned local models: Application to
dexterous manipulation. In ICRA, 2016.
P. Lancaster and L. Rodman. Algebraic Riccati Equations. Oxford science publications. Clarendon
Press, 1995. ISBN 9780191591259.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. JMLR, 17(39):1-40, 2016.
L. Z. Liao and C. A. Shoemaker. Convergence in unconstrained discrete-time differential dynamic
programming. IEEE Transactions on Automatic Control, 36, 1991.
Lennart Ljung (ed.). System Identification (2Nd Ed.): Theory for the User. Prentice Hall PTR,
Upper Saddle River, NJ, USA, 1999. ISBN 0-13-656695-2.
Karl Martensson. Gradient methods for large-scale and distributed linear quadratic control. Ph.D.
Theses, 2012.
Karl Martensson and Anders Rantzer. Gradient methods for iterative distributed control synthesis.
Conference on Decision and Control, pp. 1-6, 2009.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518, 2015.
Yurii Nesterov and Boris T. Polyak. Cubic regularization of neWton method and its global perfor-
mance. Math. Program., pp. 177-205, 2006.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, pp. 1-40, 2015. ISSN 1615-3383.
Chi-Kong Ng, Li-Zhi Liao, and Duan Li. A globally convergent and efficient method for uncon-
strained discrete-time optimal control. J. Global Optimization, 23:401-421, 2002.
J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71:1180-1190, 2007.
E.	Polak. An Historical Survey of Computational Methods in Optimal Control. SIAM Review, 15
(2):pp. 553-584, 1973. ISSN 00361445. doi: 10.1137/1015071.
B. T. Polyak. Gradient methods for minimizing functionals. USSR Computational Mathematics and
Mathematical Physics, 3(4):864878, 1963.
Aravind RajesWaran, Vikash Kumar, Abhishek Gupta, John Schulman, Emanuel Todorov, and
Sergey Levine. Learning complex dexterous manipulation With deep reinforcement learning and
demonstrations. CoRR, abs/1709.10087, 2017a. URL http://arxiv.org/abs/1709.
10087.
10
Under review as a conference paper at ICLR 2018
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization
and simplicity in continuous control. CoRR, abs/1703.02660, 2017b. URL http://arxiv.
org/abs/1703.02660.
J.B. Rawlings and D.Q. Mayne. Model Predictive Control: Theory and Design. Nob Hill Pub.,
2009. ISBN 9780975937709.
Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alterna-
tive to reinforcement learning. ArXiv e-prints, 2017.
J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel. Trust region policy optimization. In
ICML, 2015.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with
deep neural networks and tree search. Nature, 529, 2016.
Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization of complex behaviors through online
trajectory optimization. International Conference on Intelligent Robots and Systems, 2012.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. ArXiv
e-prints, 2017.
Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedback
control of constrained nonlinear stochastic systems. In American Control Conference, 2004.
11
Under review as a conference paper at ICLR 2018
A	Planning with a model
This section briefly reviews some parameterizations and solution methods for the classic LQR and
related problems from control theory.
Finite horizon LQR. First, consider the finite horizon case. The basic approach is to view it as a
dynamic program with the value function xtT Ptxt, where
Pt-1 =Q+ATPkA-ATPtB(R+BTPtB)-1BTPtA,
which in turn gives optimal control
ut = -Ktxt = -(R + BTPt+1B)-1BTPt+1Axt
(recursions run backward in time).
Another approach is to view the LQR problem as a linearly-constrained Quadratic Program in all xt
and ut (where the constraints are given by the dynamics, and the problem size equals the horizon).
The QP is clearly a convex problem, but this observation is not useful by itself as the problem size
grows with the horizon, and naive use of quadratic programming scales badly. However, the spe-
cial structure due to linear dynamics allows for simplifications and control-theoretic interpretation
as follows: the Lagrange multipliers can be interpreted as “co-state” variables, and they follow a
recursion that runs backwards in time known as the “adjoint system”. Using Lagrange duality, one
can show that this approach is equivalent to solving the Riccati recursion mentioned above.
Popular use of the LQR in control practice is often in the receding horizon LQR, Camacho & Bor-
dons (2004); Rawlings & Mayne (2009): at time t, an input sequence is found that minimizes the
T -step ahead LQR cost starting at the current time, then only the first input in the sequence is used.
The resulting static feedback gain converges to the infinite horizon optimal solution as horizon T
becomes longer.
Infinite horizon LQR. Here, the constrained optimization view (QP) is not informative as the prob-
lem is infinite dimensional; the dynamic programming viewpoint readily extends. Suppose the
system A, B is controllable (which guarantees optimal cost is finite). It turns out that the value
function and the optimal controller are static (do not depend on t) and can be found by solving the
Algebraic Riccati Equation (ARE) given in (1). The optimal K can then be found from equation
(2).
The main computational step is solving the ARE, which is extensively studied (e.g. (Lancaster &
Rodman, 1995)). One approach due to Kleinman (1968) is to simply run the recursion Pk+1 = Q +
ATPkA - ATPkB(R + BT PkB)-1BT PkA with P1 = Q, which converges to the unique positive
semidefinite solution of the ARE (since the fixed-point iteration is contractive). Other approaches
are direct and based on linear algebra, which carry out an eigenvalue decomposition on a certain
block matrix followed by a matrix inversion (Lancaster & Rodman, 1995).
Direct computation of the control input has also been considered in the optimal control literature,
e.g., gradient updates in function spaces (Polak, 1973). For the linear quadratic setup, direct iterative
computation of the feedback gain has been examined in (Martensson & Rantzer, 2009), and explored
further in (Martensson, 2012) with a view towards distributed implementations. There methods are
presented as local search heuristics without provable guarantees of reaching the optimal policy.
SDP formulation. The LQR problem can also be expressed as a semidefinite program (SDP) with
variable P, as given in (Balakrishnan & Vandenberghe, 2003) (section 5, equation (34), this is for
a continuous-time system but there are similar discrete-time versions). This SDP can be derived
by relaxing the equality in the Riccati equation to an inequality, then using the Schur complement
formula to rewrite the resulting Riccati inequality as linear matrix inequality; the objective in the
case of LQR is the trace of the positive definite matrix variable. This formulation and its dual has
been explored in (Balakrishnan & Vandenberghe, 2003).
It is important to note that while the optimal solution of this SDP is the unique positive semidefinite
solution to the Riccati equation, which in turn gives the optimal policy K*, other feasible P (not
equal to P*) do not necessarily correspond to a feasible, stabilizing policy K. This means that
the feasible set of this SDP is not a convex characterization of all P that correspond to stabilizing
K. Thus it also implies that if one uses any optimization algorithm that maintains iterates in the
12
Under review as a conference paper at ICLR 2018
feasible set (e.g. interior point methods), no useful policy can be extracted from the iterates before
convergence to P*. For this reason, this convex formulation is not helpful for parametrizing the
space of policies K in manner that supports the use of local search methods (those that directly
lower the cost function of interest), which is the focus of this work.
B Non-convexity of the set of stabilizing State Feedback Gains
Let K(A, B) denote the set of state feedback gains K such that A - BK is stable, i.e., its eigen-
values are inside the unit circle in the complex plane. This set is generally nonconvex. A concise
counterexample to convexity is provided here. Let A and B be 3 × 3 identity matrices and
1	0	-10	1	-10	0
K1 =	-1 1	0 and K2 =	0	1	0	.
0	0	1	-1	0	1
Then the spectra of A - BK1 and A - BK2 are both concentrated at the origin, yet two of the
eigenvalues of A - BK with K = (K1 + K2)/2, are outside of the unit circle in the complex
plane.
C Analysis: the exact case
This section provides the analysis of the convergence rates of the (exact) gradient based methods.
First, some helpful lemmas for the analysis are provided.
Throughout, it is convenient to use the following definition:
EK := (R + B>PKB)K - B>PKA.
The policy gradient can then be written as:
VC(K) = 2 ((R + B>PkB)K - B>PkA) ∑k =
C.1 Helper lemmas
Define the value VK (x), the state-action value QK (x, u), and the advantage AK (x, u). VK (x, t) is
the cost of the policy starting with x0 = x and proceeding with K onwards:
∞
VK (x) :=	xt>Qxt + ut>Rut
t=0
= x>PKx .
QK (x, u) is the cost of the policy starting with x0 = x, taking action u0 = u and then proceeding
with K onwards:
QK (x, u) := x>Qx + u>Ru + VK (Ax + Bu)
The advantage AK (x, u) is:
AK (x, u) = QK (x, u) - VK (x) .
The advantage can be viewed as the change in cost starting at state x and taking a one step deviation
from the policy K .
The next lemma is identical to that in (Kakade & Langford, 2002; Kakade, 2003) for Markov deci-
sion processes.
Lemma 7. (Cost difference lemma) Suppose K and K0 have finite costs. Let {x0t} and {u0t} be state
and action sequences generated by K0, i.e. starting with x00 = x and using u0t = -K0x0t. It holds
that:
VK0 (x) - VK (x) =	AK(x0t, u0t) .
t
Also, for any x, the advantage is:
AK(x,K0x) = 2x>(K0 - K)>EKx+x>(K0 - K)>(R+B>PKB)(K0 - K)x. (10)
13
Under review as a conference paper at ICLR 2018
Proof. Let Ct be the cost sequence generated by K0. Telescoping the sum appropriately:
Vk (X)- Vκ (x) = X Ct- Vκ (x)
t=0
= X(Ct + vK (Xt)- vK (Xt))- vK (X)
t=0
= X(Ct + VK(Xt+1) - VK(Xt))
t=0
=EAK (Xt,ut)
t=0
which completes the first claim.
For the second claim, observe that:
Vk(x) = XT (Q + KTRK) X + XT(A - BK)τPκ(A - BK)x
And, for U = K0x,
AK (x,u) = Qk (x,u) - VK (x)
= xt (Q + (K0)τRK0) X + xt(A - BK0)τPκ(A - BK0)x - Vk(x)
=xt (Q + (K0 - K + K)τR(K0 - K + K)) X +
xt(A - BK - B(K0 - K))τPK(A - BK - B(K0 - K))x - Vk(x)
= 2xt(KK - K)τ ((R + BtPkB)K - BtPkA) X +
xt(K0 - K)τ(R + BtPkB)(K0 - K))x,
which completes the proof.	口
This lemma is helpful in proving that C (K) is gradient dominated.
Lemma 8. (Gradient domination) Let K * be an optimal policy. Suppose K has finite cost and
μ > 0. It holds that:
C (K) - C (K *) ≤ k∑κ*kTr(E> (R + BtPk B)TEK)
≤ ^kTr(ETEk)
σmin (R)
≤ J)；R Tr(VC(K)TVC(K))
μ σmin (R)
For a lower bound, it holds that:
c(K)- C(K*)≥W⅛⅛ TY(ETEK)
Proof. From Equation 10 and by completing the square,
Qk (x,K0x) - Vk (x)
= 2Tr(xxτ(K/ - K)tEk) + Tr(xxτ(K/ - K)τ(R + BTPKB)(K/ - K))
= Tr(xxτ (K0 - K + (R + BTPKB)-1Ek)> (R + BTPKB) (K0 - K + (R + BTPKB)-1Ek))
-Tr(XXTEK(R + BtPk B)-1Ek )
≥ -Tr(XXTEK(R + BtPk B)-1Ek )	(11)
with equality when K0 = K - (R + BtPkB)-1Ek.
14
Under review as a conference paper at ICLR 2018
Let XJ= and u↑ be the sequence generated under K*. Using this and Lemma 7,
C(K) - C(K*) =	-E	AK(xJ*,uJ*) J
≤	EXTr(xJ*(xJ*)τEKτ(R+BτPKB)-1EK) J Tr(∑κ* EK (R + BτPκ B)-IEK)
≤ ≤	k∑κ* kTr(E> (R + BτPκ B)TEK) k∑κ*kk(R + BTPKB)Tk Tr(EKEK)
≤	管Rk Tr(E> Ek ) σmin (R) ^KRk Tr(∑κ1VC(K)TVC(K)∑κ1) σmin (R)
≤	4ς⅛ Tr(VC(K)TVC(K)) μ2σmin(R)
which completes the proof of the upper bound.
For the lower bound, consider K0 = K- (R+B>PKB)-1EK where equality holds in Equation 11.
Let x0J and u0J be the sequence generated under K0. Using that C(K*) ≤ C(K0),
C(K) - C(K*) ≥ C(K) - C(K0)
= -EXAK(x0J,u0J)
J
= EXTr(x0J(x0J)>EK>(R+B>PKB)-1EK)
J
≥ Tr(ΣK0EK>(R+B>PKB)-1EK)
≥	∣∣R + B>PkBk Tr(EKEK)
which completes the proof.	□
Recall that a function f is said to be smooth (or C1-smooth) if it satisfies for some finite β, it
satisfies:
If (x) - f (y) - Vf (y)τ(χ - y)| ≤ 2∣∣X - y∣∣2.	(12)
for all x, y (equivalently, it is smooth if the gradients of f are continuous).
Lemma 9. (“Almost” smoothness) C(K) satisfies:
C(K0) - C(K) = -2Tr(ΣK0(K - K0)τEK) +Tr(ΣK0(K - K0)τ(R+BτPKB)(K - K0))
To see why this is related to smoothness (e.g. compare to Equation 12), suppose K0 is sufficiently
close to K so that:
ΣK0≈ΣK+O(kK-K0k)	(13)
and the leading orderterm2Tr(ΣK0(K0 - K)τEK) would then behave as Tr((K0 -K)τVC(K)).
The challenge in the proof (for gradient descent) is quantifying the lower order terms in this argu-
ment.
Proof. The claim immediately results from Lemma 7, by using Equation 10 and taking an expecta-
tion.	□
The next lemma spectral norm bounds on PK and ΣK are helpful:
Lemma 10. It holds that:
kPKk≤ C(K), k∑κk≤ ”
μ	σmin(Q)
15
Under review as a conference paper at ICLR 2018
Proof. For the first claim, C(K) is lower bounded as:
C (K) = ExO 〜D x0 PK x0 ≥ IIPK kσmin(Ex0x0 )
Alternatively, C(K) can be lower bounded as:
C(K) = Tr(ΣK(Q + K>RK)) ≥ Tr(ΣK)σmin(Q) ≥ IΣKIσmin(Q),
which proves the second claim.	□
C.2 Gauss-Newton Analysis
The next lemma bounds the one step progress of Gauss-Newton.
Lemma 11. Suppose that:
K0 = K - η(R + B>PκB)TVC(K)ΣK1,.
If η ≤ 1, then
C (K0) - C (K *) ≤ (ι - ^η^ ) (C(K) — C (K *))
∖	IIςκ* 11/
Proof. Observe K0 = K - η(R + B >PKB)-1EK. Using Lemma 9 and the condition on η,
C(K0) - C(K) = -2ηTr(ΣK0EK>(R + B>PKB)-1EK) + η2Tr(ΣK0EK>(R + B>PKB)-1EK)
≤	-ηTr(ΣK0EK>(R+B>PKB)-1EK)
≤	-ησmin(ΣK0)Tr(EK> (R + B >PKB)-1EK)
≤ -ημTr(EK (R + BTPK B)TEK)
≤ -η^% (C(K)- C (K *)),
IIςk* k
where the last step uses Lemma 8.	□
With this lemma, the proof of the convergence rate of the Gauss Newton algorithm is immediate.
Proof. (of Theorem 5, Gauss-Newton case) The theorem is due to that η = 1 leads to a contraction
of 1 - k∑Kμ7k at every step.	□
C.3 Natural Policy Gradient Descent Analysis
The next lemma bounds the one step progress of the natural policy gradient.
Lemma 12. Suppose:
K0 = K - ηVC(K)Σ-K1
and that η ≤ UDsID . It holds that:
kR+B> PK Bk
C (K0) - C (K *) ≤(1 - ησmin (R) ɪɪ ) (C(K)- C (K *))
∖	IIςk* 11/
Proof. Since K0 = K - ηEK, Lemma 9 implies:
C(K0) - C(K) = -2ηTr(ΣK0EK>EK)+η2Tr(ΣK0EK>(R+B>PKB)EK)
The last term can be bounded as:
Tr(ΣK0EK>(R+B>PKB)EK) =Tr((R+B>PKB)EKΣK0EK>)
≤ IR+B>PKBITr(EKΣK0EK>)
= IR+B>PKBITr(ΣK0EK>EK).
16
Under review as a conference paper at ICLR 2018
Continuing and using the condition on η,
C(K0) - C(K)	≤	-2ηTr(ΣK0EK>EK)+η2kR+B>PKBkTr(ΣK0EK>EK)
≤	-ηTr(ΣK0EK>EK)
≤	-η σmin (ΣK 0)Tr(EK> EK)
≤	-ημTr(EK EK)
≤
μσmin (R)
-η k∑K*k
(C(K) - C(K*))
using Lemma 8.
□
With this lemma, the proof of the natural policy gradient convergence rate can be completed.
Proof. (of Theorem 5, natural policy gradient case) Using Lemma 10,
1	、	1	、	1
kR + B>PKBk ≥PP^mm ≥ kRk + kBk2C(K)
The proof is completed by induction: C(K1) ≤ C(K0), since Lemma 12 can be applied. The
proof proceeds by arguing that Lemma 12 can be applied at every step. If it were the case that
C(Kt) ≤ C(K0), then
/1/1/1
η ≤ kRk +间2Cg) ≤ |倒 +同2CS ≤ kR + B>PKtBk
and by Lemma 12:
C(Kt+0- C(K *) ≤ (1 -舟 kRk σmiBt⅞g))! (C(Kt)- C(K *))
which completes the proof.	口
C.4 Gradient Descent Analysis
As informally argued by Equation 13, the proof seeks to quantify how ΣK0 changes with η. Then
the proof bounds the one step progress of gradient descent.
ΣK PERTURBATION ANALYSIS
This subsections aims to prove the following:
Lemma 13. (ΣK perturbation) Suppose K0 is such that:
kK0-Kk≤
σmin(Q)μ
4C(K)∣∣Bk(∣∣A - BKk + 1)
It holds that:
k∑κo-∑κk≤ 4 (T)2 kBk (M - BKk + 1) kK - K0k
∖σmin(Q))	μ
The proof proceeds by starting with a few technical lemmas. First, define a linear operator on
symmetric matrices, TK(∙), which can be viewed as a matrix on (d+1) dimensions. Define this
operator on a symmetric matrix X as follows:
∞
TK(X) := X(A - BK)tX[(A - BK)>]t
t=0
Also define the induced norm of T as follows:
kT I，	kTK(X)k
kTK k=sXp	∣∣X k
(14)
17
Under review as a conference paper at ICLR 2018
where the supremum is over all symmetric matrices X (whose spectral norm is non-zero).
Also, define
Σ0 = Ex0x0>
Lemma 14. (TK norm bound) It holds that
kTKk ≤
C(K)
μ σmin (Q)
Proof. For a unit norm vector v ∈ Rd and unit spectral norm matrix X ,
∞
v>(TK(X))v = X v> (A - BK)tX[(A - BK)>]tv
t=0
∞
= X Tr([(A - BK)>]tvv>(A - BK)tX)
t=0
∞
= XTr([£0/2(A - BK)>]tvv>(A - BK)t∑Y2Σ-1^XΣ-1/2)
t=0
∞
≤ XTr([£0/2(A - BK)>]tvv>(A - BK)t∑Y2)k∑-1〃XΣ-"k
t=0
=k£-1/2XΣ-1-∣ (v>Tκ(∑0)v)
≤ -夫F kTκ (∑o)k
σmin(Ex0x0> )
=1k∑κk
μ
using that TK(Σ0) = ΣK. The proof is completed using the upper bound on kΣKk in Lemma 10.
□
Also, with respect to K, define another linear operator on symmetric matrices:
FK(X) = (A - BK)X(A - BK)> .
Let I to denote the identity operator on the same space. Define the induced norm k ∙ k of these
operators as in Equation 14. Note these operators are related to the operator TK as follows:
Lemma 15. When (A - BK) has spectral radius smaller than 1,
TK = (I - FK)-1.
Proof. When (A - BK) has spectral radius smaller than 1, TK is well defined and is the solution
of Tk = I + Tk ◦ FK. Therefore TK ◦ (I - FK) = I and TK = (I - FK)-1.	□
Since,
ΣK =TK(Σ0) = (I-FK)-1(Σ0).
The proof of Lemma 13 seeks to bound:
kΣK-ΣK0k=k(TK-TK0)(Σ0)k=k((I-FK)-1-(I-FK0)-1)(Σ0)k.
The following two perturbation bounds are helpful in this.
Lemma 16. It holds that:
kFK-FK0k ≤2kA-BKkkBkkK-K0k+kBk2kK-K0k2.
18
Under review as a conference paper at ICLR 2018
Proof. Let ∆ = K - K0 . For every matrix X ,
(FK -FK0)(X) = (A- BK)X(B∆)> + (B∆)X(A-BK)> - (B∆)X(B∆)>.
The operator norm ofFK -FK0 is the maximum possible ratio in spectral norm of (FK -FK0)(X)
and X. Then the claim follows because ∣∣AX∣∣ ≤ IlAkkXl∣.	□
Lemma 17. If
kTKkkFK-FK0k ≤ 1/2,
then
k (TK -TK0)(Σ)k ≤ 2kTKkkFK -FK0kkTK(Σ)k.
≤ 2kTKk2kFK-FK0kkΣk.
Proof. Define A = I - FK, and B = FK0 - FK. In this case A-1 = TK and (A - B)-1 = TK0 .
Hence, the condition kTK kkFK - FK0 k ≤ 1/2 translates to the condition kA-1 kkBk ≤ 1/2.
Observe:
(AT-(A- B)-1)(∑) = (I - (I - A-1 ◦ B)T)(AT(Σ)) = (I-(I- A-1 ◦ B)-I)(TK(∑)).
Since (I - A-1 ◦ B)-1 = I + A-1 ◦ B ◦ (I - A-1 ◦ B)-1,
k(I - A-1 ◦B)Tk ≤ 1 + kA-1 ◦Bkk(I-A-1 ◦B)Tk ≤ 1 + 1∕2∣(I - A-1 ◦B)Tk
which implies k(I - A-1 ◦ B)-1 k ≤ 2. Hence,
kI-(I-A-1°B)Tk = kA-%B。(I-A-1。B)Tk ≤ kA-1kkBkk(I-A-%B)Tk = 2kA-1kkBk∙
and so
kI - (I -A-1。B)Tk ≤ 2kA-1kkBk = 2kTκkkFκ -Wk ∙
Combining these two,
k(TK-TK0)(Σ)k ≤ k(I - (I - A-1 。 B)-1)kkTK (Σ)k ≤2kTKkkFK-FK0kkTK(Σ)k.
This proves the main inequality. The last step of the inequality is just applying definition of the norm
ofTk: kTκ(∑)k ≤ kTκkk∑k.	□
With these Lemmas, the proof is completed as follows:
Proof. (of Lemma 13) First, the proof shows kTK kkFK - FK0 k ≤ 1∕2, which is the desired
condition in Lemma 17. First, observe that under the assumed condition on kK - K0 k, implies that
kBkkK0-Kk≤
σmin(Q)μ	≤ 1 σmin (Q)μ V 1
4C(K)(∣∣A - BKk + 1) ≤ 4 C(K)	≤ 4
using that σmCnK))μ ≤ 1 due to Lemma 10. Using Lemma 16,
kFκ - Fκok ≤ (2∣∣A - BKkkBkkK - K0k + 1回2快-K0k2)
≤2kBk(kA-BKk+1)kK-K0k	(15)
Using this and Lemma 14,
kTκkkFκ - Fκok ≤ C(K) 2kBk (kA - BKk + 1) kK - K0k ≤ 2
σmin(Q)μ	2
where the last step uses the condition on kK - K0 k.
Thus,
kΣK0-ΣKk ≤2kTKkkFK-FK0kkTK(Σ0)k
≤ 2 C(K)
σmin(Q)μ
(2kBk(kA-BKk+1)kK-K0k)
C(K)
σmin (Q)
using Lemmas 10 and 16.
□
19
Under review as a conference paper at ICLR 2018
Gradient Descent Progress
Equipped with these lemmas, the one step progress of gradient descent can be bounded.
Lemma 18. Suppose that
where
K0 = K - NC(K),
η ≤ Lmin
16
J (σmin(Q)μ)2
ʌʌ C(K))
1	σmin(Q)
IIBllllVC(K)II(1 + IlA - BKk) , 2C(K)∣∣R + B丁PKBIl
It holds that:
.
(16)
C (K 0) - C (K *) ≤
…(R) ⅛i 卜 C(K)- C(K *))
Proof. By Lemma 9,
C(K0) - C(K)
=	-2ηTr(ΣK0 ΣK EK> EK) + η2Tr(ΣK ΣK0 ΣK EK> (R + B>PKB)EK)
≤ -2ηTr(ΣK EK> EK ΣK) + 2ηIΣK0 - ΣK ITr(ΣK EK> EK)
+η2IΣK0IIR + B>PKBITr(ΣKΣKEK>EK)
≤	-2ηTr(∑κEKEK∑k) + 2η k*K0 -SKk Tr(∑κEKEK∑k)
σmin(ΣK)
+η2IΣK0IIR + BKPKBITr(ΣKEKKEKΣK)
=-2η (1 - ksK0 -SKk - ηk∑K0kkR + B>PkBk) Tr(VC(K)>VC(K))
σmin(ΣK)	2
≤ -2ημ2σmin(R) (1 -ksK0- SKk - ηk∑K0kkR + B>PkBk) (C(K)- C(K*))
I∣ςk* k ∖	μ	2	J
where the last step uses Lemma 8.
By Lemma 13,
ksK0- SKk ≤ 4η ( C(K) )2 kBk (kA - BKk + 1)) kVC(K)k ≤ 1/4
μ	∖σmin(Q)μ√
using the assumed condition on η.
Using this last claim and Lemma 10,
ksKok≤kSKO-SKk + ksKk≤ 4 + σC(K)) ≤
k∑K0 k + C(K)
4	σmin(Q)
and so kSK0 k ≤ sSK⅛. HenCe,
1 - kSK0- SKk - 2ISkokkR + B>PkBk ≥ 1 - 1/4 - 234C(K) kR + B>PkBk ≥ 1/2
μ	2	2 3σmin(Q)
using the condition on η.	□
In order to prove a gradient descent convergence rate, the following bounds are helpful:
Lemma 19. It holds that
kVC(K)k
≤ ^KL J
σmin(Q)
kR + B>PkBk(C(K) - C(K*))
μ
and that:
kK k ≤ σm⅛y
(S
kR +B>PKBk(C(K)- C(K *))+ kB >Pk Ak
μ
20
Under review as a conference paper at ICLR 2018
Proof. Using Lemma 10,
kVC(K)k2 ≤ Tr(∑κEKEK∑k) ≤ k∑κk2Tr(E>EK) ≤ (^K^)2 Tr(E>EK)
σmin (Q)
By Lemma 8,
TY(E>Ek) ≤ kR + B>PκBk(C(K)-C(K少
μ
which proves the first claim.
Again using Lemma 8,
kKk ≤ k(R+B>PKB)-1kk(R+B>PKB)Kk
≤ -Λr k(R + BTPKB)Kk
σmin(R)
≤ -ΛR (k(R + BtPkB)K - BtPkAk + kB>PκAk)
σmin(R)
=kEκ k + kB>Pκ Ak
σmin(R)	σmin(R)
≤
JTr(EK EK)
σmin (R)
kB>PκAk
σmin (R)
,(C(K) — C(K *))∣∣R + B>Pκ 可 + IIBTPK Ak
√∕μσmin(R)	σmin (R)
which proves the second claim.
□
With these lemmas, the proof of the gradient descent convergence rate follows:
Proof. (of Theorem 5, gradient descent case) First, the following argues that progress is made at
t = 1. Based on Lemma 10 and Lemma 19, by choosing η to be an appropriate polynomial in
c(K0), k
BI, W
,σmin(R),σmin(Q) and μ, the stepsize condition in Equation 16 is satisfied.
Hence, by Lemma 18,
C(KI)- C(K*) ≤
(l- ^n(R) M) (C(KO)- C(K*))
which implies that the cost decreases at t = 1. Proceeding inductively, now suppose that C(Kt) ≤
C(K0), then the stepsize condition in Equation 16 is still satisfied (due to the use of C(K0) in
bounding the quantities in Lemma 19). Thus, Lemma 18 can again be applied for the update at time
t + 1 to obtain:
C(Kt+ι) — C(K*) ≤(1 — ησmin(R)i∑2^(C(Kt) — C(K*)).
Provided
T ≥ INk*||
一ημ2σmin(R)
1 C (Ko) — C (K *)
log---------------
ε
then C(Kτ) 一 C(K*) ≤ ε, and the result follows.
□
D	Analysis: the Model-free case
This section shows how techniques from zeroth order optimization allow the algorithm to run in
the model-free setting with only black-box access to a simulator. The dependencies on various
parameters are not optimized, and the notation h is used to represent different polynomial factors in
the relevant factors (*C(K(Q), kAk, kBk, kRk, 1∕σmin(R)). When the polynomial also depend on
dimension d or accuracy 1/, this is specified as parameters (h(d, 1/)).
The section starts by showing how the infinite horizon can be approximated with a finite horizon.
21
Under review as a conference paper at ICLR 2018
D.1 APPROXIMATING C(K) AND ΣK WITH FINITE HORIZON
This section shows that as long as there is an upper bound on C(K), it is possible to approximate
both C(K) and Σ(K) with any desired accuracy.
Lemma 20. For any K with finite C(K), let ∑K = E[P'-1 xix>] and C(')(K)=
E[P'-1 X>Qxi + u>Rui] = h∑K,Q + K>RKi. If
` ≥ d ∙C 2 (K)
一 卬%in(Q),
then k∑K) — Σκ k ≤ e. Also, if
` ≥ d∙C2(K)(kQk + kRkkKk2)
—	eμσm in(Q)
then C(K) ≥ C⑶(K) ≥ C(K) — e.
Proof. First, the bound on Σκ is proved. Define the operators Tκ and Fκ as in Section C.4, observe
∑κ = Tκ(∑0) and ∑K = ∑κ -(FK)'(∑κ).
If X Y , then Fκ(X)	Fκ(Y ), this follows immediately from the form of Fκ(X) = (A +
BK)X(A + BK)>. IfX is PSD then WXW> is also PSD for any W.
Now, since
`-1	∞
tr(XF'(∑0)) ≤ tr(XF'(∑0))= tr(∑κ) ≤
i=0	i=0
d ∙ C(K)
σmin
(Q).
(Here the last step is by Lemma 10), and all traces are nonnegative, then there must exists j < `
SUChthat tr(Fκ 3o)) ≤ `:minKQ).
Also，since ςκ W μσCκQ) ς0,
tr(Fκj (Σκ)) ≤
C(K)
μσmin (Q)
tr(Fκj (Σ0)) ≤
d ∙ C2(K)
tμσmin (Q)
Therefore as long as
` ≥ dC2 (K)
一卬%in(Qy
it follows that:
k∑κ — ∑κ)k≤k∑κ — ∑κ)k = kFj(∑κ)k≤ e.
Here the first step is again becaUse of all the terms are PSD, so Using more terms is always better.
The last step follows becaUse Fj (Σκ) is also a PSD matrix so the spectral norm is boUnded by
trace. In fact, it holds that tr(∑κ — ∑κ)) is smaller than e.
Next, observe C⑹(K) = h∑κ), Q + K>RKi and C(K) = h∑κ, Q + K>RKi，therefore
C(K) — C(')(K) ≤ tr(∑κ — ∑κ))(kQk + kRkkKk2).
Therefore if
` ≥ d ∙C 2(K)(IIQk + IlRllIlK ∣∣2)
一	”m in(Q)	,
then tr(∑κ 一 ∑κ)) ≤ e∕(kQk + IlRkkKk2) and hence C(K) 一 C⑶(K) ≤ e.
□
22
Under review as a conference paper at ICLR 2018
D.2 PERTURBATION OF C(K) AND VC(K)
The next lemma show that the function value and its gradient are approximate preserved if a small
perturbation to the policy K is applied.
Lemma 21. (CK perturbation) Suppose K0 is such that:
kK0 - Kk ≤ min (4C(K)kB谓A-∖κ∣∣ + 1), kKk)
then:
|C(K0)-C(K)|
≤ 6∣∣KkkRk Ekxok2 ( C(K)ɔ. ) (kKkkBkkA - BKk + 区小网 + 1) kK - K0k
∖小σmin(Q))
Proof. As in the proof of Lemma 16, the assumption implies that kTK kkFK - FK0 k ≤ 1/2, and,
from Equation 15, that
kFK-FK0k ≤2kBk(kA-BKk+1)kK-K0k
First, observe:
C(K0) - C(K) ≤ Tr(Ex0x0>)kTK0(Q+(K0)>RK0) - TK(Q + K>RK)k
= Ekx0k2kTK0(Q + (K0)>RK0) - TK(Q + K>RK)k
Hence,
kTK0(Q+(K0)>RK0)-TK(Q+K>RK)k
≤ kTK0(Q+(K0)>RK0)-TK(Q+(K0)>RK0)
-(TK(Q + K>RK) - TK(Q + (K0)>RK0)) k
= kTK0(Q+(K0)>RK0) -TK (Q + (K0)>RK0) -TK ◦(K>RK- (K0)>RK0)k
≤ 2kTKk2kFK - FK0kk(K0)>RK0)k + kTKkkK>RK - (K0)>RK0)k
≤ 2kTKk2kFκ - Fkok (k(K0)>RK0) - K>RKk + kKTRK)k)
+kTKkkK>RK-(K0)>RK0)k
≤ kTKkk(K0)>RK0) - K>RKk + 2kTKk2kFK - FK0kkK>RKk
+kTKkkK>RK-(K0)>RK0)k
= 2kTKkk(K0)>RK0) -K>RKk+2kTKk2kFK-FK0kkK>RKk
For the first term,
2kTκkk(K0)>RK0) - K>RKk ≤ 2kTκk ⑵KkkRkkK0 - Kk + kRkkK0 - Kk2)
≤2kTKk(3kKkkRkkK0-Kk)
using the assumption that kK0 - K k ≤ kK k. For the second term,
2kTκk2kFκ-Fκ0kkK>RKk ≤ 2kTκk2 2kBk (kA- BKk + 1) kK - K0k kKk2kRk.
Combining the two terms completes the proof.	□
The next lemma shows the gradient is also stable after perturbation.
Lemma 22. (VCκ perturbation) Suppose K0 is such that:
kK0 - K k ≤ min
σmin(Q)小
4C(K)∣∣Bk(kA - BKk + 1)
kKk
then there is a polynomial hgr-θjd in μC(KQ)，kAk, kBk, kRk, σrn J(R) Such that
|VC(K0)-VC(K)| ≤ hgradkK0 - Kk.
23
Under review as a conference paper at ICLR 2018
Proof. Recall VC(K) = 2Ek∑k where EK = (R + BTPKB)K - BTPKA. Therefore
VC (K 0) — VC (K) = 2Eκ0 ∑κo — 2 Ek ∑k = 2(Eκ0 — EK )∑κo + 2Ek (∑κo — ∑k ).
Let’s first look at the second term. By Lemma 8,
Tr(E>Ek) ≤ kR + "Bk(C(K)-C(K*)),
μ
then by Lemma 13
k∑K0 - ∑Kk≤ 4 (F)2 kBk (M - BKk + 1) kK - K0k
∖σmin(Q)7	μ
Therefore the second term is bounded by
8( C(K) Y
∖σmin(Q) J
(IlR + BTPKBil(C(K)- C(K*)))∣∣B∣∣ (kA - BKk + 1)
μ2
kK-K0k.
Now the first term is bounded. Since K0 - K is small enough, k∑κo k ≤ k ∑k k + σC(KQ).
For EK0 - EK, a bound on PK0 - PK is provided. By the previous lemma,
kPK-Pkk = 6 ( ( C(KK) )2 kKk2kRkkBk (kA - BKk + 1)+ (, C(KKC、)冈1倒[kK-K0∣∣.
∖∖μσmin(Q)/	∖μσmin(Q)
Therefore
EK0 -EK = R(K0 -K) + BT(PK0 -PK)A+BT(PK0 -PK)BK0+BTPKB(K0 -K).
Since kK0k ≤ 2kKk, and kKk can be bounded by C(K) (Lemma 19), all the terms can be bounded
by polynomials of related parameters multiplied by ∣∣K - K 0 k.	口
D.3 Smoothing and the gradient descent analysis
This section analyzes the smoothing procedure and completes the proof of gradient descent. Al-
though Gaussian smoothing is more standard, the objective C(K) is not finite for every K, therefore
technically Eu~n(o,σ2i) [C(K + u)] is not well defined. This is avoidable by smoothing in a ball.
Let Sr represent the uniform distribution over the points with norm r (boundary of a sphere), and
Br represent the uniform distribution over all points with norm at most r (the entire sphere). When
applying these sets to matrix a U, the Frobenius norm ball is used. The algorithm performs gradient
descent on the following function
Cr (K )= EU ~Br[C(K + U)].
The next lemma uses the standard technique (e.g. in (Flaxman et al., 2005)) to show that the gradient
of Cr(K) can be estimated just with an oracle for function value.
Lemma 23. VCr (K) = r⅛EU〜s」C(K + U)U].
This is the same as Lemma 2.1 in Flaxman et al. (2005), for completeness the proof is provided
below.
Proof. By Stokes formula,
V / C(K + U)dx = / C(K + u) —U-dx.
δBr	δSr	kU kF
By definition,
C,K_ ʃw C(K + U)dx
r( ) =	Vold(δBr)
24
Under review as a conference paper at ICLR 2018
Also,
U	Rδs C(K + U)dx
EU ~Sr [C(K +U)U]= rEU ~Sr [C(K + U) 7]= r ∙ δSr void_1 (δSrUkF	.
The Lemma follows from combining these equations, and use the fact that
V0ld(δBr)= V0ld-l(δSr) ∙ r.
□
From the lemma above and standard concentration inequalities, it is immediate that it suffices to use
a polynomial number of samples to approximate the gradient.
Lemma 24. Given an , there are fixed polynomials hr (1/), hsample (d, 1/) such that when r ≤
1/hr (1/e), with m ≥ hsampie(d, 1/e) samples of Ui,..., Un 〜Sr, with high probability (at least
1 - (d/)-d) the average
1md
V= — X F C (K + Ui)Ui
m r 2
i=1
is close to VC(K).
Further, if for X ~ D, ∣∣xk ≤ L almost surely, there are polynomials h`,grad(d, 1/e),
hr,trunc(1/), hsample,trunc(d, 1∕f, σ, L2∕μ) SUeh that When m ≥ hsample,trunc(d, 1∕f, LL /μ),
` ≥ h`,grad(d, 1/), let xij, uij (0 ≤ j ≤ `) be a single path sampled using K + Ui, then the average
m	`-1
v = m X r2 X(Xj)>Qxj + (Uj)>Ruj ]Ui
i=1	j=0
is also close to VC(K) with high probability.
Proof. For the first part, the difference is broken into two terms:
ʌ , ʌ ..
V — VC(K) = (VCr(K) — VC(K)) + (V — VCr(K)).
For the first term, choose hr (1∕) = min{1∕r0, 2hgrad∕} (r0 is chosen later). By Lemma 22
when r is smaller than 1∕hr(1∕) = ∕2hgrad, every point u on the sphere have ∣VC(K +
U) - VC(K)∣ ≤ ∕4. Since VCr(K) is the expectation of VC(K + U), by triangle inequal-
ity ∣VCr (K) - VC(K)∣ ≤ ∕2.
The proof also makes sure that r ≤ ro such that for any U 〜Sr, it holds that C (K + U) ≤ 2C (K).
By Lemma 21, 1∕r0 is a polynomial in the relevant factors.
For the second term, by Lemma 23, E[V] = VCr (K), and each individual sample has norm
bounded by 2dC(K)∕r, so by Vector Bernstein’s Inequality, know with m ≥ hsample(d, 1∕) =
Θ (d (dC(K) 卜og d∕c) samples, with high probability (at least 1 一 (d∕e)-d) ∣V 一 E[V]k ≤ 〃2.
Adding these two terms and apply triangle inequality gives the result.
For the second part, the proof breaks it into more terms. Let V0 be equal to * Pm=I r2C(') (K +
Ui)Ui (where C(') is defined as in Lemma 20), then
Σ 一 VC(K) = (Σ 一 Σ0) + (Σ0 一 Σ) + (Σ 一 VC(K)).
The third term is just what was bounded earlier, using hr,trunc(1∕) = hr(2∕) and making sure
hsample,trunc (d, 1∕) ≥ hsample(d, 2∕), this guarantees that it is smaller than ∕2.
For the second term, choose ' ≥ 16d2，V；2Qk(QRkkKk?) =： h`,grad(d, 1∕e). By Lemma 20, for
any K0 with C (K0) ≤ 2C (K), it holds thatmiC (')(K0) 一 C (K 0)∣ ≤ rd. Therefore by triangle
inequality
mm
k m X r2 C(')(K + Ui)Ui- m X r2 C(k + Ui)Uik ≤ e∕4∙
i=1	i=1
25
Under review as a conference paper at ICLR 2018
Finally for the first term it is easy to see that E[V] = V0 where the expectation is taken over the
randomness of the initial states x0. Since ∣∣χ0k ≤ L, (x0)(χ0)> W LE[χ0χ>], as a result the sum
`-1	L2
[三(xj)>Qxj + (Uj)>Ruj] ≤ -μC(K + Ui).
Therefore, V - V0 is again a sum of independent vectors with bounded norm, so by Vector Bern-
stein's inequality, when hsampie,trunc(d, 1/e, L2∕μ) is a large enough polynomial, ∣V7 -V01 ≤ e/4
with high probability. Adding all the terms finishes the proof.	□
Theorem 25. There are fixed polynomials hGD,r (1/e), hGD,sampie(d, 1/e, L2∕μ), hGD,'(d, 1/e)
such that if every step the gradient is computed as Lemma 24 (truncated at step `), pick step size η
and T the same as the gradient descent case of Theorem 5, it holds that C(KT) - C(K?) ≤ with
high probability (at least 1 - exp(-d)).
Proof. By Lemma 18, when η ≤ 1∕hGD,η for some fixed polynomial hGD,η(given in Lemma 18),
then
C (K0) - C (K *) ≤(1 - ησmin(R) i∑μ^k ) (C(K)- C (K *))
Let V be the approximate gradient computed, and let K00 = K - ηV be the iterate that uses the
approximate gradient. The proof shows given enough samples, the gradient can be estimated with
enough accuracy that makes sure
|C (K00) - C (K0 )1 ≤ 1 ησmin(R)君二∙ e.
2	k∑K*k
This means as long as C (K) 一 C (K *) ≥ e, it holds that
C(KC(K *)≤ (ι- 2 …ɪ 卜C(K)- C(K *)).
Then the same proof of Theorem 5 gives the convergence guarantee.
2
Now C (K00) 一 C (K0) is bounded. By Lemma21, if ∣K00 一 K 0∣ ≤ 1 ησmi∩(R) ∣∣∑μ * ∣∣ Y ∙ 1/hfunc
(hfunc is the polynomial in Lemma 21), then C(K00) - C(K0) is small enough. To get that, observe
K00 一 K0 = η(V - V∕), therefore it suffices to make sure
kV - VIl ≤ 2σmin(R)
F
I∣ςk* k
• E ∙ 1∕hfunc
By Lemma 22, it suffices to pick hGD,r(1∕e) = hr,trunc(2hfunck∑κ* k∕(μ2σmin(R)e)),
hGD,sample(d, 1∕f, L ∕μ)
hsample,trunc(d, 2hfunc k ∑κ* k /(μ?σmin (R)E), L^∕μ), and
hGD,'(d, 1∕e) = h`,grad(d, 2hfun∕∣∑κ* k∕M2σmin(R)e)). ThiS gives the desired upper-bound on
∣∣V 一 Vk with high probability (at least 1 一 (e∕d)-d).
Since the number of steps is a polynomial, by union bound with high probability (at least 1 -
T (E∕d)-d ≥ 1 - exp(-d)) the gradient is accurate enough for all the steps, so
c (k K- c (k *) ≤ (1- 2 …3 卜C(K)- c (k *)).
The rest of the proof is the same as Theorem 5. Note that in the smoothing, because the function
value is monotonically decreasing and the choice of radius, all the function value encountered is
bounded by 2C(Ko), so the polynomials are indeed bounded throughout the algorithm.	□
26
Under review as a conference paper at ICLR 2018
D.4 The natural gradient analysis
Before the Theorem for natural gradient is proven, the following lemma shows the variance can be
estimated accurately.
Lemma 26. If for X 〜 D, ∣∣xk ≤ L almost surely, there exists polynomials hr,var (1/e),
hvarsampie,trunc(d, 1/g L2∕μ) and h`,var (d, 1/e) Such that ifΣΣκ is estimated using at least m ≥
hvarsampie,trunc(d, 1/e, L2/μ) initial points x0,..., xm, m random perturbations Ui 〜Sr where
r ≤ 1∕hr,var (1/e), allofthese initial points are SimUlated using Ki = K + Ui to ' ≥ h`,var (d, 1/e)
iterations, then with high probability (at least 1 - (d/)-d) the following estimate
m `-1
ς = 3 XX Xj (Xj)>.
i=1 j=0
satisfies ∣∣Σ 一 ∑k∣∣ ≤ 匕 Further, when E ≤ 仙/2, it holds that σmin(∑κ) ≥ μ12
Proof. This is broken into three terms: let ∑K) be defined as in Lemma 20, let Σ = ml Pm=I ∑k+u
and Σ (') = ml Pm=I ∑K+u ,then it holds that
∑ 一 ∑k = (∑ 一 ∑(')) + (∑⑶ 一 ∑) + (∑ 一 ∑k).
First, r is chosen small enough so that C(K + Ui) ≤ 2C(K). This only requires an inverse polyno-
mial r by Lemma 21.
For the first term, note that E[Σ] = Σ(') where the expectation is taken over the initial points x0.
Since ∣∣χ0k ≤ L, (x0)(x0)> W LE[χoχ>], and as a result the sum
`-1	L2
EXj(Xj)TQ W — Σκ+Ui.
j=0	μ
Therefore, standard concentration bounds show that when hvarsample,trunc is a large enough poly-
nomial, ∣Σ ― Σ(') ∣∣ ≤ e/2 holds with high probability.
For the second term, Lemma 20 is applied. Because C(K + Ui) ≤ 2C(K), choosing ` ≥
h`,var(d, 1/e) = ：dC ((Q)), the error introduced by truncation ∣∣Σ(') 一 Σ∣∣ is then bounded by
E/4.	min
For the third term, Lemma 13 is applied. When r ≤ e ∙ (σ 1CinKQ)) 16kBk(kA-BKk+1), ∣∑κ+Ui 一
∑K∣ ≤ E/4. Since ∑ is the average of ∑K+Ui, by the triangle inequality, ∣∑ 一 ∑K∣ ≤ E/4.
Adding these three terms gives the result.
Finally, the bound on σmιin(∑κ) follows simply from Weyl S Theorem.	□
Theorem 27. Suppose C (Ko) is finite and and μ > 0. The natural gradient follows the update
rule:
Kt+ι = Kt — ηvc (Kt)∑κt
Suppose the stepsize is set to be:
_	1
η = ∣R∣ + kBk2Cg)
If the gradient and variance are estimated as in Lemma 24, Lemma 26 with r = 1/hN GD,r (1/E),
with m ≥ hNGD,sampie(d, 1/e, L1 /μ) samples, both are truncated to hNGD,'(d, 1/e) iterations,
then with high probability (at least 1 — exp(—d)) in T iterations where
∣R∣	+ 1网2。(左))log 2(。匹)一 C(K*))
σmin(R)	μσmin(R) J	ε
then the natural gradient satisfies the following performance bound:
C(KT) 一 C (K *) ≤ ε
τ> INk*II
μ
27
Under review as a conference paper at ICLR 2018
Proof. By Lemma 12,
C(K0) - C(K*) ≤
(1-2)k∑K7k )(C(K)- C(K *))
-1
Let V be the estimated gradient, ∑k be the estimated ∑k, and let K00 = K - ηV∑κ . The proof
shows that when both the gradient and the covariance matrix are estimated accurately enough, then
|C (K 0) - C (K 00) I ≤ fησmin(R) iɪ.
2	k∑K*k
This implies when C(K) - C(K?) ≥ ,
C (K 0) - C (K *) ≤(1 - 1 ησmin(R)后％ ) (C (K) - C (K *))
∖	2	k∑κ* Il J
which is sufficient for the proof.
By Lemma 21, if ∣∣K 00 - K 0∣ ≤ ^h^— ησmin(R) ∣∣∑μ 11 the desired bound on ∣C (K 0) - C (K 00) ∣
2 func	k K* k
holds. To achieve this, it suffices to have
∣VΣK1 - VC(K)∑k1∣ ≤ 2h^σmin(R)-^.
2hf unc	∣∑K * ∣
This is broken into two terms
∣VΣK1 - VC(K)∑k1∣ ≤ IIV - V∣∣ΣK1I + IlVC(K)∣∣Σ£-ΣKf1∣.
For the first term,
1
large enough I∑-K1 I
8hfUnc σmin(R) k∑K*k,
8	8 8hfunc k ςK k \
hr,trunc( μ2σmin (R)e ),
by Lemma 26 we know when the number of samples is
≤ 2∕μ. Therefore it suffices to make sure ∣V - Vk ≤
this can be done by Lemma 24 by setting hNGD,grad,r(1/)
hNGD
,gradsample
(d,1∕e")
and hNGD,',grad(d, 1/e) = h',grad(d, ^fUn⅛K1 ).
hsample”Uncld "μfun⅛κk ,L∕μ2)
-ɪ-1	. 1	μ	J	μ-	.	i	11 vɔ — 1	vɔ — 1 Il / μ	/ ∖	μ
For the second term, it suffices to make sure ∣∣∑k1 - ∑κ1∣ ≤ 4hy— σmin(R) 八夕^* k∣μ7c(κ)∣∣
By standard matrix perturbation, and σmin(∑κ) ≥ μ,
一 ..~ 一 .. ,.. 一 _ 一
(when ∣∣ΣK - Σκ∣∣	≤	μ∕2).	Therefore by
8	∩ 8hh —	8	(8hfunck£K* k∣VC(K)Il、
hNGD,var,r(1/t)	— hvar,r(	μ3σmin(R)e	),
k∑K1 - ∑k1 I ≤ 2∣ΣK - ∑k∣∕“2
Lemma 26 it suffices to choose
hNGD,varsample(d, 1∕f, L∕μ )
hvarsample,trunc(d, 8hfunμ3∑K*n窝C(K)k ,L∕Μ2)	and	hNGD,',var(d,1∕e)	—
h`,var(d, 8hfunμ3∑K*J(R)C(K)k). This is indeed a polynomial because ∣∣VC(K)∣ is bounded
by Lemma 19.
Finally, choose hN GD,r	=	max{hN GD,grad,r , hN GD,var,r }, hN GD,sample	=
max{hNGD,gradsample, hNGD,varsample}, and hNGD,' — max{hNGD,',grad, hNGD,',var }.
This ensures all the bounds mentioned above hold and that
c (k 0)- c (k *) ≤ (1-1 2) & bC(K)- c (k *))
The rest of the proof is the same as Theorem 5. Note again that in the smoothing, because the func-
tion value is monotonically decreasing and the choice of radius, all the function values encountered
are bounded by 2C(Ko), so the polynomials are indeed bounded throughout the algorithm. □
28