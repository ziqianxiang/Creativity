Workshop track - ICLR 2018
Anytime Neural Network: a Versatile Trade-
off Between Computation and Accuracy
Hanzhang Hu, Martial Hebert & J. Andrew Bagnell
Department of Computer Science
Carnegie Mellon University
{hanzhang, hebert, dbagnell}@cs.cmu.edu
Debadeepta Dey
Microsoft Research
dedey@microsoft.com
Ab stract
We present an approach for anytime predictions in deep neural networks (DNNs).
For each test sample, an anytime predictor produces a coarse result quickly, and
then continues to refine it until the test-time computational budget is depleted.
Such predictors can address the growing computational problem of DNNs by
automatically adjusting to varying test-time budgets. In this work, we study a
general augmentation to feed-forward networks to form anytime neural networks
(ANNs) via auxiliary predictions and losses. Specifically, we point out a blind-
spot in recent studies in such ANNs: the importance of high final accuracy. In
fact, we show on multiple recognition data-sets and architectures that by having
near-optimal final predictions in small anytime models, we can effectively double
the speed of large ones to reach corresponding accuracy level. We achieve such
speed-up with simple weighting of anytime losses that oscillate during training.
We also assemble a sequence of exponentially deepening ANNs, to achieve both
theoretically and practically near-optimal anytime results at any budget, at the cost
of a constant fraction of additional consumed budget.
1 Introduction
In recent years, the accuracy in visual recognition tasks has been greatly improved by increasingly
complex convolutional neural networks, from AlexNet (Krizhevsky et al., 2012) and VGG (Si-
monyan & Zisserman, 2015), to ResNet (He et al., 2016), ResNeXt (Xie et al., 2017), and
DenseNet (Huang et al., 2017b). However, the number of applications that require latency sensi-
tive responses is growing rapidly. Furthermore, their test-time computational budget can often. E.g.,
autonomous vehicles require real-time object detection, but the required detection speed depends on
the vehicle speed; web servers need to meet varying amount of data and user requests throughput
through out a day. Thus, it can be difficult for such applications to choose between slow predictors
with high accuracy and fast predictors with low accuracy. In many cases, this dilemma can be re-
solved by an anytime predictor (Horvitz, 1987; Boddy & Dean, 1989; Zilberstein, 1996), which,
for each test sample, produces a fast and crude initial prediction and continues to refine it as budget
allows, so that at any test-time budget, the anytime predictor has a valid result for the sample, and
the more budget is spent, the better the prediction is.
In this work1, we focus on the anytime prediction problem in neural networks. We follow the re-
cent works (Lee et al., 2015; Xie & Tu, 2015; Zamir et al., 2017; Huang et al., 2017a) to append
auxiliary predictions and losses in feed-forward networks for anytime predictions, and train them
jointly end-to-end. However, we note that the existing methods all put only a small fraction of the
total weightings to the final prediction, and as a result, large anytime models are often only as ac-
curate as much smaller non-anytime models, because the accuracy gain is so costly in DNNs, as
demonstrated in Fig. 1a. We address this problem with a novel and simple oscillating weightings
of the losses, and will show in Sec. 3 that our small anytime models with near-optimal final predic-
tions can effectively speed up two times large ones without them, on multiple data-sets, including
ILSVRC (Russakovsky et al., 2015), and on multiple models, including the very recent Multi-Scale-
DenseNets (MSDnets) (Huang et al., 2017a). Observing that the proposed training techniques lead
to ANNs that are near-optimal in late predictions but are not as accurate in the early predictions, we
1For the full paper, see https://arxiv.org/abs/1708.06832
1
Workshop track - ICLR 2018
(c) EANN
(a) Existing (purple) vs. the Proposed (b) Anytime Neural Network (ANN)
(red) ANNs
Figure 1: (a) By ensuring near-optimal final anytime predictions, our ANN (red) achieves the same error rates
2-5 times faster than a large existing one (purple). (b) ANN illustration. (c) EANN illustration.
assemble ANNs of exponentially increasing depths to dedicate early predictions to smaller networks,
while only delaying large networks by a constant fraction of additional test-time budgets.
2 Methods
As illustrated in Fig. 1b, given a sample (x,y) 〜D, the initial feature map x0 is set to x, and
the subsequent feature transformations f1, f2, ..., fL generate a sequence of intermediate features
xi = fi(xi-1; θi) for i ≥ 1 using parameter θi. Each feature map xi can then produce an auxiliary
prediction yi using a prediction layer gi: yi = gi (Xi; Wi) with parameter Wi. Each auxiliary predic-
tion yi then incurs an expected loss 'i := E(x,y)〜D ['(y, yi)]. We call such an augmented network as
an Anytime Neural Network (ANN). Let the parameters of the full ANN be θ = (θ1 , W1 , ..., θL, WL).
The most common way to optimize these losses, '1,…,'L, end-to-end is to optimize them in a
weighted sum minθ PL=I Bi'i(θ), where {Bi}i form the weight scheme for the losses.
Alternating SIEVE weights. Three experimental observations lead to our proposed SIEVE weight
scheme. First, the existing weights, CONST (Lee et al., 2015; Xie & Tu, 2015; Huang et al., 2017a),
and LINEAR (Zamir et al., 2017) both incur more than 10% relative increase in final test errors,
which effectively slow down anytime models multiple times. Second, we found that a large weight
can improve a neighborhood of losses thanks to the high correlation among neighboring losses.
Finally, keeping a fixed weighting may lead to solutions where the sum of the gradients are zero, but
the individual gradients are non-zero.
The proposed SIEVE scheme has half of the total weights in the final loss, so that the final gradient
can outweigh other gradients when all loss gradients have equal two-norms. It also have uneven
weights in early losses to let as many losses to be near large weights as possible. Formally for
L losses, we first add to BbL〔 one unit of weight, where [•] means rounding. We then add one
unit to each B
b kL e
for k = 1, 2, 3, and then to each B
b kL e
for k =
1, 2, ..., 7, and so on, until
all predictors have non-zero weights. We finally normalize Bi so that PiL=-11 Bi = 1, and set
BL = 1. During each training iteration, we also sample proportional to the Bi a layer i, and add
temporarily to the total loss BL'i so as to oscillate the weights to avoid spurious solutions. We call
ANNs with alternating weights as alternating ANNs (AANNs). Though the proposed techinques
are heuristics, they effectively speed up anytime models multiple times as shown in Sec. 3. We hope
our experimental results can inspire, and set baselines for, future principled approaches.
EANN. Since AANNs put high weights in the final layer, they trade early accuracy for the late
ones. We leverage this effect to improving early predictions of large ANNs: we propose to form a
sequence of ANNs whose depths grow exponentially (EANN). By dedicating early predictions to
small networks, EANN can achieve better early results. Furthermore, if the largest model has L
depths, we only compute log L small networks before the final one, and the total cost of the small
networks is only a constant fraction of the final one. Hence, we only consume a constant fraction
of additional test-time budget. Fig 1c shows how an EANN of the exponential base b = 2 works
at test-time. The EANN sequentially computes the ANNs, and only outputs an anytime result if the
current result is better than previous ones in validation. Formally, if we assume that each ANN has
near-optimal results after b of its layers, then we can prove that for any budget B, the EANN can
2
Workshop track - ICLR 2018
0uuθ.!Q!tclφ6eluθu.IQd φ>-⅛-φtf
Relative FLOPS cost to the Small Network
(a) ResNet SIEVE vs. CONST on CIFAR100
424038而343230282624
31eH-0怎 I，dol UH>ST
ɪɪ						→-	SIEVE	MSDNet26	
						→÷-	CONST, MSDNet26 CONST, MSDNet36 -		
						-∏→e-			
						-M-	CONS	,MSDP	Jet4i
									
									
									
									
									
					≡	-1			
					t-				
									C
									
(b) ResNet SIEVE vs. CONST
Budget in FLOPS	比9	Budget in FLOPS	xe9
(C) MSDNet SIEVE vs. CONST	(d) CIFAR100 EANNs vs.OPT
Figure 2: (a,b) On CIFAR100 and ILSVRC, ResNets using SIEVE achieves the same accuracy as those of
twiCe the depths but using CONST, so that the small nets (blue) effeCtively speed up the large nets (orange).
(c) Similar effeCts on ILSVRC with MSDNets: the small MSDNet26 with SIEVE (blue) is a better anytime
prediCtor than MSDNet36 and MSDNet41 with CONST (green and red). (d) Using EANN, we greatly reduCes
the early error rates, at the Cost of aChieving the final prediCtions later.
aChieve near-optimal prediCtions for budget B after spending C × B total budgets. Furthermoe, for
large b, EB^uniform(1,L) [C] ≤ 1 - 2b + 1+5" → 1, and SUPB C =2 + b-ɪ → 2.
3 Key Experiments
We present two key results: (1) small anytime models with SIEVE Can outperform large ones with
CONST, and (2) EANNs Can improve early aCCuraCy, but Cost a Constant fraCtion of extra budgets.
SIEVE vs. CONST of double costs. In Fig. 2a and Fig. 2b, we Compare SIEVE and CONST
on ANNs that are based on ResNets on CIFAR100 (Krizhevsky, 2009) and ILSVRC (Russakovsky
et al., 2015). The networks with CONST have double the depths as those with SIEVE. We observe
that SIEVE leads to the same final error rates as CONST of double the Costs, but does so muCh
faster. The two sChemes also have similar early performanCe. HenCe, SIEVE effeCtively speed up
the prediCtions of CONST by about two times. In Fig. 2C, we experiment with the very reCent
Multi-SCale-DenseNets (MSDNets) (Huang et al., 2017a), whiCh are speCifiCally modified from the
reCently popular DenseNets (Huang et al., 2017b) to produCe the state-of-the-art anytime prediC-
tions. We again observe that by improving the final anytime prediCtion of the smallest MSDNet26
without saCrifiCing too muCh early prediCtions, we make MSDNet26 effeCtively a sped-up version
of MSDNet36 and MSDNet41.
EANN vs. ANNs and OPT. In Fig. 2d, we assemble ResNet-ANNs of 45, 81 and 153 Conv layers
to form EANNs. We Compare the EANNs against the parallel OPT, whiCh is from running regular
networks of various depths in parallel. We observe that EANNs are able to signifiCantly reduCe the
early errors of ANNs, but reaCh the final error rate later. Furthermore, ANNs with more aCCurate
final prediCtions using SIEVE and EXP-LIN2 are able to outperform CONST and LINEAR, sinCe
whenever an ANN Completes in an EANN, the final result is the best one for a long period of time.
2EXP-LIN is another proposed sCheme that foCuses more on the final loss. See the full paper for details.
3
Workshop track - ICLR 2018
References
Mark Boddy and Thomas Dean. Solving time-dependent planning problems. In Proceedings of the
11th International Joint Conference OnArtificial Intelligence - Volume 2, IJCAr89,pp. 979-984,
1989.
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama. Adaptive neural networks
for fast test-time prediction. In ICML, 2017.
Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. In ICML, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Computer
Vision and Pattern Recognition (CVPR), 2016.
Eric J. Horvitz. Reasoning about beliefs and actions under computational resource constraints. In
Proceedings of the Third Conference on Uncertainty in Artificial Intelligence, UAI’87, pp. 429-
447, 1987.
G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and K. Q. Weinberger. Multi-scale dense
convolutional networks for efficient prediction. In arxiv preprint: 1703.09844, 2017a.
Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2017b.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems 25, pp. 1097-
1105, 2012.
G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without resid-
uals. In International Conference on Learning Representations (ICLR), 2017.
Chen-Yu Lee, Saining Xie, Patrick W. Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-
supervised nets. In AISTATS, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.
Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In ICCV, 2015.
Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017.
Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik, and Silvio Savarese. Feedback
networks. In Computer Vision and Pattern Recognition (CVPR), 2017.
Shlomo Zilberstein. Using anytime algorithms in intelligent systems. AI Magazine, 17(3):73-83,
1996.
4