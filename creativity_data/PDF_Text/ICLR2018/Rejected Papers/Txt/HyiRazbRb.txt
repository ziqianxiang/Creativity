Under review as a conference paper at ICLR 2018
Demystifying overcomplete nonlinear auto-
encoders: fast SGD convergence towards
sparse representation from random initializa-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Auto-encoders are commonly used for unsupervised representation learning and
for pre-training deeper neural networks. When its activation function is linear
and the encoding dimension (width of hidden layer) is smaller than the input di-
mension, it is well known that auto-encoder is optimized to learn the principal
components of the data distribution Oja (1982). However, when the activation is
nonlinear and when the width is larger than the input dimension (overcomplete),
auto-encoder behaves differently from PCA, and in fact is known to perform well
empirically for sparse coding problems.
We provide a theoretical explanation for this empirically observed phenomenon,
when rectified-linear unit (ReLu) is adopted as the activation function and the
hidden-layer width is set to be large. In this case, we show that, with signifi-
cant probability, initializing the weight matrix of an auto-encoder by sampling
from a spherical Gaussian distribution followed by stochastic gradient descent
(SGD) training converges towards the ground-truth representation for a class of
sparse dictionary learning models. In addition, we can show that, conditioning
on convergence, the expected convergence rate is O(t), where t is the number
of updates. Our analysis quantifies how increasing hidden layer width helps the
training performance when random initialization is used, and how the norm of
network weights influence the speed of SGD convergence.
1	Introduction
Let x denote a vector in Rd. An auto-encoder can be decomposed into two parts, encoder and
decoder. The encoder can be viewed as a composition function se ◦ ae : Rd → Rn ; function
ae : Rd → Rn is defined as
ae(x) := Wex+be with We ∈ Rn×d, be ∈ Rn
We and be are the network weights and bias associated with the encoder. se is a coordinate-wise
activation function defined as
se(y)j := s(yj) where s : R → R is typically a nonlinear function
The decoder takes the output of encoder and maps it back to Rd. Let xe := se(ae(x)). The decoding
function, which We denote as X, is defined as
X(Xe)= Sd(Wdxe + bd) With Wd ∈ Rd×n, bd ∈ Rd, Sd： Rd → Rd
where (Wd, bd) and sd are the network parameters and the activation function associated with the
decoder respectively.
Suppose the activation functions are fixed before training. One can view X as a reconstruction of the
original signal/data using the hidden representation parameterized by (We, be) and (Wd, bd). The
goal of training an auto-encoder is to learn the “right” network parameters, (We, be, Wd, bd), so that
X has low reconstruction error.
1
Under review as a conference paper at ICLR 2018
Weight tying A folklore knowledge when training auto-encoders is that, it usually works better if
one sets Wd = WeT. This trick is called “weight tying”, which is viewed as a trick of regularization,
since it reduces the total number of free parameters. With tied weights, the classical auto-encoder is
simplified as
X(Se(ae(x))) = Sd(W T Se(Wx + b) + bd)
In the rest of the manuscript, we focus on weight-tied auto-encoder with the following specific
architecture:
xw,b(x) = WTSReLu(a(x)) = WTSReLu(Wx + b) With SReLu(y)i ：= max{0, yi}	(1)
Here We abuse notation to use xw,b to denote the encoder-decoder function parametrized by weights
W and bias b. In the deep learning community, SReLu is commonly referred to as the rectified-linear
(ReLu) activation.
Reconstruction error A classic measure of reconstruction error used by auto-encoders is the ex-
pected squared loss. Assuming that the data fed to the auto-encoder is i.i.d distributed according to
an unknown distribution, i.e., x 〜p(x), the population expected squared loss is defined as
L(W, b) ：= 2Ex〜p(x) kx — xw,b(x) k2	(2)
Learning a “good representation” thus translates to adjusting the parameters (W, b) to minimize the
squared loss function. The implicit hope is that the squared loss will provide information about what
is a good representation. In other words, we have a certain level of belief that the squared loss char-
acterizes what kind of network parameters are close to the parameters of the latent distribution p(x).
This unwarranted belief leads to two natural questions that motivated our theoretical investigation:
•	Does the global minimum (or any of global minima, if more than one) of L(W, b) corre-
spond to the latent model parameters of distribution p(x)?
•	From an optimization perspective, since L(W, b) is non-convex in W and is shown to have
exponentially many local minima Safran & Shamir (2016), one would expect a local al-
gorithm like stochastic gradient descent, which is the go-to algorithm in practice for op-
timizing L(W, b), to be stuck in local minima and only find sub-optimal solutions. Then
how should we explain the practical observation that auto-encoders trained with SGD often
yield good representation?
Stochastic-gradient based training Stochastic gradient descent (SGD) is a scalable variant of
gradient descent commonly used in deep learning. At every time step t, the algorithm evaluates a
stochastic gradient g(∙) of the population loss function with respect to the network parameters using
back propagation by sampling one or a mini-batch of data points. The weight and bias update has
the following generic form
Wt+1 — Wt - nWgt+1(Wt), with Eg(Wt) = dLW b) (Wt)
bt+1 - bt - ntgt+1 (bt) with Eg(btt) = dL(W,b)(bt)
where ηwt and ηbt are the learning rates for updating W and b respectively, typically set to be a small
number or a decaying function of time t. The unbiased gradient estimate g(W t) and g(bt) can be
obtained by differentiating the empirical loss function defined on a single or a mini-batch of size m,
denoted by `m :
1m	1
'm(W,b) ：= — £'(W,b； xi), where '(W,b; x) := ~2 ∣∣x - xw,b(x)∣∣2	(3)
m i=1
Then the stochastic or mini-batch gradient descent update can be written as
Wt+1 - Wt - nW d'm(W,b)(Wt) and bt+1 - bt - nt d'm∂W,b)(bt)	(4)
2
Under review as a conference paper at ICLR 2018
Table 1: Organization of notations: the “parameters” are those whose value determine the perfor-
mance guarantee of auto-encoders; the “auxiliary" variables are only used to facilitate our analysis.
Model		Algorithm		Analysis
parameters	auxiliary	parameters	auxiliary	auxiliary
k(dictionary size) d (dimension) λ (incoherence) e, σ (noise)	X W * Cj	C (norm control) C (learning rate to parameters) n (width of hidden layer)	Wt bt at(∙)	^δ Ts,1, Ts,2 g(∙)
Max-norm regularization A common trick called “max-norm regularization” Srivastava et al.
(2014) or “weight clipping” is used in training deep neural networks. 1 In particular, after each step
of stochastic gradient descent, the updated weights is forced to satisfy
max kWi,?k2 ≤ c
i
for some constant c. This means the row norm of the weights can never exceed the prefixed constant
c. In practice, whenever kWi,?k2 > c, the max-norm constraint is enforced by projecting the weights
back to a ball of radius c.
2	Preliminaries
In this section, we start by defining notations. Then we introduce a norm-controlled variant of SGD
algorithm that operates on the auto-encoder architecture formalized in (1). Finally, we introduce
assumptions on the data generating model.
General principle of notations We use the same notation for network parameters W, b, and for
activation a(∙), as in Section 1. We use s(∙) as a shorthand for the ReLu activation function SReLu(∙)
We use capital letters, such as W or F, either to denote a matrix or an event; we use lower case
letters, such as x, for vectors. WT denotes the transpose of W . We use Ws,? to denote the s-th
row of W. When a matrix W is modified through time, we let Wt denote the state of the matrix at
time t, and Wtt, ? for the state of the corresponding row. We use k ∙ k for l2-norm of vectors and | ∙ |
for absolute value of real numbers. Matrix-vector multiplication between W and x (assuming their
dimensions match) is denoted by Wx. Inner product of vectors x and y is denoted by hx, yi.
Organization of notations Throughout the manuscript, we introduce notations that can be divided
into “model”, “algorithm”, and “analysis” categories according to their utility. They are organized
in Table 1 to help readers interpreting our results. For example, If a reader is interested in knowing
how to apply our result to parameter tuning in training auto-encoders, then she might ignore the
auxiliary notations and only refer to algorithmic parameters and model parameters in Table 1, and
examine how does the setting of the former is influenced by the latter in Theorem 1.
2.1	Norm-controlled SGD training
We assume that the algorithm has access to i.i.d. samples from an unknown distribution p(x).
This means the algorithm can access stochastic gradients of the population squared-loss objective
in (2) via random samples from p(x). The norm-controlled SGD variant we analyze is presented in
Algorithm 1 (it can be easily extended to the mini-batch SGD version, where for each update we
sample more than one data points). It is almost the same as what is commonly used in practice:
it random initializes the weight matrix by sampling unit spherical Gaussian, and at every step the
algorithm moves towards the direction of the negative stochastic gradient with a linearly decaying
learning rate.
However, there are two differences between Algorithm 1 and original SGD: first, we impose that the
norm of the rows ofWt be controlled; this is akin to the practical trick of “max-norm regularization”
1 The name max-norm regularization was originally proposed as a technique for low-rank matrix factoriza-
tion Srebro & Shraibman (2005), where the definition is in fact not exactly the same as what is practiced in the
deep learning community. We use the latter convention in our analysis.
3
Under review as a conference paper at ICLR 2018
as explained in Section 1; second the update of bias is chosen differently than what is usually done
in practice, which deserves additional explanation.
Comment on the setting of bias in Algorithm 1 The stochastic gradient of bias b with respect
to squared loss in (2) can be evaluated by sampling a single data point and differentiate against the
empirical loss in (3), can be derived as
∂'(W, b; x)	∂s(aj)
----------=-----hr, Wj?)(derivation can be found in (6) of the Appendix)
∂bj	∂aj
Since the gradient is noisy, the generic form of SGD suggests modifying btj using the update
bj+1 J btj+ ηt ^dj hr，WG
for a small learning rate ηbt to mitigate noise. This amounts to stepping towards the negative gradient
direction and move a little. On the other hand, we can directly find the next update btj+1 as the point
that sets the gradient to zero, that is, we find bj* such that
∂'(Wt,bt; x0)
∂b
The closed form solution to this is to choose
(bj)=0
b* = hx01{at(x0)>0}, Wt?i( ∣∣wt 炉-I)
This strategy, which is essentially Newton’s algorithm, should perform better than gradient descent
if we have an accurate estimate of the true gradient, so it would likely benefit from evaluating the
gradient using a mini-batch of data. If, on the other hand, the gradient is very noisy, then this
method will likely not work as well as the original SGD update. Analyzing the evolvement of both
Wt and bt, which has dependent stochastic dynamic if we follow the original SGD update, would
be a daunting task. Thus, to simplify our analysis, we assume in our analysis that we have access to
Exhx01{at(χ0)>0}, W∙t+1i( kWt+1∣∣2 - I)
The substitute of Wjt?+1 for Wjt? is to further simplify our analysis. In practice, this update can be
implemented by first updating Wjt? to Wjt?+1 , and then updating btj using Wjt?+1 .
2.2	A simple sparse dictionary learning model
We assume that the data x we sample follows the dictionary learning model
X = (W *)T S + E
where W * ∈ Rk×d. Here k is the size of the dictionary, which We assume to be at least two
(otherwise, the model becomes degenerate), and the true value of k is unknown to the algorithm.
The rows of W * are the dictionary items; Wj*? satisfies
kWj*?k =1,∀j∈ [k]
Let the incoherence between dictionary items be defined as λ := maxj,i6=j,i,j∈[k] |hWj*?, Wi*?i|, we
assume that λ ≤ *.In our simplified model, the coefficient vector S ∈ {0,1}k is assumed to be
1-sparse, with
Pr(Sj = 1) = k
k
Ee = 0 and E[εεt] = σ2I with σ2 ≤ —^^.
一 2√2d
Finally, we assume that the noise has bounded norm2: max ∣∣6k ≤ VI-λ2.
2This assumption can be easily relaxed to a probabilistic bound, e.g., by assuming the noise has sub-gaussian
tails. We stick with this stronger assumption for simplicity
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Norm-controlled SGD training
Input: width parameter n; norm parameter c; learning rate parameters c0 , to , δ; total number of
iterations, tmax .
Initialization of W o: For all s ∈ [n],
Wo? J Ckzk, where Z ∈ Rd, Zi 〜N(0,1)
Initialization of bo: Sample X 〜p(x); for all S ∈ [n],
Find K such that d'(x∂W°,0) (K) = 0
bθ J b	1
(version used in analysis: bθ J Eχhχ, Wwi(c⅛ - 1))
while t ≤ tmax do
Wt+1 J Wt — ηtd'(XdW ,b ), where X 〜p(x)
W t+1 J Wjt+1
j? J kWjt+1k
Draw a fresh sample X0 〜 p; for all s ∈ [n],
Find bS SUCh that 加("北t,bt) (K) = 0
bt+1 J % or equivalently, bt+1 J hx0l{at3)>0}, Wf+1i(c12 — 1)
(version used in analysis bt+1 J Exhx'1{at^χ0)>0}, Wt+1i(c12 - 1))
end while
Output: W tmax , btmax
While auto-encoders are often related to PCA, the latter cannot reveal any information about the true
dictionary under this model even in the complete case, where d = k, due to the isotropic property
of the underlying distribution.
The data generating model can be equivalently viewed as a mixture model: for example, when
Sj = 1, it means X is of the form Wj? + e. When E is Gaussian, the model coincides with mixture of
Gaussians model, with the dictionary items being the latent locations of individual Gaussians. Thus,
we adopt the concept from mixture models, and use X 〜 Cj to indicate that X is generated from the
j -th component of the distribution.
3 Main results
To formally study the convergence property of Algorithm 1, we need a measure to gauge the distance
between the learned representation at time t, Wt, and the ground-truth representation, W*, which
may have different number of rows. There are potentially different ways to go about this. The
distance measure we use is
Θ(Wt, W * )：= k X min ∆(WS?, Wj?) with ∆(WS?, Wj?) :=1 - (h
kj∈[k]s∈[n]
Wst?
kWS*k,
Wj?i)2
Note that ∆(Wst?, Wj*?) is the squared sine of the angle between the two vectors, which decreases
monotonically as their angle decreases, and equals zero if and only if the vectors align. Thus,
mins∈[n] ∆(Wst?, Wj*?) can be viewed as the angular distance from the best approximation in the
learned hidden representations of the network, to the ground-truth dictionary item W*?. And Θ(∙, ∙)
measures this distance averaged over all dictionary items.
Our main result provides recovery and speed guarantee of Algorithm 1 under our data model.
Theorem 1. Suppose we have access to i.i.d. samples X 〜 p(X), where the distribution p(X)
satisfies our model assumption in Section 2.2. Fix any δ ∈ (0, Te). Ifwe train auto-encoder with
norm-controlled SGD as described in Algorithm 1, with the following parameter setting
•	The row norm of weights set to be k Wf*k = c (∀s ∈ [n],∀t) such that 2 ≤ C ≤ √6k
•	If the bias update at t is chosen such that
bS+1 = EX hx'1{at(x0)>0}, Ws+1 i( C - 1)
5
Under review as a conference paper at ICLR 2018
•	The learning rate ofSGD is set to be ηt := +-, with C > 2kc and to ≥ 192(；? 4 * * * B (ln n)2
Then Algorithm 1 has the following guarantees
•	When random initialization with i.i.d. samples from N(0, 1) is used, the algorithm will
be initialized successfully (see definition of successful initialization in Definition 1) with
probability at least 1 一 k exp{-n(含)d-3}.
•	When random initialization with i.i.d. samples X 〜p(x) is used, the algorithm will be
initialized successfully with probability at least
nλ2	n3
(1 - k exp{- 8kB})(1 - 3exp{-1≡2})
• Conditioning on successful initialization, let Ω denote the sample space of all realizations
of the algorithm’s stochastic output, (W1, W2, . . . , ). Then at any time t, there exists a
large subset ofthe sample space, Ft ⊂ Ω, with Pr(Ft) ≥ 1 — δ, such that
E[Θ(Wt,W*)|Ft] ≤ ( to + 1 )4λ2 + (c)B(1 + ^^)¾0+1 ——1——
L V ,	“,一〈to +1 + 1， 2	3	〈 to + 1，	to +1 +1
Interpretation The first statement of the theorem suggests that the probability of successful initial-
ization increases as the width of hidden layer increases. In particular, when Gaussian initialization
is used, in order to ensure a significantly large probability of successful initialization, the analysis
suggests that the number of neurons required must scale as Ω(λ-d) = Ω(kd), which is exponential
in the ambient dimension. When the neurons are initialized with samples from the unknown distri-
bution, the analysis suggests that the number of neurons required scale as Ω(若)=Ω(k3), which
is polynomial in the number of dictionary size. Hence, our analysis suggests that, at least under
our specific model, initializing with data is perhaps a better option than Gaussian initialization. The
second statement suggests that conditioning on a successful initialization, the algorithm will have
expected convergence towards W*, measured by Θ(∙, ∙), of order O(ɪ). IfWe examine of form of
bound on the convergence rate, we see that the rate will be dominated by the second term, whose
constant is heavily influenced by the choice of learning rate parameter c0.
Explaining distributed sparse representation via gradient-based training The main advantage
of gradient-based training of auto-encoders, as revealed by our analysis, is that it simultaneously
updates all its neurons in parallel, in an independent fashion. During training, a subset of neurons
will specialize at learning a single dictionary item: some of them will be successful while others
may fail to converge to a ground-truth representation. However, since the update of each neuron
is independent (in an algorithmic sense), when larger number of neurons are used (widening the
hidden layer), it becomes more likely that each ground-truth dictionary will be learned by some
neuron, even from random initialization.
4 Related works
Despite the simplicity of auto-encoders in comparison to other deep architectures, we still have
a very limited theoretical understanding of them. For linear auto-encoders whose width n is less
than than its input dimension d, the seminal work of Oja (1982) revealed their connection to online
stochastic PCA. For non-linear auto-encoders, recent work Arpit et al. (2016) analyzed sufficient
conditions on the activation functions and the regularization term (which is added to the loss func-
tion) under which the auto-encoder learns a sparse representation. Another work Rangamani et al.
(2017) showed that under a class of sparse dictionary learning model (which is more general than
ours) the ground-truth dictionary is a critical point (that is, either a saddle point or a local miminum)
of the squared loss function, when ReLu activation is used. We are not aware of previous work
providing global convergence guarantee of SGD for non-linear auto-encoders, but our analysis tech-
niques are closely related to recent works Balsubramani et al. (2013); Ge et al. (2015); Tang &
Monteleoni (2017) that are at the intersection of stochastic (non-convex) optimization and unsuper-
vised learning.
6
Under review as a conference paper at ICLR 2018
PCA, k-means, and sparse coding The work of Balsubramani et al. (2013) provided the first
convergence rate analysis of Oja’s and Krasulina’s update rule for online learning the principal com-
ponent (stochastic 1-PCA) of a data distribution. The neural network corresponding to 1-PCA has
a single node in the hidden layer without activation function. We argue that a ReLu activated width
n auto-encoder can be viewed as a generalized, multi-modal version of 1-PCA. This is supported
by our analysis: the expected improvement of each neuron, Wst, bears a striking similarity to that
obtained in Balsubramani et al. (2013). The training of auto-encoders also has a similar flavor to
online/stochastic k-means algorithm Tang & Monteleoni (2017): we may view each neuron as try-
ing to learn a hidden dictionary item, or cluster center in k-means terminology. However, there is a
key difference between k-means and auto-encoders: the performance of k-means is highly sensitive
to the number of clusters. If we specify the number of clusters, which corresponds to the network
width n in our notation, to be larger than the true k, then running n-means will over-partition data
from each component, and each learned center will not converge to the true component center (be-
cause they converge to the mean of the sub-component). For auto-encoders, however, even when
n is much larger than k, the individual neurons can still converge to the true cluster center (dictio-
nary item) thanks to the independent update of neurons. SGD training of auto-encoders is perhaps
closest to a family of sparse coding algorithms Schnass (2015); Arora et al. (2015). For the latter,
however, a critical hyper-parameter to tune is the threshold at which the algorithm decides to cut off
insignificant signals. Existing guarantees for sparse coding algorithms therefore depend on knowing
this threshold. For ReLu activated auto-encoders, the threshold is adaptively set for each neuron s at
every iteration as -bts via gradient descent. Thus, they can be viewed as a sparse coding algorithm
that self-tunes its threshold parameter.
5 Analysis
In our analysis, we define an auxiliary variable
Φ(wS*,W* ):=1- ∆(wS*,Wj*)
Note that φ(∙, ∙) is the squared cosine of the angle between Wt? and Wj∖ which increases as their
angle decreases. Thus, φ can be thought as as measuring the angular “closeness” between two
vectors; it is always bounded between zero and one and equals one if and only if the two vectors
align.
Our analysis can be divided into three steps. We first define what kind of initialization enables
SGD to converge quickly to the correct solution, and show that when the number of nodes in the
hidden layer is large, random initialization will satisfy this sufficient condition. Then we derive
expected the per-iteration improvement of SGD, conditioning on the algorithm’s iterates staying
in a local neighborhood (Definition 4). Finally, we use martingale analysis to show that the local
neighborhood condition will be satisfied with high probability. Piecing these elements together will
lead us to the proof of Theorem 1, which is in the Appendix.
5.1	Part I: Performance guarantee of initialization
Covering guarantee from random initialization Intuitively, for each ground-truth dictionary
item, we only require that at least one neuron is initialized to be not too far from it.
Definition 1. If the rows of Wo have fixed norm c > 0. Then we define the event of successful
initialization as	_______
Fo := m min maχhWo?, Wj?i ≥ c √ι-lτ(
j∈[k] i∈[n]	2
Lemma 1 (Random initialization with Gaussian variables). Suppose Wo ∈ Rn×d is constructed by
drawing z%,j 〜N(0,1) for all i ∈ [n],j ∈ [d], and setting Wo,? = CkZi?k. Then
Pr{Fo}≥ 1 — k exp{-n( 左)/3}
Lemma 2 (Random initialization with data points). Suppose Wo ∈ Rn×d is constructed by drawing
Xι,...,Xn from the data distribution p(x), and setting Woj = C kX^ ,forall i ∈ [n]. If σ2 ≤ 2√d,
7
Under review as a conference paper at ICLR 2018
g
Figure 1: The auto-encoder in this example has 5 neurons in the hidden layer and the dictionary
has two items; in this case, g(1) = g(3) = 1, g(5) = 2, and the other two neurons do not learn any
ground-truth (neurons mapped to 0 are considered useless). Under unique firing condition, which
holds when the dictionary is sufficiently incoherent, the red dashed connection will not take place
(each neuron is learning at most one dictionary item).
then
nλ2	n3
Pr{Fo} ≥ (1 - kexp{-8kB})(1- 3exp{-≡2})
Definition 2. Conditioning on Fo, we can map the rows of Wo to an dictionary item Wj?, j ∈ [k],
according to the following firing map 3
g∙[n]	→{0 1 k}	St	W(S)= j if hWO*,Wj*i≥	C √1->
g: [n]→	{0,1,...,k}	s.t. g(s)=0 otherwise
Figure 1 provides an illustration of firing map g(∙). Note that some rows in Wo may not be mapped
to any dictionary item, in which case we let g(s) = 0. This means such neurons are not close (in
angular distance) to any ground-truth after random initialization. Also note that for some rows Wso?,
there might exist multiple j ∈ [k] such that g(s) = j according to our criterion in the definition. But
when λ ≤ 2, which is always the case by our model assumption on incoherence, Lemma 3 shows
that the assignment must be unique, in which case the mapping is well defined.
Lemma 3 (Uniqueness of firing). Suppose during training, the weight matrix has a fixed norm c. At
t	Wt
time t ,for any row Ofweight matrix Wt?, we denote by Ts,ι ：= max j h -Cs?, W?, and we denote by
τs,2 ：= maxj∈[k],j=ι KW?,Wj*?i|. Thenforany λ ≤ 1, Ts,ι ≥ √1 - λ2 =⇒ {τs,2 < Ts,ι}∙
Thus, for any s ∈ [n] with g(s) > 0, the uniqueness of firing condition holds and the mapping g is
defined unambiguously. So we simplify notations on measure of distance and closeness as
△s =∆(WS*,Wg(S))
Φs ：= Φ(wS*,w;(S))
5.2	Part II: the evolvement of weights and bias during SGD training
This section lower bounds the expected increase of φts after each SGD update, conditioning on Ft .
We first show that conditioning on Ft, the firing of a neuron s with g(s) = j, will indicate that the
data indeed comes from the j -th component, which is characterized by event Et .
Definition 3. At step t, we denote the event of correct firing of Wt as
Et := {∀0 ≤ i ≤ t, ∀s s.t. X 〜Cg(S),优?,/〉+ bs > 0}
∩{∀0 ≤ i ≤ t, ∀s s.t. g(s) > 0 and X 〜Cj, j = g(s),〈W；?, Xi + IbS < 0}
3 Note that the firing map g is only defined for the sake of analysis; the algorithm does not have access to
this information.
8
Under review as a conference paper at ICLR 2018
Definition 4. At step t, we denote the event of satisfying local condition of Wt as
Ft ：= {∀0 ≤ i ≤ t, ∀s ∈	[n] s.t	g(s)	>	O,	hWsi?, Wg(s)?i ≥ c√1 — ʌ2}
Lemma 4 (Correctness of firing).	If at t	≥ 0,	if	∀j	∈ [k], the network parameters	(Wst?, bts )	is
chosen that satisfies
bs = EhX1{aS-1>0}, Ws?i( kWt ∣∣2 — I)
with k Wt?k = C s.t. such that 3 ≤ c ≤ √6k. Thenforany t > 0, Ft =⇒ Et.
Then we proceed to characterize the expected change of φts , conditioning on Et .
Theorem 2.	Suppose Et holds, then after one step of stochastic gradient descent update on Wt,
Wt+1 satisfies
version 1
E[ΦS+1∣Et] ≥ ΦS{1 + k⅛o(1 — ΦS)}- (ηt)2B
kkWs? k
version 2
φts+1 ≥ φts{1 + kkwtq(1 — φts)}- ηtZ — (ηt)2B With E[Z∣Et] = O and |Z| ≤ B
for some constant B > 0 where B is a constant depending on the model parameter kk and the
norm of rows of weight matrix.
5.3 Part III: convergence of martingales
By Theorem 2, the sequence φos , φs1, . . . , φts , . . . is a sub-martingale. One caveat of is that the ex-
Pected increase of the cosine of angle between Wt? and W⑶？ is conditional on Et, the correct
firing condition. So showing that the correct firing event indeed holds is crucial to our overall con-
vergence analysis. Since by Lemma 4, Ft =⇒ Et, it suffices to show that Ft holds. To this end,
note that Ft ’s form a nested sequence
Fo ⊃ F1 ⊃ . . . Ft ⊃ . . .
We denote the limit of this sequence as
F∞ := lim Ft
t→∞
So F∞ is the event that
{hWf?, WD ≥ c(1 — δo),∀t ≥ 0,∀j ∈ [k]∀s ∈ [n] s.t. g(s)=j}
Theorem 3 shows that Pr(F∞) is in fact arbitrarily close to one, conditioning on Fo. We note
that there is a line of recent work that analyze the convergence of SGD on non-convex functions
Balsubramani et al. (2013); Ge et al. (2015); Tang & Monteleoni (2017), where similar technical
difficulty arise: to show local imProvement of the algorithm on a non-convex functions, one usually
needs to lower bound the Probability of the algorithm entering a “bad” region, which can be saddle
Points Ge et al. (2015); Balsubramani et al. (2013) or the Part of solution sPace outside of a local
neighborhood Tang & Monteleoni (2017). Some variant of martingale concentration is usually used
for obtaining such result. Here, since event Ft can be equivalently interPreted as Wst? remains within
the local neighborhood of W⑶？ defined as {y : hy, Wj?i ≥ c(1 一 δ0)}, We employ a technique
similar to that in Tang & Monteleoni (2017) to show that Ft holds with high Probability for all t.
Theorem 3.	Fix any δ ∈ (0, n). Suppose we choose ηt = t+c^ such that
c0 >2kc
to ≥ 192(c0)2B2(ln δ )2(1 + (α⅛ )2
Then conditioning on Fo, we have
Pr(F∞) = 1 — δ
9
Under review as a conference paper at ICLR 2018
6 Open problems
There are several interesting questions that are not addressed here. First, as noted in our discussion in
Section 2, the update of bias as analyzed in our algorithm is not exactly what is used in original SGD.
It would be interesting (and difficult) to explore whether the algorithm has fast convergence when bt
is updated by SGD with a decaying learning rate. Second, our model assumption is rather strong, and
it would be interesting to see whether similar results hold on a relaxed model, for example, where
one may relax to 1-sparse constraint to m-sparse, or one may relax the finite bound requirement on
the noise structure. Third, our performance guarantee of random initialization depends on a lower
bound on the surface area of spherical caps. Improving this bound can improve the tightness of our
initialization guarantee. Finally, it would be very interesting to examine whether similar result holds
for activation functions other than ReLu, such as sigmoid function.
References
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms
for sparse coding. CoRR, abs/1503.00778, 2015. URL http://arxiv.org/abs/1503.
00778.
Devansh Arpit, Yingbo Zhou, Hung Q. Ngo, and Venu Govindaraju. Why regularized auto-encoders
learn sparse representation? In Proceedings of the 33rd International Conference on International
Conference on Machine Learning - Volume 48, ICML’16, pp. 136-144. JMLR.org, 2016. URL
http://dl.acm.org/citation.cfm?id=3045390.3045406.
Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund. The fast convergence of incremental
PCA. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Proceedings ofa meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pp. 3174-3182, 2013. URL http://papers.nips.cc/
paper/5132- the- fast- convergence- of- incremental- pca.
Luc Devroye. The equivalence of weak, strong and complete convergence in l1 for kernel density
estimates. Ann. Statist., 11(3):896-904, 09 1983. doi: 10.1214/aos/1176346255. URL http:
//dx.doi.org/10.1214/aos/1176346255.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
COLT 2015, Paris, France, July 3-6, 2015, pp. 797-842, 2015. URL http://jmlr.org/
proceedings/papers/v40/Ge15.html.
Daniele Micciancio and Panagiotis Voulgaris. Faster exponential time algorithms for the shortest
vector problem. In Proceedings of the Twenty-first Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ’10, pp. 1468-1480, Philadelphia, PA, USA, 2010. Society for Industrial and
Applied Mathematics. ISBN 978-0-898716-98-6. URL http://dl.acm.org/citation.
cfm?id=1873601.1873720.
Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical
Biology, 15(3):267-273, Nov 1982. ISSN 1432-1416. doi: 10.1007/BF00275687. URL https:
//doi.org/10.1007/BF00275687.
Akshay Rangamani, Anirbit Mukherjee, Ashish Arora, Tejaswini Ganapathy, Amitabh Basu,
Sang Peter Chin, and Trac D. Tran. Critical points of an autoencoder can provably recover
sparsely used overcomplete dictionaries. CoRR, abs/1708.03735, 2017. URL http://arxiv.
org/abs/1708.03735.
Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural net-
works. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, pp. 774-782, 2016. URL http://jmlr.org/
proceedings/papers/v48/safran16.html.
Karin Schnass. Convergence radius and sample complexity of ITKM algorithms for dictionary
learning. CoRR, abs/1503.07027, 2015. URL http://arxiv.org/abs/1503.07027.
10
Under review as a conference paper at ICLR 2018
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Proceedings of the
18th Annual Conference on Learning Theory, COLT’05, pp. 545-560, Berlin, Heidelberg, 2θ05.
Springer-Verlag. ISBN 3-540-26556-2, 978-3-540-26556-6. doi: 10.1007/11503415.37. URL
http://dx.doi.org/10.1007/11503415_37.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15
(1):1929-1958, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation.
cfm?id=2627435.2670313.
Cheng Tang and Claire Monteleoni. Convergence rate of stochastic k-means. In Aarti Singh and
Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and
Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 1495-1503, Fort Laud-
erdale, FL, USA, 20-22 Apr 2017. PMLR. URL http://proceedings.mlr.press/
v54/tang17b.html.
7 Appendix
Derivation of stochastic gradients Upon receiving a data point x, the stochastic gradient with
respect to W is a jacobian matrix whose (j*,i*)-th entry reads
∂'(W, b; x)
∂Wj*i*
∂ d1	n	d
dwjZ7 X 2[xi- X WjiS(X Wjl xι + bj)]2
d	n	d	∂n	d
{xi -	WjiS(
Wjl Xl + bj)}{-∂wς--EWjiS(EWjlxl + bj)}
i=1	j=1	l = 1	j*i* j = 1	l = 1
d	nd	∂	d
=-X {xi-X WjiS(X Wjlxl +bj)} E w"(X W ibj*)
For i = i*, the derivative of the second term can be written using the chain rule as
∂ d	∂S(aj*)
Wj* i------S(〉Wj* lXl + bj* ) = Wj*i  -----Xi*
∂Wj* i*	∂aj *
where We let aj∙* := Wj*lxl + bj*, which is the activation of the j*-th neuron upon receiving X in
the hidden layer before going through the ReLu unit.
For i = i*, the derivative of the second term can be written using product rule and chain rule as
d
d
∂
Wj*i* T；
∂Wj* i*
S(	Wj* lxl + bj* ) + S(	Wj*lxl + bj* ) = Wj*i*
∂s(aj∙*)
l=1
l=1
∂aj*
Xi* + s(aj*)
Let r ∈ Rd be the residual vector with ri := xi - Pjn=1 WjiS(Pld=1 Wjlxl + bj). Then we have
∂'(W, b; x)
∂Wj*i*
∂S(aj*)
-{〉J riWj*i —∂------xi* + s(aj* )ri* }
i=1	j*
In vector notation, the stochastic gradient of loss with respect to the j-th row ofW can be written as
∂'(W, b; x)	∂s(aj)	，、、
-⅛Wj√ =-(段〃 hr,Wj*ix + s(aj)r)
Similarly, we can obtain the stochastic gradient with respect to the j -th entry of the bias term as
∂'(W, b; x)	∂s(aj)
∂bj	= - F hr，Wj?i
(5)
(6)
11
Under review as a conference paper at ICLR 2018
Now let Us examine the terms d∂j and hr, W7-?i. By property of ReLU function,
∂s(aj)	ʃ0 if aj <= 0
∂aj	=(1 if aj > 0
Mathematically speaking, the derivative of ReLU at zero does not exist. Here we follow the conven-
tion Used in practice by setting the derivative of ReLU at zero to be 0. In effect, the event {aj = 0}
has zero probability, so what derivative to Use at zero does not affect oUr analysis (as long as the
derivative is finite).
Proof of main theorem. Consider any time t > 0. By Lemma 1 and 2, the probability of sUccess-
fUlly initializing the network can be lower boUnded by
Pr(Fo) ≥ 1 - k exp{-n())d-3} if the network is initialized with GaUssian variables
2
or
nλ2	n3
Pr(Fo) ≥ (I- kexp{-8kB2D(I- 3exp{-际D
if initialized with data
Conditioning on Fo and applying Theorem 3, We get that for all t ≥ 0,
Pr(Ft) ≥ Pr(F∞) ≥ 1 - δ
Since Ft =⇒ Et by Lemma 4, we can apply version 1 of Theorem 2 to get the expected increase
in φts for any s sUch that g(s) > 0 as:
E[Φts∣Ft-1] ≥ ΦS-1{1 + kkW-ιk (1 - ΦS-1)}- (ηt-1)2B
Since by Fo, ∀j ∈ [k], there exists s ∈ [n] sUch that g(s) = j. Let s(j) be any s ∈ [n] sUch that
g(s) = j. Then, the ineqUality above translates to
E ∆S(j)∣F t-1] ≤ N-I- kkWWi—k N-I)(I- δ7 + (ηt-1)2B
= δ~i(I- β- ) I	(c0)2	B
= Aj)(I	to + t - i)+(to + t - 1)2 B
where
2 ≤ βt =2c0(I- △")=2c0(I- △")≤ "
一	kkWs(j)*k	kc	— kc
by oUr choice of c0 and by oUr assUmption on the initial valUe △so(j). Taking total expectation Up to
time t, conditioning on Ft, and letting β denote a lower boUnd on βt, we get
βt-1
E[∆S(j)|Ft] ≤ E[Δts-j)∖Ft](1 - t ：t_ I ) +
to + t - 1
≤ E∆S-1)∖F j](1- t-+β~1) +
(c0)2	B
(to +1 - 1)2
(c0)2	B
(to + t — 1)2
where the last ineqUality is by the same argUment as that in Lemma 8. This has the exact same form
as in Lemma D.1 of Balsubramani et al. (2013). Applying it with Ut := E[∆Sj) ∖Ft], a = β, and
b = (c0)2B (note oUr t + to matches their notion of t), we get
77r ʌ t I Tj Ir t to + 1 ∖β A o I (C ) B (Λ I__1 ∖β+1______1____
[Sj)?| t] ≤ (to +1 +1)	Sj)? + β - 1(1 + to + 1) to +1 + 1
By the upper bound on βt, We can choose β as small as 等.So We can get an upper expressed in
algorithmic and model parameters as
t	to + 1 ∖ 2c0	(c0)2 B	1	2c0 I -1	1
[S(j)? ∖	t] ≤	(to + t + 1 )kc s(j)?	+	2c0 - 1(1	+	to +	1) kc to	+1	+1
≤ ( to+1	) 2c0 λ2	+	(C0)2B (1	+	1	) 2c0+1	1
"to +1 + 1)	2 +费-10 + to + 1) to +1 + 1
12
Under review as a conference paper at ICLR 2018
The second inequality holds because by Fo,
λ2 λ2
δs(j)? = 1 - φs(j) ≤ 1 - (I - y) = f
Finally,
E[Θ(Wt, W*)|Ft] = 1 X E min
k j∈[k] s∈[n],g(s)=j
∆(WS*,Wj**) ≤ ； X E△叫j)*,Wj*?)
V( to + 1 、客 λ2 , (c0)2B
≤ (以E)kc y + 窖-1
≤( to + 1 )4 λ2 + (c0)2B
一 (to + t +1)2+	3
(1 +
(1 +
j∈[k]
，濯+1	1
to + 1 to + t + 1
ɪ) 法+1	1
to + 1 to + t + 1
where the last inequality is by our requirement that c0 > 2kc.
□
7.1 Part I: performance guarantee of initialization
ProofofLemma 1. Let U =后,where Z ∈ Rd With Zi 〜N(0,1). We know that U is a random
vector on Sd-1, the d-dimensional unit sphere. For any fixed vector v ∈ Sd-1, we have
P r(hU, vi ≥ 1 - h) = Pr(U ∈ Scap(v, h))
where Scap (v, h) is the surface of the spherical cap centered at v with height h. By property of
spherical Gaussian, we know that U is uniformly distributed on Sd-1. So we can directly calculate
the probability above as
μ(Scap(v,h))
μ(Sd-1)
Pr(U ∈ Scap(v, h)) =
where μ measures the area of surface. The latter ratio can be lower bounded (see Lemma 5 in the
Appendix) as a function of d and h:
μ(Scap(v, h))
μ(Sd-1)
≥ f(d,h)
(√h(2 - h))d-1
2d
Since for all row i ∈ [n], their entries are Wor = C亩,then for any ground-truth dictionary item
W?, j ∈ [k], we have
P r{maxhWio?, vi ≥ c(1 - h)} = P r{∃i ∈ [n], s.t.hWio?, vi ≥ c(1 - h)}
i∈[n]
= 1 -Pr{∀i ∈ [n], hWio?, vi < c(1 - h)}
= 1 - Πin=1P r{hcU, vi < c(1 - h)} = 1 - Πin=1P r{hU, vi < 1 - h}
≥ 1-Πin=1(1-f(d,h)) ≥ 1 - exp-nf(d,h)
By union bound, this implies that
Pr{∀j ∈ [k], maxhWi?, W；?i ≥ c(1 - h)} ≥ 1 - kexp-nf (d，h)
i∈[n]	i? j?
Now by our choice of the form of lower bound on the inner product, we have
h = 1 -，1 — P
substituting this into the function f(d, h), we get a nice form
(√P)d-1
1 -√1-P
(√P)d-1
1 - (1 - P)
(1 + √1-7 ) = (√ρ)d-3 (1 + √1-7) ≥ (√ρ)d-3
Substituting this into the previous inequality written in terms of h and letting P = λ2 completes the
proof.	□
13
Under review as a conference paper at ICLR 2018
Figure 2: The unit circle lies in H . For with a fixed norm, the picture illustrates one of the
configurations where the maximal angle (θ) between data point Xi (in red) and Wj? is achieved.
Lemma 5 (Lower bound on surface area of spherical cap). Let Sd-1 denote the d-dimensional
hypersphere, and let Scap (ν, h) ⊂ Sd-1 denote the surface of a hyper spherical cap centered at ν
with height h. Let μ(∙) denote measure of area of surface in d-dimensional Euclidean space. Let
f(h,d) := (√2h — h2)d-12d,then
μ(Scap(ν, h))
μ(Sd-1)
≥ f(h,d)
Proof. Let Bd denote the d-dimensional unit ball in Euclidean space, and let Bcap(ν, h) denote the
spherical cap centered at ν with height h. Then applying Lemma 4.1 of Micciancio & Voulgaris
(2010), we get
Vol(Bcap)(ν, h)
-Vol(Bd)-
/-------d—1 h
> vz2h — h2	2d
where Vol(∙) denotes measure of volume in Rd. So this lower bounds the ratio between volumes
between spherical cap and the unit ball. We show that we can use this to lower bound the ratio
between surface areas between spherical cap and the unit ball. Since by ?, we know that the ratio
between their area can be expressed exactly as
μ(Scap(ν, h)) _ 1τ ,d — 11、
μ(Sd-1)	= 2I2h-h2 (^~, 2)
and the ratio between their volume
V ol(Bcap)(ν, h)	1	d+1 1
-Vol(Bd) — — 2 I2h-h2 (^F, 2)
where Ix(a, b) is the regularized incomplete beta function. By property of Ix (a, b),
Ix(a+ 1,b) < Ix(a,b)
So we have
μ(Scap(V, h))	μ(Bcap)(V,h)	p^~j^	运d-1 h
μ(Sd-l)	>	μ(Bd)	> V2h - h	2d
□
ProofofLemma 2. For any Xi 〜 Cj, for any j ∈ [k]. We first claim that
{怕『Q} =⇒
{h Xi W * i≥ Jl — λ2}
{hkXik ,Wj?i ≥ V1	2 }
14
Under review as a conference paper at ICLR 2018
Proofofdaim. Let Us consider the two-dimensional plane H determined by Xi and Wj?. Clearly,
Ei = Wj? - Xi also lies in H. Let θ ：= ∠(Xi, Wj?) denote the angle between Xi and Wj?. Note
that cos θ = h kχik, Wj? i. Fix the norm of noise ∣∣6ik. Itis clear from elementary geometric insight
that θ is maximized (and hence cos θ is minimized) when the line ofXi is tangent to the ball centered
at Wj? with radius |匕||. We can directly calculate the value of cos θ at this point (see Figure 2) as
cos θ = ,1 一 ∣kik2, which finishes the proof of claim.	□
Now, we denote two events
A := {minX 1{χi∈Cj} ≥ 2k}
i∈[n]
and
B := {max £言]怕k21{χi∈Cj] ≤ ɪ}
j∈[k]	i∈[n] 1{Xi∈Cj}	2
The probability of event A can be lower bounded by concentration inequality for multinomial dis-
tribution Devroye (1983): Let nj := Pi∈[n] 1{Xi∈Cj}. We get
pr(-A) = Pr(∃j ∈ [k],nj - n ≤ -^k) ≤ Pr(X Inj- V | ≥ ^k)
k	2k	k	2k
j=1
n	n3
≤ 3exP{-n/25⑸)2} = 3exP{-≡2}
where the second inequality is by Lemma 3 of Devroye (1983).
Pr(BIA) ≥ 1 - k Pr(IldI2
where ∣∣6k2 is the empirical mean of |匕||2 for all Xi, i ∈ [n] belonging to the same component Cj
for some j ∈ [k]. Conditioning on A, We know that the average is taken over at least 聂 samples for
each component Cj. By one-sided Hoeffding’s inequality,
____ λ _____________ λ	2(n∕2k)( -A- )2	nλ
Pr(W ≥ √2) ≤ Pr(卬 ≥ Ekek2 + 2√2) ≤ exp{----------------B^2-} = exp{— -}
(note, we abuse notation B in the exponent as an upper bound on ∣d∣2, as in other parts of the
analysis). Thus,
Pr{Fo} =Pr{ minmaxh irXii∣ ,Wj**i ≥ 1- - ⅛-}
j∈[k] i∈[n] kXi k	2
≥ Pr{maxmin keik- ≤ —=}
j∈[k] i∈[n]	2
nλ2	n3
≥ Pr B ≥ Pr(B ∩ A) = Pr(BIA)Pr(A) ≥ (1 - k exp{- ^^B })(1 - 3exp{-1^^})
□
Proof of Lemma 3. To simplify notation, we use τ1 (τ2) as a shorthand for τs,1 (τs,2). Let j =
argmaxi∈[k] hWS?, W%i.Then hWS?, WJ = cτ>If Wt? and Wj? align, then ''
IhWS*,Wj*iI = c|hW；?，W；?iI ≤ cλ ≤ cτs,ι
where the second inequality is by relation ofλand τs,1.
If Wt? and Wj? do not align, then they determine a two-dimensional plane H. Let i ∈ [k] and i = j.
We can decompose the unit vector Wi? as
Wi? = ProjH (Wi?) + orthH(W?)
15
Under review as a conference paper at ICLR 2018
Figure 3: The unit circle lies in H. The picture illustrates one of the possible configurations where
the minimal angle (θ) between ProjH(W?) and W(? is achieved.
where ProjH(Wi?) is the orthogonal projection of W? onto H. First, note that by orthogonality,
IhprojH(Wi?),W；?il ≤ λ
Since ProjH(W?), Ws?, and Wj? lie on the same plane, We can easily see via geometric insight
(see Figure 3) that,
h
ProjH (Wi?)	Wst?
kProjH (WiI) k, kWstJ
i≤
kProjH (WiI) k
τ1+
λ2
1	kProjH (Wi?)k2
1-- τ2
λ
Therefore,
IhWi?,WSt?iI ≤ c(λτι + JkProjH(Wi?)k2- λ2∕1-72)
≤ c(λτι + pl - λ2 Jl — τ2 )c2λτι
Suppose τ2 ≥ 1++λ. It can be verified that this implies
CT2 = IhWi?,Wst*i∣≤ Cyl+λ = cτ1
Observing that τ12 ≥ 1++λ indeed holds provided λ ≤ 1 and τ1 ≥ √1 - λ2 completes the proof. □
7.2 Part II: the evolvement of weights and bias during SGD training
Proof of Lemma 4. Conditioning on F t, we show that Et holds by induction on 0 ≤ i ≤ t. We
start by showing that Eo holds.
Base case In this case
bθ = E(x,WS□( c12 — 1)
Note that by our model assumption,
E hχ,Ws□ = X 1(W；*，Wi?i
i∈[k]
Since Fo holds, we know that the firing of neuron s is unique. Let τs,1 and τs,2 as defined in Lemma
3.
Let X 〜Cj. Consider the case g(s) = j. In this case, Lemma 3 implies that
TSJ = maxhWs*,Wj*o*i = hWso*,Wj**i
We will repeatedly use the following relation, as proven by Lemma 6,
τs,2 := max KWo?,%?)]
i6=j
16
Under review as a conference paper at ICLR 2018
We have
ws?X + bθ = (WS?)T(Wj* + e) + (c12 - 1) X 1 hWS?, Wi?i
i∈[k]
Now, observe that ± - 1 < 0 and
X1 hw?,wi?i
i∈[k]
1	-1
≤ CkTsJ + c^Γτs,2
So we get,
Wo?x + bo ≥ CTs,1 - Ckek + (g - I)(CIτs,1 + c~~--τs,2)
ck	k
(11	-1
-I)(Crs,1+ ckc2λτs,I)
c{τs,ι{1 - (1 - C2)(k + 2λ)} - IIell}
Since by our assumption, λ ≤ 册,(1 - *)(ɪ + 2λ) ≤ (1 - *)4k ≤ 4k. Furthermore, since by
our assumption, ke∣ ≤ √16kλ ≤ √1 - λ2(1 - 45k) ≤ (1 - 45k)τι we get Woo?x + bS ≥ 0.
Consider the case g(s) = j0, j0 = j. We first upper bound bo in this case. Since *-1 < 0, we
would like to lower bound
Ehx ws?i = X 1 hws?, Wi?i = 1 卜Ws?, Wj*,*i + XhWs?, Wi?i ∖
i∈[k]	I	i=j
≥ T ʌ hWs?, Wj*0?i - | XhWo, Wi?i| ∖ ≥ τcτs,1	c-(CTSa ≥ τcτs,1 - c2λτs,1
k	kkk
i6=j
On the other hand,
W0?x ≤lhWo∙,W? + ei∣≤ c(τs,2 + IIell)
So
Ws?X + bo ≤ C(Ts,2 + kek) - (I - C)cτs,1 (k - 2λ)
≤ c(2λτs,ι + IIek)-(I - C)cτs,ι(k - 2λ) ≤ 0
where the last inequality is by assumptions c > 3 and ∣∣e∣ ≤ V'1-'2.
Case 0 < i ≤ t Suppose Ei holds, we show that Ei+1 holds for i ≤ t - 1. Let X 〜Cj for any
j ∈ [k]. Since Ei holds, we know that
∀s s.t. g(s) > 0, ais > 0 iff g(s) = j
So for neurons Wsi?+1 with g(s) > 0 and g(s) 6= j, we know that these neurons are not updated, that
is,
Wsi?+1 = Wsi? and bis+1 = bis
This means the two conditions in Ei+1 on neurons g(s) = j0 6= j.
For neurons Wsi?+1 with g(s) = j, bis+1 is set such that
bs+1 = Ehχ1{as>0}, ws+1 i(ɪ -1) = hE[x|x 〜Cj], wi+1i(C2 - 1) = hws+1, Wj**i( 1 - 1)
Consider the case χ0 〜Cj, we have
(ws+1 )TX0 + bs+1 = FWi+1 + 1 (ws+1)τWj** ≥ τs,ιC -Ckek ≥ Pf-12C -Ckek
17
Under review as a conference paper at ICLR 2018
where the last inequality is by assumption τs2,1 ≥ 1 - λ2. It follows that (Wsi?+1)Tx0 + bis+1 > 0
holds by our assumptions that C ≤ √6k, and that ∣∣d∣ ≤	.
Finally, consider the case χ0 〜g，for j0 = j,j ∈ [k]. Again, since Fi+1 holds,
Ts,ι = (Wj+1)TWj** ≥ cp1-λ2
we can apply Lemma 3 to get
(WS+1)Tx0 + bS+1 ≤ cτs,2 + eTWi+1 + (Wj+1)TWj*(ɪ - 1) ≤ "? + Ckek + CTsj(ɪ - 1) ≤ 0
where the last inequality holds similarly as in the base case.	□
Lemma 6. Let τs,1, τs,2 be as defined in Lemma 3, and let λ be the incoherence parameter. If
τs2,1 ≥ 1 -λ2, then τs,2 ≤ Cλτs,1.
Proof. Using the same argument as the proof of Lemma 3, we get
IhWi**, WS*il ≤ c(λτι + JIlprojH(Wi*)k2 - λ/1-72)
≤ c(λτι + pl - λ2 q/1 - τ2)
≤ C2λτ1
where the last inequality is by the relation between τι and λ.	□
Proof of Theorem 2. We start by proving version 1. The proof of version 2 follows directly. Let
X 〜Cj for any j ∈ [k]. We consider and any neuron S s.t. g(s) = j.
Again, since Et holds,
as > 0|x 〜Cj,g(s) = j
We now examine E[φs+1 ∣χ 〜Cj,g(s) = j, Et]. To ease notation, We let W := Wf*, w* := Wj**,
η := ηwt , r := rt, a := ats, and b := bts in the proof.
φt+i = (hW+1, w*i)2 = ((W + η[hr, Wix + ar])Tw*)2
s	kWj+1k2	∣∣w + η[hr, wix + ar]∣2
The denominator reads
kwk2 + 2η[(rT w)(xT w) + a(rT w)] + η2 [(rtw)2 kxk2 + 2a(rT w)(xT r) + a2krk2]
=∣∣w∣2(1 + 7-^772 2η[(rT w)(xτ w) + a(rτ w)] + -^-ɪ^ η2[(rT w)2∣x∣2 + 2a(rT w)(xτ r) + a2∣r∣2])
kwk2	kw k2
Let A(w) := (rT w)(xT w) + a(rT w) and B(w) := (rT w)2 kxk2 + 2a(rT w)(xT r) + a2 krk2.
1	_	1
kw + η[hr, Wix + ar]k2	l∣w∣2(1 + kw⅛2 2ηA(w) + 步 η2B(w))
_	1 - ( kW1k2 2ηA(w) +	η2B(W))
=kwk2{1 - (ɪ2ηA(w) + ɪη2B(w))2}
1 - (kW1k22ηA(W) +	^B(W))
≥	W
Let C(w*) := (rTw)(xTw*) + a(rT w*). The numerator reads
(wT w* + η[(rT w)(xT w*) + a(rT w*)])2 = (wT w*)2 + 2η(wTw*)C(w*) + η2C(w*)2
18
Under review as a conference paper at ICLR 2018
Putting these together,
φj+1 ≥ ∣rA72{(WTw*)2 + 2η(WTW)C(W) + η2C(w*)2}{1 - Gr^ii22ηA(W) + τ^η2η2B(W))}
j	kwk2	kwk2	kwk2
=II 1∣∣2 {(wtW*)2 + 2η(WTW*)C(w*) - ɪ 2ηA(W)(WTw*)2} (7)
kWk2	kWk2
A(W)C(W*)
kWk2
- η2
(WT W* )2B(w)
--
4 B(W)C(W*)
一η i^i 22,	}
kW k2
Bounding major direction of improvement Now let us focus on line (7), which can be viewed as
the major direction of change guided by the stochastic gradient, while the rest terms can be viewed
as noise. It equals
φj + 2η(W W )C(W ) - ∣rA72φj2ηA(W) = φj + ττ2η∣2{(WTW*)C(W*) - φjA(W)}
kWk2	kWk2	kWk2
Note that both terms C(W*) and A(W) are stochastic, since their value is determined by data we
sampled from the model distribution. We continue with the key step of our analysis: bounding
EC(W*) and E A(W). Since
rTW = xTW - akWk2 and a = xTW + b
(rT W)(xT W*) = (xTW - akWk2)(xT W*) = (xTW - (xTW + b)kWk2)(xT W*)
= WT xxT W* - (xTW + b)kWk2(xT W*)
and
a(rTW*) = (xTW + b)(xT W* - a(WT W*))
So
C(W*) = (2 - kWk2)(WT xxT W*) - a(xT W)(WT W*) - bkWk2(xT W*) + b(xTW*) - ab(WTW*)
= (2 - kWk2)(WT xxT W*) - (xTW)2 (WTW*)
-bkWk2(xT W*) + b(xTW*) - 2b(xTW)(WTW*) - b2 (WTW*)
Since b is chosen such that b = (wtw*)(k^p - 1) conditioning on Et, after some calculation We
get
Eχ{-2b(xT w)(wt w*) — b∣∣wk2(XT w*) + b(xT w*)∣Ωt}
=2(WTW*)3(1 - ij-Λ2) + (WTw*)(I -	2)(kWk2 - 1)
kWk2	kWk2
On the other hand, note that
Ex[wt xxT w*∣Ωt] = wt ExxT w* = wt E(w* + e)(w* + E)T w* = wt (w* (w*)t + EceT )w*
= WT (W*(W*)T + σI)W* = WTW* + σwT w*
Where the second inequality is by the condition that x is draWn from the j-th component, and fourth
inequality is due to our model assumption on the covariance structure of e. Similarly,
Eχ[(xT w)2∣Ωt] = Eχ[wTXxT w∣Ωt] = wt (w*(w*)t + σI )w = (wt w*)2 + σ∣∣wk2
Therefore, We can calculate the other part as
Ex{(2 -∣∣wk2)(WTXxTw*) — (xTw)2(wtw*)∣Ωt}
= (2 - kwk2)(wT w*)(1 + σ) - [(wTw*)3 + σ(wT w*)kwk2]
Combining, We get
Ex(C(w*)∣Ωt} = (2 - kwk2)(WTw*)(1+ σ) - [(wtw*)3 + σ(wτw*)∣∣w∣∣2]
+2(wTw*)3(1 -占)+ (wTw*)(1 -占)(kwk2 - 1) - Eχ[b2(wτw*)∣Ωt]
kw k2	kwk2
19
Under review as a conference paper at ICLR 2018
Thus,
Ex{C(w*)∣Ωt}⅛TWp = φj{(wTw*)2[1 -占] + 2σ - 2σ∣∣w∣∣2 + 占}- Eχ[b2∣Ωt]φj
kwk2	kwk2	kwk2
Now, we turn to the term A(w), which equals
A(w) = ((x - aw)T w)(xT w + a) = [xTw - (xTw + b)kwk2](2xT w + b)
= (2 - 2kwk2)(xT w)2 + b(xT w)(1 - 3kwk2) - b2 kwk2
So
Ex[A(w)∣Ωt] = (2 - 2∣∣wk2)[(wTw*)2 + σ∣∏∣2]
+(wTw*)2(1 - 3kwk2)(1 -占)-Ex[b2kwk2∣Ωt]
kwk2
This implies that
Ex[A(w)∣Ωt]ɪ = (φj)2(ɪ - 2 + kwk2) + σφj(2 - 2∣∏∣2) - Ex[b2∣Ωt]φj
Finally, combining the derivation for expected values relevant to C(w*) and A(w), we get
Ex{C(w*)∣Ωt} (wτw*) - Ex[A(w)∣Ωt]言=占φj(1 - φj)
kwk2	kwk2	kwk2
So the expectation of line (7) is
φs+2η 舄2 φs(I - φS)
Also, note that since the terms w, w*, x are all bounded, the noise term must be bounded by some
constant B1. Therefore, we can get a lower bound on increase of conditional expectation
E[φS+1lx 〜Cj,g(S)= j,Et] ≥ φS + 2η ∣j-Λ2 φS(I- φS)-η2Bι
s	s	kwk2 s s
Since Pr(X 〜Cj) = k, We get
E[OS+1|g(S) = j,Et] ≥ 1 {φS + 2η7r1[2φS(1 - φS) - η2Bι}
S	k S	kwk2 S S
+--7— E[0S+1|g(S) = j, Et,x 〜Cj0,j0 = j]
k
≥ φS + 2ηTΓ∣―2φφS(I - φS) - η2B1
S kkwk2 S S
where the last inequality holds because aS (x) ≤ 0 for X 〜Cjo and g(s) = j (j = j0) by Et, and for
neuron S such that atS(x) ≤ 0, the gradient evaluates to zero so those entries are not updated after t.
Thus, since Et holds, we know that
Wt+1 = WS*Ig(S) > 0,g(S)=j, and X 〜Cj
VerSion2 Let Y ：=	W	占A(w)(wτw*)2
kwk2	kwk4
Then, from the proof of version 1, we know that
ΦS+1 ≥ ΦS + 2ηtE[Y|Et, X 〜Cg(s)]Pr(X 〜Cg(S))- 2ηtE[YIEt] + 2ηtY + (ηt)2Bι
=ΦS + 2ηt占ΦS(1 - ΦS) + 2ηt(Y - E[Y∣Et]) + (ηt)2Bι
S kwk2 S S
So letting Z := Y - E[YIEt], we know that E[ZIEt] = 0 Again, since the terms w, w*, X are all
bounded, there must exists B2 ≥ ∣Z∣. Letting B := max{B1, B2} finishes the proof.	□
20
Under review as a conference paper at ICLR 2018
7.3 Part III: convergence of martingales
Proof outline of Theorem 3. Conditioning on F o, note that F ∞ can be written as the union of
events
∪s∈[n],g(s)>0 {hWS?,WD ≥ C √1 - λ2, ∀t ≥ 0, ∀j ∈ [k] s.t. g(s) = j}
We denote these individual events regarding neuron s by Fs∞, that is,
F∞ := {hWS*,Wj**i ≥ cp1 - λ, ∀t ≥ 0, ∀j ∈ [k] s.t. g(s) =j}
The proof strategy is to show that the individual probability can be lower bounded by δ, and taking
the union bound over all neurons. Since (Fs∞)c can be partitioned into a union of disjoint subsets
∪t∞=0(Fst \ Fst+1), where
Fs := {hWS*, WD ≥ cp1 - λ, ∀0 ≤ i ≤ t, ∀j ∈ [k] s.t. g(s) = j}
We can define Est similarly, and note we can easily show that
Fst =⇒ Est
by going through Lemma 4 exactly the same. To lower bound P r(Fs∞), we upper bound the indi-
vidual error probability P r(Fst \ Fst+1), and show that their summation vanishes for t → ∞. The
same approach is taken in Proposition 2 Tang & Monteleoni (2017), and our main idea is to adapt
the analysis there to our case. Thanks to the special form of inequality obtained in Theorem 2, we
can neatly re-write the statement of its version 2 in terms of ∆ts as
∆S+1 ≤ ∆S{i - -W-(i - ∆S)} + 2ηtz + (ηt)2B	(8)
k kWs? k
Essentially, the sufficient condition for the analysis of Proposition 2 Tang & Monteleoni (2017) to
work are
• The expected decrease on the objective function ∆ts is of the form
∆ts+1 ≤ ∆ts{1 -
M}+ 21⅛ Z + (Wo )2B
(9)
•	βt ≥ 2, ∀t ≥ 0
•	The noise terms Z, B are bounded, and E [Z |Ft] = 0.
By our choice of ηt , the relation in (8) satisfies the special form in (9). Since conditioning on Ft ,
we have
2c0(1 - δS) ≥
β :=	-kWS?k	≥
2c0ΦO
-c
and since conditioning on Fo,
ΦS ≥1 + (12- δo)2 ；
for g(s) > 0 and our choice of δo in Fo. Since we also set
c0 ≥ 2-c
1
≥ —
-2
we get that βt ≥ 2 always hold. On the other hand, here the noise terms are obviously bounded by
our model assumption. Therefore, we only need to slightly adapt Proposition 2 Tang & Monteleoni
(2017) for our purpose. The exact proof is included for completeness.	□
Complete proof of Theorem 3. We consider each Fst individually. Since the proof for each s is
exactly the same, we abuse the notation Ft to let it denote Fst for any fixed s. Similarly, we let ∆t
denote ∆ts. Conditioning on Ft, we know that ∃β > 2 s.t. β ≤ βt. By Lemma 7, for any λ> 0,
and any 0 ≤ i ≤ t - 1, we have
ES△「Fi" EN(I-异产Fi} exp{ 曰 + λg+B?}
≤ ES""]Fi-1} exp{ λ(c∕ B
(to + i)
+ λ2(c0)2B2 }
+ 2(to + i)2 }
21
Under review as a conference paper at ICLR 2018
where λ(1) = λ(1 - ^β+i), and the second inequality is by Lemma 8. For k ≥ 1, we define
λ(0) := λ and λ(k) := Πtk=1(1 -
to+1)λ(0)
We can similarly get, for k = 0, . . . , i,
E{ei(kX-k+i ∣fi-k} ≤ E{ei(k+1X-k|Fi-(k+1)}
λ(k)(c0)2 B	(λ(k))2 (c0)2B2
exp{ (to + i - k)2 + 2(to + i - k)2 }
Since ∀β> 0, k ≥ 1,
λ(k) = λΠk=ι(1 -	β ) ≤ λ(to + i -k + 1 )β
to+ (i - t + 1)	to + i
Since the bound is shrinking as β increases and β ≥ 2,
λ(k)	≤ Jo + i - k + 1)2 λ ≤	4λ
(to + i - k)2 — to + i	(to + i - k)2 — (to + i)2
Recursively applying the relation until we get to the term
E{eλ⑸δ1∣Fo} ≤ E{ei(i+1&|Fo} exp{T + ^^}
( o+ )	( o+ )
(i+1) o	λ(c0)2B	λ2 (c0)2B2
=expM(i+1)^} exp{ (ξ∙+ψ + 2K^+ψ}
Combining all these recursive inequalities with the bound on λ(k), we get
E{e0+1 ∣Fi}≤ ei(i+1X exp{χ-χ(4|(Cf| +)}
k=0 (to + i)2	2(to + i)2
≤ exMλ(tj+i)βδ° + He')" + "2R](T‰}
o+	(o+ )
Let τo := λ2. Then we can apply the conditional Markov’s inequality, for any λi > 0,
Pr(Fi \ Fi+1) = Pr(∆i+1 > τ0∣Fi)
=Pr(eii7+1 > eλ"o∣Fi) ≤ E[e：：T IFi]
Since event Fo = {φo ≥ 1 - λ22} implies ∆o = 1 - φo ≤ λ22 = T,we have
(t⅛)βδ°- τ ≤ δ°- τ ≤ 2
Combining this with the upper bound on E[eλi∆i+1 |Fi], we get
Pr(Fi\Fi+1) ≤eχp{-λi{To -(B + ?)(t(+⅜}}
We choose λi = ∆ ln (i：1)? with ∆ = To, and show that To - (B + λiBB2) (t(CHi)2 is lower bounded
by ∆.	o
Case 1:	B > λi2B2. Since to ≥ 32(；)2B, we get
2 τo -(B+
%B2 4(c0)2i
ɪ)(to + i)2
≥∆
22
Under review as a conference paper at ICLR 2018
Case 2:	B ≤ λiB2. We get
1 τ - (B + λiB2) 4(c0)2i
2 o (	+	2 )(to + i)2
2 4(c0)2 i
≥ 2∆ - λiB2	∖
(to + i)
1	(1 + i)24(c0)2B2i
2δ - ∆ln	E声
≥ 2∆ 一 ɪ ln
(to + i)2 4(c0)2 B2 (to + i)
(to + i)2
δ
Now we show
1ln(to + i)2 4(c0)2B2 ≤ ∆
∆ δ to + i
Since
to + i ≥ to ≥
192(c0)2B2
τo2
ln2 δ
ln δ ≥ 1, and obviously,
16(c0)2B2
∆2
≥ 16(c0 )2B2
一(2 To)2
≥ 1, We can apply Lemma ?? with b = 2, C :
1鼠2B2, t := to + i ≥ (bC ln 1)b-1, and get
4(c0)2B2 ln (to + i)2 .
∆2
δ
2C ln t + C ln1 <tb-1 = t。+ i
δ
Or equivalently, -∆ ln
(to+)2 4⅞+B2 ≤ δ∙ Thus, forboth cases,
λiB2	4(c0)2i
2δ-(B + F)(^ ≥=δ
This implies
Pr(Fi \ Fi+1) ≤ e-2(In (1+)2心= δ
(i + 1)2
Finally, we have
∞
Pr(∪i≥1Fi \ Fi+1)≤ X Pr(Fi \ Fi+1)≤ δ
i=1
Now recall that this holds for each s, that is, ∀s ∈ [n],
P r(Fs∞) ≤ δs
substituting δs = δ for δ in the proof above, and taking the union bound completes the proof. □
Lemma 7 (Inequality of moment generating function). Suppose the conditions of Theorem 2 hold.
Then conditioning on Et (Ft), we can upper bound the moment generating function of ∆t+1 as
E[eʌʌt+1 |Ft] ≤ exp{λ∆t(1 - tɪ) + λ(ηt)2B + λ(ηt)2B2}
Proof. We apply the result of Theorem 2. Rewriting version 2 of Theorem 2 using ∆t := 1 - φts,
for any s ∈ [n], we get
∆t+1 ≤ ∆t - kkW； k ∆t(i - ∆t) + 2ηtZ + (ηt)2B
= △'0- k⅛(1 - δR + 2ηtZ + (ηt)2B
23
Under review as a conference paper at ICLR 2018
We let βt := 2k(1-t；J. Conditioning on Ft, the moment generating function of the ∆t+1 is upper
bounded by
E[e^t+1 |Ft] ≤ exp{λ∆t(1 - “可\-∣W) ”) + λ(ηtpB}Eexp{2ληtZ∣Ft}
(t + to)k kWs? k
βt
=exp{λ∆t(1 - ---) + λ(ηt)2B}Eexp{2ληtZ|Ft}
t+to
By Theorem 2,
E{2ληtZ|Ft} = 0 and ∣2ληtZ| ≤ 2ληtB
Applying Hoeffding’s lemma for bounded random variable, we can bound its m.g.f. by
Eexp{2ληtZ|Ft} ≤ exp{ λ5jBi }
So, finally
E[ei、+1 |Ft] ≤ exp{λ∆t(1 - tɪ-) + λ(ηt)B + λ2(η^B-}
□
Lemma 8 (Lemma from Tang & Monteleoni (2017)). For any λ > 0,
E{e^tlFt} ≤ E{eiZ|Ft-1}
7.4 Technical lemmas
Lemma 9 (Tang & Monteleoni (2017)). For any fixed b ∈ (1, 2]. If C ≥ b-1, δ ≤ e, and t ≥
(b3C1 ln 1) b⅛ ,then tb-1 一 2C ln t 一 C ln 1 > 0.
Proof. Let f (t) := tb-1 — 2Clnt — C ln 1. Taking derivative, we get f0(t) = (b — 1)tb-2 —军 ≥ 0
when t ≥ (b--) b11. Since ln ⅛b3C1 ≥ b-⅞ ≥ 1, (ln 1 b¾)b-1 ≥ (b¾) b⅛, it suffices
to show f ((ln 1 b-)b-2ι) > 0 for our statement to hold. f((ln δb-Ci)b21) = (ln 1 b¾)2 —
2C ln{(ln 1 罟)b-21}-C ln 1 = (ln ⅛ )2(⅛ - b≡ 皿足 1 b≡) - C ln 1 = b-⅞ [ b⅛ ln 1-
ln(b-1 ln 1 )] + Cln 1 [(督尸 一1] > 0, where the first term is greater than zero because χ-ln(2x) >
0 for x > 0, and the second term is greater than zero by our assumption on C.	□
Lemma 10. Suppose our model assumptions on parameters , c, α hold, and that our assumptions
on the algorithmic parameter c in Theorem 1 holds, then
l∣ek(1 + (α 一 λ)2) ≤ (I 一 C)(—k-— - Q)
Proof.
(1- c12)(1一* 一 I) -。)
1/2 + αk 1 — α(k — 1)	1∕2α + α2k
一 1 一 α(k 一 1)	k	1 一 α(k 一 1)
1 + 2αk	1∕2α + α2k
2k	1 - α(k - 1)
1	2αk(1 — α(k — 1)) — 1∕2α — α2k
2k +	2k(1 — α(k — 1))
where
2αk(1 — α(k — 1)) — 1∕2α — α2k
=α(2k + αk - 2αk2—-)
24
Under review as a conference paper at ICLR 2018
where the last term is greater than zero because
k - 1	2k - 1/2
α < 4k2 - 3k + 1 < 2k2 - k
So theterm (1 —	)(1 — Mr 1) - α) ≥ ɪ ≥ max|旧|(1 + (α - λ)2)
c2	k	2k
□
25