Under review as a conference paper at ICLR 2018
Unbiased scalable softmax optimization
Anonymous authors
Paper under double-blind review
Ab stract
Recent neural network and language models have begun to rely on softmax distri-
butions with an extremely large number of categories. In this context calculating
the softmax normalizing constant is prohibitively expensive. This has spurred a
growing literature of efficiently computable but biased estimates of the softmax.
In this paper we present the first two unbiased algorithms for maximizing the soft-
max likelihood whose work per iteration is independent of the number of classes
and datapoints (and does not require extra work at the end of each epoch). We
compare our unbiased methods’ empirical performance to the state-of-the-art on
seven real world datasets, where they comprehensively outperform all competi-
tors.
1	Introduction
Under the softmax model1 the probability that a random variable y takes on the label ` ∈ {1, ..., K},
is given by
p(y = '∣x; W)=
eχ>w'
PK=I ex>wk
(1)
where x ∈ RD is the covariate, wk ∈ RD is the vector of parameters for the k-th class, and
W = [w1, w2, ..., wK] ∈ RD×K is the parameter matrix. Given a dataset of N label-covariate pairs
D = {(yi, xi)}iN=1, the ridge-regularized maximum log-likelihood problem is given by
NK
L(W) = X足Wyi- log(X ex>Wk) - 2kWk2,
(2)
i=1	k=1
where kW k2 denotes the Frobenius norm.
This paper focusses on how to maximize (2) when N, K, D are all large. Having large N, K, D is
increasingly common in modern applications such as natural language processing and recommenda-
tion systems, where N, K, D can each be on the order of millions or billions (Partalas et al., 2015;
Chelba et al., 2013; Bhatia et al.).
A natural approach to maximizing L(W) with large N, K, D is to use Stochastic Gradient Descent
(SGD), sampling a mini-batch of datapoints each iteration. However if K, D are large then the
O(KD) cost of calculating the normalizing sum PkK=1 exi>wk in the stochastic gradients can still
be prohibitively expensive. Several approximations that avoid calculating the normalizing sum have
been proposed to address this difficulty. These include tree-structured methods (Bengio et al., 2003;
DaUme In et al., 2016; Grave et al., 2016), sampling methods (Bengio & Senecal, 2008; Mnih &
Teh, 2012; Joshi et al., 2017) and self-normalization (Andreas & Klein, 2015). Alternative models
such as the spherical family oflosses (de Brebisson & Vincent, 2015; Vincent et al., 2015) that do not
reqUire normalization have been proposed to sidestep the issUe entirely (Martins & AstUdillo, 2016).
Krishnapuram et al. (2005) avoid calculating the sum using a maximization-majorization approach
based on lower-bounding the eigenvalues of the Hessian matrix. All2 of these approximations are
computationally tractable for large N, K, D, but are unsatisfactory in that they are biased and do not
converge to the optimal W * = argmax L(W).
1Also known as the multinomial logit model.
2The method of Krishnapuram et al. (2005) does converge to the optimal MLE, but has O(ND) runtime
per iteration which is not feasible for large N, D.
1
Under review as a conference paper at ICLR 2018
Recently Raman et al. (2016) managed to recast (2) as a double-sum over N and K. This formulation
is amenable to SGD that samples both a datapoint and class each iteration, reducing the per iteration
cost to O(D). The problem is that vanilla SGD when applied to this formulation is unstable, in that
the gradients suffer from high variance and are susceptible to computational overflow. Raman et al.
(2016) deal with this instability by occasionally calculating the normalizing sum for all datapoints at
a cost of O(NKD). Although this achieves stability, its high cost nullifies the benefit of the cheap
O(D) per iteration cost.
The goal of this paper is to develop robust SGD algorithms for optimizing double-sum formulations
of the softmax likelihood. We develop two such algorithms. The first is a new SGD method called
U-max, which is guaranteed to have bounded gradients and converge to the optimal solution of
(2) for all sufficiently small learning rates. The second is an implementation of Implicit SGD, a
stochastic gradient method that is known to be more stable than vanilla SGD and yet has similar
convergence properties (Toulis et al., 2016). We show that the Implicit SGD updates for the double-
sum formulation can be efficiently computed and has a bounded step size, guaranteeing its stability.
We compare the performance of U-max and Implicit SGD to the (biased) state-of-the-art methods
for maximizing the softmax likelihood which cost O(D) per iteration. Both U-max and Implicit
SGD outperform all other methods. Implicit SGD has the best performance with an average log-loss
4.29 times lower than the previous state-of-the-art.
In summary, our contributions in this paper are that we:
1.	Provide a simple derivation of the softmax double-sum formulation and identify why
vanilla SGD is unstable when applied to this formulation (Section 2).
2.	Propose the U-max algorithm to stabilize the SGD updates and prove its convergence (Sec-
tion 3.1).
3.	Derive an efficient Implicit SGD implementation, analyze its runtime and bound its step
size (Section 3.2).
4.	Conduct experiments showing that both U-max and Implicit SGD outperform the previous
state-of-the-art, with Implicit SGD having the best performance (Section 4).
2 Convex double-sum formulation
2.1 Derivation of double-sum
In order to have an SGD method that samples both datapoints and classes each iteration, we need
to represent (2) as a double-sum over datapoints and classes. We begin by rewriting (2) in a more
convenient form,
N
L(W) = X -log(1+ X ex>(Wk-Wyi)) - 2kWk2.	⑶
i=1	k6=yi
The key to converting (3) into its double-sum representation is to express the negative logarithm
using its convex conjugate:
- log(a) = max{av - (- log(-v) - 1)} = max{-u - exp(-u)a + 1}	(4)
where U = - log(-v) and the optimal value of U is u*(a) = log(a). Applying (4) to each of the
logarithmic terms in (3) yields
N
L(W) = X max{-Ui - e-u (1 + X ex>(Wk-Wyi)) + 1} - £ ∣∣W ∣∣2
ui∈R	2
i=1	k6=yi
= -min{f(U,W)}+N,
u≥0
where
N	+ -ui
f(U,W)=XX UK-Ti + ex> (Wk-Wyi )-ui + μ kW k2	(5)
i=1 k6=yi
2
Under review as a conference paper at ICLR 2018
is our double-sum representation that we seek to minimize and the optimal solution for ui is
遍(W) = log(1 + Pk=yiex(Wk-Wyi)) ≥ 0. Clearly f is a jointly convex function in U and W.
In Appendix A we prove that the optimal value of u and W is contained in a compact convex set
and that f is strongly convex within this set. Thus performing projected-SGD on f is guaranteed
to converge to a unique optimum with a convergence rate of O(1/T) where T is the number of
iterations (Lacoste-Julien et al., 2012).
2.2	Instability of vanilla SGD
The challenge in optimizing f using SGD is that it can have problematically large magnitude gradi-
ents. Observe that f = Eik [fik] Where i 〜unif({1,…，N}), k 〜unif({1,…，K} 一 {yi}) and
fik (u,W) = N (Ui + e-ui + (K - 1)ex> (Wk-Wyi )-ui)) + 2瓦|匹||2 + βk∣∣wkk2),	(6)
where βj = η +(改-N .)(κ-ι) is the inverse of the probability of class j being sampled either through
i or k, and nj = |{i : yi = j, i = 1, ..., N}|. The corresponding stochastic gradient is:
VWk fik (u,W) = N (K - 1)ex> (Wk-Wyi )-ui Xi + μβkWk
VWyi fik (u,W) = -N(K - 1)ex>(Wk-Wyi )-uiXi + μβyi Wyi
VWj/{…fik (u,w )=0
Vuifik(U,W) = -N(K - 1)exi>(Wk-Wyi)-ui +N(1 -e-ui)	(7)
If Ui equals its optimal value 遍(W) = log(1 + Pk=y. ex>(Wk-Wyi)) then ex>(Wk-Wyi)-ui ≤ 1 and
the magnitude of the N(K - 1) terms in the stochastic gradient are bounded by N(K - 1)kxik2.
However ifUi xi>(wk - wyi), then exi>(Wk-Wyi)-ui 1 and the magnitude of the gradients can
become extremely large.
Extremely large gradients lead to two major problems: (a) the gradients may computationally over-
flow floating-point precision and cause the algorithm to crash, (b) they result in the stochastic gra-
dient having high variance, which leads to slow convergence3. In Section 4 we show that these
problems occur in practice and make vanilla SGD both an unreliable and inefficient method4.
The sampled softmax optimizers in the literature (Bengio & Senecal, 2008; Mnih & Teh, 2012;
Joshi et al., 2017) do not have the issue of large magnitude gradients. Their gradients are bounded
by N(K 一 1) ∣∣Xi k 2 due to their approximations to u* (W) always being greater than x> (Wk 一 Wyi).
For example, in one-vs-each (Titsias, 2016), Uj=(W) is approximated by log(1 + ex>(Wk-Wyi)) >
χ> (wk 一 Wyi). However, as they only approximate 遍(W) they cannot converge to the optimal Wj.
The goal of this paper is to design reliable and efficient SGD algorithms for optimizing the double-
sum formulation f(U, W) in (5). We propose two such methods: U-max (Section 3.1) and an
implementation of Implicit SGD (Section 3.2). But before we introduce these methods we should
establish that f is a good choice for the double-sum formulation.
2.3	Choice of double-sum formulation
The double-sum in (5) is different to that of Raman et al. (2016). Their formulation can be derived
by applying the convex conjugate substitution to (2) instead of (3). The resulting equations are
L(W) = minu n N PN=I K-1 Pk = yi fik (U, W )0 + N where
fik(u, W) = N (ui - x>Wyi + ex>Wyi-ui + (K - 1)ex>Wk-Ui) + 家户”|心川||2 + βk∣∣Wkk2)
(8)
3The convergence rate of SGD is inversely proportional to the second moment of its gradients (Lacoste-
Julien et al., 2012).
4The same problems arise if we approach optimizing (3) via stochastic composition optimization (Wang
et al., 2016). As is shown in Appendix B, stochastic composition optimization yields near-identical expressions
for the stochastic gradients in (7) and has the same stability issues.
3
Under review as a conference paper at ICLR 2018
一一	一 -	-	. ,   .	-	, ■ - >	— > - ― *、
and the optimal solution for Ui is Uj=(W*) = log(Pk=ι exi wk).
Although both double-sum formulations can be used as a basis for SGD, our formulation tends to
have smaller magnitude stochastic gradients and hence faster convergence. To see this, note that
typically χ>Wyi = argmaXk{χ> Wk} and so the Uj, χ>Wyi and exa wy -Ui terms in (8) are of the
greatest magnitude. Although at optimality these terms should roughly cancel, this will not be the
case during the early stages of optimization, leading to stochastic gradients of large magnitude. In
contrast the function fik in (6) only has xi>Wyi appearing as a negative exponent, and so if xi> Wyi is
large then the magnitude of the stochastic gradients will be small. In Section 4 we present numerical
results confirming that our double-sum formulation leads to faster convergence.
3	Stable SGD methods
3.1	U-max method
As explained in Section 2.2, vanilla SGD has large gradients when Ui xi> (Wk - Wyi). This
can only occur when Ui is less than its optimum value for the current W, since Ui= (W) = log(1 +
Pj6=y exi> (wk -wyi) ) ≥ xi> (Wk - Wyi). A simple remedy is to set Ui = log(1 + exi> (wk -wyi) )
whenever Ui xi>(Wk - Wyi). Since log(1 + exi> (wk-wyi)) > xi>(Wk - Wyi) this guarantees that
Ui > xi>(Wk - Wyi) and so the gradients are bounded. It also brings Ui closer5 to its optimal value
for the current W and thereby decreases the the objective f (U, W).
This is exactly the mechanism behind the U-max algorithm — see Algorithm 1 in Appendix C for
its pseudocode. U-max is the same as vanilla SGD except for two modifications: (a) Ui is set equal
to log(1 + exi> (wk-wyi)) whenever Ui ≤ log(1 + exi> (wk-wyi)) - δ for some threshold δ > 0, (b)
Ui is projected onto [0, BU], and W onto {W : kWk2 ≤ BW}, where BU and BW are set so that
the optimal Ui= ∈ [0, BU] and the optimal W = satisfies kW = k2 ≤ BW . See Appendix A for more
details on how to set BU and BW .
Theorem 1. Let Bf = max∣∣w∣∣2≤b2/,o≤u≤bu, maxjk ∣∣Vfik(u, W)k2. Suppose a learning rate
ηt ≤ δ2 /(4Bf2), then U-max with threshold δ converges to the optimum of (2), and the rate is at
least as fast as SGD with same learning rate, in expectation.
Proof. The proof is provided in Appendix D.	□
U-max directly resolves the problem of extremely large gradients. Modification (a) ensures that
δ ≥ xi>(Wk - Wyi) - Ui (otherwise Ui would be increased to log(1 + exi> (wk-wyi))) and so the
magnitude of the U-max gradients are bounded above by N(K - 1)eδ ∣xi ∣2 .
In U-max there is a trade-off between the gradient magnitude and learning rate that is controlled
by δ. For Theorem 1 to apply We require that the learning rate η ≤ δ2∕(4Bf). A small δ yields
small magnitude gradients, which makes convergence fast, but necessitates a small ηt, which makes
convergence sloW.
3.2	Implicit SGD
Another method that solves the large gradient problem is Implicit SGD6 (Bertsekas, 2011; Toulis
et al., 2016). Implicit SGD uses the update equation
θ(t+1) = θ(t) - ηtVf(θ(t+1),ξt),	(9)
Where θ(t) is the value of the tth iterate, f is the function We seek to minimize and ξt is a random
variable controlling the stochastic gradient such that Vf (θ) = Eξt [Vf (θ, ξt)]. The update (9)
differs from vanilla SGD in that θ(t+1) appears on both the left and right side of the equation,
5Since Ui < x> (Wk - Wyi) < log(1 + ex>(Wk-Wyi)) < log(l + Pjjφy. ex>(Wk-Wyi))=谒(W).
6Also knoWn to as an “incremental proximal algorithm” (Bertsekas, 2011).
4
Under review as a conference paper at ICLR 2018
whereas in vanilla SGD it appears only on the left side. In our case θ = (u, W ) and ξt = (it, kt)
With Vf (θ(t+1),ξt) = Vfit,kt(u(t+1),W(t+1)).
Although Implicit SGD has similar convergence rates to vanilla SGD, it has other properties that can
make it preferable over vanilla SGD. It is knoWn to be more robust to the learning rate (Toulis et al.,
2016), Which important since a good value for the learning rate is never knoWn a priori. Another
property, Which is of particular interest to our problem, is that it has smaller step sizes.
Proposition 1. Consider applying Implicit SGD to optimizing f(θ) = Eξ [f (θ, ξ)] where f(θ, ξ) is
m-strongly convex for all ξ. Then
kVf (θ(t+1), ξt)k2 ≤ kVf (θ(t), ξt)k2 - mkθ(t+1) - θ(t)k2
and so the Implicit SGD step size is smaller than that of vanilla SGD.
Proof. The proof is provided in Appendix E.	□
The bound in Proposition 1 can be tightened for our particular problem. Unlike vanilla SGD Whose
step size magnitude is exponential in xi>(wk - wyi) - ui, as shoWn in (7), for Implicit SGD the step
size is asymptotically linear in xi>(wk - wyi) - ui. This effectively guarantees that Implicit SGD
cannot suffer from computational overfloW.
Proposition 2. Consider the Implicit SGD algorithm where in each iteration only one datapoint i
and one class k 6= yi is sampled and there is no ridge regularization. The magnitude of its step size
in w is O(xi> (wk - wyi ) - ui).
Proof. The proof is provided in Appendix F.2.	□
The difficulty in applying Implicit SGD is that in each iteration one has to compute a solution to (9).
The tractability of this procedure is problem dependent. We shoW that computing a solution to (9) is
indeed tractable for the problem considered in this paper. The details of these mechanisms are laid
out in full in Appendix F.
Proposition 3. Consider the Implicit SGD algorithm where in each iteration n datapoints and m
classes are sampled. Then the Implicit SGD update θ(t+1) can be computed to within accuracy in
runtime O(n(n + m)(D + n log(-1))).
Proof. The proof is provided in Appendix F.3.	□
In Proposition 3 the log(-1) factor comes from applying a first order method to solve the strongly
convex Implicit SGD update equation. It may be the case that performing this optimization is more
expensive than computing the xi>wk inner products, and so each iteration of Implicit SGD may be
significantly sloWer than that of vanilla SGD or U-max. HoWever, in the special case of n = m = 1
We can use the bisection method to give an explicit upper bound on the optimization cost.
Proposition 4. Consider the Implicit SGD algorithm with learning rate η where in each iteration
only one datapoint i and one class k 6= yi is sampled and there is no ridge regularization. Then
the Implicit SGD iterate θ(t+1) can be computed to within accuracy with only two D-dimensional
vector inner products and at most log2(-1) + log2 (|xi> (wk -wyi) -ui| +2ηNkxi k22 + log(K - 1))
bisection method function evaluations.
Proof. The proof is provided in Appendix F.1	□
For any reasonably large dimension D, the cost of the tWo D-dimensional vector inner products Will
outWeigh the cost of the bisection, and Implicit SGD Will have roughly the same speed per iteration
as vanilla SGD or U-max.
In summary, Implicit SGD is robust to the learning rate, does not have overfloW issues and its updates
can be computed in roughly the same time as vanilla SGD.
5
Under review as a conference paper at ICLR 2018
Table 1: Datasets with a summary of their properties. Where the number of classes, dimension or
number of examples has been altered, the original value is displayed in brackets.
DATASET	CLASSES	DIMENSION	EXAMPLES
MNIST	10	780	60,000
Bibtex	147 (159)	1,836	4,880
Delicious	350 (983)	500	12,920
Eurlex	838 (3,993)	5,000	15,539
AmazonCat-13K	2,709 (2,919)	10,000 (203,882)	100,000 (1,186,239)
Wiki10	4,021 (30,938)	10,000 (101,938)	14,146
Wiki-small	18,207 (28,955)	10,000 (2,085,164)	90,737 (342,664)
4	Experiments
Two sets of experiments were conducted to assess the performance of the proposed methods. The
first compares U-max and Implicit SGD to the state-of-the-art over seven real world datasets. The
second investigates the difference in performance between the two double-sum formulations dis-
cussed in Section 2.3. We begin by specifying the experimental setup and then move onto the
results.
4.1	Experimental setup
Data. We used the MNIST, Bibtex, Delicious, Eurlex, AmazonCat-13K, Wiki10, and Wiki-small
datasets7, the properties of which are summarized in Table 1. Most of the datasets are multi-label
and, as is standard practice (Titsias, 2016), we took the first label as being the true label and discarded
the remaining labels. To make the computation more manageable, we truncated the number of
features to be at most 10,000 and the training and test size to be at most 100,000. If, as a result of
the dimension truncation, a datapoint had no non-zero features then it was discarded. The features
of each dataset were normalized to have unit L2 norm. All of the datasets were pre-separated into
training and test sets. We only focus on the performance on the algorithms on the training set, as the
goal in this paper is to investigate how best to optimize the softmax likelihood, which is given over
the training set.
Algorithms. We compared our algorithms to the state-of-the-art methods for optimizing the softmax
which have runtime O(D) per iteration8. The competitors include Noise Contrastive Estimation
(NCE) (Mnih & Teh, 2012), Importance Sampling (IS)(Bengio & Senecal, 2008) and One-Vs-Each
(OVE) (Titsias, 2016). Note that these methods are all biased and will not converge to the optimal
softmax MLE, but something close to it. For these algorithms we set n = 100, m = 5, which are
standard settings9. For Implicit SGD we chose to implement the version in Proposition 4 which has
n = 1, m = 1. Likewise for U-max we set n = 1, m = 1 and the threshold parameter δ = 1. The
ridge regularization parameter μ was set to zero for all algorithms.
Epochs and losses. Each algorithm is run for 50 epochs on each dataset. The learning rate is
decreased by a factor of 0.9 each epoch. Both the prediction error and log-loss (2) are recorded at
the end of 10 evenly spaced epochs over the 50 epochs.
Learning rate. The magnitude of the gradient differs in each algorithm, due to either under- or over-
estimating the log-sum derivative from (2). To set a reasonable learning rate for each algorithm on
7All of the datasets were downloaded from http://manikvarma.org/downloads/XC/
XMLRepository.html, except Wiki-small which was obtained from http://lshtc.iit.
demokritos.gr/.
8Raman et al. (2016) have runtime O(NKD) per epoch, which is equivalent to O(KD) per iteration. This
is a factor of K slower than the methods we compare against.
9We also experimented setting n = 1, m = 1 in these methods and there was virtually no difference except
the runtime was slower. For example, in Appendix G we plot the performance of NCE with n = 1, m = 1
and n = 100, m = 5 applied to the Eurlex dataset for different learning rates and there is very little difference
between the two.
6
Under review as a conference paper at ICLR 2018
Table 2: Tuned initial learning rates for each algorithm on each dataset. The learning rate in
100,±1,±2,±3 with the lowest log-loss after 50 epochs using only 10% of the data is displayed.
Vanilla SGD applied to AmazonCat, Wiki10 and Wiki-small suffered from overflow with a learning
rate of 10-3, but was stable with smaller learning rates (the largest learning rate for which it was
stable is displayed).
DATASET	OVE	NCE	IS	Vanilla	U-max	Implicit
MNIST	101	101	101	10-2	101	10-1
Bibtex	102	102	102	10-2	10-1	101
Delicious	101	103	103	10-3	10-2	10-2
Eurlex	10-1	102	102	10-3	10-1	101
AmazonCat	101	103	103	10-5	10-2	10-3
Wiki10	10-2	103	102	10-4	10-2	100
Wiki-small	103	103	103	10-4	10-3	10-3
AmazonCat
⅛z⅛⅛⅛F⅛≠⅛=⅛=⅛=⅛⅛⅛⅛
7 -
WikilO
Figure 1:	The x-axis is the number of epochs and the y-axis is the log-loss from (2) calculated at the
current value of W.
each dataset, We ran them on 10% of the training data with initial learning rates η = 100,±1,±2,±3.
The learning rate with the best performance after 50 epochs is then used when the algorithm is
applied to the full dataset. The tuned learning rates are presented in Table 2. Note that vanilla SGD
requires a very small learning rate, otherwise it suffered from overflow.
4.2 RESULTS
Comparison to state-of-the-art. Plots of the performance of the algorithms on each dataset are
displayed in Figure 1 with the relative performance compared to Implicit SGD given in Table 3. The
Implicit SGD method has the best performance on virtually all datasets. Not only does it converge
faster in the first few epochs, it also converges to the optimal MLE (unlike the biased methods that
prematurely plateau). On average after 50 epochs, Implicit SGD’s log-loss is a factor of 4.29 lower
than the previous state-of-the-art. The U-max algorithm also outperforms the previous state-of-the-
art on most datasets. U-max performs better than Implicit SGD on AmazonCat, although in general
Implicit SGD has superior performance. Vanilla SGD’s performance is better than the previous
state-of-the-art but worse than U-max and Implicit SGD. The difference in performance between
vanilla SGD and U-max can largely be explained by vanilla SGD requiring a smaller learning rate
to avoid computational overflow.
7
Under review as a conference paper at ICLR 2018
Table 3: Relative log-loss. The values for each dataset are normalized by dividing by the corre-
sponding Implicit SGD log-loss. The lowest log-loss for each dataset is in bold.
DATASET	oVE	NCE	IS	VANILLA-SGD	U-MAX.	IMPLICIT SGD
MNIST	5.73	6.05	5.74	1.43	1.56	1.00
Bibtex	29.03	28.52	32.71	15.18	9.77	1.00
Delicious	1.90	1.91	1.89	1.25	1.11	1.00
Eurlex	6.47	6.39	6.38	3.59	2.11	1.00
AmazonCat	2.12	2.15	2.12	1.47	0.98	1.00
Wiki10	7.04	7.13	6.97	5.99	2.39	1.00
Wiki-small	1.02	1.36	1.35	1.15	1.03	1.00
Average	7.62	7.64	8.17	4.29	2.71	1.00
Double-sum formulations
-∙- Proposed
—Raman et al.
Learning rates
→- 100.0
→- 10.0
1.0
0.1
0.01
→- 0.001
→- 0.0001
Figure 2:	Log-loss of U-max on EUrleX for different learning rates with our proposed double-sum
formulation and that of Raman et al. (2016).
The sensitivity of each method to the initial learning rate can be seen in Appendix G, where the re-
sults of running each method on the Eurlex dataset with learning rates η = 100,±1,±2,±3 is presented.
The results are consistent with those in Figure 1, with Implicit SGD having the best performance for
most learning rate settings. For learning rates η = 103,4 the U-max log-loss is extremely large. This
can be explained by Theorem 1, which does not guarantee convergence for U-max if the learning
rate is too high.
Comparison of double-sum formulations. Figure 2 illustrates the performance on the Eurlex
dataset of U-max using the proposed double-sum in (6) compared to U-max using the double-sum
of Raman et al. (2016) in (8). The proposed double-sum clearly outperforms for all10 learning rates
η = i00,±1,±2,-3,-4, with its 50th-epoch log-loss being 3.08 times lower on average. This supports
the argument from Section 2.3 that SGD methods applied to the proposed double-sum have smaller
magnitude gradients and converge faster.
5	Conclusion
In this paper we have presented the U-max and Implicit SGD algorithms for optimizing the softmax
likelihood. These are the first algorithms that require only O(D) computation per iteration (without
extra work at the end of each epoch) that converge to the optimal softmax MLE. Implicit SGD can
be efficiently implemented and clearly out-performs the previous state-of-the-art on seven real world
datasets. The result is a new method that enables optimizing the softmax for extremely large number
of samples and classes.
So far Implicit SGD has only been applied to the simple softmax, but could also be applied to
any neural network where the final layer is the softmax. Applying Implicit SGD to word2vec type
models, which can be viewed as softmaxes where both X and W are parameters to be fit, might be
particularly fruitful.
10The learning rates η = 103,4 are not displayed in the Figure 2 for visualization purposes. It had similar
behavior as η = 102.
8
Under review as a conference paper at ICLR 2018
References
Jacob Andreas and Dan Klein. When and why are log-linear models self-normalizing? In HLT-NAACL, pp.
244-249, 2015.
Yoshua Bengio and Jean-Sebastien SeneCaL Adaptive importance sampling to accelerate training of a neural
probabilistic language model. IEEE Transactions on Neural Networks, 19(4):713-722, 2008.
Yoshua Bengio, Jean-Sebastien SeneCaL et al. Quick training of probabilistic neural nets by importance sam-
pling. In AISTATS, 2003.
Dimitri P Bertsekas. Incremental proximal methods for large scale convex optimization. Mathematical pro-
gramming, 129(2):163, 2011.
KH Bhatia, PJ Jain, and M Varma. The extreme classification repository: multi-label datasets & code. URL
http://manikvarma.org/downloads/XC/XMLRepository.html.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson.
One billion word benchmark for measuring progress in statistical language modeling. arXiv:1312.3005,
2013.
Hal Daume III, Nikos Karampatziakis, John Langford, and Paul Mineiro. Logarithmic time one-against-some.
arXiv:1606.04988, 2016.
Alexandre de BrebiSSOn and Pascal Vincent. An exploration of softmax alternatives belonging to the spherical
loss family. arXiv:1511.05042, 2015.
Edouard Grave, Armand Joulin, Moustapha Cisse, David Grangier, and Herve Jegou. Efficient softmax ap-
proximation for GPUs. arXiv:1609.04309, 2016.
Bikash Joshi, Massih-Reza Amini, Ioannis Partalas, Franck Iutzeler, and Yury Maximov. Aggressive sampling
for multi-class to binary reduction with applications to text classification. arXiv:1701.06511, 2017.
Balaji Krishnapuram, Lawrence Carin, Mario AT Figueiredo, and Alexander J Hartemink. Sparse multinomial
logistic regression: Fast algorithms and generalization bounds. IEEE transactions on pattern analysis and
machine intelligence, 27(6):957-968, 2005.
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an O(1/t) conver-
gence rate for the projected stochastic subgradient method. arXiv:1212.2002, 2012.
Andre FT Martins and Ramon Fernandez Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classification. CoRR, abs/1602.02068, 2016.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models.
arXiv:1206.6426, 2012.
Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric Gaussier, Ion
Androutsopoulos, Massih-Reza Amini, and Patrick Galinari. LSHTC: A benchmark for large-scale text
classification. arXiv:1503.08581, 2015.
Parameswaran Raman, Shin Matsushima, Xinhua Zhang, Hyokun Yun, and SVN Vishwanathan. DS-MLR:
Exploiting double separability for scaling up distributed multinomial logistic regression. arXiv:1604.04706,
2016.
Michalis K Titsias. One-vs-each approximation to softmax for scalable estimation of probabilities.
arXiv:1609.07410, 2016.
Panos Toulis, Dustin Tran, and Edo Airoldi. Towards stability and optimality in stochastic gradient descent. In
Artificial Intelligence and Statistics, pp. 1290-1298, 2016.
Pascal Vincent, Alexandre de BrebiSson, and Xavier Bouthillier. Efficient exact gradient update for training
deep networks with very large sparse targets. In Advances in Neural Information Processing Systems, pp.
1108-1116, 2015.
Mengdi Wang, Ji Liu, and Ethan Fang. Accelerating stochastic composition optimization. In Advances in
Neural Information Processing Systems, pp. 1714-1722, 2016.
9
Under review as a conference paper at ICLR 2018
A Proof of variable bounds and strong convexity
We first establish that the optimal values of u and W are bounded. Next, we show that within these
bounds the objective is strongly convex and its gradients are bounded.
Lemma 1 ((Raman et al., 2016)). The optimal value of W is bounded as ∣∣W*∣∣2 ≤ BW where
BW = 2 N log(K).
Proof.
-Nlog(K) = L(0) ≤ L(W*) ≤ — 2∣W*k2
Rearranging gives the desired result.	□
Lemma 2. The optimal value of Ui is bounded as 遍 ≤ Bu where Bu = log(1 + (K - 1)e2BxBw)
and Bx = maxi{∣xi ∣2}
Proof.
C *
Ui
= log(1 + X exi>(wk-wyi))
k6=yi
≤ log(1 + X ekxik2(kwkk2+kwyik2))
k6=yi
≤ log(1 + X e2BxBw)
k6=yi
= log(1 + (K - 1)e2BxBw)
□
Lemma 3. If ∣W ∣22 ≤ BW2 and Ui ≤ Bu then f(U, W) is strongly convex with convexity constant
greater than or equal to min{exp(-Bu), μ}.
Proof. Let us rewrite f as
N
f(u, W) = X ui + e-ui + X ex>(Wk-Wyi)-u + μ ∣∣W ∣∣2
i=1	k6=yi
N
=X a>θ + e-ui + X e% + 2 ∣W k2 ∙
i=1	k6=yi
where θ = (U>, w1>, ..., wk>) ∈ RN+KD with ai and bik being appropriately defined. The Hessian
offis
N
v2f (θ) = X e-uieie> + X ebikθbikb> + μ ∙ diag{0N, 1kd}
i=1	k6=yi
where ei is the ith canonical basis vector, 0N is an N -dimensional vector of zeros and 1KD is a
KD-dimensional vector of ones. It follows that
V2f(θ)占 I ∙ min{ min {e-ui},μ}
0≤u≤Bu
=I ∙ min{exp(-Bu), μ}
0.
□
Lemma 4. If ∣W ∣22 ≤ BW2 and Ui ≤ Bu then the 2-norm of both the gradient of f and each
stochastic gradient fik are bounded by
Bf = N max{1, eBu - 1} + 2(N eBu Bx + μmax{βk}BW).
k
10
Under review as a conference paper at ICLR 2018
Proof. By Jensen’s inequality
max	lVf (u, W)l2
kW k22≤BW2 ,0≤u≤Bu
max
kW k22 ≤BW2 ,0≤u≤Bu
max
kW k22 ≤BW2 ,0≤u≤Bu
max
kW k22 ≤BW2 ,0≤u≤Bu
l∣vEik fik(u, W )k2
Eik ∣Vfik(u,W )∣∣2
max lVfik(u, W)l2.
ik
≤
≤
Using the results from Lemmas 1 and 2 and the definition of fik from (6),
lVuifik(u,W)l2 = lN1-e-ui - (K - 1)exi>(wk-wyi)-ui) l2
= N|1 -e-ui(1 + (K - 1)exi>(wk-wyi))|
≤ N max{1, (1 + (K - 1)ekxik2(kwkk2+kwyik2)) - 1}
≤ N max{1, eBu - 1}
and for j indexing either the sampled class k 6= yi or the true label yi ,
kVwjfik(u,W)∣2 = k± N(K - 1)ex>(Wk-Wyi)-uiXi + μβjWj ∣2
≤ N(K - 1)ekχik2(kwkk2+kwyik2)∣∣Xi∣∣2 + μβj∣∣Wj∣∣2
≤ NeBu Bx + μ max{βk}Bw.
k
Letting
Bf = N max{1, eBu _ 1} + 2(NeBu Bx + μ max{βk }Bw)
k
we have
lVfik(u,W)l2 ≤ lVuifik(u,W)l2+lVWkfik(u,W)l2+lVWyifik(u,W)l2 =Bf.
In conclusion:
max	lVf (u, W)l2 ≤ max
kWk22≤BW2 ,0≤u≤Bu	kWk22≤BW2 ,ui≤Bu
max lVfik (u, W)l2 ≤ Bf.
ik
□
B Stochastic Composition Optimization
We Can write the equation for L(W) from (3) as (where We have set μ = 0 for notational simplicity),
N
L(W) = - X log(1 + X exi> (Wk -Wyi))
i=1	k6=yi
= Ei [hi(Ek [gk (W)])]
where i 〜Unif ({1,…,N}), k 〜Unif ({1,…,K}), hi(v) ∈ R, gk(W) ∈ RN and
hi(v) = -N log(1 + ei>v)
Kexi>(Wk-Wyi)
[gk (W)]i =	0
if k 6= yi
otherwise
Here ei>v = vi ∈ R is a variable that is explicitly kept track of with vi ≈ Ek [gk (W)]i =
Pk6=y exi>(Wk-Wyi ) (with exact equality in the limit as t → ∞). Clearly vi in stochastic com-
position optimization has a similar role as Ui has in our formulation for f in (5).
If i, k are sampled with k 6= yi in stochastic composition optimization then the updates are of the
form (Wang et al., 2016)
exi> (zk -zyi)
w7,. = w7,. + ηtNK-------------Xi
yi	yi t	i
1 + vi
exi> (zk -zyi)
Wk = Wk 一 ηtNK—----------------Xi,
1 + vi
where zk is a smoothed value of Wk. These updates have the same numerical instability issues as
vanilla SGD on f in (5): it is possible that eɪzk》1 where ideally we should have 0 ≤ eɪzk ≤ 1.
1+vi	1+vi
11
Under review as a conference paper at ICLR 2018
C U-max pseudocode
Algorithm 1: U-max
Input : Data D = {(yi, xi) : yi ∈ {1, . . . , K}, xi ∈ Rd}iN=1, number of classes K, number of
datapoints N, learning rate 小,class sampling probability βk = η卜十(改-Nk)(k-i),
threshold parameter δ > 0, bound BW on W such that kW k2 ≤ BW and bound Bu on u
such that ui ≤ Bu for i = 1, ..., N
Output: W
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Initialize
for k = 1 to K do
I Wk - 0
end
for i = 1 to N do
I Ui — log(K)
end
Run SGD
for t = 1 to T do
Sample indices
i 〜Unif ({1,…，N})
k 〜Unif ({1,…,K} -{yi})
Increase Ui
if Ui < log(1 + exi> (wk -wyi )) - δ then
I ui — log(1 + ex> (Wk-Wyi))
SGD step
Wk 一 Wk - ηt[N(K - 1)ex>(Wk-Wyi)-uiXi + μβkWk]
Wyi — Wyi- ηt[-N(K - I)ex>(Wk-Wyi)-uiXi + μ%wy」
ui — Ui - ηt[N(1 - e-ui - (K - 1)ex>(Wk-Wyi)-Ui)]
Projection
Wk — Wk ∙ min{1,Bw/∣∣Wk∣∣2}
Wyi J Wyi ∙ min{ 1,BW/kWyi ∣2}
Ui J max{0, min{Bu, Ui}}
end
D Proof of convergence of U-max method
In this section we will prove the claim made in Theorem 1, that U-max converges to the softmax
optimum. Before proving the theorem, we will need a lemma.
Lemma 5. For any δ > 0, ifUi ≤ log(1+exi>(Wk-Wyi)) -δ then setting Ui = log(1 +exi>(Wk-Wyi))
decreases f (u, W) by at least δ2∕2.
Proof. As in Lemma 3, let θ = (U>, W1>, ..., Wk>) ∈ RN+KD. Then setting Ui = log(1 +
exi>(Wk-Wyi)) is equivalent to setting θ = θ + ∆ei where ei is the ith canonical basis vector and
∆ = log(1 + exi>(Wk-Wyi)) - Ui ≥ δ. By a second order Taylor series expansion
∆2
f (θ) - f(θ + ∆ei) ≥ Vf (θ + ∆ei)>ei∆ + ɪe> V2f (θ + λ∆e,)ei	(10)
for some λ ∈ [0,1]. Since the optimal value of Ui for a given value of W is 遍(W) = log(1 +
Pk6=y exi> (Wk -Wyi)) ≥ log(1+exi>(Wk-Wyi)), we must have Vf(θ+∆ei)>ei ≤ 0. From Lemma 3
12
Under review as a conference paper at ICLR 2018
we also know that
e>V2f(θ + λ∆ei)ei = exp(-(% + λ∆)) + X ex>(Wk-Wyi)-(Ui+必)
k6=yi
= exp(-λ∆)e-ui (1 + X exi>(Wk-Wyi))
k6=yi
= exp(-λ∆) exp(-(log(1 + exi>(Wk-Wyi)) - ∆))(1 + X exi>(Wk-Wyi))
k6=yi
≥ exp(∆ - λ∆)
≥ exp(∆ - ∆)
= 1.
Putting in bounds for the gradient and Hessian terms in (10),
∆2	δ2
f (θ) - f(θ + δQ ≥ F ≥ y -
□
Now we are in a position to prove Theorem 1.
Proof of Theorem 1. Let θ(t) = (u(t), W(t)) ∈ Θ denote the value of the tth iterate. Here Θ = {θ :
kW k22 ≤ BW2 , ui ≤ Bu} is a convex set containing the optimal value of f (θ).
Let πi(δ) (θ) denote the operation of setting ui = log(1 + exi> (Wk -Wyi)) if ui ≤ log(1 +
exi>(Wk-Wyi)) - δ. If indices i, k are sampled for the stochastic gradient and ui ≤ log(1 +
exi>(Wk-Wyi)) - δ, then the value of f at the t + 1st iterate is bounded as
f(θ(t+1)) = f(πi(θ(t)) - ηtVfik(πi(θ(t))))
≤ f (πi(θ(t))) + max kηtVfik(πi(θ))k2 max kVf(θ)k2
θ∈Θ	θ∈Θ
≤ f (πi(θ(t))) + ηtBf2
≤ f (θ(t)) - δ2∕2 + ηtBf
≤ f(θ⑴-ηtVfik(θ㈤))-δ2∕2 + 2ηtBf
≤ f(θ(t) - ηtVfik(θ(t))),
since ηt ≤ δ2∕(4Bf2) by assumption. Alternatively if ui ≥ log(1 + exi> (Wk -Wyi)) - δ then
f(θ(t+1)) =f(πi(θ(t)) - ηtVfik(πi(θ(t))))
= f(θ(t) - ηtVfik (θ(t))).
Either way f(θ(t+1)) ≤ f (θ(t) - ηtVfik(θ(t))). Taking expectations with respect to i, k,
Eik [f(θ(t+1))] ≤ Eik [f(θ(t) - ηtVfik(θ(t)))].
Finally let P denote the projection of θ onto Θ. Since Θ is a convex set containing the optimum we
have f(P (θ)) ≤ f(θ) for any θ, and so
Eik[f(P(θ(t+1)))] ≤Eik[f(θ(t) - ηtVfik(θ(t)))],
which shows that the rate of convergence in expectation of U-max is at least as fast as that of standard
SGD.
□
13
Under review as a conference paper at ICLR 2018
E	Proof of general Implicit SGD gradient b ound
Proof of Theorem 2. Let f (θ, ξ) be m-strongly convex for all ξ. The vanilla SGD step size is
ηtkVf(θ(t),ξt)∣∣2 where η is the learning rate for the tth iteration. The Implicit SGD step size
is ηt∣∣Vf(θ(t+1),ξt)k2 where θ(t+1) satisfies θ(t+1) = θ(t) 一 ηtVf(θ(t+1)，ξt). Rearranging,
Vf (θ(t+1),ξt) = (θ(t)- θ(t+1))∕ηt and so it must be the case that Vf (θ(t+1),ξt)> (θ(t)- θ(t+1))=
kVf (θ(t+1) ,ξt)k2kθ(t) — θ(t+1)k2.
Our desired result follows:
kVf(θ(t),ξt)k2 ≥ Vd+1))
Vf (θ(t+1))>(θ(t) - θ(t+1)) + mkθ⑴一 θ(t+1)k2
≥	∣∣θ(t) - θ(t+1)k2
_ kVf(θ(t+1))k2kθ(t) - θ(t+1)k2 + m∣θ(t) - θ(t+1)k2
=	∣∣θ(t) - θ(t+1)∣2
= kVf (θ(t+1))k2 + mkθ(t) - θ(t+1)k2
where the first inequality is by Cauchy-Schwarz and the second inequality by strong convexity.
□
F Update equations for implicit SGD
In this section we will derive the updates for Implicit SGD. We will first consider the simplest case
where only one datapoint (xi, yi) and a single class is sampled in each iteration with no regularizer.
Then we will derive the more complicated update for when there are multiple datapoints and sampled
classes with a regularizer.
F.1 Single datapoint, single class, no regularizer
Equation (6) for the stochastic gradient for a single datapoint and single class with μ = 0 is
fik(u,W) = N(ui + e-ui + (K - 1)exi>(wk-wyi)-ui).
The Implicit SGD update corresponds to finding the variables optimizing
min n 2ηfik(U, W) + ku - uk2 + IIW - W k2 o,
u,W
where η is the learning rate and the tilde refers to the value of the old iterate (Toulis et al., 2016, Eq.
6)	. Since fik is only a function ofui, wk, wyi the optimization reduces to
min	{2ηfik(ui,wk,WyJ + (u - ui)2 + ∣∣Wyi - Wya∣∣2 + ∣∣wk - Wk∣∣2}
ui,wk,wyi
= min	2ηN(ui + e-ui + (K - 1)exi>(wk-wyi)-ui)
ui,wk,wyi
+ (Ui - ui)2 + Ilwyi - Wyik2 + Ilwk - Wkk2 〉.
The optimal value of wk ,wyi must deviate from the old value wk, Wyi in the direction of xi. FUr-
thermore we can observe that the deviation ofwk must be exactly opposite that ofwyi, that is:
Wyi = Wyi + a
xi
xi
Wk= Wk- a而服
for some a ≥ 0. The optimization problem reduces to
min 22ηN(ui + e—u，+ (K - 1)ex>(Wk-Wyi)e-a-ui) + (ui - ui)2 + a2-ɪɪ
ui,a≥0	i	i i	2kxik22
(11)
(12)
14
Under review as a conference paper at ICLR 2018
We’ll approach this optimization problem by first solving for a as a function of ui and then optimize
over ui . Once the optimal value of ui has been found, we can calculate the corresponding optimal
value of a. Finally, substituting a into (11) will give us our updated value of W.
Solving for a
We solve for a by setting its derivative equal to zero in (12)
0 = ∂a(2ηN(Ui + e-ui + (K - 1)ex>(Wk-Wyi)e-a-ui) + (Ui- Ui) + a2-ɪ^ ]
2kxik2
=-2ηN(K - 1)ex>(Wk-Wyi)-Uie-a + a-1-2
kxi k2
⇔ aea = 2ηN(K - 1)∣∣Xik2ex>(Wk-Wyi)-Ui.	(13)
The solution for a can be written in terms of the principle branch of the Lambert W function P ,
a(ui) = P(2ηN(K - 1)kxi||2ex>(Wk-Wyi)-Ui)
= P(e
χ> (Wk-Wyi )-Ui+iog(2ηN (κ-i)kχik2))
(14)
Substituting the solution to a(Ui) into (12), we now only need minimize over Ui:
min 2 2ηNui + 2ηNL + 2ηN(K - 1)ex>(Wk-Wyih-a(Ui)-Ui +(％ - Ui)2 + a(ui)2
1
Ui
=min ∣2ηNui + 2ηNe-"i + a(ui)kXik-2 + (Ui - Uiy + a(ui)2^~^2 }
where we used the fact that e-P (z) = P(z)/z. The derivative with respect to Ui in (15) is
∂u 12ηNUi + 2ηNe-"i + a(Ui)kXik-2 + (Ui - Uiy + a(Ui)2^~1k2 }
=2ηN - 2ηNe-Ui + ∂%a(Ui)kXik-2 + 2(% - Ui) + 2q(%)9%aQi*' 产
=2ηN- 2旃€-的-1,UUi) kχi k-2+2(Ui- Ui)- (i+α(dk2
(15)
(16)
where to calculate ∂% a(Ui) We used the fact that ∂z P (z) = z(fP)z)) and So
∂Ui a(Ui )
____________________a(Ui)____________________gX> (Wk-Wyi )-Ui+log(2nN (K-1)kxi k 2)
ex> (Wk-Wyi )-Ui+log(2nN(K-I)kxik2)(i + a(Ui))
a(Ui)
1 + a(Ui)
Bisection method for Ui
We can solve for Ui using the bisection method. Below we show how to calculate the initial lower
and upper bounds of the bisection interval and prove that the size of the interval is bounded (which
ensures fast convergence).
Start by calculating the derivative in (16) at Ui = Ui. If the derivative is negative then the optimal Ui
is lower bounded by Ui. An upper bound is provided by
Ui = argmin 22ηN(U + e-ui + (K - 1)ex>(Wk-Wyi)e-a(Ui)-Ui) + (Ui- Ui)2 + a^
Ui	2kXi k2
≤ argmin 2 2ηN(Ui + e-ui + (K - 1)ex>(Wk-Wyi)e-ui) + (Ui - Ui)2
Ui
≤ argmin 2 2ηN(Ui + Iu + (K - 1)ex>(Wk-Wyi)e-U)
Ui
= log(1 + (K - 1)ex>(Wk-Wyi)).
15
Under review as a conference paper at ICLR 2018
In the first inequality we set a(ui) = 0, since by the envelop theorem the gradient ofui is monotoni-
Cally increasing in a. In the second inequality We used the assumption that Ui is lower bounded by Ui.
Thus if the derivative in(16) is negative at Ui = Ui then Ui ≤ Ui ≤ log(1 + (K - 1)ex> (Wk-Wyi)). If
(K — 1)ex>(Wk-Wyi) ≤ 1 then the size of the interval must be less than log(2), since Ui ≥ 0. Other-
wise the gap must be at most log(2(K - 1)ex> (Wk-Wyi)) - Ui = log(2(K -1))+ x> (Wk -Wyi) - Ui.
Either way, the gap is upper bounded by log(2(K - 1)) + |x> (Wk - Wyi) - U/.
Now let us consider if the derivative in (16) is positive at Ui = Ui. Then Ui is upper bounded by Ui.
Denoting a0 as the optimal value of a, we can lower bound Ui using (12)
Ui = argmin 2 2ηN(Ui + e-u + (K - 1)ex>(Wk-Wyi)e-a0-ui) + (Ui - Ui)2
ui
≥ argmin U Ui + e-"，+ (K - 1)ex> (Wk-Wyi Ierj。
ui
=log(1 + (K - 1)exp(x>(Wk - Wyi) - a0))
≥ iog(κ -1) + χ>(Wk - Wyi) - a
(17)
where the first inequality comes dropping the (Ui - Ui)2 term due to the assumption that Ui < Ui.
Recall (13),
a0ea0 = 2ηN(K - 1)|山|院”>(Wk-Wyi>-u.
The solution for a0 is strictly monotonically increasing as a function of the right side of the equation.
Thus replacing the right side with an upper bound on its value results in an upper bound on a0 .
Substituting the bound for Ui ,
a ≤ min{a : aea = 2ηN(K - 1)|出|舐">(Wk-Wyi)-(IOg(KT)+x>(Wk-Wyi)-a)}
= min{a : a = 2ηN kxi k22 }
= 2ηNkxik22.	(18)
Substituting this bound for a0 into (17) yields
Ui ≥ log(K - 1) + x>(Wk- Wyi)- 2ηNIlxik2.
Thus if the derivative in (16) is postive atUi = Ui then log(K - 1) + x>(Wk - Wyi)- 2ηN∣∣xi∣2 ≤
Ui ≤ Ui. The gap between the upper and lower bound is Ui-x> (Wk-Wyi)+2ηN kxi∣2-log(K-1).
In summary, for both cases of the sign of the derivative in (16) at Ui = Ui we are able to calculate a
lower and upper bound on the optimal value of Ui such that the gap between the bounds is at most
|Ui - x>(Wk - WyJI + 2ηNkxi∣2 + log(K - 1). This allows us to perform the bisection method
where for e > 0 level accuracy we require only log? (e-1)+log2 (∣Ui - x> (Wk - Wyi) ∣ + 2ηN∣xi 12 +
log(K - 1)) function evaluations.
F.2 B ound on step size
Here we will prove that the step size magnitude of Implicit SGD with a single datapoint and sampled
class with respect to W is bounded as O(x> (Wk - Wyi) - Ui). We will do so by considering the two
cases Ui ≥ Ui and Ui < Ui separately, where Ui denotes the optimal value of Ui in the Implicit SGD
update and Ui is its value at the previous iterate.
Case: Ui ≥ Ui
Let a0 denote the optimal value of a in the Implicit SGD update. From (14)
a0 = a(U0i )
= P(e
x> (Wk-Wyi )-ui + log(2nN (K-I) kxik2))
=p (ex> (Wk-Wyi )-ui+log(2ηN (K-I)kχik2))
Now using the fact that P(z) = O(log(z)),
a0 = O(x>(Wk - Wyi)- Ui + log(2ηN(K - 1)kxi∣∣2))
=O(x>(Wk - Wyi) - Ui)
16
Under review as a conference paper at ICLR 2018
Case: Ui < Ui
If Ui < Ui then We can lower bound a0 from (18) as
a0 ≤ 2ηNkxik22.
Combining cases
Putting together the two cases,
a = O(max{x>(Wk - Wyi)- Ui, 2ηNIlxik2})
=O(x> (Wk - Wyi) - Ui).
The actual step size in W is ±α2ɪf^ɪ. Since a is O(x> (Wk - Wyi) - Hi), the step size magnitude is
also O(x>(Wk - Wyi) - Ui).
F.3 Multiple datapoints, multiple classes
The Implicit SGD update when there are multiple datapoints, multiple classes, with a regularizer is
similar to the singe datapoint, singe class, no regularizer case described above. However, there are a
few significant differences. Firstly, we will require some pre-computation to find a low-dimensional
representation of the x values in each mini-batch. Secondly, we will integrate out Ui for each data-
point (notWk). And thirdly, since the dimensionality of the simplified optimization problem is large,
we’ll require first order or quasi-Newton methods to find the optimal solution.
F.3.1 Defining the mini-batch
The first step is to define our mini-batches of size n. We will do this by partitioning the datapoint
indices into sets Si,…,Sj with Sj = {j' : ' = 1,…,n} for j = 1,…,[N/n], SJ = {J' : ' =
1,...,N mod n}, Si ∩ Sj = 0 and ∪J=ιSj = {1,…,N}.
Next we define the set of classes Cj which can be sampled for the jth mini-batch. The set Cj is
defined to be all sets of m distinct classes that are not equal to any of the labels y for points in the
mini-batch, that is, Cj = {(ki,…,km) : k ∈ {1,…,K}, k = k`∀' ∈ {1,...,m} - {i}, k =
y ∀' ∈ Sj}.
Now we can write down our objective from (5) in terms ofan expectation of functions corresponding
to our mini-batches:
f(U,W)=E[fj,C(U,W)]
where j is sampled with probability pj = |Sj |/N and C is sampled uniformly from Cj and
fj,C(u, W) = p-i X (Ui + e-ui + X	ex>(Wk-Wyi)-u + K-n X ex>(Wk-Wyi)-u
i∈Sj	k∈Sj-{i}	m k∈C
+ μ X βk kWkk2.
k∈C ∪Sj
The value of the regularizing constant βk is such that E[I[k ∈ C ∪ Sj]βk] = 1, which requires that
1J
β-i = 1 - J XI[k = Sj](1 - K⅛).
F.3.2 Simplifying the Implicit SGD update equation
The Implicit SGD update corresponds to solving
mWn Qnj (U, W) + kU - Uk2 + k W - Wk2},
17
Under review as a conference paper at ICLR 2018
where η is the learning rate and the tilde refers to the value of the old iterate (Toulis et al., 2016,
Eq. 6). Since fj,C is only a function of uSj = {ui : i ∈ Sj } and Wj,C = {wk : k ∈ Sj ∪ C} the
optimization reduces to
min	n2ηfj,C(USj,Wj,C) + IIuSj - USjk2 + kWj,C - Wj,Ck20 .
uSj ,Wj,C
The next step is to analytically minimize the uSj terms. The optimization problem in (21) decom-
poses into a sum of separate optimization problems in ui for i ∈ Sj ,
min
ui
∣2ηp-1(ui + e-uidi) + (ui - ui)2
where
di(Wj,C) = 1 + X	exi>(wk-wyi) +
k∈Sj-{i}
K — n
m
Σ
k∈C
Setting the derivative of ui equal to zero yields the solution
ui(Wj,c) = Ui — ηp-1 + P(ηp-1di(Wj,c)exp(ηp-1 - ui))
where P is the principle branch of the Lambert W function. Substituting this solution into our
optimization problem and simplifying yields
min X X(I + p(ηp-'di(wj,c)exp(ηp-1 - Ui)))2 + kWj,c - Wj,cI∣2 + μ X 见皿假)
j,C i∈Sj	k∈C∪Sj
(19)
where we have used the identity e-P (z) = P(z)/z. We can decompose (19) into two parts by
splitting Wj,C = Wjk,C + Wj⊥,C, its components parallel and perpendicular to the span of {xi :
i ∈ Sj} respectively. Since the leading term in (19) only depends on Wjk,C, the two resulting
sub-problems are
min ]χ(1 + P(ηp-1di(wj,c)exp(ηp-1 — Ui)))2 + kwj,c - Wj,ck2 + μ X βkkwkk2)
Wj,C i∈Sj	k∈C∪Sj
min IkWj⊥c -W⊥ck2 + 2 X βkkw⊥k21	QO)
Wj,C	k∈C∪Sj
Let us focus on the perpendicular component first. Simple calculus yields the optimal value wk⊥ =
ι+μβk∕2 w⊥ fork ∈ Sj ∪ C.
Moving onto the parallel component, let the span of {xi : i ∈ Sj } have an orthonormal basis11
Vj = (vj1 , ..., vjn ) ∈ RD×n with xi = Vj bi for some bi ∈ Rn. With this basis we can write
wk = Wk + Vj ak for ak ∈ Rn which reduces the parallel component optimization problem to12
min { X (1 + P (ZijC (Aj,C )))2 + X (I + μk )kak k2 + μβ w>Vj ak),	(21)
j,C i∈Sj	k∈Sj ∪C
where Aj,C = {ak : k ∈ Sj ∪ C} ∈ R(n+m)×n and
ZijC(Aj,c) = ηp-1 exp(ηp-1) 卜xp(-Ui) + X	exr(wk-wyi)-uieb>(ak-ayi)
k∈Sj -{i}
+ K — n X ex>(Wk-Wyi )-⅛eb>(ak-ayi ) ʌ .
m
k∈C
11We have assumed here that dim(span({xi : i ∈ Sj })) = n, which will be most often the case. If the
dimension of the span is lower than n then let Vj be of dimension D × dim(span({xi : i ∈ Sj })).
12Note that We have used Wk instead of Wk in writing the parallel component optimization problem. This
does not make a difference as Wk always appears as an inner product with a vector in the span of {xi : i ∈ Sj }.
18
Under review as a conference paper at ICLR 2018
The eb> (ak-ayi) factors come from
x>wk = XJ(Wk + a>Vj)
=XJwk + (Vjbi )>Vj a，k
=Xjwk + b > V> Vj ak
=Xjwk + b>αk,
since Vj is an orthonormal basis.
F.3.3 Optimizing THE Implicit SGD update equation
To optimize (21) we need to be able to take the derivative:
Vag ( X (1 + P (ZijC (Aj,C )))2 + X (1 + μ2k )∣Iak I∣2 + μβk wjVj ak j
∖i∈Sj	k∈Sj UC	)
=X 2(1 + P (ZijC (Aj,C )))dzjc (Aj,c )P (ZijC (Aj,C ))Vag zijC (Aj,C )
i∈Sj
+ (2 + μβ')ag + μβ'w~> Vj
=iX2(I+P(ZjC (Aj,C))) jj⅛⅛⅛⅛j>Vag ZjC (Aj'C)
i∈Sj
+ (2 + μβg)ag + μβ'w> Vj
=X 2P(ZijC(Aj，C" VagZijC(Aj,C) + (2 + μβe)ae + μβew]Vj
i⅛	ZijC (Aj,c)
=X 2e-P(ZijC(Aj,C"VagZijC(Aj,c) + (2 + μβg)ag + μβ'w>Vj
i∈Sj
where we used that ∂z P (z) = z(fp(z)) and e-p (z) = P (z)/z. To complete the calculation of the
derivate we need,
VacZijC(Aj,c) = Vacηp-1 exp(ηp-1)卜xp(-Ui) + X	ex>(WLWyi)-uieb>/“，)
∖	k∈Sj-{i}
+ K - n X' ex> (Wg-Wyi )—uieb> (a'-ayi ))
m
k∈C
ηp-1 exp(ηp-1)b
・，[' ∈ Sj- {i}]ex>(WLwyi)-uieb>3-a
+ i[` ∈ C] K - nex>(WLwyi)—uieb>(a'-
m
y)
-aVi )
-i[` = yi] ( X	ex)(WLWyi)-uieb>(a'-ayi)
k∈Sj —{i}
+ K - n X ex>(WLWyi)-Uieb>(a'-ayi))).
m k∈C
In order to calculate the full derivate with respect to Aj,c we need to calculate b> ak for all i ∈ Sj
and k ∈ Sj U C. This is a total of n(n + m) inner products of n-dimensional vectors, costing
O(n2(n + m)). To find the optimum of (21) we can use any optimization procedure that only uses
gradients. Since (21) is strongly convex, standard first order methods can solve to E accuracy in
O(Iag(L)) iterations (Boyd & Vandenberghe, 2004, Sec. 9.3). Thus once we can calculate all of
the terms in (21), we can solve it to E accuracy in runtime O(n2(n + m) lαg(e-1)).
19
Under review as a conference paper at ICLR 2018
Once we have solved for Aj,C, we can reconstruct the optimal solution for the parallel component
of Wk as Wk = Wk + Vj ak. Recall that the solution to the perpendicular component is w⊥ =
1+μβj2 W⊥. Thus our optimal solution is Wk = Wk + Vjak + ι+*βj2 W⊥.
If the features xi are sparse, then we’d prefer to do a sparse update to W, saving computation time.
We can achieve this by letting
Wk = Yk ∙ Irk
where Yk is a scalar and rk a vector. Updating Wk = Wk + Vjak + ι+*βj2 Wk is equivalent to
1
Yk	Yk 1 + μβk∕2
Yk = rk + μβk/2 ∙ rk + 7-1(1 + μβk∕2) ∙ Vjak.
Since we only update rk along the span of {xi : i ∈ Sj}, its update is sparse.
F.3.4 Runtime
There are two major tasks in calculating the terms in (21). The first is to calculate χ> Wk for i ∈ Sj
and k ∈ Sj ∪ C. There are a total of n(n + m) inner products of D-dimensional vectors, costing
O(n(n + m)D). The other task is to find the orthonormal basis Vj of {xi : i ∈ Sj}, which can
be achieved using the Gram-Schmidt process in O(n2D). We’ll assume that {Vj : j = 1, ..., J} is
computed only once as a pre-processing step when defining the mini-batches. It is exactly because
calculating {Vj : j = 1, ..., J} is expensive that we have fixed mini-batches that do not change
during the optimization routine.
Adding the cost of calculating the χ> Wk inner products to the costing of optimizing (21) leads to the
claim that solve the Implicit SGD update formula to accuracy in runtime O(n(n + m)D + n2(n +
m) log(-1)) = O(n(n + m)(D + nlog(-1))).
F.3.5 Initializing the Implicit SGD optimizer
As was the case in Section F.1, itis important to initialize the optimization procedure at a point where
the gradient is relatively small and can be computed without numerical issues. These numerical
issues arise when an exponent x> (Wk 一 Wyi) — Ui + b> (ak 一 ayj》0. To ensure that this does
not occur for our initial point, we can solve the following linear problem,13
R = min	kakk1
j,C	k∈C∪Sj
s.t. x>(Wk — Wyi) — Ui + b>(ak — Oyj ≤ 0 ∀i ∈ Sj, k ∈ C ∪ Sj	(22)
Note that if k = yi then the constraint 0 ≥ x> (Wk -Wyi)-Ui + b> (ak —ayj = -Ui is automatically
fulfilled since Ui ≥ 0. Also observed that setting ak = -Vjτ Wk satisfies all of the constraints, and
so
R≤
Σ
k∈C∪Sj
IlVTWk∣∣ι ≤ (n + m) max
k∈C∪Sj
∣V>Wk kι∙
We can use the solution to (22) to gives us an upper bound on (21). Consider the optimal value
A(jR,C) of the linear program in (22) with the value of the minimum being R. Since A(j,RC) satisfies
the constrain in (22) we have zijC (A(j,RC)) ≤ Kηpj-1 exp(ηpj-1). Since P(z) is a monotonically
increasing function that is non-negative for z ≥ 0 we also have (1 + P (zij C (A(jR,C))))2 ≥ (1 +
P(Kηpj-1 exp(ηpj-1)))2. Turning to the norms, we can use the fact that ∣a∣2 ≤ ∣a∣1 for any
13Instead bounding the constraints on the right with 0, we could also have used any small positive number,
like 5.
20
Under review as a conference paper at ICLR 2018
vector a to bound
X (I + μ2k)Ilakk2 + μβkw>Vjak
k∈Sj ∪C
≤ X (I + μk )kak k2 + μBk kw>Vj k1kak∣∣1
k∈Sj ∪C
≤
1 + μ ∙	max	{β }/2) X kak k1 + μ	max	{β kw>Vjkι} X kak∣∣ι
k∈Sj ∪C	k∈Sj ∪C
j	k∈Sj ∪C	j	k∈Sj ∪C
1 + μ ∙ max {βk
k∈Sj ∪C
1 + μ ∙ max {βk
k∈Sj ∪C
(n+m) kmaXJjkV>wkkι)
≤
R2 + μ max {βk} max {∣∣W>Vjkι}R
k∈Sj ∪C	k∈Sj ∪C	k
≤
+ μ I max// L max∕k w>Vjk ι} ((n + m) l max k Vj>wkk ι)
k∈Sj ∪C	k∈Sj ∪C	k∈C∪Sj
≤
(1 + μ ∙, maXL{βk})(n + m)2, max kV>Wk111
k∈Sj ∪C	k∈C∪Sj
≤
(1 + μ ∙ , max, {βk })(n + m)2 , max kwk k2.
k∈Sj ∪C	k∈C∪Sj
Putting the bounds together we have that the optimal value of (21) is upper bounded by its value at
the solution to (22), which in turn is upper bounded by
n(1 + P(Knp-IeXp(np-I)))2 + (1 + μ ∙ kmaxc{βk})(n + m尸 km∪S kwkk2∙
This bound is guarantees that our initial iterate will be numerically stable.
G Learning rate prediction and loss
Here we present the results of using different learning rates for each algorithm applied to the Eurlex
dataset. In addition to the Implicit SGD, NCE, IS, OVE and U-max algorithms, we also provide
results for NCE with n = 1, m = 1, denoted as NCE (1,1) . NCE and NCE (1,1) have near identical
performance.
21
Under review as a conference paper at ICLR 2018
Learning rates
+ 1000.0
+ 100.0
10.0
1.0
0.1
+ 0.01
+ 0.001
Implicit
NCE
NCE (1,1)
IS
U-max (Raman)
Figure 3: Log-loss on Eurlex different learning rates.
OVE
22