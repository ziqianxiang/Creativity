Under review as a conference paper at ICLR 2018
Attention-based Graph Neural Network for
Semi-supervised Learning
Anonymous authors
Paper under double-blind review
Ab stract
Recently popularized graph neural networks achieve the state-of-the-art accuracy
on a number of standard benchmark datasets for graph-based semi-supervised
learning, improving significantly over existing approaches. These architectures
alternate between a propagation layer that aggregates the hidden states of the local
neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a
linear model, that removes all the intermediate fully-connected layers, is still able
to achieve a performance comparable to the state-of-the-art models. This signif-
icantly reduces the number of parameters, which is critical for semi-supervised
learning where number of labeled examples are small. This in turn allows a
room for designing more innovative propagation layers. Based on this insight,
we propose a novel graph neural network that removes all the intermediate fully-
connected layers, and replaces the propagation layers with attention mechanisms
that respect the structure of the graph. The attention mechanism allows us to learn
a dynamic and adaptive local summary of the neighborhood to achieve more ac-
curate predictions. In a number of experiments on benchmark citation networks
datasets, we demonstrate that our approach outperforms competing methods. By
examining the attention weights among neighbors, we show that our model pro-
vides some interesting insights on how neighbors influence each other.
1	Introduction
One of the major bottlenecks in applying machine learning in practice is collecting sizable and re-
liable labeled data, essential for accurate predictions. One way to overcome the problem of limited
labeled data is semi-supervised learning, using additional unlabeled data that might be freely avail-
able. In this paper, we are interested in a scenario when this additional unlabeled data is available
in a form of a graph. The graph provides underlying pairwise relations among the data points, both
labeled and unlabeled.
Of particular interest are those applications where the presence or absence of an edge between two
data points is determined by nature, for instance as a result of human activities or natural relations.
As a concrete example, consider a citation network. Each node in the graph is a published research
paper, associated with a bag-of-words feature vector. An (directed) edge indicates a citation link.
Presence of an edge indicates that the authors of a paper have consciously determined to refer to
the other paper, and hence captures some underlying relation that might not be inferred from the
bag-of-words feature vectors alone. Such external graph data are available in several applications
of interest, such as classifying users connected via a social network, items and customers connected
by purchase history, users and movies connected by viewing history, and entities in a knowledge
graph connected by relationships. In this paper, we are interested in the setting where the graph is
explicitly given and represents additional information not present in the feature vectors.
The goal of such graph-based semi-supervised learning problems is to classify the nodes in a graph
using a small subset of labeled nodes and all the node features. There is a long line of literature
on this topic since Blum & Chawla (2001) which seeks graph cuts that preserve the known labels
and Zhu et al. (2003) which uses graph Laplacian to regularize the nearby nodes to have similar
labels. However, Kipf & Welling (2016) recently demonstrated that the existing approaches can
be significantly improved upon on a number of standard benchmark datasets, using an innovative
neural network architecture on graph-based data known collectively as graph neural networks.
1
Under review as a conference paper at ICLR 2018
Inspired by this success, we seek to understand the reason behind the power of graph neural net-
works, to guide our design of a novel architecture for semi-supervised learning on graphs. To this
end, we first found that a linear classifier of multinomial logistic regression achieves the accuracy
comparable to the best known graph neural network. This linear classifier removes all intermedi-
ate non-linear activation layers, and only keeps the linear propagation function from neighbors in
graph neural networks. This suggests the importance of aggregation information form the neighbors
in the graph. This further motivates us to design a new way of aggregating neighborhood infor-
mation through attention mechanism since, intuitively, neighbors might not be equally important.
This proposed attention-based graph neural network captures this intuition and (a) greatly reduces
the model complexity, with only a single scalar parameter at each intermediate layer; (b) discovers
dynamically and adaptively which nodes are relevant to the target node for classification; and (c)
improves upon state-of-the-art methods in terms of accuracy on standard benchmark datasets. Fur-
ther, the learned attention strengths provide some form of interpretability. They provide insights
on why a particular prediction is made on a target node and which neighbors are more relevant in
making that decision.
2	Related Work
Given a graph G(V, E) with a set of n nodes V and a set of edges E, we let Xi ∈ Rdx denote
the feature vector at node i and let Yi denote the true label. We use YL to denote the labels that
are revealed to us for a subset L ⊂ V . We let X = [X1, . . . , Xn] denote all features, labeled and
unlabeled.
Traditionally, semi-supervised learning using both labeled and un-labled data has been solved using
two different approaches - Graph Laplacian based algorithms solving for locally consistent solutions
(Zhou et al., 2004b) and Expectation Maximization based algorithms (Nigam et al., 2006) where
true-labels of the unlabeled data points are considered as the latent variables of a generative model.
Graph Laplacian regularization. Based on the assumption that nearby nodes in a graph are more
likely to have the same labels, the graph information has been used as explicit regularization:
L(X, YL) = Llabel(XL, YL) + λ LG(X) ,
where Llabel = Pi∈L l(Yi, f(Xi)) is the standard supervised loss for some loss functions l and LG
is the graph-based regularization, for example LG = P(i,j)∈E kf(Xi) - f(Xj)k2, which is called
the graph Laplacian regularization. Earlier approaches are non-parametric and searches over all f
considering it as a look-up table. Most popular one is the Label Propagation (Zhu & Ghahramani,
2002) that forces the estimated labels to agree in the labeled instances and uses weighted graph
Laplacian. This innovative formulation admits a closed form solution which makes it practically
attractive with very low computationally cost. ManiReg (Belkin et al., 2006) replaces supervised
loss with that of a support vector machine. ICA (Lu & Getoor, 2003) generalizes LP by allowing
more general local updates. A more thorough survey on using non-neural network methods for
semi-supervised learning can be found in (Chapelle et al., 2009).
More recent approaches are parametric, using deep neural networks. SemiEmb (Weston et al., 2012)
was the first to use a deep neural network to model f(x) and minimize the above loss. Planetoid
(Yang et al., 2016) significantly improves upon the existing graph regularization approaches by
replacing the regularization by another loss based on skip-grams (defined below). In a slightly
different context, Buchnik & Cohen (2017) show that the accuracy of these approaches can be further
improved by bootstrapping these models sequentially.
Unsupervised node embedding for semi-supervised learning. Several approaches have been pro-
posed to embed the nodes in some latent Euclidean space using only the connectivity in graph G.
Once the embedding is learned, standard supervised learning is applied on those embedded features
to train a model. Inspired by the success of word2vec (Le & Mikolov, 2014), several approaches
define “skip-grams” on graphs as the neighborhood (context) of a node on the graph and tries to
maximize the posterior probability of observing those skip-grams. DeepWalk (Perozzi et al., 2014)
and node2vec (Grover & Leskovec, 2016) use random walks as skip-grams, LINE (Tang et al., 2015)
uses local proximities, LASAGNE (Faerman et al., 2017) uses the Personalized PageRank random
walk. Graph2Gauss (A. Bojchevski, 2017) represents a node as a Gaussian distribution, and mini-
mizes the divergence between connected pairs. Yang et al. (2017) provide a post-processing scheme
2
Under review as a conference paper at ICLR 2018
that takes any node embedding and attempts to improve itbyby taking the weighted sum of the given
embeddings with Personalized PageRank weights. The strength of these approaches is universality,
as the node embedding does not depend on the particular task at hand (and in particular the features
or the labels). However, as they do not use the node features and the training only happens after
embedding, they cannot meet the performance of the state-of-the-art approaches (see DeepWalk in
Table 2).
Graph Neural Network (GNN). Graph neural networks are extensions of neural networks to struc-
tured data encoded as a graph. Originally introduced as extensions of recurrent neural networks,
GNNs apply recurrent layers to each node with additional local averaging layer (Gori et al., 2005;
Scarselli et al., 2009). However, as the weights are shared across all nodes, GNNs can also be inter-
preted as extensions of convolutional neural networks on a 2D grid to general graphs. Typically, a
message aggregation step followed by some neural network architecture is iteratively applied. The
model parameters are trained on (semi-)supervised examples with labels. We give a typical example
of a GNN in Section 3, but several diverse variations have been proposed in (Bruna et al., 2013;
Duvenaud et al., 2015; Li et al., 2015; Henaff et al., 2015; Sukhbaatar et al., 2016; Bronstein et al.,
2016; Defferrard et al., 2016; Dai et al., 2016; Atwood & Towsley, 2016; Niepert et al., 2016; Such
et al., 2017; Hamilton et al., 2017; Schlichtkrull et al., 2017; Such et al., 2017). GNNs have been
successfully applied in diverse applications such as molecular activation prediction (Gilmer et al.,
2017), community detection (Bruna & Li, 2017), matrix completion (Berg et al., 2017), combinato-
rial optimization (Dai et al., 2017; Nowak et al., 2017), and detecting similar binary codes (Xu et al.,
2017).
In particular, for the benchmark datasets that we consider in this paper, Kipf & Welling (2016) pro-
posed a simple but powerful architecture called Graph Convolutional Network (GCN) that achieves
the state-of-the-art accuracy. In the following section, (a) we show that the performance of GCN
can be met by a linear classifier; and (b) use this insight to introduce novel graph neural networks
that compare favourably against the state-of-the-art approaches on benchmark datasets.
3	Dissection of Graph Neural Network
In this section, we propose a novel Graph Neural Network (GNN) model which we call Attention-
based Graph Neural Network (AGNN), and compare its performance to state-of-the-art models on
benchmark citation networks in Section 5. We seek a model Z = f(X, A) ∈ Rn×dy that predicts
at each node one of the dy classes. Zic is the estimated probability that the label at node i ∈ [n] is
c ∈ [dy] given the features X and the graph A. The data features X ∈ Rn×dx has at each row dx
features for each node, and A ∈ {0, 1}n×n is the adjacency matrix of G.
The forward pass in a typical GNN alternates between a propagation layer and a single layer per-
ceptron. Let t be the layer index. We use H(t) ∈ Rn×dh to denote the current (hidden) states, with
the i-th row Hi(t) as the dh dimensional hidden state of node i. A propagation layer with respect to
a propagation matrix P ∈ Rn×n is defined as
H (t) = PH ⑶.
For example, the natural random walk1 P = DTA gives H(t) = (1/|N(i)|) Pj∈N⑴ HjtL The
neighborhood of node i is denoted by N (i), and D = diag(A1). This is a simple local averaging
common in consensus or random walk based approaches. Typical propagation layer respects the
adjacency pattern in A, performing a variation of such local averaging. GNNs encode the graph
structure of A into the model via this propagation layer, which can be also interpreted as performing
a graph convolution operation as discussed in Kipf & Welling (2016). Next, a single layer perceptron
is applied on each node separately and the weights W(t) are shared across all the nodes:
H(t+1) = σ(H(t)W(t)),
where W(t) ∈ Rdht+1 ×dht is the weight matrix and σ(∙) is an entry-wise activation function. This
weight sharing reduces significantly the number of parameters to be trained, and encodes the in-
variance property of graph data, i.e. two nodes that are far apart but have the similar neighboring
1Random walk which moves from a node to one of its neighbors selected uniformly at random.
3
Under review as a conference paper at ICLR 2018
features and structures should be classified similarly. There are several extensions to this model as
discussed in the previous section, but this standard graph neural network has proved powerful in
several problems over graphs, e.g. (Bruna & Li, 2017; Berg et al., 2017; Dai et al., 2017).
Graph Convolutional Network (GCN). Kipf & Welling (2016) introduced a simple but power-
ful architecture, and achieved the state-of-the-art performance in benchmark citation networks (see
Table 2). GCN is a special case of GNN which stacks two layers of specific propagation and per-
ceptron:
H⑴ = ReLU ((PX) W⑼),
Z = f(X,A) = Softmax(( PH⑴)W⑴),	(1)
With a choice of P = D-1/2 AD-1/2, where A = A+1, I is the identity matrix, D = diag(Al) and
1 is the all-ones vector. ReLU(a) = max{0, a} is an entry-wise rectified linear activation function,
and softmax([a1, . . . , ak]) = (1/Z)[exp(a1), . . . , exp(ak)] with Z = i exp(ai) is applied row-
wise. Hence, the output is the predicted likelihoods on the dy dimensional probability simplex. The
weights W(0) and W(1) are trained to minimize the cross-entropy loss over all labeled examples L:
dy
L = -XXYic ln Zic .	(2)
i∈L c=1
Graph Linear Network (GLN). To better understand GCN, we remove the intermediate nonlinear
activation units from GCN, which gives Graph Linear Network defined as
Z = f (X,A) = Softmax((P2X)W⑼ W(I)) ,	(3)
with the same choice of P = D-1/2 JiDD-1/2 as in GCN. The weights W⑼ and W⑴ have the
same dimensions as GCN and are trained on a cross entropy loss in (2). The two propagation layers
simply take (linear) local average of the raw features weighted by their degrees, and at the output
layer a simple linear classifier (multinomial logistic regression) is applied. This allows us to separate
the gain in the linear propagation layer and the non-linear perceptron layer.
Comparing the differences in performances in Table 2, we show that, perhaps surprisingly, GLN
achieves an accuracy comparable to the that of the best GNN, and sometimes better. This suggests
that, for citation networks, the strength of the general GNN architectures is in the propagation layer
and not in the perceptron layer. On the other hand, the propagation layers are critical in achieving
the desired performance, as is suggested in Table 2. There are significant gaps in accuracy for
those approaches not using the graph, i.e. T-SVM, and also those that use the graph differently,
such as Label Propagation (LP) and Planetoid. Based on this observation, we propose replacing the
propagation layer of GLN with an attention mechanism and test it on the benchmark datasets.
4	Attention-based Graph Neural Network (AGNN).
The original propagation layer in GCN and several other graph neural networks such as (Defferrard
et al., 2016; Atwood & Towsley, 2016; Monti et al., 2016; Such et al., 2017) use a static (does not
change over the layers) and non-adaptive (does not take into account the states of the nodes) prop-
agation, e.g. Pij = 1/，|N(i)||N j)|. Such propagations are not able to capture which neighbor
is more relevant to classifying a target node, which is critical in real data where not all edges imply
the same types or strengths of relations.
We need novel dynamic and adaptive propagation layers, capable of capturing the relevance of dif-
ferent edges, which leads to more complex graph neural networks with more parameters. However,
training such complex models is challenging in the semi-supervised setting, as the typical number of
samples we have for each class is small; it is 20 in the standard benchmark dataset. This is evidenced
in Table 2 where more complex graph neural network models by Verma et al. (2017), Monti et al.
(2016), and Such et al. (2017) do not improve upon the simple GCN.
On the other hand, our experiments with GLN suggests that we can remove all the perceptron layers
and focus only on improving the propagation layers. To this end, we introduce a novel Attention-
based Graph Neural Network (AGNN). AGNN is simple; it only has a single scalar parameter β(t) at
4
Under review as a conference paper at ICLR 2018
each intermediate layer. AGNN captures relevance; the proposed attention mechanism over neigh-
bors in (5) learns which neighbors are more relevant and weighs their contributions accordingly.
This builds on the long line of successes of attention mechanisms in summarizing long sentences or
large images, by capturing which word or part-of-image is most relevant (Xu et al., 2015; Graves
et al., 2014; Bahdanau et al., 2014). Particularly, we use the attention formulation similar to the one
used in Graves et al. (2014).2 It only has one parameter and we found this is important for suc-
cessfully training the model when the number of labels is small as in our semi-supervised learning
setting.
4.1	AGNN model
We start with a word-embedding layer that maps a bag-of-words representation of a document into
an averaged word embedding, and the word embedding W(0) ∈ Rdx×dh is to be trained as a part of
the model:
ReLU(XW(0)) .
(4)
This is followed by layers of attention-guided propagation layers parameterized by β(t) ∈ R at each
layer,
H (t+1)
P(t) H(t) ,
(5)
where the propagation matrix P(t) ∈ Rn×n is also a function of the input states H(t) and is zero for
absent edges such that the output row-vector of node i is
Hi(t+1)
X	Pi(jt)Hj(t),
j∈N(i)∪{i}
with Pi(t) = softmax [ β(t) cos(Hi(t), Hj(t)) ]j∈N(i)∪{i} and cos(x, y)
xT y/kxkkyk with the
L2 norm ∣∣χk, for t ∈ {1,...,'} and an integer '. Here ' is the number of propagation layers. Note
that the new propagation above is dynamic; propagation changes over the layers with differing β(t)
and also the hidden states. It is also adaptive; it learns to weight more relevant neighbors higher.
We add the self-loop in the propagation to ensure that the features and the hidden states of the node
itself are not lost in the propagation process. The output layer has a weight W(1) ∈ Rdh×dy:
Z = f(X,A)
Softmax( H('+1) W⑴).
(6)
The weights W(0), W⑴，and β(t)’s are trained on a cross entropy loss in (2). To ease the notations,
we have assumed that the input feature vectors to the first and last layers are augmented with a scalar
constant of one, so that the standard bias term can be included in the parameters W(0) and W(1).
The softmax function at attention ensures that the propagation layer P (t) row-sums to one. The
attention from node j to node i is
(1/C )eβ(t)cos(H尸,Hjt))
(7)
with C = Pj∈N(i)∪{i} eβ(t) cos(Hi(t) ,Hj(t)) which captures how relevant j is to i, as measured by
the cosine of the angle between the corresponding hidden states. We show how we can interpret the
attentions in Section 5.2 and show that the attention selects neighbors with the same class tobe more
relevant. On the standard benchmark datasets on citation networks, we show in Section 5 that this
architecture achieves the best performance in Table 2.
Here we note that independently from this work attention over sets has been proposed as “neighbor-
hood attention” (Duan et al., 2017; Hoshen, 2017) for a different application. The main difference
of AGNN with respect to these work is the fact that in AGNN attention is computed over a neighbor-
hood of a node on a graph, whereas in these work attention over set of all entities is used to construct
a “soft neighborhood”.
2We have experimented other types of more complex attention formulations but we found the training
becomes much less stable especially when the number of labeled nodes is small.
5
Under review as a conference paper at ICLR 2018
5	Experiments on Benchmark Citation Networks
On standard benchmark datasets of three citation networks, we test our proposed AGNN model on
semi-supervised learning tasks. We test on a fixed split of labeled/validation/test sets from Yang
et al. (2016) and compare against baseline methods in Table 2. We also test it on random splits of
the same sizes in Table 3, and random splits with larger number of labeled nodes in Table 4.
Benchmark Datasets. A citation network dataset consists of documents as nodes and citation links
as directed edges. Each node has a human annotated topic from a finite set of classes and a fea-
ture vector. We consider three datasets3. For CiteSeer and Cora datasets, the feature vector has
binary entries indicating the presence/absence of the corresponding word from a dictionary. For
PubMed dataset, the feature vector has real-values entries indicating Term Frequency-Inverse Docu-
ment Frequency (TF-IDF) of the corresponding word from a dictionary. Although the networks are
directed, we use undirected versions of the graphs for all experiments, as is common in all baseline
approaches.
Dataset	Nodes	Edges	Classes	Features	Labeled nodes			
					Table 2	Table 3	Table 4	
CiteSeer	3,327	4,732	6	3,703	120	120	2,218	2,994
Cora	2,708	5,429	7	1,433	140	140	1,805	2,437
PubMed	19,717	44,328	3	500	60	60	13,145	17,745
Table 1: Citation Network Dataset
Experimental setup. The accuracy of the baseline methods are all taken from existing literature. If
a baseline result is not reported in the existing literature, we intentionally left those cells empty in the
table for fairness, as opposed to running those experiments ourselves on untuned hyperparameters.
We train and test only the two models we propose: GLN for comparisons and our proposed AGNN
model. We do not use the validation set labels in training, but use them for optimizing hyper-
parameters like dropout rate, learning rate, and L2-regularization factor. For AGNN, we use a fixed
number of dh = 16 units in the hidden layers and use 4 propagation layers (` = 4) for CiteSeer and
Pubmed and 3 propagation layers (` = 3) for Cora as defined in (7). For GLN, we use 2 propagation
layers as defined in (1). We row-normalize the input feature vectors, as is standard in the literature.
The tables below show the average accuracy with the standard error over 100 training instances with
random weight initializations. We implement our model on TensorFlow (Abadi et al., 2016), and
the computational complexity of evaluating AGNN is O('dh∣E| + dχdhn). Detailed desription of
the experiments is provided in Appendix B.
5.1	Prediction accuracy
Fixed data splits. In this first experiment, we use the fixed data splits from Yang et al. (2016) as
they are the standard benchmark data splits in literature. All experiments are run on the same fixed
split of20 labeled nodes for each class, 500 nodes for validation, 1,000 nodes for test, and the rest of
nodes as unlabeled data. Perhaps surprisingly, the linear classifier GLN we proposed in (3) achieves
performance comparable to or exceeding the state-of-the-art performance of GCN. This leads to our
novel attention-based model AGNN defined in (6), which achieves the best accuracy on all datasets
with a gap larger than the standard error. The classification accuracy of all the baseline methods
are collected from (Yang et al., 2016; Kipf & Welling, 2016; Such et al., 2017; Monti et al., 2016;
Buchnik & Cohen, 2017; Verma et al., 2017).
In semi-supervised learning on graphs, it is critical to utilize both the structure of the graph and
the node features. Methods not using all the given data achieve performance far from the state-
of-the-art. Supervised methods-Single and Multi-layer PercePtrons-only use the labeled examples
(YL , XL ). Semi-supervised methods, e.g. T-SVM, only use the labeled and unlabeled examples YL,
and X. Skip-gram based approaches, such as DeepWalk, ignores the node features X and only use
the labels YL and the graph G.
3https://linqs.soe.ucsc.edu/node/236
6
Under review as a conference paper at ICLR 2018
Input	Method	CiteSeer	Cora	PubMed
YL,XL	Singlelayer PercePtron	57.2	57.4	69.8
	Multilayer Perceptron	64.0	57.5	71.4
YL,X	T-SVM (Joachims,1999)	64.0	57.5	62.2
YL,G	DeepWalk (Perozzi et al., 2014)	43.2	67.2	65.3
	node2vec (Grover & Leskovec, 2016)	54.7	74.9	75.3
	LP (ZhU et al., 2003)	45.3	68.0	63.0
	ICA (Lu & Getoor, 2003)	69.1	75.1	73.9
	ManiReg (Belkin et al., 2006)	60.1	59.5	70.7
	SemiEmb (Weston et al., 2012)	59.6	59.0	71.1
	DCNN (Atwood & Towsley, 2016)		76.8	73.0
	Planetoid (Yang et al., 2016)	64.7	75.7	77.2
YL,X,G	MoNet (Monti et al., 2016)		81.7	78.8
	Graph-CNN (Such et al., 2017)		76.3	
	DynamicFilter (Verma et al., 2017)		81.6	79.0
	Bootstrap (Buchnik & Cohen, 2017)	53.6	78.4	78.8
	GCN (KiPf & Welling, 2016)	70.3	81.5	79.0
	-GLN	70.9±.05	81.2±.05	78.9±.05
	AGNN (this paper)	71.7±.08	82.6±.09	79.9±.07
Table 2: Classification accuracy with a fixed split of data from (Yang et al., 2016).
A breakthrough result of Planetoid by Yang et al. (2016) significantly improved upon the exist-
ing skip-gram based method of DeepWalk and node2vec and the Laplacian regularized methods of
ManiReg and SemiEmb. Kipf & Welling (2016) was the first to apply a graph neural network to
citation datasets, and achieved the state-of-the-art performance with GCN. Other variations of graph
neural networks immediately followed, achieving comparable performance with MoNet, Graph-
CNN, and DynamicFilter. Bootstrap uses a Laplacian regularized approach of (Zhou et al., 2004a)
as a sub-routine with bootstrapping to feed high-margin predictions as seeds.
Random splits. Next, following the setting of Buchnik & Cohen (2017), we run experiments keep-
ing the same size in labeled, validation, and test sets as in Table 2, but now selecting those nodes
uniformly at random. This, along with the fact that different topics have different number of nodes
in it, means that the labels might not be spread evenly across the topics. For 20 such randomly
drawn dataset splits, average accuracy is shown in Table 3 with the standard error. As we do not
force equal number of labeled data for each class, we observe that the performance degrades for all
methods compared to Table 2, except for DeepWalk. AGNN achieves the best performance con-
sistently. Here, we note that Kipf & Welling (2016) does a similar but different experiment using
GCN, where random labeled nodes are evenly spread across topics so that each topic has exactly 20
labeled examples. As this difference in sampling might affect the accuracy, we do not report those
results in this table.
Method	CiteSeer	Cora	PubMed
DeepWalk (Perozzi et al., 2014)	^^72	70.2	72.0
node2vec (Grover & Leskovec, 2016)	47.3	72.9	72.4
Bootstrap (Buchnik & Cohen, 2017)	50.3	78.2	75.6
GLN	68.4±0.45	80.0±0.43	77.7±0.63
AGNN (this paper)	69.8±0.35	81.0±0.34	78.0±0.46
Table 3: Classification accuracy with random splits of the data.
Larger training set. Following the setting of Such et al. (2017), we run experiments with larger
number of labeled data on Cora dataset. We perform k-fold cross validation experiments for k =
3 and 10, by uniformly and randomly dividing the nodes into k equal sized partitions and then
performing k runs of training by masking the labels of each of the k partitions followed by validation
on the masked nodes. Finally the average validation accuracy across k runs is reported. We run 10
trials of this experiment and reports the mean and standard error of the average k-fold validation
7
Under review as a conference paper at ICLR 2018
accuracy. Compared to Table 2, the performance increases with the size of the training set, and
AGNN consistently outperforms the current state-of-the-art architecture for this experiment.
Method	3-fold Split 10-fold split
GraPh-CNN (SUch et al., 2017)^^87.55±1.38^^89.18±1.96
GLN	87.98±0.08	88.24±0.07
AGNN (this PaPer)	89.07±0.08 89.60±0.09
Table 4: Classification accUracy with larger sets of labelled nodes.
)/ ∣N (i)∣ + 1 ,
(9)
5.2 Qualitative analysis
One UsefUl asPect of incorPorating attention into a model is that it Provides some form of interPre-
tation caPability (BahdanaU et al., 2014). The learned Pi(jt)’s in Eq. (7) rePresent the attention from
node j to node i, and Provide insights on how relevant node j is in classifying node i. In FigUre 1,
we Provide statistics of this attention over all adjacent Pairs of nodes for Cora and CiteSeer datasets.
We refer to FigUre 3 for similar statistics on PUbMed. In FigUre 1, we show average attention from
a node in toPic c2 (colUmn) to a node in toPic c1 (row), which we call the relevance from c2 to c1
and is defined as
Relevance(c2 → ci) = ∣s^—∣	X	R(j → i) ,	(8)
| c1,c2| (i,j)∈Sc1,c2
for edge-wise relevance score defined as
1
∣N(i)l + 1
where |N (i)| is the degree of node i, and Sc1,c2 = {(i,j) ∈ Es and Yi = c1, Yj = c2} where Es =
E ∪ {(i, i) for i ∈ V } is the edge set aUgmented with self-looPs to inclUde all the attentions learned.
If We are not using any attention, then the typical propagation will be uniform Pij = 1 / (∣ N (i)∣ + 1),
in which case the above normalized attention is zero. We are measUring for each edge the variation of
attention Pij from uniform 1∕(∣N (i)∣ + 1) as a multiplicative error, normalized by 1∕(∣N (i)∣ + 1).
We believe this is the right normalization, as attention should be measure in relative strength to
others in the same neighborhood, and not in absolute additive differences. We are measuring this
multiplicative variation of the attention, averaged over the ordered pairs of classes.
Figure 1 shows the relevance score for CiteSeer and Cora datasets. (PubMed is shown in Appendix
A.) For both datasets, the diagonal entries are dominant indicating that the attention is learning to
put more weight to those in the same class. A higher value of Relevance(c2 → c1) indicates that,
on average, a node in topic c1 pays more attention to a neighbor in topic c2 than neighbors from
other topics. For CiteSeer dataset (Figure 1 left), we are showing the average attention at the first
propagation layer, P(t=1), for illustration. In the off-diagonals, the most influential relations are
HCI→Agents, Agents→ML, Agents→HCI, and ML→Agents, and the least influential relations are
AI→IR and DB→ML. Note that these are papers in computer science from late 90s to early 2000s.
For Cora dataset (Figure 1 right), we are showing the relevance score of the second propagation
layer, P(t=2), for illustration. In the off-diagonals, the most influential relations are CB→PM,
PM→CB, Rule→PM, and PM→Rule, and the least influential relations are GA→PM and PM→RL.
This dataset has papers in computer science from the 90s. We note that these relations are estimated
solely based on the available datasets for that period of time and might not accurately reflect the
relations for the entire academic fields. We also consider these relations as a static property in this
analysis. If we have a larger corpus over longer period of time, it is possible to learn the influence
conditioned on the period and visualize how these relations change.
Next, we analyze the edges with high and low relevance scores. We remove the self-loops and then
sort the edges according to the relevance score defined in Eq. (9). We take the top 100 and bottom
100 edges and with respect to their relevance scores, and report the fraction of the edges which
are connecting nodes from the same class. Table 5 shows the result on the benchmark datasets for
the relevance scores calculated using the last propagation layer. This suggests that our architecture
learns to put higher attention between nodes of the same class.
8
Under review as a conference paper at ICLR 2018
0.1«
0.12
0X16
OOO
-0.06
-0.12
-0∙lβ
-0.24
-0.30
Figure 1: Relevance score in Eq. (8) from a neighbor node with column-class to a center node
in row-class. For example the average normalized attention to Agents from HCI is = -0.141 ,
largest off-diagonal entry in CiteSeer. The average attention to Probabilistic Methods (PM) from
Case Based (CB) is 0.017, largest off-diagonal entry in Cora.
Top 100
Bottom 100
CiteSeer Cora PUbMed
^^069	064	069^^
0.34	0.09	0.13
Table 5: Fraction of edges from top 100 most relevant edges and bottom 100 least relevant edges
which are connecting two distinct nodes from the same class.
Finally, we analyze those nodes in the test sets that were mistaken by GCN bUt correctly classified by
AGNN, and show how oUr attention mechanism weighted the contribUtion of its local neighborhood,
and show three illUstrative examples in FigUre 2. More examples of this local attention network
(inclUding the legends for the color coding of the topics) are provided in Appendix A. We show
a entire 2-hop neighborhood of a target node (marked by a thick oUtline) from the test set of the
fixed data splits of Citeseer, Cora, or PUbmed. The colors denote the trUe classes of the nodes
(inclUding the target) in the target’s neighborhood, some of which are Unknown to the models at the
training time. The radiUs of a node j is proportional to the attention to the target node i aggregated
over all the layers, i.e. (P (t=4)P (t=3)P (t=2)P (t=1))ij for CiteSeer. The size of the target node
reflects its self-attention defined in a similar way. The first example on the left is node 8434 from
PUbMed. AGNN correctly classifies the target node as light blUe, whereas GCN mistakes it for
yellow, possibly becaUse itis connected to more yellow nodes. Not only has the attention mechanism
learned to pUt more weight to its light blUe 1-hop neighbor, bUt pUt eqUally heavy weights to a
path of light blUe neighbors some of which are not immediately connected to the target node. The
second example in the middle is node 1580 from PUbMed. AGNN correctly classifies it as yellow,
whereas GCN mistakes it for a red, possibly becaUse it only has two neighbors. Not only has the
attention mechanism learned to pUt more weight to the yellow neighbor, bUt it has weighted the
yellow neighbor (who is connected to many yellow nodes and perhaps has more reliable hidden
states representing the trUe yellow class) even more than itself.
The last example on the right is node 1512 from CiteSeer. AGNN correctly classifies it as light blUe,
whereas GCN mistakes it for a white. This is a special example as those two nodes are completely
isolated. DUe to the static and non-adaptive propagation of GCN, it ends Up giving the same predic-
tion for sUch isolated pairs. If the pair has two different trUe classes, then it always fails on at least
on one of them (in this case the light blUe node). However, AGNN is more flexible in adapting to
sUch graph topology and pUts more weight to the target node itself, correctly classifying both.
6 Conclusions
In this paper, we present an attention-based graph neUral network model for semi-sUpervised classi-
fication on a graph. We demonstrate that oUr method consistently oUtperforms competing methods
on the standard benchmark citation network datasets. We also show that the learned attention also
provides interesting insights on how neighbors inflUence each other. In training, we have tried more
9
Under review as a conference paper at ICLR 2018
PUbMed 1580
Figure 2: We show three selected target nodes in the test set that are mistaken by GCN but correctly
classified by AGNN. We denote this target node by the node with a thick outline (node 8434 from
PubMed on the left, node 1580 from PubMed in the middle, and node 1512 from CiteSeer on the
right). We show the strength of attention from a node in the 2-hop neighborhood to the target node
by the size of the corresponding node. Colors represent the hidden true classes (nodes with the same
color belong to the same topic). None of the nodes in the figure was in the training set, hence none
of the colors were revealed. Still, we observe that AGNN has managed to put more attention to those
nodes in the same (hidden) classes, allowing the trained model to find the correct labels.
CiteSeer1512
complex attention models. However, due to the increased model complexity the training was not
stable and does not give higher accuracy. We believe that for semi-supervised setting with such
a limited number of labeled examples, reducing model complexity is important. Note that we are
able to train deeper (4-layers) models compared to a shallower (2-layers) model of GCN, in part
due to the fact that we remove the non-linear layers and reduce the model complexity significantly.
In comparison, deeper GCN models are known to be unstable and do not give the performance of
shallower GCNs (Kipf & Welling, 2016).
References
S. Gunnemann A. Bojchevski. Deep gaussian embedding of attributed graphs: Unsupervised induc-
tive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
J. Atwood and D. Towsley. Diffusion-convolutional neural networks. In Advances in Neural Infor-
mation Processing Systems, pp. 1993-2001, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
R. van den Berg, T. N. Kipf, and M. Welling. Graph convolutional matrix completion. arXiv preprint
arXiv:1706.02263, 2017.
Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In
ICML ’01 Proceedings of the Eighteenth International Conference on Machine Learning, 2001.
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. arXiv preprint arXiv:1611.08097, 2016.
J. Bruna and X. Li. Community detection with graph neural networks. arXiv preprint
arXiv:1705.08415, 2017.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
10
Under review as a conference paper at ICLR 2018
Eliav Buchnik and Edith Cohen. Bootstrapped graph diffusions: Exposing the power of nonlinearity.
arXiv preprint arXiv:1703.02618, 2017.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o.
et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542-542, 2009.
H. Dai, B. Dai, and L. Song. Discriminative embeddings of latent variable models for structured
data. In International Conference on Machine Learning, pp. 2702-2711, 2016.
Hanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
optimization algorithms over graphs. arXiv preprint arXiv:1704.01665, 2017.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
Systems, pp. 3837-3845, 2016.
Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,
Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. arXiv preprint
arXiv:1703.07326, 2017.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Evgeniy Faerman, Felix Borutta, Kimon Fountoulakis, and Michael W Mahoney. Lasagne: Locality
and structure aware graph node embedding. arXiv preprint arXiv:1710.06520, 2017.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Neural Networks, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference
on, volume 2, pp. 729-734. IEEE, 2005.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
855-864. ACM, 2016.
W. L. Hamilton, R. Ying, and J. Leskovec. Inductive representation learning on large graphs. arXiv
preprint arXiv:1706.02216, 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In Advances in Neural Informa-
tion Processing Systems, pp. 2698-2708, 2017.
Thorsten Joachims. Transductive inference for text classification using support vector machines. In
ICML, volume 99, pp. 200-209, 1999.
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016.
Q. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of
the 31st International Conference on Machine Learning (ICML-14), pp. 1188-1196, 2014.
Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. arXiv
preprint arXiv:1511.05493, 2015.
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the 20th International
Conference on Machine Learning (ICML-03), pp. 496-503, 2003.
11
Under review as a conference paper at ICLR 2018
Federico Monti, Davide Boscaini, Jonathan Masci, EmanUele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. arXiv
preprint arXiv:1611.08402, 2016.
M. Niepert, M. Ahmed, and K. KUtzkov. Learning convolUtional neUral networks for graphs. In
Proceedings of the 33rd annual international conference on machine learning. ACM, 2016.
Kamal Nigam, Andrew McCallUm, and Tom Mitchell. Semi-sUpervised text classification Using em.
Semi-Supervised Learning, pp. 33-56, 2006.
A.	Nowak, S. Villar, A. S. Bandeira, and J. BrUna. A note on learning algorithms for qUadratic
assignment with graph neUral networks. arXiv preprint arXiv:1706.07450, 2017.
B.	Perozzi, R. Al-RfoU, and S. Skiena. Deepwalk: Online learning of social representations. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 701-710. ACM, 2014.
Franco Scarselli, Marco Gori, Ah ChUng Tsoi, MarkUs HagenbUchner, and Gabriele Monfardini.
The graph neUral network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
M. SchlichtkrUll, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling. Modeling
relational data with graph convolUtional networks. arXiv preprint arXiv:1703.06103, 2017.
Felipe Petroski SUch, Shagan Sah, MigUel DomingUez, SUhas Pillai, Chao Zhang, Andrew Michael,
Nathan Cahill, and Raymond PtUcha. RobUst spatial filtering with graph convolUtional neUral
networks. arXiv preprint arXiv:1703.00792, 2017.
Sainbayar SUkhbaatar, arthUr szlam, and Rob FergUs. Learning mUltiagent commUnication with
backpropagation. In Advances in Neural Information Processing Systems 29, pp. 2244-2252.
2016.
Jian Tang, Meng QU, Mingzhe Wang, Ming Zhang, JUn Yan, and QiaozhU Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th International Conference on World
Wide Web, pp. 1067-1077. ACM, 2015.
Nitika Verma, Edmond Boyer, and Jakob Verbeek. Dynamic filters in graph convolUtional networks.
arXiv preprint arXiv:1706.05206, 2017.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
sUpervised embedding. In Neural Networks: Tricks of the Trade, pp. 639-655. Springer, 2012.
Kelvin XU, Jimmy Ba, Ryan Kiros, KyUnghyUn Cho, Aaron CoUrville, RUslan SalakhUdinov, Rich
Zemel, and YoshUa Bengio. Show, attend and tell: NeUral image caption generation with visUal
attention. In International Conference on Machine Learning, pp. 2048-2057, 2015.
XiaojUn XU, Chang LiU, Qian Feng, Heng Yin, Le Song, and Dawn Song. NeUral network-
based graph embedding for cross-platform binary code similarity detection. arXiv preprint
arXiv:1708.06525, 2017.
Cheng Yang, Maosong SUn, ZhiyUan LiU, and CUnchao TU. Fast network embedding enhancement
via high order proximity approximation. In Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence, IJCAI, pp. 19-25, 2017.
Z. Yang, W. W. Cohen, and R. SalakhUtdinov. Revisiting semi-sUpervised learning with graph
embeddings. arXiv preprint arXiv:1603.08861, 2016.
Denny Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learning
with local and global consistency. In Advances in neural information processing systems, pp.
321-328, 2004a.
Denny Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learning
with local and global consistency. In Advances in neural information processing systems, pp.
321-328, 2004b.
12
Under review as a conference paper at ICLR 2018
Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propa-
gation. 2002.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03),pp. 912-919, 2003.
13
Under review as a conference paper at ICLR 2018
Appendix
A Additional experiments on interpretability of attention
PubMed dataset has only 3 classes, and the relevance score is shown in the figure below.
0.05
0.00
-0.05
-o.ιo
-0.15
-0.20
-0.25
-0.30
-0.35
Figure 3: Average attention in Eq. (8) from a column class to a row class
We show examples of 2-hop local neighborhood of nodes that are mistaken by GCN but correctly
classified by AGNN.
Figure 4: Examples from CiteSeer dataset of attention strength in the local neighborhood of a target
node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN.
Colors are true classes and node sizes are proportional to the attention strength from a neighbor to
the target node. Labeled nodes from training set are marked with '*’.
O AI
Q ML
O IR
O DB
Ag Agents
O HCI
* labeled train nodes
B Experiment and Architect Details
In this section we will list all the choices made in training and tuning of hyper-parameters. The pa-
rameters are chosen as to maximize the validation. All the models use Adam optimization algorithm
with full-batchs, as standard in other works on GNNs (Kipf & Welling, 2016; Such et al., 2017). We
also a weight decay term to the objective function for all the learnable weights. We add dropout to
the first and last layers of all models.
In Table 6. we show the hyper-parameters used in training the AGNN models for various settings
and datasets. For Cora dataset the architecture consist of` = 3 propagation layers, but the first prop-
14
Under review as a conference paper at ICLR 2018
.Theory
θ Reinforcement Learning
θ Genetic Algorithms
θ Neural Networks
θ Probabilistic Methods
θ Case Based
O Rule Learning
* labeled train nodes
Figure 5: Examples from Cora dataset of attention strength in the local neighborhood of a target
node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN.
Colors are true classes and node sizes are proportional to the attention strength from a neighbor to
the target node. Labeled nodes from training set are marked with ‘*’.
(ɔ Experimental
OTyPeI
• Type 2
* labeled train nodes
Figure 6: Examples from Pubmed dataset of attention strength in the local neighborhood of a target
node (in thick outline) from the test set that is mistaken by GCN but correctly classified by AGNN.
Colors are true classes and node sizes are proportional to the attention strength from a neighbor to
the target node. Labeled nodes from training set are marked with ‘*’.
agation layer P (t=1) is non-trainable and the variable β(t=1) value is fixed at zero. While training
these AGNN models we maintain the validation accuracy for each iteration and finally choose the
trained model parameters from the iteration where average validation accuracy of previous 4 epochs
is maximized. For the k-fold cross validation setting we take the epoch with maximum validation
accuracy.
15
Under review as a conference paper at ICLR 2018
Setting	Dataset	Propagation layers (`)	Hidden state dimension (dh,)	Learning Rate	Weight Decay	Dropout	Epochs
Fixed Split	CiteSeer	4	16~	0.005	0.0005	0.5	1000
Fixed Split	Cora	3	16	0.01	0.0005	0.5	1000
Fixed Split	PubMed	4	16	0.008	0.001	0.5	400
Rand. Split	CiteSeer	4	16	0.01	0.0005	0.5	1000
Rand. Split	Cora	3	16	0.01	0.0005	0.5	1000
Rand. Split	PubMed	4	16	0.008	0.001	0.5	1000
Cross Val.	Cora	3	16	0.04	0.0005	0.25	500
Table 6: Hyper-parameters for AGNN model
For the Graph Linear Network (GLN) as define in (3), we use the same hyper-parameters as GCN
(Kipf & Welling, 2016) for all the experimental settings: hidden dimension of 16, learning rate of
0.01, weight decay of 5 × 10-4, dropout of 0.5, 200 epochs and early stopping criteria with a window
size of 10.
C Performance versus Number of Propagation Layers
In this section we provide experimental results justifying the choice of number of propagation layers
for each dataset. We use the same data split and hyper-parameters (except number of propagation
layer) as in the fixed data splits setting in Section 5.1. Similar to other settings the first propagation
layer for Cora dataset is non-trainable with β(t=1) = 0. Tables 7 gives the average (over 10 trials)
testing accuracy respectively for various choices of number of propagation layers. We note that
different datasets require different number of propagation layers for best performance.
Propagation Layers (`)	CiteSeer	Cora	PubMed
1	-^69.08^^	80.52	78.31
2	70.83	83.07	79.66
3	71.06	82.62	79.56
4	71.70	82.24	80.10
5	71.37	82.07	80.13
Table 7: Average testing error of AGNN for different number of Propagation Layers.
D Performance of GCN on other dataset splits.
Method	CiteSeer	Cora	PubMed
DeepWalk (Perozzi et al., 2014)	-47.2	70.2	72.0
node2vec (Grover & Leskovec, 2016)	47.3	72.9	72.4
Bootstrap (Buchnik & Cohen, 2017)	50.3	78.2	75.6
GCN	66.9±0.50	79.2±0.46	77.5±0.61
GLN	68.4±0.45	80.0±0.43	77.7±0.63
AGNN (this paper)	69.8±0.35	81.0±0.34	78.0±0.46
Table 8: Classification accuracy with random splits of the data.
Here we provide the performance of GCN on random splits and larger training set dataset splits
from Section 5.1. We relegate these tables to the appendix since they were not present in Kipf &
Welling (2016). We conducted the experiments with the same hyper-parameters as chosen by Kipf
& Welling (2016) for the fixed split. In Tables 8 and 9 we provide average testing accuracy and
standard error over 20 and 10 runs on random splits and larger training set respectively.
16
Under review as a conference paper at ICLR 2018
Method	3-fold Split	10-fold split
GraPh-CNN (SUch et al., 2017)	87.55±1.38	89.18±1.96
GCN	87.23±0.21	87.78±0.04
GLN	87.98±0.08	88.24±0.07
AGNN (this PaPer)	89.07±0.08	89.60±0.09
Table 9: Classification accuracy with larger sets of labelled nodes.
17