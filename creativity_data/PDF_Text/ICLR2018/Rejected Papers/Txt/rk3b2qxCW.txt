Under review as a conference paper at ICLR 2018
Policy Gradient for Multidimensional Action
Spaces: Action Sampling and Entropy Bonus
Anonymous authors
Paper under double-blind review
Ab stract
In recent years deep reinforcement learning has been shown to be adept at solv-
ing sequential decision processes with high-dimensional state spaces such as in
the Atari games. Many reinforcement learning problems, however, involve high-
dimensional discrete action spaces as well as high-dimensional state spaces. In
this paper, we develop a novel policy gradient methodology for the case of large
multidimensional discrete action spaces. We propose two approaches for creating
parameterized policies: LSTM parameterization and a Modified MDP (MMDP)
giving rise to Feed-Forward Network (FFN) parameterization. Both of these ap-
proaches provide expressive models to which backpropagation can be applied for
training. We then consider entropy bonus, which is typically added to the reward
function to enhance exploration. In the case of high-dimensional action spaces,
calculating the entropy and the gradient of the entropy requires enumerating all the
actions in the action space and running forward and backpropagation for each ac-
tion, which may be computationally infeasible. We develop several novel unbiased
estimators for the entropy bonus and its gradient. Finally, we test our algorithms
on two environments: a multi-hunter multi-rabbit grid game and a multi-agent
multi-arm bandit problem.
1	Introduction
In recent years deep reinforcement learning has been shown to be adept at solving sequential de-
cision processes with high-dimensional state spaces such as in the Go game (Silver et al. (2016))
and Atari games (Mnih et al. (2013), Mnih et al. (2015), Mujika (2016), O’Donoghue et al. (2016),
Parisotto et al. (2015), Wang et al. (2016), Czarnecki et al. (2017)). In all of these success stories,
the size of the action space was relatively small. Many reinforcement learning problems, however,
involve high-dimensional action spaces as well as high-dimensional state spaces. Examples include
StarCraft (Vinyals et al. (2017), Lin et al. (2017)), where there are many agents each of which can
take a finite number of actions; and coordinating self-driving cars at an intersection, where each car
can take a finite set of actions (Sukhbaatar et al. (2016)).
In this paper, we develop a novel policy gradient methodology for the case of large multidimensional
action spaces. There are two major challenges in developing such a methodology:
•	For large multidimensional action spaces, how can we design expressive and differentiable
parameterized policies which can be efficiently sampled?
•	In policy gradient, in order to encourage sufficient exploration, an entropy bonus term
is typically added to the objective function. However, in the case of high-dimensional
action spaces, calculating the entropy and its gradient requires enumerating all the actions
in the action space and running forward and backpropagation for each action, which may
be computationally infeasible. How can we efficiently approximate the entropy and its
gradient while maintaining desirable exploration?
In this paper, we first propose two approaches for parameterizing the policy: a LSTM model and
a Modified MDP (MMDP) giving rise to Feed-Forward Network (FFN) model. For both of these
parameterizations, actions can be efficiently sampled from the policy distribution, and backpropaga-
tion can be employed for training. We then develop several novel unbiased estimators for the entropy
1
Under review as a conference paper at ICLR 2018
bonus and its gradient. These estimators can be combined with stochastic gradient descent giving a
new a class of policy gradient algorithms with desirable exploration. Finally, we test our algorithms
on two environments: a multi-agent multi-arm bandit problem and a multi-agent hunter-rabbit grid
game.
2	Policy Gradient for Multidimensional Action Spaces
Consider an MDP with a d-dimensional action space A = Ai × A2 × ∙∙∙ × Ad. Denote a =
(ai,..., ad) for an action in A. A policy ∏(∙∣s) specifies for each state S a distribution over the action
space A. In the standard RL setting, an agent interacts with an environment over a number of discrete
timesteps (Sutton & Barto (1998), Silver (2015)). At timestep t, the agent is in state st and samples
an action at from the policy distribution ∏(∙∣st). The agent then receives a scalar reward r and the
environment enters the next state st+i. The agent then samples at+i from ∏(∙∣st+ι) and so on. The
process continues until the end of the episode, denoted by T . The return Rt = PkT=-0t γkrt+k is the
discounted accumulated return from time step t until the end of the episode.
In the policy gradient formulation, We consider a set of parameterized policies ∏θ(∙∣s), θ ∈ Θ, and
attempt to find a good θ within a parameter set Θ. Typically the policy ∏θ(∙∣s) is generated by a
neural network with θ denoting the weights and biases in the network. The parameters θ are updated
by performing stochastic gradient ascent on the expected reward. One example of such an algorithm
is REINFORCE, proposed by Williams & Peng (1991), where in a given episode at timestep t the
parameters θ are updated as follows:
T
∆θ = αX Vθ log∏θ(at∣st)(Rt - bt(st))
t=0
where bt(st) is a baseline. It is well known that the policy gradient algorithm often converges to
a local optimum. To discourage convergence to a highly suboptimal policy, the policy entropy is
typically added to the update rule:
T
∆θ = α X[Vθ log ∏θ (at∣st)(Rt - bt(st)) + βV H (st)]	(1)
t=0
where
Hθ(st) ：= -E ∏θ(a∣st)log ∏θ(a|st)	(2)
a∈A
This approach is often referred to as adding entropy bonus or entropy regularization (Williams &
Peng (1991)) and is widely used in different applications of neural networks, such as optimal control
in Atari games (Mnih et al. (2016)), multi-agent games (Lowe et al. (2017)) and optimizer search
for supervised machine learning with RL (Bello et al. (2017)). β is referred to as the entropy weight.
In applying policy gradient to MDP with large multidimensional action spaces, there are two chal-
lenges. First, how do we design an expressive and differentiable parameterized policy which can
be efficiently sampled? Second, for the case of large multidimensional action spaces, calculating
the entropy and its gradient requires enumerating all the actions in the action space, which may be
infeasible. How do we then enhance exploration in a principled way?
3	Policy parameterization for efficient sampling
To abbreviate the notation, we write pθ(a) for ∏θ(a|st), with the conditioning on St being implicit.
We consider schemes whereby the sample components ai, i = 1, . . . , d, are sequentially generated.
In particular, after obtaining ai, a2, . . . , ai-i, we will generate ai ∈ Ai from some parameterized
distribution pθ(∙∣a1,a2,..., a-i) defined over the one-dimensional set Ai. After generating the
distribution pθ (∙∣ai,a2,..., a-i), i = 1,...,d and the action components a\,...,a& sequentially,
we can then define pθ (a) as pθ (a) = Qd=i pθ (ai∣ai,a2,...,ai-i). We now propose two methods
for creating the parameterized distributions pθ (a|ai, a2, . . . , ai-i), a ∈ Ai. To our knowledge, these
models are novel and have not been studied in multidimensional action space literature. We assume
that the size of the one-dimensional action sets are equal, that is, |Ai| = |A2 | = ... = |Ad| = K.
To handle action sets of different sizes, we include inconsequential actions if needed.
2
Under review as a conference paper at ICLR 2018
3.1	Using RNNs to Generate the Parameterized Policy
The policy pθ(a) can be learned with a recurrent neural network (RNN). Long Short-Term Memory
(LSTM), a special flavor of RNN, has recently been used with great success to represent conditional
probabilities in language translation tasks (Sutskever et al. (2014)). Here, as shown in Figure 1(a),
We use an LSTM to generate a parameterized multidimensional distribution pθ(∙) and to sample
a = (aι,...,ad) from that distribution. Specifically, pθ (a|ai, a2,..., ai-ι), a ∈ A is given by
the output of the LSTM. To generate ai, We run a forWard pass through the LSTM With the input
being ai-1 and the current state st (and implicitly on a1, . . . , ai-1 Which influences hi-1). This
produces a hidden state hi, Which is then passed through a linear layer, producing a K dimensional
vector. The softmax of this vector is taken to produce the one-dimensional conditional distribution
Pθ(a∣a1,a2,…，ai-ι), a ∈ Ai . Finally, a% is sampled from this one-dimensional distribution, and is
then fed into the next stage of the LSTM to produce ai+1.
After generating the action a = (a1, . . . , ad), and the conditional probabilities
Pθ (∙∣αι, a2,..., ai-ι), i = 1,...,d, we can evaluate pθ (a) as the product of the conditional
probabilities. During training, We can also use backpropagation to efficiently calculate the first term
on the RHS of the update rule in (1).
3.2	Using MMDP to Generate Parameterized Policy
As an alternative to using a LSTM to create parameterized multidimensional policies, we can modify
the underlying MDP to create an equivalent MDP for which the action space is one dimensional at
each time step. We refer to this MDP as the Modified MDP (MMDP). In the original MDP, we have
state space S and action space A = Ai × A2 ×∙∙∙×Ad where Ai = {1, 2,..., K}. In MMDP, the
state is modified to encapsulate the original state and all the action dimensions selected for state s so
far, i.e., (s, a1, a2, . . . , ai, 0, . . . , 0) with a1, . . . , ai being selected values for action dimensions 1 to
i, and 0 being the placeholder for d - i - 1 dimensions. The new action space is A =	{0,	1,	.	.	.	,	K}
and the new state	space is S × {0,	1, . . . , K}d-1. The state	transition	probabilities	for	the	MMDP
are given by
jɔ/ /	C	C∖l∕ C	c∖ ∖	1
P ((s, a1, 0, . . . ,	0)|(s, 0, . . . , 0), a1) = 1
P((s,a1,a2,0, . . . , 0)|(s, a1,0, . . . ,0), a2) = 1
.
.
.
P ((s, a1, . . . , ad-1)|(s, a1, . . . , ad-2, 0), ad-1) = 1
00
P ((s , 0, . . . , 0)|(s, a1, . . . , ad-1), ad) = P (s |s, a1, . . . , ad)
where P (s0|s, a1, . . . , ad) is the transition probabiliy of the original MDP. The reward is only gen-
erated after all d component actions are taken. It is easily seen that the MMDP is equivalent to the
original MDP.
Since the MMDP has an one-dimensional action space, we can use a feed-forward network (FFN)
to generate each action component as shown in (Figure 1(b)). Note that the FFN input layer size is
always |S | + K - 1 and the output layer size is K.
4	Entropy B onus Approximation for large action space
As shown in (1), an entropy bonus is typically included to enhance exploration. However, for large
multidimensional action spaces, calculating the entropy and the gradient of the entropy requires
enumerating all the actions in the action space and running forward and backpropagation for each
action. In this section we develop computationally efficient unbiased estimates for the entropy and
its gradient.
Let A = (Ai,..., Ad) denote a random variable with distribution pθ(∙). Let Hθ denote the exact
entropy of the distribution pθ(a):
d
Hθ = - £ps (a) log Pθ (a) = -EA 〜pθ[log Pθ (A)] = -E EA 〜pθ[log Pθ (Ai∣Ai-ι)]
a	i=i
3
Under review as a conference paper at ICLR 2018
a1
ad

7
ad
Softmax
Softmax
Softmax
Linear
+
Softmax
Linear
+
Softmax
Linear
+
Softmax
FFN
FFN
...
a3, a4
FFN
ad-1 八八八
RNN AhI-A RNN *h2 ._._.
RNN
st
[0,...,0]
J，Y .
length d
st [0,...,0]
length d - 1
st
~0 st	a1 st	at	st
(a)	The RNN architecture. To generate ai, we input st and ai-1 into
the RNN and then pass the resulting hidden state hi through a linear
layer and a softmax to generate a distribution, from which we sample
ai.
(b)	The MMDP architecture. To generate ai, we input
st and a1 , a2, . . . , ai-1 into a FFN. The output is passed
through a softmax layer, providing a distribution from which
we sample ai . Since the input size of the FFN is fixed,
when generating ai , constants 0 serve as placeholders for
ai+1 , . . . , ad-1 in the input to the FFN.
Figure 1: The RNN and MMDP architectures for generating parameterized policies.
4.1	Crude Unbiased Estimator
During training within an episode, for each state st , the policy (using, for example, LSTM or
MMDP) generates an action a = (a1 , a2 , . . . , ad). A crude approximation of the entropy bonus
is:
d
Hθrude(a) = - logpθ(a) = - X logpθ侬⑶—。
i=1
This approximation is an unbiased estimate of Hθ but its variance is likely to be large. To reduce
the variance, we can generate M action samples a(1) , a(2) , . . . , a(M) when in st and average the
log action probabilities over the samples. However, generating a large number of samples is costly,
especially when each sample is generated from a neural network, since each sample requires one
additional forward pass.
4.2	Smoothed Estimator
In this section, we develop an alternative unbiased estimator for entropy which only requires the one
episodic sample. In the course of an episode, an action a = (a1, a2, . . . , ad) is generated for each
st . The alternative estimator accounts for the entropy along each dimension of the action space.
dd
Heθ(a) := -	Pθ (a∣ai-i)log Pθ (a∣ai-i)=工工”⑶-。
i=1 a∈Ai	i=1
where
H(i)(ai-i) := - E pθ(a∣ai-i)logpθ(a∣ai-i)
a∈Ai
which is the entropy of Ai conditioned on ai-1. This approximation of entropy bonus is compu-
tationally efficient since for each dimension i, We need to obtain pθ(∙∣ai-ι), its log and gradient
anyway during training. We refer to this approximation as the smoothed entropy.
The smoothed entropy Hθ(A) has several appealing properties. The proofs of Theorem 1 and The-
orem 3 are straightforWard and omitted.
Theorem 1.	Hθ (A) is an unbiased estimator of the exact entropy Hθ.
4
Under review as a conference paper at ICLR 2018
Theorem 2.	Ifpθ(a) has a multivariable normal distribution with mean and variance depending on
θ, then:
He θ (a) = Hθ ∀a ∈ A
Thus, the smoothed entropy equals the exact entropy for a multi-variate normal parameterization of
the policy (Proof in Appendix B).
Theorem 3.	(i) Ifthere is a Sequence ofweights θ1,θ2,... such thatpθn (∙) converges to the uniform
distribution over A, then
sup Hθ (a) = dlog K
θ
(ii) Ifthere is a SeqUenCe ofweights θ1,θ2,... such that pθn (a*) → 1 for some a*, then
inf He θ (a) = 0
Thus, the smoothed entropy Hθ (a) mimics the exact entropy in that it has the same supremum and
infinum values as the exact entropy.
The above theorems indicate that Hθ (a) may serve as a good proxy for Hθ: it is an unbiased
estimator for Hθ, it has the same minimum and maximum values when varying θ; and in the special
case when pθ (a) has a multivariate normal distribution, it is actually equal to Hθ for all a ∈ A. Our
numerical experiments have shown that the smoothed estimator Hθ(a) typically has lower variance
than the crude estimator Hθcrude(a). However, it is not generally true that the smoothed estimator
always has lower variance as counterexamples can be found.
4.3 Gradient of Entropy
So far we have been looking at estimates of entropy. But the policy gradient algorithm (1) uses the
gradient of the entropy rather than just simply the entropy. As it turns out, the gradient of estimators
Hθcrude(a) and Heθ (a) are not unbiased estimates of the gradient of the entropy. In this subsection,
we provide unbiased estimators for the gradient of the entropy. For simplicity, in this section, we
assume an one-step decision setting, such as in a multi-armed bandit problem. A straightforward
calculation shows:
Vθ Hθ = EA 〜Pθ[-log Pθ (A)Vθ log Pθ (A)]	(3)
Suppose a is one sample from pθ(∙). A crude unbiased estimator for the gradient of the entropy
therefore is: - log pθ (a)Vθ log pθ (a) = log pθ (a)Vθ Hθcrude (a). Note that this estimator is equal to
the gradient of the crude estimator multiplied by a correction factor.
Analogous to the smoothed estimator for entropy, we can also derive a smoothed estimator for the
gradient of the entropy.
Theorem 4. If a isa samPlefrom pθ (∙), then
d	i-1
VθHeθ(a) + XHθ(i)(ai-1)VθXlog Pθ (aj ∣aj-i)
i=1	j=1
is an unbiased estimator for the gradient of the entroPy (Proof in APPendix C).
Note that this estimate for the gradient of the entropy is equal to the gradient of the smoothed
estimate Hθ(a) plus a correction term. We refer to this estimate of the entropy gradient as the
unbiased gradient estimate.
5	Experimental Results
We designed experiments to compare the LSTM and MMDP models, and to also compare how the
different entropy approximations perform for both. For each entropy approximation, the entropy
weight as described in (1) was tuned to give the highest episode reward. For MMDP, the number of
hidden layers was also tuned from 1 to 7. The rest of the hyperparameters are listed in Appendix A.
5
Under review as a conference paper at ICLR 2018
5.1	Hunters and Rabbits
In this environment, there is a n × n grid. At the beginning of each episode d hunters and d rabbits
are randomly placed in the grid. The rabbits remain fixed in the episode, and each hunter can move
to a neighboring square (including diagonal neighbors) or stay at the current square. So each hunter
has nine possible actions, and altogether there are |A| = 9d actions at each time step. When a hunter
enters a square with a rabbit, the hunter captures the rabbit and remains there until the end of the
game. In each episode, the goal is for the hunters to capture the rabbits as quickly as possible. Each
episode is allowed to run for at most 10,000 time steps.
To provide a dense reward signal, we formalize the goal with the following modification: capturing a
rabbit gives a reward of 1, which is discounted by the number of time steps taken since the beginning
of the episode. The discount factor is 0.8. The goal is to maximize the episode’s total discounted
reward. After a hunter captures a rabbit, they both become inactive. The representation of an active
hunter or rabbit is (1, y position, x position). The representation of an inactive hunter or rabbit is (0,
-1, -1).
Comparison of different entropy estimates for LSTM and MMDP
Table 1 shows the performance of the LSTM and MMDP models with different entropy estimates.
(smoothed mode entropy is explained in Appendix D). The evaluation was performed in a square
grid of 5 by 5 with 5 hunters and 5 rabbits. Training was run for 1 million episodes for each of the
seeds. All evaluations are averaged over 1,000 episodes per seed for a total of 5,000 episodes.
First, we observe that the LSTM model always does better than the MMDP model, particularly
for the episode length. Second, we note that policies obtained with the entropy approximations
all perform better than policies obtained without entropy or with crude entropy. For the LSTM
model, the best performing approximation is smoothed entropy, reducing the mean episode length
by 45% and increasing the mean episode reward by 10% compared to without entropy. We also note
that there is not a significant difference in performance between the smoothed entropy estimate,
smoothed mode estimate, and the unbiased gradient estimate.
Table 1: Performance of LSTM and MMDP across different entropy approximations.
	Without Entropy	Crude Entropy	Smoothed Entropy	Smoothed Mode Entropy	Unbiased Gradient Estimate
LSTM Mean Episode Length	10.1 ± 1.9	19 ± 8.7	5.6 ± 0.2	6.0 ± 0.2	6.0 ± 0.1
MMDP Mean Episode Length	21.5 ± 3.7	37.3 ± 29.6	10.6 ± 0.7	10.6 ± 0.7	9.8 ± 0.6
LSTM Mean Episode Reward	3.0 ± 0.06	3.0 ± 0.03	3.3 ± 0.04	3.2 ± 0.04	3.2 ± 0.02
MMDP Mean Episode Reward	2.8 ± 0.03	2.7 ± 0.03	2.9 ± 0.03	2.8 ± 0.04	2.9 ± 0.02
As shown in Table 2, smoothed entropy is also more robust to the initial seed than without entropy.
For example, for the LSTM model, in the case of without entropy, seed 0 leads to significantly worse
results than the seeds 1-4. This does not happen to smoothed entropy.
Entropy approximations versus exact entropy
We now consider how policies trained with entropy approximations compare with polices trained
with exact entropy. In order to calculate exact entropy in an acceptable amount of time, we reduced
the number of hunters and rabbits to 4 hunters and 4 rabbits. Training was run for 50,000 episodes.
Table 3 shows the performance differences between policies trained with entropy approximations
and exact entropy. We see that the best entropy approximations perform only slightly worse than
exact entropy for both LSTM and MMDP. Once again we see that the LSTM model performs better
than the MMDP model.
6
Under review as a conference paper at ICLR 2018
Table 2: LSTM and MMDP results across seeds.
	Without Entropy					Crude Entropy					Smoothed Entropy				
Seed	0	1	2	3	4	0	1	2	3	4	0	1	2	3	4
LSTM Mean Episode Length	14	9	11	9	8	40	12	17	11	14	5	6	6	5	6
MMDP Mean Episode Length	15	19	27	27	20	17	30	14	109	18	10	10	11	11	12
Table 3: LSTM and MMDP results for entropy approximation versus exact entropy.
	LSTM Smoothed Entropy	LSTM Exact Entropy	MMDP Unbiased Gradient Estimate	MMDP Exact Entropy
Mean Episode Length	9.0 ± 0.3	8.9 土 0.2	11.5 土 0.3	10.7 土 0.4
Mean Episode Reward	2.14 ± 0.02	2.19 土 0.02	2.01 土 0.01	2.1 土 0.01
5.2	Multi-agent multi-arm bandits
We examine a multi-agent version of the standard multi-armed bandit problem, where there are d
agents each pulling one of K arms, with d ≤ K. The kth arm generates a reward rk. The total
reward in a round is generated as follows. In each round, each agent chooses an arm. All of the
chosen arms are then pulled, with each pulled arm generating a reward. Note that the total number
of arms chosen, c, may be less than d since some agents may choose the same arm. The total reward
is the sum of rewards from the c chosen arms. The optimal policy is for the d agents to collectively
pull the d arms with the highest rewards. Additionally, among all the optimal assignments of d
agents to the d arms that yield the highest reward, We add a bonus reward with probability p* if one
particular agent-to-arms assignment is chosen.
We performed experiments with 4 agents and 10 arms, with the kth arm providing a reward of k .
The exceptional assignment gets a bonus of 200 with probability 0.01, and no bonus with probability
0.99. Thus the maximum expected reward is 36. Training was run for 100,000 rounds for each of
the seeds. Table 4 shows average results for the last 500 of the 100,000 rounds.
Table 4: Performance of LSTM policy parameterization.
	Without Entropy	Crude Entropy	Smoothed Entropy	Unbiased Gradient Estimate
Mean Reward	34.9 土 0.8	35.5 ± 1.1	35.9 ± 0.8	35.9 ± 0.3
Percentage Optimal Arms Pulled	39.8 ± 35.9	59.4 ± 35.7	95.0 ± 1.9	95.7 ± 2.7
The results for the multi-agent bandit problem are consistent with those for the hunter-rabbit prob-
lem. Policies obtained with the entropy approximations all perform better than policies obtained
without entropy or with crude entropy, particularly for the percentage of optimal arms pulled. We
again note that using the unbiased gradient estimate does not perform significantly better than using
the smoothed entropy estimate.
6	Related Work
There has been limited attention in the RL literature with regards to large discrete action spaces.
Pazis & Parr (2011) proposes generalized value functions in the form of H-value functions, and also
propose approximate linear programming as a solution technique. Their methodology is not suited
for deep RL, and approximate linear programming may lead to highly sub-optimal solutions.
7
Under review as a conference paper at ICLR 2018
Dulac-Arnold et al. (2015) embeds discrete actions in a continuous space, picks actions in the con-
tinuous space and map these actions back into the discrete space. However, their algorithm intro-
duces a new hyper-parameter that requires tuning for every new task. Our approach involves no new
hyper-parameter other than those normally used in deep learning.
In Sukhbaatar et al. (2016), each action dimension is treated as an agent and backpropagation is
used to learn coordination between the agents. The approach is particularly adept for problems
where agents leave and enter the system. However, the approach requires homogenous agents, and
has not been shown to solve large-scale problems. Furthermore, the decentralized approach will
potentially lead to highly suboptimal polices even though communication is optimized among the
agents.
To our knowledge, we are the first to propose using LSTMs and a modified MDP to create policies
for RL problems with large multidimensional action spaces. Although this leads to algorithms that
are straightforward, the approaches are natural and well-suited to multidimensional action spaces.
We also propose novel estimators for the entropy regularization term that is often used in policy gra-
dient. To the best of our knowledge, no prior work has dealt with approximating the policy entropy
for MDP with large multidimensional discrete action space. On the other hand, there has been many
attempts to devise methods to encourage beneficial exploration for policy gradient. Nachum et al.
(2016) modifies the entropy term by adding weights to the log action probabilities, leading to a new
optimization objective termed under-appreciated reward exploration.
While entropy regularization has been mostly used in algorithms that explicitly parameterize the
policies, Schulman et al. (2017) applies entropy regularization to Q-learning methods. They make
an important observation about the equivalence between policy gradient and entropy regularized
Q-learning, which they term soft Q-learning.
7	Conclusion
In this paper, we developed a novel policy gradient methodology for the case of large multidi-
mensional discrete action spaces. We proposed two approaches for creating parameterized policies:
LSTM parameterization and a Modified MDP (MMDP) giving rise to Feed-Forward Network (FFN)
parameterization. Both of these approaches provide expressive models to which backpropagation
can be applied for training. We then developed several novel unbiased estimators for entropy bonus
and its gradient. We did experimental work for two environments with large multidimensional ac-
tion space. For these environments, we found that both the LSTM and MMDP approach could
successfully solve large multidimensional action space problems, with the LSTM approach gener-
ally performing better. We also found that the smoothed estimates of the entropy and the unbiased
gradient estimate of the entropy gradient can help reduce computational cost while not sacrificing
significant loss in performance.
References
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc Le. Neural optimizer search with reinforce-
ment learning. 2017.
Wojciech Marian Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan
Pascanu. Sobolev training for neural networks. CoRR, abs/1706.04859, 2017. URL http:
//arxiv.org/abs/1706.04859.
Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap,
Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep rein-
forcement learning in large discrete action spaces, 2015.
R. A. Johnson and D. W. Wichern (eds.). Applied Multivariate Statistical Analysis. Prentice-Hall,
Inc., Upper Saddle River, NJ, USA, 1988. ISBN 0-130-41146-9.
Zeming Lin, Jonas Gehring, Vasil Khalidov, and Gabriel Synnaeve. STARDATA: A starcraft AI
research dataset. CoRR, abs/1708.02139, 2017. URL http://arxiv.org/abs/1708.
02139.
8
Under review as a conference paper at ICLR 2018
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, and et al. Human-
level control through deep reinforcement learning, Feb 2015. URL http://www.nature.
com/nature/journal/v518/n7540/abs/nature14236.html.
Volodymyr Mnih, Adri Puigdomnech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. 2016.
Asier Mujika. Multi-task learning with deep model based reinforcement learning.	CoRR,
abs/1611.01457, 2016. URL http://arxiv.org/abs/1611.01457.
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Improving policy gradient by exploring
under-appreciated rewards, 2016.
Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy
gradient and q-learning, 2016.
Emilio Parisotto, Lei Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and
transfer reinforcement learning. CoRR, abs/1511.06342, 2015. URL http://arxiv.org/
abs/1511.06342.
Jason Pazis and Ronald Parr. Generalized value functions for large action sets, 2011.
John Schulman, Pieter Abbeel, and Xi Chen. Equivalence between policy gradients and soft q-
learning, 2017.
David Silver. Ucl course on rl, 2015. URL http://www0.cs.ucl.ac.uk/staff/d.
silver/web/Teaching.html.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game ofGo with
deep neural networks and tree search. Nature, 529(7587):484-489, jan 2016. ISSN 0028-0836.
doi: 10.1038/nature16961.
Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning multiagent communication with
backpropagation. CoRR, abs/1605.07736, 2016. URL http://arxiv.org/abs/1605.
07736.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks,
2014.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press,
Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
Tieleman and Hinton. Rmsprop: Divide the gradient by a running aver-
age of its recent magnitude - university of toronto, 2012. URL https:
//www.coursera.org/learn/neural-networks/lecture/YQHki/
rmsprop- divide- the- gradient- by- a- running- average- of- its- recent- magnitude.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, John
Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David
Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence,
Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement
learning. CoRR, abs/1708.04782, 2017. URL http://arxiv.org/abs/1708.04782.
9
Under review as a conference paper at ICLR 2018
ZiyU Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray KavUkcUoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. CoRR, abs/1611.01224,
2016. URL http://arxiv.org/abs/1611.01224.
Ronald J. Williams and Jing Peng. FUnction optimization Using connectionist reinforcement learning
algorithms. 1991. doi: 10.1080/09540099108946587.
10
Under review as a conference paper at ICLR 2018
APPENDIX
A Hyperparameters
Hyperparameters for hunter-rabbit game
The LSTM policy has 128 hidden nodes. For the MMDP policy, the number of
hidden layers for smoothed entropy, smoothed mode entropy, unbiased gradient
estimate, crude entropy and without entropy are 5, 3, 3, 4 3 and 3 respectively.
Each MMDP layer has 128 nodes. We parameterize the baseline in (1) with a feed
forward neural network with one hidden layer of size 64. This network was trained
using first visit Monte Carlo return to minimize the L1 loss between actual and
predicted values of states visited during the epidode.
Both the policies and baseline are optimized after each episode with RMSprop
(Tieleman & Hinton (2012)). The RHS of (1) is clipped to [-1, 1] before updat-
ing the policy parameters. The learning rates for the baseline, LSTM and MMDP
are 10-3, 10-3, 10-4 respectively.
To obtain the results in Table 1, the entropy weights for LSTM smoothed entropy,
LSTM smoothed mode entropy, LSTM unbiased gradient estimate, LSTM crude
entropy, MMDP smoothed entropy, MMDP smoothed mode entropy, MMDP unbi-
ased gradient estimate and MMDP crude entropy are 0.02, 0.021, 0.031, 0.04, 0.02,
0.03, 0.03 and 0.01 respectively.
To obtain the results in Table 3, the entropy weights for LSTM smoothed entropy,
LSTM exact entropy, MMDP unbiased gradient estimate and MMDP exact entropy
are 0.03, 0.01, 0.03 and 0.01 respectively. The MMDP networks have three layers
with 128 nodes in each layer. Experimental results are averaged over five seeds.
Hyperparamters for Multi-Agent Multi-Arm Bandits
The experiments were run with 4 agents and 10 arms. For the 10 arms, their rewards
are i for i = 1, . . . , 10. The LSTM policy has 32 hidden nodes. The baseline
in (1) is a truncated average of the reward of the last 100 rounds. The entropy
weight for crude entropy, smoothed entropy and unbiased gradient estimate are
0.005, 0.001 and 0.003 respectively. The learning rates for without entropy, crude
entropy, smoothed entropy and unbiased gradient estimate are 0.006, 0.008, 0.002
and 0.005 respectively. Experimental results are averaged over ten seeds.
B Proof of Theorem 2
We first note that for X1	〜N ( μ1
X2∖	μ [^2_
random vectors, we have X2 | X1 = x1 〜
Σ11 Σ12
, Σ21 Σ22
N(μ, Σ) where
where X1 and X2 are
μ = μ + ∑2i∑-ι1(xι 一 μι)
ς = ς22 - ς21ς11ς12
Observe that the covariance matrix of the conditional distribution does not depend
on the value ofx1 (Johnson & Wichern (1988)).
11
Under review as a conference paper at ICLR 2018
Also note that for X 〜N(μ, Σ), the entropy of X takes the form
H (X) = 2(log 2π + 1) + 2 IςI
where k is the dimension of X and | ∙ | denotes the determinant. Therefore, the
entropy of a multivariate normal random variable depends only on the variance and
not on the mean.
Because A is multivariate normal, the distribution ofAi given A1 = a1, . . . , Ai-1 =
ai-1 has a normal distribution with a variance σi2 that does not depend on
a1 , . . . , ai-1. Therefore
Hθ(Ai∖aι,..., ai-1) = 2(log 2π + 1 + σ2)
does not depend on a1 , . . . , ai-1 and hence Hθ(a) does not depend on a. Combin-
ing this with the fact that Hθ(a) is an unbiased estimator for Hθ gives Hθ(a) = Hθ
for all a ∈ A.
C Proof of Theorem 4
From (3), we have:
dd
Vθ Hθ = -XX
E A~pθ [log Pθ (AiIAi-1) J log Pθ (AjIAj-1)]	(4)
We will now use conditional expectation to calculate the terms in the double sum.
For i < j :
Ea~pθ [log Pθ (Ai | Ai-i)Vθ log pθ (AjIAj-1) | Aj-1]
= log Pθ (AiIAi-i)Ea~pθ [Vθ log Pθ (AjIAj-1)∣Aj-1] = 0
For i > j :
EA~pθ[log Pθ (Ai ∣Ai-ι)Vθ log Pθ (AjIAj-1)∣Ai-1]
=Vθ log pθ (AjIAj-ι)EA~pθ[log Pθ (AiIAi-1)]
=-Vθ log pθ (AjIAj-I) Hθ" (Ai-I)
For i = j :
eA~pθ [log pθ (AiIAi-I)。log pθ (AiIAi-I)IAi-1] = -VθHθ" (Ai-I)
Combining these three conditional expectations with (4), we obtain:
d	i-1
VθHθ = Ea~pθ[VθHθ(A) + X Hθi)(Ai-ι)Vθ XlogPθ(AjIAj-ι)]
i=1	j=1
12
Under review as a conference paper at ICLR 2018
D Approximating entropy using the mode of the distribution
Depending on the episodic action a at a given time step in the episode, the smoothed
entropy Hθ (a) may give unsatisfactory results. For example, suppose for a partic-
ular episodic action a, Hθ(a) Hθ. In this case, the policy gradient may ignore
the entropy bonus term, thinking that the policy already has enough entropy when
it perhaps does not. We therefore consider alternative approximations which may
improve performance at modest additional computational cost.
First consider
d
E * =—ΣΣPθ(a∣a*,..., a*-ι) logpθ(a∣a*,..., a*-ι)
i=1 a∈Ai
where
a* = (a1*, . . . , a*d) = argmax pθ (a)
a∈A
Thus in this case, instead of calculating the entropy over a sample action a, we
calculate it over the most likely action a*. The problem here is that it is not easy to
find a* when the given conditional probabilities p°(a|ai,..., a—) are not in closed
form but only available algorithmically as outputs of neural networks.
A more computationally efficient approach would be to choose the action greedily:
^ι = argmax pθ (a)
a∈A1
^2 = argmax pθ (a∣aι)
a∈A2
.
.
.
ad-1 = argmax Pθ (a|&i,..., ^d-2)
a∈Ad-1
This leads to the definition
d
Hbθ = -ΣΣPθ (a∣^ι,..., ai-ι)log Pθ (a|&i,..., ^i-ι)
i=1 a∈Ai
The action a is an approximation for the mode of the distribution pθ(∙). As often
done in NLP, we can use beam search to determine an action a0 that is a better
approximation, that is, p0(a0) ≥ p0(a). Indeed, the above Hbθ definition is beam
search with beam size equal to 1. We refer to Hbθ as smoothed mode entropy.
Hθ with an appropriate beam size may be a better approximation for the entropy
Hθ than Hθ(a). However, calculating Hθ and its gradient comes with some compu-
tational cost. For example, with a beam size equal to one, we would have to make
two passes through the neural network at each time step: one to obtain the episodic
sample a and the other to obtain the greedy action a. For beam size n We would
need to make n + 1 passes.
We note that Hbθ is a biased estimator for Hθ but with no variance. Thus there is a
bias-variance tradeoff between Hθ(a) and Hθ. Note that Hθ also satisfies Theorems
2 and 3 in subsection 4.2.
13