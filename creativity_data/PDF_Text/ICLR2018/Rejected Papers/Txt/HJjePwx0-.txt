Under review as a conference paper at ICLR 2018
Better Generalization by Efficient Trust Re-
gion Method
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we develop a trust region method for training deep neural networks.
At each iteration, trust region method computes the search direction by solving a
non-convex subproblem. Solving this subproblem is non-trivial—existing meth-
ods have only sub-linear convergence rate. In the first part, we show that a simple
modification of gradient descent algorithm can converge to a global minimizer
of the subproblem with an asymptotic linear convergence rate. Moreover, our
method only requires Hessian-vector products, which can be computed efficiently
by back-propagation in neural networks. In the second part, we apply our algo-
rithm to train large-scale convolutional neural networks, such as VGG and Mo-
bileNets. Although trust region method is about 3 times slower than SGD in terms
of running time, we observe it finds a model that has lower generalization (test)
error than SGD, and this difference is even more significant in large batch training.
We conduct several interesting experiments to support our conjecture that the trust
region method can avoid sharp local minima.
1	Introduction
Despite having many optimization algorithms in the literature (Nesterov, 2013; Wright & Nocedal,
1999; Boyd & Vandenberghe, 2004), when talking about neural network optimization, by far the
most popular methods are SGD and its variants such as AdaDelta (Zeiler, 2012), AdaGrad (Duchi
et al., 2011), Adam (Kingma & Ba, 2014) and RMSProp (Hinton et al., 2006). Compared to these
stochastic first order methods, second order methods are rarely used in training deep neural networks
due to large data files and high dimensional parameter space. Moreover, neural network has non-
convex loss function and its Hessian is often ill-conditioned, making it difficult to apply second order
methods. Prior to our work, there aren’t many works on second order methods for neural network
optimization (Martens, 2010; Kiros, 2013; Botev et al., 2017), and most of them are tested on multi-
layer perceptron so it is unclear whether second order methods can be useful for more advanced
networks such as convolutional neural networks.
In this paper, we develop an efficient trust region method for training deep neural networks, where
the objective function is approximated by a quadratic term with a norm constraint, known as the
trust-region subproblem. The main difficulty in large-scale applications is how to solve this non-
convex subproblem efficiently (Yuan, 2000). Approximate solvers such as (Steihaug, 1983; Powell,
1970) cannot converge to the exact minimizer, while the best method for computing the global min-
imizer has sub-linear convergence rate (Hazan & Koren, 2016). We propose a new solver that is
guaranteed to converge to a global minimum of the nonconvex trust region sub-problem with an
asymptotic linear convergence rate. What’s more, our method only requires Hessian-vector product,
which only costs two forward-backward operations in neural networks. Empirically we find this
technique making trust region method comparable to SGD in terms of running time.
Apart from convergence rate, generalization ability is another important issue to practitioners. Start-
ing from (Zhang et al., 2016) which proposed the generalization paradox of deep models, inten-
sive works have been done to find ways to explain and close generalization gap in large batch SGD
(Keskar et al., 2016; Kawaguchi et al., 2017; Wilson et al., 2017). Among them, (Keskar et al., 2016)
empirically finds the generalization gap between large and small batch and later (Wilson et al., 2017)
notices that adaptive methods like Adam, AdaGrad and RMSprop are even worse than SGD con-
cerning large batch generalization. To close the generalization gap, (Chaudhari et al., 2016) changes
1
Under review as a conference paper at ICLR 2018
Algorithm 1 (Stochastic) Trust region method
Input: {F(x), VF(x), V2F(x)}, or their stochastic counterparts; Initial guess x°, radius r0.
Output: Suboptimal solution: xN+1.
1:	for t = 0,1,…，N do
2:	Calculate gradient g = VF(xt), H = V2 F(xt)
3:	Expand objective function:
arg min f (Z) = LzTHz + gτz
z]∣z∣∣2≤rt	2
(1)
4:
5:
Solve (1) to get zt , compute qt
F (xt)-F (xt + zt)
f(0)-f(zt)
(Deterministic version) Update iterate and radius:
•	Ifqt	≥	0.9,	then	xt+1	= xt+	zt,	rt+1 = 2rt.
•	Ifqt	≥	0.1,	then	xt+1	= xt+	zt,	rt+1 = rt.
•	Ifqt	<	0.1,	then	xt+1	= xt,	rt+1	= 0.5rt.
6:	(Stochastic version) Update iterate and radius:
•	Ifqt	≥	0.9	and	kgk	≥	rt,	then xt+1 = xt+	zt, rt+1	= min{2rt,rmax}.
•	Otherwise, xt+1 = xt, rt+1 = 0.5rt.
7:	end for
8:	return xN+1 .
the loss function to eliminate sharp local minima, (De et al., 2016) selects batch size adaptively and
(Hoffer et al., 2017) combines adaptive step size with new batch normalization layer.
In contrast to these works, we find through experiments that using trust region method alone can
effectively escape from sharp minima and achieve lower test error than SGD, especially on large-
batch training. For CIFAR-10 with VGG network, our trust region based method achieves 86.8%
accuracy and our hybrid method have 88.1%, both of them are better than SGD’s solution with
86.7% accuracy when batch size is 128. As batch size grows to 2048, our trust region method still
has 85.5% accuracy and our hybrid method is 87.0%, in contrast, SGD only has 80.3% accuracy.
Arguably, our method is a more natural solution to large batch training because it can match the small
batch accuracy simply by switching to trust region method without changing network structure, loss
function and batch size.
2	Background and Related Work
2.1	(Stochastic) Trust region method
Trust region (TR) method (Conn et al., 2000) (Algorithm 1) is a classical second order method for
solving nonlinear optimization problems. At each iteration, TR forms a second order approximation
around the current solution and updates the solution by minimizing this quadratic function within the
“trust region”. The trust region, usually a hyper ball, is chosen to make the second order approxima-
tion close enough to the original one and thus by minimizing the subproblem, the objective function
will very likely to descent. More formally, suppose F(x) is the objective function, at iteration xt the
subproblem can be written as:
arg min f (z) := ：zTHz + gτz, s.t. kz∣∣2 ≤ r.	(2)
z2
Where H = V2F(xt), g = VF(xt), and r > 0 is the radius of trust region.1
If objective function F has finite sum structure, F(x) = 1 pn=1 fi (x) or more generally F(x)=
Eξf(x, ξ) where ξ is random variable with bounded variance, then we can turn to stochastic trust re-
gion method discussed in (Chen et al., 2015). Both deterministic and stochastic trust region method
1In general the constraint can be k ∙∣∣m for any PSD matrix M, but We only discuss the simpler case here
because a Change-of-variable Z = VMz and H0 = M- THVM can transform it to (2).
2
Under review as a conference paper at ICLR 2018
have very similar algorithm as one can see in Algorithm 1. As to convergence rate, (Blanchet
et al., 2016) shows that the stochastic version is guaranteed to find a stationary Pointk ▽/(x) k ≤ E
in O(-2) iterations, while the deterministic version is able to find -suboptimal stationary point
within O(E-3/2) iterations (Curtis et al., 2017). For large scale Problem when we cannot calculate
the gradient and Hessian on the whole dataset, stochastic method is the only choice.
To aPPly trust region method to large-scale non-convex oPtimization (e.g., training deeP networks),
there are two difficulties: (a) The Hessian matrix H may not be Positive definite, so (2) is a non-
convex Problem with constraint and it is hard to comPute its global minimizer. (b) H is very large
(millions by millions) and cannot be formed exPlicitly. For (a), several algorithms have been Pro-
Posed in the literature. Traditionally, dogleg method (Powell, 1970) for H 0 and Steihaugs
method (Steihaug, 1983) for indefinite H can be aPPlied to get an aPProximated solution. However
if a more Precise solution is desired, we need to find other ways, e.g. (Hazan & Koren, 2016) is able
to find an E-SUboPtImaI solution in O(M∕√e) time, where M is the matrix Size (number of nonzero
elements). In this PaPer, we design an efficient solver based Purely on gradient information that can
find an E-suboPtimal solution in asymPtotically linear time O(M log(1/E)).
For (b), due to high dimensionality of H, matrix decomPosition such as SVD will be in-feasible.
Therefore, an efficient subProblem solver should only involve comPutations of Hessian-vector Prod-
uct, which can be comPuted by back-ProPagation in deeP networks (Pearlmutter, 1994; Baydin et al.,
2015). Furthermore, since machine learning Problems can usually be formulated as finite sum struc-
ture F(x) = N PN=1 fi(x), instead of computing the exact Hessian We can compute the subsam-
Pled Hessian on the selected batch, so the subsamPled Hessian-vector Product comPutation is only
slower than SGD computation by a constant factor. Our proposed subproblem solver only involves
Hessian-vector product, so it can be applied to large-scale problems and replace SGD seamlessly.
This subsampled version is justified by the stochastic trust region method described above.
2.2	Other Second Order Methods for Training Neural Networks
Another related technique for non-convex optimization is adaptive regularization with cubic
(ARC) (GrieWank,1981). Similar to TR method, ARC uses cubic term of positive coefficient 3 ∣∣zk2
to replace constraint in (2):
arg min f (z) ：= 2ZTHz + g|z + 3 ∣∣z∣2.	(3)
ARC method can be seen as the “twin” method of TR, since they share many similar properties (e.g.
converge rates). Intuitively the radius parameter r in (2) plays the same role as σ in ARC: a larger
r and smaller σ both mean larger “trust-region”. However, since the ARC subproblem is uncon-
strained, the treatment is relatively easy: for example, generalized Lanczos-type iterations (Gould
et al., 1999) or gradient descent (Carmon & Duchi, 2016) can be used. Our proposed subproblem
solver for TR is also based on gradient descent, but due to constraint in the TR subproblem our
algorithm and proof are different from (Carmon & Duchi, 2016).
Recently, several works have applied second order methods to neural network optimization, includ-
ing (Martens, 2010; Botev et al., 2017; Xu et al., 2017b;a). However, all of them only consider
small-scale fully connected networks with few layers. Among them, (Xu et al., 2017b;a) adopt full
gradient and subsampled Hessian and solve the trust region subproblem in the subspace spanned
by Cauchy point and minimum eigenvector v1(H). We are different from this recent paper in two
aspects. First, we develop a new trust region subproblem solver with better theoretical guarantee.
Second, the full gradient computation in their algorithm is too expensive for large-scale deep net-
work training. In contrast, we implement our algorithm with subsampled gradient and Hessian, thus
we are able to run large-scale experiments and observe better generalization compared with SGD.
2.3	Generalization Ability for First Order Methods
SGD and other stochastic first order methods are popular for training large scale neural networks
since gradient for each training sample can be computed in one forward-backward cycle. Recently
there are many works on the effect of batch size on training time as well as generalization ability.
Indeed, a larger batch makes more use of parallel processors, so the program can scale perfectly to
3
Under review as a conference paper at ICLR 2018
more GPUs. Moreover, more samples make the gradient less noisy and thus a larger learning rate
and higher converge rate are possible, see (Goyal et al., 2017; You et al., 2017a;b) and references
therein.
At the same time, large batch in SGD is harmful to generalization ability, experiments in (Keskar
et al., 2016) show that batch size can affect the geometry around the solution that SGD finds. Specif-
ically, SGD with a larger batch size tends to find sharper local minima, so even a tiny change of data
from training set to test set will lead to high loss, i.e. bad generalization ability. As to the converge
rate to local minima, although (Lee et al., 2016) shows gradient descent always converges to local
minima, (Du et al., 2017) proves the converge process can take exponential time. On the other hand
by adding extra noise, (Ge et al., 2015) claims SGD is able to escape strict saddle in poly(d/) iter-
ations, and further (Jin et al., 2017) design a full gradient with noise method that improves the result
to poly- log(d)/2 steps.
Notations: Throughout this paper, we use Sn-1 to denote the unit sphere in Rn. The model pa-
rameters X ∈ Rd and objective function F(x)= 焉 PN=I fi(x) is in C2; its gradient and Hessian
at step Xt are denoted as gt = VF(Xt) and Ht = V2F(xt), and if the index t is unimportant We
will simplify them as g and H respectively. For the Hessian matrix H, its eigenvalue decomposition
is H = Pd=I λiViV∣ such that v；Vj = δj and λ1 ≤ λ? ≤ ∙∙∙ ≤ λd, accordingly we decompose
any vector a ∈ Rd as a = Pcd=I a(i)Vi. Denote operator norm β = ∣∣H∣∣op = max{∣λ1∣, ∣λd∣}.
k ∙ k represents 2-norm if not stated explicitly, Id ∈ Rd×d is the identity matrix. Finally, following
(Carmon & Duchi, 2016) we use s to represent the global minimizer of trust region subproblem (2).
3	Efficient Trust Region subproblem solver
Algorithm 2 Proposed trust region subproblem solver
Input: Gradient g, Hessian matrix H
Output: Approximated solution z
1:	Initialize z0 and η so that Assumption 1,2 are met.
2:	boundary=false
3:	for t = 0, 1, . . . , N1do
4:	if ∣zt ∣ < 1 then
5:	zt+1 = zt - η(Hzt + g)
6:	else
7:	boundary = true
8:	break
9:	end if
10:	end for
11:	if boundary = false then return zN1+1
12:	end if
13:	for t0 = t, t + 1, . . . , N2 do
14:	Choose αt0 by Armijo line search
15，	_ zt0-αt0 (Id-Zt0 z∣0 )(HztO +g)
:	Zt' + 1 — kzt0-αt0(Id-Zt0 z∣0 )(Hzt0+g)k
16:	end for
17:	return zN2+1
In this section, we show an efficient gradient-descent based algorithm with proper initialization can
find the global minimum of the trust region subproblem (2). If the global minimum lies inside of
the sphere then gradient descent itself is guaranteed to find it; Otherwise we first conduct gradient
descent until the iterate hits the spherical constraint, then a manifold gradient descent on the sphere
can converge to the solution. We prove this simple procedure can return the global minimum of the
non-convex trust region subproblem and has asymptotically linear convergence rate. The details are
shown in Algorithm 2.
4
Under review as a conference paper at ICLR 2018
3.1	Properties of global minimum
The necessary and sufficient condition of global minimum comes from KKT condition, see (Wright
& Nocedal, 1999) for details.
Lemma 1. (Global minimum) s is the global minimum of (2) if and only if ksk ≤ 1 and there is a
scalar λ ≥ 0 such that:
gradient condition: (H + λId)s + g = 0,
complementary slackness: λ(1 - ksk2) = 0,	(4)
Hessian: H + λId	0.
Proposition 1. Based on (4) we can describe the solution(s) as follows:
•	λ1 > 0: only one global minimum. If kH-1gk < 1 then ksk < 1, otherwise ksk = 1.
•	λ1 = 0: if g(1) 6= 0, only one solution with ksk = 1; Otherwise if g(1) = 0 and
pn=2(gλ^))2 ≥ 1, one solution with ∣∣s∣∣ = 1; If g⑴=0 and Pn=?(R)2 < 1, there
are multiple solutions.
•	λ1< 0, g(1) 6= 0: only one solution.
•	λ1 < 0, g⑴=0: if Pn=2(λg-λ] )2 ≥ 1 then only one global minimum, otherwise there
are multiple ones.
Proof details are postponed to appendix. Since when λ1 ≥ 0 every stationary points are global
minimum, and further if ksk < 1 then we only need to use gradient descent to solve it (that the case
for Line 11 in Algorithm 2), otherwise ksk = 1 and our manifold based algorithm also applies, since
in this case subproblem is strongly convex, we don’t need to worry about converging to suboptimal
point. Now we only need to consider about λ1< 0 case as follows. Notice that in this case we always
have ksk = 1. In the following lemma we try to distinguish global minimum from other stationary
points, restricted to g(1) 6= 0 case (we call it “easy-case”, as oppose to “hard-case” in (Wright &
Nocedal, 1999)). As to the “hard-case” g(1) = 0, in theory we can apply a small perturbation to b
as (Carmon & Duchi, 2016) does: b0 = b + ε where ε is a small Gaussian noise. In practice due to
rounding error this case is hardly seen, for best efficiency we choose to ignore it.
The following lemma gives a sufficient condition of global minimum:
Lemma 2. When g(1) 6= 0 and λ1 < 0, among all stationary points ifs(1)g(1) ≤ 0 then s is the
global minimum.
3.2	Convergence of iteration
Now it left to see how the solution found by Algorithm 2 will meet Lemma 2. To this end, we need
to enforce following assumptions:
Assumption 1. (Bounded SteP size) Step size η < 1∕β.
Assumption 2. (Initialize) zo = —α 岛,0 < α < 1.
We remark that Assumption 2 is a good guess of global minimum if ignoring the curvature informa-
tion, and using this alone as subproblem solution will reduce trust region method to gradient descent.
Under these assumptions, Algorithm 2 is guaranteed to find (one of) the global minimum, which is
formally stated in the following theorem:
Theorem 3. Under proximal gradient descent update: zt+ι = ProXk∙∣∣ (Zt — ηVf (Zt)), and As-
sumption 1 if zt(i)g(i) ≤ 0 then zt(+i)1g(i) ≤ 0. Combining with Assumption 2 and Lemma 2, if
λ1< 0, g(1) 6= 0 then Zt converges to a global minimum s.
To see the converging process more clearly, we divide the iterations into two phases: in the first phase
Zt stays strictly inside the sphere Sn-1: kZtk < 1, and during this phase we will show that kZtk is
monotone increasing until it hits the sphere and that is when the second phase begins. In the second
phase the iterates adhere to the sphere and converge to the global minimum with asymptotically
linear rate. First of all we show the monotone increasing property based on following lemma:
5
Under review as a conference paper at ICLR 2018
Lemma 4. For Zt defined above, we have z|H▽/(Zt) ≥ βz[Pf(zt) (recall we define β as the
operator norm of H).
Now imagine there is another iterate Zt that does “plain" GD (i.e. without projection): Zt+ι =
Zt — η^f (Zt), using the same step size as Zt and same initialization Zo = z0. Such a iteration rule
guarantees that as long as kZtk < 1 for all Z ≤ τ then ZZt = Zt. By showing kZZtk is monotone
increasing, we actually prove monotone increasing property in the first phase.
Theorem 5. Suppose Zt is in the region such that proximal gradient update equals to plain GD:
Zt+1 = Zt 一 ηVf (Zt), then under this update rule, kZtk is monotone increasing.
Another key observation from Theorem 5 is the unboundedness of kZZt k, i.e. kZZt k → ∞ as t → ∞.
Indeed we have the following lemma:
Lemma 6. (Finitephase I) Assuming λι < 0, suppose t* istheindeXthatk Zt* ∣∣ < 1 and IlZt* + ι∣∣ ≥
1, then t* is bounded by:
1	1	Z(1)	1
t ≤log(1-ηλ1)	hlog(η∣gd)∣-西)一log(祈一ηλi)i.	⑸
So after t* iterations the algorithm will reach phase II. Then we turn to manifold optimization
method, which is discussed extensively in (Absil et al., 2009) and for the manifold theory we re-
fer (Do Carmo & Flaherty Francis, 1992). Here we list some concepts and its explicit form in our
problem: denote M as the smooth manifold, and Z ∈ M can be any point on manifold M. The
tangent space of Z, denoted as Tz, is the set of all tangent vectors to M at Z. When M = Sn-1
then Tz = {ξ ∈ Rn : Zlξ = 0}; Retraction is a mapping Rz(ξ) : ξ ∈ TZ → M, in Sn-1 mani-
fold, one of the commonly used retraction is Rz(ξ) = ɪf+^； Projection onto TZ, denoted as PZξ
is a mapping from Rn to Tz, for Sn-1 the projection is simply Pz (ξ) = (Id 一 ZZl)ξ. Based on
above concepts, We can write the gradient descent on M as: Zt+ι = RZt (-αtgradf (Zt)), and at is
tactically selected by line search such that f(Zt) 一 f(Zt+1) ≥ σαtIgradf (xt)I, where σ ∈ (0, 1).
Because we already know at least one of the global minimum lies on Sn-1, and according to Lemma
6 after at most t* iterations we start to use manifold based optimization, i.e. the subproblem be-
comes:
min f(z) = LZIHZ + glZ.	(6)
Z∈Sn-1	2
Therefore, our method shrinks the search space from IZI ≤ 1 to IZI = 1, by doing so we can apply
well studied manifold optimization theory (Absil et al., 2009; Udriste, 1994) to our problem. Indeed
we have following theorems:
Theorem 7.	Let {Zt} be an infinite sequence of iterates generated by line search gradient descent,
then every accumulation point of {Zt} is a stationary point of the cost function f.
This above only guarantees convergence to stationary points, however, according to Theorem 3 if
the step size αt is not too large, it actually converges to global minimum. Then it remains to show a
linear convergence rate, as guaranteed by the following theorem:
Theorem 8.	Let {Zt} be an infinite sequence of iterates generated by line search gradient descent,
suppose it converges to s. Let λH,min and λH,max be the smallest and largest eigenvalues of the
Hessian at s. Assume that s is a local minimizer then λH,min > 0 and given r in the interval (r*, 1)
with r* = 1 — min (2σαλH,mE, 4σ(1 — σ)β：H,min ), there exists an integer K such that:
f (Zt+1)- f (s) ≤ r(f (Zt)- f(s)),
for all t ≥ K.
Remarks: λH,min and λH,max are the minimum and maximum eigenvalue of Riemannian Hessian.
Specifically the Riemannian Hessian can be calculated by(Proposition 5.5.4, Absil et al. (2009)):
Hess f(x) = Hess(f ◦ Expx)(0x),
(Hess f (x)[ξ],ξi = (Hess (f ◦ EXPx)(0χ)[ξ],ξ).
(7)
6
Under review as a conference paper at ICLR 2018
By direct calculation(in appendix) We shall see:hHessf (s)[ξ], ξ = -slHs + ξlHξ - gls, where
ξ ∈ Null(s), kξk = 1. By optimal condition (4), we have:
sl(H + λId)s + slg = 0 ⇒ -slHs - gls = λ,	(8)
!!
so hHessf(s)[ξ], ξi = λ + ξlHξ ≥ λ + λ1 > 0. Where > is guaranteed by gradient condition in
(4):
(λ1 + λ)s(1) + g(1) =0,	(9)
in “easy-case”, g(1) 6= 0, so λ1 +λ 6= 0 and Hessian condition in (4) can be improved to λ1 +λ > 0.
Based on above discussion, we know λH,min ≥ λ + λ1 > 0 and λH,max ≤ λ + λn .
4 Experiments
In this section, we examine the performance of our algorithm. First of all, we use a random generated
problem to check the dynamics of our proposed trust region subproblem solver, then we compare
the performance of our trust region method with SGD on deep convolutional neural network. After
that, we focus on generalization ability of the solution that trust region method returns. We include
following algorithms in our experiments:
•	SGD: We tune the step size to get the fastest, stable convergence.
•	TR: Trust region method with our proposed subproblem solver. For each subproblem, we
choose N1 = N2 = 1 (defined in Algorithm 2). The trust region radius is set to 1 and the
step size of subproblem solver is set to 0.1.
•	Hybrid: Our proposed hybrid method, it runs trust region method until the accuracy no
longer increases, and then uses the last epoch to initialize SGD method. When the training
accuracy stabilizes, it returns the final model. Both TR and SGD here follow the same
hyper-parameters above.
4.1	S olving the Trust Region Subproblem
We sample an indefinite random matrix by H = BBl — λIn, where B ∈ Rn×(n-1) and Bij 尢
N(0,1), obviously λmg(H) = λι = -λ. Afterwards we sample a vector g by gi 匕 N(0,1).
By changing the value of λ in {10, 30, 50, 70, 90, 110}, we plot the function value decrement with
respect to number of iterations in Figure 1(left). As we can see, the iterates first stay inside of the
sphere (phase I) for a few iterations and then stay on the boundary (phase II). To inspect how λ
changes the duration of phase I, we then plot the number of iterations it takes to reach phase II,
under different λ values shown in Figure 1(middle). Recall in (5), number of iterations is bounded
as a function of λ, which can be further simplified to:
t* ≤
Iog(I + 俞|) = log(1 + cιλ)
iog(i + ηλ) - ιog(i + c2λ),
(10)
where we set z(1) = 0 to simplify the formula. By fitting the data point with function T(λ)
log(1+cιλ)
log(l+c2λ)
we find our bounds given by Lemma 6 is quite accurate.
4.2	Optimizing deep CNNs
Next, we test the performance of our proposed trust region method (actually stochastic is used,
denoted as TR) on training deep CNNs, including VGG (Simonyan & Zisserman, 2014) and Mo-
bileNets (Howard et al., 2017). VGG/MobileNets are adopted to classify CIFAR10/STL10 data.
During all experiments, we only compare trust region method with plain SGD. We did not include
other SGD variants because (1) We want to focus on whether second order information is useful
without considering the effect of momentum/acceleration. (2) It is known that SGD generalize bet-
ter than adaptive methods in large-batch (Wilson et al., 2017). For reference, we also list the best
accuracy obtained by Adam with small batch size in Table 1, which shows Adam’s accuracy is only
slightly better than SGD but still worse than our hybrid method.
7
Under review as a conference paper at ICLR 2018
w,w'w∙LL0-∙
111
uouur⅛>一fqo
Figure 1: Left: Trust region experiment, we use solid lines to indicate iterations inside the sphere
and dash lines to indicate iterations on the sphere. By changing λ we can modify the function
curvature. Middle: #Iteration it takes to reach sphere under different λ's, We also fit the curve by
model T = lθg([C1λ) derived in Lemma 6. Right: Comparing ReLU with SReLU (see definition
in Section 4.2), as α closes to zero, SReLU looks more like ReLU.
We have some tricks to make it possible to apply trust region method to a deep convolutional net-
work: First of all, rather than taking ReLU as the activation function, we use a similar yet everywhere
continuously differentiable function, namely SReLU (see Figure 1(right)):
SReLU(X)= 7x2+α + X, α> 0.
Although we can also define Hessian on ReLU function, it is not well supported on major platforms
(Theano/PyTorch). Likewise, we find max-pooling is also not supported by platforms to calculate
higher order derivative, one way to walk around is to change all the max-pooling layers to avg-
pooling——it hurts accuracy a little bit (We observe 1 〜2% accuracy loss compared with (Keskar
et al., 2016)), albeit this is not our primary concern. Secondly, the oracle in TR method is Hessian-
vector product, or Hv. Theoretically, the cost of Hv is comparable to gradient oracle because
Hv = ∂∂w (VT ∂f). However, due to inefficient implementation, in many deep learning frameworks
the cost of Hv is 〜7x slower than gradient oracle, making running time comparison unfavorable, so
we use the following numerical differentiation to replace chain-rule:
Hv = (Vf (x + εv) 一 Vf(X))/ε, where ε → 0.
Experiments show the relative error is controllable (kHcv - Hv k/kHv k ≤ 1%). Nevertheless, the
Hessian-vector operation is still a bottleneck to performance, thus if we care about running time
then 1 〜2 inner iteration(s) for each subproblem is more suitable. Lastly, although in theory we
need full gradient and full Hessian to guarantee convergence (in Xu et al. (2017b) they only need
full gradient), calculating them in each iteration is not practical, so we calculate both Hessian and
gradient on subsampled data to replace the whole dataset.
In the first experiment, we compare the training/testing accuracy w.r.t. running time and epoch on
VGG16+CIFAR10/STL10 dataset. For SGD we choose step size η = 0.1, which is the best step
size in {10-n | n = 1, 2, . . . }. For trust region subproblem we choose step size η0 = 0.1 for all
the experiments. Both of them use batch size B = 512(for CIFAR10) and B = 1024(for STL10).
Our machine has 4 Titan Xp GPUs and Xeon E5-2620 CPU. The experimental results are shown in
Figure 2. Without surprising, the trust region method is 〜3x slower than SGD, partly because we
use subsampled gradient/Hessian (currently there is no theoretical result to guarantee convergence
in this situation) and Hv operation is too expensive. Despite the slower training time, we observe
TR converges to a solution with better test error.
4.3	Better Generalization Ability in Large Batch Training
Training neural network with larger batch size has become an important issue for faster training. To
fully exploit the computational power of multi-GPU systems, we need to increase the batch size,
8
Under review as a conference paper at ICLR 2018
Oooo
4 3 2 1
(％HO±I山U而」1
0	1000	2000
Time(Second)
0	1000	2000
Time(Second)
0	50	100
Epoch
0	50	100
Epoch
Oooo
4 3 2 1
(％)」0J」山U而」1
O -1I-------1---------1-
O 200	400
Time(Second)
35
O
Q 5
5 4
(¾,HOJ9
200	400
Time(Second)
Oooo
4 3 2 1
(求HOJC-EF
50
θɪn------------------Γj 35
O IOO 200	0
Epoch
45
(¾,HOJitt9h
100	200
Epoch
Figure 2: Training/Testing error with both time and epoch. The first row is CIFAR10 dataset(batch
size B = 512) and the second row is STL10 dataset(batch size B = 1024). We find TR method has
better test accuracy on both datasets, although the running time is 2〜3x longer.
at least proportional to number of cores. However, large batch size can affect the generalization
ability (Keskar et al., 2016), so it would be interesting to see if this dilemma can be solved merely
by choosing a different optimization method.
0.2 -
o.o
0	500	1000	1500	2000
Batch size
18
(％H0Jikh
20-
Figure 3: We choose batch size from {64, 128, 256, 512, 1024, 2048}, for each batch size and each
algorithm, we independently run 5 times to gather mean accuracy and deviation (in order to show
significance). The experiment is conducted on VGG16+CIFAR10.
To do so, we design an experiment that compares testing accuracy under different batch sizes in
Figure 3. We observe that our method (TR) converges to solutions with much better test error when
batch size is larger than 128. We postulate this is because SGD has not enough noise to escape from
a sharp local minimum, especially when the large batch is used. For smaller batch size, TR method
is not as good as SGD method, this is because the approximation of gradient and Hessian is not good
enough by only a few samples. However, our Hybrid method combines the benefits of TR and SGD
methods to make it both fast and accurate, for both small and large batch size.
4.4	Loss Landscape of Different Solutions
To explore whether TR/Hybrid methods find the wide local minimum, we follow the experiment
techniques in (Keskar et al., 2016) to draw the loss curve that connects two models. Specifically, if
we use wS?GD, wT?R andwH?ybrid to denote models of SGD, trust region and hybrid method respectively,
9
Under review as a conference paper at ICLR 2018
then the interpolated model in between is
w? = αwA? + (1 - α)wB? , α ∈ [0, 1], A,B ∈ {SGD, TR, Hybrid}.	(11)
The loss and accuracy can be evaluated on both training set and test set by parameters w?, and the
result is displayed in Figure 4. We also calculate the '2-distance between model A and B, shown in
titles of sub-figures.
alpha
alpha
Dist(Hybrid, SGD)=3351.84
Figure 4: Loss(blue lines) and accuracy(red lines) curve of interpolated models, the x-label is α
defined in (11), y-label in the left is the averaged loss and y-label in the right is accuracy. We also
calculate the distance of models between α = 0 and α = 1 in the titles.
Figure 4 clearly shows the width and relative location of local minima: models of TR and hybrid
methods lie in the same wide local minimum, while SGD finds a much sharper local minimum.
Moreover, we notice Dist(TR, SGD) ≈ Dist(Hybrid, SGD) Dist(Hybrid, TR), implying that hy-
brid method is a refinement of TR method (both of them are close to the exact wide local minimum,
but hybrid method is even closer). In contrast, SGD converges to a very different solution, where
the loss curve is sharp and the model does not generalize as good as the other two.
4.5	Properties of Large Batch SGD vs TR
By far, we can only conclude that trust region method generalize better than SGD, especially when
batch size is large. In what follows, we design a novel experiment that help us identifying the dif-
ferences between those two methods. Rather than running a homogeneous algorithm, we interleave
SGD and TR as two tracks, for simplicity we denote SGD(n) as running n epochs SGD, and TR(n)
as running n epochs TR. In the first track, we run SGD(50) in the beginning, then we use the last
iteration in SGD(50) to initialize TR(100), lastly based on the latest iterate, run SGD(50) again. In
the second track we change the order: Begin with TR(100), then SGD(50), and end with TR(50).
The purpose of switching between TR and SGD methods is to detect the character of respective
solutions they find. Before running experiment we do such “thought experiment”: Imagine if large
batch SGD converges to a sharp local minimum, and TR method can successfully escape it, then
initialized by the solution that SGD finds, TR method will “climb over” the loss hill and down to a
wide minimum, during this process the loss function (as well as accuracy) first increases and then
decreases. On the contrary if TR method already finds a wide local minimum, then initialized by
that solution SGD shall not escape to other minima, so we don’t expect a sharp change in either loss
or accuracy. This thought experiment is illustrated in Figure 5.
Our experimental results are presented in Table 1, where we run VGG16+CIFAR10 and choose
batch size B = 2048. The overall running process is shown in the picture, and we further extract
some important data into the table in order to see the differences more clearly.
We explain the results from following angles:
1.	As expectation, there is a sharp raise in both train loss and test error at the 50th epoch
in SGD(50)-TR(100)-SGD(50) track, meaning trust region method does escape from the
solution that SGD(50) converges to. Meanwhile both loss function and test error change
smoothly at the 100th epoch in TR(100)-SGD(50)-TR(50) track, which is understandable
because SGD can’t escape the basin of wide minimum obtained by TR.
10
Under review as a conference paper at ICLR 2018
Figure 5: Illustration of imaginary process of Track 1(left) and Track 2(right). Note that we use
subsampled Hessian and gradient, so the iterate of trust region method will fluctuate around local
minimum.
Ooooo
0 8 6 4 2
1
(％,J 3.υ
SGD-TR-SGD
TR-SGD-TR
	Stage I		Stage II		Stage III	
	method	accuracy	method	accuracy	method	accuracy
Track 1	SGD(50)	80.1%	TR(100)	86.2%	SGD(50)	87.8%
Track 2	TR(100)	85.6%	SGD(50)	87.5%	TR(50)	86.5%
Adam	ADAM(100)	86.6%				
Table 1: Testing accuracy in different stages. In Track 1 we run SGD(50)→TR(100)→SGD(50),
in Track 2 we change the order to TR(100)→SGD(50)→TR(50), so both tracks run 200 epochs in
total. Testing accuracy are reported by the end of each stage. For both tracks we choose batch size
B = 2048 and CIFAR-10 dataset. As a reference, we also run Adam algorithm on small batch
(B = 64).
2.	Warm-started by TR(100), SGD can reach the best testing accuracy, moreover, the accuracy
merely change by enlarging batch size. This sets the foundation of our hybrid method in
Figure 3, where the testing accuracy only drops 0.35% from B = 64 to B = 1024, in
comparison SGD testing accuracy drops 6.97%.
3.	Due to subsampled Hessian and gradient, TR method can only fluctuate around the mini-
mizer, this causes a 1.6% drop in testing accuracy compared to the best result. Another clue
is the 1.0% drop in testing accuracy and raise of training loss at 150th epoch in TR(100)-
SGD(50)-TR(50) track, when SGD(50)→TR(50) happens.
11
Under review as a conference paper at ICLR 2018
We want to emphasize that even though we fixed the step size in SGD while in practice we should
damp the step size after every few epochs, this won’t change the results above, since a smaller
step size makes it even harder to escape from sharp minima. In appendix we have supplementary
experiments by using other networks combined with other datasets to show that these findings are
not specific to one network.
5 Discussion
In this paper we first show that a simple gradient based method can effectively find the global
minimum of trust region subproblem, even if it is nonconvex. By examining the convergence rate as
well as generalization ability, we find our algorithm is comparable to SGD with respect to running
time, but can converge to a solution with better generalization error. We suggest to combine trust
region with SGD to enjoy both fast and accurate properties. As an important future direction, it
would be interesting to see why trust region based algorithm can escape sharp local minima and try
to establish convergence results on subsampled trust region method.
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. arXiv preprint arXiv:1502.05767, 2015.
Jose Blanchet, Coralia Cartis, Matt Menickelly, and Katya Scheinberg. Convergence rate analysis of
a stochastic trust region method for nonconvex optimization. arXiv preprint arXiv:1609.07428,
2016.
Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical gauss-newton optimisation for deep
learning. arXiv preprint arXiv:1706.03662, 2017.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Yair Carmon and John C Duchi. Gradient descent efficiently finds the cubic-regularized non-convex
newton step. arXiv preprint arXiv:1612.00547, 2016.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-sgd: Biasing
gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region
method and random models. Mathematical Programming, pp. 1-41, 2015.
Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods. SIAM, 2000.
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with
a worst-case iteration complexity of∖mathcal {O}(∖epsilon^{-3∕2}) for nonconvex optimization.
Mathematical Programming, 162(1-2):1-32, 2017.
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein. Big batch sgd: Automated inference
using adaptive batch sizes. arXiv preprint arXiv:1610.05792, 2016.
Manfredo Perdigao Do Carmo and J Flaherty Francis. Riemannian geometry, volume 115.
BirkhauserBoston, 1992.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Barnabas Poczos, and Aarti Singh. Gradient
descent can take exponential time to escape saddle points. arXiv preprint arXiv:1705.10412,
2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
12
Under review as a conference paper at ICLR 2018
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Nicholas IM Gould, Stefano Lucidi, Massimo Roma, and Philippe L Toint. Solving the trust-region
subproblem using the lanczos method. SIAM Journal on Optimization, 9(2):504-525, 1999.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Andreas Griewank. The modification of newtons method for unconstrained optimization by bound-
ing cubic terms. Technical report, Technical report NA/12, 1981.
Elad Hazan and Tomer Koren. A linear-time algorithm for trust region problems. Mathematical
Programming, 158(1-2):363-381, 2016.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741, 2017.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ryan Kiros. Training neural networks with stochastic hessian-free optimization. arXiv preprint
arXiv:1301.3641, 2013.
Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent only con-
verges to minimizers. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th An-
nual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research,
pp. 1246-1257, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL
http://proceedings.mlr.press/v49/lee16.html.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), pp. 735-742, 2010.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Michael JD Powell. A new algorithm for unconstrained optimization. Nonlinear programming, pp.
31-65, 1970.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Trond Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM
Journal on Numerical Analysis, 20(3):626-637, 1983.
13
Under review as a conference paper at ICLR 2018
Constantin Udriste. Convex functions and optimization methods on Riemannian manifolds, volume
297. Springer Science & Business Media, 1994.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.
The marginal value of adaptive gradient methods in machine learning. arXiv preprint
arXiv:1705.08292, 2017.
Stephen J Wright and Jorge Nocedal. Numerical optimization. Springer Science, 35(67-68):7, 1999.
Peng Xu, Farbod Roosta-Khorasan, and Michael W Mahoney. Second-order optimization for non-
convex machine learning: An empirical study. arXiv preprint arXiv:1708.07827, 2017a.
Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-type methods for non-
convex optimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017b.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 2017a.
Yang You, Zhao Zhang, Cho-Jui Hsieh, and James Demmel. 100-epoch imagenet training with
alexnet in 24 minutes. arXiv preprint arXiv:1709.05011, 2017b.
Ya-xiang Yuan. A review of trust region algorithms for optimization. In ICIAM, volume 99, pp.
271-282, 2000.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
14
Under review as a conference paper at ICLR 2018
A	Proofs
A.1 Proof of Proposition 1
Proof. For clarity, we repeat (4) in Lemma 1 here:
(H + λId)s + g = 0,
λ(1-ksk2)=0,	(12)
H + λId	0.
i)	If λ1 > 0 then we always have s = -(H + λId)-1g, since ksk ≤ 1, if kH-1gk > 1 then it must
be that λ > 0and ksk = k(H+λId)-1gk = 1.
ii)	If λ1 = 0 and g(1) 6= 0, considering about λs(1) + g(1) = 0 we can infer λ 6= 0 and further
ksk = 1. So in this case the only solution is s = -(H + λId)-1g where λ is the solution of
k(H + λId)-1gk = 1.
iii)	If λ1 = 0 and g(1) = 0, in this case, either λ = 0 or s(1) = 0. If λ = 0 then Hs + g = 0,
this is equivalent to s(i) = 一 g— for i ≥ 2 (suppose λi > 0 since i ≥ 2). On the other hand
λi
∣∣sk ≤ 1, this requires Pn=2(嘤)2 ≤ 1. Otherwise λ = 0 and ∣∣sk = 1, and λ is the solution of
pn=2(R)2 = 1, such λ > 0 must exists if P=2(答)2 > 1.
iv)	If λι < 0 and g(1) = 0, then λ ≥ -λι and ∣∣s∣ = 1. Because g(1) = 0, which implies
λι > 一λι. Immediately we know S = -(H + λId)-1g and λ is the solution of Pn=ι(λ~+∖ )2 = 1,
v) If λ1 < 0 and g(1) = 0. In this condition, λ > 0 and ∣s∣ = 1. By gradient condition, we
see (λι + λ)s⑴ =0 so either λ = -λι or S(I) = 0 (or both). This is determined by ∣∣s∣ =
S(I)2 + pn=2(λ++λ-)2 = 1, if pn=2(λg-λ] )2 ≤ 1 then it is appropriate to set λ = -λι and
S(I) = ± JI - Pn=2 s(i)2. Otherwise if P：=2(弋⑺入、)2 > 1 then it must be that S(I) = 0 and
λ > 一λι such that Pn=2(λg+λ")2 = 1 holds, one can see such equation has only one solution.
□
A.2 Proof of Lemma 2
Proof. Suppose S is a stationary point, by formulating the Lagrange multiplier and using KKT
condition, there exists a λ ≥ 0 such that (H + λId)S + g = 0, and further by complementary
slackness if ∣∣s∣ < 1 then Vf (S) = HS + g = 0 this leads to λ = 0, otherwise ∣∣s∣ = 1 so in both
cases the first and second conditions in (4) are reached. According to Lemma 1 if S is not global
minimum then the third condition should be violated, implying λ1 + λ < 0, by gradient condition
(λ1 + λ)S(1) + g(1) = 0, because g(1) 6= 0 indicating S(1) 6= 0, multiply both sides by S(1) we get
S(1)g(1) > 0, that is the case for stationary points excluding global minimum. On the contrary for
global minimum, we must have S(I)g(I) ≤ 0.	□
A.3 Proof of Theorem 3
Proof. Notice the projection onto sphere will not change the sign of zt(+i)1, so:
Sgn (z(+ιg(i)) = Sgn ((1 - ηλi)z(i)g(i) - ηg(i)2)
ηt < 1∕λn ensures 1-ηλi > 0 for all i ∈ [n]. From Assumption 2 we know z0i)g(i) = -αg^ ≤ 0,
so Zyig⑸ ≤ 0 for all t.	□
A.4 Proof of Lemma 4
Proof. Define w(i) = z(i)/(-ηg(i)), then by iteration rule:
wt(+i)1 = (1 一 ηλi)wt(i) + 1,
(13)
15
Under review as a conference paper at ICLR 2018
solving this geometric series, we get:
(14)
(i)	(i)
suppose at t-th iteration, wt ≥ wt+1 which is equivalent to:
-ηλi ≥ (1-ηλ"-
(15)
because from Assumption 2 We know w0i) ≥ 0, if w0i) -	≤ 0, i.e. 0 < η∖i ≤ 1∕w((i) then
by (15) we know 1 - ηλi ≥ 1 ⇔ ηλi ≤ 0 leading to a contradiction, so w0i) - ηλ^ > 0 and
1 - ηλi ≤ 1. On the other hand, λj ≥ λi for j ≥ i, so 1 - ηλj ≤ 1, together with w0j) -/=
z((i) - / ≥ w0i) - ηλ" > 0 we conclude:
j)- ηλj ≥(I-叭 Mwj- ηλ~)⇔ Wj)
≥ wt(+j)1 forj ≥ i.
For any t, suppose i ∈ [n] is the smallest coordinate index such that w(i ) ≥ w(+), which implies
Wi < wi+ι for any i < i* and Wi ≥ wi+1 for any i ≥ i (such a i* may not exist, but it doesn't
matter). By analyzing the sign of zt we know:
Sgn (Zi(Z(i) - ⅛)ι)) = Sgn (w(i)(w(i) - w(+ι)) = Sgn (w(i) - w(+l),
finally we have:
ZlANf(Zt)
≥
≥
i*—1	n
-X λiz(i)(Zi)-Z(+1) + -X λiz(i)(z(i) - z(++l)
η i=1	η i=i*
i*—1	n
—X z(i)(z(i) - z(+ι) + 比 X z(i)(z(i) - z(+ι)
η	i=1	η i=i*
n
λ* X z(i)(z(i) - z(+ι)
η i=1
(16)
≥ βz∣Vf(zj
□
A.5 Proof of Theorem 5
Proof. First of all, notice ∣∣≡t+ιk2 = IlztlI2 — 2ηZ[V f(Z↑) + η2kVf (Zt)k2, so it remains to show
Z|Vf (Zt) ≤ 0. We prove this by induction rule, suppose z∣-ιVf (Zt-ι) ≤ 0 and from Zt =
Zt—1 - ηVf (Zt-ι) we know:
ZIVf(Zt)=ZLiVf (Zt-1)- η∣Vf (Zt-ι)k2 - ηZ]-iAVf (Zt-1)
'---------------------------------------------{z----}
(1)
+ η2Vf(Zt-1)τAVf (Zt-1).
X---------{----------}
(2)
From Lemma 4 we know (1) ≥ βZZt-1Vf(ZZt-1) and recall β is the operator norm of A, we have
(2) ≤ β∣Vf(ZZt-1)∣2, so:
Z|Vf (Zt) ≤ (1 - βη)Z[-Nf(Zt-1) - η(1 - ηβ)∣Vf (Zt)k2,	(17)
naturally, by choosing η < 1∕β we have ZIVf (Zt) ≤ 0 for all t, based on this observation ∣∣Zt+1k
is monotone increasing.	□
16
Under review as a conference paper at ICLR 2018
A.6 Proof of Lemma 6
Proof. This directly follows from:
1 ≤ η2g(1)2w(1+ι = z(1)2ι ≤ k再*+ιk2,
together with (14) immediately comes to (5).	□
A.7 Proof of Theorem 7,8
See Theorem 4.3.1 and Theorem 4.5.6 in (Absil et al., 2009).
A.8 CALCULATING λH,min AND λH,max
This is mainly brute force calculation. By definition (7), we know for ξ ∈ TxM,
(Hess f(χ)[ξ],ξi =(Hess (f ◦ Rχ)(0χ)[ξ],ξi =塞f (Rχ(tξ))[ɪɔ ,	(18)
we then expand f(Rx (tξ)) to,
,∕κ, ξ,cH	ξτHξ∙t2 + ξτHx ∙t + XTHxjgTx + gτξ ∙ t	noλ
f(Rx(tξ)) =----------2F+ξ≡-------------+ kx + tξk2 .	(19)
By differentiating t twice and set t = 0 (this can be done by software), we finally get
(Hess f(x)[ξ],ξi = -xτHx + ξτHξ - gτx.	(20)
Thus the eigenvalue of Riemannian Hessian comes from definition:
λH
,min =	min (Hess f(x)[ξ], ξi,
kξk=1,ξ∈Tx M
λH
,max =	max (Hess f(x)[ξ], ξi.
kξk=1,ξ∈Tx M
B S upplementary Experiments
B.1	MOBILENETS+CIFAR10
We apply MobileNets (Howard et al., 2017) to classify CIFAR10 data, MobileNets is a light-weight
network that designed for mobile and embedded vision applications. Here we still use η = 0.1 as
the step size for SGD, for TR we use step size η0 = 0.01, the batch size is B = 2048. Other settings
are the same with VGG16 experiment in the main text. We find the overall experiment outcome are
quit similar to VGG16 network (see Figure 6).
	Stage I	Stage II	Stage III method accuracy method accuracy method accuracy
Track 1 Track 2	SGD(50)	74.57%	TR(100)	80.22%	SGD(50)	81.46% TR(100)	78.42%	SGD(50)	79.69%	TR(50)	79.06%
Table 2: Testing accuracy in different stages. In Track 1 we first run 50 epochs SGD then 100
epochs TR followed by 50 epochs SGD; In Track 2 we change the order to first 100 epochs TR then
50 epochs SGD and end by 50 epochs TR, so both tracks run 200 epochs in total. Testing accuracy
are reported by the end of each stage. Here we choose batch size B = 2048.
B.2	VGG16+STL10
Now we change our dataset to STL10 but still using VGG16 model. The parameter setting are the
same with VGG16+CIFAR10. We choose batch size B = 1024, note that STL10 has only 5000
training samples, so even the batch size is as small as 1k can have considerable generalization loss.
As can be seen in Figure 7 and Table 3.
17
Under review as a conference paper at ICLR 2018
Figure 6: Experiments on two tracks as described in Table 2, we display training loss and test error
w.r.t epochs, the batch size is B = 2048.
Stage I	Stage II	Stage III
	method	accuracy	method	accuracy	method	accuracy
Track 1	SGD(150)	62.4%	TR(300)	64.6%	SGD(50)	66.0%
Track 2	TR(300)	63.6%	SGD(50)	66.3%	TR(150)	65.8%
Table 3: Testing accuracy in different stages. In Track 1 we first run 150 epochs SGD then 300
epochs TR followed by 50 epochs SGD; In Track 2 we change the order to first 300 epochs TR then
50 epochs SGD and end by 150 epochs TR, so both tracks run 500 epochs in total. Testing accuracy
are reported by the end of each stage. Here we choose batch size B = 1024.
O ŋ O O
8 6 4 2
(⅜eJ 3」」H
SSO-U-BJl
SGD-TR-SGD
TR-SGD-TR
IOO
200	300
Epoch
400	500
Figure 7: Experiments on two tracks as described in Table 3, we display training loss and test error
w.r.t epochs, the batch size is B = 1024.
18