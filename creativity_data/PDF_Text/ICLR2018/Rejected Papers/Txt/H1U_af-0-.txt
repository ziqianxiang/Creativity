Under review as a conference paper at ICLR 2018
Quadrature-based	features for kernel
APPROXIMATION
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of improving kernel approximation via randomized fea-
ture maps. These maps arise as Monte Carlo approximation to integral represen-
tations of kernel functions and scale up kernel methods for larger datasets. We
propose to use more efficient numerical integration technique to obtain better es-
timates of the integrals compared to the state-of-the-art methods. Our approach
allows the use of information about the integrand to enhance approximation and
facilitates fast computations. We derive the convergence behavior and conduct an
extensive empirical study that supports our hypothesis.
1 Introduction
Kernel methods proved to be an efficient technique in numerous real-world problems. The core idea
of kernel methods is the kernel trick - compute an inner product in a high-dimensional (or even
infinite-dimensional) feature space by means of a kernel function k:
k(x, y) = hψ(x), ψ(y)i,	(1)
where ψ : X → F is a non-linear feature map transporting elements of input space X into a feature
space F . It is a common knowledge that kernel methods incur space and time complexity infeasible
to be used with large-scale datasets directly. For example, kernel regression has O(N3 + Nd2)
training time, O(N 2) memory, O(N d) prediction time complexity for N data points in original
d-dimensional space X. One of the most successful techniques to handle this problem (Rahimi &
Recht (2008)) introduces a low-dimensional randomized approximation to feature maps:
k(x,y) ≈ Ψ(x)>Ψ(y).	(2)
This is essentially carried out by using Monte-Carlo sampling to approximate scalar product in (1).
A randomized D-dimensional mapping Ψ(∙) applied to the original data input allows employing
standard linear methods, i.e. reverting the kernel trick. In doing so one reduces the complexity to
that of linear methods, e.g. D-dimensional approximation admits O(N D2) training time, O(N D)
memory and O(N) prediction time.
It is well known that as D → ∞, the inner product in (2) converges to exact kernel k(x, y). Recent
research (Yang et al. (2014); Felix et al. (2016); Choromanski & Sindhwani (2016)) aims to improve
the convergence of approximation so that a smaller D can be used to obtain the same quality of
approximation.
This paper considers kernels that allow the following integral representation
1	kwk2
k(	x, y) = Eq(w)gxy (W) ≈ Ep(w)fxy (W)= Ifxy ), P(W) = (2π)d∕2 e--^,	⑶
where q(w) is a density associated with a kernel, e.g. the popular Gaussian kernel has
q(W) = p(W), so the exact equality holds with gxy(W) = fxy(W) = φ(W>x)>φ(W>y), where
φ(∙) = [cos(∙), sin(∙)]>.
The class of kernels admitting the form in (3) covers shift-invariant kernels (e.g. radial basis function
(RBF) kernels) and Pointwise Nonlinear Gaussian (PNG) kernels. They are widely used in practice
and have interesting connections with neural networks (Cho & Saul (2009), Williams (1997)).
1
Under review as a conference paper at ICLR 2018
The main challenge for the construction of low-dimensional feature maps is the approximation of
the expectation in (3) which is d-dimensional integral with Gaussian weight. While standard Monte-
Carlo rule is easy to implement, there are better quadrature rules for such kind of integrals. For
example, Yang et al. (2014) apply quasi-Monte Carlo (QMC) rules and obtain better quality kernel
matrix approximations compared to random Fourier features of Rahimi & Recht (2008).
Unlike other research studies we refrain from using simple Monte Carlo estimate of the integral,
instead, we propose to use specific quadrature rules. We now list our contributions:
1.	We propose to use advanced quadrature rules to improve kernel approximation accuracy.
We also provide an analytical estimate of the error for the used quadrature rules.
2.	We note that for kernels with specific integrand fxy (w) in (3) one can improve on its
properties. For example, for kernels with even function fxy (w) we derive the reduced
quadrature rule which gives twice smaller embedded dimension D with the same accuracy.
This applies, for example, to any RBF kernel.
3.	We use structured orthogonal matrices (so-called butterfly matrices) when designing
quadrature rule that allow fast matrix by vector multiplications. As a result, we speed
up the approximation of the kernel function and reduce memory requirements.
4.	We demonstrate our approach on a set of regression and classification problems. Empirical
results show that the proposed approach has a better quality of approximation of kernel
function as well as better quality of classification and regression when using different ker-
nels.
2 Quadrature-based random features
We start with rewriting the expectation in equation (3) as integral of fxy with respect to p(w):
I(fxy)
(2∏)-d「
-∞	-∞
e-w>>w fxy(w)dw.
Integration can be performed by means of quadrature rules. The rules usually take a form of interpo-
lating function that is easy to integrate. Given such a rule, one may sample points from the domain
of integration and calculate the value of the rule at these points. Then, the sample average of the rule
values would yield the approximation of the integral.
We use the average of sampled quadrature rules developed by Genz & Monahan (1998) to yield un-
biased estimates of I(fxy). A change of coordinates is the first step to facilitate stochastic spherical-
radial rules. Now, let w = rz, with z>z = 1, so that w>w = r2 for r ∈ [0, ∞], leaving us with
∞
I (fχy ) = (2∏)-d
Ud 0
d∞
rd-1 fxy(rz)drdz =	— / / e-r2 ∣r∣d-1fχy (rz)drdz, (4)
Ud -∞
where Ud = {z : z>z = 1, z ∈ Rd}. As mentioned earlier we are going to use a combination of
radial R and spherical S rules. We now describe the logic behind the used quadratures.
Stochastic radial rules. Stochastic radial rule R(h) of degree 2l + 1 has the form of weighted
symmetric sums:
l
R(h) = Xwi
i=0
h(ρi) + h(-ρi)
2
∞ r2
where h is an integrand in infinite range integral T(h) = J e-万 ∣r∣d-1h(r)dr. Togetan unbiased
-∞
estimate for T (h), points ρi are sampled from specific distributions which depend on the degree of
the rule. Weights wi are derived so that R has a polynomial degree 2l + 1, i.e. is exact for integrands
h(r) = rp with p = 0, 1, . . . , 2l + 1. For radial rules of degree three R3 the point ρ0 = 0, while
Pi 〜χ(d + 2) follows Chi-distribution with d + 2 degrees of freedom. Higher degrees require
samples from more complex distributions which are hard to sample from.
2
Under review as a conference paper at ICLR 2018
Stochastic spherical rules. Spherical rule S(s) approximates an integral of a function s(z) over
the surface of unit d-sphere Ud and takes the following form:
p
S(s) = X wej s(zj),
j=1
where Zj are points on Ud, i.e. z>z = 1. If we set weight Wj =忐d⅛y and sum function S values
at original and reflected vertices vj of randomly rotated d-simplex V, we will end up with a degree
three rule:
d+1
X fxy(-Qvj) +fxy(Qvj) ,
j=1
SQ3 (fxy (ρQz)) =
|Ud|
2(d + 1)
where vj is the j’th vertex of d-simplex V with vertices on Ud and Q is a random d × d orthogonal
matrix. We justify the choice of the degree in Appendix A.
Since the value of the integral is approximated as the sample average, the key to unbiased estimate
is proper randomization. In this case, randomization is attained through the matrix Q. It is crucial
to generate uniformly random orthogonal matrices to achieve an unbiased estimate for spherical
surface integrals. We consider various designs of such matrices further in Section 3.
Stochastic spherical-radial rules. Meanwhile, combining foregoing rules results in stochastic
spherical-radial rule of degree three:
SZfxy =(1 - P) fχy(0) + E X [ fxy(-ρQvj2ρ+fxy(PQvj)],⑸
33
which we finally apply to the approximation of (4) by averaging the samples of SR3Q,3,ρ :
1n
I(f) = EQ,ρ[sRQ,,ρ(fxy)] ≈ I(fxy) = .	SRQi — (fxy),	(6)
i=1
where n is the number of sampled SR rules. Speaking in terms of approximate feature maps, the
new feature dimension D in case of quadrature based approximation equals 2n(d + 1) as we sample
n rules and evaluate each of them at 2(d + 1) points. Surprisingly, empirical results (see Section 5)
show that even a small number of rule samples n provides accurate approximations.
Properties of the integrand. We also note here that for specific functions fxy (w) we can derive
better versions of SR rule by taking on advantage of the knowledge about the integrand. For exam-
ple, the Gaussian kernel has fxy(w) = cos(w> (x - y)). Note that f is even, so we can discard an
excessive term in the summation in (5), since f(w) = f (-w), i.e SR3,3 rule reduces to
SRQw(fxy) =(1 - P) fxy(0) + E X fxyTQv2∙	⑺
Variance of the error. We contribute the variance estimation for the stochastic spherical-radial
rules when applied to kernel function. To the best of our knowledge, it has not been done before. In
case of kernel functions the integrand fxy can be represented as
fxy(w) = φ(w>x)>φ(w>y) = g(z1, z2),	z1 = w>x,	z2 = w>y,	(8)
where z1 , z2 are scalar values. Using this representation of kernel function and its Taylor expansion
we can obtain the following proposition (see Appendix B for detailed derivation of the result):
Proposition 2.1. The quadrature rule (6) is an unbiased estimate of integral of any integrable func-
tion f. If function f can be represented in the form (8), i.e. f(w) = g(z1, z2), z1 = w>x,
z2 = w>y for some x, y ∈ Rd, all 4-th order partial derivatives ofg are bounded and D = 2n(d+1)
is the number of generated features, then
^	2.66M2 L8	212MiM2L6	(d + 95)M2L4
[If)] ≤ -n- +	nd3	+ 4nd2(d - 2),
3
Under review as a conference paper at ICLR 2018
∂4g	∂4g	∂4g	∂2g
where MI = max	sup	dΞ4	, sup	dΞ4	, sup	dT2dT2 I	, M2	= .max2 < L 乃 2-j (O, O)I	:
z	∂z1 z	∂z2 z	∂z1∂z2	j=0,1,2 ∂z1∂z2
andL = max{kxk, kyk}.
Constants M1, M2 in the proposition are upper bounds on the derivatives of function g and don’t
depend on the data set, while L plays the role of the scale of inputs. The proposition implies that the
error of approximation is proportional to L - the less the scale, the better the accuracy (see Figure
1). However, scaling input vectors is equivalent to changing the parameters of the kernel function.
For example, decreasing the norm of input variables for RBF kernel is equivalent to increasing the
kernel width σ . Therefore, the wide RBF kernels are approximated better than the narrow ones. This
result also gives us the rate of convergence O(1/nd) for the quadrature rule.
Figure 1: Relative error of approximation of kernel matrix with 95% confidence interval depending
on the scaling factor, Gaussian kernel was used. In this experiment for each scaling factor α we
construct approximate kernel matrix Kb and the exact kernel matrix K using scaled input vectors
X = x∕α. To plot the confidence interval We run each experiment 10 times each time generating
new weights. Experiment was conducted for 3 different input dimensions: d = 1O, 1OO, 5OO.
The quadrature rule (5) grants us some freedom in the choice of random orthogonal matrix Q.
The next section discusses such matrices and suggests butterfly matrices for fast matrix by vector
multiplication as the SR3,3 rule implementation involves multiplication of the matrix QV by the
data vector x.
3 Generating uniformly random orthogonal matrices
Previously described stochastic spherical-radial rules require a random orthogonal matrix Q (see
equation (5)). If Q follows Haar distribution on the set of all matrices in the orthogonal group O(d)
in dimension d, then the averages of spherical rules SQ3 (s) provide unbiased degree three estimates
for integrals over unit sphere. Essentially, Haar distribution means that all orthogonal matrices in the
group are equiprobable, i.e. uniformly random. Methods for sampling such matrices vary in their
complexity of generation and multiplication.
Techniques based on QR decomposition (Mezzadri (2006)) have complexity cubic in d, and the re-
sulting matrix does not allow fast matrix by vector multiplications. Another set of methods is based
on a sequence of reflectors (Stewart (1980)) or rotators (Anderson et al. (1987)). The complexity
is better (quadratic in d), however the resulting matrix is unstructured and, thus, implicates no fast
matrix by vector multiplication. In Choromanski et al. (2017) random orthogonal matrices are con-
sidered. They are constructed as a product of random diagonal matrices and Hadamard matrices and
therefore enable fast matrix by vector products. Unfortunately, they are not guaranteed to follow the
Haar distribution.
To satisfy both our requirements, i.e low computational/space complexity and generation of Haar
distributed orthogonal matrices, we propose to use so-called butterfly matrices.
Butterfly matrices. The method from Genz (1998) generates Haar distributed random orthogonal
matrix B. As it happens to be a product of butterfly structured factors, a matrix of this type conve-
niently possesses the property of fast multiplication. For d = 4 an example of butterfly orthogonal
4
Under review as a conference paper at ICLR 2018
matrix is
B(4)
c1
s1
0
0
-s1
c1
0
0
c3
s3
-s3
c3
Definition 3.1. Let ci = cos θi,
si
c2
0
s2
0
c2
0
s2
-s2
0
c2
0
sin θi for i
-s2
0
c2
c1c2
s1c2
c3s2
s3s2
-s1c2
c1c2
-s3s2
c3s2
-c1s2
-s1 s2
c3c2
s3c2
s1s2
-c1 s2
-s3 c2
c3c2
1, . . . , d - 1 be given. Assume d
2k with
k > 0. Then an orthogonal matrix B(d) ∈ Rd×d is defined recursively as follows
B(2d)
B(d)cd
B⑷Sd
-B(d)sd
B⑷Cd
B(1) = 1,
0
0
0
0
0
0
where B(d) is the same as B(d) With indexes i shifted by d, e.g.
B(2)
c1	-S1
S1	c1
c3	-S3
S3 c3
B⑵
Matrix B(d) by vector product has computational complexity O(d log d) since B(d) has dlog de
factors and each factor requires O(d) operations. Another advantage is space complexity: B(d) is
fully determined by d - 1 angles θi, yielding O(d) memory complexity.
One can easily define butterfly matrix B(d) for the cases when dis not a power of two (see Appendix
C.1 for details). The randomization is based on the sampling of angles θ and we discuss it in
Appendix C.2. The method that uses butterfly orthogonal matrices is denoted B in the experiments
section.
4 Kernels
This section gives examples on how quadrature rules can be applied to a number of kernels.
4.1	Gaussian kernel
Radial basis function (RBF) kernels are popular kernels widely used in kernel methods. Gaussian
kernel is a widely exploited RBF kernel and has the following form:
k(x, y) = exp
kx-yk2
2σ2

In this case the integral representation has φ(w>x) = [cos(w>x), sin(w>x)]>. Since fxy(0) = 1,
SR3,3 rule for Gaussian kernel has the form (σ appears due to scaling):
SRQ,,ρ(fxy)
d
d + 1
4.2	Arc-cosine kernels
Arc-cosine kernels were originally introduced by Cho & Saul (2009) upon studying the connections
between deep learning and kernel methods. The integral representation of the bth-order arc-cosine
kernel is
kb(x, y)
2
Rd
φb(w>x)φb(w>y)p(w)dw,
where φb(w>x) = Θ(w>x)(w>x)b, Θ(∙) is the Heaviside function and P is the density of the
standard Gaussian distribution. Such kernels can be seen as an inner product between the represen-
tation produced by infinitely wide single layer neural network with random Gaussian weights. They
have closed form expression in terms of the angle θ = cos-1 (口晨>m)between X and y.
0th-order arc-cosine kernel is given by ko(x, y) = 1 - θ, 1st-order kernel is given by kι(x, y)
kx∏kyk (Sin θ + (π — θ) cos θ).
5
Under review as a conference paper at ICLR 2018
Table 1: Space and time complexity for different kernel approximation algorithms.
Method	Space	Time
ORF	O(Dd)	O(Dd)
QMC	O(Dd)	O(Dd)
ROM	O(d)	O(d log d)
Quadrature based	O(d)	O(d log d)
Let φ0 (w>x) = Θ(w>x) and φ1(w>x) = max(0, w>x), then we can rewrite the integral rep-
>	>	2 n 33
resentation as follows: kb(x, y) = 2 JRd φb(w>x)φb(w>y)p(w)dw ≈ 2 P SRQa ρ ∙ For arc—
i=1	i
cosine kernel of order 0 the value of the function φ0(0) = Θ(0) = 0.5 results in
SRQ3p(fXy)=0.25 (1 - P2) + d++i X
fxy (PQVj) + fχy (-PQVj)
In the case of arc-cosine kernel of order 1, the value of φ1(0) is 0 and the SR3,3 rule reduces to
SR3,3 ( f ) = d XX fχy (IPQVjI)
Q,p(fxy)= d +1 M	2ρ2	.
4.3 Explicit mapping
The explicit mapping can be written as follows:
ψ(x) = [ a0fχy(0) a1fχy(w1>x)	. . .	aDfχy(wD>x) ]
where a0
1 - ρ21, for i = 1,∙∙∙,D
ai
~Λ∕2(d+i), Wi is the row in matrix W. The
matrix W = ρ [ - (QV)> ]. To get D features one simply stacks n = 2(4)such matrices Wj =
Pj h - ((QQjj VV))>> i so that W ∈ R2n(d+1)×d, where only Qj ∈ Rd×d and Pj (j = 1, . . . , n) are
generated randomly.
For example, in case of the Gaussian kernel the mapping can be rewritten as 2
ψG (x) =	[a0	a1 cos(w1>x)	. . .	aD	cos(wD> x)	a1 sin(w1>x)	. . .	aD	sin(wD> x)].
5	Experiments
We extensively study the proposed method on several established benchmarking datasets: Power-
plant, LETTER, USPS, MNIST, CIFAR100 (Krizhevsky & Hinton (2009)), LEUKEMIA (Golub
et al. (1999)). In Section 5.2 we show kernel approximation error across different kernels and num-
ber of features. We also report the quality of SVM models with approximate kernels on the same
data sets in Section 5.3. The compared methods are described below.
5.1	Methods
We present a comparison of our method with estimators based on simple Monte Carlo3 . The Monte
Carlo approach has a variety of ways to generate samples: unstructured Gaussian (Rahimi & Recht
(2008)), structured Gaussian (Felix et al. (2016)), random orthogonal matrices (ROM) (Choroman-
ski et al. (2017)).
1It may be the case when sampling P that 1 — ρd2 < 0, simple solution is just to resample P to satisfy the
non-negativity of the expression.
2We do not use reflected points for Gaussian kernel as noted earlier, so Wj = ρj (QjV)>.
3We also study quasi-Monte Carlo (Yang et al. (2014)) performance. See Appendix D for details.
6
Under review as a conference paper at ICLR 2018
O ①UISoɔlɔjv
MNIST
6.0 -
4.5 -
3.0 -
1. 5 -
3.0 -
2.4
1. 8 -
1. 2 -
0.6 -
0.0 -
LEUKEMIA
2.1-×10-
1. 8 -
1. 5 -
1. 2 -
0.9 -
0.6 -
n
0.5	1. 0	1. 5	2.0
× 10
2 4 6 8 0
-----
3 2 10 0
=H =
-⅛l⅛-
①UISoɔlɔjv
× 10 - 2
1	2	3	4	5
×10-2	×10-2
5
12	3	4
0.5	1. 0	1. 5	2.0
Gort
ROM
G
H
B
uc0Issnc0f>
1	2	3	4	5	1	2	3	4	5
3. 0 -
2.4-
1. 8 -
1.2 -
0.6 -
4
n	n	n	n
Figure 2: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,
Gaussian) on three datasets: LETTER (d = 16), MNIST (d = 784), CIFAR100 (d = 3072) and
LEUKEMIA(d = 7129). Lower is better. The x-axis represents the factor to which we extend the
original feature space, n =哥片,where d is the dimensionality of the original feature space, D is
the dimensionality of the new feature space.
Monte Carlo integration. The kernel is estimated as k(x, y) = %φ(Mx)φ(My), where M ∈
RD×d is a random weight matrix. For unstructured Gaussian based approximation M = G, where
G is a random matrix with iidN (0, 1) elements. Structured Gaussian has M = Gort, where Gort =
DQ, Q is obtained from RQ decomposition of G (note that this is not the same Q used in the
quadrature rules), D is a diagonal matrix with diagonal elements sampled from the χ(d) distribution.
In compliance with the previous work on ROM we use S-Rademacher with three blocks: M =
3
√d Q SDi since three blocks have been shown to yield the best results.
i=1
Quadrature rules. Our main method that uses stochastic spherical-radial rules with Q = Be 4
(butterfly matrix) is denoted by B. As mentioned earlier we also include a variant of our algorithm
that uses an orthogonal matrix Q based on a sequence of random reflectors (we denote it as H).
5.2	Kernel approximation
To measure kernel approximation quality We use relative error in Frobenius norm 平-下山,where
K and K denote exact kernel matrix and its approximation. We run experiments for the kernel
approximation on a random subset of a dataset (see Appendix D for details). Approximation was
constructed for different number of SR samples n = 2(d+i), where d is an original feature space
dimensionality and D is the new one. For the Gaussian kernel We set hyperparameter Y = 表 to
the same value for all the approximants, while arc-cosine kernels have no hyperparameters.
We run experiments for each [kernel, dataset, n] tuple and plot 95% confidence interval around
the mean value line. Figure 2 show results for kernel approximation error on LETTER, MNIST,
CIFAR100 and LEUKEMIA datasets.
4Be = (BP)1(BP)2 . . . (BP)3, where P is a permutation matrix, for explanation see Appendix C.
7
Under review as a conference paper at ICLR 2018
0 ①UISoulu.lv
Z 0.875 -
近 0.850 -
1 0.825 -
0 0.800 -
品 0.775 -
Powerplant
0.8 -
0.7 -
0.6 -
0.5 -
0.4 -
0.3 -
1	2	3	4	5
n
LETTER
0.957 -
0.954 -
0.951 -
0.948 -
0.945 -
1	2	3	4	5
n
ZHZAOBJnOOB
UBlSSn0
0.94
⅛ 0.92
G 0.90
0 0.88
S 0.86
0. 0.84
2	3	4	5
n
0.825 -
0.800 -
0.775 -
0.750 -
0.725 -
0.700 -
0.9730 -
0.9715 -
0.9700 -
0.9685 -
0.9670 -
0.9655 -
5
--exact
. Gort
—ROM
-B- G
→- B
0.936 -
0.933 -
0.930 -
0.927 -
1	2	3	4	5
12	3	4
n
0.9632 -
0.9624 -
0.9616 -
0.9608 -
0.9600 -
1	2	3	4	5
n
I ①UlSOUIUJV
Figure 3: Accuracy/R2 score using embeddings with three kernels (columns: arc-cosine 0, arc-
cosine 1, Gaussian) on three datasets (rows: Powerplant, LETTER, USPS). Higher is better. The
x-axis represents the factor to which We extend the original feature space, n = 2(指),where d is
the dimensionality of the original feature space, D is the dimensionality of the new feature space.
We drop one of our methods H here since its kernel approximation almost coincides with B.
We observe that for the most of the datasets and kernels the methods we propose in the paper (B,
H) show better results than the baselines. They do coincide almost everywhere, which is expected,
as the B method is only different from H in the choice of the matrix Q to facilitate speed up.
5.3	Classification/regression with new features
We report accuracy and R2 scores for the classification and regression tasks on the same data sets
(see Figure 3). We examine the performance with the same setting (the number of runs for each
[kernel, dataset, n] tuple) as in experiments for kernel approximation error, except now we map the
whole dataset. We use Support Vector Machines to obtain predictions. We also drop one of our
methods H here since its kernel approximation almost coincides with B.
Kernel approximation error does not fully define the final prediction accuracy - the best performing
kernel matrix approximant not necessarily yields the best accuracy or R2 score. However, the em-
pirical results illustrate that our method delivers comparable and often the best quality on the final
tasks. We also note that in many cases our method provides greater performance using less number
of features n, e.g. LETTER and Powerplant datasets with arc-cosine kernel of the first order.
6	Related work
The most popular methods for scaling up kernel methods are based on a low-rank approxima-
tion of the kernel using either data-dependent or independent basis functions. The first one in-
cludes NyStrOm method (Drineas & Mahoney (2005)), greedy basis selection techniques (Smola &
Scholkopf (2000)), incomplete Cholesky decomposition (Fine & Scheinberg (2001)).
The construction of basis functions in these techniques utilizes the given training set making them
more attractive for some problems compared to Random Fourier Features approach. In general,
data-dependent approaches perform better than data-independent approaches when there is a gap in
the eigen-spectrum of the kernel matrix. The rigorous study of generalization performance of both
approaches can be found in (Yang et al. (2012)).
In data-independent techniques, the kernel function is approximated directly. Most of the meth-
ods (including the proposed approach) that follow this idea are based on Random Fourier Features
8
Under review as a conference paper at ICLR 2018
(Rahimi & Recht (2008)). They require so-called weight matrix that can be generated in a number
of ways. Le et al. (2013) form the weight matrix as a product of structured matrices. It enables fast
computation of matrix-vector products and speeds up generation of random features.
Another work (Felix et al. (2016)) orthogonalizes the features by means of orthogonal weight matrix.
This leads to less correlated and more informative features increasing the quality of approximation.
They support this result both analytically and empirically. The authors also introduce matrices with
some special structure for fast computations. Choromanski et al. (2017) propose a generalization of
the ideas from (Le et al. (2013)) and (Felix et al. (2016)), delivering an analytical estimate for the
mean squared error (MSE) of approximation.
All these works use simple Monte Carlo sampling. However, the convergence can be improved by
changing Monte Carlo sampling to Quasi-Monte Carlo sampling. Following this idea Yang et al.
(2014) apply quasi-Monte Carlo to Random Fourier Features. In (Yu et al. (2015)) the authors
make attempt to improve quality of the approximation of Random Fourier Features by optimizing
sequences conditioning on a given dataset.
Among the recent papers there are works that, similar to our approach, use the numerical integra-
tion methods to approximate kernels. While Bach (2017) carefully inspects the connection between
random features and quadratures, they did not provide any practically useful explicit mappings for
kernels. Leveraging the connection Dao et al. (2017) propose several methods with Gaussian quadra-
tures, among them three schemes are data-independent and one is data-dependent. The authors do
not compare them with the approaches for random feature generation other than random Fourier
features. The data-dependent scheme optimizes the weights for the quadrature points to yield better
performance.
7	Conclusion
In this work we proposed to apply advanced integration rule that allowed us to achieve higher quality
of kernel approximation. Our derivation of the variance of the error implies the dependence of the
error on the scale of data, which in case of Gaussian kernel can be interpreted as width of the
kernel. However, as we have seen earlier, accuracy on the final task has no direct dependence on the
approximation quality, so we can only speculate whether better approximated wide kernels deliver
better accuracy compared to the poorer approximated narrow ones. It is interesting to explore this
connection in the future work.
To speed up the computations we employed butterfly orthogonal matrices yielding the computa-
tional complexity O(d log d). Although the procedure we used to generate butterfly matrices claims
to produce uniformly random orthogonal matrices, we found that it is not always so. However, the
comparison of the method H (uses properly distributed orthogonal matrices) with method B (some-
times fails to do so) did not reveal any differences. We also leave it for the future investigation.
Our experimental study confirms that for many kernels on the most datasets the proposed approach
delivers better kernel approximation. Additionally, the empirical results showed that the quality
of the final task (classification/regression) is also higher than the state-of-the-art baselines. The
connection between the final score and the kernel approximation error is to be explored as well.
References
Theodore W Anderson, Ingram Olkin, and Les G Underhill. Generation of random orthogonal
matrices. SIAM Journal on Scientific and Statistical Computing, 8(4):625-629,1987. 4
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.
Journal of Machine Learning Research, 18(21):1-38, 2017. 9
John A Baker. Integration over spheres and the divergence theorem for balls. The American Mathe-
matical Monthly, 104(1):36-47, 1997. 13
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in Neural
Information Processing Systems, pp. 342-350, 2009. 1, 5
9
Under review as a conference paper at ICLR 2018
Krzysztof Choromanski and Vikas Sindhwani. Recycling randomness with structure for sublinear
time kernel expansions. arXiv preprint arXiv:1605.09049, 2016. 1
Krzysztof Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness of
random orthogonal embeddings. arXiv preprint arXiv:1703.00864, 2017. 4, 6, 9
Tri Dao, Christopher M De Sa, and Christopher Re. Gaussian quadrature for kernel features. In
Advances in Neural Information Processing Systems, pp. 6109-6119, 2017. 9,17
Petros Drineas and Michael W Mahoney. On the Nystrom method for approximating a Gram matrix
for improved kernel-based learning. Journal of Machine Learning Research, 6(Dec):2153-2175,
2005. 8
Kai-Tai Fang and Run-Ze Li. Some methods for generating both an NT-net and the uniform distri-
bution on a Stiefel manifold and their applications. Computational Statistics & Data Analysis, 24
(1):29-46, 1997. 15
X Yu Felix, Ananda Theertha Suresh, Krzysztof M Choromanski, Daniel N Holtmann-Rice, and
Sanjiv Kumar. Orthogonal Random Features. In Advances in Neural Information Processing
Systems, pp. 1975-1983, 2016. 1,6, 9, 16
Shai Fine and Katya Scheinberg. Efficient SVM training using low-rank kernel representations.
Journal of Machine Learning Research, 2(Dec):243-264, 2001. 8
Alan Genz. Methods for generating random orthogonal matrices. Monte Carlo and Quasi-Monte
Carlo Methods, pp. 199-213, 1998. 4
Alan Genz and John Monahan. Stochastic integration rules for infinite regions. SIAM journal on
scientific computing, 19(2):426-439, 1998. 2
Alan Genz and John Monahan. A stochastic algorithm for high-dimensional integrals over un-
bounded regions with gaussian weight. Journal of Computational and Applied Mathematics, 112
(1):71-81, 1999. 12
Todd R Golub, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P
Mesirov, Hilary Coller, Mignon L Loh, James R Downing, Mark A Caligiuri, et al. Molecu-
lar classification of cancer: class discovery and class prediction by gene expression monitoring.
Science, 286(5439):531-537, 1999. 6
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
6
Quoc Le, TamaS Sarios, and Alex Smola. Fastfood-approximating kernel expansions in Ioglinear
time. In Proceedings of the International Conference on Machine Learning, 2013. 9
Francesco Mezzadri. How to generate random matrices from the classical compact groups. arXiv
preprint math-ph/0609050, 2006. 4
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems, pp. 1177-1184, 2008. 1, 2, 6, 9
Alex J Smola and Bernhard Scholkopf. Sparse greedy matrix approximation for machine learning.
2000. 8
G. W. Stewart. The efficient generation of random orthogonal matrices with an application to con-
dition estimators. SIAM Journal on Numerical Analysis, 17(3):403-409, 1980. ISSN 00361429.
URL http://www.jstor.org/stable/2156882. 4
Christopher KI Williams. Computing with infinite networks. In Advances in Neural Information
Processing Systems, pp. 295-301, 1997. 1
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney. Quasi-Monte Carlo feature
maps for shift-invariant kernels. In Proceedings of The 31st International Conference on Machine
Learning (ICML-14), pp. 485-493, 2014. 1, 2, 6, 9
10
Under review as a conference paper at ICLR 2018
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-HUa Zhou. Nystrom Method
vs Random Fourier Features: A Theoretical and Empirical Comparison. In Advances in Neural
Information Processing Systems,pp. 476-484, 2012. 8
Felix X Yu, Sanjiv Kumar, Henry Rowley, and Shih-Fu Chang. Compact nonlinear maps and circu-
lant extensions. arXiv preprint arXiv:1503.03893, 2015. 9
11
Under review as a conference paper at ICLR 2018
A Quadrature rules details
The degree of the rules. We now discuss the choice of the degree for the SR rule. Genz &
Monahan (1999) show that higher degree rules, being more computationally expensive, often bring
in only marginal improvement in performance. For these reasons in our experiments we use the rule
of degree three SR3,3, i.e. a combination of radial rule R3 and spherical rule S3.
B Variance of kernel function approximation using quadrature
RULE
The function fxy in quadrature rule (7) can be considered as a function of two variables, i.e. fxy =
φ(w>x)φ(w>y) = g(z1, z2), where z1 = w>x, z2 = w>y.
In the quadrature rule P 〜χ(d + 2) and Q is a random orthogonal matrix. Therefore, random
variables wi = Qvi are uniformly distributed on a unit-sphere.
Now, let’s write down 4-th order Taylor expansion with Lagrange remainder of function
g(Pwi>x, Pwi>y) + g(-Pwi>x, -Pwi>y) around 0 (odd terms cancel out)
g(Pwi>x, Pwi>y) + g(-Pwi>x,
2
-Pwi>y) ≈ 2g(0, 0) + 2 X
j=0
j!j)!(W>xj (W>y)2-j+
4
+P2X
j=0
jj(W>x)j(w>y)4-j
Where Cj = dzj∂g-j (0, 0), dj = dj∂g-j (e1, e2)+ dzj∂g-j (e3, e4), e： is between 0 and (Pw>x),
i2 is between 0 and (PWi>y), i3 is between 0 and (-PWi>x) and i4 is between 0 and (-PWi>y).
Plugging this expression into (5) we obtain
sRQ,ρ(fχy) ≈ (1 - ρ2) g(O O) +
d d+1 /	2
+ 2WF X=X [2g(0,0) + 2XX j!(⅛(WTx)j(WTy)2-j+
ρ2 X jj (WT x)j(WT y)4 j =
1	d+1
=g(0,0) + 2 X Si,
2	i=1
where	Si	=	Ai	+	Bi,	Ai	=	P22	P2=o	j(⅛(W>x)j (W>y)2-j,
Bi = p4=o j.，j)(W>x)j (W>y)4-j, Wj = Qvj, matrix Q is a random orthogonal ma-
trix uniformly distributed on a set of orthogonal matrices O(n). From uniformity of orthogonal
matrix Q it follows that vector Wj is uniform on a unit n-sphere. Also note that Ai and Bj are
independent if i 6= j, Bi and Bj are independent if i 6= j, however, Ai and Aj are dependent as they
have common random variable P. Therefore, C ov(Si, Sj) = Cov(Ai, Aj) = E(Ai Aj) -(E(Ai))2
ifi 6=j.
12
Under review as a conference paper at ICLR 2018
Let us calculate the variance of the estimate.
j=1
1 d+1	1 d+1
4 XX V (Si) + ɪ XX Cov(Si,Sj) ≤
d+1	d+1	d+1
≤ 4 X V (Si) +4 XE(AiAj) = I X V (Si) +
+ E (4p4) X E (X k!⅛! (w>x)k(w>y)2-k! ×
E (X k⅛⅛(WT x)k (WT y)2-k)[
(9)
Distribution of random variable ρ is χ(d +2), therefore
d(d - 2) ,d> 2
1. Now, let us calculate the variance VSi from the first term of equation (9)
VSi =V(Ai+Bi) = V(Ai) + V(Bi) + Cov(Ai, Bi).
Let kxk ≥ kyk. Then
V(Bi )=V 仕 jwjFj bE Sdj (W> Xu ]
=E ((d0)2(w>y)8 + (d4)2(w>x)8 + (4(d1)2 + 3d0d2)(w>x)2(w>y)6 +
=V 596	+	144	+
(4(d3)2 + 3d4d2)(w>x)6(w>y)2 +
144	+
(1296(d2)2 + 72d0d4 + 1152d1d3)(wτx)4(wτy)4 )
20736	≤ ≤
≤ M2E (WTx)8 + 7M12IIxk2E (WTx)6 + 0.122Μ2kxk4E (WTx)4 ,	(10)
298	72
Z1,Z2) , SUp I —4(Z1,Z2) , SUp
∂Z2∂Z2 (Zjz2
z
z
where M1 = max sup
To calculate expectations of the form E(Wτx)k we will use expression for integral of
monomial over unit sphere Baker (1997)
Jk k k) _ / χkl χk2 Xkd dχ- Cr (k1 - 1)!!(k2 - 1)!! . 一 (kd - 1)!!
J (k1,k2,...,kd) = JSa 1 xι x2 ∙∙∙ Xd dx = σd —d(d + 2) ... (d + |k|-2)—,
(11)
where ki = 2si , si ∈ Z+, |k | =	i ki , σd is a volume of an d-dimensional unit sphere.
For example, let us show how to calculate E(Wτx)4:
E(Wτx)4 = E	WiWjWkWmxixjxkxm
i,j,k,m
thanks to symmetry all terms, for which at least one index doesn’t coincide with
other indices, are equal to 0. = E	Wi4xi4 + 3	Wi2Wj2xi2xj2
i	i6=j
d⅛2) X x4 + d⅛2) X x2x2= d⅛2) kxk4∙
13
Under review as a conference paper at ICLR 2018
Using the same technique for (10) we obtain
V(Bi) ≤ m2 kxk8 ('0-."8.464",'Q
d(d + 2)(d + 4)(d + 6)
2.66M2kx∣∣8
≤	d(d + 2)
10.21	0.37 ʌ
+ d(d + 2)(d + 4) + d(d + 2))
≤
(12)
For the variance of Ai we have
V(Ai) = V
2 X Cj(WTx)j (WTy)2-j
P2 j=0	j!(2-j)!
<E ( 2 X Cj(WT χ)j(wT y)2-j∖
≤ E [ P2 j=0 —jI2-j—)
4 E (X Cj (WTx)j(WTy)2-j ʌ =
d(d- 2)	(j⅛	j!(2-j)!	)一
4	( 3c0 kxk4 + 3c2kyk4 + (4c1 + 2c0c2)(IIXk2kyk2 + 2(XT y)2)
d(d — 2) V	4d(d +2)
≤
we assume that kxk ≥ kyk
≤
24
而一M2kxk4,
(13)
where M2=jma1X2 (∣∂Z1⅛ (0,0J.
Let’s estimate covariance Cov(Ai, Bi):
COv(Ai,Bi) ≤ E(AiBi) + IEAiEBi|.
The first term of the right hand side:
F 2 X	Cj(WTx)j(WTy)2-j	X	dj(WTx)j(WTy)4-j	)
1 P2 j=0 j!(2-j)!	j=0 j!(4-j)!	)	≤
we assume that kxk ≥ kyk ≤
210M1M2kxk6
d2(d+2)(d+4).
The second term EAiEBi (again for kxk ≥ kyk)
IEAiEBi I ≤
2M1M2 kxk6
d3(d +2)
Combining the derived inequalities we obtain
≤	24 M2	4 + 2.66M2kxk8 + 210MιM2kxk6 + 2M1M2kxk6 ≤
一 d2(d2 — 4) 2	d(d + 2)	d2(d + 2)(d + 4)	d3(d + 2)-
2.66M2kxk8	212MιM2∣∣xk6	24M2kxk4
—	d(d +2)	+	d3(d +2) — + d2(d2 — 4).	('
2. Now, let’s examine the expectation of the second term in (9):
e( X &⅛(W>x)k(W>y)21 =
clE(W>y)2 + CIE(W>X)(W>y) + c2E(W>x)2 ≤
≤
we assume that kxk ≥ kyk
≤ M2kxk2
(15)
d
14
Under review as a conference paper at ICLR 2018
(a)	(b)
Figure 4: (a) Butterfly orthogonal matrix factors for d = 16. (b) Sparsity pattern for BPBPBP
(left) and B (right), d = 15.
Substituting (14) and (15) into (9) we obtain
V SR3Q,3,ρ(fxy) ≤ (d+ 1)
(2.66M2 ∣∣xk8	212M1M2 ∣∣x∣∣6	24M2 ∣∣x∣4∖
(d(d + 2)	+	d3 (d + 2)+ d2(d2 - 4) ) +
+ 4d(d1- 2) d(d - 1)
(M2∣xk2 J ≤
≤
2.66M2 ∣∣x∣8	212Mι M2 ∣∣x∣∣6	(d + 95) M2 ∣∣x∣4
d +	d3	+	4d2(d - 2)
And finally
1n
V - X SRLi (fxy)
n
i=1
≤
2.66M2 ∣∣x∣8 + 212Mι M2 ∣∣x∣∣6 + (d + 95)M2 ∣∣x∣4
C Butterfly matrices generation details
C.1	Not a power of two
We discuss here the procedure to generate butterfly matrices of size d × d when d is not a power
of 2.
Let the number of butterfly factors k = dlog de. Then B(d) is constructed as a product of k factor
matrices of size d × d obtained from k matrices used for generating B(2k) . For each matrix in the
product for B(2k), we delete the last 2k - d rows and columns. We then replace with 1 every ci in
the remaining d × d matrix that is in the same column as deleted si .
For the cases when d is not a power of two, the resulting B has deficient columns with zeros (Figure
4b, right), which introduces a bias to the integral estimate. To correct for this bias one may apply
additional randomization by using a product BP, where P ∈ {0, 1}d×d is a permutation matrix.
Even better, use a product of several BP’s: B = (BP)1 (BP)2 . . . (BP)t. We set t = 3 in the
experiments.
C.2 Butterfly randomization
The key to uniformly random orthogonal butterfly matrix B is the sequence of d-1 angles θi. To get
B(d) Haar distributed, we follow Fang & Li (1997) algorithm that first computes a uniform random
point u from Ud . It then calculates the angles by taking the ratios of the appropriate u coordinates
θi = UU^, followed by computing cosines and sines of the θ's.
D Kernel approximation errors on different data sets
Here we discuss the datasets that did not appear in the main body of the paper. Table 2 displays the
settings for the experiments across the datasets. Figure 5 shows the results for the kernel approxi-
15
Under review as a conference paper at ICLR 2018
LETTER
2.00 710T
1.75 -
1.50 -
1.25 -
1.00 -
0.75 -
Powerplant
USPS
× 10一 2
6 8 0 2 4 6
------
5 4 4 3 2 1
=⅛ =
= ⅛l⅛ =
O ①UISoɔlɔjv
5.6 -
4.8 -
4.0 -
3.2 -
2.4-
1. 6
MNIST
× 10 - 2
3.5 -
3.0 -
2.5 -
2.0 -
1. 5 -
1. 0」
= ⅛l⅛ =
I ①UISoɔlɔjv
Figure 5: Kernel approximation error across three kernels (columns: arc-cosine 0, arc-cosine 1,
Gaussian) on three datasets: Powerplant (d = 4), LETTER (d = 16), USPS (d = 256), MNIST
(d = 784). Lower is better. The x-axis represents the factor to which we extend the original
feature space, n = 2(dD+i), where d is the dimensionality of the original feature space, D is the
dimensionality of the new feature space.
mation error on Powerplant, LETTER, USPS and MNIST datasets. For these datasets we include
QMC with Halton sequences into comparison as well.
Table 2: Experimental settings for the datasets. N is the total number of objects, d is dimensionality
of the original feature space.
Dataset	N	d	Number of samples	Number of runs
Powerplant	9568	4	550	500
LETTER	20000	16	550	500
USPS	9298	256	550	500
MNIST	70000	784	50	50
CIFAR100	60000	3072	50	50
LEUKEMIA	72	7129	10	10
QUasi-Monte Carlo integration boasts improved rate of convergence 1/D compared to 1∕√D of Monte
Carlo, however, empirical results illustrate its performance is poorer than that of orthogonal random
featUres (Felix et al. (2016)). It also has larger constant factor hidden Under O notation and higher
complexity. For QMC the weight matrix M is generated as a transformation of qUasi-random se-
qUences. We rUn oUr experiments with Halton seqUences in compliance with the previoUs work (see
FigUre 5 with QMC method inclUded).
AlthoUgh, for the arc-cosine kernels, oUr methods are the best performing estimators, for the GaUs-
sian kernel the error is not always the lowest one and depends on the dataset, e.g. on the USPS
dataset the lowest is Monte Carlo with ROM. However, for the most of the datasets we demonstrate
sUperiority of oUr approach with this kernel.
We also notice that the dataset with a small amoUnt of featUres, Powerplant, enjoys Halton and
Orthogonal Random FeatUres best, while ROM’s convergence stagnates at some point. This coUld
be dUe the small inpUt featUre space with d = 4 and we leave it for the fUtUre investigation.
16
Under review as a conference paper at ICLR 2018
E Comparis on with Gaus sian quadratures
We also included subsampled dense grid method from Dao et al. (2017) into our comparison as it is
the only data-independent approach from the paper that is shown to work well. We reimplemented
code for the paper to the best of our knowledge since it is not open sourced. We run comparison on
all datasets with Gaussian kernel. The Figure 6 illustrates the performance on the LETTER dataset
across different expansions. We can see almost coinciding performance of the method (denoted
GQ) with the baseline RFF (denoted G). For other datasets the figures are very similar to the
case of LETTER, with RFF and GQ methods showing nearly matching relative error of kernel
approximation.
Figure 6: Kernel approximation error for Gaussian kernel on LETTER dataset. Subsampled dense
grid method (denoted GQ) from Dao et al. (2017) show very similar performance to the baseline
random Fourier features (denoted G on the picture).
It should also be noted that explicit mappings produced by Gaussian quadratures do not possess any
convenient structure and, thus, cannot boast any better computational complexity.
F Walltime experiment
We have also run the time measurement on our somewhat unoptimized implementation of the pro-
posed method. Indeed, Figure 7 demonstrates that the method scales as theoretically predicted with
larger dimensions thanks to the structured nature of the mapping.
number of features
Figure 7: Walltime experiment measures the time spent on applying the explicit mapping across
different methods. The x-axis represents the 5 datasets with increasing input number of features:
LETTER (d = 16), USPS (d = 256), MNIST (d = 784), CIFAR100 (d = 3072) and LEUKEMIA
(d = 7129). Thanks to structured explicit mapping of the proposed method B, it is favorable for
higher dimensional data.
17