Under review as a conference paper at ICLR 2018
Revisiting Knowledge Base Embedding as Ten-
sor Decomposition
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of knowledge base (KB) embedding, which is usually ad-
dressed through two frameworks—neural KB embedding and tensor decompo-
sition. In this work, we theoretically analyze the neural embedding framework
and subsequently connect it with tensor based embedding. Specifically, we show
that in neural KB embedding the two commonly adopted optimization solutions—
margin-based and negative sampling losses—are closely related to each other. We
also reach the closed-form tensor that is implicitly approximated by popular neu-
ral KB approaches, revealing the underlying connection between neural and ten-
sor based KB embedding models. Grounded in the theoretical results, we further
present a tensor decomposition based framework KBTD to directly approximate
the derived closed form tensor. Under this framework, the neural KB embedding
models, such as NTN (Socher et al., 2013), TransE (Bordes et al., 2013), Bilin-
ear (Jenatton et al., 2012), and DISTMULT (Yang et al., 2015), are unified into
a general tensor optimization architecture. Finally, we conduct experiments on
the link prediction task in WordNet and Freebase, empirically demonstrating the
effectiveness of the KBTD framework.
1	introduction
Knowledge bases (KBs) power many of semantic-oriented techniques and applications, such as
question answering and intelligent personal assistant. A classical example is the automatic answer
to the query “Who is Barack Obama’s wife” by KB-supported search engines. Most if not all of KBs
achieve this by storing the facts about the world in the form of RDF triplets (W3C, 1999), wherein a
triplet (subject, predicate, object), in short (s, r, o), records a piece of fact about the relation between
the two entities—the subject and object. To automatically construct Web-scale KBs with billions
of facts (triplets), a significant line of effort has been devoted to knowledge base embedding—the
technique of encoding entities and their relational information into latent representations (Bordes
et al., 2011; 2013; Socher et al., 2013; Yang et al., 2015).
In particular, the direction of neural embedding has been extensively explored for learning rep-
resentations for KBs, offering state-of-the-art performance for validating and completing unseen
facts (Yang et al., 2015). Briefly, given a KB represented by triplets T = {(s, r, o)}, neural embed-
ding models take these known triplets as positive instances and corrupted triplets {(s0, r, o0)} as neg-
ative ones. For each triplet, a scoring function f(s, r, o)— parameterized with a neural network—is
designed to project the associated entities s, o and their relational information r into a scalar. Most
of the existing models are then trained through two popular choices of loss functions, including the
margin-based ranking loss by NTN (Socher et al., 2013), TransE (Bordes et al., 2013), and DIST-
MULT (Yang et al., 2015), as well as several practices of the Negative Sampling loss (Mikolov et al.,
2013) by Bilinear (or LFM) (Jenatton et al., 2012) and CONV (Toutanova et al., 2015). In addi-
tion, another line of KB embedding is focused on tensor decomposition based frameworks, such as
RESCAL (Nickel et al., 2011; 2012).
Notwithstanding the rapid development and progress of KB embedding techniques, insights con-
cerning their underlying mechanisms are to date sorely lacking. For example, a natural question that
arises is what are the relationships or differences between the margin-based ranking loss and the
negative sampling loss for neural KB embedding. Moreover, what is the quantity that is optimized
by conventional neural KB embedding models? With an eye toward comprehensively understanding
1
Under review as a conference paper at ICLR 2018
Table 1: Knowledge Base Embedding.
local margin-based loss (ReLU)	max(0, f (S, r, o0) - f(s, r, o) + Y)	
local margin-based loss (Softplus)	log(1 + ef(s0,r,o0)-f(S,r,O)+γ)
local negative sampling loss	log ∩ + ef(s0,r,o0)-f(s,r,o) + ef(s0,r,o0) + e-f(s,r,o)∖
closed-form tensor	log j b⅞⅛)
See detailed notations in Sections 3 and 4. A brief introduction is listed below:
• (s, r, o) & (s0 , r, o0): the known and corrupted triplets, respectively;
• E & R: the entity and relation sets of a given knowledge base, with s, o ∈ E and r ∈ R;
• X ∈ {0, 1}E×R×E: a three-way binary tensor, with Xs,r,o = 1 indicating (s, r, o) ∈ T and otherwise 0;
• dosu,rt = Po Xs,r,o & dion,r = Ps Xs,r,o: the out-/in- degree of entity s/o under relation r, respectively;
• b: the number of negative samples.
KB embedding, we investigate (1) the connection between margin-based ranking loss and negative
sampling loss in neural KB models, (2) the relationship between neural KB models and classical
tensor-based KB models, and (3) the universal framework for KB learning.
Contributions. In this work, we unveil two fundamentals of KB embedding, according to which we
further present a tensor decomposition based KB embedding framework—KBTD, yielding signifi-
cant outperformance over neural KB embedding models in most cases.
First, with softplus’ smooth approximation to ReLU in the margin-based loss (Dugas et al., 2001),
we show that the margin-based loss is closely connected to the negative sampling loss (See rows
2 & 3 in Table 1). In specific, both losses aim to encourage positive triplets (s, r, o) and penal-
ize corrupted ones (s0, r, o0), and the slight difference lies in the extra reward or penalization to
corresponding triplets in the negative sampling loss.
Second, we derive the closed form tensor (See row 4 in Table 1) whose entry is implicitly fitted (ap-
proximated) by scoring function f (s, r, o), when optimizing neural KB embedding models through
the negative sampling loss. This closed form generalizes the ultimate objective of previous attempts
on designing various scoring functions, such as NTN, TransE, Bilinear, and DISTMULT. This find-
ing also links the neural KB embedding framework with the tensor-based KB embedding approach.
Third, building upon the discoveries, we propose a tensor decomposition based KB embedding
framework, KBTD, to directly fit the closed form tensor with by leveraging the scoring functions
proposed in several popular neural models. Our extensive experiments on WordNet and Free-
Base demonstrate the outstanding performance of KBTD over the conventional margin-based neural
framework. In addition, we point out the limitation of dissimilarity/distance based scoring function
design, which is wildly adopted by the TransE/H/R/D models (Bordes et al., 2013; Wang et al.,
2014; Lin et al., 2015; Ji et al., 2015).
The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 unveils the
connection between the margin-based ranking loss and the negative sampling loss. Section 4 per-
forms the theoretical analysis and subsequently presents our KBTD framework. Section 5 introduces
the detailed experiments on the link prediction task for KBs. Section 6 concludes this paper.
2	Related Work
Knowledge base embedding is being extensively explored and developed over the last few years,
during which the major breakthroughs are resulted from the neural embedding and tensor factor-
ization models (Bordes et al., 2013; Nickel et al., 2012). Our work focuses on understanding the
fundamentals of neural KB embedding, as well as its connection with tensor models.
Neural KB Embedding: Loss Functions. Neural knowledge base embedding is usually formulated
as an optimization problem with different loss functions. The majority of existing KB models em-
ploy the margin-based ranking loss, which was first proposed (Collobert et al., 2011) for addressing
the efficiency issue of softmax (Bengio et al., 2003) in the field of natural language models. A brief
collection of recent margin-based KB embedding methods include the SE, Unstructured, and SME
models (Bordes et al., 2011; 2012; 2014), SLM and NTN (Socher et al., 2013), DISTMULT (Yang
2
Under review as a conference paper at ICLR 2018
et al., 2015), TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransR (Lin et al., 2015),
TransD (Ji et al., 2015), and ProjE (Shi & Weninger, 2017). In addition, there are several models—
Bilinear model (Jenatton et al., 2012) and CONV (Toutanova et al., 2015)—that adopt the nega-
tive sampling (NE) loss. Our work contributes to this line of research by providing the relationship
between margin-based and negative sampling based KB embedding models.
Neural KB Embedding: Scoring Functions. As summarized in (Yang et al., 2015), given a KB
represented by a list of triplets, neural models learn embeddings by utilizing a neural network,
wherein the first layer projects the two entities of each triplet into latent low-dimensional vectors, and
the second layer leverages a scoring function to operate on each pair of entity vectors with relation-
specific parameters. The major difference between neural KB models lie in the various ways that
they design the scoring functions — TransE, NTN, Bilinear, and DISTMULT (See details in Table
3 of Section 5). To date, few attempts have been conducted to understand any common grounds
behind these models. Our work furthers this direction by proposing a general tensor decomposition
framework (KBTD) which unifies existing neural KB models.
Tensor Decomposition for KB Embedding. Tensor decomposition has seen successes in structural
and relational learning over decades (Kolda & Bader, 2009; Sun et al., 2006). Recent years also
witness the natural application of this technique on learning KB embeddings, including the BCTF
model (Sutskever et al., 2009), and RESCAL (Nickel et al., 2011). In this work, we show the closed
relationship between tensor decomposition based KB embedding and neural KB embedding models.
3	Connecting Margin-based Loss with Negative S ampling Loss
Given a KB with entity set E and relation set R, represented by a set of triplets T = {(s, r, o)}
with s, o ∈ E and r ∈ R, the goal of neural KB models is to learn a scoring function f (s, r, o)
which evaluates an arbitrary triplet and outputs a scalar to measure the acceptability of this triplet,
where high/low score indicates that the input triplet tends to be correct/wrong. As summarized by
(Yang et al., 2015), most existing scoring functions can be unified by a neural network, where the
first layer projects the two entities of each triplet into latent low-dimensional vectors, and the second
layer applies either linear or bilinear transformation (or both) on entity vectors with relation-specific
parameters.
The scoring function is then fitted to a loss function to learn the representations of both entities
and relations. The majority of neural KB embedding models adopt either a margin-based ranking
loss or a negative sampling loss. Both loss functions leverage the known triplets T as positive
samples and the corrupted triplets T0 as the negative ones. Following the literature, given a known
triplet (s, r, o) ∈ T , its corrupted triplets T(0s,r,o) are constructed by replacing either the subject
entity s or the object entity t with an arbitrary entity from E, i.e., T(0s,r,o) = {(s0, r, o)|s0 ∈ E} ∪
{(s, r, o0)|o0 ∈ E}. Given both positive and corrupted triplets, the objective of the margin-based
ranking loss is to minimize:
LMARGIN = Σ	Σ max 0, γ + f (s0 , r, o0) - f (s, r, o) .
(s,r,o)∈T (s0,r,o0)∈T(0s,r,o)
Its challenge lies in, however, the second summation, which takes O(|E|) complexity to enumerate
the whole entity set E and is extremely time demanding. Therefore, in practice, the second sum-
mation is commonly approximated with sampling and the sample size is usually set to one (Bordes
et al., 2013). Considering the local loss function 'margin(s, r, o) for each positive triplet (s, r, o)
associated with one (sampled) corrupted triplet (s0, r, o0), the max(0, ∙) loss can be smoothly ap-
proximated by the softplus function (Dugas et al., 2001), that is:
'margin(s,r,o) = max (0,γ + f (s0,r,o0) - f (s,r,o))
≈ log 1 + eγ+f(s0,r,o0)-f(s,r,o)	.	(1)
On the other hand, the negative sampling loss aims to optimize the following objective:
LNEG = - X	(logσ(f (s,r,o)) + bE(s，")〜丁(…)[logσ(-f (s0,r,o0))]),	⑵
(s,r,o)∈T	, ,
where b is the number of negative samples and σ(∙) is the sigmoid function. Similarly, the ex-
pectation term can be replaced with its Monte Carlo approximation. To align with our previous
3
Under review as a conference paper at ICLR 2018
discussion on the margin-based ranking loss, we also set negative sample size b = 1 and derive the
local objective for a certain positive triplet (s, r, o) to be:
'neg(s,r, o) = - log σ(f(s,r, o)) - log σ(-f(s0,r, o0))
= log 1 + ef (s0,r,o0)-f (s,r,o) + ef (s0,r,o0) + e-f (s,r,o)
(3)
Eq. 1 and Eq. 3 reveal the close relationship between margin-based ranking loss and negative sam-
pling loss in neural KB embedding. First, observed from the similar term ef (s0,r,o0)-f (s,r,o) , both
loss functions implicitly encourage positive triplets to have relatively higher scores than the cor-
rupted ones. Second, the extra term ef(s0,r,o0) + e-f (s,r,o) in the negative sampling loss suggests
that in addition to the implicit comparison, it explicitly rewards the positive triplets to have high
scores, and also encourages the corrupted ones to have low scores.
4	Unifying Neural KB Embedding as Tensor Decomposition
From the above section, we observe that the margin-based loss and negative sampling loss share
very similar form. In this section, we unify existing neural KB embedding models by assuming a
general scoring function f : E × R × E → R and the use of negative sampling loss. We present
a theoretical analysis in Section 4.1, followed by our KBTD framework which formally defines KB
embedding problem as a tensor decomposition problem in Section 4.2. Additionally, the connection
between KBTD and classical tensor decomposition models (Nickel et al., 2011; 2012) is discussed
in Section 4.2.
4.1	Theoretical Analysis of Neural KB Embedding
To facilitate our analysis, we represent the KB triplets as a three-way binary tensor X ∈
{0, 1}E×R×E, where Xs,r,o = 1 indicates (s, r, o) ∈ T, while Xs,r,o = 0 for non-existing or
unknown triplets. The loss function LNEG in Eq. 2 can be re-formatted with Xs,r,o as
-X Xs,r,o {log σ (f(s, r, o)) + 2Eo0 2 PN [log σ (-f(s,r,o0))] + bEs0 2Pn [log σ (-f (S0,r, O))] },
s,r,o
where PN is a uniform distribution over all entities, i.e., PN(∙)=看. We further break down the
summation and arrive at the following form:
LNEG = -	Xs,r,o logσ (f (s, r, o))
s,r,o
b
2
[log σ (-f(s,r,o0))] + ɪ^dθn,rEs,一Pn
r,o
[log σ (-f (s,r,
(4)
where dosu,rt = Po Xs,r,o is the out-degree of entity s under relation r, and dion,r = Ps Xs,r,o is the
in-degree of entity o under relation r. Then we explicitly express the two expectation terms:
Eo02Pn [log σ (-f(s,r,o0))] = X 卷 log σ (-f(s,r,o0))
o0 |E|
=|E| log σ (-f (S,r, O))+ X |E| log σ (-f (s,r, o0))
Es02PN [log σ (-f(s0,r, O))] = |E| log σ (-f(s,r, O)) + X |E| log σ (-f(s0,r, O)).
Then, by utilizing the above expectation terms, the local loss function for each specific triplet (s, r, o)
in Eq. 4 can be defined as
'(s,r, O) = -Xs,r,ο log σ (f (s,r,O))-
b ∙ (dsur+donr)	/	/	、\
2|E|	log σ (-f….
4
Under review as a conference paper at ICLR 2018
Table 2: The comparison between RESCAL and our KBTD framework with parameters Θ initial-
ized in the bilinear way, that is, Θ = {aι, •…，a∣E∣, Wι, ∙∙∙ , W∣r∣}.
RESCALl ming Pr,o (Xs“ - a§> Wra,2
KBTD	min& Pr,0 ⑷…％。- a§> Wr aj
The work in (Levy & Goldberg, 2014) suggested that for sufficient large embedding dimensionality,
each individual f(s, r, o) can assume a value independence1. Following this assumption enables us
to treat the objective L as a function of independent f (s, r, o) terms. The partial derivative with
respect to f(s, r, o) can be taken as:
∂L
∂f (s,r,o)
∂'(s,r,o)	Z b(	b ∙ (dSur + d0,r) /ʃ/
∂τκr^ = -Xs“°(-f (s，r，0))+-iei- σ (JBr ⑼).
By setting the derivative to zero, we have
e2f (s,r,o) - 2	2 |E| Xs,r,0	- ι! ef(s,r,o) -	2 |E| Xs,r,0	= 0
∖b ∙ (dsur + dθ,r)	)	b ∙ (dsur + dθ,r)一,
which implies
2 WEF).	⑸
The RHS of Eq. 5 defines a “transformed” tensor based on X, and the LHS of Eq. 5 implies a re-
gression problem, i.e., try to fit the (s, r, o)-entry of the transformed tensor with the scoring function
f(s, r, o). In next section, we formally define this problem and then propose our KBTD framework.
4.2 KBTD: NEURAL KB EMBEDDING AS TENSOR DECOMPOSITION
In this section, we formalize the KB embedding problem analyzed in Section 4.1 as a tensor decom-
position problem. We further present our framework—KBTD—to learn latent embedding for KB
entities and relations. Its connection with classical tensor decomposition methods is also discussed.
First, as mentioned at the end of Section 4.1, RHS of Eq. 5 defines a transformed tensor based on
X. Here We denote it to be tensor Y ∈ RlEl×lRl×lEl, with the (s, r, o)-entry defined to be
笛-1	2	2 ∣E∣Xs,r,o ∖	八
Ys"ig (b .(曙r + 吧r)).	⑹
Second, our discussion in Section 4.1 actually implies a weighted tensor decomposition problem.
This is mainly due to the negative sampling mechanism — we only care about positive triplets and
corrupted triplets. This mechanism can be characterized by a binary tensor W ∈ {0,1}lEl×lRl×lEl,
wherein Ws,r,o = 1 if and only if (s, r, o) is either a positive triplet or a corrupted triplet. Given the
definition of tensor Y and tensor W, we can formalize the following tensor decomposition problem:
min X Ws,r,o (Ys,r,o - fΘ (s, r, o))2 ,	(7)
Θ
s,r,o
where fΘ is the scoring function parameterized by Θ.
Revisiting RESCAL (Nickel et al., 2011; 2012). Before introducing how we optimize Eq. 7,
we would like to discuss the connection between KBTD and RESCAL—a classical tensor decom-
position model for KB embedding. Table 2 lists the optimization problems solved by RESCAL
and our framework, in which the scoring function in Eq. 7 is initialized as a bilinear function, i.e.,
f(s, r, o) = as>Wrao, where as, ao ∈ Rd and Wr ∈ Rd×d. We observe the following connections
and differences between them. First, both models explain a RDF triplet (s, r, o) through the latent
1We realize that there has been discussion that this supposition may not be rigorous enough (Arora et al.,
2016).
5
Under review as a conference paper at ICLR 2018
1
2
3
4
5
6
7
8
9
10
Algorithm 1: The KBTD Framework
input: Training set T = {(s, r, o)}, entity and relation set E and R, corrupted triplets multiplier λ,
mini-batch size B
output: Models parameters Θ, including entity and relation embeddings
Initialize model parameters Θ;
while do
/* Sample a mini-batch of size B	*/
Tbatch — SamPle(T, B);
/* Sample corrupted triplets for this mini-batch	*/
Tbatch J °；
for (s, r, o) ∈ Tbatch do
for i = 1 to λ do
s0 J samPle(E);
o0 J samPle(E);
Tb0atch J Tb0atch ∪ (s0, r, o) ∪ (s, r, o0) ;
UPdate Parameter Θ w.r.t. P(s,r,o)∈Tbatch∪Tb0atch (Ys,r,o - fΘ(s, r, o))2; * 5
rePresentations as , ao and Wr. To learn the rePresentations, however, RESCAL directly factor-
izes the binary tensor X, while our model decomPoses a transformed real-value tensor Y. Second,
KBTD also differs with RESCAL in the way they treat the unobserved triPlets. Notice that given a
KB of observed (Positive) triPlets, the unobserved triPles includes both Positive and negative ones.
This issue is known as the one-class Problem (Moya & Hush, 1996; Pan et al., 2008). Two common
solutions to this Problem are AMAN (all missing as negative) and AMAU (all missing as unknown).
The RESCAL model simPly adoPts the AMAN strategy by assuming all unobserved triPlets as neg-
ative ones. However, our model is able to imPlicitly comPromise between AMAN and AMAU by
only treating corruPted triPlets as negative.
KBTD Learning. The detailed oPtimization Procedure for KBTD is described in Algorithm 1.
We oPtimize the objective function in Eq. 7 using mini-batch stochastic gradient descent with Ada-
Grad (Duchi et al., 2011). At each main iteration (Line 3-10), we first samPle a mini-batch of
Positive triPlets (Line 4) and then samPle their corruPted triPlets whose size is controlled by a multi-
Plier λ (Line 6-10). The Parameters are uPdated with resPect to the samPled Positive triPlets as well
as corresPonding corruPted ones. In this setting, we avoid generating the dense tensors Y and W,
which may in Practice result in memory issues.
There is a comPutational issue that comes from the log oPerator. For an unobserved triPlet
(s, r, o) (i.e., Xs,r,o = 0), Ys,r,o = log0 = -∞. Previously, two aPProaches have been ProPosed
for addressing it (Levy & Goldberg, 2014). One is to smooth the logarithm by adding a small con-
stant to tensor X, generating a dense tensor. The other one is to aPPly an additional shifted-truncated
oPerator, that is, max(Ys,r,o - c, 0), generating a sParse tensor with the loss of certain information.
Due to the obvious drawbacks, we instead ProPose to use a simPle and effective solution, wherein
the oPeration logx is rePlaced with log( + x) with as a tunable Parameter.
5 Experiments
In this section, we evaluate the ProPosed KBTD framework on the canonical link Prediction task
against several PoPular KB embedding methods on two datasets extracted from WordNet and Free-
Base. In this task, we are given a KB with a certain fraction of triPlets removed, and our target is to
Predict these missing triPlets. We first introduce our exPerimental setuP in Section 5.1, followed by
detailed discussion on exPerimental results in Section 5.2.
5.1 Experimental Setup
Datasets. We use WordNet (WN18) and FreeBase (FB15k) datasets as introduced in (Bordes
et al., 2013) where WN18 consists of 151, 442 triPlets with 40,943 entities and 18 relations, and
FB15k contains 592,213 triPlets with 14,951 entities and 1,345 relations. We use the same train-
ing/validation/test sPlit as in (Bordes et al., 2013; Yang et al., 2015).
6
Under review as a conference paper at ICLR 2018
Table 3: Scoring Functions and Parameters.
Model Name	Parameters Θ	Scoring Function fθ (s, r, o)		
NTN	{uι,…，|R|, Wι[1r∣R∣,匕…，|R|，aι,…，∣E∣}	u> tanh f as Wr[1:fc]ao + Vr	as ao	+ br
TranSE	{w1,…，|R|, a1,…，∣E∣}		- ∣∣as + Wr - ao∣∣2		
Bilinear	{W1,…，|R|, a1,…，∣E∣}		aS Wrao			
DISTMULT	{w1,…，|R|, a1,…，∣E∣}		aS diag(wr)ao		
Table 4: Experimental Results on theWN18 Dataset.
	Results from Neural KB Embedding		Results from our KBTD framework	
	MRR	HrrS@10	MRR	HrrS@10
NTN	0.53	66!0	085	9050
TransE-	0.38	9090	039	82T8
BiIinear-	0.89	92:80	092	9462
DISTMULT	0.83	94.20	0.81 —	94.62
Table 5: Experimental Results on the FB15k Dataset.
	Results from Neural KB Embedding		Results from our KBTD framework	
	MRR	HITS@10	MRR	HITS@10
NTN	0.25	4140	037	5906
TransE-	0.31	5390	0.30	49:94
Bilinear-	0.31	5190	031	5495
DISTMULT-	0.35	57.70	0.35 —	59.91
Baselines. We compare our proposed framework with TransE (Bordes et al., 2013), NTN (Socher
et al., 2013), Bilinear (Jenatton et al., 2012) and DISTMULT (Yang et al., 2015). The origi-
nal TransE model is based on dissimilarity/distance function. To fit our framework, we define
the scoring function for TransE to be negative dissimilarity/distance function, i.e., f (s, r, o) =
- kas + ar - aok22. For NTN, Bilinear and DISTMULT, we inherit the scoring functions from
their paper. The detailed scoring functions as well as their parameters are listed in Table 3. For the
meaning of parameters and the intuition behind scoring functions, readers can refer to the original
papers.
Evaluation Protocol. We exactly follow the experimental procedure and treatment used in
TransE (Bordes et al., 2013) and DISTMULT (Yang et al., 2015). For each triplet (s, r, o) in the
test set, the subject entity s is replaced with each of entities from entity set E in turn. We apply
corresponding scoring function f on those corrupted triplets and then sort them in non-increasing
order to get the rank of the correct triplet. This procedure is then repeated for the object entity o. For
evaluation metrics, we consider Mean Reciprocal Rank (MRR) which is defined to be an average of
the reciprocal rank of the correct triplets over all test triplets, and HITS@10 (top-10 accuracy). If
possible, we list the experimental results reported in (Yang et al., 2015) directly. In addition, we ap-
ply the filtered setting from (Bordes et al., 2013; Yang et al., 2015) in evaluation. In this setting, for
one certain test triplet (s, r, o), we removed from the list of corrupted triplets all the triplets which
appear in training, validation, or test set, except (s, r, o) itself. This setting, for example, can avoid
cases where lots of triplets in training set rank above the one of interest.
Implementation Details. All the models in our framework were implemented using PyTorch in
a machine with one 12GB GPU. Since the complexities of the aforementioned approaches vary a
lot, in order to achieve the best accuracy for all the models, we cross-validate using the validation
set to find the best hyperparameters. We found that, except for TransE on WN18, all the methods
on both datasets can share the same hyper-parameters: (1) dimensionality d = 100; (2) smoothing
parameter = 0.01; (3) multiplier λ mentioned in Algorithm 1 was set to 2; (4) the learning rate of
AdaGrad algorithm was set to 0.1 (0.01 for TranSE on WN18); (5) '2-regularization applied to all
the parameters using the weight 0.0001; (6) the mini-batch size is set to 2,048 (4,800 for TransE on
WN18); (6) b = 1 in Eq. 6 (200 for TranSE on WN18). For the additional hyper-parameter in NTN
method, i.e., the number of slices k, was set to 2. We allow all the algorithms to run at most 10,000
epochs over the training data, and the best model was selected by early stopping using HITS@10
7
Under review as a conference paper at ICLR 2018
Table 6: Model Complexity in terms of #Parameters. d is the embedding dimension, and k in NTN
is the number of slices. _________________________________________
Methods	# Parameters
NTN	O(∣R∣ d2k + |E| d)
TransE	-O(|R| d + |E| d)-
Bilinear	O(∣R∣ d2 + |E| d)
DISTMULT	O(∣R∣ d + |E| d)
score on the validation sets. By taking advantages of GPU computation, every training experiment
can be finished within 4 hours.
5.2 Experimental Results
Table 4 and Table 5 list the overall results on the WN18 and FB15k datasets for several popular
models under both our KBTD framework and the neural KB embedding framework, respectively. In
general, we have the following key observations and insights:
(1)	On the WN18 dataset, the KBTD framework achieves the best performance among most cases.
In terms of MRR, KBTD outperforms all baselines except DISTMULT with an impressive improve-
ment up to 60.4% (0.85 v.s. 0.53) on NTN. In terms of HITS@10, KBTD outperforms baselines
except TransE with an improvement up to 36.9% (90.50 v.s. 66.10) on NTN. Similar results can
also be observed on the FB15k dataset in Table 5. In terms of both MRR and HITS@10, KBTD
outperforms all baselines except TransE with an improvement greater than 42.7% on NTN.
(2)	It is notable that KBTD outperforms NTN by large margins on both datasets. We conjecture
that this comes from the very high model complexity of NTN, as suggested by Table 6. In our
KBTD framework, we reduce the previously considered margin-based ranking problem in the origi-
nal NTN to a simple regression problem. As a result, KBTD enables the efficient training procedure
to significantly boost up NTN with respect to both the MRR and HITS@10 metrics.
(3)	Itis also worth noting that under the KBTD framework, most models generate comparable or bet-
ter results than their neural KB embedding versions. In terms of HITS@10, KBTD underperforms
TransE by 9.6% (82.18 v.s. 90.90) on WN18 and 7.3% (49.94 v.s. 53.90) on FB15k. We attribute
this underperformance to the constraint on TransE’s scoring function. As showed in Table 3, when
instantiating the KBTD framework with TransE, the scoring function is set to be the negative dis-
similarity function—f (s, r, o) = - kas + wr - aok22, which is for sure non-positive. However, the
tensor Y in Eq. 7 that KBTD aims to fit allows both positive and negative entries. In specific, for
the observed triplets and a moderate b, tensor entries are usually positive; for the corrupted triplets
and a small , tensor entries reach negative values. On the contrary, the scoring functions of NTN,
Bilinear, and DISTMULT are able to model both positive and negative tensor entries. That said,
the non-positive constraint of TransE’s scoring function limits its ability to learn better latent KB
representations in this link prediction task.
6 conclusion
In this work, we provide a theoretical analysis of conventional neural KB embedding models and
unveil the link between them and tensor-based KB embedding models. We show that the existing
neural KB models can be unified into one tensor decomposition framework. We further propose the
KBTD framework to directly fit the derived closed-form tensor. Our extensive experiments suggest
that KBTD achieves consistent performance improvements over NTN, Bilinear, and DISTMULT
under the neural KB embedding framework.
For further work, one interesting direction is to exploit efficient and scalable algorithms that extend
KBTD to web-scale KBs. Another direction is to leverage the effective techniques from the matrix
factorization community to enhance our tensor framework, such as the usage of bias terms and rich
contextual information.
8
Under review as a conference paper at ICLR 2018
References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Rand-walk: A latent
variable model approach to word embeddings. Transactions of the Association for Computational
Linguistics (TACL), 4, 2016.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal ofmaChine learning research, 3(Feb):1137-1155, 2003.
Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. Learning structured embed-
dings of knowledge bases. In AAAI ’11, volume 6, pp. 6, 2011.
Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. Joint learning of words and
meaning representations for open-text semantic parsing. In AISTATS ’12, pp. 127-135, 2012.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NIPS ’13, pp. 2787-2795, 2013.
Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching energy
function for learning with multi-relational data. Machine Learning, 94(2):233-259, 2014.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray KavukCuoglu, and Pavel
Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-
search, 12(Aug):2493-2537, 2011.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Charles Dugas, Yoshua Bengio, Francois Belisle, Claude Nadeau, and Rene Garcia. Incorporating
second-order functional knowledge for better option pricing. In NIPS ’01, pp. 472-478, 2001.
Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, and Guillaume R Obozinski. A latent factor
model for highly multi-relational data. In NIPS ’12, pp. 3167-3175, 2012.
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding via
dynamic mapping matrix. In ACL ’15, pp. 687-696, 2015.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):
455-500, 2009.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS
’14, pp. 2177-2185, 2014.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In AAAI ’15, pp. 2181-2187, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In NIPS ’13, pp. 3111-3119, 2013.
Mary M. Moya and Don R. Hush. Network constraints and multi-objective optimization for one-
class classification. Neural Networks, 9(3):463-474, April 1996.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning
on multi-relational data. In ICML ’11, pp. 809-816, 2011.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine
learning for linked data. In WWW ’12, pp. 271-280, 2012.
Rong Pan, Yunhong Zhou, Bin Cao, Nathan N Liu, Rajan Lukose, Martin Scholz, and Qiang Yang.
One-class collaborative filtering. In ICDM ’08, pp. 502-511, 2008.
Baoxu Shi and Tim Weninger. Proje: Embedding projection for knowledge graph completion. In
AAAI ’17, pp. 1236-1242, 2017.
Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural
tensor networks for knowledge base completion. In NIPS ’13, pp. 926-934, 2013.
9
Under review as a conference paper at ICLR 2018
Jimeng Sun, Dacheng Tao, and Christos Faloutsos. Beyond streams and graphs: dynamic tensor
analysis. In KDD 06,pp. 374-383, 2006.
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan R Salakhutdinov. Modelling relational data using
bayesian clustered tensor factorization. In NIPS ’09, pp. 1821-1828, 2009.
Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael
Gamon. Representing text for joint embedding of text and knowledge bases. In EMNLP ’15,
volume 15, pp. 1499-1509, 2015.
W3C. Resource description framework (RDF) model and syntax specification. https://www.
w3.org/TR/PR-rdf-syntax/, 1999. [Online; accessed Oct. 25, 2017].
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by trans-
lating on hyperplanes. In AAAI ’14, pp. 1112-1119, 2014.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knowledge bases. In ICLR’ 15, 2015.
10