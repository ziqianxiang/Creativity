Under review as a conference paper at ICLR 2018
Evolutionary Expectation Maximization for
Generative Models with B inary Latents
Anonymous authors
Paper under double-blind review
Ab stract
We establish a theoretical link between evolutionary algorithms and variational pa-
rameter optimization of probabilistic generative models with binary hidden vari-
ables. While the novel approach is independent of the actual generative model,
here we use two such models to investigate its applicability and scalability: a
noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse
Coding (as a model for continuous data). Learning of probabilistic generative
models is first formulated as approximate maximum likelihood optimization us-
ing variational expectation maximization (EM). We choose truncated posteriors as
variational distributions in which discrete latent states serve as variational param-
eters. In the variational E-step, the latent states are then optimized according to a
tractable free-energy objective. Given a data point, we can show that evolutionary
algorithms can be used for the variational optimization loop by (A) considering the
bit-vectors of the latent states as genomes of individuals, and by (B) defining the
fitness of the individuals as the (log) joint probabilities given by the used genera-
tive model. As a proof of concept, we apply the novel evolutionary EM approach
to the optimization of the parameters of noisy-OR Bayes nets and binary sparse
coding on artificial and real data (natural image patches). Using point mutations
and single-point cross-over for the evolutionary algorithm, we find that scalable
variational EM algorithms are obtained which efficiently improve the data likeli-
hood. In general we believe that, with the link established here, standard as well as
recent results in the field of evolutionary optimization can be leveraged to address
the difficult problem of parameter optimization in generative models.
1	Introduction
Evolutionary algorithms (EA) have been introduced (e.g. Fogel et al., 1966; Rechenberg, 1965) as
a technique for function optimization using methods inspired by biological evolutionary processes
such as mutation, recombination, and selection. As such EAs are of interest as tools to solve Machine
Learning problems, and they have been frequently applied to a number of tasks such as clustering
(Pernkopf & Bouchaffra, 2005; Hruschka et al., 2009), reinforcement learning (Salimans et al.,
2017), and hierarchical unsupervised (Myers et al., 1999) or deep supervised learning (e.g., Stanley
& Miikkulainen 2002 and Suganuma et al. 2017; Real et al. 2017 for recent examples). In some of
these tasks EAs have been investigated as alternatives to standard procedures (Hruschka et al., 2009),
but most frequently EAs are used to solve specific sub-problems. For example, for classification with
Deep Neural Networks (DNNs LeCun et al., 2015; Schmidhuber, 2015), EAs are frequently applied
to solve the sub-problem of selecting the best DNN architectures for a given task (e.g. Stanley &
Miikkulainen, 2002; Suganuma et al., 2017) or more generally to find the best hyper-parameters of
a DNN (e.g. Loshchilov & Hutter, 2016; Real et al., 2017).
Inspired by these previous contributions, we here ask if EAs and learning algorithms can be linked
more tightly. To address this question we make use of the theoretical framework of probabilistic
generative models and expectation maximization (EM Dempster et al., 1977) approaches for param-
eter optimization. The probabilistic approach in combination with EM is appealing as it establishes
a very general unifying framework able to encompass diverse algorithms from clustering and dimen-
sionality reduction (Roweis, 1998; Tipping & Bishop, 1999) over feature learning and sparse coding
(Olshausen & Field, 1997) to deep learning approaches (Patel et al., 2016). However, for most gen-
erative data models, EM is computationally intractable and requires approximations. Variational
1
Under review as a conference paper at ICLR 2018
EM is a very prominent such approximation and is continuously further developed to become more
efficient, more accurate and more autonomously applicable. Variational EM seeks to approximately
solve optimization problems of functions with potentially many local optima in potentially very high
dimensional spaces. The key observation exploited in this study is that a variational EM algorithm
can be formulated such that latent states serve as variational parameters. If the latent states are then
considered as genomes of individuals, EAs emerge as a very natural choice for optimization in the
variational loop of EM.
2	Truncated Variational EM
A probabilistic generative model stochastically generates data points y~ using a set of hidden (or
latent) variables ~s. The generative process can be formally expressed in the form of joint probability
p(~s, y~ | Θ), where Θ are the model parameters. Given a set of N data points, y~(1) , . . . , ~y(N) =
~y(1:N), learning seeks to change the parameters Θ so that the data generated by the generative model
becomes as similar as possible to the N real data points. One of the most popular approaches to
achieve this goal is to seek maximum likelihood (ML) parameters Θ*, i.e., parameters that maximize
the data log-likelihood for a given generative model:
L(Θ) := log(L(Θ)) = X log (XP (~n, S | Θ))	(1)
n	{~s}
To efficiently find (approximate) ML parameters we follow Saul & Jordan (1996); Neal & Hinton
(1998); Jordan et al. (1999) who reformulated the problem in terms of a maximization of a lower
bound of the log-likelihood, the free energy F (q~, Θ). Free energies are given by
NN
F (q(1:N), Θ) = X(X q(n)(~) log (p(~,~ ⑺ | Θ)))+ X H (q(n)(s)),	⑵
n=1 {~s}	n=1
where q(n) (~s) are variational distributions, and where H(q) denotes the entropy of a distribution q.
For the purposes of this study, we consider elementary generative models which are difficult to train
because of exponentially large state spaces. These models serve well for illustrating the approach
but we stress that any generative model which gives rise to a joint distribution P(~s, y~| Θ) can be
trained with the approach discussed here as long as the latents ~s are binary.
In order to find approximate maximum likelihood solutions, distributions q(n) (~s) are sought that
approximate the intractable posterior distributions P(~s | ~y(n), Θ) as well as possible, which results
in the free-energy being as similar (or tight) as possible to the exact log-likelihood. At the same
time variational distributions have to result in tractable parameter updates. Standard approaches
include Gaussian variational distributions (e.g. Opper & Winther, 2005) or mean-field variational
distributions (Jordan et al., 1999). If we denote the parameters of the variational distributions by Λ,
then a variational EM algorithm consists of iteratively maximizing F(Λ, Θ) w.r.t. Λ in the variational
E-step and w.r.t. Θ in the M-step. The M-step can hereby maintain the same functional form as for
exact EM but the expectation values now have to be computed w.r.t. the variational distributions.
Instead of using parametric functions such as Gaussians or factored (mean-field) distributions, for
our purposes we choose truncated variational distributions defined as a function of a finite set of
states (Lucke & Eggert, 2010; Sheikh et al., 2014; Shelton et al., 2017). These states will later serve
as populations of evolutionary algorithms. If we denote Kn a population of hidden states for a given
data point y~(n), then variational distributions and their corresponding expectation values are given
by (e.g. Lucke & Eggert, 2010; Sheikh et al., 2014):
n Θ	P P(~s, ~yn | Θ)g(~s)
qn(~ ∣κn, Θ) ：= P(S )y ∣,	δ(~∈κn),	hg(~)iqn = ~∈P	「	∙	⑶
P (~s0 | y~n, Θ)	q	P(~s0, ~yn | Θ)
~s0 ∈Kn	~s0 ∈Kn
where δ(~s ∈ Kn ) is 1 if Kn contains the hidden state ~s, zero otherwise. If the set Kn contains all
states with significant posterior mass, then (3) approximates expectations w.r.t. full posteriors very
well. By inserting truncated distributions as variational distribution of the free-energy (2), it can be
2
Under review as a conference paper at ICLR 2018
shown (Lucke, 2016) that the free-energy takes a very compact simplified form given by:
F(K,Θ) =Xlog(X p(~yn,~s | Θ)), where K= (K1,...,KN).	(4)
n	~s∈Kn
As the variational parameters of the variational distribution (3) are now given by populations of
hidden states, a variational E-step now consists of finding for each data point n the population Kn
that maximizes P~s∈Kn p (~yn, ~s | Θ).
3	Evolutionary Optimization
For the generative models considered here, each latent state ~s takes the form of a bit vector. Hence,
each population Kn is a collection of bit vectors. Because of the specific form (4), the free-energy is
(n)
increased in the variational E-step if and only ifwe replace and individual ~s in population K by a
new individual ~snew so far not in K (n) such that:
p(snew,~n | Θ) >p(~,yn | Θ).	(5)
More generally, this means that the free energy is maximized in the variational E-step if we find
for each n those S individuals with the largest joints p(~s, ~yn | Θ), where p(~s, ~yn | Θ) is given by the
respective generative model (compare Lucke, 2016; Forster & Lucke, 2017, for formal derivations).
Full maximization of the free-energy is often a computationally much harder problem than increas-
ing the free-energy; and in practice an increase is usually sufficient to finally approximately max-
imize the likelihood. As we increase the free-energy by applying (5) we can choose any fitness
function F (~s; y~n, Θ) for an evolutionary optimization which fulfils the property:
F(snew; yn, Θ) > F(~; yn, Θ)	⇔	P(Snew,yn | Θ) > p(~,yn | Θ) .	(6)
Any mutations selected such that the fitness F (~s; ~yn, Θ) increases will result in provably increased
free-energies. Together with M-step optimizations of model parameters, the resulting variational
EM algorithm will monotonously increase the free-energy. The freedom in choosing a fitness func-
tion satisfying (6) leaves us free to pick a form that enables an efficient parent selection proce-
dure. More concretely (while acknowledging that other choices are possible) we define the fitness
F(~snew;~yn, Θ) to be:
F(~) = F(~； yn,⑼=]P(~; yn,⑼-2min(]P(~; yn,⑼)	⑺
s
.	Γ^''≤ ♦… I	...	...	...	……	I
where logP is defined as the logarithm of the joint probability where summands that do not depend
on the state ~s have been elided. logP is usually more efficiently computable than the joint probabili-
ties and has better numerical stability, while being a monotonously increasing function of the joints
when the data-point y~n is considered fixed. As we will want to sample states proportionally to their
fitness, an offset is applied to logP to make sure F always takes positive values. As previously men-
tioned, other choices of F are possible as long as (6) holds. From now on we will drop the argument
~yn or index n (while keeping in mind that an optimization is performed for each data point ~yn).
Our applied EAs then seek to optimize F (~s) fora population of individual K (we also drop the index
n here). More concretely, given the current population K of unique individuals ~s, the EA iteratively
seeks a new set K0 with higher overall fitness. For our models, ~s are bit-vectors of length H, and
we usually require that populations K0 and K to have the same size as is customary for truncated
approximations (e.g. Lucke & Eggert, 2010; Shelton et al., 2017). Our example algorithm includes
three common genetic operators, discussed in more detail below: parent selection, generation of
children by single-point crossover and stochastic mutation of the children. We repeat this process
over Ng generations in which subsequent iterations use the output of previous iterations as input
population.
Parent Selection. This step selects Np parents from the population K. Ideally, the selection proce-
dure should be balanced between exploitation of parents with high fitness (which will more likely
produce children with high fitness) and exploration of mutations of poor performing parents (which
might eventually produce children with high fitness while increasing population diversity). Diver-
sity is crucial, as K is a set of unique individuals and therefore the improvement of the overall
3
Under review as a conference paper at ICLR 2018
A) Parent selection
Figure 1: Components of the genetic algorithm.
B) Crossover
Sa : (1 …;11)	(1 …0 1)
Sb :(0 •••：：0 1)	(0 …1 1)
C) Mutation
(1 …0 1)
p1l	Ip0
(0 …1 1)
fitness of the population depends on generating different children with high fitness. In our numerical
experiments we explored both fitness-proportional selection of parents (a classic strategy in which
the probability of an individual being selected as a parent is proportional to its fitness) and random
uniform selection of parents.
Crossover. During the crossover step, random pairs of parents are selected; then each pair is as-
signed a number c from 1 to H - 1 with uniform probability (this is the single crossover point);
finally the parents swap the last H - c bits to produce the offspring. We denote Nc the number of
children generated in this way. The crossover step can be skipped, making the EA more lightweight
but decreasing variety in the offspring.
Mutation. Finally, each of the Nc children undergoes one or more random bitflips to further increase
offspring diversity. In our experiments we compare results of random uniform selection of the bits
to flip with a more refined sparsity-driven bitflip algorithm. This latter bitflip schemes assignes
to 0’s and 1’s different probabilities of being flipped in order to produce children with a sparsity
compatible with the one learned by the model. In case the crossover step is skipped, a different
bitflip mutation is performed on Nc identical copies of each parent.
Algorithm 1: Evolutionary Expectation Maxi-
mization
choose initial model parameters Θ and initial sets
K(n)
repeat
for each data-point n do
candidates = {}
for g = 0 to Ng do
parents = Select.parents
children =
mutation(crossover(parents))
candidates = candidates ∪ children
K (n) = Select_best(K(n) ∪ candidates)
(n)
update Θ using M-steps with (3) and K
until F has increased sufficiently
A full run of the evolutionary algorithm
therefore produces NgNcNp children (or
new states S⅛). Finally We compute the
union set of the original population K with
all children and select the S fittest individ-
uals of the union as the new population K0 .
The EEM Algorithm. We now have all
elements required to formulate a learning
algorithm with EAs as its integral part.
Alg. 1 summarizes the essential computa-
tional steps. Note that this E-step can be
trivially parallelized over data-points. Fi-
nally, it is worth pointing out that algo-
rithm 1, by construction, never decreases
the free-energy.
4 The Generative Models
We will use the EA formulated above as integral part of an unsupervised learning algorithm. The
objective of the learning algorithm is the optimization of the log-likelihood 1. D denotes the number
of observed variables, H the number of hidden units, and N the number of data points.
Noisy-OR. The noisy-OR model is a highly non-linear bipartite data model with all-to-all connec-
tivity among hidden and observable variables. All variables take binary values. The model assumes
a Bernoulli prior for the latents, and active latents are then combined via the actual noisy-OR rule.
p(~S | Θ) =Yπhsh(1-πh)1-sh	(8)
h
p(y~ | ~S, Θ) =YNd(~S)yd(1-Nd(~S))1-yd where	Nd(~S) := 1-Y(1-WdhSh)	(9)
4
Under review as a conference paper at ICLR 2018
Figure 2: A small Noisy-OR
model. Each observable yd
is conditionally dependent
on all sh . The generative
process first samples each
sh from a Bernoulli distribu-
tion; then each yd is sampled
from a Bernoulli distribution
of parameter Nd(~s), gener-
ating a data-point.
In the context of the Noisy-OR model, Θ = {~π, W }, where ~π is the set of values πh ∈ [0, 1]
representing the prior activation probabilities for the hidden variables sh and W~ is a D ×H matrix
of values Wdh ∈ [0, 1] representing the probability that the latent sh activates the observable yd.
Section A of the appendix contains the explicit forms of the free energies and the M-step update
rules for noisy-OR.
Binary Sparse Coding. As a second model and one for continuous data, we consider Binary Sparse
Coding (BSC; Henniges et al., 2010). BSC differs from standard Sparse Coding in its use of binary
latent variables. The latents are assumed to follow a univariate Bernoulli distribution which uses the
same activation probability for each hidden unit. The combination of the latents is described by a
linear superposition rule. Given the latents, the observables are independently and identically drawn
from a Gaussian distribution:
H	DH
p(~s | Θ) = Y πsh (1 - π)1-sh , p(~y | ~s, Θ) = YN(yd; X Wdhsh, σ2) .	(10)
h=1	d=1	h=1
The parameters of the model are Θ = (π, W, σ2), where W is a D × H matrix whose columns
contain the weights associated with each hidden unit sh and where σ2 determines the variance of
the Gaussian. M-step update rules for BSC can be derived in close-form by optimizing the free
energy (2) wrt. all model parameters (compare, e.g., Henniges et al., 2010). We report the final
expressions in appendix B.
5	Numerical Experiments
We describe numerical experiments performed to test the applicability and scalability of EEM.
Throughout the section, the different evolutionary algorithms are named by indicating which parent
selection procedure was used (“fitparents” for fitness-proportional selection, “randparents” for ran-
dom uniform selection) and which bitflip algorithm (“sparseflips” or “randflips”). We add “cross”
to the name of the EA when crossover was employed.
5.1	Artificial Data
First we investigate EMM using artificial data where the ground-truth components are known. We
use the bars test as a standard setup for such purposes (Foldiak, 1990; Hoyer, 2003; Lucke & Sa-
hani, 2008). In the standard setup, Hgen/2 non-overlapping vertical and Hgen/2 non-overlapping
horizontal bars act as components on D = Hgen × Hgen pixel images. N images are then generated
by first selecting each bar with probability πgen . The bars are then superimposed according to the
noisy-OR model (non-linear superposition) or according to the BSC model. In the case of BSC
Gaussian noise is then added.
Noisy-OR. Let us start with the standard bars test which uses a non-linear superposition (F0ldiak,
1990) of 16 different bars (SPratling, 1999; Lucke & Sahani, 2008), and a standard average crowd-
edness of two bars per images (∏gen = Hgen). We apply EEM for noisy-OR using different con-
figurations of the EA. We use H = 16 generative fields. As a performance metric we here employ
reliability (compare, e.g., Spratling, 1999; Lucke & Sahani, 2008), i.e., the fraction of runs whose
learned free energies are above a certain minimum threshold and which learn the full dictionary of
bars as well as the correct values for the prior probabilities π.
5
Under review as a conference paper at ICLR 2018
Figure 3: Reliability for the
listed EAs over 10 runs of
EEM for noisy-OR on 8x8
bars images. In this figure,
black bars indicate both pri-
ors and bars were recovered
correctly, grey bars indicate
bars were recovered but not
priors. For all runs H = 16,
N = 104, Ng = 2, Np = 8,
Nc = 7, S = 120. Each run
performed 100 iterations.
Figure 3 shows reliabilities over 10 different runs for each of the EAs. On 8x8 images the more
exploitative nature of “fitparents-sparseflips” is advantageous over the simpler and more explorative
“randparents-randflips”. Note that this is not necessarily true for lower dimensionalities or otherwise
easier-to-explore state spaces, in which also a naive random search might quickly find high-fitness
individuals. In this test the addition of crossover reduces the probability of finding all bars and leads
to an overestimation of the crowdedness πH .
After the initial verification on a standard bars test, we now make the component extraction problem
more difficult by increasing overlap among the bars. A highly non-linear generative model such as
noisy-OR is a good candidate to model occlusion effects in images. Figure 4 shows the results of
training noisy-OR with EEM on a bars data-set in which the latent causes have sensible overlaps.
The test parameters were chosen to be equal to those in (Lucke & Sahani, 2008, Fig. 9). After
applying EEM with noisy-OR (H = 32) to N = 400 images with 16 strongly overlapping bars, we
observed that all Hgen = 16 bars were recovered in 13 of 25 runs, which is competitive especially
when keeping in mind that no additional assumptions (e.g., compared to other models applied to this
test) are used by EEM for noisy-OR.
Sample data-points	Learned weights
[j[lB□ ≡H≡raBH≡E
m[i□□ Ξan≡QH∏≡
anno ≡bħq∏[ihb
hdh≡ BmasHnsH
Figure 4: Sample input (left) and learned generative fields (right) for a run on overlapping bars.
Out of 25 runs, 13 recovered all 16 ground-truth generative components (14.92 recovered bars in
average, median 16). As H = 32, the extra generative fields are used to explain common overlaps
and noise.
BSC. Like for the non-linear generative model, we first evaluate EEM for the linear BSC model on
a bars test. For BSC, the bars are superimposed linearly (Henniges et al., 2010), which makes the
problem easier. As a consequence, standard bars test were solved with very high reliability using
EEM for BSC even if merely random bitflips were used for the EA. In order to make the task more
challenging, we therefore (A) increased the dimensionality of the data to D = 10 × 10 bars images,
(B) increased the number of components to Hgen = 20, and (C) increased the average number of
bars per data point from two (the standard setting) to five. We employed N = 5, 000 training data
points and tested the same five different configurations of the EA as were evaluated for noisy-OR.
We set the number of hidden units to H = Hgen = 20 and used S = 120 variational states. Per
data point and per iteration, in total 112 new states (Np = 8, Nc = 7, Ng = 2) were sampled to
vary Kn. Per configuration of the EA, we performed 20 independent runs, each with 300 iterations.
The results of the experiment are depicted in Fig. 5. We observe that a basic approach such as
random uniform selection of parents and random uniform bitflips for the EA works well. However,
more sophisticated EAs improve performance. For instance, combining bitflips with crossover and
selecting parents proportionally to their fitness shows to be very benefical. The results also show
that sparseness-driven bitflips lead generally to very poor performance, even if crossover or fitness-
6
Under review as a conference paper at ICLR 2018
Figure 5: Reliability for the
listed EAs over 20 runs of
EEM for BSC on 10x10 bars
images.
proportional selection of the parents is included. This effect may be explained with the initialization
of Kn. The initial states are drawn from a Bernoulli distribution with parameter H which makes
it more difficult for sparseness-driven EAs to explore and find solutions with higher crowdedness.
Fig. 8 in appendix C depicts the averaged free energy values for this experiment.
5.2	Natural image patches
Next, we verify the approach on natural data. We use patches of natural images, which are known
to have a multi-component structure, which are well investigated, and for which typically models
with high-dimensional latent spaces are applied. The image patches used are extracted from the van
Hateren image database (van Hateren & van der Schaaf, 1998).
Noisy-OR. First we consider raw images patches, i.e., images without substantial pre-processing
which directly reflect light intensities. Such image patches were generated by extracting random
square subsections ofa single 255x255 image of overlapping grass wires (part of image 2338 of the
database). We removed the brightest 1% pixels from the data-set, scaled each data-point to have
gray-scale values in the range [0, 1] and then created data points with binary entries by repeatedly
choosing a random gray-scale image and sampling binary pixels from a Bernoulli distribution with
parameter equal to the gray-scale value of the original pixel (cfr. figure 6). Note that components
in such light-intensity images can be expected to superimpose non-linearly because of occlusion,
which motivates the application of a non-linear generative model such as noisy-OR. We employ the
“fitparents-sparseflips” evolutionary algorithm that was shown to perform best on artificial data (3).
Parameters were H = 100, S = 120, Ng = 2, Np = 8, Nc = 7. Figure 6 shows the generative fields
learned over 200 iterations. EEM allows learning of generative fields resembling curved edges, in
line with expectations and with the results obtained in (Lucke & Sahani, 2008).
□S□□□OE□□Ξ
QEaSBQHHGQ
Figure 6: 50 generative fields learned by applying EEM ("fitparents-sparseflips")for noisy-OR to
natural image patches. See Appendix F for a run at H = 200.
BSC. Finally, we consider pre-processed image patches using common whitening approaches as
they are customary for sparse coding approaches (Olshausen & Field, 1997). We use N = 100, 000
patches of size D = 16 × 16, randomly picked from the whole data set. The highest 2 % of the am-
plitudes were clamped to compensate for light reflections and patches without significant structure
were excluded for learning. ZCA whitening (Bell & Sejnowski, 1997) was applied retaining 95 %
of the variance (We used the procedure of a recent paper Exarchakis & Lucke, 2017). We trained the
BSC model for 4,000 iterations using the “fitparents-cross-sparseflips” EA and employing H = 300
hidden units and S = 200 variational states. Per data point and per iteration, in total 360 new states
(Np = 10, Nc = 9, Ng = 4) were sampled to vary Kn . The results of the experiment are depicted
in Fig. 7. The obtained generative fields primarily take the form of Gabor functions with different
locations, orientations, phase, and spatial frequencies. This is a typical outcome of sparse coding
being applied to images. On average more than five units were activated per data point showing that
7
Under review as a conference paper at ICLR 2018
the learned code makes use of the generative model’s multiple causes structure. The generative fields
converged faster than prior and noise parameters (similar effects are known from probabilistic PCA
for the variance parameter). The finit slope of the free-energy after 4000 iterations is presumably
due to these parameters still changing slowly.
A
■■■■■■■■■■■■■
7出「寺b.上♦■■■■■■
■■■■■■■■■■■■■■■■■
b V N 一梦=Z ,外
BCD
/ (V①E
Figure 7: Results on training the BSC model on natural images using the “fitparents-cross-
sparseflips” EA. A 60 of the 300 generative fields obtained through training (see Appendix for all
fields). B Evolution of the free energy per data point over iterations. C Evolution of the expected
number of active hidden units per data point over iterations. D Evolution of the standard deviation
over iterations.
0.6
匕0.5
0.4
0	1000	2000	3000	4000
Iteration
6	Discussion
The training of generative models is a very intensively studied branch of Machine Learning. If
EM is applied for training, most non-elementary models require approximations. For this reason,
sophisticated and mathematically grounded approaches such as sampling or variational EM have
been developed in order to derive sufficiently precise and efficient learning algorithms.
Evolutionary algorithms (EAs) have also been applied in conjunction with EM. Pernkopf &
Bouchaffra (2005), for instance, have used EAs for clustering with Gaussian mixture models
(GMMs). However, the GMM parameters are updated by their approach relatively convention-
ally using EM, while EAs are used to select the best GMM models for the clustering problem
(using a min. description length criterion). Such a use of EAs is similar to DNN optimization where
EAs optimize DNN hyperparameters in an outer optimization loop (Stanley & Miikkulainen, 2002;
Loshchilov & Hutter, 2016; Real et al., 2017; Suganuma et al., 2017, etc), while the DNNs them-
selves are optimized using standard error-minimization algorithms. Still other approaches have used
EAs to directly optimize, e.g., a clustering objective. But in these cases EAs replace EM approaches
for optimization (compare Hruschka et al., 2009). In contrast to all such previous applications, we
have here shown that EAs and EM can be combined directly and intimately: Alg. 1 defines EAs as
an integral part of EM, and as such EAs address the key optimization problem arising in the training
of generative models.
We see the main contribution of our study in the establishment of this close theoretical link between
EAs and EM. This novel link will make it possible to leverage an extensive body of knowledge and
experience from the community of evolutionary approaches for learning algorithms. Our numerical
experiments are a proof of concept which shows that EAs are indeed able to train generative models
with large hidden spaces and local optima. For this purpose we used very basic EAs with elementary
selection, mutation, cross-over operators.
EAs more specialized to the specific optimization problems arising in the training of generative
models have great potentials in future improvements of accuracy and scalability, we believe. In
our experiments, we have only just started to exploit the abilities of EAs for learning algorithms.
Still, our results represent, to the knowledge of the authors, the first examples of noisy-OR or sparse
coding models trained with EAs (although both models have been studied very extensively before).
Most importantly, we have pointed out a novel mathematically grounded way how EAs can be used
for generative models with binary latents in general. The approach here established is, moreover,
8
Under review as a conference paper at ICLR 2018
not only very generically formulated using the models’ joint probabilities but it is also very straight-
forward to apply.
References
A. J. Bell and T. J. Sejnowski. The “independent components” of natural scenes are edge filters.
VisionResearch, 37(23):3327-38,1997.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the
EM algorithm (with discussion). Journal of the Royal Statistical Society B, 39:1-38, 1977.
G. Exarchakis and J. Lucke. Discrete sparse coding. Neural Computation, 29:2979-3013, 2017.
L.	J. Fogel, A. J. Owens, and M. J. Walsh. Artificial intelligence through simulated evolution. 1966.
Peter Foldiak. Forming sparse representations by local anti-hebbian learning. Biological Cybernet-
ics, 64(2):165-170, 1990.
D. Forster and J. Lucke. Truncated variational EM for semi-supervised Neural Simpletrons. In
IJCNN, pp. 3769-3776, 2017.
M.	Henniges, G. Puertas, J. Bornschein, J. Eggert, and J. Lucke. Binary sparse coding. In Proceed-
ings LVA/ICA, LNCS 6365, pp. 450-57. Springer, 2010.
P. O. Hoyer. Modeling receptive fields with non-negative sparse coding. Neurocomputing, 52-54:
547-52, June 2003. ISSN 09252312. doi: 10.1016/S0925-2312(02)00782-8.
P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of Machine
Learning Research, 5:1457-69, 2004.
E. R. Hruschka, R. JGB Campello, A. A. Freitas, et al. A survey of evolutionary algorithms for
clustering. IEEE Trans. on Systems, Man, and Cybernetics, 39(2):133-155, 2009.
M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for
graphical models. Machine Learning, 37:183-233, 1999.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
I. Loshchilov and F. Hutter. CMA-ES for hyperparameter optimization of deep neural networks. In
ICLR Workshop, pp. 513-520, 2016.
J. Lucke. Truncated variational expectation maximization. arXivpreprint, arXiv:1610.03113, 2016.
J. Lucke and J. Eggert. Expectation truncation and the benefits of preselection in training generative
models. JMLR, 11:2855-900, 2010.
J. Lucke and M. Sahani. Maximal causes for non-linear component extraction. Journal OfMachine
Learning Research, 9:1227-67, 2008.
J. W. Myers, K. B. Laskey, and K. A. DeJong. Learning Bayesian networks from incomplete data
using evolutionary algorithms. In Proc. Annual Conference on Genetic and Evolutionary Com-
putation, pp. 458-465, 1999.
R. Neal and G. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other
variants. In M. I. Jordan (ed.), Learning in Graphical Models. Kluwer, 1998.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed
by V1? Vision Research, 37(23):3311-3325, 1997.
M. Opper and O. Winther. Expectation consistent approximate inference. JMLR, 6:2177-04, 2005.
A. B. Patel, T. Nguyen, and R. G. Baraniuk. A probabilistic theory of deep learning. In NIPS, pp.
2558-2566, 2016.
9
Under review as a conference paper at ICLR 2018
F. Pernkopf and D. Bouchaffra. Genetic-based em algorithm for learning gaussian mixture models.
IEEE Trans. on Pattern Analysis and Machine Intelligence, 27(8):1344-1348, 2005.
E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, and A. Kurakin. Large-scale
evolution of image classifiers. In ICML, pp. 2902-2911, 2017.
I.	Rechenberg. Cybernetic solution path of an experimental problem. 1965.
S.	Roweis. EM algorithms for PCA and SPCA. NIPS, pp. 626-32, 1998.
T.	Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to
reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
L. K. Saul and M. Jordan. Exploiting tractable substructures in intractable networks. NIPS, pp.
486-492, 1996.
J. Schmidhuber. Deep learning in neural networks. Neural networks, 61:85-117, 2015.
A.-S. Sheikh, J. A. Shelton, and J. Lucke. A truncated EM approach for SPike-and-slab sparse
coding. Journal of Machine Learning Research, 15:2653-2687, 2014.
J.	A. Shelton, J. Gasthaus, Z. Dai, J. Lucke, and A. Gretton. Gp-select: Accelerating em using
adaptive subspace preselection. Neural Computation, 29(8):2177-2202, 2017.
M. W. Spratling. Pre-synaptic lateral inhibition provides a better architecture for self-organising
neural networks. Network: Computation in Neural Systems, 10:285 - 301, 1999.
M. W. Spratling, K. De Meyer, and R. Kompass. Unsupervised learning of overlapping image
components using divisive input modulation. Computational Intelligence and Neuroscience, pp.
1-19, 2009. ISSN 1687-5265.
K.	O. Stanley and R. Miikkulainen. Evolving neural networks through augmenting topologies.
Evolutionary Computing, 10(2):99-127, 2002. ISSN 1063-6560.
M. Suganuma, S. Shirakawa, and T. Nagao. A genetic programming approach to designing convo-
lutional neural network architectures. In GECCO, pp. 497-504, 2017.
M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statis-
tical Society. Series B, 61, 1999.
J.	H. van Hateren and A. van der Schaaf. Independent component filters of natural images compared
with simple cells in primary visual cortex. Proceedings of the Royal Society of London B, 265:
359-66, 1998.
10
Under review as a conference paper at ICLR 2018
Appendix
A: NOISY-OR
The truncated free energy takes on the following form for Noisy-OR:
FNOR(K, Θ) =NXlog(1-πh)+Xlog X exp Fe
h	n	~s∈K(n)
Fe(~,⑼ := X Sh log (1-h∏h)
+ X yn log Q Q-∩——7 -1)
d d	h (1 - WdhSh)
+ X log(1 - WdhSh)
h
The M-step equations for noisy-OR are obtained by taking derivatives of the free energy, equating
them to zero and solving the resulting set of equations. We report the results here for completeness:
I	Σn(yn - DhDdhMqn
+ —Pn hC")qn一
(11)
(12)
where
Ddh ( ~S) :
Wdh(~)sh
Nd(~)(1 - Nd(~))
..	-~~■ .. ..
Cdh(~S) :=Wfdh(~S)Ddh(~S)
(13)
Wfdh(~S) :=	(1 - Wdh0Sh0 )
h0 6=h
The update rule for ~π is quite straightforward. The update equations for the weights Wdh, on the
other hand, do not allow a closed form solution (i.e. no exact M-step equation can be derived). The
rule presented here, instead, expresses each Wdnhew as a function of all current W~ ; this is a fixed-
point equation whose fixed point would be the exact solution of the maximization step. Rather than
solving the equation numerically at each step of the learning algorithm, we exploit the fact that in
practice one single evaluation of 13 is enough to (noisily, not optimally) move towards convergence.
Since TV-EM is guaranteed to never decrease F, drops of the free-energy during training can only
be ascribed to this fixed-point equation; this provides a simple mechanism to check and possibly
correct for misbehaviors of 13 if needed.
B: M-step update rules for BSC
The free energy for BSC follows from inserting (10) into (2). Update rules can be obtained by
optimizing the resulting expression separately for the model parameters π, σ2 and W (compare,
e.g., Henniges et al., 2010). For the sake of completeness, we show the result here:
NH
π = N XX hshiqn	(14)
n=1 h=1
N
σ2 =而 XD∣∣ ~ (n)- W~ II2E	(15)
ND	qn
n=1
11
Under review as a conference paper at ICLR 2018
Exact EM can be obtained by setting qn to the exact posterior p(~s | y~ (n) , Θ). As this quickly be-
comes computational intractable with higher latent dimensionality, we approximate exact posteriors
by truncated variational distributions (3). For BSC, the truncated free energy (4) takes the form
F (K, Θ) = - ND log (2πσ2)+NH log (1 - ∏)+X log ( X exp (log p(y(n), ~∣Θ)^ ) (17)
n	~s∈Kn
where
logp(y，用⑼=-2⅛(~-Ws)T(~- W~)+ 同 log (H)	(18)
C: Further experimental results for BSC
N、q①K
-220
O
50
100	150	200
Iteration
250	300
Figure 8: Results of the experiment with artificial data (10x10 bars) for the BSC model. Depicted
is the evolution of the free energy for different EAs averaged over 20 independent runs. Dots and
vertical errorbars show the mean and the standard deviation, respectively.
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
v' ML 1nW " . 1多.产h工点Fi W 叫∙ ]F d ；1腌飞鸟
W多至三好鼠h -中国弊3"皿犷"就俨”3咚，
黑/感⅛ ，∙,ysJyW空箪爸分』/，』H N可⅛≡ ill鹭
A 1T⅛	/ 小弟林版..W/麻/您F濯岁鼠”做弃掇“胃H
果，XW M至AW奏TM瓠犷J-W域T/谕.滑鎏声一I卅询『版
“m电通浪气/守，群寻堂起.检选4整W雯=A彖M资9滑E缪之
Figure 9: Full dictionary learned from natural images by the BSC model trained with the “fitparents-
cross-sparseflips” EA. Depicted is the dictionary at iteration 4,000. The generative fields are ordered
according to their activation, starting with most active fields.
12
Under review as a conference paper at ICLR 2018
D:	Sparsity-driven b itflips
When performing sparsity-driven bitflips, We flip each bit of a particular child 于 with probability
p0 if it is 0, with probability p1 otherwise. We call pbf the average probability of flipping any bit in
s⅛. We impose the following constraints on po and pi:
•	p1 = αp0 for some constant α
•	the average number of on bits after mutation is set at se
which yield the following expressions for p0 and p1 :
(H -同)∙((Hpbf) -(e--))
=	(e -同 + Hpbf )冈
_	Hpbf
p0 = H + (α - 1)冈
pi = α ∙ p0
Trivially, random uniform bitflips correspond to the case p0 = p1 = pbf.
E:	Reliability of EEM for noisy-OR on overlapping bars
With respect to the tests shown in figure 4 and discussed in section 5.1, it is worth to spend a few
more words on comparisons with the other algorithms shown (LuCke & Sahani, 2008, Fig. 9). Quan-
titative comparison to NMF approaches, neural nets (DI Spratling et al., 2009), and MCA (Lucke
& Sahani, 2008) shows that EMM for noisy-OR performs well but there are also approaches with
higher reliability. Of all the approaches which recover more than 15 bars on average, most require
additional assumptions. E.g., all NMF approaches, non-negative sparse coding (Hoyer, 2004) and
R-MCA2 require constraints on weights and/or latent activations. Only MCA3 does not require
constraints and presumably neither DI. DI is a neural network approach, which makes the used as-
sumptions difficult to infer. MCA3 is a generative model with a max-non-linearity as superposition
model. For learning it explores all sparse combinations with up to 3 components. Applied with
H = 32 latents, it hence evaluates more than 60000 states per data point per iteration for learning.
For comparison, EEM for noisy-OR evaluates on the order of S = 100 states per data point per
iteration.
F:	Higher-scale natural image patches for Noisy-OR
M眼■•回口同国鹏宏㈤阖国凰M0*网阿蛭3瑞因£3国杷匿^钻出图国0R* ■回园园国
阿医0团■胤瑞国寓风留修■修国因□□口 口 HED田匚由回囹回幽网敛回园因国男富阳!!因
Figure 10: Generative fields learned running EEM for noisy-OR (“fitparents-sparseflips”) for 175
iterations with H = 200 latent variables. Learned crowdedness πH was 1.6.
13