Under review as a conference paper at ICLR 2018
Automata Guided Hierarchical Reinforce-
ment Learning for Zero-Shot S kill Composi-
TION
Anonymous authors
Paper under double-blind review
Ab stract
An obstacle that prevents the wide adoption of (deep) reinforcement learning (RL)
in control systems is its need for a large number of interactions with the environ-
ment in order to master a skill. The learned skill usually generalizes poorly across
domains and re-training is often necessary when presented with a new task. We
present a framework that combines techniques in formal methods with hierarchi-
cal reinforcement learning (HRL). The set of techniques we provide allows for
the convenient specification of tasks with logical expressions, learns hierarchical
policies (meta-controller and low-level controllers) with well-defined intrinsic re-
wards using any RL methods and is able to construct new skills from existing ones
without additional learning. We evaluate the proposed methods in a simple grid
world simulation as well as simulation on a Baxter robot.
1	Introduction
Reinforcement learning has received much attention in the recent years because of its achievements
in games Mnih et al. (2015), Silver et al. (2016), robotics manipulation Jang et al., Levine et al.
(2016), Gu et al. (2016) and autonomous driving Isele et al. (2017), Madrigal (2017). However,
training a policy that sufficiently masters a skill requires an enormous amount of interactions with
the environment and acquiring such experience can be difficult on physical systems. Moreover,
most learned policies are tailored to mastering one skill (by maximizing the reward) and are hardly
reusable on a new skill.
Skill composition is the idea of constructing new skills out of existing skills (and hence their policies)
with little to no additional learning. In stochastic optimal control, this idea has been adopted by
authors of Todorov (2009) and Da Silva et al. (2009) to construct provably optimal control laws
based on linearly solvable Markov decision processes. Authors of Haarnoja et al. (2017), Tang
& Haarnoja have showed in simulated manipulation tasks that approximately optimal policies can
result from adding the Q-functions of the existing policies.
Hierarchical reinforcement learning is an effective means of achieving transfer among tasks. The
goal is to obtain task-invariant low-level policies, and by re-training the meta-policy that schedules
over the low-level policies, different skills can be obtain with less samples than training from scratch.
Authors of Heess et al. (2016) have adopted this idea in learning locomotor controllers and have
shown successful transfer among simulated locomotion tasks. Authors of Oh et al. (2017) have
utilized a deep hierarchical architecture for multi-task learning using natural language instructions.
Temporal logic is a formal language commonly used in software and digital circuit verification Baier
& Katoen (2008) as well as formal synthesis Belta et al. (2017). It allows for convenient expression
of complex behaviors and causal relationships. TL has been used by Sadraddini & Belta (2015),
Leahy et al. (2015) to synthesize provably correct control policies. Authors of Aksaray et al. (2016)
have also combined TL with Q-learning to learn satisfiable policies in discrete state and action
spaces.
In this work, we focus on hierarchical skill acquisition and zero-shot skill composition. Once a set of
skills is acquired, we provide a technique that can synthesize new skills without the need to further
interact with the environment (given the state and action spaces as well as the transition remain the
1
Under review as a conference paper at ICLR 2018
same). We adopt temporal logic as the task specification language. Compared to most heuristic
reward structures used in the RL literature to specify tasks, formal specification language excels at
its semantic rigor and interpretability of specified behaviors. Our main contributions are:
•	We take advantage of the transformation between TL formula and finite state automata
(FSA) to construct deterministic meta-controllers directly from the task specification with-
out the necessity for additional learning. We show that by adding one discrete dimension
to the original state space, structurally simple parameterized policies such as feed-forward
neural networks can be used to learn tasks that require complex temporal reasoning.
•	Intrinsic motivation has been shown to help RL agents learn complicated behaviors with
less interactions with the environment Singh et al. (2004), Kulkarni et al. (2016), Jader-
berg et al. (2016). However, designing a well-behaved intrinsic reward that aligns with the
extrinsic reward takes effort and experience. In our work, we construct intrinsic rewards
directly from the input alphabets of the FSA (a component of the automaton), which guar-
antees that maximizing each intrinsic reward makes positive progress towards satisfying the
entire task specification. From a user’s perspective, the intrinsic rewards are constructed
automatically from the TL formula.
•	In our framework, each FSA represents a hierarchical policy with low-level controllers that
can be re-modulated to achieve different tasks. Skill composition is achieved by manipu-
lating the FSA that results from their TL specifications in a deterministic fashion. Instead
of interpolating/extrapolating among existing skills, we present a simple policy switching
scheme based on graph manipulation of the FSA. Therefore, the compositional outcome is
much more transparent. We introduce a method that allows learning of such hierarchical
policies with any non-hierarchical RL algorithm. Compared with previous work on skill
composition, we impose no constraints on the policy representation or the problem class.
2	Preliminaries
2.1	The Options Framework in Hierarchical Reinforcement learning
In this section, we briefly introduce the options framework Sutton et al. (1998), especially the ter-
minologies that we will inherit in later sections. We start with the definition of a Markov Decision
Process.
Definition 1. An MDP is defined as a tuple M =(S, A,p(∙∣∙, ∙), R(∙, ∙, •)〉, where S ⊆ IRn is the
state space ; A ⊆ IRm is the action space (S and A can also be discrete sets); p : S × A × S → [0, 1]
is the transition function with p(s0|s, a) being the conditional probability density of taking action
a ∈ A at state s ∈ S and ending up in state s0 ∈ S; R : S× A × S → IR is the reward function. letT
be the length of a fixed time horizon. The goal is to find a policy π? : S → A (or π? : S × A → [0, 1]
for stochastic policies) that maximizes the expected return, i.e.
π? = arg maxEπ [R(τT )]
π
(1)
where τT = (s0, a0, ..., sT, ) denotes the state-action trajectory from time 0 to T.
The options framework exploits temporal abstractions over the action space. An option is defined
as a tuple o = hI, πo , βi where I is the set of states that option o can be initiated (here we let
I = S for all options), πo : S → A is an options policy and β : S → [0, 1] is the termination
probability for the option at state s. In addition, there is a policy over options πh : S → O (where
O is a set of available options) that schedules among options. At a given time step t, an option o is
chosen according to πh (st) and the options policy πo is followed until the termination probability
β(s) > threshold at time t + k, and the next option is chosen by πh(st+k).
2
Under review as a conference paper at ICLR 2018
2.2	scTLTL And Automata
We consider tasks specified with Truncated Linear Temporal Logic (TLTL). We restrict the set of
allowed operators to be
Φ ：= > | f (s) < c l-Φ | Φ ∧ Ψ | Φ ∨ Ψ |
◊。| φ U ψ | φ T ψ | Q φ | φ ⇒ ψ
(2)
where f(s) < c is a predicate,  (negation/not), ∧ (conjunction/and), and ∨ (disjunction/or) are
Boolean connectives, and ◊ (eventually), U (until), T (then), Q (next), are temporal operators.
Implication is denoted by ⇒ (implication). Essentially We excluded the Always operator (□) with
reasons similar to Kupferman & Vardi (2001). We refer to this restricted TLTL as syntactically
co-safe TLTL (scTLTL) (Vasile et al. (2017) used similar idea for LTL). There exists a real-value
function p(so：T, φ) called robustness degree that measures the level of satisfaction of trajectory so：T
with respective to φ. p(so：T, φ) > 0 indicates that so：T satisfies φ and vice versa. Definitions for the
boolean semantics and robustness degree are provided in Appendix E.
Any scTLTL formula can be translated into a finite state automata (FSA) with the following defini-
tion:
Definition 2. An FSA is defined as a tuple Aφ = hQφ, Ψφ,q0,Pφ(∙∣∙), Fφ, where Qφ is a set
of automaton states; Ψφ is an input alphabet, we denote ψqi ,qj ∈ Ψφ the predicate guarding the
transition from qi to qj (as illustrated in Figure 1 ); qo ∈ Qφ is the initial state; pφ ： Qφ × Qφ →
[0, 1] is a conditional probability defined as
Pφ(qj I%) = ]1 晚QstrUu	(3)
0 otherwise.
In addition, given an MDP state s, we can calculate the transition in automata states at s by
Pφ(qj |qi,S)=
10
ρ(s,ψqi,qj) > 0
otherwise.
(4)
We abuse the notation pφ to represent both kinds of transitions when the context is clear. Fφ is a set
of final automaton states.
The translation from TLTL formula to FSA to can be done automatically with available packages
like Lomap Ulusoy (2017).
Example 1. Figure 1 (left) illustrates the FSA resulting from formula φ = b U a. In English, φ
entails during a run, b cannot be true until a is true and a needs to be true at least once. The FSA
has three automaton states Qφ = {qo, qf , trap} with qo being the input(initial) state (here qi serves
to track the progress in satisfying φ). The input alphabet is defined as the Ψφ = {a ∧ b, a ∧
b, a∧-b,a∧b}. Shorthands are used in thefigure,for example a = (a∧b) ∨ (a∧-b). Ψφ represents
the power set of {a, b}, i.e. Ψφ = 2{a,b}. During execution, the FSA always starts from state qo and
transitions according to Equation (3) or (4). The specification is satisfied when qf is reached and
violated when trap is reached. In this example, qf is reached only when a becomes true before b
becomes true.
3 Problem Formulation And Approach
We start with the following problem definition:
Problem 1. Given an MDP in Definition 1 with unknown transition dynamics p(s0|s, a) and a
scTLTL specification φ over state predicates (along with its FSA Aφ) as in Definition 2. Find a
policy πφ? such that
∏φ = argmaxEπφ[l(ρ(so:T,φ) > 0)].
πφ
where I(P(So：T, φ) > 0) is an indicatorfunction with value 1 if p(so：T, φ) > 0 and 0 otherwise.
(5)
3
Under review as a conference paper at ICLR 2018
Problem 1 defines a policy search problem where the trajectories resulting from following the opti-
mal policy should satisfy the given scTLTL formula in expectation.
Problem 2. Given two scTLTL formula φ1 and φ2 along with policy πφ1 that satisfies φ1 and πφ2
that satisfies φ2. Obtain a policy πφ that satisfies φ = φ1 ∧ φ2.
Problem 2 defines the problem of task composition. Given two policies each satisfying a scTLTL
specification, construct the policy that satisfies the conjunction of the given specifications. Solving
this problem is useful when we want to break a complex task into simple and manageable compo-
nents, learn a policy that satisfies each component and ”stitch” all the components together so that
the original task is satisfied. It can also be the case that as the scope of the task grows with time, the
original task specification is amended with new items. Instead of having to re-learn the task from
scratch, we can only learn a policy that satisfies the new items and combine them with the old policy.
We propose to solve Problem 1 by constructing a product MDP from the given MDP and FSA
that can be solved using any state-of-the-art RL algorithm. The idea of using product automaton
for control synthesis has been adopted in various literature Leahy et al. (2015), Chen et al. (2012).
However, the methods proposed in these works are restricted to discrete state and actions spaces.
We extend this idea to continuous state-action spaces and show its applicability on robotics systems.
For Problem 2, we propose a policy switching scheme that satisfies the compositional task spec-
ification. The switching policy takes advantage of the characteristics of FSA and uses robustness
comparison at each step for decision making.
4 FSA Augmented MDP
Problem 1 can be solved with any episode-based RL algorithm. However, doing so the agent suffers
from sparse feedback because a reward signal can only be obtained at the end of each episode. To
address this problem as well as setting up ground for automata guided HRL, we introduce the FSA
augmented MDP
Definition 3. An FSA augmented MDP corresponding to scTLTL formula φ is defined as Mφ =
(S,A,p(∙∣∙, ∙),R(∙, •)〉where S ⊆ S X Qφ, A is the same as the original MDP p(s0∣s,a) is the
probability of transitioning to s0 given s and a, in particular
P(S7区 a) = P((S0,qO)IGq),a)
Ip(SlS,a) Pφ(q0lq,s) = 1
0	otherwise.
(6)
Here Pφ is defined in Equation (4). R : S × S → IR is the FSA augmented reward function, defined
by
R(S,S∖ = IS(S0, Dφ) > 0)1	⑺
where Ωq is the set of automata states that are connected with q through outgoing edges. Dφ =
Vq0∈Ω Ψqα represents the disjunction ofallpredicates guarding the transitions that originatefrom
q. The goal is to find the optimal policy that maximizes the expected sum of discounted return, i.e.
π? = arg maxEπ
π
T-1
X Yt+1R(St,St+ι)
t=0
(8)
where γ < 1 is the discount factor, T is the time horizon.
As a quick example to the notation Dφ, consider the state qo in the FSA in Figure 1 , Ωq0 =
q
{traP, qf}, Dφq0 =ψq0 ,trap ∨ ψq0,qf = b ∨ a. The goal is then to find a policy π : S → A that
maximizes the expected sum of R over the horizon T .
1Because Dφ is a predicate without temporal operators, the robustness p(st：t+k, Dφ) is only evaluated at
st (refer to Appendix E). Therefore, We use the shorthand ρ(st, Dφ) = p(st：t+k, Dφ)
4
Under review as a conference paper at ICLR 2018
Figure 1 : FSA constructed from φ = b U a. (right): Specification amendment example. Aφ1 is constructed
from φ1 = ♦a ∧ ♦b. Aφ2 is constructed from φ2 = bU a. Aφ is constructed from φ = φ1 ∧ φ2. The
automaton state pair in parenthesis denote the corresponding states from Qφ1 and Qφ2 that the product state is
constructed from. qi0 denotes the initial state of Aφi , qi,j denotes the jth state of Qφi .
The FSA augmented MDP can be constructed with any standard MDP and a scTLTL formula. And
it can be solved with any off-the-shelf RL algorithm. By directly learning the flat policy π we
bypass the need to learn multiple options policies separately. After obtaining the optimal policy
π?, the optimal options policy for any option Oqcan be extracted by executing π*(a∣s, q) without
transitioning the automata state, i.e. keeping qi fixed (denoted πq? ). And πq? satisfies
T-1
n?i = argmaxEπq1 X γt+1l(ρ(st+ι,Dφ) > 0).	(9)
πqi	t=0
In other words, the purpose of πqi is to activate one of the outgoing edges of qi as soon as possible
and by doing so repeatedly eventually reach qf.
The reward function in Equation (7) encourages the system to exit the current automata state and
move on to the next, and by doing so eventually reach the final state qf . However, this reward does
not distinguish between the trap state and other states and therefore will also promote entering of
the trap state. One way to address this issue is to impose a terminal reward on both qf and trap.
Because the reward is an indicator function with maximum value of 1, we assign terminal rewards
Rqf = 2 and Rtrap = -2.
Appendix D describes the typical learning routine using FSA augmented MDP. The algorithm uti-
lizes memory replay which is popular among off-policy RL methods (DQN, A3C, etc) but this is
not a requirement for learning with Mφ . On-policy methods can also be used.
5	Automata Guided Task Composition
In section, we provide a solution for Problem 2 by constructing the FSA of φ from that of φ1 and φ2
and using φ to synthesize the policy for the combined skill. We start with the following definition.
Definition 4. Given Aφ1 = hQφ1, Ψφ1, q10,pφ1,Fφ1i and Aφ2 = hQφ2, Ψφ2, q20,pφ2,Fφ2i,
The FSA of φ is the product automaton of Aφ1 and Aφ1, i.e. Aφ=φ1∧φ2 = Aφ1 × Aφ2 =
hQφ, Ψφ, q0 , pφ, Fφi where Qφ ⊆ Qφ1 × Qφ2 is the set of product automaton, states, q0 = (q10, q20)
is the product initial state, F ⊆ Fφ1 ∩ Fφ2 is the final accepting states. Following Definition 2, for
states q = (q1, q2) ∈ Qφ and q0 = (q10 , q20 ) ∈ Qφ, the transition probability pφ is defined as
f 1 pφi (q1|qI)PΦ2 (q2 |q2) = 1
0 otherwise.
(10)
Example 2. Figure 1 (right) illustrates the FSA of Aφ1 and Aφ2 and their product automaton Aφ.
Here φ1 = ♦a ∧ ♦b which entails that both a and b needs to be true at least once (order does not
matter), and φ2 = b U a which is the same as Example 1. The resultant product corresponds to
theformula φ = (♦a ∧♦b) ∧ (-b U a) which dictates that a and b need to be true at least once, and
5
Under review as a conference paper at ICLR 2018
a needs to be true before b becomes true (an ordered visit). We can see that the trap state occurs
in Aφ2 and Aφ, this is because if b is ever true before a is true, the specification is violated and
qf can never be reached. In the product automaton, we aggregate all state pairs with a trap state
component into one trap state.
For q = (q1, q2) ∈ Qφ, let Ψq, Ψq1 and Ψq2 denote the set of predicates guarding the outgoing edges
of q, q1 and q2 respectively. Equation (10) entails that a transition at q in the product automaton
Aφ exists only if corresponding transitions at q1, q2exist in Aφ1 and Aφ2 respectively. Therefore,
ψq,q0 = ψq1,q10 ∧ ψq2,q20, for ψq,q0 ∈ Ψq,ψq1,q10 ∈ Ψq1,ψq2,q20 ∈ Ψq2 (here qi0 is a state such that
Pφi (qi∣qi) = 1). Following Equation (9),
T-1
∏? = argmaxEπq[X γt+1lS(St+1, Dlφ) > 0)],
πq	t=0	(11)
where Dφq =	(ψq1,q10 ∧ ψq2,q20).
q10 ,q20
Repeatedly applying the distributive law (∆∧ Ωι) ∨ (∆∧ Ω2) = ∆∧ (Ωι ∨ Ω2) tothe logic formula
Dφq transforms the formula to
Dφ = (Vψqι,q1)∧( Vψq2,q2)= DΦ1 ∧ DΦ2.	(12)
q10	q20
Therefore,
T-1
π? = argmaχEπq [ X Yt+1i(ρ(st+ι,Dφ11 ∧ Dφ2) > O))]
πq	t=0
T-1
=argmaχEπq [ X γt+11 (mm(P(St+1 ,Dφi),ρ(st+ι,Dφ2)) > 0))]
πq	t=0
(13)
The second step in Equation (13) follows the robustness definition. Recall that the optimal options
policies for q1 and q2 satisfy
T-1
n?i = argmaxEπφi [X γt+1l(ρ(st+ι,Dφ;) > 0))], i = 1, 2.	(14)
πqi	t=0
Equation (13) provides a relationship among πq?, πq? and πq? . Given this relationship, We propose
a simple switching policy based on stepwise robustness comparison that satisfies φ = φ1 ∧ φ2 as
follows
π (S q) = πφ1(S,q1) ρ(St,Dφq11) < ρ(St,Dφq22)
φ ,	πφ2 (S, q2) otherwiSe
(15)
We show empirically the use of this switching policy for skill composition and discuss its limitations
in the following sections.
6	Experiments And Discussion
6.1	Grid World Simulation
In this section, we provide a simple grid world navigation example to illustrate the techniques pre-
sented in Sections 4 and 5. Here we have a robot navigating in a discrete 1 dimensional space. Its
6
Under review as a conference paper at ICLR 2018
Figure 2 : upper left: Optimal policy for φ1 = ♦a ∧ ♦b trained using Q-Learning. The arrows represent the
action at each state and the dot represents stay still at that state. (upper right): Optimal policy for φ2 = b U a.
(lower left):Optimal policy for φ = (♦a ∧ ♦b) ∧ (b U a). (lower right ): Robustness comparison used for
construction of policy πφ1∧φ2. The robustness value is zero for states where bars disappear.
MDP state space S = {s|s ∈ [-5, 5), s is discrete}, its action space A = {lef t, stay, right}. The
robot navigates in the commanded direction with probability 0.8, and with probability 0.2 it ran-
domly chooses to go in the opposite direction or stay in the same place. The robot stays in the same
place if the action leads it to go out of bounds.
We define two regions a : -3 < s < -1 and b : 2 < s < 4. For the first task, the scTLTL
specification φ1 = ♦ a ∧ ♦b needs to be satisfied. In English, φ1 entails that the robot needs to
visit regions a and b at least once. To learn a deterministic optimal policy πφ? : S × Q → A, we
use standard Q-Learning Watkins (1989) on the FSA augmented MDP for this problem. We used a
learning rate of 0.1, a discount factor of 0.99, epsilon-greedy exploration strategy with decaying
linearly from 0.0 to 0.01 in 1500 steps. The episode horizon is T = 50 and trained for 500 iterations.
All Q-values are initialized to zero. The resultant optimal policy is illustrated in Figure 2 .
We can observe from the figure above that the policy on each automaton state q serves a specific
purpose. πq? tries to reach region a or b depending on which is closer. πq? always proceeds to region
a. n?2 always proceeds to region b. This agrees with the definition in Equation 9. The robot can start
anywhere on the s axis but must always start at automata state q0. Following πφ1 , the robot will first
reach region a or b (whichever is nearer), and then aim for the other region which in turn satisfies φ.
The states that have stay as their action are either goal regions (states (-2, q0), (3, q1), etc) where
a transition on q happens or states that are never reached (states (-3, q1), (-4, q2), etc) because a
transition on q occurs before they can be reached.
To illustrate automata guided task composition described in Example 2, instead of learning the
task described by φ from scratch, we can simply learn policy πφ2 for the added requirement φ2 =
b U a. We use the same learning setup and the resultant optimal policy is depicted in Figure 4
. It can be observed that πφ2 tries to reach a while avoiding b. This behavior agrees with the
specification φ2 and its FSA provided in Figure 2 . The action at s = 4 is stay because in order for
the robot to reach a it has to pass through b, therefore it prefers to obtain a low reward over violating
the task.
Having learned policies πφ1 and πφ2 , we can now use Equation 15 to construct policy πφ1∧φ2 . The
resulting policy for πφ1∧φ2 is illustrated in Figure 2 (upper right). This policy guides the robot to
first reach a (except for state s = 4) and then go to b which agrees with the specification.
Looking at Figure 1 ,theFSAof φ = φ1∧φ2 have two options policies ∏φ(∙, qo) and ∏φ(∙, qι)2 (trap
state and qf are terminal states which don’t have options). State q1 has only one outgoing edge with
the guarding predicate ψq1,qf : b, which means ∏φ(∙, qι) = ∏ψ1 (∙, q2)(they have the same guarding
predicate). Policy ∏φ(∙, qo) is a switching policy between ∏φι (∙,qo) and ∏φ2 (∙, qo). Figure 2 (lower
2∏φ(∙,qi) is the options policy of ∏φ at automata state qi (definiton in Equation (9)). Writing in this form
prevents cluttered subscripts
7
Under review as a conference paper at ICLR 2018
Figure 3 : (left): Baxter simulation Environment with three square regions (black,red, blue), two circular
regions (red, blue), two boxes (red, blue) that the robot can manipulate and an interactive ball that the user can
place anywhere on the table. Tasks are specified using these elements in Appendix A. (upper right): Learning
curve for task φ1 over 5 random seeds. (lower right): Policy deployment success rate
left) shows the robustness comparison at each state. The policy with lower robustness is chosen
following Equation (15). We can see that the robustness of both policies are the same from s = -5
to s = 0. And their policies agree in this range (Figures 3 and4 ). As s becomes larger, disagreement
emerge because ∏φι (∙,qo) wants to stay closer to b but ∏φ2 (∙, qo) wants otherwise. To maximize the
robustness of their conjunction, the decisions of π°? (∙, qo) are chosen for states s > 0.
6.2	Simulated Baxter
In this section, we construct a set of more complicated tasks that require temporal reasoning and
evaluate the proposed techniques on a simulated Baxter robot. The environment is shown in Figure 3
(left). In front of the robot are three square regions and two circular regions. An object with planar
coordinates p = (x, y) can use predicates Sred(p), Sblue(p), Sblack(p), Cred(p), Cblue(p) to evaluate
whether or not it is within the each region. The predicates are defined by S : (xmin < x < xmax) ∧
(ymin < y < ymax) and C
dist((x, y), (x, y)center) < r. (xmin, ymin) and (xmax,
ymax ) are the
boundary coordinates of the square region, (x, y)center and r are the center and radius of the circular
region. There are also two boxes which planar positions are denoted as predbox = (x, y)redbox and
pbluebox = (x, y)bluebox. And lastly there is an interactive ball that a user can move in space which
2D coordinate is denoted as psphere = (x, y)sphere (all objects move in the table plane).
We design seven tasks each specified by a scTLTL formula. The task specifications and their En-
glish translations are provided in Appendix A. Throughout the experiments in this section, we use
proximal policy search Schulman et al. (2017) as the policy optimization method. The hyperparam-
eters are kept fixed across the experiments and are listed in Appendix B. The policy is a Gaussian
distribution parameterized by a feed-forward neural network with 2 hidden layers, each layer has 64
relu units. The state and action spaces vary across tasks and comparison cases, and are described in
Appendix C.
We use the first task φ1 to evaluate the learning outcome using the FSA augmented MDP. As com-
parisons, We design two other rewards structures. The first is to use the robustness p(so：T, φ) as
the terminal reward for each episode and zero everywhere else, the second is a heuristic reward that
aims to align with φ1. The heuristic reward consists ofa state that keeps track of whether the sphere
is in a region and a set of quadratic distance functions. For φ1, the heuristic reward is
rφ1
-dist(predbox , predsquarecenter)
-dist(predbox , Pblack square center)
psphere is in red circle
otherwise.
Heuristic rewards for other tasks are defined in a similar manner and are not presented explicitly.
(16)
8
Under review as a conference paper at ICLR 2018
Figure 4 : (left): Learning curves for tasks φ6 and φ7 (task definitions provided in Appendix A). (right): Policy
deployment success rate for tasks φ6 and φ7
The results are illustrated in Figure 3 (right). The upper right plot shows the average robustness over
training iterations. Robustness is chosen as the comparison metric for its semantic rigor (robustness
greater than zero satisfies the task specification). The reported values are averaged over 60 episodes
and the plot shows the mean and 2 standard deviations over 5 random seeds. From the plot we can
observe that the FSA augmented MDP and the terminal robustness reward performed comparatively
in terms of convergence rate, whereas the heuristic reward fails to learn the task. The FSA augmented
MDP also learns a policy with lower variance in final performance.
We deploy the learned policy on the robot in simulation and record the task success rate. For each
of the three cases, we deploy the 5 policies learned from 5 random seeds on the robot and perform
10 sets of tests with randomly initialized states resulting in 50 test trials for each case. The average
success rate is presented in Figure 3 (lower right). From the results we can see that the FSA aug-
mented MDP is able to achieve the highest rate of success and this advantage over the robustness
reward is due to the low variance of its final policy.
To evaluate the policy switching technique for skill composition, we first learn four relatively simple
policies πφ2, πφ3 , πφ4, πφ5 using the FSA augmented MDP. Then we construct πφ6 = πφ2∧φ3 and
πφ7 = πφ2 ∧φ3 ∧φ4 ∧φ4 using Equation (15) (It is worth mentioning that the policies learned by the
robustness and heuristic rewards do not have an automaton state in them, therefore the skill compo-
sition technique does not apply). We deploy πφ6 and πφ7 on tasks 6 and 7 for 10 trials and record the
average robustness of the resulting trajectories. As comparisons, we also learn tasks 6 and 7 from
scratch using terminal robustness rewards and heuristic rewards, the results are presented in Figure 4
. We can observe from the plots that as the complexity of the tasks increase, using the robustness
and heuristic rewards fail to learn a policy that satisfies the specifications while the constructed pol-
icy can reliably achieve a robustness of greater than zero. We perform the same deployment test as
previously described and looking at Figure 4 (right) we can see that for both tasks 6 and 7, only the
policies constructed by skill composition are able to consistently complete the tasks.
7	Conclusion
In this paper, we proposed the FSA augmented MDP, a product MDP that enables effective learning
of hierarchical policies using any RL algorithm for tasks specified by scTLTL. We also introduced
automata guided skill composition, a technique that combines existing skills to create new skills
without additional learning. We show in robotic simulations that using the proposed methods we
enable simple policies to perform logically complex tasks.
Limitations of the current framework include discontinuity at the point of switching (for Equa-
tion (15)), which makes this method suitable for high level decision tasks but not for low level
control tasks. The technique only compares robustness at the current step and chooses to follow
a sub-policy for one time-step, making the switching policy short-sighted and may miss long term
opportunities. One way to address this is to impose a termination condition for following each sub-
9
Under review as a conference paper at ICLR 2018
policy and terminate only when the condition is triggered (as in the original options framework).
This termination condition can be hand designed or learned
References
Derya Aksaray, Austin Jones, Zhaodan Kong, Mac Schwager, and Calin Belta. Q-learning for robust
satisfaction of signal temporal logic specifications. In Decision and Control (CDC), 2016 IEEE
55th Conference on, pp. 6565-6570. IEEE, 2016.
Christel Baier and JooSt-Pieter Katoen. PrinCiPleS Of Model CheCking, volume 950. 2008.
Calin Belta, Boyan Yordanov, and Ebru Aydin Gol. Formal methods for discrete-time dynamical
systems, 2017.
Yushan Chen, Kun Deng, and Calin Belta. Multi-agent persistent monitoring in stochastic environ-
ments with temporal logic constraints. In DeCiSion and Control (CDC), 2012 IEEE 51st Annual
Conference on, pp. 2801-2806. IEEE, 2012
Marco Da Silva, Fredo Durand, and Jovan Popovic. Linear bellman combination for control of
character animation. ACm transactions on graphics (tog), 28(3):82, 2009.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. arXiv PrePrint arXiv:1610.00633,
2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. arXiv PrePrint arXiv:1702.08165, 2017.
Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver.
Learning and transfer of modulated locomotor controllers. arXiv PrePrint arXiv:1610.05182,
2016.
David Isele, Akansel Cosgun, Kaushik Subramanian, and Kikuo Fujimura. Navigating Intersections
with Autonomous Vehicles using Deep Reinforcement Learning. may 2017. URL http://
arxiv.org/abs/1705.01196.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
PrePrintarXiV:1611.05397, 2016.
Eric Jang, Google Brain, Sudheendra Vijayanarasimhan, Peter Pastor, Julian Ibarz, and Sergey
Levine. End-to-End Learning of Semantic Grasping. URL https://arxiv.org/pdf/
1707.01932.pdf.
Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. Hierarchical
Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. 2016.
URL http://arxiv.org/abs/1604.06057.
Orna Kupferman and Moshe Y Vardi. Model checking of safety properties. Formal MethOdS in
SyStem Design, 19(3):291-314, 2001.
Kevin Leahy, Austin Jones, Mac Schwager, and Calin Belta. Distributed information gathering poli-
cies under temporal logic constraints. In DeCiSiOn and Control (CDC), 2015 IEEE 54th AnnUal
COnference on, pp. 6803-6808. IEEE, 2015.
Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning Hand-Eye Coordi-
nation for Robotic Grasping with Deep Learning and Large-Scale Data Collection. arXiv, 2016.
doi: 10.1145/2835776.2835844. URL http://arxiv.org/abs/1603.02199v1.
Xiao Li, Cristian-Ioan Vasile, and Calin Belta. Reinforcement learning with temporal logic rewards.
arXiv PrePrint arXiv:1612.03471, 2016.
10
Under review as a conference paper at ICLR 2018
Alexis C Madrigal. Inside Waymo’s Secret World for Training Self-Driving Cars,
2017. URL https://www.theatlantic.com/technology/archive/2017/08/
inside-waymos-secret-testing-and-simulation-facilities/537648/.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015. doi:10.1038/nature14236.
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-Shot Task Generalization
with Multi-Task Deep Reinforcement Learning. 2017.
Sadra Sadraddini and Calin Belta. Robust Temporal Logic Model Predictive Control. 53rd Annual
Conference on Communication, ControL and CompUting (Allerton), 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv Preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, and Koray Kavukcuoglu. Mastering the game of Go with deep neural networks and tree
search. Nature, 529(7585):484^89, 2016. ISSN 0028-0836. doi: 10.1038∕nature16961. URL
http://dx.doi.org/l0.1038/nature16961.
S. Singh, A.G. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. 18thAnnual
COnferenCe on Neural InfOrmatiOn PrOCessing Systems (NIPS), 17(2):1281-1288, 2004. ISSN
1943-0604. doi:10.1109/TAMD.2010.2051031.
Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and Semi-MDPs: Learning,
Planning, and Representing Knowledge at Multiple Temporal Scales. ArtifiCial Intelligence, 1
(98-74):1-39, 1998.
Haoran Tang and Tuomas Haarnoja. Learning Diverse Skills via Maximum Entropy Deep
Reinforcement Learning. URL http://bair.berkeley.edu/blog/2017/10/06/
soft-q-learning/.
Emanuel Todorov. Compositionality of optimal control laws. In AdvanCeS in NeUral InfOrmatiOn
PrOCeSSing Systems, pp. 1856-1864, 2009.
A Ulusoy. Ltl optimal multi-agent planner (lomap). GithUb repository, 2017.
Cristian-Ioan Vasile, Jana Tumova, Sertac Karaman, Calin Belta, and Daniela Rus. Minimum-
violation scltl motion planning for mobility-on-demand. In Robotics and AUtOmatiOn (ICRA),
2017 IEEE International COnferenCe on, pp. 1481-1488.IEEE, 2017.
Christopher John Cornish Hellaby Watkins. Learning From DeIayed Rewards. PhD thesis, King,s
College, Cambridge, England, 1989.
11
Under review as a conference paper at ICLR 2018
Appendix
A Task S pecifications
Task
scTLTL Formula
English Description
φ1
(Cred(Pball) -→ ♦Sred(predbox)') ∧
Cred (pball )	→ ♦Sblack (predbox )
φ2
(Cred(pball )	→ ♦Sred(predbox ) ∧
(—(Cred(Pball) ∨ (Sback (Pball)) -→ QSblack (Predbox)
φ	Cblue (pball ) → ♦Sblue (pbluebox ) ∧
(—(Cred(Pball ) ∨ (Sback (Pball)) -→ ♦ S black Cpbluebox))
φ4	Sblack (Pball ) → ♦Sblue (Predbox )
φ5	Sblack (Pball ) → ♦Sred (Pbluebox )
φ6	φ2 ∧ φ3
φ7	φ2 ∧ φ3 ∧ φ4 ∧ φ5
If ball in red circle,
then red box eventually in
red square. Otherwise red box
eventually in black square
If ball in red circle,
then red box eventually in
red square. If ball is
not in red circle or
black square, then red box
eventually in black square
If ball in blue circle,
then blue box eventually in
blue square. If red ball
is not in blue circle
or black square, then blue
box eventually in black square
If ball in black square,
then eventually red box
in blue square
If ball in black square,
then eventually blue box
in red square
Conjunction of task 2 and 3
Conjunction of tasks 2, 3, 4, 5
B Hyperparameters For Proximal Policy Optimization
Hyperparameter	Value
Num. Hidden Layers	2
Num. Units per layer	64
Activation	Relu
Policy Learning Rate	0.009
Value Learning Rate	0.009
Discount	0.99
Batch Size	60
GAE parameter	0.99
Num. Iterations	100
Num. Epochs	20
Clipping Parameter	0.2
Horizon	20
12
Under review as a conference paper at ICLR 2018
C State And Action Spaces
For experiments with the simulated Baxter robot, we delegate low level control to motion planning
packages and only learn high level decisions. Depending on the task, the states are the planar
positions of objects (red box, blue box, ball) and the automata state. The actions are the target
positions of the objects. We assume that the low level controller can take objects to the desired
target position with minor uncertainty that will be dealt with by the learning agent. The table below
shows state and action spaces used for each task. sM and aM denote the spaces for regular MDP
(used for terminal robustness rewards and heuristic rewards). SM and aʌ^ denote the spaces for
FSA augmented MDP. q denotes the automata state.
Task	State Space	Action Space
φ	sM = (pball , predbox ) s	SM = (Pball ,predbox,q') φ	sM = (pball , predbox ) SM = (Pball, predbox, q) φ	SM = (pball , pbluebox) SM = (PbalI ,Pbluebox ,q) φ	SM = (Pball , Predbox) s	SM = (pball ,predbox,q') φ	SM = (Pball , Pbluebox) SM = (pball, pbluebox, q) φ	SM = (Pball , Predbox, Pbluebox) SM = (pball, predbox, pbluebox, qφ2 , qφ3 ) φ	SM = (Pball , Predbox, Pbluebox) SM = (pball, Predbox, Pbluebox, qφ2 , qφ3 , qφ4 , qφ5 )	aM/MM = (Predbox) target aM/M = (Predbox) target aM/MM = (Pbluebox) target aM/MM = (Predbox) target aM/MM = (PbIuebox) target aM/MM = (P，d) target aM/MM = (P，d) target
For tasks φ6 and φ7, the action space is three dimensional, the first two dimension p = (x, y) is a
target position, the third dimension d controls which object should be placed at p. If d < 0.5, then
p = predbox and if d > 0.5, then p = pbluebox .
13
Under review as a conference paper at ICLR 2018
D Learning With FSA Augmented MDP (off-policy version)
Algorithm 1 Automata Guided RL (off-policy version)
1: Inputs: Episode horizon T, Mφ (consisting of an MDP and FSA Aφ), maximum size for replay pool N
2: Initialize parameterized policy π θ	. θ is the policy parameters
3: Initialize replay pool B — {}
4: for n = 1 to number of training episodes do
5:	Select initial state ⅞ = (so, q0)	. so can be randomly selected, qo is always the initial
automaton state q0
6:	for t=0 toT do at = π(st)
7:	st+ι = GetNextState(3t, at)
8:	if qt+1 == qf then
9:	rt = 2	. terminal reward for satisfying φ
10:	break	. φ is satisfied, restart episode
11:	else if qt+1 == trap then
12:	rt = -2	. terminal reward for violating φ
13:	break	. φ is violated, restart episode
14:	else
15:	rt = GetReWard(3t, st+ι)	. using Equation 7
16:
17:	end if
18:	B — (st, at, st+ι,rt)	. store experience in replay pool
19:	if size(B) > N then
20:	pop(B[0])
21:	end if
22:	θ — UpdatePolicy(B) . this can be any RL update rule and doesn’t necessarily have to
occur at this location
23:	end for
24: end for
14
Under review as a conference paper at ICLR 2018
E Semantics for scTLTL
Following the syntax for scTLTL provided in Section 2.2, here we define the semantics for the
language. We denote st ∈ S to be the state at time t, and st:t+k to be a sequence of states (state
trajectory) from time t to t + k, i.e., st:t+k = stst+1...st+k. The Boolean semantics of scTLTL is
defined as:
St:t+k	|= f(S) < c	⇔	f(St) < c,
St:t+k	=-φ	⇔	-(St：t+k = O),
St:t+k	|= φ⇒ ψ	⇔	(St:t+k |= O) ⇒ (St:t+k |= ψ),
St:t+k	|= φ∧ ψ	⇔	(St:t+k |= O) ∧ (St:t+k |= ψ),
St:t+k	|= φ∨ ψ	⇔	(St:t+k |= O) ∨ (St:t+k |= ψ),
St:t+k	|=	φ	⇔	(St+1:t+k |= O) ∧ (k > 0),
St:t+k	=♦O	⇔	∃t0 ∈ [t, t + k) St0:t+k |= O,
St:t+k	|= φ U ψ	⇔	∃t0 ∈ [t,t+ k) S.t. St0:t+k |= ψ
			∧ (∀t00 ∈ [t, t0) St00:t0 |= O),
St:t+k	|= O T ψ	⇔	∃t0 ∈ [t,t+ k) S.t. St0:t+k |= ψ
			∧ (∃t00 ∈ [t, t0) St00:t0 |= O).
A trajectory s of horizon T is said to satisfy formula φ if s0:T |= φ.
We also define the quantitative semantics for scTLTL (robustness degree) , i.e., a real-valued function
p(st：t+k, φ) of state trajectory st：t+k and a ScTLTL specification φ that indicates how far st：t+k is
from satisfying or violating the specification φ. The quantitative semantics of scTLTL is defined as
follows:
p(st:t+k, T)
P(St：t+k,f(St) < C)
P(St：t+k, -O)
P(St：t+k,Φ ⇒ Ψ)
P(St:t+k, φ1 ∧ φ2)
P(St:t+k, φ1 ∨ φ2)
P(St:t+k,	φ)
P(St:t+k, Qφ)
P(St:t+k, φU ψ)
P(St:t+k, φT ψ)
Pmax,
c - f(St),
- P(St:t+k, φ),
max(-P(St:t+k, φ), P(St:t+k, ψ))
min(p(St：t+k ,φι ),ρ(Sft+k ,Φ2)),
max(p(St：t+k ,φι),ρ(St.t+k ,φ2)),
P(St+1：t+k ,φ) (k > O),
max (P(St0:t+k, φ)),
t0∈[t,t+k)
max (min(P(St0:t+k, ψ),
t0∈[t,t+k)
min P(St00:t0, φ))),
t00∈[t,t0)
max (min(P(St0:t+k, ψ),
t0∈[t,t+k)
max P(St00:t0, φ))),
t00∈[t,t0)
where Pmax represents the maximum robustness value. Moreover, P(St:t+k, φ) > 0 ⇒ St:t+k |= φ
and P(St:t+k, φ) < 0 ⇒ St:t+k 6|= φ, which implies that the robustness degree can substitute
Boolean semantics in order to enforce the specification φ (refer to Li et al. (2016) for a more detailed
description of TLTL and robustness).
15