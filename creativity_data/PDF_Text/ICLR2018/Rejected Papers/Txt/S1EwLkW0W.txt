Under review as a conference paper at ICLR 2018
Dissecting Adam: The Sign, Magnitude and
Variance of Stochastic Gradients
Anonymous authors
Paper under double-blind review
Ab stract
The adam optimizer is exceedingly popular in the deep learning community. Of-
ten it works very well, sometimes it doesn’t. Why? We interpret adam as a
combination of two aspects: for each weight, the update direction is determined
by the sign of the stochastic gradient, whereas the update magnitude is solely de-
termined by an estimate of its relative variance. We disentangle these two aspects
and analyze them in isolation, shedding light on adam’s inner workings. Trans-
ferring the “variance adaptation” to momentum-s g d gives rise to a novel method,
completing the practitioner’s toolbox for problems where adam fails.
1 Introduction
Many prominent machine learning models pose empirical risk minimization problems of the form
min L(θ) = ɪ X '(θ; Xk), with gradient VL(θ)
θ∈Rd	M
k=1
1M
M ∑v'(θ; xk),	(1)
where θ ∈ Rd is a vector of parameters, {x1, . . . , xM} a training set, and `(θ; x) is a loss quantifying
the performance of parameter vector θ on example x. Computing the exact gradient in each step of
an iterative optimization algorithm becomes inefficient for large M. Instead, we construct a mini-
batch B ⊂ {1, . . . , M} of |B| M data points sampled uniformly and independently from the
training set and compute an approximate stochastic gradient
g(0)=|B| x v`(θ; Xk),
|B| k∈B
(2)
which is an unbiased estimate, E[g(θ)] = VL(θ). We will denote by σ(θ)i2 := var[g(θ)i] its
element-wise variances.1
The basic stochastic optimizer is stochastic gradient descent (sgd, Robbins & Monro, 1951) and
its momentum variants (Polyak, 1964; Nesterov, 1983). A number of methods, widely-used in the
deep learning community, choose per-element update magnitudes based on the history of stochastic
gradient observations. Among these are adagrad (Duchi et al., 2011), rmsprop (Tieleman &
Hinton, 2012), adadelta (Zeiler, 2012) and adam (Kingma & Ba, 2015).
1.1 A Closer Look at Adam
We start out from a reinterpretation of the widely-used adam optimizer. Some of the considerations
naturally extend to adam’s close relatives rmsprop and adadelta, but we restrict our attention to
adam to keep the presentation concise. adam maintains moving averages of the observed stochas-
1 With k 〜 U({1,...,M}), the gradient V'k(θ) := V'(θ,Xk) is a random variable with mean
E[V'k(θ)] = VL(θ) and variances var[V'k(θ)i] = MT PMM=1 (V'k，(θ)i — VL(θ)i)2. If the elements
of B are drawn iid with replacement, the variance of g(θ) scales inversely with |B|: σ(θ)i2 := var[g(θ)i] =
∣B∣-1var[V'k(θ)i]. This holds approximately for sampling without replacement if |B| y M.
1
Under review as a conference paper at ICLR 2018
Figure 1: Conceptual sketch of variance adaptation, ignoring the sign aspect of adam. The left panel
shows the true gradient VL = (2,1) and stochastic gradients scattered around it with (σι, σ2)=
(1, 1.5). In the right panel, we employ a variance adaptation (to be derived in §3.2) that scales the
i-th coordinate by (1 + ηi2)-1. In this example, the θ2-coordinate has much higher relative variance
(η22 = 2.25) than the θ1-coordinate (η12 = 0.25) and is thus shortened. This reduces the variance of
the update direction at the expense of biasing it away from the true gradient in expectation.
tic gradients and their element-wise square2,
m t = βιmm t-i + (1 — βι)gt,
Vt = β2Vt-1 + (1 — β2)g2,
mt = (1 — β1 )-1m t,	(3)
Vt = (1 — β2 )-1Vt.	(4)
Here, mt and Vt are “bias-corrected” versions of the exponential moving averages to obtain convex
combinations of past observed (squared) gradients. adam then updates
θt+1 = θt — α
mt
√vt + ε
(5)
with a small constant ε > 0 guaranteeing numerical stability of this division. Ignoring ε and assum-
ing |mt,i| > 0 for the moment, we can rewrite the update direction as3
mt	sign(mt)|mt|	sign(mt)
sign(mt)
/1 I vt-m2
V1 + FF
(6)
Since mt and Vt approximate the first and second moment of the stochastic gradient gt, respectively,
Vt — mt2 can be seen as an estimate of element-wise stochastic gradient variances. The division by
the non-central second moment effectively removes the magnitude ofmt; it only appears in the ratio
(Vt — mt2)/mt2. Hence, ADAM can be interpreted as a combination of the two following aspects:
•	The update direction (±) for the i-th weight is given by the sign of mt,i .
•	The update magnitude for the i-th weight is uniquely determined by the global step size α
and an estimate of the relative variance,
η2,i
2
vt,i- m2,i
~2.
mt2,i
: ηt2,i
(7)
-1/2
Specifically, the update in the i-th coordinate is scaled by(1 + η2,J	, shortening steps
in high-relative-variance coordinates. Fig. 1 shows a sketch of this variance adaptation.
2 Notation: Divisions, squares, etc. on vectors are to be understood element-wise. denotes element-wise
multiplication. We occasionally drop θ, writing g instead of g(θ), etc. We use the shorthands VLt, gt, σ2
etc. for sequences θt and double-indices, e.g. gt,i = g(θt)i, to denote vector elements.
3 For convenience, we define sign(x) = 1 for x ≥ 0 and sign(x) = -1, x < 0, for our theoretical
considerations, but use sign(0) = 0 in practice. Application to vectors is to be understood element-wise.
2
Under review as a conference paper at ICLR 2018
Table 1: The methods under consideration in this paper.
	Sign + Magnitude	Sign
Not Variance-Adapted	SGD	SSD “Stochastic Sign Descent”
Variance-Adapted	SVAG	ADAM “Stochastic Variance-Adapted Gradient”
1.2	Overview
Both aspects of adam—taking the sign and variance adaptation—are briefly mentioned in Kingma
& Ba (2015), who note that “[t]he effective stepsize [...] is also invariant to the scale of the gradients”
and refer to mt/√vt as a “signal-to-noise ratio”. The purpose of this paper is to disentangle these
two intertwined aspects in order to discuss and analyze them in isolation.
This perspective naturally suggests two alternative methods by incorporating one of the aspects but
not the other (see Table 1). Taking the sign of the stochastic gradient (or momentum term) without
any further modification gives rise to “Stochastic Sign Descent” (ssd). On the other hand, “Stochas-
tic Variance-Adapted Gradient” (svag) applies element-wise variance adaptation factors directly on
the stochastic gradient (or momentum term) instead of on its sign. We proceed as follows: In Sec-
tion 2, we investigate the sign aspect. In the simplified setting of stochastic quadratic problems, we
derive conditions under which the element-wise sign of a stochastic gradient can be a better update
direction than the stochastic gradient itself. Section 3 discusses the variance adaptation. We present
a principled derivation of “optimal” element-wise variance adaptation factors for a stochastic gra-
dient as well as its sign. Subsequently, we incorporate momentum and briefly discuss the practical
estimation of stochastic gradient variance. Section 4 presents some experimental results.
1.3	Related Work
The idea of using the sign of the gradient as the principal source of the optimizer update has already
received some attention in the literature. The rprop algorithm (Riedmiller & Braun, 1993) ignores
the magnitude of the gradient and dynamically adapts the per-element magnitude of the update based
on observed sign changes. With the goal of reducing communication cost in distributed training of
neural networks, Seide et al. (2014) empirically investigate the use of the sign of stochastic gradients.
Regarding the variance adaptation, Schaul et al. (2013) derive element-wise step sizes for stochastic
gradient descent that have (among other factors) a dependency on the stochastic gradient variance.
1.4	The Sign of a Stochastic Gradient
We briefly establish a fact that will be used throughout the paper. The sign of a stochastic gradient
s(θ) = sign(g(θ)) estimates the sign of the true gradient. Its distribution (and thus the quality of this
estimate) is fully characterized by the success probabilities Pi := P [s(θ)i = sign(VL(θ)i)]. These
depend on the distribution of the stochastic gradient. If we assume g(θ) to be Gaussian—which is
strongly supported by a Central Limit Theorem argument on Eq. (2)—we have
Pi := P [s(θ)i = Sign(VL(θ)i)] = 1 + 1 erf (√≡^) ,	(8)
2	2	2σ(θ)i
see §B.2 in the supplements. Furthermore, it is E[s(θ)i] = (2Pi - 1) sign(VL(θ)i).
2	Why the Sign?
Can it make sense to ignore the gradient magnitude? We provide some intuition under which circum-
stances the element-wise sign of a stochastic gradient is a better update direction than the stochastic
gradient itself. This question is difficult to tackle in general, which is why we restrict the problem
3
Under review as a conference paper at ICLR 2018
class to the simple, yet insightful, case of stochastic quadratic problems, where we can investigate
the effects of curvature properties and its interaction with stochastic noise.
Model Problem (Stochastic Quadratic Problem (QP)). Consider the loss function `(θ, x) =
0.5 (θ - x)T Q(θ - x) with a symmetric positive definite matrix Q ∈ Rd and “data” coming from
the distribution X 〜N (x*,ν2I) .It is
L(θ) ：= Eχ['(θ, x)] = 1(θ - x*)TQ(θ - x*) + ν22 tr(Q),	(9)
with VL(θ) = Q(θ 一 x*). Stochastic gradients are given by g(θ) = Q(θ 一 x)〜N(x*, V2I).
2.1	Theoretical Comparison
We want to compare update directions on stochastic QPs in terms of their expected decrease in
function value from a single update step. If we update from θ to θ + αz, we have
2
E[L(θ + αz)] = L(θ) + αVL(θ)T E[z] + 5 E[zτ Qz].
(10)
For this comparison of update directions, we allow for the optimal step size that minimizes Eq. (10),
which is easily found to be α* = -VL(θ)T E[z]/E[zT Qz] and yields an expected improvement of
I(z):= ∣E[L(θ + α*z)] -L(θ)∣
(VL(θ)τ E[z])2
2E[zτ Qz]
(11)
We find the following expressions/bounds for the improvement of sgd and ssd:
I(g)
1	(VL(θ)τ VL(θ))2
2 VL(θ)τQVL(θ) + V2 Pd=1 λ
I(S) ≥ 1 (P3(2ρi-I)IVL(冽2
2	Pd,j=ι∣%j∣
(12)
where the λi ∈ R+ are the eigenvalues of Q with orthonormal eigenvectors vi ∈ Rd . Derivations
can be found in §B.1 of the supplements. Comparing these expressions, we make two observations.
Firstly, I(s) has a dependency on Pi,j |qij |. This quantity relates to the eigenvalues, as well as
the orientation of the eigenbasis of Q. By writing Q in its eigendecomposition one finds that
Pi,j |qij | ≤ Pi λi kvi k21. If the eigenvectors are perfectly axis-aligned (diagonal Q), their 1-norms
are IlvikI = IlviIl2 = 1. It is intuitive that this is the best case for the intrinsically axis-aligned sign
update. In general, the 1-norm is only bounded by kvikι ≤ √d∣∣vi∣∣2 = √d, suggesting that the
sign update will have difficulties with arbitrarily oriented eigenbases. We can alternatively express
this matter in terms of “diagonal dominance”. Assuming Q has a percentage c ∈ [0, 1] of its “mass”
on the diagonal, i.e., Pi |qii| ≥ c Pi,j |qij|, we can write
1 (Pd=ι(2ρi - 1)∣VL(θ)i∣)2 _ 1 (Pd=ι(2ρi - 1)∣VL(θ)i∣)2
I(S) ≥ 2	CT Pd=ι “I	= 2	CT Pd=ι λi
(13)
Becker & LeCun (1988) empirically investigated the diagonal dominance of Hessians in optimiza-
tion problems arising from neural networks and found relatively high percentages of mass on the
diagonals of C = 0.1 up to C = 0.6 for the problems they investigated.
Secondly, I(g) contains the constant offset V2 Pid=1 λi3 in the denominator, which can become
hugely obstructive for ill-conditioned and noisy problems. In I(S), on the other hand, there is no
such interaction between the magnitude of the noise and the eigenspectrum; the noise only manifests
in the element-wise success probabilities ρi, its effect in the denominator is bounded. A recent paper
(Chaudhari et al., 2016) investigated the eigenspectrum in deep learning problems and found it to be
very ill-conditioned with the majority of eigenvalues close to zero and a few very large ones.
In summary, we can expect the sign update to be beneficial for noisy, ill-conditioned problems with
“diagonally dominant” Hessians. There is some (weak) empirical evidence that these conditions
might be fulfilled in deep learning problems.
4
Under review as a conference paper at ICLR 2018
Eigenvalues
Noise-free
50
一 一 SGD
SSD
Steps
Figure 2: Performance of sgd and ssd on 100-dimensional stochastic quadratic problems. Rows
correspond to different QPs: the eigenspectrum is shown and each is used with a randomly rotated
and an axis-aligned eigenbasis. Columns correspond to different noise levels. Horizontal axis is
number of steps; vertical axis is log function value and is shared per row for comparability.
Steps
High noise
0	25	50	75	100
Steps
ROtated AXiS-aligned ROtated AXiS-aligned
2.2	Experimental Evaluation
We verify the above findings on artificially generated stochastic QPs, where all relevant quantities
are known analytically and controllable. We control the eigenspectrum by specifying a diagonal
matrix Λ of eigenvalues: (1) a mildly-conditioned problem with eigenvalues drawn uniformly from
[0.1, 1.1] and (2) an ill-conditioned problem with a structured eigenspectrum similar to the one re-
ported for neural networks by Chaudhari et al. (2016) by uniformly drawing 90% of the eigenvalues
from [0, 1] and 10% from [30, 60]. Q is then generated by (1) Q = Λ to produce an axis-aligned
problem and (2) Q = RΛRT with a rotation matrix R drawn uniformly at random (see Diaco-
nis & Shahshahani, 1987). This makes four different matrices, which we consider at noise levels
ν ∈ {0, 0.1, 4.0}. We compare sgd and ssd, both with the optimal step size as derived from
Eq. (10), which can be computed exactly in this setting.
Figure 2 shows the results, which confirm the theoretical findings. On the well-conditioned, noise-
free problem, gradient descent vastly outperforms the sign-based method. Surprisingly, adding even
a little noise almost evens out the difference in performance. The orientation of the eigenbasis had
little effect on the performance of ssd in the well-conditioned case. On the ill-conditioned problem,
the methods work roughly equally well when the eigenbasis is randomly rotated. As predicted, ssd
benefits drastically from an axis-aligned eigenbasis (last row), where it clearly outperforms sgd.
3	Variance-Based Element-Wise Step Size Adaptation
Besides the sign direction, the other defining property of adam are variance-based element-wise
step sizes. Considering the variance adaptation in isolation from the sign aspect naturally suggests
to employ it directly on the stochastic gradient, without taking the sign. In both cases, a motivation
arises from the following consideration:
Assume we want to update in a direction p ∈ Rd (or sign(p)), but only have access to an unbiased
estimate P ∈ Rd with E[p] = p. We allow for element-wise factors Y ∈ Rd, i.e., We update Y Θ P
or γ Θ Sign(P). One way to make “optimal" use of these factors is to choose them such as to
minimize the expected distance to the desired update direction. Using the squared Euclidean norm
as a distance measure, we find the following result.
5
Under review as a conference paper at ICLR 2018
Lemma 1. LetP ∈ Rd be a random variable with E[p] = P and var[pi] = σ2. Then
2	pi2	1
min E[kγ GlP — Pk2] IssoIvedby Yi = ɪ2 = —~~γΓ2	(14)
γ∈Rd	Pi + σ2	1 + σ2∕p2
and
min E[∣∣γ Θ Sign(P) — Sign(P)ki] issolVedby Yi = (2ρi — 1),	(15)
γ∈Rd
where Pi = P[sign(Pi) = Sign(Pi)].
In the sign case, Yi is proportional to the success probability with Yi = 1 if we are certain about the
sign (ρi = 1) and Yi = 0 if we have no information about the sign at all (ρi = .5).
3.1	Variance Adaptation for the Sign of a Stochastic Gradient
Applying Eq. (15) to P = g, the optimal variance adaptation factors for the sign of a stochastic
gradient are found to be Yi = 2ρi — 1, where Pi = P[sign(gi) = sign(VLi)]. Recall from Eq. (8)
that, under the Gaussian assumption, the success probabilities of the sign of a stochastic gradient
are 2ρi — 1 = erf[(√¾)-1]. ADAM uses the variance adaptation factors (1 + η2)-1/2, which turns
out to be a close approximation of erf[(√¾)-1], as shown in Figure 5 in the supplements. Hence,
adam can be regarded as an approximate realization of this optimal variance adaptation scheme.
We experimented with both variants and found them to have identical effects. The small difference
between them can be regarded as insignificant when η itself is subject to approximation error. We
thus stick to (1 + η2)-1/2 for accordance with ADAM and to avoid the (more costly) error function.
3.2	Stochastic Variance-Adapted Gradient (svag)
Applying Eq. (14) to P = g, the optimal variance adaptation factors for SGD are found to be
_	1	_	1
Yi= 1+ σi∕VLi = TT^.
This term is known from Schaul et al. (2013), where it appears together with diagonal curvature
estimates in element-wise step sizes for sgd. We refer to this method (without curvature estimates)
as “Stochastic Variance-Adapted Gradient” (svag). A momentum variant will be derived below.
Intriguingly, variance adaptation of this form guarantees convergence without manually decreasing
the global step size. We recover the O(1∕t) rate of sgd for smooth, strongly convex functions. We
emphasize that this result considers an “idealized” version of SVAG with exact ηi2. It is a motivation
for this form of variance adaptation, nota statement about the performance with estimated variances.
Theorem 1. Let f be μ-strongly convex and L-smooth. Assume we update θt+ι = θt — α(γt Θ gt),
where gt is a stochastic gradient with E[gt∣θt] = Vf (θt), var[gt,i∣θt] = σi,i, variance adaptation
factors Yt,i = (1 + σt2,i ∕Vft2,i)-1, and α = 1∕L. Assume E[kgtk2] ≤ G2. Then
E[f(θt) — f*] ∈O
(17)
where f* is the minimum value of f.
(Proof in §B.4)
3.3	Estimating Gradient Variance
In practice, the relative variance is of course not known and must be estimated. As noted in the
introduction, adam obtains an estimate of the stochastic gradient variance from moving averages,
σii ≈ St,i = vt,i — mi i. The underlying assumption is that the function does not change drastically
over the “effective time horizon” of the moving average, such that the recent gradients can approx-
imately be considered to be iid draws from the stochastic gradient distribution. An estimate of the
relative variance can then be obtained by (vt — mt2)∕(mt2), as in ADAM.
Unlike ADAM we do not use different moving average constants for mt and vt . The constant for
the moving average should define a time horizon over which the gradients can approximately be
6
Under review as a conference paper at ICLR 2018
considered to come from the same distribution. From this perspective, it is hardly justifiable to use
different horizons for the gradient and its square. Furthermore, we found individual moving average
constants for mt and vt to have only minor effect on the performance of our methods.
An alternative variance estimate can be computed locally “within” a single mini-batch. A more
detailed discussion of both estimators can be found in §C of the supplements. We have experimented
with both estimators and found them to work equally well for our purpose of variance adaptation.
We thus stick to moving average-based estimates for the main paper. Appendix D provides details
and experimental results for the mini-batch variant.
3.4	Incorporating Momentum
When we add momentum—i.e., we want to update in the direction rt or sign(rt) with a momentum
term r = μrt-ι + gt = P：=。μsgt-s—the variance adaptation factors should be determined by
the relative variance ofrt, according to Lemma 1. It is
t	tt
E[rt] = X μsVLt-s,	var[rt,i] = X(μs)2var[gt-s,i] = X μ2s σ2-s,i.	(18)
s=0	s=0	s=0
Replacing E[gt-s] ≈ mt-s and var[gt-s] ≈ vt-s - mt2-s we could compute these quantities.
However, this would require two additional moving averages and can thus be discarded as imprac-
tical. Fortunately, we can motivate an approximation that does not require any additional memory
requirements (see §C):
var[rt ]	vt - m2
1EF[F ≈ κ(μ,t) K
with
κ(μ, t):
(1- μ2t)(1- μ)2
(1 - μ2)(1 - μt)2 .
(19)
Note that the correction factor κ(μ,t) does not appear in ADAM, which updates in the direction
sign(mt) = sign(rt) but performs variance adaptation based on (vt - mt2)/mt2. The supplements
contain experiments with a variant of adam that includes this correction factor.
4 Experiments
We compare momentum-s g d (m-sgd) and adam to two new methods: First, we consider m-ssd:
stochastic sign descent using a momentum term. The second method is m- svag, i.e., sgd with
momentum and variance adaptation of the form (1 + η2)-1, where the relative variance of the
momentum term is estimated from moving averages according to Eq. (19). These four methods are
the four possible recombinations of the sign aspect and the variance adaptation aspect of adam,
as laid out in Table 1. Algorithms 1 and 2 provide pseudo-code for m-ssd and m-svag. For
all experiments, We use μ = 0.9 for m-sgd, M-SSD and M-SVAG and default parameters (βι =
0.9, β2 = 0.999, ε = 10-8) for adam. Note that m-svag does not use an ε-parameter, see Alg. 2.
Algorithm 1 m-ssd (Stochastic Sign Descent with Momentum)
Require: initial value θo, step size a, momentum parameter μ ∈ [0,1], number of steps T
1: Initialize m = 0, v= 0
2: for t = 1, . . . , T do
3:	Compute stochastic gradient g = g(θ)
4:	Update moving average m J μm + g
5:	Update θ J θ — α sign(m)
6: end for
7
Under review as a conference paper at ICLR 2018
Algorithm 2 m-svag (Stochastic Variance-Adapted Gradient with Momentum)
Require: initial value θo, step size a, momentum parameter μ ∈ [0,1], number of steps T
1:	Initialize Tm = 0, v = 0
2:	for t = 1, . . . , T do
3:	Compute stochastic gradient g = g(θ)
4:	Update moving averages m J μm + (1 — μ)g, V J μV+(1 一 μ)g2
5:	Bias-correct m =(1 一 μt)-1m, V = (1 一 μt)-1V
6:	Compute relative variance estimate η2 = κ(μ, t) Vm	. Eq. (19)
7:	Compute variance adaptation factors Y = (1 + η2)-1
8:	Update θ J θ — α(γ Θ m)
9:	end for
We do not use an ε-parameter as in ADAM. In the (rare) case that mi = 0 for coordinate i, the
division by zero in line 6 is caught and the update magnitude will be set to zero in line 8.
4.1	Experimental Set-Up
We tested all methods on three problems: a simple fully-connected neural network on the mnist
data set (LeCun et al., 1998), as well as convolutional neural networks (CNNs) on the cifar- 1 0
and cifar- 1 00 data sets (Krizhevsky, 2009). On cifar- 10, we used a simple CNN with three
convolutional layers, interspersed with max-pooling, and three fully-connected layers. On cifar-
100 we used the AllCNN architecture of Springenberg et al. (2014) with a total of nine convolutional
layers. A complete description of all network architectures has been moved to §A. While mnist and
cifar- 10 are trained with a constant global step size (α), we used a fixed decreasing schedule for
cifar- 100, dividing by 10 after 40k and 50k steps (adopted from Springenberg et al., 2014). We
used a batch size of 128 on mnist and 256 on the two cifar data sets.
Step sizes (initial step sizes in the case of cifar- 1 00) were tuned for each method individually by
first finding the maximal stable step size by trial and error, then searching downwards over two orders
of magnitude (details in §A). We selected the one that yielded maximal overall test accuracy within
the fixed number of training steps. Experiments with the best step size have been replicated ten
times with different random seeds and all performance indicators are reported as mean plus/minus
one standard deviation.
4.2	Results
Results are shown in Figure 3. On mnist, adam clearly outperforms m-sgd. Interestingly, there
is only a very small difference in performance between the two sign-based methods, m-ssd and
adam. Apparently, the advantage of adam over m-sgd on this problem is primarily due to the
sign aspect. Going from m-sgd to m-svag, gives a considerable boost in performance, but m-
svag is still outperformed by the two sign-based methods.
On cifar- 10, the sign-based methods again have superior performance. Neither m-ssd nor m-sgd
can benefit significantly from adding variance adaptation.
Finally, the situation is reversed on cifar- 1 00, where m-sgd outperforms adam. It attains lower
minimal loss values (both training and test) and converges faster. This is also reflected in the test
accuracies, where m-sgd beats adam by almost 10 percentage points. Furthermore, adam is much
less stable with significantly larger variance in performance. On this problem, variance adaptation
has a small but significant positive effect for the sign-based methods as well as for m-sgd. When
going from m-sgd to m-svag we gain some speed in the initial phase. The difference is later
evened out by the manual learning rate decrease (which was necessary, for all methods, to train this
architecture to satisfying performance).
5 Discussion and Conclusion
We have argued that adam combines two aspects: taking signs and variance adaptation. Our sepa-
rate analysis of both aspects provides some insight into the inner workings of this method.
8
Under review as a conference paper at ICLR 2018
CIFAR-10	CIFAR-100
MNIST
4 2 0 8 6 4
------
Illooo
SSoIMulseJI
4 2 0 8 6 4
------
Illooo
ssolsəɪ
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Steps(∙10-3)
SSoIMuɪsejɪ
4 2 0 8 6
-----
Illoo
4 2 0 8 6
-----
Illoo
ssolsəɪ
0	5	10	15	20	25
Steps(∙10-3)
0 5 0 5 0 5 0
-------
4 3 3 2 2 1 1
SSoIuɪuɪejɪ
5 0 5 0 5 0 5
-------
4 4 3 3 2 2 1
ssolsəɪ
0	10 20 30 40 50 60
Steps(∙10-3)
Test accuracies
m-sgd	92.0 ± 0.2%	M-SGD	84.5 ± 0.6%	M-SGD	67.8 ± 0.3%
ADAM	91.8 ± 0.3%	ADAM	84.8 ± 0.6%	ADAM	58.3 ± 1.3%
m-ssd	92.0 ± 0.2%	M-SSD	85.0 ± 0.4%	M-SSD	51.4 ± 4.9%
m-svag	91.6 ± 0.4%	M-SVAG	84.5 ± 0.8%	M-SVAG	67.7 ± 0.2%
Figure 3: Experimental results on the three test problems. Plots display training and test loss over
the number of steps. Curves for the different optimization methods are color-coded. The shaded area
spans plus/minus one standard deviation, obtained from ten replications. The table below contains
test accuracies evaluated after the last iteration.
Taking the sign can be beneficial, but does not need to be. Our theoretical analysis suggests that it
depends on the interplay of stochasticity, the conditioning of the problem, and its “axis-alignment”.
Our experiments confirm that sign-based methods work well on some, but not all problems.
Variance adaptation can be applied to any stochastic update direction. In our experiments it was
beneficial in all cases, but its effect can sometimes be minuscule. m-svag, a variance-adapted
variant of momentum-s gd, is a useful addition to the practitioner’s toolbox for problems where
sign-based methods like adam fail. Its memory and computation cost are identical to adam and it
has two hyper-parameters, the momentum constant μ and the global step size a. Our TensorFloW
(Abadi et al., 2015) implementation of this method will be made available upon publication.
Acknowledgments
We want to thank [names removed] for many helpful discussions.
References
Martin Abadi, AShiSh Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
9
Under review as a conference paper at ICLR 2018
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from
tensorflow.org.
Lukas Balles, Maren Mahsereci, and Philipp Hennig. Automizing stochastic optimization with
gradient variance estimates. In Automatic Machine Learning Workshop at ICML 2017, 2017a.
Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive batch sizes with learning rates.
In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence (UAI), pp.
410-419, 2017b.
Sue Becker and Yann LeCun. Improving the convergence of back-propagation learning with second
order methods. In Proceedings of the 1988 Connectionist Models Summer School, pp. 29-37,
1988.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, and Yann LeCun. Entropy-SGD: Biasing
gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.
Persi Diaconis and Mehrdad Shahshahani. The subgroup algorithm for generating uniform random
variables. Probability in the Engineering and Informational Sciences, 1(01):15-32, 1987.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Diederik Kingma and Jimmy Ba. ADAM: A method for stochastic optimization. The International
Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University
of Toronto, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Maren Mahsereci and Philipp Hennig. Probabilistic line searches for stochastic optimization. In
Advances in Neural Information Processing Systems 28, pp. 181-189, 2015.
Maren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without a
validation set. arXiv preprint arXiv:1703.09580, 2017.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1/k2). In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learn-
ing: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on, pp.
586-591. IEEE, 1993.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, pp. 400-407, 1951.
Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. In Proceedings of the
30th International Conference on Machine Learning (ICML), pp. 343-351, 2013.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech DNNs. In Fifteenth Annual Conference
of the International Speech Communication Association, 2014.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Tijmen Tieleman and Geoffrey Hinton. RMSPROP: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, Lecture 6.5, 2012.
Matthew D Zeiler. ADADELTA: An adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
10
Under review as a conference paper at ICLR 2018
Supplementary Material
A Description of Experiments
A.1 Network Architectures
MNIST We train a simple fully-connected neural network with three hidden layers of 1000, 500
and 100 units with ReLU activation. The output layer has 10 units with softmax activation. We use
the cross-entropy loss function and apply L2-regularization on all weights, but not the biases. We
use a batch size of 128. The global learning rate α stays constant.
CIFAR-10 The cifar- 1 0 data set consists of 32×32px RGB images with one of ten categorical
labels. We train a convolutional neural network (CNN) with three convolutional layers (64 filters
of size 5×5, 96 filters of size 3×3, and 128 filters of size 3×3) interspersed with max-pooling over
3×3 areas with stride 2. Two fully-connected layers with 512 and 256 units follow. We use ReLU
activation function for all layers. The output layer has 10 units for the 10 classes of cifar- 1 0
with softmax activation. We use the cross-entropy loss function and apply L2-regularization on all
weights, but not the biases. During training we perform some standard data augmentation operations
(random cropping of sub-images, left-right mirroring, color distortion) on the input images. We use
a batch size of 256. The global learning rate α stays constant.
CIFAR-100 We use the AllCNN architecture of Springenberg et al. (2014). It consists of seven
convolutional layers, some of them with stride, and no pooling layers. The fully-connected layers
are replaced with two layers of 1×1 convolutions with global spatial averaging in the end. ReLU
activation function is used in all layers. Details can be found in the original paper. We use the
cross-entropy loss function and apply L2-regularization on all weights, but not the biases. We used
the same data augmentation operations as for cifar- 1 0 and a batch size of 256. The global learning
rate α is decreased by a factor of 10 after 40k and 50k steps.
A.2 Learning Rate Tuning
Learning rates for each optimizer have been tuned by first finding the maximal stable learning rate
by trial and error and then searching downwards over two orders of magnitude with learning rates
6 ∙ 10m, 3 ∙ 10m, and 1 ∙ 10m for order of magnitude m. We evaluated loss and accuracy on the full
test set at a constant interval and selected the best-performing learning rate for each method in terms
of maximally reached test accuracy. Using the best learning rate, we replicated the experiment ten
times with different random seeds.
B Mathematical Details
B.1	Details of the Analysis on Stochastic QPs
We derive the expressions for I(s) and I(g) in Eq. (12). We drop the fixed θ from the notation for
readability. For SGD, we have E[g] = vL and E[gTQg] = VLTQVL + tr(Qcov[g]), which is a
general fact for quadratic forms of random variables. For the stochastic QP the gradient covariance is
cov[g] = ν2QQ, thus tr(Qcov[g]) = ν2 tr(QQQ) = ν2 Pi λi3. Plugging everything into Eq. (11)
yields
(VLT VL)2
VLTQVz+Τ2P=1λ3
(20)
For stochastic sign descent, we have E[si] = (2ρi - 1) sign(VLi) and thus VLT E[s]
Pid=1 VLiE[si] = Pi(2ρi - 1)|VLi |. Regarding the denominator, it is
d
0 ≤ sTHs = |sT Qs| =	qij si sj
i=1
dd
≤	|qij||si||sj| =	|qij|.
i=1	i=1
(21)
11
Under review as a conference paper at ICLR 2018
0.6
0.5
0.4
0.3
0.2
0.1
Figure 4: Probability density functions (pdf) of three Gaussian distributions, all with μ = 1, but
different variances σ2 = 0.5 (left), σ2 = 1.0 (middle), σ2 = 4.0 (right). The shaded area under the
curve corresponds to the probability that a sample from the distribution has the opposite sign than its
mean. For the Gaussian distribution, this probability is uniquely determined by the fraction σ∕∣μ∣,
as shown in Lemma 2.
0.6
0.5
0.4
仑
0.3
P
d
0.2
0.1
Plugging everything into Eq. (11) yields
(Pd=ι(2Pi-1)VLi∣)2
Z(S) ≥ ʌ-----7.........-
Pd=I |qj I
(22)
B.2	Success Probabilities of the Sign of a Stochastic Gradient
We have stated in the main text that the sign ofa stochastic gradient, s(θ) = sign(g(θ)), has success
probabilities
Pi = P[s(θ)i = Sign(VL(θ)i)] = 2 + 2 erf (%，1)	(23)
under the assumption that g 〜 N(VL, Σ). The following Lemma formally proves this statement
and Figure 4 provides a pictorial illustration.
Lemma 2. If X 〜N(μ, σ2) then
P = P[sign(X) = sign(μ)] = 2 (l + erf (7^)) ∙	(24)
Proof. The cumulative density function (Cdf) of X 〜N(μ,σ2) is P[X ≤ x] = Φ((x - μ)∕σ),
where Φ(z) = 0.5(1 + erf(z/√2)) is the cdf of the standard normal distribution. If μ < 0, then
P=P[X <0]
0 — μ
σ
1 + erf
(25)
Φ
If μ > 0, then
P = P[X > 0] = 1 - P[X ≤ 0] = 1 - Φ ( —ʃ-
(26)
where the last step used the anti-symmetry of the error function.
□
12
Under review as a conference paper at ICLR 2018
Figure 5: Variance adaptation factors as
functions of the relative standard deviation
η. (1 + η2)-1 is the optimal variance adap-
tation factor for sgd (Eq. 16). The optimal
factor for the sign of a stochastic gradient is
erf((√2η)-1) under the GaUSSian assump-
tion (Eq. 15). It is closely approximated by
(1 + η2 )-1/2 , which is the factor implicitly
employed by adam (Eq. 6).
(27)
(28)
2
(29)
(30)
□
B.3	Details on Variance Adaptation Factors
Proofof Lemma 1. Using E[pi] = Pi and E[p2] = p2 + σ2, We get
dd
E[kγ ΘP — Pk2] = X E[(YiPi- Pi)2] = X γi2E[p2] - 2YiPiE[pi] + P2
i=1	i=1
d
= X Yi2 (Pi2 + σi2) - 2YiPi2 + Pi2 .
i=1
Setting the derivative W.r.t. Yi to zero, We find the optimal choice
7	- P2
Yi = -2 ^^I	2 .
Pi2 + σi2
Using E[sign(Pi)] = (2ρi - 1)sign(Pi) and sign(∙)2 = 1, we get
d
E[kγ Θ Sign(P)- Sign(P) k2] = X γ2E[sign(Pi)2] - 2γi Sign(Pi)E[sign(Pi)] + Sign(Pi)
i=1
=Yi2 -2Yi(2ρi-1)+1
and easily find the optimal choice
Yi = 2ρi - 1.
by setting the derivative to zero.
See Figure 5 for a plot of the variance adaptation factors considered in this paper.
B.4 Convergence of Idealized Stochastic Variance-Adapted Gradient
We proof the convergence results for idealized variance-adapted stochastic gradient descent. We
have to clarify an aspect that we have glossed over in the main text. A stochastic optimizer gen-
erates a discrete stochastic process {θt}t∈No. We denote as Et[∙] = E[∙∣θo, ...,θt] the conditional
expectation given a realization of that process up to time step t. Recall that E[Et[∙]] = E[∙].
ProofofTheorem 1. Using the Lipschitz continuity of Vf, we can bound f (θ + ∆θ) ≤ f (θ) +
Vf(θ)T∆θ + LL∣∣∆θk2. Hence,
Et[ft+ι] ≤ ft - αEt[vfT(Yt θ gt)] +—2-Et[kYt θ gtk2]
1d	1 d
=ft - L X γt,ivft,iE[gt,i] + 2L X γ2,iEt [g2,i]	(31)
i=1	i=1
dd
=ft - L XYtNf2 + 2L Xγ2,i(Vft2,i + σ2,i).
i=1	i=1
13
Under review as a conference paper at ICLR 2018
Plugging in the definition
-f
Yt，i = f + σ2,i
and simplifying, we get
(32)
Et[ft+1] ≤ft -
1 3 V∕2i 2
2L X Vf2 + 吃 vft,i
(33)
Using Jensen’s inequality4
d
X
i=1
Vft2,i
Vf2 + 味
f kvM2X f
(Vf2 + σ2.
I f
≥ kvftk2 (X 高F vffσ2
=	kVftk4	≥ kVftk4
Pd=I(Vf2i + σ2,i) - G2	.
(34)
DUe to strong convexity, We have IlVftk2 ≥ 2μ(ft 一 f*) and can further bound
X	Vf2i	V C 4 4μ2(ft - f*)2
⅛ Vft2,i+σ2 fi≥	G2
(35)
Inserting this in (33) and subtracting f*, We get
Et [ft+1] - f* ≤ ft - f* - LG2 (ft - f *)2
and, consequently, by total expectation
E[ft+1 - f*] = E [Et[ft+1] - f*] ≤ E[ft - f*] - τE2 EKft- f*)2]
LG
≤ E[ft - f*] - τE[ E[ft - f*]2,
LG
(36)
(37)
Which We reWrite, using the shorthand et := E[ft - f*], as
0 ≤ et+1 ≤ et(1 - cet),
2μ2
LG2.
(38)
To conclude the proof, we will show that this implies et ∈ O(t). Without loss of generality, we
assume et+1 > 0 and get
et-+11 ≥ et-1(1 - cet)-1 ≥ et-1(1 + cet) = et-1 + c,	(39)
where the second step is due to the simple fact that (1 -x)-1 ≥ (1 +x) for any x ∈ [0, 1). Summing
this inequality over t = 0, . . . , T - 1 yields eT-1 ≥ e0-1 + Tc and, thus,
TeT ≤ (ɪ+ C)	T→∞ 1 < ∞,	(40)
Te0	c
which shows that et ∈ O(t).	□
4 Jensen’s inequality says that Pi ci φ(xi) ≥ φ(Pi cixi) for a convex function φ and convex coeffi-
cients ci ≥ 0, Pi ci = 1. Here, we apply it to the convex function φ(x) = 1/x, x > 0, and coefficients
Vf2,i∕kVftk2.
14
Under review as a conference paper at ICLR 2018
C More on Gradient Variance Estimation
C.1 Estimates from Moving Averages
Iterating the recursive formula for Tmt backwards, We get
mt =	Imi	=	iɪ^	(β1mt-1	+	(1	- β1)gt)	= ... =	1~β X βs gt-s .	(41)
1 - β1	1 - β1	1 - β1 s 0
Hence, mt is a weighted average of past observed gradients with coefficients c(β1, t, s) := β1s (1 -
βι)∕(1 - β1), which SUmto one, since P：=0 βS = (1 - βt )/(1 - βι) by the geometric sum formula.
The analogous statement holds for vt . The basic rationale that facilitates a variance estimate from
past gradient observation is to assume that the true gradient does not change drastically over the
effective time horizon of the exponential moving average. For mathematical simplicity, we can
translate this assumption to mean that, at the t-th step, we treat all {gt-s,i | s = 0, . . . , t - 1} as iid
with mean VLt,i and variance σt %. This will of course be utterly wrong for gradient observations
that are far in the past, but since c(μ,t, S) is very small for large t 一 s, these won,t contribute
significantly to the moving average. The moving average constant defines the effective time horizon,
for which we implicitly make this assumption.
Under this peculiar assumption, mt and vt are unbiased estimates of the first and second moment of
gt, respectively:
t-1	t-1
E[mt,i] = X c(μ,t, s)E[gt-s,i] = VLt,i X c(μ,t, s) = VLt,i,	(42)
s=0	s=0
t-1	t-1
E[vt,i] = X c(μ, t, S)E[g2-s,i] = (VL2,i + σ2,i) X c(μ, t, s) = vL2,i + σ2,i,	(43)
s=0	s=0
motivating vt - mt2 as a gradient variance estimate. However, vt - mt2 is not an unbiased variance
estimate due to the fact mt2 is not an unbiased estimate of VLt2 . The error arising from this bias
should generally be dominated by other error sources and will thus be ignored.
C.2 Mini-Batch Estimates
An alternative gradient variance estimate can be obtained locally, within a single mini-batch. The
individual gradients V'(θ, Xk) in a mini-batch are iid random variables and, as noted in the intro-
duction, var[g(θ)] = ∣B∣-1var[V'(θ,Xk)]. We can thus estimate g(θ)′s variances by computing
the sample variance of the {V'(θ, Xk )}k∈B, then scaling by |B|-1,
^(θ) =看 ∖-∖B-1i X V'(θ,xk)2- g(θ)2J	(44)
k∈B
Several recent papers (Mahsereci & Hennig, 2015; Balles et al., 2017b; Mahsereci et al., 2017) have
used this variance estimate for other aspects of stochastic optimizers. In contrast to vt-mt2, this is an
unbiased estimate of the local gradient variance. The (non-trivial) implementation of this estimator
for neural networks is described in Balles et al. (2017a).
C.3 Relative Variance of a Momentum Term (Derivation of Eq. 19)
When estimating the variance with moving averages, we assume that E[gt] = mt and var[gt] =
vt - mt2 . Plugging this into Eq. (18) we can approximate the mean and variance of the momentum
term by
E[rt]2 ≈
t
var[rt] ≈ Eμ2s(Vt-S — m2-s).
s=0
(45)
Computing these two expressions would require two more moving averages in addition to mt and vt .
However, mt and vt will change slowly over time and, by using vt - mt2 as the variance estimate for
15
Under review as a conference paper at ICLR 2018
gt , we anyways make the assumption that all gradients in the effective time horizon of the moving
average have the same mean and variance. We thus further approximate by replacing mt-s with mt
and get
E[rt]2 ≈ m2 (Xμs! = m2 (* 1 * * *--μ)
t
var[rt] ≈ (Vt — m2)	μ2s
s=0
2「- μ2t
-mt)-l--2.
1 — μ2
(46)
(47)
The two scalar factors lead to the correction term κ(μ, t) in Eq. (19).
When estimating the gradient variance from the mini-batch (Eq. 44), we can obtain an unbiased
estimate of var[rt] in Eq. (18) via
St = μi2st-ι + st,	(48)
where ^t is given by Eq. (44).
D Variations of Variance-Adapted Methods
Based on the considerations in Section 3, we examined three more variance-adapted methods. The
first is a variation of m-svag which estimates stochastic gradient variances locally within the mini-
batch, as explained in §C.2. Pseudo-code can be found in Alg. 5. Furthermore, we tested a variant of
adam that applies the correction factor from Eq. (19) to the estimate of the relative variance of the
momentum term. We refer to this method as adam*. Two variants of adam* with the two variance
estimates can be found in Algorithms 4 and 5.
Algorithm 3 m-svag-mb (with mini-batch variance estimates)
Require: initial value θo, step size a, momentum parameter μ ∈ [0,1], number of steps T
1:	Initialize m = 0, sS = 0
2:	for t = 1, . . . , T do
3:	Compute stochastic gradient g(θ) and variance estimate s(θ)	. Eq. (44)
4:	Update aggregators m J μm + g(θ),	s J μ2s + s(θ)
5:	Compute relative variance estimate η2 = s/m2
6:	Compute variance adaptation factors Y = (1 + η2)-1
7:	Update θ J θ — α(γ Θ m)
8:	end for
Algorithm 4 adam* (with exp. moving average variance estimates)
Require: initial value θo, step size a, momentum/averaging constant μ ∈ [0,1], number of steps T
1: Initialize m = 0, v = 0
2: for t = 1, . . . , T do
3:	Compute stochastic gradient g = g(θ)
4:	Update moving averages m J μm + (1 — μ)g, V J μv +(1 — μ)g2
5:	Bias-correct m = (1 — μt)-1m, V = (1 — μt)-1V
6:	Compute relative variance estimate η2 = κ(μ, t) Vm	. Eq. (19)
7:	Compute variance adaptation factors Y = (1 + η2)-1/2
8:	Update θ J θ — α(γ Θ sign(m))
9: end for
This is ADAM (βι = β2 = μ, ε = 0), expect for the correction factor κ(μ, t) for the relative variance.
16
Under review as a conference paper at ICLR 2018
1.4
2 0 8 6
- - - -
Iloo
SSoIe,II
ssolsəɪ
0.6
CIFAR-10
0	5	10	15	20	25
Steps(∙10-3)
CIFAR-100
4.5
4.0
5 0 5 0 5 0
------
3 3 2 2 1 1
SSoIu,II
5 0 5
- - -
4 4 3
0 5 0
- - -
3 2 2
ssolsəɪ
1.5
1.0
0	10 20 30 40 50 60
Steps(∙10-3)
4 2
- -
1 1
Figure 6:	Comparison of the original adam algorithm to the variants in Algs. 4 and 5. Set-up of the
plots as in Fig. 3. All three algorithms exhibit very similar performance on both problems.
Algorithm 5 adam*-mb (with mini-batch variance estimates)
Require: initial value θo, step size a, momentum/averaging constant μ ∈ [0,1], number of steps T
1:	Initialize m = 0, s = 0
2:	for t = 1, . . . , T do
3:	Compute stochastic gradient g(θ) and variance estimate S(θ)	. Eq. (44)
4:	Update aggregators m J μm + g(θ),	W J μ2s + s(θ)
5:	Compute relative variance estimate η2 = s/m2
6:	Compute variance adaptation factors Y = (1 + η2)-1/2
7:	Update θ J θ — α(γ Θ sign(m))
8:	end for
D.1 Experimental Results
We evaluated the variants on the two cifar test problems. Figure 6 shows a comparison of the two
adam* variants with the original adam. Figure 7 compares the mini-batch variant of m-svag to
the one with exponential moving averages.
17
Under review as a conference paper at ICLR 2018
1.4
SSoIMU≡-3,II
CIFAR-10
SSoIM5'aejl
CIFAR-100
4.5 I——,——,——I——i-
4.0 J
4 2 0 8
- - - -
Illo
sslsəɪ
0.6
0	5	10	15	20	25
Steps(∙10-3)
1.5——；-
1.0-1-1--1-1-1--
0	10	20	30	40	50	60
Steps(∙10-3)
Figure 7:	Comparison of the two variants of the m-svag algorithm. Set-up of the plots as in Fig. 3.
Both variants exhibit very similar performance on both problems.
18