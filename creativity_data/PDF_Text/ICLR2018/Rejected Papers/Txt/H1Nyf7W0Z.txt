Under review as a conference paper at ICLR 2018
Alpha-divergence bridges maximum likeli-
HOOD AND REINFORCEMENT LEARNING IN NEURAL
SEQUENCE GENERATION
Anonymous authors
Paper under double-blind review
Ab stract
Neural sequence generation is commonly approached by using maximum-
likelihood (ML) estimation or reinforcement learning (RL). However, it is known
that they have their own shortcomings; ML presents training/testing discrepancy,
whereas RL suffers from sample inefficiency. We point out that it is difficult to
resolve all of the shortcomings simultaneously because of a tradeoff between ML
and RL. In order to counteract these problems, we propose an objective function
for sequence generation using α-divergence, which leads to an ML-RL integrated
method that exploits better parts of ML and RL. We demonstrate that the proposed
objective function generalizes ML and RL objective functions because it includes
both as its special cases (ML corresponds to α → 0 and RL to α → 1). We provide
a proposition stating that the difference between the RL objective function and the
proposed one monotonically decreases with increasing α. Experimental results on
machine translation tasks show that minimizing the proposed objective function
achieves better sequence generation performance than ML-based methods.
1	Introduction
Neural sequence models have been successfully applied to various types of machine learning tasks,
such as neural machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015),
caption generation (Xu et al., 2015; Chen & Lawrence Zitnick, 2015), conversation (Vinyals & Le,
2015), and speech recognition (Chorowski et al., 2014; 2015; Bahdanau et al., 2016). Therefore,
developing more effective and sophisticated learning algorithms can be beneficial.
Popular objective functions for training neural sequence models include the maximum-likelihood
(ML) and reinforcement learning (RL) objective functions. However, both have limitations, i.e.,
training/testing discrepancy and sample inefficiency, respectively. Bengio et al. (2015) indicated
that optimizing the ML objective is not equal to optimizing the evaluation metric. For example,
in machine translation, maximizing likelihood is different from optimizing the BLEU score (Pa-
pineni et al., 2002), which is a popular metric for machine translation tasks. In addition, during
training, ground-truth tokens are used for the predicting the next token; however, during testing, no
ground-truth tokens are available and the tokens predicted by the model are used instead. On the
contrary, although the RL-based approach does not suffer from this training/testing discrepancy, it
does suffer from sample inefficiency. Samples generated by the model do not necessarily yield high
evaluation scores (i.e., rewards), especially in the early stage of training. Consequently, RL-based
methods are not self-contained, i.e., they require pre-training via ML-based methods. As discussed
in Section 2, since these problems depend on the sampling distributions, itis difficult to resolve them
simultaneously.
Our solution to these problems is to integrate these two objective functions. We propose a new
objective function α-DM (α-divergence minimization) for a neural sequence generation, and we
demonstrate that it generalizes ML- and RL- based objective functions, i.e., α-DM can represent
both functions as its special cases (α → 0 and α → 1). We also show that, for α ∈ (0, 1), the
gradient of the α-DM objective is a combinations of the ML- and RL-based objective gradients. We
apply the same optimization strategy as Norouzi et al. (2016), who useed importance sampling, to
optimize this proposed objective function. Consequently, we avoid on-policy RL sampling which
1
Under review as a conference paper at ICLR 2018
suffers from sample inefficiency, and optimize the objective function closer to the desired RL-based
objective than the ML-based objective.
The experimental results for a machine translation task indicate that the proposed α-DM objective
outperforms the ML baseline and the reward augmented ML method (RAML; Norouzi et al., 2016),
upon which we build the proposed method. We compare our results to those reported by Bahdanau
et al. (2017), who proposed an on-policy RL-based method. We also confirm that α-DM can provide
a comparable BLEU score without pre-training.
The contributions of this paper are summarized as follows.
•	We propose the α-DM objective function using α-divergence and demonstrate that it can
be considered a generalization of the ML- and RL-based objective functions (Section 4).
•	We prove that the α-DM objective function becomes closer to the desired RL-based objec-
tives as α increases in the sense that the upper bound of the maximum discrepancy between
ML- and RL-based objective functions monotonically decreases as α increases.
•	The results of machine translation experiments demonstrate that the proposed α-DM ob-
jective outperforms the ML-baseline and RAML (Section 7).
2	Comparing objective functions
In this section, we introduce ML-based and RL-based objective functions and the problems in as-
sociation with learning neural sequence models using them. We also explain why it is difficult to
resolve these problems simulataneously.
Maximum-likelihood An ML approach is typically used to train a neural sequence model. Given
a context (or input sequence) x ∈ X and a target sequence y = (y1, . . . , yT) ∈ Y, ML minimizes
the negative log-likelihood objective function
L(θ) =-XXq(y∣x)log pθ (y|x),	(1)
x∈X y∈Y
where q(y|x) denotes the true sampling distribution. Here, we assume that x is uniformly sam-
pled from X and omit the distribution of x from Eq. (1) for simplicity. For example, in machine
translation, if a corpus contains only a single target sentence y* for each input sentence x, then
q(y∣x) = δ(y - y*) and the objective becomes L(θ) = - Pχ∈χ logpθ(y*|x).
ML does not directly optimize the final performance measure; that is, training/testing discrepancy
exists. This arises from at leset these two problems:
(i)	Objective score discrepancy. The reward function is not used while training the model;
however, it is the performance measure in the testing (evaluation) phase. For example, in
the case of machine translation, the popular evaluation measures such as BLEU or edit
rate (Snover et al., 2006) differ from the negative likelihood function.
(ii)	Sampling distribution discrepancy. The model is trained with samples from the true
sampling distribution q(y|x); however, it is evaluated using samples generated from the
learned distribution pθ (y |x).
Reinforcement learning In most sequence generation task, the optimization of the final perfor-
mance measure can be formulated as the minimization of the negative total expected rewards ex-
pressed as follows:
L(θ) = - X Xpθ(y∣χ)r(y,y1χ),	(2)
x∈X y∈Y
where r(y, y*|x) is a reward function associated with the sequence prediction y, i.e., the BLEU score
or the edit rate in machine translation. RL is an approach to solve the above problems. The objective
function of RL is L* in Eq. (2), which is a reward-based objective function; thus, there is no objective
score discrepancy, thereby resolbing problem (i). Sampling from pθ (y|x) and taking the expectation
with pθ(y|x) in Eq. (2) also resolves problem (ii). Ranzato et al. (2016) and Bahdanau et al. (2017)
2
Under review as a conference paper at ICLR 2018
directly optimized L* using policy gradient methods (Sutton et al., 2000). A sequence prediction
task that selects the next token based on an action trajectory (y1, . . . , yt-1) can be considered to be
an RL problem. Here the next token selection corresponds to the next action selection in RL. In
addition, the action trajectory and the context x correspond to the current state in RL.
RL can suffer from sample inefficiency; thus, it may not generate samples with high rewards, par-
ticularly in the early learning stage. By definition, RL generates training samples from its model
distribution. This means that, if model pθ (y|x) has low predictive ability, only a few samples will
exist with high rewards.
(iii)	Sample inefficiency. The RL model may rarely draw samples with high rewards, which
hinders to find the true gradient to optimize the objective function.
Machine translation suffers from this problem because the action (token) space is vast (typically
>10, 000 dimensions) and rewards are sparse, i.e., positive rewards are observed only at the end of
a sequence. Therefore, the RL-based approach usually requires good initialization and thus is not
self-contained. Previous studies have employed pre-training with ML before performing on-policy
RL-based sampling (Ranzato et al., 2016; Bahdanau et al., 2017).
Entropy regularized RL To prevent the policy from becoming overly greedy and deterministic,
some studies have used the following entropy-regularized version of the policy gradient objective
function (Mnih et al., 2016):
L(τ)(θ) ：= X{-τH(pθ(y∣x)) - XPθ(y∣x)r(y,y*∣x)}.	⑶
x∈D	y∈Y
Note that limτ→0 L((τ) = L( holds.
Reward augmented ML Norouzi et al. (2016) proposed RAML, which solves problems (i) and
(iii) simultaneously. RAML replaces the sampling distribution of ML, i.e., q(y|x) in Eq. (1), with a
reward-based distribution q(「)(y|x) α exp {r(y, y*∣x)∕τ}. In other words, RAML incorporates the
reward information into the ML objective function. The RAML objective function is expressed as
follows:
L(τ)(θ) := -ΣΣq(τ)(y|X)Iog pθ (y⑻.	3 (4)
x∈X y∈Y
However, problem (ii) remains.
Despite these various attempts, a fundamental technical barrier exists. This barrier prevents solving
the three problems using a single method. The barrier originates from a trade-off between sampling
distribution discrepancy (ii) and sample inefficiency (iii), because these issues are related to the
sampling distribution. Thus, our approach is to control the trade-off of the sampling distributions by
combining them.
3 α-DIVERGENCE
The proposed method utilizes α-divergence DA(α)(pkq), which measures the asymmetric distance
between two distributions p and q (Amari, 1985). A prominent feature of α-divergence is that
it can behave as DKL(pkq) or DKL (qkp) depending on the value of α, i.e., DA(1)(pkq) :=
limα→1 DA(α)(pkq) = DKL(pkq) and DA(0)(pkq) := limα→0 DA(α)(pkq) = DKL(qkp). This fact
follows from the definition of α-divergence
Df)(Pkq) := α(r⅛
n1- Xpα(y)q1-α(y)} = - 1 Xp(y)log(a)
y∈Y
y∈Y
(5)
where logg)(∙) is the generalized logarithm log(a)(x) := (1 - α)-1 (x1-α - 1). Furthermore,
α-divergence becomes a Hellinger distance when α equals to 1/2.
3
Under review as a conference paper at ICLR 2018
4 PROPOSED OBJECTIVE FUNCTION: α-DM
In this section, we describe the proposed objective function α-DM and its gradient. Furthermore,
we demonstrate that it can smoothly bridge both ML- and RL- based objective functions.
4.1	Objective function
Maximum likelihood
based
α → 0
Reinforcement learning
based
α → 1
UOQeZLI-ngdx
Maximum likelihood
T T → 0
RAML
(NoroUZi et al., 2016)
ɑ → 0	α → 1
--------- Proposed method ------------
Policy gradient
(w/o regularization)
T T → 0
Policy gradient
(w/ entropy regularization)
Figure 1: α-DM objective bridges ML- and RL-based objectives.
We define the α-DM objective function as the α-divergence between pθ and q(τ) :
L(α,τ)(θ)
τ X DA(α)(pθ l∣q(τ)) = - T XX
Pθ (y∣χ)log(α)
x∈X	x∈X y∈Y
(q(τ)(y|X))
pθ(y|x))
(6)
0 < α < 1
This α-divergence is equal to LjT)in Eq. (3) or L(T) in Eq. (4) by employing α → 1 or α → 0
limits, respectively (up to constant).
lim L(α,T) (θ) = τ DKL(pθ lq(T)) = L(jT) (θ) + constant,	(7)
α→	x∈X
lim L(α,T) (θ) = τ	DKL(q(T) lpθ) = τL(T) (θ) + constant.	(8)
α→	x∈X
Figure 1 illustrates how the α-DM objective bridges the ML- and RL-based objective functions.
Although the objectives L(jα,T)(θ), L(jT)(θ), and L(T) (θ) have the same global minimizerpθ (y |x) =
q(T)(y|x), empirical solutions often differ.
4.2 Objective function gradient
To train neural network or other machine learning models via α-divergence minimization, one can
use the gradient of α-DM objective function. The gradient of Eq. (6) can be expressed as
VθL(α,τ)(O) = — X Xpθα,τ)(ylx)VθlogPθ(yIx),	⑼
x∈X y∈Y
where
p(α,τ )(y∣x) = τ^-Pα(y∣x)q1-α(y∣x)	(10)
1 — α
is a weight that mixes sampling distributions pθ and q(T). This weight makes it clear that the α-DM
objective can be considered as a mixture of ML- and RL-based objective functions. See Appendix A
for the derivation of this gradient. It converges to the gradient of entropy regularized RL or RAML
by taking α → 1 or α → 0 limits, respectively (UP to constant); i.e., limα→ι NθL(a,τ) = NθLjT)
and limα→0 NθL(α,τ) = T ▽&£(「).
In Appendix C, we summarize all of the objective functions, gradients, and their connections.
5 α-DM ANALYSIS
In this section, we characterize the difference between α-DM objective function L(α,T) and the
desired RL-based objective function L(jT ) with respect to sup-norm. Our main claim is that, with
4
Under review as a conference paper at ICLR 2018
respect to sup-norm, the discrepancy between L(α,τ) and LjT)decreases linearly as α increases to
1. We utilize this analysis to motivate our α-DM objective function with larger α if there are no
concerns about the sampling inefficiency.
Proposition 1 Assume that pθ has the same finite support S as that of q(τ), and that for any s ∈ S,
there exists δ > 0 such thatpθ(s) > δ holds. For any α ∈ (0, 1), the following holds.
SuplLjT)(θ)-L(α,τ)(θ)∣ ≤ Cι(1- α) + C2,	(11)
where L(α,T) := αL(α,T). Here, C1, C2 is universal constants irrelevant to α.
The following proposition immediately proves the theorem above.
Proposition 2 Assume that probability distribution p has the same finite support S as that of q, and
that for any s ∈ S there exists δ > 0 such that p(s) > δ holds. For any α ∈ (0, 1), the following
holds.
sup ∣∣∣DKL(pkq) - αDA(α)(pkq)∣∣∣ ≤ C(1 - α).	(12)
Here，C = max{supp ∣P P log2 (q/p) ∣, suPp∣P q log2 IqlP) ∣ }.
For the proof of the Proposition 1 and Proposition 2, see Appendix B.
6 OPTIMIZATION OF α-DM OBJECTIVE FUNCTION
In this paper, we employed the optimization strategy which is similar to that of RAML. We sample
target sentence y for each x from another data augmentation distribution q0(y|x), and then estimate
the gradient by importance sampling (IS). For example, we add some noise to the ground truth
target sentence yj by insertion, substitution, or deletion, and the distribution p0(y|x) assigns some
probability to each modified target sentence. Given samples from this proposal ditribution p0(y|x),
we update the parameter using the following IS estimator
VθL(α,τ )(θ)
-	q0(y|x)
x∈X y∈Y
(pθα,τ) ⑶X))
k qo(yIx))
V logpθ(y|x)
(13)
N
'—EwiV log pθ (yi∣xi).	(14)
i=0
Here, {(x1, y1), . . . , (xN, yN)} are the N samples from the proposal distribution q0(y|x), and wi is
the importance weight which is proportional to pθa,τ) (yi |xi):
wi H pα(yi∣xi)q1-)α(yi∣xi).	(15)
Note that the difference betweene RAML and α-DM is only this importance weight wi . In RAML,
wi depends only on q(「)(yi∣xi) but not on pθ(yi∣xi). We normalize wi in each minibatch in order
to use same hyperparameter (e.g., learning rate) as ML baseline. Thus, this estimator becomes a
weighted IS estimator. A weighted IS estimator is not unbiased, yet but it has smaller variance.
Also, We found that normalizing q(「)(yi∣xi) and p§ (y∕xi) in each minibatch leads to good results. 7
7 Numerical experiments
We evaluate the effectiveness of α-DM experimentally using neural machine translation tasks. We
compare the BLEU scores of ML, RAML, and the proposed α-DM on the IWSLT'14 German-
English corpus (Cettolo et al., 2014). In order to evaluate the impact of training objective function,
we train the same attention-based encoder-decoder model (Bahdanau et al., 2015; Luong et al.,
2015) for each objective function. Furthermore, we use the same hyperparameter (e.g., learning
rate, dropout rate, and temperature τ) between all the objective functions. For RAML and α-DM,
we employ a data augmentation procedure similar to that of Norouzi et al. (2016), and thus we
generate samples from a data augmentation distribution q0(y|x). Note that the difference between
RAML and α-DM is only the weight wi of Eq. (14). The details of data augmentation distribution
are described in Section 7.2.
5
Under review as a conference paper at ICLR 2018
----α-DM (fixed)
----α-DM (annealed)
----RAML
---- ML
Figure 2: Performance on different hyperparameters α. The α-DM approach with larger α performs
better than that with smaller α.
Table 1: BLEU on IWSLT'14 German-English.
		k = 1	k = 10
ML		27.59(±0.18)	28.15 (±0.16)
RAML		27.74 (±0.06)	28.44 (±0.13)
α-DM, α	0.3 fixed	27.92 (±0.10)	28.51 (±0.10)
α-DM, α	0.4 fixed	28.05 (±0.07)	28.65 (±0.06)
α-DM, α	0.5 fixed	28.01 (±0.16)	28.52 (±0.10)
α-DM, α	0.3 annealed	27.99 (±0.11)	28.60 (±0.09)
α-DM, α	0.4 annealed	27.93 (±0.11)	28.51 (±0.15)
α-DM, α	0.5 annealed	27.98 (±0.11)	28.48 (±0.15)
IWSLT'14 German-English. The training data comprised approximately 153K German-English
sentence pairs and 7K development/test sentence pairs. The vocabulary size for the source/target
were 32 009 and 22 822, respectively. The model architecture and parameters follow that of Ranzato
et al. (2016) and Bahdanau et al. (2017). Specifically, we trained attention-based encoder-decoder
model with the encoder of a bidirectional LSTM with 256 units and the LSTM decoder with the
same number of layers and units. We exponentially decay the learning rate, and the initial learning
rate is chosen using grid search to maximize the BLEU performance ofML baseline on development
dataset. The important hyperparameter τ of RAML and α-DM is also determined to maximize the
BLEU performance of RAML baseline on development dataset. As a result, the initial learning rate
of 0.5 and τ of 1.0 were used. Our α-DM used the same hyperparameters as ML and RAML includ-
ing the initial learning rate, τ , and so on. Details about the models and parameters are discussed in
Section 7.2.
7.1	Results
To investigate the impact of hyperparameter α, we train the neural sequence models using α-DM
5 times for each fixed α ∈ {0.0, 0.1, . . . , 0.9}, and then reported the BLEU score of test dataset.
Moreover, assuming that the underfitted model prevents the gradient from being stable in the early
stage of training, we train the same models with α being linearly annealed from 0.0 to larger values;
we increase the value of α by adding 0.03 at each epoch. Here, the beam width k was set to
1 or 10. All BLEU scores and their averages are plotted in Figure 2. The results show that for
both k = 1, 10, the models performance are better than smaller or larger α when α is around 0.5
(α = 0.5). However, for larger fixed α, the performance was worse than RAML and ML baselines.
On the other hand, we can see that the annealed versions of α-DM improve the performance of the
corresponding fixed versions in relatively larger α. As a result, in the annealed scenario, α-DM with
wide range of α ∈ (0, 1) improves on the performance consistently. This implies that the underfitted
model makes the performance worse.
6
Under review as a conference paper at ICLR 2018
We summarize the average BLEU scores and their standard deviation of ML, RAML, and α-DM
with α ∈ {0.3, 0.4, 0.5} in Table. 1. The result shows that the BLEU score (k = 10) of our α-DM
outperforms ML and RAML baseline. Furthermore, although the ML baseline performances differ
between our results and those of Bahdanau et al. (2017), the proposed α-DM performance with
α = 0.5 without pre-training is comparable with the on-policy RL-based methods (Bahdanau et al.,
2017). We believe that these results come from the fact that α-DM with α > 0 has smaller bias than
that of α = 0 (i.e., RAML).
7.2	Details
We utilized a stochastic gradient descent with a decaying learning rate. The learning rate decays
from the initial learning rate to 0.05 with dev-decay (Wilson et al., 2017), i.e., after training each
epoch, we monitored the perplexity for the development set and reduced the learning rate by mul-
tiplying it with δ = 0.5 only when the perplexity for the development set does not update the best
perplexity. The mini-batch size is 128. We used the dropout with probability 0.3. Gradients are
rescaled when the norms exceed 5. In addition, if an unknown token, i.e., a special token represent-
ing a word that is not in the vocabulary, is generated in the predicted sentence, it was replaced by
the token with the highest attention in the source sentence (Jean et al., 2015). We implemented our
models using a fork from the PyTorch1 version of the OpenNMT toolkit (Klein et al., 2017). We
calculated the BLEU scores with multi-bleu.perl2 script for both the development and test sets.
We obtained augmented data in the same manner as the RAML framework (Norouzi et al., 2016).
For each target sentence, some tokens were replaced by other tokens in the vocabulary and we used
the negative Hamming distance as reward. We assumed that Hamming distance e for each sentence
is less than [m × 0.25], where m is the length of the sentence and [a] denotes the maximum integer
which is less than or equal to a ∈ R. Moreover, the Hamming distance for a sample is uniformly
selected from 0 to [m × 0.25]. One can also use BLEU or another machine translation metric for
this reward. However, we assumed proposal distribution q0(y|x) different from that of RAML. We
assumed the simplified proposal distribution q0(y|x), which is a discrete uniform distribution over
[0, m × 0.25]. This results in hyperparameter τ used in this experiment being different from that of
RAML. We search the τ , which maximize the BLEU score of RAML on the development set. As a
results, τ = 1.0 was chosen, and α-DM also uses this fixed τ in all the experiments.
8	Related works
From the RL literature, reward-based neural sequence model training can be separated into on-policy
and off-policy approaches, which differ in the sampling distributions. The proposed α-DM approach
can be considered an off-policy approach with importance sampling.
Recently, on-policy RL-based approaches for neural sequence predictions have been proposed. Ran-
zato et al. (2016) proposed a method that uses the REINFORCE algorithm (Williams, 1992). Based
on Ranzato et al. (2016), Bahdanau et al. (2017) proposed a method that estimates a critic network
and uses it to reduce the variance of the estimated gradient. Bengio et al. (2015) proposed a method
that replaces some ground-truth tokens in an output sequence with generated tokens. Yu et al. (2017),
Lamb et al. (2016), and Wu et al. (2017) proposed methods based on GAN (generative adversarial
net) approaches (Goodfellow et al., 2014). Note that on-policy RL-based approaches can directly
optimize the evaluation metric. Degris et al. (2012) proposed off-policy gradient methods using
importance sampling, and the proposed α-DM off-policy approach utilizes importance sampling to
reduce the difference between the objective function and the evaluation measure when α > 0.
As mentioned previously, the proposed α-DM can be considered an off-policy RL-based approach
in that the sampling distribution differs from the model itself. Thus, the proposed α-DM approach
has the same advantages as off-policy RL methods compared to on-policy RL methods, i.e., com-
putational efficiency during training and learning stability. On-policy RL approaches must generate
samples during training, and immediately utilize these samples. This property leads to high com-
putational costs during training and if the model falls into a poor local minimum, it is difficult to
1http://pytorch.org
2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/
multi- bleu.perl
7
Under review as a conference paper at ICLR 2018
recover from this failure. On the other hand, by exploiting data augmentation, the proposed α-DM
can collect samples before training. Moreover, because the sampling distribution is a stationary dis-
tribution independent of the model, one can expect that the learning process of α-DM is more stable
than that of on-policy RL approaches. Several other methods that compute rewards before training
can be considered off-policy RL-based approaches, e.g., minimum risk training (MRT; Shen et al.,
2016, RANDOMER (Guu et al., 2017), and Google neural machine translation (GNMT; Wu et al.,
2016).
While the proposed approach is a mixture of ML- and RL-based approaches, this attempt is not
unique. The sampling distribution of scheduled sampling (Bengio et al., 2015) is also a mixture of
ML- and RL-based sampling distributions. However, the sampling distributions of scheduled sam-
pling can differ even in the same sentence, whereas ours are sampled from a stationary distribution.
To bridge the ML- and RL-based approaches, Guu et al. (2017) considered the weights of the gra-
dients of the ML- and RL-based approaches by directly comparing both gradients. In contrast, the
weights of the proposed α-DM approach are obtained as the results of defining the α-divergence
objective function. GNMT (Wu et al., 2016) considered a mixture of ML- and RL-based objective
functions by the weighted arithmetic sum of L and L*. Comparing this weighted mean objective
function and a-DM's objective function could be an interesting research direction in future.
9	Conclusion
In this study, we have proposed a new objective function as α-divergence minimization for neural
sequence model training that unifies ML- and RL-based objective functions. In addition, we proved
that the gradient of the objective function is the weighted sum of the gradients of negative log-
likelihoods, and that the weights are represented as a mixture of the sampling distributions of the
ML- and RL-based objective functions. We demonstrated that the proposed approach outperforms
the ML baseline and RAML in the IWSLT’14 machine translation task.
In this study, we focus our attention on the neural sequence generation problem, but we expect our
framework may be useful to broader area of reinforcement learning. The sample inefficiency is
one of major problems in reinforcement learning, and people try to mitigiate this problem by using
several type of supervised learning frameworks such as imitation learning or apprenticisip learning.
This alternative approaches bring another problem similar to the neural sequence generaton problem
that is originated from the fact that the objective function for training is different from the one for
testing. Since our framework is general and independent from the task, our approach may be useful
to combine these approaches.
References
Shun-ichi Amari. Differential-Geometrical Methods in Statistics. Springer, 1985.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of International Conference on Learning Repre-
sentations (ICLR), 2015.
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-
to-end attention-based large vocabulary speech recognition. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In Proceedings
of International Conference on Learning Representations (ICLR), 2017.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence
prediction with recurrent neural networks. In Advances in Neural Information Processing Systems
(NIPS), 2015.
Mauro Cettolo, Jan Niehues, Sebastian Stuker, Luisa Bentivogli, and Marcello Federico. Report on
the 11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of International Workshop
on Spoken Language Translation (IWSLT), 2014.
8
Under review as a conference paper at ICLR 2018
Xinlei Chen and C Lawrence Zitnick. Mind’s eye: A recurrent visual representation for image
caption generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
KyUnghyUn Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2014.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end con-
tinuous speech recognition using attention-based recurrent NN: First results. arXiv preprint
arXiv:1412.1602, 2014.
Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
Attention-based models for speech recognition. In Advances in Neural Information Processing
Systems (NIPS), 2015.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of
International Conference on Machine Learning (ICML), 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2014.
Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang. From language to programs:
Bridging reinforcement learning and maximum marginal likelihood. In Proceedings of the Annual
Meeting of Association for Computational Linguistics (ACL), 2017.
Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target
vocabulary for neural machine translation. In Proceedings of the Annual Meeting of Association
for Computational Linguistics (ACL), 2015.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. OpenNMT:
Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810, 2017.
Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, and Yoshua Bengio.
Professor forcing: A new algorithm for training recurrent networks. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2016.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. In Proceedings of Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of International Conference on Machine Learning (ICML), 2016.
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. Reward augmented maximum likelihood for neural structured prediction. In Advances In
Neural Information Processing Systems (NIPS), 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the Annual Meeting of Association for Com-
putational Linguistics (ACL), 2002.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-
ing with recurrent neural networks. In Proceedings of International Conference on Learning
Representations (ICLR), 2016.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Mini-
mum risk training for neural machine translation. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL), 2016.
9
Under review as a conference paper at ICLR 2018
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of
translation edit rate with targeted human annotation. In Proceedings of Association for Machine
Translation in the Americas (AMTA), 2006.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Systems (NIPS), 2014.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2000.
Oriol Vinyals and Quoc Le. A neural conversational model. In Proceedings of International Con-
ference on Machine Learning (ICML), 2015.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229-256,1992.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.
The marginal value of adaptive gradient methods in machine learning. arXiv preprint
arXiv:1705.08292, 2017.
Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Adversarial
neural machine translation. arXiv preprint arXiv:1704.06933, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Proceedings of International Conference on Machine Learning (ICML), 2015.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. SeqGAN: Sequence generative adversarial nets
with policy gradient. In Proceedings of AAAI Conference on Artificial Intelligence (AAAI), 2017.
A GRADIENT OF α-DM OBJECTIVE
The gradient of α-DM can be obtained as follows:
VθL(α,τ)(θ) = Vθ{- X α-τ-α){ι - XPα(y∣χ)q1τ-α(y∣χ)}}
x∈X	y∈Y
-α(1-α) XX vθ pα(ylx)q1-)α(ylx)
-a XXpα(y∣χ)q1-α(y∣χ)Vθ log p (y|x)
x∈X y∈Y
-XXpθα,τ )(y∣x)Vθ log Pθ (y|x),
x∈X y∈Y
where
pθα,τ) (y|X) = ι--αpα(ylx)q1-α (y⑻.
(16)
(17)
(18)
(19)
(20)
In Eq. (18), We used the so-called log-trick: Vθpθ(y∣x) = pθ(y∣x)Vθ logpθ(y|x).
10
Under review as a conference paper at ICLR 2018
B Proofs of Prepositions
Proposition 1 Assume that probability distribution p has the same finite support S as that of q, and
that for any s ∈ S there exists δ > 0 such that p(s) > δ holds. For any α ∈ (0, 1) the following
holds.
sup DKL(pkq) - αDA(α)(pkq) ≤ C(1 - α).	(21)
Here，C = max{sUpplPPlog2(q∕p)∣, SUpplP qlog2(q∕p)∣}.
Proof. By Taylor’s theorem, there is an α0 ∈ (α, 1) such that
x(1-α0)
x(1-α) = 1 — log X ∙ (α — 1) +--------log2 X ∙ (α — 1)2	(22)
Therefore,
D(T)(Pkq) := -α XPlog(α)
-IXP log
(23)
(24)
(25)
(26)
Therefore, by Jensen’s inequality we have
sUp llDKL(Pkq) — αDA(α)(Pkq)ll
(1 — α) sUp
p
(1 — α) sUp
p
≤ (1 — α) sUp
p
X P (P)(j:2(P
XPα0 q1-α0 log2 (P
≤ (1 — α) {α0 SUp XP log2 (p)
≤ (1 — α){α0C +(1 — α0)C}
= C(1 — α),
+ (1 — α0)sup X q log2 (P) }
(27)
(28)
(29)
(30)
(31)
(32)
(33)
where C = max{SUpplP P log2(q∕P)l, SUpplP q log2(q∕P)l}.
Proposition 2 Assume that Pθ has the same finite support S as that of q(τ), and that for any s ∈ S,
there exists δ > 0 such thatPθ(s) > δ holds. For any α ∈ (0, 1), the following holds.
sup∣L(τ)(θ) —L(a,T)(θ)∣ ≤ Cι(1 — α) + C2,	(34)
where L(α,τ) := αL(α,τ). Here, C1, C2 is universal constants irrevant to α.
11
Under review as a conference paper at ICLR 2018
Proof. Note that LjT )(θ)	=	TDKL (pθ ∣q(τ)) — Z (T) where Z (T)	:=
Px∈x Py∈γ exp(r(y, y* ∣x)∕τ). By Proposition 1 We have
SupLT )(θ)-L(α,τ )(θ)∣	(35)
=T X DKL(PθΙq(T)) - Z(T) - ατX DA(pθΙq(T))	(36)
≤ T X ∣DκL(Pθ|q(T)) - αDα(Pθ|q(T))∣ + IZ(T)|	(37)
x∈X
≤ Cι(1 — α)+ C2,	(38)
where CI = T maχ{supθ ∣ pχ∈χ py∈γ Pθ log2 (q(T )/pθ )∣, supθ ∣ pχ∈χ py∈γ q(τ) log2 (q(T )/pθ )∣}
and C2 = |Z(T)|.
C Catalog of objective functions and their gradients
In this section, we summarize the objective functions of
•	ML (Maximum Likelihood),
•	RL (Reinforcement Learning),
•	RAML (Reward Augmented Maximum Likelihood; Norouzi et al., 2016),
•	EnRL (Entropy regularized Reinforcement Learning), and
•	α-DM (α-Divergence Minimization Training).
Objectives. The objective functions of ML, RL, RAML, EnRL, and α-DM are as follows
L(θ) =	—	q(y∣x)log pθ (y∣x), x∈X y∈Y	(39)
Lj (θ) =	—	pθ (yIx)r3,y) x∈X y∈Y	(40)
L(T) (θ) =	—	q(T)(yIx) log Pθ(yIx), x∈X y∈Y	(41)
L(jT) (θ) =	— X X Pθ(yIx) r(y, yjIx) — T logPθ(yIx) , x∈X y∈Y	(42)
L(α,T) (θ) =	二-ατι⅛ X X{1 - pa(y|x)q(T-a)(y|x)0， (	) x∈Xy∈Y	(43)
where q(「)(y∣x) H exp {r(y, yj∣x)∕T}. Typically, q(y∣x) = δ(y,yj∣x) where yj is the target with
the highest reward.
We can rewrite some of these functions using KL or α-divergences:
L(T) (θ)	DKL(q(T)kPθ) + constant, x∈X	(44)
L(jT)(θ)	T	DKL(Pθkq(T)) + constant,	(45)
	x∈X	
L(α,T)(θ)	T X DA(α)(Pθkq(T)). x∈X	(46)
12
Under review as a conference paper at ICLR 2018
In the limits, there are the following connections between the objectives.		
	T→0 L(T ) = L(θ),	(47)
	Ti→0 LT) = L*(θ),	(48)
	lim L(α,τ) (θ) = τL(τ) (θ) + constant,	(49)
	lim L(α,τ)(θ) = L：T)(θ) + constant.	(50)
Gradients. We list the gradient of each objective function and summarize the connections of them in the limit.		
VθL(θ) =	—	q(yIx)Vθ log pθ(yIx), x∈X y∈Y	(51)
Vθ L*(θ) =	—	pθ (y∣x)r(y, y")Vθ log pθ (y∣x)), x∈X y∈Y	(52)
VθL(τ)(θ) =	—	q(T)(yIx)Vθ logpθ(yIx), x∈X y∈Y	(53)
Vθ L↑τ )(θ)=	— X Xpθ(yIx) r(y, y：Ix) — τlogpθ(yIx) Vθlogpθ(yIx), x∈X y∈Y	(54)
VθL(α,τ)(θ) =	=-iɪ- X XPa(yIx)q1-α(yIx)Vθlogpθ(yIx) x∈X y∈Y	(55)
Each gradient corresponds to ML, RL, RAML, EnRL, and α-DM. To derive Eq. (54), we used Py∈Ypθ(yIx)Vθ logpθ(yIx) = Vθ Py∈Y pθ(yIx) = 0.		
The following connections hold.		
	lim VθL(T) (θ) = VθL(θ) T ɪ0	(56)
	^im VθL：t)(θ) = VθL*(θ)	(57)
	α→m0 Vθ L(α,τ )(θ)= T Vθ L(τ )(θ)	(58)
	lim VθL(α,τ)(θ) = VθL：T)(θ) + constant.	(59)
Here, Eq. (59) are derived by
lim VθL(α,τ)(θ)
α→1
= - lim ⅛ XXpθα(yIx)q(1τ-)α(yIx)Vθ log pθ(yIx)
α→ 1 -α x∈X y∈Y
=—ɪiɪɪi i——- X{XPα(y∣x)q1-α(y∣x)Vθlogpθ(y∣x) — RB Xpθ(y∣x)}
α→	1-α x∈X y∈Y	y∈Y
-τ IX Xpθ (y|x){ aɪi L {(qs≡ 厂
x∈X y∈Y
Vθ log Pθ (y|x)
-τ	pθ(yIx) log
x∈X y∈Y
q(τ)(y Ix)
pθ (yIx)
Vθ log Pθ (y|x)
—X X Pθ(y∣x){r(y,y*∣x) - T logp(y∣x)} Vθ logp(y∣x) + constant.
x∈X y∈Y
(60)
(61)
(62)
(63)
(64)
(65)
13