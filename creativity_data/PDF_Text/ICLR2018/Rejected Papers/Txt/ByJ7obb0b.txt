Under review as a conference paper at ICLR 2018
Understanding and Exploiting the Low-Rank
Structure of Deep Networks
Anonymous authors
Paper under double-blind review
Ab stract
Training methods for deep networks are primarily variants on stochastic gradient
descent. Techniques that use (approximate) second-order information are rarely
used because of the computational cost and noise associated with those approaches
in deep learning contexts. However, in this paper, we show how feedforward deep
networks exhibit a low-rank derivative structure. This low-rank structure makes
it possible to use second-order information without needing approximations and
without incurring a significantly greater computational cost than gradient descent.
To demonstrate this capability, we implement Cubic Regularization (CR) on a
feedforward deep network with stochastic gradient descent and two of its vari-
ants. There, we use CR to calculate learning rates on a per-iteration basis while
training on the MNIST and CIFAR-10 datasets. CR proved particularly successful
in escaping plateau regions of the objective function. We also found that this ap-
proach requires less problem-specific information (e.g. an optimal initial learning
rate) than other first-order methods in order to perform well.
1	Introduction
1.1	Gradient-Based Optimization and Deep Learning
Gradient-based optimization methods use derivative information to determine intelligent search di-
rections when minimizing a continuous objective function. The steepest descent method is the most
basic of these optimization techniques, but it is known to converge very slowly in ill-conditioned
systems. Even outside of these cases, it still only has a linear rate of convergence. Newton’s method
is a more sophisticated approach - one that uses second-order derivative information, which allows
the optimizer to model the error surface more accurately and thus take more efficient update steps.
When it converges, it does so quadratically, but Newton’s method also has limitations of its own.
Firstly, it does not scale well: it can be very expensive to calculate, store, and invert the objective
function Hessian. Secondly, the method may fail if the Hessian is indefinite or singular.
A variety of methods have been developed to try and appropriate the strengths of each approach
while avoiding their weaknesses. The conjugate gradient method, for example, uses only first-
order information but uses the history of past steps taken to produce a better convergence rate than
steepest descent. Quasi-Newton methods, on the other hand, approximate the Hessian (or its inverse)
using first-order information and may enforce positive-definiteness on its approximation. Other
approaches like trust region methods use second-order information without requiring convexity. For
further information about gradient-based optimization, see Nocedal & Wright (2006).
Deep learning (DL) provides a set of problems that can be tackled with gradient-based optimization
methods, but it has a number of unique features and challenges. Firstly, DL problems can be ex-
tremely large, and storing the Hessian, or even a full matrix approximation thereto, is not feasible
for such problems. Secondly, DL problems are often highly nonconvex. Thirdly, training deep net-
works via mini-batch sampling results in a stochastic optimization problem. Even if the necessary
expectations can be calculated (in an unbiased way), the variance associated with the batch sample
calculations produces noise, and this noise can make it more difficult to perform the optimization.
Finally, deep networks consist of the composition of analytic functions whose forms are known. As
such, we can calculate derivative information analytically via back-propagation (i.e. the chain rule).
1
Under review as a conference paper at ICLR 2018
1.2	Training Methods for Deep Learning
These special characteristics of DL have motivated researchers to develop training methods specifi-
cally designed to overcome the challenges with training a deep neural network. One such approach
is layer-wise pretraining (Bengio et al., 2007), where pretraining a neural network layer-by-layer
encourages the weights to initialize close to a optimal minimum. Transfer learning (Yosinski et al.,
2014) works by a similar mechanism, relying on knowledge gained through previous tasks to en-
courage nice training on a novel task. Outside of pretraining, a class of optimization algorithms
have been specifically designed for training deep networks. The Adam, Adagrad, and Adamax set
of algorithms provide examples of using history-dependent learning rate adjustment (Kingma &
Ba, 2014). Similarly, Nesterov momentum provides a method for leveraging history dependence in
stochastic gradient descent (Sutskever et al., 2013). One could possibly argue that these methods
implicitly leverage second order information via their history dependence, but the stochastic nature
of mini-batching prevents this from becoming explicit.
Some researchers have sought to use second-order information explicitly to improve the training
process. Most of these methods have used an approximation to the Hessian. For example, the L-
BFGS method can estimate the Hessian (or its inverse) in a way that is feasible with respect to
memory requirements; however, the noise associated with the sampling techniques can either over-
whelm the estimation or require special modifications to the L-BFGS method to prevent it from
diverging (Byrd et al., 2016). There have been two primary ways to deal with this: subsampling
(Byrd et al., 2016; Moritz et al., 2016) and mini-batch reuse (Schraudolph et al., 2007; Mokhtari &
Ribeiro, 2014). Subsampling involves updating the Hessian approximation every L iterations rather
than every iteration, as would normally be done. Mini-batch reuse consists of using the same mini-
batch on subsequent iterations when calculating the difference in gradients between those two iter-
ations. These approximate second-order methods typically have a computational cost that is higher
than, though on the same order of, gradient descent, and that cost can be further reduced by using
a smaller mini-batch for the Hessian approximation calculations than for the gradient calculation
(Byrd et al., 2011). There is also the question of bias: it is possible to produce unbiased low-rank
Hessian approximations (Martens et al., 2012), but if the Hessian is indefinite, then quasi-Newton
methods will prefer biased estimates - ones that are positive definite. Other work has foregone these
kinds of Hessian approximations in favor of using finite differences (Martens, 2010).
1.3	Contributions
In this paper, we prove, by construction, that the first and second derivatives of feedforward deep
learning networks exhibit a low-rank, outer product structure. This structure allows us to use and ma-
nipulate second-order derivative information, without requiring approximation, in a computationally
feasible way. As an application of this low-rank structure, we implement Cubic Regularization (CR)
to exploit Hessian information in calculating learning rates while training a feedforward deep net-
work. Finally, we show that calculating learning rates in this fashion can improve existing training
methods’ ability to exit plateau regions during the training process.
2	The Low-Rank S tructure of Deep Network Derivatives
Second-order derivatives are not widely used in DL, and where they are used, they are typically
estimated. These derivatives can be calculated analytically, but this is not often done because of
the scalability constraints described in Section 1.1. If we write out the first and second derivatives,
though, we can see that they have a low-rank structure to them - an outer product structure, in fact.
When a matrix has low rank (or less than full rank), it means that the information contained in that
matrix (or the operations performed by that matrix) can be fully represented without needing to know
every entry of that matrix. An outer product structure is a special case of this, where an mxn matrix
A can be fully represented by two vectors A = uvT. We can then calculate, store, and use second-
order derivatives exactly in an efficient manner by only dealing with the components needed to
represent the full Hessians rather than dealing with those Hessians themselves. Doing this involves
some extra calculations, but the storage costs are comparable to those of gradient calculations.
In this section, we will illustrate the low-rank structure for a feedforward network, of arbitrary depth
and layer widths, consisting of ReLUs in the hidden layers and a softmax at the output layer. A
2
Under review as a conference paper at ICLR 2018
feedforward network with arbitrary activation functions has somewhat more complicated derivative
formulae, but those derivatives still exhibit a low-rank structure. That structure also does not depend
on the form of the objective function or whether a softmax is used, and it is present for convolutional
and recurrent layers as well. The complete derivations for these cases are given in Appendix B.
In our calculations, we make extensive use of index notation with the summation convention (Ivance-
vic & Ivancevic, 2007). In index notation, a scalar has no indices (v), a vector has one index (v as
vi or vi), a matrix has two (V as V ij , Vji , or Vij ), and so on. The summation convention holds that
repeated indices in a given expression are summed over unless otherwise indicated. For example,
aT b = Pi aibi = aibi. The pair of indices being summed over will often consist of a superscript
and a subscript; this is a bookkeeping technique used in differential geometry, but in this context,
the subscripting or superscripting of indices will not indicate covariance or contravariance. We have
also adapted index notation slightly to suit the structure of deep networks better: indices placed in
brackets (e.g. the k in v(k),j) are not summed over, even if repeated, unless explicitly indicated by
a summation sign. A tensor convention that we will use, however, is the Kronecker delta: δij, δji ,
or δij . The Kronecker delta is the identity matrix represented in index notation: it is 1 for i = j
and 0 otherwise. The summation convention can sometimes be employed to simplify expressions
containing Kronecker deltas. For example, δij vi = vj and δij Vjk = Vik .
Let us consider a generic feedforward network with ReLU activation functions in n hidden layers, a
softmax at the output layer, and categorical cross-entropy as the objective function (defined in more
detail in Appendix B. The first derivatives, on a per-sample basis, for this deep network are
∂f
∂wjk),i
f = f v(n),j
∂uij	∂pi
—dfU (n,k),mv(k-1),j
=dpi%	V
(1)
(2)
where f is the per-sample objective function, v(k),j is the vector output of layer k, uij is the matrix
of weights in the softmax, and pj = uij v(n),i (which is the vector quantity evaluated by the soft-
max). For the full derivation, and the definition of the matrix quantity ηi(n,k),m , see Appendix B. In
calculating these expressions, We have deliberately left ∂∂f unevaluated. This keeps the expression
relatively simple, and programs like TensorFlow (Abadi et al., 2015) can easily calculate this for
us. Leaving it in this form also preserves the generality of the expression - there is no low-rank
structure contained in ∂∂f, and the low-rank structure of the network as a whole is therefore shown
to be independent of the objective function and whether or not a softmax is used. In fact, as long
as Equation 13 holds, any sufficiently smooth function of pj may be used in place of a softmax
without disrupting the low-rank structure. The one quantity that needs to be stored here is ηi(n,k),j
for k = 1, 2, . . . , n - 1; it will be needed in the second derivative calculations. Note, however, that
this is roughly the same size as the gradient itself.
We can now see the low-rank structure: ∂f is the outer product (or tensor product) of the vectors
∂f and v(n),j, and ?小 is the outer product of 爵Ulmn(I*k,m (which ends up being a rank-1
tensor) and v(k-1),j . The index notation makes the outer product structure clear. It is important to
note that this low-rank structure only exists for each SamPie - a weighted sum of low-rank matrices
is not necessarily (and generally, will not be) low rank. In other words, even if the gradient of f is
low rank, the gradient of the expectation, F = E [f], will not be, because the gradient of F is the
weighted sum of the gradients of f. The second-order objective function derivatives are then
∂2f
∂uii∂w∖kk,
jt
d2f = d2f v(n),j v(n),t
∂uj ∂uS	∂pi∂ps
Ifi n(n，k),jv(I),t + ^fn VgjUImnsnk)，mv(k-1),
∂pi s	∂pi ∂pl	m s
(3)
(4)
3
Under review as a conference paper at ICLR 2018
∂2f
∂2f
lmηi(n,k),mv(k-1),juqaηs(n,r),av(r-1),t
----ττ-r----— = —~~———U
∂wj(k),i∂wt(r),s	∂pl∂pq
df	f nSn，r)，mn(rTk),tv(I),j r>k
+∂∏ Um × \	0	r = k
dp	[ η(n,ko,mηSkTr),jv(r-1),t r < k
(5)
Calculating all of these second derivatives requires the repeated Use of ∂dp2. Evaluating that Hessian
is straightforward given knowledge of the activation functions and objective used in the network,
and storing it is also likely not an issue as long as the number of categories is small relative to the
number of weights. For example, consider a small network with 10 categories and 1000 weights. In
such a case,第 would only contain 100 entries - the gradient would be 10 times larger. We now
find that we have to store ηjn,k),i values in order to calculate the derivatives. In Iwf∙, we also end
up needing ηjr,k),i for r = n. In a network with n hidden layers, we would then have n"1) of the
ηj(r,k),i matrices to store. For n = 10, this would be 45, for n = 20, this would be 190, and so on.
This aspect of the calculations does not seem to scale well, but in practice, it is relatively simple to
work around. It is still necessary to store ηj(n,k),i, k < n, but ηj(r,k),i, r < n, only actually shows
up in one place, and thus it is possible to calculate each ηj(r,k),i matrix, use it, and discard it without
needing to store it for future calculations. The key thing to note about these second derivatives is
that they retain a low-rank structure - they are now tensor products (or the sums of tensor products)
of matrices and vectors. For example,
× ηs(n,k),j × v(k-1),t + ais × v(n),j × v(k-1),t	(6)
ais = 7⅛Γ Umnsn'k),m	(7)
∂pi∂pl
With these expressions, it would be relatively straightforward to extract the diagonal of the Hessian
and store or manipulate it as a vector. The rank of the weighted sum of low rank components
(as occurs with mini-batch sampling) is generally larger than the rank of the summed components,
however. As such, manipulating the entire Hessian may not be as computationally feasible; this
will depend on how large the mini-batch size is relative to the number of weights. The low rank
properties that we highlight here for the Hessian exist on a per-sample basis, as they did for the
gradient, and therefore, the computational savings provided by this approach will be most salient
when calculating scalar or vector quantities on a sample-by-sample basis and then taking a weighted
sum of the results. In principle, we could calculate third derivatives, but the formulae would likely
become unwieldy, and they may require memory usage significantly greater than that involved in
storing gradient information. Second derivatives should suffice for now, but of course ifa use arose
for third derivatives, calculating them would be a real option. Thus far, we have not included bias
terms. Including bias terms as trainable weights would increase the overall size of the gradient
(by adding additional variables), but it would not change the overall low-rank structure. Using the
calculations provided in Appendix B, it would not be difficult to produce the appropriate derivations.
3	Cubic Regularization in Deep Learning
Cubic Regularization (CR) is a trust region method that uses a cubic model of the objective function:
f (X) ≈ mj (Sj) = f (Xj) +ST ∂f + 2ST HjSj + 6 σj ksj k3	(8)
at the j-th iteration, where Hj is the objective function Hessian and sj = x - xj . The cubic term
makes it possible to use information in the Hessian without requiring convexity, and the weight σj on
4
Under review as a conference paper at ICLR 2018
that cubic term can have its own update scheme (based on how well m (sj ) approximates f (Kohler
& Lucchi, 2017)). Solving for an optimal sj value then involves finding the root of a univariate
nonlinear equation (Nesterov & Polyak, 2006). CR is not commonly used in deep learning; we
have seen only one example of CR applied to machine learning (Kohler & Lucchi, 2017) and no
examples with deep learning. This is likely the case because of two computationally expensive
operations: calculating the Hessian and solving for sj . We can overcome the first by using the low-
rank properties described above. The second is more challenging, but we can bypass it by using CR
to calculate a step length (i.e. the learning rate) for a given search direction rather than calculating
the search direction itself.
3.1	Implementation
Our approach in this paper is to use CR as a metamethod - a technique that sits on top of existing
training algorithms. The algorithm calculates a search direction, and then CR calculates a learning
rate for that search direction. For a general iterative optimization process, this would look like
xj+1 = xj + αj gj , where gj is the search direction (which need not be normalized), αj is the
learning rate, and the subscript refers to the iteration. With the search direction fixed, m would then
be a cubic function of α at each iteration. Solving d∂α = 0 as a quadratic equation in α then yields
α
-gTHg ± J(gTHg)2- 2(gTVf) (σ kgk3
σ l∣gk3
(9)
If we assume that gTVf < 0 (i.e. g is a descent direction), then α is guaranteed to be real.
Continuing under that assumption, of the two possible α values, we choose the one guaranteed to
be positive. The sampling involved in mini-batch training means that there are a number of possible
ways to get a final αjgj result. One option would be to calculate E [αj gj]. This would involve
calculating an α value with respect to the search direction produced by each sample point and then
averaging the product αg over all of the sample points. Doing this should produce an unbiased
estimate ofαjgj, but in practice, we found that this approach resulted in a great deal sampling noise
and thus was not effective. The second approach would calculate E [αj] × E [gj]. To do this, we
would calculate an α value with respect to the search direction produced by each sample point, as
in the first option, calculate an average α value, and multiply the overall search direction by that
average. This approach, too, suffered from excessive noise. In the interest of reducing noise and
increasing simplicity, we chose a third option: once the step direction had been determined, we
considered that fixed, took the average ofgTHg and gTVf over all of the sample points to produce
m (α) and then solved for a single αj value. This approach was the most effective of the three.
3.2	Computational Results
To test CR computationally, we created deep feedforward networks using ReLU activations in the
hidden layers, softmax in the output layer, and categorical cross-entropy as the error function; we
then trained them on the MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky & Hinton, 2009)
data sets. This paper shows results from networks with 12 hidden layers, each 128 nodes wide.
For the purposes of this paper, we treat network training strictly as an optimization process, and
thus we are not interested in network performance measures such as accuracy and validation error
- the sole consideration is minimizing the error function presented to the network. As we consider
that minimization progress, we will also focus on optimization iteration rather than wall clock time:
the former indicates the behaviour of the algorithm itself, whereas the latter is strongly dependent
upon implementation (which we do not want to address at this juncture). Overall computational
cost per iteration matters, and we will discuss it, but it will not be our primary interest. Further
implementation details are found in Appendix A.
Figure 1 shows an example of CR (applied on top of SGD). In this case, using CR provided little to
no benefit. The average learning rate with CR was around 0.05 (a moving average with a period of
100 is shown in green on the learning rate plot both here and in the rest of the paper), which was
close to our initial choice of learning rate. This suggests that 0.02 was a good choice of learning rate.
Another reason the results were similar, though, is that the optimization process did not run into any
5
Under review as a conference paper at ICLR 2018
(a) Error (SGD in blue, SGD with CR in red) (b) Learning rate (calculated rate in red, period-100
moving average in green)
Figure 1: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learning
rate = 0.01, σ = 100
(a) Error (SGD in blue, SGD with CR in red) (b) Learning rate (calculated rate in red, period-100
moving average in green)
Figure 2: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD); initial learning
rate = 0.02, σ = 100
plateaus. We would expect CR to provide the greatest benefit when the optimization gets stuck on
a plateau - having information about the objective function curvature would enable the algorithm
to increase the learning rate while on the plateau and then return it to a more typical value once it
leaves the plateau. To test this, we deliberately initialized our weights so that they lay on a plateau:
the objective function is very flat near the origin, and we found that setting the network weights to
random values uniformly sampled between 0.1 and -0.1 was sufficient.
(a) Error (SGD in blue, SGD with CR in red) (b) Learning rate (calculated rate in red, period-100
moving average in green)
Figure 3: Cubic Regularization (CR) applied to Stochastic Gradient Descent (SGD) on the CIFAR-
10 Dataset; initial learning rate = 0.01, σ = 1000
6
Under review as a conference paper at ICLR 2018
Figure 2 shows the results of SGD with and without CR when stuck on a plateau. There, we see
a hundred-fold increase in the learning rate while the optimization is on the plateau, but this rate
drops rapidly as the optimization exits the plateau, and once it returns to a more normal descent,
the learning rate also returns to an average of about 0.05 as before. The CR calculation enables the
training process to recognize the flat space and take significantly larger steps as a result. Applying
CR to SGD when training on CIFAR-10 (Figure 3) produced results similar to those seen on MNIST.
We then considered if this behaviour would hold true on other training algorithms: we employed CR
with Adagrad (Duchi et al., 2011) and Adadelta (Zeiler, 2012)on MNIST. The results were similar.
CR did not provide a meaningful difference when the algorithms performed well, but when those
algorithms were stuck on plateaus, CR increased the learning rate and caused the algorithms to exit
the plateau more quickly than they otherwise would have (Figures 4 and 5). The relative magnitudes
of those increases were smaller than for SGD, but Adagrad and Adadelta already incorporate some
adaptive learning rate behaviour, and good choices for the initial learning rate varied significantly
from algorithm to algorithm. We also used a larger value for σ to account for the increased variability
due to those algorithms’ adaptive nature. The result with Adadelta showed some interesting learning
rate changes: the learning rate calculated by CR dropped steadily as the algorithm exited the plateau,
but it jumped again around iteration 1200 as it apparently found itself in a flat region of space.
(a) Error (Adagrad in blue, Adagrad with CR in red)
(b) Learning rate (calculated rate in red, period-100
moving average in green)
Figure 4: Cubic Regularization (CR) applied to Adagrad; initial learning rate = 0.1, σ = 1000
(a) Error (Adadelta in blue, Adadelta with CR in red)
(b) Learning rate (calculated rate in red, period-100
moving average in green)
Figure 5: Cubic Regularization (CR) applied to Adadelta; initial learning rate = 1.0, σ = 1000
4	Discussion
We see this CR approach as an addition to, not a replacement for, existing training methods. It
could potentially replace existing methods, but it does not have to in order to be used. Because of
the low-rank structure of the Hessian, we can use CR to supplement existing optimizers that do not
explicitly leverage second order information. The CR technique used here is most useful when the
optimization is stuck on a plateau prior to convergence: CR makes it possible to determine whether
7
Under review as a conference paper at ICLR 2018
the optimization has converged (perhaps to a local minimum) or is simply bogged down in a flat
region. It may eventually be possible to calculate a search direction as well as a step length, which
would likely be a significant advancement, but this would be a completely separate algorithm.
We found that applying CR to Adagrad and Adadelta provided the same kinds of improvements that
applying CR to SGD did. However, using CR with Adam (Kingma & Ba, 2014) did not provide
gains as it did with the other methods. Adam generally demonstrates a greater degree of adaptivity
than Adagrad or Adadelta; in our experiments, we found that Adam was better than Adagrad or
Adadelta in escaping the plateau region. We suspect that trying to overlay an additional calculated
learning rate on top of the variable-specific learning rate produced by Adam may create interference
in both sets of learning rate calculations. Analyzing each algorithm’s update scheme in conjunction
with the CR calculations could provide insight into the nature and extent of this interference, and
provide ways to further improve both algorithms. In future work, though, it would not be difficult to
adapt the CR approach to calculate layer- or variable-specific learning rates, and doing that could ad-
dress this problem. Calculating a variable-specific learning rate would essentially involve rescaling
each variable’s step by the corresponding diagonal entry in the Hessian; calculating a layer-specific
learning rate would involve rescaling the step of each variable in that layer by some measure of the
block diagonal component of the Hessian corresponding to those variables. The calculations for
variable-specific learning rates with CR are given in Appendix B.
There are two aspects of the computational cost to consider in evaluating the use of CR. The first
aspect is storage cost. In this regard, the second-order calculations are relatively inexpensive (com-
parable to storing gradient information). The second aspect is the number of operations, and the
second-order calculations circumvent the storage issue by increasing the number of operations. The
number of matrix multiplications involved in calculating the components of Equation 9, for exam-
ple, scales quadratically with the number of layers (see the derivations in Appendix B). Although the
number of matrix multiplications will not change with an increase in width, the cost of naive matrix
multiplication scales cubically with matrix size. That being said, these calculations are parallelizable
and as such, the effect of the computation cost will be implementation-dependent.
A significant distinction between CR and methods like SGD has to do with the degree of knowledge
about the problem required prior to optimization. SGD requires an initial learning rate and (usually)
a learning rate decay scheme; an optimal value for the former can be very problem-dependent and
may be different for other algorithms when applied to the same problem. For CR, it is necessary
to specify σ, but optimization performance is relatively insensitive to this - order of magnitude
estimates seem to be sufficient - and varying σ has a stronger affect on the variability of the learning
rate than it does on the magnitude (though it does affect both). If the space is very curved, the choice
ofσ matters little because the step size determination is dominated by the curvature, and if the space
if flat, it bounds the step length. It is also possible to employ an adaptive approach for updating σ
(Kohler & Lucchi, 2017), but we did not pursue that here. Essentially, using CRis roughly equivalent
to using the optimal learning rate (for SGD).
5	Conclusions
In this paper, we showed that feedforward networks exhibit a low-rank derivative structure. We
demonstrate that this structure provides a way to represent the Hessian efficiently; we can exploit
this structure to obtain higher-order derivative information at relatively low computational cost and
without massive storage requirements. We then used second-order derivative information to imple-
ment CR in calculating a learning rate when supplied with a search direction. The CR method has
a higher per-iteration cost than SGD, for example, but it is also highly parallelizable. When SGD
converged well, CR showed comparable optimization performance (on a per-iteration basis), but the
adaptive learning rate that CR provided proved to be capable of driving the optimization away from
plateaus that SGD would stagnate on. The results were similar with Adagrad and Adadelta, though
not with Adam. CR also required less problem-specific knowledge (such as an optimal initial learn-
ing rate) to perform well. At this point, we see itas a valuable technique that can be incorporated into
existing methods, but there is room for further work on exploiting the low-rank derivative structure
to enable CR to calculate search directions as well as step sizes.
8
Under review as a conference paper at ICLR 2018
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from
tensorflow.org.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. In Advances in neural information processing systems, pp. 153-160, 2007.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic Hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21
(3):977-995, 2011.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Vladimir G Ivancevic and Tijana T Ivancevic. Applied differential geometry: a modern introduction.
World Scientific, 2007.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-
mization. arXiv preprint arXiv:1705.05933, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), pp. 735-742, 2010.
James Martens, Ilya Sutskever, and Kevin Swersky. Estimating the Hessian by back-propagating
curvature. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, pp. 963-970. Omnipress, 2012.
Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic BFGS algorithm. IEEE Trans-
actions on Signal Processing, 62(23):6089-6104, 2014.
Philipp Moritz, Robert Nishihara, and Michael Jordan. A linearly-convergent stochastic L-BFGS
algorithm. In Artificial Intelligence and Statistics, pp. 249-258, 2016.
Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global perfor-
mance. Mathematical Programming, 108(1):177-205, 2006.
Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 2006.
Nicol N Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-Newton method for online
convex optimization. In Artificial Intelligence and Statistics, pp. 436-443, 2007.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
9
Under review as a conference paper at ICLR 2018
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in neural information processing Systems, pp. 3320-3328, 2014.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
A Experimental Procedure
Starting at a point far from the origin resulted in extremely large derivative and curvature values (not
to mention extremely large objective function values), and this could sometimes cause difficulties
for the CR method. This was easy to solve by choosing an initialization point relatively near the
origin; choosing an initialization relatively near the origin also provided a significantly better initial
objective function value. We initialized the networks’ weights to random values between an upper
and lower bound: to induce plateau effects, we set, the bounds to ±0.1, otherwise, we set them to
±0.2.
All of the networks used a mini-batch size of 32 and were implemented in TensorFlow (Abadi et al.,
2015). The initial learning rate varied with network size; we chose learning rates that were large and
reasonable but perhaps not optimal, and for optimization algorithms with other parameters governing
the optimization, we used the default TensorFlow values for those parameters. For the learning rate
decay, we used an exponential decay with a decay rate of 0.95 per 100 iterations. The σ value used
is specified along with the initial learning rate for each network’s results. This value was also not
optimized but was instead set to a reasonable power of 10.
B Low-Rank Derivations for Deep Networks
B.1 Feedforward Network with ReLU Activations
Table 1 provides a nomenclature for our deep network definition.
Table 1: Nomenclature - Formulation
Quantity	Description
n	Number of hidden layers
Xi	Vector of inputs for a single sample
v(k),j	Vector output of layer k
w(k),j	Matrix of weights for layer k
A(∙)	Activation function
j Ui	Matrix of output layer weights
Pj	Vector of intermediate variables for the output layer
yl	Vector of outputs for a single sample
yl	Vector of labels for a single sample
f	Scalar objective function value for a single sample
F	Scalar objective function
Equations 10-16 define a generic feedforward network with ReLU activation functions in the hidden
layers, n hidden layers, a softmax at the output layer, and categorical cross-entropy as the objective
function.
10
Under review as a conference paper at ICLR 2018
v(k),j = A	wi(k),j v(k-1),i	, k = 1, . . . , n		(10)
A (z) = max (z, 0)		(11)
v(0),i = xi		(12)
pj = uijv(n),i		(13)
= eχp (Pj) y	P exp (pl) l		(14)
f = -ylln yl		(15)
F = E [f]		(16)
The relevant first derivatives for this deep network are		
A0 (z) =	1	z > 0 A (z) =	0 z < 0		(17)
f	0	l>k	
dv(k),j = f	δjA0 (Wy)jv(I),i) v(I),t dwil's	f AOjwy),jv(kτ),i)w(k),j∂v(k-⅛q i	∂wt ,	l=k	(18)
	l<k	
where there is no summation over j in Equation 18. We now define several intermediate quantities
to simplify the derivation process:
γs(k),j ≡ δsjA0	wi(k),jv(k-1),i
βi(k),j ≡A0	wl(k),jv(k-1),l	wi(k),j =γs(k),jwi(k),
Qk	βj(ii-),1ji	k>l
i=l+1
δji	k = l
0	k<l
(k,l),jk	(l,m),jl	(k,m),jk
αjl	αjm	= αjm
ηi(k,l),j ≡ α(sk,l),jγi(k),s
(19)
(20)
(21)
(22)
(23)
where there is no summation over j in Equations 19 and 20. We can now complete our calculations
of the first derivatives.
	∂v(k),j	0		l>k	(24)
		K—= ∂wt(l),s	ηs(k,l),j v	(l-1),t	l≤k	
		dPj	j 福=δlV	(n),k		(25)
		∂Pj -	T~~:~~7 = ∂v(n),i	uij		(26)
	∂f =	∂f ∂pk	f V(n)j		(27)
	duj 一	∂pk ∂uj			
∂f _	∂f ∂pl	∂v(n),m	=∂f	ulmηi(n,k),mv(k-1),j	(28)
∂wjk,-	一∂pl ∂v(n),m ∂w(k),i		=∂pl		
11
Under review as a conference paper at ICLR 2018
We then start our second derivative calculations by considering some intermediate quantities:
∂α
(n,k),m
q
∂w(r),s
A00 (z) = 0
α(an,r),mA0 wp(r),av(r-1),p δsaδbtα(qr-1,k),b = ηs(n,r),mα(qr-1,k),t
∂γSk)j
^Tr =
∂η(n,k),m = ∂αqn,k),m (k),q
∂w(r),s = ∂w(r),s Yi
∂2v(n),m
∂wjk),i∂w(r),s
(n,k),m∂v(kτ),j
η
∂α(n,k),m (k),q (k-1) j
nWtkγ(),qv(k 1)j
0
∂w(l),s
r>k
r=k
r<k
ηs(n,r),mηi(r-1,k),tv(k-1),j	r> k
0	r=k
ηi(n,k),mηs(k-1,r),jv(r-1),t	r < k
(29)
(30)
(31)
(32)
(33)
The second derivative of the ReLU vanishes, which simplifies the second derivative calculations
significantly. Technically, the second derivative is undefined at the origin, but the singularity is
removable, and thus we can define the second derivative to be 0 at the origin. We can then calculate
the second-order objective function derivatives:
d2f = df ∂pk ∂p = d2f v(n)j v(n),t	(34)
∂uij∂uts	∂pk∂pl ∂uij ∂uts	∂pi∂ps
∂2f _ ∂f ∂v⑺j ∂2f (n)j ∂pl ∂v(n),m
∂uj∂w(k),s — dPi ∂w(k),s + dPidP1 V dv(n),m ∂w(k),s
=dfη(n,k),j v(k-1),t + -d2f-7v(n),j u1mη(n,k),mv(k-1),t-	(35)
dPi s	dPidPl	m s
d2f _ d2f	dp1 dv(n)，m dpq dv(n)，a ι df dp1 d2v(n)，m
dw(k),idw(r),s = dpldpq dv(n),m dw(k)，i dv(n)，a dw(r)，s + dp1 dv(n),m dw(k),idw(r),s
=蒜 umη(n,k),mv(k-1),j uanSn，r)，av(r-1)，t
df ( ηSn,r),mηirτ,k),tV(I)j r>k
+÷7 Um ×{	0	r = k	(36)
dp	I ηin,k),mηSk-1,r) jv(r-1),t r < k
To use CR, we calculate αas
-gTHg + d(gTHg)2 - 2(gTVf) (σ kgk3)
σ l∣gk3
(37)
For a given iteration for the deep network described above (dropping the subscript j ’s so as not to
interfere with the index notation), the quantities in this equation are
12
Under review as a conference paper at ICLR 2018
gTVf = ∂Fmjωj + X ∂FUmη"m…，j
k
gT Hg=ω K v(n)，jv(n)，％s
+2 X ωj (∂FηSn'k),jv(I)，t + ∂p2∂pV(MUmη”"v(D,t) φ(k),
+ X φjk),i ∂∂F umη"),mv(I)，jua nSn，k)，aV(I) ,tφ(k),s
n k-1
+2 XX φ" ∂ττ Umn 厂"芯-⑺"vg"φRs
k=2 r=1	∂p
3
kgk3=(jωj+ X φj*jk))
(38)
(39)
(40)
With these formulae in hand, and using the generalized binomial theorem (√1 + C ≈ 1 + e/2 for
1), we can consider what happens to α in limiting cases:

α≈
gT ▽/
gTHg
gT Hg	gT Vf	σ kgk3	and gT Hg	>	0
gT Hg	gT Vf	σ kgk3	and gT Hg	>	0
gT Hg	gT Vf	σ kgk3	and gT Hg	<	0
The weight update scheme for a single learning rate at each iteration is
uj+1 = uj + αjωj
w(jk+)1 = wj(k) + αj φ(jk)
(41)
(42)
(43)
We could instead consider a weight-specific learning rate. If we assume that the baseline g vector
for each variable is a unit step in the direction of that variable, and we ignore superscripts indicating
the iteration number, the calculations for variable-specific learning rates αju)'i for Uj and ɑjw'k)'i
are as shown in Table 2:
Table 2: Variable-Specific Learning Rate Calculations
Learning Rate	gtVf	gT Hg	kgk3
ji (w，k)，i αj	dF v(n)，j	dF v(n)，jv(n)，t	1 ∂pi v	∂pi∂pSvv	1 ∂F Umη""v(kf3	∂p⅛ umη(n点mV(I)"做")，。V(I)，t	1
B.2 Convolutional and Recurrent Layers
Convolutional and recurrent layers preserve the low-rank derivative structure of the fully connected
feedforward layers considered above, and we will show this in the following sections. Because we
are only considering a single layer of each, we calculate the derivatives of the layer outputs with
respect to the layer inputs - in a larger network, those derivatives will be necessary for calculating
total derivatives via back-propagation.
13
Under review as a conference paper at ICLR 2018
B.2. 1	Convolutional Layer
We can define a convolutional layer as
Vs = A ((xs)k Wk)	(44)
(xs)k = χσs+k-1	(45)
where xij is the layer input, σ is the vertical stride, τ is the horizontal stride, A is the activation
function, and vts is the layer output. A convolutional structure can make the expressions somewhat
complicated when expressed in index notation, but we can simplify matters by using the simplifica-
tion ztskk	xτσts++kk--11. The layer definition is then
vs = A(Zsk WIk)	(46)
The derivatives of the convolutional layer are
	s	∂zsm ττ^i = A0 (ZskWk) Wm Atqq	(47) ∂xij	p ∂xij ∂Ztsqm	1 i = σs + m - 1 and j = τt + p - 1 "∂Xj = t 0	else	(48) s ∂Wp = A (ZtkWk) ztq	(49) 2 s	sm	sa ∂Xqp∂⅛ = A00 (Zsk Wk) Wm 京 Wa 徜	(50) d2pvs . = A0 (ZtkWk)爹 + A0 (ZtkWk) ZspWmdZsm	(51) ∂Wqp ∂xij	tk k ∂xij	tk k tq r ∂xij ∂Wp∂Wa = A0 (ZskWk) Zsq)Zsa	(52)
with no summation over s and t in any of the expressions above. Using the simplification with ztskk
makes it significantly easier to see the low rank structure in these derivatives, but that structure is
still noticeable without the simplification.
14
Under review as a conference paper at ICLR 2018
A0
∂vs
∂wP
dV =
∂xj
a ((χs)k Wk) (χs)p
A0
∂ 2vS
∂w1p∂wa
lk wkl	+A00
A00
∂2vS
∂Xp∂xj
A0
l l σs+p-1
k wkl xτt+q-1
lk wkl wji++11--στst i + 1 > σs,j + 1 > τt
0	else
A00
l l σs+p-1 σs+a-1
k wk xτ t+q-1 xτ t+b-1
∂2vS
∂wg∂xj
l l i+1-σs σs+p-1
k wk wj+1-τ txτ t+q-1
lk wkl	wji++11--στstxτσts++qp--11
A00
l wl wi+1-σswp+1-σs
k wk wj+1-τ t wq+1-τ t
(53)
(54)
(55)
i + 1 - σs = p, j + 1 - τt = q
i + 1 - σs > 0, i + 1 - σs 6= p,
j + 1 - τt > 0, j + 1 - τt 6= q
else
(56)
p + 1 > σs, i + 1 > σs
q+ 1 > τt,j + 1 > τt
(57)
else
0
0
The conditional form of the expressions is more complicated, but it is also possible to see how the
derivatives relate to wji and submatrices of xij .
B.2.2 Recursive Layer
We can define our recursive layer as
v(jt) = A wijv(it-1)
jj
v(0) = x
(58)
(59)
where t indicates the number of times that the recursion has been looped through. If we inspect this
carefully, we can actually see that this is almost identical to the hidden layers of the feedforward
network: they are identical if we stipulate that the weights of the feedforward network are identical
at each layer (i.e. wj(k),i = wji ∀k) and if we treat the recursive loops like layers. This observation
allows us to reuse some of our previous derivations. Primarily, we will use the fact that
∂(∙)
∂wj
X ∂(∙)	∂wjk),i
V ∂wPk),m ∂wj
X d S
J ∂Wjk7
(60)
The first-order derivatives are then
∂v(mt)
-----=
∂wrs
dvm)
∂xi
ηs(t,k),mv(rt-1)
k
ηs(t,1),mws
(61)
(62)
If A is a ReLU, then the second derivatives are relatively simple
15
Under review as a conference paper at ICLR 2018
叫)=X	d%)
dWdwr	kp ∂wjk)"∂wrp),q
t k-1
2XXηi(t,k),mηq(k-1,p),jv(rp-1)
k=2 p=1
工=0
∂xi∂xl
六「=η""δS + X *二 Ws
∂wrq∂xi s i	∂w(k),s i
(63)
(64)
ηs(t,1),jδis +	ηq(t,k),mηs(k-1,1),rwis	(65)
k
If A is not a ReLU, then we would use the results in the next section to calculate the second deriva-
tives. Regardless of the exact form of A, though, we retain the low-rank structure as long as A is an
entry-wise function of its arguments.
B.3 Deep Network with General Activation Functions
For a deep network with general entry-wise functions, the first derivatives are all identical to the
derivations given in Section 2 save that A0 will be different. Before calculating second-order deriva-
tives, though, we do some preliminary calculations that were not necessary before because A00 = 0
for ReLUs. First, we calculate the derivatives of γr(l),m:
{0	l < q
δmA00 (Wjl),mv(lT),j) δmv(IT),t	l = q
δmA00 (Wjl)，mv(lT),j) Wal) ,mη( j，q)，av(qT)，t	ι> q
λ(rls),m ≡ δrmδsmA00 (Wj(l),mv(l-1),j
0	l<q
λ(rls),mv(l-1),t	l=q
λ(rlp),mWa(l),pηs(l-1,q),av(q-1),t l > q
(66)
(67)
(68)
where there is no summation over m in any of these equations. Next, we calculate the derivatives of
α(mk,l),j:
16
Under review as a conference paper at ICLR 2018
α
(k,l),j
m
α
(k,r),j α(r,r-1),bα(r-1,l)
b αa αm
,a
α(ar,r-1),b = βa(r),b = γs(r),bwa(r),s
∂β(r, = J
∂w(q),s ɪ
0
λ(prs),bwa(r),pv(r-1),t + γs(r),bδat
λ(drp),bwa(r),dwm(r),pηs(r-1,q),mv(q-1),t
r<q
r=q
r>q
(69)
(70)
(71)
∂αm,l),j = X
E = r=+ια
(k,r)j dβa ), α(rT,l),a
b	∂w(q,
0
k<q
k
P
r=q+1
+α(k,q),j
k
P
r=l+1
),bwa(r),dwc(r),pηs(r-1,q),cv(q-1),tα(mr-1,l),a
,bwa(q),pv(q-1),t + γs(q),bδat α(mq-1,l),a
),bwa(r),dwc(r),pηs(r-1,q),cv(q-1),tα(mr-1,l),a
Thirdly, we calculate the derivatives of ηl(n,k),j :
l<q≤k
q≤l
(72)
(n,k),j	(n,k),j	(k),m
d% = dαm	γ(k),m + α(n,k),j dγl
∂w(q),s	∂w(q),s	l	m	∂w(q,
∂η(n,k),j
∂w(q,
n
P α(bn,r),jλ(drp),bwa(r),dwc(r),pηs(r-1,q),cv(q-1),tα(mr-1,k),aγl(k),m
r=q+1
+α(bn,q),j λ(pqs),bwa(q),pv(q-1),t + γs(q),bδat αm(q-1,k),aγl(k),m
n
P α(bn,r),jλ(drp),bwa(r),dwc(r),pηs(r-1,k),cv(k-1),tαm(r-1,k),aγl(k),m
r=k+1
+α(mn,k),jλl(sk),mv(k-1),t
q>k
q=k
(73)
(74)
n
P	α(bn,r),jλ(drp),bwa(r),dwc(r),pηs(r-1,k),cv(k-1),tαm(r-1,k),aγl(k),m
r=k+1
+α(mn,k),jλl(pk),mwa(k),pηs(k-1,q),av(q-1),t
n
P a”)j λdP%w Cdwp中肃-1，m。V(LI),tη(rτ,k),a
r=q+1
+αbn,G,jλΡ2,bwar),Pv(LI) ,tη(qτ,k),a + ηSn,q),j η(qτ,k),t
n
J P α(bn,r),jλ(drp),bwa(r),dwc(r),pηs(r-1,k),cv(k-1),tηl(r-1,k),a
r=k + 1
+αm≠),jλ(k),mv(I) ,t
n
P αbn,r),jλdp),bwar),dwCr),pηSrτ,k),cV(I),tη(rτ,k),a
r=k + 1
+αmn,k),jλ(k),mwak),pηSkτ,q),avsτ),t
q<k
q>k
q=k
q<k
(75)
17
Under review as a conference paper at ICLR 2018
λ2 (n) j	f	劭(：?“ v(k-1),i	≥ k
∂2v⑺j	= I	∂w(6s V	q ≥ k	(76)
∂w^l∂w^s	∖ ^j V(AT),i + η 尸，，竽蔡■ q<k
I ∂wt	"	∂wt
'	n
P α")jλdp)%"dwCr)PnsrT，q)，CV(LI)"n(i，k)，av(I),i
r=q+1	q > k
+αbnMjλΡ0MQ)PV(LI)"n(qT，k)，aV(I),i+ nsnM,j n(qT,k)"V(I),i
n
P α”),jλdr)%"dwCr)PnsrTk)，cV(I)"n『T，k)，aV(I),i
=<	r=k+ι	q = k
+αlmM,j 烦冲出TNVni
n
P α")jλdr)%"dWCr)PnsrTk),cV(I)"n『T，k)，aV(I),i
r = k + 1	q < k
+α 相k)"λamwak),pnskTq),aV(qT),tV(kτ),i + η(n'k'),j，储-"'%&TN
(77)
The second-order derivatives of the objective function are then
∂ 2F
∂UIi ∂us
j t
d2F dPk dp = d F V(n),j V(n),t
∂pk ∂pl ∂uj ∂us	∂pi∂ps
(78)
∂2F	_ ∂F ∂V(n),j	∂2F	(n),j ∂pl ∂V(n),m
∂uj∂w(k),s — ⅜i ∂w(k),s +加加 V 讥⑺冲 ∂w(k),s
IF. n(n，k),j V(k-1),t + J2J—
∂pi	∂pi∂pl
V(n),j umηsn'k),mV(k-1),t
(79)
∂2F	_	∂ 2F	∂pa	∂V(n),j	∂pb	∂v5),C	∂F	∂pm	∂2V(n),j
∂w(k),l∂w(q),s	∂pa∂pb ∂V(n),j aw®，1 ∂v5),c ∂w(q),s + ∂pm ∂V(n),j ∂w(k),l∂w(q),s
∂ 2F
∂pa∂pb
a (n,k),j (k—1),i b (n,q),Ct,(qT),t + ∂F 〃a	∂ V''"
"n	uC"	+∂pauj ∂wi(k),l∂w”
(80)
18