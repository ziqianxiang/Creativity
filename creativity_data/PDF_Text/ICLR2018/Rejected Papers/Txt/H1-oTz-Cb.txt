Under review as a conference paper at ICLR 2018
Parametrizing filters of a CNN with a GAN
Anonymous authors
Paper under double-blind review
Ab stract
It is commonly agreed that the use of relevant invariances as a good statistical
bias is important in machine-learning. However, most approaches that explicitely
incorporate invariances into a model architecture only make use of very simple
transformations, such as translations and rotations. Hence, there is a need for
methods to model and extract richer transformations that capture much higher-
level invariances. To that end, we introduce a tool allowing to parametrize the
set of filters of a trained convolutional neural network with the latent space of a
generative adversarial network. We then show that the method can capture highly
non-linear invariances of the data by visualizing their effect in the data space.
1	Introduction
In machine-learning, solving a classification task typically consists of finding a function f : X → Y,
from a rather large data space X to a much smaller space of labels Y. Such a function will there-
fore necessarily be invariant to a lot of transformations of its input data. It is now clear that being
able to characterize such transformations can greatly help the learning procedure, one of the most
striking examples being perhaps the use of convolutional neural networks (CNN) for image classifi-
cation (Krizhevsky et al., 2012), with built-in translation invariance via convolutions and subsequent
pooling operations. But as a convolutional layer is essentially a fully connected layer with a con-
straint tying some of its weights together (LeCun et al., 1995), one could expect other invariances
to be encoded in its weights after training. Indeed, from an empirical perspective, CNNs have been
observed to naturally learn more invariant features with depth (Goodfellow et al., 2009; Lenc &
Vedaldi, 2015), and from a theoretical perspective, it has been proven that under some conditions
satisfied by the weights of a convolutional layer, this layer could be re-indexed as performing a
convolution over a bigger group of transformations than only translations (Mallat, 2016).
It is exciting to note that there has recently been a lot of interest in theoretically extending such suc-
cessful invariant computational structures to general groups of transformations, notably with group
invariant scattering operators (Mallat, 2012), deep symmetry networks (Gens & Domingos, 2014),
group invariant signal signatures (Anselmi et al., 2015), group invariant kernels (Mroueh et al.,
2015) and group equivariant convolutional networks (Cohen & Welling, 2016). However, practical
applications of these have mostly remained limited to linear and affine transformations. Indeed, it is
a challenge in itself to parametrize more complicated, non-linear transformations preserving labels,
especially as they need to depend on the dataset. In this work, we seek to answer this fundamental
question:
What invariances in the data has a CNN learned during its training on a classification task and
how can we extract and parameterize them?
The following is a brief summary of our method: Considering an already trained CNN on a labeled
dataset, we train a generative adversarial network (GAN) (Goodfellow et al., 2014) to produce filters
of a given layer of this CNN, such that the filters’ convolution output be indistinguishable from the
one obtained with the real CNN. We combine this with an InfoGAN (Chen et al., 2016) discrim-
inator to prevent the generator from producing always the same filters. As a result, the generator
provides us with a smooth, data-dependent, non-trivial parametrization of the set of filters of this
CNN, characterizing complicated transformations irrelevant for this classification task. Finally, we
describe how to visualize what these smooth transformations of the filters would correspond to in
the image space.
1
Under review as a conference paper at ICLR 2018
2	Background
2.1	Generative Adversatial Networks
A Generative Adversarial Network (Goodfellow et al., 2014) consists of two players, the generator
G and the discriminator D, playing a minimax game in which G tries to produce samples indis-
tinguishable from some given true distribution pdata , and D tries to distinguish between real and
generated samples. G typically maps a random noise z to a generated sample G(z), transforming
the noise distribution into a distribution pg supposed to match pdata . The objective function of this
minimax game with maximum likelihood is given by
mGnmDax V(D, G)= Ex〜Pdata [log D(X)] + Ez[lOg(I- D(G(Z)))].
The noise space, input of the generator, is also called its latent space.
2.2	Information Maximizing Generative Adversarial Nets
In an InfoGAN (Chen et al., 2016), the generator takes as input not only the noise z but also another
variable c, called the latent code. The aim is to make the generated samples G(z, c) depend on
c := (c1 , ..., cn) in a structured way, for instance by choosing independent ci’s, modelling p(c) as
Qi p(ci). In order to avoid a trivial correspondence between c and G(z, c), the InfoGAN procedure
maximizes the mutual information I(c, G(z, c)).
The mutual information I(X, Y ) between two random variables, defined with an entropy H as
I(X, Y ) := H(X) - H(X|Y ) = H(Y ) - H(Y |X) is symmetric, measures the amount of infor-
mation that is known about the value of one random variable when the value of the other one is
known, and is equal to zero when they are independent. Hence, maximizing the mutual information
I(c, G(z, c)) prevents G(z, c) from being independent of c.
In practice, I(c, G(z, c)) is indirectly maximized using a variational lower bound
LI (G, Q) ≤I(c,G(z,c)),
where Q(c|x) approximates P (c|x) and
LI(G, Q) ：= Ec〜p(c),x〜G(z,c) [log Q(c∣x)] + H(c).
The minimax game becomes
minmaxVI(D,G,Q) = V(D, G) - λLI(G, Q),
G,Q D
where λ is a hyperparameter controlling the mutual information regularization.
3	Extracting Invariances by Learning Filters
Let CNN be an already trained CNN, whose 'th-layer representation of an image I will be denoted by
CNN'(I), with CNNo(I) = I. As our goal is to learn What kind of filters such a CNN would learn at
layer `, it could be tempting to simply train a GAN to match the distribution of filters of this CNN’s
layer. However, this set of filters is way too small of a dataset to train a GAN (G, D), which would
cause the generator G to massively overfit instead of extracting the smooth, hidden structure lying
behind our discrete set of filters. To cope with this problem, instead of feeding the discriminator
D alternatively with filters produced by G and real filters from CNN', We propose to feed D with
the activations of these filters caused by the data passing through the CNN, i.e. alternatively with
CNN'(I) or Conv(CNN'-1(I), G(z)), corresponding respectively to real and fake samples. Here, I is
an image sampled from data, z is sampled from the latent space of G and Conv(CNN'-1 (I), G(z))
is the activation obtained by passing I through each layer of CNN but while replacing the filters of
the 'th-layer by G(z).
In short, in each step, the generator G produces a set of filters for the 'th-layer of the CNN. Next,
different samples of data are passed through one CNN using its real filters and through the same
CNN, but having its 'th-layer filters replaced by the fake filters produced by G. The discriminator
2
Under review as a conference paper at ICLR 2018
D will then try to guess if the activation it is fed was produced using real or generated filters at
the 'th-layer, while the generator G will try to produce filters making the subsequent activations
indistinguishable from some obtained with real filters.
However, even though this formulation allows us to train the GAN on a dataset of reasonable size,
saving us from an otherwise unavoidable overfitting, G could a priori still always produce the same
set of filters to fool D. Ideally it simply reproduces the real filters of CNN'. To overcome this
problem, we augment our model by an InfoGAN discriminator whose goal will be to predict which
noise Z was used to produce Conv(CNN'-ι(I), G(z)). This prevents G from always producing the
same filters, by preventing z and G(z) from being indenpendent random variables. Note that, just
as the GAN discriminator, the InfoGAN discriminator does not act directly on the output of G - the
filters - but on the activation output that these filters produce.
In this setting, the noise z of the generator G plays the role of the latent code c of the InfoGAN. As
in the original InfoGAN paper (Chen et al., 2016), we train a neural network Q to predict the latent
code z, which shares all its layers but the last one with the discriminator D. Finally, by modelling
the latent codes as independent gaussian random variables, the term LI (G, Q) in the variational
bound being a log-likelihood, it is actually given by an L2 -reconstruction error. The joint training
of these three neural networks is described in Algorithm 1 and illustrated in Figure 1.
ta
a
D
Real Layer L
r	ʌ
Predicts
real / fake
signal D
Fake Layer L
Predicts Z
from signal
SJg=LL
Figure 1: Illustration of how the different neural networks interact with each other. CNN layers are
depicted in light gray. The flow of data is shown in green, while the generation of the filters by the
generative model is shown in red. The discriminator part of the GAN is shown in blue. Note that
the discriminator does not have direct access to the generated filters, but can only observe the data
after it has passed through them. The CNN is fixed, while G, D and Q are trained jointly.

Q
3
Under review as a conference paper at ICLR 2018
Algorithm 1 Minibatch stochastic gradient descent training of D, G and Q.
1:	for number of training iterations do
2:	• Sample minibatch of m noise samples {z(1), ..., z(m)} from noise priorpnoise(z).
3:	• Generate the filters Gθg (z(1)), ..., Gθg (z(m)).
4:	• Sample minibatch of m examples {I(1), ..., I(m)} from data distribution pdata(I).
5:	• Pass the data through the CNN with the real and generated filters, to obtain the CNN'(I(i) ),s
and Conv(CNN'-ι(I (i)), Gθg (z(i)))’s respectively.
6:	• Feed these to Dθd and Qθq, letting Dθd guess if it was fed CNN'(I(i)) or
Conv(CNN'-ι(I(i)), Gθg (z(i))), and letting Qθq recover the Z⑶.
7:	• Update the discriminator by ascending its stochastic gradient:
m
Vθd — X[logDθd(CNN'(I⑴))+ log(1 - Dθ~(Conv(CNN j(I⑴),Ge.(z⑴))))].
m
i=1
8:	• Update the generator by descending its stochastic gradient:
m
Vθg — X[log(1 - Dθd (Conv(CNN'-i(I⑺)，Gθ, (z⑴))))
m
i=1
+ λkz(i) -Qθq(Conv(CNN'-1(I(i)),Gθg(z(i))))k22].
9:	• Update the InfoGAN discriminator by descending its stochastic gradient:
Vθq & X kz(i) - Qθq (Conv(CNN'-i(I⑴),Gθ,(z⑴)))k2).
end for
10:	The gradient-based updates can use any standard gradient-based learning rule. We used RM-
Sprop in our experiments.
4	Visualizing the learned transformations
Using our method, we can parameterize the filters ofa trained CNN and thus characterize its learned
invariances. But in order to assess what has actually been learned, we need a way to visualize these
invariances once the GAN has been trained. More specifically, given a data sample x, we would like
to know what transformations of x the CNN regards as being invariant. We do this in the following
manner:
We take some latent noise vector z and obtain its generated filters G(z). Using those filters, we pass
the data sample x through the network to obtain Conv(CNN'-1 (x), Gθg (z)) =: a(x|z), which we
call the activation profile of x given z .
Next we select two dimensions i andj ofz at random and construct a grid of noise vectors {zk|zki ∈
[-ξ, ξ], zkj ∈ [-ξ, ξ]}k by moving around z in the dimensions i and j in a small neighborhood.
For each zk, we use Gradient Descent to start from x and find the data point x0k that gives the same
activation profile for the filters generated using zk as the data point x gave for the filters generated
using z. Formally, for each zk we want to find x0k, s.t.
x0k = argminka(x0|zk) - a(x|z)k22 + Ψ(x0),
x0
where Ψ(x0) is a regularizer corresponding to a natural image prior. Specifically, we use the loss
function proposed in (Mahendran & Vedaldi, 2015).
By using Gradient Descent and starting from the original data point x, we make sure that the solution
we find is likely in the neighborhood of x, i.e. can be obtained by applying a small transformation
to x.
4
Under review as a conference paper at ICLR 2018
As a result, from our grid of z-vectors, we obtain a grid of x-points. This grid in data space represents
a manifold traversal in a small neighborhood on the manifold of learned invariances. If our method
is successful, we expect to see sensible continuous transformations of the original data point along
the axes of this grid.
5	Experimental Results
We apply our method of extracting invariances on a convolutional neural network trained on the
MNIST dataset. In particular, we train a standard architecture featuring 5 convolutional layers with
ReLU nonlinearities, max-pooling and batch normalization for 10 epochs on the 10-class classifica-
tion task.
Once converged, we use our GAN approach to learn the filters of the 4th convolutional layer in the
CNN. Since this is one of the last layers in the network, we expect the invariances that we extract to
be very high-level and highly nonlinear.
5.1	Visualizing the learned invariances
The results can be seen in Figure 3 and a sample of the learned filters themselves can be seen in
Figure 2. Our expectations are clearly met as the resulting outputs are in fact an ensemble of highly
nonlinear and high-level transformations of the data. Even more visualizations can be found in the
Appendix.
We further hypothesize that if we apply the same method to the filters of one of the first layers in the
network, the transformations that we learn will be much more low-level and more pixel-local. To
test this, we use our method on the same CNN’s second convolutional layer. The results can be seen
in Figure 4. As expected, the transformations are much more low-level, such as simple brightness
changes or changes to the stroke width.
U« WiNff
Figure 2: Learned filters of the CNN’s 4th layer. We summed one third of the orignal channels
together in order to visualize the learned filters.
5
Under review as a conference paper at ICLR 2018
££次安££次T0
史史史£^次次中⅛
比次2④ææ出次由
比史松Z*垓
6登5⅛⅛⅛⅛∙⅛既
史6登5⅛⅛⅛⅛原
E5登5登 5⅛⅛为
3
-fljfliflifli(ɪi[ɪi仅仅广
≡⅛iaIIJ6mia[δ∙
0il!扬00PJ值田」
◎⑦^⅛λ≡[u俗俗广
0Q,0Q∙QflfIVl≡广
国与'与黑当2)^//
s≈⅛肾 N^为肾 N 胃 ⅛⅛⅛,00⅛^s
s3sm 肾/肾/ Na 必 W 0!⅛⅛⅛^
≈s肾肾k身质身焉a " “ "⅛国0国
SS肾学肾N肾肾N觉” “ ”，好⅝⅛
艰S255白耳&5焉N
旗s⅛⅛> 3 & N 肾N
OBQO,
Oe海
QQQ¾¾¾.
o^o⅛⅛⅛.
aLF
92微及F
刍卷也F
O4^¾⅛⅛
一归卓空
一户宜百
q Sq
q q⅛
,q .qΛ.
Λg.4.
H 4.4.
α.qΛ.
q q q
222⑵,2,
2,2 必&
Figure 3: Invariance transformations extracted from the CNN’s 4th layer. The middle sample of
each grid represents the original data sample, while the rest of the grid are found by matching the
original sample’s activation profile.
PIR I
1 ・7，1，
Λ^⅛⅛÷⅛⅛⅛⅛
ΛΛΛλvrtvdvdvΛ⅛
QeGG⅛⅛⅛⅛6⅛
⅛⅛⅛⅛⅜⅜⅜⅛⅛ 不
RM Mmyq q qc.
fh¾⅛i⅞⅛E⅛≡^l>
⅛⅛∙a,⅛-s,⅛*s,⅛1⅛
⅛>⅛⅛ja,∙8∙⅛*a*⅛1⅛
⅛⅛喻⅛⅛-⅛fy-⅛1 al
⅛⅛⅛a)Aσ⅛⅛¾p5
^⅛⅛⅛⅛⅛⅛⅛^
⅛⅛⅛⅛⅛⅛⅛⅛⅛
递降同'⅛''⅛'⅛徇⅛⅛l⅛
6
Under review as a conference paper at ICLR 2018
3 3 3 3 3 3
S 3 3 3 3 3 3 s3
wsss⅛Q3i
W S0⅛3⅛3r
■ SW3 3 3⅛.二
Ss⅛3⅛s3一
∙ SS⅛ss 二二
二33SSS 3 Sw
N-33s 3 33S3
OOooOoo
0。<>图0 0 0
0。西昼0 0
。口母号。
0 0⅛J¾⅛0 0
O。0⅛: O
Jjjjjjjj ...
J;嚼替
∙⅜≡2
JJ.
二软3? ,
JJJJJJWW
Figure 4: Invariance transformations extracted from the CNN’s 2nd layer. The middle sample of
each grid represents the original data sample, while the rest of the grid are found by matching the
original sample’s activation profile.
7
Under review as a conference paper at ICLR 2018
5.2 Assessing the quality of the generator
In order to assess the quality of the generator, we need to be sure that: (i) filters produced by the
generator would yield a good accuracy on the original classification task of our CNN, and (ii) the
generator can produce a variety of different filters for different latent noises.
For the first part, we randomly drew 10 noise vectors z(1) , ..., z(10), computed the corresponding
set of filters G(z(i)) for each of them, and then each data sample x, after going through CNNl-1, is
passed through each of these 10 'th-layer and averaged over them, so that the signal fed to the next
layer becomes:
i=1
all the next layers being re-trained. This averaging can be seen as an average pooling w.r.t. the
transformations defined by the generator, which, if the transformations we learned were indeed
irrelevant for the classification task, should not induce any loss in accuracy. Our expectations are
confirmed, as the test accuracy obtained by following the above procedure is of 0.982, against a test
accuracy of 0.971 for the real CNN.
As for the second part, Figure 5 shows a Multi-Dimensional Scaling (MDS) of both the original
set of filters of CNN', and of generated filters for randomly sampled noise vectors. We observe that
different noise vectors produce a variety of different filters, which confirms that the generator has
not overfitted on the set of real filters. Further, since the generator has learned to produce a variety of
filters for each real filter, all the while retaining its classification accuracy, this means that we have
truly captured the invariances of the data with regard to the CNN’s classification task.
Figure 5: Multi-Dimensional Scaling for the filters produced by the GAN. Individual colors rep-
resent different samples for the same filter of the true CNN. The large cluster sizes shows that the
GAN is producing a wide variety of different filters for each corresponding real filter.
6 Conclusion and future work
Introducing an invariance to irrelevant transformations for a given task is known to constitute a
good statistical bias, such as translations in CNNs. Although a lot of work has already been done
regarding how to implement known invariances into a computational structure, practical applications
of these mostly include very simple linear or affine transformations. Indeed, characterizing more
complicated transformations seems to be a challenge in itself.
In this work, we provided a tool allowing to extract transformations w.r.t. which a CNN has been
trained to be invariant to, in such a way that these transformations can be both visualized in the
image space, and potentially re-used in other computational structures, since they are parametrized
by a generator. The generator has been shown to extract a smooth hidden structure lying behind the
discrete set of possible filters. It is the first time that a method is proposed to extract the symmetries
learned by a CNN in an explicit, parametrized manner.
8
Under review as a conference paper at ICLR 2018
Applications of this work are likely to include transfer-learning and data augmentation. Future work
could apply this method to colored images. As suggested by the last subsection, the parametrization
of such irrelevant transformations of the set of filters could also potentially define another type of
powerful pooling operation.
References
Fabio Anselmi, Joel Z Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso Poggio.
Unsupervised learning of invariant representations in hierarchical architectures. Theoret. Comput.
Sci., dx.doi.org/10.1016/j.tcs.2015.06.048, 2015.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172-2180, 2016.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In International Confer-
ence on Machine Learning, pp. 2990-2999, 2016.
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in neural information
processing systems, pp. 2537-2545, 2014.
Ian Goodfellow, Honglak Lee, Quoc V Le, Andrew Saxe, and Andrew Y Ng. Measuring invariances
in deep networks. In Advances in neural information processing systems, pp. 646-654, 2009.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361(10):1995, 1995.
Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equiv-
ariance and equivalence. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 991-999, 2015.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188-5196, 2015.
Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics,
65(10):1331-1398, 2012.
Stephane Mallat. Understanding deep convolutional networks. Phil. Trans. R. Soc. A, 374(2065):
20150203, 2016.
Youssef Mroueh, Stephen Voinea, and Tomaso A Poggio. Learning with group invariant features:
A kernel perspective. In Advances in Neural Information Processing Systems, pp. 1558-1566,
2015.
9
Under review as a conference paper at ICLR 2018
A More Invariance Visualizations
NYYNZ 一
.???»、.C :
ZNa20。口
02?NaZb
0
川
1if⅛
l<κ(


H H ■ --.,
≡r≡r"∕≡f≡H 皿

Iii
_ri .

月空浇用料网网⅛!w⅛!时慰
H泠月当闵流空号四界押5⅛⅛⅛⅛一D0;
q q⅛胃用流砥5¼⅛w钳M⅛由d⅛>3 ,: ⅛cl
AΛ/M 产用凌2 y 为⅛⅛01⅛⅛∖B⅛⅛与 >⅛A∙⅛a暇
3 333 & 0 g.取 R 集笆合⅛a∙⅛∙⅛∙⅛¾f.麻
但l⅛卷一惮吟窝空⅛罟⅛⅛g⅛出⅛a 由∙¾∙⅛∙⅛∙⅛⅛0α1

3 q,Λ3g03 0S一
.q .q ,q..q .q∙,gΓ- ■
曾言„£沦泊一
，/ I ， , ‘ t 1■ «- › /
‘尸C
∕77∕^∕^V77
∕λλλ^aλλ
—，— X信售盘盘忠忠信4£品用置Sr龄引王必>_*___________
.；：，——」 * 4:; ——r'§444夕
・㈱翻勰潮
歹
声争正夕⅛谑
算卡寮l∖√⅛
初亲声察炉歹一
一 ∙1 tt tl f SlVtmHIJ —1»I，j ,
I⅛⅛⅛ΘΘIHKHW¾I∙⅛∙⅛∙⅛∙⅛^⅛>∙⅛⅛
鬻■.瞬觌
,帚於
.⅛⅛⅛w⅛⅜
F<rj<rl^⅛⅛3
H¾¾¾⅛⅛i
孑 d ^⅛⅛B⅜
w∙ ^>>>.⅛⅛⅛
s⅛
CJG
2ZZ22ZZZ Z
-KlC 七 W Wmu

qi:;TEL______)Jf > H:火及
Q4咨庄普王岗46必6负2¾置2 Z
C•«•<衿N⅛GW4%4国股切
二64W4百岗益参忆
EG⅛wq∕Λ百岗岗 L7'
邙心臼均44A44
in
2 2,^R4J∖f 已回⑵
τvεvεv¾IR一
p.evevεv¾一
,p.p.p.p⅛
pp⅛.eve√
p,pp.p,.一
B..2WA.2_
。"p. MeV-
“pτvεvev
。力Qe.eυ
s≈s2f'<⅛2r2f^/
s'≈¾≈9 G///
我与当当白白///
2)⅛2)白
勾与另当白^为身^
肾百肾学当/肾肾N
身名^行昭肾肾肾N
Figure 6: Invariance transformations extracted from the CNN’s 4th layer. The middle sample of
each grid represents the original data sample, while the rest of the grid are found by matching the
original sample’s activation profile.
10
Under review as a conference paper at ICLR 2018
√ √ √ ⅞/ √ √ 〃出2
√ √ √ √ √，:“2f⅛Γ
√ √ √ √ √ “:«?3以
才，娟娟&笈?d
，，，〃/涅笈笈?d
"Y 〃?d笈笈笈?d
““〃”之笈也笈料
，"却我戈》笈a弘
，■ , ■ « 1- ■ 1- ■ 1- ■ ■ f -J - - J
3 '3 '2 '2 '2 '? 2 2 2
H次贝＜⅛0瀛原
'S、8 H ■烟‹?谈2
图＜i⅛¾婢飒挚
⅛J5⅛?:烟海萃
次迦加史於以
WG 'q.'q. McqAPCa
若缶俗缶皆川

Iy胃G暂焉
3⅛⅛¾

QQQQXWW¾
QQQQZWW5
^0^^⅛⅛⅛w⅛
0^0⅛l^⅛⅛
β⅛⅛⅛o⅛⅛s
D^^l⅛⅞s⅛
ps^§§,
ɔ 2 2 2 <vs∙-2n
eee幺刍幺幺白r
幺幺幺里幺¾¾2;
NNNNNNNaZ
IOrVMiM λ∙I*-Γ<*-<¥"
%⅛⅛⅛⅛⅛舅谢
出话.¾∙簿⅛4(
Im;;;; .√E⅛⅛⅛占冷熏5常臼玄益尚5^!p,!
's'5^^<5<⅛^⅛.然⅛⅛⅛B有3¾em>m>,⅛
----------------------------相定潦米哈哈黑黑琮96」446幺必4患
N⅛⅛-Λ应ΛΛI祗 ^
丁 14%_
^∙31⅞⅜⅞¾.
一4百以4片4笈度
就格法翦第M
至决就饭叨沙纪
卷於猿求患访
卷软5盘超
⅛,¾^⅛⅜⅞
决出记£££££
L渝
芍 44纣444^5>聚俄¾⅛?篁ya。⅛
" ' < 'I ' I -- CΛZn∙'ncncnAnL
1，，：，：，■ Is —4 * W L- b^J⅝∙i∙-*; <*- ,p)p:j^ɛŋz^ŋ/?ŋr^ŋ、OWI0,0

Figure 7: Invariance transformations extracted from the CNN’s 4th layer. The middle sample of
each grid represents the original data sample, while the rest of the grid are found by matching the
original sample’s activation profile.
11