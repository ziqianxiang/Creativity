Under review as a conference paper at ICLR 2018
Diffusing Policies : Towards Wasserstein
Policy Gradient Flows
Anonymous authors
Paper under double-blind review
Ab stract
Policy gradients methods often achieve better performance when the change in
policy is limited to a small Kullback-Leibler divergence. We derive policy gradi-
ents where the change in policy is limited to a small Wasserstein distance (or trust
region). This is done in the discrete and continuous multi-armed bandit settings
with entropy regularisation. We show that in the small steps limit with respect
to the Wasserstein distance W2, policy dynamics are governed by the heat equa-
tion, following the Jordan-Kinderlehrer-Otto result. This means that policies un-
dergo diffusion and advection, concentrating near actions with high reward. This
helps elucidate the nature of convergence in the probability matching setup, and
provides justification for empirical practices such as Gaussian policy priors and
additive gradient noise.
1	Introduction and setting
Deep reinforcement learning algorithms have enjoyed tremendous practical success at scale Mnih
et al. (2015; 2016). Separately, theoretical and practical success through smoothing has also been
achieved by generative adversarial networks with the introduction of Wasserstein GANs Arjovsky
et al. (2017). In both instances, a smooth relaxation of the original problem has been key to further
theoretical understanding. In this work, we take the view of policy gradients iteration through the
lens of converging towards a function of the rewards field r(s, a) for a given state s. This view uses
optimal transport metrized by the second Wasserstein distance rather than the standard Kullback-
Leibler divergence. Simultaneously, gradient flows relax and generalize to continuous time the
notion of gradient steps. An important mathematical result due to Jordan et al. (1998) shows that in
that setting, continuous control policy transport is smooth; this achieved by the heat flow following
the Fokker-Planck equation, which also admits a stochastic diffusion representation, and sheds light
on qualitative convergence towards the optimal policy. This is to our knowledge the first time that
the connection between variational optimal transport and reinforcement learning is made.
Policy gradient methods Williams & Peng. (1991); Mnih et al. (2016) look to directly maximize the
functional of expected reward under a certain policy ∏. ∏(a∣s) is the probability of taking action
a in state s under policy π. A policy can hence be identified to a probability measure π ∈ P, the
space of all policies. In what follows, functionals are applications from P → R. Out of a desire for
simplification, we focus on formal derivations, and skip over regularity and integrability questions.
We investigate policy gradients with entropy regularisation in the following setting:
•	Bandits, or reinforcement learning with 1-step returns
•	Continuous action space
•	Deterministic rewards
It is already known Sabes & Jordan. (1996) that entropic regularization of policy gradients leads to
a limit energy-based policy that probabilistically matches the rewards distribution. We investigate
the dynamics of that convergence. Our contributions are as follows:
1.	We interpret the mathematical concept of gradient flow as a continuous-time version of
policy iteration.
1
Under review as a conference paper at ICLR 2018
2.	We show that the choice of a Wasserstein-2 trust region in such a setting leads to solving
the Fokker-Planck equation in (infinite dimensional) policy space, leading to the concept
of policy transport. This shows optimal policies are arrived at via diffusion and advection.
This also justifies empirical practices such as adding Gaussian noise to gradients.
2	Gradient flows
2.1	Entropy-regularised rewards
Let r(a) be the reward obtained by taking action a. The expected reward with respect to a policy π
is:
Kr(π) = Eπ r(a) = r(a)dπ(a)
A
(1)
Shannon entropy is often added as a regularization term to improve exploration and avoid early
convergence to suboptimal policies. This gives us the entropy-regularised reward, which is a free
energy functional, named by analogy with a similar quantity in statistical mechanics 1 :
J(π)=	r(a)dπ(a) - β log π(a)dπ(a)
= Kr(π) - βH(π)
(2)
2.2	Policy iteration as gradient flow
We are interested in the process of policy iteration, that is, finding a sequence of policies (πn)
converging towards the optimal policy π*. In this section We follow closely the exposition by San-
tambrogio. (2015). Policy iteration is often implemented using gradient ascent according to
Πk+1 = ∏k + T VJ (∏k)	(3)
Rearranging gives the explict Euler method
πk+1- πk - VJ(∏k) = 0
τ
(4)
In this article we are more interested in the implicit Euler method which simply replaces VJ (πkτ )
with VJ(πkτ+1)
πk+1-πk -VJ(∏k+ι) = 0
τ
(5)
This is a policy iteration method. If integrated and interpreted as an L2 regularized iterative problem,
it is strictly equivalent to finding a solution to the proximal problem:
∏k+ι = arg min "" - "k11——J(∏)	(6)
π	2τ
Rather than just the L2 distance between policies for constraining and regularization, one can envi-
sion the more general case of any policy distance d:
d2(π,πk)
∏k+ι = arg min- -----------J(π)	(7)
π	2τ
2.3	Wasserstein proximal mapping: Policy transport
d can be chosen as any general metric distance or divergence. For instance, should we choose
d = √Dkl, the square root of the Kullback-Leibler divergence, in the proximal mapping above,
1In this article we follow the convention of convex analysis and optimal transport, that is, entropy H is
taken to be convex, rather than that of information theory with H preceded by a negative sign and concave.
2
Under review as a conference paper at ICLR 2018
we’d get a policy iteration procedure close in spirit to Schulman’s trust region policy optimiza-
tion Schulman et al. (2015a.).
The case of interest in this paper is when d = W2 is the second Wasserstein distance, that gives the
optimal transport cost for cost function c(x, y) = 2 |x - y|2. We therefore do iterative minimization
in the Wasserstein-2 space W2 :
πk+1
W22E,πQ
2T
- J (π)
argminW2Enk)
2τ
—/ r(s,a)dπ + β / log π(a∣s)dπ
(8)
π
A gradient flow is obtained in the small step limit τ → 0. This proximal mapping is an example of
Moreau envelope, and remains in a convex optimization setting when J is convex. The Wasserstein
distance W2 is defined for pairs of measures (μ, V) seen as marginals of a coupling Y by:
∀(μ,ν) ∈ P2, W22(μ, V)
γ∈Γnμ J lx - yl2dγ(x, y) = X〜μf 〜VEIX - Y12
(9)
The optimal coupling γ* in the infimum above is also called the optimal transport plan.
Also note that later, we will reformulate this Monge-Kantorovich problem Kantorovich. (1942) as
an equivalent linear problem of inner product minimization in L2 (P2 ):
W2(μ,ν) =	inf	hc(x,y),dγ(x,y)i	(10)
Y∈Γ(",ν)
3	Deriving the Fokker-Planck equation
Jordan, Kinderlehrer and Otto showed in a seminal result that several partial differential equations
can be interpreted as steepest descent, or gradient flows, of functionals in Wasserstein space W2 (Jor-
dan et al., 1998). Here we compute the PDE associated with the entropy-regularized rewards func-
tional J and its steepest descent within W2 .
3.1	The Euler continuity equation
Ifwe take the limit of small steps size τ in the implicit Euler method of equation 5 we get the Cauchy
problem
∂π
dt = ∏0(t) = VJ (∏(t))	(11)
π(0) = π0	(12)
which describes a gradient flow.
Just like gradient flows are the continuous-time analogue of discrete gradient descent steps, the Euler
continuity equation is the continuous-time analogue of the discrete Euler methods seen earlier.
A classic result in Wasserstein space analysis is that because optimal transport acts on probability
measures, it must satisfy conservation of mass. Hence all absolutely continuous curves, or flows
of measures, (πt) in W2(P) are solutions of the Euler continuity equation. The Euler continuity
equation can be seen as the formal continuous-time limit of the Euler implicit method described
above
∂t∏t = -V ∙ (vt∏t)	(13)
where Vt is a suitable vector velocity field (V∙ is the divergence operator). In the case We are looking
at:
Vt = v( δ∏- (π))(a)	(14)
where J (∏) is the first variation density defined via the Gateaux derivative as
J (∏)dξ = iJ (∏+⑹
(15)
=0
3
Under review as a conference paper at ICLR 2018
for every perturbation ξ = π0 - π.
Substituting vt into 13 gives us a partial differential equation necessarily of the form:
∂t∏ = -V∙(πV( δJ (∏))
(16)
This is proven rigorously in Jordan et al. (1998).
3.2	The Fokker-Planck Equation
It remains to compute the first variation density δ∏ (∏) for entropy regularised policy gradients.
First, the linear part Kr , or potential energy has first variation given naturally by
δKr
rdπ ⇒ ---(π) = r
A	δπ
(17)
Second, the entropy part H is a special case H = Utlogt of the general internal energy density
functional:
Uf ⑺=Z f( ^na )da ⇒ 誓⑺=f 0(π)	(18)
A da	δ π
In the case of entropy, f(t) = t log t, f0(t) = 1 + log t, and therefore
δH
(π) = 1 + log π.	(19)
δπ
Finally we require the gradient of this first variation density, given by:
V(y^) = Vr 一 βV(1 + log π) = Vr 一 β^π	(20)
The gradients δ∕δ∏ are functional, whereas the gradients V are action-gradients Va with respect to
actions a ∈ A.
Plugging this into 16 gives us the partial differential equation associated with steepest descent within
W2 for entropy-regularized rewards:
∂t∏ = -V ∙ (πVr) + β∆π	(21)
4	Interpretation
4.1	Converging to the optimal policy
The entropy-regularized rewards J is convex in the policy π , which means there is a single optimal
policy. The optimal policy will be achieved as long as each step improves the policy and this will be
the case as long as the steps taken as are small enough.
Given that we converge to the optimal policy, we know that at optimality ∂tπ = 0. Using equation 16
then gives US a necessary condition for the optimal policy ∏*:
-V∙(πV( J (π)) =0	(22)
By setting δ∏ (∏) = 0 and substituting in 20 We get
L	V∏*	士
Vr = β------= β V log π*	(23)
π*
which gives us the optimal policy
π* (X er/e	(24)
This is the Gibbs measure of the rewards density per action - also seen as an energy-based policy,
in line with the static result of Sabes & Jordan. (1996). The unregularized case β = 0 appears
degenerate.
4
Under review as a conference paper at ICLR 2018
4.2	The Fokker-Planck equation
The gradient flow associated with the pure entropy functional βH is the heat equation ∂tπ = β∆π.
Here the Laplacian ∆ is in action space. This is one of the key messages of the derivations we have
done in this part: the Wasserstein gradient flow turns the entropy into the Laplacian operator2.
For the full, entropy-regularized reward J(π), there is an extra transport term generated by the re-
wards, and the PDE is therefore the Fokker-Planck equation. This means that taking policy gradient
ascent steps in W2 (according to equation 8), is equivalent to solving the Fokker-Planck equation
for the policy π with potential field equal to the gradient of rewards r. Alternately, we can say, the
policy undergoes diffusion and advection - it concentrates around actions with high reward.
4.3	B rownian motion and noisy gradients
A partial differential equation for diffuse measures also admits a particle interpretation. The result
above can also be written, through Ito’s lemma Revuz & Yor. (1999), as the stochastic diffusion
version of the Fokker-Planck equation Jordan et al. (1998):
dΠt = -Vr(Πt)dt + PedBt	(25)
with Πt a finite dimensional discretization of policy πt , and Bt a Brownian motion of same di-
mensionality. In that case, the density of solutions verifies equation 21 - formally, one replaces
increments of the BroWnian motion dBt by its infinitesimal generator, the Laplacian 1 ∆.
This stochastic differential equation can also be seen as a Cauchy problem of on-policy rewards
maximization, this time with added isotropic Gaussian policy noise √2βdBt.
Discretizing again - but in the time variable rather than the action space - one writes, with an explicit
method:
πn+1 - πn = -VrEn) + P2β ∙ N(0, Iq)	(26)
This is just noisy stochastic action-gradients ascent on the rewards field. This shows a link between
entropy-regularization and noisy gradients. It also suggests a possible technique for implementation.
The key issue to overcome is to generate gradient noise in parameter space that is equivalent to
isotropic Gaussian policy noise.
4.4	Relationship between Wasserstein and Kullback-Leibler distances
We note the Kullback-Leibler and Wasserstein-2 problems are related by the Talagrand p-
inequalities Gozlan & LeOnard. (2010), which for P = 2 and under some conditions, ensure that for
some constant C and given a suitable reference measure V, ∀μ ∈ P(X), W22 (μ,ν) ≤ C ∙ DKl (μ,ν).
This justifies the square root in d = √DKL in the proximal mappings studied earlier (equation 8),
but more would be beyond the scope of this article.
4.5	Energy-based policies as Gibbs measures
Since the first variation process Jnn) is known, and we derive it explicitly earlier, we can use a
variational argument specific to W2 (and invalid in W1 !). We admit (Santambrogio. (2015)) that
the solution of the minimization problem has to satisfy:
—学(∏) + "W = ConStant	(27)
δπ τ
with φw2 a Kantorovich potential function for transport cost c(x, y) = 1 |x — y|2. One useful way to
think of the Kantorovich potential is that it is a function, whose gradient field generates the difference
between the optimal transport plan T and the identity, according to the equation T(x) = x — Vφ(x).
It is well known Revuz & Yor. (1999) that the Gibbs distribution is the invariant measure of the
stochastic differential equation above. Therefore we expect it to play a role of primary importance.
2This is part of what’s known as Otto calculus.
5
Under review as a conference paper at ICLR 2018
In fact, the solution of the W2 gradient flow with discrete steps is known explicitly Santambrogio.
(2015) if we know the successive Kantorovich transport potentials associated:
,l 、	r(S,G+φnS)	r(S,a)
∏n(a∣s)〜e	β → e β 〜π*(a∣s)	(28)
n→∞
In practice, deriving the W2 optimal transport as well as its cost at each gradient flow step is numer-
ically instable and computationally expensive. Furthermore, numerical estimators for the gradients
of Wasserstein distances have been found to be biased; alternatives such as the Cramer distance be-
have better in practice but not in theory Bellemare et al. (2017b). (To our knowledge, the gradient
flow of the Cramer distance is not known, and no results exist that relate it to the entropy and Fisher
information functionals). In appendix, we use fast approximate algorithms in their small parameter
regime. We show that another Gibbs measure, the two-dimensional kernel e-c/, where c is the
ground transport cost, and an auxiliary regularization strength, arises naturally in this context, and
leads to taking successive Kullback-Leibler steepest descent steps in the coupling, in a spirit close
to trust region policy optimization, but using optimal transport and the Sinkhorn algorithm.
5	Related work
Optimal transport, and the study of Wasserstein distances, is a very active research field. Founda-
tional developments are found in Villani’s reference opus Villani. (2008). The gradient flows per-
spective is presented in Ambrosio’s book Ambrosio et al. (2006) for a complete theoretical treatment,
and in Santambrogio Santambrogio. (2015) for a more applied view including a presentation of the
Jordan-Kinderlehrer-Otto result. A classic reference for connecting Brownian motions and partial
differential equations is Revuz-Yor Revuz & Yor. (1999). Efficient algorithms for regularized opti-
mal transport were first explored by CUtUri Cuturi. (2013), and then Peyre Peyre (2015) Who showed
the equivalence to steepest descent of KL with respect to the smoothed Gibbs ground cost, and its
formulation as a convex problem. Carlier Carlier et al. (2015) gives proofs of Γ-convergence of W2,
to W2. Leonard Leonard. (2014) makes the connection with the Schrodinger problem Schrodinger.
(1931) and concentration of measure.
In the context of neural networks, partial differential equations and convex analysis methods are
covered by Chaudhari Chaudhari et al. (2017). The Monge-Kantorovich duality in the W1 case, and
Wasserstein representation gradients, are applied to generative adversarial networks by Arjovsky Ar-
jovsky et al. (2017). The W2 connection with generative models is studied by Bousquet Bousquet
et al. (2017). Similarly, Genevay et al Genevay et al. (2017) define Minimum Kantorovich Estima-
tors in order to formulate a wide array of machine learning problems in a Wasserstein framework.
6	Discussion and further work
We have used tools of quadratic optimal transport in order to provide a theoretical framework for
entropy-regularized reinforcement learning, under the strongly restrictive assumption of maximising
one-step returns. There, we equate policy gradient ascent in Wasserstein trust regions with the heat
equation using the JKO result. We show advection and diffusion of policies towards the optimal
policy. This optimal policy is the Gibbs measure of rewards, and is also the stationary distribu-
tion of the heat PDE. Recast as a stochastic Brownian diffusion, this helps explain recent methods
used empirically by practitioners - in particular it sheds some light on the success of noisy gradient
methods. It also provides a speculative mechanism besides the central limit theorem for why Gaus-
sian distributions seem to arise in practice in distributional reinforcement learning Bellemare et al.
(2017a).
Our contribution largely consists in highlighting the connection between the functional of reinforce-
ment learning and these mathematical methods inspired by statistical thermodynamics, in particular
the Jordan-Kinderlehrer-Otto result. While we have aimed to keep proofs in this paper as simple
and intuitive as possible, an extension to the n-step returns (multi-step) case is the most urgent and
obvious line of further research. Finally, exploring efficient numerical methods for heat equation
flows compatible with function approximation, are directions that will also be considered in future
research.
6
Under review as a conference paper at ICLR 2018
References
S. Amari. Information Geometry and Its Applications. Springer, Applied Mathematical Sciences.,
2016.
L.	Ambrosio, N. Gigli, and G. Savare. Gradient flows: in metric spaces and in the space of proba-
bility measures. Springer., 2006.
M.	Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
arXiv preprint arXiv:1707.06887, 2017a.
M. G. Bellemare, I. Danihelka, W. Dabney, S. Mohamed, B. Lakshminarayanan, S. Hoyer, and
R. Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint
arXiv:1705.10743, 2017b.
O.	Bousquet, S. Gelly, I. Tolstikhin, C. J. Simon-Gabriel, and B. Scholkopf. From optimal transport
to generative modeling: the vegan cookbook. arXiv preprint arXiv:1705.07642, 2017.
L.M. Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematicalphysics, 7(3):200-217, 1967.
G. Carlier, V.Duval, G. Peyre, and B. Schmitzer. Convergence of entropic schemes for optimal
transport and gradient flows. arXiv preprint arXiv:1512.02783, 2015.
P.	Chaudhari, A. Oberman, S. Osher, S. Soatto, and G. Carlier. Deep relaxation: partial differential
equations for optimizing deep neural networks. arXiv preprint arXiv:1704.04932, 2017.
R. Cominetti and J. San Martin. Asymptotic analysis of the exponential penalty trajectory in linear
programming. Mathematical Programming 67 169-187, 1994.
M.	Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural
Information Processing Systems (NIPS) 26, pages 2292-2300, 2013.
C. Frogner, C. Zhang, H. Mobahi, M. Araya-Polo, and T. Poggio. Learning with a wasserstein loss.
arXiv preprint arXiv:1506.05439, 2015.
A. Genevay, G. Peyre, and M. Cuturi. Gan and Vae from an optimal transport point of view. arXiv
preprint arXiv:1706.01807, 2017.
N.	Gozlan and C. Leonard. Transport inequalities. a survey. Markov Processes Relat Fields 16,
635736, 2010.
R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the Fokker-Planck equation.
SIAM journal on mathematical analysis, 29(1):1-17, 1998.
L. Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):227-229,
1942.
C.	Leonard. A survey of the Schrodinger problem and some of its connections with optimal transport.
Discrete Contin. Dyn. Syst. A, 34(4):1533-1574, 2014.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, and et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529-533, 2015.
V.	Mnih, A. Puigdomenech Badia, M. Mirza, A. Graves, T. P Lillicrap, T. Harley, D. Silver,
and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. arXiv preprint
arXiv:1602.01783, 2016.
Y.	Nesterov, A. Nemirovskii, and Y. Ye. Interior-point polynomial algorithms in convex program-
ming. SIAM, volume 13, 1994.
7
Under review as a conference paper at ICLR 2018
G. Peyre. EntroPic Wasserstein gradient flows. arXiv preprint arXiv:1502.06216, 2015.
D.	Revuz and M. Yor. Continuous martingales and Brownian motion. Springer, Grundlehren der
mathematischen Wissenschaften, volume 293., 1999.
P. Sabes and M. Jordan. Reinforcement learning by Probability matching. MIT Technical Report,
1996.
F. Santambrogio. Optimal Transport for Applied Mathematicians : Calculus of Variations, PDEs
and Modeling. Progress in Nonlinear Differential Equations and Their APPlications, 87., 2015.
E.	Schrodinger. Uber die umkehrung der naturgesetze. Sitzungsberichte Preuss. Akad. Wiss. Berlin.
Phys. Math., 144:144-153, 1931.
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. arXiv preprint arXiv:1502.05477, 2015a.
R. Sinkhorn and P . KnoPP. Concerning nonnegative matrices and doubly stochastic matrices. em
Pacific J. Math., 21:343-348, 1967.
C. Villani. Optimal Transport : Old and New. Grundlehren der mathematischen Wissenschaften,
volume 338., 2008.
R. J. Williams and J. Peng. Function oPtimization using connectionist reinforcement learning algo-
rithms. Connection Science, 1991.
8
Under review as a conference paper at ICLR 2018
A Justifying steepest descent of relative entropy : Entropic
REGULARIZATION OF W2 ITSELF
Here we show that while taking discrete proximal steps according to distance d = W2 is ill-advised,
we can perform optimal transport with respect to the entropic regularization of the Wasserstein
distance itself. This is a second layer of entropic regularization.
If we let be a small positive real number, then we define W22, as a regularization of the W2
minimization in equation 9:
∀(μ, V) ∈ P2,w2,e(μ, ν)
YMJ |x- yl2dγ(χ,y)- eH (γ)
(29)
with the entropy H(Y) now extended to two-dimensional coupling space Γ(μ,ν) as, in the dis-
cretized case, with cardA = q,
q
H(Y) = - X Yi,j (log(Yij) - 1)	(30)
i,j=1
and by an analogue of continuity named「-convergence Carlier et al ( 01 ), W2,e(μ, ν) →
W2,(μ, ν). In fact, W2e converges exponentially Cominetti & Martin ( 99 ) fast towards W2
as e → 0. W22,e is not a distance anymore, but rather a divergence. Yet it enjoys better numeri-
cal properties, and enables closed-form solutions for various problems. We do not have a linear
Monge-Kantorovich minimization program anymore, but rather a strictly convex program. With
c(χ,y) = 1 ∣χ - y|2:
∀(μ, V) ∈ P2, W2,e(μ, V) = ^inf V)(hc, Yi- eH(γ))	(31)
This change from a linear to a convex problem makes the solution set better conditioned numerically;
the solution does not have to lie on a vertex of a convex polytope (by analogy with the simplex
algorithm, see Nesterov et al. (1994)) anymore, and therefore, is more robust to initial conditions.
In practice, e cannot be chosen too small or these stability properties are lost. A certain amount of
smoothing is to be tolerated, which is acceptable in the reinforcement learning context, due to the
inherent uncertainty on the rewards distribution. Returning to our optimization problem, moving the
inner product bracket inside the H part turns the expression into a single KL divergence. This yields
the equivalent problem, as detailed in Peyre (2015)
∀(μ,ν) ∈ P2, W2e(μ,ν)=	inf、Hei (γ)	(32)
,	Y∈Γ(μ,ν)
At this stage, the link with earlier sections becomes intuitively very clear, since the reference measure
-|x-y|2
e-c∕e is for cw2(χ, y) = 1 |x - y|2 simply the heat kernel e_2-. It is therefore not surprising
the evolution gradient flows considered earlier were linked to the heat equation. Performing JKO
stepping from from d2 = W22,e rather than W22 reads
W22 (π, πτ,e )
∏T+ι = argmin 2,∖ , k，+ F(π)	(33)
π	2τ
instead of equation 8. Combining both definitions therefore gives the problem
2τ
γ = argmmHe-c/e (γ) +-------F(π ∙ 1)	(34)
Y∈r(μ,V)	e
to be solved in 2-d coupling space. With this entropic smoothing, we can now re-cast the optimal
transport problem as a Kullback-Leibler problem, trading a single optimal transport proximal step
for several, ’fast’ KL steps. This is done next section using iterative convex projection algorithms.
9
Under review as a conference paper at ICLR 2018
B Derivation of the Sinkhorn algorithm
B.0.1 The Bregman algorithm
This algorithm is used in convex optimization for iterative projections. The method generalizes the
computation of the projection on the intersection of convex sets. Assume we give ourselves a convex
function Ψ and that we consider the associated Bregman divergence DΨ Amari. (2016) defined by
∀(π,ξ) ∈ P2,	Dψ(π∣ξ) = ψ(π)- Ψ(ξ) -hVΨ(ξ),π-ξ	(35)
We look to minimize this Bregman divergence DΨ on the intersection of convex sets C = ∩ Ci .In
our case of interest there are two such sets C1 and C2. For y a given point, or function, we solve for
inf DΨ (x, y)	(36)
x∈C
or equivalently with φ1 and φ2(π) playing the role of indicator barrier functions
inf Dψ(∏∣ξ) + Φι(∏) + φ2(∏)	(37)
In the case where Ψ = H, we get VΨ = log, VΨ* = exp through Legendre transform gradient
bijection, and Dψ = Hψ = Dkl(∙∖Ψ).
The Bregman algorithm Bregman. (1967) simply consists in solving the problem 36 by iteratively
performing projection on each of the sets Ci in a cyclical manner, therefore building the sequence
xn+ι = PDψ (Xn) → x* = inf Dψ(x,y)	(38)
[n]	n→∞	x∈C
with [n] the modulo operator ensuring cyclicality of the projections. Therefore, any problem that
can be cast under the convex Bregman form 36 can be solved by taking many steepest descent steps.
We now proceed to explicit the PCDΨ operators, which in our case are PCKL KL proximal steps, and
integrate them into an efficient practical algorithm.
B.0.2 The Sinkhorn algorithm
We start with the need to minimize the convex form ɪ Pij Pijlog Pij + Pij Cj , subject to marginal
constraints that the discretized measure μ is transported by P onto V. From a matrix perspective this
translates into the two following constraints: that the sum in column of P being equal to vector μ,
and the sum across lines ofP equal to ν:
P ∈ U(μ, V) = {M ∈ Rq×q,	M 1q = μ, MTIq = v}	(39)
The matrix set U (μ, V) is convex. We can form the Lagrangian of this optimization problem in Pij
using vector Lagrangian multipliers α, β,
L(P,α,β) = ɪ XPijlogPij + Pijcij + ατ(P 1q — μ) + βT(PTIq — V)	(40)
ij
A necessary condition for optimality is then
∀(i,j),	∂L = O ⇒	P*j =exP( — 1 — αi )exp(-Cj )exp(-1 — βj)	(41)
∂Pij	2	2
Hence we have shown that the optimal coupling is a diagonal scaling of the ground cost’s Gibbs
kernel. With the two positive vectors u, V defined as diag(u) = e-1 -α and diag(v) = e-2-^j,
this is formally Pi*j = ui exp(—cij /)vj. Once this is derived, the iterative convex projections
framework of the Bregman algorithm enables us to derive the full Sinkhorn method.
If one recasts the entropic transport problem
N
m⅛( X ci,j Yi,j + EH(Y ∖(μivj )ij ))	(42)
γ∈	i,j=1
10
Under review as a conference paper at ICLR 2018
as
min H (Y IY)	⇒	Yij = exp(-ci,j/e)μiνj
γ∈Γ
(43)
then this reads as a two-dimensional KL projection algorithm of point Y on set of marginal con-
straints Cι,μ enforcing U Θ (Kv) = μ, and C2,ν enforcing V Θ (KTU) = ν. Therefore
Yij = uiγi,jVj = ui ∙ μi exp (-ci,j/61Vj vj
and ultimately yields the Sinkhorn balancing algorithm:
(44)
(45)
μi	= ɪ V _	Vj	= ɪ
Pj Yi,jvj	i	Kv	Pi YMjUJj	KTU
these two updates being merged in Algorithm 1.
B.1 The Sinkhorn algorithm
The Sinkhorn algorithm is a fast, iterative algorithm for optimal transport; it mostly involves matrix
multiplications and vector operations such as term-by-term division, and as such scales extremely
well on GPU platforms. This makes it possible to use the Sinkhorn algorithm to numerically ap-
proximate optimal couplings. We give its outline below, the interested reader can find more details
can be found in Cuturi’s original article Cuturi. (2013); Sinkhorn & Knopp. (1967), or in Frogner’s
version applied to deep learning Frogner et al. (2015).
We will want to compute the optimal coupling, transport cost, and gradient pertaining to distance
W22,. First we remember the regularized transport problem as per equation 31. The 2-dimensional,
relaxed coupling Y can be discretized to a 2-dimensional matrix P with entries (pi,j). We show (see
Appendix) that necessarily
Pi = diag(u)Kdiag(V)	K = e-普	(46)
where the matrix exponential of the ground cost is taken term-by-term. Recalling the equality con-
straints on the row and column sums given by μ and V in 39, We find that We have to solve a matrix
balancing problem, using the terminology of linear algebra. Once we have formed K, and have poli-
cies μ and V as inputs, we can run through iterations of the Sinkhorn algorithm till convergence to a
fixed point. This is done below and runs a one-line while loop on vector (x)i, the component-wise
inverse of (U)i. Dotted operations are taken component-wise:
Algorithm 1 Computation of policy transport Wff (μ, V) by Sinkhorn iteration.
Input C, e, μ, v.
I =(μ > 0); μ=μ(I); C = c(I,:) ; K=exp(-C∕e)
χ=ones(length(μ),size(ν ,2))"ength(μ);
while x has not converged do
x=diag(1")*K*(v.*(1./(K′*(1./x))))
end while
u=1./x; v=V.*(1./(K’*u))
W其 μ,ν )=sum(u.*((K.*C)*v))
dW∂y = e log u (up to a constant parallel shift)
The Sinkhorn algorithm converges linearly. Its theoretical justification is that it can be seen as an
instance of the iterative convex projections explained previous section. It is critical to notice that the
distance W2, is differentiable in the policy, unlike W2. The vector U above is not unique; but one
suitable gradient of the Wasserstein distance with respect to the first variable policy is known, and
given by the formula in the algorithm above, simply proportional to the element-wise log of scaling
vector U. This closed form differentiation allows us to perform gradient descent in Sinkhorn layers
embedded in neural network systems. In general, this gradient, just like vector U, is defined up to a
constant shift only; the normalizing shift generally found in the literature is
dWC (μ, V)
∂μ
e log u - e IogUTI 1
(47)
11
Under review as a conference paper at ICLR 2018
that makes u tangent to the simplex Frogner et al. (2015). Under this form, the algorithm is compati-
ble with function approximation, where policy μ is a function of a parameter and reads μθ. We note
that another possibility to create this compatibility would be to unroll a fixed number of iterations
of the algorithm, as they are effectively matrix and vector operations, as has already been done with
generative adversarial networks and deep Q-networks. We hypothesize that learning with a Wasser-
stein loss, in a continuous action state setting, will help agents pick actions that are semantically
close to the optimum action, therefore increasing policy quality, and reducing the ’unnaturalness’ of
policy mistakes. It is our hope that a Wasserstein loss, by implying relevant semantic directions in
action space, will speed up convergence and training of reinforcement learning agents. In practice,
we are still limited by our fundamental assumption that the MDP and the statewise rewards density
a → r(s, a) are known. Possibilities such as bootstrapping the rewards density distribution exist,
and will be explored practically in further work.
12