Under review as a conference paper at ICLR 2018
Multi-label Learning for Large Text Cor-
pora using Latent Variable Model with Prov-
able Guarantees
Anonymous authors
Paper under double-blind review
Ab stract
Here we study the problem of learning labels for large text corpora where each
document can be assigned a variable number of labels. The problem is trivial
when the label dimensionality is small and can be easily solved by a series of
one-vs-all classifiers. However, as the label dimensionality increases, the param-
eter space of such one-vs-all classifiers becomes extremely large and outstrips the
memory. Here we propose a latent variable model to reduce the size of the param-
eter space, but still efficiently learn the labels. We learn the model using spectral
learning and show how to extract the parameters using only three passes through
the training dataset. Further, we analyse the sample complexity of our model us-
ing PAC learning theory and then demonstrate the performance of our algorithm
on several benchmark datasets in comparison with existing algorithms.
1	Introduction
Multi-label learning for large text corpora is an upcoming problem in Large-Scale Machine Learn-
ing. Unlike the multi-class classification where a document is assigned only one label from a set of
labels, here a document can have a variable number of labels. A basic approach to the problem is
to use 1-vs-all classification by training a single binary classifier for every label. If the vocabulary
size of a text corpus is D and the label dimensionality is L, then the 1-vs-all model require O(DL)
parameters. Most of the text corpora have moderate to high vocabulary size (D), and 1-vs-all clas-
sification is feasible as long as D L. However, as the number of labels increases to a point when
L 〜D, the size of the parameter space increases to O(D2), and We can no longer store the 1-vs-all
models in the memory Yu et al. (2014).
A fairly intuitive approach to reducing the size of the parameter space is to use loW-rank models.
The existing loW-rank approaches in the literature are mainly based on discriminative models that
use a mapping Φ : RD → RL betWeen the Word space and the label space. If the rank of the model
is restricted to K D, then such mappings have the form Z = HW>, Where W ∈ RD×K and
H ∈ RL×K contain the loW-rank features of the Words and the labels respectively. The parameter
space of such models has the size Θ ((D + L)K), Which is much smaller than O(DL) for large
values of L and can be conveniently stored the memory.
Amongst the recent literature on discriminative loW-rank models, WSABIE Weston et al. (2011)
defines Weighted approximate pair-Wise rank (WARP) loss on the mapping and optimises the loss
on the training dataset. LEML Yu et al. (2014) generalises the loss function to squared-loss, sigmoid
loss or hinge loss, Which are typical of Linear Regression, Logistic Regression, and Linear SVM
respectively. Both of LEML and WSABIE use gradient descent to optimise the loss on training
datasets and therefore are susceptible to overfitting. Also, both of them run several iterations through
the dataset and can sloW doWn for large datasets containing millions of text instances.
Here We propose a latent variable model for multi-label learning. Our model can be vieWed as a
generative counterpart of the discriminative loW-rank models like WSABIE or LEML. Unlike the
usual cases Where such latent variable models are trained using EM algorithm, We use Spectral
Learning or Method of Moments Anandkumar et al. (2014) to extract the parameters. Method of
Moments is an upcoming non-iterative approach for latent variable based models, and has success-
fully been applied in Spherical Gaussian Model Hsu & Kakade (2013), Hidden Markov Models
1
Under review as a conference paper at ICLR 2018
Figure 1: Latent Variable Model
(HsU et al. (2012) and Song et al. (2010)), Bayesian Non-parametrics TUng & Smola (2014), Topic
Models Anandkumar et al. (2012), and in various NLP applications (Dhillon et al. (2012) and Cohen
et al. (2014)). We first derive a closed form expression to estimate the model parameters using the
Method of Moments. Further, we derive the convergence bounds on the estimated parameters, and
then compare the performance of our algorithm with the discriminative low-rank models both from
the perspective of accuracy and computation time.
There are other approaches beyond low-rank mappings, such as FastXML Prabhu & Varma (2014)
that uses tree-based hierarchies for the labels, or SLEEC Bhatia et al. (2015) that uses kNN based
embedding of the texts in the training dataset. However, these approaches use a combination of
different algorithms and are geared towards end-to-end solutions. Whereas, we look into the problem
from a modelling perspective and limit our discussion to the low-rank models since they have a
similar model complexity to the latent variable model.
2	Methodology
We build our model based on the “bag of words” assumption on the documents taking into account
only the occurrence of the words, not their counts. Let us assume that there are N labeled documents
in a corpus with vocabulary size D and label dimensionality L. Then using a latent variable h that
assumes K states, the probability ofa label l being assigned to a document d can be expressed as:
K
P [l|d] = XP[l|h = k]P[h = k|d]
k=1
(1)
where the conditional probability of a document d given h can be modelled from the set of distinct
words Wd present in it as:
P[d|h = k] = ɪɪ P[v|h = k] ɪɪ (1 - P[v|h = k])
v∈Wd	v∈Wd
(2)
From here, P[h = k|d] can be expressed using Bayes Theorem as:
P[h = k|d]
P [h = k]P [d|h = k]
PK=I P[h = k]P[d|h = k]
(3)
The latent variable model is described by the plate notation in Figure 1, where the words are denoted
by v, the labels by l and the latent variable by h. Our model parameters are P [h], P [v|h] and P [l|h],
and the number of parameters is Θ ((D + L)K), same as the discriminative low-rank models.
Let us define πk ∈ [0, 1] as the probability of the latent variable h assuming the state k ∈ [K].
πk = P[h = k]	(4)
Let us define μk ∈ [0, 1]d as the probability vector of a word V conditional to the latent state
k∈ [K].
μk = P [v[h = k]	(5)
2
Under review as a conference paper at ICLR 2018
Similarly, let us define γk ∈ [0, 1]L as the probability vector of a label l conditional to the latent
state k ∈ [K].
γk = P [l|h = k]	(6)
One key difference of our parameterization from Anandkumar et al. (2012) and Wang & Zhu (2014)
is that they define {μk}3ι as the expected count of the words (E[v∣h = k]) for each topic k, while
We define {μk}3ι as the probability of the words. A probabilistic formulation enables Us to model
the documents using Equation 2, which is not possible using expectations. We later use the document
probabilities to predict the labels for a test document (Section 4). Therefore, unlike Anandkumar
et al. (2012) or Wang & Zhu (2014) that uses moments on the word counts defined on the domain
Np RD (for pth order moment), we use moments based on the joint probability of the words defined
on the domain Np[0, 1]D. We outline the derivation for the probability moments in this section, and
then show that our formulation achieves a tighter convergence bound on the parameters in Section
3.1. The label parameters {γk}kK=1 are unique to our model.
2.1	Moment Formulation
We first try to formulate the joint probability of a pair of words in terms of ∏ and μ. Let US assume
that we choose two words w1 and w2 from a document at random. The term P [w1 = vi, w2 = vj]
represents the probability by which any pair of words picked at random from a document turns out
to be vi and vj, and it is nothing but P [vi, vj], where i, j ∈ [D]. Similarly, P [w1 = vi] is same as
P [vi], and the same holds for P [w2 = vj].
Following the generative model in Figure 1, any two words w1 and w2 in a document are condition-
ally independent given h. Therefore,
P [w1 = vi, w2 = vj]
K
=	P [w1 = vi, w2 = vj |h = k]P [h = k]
k=1
K
=	X P[w1 = vi|h = k]P [w2 = vj|h = k]P[h = k]
k=1
K
=	XP[vi|h= k]P[vj|h=k]P[h= k]
k=1
K
=	Enk μkiμkj
k=1
where μki is the ith element of the vector μk and so on.
K
二. P [vi, vj] =): πk μkiμkj ∀ij ∈ [D]	⑺
k=1
If M2 ∈ [0, 1]D×D is the matrix containing the joint probability of the pairs of words with M2i,j =
P[vi , vj] ∀i,j ∈ [D], then it can be expressed as:
K
M2 =	∏k μk μ>	(8)
k=1
Similarly, if the tensor M3 ∈ [0, 1]D×D×D is defined as the moment containing the joint probability
of a triplet of words with M3i,j,κ = P[vi, vj, vκ], ∀i, j, κ ∈ [D], then
K
M3 =): πk μk % μk % μk	⑼
k=1
where % stands for the tensor product.
3
Under review as a conference paper at ICLR 2018
Finally, we define ML ∈ [0, 1]L×D×D as the probability moment of a label occurring with a pair of
words such that MLκ,i,j = P [lκ, vi , vj], ∀κ ∈ [L] and ∀i, j ∈ [D]. Then,
K
ML = E ∏k Yk 脸 μk 总 μk	(10)
k=1
2.2 Closed Form Expression of the Parameters
We derive the equations for extracting ∏, μ and Y in this section. The first step is to whiten the
matrix M2, where we try to find a matrix low rank W such that W >M2W = I. This is similar to
the whitening in ICA Hyvarinen et al. (2004), with the covariance matrix being replaced by the Co-
occurrence probability matrix. The whitening is usually done through singular value decomposition
of M2. If the K maximum singular values of M2 are {νk}kK=1, and {ωk}kK=1 are the corresponding
left singular vectors, then the whitening matrix of rank K is computed as W = ΩΣ-1/2, where
Ω = [ω1∣ω2∣... ∣ωκ] and Σ = diag (VL ν2,..., VK).
Upon whitening M2 takes the form
KK	K
W>M2W = W>(X∏kμkμ>)W = X (√πkW>μk)(√∏kW>μk), = Xμkμ> = I (11)
k=1	k=1	k=1
Hence μk = √πkW>μk are orthonormal vectors. Multiplying M3 along all three dimensions by
W, we get
KK
M3 = M3(W, W,W) = X ∏k(W>μk)型(W>μk)型(W>μk) = X ɪμk 型 μk 型 μk (12)
M3 is a tensor in the domain rk×k×k. Upon the factorisation of M3, if the eigenvalues and
eigenvectors are {λk}K=ι and {uk}K=1 respectively, then λk = √= =⇒ ∏k = λ-2, and
Uk = μk = √∏k W >μk = γ-W >μk	(13)
λk
Hence, {μk }K=1 can be recovered as μk = λkW %k, where W * = W (W >W) 1 is the pseudo-
inverse of W>. Since we compute P[v|h] by normalising μk as
P[v|h = k] = PIk , it suffices to compute μk = W*Uk as λk will be cancelled during normali-
v v_1 μkv
sation.
It is possible to compute Y through the factorisation of second and third order moments of the
labels in a similar way to μ. However, there is no guarantee that the label probabilities of Yk will
correspond to the same topic as the word probabilities in μk for every k when we compute them
separately. Without the topic alignment between μ and Y, label prediction is not possible.
To overcome this limitation, we use the cross-moment ML to compute Y . If we multiply ML twice
by W, then
KK
Ml(W,W) = X∏kYk 0 (W>μk) 0 (W>μk) = X Yk 0 (√πkW>μk) 0 (√πkW>μk)
k=1	k=1
K
=EYk 0 μk 0 μk	(14)
k=1
If Uk is the kth eigenvector of M3, then
u>Ml(W, W)uk = ">Ml(W,W)μk = μ> (X Yk 0 μk 0 "J μk = Yk (15)
since μk S are orthonormal. Computing Yk by this method ensures that Yk and μk correspond to the
same topic for every k, which in turn ensures that Yk and μk correspond to the same topic.
4
Under review as a conference paper at ICLR 2018
3 Parameter Estimation
So far we have shown how to extract the parameters from the global moments M2, M3 and ML
defined on population. In practice, we cannot compute the population parameters; all we can do is
to estimate them from the sample in hand and compute an error bound for the estimation. We denote
the estimated values of M2, M3, Ml, ∏, μ, γ, W and U from the sample as M2, M3, Ml, ∏, μ, ^,
W and U, conforming with the notations used in the previous literature.
If we take into account only the occurrence of each distinct word in a document, then using “one
hot encoding,” we can represent the words as a binary vector x ∈ {0, 1}D, and the labels as another
y ∈ {0, 1}L. If there are N samples, then we can represent the words in the entire corpus as
X ∈ {0, 1}N×D, and the associated labels as Y ∈ {0, 1}N×L. The pairwise counts of the words
can be estimated by X>X, whose sum of all elements is,
DD
X X(X>X)v1,v2
v1=1 v2=1
DDN
xi,v1xi,v2
v1=1 v2=1 i=1
NDD
xi,v1xi,v2
i=1 v1=1 v2=1
N
nnz (xi)2
i=1
where xi is the row of X corresponding to the ith document, xi,v is the vth element in it (v ∈ [D]),
and nnz(xi) is the number of non-zero elements in xi, i.e., the number of distinct words in the ith
document.
Therefore, the joint probability matrix of the words (M2) can be estimated as,
(16)
Similarly, the triple-wise occurrences of the words can be estimated by the tensor X 0 X 0 X, and
the sum of all of the elements of this tensor is
DDD	N
XXX
(X 0 X 0 X)v ,v ,v =Xnnz(xi)3
v1=1 v2=1 v3=1	i=1
Therefore, M3 can be estimated as
F----------X 0 X 0 X
Pi=1 nnz(xi)3
(17)
The dimensions of M2 and M3 are D2 and D3 respectively. But in practice, these quantities are
extremely sparse. We do not need to compute M3 in practice. If the whitening matrix computed
ʌ ʌ ~
through the Singular Value Decomposition of M2 is W, then M3 can be estimated straight away
using Equation 12 as,
1
M3 = —ʊ--------------XW 0 XW 0 XW	(18)
PN=i nnz(xi)3
Since M3 has a dimension of K3 and K W D, it can be conveniently stored in the memory. The
ʌ
.	.∙	r∙ τι^r . ι	.1	ι.ι t .	.	t .1	.	r∙ τι~r . ι	. 1
computation of M2 takes one pass through the dataset, and the computation of M3 takes another.
Also, the counts of the labels occurring with the pairs of words can be estimated by the tensor
Y 0 X 0 X , whose sum of all element is
LDD	N
X X X (Y 0X0X)l,v1,v2 = X nnz(yi)nnz(xi)2
l=1 v1 =1 v2 =1	i=1
where yi represents the ith row of Y and nnz(yi) is the number of labels associated with the ith
document. Therefore, ML can be estimated as,
ML
Y 0X0X
(19)
5
Under review as a conference paper at ICLR 2018
Algorithm 1 Three-pass Algorithm for Parameter Extraction
Input: Sparse Binary Data X ∈ {0,1}N×D, Labels Y ∈ {0,1}N×L and K ∈ Z+
Output: π, P[v∣h] and P[l∣h]
1.	Estimate M2 = (X>X)/PN=I nnz(x∕2	(pass # 1)
2.	Compute K maximum singular values of M2 as {νk}3ι, and corresponding left singu-
lar vectors as {ωk}3ι∙ Define Ω = [ω1∣ω2∣... ∣ωκ], and Σ = diag (ν1,ν2,..., VK)
3.	Estimate the whitening matrix W = ΩΣ-1/2 ∈ Rd×k
ʌ
4.	Estimate M3 = (XW 0 XW 0 XW)/ PN=I nnz(xi)3	(pass # 2)
ʌ
5.	Compute eigenvalues {λk}3ι and eigenvectors {Uk}3ι of M3.	Assign U =
[U1∣U2∣... [Uk]
-1
6.	Estimate μk = W3k, where W* = W(W>W)	, and ∏k = λ-2, ∀k ∈ 1,2 ...K.
7.	Estimate
P[v∣h = k] = Pμkv—, ∀k ∈ 1 ...K,v ∈ vι ...vD
Vμv μkv
8.	Estimate Γ = Y> (XWU) • (XWU) , where • stands for the element-wise product
(pass # 3)
9.	Estimate
P[l∣h = k] = rlk ,∀k ∈ 1...K,l ∈ l1 ...IL
l Γlk
If kth eigenvector of M3 is Uk, then from Equation 15
Yk = U>ML(W,W)Uk
u>(γ 0 XW 0 XW )Uk
J 、Y 0 XWUk 0 XWUk
Z (X, Y)
Z7XΓvy Y>((XW Uk )• (XW Uk))
Z(X, Y )
where • stands for the element-wise product, and Z(X, Y) = PiN=1 nnz(yi )nnz(xi )2 is a constant.
If We assign Γ = [^ι ∣Y21... ∣γκ], then it can be computed as,
ʌ
γ =[夕1|钿…lγK]
= J 、Y>h(XWUi• XWUi)I(XWU2• XWU2)I
Z (X, Y )
...1 (XW UK • XW UK)i
=Z7X1vτ Y > ((XW U) • (XW U))	(20)
Z (X, Y )
ʌ	A	ʌ
where U = [U1∣U2∣... [Uk ]. Since we estimate P [l|h] by normalising the columns of Γ as P [l|h =
k] = P l^ , it is sufficient to compute Γ = Y> ((XWU) • (XWU)) as the constant Z(X, Y) will
l Γlk
be cancelled out during normalisation. Instead of computing all the Yk S separately, we can compute
ʌ
Γ in one step by just one pass through the entire dataset. The overall method takes three passes
through the dataset as outlined in Algorithm 1. There is no need to compute ML explicitly since Γ
can be computed straight away from X, Y and the intermediate parameters.
6
Under review as a conference paper at ICLR 2018
3.1 Convergence Bounds
Theorem 1. Let us assume that we run Algorithm 1 on N i.i.d. samples with word vectors
x1 , x2 . . . xN and label vectors y1 , y2 . . . yN.
Let us define ε1
(ι+q⅛)
and ε2
(ι+q⅛))
for some δ ∈ (0, 1). If N ≥ max(n1, n2, n3), where
1.
n1 = c2
(log K+loglog (f ∙ q∏max))
2. n = Ω I (--—ει	) I
Ikd2sσΚ (M2)/ J
3.
n3 = Ω
10	2√2	V 2
σκ (M2)5/2 + d3s σκ (M2)3/2 J %
for some constants c1 and c2, then following bounds on the estimated parameters hold with proba-
bility at least 1 - δ,
Uμk - μk || ≤
||Yk - Y || ≤
∣∏k - ∏k | ≤
1 l60Pσ1(M2)	32p2σ1(M2)	4Pσ1(M2)、ει
(<⅛sσκ(M2)5/2 + d-3sσκ(M2)3/2 + d-2sσκ (M2)) √N
1	160	32√2	2 + √2	! 2ε1	8ε2
(⅛3σκ(M2)7/2 + d3sσκ(M2)5/2 + 后KTM了) √N + disσκ(M2)√N
2	200	40√2	! ε1
∖ σκ(M2)5/2 + σκ(M2)3/2 d3s√N
The terms σ1(M2)... σκ(M2) are the K largest eigenvalues of the matrix M2, whereas d2s =
E [nnz(x)2], ʤ = E [nnz(x)3] and 丸 = E [nnz(y)nnz(x)2], with nnz(x) representing the
number of distinct words and nnz(y) representing the number of labels present in an instance. The
proof is included in the appendix.
The convergence bounds on ∏ and μ are very similar to those in Anandkumar et al. (2012) and Wang
∙-v	∙-v	∙-v
& Zhu (2014), except for the terms d2s, d3s and dls in the denominator. These terms arise in our
convergence bounds since we use the probability moments, whereas Anandkumar et al. (2012) and
Wang & Zhu (2014) use the moments of word counts. The parameter γ is unique to our model,
although its bound contains those terms too.
We need at least one document with 3 distinct words or more to construct the third order mo-
ment M3, i.e., nnz(x) ≥ 3 for at least one document. Therefore, <⅛s = E [nnz(x)2] > 1 and
d3s = E Innz(X)3] > 1. Also, since every document has least one label, nnz(y) ≥ 1 for all
the documents, and therefore, and dis > 1. In practice, these terms can be much larger than 1 for
any real-life text corpus (The estimates of them for different experimental datasets are in Table 1).
Therefore, our algorithm achieves a tighter convergence bound than Anandkumar et al. (2012) or
∙-v	∙-v
Wang & Zhu (2014). Further, since the terms d2s and d3s appear in the denominators of n2 and n3
too, using probability moments also lowers the minimum number of samples required.
3.2 Computational Complexity
The bottleneck of the algorithm is the whitening step of M2, especially for the large datasets. M2
computed using Equation 16 is a symmetric p.s.d. matrix with very high sparsity. The number of
elements of M2 is O(PN=Innz(xi)2, with the worst case occurring when no two documents have
any word in common, and every element in X>X is 1. The top K eigenvalues for large symmetric
matrices are usually computed through Lanczos method. For a sparse matrix of size D × D with
O(PN=Innz(xi)2) number of elements, this step has a complexity of O((PN=I nnz(xi)2)K +
DK2 Mahadevan (2008).
7
Under review as a conference paper at ICLR 2018
ʌ
-J-VT t	.	. . 1 . ) ∙ t	t .	7l^Γ	7l^Γ	t ∙ ∙ . t	EI	.	.	. τι~r	♦
We do not compute the third order tensors M3 or ML explicitly. The step to compute M3 using
Equation 18 has a complexity of O(NK3), while the computation of ΓL using Equation 20 has a
ʌ
complexity of O((PN=Innz(yi))K2). We carry out the tensor factorisation of M3 ∈ RK×K×K
using the Tensor Toolbox Bader et al. (2015), and this step has a complexity of O(K4 log ) to
compute each of the K eigenvalues up to an accuracy of Kolda & Mayo (2011). These steps
contribute the most to the computational cost. The overall time complexity of Algorithm 1 is
O((PN=I nnz(Xi)I2)K + (D + PN=I nnz(yi))K2 + NK3 + K4 log )
As per as space requirement is concerned, the storage of M2 takes UP O(PN=I nnz(xi)2) space,
ʌ ʌ
whereas the word parameters like W and μ±K take up a space of Θ(DK), the label parameter Γ
takes up a space of Θ(LK), and M3 takes up a space of Θ(K3). The overall space complexity is
O(PN=I nnz(xi)2 + (D + L)K + K3).
4 Label Prediction
ʌ ʌ ʌ
Once we have extracted the model parameters P [h], P [v|h] and P [l|h] using Algorithm 1, a new
test document d with a distinct set of words Wd can be modelled as,
P[d∣h = k] = Y P[v∣h = k] Y (1 — P[v∣h = k])
v∈Wd	v/Wd
This step will take D multiplications involving floating point operations. However, since
ʌ ʌ
v P [v|h = k] = 1 for v ∈ {v1 . . . vD} and D can be very large, the values of P [v|h = k]
are much less than one for the most, and (1 一 P[v∣h = k]) ≈ 1. If we threshold P[v∣h = k] when
ʌ
v ∈/ Wd using some threshold η (say η = 0.01 or 0.001), then P [d|h = k] takes the form:
P[d∣h = k] = ɪɪ F,[v[h = k]	ɪɪ	(1 一 P[v∣h = k]
v∈Wd
ʌ v/Wd
P[v∣h=k]>η
ʌ
Usually only afew ofP [v|h = k] will be greater than the threshold, and this will significantly reduce
the number of multiplications when v ∈/ Wd without sacrificing accuracy. Also, since a document
has only a few distinct words, i.e. |Wd|	D, the left multiplicand (when v ∈ Wd) accounts for
only a few multiplications. From there, we can estimate the following using Bayes rule.
ʌ ʌ
p [h=k|d] = ∑⅛h=^≡⅛
∏k Y P[v∣h = k]	Y (1一 P[v∣h = k])
v∈Wd	v/Wd
P[v∣h=k]>η
-K
Xnk Y P[v∣h = k]	Y	(1一 P[v∣h = k])
k=1 v∈Wd	ʌ v/Wd
P[v∣h=k]>η
ʌ ʌ
And finally, the label probabilities for the document (P [l|d]) can be estimated from P [l|h] and
ʌ
P [h|d] using Equation 1.
5 Experimental Results
We carry out our experiments on six datasets ranging from small datasets like Bibtex with 4, 880
training instances to large datasets like WikiLSHTC with more than 1.7M training instances. The
description of the datasets are listed in Table 1. We categorise the datasets into three groups:
1.	Small: Bibtex and Delicious
2.	Medium: Wiki-31K and NYTimes
3.	Large: AmazonCat and WikiLSHTC
8
Under review as a conference paper at ICLR 2018
Name	# of Train Points	# of Test Points	Feature Dimen- sion	Label Dimen- sion	Average # of Features	Median # of Features	Average # of Labels	Median # of Labels	d∙2s	ʤs	dis
Bibtex	-4,880-	-2,515-	-1,836-	-159-	-68.67-	69	24	2	5,957	597K	15K
Delicious	12,920	-3,185-	-500-	-983-	-1829-	6	-19.02-	20	1,892	363K	24K
Wiki-31K	14,146	-6,616-	101,938	30,938	-669.05-	5Γ3	-18.64-	19	559K	545M	11M
NYTimes	14,669	15,989	24,670	-4,185~	-37391-	354	-5.40-	5	175K	95M	755K
AmazonCat	1,186,239	306,782	203,882	13,330	-71.09-	45	-5.04-	4	6,301	651K	27K
WikiLSHTC	’1,778,351	587,084	1,617,899	325,056	42.15	30	3.19	2	2,210	135K	7,594
Table 1: Description of the Datasets
True Labels
“airlines and
airplanes”,
“hijacking”,
“terrorism”
“armament,
defense and
military
forces”, “civil
war and
guerrilla
warfare”,
“politics and
government”
LEML
“airlines and airplanes”(0.34),
“terrorism” (0.30), “united states
international relations” (0.27), “elections”
(0.22), “armament, defense and military
forces” (0.18), “internationalrelations”
(0.18), “bombs and explosives” (0.15),
“murders and attempted murders ” (0.13),
“biographical information” (0.13), “islam”
(0.12)
“civil war and guerrilla warfare” (0.62),
“united states international relations”
(0.39), “united states armament and
defense” (0.23), “armament, defense and
military forces” (0.23),
“internationalrelations” (0.17), “oil
(petroleum) and gasoline” (0.11), “surveys
and series” (0.10),“military action” (0.09),
“foreign aid” (0.08), “independence
movements” (0.08)
MoM
“terrorism” (0.12), “united states
international relations” (0.08), “airlines
and airplanes” (0.07), “world trade center
(nyc)” (0.07), “hijacking” (0.07), “united
states armament and defense” (0.07),
“pentagon building” (0.03), “bombs and
explosives” (0.03), “islam” (0.02),
“missing persons ” (0.02)
“civil war and guerrilla warfare” (0.09),
“united states international relations”
(0.09), “united states armament and
defense” (0.06), “politics and
government” (0.04), “armament, defense
and military forces” (0.03),
“internationalrelations” (0.02),
“immigration and refugees” (0.02),
“foreign aid” (0.02), “terrorism” (0.02),
“economic conditions and trends” (0.02)
Table 2: Examples of label prediction from the NYTimes dataset. The numbers in parenthesis are
the scores for the top 10 labels. The scores of LEML and MoM have different ranges.
Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small
and large-scale datasets in Yu et al. (2014), we benchmark the performance of our method against
LEML. Also, there are other methods proposed on topic based embedding of the labels, most recent
of which is Rai et al. (2015) that extends Latent Dirichlet Allocation Blei et al. (2003) to Multi-label
learning, and uses Bayesian Learning through Gibbs Sampling. However, use of Gibbs sampler
limits the use of the algorithms only to the datasets of limited size. The largest dataset used in Rai
et al. (2015) is EurLex that is similar to NYTimes dataset in size. Using any MCMC based sampling
scheme is not viable for the large datasets such as AmazonCat or WikiLSHTC containing millions
of training instances.
In our experiments, we chose to measure AUC (of Receiver Operator Characteristics) against the
latent dimensionality (K). AUC is a versatile measure and has been used in a range of problems
from binary classification to ranking. Also, it is shown that there exists a one-to-one relation between
AUC and Precision-Recall curves in Davis & Goadrich (2006), i.e., the same algorithm achieving
a higher AUC will also produce a better Precision-Recall curve. We carried out our experiments
on Unix Platform on a single machine with Intel i5 Processor (2.4GHz) and 16GB memory, and no
multi-threading or any other performance enhancement method is used in the code.1 For the label
prediction step of MoM, we chose η = 0.001. For LEML, we ran ten iterations for the small datasets
and five iterations for the medium and large datasets, since the authors of LEML chose a similar
number of iterations for their experiments in Yu et al. (2014). For AmazonCat and WikiLSHTC
datasets, we ran LEML on an i2.4xlarge instance of Amazon EC2 with 122 GB of memory since
LEML needs significantly larger memory for these two datasets (Figure 2).
1To be shared later
9
Under review as a conference paper at ICLR 2018
(a) AUC and Memory (GB) of Bibtex Dataset
(b) AUC and Memory (GB) of Delicious Dataset
(c) AUC and Memory (GB) of NYTimes Dataset
Figure 2: AUC and Memory vs. Latent Dimensionality (K)
(d) AUC and Memory (GB) of Wiki-31K Dataset
(f) AUC and Memory (GB) of WikiLSHTC Dataset
Dataset	LEML	MoM	Speed-up (×)
Bibtex —	160s. —	300s.	0.53 —
Delicious 		60s. 		150s.	0.4
NYtimes	1 hour	6min. 		10 	
Wiki-31K	3 hr. 40 min.	15 min.	15
AmazonCat	13hrJ	1 hr. 15 min.	10
WikiLSHTC	>2 days t	3hr.	16
t Runtime on i2.4xlarge instance of Amazon EC2
Table 3: Training Time
We computed AUC for every test documents and performed a macro-averaging across the docu-
ments, and repeated the experiments for K = {50,75,100,125,150} (Figure 2). Both LEML and
Method of Moments perform very similarly, but the memory footprint of MoM is significantly less
than LEML. MoM takes longer to finish for the small datasets since tensor factorisation takes much
more time compared to the LEML iterations. However, as the size of the datasets grows, the LEML
iterations become more and more costly. For the medium and large datasets, MoM takes a fraction
of the time taken by LEML. For WikiLSHTC, LEML takes more than two days to finish, while
MoM finishes within a few hours. The training times of LEML and MoM for different datasets are
listed in Table 3.
6 Conclusion
Here we propose a latent variable model for multi-label learning in large-scale text corpus and
show how to extract the model parameters based on the spectral decomposition of the probability
moments of the words and the labels. Our method gives similar performance in comparison with
state-of-art algorithms like LEML while taking a fraction of time and memory for the medium and
the large datasets. Our method takes only three passes through the training dataset to extract all
10
Under review as a conference paper at ICLR 2018
the parameters, which contributes to its superior time performance. Also, the memory requirement
of our method is nominal when compared to the existing algorithms, and it scales to large datasets
like WikiLSHTC containing millions of Wikipedia articles just on a single node with 16GB of
memory. Lastly, since our method consists of only linear algebraic operations, it is embarrassingly
parallel and can easily be scaled up in any parallel ecosystem using linear algebra libraries. In our
implementation, we used Matlab’s linear algebra library based on LAPACK/ARPACK, although we
did not incorporate any parallelisation.
LEML has a bound of O(1/ √N) on the training loss Yu et al. (2014), whereas our method (MoM)
has a convergence bound of O(1∕√N) on the model parameters w.r.t. the number of training in-
stances N. However, when we compute AUC on the test dataset, the AUC of LEML decreases with
the latent dimensionality(K) for three datasets including AmazonCat with more than a million of
training instances. It shows the possibility of overfitting in LEML. MoM, on the other hand, is not an
optimisation algorithm, and the parameters are extracted through spectral decomposition of matrices
and tensors, rather than optimising any target function. It is not susceptible to over-fitting, which
is evident from its performance. But MoM has the requirement of N ≥ Ω(K2), and will not work
when N < Θ(K2). However, for smaller text corpora where N < Θ(K2) holds, 1-vs-all classifiers
are usually sufficient to predict the labels. We need low-rank models for large text corpora where
1-vs-all classifiers fail, and MoM provides a very competitive choice for such cases.
References
Anima Anandkumar, Yi-kai Liu, Daniel J Hsu, Dean P Foster, and Sham M Kakade. A spectral
algorithm for latent dirichlet allocation. In Advances in Neural Information Processing Systems,
pp. 917-925, 2012.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. Journal of Machine Learning Research, 15:
2773-2832, 2014. URL http://jmlr.org/papers/v15/anandkumar14b.html.
Brett W. Bader, Tamara G. Kolda, et al. Matlab tensor toolbox version 2.6. Available online,
February 2015. URL http://www.sandia.gov/~tgkolda/TensorToolbox/.
Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. Sparse local em-
beddings for extreme multi-label classification. In Advances in Neural Information Processing
Systems, pp. 730-738, 2015.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993-1022, March 2003. ISSN 1532-4435.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P. Foster, and Lyle H. Ungar. Spectral learn-
ing of latent-variable pcfgs: algorithms and sample complexity. Journal of Machine Learning
Research, 15(1):2399-2449, 2014.
Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc curves. In
Proceedings of the 23rd international conference on Machine learning, pp. 233-240. ACM, 2006.
Paramveer S. Dhillon, Jordan Rodu, Michael Collins, Dean P. Foster, and Lyle H. Ungar. Spectral
dependency parsing with latent variables. In Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and Computational Natural Language Learning,
EMNLP-CoNLL 2012, July 12-14, 2012, Jeju Island, Korea, pp. 205-213, 2012.
Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: Moment methods
and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical
Computer Science, ITCS ’13, pp. 11-20, 2013. ISBN 978-1-4503-1859-4.
Daniel Hsu, Sham M Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences, 78(5):1460-1480, 2012.
Aapo Hyvarinen, Juha Karhunen, and Erkki Oja. Independent component analysis, volume 46. John
Wiley & Sons, 2004.
11
Under review as a conference paper at ICLR 2018
Tamara G. Kolda and Jackson R. Mayo. Shifted power method for computing tensor eigenpairs.
SIAM Journal on Matrix Analysis and Applications, 32(4):1095-1124, OCtober 2011. doi: 10.
1137/100801482.
Sridhar Mahadevan. Fast spectral learning using lanczos eigenspace projections. In AAAI, pp.
1472-1475, 2008.
Yashoteja Prabhu and Manik Varma. Fastxml: A fast, accurate and stable tree-classifier for ex-
treme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference
on Knowledge discovery and data mining, pp. 263-272. ACM, 2014.
Piyush Rai, Changwei Hu, Ricardo Henao, and Lawrence Carin. Large-scale bayesian multi-label
learning via topic-based label embeddings. In Advances in Neural Information Processing Sys-
tems, pp. 3222-3230, 2015.
Le Song, Byron Boots, Sajid M Siddiqi, Geoffrey J Gordon, and Alex J Smola. Hilbert space
embeddings of hidden markov models. In Proceedings of the 27th international conference on
machine learning (ICML-10), pp. 991-998, 2010.
Hsiao-Yu Tung and Alex J. Smola. Spectral methods for indian buffet process inference. In Ad-
vances in Neural Information Processing Systems 27: Annual Conference on Neural Information
Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 1484-1492,
2014.
Yining Wang and Jun Zhu. Spectral methods for supervised topic models. In Advances in Neural
Information Processing Systems, pp. 1511-1519, 2014.
Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image
annotation. In IJCAI, volume 11, pp. 2764-2770, 2011.
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit S Dhillon. Large-scale multi-label learn-
ing with missing labels. In ICML, volume 31, 2014.
A Matrix Norm Inequalities
Here we paraphrase the inequalities regarding the matrix norms from Anandkumar et al. (2014),
which we will use further in our final proof. Let the true pairwise probability matrix and the third
order probability moment be M2 = p(v, v) and M3 = p(v, v, v), where v stands for the words.
Let us assume that we select N i.i.d. samples x1, . . . xN from the population, and the estimates
ʌ ʌ
of pairwise matrix and third order moment are M2 = p(y, y) and M3 = p(y, y, y). Let em2 =
∣∣M2 - M2∣∣2. We use the second order operator norm of the matrices here. Let Us assume em2 ≤
σK (M2 )/2, where σK is the Kth largest eigenvalue of M2 . We will derive the conditions which
satisfies this later.
If Σ = diag(σ1, σ2 . . . σK) are the top-K eigenvalues of M2, and U are the corresponding eigen-
vectors, then the whitening matrix W = UΣ-1/2. Also, W>M2 W = Ik×k. Then,
||W ∣∣2 = m max eig(W >W) = PmaX eig(Σ-1)
1
PQK (M2)
Similarly, if W t = W (W >W )-1,then W t = W Σ = U Σ1/2. Therefore,
||W 1∣2 = PmaX eig(Σ) = pσl(M2)
(21)
ʌ ʌ ʌ —∣- ʌ ʌ
Let W be the whitening matrix for M2 , i.e.,W>M2W=IK×K.Then by Weyl’s inequality,
ʌ ʌ
Qk (M2) - Qk (M2) ≤ ||M2 - M2∣∣, ∀k = 1, 2 ...K.
Therefore,
I∣W∣∣2
11
-k ≤ ——―-x—
QK (M2)	QK (M2) -∣M2 — M2∣∣
/	2
≤ QK (M2)
(22)
12
Under review as a conference paper at ICLR 2018
Also, by Weyl,s Theorem,
IlWt111 = σι(M2) ≤ σ1(M2) + εM2 ≤ 1.5σ1(M2) =⇒ ||W"∣2 ≤ p1.5σ1(M2) ≤ 1.5pσ1(M2)
(23)
Let U * be the eigenvectors of W M2 W, and Λ be the corresponding eigenvalues. Then we can write,
W M2 W=U *ΛU *>. Then W = W U *Λ-1∕2U *> whitens M2, i.e., W τM2W = I. Therefore,
∣∣I - Λ∣∣2 = ∣∣I - U*ΛU*τ∣∣2
=∣∣I - W M2W∣∣2
ʌ ʌ ʌ ʌ ʌ
=∣∣WM2W - WM2W∣∣2
≤ ∣∣W∣∣2∣∣M2 - M2∣∣
2
≤ 丁五KM2	(24)
σκ (M2)
εw = ∣∣W - W∣∣
=∣∣W - WU*Λ1∕2U*τ∣∣2
=∣∣W∣∣2∣∣I - U*λ1∕2u*τ∣∣2
=∣∣W∣∣2∣∣I - Λ1∕2∣∣2
≤∣∣W∣∣2∣∣I - Λ∣∣2
2
≤ σκ(M2)3∕2εM2
(25)
εW t
=∣∣Wt - Wt∣∣2
=∣∣WtU*A1/2U*τ - Wt∣∣2
=∣∣Wt∣∣2∣∣I - U*A1/2U*τ∣∣2
≤∣∣Wt∣∣2∣∣I - Λ∣∣2 ≤ 2Pσ;(MI)εM2
σκ (M2)
(26)
B Tensor Norm Inequalities
Let us define the second order operator norm of a tensor T ∈ rd0×d×d With D, D0 ∈ Z+ as,
∣∣T∣∣2 =sup{∣∣T(∙,u,u)∣∣ : U ∈ RD&∣∣u∣∣ = 1}
(27)
Lemma 1. For a tensor T ∈ RD'×d×d, ∣∣T∣∣2 ≤ ∣∣T∣∣F, where ∣∣T∣∣F is the Frobenius norm
defined as,
∣∣T ∣∣F
JX(Tij,k )2
Vijk
Proof. The tensor T can be unfolded as an array of D0 matrices each of size D × D, as T
[T1,T2 …TD0]. Then,
T(∙, u, u) = [uτTιu, UTT2u,... uτTDou] ∈ RD
13
Under review as a conference paper at ICLR 2018
Therefore,
||T ||| = sup I I [uτTιu, uγT2u,... uτTp∕ u] I I 2
IIuII = I
=sup (IUT Tiu|2 + |u>T|u|2 + …+ IUTTD u|2)
IIuII = I
≤ sup |uT Tiu|2 + sup |uTT2u|2 + …+ sup IUTTD u|2	(28)
I I u 11 = 1	I I u 11 = 1	I I u 11 = 1
Let US assume that the singular value decomposition of Td has the form (d ∈ [D0]),
Td = σi≠i≠t + σ2ψ2φ2 +-------+ OdΨd φD
where {ψi}D=ι are the left singular vectors, {φi}D=ι are the right singular vectors and {σ^}D=I are
the singular values.
Each of [ψι, ψ2... ψd] and [φι, φ2... Φd] forms a spanning set in RD, and any vector U ∈ RD can
be spanned using both ψ±D and。上D as the basis.
Let U =	α1ψ1	+ α2ψ2 +------+ QDΨd	= β1Φ1 +	β2Φ2	+----+ βDΦd,	where ai：D	and	e上D are
scalars.
Since ||u|| = √< u, u > = 1,
||a|| = { q + q2 +... qD =1 and ||e|| = Jβ2 + β2 + ∙ ∙ ∙ βD =1
Therefore,
|uT Tdu| =
≤
≤
1αiσιβι + Q2σ2β2 ... QDσDβD |
,σ2 + σ2 + ∙∙∙ + σD JQ2β2 + α2β2 . ∙ ∙ QDβD (Cauchy-Schwartz Inequality)
Jσ2 + 峭 + ∙∙∙ + σD JQI + QI ∙∙∙+ QD q∕β2 + β2 ∙∙∙+ βD
Jσ2 + 峭 + ∙∙∙ + σD
||Td||F
Equality holds when Td is of rank 1. Since this holds for any vector u ∈ RD such that ||u|| = 1,
sup IUTTdUI ≤ ||Td||F for d ∈ [D0]. Therefore, from Equation 28,
I I u I I = 1
IIT||2 ≤ (IIT1IIF + ∣∣IT2IIF + ∙∙∙ + ||Td||F) = ||t||F P ||tk ≤iit||f
□
Lemma 2. (Robust Power Method from (Anandkumar et al., 2014)) If T = T + E ∈ rk×k×k,
where T is an symmetric tensor with orthogonal decomposition T = PK=I λkUk 0 Uk 0 Uk with
each λk > 0, and E has operator norm ∣∣E∣∣2 ≤ e. Let λmi∏ = minK=1{λk} and λmax =
maxK=1{λk}. Let there exist constants c1, c2 such that E ≤ c1 ∙ (λmin∕K), and N ≥ c2(log K +
log log (λmax∕e)). Then if Algorithm 1 in (Anandkumar et al., 2014) is called for K times, with
L = Poly(K)log(1∕η) restarts each time for some η ∈ (0,1), then with probability at least 1 一 η,
there exists a permutation Π on [K], such that,
∣∣uΠ(k) - uk∣∣ ≤ 8、--, ∣λk - λΠ(k)∣ ≤ 5e ∀k ∈ [K]	(29)
λΠ(k)
Since E ≤ c1 ∙ (λmin∕K) and λk = √= ,∀k ∈ [K], we need
N ≥ c2	(logK	+ loglog (Kλmax))	= c2	(logK	+ loglog	(KIrnmZ))	(30)
∖	∖ c1λmin ) )	∖	∖ c1 V πmin J J
This contributes in the first lower bound (n1) of N in Theorem 1.
14
Under review as a conference paper at ICLR 2018
C Tail Inequality
Lemma 3. If we draw N i.i.d. sample documents x1, x2 . . . xN, and probability mass function,
pairwise probability and triplet probability ofthe words V estimatedfrom these N samples are p(v),
p(v,v) and p(v,v,v) respectively, whereas the true probabilities are p(v), p(v,v) and p(v,v,v)
respectively with v ∈ {v1, v2 . . . vD} , then with probability at least 1 - δ with δ ∈ (0, 1),
||P(V)- P(V)IIF
||p(v, v) -p(v,v)∣∣f
∣∣p(v, v, v) -p(v,v,v)∣∣f
(31)
(32)
(33)
where, d1s = E [nnz(x)], d2s = E Innz(X)2], d3s = E [nnz(x)3], and nnz(x) is the non-zero
entries in the binary vector representing the words in the documents as described in Section 3 (main
paper).
Proof. Since the number of distinct words in a document is always bounded, we can assume IIxII ≤
1 ∀x without loss of generality. Then from Lemma 7 of supplementary material of Wang & Zhu
(2014), with probability at least 1 - δ with δ ∈ (0, 1),
∣∣E[x
∣∣E[χ]- E[x]∣∣
∣ ∣E[xx>] — E[xx>] 11
® x ® x] — E[x 0 x ® x] 11
(34)
(35)
(36)
ʌ
where E stands for true expectation, and E stands for the expectation estimated from the N samples,
i.e.,
1N	1
E[x] = N X Xi = NX 1
i=1
1N	1
E[xx>] = NEXi X Xi = NX>X
i=1
1N	1
E[x 0 x 0 x] = 一 ɪ2 Xi X Xi 0 Xi = 一X 0 X 0 X
N i=1	N
Now, since the samples contains binary data, probability of the items can be computed from as
E[x]
P(V) = PvERi
(37)
(38)
Similarly,
P(V)
E[x]
PV E[xv]
15
Under review as a conference paper at ICLR 2018
入	~
∑ E[xv]. Assigning dιs
v = 1
D
Now, since x is a binary vector of dimension D, the sum across its dimensions is P Xv = nnz(x),
v = 1
DD
and therefore, E E[xv] = E[£ xv] = E[nnz(x)] ≈ E[nnz(x)]
v=1	v=1
E[nnz(x)], from Equation 37 and 38, we get
ʌ
” 、E E	E[x] - E[x]
P(V) - p(v)=-----------
d1s
DD
Similarly, summing across the respective dimensions, we get P P (XXT)V1,v2
vɪ = 1 V2 = 1
DD	DDD
Σ Σ xvɪxv2 — nnz(x) and〉: 〉:〉: (x (^) X (^) x)v1,v2,v3 — nnz(x).
vι = 1 v2 = 1	vɪ = 1 v2 = 1 v3 = 1
DD	DD
Therefore, P P E[(xxτ)v1,v2] = E[nnz(x)2] = d^ ≈ P P E[(xxτ)v1,v2], and
v1=1 v2=1	v1=1 v2=1
DDD	DDD
E E ∑ E[(x0X0x)v1,v2,v3]= E[nnz(x)3] = ⅛s ≈ ∑ E £ E[(x0X0x)v1,v2,v3].
v1=1 v2=1 v3=1	v1=1 v2=1 v3=1
From here, we can show that,
E[xxτ] — E[xxτ]
p(qv) - p(qv)=------------------
d2s
ʌ
E[x 0 x 0 x] — E[x 0 x 0 x]
P(V, v, v) — p(v,v,v)=----------工-------------
Plugging these equations in Equation 34, 35 and 36, we can complete the proof.
□
Also, if y represents the label vector of the documents, since the number of labels per documents is
limited, we can assume ||y|| ≤ 1 without a loss of generality. Then,
ʌ
E[y 0 x 0 x] — E[y 0 X 0 x]
1 N
=N E y 0 Xi 0 Xi — E[y 0 X 0 x]
1N	1N	1N
=N E yi 0 Xi 0 Xi — N E yi 0 E[x 0 x] + N E yi 0 E[x 0 x] — E[y 0 X 0 x]
1N	1N
N): yi 0 (Xi 0 Xi — E[x 0 X]) + N Σ yi 0 E[x 0 x] — E[y 0 x 0 x]
Therefore,
I I E[y 0 x 0 x] — E[y 0 X 0 x] ∣ ∣ F ≤ ∣ ∣ E[y] ∣ ∣ ∣ ∣ E[x 0 x] — E[x 0 x] ∣ ∣ F + ∣ ∣ E[y] — E[y] ∣ ∣ ∣ ∣ E[x 0 x] ∣ ∣ F
=∣ ∣ E[y] ∣ ∣ ∣ ∣E[xxτ]- E[xx>] ∣ ∣ F +1 ∣E[y]- E[y] ∣ ∣ ∣ ∣E[xxτ] ∣ ∣ F
Since ||y|| ≤ 1, ||E[y]|| ≤ 1. Also, from ||x|| ≤ 1, we get HE[xxτ]H ≤ ||x||2 ≤ 1. Therefore,
∣ ∣ E[y 0 x 0 x] — E[y 0 X 0 x] ∣ ∣ F ≤ ∣ ∣ E[xxτ] — E[xxτ] ∣ ∣ F + ∣ ∣ E[y] — E[y] ∣ ∣	(41)
16
Under review as a conference paper at ICLR 2018
From Equation 34 and 35,
P
P
心朋-EMF ≤√⅛(1+rwδ)) ≥1 -δ
I I Elx”〕-Elx”〕I I F ≤√2N 0+r l⅛n)] ≥1 -δ
Reversing the tail inequalities,
≤ δ
P11 i E[χχ>] - E[χxτ] ∣ i F ≥ √2N
≤ δ
If Ei and E are two events,
P (E1 ∩ E2) = P (Ei) + P (E2) - P (Ei U E2)
≤ P(Ei) + P(E2)
Therefore,
Or,
P ∣ ∣ E[y] - E[y] ∣ ∣ F + ∣ ∣ E[xxτ] - E[xxτ] ∣ ∣ F ≥
4
√N
≤ 2δ
P 1 1 E[y] - E[y] 1 I f + ∣ 1 E[xxτ] - E[xxτ] ∣ ∣
^)l≥1 - 2δ
From Equation 41,
P | ∣ E[y 0 X 0 x] — E[y 0 X 0 x] ∣ ∣ F ≤
Replacing δ by δ∕2,
P ∣ ∣ E[y 0 X 0 x] — E[y 0 X 0 x] ∣ ∣ F ≤
LDD
Similar to previous cases PP P (y 0 X 0 x)ι,v1,v2
l=1 vι = 1 v2 = l
LD D
EEE E[(y 0 x 0 x)ι,v1,v2] = E[nnz(y)nnz(x)2]
l=i v1=i v2=i
∙-v
dis = E[nnz(y)nnz(x)2], we can show that,
= nnz(y)nnz(x)2, and therefore,
≈ E[nnz(y)nnz(x)2]. Assigning
ʌ
E[y 0 x 0 x] - E[y 0 x 0 x]
p(l, v, V) - p(l,5 V) =------------7--------------
dls
where l stands for the labels (l ∈ {l1,l2 ...Il}). Therefore, with probability at least 1 一 δ,
Mp0, v, V) - p(1, v,v)∣∣F ≤ y-√== (1 + J log(∕δ))
dis √N ∖ N2J
(42)
(43)
F ≤√N (1 + j 吟"
17
Under review as a conference paper at ICLR 2018
D Completing THE Proof
Assigning ε1
in the inequalities of Lemma 3, we get
εM
εM3
∣∣M2 — M2∣∣2 = Ilp(V, v) — p(v,v)∖∖2 ≤ Ilp(V, v) — P(V,V)IIF ≤
2ε1
d2s√N
, and
llM3 — M3H2 = UP(V, v, v) — P(V,v,v)||2 ≤ UP(V,v,v) — P(V, v, v)||f ≤
2ε1
d3s√N
since operator norm is smaller than Frobenius norm for both matrices and tensors (Lemma 1).
Therefore, to satisfy sm2 ≤ σκ(M2)∕2, we need N ≥ Ω
second lower bound (n2) of N in Theorem 1.
d⅛) )2
. This contributes in the
Similarly, assigning ε2 = f 1 + JIOg(：/6 ) in Equation 43, we get
εML = HmL - Ml∖∖2 = ||P(l, v,v) — P(l,V,V)||2 ≤ ||P(l, v, v) — P(l,V,v)||F ≤
4ε2
dls√N
From Appendix B in (?),
εtw = ||M3(W, W,W) — M3(W,W, W)||2
≤ ||M3||2 (uW"2 + H^W||2||W“2 + ||WM2) εW + UW||3£M3
(2 + √2 + 1)	2√2
≤ Hm3H2 σκ(M2)	ɛ^ + σκ(M2)3∕2 εM3
(3 + √2)	2	2√2
≤ Hm3H2∙ σκ(M2)3∕2 εM2 + σκ(M2)3∕2 εM3
/ 10||M3||2	i	2√2
≤ σκ(M2)5/2 ' εM2 + σκ(M2)3/2 εM3
≤ (-o—+~	2√2	)筌
一∖dσκκ(M2)5/2	d3sσκ(M2)3/2 / √N
(44)
From Lemma 1, ||M3||2 ≤ ||M3||f ≤ 1, because M3 is a tensor with individual elements as
probabilities, with the sum of all elements being 1.
From Lemma 2, E ≤ ci ∙ (λmin∕K), and we can assign E as the upper bound of εtw. To satisfy this,
we need
10	,	2√2	ʌ 2ε	λmin
+	≤ ci	, or,
d2sσκ(M2)5/2	⅛sσκ(”2)3/2/ √N — K
10	2√2	ʌ 2ε	1
+	≤≤ Ci
d∕2sσ κ (Mz)5/2	d13sσ κ (Mz)3∕2J λ∕N	K √πmax
SinCe πmax
n3 in Theorem 1.
≤ L we need N ≥ ω (K 2 (MK(M2)5/2 + £屋温)3/2	£) ∙ ThiS conIributes tθ
18
Under review as a conference paper at ICLR 2018
Here, we will derive the convergence bounds for the parameters. Since μk = W^uk (Algorithm 1
in main paper), with probability at least 1 - δ,
∖∖μk - μk∖∖
∖∖WtUk - WtUk∖∖
=∖∖WtUk - WtUk + WtUk - WtUk∖∖
≤ ∖∖Wt∖∖2∖∖Uk -Uk∖∖ + ∖∖wt - Wt∖∖2∖∖Uk∖∖
,	8f
≤ ∖∖Wt∖∖2丁 + εwt
λk
≤ 8√σiE + 叶黑 εM2
σK (M2)
(45)
where ∖∖Uk - Uk∖∖ ≤ 8e∕λk from Lemma 2, and ∖∖Uk∖∖ = 1. Since 1∕λk = √πk ≤ 1, assigning C as
the upper bound of εtw in Equation 44, we can say that with probability at least 1 - δ,
M-μk∖∖≤ 8√σ1w (— + ~	2√2	)筌 + 2pσ^a
(⅛3σκ(M2)5/2	d3sσκ(M2)3/2 J √N σκ (M2) ⅛3√>
/ / 160√σι(M2) l 32√2σι(M2) l 4√σ1(M2) ʌ ɛi	，心
≤	+ +	+ +	(46)
一∖d2sθκ (M2)5/2	d3sσκ (M2)3/2	d2sOκ (M2)) √N
ɛi
(48)
ʤs √N
Also,
ʌ ʌ
∖∏k - ∏k∖ = = - L =	("	+ ”(：k^k	= I √不前k	(√∏k +	√∏k)	(ʌk	-	ʌk) I
λk	λk	λkλk	1	V	7	1
≤ 2∖λk - λk∖ ≤ 10C	(47)
since ∖λk - λk ∖ ≤ 5c from Lemma 2. Therefore, with probability at least 1 - δ, we get
∖「∖ J 200	40√2
∖πk πk∖ ≤ (σκ(M2)5/2 + σκ(M2)3/2
where ε = (1 + J log(Jδ)).
Further, since Y = u>ML(W, W)Uk,
I I Yk- Yk I I
=I I U>ML(W,W)Uk - UkTML(W,W)Uk I I
≤ I	I U>ML(W, W)Uk - U>ML(W, W)Uk I I + I I U>ML(W,W)Uk - U>Ml(W, W)Uk I I
≤ I	I U>ML(W,W )Uk - U>ML (W, W )Uk + U>ML(W,W )Uk - u> ML (W, W )Uk I I + I	I Uk I I2I	I μl(w,w )	- Ml(W,W)
≤ I	I Uk - Uk I I I I ML (W, W) I I 2 I I Uk II + I I Uk - Uk I I I I ML (W, W) I I 2 I ® I I + I I Uk I 印 Μl(w,w	) - Ml(W, W) I I 2
=2 I I Uk - Uk I I I I ML (W, W) I I 2 + I I ML (W, W) - ML (W, W) I I 2
≤ 2 I I Uk - Uk I I I I W I I 2 I I ML I I 2 + I I ML (W, W) - M(W, W) I I 2
≤ 2 I I Uk - Uk I I I I W I I 2 + I I ML (W, W) - M(W,W) I I 2	(49)
SinCeI I Uk〔 I =〔1 Uk I1 = 1. Also, from Lemma 1, I I ML I1 2 ≤〔1 ML I1 F ≤ 1, since11 ML I1 2 is a
tensor with the sum of its elements as 1.
19
Under review as a conference paper at ICLR 2018
Now,
IlMLMW) - Ml(W,W)∣∣2
=I I Ml(w,w) - ML(W, W) + ML(W, W) - ML(W, W) I ∣ 2
≤ I I Ml (W, W) - Ml(W,W) I I 2 + ∣ I WW11 2 ∣ ∣ ML - ML ∣ ∣ 2
=I I Ml (W,	W) - Ml(w,W) + Ml(w,W)- Ml(W,W) ∣ ∣ 2 + ∣ I W∣ I 2 I ∣ ML - ML ∣	∣ 2
≤ I I Ml (W,	W) - Ml(w,W) I I 2 + ∣ I Ml(w,W)- Ml(W,W) I I 2 + ∣ I WW ∣ ∣ 2 ∣ ∣ ML	-	ML ∣ ∣ 2
≤I I w I I I Iw - WI I I I MLI I 2 + I I WI I I I w - WI I I I MLI I 2 + I I WI I 2 I I ML- ML I I 2
=(I I w I I +	I I WI I) I I MLI I 2≡w + I IwI I2SMl
≤ ( I I W I ∣ + ∣ I W∣ ∣)εw + ∣ I W∣ I 2SMl	(50)
Therefore,
I I Yk — Yk ∣ ∣
≤	2 I I Uk - UkI I I I W I I 2 + ( I I W I ∣ + ∣ I W∣ I )εw + ∣ I WII2SMl
≤	16 三 I I W ∣ ∣ 2 + ( I I W ∣ ∣ + ∣ I W ∣ ∣ )εw + ∣ ∣ W∣ ∣2sml
入k
≤	16σ (fM)+ ( i I W I i + i I WI I )εW + i I WI I 2εML
K ∖2 2 )
Since 1∕λk = √πk ≤ 1. Assigning E as the upper limit of εtw in Equation 44, with probability
1 - δ,
l	bk -γk ∣ l
-	16	(	10	2√2	)	2ε1	1 +	√2	2	2ε1	2	4s2
≤ σκ(M2) (d2Wκ(M2)5/2 + d3Wκ(M2)3/2 ) √N + √σκ(M2) σκ(M2)3/2 √⅛ + σκM √Ndls
(	80	16√2	1 + √2 ' 4ει	8ε2
_ (Zs^K(M2)7/2 + d3sσκ(M2)5/2 + a,2sθκ(M2)2 J √N + dlsσK(M2)√N
where, ε1 = (1 + Jlog(y) and ε2 = (1 + Jlog(y). ThiS completes the proof of Theorem
1.
20