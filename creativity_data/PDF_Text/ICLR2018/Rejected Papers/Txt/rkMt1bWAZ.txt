Under review as a conference paper at ICLR 2018
Bias-Variance Decomposition for
B oltzmann Machines
Anonymous authors
Paper under double-blind review
Ab stract
We achieve bias-variance decomposition for Boltzmann machines using an infor-
mation geometric formulation. Our decomposition leads to an interesting phe-
nomenon that the variance does not necessarily increase when more parameters
are included in Boltzmann machines, while the bias always decreases. Our result
gives a theoretical evidence of the generalization ability of deep learning archi-
tectures because it provides the possibility of increasing the representation power
with avoiding the variance inflation.
1	Introduction
Understanding why the deep learning architectures can generalize well despite their high representa-
tion power with a large number of parameters is one of crucial problems in theoretical deep learning
analysis, and there are a number of attempts to solve the problem with focusing on several aspects
such as sharpness and robustness (Dinh et al., 2017; Wu et al., 2017; Keskar et al., 2017; Neyshabur
et al., 2017; Kawaguchi et al., 2017). However, the complete understanding of this phenomenon is
not achieved yet due to the complex structure of deep learning architectures.
To theoretically analyze the generalizability of the architectures, in this paper, we focus on Boltz-
mann machines (Ackley et al., 1985) and its generalization including higher-order Boltzmann ma-
chines (Sejnowski, 1986; Min et al., 2014), the fundamental probabilistic model of deep learning
(see the book by Goodfellow et al. (2016, Chapter 20) for an excellent overview), and we firstly
present bias-variance decomposition for Boltzmann machines. The key to achieve this analysis is
to employ an information geometric formulation of a hierarchical probabilistic model, which was
firstly explored by Amari (2001); Nakahara & Amari (2002); Nakahara et al. (2006). In particular,
the recent advances of the formulation by Sugiyama et al. (2016; 2017) enables us to analytically
obtain the Fisher information of parameters in Boltzmann machines, which is essential to give the
lower bound of variances in bias-variance decomposition.
We show an interesting phenomenon revealed by our bias-variance decomposition: The variance
does not necessarily increase while the bias always monotonically decreases when we include more
parameters in Boltzmann machines, which is caused by its hierarchical structure. Our result indicates
the possibility of designing a deep learning architecture that can reduce both of bias and variance,
leading to better generalization ability with keeping the representation power.
The remainder of this paper is organized as follows: First we formulate the log-linear model of hi-
erarchical probability distributions using an information geometric formulation in Section 2, which
includes the traditional Boltzmann machines (Section 2.2) and arbitrary-order Boltzmann machines
(Section 2.3). Then we present the main result of this paper, bias-variance decomposition for Boltz-
mann machines, in Section 3 and discuss its property. We empirically evaluate the tightness of our
theoretical lower bound of the variance in Section 4. Finally, we conclude with summarizing the
contribution of this paper in Section 5.
2	Formulation
To theoretically analyze learning of Boltzmann machines (Ackley et al., 1985), we introduce an
information geometric formulation of the log-linear model of hierarchical probability distributions,
which can be viewed as a generalization of Boltzmann machines.
1
Under review as a conference paper at ICLR 2018
2.1	Preliminary: Log-Linear Model
First we prepare a log-linear probabilistic model on a partial order structure, which has been intro-
duced by Sugiyama et al. (2016; 2017). Let (S, ≤) be a partially ordered set, or a poset (Gierz et al.,
2003), where a partial order ≤ is a relation between elements in S satisfying the following three
properties for all x, y, z ∈ S: (1) x ≤ x (reflexivity), (2) x ≤ y, y ≤ x ⇒ x = y (antisymmetry),
and (3) x ≤ y, y ≤ z ⇒ x ≤ z (transitivity). We assume that S is always finite and includes the
least element (bottom) ⊥ ∈ S; that is, ⊥ ≤ x for all x ∈ S. We denote S \ {⊥} by S+ .
We use two functions, the zeta function Z: S × S → {0, 1} and the Mobiusfunction μ: S × S → Z.
The Zeta function Z is defined as Z(s, x) = 1 if S ≤ x and Z(s, x)=0 otherwise, while the MobiUs
function μ is its convolutional inverse, that is,
XZ(x,s)μ(s,y)= X μ(s,y) = { 0 otherwise,
s∈S	x≤s≤y
which is inductively defined as
(1	if X = y,
μ(χ,y) = - - Hx≤s<y μ(x,s) if x<y,
(0	otherwise.
For any functions f, g, and h with the domain S such that
g(x) = X Z(s, x)f (s) = X f(s),	h(x) = X Z(x, s)f(s) = X f(s),
s∈S	s≤x	s∈S	s≥x
f is uniquely recovered using the MObiuS function:
f (χ) = fμ (s,x) g (S),	f (χ) = E μ (χ, S)h(S).
s∈S	s∈S
This is the Mobius inversion formula and is fundamental in enumerative combinatorics (Ito, 1993).
Sugiyama et al. (2017) introduced a log-linear model on S, which gives a discrete probability distri-
bution with the structured outcome space (S, ≤). Let P denote a probability distribution that assigns
a probability p(x) for each x ∈ S satisfying Px∈S p(x) = 1. Each probability p(x) for x ∈ S is
defined as
log p(x) := X Z(S, x)θ(S) = X θ(S).	(1)
s∈S	s≤x
From the MObiUS inversion formula, θ is obtained as
θ (χ) = Eμ (s,x )log p (S).	⑵
s∈S
In addition, we introduce η : S → R as
η (χ) := EZ (χ, s) p (s) = £p( S), p (χ) = fμ (χ, s) η (S) .	⑶
s∈S	s≥x	s∈S
The second equation is from the MObiUS inversion formula. Sugiyama et al. (2017) showed that
the set of distributions S = {P | 0 < p(x) < 1 and P p(x) = 1} always becomes the dually
flat Riemannian manifold. This is why two functions θ and η are dual coordinate systems of S
connected with the Legendre transformation, that is,
θ = R中 (η), η = Nψ (θ)
with two convex functions
ψ(θ) := -θ(⊥) = - logp(⊥), ψ(η) := £p(χ)iogp(χ).
x∈S
Moreover, the Riemannian metric g(ξ ) (ξ = θ or η) such that
g (θ) = RRψ (θ),	g (η) = w^(η),
2
Under review as a conference paper at ICLR 2018
which corresponds to the gradient of θ or η, is given as
∂η(x)	log p(s) log p(s)
gχy(θ) = ∂θy = E -χ^r= EC(x,s)ζ(y，s)p(s)-η(x)η(y))	(4)
s∈S
gχy (η ) = dθχ = E [ l^η(Xs) logys) ] = XX μ (s，x)μ (s，y)p (S )-1 .	⑸
for all x,y ∈ S+. Furthermore, S is in the exponential family (Sugiyama et al., 2016), where θ
coincides with the natural parameter and η with the expectation parameter.
Let us consider two types of submanifolds:
Sα = { P ∈ S | θ(x) = α(x) for all x ∈ dom(α) } ,
Sβ = { P ∈ S | η(x) = β (x) for all x ∈ dom(β ) }
specified by two	functions α, β with dom(α), dom(β) ⊆ S+, where the former	submanifold Sα has
constraints on θ	while	the latter Sβ has those on η. It is known in information	geometry that Sα	is
e-flat and Sβ is m-flat, respectively (Amari, 2016, Chapter 2.4). Suppose that dom(α) ∪ dom(β) =
S+ and dom( α) ∩ dom(β) = 0. Then the intersection Sa G Sβ is always the singleton, that
is, the distribution Q satisfying Q ∈ Sα and Q ∈ Sβ always uniquely exists, and the following
Pythagorean theorem holds:
DKL(P, R) = DKL(P, Q) + DKL(Q, R),	(6)
DKL(R, P) = DKL(R, Q) + DKL(Q, P)	(7)
for any P ∈ Sα and R ∈ Sβ .
2.2	Standard B oltzmann machines
A Boltzmann machine is represented as an undirected graph G = (V, E) with a vertex set V =
{1, 2,..., n} and an edge set E ⊆ {{i,j} | i,j ∈ V}. The energy function Φ:{0, 1 }n → R of the
Boltzmann machine G is defined as
Φ(x; b, w) = -	bixi -	wij xi xj
i∈V	{i,j}∈E
for each x = (x1, x2, . . . , xn) ∈ {0, 1}n, where b = (b1, b2, . . . , bn) and w = (w12, w13, . . . ,
wn-1n) are parameter vectors for vertices (bias) and edges (weight), respectively, such that wij = 0
if {i, j} 6∈ E. The probability p(x; b, w) of the Boltzmann machine G is obtained for each x ∈
{0, 1}n as
p(ʃ； b, W)=exp(-*; b W))	⑻
Z
with the partition function Z such that
Z =	exp(-Φ(x; b, W))	(9)
x∈{0,1}n
to ensure the condition Px∈{0,1}n p(x; b, W) = 1.
It is clear that a Boltzmann machine is a special case of the log-linear model in Equation (1) with
S = 2V, the power set ofV, and ⊥ = 0. Let each x ∈ S be the set of indices of “1” ofx∈ {0, 1}n
and ≤ be the inclusion relation, that is, x ≤ y if and only if x ⊆ y. Suppose that
B = { x ∈ S+ I ∣x∣ = 1 or x ∈ E } ,	(10)
where |x| is the cardinality of x, which we call a parameter set. The Boltzmann distribution in
Equations (8) and (9) directly corresponds to the log-linear model in Equation (1):
log p(x) = X ζ(s, x)θ(s) - ψ(θ),	ψ(θ) = -θ(⊥) = log Z,	(11)
s∈B
3
Under review as a conference paper at ICLR 2018
where θ(x) = bx if |x| = 1 and θ(x) = wx if |w| = 2. This means that the set of Boltzmann
distributions S(B) that can be represented by a parameter set B is a submanifold of S given as
S(B) := { P ∈ S | θ(x) = 0forallx 6∈ B } .	(12)
Given an empirical distribution P. Learning of a Boltzmann machine is to find the best approxima-
tion of P from the Boltzmann distributions S(B), which is formulated as a minimization problem
of the KL (Kullback-Leibler) divergence:
p^(s)	p( s)
min D KL ( P,Pb ) = mm fp( S )log ɪrv	(13)
PB∈S(B)	PB∈S(B) pB(s)
s∈S
This is equivalent to maximize the log-likelihood L(PB) = N P∈ p(S) logPB (S) with the sample
size N. Since we have
∂∂
Ka(、Dkl(P, Pb) = AQ ( 、£p(S)logPB(S)
∂θB (x)	∂θB (x)
s∈S
=∂⅛ XS (p(S s ⊥X≤θ (U) 卜 ∂⅛yψ θB) X p( s )
=η( χ) — ηB (χ),
ʌ
the KL divergence Dkl(P, PB) is minimized when η(x) = ηB (x) for all x ∈ B, which is well
known as the learning equation of Boltzmann machines as ^^(x) and ηB (x) coincides with the ex-
pectation for the outcome x with respect to the empirical distribution P obtained from data and
the model distribution PB represented by a Boltzmann Machine, respectively. Thus the minimizer
PB ∈ S(B) of the KL divergence DKL(P, PB) is the distribution given as
J ηB (x) = η(x) if X ∈ B u {±},	(14)
θB (x) = 0	otherwise.
This distribution PB is known as m-projeCtion of P onto S (B) (Sugiyama et al., 2017), which is
unique and always exists as S has the dually flat structure with respect to (θ, η).
2.3	Arbitrary-Order B oltzmann Machines
The parameter set B is fixed in Equation (10) in the traditional Boltzmann machines, but our log-
linear formulation allows us to include or remove any element in S+ = 2V \ {⊥} as a parameter.
This attempt was partially studied by Sejnowski (1986); Min et al. (2014) that include higher order
interactions of variables to increase the representation power of Boltzmann machines. For S = 2V
with V = {1, 2,...,n} and a parameter set B ⊆ S+, which is an arbitrary subset of S+ = S \ {0},
the Boltzmann distribution given by an arbitrary-order Boltzmann machine is defined by
logp(x) = X Z(S, x)θ(S) — ψ(θ), ψ(θ) = —3U),
s∈B
and the submanifold of Boltzmann distributions is given by Equation (12). Hence Equation (14)
gives the MLE (maximum likelihood estimation) of the empirical distribution P.
Let B1, B2, . . . , Bm be a sequence of parameter sets such that
B1 ⊆ B2 ⊆ …⊆ Bm-1 ⊆ Bm = S+ .
Since we have a hierarchy of submanifolds
S(Bι) ⊆ S(B2) ⊆ …⊆ S(Bm-1) ⊆ S(Bm) = S,
we obtain the decreasing sequence of KL divergences:
D kl( P, Pb 1) ≥ D kl( P,Pb 2) ≥ …≥ D kl( P,PBm-J ≥ D kl( P,Ppm) = 0,	(15)
ʌ ʌ
where each PBi = argminP∈S(Bi) DKL (P, P), the best approximation ofP using Bi.
4
Under review as a conference paper at ICLR 2018
There are two extreme cases as a choice of the parameter set B. On the one hand, if B = 0,
the Boltzmann distribution is always the uniform distribution, that is, p(x) = 1/2|V | for all x ∈
S . Thus there is no variance but nothing will be learned from data. On the other hand, if B =
S+, the Boltzmann distribution can always exactly represent the empirical distribution P, that is,
DKL(P, PB) = DKL(P, P ) = 0. Thus there is no bias in each training but the variance across
different samples will be large. To analyze the tradeoff between the bias and the variance, we
perform bias-variance decomposition in the next section.
Another strategy to increase the representation power is to use hidden variables (Le Roux & Bengio,
2008) such as restricted Boltzmann machines (RBMs) (Smolensky, 1986; Hinton, 2002) and deep
Boltzmann machines (DBMs) (Salakhutdinov & Hinton, 2009; 2012). A Boltzmann machine with
hidden variables is represented as G = (V ∪H, E), where V and H correspond to visible and hidden
variables, respectively, and the resulting domain S = 2V∪H (see Appendix for the formulation of
Boltzmann machines with hidden variables as the log-linear model). It is known that the resulting
model can be singular (Yamazaki & Watanabe, 2005; Watanabe, 2007) and its statistical analysis
cannot be directly performed. Studying bias-variance decomposition for such Boltzmann machines
with hidden variables is the interesting future work.
3 Bias-Variance Decomposition
Here we present the main result of this paper, bias-variance decomposition for Boltzmann machines.
We focus on the expectation of the squared KL divergence E[Dkl(P*,Pb)2] from the true (un-
ʌ ʌ
known) distribution P * to the MLE PB of an empirical distribution P by a Boltzmann machine with
a parameter set B, and decompose it using information geometric properties.
In the following, we use the MLE PB* of the true distribution P*, which is the closest distribution in
the set of distributions that can be modeled by Boltzmann machines in terms of the KL divergence
and is mathematically obtained with replacing P with P * in Equation (13).
Theorem 1 (Bias-variance decomposition of the KL divergence). Given a Boltzmann machine with
a parameter set B. Let P * ∈ S be the true (unknown) distribution, PB ,Pb ∈ S (B) be the MLEs
of P * and an empirical distribution P, respectively. We have
Eh D KL (P * ,Pb )2i = D KL (P *,PB )2 + Eh D kl( PB ,Pb H
= DKL (P*, PB* )2 + var(PB* , B) + irreducible error
|------{z-----} |-----{z----}
bias2	variance
≥ Dkl(P*, PB)2 + var(PB, B) + irreducible error,
var(PB*,B) =XX
η*(S)η*(U)cov(Θb(S),Θb(U)),
s∈B u∈B
var( PB ,B )=N XX η* (s) η* (u)(IT) S
s∈B u∈B
with the equality holding when the sample size N → ∞, where cov(θB (S), θB (U)) denotes the error
covariance between θB (S) and θB (U) and I-1 is the inverse of the Fisher information submatrix
I ∈ R lBl×lBl of PB with respect to the parameter set B given in Equation (4).
Proof. From the Pythagorean theorem illustrated in Figure 1,
Eh D kl( P*,Pb )2i = Eh(D KL (P *,PB) + D kl( PB P ))2]
=Eh D KL (P *,PB )2 + 2 D KL (P *,PB) D kl( PB ,Pb )+ D kl (PB ,Pb )2]
=Dkl(P*,PB)2 + 2Dkl(P*,PB)e[Dkl(PB,Pb)] + EhDkl(PB,Pb)2] ∙
5
Under review as a conference paper at ICLR 2018
PB^ 5′( B)	θ (X) = 0, ∀X ∉ B
∖ ^ _ 1, ^ _
∖	D KL( P *，PB) = D KL( P *，P*) + D KL(PB, PB)
P* Q
\'s(P*)	η (x) = η *(x), ∀x ∈ B
Figure 1: Illustration of the Pythagorean theorem.
Since Θb (S) is an unbiased estimator of θB (S) for every S ∈ S, it holds that
E 忙 pB(ʃ)log pBχ)]
ED KL (PB ,Pb )]
X PB (x) X e[θb (S) - Θb (S)] =0.
x∈S	s≤x
Hence we have
EhDkl(P*,Pb)2i = DKL(P*,PB)2 + EhDkl(PB,Pb)2].	(16)
The second term is
Eh D KL (PB ,Pb )2]
=E ](2B(X)log黑!2'
]"xsy^pB()pB ⑻ p pB (x) p pB (y) j
=E XX PB (x) PB (y) I X( θB (S) - Θb (S »-“ (θB) - ψ ( Θb )))
x∈S y∈S	s∈B ,s≤x
I X(θB (U) - Θb (u)) - (Ψ(θB) - ψ(Θb )))1
u∈B,u≤y
E
ΣΣΣΣPB(x)PB(y)ζ(S,x)ζ(u,y) θBs(S) - θb (S)) (θB(U) - θb
x∈S y∈S s∈B u∈B
-2E
X PB (x) X Θ*b (S) - Θb (S)) (ψ(θB) - Ψ(Θb ))
x∈S	s∈B,s≤x
+ E [(ψ(θB) - ψ(Θb))2
ΣΣηB (S)ηB (U)cov (Θb (S),Θb (U)) +var (ψ(Θb )) - 2 X ηB (S)cov (Θb (S),ψ(Θb ))
s∈B u∈B
ΣΣη* (S)η* (u)cov(Θb (S),Θb (u)) + var (ψ(Θb )
s∈B u∈B	s∈B
ʌ ʌ ʌ
where cov( θB (S), θB(U) ) denotes the error covariance between θB (S)
ʌ
ʌ
ʌ
and Θb (U) and var( ψ ( Θb ))
denotes the variance of ψ(Θb ), and the last equality comes from the condition in Equation (14).
Here the term of the (co)variance of the normalizing constant (partition function) ψ(θ):
var (ψ(Θb )) - 2 X η* (S)cov(Θb (S),ψ(Θb ))
s∈B
is the irreducible error since ψ(θ) = -θ(⊥) is orthogonal for every parameter θ(S), S ∈ S and the
Fisher information vanishes from Equation (4), i.e.,
∂ log P(S) ∂ log P(S)
∂θ( S )	∂θ ( ⊥ )
E
ζ(x, S)p(S) - η(x) = η(x) - η(x) = 0.
s∈S
6
Under review as a conference paper at ICLR 2018
For the variance term
var( PB ,B)= XX
η( S)η (U )cov(Θb( S),Θb( u)),
s∈B u∈B
We have from the Cramer-Rao bound (Amari, 2016, Theorem 7.1) since Θb (S), S ∈ B is unbiased
cov (Θb(S),Θb(U)) ≥ N(I-1)Su
With the equality holding When N → ∞, where I ∈ RlBl×lBl is the Fisher information matrix with
respect to the parameter set B such that
Isu = X Z(S,x)ζ(u,x)pB (X) - ηB (S)ηB(U)
x∈S
for all S, u ∈ B, which is given in Equation (5), and I-1 is its inverse. Finally, from Equation (16)
we obtain
Eh D KL (P *,Pb )2i
=D KL (P *,PB )2 + Eh D KL (PB ,Pb )2]
=DKL (P*,PB )2 + X X η* (S)η* (u)cov(Θb (S) ,Θb (u)) + irreducible error
s∈B u∈B
≥ DKL(P*,PB)2 + NN XX η*(S)η(U)(IT)su + irreducible error
s∈B u∈B
with the equality holding when N → ∞.	□
Let B, B0 ⊆ S0 = 2V∪H such that B ⊆ B0, that is, B0 has more parameters than B. Then it is clear
that the bias always reduces, that is,
DKL(P*,PB*) ≥ DKL(P*,PB*0)
because S(B) ⊆ S(B0). However, this monotonicity does not always hold for the variance. We
illustrate this non-monotonicity in the following example. Let S = 2V with V = {1, 2, 3} and
assume that the true distribution P* is given by
(p* ({0}),p* ({1}),p* ({2}),p* ({3}),p* ({1, 2}),p* ({1, 3}),P* ({2, 3}),P* ({1, 2, 3}))
= (0.2144, 0.0411, 0.2037, 0.145, 0.1423, 0.0337, 0.0535, 0.1663),
which was randomly generated from the uniform distribution. Suppose that we have three types of
parameter sets B1 = {{1}}, B2 = {{1}, {2}}, and B3 = {{1}, {2}, {1, 2}} such that B1 ⊂ B2 ⊂
B3 . Then biases become
DKL(P*,PB*1)2 = 0.0195, DKL(P*,PB*2)2 = 0.0172, DKL(P *, PB*3)2 = 0.0030,
where we can confirm that the bias monotonically decreases, while the lower bound of the variance
with N = 100 are
var( PB 1 ,B 1) = 0.0062, var( PB 2 ,B 2) = 0.0192,	var( PB 3 ,B 3) = 0.0178.
Moreover, we have computed the exact variance var(PB* , B) by repeating 1, 000 times generating a
sample with N = 100 and obtained the following:
var(PB*1,B1) = 0.0066, var(PB*2,B2) = 0.0267,	var(PB*3,B3) = 0.0190,
hence var(PB*2, B2) > var(PB* 3, B3) happens, that is, the variance actually decreases when we
include more parameters. This interesting property, non-monotonicity of the variance with respect
to the growth of parameter sets, comes from the hierarchical structure of S .
7
Under review as a conference paper at ICLR 2018
1.0 -10
a() ecnairaV
lθ
①uueue>
-Q-- Empirical
—△— Theoretical bound	∖↑1
10-4^---------1-ɪ-I--------------------H
101	102	103	104	105
Sample size N
0.3
0.2
(b) 0.4
0.1
0.0
Figure 2: Empirical evaluation of variance. Empirically estimated variances (blue, dotted lines) and
theoretically obtained lower bounds (red, solid line) with fixing n = 5 in (a) and N = 100 in (b).
4	Empirical Evaluation of Tightness
We empirically evaluate the tightness of our lower bound. In each experiment, we randomly
generated a true distribution P *, followed by repeating 1,000 times generating a sample (train-
ing data) with the size N from P * to empirically estimate the variance var( PB, B). We Consis-
tently used the Boltzmann machine represented as the fully connected graph G = (V, E) such that
V = {1, 2, . . . , n} and E = {{i, j} | i, j ∈ V }. Thus the parameter set B is given as
B = { x ∈ S+ =2v ∖{0} I ∣x∣ ≤ 2 }.
We report in Figure 2 the mean ± SD (standard deviation) of the empirically obtained variance
and its theoretical lower bound var(PB, B) obtained by repeating the above estimation 100 times.
In Figure 2(a) the sample size N is varied from 10 to 10, 000 with fixing the number of variables
n=5 while in Figure 2(b) n is varied from 3 to 7 with fixing N = 100. These results overall show
that our theoretical lower bound is tight enough if N is large and is reasonable across each n.
5	Conclusion
In this paper, we have firstly achieved bias-variance decomposition of the KL divergence for Boltz-
mann machines using the information geometric formulation of hierarchical probability distribu-
tions. Our model is a generalization of the traditional Boltzmann machines, which can incorpo-
rate arbitrary order interactions of variables. Our bias-variance decomposition reveals the non-
monotonicity of the variance with respect to growth of parameter sets, which has been also reported
elsewhere for non-linear models (Faber, 1999). This result indicates that it is possible to reduce both
bias and variance when we include more higher-order parameters in the hierarchical deep learning
architectures. To solve the open problem of the generalizability of the deep learning architectures,
our finding can be fundamental for further theoretical development.
References
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines. Cognitive
science, 9(1):147-169,1985.
S. Amari. Information geometry on hierarchy of probability distributions. IEEE Transactions on Information
Theory, 47(5):1701-1711, 2001.
S. Amari. Information Geometry and Its Applications. Springer, 2016.
S. Amari, K. Kurata, and H. Nagaoka. Information geometry of Boltzmann machines. IEEE Transactions on
Neural Networks, 3(2):260-271, 1992.
L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp minima can generalize for deep nets. In Proceedings of
the 34th International Conference on Machine Learning, pp. 1019-1028, 2017.
8
Under review as a conference paper at ICLR 2018
N. M. Faber. A closer look at the bias-variance trade-off in multivariate calibration. Journal of Chemometrics,
13(2):185-192,1999.
G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, and D. S. Scott. Continuous Lattices and
Domains. Cambridge University Press, 2003.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):
1771-1800, 2002.
K. Ito (ed.). Encyclopedic Dictionary of Mathematics. The MIT Press, 2 edition, 1993.
K. Kawaguchi, L. P. Kaelbling, and Y. Bengio. Generalization in deep learning. arXiv:1710.05468, 2017.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training for deep
learning: Generalization gap and sharp minima. In Proceedings of 5th International Conference on Learning
Representations, 2017.
N. Le Roux and Y. Bengio. Representational power of restricted Boltzmann machines and deep belief networks.
Neural computation, 20(6):1631-1649, 2008.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436-444, 2015.
M. R. Min, X. Ning, C. Cheng, and M. Gerstein. Interpretable sparse high-order Boltzmann machines. In
Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, pp. 614-622, 2014.
H. Nakahara and S. Amari. Information-geometric measure for neural spikes. Neural Computation, 14(10):
2269-2316, 2002.
H. Nakahara, S. Amari, and B. J. Richmond. A comparison of descriptive models of a single spike train by
information-geometric measure. Neural computation, 18(3):545-568, 2006.
B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro. Exploring generalization in deep learning. In
Advances in Neural Information Processing Systems 30, 2017.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In Proceedings of the 12th International
Conference on Artificial Intelligence and Statistics, pp. 448-455, 2009.
R. Salakhutdinov and G. E. Hinton. An efficient learning procedure for deep Boltzmann machines. Neural
Computation, 24(8):1967-2006, 2012.
T. J. Sejnowski. Higher-order Boltzmann machines. In AIP Conference Proceedings, volume 151, pp. 398-403,
1986.
P. Smolensky. Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumel-
hart, J. L. McClelland, and PDP Research Group (eds.), Parallel Distributed Processing: Explorations in the
Microstructure of Cognition, Vol. 1, pp. 194-281. MIT Press, 1986.
M. Sugiyama, H. Nakahara, and K. Tsuda. Information decomposition on structured space. In IEEE Interna-
tional Symposium on Information Theory, pp. 575-579, 2016.
M. Sugiyama, H. Nakahara, and K. Tsuda. Tensor balancing on statistical manifold. In Proceedings of the 34th
International Conference on Machine Learning, pp. 3270-3279, 2017.
S. Watanabe. Almost all learning machines are singular. In 2007 IEEE Symposium on Foundations of Compu-
tational Intelligence, pp. 383-388, 2007.
L. Wu, Z. Zhu, and W. E. Towards understanding generalization of deep learning: Perspective of loss land-
scapes. In ICML 2017 Workshop on Principled Approaches to Deep Learning, 2017.
K. Yamazaki and S. Watanabe. Singularities in complete bipartite graph-type Boltzmann machines and upper
bounds of stochastic complexities. IEEE Transactions on Neural Networks, 16(2):312-324, 2005.
9
Under review as a conference paper at ICLR 2018
Input Hidden Hidden
layer layer 1 layer 2
Figure 3: An example of a deep Boltzmann machine (left) with an input (visible) layer V = {1, 2}
with two hidden layers H1 = {3} and H2 = {4}, and the corresponding domain set SV∪H (right).
In the right-hand side, the colored objects {1}, {2}, {3}, {4}, {1, 3}, {2, 3}, and {3, 4} denote the
parameter set B , which correspond to nodes and edges of the DBM in the left-hand side.
A B oltzmann Machines with Hidden nodes
A Boltzmann machine with hidden variables is represented as G = (V ∪ H, E), where V and H
correspond to visible and hidden variables, respectively, and the resulting domain S = 2V ∪H . In
particular, restricted Boltzmann machines (RBMs) (Smolensky, 1986; Hinton, 2002) are often used
in applications, where the edge set is given as
E = {{i,j} | i ∈ V,j ∈ H}
Moreover, in a deep Boltzmann machine (DBM) (Salakhutdinov & Hinton, 2009; 2012), which is
the beginning of the recent trend of deep learning (LeCun et al., 2015; Goodfellow et al., 2016), the
hidden variables H are divided into k disjoint subsets (layers) H1, H2, . . . , Hk and
E = { {i, j} | i ∈ Hl-1,j ∈ Hl, l ∈ {1, . . . , k} } ,
where V = H0 for simplicity.
Let S = 2V and S0 = 2V∪H and S and S0 be the set of distributions with the domains S and S0,
respectively. In both cases of RBMs and DBMs, we have
B = { x ∈ S0 | |x| = 1 orx ∈ E } ,
(see Figure 3) and the set of Boltzmann distributions is obtained as
S 0 (B) = { P ∈ S0 I θ (N) = 0 for all x∈ B }.
Since the objective of learning Boltzmann machines with hidden variables is MLE (maximum like-
lihood estimation) with respect to the marginal probabilities of the visible part, the target empirical
distribution P ∈ S is extended to the submanifold S0 (P) such that
S0 (P) = { P ∈ S0 ∣ η(x) = η(x) forall X ∈ S },
and the process of learning Boltzmann machines with hidden variables is formulated as double
minimization of the KL divergence such that
min	DKL (P, PB).	(17)
P∈S0(P),Pb ∈S0(B)
Since two submanifolds S0 (B) and S0 (P) are e-flat and m-flat, respectively, it is known that the
EM-algorithm can obtain a local optimum of Equation (17) (Amari, 2016, Section 8.1.3), which
was first analyzed by Amari et al. (1992). Since this computation is infeasible due to combinatorial
explosion of the domain S0 = 2V ∪H, a number of approximation methods such as Gibbs sampling
have been proposed (Salakhutdinov & Hinton, 2012).
Let US fix an empirical distribution P and consider two RBMS with parameter sets B,B0 ⊆ S0 =
2V ∪H . If B ⊆ B0, that is, B0 has more hidden nodes than B, we always have the monotonicity:
min	DKL (P, PB0) ≤ min	DKL (P, PB )	(18)
P∈S0(P),Pbo ∈S0(B0)	P∈S0 (P),Pb ∈S0(B)
as B ⊆ B0 implies S0(B) ⊆ S0(B0). This result corresponds to Theorem 1 in (Le Roux & Bengio,
2008), the representation power of RBMs.
10