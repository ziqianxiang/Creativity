Under review as a conference paper at ICLR 2018
Softmax Q-Distribution Estimation for Struc-
tured Prediction: A Theoretical Interpreta-
tion for RAML
Anonymous authors
Paper under double-blind review
Ab stract
Reward augmented maximum likelihood (RAML), a simple and effective learning
framework to directly optimize towards the reward function in structured prediction
tasks, has led to a number of impressive empirical successes. RAML incorporates
task-specific reward by performing maximum-likelihood updates on candidate out-
puts sampled according to an exponentiated payoff distribution, which gives higher
probabilities to candidates that are close to the reference output. While RAML
is notable for its simplicity, efficiency, and its impressive empirical successes,
the theoretical properties of RAML, especially the behavior of the exponentiated
payoff distribution, has not been examined thoroughly. In this work, we introduce
softmax Q-distribution estimation, a novel theoretical interpretation of RAML,
which reveals the relation between RAML and Bayesian decision theory. The
softmax Q-distribution can be regarded as a smooth approximation of the Bayes
decision boundary, and the Bayes decision rule is achieved by decoding with this Q-
distribution. We further show that RAML is equivalent to approximately estimating
the softmax Q-distribution, with the temperature τ controlling approximation error.
We perform two experiments, one on synthetic data of multi-class classification
and one on real data of image captioning, to demonstrate the relationship between
RAML and the proposed softmax Q-distribution estimation method, verifying
our theoretical analysis. Additional experiments on three structured prediction
tasks with rewards defined on sequential (named entity recognition), tree-based
(dependency parsing) and irregular (machine translation) structures show notable
improvements over maximum likelihood baselines.
1	Introduction
Many problems in machine learning involve structured prediction, i.e., predicting a group of outputs
that depend on each other. Recent advances in sequence labeling (Ma & Hovy, 2016), syntactic
parsing (McDonald et al., 2005) and machine translation (Bahdanau et al., 2015) benefit from the
development of more sophisticated discriminative models for structured outputs, such as the seminal
work on conditional random fields (CRFs) (Lafferty et al., 2001) and large margin methods (Taskar
et al., 2004), demonstrating the importance of the joint predictions across multiple output components.
A principal problem in structured prediction is direct optimization towards the task-specific metrics
(i.e., rewards) used in evaluation, such as token-level accuracy for sequence labeling or BLEU score
for machine translation. In contrast to maximum likelihood (ML) estimation which uses likelihood to
serve as a reasonable surrogate for the task-specific metric, a number of techniques (Taskar et al.,
2004; Gimpel & Smith, 2010; Volkovs et al., 2011; Shen et al., 2016) have emerged to incorporate
task-specific rewards in optimization. Among these methods, reward augmented maximum likelihood
(RAML) (Norouzi et al., 2016) has stood out for its simplicity and effectiveness, leading to state-of-
the-art performance on several structured prediction tasks, such as machine translation (Wu et al.,
2016) and image captioning (Liu et al., 2016). Instead of only maximizing the log-likelihood of the
ground-truth output as in ML, RAML attempts to maximize the expected log-likelihood of all possible
candidate outputs w.r.t. the exponentiated payoff distribution, which is defined as the normalized
exponentiated reward. By incorporating task-specific reward into the payoff distribution, RAML
combines the computational efficiency of ML with the conceptual advantages of reinforcement
1
Under review as a conference paper at ICLR 2018
learning (RL) algorithms that optimize the expected reward (Ranzato et al., 2016; Bahdanau et al.,
2017). Simple as RAML appears to be, its empirical success has piqued interest in analyzing and
justifying RAML from both theoretical and empirical perspectives. In their pioneering work, Norouzi
et al. (2016) showed that both RAML and RL optimize the KL divergence between the exponentiated
payoff distribution and model distribution, but in opposite directions. Moreover, when applied
to log-linear model, RAML can also be shown to be equivalent to the softmax-margin training
method (Gimpel & Smith, 2010; Gimpel, 2012). Nachum et al. (2016) applied the payoff distribution
to improve the exploration properties of policy gradient for model-free reinforcement learning.
Despite these efforts, the theoretical properties of RAML, especially the interpretation and behavior
of the exponentiated payoff distribution, have largely remained under-studied (§2). First, RAML
attempts to match the model distribution with the heuristically designed exponentiated payoff distribu-
tion whose behavior has largely remained under-appreciated, resulting in a non-intuitive asymptotic
property. Second, there is no direct theoretical proof showing that RAML can deliver a prediction
function better than ML. Third, no attempt (to our best knowledge) has been made to further improve
RAML from the algorithmic and practical perspectives.
In this paper, we attempt to resolve the above-mentioned under-studied problems by providing an
theoretical interpretation of RAML. Our contributions are three-fold: (1) Theoretically, we introduce
the framework of softmax Q-distribution estimation, through which we are able to interpret the role
the payoff distribution plays in RAML (§3). Specifically, the softmax Q-distribution serves as a
smooth approximation to the Bayes decision boundary. By comparing the payoff distribution with
this softmax Q-distribution, we show that RAML approximately estimates the softmax Q-distribution,
therefore approximating the Bayes decision rule. Hence, our theoretical results provide an explanation
of what distribution RAML asymptotically models, and why the prediction function provided by
RAML outperforms the one provided by ML. (2) Algorithmically, we further propose softmax
Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes
decision boundary asymptotically. (3) Experimentally, through one experiment using synthetic data
on multi-class classification and one using real data on image captioning, we verify our theoretical
analysis, showing that SQDML is consistently as good or better than RAML on the task-specific
metrics we desire to optimize. Additionally, through three structured prediction tasks in natural
language processing (NLP) with rewards defined on sequential (named entity recognition), tree-based
(dependency parsing) and complex irregular structures (machine translation), we deepen the empirical
analysis of Norouzi et al. (2016), showing that RAML consistently leads to improved performance
over ML on task-specific metrics, while ML yields better exact match accuracy (§4).
2	Background
2.1	Notations
Throughout we use uppercase letters for random variables (and occasionally for matrices as well), and
lowercase letters for realizations of the corresponding random variables. Let X ∈ X be the input, and
Y ∈ Y be the desired structured output, e.g., in machine translation X and Y are French and English
sentences, resp. We assume that the set of all possible outputs Y is finite. For instance, in machine
translation all English sentences are UP to a maximum length. r(y, y*) denotes the task-specific
reward function (e.g., BLEU score) which evaluates a predicted output y against the ground-truth y*.
Let P denote the true distribution of the data, i.e., (X, Y)〜 P, and D = {(xi, yi)}n=ι be our
training samples, where {xi, i = 1, . . . , n} (resp. yi) are usually i.i.d. samples of X (resp. Y). Let
P = {Pθ : θ ∈ Θ} denote a parametric statistical model indexed by parameter θ ∈ Θ, where Θ is the
parameter space. Some widely used parametric models are conditional log-linear models (Lafferty
et al., 2001) and deep neural networks (Sutskever et al., 2014) (details in Appendix D.2). Once
the parametric statistical model is learned, given an input x, model inference (a.k.a. decoding) is
performed by finding an output y* achieving the highest conditional probability:
j = argmax P^(y∣x)	(1)
y∈Y
1	A ∙ . 1	. i'	.	1	1	.	1 . 7-Λ
where θ is the set of parameters learned on training data D.
2
Under review as a conference paper at ICLR 2018
2.2	Maximum Likelihood
Maximum likelihood minimizes the negative log-likelihood of the parameters given training data:
n
Θml = argminX TogPθ(yi∣xi) = argminEP(X)[KL(P(∙∣X)∣∣Pθ(∙∣X))]	(2)
where P(X) and P(∙∣X) is derived from the empirical distribution of training data D:
1n
P(X = χ, Y = y) = - EI(Xi = χ,y = y)	(3)
n i=1
and I(∙) is the indicator function. From (2), ML attempts to learn a conditional model distribution
P^ml(∙∣X = x) that is as close to the conditional empirical distribution P(∙∣X = x) as possible, for
each x ∈ X . Theoretically, under certain regularity conditions (Wasserman, 2013), asymptotically as
n → ∞, PjML(∙∣X = x) converges to the true distribution P(∙∣X = x), since P(∙∣X = x) converges
to P(∙∣X = x) for each X ∈ X.
2.3	Reward Augmented Maximum Likelihood
As proposed in Norouzi et al. (2016), RAML incorporates task-specific rewards by re-weighting the
log-likelihood of each possible candidate output proportionally to its exponentiated scaled reward:
n
Θraml = argmin V {-废 q(y∖yi, T)logPθ(y∣Xi)}
θ∈Θ
∈	i=1	y∈Y
(4)
where the reward information is encoded by the exponentiated payoff distribution with the temperature
τ controlling it smoothness
(। *; ) =	exp(r(y,y*)∕τ)	= exp(r(y,y*)∕τ)
qyy " P exp(r(y0,y*)∕τ)	Z (y*; τ)
y0∈Y
(5)
Norouzi et al. (2016) showed that (4) can be re-expressed in terms ofKL divergence as follows:
Θraml = argmin EP(X,γ )[KL(q(∙∣Y ； τ )∣∣Pθ (∙∣X))]	(6)
where P is the empirical distribution in (3).
Discussion As discussed in Norouzi et al. (2016), the globally optimal solution of RAML is
achieved when the learned model distribution matches the exponentiated payoff distribution, i.e.,
P^raml(∙∣X = x) = q(∙∣Y = y； T) for each (x, y) ∈ D with some fixed value of T. It makes the
payoff distribution appear to be the “target” that the model is attempting to learn. Though both
P^raml(∙∣X = x) and q(∙∣Y = y;T) are distributions defined over the output space Y, the latter
is conditioned on the output Y which appears to serve as ground-truth but is sampled from data
distribution P. This makes the behavior of RAML attempting to match them unintuitive; Specifically,
supposing that in the training data there exist two training instances with the same input but different
outputs, i.e., (x,y), (x,y0) ∈ D. Then PjrAML(∙∣X = x) has two “targets” q(∙∣Y = y;T) and
q(∙∣Y = y0; T), making it unclear what distribution P^AML (∙∣X = x) asymptotically converges to.
Open Problems in RAML Based on the discussion above, we identify two open issues in the
theoretical interpretation of RAML: i) What is the (asymptotically) globally optimal solution of
RAML, i.e. what distribution P^AMl(∙∣X = x) (asymptotically) converges to; ii) There is no rigorous
theoretical evidence showing that generating from P^rAML(y|x) yields a better prediction function
than generating from P^ML(y|x).
To our best knowledge, no attempt has been made to theoretically address these problems. The
main goal of this work is to theoretically analyze the properties of RAML, in hope that we may
eventually better understand it by answering these questions and further improve it by proposing new
training framework. To this end, in the next section we introduce a softmax Q-distribution estimation
framework, facilitating our later analysis.
3
Under review as a conference paper at ICLR 2018
3	S oftmax Q-Distribution Estimation
With the end goal of theoretically interpreting RAML in mind, in this section we present the softmax
Q-distribution estimation framework. We first provide background on Bayesian decision theory
(§3.1) and softmax approximation of deterministic distributions (§3.2). Then, we propose the softmax
Q-distribution (§3.3), and establish the framework of estimating the softmax Q-distribution from
training data, called softmax Q-distribution maximum likelihood (SQDML, §3.4). In §3.5, we analyze
SQDML, which is central in linking RAML and softmax Q-distribution estimation.
3.1	Bayesian Decision Theory
Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification,
which quantifies the trade-offs between various classification decisions using the probabilities and
rewards (losses) that accompany such decisions.
Based on the notations setup in §2.1, let H denote all the possible prediction functions from input to
output space, i.e., H = {h : X → Y}. Then, the expected reward of a prediction function h is:
R(h) =EP(X,Y)[r(h(X),Y)]	(7)
where r(∙, ∙) is the reward function accompanied with the structured prediction task.
Bayesian decision theory states that the global maximum of R(h), i.e., the optimal expected prediction
reward is achieved when the prediction function is the so-called Bayes decision rule:
h*(x) = argmax EP (γ ∣χ=χ)[r(y,Y)] = argmax R(y∣x)	(8)
y∈Y	y∈Y
where R(y|x) = EP(Y|X=x) [r(y, Y)] is called the conditional reward. Thus, the Bayes decision rule
states that to maximize the overall reward, compute the conditional reward for each output y ∈ Y and
then select the output y for which R(y|x) is maximized.
Importantly, when the reward function is the indicator function, i.e., I(y = y0), the Bayes decision
rule reduces to a specific instantiation called the Bayes classifier:
hc(x) = argmax P (y|X = x)	(9)
y∈Y
where P (Y|X = x) is the true conditional distribution of data defined in §2.1.
In §2.2, we see that ML attempts to learn the true distribution P. Thus, in the optimal case, decoding
from the distribution learned with ML, i.e., P©ML (Y|X = x), produces the Bayes classifier hc(x), but
not the more general Bayes decision rule h (x). In the rest of this section, we derive a theoretical
proof showing that decoding from the distribution learned with RAML, i.e., P^AML(Y|X = x)
approximately achieves h*(x), illustrating why RAML yields a prediction function with improved
performance towards the optimized reward function r(∙, ∙) over ML.
3.2	Softmax Approximation of Deterministic Distributions
Aimed at providing a smooth approximation of the Bayes decision boundary determined by the Bayes
decision rule in (8), we first describe a widely used approximation of deterministic distributions using
the softmax function.
Let F = {fk : k ∈ K} denote a class of functions, where fk : X → R, ∀k ∈ K. We assume that K
is finite. Then, we define the random variable Z = argmaxk∈K fk(X) where X ∈ X is our input
random variable. Obviously, Z is deterministic when X is given, i.e.,
(1, if z = argmax fk (x)
k∈K	(10)
0, otherwise.
for each z ∈ K and x ∈ X .
The softmax function provides a smooth approximation of the point distribution in (10), with a
temperature parameter, τ > 0, serving as a hyper-parameter that controls the smoothness of the
4
Under review as a conference paper at ICLR 2018
approximating distribution around the target one:
Q(Z = z|X = x.τ) =	expfz(x)∕τ)
Q( = | = ; )= P exp(fk(x)∕τ)	( )
k∈K
It should be noted that at τ → 0, the distribution Q reduces to the original deterministic distribution
P in (10), and in the limit as τ → ∞, Q is equivalent to the uniform distribution Unif(K).
3.3	Softmax Q-distribution
We are now ready to propose the softmax Q-distribution, which is central in revealing the relation-
ship between RAML and Bayes decision rule. We first define random variable Z = h"X) =
argmaxy∈Y EP(Y |X) [r(y, Y )]. Then, Z is deterministic given X, and according to (11), we define
the softmax Q-distribution to approximate the conditional distribution of Z given X :
Q(Z = zX = τ.τ)= exp (EP(Y∣X=χ)[r(Z,γ)]/T)	(12)
Q(Z = ZX = x;τ )= P exp (Ep(γ∣χ=χ) [r(y,γ )]∕τ)	(12)
y∈Y
for each x ∈ X and Z ∈ Y.1 Importantly, one can verify that decoding from the softmax Q-distribution
provides us with the Bayes decision rule,
h(x) = argmax Q(y∣x; T) = argmax EP (γ ∣χ=χ)[r(y,Y)] = h*(x)	(13)
y∈Y	y∈Y
with any value of τ > 0.
3.4	Softmax Q-distribution Maximum Likelihood
Because making predictions according to the softmax Q-distribution is equivalent to the Bayes
decision rule, we would like to construct a (parametric) statistical model P to directly model the
softmax Q-distribution in (12), similarly to how ML models the true data distribution P . We call
this framework softmax Q-distribution maximum likelihood (SQDML). This framework is model-
agnostic, so any probabilistic model used in ML such as conditional log-linear models and deep
neural networks, can be directly applied to modeling the softmax Q-distribution.
Suppose that we use a parametric statistical model P = {Pθ : θ ∈ Θ} to model the softmax
Q-distribution. In order to learn “optimal” parameters θ from training data D = {(xi, yi)}in=1,
an intuitive and well-motivated objective function is the KL-divergence between the empirical
conditional distribution of Q(∙∣X), denoted as Q(∙∣X), and the model distribution Pθ(∙∣X):
0	_	-	, ~ , ___________
Θsqdml = argmin EQ(X) [KL(Q(∙∣X )∣∣Pθ (∙∣X))]
θ∈Θ	Q( )
(14)
5 τ	∙>♦	. 1	. 入 <~τ~∖ /	∖	< ∙	< ι	.ι	λ ι	t~ ι t` ∙ .ι	∙ ∙ i	∙> ♦ , ♦ ι
We can directly set Q(X) = P (X), which leaves the problem of defining the empirical conditional
1 ∙	.	∙1	. ∙	/ Γ7∖ -tr ∖ ɪʌ i-	1 Γ~	/ Γ7∖ ^tr∖	C .	.	,1	, ∙ i~ .1	1 Γ~ 1	♦♦	1	1 ∙	.	∙1	. ∙
distribution Q(Z|X). Before defining Q(Z|X), we first note that if the defined empirical distribution
Q(X, Z) asymptotically converges to the true Q-distribution Q(X, Z), the learned model distribution
P^sqdml(∙∣X = x) converges to Q(∙∣X = x). Therefore, decoding from P^qdml(∙∣X = x) ideally
achieves the Bayes decision rule h*(χ).
A straightforward way to define Q(Z|X = x) is to use the empirical distribution P (Y|X = x):
e	exp (EP(Y∣x=x)[r(z,γ)]/T
Q(Z = z|X = x) =--------7--------------------:
Pexp (EP(YIX=χ)[r(y,Y)]/T
(15)
1	t ∖ - ,1	∙ ∙ 1 1 ∙ , ∙1	C 7-» 1 C 1 ∙ ∕C∖ A	. . ∙ 11	i∖	,
where P is the empirical distribution of P defined in (3). Asymptotically as n → ∞, P converges to
P . Thus, Q asymptotically converges to Q.
1In the following derivations we omit τ in Q(Z|X; τ) for simplicity when there is no ambiguity.
5
Under review as a conference paper at ICLR 2018
Unfortunately, the empirical distribution Q (15) is not efficient to compute, since the expectation
term is inside the exponential function (See appendix D.2 for approximately learning θSQDML in
practice). This leads us to seek an approximation of the softmax Q-distribution and its corresponding
empirical distribution. Here we propose the following Q0 distribution to approximate the softmax
Q-distribution Q defined in (12):
QO(Z = ZIX = x; T) = EP(Y|X=x)
exp (r(z, Y)∕τ)
P exp (r(y,Y)∕τ)
y∈Y
(16)
where we move the expectation term outside the exponential function. Then, the corresponding
empirical distribution of Q0(X, Z) can be written in the following form:
~ ,
Q0(X
x,Z=Z) = n Xj X
i=1 I y∈Y
eχp(r(z,y)∕τ) I(χ
P exp(r(y0,y)∕τ) l i
y0∈Y
x, yi = y)
(17)
Approximating Q(X, Z) with Q0(X, Z), and plugging (17) into the RHS in (14), we have:
A
θSQDML
一 - ,~ , ________. . _ , ___
≈ argmin EQo(χ)[KL(Q0(∙∣X )I∣Pθ (∙∣X))]
θ∈Θ	Q ( )
= argmin P { - P q(y∣yi； τ)logPθ(y∣xi)}
θ∈Θ i=1 y∈Y
£
θRAML
(18)
where q(y∣y*; T) is the exponentiated payoff distribution of RAML in (5).
Equation (18) states that RAML is an approximation of our proposed SQDML by approximating
Q with Q0. Interestingly and mostly in practice, when the input is unique in the training data, i.e.,
6∃(x1,y1), (x2, y2) ∈ D, s.t. x1 = x2 ∧ y1 6= y2, we have Q = Q0, resulting in θSQDML = θRAML.
It states that the estimated distribution P^sqdML and P^raml are exactly the same when the input X is
unique in the training data, since the empirical distributions Q and Q0 estimated from the training
data are the same.
3.5 Analysis and Discussion of S QDML
In §3.4, we provided a theoretical interpretation of RAML by establishing the relationship between
RAML and SQDML. In this section, we try to answer the questions of RAML raised in §2.3 using
this interpretation and further analyze the level of approximation from the softmax Q-distribution Q
in (13) to Q0 in (16) by proving a upper bound of the approximation error.
Let’s first use our interpretation to answer the open questions regarding RAML in §2.3. First, instead
of optimizing the KL divergence between the artificially designed exponentiated payoff distribution
and the model distribution, RAML in our formulation approximately matches model distribution
Pθ(∙∣X = x) with the softmax Q-distribution Q(∙∣X = x; T). Second, based on our interpretation,
asymptotically as n → ∞, RAML learns a distribution that converges to Q0(∙) in (16), and therefore
approximately converges to the softmax Q-distribution. Third, as mentioned in §3.3, generating from
the softmax Q-distribution produces the Bayes decision rule, which theoretically outperforms the
prediction function from ML, w.r.t. the expected reward.
It is necessary to mention that both RAML and SQDML are trying to learn distributions, decoding
from which (approximately) delivers the Bayes decision rule. There are other directions that can
also achieve the Bayes decision rule, such as minimum Bayes risk decoding (Kumar & Byrne, 2004),
which attempts to estimate the Bayes decision rule directly by computing expectation w.r.t the data
distribution learned from training data.
So far our discussion has concentrated on the theoretical interpretation and analysis of RAML,
without any concerns for how well Q0(X, Z) approximates Q(X, Z). Now, we characterize the
approximating error by proving a upper bound of the KL divergence between them:
Theorem 1. Given the input and output random variable X ∈ X andY ∈ Y and the data distribution
P (X, Y). Suppose that the reward function is bounded 0 ≤ r(y,y*) ≤ R. Let Q(Z∣X; τ) and
Q0(Z|X; T) be the softmax Q-distribution and its approximation defined in (12) and (16). Assume
that Q(X) = Q0(X) = P(X). Then,
KL(Q(∙, ∙)kQ0(∙, ∙)) ≤ 2R∕τ	(19)
6
Under review as a conference paper at ICLR 2018
From Theorem 1 (proof in Appendix A.1) we observe that the level of approximation mainly depends
on two factors: the upper bound of the reward function (R) and the temperature parameter τ . In
practice, R is often less than or equal to 1, when metrics like accuracy or BLEU are applied.
It should be noted that, at one extreme when τ becomes larger, the approximation error tends to
be zero. At the same time, however, the softmax Q-distribution becomes closer to the uniform
distribution Unif(Y), providing less information for prediction. Thus, in practice, it is necessary to
consider the trade-off between approximation error and predictive power.
What about the other extreme — τ “as close to zero as possible”? With suitable assumptions about the
data distribution P, we can characterize the approximating error by using the same KL divergence:
Theorem 2. Suppose that the reward function is bounded 0 ≤ r(y,y*) ≤ R, and ∀y0 = y,
r(y, y) - r(y0, y) ≥ γR where γ ∈ (0, 1) is a constant. Suppose additionally that, like a sub-
Gaussian, for every x ∈ X, P(Y |X = x) satisfies the exponential tail bound w.r.t. r — that is, for
each X ∈ X, there exists a unique y* ∈ Y such thatfor every t ∈ [0,1)
_c t2
P(r(y*,y*) - r(Y,y*) ≥ tR|X = x) ≤ e (1-t)2	(20)
where c is a distribution-dependent constant. Assume that Q(X) = Q0(X) = P(X). Denote
2
b = (i-γ)2. Then, as T → 0,
KL(Q(∙, ∙)kQ0(∙, ∙)) ≤ T^b.	(21)
1 + ecb
Theorem 2 (proof in Appendix A.2) indicates that RAML can also achieve little approximating error
when τ is close to zero.
4 Experiments
In this section, we performed two sets of experiments to verity our theoretical analysis of the relation
between SQDML and RAML. As discussed in §3.4, RAML and SQDML deliver the same predictions
when the input x is unique in the data. Thus, in order to compare SQDML against RAML, the first
set of experiments are designed on two data sets in which x is not unique — synthetic data for
cost-sensitive multi-class classification, and the MSCOCO benchmark dataset (Chen et al., 2015) for
image captioning. To further confirm the advantages of RAML (and SQDML) over ML, and thus the
necessity for better theoretical understanding, we performed the second set of experiments on three
structured prediction tasks in NLP. In these cases SQDML reduces to RAML, as the input is unique
in these three data sets.
4.1	Experiments on SQDML
4.1.1	Cost-sensitive Multi-class Classification
First, we perform experiments on synthetic data for cost-sensitive multi-class classification designed
to demonstrate that RAML learns a distribution approximately producing the Bayes decision rule,
which is asymptotically the prediction function delivered by SQDML.
The synthetic data set is for a 4-class classification task, where x ∈ X = [-1, +1] × [-1, +1] ⊂ R2,
and y ∈ Y = {0, 1, 2, 3}. We define four base points, one for each class:
For data generation, the distribution P(X) is the uniform distribution on X, and the log form of the
conditional distribution P(Y|X = x) for each x ∈ X is proportional to the negative distance of each
base point:
logP(Y = y|X = x) H -d(x, Xy), for y ∈ {0,1, 2, 3}	(22)
where d(∙, ∙) is the Euclidean distance between two points. To generate training data, We first draw 1
million inputs x from P(X). Then, we independently generate 10 outputs y from P(Y|X = x) for
7
Under review as a conference paper at ICLR 2018
(a) Validation
(b) Test
Figure 1: Average reward relative to the temperature parameter τ, ranging from 0.1 to 3.0, on
validation and test sets, respectively.
Figure 2: Average reward relative to a wide range of τ (from 1.0 to 10,000) on validation and test
sets, respectively.
each x to build a data set with multiple references. Thus, the total number of training instances is
10 million. For validation and test data, we independently generate 0.1 million pairs of (x, y) from
P(X, Y ), respectively.
The model we used is a feed-forward (dense) neural networks with 2 hidden layers, each of which
has 8 units. Optimization is performed with mini-batch stochastic gradient descent (SGD) with
learning rate 0.1 and momentum 0.9. Each model is trained with 100 epochs and we apply early
stopping (Caruana et al., 2001) based on performance on validation sets.
The reward function r(∙, ∙) is designed to distinguish the four classes. For “correct” predictions, the
specific reward values assigned for the four classes are:
-r(0, 0)-		e2.0
r(1,1)		e1.6
r(2, 2)		ei.2
r(3, 3)		e1.1
For “wrong” predictions, rewards are always zero, i.e. r(y, y*) = 0 when y = y*.
Figure 1 depicts the effect of varying the temperature parameter τ on model performance, ranging
from 0.1 to 3.0 with step 0.1. For each fixed τ, we report the mean performance over 5 repetitions.
Figure 1 shows the averaged rewards obtained as a function of τ on both validation and test datasets
8
Under review as a conference paper at ICLR 2018
T		RAML		SQDML		τ		RAML		SQDML	
		Reward	:BLEU	Reward	:BLEU			Reward	:BLEU	Reward	:BLEU
T	= 0.80	10.77	:27.02	10.82	:27.08	τ	= 1.00	10.84	:27.26	10.82	:27.03
T	= 0.85	10.81	I 27.27	10.78	I 26.92	T	= 1.05	10.82	I 27.29	10.80	I 27.20
T	= 0.90	10.88	I 27.62	10.91	I 27.54	T	= 1.10	10.74	I 26.89	10.78	I 26.98
T	= 0.95	10.82	1 27.33	10.79	1 27.02	T	= 1.15	10.77	1 27.01	10.72	1 26.66
Table 1: Average Reward (sentence-level BLEU) and corpus-level BLEU (standard evaluation
metric) scores for image captioning task with different τ .
of ML, RAML and SQDML, respectively. From Figure 1 we can see that when τ increases, the
performance gap between SQDML and RAML keeps decreasing, indicting that RAML achieves better
approximation to SQDML. This evidence verities the statement in Theorem 1 that the approximating
error between RAML and SQDML decreases when τ continues to grow.
The results in Figure 1 raise a question: does larger τ necessarily yield better performance for
RAML? To further illustrate the effect of τ on model performance of RAML and SQDML, we
perform experiments with a wide range of τ — from 1 to 10,000 with step 200. We also repeat
each experiment 5 times. The results are shown in Figure 2. We see that the model performance
(average reward), however, has not kept growing with increasing τ . As discussed in §3.5, the softmax
Q-distribution becomes closer to the uniform distribution when τ becomes larger, making it less
expressive for prediction. Thus, when applying RAML in practice, considerations regarding the
trade-off between approximating error and predictive power of model are needed. More details,
results and analysis of the conducted experiments are provided in Appendix B.
4.1.2 Image Captioning with Multiple References
Second, to show that optimizing toward our proposed SQDML objective yields better predictions than
RAML on real-world structured prediction tasks, we evaluate on the MSCOCO image captioning
dataset. This dataset contains 123,000 images, each of which is paired with as least five manually
annotated captions. We follow the offline evaluation setting in (Karpathy & Li, 2015), and reserve
5,000 images for validation and testing, respectively. We implemented a simple neural image
captioning model using a pre-trained VGGNet as the encoder and a Long Short-Term Memory
(LSTM) network as the decoder. Details of the experimental setup are in Appendix C.
As in §4.1.1, for the sake of comparing SQDML with RAML to verify our theoretical analysis,
we use the average reward as the performance measure by simply defining the reward as pairwise
sentence level BLEU score between model’s prediction and each reference caption2, though the
standard benchmark metric commonly used in image captioning (e.g., corpus-level BLEU-4 score) is
not simply defined as averaging over the pairwise rewards between prediction and reference captions.
We use stochastic gradient descent to optimize the objectives for SQDML (14) and RAML (4).
However, the denominators of the SoftmaX-Q distribution for SQDML Q(Z |X; T) (15) and the payoff
distribution for RAML q(y∣y*; T) (5) contain summations over intractable exponential hypotheses
space Y . We therefore propose a simple heuristic approach to approXimate the denominator by
restricting the exponential space Y using a fixed set S of sampled targets, i.e., Y ≈ S. Approximating
the intractable hypotheses space using sampling is not new in structured prediction, and has been
shown effective in optimizing neural structured prediction models (Shen et al., 2016). Specifically,
the sampled candidate set S is constructed by (i) including each ground-truth reference y* into S;
and (ii) uniformly replacing an n-gram (n ∈ {l, 2, 3}) in one (randomly sampled) reference y* with
a randomly sampled n-gram. We refer to this approach as n-gram replacement. We provide more
details of the training procedure in Appendix C.
Table 1 lists the results. We evaluate on both the average reward and the benchmark metric (corpus-
level BLEU-4). We also tested on a vanilla ML baseline, which achieves 10.71 average reward and
26.91 corpus-level BLEU. Both SQDML and RAML outperform ML according to the two metrics.
Interestingly, comparing SQDML with RAML we did not observe a significant improvement of
2Not that this is different from standard multi-reference sentence-level BLEU, which counts n-gram matches
w.r.t. all sentences then uses these sufficient statistics to calculate a final score.
9
Under review as a conference paper at ICLR 2018
Ref 1	A group of people standing around a wine cellar
Ref 2	A couple of people are standing around a table with wine
Ref 3	Men and women are gathered around the table
Ref 4 People Standing at a table with a lot of wine glasses and
different flavors of wine
Ref 5	A bunch of people at a table filled with wine glasses
SQDML A group of people Standing around a table with wine
glasses
Avg. Reward: 0.3040 Max Reward: 0.5900
RAML A group of people Standing around a table
Avg. Reward: 0.2557 Max Reward: 0.7421
ML	A group of people Standing around a table
Avg. Reward: 0.2557 Max Reward: 0.7421
Refl	A one way sign that is on a pole
Ref2	A black and white picture of a traffic signal in a city
Ref3	A black and white image of some buildings and a street light
Ref4	Intersection with traffic signals in large metropolitan area
Ref5	Traffic lights in front of large buildings with a one way sign
SQDML	A black and white photo of a street sign on a pole Avg. Reward: 0.1424 Max Reward: 0.2620
RAML	A black and white photo of a traffic light Avg. Reward: 0.1409 Max Reward: 0.3093
ML	A black and white photo of a street sign Avg. Reward: 0.1253 Max Reward: 0.2643
Figure 3: Testing examples from MSCOCO image captioning task
average reward. We hypothesize that this is due to the fact that the reference captions for each image
are largely different, making it highly non-trivial for the model to predict a “consensus” caption
that agrees with multiple references. As an example, we randomly sampled 300 images from the
validation set and compute the averaged sentence-level BLEU between two references, which is
only 10.09. Nevertheless, through case studies we still found some interesting examples, which
demonstrate that SQDML is capable of generating predictions that match with multiple candidates.
Figure 3 gives two examples. In the two examples, SQDML’s predictions match with multiple
references, registering the highest average reward. On the other hand, RAML gives sub-optimal
predictions in terms of average reward since it is an approximation of SQDML. And finally for ML,
since its objective is solely maximizing the reward w.r.t a single reference, it gives the lowest average
reward, while achieving higher maximum reward.
4.2 Experiments on Structured Prediction
Norouzi et al. (2016) already evaluated the effectiveness of RAML on sequence prediction tasks of
speech recognition and machine translation using neural sequence-to-sequence models. In this section,
we further confirm the empirical success of RAML (and SQDML) over ML: (i) We apply RAML
on three structured prediction tasks in NLP, including named entity recognition (NER), dependency
parsing and machine translation (MT), using both classical feature-based log-linear models (NER and
parsing) and state-of-the-art attentional recurrent neural networks (MT). (ii) Different from Norouzi
et al. (2016) where edit distance is uniformly used as a surrogate training reward and the learning
objective in (4) is approximated through sampling, we use task-specific rewards, defined on sequential
(NER), tree-based (parsing) and complex irregular structures (MT). Specifically, instead of sampling,
we apply efficient dynamic programming algorithms (NER and parsing) to directly compute the
analytical solution of (4). (iii) We present further analysis comparing RAML with ML, showing that
due to different learning objectives, RAML registers better results under task-specific metrics, while
ML yields better exact-match accuracy.
4.2. 1 Setup
In this section we describe experimental setups for three evaluation tasks. We refer readers to
Appendix D for dataset statistics, modeling details and training procedure.
10
Under review as a conference paper at ICLR 2018
Method		Dev.	Results	Test	Results
		Acc	F1	Acc	F1
ML Baseline		98.2	90.4	97.0	84.9
τ	0.1	98.3	90.5	97.0	85.0
τ	0.2	98.4	91.2	97.3	86.0
τ	0.3	98.3	90.2	97.1	84.7
τ	0.4	98.3	89.6	97.1	84.0
τ	0.5	98.3	89.4	97.1	83.3
τ	0.6	98.3	88.9	97.0	82.8
τ	0.7	98.3	88.6	97.0	82.2
τ	0.8	98.2	88.5	96.9	81.9
τ	0.9	98.2	88.5	97.0	82.1
Table 2: Token accuracy and official F1 for NER.
Method	Dev. Results	Test Results
	UAS 一	UAS 一
ML Baseline	91.3 二	90.7
τ =0.1	910	90.6
τ =0.2	91.5	91.0
τ =0.3	91.7	91.1
τ =0.4	91.4	90.8
τ = 0.5	91.2	90.7
τ = 0.6	91.0	90.6
τ =0.7	90.8	90.4
τ =0.8	90.8	90.3
τ =0.9	90.7	90.1
Table 3: UAS scores for dependency parsing.
Named Entity Recognition (NER) For NER, we experimented on the English data from CoNLL
2003 shared task (Tjong Kim et al., 2003). There are four predefined types of named entities:
PERSON, LOCATION, ORGANIZATION, and MISC. The dataset includes 15K training sentences,
3.4K for validation, and 3.7K for testing.
We built a linear CRF model (Lafferty et al., 2001) with the same features used in Finkel et al. (2005).
Instead of using the official F1 score over complete span predictions, we use token-level accuracy as
the training reward, as this metric can be factorized to each word, and hence there exists efficient
dynamic programming algorithm to compute the expected log-likelihood objective in (4).
Dependency Parsing For dependency parsing, we evaluate on the English Penn Treebanks
(PTB) (Marcus et al., 1993). We follow the standard splits of PTB, using sections 2-21 for training,
section 22 for validation and 23 for testing. We adopt the Stanford Basic Dependencies (De Marneffe
et al., 2006) using the Stanford parser v3.3.03. We applied the same data preprocessing procedure as
in Dyer et al. (2015).
We adopt an edge-factorized tree-structure log-linear model with the same features used in Ma &
Zhao (2012). We use the unlabeled attachment score (UAS) as the training reward, which is also
the official evaluation metric of parsing performance. Similar as NER, the expectation in (4) can be
computed deficiently using dynamic programming since UAS can be factorized to each edge.
Machine Translation (MT) We tested on the German-English machine translation task in the
IWSLT 2014 evaluation campaign (Cettolo et al., 2014), a widely-used benchmark for evaluating
optimization techniques for neural sequence-to-sequence models. The dataset contains 153K training
sentence pairs. We follow previous works (Wiseman & Rush, 2016; Bahdanau et al., 2017; Li et al.,
2017) and use an attentional neural encoder-decoder model with LSTM networks. The size of the
LSTM hidden states is 256. Similar as in §4.1.2, we use the sentence level BLEU score as the training
reward and approximate the learning objective using n-gram replacement (n ∈ {1, 2, 3, 4}). We
evaluate using standard corpus-level BLEU.
4.2.2	Main Results
The results of NER and dependency parsing are shown in Table 2 and Table 3, respectively. We
observed that the RAML model obtained the best results at τ = 0.2 for NER, and τ = 0.3 for
dependency parsing. Beyond τ = 0.4, RAML models get worse than the ML baseline for both the
two tasks, showing that in practice selection of temperature τ is needed. In addition, the rewards we
directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are
more stable w.r.t. τ than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a
training reward that correlates well with the evaluation metric is important.
Table 4 summarizes the results for MT. We also compare our model with previous works on incorpo-
rating task-specific rewards (i.e., BLEU score) in optimizing neural sequence-to-sequence models
(c.f. Table 5). Our approach, albeit simple, surprisingly outperforms previous works. Specifically,
3http://nlp.stanford.edu/software/lex-parser.shtml
11
Under review as a conference paper at ICLR 2018
τ		S-B	:C-B	T		S-B	:C-B
τ	= 0.1	28.67	；27.42	T	= 0.6	29.37	；28.49
τ	= 0.2	29.44	I 28.38	τ	= 0.7	29.52	I 28.59
τ	= 0.3	29.59	I 28.40	T	= 0.8	29.54	I 28.63
τ	= 0.4	29.80	I 28.77	T	= 0.9	29.48	I 28.58
τ	= 0.5	29.55	1 28.45	T	= 1.0	29.34	1 28.40
Table 4: Sentence-level BLEU (S-B, training reward) and corpus-level BLEU (C-B, standard evalua-
tion metric) scores for RAML with different τ .
			
			
Methods	ML Baseline	Proposed Model
Ranzato et al. (2016)	20.10 二	21.81
Wiseman & Rush (2016)	24.03	26.36
Li et al. (2017)	27.90	28.30
Bahdanau et al. (2017)	27.56	28.53
This Work	2766	28.77
Table 5: Comparison of our proposed approach with previous works. All previous methods require
pre-training using an ML baseline, while RAML learns from scratch.
		NER				Parsing			MT			
Metric	Acc.	F1	E.M.	UAS	E.M.	S-B	C-B	E.M.
ML	97.0	84.9	78.8	90.7	39.9	29.15	27.66	3.79
RAML	97.3	86.0	80.1	91.1	39.4	29.80	28.77	3.35
Table 6: Performance of ML and RAML under different metrics for the three tasks on test sets. E.M.
refers to exact match accuracy.
all previous methods require a pre-trained ML baseline to initialize the model, while RAML learns
from scratch. This suggests that RAML is easier and more stable to optimize compared with existing
approaches like RL (e.g., Ranzato et al. (2016) and Bahdanau et al. (2017)), which requires sampling
from the moving model distribution and suffers from high variance. Finally, we remark that RAML
performs consistently better than the ML (27.66) across most temperature terms.
4.2.3	Further Comparison with Maximum Likelihood
Table 6 illustrates the performance of ML and RAML under different metrics of the three tasks. We
observe that RAML outperforms ML on both the directly optimized rewards (token-level accuracy for
NER, UAS for dependency parsing and sentence-level BLEU for MT) and task-specific evaluation
metrics (F1 for NER and corpus-level BLEU for MT). Interestingly, we find a trend that ML gets
better results on two out of the three tasks under exact match accuracy, which is the reward that ML
attempts to optimize (as discussed in (9)). This is in line with our theoretical analysis, in that RAML
and ML achieve better prediction functions w.r.t. their corresponding rewards they try to optimize.
5 Conclusion
In this work, we propose the framework of estimating the softmax Q-distribution from training
data. Based on our theoretical analysis, asymptotically, the prediction function learned by RAML
approximately achieves the Bayes decision rule. Experiments on three structured prediction tasks
demonstrate that RAML consistently outperforms ML baselines.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In Proceedings of ICLR, San Diego, California, 2015.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In Proceedings
of ICLR, Toulon, France, 2017.
12
Under review as a conference paper at ICLR 2018
Rich Caruana, Steve Lawrence, and Giles Lee. Overfitting in neural nets: Backpropagation, conjugate
gradient, and early stopping. In Proceedings of NIPS, volume 13, pp. 402. MIT Press, 2001.
Mauro Cettolo, Jan Niehues, Sebastian Stuker, Luisa Bentivogli, and Marcello Federico. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings for the International Workshop on
Spoken Language Translation, pp. 2-11, 2014.
Xinlei Chen, Hao Fang, TsUng-Yi Lin, Ramakrishna Vedantam, SaUrabh Gupta, Piotr Dolldr, and
C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR,
abs/1504.00325, 2015.
Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. Generating typed
dependency parses from phrase strUctUre parses. In Proceedings of LREC, pp. 449-454, 2006.
Chris Dyer, MigUel Ballesteros, Wang Ling, AUstin Matthews, and Noah A. Smith. Transition-based
dependency parsing with stack long short-term memory. In Proceedings of ACL, pp. 334-343,
Beijing, China, JUly 2015.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information
into information extraction systems by gibbs sampling. In Proceedings of ACL, pp. 363-370, Ann
Arbor, Michigan, JUne 2005.
K. Gimpel. Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation. PhD
thesis, Carnegie Mellon University, 2012.
Kevin Gimpel and Noah A. Smith. Softmax-margin CRFs: Training log-linear models with cost
fUnctions. In Proceedings of NAACL, pp. 733-736, Los Angeles, California, JUne 2010.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Andrej Karpathy and Fei-Fei Li. Deep visual-semantic alignments for generating image descriptions.
In Proceedings of CVPR, pp. 3128-3137, Boston, MA, USA, June 2015.
Shankar Kumar and William Byrne. Minimum bayes-risk decoding for statistical machine translation.
Technical Report, 2004.
John Lafferty, Andrew McCallum, Fernando Pereira, et al. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of ICML, volume 1, pp.
282-289, San Francisco, California, 2001.
Jiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success. CoRR,
abs/1701.06549, 2017.
Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image
description metrics using policy gradient methods. CoRR, abs/1612.00370, 2016.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of EMNLP, pp. 1412-1421, Lisbon, Portugal, 2015.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF.
In Proceedings of ACL, pp. 1064-1074, Berlin, Germany, August 2016.
Xuezhe Ma and Hai Zhao. Probabilistic models for high-order projective dependency parsing.
Technical Report, arXiv:1502.04174, 2012.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330, 1993.
Ryan McDonald, Koby Crammer, and Fernando Pereira. Online large-margin training of dependency
parsers. In Proceedings of ACL, pp. 91-98, Ann Arbor, Michigan, June 25-30 2005.
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Improving policy gradient by exploring
under-appreciated rewards. arXiv preprint arXiv:1611.09321, 2016.
13
Under review as a conference paper at ICLR 2018
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans,
et al. Reward augmented maximum likelihood for neural structured prediction. In Proceedings of
NIPS ,pp.1723-1731, Barcelona, Spain, 2016.
Mark A Paskin. Cubic-time parsing and learning algorithms for grammatical bigram models.
Citeseer, 2001.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training
with recurrent neural networks. In Proceedings of ICLR, San Juan, Puerto Rico, 2016.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum
risk training for neural machine translation. In Proceedings of ACL, pp. 1683-1692, Berlin,
Germany, August 2016.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proceedings of ICLR, San Diego, California, 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Proceedings of NIPS, pp. 3104-3112, Montreal, Canada, 2014.
Ben Taskar, Carlos Guestrin, and Daphne Koller. Max-margin markov networks. Advances in neural
information processing systems, 16:25, 2004.
Sang Tjong Kim, Erik F., and Fien De Meulder. Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings of CoNLL-2003 - Volume 4, pp. 142-147,
Edmonton, Canada, 2003.
Maksims N Volkovs, Hugo Larochelle, and Richard S Zemel. Loss-sensitive training of probabilistic
conditional random fields. arXiv preprint arXiv:1107.1805, 2011.
Hanna M Wallach. Conditional random fields: An introduction. 2004.
Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013.
Sam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization.
In Proceedings of EMNLP, pp. 1296-1306, Austin, Texas, 2016.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys-
tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.
Zhilin Yang, Ye Yuan, Yuexin Wu, William W Cohen, and Ruslan R Salakhutdinov. Review networks
for caption generation. In Proceedings of NIPS, pp. 2361-2369, 2016.
14
Under review as a conference paper at ICLR 2018
Appendix： SOFTMAX Q-Distribution Estimation for Structured
Prediction： a Theoretical Interpretation for RAML
A Softmax Q-distribution Maximum Likelihood
A.1 Proof of Theorem 1
Proof. Since the reward function is bounded 0 ≤ r(y, y*) ≤ R, ∀y, y* ∈ Y, we have:
1 ≤ exp(r(y,y*)∕τ) ≤ eR/r
Then,
1	< ________1________ ≤	exp(r(y,y*)∕τ)	≤	eR∕τ	< eR∕τ
∣Y∣eR∕τ 1 + (|Y| - 1)eR∕τ - P exp(r(y0,y*)∕τ) - |Y| - 1 + eR/	-jʃf	(
y0∈Y
Now we can bound the conditional distribution Q(z[x) and Q0(z|x):
1	< Q(Z = z∖x = χ.τ) = eχP(EP卜(z,Y)|X = x]/T)	< 士	⑵
|Y|eR4 <Q( = | = ; )= P exp(Ep[r(y,Y)|X = x]∕τ) < |Y|	()
y∈Y
and,
IY|：R/t < QO(Z = Z|X = X； T) = EP
eχp(r(z,Y )/t ) I = X
P exp (r(y,Y)∕τ) I
y∈Y	.
eR/T
< -----
|Y|
⑶
Thus, ∀x ∈ X ,z ∈Y,
lθg !IM < 2R/T
To sum up, we have：
KL(Q(∙, ∙)kQ0(∙, ∙))= P Q(x) P Q(z|x)log ⅛gQ⅛
χ∈X z∈Y
= P Q(x) P Q(ZIX) log 舞得
x∈X	z∈Y
< P Q(x) P Q(z|x)2R/τ
x∈X	z∈Y
=2R∕τ
A.2 Proof of Theorem 2
Lemma 3. For every X ∈ X,
P(Y = y*|X = x) ≤ e-cb
where b
γ2
(T-YF.
∆
Proof. From the assumption in Theorem 2 of Eq. (20) in §3.5, we have
γ2
P(Y = y*|X = x) = P(r(y*,y*) - r(Y,y*) ≥ YR) ≤ e (1-Y)2 = e-Cb
□
Lemma 4.
1+(∣Y∣-i)eR∕τ	≤	q(y|y*)	≤	eYR∕τ + (∣γ1 - 1)e-R∕τ	,	if y	= y*
1+(∣Y∣-1)e-yr∕T	≤	q(y|y )	≤	1 + (∣Y∣-1)e-R∕τ	,	if y	= y
15
Under review as a conference paper at ICLR 2018
Proof. From Eq.(1), we have
1 + (|Y| - 1)eR∕τ ≤ αM) ≤ 1 + (|Y| - 1)e-R∕τ
If y = y*,
(| *) =	exp(r(y,y*)∕τ)	≤	er(y'y*"τ	≤ ______________1__________
q y y _ P exp(r(y∖y*)∕τ) ~ er3*,y*"τ + (|Y| - 1) - 6r" + (|Y| - 1)e-R/T
y'∈Y
If y = y*,
q(y|y ) = q(y |y )=1+ P e(r(y0,y*)-r(y*,y*))∕τ ≤ 1 + (|^| - 1)e-γR∕τ
y 0=y*
□
Lemma 5.
1+(|Y|-i)eR/T	≤	QZ(ZIx)	≤	e7R∕τ + (|Y|-1)e-R/T	+ 1+(∣"1)e-R∕τ	, if Z	=期*
1 + ( JI-I)e-”/T	≤	QZ(Wx)	≤	1 + (|Y|-I)e-R/T	鹏 y	=短
Proof.
QZ(ZIx)= Ep[q(z|Y)] = X p(y|x)q(z|y)
y∈Y
From Eq. (3) we have,
1
1 + (|Y|- 1)eR/T
≤ Q (ZIx) ≤ 1 + (|Y| - 1)e-R/T
If Z = y*,
QZ(ZIx) = P(y*|x)q(Z|y*)+ X P(y|x)q (ZIy) ≤ q(Hy*)+1 + (3 - 1)e- R/T P (Y = y*|X = x)
y=y*	U I J
From Lemma 3 and Lemma 4,
1	e-cb
Q (ZIx) ≤ e)R/T + (|Y| - 1)e-R/t + 1 + (|Y| - 1)e-R/t
If Z = y*,
QZ(ZIx)= p(y*|x)q(Z|y*) + X p(y|x)q(Z|y) ≥ p(y*|x)q(Z|y*)
y=y*
From Lemma 3 and Lemma 4,
0	1 - e-cb
Q (ZIx) ≥ 1 + (|Y| - 1)e-γR/T
Lemma 6.
0	≤	E[r(Z, Y )∕τ ]	≤	P (y*Ix)r(Z,y* )∕τ + e-cbR∕τ	,if z = y*
P(y*Ix)r(y*,y*)∕τ	≤	E[r(z, Y)∕τ]	≤	R∕τ	, if Z = y*
Proof.
E[r(z,Y)∕τ] = X P(yIx)r(z,y)∕τ
y∈Y
Since for every y, yZ ∈ Y, 0 ≤ r(y, yZ) ≤ R, we have
0 ≤ E[r(Z,Y)∕τ] ≤ R∕τ
16
Under review as a conference paper at ICLR 2018
If Z = y,
E[r(z,Y)∕τ] = P(y*∣x)r(z,y*)∕τ + E P(y∖x')r(z,y)/τ ≤ P(y* ∣x)r(z, y*)∕τ + e-cbR∕τ
y≠yt
If Z = y*,
E[r(z,Y)∕τ] = P(y*∖x)r(z,y*')∕τ + E P(y∖x)r(z,y)∕τ ≥ P(y* ∣x)r(y*, y*)∕τ
y=y*
□
Lemma 7.
1 + (∣γJ1)eR∕τ	≤ Q(ZIX)
1 + (∣Y∣-1)e-αR∕τ	≤ Q(ZIX)
where Q = Y — (1 + γ)e-cb < Y.
eαR∕τ +(∣Y∣-1)e-R∕τ
,ifZ = y*
,if y =y
Proof.
eE[r(z,Y )∕τ ]
Q(ZIX) = P eE[r(y0,Y )∕τ]
y0∈Y
From Lemma 6,
If Z = y*,
Q(Z∖x) ≤
≤
≤
1
1 + (∖Y∖ — 1)eR/T
≤ Q(ZIX) ≤ 1 + (∖Y∖- 1)e-R4
田京*,W∣-1 ≤ (e则y*,Y)/T]-则寸)/T] + (∖Y∖ - 1)e-R/T)
(eP(y*∣x)(r(y*,y*)∕τ-r(z,y*)∕τ) - e-cbR∕τ + (∖y∖ - 1)e-R∕τ)-1
(e(1-e-cb)YR∕τ-e-cbR∕τ + (∖y∖ - 1)e-R∕τ) 1
__________1_________
eαR∕τ +(∣Y∣-1)e-R∕τ
≤
≤
1
If Z = y,
-1
Q(Z∖X)
1+ E eEMy,Y)∕τ]-£[心*丫"「〕
y=y*
1
1 + (∖Y∖ - 1)e-aR/T
≤
□
Now, we can prove Theorem 2 with the above lemmas.
Proof of Theorem 2
Proof.
KL(Q(IX = X)IIQ0TX = x))
— P Q(?∕x)1cp∙	Q(y∣x)	= Q(?∕*∖x)1op∙ Q(y*∣x)	+	P Q(?∕x)1op∙ Q(y∣x)
=匚 Q(JIX)log	Q0(y∣x)	= Q(y	IX)IOg Q0 (y*∣χ)十 Q(JIX)Iog Q0(y∣x)
y∈Y	' y=y*
≤ (1 + (IYI - 1)e-R/T)-1 log (⅛ 1j;)
□________∣Y∣-1______ioo∙	1+(IYI-I)eR∕τ
+ eɑR∕τ + (∣Y∣-1)e-R∕τ 1Og eɑR∕τ + (∣Y∣-1)e-R∕τ
limKL(Q(∙∖X = x)∣∣Q0(∙IX = x)) ≤ log ɪɪ
T →0	1 e
=log 1-J-cb
= log 1⅛b
≤ e-cb =
≤ 1-e-cb =
+ lim " log(∖Y∖ - 1)e(1-a)R/T
T→0 e
+ lim eγ⅛71 (iog(∖Y∖ - 1) + (1 - α)R∕τ)
T→0 e
—cb
=log 1 + 1-e-cb
1
1+ecb
□
17
Under review as a conference paper at ICLR 2018
(a) Bayes decision rule	(b) ML
(c) RAML (τ = 0.5)	(d) RAML (best)	(e) RAML (τ = 10000)
(f) SQDML (τ = 0.5)
(g) SQDML (best)
(h) SQDML (τ = 10000)
Figure 4: Decision boundaries of different models, together with the Bayes decision rule in (a). (b)
display the decision boundary of ML. (c), (d), (e) are the decision boundaries of RAML with τ = 0.5,
τ = 10000 and the one achieves the best performance τ = 2.4. (f), (g), (h) are the corresponding
boundaries of SQDML. The best performance is achieved with τ = 1.1
B Cost-sensitive Multi-class Classification
To better illustrate the properties of ML, RAML and SQDML, we display the decision boundary
of the learned models in Figure 4. Figure 4a gives the boundary of the Bayes decision rule, and
Figure 4b is the boundary of ML. We can see that, as expected, ML gives “unbiased” boundary
because it does not incorporating any information of the task-specific reward.
Figure 4c and 4f are the decision boundaries of RAML and SQDML with τ = 0.5. We can see that,
even with small τ, SQDML is able to achieve good decision boundary similar to that of the Bayes
decision rule, while the boundary of RAML is similar to that of ML. This might suggest that RAML,
as an approximation of SQDML, might “degenerates” to ML due to approximation error.
Figure 4d and 4g provide the boundary of RAML and SQDML that achieve the best performance
(τ = 2.4 for RAML and τ = 1.1 for SQDML). RAML is able to produce surprisingly good decisions
with proper τ , which is comparable with SQDML.
Figure 4e and 4h are the decision boundaries of RAML and SQDML with large τ = 10000. We
can see that, consistently matching our analysis, neither RAML or SQDML can learn reasonable
prediction function. The reason is, as we discussed in §3.5, when τ becomes larger, the softmax
18
Under review as a conference paper at ICLR 2018
Q-distribution becomes closer to the uniform distribution, providing less information of prediction,
even though the approximation error tends to be zero.
C Image Captioning with Multiple References
Encoder Following (Yang et al., 2016), we adopt the widely-used CNN architecture VGGNet (Si-
monyan & Zisserman, 2015) as the image encoder. Specifically, we use the last fully connected
layer fc7 as image representation (4096-dimensional), which is further fed into a decoder to generate
captions. We use a pre-trained VGGNet model4 5, and keep it fixed during the training of decoder.
Decoder We use an LSTM network as the decoder to predicate a sequence of target words:
{y1, y2, . . . , yT}. Formally, the decoder uses its internal hidden state st at each time step to track the
generation process, defined as
st = fLSTM(yt-1, st-1),
where yt-1 is the embedding of the previous word yt-1. We initialize the memory cell of the decoder
by passing the fixed-length image representation x through an affine transformation layer. The
probability of the target word yt is then given by
p(yt|y<t,x) = softmax(Wsst).
Training by N -gram Replacement As discussed in §4.1.2, we approximate the exponentially large
hypotheses space Y using a subset of sampled hypotheses S . Formally, the training set D consists of
pairs of images X and multiple references {y*}, i.e., D = {(x, {y*}>}. For RAML, We split a single
training instance (x, {y*}i into multiple ones by pairing X with each y*, i.e., {(x, y*i}. And for each
instance (x, y*∖ we maximize
X q(y∣y*;T) 唳pθ(y∣χ) = X P exp窘：X)pθ(y∣χ)
y∈Y	y∈Y 乙y0∈Yexp V (y，y )∕τ)
exp (r(y,y* )∕τ)
py0∈s eχp (r(y0,y*"τ)
logPθ(y∣X).

Σ
y∈S
For SQDML, for each training example (x, {y*}i we directly maximize the weighted log-likelihood
w.r.t. the softmax-Q distribution
Q(y∣x; τ) log Pθ(y∣x) y∈Y	L	eχp (py* R⅛ r(y,y*"τ)	ι τyt l ʌ -y∈Y py0∈γexp(py* ∏y⅛r(y0,y*)/T) 0g θ y ” ≈ XPexp (PyPy^ r(y,y*"?/)iogPθ (y∣χ). M Σy0∈sexp (∑y. Er(y0, y*)/T)
In our experiments, the size of the sampled targets S is 500 for SQDML and 100 for RAML5. For the
sake of efficiency, at each iteration of stochastic gradient descent, we only use k randomly-selected
hypotheses from S to perform gradient update. k is 50 for SQDML and 10 for RAML.
Configuration We use the sentence-level BLEU with NIST geometric smoothing as the reward. We
replace word types whose frequency is less than five with a special <unk> token. The resulting
vocabulary size is 10,102. The dimensionality of word embeddings and LSTM hidden sates is 256
and 512, respectively. For decoding, we use beam search with a beam size of 5. We use a batch size
of 10 for the ML baseline and a larger size of 100 for SQDML and RAML for the sake of efficiency.
D	Experiments on Structured Prediction
D. 1 Dataset Statistics
We present statistics of the datasets we used in Table 7.
4Downloaded from https://github.com/kimiyoung/review_net
5Since each image has around five reference captions, this ensures that the number of sampled candidate y’s
for each training example is roughly the same for SQDML and RAML.
19
Under review as a conference paper at ICLR 2018
Dataset		CoNLL2003	PTB	IWSLT2014
Train	#Sent #Token	14,987 二 204,567	39,832 843,029	153,326 2,687,420 / 2,836,554
Dev.	#Sent #Token	3,466 51,578	1,700 35,508	6,969 122,327 / 129,091
Test	#Sent #Token	3,684 46,666	2,416 49,892	6,750 125,738 / 131,141
Table 7: Dataset statistics. #Sent and #Token refer to the number of sentences and tokens in each data
set, respectively (for IWSLT, they refer to the number of sentence pairs and tokens of source/target
languages).
D.2 Models for Structured Prediction
D.2.1 Log-linear Model
A commonly used log-linear model defines a family of conditional probability Pθ (y|x) over Y with
the following form:
Pθ (y|x)
Φ(y,χ; θ)
P Φ(y0,χ; θ)
y0∈Y
exp(θT φ(y,x))
P exp(θTφ(y0,x))
y0∈Y
(4)
where φ(y, x) are the feature functions, θare parameters of the model and Φ(y, x; θ) captures the
dependency between the input and output variables. We define the partition function: Z(x; θ) =
P exp(θT φ(y0, x)). Then, the conditional probability in (4) can be written as:
y0∈Y
Pθ (y|x)
exp(θT φ(y,x))
-Z(x; θ)
Now, the objective of RAML for one training instance (x, y) is:
L(θ) = - E q(y0∣y;T)logPθ(y0∣χ) = -θτ ( E q(y0∣y; T)φ(y0,χ) > + logZ(x; θ)
y0∈Y	(y0∈Y
and the gradient is:
喑 =-P q(y0∣y; T)Φ(y0,χ) + dlo⅛feθ)
y0∈Y
=-P q(y0∣y; T)φ(y0,χ) + P Pθ(y0∣χ)Φ(y0,χ)
y0∈Y	y0∈Y
= P (Pθ(y0∣χ) - q(y0∣y; T)) Φ(y0,χ)
y0∈Y
(5)
(6)
To optimize L(θ), we need to efficiently compute the objective and its gradient. In the next two sec-
tions, we see that when the feature φ(y, x) and the reward r(y, y*) follow some certain factorizations,
efficient dynamic programming algorithms exist.
D.2.2 SEQUENCE CRF
In sequence CRF, Φ usually factorizes as sum of potential functions defined on pairs of successive
labels:
L
Φ(y, x; θ) =	ψi(yi-1, yi, x; θ)
i=1
where ψi(yi-1, yi, x; θ) = exp(θT φi(yi-1, yi, x)). When we use the token level label accuracy as
reward, the reward function can be factorized as:
L
r(y,y*) = X I(yi = y↑)
i=1
where yi is the label of the ith token (word). Then, the objective and gradient in (5) and (6) can be
computed by using the forward-backward algorithm (Wallach, 2004).
20
Under review as a conference paper at ICLR 2018
D.2.3 Edge-factorized Tree-structure Model
In dependency parsing, y represents a generic dependency tree which consists of directed edges
between heads and their dependents (modifiers). The edge-factorized model factorizes potential
function Φ into the set of edges:
Φ(y, x; θ) = Y ψe(e, x; θ)
e∈y
where e is an edge belonging to the tree y. ψe(e; θ) = exp(θT φe(e, x)). The reward of UAS can be
factorized as:
L
r(y,y*) = X I(yi = y"
i=1
where yi is the head of the ith word in the sentence x. Then, we have:
P Pθ(ylx)φ(y,χ) = PP Pθ(ylx)φe(e, x)
y∈Y	y∈Y e∈y
P Pθ(y|x)
y∈Y(e)
where E is the set of all possible edges for sentence x and Y(e) = {y ∈ Y : e ∈ y}. With similar
derivation, we have
X q(y0∣y; T)φ(y0,x) = XΦe(e,x) < X q(y0∣y)卜	(8)
y0∈Y	e∈E	(y0∈Y(e)
Both (7) and (8) can be computed by using the inside-outside algorithm (Paskin, 2001; Ma & Zhao,
2012)
D.2.4 Attentional Neural Machine Translation Model
(7)
)
P φe(e, x)
e∈E
Model Overview
We apply a neural encoder-decoder model with attention and input feeding (Luong et al., 2015).
Given a source sentence x of N words {xi}iN=1, the conditional probability of the target sentence
y = {yi}iT=1, p(y|x), is factorized as p(y|x) = QtT=1 p(yt|y<t, x). The probability is computed
using a bi-directional LSTM encoder and an LSTM decoder:
Encoder Let xi denote the embedding of the i-th source word xi . We use two unidirectional LSTMs
to process x in forward and backward order, and get the sequence of hidden states {~hi }iN=1 and
{hi }iN=1 in the two directions:
~hi = fL→STM(xi, ~hi-1)
hi = fLSTM (xi, hi+1),
where f→TM and fLlrM are standard LSTM update functions as in Hochreiter & Schmidhuber (1997).
The representation of the i-th word, hi , is then given by concatenating ~hi and hi .
Decoder An LSTM is used as the decoder to predict a target word yt at each time step t. Formally,
the decoder maintains a hidden state st to track the translation process, defined as
St = fLSTM([yt-1 : St-l], St-1),
where [:] denotes vector concatenation, and yt-1 is the embedding of the previous target word. We
initialize the first memory cell of the decoder using the last hidden states of the two encoding LSTMs:
cell0 = W[~hN : hN] + b. And the first hidden state of the decoder is initialized as S0 = tanh(cell0).
The attentional vector St is computed as
St = tanh(WJst : ct]),
21
Under review as a conference paper at ICLR 2018
Method		Bleu
ML Baseline		27.66
τ	二 0.10	28.22
τ	0.20	28.22
τ	0.30	28.22
τ	0.40	28.31
τ	0.50	28.28
τ	0.60	28.23
τ	0.70	28.61
τ	0.80	28.30
τ	0.90	28.40
τ	1.00	28.42
N-Gram		28.77
Table 8: Corpus-level BLEU score of RAML using importance sampling
Method		BLEU
ML BaSelinl		27.66
τ	二 0.60	27.96
τ	0.65	27.94
τ	0.70	28.18
τ	0.75	27.96
τ	0.80	27.93
τ	0.85	27.97
τ	0.90	28.39
τ	0.95	28.30
τ	1.00	28.32
τ	二 1.05	27.92
Impt. Sample		28.61
N -GRAM		28.77
Table 9: Corpus-level BLEU score of RAML using negative Hamming distance as the reward function
where the context vector ct is a weighted sum of the source encodings {hi } via attention (Bahdanau
et al., 2015). The probability of the target word yt is then given by
p(yt∣y<t,x) = SOftmax(WsSt).
Configuration We use the same pre-processed dataset as in Wiseman & Rush (2016). The vocabulary
size of the German and English data is 32,008 and 22,821 words, resp. Similar as Bahdanau et al.
(2017), the dimensionality of word embeddings and LSTM hidden states is 256. All neural network
parameters are uniformly initialized between [-0.1, +0.1]. We use Adam optimizer. We validate
the perplexity of the development set after every epoch, and halve the learning rate if the validation
performance drops. We use the sentence level BLEU with NIST geometric smoothing as the training
reward, and use the official multi-bleu.perl script for evaluating corpus-level BLEU. The
beam size for decoding is 5. We use a batch size of 64 for ML baseline and a larger size of 100 for
RAML for the sake of efficiency.
Approximating the Learning Objective using Importance Sampling
As suggested in Norouzi et al. (2016), with BLEU as the training reward, the objective function (4) of
RAML could be optimized using importance sampling. To verify this, we conducted experiment using
importance sampling. Since we cannot directly sample from the exponentiated payoff distribution
parameterized by BLEU score (i.e., qBLEu(y∣y*,τ)), We use the payoff distribution with negative
Hamming distance (i.e., qhm(y∣y*,τ)) as the proposal distribution, and sample from qhm(y∣y*,τ)
22
Under review as a conference paper at ICLR 2018
instead. Specifically, at each training iteration, we approximate (4) by
£q(y|y'; τ )iog Pθ (y∣χi)
y∈Y
≈
y 〜qhm (.|y*,τ)
_________GBLEU(y|y*; T )∕⅜ιm(y∣y*; T)________
PyO 〜qhm(∙∣y*,τ ) qBLEU(y0|y*； T ) A⅛m® । U * ； T )
log Pθ(y∣χi)
Σ
y 〜qhm
exp{BLEU(y,y*)∕τ}∕ exp{hm(y,y*)∕τ}
PyO 〜qhm exp{BLEU(y0,y*)∕τM eχp{hm(y0,y*)λτ}
log Pθ (y∣χi),
where the q(∙)'s denote the payoff distributions without normalization terms. BLEU(∙) and hm(∙)
denote sentence-level BLEU score and negative Hamming distance, respectively. We use a sample
size of 10.6
To draw a sample y from qhm(y∣y* ,τ), We follow Norouzi et al. (2016) and apply stratified sampling.
We first sample a distance d ∈ [0,1,2,..., ∣y*∣-1, ∣y*∣],andthensampleasentence y with Hamming
distance d from y*. Let c(d, L) denote the number of y's with length L and an Hamming distance of
d from the ground-truth y*, qhm(y∣y*,τ) is then defined as:
qhm(y|y*, τ)
exp{hm(y,y*)∕τ }
Pd=Oc(d, |y*|) ∙ exP{-d/T}
Similar as in Norouzi et al. (2016), c(d, L) is approximated by considering d substitutions of words
from y * :
c(d, L) = Ld(V -1)d,
where V is the vocabulary size.7
Table 8 lists the performance of importance sampling with different temperatures. The best model
(under τ = 0.8) is comparable with the one achieved by n-gram replacement. However, n-gram
replacement is much simpler to implement, and importance sampling requires extra computation of
the proposal distribution and associated importance weights, which would be less computationally
efficient. In our experiments, our highly optimized RAML model achieves a training speed of 18,000
words/sec for importance sampling and 21,000 words/sec for n-gram replacement.
Extra Experiments using Negative Hamming Distance as Training Reward
For the sake of completeness, we also experimented using the negative Hamming distance as the
reward function for RAML, as proposed in Norouzi et al. (2016). Results are listed in Table 9. The
best model gets a corpus-level BLEU score of 28.39, which is worse than the best results achieved by
optimizing directly towards BLEU scores (c.f. Table 4).
6We also tried larger sample size but did not observe significant gains.
7Through correspondence with the authors, We scale T by ι+ι°g(v_])
23