Under review as a conference paper at ICLR 2018
The Information-Autoencoding Family:
A Lagrangian Perspective on Latent Variable
Generative Modeling
Anonymous authors
Paper under double-blind review
Ab stract
A variety of learning objectives have been recently proposed for training gener-
ative models. We show that many of them, including InfoGAN, ALI/BiGAN,
ALICE, CycleGAN, VAE, β-VAE, adversarial autoencoders, AVB, and InfoVAE,
are Lagrangian duals of the same primal optimization problem. This generaliza-
tion reveals the implicit modeling trade-offs between flexibility and computational
requirements being made by these models. Furthermore, we characterize the class
of all objectives that can be optimized under certain computational constraints.
Finally, we show how this new Lagrangian perspective can explain undesirable
behavior of existing methods and provide new principled solutions.
1	Introduction
Deep generative models have been successfully utilized in a wide variety of tasks (Radford et al.,
2015; Zhu et al., 2017; Yang et al., 2017; Li et al., 2017b). Prominent examples include Variational
Autoencoders (VAE, Kingma and Welling (2013); Rezende et al. (2014)) with extensions such as β-
VAE (Higgins et al., 2016), Adversarial Autoencoders (Makhzani et al., 2015), and InfoVAE (Zhao
et al., 2017); adversarially trained models such as Generative Adversarial Networks (Goodfellow
et al., 2014) and their extensions such as ALI/BiGAN (Dumoulin et al., 2016a; Donahue et al., 2016),
InfoGAN (Chen et al., 2016a) and ALICE (Li et al., 2017a); hybrid objectives such as CycleGAN
(Zhu et al., 2017), DiscoGAN (Kim et al., 2017) and AVB (Mescheder et al., 2017). All these
models attempt to fit an empirical data distribution, but differ in the measure of similarity between
distributions, whether or not they allow for efficient (amortized) inference, and the extent to which
the latent variables should retain or discard information about the data. These methods also crucially
differ in terms of the optimization techniques used, which can be likelihood-based or likelihood-
free (Mohamed and Lakshminarayanan, 2016).
In this paper, we revisit the question of designing a training objective for a latent variable genera-
tive model. We formulate a constrained (variational) optimization problem that explicitly captures
the various desired properties of a generative model, such as fit to the data distribution, tractable
(amortized) inference, and lossy vs. lossless compression. Surprisingly, we show that all the pre-
viously mentioned models are different Lagrangian duals of the same primal optimization problem.
They only differ in the value assigned to the Lagrange multipliers, corresponding to different prior-
itizations of the various (potentially conflicting) desiderata. In fact, by considering all possible La-
grangian duals, we obtain a strictly more general class of models, of which InfoGAN, ALI/BiGAN,
ALICE, VAE, β-VAE, adversarial autoencoders, AVB, CycleGAN/DiscoGAN and InfoVAE are all
special cases.
This general formulation reveals the modeling trade-offs being made between flexibility and the
tractability of computing and optimizing the resulting objective. We show that each of the previously
mentioned models correspond to a special choice of the Lagrange multipliers that ‘cancels out’
certain intractable terms from the training objective, so that a particular computational constraint
is satisfied (e.g., it can be optimized likelihood-based or likelihood-free). In addition, under some
mild assumptions, we are able to characterize all the training objectives that can be optimized under
certain computational constraints.
1
Under review as a conference paper at ICLR 2018
This generalization also provides new insights into the properties of these models, which are implied
by particular choices of the Lagrange multipliers. First, solving the dual problem requires optimiza-
ton over the Lagrange multipliers (to obtain the best approximation to the primal). However, existing
models use fixed values for the Lagrangian multipliers. We explain the consequences of this approx-
imation, and demonstrate scenarios where this approach fails to solve the primal. Furthermore we
show that when the primal is infeasible (e.g., due to insufficient modeling capacity), the choice of
Lagrange multipliers prioritizes different constraints, and show that appropriate prioritization can
lead to improved log-likelihood score.
The paper is organized as follows. In Section 2, we start from an axiomatic derivation of VAEs
from minimum desiderata, and realize that most generative model objectives can be written as the
Lagrangian dual of a constrained variational optimization problem. In Section 3, we consider the
more general case in which the Lagrangian multipliers can be arbitrarily selected, and enumerate
over all possible objectives under different computation constraints. In Section 4, we introduce two
applications of this formulation that allows us to diagnose potential problems, compare different
approaches and design new models.
2	The Information-Autoencoding Family
We consider a class of probabilistic generative models with two types of variables: observed ‘data’
variables (denoted x ∈ X) and latent ‘feature’ variables (denoted z ∈ Z). For ease of exposition,
we describe the case of discrete variables, but the results apply to continuous variables as well1 . The
model family specifies a joint probability distribution pθ (x, z) parameterized by θ. We assume that
the distribution can be factored as
pθ(x, z) = p(z)pθ (x|z)
where p(z) is a given, fixed distribution. We assume that it is efficient to sample from p(z) and
evaluate p(z) for any z. Likewise, We assume it is easy to evaluate and to sample from pθ(x|z) for
any x and z. In general, pθ(x) is intractable to compute but easy to sample from (by sampling (x, z)
from pθ(x, Z) and discarding z); pθ(z|x), on the other hand, is generally both difficult to evaluate
and to sample from.
Given samples from a data distribution pdata(x), the learning goal is to find θ such that pθ(x) ≈
pdata(x). The hope is often to discover latent structure in the data through the latent variables
z, evocatively called ’features’. Given a data point x, one would want to infer the corresponding
latent variables using pθ(z|x). However, as noted above, that is generally intractable to evaluate
and sample from. We consider the case of a separate amortized inference distribution qφ(z∣χ), and
assume qφ(z∣χ) is easy to evaluate and to sample from. This results in the following approximation
(lower bound) to the intractable pθ (x), known as the ELBO:
log Pθ (X) ≥ Epdata(x)qφ(z∣x) [log Pθ (X|z)] - Epdata(X)DKL Sφ (ZIx)IIp(Z))]	⑴
where DKL denotes the divergence: DKL(pkq) = Ep [log p - log q].
2.1	Axiomatic Derivation of Variational Autoencoders
Instead of deriving a learning objective as an approximation to the intractable marginal likelihood
pθ(X), we start from a list of basic properties that any learned model should satisfy. Intuitively, under
ideal conditions, the generative model should model the true data distribution, while the amortized
inference model qφ(ZIX) should capture the true posteriorpθ(ZIX). We formalize this in the follow-
ing definition:
Definition 1 (Consistency Condition). A distribution pθ (X, Z) over X × Z, a distribution pdata(X)
over X, and a family of conditional (inference) distributions {qφ(ZIX)}x∈X are said to be consistent
with each other if the following holds:
•	Correct Data Marginal: ∀X, pθ(X) = pdata(X).
•	Correct Feature Inference: ∀X such that pdata(X) 6= 0, qφ(ZIX) = pθ(ZIX).
1under mild technical assumptions, such as the existence of the relevant densities.
2
Under review as a conference paper at ICLR 2018
We can alternatively express the consistency requirement using the following definition (Dumoulin
et al., 2016b; Donahue et al., 2016)
Definition 2 (Joint Inference Distribution). Given pdata(x) over X, and a family of conditional
(inference) distributions {qφ(z∣x)}x∈x we define the joint inference distribution as
qφ (χ,z) = Pdata(X)qφ(z∣χ)
For compactness, we denote q(x) = pdata(x).
Corollary 1. pθ(x, Z), Pdata(X) and {qφ(z∣x)}x∈x are consistent with each other ifand only if
qφ(x, z) = pθ (x, z)	(2)
By noticing that the roles played by z and X are symmetric, we also have the following equivalent
definition of consistency:
Definition 3 (Feature Marginal and Posterior). Given the joint distribution qφ(X, z) defined in Defi-
nition 2 we define the following
qφ(z) = Eqφ(x,z)	qφ (XIz) = qφ(χ,z"qφ(z)
x
Corollary 2. pθ (x, z), Pdata(X) and {qφ(z∣x)}χ∈χ are consistent with each other ifand only if
•	Correct Feature Marginal: ∀z, qφ(z) = P(z).
•	Correct Data Inference: ∀z such that p(z) = 0, qφ(χ∣z) = pθ(χ∣z).
2.2	A Constrained Optimization Formulation
It is not difficult to see that for a given data distribution Pdata(X), there can be multiple distinct
choices of θ, φ satisfying the stated consistency condition (Chen et al., 2016b; Zhao et al., 2017;
Li et al., 2017a). A few examples are provided in the appendix. To avoid this problem, we need
to specify a preference for which solution is desirable. That is, we consider the following general
optimization problem 2
min	f(θ, φ) s.t. P, Pdata, q are consistent with each other	(3)
θ,φ
Any f(θ, φ) can in principle be used. In this paper, we study a special but important case:
f(θ, φ) = αιIp(x; z) +(02Iq(x; z)	(4)
where Ip(X; z), Iq(X; z) are mutual information between X and z under distributions P(X, z) and
q(X, z) respectively. The coefficients α1, α2 can be either positive or negative, to maximize or min-
imize mutual . Both cases could be desirable, depending on the goal. Models like InfoGAN (Chen
et al., 2016a) explicitly maximize mutual information with respect to a group of latent variables to
ensure that this group of variables is used. Information maximization is also useful to avoid the
uninformative latent code problem (Chen et al., 2016b; Zhao et al., 2017), where latent features are
not used when using certain training objectives. In other scenarios we may wish to minimize mutual
information. For example, the information bottleneck (Tishby and Zaslavsky, 2015) approach min-
imizes mutual information between input and latent features under the constraint that the features
can still predict some labels. Similar ideas are also widely used in compression (Shamir et al., 2010)
where minimum information is retained while still being able to perform certain tasks.
2.3	Variational Autoencoding as Constrained Optimization
To solve the above optimization problem (3) constrained with the consistency condition we use the
notion of a divergence between probability distributions.
Definition 4. Let P(A) be the set of all probability distributions over a set A. A functional D :
P(A) × P(A) → R+ is called a strict divergence if D(Pkq) = 0 if and only ifP = q.
2Starting from this section we neglect the subscript pθ and qφ to simplify the notation.
3
Under review as a conference paper at ICLR 2018
min
θ,φ
s.t.
or
or
Examples of strict divergences include Kullback-Leibler (KL) divergence, maximum-mean discrep-
ancy (with a universal kernel, see Gretton et al. (2007)), Jensen-Shannon Divergence (Nowozin
et al., 2016), etc. In particular, the reverse KL divergence is defined as D-KL(Pkq) = DκL(q∣∣p)∙
Given strict divergences D, and D0, and using the equivalent formulation of consistency in Definition
1 and Corollaries 1 and 2, we can define the following optimization problems with one of three
possible constraints3:
α1Ip(x; z) +
D(p(x, z)kq(x, z)) = 0	(5)
D(q(x)kp(x)) = 0 , D0(q(z|x)kp(z|x)) = 0	∀x ∈ X	(6)
D(p(z)kq(z)) = 0 , D0(p(x|z)kq(x|z)) = 0	∀z ∈ Z	(7)
Itis easy to see that using any of the three constraints (5)(6)(7) is equivalent to the original optimiza-
tion problem in Eq.(3). In fact, any superset of the constraints (5)(6)(7) also leads to an equivalent
optimization problem (with redundancy).
2.4	Variational Autoencoding as Lagrangian Dual
A natural approach to solve constrained optimization problems is to consider their Lagrangian relax-
ation. For example, the Lagrangian corresponding to (5) would be f(θ, φ) + λD(p(x, z)kq(x, z))
where λ is the Lagrange multiplier. More generally, we define a broad class of Lagrangian relax-
ations corresponding to the constraints in (5), (6), and (7) and their supersets. This leads to the
following general class of training objectives, which we shall see encompasses a large number of
existing probabilistic autoencoding models.
Definition 5. The Information(Info)-Autoencoding family is the set of all objectives of the form
n
L(λ, α1, α2) = α1Ip(x; z) + α2Iq(x; z) +	λiDi	(8)
i=1
where α1, α2 ∈ R, λi ∈ R, λi > 0 are Lagrangian parameters, and the following are satisfied:
1)	For all i = 1,…,n, each Di is one of the following
D(p(x, z)kq(x, z)), D(p(x)kq(x)), D(p(z)kq(z))
Er(x)[D(p(z|x)kq(z|x))], Et(z)[D(p(x|z)kq(x|z))]
where D is any strict divergence, and r, t are any distributions supported on X, Z respectively
2)	There exists strict divergences D, D0, and distributions r, t supported on X, Z respectively, so
that one Ofthefollowing sets Ci, C2,C3 is a subset of {Dι, •…,Dn}
C1 = {D(p(x, z)kq(x, z))}	(9)
C2 = {D(p(x)kq(x)), Er(x)[D0(p(z|x)kq(z|x))]}	(10)
C3 = {D(p(z)kq(z)), Et(z)[D0(p(x|z)kq(x|z))]}	(11)
We call this family the KL Information-Autoencoding family ifin the above definition all divergences
D are either KL or reverse KL, r(x) is p(x) or q(x), and t(z) is p(z) or q(z).
In the above definition, r(x) is itself a set of Lagrangian parameters, corresponding to the set of
constraints {D(q(z|x)kp(z|x)) | x ∈ support(q(x))}. In this paper we assume r(x) is either p(x)
or q(x). Similarly, we assume t(z) is eitherp(z) or q(z). We leave the study of more general choices
for r, t as future work.
In general We must maximize over the Lagrangian dual parameters λ = (λι,…，λn) to find the
tightest relaxation to the primal. However we shall show in the following section that many existing
models simply pick specific (suboptimal) values of λ. In Section 3, We Will shoW that this is due to
tractability reasons: the Lagrangian is easy to optimize (in a certain sense) only for some λ.
3To avoid cluttering the notation, We assume p(x, z) and q(x, z) are supported over the entire space X × Z.
4
Under review as a conference paper at ICLR 2018
2.5	Relationship with Existing Models
The learning objectives used to train many existing latent variable models (such as ELBO, InfoGAN,
etc.) can be rewritten in the Lagrangian form introduced in Definition 5. By rewriting them explicitly
as in Eq.(8), we uncover the implicit tradeoffs being made (by choosing the Lagrange multipliers,
i.e., pricing constraint violations), revealing similarities and symmetries between existing models
along the way.
1)	The original evidence lower bound (ELBO) in Eq.(1) can be rewritten into equivalent Lagrangian
dual forms
LELBO = Eq(z) [DKL(q(x|z)kp(x|z))] + DKL(q(z)kp(z))	(12)
= Eq(x)[DKL(q(z|x)kp(z|x))] + DKL (q(x)kp(x))	(13)
The second form (13) is well known (Kingma and Welling, 2013) and is traditionally the explanation
why optimizing ELBO leads to consistency condition. The same conclusion could also be derived
symmetrically from Eq.(12).
2)	ELBO has been extended to include a β scaling parameter (Higgins et al., 2016)
Lβ-ELBO = -Eq(x,z)[log p(x|z)] + βEq(x) [DKL(q(z|x)kp(z))]	(14)
This can be written as
Lβ-ELBO = (β - 1)Iq(x; z) + Eq(z) [DKL(q(x|z)kp(x|z))] + βDKL (q(z)kp(z))	(15)
which is the Lagrangian for Eq.(11) choosing α1 = 0, α2 = β - 1. This shows clearly the infor-
mation preference of the model. If β > 1 this objective is information minimizing; if β < 1 this
objective is information maximizing; and neutral when β = 0 which is the original ELBO objective.
This neutrality corresponds to the observations made in (Chen et al., 2016b; Zhao et al., 2017).
3)	The InfoGAN objective in (Chen et al., 2016a)4 can be converted to Lagrangian form for Eq.(10)
LInfoGAN = -Ip(x; z) + Ep(x)[DKL(p(z|x)kq(z|x))] + DJS (p(x)kq(x))
where α1 = -1, α2 = 0, and the Jensen-Shannon divergence DJS is approximately optimized in
a likelihood-free way with adversarial training. This equivalent formulation highlights the mutual
information maximization property of InfoGAN. It also reveals that the InfoGAN objective encour-
ages correct inference because of the DKL(p(z|x)kq(z|x)) term.
4)	InfoVAE/AAE (Zhao et al., 2017; Makhzani et al., 2015) written in Lagrangian form becomes
LInfoVAE = -Iq(x; z) + Eq(z) [DKL(q(x|z)kp(x|z))] + D(q(z)kp(z))
where D is any divergence that can be optimized with likelihood-free approaches. This reveals
an elegant symmetry between InfoGAN and InfoVAE. Both maximize mutual information, except
InfoGAN is the Lagrangian of constraints in Eq.(10) while InfoVAE is the Lagrangian of those in
Eq.(11).
5)	ALI/BiGAN corresponds to direct minimization of DJS (q(x, z)kp(x, z)), while ALICE (Li et al.,
2017a) has the following Lagrangian form:
LALICE = -Iq(x; z) + Eq(z) [DKL(q(x|z)kp(x|z))] + DJS(q(x, z)kp(x, z))
This is similar to InfoVAE except that the final divergence is taken with respect to the joint pair
(x, z) rather than z only.
6)	CycleGAN/DiscoGAN (Zhu et al., 2017; Kim et al., 2017) can be written as
LCycleGAN = -Iq(x; z) - Ip(x; z) + Eq(z) [DKL(q(x|z)kp(x|z))] + Ep(x)[DKL(p(z|x)kq(z|x))]
+ DJS(q(x)kp(x)) + DJS (q(z)kp(z))
This reveals clearly that the model is maximizing mutual information, and encouraging correct in-
ference in both direction by both matching q(x|z) and p(x|z) and matching p(z|x) and q(z|x).
4The original InfoGAN applies information maximization only to a subset of latent variables z .
5
Under review as a conference paper at ICLR 2018
3	Enumeration of Tractable Families
In the previous section, we introduced a very broad family of training objectives encompassing
several widely used approaches. While all the objectives in the family are reasonable, the special
cases we have identified have a very special property in common: they are (in a certain sense) ‘easy’
to optimize. This is a highly non-trivial property to have. In fact, mutual information and most
divergences used in Definition 5 are generally not tractable to compute and/or optimize. Thus, the
objectives will be ‘easy’ to optimize only if we carefully choose certain values of the Lagrange
multipliers λi, so that certain terms ‘cancel out’.
In this section, we propose a general framework to formally characterize all training objectives in the
family that satisfy certain tractability requirements. We formalize tractability by defining the notion
of a tractable “tool box”, corresponding to a set of terms that we assume can be computed/optimized
reasonably well, such as the ones used in ELBO (Kingma and Welling, 2013; Rezende et al., 2014),
InfoGAN (Chen et al., 2016a), InfoVAE (Zhao et al., 2017), etc. Note that these objectives are
generally not provably tractable to optimize, as they involve non-convexities, expectations over high-
dimensional spaces, etc. There is, however, significant empirical evidence that they can be optimized
reasonably well in practice, and we therefore group them based on the techniques employed.
The main technical challenge is dealing with all the possible simplifications and transformations that
leave an objective invariant. For example, we need to account for objectives like (15), which appear
to involve components that are difficult to estimate such as Iq(x; z), but can actually be simplified to
a more manageable form (14) that can be optimized in a likelihood-based way.We therefore define
a set of transformations that leave an objective equivalent according to the following definition.
Definition 6. An objective L is equivalent to L0 when ∃C, so that for all parameters θ, φ, L(θ, φ) =
L0 (θ, φ) + C. We denote this as L ≡ L0.
We are then able to analyze all the objectives that contain elements from a given “tool box”, and all
objectives derived from them through equivalent transformations.
3.1	Likelihood-based and likelihood-free Components
We formalize the idea of ”computation toolbox” by the following definition:
Definition 7. For any divergence D, we define
(1)	Likelihood-based terms as following set
T1 = {Ep[log p(x|z)], Ep[log p(x, z)], Ep[log p(z)], Ep[log q(z|x)]
Eq[log p(x|z)], Eq[log p(x, z)], Eq[log p(z)], Eq[log q(z|x)]}
(2)	Unary likelihood-free terms as the following set
T2 = {D(q(x)kp(x)), D(q(z)kp(z))}
(3)	Binary likelihood-free terms as the following set
T3 = {D(q(x, z)kp(x, z))}
Define an objective L as likelihood-based computable if there exists equivalent transformations
L0 ≡ L so that L0 only contains terms in T1; unary likelihood-free computable if L0 only contains
terms in T1 ∪ T2; binary likelihood-free computable if L0 only contains terms in T1 ∪ T2 ∪ T3.
Consider the set T1 of likelihood-based terms. Because we can directly compute likelihoods for
logp(x|z), log p(x, z), log p(z) and log q(z|x), optimizing over an expectation over these terms
can be achieved by back-propagation using path-wise derivative methods (the reparameterization
trick) (Kingma and Welling, 2013; Schulman et al., 2015; Jang et al., 2016; Maddison et al., 2016),
and by likelihood ratios when reparameterization is not applicable (Mnih and Gregor, 2014; Schul-
man et al., 2015).
Likelihood-free methods have been extensively studied in recent years. Examples include Jensen
Shannon divergence (Goodfellow et al., 2014), f-divergences (Nowozin et al., 2016), Wasserstein
6
Under review as a conference paper at ICLR 2018
P(X)Z)
q(ZIx)
distance (Arjovsky et al., 2017; Gulrajani et al., 2017) and Maximum Mean Discrepancy (Gretton
et al., 2007; Li et al., 2015). Even though such models are still difficult to train, significant improve-
ments have been made in terms of stability and efficiency. Here, we distinguish between the (unary)
case, where we consider a single group of variable at a time (either x or z), and the more general
binary case. The reason is that the binary case has been empirically shown to be more difficult to
train, for example leading to inaccurate inference (Dumoulin et al., 2016b; Donahue et al., 2016).
3.2	Elementary Transformations
Many elementary probabilistic identities can be used to rewrite an objective into another one which
is equivalent according to Definition 6. An example is the chain rule identity
p(x, z) = p(x)p(z|x) = p(z)p(x|z)
In addition, terms that do not contain trainable parameters (φ, θ) can be freely added to the objective
without affecting the optimization. Examples include Ep [log p(z)] and Eq [log q(x)]. We formalize
the above intuition in the following definition.
Definition 8. Define the following set of transformations as the elementary transformations
EJlogp(x,z)] ≡ EJlogp(x) + logp(z∣x)] ≡ EJlogp(z) + logp(x∣z)]
EJlog q(x,z)] ≡ EJlog q(x) + log q(z∣x)] ≡ EJlog q(z) + log q(x∣z)]
Eq [log q(x)] ≡ 0	Ep[log p(z)] ≡ 0
where E* indicates that the equivalence holds for both Ep and Eq.
Note that this set does not contain all possible transformations, such as importance sampling
Eq [logp(x)] ≡ Eq(x) log Eq(z|x)
We leave extensions to more general transformations to future work. Nevertheless, we are now in a
position to study all the objectives that can be constructed using the building blocks from Definition
7, and any objective derived by applying elementary transformations. From now on, we restrict the
definition of tractable family in Definition 7 to equivalence by elementary transformation only.
3.3	Enumeration of KL Information-Autoencoding Family
In the previous sections we have formally defined a grammar for tractable models. In this section, we
will use these definitions to find all possible objectives that can be optimized under given tractability
assumptions. We shall write the largest family under each tractability constraint in two forms: a
Lagrangian dual form, and an equivalent tractable form that only contains terms satisfying assumed
computability constraints.
In this section we focus on the KL Information-Autoencoding family defined in Definition 5 where
all divergences in the constraints are either DKL or DKL.5 We discuss KL divergence because
it is the most interesting case to study under elementary transformations. KL divergence is a sum
of expectations over log probabilities DKL(p||q) = Ep [log p - log q] while the elementary trans-
formations are all linear operations on log probabilities. We will discuss generalization to other
divergences in the next section.
For likelihood based computable families, we show that variational mutual information maximiza-
tion (Barber and Agakov, 2003) and β-ELBO (Higgins et al., 2016) are the only possible families
(up to equivalences). Based on the set of transformations we considered, there do not exist other
objectives satisfying the stated computational constraints. This is formally given by the following
theorem.
Theorem 1. Define the following objectives
5To avoid cluttering the notation, if a DKL term has free variables such as z in DKL (q(x|z)kp(x|z)), then
we take the expectation for that free variable with respect to p when the first distribution of the KL divergence
is DKL(Pl∣∙), and With respect to q otherwise, e.g. Eq(z)[DKL(q(x|z)||p(x|z))], Ep(z)[DκL (p(x∣z)kq(x∣z)].
7
Under review as a conference paper at ICLR 2018
1)	Variational mutual information maximization
LVMI = -Ip(x; z) + DKL(p(z|x)kq(z|x)) = Ep[log q(z|x)]
2)	β -ELBO, where β is any real number
Lβ-VAE = (β - 1)Iq(x, z) + DKL(q(x|z)kp(x|z)) + βDKL (q(z)kp(z))
= -Eq[logp(x|z)] + βEq[log q(z|x)] - βEq [log p(z)]
Then any likelihood based computable objective of the KL Info-Autoencoding family is equivalent
by elementary transformation to a linear combination of Lβ-ELBO and LVMI.
We formally prove this theorem in the appendix and give a brief sketch here. This theorem is proved
by demonstrating that a set of terms such as Eq [log p(x)] can be selected, so that each objective
of the KL Information-Autoencoding family is a linear combination of these basic terms. Treating
the set of basic terms as basis for a vector space (over functions), each training objective can be
represented as a vector in this space. Each family of objectives can be represented as a subset and in
some cases as a linear subspace. The key technical difficulty is to show that the following family of
objectives correspond to the same linear subspace:
1.	Linear combinations of Lβ-ELBO and LVMI, and their elementary transformations.
2.	Likelihood-based objectives (as in Definition 7), and their elementary transformations.
For unary likelihood free computable objectives, we show that a slight generalization of InfoGAN
and InfoVAE are the only possible families. This is formally given by the following theorem.
Theorem 2. Define the following objectives
3)	KL InfoGAN, where λ1 , λ2 are any real number
LInfoGAN = -Ip(x; z) + DKL(p(z|x)kq(z|x)) + λ1DKL(p(x)kq(x)) + λ2DKL(q(x)kp(x))
= -Ep[log q(z|x)] + λ1 DKL (p(x)kq(x)) + λ2DKL(q(x)kp(x))
4)	KL InfoVAE, where α, λ1 , λ2 are any real number
LInfoVAE = αIq(x; z) + DKL(q(x|z)kp(x|z)) + λ1DKL(q(z)kp(z)) + λ2DKL(p(z)kq(z))
= -Eq [log p(x|z)] + (α + 1)Eq [log q(z|x)] - (α + 1)Eq[logp(z)]+
(λ1 - α - 1)DKL(q(z)kp(z)) + λ2DKL (p(z)kq(z))
Then any unary likelihood free computable objective of the KL Info-Autoencoding family is equiva-
lent by elementary transformation to a linear combination of LInfoGAN and LInfoVAE.
Note that there is a slight asymmetry between InfoGAN and InfoVAE because log q(x) is assumed
to be intractable, while log p(z) is assumed tractable. The consequence is that for InfoGAN, the
coefficient for the mutual information term Ip (x; z) must be tied together with the coefficient for
DKL(q(x|z)kp(x|z)) to meet the computational constraint. This means that InfoGAN can only
maximize mutual information. On the other hand, InfoVAE can either maximize or minimize mutual
information.
Finally for the most general case, we show that any binary likelihood free computable member of
the KL Information-Autoencoding family can be written as a linear combination of InfoVAE and an
information maximizing/minimizing form of BiGAN/ALI/ALICE (Donahue et al., 2016; Dumoulin
et al., 2016b; Li et al., 2017a), which we term InfoBiGAN.
Theorem 3. Define the following objective
5) KL InfoBiGAN, where α, λ1, λ2 are any real number
LInfoBiGAN = αIp(x; z) + DKL(p(z|x)kq(z|x)) + λ1DKL(p(x)kq(x)) + λ2DKL(q(x)kp(x))
= (α + 1)DKL(p(x, z)kq(x, z)) + (λ1 - α - 1)DKL(p(x)kq(x))+
λ2DKL (q(x)kp(x)) - αEp[log q(z|x)]
Then any binary likelihood free computable objective of the KL Info-Autoencoding family is equiv-
alent by elementary transformation to a linear combination of LInfoVAE and LInfoBiGAN .
8
Under review as a conference paper at ICLR 2018
3.4 Extensions and Limitations
A simple extension beyond KL divergence is possible. For any KL Info-Autoencoding family un-
der the given tractability assumptions, we may add terms in the same tractability category and the
resulting objective will still satisfy the same tractability assumptions. For example, for a subset of
the KL InfoVAE family, we can add the Maximum Mean Discrepancy (MMD) distance to derive
the MMD InfoVAE (Zhao et al., 2017) in Lagrangian form.
LMMD-VAE = αIq(x; z) + DKL(q(x|z)kp(x|z)) + λDMMD(p(z)kq(z))
and this will still be unary likelihood free computable.
There are two limitations to our analysis. First we do not prove uniqueness results for families with
arbitrary divergences. Second we use restricted computability toolboxes and elementary transfor-
mations. More general results could be derived by considering a broader family of divergences,
computable terms, and transformations. For example, enumeration for models including Renyi di-
vergence variational inference (Li and Turner, 2016) and Stein variational gradient (Liu and Wang,
2016) are interesting directions for future work.
4	Applications
The theory developed in the previous section reveals the tradeoffs between modeling flexibility and
computational costs. We may gain additional insight into the behavior of the models by analyzing
another trade-off: the Lagrangian dual parameters λ and the information preference parameters
α1 , α2 specify how different terms are weighted or “priced” when it is not possible to optimally
minimize all of them simultaneously. This gives us insight into the behavior of the models under
finite capacity conditions.
4.1	Correctness of Lagrangian Dual
In theory, we should optimize L over both θ, φ (minimize) and λ (maximize) to solve the original
primal optimization problem (assuming strong duality holds, or to get the tightest relaxation). How-
ever, as we have shown in Section 3, that this is not always possible, as only some choices of λ
lead to a dual with a tractable equivalent form (so that some terms cancel out). In addition, many
existing models optimize the Lagrangian dual over θ, φ only for a single choice of λ but still achieve
impressive log likelihoods or sample quality, indicating that the primal is “approximately” solved.
Intuitively when f(θ, φ) is lower bounded, and when the model families {pθ} and {qφ} are highly
flexible, we may achieve both near-optimality for the primal objective f(θ, φ), and near-satisfaction
of the feasibility constraints. Therefore it might be acceptable to use a single value of λ, as long as
λ > 0. On the other hand, iff(θ, φ) is not lower bounded, regardless of model capacity, for any fixed
Lagrangian parameter λ, the Lagrangian can be minimized over θ, φ by only driving down f (θ, φ)
without lowering D(p(x, z)||q(x, z)). It is even possible that D(p(x, z)||q(x, z)) may increase if
minimizing f(θ, φ) is in conflict with minimizing D(p(x, z)||q(x, z)).
We verify this intuition experimentally for a simple scenario of a mixture of two Gaussians, where
it is clear that a reasonably large neural network is flexible enough to fit the distributions.6
q(x)〜1/2N(-1,1/4) + 1/2N(1, 1/4)
We use the β-VAE model class (Higgins et al., 2016). As discussed in Section 2 and Section 3, this
is a Lagrangian dual to the following optimization problem
min	(β - 1)Iq (x; z)
s	.t.	DKL (q(z)kp(z)) = 0,	DKL(q(x|z)kp(x|z)) = 0
When β < 1 the optimization objective (β - 1)Iq(x; z) is unbounded below for continuous distri-
butions. So optimizing the Lagrangian dual with fixed Lagrangian multipliers may be problematic.
Figure 1 shows the mutual information and negative log likelihood during training. It can be ob-
served that when β < 1 mutual information grows quickly initially (left panel), while at the same
6Experimental setting provided in Appendix.
9
Under review as a conference paper at ICLR 2018
time the log- likelihood (right panel) actually gets worse. However, as soon as mutual information
stops growing, either due to finite model capacity (solid line), or when we explicitly upper bound it
(dashed line), log-likelihood of the data starts to improve (around iteration 20000 for dashed lines,
and around iteration 40000 for solid lines). In the latter case, true log-likelihood actually becomes
as good as the one for β = 1, which is the original ELBO. However, choosing β < 1 results in
significantly larger mutual information Iq (x; z), which might be desirable (Chen et al., 2016b; Zhao
et al., 2017).
Figure 1: Estimated mutual information (Left) vs. true negative log likelihood (Right). Dashed lines
correspond to the same objective but with an upper bound on mutual information term. When mutual
information increases, log likelihood gets worse. As soon as mutual information stops increasing,
log likelihood starts to improve.
This example illustrates what we believe is a general pattern. With highly flexible {pθ } and {qφ}, a
fixed (suboptimal) λ can generally be used if the primal objective has an achievable lower bound. As
soon as the model approaches this lower bound, it will focus on satisfying the consistency conditions.
4.2 Trade-off for Infeasible Optimization Problems
All objectives of the Info-Autoencoding family are Lagrangian duals of the same primal problem.
However, different choices for λ may lead to very different behaviors when optimality cannot be
achieved, either due to limited model capacity or optimization difficulties. The Lagrangian param-
eters λ give the cost of violating each constraint, so that when not all constraints can be satisfied,
the model will prioritize constraints with larger Lagrangian coefficients. We use this perspective to
analyze the benefit of the additional flexibility gained by giving up likelihood based computability.
For example, we consider the consistency condition D(q(z)||p(z)) = 0, D(q(x|z)||p(x|z)) = 0.
There is an asymmetry between the input space X and the latent feature space Z . X is usually
high dimensional, while Z is low dimensional. Therefore any mistake fitting X is likely to be
magnified. For example, consider fitting an n dimensional distribution N (0, I) with N(, I) using
KL divergence:
DKL(N(0,I),N(,I))=n2/2
As n increases with some fixed e, the Euclidean distance between the mean of the two distributions
is Θ(√n), yet the corresponding DKL becomes Θ(n). For natural images, the dimensionality of X
is often orders of magnitude larger than the dimensionality of Z . This means that the model will
tend to sacrifice consistency between p(z) and q(z) in order to fit q(x|z) and p(x|z). We may wish
to calibrate this with a larger multiplier for the D(q(z)kp(z)) = 0 constraints. We show that doing
so improves performance metrics such as log likelihood, but it is only feasible with likelihood free
optimization.
We consider the following two classes of models written in Lagrangian dual form:
Lβ-ELBO = (β - 1)Iq(x; z) + βDKL(q(z)kp(z)) + DKL(q(x|z)kp(x|z))
LInfoVAE = Lβ-ELBO + λD(q(z)kp(z))
We use a slightly different but equivalent definition from Theorem 2 to highlight the fact that In-
foVAE has an additional free parameter λ. Note that the additional degree of freedom is not pos-
sible for β-ELBO as a result of the likelihood based requirement; if we increase the weighting for
10
Under review as a conference paper at ICLR 2018
DKL(q(z)kp(z)) we must penalize the mutual information term by the same amount to maintain
likelihood-based tractability.
We compute an estimate of the true log likelihood over the binarized MNIST dataset, where the
objectives are Lβ-ELBO and LInfoVAE respectively. Here Lβ-ELBO only depends on β, whereas
LInfoVAE is allowed to select the λ parameter, allowing for an additional degree of freedom 7. We
can choose λ > 0 which encourages q(z) and p(z) to be close. Interestingly, even for a very large
λ = 5000 the InfoVAE objective is still able to perform reasonably well, which is due to the fact that
z has a much smaller dimension than x (50 compared to 784 dimensions). As shown in Figure 2,
the additional degree of freedom provided by λ is able to result in better density estimation in both
training and test sets, even when β takes suboptimal values indicating either too much or too little
mutual information between x and z. This is surprising given that for λ > 0 or β < 1, we are no
longer optimizing over a lower bound on the log-likelihood directly.
Train	Test
115
♦ InfoVAE, Λ = 50
-⅜- InfOVAE,∙λ = 500
InfoVAE, ： = 200。
-φ- InfoVAE1A = 5000
♦ 6-ELBO
5	6
Figure 2: Estimated negative log likelihood under different β and λ values (lower is better). For
both training set (left) and test set (right) the extra degree of freedom from λ improves performance
(the curresponding curves are below the yellow one, for all values of β).
5	Conclusion
In this paper we explored a large family of learning objectives and models that generalize ELBO,
β-ELBO, BiGAN/ALI/ALICE, InfoGAN, AVB, CycleGAN/DiscoGAN, AAE/InfoVAE. The La-
grangian form reveals several important properties of the model, while the corresponding ”tractable”
form tells us how we may optimize it. Under some assumptions, we were able to identify the most
general class of objectives that can be optimized using likelihood-based and likelihood-free methods.
Future work includes exploring additional model classes, generalizing the analysis beyond two
groups of variables, and extending the tractability analysis beyond KL divergence and elementary
transformations.
Since the Information-Autoencoding Family introduces a wide range of valid objective configura-
tions through α1, α2, λ parameters, it is also interesting to see scalable methods to optimize/search
for the most effective set of parameters. Bayesian optimization methods Snoek et al. (2012) and re-
cent advances in neural architecture search (Zoph and Le, 2016; Brock et al., 2017) might be useful
for this purpose.
References
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. ArXiv e-prints, January 2017.
David Barber and Felix Agakov. The im algorithm: a variational approach to information maxi-
mization. In Proceedings of the 16th International Conference on Neural Information Processing
Systems, pages 201-208. MIT Press, 2003.
7We include the exact training procedure in the Appendix.
11
Under review as a conference paper at ICLR 2018
Andrew Brock, Theodore Lim, JM Ritchie, and Nick Weston. Smash: One-shot model architecture
search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Info-
gan: Interpretable representation learning by information maximizing generative adversarial nets.
arXiv preprint arXiv:1606.03657, 2016a.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016b.
Jeff Donahue, PhiliPP Krahenbuhl, and Trevor Darrell. Adversarial feature learning. CoRR,
abs/1605.09782, 2016. URL http://arxiv.org/abs/1605.09782.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier MastroPi-
etro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016a.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier MastroPi-
etro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pages 2672-2680, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A ker-
nel method for the two-samPle-Problem. In Advances in neural information processing systems,
pages 513-520, 2007.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. CoRR, abs/1703.05192, 2017. URL
http://arxiv.org/abs/1703.05192.
D. P Kingma and M. Welling. Auto-Encoding Variational Bayes. ArXiv e-prints, December 2013.
Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, and Lawrence
Carin. Towards understanding adversarial learning for joint distribution matching. 2017a.
Yingzhen Li and Richard E Turner. Renyi divergence variational inference. In Advances in Neural
Information Processing Systems, pages 1073-1081, 2016.
Yujia Li, Kevin Swersky, and Richard Zemel. Generative moment matching networks. In Interna-
tional Conference on Machine Learning, pages 1718-1727, 2015.
Yunzhu Li, Jiaming Song, and Stefano Ermon. Inferring the latent structure of human decision-
making from raw visual inputs. 2017b.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. arXiv preprint arXiv:1608.04471, 2016.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
12
Under review as a conference paper at ICLR 2018
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.
arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722,
2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
preprint arXiv:1402.0030, 2014.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pages 271-279, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
D. Rezende, S. Mohamed, and D. Wierstra. Stochastic Backpropagation and Approximate Inference
in Deep Generative Models. ArXiv e-prints, January 2014.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using
stochastic computation graphs. In Advances in Neural Information Processing Systems, pages
3528-3536, 2015.
Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theoretical Computer Science, 411(29-30):2696-2711, 2010.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951-2959,
2012.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. CoRR,
abs/1503.02406, 2015. URL http://arxiv.org/abs/1503.02406.
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse. On the quantitative analysis
of decoder-based generative models. http://openreview.net/pdf?id=B1M8JF9xx,
2016.
Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. CoRR, abs/1702.08139, 2017. URL
http://arxiv.org/abs/1702.08139.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational au-
toencoders. CoRR, abs/1706.02262, 2017. URL http://arxiv.org/abs/1706.02262.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image trans-
lation using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017. URL http:
//arxiv.org/abs/1703.10593.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
13
Under review as a conference paper at ICLR 2018
A Examples of Under-Determination of Consistency Condition
Example 1:	Let X and Z be discrete sets with n distinct elements. Thenpθ(x|z), qφ(z∣χ) are tables
with n(n- 1) degrees of freedom each, giving a total of 2n(n - 1) free parameters. The requirement
Pθ(x) = Pdata(X) for all X ∈ X involves n - 1 linear constraints, while qφ(z∣x) = pθ(z|x) for all X
adds additional n(n - 1) linear constraints. Therefore, there are at least
2n(n - 1) - (n + 1)(n - 1) = (n - 1)2
free parameters satisfying consistency constraints. Therefore for all n ≥ 2, there are multiple
consistent solutions.
Example 2:	If for any z, pθ(x|z) is itself a sufficiently flexible distribution family (e.g. represented
by PixelCNN), then multiple solutions may exists. For example, one solution could be
Pθ(x|z) = q(x), ∀z ∈ SuPPort(q(x)) qφ(z∣x) = p(z), ∀x ∈ SuPPort(P(Z))
which means that input X and latent code z are completely independent. This may be an undesirable
solution, and several models have been proposed to avoid this (Chen et al., 2016b; Zhao et al., 2017).
B Proofs
Proof of Corollary 1. Assume they are consistent. Then for any X such that Pdata (X) 6= 0 and for
any z,
qφ(χ,z) = Pdata(χ)qφ (z|x) = pθ (χ)qφ(z∣χ) = pθ (x)pθ (z|x)
For any X such that Pdata (X) = 0 and for any z,
qφ(x,z) = Pdata(X)qφ(z∣x) = 0
and because 0 = Pdata(X) = Pθ(X) = Pz Pθ (X, z), it follows that P(X, z) = 0 for all z.
Assuming qφ(X, z) = Pθ (X, z) we have that
Pθ(X) =	Pθ(X, z) =	qφ(X, z) = Pdata(X)
zz
Thus, the conditionals need to match when they are defined, i.e., when Pdata(X) = pθ(x) = 0.	□
Proof of Theorem 1 2 3. If all divergences are KL or reverse KL, then we can enumerate over every
possible mutual information or divergence term in Definition 5.
Ip(X; z)
DKL(P(X, z)||q(X, z))
DKL(P(X)||q(X))
DKL (P(z)||q(z))
Ep[DKL(P(z|X)||q(z|X))]
Ep[DKL(P(X|z)||q(X|z))]
Iq(X; z)
DKL (q(X, z)||P(X, z))
DKL(q(X)||P(X))
DKL (q(z)||P(z))
Eq[DKL(q(z|X)||P(z|X))]
Eq[DKL(q(X|z)||P(X|z))]
(16)
Note that we do not consider the terms
Eq[DKL(P(z|X)||q(z|X))]
Eq[DKL(P(X|z)||q(X|z))]
Ep[DKL(q(z|X)||P(z|X))]
Ep[DKL(q(X|z)||P(X|z))]
(17)
because these terms are not in any tractability toolbox in Definition 7, nor can any elementary
transformation be applied to them. So any KL Info-Autoencoding objective that contain any of
these terms do not have a equivalent computable form. We only consider the 12 possibilities in (16).
To prove this theorem we first map all optimization objectives into a vector space V ⊆
(Θ × Φ → R) with standard function addition and scalar multiplication as operations. Here Θ is
the domain for θ, Φ is the domain for φ, and f : Θ × Φ → R represents the objective function.
14
Under review as a conference paper at ICLR 2018
Define a set of basic terms
( logp(x,z) ∖
log p(x∣z)
Patoms =	log P(ZIx)
log p(x)
logp(z)
log q(x, z)
log q(x∣z)
qatoms =	log q(ZIx)
log q(x)
log q(Z)
Define the following set of basis vectors with 20 objective functions
E
(Ep(χ,z) [patoms]
Ep(x,z) [qatoms]
Eq(x,z) [patoms]
Eq(x,z) [qatoms]
It is easy to verify that all objectives of the KL Information Autoencoding family in (16), tractable
terms in Definition 7 and transformations defined Definition 8 can be written as linear combinations
of this set of basis, so they belong to the vector space spanned by these basis vectors. Therefore any
optimization objective L in our family can be represented as a vector n ∈ R20, where
L = En
For any m ∈ N, the column space of any matrix N ∈ Rn×m can be used to represent a family of
objectives. Each column ni of N corresponds to an objective Eni, and the family is defined by any
linear combination of {Enι,…,Enm,} Therefore We have embedded optimization objectives and
linear model families into vectors and column spaces of matrices in this vector space. Now we write
each objective and transformation We have discussed as vectors and matrices in this space.
1)	KL Info-Autoencoding family.
Let R ∈ R20×12 be
/ 0	1	0	0	0	0
0	0	1	0	0	0
10	0	10	0
0	0	0	0	1	0
-1	0	0	0	0	1
0 -10000
0 0 -1000
0 0 0 -1 0 0
0000 -1 0
00000 -1
∖
0
0
∖
0	1	0	0	0	0
0	0	1	0	0	0
0	0	0	1	0	0
0	0	0	0	1	0
0	0	0	0	0	1
0 -10000
0	0	-1	0	0	0
1	0	0 -10	0
0	0	0	0	-1	0
-1	0	0	0	0	-1
The column space of this matrix form a basis Where each column is one of the tWelve terms defined in
Eq.(16). Denote the subspace spanned by the columns as R. This subspace represents all objectives
that are a linear combination of Eq.(16), or equivalently, every possible objective in Lagrangian dual
form in Definition 5 Where each divergence is either KL or reverse KL and does not include terms
in (17).
2)	Equivalent transformations. Note that all elementary transformations are log linear. For ex-
ample, replacing Ep [logp(x, Z)] With Ep[logp(xIZ)] + Ep[logp(Z)] leaves any objective equivalent.
This means that for any b ∈ R
n = n + b(1,-1,0, 0,-1,0,…，0)t
15
Under review as a conference paper at ICLR 2018
Results in
En ≡ En0
Enumerating over all 10 equivalent transformations we considered in Definition 8, we define the
matrix P ∈ R20×10 as
/ 1	1 0
-10 0
0 -10
0 -1 0
-1 0 1
1	1
-1 0
0 -1
0 -1
-1 0
∖
0
0
∖
1	1
-1 0
0 -1
0 -1
-1 0
1	1 0
-10 0
0 -1 0
0 -1 1
-1 0 0
It is easy to see that for each column c of P, n + c corresponds to performing one of the elementary
transformations in Definition 8. Any sequence of elementary transformations can be written as
n + Pb for some vector b.
3)	Tractable Families. Tractable families are given by Definition 7. We find the corresponding
basis matrix T and subspace T for each tractability assumption in the definition. Then any tractable
objective under elementary transformation lie in the subspace T + P . + is the sum of vector spaces
T and P, or space of all vectors that can be written as the linear combination of vectors in T and
vectors in P .
For likelihood based computable objectives, if is easy to see that the eight likelihood based com-
putable terms in Definition 7 can be written as the columns of Tlb defined as
/ 1 0 0 0\
0100
0 0 0 0
0 0 0 0
TP _ Tq _	0010
lb = lb =	0 0 0 0
0 0 0 0
0 0 0 1
0000
0000
Tlb
Tlpb 0
0 Tlqb
denote the corresponding subspace as Tlb .
For unary likelihood free computable objectives, aside from all likelihood based computable objec-
tives, we also add the four divergences
DKL(p(x)||q(x)), DKL(q(x)||p(x)), DKL(p(z)||q(z)), DKL(q(z)||p(z))
16
Under review as a conference paper at ICLR 2018
similarly we have
/ 0 0 ∖
0	0
0	0
-1 0
Tib
0 -1
00
Tulf
00
0	0
10
01
All unary likelihood free computable objectives are columns of TUif, denote the corresponding sub-
space as TUif.
For binary likelihood free computable objectives, aside from all unary likelihood free computable
objectives, we also add the two divergences
DKL(P(X, z)∣∣q(x, z)), DKL(q(x, z)∖∖p(χ,z)')
similarly we have
1
0
0
0
T 0
T ulf 1
一1
0
0
0
0
All binary likelihood free computable objectives are columns of Tbif, denote the corresponding
subspace as Tbif.
4)	Known Objective Families. We write out the subspace spanned by the two likelihood based
computable family β-ELBO and Variational mutual information maximization
0
0
0
(SVMI Se-ELBO )
0
1
0
0
0
-1		
0 0	0	0
	-1	0
	0	0
	0	0
	0	-1
0	0	0
	0	-1
	0	0
	1	1
	0	1
Denote the corresponding column space as SVMI + Se-ELBo.
17
Under review as a conference paper at ICLR 2018
For subspace spanned by InfoGAN and InfoVAE we have ( SInfoGAN SInfoVAE ) as
0	0	0	∖
-1	0	0	
1	0	0	
1	1	-1	
0	0	0	
	0		0
0	0	0	
0	0	0	
-1	0	0	
0	-1	1	
0	0	0	
	0	0	0	0
	0	0	-1 0
	0	0	0	0
	0	0	0	0
	1	0	0 -1
0		0	
	0	0	0	0
	0	0	1	0
	0	1	0	0
	0	0	00
∖	-1	-1	01
Denote the corresponding column space as SInfoGAN + SInfoVAE.
For subspace spanned by InfoBiGAN and InfoVAE we have ( SInfoBiGAN SInfoVAE ) as
0	0	0	0	
1	0	0	0	
0	1	0	0	
-1	0	1	-1	
0	0	0	0	
			0	0
0	0	0	0	
0	0	0	0	
0	-1	0	0	
0	0	-1	1	
0	0	0	0	
			0	000
			0	0 -1 0
			0	000
			0	000
			1	0	0 -1
	0		0	
			0	000
			0	010
			0	100
			0	000
卜			-1	-1 0	1
Denote the corresponding column space as SInfoBiGAN + SInfoVAE .
5)	Subspace Equivalence Relationship.
As before denote A + B as the sum of two subspaces A and B, or all vectors that are linear combi-
nations of vectors in A and vectors in B. Consider first the Lβ-ELBO family and LVMI family.
LVMI = -λιIp(x; z) + λιDκL(p(z∣x)kq(z∣x))	(18)
= λ1Ep[log q(z|x)]	(19)
Le-ELBO = (λ2 - λι)Iq(x,z) + λιDkl(q(x|z)∣∣p(x|z)) + λ2DκL(q(z)∣∣p(z))	(20)
=-λιEq [log p(x∣z)] + λ2DκL(q(z∣x)kp(z))	(21)
Notice that any member of the Lβ-ELBO family and LVMI family can be transformed by elementary
transformation to a Lagrangian dual form Eq.(18)(20), or to a tractable form that only contains
18
Under review as a conference paper at ICLR 2018
terms in Tlb in Eq.(19)21). This means that for any such member s, there is a r in the space of all
Lagrangian dual forms R, and a p in space of all transformations P so that s = r + p. Therefore
Sβ-ELBO + SVMI ⊂ R + P
Similarly, for any such member s, there is a t ∈ Tlb, and p ∈ P so that s = t + p. so
Sβ-ELBO + SVMI ⊂ Tlb + P
Therefore
Sβ-ELBO + SVMI ⊂ (R + P) ∩ (Tlb + P)
In addition we have
P ⊂ (R+P) ∩ (Tlb +P)
so
Sβ-ELBO + SVMI + P ⊂ (R + P) ∩ (Tlb + P)	(22)
Therefore if we can further have
dim(Sβ-ELBO + SVMI + P) = dim ((R + P) ∩ (Tlb + P))
the above subspaces must be equivalent. This is because if the two spaces are not equivalent, and
there exists vector v ∈ (R + P) ∩ (Tlb + P) but v 6∈ Sβ-ELBO + SVMI + P. However
Sβ-ELBO + SVMI + P + {v} ⊂ (R + P) ∩ (Tlb + P)
But
dim (Sβ-ELBO + SVMI + P + {v}) >
dim (Sβ-ELBO + SVMI +P) = dim((R+P) ∩ (Tlb +P))
leading to a contradiction.
Similarly we have for the other families
SInfoGAN + SInfoVAE + P ⊂ (R+P)∩(Tulf+P)	(23)
SInfoBiGAN + SInfoVAE + P ⊂ (R + P) ∩ (Tblf + P)	(24)
To compute the above dimensions, we compute a basis for the each space under discussion. The
rank of basis matrix is equal to the dimensionality of its column space. Let A, B be the column
space of matrices A, B, then the basis matrix of A + B can be derived by ( A B ). the basis
matrix of A ∩ B can be derived by the following process: Compute the null space for
( A B )	uv	= 0
Let U, V be basis matrix for the null space of u, v respectively. Then AU = -BV is a basis matrix
for A ∩ B.
We can perform this procedure for all the above mentioned subspaces, and derive the following
table.
subspace
(Se-ELBO + SVMI) + P
(Tlb + P) ∩ (R + P)
(SInfoGAN ∪ SInfoVAE ) + P
(TUlf + P) ∩(R + P)
(SInfoBiGAN ∪ SInfoVAE ) + P
(Tblf+P)∩(R+P)
dimension
13
13
17
17
18
18
19
Under review as a conference paper at ICLR 2018
Therefore we have verified that dimensionality match for Eq. (22,	23, 24). Therefore we have
Sβ-ELBO + SVMI + P	= (R + P)	∩	(Tlb + P)
SInfoGAN + SInfoVAE + P	= (R + P)	∩	(Tulf + P)
SInfoBiGAN + SInfoVAE + P	= (R + P)	∩	(Tblf + P)
This implies that for any objective r ∈ R of the Info-Autoencoding family, if r can be converted
into likelihood based tractable form by elementary transformations, or r ∈ Tlb + P, then
r ∈R∩(Tlb+P) ⊂ (R+P)∩(Tlb+P)
This means
r ∈ Sβ-ELBO + SVMI + P
which implies that r can be converted by elementary transformation into a linear combination of
elements in Sβ-ELBO and elements in SVMI. We can derive identical conclusion for the other two
objective families.	□
C Experimental Setup
C.1 Experimental Setup for Section 4.1
The latent space z is 1-dimensional, and both p(x|z) and q(z|x) are 1-dimensional Gaussians with
arbitrary mean and variance. p(z) is the standard Gaussian, and the mapping z → p(x|z) and
x → q(z|x) are both fully connected neural networks with two hidden layers each with 1024 units.
To estimate mutual information we can use -Eq [logp(x|z)] because
-Eq[logp(x|z)] = Iq(x; z) - DKL(q(x|z)kp(x|z))
so we get a upper bound on the mutual information Iq(x; z) that is tight if p(x|z) matches q(x|z)
well. We also upper bound -Eq[logp(x|z)] to place a explicit upper bound on the mutual informa-
tion. To compute true log likelihood for this toy problem it is sufficient to estimate the integration
by sampling
logp(x) = logEp(z)[p(x|z)] ≈ log Nn X	p(x∣zi)
i=1,…，N,Zi〜p(z)
C.2 Experimental Setup for Section 4.2
We consider p(x|z) with one layer of latent code, and its corresponding inference network q(z|x).
Both p and q have two hidden layers with 1024 neurons each and tanh activations; the dimension
of the latent variable z is 50. We obtain a binarized MNIST dataset with 50000 training examples
and 10000 test examples by sampling each dimension from a Bernoulli distribution with probability
given by the MNIST dataset. We use importance sampling (instead of AIS (Wu et al., 2016))
logp(x) = log Ep(z∣χjpχ4] ≈ logɪ X	P(χzi)
Iq(ZIx)]	N i=1,…，N,Zi~q(z∣x) q(ZiIx)
to evaluate the negative log-likelihood. Hence the results reported are slightly worse than the state of
the art. This does not affect our arguments though, since we are comparing the relative performance
of β-ELBO and InfoVAE.
20