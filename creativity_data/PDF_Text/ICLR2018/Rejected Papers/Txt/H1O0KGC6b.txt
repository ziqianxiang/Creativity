Under review as a conference paper at ICLR 2018
Post Training in Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
One of the main challenges of deep learning methods is the choice of an appro-
priate training strategy. In particular, additional steps, such as unsupervised pre-
training, have been shown to greatly improve the performances of deep structures.
In this article, we propose an extra training step, called post-training, which only
optimizes the last layer of the network. We show that this procedure can be ana-
lyzed in the context of kernel theory, with the first layers computing an embedding
of the data and the last layer a statistical model to solve the task based on this em-
bedding. This step makes sure that the embedding, or representation, of the data
is used in the best possible way for the considered task. This idea is then tested on
multiple architectures with various data sets, showing that it consistently provides
a boost in performance.
1	Training Neural Networks
One of the main challenges of the deep learning methods is to efficiently solve the highly complex
and non-convex optimization problem involved in the training step. Many parameters influence the
performances of trained networks, and small mistakes can drive the algorithm into a sub-optimal
local minimum, resulting into poor performances (Bengio & LeCun, 2007). Consequently, the
choice of an appropriate training strategy is critical to the usage of deep learning models.
The most common approach to train deep networks is to use the stochastic gradient descent (SGD)
algorithm. This method selects a few points in the training set, called a batch, and compute the
gradient of a cost function relatively to all the layers parameter. The gradient is then used to update
the weights of all layers. Empirically, this method converges most of the time to a local minimum
of the cost function which have good generalization properties. The stochastic updates estimate the
gradient of the error on the input distribution, and several works proposed to use variance reduction
technique such as Adagrap (Duchi et al., 2011), RMSprop (Hinton et al., 2012) or Adam (Kingma
& Ba, 2015), to achieve faster convergence.
While these algorithms converge to a local minima, this minima is often influenced by the properties
of the initialization used for the network weights. A frequently used approach to find a good starting
point is to use pre-training (Larochelle et al., 2007; Hinton et al., 2006; Hinton & Salakhutdinov,
2006). This method iteratively constructs each layer using unsupervised learning to capture the
information from the data. The network is then fine-tuned using SGD to solve the task at hand. Pre-
training strategies have been applied successfully to many applications, such as classification tasks
(Bengio & LeCun, 2007; Poultney et al., 2006), regression (Hinton & Salakhutdinov, 2008), robotics
(Hadsell et al., 2008) or information retrieval (Salakhutdinov & Hinton, 2009). The influence of
different pre-training strategies over the different layers has been thoroughly studied in Larochelle
et al. (2009). In addition to improving the training strategies, these works also shed light onto the
role of the different layers (Erhan et al., 2010; Montavon et al., 2011). The first layers of a deep
neural network, qualified as general, tend to learn feature extractors which can be reused in other
architectures, independently of the solved task. Meanwhile, the last layers of the network are much
more dependent of the task and data set, and are said to be specific.
Deep Learning generally achieves better results than shallow structures, but the later are generally
easier to train and more stable. For convex models such as logistic regression, the training problem
is also convex when the data representation is fixed. The separation between the representation and
the model learning is a key ingredient for the model stability. When the representation is learned
simultaneously, for instance with dictionary learning or with EM algorithms, the problem often
1
Under review as a conference paper at ICLR 2018
Features ΦL-1 (x)	Weights WL
Figure 1: Illustration of post-training applied to a neural network. During the post-training, only the
weights of the blue edges are updated. The blue nodes can be seen as the embedding of x in the
feature space XL .
become non-convex. But this coupling between the representation and the model is critical for
end-to-end models. For instance, Hinton et al. (2006) showed that for networks trained using pre-
training, the fine-tuning step - where all the layers are trained together - improves the performances
of the network. This shows the importance of the adaptation of the representation to the task in
end-to-end models.
Our contribution in this chapter is an additional training step which improves the use of the repres-
entation learned by the network to solve the considered task. This new step is called post-training.
It is based on the idea of separating representation learning and statistical analysis and it should be
used after the training of the network. In this step, only the specific layers are trained. Since the
general layers - which encode the data representation - are fixed, this step focuses on finding the
best usage of the learned representation to solve the desired task. In particular, we chose to study
the case where only the last layer is trained during the post-training, as this layer is the most spe-
cific one (Yosinski et al., 2014). In this setting, learning the weights of the last layer corresponds
to learning the weights for the kernel associated to the feature map given by the previous layers.
The post-training scheme can thus be interpreted in light of different results from kernel theory. To
summarize our contributions:
•	We introduce a post-training step, where all layers except the last one are frozen. This
method can be applied after any traditional training scheme for deep networks. Note that
this step does not replace the end-to-end training, which co-adapts the last layer represent-
ation with the solver weights, but it makes sure that this representation is used in the most
efficient way for the given task.
•	We show that this post-training step is easy to use, that it can be effortlessly added to most
learning strategies, and that it is computationally inexpensive.
•	We highlight the link existing between this method and the kernel techniques. We also
show numerically that the previous layers can be used as a kernel map when the problem
is small enough.
•	We experimentally show that the post-training does not overfit and often produces improve-
ment for various architectures and data sets.
The rest of this article is organized as follows: Section 2 introduces the post-training step and dis-
cusses its relation with kernel methods. Section 4 presents our numerical experiments with multiple
neural network architectures and data sets and Section 5 discusses these results.
2	Post-training
In this section, we consider a feedforward neural network with L layers, where X1 , . . . , XL denote
the input space of the different layers, typically Rdl with dl > 0 and Y = XL+1 the output space of
our network. Let φl : Xl 7→ Xl+1 be the applications which respectively compute the output of the
l-th layer of the network, for 1 ≤ l ≤ L, using the output of the l-1-th layer and Φl = Φl ◦•••◦ φι
be the mapping of the full network from X1 to Y . Also, for each layer l, we denote Wl its weights
matrix and ψl its activation function.
2
Under review as a conference paper at ICLR 2018
The training of our network is done using a convex and continuous loss function ` : Y × Y 7→ R+ .
The objective of the neural network training is to find weights parametrizing ΦL that solves the
following problem:
minE(x,y)〜P ' (Φl(x),y)	.	(1)
for a certain input distribution P in (Xi, Y). The training set is D = (xi, y，N=i, drawn from this
input distribution.
Using these notations, the training objective (1) can then be rewritten
φLm1n L E(X,y)~P
(2)
This reformulation highlights the special role of the last layer in our network compared to the others.
When ΦL-1 is fixed, the problem of finding WL is simple for several popular choices of activation
function ψL and loss ` . For instance, when the activation function ψL is the softmax function and
the loss ` is the cross entropy, (2) is a multinomial logistic regression. In this case, training
the last layer is equivalent to a regression of the labels y using the embedding of the data x in XL by
the mapping ΦL-1 . Since the problem is convex in WL (see Appendix A), classical optimization
techniques can efficiently produce an accurate approximation of the optimal weights WL - and this
optimization given the mapping ΦL-1 is the idea behind post-training.
Indeed, during the regular training, the network tries to simultaneously learn suitable representation
for the data in the space XL through its L - 1 first layer and the best use of this representation with
WL. This joint minimization is a strongly non-convex problem, therefore resulting in a potentially
sub-optimal usage of the learned data representation.
The post-training is an additional step of learning which takes place after the regular training and
proceeds as follows :
1.	Regular training: This step aims to obtain interesting features to solve the initial problem,
as in any usual deep learning training. Any training strategy can be applied to the network,
optimizing the empirical loss
argmi
ΦL
1N
n N X' (φL(χi),y).
N i=1
(3)
The stochastic gradient descent explores the parameter space and provides a solution for
ΦL-1 and WL . This step is non restrictive: any type of training strategy can be used
here, including gradient bias reduction techniques, such as Adagrad (Duchi et al., 2011), or
regularization strategies, for instance using Dropout (Dahl et al., 2013). Similarly, any type
of stopping criterion can be used here. The training might last for a fixed number of epochs,
or can stop after using early stopping (Morgan & Bourlard, 1990). Different combinations
of training strategies and stopping criterion are tested in Section 4.
2.	Post-training: During this step, the first L - 1 layers are fixed and only the last layer of
the network, φL , is trained by minimizing over WL the following problem
1N
argmin — X1 (Φl-i(xJW,L,y) + λ∣∣Wl∣∣2 ,
WL N i=1
(4)
where '(χ, y) := '(Ψl(x), y) . ThiS extra learning step uses the mapping Φl-i as an em-
bedding of the data in XL and learn the best linear predictor in this space. This optimization
problem takes place in a significantly lower dimensional space and since there is no need
for back propagation, this step is computationally faster. To reduce the risk of overfitting
with this step, a '2-regularization is added. Figure 1 illustrates the post-training step.
We would like to emphasize the importance of the '2-regularization used during the post-training
(4). This regularization is added regardless of the one used in the regular training, and for all the
network architectures. The extra term improves the strong convexity of the minimization problem,
3
Under review as a conference paper at ICLR 2018
making post-training more efficient, and promotes the generalization of the model. The choice of the
'2 -regularization is motivated from the comparison with the kernel framework discussed in Section 3
and from our experimental results.
Remark 1 (Dropout.). It is important to note that Dropout should not be applied on the previous
layers of the network during the post-training, as it would lead to changes in the feature function
ΦL-1.
3	Link with Kernels
In this section, We show that for the case where XL = RdL for some dL > 0 and Xl+i = R, WL
can be approximated using kernel methods. We define the kernel k as follows,
k : X1 × X1 7→ R
(x1,x2) → ΦL-1(x1),ΦL-1(x2)	.
Then k is the kernel associated with the feature function ΦL-1. It is easy to see that this kernel is
continuous positive definite and that for W ∈ RdL , the function
gW : X1 7→ XL+1
x → ΦL-1(x),W
(5)
belongs by construction to the Reproducing Kernel Hilbert Space (RKHS) Hk generated by k. The
post-training problem (4) is therefore related to the problem posed in the RKHS space Hk, defined
by
1N
g = argmin 而 V^ (9―迅)+ λ∣∣g∣∣Hk ,
g∈Hk N i=1	k
El ∙	1 1	∙	1	∙ Γ∙ , 1 1	1	, 1 1 -c-r τ∙ , 1	∙ 1 1 1	, 1	7l .l	1 ∙ 1
This problem is classic for the kernel methods. With mild hypothesis on `, the generalized repres-
enter theorem can be applied (SChOlkOPf et al., 2001). As a consequence, there exists α* ∈ RN such
that
1
g* ：= argmin 方	(g(xi),yj + λ∣∣gkHk
g∈Hk N i=1	k
NN
=X αtk(Xi, ∙) = X(可φL-1 (Xi) , φL-1
i=1	i=1
Rewriting (6) with g* of the form (5), we have that g* = gw *, with
N
W * = X α*ΦL-i (Xi) ’.
i=1
(6)
(7)
We emphasize that W * gives the optimal solution for the problem (6) and should not be confused
with W *L , the optimum of (4). However, the two problems differ only in their regularization, which
are closely related (see the next paragraph). Thus W * can thus be seen as an approximation of the
optimal value W*L . It is worth noting that in our experiments, W* appears to be a nearly optimal
estimator ofW*L (see Subsection 4.3).
Relation between k ∙ ∣∣h and k ∙ k2. The problems (6) and (4) only differ in the choice of the
regularization norm. By definition of the RKHS norm, we have
kgW kH = inf kv k2	∀X ∈ X1 ,
hv, ΦL-1(X)i = gW
Consequently, we have that kgW kH ≤ kW k2 , with equality when Vect(ΦL-1(X1)) spans the entire
space Xl. In this case, the norm induced by the RKHS is equal to the '2-norm. This is generally the
case, as the input space is usually in a far higher dimensional space than the embedding space, and
since the neural network structure generally enforces the independence of the features. Therefore,
while both norms can be used in (4), we chose to use the '2-norm for all our experiments as it is
easier to compute than the RKHS norm.
4
Under review as a conference paper at ICLR 2018
32
Figure 2: Illustration of the neural network structure used for CIFAR-10. The last layer, represented
in blue, is the one trained during the post-training. The layers are composed with classical layers:
5x5 convolutional layers (5x5 conv), max pooling activation (max pool), local response normaliza-
tion (lrn) and fully connected linear layers (fc).
Close-form Solution. In the particular case where '(y1,y2) = ∣∣yι - y2k2 and f (x) = x, (6) can
be reduced to a classical Kernel Ridge Regression problem. In this setting, W * can be computed by
combining (7) and
α*= ΦL-1(D)TΦL-1(D) + λIN -1 Y ,	(8)
where ΦL-1 (D)
ΦL-1(x1), . . .ΦL-1(xN)
represents the matrix of the input data x1, . . . xN
embedded in XL, Y is the matrix of the output data y1, . . . , yN and IN is the identity matrix in
RN . This result is experimentally illustrated in Subsection 4.3. Although data sets are generally
too large for (8) to be computed in practice, it is worth noting that some kernel methods, such as
Random Features (Rahimi & Recht, 2007), can be applied to compute approximations of the optimal
weights during the post-training.
Multidimensional Output. Most of the previously discussed results related to kernel theory hold
for multidimensional output spaces, i.e. dim(XL+1) = d > 1, using multitask or operator valued
kernels (Kadri et al., 2015). Hence the previous remarks can be easily extended to multidimensional
outputs, encouraging the use of post-training in most settings.
4	Experimental Results
This section provides numerical arguments to study post-training and its influence on performances,
over different data sets and network architectures. All the experiments were run using python
and Tensorflow. The code to reproduce the figures is available online1. The results of all the
experiments are discussed in depth in Section 5.
4.1	Convolutional Neural Networks
The post-training method can be applied easily to feedforward convolutional neural network, used
to solve a wide class of real world problems. To assert its performance, we apply it to three classic
benchmark datsets: CIFAR10 (Krizhevsky, 2009), MNIST and FACES (Hinton & Salakhutdinov,
2006).
CIFAR10. This data set is composed of 60, 000 images 32 × 32, representing objects from 10
classes. We use the default architecture proposed by Tensorflow for CIFAR10 in our experi-
ments, based on the original architecture proposed by Krizhevsky (2009). It is composed of 5 layers
described in Figure 2. The first layers use various common tools such as local response normal-
ization (lrn), max pooling and RELU activation. The last layer have a softmax activation function
1The code is available at anonymous github repo
5
Under review as a conference paper at ICLR 2018
(-0e-tt03 6u'E-Sl
(L0，-0」」山ttωl
IO3	IO4
Iteration
Figure 3: Evolution of the performances of the neural network on the CIFAR10 data set, (dashed)
with the usual training and (solid) with the post-training phase. For the post-training, the value of
the curve at iteration q is the error for a network trained for q - 100 iterations with the regular
training strategy and then trained for 100 iterations with post-training. The top figure presents the
classification error on the training set and the bottom figure displays the loss cost on the test set. The
curves have been smoothed to increase readability.
and the chosen training loss was the cross entropy function. The network is trained for 90k itera-
tions, with batches of size 128, using stochastic gradient descent (SGD), dropout and an exponential
weight decay for the learning rate. Figure 3 presents the performance of the network on the training
and test sets for 2 different training strategies. The dashed line present the classic training with SGD,
with performance evaluated every 100 iterations and the solid line present the performance of the
same network where the last 100 iterations are done using post-training instead of regular training.
To be clearer, the value of this curve at iteration q is the error of the network, trained for q - 100
iterations with the regular training strategy, and then trained for 100 iterations with post-training.
The regularization parameter λ for post-training is set to 1 × 10-3.
The results show that while the training cost of the network mildly increases due to the use of
post-training, this extra step improves the generalization of the solution. The gain is smaller at the
end of the training as the network converges to a local minimum, but it is consistent. Also, it is
interesting to note that the post-training iterations are 4× faster than the classic iterations, due to
their inexpensiveness.
Additional Data Sets. We also evaluate post-training on the MNIST data set (65000 images 27 ×
27, with 55000 for train and 10000 for test; 10 classes) and the pre-processed FACES data set (400
images 64 × 64, from which 102400 sub-images, 32 × 32, are extracted, with 92160 for training
and 10240 for testing; 40 classes). For each data set, we train two different convolutional neural
networks - to assert the influence of the complexity of the network over post-training:
•	a small network, with one convolutional layer (5 × 5 patches, 32 channels), one 2 × 2 max
pooling layer, and one fully connected hidden layer with 512 neurons,
•	a large network, with one convolutional layer (5 × 5 patches, 32 channels), one 2 × 2 max
pooling layer, one convolutional layer (5 × 5 patches, 64 channels), one 2 × 2 max pooling
layer and one fully connected hidden layer with 1024 neurons.
6
Under review as a conference paper at ICLR 2018
Table 1: Comparison of the performances (classification error) of different networks on different
data sets, at different epochs, with or without post-training.
Data set	Network	Iterations	Mean (Std) Error in %	Mean (Std) Error with post-training in %
		5000	21,5 (10)	19,1 (12)
	Small	10000	20(4)	19 (3,5)
		20000	18 (0,9)	16,5 (0,8)
		5000	25 (15)	24 (15)
FACES	Large	10000	15 (5)	12(5)
		20000	11 (0,5)	10 (0,5)
		1000	10.7 (1)	9.2(1,1)
	Small	2000	7,5 (0,7)	6,7 (0,6)
MNIST		5000	4,1 (0,2)	3,9 (0,2)
		1000	9,1 (1,3)	8,5(1,4)
	Large	2000	4,1 (0,2)	3,5 (0,2)
		5000	1,1(0,01)	0,9 (0,01)
We use dropout for the regularization, and set λ = 1 × 10-2. We compare the performance gain
resulting of the application of post-training (100 iterations) at different epochs of each of these
networks. The results are reported in Table 1.
As seen in Table 1, post-training improves the test performance of the networks with as little as
100 iterations - which is negligible compared to the time required to train the network. While the
improvement varies depending on the complexity of the network, of the data set, and of the time
spent training the network, it is important to remark that it always provides an improvement.
4.2	Recurrent Neural Network
While the kernel framework developed in Section 2 does not apply directly to Recurrent Neural
Network, the idea of post-training can still be applied. In this experiment, we test the performances
of post-training on Long Short-Term Memory-based networks (LSTM), using PTB data set (Marcus
et al., 1993).
Penn Tree Bank (PTB). This data set is composed of 929k training words and 82k test word, with
a 10000 words vocabulary. We train a recurrent neural network to predict the next word given the
word history. We use the architecture proposed by Zaremba et al. (2014), composed of 2 layers of
1500 LSTM units with tanh activation, followed by a fully connected softmax layer. The network
is trained to minimize the average per-word perplexity for 100 epochs, with batches of size 20, using
gradient descent, an exponential weight decay for the learning rate, and dropout for regularization.
The performances of the network after each epoch are compared to the results obtained if the 100
last steps (i.e. 100 batches) are done using post-training. The regularization parameter for post-
training, λ, is set to 1 × 10-3. The results are reported in Figure 4, which presents the evolution of
the training and testing perplexity.
Similarly to the previous experiments, post-training improves the test performance of the networks,
even after the network has converged.
4.3	Optimal Last Layer for Deep Ridge Regression
In this subsection we aim to empirically evaluate the close-form solution discussed in Section 2
for regression tasks. We set the activation function of the last layer to be the identity fL (x) = x,
and consider the loss function to be the least-squared error '(x, y) = ∣∣x - y∣∣2 in (1). In in each
experiment, (8) and (7) are used to compute W * for the kernel learned after the regular training of
7
Under review as a conference paper at ICLR 2018
2 10
Ooo
111
(0 寸 —)A4-ad u-l
IO1	IO2
Iterations
Figure 4: Evolution of the performances of the Recurrent network on the PTB data set. The top
figure presents the train perplexity and the bottom figure displays the test perplexity. For the post-
training, the value of the curve at iteration q is the error for a network trained for q - 100 iterations
with the regular training strategy and then trained for 100 iterations with post-training.
the neural network, which learn the embedding ΦL-1 and an estimate WL . In order to illustrate
this result, and to compare the performances of the weights W * with respect to the weights Wl,
learned either with usual learning strategies or with post-training, we train a neural network on two
regression problems using a real and a synthetic data set. 70% of the data are used for training, and
30% for testing.
Real Data Set Regression. For this experiment, we use the Parkinson Telemonitoring data set
(Tsanas et al., 2010). The input consists in 5, 875 instances of 17 dimensional data, and the output
are one dimensional real number. For this data set, a neural network made of two fully connected
hidden layers of size 17 and 10 with respectively tanh and RELU activation, is trained for 250,
500 and 750 iterations, with batches of size 50. The layer weights are all regularized with the `2 -
norm and a fixed regularization parameter λ = 10-3 . Then, starting from each of the trained
networks, 200 iterations of post-training are used with the same regularization parameter λ and the
performances are compared to the closed-form solutions computed using (8) for each saved network.
The results are presented in Table 2.
Simulated Data Set Regression. For this experiment, we use a synthetic data set. The inputs were
generated using a uniform distribution on 0, 1 10. The outputs are computed as follows:
Y = tanh(XW1)W2
where W1 ∈ -1, 110×5 and W2 ∈ -1, 15 are randomly generated using a uniform law. In total,
the data set is composed of 10, 000 pairs (xi, yj). For this data set, a neural network with two fully
connected hidden layers of size 10 with activation tanh for the first layer and RELU for the second
layer is trained for 250, 500 and 750 iterations, with batches of size 50. We use the same protocol
with 200 extra post-training iterations. The results are presented in Table 2.
For these two experiments, the post-training improves the performances toward these of the optimal
solution, for several choices of stopping times. Itis worth noting that the performance of the optimal
solution is better when the first layers are not fully optimized with Parkinson Telemonitoring data
set. This effect denotes an overfitting tendency with the full training, where the first layers become
overly specified for the training set.
8
Under review as a conference paper at ICLR 2018
Table 2: Comparison of the performances (RMSE) of fully connected networks on different data
sets, at different epochs, with or without post-training.
Data set	Iterations	Error with CIaSSiC training	Error with post-training	Error with OptimaI last layer
	-250-	0.832	0.434	0.119
Parkinson	500	0.147	0.147	0.140
	750	0.134	0.132	0.131
	-250-	1.185	1.117	Γ075
Simulated	500	0.533	0.450	0.447
	750	0.322	0.300	0.296
5	Discussion
The experiments presented in Section 4 show that post-training improves the performances of all
the networks considered - including recurrent, convolutional and fully connected networks. The
gain is significant, regardless of the time at which the regular training is stopped and the post-
training is done. In both the CIFAR10 and the PTB experiment, the gap between the losses with
and without post-training is more pronounced if the training is stopped early, and tends to be smaller
as the network converges to a better solution (see Figure 4 and Figure 3). The reduction of the
gap between the test performances with and without post-training is made clear in Table 1. For the
MNIST data set, with a small-size convolutional neural network, while the error rate drops by 1.5%
when post-training is applied after 5000 iterations, this same error rate only drops by 0.2% when it
is applied after 20000 iterations. This same observation can be done for the other results reported in
Table 1. However, while the improvement is larger when the network did not fully converge prior
to the post-training, it is still significant when the network has reached its minimum: for example in
PTB the final test perplexity is 81.7 with post-training and 82.4 without; in CIFAR10 the errors are
respectively 0.147 and 0.154.
If the networks are allowed to moderately overfit, for instance by training them with regular al-
gorithm for a very large number of iterations, the advantage provided by post-training vanishes: for
example in PTB the test perplexity after 2000 iterations (instead of 400) is 83.2 regardless of post-
training. This is coherent with the intuition behind the post-training: after overfitting, the features
learned by the network become less appropriate to the general problem, therefore their optimal usage
obtained by post-training no longer provide an advantage.
It is important to note that the post-training computational cost is very low compared to the full
training computations. For instance, in the CIFAR10 experiment, each iteration for post-training
is 4× faster on the same GPU than an iteration using the full gradient. Also, in the different ex-
periments, post-training produces a performance gap after using as little as 100 batches. There are
multiple reasons behind this efficiency: first, the system reaches a local minimum relatively rapidly
for post-training as the problem (4) has a small number of parameters compared to the dimension-
ality of the original optimization problem. Second, the iterations used for the resolution of (4) are
computationally cheaper, as there is no need to chain high dimensional linear operations, contrarily
to regular backpropagation used during the training phase. Finally, since the post-training optimiza-
tion problem is generally convex, the optimization is guaranteed to converge rapidly to the optimal
weights for the last layer.
Another interesting point is that there is no evidence that the post-training step leads to overfitting.
In CIFAR10, the test error is improved by the use of post-training, although the training loss is
similar. The other experiments do not show signs of overfitting either as the test error is mostly
improved by our additional step. This stems from the fact that the post-training optimization is much
simpler than the original problem as it lies in a small-dimensional space - which, combined with
the added `2 -regularization, efficiently prevents overfitting. The regularization parameter λ plays an
important role in post-training. Setting λ to be very large reduces the explanatory capacity of the
9
Under review as a conference paper at ICLR 2018
networks whereas if λ is too small, the capacity can become too large and lead to overfitting. Overall,
our experiments highlighted that the post-training produces significant results for any choice of λ
reasonably small (i.e 10-5 ≤ λ ≤ 10-2 ). This parameter is linked to the regularization parameter
of the kernel methods, as stated in Section 3.
Overall, these results show that the post-training step can be applied to most trained networks,
without prerequisites about how optimized they are since post-training does not degrade their per-
formances, providing a consistent gain in performances for a very low additional computational
cost.
In Subsection 4.3, numerical experiments highlight the link between post-training and kernel meth-
ods. As illustrated in Table 2, using the optimal weights derived from kernel theory immediately
a performance boost for the considered network. The post-training step estimate numerically this
optimal layer with the gradient descent optimizer. However, computing the optimal weights for the
last layer is only achievable for small data set due to the required matrix inversion. Moreover, the
closed form solution is known only for specific problems, such as kernelized least square regression.
But post-training approaches the same performance in these cases solving (4) with gradient-based
methods.
The post-training can be linked very naturally to the idea of pre-training, developed notably by
Larochelle et al. (2007), Hinton et al. (2006) and Hinton & Salakhutdinov (2006). The unsupervised
pre-training of a layer is designed to find a representation that captures enough information from the
data to be able to reconstruct it from its embedding. The goal is thus to find suitable parametrization
of the general layers to extract good features, summarizing the data. Conversely, the goal of the post-
training is, given a representation, to find the best parametrization of the last layer to discriminate
the data. These two steps, in contrast with the usual training, focus on respectively the general or
specific layers.
6	Conclusion
In this work, we studied the concept of post-training, an additional step performed after the regular
training, where only the last layer is trained. This step is intended to take fully advantage of the data
representation learned by the network. We empirically shown that post-training is computationally
inexpensive and provide anon negligible increase of performance on most neural network structures.
While We chose to focus on post-training solely the last layer - as it is the most specific layer
in the network and the resulting problem is strongly convex under reasonable prerequisites - the
relationship betWeen the number of layers frozen in the post-training and the resulting improvements
might be an interesting direction for future works.
References
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. Large-scale kernel
machines, 34(5):1-41, 2007.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for LVCSR
using rectified linear units and dropout. In IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 8609-8613, Vancouver, Canada, 2013. IEEE.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine Learning Research (JMLR), 12:2121-2159,
2011. ISSN 15324435. URL http://jmlr.org/papers/v12/duchi11a.html.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and
Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine
Learning Research (JMLR), 11(Feb):625-660, 2010.
Raia Hadsell, Ayse Erkan, Pierre Sermanet, Marco Scoffier, Urs Muller, and Yann LeCun. Deep
belief net learning in a long-range vision system for autonomous off-road driving. In IEEE/RSJ
International Conference on Intelligent Robots and Systems, pp. 628-633. IEEE, 2008.
10
Under review as a conference paper at ICLR 2018
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504-507, 2006.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Using deep belief nets to learn covariance kernels
for Gaussian processes. In Advances in Neural Information Processing Systems (NIPS), pp. 1249-
1256, Vancouver, Canada, 2008.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural computation, 18(7):1527-1554, 2006.
Geoffrey E Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6a- overview of mini-batch
gradient descent. Slide for online class COURSERA: Neural Networks for Machine Learn-
ing, 2012. URL http://www.cs.toronto.edu/$\sim$tijmen/csc321/slides/
lecture_slides_lec6.pdf.
Hachem Kadri, Emmanuel DUFLOS, Philippe Preux, Stephane canu, Alain Rakotomamonjy, and
Julien Audiffren. Operator-valued Kernels for Learning from Functional Response Data. Journal
of Machine Learning Research (JMLR), 2015.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representation (ICLR), pp. 1-10, San Diego, CA, USA, 2015. ISBN
9781450300728. doi: http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503. URL
http://arxiv.org/abs/1412.6980.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University
of Toronto, 2009.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empir-
ical evaluation of deep architectures on problems with many factors of variation. In International
Conference on Machine Learning (ICML), pp. 473-480, Corvallis, United States, 2007. ACM.
Hugo Larochelle, YoshUa Bengio, J6r6me Louradour, and Pascal Lamblin. Exploring strategies for
training deep neural networks. Journal of Machine Learning Research (JMLR), 10(Jan):1-40,
2009.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: The Penn Treebank. Computational linguistics, 19(2):313-330, 1993.
Gregoire Montavon, Mikio L Braun, and Klaus-Robert Muller. Kernel analysis of deep networks.
Journal of Machine Learning Research (JMLR), 12(Sep):2563-2581, 2011. ISSN 1532-4435.
Nelson Morgan and Herve Bourlard. Generalization and parameter estimation in feedforward nets:
Some experiments. International Computer Science Institute, Denver, United States, 1990.
Christopher Poultney, Sumit Chopra, Yann L Cun, and Others. Efficient learning of sparse repres-
entations with an energy-based model. In Advances in Neural Information Processing Systems
(NIPS), pp. 1137-1144, Vancouver, Canada, 2006.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems (NIPS), pp. 1177-1184, Vancouver, Canada, 2007.
Ruslan Salakhutdinov and Geoffrey Hinton. Semantic hashing. International Journal of Approxim-
ate Reasoning, 50(7):969-978, 2009.
Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In
International Conference on Computational Learning Theory (COLT), pp. 416-426. Springer,
2001.
Athanasios Tsanas, Max A Little, Patrick E McSharry, and Lorraine O Ramig. Accurate telem-
onitoring of Parkinson’s disease progression by noninvasive speech tests. IEEE Transactions on
Biomedical Engineering, 57(4):884-893, 2010.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems (NIPS), pp. 3320-3328,
Montreal, Canada, 2014. URL http://arxiv.org/abs/1411.1792.
11
Under review as a conference paper at ICLR 2018
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint, arXiv:1409(2329), 2014.
A Convex loss
We show here, for the sake of completeness, that the post-training problem is convex for the softmax
activation in the last layer and the cross entropy loss. This result is proved showing that the hessian
of the function is positive semidefinite, as it is a diagonally dominant matrix.
Proposition 2 (convexity). ∀N, M ∈ N, ∀X ∈ RN, ∀j ∈ 1, M, the following function F is
convex:	F :RN×M → R / M	∖	M W → log Xexp(XWi) - X δij log exp(XWi) . i=1	i=1
where δ is the Dirac function, and Wi denotes the i-th row of a W.
Proof 1. Let	Pi(W) =	JxP(XWi). PM=1 exp(XWj)
then	∂Pi	_ ( - XnPi(W)Pm(W)	if i = m dWm,n	[ - XnPm(W) + XnPm(W) otherwise
Noting that	M F(W) = - X δijlog Pi(W) , i=1
we have	∂F (W) = _XX δ	1	∂Pi ∂Wm,n = - = ij Pi(W) ∂Wm,n =Xn (X δijPm(W) - δm) = Xn Pm (W ) - δmj,
hence	∂2F(W)	=	∂ ∂Pm ! ∂Wm,n∂Wp,q = Xn IdW, = XnXqPm(W) δm,p - Pp(W) .
Hence the following identity
H (F) = P(W) 0 (XX T)
where 0 is Pm(W) δm,p	the Kronecker product, and the matrix P(W) is defined by Pm,p = - Pp (W ) . Now since ∀1 ≤ m ≤ M , MM X	Pm,p = Pm (W) X	Pp(W) p=1,p6=m	p=1,p6=m =Pm(W) 1 - Pm(W) = Pm,m
12
Under review as a conference paper at ICLR 2018
P(W) is thus a diagonally dominant matrix. Its diagonal elements are positive
Pm,m =Pm(W) 1-Pm(W) ≥ 0,	asPm(W) ∈ [0, 1]
and thus P(W) is positive semidefinite. Since XXT is positive semidefinite too, their Kronecker
product is also positive semidefinite, hence the conclusion.
□
13