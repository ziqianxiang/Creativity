Under review as a conference paper at ICLR 2018
Analysis on Gradient Propagation in Batch
Normalized Residual Networks
Anonymous authors
Paper under double-blind review
Ab stract
We conduct mathematical analysis on the effect of batch normalization (BN) on
gradient backpropogation in residual network training, which is believed to play a
critical role in addressing the gradient vanishing/explosion problem, in this work.
By analyzing the mean and variance behavior of the input and the gradient in
the forward and backward passes through the BN and residual branches, respec-
tively, we show that they work together to confine the gradient variance to a cer-
tain range across residual blocks in backpropagation. As a result, the gradient
vanishing/explosion problem is avoided. Furthermore, we use the same analy-
sis to discuss the tradeoff between depths and widths of a residual network and
demonstrate that shallower yet wider resnets have stronger learning performance
that deeper yet thinner resnets.
1 Introduction
Convolutional neural networks (CNNs) (LeCun et al., 1989; Bengio et al., 2009; Krizhevsky et al.,
2012) aim at learning a feature hierarchy where higher level features are formed by the composition
of lower level features. The deep neural networks act as stacked networks with each layer depending
on its previous layer’s output. The stochastic gradient descent (SGD) method (Simard et al., 1998)
has proved to be an effective way in training deep networks. The training proceeds in steps with
SGD, where a mini-batch from a given dataset is fed at each training step. However, one factor that
slows down the stochastic-gradient-based learning of neural networks is the internal covariate shift.
It is defined as the change in the distribution of network activations due to the change in network
parameters during the training.
To improve training efficiency, Ioffe & Szegedy (2015) introduced a batch normalization (BN) pro-
cedure to reduce the internal covariate shift. The BN changes the distribution of each input element
at each layer. Let X = (χ1,χ2, •…,XK), be a K-dimensional input to a layer. The BN first normal-
izes each dimension of x as
new
xk
Xk - E(Xk)
VzVar(Xk)
(1)
and then provide the following new input to the layer
zk = γkXknew +βk,	(2)
where k = 1,…，K and Yk and βk are parameters to be determined. Ioffe & Szegedy (2015)
offered a complete analysis on the BN effect along the forward pass. However, there was little
discussion on the BN effect on the backpropagated gradient along the backward pass. This was
stated as an open research problem in (Ioffe & Szegedy, 2015). Here, to address this problem, we
conduct a mathematical analysis on gradient propagation in batch normalized networks.
The number of layers is an important parameter in the neural network design. The training of
deep networks has been largely addressed by normalized initialization (Simard et al., 1998; Glo &
Bengio, 2015; Saxe et al., 2013; He et al., 2015) and intermediate normalization layers (Ioffe &
Szegedy, 2015). These techniques enable networks consisting of tens of layers to converge using
the SGD in backpropagation. On the other hand, it is observed that the accuracy of conventional
CNNs gets saturated and then degrades rapidly as the network layer increases. Such degradation is
not caused by over-fitting since adding more layers to a suitably deep model often results in higher
1
Under review as a conference paper at ICLR 2018
training errors (Srivastava et al., 2015; He & Sun, 2015). To address this issue, He et al. (2016)
introduced the concept of residual branches. A residual network is a stack of residual blocks, where
each residual block fits a residual mapping rather than the direct input-output mapping. A similar
network, called the highway network, was introduced by Srivastava et al. (2015). Being inspired
by the LSTM model (Gers et al., 1999), the highway network has additional gates in the shortcut
branches of each block.
There are two major contributions in this work. First, we propose a mathematical model to analyze
the BN effect on gradient propogation in the training of residual networks. It is shown that resid-
ual networks perform better than conventional neural networks because residual branches and BN
help maintain the gradient variation within a range throughout the training process, thus stabilizing
gradient-based-learning of the network. They act as a check on the gradients passing through the
network during backpropagation so as to avoid gradient vanishing or explosion. Second, we provide
insights into wide residual networks based on the same mathematical analysis. The wide residual
network was recently introduced by Zagoruyko & Komodakis (2016). As the gradient goes through
the residual network, the network may not learn anything useful since there is no mechanism to force
the gradient flow to go through residual block weights during the training. In other words, it might
be possible that there are only a few blocks that learn useful representations while a large number
of blocks share very little information with small contributions to the ultimate goal. We will show
that residual blocks that stay dormant are the chains of blocks at the end of each scale of the residual
network.
The rest of this paper is organized as follows. Related previous work is reviewed in Sec. 2. Next,
we derive a mathematical model for gradient propagation through a layer defined as a combination
of batch normalization, convolution layer and ReLU in Sec. 3. Then, we apply this mathematical
model to a resnet block in Sec. 4. Afterwards, we use this model to show that the dormant residual
blocks are those at the far-end of a scale in deep residual networks in Sec. 5. Concluding remarks
and future research directions are given in Sec. 6.
2	Review of Related Work
One major obstacle to the deep neural network training is the vanishing/exploding gradient problem
(Bengio et al., 1994). It hampers convergence from the beginning. Furthermore, a proper initializa-
tion of a neural network is needed for faster convergence to a good local minimum. Simard et al.
(1998) proposed to initialize weights randomly, in such a way that the sigmoid is activated in its
linear region. They implemented this choice by stating that the standard deviation of the output of
each node should be close to one.
Glo & Bengio (2015) proposed to adopt a properly scaled uniform distribution for initialization. Its
derivation was based on the assumption of linear activations used in each layer . Most recently, He
et al. (2015) took the ReLU/PReLU activation into consideration in deriving their proposal. The ba-
sic principle used by both is that a proper initialization method should avoid reducing or magnifying
the magnitude of the input and its gradient exponentially. To achieve this objective, they first initial-
ized weight vectors with zero mean and a certain variance value. Then, they derived the variance of
activations at each layer, and equated them to yield an initial value for the variance of weight vectors
at each layer. Furthermore, they derived the variance of gradients that are backpropagated at each
layer, and equated them to obtain an initial value for the variance of weight vectors at each layer.
They either took an average of the two initialized weight variances or simply took one of them as the
initial variance of weight vectors. Being built up on this idea, we attempt to analyze the BN effect
by comparing the variance of gradients that are backpropagated at each layer below.
3	Gradient Propagation Through A Layer
3.1	BN Layer Only
We first consider the simplest case where a layer consists of the BN operation only. We use X and X
to denote a batch of input and output values to and from a batch normalized (BN) layer, respectively.
The standard normal variate of X is z. In gradient backpropagation, the batch of input gradient
values to the BN layer is ∆X while the batch of output gradient values from the BN layer is ∆x.
2
Under review as a conference paper at ICLR 2018
Mathematically, we have
X = BN (x)	(3)
By simple manipulation of the formulas given in Ioffe & Szegedy (2015), we can get
∆xi =	:Y	((∆Xi - E(∆Xi)) - ZiE(∆XiZi)),	(4)
Std(xi)
where xi is the ith element of batch x and Std is the standard deviation. Then, it is straightforward
to derive
γ2
E(∆xi) = 0,	and Var(∆xJ = ——---(Var(∆xi) - (E(∆xiZi))2).	(5)
Var(xi)
3.2	CASCADED BN/RELU/CONV LAYER
Next, we examine a more complex but common case, where a layer consists of three operations in
cascade. They are: 1) batch normalization, 2) ReLU activation, and 3) convolution. To simplify the
gradient flow calculation, we make some assumptions which will be mentioned whenever needed.
The input to the Lth Layer of a deep neural network is yL-1 while its output is yL . We use BN,
ReLU and CONV to denote the three operations in each sub-layer. Then, we have the following
three equations:
yL-1 = BN (yL-ι), yL-1 = ReLU (yL-ι), y = CONV (yL-ι).	(6)
The relationship between yL-ι, ∕l-i, 9l-i and ZL is shown in Fig. 1. As shown in the figure,
yL-ι denotes the batch of output elements from the BN sub-layer. It also serves as the input to the
ReLU sub-layer. ^^l-i denotes the batch of output elements from the ReLU sub-layer. It is fed into
the convolution sub-layer. Finally, yL is the batch of output elements from the CONV sub-layer.
Gradient vectors have ∆ as the prefix to their corresponding vectors in the forward pass. In this
figure, WLf is the weight vector of the convolution layer seen by the input and WLb is the weight
vector of the convolution layer seen by the back-propagating gradient. The dimensions of yL and
∆yL are nL and n0L, respectively. yL-1,i denotes the ith feature of activation yL-1. The variance
and mean of any activation or gradient is always calculated across a batch of activations or gradients
because we adopt the batch as a representative of the entire sample.
Figure 1: Illustration of a layer that consists of BN, ReLU and CONV three sub-layers.
3.3	Variance Analysis in Forward Pass
We will derive the mean and variance of output yL,i from the input yL-1. First, we examine the
effect of the BN sub-layer. The output of a batch normalization layer is γizi + βi, where zi is the
standard normal variate ofyL-1,i, calculated across a batch of activations. Clearly, we have
E (yL-ι,i) = βi, and Var(yL-ι,i) = γ2.	(7)
Next, We consider the effect of the ReLU sub-layer. Let a = βi. We assume that a is small enough
so that the standard normal variate zi follows a nearly uniform distribution in interval (0, a). In
Appendix A, we show a step-by-step procedure to derive the mean and variance of the output of the
ReLU sub-layer when it is applied to the output of a BN layer. Here, we summarize the main results
3
Under review as a conference paper at ICLR 2018
below:
E(yL-1,i)
EGL-I,i)
(8)
(9)
Finally, we consider the influence of the CONV sub-layer. To simplify the analysis, we assume
that all elements in WLf are mutually independent and with the same distribution of mean 0 and all
elements in yL-1 are also mutually independent and with the same distribution across a batch of
activations. Furthermore, yL-1 and WLf are independent of each other. Then, we get
Var(yLi) = nLVar(Wf,i)E ((yL-i,i)2).	(10)
Note that assuming the weight elements come from a distribution with mean 0 is a fair assumption
because we initialize the weight elements from a distribution with mean 0 and in the next section, we
see that the mean of gradient that reaches the convolution layer during backpropagation has mean 0
across a batch.
3.4	Variance Analysis in Backward Pass
We consider backward propagation from the Lth layer to the (L - 1)th layer and focus on gradient
propagation. Since, the gradient has just passed through the BN sub-layer of Lth layer, using (5) we
get E(∆yL) = 0. First, gradients go through the CONV sub-layer.
Under the following three assumptions: 1) elements in WbL are mutually independent and with
the same distribution of mean 0, 2) elements in ∆yL are mutually independent and with the same
distribution across a batch, and 3) ∆yL and WbL are independent of each other. Then, we get
Var(∆^L-i,i) = nLVar(∆yL,i)Var(Wb,i), and E(∆y∣L-i,i) = E(∆yL,i) = 0.	(11)
Next, gradients go through the ReLU sub-layer. Itis assumed that the function applied to the gradient
vector on passing through ReLU and the elements of gradient are independent of each other. Since
the input in the forward pass was a shifted normal variate (a = βγi), We get
E(AyL-1,i) = (0.5+^-7==)E (^yL-1,i) =0.0, and VarQyLfi) = (0.5+ 7==)V ar(^yL-1,i ).
2π	2π
(12)
In the final step, gradients go through the BN sub-layer. If the standard normal variate, z, to
the BN sub-layer and the incoming gradients ∆y are independent, we have E(zi∆yL-1,i) =
E(zi)E(∆yL-1,i) = 0. The last equality holds since the mean of the standard normal variate is
zero. The final result is
V ar(∆yL-1,i)
V ar(∆yL,i)
0.5 +
nL-1 Var(Wf-Ii) 0.5 + qa + 0.5a2 + ɪ
π	3 2π
(13)
Note that the last product term in the derived formula is the term under consideration for checking
gradient explosion or vanishing. The other two fractions are properties of the network, that compare
two adjacent Layers. The skipped steps are given in Appendix B.
3.5	Discussion
Initially, we set βi = 0 and γi = 1 so that a = 0. Then, the last product term in the RHS of Eq.
(13) is equal to one. Hence, if the weight initialization stays equal across all the layers, propagated
gradients are maintained throughout the network. In other words, the BN simplifies the weight
initialization job. For intermediate steps, we can estimate the gradient variance under simplifying
assumptions that offer a simple minded view of gradient propagation. Note that, when a = Y is
small, the last product term is nearly equal to one. The major implication is that, the BN helps
maintain gradients across the network, throughout the training, thus stabilizing optimization.
4
Under review as a conference paper at ICLR 2018
4	Gradient Propagation Through A Resnet Block
4.1	Resnet Block
The resnet blocks in the forward pass and in the gradient backpropagation pass are shown in Figs. 2
and 3, respectively. A residual network has multiple scales, each scale has a fixed number of residual
blocks, and the convolutional layer in residual blocks at the same scale have the same number of
filters. In the analysis, we adopt the model where the filter number increases k times from one scale
to the next one. Although no bottleneck blocks are explicitly considered here, our analysis holds for
bottleneck blocks as well. As shown in Fig. 2, the input passes through a sequence of BN, ReLU
and CONV sub-layers along the shortcut branch in the first residual block of a scale, which shapes
the input to the required number of channels in the current scale. For all other residual blocks in
the same scale, the input just passes through the shortcut branch. For all residual blocks, the input
goes through the convolution branch which consists of two sequences of BN, ReLU and CONV
sub-layers. We use a layer to denote a sequence of BN, ReLU and CONV sub-layers as used in the
last section and F to denote the compound function of one layer.
Figure 2: A residual block in the forward pass.
Figure 3: A residual block in the gradient backpropagation pass.
To simplify the computation of the mean and variance of yL,i and ∆yL,i, We assume that a =守 is
small (<1) across all the layers so that we can assume a as constant for all the layers. We define the
folloWing tWo associated constants.
ci = 0.5 + 2 2∏a	(14)
C2 = 0.5 + ∖ —a + 0.5a2 +------^=	(15)
π	3√2∏
Which Will be needed later.
4.2	Variance Analysis
As shoWn in Fig. 2, block L is the Lth residual block in a scale With its input yL-1 and output
yL . The outputs of the first and the second BN-ReLU-CONV layers in the convolution branch are
5
Under review as a conference paper at ICLR 2018
yL = F(yL-ι) and yL = F(F(yL-ι)), respectively. The weight vectors of the CONV sub-layer of
the first and the second layers in the convolution branch of block L are WL and Wl, respectively.
The weight vector in the shortcut branch of the first block is W ι. The output of the shortcut branch
is @L. For L = 1, we have y1 = F(yo), where yo is the output of last residual block of the previous
scale. For L>1, we have yL = yL-ι. For the final output, we have
yL = yL + yL.	(16)
For L>1, block L receives an input of size ns in the forward pass and an input gradient of size n0s
in the backpropagation pass. Since block 1 receives its input y0 from the previous scale, it receives
an input of size 贷 in the forward pass.
By assuming 可L and yL are independent, we have
VaryL,i) = Var(yL,i) + Var(Ol/	(17)
We will show how to compute the variance of yL,i step by step in Appendix C for L = 1,…，N.
When L = N, we obtain
旦 …	1	_f	一
Var(VNi) = C2ns(£ Var(WJi + -(Vαr(W f,i) + Var(Wfj)),	(18)
J=2
where c2 is defined in Eq. (15).
We use ∆ as prefix in front of vector representations at the corresponding positions in forward pass
to denote the gradient in Fig. 3 in the backward gradient propagation. Also, as shown in Fig. 3, we
represent the gradient vector at the tip of the convolution branch and shortcut branch by Δl and ∆L
respectively. As shown in the figure, we have
△vl-i = ∆ l + Δl
(19)
A step-by-step procedure in computing the variance of △yL-1,i is given in Appendix D. Here, we
show the final result below:
Vαr(∆yL-ii)
1+( Ci )2
Var(WL,i)______
Var(WL,i) PL-I
Var(∆yL,i).
(20)
4.3	Discussion
We can draw two major conclusions from the analysis conducted above. First, it is proper to relate
the above variance analysis to the gradient vanishing and explosion problem. The gradients go
through a BN sub-layer in one residual block before moving to the next residual block. As proved in
Sec. 3, the gradient mean is zero when it goes through a BN sub-layer and it still stays at zero after
passing through a residual block. Thus, if it is normally distributed, the probability of the gradient
values between ± 3 standard deviations is 99.7%. A smaller variance would mean lower gradient
values. In contrast, a higher variance implies a higher likelihood of discriminatory gradients. Thus,
we take the gradient variance across a batch as a measure for stability of gradient backpropagation.
Second, recall that the number of filters in each convolution layer of a scale increases by k times
with respect to its previous scale. Typically, k = 1 or 2. Without loss of generality, we can assume
the following: the variance of weights is about equal across layers, c1/c2 ≈ 1, and k = 2. Then,
Eq. (20) can be simplified to
Var(∆yL-i,i) ≈ LLIVar(∆yL,i).	(21)
We see from above that the change in the gradient variance from one residual block to its next is
little. This is especially true when the L value is high. This point will be further discussed in the
next section.
6
Under review as a conference paper at ICLR 2018
4.4	Experimental Verification
We trained a Resnet-15 model that consists of 15 residual blocks and 3 scales on the CIFAR-10
dataset, and checked the gradient variance across the network throughout the training. We plot the
mean of the gradient variance and the l2-norm of the gradient at various residual block locations
in Figs. 4 and 5, respectively, where the gradient variance is calculated for each feature across one
batch. Since gradients backpropagate from the output layer to the input layer, we should read each
plot from right to left to see the backpropagation effect. The behavior is consistent with our analysis.
There is a gradual increase of the slope across a scale. The increase in the gradient variance between
two residual blocks across a scale is inversely proportional to the distance between the residual
blocks and the first residual block in the scale. Also, there is a dip in the gradient variance value
when we move from one scale to its previous scale. Since the BN sub-layer is used in the shortcut
branch of the first block of a scale, it ensures the decrease of the gradient variance as it goes from
one scale to another. Some other experiments that we conducted to support our theory can be found
in Appendix E.
Figure 4: The mean of the gradient variance as a function of the residual block position at Epoch 1
(left), Epoch 25000 (middle) and Epoch 50000 (right).
Figure 5: The l2 norm of the gradient as a function of the residual block position at Epoch 1 (left),
Epoch 25000 (middle) and Epoch 50000 (right).
5	Width versus Depth in Resnets
Veit et al. (2016) showed that the paths which gradients take through a ResNet are typically far
shorter than the total depth of that network. For this reason, they introduced the “effective depth”
idea as a measure for the true length of these paths. They showed that almost all of gradient updates
in the training come from paths of 5-17 modules in their length. Wu et al. (2016) also presented a
similar concept. That is, residual networks are actually an ensemble of various sub-networks and
it echoes the concept of effective depth. Overall, the main point is that some residual blocks stay
dormant during gradient backpropagation.
Based on our analysis in Sec. 4, the gradient variance should increase by L/(L - 1) after pass-
ing through a residual block, where (L - 1) is the distance of the current residual block from the
first residual block in a scale. Thus, the gradient variance should not change much as the gradient
backpropagates through the chain of residual networks at the far end of a scale if we use a residual
network of high depth. Since a lower gradient variance value implies non-varying gradient values, it
supports the effective path concept as well. As a result, the weights in the residual blocks see similar
gradient variation without learning much discriminatory features. In contrast, for networks of lower
depth , the gradient variance changes more sharply as we go from one residual block to another. As
7
Under review as a conference paper at ICLR 2018
a result, all weights present in the network are used in a more discriminatory way, thus leading to
better learning.
We compare the performance of the following three Resnet models:
1.	Resnet-99 with 99 resnet blocks,
2.	Resnet-33 with 33 resnet blocks and tripled filter numbers in each resnet block,
3.	Resnet-9 with 9 resnet blocks and 11 times filter numbers in each resnet block.
Note that the total filter numbers in the three models are the same for fair comparison. We trained
them on the CIFAR-10 dataset.
Figure 6: Comparison of training accuracy for Resnet-99 (red) and Resnet-9 (blue).
Figure 7: Comparison of training accuracy for Resnet-33 (violet) and Resnet-9 (blue).
First, we compare the training accuracy between Resent-9 and Resnet-99 in Fig. 6 and that between
Resent-9 and Resnet-33 in Fig. 7, where the horizontal axis shows the epoch number. We see that
Resnet-9 reaches the higher accuracy faster than both Resnet-99 and Resnet-33, yet the gap between
Resnet-9 and Resnet-33 is smaller than that between Resnet-9 and Resnet-99. This supports our
claim that a shallow-wide Resnet learns faster than a deep-narrow Resnet. Next, we compare their
test set accuracy in Table 1. We see that Resnet-9 has the best performance while Resnet-99 the
worst. This is in alignment with our prediction and the experimental results given above.
Model	Final accuracy
Resnet With 99 resnet blocks-	93.4% —
Resnet with 33 resnet blocks	938%
Resnet with 9 resnet blocks	944%
Table 1: Comparison of test accuracy of three Resnet models at epoch 100,000
Furthermore, we plot the mean of the gradient variance, calculated for each feature across one
batch, as a function of the residual block index at epochs 1, 25,000 and 50,000 in Figs. 8, 9 and 10,
respectively, where the performance of Resnet-99, Resnet-33 and Resnet-9 is compared. We observe
that the gradient variance does not change much across a batch as it passes through the residual
blocks present at the far end of a scale in Resnet-99. For Resnet-33, there are fewer resnet blocks at
the far end of a scale that stay dormant. We can also see clearly the gradient variance changes more
8
Under review as a conference paper at ICLR 2018
sharply during gradient backpropagation in resnet-9. Hence, the residual blocks present at the end
of a scale have a slowly varying gradient passing through them in Resnet-99, compared to Resnet-33
and Resnet-9. These figures show stronger learning performance of shallower but wider resnets.
Figure 8: The gradient variance as a function of the residual block index during backpropagation in
Resnet-99 (left), Resnet-33 (middle) and Resnet-9 (right) at Epoch 1.
Figure 9: The gradient variance as a function of the residual block index during backpropagation in
Resnet-99 (left), Resnet-33 (middle) and Resnet-9 (right) at Epoch 25000.
Figure 10: The gradient variance as a function of the residual block index during backpropagation
in Resnet-99 (left), Resnet-33 (middle) and Resnet-9 (right) at Epoch 50000.
6 Conclusion and Future Work
Batch normalization (BN) is critical to the training of deep residual networks. Mathematical analysis
was conducted to analyze the BN effect on gradient propagation in residual network training in this
work. We explained how BN and residual branches work together to maintain gradient stability
across residual blocks in back propagation. As a result, the gradient does not explode or vanish in
backpropagation throughout the whole training process. Furthermore, we applied this mathematical
analysis to the decision on the residual network architecture - whether it should be deeper or wider.
We showed that a slowly varying gradient across residual blocks results in lower learning capability
and deep resnets tend to learn less than their corresponding wider form. The wider resnets tend to
use their parameter space better than the deeper resnets.
The Saak transform has been recently introduced by Kuo & Chen (2017), which provides a brand
new angle to examine deep learning. The most unique characteristics of the Saak transform approach
9
Under review as a conference paper at ICLR 2018
is that neither data labels nor backpropagation is needed in training the filter weights. It is interesting
to study the relationship between multi-stage Saak transforms and residual networks and compare
their performance in the near future.
References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.
Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trendsR in Machine
Learning, 2(1):1-127, 2009.
Felix A Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction
with lstm. 1999.
Xavier Glo and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks-glorot10a. pdf. 2015.
Kaiming He and Jian Sun. Convolutional neural networks at constrained time cost. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
C.-C. J. Kuo and Yueru Chen. On data-drive saak transform. arXiv preprint arXiv:1710.04176,
2017.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-
bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Patrice Simard, Yann LeCun, John Denker, and Bernard Victorri. Transformation invariance in
pattern recognitiontangent distance and tangent propagation. Neural networks: tricks of the trade,
pp. 549-550, 1998.
Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015.
Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. In Advances in Neural Information Processing Systems, pp. 550-558,
2016.
Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet
model for visual recognition. arXiv preprint arXiv:1611.10080, 2016.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.
URL http://arxiv.org/abs/1605.07146.
10
Under review as a conference paper at ICLR 2018
Appendix A
We apply the ReLU to the output of a BN layer, and show the step-by-step procedure in calculating
the variance and the mean of the output of the ReLU operation. In the following derivation, we
drop the layer and the element subscripts (i.e., L and i) since there is no confusion. It is assumed
that scaling factors, β and γ, in the BN are related such that a = β∕γ is a small number and the
standard normal variable z has a nearly uniform distribution in (-a,0). Then, we can write the
shifted Gaussian variate due to the BN operation as
γz + β = γ(z + a).	(22)
Let y = ReLU (z + a). Let a > 0. We can write
E(y) = P(z < -a)E(y|z < -a) + P(-a < z < 0)E(y| - a < z < 0) + P(z > 0)E(y|z > 0).
(23)
The first right-hand-side (RHS) term of Eq. (23) is zero since y = 0 if z < -a due to the ReLU
operation. Thus, E(y|z < -a) = 0. For the second RHS term, z is uniformly distributed with
probability density function equal to a-1 in range (-a, 0) if 0 < a << 1. Then, we have
P(—a < z < 0) = √^—, and E(y | — a < z < 0) = —.	(24)
For the third RHS term, P (z > 0) = 0.5. Besides, z > 0 is half-normal distributed. Thus, we have
E(y∣z > 0) = E(|z|) + a = ∏—+ a.	(25)
Based on the above results, we get
E⑺=√2π + 2 + 2√√2∏.	(26)
Similarly, we can derive a formula for E(y2) as
E(y2) = P(z < -a)E(y2|z < -a) + P(-a < z < 0)E(y2 | - a < z < 0)
+P(z > 0)E(y2|0 < z < a).	(27)
For the first RHS term of Eq. (27), we have E(y2|z < -a) = 0 due to the ReLU operation. For the
second RHS term of Eq. (27), z is uniformly distributed with probability density function a-1 for
-a<z<0 so that P(-a < z < 0) = √2∏ and E(y2∣ - a < z < 0) = a2. For the third RHS term
P (z > 0) = 0.5 for z > 0. The random variable z > 0 is half normal distributed so that
E(y2|z > 0) = E((∣z∣ + a)2) = E(|z|2) + a2 + 2aE(∣z∣) = a2 + 2∖J^a + 1.	(28)
Then, we obtain
E (y2) = 0.5 + J—a + 0.5a2 +----j=	(29)
π	3 2π
We can follow the same procedure for a < 0. The final results are summarized below.
E(ReLU(γz+β)) = γE(y) ,and E((ReLU(γz + β))2) = γ2E(y2),	(30)
where E(y) and E(y2) are given in Eqs. (26) and (29), respectively.
Appendix B
•	We assumed that the function(F) applied by ReLU to the gradient vector and the gradient
elements are independent of each other. Function F is defined as
F (∆y) = ∆yIy>0
where ∆y denotes input gradient in gradient backpropagation and y denotes the input acti-
vation during forward pass to the ReLU layer. Coming back to our analysis, since ιyL-ι,i
11
Under review as a conference paper at ICLR 2018
is a normal variate shifted by a, the probability that the input in forward pass to the ReLU
layer, i.e. yL-ι,i is greater than 0 is
P(yL-1,i > O)= 0.5 +—TE=
2π
Thus, E(F(∆yL-ι,i)) = E(∆yL-ι,i) P0L-i,i > 0), and so
E(^yL-1,i) = (0.5 + √2∏) E(^yL-1,i)
Similarly, We can solve for Var(∆yL-ι,i) and thus, get Eq. (12).
•	First, using eq 5 and the assumption that the input standard normal variate in forward pass
and the input gradient in gradient pass are independent, We have
γ2
Var(δUL-1,J = ——i----------ʌ Var(δUl-1,J	(31)
V ar(yL-1,i)
=varγL-^nl(0.5 + √α∏”(WLUVargyL)	(32)
Then, using Eq. (10) for YL-1 (yet With L replaced With L - 1), We can get Eq. (13).
Appendix C
For L = 1,可ι = F (yo). Since the receptive field for the last scale is k times smaller, we get the
folloWing from Eq. (10),
Var(yi,i) = c2 ns Var(Wf,i).	(33)
Also, since yι = F (F (yo)), we have
Var(yι,i) = c ns Var(Wfi)
k,
based on Eq. (10). Therefore, we get
Var(yι,i) = c ns Var(W 1) + c ns Var(Wfi)	(34)
k	,k	,
=C2 ns (Var(W f,i) + Var(Wfi)).	(35)
k,
For L = N > 1, the input just passes through the shortcut branch. Then,
Var (yN,i) = Var (yN-1,i)
Also, since yN = F (F (yN-ι)), we have
Var(yN,i) = c ns Var(WNG.
due to using Eq. (10). Thus,
Var(yN,i) = Var(yw-i,i) + c ns Var(Wfi).	(36)
Doing this recursively from L = 1 to N, we get
N	1 一
Var(yNi) = c2ns(]Γ Var(Wfi) + -(Var(Wf,i) + Var(Wfi)))
J=2
(37)
12
Under review as a conference paper at ICLR 2018
Appendix D
For block L = N > 1, the gradient has to pass through two BN-ReLU-Conv Layers in convolution
branch. Since, the receptive field doesn’t change in between the two BN-ReLU-Conv Layers in the
convolution branch of the block, we use Eq. (13) and find that for same receptive field between the
two layers i.e. nL = n0L-1 ,
Var(∆yL,i)=
V ar(∆yL,i).
(38)
When gradient passes through the first BN-ReLU-Conv Layer, the variance of the forward activation
that BN component sees is actually the variance of the output of previous block. Hence, using
Var(yL-1,i), which is the output of previous residual block, in place of the denominator in Eq. (31),
we get
Var(∆L,i) =
C1______________Var(WL)________________
c2 PL-2 Var(Wf,i) + k (Var(WL) + Var(Wfi))
Var(∆yL,i)
(39)
We assume that ∆L and ∆L are independent of each other. Since We are calculating for Block L>1
where there is no BN-ReLU-Conv Layer in shortcut branch, we have Var(∆L,i) = V ar(∆yL,i).
As,
Var(∆yL-ι,i) = Var(∆L,i) + Var(∆ L,i).
Finally, we obtain
Var (∆yL-1,i) =
1+( Cl )2
Var(WfOPJ= Var(Wf,i) + 1(Var(Wf) + Var(Wfi))
J Var(∆yj,i).
(40)
Appendix E
We compared two variations of residual network with the original model. The models were trained
on CIFAR-10. The models compared were
•	Model1: Residual network with BN and residual branches
•	Model2: Residual network with BN but residual branches removed
•	M odel3 : Residual network with residual branches but BN removed
Figure 11: Comparison of training accuracy for M odel1 (red), M odel2 (violet) and M odel3 (blue).
All the models had 15 residual blocks, 5 in each scale. The parameters of each model were initialized
similarly and were trained for same number of epochs. The weights were initialized with xavier
initialization and the biases were initialized to 0. First, we compare the training accuracy among the
three models in Fig. 11, where the horizontal axis shows the epoch number. We see that M odel1
reaches higher accuracy faster than the other two models. However, M odel2 isn’t far behind. But
13
Under review as a conference paper at ICLR 2018
Model	Final accuracy
Model1	92.5% ~[
Model2	906%
Model3	909%
Table 2: Comparison of test accuracy of three Resnet models.
M odel3, which has BN removed, doesn’t learn anything. Next, we compare their test set accuracy
in Table 2. We see that M odel1 has the best performance while M odel2 isn’t far behind.
Furthermore, we plot the mean of the gradient variance, calculated for each feature across one batch,
as a function of the residual block index at epochs 25,000, 50,000 and 75,000 in Figs. 12, 13 and 14,
respectively, where the performance of M odel1 and M odel2 is compared. We observe that the gra-
dient variance also stays within a certain range, without exploding or vanishing, in case of M odel2 .
However, the change in gradient variance across a scale doesn’t follow a fixed pattern compared to
M odel1. We also plot a similar kind of plot for M odel3 at epoch-1 in Fig 15. We observed gra-
dient explosion, right from the start, in case of M odel3 and the loss function had quickly become
undefined. This was the reason, why M odel3 didn’t learn much during the course of training.
This experiment shows that BN plays a major role in stabilizing training of residual networks. Even
though we remove the residual branches, the network still tries to learn from the training set, with
its gradient fixed in a range across layers. However, removing BN hampers the training process
right from the start. Thus, we can see that batch normalization helps to stop gradient vanishing and
explosion throughout training, thus stabilizing optimization.
Figure 12: The gradient variance as a function of the residual block index during backpropagation
in M odel1 (left), and M odel2 (right) at Epoch 25000.
Figure 13: The gradient variance as a function of the residual block index during backpropagation
in M odel1 (left), and M odel2 (right) at Epoch 50000.
14
Under review as a conference paper at ICLR 2018
Figure 14: The gradient variance as a function of the residual block index during backpropagation
in M odel1 (left), and M odel2 (right) at Epoch 75000.
Figure 15: Gradient explosion observed during back propagation in M odel3 at epoch-1
15