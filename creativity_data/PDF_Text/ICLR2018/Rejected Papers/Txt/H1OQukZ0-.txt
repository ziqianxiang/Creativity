Under review as a conference paper at ICLR 2018
Online hyper-parameter optimization
Anonymous authors
Paper under double-blind review
Ab stract
We propose an efficient online hyperparameter optimization method which uses a
joint dynamical system to evaluate the gradient with respect to the hyperparame-
ters. While similar methods are usually limited to hyperparameters with a smooth
impact on the model, we show how to apply it to the probability of dropout in
neural networks. Finally, we show its effectiveness on two distinct tasks.
1	Introduction
With the growing size and complexity of both datasets and models, training times keep increasing
and it is not uncommon to train a model for several days or weeks. This effect is compounded by the
number of hyperparameters a practitioner has to search through. Even though search through hyper-
parameter space has improved beyond grid search, this task is still often computationally intensive,
mainly because these techniques are offline, in that they need to perform a full learning before try-
ing a new value. Recently, several authors proposed online hyperparameter optimization techniques,
where the hyperparameters are tuned alongside the parameters of the model themselves by running
short runs of training and updating the hyperparameter after each such run. By casting the joint
learning of parameters and hyperparameters as a dynamical system, we show that these approaches
are unstable and need to be stopped using an external process, like early stopping, to achieve a good
performance. We then modify these techniques such that the joint optimization procedure is sta-
ble as well as efficient by changing the hyperparameter at every time step. Further, while existing
techniques are limited in the type of hyperparameters they can optimize, we extend the process to
dropout probability optimization, a popular regularization technique in deep learning.
2	Related work
Historically, optimizing hyperparameters was done by selecting a few values for each hyperparam-
eter, computing the Cartesian product of all these values, then by running a full training for each
set of values. Bergstra & Bengio (2012) showed that performing a random search rather than a grid
search was vastly more efficient, in particular by avoiding spending too much time training models
with one of the hyperparameters set to a poor value. This technique was later refined in Bousquet
et al. (2017) by using quasi random search. However, the parameters and their ranges have to be
selected in advance, and potentially many trainings have to be performed to find good parameters.
To remedy this issue, Snoek et al. (2012) used Gaussian processes to model the validation error as a
function of the hyperparameters. Each training further refines this function to minimize the number
of sets of hyperparameters to try.
All these methods are “black-box” methods, in that they assume no knowledge about the internal
training process to optimize hyperparameters. In particular, they are gradient-free since they do not
have access to the gradient of the validation loss with respect to the hyperparameters.
To solve this issue, Maclaurin et al. (2015) and Pedregosa (2016) explicitly used the parameter
learning process to obtain such a gradient. These techniques, however, still need to complete a
full optimization between each update of the hyperparameters. This can be an issue for very long
trainings as poor choices of hyperparameters are not discarded right away, leading to unnecessary
computations.
The work most closely related to ours is that of Franceschi et al. (2017) where the hyperparameters
are changed after a certain number of parameter updates. However, not only must that number of
1
Under review as a conference paper at ICLR 2018
updates be chosen manually, the proposed algorithm is not stable and moves away from the opti-
mum after some time. While this issue can be solved using other techniques, e.g., early stopping, we
propose a stable algorithm by casting the joint learning of parameters and hyperparameters as a dy-
namical system. We also show how convergence can be obtained by changing the hyperparameters
after each parameter update, thus further simplifying the algorithm. We then propose modifications
to improve the speed and robustness of the optimization. Finally, we demonstrate the performance
of our method on several problems.
3	Hyperparameter optimization
The goal of learning is to find parameters which minimize the true expected risk. As we do not
have access to that risk, we rely instead on the minimization of the empirical risk obtained using
a training set. However, it is well-known that this can lead to overfitting, which can be prevented
by regularization. A common method to choose which and how much regularization to use is to
hold out part of the training set and to find which regularization yields the best performance on that
held-out, or validation, set. We denote by parameters the parameters of the function being learnt
and by hyperparameters the parameters of the regularization being used.
Hyperparameter optimization looks for the hyperparameters λ such that the minimization of the
regularized training loss over model parameters θ leads to the best performance on the validation
set. Using this nomenclature, the best hyperparameters are selected according to:
λ* = argmin LV (θ*(λ)) with θ*(λ) = argmin LT (θ,λ)	(1)
λθ
where LV is the unregularized validation loss and LT the regularized training loss. It is important to
note that this work focuses exclusively on regularization hyperparameters. In particular, we do not
attempt to optimize optimization parameters such as the learning rate.
Eq. (1) shows that, to determine an optimization strategy for λ, one may compute the gradient of
LV(θ*(λ)) with respect to λ, which We call the hypergradient, and perform gradient descent.
3.1	Full Optimization
By the chain rule, we have
∂Lv(θ*(λ))
∂λ
gV (θ*(λ)) M
where, to simplify notations, We denoted by gv the gradient of LV with respect to θ, i.e. gv := ddLV.
Similarly, we denote gτ := d∂Lτ the gradient of the regularized training loss with respect to θ.
Since, by definition of the optimum, we have gτ(θ*(λ), λ) = 0, we can use the implicit function
theorem to get:
dθ*m _	dgτ (θ* a-ι dgτ (θ* a
∂λ(λ) =-嬴("分 而(θ,λ).
(2)
Several algorithms propose to compute the hypergradient exactly Maclaurin et al. (2015) compute
this derivative by backpropagating through the whole training procedure. Unfortunately this is very
costly both in memory footprint and in wall time as several training procedures need to be serialized,
hence is not easily scalable to large models. Pedregosa (2016) computes an approximate derivative
when the model parameters are close to the optimal ones. In both cases, one needs to perform a full,
or almost full, optimization to compute the gradient, leading to expensive updates.
We shall now see how we can compute approximate updates using far fewer optimization steps.
3.2	Alternating optimization
The core idea is that the convergence of iterates θt to θ* should allow US to use these iterates to
update λ rather than wait until convergence. In doing so, we could optimization the hyperparam-
2
Under review as a conference paper at ICLR 2018
eters simultaneously with the optimization of the model parameters. This idea has been explored
by Franceschi et al. (2017) who proposed to optimize the validation error obtained when running
exactly K steps of gradient descent with fixed hyperparameters, i.e.
min	LV (θK (λ, θ0))
subject to:	θt+1 = θt - ηgT(θt, λ)
(3)
(4)
where the constraint corresponds to the updates of θ using gradient descent with a learning rate η
and where θt (t > 0) implicitly depends on λ.
The K-iterate hypergradient is then given by:
∂Lv (θκ (λ, θo))
∂λ
∂θκ (λ,θo)
gV (θK)	∂λ
(5)
Computing ∂θK Can be done recursively by differentiating the gradient update recurrence in Eq. (4)
with respect to λ:
∂θt+1	∂θt
~∂Γ =荻-η
(θ ∙) ∂θt+迎(θt ∙)
(t, ) ∂λ + ∂λ (t, )
Defining yt = 绘,We have two dynamical systems:
θt+1 = θt - ηgT (θt, λ)
yt+ι = yt - η (dgτ(θt,λ)yt + d∂gT(θt,λ)
(6)
(7)
starting from θ0 = 0, y0 = 0. It is important to emphasize that, although we care about the conver-
gence of the second system to compute the hypergradient, its trajectory is completely determined by
that of θt and thus by the first system. In other words, the value ofyt does not affect the optimization
process over θ.
Even though the system defined in Eq. (7) converges to the right solution, it can do so very slowly.
For instance, assume that we are at hyperparameter λ0 and that θ0 is initialized to the optimal
value, i.e. θ0 = θ*(λ0). In that case, the system defined in Eq. (6) is already at convergence and
θt = θ*(λo) for all t. The second system, however, will take some time to converge to the final
∂θ*(λ.θ∩ )
value ——∂λ, 0). Fig. A m the Appendix depicts the issue.
We now study the behavior of yt =缭(λ0) whose recurrence is:
yt+1 = yt - η (Ayt + B)
with ʃ A = ∂∂T(θ*(λ0), λ0) = ∂θ∂Lθτ (θ*(λ0), λ0)
I B = dgT(θ*(λ0),λ0).
The fixed point of this recurrence is y* = -ATB which is equal to the true hypergradient 桨(λ0)
according to Eq. (2). The convergence rate of yt depends on the spectrum of I - ηA. If η is too
small, convergence will be slow and using a fixed number of steps K can lead to a poor estimation
of等
This poor estimation is mitigated by the fact that, if η is small and y0 = 0, kyK k will be in O(Kη)
and thus the steps taken in hyperparameter space will also be small. We thus believe the overall
effect of a small η on the hyperparameter optimization will be limited to a smaller convergence.
We described how to perform one hyperparameter update using gradient descent with the hypergra-
dient dL∂λθK). Franceschi et al. (2017) repeat this process in an outer loop, each time setting θo
to the previous θK and initializing y0 to 0. This reinitialization of y at the beginning of each inner
loop prevents the optimization from capturing any long term dependency of λ on θ and yt from
converging to the true hypergradient.
We now propose another formulation which maintains a growing history of the dependency of λ on
θ, yielding increased stability.
3
Under review as a conference paper at ICLR 2018
3.3	First order hyperparameter optimization
Using the method of Franceschi et al. (2017) with K = 1 updates the hyperparameters at every
gradient step, as proposed by LUketina et al. (2016). They compute y(λt) = -nd∂gτ(θt, λt), so that
the hypergradient is estimated by
—,—
∂L (θ )	∂g
一=一 =< gv(θt),y(λt) >= -n < gv(θt), ~^~(θt,λt) >	(8)
∂λ	∂λ
Under this formulation, minimizing the validation loss over λ is equivalent to maximizing <
gv(θt), gT(θt, λt) > using a specific scaling η for the learning rate.
Assuming we are ata θ lying on the manifold {θ* (λ) : λ}, then the hypergradient defined by Eq. (8)
is proportional to
ʌrɔ`'	一 , _ ..
dLV	仆仆	dgτ(θ,λ)
F (X-gv(θ ㈤)
θ=θ*
which is in general not equal to the true hypergradient
∂Lv (θ*)
-∂λ-
-1
T (θ*,λ).
The two hypergradients are only proportional when the Hessian dd∂gτ is a multiple of the identity.
This suggests these first order methods can fail to converge to a local optimum. However, their
simplicity makes them good candidates for the early stages of the optimization.
3.4	Online optimization with moving estimates
Instead of reinitializing y0 (λt) to 0 after every hyperparameter update, another possibility is to
initialize y0(λt) to the last value yK(λt-1) obtained using the previous value of λ. While this is
beneficial when λt is close to λt-1, issues might arise earlier in the optimization. Indeed, stopping
the system before convergence could yield a value of yt much larger than y∞, overstimating the
norm of the gradient and leading to a large change in λ. Although reinitializing y0 to 0 every time is
crude, it favors smaller values ofyt and thus smaller changes in λ, increasing stability.
To keep this stability while maintaining as much information about y as possible, we propose to
modify recurrence yt by constraining yt to lie within a ball:
yt = PB(r)
(θt-1, λ)yt-1 +
(9)
where PB(r) is the projection on the ball of radius r and is formally defined by PB(r) (x) =
r
------------X
max(kxk ,r)
Every time the norm is clipped, this is equivalent to changing the stepsize for λ but not the direction
of the gradient. However, due to the dynamical nature of the system, it also affects future updates.
As the learning rate decreases, so does the probability of clipping since:
yt-ι - n (dgT(θt-ι, λ)yt-ι + 黑(%—，λ)) Il - ∣∣yt-ιk2
O(η) .
r is a hyperparameter which was chosen in the experiments so that clipping occurs almost at every
step at the beginning of optimization, behaving like the method of Luketina et al. (2016). Our
proposed method is summarized in Algorithm 1.
4
Under review as a conference paper at ICLR 2018
Algorithm 1 Online hyperparameter optimization.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
procedure HYPERPARAMETEROPTIMIZATION(num_steps,r, enable_projection)
(θo, λo) J Initial value of the (parameters, hyperparameters)
yo J 0
α0 J 0
for t < num_steps do
yt+1 J PB(r) (yt - η (dgT(θt, λt)yt + dgT(θt, λtD)
θt+1 J θt - ηgT
gP J gV
if t ≥ WarmUP_time then
λt+1 J λt - cηgP yt with: c constant scaling
else
λt+1 J λt
3.5 Regularization specifics
We noW describe in more details hoW We optimized tWo different regularizers: `2 penalty, Which has
been optimized previously using hyperparameter optimization techniques, and dropout probability,
for Which, to the best of our knoWledge, no existing techniques can be applied.
3.5.1	`2 REGULARIZATION
The training loss, in case of '2 regularization, is given by LT(θ, λ) = L(θ) + 2λ ∣∣θ∣2.
In neural networks, we can differentiate two types of linear layers depending on whether L(θ)
is sensitive to the norm of the weights or not. For example, the unregularized loss does not
depend on the norm when a linear layer is followed by a normalization layer like batch norm:
any change of the norm is compensated by the normalization layer. In those cases, `2 regu-
larization does not prevent overfitting as the norm can be decreased arbitrarily close to 0 with-
out changing the function represented by the neural network. However, it has an impact on the
dynamic of the training. van Laarhoven (2017) showed that for such a layer represented by
η
weights θ, the effective learning rate is ηeff = -----Since the gradient is orthogonal to the
∣θ∣2
vector of weights Salimans & Kingma (2016), the norm of the weights after a gradient update
is
θ-η( dL+λθ)ll
(1 - ηλ)2 kθk2 + η2
∂L
∂θ
2
and keeps increasing if there is
no `2 regularization (i.e. λ = 0). The norm remains stable after a gradient update only When
∣∣θk2 = 2λ-n¥ Il∂∂θ∣∣2 Assuming a learning rate small enough such that ηλ << 1, we have
∣∣θk2 ≈ 长 ∣∣ ∂∂θ∣∣2. In terms of effective learning rate, the norm remains stable when ηeff = .. ∂Lλ∣ ∣2,
k ∂θ k
i	.e. when the effective learning rate does not depend on the initial learning rate. This short analy-
sis shows that `2 regularization can have a significant impact on the optimization without having a
proper regularization effect. In this paper, we do not intend to address the problem of optimizing
hyperparameters that have only an impact on the dynamic of the training and focus on the original
intent of `2 regularization as a way to prevent overfitting.
3.5.2	Dropout
Introduced in Hinton et al. (2012) and further studied in Srivastava et al. (2014), dropout is a way
to regularize by preventing co-adaptation of output units of a neural network. Regularization is
achieved by considering an ensemble of network architectures which differ only by their connections
between the output units and the input units of the next layer, the weights being shared. Each output
unit can be either kept with probability p or dropped, meaning there is no connection to the next
layer. The keep/drop decision can be represented by a vector mask m which indicates for each
output unit whether this one is kept or dropped. The probability to keep an output unit is often
considered as an hyperparameter of the model, denoted as λ in this section. The training loss can be
5
Under review as a conference paper at ICLR 2018
computed as an expectation over dropout masks m:
LT(θ, λ) = Em〜B(p=λ)[LT(θ, m)]
where B(p = λ) denotes the Bernoulli distribution. To compute the dependencies between the state
θ and the hyperparameter λ (See Eq. (6) and (7)), We need to have access to ∂LT. This cannot be
formally computed, but can be approximated with finite differences:
∂LT	1/_	一 ，一 _	—	一 ，一 、八
J1≈ 2^ (Em^B(p=λ+e) [LT(θ, m)] - Em-B(p=λ-e) [LT(θ, m)])
In order to minimize the complexity, We just sample one dropout mask for p = λ + and one
for p = λ - and compute the approximate derivative of the loss. The variance of the derivative
computed this Way can be quite large though. Instead, We use the fact that:
p1
B(P = Pi) = B(P = P2)B(p = 一)for pi < p2
p2
(10)
to sample a mask for Pi = λ - Which is not independent from the mask sampled using P2 = λ + .
4	Experiments
We compare several methods to train the hyperparameters in an online Way:
(a)	Unroll1-gTgV, a first order method directly maximizing gTgV (Section 3.3, no η),
(b)	UnrollK, the version from Franceschi et al. (2017) (Section 3.2) Which optimizes the hy-
perparameters over a fixed training WindoW of size K,
(c)	ClipR, described in Algorithm 1 With a clipping threshold equal to R.
Note that Unroll1 is a special case of UnrollK Which differs from Unroll1-gTgV by the factor η
(Which can have an influence When η is depends on t).
We evaluate these methods on models of increasing complexity, starting With a toy problem and
ending on a typical deep learning setup. As a baseline, We use the typical one-shot hyperparameter
optimization Where the model is learnt N times, once for each value of the hyperparameters, keeping
the hyperparameters achieving the best validation loss.
While conducting the evaluation, We should be aWare of possible overfitting on the validation set:
information leaked from that set should be the same as With the typical one shot optimization algo-
rithm. In particular, tested methods contain hyper-hyperparameters, like the clipping threshold in
method ClipR. Since our original goal is to simplify hyperparameter optimization, We also test hoW
sensitive to the particular values of these hyper-hyperparameters the final result is.
Finally, We evaluate the intrisic stability of the various online algorithms. To do so, We shall compare
the performance of each online hyperparameter optimization method With and Without using early
stopping. A large gap betWeen these tWo values indicates the best hyperparameter value is not a
stable point for this particular method. Again, We use as baseline the same gap computed When the
training is done With fixed hyperparameters.
While We mostly report final results, more detailed reports of all these experiments are available in
the appendix.
4.1	Quadratic function
We consider a strongly convex optimization problem Where the training loss and the validation loss
are given by Lτ(θ, λ) = ɪ(θ 一 夕)tH(θ 一 θ) + ^λθτHθ and LV(θ) = ɪ(θ 一 θ)TH(θ 一 θ) where
θ, θ are of dimension 20 and H is a diagonal matrix. The optimal λ can be computed analytically
andisequalto λ∣ =尹H®一 θ
θτ Hθ
For the first 20K steps, the learning rate η is set to 10-3 and the parameters at the end of this first
phase are denoted by (θi, λi). η is then set to 10-4 for the following 20K steps and the parameters
6
Under review as a conference paper at ICLR 2018
Method	∆LV (θ1)	N - λ”	∆LV(θ2)	∣λ2 — λ“
Unroll1	+0.77%	0.047	+0.77%	0.047
Unroll5	+0.56%	0.039	+0.72%	0.045
Unroll10	+0.37%	0.032	+0.71%	0.044
Clip2	+0.07%	0.012	+0.07%	0.012
Clip5	+0.00%	0.000	+0.00%	0.000
Table 1: Quadratic function: we see that longer rollouts lead to lower validation losses and more
accurate values for the regularization parameter λ. Not only do clipping methods achieve better
results, they are also more stable.
Method	Cross-entropy loss (×103)		Stability
	With early stopping	Without early stopping	
Fixed λ	71.6	73.3	+2.4%
Unroll1-gTgV	71.9	73.2	+1.8%
Unroll1	73.0	73.6	+0.8%
Unroll5	73.7	74.3	+0.8%
Unroll10	73.6	74.8	+1.6%
Clip5	71.9	73.0	+1.5%
Clip10	71.4	72.7	+1.8%
Clip20	70.8	71.8	+1.4%
Table 2: Performance on MNIST with a 4-hidden layer network (lower is better). Clipping leads to
the best results, with or without early stopping. In that case, small unrolls are the most stable.
at the end of this phase are (θ2, λ2). The learning rate for the hyperparameters is always set to: 0.1η.
Constants θ, θ and H are sampled so that λ* lie in [0.2,0.4].
We repeat each training 50 times using different θ, θ, H, and we evaluate the performance of each
method using two metrics: (a) the average distance between λ%(i ∈ 1, 2) and λ*, (b) the increase in
validation loss compared to optimal value θ*(λ±).
Table 4.1 shows that, in line with the theoretical observations, methods UnrollK methods are unable
to estimate the hypergradient when the learning rate decreases, leading to a slight increase of the
loss. Additionally, while ∣∣ 某 IiS between 3.0 and 5.0, using 2.0 as a clipping threshold leads to
only a slight increase of the validation loss while Clip5 converges to the correct value.
4.2	Image classification
We consider here a feedforward network with 4 fully connected layers, of size 100 for the 3 hidden
layers and of size 10 for the last layer. It is trained on MNIST using `2 regularization. We also
use a decaying schedule for the learning rate as it has been shown to possibly impact the online
hyperparameter optimization algorithms. The dataset is split between a training set (75% of the
samples) and a validation set (25% of the samples). Every online algorithm is run 6 times with
a different initialization of the weight decay, equally spaced in the log domain between 10-6 and
10-1. The results are averaged over those 6 runs. The metric we optimize for is the cross-entropy
on the validation dataset.
Table 4.2 shows that clipping outperforms the other methods, especially when early stopping is not
used, displaying higher stability. We also note that Clip20 performs better than a fixed λ found
through grid search, showing the efficiency of the method on more realistic problems.
7
Under review as a conference paper at ICLR 2018
Method	Cross-entro With early stopping		py loss (×103) Without early stopping	Stability
Fixed λ: λ =	0.35	84.9	85.2	+0.4%
Fixed λ: λ =	0.40	84.3	86.3	+2.3%
Fixed λ: λ =	0.45	85.5	88.2	+3.1%
Unroll1-gTgV		85.6	95.8	+11.9%
Unroll1		86.1	92.6	+7.6%
Unroll5		84.3	89.1	+5.6%
Unroll10		84.2	87.4	+3.8%
Clip r=5.0		83.2	83.8	+0.8%
Clip r=10.0		83.1	83.7	+0.7%
Clip r=20.0		84.2	84.7	+0.6%
Clip r=5.0, LR scale in {1.0, 0.1}		83.7	84.4	+0.8%
Table 3: Performances on PTB with an LSTM (lower is better).
4.3	Language model
Finally, we consider a language modeling task using the PTB dataset (Marcus et al. (1993)) and a
typical LSTM-based network architecture1. In this architecture, the use of dropout is critical in order
to prevent overfitting on the training dataset. Training is done using a decaying learning rate with a
multiplicative decay of 0.95 every 5K mini-batches. The hyperparameter learning rate is chosen as
a constant scaling of the parameter learning rate.
As described previously, the performance metric (the perplexity on the validation dataset in this case)
in Table 3 are given with and without early stopping so as to derive a measure of intrisic instability of
the online algorithms. Combined with early stopping, all the hyperparameter optimization methods
achieve good performance with slightly worse results for Unroll1-gTgV and Unroll1.
However, the algorithms are not equivalent in terms of stability. Unrolling methods are the most
unstable when close to convergence as they drift to higher probabilities of keeping the weights
(under-regularization). As explained in Section 3.3, method Unroll1 attenuates this effect compared
to Unroll1-gTgV by using a lower effective learning rate when this drift occurs. Contraty to what
we observed on MNIST, longer rollouts seem here to increase the stability. On the other side, none
of the flavors of the gradient clipping algorithm is subject to this instability: the instability metric is
low and of the same order as the one derived using a fixed keep probability of λ = 0.35.
Last, online optimization of the hyperparameter does not increase overfitting on the validation
dataset compared to a typical one shot algorithm: the minimum of the validation loss for those
algorithms being close to the one obtained with one shot hyperparameter optimization.
5	Conclusion
Progress in optimization methods led to faster model training but the multiplication of hyperparam-
eters means that one often must train many models to find the best one, an inefficient process putting
most tasks out of reach for most. By casting hyperparameter optimization as a dynamical system,
similar to standard parameter optimization, we derive an extension to the works of Luketina et al.
(2016) and Franceschi et al. (2017) which exhibits higher stability and performance.
However, many questions remain. First, even though the method can be applied to any number of
hyperparameters, we restricted our experiments to just one. There might be dynamics that are yet
to be understood. Second, we did not apply this method to the hyperparameters of the optimizer,
such as the step size or the decaying factor. As such hyperparameters affect the dynamics of the
system, it is likely that the methods to optimize them differ from ours. Finally, our method involves
other hyperparameters, such as the clipping factor. While our experiments point to the fact that their
impact is limited, it would be best to get rid of them entirely.
1Medium configuration of the tensorflow model from https://goo.gl/zZahPt
8
Under review as a conference paper at ICLR 2018
References
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research,13(Feb):281-305, 2012.
Olivier Bousquet, Sylvain Gelly, Karol Kurach, Olivier Teytaud, and Damien Vincent. Critical
hyper-parameters: No random, no cry. CoRR, abs/1706.03200, 2017. URL http://arxiv.
org/abs/1706.03200.
L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and Reverse Gradient-Based Hyper-
parameter Optimization. ArXiv e-prints, March 2017.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural
networks by preventing co-adaptation of feature detectors. ArXiv e-prints, July 2012.
Jelena Luketina, Tapani Raiko, Mathias Berglund, and Klaus Greff. Scalable gradient-based tuning
of continuous regularization hyperparameters. In ICML, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Comput. Linguist., 19(2):313-330, June 1993. ISSN
0891-2017. URL http://dl.acm.org/citation.cfm?id=972470.972475.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning, pp. 737-746, 2016.
Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. CoRR, abs/1602.07868, 2016. URL http://
arxiv.org/abs/1602.07868.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15
(1):1929-1958, January 2014. ISSN 1532-4435. URL http://dl.acm.org/citation.
cfm?id=2627435.2670313.
Twan van Laarhoven. L2 regularization versus batch and weight normalization.	CoRR,
abs/1706.05350, 2017. URL http://arxiv.org/abs/1706.05350.
9
Under review as a conference paper at ICLR 2018
A Convergence of the derivative
Fig. 1 shows that, even if We start at the optimal value θ*(λ) for the parameters, the dynamical
system {yt} will take some time to converge to the true solution.
Figure 1: Illustration of the convergence of
∂∂θt(λo) when θ0(λ) is a scalar initialized to
θ*(λo). θt has always the same value in λo but
its derivative converges to ∂∂λt(λo).
10
Under review as a conference paper at ICLR 2018
B Experiments: dynamic of the training
We now show how the value of the hyperparameter and the training and validation loss vary during
optimization. In particular, this will help determine when there are instabilities, i.e. when the best
validation loss is not obtained at the end of the optimization.
B.1	MNIST WITH `2 REGULARIZATION
Training using Unroll1-gTgV and Unroll1
1K steps
3K steps
5K steps
15K steps
40K steps
80K steps
11
Under review as a conference paper at ICLR 2018
Training using Unroll5 and Clip5
1K steps
3K steps
5K steps
15K steps
40K steps
80K steps
12
Under review as a conference paper at ICLR 2018
B.2	PTB with dropout
Training using Unroll1-gTgV and Unroll1
4K steps
8K steps
15K steps
30K steps
60K steps
90K steps
120K steps
180K steps
13
Under review as a conference paper at ICLR 2018
Training using Unroll5 and Clip5
4K steps
8K steps
15K steps
30K steps
60K steps
90K steps
120K steps
250K steps
14