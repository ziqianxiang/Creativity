Under review as a conference paper at ICLR 2018
Avoiding Catastrophic States with
Intrinsic Fear
Anonymous authors
Paper under double-blind review
Abstract
Many practical reinforcement learning problems contain catastrophic states that
the optimal policy visits infrequently or never. Even on toy problems, deep rein-
forcement learners periodically revisit these states, once they are forgotten under
a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping
that accelerates deep reinforcement learning and guards oscillating policies against
periodic catastrophes. Our approach incorporates a second model trained via super-
vised learning to predict the probability of imminent catastrophe. This score acts
as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that
the perturbed objective yields the same average return under strong assumptions
and an -close average return under weaker assumptions. Our analysis also shows
robustness to classification errors. Equipped with intrinsic fear, our DQNs solve
the toy environments and improve on the Atari games Seaquest, Asteroids, and
Freeway.
1	Introduction
Following success on Atari games (Mnih et al., 2015) and the board game Go (Silver et al., 2016),
many researchers have begun exploring practical applications of deep reinforcement learning (DRL).
Some investigated applications include robotics (Levine et al., 2016), dialogue systems (Fatemi et al.,
2016; Lipton et al., 2016), energy management (Night, 2016), and self-driving cars (Shalev-Shwartz
et al., 2016). Amid this push to apply DRL, we might ask, can we trust these agents in the wild?
Agents acting in real-world environments might possess the ability to cause catastrophic outcomes.
Consider a self-driving car that might hit pedestrians or a domestic robot that might injure a child.
We might hope to prevent DRL agents from ever making catastrophic mistakes. But doing so requires
extensive prior knowledge of the environment in order to constrain the exploration of policy space
(Garcia and Ferndndez, 2015).
Many conflicting definitions of safety and catastrophe exist, a problem that invites further philosophi-
cal consideration. In this paper, we introduce a specific but plausible notion of avoidable catastrophes.
These are states that prior knowledge dictates an optimal policy should never visit. For example,
we might believe that an optimal self-driving algorithm would never hit a pedestrian. Moreover, we
assume that an optimal policy never even comes near an avoidable catastrophe state. We define
proximity in trajectory space, and not by the geometry of feature space. We denote states proximal to
avoidable catastrophes as danger states. While we don’t assume prior knowledge of which states are
dangerous, we do assume the existence of a catastrophe detector. After encountering a catastrophic
state, an agent can realize this and take action to avoid dangerous states in the future.
Given this definition, we address two challenges: First, can we expect DRL agents, after experiencing
some number of catastrophic failures, to avoid perpetually making the same mistakes? Second, can
we use our prior knowledge that catastrophes should be kept at a distance to accelerate learning of a
DRL agent? Our experiments show that even on toy problems, the deep Q-network (DQN), a basic
algorithm behind many of today’s state-of-the-art DRL systems, struggles on both counts. Even in
toy environments, DQNs may encounter thousands of catastrophes before learning to avoid them and
are susceptible to repeating old errors. We call this latter problem the Sisyphean curse.
This poses a formidable obstacle to using DQNs in the real world. How can we hand over responsibil-
ity for consequential actions (control of a car, say) to a DRL agent if it may be doomed to periodically
remake every kind of mistake, however grave, so long as it continues to learn? Imagine a self-driving
1
Under review as a conference paper at ICLR 2018
car that had to periodically hit a few pedestrians in order to remember that is undesirable. In the
tabular setting, an RL agent never forgets the learned dynamics of its environment, even as its policy
evolves. Moreover, if the Markovian assumption holds, eventual convergence to a globally optimal
policy is guaranteed. Unfortunately, the tabular approach becomes infeasible in high-dimensional,
continuous state spaces.
The trouble for DQNs owes to the use of function approximation (Murata and Ozawa, 2005). When
training a DQN, we successively update a neural network based on experiences. These experiences
might be sampled in an online fashion, from a trailing window (experience replay buffer), or uniformly
from all past experiences. Regardless of which mode we use to train the network, eventually, states
that a learned policy never encounters will come to form an infinitesimally small region of the
training distribution. At such times, our networks are subject to the classic problem of catastrophic
interference (McCloskey and Cohen, 1989; McClelland et al., 1995). Nothing prevents the DQN’s
policy from drifting back towards a policy that revisits long-forgotten catastrophic mistakes.
More formally, we characterize the problem as unfolding in the following steps: (i) Training under
distribution D, our agent produces a safe policy πs that avoids catastrophes (ii) Collecting data
generated under πs yields a new distribution of transitions D0 (iii) Training under D0, the agent
produces πd, a policy that once again experiences avoidable catastrophes. To illustrate the brittleness
of modern DRL algorithms, we introduce a simple pathological problem called Adventure Seeker.
This problem consists of a one-dimensional continuous state, two actions, simple dynamics, and a
clear analytic solution. Nevertheless, the DQN fails. We then show that similar dynamics exist in the
classic RL environment Cart-Pole.
In this paper, to combat these problems, we propose intrinsic fear. In this approach, we train a
supervised fear model that predicts which states are likely to lead to a catastrophe within kr steps. The
output of the fear model (a probability), scaled by a fear factor penalizes the Q-learning target. Our
approach draws inspiration from intrinsic motivation (Chentanez et al., 2004). However, instead of
perturbing the reward function to encourage the discovery of novel states, we perturb it to discourage
revisiting catastrophic states.
We validate the approach both empirically and theoretically. Our experiments address both our
Adventure Seeker problem and Cartpole as well as the Atari games Seaquest and Asteroids, and
Freeway. For these environments, we label each loss of a life as a catastrophic state. On the toy
environments, the intrinsic fear agent learns to avoid death indefinitely, achieving unbounded reward
per episode. On Seaquest and Asteroids, the intrinsic fear agent improves markedly and on Freeway
the improvement is dramatic. Theoretically, we demonstrate the following: First, we prove that when
the reward is bounded and the optimal policy rarely visits the catastrophic states, the policy learned
on the altered value function has return similar to the optimal policy on the original value function.
Second we prove that the method is robust to noise in the danger model.
2	Intrinsic fear
Over a series of turns, an agent interacts with its environment via a Markov decision process, or MDP,
(S, A, T, R, γ). At each step t, an agent observes a state s ∈ S. The agent then chooses an action
a ∈ A according to some policy π. In turn, the environment transitions to a new state st+1 ∈ S
according to transition dynamics T (st+1 |st, at) and generates a reward rt with expectation R(s, a).
This cycle continues until each episode terminates.
The goal of an agent is to maximize the cumulative discounted return PtT=0 γtrt . Temporal-
differences (TD) methods (Sutton, 1988) such as Q-learning (Watkins and Dayan, 1992) model
the Q-function, which gives the optimal discounted total reward of a state-action pair; the greedy
policy w.r.t. the Q-function is optimal (Sutton and Barto, 1998). Problems of practical interest tend to
have large state spaces, thus the Q-function is typically approximated by parametric models such as
neural networks.
In Q-learning with function approximation, an agent alternately collects experiences by acting
greedily with respect to Q(s, a; θQ ) and updates its parameters θQ . Updates proceed as follows. For
a given experiences (st, at, rt, st+1), we minimize the squared Bellman error:
L =(Q(St,at; OQ) - yt)2	(1)
2
Under review as a conference paper at ICLR 2018
for yt = rt + Y ∙ max。，Q(st+ι, a0; Θq). Traditionally, the parameterised Q(s, a; θ) is trained by
stochastic approximation, estimating the loss on each experience as it is encountered, yielding the
update:
θt+ι Jθt + α(yt - Q(St, at; θt))VQ(st, at; θt).	(2)
Q-learning methods also require an exploration strategy for action selection. For simplicity, we
consider only the -greedy heuristic.
A few tricks help to stabilize Q-learning with function approximation. Of particular relevance to this
work is experience replay (Lin, 1992): the RL agent maintains a buffer of past experiences, applying
TD-learning on randomly selected mini-batches of experience to update the Q-function.
In this paper, we propose a new formulation of the safety problem. We suppose there exists a
subset C ⊂ S of states that an optimal policy encounters them very rarely or never and denote
them catastrophic states. Moreover, we assume that for some environments, optimal policies are
rarely within a short distance of a catastrophic state. As a measure of distance, we consider steps
in trajectory space. We define the distance d(si, sj ) to be length N of the smallest sequence of
transitions {(st, at, rt, st+1)}tN=1 that traverses state space from si to sj.1
Definition 2.1. Suppose that we are given a priori knowledge that acting according to the optimal
policy π*, an agent never encounters states S ∈ S for which lie within distance d(s, C) < k「for any
catastrophe state c ∈ C. Then each state s for which ∃c ∈ C s.t. d(s, c) < kτ is a danger state.
We also suppose that the agent can recognize the catastrophe states as they are encountered.
Definition 2.2. A catastrophe detector is a function f : S 7→ {0, 1} that returns 1 if and only if a
state is a catastrophe state.
We propose Intrinsic Fear (IF) (Algorithm 1), a novel algorithm for avoiding catastrophes when
learning online with function approximation. In our approach, we maintain both a DQN and a
separate, supervised fear model F : S 7→ [0, 1]. Our fear model F provides an auxiliary source of
reward, penalizing the Q-learner for entering possibly dangerous states.
The goal in modeling danger states is twofold. First, by shaping rewards away from suboptimal states,
we encode prior knowledge about the environment and can thus accelerates learning. Second, when
catastrophic states correspond to especially undesirable outcomes, the learned reward shaping can
protect DQNs, which are susceptible to catastrophic forgetting, from drifting close to catastrophic
states. Owing to this self-assigned reward, once the fear model is trained, a Q-learner might update
to avoid catastrophes without having to actually repeat them, so long as the fear model is not itself
susceptible to catastrophic forgetting. We draw some inspiration from the idea of a parent scolding
a child for running around with a knife. The child can learn to adjust its behavior without actually
having to stab someone. We also draw inspiration from the way humans appear to process traumatic
experience, remembering especially bad events vividly even as most other memories from the same
time period fade. Perhaps this selective memorization of bad events confers a benefit for avoiding
similar outcomes in the future.
Our instantiation of intrinsic fear works as follows: In addition to the DQN, we maintain a binary
classifier that we term a fear model. In our case, we use a neural network of the same architecture
as the DQN (but for the output layer). The fear model’s purpose is to predict the probability that
any state will lead to catastrophe within k moves. Over the course of training, our agent adds each
experience (S, a, r, S0) to its experience replay buffer. As each catastrophe is reached at the nth turn
of an episode, we add the kr (fear radius) states leading up to the catastrophe to a list of danger
states. We add the preceding n - kr states to a list of safe states. When n < kr , all states for that
episode are added to the list of danger states. Then after each turn, in addition to making one update
to the Q-network, we make one mini-batch update to the fear model. To make this update, we sample
50% of samples in the batch from the danger states, assigning them label 1 and the remaining 50%
from the safe states, assigning them label 0.
For each update to the DQN, we perturb the TD target yt. Instead of updating Q(St, at; θQ) towards
rt + maxa0 Q(St+1, a0; θQ), we introduce the intrinsic fear to the model via the target:
yIF = r + max Q(st+ι,a0; Θq) - λ ∙ F(st+ι; Θf)	(3)
a0
1In the stochastic dynamics setting, the distance is the minimum mean passing time between the states.
3
Under review as a conference paper at ICLR 2018
	
Algorithm 1 Training DQN with Intrinsic Fear	
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	Input: Two models: Q (DQN) and F (fear model), fear factor λ, fear phase-in length kλ , fear radius kr Output: Learned parameters θQ and θF Initialize parameters θQ and θF randomly Initialize replay buffer D, danger state buffer DD, and safe state buffer DS Start per-episode turn counter ne for t in 1:T do With probability E select random action at Otherwise, select action at = argmaxa0 Q(st, a0; θQ) Execute action at in environment, observing reward rt and successor state st+1 Store transition (st, at, rt, st+1) in D if st+1 is a catastrophe state then Add states st-kr through st to DD else Add states st-ne through st-kr-1 to DS Sample random minibatch of transitions (sτ , aτ, rτ, sτ+1) from D λτ — min(λ, λλt) rτ - λτ ,	for terminal sτ+1 yτ	r rτ + maxa0 Q(sτ+ι, a0; Θq) - λ ∙ F(s「+1； Θf) for non-terminal s「十ι ʃ θQ J θQ - η ∙ NeQ (yτ - Q(Sτ, aτ； θQ))2 Sample random mini-batch sj with 50% of examples from DD and 50% from DS J 1, forsj∈DD yj J	0, for sj ∈DS Θf - Θf - η ∙ VθfIossf(yj, F(Sj; Θf))
where F(s; θF) is the fear model and λ is a fear factor determining the scale of the impact of intrinsic
fear on the Q-function update.
Note that IF perturbs the objective function. Thus, one might be concerned that the perturbed reward
might indicate a different optimal policy. Fortunately, if the labeled catastrophe states and danger
zone do not violate our assumptions, and if the fear model reaches arbitrarily high accuracy, then this
will not happen.
For an MDP, M = hS, A, T, R, γi, with 0 ≤ γ ≤ 1, the average reward return is as follows:
ηM (π) :
limτ→∞ T EM [PT rt|n]
(I-Y)EM hP∞ Ytrt|ni
if	γ = 1
if	0 ≤ γ < 1
(4)
The optimal policy ∏ of the model M is the policy which maximizes the average reward return,
∏* = max∏∈p η(∏) where P is a set of stationary polices.
Theorem 1.	For a given MDP, M, with Y ∈ [0,1] and a catastrophe detector f, let π* denote an
optimal policy of M, and π denote an optimal policy of M equipped withfear model F and λ. Ifthe
probability π* visits the states in the danger zone is at most G and Rmin ≤ R(s, a) ≤ RmaX, then
ηM ≥ ηM (∏) ≥ ηM,F (∏) ≥ ηM - λe(Rmaχ - Rmin).
(5)
Proof. Appendix A.
It is worth noting that when at least one of the optimal policies of M, does not visit the fear zone
(E = 0), then ηM = ηM,F(∏) and the fear signal can boost UP the process of learning the optimal
policy.
4
Under review as a conference paper at ICLR 2018
Since we learn the catastrophe detector f and fear model F empirically using the collected data, our
RL agent has access to an imperfect detector f and imperfect fear model F, and therefore assumes
the fear model is F. In this case, the RL agent trains with intrinsic fear generated by f, learning
a different value function than the RL agent with perfect f . To show robustness against modeling
errors, we are interested in the average deviation in the value functions of the two agents.
In general, in practical RL problems, We use discount factors γ < 1 (Kocsis and Szepesvari, 2006) in
order to reduce the planing horizon, and computation cost. Moreover, (Jiang et al., 2015) suggests
that When We have estimation (up to the confidence intervals) of our MDP model, it is better to use
smaller discount factors in order to prevent over-fitting to the estimated model. We shoW that under
modeling errors, if the actual objective function to optimize for Eq. 4 has With discount factor Teval ,
it’s better to use some T ≤ Teval because it reduces the average deviation in the value functions.
∏ *
For a given environment, With fear model F1 and discount factor T1, let VF F,2γ,γ2 (s), s ∈ S, denote
the state value function under the optimal policy ofa environment With fear model F2 and the discount
factor T2. On the same environment, let ωFπF2,γ2 (s) denote the stationary distribution over states.
Therefore We are interested in the average deviation on value functions caused by imperfect classifier:
L(F, Fb, γeval, γ) := (1 - γeval)	ωFπFb,γ
s∈S
∏t,	∏ ^
(s) VFπ,Fγ,eγveavlal (s) - VF,Fγbe,γval
(s) ds
Theorem 2.	For a given MDP model, the average deviation on the value functions, L(F, F , Teval , T),
F,F ∈ F, vanishes as the number ofsamples N increases
L = O λ(Rmax - Rmin)
1 - γeval VC (F ) + log 1 *
1-γ
N
Teval - T
1 - T
(6)
with probability at least 1 - δ. VC(F) is the VC dimension of the hypothesis class F.
Proof. Appendix B
Thm. 2, holds for both tabular MDPs and continuous state-action MDPs. In addition to proofs of
these results, We provide a deeper theoretical analysis on deterministic and stochastic fear models in
the tabular setting in Appendix B.
Over the course of our experiments, We discovered the folloWing pattern: Intrinsic fear models are
more effective When the fear radius kr is large enough that the model can experience danger states at
a safe distance and correct the policy, Without experiencing many catastrophes. When the fear radius
is too small, the danger probability is only nonzero at states from Which catastrophes are inevitable
anyWay and intrinsic fear seems not to help. We also found that Wider fear factors train more stably
When phased in over the course of many episodes. So, in all of our experiments We gradually phase
in the fear factor λ from 0 to λ reaching full strength at predetermined time step kλ . In our Cart-Pole
experiments, We phase λ in over 1M steps.
3	Environments
We demonstrate our algorithms on three environments. These include Adventure Seeker, a toy
pathological environments Which We designed to demonstrate the Sisyphean curse; Cartpole, a
classic reinforcement learning environment; and three Atari games, Seaquest, Asteroids, and Freeway,
simulated in the Arcade Learning Environment (Bellemare et al., 2013).
Adventure Seeker We imagine a player placed on a hill, sloping upWard to the right (Figure 1a).
At each turn, the player can move to the right (up the hill) or left (doWn the hill). The environment
adjusts the player’s position accordingly, adding some random noise. BetWeen the left and right edges
of the hill, the player gets more reWard for spending time higher on the hill. But if the player goes
too far to the right, he/she Will fall off (a catrastrophic state), terminating the episode and receiving
a return of 0. Formally, the state consists of a single continuous variable s ∈ [0, 1.0], denoting the
player’s position. The starting position for each episode is chosen uniformly at random in the interval
[.25, .75]. The available actions consist only of {-1, +1} (left and right). Given an action at in
5
Under review as a conference paper at ICLR 2018
(a) Adventure Seeker (b) Cart-Pole
(c) Seaquest (d) Asteroids
(e) Freeway
Figure 1: In experiments, we consider two toy environments (a,b) and the Atari games Seaquest (c),
Asteroids (d), and Freeway (e)
state st, T(st+1 ∣st, at) gives successor state st+1 - st + .01 ∙ at + η where η 〜 N(0, .012). The
reward at each turn is equal to st (proportional to height). The player falls off the hill, entering the
catastrophic terminating state, whenever st+1 > 1.0 or st+1 < 0.0.
This game admits an obvious analytic solution; There exists some threshold above which the agent
should always choose to go left, and below which it should always go right. And yet a state-of-the-art
DQN model learning online or with experience replay successively plunges to its death. To be clear,
the DQN does learn a near-optimal thresholding policy quickly. But over the course of continued
training, the agent oscillates between a reasonable thresholding policy and one which always moves
right, regardless of the state. The pace of this oscillation evens out and all networks (over multiple
runs) quickly reach a constant catastrophe per turn rate that does not attenuate with continued training.
How could we trust a system that can’t solve Adventure Seeker to make consequential decisions?
Cart-Pole In this classic RL environment, an agent balances a pole atop a cart (Figure 1b). Qualita-
tively, the game exhibits four distinct catastrophe modes. The pole could fall down to the right or fall
down to the left. Additionally, the cart could run off the right boundary of the screen or run off the
left. Formally, at each time, the agent observes a four-dimensional state vector (x, v, θ, ω) consisting
respectively of the cart position, cart velocity, pole angle, and the pole’s angular velocity. At each
time step, the agent chooses an action, applying a force of either -1 or +1. For every time step that
the pole remains upright and the cart remains on the screen, the agent receives a reward of 1. If the
pole falls, the episode terminates, giving a return of 0 from the penultimate state. In experiments,
we use the implementation CartPole-v0 contained in the openAI gym (Brockman et al., 2016). Like
Adventure Seeker, this problem admits an analytic solution. A perfect policy should never drop the
pole. But, as with Adventure Seeker, a DQN converges to a constant rate of catastrophes per turn.
Atari games In addition to these pathological cases, we address Freeway, Asteroids, and Seaquest,
games from the Atari Learning Environment. In Freeway, the agent controls a chicken with a goal of
crossing the road while dodging traffic. The chicken loses a life and starts from the original location
if hit by a car. Points are only rewarded for successfully crossing the road. In Asteroids, the agent
pilots a ship and gains points from shooting the asteroids. She must avoid colliding with asteroids
which cost it lives. In Seaquest, a player swims under water. Periodically, as the oxygen gets low, she
must rise to the surface for oxygen. Additionally, fishes swim across the screen. The player gains
points each time she shoots a fish. Colliding with a fish or running out of oxygen result in death. In
all three games, the agent has 3 lives, and the final death is a terminal state. We label each loss of a
life as a catastrophe state.
4	Experiments
To assess the effectiveness of the intrinsic fear model, we evaluate both a standard DQN (DQN-
NoFear) and one enhanced by intrinsic fear (DQN-Fear). In both cases, we use multilayer perceptrons
(MLPs) with a single hidden layer and 128 hidden nodes. We train all MLPs by stochastic gradient
descent using the Adam optimizer Kingma and Ba (2015) to adaptively tune the learning rate.
6
Under review as a conference paper at ICLR 2018
(d) Seaquest (reward)
(e) Asteroids (reward)
(f) Freeway (reward)
Figure 2: Catastrophes and reward/episode for DQNs and Intrinsic Fear. On Adventure Seeker, all
Intrinsic Fear models cease to “die” within 14 runs, giving unbounded (unplottable) reward thereafter.
On Seaquest, the IF model achieves a similar catastrophe rate but significantly higher total reward.
On Asteroids, the IF model outperforms DQN. For Freeway, a randomly exploring DQN (under our
time limit) never gets reward but IF model learns successfully.
Because, for the Adventure Seeker problem, an agent can escape from danger with only a few time
steps of notice, we set the fear radius kr to 5. We phase in the fear factor quickly, reaching full
strength in just 1000 moves. On this problem we set the fear factor λ to 40.
For Cart-Pole, we set a wider fear radius of kr = 20. We initially tried training this model with a
shorter fear radius but made the following observation. Some models would learn well surviving
for millions of experiences, with just a few hundred catastrophes. This compared to a DQN (Figure
2) which would typically suffer 4000-5000 catastrophes. When examining the output from the fear
models on successful vs unsuccessful runs, we noticed that the unsuccessful models would output
danger of probability greater than .5 for precisely the 5 moves before a catastrophe. But by that time
it would be too late for an agent to correct course. In contrast, on the more successful runs, the fear
model typically outputs predictions in the range .1 - .5. We suspect that the gradation between mildly
dangerous states and those with imminent danger provides a richer reward signal to the DQN.
On both the Adventure Seeker and Cart-Pole environments, the DQNs augmented by intrinsic fear far
outperform their otherwise identical counterparts (Figure 2). We cannot plot the reward per episode
for the intrinsic fear models on these environments because after the first several deaths, the episodes
never terminate. In contrast, both the DQN and related approaches like expected SARSA continue to
visit the catastrophic states regularly. We compared our approach against some traditional approaches
for mitigating catastrophic forgetting. For example, we tried a memory-based method in which we
preferentially sample the catastrophic states for updating the model, but they did not improve over
the DQN. It seems that the notion of a danger zone is necessary here.
For Seaquest, Asteroids, and Freeway, we use a fear radius of 5 and a fear factor of .5. For all
Atari games, the IF models outperform their DQN counterparts. Interestingly while for all games,
the IF models achieve higher reward, on Seaquest, models trained with Intrinsic Fear have similar
catastrophe rates. More precisely, they appear to have fewer catastrophes early on but eventually
enter a different reward regime, exchanging more catastrophes for higher reward. This result suggests
an interplay between the various reward signals that warrants further exploration. For Asteroids and
Freeway, the improvements are more dramatic. Over just a few thousand episodes of Freeway, a
randomly exploring DQN achieves zero reward. However, the reward shaping of intrinsic fear leads
to rapid improvement.
5	Related work
The paper addresses safety in RL, intrinsically motivated RL, and the stability of Q-learning with
function approximation under distributional shift. Our work also has some connection to reward
7
Under review as a conference paper at ICLR 2018
shaping. We attempt to highlight the most relevant papers here. Several papers address safety in RL.
(Garcia and Fernandez, 2015) provide a thorough review on the topic, identifying two main classes of
methods: those that perturb the objective function and those that use external knowledge to improve
the safety of exploration.
While a typical reinforcement learner optimizes expected return, some papers suggest that a safely
acting agent should also minimize risk. (Hans et al., 2008) defines a fatality as any return below some
threshold τ . They propose a solution comprised of a safety function, which identifies unsafe states,
and a backup model, which navigates away from those states. Their work, which only addresses
the tabular setting, suggests that an agent should minimize the probability of fatality instead of
maximizing the expected return. Heger (1994) suggests an alternative Q-learning objective concerned
with the minimum (vs expected) return.Other papers suggest modifying the objective to penalize
policies with high-variance returns (Garcia and Fernandez, 2015). Maximizing expected returns
while minimizing their variance is a classic problem in finance, where a common objective is the ratio
of expected return to its standard deviation (Sharpe, 1966). (Moldovan and Abbeel, 2012) gives a
definition of safety based on ergodicity. They consider a fatality to be a state from which one cannot
return to the start state. Shalev-Shwartz et al. (2016) theoretically analyzes how strong a penalty
should be to discourage accidents. They also consider hard constraints to ensure safety. None of the
above works address the case where distributional shift dooms an agent to perpetually revisit known
catastrophic failure modes. Other papers incorporate external knowledge into the exploration process.
Typically, this requires access to an oracle or extensive prior knowledge of the environment. In the
extreme case, some papers suggest confining the policy search to the subset of policies known to be
safe. For reasonably complex environments or classes of policies this seems infeasible.
The potential oscillatory or divergent behavior of Q-learners with function approximation has been
previously identified (Boyan and Moore, 1995; Baird et al., 1995; Gordon, 1996). Outside of RL, the
problem of covariate shift has been extensively studied (Sugiyama and Kawanabe, 2012). Murata
and Ozawa (2005) addresses the problem of catastrophic forgetting owing to distributional shift in
RL with function approximation, proposing a memory-based solution. Many papers address intrinsic
rewards, which are internally assigned, vs the standard (extrinsic) reward. Typically, intrinsic rewards
are used to encourage exploration (Schmidhuber, 1991; Bellemare et al., 2016) and to acquire a
modular set of skills (Chentanez et al., 2004). Some papers refer to the intrinsic reward for discovery
as curiosity. Like classic work on intrinsic motivation, our methods perturb the reward function.
But instead of assigning bonuses to encourage discovery of novel transitions, we assign penalties to
discourage catastrophic transitions.
Key differences In this paper, we undertake a novel treatment of safe reinforcement learning,
While the literature offers several notions of safety in reinforcement learning, we see the following
problem: Existing safety research that perturbs the reward function requires little foreknowledge,
but fundamentally changes the objective globally. On the other hand, processes relying on expert
knowledge may presume an unreasonable level of foreknowledge. Moreover, little of the prior work
on safe reinforcement learning, to our knowledge, specifically addresses the problem of catastrophic
forgetting. This paper proposes a new class of algorithms for avoiding catastrophic states and a
theoretical analysis supporting its robustness.
6	Conclusions
Our experiments demonstrate that DQNs are susceptible to periodically repeating mistakes, however
bad, raising questions about their real-world utility when harm can come of actions. While it’s easy to
visualize these problems on toy examples, similar dynamics are embedded in more complex domains.
Consider a domestic robot acting as a barber. The robot might receive positive feedback for giving
a closer shave. This reward encourages closer contact at a steeper angle. Of course, the shape of
this reward function belies the catastrophe lurking just past the optimal shave. Similar dynamics
might be imagines in a vehicle that is rewarded for traveling faster but could risk an accident with
excessive speed. Our results with the intrinsic fear model suggest that with only a small amount of
prior knowledge (the ability to recognize catastrophe states after the fact), we can simultaneously
accelerate learning and avoid catastrophic states. This work represents a first step towards combating
some issues relating to safety in RL stemming from catastrophic forgetting.
8
Under review as a conference paper at ICLR 2018
References
Leemon Baird et al. Residual algorithms: Reinforcement learning with function approximation.
1995.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. J. Artf InteIL Res.(JAIR), 47:253-279,
2013.
Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. In NIPS, 2016.
Justin Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximating
the value function. In NIPS, 1995.
Greg Brockman et al. OpenAI gym. arXiv:1606.03152, 2016.
Nuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement
learning. In NIPS, 2004.
Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, and Kaheer Suleman. Policy networks with
two-stage training for dialogue systems. In SIGDIAL, 2016.
Javier Garcia and Fernando Ferndndez. A comprehensive survey on safe reinforcement learning.
JMLR, 2015.
Geoffrey J Gordon. Chattering in SARSA(λ) - a CMU learning lab internal report. 1996.
Steve Hanneke. The optimal sample complexity of pac learning. Journal of Machine Learning
Research, 17(38):1-15, 2016.
Alexander Hans, Daniel SchneegaB, Anton Maximilian Schafer, and Steffen Udluft. Safe exploration
for reinforcement learning. In ESANN, 2008.
Matthias Heger. Consideration of risk in reinforcement learning. In Machine Learning, 1994.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous
Agents and Multiagent Systems, pages 1181-1189. International Foundation for Autonomous
Agents and Multiagent Systems, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Levente Kocsis and Csaba Szepesvdri. Bandit based monte-carlo planning. In ECML, volume 6,
pages 282-293. Springer, 2006.
Sergey Levine et al. End-to-end training of deep visuomotor policies. JMLR, 2016.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 1992.
Zachary C Lipton et al. Efficient exploration for dialogue policy learning with BBQ networks &
replay buffer spiking. In NIPS Workshop on Deep Reinforcement Learning, 2016.
James L McClelland, Bruce L McNaughton, and Randall C O’Reilly. Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures of
connectionist models of learning and memory. Psychological review, 1995.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of learning and motivation, 1989.
Volodymyr Mnih et al. Human-level control through deep reinforcement learning. Nature, 2015.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In ICML,
2012.
9
Under review as a conference paper at ICLR 2018
Makoto Murata and Seiichi Ozawa. A memory-based reinforcement learning model utilizing macro-
actions. In Adaptive and Natural Computing Algorithms. Springer, 2005.
Will Night. The AI that cut google’s energy bill could soon help you. MIT Tech Review, 2016.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Jurgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural
controllers. In From animals to animats: proceedings of the first international conference on
simulation of adaptive behavior (SAB90). Citeseer, 1991.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv:1610.03295, 2016.
William F Sharpe. Mutual fund performance. The Journal of Business, 1966.
David Silver et al. Mastering the game of go with deep neural networks and tree search. Nature,
2016.
Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:
Introduction to covariate shift adaptation. MIT Press, 2012.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
1988.
Richard S. Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 1998.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279-292, 1992.
10
Under review as a conference paper at ICLR 2018
ηM(n)=妈∞ TE
A Loss in Optimal Value (Proof of Theorem 1)
The average return of the reward under a policy π is as follows:
1T
IrIM(π) = Iim 万E ErtIn .	⑺
T→∞ T
t
Let us assume that any stationary policy π induces a stationary distribution ωπ(s), s ∈ S. Therefore
we can rewrite Eq. 7in terms of stationary distribution (Puterman, 2014).
ErtIn =	ωπ (s)n (aIs)R(s, a) .
In RL, We are interested in a policy n* that maximizes the expected average reward:
n* := arg max ηM (n).
π
Denote ηM = ηM(n*). In a first place, the optimization in Eq. 7 looks linear in n but actually the
policy n derives the stationary distribution ω∏ (∙), which makes the optimization problem harder.
Given the policy n, let’s define the joint distribution in (s, a) as follows:
μ∏(s, a) := P(s, a∣n) = ω∏(s)n(a∣s), ∀s ∈ S, a ∈ A.
Then we can rewrite the optimization problem in terms of thejoint probability distribution μ∏.
ηm ( μπ ) : = ΣΣμ∏(s, a)R(s, a) .	(8)
s∈S a∈A
We can see that this new formalization, turns our optimization objective into a linear function of μ∏.
Since μ∏ is a join distribution of (s, a) under the model dynamics T, it can not take any arbitrary
value. Let ∆ denote the set of feasible value for μ∏, then
△ := {μ : μ ≥ 0, ^X μ(s, a)= 1, ^X μ(s0, a0) = ^X T(s0∣s, a)μ(s, a), ∀s0 ∈ S} .	(9)
Clearly, △ is a polytope on the simplex in RS×A .
Now, we can rewrite the optimization problem Eq. 7asa constrained linear program on μ∏:
ηM = maxΣΣμ(s, a)R(s, a).
μ ∈ z∆
s∈S a∈A
This change of variable allows us to analyze the introduction of intrinsic fear in different situations.
Among all the optimal policies of Eq. 7, consider the one which minimizes E := Ps∈c 0 μ∏* (s, a).
let’s assume that the negative reward assigned to the states in the danger zone is λ(Rmax - Rmin)
and the optimal policy n of the environment with the intrinsic fear has return of ηM,F (n).
Applying policy n* on the environment with intrinsic fear gives a return of ηM - λe(Rmaχ - Rmin).
Therefore, the return of n on the environment with intrinsic fear, ηM,F, is lower bounded by
ηM - λe(Rmaχ - Rmin). Therefore, applying n on the environment without intrinsic fear gives a
return of ηM(n) which is lower bounded by ηM - λe(Rmaχ - Rmin).
ηM ≥ ηM (n) ≥ ηM,F (n) ≥ ηM - λe(Rmaχ - Rmin).	(10)
A.1 Discounted Cumulative Reward
For the γ-discounted setting, we are interested in
η(n) = (1 - γ) lim E	γtrt
T →∞ t=0
(11)
The above mentioned equations hold in this setting as well, i.e., ηM ≥ ηM(n) ≥ ηM,F(n) ≥
ηM - λE(Rmax - Rmin ).
11
Under review as a conference paper at ICLR 2018
B Imperfect Classifier (Proof of Theorem 2)
In the previous section, we assumed that we have access to the perfect classifier F which can exactly
label the danger zone. This assumption does not hold in real world where we train the classifier. In
this section we derive an analysis in order to show that imperfect classifier F can not change the
overall performance by much.
In general, in practical RL problems, We use discount factors Yeval < 1 (Kocsis and Szepesvdri,
2006) in order to reduce the planing horizon, and computation cost. Moreover, (Jiang et al., 2015)
suggest that When We have an estimation (up to the confidence intervals) of our MDP model, it is
better to use γ ≤ γeval. They shoW that since larger discount factor enriches the class of optimal
policies for a given set of plausible models, large discount factors enrich models and end up over
fitting to the noisy estimate of the environment.
In this section, We shoW hoW to choose the discount factor γ ≤ γeval such that the learned Value
∏ *
function stays close to the Value function under the perfect classifier F is perfect. Let, VFF1,γ1 (s),
2 ,γ2
s ∈ S, denote the state value under the optimal policy of model With classifier F1 under the discount
factor γ1 on the environment equipped With classifier F and discount factor γ2. On the same
environment, ωFπF1,γ1(s) denotes the stationary distribution over states. We are interested in the
average deviation on value functions caused by the imperfect classifier:
π*	π*	π*
L:= (1 - γeval) X ωFFb,γ (s) VFπ,Fγ,eγveavlal (s) - VF,Fγbe,γval (s)
s∈S
This quantity can be upper bounded by
π*	π*
L ≤ (1 - γeval)kVFπ,Fγ,eγveavlal -VF,Fγbe,γvalk∞	(12)
The goal is to find an γ* which minimizes this loss, i.e. γ* = argminγ≤γevaι L with high probability.
For simplicity and Without loss of generality, let’s assume that all the reWards, including the intrinsic
fears, are in [0, 1] and call λ0, the transformed version of λ 2. One can decompose the upper bound in
Eq. 12 as follows:
π*	π*
VFπ,Fγ,eγveavlal(s) -VF,Fγbe,γval(s)
+
*
π
γeval (s) - VF,Fγbe,γval (s)
(13)
The first term is the deviation on value function when applying same policy on the same environment
π*	π*
but with different discount factors. Since γ ≤ γeval we have VF,γ,γeval (s) ≤ VF,γ,γeval (s).
π*	π*
VF,Fγ,eγveavlal (s) - VF,Fγ,γeval (s) = EF
∞
EYtvalrt|s0 = S,nF“al - EF
t=0
∞
Ytrt|s0
t=0
*
s, πF,γeval
(14)
EF X (Yeval-Yt)rtls0 = "F T ≤ (i-⅛ - 士)
(15)
The second part of Eq. 13 is the deviation in value function under different policies and different
πb
classifiers. Again, since Y ≤ Yeval, we have VF,Fγ,γ
γeval
(s)-
*
π
V F,γ
F,γeval
π*
(s) ≥ VF,Fγb,γ (s)
(16)
2Shifting and then rescaling the reward is equivalent to shifting and rescaling the Q and Value function, and
does not change the optimal policy. Moreover the mentioned transformation is r → (r - (Rmin - λ(Rmax -
Rmin)))/((Rmax - Rmin)(I + λ)), therefore, λ0 = λ∕(1 + λ)
12
Under review as a conference paper at ICLR 2018
where the last inequality is due to the optimality of ∏F Y on the environment of F,γ. To bound this
part we exploit the proof trick used in (Jiang et al., 2015).
VFF,γ(s) - VFF,γ (s) = (VF⅛γ (s) - V∏Fγγ (s)) + (V⅞γ(s) - V⅛γ (s)) +
(17)
since the middle term is negative we have
"*	π-
VFπ,Fγ,γ(s)-VF,Fγb,γ(s)≤
- VFbπ,F*γ,γ (s)	+
≤ 2 {π*maπx* } γ(s) - VFπ,γ(s)
{πFb,γ,πFb,γ}
(18)
This quantity Vbπ (s) - VFπ,γ (s) is the difference between the performance of the same policy on two
different environments. These two values functions should satisfy the following bellman equations:
V∏γf(S) = R(s, ∏(s)) + λ0F(S) + YE T(SlS, ∏(s))VF,γ(s0)
s0∈S
VFyS) = R(s, π(s)) + λ0F(s) + γ X T(SlS, ∏(s))V∏,γ(s0)
,	s0∈S	,
To compute the solution two this equation, we use dynamic programing. Let initialize V0 , V = V(an
arbitrary value) and construct the following updates.
for i ∈ {1, . . . ∞}
Vn(s) = R(s, π(s)) + λ0F(s) + γ X T(s0∣s, ∏(s))V-i(s)
s0∈S
Vbiπ(S) = R(S, π(S)) + λ0Fb(S) + γ	T (S0|S, π(S))Vbiπ-1(S0)
s0∈S
As i tends to infinity, these two dynamics updates converge to
bound the right hand side of Eq. 18 we have
VFπ,γ (S), and VFbπ,γ (S) respectively.
To
Vn(s) - Vn(s) = λ0F(s) - λ0F(S) + YE T(SlS,∏(s))
s0∈S
i
≤ λ0 X Yi0 max F (S) - Fb(S)
i0=0	s
(19)
As i tends to infinity, we have
max lim
π i→∞
i
≤ λ0 X Yi0 max ∣F (S) - Fb(S)∣ ≤ λ0 max
s,π	s,π
i0=0
∣F(s) - F(S)I
(1 - Y)
B.1 Lookup Table Classifier
If we consider the fear model as a lookup table, and deterministic, then observing each state once is
enough to exactly recover the classifier.
For the stochastic F, at time step N
∣F(S) - F(S)I ≤ M
(20)
13
Under review as a conference paper at ICLR 2018
with probability δ where N(s) is the number visits to a state s at time step N. The trajectory produced
by algorithm does not produce i.i.d. samples of state. Therefore, for Eq. 20 we use Hoeffding’s
inequality accompanied with union bound over time N. In order to have this bound to hold for all the
states at once, we need another union bounds over states and all possibly optimal policies Πγ under
noisy classifier, which requires to replace δ → δ∕SAΠγ. Let's assume a minimum number of visit
N to each state,
*	*	W	/]Ct NSAnY
M γ,γ - VF,Y,γk∞ ≤ L《 N δ	(21)
Finally, adding Eq. 14 and Eq. 21, the upper bound on L is as follows:
1	N	NSAn
L ≤ λ 1 一 Yeval . Iog	1-δ
-	1 - Y V N
Yeval - Y
1 - Y
B.2 Classifier from a set of functions
Let F denote a set of given binary classifiers and F ∈ F. In this case, let’s assume that we are given
πF*b
a set of N i.i.d samples from the stationary distribution ωFF,γ . Given a policy π, the MDP transition
process reduces to a Markov chain with transition probability Tπ . Now we rewrite the Eq. 19
in a matrix format where Viπ , F ∈ RS are vectors of concatenation of Viπ (s) and F (s), ∀s ∈ S
respectively.
Viπ -	Vbiπ	=λ0F	-λ0Fb+YTπ	Viπ-1	- Vbiπ-1	≤λ0X(YTπ)i0	F -Fb	(22)
i0=0
as i goes to infinity we have
VFF - vFπF,γ ≤ λ (1 - yTπ)-1(F - F)
Using PAC analysis of binary classification in (Hanneke, 2016) a follow up to (Vapnik, 2013), we
have
I ^∣> ∏b	VC(F) + log1
∣F - F∣ ω鼠 ≤ 3200 —
with probability at least 1 - δ where VC(F) is the VC dimension of the hypothesis class and | ∙ | is
entry-wise absolute value. Since y < 1, then ɑmaχ, the maximum eigenvalue of (1 - YTπ)-1, is
bounded above and we have
kV⅛γ - V⅛γkι ≤ 3200λ0αmaχ VC(FN+log 1
and therefore,
L ≤ 3200λ0αmax(1 - Yeval)
VC (F) +log 1
N
Yeval - Y
1 - Y
The remaining part is to solve
Y* = argminγ≤Yeval L
to find the optimal Y .
The same analysis, up to a slight modification3 *, holds for the continuous state and action spaces.
3Instead of having V as a vector of state values indexed by states, it is a continuous function of states.
Furthermore, the Transition kernel is over continuous distribution therefore the same bellman update in Eq. 22
holds.
14