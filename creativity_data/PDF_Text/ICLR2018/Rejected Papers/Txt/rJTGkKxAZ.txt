Under review as a conference paper at ICLR 2018
Learning Generative Models with Locally
Disentangled Latent Factors
Anonymous authors
Paper under double-blind review
Ab stract
One of the most successful techniques in generative models has been decompos-
ing a complicated generation task into a series of simpler generation tasks. For
example, generating an image at a low resolution and then learning to refine that
into a high resolution image often improves results substantially. Here we explore
a novel strategy for decomposing generation for complicated objects in which we
first generate latent variables which describe a subset of the observed variables,
and then map from these latent variables to the observed space. We show that
this allows us to achieve decoupled training of complicated generative models and
present both theoretical and experimental results supporting the benefit of such an
approach.
1	Introduction
Learning useful intermediate representations in a hierarchical manner has been a driving factor in
the recent success of deep learning (Goodfellow et al., 2016) (Krizhevsky et al., 2012). When ample
amounts of labelled data are available, supervised learning methods are successful in learning useful
intermediate representations (Zeiler & Fergus, 2014) (Nguyen et al., 2016a). However, the task
is significantly more challenging in the context of unsupervised learning. One such approach to
unsupervised learning is to learn a generative model of high-dimensional observed variables with
low-dimensional latent variables, such that the latent variables capture the salient features of the
data, which could then be used for other upstream tasks.
Recently, there has been an increased interest in learning hierarchical latent variable generative
models. While a typical latent variable generative model uses multilayer neural networks, the gen-
erative process still assumes a single latent variable structure which generates the observed variable.
The stochasticity of the observed variables has to be completely captured in a single latent vector.
A hierarchical latent variable model posits that the data could be better explained by a sequential
process with multiple latent variables, z0 → ... → zn-1 → x. The motivation for hierarchical latent
variables follows from the analogous result in supervised representation learning (Lin et al., 2017)
for deterministic intermediate representations, where the authors argue that a hierarchical neural
network uses capacity exponentially more efficiently.
Recent work by (Zhao et al., 2017) argues why hierarchical latent variables models are often not
able to take advantage of the hierarchy, and only the lowest-level latent variables learn any useful
representations. We posit that this is possibly because the vanilla hierarchical latent variable struc-
ture by itself only adds a very weak prior (that of devoting more processing to higher-level latent
variables). When parameterizing the conditional distributions with powerful deep neural networks,
this could admit a local optima in which all factors of variation are sub-optimally explained by the
lowest-level latent variable. Notably, this phenomenon was also common in supervised training
of deep neural networks, before (Ioffe & Szegedy, 2015) introduced batch normalization, which
successfully disentangles the learning dynamics at each layer, as if each layer has an independent
objective function.
Recent success in hierarchical latent variable models (Reed et al., 2017; Zhang et al., 2016; Nguyen
et al., 2016b) contribute motivating examples of successful hierarchical latent variable models. Such
approaches either use a resolution-based hierarchy or extract the hierarchy from a discriminative
model trained with labelled data. These examples give sufficient evidence of the existence of good
hierarchical latent variable models, but discover them using overly strong priors or label information.
1
Under review as a conference paper at ICLR 2018
For example, the resolution-based hierarchy is well suited to images because lower resolution images
capture some factors of variation (such as objects) but discards other factors of variation (such as
texture and details), giving the low and high level models distinct responsibilities. However, this
decomposition is a strong prior and may not work well or apply to other types of data (for example,
it is not clear how it would apply to language or video).
This motivates the need for an unsupervised method for learning hierarchical latent variables with a
requisite but general prior to facilitate disentangled learning dynamics in each level.
Our proposed approach, which we call Locally Disentangled Factors (ldf), has the following de-
sired features:
•	Decoupled level-wise training objectives which significantly accelerate training.
•	A graphical model based on spatial locality which aids in credit assignment, and can be
thought of as a generalization to resolution-based hierarchies.
•	Vastly reduced memory consumption which allows training generative models on large
objects, such as videos, where this is known to be a prohibitive limitation.
•	Applicable to variable-length objects, such as videos and text.
2	Proposed Approach
2.1	Motivating Example
Consider, for instance, a photograph of a busy street. Such an image has various types of stochas-
ticity. At the highest level, consider a factor that specifies the time of day, or even the season. Such
a high-level factor of variation affects the observation drastically. Every pixel could be different
based on this factor. Moreover, every other factor of variation depends on this highest-level factor of
variation. For example, the clothing of a person depicted in the picture, which is a mid-level latent
variable, depends on the season factor. At the lowest-level, we have details such as leaves, texture,
etc. Such low-level factors explain pixels in a small region.
The above example motivates us to introduce a spatial structure to the hierarchy, i.e. assign the
lowest-level latent variables to only model small regions of the image. The subsequent level then
models a small local group of latent variables below it, and so on. At the end, the topmost latent
variable models all the latent variables below it and, thus, indirectly the whole image.
2.2	Hierarchical Latent Variable Generative Models
Let X = {x(i)}iN=1 be a dataset of samples of random variable x drawn from a distribution P(x).
We now assume that the data is generated by a stochastic process defined by the Bayesian network
z0 → ... → zn-1 → x
such that the joint distribution factorizes as
P(z0, ...,zn-1,x) = P(z0) Y P(zi|zi-1) P(x|zn-1)
Our approach uses the adversarially learnt inference (ALI) framework (Dumoulin et al., 2017),
which matches the generative process and the inference process using the generative adversarial
learning framework (Goodfellow et al., 2014).
Consider the joint distribution specified by the inference process
Q(z0, ...,zn-1,x) = Q(x)Q(zn-1 |x)	Y Q(zi-1|zi) .
2
Under review as a conference paper at ICLR 2018
Consider also a discriminator Di(zi, zi+1), which takes as input samples from two consecutive
levels, and attempts to learn whether these samples are from the generative process or the inference
process.
The task for the generative and inference processes is to converge to the same distribution such that
the discriminator is not able to to differentiate between the two.
The objective is to find the saddle-point of the following minmax game for each pair of consecutive
random variables zi, zi+1, where zn is taken to mean xi+1.
inf sup E	[logDi(zi,zi+1)] - E [log(1 - Di(zi,zi+1))]
P,Q D Zi,Zi+ι〜P	Zi,Zi+ι〜Q
As is shown in (Goodfellow et al., 2014; Nowozin et al., 2016), the Nash equilibrium of this min-
imax game results in the inference and generative distributions minimizing the Jensen-Shannon
Divergence which is minimized when P ≈ Q, which is our desired goal.
The ldf approach can be seen as a hierarchical variant of Adversarially Learned Inference (Du-
moulin et al., 2017) where each level of latent variables is constrained to follow a prior, which
allows us to decouple each level.
3	Locally Disentangled Factors
The above is a general framework for estimating latent variable generative models. We further
introduce local connectivity and disentanglement in latent variables. Local connectivity further in-
troduces independence assumptions in the conditional distributions of the generative process. For
simplicity let’s assume that the random variable zi is a d-dimensional vector.
Disentanglement As popularized by (Kingma & Welling, 2013), disentanglement by factorization
assumes that the conditional distribution is independently factorizable, i.e.
d
P(zi+1|zi) = Y P(zi+1,j |zi)
j=1
where zi+1,j is the jth element of the vector zi+1.
Local Connectivity While local connectivity has a clear analogy to convolutions (LeCun et al.,
1998) in supervised learning, we believe this is the first use of local connectivity in latent variable
generative models. The local connectivity independent assumption is given by
P (zi+1,j |zi) = P (zi+1,j |zi,j-p:j+p)
Here, zi,j-p:j+p is a slice of the vector comprising of elements with index between j - p andj +p.
3.1	Implementation Details
For simplicity, we only considered hierarchies with two levels, although conceptually ldf could
work with more than two levels. For videos, the bottom level is a single image and the top level is
the collection of images across time. For images we cut the image into a grid (for example 64x64 is
cut into a 4x4 grouping of 16x16 patches).
The training procedure is bottom up, starting from the leaves (observed variables), we learn dis-
entangled latent factors at each node which also are able to reconstruct the observed variables. In
theory if Adversarially Trained Inference is trained to optimality and with a stochastic decoder, the
reconstructions (formed by running the inference network on real data points and then the genera-
tion network on the inferred latent states) should be exactly identical. Conceptually this works with
reconstruction penalties or an ALI objective (Dumoulin et al., 2017), and in our case we use both
3
Under review as a conference paper at ICLR 2018
Figure 1: Diagram illustrating the training procedure for locally disentangled factors. On the left we
train an ALI network on each locally disentangled factor with shared parameters. On the right we
train a global generator network to produce these disentangled latent factors.
Figure 2: Illustration of a simple task where using locally disentangled factors would be expected
to make training easier. In this case, we consider a video dataset where a given shape moves a little
bit in each frame. Each frame can be constructed exactly at the pixel level from two independent
latent factors: the shape of the object and its position. In the pixel space, a large fraction of the
pixels have correlated values between different frames, but in the disentangled latent space, different
independent aspects can be modeled separately (shape and movement).
the normal ALI objective as well as “shortcut reconstructions” which go through the final hidden
layer before the latent variables.
In all cases we used the stabilizing regularization objective for all GAN discriminators (both higher
and lower levels) (Roth et al., 2017).
3.2	Learning with limited samples from the joint distribution
Hierarchical models come with a certain statistical benefit: individual factors (e.g. a model for the
frames of a video, or patches of an image) are easier to learn. For example, learning the marginal
distribution for the frames of video sequences only requires data on individual frames. Importantly,
those samples do not have to be drawn from the joint distribution. Then assuming that the depen-
dencies between the factors can be described by a few parameters, one would only need a handful
of samples from the joint to learn them.
We see this in practice in our experiments in Section 5; here we give a very simple argument on a toy
example. Consider the class of zero-mean, p-variate Gaussian distributions and samples, (X(t))t,
corrupted by additive, isotropic Gaussian noise of variance σ2 per covariate. Learning the distri-
bution from samples reduces to estimating its covariance matrix, Σ. Classic results suggest that,
4
Under review as a conference paper at ICLR 2018
for a constant-rank Σ, n = O(σ4p) samples from the joint, p-variate distribution are necessary and
sufficient for recovery (Johnstone, 2001; Baik et al., 2005).
Now consider a hierarchical model that splits its p variables into k blocks, Xi (t), for i ∈ 1 . . . k.
The covariance matrix can be broken UP into its block components, Σj ∈ Rm, where m = (P). All
the parameters on the main block diagonal, ∑u can be learned using k sets of O(σ4p∕k) samples
from the (p/k)-variate marginals. Now if cross-covariances are modeled approximately Using a
parametric model or trainable upper level, like in our experiments, we will only need enough samples
from the joint to learn those parameters.
4	Related Work
Resolution-based Hierarchy
The resolution hierarchy approach involves initially generating at a low resolution and then gener-
ating at higher resolutions while conditioning on the lower resolution and it has had great success
in the generative models literature (Zhang et al., 2016; Reed et al., 2017; Denton et al., 2015). This
is distinct from ldf but shares a closely related motivation. The resolution hierarchies approach
decomposes generation into multiple stages, but the content of the higher levels is fixed (it is the
downsampled, lower resolution version of the original image). This can simplify the task of training
because different stages in generation become responsible for different factors of variation: gen-
erating low resolution images requires knowledge about the general shape and location of objects,
whereas generating high resolution images conditioned on the low resolution images requires greater
knowledge about the fine-grained details of images. In ldf, the higher levels of the hierarchy are
learned to be representations which capture the details at the lower levels but are disentangled or
nearly disentangled. Because the latent factors in ldf are supposed to capture all of the details at
the lower level, it is possible to do decoupled training across different local factors, whereas this is
not generally possible with Resolution-based Hierarchies because downsampling can remove details
(for example, drawing a person’s eye color needs to be done the same way in the two eyes in a face
image, and this detail of eye color may not be present in a lower resolution version of the image). In
addition resolution-based downsampling may perform poorly or not be applicable to certain types
of data - for example it is not clear how it would be defined for language data.
Synthetic Gradients
Jaderberg et al. (2016) introduced the idea of improving the scalability of training deep networks by
decoupling the computations for making an update to the parameters of each layer. The synthetic
gradients approach trains a network for each layer which takes states of that layer as input and
estimates the gradient of the loss with respect to that layer. While this still requires computing
gradients through the entire network, additional updates can be made in a decoupled fashion by
using these synthetic gradient modules. Like Synthetic Gradients, Locally Disentangled Factors
allows for decoupled updating. However, unlike Synthetic Gradients, Locally Disentangled Factors
relies on the statistical properties of the data to achieve decoupled training - more specifically it
assumes that the selected local regions can be efficiently described by disentangled latent factors.
On the other hand, the decoupling in synthetic gradients is achieved when the synthetic gradient
modules are able to successfully predict the full gradients (or at least produce gradients that work
equally well for training). At a first glance, these two approaches are complementary, but exactly
how they relate could be an interesting topic for future work.
Adversarial Message Passing
Karaletsos (2016) proposed to perform training and inference in graphical models by using local
discriminators which only see the variables in local factors. This is related to ldf in that both use
discriminators which only see a subset of the variables, yet differs in that no part of the Adversarial
Message Passing objective explicitly encourages the lower level latent variables to learn disentangled
representations.
Learning Hierarchical Features from Generative Models
Zhao et al. (2017) presents a critique of hierarchical latent variable models, which essentially argues
that the value of having multiple layers in the hierarchy is limited, because sampling from the joint
5
Under review as a conference paper at ICLR 2018
distribution can be achieved by doing blocked Gibbs sampling between the observed variables and
the lowest level of latent variables. They demonstrate that this prevents hierarchical variational au-
toencoders from learning useful latent variables from the higher levels. An important assumption in
their critique is that the Markov chain that is sampled on the lower levels is ergodic, which is gener-
ally the case when the noise injected in the lower levels of the hierarchy is Gaussian. However, since
the lower level of our hierarchy is an ALI network, we can make this sampling process potentially
non-ergodic by making q(zi|xi) and p(xi|zi) deterministic.
As a further illustration of this limitation in the critique by Zhao et al. (2017), consider a hierarchical
model which first generates faces at a 256x256 resolution and then generates, conditioned on that, at
a resolution of 512x512. If one were to do blocked Gibbs sampling between the 256x256 resolution
and 512x512 resolution on faces for example, the sampling process would be non-ergodic and the
chain would not mix (i.e. running a sampling chain between 256x256 and 512x512 resolution
images of a face will never significantly change the identity of the face), which means that the
model could be getting value out of having a deep hierarchical model. Indeed it has been observed
that similar models have been highly successful (Zhang et al., 2016), despite hypothetically being
subject to the Zhao et al. (2017) critique if the blocked Gibbs sampling on the lower two levels were
ergodic.
5	Experiments
The main goal of our experiments is to demonstrate that ldf is able to successfully learn the depen-
dencies in the data, even though the training is localized: no discriminator covers all of the variables
and no gradient flows through the entire graph (the higher level and lower level models are updated
locally). We demonstrate that this can be done successfully on both image generation, where we di-
vide the image into quadrants with distinct latent factors, and on video generation, where we divide
a video with five frames into single images each with distinct latent factors.
5.1	Image Generation
We evaluate our approach on image generation on the CIFAR dataset. We selected this dataset pri-
marily because the Inception Score method (Salimans et al., 2016) provides a way of quantitatively
evaluating results. Each image is 32x32, and for ldf, we divide this into a 2x2 grid giving us four
lower level latent segments. The lower level models share parameters across all positions.
5.2	Video Generation
We evaluated our model on unconditional video generation by considering the Pacman video dataset
collected by (Cooper, 2017). This dataset consists of 20000 videos each containing 5 adjacent
frames. The 32x32 video patches are shown in figure 5 and were selected randomly from full
Pacman game screens but filtered so that most of the clips contain movement. The Pacman dataset
was selected because it is visually simple and has predictable motion (the movement of Pacman and
the ghosts). Additionally, because the background never changes, the ability of the model to keep a
constant background is a test of the ability of ldf to learn the relationship between different steps.
For ldf, each frame’s lower level model (generator, inference network, and discriminator) are con-
volutional neural networks with an architecture similar to that used in (Dumoulin et al., 2017). The
higher level models (generator and discriminator) are fully-connected MLPs. For the baseline model
referenced in the Pacman figures, we train a simple GAN which has the frames stacked together
across the filters with each video treated as if it were an image with 15 channels (5 frames and 3
colors).
5.3	Evidence that Local Disentangling Simplifies Training
We conducted experiments to demonstrate that using ldf simplifies the training procedure. We did
this in two ways. First, we showed that using ldf trains faster than a joint training baseline (a
GAN where the discriminator sees all of the visible variables directly and the generator outputs all
variables in the visible space directly). This is shown in 7. Second, we showed that in the case
where all of the lower level samples are available (i.e. all of the frames in the video dataset) but only
6
Under review as a conference paper at ICLR 2018
φ,lo□s UO⅛B□U-
X Baseline
▲ Locally Disentangled
Factors (pretrain bottom)
■ Locally Disentangled
Factors (no pretraining)
Training Time (Seconds)
Figure 3: Inception Scores on CIFAR after 8 epochs of training with three different models. The
baseline (blue) is a GAN where the discriminator sees all of the pixels and the generator produces
the entire image. ldf with a pre-trained lower level (red) and ldf with training starting from scratch
all train faster than the baseline GAN.
a few full samples are available (actual video sequences), training with ldf is more successful.
This is shown in Fig. 6 where both ldf and the baseline joint model are only trained on 256 video
sequences.
6	Conclusion
We have proposed Locally Disentangled Factors (ldf), a powerful new approach to decomposing
the training of generative models. We have shown that ldf is able to successfully generate joint
distributions over complicated objects, even though the discriminators and gradient flow are entirely
local within the hierarchy. We have also shown that this allows for decoupled training and improved
ability to learn from small amounts of data from the joint distribution. While our method assumes
a more general prior than resolution-hierarchy style approaches, it still leaves the decision of what
the local factors would be on the practitioner. Finding methods that enjoy the same computational
and sample-complexity benefits with fewer assumptions about the data is an interesting research
direction.
References
Jinho Baik, Gerard Ben Arous, Sandrine Peche, et al. Phase transition of the largest eigenvalue for
nonnull complex sample covariance matrices. TheAnnalsofProbability, 33(5):1643-1697, 2005.
Matt Cooper. Adversarial video generation. https://github.com/dyelax/
Adversarial_Video_Generation, 2017.
Emily L. Denton, Soumith Chintala, Arthur Szlam, and Robert Fergus. Deep generative image
models using a laplacian pyramid of adversarial networks. CoRR, abs/1506.05751, 2015. URL
http://arxiv.org/abs/1506.05751.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropi-
etro, and Aaron Courville. Adversarially learned inference. In Proceedings of the International
Conference on Learning Representations (ICLR), 2017. arXiv:1606.00704.
7
Under review as a conference paper at ICLR 2018
Figure 4: Unconditioned video generation samples trained using a joint model where a discriminator
considers all frames (left) and locally disentangled factors (right).
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems,pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http:
//www.deeplearningbook.org.
8
Under review as a conference paper at ICLR 2018
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, and Ko-
ray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. CoRR, abs/1608.05343,
2016. URL http://arxiv.org/abs/1608.05343.
Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis.
Annals of statistics, pp. 295-327, 2001.
T. Karaletsos. Adversarial Message Passing For Graphical Models. ArXiv e-prints, December 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
Journal of Statistical Physics, 168(6):1223-1247, 2017.
Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In Advances in
Neural Information Processing Systems, pp. 3387-3395, 2016a.
Anh Nguyen, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. Plug & play
generative networks: Conditional iterative generation of images in latent space. arXiv preprint
arXiv:1612.00005, 2016b.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Scott Reed, Aaron van den Oord, Nal Kalchbrenner, Sergio Gomez Colmenarejo, ZiyU Wang, Dan
Belov, and Nando de Freitas. Parallel multiscale autoregressive density estimation. arXiv preprint
arXiv:1703.03664, 2017.
Kevin Roth, AureIien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. CoRR, abs/1705.09367, 2017. URL
http://arxiv.org/abs/1705.09367.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. CoRR, abs/1606.03498, 2016. URL http://arxiv.
org/abs/1606.03498.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dim-
itris N. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative ad-
versarial networks. CoRR, abs/1612.03242, 2016. URL http://arxiv.org/abs/1612.
03242.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from generative
models. CoRR, abs/1702.08396, 2017. URL http://arxiv.org/abs/1702.08396.
9
Under review as a conference paper at ICLR 2018
Appendix
Real video sequences
Figure 5: Real video sequences from the Pacman dataset (Cooper, 2017).
Model Samples
rirl，「Fr一
I⅝Fi≡rilf.
aκt-*⅛
fwrw尸

Kl - > ■
;Iv・—一 曲
Ill^l
一■-'-■ L ■

■ rfe ∙
I ιτ τ
Figure 6: Unconditioned video generation samples where only 256 video samples are available (but
the locally disentangled factors model can use all of the individual frames to train the local model).
Baseline with joint generation model (left) and ours (right). Both models were trained for the same
number of updates (five thousands epochs).
10
Under review as a conference paper at ICLR 2018
-、-- 1 -. ∣.. --J∣λγ-
■ W叫叫叫，
F.t一
■ ■
■
'%⅛*E什BI宿装 Iw
■
・垂
IA
■ ■
■I i
--IJ⅛ I- -Bi^ ■ art ⅜ - ■ ■ - » MT L T
≡tla≡-l∙i=Mrhl,!∙≡H
^iEm
^iEaSHS





Figure 7: Unconditioned video generation samples with only 21 minutes of wall-clock training time
using full video sequences. Baseline (left) and locally disentangled factors (right). The baseline runs
for 10 epochs.

11