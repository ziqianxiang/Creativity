Under review as a conference paper at ICLR 2018
Dense Recurrent Neural Network
with Attention Gate
Anonymous authors
Paper under double-blind review
Ab stract
We propose the dense RNN, which has fully connections from each hidden state
directly to multiple preceding hidden states of all layers. As the density of the
connection increases, the number of paths through which the gradient flows is
increased. The magnitude of gradients is also increased, which helps to prevent
the vanishing gradient problem in time. Larger gradients, however, may cause
exploding gradient problem. To complement the trade-off between two problems,
we propose an attention gate, which controls the amounts of gradient flows. We
describe the relation between the attention gate and the gradient flows by approx-
imation. The experiment on the language modeling using Penn Treebank corpus
shows dense connections with the attention gate improve the model’s performance
of the RNN.
1	Introduction
In order to analyze sequential data, it is important to choose an appropriate model to represent the
data. Recurrent neural network (RNN), as one of the model capturing sequential data, has been
applied to many problems such as natural language (Mikolov et al., 2013), machine translation
(Bahdanau et al., 2014), speech recognition (Graves et al., 2013). There are two main research
issues to improve the RNNs performance: 1) vanishing and exploding gradient problems and 2)
regularization.
The vanishing and exploding gradient problems occur as the sequential data has long-term depen-
dency (Hochreiter, 1998; Pascanu et al., 2013). One of the solutions is to add gate functions such
as the long short-term memory (LSTM) and gated recurrent unit (GRU). The LSTM has additional
gate functions and memory cells (Hochreiter & Schmidhuber, 1997). The gate function can prevent
the gradient from being vanished during back propagation through time. Gated recurrent unit (GRU)
has similar performance with less gate functions (Chung et al., 2014).
The part of sequential data whose boundary to distinguish the consecutive other parts, has the hier-
archical structures. To handle the hierarchy, the model should capture the multiple timescales. In
hierarchical multiple recurrent neural network (HM-RNN, Chung et al. (2016)), the boundary infor-
mation is also learned by implementing three operations such as update, copy and flush operator.
In clockwork RNN (Koutnik et al., 2014), the hidden states are divided into multiple sub-modules,
which act with different periods to capture multiple timescales. As all previous states within the
recurrent depth do not always affect the next state, memory-augmented neural network (MANN,
Gulcehre et al. (2017)) uses the memory to remember previous states and retrieve some of previous
states if necessary.
The basic way to handle multiple timescales along with preventing the vanishing gradient problem is
to increases both of feedforward depth and recurrent depth to capture multiple timescales. Feedfor-
ward depth is the longest path from the input layer to the output layer. Recurrent depth is the longest
path from arbitrary hidden state at time t to same hidden sate at time t + t0 (Zhang et al., 2016).
Increasing feedforward depth means stacking multiple recurrent layers deeply. It can capture fast
and slow changing components in the sequential data (Schmidhuber, 1992; El Hihi & Bengio, 1996;
Hermans & Schrauwen, 2013). The low level layer in the stacked RNN captures short-term depen-
dency. As the layer is higher, the aggregated information from lower layer is abstracted. Thus, as the
layer is higher, the capacity to model long-term dependency increases. The number of nonlinearities
1
Under review as a conference paper at ICLR 2018
in the stacked RNN, however, is proportional to the number of unfolded time steps regardless of the
feedforward depth. Thus, the simple RNN and stacked RNN act identically in terms of long run.
Increasing recurrent depth also increases the capability to capture long-term dependency in the data.
The hidden state in vanilla RNN has only connection to previous time step’s hidden state in the same
layer. Adding the connections to multiple previous time steps hidden states can make the shortcut
paths, which alleviates the vanishing problem. Nonlinear autoregressive exogenous model (NARX)
handles the vanishing gradient problem by adding direct connections from the distant past in the
same layer (Lin et al., 1996). Similarly, higher-order RNN (HO-RNN) has the direct connections to
multiple previous states with gating to each time step (Soltani & Jiang, 2016). Unlike other recurrent
models that use one connection between two consecutive time steps, the recurrent highway network
(RHN) adds multiple connections with sharing parameters between transitions in the same layer
(Zilly et al., 2016).
The vanilla RNN has only one path connected with previous hidden states. Thus, it is hard to ap-
ply standard dropout technique for regularization as the information is being diluted during training
of long-term sequences. By selecting the same dropout mask for feedforward, recurrent connec-
tions, respectively, the dropout can apply to the RNN, which is called a variational dropout (Gal &
Ghahramani, 2016).
This paper proposes a dense RNN that has both of feedforward and recurrent depths. The stacked
RNN increases the complexity by increasing feedforward depth. NARX-RNN and HO-RNN in-
crease the complexity by increasing recurrent depth. The model with the feedforward depth can be
combined with the model with the recurrent depth, as the feedforward depth and recurrent depth have
an orthogonal relationship. Gated feedback RNN has the fully connection between two consecutive
timesteps. As the connection of gated feedback is not overlapped with the model with orthogonal
depths, all three features, adding feedforward depth, recurrent depth, and gated feedback, can be
modeled jointly . With the three features, we propose the attention gate, which controls the flows
from each state so that it enhances the overall performance.
The contributions of this paper are summarized: 1) dense RNN that is aggregated model with feed-
forward depth, recurrent depth and gated feedback function, 2) extension of the variational dropout
to the dense RNN.
2	Background
There are largely two methods to improve the performance of RNN. One is to extend previous
model by stacking multiple layers or adding gate functions. The other is using regularization such
as dropout to avoid overfitting.
2.1	Extension of Recurrent Neural Network Models
In simple recurrent layer, ht, the hidden state at time t, is a function of input xt and preceding hidden
state ht-1, which is defined as follows:
ht = φ(ht-1, xt) = φ(W xt + Uht-1)	(1)
where U and W are respectively the feedforward and recurrent weight matrix and φ means an
element-wise nonlinear function such as T anh. In simple recurrent layer, the last hidden state ht-1
has to memorize all historical inputs. As the memorizing capacity of the hidden state is limited, it is
hard to capture long-term dependency in sequential data. Stacked recurrent neural network, stacked
of the simple recurrent layers can capture the long-dependency, which is defined as follows:
htj = φ(Wjhtj-1 + Uj→jhtj-1)	(2)
where Wj is the weight matrix for transition from layer j - 1 to j and Uj is the weight matrix
for transition from in timestep t - 1 to timestep t at layer j . The stacked RNN can model multiple
timescales of the sequential data. As the information travels toward upper layer, the hidden state can
memorize abstracted information of sequential data that covers more long-term dependency.
The other way to capture the long term dependency is to increase the recurrent depth by connecting
the hidden state at timestep t directly to multiple preceding hidden states (Soltani & Jiang, 2016),
2
Under review as a conference paper at ICLR 2018
which is defined as follows:
K
htj = φ(Wjhtj-1 + X U(k,j)→jhtj-k)	(3)
k=1
where U (k,j)→j is the weight matrix from layer j at timestep t - k to layer j at timestep t, and K is
the recurrent depth. The direct connections make the shortcut paths from preceding multiple hidden
states. Compared with the model without shortcut paths, the model with shortcut paths enables to
access preceding hidden states further way from htj with same number of transitions.
Most recurrent models have only recurrent connections between hidden states with same layers. By
adding feedback connections to the hidden state htj from the preceding hidden states hit-1 at differ-
ent layer of htj , the model adaptively captures the multiple timescales from long-term dependency,
which is defined as follows:
L
htj = φ(Wjhtj-1 + X Ui→jhit-1)	(4)
i=1
where Ui→j is the weight matrix from layer i at timestep t - 1 to layer j at timestep t, and L is the
feedforward depth. To control the amount of flows between different hidden states with different
time scales (Chung et al., 2014), the global gate is applied as follows:
L
htj = φ(Wjhtj-1 + X gi→j U i→j hit-1).	(5)
i=1
In (5), gi→j is the gate to control the information flows from each hidden state to multiple preceding
hidden states of all layers, which is defined as follows:
gi→j = σ(wg hj-1 + ug→j ht-1)	(6)
where wgj is a vector whose dimension is same with htj-1, uig→j is a vector whose dimension is
same with h；—、that is a concatenated vector of all hidden states from previous time step, and σ is
an element-wise sigmoid function.
Gated feedback function is also applied to LSTM. In gated LSTM, input gate itj , forget gate ftj ,
output gate oj, and memory cell gate C are defined as follows:
itj = σ(Wijhtj-1 + Uij→jhtj-1),	(7a)
ftj = σ(Wfjhtj-1 + Ufj→jhtj-1),	(7b)
otj =σ(Wojhtj-1 + Uoj→jhtj-1),	(7c)
L
Rt = Φ((Wj hj-1 + X gi→jUC→j hj-ι).	(7d)
i=1
Compared with conventional LSTM, the gated feedback LSTM has gated feedback function in the
memory cell gate C. Through gates and memory cell state in (7a)-(7d), the new memory cell state
ctt and hidden state htt respectively are calculated as follows:
C = ftj ∙ cj-1 + ij ∙ ct,	(8a)
hj = ot ∙ φ(ct)	(8b)
where the dot product means element-wise multiplication.
3
Under review as a conference paper at ICLR 2018
2.2	Dropout of Recurrent Neural Network
As dropout is one method of neural network regularization, it prevents the model from being over-
fitted to training set. However, it is hard to apply the standard dropout to recurrent connections. As
the sequence is longer, the information is affected by the dropout many time during backpropagation
through time, which makes memorizing long sequences hard. Thus, applying the standard dropout
only to feedforward connections is recommended (Zaremba et al., 2014). It is expressed as follows:
hj = φ(WjDj(hj-1) + Uj→jhj-ι) = φ(Wj(mj-1 ∙ hj-1) + Uj→jhj-ι)	(9)
where Dtj , as a dropout operator for every time step, makes htj-1 being masked with Bernoulli
dropout mask mtj-1 randomly generated for every time step. Moon et al. (2015) proposed how to
apply the dropout to recurrent connections efficiently. By considering the whole sequential data as
one input at a sequential level, same dropout mask is applied to recurrent connections at all time
steps during training, which is expressed as follows:
hj = φ(WjDj(hj-1) + Uj→jDj→j(hj-ι)) = φ(Wj(mj-1 ∙ hj-1) + Uj→j(mj ∙ hj7)) (10)
where Dj→j , as a time-independent dropout operator, makes htj-1 being masked with Bernoulli
dropout mask mj randomly generated regardless of time.
Gal & Ghahramani (2016) applied variational dropout to RNN and proved the relation between
Bayesian inference and dropout theoretically. Variational dropout in RNN applies same masks re-
gardless of time to feedforward connections, similar to recurrent connections, which is expressed as
follows:
hj = φ(WjDj(hj-1) + Uj→jDj→j(hj-1)) = φ(Wj(mj-1 ∙ hj-1) + Uj→j(mj ∙ hj-1)). (11)
3	Dense Recurrent Neural Network
The skip connections that bypass some layers enables deep networks to be trained better than the
models without skip connections. Many research (He et al., 2016; Huang et al., 2016) uses skip
connections toward feedback connections. In this paper, we apply the skip connections toward
recurrent connections. The shortcut paths from preceding multiple hidden states to the hidden state
at time t is equal to the skip connections through time. The shortcut paths include the feedback
connections between different layers of different timesteps. Each connection is controlled by the
attention gate, similar to the global gate in the gated feedback RNN. The dense RNN is defined as
follows:
KL
htj = φ(Wj htj-1+XX
g	t-k)	(12)
k=1 i=1
where g(k,i)→j is the attention gate, which is defined as follows:
g(k,i)→j=σ(wgihtj-1+u(gk,i)→jhit-k).	(13)
(13) is a function of preceding hidden state at layer i and time t - k, while (6) is a function of
concatenated all preceding hidden states. The intuition why we change the gate function and we call
it to attention gate is described in Section 3.1.
The dense RNN can be easily extended to dense LSTM. The dense LSTM is defined as follows:
KL
itj = σ(Wijhtj-1 +XXgi(k,i)→jUi(k,i)→jhit-k),	(14a)
k=1 i=1
KL
ftj = σ(Wfjhtj-1 + XXgf(k,i)→jUf(k,i)→jhit-k),	(14b)
k=1 i=1
KL
otj = σ(Wojhtj-1 + X X go(k,i)→j Uoj→j htj-1),	(14c)
k=1 i=1
4
Under review as a conference paper at ICLR 2018
(a)	(b)	(c)
Figure 1: (a) Conventional RNN unfolded in time. (b) Gated feedback RNN.(c) RNN with connec-
tions across multiple preceding states. (d) Dense RNN integrated with (b) and (c). Hidden states
are represented in red. The connections used in current steps feedforward are highlighted in bold.
The feedback connections between upper and lower layers are represented in yellow. (e) Unfolded
Dense RNN in both of time and the number of preceding states.
(e)
K L
C = Φ((Wj hj-1 + XX g"→g→j hj-1).	(14d)
k=1i=1
Unlike gated feedback LSTM, the attention gate g is applied to all gates and memory cell state.
We analyze the advantages of dense connections intuitively and theoretically. In addition, we pro-
pose the dropout method for dense connections.
3.1	Intuitive Analysis
Recurrent connections enable to predict next data given previous sequential data. In the language
modeling, the RNNs can predict next word based on the last word and the last context accumulated
before the last word. It assumes only last word affect to predict next word. For instance, “the sky
is” is given from the full sentence “the sky is blue” and the goal is to predict the word “blue”. In
this case, the preceding word “sky provides the better clue than the preceding word “is”. Inspired
by the fact, we propose the dense model that predicts the next word by directly referring to recent
preceding words. In other words, the output hj is a function of input hj-1 and recent preceding
output hj-k as in (3).
The higher the layer in a neural network, the more abstract the hidden states. In language modeling,
hidden states represent the characteristics of words, sentences, and paragraphs as the layer increases.
The conventional RNN has only the connection between same layer. It means the preceding words,
sentences, paragraphs determine next words, sentences and paragraphs, respectively. The given
5
Under review as a conference paper at ICLR 2018
Figure 2: (a) The stable region from exploding gradient problem. The attention gate increases
the stable region from 1 to 1/(KL). (b) The stable region from vanishing gradient problem. The
attention gate decreases the stable region from 1 to 1∕(KL). (c) The approximation of the attention
gate to determine vanishing and exploding gradient boundary.
word, however, can also determine the context of next paragraph. Also, the given paragraph can
determine next word. For instance, the word “mystery” in “it is mystery” can be followed by the
paragraph related to “mystery” and vice versa. The feedback connections can reflect the fact.
In (4), preceding words, sentences, and paragraphs affects next words, sentences, and paragraphs
with same scale. Preceding words, however, dont affects next word prediction evenly. In the sen-
tence “The sky is blue”, the word “sky” has a very close relation with the word “blue”. The word
“The”, as an article, has a less relation with the word “blue”. The amount how two words are related
depends on the kind of the two words. We define the the degree of relevance as gated attention g as
in (5).
The attention g is determined by the preceding word itself and the last word given as input. In
the sentence “The sky is blue”, the features of the word “the”, and “sky” denote ht-2, and ht-1,
respectively and the word “is” denotes xt or ht0 . Then, the attention to predict the word“blue” from
the word “The” is determined by the word “The and “blue”. The attention to predict the word
“blue” from the word “is” is determined by the word “is” and “blue”. In other words, the attention
is dependent on the previous hidden state htj-k and input htj-1 at certain time step as in (13).
3.2	Theoretical Analysis
The vanishing and exploding gradient problems happen the sequential data has long term depen-
dency. During backpropagation through time, error ET ’s gradient with respect to the recurrent
weight matrix Uj is vanished or exploded as the sequence is longer, which is expressed as follows:
∂Et _ X ∂Et _ X ∂Et ∂hjτ ∂hj _ X ∂Et YY ∂hτ ∂hj
后=t=1 dUj = t=1 时疏 dUj = t=1 河［“I ∂hτ-1) 而
(15)
The critical term related to vanishing and exploding gradient problems is ∂hjτ /∂hjτ-1. To find the
relation between vanishing and exploding gradient problems and dense connections, we assume
the worst situation in which the vanishing and exploding gradient problem may arise. The term
∂hT /∂hτ-1 denotes Aj. If the Amax is less than 1, the gradient with respect to Uj would be
exploded and if the Ajmin is greater than 1, gradient with respect to Uj would be vanished. In dense
recurrent network, there are more paths to flow the gradients. The Aj in dense recurrent network is
approximated as follows:
KL
Aj=kX=1Xi=1
dhT+k-1
dhT-1
KL
A(k,i)→j
k=1 i=1
(16)
6
Under review as a conference paper at ICLR 2018
where the superscript (k, i) → j means the direction of the path from hiτ+k-1 to hjτ. Assuming
A(k,i)→j is A(mka,ix)→j, Ajmax is K LA(mka,ix)→j, which reduces the vanishing gradient boundary from 1
to 1/(KL) as shown in Figure 2(a).
Though dense connections are able to alleviate the vanishing gradient problem, it causes the gradient
exploding. To alleviate the problem caused by dense connection, we add the attention gates as in
(13). The attention gate g(k,i)→j can control the magnitude of A(k,i)→j , which is expressed as
follows:
KL
Aj=XX
k=1 i=1
∂hT+k-ι
dhT-1
KL
ΣΣ
k=1 i=1
g(k㈤→j ∙ A是,∙0→j
(17)

In (17), the g(k,i)→j is trainable so that the vanishing and exploding boundary is determined adap-
tively. In dense RNN with attention gates, hiτ +k-1 is expressed as follows:
h+k-1 = φ(g(k,i→j U(k* hjτ-1 + θ)	(18)
where θ is not relevant parameters with hjτ-1, g(k,i)→j is σ(Ug(k,i)→jhjτ-1). For simplicity, (18) is
expressed as y = φ(g(Ugx) ∙ Ux + θ). Gradient of y with respect to X is calculated as follows:
∂y
∂x = y(i - y)[g(i - g)Ug Ux + gU]
=y(i - y)u[g(i - g)UgX + g] = gy(i - y)u.
(19)
The (19) is scaled With g compared to 舞=y(1 - y)U without attention gate g. As g and g are
similar as shown in Figure 2(c), g is approximated g as in (19).
In recurrent highway network (RHN, Zilly et al. (2016)), the effect of highway was described using
the Gersgorin circle theorem (GST, Gershgorin (1931)). Likewise, the dffect of the attention gate
in the proposed model can be interpreted using GST. For simplicity, we only formulate recurrent
connection with omitting feedforward connection, ht+1 = φ(Uht). Then, the Jacobian matrix
A = dhh+1 is UTdiag[φ0(Uht)]. By letting Y be a maximal bound on diag[φ0(Uht)] and Pmax be
the largest singular value of UT, the norm of the Jacobian satisfies using the triangle inequality as
follows:
kAk ≤ kUTkdiag[φ0(Uht)] ≤γρmax.	(20)
The value γρmax is less than 1, the vanishing gradient problem happens and the kAk is greater than
1, the exploding gradient problem happens as the range of the kAk has no explicit boundary. The
spectrum of A, the set of λ in A, is evaluated as follows:
n
spec(A) ⊂	U {λ ∈ C∣kλ - aiikc ≤ X M∣},	(21)
i∈1,...,n	j,j6=i
which means the eigenvalue λ lies on the circle whose radius is the summation of abstract values of
all elements except the diagonal element aii and center is the diagonal element aii. The simplified
recurrent connection with the attention gate is ht+1 = φ(gU ht) where g is Ug , ht . Then, the
Jacobian matrix A = dh+1 is expressed as follows:
A = (g + g0diag(Ug )diag(ht,i))U T diag [φ0(gUht)].	(22)
The spectrum of (22) is expressed as follows:
n
spec(A) ⊂	U {λ ∈ C∣kλ - (g + g0Ug,iht,i)%kc ≤ X |gaj|}.	(23)
i∈1,...,n	j,j6=i
The scaled term term g+ g0Ug,iht,i in (23) can be approximated as g as shown in Figure 2(b). Thus,
the upper bound of kAk is approximately less than 1 so that the attention gate g can alleviate the
exploding problem.
7
Under review as a conference paper at ICLR 2018
Table 1: Perplexity on the Penn Treebank language modeling task.
Network
# Hidden
size
# Feed
forward
# Recurrent # Param.
Valid
Test
VD-LSTM+RE
(Inan et al., 2016)
LSTM
(Zaremba et al., 2014)
200	2	1	5M	104	100.4
650	2	1	20M	86.2	82.7
1500	2	1	66M	82.2	78.4
200	2	1	2.65M	89.9	85.1
650	2	1	8.6M	77.4	74.7
1500	2	1	19.8M	74.5	71.2
Dense LSTM
(2 hidden layers)
2222
0000000000
2222
1234
MMMMM
.0.6.3.9.
3344
99680
.3.3.5.2.7
53333
88888
54529
.3.6.0.8.9
08988
87777
200		3	1	3.9M	83.12	78.95
Dense LSTM	200	3	2	5.4M	83.08	78.67
	200	3	3	6.9M	82.38	78.15
(3 hidden layers)	200	3	4	8.3M	82.56	77.91
	200	3	5	9.8M	82.98	78.69
3.3 Extension of Variational Dropout
In dense RNN, as recurrent depth increases, the number of parameters also increases, which makes
the model vulnerable to overfitting to training dataset. To prevent the model from overfitting to
training dataset, the dropout is applied. The variational dropout, proved to show good performance
in the RNN models, uses same random masks at every time step for feedforward connections, and
recurrent connections, respectively. In implementation of variational dropout, each state is dropped
with the random mask, which is followed by weighted sum. This concept is extensively applied to
dense RNN so that the same random mask is used regardless of time and recurrent depth. Extension
of variational dropout to the dense connection is expressed as follows:
KL
hj = φ(Wj (mj-1 ∙htj-1)+XXg(k,i)→jU(k,i)→j(mi ∙ ht-k)).	(24)
4	Experiment
In our experiment, we used Penn Tree corpus (PTB) for language modeling. PTB consists of 923k
training set, 73k validation set, and 82k test set. As a baseline model, we select the model proposed
by Zaremba et al. (2014), which proposed how to predict next word based on the previous words.
To improve the accuracy, Zaremba et al. (2014) proposed regularization method for RNN.
The baseline models hidden sizes are fixed as 200, 650, and 1500, which are called as small, medium,
and large network. In this paper, we fixed hidden size as 200 for all hidden layers with varying the
feedforward depth from 2 to 3, recurrent depth from 1 to 5. The word embedding weights were tied
with word prediction weights.
To speed up the proposed method, all of matrix multiplications in (12) was performed by the batch
matrix-matrix product of two matrices. Each of batches is rewritten as follows:
hj→i = U (k,i)→j ht-k.	(25)
The (12) is rewritten as follows:
KL
htj = φ(Wj htj- 1+XX
ggi→jhJk).	(26)
k=1 i=1
8
Under review as a conference paper at ICLR 2018
The proposed dense model is trained with stochastic gradient decent. The initial rate was set to 20,
which was decayed with 1.1 after the epoch was over 12. The training was terminated when the
epoch reaches 120. To prevent the exploding gradient, we clipped the max value of gradient as 5.
As a regularization, we adopt variational dropout, which uses the random masks regardless of time.
We configured the word embeddings dropout to 0.3, feedforward connections dropout 0.2, and re-
current connections dropout rate varies from 0.2 to 0.5. The Table 1, as a trained result, compares
the baseline model, RNN model with variational dropout and using same word space. In dense RNN,
the perplexity is better than two models. The best models recurrent depth is 2 and the perplexity of
valid set is 83.28 and that of test set is 78.82.
5	Conclusion
This paper proposed dense RNN, which has fully connections from each hidden state to multiple
preceding hidden states of all layers directly. Each previous hidden state has its attention gate that
controls the amount of information flows. To evaluate the effect of dense connections, we used Penn
Treebank corpus (PTB). The result of dense connection was confirmed by varying the recurrent
depth with the attention gate. The dense connections with the attention gate made the model’s
perplexity less than conventional RNN.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-
works. arXiv preprint arXiv:1609.01704, 2016.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependen-
cies. In Advances in neural information processing systems, pp. 493-499, 1996.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pp. 1019-1027, 2016.
Semyon Aranovich Gershgorin. Uber die abgrenzung der eigenwerte einer matrix. Bulletin de
l’Academie des Sciences de l’URSS. Classe des sciences mathematiques, (6):749-754, 1931.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international
conference on, pp. 6645-6649. IEEE, 2013.
Caglar Gulcehre, Sarath Chandar, and Yoshua Bengio. Memory augmented neural networks with
wormhole connections. arXiv preprint arXiv:1701.08718, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Michiel Hermans and Benjamin Schrauwen. Training and analysing deep recurrent neural networks.
In Advances in neural information processing systems, pp. 190-198, 2013.
Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem
solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):
107-116, 1998.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
9
Under review as a conference paper at ICLR 2018
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A
loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In Inter-
national Conference on Machine Learning,pp. 1863-1871, 2014.
Tsungnan Lin, Bill G Horne, Peter Tino, and C Lee Giles. Learning long-term dependencies in narx
recurrent neural networks. IEEE Transactions on Neural Networks, 7(6):1329-1338, 1996.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representa-
tions of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems
26, pp. 3111-3119. Curran Associates, Inc., 2013.
Taesup Moon, Heeyoul Choi, Hoshik Lee, and Inchul Song. Rnndrop: A novel dropout for rnns in
asr. In Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on, pp.
65-70. IEEE, 2015.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. ICML (3), 28:1310-1318, 2013.
Jurgen Schmidhuber. Learning complex, extended sequences using the principle of history Com-
pression. Neural Computation, 4(2):234-242, 1992.
Rohollah Soltani and Hui Jiang. Higher order recurrent neural networks. arXiv preprint
arXiv:1605.00064, 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan R Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. In Advances
in Neural Information Processing Systems, pp. 1822-1830, 2016.
Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnlk, and Jurgen Schmidhuber. Recurrent
highway networks. arXiv preprint arXiv:1607.03474, 2016.
10