Under review as a conference paper at ICLR 2018
Learning Deep ResNet Blocks Sequentially
using Boosting Theory
Anonymous authors
Paper under double-blind review
Ab stract
We prove a multiclass boosting theory for the ResNet architectures which simulta-
neously creates a new technique for multiclass boosting and provides a new algo-
rithm for ResNet-style architectures. Our proposed training algorithm, BoostRes-
Net, is particularly suitable in non-differentiable architectures. Our method only
requires the relatively inexpensive sequential training of T “shallow ResNets”.
We prove that the training error decays exponentially with the depth T if the weak
module classifiers that we train perform slightly better than some weak baseline.
In other words, we propose a weak learning condition and prove a boosting the-
ory for ResNet under the weak learning condition. A generalization error bound
based on margin theory is proved and suggests that ResNet could be resistant to
overfitting using a network with l1 norm bounded weights.
1	Introduction
Why do residual neural networks (ResNets) (He et al., 2016) and the related highway net-
works (Srivastava et al., 2015) work? And if we study closely why they work, can we come up
with new understandings of how to train them and how to define working algorithms?
Deep neural networks have elicited breakthrough successes in machine learning, especially
in image classification and object recognition (Krizhevsky et al., 2012; Sermanet et al., 2013;
Simonyan & Zisserman, 2014; Zeiler & Fergus, 2014) in recent years. As the number of lay-
ers increases, the nonlinear network becomes more powerful, deriving richer features from input
data. Empirical studies suggest that challenging tasks in image classification (He et al., 2015;
Ioffe & Szegedy, 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2015) and object recogni-
tion (Girshick, 2015; Girshick et al., 2014; He et al., 2014; Long et al., 2015; Ren et al., 2015) often
require “deep” networks, consisting of tens or hundreds of layers. Theoretical analyses have further
justified the power of deep networks (Mhaskar & Poggio, 2016) compared to shallow networks.
However, deep neural networks are difficult to train despite their intrinsic representational power.
Stochastic gradient descent with back-propagation (BP) (LeCun et al., 1989) and its variants are
commonly used to solve the non-convex optimization problems. A major challenge that exists for
training both shallow and deep networks is vanishing or exploding gradients (Bengio et al., 1994;
Glorot & Bengio, 2010). Recent works have proposed normalization techniques (Glorot & Bengio,
2010; LeCun et al., 2012; Ioffe & Szegedy, 2015; Saxe et al., 2013) to effectively ease the problem
and achieve convergence. In training deep networks, however, a surprising training performance
degradation is observed (He & Sun, 2015; Srivastava et al., 2015; He et al., 2016): the training per-
formance degrades rapidly with increased network depth after some saturation point. This training
performance degradation is representationally surprising as one can easily construct a deep network
identical to a shallow network by forcing any part of the deep network to be the same as the shal-
low network with the remaining layers functioning as identity maps. He et al. (He et al., 2016)
presented a residual network (ResNet) learning framework to ease the training of networks that are
substantially deeper than those used previously. And they explicitly reformulate the layers as learn-
ing residual functions with reference to the layer inputs by adding identity loops to the layers. It
is shown in (Hardt & Ma, 2016) that identity loops ease the problem of spurious local optima in
shallow networks. Srivastava et al. (Srivastava et al., 2015) introduce a novel architecture that en-
ables the optimization of networks with virtually arbitrary depth through the use of a learned gating
mechanism for regulating information flow.
1
Under review as a conference paper at ICLR 2018
Empirical evidence overwhelmingly shows that these deep residual networks are easier to optimize
than non-residual ones. Can we develop a theoretical justification for this observation? And does
that justification point us towards new algorithms with better characteristics?
1.1	Summary of Results
We propose a new framework, multi-channel telescoping sum boosting (defined in Section 4), to
characterize a feed forward ResNet in Section 3. We show that the top level (final) output of a
ResNet can be thought of as a layer-by-layer boosting method (defined in Section 2). Error bounds
for telescoping sum boosting are provided.
We introduce a learning algorithm (BoostResNet) guaranteed to reduce error exponentially as depth
increases so long as a weak learning assumption is obeyed. BoostResNet adaptively selects training
samples or changes the cost function (Section 4 Theorem 4.2). In Section 4.4, we analyze the
generalization error of BoostResNet and provide advice to avoid overfitting. The procedure trains
each residual block sequentially, only requiring that each provides a better-than-a-weak-baseline in
predicting labels.
BoostResNet requires radically lower computational complexity for training than end-to-end back
propagation (e2eBP). Memorywise, BoostResNet requires only individual layers of the network to
be in the graphics processing unit (GPU) while e2eBP inevitably keeps all layers in the GPU. For
example, in a state-of-the-art deep ResNet, this might reduce the RAM requirements for GPU by a
factor of the depth of the network. Similar improvements in computation are observed since each
e2eBP step involves back propagating through the entire deep network.
Experimentally, we compare BoostResNet with e2eBP over two types of feed-forward ResNets,
multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual
network (CNN-ResNet), on multiple datasets. BoostResNet shows substantial computational per-
formance improvements and accuracy improvement under the MLP-ResNet architecture. Under
CNN-ResNet, a faster convergence for BoostResNet is observed.
One of the hallmarks of our approach is to make an explicit distinction between the classes of the
multiclass learning problem and channels that are constructed by the learning procedure. A channel
here is essentially a scalar value modified by the rounds of boosting so as to implicitly minimize the
multiclass error rate. Our multi-channel telescoping sum boosting learning framework is not limited
to ResNet and can be extended to other, even non-differentiable, nonlinear hypothesis units, such as
decision trees or tensor decompositions.
1.2	Related Works
Training deep neural networks has been an active research area in the past few years. The main
optimization challenge lies in the highly non-convex nature of the loss function. There are two main
ways to address this optimization problem: one is to select a loss function and network architecture
that have better geometric properties (details refer to appendix A.1), and the other is to improve the
network’s learning procedure (details refer to appendix A.2).
Many authors have previously looked into neural networks and boosting, each in a different way.
Bengio et al. (2006) introduce single hidden layer convex neural networks, and propose a gradient
boosting algorithm to learn the weights of the linear classifier. The approach has not been gen-
eralized to deep networks with more than one hidden layer. Shalev-Shwartz (2014) proposes a
selfieBoost algorithm which boosts the accuracy of an entire network. Our algorithm is different as
we instead construct ensembles of classifiers. Veit et al. (2016) interpret residual networks as a col-
lection of many paths of differing length. Their empirical study shows that residual networks avoid
the vanishing gradient problem by introducing short paths which can carry gradient throughout the
extent of very deep networks. The authors of AdaNet (Cortes et al., 2016) consider ensembles of
neural layers with a boosting-style algorithm and provide a method for structural learning of neu-
ral networks by optimizing over the generalization bound, which consists of the training error and
the complexity of the AdaNet architecture. AdaNet uses the traditional boosting framework where
weak classifiers are being boosted. Therefore, to obtain low training error guarantee, AdaNet maps
the feature vectors (hidden layer representations) to a classifier space and boosts the weak classi-
fiers. Our BoostResNet, instead, boosts representations (feature vectors) over multiple channels,
and therefore produces a less “bushy” architecture. BoostResNet focuses on a ResNet architecture,
provides a new training algorithm for ResNet, and proves a training error guarantee for deep ResNet
architecture. A ResNet-style architecture is a special case of AdaNet, so AdaNet generalization
guarantee applies here and our generalization analysis is built upon their work.
2
Under review as a conference paper at ICLR 2018
2	Preliminaries
A residual neural network (ResNet) is composed of stacked enti-
ties referred to as residual blocks. Each residual block consists of a
neural network module and an identity loop (shortcut). Commonly
used modules include MLP and CNN. Throughout this paper, we
consider training and test examples generated i.i.d. from some dis-
tribution D over X × Y , where X is the input space and Y is the
label space. We denote by S = ((x1 , y1), (x2, y2), . . . , (xm, ym))
a training set of m examples drawn according to Dm .
A Residual Block of ResNet ResNet consists of residual blocks.
Each residual block contains a module and an identity loop. Let
each module map its input X to ft(X) where t denotes the level of
the modules. Each module ft is a nonlinear unit with n channels,
i.e., ft(∙) ∈ Rn. In multilayer perceptron residual network (MLP-
ResNet), ft is a shallow MLP, for instance, ft(X) = Vtrσ(W(TX)
where Wt ∈ Rn×k, Vt ∈ Rk×n and σ is a nonlinear operator such
as sigmoidal function or relu function. Similarly, in convolutional
neural network residual network (CNN-ResNet), ft(∙) represents
the t-th convolutional module. Then the t-th residual block outputs
gt+1(X)
gt+1(X) = ft(gt(X)) + gt(X),
Figure 1: The architecture of
a residual network (ResNet).
(1)
where X is the input fed to the ResNet. See Figure 1 for an illustration of a ResNet, which consists
of stacked residual blocks (each residual block contains a nonlinear module and an identity loop).
Output of ResNet Due to the recursive relation specified in Equation (1), the output of the T -th
residual block is equal to the summation over lower module outputs, i.e., gT +1(X) = fT (gT (X)) +
gτ(x) = ET=I ft(gt(x)), where gi(x) = x. For binary classification tasks, the final output of a
ResNet given input X is rendered after a linear classifier w ∈ Rn on representation gT +1(X) (In the
multiclass setting, let C be the number of classes; the linear classifier W ∈ Rn×C is a matrix instead
of a vector.):
y = σ(F (x)) = σ(wτ gτ+ι(χ)) = σ
(WT Σ ft (Ot(X)))
(2)
where F(x) = WTgT+ι(χ) and σ(∙) denotes a map from classifier outputs (scores) to labels.
For instance σ(z) = Sign(Z) for binary classification (σ(z) = arg max Zi for multiclass clas-
i
sification). The parameters of a depth-T ResNet are {w, {ft(∙), ∀t ∈ T}}. A ResNet training
involves training the classifier W and the weights of modules ft(∙) ∀t ∈ [T ] when training examples
(x1, y1), (x2, y2), . . . , (xm, ym) are available.
Boosting Boosting (Freund & Schapire, 1995) assumes the availability of a weak learning algo-
rithm which, given labeled training examples, produces a weak classifier (a.k.a. base classifier).
The goal of boosting is to improve the performance of the weak learning algorithm. The key idea
behind boosting is to choose training sets for the weak classifier in such a fashion as to force it to
infer something new about the data each time it is called. The weak learning algorithm will finally
combine many weak classifiers into a single strong classifier whose prediction power is strong.
From empirical experience, ResNet remedies the problem of training error degradation (instability
of solving non-convex optimization problem using SGD) in deeper neural networks. We are curi-
ous about whether there is a theoretical justification that identity loops help in training. More im-
portantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation
(e2eBP) through the deep network and thus is immune to the instability of SGD for non-convex
optimization of deep neural networks.
3	ResNet in Telescoping Sum Boosting Framework
As we recall from Equation (2), ResNet indeed has a similar form as the strong classifier in boosting.
The key difference is that boosting is an ensemble of estimated hypotheses whereas ResNet is an
3
Under review as a conference paper at ICLR 2018
ensemble of estimated feature representations ET=I ft(gt(x)). To solve this problem, we introduce
an auxiliary linear classifier wt on top of each residual block to construct a hypothesis module.
Formally, a hypothesis module is defined as
ot(x) =f WJgt(X) ∈ R	(3)
in the binary classification setting. Therefore ot+ι(χ) = Wt+ι[ft(gt(x)) + gt(x)] as gt+ι(x) =
ft (gt(x)) + gt(x). We emphasize that given gt (x), we only need to train ft and Wt+1 to train
ot+1(x). In other words, we feed the output of previous residual block (gt(x)) to the current module
and train the weights of current module ft (∙) and the auxiliary classifier wt+1.
Now the input, gt+1(x), of the t + 1-th residual block is the output, ft(gt(x)) + gt(x), of the t-th
residual block. As a result, ot(x) = Et-=1ι WJft’(gt'(x)). In other words, the auxiliary linear
classifier is common for all modules underneath. It would not be realistic to assume a common
auxiliary linear classifier, as such an assumption prevents us from training the T hypothesis module
sequentially. We design a weak module classifier using the idea of telescoping sum as follows.
Definition 3.1. A weak module classifier is defined as
def
ht(x) = αt+1ot+1(x) - αtot (x)	(4)
where ot(x) d=ef WtJgt(x) is a hypothesis module, and αt is a scalar. We call it a “telescoping sum
boosting” framework if the weak learners are restricted to the form of the weak module classifier.
ResNet: Ensemble of Weak Module Classifiers Recall that the T-th residual block of a ResNet
outputs gT +1(x), which is fed to the top/final linear classifier for the final classification. We show
that an ensemble of the weak module classifiers is equivalent to a ResNet’s final output. We state it
formally in Lemma 3.2. For purposes of exposition, we will call F(x) the output of ResNet although
a σ function is applied on top of F(x), mapping the output to the label space Y.
Lemma 3.2. Let the input gt(x) of the t-th module be the output of the previous module, i.e.,
gt+1(x) = ft(gt(x)) + gt(x). Then the summation of T weak module classifiers divided by αT+1 is
identical to the output, F(x), of the depth-T ResNet,
1T
F(x) = WJgT +ι(x) ≡ ɑ^	ht(x),	(5)
where the weak module classifier ht(x) is defined in Equation (4).
See Appendix B for the proof. Overall, our proposed ensemble of weak module classifiers is a new
framework that allows for sequential training of ResNet. Note that traditional boosting algorithm
results do not apply here. We now analyze our telescoping sum boosting framework in Section 4.
Our analysis applies to both binary and multiclass, but we will focus on the binary class for simplicity
in the main text and defer the multiclass analysis to the Appendix F.
4	Telescoping Sum Boosting for B inary Classification
Below, we propose a learning algorithm whose training error decays exponentially with the number
of weak module classifiers T under a weak learning condition. We restrict to bounded hypothesis
modules, i.e., |ot(x)| ≤ 1.
4.1	Weak Learning Condition
The weak module classifier involves the difference between (scaled version of) ot+1(x) and ot(x).
Let Yt =f Ei^Dt-1 [yiθt(xi)] > 0 be the edge of the hypothesis module ot(x), where Dt-ι is the
weight of the examples. As the hypothesis module ot(x) is bounded by 1, We obtain ∣γt∣ ≤ 1.
So γYt characterizes the performance of the hypothesis module ot(x). A natural requirement would
be that ot+ι(χ) improves slightly upon ot(x), and thus γt+ι - Yt ≥ Y' > 0 could serve as a
weak learning condition. However this weak learning condition is too strong: even when current
hypothesis module is performing almost ideally (YYt is close to 1), we still seek a hypothesis module
which performs consistently better than the previous one by Y'. Instead, we consider a much weaker
learning condition, inspired by training error analysis, as follows.
4
Under review as a conference paper at ICLR 2018
Definition 4.1 (γ -Weak Learning Condition). A weak module classifier ht(x) = αt+1ot+1 - αtot
Y2 -彳2
satisfies the Y-Weak learning condition If t+11j	≥ Y2 > 0 and the covariance between
1-γt
exp(-yot+1 (x)) and exp(yot(x)) is non-positive.
The weak learning condition is motivated by the learning theory and it is met in practice (refer to
Figure 4).	def
Interpretation of weak learning condition For each weak module classifier ht(x), Yt =
ʌ γt+1-Yt characterizes the normalized improvement of the correlation between the true labels
1-γt
y and the hypothesis modules ot+1(x) over the correlation between the true labels y and the hypoth-
esis modules ot (x). The condition specified in Definition 4.1 is mild as it requires the hypothesis
module ot+1(x) to perform only slightly better than the previous hypothesis module ot(x). In resid-
ual network, since ot+1 (x) represents a depth-(t + 1) residual network which is a deeper counterpart
of the depth-t residual network ot(x), it is natural to assume that the deeper residual network im-
proves slightly upon the shallower residual network. When Yt is close to 1, γ2+ι only needs to be
slightly better than γ2 as the denominator 1 - γ2 is small. The assumption of the covariance between
exp(-yot+1 (x)) and exp(yot(x)) being non-positive is suggesting that the weak module classifiers
should not be adversarial, which may be a reasonable assumption for ResNet.
4.2 BoostResNet
We now propose a novel training algorithm for telescoping sum boosting under binary-class clas-
sification as in Algorithm 1. In particular, we introduce a training procedure for deep ResNet in
Algorithm 1 & 2, BoostResNet, which only requires sequential training of shallow ResNets.
The training algorithm is a module-by-module procedure following a bottom-up fashion as the out-
puts of the t-th module gt+1(x) are fed as the training examples to the next t+ 1-th module. Each of
the shallow ResNet ft(gt(x)) + gt(x) is combined with an auxiliary linear classifier wt+1 to form
a hypothesis module ot+1 (x). The weights of the ResNet are trained on these shallow ResNets.
The telescoping sum construction is the key for successful interpretation of ResNet as ensembles
of weak module classifiers. The innovative introduction of the auxiliary linear classifiers (wt+1 )
is the key solution for successful multi-channel representation boosting with theoretical guarantees.
Auxiliary linear classifiers are only used to guide training, and they are not included in the model
(proved in Lemma 3.2). This is the fundamental difference between BoostResNet and AdaNet.
AdaNet (Cortes et al., 2016) maps the feature vectors (hidden layer representations) to a classifier
space and boosts the weak classifiers. Our framework is a multi-channel representation (or infor-
mation) boosting rather than a traditional classifier boosting. Traditional boosting theory does not
apply in our setting.
Algorithm 1 BoostResNet: telescoping sum boosting for binary-class classification
Input: m labeled samples [(xi, yi)]m where yi ∈ {-1, +1} and a threshold Y
Output: {ft(∙),∀t} and WT +ι	> Discard wt+ι, ∀t = T
1:	Initialize t J 0, Y0 J 0, α0 J 0, o0(x) J 0
2:	Initialize sample weights at round 0: D0(i) J 1/m, ∀i ∈ [m]
3:	while Yt > Y do
4:	ft(∙), αt+ι, wt+1,ot+1(x) J Algorithm 2(gt(x), Dt, ot(x), α)
√Y2-L —Y2
t+1 γ2	> where γt+ι J Ei〜Dt [yiθt+ι(χi)]
6:	Update Dt+ι(i) J——mDt(i)exp(-yiMxi))	> where ht(x) = αt+10t+1 (x) — αot(x)
E Dt (i) exp[-yiht(xi)]
i=1
7:	t J t + 1
8:	end while
9:	T J t - 1
Theorem 4.2. [ Training error bound ] The training error ofa T -module telescoping sum boosting
framework using Algorithms 1 and2 decays exponentially with the number of modules T,
Pr
i〜S
ht (xi) = yi	≤ e
-1 Tγ2
5
Under review as a conference paper at ICLR 2018
Algorithm 2 BoostResNet: oracle implementation for training a ResNet module
Input: gt(x),Dt,ot(x) and αt
Output： ft。, αt+ι, wt+ι and 0t+1(x)
m
1:	(ft,αt+ι, wt+ι) - arg min EDt(i)exp (-yiavτ [f (gt(xi)) + gt(xi)]+ yiαg(xi)
(f,α,v) i=1
2:	0t+1(x) J Wt+ι [ft(gt(x)) + gt(x)]
if ∀t ∈ [T] the weak module classifier ht (x) satisfies the γ -weak learning condition defined in
Definition 4.1.
The training error of Algorithms 1 and 2 is guaranteed to decay exponentially with the ResNet depth
even when each hypothesis module ot+1 (x) performs slightly better than its previous hypothesis
module ot (x) (i.e., γ > 0). Refer to Appendix F for the algorithm and theoretical guarantees for
multiclass classification.
4.3	Oracle Implementation for ResNet
In Algorithm 2, the implementation of the oracle at line 1 is equivalent to
1m
(ft,αt+ι, wt+ι) = arg min -Vexp (—yiαvT [f(gt(Xi))+ gt(xi)])	(6)
(f,α,v) m
i=1
The minimization problem over f corresponds to finding the weights of the t-th nonlinear mod-
ule of the residual network. Auxiliary classifier wt+1 is used to help solve this minimization
problem with the guidance of training labels yi. However, the final neural network model in-
cludes none of the auxiliary classifiers, and still follows a standard ResNet structure (proved in
Lemma 3.2). In practice, there are various ways to implement Equation (6). For instance, Janzamin
et. al. (Janzamin et al., 2015) propose a tensor decomposition technique which decomposes a tensor
formed by some transformation of the features x combined with labels y and recovers the weights
ofa one-hidden layer neural network with guarantees. One can also use back-propagation as numer-
ous works have shown that gradient based training are relatively stable on shallow networks with
identity loops (Hardt & Ma, 2016; He et al., 2016).
Computational and Memory Efficiency It is worth noting that BoostResNet training is memory
efficient as the training process only requires parameters of two consecutive residual blocks to be in
memory. Given that the limited GPU memory being one of the main bottlenecks for computational
efficiency, BoostResNet requires significantly less training time than e2eBP in deep networks as
a result of reduced communication overhead and the speed-up in shallow gradient forwarding and
back-propagation. Let M1 be the memory required for one module, and M2 be the memory required
for one linear classifier, the memory consumption is M1 + M2 by BoostResNet and M1T + M2 by
e2eBP. Let the flops needed for gradient update over one module and one linear classifier be C1 and
C2 respectively, the computation cost is C1 + C2 by BoostResNet and C1 T + C2 by e2eBP.
4.4	Generalization Error Analysis
In this section, we analyze the generalization error to understand the possibility of overfitting under
Algorithm 1. The strong classifier or the ResNet is F(x)= 之；；,). Now we define the margin
for example (x, y) as yF (x). For simplicity, we consider MLP-ResNet with n multiple channels
and assume that the weight vector connecting a neuron at layer t with its preceding layer neurons is
l1 norm bounded by Λt,t-1. Recall that there exists a linear classifier w on top, and we restrict to
lι norm bounded classifiers, i.e., ∣∣ w∣∣ ι ≤ Co < ∞. The expected training examples are l∞ norm
bounded r∞ = Es~d [maxi∈[m] ∣∣Xi∣∣∞] < ∞. We introduce Corollary 4.3 which follows directly
from Lemma 2 of (Cortes et al., 2016).
Corollary 4.3. (Cortes et al., 2016) Let D be a distribution over X × Y and S be a sample of m
examples Chosen independently at random according to D. With probability at least 1 — δ ,for θ > 0,
6
Under review as a conference paper at ICLR 2018
the strong classifier F (x) (ResNet) satisfies that
Pr (yF(x) ≤ 0) ≤ Pr(yF(x) ≤ θ) + 4C0r∞ ∖∕lθg(2n) E At + 2 ∖∕l0gT + β(θ, m,T, δ)⑺
D	S	θ	2m t	θ m
WhereAt 照 ∏t'=ι 2At',t'-i andβ(θ, m, t, δ) == ʌ/fθ42 log fjθ0gmτɔllogτ + ⅛2gmδ-.
From Corollary 4.3, we obtain a generalization error bound in terms of margin bound
PrS (yF(x) ≤ θ) and network complexity 4Cθr∞ JlotImn) ET=I At + 2 JIommT + β(θ, m, T, δ).
Larger margin bound (larger θ) contributes positively to generalization accuracy, and l1 norm
bounded weights (smaller Et=I At ) are beneficial to control network complexity and to avoid
overfitting. The dominant term in the network complexity is
4c0r∞ ∕log(ln)
θ V	Im
EL At which scales
as least linearly with the depth T. See appendix D for the proof.
This corollary suggests that stronger weak module classifiers which produce higher accuracy pre-
dictions and larger edges, will yield larger margins and suffer less from overfitting. The larger the
value of θ, the smaller the term 4Cθr∞ ʌ/loglmmn) Et=I At +1 JlOmmT + β(θ, m, T, δ) is. With larger
edges on the training set and when NT +1 < 1, we are able to choose larger values of θ while keeping
the error term zero or close to zero.
5 Experiments
We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the
MNIST (LeCun et al., 1998), street view house numbers (SVHN) (Netzer et al., 2011), and CIFAR-
10 (Krizhevsky & Hinton, 2009) benchmark datasets. Two different types of architectures are tested:
a ResNet where each module is a fully-connected multi-layer perceptron (MLP-ResNet) and a more
common, convolutional neural network residual network (CNN-ResNet). In each experiment the
architecture of both algorithms is identical, and they are both initialized with the same random seed.
As a baseline, we also experiment with standard boosting (AdaBoost.MM Mukherjee & Schapire
(2013)) of convolutional modules in appendix H.2 for SVHN and CIFAR-10 datasets. Our exper-
iments are programmed in the Torch deep learning framework for Lua and executed on NVIDIA
Tesla P100 GPUs. All models are trained using the Adam variant of SGD Kingma & Ba (2014).
MLP-ResNet on MNIST The MNIST database (LeCun et al., 1998) of handwritten digits has a
training set of 60,000 examples, and a test set of 10,000 examples. The data contains ten classes.
We test the performance of BoostResNet on MLP-ResNet using MNIST dataset, and compare it
with e2eBP baseline. Each residual block is composed of an MLP with a single, 1024-dimensional
hidden layer. The training and test error between BoostResNet and e2eBP is in Figure 2 as a function
of depth. Surprisingly, we find that training error degrades for e2eBP, although the ResNet’s identity
loop is supposed to alleviate this problem. Our proposed sequential training procedure, BoostResNet,
relieves gradient instability issues, and continues to perform well as depth increases.
(a) depth vs training accuracy
Figure 2: Comparison of BoostResNet (ours, blue) and e2eBP (baseline, red) on multilayer percep-
tron residual network on MNIST dataset.
(b) depth vs test accuracy
CNN-ResNet on SVHN SVHN (Netzer et al., 2011) is a real-world image dataset, obtained from
house numbers in Google Street View images. The dataset contains over 600,000 training images,
7
Under review as a conference paper at ICLR 2018
and about 20,000 test images. We fit a 50-layer, 25-residual-block CNN-ResNet using both Boost-
ResNet and e2eBP (figure 3a). Each residual block is composed of a CNN using 15 3 × 3 filters.
We refine the result of BoostResNet by initializing the weights using the result of BoostResNet and
run end-to-end back propagation (e2eBP). From figure 3a, our BoostResNet converges much faster
(requires much fewer gradient updates) than e2eBP. The test accuracy of BoostResNet is comparable
with e2eBP.
(a) SVHN
Figure 3: Convergence performance comparison between e2eBP and BoostResNet on the SVHN
and CIFAR-10 dataset. The vertical dotted line shows when BoostResNet training stopped, and We
began refining the network with standard e2eBP training.
(b) CIFAR-10
CNN-ResNet on CIFAR-10 The CIFAR-10 dataset is a benchmark dataset composed of 10
classes of small images, such as animals and vehicles. It consists of 50,000 training images and
10,000 test images. We again fit a 50-layer, 25-residual-block CNN-ResNet using both BoostResNet
and e2eBP (figure 3b). BoostResNet training converges to the optimal solution faster than e2eBP.
Unlike in the previous two datasets, the efficiency of BoostResNet comes at a cost when training
with CIFAR-10. We find that the test accuracy of the e2eBP refined BoostResNet to be slightly lower
than that produced by e2eBP.
Weak Learning Condition Check The weak learning condition (Definition 4.1) inspired by learn-
ing theory is checked in Figure 4. The required better than random guessing edge γt is depicted in
Figure 4a, it is always greater than 0 and our weak learning condition is thus non-vacuous. In Fig-
ure 4b, the representations we learned using BoostResNet is increasingly better (for this classification
task) as the depth increases.
depth T	depth T
(a) Yt	(b) Yt
Figure 4: Visualization of required larger than 0 edge Yt and edge for each residual block γt. The
x-axis represents depth, and the y-axis represents Yt or Yt values. The plots are for a convolutional
network composed of 50 residual blocks and trained on the SVHN dataset.
6 Conclusions and Future Works
Our proposed BoostResNet algorithm achieves exponentially decaying (with the depth T ) training
error under the weak learning condition. BoostResNet is much more computationally efficient com-
pared to end-to-end back-propagation in deep ResNet. More importantly, the memory required by
BoostResNet is trivial compared to end-to-end back-propagation. It is particularly beneficial given
the limited GPU memory and large network depth. Our learning framework is natural for non-
differentiable data. For instance, our learning framework is amenable to take weak learning oracles
using tensor decomposition techniques. Tensor decomposition, a spectral learning framework with
theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al., 2015). We plan to
extend our learning framework to non-differentiable data using general weak learning oracles.
8
Under review as a conference paper at ICLR 2018
References
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
The shattered gradients problem: If resnets are the answer, then what is the question? arXiv
preprint arXiv:1702.08591, 2017.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE transactions on neural networks, 5(2):157-166,1994.
Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. Advances in neural information processing systems, 18:123, 2006.
Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep boosting. In Proceedings of the 31st Inter-
national Conference on Machine Learning (ICML-14), pp. 1179-1187, 2014.
Corinna Cortes, Xavi Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Adanet: Adap-
tive structural learning of artificial neural networks. arXiv preprint arXiv:1607.01097, 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Yoav Freund and Robert E Schapire. A desicion-theoretic generalization of on-line learning and an
application to boosting. In European conference on computational learning theory, pp. 23-37.
Springer, 1995.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1440-1448, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 580-587, 2014.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Aistats, volume 9, pp. 249-256, 2010.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,
2016.
Kaiming He and Jian Sun. Convolutional neural networks at constrained time cost. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep con-
volutional networks for visual recognition. In European Conference on Computer Vision, pp.
346-361. Springer, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
9
Under review as a conference paper at ICLR 2018
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
Iutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-
bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert MUller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic seg-
mentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3431-3440, 2015.
Hrushikesh N Mhaskar and Tomaso Poggio. Deep vs. shallow networks: An approximation theory
perspective. Analysis and Applications, 14(06):829-848, 2016.
Indraneel Mukherjee and Robert E Schapire. A theory of multiclass boosting. Journal of Machine
Learning Research, 14(Feb):437-497, 2013.
Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of conver-
gence o (1/k2). In Doklady an SSSR, volume 269, pp. 543-547, 1983.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12
(1):145-151, 1999.
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. In Advances in neural information processing systems,
pp. 91-99, 2015.
Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to combina-
torial optimization, Monte-Carlo simulation and machine learning. Springer Science & Business
Media, 2013.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun.
Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv
preprint arXiv:1312.6229, 2013.
Shai Shalev-Shwartz. Selfieboost: A boosting algorithm for deep learning. arXiv preprint
arXiv:1411.3436, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Rupesh Kumar Srivastava, Klaus Greff, and JUrgen Schmidhuber. Highway networks. arXivpreprint
arXiv:1505.00387, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, 2015.
Mark Tygert, Arthur Szlam, Soumith Chintala, Marc’Aurelio Ranzato, Yuandong Tian, and Woj-
ciech Zaremba. Convolutional networks and learning invariant to homogeneous multiplicative
scalings. arXiv preprint arXiv:1506.08230, 2015.
10
Under review as a conference paper at ICLR 2018
Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of
relatively shallow networks. In Advances in Neural Information Processing Systems, pp. 550-558,
2016.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
11
Under review as a conference paper at ICLR 2018
Appendix: Learning Deep ResNet Blocks Sequentially
using Boosting Theory
A Related Works
A.1 Loss function and architecture selection
In neural network optimization, there are many commonly-used loss functions and criteria,
e.g., mean squared error, negative log likelihood, margin criterion, etc. There are extensive
works (Girshick, 2015; Rubinstein & Kroese, 2013; Tygert et al., 2015) on selecting or modifying
loss functions to prevent empirical difficulties such as exploding/vanishing gradients or slow learn-
ing (Balduzzi et al., 2017). However, there are no rigorous principles for selecting a loss function
in general. Other works consider variations of the multilayer perceptron (MLP) or convolutional
neural network (CNN) by adding identity skip connections (He et al., 2016), allowing information
to bypass particular layers. However, no theoretical guarantees on the training error are provided de-
spite breakthrough empirical successes. Hardt et al. (Hardt & Ma, 2016) have shown the advantage
of identity loops in linear neural networks with theoretical justifications; however the linear setting
is unrealistic in practice.
A.2 Learning algorithm design
There have been extensive works on improving BP (LeCun et al., 1989). For instance, mo-
mentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al.,
2011) and its extension Adadelta (Zeiler, 2012). Most recently, Adaptive Moment Estimation
(Adam) (Kingma & Ba, 2014), a combination of momentum and Adagrad, has received substan-
tial success in practice. All these methods are modifications of stochastic gradient descent (SGD),
but our method only requires an arbitrary oracle, which does not necessarily need to be an SGD
solver, that solves a relatively simple shallow neural network.
B	Proof for Lemma 3.2: the strong learner is a ResNet
Proof. In our algorithm, the input of the next module is the output of the current module
gt+1(x) = ft(gt(x)) + gt(x),	(8)
we thus obtain that each weak learning module is
ht(x) = αt+1Wt+1(ft(gt(x)) + gt(x)) - αtw[gt(x)	(9)
=αt+1wt+lgt+1(X) - αtwΓgt(X),	(IO)
and similarly
ht+1 = αt+2wt+2gt+2(X) - αt+1wt+lgt+1(X).	(II)
Therefore the sum over ht(X) and ht+1(X) is
ht(x) + ht+ι(x) = αt+2wt+2gt+2(x) - atw；gt(x)	(12)
And we further see that the weighted summation over all ht(X) is a telescoping sum
T
Eht(X) = aτ +1 w；+igτ +ι(x) - aιw；gi(x) = aτ +iw；+igτ +ι(x).	(13)
t=1
□
C Proof for Theorem 4.2: binary class telescoping sum boosting
THEORY
Proof. We will use a 0-1 loss to measure the training error. In our analysis, the 0-1 loss is bounded
by exponential loss.
12
Under review as a conference paper at ICLR 2018
The training error is therefore bounded by
Pr (p(aτ +1wJ+1 gτ +1(Xi)) = yi)
i~D1
m
=EDι(i)1{σ(ατ +ιwJ+ι9τ +ι(χi)) = yi}
(14)
(15)
i=1
m
EDI”
i=1
m
{σ (W Mg))="
(16)
≤ EDι(i)
i=1
m
exp - -yi
EMXi)》
t=1	)
(17)
T
EDT +1(i)∏ Zt
(18)
i=1
T
Zt
t=1
t=1
(19)
m
where Zt = E Dt(i)exp(-%ht(χi)).
i=1
We choose at+ι to minimize Zt.
∂Zt
∂αt+ι
m
-E Dt(i)yiθt+ι exp (-yiht(χi))
i=1
m
-Zt E Dt+1(i)yiot+1(i) = 0
i=1
(20)
(21)
Furthermore each learning module is bounded as we see in the following analysis. We obtain
m
Zt = E Dt(i)e—yiht (Xi)
i=1
m
=^E Dt(i)e—at+1yiot+1(Xi)+atyiot(Xi)
(22)
(23)
i=1
m
≤ E Dt(i)e—at+1yiot+1(Xi)
i=1
m
=E Dt(D
i=1
m
≤ EDt ⑻
i=1
m
EDt(M-
i=1	、
m
=E Dt(D
i=1
m
=E Dt(D
i=1
e—αt+1
(
(
m
E Dt(i)eatyi°t(Xi)
i=1
l + yiot+1(xi)
2
1 + yiθt+ι(χi)
2
1 + yiθt(χi)
(
(24)
2
m
+ at+1iθ2+1(Xi) E Dt(i)e≈t
i=1
e—at+1 +
eat +
1 + yiθt+ι(χi)
(
ι + yiot(xi)
2
— at
1-yiot(xi)
2
(25)
2
1 - %ot+ι(xi)
2
eat+1)
(26)
1 - yOt(Xi) e—at
2
)
(27)
e—at+1 +
1 - yiθt+ι(χi)
2
eat+1)
eat + e—at
2
(28)
e—at+1 + gat+1	6-a t+1  ɛɑt+1
2
+
2
yiθt+ι(χi))
eat + e—at
2
(29)
e-at+1 + eat+1	e-at+1 - eat+1
2
+
2
、eat + e—at
,γtJ 2~-
(30)
13
Under review as a conference paper at ICLR 2018
Equation (24) is due to the non-positive correlation between exp(-yot+1(x)) and exp(yot(x)).
Jensen’s inequality in Equation (27) holds only when |yiot+1(xi)| ≤ 1 which is satisfied by the
definition of the weak learning module.
The algorithm chooses αt+1 to minimize Zt.
minimizing the bound in Equation (30)
We achieve an upper bound on Zt
I I-Yt
,VI-Y2
by
Zt ≤ θ
=/
e-αt+1 + eαt+1	e-αt+1 - eαt+1	eαt + e-αt
2	+	2	Ytl	2
1⅛ = L
αt+1 = 1 ln( τ+γt )
(31)
(32)
Therefore over the T modules, the training error is upper bounded as follows
T
T
iPD(P(aT +1wT+lgT +1(Xi ))) = y)≤∏ 行 ≤∏ ^-Y
t=1
t=1
eχp --1TY2)
(33)
Overall, Algorithm 1 leads us to consistent learning of ResNet.
□
D Proof for Corollary 4.3: Generalization Bound
Rademacher complexity technique is powerful for measuring the complexity of H any family of
functions h : X → R, based on easiness of fitting any dataset using classifiers in H (where X is
any space). Let S =< x1, . . . , xm > be a sample of m points in X. The empirical Rademacher
complexity of H with respect to S is defined to be
RS(H) d=ef Eσ
1m
sup — 5"^σih(xi)
h∈H m i=1
(34)
where σ is the Rademacher variable. The Rademacher complexity on m data points drawn from
distribution D is defined by
Rm (H) = ES〜D [Rs(H)].	(35)
Proposition D.1. (Theorem 1 Cortes et al. (2014)) Let H be a hypothesis set admitting a decompo-
sition H = ∪li=1Hi for some l > 1. Hi are distinct hypothesis sets. Let S be a random sequence
of m points chosen independently from X according to some distribution D. For θ > 0 and any
H = ET=I ht, with probability at least 1 — δ,
T
pr(yH(X) ≤ 0) ≤ PMyH(χ) ≤ θ) +4 立Rm(Hkt) + 2√lθg-
D	S	θ	t θm
t=1
+ j θ2 log ( θm )嚓 + 察	(36)
for all ht ∈ Hkt .
∙-v	__l_ ∙-v	∙~v	∙-v	∙-v
Lemma D.2. Let h	= W 1 f,	where W ∈ Rn, f	∈ Rn.	Let H	and F	be two hypothesis sets, and
∙∙W	∙~V	∙∙W	∙~v	∙~v	∙~v
h ∈ H , fj ∈ F, ∀j ∈ [n]. The Rademacher complexity of H and F with respect to m points from
D are related as follows
∙-v	∙-v
Rm (H) = Il WllIRm(F).	(37)
D.1 ResNet Module Hypothesis Space
Let n be the number of channels in ResNet, i.e., the number of input or output neurons in a module
ft(gt(X)). We have proved that ResNet is equivalent as
T
F(x) = WT E f(gt(x))
t=1
(38)
14
Under review as a conference paper at ICLR 2018
We define the family of functions that each neuron ft,j , ∀j ∈ [n] belong to as
Ft = {x → ut-ι,j(σ ◦ ft-ι)(x): ut-ι,j ∈ RnJut-Ij∣∣ι ≤ Λt,t-ι,工…∈ Ft-ι}	(39)
where ut-1,j denotes the vector of weights for connections from unit j to a lower layer t - 1,
σ ◦ ft-1 denotes element-wise nonlinear transformation on ft-1. The output layer of each module is
connected to the output layer of previous module. We consider 1-layer modules for convenience of
analysis.
Therefore in ResNet with probability at least 1 - δ,
Pr(yF(X) ≤O) ≤ PMyF (X) ≤ θ) + 4 AWkRm (Ft) + 2 J黑
t=1
+《京 log (容) lomT+logmL	(40)
for all ft ∈Ft.
Define the maximum infinity norm over samples as r∞ = ES〜D [maxi∈[mj] ∣Xi∣∞] and the product
of lι norm bound on weights as Λt = ∩t'=ι 2Λt',t'-ι. According to lemma 2 of Cortes etal.
(2016), the empirical Rademacher complexity is bounded as a function of r∞, Λt and n:
Rm(Ft) ≤ r∞Λt Jlog2n)	(41)
2m
Overall, with probability at least 1 - δ,
PDr (yF(X) ≤ 0) ≤ PSr (yF(X) ≤ θ) +
4Whr∞ ≠0gmn)
θ
T
E Λt
t=1
+2θ∕omT
]logɪ +
m
log 2
2m
(42)
∑2 log
for all ft ∈ Ft.
E Proof for Theorem E: Margin and Generalization Bound
Theorem E.1. [ Generalization error bound ] Given algorithm 1, the fraction of training examples
with margin at most θ is at most (1 + ι 2	1)2 exp(-1Y2T). And the generalization error
√γT+1
PrD (yF(X) ≤ 0) satisfies
Pr(yF(X) ≤ 0) ≤ (1 + -τ~~7)2 eχp(-1Y2T)
D	- 1	2
γT+1
l 4C0r∞ 八Og(2n) yTΛ 2 /logT l αtα T χ∖	,彳。、
+	∑ At + θ V 丁 + β(θ, m，T，δ)	(43)
with probability at least 1 - δ for β(θ, m, T, δ)
def
log
(θ2m ʌl log T + log 2
y log T J I m + 2m
Now the proof for Theorem E is the following.
15
Under review as a conference paper at ICLR 2018
Proof. The fraction of examples in sample set S being smaller than θ is bounded
1 m
Pr(yF(x) ≤ θ) ≤ -1 V 1{yiF(Xi) ≤ θ}
S	m
i=1
ɪ m T
=—E 1{yi E ht(xi) ≤ θατ +1}
m i=1	t=1
mT
≤ 土 E exp(-yi £ ht(xi) + θɑτ+1)
m i=1	t=1
]m	T
=exp(θατ +1)- EeXP(-yi E ht(xi))
m i=1	t=1
T
=exp(θɑτ +1) ɪɪ Zt
t=1
(44)
(45)
(46)
(47)
(48)
To bound exp(θατ +1) = J(I-YT：彳,we first bound YT +1: We know that ET=I ∏T=t+1(1 -
Yt )γ2 ≤ (1 - Y2)tTY2 for all ∀γt ≥ Y2 + C if Y2 ≥ 宁.Therefore ∀ Yt ≥ Y2 + C and Y2 ≥ 宁
YT+1 =(1 - YT )YT + YT	(49)
TT	T	
=En (1-Y2,)Y2 + ∏(1-Y2)Y2	(50)
t=1 t' = t+1	t=1	
T ≤ E(I-Y2)t-tY2 + (1 - Y2)ty2	(51)
t=1	
T-1	
= E(I-Y2)tγ2 + (1 - y2)t y2	(52)
t=0	
=1 - (1 - y2)t + (1 - y2)t y2	(53)
=1 - (1 - y2)(1 - Y2)t	(54)
Therefore	
T pr(yF(x) ≤ θ) ≤ exp(θ0r+1) 口 Zt	(55)
t=1	
T = (l + 7T+1) 2 ʊ Zt ^- yt+1	& t	(56)
= (FTW)2II「F 1 - yt+1 廿 V	(57)
=(1 + -ɪ2~d 2 exp(-2Y2T) -		1	2	(58)
YT + 1	
≤ (1+	1 2 —~/2 exp(-1Y2T)	(59)
√1-(1-Y2)(1-γ2)τ	
As T → ∞, PrS(yF(x) ≤ θ) ≤ 0 as exp(-1 y2T) decays faster than (1 +	ι2	_1_)2.
λ∕ι-(ι-γ2)(ι-γ2)T
□
16
Under review as a conference paper at ICLR 2018
F	Telescoping Sum Boosting for Multi-calss Classification
Recall that the weak module classifier is defined as
ht(x) = αt+1ot+1 (x) - αtot (x) ∈ RC,	(60)
where ot(x) ∈ ∆C-1.
The weak learning condition for multi-class classification is different from the binary classification
stated in the previous section, although minimal demands placed on the weak module classifier
require prediction better than random on any distribution over the training set intuitively.
We now define the weak learning condition. It is again inspired by the slightly better than random
idea, but requires a more sophisticated analysis in the multi-class setting.
F.1 Cost Matrix
In order to characterize the training error, we introduce the cost matrix C ∈ Rm×C where each
row denote the cost incurred by classifying that example into one of the C categories. We will
bound the training error using exponential loss, and under the exponential loss function defined as in
Definition G.1, the optimal cost function used for best possible training error is therefore determined.
Lemma F.1. The optimal cost function under the exponential loss is
exp (st(xi, l) - st(xi, yi))	if l = yi
Ct(i,l) = - - H exp(st(xi,l') - st(xi,yi) if I = y	(61)
I ι'=yi
t
where st (x) =	hτ (x).
τ=1
F.2 Weak Learning Condition
mm
-£ <Ct(i,:),ot+i(Xi)>	-E <Ct-i(i,:),ot(Xi)>
Definition F.2. Let γt+ι =	*=1 m-and Yt =	i=m-----. A multi-class
E E Ct(i,i)	E E Ct-ι(i,i)
i=1 l=yi	i=1 l=yi
weak module classifier ht(x) = αt+1ot+1(x) - αt ot (x) satisfies the γ -weak learning condition if
γ2	γ2
t+-旌 Y ≥ Y2 > 0, and Cov(< Ct(i, ：),ot+i(xi) >,< Ct(i, ：),ot+i(xi) >) ≥ 0.
We propose a novel learning algorithm using the optimal edge-over-random cost function for training
ResNet under multi-class classification task as in Algorithm 3.
Theorem F.3. The training error of a T -module ResNet using Algorithm 3and 4 decays exponen-
tially with the depth of the ResNet T,
C 1m	2
-m-- ELnxp(ST(Xi)) ≤ (C - 1)e- 1Tγ
i=1
if the weak module classifier ht(x) satisfies the Y -weak learning condition ∀t ∈ [T].
The exponential loss function defined as in Definition G.1
(62)
17
Under review as a conference paper at ICLR 2018
Algorithm 3 BoostResNet: telescoping sum boosting for multi-class classification
Input: Given (x1, y1), . . . (xm, ym) where yi ∈ Y = {1, . . . , C} and a threshold γ
Output: {ft(∙),∀t} and WT +ι	> Discard wt+ι, ∀t = T
1:	Initialize t _ 0, γo — 1, a0 — 0, oo — 0 ∈ RC, so(xi,l) = 0, ∀i ∈ [m], l ∈ Y
2:	Initialize cost function C°(i,l) J
3:	while γt > γ do
1 if l = yi
1	- C if l = yi
4:
5:
ft(∙), αt+1, wt+ι, ot+ι(X) J Algorithm4(gt(χ), Ct, Ot(X), Qt)
Compute γt J
JYt+1-γ
V * 1-γ2
> where γt+ι J
m
-E Ct(i,)0t+1(xi)
i=1
m
E E Ct(W
i=1 l=yi
6:
7:
8:
9:
Update st+ι(xi,l) J St(Xi,l) + ht(xi,l) > where ht(xi,l) = Qt+ιOt+ι(xi,l) 一 αtθt(xi,l)
est+1 (xi,l)-st+1 (xi,yi)	if l = yi
一 E est+ι(χi^li')-st+ι(a,yi')	ifl = y,
l'=yi	?
tJt+1
end while
10:	T J t 一 1
Algorithm 4 BoostResNet: oracle implementation for training a ResNet module (multi-class)
Input： gt(x),st,ot(x) and at
Output： ft(∙), Qt+ι, Wt+ι and ot+ι(x)
m	，，丁
1:	(ft	at+ι	VWt+1)	j-	arg min〉:〉:	eαV	[f(gt(xi),l)	f(gt(Xi),yi)+gt(xi,l)	gt(χi,yi)]
(f,α,V) i=1 l=yi
2:	ot+1 (X) J Wt+ι [ft(gt(X)) + gt(x)]
F.3 Oracle Implementation
def m
We implement an oracle to minimize Zt = E E est(xi,l) St(Xi,yi)eht(Xi,l) ht(xi,yi) given current
i=1 l=yi
state st and hypothesis module ot(X). Therefore minimizing Zt is equivalent to the following.
m
min	est(χi,l)-St(Xi,yi)e-αt(Ot(Xi,l)-ot(χi,yi))eaVτ[f(gt(χi),l)-f(gt(χi),yi)+gt(χi,l)-gt(χi,yi)]
(f,α,V)
(f,α,V) i=1 l=yi
(63)
m
≡ (fminV )	eaV If (gt(χi),i)-f (gt(χi),yi)+gt(χi,i)-gt(χi,yi)]
(f,α,V) i=1 l=yi
m
≡ mfin	e-avτ [f (χi,yi)+gt(χi,yi)] Eeαvτ [f(Xi,l)+gt(Xi,l)]
α,f,v i=1	l=yi
(64)
(65)
G Proof for Theorem F.3 multiclas s boosting theory
Proof. To characterize the training error, we use the exponential loss function
Definition G.1. Define loss function for a multiclass hypothesis H(Xi ) on a sample (Xi, yi) as
Lenx(H(Xi),yi) = E exp((H(Xi, l) - H(Xi,yi))).	(66)
l=yi
t
Define the accumulated weak learner St(Xi,l)	= E ht′(Xi,l) and the loss Zt
t'=1
m
exp(St(Xi, l) 一 St(Xi, yi)) exp(ht(Xi, l) 一 ht(Xi, yi)).
i=1 l=yi
18
Under review as a conference paper at ICLR 2018
t
Recall that St(Xi,l) = E ht' (XiJ) = αt+1Wt+1gt+1(xi), the loss for a T-module multiclass
t'=1
ResNet is thus
.Pr (p(ατ +1WT+1gτ +1(χi)) = yi) ≤
i〜D1
m
—ELnXP(ST(Xi))
m /
i=1
m
m∑∑exp (η(sτ(XiN) - ST(xi,%)))
r= i=1 l=yi
1Z
ZT
m
fl 二
U Zt-1
t=1 t-1
(67)
(68)
(69)
(70)
≤
≤
Note that Z0 =煮 as the initial accumulated weak learners s0(xi, l) = 0.
The loss fraction between module t and t - 1, ZZtI, is related to Zt - Zt-1 as ZZtI
The Zt is bounded
m
Zt =ΣΣexp(st(xi,l) - st(i,yi) + ht(xi,l) — ht(xi,yi))
+1.
(71)
i=i ι=yi
m
m
VΣΣ est(xi,l)-st(xi,yi)eat+1θt+1(xi,l)-at+1θt+1(xi,yi) ΣΣ gSt(xi,l)-st(xi,yi) e-atot(xi,l) + αtθt(xi,yi)
(72)
m
≤ΣΣ Cs t(Xi ,L) - St(Xilyi)(
i=1 l=yi	`
m
e-at+1 + eat+1 e
2	—
,-at+1 - g«t+1
2
(ot+ι(χi,yi) - ot+ι(χi,l)))
EEeStT(xi,i)-st-1 (χi,yi)
i=1 i=yi
(eat + e-at)
(73)
=(
e-αt+1 + eαt+1 - 2
Zt-I +
eat+1 - e-at+1
m
E
i=1
Ct(χi, ：),ot+i(xi,：) >)(
eαt + e-αt、
2)
(74)
e-at+1 + eat+1 - 2
V(--------2-------Zt-I+
eat+1 - e-at+1 E
2 Z
Ct(xi, ∙∙),Uγ (xi,:) >)(
eαt + e-αt
2
(75)
=(
e-at+1 + eat+1 - 2
Zt-i +
eat+1 - e-at+1
"eat+ e-atλ
(-Yt)Zt-I) I--2----)
(76)
2
2
<
<
)
2
2
Therefore
Zt / C e-αt+1
Z-1 ≤ k
+ eαt+1
2	+
e-αt+1 - e'
°t+1	∖ e eαt + e-αt∖
(77)
2
The algorithm chooses at+1 to minimize Zt. We achieve an upper bound on Zt,
/ 1-γ
VT-^
19
Under review as a conference paper at ICLR 2018
by minimizing the bound in Equation (77)
e-αt+1 + eαt+1	e-αt+1 - eαt+1	eαt + e-αt
Zt ≤ (	2	+	2	γt) -2一
αt+1 = 1 ln( T+γt )
=j⅛⅛ = ^-Y
Therefore over the T modules, the training error is upper bounded as follows
(78)
(79)
Pr (p(ατ +1wJ+1gτ +1
i〜D
T __________
(Xi))) = yi) ≤ ∏ JI-Y2
t=1
T
≤ ∏ √1 - Y2 = exp
t=1
(80)
Overall, Algorithm 3 and 4 leads us to consistent learning of ResNet.
H Experiments
H. 1 Training error degradation of e2eBP on ResNet
We investigate e2eBP training performance on various depth ResNet. Surprisingly, we observe a
training error degradation for e2eBP although the ResNet’s identity loop is supposed to alleviate this
problem. Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local
optima. This phenomenon is explored further in Figures 5a and 5b, which respectively show how
training and test accuracies vary throughout the fitting process. Our proposed sequential training
procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth
increases.
se」n£ 6u-u-e」l
3	4	5	6	7
Number of Batches
8	9	10
×104
>oe」n。。—κel
(a) e2eBP training accuracy	(b) e2eBP test accuracy
Figure 5: Convergence of e2eBP (baseline) on multilayer perceptron residual network (of various
depths) on MNIST dataset.
H.2 SVHN and CIFAR-10 performance results
Besides e2eBP, we also experiment with standard boosting (AdaBoost.MM Mukherjee & Schapire
(2013)), as another baseline, of convolutional modules. In this experiment, each weak learner is a
residual block of the ResNet, paired with a classification layer. We do 25 rounds of AdaBoost.MM
and train each weak learner to convergence.
Table 1 and table 2 exhibit a comparison of BoostResNet, e2eBP and AdaBoost performance on
SVHN and CIFAR-10 dataset respectively.
On SVHN dataset, the advantage of BoostResNet over e2eBP is obvious. Using 3 × 108 number of
gradient updates, BoostResNet achieves 93.8% test accuracy whereas e2eBP obtains a test accuracy
of 83%. The training and test accuracies of SVHN are listed in Table 1. BoostResNet training
allows the model to train much faster than end-to-end training, and still achieves the same test
accuracy when refined with e2eBP. To list the hyperparameters we use in our BoostResNet training
20
Under review as a conference paper at ICLR 2018
after searching over candidate hyperparamters, we choose learning rate to be 0.004 with a 9 × 10-5
learning rate decay. The gamma threshold is set to be 0.001 and the initial gamma value on SVHN
is 0.75.
On CIFAR-10 dataset, the main advantage of BoostResNet over e2eBP is the speed of training.
BoostResNet refined with e2eBP obtains comparable results with e2eBP. This is because we are
using a suboptimal architecture of ResNet which overfits the CIFAR-10 dataset. AdaBoost, on
the other hand, is known to be resistant to overfitting. Therefore, AdaBoost achieves the highest
test accuracy on CIFAR-10. To list the hyperparameters we use in our BoostResNet training after
searching over candidate hyperparamters, we choose learning rate to be 0.014 with a 3.46 × 10-5
learning rate decay. The gamma threshold is set to be 0.007 and the initial gamma value on CIFAR-
10 is 0.93.
Training:	BoostResNet	e2eBP	BOOSTRESNET+e2eBP	e2eBP	AdaBoost
NGU	3 × 108	3 × 108	2 × 1010	2 × 1010	-1.5 × 109-
TRAIN	96.9%	85%	98.8%	98.8%	95.6%
TEST	93.8%	83%	96.8%	96.8%	92.3%
Table 1: Accuracies of SVHN task. NGU is the number of gradient updates taken by the algorithm
in training.
Training:	BoostResNet	e2eBP	BoostResNet+e2eBP	e2eBP	AdaBoost
NGU	3 × 109	3 × 109	1 × 1011	1 × 1011	1.5 × 1010
TRAiN	92.1%	82%	99.6%	99.7%	95.6%
TEST	82.1%	80%	88.1%	90.0%	92.3%
Table 2: Accuracies of CIFAR-10 task. NGU is the number of gradient updates taken by the
algorithm in training.
21