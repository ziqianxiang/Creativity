Under review as a conference paper at ICLR 2018
A Tensor Analysis on Dense Connectivity via
Convolutional Arithmetic Circuits
Anonymous authors
Paper under double-blind review
Ab stract
Several state of the art convolutional networks rely on inter-connecting differ-
ent layers to ease the flow of information and gradient between their input and
output layers. These techniques have enabled practitioners to successfully train
deep convolutional networks with hundreds of layers. Particularly, a novel way
of interconnecting layers was introduced as the Dense Convolutional Network
(DenseNet) and has achieved state of the art performance on relevant image recog-
nition tasks. Despite their notable empirical success, their theoretical understand-
ing is still limited. In this work, we address this problem by analyzing the effect of
layer interconnection on the overall expressive power of a convolutional network.
In particular, the connections used in DenseNet are compared with other types of
inter-layer connectivity. We carry out a tensor analysis on the expressive power
inter-connections on convolutional arithmetic circuits (ConvACs) and relate our
results to standard convolutional networks. The analysis leads to performance
bounds and practical guidelines for design of ConvACs. The generalization of
these results are discussed for other kinds of convolutional networks via general-
ized tensor decompositions.
1	Introduction
Recently, densely connected networks such as FractalNet (Larsson et al., 2016), ResNet (He et al.,
2016), and DenseNet (Huang et al., 2016), have obtained state of the art performance on large
problems where highly deep network configurations are used. Adding dense connections between
different layers of a network virtually shortens its depth, thus allowing a better flow of information
and gradient through the network. This makes possible the training of highly deep models. Mod-
els with these types of connections have been successfully trained with hundreds of layers. More
specifically, DenseNets have achieved state of the art performance on the CIFAR-10, CIFAR-100,
SVHN, and ImageNet datasets, using models of up to 1 thousand layers in depth. Nevertheless,
whether these connections provide a fundamental enhancement on the expressive power of a net-
work, or just improve the training of the model, is still an open question. In Huang et al. (2016),
DenseNet models with 3 times less parameters than its counterpart (ResNets) were able to achieve
the same performance on the ImageNet challenge. Moreover, a theoretical understanding of why the
connections used by DenseNets lead to better performance compared with FractalNets or ResNets
is still pending.
Despite the popularity of these models, there are few theoretical frameworks explaining the power
of these models and providing insights to their performance. In Cohen et al. (2016a), the authors
considered convolutional networks with linear activations and product pooling layers, called convo-
lutional arithmetic circuits (ConvACs), and argued for the expressiveness of deep networks using
a tensor based analysis. This analysis has been extended to rectifier based convolutional networks
via generalization of the tensor product Cohen & Shashua (2016a). In Cohen & Shashua (2016a),
it was shown that ConvACs enjoy a greater expressive power than rectifier based models despite
the popularity of rectifier based networks in practice. Indeed the empirical relevance of ConvAC
was demonstrated through an architecture called SimNets Cohen et al. (2016b). In addition, the
generative ConvAC of Sharir et al. (2016) achieved state of the art performance in classification of
images with missing pixels. These results served as motivation for the works of Cohen & Shashua
(2016b); Cohen et al. (2017); Levine et al. (2017); Sharir & Shashua (2017), where different aspects
of ConvACs were studied from a theoretical perspective.
1
Under review as a conference paper at ICLR 2018
In Cohen & Shashua (2016b) the inductive bias introduced by pooling geometries was studied.
Later, Levine et al. (2017) makes use of the quantum entanglement measure to analyze the inductive
bias introduced by the correlations among the channels of ConvACs. Moreover, Sharir & Shashua
(2017) generalizes the convolutional layer of ConvACs by allowing overlapping receptive fields,
in other words permitting stride values lower than the convolution patch size. These locally over-
lapping connections led to an enhancement on the expressive capacity of ConvACs. The notion of
inter-layer connectivity for ConvACs was addressed by Cohen et al. (2017) in the context of sequen-
tial data processing, such as audio and text related tasks. In that work, the expressive capabilities of
interconnecting processing blocks from a sequence was studied. Nevertheless, these types of inter-
connections are related to the sequential nature of the problem and different from the ones used in
ResNet, FractalNet and DenseNet.
In this work, we extend the tensor analysis framework of Cohen et al. (2016a) to obtain insightful
knowledge about the effect of dense connections, from the kind used in DenseNets, FractalNet and
ResNet, on the expressiveness of deep ConvACs. We study the expressive capabilities provided by
different types of dense connections. Moreover, from these results we derive performance bounds
and practical guidelines for selection of the hyperparameters ofa deep ConvAC, such as layer widths
and the topology of dense connections. These results serve as the first step into understanding dense
connectivity in rectifier networks as well, since they can be further extended to include rectifier
linear units, in the same spirit as the generalization of the tensor products done by Cohen & Shashua
(2016a).
The remainder of this paper is organized as follows. In Section 2, we introduce the notation and
basic concepts from tensor algebra. In Section 3, we present the tensor representation of ConvACs
as introduced by Cohen et al. (2016a), and later in Section 4, we obtain tensor representations for
densely connected ConvACs. In Section 5, performance bounds and design guidelines are derived
for densely connected ConvACs.
2	Preliminaries
The term tensor refers to a multi-dimensional array, where the order of the tensor corresponds
to the number of indexes required to access one of its entries. For instance, a vector is a tensor
of order 1 while a matrix is a tensor of order 2. In general a tensor A of order N requires N
indexes (d1 , . . . , dN) to access one of its elements. For the sake of notation, given I ∈ N, we
use the expression [I] to denote the set {1, 2, . . . , I}. In addition, the (d1 , . . . , dN)-th entry of
a given tensor of order N and size Mi X M2 X …X MN is denoted as Adi,…环,where di ∈
[Mi] for all i ∈ [N]. Moreover, for the particular case of tensors of order N with symmetric
sizes Mi = M2 =…=MN = M, We use (RM产N as shorthand notation for RM× ×M. A
crucial operator in tensor analysis is the tensor product a since it is necessary for defining the rank
of a tensor. For two tensors B ∈ RM1 × ×Mp and C ∈ RMp+i × ×Mp+q, the tensor product is
defined such that B X C ∈ RM1×…×Mp+q and (B 0 C)di,…&十勺=Bdi,...,dp°dp+ι,…,dp+q for all
(di,..., dp+q). In tensor algebra, a tensor A ∈ RM1×M2× ×MN is said to have rank 1 if it can
be expressed as A = V(I) 0 ∙ ∙ ∙ 0 V(N), where v(i) ∈ RMi for all i ∈ [N]. Moreover, any tensor
A ∈ RMi ×M2 × ×MN can be expressed as a sum of rank-1 tensors, that is
Z
A = X vZ1) 0 …0 VzN),	⑴
z=i
where Z ∈ N is sufficiently large and Vz(i) ∈ RMi for i ∈ [N]. Note that this statement is
trivial for Z = QiN=i Mi. On the other hand, when Z is the minimum number such that (1)
is satisfied, the rank of the tensor is defined to be rank(A) = Z and (1) becomes equivalent
to the well known CANDECOMP/PARAFAC (CP) decomposition of A. Another operator, that
is on the core of the former works of Cohen & Shashua (2016a); Cohen et al. (2016a); Levine
et al. (2017), is the matricization operator. The operator [A] denotes the matricization of a tensor
A ∈ RM1 × × MN of order N. This matricization of the tensor A re-orders its elements into a matrix
[A] ∈ RM1∙M3…MN-1×M2∙M4…MN with Ayi,...,dN intherow 1 + PK(d2i-1-1) QN/+1 M2j-1
and column 1 + PiN=/i2 (d2i-i - 1) QjN=/i2+i M2j. This is operator is of great use since it enjoys
properties such as [A0 B] = [A] [B] and rank(A) ≥ rank([A]), where denotes the Kronecker
2
Under review as a conference paper at ICLR 2018
product of two matrices. Note that, since the Kronecker product is multiplicative in the rank, we
have that rank(A ③ B) ≥ rank([A 0 B]) = rank([ADrank([B]) which is a central property of this
theoretical analysis framework.
3	Convolutional Arithmetic Circuits as Tensor Decompositions
fθd(xi)
conv(i, z) = haz,i, δii	hy(X) = hay, pool(:)i
δi = [fθ1(xi), . . . ,fθM(xi)]
Figure 1: (a) Example of a shallow (i.e., L = 1) convolutional arithmetic circuit. (b) Example of a
deep convolutional arithmetic circuit.
A ConvAC is a convolutional neural network that utilizes linear activation functions with product
pooling, unlike most popular convolutional networks which make use of rectifier activations with
max or average pooling. Moreover, the input of the network is modeled by X = (x1, . . . , xN) ∈
(Rs )N , where xi ∈ Rs denotes the vectorization of the i-th patch of the input image. For this
analysis, it is assumed that a set of M features is obtained from every patch, that is fθd(xi) ∈ R for
all i ∈ [N], d ∈ [M]. These features are selected from a given parametric family F = {fθ : Rs →
R : θ ∈ Θ}, such as Gaussian kernels, wavelet functions, or learned features. Then, to determine
whether an input X belongs to a class belonging to the set Y, the network evaluates the some score
functions hy(X) ∈ R and decides for the class y ∈ Y such that
hy(X) =maxhy(X).
y∈Y
Using this formulation, in Figure 1(a) we observe an example of a single hidden layer ConvAC,
while in Figure 1(b) we observe the general case of a deep arithmetic circuit of L layers. As shown
by Cohen et al. (2016a), any score function of a ConvAC can be expressed as an homogeneous
polynomial with degree N on the input features of the form
MN
hy(X) =	X	Ayd1,...,dNYfθdi(xi),	(2)
d1 ,...,dN =1	i=1
where Adl dN ∈ R are the polynomial coefficients stored in the grid-tensor Ay ∈ (RM)0N.
In other words, a score function hy(X) is a polynomial of MN variables fθd (xi) ∈ R for all
i ∈ [N], d ∈ [M], degree N, and MN polynomial coefficients stored in the grid-tensor Ay.
3
Under review as a conference paper at ICLR 2018
For the special case of a shallow ConvAC with 1 × 1 convolutions and Z hidden units1, shown in
Figure 1(a), the score functions are computed from the weight vectors az,i , [az1,i, . . . , azM,i]T ∈ RM
and ay , [ay1 , . . . , ayZ]T ∈ RZ for all i ∈ [N] and z ∈ [Z]. This leads to the score function
Z	Z NM
hy(X) = hay, pool(:)i = X azy pool(z) = X azy Y X azd,ifθd(xi).	(3)
z=1	z=1	i=1 d=1
The first step of the tensor analysis framework is to obtain an expression (in terms of the network
parameters azy and azd,i) of the grid-tensor Ay that represents this concrete network architecture.
In other words, obtaining the expression for Ay that transforms (2) into (3). This expression was
already obtained in Cohen & Shashua (2016a) as
Z
Ay = X ayaz，1 ㊈ az，2 ㊈…㊈ az，N ,
z=1
(4)
where 0 denotes the tensor product. Note that (4) is in the form of a standard CP decomposition of
the grid tensor Ay. This implies that the rank of Ay is bounded by rank(Ay) ≤ Z. Moreover, the
obtained results where generalized in Cohen et al. (2016a) for the case ofa deep ConvAC with size-2
pooling windows2, thus L = log2N hidden layers as shown in Figure 1(b), leading to a grid-tensor
given by the hierarchical tensor decomposition
φ1,j,γ
r0
X a1α,j,γ a0,2j-1,α 0 a0,2j,α
α=1
rl-1
φl,j,γ = X alα,j,γφl-1,2j-1,α 0 φl-1,2j,α
α=1
rL-1
Ay = φL,1,1 = X aαL,1,yφL-1,1,α 0 φL-1,2,α ,	(5)
α=1
where r0, . . . , rL-1 ∈ Nare the number of channels in the hidden layers, {a0,j,γ ∈ RM}j∈[N],γ∈[r0]
are the weights in the first hidden convolutions, {al,j,γ ∈ RM}j∈[N∕2i],γ∈[r1] are the weights of the
hidden layers, and aL,1,y ∈ RrL-1 stores the weights corresponding to the output y in the output
layer.
4	Densely Connected Arithmetic Circuits
The recent empirical success of densely connected networks (DenseNets), presented by Huang et al.
(2016), has served as motivation for our theoretical analysis on dense connectivity. Dense connec-
tivity in a convolutional neural network refers to the case when a number k ∈ N (known as growth
rate) of previous layers serve as input of the forthcoming layer. More precisely, in Huang et al.
(2016), a DenseNet performs this via concatenation along the feature dimension of the current layer
inputs with the preceding layer features. Note that these feature must have compatible sizes along
the spatial dimension for the concatenation to be possible. To address this issue, Huang et al. (2016)
proposed to group blocks of the same spatial dimensions into a dense block, as shown in Figure 2.
These dense blocks do not contain operations such as pooling, that alter the spatial dimensions of the
input features. Moreover, in the DenseNet architecture the layers that perform the pooling operation
are called transition layers, since they serve as transition between dense blocks. For example, in
Figure 2 we depict a dense block of 4 layers with growth rate k = 2, followed by a transition layer.
1We must mention that the generalization to w × w convolutions is straightforward and was already covered
by Cohen & Shashua (2016a).
2Note that the generalization to different pooling sizes is straight forward and was done by Cohen & Shashua
(2016a).
4
Under review as a conference paper at ICLR 2018
Conv + act-fun
----H Concat
----A pool
Dense BloCk	Transition Layer
Figure 2: Example of a dense bloCk of size 4 with growth rate k = 2.
In the original DenseNet these transition layers inCluded one Convolution layer before the pooling
operation. Nevertheless, for this work we Consider transition layers Composed of only pooling oper-
ations. Note that this does not affeCt the generality of the model, sinCe avoiding dense ConneCtions
on the Convolutional layer preCeding the transition layer is equivalent to inCluding a Convolution in
that transition layer3.
In the Case of ConvACs, any dense bloCk of size greater than 1 Can be represented as a dense bloCk of
size 1, sinCe the aCtivation funCtion is the linear funCtion (the non-linearity Comes from the produCt
pooling operator in the transition layer). Therefore, for ConvACs, it is only reasonable to analyze
dense bloCks of size 1. Note that, ifwe only allow dense ConneCtions between hidden layers within
a dense bloCk, a ConvAC is limited to a maximum growth rate of k = 1. In order to analyze the
effeCt of broader ConneCtivity we extend the ConCept of growth rate by allowing dense ConneCtions
between dense bloCks. With proper pooling, outputs of hidden layers belonging to different dense
bloCks Can also be ConCatenated along the feature dimension. In the reminder of this paper we refer
to the dense ConneCtions between hidden layers of the same bloCk as intra-block connections, while
the ConneCtions between hidden layers of different bloCks as inter-block connections.
4.1	Dense Intra-block Connections
In this seCtion we analyze the effeCt of intra-bloCk ConneCtions. We first start by ConstruCting a
densely ConneCted version ofa single hidden layer ConvAC. The resulting network with growth rate
k = 1 is shown in Figure 3(a). In the same manner as in (3), this arChiteCture leads to the sCore
funCtion
Z	N M	Z+M	N
hy(X) =XazyYXazd,ifθd(xi)+ X azy Y fθz-Z (xi).	(6)
z=1	i=1 d=1	z=Z+1	i=1
Then, we present the following proposition regarding shallow ConvACs with dense ConneCtions of
growth rate k = 1.
Proposition 1 The network’s function of a densely connected shallow ConvAC shown in (6) corre-
sponds to the grid tensor
Z
Ay = X ayaz,1 0 az,2 0∙∙∙0 az,N + Sdiag {a?+z }M] ,	(7)
z=1	N	z=
where SdiagN {ay+z }M=1 ∈ (RM产N denotes the super-diagonal tensor of order N with
ayZ+1 , . . . , ayZ+M in its diagonal.
Proof See appendix B.1.
Note that the rank of this tensor is now bounded by rank(Ay) ≤ Z + M instead of Z, but adding
these dense ConneCtions inCreases the number of parameters of the network from MNZ + Z to
3This would effeCtively reduCe the dense bloCk size by 1.
5
Under review as a conference paper at ICLR 2018
, ConV
* Concat
+ pool
(c)
Figure 3: (a) Example of a shallow (L = 1) ConVolutional arithmetiC CirCuit with one intra-bloCk
ConneCtion. (b) Example of a 3 layered (L = 3) ConVolutional arithmetiC CirCuit with multiple intra-
bloCk ConneCtions. (c) Example of a 3 layered (L = 3) ConVolutional arithmetiC CirCuit with one
inter-bloCk ConneCtion.
ConV(i, z) = haz,i, δii
(a)
fθd(xi)
hy(X) = hay, pool(:)i
pool(z) =	iN=1 ConV(i, z)
MNZ+Z+M. Then, for large Values ofN, there is no Clear adVantage on using dense ConneCtions
on a shallow ConVAC. NeVertheless, in SeCtion 5 we show that dense ConneCtions are Capable of
inCreasing the expressiVe power of deep ConVACs, speCially for large Values of N .
We now generalize the obtained results for the Case of a L-layered dense arithmetiC CirCuit, with
growth rare k = 1, as the one in Figure 3(b). Similarly to (5), the obtained grid tensor has the
hierarChiCal deComposition giVen by
r0	M
φ1,j,γ = X aα,j,γa0,2，-1。0 a0,2j,α + Sdiag ∣a：+? |
2	0 α=1
α=1
r1	r
φ2,j,γ = X a2α,j,γφ1,2j-1,α 0 φ1,2j,α + Sdiag a2α,+j,rγ1
α=1	22	α=1
(8)
rL-1	r
Ay = φL,1,1 = X aαL,1,yφL-1,1,α 0 φL-1,2,α + Sdiag aαL+,j,rγL 1	L-2 .	(9)
α=1	2L	- α=1
From this result we obserVe that inter bloCk ConneCtions aCCount for Virtually inCreasing the width
of the network's hidden layers from r to ri，r + rι-ι for all l = 0,1,...,L — 1, where r-ι，
6
Under review as a conference paper at ICLR 2018
M . Note that this increased width comes at the expense of increasing the network’s parameters.
Moreover, in Section 5 we discuss whether increasing the network’s width via intra block dense
connections leads to an enhancement in its overall expressive power.
4.2	Dense Inter-block Connections
In this section we study broader connectivity via dense inter-block connections. As discussed in Sec-
tion 4, proper pooling of the preceding features must take place before the concatenating them into
the current layer. Since this type of connections have not been considered in the former DenseNets,
we propose 3 possible ways of realizing such connections (via product, average, or max pooling).
For a ConvAC with pooling window size wpool, an inter block connection that connects block l ∈ [L]
with block p ∈ [L] is said to be of jump length Ljump ∈ [L - 1] ifp = l + Ljump. An example of an
inter block connection of jump length Ljump = 1 can be seen in Figure 3(c). To perform this inter
block connections, the sizes along the spatial dimensions of preceding features must be reduced by
Ljumpwpool , before concatenating them along the feature dimension of layer l. This spatial size re-
duction may be realized via pooling of the preceding features with window size Ljump wpool. When
using a pooling layer the size along the feature dimension remains unchanged. Moreover, the type of
pooling employed (product, average, or maximum) affects the expressive potential of the resulting
ConvAC. Furthermore, the following proposition addresses the effect that adding dense inter block
connections, via average pooling, has on the network function of a ConvAC.
Proposition 2 Adding inter block connections via average pooling of jump length Ljump ≥ 1 to a
standard ConvAC with grid-tensor Ay ∈ (RM)0N leads to a networkfunction ofthe form
MN
hy(X) =	X	Ayd1,...,dNYfθdi(xi)+g(X),
d1 ,...,dN =1	i=1
where g(X) contains polynomial terms on fθd (xi) for d ∈ [M], i ∈ [N] of degree lower than N.
Remark 1 This result is also valid when the connections are done by addition instead of concate-
nation, as it is done in ResNet and FractalNet.
Proof See appendix B.2.
From this proposition we conclude that adding inter block connections average pooling does not alter
the grid tensor Ay , instead these connections account for extra polynomial terms of degree strictly
less than N . Note that, for the special case where the input features belong to an exponential kernel
family, such as F = {fθ (x) = eθTx : Rs → R : θ ∈ Θ} or F = {fθ (x) = ekθ-xkp : Rs → R :
θ ∈ Θ} where ∣∣ ∙ kp denotes the 'p norm with P ∈ N, the number of polynomial terms is equivalent
to the number of exponential basis that the network function can realize. Therefore, the another
valid measure of expressiveness is the number of polynomial terms a ConvAC is able to realize.
Given a certain ConvAC topology, the number of polynomial terms can be computed inductively
by expanding the polynomial products of every layer via generalized binomial expansions. Such
an analysis is left for future contributions. Moreover, if we perform this connections via product
poling, the features tobe concatenated correspond to polynomial terms of the same order. This leads
to a generalization of the intra-block connections from 4.1, leading to virtually increased widths
rι ，ri + PL==mp r1-1-q. Finally, We leave the analysis of inter-block connections via maximum
pooling for future work and consider only product pooling inter-block connections in the remainder
of this paper.
5 Practical Implications
For the sake of comparison, let us assume networks with hidden layer widths rι decaying (or in-
creasing) at an exponential rate of λ ∈ R. Formally, this is rι = λrι-1 ∈ N, thus rι = (λ)ιr for all
l = 0, 1, . . . , L - 1, where r , r0. To shorten the notation, we denote as (L, r, λ, k) to a ConvAC
with of exponential width decay λ ∈ R, length L ∈ N, initial with r ∈ N and growth-rate k ∈ N. A
growth-rate of k = 0 refers to a standard ConvAC with no dense connections.
7
Under review as a conference paper at ICLR 2018
Definition 1 Suppose that the weights of a (L, r, λ, k) ConvAC, with L, k ∈ N and r, λ ∈ R, are
randomly drawn according to some continuous non-vanishing distribution. Then, this (L, r, λ, k)
ConvAC is said to have weak dense gain Gw ∈ R if, with probability p > 0, we obtain score
functions that cannot be realized by a (L, r0, λ, 0) ConvAC with r0 < Gwr. When p = 1, this
(L, r, λ, k) ConvAC is said to have a strong dense gain Gs = Gw ∈ R.
Using this definition we present a bound for the weak dense gain Gw in the following theorem.
Theorem 5.1 Given M ∈ N, any a (L, r, λ, k) ConvAC with L > 1, r ≤ M, λ ≤ 1, k > 0 has a
dense gain is bounden by GW ≤ M.
Proof See appendix B.3.
This general bound may serve as guideline for tayloring M and the widths r0 , . . . , rL-1 such that
we exploit the expressiveness added by dense connections.
Theorem 5.2	For the particular case of theorem 5.1 When k = 1, the Weak dense gain is bounded
by GW ≤ min(1 + 1, M).
Proof See appendix B.3.
Using this result, we able able to quantify the expressive gain provided by dense inter block connec-
tions. If a ConvAC has a dense gain Gw = (1 + 1) that is already close to the general bound from
Theorem 5.1 it is less encouraging to include broader dense connections, since it would increase the
number of parameters of the model while there is no room for a significant expressive gain increase.
In this scenario, connections as the ones in ResNet and FractalNet may result more beneficial since
they do not increase the size of the model, while at the same time enhancing its trainability.
Theorem 5.3	For the particular case of theorem 5.1 when k =Lf r ≤ 1+1λ VM, then the bound
of theorem 5.2 is achieved with equality and strong dense gain Gs = 1 + 1.
Proof See appendix B.3.
This last theorem shows that there exist a regime where this bounds can be achieved with strong
dense gain. Whether this is true outside this regime is still an open question, since further knowl-
edge about the rank of random tensors is limited. Moreover, these theorems does not consider the
additional amount of parameters added by dense connections. We complete our analysis by address-
ing this issue in the following proposition.
Proposition 3 Let ∆Pdense ∈ N be the additional number of parameters that are added to a
(L, r, λ, 0) ConvAC When We introduce dense connections of groWth-rate k > 0. In the same man-
ner, let ∆Pstand ∈ N be the number of parameters that are added to a (L, r, λ, 0) ConvAC When We
increase its initial Width r by a factor G ∈ R. Then the ratio betWeen ∆Pdense and ∆Pstand is greater
than
△PStand ≥	(G - I)M	+ (G2 - I)
ZPese - rPq= λ-1-qpl=i(λ22)l	Pq= λ-q.
Proof See appendix B.4.
The factor G from this proposition directly relates to the dense gain of a ConvAC, thus this ratio
may be used to decide whether is interesting to add dense connections to a model (we want this ratio
to be as large as possible). Finally Theorems 5.1 and 5.2 directly bound this ratio, which give the
practitioner a guideline to decide which connections (if any) should be added to a given model.
8
Under review as a conference paper at ICLR 2018
References
Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report, pp. 05-02, 2005.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor de-
compositions. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The
33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine
Learning Research, pp. 955-963, New York, New York, USA, 20-22 Jun 2016a. PMLR. URL
http://proceedings.mlr.press/v48/cohenb16.html.
Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling
geometry. CoRR, abs/1605.06743, 2016b. URL http://arxiv.org/abs/1605.06743.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir (eds.), 29th Annual Conference
on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pp. 698-728,
Columbia University, New York, New York, USA, 23-26 Jun 2016a. PMLR. URL http://
proceedings.mlr.press/v49/cohen16.html.
Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2016b.
Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with
mixed tensor decompositions. CoRR, abs/1703.06846, 2017. URL http://arxiv.org/
abs/1703.06846.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural net-
works without residuals. arXiv preprint arXiv:1605.07648, 2016.
Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entan-
glement: Fundamental connections with implications to network design. CoRR, abs/1704.01552,
2017. URL http://arxiv.org/abs/1704.01552.
Or Sharir and Amnon Shashua. On the expressive power of overlapping operations of deep networks.
arXiv preprint arXiv:1703.02065, 2017.
Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tractable generative convolutional
arithmetic circuits. CoRR, abs/1610.04167, 2016. URL http://arxiv.org/abs/1610.
04167.
9
Under review as a conference paper at ICLR 2018
A Preliminary Lemmas
Lemma 1 Given Z ∈ N, let A ∈ (RM 产 P be a random tensor of even order P ≥ 2 such that
Z
A = X aZI)㊈…㊈ aZP),
z=1
where a(zk) ∈ RM are randomly drawn from a non-vanishing continuous distribution for all k ∈ [P]
and z ∈ [Z]. Then, ifZ ≤ MP/2 we have that rank(A) = rank([A]) = Z with probability 1. This
lemma also holds when for a subset Z ⊆ [Z] we have that a(zk) = azez ∈ RM for all z ∈ Z, where
az ∈ R are randomly drawn from a non-vanishing continuous distribution.
Proof Using the definition of the matricization operator, we get that the matricization A is
Z
[A] = X(吧
Θ aZ3) Θ∙∙∙Θ aZP-1))(aZ2) Θ aZ4) Θ∙∙∙Θ a(P))T ∈ RM(P/2) ×M (P/2) .
----------{z	} '	V-}
(odd)--------------------------------------------------(even)
az	az
(10)
Note that, from this expression, it is straight forward to see that the rank of [A] is always less or
equal than Z.
Let Z ⊆	[MP/2]	be the subset Z=	{Z	: Z=	M(M-IT	(Z	— 1) + 1 : Z ∈	[Z]}	and U ∈
RM(P/2)×M(P/2) be a permuted version of [A] such that the first Z rows of U correspond to the
rows z ∈ Z of [A], and the first Z columns of U correspond to the columns Z ∈ Z of [A]. Since
permuting the rows and the columns of a matrix does not alter its rank, we have that U has the same
rank as [A]. Now, let us partition U into blocks as
U=P Q
U= W Z ,
where P is of size Z-by-Z, and Q, W, Z have matching dimensions. Note that, if rank(P) = Z
then rank(U) ≥ Z, which leads to Z ≤ rank(U) = rank([A]) ≤ Z, thus rank([A]) = Z.
Therefore, it is sufficient to show that rank(P) = Z with probability 1 to conclude this proof. To
that end, let us define the mapping from x ∈ RMPZ to P = P(x) as
x, ha(11)T,...,a(Z1)T,a(12)T,...,a(ZP)TiT .
Note that this definition of x implies that a(zi) = a(zi) (x) for all Z ∈ [Z] and i ∈ [P]. Therefore,
since [A] is computed as in (10), we have that [A] = [A](x), thus Q = Q(x) and P = P(x).
Now, det P(x) is a polynomial on x, then it either vanishes in a set of measure zero or its the
zero-polynomial (see Caron & Traynor (2005)).
If we set x to be equal to some x0 ∈ RMPZ such that a(zi) = ez for all Z ∈ [Z] and i ∈ [P], we
have that aZodd) = aZeven) = ez ∈ RMP/2 with Z，(PP=OT Mn)(z — 1) + 1 = MM-IT (Z —
1) + 1. Therefore, since [A] is calculated as in (10) and aZodd)aZeven) is now a matrix with 1 on
the entry (Z, Z) and zero elsewhere, we have that [A](xo) is a diagonal matrix with ones on the
diagonal elements Z ∈ Z and zero elsewhere. This leads to P(xo) = IZ which has a determinant
det P(xo) 6= 0. Finally, since there exist xo such that the polynomial det P(xo) is not zero, we
conclude that det P(x) is not the zero-polynomial, which means that det P(x) 6= 0 with probability
1, thus proving this lemma.
Lemma 2 Let A ∈ (RM FP and B ∈ (RM FP be random tensors ofeven order P ≥ 2 and Z ∈ N
be tensors such that
Z1	Z2
A = X a'1) X …X aZP), B = X bZ1) X …X b'P),
z=1	z=1
10
Under review as a conference paper at ICLR 2018
where a(zi) ∈ RM and b(zi) ∈ RM are randomly drawn from a non-vanishing continuous distribu-
tion. Then, if Zi ≤ MP/2 and Z2 ≤ MP/2, we have that rank (A 0 B) = Z1Z2 with probability
1.
Proof Let C ∈ (RM 产 P be a random tensor defined as C = A0B. Therefore, We may express C
as
ZZ
C = XXaqi) 0 …0 aqP) 0 bZ1) 0 …0 bZP) .
Then, we define rank-1 tensors C(q,z) to be C(q,z) = a，1) 0 …0 a)P) 0 bZ1) 0 …0 bZP) to get
Z
C
C(q,z) .
q,z=1
Since C is now expressed as a sum of Z1Z2 rank-1 tensors, we have that rank(C) ≤ Z1Z2.
Since Z1 ≤ MP/2 and Z2 ≤ MP/2 we may use Lemma 1, this leads to rank([A]) = Z1 and
rank([B]) = Z2 with probability 1. Finally we, use the properties of the Kronecker product to obtain
the rank of the matricization C as rank([C]) = rank([A 0 B]) = rank([A])rank([B]), leading to
rank([C]) = Z1Z2 ⇒ Z1Z2 = rank([C]) ≤ rank(C) ≤ Z1Z2 ⇒ rank(C) = Z1Z2
with probability 1, thus proving the Lemma.
Lemma 3 Let A ∈ (RM 产P and B ∈ (RM 产P be tensors of order P > 2 and Z ∈ N be tensors
such that
Z1	Z2
A = X aZ1) 0∙∙∙0 aZP), B = X bZ1) 0∙∙∙0 bZP),
z=1	z=1
where a(Zi) ∈ RM and b(Zi) ∈ RM are randomly drawn from a non-vanishing continuous distribu-
tion. Then, ifZ1 + Z2 ≤ MP/2, we have that rank (A + B) = Z1 + Z2 with probability 1.
Proof Let C ∈ RM × × M bea tensor of order P defined as C = A + B. Therefore, we may express
C as
Z1	Z2	Z1 +Z2
C = X aZ1) 0∙∙∙0 aZP) + X bZ1) 0∙∙∙0 bZP) = X cZ1) 0∙∙∙0 CZP),
Z=1	Z=1	Z=1
where
c(i) , a(Zi)	0 < z ≤ Z1
Z	b(Zi) Z1 < z ≤ Z1 + Z2 .
Since Z1 + Z2 ≤ MP/2 we may use Lemma 1, leading to rank (C) = Z1 + Z2 with probability 1,
thus proving this Lemma.
Corollary 1 Let A, B, C be tensors of the same size with ranks ZA , rank (A), ZB , rank (B),
and ZC , rank (C). Then, the following statements hold true.
rank(A+B+C) = ZA +ZB +ZC ⇒ rank(A+B) = ZA +ZB
rank ((A + B) 0C) = (ZA +ZB)ZC ⇒ rank(A0C) = ZAZC .
11
Under review as a conference paper at ICLR 2018
B Deferred Proofs
B.1 Proof Proposition 1
Proof We reformulate this (6) to have the same form as (3). To that end we define azd,i, for z =
Z + 1, . . . , Z + M, to be azd,i = 1 if z - Z = d and zero otherwise. This definition of azd,i leads to
PdM=1 azd,ifθd(xi) = fθz-Z(xi) for z = Z + 1, . . . , Z + M. Using this relation we get
Z	N M	Z+M	N M
hy(X) =XazyYXazd,ifθd(xi)+ X azyYXazd,ifθd(xi)
z=1	i=1 d=1	z=Z+1	i=1 d=1
Z+M	N M
= X azyYXazd,ifθd(xi) ,
z=1	i=1 d=1
which has the same form as (3). Therefore, as done in (4), we obtain the grid tensor for this archi-
tecture as
Z+M
Ay = X ayaz，1 ㊈ az，2 ㊈…㊈ az，N
z=1
Z	Z+M
=X ay az，1 乳 az，2 乳…乳 az，N + X ay az，1 乳 az，2 乳…乳 az，N
z=1	z=Z+1
Z	Z+M
=E ayaz,1 0 az，2 0∙∙∙0 az，N + E ayez-z 0 ez-z 0∙∙∙0 ez-z
z=1	z=Z+1
z
=X ayaz,1 0 az，2 0 …0 az,N + Sdiag {ay+ζ }M1,
z=1	N
thus proving this proposition.
B.2 Proof Proposition 2
Proof Given X ， f (xι),…，fθM (x1),fθ2(x1),…，fθM (XN)]T ∈ RMN, the output of
the l-th layer of a (L,r,λ, 0) ConvAC can be stored into the vectors of mappings δl,j(x) ，
[δ1,j(x),...,δrj(x)]T ∈ Rrl for j ∈ [N/2l] and l ∈ [L]. Moreover, since the entries of these
vectors are the result of l - 1 convolution-pooling layers with product pooling of window size 2, all
the mappings δ1,j (x) can be expressed as a sum of polynomial terms on X of degree 2l.
Now, let the coefficient vectors al,j,γ，[a1,j,γ,..., arj，Y]T ∈ Rrl for j ∈ [N/2l] and Y ∈ [r1+1], be
the weight vectors for the convolution of the l-th layer. To shorten the notation We use hal，j，Y, δlji =
Pd=I aj δldj as shorthand for the convolution between these vectors. Then, the outputs the the
layer l of this ConvAC are given by δl+1j ∈ Rrl+1 with 6午1，j = hal，2j-1，Y, 81，勿'-1〉〈*，2么), §1，勿)
for j ∈ [N/2l+1], γ ∈ [rl+1]. If we recursively calculate these out vectors up to the L-th layer we
obtain the score functions hsiand(x)，bL，1(x) = δL'1(x) ∈ R.
We now consider the effecct of adding dense connections via average pooling from some k ∈ N
preceding layers l 一 1,...,l 一 k. To this end, let % = P：=i rι-q be the total size along the feature
dimesnion of the vectors to be concatenated. In addition, let ωlj(x)，[ω1j (x),..., ωrj (x)]t ∈
Rrl be the vectors of mappings of the corresponding preceeding features at the layer l for j ∈ [N/rl ].
In order to compute the convolutions of this layer, an additional vector of coefficients is required as
bl，j，Y , [b1，j，Y,..., b"γ]τ ∈ Rrl. Then, the outputs of the l-th layer of this (L, r, λ, k) ConvAC
are the denoted as the vectors δlj(x)，[δ1j (x),..., %+](x)]τ ∈ Rrl+1 for j ∈ [N∕2l+1] where
δl+1j
h
al，2j-i，Y] 「bl，2jT] 「al，2j"
bl，2j-1，Y , 3l，2j-1 ih bl，2j，Y
「bl，2j\
3l，2j i .
12
Under review as a conference paper at ICLR 2018
From this expression it follows
δl+1,j = (hal,2jτ,γ, δl,2jτi + hbl,2jT,γ, ωl,2ji)(hal,2j,γ, δl,2j) + Hj, ωl,2ji)
= δl+1,j + ωl+1,j,
where
ωl+1,j = hbl,2j-1,γ, ωl,2j-1ihal,2j,γ, δl,2ji + hal,2j-1,γ, δl,2j-1ihbl,2j,γ, ωl,2ji
+ hbl,2j-1,γ, ωl,2j-1ihbl,2j,γ, ωl,2j i.
Note that the entries of ωl,j(x) are assumed to come from preceding layers with an appropiate aver-
age pooling. Since performing avergae pooling does not increase the degree of the polynomial terms
involved (only product pooling does) and the jump length Ljump is at least 1, the entries of ωl,j (x)
have at most polynomial degree 2l-1, which is strictly less than the degree of the entries of δl,j (x)
(i.e., 2l). Therefore, from the obtained expression of ωl+1,j we observe that it has polynomials
withb degree no greater than 2l + 2l-1, while the entries of δl+1,j have a strictly higher degree of
2l +2l = 2l+1.
Moreover, since hal,j,γ, δl,j + ωl,j i can be expressed as
hal,j,γ,δl,j+ωl,ji=haall,,jj,,γγ , ωδll,,jji	(11)
we can make use of the obtained results in an unductive manner up to the L-th layer, thus leading to
hdyense(x) = hsytand(x) +g(x),
where g(x) contains polynomial terms of x of order strictly less than N, thus proving this theorem.
Note that this result also applies to additive and resudial connections, as de ones used in ResNet and
FractalNet, since they can be expressed as in (11).
B.3 Proof of Theorems 5.1 to 5.3
Proof Given M ∈ N, a (L, r, λ, 0) ConvAC with L > 1, r ≤ M, λ ≤ 1 has a grid tensor Asytand ∈
(RM 产N. For the forthcoming analysis let Us assume r0 ≤ M. This assumption is done, so that We
can write min{r0, M} = r0, merely for notation purposes since we show that this does not affect
the generality of the results. Using this assumption, We upper bound the rank of the grid tensor as
rank (φ1'j,γ)=rank (X aα,j,γ a0,2ji X a0,2j,α! ≤ ms"} = r0
r1	r1
rank (φ2,j,γ) ≤ ^X rank (φ1,2j-1,α 0 φ1,2j,α) ≤ ^X rank (φ1,2j-1,α) rank (φ1,2j,α) = rιr0
α=1	α=1
rl-1	rl-1
rank (φl,j,γ) ≤ X rank (φl-1,2jT,α 0 φl-1,2j,α) ≤ X rank (φlτ,2jτ,α) rank (φlτ,2j,α).
α=1	α=1
(12)
It Was shoWn in Cohen & Shashua (2016a) that, When the Weights are independently generated from
some continuous distribution, We have that rank φ1,j,γ = min{r0, M} With probability 1. Note
that, the bounds obtained for r0 values greater than M is the same as for r0 = M, thus implying
that the assumption of r0 ≤ M does not affect the generality of the results. Finally, by induction up
to the L-th layer, We obtain a bound for the grid tensor rank as
rL-1	L-1
rank(Aytand) = rank (ΦL,1,1) ≤ ^X rank (φL-1,1,α) rank (φL-1,2,α) = Y r2	ɪ . (13)
α=1	l=0
Since We assumed netWorks With hidden layer Widths rl decaying (or increasing) at an exponential
rate of λ ∈ R. Formally, this is rl = λrl-1 ∈ N, thus rl = (λ)lr for all l = 0, 1, . . . , L - 1, Where
13
Under review as a conference paper at ICLR 2018
r , r0. Therefore, we may simplify the obtained bound to
L-1
rank (Asytand) ≤ Y((λ)lr)2L-l-1=(λ)PlL=-01l2L-l-1rPlL=-012L-l-1=(λ)2L-1-Lr2L-1.
l=0
We this analysis by proving Theorem 5.1. To that end let Aydense be the grid tensor of a dense
(L, r, λ, k) ConvAC with k > 0, while Asytand is the grid tensor of a (L, r0 , λ, 0) ConvAC with
r0 ∈ R. As discussed in Section 4, this dense version of the former (L, r, λ, 0) ConvAC is equiva-
lent to virtually increasing the widths of the ConvAC, which translates extra additive terms in the ex-
pressions from 12. Moreover, using corollary 1 we observe that, if the ranks of the tensors φl,j,γ are
additive and multiplicative up to rank(Aydense) > rank(Asytand), so they are up to rank(Asytand). A
weak dense gain value Gw ∈ Ris achieved when there is a set of functions realized by the (L, r, λ, k)
ConvAC that cannot be realized by (L, r0, λ, 0) ConvAC unless r0 = Gwr. To bound this gain, let
us assume the best case scenario where rank(Aydense) reaches the maximum possible rank, from the
size of Aydense this can be at most rank(Aydense) = M2L-1. As discussed, would imply that the
ranks of the tensors φl,j,γ are additive up to M2L-1 for both ConvACs. Therefore, Asytand achieves
its maximum rank given by rank (Asytand) = (λ)2L-1-L(r0)2L-1. Then, a (L, r0, λ, 0) ConvAC is
able to realize the functions of a (L, r, λ, k) ConvAC when rank (Asytand) = rank (Aydense), thus
(λ)2L-1-L(r0)2L-1 = M2L-1. Finally, since r0 = Gwr, this leads to a maximum value of
Gw = Mλ
λr
2l-1 ≤ ---
_ λr ,
L
M
which proves Theorem 5.1.
For Theorem 5.2 we use consider the particular case of k = 1, which yields a core tensor given by
the hierarchical tensor decomposition from (9). We use the same assumption of r0 ≤ M and define
the virtually increased widths ri ,ri + rι-ι ∈ N for l = 1,...,L - 1 and ro，M. This leads to
r0+M
rank (φ1,j,γ) = rank I ^X a1αj,γa0,2j-1,α 0 a0,2j,α I ≤ min{r0 + M, M} = ro
r1+r0	%
rank (φ2,j,γ)	≤	^X	rank (φ1,2j-1,α 0	φ1,2j,α^	≤	^X rank	(φ1,2j-1,α)	rank (φ1,2j,α) =	r"2
α=1	α=1
ri-ι+ri-2	ri-i
rank (φl,j,γ) ≤ X rank ^φl-1,2j-1,a 0 φlτ,2j,α) ≤ X rank (φl-1,2j-1,α) rank (φlτ,2j,α)
α=1	α=1
and
L-1
rank(Ayens) = rank (ΦL,1,1) ≤ Y 常 ɪ .	(14)
i=0
Note that for r = λrι-ι ∈ N (λ ∈ R), We get virtually increased widths ” = (1 + λ)ir =
(λ(1 + λ))i r, for all l = 0,1,...,L - 1, leading to
L-1	2L-l-1
rank (Ayens) ≤ Y ((λ(1 + 1∕λ))i r)	= (λ(1 + 1∕λ))PL=01 匕L-TrPk0 2L-l-1
i=0
= (λ(1 + 1∕λ))2L-1-L r2L-1.	(15)
As in for the proof of Theorem 5.1, the maximum dense gain Gw is obtained when
rank(Aydense) reaches the maximum possible rank. In this particular case, this corresponds
to rank(Aydense) = min (λ(1 + 1∕λ))2L-1-L r2L-1, M2L-1 . Furthermore, for obtaining
rank(Asytand) = rank (Aydense) a gain of
Gw ≤ min(1 + ； ,M)
λ λr
14
Under review as a conference paper at ICLR 2018
is required, thus proving Theorem 5.2.
Finally, for proving Theorem 5.3 we show that this bound can be attained with prob-
ability 1. Note that this bound is achieved when the inequalities from (12) hold
with equality for all l ∈ [L]. The first inequality of (12) holds when the tensors
(φlτ,2jτ,1 0 φlτ,2j,1),…，(φlτ,2jτ,rlτ 0 φlτ,2j,rl-1) are additive on the rank. This can
be proven to be true with probability 1 when
rl-1
X rank (φl-1,2j-1,α 0 φl-1,2j,α) ≤ M2l-1	(16)
α=1
by applying Lemma 3. In the same manner, the second inequality of (12) holds when the tensor
pairs φl-1,2j-1,α, φl-1,2j,α are multiplicative in the tensor rank. We may use Lemma 2 to prove
this is the case with probability 1 if we can bound
rank (φl-1,2j-1,α) ≤ M2l-2 .	(17)
In summary, if equations (16) and (17) hold, we may use Lemmas 3 and 2 to prove that (12) reaches
equality with probability 1, thus implying that (13) also reaches equality everywhere outside a set
Lebesgue measure zero. It is straight forward to see that, if (16) holds, so does (17).
For the case of a network with exponential width decay λ and r ≤ λ √M We have that
X rank (φlτ,2jτ,α 0 φl-1,2j,α) ≤ (λ)2l-1-lr2l-1 ≤ (λ)2lr2l ≤ (λ)2l (1 √M)	= M2l-1 ,
α=1
(18)
thus (16) holds. Therefore, within this regime of r ≤ λ √M, we may ensure that the tensor rank is
additive and multiplicative with probability 1.
Now We apply the same reasoning for a densely connected arithmetic circuit of L layers and width
decay λ such that r ≤ j+^ √M = λ(J∕λ) √M. In the same manner as for the standard ConvAC
we bound
X rank (φl-1,2j-1,α 0 φl-1,2j,α) ≤ (λ(1 + 1∕λ))2l-1-l r2l-1 ≤ (λ(1 + 1∕λ))2l r2l
α=1
≤ -M ( Λ(1⅛Λ) √MI M A
which enables US to make use of Lemmas 3 and 2 to prove that (15) holds with equality everywhere
except from a set of Lebesgue measure zero. Note that, for r ≤ * √M < 1 √M, we have that
both equations (13) and (15) reach equality with probability 1, thus proving Theorem 5.3.
B.4 Proof Proposition 3
Proof Let P(L, r, λ, k) ∈ N be the number of parameters of a (L, r, λ, k) ConvAC. A standard
(L, r, λ, 0) ConvAC is composed of the weights {a0,j,γ ∈ RM}j∈[N],γ∈[r0] in the first hidden con-
volutions, {al,j,γ ∈ RM}j∈[N∕2i],γ∈[r1] in the hidden layers, and aL,1,y ∈ RrLT in the weights
corresponding to the output y in the output layer. Therefore, this ConvAC has a number of weights
LN
P(L,r, λ, 0) = MNro + E 好rιri-i + YrL
l=1
When adding dense connections of growth-rate k, we need additional weights for the convolution
between the preceding layers and the current layer. Therefore, at the l-th layer we have an extra
Pk=INrιrι-ι-q weights, which leads to
LN	L kN
P(L,r, λ,k) = MNro +	^yM1-1 + 工三灰rιri-i-q + Y”∙
15
Under review as a conference paper at ICLR 2018
By definition, we have that ∆Pstd = P (L, Gr, λ, 0) - P (L, Gr, λ, 0) and ∆Pdense
P (L, Gr, λ, k) - P (L, r, λ, 0), thus yielding
LN
∆Pstd = (G - U(MNTO + YrL) + (G2 - 1) £ 刃rri-i
l=1
LN
=(G - 1)(MNr + λLYr) + (G2 - 1) E "λ2l-1r2
l=1 2
L λ2
=(G - 1)(MN + λLY)r + (G2 - 1)Nλ-1r2 E(W)I
l=1 2
and
kL	k	L 2
∆Pdense = XX
N riri-i-q = Nr2 X λ-1-q X( ⅛ )l.
Finnaly, we use these expressions to compute the ratio of interest as
∆Rtd =(G - D(MN + λLY)r + (G2 - 1)Nλ-1r2 £匕式竽)l
APdense -	NrIP=^-ZqP=W
(G - 1)(MN + λLY)	,	(G2 - 1)	、	(G - 1)M	, (G2 - 1)
=NrPk=i λ-1-q PL=ι(λ22)l + P" λ-1-q ≥ rp3L^PEW7 + P" λ-q
which proves this proposition.
16