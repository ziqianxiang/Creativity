Under review as a conference paper at ICLR 2018
Convolutional Normalizing Flows
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian posterior inference is prevalent in various machine learning problems.
Variational inference provides one way to approximate the posterior distribution,
however its expressive power is limited and so is the accuracy of resulting approxi-
mation. Recently, there has a trend of using neural networks to approximate the
variational posterior distribution due to the flexibility of neural network architec-
ture. One way to construct flexible variational distribution is to warp a simple
density into a complex by normalizing flows, where the resulting density can be
analytically evaluated. However, there is a trade-off between the flexibility of
normalizing flow and computation cost for efficient transformation. In this paper,
we propose a simple yet effective architecture of normalizing flows, ConvFlow,
based on convolution over the dimensions of random input vector. Experiments on
synthetic and real world posterior inference problems demonstrate the effectiveness
and efficiency of the proposed method.
1	Introduction
Posterior inference is the key to Bayesian modeling, where we are interested to see how our belief
over the variables of interest change after observing a set of data points. Predictions can also benefit
from Bayesian modeling as every prediction will be equipped with confidence intervals representing
how sure the prediction is. Compared to the maximum a posterior estimator of the model parameters,
which is a point estimator, the posterior distribution provide richer information about the model
parameter hence enabling more justified prediction.
Among the various inference algorithms for posterior estimation, variational inference (VI) and
Monte Carlo markov chain (MCMC) are the most two wisely used ones. It is well known that MCMC
suffers from slow mixing time though asymptotically the samples from the chain will be distributed
from the true posterior. VI, on the other hand, facilitates faster inference, since it is optimizing an
explicit objective function and convergence can be measured and controlled, and it’s been widely
used in many Bayesian models, such as Latent Dirichlet Allocation (Blei et al., 2003), etc. However,
one drawback of VI is that it makes strong assumption about the shape of the posterior such as the
posterior can be decomposed into multiple independent factors. Though faster convergence can be
achieved by parameter learning, the approximating accuracy is largely limited.
The above drawbacks stimulates the interest for richer function families to approximate posteriors
while maintaining acceptable learning speed. Specifically, neural network is one among such models
which has large modeling capacity and endows efficient learning. (Rezende & Mohamed, 2015)
proposed normalization flow, where the neural network is set up to learn an invertible transformation
from one known distribution, which is easy to sample from, to the true posterior. Model learning
is achieved by minimizing the KL divergence between the empirical distribution of the generated
samples and the true posterior. After properly trained, the model will generate samples which are
close to the true posterior, so that Bayesian predictions are made possible. Other methods based
on modeling random variable transformation, but based on different formulations are also explored,
including NICE (Dinh et al., 2014), the Inverse Autoregressive Flow (Kingma et al., 2016), and Real
NVP (Dinh et al., 2016).
One key component for normalizing flow to work is to compute the determinant of the Jacobian of
the transformation, and in order to maintain fast Jacobian computation, either very simple function
is used as the transformation, such as the planar flow in (Rezende & Mohamed, 2015), or complex
tweaking of the transformation layer is required. Alternatively, in this paper we propose a simple
1
Under review as a conference paper at ICLR 2018
and yet effective architecture of normalizing flows, based on convolution on the random input vector.
Due to the nature of convolution, bi-jective mapping between the input and output vectors can be
easily established; meanwhile, efficient computation of the determinant of the convolution Jacobian
is achieved linearly. We further propose to incorporate dilated convolution (Yu & Koltun, 2015;
Oord et al., 2016a) to model long range interactions among the input dimensions. The resulting
convolutional normalizing flow, which we term as Convolutional Flow (ConvFlow), is simple and yet
effective in warping simple densities to match complex ones.
The remainder of this paper is organized as follows: We briefly review the principles for normalizing
flows in Section 2, and then present our proposed normalizing flow architecture based on convolution
in Section 3. Empirical evaluations and analysis on both synthetic and real world data sets are carried
out in Section 4, and we conclude this paper in Section 5.
2	Preliminaries
2.1	Transformation of random variables
Given a random variable z ∈ Rd with density p(z), consider a smooth and invertible function
f : Rd → Rd operated on z. Let z0 = f(z) be the resulting random variable, the density of z0 can
be evaluated as
p(z0) = p(z) det df-1 = p(z) det f	(1)
∂ z 0	∂ z
thus
0	∂f
logp(z ) = logp(z) - log det -	(2)
∂ z
2.2	Normalizing flows
Normalizing flows considers successively transforming z0 with a series of transformations
{f1,f2, ...,fK} to construct arbitrarily complex densities for zK = fK ◦ fK-1 ◦ ... ◦ f1(z0) as
logp(zK)
K
logp(z0) -	log
k=1
det f
∂zk-1
(3)
Hence the complexity lies in computing the determinant of the Jacobian matrix. Without further
assumption about f, the general complexity for that is O(d3) where d is the dimension of z. In order
to accelerate this, (Rezende & Mohamed, 2015) proposed the following family of transformations
that they termed as planar flow:
f (z) = z + uh(w>z + b)	(4)
where W ∈ Rd, U ∈ Rd, b ∈ R are parameters and h(∙) is a univariate non-linear function with
derivative h0(∙). For this family of transformations, the determinant of the Jacobian matrix can be
computed as
det f = det(I + uψ(z)>) = 1+ u>ψ(z)	(5)
where ψ(z) = h0(w>z + b)w. The computation cost of the determinant is hence reduced from
O(d3) to O(d).
Applying f to z can be viewed as feeding the input variable z to a neural network with only one
single hidden unit followed by a linear output layer which has the same dimension with the input
layer. Obviously, because of the bottleneck caused by the single hidden unit, the capacity of the
family of transformed density is hence limited.
3	A new transformation unit
In this section, we first propose a general extension to the above mentioned planar normalizing flow,
and then propose a restricted version of that, which actually turns out to be convolution over the
dimensions of the input random vector.
2
Under review as a conference paper at ICLR 2018
3.1	NORMALIZING FLOW WITH d HIDDEN UNITS
Instead of having a single hidden unit as suggested in planar flow, consider d hidden units in the
process. We denote the weights associated with the edges from the input layer to the output layer as
W ∈ Rd×d and the vector to adjust the magnitude of each dimension of the hidden layer activation
as u, and the transformation is defined as
f(z) = u h(Wz + b)	(6)
where denotes the point-wise multiplication. The Jacobian matrix of this transformation is
f = diag(u Θ h0(Wz + b))W	(7)
∂z
det f = det[diag(u Θ h0(Wz + b))] det(W)	(8)
∂z
As det(diag(u Θ h0(Wz + b))) is linear, the complexity of computing the above transformation lies
in computing det(W). Essentially the planar flow is restricting W to be a vector of length d instead
of matrices, however we can relax that assumption while still maintaining linear complexity of the
determinant computation based on a very simple fact that the determinant of a triangle matrix is also
just the product of the elements on the diagonal.
3.2	Convolutional Flow
Since normalizing flow with a fully connected layer may not be bijective and generally requires
O(d3) computations for the determinant of the Jacobian even it is, we propose to use 1-d convolution
to transform random vectors.
(a)	(b)
Figure 1: (a) Illustration of 1-D convolution, where the dimensions of the input/output variable are
both 8 (the input vector is padded with 0), the width of the convolution filter is 3 and dilation is 1; (b)
A block of ConvFlow layers stacked with different dilations.
Figure 1(a) illustrates how 1-d convolution is performed over an input vector and outputs another
vector. We propose to perform a 1-d convolution on an input random vector z, followed by a
non-linearity and necessary post operation after activation to generate an output vector. Specifically,
f(z) = z + u Θ h(conv(z, w))	(9)
where w ∈ Rk is the parameter of the 1-d convolution filter (k is the convolution kernel width),
conv(z, W) is the 1d convolution operation as shown in Figure 1(a), h(∙) is a monotonic non-
linear activation function1, Θ denotes point-wise multiplication, and u ∈ Rd is a vector adjusting
1Examples of valid h(x) include all conventional activations, including sigmoid, tanh, softplus, rectifier
(ReLU), leaky rectifier (Leaky ReLU) and exponential linear unit (ELU).
3
Under review as a conference paper at ICLR 2018
the magnitude of each dimension of the activation from h(∙). We term this normalizing flow as
Convolutional Flow (ConvFlow).
ConvFlow enjoys the following properties
•	Bi-jectivity can be easily achieved if proper padding and a monotonic activation function
are adopted;
•	Due to local connectivity, the Jacobian determinant of ConvFlow only takes O(d) computa-
tion independent from convolution kernel width k since
f = I + diag(wιu Θ h0(conv(z, W)))	(10)
∂z
where w1 denotes the first element of w.
For example for the illustration in Figure 1(a), the Jacobian matrix of the 1d convolution
conv(z, w ) is
w1 w2 w3
w1 w2 w3
wi W2
∂ conv(z, W) _	wi
∂z
w3
w2	w3
w1	w2
w1
w3
w2	w3
w1 w2
w1
(11)
which is a triangular matrix whose determinant can be easily computed;
•	ConvFlow is much simpler than previously proposed variants of normalizing flows. The
total number of parameters of one ConvFlow layer is only d + k where generally k < d,
particularly efficient for high dimensional cases. Notice that the number of parameters in the
planar flow in (Rezende & Mohamed, 2015) is 2d and one layer of Inverse Autoregressive
Flow (IAF) (Kingma et al., 2016) and Real NVP (Dinh et al., 2016) require even more
parameters. In Section 3.3, we discuss the key differences of ConvFlow from IAF in detail.
A series of K ConvFlows can be stacked to generate complex output densities. Further, since
convolutions are only visible to inputs from adjacent dimensions, we propose to incorporate dilated
convolution to the flow to accommodate interactions among dimensions with long distance apart.
Figure 1(b) presents a block of 3 ConvFlows stacked, with different dilations for each layer. Larger
receptive field is achieved without increasing the number of parameters. We term this as a ConvBlock.
From the block of ConvFlow layers presented in Figure 1(b), it is easy to verify that dimension
i (1 ≤ i ≤ d) of the output vector only depends on succeeding dimensions, but not preceding ones.
In other words, dimensions with larger indices tend to end up getting little warping compared to the
ones with smaller indices. Fortunately, this can be easily resolved by a Revert Layer, which simply
outputs a reversed version of its input vector. Specifically, a Revert Layer g operates as
g(z) := g([z1, z2, ..., zd]>) = [zd,zd-1, ..., z1]>	(12)
It’s easy to verify a Revert Layer is bijective and that the Jacobian of g is a d × d matrix with 1s on
its anti-diagonal and 0 otherwise, thus log Idet ∂g ∣ is 0. Therefore, we can append a Revert Layer
after each ConvBlock to accommodate warping for dimensions with larger indices without additional
computation cost for the Jacobian as follows
Z → COnVBlOCk → ReVert → COnVBlOCk → ReVert → …→ f (z)	(13)
|^^^^^^^^^^^^^^™}
Repetions of ConvBlock+Revert for K times
3.3	Connection to Inverse Autoregressive Flow
Inspired by the idea of constructing complex tractable densities from simpler ones with bijective
transformations, different variants of the original normalizing flow (IAF) (Rezende & Mohamed,
2015) have been proposed. Perhaps the one most related to ConvFlow is Inverse Autogressive
4
Under review as a conference paper at ICLR 2018
Flow (Kingma et al., 2016), which employs autoregressive transformations over the input dimensions
to construct output densities. Specifically, one layer of IAF works as follows
f (z) = μ(z) + σ(z) Θ Z	(14)
where
[μ(z), σ(z)] — AUtoregreesiveNN(Z)	(15)
are outputs from an autoregressive neural network over the dimensions of z . There are two drawbacks
of IAF compared to the proposed ConvFlow:
•	The aUtoregressive neUeral network over inpUt dimensions in IAF is represented by a Masked
AUtoencoder (Germain et al., 2015), which generally reqUires O(d2) parameters per layer,
where d is the inpUt dimension, while each layer of ConvFlow is mUch more parameter
efficient, only needing k + d parameters (k is the kernel size of 1d convolUtion and k < d).
•	More importantly, dUe to the coUpling of σ(Z) and Z in the IAF transformation, in order to
make the computation of the overall Jacobian determinant det 票 linear in d, the Jacobian of
the aUtoregressive NN transformation is assUmed to be strictly triangUlar (EqUivalently, the
Jacobian determinants of μ and σ w.r.t Z are both always 0. This is achieved by letting the
ith dimension of μ and σ depend only on dimensions 1, 2,…，i 一 1 of z). In other words,
the mappings from Z onto μ(z) and σ(z) via the autogressive NNare always singular,
no matter how their parameters are updated, and because of this, μ and σ will only be
able to cover a subspace of the input space z belongs to, which is obviously less desirable
for a normalizing flow.2 Though these sigularity transforms in the autoregressive NN are
somewhat mitigated by their final coupling with the input z, IAF still performs slightly worse
in empirical evaluations than ConvFlow as no singular transform is involved in ConvFlow.
4	Experiments
We test performance the proposed ConvFlow on two settings, one on synthetic data to infer unnor-
malized target density and the other on density estimation for hand written digits and characters.
4.1	Synthetic data
We conduct experiments on using the proposed ConvFlow to approximate an unnormalized target
density of Z with dimension 2 such thatP(Z) H exp(-U(z)) where U(z) = 1 [z2-W41(z)] and
wi(z) =Sin (π2τ1). The target density of Z are plotted as the left most column in Figure 2, and we
test to see if the proposed ConvFlow can transform a two dimensional standard Gaussian to the target
density by minimizing the KL divergence
min KL(qK(zk)||p(z)) = Ezk logqK(zk)) 一 Ezk logp(zk)
=Ezo log q0(Z0))- Ezo log det 得
(16)
+Ez0U(f(z0)) + const (17)
where all expectations are evaluated with samples taken from q0(z0). We use a 2-d standard Gaussian
as q0(z0) and we test different number of ConvBlocks stacked together in this task. Each ConvBlock
in this case consists a ConvFlow layer with kernel size 2, dilation 1 and followed by another ConvFlow
layer with kernel size 2, dilation 2. Revert Layer is appended after each ConvBlock, and leaky ReLU
with a negative slope of 0.01 is adopted in ConvFlow.
Experimental results are shown in Figure 2 for different layers of ConvBlock to be stacked to compose
f. It can be seen that even with 4 layers of ConvBlocks, it’s already approximating the target density
despite the underestimate about the density around the boundaries. With 8 layers of ConvFlow, the
transformation from a standard Gaussian noise vector to the desired target unnormalized density can
be accurately learned. Notice that with 8 layers, we are only using 40 parameters ((4 + 1) * 8 with
bias terms of convolution counted).
2Since the singular transformations will only lead to subspace coverage of the resulting variable μ and σ,
one could try to alleviate the subspace issue by modifying IAF to set both μ and σ as free parameters to be
learned, the resulting normalizng flow of which is exactly a version of planar flow as proposed in (Rezende &
Mohamed, 2015).
5
Under review as a conference paper at ICLR 2018
(a) K = 2
P(Z)	qo(z)	zκ ~ qκ(z)
(b) K = 4
(c) K = 8
Figure 2: Approximation performance with different number of ConvBlocks
4.2	Handwritten digits and characters
4.2	. 1 Setups
To test the proposed ConvFlow for variational inference we use standard benchmark datasets MNIST3
and OMNIGLOT4 (Lake et al., 2013). Our method is general and can be applied to any formulation
of the generative model pθ (x, z); For simplicity and fair comparison, in this paper, we focus on
densities defined by stochastic neural networks, i.e., a broad family of flexible probabilistic generative
models with its parameters defined by neural networks. Specifically, we consider the following two
family of generative models
Gi : Pθ(x, Z) = Pθ(Z)pθ(x|z)	(18)
G2 : Pθ(x,Zi,Z2) = Pθ(zι)pθ(Z2∣Zι)Pθ(x∣Z2)	(19)
where p(Z) and p(Z1) are the priors defined over Z and Z1 for G1 and G2, respectively. All other
conditional densities are specified with their parameters θ defined by neural networks, therefore
ending up with two stochastic neural networks. This network could have any number of layers,
however in this paper, we focus on the ones which only have one and two stochastic layers, i.e., G1
and G2, to conduct a fair comparison with previous methods on similar network architectures, such
as VAE, IWAE and Normalizing Flows.
We use the same network architectures for both G1 and G2 as in (Burda et al., 2015), specifically
shown as follows
G1 : A single Gaussian stochastic layer Z with 50 units. In between the latent variable Z and
observation x there are two deterministic layers, each with 200 units;
3Data downloaded from http://www.cs.toronto.edu/~larocheh/public/datasets/
binarized_mnist/
4Data downloaded from https://github.com/yburda/iwae/raw/master/datasets/
OMNIGLOT/chardata.mat
6
Under review as a conference paper at ICLR 2018
G2 : Two Gaussian stochastic layers z1 and z2 with 50 and 100 units, respectively. Two deter-
ministic layers with 200 units connect the observation x and latent variable z2, and two
deterministic layers with 100 units are in between z2 and z1 .
where a Gaussian stochastic layer consists of two fully connected linear layers, with one outputting
the mean and the other outputting the logarithm of diagonal covariance. All other deterministic
layers are fully connected with tanh nonlinearity. Bernoulli observation models are assumed for both
MNIST and OMNIGLOT. For MNIST, we employ the static binarization strategy as in (Larochelle &
Murray, 2011) while dynamic binarization is employed for OMNIGLOT.
The inference networks q(z|x) for G1 and G2 have similar architectures to the generative models, with
details in (Burda et al., 2015). ConvFlow is hence used to warp the output of the inference network
q(z|x), assumed be to Gaussian conditioned on the input x, to match complex true posteriors. Our
baseline models include VAE (Kingma & Welling, 2013), IWAE (Burda et al., 2015) and Normalizing
Flows (Rezende & Mohamed, 2015). Since our propose method involves adding more layers to the
inference network, we also include another enhanced version of VAE with more deterministic layers
added to its inference network, which we term as VAE+.5 With the same VAE architectures, we also
test the abilities of constructing complex variational posteriors with IAF and ConvFlow, respectively.
All models are implemented in PyTorch. Parameters of both the variational distribution and the
generative distribution of all models are optimized with Adam (Kingma & Ba, 2014) for 2000 epochs,
with a fixed learning rate of 0.0005, exponential decay rates for the 1st and 2nd moments at 0.9 and
0.999, respectively. Batch normalization (Ioffe & Szegedy, 2015) is also used, as it has been shown
to improve learning for neural stochastic models (S0nderby et al., 2016).
For inference models with latent variable z of 50 dimensions, a ConvBlock consists of following
ConvFlow layers
[ConvFlow(kernel size = 5, dilation = 1), ConvFlow(kernel size = 5, dilation = 2),
ConvFlow(kernel size = 5, dilation = 4), ConvFlow(kernel size = 5, dilation = 8),
ConvFlow(kernel size = 5, dilation = 16), ConvFlow(kernel size = 5, dilation = 32)]	(20)
and for inference models with latent variable z of 100 dimensions, a ConvBlock consists of following
ConvFlow layers
[ConvFlow(kernel size = 5, dilation = 1), ConvFlow(kernel size = 5, dilation = 2),
ConvFlow(kernel size = 5, dilation = 4), ConvFlow(kernel size = 5, dilation = 8),
ConvFlow(kernel size = 5, dilation = 16), ConvFlow(kernel size = 5, dilation = 32),
ConvFlow(kernel size = 5, dilation = 64)]	(21)
A Revert layer is appended after each ConvBlock and leaky ReLU with a negative slope of 0.01 is used
as the activation function in ConvFlow. For IAF, the autoregressive neural network is implemented as
a two layer masked fully connected neural network.
4.2.2	Generative Density Estimation
For MNIST, models are trained and tuned on the 60,000 training and validation images, and estimated
log-likelihood on the test set with 5000 importance weighted samples are reported. Table 1 presents
the performance of all models, when the generative model is assumed to be from both G1 and G2 .
Firstly, VAE+ achieves higher log-likelihood estimates than vanilla VAE due to the added more layers
in the inference network, implying that a better posterior approximation is learned (which is still
assumed to be a Gaussian). Second, we observe that VAE with ConvFlow achieves much better
density estimates than VAE+, which confirms our expectation that warping the variational distribution
with convolutional flows enforces the resulting variational posterior to match the true complex
posterior. Also, adding more blocks of convolutional flows to the network makes the variational
posterior further close to the true posterior. We also observe that VAE with Inver Autoregressive Flows
(VAE+IAF) does not always improve likelihood estimates, and even if they do, the improvements are
not as significant as ConvFlow. This also confirms our analysis on the singular transformation and
5VAE+ adds more layers before the stochastic layer of the inference network while the proposed method is
add convolutional flow layers after the stochastic layer
7
Under review as a conference paper at ICLR 2018
Table 1: MNIST test set NLL with generative models G1 and G2 (lower is better K is number of
ConvBlocks)
MNIST (static binarization)	- log p(x) on G1	- log p(x) on G2
VAE (Burda et al., 2015)	87.88	85.65
IWAE (IW = 50) (Burda et al., 2015)	86.10	84.04
VAE+NF (Rezende & Mohamed, 2015)	-	≤ 85.10
VAE+ (K =1)	87.56	85.53
VAE+ (K= 4)	87.40	85.23
VAE+ (K = 8)	87.28	85.07
VAE+IAF (K =1)	88.50	86.00
VAE+IAF (K = 2)	88.27	85.86
VAE+IAF (K = 4)	88.03	85.95
VAE+IAF (K = 8)	87.97	85.50
VAE+ConvFlow (K =1)	86.91	85.45
VAE+ConvFlow (K = 2)	86.40	85.37
VAE+ConvFlow (K= 4)	84.78	81.64
VAE+ConvFlow (K= 8)	83.89	81.21
IWAE+ConvFlow (K = 8, IW = 50)	79.78	78.51
subspace issue in IAF. Lastly, combining convolutional normalizing flows with multiple importance
weighted samples, as shown in last row of Table 1, further improvement on the test set log-likelihood
is achieved. Overall, the method combining ConvFlow and importance weighted samples achieves
best NLL on both settings, outperforming IWAE significantly by about 6.3 nats on G1 and 5.5 nats on
G2. Notice that, ConvFlow combined with IWAE achieves an NLL of 78.51, slightly better than the
best published result of 79.10, achieved by PixelRNN (Oord et al., 2016b). Also it’s 1 nat better than
the best IAF result of 79.88 reported in (Kingma et al., 2016), which demonstrates the representative
power of ConvFlow compared to IAF6.
Results on OMNIGLOT are presented in Table 2 where similar trends can be observed as on MNIST.
One observation different from MNIST is that, the gain from IWAE+ConvFlow over IWAE is not as
large as it is on MNIST, which could be explained by the fact that OMNIGLOT is a more difficult
set compared to MNIST, as there are 1600 different types of symbols in the dataset. Again on
OMNIGLOT we observe IAF with VAE doesn’t perform as well as ConvFlow.
4.2.3	Generated Samples
After the models are trained, generative samples can be obtained by feeding Z 〜N(0, I) to the
learned generative model Gi (or z2 〜N(0, I) to G2). Since higher log-likelihood estimates are
obtained on G2, Figure 3 shows the random generative samples from our proposed method trained
with G2 on both MNIST and Ominiglot, compared to real samples from the training sets. We observe
the generated samples are visually consistent with the training data.
5	Conclusions
This paper presents a simple and yet effective architecture to compose normalizing flows based
on convolution on the input vectors. ConvFlow takes advantage of the effective computation of
convolution, as well as maintaining as few parameters as possible. To further accommodate long range
interactions among the dimensions, dilated convolution is incorporated to the framework without
6The result in (Kingma et al., 2016) are not directly comparable, as their results are achieved with a more
sophiscated VAE architecture and a much higher dimension of latent code (d = 1920 for the best NLL of 79.88).
However, in this paper, we only assume a relatively simple VAE architecture compose of fully connected layers
and the dimension of latent codes to be relatively low, 50 or 100, depending on the generative model in VAE.
One could expect the performance of IAF to drop further if simpler VAE architecture and latent codes with lower
dimensions are used.
8
Under review as a conference paper at ICLR 2018
Table 2: OMNIGLOT test set NLL with generative models G1 and G2 (lower is better, K is number
of ConvBlocks)
Omniglot	- log p(x) on G1	- log p(x) on G2
VAE (Burda et al., 2015)	108.86	107.93
IWAE (IW = 50)(Burda et al., 2015)	104.87	103.93
VAE+ (K = 1)	108.80	107.89
VAE+ (K = 4)	108.64	107.80
VAE+ (K = 8)	108.53	107.67
VAE+IAF (K =1)	109.44	108.74
VAE+IAF (K = 2)	109.69	108.36
VAE+IAF (K= 4)	109.47	107.61
VAE+IAF (K = 8)	109.34	107.43
VAE+ConvFlow (K = 1)	107.41	106.32
VAE+ConvFlow (K = 2)	107.05	105.80
VAE+ConvFlow (K = 4)	106.24	104.35
VAE+ConvFlow (K = 8)	105.87	103.58
IWAE+ConvFlow (K = 8,IW = 50)	104.21	103.02
d 7-s-1 uʃ?6dG
3 夕">y∙0cv9o6
4 g3 t∏ 5 l∖/
048 I ς⅛ 71 ZGG
snir*7qv,
ruF∕∕⅛7OB6 f —
q夕胃I g¥夕”
4 qfτ,6勺 £0S
4 工ki
7 Q» 3 /Nf,
/C*<rq37?不 7幺
szm's/yc
3，■> O 5，，夕 ∖y
HnTnVJf54 7 RD
夕/¢6J ðf ʒ √ ə
7 Λ¾ I 7 V37ol
3。，：7 7 ZDr
夕3 & 0。？O V-Ms
m 了 ING彳 SZ
AV∕ftrlzql*ΓΛΛ6^
£，夕外4Q5
rv S 夕 5 G 4 g 76 4
7 7α S 户e
3 G& 6 7 Ollt √- ¥ /-
9d6q/
3 rf I r j / 5，7 F
，fτi二夕 7步
夕 3夕，2?
夕『。。叶f
6次3t2 0 Ug
j5q7qlg‹⅛ 歹
I /S I F 7x9/
3FP*5f2∕13H
03y7Ni 乙 + f
(6 gG2 鲤夕
6⅛"3 0 CiQy
名//b石&
£。3 夕3ysl
(a) MNIST Training data (b) Random samples 1 from (c) Random samples 2 from (d) Random samples 3 from
IWAE-ConvFlow (K = 8) IWAE-ConvFlow (K = 8) IWAE-ConvFlow (K = 8)
mmt>xzj∙zi-申 nJ" F
2 b。工应XtJJ30
匕目为何1建。|。-<4心
X Ps , .∙ ɪ co K W ^¼- 7 ɪ-
A %- -b，< Q 3 百 V
3 》 q XHa Emtn G a
⑪m A今力「 XL?次
IU 1-怩 μ M Ctl √2 JxJn
卜⑨/ 户口杵
(e) OMNIGLOT Training (f) Random samples from (g) Random samples from (h) Random samples from
data	IWAE-ConvFlow (K = 8) IWAE-ConvFlow (K = 8) IWAE-ConvFlow (K = 8)
Figure 3: Training data and generated samples
increasing model parameters. A Revert Layer is used to maximize the opportunity that all dimensions
get as much warping as possible. Experimental results on inferring target complex density and density
estimation on generative modeling on real world handwritten digits data demonstrates the strong
performance of ConvFlow. Particularly, density estimates on MNIST show significant improvements
over state-of-the-art methods, validating the power of ConvFlow in warping multivariate densities.
It remains an interesting question as to how many layers of ConvFlows are best to exploit its full
performance. We hope to address the theoretical properties of ConvFlow in future work.
9
Under review as a conference paper at ICLR 2018
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993-1022, 2003.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: masked autoencoder for
distribution estimation. In Proceedings of the 32nd International Conference on Machine Learning
(ICML-15), pp. 881-889, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448-456, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, Rafal JOzefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improving variational autoencoders with inverse autoregressive flow. In Advances in Neural
Information Processing Systems 29: Annual Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pp. 4736-4744, 2016.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. One-shot learning by inverting
a compositional causal process. In Advances in Neural Information Processing Systems 26: 27th
Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting
held December 5-8, 2013, Lake Tahoe, Nevada, United States., pp. 2526-2534, 2013.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In Proceedings
of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011,
Fort Lauderdale, USA, April 11-13, 2011, pp. 29-37, 2011.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
arXiv preprint arXiv:1601.06759, 2016b.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France,
6-11 July 2015, pp. 1530-1538, 2015.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Annual Conference on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, pp. 3738-3746, 2016.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv
preprint arXiv:1511.07122, 2015.
10