Under review as a conference paper at ICLR 2018
Stochastic Hyperparameter Optimization
through Hypernetworks
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning models are usually tuned by nesting optimization of model
weights inside the optimization of hyperparameters. We give a method to col-
lapse this nested optimization into joint stochastic optimization of both weights
and hyperparameters. Our method trains a neural network to output approximately
optimal weights as a function of hyperparameters. We show that our method con-
verges to locally optimal weights and hyperparameters for sufficiently large hyper-
nets. We compare this method to standard hyperparameter optimization strategies
and demonstrate its effectiveness for tuning thousands of hyperparameters.
1 Introduction
× × Train loss of optimized weights
Model selection and hyperparameter tuning is a major
bottleneck in designing predictive models. Hyperpa-
rameter optimization can be seen as a nested optimiza-
tion: The inner optimization finds model parameters w
which minimizes the training loss LTrain given hyper-
parameters λ. The outer optimization chooses λ to min-
imize a validation loss LValid. :
argmin L argmin L (w, λ)	(1)
λ	Valid.	w Train
Standard practice in machine learning solves (1) by
gradient-free optimization of hyperparameters, such
as grid search, random search, or Bayesian optimiza-
tion. Each set of hyperparameters is evaluated by re-
initializing weights and training the model to comple-
tion. This is wasteful, since it trains the model from
scratch each time, even if the hyperparameters change
a small amount. Hyperband (Li et al., 2016) and freeze-
thaw Bayesian optimization (Swersky et al., 2014) re-
sume model training and do not waste this effort. Fur-
thermore, gradient-free optimization scales poorly be-
yond 10 or 20 dimensions.
How can we avoid re-training from scratch each time?
We usually estimate the parameters with stochastic op-
timization, but the true optimal parameters are a deter-
ministic function of the hyperparameters λ:
w*(λ) = argmin L (w,λ)	(2)
w Train
We propose to learn this function. Specifically, we train
a neural network with inputs of hyperparameters, and
Train loss Ofhypemet weights
× × Valid, loss of optimized weights
Valid, loss ofhypemet weights
— Optimal hypeιparameter λ
Figure 1: Training and validation loss
of a neural net, estimated by cross-
validation (crosses) or by a hypernet
(lines), which outputs 7, 850-dimensional
network weights. The training and valida-
tion loss can be cheaply evaluated at any
hyperparameter value using a hypernet.
Standard cross-validation requires train-
ing from scratch each time.
outputs of an approximately optimal set of weights given the hyperparameters.
This provides two major benefits: First, we can train the hypernet to convergence using stochastic
gradient descent, denoted SGD, without training any particular model to completion. Second, differ-
entiating through the hypernet allows us to optimize hyperparameters with gradient-based stochastic
optimization.
1
Under review as a conference paper at ICLR 2018
Γ(w*(λ),λ)
-^(w^*(λ),λ)
____ χ
*	w*(λ),w^*(λ)
(YCm)TWLy SSOq
(M)PiP>7 SSoa
-----λ*
*	w*(λ*)
λ≠*
*	W⅛* (λ⅛*)
Figure 2: A visualization of exact (blue) and approximate (red) optimal weights as a function of
given hyperparameters. Left: The training loss surface. Right: The validation loss surface. The
approximately optimal weights w@* are output by a linear model fit at A. The true optimal hyperpa-
rameter is λ*, while the hyperparameter estimated using approximately optimal weights is nearby at
λφ* .
2	Training a network to output optimal weights
How can we train a neural network to output approximately optimal weights of another neural net-
work? A neural net which outputs the weights of another neural net is called a hypernet (Ha et al.,
2016). The basic idea is that at each iteration, we ask a hypernet to output a set of weights given some
hyperparameters: W = wφ(λ). Instead of updating weights W using the loss gradient dL(W)/∂w, We
update the hypernet weights φ using the chain rule: ^^,dWφ. We call this method hyper-training
and contrast it with standard training methods in Figure 3.
We call the function w*(λ) that outputs optimal weights for hyperparameters a best-response fUnc-
tion (Fudenberg & Levine, 1998). At convergence, we want our hypernet wφ(λ) to closely match
the best-response function.
2.1	Advantages of hypernet-based optimization
We can compare hyper-training to other model-based hyperparameter schemes, such as Bayesian
optimization. Bayesian optimization (Snoek et al., 2012) builds a model of the validation loss as a
function of hyperparameters, usually using a Gaussian process (Rasmussen & Williams, 2006) to
track uncertainty. This approach has several disadvantages compared to hyper-training.
First, obtaining data for standard Bayesian optimization requires optimizing models from initializa-
tion for each set of hyperparameters. In contrast, hyper-training never needs to fully optimize any
one model.
Second, standard Bayesian optimization treats the validation loss as a black-box function:
LValid.(λ) = f (λ). In contrast, hyper-training takes advantage of the fact that the validation loss
is a known, differentiable function: LValid. (λ) = LValid. (wφ(λ)) This removes the need to learn a
model of the validation loss. This function can also be evaluated stochastically by sampling points
from the validation set.
What sort of parameters can be optimized by our approach? Hyperparameters typically fall into two
broad categories: 1) Optimization hyperparameters such as learning rates and initialization schemes,
and 2) Regularization or model architecture parameters. Hyper-training does not have inner opti-
mization hyperparameters because there is no inner training loop, so we can not optimize these. We
must still choose optimization parameters for the fused optimization loop, but this is also the case for
any model-based hyperparameter optimization method. The method can be applied to unconstrained
bi-level optimization problems which consist of an inner loss function, inner parameter, outer loss
function, and an outer parameter.
2
Under review as a conference paper at ICLR 2018
Algorithm 1: Standard cross-validation with stochastic optimization			Algorithm 2: Stochastic optimization of hy- pernet, then hyperparameters
1	for i = 1, . . . , Touter	1	
2	initialize w	2	: initialize φ
3	λ = hyperopt(..., λ(i), LValid. (w(i)))	3	: initialize λ
4	for Tinner steps	4	: for Thypernet steps
5	X 〜Training data	5	x 〜Training data, λ 〜P (λ)
6	w = w - αVw LTrain(X, W λ)	6	:	φ = φ - αVφ LTrain(X, wφ(λ), λ)
7	λi, wi = λ, w	7	
8	for i = 1, . . . , Touter	8	: for Thyperparameter steps
9	if LValid. w(i) < LValid. (w) then	9	x 〜Validation data
10	K, w = λi, wi	10	c	c C 一 ~	/	/屋、 λ = λ - βvλ LValid. (x, wφ(λ))
11	return λ, w	11	: return λ, wφ(λ)
Figure 3: A comparison of standard hyperparameter optimization, and our first algorithm. Instead of
updating weights W using the loss gradient dL(W)/∂w, We update hypernet weights φ using the chain
rule: ©LWW? dWWφφ. Also, instead of returning the best hyperparameters from a fixed set, our method
uses gradient-based hyperparameter optimization. Here, hyperopt refers to a generic hyperparameter
optimization.
2.2	Limitations of hypernet-based optimization
Hyper-training can handle discrete hyperparameters but does not offer special advantages for op-
timizing over continuous hyperparameters. The hyperparameters being optimized must affect the
training loss - this excludes optimization hyperparameters like learning rate. Also, our approach
only proposes making local changes to the hyperparameters and does not do uncertainty-based ex-
ploration. Uncertainty can be incorporated into the hypernet by using stochastic variational infer-
ence as in Blundell et al. (2015), but we leave this for future work. Finally, it is not obvious how
to choose the distribution over hyperparameters p(λ). If we do not sample a sufficient number of
hyperparameters we may reach sub-optimal solutions. We approach this problem in section 2.4.
An obvious difficulty of this approach is that training a hypernet typically requires training several
times as many parameters as training a single model. For example, training a fully-connected hy-
pernet with a single hidden layer of H units to output D parameters requires training at least D × H
hypernet parameters. Again, in section 2.4 we propose an algorithm that requires training only a
linear model mapping hyperparameters to model weights.
2.3	Asymptotic convergence properties
Algorithm 2 trains a hypernet using SGD, drawing hyperparameters from a fixed distribution p(λ).
This section proves that Algorithm 2 converges to a local best-response under mild assumptions. In
particular, we show that, for a sufficiently large hypernet, the choice of p(λ) does not matter as long
as it has sufficient support. Notation as ifWφ has a unique solution for φ or W is used for simplicity,
but is not true in general.
Theorem 2.1. Sufficiently powerful hypernets can learn continuous best-response functions, which
minimizes the expected loss for any hyperparameter distribution.
There exists φ*, such that for all λ ∈ SuPPort(P (λ)),
L (wφ* (λ), λ) = min L (w, λ)
Train	W Train
and φ* = argmin E L (wφ(λ0),λ0)
φ	p(λ0 ) Train
Proof. If wφ is a universal approximator (Hornik, 1991) and the best-response is continuous in
λ (which allows approximation by Wφ), then there exists optimal hypernet parameters φ* such
that for all hyperparameters λ, wφ* (λ) = argminW LTrain(w, λ). Thus, LTrain(wφ* (λ) , λ) =
minW LTrain (w, λ). In other words, universal approximator hypernets can learn continuous best-
responses.
3
Under review as a conference paper at ICLR 2018
	Algorithm 2: Stochastic optimization of hy-		Algorithm 3: Stochastic optimization of hy-
	pernet, then hyperparameters		pernet and hyperparameters jointly
1	initialize φ, λ	1	initialize φ, λ
2	for Thypernet steps	2	for Tjoint steps
3	X 〜Training data, λ 〜P (λ)	3	X 〜Training data, λ 〜P(λ∣λ)
4	Φ = Φ — αVφ LTrain(X, wφ(')/)	4	φ = φ - αVφ LTrain(X, wφ(λ), λ)
5	for Thyperparameter steps	5	
6	x 〜Validation data	6	x 〜Validation data
7	λ = λ ― βvλ LValid. (x, wφ(λ))	7	λ = λ ― βvλ LValid. (x, wφ(λ))
8	return λ, wφ(λ)	8	return λ, wφ(λ)
Figure 4: A side-by-side comparison of two variants of hyper-training. Algorithm 3 fuses the hy-
pernet training and hyperparameter optimization into a single loop of SGD.
Substituting φ* into thetraining loss gives Ep(λ) [Lτrain (wφ* (λ), λ)] = Ep°) [minφ LTrain(Wφ(λ), λ)].
By Jensen’s inequality, minφEp(λ)[LTrain(wφ(λ), λ)] ≥ Ep(λ)[minφ LTrain(wφ(λ), λ)] where
minφ is a convex function on the convex vector space of functions {LTrain(wφ(λ), λ) for λ ∈
SuPPort(P (λ))} if SuPPort(P (λ)) is convex and LTrain(w,λ) = Ex〜TrainiLPred(x, w)] +
LReg(w, λ) with LReg(W,λ) = λ ∙ L(w). Thus, φ* = argminφ Ep(λ) [Lτrain(wφ(λ), λ)]. In other
words, if the hypernet learns the best-response it will simultaneously minimize the loss for every
point in the SuPPort(P (λ)).	□
Thus, having a universal approximator and a continuous best-response implies for all λ ∈
SuPPort(P(λ)), LValid.(wφ*(λ)) = LValid.(w*(λ)) because wφ*(λ) = w*(λ). Thus, under mild
conditions, we will learn a best-response in the support of the hyperparameter distribution.If the
best-response is differentiable, then there is a neighborhood about each hyperparameter where the
best-response is approximately linear. Ifwe select the support of the hyperparameter distribution to
be the neighborhood where the best-response is approximately linear then we can use linear regres-
sion to learn the best-response locally.
Theorem 2.1 holds for any P (λ). However in practice, we have a limited-capacity hypernet, and so
should choose aP (λ) that puts most of its mass on promising hyperparameter values. This motivates
the joint optimization of φ and P (λ). Concretely, we can introduce a “current” hyperparameter λ
and define a conditional hyperparameter distribution P(λ,) which places its mass near λ. This
allows us to use a limited-capacity hypernet, at the cost of having to re-train the hypernet each time
We update λ.
In practice, there are no guarantees about the network being a universal approximator or the finite-
time convergence of optimization. The optimal hypernet will depend on the hyperparameter dis-
tribution P(λ), not just the support of this distribution. We appeal to experimental results that our
method is feasible in practice.
2.4	Jointly training parameters and hyperparameters
Because in practice we use a limited-capacity hypernet, it may not be possible to learn a best-
response for all hyperparameters. Thus, we propose Algorithm 3, which only tries to learn a best-
response locally. We introduce a “current” hyperparameter A, which is updated each iteration. We
define a conditional hyperparameter distribution, pC), which only puts mass close to A.
Algorithm 3 combines the two phases of Algorithm 2 into one. Instead of first learning a hypernet
that can output weights for any hyperparameter then optimizing the hyperparameters, Algorithm 3
only samples hyperparameters near the current best guess. This means the hypernet only has to be
trained to estimate good enough weights for a small set of hyperparameters. The locally-trained
hypernet can then be used to provide gradients to update the hyperparameters based on validation
set performance.
How simple can we make the hypernet, and still obtain useful gradients to optimize hyperpa-
rameters? Consider the case used in our experiments where the hypernet is a linear function
of the hyperparameters and the conditional hyperparameter distribution is P(λ∣λ) = N(5,σ1)
for some small σ. This hypernet learns a tangent hyperplane to a best-response function and
only needs to make small adjustments at each step if the hyperparameter updates are sufficiently
4
Under review as a conference paper at ICLR 2018
small. We can further restrict the capacity of a linear hypernet by factorizing its weights, ef-
fectively adding a bottleneck layer with a linear activation and a small number of hidden units.
× × Train loss of optimized weights
Train loss Ofhypemet weights
× × Valid, loss of optimized weights
Valid, loss ofhypemet weights
— Optimal hypeιparameter λ
p(ʌlʌ)
Figure 5: The training and validation
losses of a neural network, estimated by
cross-validation (crosses) or a linear hy-
pernet (lines). The hypernet’s limited ca-
pacity makes it only accurate where hy-
perparameter distribution put mass.
3	Related Work
Our work is closely related to the concurrent work of
Brock et al. (2017), whose SMASH algorithm also ap-
proximates the optimal weights as a function of model
architectures, to perform a gradient-free search over
discrete model structures. Their work focuses on ef-
ficiently evaluating the performance of a variety of dis-
crete model architectures, while we focus on efficiently
exploring continuous spaces of models.
Model-free approaches Model-free approaches only
use trial-and-error to explore the hyperparameter space.
Simple model-free approaches applied to hyperpa-
rameter optimization include grid search and random
search (Bergstra & Bengio, 2012). Hyperband (Li et al.,
2016) combines bandit approaches with modeling the
learning procedure.
Model-based approaches Model-based approaches
try to build a surrogate function, which often allows
gradient-based optimization or active learning. A com-
mon example is Bayesian optimization (Snoek et al.,
2012). Freeze-thaw Bayesian optimization can condi-
tion on partially-optimized model performance.
Optimization-based approaches Another line of re-
lated work attempts to directly approximate gradients
of the validation loss with respect to hyperparameters. Domke (2012) proposes to differentiate
through unrolled optimization to approximate best-responses in nested optimization and Maclaurin
et al. (2015a) differentiate through entire unrolled learning procedures. DrMAD (Fu et al., 2016) ap-
proximates differentiating through an unrolled learning procedure to relax memory requirements for
deep neural networks. HOAG (Pedregosa, 2016) finds hyperparameter gradients with implicit dif-
ferentiation by deriving an implicit equation for the gradient with optimality conditions. Franceschi
et al. (2017) study forward and reverse-mode differentiation for constructing hyperparameter gradi-
ents. Also, Feng & Simon (2017) establish conditions where the validation loss of best-responding
weights are almost everywhere smooth, allowing gradient-based training of hyperparameters.
A closely-related procedure to our method is the T1 - T2 method of Luketina et al. (2016), which
also provides an algorithm for stochastic gradient-based optimization of hyperparameters. The con-
vergence of their procedure to local optima of the validation loss depends on approximating the
Hessian of the training loss with respect to parameters with the identity matrix. In contrast, the
convergence of our method depends on having a suitably powerful hypernet.
Game theory Best-response functions are extensively studied as a solution concept in discrete
and continuous multi-agent games (Fudenberg & Levine, 1998). Games where learning a best-
response can be applied include adversarial training (Goodfellow et al., 2014), or Stackelberg com-
petitions (Bruckner & Scheffer, 2011). For adversarial training, the analog of our method would be
a discriminator which is trained while observing all of the generator’s parameters.
4 Experiments
In our experiments, we examine the standard example of stochastic gradient-based optimization
of neural networks, with a weight regularization penalty. Gradient-based methods explicitly use
5
Under review as a conference paper at ICLR 2018
the gradient of a loss, while gradient-free methods do not (but can use the gradient of a surrogate
loss that is learned). Our algorithm may be contrasted with gradient-free techniques like Bayesian
optimization, gradient-based methods only handle hyperparameters affecting the training loss and
gradient-based methods which can additionally handle optimization parameters. The best compar-
ison for hyper-training is to gradient-based methods which only handle parameters affecting the
training loss because other methods apply to a more general set of problems. In this case, the train-
ing and validation losses can be written as:
L (w, λ) = E L (x, w) + L (w, λ)
Train	x~Train Pred	Reg
L (w) = E L (x, w)
Valid.	x~Valid. Pred
In all experiments, Algorithms 2 or 3 are used to optimize weights of with mean squared error on
MNIST (LeCun et al., 1998) with LReg as an L2 weight decay penalty weighted by exp(λ). The
elementary model has 7, 850 weights. All hidden units in the hypernet have a ReLU activation (Nair
& Hinton, 2010) unless otherwise specified. Autograd (Maclaurin et al., 2015b) was used to compute
all derivatives. For each experiment, the minibatch samples 2 pairs of hyperparameters and up to
1, 000 training data points. Adam was used for training the hypernet and hyperparameters, with a
step size of 0.0001. All experiments were run on a 2012 MacBook pro.
4.1	Learning a global best-response
Our first experiment, shown in Figure 1, demonstrates learning a global approximation to a best-
response function using Algorithm 2. To make visualization of the regularization loss easier, we use
10 training data points to exacerbate overfitting. We compare the performance of weights output by
the hypernet to those trained by standard cross-validation (Algorithm 1). Thus, elementary weights
were randomly initialized for each hyperparameter setting and optimized using Adam (Kingma &
Ba, 2014) for 1, 000 iterations with a step size of 0.0001.
When training the hypernetwork, hyperparameters were sampled from a broad Gaussian distribution:
p (λ) = N (0, 1.5). The hypernet has 50 hidden units which results in 400, 450 parameters of the
hypernetwork.
The minimum of the best-response in Figure 1 is close to the true minimum of the validation loss,
which shows a hypernet can satisfactorily approximate a global best-response function in small
problems.
4.2	Learning a local best-response
Figure 5 shows the same experiment, but using the fused updates of Algorithm 3. This results in
finding a best-response approximation whose minimum is the true minimum faster than the prior
experiment. The conditional hyperparameter distribution is given by p(λ,) = N(A, 0.00001). The
hypernet is a linear model, with only 15, 700 weights. We use the same optimizer as the global
best-response to update both the hypernet and the hyperparameters.
Again, the minimum of the best-response at the end of training is the true optimum on the validation
loss. This experiment shows that using only a locally-trained linear best-response function can give
sufficient gradient information to optimize hyperparameters on a small problem. Algorithm 3 is also
less computationally expensive than Algorithms 1 or 2.
4.3	Hyper-training and unrolled optimization
In order to compare hyper-training with other gradient-based hyperparameter optimization we
trained a model with 7, 850 hyperparameters with a separate L2 weight decay applied to each weight
in a 1 layer (linear) model. The conditional hyperparameter distribution and optimizer for the hyper-
net and hyperparameters is the same the prior experiment. The weights for the model are factorized
by selecting a hypernet with 10 hidden units. The factorized linear hypernet has 10 hidden units
giving 164,860 weights. This means each hypernet iteration 2 ∙ 10 times as expensive as an iteration
on just the model, because there is the same number of hyperparameters as model parameters.
6
Under review as a conference paper at ICLR 2018
Optimizing with deeper networks
Hyper-training and unrolled optimization
Figure 6: Validation and test losses during hyperparameter optimization with a separate L2 weight
decay applied to each weight in the model. Thus, models with more parameters have more hy-
perparameters. Left: The 7, 850 dimensional hyperparameter optimization problem from having
a linear model is solved with multiple algorithms. Hypernetwork-based optimization converges
faster than unrolled optimization from Maclaurin et al. (2015a) but to a sub-optimal solution. Right:
Hyper-training is applied different layer configurations in the model. The hand-tuned regulariza-
tion parameters on the 784-10, 784-100-10, and 784-100-100-10 models have a validation losses of
0.434, 0.157 and 0.206 respectively.
Figure 6, left, shows that Algorithm 3 converges more quickly than the unrolled reverse-mode opti-
mization introduced in Maclaurin et al. (2015a) with an implementation by Franceschi et al. (2017).
Hyper-training reaches sub-optimal solutions because of limitations on how many hyperparameters
can be sampled for each update but overfits validation data less than unrolling. Standard Bayesian
optimization cannot be scaled to this many hyperparameters. Thus, this experiment shows Algo-
rithm 3 can effectively partially optimize thousands of hyperparameters.
0.10
0.08
0.06
0.04
0.02
^Valid. (Wφ*(Λ))
Hand-Tuned £Valid. (Wφ*(λ))
£TeSt(VV0*(力)
784-10
784-100-10
784-100-100-10
200	400	600	800
Runtime in seconds
0
4.4	Optimizing with deeper networks
In order to to see if we can optimize deeper networks with hyper-training we optimize models
with 1, 2, and 3 layers with a separate L2 weight decay applied to each weight. The conditional
hyperparameter distribution and optimizer for the hypernet and hyperparameters is the same the
prior experiment. The weights for each model are factorized by selecting a hypernet with 10 hidden
units. Again, standard Bayesian optimization cannot be scaled to this many hyperparameters.
Figure 6, right, shows that Algorithm 3 can scale to networks with multiple hidden layers and out-
perform hand-tuned settings. As more layers are added the difference between validation loss and
testing loss decreases, and the model performs better on the validation set. Future work should com-
pare other architectures like recurrent or convolutional networks. Additionally, note that more layers
perform with lower training (not shown), validation, and test losses, as opposed to lower training
loss but higher validation or test loss. This indicates that using weight decay on each weight could
be a prior for generalization, or that hyper-training enforces another useful prior like the continuity
of a best-response.
4.5	Estimating weights versus estimating loss
As mentioned above, our approach differs from Bayesian optimization in that we try to learn to pre-
dict optimal weights, while Bayesian optimization attempts to directly model the validation loss of
optimized weights. In this final experiment, we untangle the reason for the better performance of our
method: Is it because of a better inductive bias, or because our method can see more hyperparameter
settings during optimization?
First, we constructed a hyper-training set: We optimized 25 sets of weights to completion, given
randomly-sampled hyperparameters. We chose 25 samples since that is the regime in which we
expect Gaussian process-based approaches to have the largest advantage. We also constructed a
validation set of 10, 215 (optimized weight, hyperparameter) tuples generated in the same manner.
We then fit a Gaussian process (GP) regression model with an RBF kernel from sklearn on the
hyper-training data. A hypernet is fit to the same dataset. However, this hypernet was trained to
7
Under review as a conference paper at ICLR 2018
1.1
窈1.0
§5 0.9
eW°∙8
Z ⅞ 0.7
O⅞ 0.6
£ 0∙5
0.4
1.1
1 g ι.o
δb3 o.9
导M °∙8
H 苫 0.7
3 9 0.6
然笳
0.4
∙δ 1.1
e 窈 i.o
&3 09
及0.8
宗岂0・7
⅜⅞ O f
且 £ θ∙5
∞	0.4
0.6	0.7	0.8	0.9	1.0	1.1
True loss
Figure 7: Comparing three approaches to predicting validation loss. First row: A Gaussian pro-
cess, fit on a small set of hyperparameters and the corresponding validation losses. Second row: A
hypernet, fit on the same small set of hyperparameters and the corresponding optimized weights.
Third row: Our proposed method, a hypernet trained with stochastically sampled hyperparameters.
Left: The distribution of predicted and true losses. The diagonal black line is where predicted loss
equals true loss. Right: The distribution of differences between predicted and true losses. The Gaus-
sian process often under-predicts the true loss, while the hypernet trained on the same data tends to
over-predict the true loss.
fit optimized training weights, not optimized validation loss. Finally, we optimize another hypernet
using Algorithm 2, for the same amount of time as building the hyper-training set. The two hypernets
were linear models and were trained with the same optimizer parameters as the 7, 850-dimensional
hyperparameter optimization.
Figure 7 shows the distribution of prediction errors of these three models. We can see that the
Gaussian process tends to underestimate loss. The hypernet trained with the same small fixed set of
examples tends to overestimate loss. We conjecture that this is due to the hypernetwork producing
bad weights in regions where it doesn’t have enough training data. Because the hypernet must pro-
vide actual weights to predict the validation loss, poorly-fit regions will overestimate the validation
loss. Finally, the hypernet trained with Algorithm 2 produces errors tightly centered around 0. The
main takeaway from this experiment is a hypernet can learn more accurate surrogate functions than
a GP for equal compute budgets because it views (noisy) evaluations of more points.
Code for all experiments will be made available upon publication.
5 Conclusions
In this paper, we:
•	Presented algorithms that efficiently learn a differentiable approximation to a best-response
without nested optimization.
•	Showed empirically that hypernets can provide a better inductive bias for hyperparameter
optimization than Gaussian processes fit directly to the validation loss.
•	Gave a theoretical justification that sufficiently large networks will learn the best-response
for all hyperparameters it is trained against.
We hope that this initial exploration of stochastic hyperparameter optimization will inspire further
refinements, such as hyper-regularization methods, or uncertainty-aware exploration using Bayesian
hypernetworks.
8
Under review as a conference paper at ICLR 2018
References
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research,13(Feb):281-305, 2012.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Andrew Brock, Theodore Lim, JM Ritchie, and Nick Weston. Smash: One-shot model architecture
search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.
Michael Bruckner and Tobias Scheffer. Stackelberg games for adversarial prediction problems. In
Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining, pp. 547-555. ACM, 2011.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and
Statistics, pp. 318-326, 2012.
Jean Feng and Noah Simon. Gradient-based regularization parameter selection for problems with
non-smooth penalty functions. arXiv preprint arXiv:1703.09813, 2017.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1165-1173.
PMLR, 2017.
Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, and Tat-Seng Chua. Drmad: distilling reverse-
mode automatic differentiation for optimizing hyperparameters of deep neural networks. arXiv
preprint arXiv:1601.00917, 2016.
Drew Fudenberg and David K Levine. The theory of learning in games, volume 2. MIT press, 1998.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yann LeCun, Leon Bottou, YoShua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy-
perband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint
arXiv:1603.06560, 2016.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning
of continuous regularization hyperparameters. In International Conference on Machine Learning,
pp. 2952-2960, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015a.
Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Effortless gradients in numpy.
In ICML 2015 AutoML Workshop, 2015b.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814,
2010.
9
Under review as a conference paper at ICLR 2018
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning,pp. 737-746, 2016.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning,
volume 1. MIT press Cambridge, 2006.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. arXiv
preprint arXiv:1406.3896, 2014.
A Extra Experiments
A.1 OPTIMIZING 10 HYPERPARAMETERS
Here, we optimize a model with 10 hyperparameters, in which a separate L2 weight decay is ap-
plied to the weights for each digit class in a linear regression model to see if we can optimize
medium-sized models. The conditional hyperparameter distribution and optimizer for the hyper-
net and hyperparameters is the same the prior experiments. A linear hypernet is used, resulting in
86, 350 hyper-weights. Algorithm 3 is compared against random search and .
Figure 8, right, shows that our method converges more quickly and to a better optimum than either
alternative method, demonstrating that medium-sized hyperparameter optimization problems can be
solved with Algorithm 3.
Optimizing 10 hyperparameters
0.050ι
0.048
0.046
0.044
0.042
0.040
Random search JCValid
Bayesian opt. JCValid.
... Hypemet ^va∣id(wφ*W))
二二、RandOm search Γτest
...'x×'βayesian opt. JCTeSt
⅛pemet jCτest(Wφ*(Λ))
25	50	75	100	125
Runtime in seconds
0.038 κ-θ
Figure 8: Validation and test losses during hyperparameter optimization. A separate L2 weight
decay is applied to the weights of each digit class, resulting in 10 hyperparameters. The weights
wφ* are output by the hypernet for current hyperparameter λ, while random losses are for the best
result of a random search. Hypernetwork-based optimization converges faster than random search
or Bayesian optimization. We also observe significant overfitting of the hyperparameters on the
validation set, which may be reduced be introducing hyperhyperparameters (parameters of the hy-
perparameter prior). The runtime includes the inner optimization for gradient-free approaches so
that equal cumulative computational time is compared for each method.
Factors affecting this include removing the overhead of constructing tuples of hyperparameters and
optimized weights, viewing more hyperparameter samples, or having a better inductive bias from
learning weights.
10