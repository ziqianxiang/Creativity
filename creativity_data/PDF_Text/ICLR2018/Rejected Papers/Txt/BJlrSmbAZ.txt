Under review as a conference paper at ICLR 2018
Bayesian Uncertainty Estimation for Batch
Normalized Deep Networks
Anonymous authors
Paper under double-blind review
Ab stract
Deep neural networks have led to a series of breakthroughs, dramatically im-
proving the state-of-the-art in many domains. The techniques driving these ad-
vances, however, lack a formal method to account for model uncertainty. While
the Bayesian approach to learning provides a solid theoretical framework to han-
dle uncertainty, inference in Bayesian-inspired deep neural networks is difficult.
In this paper, we provide a practical approach to Bayesian learning that relies on
a regularization technique found in nearly every modern network, batch normal-
ization. We show that training a deep network using batch normalization is equiv-
alent to approximate inference in Bayesian models, and we demonstrate how this
finding allows us to make useful estimates of the model uncertainty. Using our
approach, it is possible to make meaningful uncertainty estimates using conven-
tional architectures without modifying the network or the training procedure. Our
approach is thoroughly validated in a series of empirical experiments on different
tasks and using various measures, showing itto outperform baselines on a majority
of datasets with strong statistical significance.
1	Introduction
Deep learning has dramatically advanced the state of the art in a number of domains, and now sur-
passes human-level performance for certain tasks such as recognizing the contents of an image (He
et al., 2015) and playing Go (Silver et al., 2017). But, despite their unprecedented discriminative
power, deep networks are prone to make mistakes. Sometimes, the consequences of mistakes are mi-
nor - misidentifying a food dish or a species of flower (LiU et al., 2016) may not be life threatening.
But deep networks can already be found in settings where errors carry serious repercussions such
as aUtonomoUs vehicles (Chen et al., 2016) and high freqUency trading. In medicine, we can soon
expect aUtomated systems to screen for skin cancer (Esteva et al., 2017), breast cancer (Shen, 2017),
and to diagnose biopsies (DjUric et al., 2017). As aUtonomoUs systems based on deep learning are
increasingly deployed in settings with the potential to caUse physical or economic harm, we need
to develop a better Understanding of when we can be confident in the estimates prodUced by deep
networks, and when we shoUld be less certain.
Standard deep learning techniqUes Used for sUpervised learning lack methods to accoUnt for Un-
certainty in the model, althoUgh sometimes the classification network’s oUtpUt vector is mistakenly
Understood to represent the model’s Uncertainty. The lack ofa confidence measUre can be especially
problematic when the network encoUnters conditions it was not exposed to dUring training. For ex-
ample, if a network trained to recognize dog breeds is given an image of a cat, it may predict it to
belong to a breed of small dog with high probability. When exposed to data oUtside of the distribU-
tion it was trained on, the network is forced to extrapolate, which can lead to Unpredictable behavior.
In sUch cases, if the network can provide information aboUt its Uncertainty in addition to its point
estimate, disaster may be avoided. This work focUses on estimating sUch predictive Uncertainties in
deep networks (FigUre 1).
The Bayesian approach provides a solid theoretical framework for modeling Uncertainty (Ghahra-
mani, 2015), which has prompted several attempts to extend neUral networks (NN) into a Bayesian
setting. Most notably, Bayesian neUral networks (BNNs) have been stUdied since the 1990’s (Neal,
2012). AlthoUgh they are simple to formUlate, BNNs reqUire sUbstantially more compUtational
resoUrces than their non-Bayesian coUnterparts, and inference is difficUlt. Importantly, BNNs do
1
Under review as a conference paper at ICLR 2018
Figure 1: We propose a method to estimate uncertainty in any network using batch normalization
(MCBN). Here, we show results on a toy dataset from networks with three hidden layers (30 units
per layer). The solid line is the predictive mean of 500 stochastic forward passes. The outer area
depicts the model’s uncertainty as the 95% CI of the predictive distribution for each x value (inner
shaded area is 50% CI). On the right, we show a similar plot using dropout to estimate uncertainty
(MCDO)(Gal & Ghahramani, 2015). The bottom row depicts a minimally useful baseline - the
same networks but with a constant uncertainty (CUBN, CUDO).
not scale well and struggle to compete with modern deep learning architectures. Recently, Gal &
Ghahramani (2015) developed a practical solution to obtain uncertainty estimates by casting dropout
training in conventional deep networks as an approximate Bayesian model. They showed that any
network trained with dropout is an approximate Bayesian model, and uncertainty estimates can be
obtained by computing the variance on multiple predictions with different dropout masks.
This technique, called Monte Carlo Dropout (MCDO), has a very attractive quality: it can be applied
to existing NNs without any modification to the architecture or the way the network is trained.
Uncertainty estimates come (nearly) for free. However, in recent years dropout has fallen out of
favor, limiting MCDO’s utility. Google’s Inception network, which won ILSVRC in 2014, did not
use dropout (Szegedy et al., 2015), nor did the ILSVRC 2015 winner, Microsoft’s residual learning
network (He et al., 2016). In place of traditional techniques like dropout, most modern networks
such as Inception and ResNet have adopted other regularization techniques. In particular, batch
normalization (BN) has become widespread thanks to its ability to stabilize learning with improved
generalization (Ioffe & Szegedy, 2015).
An interesting aspect of BN is that the mini-batch statistics used for training each iteration depend
on randomly selected batch members. We exploit this stochasticity and show that training using
batch normalization, like dropout, is equivalent to approximate inference in Bayesian models1. We
demonstrate how this finding allows us to make meaningful estimates of the model uncertainty in a
technique we call Monte Carlo Batch Normalization (MCBN) (Figure 1). The method we propose
makes no simplifying assumptions on the use of batch normalization, and applies to any network
using BN as it appears in practical applications.
We validate our approach by empirical experiments on eight standard datasets used for uncertainty
estimation. We measure uncertainty quality relative to a baseline of fixed uncertainty, and show that
MCBN outperforms the baseline on nearly all datasets with strong statistical significance. We also
show that the uncertainty quality of MCBN is on par with that of MCDO. As a practical demon-
stration of MCBN, we apply our method to estimate segmentation uncertainty using a conventional
segmentation network (Badrinarayanan et al., 2015). Finally, as part of our evaluation, we make
contributions to the methodology of measuring uncertainty quality by defining performance bounds
on existing metrics and proposing a new visualization that provides an intuitive understanding of
uncertainty quality.
1The possibility of using other stochastic regularization techniques is mentioned in Gal (2016).
2
Under review as a conference paper at ICLR 2018
2	Related Work
Bayesian models provide a natural framework for modeling uncertainty, and several approaches
have been developed to adapt NNs to Bayesian reasoning. A common approach is to place a prior
distribution (often a Gaussian) over each weight. For infinite weights, the resulting model corre-
sponds to a Gaussian process (Neal, 1995), and for a finite number of weights it corresponds to
a Bayesian neural network (MacKay, 1992). Although simple to formulate, inference in BNNs is
difficult (Gal, 2016). Therefore, focus has shifted to techniques to approximate the posterior distri-
bution, leading to approximate BNNs. Methods based on variational inference (VI) typically rely on
a fully factorized approximate distribution (Kingma & Welling, 2014; Hinton & Van Camp, 1993)
but these methods do not scale easily. To alleviate these difficulties, Graves (2011) proposed a
model using sampling methods to estimate a factorized posterior. Another approach, probabilis-
tic backpropagation (PBP), also estimates a factorized posterior based on expectation propagation
(Hemandez-Lobato & Adams, 2015).
Deep Gaussian Processes (DGPs) formulate GPs as Bayesian models capable of working on large
datasets with the aid of a number of strategies to address scaling and complexity requirements (Bui
et al., 2016). The authors compare DGP with a number of state-of-the-art approximate BNNs,
showing superior performance in terms of RMSE and uncertainty quality2. Another recent approach
to Bayesian learning, Bayesian hypernetworks, use a neural network to learn a distribution of para-
maters over another neural network (Krueger et al., 2017). Although these recent techniques address
some of the difficulties with approximate BNNs, they all require modifications to the architecture or
the way networks are trained, as well as specialized knowledge from practitioners.
Recently, Gal (2016) showed that a network trained with dropout implicitly performs the VI ob-
jective. Therefore any network trained with dropout can be treated as an approx. Bayesian model
by making multiple predictions as forward passes through the network while sampling different
dropout masks for each prediction. An estimate of the posterior can be obtained by computing the
mean and variance of the predictions. This technique, referred to here as MCDO, has been em-
pirically demonstrated to be competitive with other approx. BNN methods and DGPs in terms of
RMSE and uncertainty quality (Li & Gal, 2017). However, as the name implies, MCDO depends on
dropout. While once ubiquitous in training deep learning models, dropout has largely been replaced
by batch normalization in modern networks, limiting its usefulness.
3	Method
The methodology of this work is to pose a deep network trained with batch normalization as a
Bayesian model in order to obtain uncertainty estimates associated with its predictions. In the fol-
lowing, we briefly introduce Bayesian models and a variational approximation to it using Kullback-
Leibler (KL) divergence following Gal & Ghahramani (2015). We continue by showing a batch
normalized deep network can be seen as an approximate Bayesian model. Then, by employing the-
oretical insights as well as empirical analysis, we study the induced prior on the parameters when
using batch normalization. Finally, we describe the procedure we use for estimating uncertainty of
batch normalized deep networks’ output.
3.1	Bayesian Modeling
We assume a finite training set D = {(xi, yi)}i=1:N where each (xi, yi) is a sample-label pair.
Using D, we are interested in learning an inference function fω(x, y) with parameters ω. In deter-
ministic models, the estimated label ^ is obtained as follows:
y = arg max fω (x, y)
y
We assume fω(x, y) = p(y|x, ω) (e.g. in soft-max classifiers), and is normalized to a proper prob-
ability distribution. In Bayesian modeling, in contrast to finding a point estimate of the model
parameters, the idea is to estimate an (approximate) posterior distribution of the model parameters
2By uncertainty quality, we refer to predictive probability distributions as measured by PLL and CRPS.
3
Under review as a conference paper at ICLR 2018
p(ω∣D) to be used for probabilistic prediction:
p(y|x, D) =
fω (x, y)p(ω∣D)dω
The predicted label, y, can then be accordingly obtained by sampling p(y∣x, D) or takings its max-
ima.
Variational Approximation In approximate Bayesian modeling, it is a common approach to
learn a parametrized approximating distribution qθ(ω) that minimizes KL(qθ(ω)∣∣p(ω∣D)); the
Kullback-Leibler (KL) divergence of posterior w.r.t. its approximation, instead of the true posterior.
Minimizing this KL divergence is equivalent to the following minimization while being free of the
data term p(D) 3 4:
LVA(θ) :
N
-XZ
i=1
qθ(ω)lnfω(xi, yi)dω + KL(qθ(ω)∣∣p(ω))
Using Monte Carlo integration to approximate the integral with one realized ωi for each sample i 4,
and optimizing over mini-batches of size M, the approximated objective becomes:
NM
LVA(θ):= -M Elnfωi(Xi, yi)+ KL(qθ(ω)∣∣p(ω))	(1)
M i=1
The first term is the data likelihood and the second term is divergence of the model prior w.r.t. the
approximated distribution.
We now describe the optimization procedure of a deep network with batch normalization and draw
the resemblance to the approximate Bayesian modeling in Eq (1).
3.2	Batch Normalized Deep Nets as Bayesian Modeling
The inference function of a feed-forward deep network with L layers can be described as:
fω (x) = WLa(WL-1 ...a(W2a(W1x))
where a(.) is an element-wise nonlinearity function and Wl is the weight vector at layer l. Further-
more, we denote the input to layer l as xl with x1 = x and we then set hl = Wlxl . Parenthesized
super-index for matrices (e.g. W(j)) and vectors (e.g. x(j)) indicates jth row and element respec-
tively. Super-index u refers to a specific unit at layer l, (e.g. Wu = Wl,(j), hu = hl,(j)). 5
Batch Normalization Each layer of a deep network is constructed by several linear units whose
parameters are the rows of the weight matrix W. Batch normalization is a unit-wise operation
proposed in Ioffe & Szegedy (2015) to standardize the distribution of each unit’s input. It essentially
converts a unit’s output hu in the following way:
hU = hu - E[hu]
PVar [hu]
where the expectations are computed over the training set6. However, often in deep networks, the
weight matrices are optimized using back-propagated errors calculated on mini-batches of data.
Therefore, during training, the estimated mean and variance on the mini-batch B is used, which
We denote by μB and σB respectively. This makes the inference at training time for a sample X a
stochastic process, varying based on other samples in the mini-batch.
3achieved by constructing the Evidence Lower Bound, called ELBO, and assuming i.i.d. observation noise;
details can be found in the appendix sec 6.1.
4while a MC integration using a single sample is a weak approximation, in an iterative optimization for θ
several samples will be taken over time.
5For a (softmax) classification network, fω (x) is a vector with fω (x, y) = fω (x)(y), for regression net-
works with i.i.d. Gaussian noise we have fω (x, y) = N (fω (x), τ-1I).
6It further learns an affine transformation for each unit using parameters γ and β, which we omit in favor
of brevity: X(fine = Yj)Xj) + β(j).
4
Under review as a conference paper at ICLR 2018
Loss Function and Optimization Training deep networks with mini-batch optimization involves
a (regularized) risk minimization with the following form:
1M
LRR(ω) := ME l(yi, yi) + ω3
i=1
Where the first term is the empirical loss on the training data and the second term is a regularization
penalty acting as a prior on model parameters ω. If the loss l is cross-entropy for classification or
sum-of-squares for regression problems (assuming i.i.d. Gaussian noise on labels), the first term is
equivalent to minimizing the negative log-likelihood:
LRR(ω) := -ɪ Xlnfω(Xi, yi) +Ω(ω).
Mτ
i=1
with τ = 1 for classification. In a batch normalized network the model parameters are
{W1:L, Y1:L, β1L, μBL, σBL}. If We decouple the learnable parameters θ = {W1:L, γ1'∙L, β1L}
from the stochastic parameters ω = {μBL, σBL}, we get the following objective at each step of the
mini-batch optimization of a batch normalized netWork:
LRR(O) = -MT X ln f{θ,ωi}(xi, yi) + ω⑻	⑵
τ i=1
where ωi is the mean and variances for sample i's mini-batch at a certain training step. Note that
while ωi formally needs to be i.i.d. for each training example, a batch normalized network samples
the stochastic parameters once per training step (mini-batch). For a large number of epochs, how-
ever, the distribution of sampled batch members for a given training example converges to the i.i.d.
case.
Comparing Eq. (1) and Eq. (2) reveals that the optimization objectives are identical, if there ex-
ists a prior p(ω) corresponding to Ω(θ) such that aKL(qθ(ω)∣∣p(ω)) = Nτ舄Ω(θ). Inabatch
normalized network, qθ (ω) corresponds to the joint distribution of the normalization parameters
μBL,σBL, as implied by the repeated sampling from D during training. This is an approximation
of the true posterior, where we have restricted the posterior to lie within the domain of our paramet-
ric network and source of randomness. With that we can use a pre-trained batch normalized network
to estimate the uncertainty of its prediction using the inherent stochasticity of BN. Before that, we
briefly discuss what Bayesian prior is induced in a typical batch normalized network.
3.3	PRIOR p(ω)
The purpose of Ω(θ) is to reduce variance in deep networks. L2-regularization, also referred to as
weight decay (Ω(θ) = λ Pl=LL ∣∣W11∣2), is apopular technique in deep learning. The inducedprior
from L2-regularization is studied in Appendix 6.5. Under some approximations as outlined in the
Appendix, we find that BN for a deep network with FC layers and ReLU activations induce Gaussian
distributions over BN unit’s means and standard deviations, centered around the population values
given by D (Eq. (6), details in Appendix 6.3). Factorizing this distribution across all stochastic
parameters and assuming Gaussian priors, we find the approximate corresponding priors:
p(μB)=N(0,j⅛)
p(σB) = N (μp,σp)
where Jl-1 is the dimensionality of the layer’s inputs andxis the average input over D for all input
units. In the absence of scale and shift transformations from the previous BN layer, it converges
towards an exact prior for large training datasets and deep networks (under the assumptions of the
factorized distribution). The mean and variance for the BN unit,s standard deviation, μ? and σp,
have no relevance for the reconciliation of the optimization objectives of Eq. (1) and (2).
5
Under review as a conference paper at ICLR 2018
3.4	Predictive Uncertainty in Batch Normalized Deep Nets
In the absence of the true posterior we rely on the approximate posterior to express an approximate
predictive distribution:
P*(y∣x,D) := / fω (x, y)qθ (ω)dω
Following Gal & Ghahramani (2015) we estimate the first and second moment of the predictive
distribution empirically (see Appendix 6.4 for details). For regression, the first two moments are:
1T
Ep* [y] ≈ TEfω>i(X)
i=1
1T
Covp* [y] ≈ TTI + TEfω"x)lfω"x) - Ep* [y]lEp* [y]
T i=1
where each ωi corresponds to sampling the net's stochastic parameters ω = {μBL, σ1BL} the same
way as during training. Sampling ωi therefore involves sampling a batch B from the training set
and updating the parameters in the BN units, just as ifwe were taking a training step with B. Recall
that from a VA perspective, training the network amounted to minimizing KL(qθ(ω)∣∣p(ω∣D)) wrt
θ. Sampling ωi from the training set, and keeping the size of B consistent with the mini-batch
size used during training, ensures that qθ (ω) during inference remains identical to the approximate
posterior optimized during training.
After each update of the net’s stochastic parameters, we take a forward pass with input x, producing
output fωi (x). After T such stochastic forward passes, We compute the mean and sample variance
of outputs to find the mean Ep* [y] and variance Cov?* [y] of the approximate predictive distribution.
Note that Cov?* [y] also requires addition of constant variance from observation noise, T-1I.
The network is trained just as a regular BN network. The difference is in using the trained network
for prediction. Instead of replacing ω = {μBL, σBL} with population values from D, We update
these parameters stochastically, once for each forward pass .7
The form of p* can be approximated by a Gassuian for each output dimension (for regression).
We assume bounded domains for each input dimension, wide layers throughout the network, and a
unimodal distribution of weights centered at 0. By the Liapounov CLT condition, the first layer then
receives approximately Gaussian inputs (a proof can be found in Lehmann (1999)). Having sampled
μB and σB from a mini-batch, each BN unit,s output is bounded. CLT thereby continues to hold for
deeper layers, including fω (x) = WLxL. A similar motivation for a Gaussian approximation of
Dropout has been presented by Wang & Manning (2013).
The actual form of p* is likely to be highly multimodal, as can be seen immediately from fω (x)=
WLxL with elements in xL normalized, scaled and shifted differently. Gal & Ghahramani (2015)
note the multimodality as well, since MCDO implies a bimodal variational distribution over each
weight matrix column.
4	Experiments and Results
We assess the uncertainty quality of MCBN quantitatively and qualitatively. Our quantitative anal-
ysis relies on eight standard regression datasets, listed in Table 1. Publicly available from the
UCI Machine Learning Repository (University of California, 2017) and Delve (Ghahramani, 1996),
these datasets have been used to benchmark comparative models in recent related literature (see
Hernandez-Lobato & Adams (2015), Gal & Ghahramani (2015), Bui et al. (2016) and Li & Gal
(2017)). We report results using standard metrics, and also propose useful upper and lower bounds
to normalize these metrics for a more meaningful interpretation in Section 4.2.
7As an alternative to using the training set D to sample ω^i, We could sample from the implied qθ (ω) as
modeled in Appendix 6.3. This would alleviate having to store D for use during prediction. In our experiments
We used D to sample ω^i however, and leave the evaluation of the modeled q§ (ω) for future research.
6
Under review as a conference paper at ICLR 2018
Dataset name	N	Q	Target Feature
Boston Housing	506	13	
Concrete Compressive Strength	1,030	8	
Energy Efficiency	768	8	Heating Load
Kinematics 8nm	8,192	8	
Power Plant	9,568	4	
Protein Tertiary Structure	45,730	9	
Wine Quality (Red)	1,599	11	
Yacht Hydrodynamics	308	6	
Table 1: Properties of the eight regression datasets used to evaluate MCBN. N is the dataset size
and Q is the n.o. input features. Only one target feature was used. In cases where the raw datasets
contain more than one target feature, the feature used is specified by target feature.
Our qualitative results consist of three parts. First, in Figure 1 we demonstrate that MCBN produces
reasonable uncertainty bounds on a toy dataset in the style of (Karpathy, 2015). Second, we develop
a new visualization of uncertainty quality by plotting test errors sorted by predicted variance in
Figure 2. Finally, we apply MCBN to SegNet (Kendall et al., 2015), demonstrating the benefits of
MCBN in an existing batch normalized network.
4.1	Metrics
We evaluate uncertainty quality based on two metrics, described below: Predictive Log Likelihood
(PLL) and Continuous Ranked Probability Score (CRPS). We also propose upper and lower bounds
for these metrics which can be used to normalize them and provide a more meaningful interpretation.
Predictive Log Likelihood (PLL) Predictive Log Likelihood is a widely accepted metric for un-
certainty quality, used as the main uncertainty quality metric for regression (e.g. (Hernandez-Lobato
& Adams, 2015), (Gal & Ghahramani, 2015), (Bui et al., 2016) and (Li & Gal, 2017)). A key prop-
erty is that PLL makes no assumtions about the form of the distribution. The measure is defined for
a probabilistic model fω (x) and a single observation (yi, xi) as:
PLL(fω (x), (yi, Xi))= log p(yi∣fω (Xi))
where p(y∕fω(Xi)) is the model's predicted PDF evaluated at y given the input Xi. A more
detailed description is given in Appendix 6.4. The metric is unbounded and maximized by a perfect
prediction (mode at yi) with no variance. As the predictive mode moves away from yi , increasing
the variance tends to increase PLL (by maximizing probability mass at yi). While PLL is an elegant
measure, it has been criticized for allowing outliers to have an overly negative effect on the score
(Selten, 1998).
Continuous Ranked Probability Score (CRPS) Continuous Ranked Probability Score is a less
sensitive measure that takes the full predicted PDF into account. A prediction with low variance
that is slightly offset from the true observation will receive a higher score form CRPS than PLL.
In order for CRPS to be analytically tractable, we need to assume a Gaussian unimodal predictive
distribution. CRPS is defined as
CRPS(fω(xi), (yi,xi))
Z∞
∞
(F(y) - i(y ≥ yi))2dy
where F(y) is the predictive CDF, and l(y ≥ yi) = 1 if y ≥ yi and 0 otherwise (for univariate dis-
tributions) (Gneiting & Raftery, 2007). CRPS is interpreted as the sum of the squared area between
the CDF and 0 where y < yi and between the CDF and 1 where y ≥ yi . A perfect prediction with
no variance yields a CRPS of 0; for all other cases the value is larger. CRPS has no upper bound.
4.2	Benchmark models and normalized metrics
In order to establish a lower bound on useful performance for uncertainty estimates, we define a
baseline that predicts constant variance regardless of input. This benchmark model produces iden-
tical point estimates as MCBN, which yield the same predictive means. The variance is set to a
fixed value that optimizes CRPS on validation data. This model reflects our best guess of constant
7
Under review as a conference paper at ICLR 2018
variance on test data - any improvement in uncertainty quality from MCBN would indicate a sen-
sible estimate of uncertainty. We call this model Constant Uncertainty BN (CUBN). Implementing
MCDO as a comparative model, we similarly define a baseline for dropout, Constant Uncertainty
Dropout (CUDO). The difference in variance modeling between MCBN, CUBN, MCDO and CUDO
are visualized in plots of uncertainty bounds on toy data in Figure 1.
For a probabilistic model f, an upper bound on uncertainty performance can also be defined for
CRPS and PLL. For each observation (yi, xi), a value for the predictive variance Ti can be cho-
sen that maximizes PLL or minimizes CRPS8. Using CUBN as a lower bound and the optimized
CRPS score as the upper bound, uncertainty estimates can be normalized between these bounds (1
indicating optimal performance, and 0 indicating performance on par with fixed uncertainty). We
call this normalized measure CRPS =	∙ CRPSfWi,Xi))-CRPSfCU,(yi,xi))、、× 100, and the PLL
minT CRPS(f,(yi,xi))-CRPS(fCU,(yi,xi))	,
analogue PLL = -PLLf, (yi ,,xi)) -PLLf CU, (yi ,,xi))、、X 100. This normalized measure gives an intu-
maxT PLL(f,(yi,xi))-PLL(fCU,(yi,xi))
itive understanding of how close a Bayesian model is to estimating the perfect uncertainty for each
prediction.
We also evaluate CRPS and PLL for an adaptation of the authors, implementation of Multiplicative
Normalizing Flows (MNF) for variational Bayesian networks (Louizos & Welling, 2017). This is
a recent model specialized to allow a more flexible posterior what is achievable by e.g. MCDO’s
bimodal variational over weight columns. MNF uses auxillary variables on which the posterior is
a latent. By applying normalizing flows to the auxillary variable such that it can take on complex
distributions, the approximate posterior becomes highly flexible.
4.3	test setup
Our evaluation of MCBN and MCDO is largely comparable to that of Hernandez-Lobato & Adams
(2015), in that we use similar datasets and metrics. This setup was later also followed by Gal
& Ghahramani (2015), where we in comparison implement a different hyperparameter selection,
allow for a larger range of dropout rates, and use larger networks with two hidden layers.
With the exception of Protein Tertiary Structure9, all our models share a similar architecture: two
hidden layers with 50 units each, using ReLU activations. Input and output data were normal-
ized during training. Results were averaged over five random splits of 20% test and 80% training
and cross-validation (CV) data. For each split, 5-fold CV by grid search with a RMSE minimiza-
tion objective was used to find training hyperparameters and optimal n.o. epochs. For BN-based
models, the hyperparameter grid consisted of a weight decay factor ranging from 0.1 to 1-15 by
a log 10 scale, and a batch size range from 32 to 1024 by a log 2 scale. For DO-based models,
the hyperparameter grid consisted of the same weight decay range, and dropout probabilities in
{0.2, 0.1, 0.05, 0.01, 0.005, 0.001}. DO-based models used a batch size of 32 in all evaluations.
The model with optimal training hyperparameters was used to optimize τ numerically. This opti-
mization was made in terms of average CV CRPS for MCBN, CUBN, MCDO, and CUDO respec-
tively, before evaluation on the test data.
All estimates for the predictive distribution were obtained by taking 500 stochastic forward passes
through the network, throughout training and testing. The implementation was done with Tensor-
Flow. The Adam optimizer was used to train all networks, with a learning rate of 0.001. The exten-
sive part of the experiments (i.e. training and cross validation) was done on Amazon web services
using 3000 machine-hours. All code necessary for reproducing both the quantitative and qualitative
results is released in an anonymous github repository (https://github.com/iclr-mcbn/mcbn).
4.4	test results
A summary of the results measuring uncertainty quality of MCBN, MCDO and MNF are provided
in Table 2. Tests are run over eight datasets using 5 random 80-20 splits of the data with 5 different
random seeds each split. We report CRPS and PLL, expressed as a percentage, which reflects how
close the model is to the upper bound. The upper bounds and lower bounds for each metric are de-
8Ti can be found analytically for PLL, but must be found numerically for CRPS.
9Where we used 100 units per hidden layer and 2-fold CV.
8
Under review as a conference paper at ICLR 2018
Dataset	MCBN		CRPS MCDO		MNF		MCBN		PLL MCDO		MNF	
Boston	8.50	****	3.06	****	8.30	****	10.49	****	5.51	****	3.58	***
Concrete	3.91	****	0.93	*	6.05	****	-36.36	**	10.92	****	9.71	****
Energy	5.75	****	1.37	ns	3.45	ns	10.89	****	-14.28	*	2.62	ns
Kin8nm	2.85	****	1.82	****	1.01	*	1.68	***	-0.26	ns	-0.44	ns
Power	0.24	***	-0.44	****	-0.83	***	0.33	**	3.52	****	-1.38	****
Protein	2.66	****	0.99	****	TBU		2.56	****	6.23	****	TBU	
Wine (Red)	0.26	**	2.00	****	TBU		0.19	*	2.91	****	TBU	
Yacht	-56.39	***	21.42	****	-54.18	****	45.58	****	-41.54	ns	71.18	****
Table 2: Uncertainty quality measured on eight datasets. MCBN, MCDO and MNF are compared
over 5 random 80-20 splits of the data with 5 different random seeds each split. Reported values
are uncertainty metrics CRPS and PLL normalized to a lower bound of constant variance and upper
bound that maximizes the metric. CRPS and PLL are expressed as a percentage, reflecting how
close the model is to the upper bound. We check to see if CRPS and PLL significantly exceed the
baseline using a one sample t-test (significance level indicated by *’s). Best performer versus their
baseline for each dataset and metric is marked by bold. See text for further details.

」。」」8 UO-c-po-ld
Yacht
MCBN
95% CI
50% CI
12∙
Yacht
4.5	∙ MCDO
4-
3.5-	∙
3-
95% CI
50% CI
--∖∕l∕τ
—run mean
2.5-
2-
1.5-
1 -
0.5
0.3-
10卜
8-
6-
4
2
12-
0.3-
10-
8-
6-
4 ∙
2
Kinematics
MCBN
Boston Housing
MCDO
Boston Housing
MCBN
95% CI
50% CI
--∖∕l∕τ
95% CI
50% CI
--
95% CI
50% CI
--y∕l∕τ
UOυ一 po」d
95% CI
50% CI
--
Kinematics
MCDO
2
0 0.2-



Figure 2: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The
shaded areas show MCBN’s (blue) and MCDO’s (red) model uncertainty (light area 95% CI, dark
area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a
running mean of the errors. The dashed line indicates the optimized constant uncertainty. A corre-
lation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty
estimates are meaningful for estimating errors. See Appendix for complete results.
scribed in Section 4.2. We check to see if the reported values of CRPS and PLL significantly exceed
the lower bound models (CUBN and CUDO) using a one sample t-test, where the significance level
is indicated by *’s. Further details from the experiment are available in Appendix 6.6.
In Figure 2, we provide a novel visualization of uncertainty quality visualization in regression
datasets. Errors in the model predictions are sorted by estimated uncertainty. The shaded areas
show the model uncertainty and gray dots show absolute prediction errors on the test set. A gray
line depicts a running mean of the errors. The dashed line indicates the optimized constant uncer-
tainty. In these plots, we can see a correlation between estimated uncertainty (shaded area) and mean
error (gray). This trend indicates that the model uncertainty estimates can recognize samples with
larger (or smaller) potential for predictive errors.
Qualitative results for Bayesian SegNet using MCBN was produced by using the main CamVid
model in Kendall et al. (2015). The pre-trained model was obtained from the online model zoo
and was used without modification. 10 instances of mini-batches with size 6 were used to estimate
the mean and variance of MCBN. Qualitative results can be found in Figure 3 depicting intuitive
9
Under review as a conference paper at ICLR 2018
Figure 3: Results applying MCBN to Bayesian SegNet (Kendall et al., 2015). In the upper left, a
scene from the CamVid driving scenes dataset. In the upper right, the Bayesian estimated segmen-
tation. In the lower left, estimated uncertainty using MCBN for the car class. In the lower right, the
estimated uncertainty of MCBN for all 11 classes.
uncertainty at object boundaries. Quantitative measures on various segmentation datasets can be
obtained and is beyond the scope of this work.
We provide additional experimental results in Appendix 6.6. In Tables 3 and 4, We show the mean
CRPS and PLL values for MCBN and MCDO. These results indicate that MCBN performs on
par with MCDO across several datasets. In Table 6 we provide RMSE results of the MCBN and
MCDO networks in comparison with non-stochastic BN and DO networks. These results indicate
that the procedure of multiple forward passes in MCBN and MCDO show slight improvements in
the predictive accuracy of the network.
5	Discussion
The results presented in Table 2 and Appendix 6.6 indicate that MCBN generates meaningful uncer-
tainty estimates which correlate with actual errors in the model,s prediction. We show statistically
Significant improvements over CUBN in the majority of the datasets, both in terms of CRPS and
PLL. The visualizations in Figure 2 and in Appendix 6.6 show clear correlations between the esti-
mated model uncertainty and actual errors produced by the network. We perform the same experi-
ments using MCDO, and find that MCBN generally performs on par with MCDO. Looking closer,
in terms of CRPS, MCBN performs better than MCDO in more cases than not. However, care must
be used when comparing different models. The learned network parameters are different, leading to
different predictive means which can confound direct comparison.
The results on the Yacht Hydrodynamics dataset seem contradictory. The CRPS score for MCBN
is extremely negative, while the PLL score is extremely positive. The opposite trend is observed
for MCDO. To add to the puzzle, the visualization in Figure 2 depicts an extremely promising
uncertainty estimation that models the predictive errors with high fidelity. We hypothesize that this
strange behavior is due to the small size of the data set, which only contains 60 test samples, or due
to the Gaussian assumption of CRPS. There is also a large variability in the model’s accuracy on
this dataset, which further confounds the measurements for such limited data.
One might criticize the overall quality of the uncertainty estimates of MCBN and MCDO based on
the magnitude of the CRPS and PLL scores in Table 2. The scores rarely exceed 10% improvement
over the lower bound. However, we caution that these measures should be taken in context. The
upper bound is very difficult to achieve in practice (it is optimized for each test sample individually),
10
Under review as a conference paper at ICLR 2018
and the lower bound is a quite reasonable estimate for uncertainty. We have further compared against
the recent work of Louizos & Welling (2017), and find comparable results to their MNF-based
variational technique specifically targeted to increase the flexibility of the approximate posterior.
Our approximation of the implied prior in Appendix 6.5 also provides a new interpretation of the
empirical evidence that significantly lower λ should be used in batch normalized networks (Ioffe &
Szegedy, 2015). From a VA perspective, too strong a regularization for a given dataset size could be
seen as constraining the prior distribution ofBN units’ means, effectively narrowing the approximate
posterior.
In this work, we have shown that training a deep network using batch normalization is equivalent to
approximate inference in Bayesian models. Using our approach, it is possible to make meaningful
uncertainty estimates using conventional architectures without modifying the network or the training
procedure. We show evidence that the uncertainty estimates from MCBN correlate with actual
errors in the model’s prediction, and are useful for practical tasks such as regression or semantic
image segmentation. Our experiments show that MCBN yields an improvement over the baseline of
optimized constant uncertainty on par with MCDO and MNF. Finally, we make contributions to the
evaluation of uncertainty quality by suggesting new evaluation metrics based on useful baselines and
upper bounds, and proposing a new visualization tool which gives an intuitive visual explanation of
uncertainty quality. Finally, it should be noted that, over the past few years, batch normalization has
become an integral part of most-if-not-all cutting edge deep networks which signifies the relevance
of our work for estimating model uncertainty.
References
Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-
decoder architecture for image segmentation. arXiv preprint arXiv:1511.00561, 2015.
Thang D. Bui, Daniel Hernandez-Lobato, Yingzhen Li, Jose MigUel Hernandez-Lobato, and
Richard E. Turner. Deep Gaussian Processes for Regression using Approximate Expectation
Propagation. In ICML, 2016.
Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monoc-
ular 3d object detection for autonomous driving. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2147-2156, 2016.
Ugljesa Djuric, Gelareh Zadeh, Kenneth Aldape, and Phedias Diamandis. Precision histology: how
deep learning is poised to revitalize histomorphology for personalized cancer care. npj Precision
Oncology, 1(1):22, 2017.
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin Ko, Susan M. Swetter, Helen M. Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
Nature, Feb 2017.
Yarin Gal. Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation : Representing Model
Uncertainty in Deep Learning. ICML, 48:1-10, 2015.
Zoubin Ghahramani. Delve Datasets. University of Toronto, 1996. URL http://www.cs.
toronto.edu∕{~}delve∕data∕kin∕desc.html.
Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):
452-459, May 2015.
Tilmann Gneiting and Adrian E Raftery. Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association, 102(477):359-378, 2007.
Alex Graves. Practical Variational Inference for Neural Networks. NIPS, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
11
Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jose MigUel Hemandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learn-
ing of bayesian neural networks. In International Conference on Machine Learning, pp. 1861-
1869, 2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neUral networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13. ACM, 1993.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training
by RedUcing Internal Covariate Shift. Arxiv, 2015. URL http://arxiv.org/abs/1502.
03167.
Andrej Karpathy. Convnetjs demo: toy 1d regression, 2015. URL http://cs.stanford.
edu/people/karpathy/convnetjs/demo/regression.html.
Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla. Bayesian SegNet: Model Uncertainty in
Deep ConvolUtional Encoder-Decoder ArchitectUres for Scene Understanding. CoRR, abs/1511.0,
2015. URL http://arxiv.org/abs/1511.02680.
Diederik P Kingma and Max Welling. AUto-Encoding Variational Bayes. In ICLR, 2014.
David KrUeger, Chin-Wei HUang, Riashat Islam, Ryan TUrner, Alexandre Lacoste, and Aaron
CoUrville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.
Erich Leo Lehmann. Elements of Large-Sample Theory. Springer Verlag, New York, 1999. ISBN
0387985956.
Yingzhen Li and Yarin Gal. DropoUt Inference in Bayesian NeUral Networks with Alpha-
divergences. arXiv, 2017.
Xiao LiU, Tian Xia, Jiang Wang, Yi Yang, Feng ZhoU, and YUanqing Lin. FUlly convolUtional
attention networks for fine-grained recognition. arXiv preprint arXiv:1603.06765, 2016.
Christos LoUizos and Max Welling. MUltiplicative normalizing flows for variational Bayesian neUral
networks. In Doina PrecUp and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volUme 70 of Proceedings of Machine Learning Research, pp.
2218-2227, International Convention Centre, Sydney, AUstralia, 06-11 AUg 2017. PMLR. URL
http://proceedings.mlr.press/v70/louizos17a.html.
David JC MacKay. A practical bayesian framework for backpropagation networks. Neural compu-
tation, 4(3):448-472, 1992.
Radford M Neal. BAYESIAN LEARNING FOR NEURAL NETWORKS. PhD thesis, University of
Toronto, 1995.
Radford M Neal. Bayesian learning for neural networks, volUme 118. Springer Science & BUsiness
Media, 2012.
Reinhard Selten. Axiomatic characterization of the qUadratic scoring rUle. Experimental Economics,
1(1):43-62, 1998.
Li Shen. End-to-end training for whole image breast cancer diagnosis Using an all convolUtional
design. arXiv preprint arXiv:1708.09427, 2017.
David Silver, JUlian Schrittwieser, Karen Simonyan, Ioannis AntonogloU, Aja HUang, ArthUr GUez,
Thomas HUbert, LUcas Baker, Matthew Lai, Adrian Bolton, YUtian Chen, Timothy Lillicrap, Fan
HUi, LaUrent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering
the game of go withoUt hUman knowledge. Nature, 550(7676):354-359, Oct 2017.
12
Under review as a conference paper at ICLR 2018
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Irvine University of California. UC Irvine Machine Learning Repository, 2017. URL https:
//archive.ics.uci.edu/ml/index.html.
Sida I Wang and Christopher D Manning. Fast dropout training. Proceedings of the
30th International Conference on Machine Learning, 28:118-126, 2013. URL http://
machinelearning.wustl.edu/mlpapers/papers/wang13a.
13
Under review as a conference paper at ICLR 2018
6 Appendix
6.1 Variational Approximation
Assume we were to come up with a faimly of distributions parametrised by θ in order to approximate
the posterior, qθ (ω). Our goal is to set θ such that qθ (ω) is as similar to p(ω∣D) as possible.
One strategy is to minimizing KL(qθ(ω)∣∣p(ω∣D)), the KL divergence of p(ω∣D) wrt q®(ω). Min-
imizing KL(qθ(ω)∣∣p(ω∣D)) is equivalent to maximizing the ELBO:
I
ω
qθ(ω) lnp(Y∣X, ω)dω - KL(q®(ω)∣∣p(ω))
Assuming i.i.d. observation noise, this is equivalent to minimizing:
/ qθ (ω)ln p(y∕fω (xi))dω + KL(q® (ω)∣∣p(ω))
N
LVA(θ) := -X
n=1
Instead of making the optimization on the full training set, we can use a subsampling (yielding an
unbiased estimate of LVA(θ)) for iterative optimization (as in mini-batch optimization):
N
LVA(θ):= -MEJ qθ(ω)lnp(yi∣fω(xi))dω + KL(q®(ω)∣∣p(ω))
We now make a reparametrisation: set ω = g(θ, ) where is a RV. The function g and the distri-
bution of E must be such that p(g(θ, E)) = q® (ω). Assume q® (ω) can be written fe q®(ω∣e)p(e)de
where q®(ω∣e) = δ(ω - g(θ, e)). Using this reparametrisation We get:
LVA(θ)
-M X∕p(e)lnP(yi|fg(e,e)(xi))dE
+ KL(q® (ω)∣∣p(ω))
6.2 KL Divergence of factorized Gaussians
If q®(ω) andp(ω) factorize over all stochastic parameters:
KL(q® (ω)∣∣p(ω))
-/Y[qe(ωi)] ln Q^iLdω
Jω?	∏i q® (ωi)
-/ Y[q®M)]X[ln S一
7ω ɪɪ	V L q®(“)」
dωi
i
[q® MM ln qpj Y dωii
p(ω )
q® (ωj )ln -d—ʌ dωj	Y q® (ωi )dωi]
q® 3)	Jωz=j i=j	」
X - / q®(ωi)ln p(ωi) dωi
V 儿	q® (ωi)
(3)
EKL(q® (ωi)Hp(ωi))
i
such that KL(q®(ω)∣∣p(ω)) is the sum of the KL divergence terms for the individual stochastic
parameters ωi. If the factorized distributions are Gaussians, where q® (ω∕ = N (μq ,σj) and p(ωi)=
14
Under review as a conference paper at ICLR 2018
N(μp,σp) We get:
KL(qθ M)Mg))= ∕iqθ M)In M dωi
-	H(qθ(ωi)) -	qθ(ωi) lnp(ωi)dωi
ωi
-	2(I + ln(2πσq))
-	Z qθ(ωi)ln 72—2μ∕2 exp n - i i 2 μp^
ωi	(2πσp )	2σp
-	2(1+in(2πσq))
(4)
+ 2 ln(2πσp) +
Eq [ω2 ] - 2μpEq 心]+ μp
ln σP + 肃 + 兀 ”)
σq	2σP
2σp2
1
——
2
for each KL divergence term. Here H(qθ(ω∕) = 2(1 + ln(2πσj)) is the differential entropy of
qθ(ωi).
6.3	Distribution of μB, σB
Here We approximate the distribution of mean and standard deviation of a mini-batch, separately to
tWo Gaussians. For the mean We get:
μB
ΣmM=1W(j)xm
M
Where xm are the examples in the sampled batch. We Will assume these are sampled i.i.d.10. Samples
of the random variable W(j)xm are then i.i.d.. Then by central limit theorem (CLT) the folloWing
holds for sufficiently large M (often ≥ 30):
For standard deviation:
σB
∕∑M=l(Wj)Xm -μB)2
M	M
Then
√M(σB - σ) = √M (渭=(WMm-逝
—
We Want to reWrite
∑M=ι(Wj)Xm-μB)2
.We take a Taylor expansion of f (x) = √X around a = σ2.
With x
∑M=l(Wj)χm-μB)2.
M
M
√x = √σ2 + zjɜ(X - σ2) + O[(x — σ2)2]
10Although in practice With deep learning, mini-batches are sampled Without replacement, stochastic gradient
descent samples With replacement in its standard form.
15
Under review as a conference paper at ICLR 2018
so
√M (σB — σ) = √M
∑M=ι(Wj)χm - mb)2
1
2σ√M
O
O
M
∑M=1(Wj)χm - 〃B )
M
-σ2 ) +
T〔)
(Wj)Xm - 〃B)2 - σ2) +
∑M=1(W⑶Xm - 〃b)2 _ 2λ2
M	σ )
C)Xm - mb)2 - Mσ2 ) +
∑M=1(W⑶Xm - 〃b)2 _ 2、2
M	(J)
consider ΣM=1(WS)Xm - μB)2. We know that E[W(j)xm] = μ and write
∑M=1(Wj)χm - μB)2
= ∑M=1((W⑶Xm - μ)-(μB- μ))2
= ∑M=1((W⑶Xm - μ)2 + (μB - μ)2 - 2(W(j)Xm - μ)(μB - μ))
= ∑M=1(W⑶Xm - μ)2 + M(μB - μ)2 - 2(μB - μ)∑M=1 (W(j)Xm
= ∑M=1(W⑶Xm - μ)2 - M(μB - μ)2
= ∑M=1((W⑶Xm - μ)2-(μB- μ)2)
μ)
16
Under review as a conference paper at ICLR 2018
then
√M (TB- σ) = -√= (∑M=ι((Wj)Xm - μ)2 - (μB - μ)2) - Mt2、+
2T M
∑	∑M=l(Wj)Xm-μB)2	2、2一
O ------------M----------σ)
-⅛ (∑M=ι(Wj)Xm - μ)2 - ∑M=ι(μB - μ)2 - Mt2、+
2σ M
C	∑M=l(Wj)Xm - 〃B)2	2丫
O ------------M-----------(T)
-⅛ (∑M=1 ((Wj)Xm - μ)2 - σ2) - ∑M=ι(μB - μ)2) +
2σ M
C	∑M=l(Wj)Xm - 〃B)2	2丫
O ------------M-----------(T)
1
2σ√M
1
∑M=l((W(j)Xm - μ)2 - σ2)
2σ√M
ςM=I(MB - μ)2
—
+O
√M ( *M=I(W(j)xm - μB)2 - σ2)2
—√=∑M=ι((W(j)Xm - μ)2-σ2)
2T M
'--------------{----------------}
term A
-P…2
2σ
|
___ -	J
^z'-/^^^^^^^
term B
∑M=l(W(j)χm - μB)2 _ 2、2
M	σ )
+O
|
^^{^^™
term C
}
We go through each term in turn
Term A
Term A = ^^∑M=ι((Wj)Xm - μ)2 - σ2)
2T M
where ∑M=ι (Wj)Xm - μ)2 is the sum of M RVs (Wj)Xm - μ)2. Note that since E[W j)Xm] = μ
it holds that E[(W j)Xm - μ)2] = σ2. Since (Wj)Xm - μ)2 is sampled approximately iid (by
assumptions above), for large enough M by CLT it holds approximately that
∑M=i(Wj)Xm - μ)2 〜N(Mσ2, MVar((Wj)Xm - μ)2))
where
Var((Wj)Xm - μ)2) = E[(Wj)Xm - μ)2*2] - E[(Wj)Xm - μ)2]2
=E[(Wj)Xm - μ)4] - σ4
Then
∑M=1((Wj)Xm - μ)2 - σ2)〜N(0, M * E[(Wj)Xm - μ)4] - Mσ4)
so
TermA 〜N(0, E[(Wj)Xm - 〃)4] - σ4)
4T2
17
Under review as a conference paper at ICLR 2018
Term B
We have
TermB = 2σ (mb - μ)2 = 2σjy^M(μB - μ)(μ^B - μ)
Consider (μB - μ). As μB → μ when M → ∞ We have μB - μ → 0. We also have
√M(μB - μ) = "M=IWj)Xm - √Mμ
√M
which by CLT is approximately Gaussian for large M. We can then make use of the Cramer-Slutzky
Theorem, which states that if (Xn)n≥1 and (Yn)n≥1 are two sequences such that Xn -→d X and
Yn → a as n → ∞ where a is a constant, then as n → ∞, it holds that Xn * Yn → X * a. Thus,
Term B is approximately 0 for large M.
Term C
We have
Term C = O
√M ( ςM=I(Wj)Xm - μB)2
∖	M
Since E[(W(j)xm - μ)2] = σ2 we can make the same use of Cramer-Slutzky as for Term B, such
that Term C is approximately 0 for large M.
Finalizing the distribution
We have approximately
σB 〜N(σ,
E[(W(j)Xm - μ)4] - σ4 )
4σ2
so
E[(W(j)Xm - μ)4] - σ4
4σ2M	)
6.4	predictive distribution properties
This section provides derivations of properties of the predictive distribution p*(y∣x, D) in section
3.4, following Gal (2016). We first find the approximate predictive mean and variance for the ap-
proximate predictive distribution, then show how to estimate the predictive log likelihood, a measure
of uncertainty quality used in the evaluation 4.
Predictive mean Assuming Gaussian iid noise defined by model precision τ, i.e. fω (X, y)
p(y∣fω (x)) = N(y; fω (χ),τ-1i):
Ep* [y] = / yp*(y∣χ,D)dy
yy ω
ZyyZω
fω (X, y)qθ (ω)dω dy
N(y; fω(X), τ -1 I)qθ (ω)dω dy
=	yN (y; fω(X), τ -1 I)dyqθ (ω)dω
=	fω (X)qθ(ω)dω
ω
1T
≈ TEfωi(x)
i=1
where we take the MC Integral with T samples of ω for the approximation in the final step.
18
Under review as a conference paper at ICLR 2018
Predictive variance Our goal is to estimate:
Covp* [y] = Ep* [y|y] - Ep* [y]lEp* [y]
We find that:
Ep* [y|y] = y ylyp*(y∣χ,D)dy
y
=	y|y	fω (x, y)qθ(ω)dωdy
=/ (/ ylyfω (χ, y)dy)qe (ω)dω
+ fω(x)lfω(x) qθ(ω)dω
fω (χ,y)(y) + Efω (x,y) [y]∣Efω (x,y) [y]) qθ (ω)dω
=TTI + Eqθ(ω)[fω (X)Tfω(X)]
1T
≈ T 1I + TEfω"X)Tfω>i(X)
i=1
where we use MC integration with T samples for the final step. The predictive covariance matrix is
given by:
1T
Covp* [y] ≈ TTI + TEfω>i(X)Tfωi(X)- Ep* [y]τEp* [y]
T i=1
which is the sum of the variance from observation noise and the sample covariance from T stochastic
forward passes though the network.
Predictive Log Likelihood We use the Predictive Log Likelihood (PLL) as a measure to estimate
the model’s uncertainty quality. For a certain test point (yi, Xi), the PLL definition and approxima-
tion can be expressed as:
PLL(fω(x), (yi, Xi)) = logp(yi∣fω(Xi))
=log / fω(Xi, yi)p(ω∣D)dω
≈ log	fω (Xi, yi)qθ (ω)dω
T
≈ log £p(yi∣fωj (Xi))
j=1
where ωj represents a sampled set of stochastic parameters from the approximate posterior distrub-
tion qθ(ω) and we take a MC integration with T samples. For regression, due to the iid Gaussian
noise, we can further develop the derivation into the form we use when sampling:
T
PLL(fω(x), (yi, Xi)) = log XN(yi∣fω。(Xi),τ-1I)
i=1
=logsumeχPj=ι,..,τ( - 1 ||yi - fωj (Xi)||2)
+ log T - 1 log 2∏ + 2 log τ
Note that PLL makes no assumPtion on the form of the aPProχimate Predictive distribution. The
measure is based on repeated sampling ωj from q® (ω), which may be highly multimodal (see section
3.4).
19
Under review as a conference paper at ICLR 2018
6.5	Prior
We assume training by SGD with mini-batch size M, L2-regularization on weights and Fully Con-
nected layers. With θk ∈ θ, equivalence between the objectives of Eq. (1) and (2) then requires:
∂∂
--KL(qθ (5||P(M) = NT
∂θk	∂θk
∂L
=Nτ∂θk X λιιwlll2
k l=1
(5)
To proceed with the LHS of Eq. (5) we first need to find the approximate posterior qθ (ω) that
batch normalization induces. As shown in Appendix 6.3, with some weak assumptions and approx-
imations the Central Limit Theorem (CLT) yields Gaussian distributions of the stochastic variables
μB,σB, for large enough M:
σ
u
B
μB «N(μu,
SSN (σu,
E[(hu - μu)4] - (σu)4
4(σu)2 M	)
(6)
where μu and σu are the population-level moments (i.e. moments over D), and hu is the BN unit's
input. We use i as an index of the set of stochastic variables, i.e. ωi ∈ {μBL, σBL}, and denote by
ωi the stochastic variables in a certain layer, ωi ∈ {μB, σg}. We assume qθ(ω) andp(ω) factorize
over all individual ωi, i.e. independence between all stochastic variables.11 As shown in Eq. (3) in
Appendix 6.2, the factorized distributions yield:
KL(qθ (ω)∣∣p(ω)) = EKL(qθ (ωi)∣∣p(ω/
Note that each BN unit produces two KL(qθ(ωi)∣∣p(ωi)) terms: one for ω% = μB and one for
ωi = σBu .
We assume a Gaussian prior p(ωi) = N(μp, σp) and, for consistency, use the notation qθ(ωi) =
N(μq, σj). As shown in Eq. (4) in Appendix 6.2:
KL(qθ(㈤忸㈤)=^Ip-+σq+¾-μP) - 2
Then, letting (∙)0 denote 品(∙):
高KL(qθ("i)")*'"FZ …q" +
2∣p∣q∣q
σP
(7)
where the last step makes use of the fact that μ1 = 0 and ∣p = 0 (as p("i) cannot depend on θk,
which changes during training).
We assume that only parameters preceding a BN unit in the same layer affects the unit’s stochastic
parameters, such that the stochastic variables in the j:th BN unit are only affected by weights in
θk ∈ Wl,(j). Let the vector of average inputs over D from the preceding layers be denoted by x.
We denote a weight connecting the m:th input unit to thej:th BN unit by W(j,m). For such weights,
11The empirical distributions have been numerically checked to be linearly independent and the joint distri-
bution is close to a bi-variate Gaussian.
20
Under review as a conference paper at ICLR 2018
Figure 4: Batch statistics used to train the network are normal. A one-sample Kolmogorov-Smirnov
test checks that μB and σB Come from a standard normal distribution. More examples are available
in Appendix 6.7.
70
60
50
40
30
20
10
0
batch StdeV. (Unit-1, layer-1, batch SiZe=32, epoch=10)
0.08	0.09	0.1	0.11	0.12	0.13	0.14	0.15
We need to derive μ∖ and σ,q, for both ω% = μB and ω% = σB. Starting with ω% =端:
μ = d OWj)X = X(m)
μq	∂ Wj,m)	N
0 =	∂	JPχ∈D(Wj)x -μq)2] 1
σq — ∂W(j,m) [	NM
1 JPx∈D(wj)χ - μq )2「2 Px∈D 2(Wj)X - μq)(Xg)- Xg)
2	NM	NM
Using the result that σ∖ = 0, one can easily find for 3' = σB that μ∖ = 0 and σ∖ = 0, nullifying
Eq. (7). We need only consider the partial derivatives of the KL divergence terms where 3' = μB .
If we let μp = 0, Eq. (7) reduces to:
∂	μq X(m)	X(m)Wj)X
∂WjmKL(qθ(3i)||p(3i)) = F =—-
for each BN unit and connecting weights from the previous layer, W(j,m). Taking partial derivatives
for all KL(qθ(ω. ∣∣p(ωi)) components in the layer:
∂ tλt / / ∖ιι ( (∖	「—(m)[	WljrmX(m)
∑∑∑dW(j,m)KL(qθ3)收3))= £[x( )]*∑Σ —σ2—
ωl j m ∂W	m	j m	σp
(8)
We consider ReLU activations, such that for large N, X(m) > 0 ∀m. Note that this does not hold
for the first layer (which is possibly normalized), but the effect of including these weights in the
L2-regularization would be smaller the deeper the network. We assume most outputs from previous
layer’s BN units remain normalized through the scale and shift transformation, such that we can
approximate X(m) by the average of all input units over test data, X in Eq. (8). With J- units in
the input layer, setting σp2
for any Gaussian p(σBu ).
J2	J2
2Nλ , such that p(μB) = N(0, 23皆匕)would then reconcile Eq. (5),
6.6	Extended experimental results
Below, we provide extended results measuring uncertainty quality. In Tables 3 and 4, we provide
tables showing the mean CRPS and PLL values for MCBN and MCDO. These results indicate
that MCBN performs on par or better than MCDO across several datasets. In Table 5 we provide
the raw PLL and CRPS results for MCBN and MCDO. In Table 6 we provide RMSE results of
the MCBN and MCDO networks in comparison with non-stochastic BN and DO networks. These
results indicate that the procedure of multiple forward passes in MCBN and MCDO show slight
improvements in the accuracy of the network.
In Figure 5 and Figure 6, we provide a full set of our uncertainty quality visualization plots, where
errors in predictions are sorted by estimated uncertainty. The shaded areas show the model uncer-
tainty and gray dots show absolute prediction errors on the test set. A gray line depicts a running
21
Under review as a conference paper at ICLR 2018
mean of the errors. The dashed line indicates the optimized constant uncertainty. In these plots,
we can see a correlation between estimated uncertainty (shaded area) and mean error (gray). This
trend indicates that the model uncertainty estimates can recognize samples with larger (or smaller)
potential for predictive errors.
Dataset	CRPS			
	MCBN	p-value	MCDO	p-value
Boston Housing	8.50 ±0.86	6.39e-10	3.06 ±0.33	1.64e-9
Concrete	3.91 ±0.25	4.53e-14	0.93 ±0.41	3.13e-2
Energy Efficiency	5.75 ±0.52	6.71e-11	1.37 ±0.89	1.38e-1
Kinematics 8nm	2.85 ±0.18	2.33e-14	1.82 ±0.14	1.64e-12
Power Plant	0.24 ±0.05	2.32e-4	-0.44 ±0.05	2.17e-8
Protein	2.66 ±0.10	2.77-12	0.99 ±0.08	2.34e-12
Wine Quality (Red)	0.26 ±0.07	1.26e-3	2.00 ±0.21	1.83e-9
Yacht Hydrodynamics	-56.39 ±14.27	5.94e-4	21.42 ±2.99	2.16e-7
Table 3: CRPS measured on eight datasets over 25 random 80-20 splits of the data. Mean values
for MCBN and MCDO are reported along with standard error. A significance test was performed to
check if CRPS significantly exceeds the baseline. The p-value from a one sample t-test is reported.
Marked in bold is the best performing method versus its baseline.
Dataset	PLL			
	MCBN	p-value	MCDO	p-value
Boston Housing	10.49 ±1.35	5.41e-8	5.51 ±1.05	2.20e-5
Concrete	-36.36 ±12.12	6.19e-3	10.92 ±1.78	2.34e-6
Energy Efficiency	10.89 ±1.16	1.79e-9	-14.28 ±5.15	1.06e-2
Kinematics 8nm	1.68 ±0.37	1.29e-4	-0.26 ±0.18	1.53e-1
Power Plant	0.33 ±0.14	2.72e-2	3.52 ±0.23	1.12e-13
Protein	2.56 ±0.23	4.28e-11	6.23 ±0.19	2.57e-21
Wine Quality (Red)	0.19 ±0.09	3.72e-2	2.91 ±0.35	1.84e-8
Yacht Hydrodynamics	45.58 ±5.18	5.67e-9	-41.54 ±31.37	1.97e-1
Table 4: PLL measured on eight datasets over 25 random 80-20 splits of the data. Mean values for
MCBN and MCDO are reported along with standard error. A significance test was performed to
check if PLL significantly exceeds the baseline. The p-value from a one sample t-test is reported.
Marked in bold is the best performing method versus its baseline.
Dataset	CRPS		PLL	
	MCBN	MCDO	MCBN	MCDO
Boston Housing	1.45±0.02	1.41±0.02	-2.38±0.02	-2.35±0.02
Concrete	2.40±0.04	2.42±0.04	-3.45±0.11	-2.94±0.02
Energy Efficiency	0.33±0.01	0.26±0.00	-0.94±0.04	-0.80±0.04
Kinematics 8nm	0.04±0.00	0.04±0.00	1.21±0.01	1.24±0.00
Power Plant	2.00±0.01	2.00±0.01	-2.75±0.00	-2.72±0.01
Protein Tertiary Structure	1.95±0.01	1.95±0.00	-2.73±0.00	-2.70±0.00
Wine Quality (Red)	0.34±0.00	0.33±0.00	-0.95±0.01	-0.89±0.01
Yacht Hydrodynamics	0.68±0.02	0.32±0.01	-1.39±0.03	-2.57±0.69
Table 5: CRPS and PLL measured on eight datasets over 25 random 80-20 splits of the data. Mean
values and standard errors are reported for MCBN and MCDO. Marked in bold is the best performing
method for each metric.
6.7	Batch normalization statistics
In Figure 7 and Figure 8, we provide statistics on the batch normalization parameters used for
training. The plots show the distribution of BN mean and BN variance over different mini-batches
22
Under review as a conference paper at ICLR 2018
Dataset	RMSE			
	MCBN	BN	MCDO	DO
Boston Housing	2.75 ±0.05	2.77 ±0.05	2.65 ±0.05	2.69 ±0.05
Concrete	4.78 ±0.09	4.89 ±0.08	4.80 ±0.10	4.99 ±0.10
Energy Efficiency	0.59 ±0.02	0.57 ±0.01	0.47 ±0.01	0.49 ±0.01
Kinematics 8nm	0.07 ±0.00	0.07 ±0.00	0.07 ±0.00	0.07 ±0.00
Power Plant	3.74 ±0.01	3.74 ±0.01	3.74 ±0.02	3.72 ±0.02
Protein	3.66 ±0.01	3.69 ±0.01	3.66 ±0.01	3.68 ±0.01
Wine Quality (Red)	0.62 ±0.00	0.62 ±0.00	0.60 ±0.00	0.61 ±0.00
Yacht Hydrodynamics	1.23 ±0.05	1.28 ±0.06	0.75 ±0.03	0.72 ±0.04
Table 6: RMSE measured on eight datasets over 25 random 80-20 splits of the data. Mean values and
standard errors are reported for MCBN and MCDO as well as conventional non-Bayesian models
BN and DO. Marked in bold is the best performing method overall.
of an actual training of Yacht dataset for one unit in the first hidden layer and the second hidden
layer. Data is provided for different epochs and for different batch sizes.
23
Under review as a conference paper at ICLR 2018
12∙
Boston Housing
MCBN
10-
95% CI
50% CI
--∖∕l∕τ
—run mean
12-
10-
Boston Housing 95% CI
MCDO	.50% CI
--ʌ/l/r
—run mean
8-
8-
25-
6-
4 ∙ .	∙	∙ ∙ ∙
2
Concrete
MCDO
--∖∕l∕τ
---run mean.
110-
20-
o
φ
⊂ 15-
20-
5
3-
95% CI
50% CI
--χ∕l∕τ
⊂ 15-
Energy
MCBN
95% CI
50% CI
--，1/t
—run mean
3
95% CI
50% CI
--χ∕l∕τ
—run mean
Energy
MCDO
2.5-
2.5
—run mean
ɑ 1.5 -
1 -
0.5-
--run mean
Kinematics
MCDO
.2 1.5-
P
ω
95% CI
50% CI
--∖∕l∕τ
0
2 2
U
O
2 2-
U
1 -
0.3-
0.3-
出0.2
Figure 5: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The
shaded areas show MCBN’s (blue) and MCDO’s (red) model uncertainty (light area 95% CL dark
area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a
running mean of the errors. The dashed line indicates the optimized constant uncertainty. A corre-
lation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty
estimates are meaningful for estimating errors.
Kinematics
MCBN
95% CI
50% CI
—∕/
--run mean
0
前 0.2-
0.1 -

24
Under review as a conference paper at ICLR 2018
16-
PoWer Plant
MCBN
95% CI
50% CI
16-
Power Plant
MCDO
95% CI
50% CI
14-
12-
φ
U
O
O
ŋ
φ
θZ
14-
--∖∕l∕τ
⅜
12-
——run mean
10
8
6
4
2
Φ
U
O
O
15
φ
θZ
--ʌ/l/T
10
8
6
4
JoXl ① uo-o一 Pald
2
JoXl ① uo-o一 Pald
2.5-
2-
95% CI
50% CI
--λ∕1∕t
——run mean
5
N
Wine Quality
MCBN
Wine Quality
MCDO
⅛tφ Uoq0_p ①」O-
4.5 -
95% CI
50% CI
--ʌ/l/r
⅛tφ uo-lɔ-pəo:
1
0.5
1.5
4.5-
95% CI
50% CI
—∙∖∕l∕τ
Yacht
MCBN
Yacht
MCDO
4-
4-
Figure 6: Errors in predictions (gray dots) sorted by estimated uncertainty on select datasets. The
shaded areas show MCBN’s (blue) and MCDO’s (red) model uncertainty (light area 95% CL dark
area 50% CI). Gray dots show absolute prediction errors on the test set, and the gray line depicts a
running mean of the errors. The dashed line indicates the optimized constant uncertainty. A corre-
lation between estimated uncertainty (shaded area) and mean error (gray) indicates the uncertainty
estimates are meaningful for estimating errors.
25
Under review as a conference paper at ICLR 2018
batch mean (unit-1, layer-1, batch size=128, epoch=10)
80「
KS normality test
70 p = 6.353e-101
60 -
batch mean
^-fitted Normal
median
8q≡nu
30
20
10
0
-0.06	-0.04	-0.02	0
0.02	0.04	0.06
batch mean (unit-1, layer-2, batch SiZe=128, epoch=10)
70
batch mean (unit-1, layer-2, batch size=128, epoch=100)
60「
KS normality test
60 p = 8.417e-123
50 -
batch mean
KS normality test
50 p = 1.393e-113
batch mean
^-fitted Normal
median
8q≡nu
fitted Normal
median
8q≡nu
20
10
0
0.072 0.074 0.076 0.078 0.08 0.082 0.084 0.086 0.088 0.09 0.092
20
10
0
0.021	0.0215	0.022	0.0225	0.023
Figure 7: The distribution of means of mini-batches during training of one of our datasets. The dis-
tribution closely follows our analytically approximated Gaussian distribution. The data is collected
for one unit of each layer and is provided for different epochs and for different batch sizes.
26
Under review as a conference paper at ICLR 2018
70
60
50
40
30
20
10
0
batch StdeV. (Unit-1, layer-1, batch SiZe=32, epoch=10)
0.08	0.09	0.1	0.11	0.12	0.13	0.14	0.15
Figure 8: The distribution of standard deviation of mini-batches during training of one of our
datasets. The distribution closely follows our analytically approximated Gaussian distribution. The
data is collected for one unit of each layer and is provided for different epochs and for different
batch sizes.
27