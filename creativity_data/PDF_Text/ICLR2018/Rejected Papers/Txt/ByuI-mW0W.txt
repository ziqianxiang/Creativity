Under review as a conference paper at ICLR 2018
Towards a testable notion of generalisation
FOR GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We consider the question of how to assess generative adversarial networks, in
particular with respect to whether or not they generalise beyond memorising the
training data. We propose a simple procedure for assessing generative adversarial
network performance based on a principled consideration of what the actual goal
of generalisation is. Our approach involves using a test set to estimate the Wasser-
stein distance between the generative distribution produced by our procedure, and
the underlying data distribution. We use this procedure to assess the performance
of several modern generative adversarial network architectures. We find that this
procedure is sensitive to the choice of ground metric on the underlying data space,
and suggest a choice of ground metric that substantially improves performance.
We finally suggest that attending to the ground metric used in Wasserstein gener-
ative adversarial network training may be fruitful, and provide a concrete formu-
lation for doing so.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) have attracted significant interest
as a means for generative modelling. However, recently concerns have been raised about their
ability to generalise from training data and their capacity to overfit (Arora & Zhang, 2017; Arora
et al., 2017). Moreover, techniques for evaluating the quality of GAN output are either ad hoc, lack
theoretical rigor, or are not suitably objective - often times “visual inspection" of samples is the
main tool of choice for the practitioner. More fundamentally, it is sometimes unclear exactly what
we want a GAN to do: what is the learning task that we are trying to achieve?
In this paper, we provide a simple formulation of the GAN training framework, which consists of
using a finite dataset to estimate an underlying data distribution. The quality of GAN output is
measured precisely in terms of a statistical distance D between the estimated and true distribution.
Within this context, we propose an intuitive notion of what it means for a GAN to generalise.
We also show how our notion of performance can be measured empirically for any GAN archi-
tecture when D is chosen to be a Wasserstein distance, which - unlike other methods such as the
inception score (Salimans et al., 2016) - requires no density assumptions about the data-generating
distribution. We investigate this choice of D empirically, finding that its performance is heavily
dependent on the choice of ground metric underlying the Wasserstein distance. We suggest a novel
choice of ground metric that we show performs well, and also discuss how we might otherwise use
this observation to improve the design of Wasserstein GANs (WGANs) (Arjovsky et al., 2017).
2	The objective of generative modelling
GANs promise a means for learning complex probability distributions in an unsupervised fashion.
In order to assess their effectiveness, we must first define precisely what we mean by this. We seek
to do so in this section, presenting a formulation of the broader goal of generative modelling that we
believe is widely compatible with much present work in this area. We also provide a natural notion
of generalisation that arises in our framework.
Our setup consists of the following components. We assume some distribution π on a set X . For
instance, X may be the set of 32x32 colour images, and π the distribution from which the CIFAR-10
1
Under review as a conference paper at ICLR 2018
dataset was sampled. We assume that π is completely intractable: we do not know its density (or
even if it has one), and we do not have a procedure to draw novel samples from it. However, we do
suppose that We have a fixed dataset X consisting of samples xi, ∙∙∙ ,χ∣χ ∣ iid ∏. Equivalently, We
have the empirical distribution
Where δ denotes the Dirac mass.
Let P(X) denote the set of probability distributions on X . Our aim is to use X to produce a
distribution in P(X) that is as “close” as possible to π. We choose to measure closeness in terms of
a function D : P(X) × P(X) → R. Usually D Will be chosen to be a statistical divergence, Which
means that D(P, Q) ≥ 0 for all P and Q, With equality iffP = Q. The task of a learning algorithm
α in this context is then as folloWs:
Select ɑ(X) ∈ P(X) such that D(α(X), π) is as small as possible.	(1)
We believe (1) constitutes an intuitive and useful formulation of the problem of generative modelling
that is largely in keeping With present research efforts.
Now, we can immediately see that one possibility is simply to choose α(X) := X. Moreover, in the
case that D is a metric for the Weak topology on P(X) such as a Wasserstein distance, We have that
D(X, π) → 0 almost surely as |X| → ∞, so that, assuming |X| is large enough, we can already
expect D(X, π) to be small. This then suggests the following natural notion of generalisation:
A 1	i'	1 ∙	i'	∙	-χr ∙ i' TTA /	/ ʌr ∖	∖	.	T∙∖，令 ∖	∕A∖
A choice of α generalises for a given X if D(α(X), π) < D(X, π).	(2)
In other words, using α here has actually achieved something: perhaps through some process of
smoothing or interpolation, it has injected additional information into X that has moved US closer to
π than we were a priori.
3	Generalisation in GANs
The previous section presented (1) as a general goal of generative modelling. In this section, we
turn specifically to GANs. We begin by providing a general model for how many of the existing
varieties of GAN operate, at least ideally. We then show how this model fits into our framework
above, before considering the issue of generalisation in this context.
Most GAN algorithms in widespread use adhere to the following template: they take as input a
distribution P, from which we assume we can sample, and compute (or approximate)
Γ(P) := arg min DΓ(P, Q)
Q∈Q
for some choices of Q ⊆ P(X) and DΓ : P(X) × P(X) → R. In other words, in the ideal case,
a GAN maps P to its DΓ-projection onto Q. Note that we will not necessarily have that DΓ = D:
DΓ is fixed given a particular GAN architecture, whereas the choice of D is simply a feature of our
problem definition (1) and is essentially ours to make.
In practice, Q is the set of pushforward measures ν ◦ G-1 obtained from a fixed noise distribution
ν on a noise space Z and some set G of functions G : Z → X. Precisely, Q = ν ◦ G-1 : G ∈ G .
G itself usually corresponds to the set of functions realisable by some neural network architecture,
and ν is some multivariate uniform or Gaussian distribution. However, numerous choices of DΓ
have been proposed: the original GAN formulation (Goodfellow et al., 2014) took DΓ to be the
Jenson-Shannon divergence, whereas the f -GAN (Nowozin et al., 2016) generalised this to arbitrary
f -divergences, and the Wasserstein GAN (Arjovsky et al., 2017) advocated the Wasserstein distance.
Many of the results proved in these papers involve showing that (usually under some assumptions,
such as sufficient discriminator capacity) a proposed objective for G is in fact equivalent to Dr (P, V◦
G-1).
2
Under review as a conference paper at ICLR 2018
In terms of our framework in the previous section, using a GAN Γ amounts to choosing
. . .^ . . ^
α(X):= Γ(X) = arg min Dr(X, Q).
Q∈Q
We emphasise again the important distinction between D and DΓ . In our setup, minimising D
defines our ultimate goal, whereas minimising DΓ (over Q) defines how we will attempt to achieve
that goal. Even if D 6= DΓ, it is still at least conceivable that D(Γ(X), π) might be small, and
therefore this choice of α might be sensible. Also note that, crucially, Γ receives X as input rather
than π itself. We only have access to a fixed number of CIFAR-10 samples, for example, not
an infinite stream. Moreover, training GANs usually involves making many passes over the same
dataset, so that, in effect, sampling from P will repeatedly yield the same data points. We would not
expect this to occur with nonzero probability if P = π for most π of interest.
The observation that P is X rather than ∏ was also recently made by Arora et al. (2017). The
authors argue that this introduces a problem for the ability of GANs to generalise, since, if DΓ is a
divergence (which is almost always the case), and if Q is too big (in particular, ifit is big enough that
X ∈ Q), then we trivially have that Γ(X) = X. In other words, the GAN objective appears actively
to encourage Γ(X) to memorise the dataset X, and never to produce novel samples from outside of
it. The authors’ proposed remedy involves trying to find a better choice of DΓ . The problem, they
argue, is that popular choices of DΓ do not satisfy the condition
DΓ (π, Q) ≈ DΓ(X, Q) with high probability given a modest number of samples in X.	(3)
They point out that this is certainly violated when DΓ is the Jensen-Shannon divergence JS, since
JS(PkQ)=log2
when one of P and Q is discrete and the other continuous, and give a similar result for DΓ a
Wasserstein distance in the case that π is Gaussian. As a solution, they introduce the neural network
distance DNN defined by
Dnn(P, Q)= max Eχ~p [f (x)] - Eχ~Q [f (x)]
f∈F
for some choice of a class of functions F. They show that, assuming some smoothness conditions on
the members ofF, the choice DΓ = DNN satisfies (3), which means that if we minimise DNN(X, Q)
in Q then we can be confident that the value of DNN(π, Q) is small also.
However, we do not believe that (3) is sufficient to ensure good generalisation behaviour for GANs.
What we care about ultimately is not the value of DΓ, but rather of D, and (3) invites choosing DΓ
in such a way that gives no guarantees about D at all. We see, for instance, that the degenerate
choice D0 (P, Q) := 0 trivially satisfies (3), and indeed is also a pseudometric, just like DNN. It is
therefore unclear what mathematical properties of DNN render it more suitable for estimating π than
the obviously undesirable D0 . The authors do acknowledge this shortcoming of DNN, pointing out
that DNN(P, Q) can be small even if P and Q are “quite different” in some sense.
The problematic consequences of the fact that P = X apply only in the case that Q is too large.
In practice, however, Q is heavily restricted, since G is restricted via a choice of neural network
architecture; hence we do not know a priori whether Γ(X) = X is even possible. As such, we
do not see the choice of α(X) = Γ(X) as necessarily a bad idea, and instead believe that it is an
open empirical question as to how well GANs perform the task (1). In fact, this α falls perfectly
within the framework of minimum distance estimation (Wolfowitz, 1957; Basu et al., 2011), which
involves estimating an underlying distribution by minimising a distance measure to a given empirical
distribution.
4	Testing GANs
Our goal in this section is to assess how well GANs achieve (1) by estimating D(Γ(X), π) for
various Γ and π. This raises some difficulties, given that π is intractable. Our approach is to take D
to be the first Wasserstein distance WdX defined by
WdX (P, Q)
inf
γ∈Π(P,Q)
/
X×X
dX (x, y) dγ(x, y),
3
Under review as a conference paper at ICLR 2018
where dX is a metric on X referred to as the ground metric, and Π(P, Q) denotes the set of joint
distributions on the product space X × X with marginals P and Q. The Wasserstein distance is
appealing since it is sensitive to the topology of the underlying set X, which we control by our
choice of dX. Moreover, WdX metricises weak convergence for the Wasserstein space PdX (X)
defined by
PdX (X) :
P ∈ P(X) :	dX(x0, y) dP (y) < ∞ for some x0 ∈ X
X
(See (Villani, 2008)). Consequently, if We denote by A a set of samples aι, ∙∙∙ , α∣A∣ 〜α(X),
and by Y a set of samples (separate from X) yι,…，y∣γ∣ 吧 ∏, with A and Y the corresponding
empirical distributions, then, provided
α(X),π∈PdX(X),	(4)
we have that D(A, Y) → D(α(X), π) almost surely as min {|A| , |Y|} → ∞. Note that condition
(4) holds automatically in the case that (X, dX) is compact, since then PdX (X) = P(X).
As such, to estimate D(α(X), π), for D = WdX, we propose the following. Before training, we
move some of our samples from X into a testing set Y . We next train our GAN on X, obtaining
α(X). We then take samples A from α(X), and obtain the estimate
.^ ^ . . .
WdX(A,Y) ≈ WdX(α(X),π),
where the left-hand side can be computed exactly by solving a linear program since both A and
Y are discrete (Villani, 2003). We can also use the same methodology to estimate WdX (X, π) by
TT7^ / 4z^ -χ`τ^∖	1 ∙ 1	,	∙ l'
WdX (X, Y), which suggests testing if
WdX(A,Y) < WdX(X,Y).	(5)
as a proxy for determining whether (2) holds. A summary of our procedure is given in Algorithm 1.
Algorithm 1 Procedure for testing GANs
1:	Split samples from π into a training set X and a testing set Y
2:	Compute α(X) by training a GAN on X
3:	Obtain a sample A from α(X)
4:	Approximate
. . . . .^ ^ .
W(α(X),π) ≈ W(A,Y),
where the right-hand side can be computed by solving a linear program
5:	Similarly, test whether
W (A,Y) < W (X,Y)
as a proxy for W (α(X), π) < W(A, π) * 4
4.1 Results
We applied our methodology to test two popular GAN varieties - the DCGAN (Radford et al.,
2015) and the Improved Wasserstein GAN (I-WGAN) (Gulrajani et al., 2017) - on the MNIST and
CIFAR-10 datasets. In all cases when computing the relevant Wasserstein distances, our empirical
distributions consisted of 10000 samples.
4.1.1 L2 AS GROUND METRIC
We initially took our ground metric dX to be the L2 distance. This has the appealing property of
making (X , dX ) compact when X is a space of RGB or greyscale images, therefore ensuring that
(4) holds.
Figure 1 shows WL2 (A, Y) plotted over multiple training runs for MNIST and CIFAR-10 when A
is obtained from an I-WGAN, with typical samples shown in Figures 6 and 7 in the appendix. For
4
Under review as a conference paper at ICLR 2018
Figure 1: Output of Algorithm 1 with dX = L2 for I-WGAN trained on MNIST (left) and CIFAR-10
(right)
Batch
Figure 2: Output of Algorithm 1 with dX = L2 for DCGAN trained on CIFAR-10
1	,1	1	,	, TTT' / Λ -χ`τ^∖ F	,	F	,	,1	,	∙	1	1,,1	∙	1
both datasets, WL2 (A, Y ) decays towards an asymptote in a way that nicely corresponds to the visual
quality of the samples produced. Moreover, WL2 (A, Y ) is much closer to WL2 (X, Y ) towards the
tail end of training for MNIST than for CIFAR-10. This seems to reflect the fact that, visually,
the eventual I-WGAN MNIST samples do seem quite close to true MNIST samples, whereas the
eventual I-WGAN CIFAR-10 samples are easily identified as fake.
However, when we re-ran the same experiment using a DCGAN on CIFAR-10, we obtained the
WL2 (A, Y ) trajectories shown in Figure 2. Typical examples are shown in Figure 8. Strangely, we
observe that Wl2 (A, Y) < Wl2 (X, Y) very early on m training - at around batch 500 - when
the samples resemble the heavily blurry Figure 8b. This raises some obvious concerns about the
appropriateness of WL2 as a metric for GAN quality.
We therefore sought to understand this strange behaviour. Motivated by the visual blurriness of the
samples in Figure 8b, we explored the effect on WLp(X, Y) of blurring the CIFAR-10 training set
X. In particular, weletX andY each consist of 10000 distinct CIFAR-10 samples in X. We then in-
dependently convolved each channel of each image with a Gaussian kernel having standard deviation
σ, obtaining a blurred dataset βσ(X) and corresponding empirical distribution βσ(X). The visual
effect of this procedure is shown in Figure 9 in the appendix. We then computed WLp (βσ (X), Y)
with σ ranging between 0 and 10 for a variety of values of p. The results of this experiment in the
case p = 2 are shown in Figure 3, and similar results were observed for other values of p: in all
cases, we found that
WLp(X,Y) > Wlp(βσ(X),Y)
i	. zʌ El , ∙ i i ∙ ʌr ι	, ι	ʌr ι	, ^Cr ∙ ττr .ι	.
whenever σ > 0. That is, blurring X by any amount brings X closer to Y in WLp than not.
This occurs even though X is distributed identically to Y (both being drawn from π), while βσ (X)
(presumably) is not when σ > 0.
5
Under review as a conference paper at ICLR 2018
Figure 3: Effect ofblurring CIFAR-10 on 卬工2 (left) and 卬工2。n(right)
4.1.2 EMBEDDED L2 AS GROUND METRIC
To remedy these issues, we sought to replace L2 with a choice ofdX that is more naturally suited to
the space of images in question. To this end we tried mapping X through a fixed pre-trained neural
network η into a feature space Y , and then computing distances using some metric dY on Y , rather
than in X directly. It is easily seen that, provided η is injective, dY ◦ η : X × X → R defined by
(dY ◦ η)(x, x0) := dY (η(x), η(x0))
is a metric. It also holds that, when η is (dX, dY)-continuous, (X, dY ◦ η) is compact when (X, dX)
is: given a sequence xi ⊆ X, there exists a subsequence xi0 that converges in dX to some x (by
compactness), so that (dY ◦ η)(xi0 , x) = dY (η(xi0), η(x)) → 0 by continuity of η. Neither of
these conditions on η 一 injectivity and (dχ ,dγ)-continuity - are unreasonable to expect from a
typical neural network trained using stochastic gradient descent (at least, when dX and dY are typical
metrics such as Lp distances). Consequently, WdYOn constitutes a valid metric on PdYOn (X), and
(4) is automatically satisfied.
To test the performance of WdYOn, we repeated the blurring experiment described above. We took
η(x) to be the result of scaling x ∈ X to size 224x224, mapping the result through a DenseNet-121
(Huang et al., 2016) pre-trained on ImageNet (Deng et al., 2009), and extracting features imme-
diately before the linear output layer. Under the same experimental setup as above otherwise, we
obtained the plot of WL2On(βσ(X), Y ) shown in Figure 3. Happily, we now see that this curve in-
creases monotonically as σ grows in accordance with the declining visual quality of βσ (X) shown
in Figure 9.
Next, we computed WL2On(A, Y ) over the course of GAN training. For the I-WGAN we obtained
the results on MNIST and CIFAR-10 shown in Figure 4;1 for the DCGAN on CIFAR-10, we ob-
tained the curve shown in Figure 5. In all cases we see that WL2On(A, Y ) decreases monotoni-
cally towards an asymptote in a way that accurately summarises the visual quality of the samples
throughout the training run. Moreover, there is always a large gap between the eventual value of
TT7^	/ Λ -χ`τ^∖	F TT7^	/ ʌr -χ`τ^∖	1 ∙	1 Γt ,,1 l' ,,1	,,1 zɔ A 、T	1	, ∙11	∙	11
WL2On(A, Y ) and WL2On(X, Y ), which reflects the fact that the GAN samples are still visually
distinguishable from real π samples. In particular, we see an improvement in this respect for the
I-WGAN on MNIST: in Figure 1 the asymptotic value of Wl2 (A, Y) for MNIST was barely dis-
cernible from WL2 (X, Y ), despite the fact that itis still quite easy to tell real samples from generated
ones (see e.g. the various mistakes present in Figure 6).
5	Discussion and future work
We believe our work reveals two promising avenues of future inquiry. First, we suggest that WLpOn is
an appealing choice of D, both due to its nice theoretical properties - it metricises weak convergence,
and does not require us to make any density assumptions about ∏ 一 and due to its sound empirical
performance demonstrated above. It would be very interesting to use this D to produce a systematic
and objective comparison of the performance of all current major GAN implementations, and indeed
1 Note that on MNIST in this case we first duplicated each input across three channels for shape compatibility
with the DenseNet.
6
Under review as a conference paper at ICLR 2018
Figure 4: Output of Algorithm 1 with dX
CIFAR-10 (right)
L2 ◦ η for I-WGAN trained on MNIST (left) and
Figure 5: Output of Algorithm 1 with dX = L2 ◦ η for DCGAN trained on CIFAR-10
to use this as a metric for guiding future GAN design. We also view the test (5) as potentially useful
for determining whether our algorithms are overfitting. This would be particularly so if applied via a
cross-validation procedure: ifwe consistently observe that (5) holds when training a GAN according
to many different X and Y partitions of our total π samples, then it seems reasonable to infer that
α(X) has indeed learnt something useful about π.
We also believe that the empirical inadequacy of WL2 that we observed suggests a path towards a
better WGAN architecture. At present, WGAN implementations implicitly use WL2 for their choice
of Dr. We suspect that altering this to our suggested Wl20n may yield better quality samples. We
briefly give here one possible way to do so that is largely compatible with existing WGAN setups.
In particular, following Arjovsky et al. (2017), we take
Dr (P Q) = max Ex〜P [f (x)] - Ex〜Q [f (x)]
f∈F
for a class F of functions f : X → R that are all (L2 ◦ η, dR)-Lipschitz for some fixed Lipschitz
constant K . Here dR denotes the usual distance on R. To optimise over such an F in practice, we
can require our discriminator f : X → R to have the form f(x) := h(η(x)), where h : Y → R is
(dY , dR )-Lipschitz, which entails that f is Lipschitz provided η is (which is almost always the case
in practice). In other words, we compute
Dr (P, Q) = maxEx〜P [h(η(x))] - Ex〜Q [h(η(x))],
where F0 is a class of (dY, dR)-Lipschitz functions. Optimising over this objective may now proceed
as usual via weight-clipping like (Arjovsky et al., 2017), or via a gradient penalty like (Gulrajani
et al., 2017). Note that this suggestion may be understood as training a standard WGAN with the
initial layers of the discriminator fixed to the embedding η; our analysis here shows that this is equiv-
alent to optimising with respect to 卬工2。n instead of Wl2 . We have begun some experimentation in
this area but leave a more detailed empirical inquiry to future work.
7
Under review as a conference paper at ICLR 2018
It is also clearly important to establish better theoretical guarantees for our method. At present, we
have no guarantee that the number of samples in A and Y are enough to ensure that
D(A^,Y) ≈ D(α(X),Y)
(perhaps with some fixed bias that is fairly independent of α, so that it is valid to use the value of
D(A, Y) to compare different choices of α), or that (5) entails (2) with high probability. We do
however note that some recent theoretical work on the convergence rate of empirical Wasserstein
estimations (Weed & Bach, 2017) does suggest that it is plausible to hope for fast convergence of
D(A, Y) to D(α(X), Y). We also believe that the convincing empirical behaviour of 卬工2。n does
suggest that it is possible to say something more substantial about our approach, which we leave to
future work.
6	Related Work
The maximum mean discrepancy (MMD) is another well-known notion of distance on probability
distributions, which has been used for testing whether two distributions are the same or not (Gretton
et al., 2012) and also for learning generative models in the style of GAN (Li et al., 2015; Dziugaite
et al., 2015; Sutherland et al., 2016; Li et al., 2017). It is parameterised by a characteristic kernel k,
and defines the distance between probability distributions by means of the distance of k’s reproduc-
ing kernel Hilbert space (RKHS). The MMD induces the same weak topology on distributions as the
one of the Wasserstein distance. Under a mild condition, the MMD between distributions P and Q
under a kernel k can be understood as the outcome of the following two-step calculation. First, we
pushforward P and Q from their original space X to a Hilbert space H (isomorphic to k’s RKHS)
using a feature function φ : X → H induced by the kernel k. Typically, H is an infinite-dimensional
space, such as the set `2 of square-summable sequences in R∞ as in Mercer’s theorem. Let P0
and Q0 be the resulting distributions on the feature space. Second, we compute the supremum of
EP0(X) [f(X)] - EQ0(X) [f(X)] over all linear 1-Lipschitz functions f : H → R. The result of this
calculation is the MMD between P and Q.
This two-step calculation shows the key difference between MMD and our use of Wasserstein
distance and neural embedding η . While the co-domain of the feature function φ is an infinite-
dimensional space (e.g. '2) in most cases, its counterpart η in our setting uses a finite-dimensional
space as co-domain. This means that the MMD possibly uses a richer feature space than our ap-
proach. On the other hand, the MMD takes the supremum over only linear functions f among all
1-Lipschitz functions, whereas the Wasserstein distance considers all these 1-Lipschitz functions.
These different balancing acts between the expressiveness of features and that of functions taken in
the supremum affect the learning and testing of various GAN approaches as observed experimentally
in the literature. One interesting future direction is to carry out systematic study on the theoretical
and practical implications of these differences.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv
preprint arXiv:1706.08224, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and Equilibrium
in Generative Adversarial Nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Ayanendranath Basu, Hiroyuki Shioya, and Chanseok Park. Statistical inference: the minimum
distance approach. CRC Press, 2011.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on,pp. 248-255. IEEE, 2009.
8
Under review as a conference paper at ICLR 2018
Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Con-
ference on Uncertainty in Artificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The
Netherlands,pp. 258-267, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander J.
Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723-773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. MMD GAN:
towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584,
2017.
Yujia Li, Kevin Swersky, and Richard S. Zemel. Generative moment matching networks. In Pro-
ceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France,
6-11 July 2015, pp. 1718-1727, 2015.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Dougal J. Sutherland, Hsiao-Yu Fish Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas,
Alexander J. Smola, and Arthur Gretton. Generative models and model criticism via optimized
maximum mean discrepancy. arXiv preprint arXiv:1611.04488, 2016.
Cedric Villani. Topics in optimal transportation. Number 58. American Mathematical Soc., 2003.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of em-
pirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
Jacob Wolfowitz. The minimum distance method. The Annals of Mathematical Statistics, pp. 75-88,
1957.
9
Under review as a conference paper at ICLR 2018
A Example I-WGAN samples
(a) Batch 0
(b) Batch 600
9 3 斗，$
，q a 5 彳
曰 CfqZG
7 7 w I y
S 6 I ʃ ¾
J ?，N 6
一 IqX
星令G 3 Q
√ 1 S 2
(c) Batch 5000
(d) Batch 10000
P 2 / Z 3
* S Q 2。
4 ； / 2 2
3 " 5 / 夕
3 5 ”，7
《飞Jq
6 K 4 *7
2 W牛0
齐Q 5 4
/ 2吁。
(e) Batch 50000
(f) Batch 69800
Figure 6: Samples from I-WGAN trained on MNIST
10
Under review as a conference paper at ICLR 2018
(a) Batch 0
(b) Batch 500
Figure 7: Samples from I-WGAN trained on CIFAR-10
11
Under review as a conference paper at ICLR 2018
B Example DCGAN samples
(a) Batch 0	(b) Batch 500
(c) Batch 1000
(d) Batch 2000
(e) Batch 3000
(f) Batch 3920
Figure 8: Samples from DCGAN trained on CIFAR-10 test set
12
Under review as a conference paper at ICLR 2018
C
(g) σ = 8
Figure 9: Effect of different σ on βσ(X), for X the CIFAR-10 training set
13