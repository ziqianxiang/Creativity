Under review as a conference paper at ICLR 2018
CayleyNets: Spectral Graph CNNs with
Complex Rational Filters
Anonymous authors
Paper under double-blind review
Abstract
The rise of graph-structured data such as social networks, regulatory networks, cita-
tion graphs, and functional brain networks, in combination with resounding success
of deep learning in various applications, has brought the interest in generalizing
deep learning models to non-Euclidean domains. In this paper, we introduce a new
spectral domain convolutional architecture for deep learning on graphs. The core
ingredient of our model is a new class of parametric rational complex functions
(Cayley polynomials) allowing to efficiently compute spectral filters on graphs
that specialize on frequency bands of interest. Our model generates rich spectral
filters that are localized in space, scales linearly with the size of the input data for
sparsely-connected graphs, and can handle different constructions of Laplacian
operators. Extensive experimental results show the superior performance of our ap-
proach on spectral image classification, community detection, vertex classification
and matrix completion tasks.
1	Introduction
In many domains, one has to deal with large-scale data with underlying non-Euclidean structure.
Prominent examples of such data are social networks, genetic regulatory networks, functional
networks of the brain, and 3D shapes represented as discrete manifolds. The recent success of
deep neural networks and, in particular, convolutional neural networks (CNNs) LeCun et al. (1998)
have raised the interest in geometric deep learning techniques trying to extend these models to data
residing on graphs and manifolds. Geometric deep learning approaches have been successfully
applied to computer graphics and vision Masci et al. (2015); Boscaini et al. (2015; 2016b;a); Monti
et al. (2017a), brain imaging Ktena et al. (2017), and drug design Duvenaud et al. (2015) problems,
to mention a few. For a comprehensive presentation of methods and applications of deep learning on
graphs and manifolds, we refer the reader to the review paper Bronstein et al. (2016).
Related work. The earliest neural network formulation on graphs was proposed by Gori et al. (2005)
and Scarselli et al. (2009), combining random walks with recurrent neural networks (their paper has
recently enjoyed renewed interest in Li et al. (2015); Sukhbaatar et al. (2016)). The first CNN-type
architecture on graphs was proposed by Bruna et al. (2013). One of the key challenges of extending
CNNs to graphs is the lack of vector-space structure and shift-invariance making the classical notion
of convolution elusive. Bruna et al. formulated convolution-like operations in the spectral domain,
using the graph Laplacian eigenbasis as an analogy of the Fourier transform (Shuman et al. (2013)).
Henaff et al. (2015) used smooth parametric spectral filters in order to achieve localization in the
spatial domain and keep the number of filter parameters independent of the input size. Defferrard et al.
(2016) proposed an efficient filtering scheme using recurrent Chebyshev polynomials applied on the
Laplacian operator. Kipf & Welling (2016) simplified this architecture using filters operating on 1-hop
neighborhoods of the graph. Atwood & Towsley (2016) proposed a Diffusion CNN architecture based
on random walks on graphs. Monti et al. (2017a) (and later, Hechtlinger et al. (2017)) proposed a
spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian
mixture models, showing a significant advantage of such models in generalizing across different
graphs. In Monti et al. (2017b), spectral graph CNNs were extended to multiple graphs and applied
to matrix completion and recommender system problems.
1
Under review as a conference paper at ICLR 2018
Main contribution. In this paper, we construct graph CNNs employing an efficient spectral
filtering scheme based on Cayley polynomials that enjoys similar advantages of the Chebyshev filters
(Defferrard et al. (2016)) such as localization and linear complexity. The main advantage of our
filters over Defferrard et al. (2016) is their ability to detect narrow frequency bands of importance
during training, and to specialize on them while being well-localized on the graph. We demonstrate
experimentally that this affords our method greater flexibility, making it perform better on a broad
range of graph learning problems.
Notation. We use a, a, and A to denote scalars, vectors, and matrices, respectively. W denotes the
conjugate of a complex number, Re{z} its real part, and i is the imaginary unit. diag(a1,...,an)
denotes an n × n diagonal matrix with diagonal elements a1,...,an. Diag(A) = diag(a11,...,ann)
denotes an n × n diagonal matrix obtained by setting to zero the off-diagonal elements of A.
Off(A)=A - Diag(A) denotes the matrix containing only the off-diagonal elements of A. I is
the identity matrix and A ◦ B denotes the Hadamard (element-wise) product of matrices A and B.
Proofs are given in the appendix.
2	Spectral techniques for deep learning on graphs
Spectral graph theory. Let G =({1,...,n}, E, W) be an undirected weighted graph, represented
by a symmetric adjacency matrix W = (wij). We define wij =0if (i, j) ∈/ Eand wij > 0
if (i, j) ∈E. We denote by Nk,m the k-hop neighborhood of vertex m, containing vertices that
are at most k edges away from m. The unnormalized graph Laplacian is an n × n symmetric
PoSitiVe-Semidefinite matrix ∆U = D — W, where D = diag(Ej=2 Wij) is the degree matrix. The
normalized graph Laplacian is defined as ∆n = D—1 //∆UD—1 /2 = I 一 D—1 /2WD—1 /2. In the
following, we use the generic notation ∆ to refer to some Laplacian.
Since both normalized and unnormalized Laplacian are symmetric and positive semi-definite matrices,
they admit an eigendeComPoSition ∆ = ΦΛΦT, where Φ = (φ 1,... φn) are the orthonormal
eigenvectors and Λ = diag(λ1,...,λn) is the diagonal matrix of corresponding non-negative
eigenvalues (sPectrum) 0=λ1 ≤ λ2 ≤ . .. ≤ λn . The eigenvectors Play the role of Fourier atoms
in classical harmonic analysis and the eigenvalues can be interPreted as (the square of) frequencies.
Given a signal f =( f1,...,fn)T on the vertices of graph G, its graph Fourier transform is given
by f = ΦT f. Given two signals f, g on the graph, their SPectral convolution can be defined as the
element-wise product ofthe Fourier transforms, f *g = Φ ((ΦTg) ◦ (ΦTf)) = Φ diag(g1,..., gn)f,
which corresponds to the property referred to as the Convolution Theorem in the Euclidean case.
Spectral CNNs. Bruna et al. (2013) used the spectral definition of convolution to generalize CNNs
on graphs, with a spectral convolutional layer of the form
fout = ξ (E ΦkGi"ΦTfin
Here the n × p and n × q matrices Fin =(f1in,. ..,fpin) and Fout =(f1out,...,fqout) represent
respectively the p- and q-dimensional input and output signals on the vertices of the graph, Φk =
(φ 1,..., φk) is an n × k matrix of the first eigenvectors, G ι,u = diag(仇,“」，...，gι,ι>,k) is a k × k
diagonal matrix of spectral multipliers representing a learnable filter in the frequency domain, and
ξ is a nonlinearity (e.g., ReLU) applied on the vertex-wise function values. Pooling is performed
by means of graph coarsening, which, given a graph with n vertices, produces a graph with n ‹ n
vertices and transfers signals from the vertices of the fine graph to those of the coarse one.
This framework has several major drawbacks. First, the spectral filter coefficients are basis dependent,
and consequently, a spectral CNN model learned on one graph cannot be transferred to another graph.
Second, the computation of the forward and inverse graph Fourier transforms incur expensive O(n2 )
multiplication by the matrices Φ, ΦT, as there is no FFT-like algorithms on general graphs. Third,
there is no guarantee that the filters represented in the spectral domain are localized in the spatial
domain (locality property simulates local reception fields, Coates & Ng (2011)); assuming k = O(n)
Laplacian eigenvectors are used, a spectral convolutional layer requires O(pqk) = O(n) parameters
to train.
(1)
.
2
Under review as a conference paper at ICLR 2018
To address the latter issues, Henaff et al. (2015) argued that smooth spectral filter coefficients result
in spatially-localized filters (an argument similar to vanishing moments). The filter coefficients are
represented as ^ = g(λ%), where g(λ) is a smooth transfer function of frequency λ. Applying such
filter to signal f can be expressed as Gf = g(∆)f = Φg(Λ)ΦTf = Φ diag(g(λ 1),..., g(λn))ΦTf,
where applying a function to a matrix is understood in the operator functional calculus sense (applying
the function to the matrix eigenvalues). Henaff et al. (2015) used parametric functions of the form
g(λ) = Er =ι ɑjβj(λ), where βι(λ),..., βr (λ) are some fixed interpolation kernels such as splines,
and α =(α1 ,. .., αr ) are the interpolation coefficients used as the optimization variables during
the network training. In matrix notation, the filter is expressed as Gf = Φdiag(Ba)ΦTf, where
B = (bij) = (βj(λi)) is a k × r matrix. Such a construction results in filters with r = O(1)
parameters, independent of the input size. However, the authors explicitly computed the Laplacian
eigenvectors Φ, resulting in high complexity.
ChebNet. Defferrard et al. (2016) used polynomial filters represented in the Chebyshev basis
r
gα (ʌ) = E αj Tj (λ)	(2)
j=0
applied to rescaled frequency λ ∈ [-1, 1]; here, α is the (r + 1)-dimensional vector of polynomial
coefficients parametrizing the filter and optimized for during the training, and Tj(λ)=2λTj-1(λ) -
Tj-2(λ) denotes the Chebyshev polynomial of degree j defined in a recursive manner with T1(λ) = λ
and T0(λ) = 1. Chebyshev polynomials form an orthogonal basis for the space of polynomials of
order r on [-1, 1]. Applying the filter is performed by gα(∆)f, where ∆ = 2λn-1∆ - I is the
rescaled Laplacian such that its eigenvalues Λ = 2λn-1Λ - I are in the interval [-1, 1].
r
Such an approach has several important advantages. First, since gα(∆) = j=0 αj Tj (∆) contains
only matrix powers, additions, and multiplications by scalar, it can be computed avoiding the explicit
expensive O(n3) computation of the Laplacian eigenvectors. Furthermore, due to the recursive
definition of the Chebyshev polynomials, the computation of the filter gα (∆)f entails applying the
Laplacian r times, resulting in O(rn) operations assuming that the Laplacian is a sparse matrix with
O(1) non-zero elements in each row (a valid hypothesis for most real-world graphs that are sparsely
connected). Second, the number of parameters is O(1) as r is independent of the graph size n. Third,
since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and a polynomial of
degree r of the Laplacian affects only r-hops, the resulting filters have guaranteed spatial localization.
A key disadvantage of Chebyshev filters is the fact that using polynomials makes it hard to produce
narrow-band filters, as such filters require very high order r, and produce unwanted non-local filters.
This deficiency is especially pronounced when the Laplacian has clusters of eigenvalues concentrated
around a few frequencies with large spectral gap (Figure 3, middle right). Such a behavior is
characteristic of graphs with community structures, which is very common in many real-world graphs,
for instance, social networks. To overcome this major drawback, we need a new class of filters, that
are both localized in space, and are able to specialize in narrow bands in frequency.
3	Cayley filters
A key construction of this paper is a family of complex filters that enjoy the advantages of Chebyshev
filters while avoiding some of their drawbacks. A Cayley polynomial of order r is a real-valued
function with complex coefficients,
r
gch(λ) = c0 + 2Re{ ECj(hλ — i)j(hλ + i)-j]	(3)
j=1
where c =(c0 ,. ..,cr ) is a vector of one real coefficient and r complex coefficients and h>0 is the
spectral zoom parameter, that will be discussed later. A Cayley filter G is a spectral filter defined on
real signals f by
r
Gf = g c ,h (∆)f = c of + 2Re { E Cj (h △-江)j (h ∆ + i I)-j f},	(4)
j=1
3
Under review as a conference paper at ICLR 2018
Figure 1: Eigenvalues of the unnormalized Laplacian h∆u of the 15-communities graph mapped on
the complex unit half-circle by means of Cayley transform with spectral zoom values (left-to-right)
h = 0.1, 1, and 10. The first 15 frequencies carrying most of the information about the communities
are marked in red. Larger values of h zoom (right) on the low frequency band.
where the parameters c and h are optimized for during training. Similarly to the Chebyshev filters,
Cayley filters involve basic matrix operations such as powers, additions, multiplications by scalars,
and also inversions. This implies that application of the filter Gf can be performed without explicit
expensive eigendecomposition of the Laplacian operator. In the following, we show that Cayley filters
are analytically well behaved; in particular, any smooth spectral filter can be represented as a Cayley
polynomial, and low-order filters are localized in the spatial domain. We also discuss numerical
implementation and compare Cayley and Chebyshev filters.
Analytic properties. Cayley filters are best understood through the Cayley transform, from which
their name derives. Denote by eiR = {eiθ : θ ∈ R} the unit complex circle. The Cayley transform
C(x) = χ+i is a smooth bijection between R and eR \ {1}. The complex matrix C(h∆) =
(h∆ - iI)(h∆ + iI)-1 obtained by applying the Cayley transform to the scaled Laplacian h∆ has
its spectrum in eR and is thus unitary. Since N-1 = Z for Z ∈ eir, we can write CjCj(h∆)=
CC-j(h∆). Therefore, using 2Re{z} = Z + Z, any Cayley filter (4) can be written as a ConjUgate-
even Laurent polynomial w.r.t. C(h∆),
r
G = c o I + E CjCj (h ∆) + cjC-j(h ∆.
j=1
(5)
Since the spectrum of C(h∆) is in eiR, the operator Cj(h∆) can be thought of as a multiplication by
a pure harmonic in the frequency domain eiR for any integer power j ,
Cj(h∆) = Φdiag([C(hλι)]j,..., [C(hλn)]j)Φτ.
A Cayley filter can be thus seen as a multiplication by a finite Fourier expansions in the frequency
domain eiR . Since (5) is conjugate-even, it is a (real-valued) trigonometric polynomial.
Note that any spectral filter can be formulated as a Cayley filter. Indeed, spectral filters g(∆)
are specified by the finite sequence of values g(λ1),..., g(λn), which can be interpolated by a
trigonometric polynomial. Moreover, since trigonometric polynomials are smooth, we expect low
order Cayley filters to be well localized in some sense on the graph, as discussed later.
Finally, in definition (4) we use complex coefficients. If cj ∈ R then (5) is an even cosine polynomial,
and if cj ∈ iR then (5) is an odd sine polynomial. Since the spectrum of h∆is in R+, it is mapped to
the lower half-circle by C , on which both cosine and sine polynomials are complete and can represent
any spectral filter. However, it is beneficial to use general complex coefficients, since complex Fourier
expansions are overcomplete in the lower half-circle, thus describing a larger variety of spectral filters
of the same order without increasing the computational complexity of the filter.
Spectral zoom. To understand the essential role of the parameter h in the Cayley filter, consider
C(h∆). Multiplying ∆ by h dilates its spectrum, and applying C on the result maps the non-negative
spectrum to the complex half-circle. The greater h is, the more the spectrum of h∆ is spread apart in
R+, resulting in better spacing of the smaller eigenvalues of C(h∆). On the other hand, the smaller
h is, the further away the high frequencies of h∆ are from ∞, the better spread apart are the high
frequencies of C(h∆) in eiR (see Figure 1). Tuning the parameter h allows thus to ‘zoom’ in to
different parts of the spectrum, resulting in filters specialized in different frequency bands.
Numerical properties. The numerical core of the Cayley filter is the computation of Cj (h∆)f for
j =1,...,r, performed in a sequential manner. Let y0,...,yr denote the solutions of the following
4
Under review as a conference paper at ICLR 2018
linear recursive system,
y0 = f,	(h∆ + iI)yj =(h∆ - iI)yj-1 ,j=1,...,r.	(6)
Note that sequentially approximating yj in (6) using the approximation of yj-1 in the rhs is stable,
since C(h∆) is unitary and thus has condition number 1.
Equations (6) can be solved with matrix inversion exactly, but it costs O(n3). An alternative is to
use the Jacobi method,1 which provides approximate solutions yj ≈ yj. Let J = 一(Diag(h∆ +
iI))-1Off(h∆+iI) be the Jacobi iteration matrix associated with equation (6). For the unnormalized
Laplacian, J =(hD + iI)-1hW. Jacobi iterations for approximating (6) for a given j have the form
yjk +1) = Jyjk) + bj,	bj = (Diag(h∆ + iI)厂 1(h∆ 一I)yj-1,	(7)
initialized with y(0) = bj and terminated after K iterations, yielding yj = y(K). The application of
r
the approximate Cayley filter is given by Gf = Ej=O Cjyj ≈ Gf, and takes O(rKn) operations
under the previous assumption of a sparse Laplacian. The method can be improved by normalizing
H yjn 2 = H 印 2.
Next, we give an error bound for the approximate filter. For the unnormalized Laplacian, let
d = maxj {dj,j} and K = HJb = √hhd2 + ι < 1. For the normalized Laplacian, we assume that
(h∆n + iI) is dominant diagonal, which gives K = H J b < 1.
..	.	.	.	.	UCr ^ʧn,	_ _ r/ .	_ _	------
Proposition 1. Under the above assumptions, -f-— ≤ MKK, where M = √n E(=i j ∣Cj ∣ in
the general case, and M =	jr=1 j |cj | if the graph is regular.
Proposition 1 is pessimistic in the general case, while requires strong assumptions in the regular
case. We find that in most real life situations the behavior is closer to the regular case. It also follows
from Proposition 1 that smaller values of the spectral zoom h result in faster convergence, giving this
parameter an additional numerical role of accelerating convergence.
Complexity. In practice, an accurate inversion of (h∆ + iI) is not required, since the approximate
inverse is combined with learned coefficients, which “compensate”, as necessary, for the inversion
inaccuracy. In a CayleyNet for a fixed graph, we fix the number of Jacobi iterations. Since the
convergence rate depends on K, that depends on the graph, different graphs may need different
numbers of iterations. The convergence rate also depends on h. Since there is a trade-off between the
spectral zoom amount h, and the accuracy of the approximate inversion, and since h is a learnable
parameter, the training finds the right balance between the spectral zoom amount and the inversion
accuracy. We study the computational complexity of our method, as the number of edges n of the
graph tends to infinity. For every constant of a graph, e.g d, K, we add the subscript n, indicating
the number of edges of the graph. For the unnormalized Laplacian, we assume that dn and hn are
bounded, which gives Kn <a< 1 for some a independent of n. For the normalized Laplacian, we
assume that Kn <a<1. By Theorem 1, fixing the number of Jacobi iterations K and the order of the
filter r, independently of n, keeps the Jacobi error controlled. As a result, the number of parameters
is O(1), and for a Laplacian modeled as a sparse matrix, applying a Cayley filter on a signal takes
O(n) operations.
Localization. Unlike Chebyshev filters that have the small r-hop support, Cayley filters are rational
functions supported on the whole graph. However, it is still true that Cayley filters are well localized
on the graph. Let G be a Cayley filter and δm denote a delta-function on the graph, defined as one at
vertex m and zero elsewhere. We show that Gδm decays fast, in the following sense:
Definition 2 (Exponential decay on graphs). Let f be a signal on the vertices of graph G, 1 ≤ p ≤∞,
and 0 < e < L Denote by S ⊆ {1 ,...,n} a subset of the vertices and by Sc its complement. We
say that the Lp-mass of f is supported in S up to E if ∣∣f|s0 ∣∣p ≤ e ∣∣fHp, where f ∣sc = (f)心。is the
restriction of f to Sc. We say that f has (graph) exponential decay about vertex m, if there exists
some γ ∈ (0, 1) and c>0 such that for any k, the Lp-mass off is supported in Nk,m up to cγk.
Here, Nk,m is the k-hop neighborhood of m.
1We remind that the Jacobi method for solving Ax = b consists in decomposing A = Diag(A) + Off(A)
and obtaining the solution iteratively as x(k+1) = -(Diag(A))-1 Off (A)x(k) + (Diag(A))-1 b.
5
Under review as a conference paper at ICLR 2018
I(γ)^l
Figure 2: Filters (spatial domain, top and spectral domain, bottom) learned by CayleyNet (left) and
ChebNet (center, right) on the MNIST dataset. Cayley filters are able to realize larger supports for
the same order r .
Remark 3. Note that Definition 2 is analogous to classical exponential decay on Euclidean space:
∣f (x) | ≤ RY-X ifffor every ball BP of radius ρ about 0, ^f ∣bc ∣∣m ≤ cγ-ρ ∣∣f ∣∣g With C = ^fR .
Theorem 4. Let G be a Cayley filter of order r. Then, Gδm has exponential decay about m in L2,
with constants c = 2M RGδ r? and Y = K1 /r (where M and K arefrom Proposition 1).
Cayley vs Chebyshev. Below, we compare the two classes of filters:
Chebyshev as a special case of Cayley. For a regular graph with D = dI, using Jacobi inversion
based on zero iterations, we get that any Cayley filter of order r is a polynomial of ∆ in the monomial
base (h∆+ii)j. In this situation, a ChebysheV filter, which is a real valued polynomial of ∆, is a
special case of a Cayley filter.
Spectral zoom and stability. Generally, both Chebyshev polynomials and trigonometric polynomials
give stable approximations, optimal for smooth functions. However, this crude statement is over-
simplified. One of the drawbacks in Chebyshev filters is the fact that the spectrum of ∆ is always
mapped to [-1, 1] in a linear manner, making it hard to specialize in small frequency bands. In
Cayley filters, this problem is mitigated with the help of the spectral zoom parameter h. As an
example, consider the community detection problem discussed in the next section. A graph with
strong communities has a cluster of small eigenvalues near zero. Ideal filters g(∆) for extracting
the community information should be able to focus on this band of frequencies. Approximating
such filters with Cayley polynomials, we zoom in to the band of interest by choosing the right h,
and then project g onto the space of trigonometric polynomials of order r, getting a good and stable
approximation (Figure 3, bottom right). However, if we project g onto the space of Chebyshev
polynomials of order r, the interesting part of g concentrated on a small band is smoothed out and
lost (Figure 3, middle right). Thus, projections are not the right way to approximate such filters, and
the stability of orthogonal polynomials cannot be invoked. When approximating g on the small band
using polynomials, the approximation will be unstable away from this band; small perturbations in g
will result in big perturbations in the Chebyshev filter away from the band. For this reason, we say
that Cayley filters are more stable than Chebyshev filters.
Regularity. We found that in practice, low-order Cayley filters are able to model both very concentrated
impulse-like filters, and wider Gabor-like filters. Cayley filters are able to achieve a wider range of
filter supports with less coefficients than Chebyshev filters (Figure 2), making the Cayley class more
regular than Chebyshev.
Complexity. Under the assumption of sparse Laplacians, both Cayley and Chebyshev filters incur
linear complexity O(n). Besides, the new filters are equally simple to implement as Chebyshev
filters; as seen in Eq.7, they boil down to simple sparse matrix-vector multiplications providing a
GPU friendly implementation.
4	Results
Experimental settings. We test the proposed CayleyNets reproducing the experiments of Deffer-
rard et al. (2016); Kipf & Welling (2016); Monti et al. (2017a;a) and using ChebNet (Defferrard
et al. (2016)) as our main baseline method. All the methods were implemented in TensorFlow of
M. Abadi et. al. (2016). The experiments were executed on a machine with a 3.5GHz Intel Core i7
CPU, 64GB of RAM, and NVIDIA Titan X GPU with 12GB of RAM. SGD+Momentum and Adam
6
Under review as a conference paper at ICLR 2018
(Kingma & Ba (2014)) optimization methods were used to train the models in MNIST and the rest of
the experiments, respectively. Training and testing were always done on disjoint sets.
Community detection. We start with an experiment on a synthetic graph consisting of 15 commu-
nities with strong connectivity within each community and sparse connectivity across communities
(Figure 3, left). Though rather simple, such a dataset allows to study the behavior of different algo-
rithms in controlled settings. On this graph, we generate noisy step signals, defined as fi = 1+σi if
i belongs to the community, and f = σi otherwise, where σi 〜N(0, 0.3) is Gaussian i.i.d. noise.
The goal is to classify each such signal according to the community it belongs to. The neural network
architecture used for this task consisted of a spectral convolutional layer (based on Chebyshev or
Cayley filters) with 32 output features, a mean pooling layer, and a softmax classifier for producing
the final classification into one of the 15 classes. The classification accuracy is shown in Figure
3 (right, top) along with examples of learned filters (right, bottom). We observe that CayleyNet
significantly outperforms ChebNet for smaller filter orders, with an improvement as large as 80%.
Studying the filter responses, we note that due to the capability to learn the spectral zoom parameter,
CayleyNet allows to generate band-pass filters in the low-frequency band that discriminate well the
communities ( Figure 3 bottom right).
Figure 3: Left: synthetic 15-communities graph. Right: community detection accuracy of ChebNet
and CayleyNet (top); normalized responses of four different filters learned by ChebNet (middle)
and CayleyNet (bottom). Grey vertical lines represent the frequencies of the normalized Laplacian
(λ = 2λ-1 λ - 1 for ChebNet and C(λ) = (hλ - i)/(hλ + i) unrolled to a real line for CayleyNet).
Note how thanks to spectral zoom property Cayley filters can focus on the band of low frequencies
(dark grey lines) containing most of the information about communities.
Complexity. We experimentally validated the computational complexity of our model applying
filters of different order r to synthetic 15-community graphs of different size n using exact matrix
inversion and approximation with different number of Jacobi iterations (Figure 4 center and right,
Figure 6 in the appendix). All times have been computed running 30 times the considered models
and averaging the final results. As expected, approximate inversion guarantees O(n) complexity. We
further conclude that typically very few Jacobi iterations are required (Figure 4, left shows that our
model with just one Jacobi iteration outperforms ChebNet for low-order filters on the community
detection problem).
MNIST. Following Defferrard et al. (2016); Monti et al. (2017a), for a toy example, we approached
the classical MNIST digits classification as a learning problem on graphs. Each pixel of an image
is a vertex of a graph (regular grid with 8-neighbor connectivity), and pixel color is a signal on the
graph. We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev
and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling
layers performing 4-times graph coarsening using the Graclus algorithm (Dhillon et al. (2007)), and
7
Under review as a conference paper at ICLR 2018
Figure 4: Left: community detection test accuracy as function of filter order r. Center and right:
computational complexity (test times on batches of 100 samples) as function of filter order r and
graph size n. Shown are exact matrix inversion (dashed) and approximate Jacobi with different
number of iterations (colored). For reference, ChebNet is shown (dotted).
finally a fully-connected layer (this architecture replicates the classical LeNet5, LeCun et al. (1998),
architecture, which is shown for comparison). MNIST classification results are reported in Table 1.
CayleyNet (11 Jacobi iterations) achieves the same (near perfect) accuracy as ChebNet with filters
of lower order (r = 12 vs 25).Examples of filters learned by ChebNet and CayleyNet are shown in
Figure 2. 0.1776 +/- 0.06079 sec and 0.0268 +/- 0.00841 sec are respectively required by CayleyNet
and ChebNet for analyzing a batch of 100 images at test time.
Table 1: Test accuracy obtained with different methods on the MNIST dataset.			Table 2: Test accuracy of different methods on the CORA dataset. Scaled LaPlacian and normalized LaPlacian with real Polynomials of order 1 have been resPectively exPloited for CayleyNet and ChebNet.
Model	Order Accuracy #Params		
LeNet5	-99.33%	1.66M	Method	Accuracy #Params
ChebNet CayleyNet	25	99.14% 12	99.18%	1.66M 1.66M	DCNN (Atwood & Towsley (2016)) 86.01 %	47K GCN (KiPf & Welling (2016))	86.64 %	47K
			ChebNet (Defferrard et al. (2016))	87.07 %	46K CayleyNet	88.09 %	46K
Citation network. Next, we address the problem of vertex classification on graphs using the
popular CORA citation graph, Sen et al. (2008). Each of the 2708 vertices of the CORA graph
represents a scientific paper, and an undirected unweighted edge represents a citation (5429 edges
in total). For each vertex, a 1433-dimensional binary feature vector representing the content of
the paper is given. The task is to classify each vertex into one of the 7 groundtruth classes. We
split the graph into training (1,708 vertices), validation (500 vertices) and test (500 vertices) sets,
for simulating the labeled and unlabeled information. We train ChebNet and CayleyNet with the
architecture presented in Kipf & Welling (2016); Monti et al. (2017a) (two spectral convolutional
layers with 16 and 7 outputs), DCNN (Atwood & Towsley (2016)) with 2 diffusion layer (10 hidden
features and 2 diffusion hops) and GCN (Kipf & Welling (2016)) with 3 convolutional layer (32
and 16 hidden features). Figure 5 compares ChebNets and CayleyNets, in a number of different
settings. Since ChebNets require Laplacians with spectra bounded in [-1, 1], we consider both the
normalized LaPlacian (the two left figures), and the scaled unnormalized LaPlacian (2∆/λmaχ — I),
where ∆ is the unnormalized Laplacian and λmax is its largest eigenvalue (the two right figures).
For fair comParison, we fix the order of the filters (toP figures), and fix the overall number of
network Parameters (bottom figures). In the bottom figure, the Cayley filters are restricted to even
cosine Polynomials by considering only real filter coefficients. Table 2 shows a comParison of the
Performance obtained with different methods (all architectures roughly Present the same amount of
Parameters). The best CayleyNets consistently outPerform the best comPetitors.
Recommender system. In our final exPeriment, we aPPlied CayleyNet to recommendation system,
formulated as matrix comPletion Problem on user and item graPhs, Monti et al. (2017a). The task
is, given a sParsely samPled matrix of scores assigned by users (columns) to items (rows), to fill in
the missing scores. The similarities between users and items are given in the form of column and
row graPhs, resPectively. Monti et al. (2017a) aPProached this Problem as learning with a Recurrent
GraPh CNN (RGCNN) architecture, using an extension of ChebNets to matrices defined on multiPle
graPhs in order to extract sPatial features from the score matrix; these features are then fed into an
8
Under review as a conference paper at ICLR 2018
Normalized Laplacian
% ycaruccA % ycarucc
88
86
84
82
187.1 87.9,86.6 86.9,86.2 87.1,85.2 86.6 ∣84.9∣86.5Hs4.5∣86.8∣
1
2
3
4
5
6
Order r
88
86
84
82
46K	69K	92K	115K	138K	161K
#Params
% ycaruccA % ycarucc
90
Scaled unnormalized Laplacian
90
85
80
75
Order r
85
80
75
46K	69K	92K	115K	138K	161K
#Params
Figure 5: ChebNet (blue) and CayleyNet (orange) test accuracies obtained on the CORA dataset
for different polynomial orders. Polynomials with complex coefficients (top) and real coefficients
(bottom) have been exploited with CayleyNet in the two analysis. Orders 1 to 6 have been used in
both comparisons. The best CayleyNet consistently outperform the best ChebNet requiring at the
same time less parameters (CayleyNet with order r and complex coefficients requires a number of
parameters equal to ChebNet with order 2r).
RNN producing a sequential estimation of the missing scores. Here, we repeated verbatim their
experiment on the MovieLens dataset (Miller et al. (2003)), replacing Chebyshev filters with Cayley
filters. We used separable RGCNN architecture with two CayleyNets of order r = 4 employing 15
Jacobi iterations. The results are reported in Table 3. To present a complete comparison we further
extended the experiments reported in Monti et al. (2017a) by training sRGCNN with ChebNets of
order 8, this provides an architecture with same number of parameters as the exploited CayleyNet
(23K coefficients). Our version of sRGCNN outperforms all the competing methods, including the
previous result with Chebyshev filters reported in Monti et al. (2017a). sRGCNNs with Chebyshev
polynomials of order 4 and 8 respectively require 0.0698 +/- 0.00275 sec and 0.0877 +/- 0.00362 sec
at test time, sRGCNN with Cayley polynomials of order 4 and 15 jacobi iterations requires 0.165 +/-
0.00332 sec.
Table 3: Performance (RMSE) of different matrix completion methods on the MovieLens dataset.
Method	RMSE
MC (Candes & Recht (2012))	0.973
IMC (Jain & Dhillon (2013); Xu et al. (2013))	1.653
GMC (Kalofolias et al. (2014))	0.996
GRALS (Rao et al. (2015))	0.945
sRGCNNCheby,r=4 (Monti et al. (2017a))	0.929
sRGCNNCheby,r=8 (Monti et al. (2017a))	0.925
SRGCNNCayley	0.922
5	Conclusions
In this paper, we introduced a new efficient spectral graph CNN architecture that scales linearly with
the dimension of the input data. Our architecture is based on a new class of complex rational Cayley
filters that are localized in space, can represent any smooth spectral transfer function, and are highly
regular. The key property of our model is its ability to specialize in narrow frequency bands with a
small number of filter parameters, while still preserving locality in the spatial domain. We validated
these theoretical properties experimentally, demonstrating the superior performance of our model in a
broad range of graph learning problems.
9
Under review as a conference paper at ICLR 2018
References
J. Atwood and D. Towsley. Diffusion-convolutional neural networks. arXiv:1511.02136v2, 2016.
D.	Boscaini, J. Masci, S. Melzi, M. M. Bronstein, U. Castellani, and P. Vandergheynst. Learning
class-specific descriptors for deformable shapes using localized spectral convolutional networks.
Computer Graphics Forum, 34(5):13-23, 2015. ISSN 1467-8659.
D. Boscaini, J. Masci, E. RodolW, and M. M. Bronstein. Learning shape correspondence with
anisotropic convolutional neural networks. In Proc. NIPS, 2016a.
D. Boscaini, J. Masci, E. RodolW, M. M. Bronstein, and D. Cremers. Anisotropic diffusion descriptors.
Computer Graphics Forum, 35(2):431-441, 2016b.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:
going beyond euclidean data. arXiv:1611.08097, 2016.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks
on graphs. Proc. ICLR, 2013.
E.	Candes and B. Recht. Exact matrix completion via convex optimization. Comm. ACM, 55(6):
111-119, 2012.
A. Coates and A.Y. Ng. Selecting Receptive Fields in Deep Networks. In Proc. NIPS, 2011.
M. Defferrard, X. Bresson, and P. Vandergheynst. Convolutional neural networks on graphs with fast
localized spectral filtering. In Proc. NIPS, 2016.
Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Weighted graph cuts without eigenvectors a
multilevel approach. Trans. PAMI, 29(11), 2007.
D. K. Duvenaud et al. Convolutional networks on graphs for learning molecular fingerprints. In Proc.
NIPS, 2015.
M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. In Proc.
IJCNN, 2005.
Yotam Hechtlinger, Purvasha Chakravarti, and Jining Qin. A generalization of convolutional neural
networks to graph-structured data. arXiv:1704.08165, 2017.
M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data.
arXiv:1506.05163, 2015.
P. Jain and I. S. Dhillon. Provable inductive matrix completion. arXiv:1306.0626, 2013.
V. Kalofolias, X. Bresson, M. M. Bronstein, and P. Vandergheynst. Matrix completion on graphs.
arXiv:1408.1717, 2014.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.
T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks.
arXiv:1609.02907, 2016.
Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and Daniel
Rueckert. Distance metric learning using graph convolutional networks: Application to functional
brain networks. arXiv:1703.02161, 2017.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proc. IEEE, 86(11):2278-2324, 1998.
Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks.
arXiv:1511.05493, 2015.
M. Abadi et. al. Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.
arXiv preprint arXiv:1603.04467, 2016.
10
Under review as a conference paper at ICLR 2018
J. Masci, D. Boscaini, M. M. Bronstein, and P. Vandergheynst. Geodesic convolutional neural
networks on Riemannian manifolds. In Proc. 3DRR, 2015.
B. N. Miller et al. MovieLens unplugged: experiences with an occasionally connected recommender
system. In Proc. Intelligent User Interfaces, 2003.
F. Monti, D. Boscaini, J. Masci, E. RodolW, J. Svoboda, and M. M. Bronstein. Geometric deep
learning on graphs and manifolds using mixture model CNNs. In Proc. CVPR, 2017a.
Federico Monti, Michael M Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. Proc. NIPS, 2017b.
N. Rao, H.-F. Yu, P. K. Ravikumar, and I. S. Dhillon. Collaborative filtering with graph information:
Consistency and scalable methods. In Proc. NIPS, 2015.
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network
model. IEEE Trans. NeuralNetworks, 20(1):61-80, 2009.
P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in
network data. AI Magazine, 29(3):93, 2008.
D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst. The emerging field of
signal processing on graphs: Extending high-dimensional data analysis to networks and other
irregular domains. IEEE Sig. Proc. Magazine, 30(3):83-98, 2013.
S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation.
arXiv:1605.07736, 2016.
M. Xu, R Jin, and Z.-H. Zhou. Speedup matrix completion with side information: Application to
multi-label learning. In Proc. NIPS, 2013.
11
Under review as a conference paper at ICLR 2018
Appendix
proof of proposition 1
First note the following classical result for the approximation of Ax = b using the Jacobi method:
if the initial condition is x(0) = 0, then (x 一 x(k)) = Jk x. In our case, note that if we start with
initial condition y；0)= 0, the next iteration gives y；0)= bj, which is the initial condition from our
construction. Therefore, since we are approximating yj = C(h∆)yj-1 by yj = yjk), we have
C(h∆)yj-1 一 yj = JK+1C(h∆)y-1	(8)
Define the approximation error in C(h∆)jf by
=Il Cj h A)f - y∕∣2
e	HC j (h ∆)f∣∣ 2	∙
By the triangle inequality, by the fact that Cj (h∆) is unitary, and by (8)
≤ Il Cj(h Hf — C (h A)%-1∣∣2 + HC (h ∆)y-ι - y ∣∣ ?
ej -	HCj (h∆)fH2	+	HCj (h∆)fH2
=j^∆f∑j1 , IlJK+1C(h∆)yj-1∣∣2
=HCj-1( h ∆)f H 2	+	H f H 2	(9)
《尸 I ∣∣[K+1∣∣ IIc (h △)%-1 Il 2	I Il K++1∣ H Nj-1 H 2
-ejT + ∣∣j	∣∣2	H<= j 1 + ∣∣j ∣∣2 -Jf∣Γ
-ej-1+ ∣∣j k+1∣∣2 (I+ ej-1)
where the last inequality is due to
Hyj-1H2 - ∣∣cj-1(h∆)f∣∣2 + ∣∣Cj-1(h∆)f — yj-1∣∣2 = HfH2 + HfH2 ej-1∙
Now, using standard norm bounds, in the general case we have∣∣JK+1∣∣2 -√q∣JK +1L∙ Thus,
by K = H JHg we have
ej — ej-1 + √n HJ∣∣g+1 (1 + ej-1) = (1 + √nκκ+1)ej-1 + √γlkk+1∙
The solution of this recurrent sequence is
ej — (1 + √nκκ+1)j — 1 = j√nκκ +1 + O (K2 κ+2) ∙
If we use the version of the algorithm, in which each Nj is normalized, we get by (9) ej — ej-1 +
√nκκ +1. The solution of this recurrent sequence is
ej — j√nκκ+1 ∙
We denote in this case Mj = j√n
In case the graph is regular, we have D = dI. In the non-normalized Laplacian case,
hh
J = T hd i + 汨-1 h (A - d D = E(d i - A) = E W ∙	(10)
The spectral radius of A is bounded by 2d. This can be shown as follows. a value λ is not an
eigenvalue of A (namely it is a regular value) if and only if (A - λI) is invertible. Moreover, the
matrix (A - λI) is strictly dominant diagonal for any ∣λ∣ > 2d. By Levy-Desplanques theorem, any
strictly dominant diagonal matrix is invertible, which means that all of the eigenvalues of A are less
than 2d in their absolute value. As a result, the spectral radius of (dI - A) is realized on the smallest
eigenvalue of A, namely it is ∣d - 01 = d. This means that the specral radius of J is ^+1 ∙ AS a
result H J H 2 = √h吃+1 = K. We can now continue from (9) to get
ej - ej-1 + Il jHK +1 (1 + ej-1) = ej-1 + KK +1 (1 + ej-1)∙
12
Under review as a conference paper at ICLR 2018
As before, we get e§ ≤ jκκ+1 + O(R2K+2), and e§ ≤ jκκ+1 if each yj is normalized. We denote
in this case Mj = j.
In the case of the normalized Laplacian of a regular graph, the spectral radius of ∆n is bounded by
2, and the diagonal entries are all 1. Equation (10) in this case reads J =总(I 一 ∆n), and J has
spectral radius ^hh^ɪ. Thus ∣∣ J∣∣2 = Vhh+ɪ = K and we continue as before to get ej ≤ jκκ+1 and
Mj = j .
In all cases, we have by the triangle inequality
Gf 一Gf IL
∣ f 12
r
≤ E心I
j=1
IICj(hA)f - yj∣2
IICj (h ∆)f∣ 2
rr
E ∣Cj I ej≤ E Mj ∣Cj I
j=1	j=1
rk+1
Proof of Theorem 4
In this proof we approximate Gδm by Gδm. Note that the signal δm is supported on one vertex,
and in the calculation of Gδm, each Jacobi iteration increases the support of the signal by 1-hop.
Therefore, the support of Gδm is the r (K + 1)-hop neighborhood Nr(K +1) ,m of m. Denoting
l = r(K +1), and using Proposition 1, we get
I∣G δm 一 G δm∣Nt,m∣∣2 ≤ ∣∣G δm 一 Gδm∣I2 + ∣∣Gδm 一 G δm IMmIL
≤ ∣∣gδm 一 Gδm II2 + ∣∣Gδm - Gδm (	(“)
= 2∣∣G δm 一 Gδm∣∣2 ≤ 2 MKK+1 MmIl 2
=2 M (K1 /)l.
13
Under review as a conference paper at ICLR 2018
Computational Complexity
• ChebNet —CayleyNet (1 iteration) CayleyNet (5 iterations)
—CayleyNet (9 iterations) —CayleyNet (13 iterations) 一 •一 CayleyNet (inversion)
Testing
gəs) sω1ml
0.12
0.10
0.08
0.06
0.04
0.02
0.00
5 0 5 0
3 3 2 2
5 0 5 0
1 1
SWH ə 日口
1	3	5	7	9	11	13
Order r
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1	3	5	7	9	11	13
Order r
16
14
12
10
8
6
4
2
0
Illll
200	400	600	800	1,000
#Vertices n
200	400	600	800	1,000
#VertiCeS n
Training
0.40
0.35
-0.30
0 0.25
二 0.20
0 0. 15
口 0. 10
0.05
0.00
1	3	5	7	9	11	13
Order r
s∙2⅛x ə ELL
0.30
0.25
0.20
0. 15
0. 10
0.05
0.00
200	400	600	800
#Vertices n
SoH
16
14
12
10
8
6
4
2
0
200	400	600	800	1,000
1,000
#Vertices n
(oəs) səmɪɪ
Figure 6: Test (above) and training (below) times with corresponding ratios as function of filter order
r and graph size n on our community detection dataset.
14