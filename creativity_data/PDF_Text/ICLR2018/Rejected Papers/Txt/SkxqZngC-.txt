Under review as a conference paper at ICLR 2018
A Bayesian Nonparametric Topic Model with
Variational Auto-Encoders
Anonymous authors
Paper under double-blind review
Ab stract
Topic modeling of text documents is one of the most important tasks in represen-
tation learning. In this work, we propose iTM-VAE, which is a Bayesian non-
parametric (BNP) topic model with variational auto-encoders. On one hand, as a
BNP topic model, iTM-VAE potentially has infinite topics and can adapt the topic
number to data automatically. On the other hand, different with the other BNP
topic models, the inference of iTM-VAE is modeled by neural networks, which
has rich representation capacity and can be computed in a simple feed-forward
manner. Two variants of iTM-VAE are also proposed in this paper, where iTM-
VAE-Prod models the generative process in products-of-experts fashion for bet-
ter performance and iTM-VAE-G places a prior over the concentration parameter
such that the model can adapt a suitable concentration parameter to data automat-
ically. Experimental results on 20News and Reuters RCV1-V2 datasets show that
the proposed models outperform the state-of-the-arts in terms of perplexity, topic
coherence and document retrieval tasks. Moreover, the ability of adjusting the
concentration parameter to data is also confirmed by experiments.
1	Introduction
Probabilistic topic models focus on discovering the abstract “topics” that occur in a collection of
documents and representing a document as a weighted mixture of the discovered topics. Classical
topic models, the most popular being LDA (Blei et al., 2003), have achieved success in a range
of applications, such as information retrieval (Wei & Croft, 2006), document understanding (Blei
et al., 2003), computer vision (Rasiwasia & Vasconcelos, 2013) and bioinformatics (Rogers et al.,
2005). A major challenge of topic models is that the inference of the distribution over topics does not
have a closed-form solution and must be approximated, using either MCMC sampling or variational
inference. Hence, any small change to the model requires re-designing a new inference method
tailored for it. Moreover, as the model grows more expressive, the inference becomes increasingly
complex, which becomes the bottleneck to discover the latent semantic structures of complicated
data. Hence, black-box inference methods (Ranganath et al., 2014; Mnih & Gregor, 2014; Kingma
& Welling, 2014b; Rezende et al., 2014), which require only limited knowledge from the models
and can be flexibly applied to new models, is desirable for topic models.
Among all the black-box inference methods, Auto-Encoding Variational Bayes (AEVB) (Kingma
& Welling, 2014b; Rezende et al., 2014) is a promising one for topic models. AEVB contains an
inference network that can map a document directly to a variational posterior without the need for
further local variational updates on test data, and the Stochastic Gradient Variational Bayes (SGVB)
estimator allows efficient approximate inference for a broad class of posteriors, which makes topic
models more flexible. Hence, an increasing number of work has been proposed recently to combine
topic models with AEVB, such as (Miao et al., 2016; Srivastava & Sutton, 2017; Card et al., 2017;
Miao et al., 2017).
Deciding the number of topics is another challenge for topic models. One option is to use model
selection, which trains models with different topic numbers and selects the best on the validation set.
Bayesian nonparametric (BNP) topic models, however, side-step this issue by making the number
of topics adaptive to data. For example, Teh et al. (2006) proposed Hierarchical Dirichlet Process
(HDP), which models each document with a Dirichlet Process (DP) and all DPs for documents in
a corpus share a base distribution that is itself also from a DP. HDP extends LDA in that it can
1
Under review as a conference paper at ICLR 2018
adapt the number of topics to data. Hence, HDP has potentially an infinite number of topics and
allows the number to grow as more documents are observed. Unlike the black-box inference based
models, traditionally, one needs to redesign the inference methods when there are some changes in
the generative process of HDP (Teh et al., 2006; Wang et al., 2011; Hughes et al., 2015).
In this work, we make progress on this problem by proposing an infinite Topic Model with Vari-
ational Auto-Encoders (iTM-VAE), which is a Bayesian nonparametric topic model with AEVB.
Coupling Bayesian nonparametric techniques with deep neural networks, iTM-VAE is able to cap-
ture the uncertainty regarding to the number of topics, and the inference can be conducted in a
simple feed-forward manner. More specifically, iTM-VAE uses a stick-breaking process (Sethura-
man, 1994) to generate the mixture weights for a countably infinite set of topics, and use neural
networks to approximate the variational posteriors. The main contributions of the paper are:
•	We propose iTM-VAE, which, to our best knowledge, is the first Bayesian nonparametric
topic model equipped with AEVB.
•	We propose iTM-VAE-Prod whose distribution over words is a product of experts rather
than a mixture of multinomials.
•	We propose iTM-VAE-G, which helps the model to adjust the concentration parameter to
data automatically.
•	The experimental results show that iTM-VAE and its two variants outperform the state-of-
the-art models on two challenging benchmarks significantly.
2	Related Work
Topic models have been studied extensively in a variety of applications such as document model-
ing, information retrieval, computer vision and bioinformatics (Blei et al., 2003; Wei & Croft, 2006;
Putthividhy et al., 2010; Rasiwasia & Vasconcelos, 2013; Rogers et al., 2005). Please see (Blei,
2012) for an overview. Recently, with the impressive success of deep learning, neural topic mod-
els have been proposed and achieved encouraging performance in document modeling tasks, such
as Replicated Softmax (Hinton & Salakhutdinov, 2009), DocNADE (Larochelle & Lauly, 2012),
fDARN (Mnih & Gregor, 2014) and NVDM (Miao et al., 2016). These models achieved compet-
itive performance on modeling documents. However, they do not explicitly model the generative
story of documents, hence are less explainable.
Several recent work has been proposed to model the generative procedure explicitly and the in-
ference of the topic distributions is computed by deep neural networks. This makes these models
interpretable, powerful and easily extendable. For example, Srivastava & Sutton (2017) proposed
AVITM model, which embeds the original LDA (Blei et al., 2003) formulation with AEVB. By
utilizing Laplacian approximation for the Dirichlet distribution, AVITM can be optimized by the
Stochastic Gradient Variational Bayes (SGVB) (Kingma & Welling, 2014b; Rezende et al., 2014)
estimator efficiently. AVITM achieved the state-of-the-art performance on the topic coherence met-
ric (Lau et al., 2014), which indicates the topics learned match closely to human judgment.
The Bayesian nonparametric topic models Teh et al. (2006); Kim & Sudderth (2011); Archambeau
et al. (2015); Lim et al. (2016), potentially have infinite topic capacity and are able to adapt the
topic number to data. However, we do not notice any topic models that combine BNP techniques
with deep neural networks. Actually, Nalisnick & Smyth (2017) proposed Stick-Breaking VAE
(SB-VAE), which is a Bayesian nonparametric version of traditional VAE with a stochastic dimen-
sionality. However, iTM-VAE differs with SB-VAE in that it is a kind of topic model that models
discrete text data. Furthermore, we also proposed iTM-VAE-G which places a prior on the stick-
breaking process such that the model is able to adapt the concentration parameter to data. Another
related work is (Miao et al., 2017), which proposed GSM, GSB, RSB and RSB-TF to model docu-
ments. The RSB-TF from (Miao et al., 2017), which uses a heuristic indicator to guide the growth of
the topic numbers, also has the ability to adapt the topic number. However, the performance of RSB-
TF does not match its complexity. Instead, iTM-VAE exploits Bayesian nonparametric techniques
to decide the number of topics, which is much more elegant. And the performance of iTM-VAE also
outperforms RSB-TF.
2
Under review as a conference paper at ICLR 2018
3	Preliminary
In this section, we briefly describe the stick-breaking process (Sethuraman, 1994; Murphy, 2012),
Variational Auto-Encoders (Kingma & Welling, 2014b; Rezende et al., 2014) and the Kumaraswamy
distribution (Kumaraswamy, 1980), which are all essential for iTM-VAE.
3.1	The Stick-Breaking Process
We first describe the stick-breaking process, which is used to model the mixture weights over the
countably infinite topics for the generative procedure of iTM-VAE. More specifically, the stick-
breaking process generates an infinite sequence of mixture weights π = {πk}k∞=1 as follows:
k-1	k-1
Vk 〜Beta(1,α)	∏k = Vk ɪɪ(l - Vl) = Vk(1 - Enl).	(1)
l=1	l=1
This is often denoted as π 〜 GEM(α), where GEM stands for Griffiths, Engen and Mc-
Closkey (Ewens, 1990) and α is the concentration parameter. One can see that πk satisfies
0 ≤ πk ≤ 1 and Pk∞=1 πk = 1.
3.2	Variational Auto-Encoders
Variational Auto-Encoder (VAE) is among the most successful generative models recently. Specifi-
cally, it assumes that a sample x is generated mathematically as follows:
Z 〜p(z), X 〜Pφ(x∣z)	(2)
where p(z) is the prior of the latent representation Z and pφ(x∣z) is a conditional distribution with
parameters φ to generate x. The approximation to the posterior of the generative process, qψ (z|x), is
parametrized by an inference neural network with parameters ψ . According to (Kingma & Welling,
2014b; Rezende et al., 2014), the Evidence Lower Bound (ELBO) of VAE can be written as:
L(x∣φ,ψ) = Eqψ(z|x) [logPφ(x∣z)] - KL (qψ(z∣x)∣∣p(z))	(3)
where KL is the Kullback-Leibler divergence.
The parameters φ and ψ can be optimized jointly by applying the Stochastic Gradient Variational
Bayes (SGVB) (Kingma & Welling, 2014b) estimator and Reparameterization Trick (RT). An essen-
tial requirement of SGVB and RT is that the latent variable z can be represented in a differentiable,
non-centered parameterization (DNCP) (Kingma & Welling, 2014a), which allows the gradients
to go through the MC expectation. However, this is not always satisfied by the Beta prior of the
stick-breaking process for iTM-VAE. Hence, we resort to the little known Kumaraswamy distribu-
tion (Kumaraswamy, 1980) to solve this problem.
3.3	Kumaraswamy distribution
Kumaraswamy distribution (Kumaraswamy, 1980) is a continuous distribution, which is mathemat-
ically defined as:
Kumaraswamy(x; a, b) = abxa-1 (1 - xa)b-1	(4)
where x ∈ [0, 1] and a, b > 0. The inverse cumulative distribution function (CDF) can be expressed
ina simple and closed-form formulation, and samples from Kumaraswamy distribution can be drawn
by:
X = (1 — U1)1, where U 〜Uniform(0,1)	(5)
Kumaraswamy is similar to Beta distribution, yet more suitable to SGVB since it satisfies the DNCP
requirement. Moreover, the KL divergence between Kumaraswamy and Beta can be closely approx-
imated in closed-form. Hence, we use it to model the inference procedure of iTM-VAE.
3
Under review as a conference paper at ICLR 2018
4	The Model
In this section, we describe iTM-VAE, a Bayesian nonparametric topic model with VAE. Specifi-
cally, we first describe the generative process of iTM-VAE in Section 4.1, and then the inference
process is introduced in Section 4.2. After that, two variants of the model, iTM-VAE-Prod and
iTM-VAE-G, are described in Section 4.3 and Section 4.4, respectively.
4.1	The Generative Procedure
The generative process of iTM-VAE is similar to the original VAE. The key difference is that the
topic distribution (latent variable) π = {πk}k∞=1 is drawn from the GEM distribution to make sure
of Pk∞=1 πk = 1 and 0 ≤ πk ≤ 1. Specifically, we suppose each topic θk = σ(φk) is a probability
distribution over vocabulary, where φk ∈ RV is the parameter of the topic-specific word distribution,
σ(∙) is the Softmax function and V is the vocabulary size. In iTM-VAE, there are unlimited number
of topics and we denote Θ = {θk}k∞=1 and Φ = {φk }k∞=1 as the collections of these countably
infinite topics and the corresponding parameters. The generation of a document by iTM-VAE can
then be mathematically described as:
•	Draw a topic distribution π 〜 GEM(α)
•	Then we get a distribution G(θ; π, Θ) = Pk∞=1 πkδθk (θ)
•	For each word Wiin the document: 1) draw a topic θi 〜G(θ; π, Θ); 2) Wi 〜Cat(θi)
where α is a hyper-parameter, Cat(θi) is a categorical distribution parameterized by θi, and δθk (θ)
is a discrete dirac function, which equals to 1 when θ = θk and 0 otherwise.
Thus, the joint probability of a document with N words w1:N = {Wi}iN=1, the topic distribution π
and the sampled topics θ±N = {θi}N=ι can be written as:
N
/	A I ʌʌʌ / I ʌ π / IA ʌ /A I ʌʌʌ	“、
P(W1：N, ∏, θi:N∣α, Θ) = p(∏∣α) ]]p(wi∣θi)p(θi∣∏, Θ)	(6)
i=1
wherep(∏∣α) = GEM(α), p(θ∣∏, Θ) = G(θ; ∏, Θ) andp(w∣θ) = Cat(θ).
Similar to (Srivastava & Sutton, 2017), we collapse the variable θ±N and rewrite Equation 6 as:
N
P(Wi：N, ∏∣α, Θ) = p(∏∣α) ɪɪp(wiln, Θ)	(7)
i=1
wherep(w∕∏, Θ) = CAT(θ) and θ = P∞=ι ∏kθk.
In practice, following (Miao et al., 2017), we factorize the parameter φk of topic θk as φk = tkW
where tk ∈ RH is the k-th topic factor vector, W ∈ RH ×V is the word factor matrix and H ∈ R+
is the factor dimension. For simplicity, we still use Φ to denote all the parameters regarding to the
generative procedure of iTM-VAE. Note that, although we parameterize the generative process with
Φ, iTM-VAE is still a nonparametric model, since it has potentially infinite model capacity, and can
grow the number of parameters with the amount of training data.
Notably, different with traditional nonparametric Bayesian topic models, the topics in iTM-VAE
are not drawn from a base distribution, but are treated as part of the parameters of the model and
are optimized directly. This key difference indicates that there is no need to use an additional base
distribution to generate the countably infinite candidate topics such that these topics are shared by
different documents. Instead, the topic parameters of iTM-VAE are shared across all documents
naturally.
4.2	The Inference Procedure
In this section, we describe the inference process of iTM-VAE, i.e. how to draw π given a document
w1:N. To elaborate, suppose ν = [ν1, ν2, . . . , νK-1] is a K - 1 dimensional vector where νk is a
4
Under review as a conference paper at ICLR 2018
random variable sampled from a Kumaraswamy distribution κ(ν; ak, bk) parameterized by ak and
bk, iTM-VAE models the joint distribution qψ (ν |w1:N) as: 1
[a1, . . . ,aK-1;b1,. . . , bK-1] = g(w1:N; ψ)	(8)
K-1
qψ (ν |w1:N) =	κ(νk; ak, bk)	(9)
k=1
where g(w1:N ; ψ) is a neural network with parameters ψ. Then, π = {πk }kK=1 can be drawn by:
V 〜qψ (V ∣Wi:N)	(10)
K-2	K-1
ν1, ν2(1 - ν1), . . . , νk-1	(1 - νl),	(1 - νl) (11)
l=1	l=1
In the above procedure, we truncate the infinite sequence of mixture weights π = {πk }k∞=1 by K
elements, and νK is always set to 1 to ensure PkK=1 πk = 1. Notably, as discussed in (Blei et al.,
2006), the truncation of variational posterior does not indicate that we are using a finite dimensional
prior, since we never truncate the GEM prior. Moreover, the truncation level K is not part of the
generative procedure specification. Hence, iTM-VAE still has the ability to model the uncertainty of
the number of topics and adapt it to data. People can manage to use truncation-free posteriors in the
model, however, as observed by Nalisnick & Smyth (2017), it does not work well. On the opposite,
the truncated-fashion posterior of iTM-VAE is simple and works well in practice.
iTM-VAE can be optimized by maximizing the Evidence Lower Bound (ELBO):
L(wi:N∣Φ,Ψ) = Eqψ(v|wi：N) [logp(wi:N∣∏, Φ)] - KL (qψ(ν∣Wi:N)∣∣p(v∣α))	(12)
where We replace Θ with Φ for p(w、：N ∣π, Φ) to emphasize that Φ is the parameter to be optimized,
and p(ν∣α) is products of K - 1 Beta(1, α) distributions according to Section 3.1. The details of
the optimization can be found in Appendix 7.2.
4.3	iTM-VAE with Products of Experts
In Equation 7, θ is a mixture of multinomials. One drawback of this formulation is that it cannot
make any predictions that are sharper than the distributions being mixed, pointed out by (Hinton &
Salakhutdinov, 2009), which may result in some topics that are of poor quality and do not match
well with human judgment. One solution to this issue is to replace the mixture of multinomials with
a weighted product of experts which is able to make sharper predictions than any of the constituent
experts (Hinton, 2006). We develop a products-of-experts version of iTM-VAE, which is referred as
iTM-VAE-Prod. Specifically, we compute a mixed topic distribution θ = σ(P∞=ι πkφk) for each
document, where πk is sampled from GEM(α), and then each word of the document is sampled
from Cat(θ ). The benefit of the products-of-experts is demonstrated in Section 5.
4.4	Placing a Prior on the Concentration Parameter
In the generative process, the concentration parameter α of GEM(α) can have significant impact on
the growth of number of topics. The larger the α is, the more “breaks” it will create, and conse-
quently, more topics will be used. Hence, it is generally reasonable to consider placing a prior on α
so that the model can adjust the concentration parameter to data automatically.
Concretely, since the Gamma distribution is conjugate to Beta(1, α), we place a Gamma(s1, s2)
prior on α. Then the ELBO of iTM-VAE can be written as:
L(W 1:N lφ, ψ) = Eqψ(ν∣WLN) [log P(W 1: N |n, φ)] + Eqψ (ν ∣wi:N )q(α∣ γι ,γ2) [log P(V|a)]
-Eqψ(v|wi：N)[logqψ(ν∣Wi:N)] - KL(q(α∣γι, Y2)∣∣p(α∣s1, s2))	(13)
1Ideally, Beta distribution is the most suitable probability candidate, since iTM-VAE assumes π is drawn
from a GEM distribution in the generative process. However, as Beta does not satisfy the DNCP requirement
of SGVB, we use the Kumaraswamy distribution, which is described in Section 3.3, for iTM-VAE.
5
Under review as a conference paper at ICLR 2018
wherep(α∣sι, s2) = Gamma(sι, s2), P(Vk ∣α) = Beta(I,α), q(α∣γ1,γ2) is the variational posterior
for α with γ1 , γ2 as parameters across the whole dataset. The derivation for Equation 13 can be
found in Appendix 7.3. In experiments, we find iTM-VAE-Prod always performs better than iTM-
VAE, therefore we only place the prior for iTM-VAE-Prod, and refer this variant as iTM-VAE-G.
5	Experiments
In this section, we evaluate the performance of iTM-VAE and its variants on two public benchmarks:
20News and RCV1-V2. To make a fair comparison, we use exactly the same data and vocabulary as
(Srivastava & Sutton, 2017). 2 We compare iTM-VAE and its variants with several state-of-the-arts,
such as DocNADE, NVDM, NVLDA and ProdLDA (Srivastava & Sutton, 2017), GSM, GSB, RSB
and RSB-TF, as well as some classical topic models such as LDA and HDP.
The configuration of the experiments is as follows. We use a two-layer fully-connected neural net-
work for g(w1:N; ψ) of Equation 8, and the number of hidden units is set to 256 and 512 for 20News
and RCV1-V2, respectively. The factor dimension is set to 200 and 1000 for 20News and RCV1-
V2, respectively. The truncation level K in Equation 11 is set to 200 so that the maximum topic
numbers will never exceed the ones used by baselines. 3 Batch-Normalization (Ioffe & Szegedy,
2015) is used to stabilize the training procedure. The hyper-parameter α for GEM distribution is
cross-validated on validation set from [10, 20, 30, 50, 100]. We use Adam (Kingma & Ba, 2015) to
optimize the model and the learning rate is set to 0.01 for all experiments. The code of iTM-VAE
and its variants is available at http://anonymous.
5.1	Perplexity Evaluation
Perplexity is widely used by topic models to measure the goodness-to-fit capability, which is defined
as: exp(-D PD=I w∣ logP(Wd)), where D is the number of documents, and |wd| is the number
of words in the d-th document wd . Following previous work, the variational lower bound is used to
estimate the perplexity.
Table 1 shows the perplexity of different topic models on 20News and RCV1-V2 datasets. Among
these baselines, RSB and GSM (Miao et al., 2017) achieved the lowest perplexities on 20News and
RCV1-V2, which are 785 and 521, respectively. While the perplexities achieved by iTM-VAE-Prod
on these two benchmarks are 769 and 508, respectively, which performs better than the state-of-the-
art. Moreover, Figure 1-(a) demonstrates perplexities of finite topic models with different number
of topics on 20News. We can see that, a suitable topic number of these models should be around
10 and 20. Interestingly but as expected, the number of effective topics4 discovered by iTM-VAE is
about 19, which indicates that the Bayesian nonparametric topic model, iTM-VAE, has the ability
to determine a suitable number of topics automatically.
5.2	Topic Coherence Evaluation
As the quality of the learned topics is not directly reflected by perplexity (Newman et al., 2010), topic
coherence is designed to match the human judgment. As Lau et al. (2014) showed that Normalized
Pointwise Mutual Information (NPMI) matches the human judgment most closely, we adopt it as the
measurement of topic coherence , same as (Miao et al., 2017; Srivastava & Sutton, 2017).5 Since
the number of topics discovered by iTM-VAE is dynamic, we define the Effective Topic as the topic
which becomes the top-1 significant topic of a training sample among the training set more than
τ × D times, where D is the number of training samples and τ is a ratio, which is set to 0.5% in our
experiments. 6
2Since there are no labels for the datasets provided by (Srivastava & Sutton, 2017) and (Miao et al., 2017),
we used a version from (Srivastava et al., 2013) for document retrieval tasks, which contains label information.
3In these baselines, at most 200 topics are used. Please refer Table 1 for details.
4The definition of Effective Topic can be found in Section 5.2.
5We use the code provided by (Lau et al., 2014) at https://github.com/jhlau/topic_
interpretability/
6This means the ineffective topics are the ones act as top-1 important topic less than 0.5% times among all
documents in the training set.
6
Under review as a conference paper at ICLR 2018
Methods	Perplexity				Coherence			
	20News		RCV1-V2		20News		RCV1-V2	
#Topics	50	200	50	200	50	200	50	200
LDAt	893^^	1015	1062^^	1058	0.131	0.112		-
DocNADE	797	804	856	670	0.086	0.082	0.079	0.065
HDPt NVDMt	937 837	873	918 717	588	- 0.186	0.157		- -
NVLDA	1078	993	791	797	0.162	0.133	0.153	0.172
ProdLDA	1009	989	780	788	0.236	0.217	0.252	0.179
GSMt	787	829	653	521	0.223	0.186		-
GSBt	816	815	712	544	0.217	0.171		-
RSBt	785	792	662	534	0.224	0.177		-
RSB-TFt iTM-VAE	788 882		532 1124		- 0.205		- 0.218	
iTM-VAE-Prod	769		508		0.291		0.3	
↑: Taken from Miao et al. (2017), where LDA is based on (Hoffman et al., 2010) and
HDP is based on (Wang et al., 2011).
Table 1: Comparison of perplexity (lower is better) and topic coherence comparison (higher is better)
of different topic models on 20News and RCV1-V2 datasets. The symbol ”-” indicates either the
model fails to converge within 24 hours, or the original paper does not provide the corresponding
values.
(a) Perplexity
(b) Topic Coherence
Figure 1: Perplexity and topic coherence with different number of topics. The lines for iTM-VAE
and iTM-VAE-Prod are horizontal as the topics used by the models are dynamic and adapted to data.
Following (Miao et al., 2017), we use an average over topic coherence computed by top-5 and top-10
words for all topics across five random runs, which is more stable and robust (Lau & Baldwin, 2016).
Table 1 shows the topic coherence of different topic models on 20News and RCV1-V2 datasets.
We can clearly see that iTM-VAE-Prod outperforms all the other topic models significantly, which
indicates that the topics discovered by iTM-VAE-Prod match more closely to human judgment.
Some topics learned by iTM-VAE-Prod are illustrated in Appendix 7.1.
We also illustrate the topic coherence of finite topic models with different numbers of topics, and
compare them with iTM-VAE-Prod and iTM-VAE on 20News dataset, shown in Figure 1-(b). The
topic coherence of iTM-VAE-Prod outperforms all baselines over all topic numbers. Another obser-
vation is that the best topic coherence of ProdLDA is achieved as the topic number is 15, which is
close to the number of effective topics discovered by iTM-VAE-Prod.
5.3	Document Retrieval Evaluation
The document retrieval task is to evaluate the discriminative power of the document representations
learned by each model. We compare iTM-VAE and iTM-VAE-Prod with LDA, DocNADE, NVLDA
7
Under review as a conference paper at ICLR 2018
and ProdLDA. 7 The setup of the retrieval task is as follows. The documents in the training/validation
sets are used as the database for retrieval, and the test set is used as the query set. Given a query
document, documents in the database are ranked according to the cosine similarities to the query.
Precision/Recall (PR) curves can then be computed by comparing the label of the query with those
of the database documents. For documents who have multiple labels (e.g. RCV1-V2), the PR curves
for each of its labels are computed individually and then averaged for each query document. Finally,
the global average of these curves is computed to compare the retrieval performance of each model.
Figure 2 illustrates the PR curves of different models with hidden representations of length 128.
Specifically, the mean of the variational Gaussian posterior is used as the features for NVLDA and
ProdLDA. Since the effective topic numbers of iTM-VAE-Prod and iTM-VAE are dynamic and
usually much smaller than 128 on both datasets, we use a weighted sum of topic factor vectors over
the topic distributions, where the factor dimension is 128. As shown in Figure 2(a) and Figure 2(b),
iTM-VAE-Prod always yields competitive results on both datasets, and outperforms the others in
most cases. We also map the latent representations learned by iTM-VAE-Prod to a 2D space by
TSNE (Maaten & Hinton, 2008) and visualize the representations in Figure 2(c).
(a) 20News
(b) RCV1-V2
(c) TSNE
Figure 2: Precision-Recall curves for document retrieval task on 20News (a) and RCV1-V2 (b).
TSNE-visualization of the representations learned by iTM-VAE-Prod on 20News (c).
5.4	ADAPTING α AUTOMATICALLY BY ITM-VAE-G
As mentioned in Section 4.4, the concentration parameter α of GEM(α) has significant impact on
the growth of the number of topics, which affects the performance of iTM-VAE significantly. As
a result, α has to be chosen by cross-validation for better performance. Figure 3(a) also confirms
that the number of effective topics increases with α. Consequently, iTM-VAE-G, which places a
Gamma(s1, s2) prior on α, is proposed to enable the model to adapt α to data automatically.
Another commonly mentioned problem of the optimization under VAE framework is that the latent
representation will tend to collapse to the prior (Bowman et al.; S0nderby et al., 2016; Chen et al.,
2017). In our model, this means the choice of α will control the number of the learned topics very
tightly when the decoder is strong (e.g. iTM-VAE-Prod with large factor dimension H), which might
cause the model to be lack of adaptive power. Common tricks to alleviate the training problem are
annealing the relative weight of the KL divergence term (S0nderby et al., 2016), or regularizing the
decoder (Bowman et al.). Rather than regularizing the decoder, iTM-VAE-G can be regarded as a
relaxation on the prior placed on the latent space, which is more effective in improving the adaptive
power of our model.
To verify this, we compare the adaptive power of iTM-VAE-Prod with KL annealing and decoder
regularization, to iTM-VAE-G, on several subsets of 20News dataset, which contain 1, 2, 5, 10 and
20 (the whole dataset) classes, respectively. For these two models, we use a MLP of two layer of
256 units as the encoder, and the factor dimension of the decoder is H = 200. For iTM-VAE-Prod,
we set α = 4, and try different tricks to alleviate the collapse problem: KL annealing where the
relative weight of the KL divergence term in the ELBO at epoch n is min(0.005n, 1); Decoder
regularization where the weight of the L2 regularization on the decoder is set to 0.1. For iTM-
VAE-G, we add a relatively non-informative prior Gamma(1, 0.25) on α, and initialize the global
7Since Miao et al. (2017) did not test the document retrieval performance and the code is not available now,
we cannot compare with it during the draft preparation.
8
Under review as a conference paper at ICLR 2018
variational parameters γ1 and γ2 of Equation 13 the same as the non-informative prior. A SGD
optimizer with a learning rate of 0.01 is used to optimize γ1 and γ2 . No KL annealing and decoder
regularization is used for iTM-VAE-G.
The number of effective topics learned by iTM-VAE-Prod on subsets of 20News dataset is shown in
Table 2. We can see that training tricks like KL-annealing and regularizing the decoder do not help
much when the decoder is strong. However, by placing a prior on the concentration parameter α,
iTM-VAE-G can increase the adaptive power of the model.
The corpus-level variational posterior of α and the number of effective topics learned by iTM-VAE-
G is shown in Table 3. As for iTM-VAE-G, before training, Eq(α∣γ1,γ2)[α], the expectation of α given
the variational posterior q(α∣γι, γ2) is 4. Once the training is done, Eq(α∣γι,γ2)[α] Will be adjusted
to the training set. Table 3 illustrates γι, γ2, Eq(α∣γ1,γ2)[α] and the number of effective topics that
are learned from data. We can see that, if the training set contains only 1 class of documents,
Eq(α∣γ1,γ2) [α] will drop to 2.35, and only 6 effective topics are used to model the dataset. Whereas,
when the training set consists of 10 classes of documents, Eq(α∣γ1,γ2)[α] increases to 6.62, and 11
effective topics are discovered by the model to explain the dataset. This indicates that iTM-VAE-G
can learn to adjust α to data.
Figure 3(b) illustrates the topic coverage w.r.t the number of topics when the training set contains
1, 2, 5, 10 and 20 classes, respectively. To this end, we compute the top-1 significant topic for each
training document, and sort the topics according to the frequency that it is assigned as top-1. The
topic coverage is then defined as the cumulative sum of these frequencies. Figure 3(b) shows that,
with the increasing of the number of classes, more topics are utilized by iTM-VAE-G to reach the
same level of topic coverage, which indicates that the model has the ability to adapt to data.
#classes	H =200	H = 200 with KL annealing	H = 200 with decoder L2 regularization
1	10	10	9
2	10	9	10
5	9	10	10
10	9	9	10
20	10	9	10
Table 2: Number of effective topics learned by iTM-VAE-Prod on subsets of 20News dataset.
#classes	γ1	γ2	Eq(α∣γι ,Y2) [α]	#Effective topics learned by iTM-VAE-G H = 200
1	18.30	7.80	235	6
2	25.03	6.38	3.92	9
5	34.10	6.19	5.51	9
10	42.96	6.49	6.62	11
20	52.97	7.23	7.33	14
Table 3: Learned posterior distribution of α and number of effective topics learned by iTM-VAE-G
on subsets of 20News dataset.
6	Conclusion
In this paper, we propose iTM-VAE, which, to our best knowledge, is the first Bayesian nonparamet-
ric topic model that is modeled by Variational Auto-Encoders. Specifically, a stick-breaking prior is
used to generate the mixture weights of countably infinite topics and the Kumaraswamy distribution
is exploited such that the model can be optimized by AEVB algorithm. Two variants of iTM-VAE
are also proposed in this work. One is iTM-VAE-Prod, which replaces the mixture of multinomials
assumption of iTM-VAE with a product of experts for better performance. The other one is iTM-
VAE-G which places a Gamma prior on the concentration parameter of the stick-breaking process
such that the model can adapt the concentration parameter to data automatically. The advantage of
iTM-VAE and its variants over the other Bayesian nonparametric topics models is that the inference
9
Under review as a conference paper at ICLR 2018
(b)
Figure 3: (a) Number of effective topics w.r.t α on 20News. (b) Topic coverage w.r.t number of
topics learned by iTM-VAE-G.
is performed by feed-forward neural networks, which is of rich representation capacity and requires
only limited knowledge of the data. Hence, it is flexible to incorporate more information sources to
the model, and we leave it to future work. Experimental results on two public benchmarks show that
iTM-VAE and its variants outperform the state-of-the-art baselines significantly.
References
Cedric Archambeau, Balaji Lakshminarayanan, and Guillaume Bouchard. Latent ibp compound
dirichlet allocation. IEEE transactions on pattern analysis and machine intelligence, 37(2):321-
333, 2015.
David M Blei. Probabilistic topic models. Communications of the ACM, 55(4):77-84, 2012.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. JMLR, 2003.
David M Blei, Michael I Jordan, et al. Variational inference for dirichlet process mixtures. Bayesian
analysis, 1(1):121-143, 2006.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal JOzefowicz, and Samy
Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349.
Dallas Card, Chenhao Tan, and Noah A Smith. A neural framework for generalized topic models.
arXiv preprint arXiv:1705.09296, 2017.
Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. 2017.
Warren John Ewens. Population genetics theory-the past and the future. In Mathematical and
statistical developments of evolutionary theory, pp. 177-227. Springer, 1990.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8), 2006.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Replicated softmax: an undirected topic model. In
NIPS, pp. 1607-1614, 2009.
Matthew Hoffman, Francis R. Bach, and David M. Blei. Online learning for latent dirichlet alloca-
tion. In NIPS. 2010.
Michael Hughes, Dae Il Kim, and Erik Sudderth. Reliable and scalable variational inference for the
hierarchical dirichlet process. In Artificial Intelligence and Statistics, pp. 370-378, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, pp. 448-456, 2015.
Dae I Kim and Erik B Sudderth. The doubly correlated nonparametric topic model. In Advances in
Neural Information Processing Systems, pp. 1980-1988, 2011.
10
Under review as a conference paper at ICLR 2018
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Diederik Kingma and Max Welling. Efficient gradient-based inference through transformations
between bayes nets and neural nets. In ICML, pp.1782-1790, 2014a.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014b.
Ponnambalam Kumaraswamy. A generalized probability density function for double-bounded ran-
dom processes. Journal of Hydrology, 46(1-2):79-88, 1980.
Hugo Larochelle and Stanislas Lauly. A neural autoregressive topic model. In NIPS, 2012.
Jey Han Lau and Timothy Baldwin. The sensitivity of topic coherence evaluation to topic cardinality.
In NAACL HLT, pp. 483-487, 2016.
Jey Han Lau, David Newman, and Timothy Baldwin. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality. In EACL, pp. 530-539, 2014.
Kar Wai Lim, Wray Buntine, Changyou Chen, and Lan Du. Nonparametric bayesian topic modelling
with the hierarchical pitman-yor processes. International Journal of Approximate Reasoning, 78:
172-191, 2016.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 2008.
Yishu Miao, Lei Yu, and Phil Blunsom. Neural variational inference for text processing. In ICML,
pp. 1727-1736, 2016.
Yishu Miao, Edward Grefenstette, and Phil Blunsom. Discovering discrete latent topics with neural
variational inference. In ICML, 2017.
Joseph Victor Michalowicz, Jonathan M. Nichols, and Frank Bucholtz. Handbook of differential
entropy. Crc Press, 2013.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
ICML, 2014.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Eric Nalisnick and Padhraic Smyth. Stick-breaking variational autoencoders. In ICLR, 2017.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. Automatic evaluation of topic
coherence. In NAACL HLT, pp. 100-108, 2010.
Duangmanee Putthividhy, Hagai T Attias, and Srikantan S Nagarajan. Topic regression multi-modal
latent dirichlet allocation for image annotation. In CVPR, pp. 3408-3415. IEEE, 2010.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In AISTATS,
2014.
Nikhil Rasiwasia and Nuno Vasconcelos. Latent dirichlet allocation models for image classification.
IEEE transactions on pattern analysis and machine intelligence, 35(11):2665-2679, 2013.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
variational inference in deep latent gaussian models. In ICML, 2014.
Simon Rogers, Mark Girolami, Colin Campbell, and Rainer Breitling. The latent process decompo-
sition of cdna microarray data sets. IEEE/ACM TCBB, 2(2):143-156, 2005.
Jayaram Sethuraman. A constructive definition of dirichlet priors. Statistica sinica, 1994.
Casper Kaae S0nderby, TaPani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder
variational autoencoders. In Advances in Neural Information Processing Systems, pp. 3738-3746,
2016.
Akash Srivastava and Charles Sutton. Autoencoding variational inference for topic models. In ICLR,
2017.
11
Under review as a conference paper at ICLR 2018
Nitish Srivastava, Ruslan R Salakhutdinov, and Geoffrey E Hinton. Modeling documents with deep
boltzmann machines. In UAI, 2013.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Association, 101(476):1566-1581, 2006.
Chong Wang, John Paisley, and David Blei. Online variational inference for the hierarchical dirichlet
process. In AISTATS, pp. 752-760, 2011.
Xing Wei and W Bruce Croft. Lda-based document models for ad-hoc retrieval. In SIGIR, 2006.
12
Under review as a conference paper at ICLR 2018
7	Appendix
7.1	Learned Topics and Discussion
Geography	turkish armenians turks armenia armenian turkey azerbaijan greek greece village
Sports	season team player nhl score playoff hockey game coach hitter
Religion	jesus bible god faith scripture christ doctrine belief eternal church
Space	orbit shuttle launch lunar spacecraft nasa satellite probe rocket moon
Hardware	scsi ide scsus motherboard ram controller upgrade meg cache floppy
Encryption	ripem escrow rsa des encrypt cipher privacy crypto chip nsa
Trade	shipping sale annual manual tor VS det cd price excellent
X system	window xterm font colormap server xlib widget xt windows toolkit
Hockeyt	det tor buf cal pit que mon pt vs calgary
Health	msg* patient disease symptom doctor food pain mouse cancer hospital
Circuit	voltage puck connector signal amp input circuit pin wire connect
Lawsuit	gun homicide militia weapon amendment handgun criminal firearm crime knife
Traffic	bike brake car tire ride engine honda bmw rear motorcycle
f: All these words are about hockey teams of different cities, e.g. “que” means Quebec.
∣: “msg” means monosodium glutamate.
Table 4: Top 10 words of topics learned by iTM-VAE-Prod without cherry picking.
As shown in Table 4, iTM-VAE-Prod can learn topics that are diverse and of high quality. One
possible reason is that the stick-breaking prior for the document-specific π encourages the model
to learn sparse representation, and the model can adjust the number of topics according to the data.
Thus the topics can be sufficiently trained and of high diversity. The comparison of representation
sparsity is illustrated in Figure 4(a).
In contrast, the topics learned by ProdLDA (Srivastava & Sutton, 2017) lack diversity. As we listed
in Table 5, there are a lot of redundant topics. As a result, the latent representation learned by
ProdLDA is of poor discriminative power. Figure 4(c) shows the TSNE-visualization of the repre-
sentations learned by ProdLDA with the best topic coherence on 20News.
Figure 4: (a) Representation sparsity of different models on 20News. We sample one topic assign-
ment π for each document, sort and then average across the test set.9(b) The TSNE-visualization
of the representation learned by by iTM-VAE-Prod. (c) The TSNE-visualization of the represen-
tation learned by ProdLDA (Srivastava & Sutton, 2017) with the best topic coherence on 20News
(K = 50).
7.2	The evidence lower bound of iTM-VAE
In this section we show how to compute the Evidence Lower Bound (ELBO) of iTM-VAE which
can be written as:
L(WLN∣Φ,ψ) = Eqψ(v|wi：N) [logp(wi:N∣∏,Φ)] - KL(qψ(ν|wi：N)∣∣p(ν∣α))	(14)
9We use the code provided by (Srivastava & Sutton, 2017) to get their sparsity curve.
13
Under review as a conference paper at ICLR 2018
Topics
about
Religion
1
2
3
4
5
6
jesus christian scripture faith god christ heaven christianity verse resurrection
jesus christ doctrine revelation verse
scripture satan christian interpretation god
belief god passage scripture moral atheist christian truth principle jesus
god belief existence faith jesus atheist bible christian religion sin
jesus son holy christ father god doctrine heaven spirit prophet
homosexual marriage belief islam moral
Christianity truth islamic religion god
Topics
about
Hardware
Topics
about
Politics
7
8
9
10
11
12
13
14
15
16
17
18
floppy controller ScsUs ide Scsi ram hd mb cache isa
printer meg adapter scsi motherboard windows modem mhz vga hd
ide mb connector controller isa scsi scsus floppy jumper disk
mb controller bio rom interface mhz scsus scsi floppy ide
ide meg motherboard shipping adapter simm hd mhz monitor scsi
ram controller dos bio windows disk scsi rom scsus meg
honda motherboard bike amp quadra hd brake apple upgrade meg
decision stephanopoulos president armenian gay
package congress myers february armenians
armenian turkish genocide armenians turks
jesus massacre muslim armenia muslims
armenians father gang soldier neighbor apartment girl armenian troops rape
muslim greek turks turkish armenian muslims village genocide armenia jews
armenian turks armenians armenia turkish
Topics
about
Lawsuit
19
20
21
22
muslim massacre village turkey greek
armenians armenian neighbor apartment woman
soviet kill bus azerbaijan hide
morality truth moral objective absolute
belief murder existence principle human
homicide vancouver seattle handgun firearm child states percent study file
murder moral constitution morality criminal objective rights gun law weapon
Table 5: Top 10 words of some redundant topics learned by ProdLDA.
• Eqψ(v|wi：N) [logP(WLN |n, φ)]:
Similar to other VAE-based models, the SGVB estimator and reparameterization trick can be
used to approximate this intractable expectation and propagate the gradient flow into the inference
network g. Specifically, we have:
1LN
Eqψ (ν∣wιN) [log P(W1: N ∣∏, Φ)] = L XX log p(wi∣∏(l), Φ)	(15)
l=1 i=1
where L is the number of Monte Carlo samples in the SGVB estimator and can be set to 1. N is
the number of words in the document.
According to Section 4.2, π(l) can be obtained by
[a1, . . . , aK-1; b1, . . .,bK-1] = g(W1:N; ψ)	(16)
Vk 〜K(V; ak,bk)	(17)
π = (π1 , π2 , . . . , πK-1 , πK)
K-2	K-1
V1, V2(1 - V1), . . . , Vk-1 Y (1 - Vl), Y (1 - Vl)	(18)
l=1	l=1
where g (W1:N; ψ) is an inference network with parameters ψ, κ denotes the Kumaraswamy dis-
tribution and K ∈ R+ is the truncation level. Here We omit the superscript * *(l) for simplicity.
According to the generative procedure in Section 4.1, p(wi∣π(l), Φ) can be computed by
p(Wi∣∏(i), Φ) = (PYl*kW) ≈ PK=Kπkl)σ(tkW)	iTM-VAE	(19)
σ(Pk=1 πk tkW) ≈ σ (Pk=1 πk tkW)	iTM-VAE-Prod
14
Under review as a conference paper at ICLR 2018
where tk ∈ RH is the k-th topic factor vector, W ∈ RH ×V is the word factor matrix, H is the
factor dimension, V is the vocabulary size and σ(∙) is the Softmax function.
• KL (qψ (v|wi：N)∣∣p(ν∣α)): By applying the KL divergence of a Kumaraswamy distribution
κ(ν; ak, bk) from a beta distributionp(ν; 1, α), we have:
K-1
KL (qψ(V|wi：N)∣∣p(ν∣α)) = E KL (qψ(Vk |wi：N)∣∣p(νk∣α))
k=1
K-1 a 1	1
E -------- ( -Y - Ψ(bk) - -r ) + log akbk + log B (1, a)
k=1 ak	bk
∞
+(α-1) X
m=1
—bk— B
m + akbk
bk - 1
bk
(20)
where B(∙) is the Beta function and Y is the Euler's constant.
7.3 The evidence lower bound of iTM-VAE-G
In this section we show how to compute the Evidence Lower Bound (ELBO) of iTM-VAE-G which
can be written as:
L(W 1： N lφ,ψ) = Eqψ(ν∣WLN )[log P(WLN |n, φ)] + Eqψ (V |wi：n )q(α∣ γι ,γ2) [log P(v|a)]
-Eqψ(V|wi：N)[logqψ(ν∣Wi:N)] - KL(q(α∣γι, Y2)∣∣p(α∣s1, S2))	(21)
Specifically, each item in Equation 21 can be obtained as follows:
•	Eqψ (ν∣wi:N) [log P(WLN |n, φ)]:
The derivation is exactly the same as Appendix 7.2.
•	Eqψ (ν∣Wi:N )q(α∣Y1,Y2) [log P(V Ia)]:
Recall that the prior of the stick length variable Vk is Beta(1,α): p(vk∣a) = a(1 - Vk )α-1 and the
variational posterior of the concentration parameter α is a gamma distribution q(α; Y1, Y2), we
have
K-1
Eqψ (ν |wi：n )q(α∣γ1,γ2)[log P(V|a)] = Eqψ (ν |wi：n )[ £ Eq(ɑ∣ γι ,γ2) [log a + (a - 1)log(1 - Vk 川
k=1
K-1
=(K - 1)Eq(α∣γι,γ2)[log a] + X γi-^2 Eqψ (VkIwLN )[log(1 - "k )]	(22)
k=1	Y2
Now, we provide more details about the calculation of these two expectations in Equation 22 as
follows:
◦	Eq(α∣γι,γ2) [log a]:
Fisrt, we can write the gamma distribution q(a; Y1, Y2) in its exponential family form:
q(a; Y1,Y2) = aexp( - γ2a + Yi log a - (logΓ(γι) - γι log γ2))	(23)
Considering the general fact that the derivative of the log normalizor logΓ(Y1) - Y1 log Y2
of a exponential family distribution with respect to its natural parameter Y1 is equal to the
expectation of the sufficient statistic log a, We can compute Eq(α∣γι ,γ2) [log a] in the first term
of Equation 22 as follows:
Eq(α∣Yι,Y2)[log a] = ψ(Yl) - log Y2	(24)
where Ψ is the digamma function, the first derivative of the log Gamma function.
◦	Eqψ (Vk |wi：n )[log(1 - Vk )]:
By applying the Taylor expansion, Eq(VkIWLN) [log(1 - Vk)] can be written as the infinite sum
of the Kumaraswamy’s mth moment:
Eqψ (Vk |wi：n )[log(1 - Vk )]
∞1	∞ b	m
-X —Eqψ (VkIWLN )[v，] = - X -ɪ	彳B(——，bk ) (25)
m ψ k 1:N	m + akbk	ak
m=1	m=1
where B(∙) is the Beta function.
15
Under review as a conference paper at ICLR 2018
By substituting Equation 24 and Equation 25 into Equation 22, we can obtain:
Eqψ (v|wi：N )q(α∣γ1,γ2 ) [log P(VIa)]
K-1 ∞
=(K - 1)(Ψ(Y1) - logY2) - ɪ-ɪ X X -^B-B(m,bk)
γ2	k=1 m=1 m + akbk ak
(26)
• -Eqψ (v|wi：N )[log qψ (V |w1:N )]：
According to Section 4.11 of (Michalowicz et al., 2013), the Kumaraswamy’s entropy is given as
K-1
-Eqψ (v|wi：N )[log qψ (V |w1:N )]
-E Eqψ (Vk∣wi:N )[log qψ (VkIWLN )]
k=1
X - log(akbk) + ----------(Y + ψ(bk) + ξ-) + -k7--
ak	bk	bk
k=1
(27)
where γ is the Euler’s constant.
• KL(q(a|Yi ,Y2)llP(alsι,s2)):
The KL divergence of one gamma distribution q(α; γ1, γ2) from another gamma distribution
P(α; s1, s2) evaluates to
κL(qllp)=s1 log γ2- logΓ(⅛+(γ1- s1)ψ(γ1)- (γ2- s2) γ2
(28)
16