Under review as a conference paper at ICLR 2018
Learning Gaussian Policies from
Smoothed Action Value Functions
Anonymous authors
Paper under double-blind review
Ab stract
State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learn-
ing (RL), giving rise to popular algorithms such as SARSA and Q-learning. We
propose a new notion of action value defined by a Gaussian smoothed version
of the expected Q-value. We show that such smoothed Q-values still satisfy a
Bellman equation, making them learnable from experience sampled from an en-
vironment. Moreover, the gradients of expected reward with respect to the mean
and covariance of a parameterized Gaussian policy can be recovered from the
gradient and Hessian of the smoothed Q-value function. Based on these rela-
tionships we develop new algorithms for training a Gaussian policy directly from
a learned smoothed Q-value approximator. Our approach is amenable to proxi-
mal optimization techniques by augmenting the objective with a penalty on KL-
divergence from a previous policy. We find that the ability to learn both a mean
and covariance during training allows this approach to achieve much better results
on standard continuous control benchmarks.
1	Introduction
Model-free reinforcement learning algorithms often alternate between two concurrent but interact-
ing processes: (1) policy evaluation, where an action value function (i.e., a Q-value) is updated to
obtain a better estimate of the return associated with taking a specific action, and (2) policy im-
provement, where the policy is updated aiming to maximize the current value function. In the past,
different notions of Q-value have led to distinct but important families of RL methods. For example,
SARSA (Rummery & Niranjan, 1994; Sutton & Barto, 1998; Van Seijen et al., 2009) uses the ex-
pected Q-value, defined as the expected return of following the current policy. Q-learning (Watkins,
1989) exploits a hard-max notion of Q-value, defined as the expected return of following an optimal
policy. Soft Q-learning (Haarnoja et al., 2017) and PCL (Nachum et al., 2017a) both use a soft-max
form of Q-value, defined as the future return of following an optimal entropy regularized policy.
Clearly, the choice of Q-value function has a considerable effect on the resulting algorithm; for ex-
ample, restricting the types of policies that can be expressed, and determining the type of exploration
that can be naturally applied.
In this work We introduce a new notion of action value: the smoothed action value function Qπ.
Unlike previous notions, which associate a value with a specific action at each state, the smoothed Q-
value associates a value with a specific distribution over actions. In particular, the smoothed Q-value
of a state-action pair (s, a) is defined as the expected return of first taking an action sampled from
a normal distribution N(a, Σ(s)), centered at a, then following actions sampled from the current
policy thereafter. In this way, the smoothed Q-value can also be interpreted as a Gaussian-smoothed
or noisy version of the expected Q-value.
We show that smoothed Q-values possess a number of interesting properties that make them at-
tractive for use in RL algorithms. For one, the smoothed Q-values satisfy a single-step Bellman
consistency, which allows bootstrapping to be used to train a function approximator. Secondly, for
Gaussian policies, the standard optimization objective (expected return) can be expressed in terms of
smoothed Q-values. Moreover, the gradient of this objective with respect to the mean and covariance
of the Gaussian policy is equivalent to the gradient and the Hessian of the smoothed Q-value func-
tion, which allows one to derive updates to the policy parameters by having access to the derivatives
of a sufficiently accurate smoothed Q-value function.
1
Under review as a conference paper at ICLR 2018
This observation leads us to propose an algorithm called Smoothie, which in the spirit of (Deep)
Deterministic Policy Gradient (DDPG) (Silver et al., 2014; Lillicrap et al., 2016), trains a policy
using the derivatives of a trained (smoothed) Q-value function, thus avoiding the high-variance of
stochastic updates used in standard policy gradient algorithms (Williams & Peng, 1991; Konda &
Tsitsiklis, 2000). Unlike DDPG, which is well-known to have poor exploratory behavior (Haarnoja
et al., 2017), the approach we develop is able to utilize a non-deterministic Gaussian policy param-
eterized by both a mean and a covariance, thus allowing the policy to be exploratory by default and
alleviating the need for excessive hyperparameter tuning.
Furthermore, we show that Smoothie can be easily adapted to incorporate proximal policy opti-
mization techniques by augmenting the objective with a penalty on KL-divergence from a previous
version of the policy. The inclusion of a KL-penalty is not feasible in the standard DDPG algorithm,
but we show that it is possible with our formulation, and it significantly improves stability and over-
all performance. On standard continuous control benchmarks, our results are competitive with or
exceed state-of-the-art, especially for more difficult tasks in the low-data regime.
2	Notation & Background
We consider the standard model-free RL framework, where an agent interacts with a stochastic
black-box environment by sequentially observing the state of the environment, emitting an action,
and receiving a reward feedback; the goal is to find an agent that achieves maximal cumulative
discounted reward. This problem can be expressed in terms of a Markov decision process (MDP)
that consists of a state space S and an action space A, where at iteration t the agent encounters
a state st ∈ S and emits an action at ∈ A, after which the environment returns a scalar reward
rt 〜 R(st, at) and places the agent in a new state st+ι 〜 P (st, at).
We model the behavior of the agent using a stochastic policy π that produces a distribution over
feasible actions at each state s as π(a | s). The optimization objective (expected discounted return),
as a function of the policy, can then be expressed in terms of the expected action value function
Qπ(s, a) by,
OER(π) = π(a | s)Qπ (s, a) da dρπ (s) ,
SA
(1)
where ρπ(s) is the stationary distribution of the states under π, and Qπ(s, a) is recursively defined
using the Bellman equation,
Qπ(s,a) = Er,s0
r + γ Qπ(s0, a0)π(a0 | s0) da
A
(2)
where γ ∈ [0, 1] is the discount factor. For brevity, we will often suppress explicit denotation of the
sampling distribution R over immediate rewards and the distribution P over state transitions.
The policy gradient theorem (Sutton et al., 2000) expresses the gradient of OER(πθ) w.r.t. θ, the
tunable parameters of a policy πθ, as,
Vθ Oer (∏θ )
[[Vθ∏θ(a | s)Qπ(s, a) dadρπ(S)
SA
E Ea〜∏θ(a|s) [Vθlog∏θ(a | s)Qπ(s,a)]dρπ(s).
S
(3)
(4)
Many reinforcement learning algorithms, including policy gradient and actor-critic variants, trade off
variance and bias when estimating the random variable inside the expectation in (4); for example, by
attempting to estimate Qπ (s, a) accurately using function approximation. In the simplest scenario,
an unbiased estimate of Qπ (s, a) is formed by accumulating discounted rewards from each state
forward using a single Monte Carlo sample.
In this paper, we focus on multivariate Gaussian policies over continuous action spaces, A ≡ Rda .
We represent the observed state of the MDP as a ds-dimensional feature vector Φ(s) ∈ Rds, and
parametrize the GaUssian policy by a mean and covariance function, respectively μ(s) : Rds → Rda
and Σ(s) : Rds → Rda × Rda. These map the observed state of the environment to a Gaussian
distribution,
π(a |S) = N (a | μ(S), ς(S)) = ∣2πς(S)IT/2exp{-2 Ila-μ(S)IlΣ(s)-J ,	⑸
2
Under review as a conference paper at ICLR 2018
where kv k2A = vTAv. Below we develop new RL training methods for this family of parametric
policies, but some of the ideas presented may generalize to other families of policies as well. We
begin the formulation by reviewing some prior work on learning Gaussian policies.
2.1 Deterministic Policy Gradient
Silver et al. (2014) present a new formulation of the policy gradient, called the deterministic policy
gradient, for the family of Gaussian policies in the limit where the policy covariance approaches
zero. In such a scenario, the policy becomes deterministic because sampling from the policy always
returns the Gaussian mean. The key observation of (Silver et al., 2014) is that under a deterministic
policy ∏ ≡ (μ, Σ → 0), one can estimate the expected future return from a state S as,
lim
Σ→0
π(a | s)Qπ (s, a) da
Qn(S,μ(S)).
(6)
A
Then, one can express the gradient of the optimization objective (expected discounted return) for a
parameterized ∏θ ≡ μθ as,
VθOer(∏θ) = ZS VθQn(s,μθ(s))dρπ(s) = ZS dQ∂(jakμθ(S)Vθμθ(s)dρπ(s).⑺
This can be thought of as a characterization of the policy gradient theorem for deterministic policies.
In the limit of Σ → 0, one can also re-express the Bellman equation (2) as,
Qn(s, a) = Er,s0 [r + Qπ(s0,μ(s0))].
(8)
Therefore, a value function approximator Qπw can be optimized by minimizing the Bellman error,
E(W)= E	(Qw(s,a) -τ -YQW(s0,μ(SO))2 ,
(s,a,r,s0)∈D
(9)
for transitions (S, a, r, S0) sampled from a dataset D of interactions of the agent with the environment.
Algorithms like DDPG (Lillicrap et al., 2016) alternate between improving the value function by
gradient descent on (9), and improving the policy based on (7).
In practice, to gain better sample efficiency, Degris et al. (2012) and Silver et al. (2014) replace
the on-policy state distribution ρπ (S) in (7) with an off-policy distribution ρβ (S) based on a replay
buffer. After this substitution, the policy gradient identity in (7) does not hold exactly, however,
prior work finds that this works well in practice and improves sample efficiency. We also adopt a
similar approximation in our method to make use of off-policy data.
3	Smoothed Action Value Functions
In this paper, we introduce smoothed action value functions, the gradients of which provide an
effective signal for optimizing the parameters of a Gaussian policy. Our notion of smoothed Q-
values, denoted Qπ (S, a), differs from ordinary Q-values Qπ (S, a) in that smoothed Q-values do
not assume the first action of the agent is fully specified, but rather they assume that only the mean
of the distribution of the first action is known. Hence, to compute Qπ (S, a), one has to perform an
expectation of Qn (s, α) for actions a drawn in the vicinity of a. More formally, smoothed action
values are defined as,
-τ _ ,	、
Qaπ (S, a)
AN(aa|
a, Σ(S)) Qπ (S, aa)daa .
(10)
〜
With this definition of Qπ , one can re-express the expected reward objective for a Gaussian policy
π ≡ (μ, Σ) as,
OER(π) =
-T _ ,	,一_	_ ,
Q (s,μ(s))dρ (s).
(11)
The insight that differentiates this approach from prior work including Heess et al. (2015); Ciosek
& Whiteson (2017) is that instead of learning a function approximator for Qπ (S, a) and then draw-
ing samples to approximate the expectation in (10) and its derivative, we directly learn a function
approximator for Qπ (S, a).
3
Under review as a conference paper at ICLR 2018
EI 1	F	. ∙	.1	Fl	. Λ	. .	♦	Γ∙	,llzʌ 1	入F <	∖ ∙ . 1 ..)
The key observation that enables direct bootstrapping of smoothed Q-values, Qπ (s, a), is that their
form allows a notion of Bellman consistency. First, note that for Gaussian policies ∏ ≡ (μ, Σ) We
have
Qn(s,a) = Er,s0[r + γQπ(s0,μ(s0))] .	(12)
Then, combining (10) and (12), one can derive the following one-step Bellman equation for
smoothed Q-values,
Qn(s, a) = / N(a I a, ∑(s)) Er," [r + γQπ(30, μ(30))] da ,
(13)
where r and s0 are sampled from R(s, a) and P(s, α). Below, we elaborate on how one can make
use of the derivatives of Qn to learn μ and Σ, and how the Bellman equation in (13) enables direct
optimization of Qαπ .
3.1	Policy Improvement - Optimizing (μθ, Σφ) Given Qπ
We parameterize a Gaussian policy ∏θ,φ ≡ (μθ, Σφ) in terms of two sets of parameters θ and φ for
the mean and the covariance. The gradient of the objective w.r.t. mean parameters follows from the
policy gradient theorem and is almost identical to (7),
απ
Vθ OER (Πθ,φ) = LJLdaLj)∖a = μθ(sfθ μθ (s)dρπ (s).
(14)
Estimating the derivative of the objective w.r.t. covariance parameters is not as straightforward, since
Qαπ is not a direct function ofΣ. However, akey observation of this work is that the second derivative
of Qπ w.r.t. actions is sufficient to exactly compute the derivative of Qπ w.r.t. Σ,
_	~	- C ~	.
∂Qπ(s,a) _ 1 ∂2Qπ(s,a)
∂Σ(s) = 2	∂02-
(15)
A proof of this identity is provided in the Appendix. The proof may be easily derived by expressing
both sides of the equation using standard matrix calculus like ∂A|A|-1/2 = 一2|A|-1/2AT and
∂A∣∣v∣∣A-ι = -ATvvT A-1.
Then, the full derivative w.r.t. φ takes the form,
2 απ
VφOER(∏θ,φ) = £/ dQna；,ak“.(s)ve@(s)dpn(s).	(16)
Sa
3.2	policy Evaluation - Optimizing Qw Given (μ, Σ)
We can think of two ways to optimize Qαπw. The first approach leverages (10) to update Qαπ based on
expected Q-value function Qπ . in such an approach, one trains a parameterized Qπw to approximate
the standard expected Q-value function Qπ using standard methods (see e.g., Rummery & Niranjan
(1994); Sutton & Barto (1998); van Seijen et al. (2009)). Then, one fits Qαπw based on Qπw. in
particular, given transitions (s, a, r, s0) sampled from interactions with the environment, one can
train Qw to minimize the Bellman error (Qw (s, a) 一r 一 YQw (s0,a0))2 where a0 〜N(μ(s0), Σ(s0)).
Then, Qw can be optimized to minimize the squared error (Qw(s, a) 一 EGQw(s, α))2 where a 〜
N(a, Σ(s)), using several samples. When the target values in these residuals are treated as fixed
(i.e., using a target network), such a training procedure will achieve a fixed point when Qπw (s, a)
satisfies the recursion in the Bellman equation (10).
The second approach requires a single function approximator for Qπw (s, a), resulting in a simpler
implementation, and thus we use this approach in our experimental evaluation. Suppose one has ac-
cess to a tuple (s, aα, rα, sα0) sampled from a replay buffer with knowledge of the sampling probability
q(aα | s) (possibly unnormalized). Then assuming that this sampling distribution has a full support,
we draw a phantom action a 〜N (α, Σ(s)) and optimize Qw (s, a) by minimizing a weighted Bell-
man error 勺(楠(Qw (s, a) 一 r 一 YQw (α0, μ(α0))2. For a specific pair of state and action (s, a) the
4
Under review as a conference paper at ICLR 2018
expected value of the objective is,
E(W I (s,a)) = Eq(a∣s),r," [N(a]f, *、(S)) (QW(s,a) - r -YQW(S0,μ(50)))21 .	(17)
|_	q(a I S)
Note that N(a∣a, Σ(s)) = N(a∣a, Σ(s)). Therefore, when the target value r + YQw(s0,μ(s0)) is
treated as fixed (e.g., when using target networks) this training procedure reaches an optimum when
Qπw(S, a) satisfies the recursion in the Bellman equation (13).
In practice, We find that it is unnecessary to keep track of the probabilities q(a ∣ s), and assume
the replay buffer provides a near-uniform distribution of actions conditioned on states. Other recent
work has also benefited from ignoring or heavily damping importance weights (Munos et al., 2016;
Wang et al., 2017; Schulman et al., 2017). However, it is possible when interacting with the envi-
ronment to save the probability of sampled actions along with their transitions, and thus have access
to q(a ∣ S) ≈ N (a ∣ μold(s), ∑old(s)).
3.3	Proximal Policy Optimization
Policy gradient algorithms are notoriously unstable, particularly in continuous control problems.
Such instability has motivated the development of trust region methods that attempt to mitigate
the issue by constraining each gradient step to lie within a trust region (Schulman et al., 2015),
or augmenting the expected reward objective with a penalty on KL-divergence from a previous
policy (Nachum et al., 2017b; Schulman et al., 2017; Azar et al., 2012). These stabilizing techniques
have thus far not been applicable to algorithms like DDPG, since the policy is deterministic. The
formulation we propose in this paper, however, is easily amenable to trust region optimization.
Specifically, we may augment the objective (11) with a penalty
OTR(π) = OER(π) - λ
KL(π k
πold) dρπ(S),
(18)
where ∏old ≡ (μold, ∑old) is a previous parameterization of the policy. The optimization is straight-
forward, since the KL-divergence of two Gaussians can be expressed analytically.
4	Related Work
This paper follows a long line of work that uses Q-value functions to stably learn a policy, which
in the past has been used to either approximate expected (Rummery & Niranjan, 1994; Van Seijen
et al., 2009; Gu et al., 2017) or optimal (Watkins, 1989; Silver et al., 2014; Nachum et al., 2017a;
Haarnoja et al., 2017; Metz et al., 2017) future value.
Work that is most similar to what we present are methods that exploit gradient information from
the Q-value function to train a policy. Deterministic policy gradient (Silver et al., 2014) is perhaps
the best known of these. The method we propose can be interpreted as a generalization of the
deterministic policy gradient. Indeed, if one takes the limit of the policy covariance Σ(S) as it goes
to 0, the proposed Q-value function becomes the deterministic value function of DDPG, and the
updates for training the Q-value approximator and the policy mean are identical.
Stochastic Value Gradient (SVG) (Heess et al., 2015) also trains stochastic policies using an update
that is similar to DDPG (i.e., SVG(0) with replay). The key differences with our approach are that
SVG does not provide an update for the covariance, and the mean update in SVG estimates the
gradient with a noisy Monte Carlo sample, which we avoid by estimating the smoothed Q-value
function. Although a covariance update could be derived using the same reparameterization trick as
in the mean update, that would also require a noisy Monte Carlo estimate. Methods for updating
the covariance along the gradient of expected reward are essential for applying the subsequent trust
region and proximal policy techniques.
More recently, Ciosek & Whiteson (2017) introduced expected policy gradients (EPG), a general-
ization of DDPG that provides updates for the mean and covariance of a stochastic Gaussian policy
using gradients of an estimated Q-value function. In that work, the expected Q-value used in stan-
dard policy gradient algorithms such as SARSA (Sutton & Barto, 1998; Rummery & Niranjan,
1994; Van Seijen et al., 2009) is estimated. The updates in EPG therefore require approximating
5
Under review as a conference paper at ICLR 2018
an integral of the expected Q-value function. Our analogous process directly estimates an integral
(via the smoothed Q-value function) and avoids approximate integrals, thereby making the updates
simpler. Moreover, while Ciosek & Whiteson (2017) rely on a quadratic Taylor expansion of the es-
timated Q-value function, we instead rely on the strength of neural network function approximators
to directly estimate the smoothed Q-value function.
The novel training scheme we propose for learning the covariance of a Gaussian policy relies on
properties of Gaussian integrals (Bonnet, 1964; Price, 1958). Similar identities have been used in
the past to derive updates for variational auto-encoders (Kingma & Welling, 2014) and Gaussian
back-propagation (Rezende et al., 2014).
Finally, the perspective presented in this paper, where Q-values represent the averaged return of a
distribution of actions rather than a single action, is distinct from recent advances in distributional
RL (Bellemare et al., 2017). Those approaches focus on the distribution of returns of a single action,
whereas we consider the single average return of a distribution of actions. Although we restrict our
attention in this paper to Gaussian policies, an interesting topic for further investigation is to study
the applicability of this new perspective to a wider class of policy distributions.
5	Experiments
We utilize the insights from Section 3 to introduce a new RL algorithm, Smoothie. Smoothie main-
tains a parameterized Qπw trained via the procedure described in Section 3.2. It then uses the gradient
and Hessian of this approximation to train a Gaussian policy μθ, Σφ using the updates stated in (14)
and (16). See Algorithm 1 for a simplified pseudocode of our algorithm.
Algorithm 1 Smoothie
Input: Environment ENV, learning rates η∏ ,ηQ, discount factor γ, KL-Penalty λ, batch size B,
number of training steps N, target network lag τ .
Initialize θ, φ, w , set θ0 = θ, φ0 = φ, w0 = w.
for i = 0 to N - 1 do
// Collect experience
Sample action a 〜 N(μe(s), Σφ(s)) and apply to ENV to yield r and s0.
Insert transition (s, a, r, s0) to replay buffer.
//Train μ, Σ
Sample batch {(sk, ak, rk, s0k)}kB=1 from replay buffer.
Compute gradients gk
Compute Hessians Hk
dQW (Sk,a) I
da	∣a=μθ(Sk).
∂2QW(sk,a) I
da2	∣a=μθ (sk)
Compute KL-penalties KLk = KL(μθ, ∑φ∣∣μθo, Σφo).
Compute updates
∆θ = ∆φ	B BB PB=1 gk Vθμθ(sk) — λVθKLk, 二 ⅛ Pk=1 1 HkV0”(sk) — λvφKLk.
Update θ J θ + η∏ ∆θ, φ J φ + η∏ ∆φ.
一一 :一
//Train Qπ
Sample batch {(sk ,dk ,rk, sk )}B=ι from replay buffer.
Sample phantom actions ak 〜 N(Gk, Σφ(sk)).
Compute loss L(W) = B PB=I(Qw(s,a) - r - γQW,(S0,μθo(S0)))2.
Update W J W — ηQVwL(W)
// Update target variables
Update θ0 J (1 - τ)θ0 + τθ, φ0 J (1 - τ)φ0 + τφ, W0 J (1 - τ)W0 + τW.
end for
6
Under review as a conference paper at ICLR 2018
We perform a number of evaluations of Smoothie compared to DDPG. We choose DDPG as a base-
line because it (1) utilizes gradient information of a Q-value approximator, much like our algorithm;
and (2) is a standard algorithm well-known to have achieve good, sample-efficient performance on
continuous control benchmarks.
5.1	A Synthetic Task
To evaluate Smoothie we begin with a simple synthetic task which allows us to study its behavior
in a restricted setting. We devised a simple single-action one-shot environment in which the reward
function is a mixture of two Gaussians, one better than the other (see Figure 1 (Right)). We initialize
the policy mean to be centered on the worse of the two Gaussians. We plot the learnable policy
mean and standard deviation during training for Smoothie and DDPG in Figure 1 (Left). Smoothie
learns both the mean and variance, while DDPG learns only the mean and the variance plotted is the
exploratory noise, whose scale is kept fixed during training.
As expected we observe that DDPG cannot escape the local optimum. At the beginning of training it
exhibits some movement away from the local optimum (likely due to the initial noisy approximation
given by Qπw), it is unable to progress very far from the initial mean. Note that this is not an issue
of exploration. The exploration scale is high enough that Qπw is aware of the better Gaussian. The
issue is in the update for μθ, which is only with regard to the derivative of Qw at the current mean.
On the other hand, we find Smoothie is successfully able to solve the task. This is because the
smoothed reward function approximated by Qw has a derivative which clearly points μθ towards the
better Gaussian. We also observe that Smoothie is able to suitably adjust the covariance Σφ during
training. Initially, Σφ decreases due to the concavity of the smoothed reward function. As a region
of convexity is entered, it begins to increase, before again decreasing to near-zero as μθ approaches
the global optimum.
Smoothie	DDPG	Reward	Smoothed Reward I
Figure 1: Left: The learnable policy mean and standard deviation during training for Smoothie and
DDPG on a simple one-shot synthetic task. The standard deviation for DDPG is the exploratory
noise kept constant during training. Right: The reward function for the synthetic task along with
its Gaussian-smoothed version. We find that Smoothie can successfully escape the lower-reward
local optimum. We also notice Smoothie increases and decreases its policy variance as the convex-
ity/concavity of the smoothed reward function changes.
5.2	Continuous Control
We now turn our attention to standard continuous control benchmarks available on OpenAI
Gym (Brockman et al., 2016) utilizing the MuJoCo environment (Todorov et al., 2012).
Our implementations utilize feed forward neural networks for policy and Q-values. We parame-
terize the covariance Σφ as a diagonal given by eφ . The exploration for DDPG is determined by
an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930; Lillicrap et al., 2016). Additional
implementation details are provided in the Appendix.
7
Under review as a conference paper at ICLR 2018
HalfCheetah
Swimmer
Humanoid
Ant
TRPO
Smoothie with KL-PenaIty
DDPG
Figure 2: Results of Smoothie, DDPG, and TRPO on continuous control benchmarks. The x-axis
is in millions of environment steps. Each plot shows the average reward and standard deviation
clipped at the min and max of six randomly seeded runs after choosing best hyperparameters. We
see that Smoothie is competitive with DDPG even when DDPG uses a hyperparameter-tuned noise
scale, and Smoothie learns the optimal noise scale (the covariance) during training. Moreoever, we
observe significant advantages in terms of final reward performance, especially in the more difficult
tasks like Hopper, Walker2d, and Humanoid. Across all tasks, TRPO is not sufficiently sample-
efficient to provide a competitive baseline.
We compare the results of Smoothie and DDPG in Figure 2. For each task we performed a hyper-
parameter search over actor learning rate, critic learning rate and reward scale, and plot the average
of six runs for the best hyperparameters. For DDPG we extended the hyperparameter search to also
consider the scale and damping of exploratory noise provided by the Ornstein-Uhlenbeck process.
Smoothie, on the other hand, contains an additional hyperparameter to determine the weight on
KL-penalty.
Despite DDPG having the advantage of its exploration decided by a hyperparameter search while
Smoothie must learn its exploration without supervision, we find that Smoothie performs competi-
tively or better across all tasks, exhibiting a slight advantage in Swimmer and Ant, while showing
more dramatic improvements in Hopper, Walker2d, and Humanoid. The improvement is especially
dramatic for Hopper, where the average reward is doubled. We also highlight the results for Hu-
manoid, which as far as we know, are the best published results for a method that only trains on the
order of millions of environment steps. In contrast, TRPO, which to the best of our knowledge is the
only other algorithm which can achieve better performance, requires on the order of tens of millions
of environment steps to achieve comparable reward. This gives added evidence to the benefits of
using a learnable covariance and not restricting a policy to be deterministic.
Empirically, we found the introduction of a KL-penalty to improve performance of Smoothie, es-
pecially on harder tasks. We present a comparison of results of Smoothie with and without the
KL-penalty on the four harder tasks in Figure 3. A KL-penalty to encourage stability is not possible
in DDPG. Thus, our algorithm provides a much needed solution to the inherent instability in DDPG
training.
8
Under review as a conference paper at ICLR 2018
3500
3000
2500
2000
1500
1000
500
0.0	0.5	1.0	1.5	2.0
Hopper	Walker2d	Ant	Humanoid
Smoothie without KL-PenaIty
Figure 3: Results of Smoothie with and without a KL-penalty. The x-axis is in millions of en-
vironment steps. We observe benefits of using a proximal policy optimization method, especially
in Hopper and Humanoid, where the performance improvement is significant without sacrificing
sample efficiency.
Smoothie with KL-penalty
6	Conclusion
We have presented a new Q-ValUe function, Qπ, that is a Gaussian-smoothed version of the standard
expected Q-value, Qπ. The advantage of using Qπ over Qπ is that its gradient and Hessian possess
an intimate relationship with the gradient of expected reward with respect to mean and covariance
of a Gaussian policy. The resulting algorithm, Smoothie, is able to successfully learn both mean
and covariance during training, leading to performance that can match or surpass that of DDPG,
especially when incorporating a penalty on divergence from a previous policy.
The success of Qn is encouraging. Intuitively it may be argued that learning Qπ is more sensible
than learning Qπ . The smoothed Q-values by definition make the true reward surface smoother, thus
possibly easier to learn; moreover the smoothed Q-values have a more direct relationship with the
expected discounted return objective. We encourage future work to further investigate these claims
as well as techniques to apply the underlying motivations for Qπ to other types of policies.
References
Mohammad Gheshlaghi Azar, Vicenc Gomez, and Hilbert J Kappen. Dynamic policy programming.
JMLR, 13, 2012.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In ICML,pp. 449-458, 2017.
Georges Bonnet. Transformations des signaux ale´atoires a travers les systemes non line´aires sans
me´moire. Annals of Telecommunications, 19(9):203-220, 1964.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.
Kamil Ciosek and Shimon Whiteson. Expected policy gradients. arXiv preprint arXiv:1706.05374,
2017.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. ICML, 2012.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, Bernhard Scholkopf, and
Sergey Levine. Interpolated policy gradient: Merging on-policy and off-policy gradient estimation
for deep reinforcement learning. NIPS, 2017.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. ICML, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In NIPS, 2015.
9
Under review as a conference paper at ICLR 2018
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms, 2000.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR,
2016.
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of
continuous actions for deep RL. CoRR, abs/1705.05035, 2017. URL http://arxiv.org/
abs/1705.05035.
Remi Munos, Tom StePleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In NIPS, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. NIPS, 2017a.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy
trust region method for continuous control. arXiv preprint arXiv:1707.01891, 2017b.
Robert Price. A useful theorem for nonlinear devices having gaussian inputs. IRE Transactions on
Information Theory, 4(2):69-72, 1958.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278-1286, 2014.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, vol-
ume 37. 1994.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In ICML, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning. MIT Press, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. NIPS, 2000.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
George E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical
review, 36(5):823, 1930.
Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and
empirical analysis of expected sarsa. In Adaptive Dynamic Programming and Reinforcement
Learning, 2009. ADPRL’09. IEEE Symposium on, pp. 177-184. IEEE, 2009.
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and
Nando de Freitas. Sample efficient actor-critic with experience replay. ICLR, 2017.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, University
of Cambridge England, 1989.
Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 1991.
10
Under review as a conference paper at ICLR 2018
A Proof of Equation (15)
We note that similar identities for Gaussian integrals exist in the literature (Price, 1958; Rezende
et al., 2014) and point the reader to these works for further information.
The specific identity we state may be derived using standard matrix calculus. We make use of the
fa"that	∂A∣A∣T2 = - 1 |A|-3/2 ∂A ⑷=- 1 回-1/q1，	(⑼
∂A	2 ∂A	2
and for symmetric A,
∂
dA IIvIlA-I= -ATvvT A-1.	(20)
∂A
We omit s from Σ(s) in the following equations for succinctness. The LHS of (15) is
∂
Q Q (s, a)T^N(o∣a, Σ)do
A	∂Σ
=ZA Qπ(s, a) exp {一2||a - all∑-1} (∂∑|2开夕|-1/2 - 2|2开夕|-1/* 2 * *∂∑Ila - all∑-1) da
Q Q QK(S,O)N(a∣a, Σ) (—Σ-1 + Σ-1(α — a)(α — a)TΣ-1) da.
2A
Meanwhile, towards tackling the RHS of (15) we note that
aπ
Q ( , ) = Q Qn(s, a)N(a∖a, Σ)Σ-1(a — a)da.
∂a	A
Thus we have
∂2Qπ(s, a)
∂a2
Q Qπ(s, a) ΣΣ-1(a — a)-ττ-N(a|a, ∑) + N(a∖a, Σ)g∑-1(O — a) j da
A	∂a	∂a
=	Qπ (s, aa)N (aaIa, Σ)(Σ-1(aa — a)(aa — a)T Σ-1 — Σ-1)daa.
A
(21)
(22)
(23)
(24)
B	Compatible Function Approximation
A function approximator Qw of Qn should be sufficiently accurate so that updates for μθ, Σφ are
not affected by substituting 启飞：向 and " Qw(S,a) for (Qd:向 and 茶空^⑷,respectively.
We claim that a Qw is compatible with respect to μθ if
L VaQw (s,a)lα="θ(S)= NBμθ(s)Tw,
2. Vw RS (VaQw(s,a)∣a=μθ(S)-VaQπ(s,a)∣a=μθ(S)) dρπ(s) = 0 (ie., W minimizes the
expected squared error of the gradients).
Additionally, Qπw is compatible with respect to Σφ if
L VaQ w (s,a)la=μθ(s) = vΦςφ(s)t w,
2. Vw Rs (VaQw (s, a)la=μθ (s) - VaQπ(s, a)la=μθ (S)) dρπ (s) = 0 (i.e., w minimizes the
expected squared error of the Hessians).
One possible parameterization of Qπw may be achieved by taking w = [w0, w1, w2] and parameter-
izing
Qw(s, a) = VWo (s) + (a - μθ(S))TTVθμθ(S)Twι + (a - μθ(S))TVφΣφ(s)TW2(a — μθ(s)). (25)
11
Under review as a conference paper at ICLR 2018
Hyperparameter	Range	Sampling
actor learning rate	[1e-6,1e-3]	log
critic learning rate	[1e-6,1e-3]	log
reward scale	[0.01,0.3]	log
OU damping	[1e-4,1e-3]	log
OU stddev	[1e-3,1.0]	log
λ	[1e-6, 4e-2]	log
discount factor	0.995	fixed
target network lag	0.01	fixed
batch size	128	fixed
clipping on gradients of Q	4.0	fixed
num gradient updates per observation	1	fixed
Huber loss clipping	1.0	fixed
Table 1: Random hyperparameter search procedure. We also include the hyperparameters which
we kept fixed.
Proof. We shall show how the conditions stated for compatibility with respect to Σφ are sufficient.
The reasoning for μθ follows via a similar argument. We also refer the reader to Silver et al. (2014)
which includes a similar procedure for showing compatibility.
From the second condition for compatibility with respect to Σφ we have
ZS RaQW (s, a)lα=μθ(s) - Q (s, a)lα=μθ(s)) NN RZ(Qn (s, a)lα=μθ (S)) dρ"(S)= 0. (26)
We may combine this with the first condition to find
(VaQ W (s,a)∣α=μθ (s)VφΣφ(s)dρπ (S) = (VaQπ (s,a)∣α=μθ (s)VφΣφ(s)dρπ (s),	(27)
which is the desired property for compatibility.
While it is reassuring to know that there exists a class of function approximators which are compat-
ible, this fact is largely ignored in practice. Not only is the class of compatible functions heavily
restricted in terms of expressiveness, due to the first set of conditions for μθ, Σφ, it is also impossible
to satisfy the second set of conditions without access to derivative and Hessian information of the
true Qπ . This problem is also present in DDPG, and we feel this issue merits additional investigation
in future work.
C Implementation Details
We utilize feed forward networks for both policy and Q-value approximator. For μθ(s) We use two
hidden layers of dimensions (400, 300) and relu activation functions. For QπW (S, a) and QπW (S, a)
we first embed the state into a 400 dimensional vector using a fully-connected layer and tanh non-
linearity. We then concatenate the embedded state with a and pass the result through a 1-hidden
layer neural network of dimension 300 with tanh activations. We use a diagonal Σφ(S) = eφ for
Smoothie, with φ initialized to -1.
To find optimal hyperparameters we perform a 100-trial random search over the hyperparameters
specified in Table 1. The OU exploration parameters only apply to DDPG. The λ coefficient on
KL-penalty only applies to Smoothie with a KL-penalty.
12