Under review as a conference paper at ICLR 2018
Three factors influencing minima in SGD
Anonymous authors
Paper under double-blind review
Ab stract
We study the statistical properties of the endpoint of stochastic gradient descent
(SGD). We approximate SGD as a stochastic differential equation (SDE) and
consider its Boltzmann Gibbs equilibrium distribution under the assumption of
isotropic variance in loss gradients.. Through this analysis, we find that three fac-
tors - learning rate, batch size and the variance of the loss gradients - control the
trade-off between the depth and width of the minima found by SGD, with wider
minima favoured by a higher ratio of learning rate to batch size. In the equilibrium
distribution only the ratio of learning rate to batch size appears, implying that it’s
invariant under a simultaneous rescaling of each by the same amount. We experi-
mentally show how learning rate and batch size affect SGD from two perspectives:
the endpoint of SGD and the dynamics that lead up to it. For the endpoint, the ex-
periments suggest the endpoint of SGD is similar under simultaneous rescaling of
batch size and learning rate, and also that a higher ratio leads to flatter minima,
both findings are consistent with our theoretical analysis. We note experimentally
that the dynamics also seem tobe similar under the same rescaling of learning rate
and batch size, which we explore showing that one can exchange batch size and
learning rate in a cyclical learning rate schedule. Next, we illustrate how noise
affects memorization, showing that high noise levels lead to better generalization.
Finally, we find experimentally that the similarity under simultaneous rescaling of
learning rate and batch size breaks down if the learning rate gets too large or the
batch size gets too small.
1	Introduction
Despite being massively over-parameterized (Zhang et al., 2016), deep neural networks (DNNs)
have demonstrated good generalization ability and achieved state-of-the-art performances in many
application domains such as image (He et al., 2016) and speech recognition (Amodei et al., 2016).
The reason for this success has been a focus of research recently but still remains an open question.
Our work provides new theoretical insights and useful suggestions for deep learning practitioners.
The standard way of training DNNs involves minimizing a loss function using SGD and its vari-
ants (Bottou, 1998). In SGD, parameters are updated by taking a small discrete step depending on
the learning rate in the direction of the negative loss gradient, which is approximated based on a
small subset of training examples (called a mini-batch). Since the loss functions of DNNs are highly
non-convex functions of the parameters, with complex structure and potentially multiple minima
and saddle points, SGD generally converges to different regions of parameter space depending on
optimization hyper-parameters and initialization.
Recently, several works (Arpit et al., 2017; Advani & Saxe, 2017; Shirish Keskar et al., 2016) have
investigated how SGD impacts generalization in DNNs. It has been argued that wide minima tend to
generalize better than sharp minima (Hochreiter & Schmidhuber, 1997; Shirish Keskar et al., 2016).
This is entirely compatible with a Bayesian viewpoint that emphasizes targeting the probability mass
associated with a solution, rather than the density value ata solution (MacKay, 1992b). Specifically,
(Shirish Keskar et al., 2016) find that larger batch sizes correlate with sharper minima. In contrast,
we find that it is the ratio of learning rate to batch size which is correlated with sharpness of minima,
not just batch size alone. In this vein, while (Dinh et al., 2017) discuss the existence of sharp
minima which behave similarly in terms of predictions compared with wide minima, we argue that
SGD naturally tends to find wider minima at higher noise levels in gradients, and such wider minima
seem to correlate with better generalization.
1
Under review as a conference paper at ICLR 2018
In order to achieve our goal, we approximate SGD as a continuous stochastic differential equation
(Bottou, 1991; Mandt et al., 2017; Li et al., 2017). Assuming isotropic gradient noise, we derive the
Boltzmann-Gibbs equilibrium distribution of this stochastic process, and further derive the relative
probability of landing in one local minima as compared to another in terms of their depth and width.
Our main finding is that the ratio of learning rate to batch-size along with the gradient’s covariances
influence the trade-off between the depth and sharpness of the final minima found by SGD, with a
high ratio of learning rate to batch size favouring flatter minima. In addition, our analysis provides
a theoretical justification for the empirical observation that scaling the learning rate linearly with
batch size (up to a limit) leads to identical performance in DNNs (Krizhevsky, 2014; Goyal et al.,
2017).
We verify our theoretical insights experimentally on different models and datasets. In particular, we
demonstrate that high learning rate to batch size ratio (due to either high learning rate or low batch-
size) leads to wider minima and correlates well with better validation performance. We also show
that a high learning rate to batch size ratio helps prevent memorization. Furthermore, we observe that
multiplying each of the learning rate and the batch size by the same scaling factor results in similar
training dynamics. Extending this observation, we validate experimentally that one can exchange
learning rate and batch size for the recently proposed cyclic learning rate (CLR) schedule (Smith,
2015), where the learning rate oscillates between two levels. Finally, we discuss the limitations of
our theory in practice.
2	Related work
The relationship between SGD and sampling a posterior distribution via stochastic Langevin meth-
ods has been the subject of discussion in a number of papers (Chaudhari et al., 2017; Chen et al.,
2014; Ding et al., 2014; Vollmer et al., 2015; Welling & Teh, 2011; Shang et al., 2015; Sato &
Nakagawa, 2014). In particular, (Mandt et al., 2017) describe the dynamics of stochastic gradient
descent (SGD) as a stochastic process that can be divided into three distinct phases. In the first
phase, weights diffuse and move away from the initialization. In the second phase the gradient
magnitude dominates the noise in the gradient estimate. In the final phase, the weights are near the
optimum. (Shwartz-Ziv & Tishby, 2017) make related observations from an information theoretic
point of view and suggest the diffusion behaviour of the parameters in the last phase leads to the
minimization of mutual information between the input and hidden representation. We also relate the
SGD dynamics to the stationary distribution of the stochastic differential equation. Our derivation
bears similarity with (Mandt et al., 2017). However, while (Mandt et al., 2017) study SGD as an
approximate Bayesian inference method in the final phase of optimization in a locally convex set-
ting, our end goal is to analyze the stationary distribution over the entire parameter space reached by
SGD. Further, our analysis allows us to compare the probability of SGD ending up in one minima
over another (in terms of width and depth), which is novel in our case.
We discuss the Fokker-Planck equation which has appeared before in the machine learning literature
though the exact form and solution we consider we believe is novel. For example, in the online
setting (Heskes & Kappen, 1993) derive a Gibbs distribution from the Fokker-Planck equation, but
the relation there does not give the temperature of the Gibbs distribution in terms of the learning
rate, batch size and gradient covariance.
Our work is also closely related to the ongoing discussion about the role of large batch size and the
sharpness of minima found in terms of generalization (Shirish Keskar et al., 2016). (Shirish Keskar
et al., 2016) showed that SGD ends up in sharp minimum when using large batch size. (Goyal et al.,
2017; Hoffer et al., 2017) empirically observed that scaling up the learning rate, and training for
more epochs, leads to good generalization when using large batch size. Our novelty is in explaining
the importance of the ratio of learning rate to batch size. In particular, our theoretical and empirical
results show that simultaneously rescaling the batch size and learning rate by the same amount leads
SGD to minima having similar width despite using different batch sizes.
Concurrent with this work, (Smith & Le, 2017; Chaudhari & Soatto, 2017) have both analyzed SGD
approximated as a continuous time stochastic process and stressed the importance of the learning
rate to batch size ratio. (Smith & Le, 2017) focused on the training dynamics while (Chaudhari &
Soatto, 2017) explored the stationary non-equilibrium solution for the stochastic differential equa-
tion for non-isotropic gradient noise, but assuming other conditions on the covariance and loss to
2
Under review as a conference paper at ICLR 2018
enforce the stationary distribution to be path-independent. Their solution does not have an explicit
solution in terms of the loss in this case. In contrast to other work, we strictly focus on the explic-
itly solvable case of the Boltzmann-Gibbs equilibrium distribution with isotropic noise. This focus
allows us to relate the noise in SGD, controlled by the learning rate to batch size ratio, with the
width of its endpoint. We empirically verify that the width and height of minima correlates with the
learning rate to batch size ratio in practice.
Our work continues the line of research on the importance of noise in SGD (Bottou, 1998; Roux
et al., 2008; Neelakantan et al., 2015; Mandt et al., 2017). Our novelty is in formalizing the impact
of batch size and learning rate (i.e. noise level) on the width and depth of the final minima, and
empirical verifications of this.
3	Insights From Fokker-Planck
Our focus in this section is on finding the relative probability with which we end optimization in a
region near a minimum characterized by a certain loss value and Hessian determinant. We will find
that the relative probability depends on the local geometry of the loss function at each minimum
along with batch size, learning rate and the covariance of the loss gradients. To reach this result, we
first derive the equilibrium distribution of SGD over the parameter space under a stochastic differen-
tial equation treatment. We make the assumption of isotropic covariance of the loss gradients, which
allows us to write down an explicit closed-form analytic expression for the equilibrium distribution,
which turns out to be a Boltzmann-Gibbs distribution.
3.1	Setup
We follow a similar (though not identical) theoretical setup to (Mandt et al., 2017), approximating
SGD with a continuous-time stochastic process, which we now outline.
Let us consider a model parameterized by θ = {θ1, . . . , θq }. For N training examples xi , i ∈
{1, ..., N}, the loss function, L(θ), and the corresponding gradient g(θ), are defined based on the
sum over the loss values for all training examples. Stochastic gradients g* * * * (S) (θ) arise when we
consider a batch B of size S < N of random indices drawn uniformly from {1, ..., N} and form an
(unbiased) estimate of loss and gradient based on the corresponding subset of training examples
L(S) (θ) = S X /G Xn)，
n∈B
g(S)(θ)=之L(S)(θ).
∂θ
We consider stochastic gradient descent (SGD) with learning rate η, as defined by the update rule
θ(t + 1) = θ(t) - ηg(S)(θ) .
We now make the following assumptions:
(1)	By the central limit theorem (CLT), we assume the noise in the stochastic gradient is Gaus-
Sian with covariance matrix 1 C(θ)
g(S)(θ) = g(θ) + √1S∆g(θ), where ∆g(θ)〜N(0, C(θ)).
We note that the covariance is symmetric positive-semidefinite, and so can be decomposed
into the product of two matrices C(θ) = B(θ)B> (θ) .
(2)	We assume the discrete process of SGD can be approximated by the continuous time limit
of the following stochastic differential equation (known as a Langevin equation)
dθ = -ηg(θ) + √¾ 卬蛇⑴
where f(t) is a normalized Gaussian time-dependent stochastic term.
(1)
Note that the continuous time approximation of SGD as a stochastic differential equation has been
shown to hold in a weak approximation on the condition that the learning rate is small (Li et al.,
2017).
3
Under review as a conference paper at ICLR 2018
Note that we have not made Assumption 4 of (Mandt et al., 2017), where they assume the loss can
be globally approximated by a quadratic. Instead, we allow for a general loss function, which can
have many local minima.
3.2	Three factors influencing equilibrium distribution
The Langevin equation is a stochastic differential equation, and we are interested in its equilib-
rium distribution which gives insights into the behavior of SGD and the properties of the points
it converges to. Assuming isotropic noise, the Langevin equation is well known to have a Gibbs-
Boltzmann distribution as its equilibrium distribution. This equilibrium distribution can be derived
by finding the stationary solution of the Fokker-Planck equation, with detailed balance, which gov-
erns the evolution of the probability density of the value of the parameters with time. The Fokker-
Planck equation and its derivation is standard in the statistical physics literature. In Appendix A we
give the equation in the machine learning context in Eq. (5) and for completeness of presentation we
also give its derivation. In Appendix C we restate the standard proofs of the stationary distribution of
a Langevin system, and provide the resulting Gibbs-Boltzmann equilbirium distribution here, using
the notation of this paper:
Theorem 1	(Equilibrium Distribution). Assume1 that the gradient covariance is isotropic,
i.e. C(θ) = σ2I, where σ2 is a constant. Then the equilibrium distribution of the stochastic dif-
ferential equation 1 is given by
P(θ) = Po exp --2L(θ)
nσ2
(2)
where n ≡ S and Po is a normalization constant, which is well defined for loss functions with L
regularization.
Discussion: Here P(θ) defines the density over the parameter space. The above result says that if
we run SGD for long enough (under the assumptions made regarding the SGD sufficiently matching
the infinitesimal limit), then the probability of the parameters being in a particular state asymptoti-
Cally follows the above density. Note, that n ≡ IS is a measure of the noise in the system set by the
choice of learning rate η and batch size S . The fact that the loss is divided by n emphasizes that the
higher the noise n, the less granular the loss surface appears to SGD. The gradient variance C(θ)
on the other hand is determined by the dataset and model priors (e.g. architecture, model parameter-
ization, batch normalization etc.). This reveals an important area of investigation, i.e., how different
architectures and model parameterizations affect the gradient’s covariance structure. We note that in
the analysis above, the assumption of the gradient covariance C(θ) being fixed and isotropic in the
parameter space is unrealistic. However it is a simplification that enables straightforward insights
regarding the relationship of the noise, batch size and learning rate in the Gibbs-Boltzmann equilib-
rium. We empirically show that various predictions based on this relationship hold in practice.
Returning to SGD as an optimization method, we can ask, given the probability density P(θ) can
we derive the probability of ending at a given minimum, θA, which we will denote by lowercase
PA = PAC, where C is a normalization constant which is the same for all minima (the unnormalized
probability PA is all We are interested in when estimating the relative probability of finishing in a
given minimum compared to another one). This probability is derived in Appendix D, and given in
the following theorem, which is the core insight from our theory.
Theorem 2	(Probability of ending in region near minima θA). Assume the loss has a series of
separated local minima. Consider one such minima, with Hessian HA and loss LA at a minimum
θA. Then the unnormalized probability of ending in a region near minima θA is
1	2LA
PA = √detHAexp (-Q
(3)
where n = IS is the noise used in the SGD process to reach Θa.
Discussion: For this analysis, we qualitatively categorize a minima θA by its loss LA (depth) and
the determinant of the Hessian detHA (a larger determinant implies a sharper minima). The above
1Here we also assume a weak regularity condition that the loss L(θ) includes the regularization term τkθk22
for some τ > 0.
4
Under review as a conference paper at ICLR 2018
result shows that the probability of landing in a specific minimum depends on three factors - learning
rate, batch-size and covariance of the gradients. The two factors that we directly control only appear
in the ratio given by the noise n = η∕S. Note that the proof of this result utilizes a Laplace
Approximation in which the loss near a given minimum can be approximated using a second order
Taylor series in order to evaluate an integral. We emphasize this is not the same as globally treating
the loss as a quadratic.
To study which kind of minima are more likely if we were to reach equilibrium, it is instructive to
consider the ratio of probabilities pA and pB at two distinct minima θA and θB respectively given
by
pA	det HB	2
PB = VdetHAexp (nσ2(LB - LA)
To highlight that towards the equilibrium solution SGD favors wider rather than sharper minima,
let’s consider the special case when LA = LB, i.e., both minima have the same loss value. Then,
PA
PB
det HB
det HA
This case highlights that in equilibrium, SGD favors the minimum with lower determinant of the
Hessian (i.e. the flatter minima) when all other factors are identical. On the flip side, it can be seen
that if two minima have the same curvature (det HA = det HB), then SGD will favor the minima
with lower loss. Finally in the general case when LA ≥ LB, it holds that PA ≥ PB if and only if
ι 2σ2 log(JdetHA)
≤
n	(LA - LB)
≡Y .
That is, there is an upper bound on the inverse of the noise for θA to be favored in the case that its
loss is higher than at θB, and this upper bound depends on the difference in the heights compared
to the ratio of the widths. In particular we can see that if det HB < det HA, then Y < 0, and so
no amount of noise will result in θA being more probable than θB. In words, if the minimum at θA
is both higher and sharper than the minimum at θB, it is never reached with higher probability than
θB, regardless of the amount of noise. However, if det HB > det HA then Y > 0, and there is a
lower bound on the noise
n>
(LA - LB)
2σ2 log(√SHA
(4)
to make θA more probable than θB. In words, if the minimum at θA is higher but flatter than the
minimum at θB, it is favored over θB, as long as the noise is large enough, as defined by eq. (4).
To summarize, the presented theory shows that the noise level in SGD (which is defined by the ratio
of learning rate to batch size) controls the extent to which optimization favors wider over deeper
minima. Increasing the noise by increasing the ratio of learning rate to batch size increases the
probability of wider compared to deeper minima. For a discussion on the relative probabilities of
critical points that are not strictly minima, see appendix D.
4 Experiments
4.1	IMPACT OF S ON THE MINIMA SGD FINDS
In this section, we empirically study the impact of learning rate η and batch size S on the local
minimum that SGD finds. We first focus on a 4-layer Batch Normalized ReLU MLP trained on
Fashion-MNIST (Xiao et al., 2017). We study how the noise ratio n = S leads to minima with
different curvatures and validation accuracy. To measure the curvature at a minimum, we compute
the norm of its Hessian (a higher norm implies higher sharpness of the minimum) using the finite
difference method (Wu et al., 2017). In Figure 1a, we report the norm of the Hessian for local
minima obtained by SGD for different n = η, where η ∈ [5e - 3,1e - 1] and S ∈ [25,1000].
Each experiment is run for 200 epochs; most models reach approximately 100% accuracy on train
5
Under review as a conference paper at ICLR 2018
E-OU Ue-SSωH 6o~∣
4E-01
2E-01
OE+OO
0E-i-00	2E-03	4E-03
η∕s
(a) Correlation of η with logarithm of
norm of Hessian.
U 90.5%∙
U
∈ 90.0%
L-
t 89.5%∙
Φ
q-89.0%
U
5 88.5%∙
e
~ 88.0%
(0
> 87.5%
0E-i-00 IE力 3 2E-03 3E=03 4E：03
η∕s
(b) Correlation of S with validation
accuracy.
Figure 1:	Impact on SGD with ratio of learning rate η and batch size S for 4 layer ReLU MLP on
FashionMNIST.
η=0∙1	η=0∙1
(a) left s=128 , right S=1024
(b) left η=0∙1 right η=0∙01
(U)Ieft S=128 , right S=128
(Ada4u3,ssob p3-eus pu-Aue∙Jn84
(C) left n=0∙1 right η=0∙01
(C)Ieft S=1024 , right S=128
Figure 2:	Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,
SS. α (x-axis) corresponds to the interpolation coefficient. As predicted by our theory, lower Sn ratio
leads to sharper minima (as shown by the left and middle plot).
set. As n grows, we observe that the norm of the Hessian at the minima also decreases, suggesting
that higher SS pushes the optimization towards flatter minima. This agrees with Theorem 2, Eq.(3),
that higher Sn favors flatter over sharper minima.
Figure 1b shows the results from exploring the impact of n = Sn on the final validation performance,
which confirms that better generalization correlates with higher n. Taken together, Fig. 1a and
Fig. 1b imply wider minima correlate well with better generalization. As n = SS increases, SGD
finds local minima that generalize better. In Appendix F.1, we report similar results for Resnet56
applied on CIFAR10 in Figure 8, for a 20 layer ReLU network with good initialization schemes in
Figures 9a and 9c, and with bad initilization in Figure 9b.
To further illustrate the behavior of SGD with different noise levels, we train three Resnet56 models
on CIFAR10 using SGD (without momentum) with different Sn. Our baseline model uses
η=0.1
S=128 .
In comparision, we investigate a large batch model with S=* and a small learning rate model
with S=0.01, which have approximately the same η ratio. We follow (Goodfellow et al., 2014)
by investigating the loss on the line interpolating between the parameters of two models. More
specifically, let θι and θ2 be the final parameters found by SGD using different Sη, we report the
loss values L((1 - α)θ1 + αθ2) for α ∈ [-1, 2]. Results indicate that models with large batch size
(Fig. 2-left) or low learning rate (Fig. 2-middle; both having a lower SS than the baseline) end up in
a sharper minimum relative to the baseline model. These plots are consistent with our theoretical
analysis that higher n = n/S gives preference to wider minima over sharper minima. On the other
hand, figure 2 (right) shows that models trained with roughly the same level of noise end up in
minima of similar quality. The following experiment explores this aspect further.
We train VGG-11 models (Simonyan & Zisserman, 2014) on CIFAR-10, such that all the mod-
els are trained with the same noise level but with different values of learning rate and batch size.
Specifically, we use S=0.1×β, where we set β = 0.25,1,4. We then interpolate between the model
parameters found when training with β = 1 and β = 4 (Fig. 3-left), and β = 1 and β = 0.25
6
Under review as a conference paper at ICLR 2018
CIFAR10 (VGGII): B = L 6 = 4
CIFAR10 (VGGll): β = l, β = 0.25
'dαJ-juφ,SSQJU pφ"u* Pu-X3ejnuu<
(a) β = 1 corresponds to model at α = 0 and (b) β = 1 corresponds to model at α = 0 and
β = 4 corresponds to model at α = 1	β = 0.25 corresponds to model at α = 1
Figure 3:	Interpolation between parameters of models trained with the same learning rate (η) to
batch-size (S) ratio: S=0.1×β, but different η and S values determined by β. As predicted by our
theory, the minima for models with identical noise levels should be qualitatively similar as can be
seen by these plots.
ιoo
80
100
80
-↑-f-- ----- bs=128, lr=0.001 train
—bs=640, IrO.005 train
bs=128, lr=0.001 test
bs=640, lr=0.005 test
---Cyclic BS Train
一Cyclic LRTrain
--Cyclic BS Test
Cyclic LRTest
20


0	100	200	300	0	100	200	300
Epoch	Epoch
Figure 4:	Learning rate schedule can be replaced by an equivalent batch size schedule. The ratio of
learning rate to batch size is equal at all times for both red and blue curves in each plot. Above plots
show train and test accuracy for experiments involving VGG-11 architecture on CIFAR10 dataset.
Left: cyclic batch size schedule (blue) in range 128 to 640, compared to cyclic learning rate schedule
(red) in range 0.001 to 0.005. Right: constant batch size 128 and constant learning rate 0.001 (blue),
compared to constant batch size 640 and constant learning rate 0.005 (red).
(Fig. 3-right). The interpolation results indicate that all the minima have similar width and depth,
qualitatively supporting our theoretical observation that for the same noise ratio SGD ends up in
minima of similar quality.
4.2	S RATIO INFLUENCES LEARNING DYNAMICS OF SGD
In this section we look at two experimental phenomena: firstly, the equilibrium endpoint of SGD
and secondly the dynamical evolution of SGD. The former, was theoretically analysed in the theory
section, while the latter is not directly addressed in the theory section, but we note that the two are
related - the endpoint is the result of the intermediate dynamics.
We experimentally study both phenomena in the following four experiments involving the VGG-
11 architecture on the CIFAR10 dataset, shown in Fig 4. The left plot compares two experiments:
cyclic batch size schedule (blue) in range 128 to 640, compared to cyclic learning rate schedule (red)
in range 0.001 to 0.005. The right plot compares two other experiments: constant learning rate to
hatch ci7P ra`HC Cr η — 0.00I ∕hlιιaʌ CITIrI η — 0.005 ∕rar!∖
batch-size Iatio of S - -ɪ2ɛ- (blue) and S — 64。 (red).
Regarding the first phenomena, of the endpoint of SGD, the test accuracy when training with a cyclic
batch size and cyclic learning rate is 89.39% and 89.24%, respectively, and we emphasize that these
are similar scores. For a constant learning rate to batch-size ratio of TS —岑翳 and TS —智宇,the
7
Under review as a conference paper at ICLR 2018
25.0% random IabelS
76%
岩74%
372%
to 70%
g 68%
⅛ 65%
P
=63%
rŋ
> 62%
603¾% 20% 40% 60% 80% 100%
Random label accuracy
Figure 5: Impact of IS on memorization of MNIST when 25% and 50% of labels in the training set
are replaced with random labels, using no momentum (on the right) or a momentum with param-
eter 0.9 (on the left). We observe that for a specific level of memorization, high η leads to better
generalization. Red has higher value of S than blue.
test accuracy is 87.25% and 86.92%, respectively, and we again emphasize that these two scores are
similar to each other. That in each of these experiments the endpoint test accuracies are similar shows
exchangability of learning rate for batch size for the endpoint, and is consistent with our theoretical
calculation which says characteristics of the minima found at the endpoint are determined by the
ratio of learning rate to batch-size, but not individually on learning rate or batch size. Additional
results exploring cyclical learning rate and batch size schedule are reported in Appendix F.4.
Regarding the second phenomena of the dynamical evolution, we note the similarity of the train-
ing and test accuracy curves for each pair of same-noise curves in each experiment. Our theoreti-
cal analysis does not explain this phenomena, as it does not determine the dynamical distribution.
Nonetheless, we report it here as an interesting observation, and point to Appendix B for some in-
tuition on why this may occur from the Fokker-Planck equation. In Appendix F.2, Fig. 13 we show
in more detail the loss curves. While the epoch-averaged loss curves match well when exchanging
batch size for learning rate, the per-iteration loss is not invariant to switching batch size for learning
rate. In particular, we note that each run with smaller batch-size has higher variance in per-iteration
loss than it’s same-noise pair. This is expected, since from one iteration to the next, the examples
will have higher variance for a smaller batch-size.
The take-away message from this section is that the endpoint and dynamics of SGD are approxi-
mately invariant if the batch size and learning rate are simultaneously rescaled by the same amount.
This is in contrast to a commonly used heuristic consisting of scaling the learning rate with the
square root of the batch size, i.e. of keeping the ratio η/√S constant. This is used for example by
(Hoffer et al., 2017) as a way of keeping the covariance matrix of the parameter update step the
same for any batch size. However, our theory and experiments suggest changing learning rate and
batch size in a way that keeps the ratio n = n/S constant instead, since this results in the same
equilibrium distribution.
4.3	Impact of SGD on Memorization
To generalize well, a model must identify the underlying pattern in the data instead of simply per-
fectly memorizing each training example. An empirical approach to test for memorization is to
analyze how good a DNN can fit a training set when the true labels are partly replaced by random
labels (Zhang et al., 2016; Arpit et al., 2017). The experiments described in this section highlight
that SGD with a sufficient amount of noise improves generalization at a given level of memorization.
Experiments are performed on the MNIST dataset with an MLP similar to the one used by (Arpit
et al., 2017), but with 256 hidden units. We train the MLP with different amounts of random labels
in the training set. For each level of label noise, We evaluate the impact of S on the generalization
performance. Specifically, We run experiments with TS taking values in a grid with batch size in
{50, 100, 200, 500, 1000}, learning rate in {0.005, 0.01, 0.02, 0.05, 0.07, 0.1}, and momentum in
{0.0, 0.9}. Models are trained for 300 epochs. Fig. 5 reports the MLPs performances on both the
noisy training set and the validation set. The results show that larger noise in SGD (regardless if
induced by using a smaller batch size or a larger learning rate) leads to solutions which generalize
better for the same amount of random labels memorized on the training set. Thus, our analysis
8
Under review as a conference paper at ICLR 2018
90
90
S'80
ɔ70
∪60
∏J
C 50
O
440
∏J
P 30
S-
10
10
80
Φ7°
3-
U
B 50
⊂
O 40
⅛30
5 20
β = l (83.0±0.15)
β = 3 (82.7±0.07)
0 = 5 (82.5±0.29)
β = 7 (82.1±0.06)
β = 9 (81.4±0.27)
β = 11 (81.0±0.3)
β = 13 (69.3±7.83)
日=15 (77.5±1.05)*
125	150	175	200
epoch
(a) Train dataset size 12000
——6 = 1 (90.4±0.12)
——β = 3 (90.3±0.12)
——β = 5 (90.3±0.14)
——6 = 7 (90.2±0.09)
——"9 (90.0±0.11)
——P = Il (89.7±0.22)
β=13 (89.1±0.42)
15 (88.6±0.0)*
0	25	50	75	100	125	150	175	200
epoch
(c) Train dataset size 45000
›80
fŋ 70
n
U 60
U
∏J
U 5。
W 40
∏J
ŋ 30
元
> 20
10
10
——β = l (87.1±0.13)
——β = 3 (87.0±0.2)
——0 = 5 (86.6±0.15)
——8 = 7 (86.4±0.14)
——P = 9 (86.3±0.17)
——β = ll (77.8±13.9)
β = 13 (85.8±0.01)*
B = 15 (80.4±3.09)*
0	25	50	75	100	125	150	175	200
epoch
(b) Train dataset size 22500
90
S'8。
⅛7°
∪60
第
C 50
O
440
∏J
P 30
0	25	50	75	100	125	150	175	200
epoch
(d) Half the noise
1 (89.8±0.09)
5 (89.6±0.14)
——P = 9 (89.5±0.11)
——β = 13 (89.3±0.11)
——B = 17 (89.2±0.1)
——B = 21 (88.9±0.16)
β = 25 (85.4±5.43)
3 = 29 (85.1±1.01)*

Figure 6: Breaking point analysis: Our theory suggests the final performance should be similar when
the SGD noise level β∣××η is kept the same. Here We study its breaking point in terms of too large
a learning rate or too small a batch size. (a,b,c)- Validation accuracy for different dataset sizes and
different β values for a VGG-11 architecture trained on CIFAR10. In each experiment, We multiply
the learning rate (η) and batch size (S) with β such that the ratio β××(S=50) is fixed. We observe
that for the same ratio, increasing the learning rate and batch size yields similar performance up
to a certain β value, for which the performance drops significantly. (d)- Breaking point analysis
when half the noise level .nS=00^ is used. The breaking point happens for much larger β when
using a smaller noise. All experiments are repeated 5 times with different random seeds. The graphs
denote the mean validation accuracies and the numbers in the brackets denote the mean and standard
deviation of the maximum validation accuracy across different runs. The * denotes at least one seed
lead to divergence.
highlights that SGD with low noise n = S steers the endpoint of optimization towards a minimum
with low generalization ability.
While Fig 5 reports the generalization at the endpoint, we observe that SGD with larger noise con-
tinuously steers away from sharp solutions throughout the dynamics. We also reproduce the obser-
vation reported by (Arpit et al., 2017): that memorization roughly starts after reaching maximum
generalization. For runs with momentum we exclude learning rates higher than 0.02 as they lead to
divergence. Full learning curves are reported in Fig. 14 included in Appendix F.3.
4.4	Breaking Point of the Theory in Practice
Our analysis relies on the assumption that the gradient step is sufficiently small to guarantee that
the first order approximation of a Taylor’s expansion is a good estimate of the loss function. In
the case where the learning rate becomes too high, this approximation is no longer suitable, and
the continuous limit of the discrete SGD update equation will no longer be valid. In this case, the
9
Under review as a conference paper at ICLR 2018
stochastic differential equation doesn’t hold, and hence neither does the Fokker-Planck equation,
and so we don’t expect our theory to be valid. In particular, we don’t expect to arrive at the same
stationary distribution as indicated by a fixed ratio η∕S,if the learning rate gets too high.
This is exemplified by the empirical results reported in Fig. 6, where similar learning dynamics
and final performance can be observed when simultaneously multiplying the learning rate and batch
size by a factor β up to a certain limit. This is done for different training set sizes to investigate if
the breaking point depends on this factor (Fig. 6 a,b,c). The plots suggest that the breaking point
happens for smaller β values if the dataset size is smaller. We also investigate the influence of β
when half the noise level is used, due to halving the learning rate, in (figure 6 d). These experiments
strongly suggest that the reason behind breaking point is the use ofa high learning rate because the
performance drops at much higher β when the base learning rate is halved. A similar experiment is
performed on Resnets (for results see Fig 7 in the appendix). We highlight other limitations of our
theory in appendix E.
5	Discussion
In the theoretical section of this work we treat the learning rate as fixed throughout training. How-
ever, in practical applications, the learning rate is annealed to a lower value, either gradually or in
discrete jumps. When viewed within our framework, at the beginning with high noise, SGD favors
width over depth of a region, then as the noise decreases, SGD prioritizes the depth more strongly -
this can be seen from Theorem 3 and the comments that follow.
In the theoretical section we made the additional assumption that the covariance of the gradients
is isotropic, in order to be able to derive a closed form solution for the equilibrium distribution.
We do not expect this assumption to hold in practice, but speculate that there may be mechanisms
which drive the covariance towards isotropy, for example one may be able to tune learning rates
on a per-parameter basis in such a way that the combination of learning rate and covariance matrix
is approximately isotropic - this may lead to improvements in optimization. Perhaps some exist-
ing mechanisms such as batch normalization or careful initialization give rise to more equalized
covariance - we leave study of this for future work.
We note further that our theoretical analysis considered an equilibrium distribution, which was in-
dependent of the intermediate dynamics. However, this may not be the case in practice. Without the
isotropic covariance, the system of partial differential equations in the late time limit will in gen-
eral have a solution which will depend on the path through which optimization occurs, unless other
restrictive assumptions are made to force this path dependence to disappear (Chaudhari & Soatto,
2017). Despite this simplifying assumption, our empirical results are consistent with the developed
theory. We leave study of path dependence and dynamics to future work.
In experiments investigating memorization we explored how the noise level changes the preference
of wide minima over sharp ones. (Arpit et al., 2017) argues that SGD first learns true labels, before
focusing on random labels. Our insight is that in the second phase the high level of noise main-
tains generalization. This illustrates the trade-off between width of minima and depth in practice.
When the noise level is lower, DNNs are more likely to fit random labels better, at the expense of
generalizing less well on true ones.
6	Conclusions
We shed light on the role of noise in SGD optimization of DNNs and argue that three factors (batch
size, learning rate and gradient variance) strongly influence the properties (loss and width) of the
final minima at which SGD converges. The learning rate and batch size of SGD can be viewed as
one effective hyper-parameter acting as a noise factor n = η∕S. This, together with the gradient
covariance influences the trade-off between the loss and width of the final minima. Specifically,
higher noise favors wider minima, which in turn correlates with better generalization.
Further, we experimentally verify that the noise n = η∕S determines the width and height of the
minima towards which SGD converges. We also show the impact of this noise on the memorization
phenomenon. We discuss the limitations of the theory in practice, exemplified by when the learning
10
Under review as a conference paper at ICLR 2018
rate gets too large. We also experimentally verify that η and S can be simultaneously rescaled as
long as the noise n/S remains the same.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-
to-end speech recognition in english and mandarin. In International Conference on Machine
Learning,pp.173-182, 2016.
Devansh Arpit, StaniSlaW JaStrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep netWorks. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings of Machine Learning Research, pp. 233-242, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://Proceedings.mlr.press/
v70/arpit17a.html.
Leon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nimes, 91(8),
1991.
Leon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,
17(9):142, 1998.
Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. arXiv preprint arXiv:1710.11029, 2017.
Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillame Carlier. Deep
relaxation: partial differential equations for optimizing deep neural networks. arXiv preprint
arXiv:1704.04932, 2017.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proceedings
of the 31st International Conference on Machine Learning, pp. 1683-1691, 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using
stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27, pp.
3203-3211, 2014.
L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp Minima Can Generalize For Deep Nets. ArXiv
e-prints, March 2017.
C.	Gardiner. Stochastic Methods: A Handbook for the Natural and Social Sciences. Springer
Series in Synergetics. Springer Berlin Heidelberg, 2010. ISBN 9783642089626. URL https:
//books.google.ca/books?id=321EuQAACAAJ.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network
optimization problems. arXiv preprint arXiv:1412.6544, 2014.
P. Goyal, P. Dolldr, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and
K. He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. ArXiv e-prints, June 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Tom M. Heskes and Bert Kappen. On-line learning processes in artificial neural networks. vol-
ume 51 of North-Holland Mathematical Library, pp. 199 - 233. Elsevier, 1993. doi: https:
//doi.org/10.1016/S0924-6509(08)70038-2. URL http://www.sciencedirect.com/
science/article/pii/S0924650908700382.
11
Under review as a conference paper at ICLR 2018
SePP Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1-42, 1997.
E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap
in large batch training of neural networks. ArXiv e-prints, May 2017.
Robert E. Kass and Adrian E. Raftery. Bayes factors. Journal of the American Statistical As-
sociation, 90(430):773-795, 1995. doi: 10.1080/01621459.1995.10476572. URL http:
//amstat.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572.
Alex Krizhevsky. One weird trick for Parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaPtive stochastic gradi-
ent algorithms. In Doina PrecuP and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, PP.
2101-2110, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/li17f.html.
D.	J. C. MacKay. A Practical bayesian framework for backProPagation networks. Neural Computa-
tion, 4(3):448-472, May 1992a. ISSN 0899-7667. doi: 10.1162/neco.1992.4.3.448.
David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of
Technology, 1992b.
S.	Mandt, M. D. Hoffman, and D. M. Blei. Stochastic Gradient Descent as APProximate Bayesian
Inference. ArXiv e-prints, APril 2017.
A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding
Gradient Noise ImProves Learning for Very DeeP Networks. ArXiv e-prints, November 2015.
Nicolas L. Roux, Pierre antoine Manzagol, and Yoshua Bengio. ToPmoumoute on-
line natural gradient algorithm. In J. C. Platt, D. Koller, Y. Singer, and S. T.
Roweis (eds.), Advances in Neural Information Processing Systems 20, PP. 849-
856. Curran Associates, Inc., 2008. URL http://papers.nips.cc/paper/
3234-topmoumoute-online-natural-gradient-algorithm.pdf.
Issei Sato and Hiroshi Nakagawa. APProximation analysis of stochastic gradient langevin dynam-
ics by using fokker-Planck equation and ito Process. In Eric P. Xing and Tony Jebara (eds.),
Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceed-
ings of Machine Learning Research, PP. 982-990, Bejing, China, 22-24 Jun 2014. PMLR. URL
http://proceedings.mlr.press/v32/satoa14.html.
Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, and Amos J Storkey. Covariance-controlled
adaPtive Langevin thermostat for large-scale Bayesian samPling. In C. Cortes, N. D. Lawrence,
D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 28, PP. 37-45, 2015.
N. Shirish Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large-Batch
Training for DeeP Learning: Generalization GaP and SharP Minima. ArXiv e-prints, SePtember
2016.
Ravid Shwartz-Ziv and Naftali Tishby. OPening the black box of deeP neural networks via informa-
tion. CoRR, abs/1703.00810, 2017. URL http://arxiv.org/abs/1703.00810.
Karen Simonyan and Andrew Zisserman. Very deeP convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
L.	N. Smith. Cyclical Learning Rates for Training Neural Networks. ArXiv e-prints, June 2015.
Samuel L Smith and Quoc V Le. Understanding generalization and stochastic gradient descent.
arXiv preprint arXiv:1710.06451, 2017.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (Non-) asymPtotic ProPerties of stochastic gradient
Langevin dynamics. arXiv preprint arXiv:1501.00438, 2015.
12
Under review as a conference paper at ICLR 2018
M.	Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Pro-
Ceedings of the 28th International Conference on Machine Learning, pp. 681-688, 2011.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective
of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking
Machine Learning Algorithms. ArXiv e-prints, August 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Appendix
A Derivation of the Fokker-Planck Equation
In this appendix we derive the Fokker-Planck equation. For any stochastic differential equation,
since the evolution is noisy we can’t say exactly where in parameter space the parameter values
will be at any given time. But We can talk about the probability P(θ,t∣θo,to) that a parameter
takes a certain value θ at a certain time t, given that it started at θ0 , t0 . That is captured by the
Fokker-Planck equation, which reads
∂P (θ,t)
-∂t-
2
Vθ ∙ ηg(θ)P(θ,t) + n-Vθ ∙ [C(θ)P(θ,t)].
2S
(5)
In this appendix we will derive the above equation from the stochastic differential equation (6).
We will not be interested in pure mathematical rigour, but intend the proof to add intuition for the
machine learning audience.
For brevity we will sometimes write the probability just as P(θ, t). We will sometimes make use
of tensor index notation, where a tensor is denoted by its components, (for example θi are the
components of the vector θ), and we will use the index summation convention, where a repeated
index is to be summed over.
We start with the stochastic differential equation
dθ = -ηg(θ)+ √ηS B⑻f ⑴.
(6)
A formal expression for the probability for a given noise function f is given by P(θ, t) = δ(θ - θf),
but since we don’t know the noise, we instead average over all possible noise functions to get
P(θ, t) =E[δ(θ-θf)].
(7)
While this is just a formal solution we will make use of it later in the derivation.
We now consider a small step in time δt, working at first order in δt, and ask how far the parameters
move, denoted δ θ, which is given by integrating (6)
δθ
η	t+δt
-ηgδt + √s * * B J	f (t )dt
(8)
where we’ve assumed that δθ is small enough that we can evaluate g at the original θ. We now look
at expectations of δθ. Using the fact that the noise is normalized Gaussian, E(f (t)) = 0, we get
(switching to index notation for clarity)
E(δθi) = -ηgiδt	(9)
and using that the noise is normalized Gaussian again, we have that E(f (t)f (t0)) = Iδ(t - t0),
leading to
η2
E(δθiδθj) = η-Cij δt + O(δt2).
S
(10)
13
Under review as a conference paper at ICLR 2018
If at time t we are at position θ0 and end up at position θ = θ0 + δθ at time t + δt, then we can take
(7) with θf = θ0 and Taylor expand it in δθi
∂	1	∂2
P(θ,t + δt∣θ0,t)= h+ E(δθi) — + 2E(δθiδθj)dθ7dθ7 + O(δt2)∖ δ(θ - θ0). (11)
We deal with the derivatives of the delta functions in the following way. We will use the following
identity, called the Chapman-Kolmogorov equation, which reads
P(θ,t + δt∣θo,to)
Z+∞
∞
dθ0P(θ, t + δt∣θ0, t0)P (θ0, t0∣θo, to)
(12)
for any t0 , such that t0 ≤ t0 ≤ t + δt. This identity is an integral version of the chain rule of
probability, stating there are multiple paths of getting from an initial position θ0 to θ at time t + δt
and one should sum all of these paths.
We will now substitute (11) into the first term on the right hand side of (12) with t0 set to be t,
and apply integration by parts (assuming vanishing boundary conditions at infinity) to move the
derivatives off of the delta functions and onto the other terms. We end up with
∂
P(θ,t + δt∣θ0,t)= P(θ,t∣θo,to) — - (E(δθi)P(θ,t∣θo,to))
∂θi
1	∂2
+ 2∂θ7∂θ7 (E(δθiδθj)P(θ,t∣θ0,t0)).
ij
(13)
We can then take the first term on the right hand side of (13) to the other side, insert (9) and (10),
divide by δt and take the limit δt → 0, getting a partial derivative with respect to time on the left
hand side, leading directly to the Fokker-Planck equation quoted in the text (5) (where we have
reverted back to vector notation from index notation for conciseness)
∂P(θ,t)	▽
= vθ ∙
2
ηg(θ)P(θ,t) + n-Vθ ∙ [C(θ)P(θ,t)].
2S
(14)
B Intuitions from Fokker-Planck
In this appendix we will give some supplementary comments about the intuition we can gain from
the Fokker-Planck equation (5). If the learning rate, batch size are constant and the covariance is
proportional to the identity C = σ2I and σ2 is constant, then we can rewrite the Fokker-Planck
equation in the following form
7P∂θJ) = Vθ ∙ [g(θ)P(θ,tη)] + ησ2vθP(θ,tη),
(15)
where we have rescaled the time coordinate to be tη ≡ tη. One can now see that in terms of this
rescaled time coordinate, the ratio between the drift and diffusion terms is governed by the following
ratio
ησ2
式.
(16)
In terms of the balance between drift and diffusion, we see that a higher value of this ratio gives
rise to a more diffusive evolution, while a lower value allows the potential drift term to dominate.
In the next section we will see how this ratio controls the stationary distribution at which SGD
converges to. For now we highlight that in terms of this rescaled coordinate, only this ratio controls
the evolution towards this stationary distribution (not just its endpoint), in terms of the rescaled time
tη . That is, learning rate and batch size are interchangable in the sense that the ratio is invariant
under transformations S → aS, η → aη, for a > 0. But note that the time it takes to reach the
stationary distribution depends on η as well, because of the rescaled time variable. For example,
for a higher learning rate, but constant ratio, one arrives at the same stationary distribution, but in a
quicker time by a factor of 1∕η.
However, a caution here is necessary. The first order SGD update equation only holds for small
enough η that a first order approximation to a Taylor expansion is valid, and hence we expect the
first order approximation of SGD as a continuous stochastic differential equation to break down for
high η. Thus, we expect learning rate and batch size to be interchangable up to a maximum value of
η at which the approximation breaks.
14
Under review as a conference paper at ICLR 2018
C Equilibrium Distribution
In this appendix we will prove equation (2) quoted in the main text, which claims that for an isotropic
covariance, C(θ) = σ2I, with σ constant, the equilibrium solution of the Fokker-Planck equation
has the following form
P(θ) = Po exp --2L(θ))
nσ2
(17)
where n ≡ Sn and P0 is a normalization constant, which is well defined for loss functions with L2
regularization.
In order to prove this is the equilibrium distribution, we need to solve the Fokker-Planck equation
(5) with the left hand side set equal to zero (which would give a stationary distribution) and further
we require for equilibrium that detailed balance holds. To do this, we begin by writing the Fokker-
Planck equation in a slightly different form, making use of a probability current J, defined
2
J ≡ ηg(θ)P(θ,t) + MVθ ∙ (c(θ)P(θ,t))
2S
in which the Fokker-Planck equation becomes
∂P(θ,t) _v J
∂t = vθ .
At this point we use the assumption that c(θ) = σ2I to get
J = ηg(θ)P (θ,t) + η2σ2 Vθ P(θ,t).
2S
(18)
(19)
(20)
The stationary solution has dP∂θ," = 0 and hence Vθ ∙ J = 0. But We require the equilibrium
solution, which is a stronger demand than just the stationary solution. The equilibrium solution is
a particular stationary solution in Which detailed balance occurs. Detailed balance means that in
the stationary solution, each individual transition balances precisely With its time reverse, resulting
in zero probability currents, see §5.3.5 of (Gardiner, 2010), i.e. that J = 0. Detailed balance is a
sufficient condition for having entropy increasing with time. Non-Zero J with Vθ ∙ J = 0 would
correspond to a non-equilibrium stationary distribution, Which We don’t consider here.
For the equilibrium solution, J = 0, we get the result for the probability distribution
P(θ) = P0 exp
(21)
which is the desired stationary solution we intended to find. Finally, to ensure that P0 is a finite
value, we note that the loss function L(θ) can be decomposed as,
L(θ) =L0(θ) +τkθk2	(22)
where τ is some non-negative constant controlling L2 regularization. Then we see that,
P0
θ
Zθ
Zθ
2S		
exp	-宿L叫）	
exp	-二 Lo(θ) ησ2	- τkθk
exp	-至2 Lo(θ)'	exp (-
2
τkθk2
Since Lo(θ) ≥ 0 and / > 0, We have that exp (一券Lo(θ)) ≤ 1, thus,
Z exp (一ηS2Lo(θ))exp(-τ∣∣θ∣∣2) ≤ Z exp(-τ∣∣θ∣∣2)
(23)
(24)
(25)
(26)
and we now note that the right hand side of (26) is finite because it is just a multidimensional
Gaussian integral. Thus, P0 has a finite value and hence the stationary distribution P(θ) is well
defined.
15
Under review as a conference paper at ICLR 2018
D Derivation of Probability of Ending in a given Minima
In this appendix we derive the discrete set of probabilities of ending at each minima, as given in (3).
Essentially, we will use Laplace’s method to approximate the integral of the probability density in
the region near a minimum. This is a common approach used to approximate integrals, appearing
for example in (MacKay, 1992a) and (Kass & Raftery, 1995). We work locally near θA and take the
following approximation of the loss function, since it is near a minimum,
L(O) [ ≈ LA + 2(θ ― θA)>HA(θ ― θA).	(27)
where HA is the Hessian, and is positive definite for a minimum, and where the subscript RA
indicates that this approximation only holds in the region RA near the minimum θA . We emphasize
this is not the same as Assumption 4 of (Mandt et al., 2017), where they assume the loss can be
globally approximated by a quadratic. In contrast, we allow for a general loss, with multiple minima,
and locally to each minima approximate the loss by its second-order Taylor expansion in order to
evaluate the integral in the Laplace method.
The distribution PA(θ) is a probability density, while we are interested in the discrete set of prob-
abilities of ending at a given minimum, which we will denote by lowercase pA . To calculate this
discrete set of probabilities, for each minimum we need to integrate this stationary distribution over
an interval containing the minimum.
Integrating (17) in some region RA around θA and using (27)
pA ≈


P0 Z exp (---2L(O)I )
RA	ησ2	RA
P0 [ exP  ----2 [LA + ɔ(θ - θA)>HA(θ - θA)
RA	ησ2	2	.
P0 exp (-2SLA W d⅛A
(28)
(29)
(30)
where in the last line we assume the region is large enough that an approximation to the full Gaus-
sian integral can be used (i.e. that the tails don’t contribute, which is fine as long as the region is
sufficiently larger than det HA) - note that the region can't be too large, otherwise We would inval-
idate our local assumption. The picture is that the minima are sufficiently far apart that the region
can be taken sufficiently large for this approximation to be valid. Note that P0 is different to P0 and
includes the normalization factors from performing the Gaussian integral, which are the same for all
minima.
Since we are interested in relative probabilities between different minima, so we can consider the
unnormalized probability, dropping factors that are common amongst all minima we get
(31)
This is the required expression given in the main text.
We note that the derivation used above talks about strict minima, i.e., minima with a positive definite
Hessian. In practice however, for deep neural networks with a large number of parameters, it is
unrealistic to expect the endpoint of training to be in a strict minimum. Instead, it is more likely
to be a point at which the Hessian has positive eigenvalues in a few directions, while the other
eigenvalues are (approximately) zero. In such cases, to understand which minima SGD favors, we
can consider the fact that at equilibrium, the iterate of SGD follows the distribution,
P(θ) = Po exp (-")
nσ2
(32)
By definition this means that at any time during equilibrium, the iterate is more likely to be found in
a region of higher probability. In the restrictive case of strict minimum, we model it as a Gaussian
and characterize the probability of landing in a minimum depending on the curvature and depth of
the loss around that minimum. In the general case of minima with degenerate (flat) directions, we
can say that a minimum with more volume is a more probable one.
16
Under review as a conference paper at ICLR 2018
O 50 IOO 150	200	250	300
Epoch
Figure 7: Experiments involving Resnet56 architecture on CIFAR10 dataset. In each curve, we
multiply the IS ratio by a given factor (increasing both batch size and learning rate). We observe that
multiplying the ratio by a factor up to 5 results in similar performances. However, the performance
degrades for factor superior to 5.
E Limitations of our Analysis
Due to our assumptions, we expect the theory to become unreliable when the discrete to continuous
approximation fails, when the covariance in gradients is non-isotropic, when the batch size becomes
comparable to the finite size of the training set and when momentum is considered.
We discussed the limits of the discrete to continous approximation in 4.4 (further illustrated in 7).
When the covariance in gradients is highly non-isotropic, the equilibrium solution to the Fokker-
Planck equation is the solution to a complicated partial differential equation, and one can’t easily
spot the solution by inspection as we do in Appendix C. We expect this approximation to break
down especially in the case of complicated architectures where different gradient directions will be
have very different gradient covariances.
Our theory does not involve the finite size of the training set, which is a drawback of the theory. This
may be especially apparent when the batch size becomes large, compared to the training set size,
and we expect the theory to break down at this point.
Finally, we mention that momentum is used in practical deep learning optimization algorithms. Our
theory does not consider momentum, which is a drawback of the theory and we expect it to break
down in models in which momentum is important. We can write a Langevin equation for the case
of momentum, with momentum damping coefficient μ,
dv
dt
-μv - ηg +
where the velocity is v = dθ/dt.
The Fokker-Planck equation corresponding to this Langevin equation with momentum is
(It + v ∙ vθ) P = Ne ∙ μvVP + gP) + ⅛2vθP
where P is now a function of velocity v as well as θ and t, and we’ve again assumed the gradient
covariance varies slowly compared to the loss.
From this more complicated Fokker-Planck equation, it is hard to spot the equilibrium distribution.
We leave the study of this for further work. For now, we just note that a factor of η can be taken
from the right hand side, and then the ratio between the diffusion and drift terms will again be the
ratio ησ2∕(2S).
17
Under review as a conference paper at ICLR 2018
IDT
10~3
LR/BS
Figure 8:	Validation accuracy of Resnet56 networks against different ratios learning rate to batch
size on CIFAR10. Trained with SGD.
IE-Ol-
8E-02
4E-02
0E+00-
E」OU u5ssωH ∙60~l
OE+OO 4E-04	8E-04
∏∕s
φ
H 89%-
(0
⅛ 88%-
t
Φ
0.87%-
A 86%-
ra
ra 85%-
0E+001E-04 2E-04 3E-04 4E-04 5E-04 6E-04
η∕S
φ 89%
(J
≡ 89%
(o
E 88%
⅛ 88%
(D
0.87%
qS7%
⅛ 86%
i86%
> 85%∙
OE-I-OO 2E-04 4E-04 6E-04 8E4)4 IE-03
n/s
(a) Correlation of S With Log (b) Correlation of S With Valida- (C) Correlation of S With Valida-
Hessian Norm.	tion accuracy for bad initializa- tion accuracy.
tion.
Figure 9:	ImpaCt on SGD With ratio of learning rate η and batCh size S for 20 layer ReLU MLP
Without BatCh Normalization on FashionMNIST.
F	Other Empirical Results
F.1 Further Impacts of SGD on Minima
In this appendix We look at other experiments exploring the Correlation betWeen learning rate to
batCh-size ratio and sharpness of minima, and to Validation performanCe. In Figure 8, We report the
Validation aCCuraCy for Resnet56 models trained on CIFAR10 With different learning rate to batCh-
size ratio. We notiCe there is a peak Validation aCCuraCy at a learning rate to batCh-size ratio of
around 2 × 10-3. In partiCular, this example emphasizes that higher learning rate to batCh-size ratio
doesn’t neCessarily lead to higher Validation aCCuraCy. Instead, it aCts as a Control on the Validation
aCCuraCy, and there is an optimal learning rate to batCh-size ratio.
Figures 9a and 9C shoW the results of a Variant of 4 layer ReLU experiment, Where We use 20
layers and remoVe BatCh Normalization. The inspiration is to test prediCtions of theory in the more
Challenging setup. We again obserVe a Correlation of Hessian norm With learning rate to batCh-size
ratio (though Weaker), and similarly betWeen Validation performanCe and learning rate to batCh-size
ratio.
Figures 10,11, 12 report additional line interpolation plots betWeen models that uses different learn-
ing rate to batCh size ratio but similar learning rate deCay throught training. We repeat the experiment
seVeral time to ensure robustness With respeCt to the model random initializations. Results indiCate
that models With large batCh size or loW learning rate end up in a sharper minimum relatiVe to the
baseline model.
F.2 More Learning Rate & Batch Size Exchangeability
In this appendix We shoW in more detail the experiments of SeCtion 4.2 WhiCh shoW the exChange-
ability of learning rate and batCh size. In Fig. 13 We shoW the log Cross entropy loss, the epoCh-
aVeraged log Cross entropy loss, train, Validation and test aCCuraCy as Well as the batCh size sChedule,
learning rate sChedule. We see in the learning rate to batCh-size ratio plot that the orange and blue
18
Under review as a conference paper at ICLR 2018
η∕S = 0.1/128 ηlS = 0.U1024	η∣S = 0.1/128 ηlS =。山1。24
----B"∙ccwwqr
I---Kvleccoivcir
----BfIlQW
----ESe
ηlS = 0.1/128 η∕S = 0.171024	η∣S = 0.1/128 η∕S = 0.171024
∞ ∞ « M O
sso-l/Aae-nusv
—bvkι Kcoivcir
—Vtfaccwwcir
—bvkι tow
—BlQw
Figure 10: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,
SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using a
different random initialization.
ηlS= 0.1/128 η∕S = 0.01/128
----bvkι BccoiVCir
----Vtfaccwwcir
---Iivkitow
----BllBa
∞ ∞ « M O
B3⅞um⅛uu<
ηlS= 0.1/128 η∕S = 0.01/128
∞ ∞ « M O
B3⅞um⅛uu<
ηlS = 0.1/128 η∕S = 0.01/128	ηlS = 0.1/128 η∕S = 0.01/128
---bvkι KCQrncy
---Vtfaccwwcv
---bθιg
---ViiIiBa
∞ ∞ « M O
SSo-I/A3e-n3v
-1.0 -0.5 0.0	0.5	1.0	1.5	2.0
806040m
sso-l/Aae-nusv
-1.0 -0.5	0.0	0.5	1.0	1.5	2.0
Figure 11: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,
SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using a
different random initialization.
1∞806040
B3⅞um⅛uu<
n∣S= 0.1/1024 n∕S= 0.01/128
806040m
B3⅞um⅛uu<
η∕S = 0.1/1024 η∣S = 0.01/128	η∕S = 0.1/1024 η∣S = 0.01/128
806040m
B3⅞UE3UU<
1∞806040m
B3⅞UE3UU<
Figure 12: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,
SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using a
different random initialization.
19
Under review as a conference paper at ICLR 2018
	Test acc	Valid acc	Loss	Hessian norm.
Discrete η	90.04% ± 0.18%	90.30% ± 0.07%	0.048 ± 0.001	36470
Discrete S	90.07% ± 0.32%	90.25% ± 0.06%	0.050 ± 0.002	13918
Triangle η	90.03% ± 0.10%	90.04% ± 0.23%	0.068 ± 0.002	35310
Baseline	87.70% ± 0.56%	88.36% ± 0.13%	0.033 ± 0.001	57838
Table 1: Comparison between different cyclical training schedules (cycle length and learning rate
are optimized using a grid search). Discrete schedules perform similarly, or slightly better than
triangular. Additionally, discrete S schedule leads to much wider minima for similar loss. Hessian
norm is approximated on 1 out of 6 repetitions and measured at minimum value (usually endpoint
of training).
lines (cyclic schedules) have the same ratio of learning rate to batch-size as each other throughout,
and that their dynamics are very similar to each other. The same holds for the green and red lines, of
constant batch size and learning rate. This supports the theoretical result of this paper, that the ratio
S/n governs the stationary distribution of SGD.
We note that it is not just in the stationary distribution that the exchangeability holds in these plots - it
appears throughout training, highlighted especially in the cyclic schedules. We postulate that due to
the scaling relation in the Fokker-Planck equation, the exchangeability holds throughout learning as
well as just at the end, as long as the learning rate does not get so high as to ruin the approximations
under which the Fokker-Planck equation holds. We note similar behaviour occurs also for standard
learning rate annealing schedules, which we omit here for brevity.
F.3 Details on memorization experiment
We report learning curves from memorization experiment with 0.0 momentum, see Fig. 14. We
additionally confirm similar results to the previous experiments and show correlation between batch
size and learning rate ratio and norm of Hessian, see Fig. 15 without momentum (left) and with
momentum 0.9 (right).
F.4 Cyclical Batch and Learning Rate Schedules
It has been observed that a cyclic learning rate (CLR) schedule leads to better generalization (Smith,
2015). In Sec. 4.2 we demonstrated that one can exchange cyclic learning rate schedule (CLR) with
cyclic batch size (CBS) and approximately preserve the practical benefit of CLR. This exchange-
ability shows that the generalization benefit of CLR must come from the varying noise level, rather
than just from cycling the learning rate. To explore why this helps generalization, we run VGG-
11 on CIFAR10 using 4 training schedules: we compared two discrete schedules (where either η
or S switches discretely from one value to another between epochs) and two baseline schedules,
one constant (η is constant) and one triangle (η is interpolated linearly between its maximum and
minimum value). We track the norm of the Hessian and the training loss throughout training. Each
experiment is repeated six times. For each schedule we optimize η in [1e - 3, 5e - 2] and cycle
length in {5, 10, 15} on a validation set. In all cyclical schedules the maximum value (of η or S) is
5× larger than the minimum value.
First, we observe that cyclical schemes oscillate between sharp and wide regions of the parameter
space, see Fig. 16. Next, we empirically demonstrate that a discrete schedule varying either S or η
performs similarly, or slightly better than triangular CLR schedule, see Tab. 1. Finally, we observe
that cyclical schemes reach wider minima at the same loss value, see Fig. 16 and Tab. 1. All of the
above suggest that by changing noise levels cyclical schemes reach different endpoints than constant
learning rate schemes with same final noise level. We leave the exploration of the implications and
a more thorough comparison with other learning schedules for future work.
20
Under review as a conference paper at ICLR 2018
Figure 13: We show in more detail exchangeability of batch size and learning rate in a one-to-one
ratio. In blue, cyclic batch size schedule between size 128 and 640 and fixed learning rate 0.005,
is exchangeable with orange cyclic learning rate schedule between learning rates 0.001 and 0.005
with fixed batch size 128. In green, constant batch size 640 and constant learning rate 0.005 is
exchangeable with, in red, constant batch size 128 and constant learning rate 0.005.
21
Under review as a conference paper at ICLR 2018

25.0% random labels 25.0% random labels
100%
80%
60%
40%-
20%
0	50 100 150 200 250 300	0	50 100 150 200 250 300
50.0% random labels

100%
80%
60%
40%
20%
50.0% random labels
Epoch
Epoch
Figure 14: Learning curves for memorization experiment with momentum 0.0. Solid lines represent
training accuracy, dotted validation accuracy. Warm color indicates higher η ratio.
6.8E+00
∈ 6.6E+00
O
U 6.4E+00
C
~ 6.2E+00-
S
Φ
ɪ 6.0E+00
σι
O 5.8E+00
-I
5.6E+00-
0.0E+00
1.0E-03
ES
2.0E-03
0.0E+00	1.0E-03	2.0E-03
η∕S
Figure 15: Correlation between (approximate) norm of Hessian of best validation minima and learn-
ing rate to batch-size ratio for 0.0 (left) and 0.9 (right) momentum.
Discrete S
Discrete
= 0.001 一 n = 0.005
OSOlOOuO 200 ZSOwO	0 SO 100 UO 200	ZSO 3β0 OSOloOuO 200	ZSOwO OSOlOOuO 200	ZSOwO OSOloOuO 200	ZSOwO
Epoch	Epoch	Epoch	Epoch	Epoch
Figure 16: Cyclical schemes oscillate between sharp and wide regions. Additionally, cyclical
schemes find wider minima than baseline run for same level of loss, which might explain their
better generalization. All cyclical schedules use base η = 0.005 and cycle length 15 epochs, which
approximate convergence at the end of each cycle. Plots from left to right: discrete S, discrete η,
triangle η, constant learning rate η = 0.001, constant learning rate η = 0.005. On vertical axis we
report loss (red) and approximated norm of Hessian (blue).
22