Under review as a conference paper at ICLR 2018
Stochastic Training of
Graph Convolutional Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCNs) are powerful deep neural networks for
graph-structured data. However, GCN computes nodes’ representation recursively
from their neighbors, making the receptive field size grow exponentially with the
number of layers. Previous attempts on reducing the receptive field size by sub-
sampling neighbors do not have any convergence guarantee, and their receptive
field size per node is still in the order of hundreds. In this paper, we develop a
preprocessing strategy and two control variate based algorithms to further reduce
the receptive field size. Our algorithms are guaranteed to converge to GCN’s local
optimum regardless of the neighbor sampling size. Empirical results show that our
algorithms have a similar convergence speed per epoch with the exact algorithm
even using only two neighbors per node. The time consumption of our algorithm
on the Reddit dataset is only one fifth of previous neighbor sampling algorithms.
1	Introduction
Graph convolution networks (GCNs) (Kipf & Welling, 2017) generalize convolutional neural net-
works (CNNs) (LeCun et al., 1995) to graph structured data. The “graph convolution” operation ap-
plies same linear transformation to all the neighbors of a node, followed by mean pooling. By stack-
ing multiple graph convolution layers, GCNs can learn nodes’ representation by utilizing informa-
tion from distant neighbors. GCNs have been applied to semi-supervised node classification (Kipf &
Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling,
2016; Berg et al., 2017) and knowledge graphs (Schlichtkrull et al., 2017), outperforming multi-layer
perceptron (MLP) models that do not use the graph structure and graph embedding approaches (Per-
ozzi et al., 2014; Tang et al., 2015; Grover & Leskovec, 2016) that do not use node features.
However, the graph convolution operation makes it difficult to train GCN efficiently. A node’s
representation at layer L is computed recursively by all its neighbors’ representations at layer L - 1.
Therefore, the receptive field of a single node grows exponentially with respect to the number of
layers, as illustrated in Fig. 1(a). Due to the large receptive field size, Kipf & Welling (2017)
proposed training GCN by a batch algorithm, which computes the representation for all the nodes
altogether. However, batch algorithms cannot handle large scale datasets because of their slow
convergence and the requirement to fit the entire dataset in GPU memory.
Hamilton et al. (2017a) made an initial attempt on developing stochastic algorithms to train GCNs,
which is referred as neighbor sampling (NS) in this paper. Instead of considering all the neighbors,
they randomly subsample D(l) neighbors at the l-th layer. Therefore, they reduce the receptive field
size to Ql D(l), as shown in Fig. 1(b). They found that for two layer GCNs, keeping D(1) = 10 and
D (2) = 25 neighbors can achieve comparable performance with the original model. However, there
is no theoretical guarantee on the predictive performance of the model learnt by NS comparing with
the original algorithm. Moreover, the time complexity of NS is still D(1)D(2) = 250 times larger
than training an MLP, which is unsatisfactory.
In this paper, we develop novel stochastic training algorithms for GCNs such that D(l) can be as
low as two, so that the time complexity of training GCN is comparable with training MLPs. Our
methods are built on two techniques. First, we propose a strategy which preprocesses the first graph
convolution layer, so that we only need to consider all neighbors within L- 1 hops instead ofL hops.
This is significant because most GCNs only have L = 2 layers (Kipf & Welling, 2017; Hamilton
1
Under review as a conference paper at ICLR 2018
GraPhConV
"2)
GraphConv
• Latest activation
• Historical activation
Dropout
H
GraPhConV
4
GraphConv
Dropout
Input
(a) Exact	(b) Neighbour sampling	(C) Control variate
(d) CVD network
Figure 1: Two-layer graph convolutional networks, and the receptive field of a single vertex.
et al., 2017a). Second, we develop two control variate (CV) based stochastic training algorithms.
We show that our CV-based algorithms have lower variance than NS, and for GCNs without dropout,
our algorithm provably converges to a local optimum of the model regardless of D(l).
We empirically test on six graph datasets, and show that our techniques significantly reduce the
bias and variance of the gradient from NS with the same receptive field size. Our algorithm with
D (l) = 2 achieves the same predictive performance with the exact algorithm in comparable number
of epochs on all the datasets, while the training time is 5 times shorter on our largest dataset.
2	Backgrounds
We now briefly review graph convolutional networks (GCNs) (Kipf & Welling, 2017) and the neigh-
bor sampling (NS) algorithm (Hamilton et al., 2017a).
2.1	Graph convolutional networks
The original GCN was presented in a semi-supervised node classification task (Kipf & Welling,
2017). We follow this setting throughout this paper. Generalization of GCN to other tasks can be
found in Kipf & Welling (2016); Berg et al. (2017); Schlichtkrull et al. (2017) and Hamilton et al.
(2017b). In the node classification task, we have an undirected graph G = (V, E) with V = |V|
vertices and E = |E | edges, where each vertex v consists of a feature vector xv and a label yv . The
label is only observed for some vertices VL and we want to predict the label for the rest vertices
VU := V\VL . The edges are represented as a symmetric V × V adjacency matrix A, where Av,v0
is the weight of the edge between v and v0 , and the propagation matrix P is a normalized version of
1	1	1	1	11	1	1	1
A: A = A + I, Dvv = Ev，AvvO, and P = D 2 AD 2. A graph convolution layer is defined as
H (l) = Dropoutp (H ⑷)，Z (l+1) = PH (I)W (I), H (l+1) = σ(Zl+1),	(1)
where H (l) is the activation matrix in the l-th layer, whose each row is the activation of a graph
node. H⑼=X is the input feature matrix, W(l) is a trainable weight matrix, σ(∙) is an activation
function, and Dropoutp(∙) is the dropout operation (Srivastava et al., 2014) with keep probability p.
Finally, the loss is defined as L = ∣^j Pv∈vl f (yv, zVL)), where f (∙, ∙) can be the square loss,
cross entropy loss, etc., depending on the type of the label.
When P = I, GCN reduces to a multi-layer perceptron (MLP) model which does not use the graph
structure. Comparing with MLP, GCN is able to utilize neighbor information for node classification.
We define n(v, L) as the set of all the L-neighbors of node v, i.e., the nodes that are reachable from
v within L hops. It is easy to see from Fig. 1(a) that in an L-layer GCN, a node uses the information
from all its L-neighbors. This makes GCN more powerful than MLP, but also complicates the
stochastic training, which utilizes an approximated gradient VL ;≈	Pv∈Vb Vf (yv, Zv)),
where VB ⊂ VL is a minibatch of training data. The large receptive field size | ∪v∈VB n(v, L)| per
minibatch leads to high time complexity, space complexity and amount of IO. See Table 1 for the
average number of 1- and 2-neighbors of our datasets.
2.2	Alternative notation
We introduce alternative notations to help compare different algorithms. Let U(l) = PH(l), or
UVl) = Pv0∈n(v 1)Pv,vohvlo), we focus on studying how UV is computed based on node v,s neighbors.
To keep notations simple, we omit all the subscripts and tildes, and exchange the ID of nodes such
2
Under review as a conference paper at ICLR 2018
Dataset	V	E	Degree	Degree 2	Type
Citeseer	3,327^^	12,431	4	15	Document network
Cora	2,708	13,264	5	37	Document network
PubMed	19,717	108,365	6	60	Document network
NELL	65,755	318,135	5	1,597	Knowledge graph
PPI	14,755	458,973	31	970	Protein-protein interaction
Reddit	232,965	23,446,803	101	10,858	Document network
Table 1: Number of vertexes, edges, average number of 1- and 2-neighbors per node for each dataset.
Undirected edges are counted twice and self-loops are counted once. Reddit is already subsampled
to have a max degree of 128 following Hamilton et al. (2017a).
that n(v, 1) = [D]+, 1 where D = |n(v, 1)| is the number of neighbors. We get the propagation
rule U = PD=I PVhv, which is used interchangeably with the matrix form U(l) = PH(l).
2.3	Neighbor sampling
To reduce the receptive field size, Hamilton et al. (2017a) propose a neighbor sampling (NS) al-
gorithm. On the l-th layer, they randomly choose D (l) neighbors for each node, and develop an
estimator UNS of U based on Monte-Carlo approximation U ≈ UNS = DDy Pv∈d0) PVhv, where
D(l) ⊂ [D]+ is a subset of D(l) neighbors. In this way, they reduce the receptive field size from
| ∪v∈VB n(v, L)| to O(|VB| QlL=1 D(l)). Neighbor sampling can also be written in a matrix form as
HNS = Dropoutp(HNN)S),	zN+1) = P(I)HNS W ⑷，HN I)= σ(ZN 1)),	(2)
where P(I) is a sparser unbiased estimator of P, i.e., EP(I) = P. The approximate prediction ZNS
used for testing and for computing stochastic gradient ^^ Pvv∈l^b Vf (yv, zCLVv) during training.
The NS estimator UNS is unbiased. However it has a large variance, which leads to biased prediction
and gradients after the non-linearity in subsequent layers. Due to the biased gradients, training
with NS does not converge to the local optimum of GCN. When D(l) is moderate, NS may has
some regularization effect like dropout (Srivastava et al., 2014), where it drops neighbors instead of
features. However, for the extreme ease D(l) = 2, the neighbor dropout rate is too high to reach high
predictive performance, as we will see in Sec. 5.4. Intuitively, making prediction solely depends on
one neighbor is inferior to using all the neighbors. To keep comparable prediction performance with
the original GCN, Hamilton et al. (2017a) use relatively large D(1) = 10 and D(2) = 25. Their
receptive field size D(1) × D(2) = 250 is still much larger than MLP, which is 1.
3	Preprocessing first layer
We first present a technique to preprocess the first graph convolution layer, by approximating
ADropoutp (X) with Dropoutp(AX). The model becomes
Z(l+1) = Dropoutp(P H(l))W (l), H(l+1) = σ(Zl+1).	(3)
This approximation does not change the expectation because E ADropoutp(X)	=
E Dropoutp(AX) , and it does not affect the predictive performance, as we shall see in Sec. 5.1.
The advantage of this modification is that we can preprocess U(0) = PH(0) = PX and takes U(0)
as the new input. In this way, the actual number of graph convolution layers is reduced by one — the
first layer is merely a fully connected layer instead of a graph convolution one. Since most GCNs
only have two graph convolution layers (Kipf & Welling, 2017; Hamilton et al., 2017a), this gives a
significant reduction of the receptive field size from the number of L-neighbors | ∪v∈VB n(v, L)| to
the number of L - 1-neighbors | ∪v∈VB n(v, L - 1)|. The numbers are reported in Table 1.
4	Control variate based stochastic approximation
We now present two novel control variate based estimators that have smaller variance as well as
stronger theoretical guarantees than NS.
1For an integer N, We define [N] = {0,..., N} and [N]+ = {1,..., N}.
3
Under review as a conference paper at ICLR 2018
4.1	Control variate based estimator
We assume that the model does not have dropout for now and will address dropout in Sec. 4.2. The
idea is that we can approximate u = PvD=1 pv hv better if we know the latest historical activations
hv of the neighbors, where We expect h and h are similar if the model weights do not change too
fast during the training. With the historical activations, we approximate
DD	D	D
U =	Epvhv = Epv(hv - hv)	+ Epvhv ≈	Dpv0∆hv0 + Epvhv ：= UCV,	(4)
v=1	v=1	v=1	v=1
where v0 is a	random neighbor, and ∆hvo	= hvo - hvo.	For	the	ease of presentation, we	assume
that we only	use the latest activation of one neighbor, while	the	implementation also include the
node itself besides the random neighbor, so D(l) = 2. Using historical activations is cheap because
they need not to be computed recursively using their neighbors’ activations, as shown in Fig. 1(c).
Unlike NS, we apply Monte-Carlo approximation on Pv pv ∆hv instead of Pv pv hv . Since we
expect hv and hv to be close, ∆hv will be small and UCV should have a smaller variance than
UNS. Particularly, if the model weight is kept fixed, hv should be eventually equal with hv, so that
UCV = 0 + PvD=I pvhv = PvD=I pvhv = u, i.e., the estimator has zero variance. The term CV =
UCV-UNS = -Dpvohvo+PD=ι pvhv isa control variate (Ripley, 2009, Chapter 5), which has zero
mean and large correlation with UNS, to reduce its variance. We refer this stochastic approximation
algorithm as CV, and we will formally analyze the variance and prove the convergence of the training
algorithm using CV for stochastic gradient in subsequent sections.
In matrix form, CV computes the approximate predictions as follows, where we explicitly write
down the iteration number i and add the subscript CV to the approximate activations 2
7(l+I) j_ (^(I)Zfy(I)	疔(I) ∖ _i_ P 疔(I) 、τ4/(I)	d
ZCV,i J	(Pi (HCV,i- HCV,i)+	PHCVi) Wi	,	(5)
τr(l+1) j_	/ 7,(l+1)∖	疔(I) j_	巾(I)H(I)	_i_	∩	Tn(I八疔(I)	/G
HCV,i J	σ(ZCV,i ),	HCV,i+1 J	mi HCV,i +	(1	- mi )HCV,i,	(6)
where hCCVi v stores the latest activation of node V on layer l computed before time i. Formally, let
m(l) ∈ RV×v be a diagonal matrix, and (m(l))vv = 1 if (P(I))vov > 0 for any ν0. After finishing
one iteration we update history H with the activations computed in that iteration as Eq.(6).
4.2	Control variate for dropout
With dropout, the activations H are no longer deterministic. They become random variables whose
randomness come from different dropout configurations. Therefore, ∆hv = hv-h v is not necessar-
ily small even if hv and hv have the same distribution. We develop another stochastic approximation
algorithm, control variate for dropout (CVD), that works well with dropout.
Our method is based on the weight scaling procedure (Srivastava et al., 2014) to approximately
compute the mean μv := E [hv]. That is, along with the dropout model, we can run a copy of the
model with no dropout to obtain the mean μv, as illustrated in Fig. 1(d). With the mean, we can
obtain a better stochastic approximation by separating the mean and variance
D
D
U =): pv [(hv - μv ) + (μv - μv ) + μv] ≈ DDpvo (hvo - μvO) + DpvO δMvo +): pvμv := uCVD ,
v=1
v=1
where μv is the historical mean activation, obtained by storing μv instead of hv, and ∆μ = μv - μv.
ucvd an unbiased estimator of U because the term √Dpvo (hvo - μvo) has zero mean, and the
Monte-Carlo approximation PD=Ipv(μv - μv) ≈ Dpvo∆μvo does not change the mean. The
approximation PD=I pv (hv -μv) ≈ √Dpvo (hvo -μvo) is made by assuming hv,s to be independent
Gaussians, which we will soon clarify. The pseudocodes for CV and CVD are in Appendix E.
4.3	Variance analysis
NS, CV and CVD are all unbiased estimators of U = Pv pv hv . We analyze their variance in a
simple independent Gaussian case, where we assume that activations are Gaussian random variables
2We will omit the subscripts CV and i in subsequent sections when there is no confusion.
4
Under review as a conference paper at ICLR 2018
Alg.	Estimator	Var. from MC. approx.		Var. from dropout
Exact	U = PPv Pv hv	0		s2
NS	UNS = Dpvo hvo	1 Pv,vo(pv μv	—pvo μvo )	Ds2
CV	UCV = Dpvo∆hvo + Pv Pv h v	2 Pv,vo(Pv ∆μv	—pvo ∆μv0 )	Ds2 + (D — 1)s2
CVD	UCVD = VDPvo (hvo — μvo) +Dpvo ∆μvo + ppv pv μv	2 Pv,vo(Pv ∆μv	—pvo ∆μv0 )	s2
Table 2: Variance of different algorithms in the independent Gaussian case.
hv 〜N(μv, Sv) following Wang & Manning (2013). Without loss of generality, We assume that
all the activations hv are one dimensional. We also assume that all the activations h1, . . . , hD and
historical activations hi,..., h° are independent, where the historical activations h 〜N(口嵬,s2).
We introduce a few more notations. ∆μv and ∆s2 are the mean and variance of ∆h = h -h,
where ∆μv = μv — μv and ∆s2 = s2 + Sfv. μ and s2 are the mean and variance of Pv Pv h, where
μ = Pvpvμv and s2 = PvPvs2. Similarly, ∆μ, ∆s2, μ and S are the mean and variance of
Pv pv∆hv and Pv Pv hv, respectively.
With these assumptions and notations, we list the estimators and variances in Table 2, where the
derivations can be found in Appendix C. We decompose the variance as two terms: variance from
Monte-Carlo approximation (VMCA) and variance from dropout (VD).
If the model has no dropout, the activations have zero variance, i.e., sv = sSv = 0, and the only
source of variance is VMCA. We want VMCA to be small. As in Table 2, the VMCA for the exact
estimator is 0. For the NS estimator, VMCA is 11 Pv vo (Pvμv — pvoμvo)2, whose magnitude depends
on the pairwise difference (Pvμv — pvoμvo )2, and VMCA is zero if and only if pvμv = pvoμvo for all
v, v0. Similarly, VMCA for both CV and CVD estimators is 2 Pv vo (Pv∆μv — pvo∆μvo)2, which
should be smaller than NS estimator,s VMCA if (Pv∆μv — pvo∆μvo)2 < (Pvμv — pvoμvo)2, which
is likely because ∆μv should be smaller than μv. Since CV and CVD estimators have the same
VMCA, we adopt the CV estimator for models without dropout, due to its simplicity.
The VD of the exact estimator is s2, which is overestimated by both NS and CV. NS overestimates
VD by D times, and CV has even larger VD. Meanwhile, the VD of the CVD estimator is the same
as the exact estimator, indicating CVD to be the best estimator for models with dropout.
4.4	Exact testing
Besides smaller variance, CV also has stronger theoretical guarantees than NS. We can show that
during testing, CV’s prediction becomes exact after a few testing epochs. For models without
dropout, we can further show that training using the stochastic gradients obtained by CV converges
to GCN’s local optimum. We present these results in this section and Sec. 4.5. Note that the analysis
does not need the independent Gaussian assumption.
Given a model W , we compare the exact predictions (Eq. 1) and CV’s approximate predictions
(Eq. 5,6) during testing, which uses the deterministic weight scaling procedure. To make predictions,
we run forward propagation by epochs. In each epoch, we randomly partition the vertex set V as I
minibatches V1 , . . . , VI and in the i-th iteration, we run a forward pass to compute the prediction for
nodes in Vi . Note that in each epoch we scan all the nodes instead of just testing nodes, to ensure
that the activation of each node is computed at least once per epoch. The following theorem reveals
the connection of the exact predictions and gradients, and their approximate versions by CV.
Theorem 1. For a fixed W and any i > LI we have: (1) (Exact Prediction) The activations
computed by CV are exact, i.e., ZC(lV),i = Z(l) for each l ∈ [L] and HC(lV),i = H(l) for each l ∈ [L—1].
(2)(UnbiasedGradient) The Stochasticgradient gcν,i(W) ：= ∣v1j Pv∈yβ Vwf (yv,z(L)i v) is an
unbiased estimator ofGCN,s gradient, i.e., EPVBgcν,i(W) = VW6 Pv∈v f (yv, zvL)).
Theorem 1 shows that at testing time, we can run forward propagation with CV for L epoches and get
the exact prediction. This outperforms NS, which cannot recover the exact prediction. Comparing
with directly making exact predictions by a batch algorithm, CV is more scalable because it does
not need to load the entire graph into memory. The proof can be found in Appendix A.
5
Under review as a conference paper at ICLR 2018
4.5	Convergence guarantee
The following theorem shows that for a model without dropout, training using CV’s approximated
gradients converges to a local optimum of GCN, regardless of the neighbor sampling size D(l) .
Therefore, we can choose arbitrarily small D(l) without worrying about the convergence.
Theorem 2. Assume that (1) all the activations are ρ-Lipschitz, (2) the gradient of the cost func-
tion Vzf(y,z) is P-Lipschitzandbounded, (3) kgcv(W)k∞ and Ilg(W)k∞ = ∣∣VL(W)k∞ are
bounded by G > 0 for all P, VB and W. (4) The loss L(W) is P-smooth, i.e., ∣L(W2) 一
L(Wi) 一 hVL(W1),W2 一 Wιi∣ ≤ P ∣∣W2 一 Wι∣2 ∀W1,W2, where {A,B> = tr(A>B) is the
inner product of matrix A and matrix B. We randomly run SGD for R ≤ N iterations, where
2γi -ργ2
PR(R =	i)	=	PN	：2y：-PY2).	Then, for the updates	Wi+i	= Wi	—	YigCV(Wi)	and Step	sizes
Yi = min{ P, √N }, there exists constants Ki and K which are irrelevant with N, s.t. ∀N > LI,
2	2ρ(L(Wι) —L(W*)) + K2	2(L(WI)- L(W*)) + Ki
ER〜PREp,Vb IlVL(WR)Il ≤---------------N--------------1---------√N------------.
The proof can be found in Appendix B. Particularly, limw→∞ ER〜PREP VB ∣∣VL(Wr)∣2 = 0.
Therefore, our algorithm converges to a local optimum.
4.6 Time complexity and implementation details
Finally we discuss the time complexity of different algorithms. We decompose the time complex-
ity as sparse time complexity for sparse-dense matrix multiplication such as PH(l), and dense time
complexity for dense-dense matrix multiplication such as U (l)W (l). Assume that the node fea-
ture is K-dimensional and the first hidden layer is A-dimensional, the batch GCN has O(EK)
sparse and O(V KA) dense time complexity per epoch. NS has O(V QlL=i D(l)K) sparse and
O(V QlL=2 D(l)KA) dense time complexity per epoch. The dense time complexity of CV is the
same as NS. The sparse time complexity depends on the cost of computing the sum Pv Pvμv.
There are V QlL=2 D(l) such sums to compute on the first graph convolution layer, and overall cost
is not larger than O(V D QlL=2 D(l)K), if we subsample the graph such that the max degree is D,
following Hamilton et al. (2017a). The sparse time complexity is D/D(i) times higher than NS.
Our implementation is similar as Kipf & Welling (2017). We store the node features in the main
memory, without assuming that they fit in GPU memory as Hamilton et al. (2017a), which makes
our implementation about 2 times slower than theirs. We keep the histories in GPU memory for
efficiency since they are only LH < K dimensional.
5 Experiments
We examine the variance and convergence of our algorithms empirically on six datasets, including
Citeseer, Cora, PubMed and NELL from Kipf & Welling (2017) and Reddit, PPI from Hamilton et al.
(2017a), as summarized in Table 1. To measure the predictive performance, we report Micro-F1 for
the multi-label PPI dataset, and accuracy for all the other multi-class datasets. We use the same
model architectures with previous papers but slightly different hyperparameters (see Appendix D
for the details). We repeat the convergence experiments 10 times on Citeseer, Cora, PubMed and
NELL, and 5 times on Reddit and PPI. The experiments are done on a Titan X (Maxwell) GPU.
5.1 Impact of preprocessing
We first examine the approximation in Sec. 3 that switches the order of dropout and aggregating the
neighbors. Let M0 be the original model (Eq. 1) and M1 be our approximated model (Eq. 3), we
compare three settings: (1) M0, D(l) = ∞ is the exact algorithm without any neighbor sampling. (2)
M1+PP, D(l) = ∞ changes the model from M0 to M1. Preprocessing does not affect the training for
D(l)
= ∞. (3) M1+PP, D(l) = 20 uses NS with a relatively large number of neighbors. In Table 3
we can see that all the three settings performs similarly, i.e., our approximation does not affect the
predictive performance. Therefore, we use M1+PP, D(l) = 20 as the exact baseline in following
convergence experiments because it is the fastest among these three settings.
6
Under review as a conference paper at ICLR 2018
Algorithm Epochs	Citeseer 200	Cora 200	PubMed 200	NELL 200	PPI 500	Reddit 10
M0, D(I) = ∞	708 ± .1	81.7 ± .5	79.0 ± .4	-	97.9 ±.04	96.2 ±.04
M1+PP, Da) =∞	70.9 ± .2	82.0 ± .8	78.7 ± .3	64.9 ± 1.7	97.8 ± .05	96.3 ± .07
M1+PP, Da) =20	709 ± .2	81.9 ± .7	78.9 ± .5	64.2 ± 4.6	97.6 ± .09	96.3 ± .04
Table 3: Testing accuracy of different algorithms and models after fixed number of epochs. Our
implementation does not support M0, D(L) = ∞ on NELL so the result is not reported
1.0
03
O6
。*
1.5
1.0
0.5
0.8
Citeseer
0
3
0.2λ
50	100	150	200	0
nell
l⅝i ------------------- °∙4
cora
0.8
50	100	150
redd it
0.2
200	0
0.6
0.4
pubmed
200
50	100	150
PPi
--Exact
——NS
——NS+PP
——CV+PP
100	200	300	400
0
2
0
1
0.4
0.2
10	20	30	40
50000
200	400	600
800
0
Figure 2:	Comparison of training loss with respect to number of epochs without dropout. The
CV+PP curve overlaps with the Exact curve in the first four datasets.
5.2	Convergence with no dropout
We now study how fast our algorithms converge with a very small neighbor sampling size Da) = 2.
We compare the following algorithms: (1) Exact, which is M1+PP, D(I) = 20 in Sec. 5.1 as a
surrogate of the exact algorithm. (2) NS, which is the NS algorithm with no preprocessing and
D (l) = 2. (3) NS+PP, which is same with NS but uses preprocessing. (4) CV+PP, which replaces
the NS estimator in NS+PP with the CV estimator. (5) CVD+PP, which uses the CVD estimator.
We first validate Theorem 2, which states that CV+PP converges to a local optimum of Exact, for
models without dropout, regardless of D(l). We disable dropout and plot the training loss with
respect to number of epochs as Fig. 2. We can see that CV+PP can always reach the same training
loss with Exact, which matches the conclusion of Theorem 2. Meanwhile, NS and NS+PP have a
higher training loss because their gradients are biased.
5.3	Convergence with dropout
Next, we compare the predictive accuracy obtained by the model trained by different algorithms,
with dropout turned on. We use different algorithms for training and the same Exact algorithm for
testing, and report the validation accuracy at each training epoch. The result is shown in Fig. 3.
We find that CVD+PP is the only algorithm that is able to reach comparable validation accuracy
with Exact on all datasets. Furthermore, its convergence speed with respect to the number of epochs
is comparable with Exact despite its D(l) is 10 times smaller. Note that CVD+PP performs much
better than Exact on the PubMed dataset; we suspect it finds a better local optimum.
0.72
Citeseer
0.80
0.79
cora
50	100	150	200
redd it
0.70
0-690
0.78
0.77rt
200	0
50	100	150
nell
---Exact
——NS
——NS+PP
—CV+PP
——CVD+PP
Figure 3:	Comparison of validation accuracy with respect to number of epochs. NS converges to
0.94 on the Reddit dataset and 0.6 on the PPI dataset.
7
Under review as a conference paper at ICLR 2018
Alg.	Valid. acc.	Epochs	Time (s)	Sparse GFLOP	Dense TFLOP
Exact	96.0	42^^	252	507	7.17
NS	94.4	102.0	577	76.5	21.4
NS+PP	96.0	35.0	195	2.53	7.36
CV+PP	96.0	7.8	56	40.6	1.64
CVD+PP	96.0	5.8	50	60.3	2.44
Figure 4: Comparison of the accuracy
of different testing algorithms. The y-
axis is Micro-F1 for PPI and accuracy
otherwise.
Table 4: Time complexity comparison of different algo-
rithms on the Reddit dataset.
Bias (without dropout)
Standard deviation (without dropout)
Dataset
Standard deviation (with dropout)
Algorithm
■ NS
■ NS+PP
■ CV+PP
■ Exact
Dataset
Bias (with dropout)
Figure 5: Bias and standard deviation of the gradient for different algorithms during training.
Meanwhile, simper algorithms CV+PP and NS+PP work acceptably on most of the datasets. CV+PP
reaches a comparable accuracy with Exact for all datasets except PPI. NS+PP works slightly worse
but the final validation accuracy is still within 2%. These algorithms can be adopted if there is no
strong need for predictive performance. We however emphasize that exact algorithms must be used
for making predictions, as we will show in Sec. 5.4. Finally, the algorithm NS without preprocessing
works much worse than others, indicating the significance of our preprocessing strategy.
5.4 Further Analysis on Time complexity, Testing accuracy and Variance
Table 4 reports the average number of epochs, time, and total number of floating point operations to
reach a given 96% validation accuracy on the largest Reddit dataset. Sparse and dense computations
are defined in Sec. 4.6. We found that CVD+PP is about 5 times faster than Exact due to the
significantly reduced receptive field size. Meanwhile, simply setting D(l) = 2 for NS does not
converge to the given accuracy.
We compare the quality of the predictions made by different algorithms, using the same model
trained by Exact in Fig. 4. As Thm. 1 states, CV reaches the same testing accuracy as Exact, while
NS and NS+PP perform much worse. Testing using exact algorithms (CV or Exact) corresponds to
the weight scaling algorithm for dropout (Srivastava et al., 2014).
Finally, we compare the average bias and variance of the gradients per dimension for first layer
weights relative to the weights’ magnitude in Fig. 5. For models without dropout, the gradient of
CV+PP is almost unbiased. For models with dropout, the bias and variance of CV+PP and CVD+PP
are ususally smaller than NS and NS+PP, as we analyzed in Sec. 4.3.
6 Conclusions
The large receptive field size of GCN hinders its fast stochastic training. In this paper, we present
a preprocessing strategy and two control variate based algorithms to reduce the receptive field size.
Our algorithms can achieve comparable convergence speed with the exact algorithm even the neigh-
bor sampling size D(l) = 2, so that the per-epoch cost of training GCN is comparable with training
MLPs. We also present strong theoretical guarantees, including exact prediction and convergence to
GCN’s local optimum, for our control variate based algorithm.
8
Under review as a conference paper at ICLR 2018
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix completion.
arXiv preprint arXiv:1706.02263, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864. ACM, 2016.
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. arXiv preprint arXiv:1706.02216, 2017a.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584, 2017b.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In ICLR, 2017.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361(10):1995, 1995.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Brian D Ripley. Stochastic simulation, volume 316. John Wiley & Sons, 2009.
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convolutional networks. arXiv preprint
arXiv:1703.06103, 2017.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning
research, 15(1):1929-1958, 2014.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In Proceedings of the 24th International Conference on
World Wide Web, pp. 1067-1077. International World Wide Web Conferences Steering Commit-
tee, 2015.
Sida Wang and Christopher Manning. Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13), pp. 118-126, 2013.
9
Under review as a conference paper at ICLR 2018
A Proof of Theorem 1
Proof. 1. We prove by induction. After the first epoch the activation hi(,0v) is at least computed once
for each node v, so HHCVi = HCVi = H(O) for all i > I. Assume that We have HHCVi = HCVi =
H (l) for all i > (l + 1)I. Then for all i > (l + 1)I
Z(l+1) =	(Pa)(H(I)	- H(I) )	+ PH(I) \	W⑷=PH⑷ W⑷=PH(I)W(l)	= Z(l+1)
ZCV,i =	Pi (HCV,i	- HCV,i)	+ P HCV,i	W = P HCV,iW = P H W	= Z .
(7)
H(l+1) = σ(Z(l+1)) = H(l+1)
HCV,i = σ(ZCV,i ) = H
After one more epoch, all the activations h"?V are computed at least once for each v, so HCl+i) =
HC(lV+,1i) = H(l+1) for all i > (l + 2)I. By induction, We knoW that after LI steps, We have
HCLViI) = HCLViI) = H(L-1). By Eq. 7 we have ZCLVi = Z(L).
2. We omit the time subscript i and denote fCV,v := f(yv, zC(LV),v). By back propagation, the
approximated gradients by CV can be computed as follows
VHC(lV) fCV,V	=P(I)VZ(1 + 1) fCV,v W(I)T ZCV	l=	1, . . . ,L -	1	
VZ(l) fCV,V	σ0(ZC V) ◦ VH (l) fCV,V CV	l=	1,...,L-	1	
VW (l) fCV,V	= (P(I)HCV )>Vz(i+i) fCV,v	l=	0, . . . , L -	1,	
gCV(W)	=击 X vW fCV，v， B V∈VB				(8)
where ◦ is the element wise product and σ0(ZC(lV) ) is the element-wise derivative. Similarly, denote					
fV := f(yV, ZV(l)), the exact gradients can be computed as follows					
VH(l)fV =	P>VZ(l+1)fVW(l)>	l	= 1,	...,L-1		
VZ(l)fV =	σ0(Z(l)) ◦VH(l)fV	l	= 1,	...,L-1		
VW (l) fV =	(PH(l))>VZ(l+1)fV	l=	0, .	..,L- 1,		
g(W) =	=V X VWfV. V∈V				(9)
Applying EP = Ep(i)	P(L) to both sides of Eq. 8, and utilizing
1’s conclusion that after L epoches, ZC(lV) = Z(l)
so Vz(L) fCV,v
is also determinstic.
EP [vz(i) fCV,v] = Ep(i),...,p(L) [vz(i) fCV,v].
EP [V H(l) fCV,v ] = Ep(i),...,p(L) [v H(I) fCV,v].
we have
EP(1),…,P(L) VHgbfCV,v = EP(1) P(I)>EP(1 + 1),…,P(L) [Vzg + i) fCV,v]W(I)>
P),…,P(L)VZCV fCV,V = σ0(ZCV) ◦ EP(I),…,P(L)VHCV fCV,V
EP VW (I) fCV,v = H(I)> EP(1) P(I)>EP(1+1),…,P(L)Vz稣i) f°V,v
gCV (W) = ∣V^-∣ X EP vW fCV,v .
l= 1,...,L- 1
l=1,...,L-1
l = 0, . . . , L - 1,
(10)
Comparing Eq. 10 and Eq. 9 we get
eP vw(I) fCV,v = vw(I) fV, l = 0,...,l - 1,
10
Under review as a conference paper at ICLR 2018
so
eP,Vb gcv(W) = EVB ∣v~। X EPVwfcv,v = V X Vw f.
|VB| v∈VB	V v∈V
□
B Proof of Theorem 2
We proof Theorem 2 in 3 steps:
1.	Lemma 1: For a sequence of weights W(1), . . . , W(N) which are close to each other, CV’s
approximate activations are close to the exact activations.
2.	Lemma 2: For a sequence of weights W(1), . . . , W(N) which are close to each other, CV’s
gradients are close to be unbiased.
3.	Theorem 2: An SGD algorithm generates the weights that changes slow enough for the
gradient bias goes to zero, so the algorithm converges.
The following proposition is needed in our proof
Proposition 1. Let kAk∞ = maxij |Aij |, then
•	kABk∞ ≤ col(A) kAk∞ kBk∞, where col(A) is the number of columns of the matrix A.
•	kA ◦ Bk∞ ≤ kAk∞ kBk∞.
•	kA+Bk∞≤kAk∞+kBk∞.
Proof.
kABk∞=maxXAikBik ≤ max X kAk∞ kBk∞ = col(A) kAk∞ kBk∞.
∞ ij k	ij k ∞	∞	∞	∞
kA ◦ Bk∞ = max IAijBijI ≤ maχ IIAk∞ IIBk∞ = IIAk∞ IIBk∞ .
ij	ij
IA+ BI∞ = maχ IAij +BijI ≤ maχ {IAij I + IBijI} ≤ maχ IAij I +maχIBijI = IAI∞ + IBI∞ .
ij	ij	ij	ij
□
We define C := maχ{col(P), col(H (0)), . . . , col(H (L) )}.
B.1	Proof of Lemma 1
Proposition 2. There are a series of T inputs X1, . . . , XT, XCV,1, . . . , XCV,T and weights
W1, . . . , WT feed to an one-layer GCN with CV
Zcv,i = (Pi(Xi- Xi) + PX) Wi,	Hcv,i = σ(Zcv,i),	Hcv,i+ι = SiHCV,i+(1 -Si)Hcv,i∙
and an one-layer exact GCN
Zi = PXiWi, Hi = σ(Zi).
If
1.	The activation σ(∙) is P-Lipschitz;
2.	IXCV,i -XCV,jI∞ < and IXCV,i -XiI∞ < foralli,j ≤ T and > 0.
Then there exists some K > 0, s.t., IHCV,i - HCV,jI∞ < K and IHCV,i - HiI∞ < K for all
I < i, j ≤ T, where I is the number of iterations per epoch.
11
Under review as a conference paper at ICLR 2018
Proof. Because for all i > I, the elements of Xcv,i are all taken from previous epochs, i.e.,
XCV,1, . . . , XCV,i-1, we know that
I∣χcv,i - Xcv,i∣∣∞ ≤ max IlXCVj- Xcv,ik∞ ≤ C (∀i > I).	(II)
j≤i
By triangular inequality, we also know
∣∣Xcv,i- Xcv,j∣∣∞ < 3e (∀i,j>I).	(12)
∣∣Xcv,i- Xi∣∣∞ < 2c M>I).	(13)
Since ∣Xcv,ιk∞ ,..., ∣Xcv,tk∞ are bounded, ∣∣Xcv∕∣∞ is also bounded for i > I. Then,
IHCV,i - HCV,jI∞ ≤ ρ IZCV,i - ZCV,jI∞
≤ P ∣∣(Pi(XCV,i - χcv,i) + PXCV,i) Wi-(鸟(XCVj- XCVj) + PXCVj) Wj ∣∣
≤ P ∣∣2(XCV,i - XCV,i)Wi - Pj(XCVj- XCVj)Wj-∣∣oo + P ∣∣PXCV,iWi- PXCVjWj∣∣∞
≤ PC2[∣∣Pi- PP'∣∣∞ ∣∣XCV,i - XCV,i∣∣∞ IIWik∞
+ ∣∣PP'∣∣∞ ∣∣XCV,i - XCVL XCVj + XCVj ∣∣∞ ∣∣Wik∞
+1∣Pj ∣∣∞ ∣∣XCVj- XCVj ∣∣∞ kWi- Wj k∞
+ kP k∞∣∣XCV,i - XCVjL kWik∞ + kP k∞ ∣∣XCV,j∣∣∞ kWi - Wj k∞]
≤ pc 2e[∣∣Pi - Pj∣∣∞ ∣Wik∞+2 ∣∣Pj∣∣∞ ∣Wik∞ +1∣PjILkWi- Wjk∞+
31∣PjILkWik∞+∣∣PjILHXCVj ∣∣∞ι
= K1 C,
and
kHCV,i - Hik∞ ≤ P kZCV,i - Zik∞
≤ P ∣∣(Pi(XCV,i - XCV,i) + P(XCV,i - Xi)) ∣∣ Wi
≤	PC(HPiL c+2 ∣p k∞ e)kWik∞
≤	K2C.
□
The following lemma bounds CV’s approximation error of activations
Lemma 1. Given a sequence of model weights W1, . . . , WT. If kWi - Wj k∞ < C, ∀i, j, and all
the activations are P-Lipschitz, there exists K > 0, s.t.,
•	∣∣Hil - HCl V,i∣∣∞ <KC,∀i>LI,l=1,...,L-1,
•	∣∣Zil-ZClV,i∣∣∞ <KC,∀i>LI,l=1,...,L.
Proof. We prove by induction. Because H0 = X is constant, HCV,i = H0 after I iterations. So
HCV,i = σ( (Pi(HCV,i - HCV,i) + PHCV,i) Wi0 = σ(PXWi0) = Hi1, and
∣∣HC1V,i-HC1V,j∣∣∞= ∣∣σ(PXWi0)-σ(PXWj0)∣∣∞ ≤ PC kPk∞ kXk∞ C.
Repeatedly apply Proposition B.1 for L - 1 times, We get the intended results.	□
12
Under review as a conference paper at ICLR 2018
B.2 Proof of Lemma 2
The following lemma bounds the bias of CV’s approximate gradient
Lemma 2. Given a sequence of model weights W1, . . . , WT, if
1.	kWi-Wjk∞ <,∀i,j,
2.	all the activations are ρ-Lipschitz,
3.	the gradient of the costfunction Nz f (y, Z) is P -Lipschitz and bounded,
then there exists K > 0, s.t.,
∣∣Ep,VBgcv(Wi)- g(Wi)∣∣oo < Ke,∀i > LL
Proof. By Lipschitz continuity of Nzf(y, z) and Lemma 1, there exists K > 0, s.t.,
∣∣∣NZC(lV) fCV,v -NZ(l)fv∣∣∣∞ ≤ ρ ∣∣∣ZC(l)V -Z(l)∣∣∣∞ ≤ ρKe.
Assume that ∣∣EpNz(i+i)fcv,v - Vz(i+i)fv∣∣	< Kιe, we now prove that there exists K > 0,s.t.,
∣∣EpVz(i)fcv,v 一 Vz(i)fv∣∣	< Ke. By Eq. 9, Eq. 10 and Lemma 1, we have
∣∣ePvH(ffvfCV,v 一 vh(i)fv∣∣
= ∣∣P>Ep[Vzg+i)fcv,v]W(I)T - P>Vz(i+1)fvW(I)T
≤ ∣∣P >∣∣∞ K1C2e ∣∣∣W (l)>∣∣∣∞,
∞
and
∣∣EPVZgbfCV,v 一 vZ(I)fv∣∣
=B [σ0(ZCV) N吟V fcv,vi 一 σ0(Z(I)) oVH(i)fv∣L
≤ ∣∣eP h(σ0(zClV ) 一 σ (Z (I))) ◦ V Hffv fCV,v i ∣∣oo + ∣∣σ0(Z (I))(EP [VH” fCV,v] 一 vH(l)fv )∣∣θo
≤ ∣∣ep hρKCe 0%v fcv,v]i∣L+∣∣σ0(z (I))LIIP >∣∣∞ KIC 3e∣∣”>∣L
WPKC 2e ∣∣EP VHCV fcv,v∣∣∞+W(Z (I))ILIIP >∣∣∞ KIC 3e ∣∣W(I)>∣∣∞ ≤ K
By induction we know that for l = 1, . . . , L there exists K, s.t.,
∣∣EpVZ2fcv,v - Vz(i)fv∣L ≤ Ke.
Again by Eq. 9, Eq. 10, and Lemma 1,
IIEP VW ⑴ fCV,v 一 vW ⑴ fv∣∣∞
= ∣∣Ep [hc>P>Vz2fcv,vi - HIlτPtVz(I)fv∣∣oo
≤∣∣Ep [(hc> - H(l)>) PτVZ(i) fcv,vi ∣∣	+∣∣H(l)>Pτ (EPVZ(i)fcv,v 一 Vz(i)fv)∣∣
ZCV	∞	ZCV	∞
≤ IEP hKCeP >vzcv fcv,v i∣∣∞+C2IIH(I)τ∣∣∞ IIP τ∣∣∞Ke
≤kc2e kPk∞∣EP hvzCVfcv,vi ∣∣∞ + IIH(I)>II∞ IIP>II∞ KC2e ≤ K3e
13
Under review as a conference paper at ICLR 2018
Finally,
∣∣Ep,Vb gcv (Wi) — g(Wi) ∣∣oo
卜VB (IV1BI VXBEP [VW/v]- V XVW(l)fV
v∈V	∞
[VW (l) fCV,v] - VW (l) fv
v∈V
≤ K3.
∞
□
B.3 Proof of Theorem 2
Proof. This proof is a modification of Ghadimi & Lan (2013), but using biased stochastic gradients
instead. We assume the algorithm is already warmed-up for LI steps with the initial weights W0, so
that Lemma 2 holds for step i > 0. Denote δi = gCV(Wi) - VL(Wi). By smoothness we have
L(Wi+ι) ≤ L(Wi) + hVL(Wi),Wi+ι - Wii + PY kgcv(Wi)k2
=L(Wi)- YihVL(Wi), gcv(Wi)i + Pγ2 kgcv(Wi)k2
=L(Wi)- YihVL(Wi), δii- γi kVL(Wi)k2 + Pγ2 [∣∣δik2 + kVL(Wi)k2 + 2(瓯 VL(WiM
2
=L(Wi)-(Yi- Pγ2)hVL(Wi),δii- (% -等)kVL(Wi)k2 + PY |同|2.	(14)
Consider the sequence of LI + 1 weights Wi-LI , . . . , Wi .
i-1
i-LmI≤ajxk≤i kWj - Wkk∞ ≤ X kWj - Wj+1k∞
,	j=i-LI
i-1	i-1
= X Yjkgcv(Wj)k∞ ≤ X YjG≤LIGYi-LI.
j=i-LI	j=i-LI
By Lemma 2, there exists K > 0, s.t.
Ep,Vb kδik∞ = Ep,Vb kgcv(Wi) - VL(Wi)k∞ ≤ KLIGYi-LI,	∀i > 0.
Assume that W is D-dimensional,
Ep,VB hVL(Wi),δii ≥ -Ep,vBD kVL(Wi)k∞ |脱k∞ ≥ -KLIDG2γjI = KIYi-LI,
Ep,VB kδik2 ≤ D (Ep,vB kδik∞)2 ≤ DK2L2B2G2Yi-LI = K2Yi-LI,
where Ki = KLIDG2 and K = DK2L2B2G2. Taking EPVB t° both sides of Eq. 14 We have
2
L(Wi+1)≤ L(Wi) + (Yi- PY2) KIYi-LI-(Yi---2 )eP,Vb kVL(Wi)『+ 2 Y2K2Yi-LI.
Summing up the above inequalities and re-arranging the terms, we obtain,
N2
X(Yi -等)Ep,Vb kVL(Wi)k2
i=1	2
N	PK N
≤L(WI)- L(W*) + KI 工(Yi - PY2)Yi-LI +	Y2Yi-LI.
14
Under review as a conference paper at ICLR 2018
Dividing both sides by PN=I(Yi - PYL),
ER〜PREp,Vb IIVL(Wr)II2
≤2L(WI)- L(W*) + K PL(Yi - PY2)%-LI + 吟 PN=ι y2%-li
-	PN=I Yi(2-PYi)
Taking Yi = Y := min{P, √= }, for all i = 1,...,N, We have
ER〜PREp,vB IIVL(Wr)II2
L(Wi) - L(W*) + KiN(Y - py2)y + PKNy3
’2	NY(2 - PY)
L(Wι) - L(W*) + KιN(γ - ργ2)γ + 等Nγ3
≤	Nγ
≤2 L(WI) L *) +K1Y(1-PY) + PK2Y 2
NY
2ρ(L(Wι) - L(W*)) + K	2(L(Wi) - L(W*)) + K
≤	N	+	√N	.
Particularly, when N → ∞, we have ER〜PREPVB I∣VL(Wr)∣2 = 0, which implies that the
gradient is asymptotically unbiased.
□
C Derivation of the variance
2
Var[u] =E X : Pv (hv - μv )
v
=XPvE [(hv - μv)]2
v
P2vs2v
v
s2.
Var[uNs] =E [Dpv0 hvo - μ]2
=Evo{D2pvo (μ2o + svo) + μ2 - Dμpvoμvp}
=Ds2 + (D XPvμv - μ2)
v
=Dss1 + 2 X X(Pvμv - pvoμv0)2.
v,vo
15
Under review as a conference paper at ICLR 2018
2
VarlUCV] =E Dpv0 δAv0 +〉： Pv (hv - μv )
v
2
=E Dpv0∆hv0 + Epv(hv - μv) - ∆μ
v
(
=Ev0
DPJOE (∆hv0)2 + XpvE (hv - μv)2 + ∆μ2 + 2Dpv0 XPvE [∆hv,(hv - μv)]
1
—EjO {2Dpv0∆μE∆hjθ — 2∆μ ^XPvE(hv — μv)}
=EvO ∖ D2P2o (Δμ2o + ∆s2o) + Xp2 sJ + ∆μ2 - 2Dp2o S2o -
v
=D XP2vΔμV + D∆s2 + s2 + ∆μ2 - 2s2 - 2∆μ2
v
2DPjO∆μ∆μjθ}
=IDδS- s2) + (D EpVδ〃v - δ42)
v
=[Ds2 + (D - 1)s2] + 2 X(pvδMv -PvO ∆μvθ)2.
v,vO
2
Var[ucvD] =E VDpv，(hv，- μjO) + DpvoΔμjO + EpV(μv - μv)
v
=E [√DpjO (hjO — μjO) + DpvOΔμjO — ∆μ]
=EvO {dpJoE(hjO - μvO )2 + D2p2O∆μ2O + ∆μ2 - 2DpjO∆μjO∆μ}
=X pv SV + D X pv ∆μV +∆μ2 - 2∆μ2
=s2 + (D X pv^v- ^2)
v
=S + 2〉：(pv^ʌμv - pvO AMvo )2 .
v,vO
D Experiment setup
In this sections we describe the details of our model architectures. We use the Adam opti-
mizer Kingma & Ba (2014) with learning rate 0.01.
•	Citeseer, Cora, PubMed and NELL: We use the same architecture as Kipf & Welling
(2017): two graph convolution layers with one linear layer per graph convolution layer.
We use 32 hidden units, 50% dropout rate and 5 × 10-4 L2 weight decay for Citeseer, Cora
and PubMed and 64 hidden units, 10% dropout rate and 10-5 L2 weight decay for NELL.
•	PPI and Reddit: We use the mean pooling architecture proposed by Hamilton et al. (2017a).
We use two linear layers per graph convolution layer. We set weight decay to be zero,
dropout rate to be 0.2%, and adopt layer normalization (Ba et al., 2016) after each linear
layer. We use 512 hidden units for PPI and 128 hidden units for Reddit. We find that our
architecture can reach 97.8% testing micro-F1 on the PPI dataset, which is significantly
higher than 59.8% reported by Hamilton et al. (2017a). We find the improvement is from
wider hidden layer, dropout and layer normalization.
16
Under review as a conference paper at ICLR 2018
Algorithm 1 Training with the CV algorithm
for each minibatch VB ⊂ V do
Randomly sample propagation matrices 户⑼,...，P(LT)
Compute the receptive fields m(0), . . . , m(L-1)
(Forward propgation)
for each layer l J 0 to L - 1 do
Z(l+1) —(户(I) (H(I) - H(I) + PH(I)) W(I)
H(l+1) J σ(Z(l+1))
end for
Compute the loss L = ∣V1B∣ Pv∈vB f (yv,zVL))
(Backward propagation)
W J W - YiVwL
(Update historical activations)
for each layer l J 0 to L - 1 do
H(I) J m(I)H(I) + (1 - m(I))H(I)
end for
end for
Algorithm 2 Training with the CVD algorithm
for each minibatch VB ⊂ V do
Randomly sample propagation matrices P(O),..., P(LT)
Compute the receptive fields m(0), . . . , m(L-1)
(Forward propgation)
for each layer l J 0 to L - 1 do
U J (P(I)(H(I) - μ(I))+ P(I)(μ(I) - μ(I)) + PH(I)
H(l+1) J σ(Dropoutp(U)W (l))
μ(l+1) j σ(UW(I))
end for
Compute the loss L = ∣v1B∣ Pv∈vB f (yv,hVL))
(Backward propagation)
WJ W -γiVWL
(Update historical activations)
for each layer l J 0 to L - 1 do
μ(I) J m(l)μ(I) + (1 — m(l))μ(I)
end for
end for
E Pseudocode
E.1 Training with the CV estimator
Alg. 1 depicts the training algorithm using the CV estimator in Sec. 4.1. We perform forward
propagation according to Eq. (5,6), compute the stochastic gradient, and then update the historical
activations H(I) according to Eq.(6). We omit the subscripts CV and the iteration number i for con-
cise. Let W = (W(0), . . . , W(L-1)) be all the trainable parameters, the gradient VWL is computed
automatically by frameworks such as TensorFlow. The diagonal matrix m(l) denotes the receptive
field at layer l, i.e., the nodes that need to be computed in order to approximate zv(l) for v in the
minibatch VB. We only need to compute and update the activations H(I), Z(I), H(I) for nodes in
m(l).
17
Under review as a conference paper at ICLR 2018
Figure 6: Comparison of validation accuracy with respect to number of epochs for 3-layer GCNs.
Table 5: Time to reach 0.95 testing accuracy.
Alg.	Valid. acc.	Epochs	Time (S)	Sparse GFLOP	Dense TFLOP
Exact	0.940	-^30^^	199	306^^	^^117-
NS	0.940	24.0	148	33.6	9.79
NS+PP	0.940	12.0	68	2.53	4.89
CV+PP	0.940	5.0	32	8.06	2.04
CVD+PP	0.940	5.0	36	16.1	4.08
E.2 Training with the CVD estimator
Training with the CVD estimator is similar with the CV estimator, except it runs two versions of the
network, with and without dropout, to compute the samples H and their mean μ of the activation.
The matrix JPv,vo = ]Pv,vo / Vz∣n(v, 1)| , where |n(v, 1) | is the degree of node v.
F	Experiment for 3-layer GCNs
We test 3-layer GCNs on the Reddit dataset. The settings are the same with 2-layer GCNs in Sec. 5.3.
To ensure the exact algorithm can run in a reasonable amount of time, we subsample the graph so
that the maximum degree is 10. The convergence result is shown as Fig. 6, which is similar with the
two-layer models. The time consumption to reach 0.94 testing accuracy is shown in Table 5.
G Justification of the independent Gaussian assumption
G. 1 Results for 2-layer GCNs
We justify the independent Gaussian assumption in Sec. 4.3 by showing that for a 2-layer GCN with
the first layer pre-processed, the neighbor’s activations are independent. Without loss of generality,
suppose that we want to compute z1(2), and the neighbors of node 1 are 1, . . . , D. By Eq. (1),
hV1) = σ ((φv o uV0) )W(0)) is a random variable with respect to φv, where φv 〜Bernoulli(p) is
the dropout mask and u(v0) = (P H(0))v. The indepedent Gaussian assumption states that
1.	h(v1) is a Gaussian random variable with diagonal covariance;
2.	h(v1) and h(v10) are independent, for v 6= v0.
Assumption 1 is not GCN-specific and is discussed in Wang & Manning (2013), we now prove
assumption 2 by the following lemma.
Lemma 3. If a and b are independent random variables, then their transformations f1(a) and f2(b)
are independent.
Because for any event A and B, P (f1(a) ∈ f1(A), f2(b) ∈ f2(B)) = P (a ∈ A, b ∈ B) =
P (a∈A)P (b∈ B) =P (f1(a) ∈ f1(A))P (f2(B) ∈ f2(B)),wheref1(A) = {f1(a)|a ∈ A} and
f2(B) = {f2(b)|b ∈ B}.
18
Under review as a conference paper at ICLR 2018
Figure 7: Average feature and neighbor correlations in a 10-layer GCN.
Let h(v1) = f1(φv) := σ (φv ◦ u(v0))W(0) and h(v10) = f1(φv0) := σ (φv0 ◦ u(v00))W (0) , because
φv and φv0 are independent Bernoulli random variables, h(v1) and h(v10) are independent.
The result can be further generalized to deeper models. If the receptive fields of two nodes does not
overlap, they should be independent.
G.2 Empirical results for deeper GCNs
Because we only sample two neighbors per node, the sampled subgraph is very close to a graph with
all its nodes isolated, which reduces to the MLP case that Wang & Manning (2013) discuss.
We empirically study the correlation between feature dimensions and neighbors. The definition of
the correlation between feature dimensions is the same with Wang & Manning (2013). For each
node v on layer l, we compute the correlation between each feature dimension of h(vl)
C[h(vli),h(vlj)]
Coj
《co(FrCVj，
where i and j are the indices for different hidden dimensions, and C[X, Y] = E[(X - EX)(Y -
EY )] is the coVariance between two random Variables X and Y . We approximate CoVi(jl,v) with
1,000 samples of the actiVations h(vli) and h(vlj), by running the forward propagation 1,000 times with
different dropout masks. We define the average feature correlation on layer l tobe CoVi(jl,v) aVeraged
by the nodes v and dimension pairs i 6= j .
To compute the correlation between neighbors, we treat each feature dimension separately. For
each layer l + 1, node v, and dimension d, we compute the correlation matrix of all the actiVations
{h(d∖i ∈ n(I) (v)} that are needed by hVl+1), where n(I)(V) = {i∣P(? = 0} is the set of subsampled
neighbors for node v :
C[hi(dl),h(jld)]
CoVi(l,v,d)
____________ij____________
Jcov(i,v,d) Jcovjw,d)
CoVi(jl,v):
Corri(jl,v) :
ij
CoVi(jl,v,d) :
Corri(l,v,d) :
ij
19
Under review as a conference paper at ICLR 2018
where the indices i, j ∈ n (l)(v). Then, We compute the average correlation of all pairs of neighbors
i 6= j .
AvgCorr(I,v⑷：=―(l∖,、| “ 1〃“，∣—— XCorr(j,v,d),
In(I)(V)I (In(I)(V)I- I) W i
and define the average neighbor correlation on layer l as AvgCorr(l,v,d) averaged over all the nodes
V and dimensions d.
We report the average feature correlation and the average neighbor correlation per layer, on the
Citeseer, Cora, PubMed and PPI datasets. These quantities are too expensive to compute for NELL
and Reddit. On each dataset, we train a GCN with 10 graph convoluation layers until early stopping
criteria is met, and compute the average feature correlation and the average neighbor correlation for
layer 1 to 9. We are not interested in the correlation on layer 10 because there are no more graph
convolutional layers after it. The result is shown as Fig. 7. As analyzed in Sec. G.1, the average
neighbor correlation is close to zero on the first layer, but it is not exactly zero due to the finite sample
size for computing the empirical covariance. There is no strong tendency of increased correlation
as the number of layers increases, after the third layer. The average neighbor correlation and the
average feature correlation remain on the same order of magnitude, so bringing correlated neighbors
does not make the activations much more correlated than the MLP case (Wang & Manning, 2013).
Finally, both correlations are much smaller than one.
20