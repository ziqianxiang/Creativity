Under review as a conference paper at ICLR 2018
Convergence rate of sign stochastic gradient
descent for non-convex functions
Anonymous authors
Paper under double-blind review
Abstract
The sign stochastic gradient descent method (signSGD) utilises only the sign of
the stochastic gradient in its updates. For deep networks, this one-bit quantisation
has surprisingly little impact on convergence speed or generalisation performance
compared to SGD. Since signSGD is effectively compressing the gradients, it is
very relevant for distributed optimisation where gradients need to be aggregated
from different processors. What’s more, signSGD has close connections to com-
mon deep learning algorithms like RMSprop and Adam. We study the base the-
oretical properties of this simple yet powerful algorithm. For the first time, we
establish convergence rates for signSGD on general non-convex functions under
transparent conditions. We show that the rate of signSGD to reach first-order crit-
ical points matches that of SGD in terms of number of stochastic gradient calls,
but loses out by roughly a linear factor in the dimension for general non-convex
functions. We carry out simple experiments to explore the behaviour of sign gra-
dient descent (without the stochasticity) close to saddle points and show that it
can help to completely avoid certain kinds of saddle points without using either
stochasticity or curvature information.
1	Introduction
Deep neural network training takes place in an error landscape that is high-dimensional, non-convex
and stochastic. In practice, simple optimization techniques perform surprisingly well but have very
limited theoretical understanding. While stochastic gradient descent (SGD) is widely used, algo-
rithms like Adam (Kingma & Ba, 2015), RMSprop (Tieleman & Hinton, 2012) and Rprop (Ried-
miller & Braun, 1993) are also popular. These latter algorithms involve component-wise rescaling
of gradients, and so bear closer relation to signSGD than SGD. Currently, convergence rates have
only been derived for close variants of SGD for general non-convex functions, and indeed the Adam
paper gives convex theory.
Recently, another class of optimization algorithms has emerged which also pays attention to the re-
source requirements for training, in addition to obtaining good performance. Primarily, they focus
on reducing costs for communicating gradients across different machines in a distributed training
environment (Seide et al., 2014; Strom, 2015; Li et al., 2016; Alistarh et al., 2017; Wen et al., 2017).
Often, the techniques involve quantizing the stochastic gradients at radically low numerical preci-
sion. Empirically, it was demonstrated that one can get away with using only one-bit per dimension
without losing much accuracy (Seide et al., 2014; Strom, 2015). The theoretical properties of these
approaches are however not well-understood. In particular, it was not known until now how quickly
signSGD (the simplest incarnation of one-bit SGD) converges or even whether it converges at all to
the neighborhood of a meaningful solution.
Our contribution: we supply the non-convex rate of convergence to first order critical points for
signSGD. The algorithm updates parameter vector xk according to
xk+ι = Xk - δksign(gk)
(1)
where gk is the mini-batch stochastic gradient and δk is the learning rate. We show that for non-
convex problems, signSGD entertains convergence rates as good as SGD, up to a linear factor in the
dimension. Our statements impose a particular learning rate and mini-batch schedule.
1
Under review as a conference paper at ICLR 2018
Ours is the first work to provide non-convex convergence rates for a biased quantisation procedure
as far as we know, and therefore does not require the randomisation that other gradient quantisation
algorithms need to ensure unbiasedness. The technical challenge we overcome is in showing how to
carry the stochasticity in the gradient through the sign non-linearity of the algorithm in a controlled-
fashion.
Whilst our analysis is for first order critical points, we experimentally test the performance of sign
gradient descent without stochasticity (signGD) around saddle points. We removed stochasticity in
order to investigate whether signGD has an inherent ability to escape saddle points, which would
suggest superiority over gradient descent (GD) which can take exponential time to escape saddle
points if it gets too close to them (Du et al., 2017).
In our work we make three assumptions. Informally, we assume that the objective function is lower-
bounded, smooth, and that each component of the stochastic gradient has bounded variance. These
assumptions are very general and hold for a much wider class of functions than just the ones en-
countered in deep learning.
Outline of paper: in Sections 3, 4 and 5 we give non-convex theory of signSGD. In Section 6 we
experimentally test the ability of the signGD (without the S) to escape saddle points. And in Section
7 we pit signSGD against SGD and Adam on CIFAR-10.
2	Related work
Deep learning: the prototypical optimisation algorithm for neural networks is stochastic gradient
descent (SGD)—see Algorithm 2. The deep learning community has discovered many practical
tweaks to ease the training of large neural network models. In Rprop (Riedmiller & Braun, 1993)
each weight update ignores the magnitude of the gradient and pays attention only to the sign, bring-
ing it close to signSGD. It differs in that the learning rate for each component is modified depending
on the consistency of the sign of consecutive steps. RMSprop (Tieleman & Hinton, 2012) is Rprop
adapted for the minibatch setting—instead of dividing each component of the gradient by its mag-
nitude, the authors estimate the rescaling factor as an average over recent iterates. Adam (Kingma
& Ba, 2015) is RMSprop with momentum, meaning both gradient and gradient rescaling factors
are estimated as bias-corrected averages over iterates. Indeed switching off the averaging in Adam
yields signSGD. These algorithms have been applied to a breadth of interesting practical problems,
e.g. (Xu et al., 2015; Gregor et al., 2015).
In an effort to characterise the typical deep learning error landscape, Dauphin et al. (2014) frame
the primary obstacle to neural network training as the proliferation of saddle points in high dimen-
sional objectives. Practitioners challenge this view, suggesting that saddle points may be seldom
encountered at least in retrospectively successful applications of deep learning (Goodfellow et al.,
2015).
Optimisation theory: in convex optimisation there is a natural notion of success—rate of conver-
gence to the global minimum x*. Convex optimisation is eased by the fact that local information in
the gradient provides global information about the direction towards the minimum, i.e. Vf (x) tells
you information about x* - x.
In non-convex problems finding the global minimum is in general intractable, so theorists usually
settle for measuring some restricted notion of success, such as rate of convergence to stationary
points (e.g. Allen-Zhu (2017a)) or local minima (e.g. Nesterov & Polyak (2006)). Given the impor-
tance placed by Dauphin et al. (2014) upon evading saddle points, recent work considers the efficient
use of noise (Jin et al., 2017; Levy, 2016) and curvature information (Allen-Zhu, 2017b) to escape
saddle points and find local minima.
Distributed machine learning: whilst Rprop and Adam were proposed by asking how we can use
gradient information to make better optimisation steps, another school asks how much information
can we throw away from the gradient and still converge at all. Seide et al. (2014); Strom (2015)
demonstrated empirically that one-bit quantisation can still give good performance whilst dramat-
ically reducing gradient communication costs in distributed systems. Convergence properties of
quantized stochastic gradient methods remain largely unknown. Alistarh et al. (2017) provide con-
vergence rates for quantisation schemes that are unbiased estimators of the true gradient, and are
2
Under review as a conference paper at ICLR 2018
thus able to rely upon vanilla SGD convergence results. Wen et al. (2017) prove asymptotic conver-
gence ofa {-1, 0, 1} ternary quantization scheme that also retains the unbiasedness of the stochastic
gradient. Our proposed approach is different, in that we directly employ the sign gradient which is
biased. This avoids the randomization needed for constructing an unbiased quantized estimate. To
the best of our knowledge, the current work is the first to establish a convergence rate for a biased
quantisation scheme, and our proof differs to that of vanilla SGD.
Parallel work: signSGD is related to both attempts to improve gradient descent like Rprop and
Adam, and attempts to damage it but not too badly like quantised SGD. After submitting we became
aware that Anonymous (2018) also made this link in a work submitted to the same conference. Our
work gives non-convex theory of signSGD, whereas their work analyses Adam in greater depth, but
only in the convex world.
3	Assumptions
Assumption 1 (The objective function is bounded below). For all X and some constant f *, the
objective function satisfies
f(x) ≥ f* (2)
Remark: this assumption applies to every practical objective function that we are aware of.
Assumption 2 (The objective function is L-Lipschitz smooth). Let g(x) denote the gradient of the
objective f(.) evaluated at point x. Then for every y we assume that
If ⑻-f (X) + g(X)T(y - x)]∣ ≤ L2ky - χk2	⑶
Remark: this assumption allows us to measure the error in trusting the local linearisation of our
objective, which will be useful for bounding the error in a single step of the algorithm. For signSGD
we can actually relax this assumption to only hold only for y within a local neighbourhood of X,
since signSGD takes steps of bounded size.
Assumption 3 (Stochastic gradient oracle). Upon receiving query X, the stochastic gradient oracle
gives us an independent estimate g satisfying
E[g(x)] = g(x),	Var(g(x)[i]) ≤ σ2 ∀i = 1,..., d.
Remark: this assumption is standard for stochastic optimization, except that the variance upper
bound is now stated for every dimension separately. A realization of the above oracle is to choose
a data point uniformly at random, and to evaluate its gradient at point X. In the algorithm, we
will be working with a minibatch of size nk in the kth iteration, and the corresponding minibatch
stochastic gradient is modeled as the average of nk calls of the above stochastic gradient oracle at
Xk. Therefore in this case the variance bound is squashed to σ2/n.
4	Non-convex convergence rate of signsgd
Informally, our primary result says that if we run signSGD with the prescribed learning rate and
mini-batch schedules, then after N stochastic gradient evaluations, we should expect that somewhere
along the optimisation trajectory will be a place with gradient 1-norm smaller than O(N-0.25). This
matches the non-convex SGD rate, insofar as they can be compared, and ignoring all (dimension-
dependent!) constants.
Before we dive into the theorems, here’s a refresher on our notation—deep breath—gk is the gradient
at step k, f * is the lower bound on the objective function, f0 is the initial value of the objective
function, d is the dimension of the space, K is the total number of iterations, NK is the cumulative
number of stochastic gradient calls at step K, σ is the intrinsic variance-proxy for each component
of the stochastic gradient, and finally L is the maximum curvature (see Assumption 2).
3
Under review as a conference paper at ICLR 2018
Algorithm 1 Sign stochastic gradient descent (signSGD)
1:	Inputs: x0, K	. initial point and time budget
2:	for k ∈ [0,K- 1] do
3:	δk J learningRate(k)
4:	nk J miniBatchSize(k)
5:	gk J nk PnkI stochasticGradient(xk)
6:	xk+i J Xk 一 δksign(gk)	. the sign operation is element-wise
Algorithm 2 Stochastic gradient descent
1:	Inputs: x0, K . initial point and time budget
2:	for k ∈ [0,K 一 1] do
3:	δk J learningRate(k)
4:	nk J miniBatchSize(k)
5:	gk J 5 Pn=I StochasticGradient(Xk)
6:	xk+1 J Xk — δkgk
Theorem 1	(Non-Convex convergence rate of SignSGD). Apply Algorithm 1 under Assump-
tions 1, 2 and 3. Schedule the learning rate and mini-batch size as
δ
√k + 1
nk = k + 1
(4)
Let NK be the cumulative number ofstochastic gradient calls up to step K, i.e. NK = O(K2)
Then we have
E
min kgkki
0≤k≤K-1	1
/0 K '* + d(2 + log(2NK-1))(σ +
δ
2
(5)
Theorem 2	(Non-convex convergence rate of stochastic gradient descent). Apply Algorithm 2
under Assumptions 1, 2 and 3. Schedule the learning rate and mini-batch size as
^ δ
δk = -/?	ι nk = 1 (6)
√k + 1
Let NK be the cumulative number of stochastic gradient calls up to step K, i.e. NK = K.
Then we have that
E min kgk ∣∣2
0≤k≤K -Jgk"2
δL
+ d(I+log NK) -~~2Jl σ2
1 G
⑺
The proofs are deferred to Appendix B and here we sketch the intuition for Theorem 1. First consider
the non-stochastic case: we know that ifwe take lots of steps for which the gradient is large, we will
make lots of progress downhill. But since the objective function has a lower bound, it is impossible
to keep taking large gradient steps downhill indefinitely, therefore increasing the number of steps
requires that we must run into somewhere with small gradient.
To get a handle on this analytically, we must bound the per-step improvement in terms of the norm of
the gradient. Assumption 2 allows us to do exactly this. Then we know that the sum of the per-step
improvements over all steps must be smaller than the total possible improvement, and that gives us
a bound on how large the minimum gradient that we see can be.
In the non-stochastic case, the obstacle to this process is curvature. Curvature means that ifwe take
too large a step the gradient becomes unreliable, and we might move uphill instead of downhill.
Since the step size in signSGD is set purely by the learning rate, this means we must anneal the
learning rate if we wish to be sure to control the curvature-induced error and make good progress
downhill. Stochasticity also poses a problem in signSGD. In regions where the gradient signal is
4
Under review as a conference paper at ICLR 2018
smaller than the noise, the noise is enough to flip the sign of the gradient. This is more severe than
the additive noise in SGD, and so the batch size must be grown to control this effect.
You might expect that growing the batch size should lead to a worse convergence rate than SGD.
This is forgetting that signSGD has an advantage in that it takes large steps even when the gradient
is small. It turns out that this positive effect cancels out the fact that the batch size needs to grow,
and the convergence rate ends up being the same as SGD.
For completeness, we also present the convergence rate for SGD derived under our assumptions.
The proof is given in Appendix C. Note that this appears to be a classic result, although we are not
sure of the earliest reference. Authors often hide the dimension dependence of the variance bound.
SGD does not require an increasing batch size since the effect of the noise is second order in the
learning rate, and therefore gets squashed as the learning rate decays. The rate ends up being the
same in NK as signSGD because SGD makes slower progress when the gradient is small.
5 Comparing the convergence rate to sgd
To make a clean comparison, let Us set δ = L (as is often recommended) and hide all numerical
constants in Theorems 1 and 2. Then for signSGD, we get
Ehminkgk∣∣1i 〜√NhL(fo - f*)+d(σ+I)IogNi ； ⑻
and for SGD we get
Ehminkgkk2i 〜√n hL(fo - f*) + dσ2logNi	⑼
where 〜denotes general scaling. What do these bounds mean? They say that after We have made
a cumulative number of stochastic gradient evaluations N, that we should expect somewhere along
our trajectory to have hit a point with gradient norm smaller than N- 4.
One important remark should be made. SignSGD more naturally deals with the one norm of the
gradient vector, hence we had to square the bound to enable direct comparison with SGD. This
means that the constant factor in signSGD is roughly worse by a square. Paying attention only to
dimension, this looks like
signSGD: Ehminkgkk1i
d2
SGD: Ehminkgk∣2]〜√n
(10)
This defect in dimensionality should be expected in the bound, since signSGD almost never takes
the direction of steepest descent, and the direction only gets worse as dimensionality grows. This
raises the question, why do algorithms like Adam, which closely resemble signSGD, work well in
practice?
Whilst answering this question fully is beyond the scope of this paper, we want to point out one
important detail. Whilst the signSGD bound is worse by a factor d, it is also making a statement
about the 1-norm of the gradient. Since the 1-norm of the gradient is always larger than the 2-
norm, the signSGD bound is stronger in this respect. Indeed, if the gradient is distributed roughly
uniformly across all dimensions, then the squared 1-norm is roughly d times bigger than the squared
2-norm, i.e.
kgkk1 〜d∣gkk2
and in this limit both SGD and signSGD have a bound that scales as √N.
6 Swinging by saddle points? An experiment
Seeing as our theoretical analysis only deals with convergence to stationary points, it does not ad-
dress how signSGD might behave around saddle points. We wanted to investigate the naive intuition
that gradient rescaling should help flee saddle points—or in the words of Zeyuan Allen-Zhu—swing
by them.
5
Under review as a conference paper at ICLR 2018
Figure 1: Descending the tube function of (Du et al., 2017). To reach the minimum, the algorithm
must navigate a series of saddle points. Optimisers tested were gradient descent (GD), perturbed
gradient descent (PGD) (Jin et al., 2017), sign gradient descent (signGD) and the rescaled gradient
method (noiseless version of (Levy, 2016)). No learning rate tuning was attempted, so we suggest
only focusing on the qualitative behaviour. Left: signGD appears not to ‘see’ the saddle points
in the original objective function. Middle: after breaking the objective function’s axis alignment by
rotating it, the sign method’s performance is still quantitatively different. Also the numerical error in
our rotation operation appears to help unstick GD from the saddle points, illustrating the brittleness
of Du et al. (2017)’s construction. Right: for some rotations, the sign method (with fixed learning
rate and zero stochasticity) can get stuck in perfectly periodic orbits around saddle points.
For a testbed, the authors of (Du et al., 2017) kindly provided their 10-dimensional ‘tube’ function.
The tube is a specially arranged gauntlet of saddle points, each with only one escape direction, that
must be navigated in sequence before reaching the global minimum of the objective. The tube was
designed to demonstrate how stochasticity can help escape saddles. Gradient descent takes much
longer to navigate the tube than perturbed gradient descent of (Jin et al., 2017). It is interesting to
ask, even empirically, whether the sign non-linearity in signSGD can also help escape saddle points
efficiently. For this reason we strip out the stochasticity and pit the sign gradient descent method
(signGD) against the tube function.
There are good reasons to expect that signGD might help escape saddles—for one, it takes large
steps even when the gradient is small, which could drive the method away from regions of small
gradient. For another, it is able to move in directions orthogonal to the gradient, which might help
discover escape directions of the saddle. We phrase this as signGD’s greater ability to explore.
Our experiments revealed that these intuitions sometimes hold out, but there are cases where they
break down. In Figure 1, we compare the sign gradient method against gradient descent, perturbed
gradient descent (Jin et al., 2017) and rescaled gradient descent (Xk+1 = Xk -品 W which is a
noiseless version of the algorithm considered in (Levy, 2016). No learning rate tuning was con-
ducted, so we suggest paying attention to the qualitative behaviour rather than the ultimate conver-
gence speed. The left hand plot pits the algorithms against the vanilla tube function. SignGD has
very different qualitative behaviour to the other algorithms—it appears to make progress completely
unimpeded by the saddles. We showed that this behaviour is partly due to the axis alignment of the
tube function, since after randomly rotating the objective the behaviour changes (although it is still
qualitatively different to the other algorithms).
One unexpected result was that for certain random rotations of the objective, signGD could get stuck
at saddle points (see right panel in Figure 1). On closer inspection, we found that the algorithm was
getting stuck in perfect periodic orbits around the saddle. Since the update is given by the learning
rate multiplied by a binary vector, if the learning rate is constant it is perfectly possible for a sequence
of updates to sum to zero. We expect that this behaviour relies on a remarkable structure in both
the tube function and the algorithm. We hypothesise that for higher dimensional objectives and
a non-fixed learning rate, this phenomenon might become extremely unlikely. This seems like a
worthy direction of future research. Indeed we found empirically that introducing momentum into
the update rule was enough to break the symmetry and avoid this periodic behaviour.
6
Under review as a conference paper at ICLR 2018
7	CIFAR-10experiments
To compare SGD, signSGD and Adam on less of a toy problem, we ran a large grid search over
hyperparameters for training Resnet-20 (He et al., 2016) on the CIFAR-10 dataset (Krizhevsky,
2009). Results are plotted in Figure 2. We evaluate over the hyperparamater 3-space of (initial
learning rate, weight decay, momentum), and plot slices to demonstrate the general robustness of
each algorithm. We find that, as expected, signSGD and Adam have broadly similar performance.
For hyperparameter configurations where SGD is stable, it appears to perform better than Adam and
signSGD. But Adam and signSGD appear more robust up to larger learning rates. Full experimental
details are given in Appendix A.
8	Discussion
First we wish to discuss the connections between signSGD and Adam (Kingma & Ba, 2015). Note
that setting the Adam hyperparameters βι = β2 = E = 0, Adam and SignSGD are equivalent.
Indeed the authors of the Adam paper suggest that during optimisation the Adam step will com-
monly look like a binary vector of ±1 (multiplied by the learning rate) and thus resemble the sign
gradient step. If this algorithmic correspondence is valid, then there seems to be a discrepancy be-
tween our theoretical results and the empirical good performance of Adam. Our convergence rates
suggest that signSGD should be worse than SGD by roughly a factor of dimension d. In deep neu-
ral network applications d can easily be larger than 106. We suggest a resolution to this proposed
discrepancy—there is structure present in deep neural network error surfaces that is not captured by
our simplistic theoretical assumptions. We have already discussed in Section 5 how the signSGD
bound is improved by a factor d in the case of gradients distributed uniformly across dimensions.
It is also reasonable to expect that neural network error surfaces might exhibit only weak coupling
across dimensions. To provide intuition for how such an assumption can help improve the dimension
scaling of signSGD, note that in the idealised case of total decoupling (the Hessian is everywhere di-
agonal) then the problem separates into d independent one dimensional problems, so the dimension
dependence is lost.
Next, let’s talk about saddle points. Though general non-convex functions are littered with local
minima, recent work rather characterises successful optimisation as the evasion of a web of saddle
points (Dauphin et al., 2014). Current theoretical work focuses either on using noise Levy (2016); Jin
et al. (2017) or curvature information (Allen-Zhu, 2017b) to establish bounds on the amount of time
needed to escape saddle points. We noted that merely passing the gradient through the sign operation
introduces an algorithmic instability close to saddle points, and we wanted to empirically investigate
whether this could be enough to escape them. We removed stochasticity from the algorithm to focus
purely on the effect of the sign function.
We found that when the objective function was axis aligned, then sign gradient descent without
stochasticity (signGD) made progress unhindered by the saddles. We suggest that this is because
signGD has a greater ability to ‘explore’, meaning it typically takes larger steps in regions of small
gradient than SGD, and it can take steps almost orthogonal to the true gradient direction. This
exploration ability could potentially allow it to break out of subspaces convergent on saddle points
without sacrificing its convergence rate—we hypothesise that this may contribute to the often more
robust practical performance of algorithms like Rprop and Adam, which bear closer relation to
signSGD than SGD. For non axis-aligned objectives, signGD could sometimes get stuck in perfect
periodic orbits around saddle points, though we hypothesise that this behaviour may be much less
likely for higher dimensional objectives (the testbed function had dimension 10) with non-constant
learning rate.
Finally we want to discuss the implications of our results for gradient quantisation schemes. Whilst
we do not analyse the multi-machine case of distributed optimisation, we imagine that our results
will extend naturally to that setting. In particular our results stand as a proof of concept that we can
provide guarantees for biased gradient quantisation schemes. Existing quantisation schemes with
guarantees require delicate randomisation to ensure unbiasedness. If a scheme as simple as ours
can yield provable guarantees on convergence, then there is a hope that exploring further down this
avenue can yield new and useful practical quantisation algorithms.
7
Under review as a conference paper at ICLR 2018
SGD
^}∣^
əlg 6u-u-leθ-
signSGD
Adam
əlg 6u-u-leθ-
Figure 2: Results for training Resnet-20 (He et al., 2016) on CIFAR-10 (Krizhevsky, 2009) for SGD,
signSGD and Adam. We plot test errors over a large grid of initial learning rate, weight decay and
momentum combinations. (In signSGD, momentum corresponds to taking the sign of a moving av-
erage of gradients—see Appendix A for the detailed experimental setup.) All algorithms at the least
get close to the baseline reported in (He et al., 2016) of 91.25%. Note the broad similarity in general
shape of the heatmap between Adam and signSGD, supporting a notion of algorithmic similarity.
Also note that whilst SGD has a larger region of very high-scoring hyperparameter configurations,
signSGD and Adam appear more stable for large learning rates.
9	Conclusion
We have investigated the theoretical properties of the sign stochastic gradient method (signSGD) as
an algorithm for non-convex optimisation. The study was motivated by links that the method has
both to deep learning stalwarts like Adam and Rprop, as well as to newer quantisation algorithms
that intend to cheapen the cost of gradient communication in distributed machine learning. We have
proved non-convex convergence rates for signSGD to first order critical points. Insofar as the rates
8
Under review as a conference paper at ICLR 2018
can directly be compared, they are of the same order as SGD in terms of number of gradient evalua-
tions, but worse by a linear factor in dimension. SignSGD has the advantage over existing gradient
quantisation schemes with provable guarantees, in that it doesn’t need to employ randomisation
tricks to remove bias from the quantised gradient.
We wish to propose some interesting directions for future work. First our analysis only looks at con-
vergence to first order critical points. Whilst we present preliminary experiments exhibiting success
and failure modes of the algorithm around saddle points, a more detailed study attempting to pin
down exactly when we can expect signSGD to escape saddle points efficiently would be welcome.
This is an interesting direction seeing as existing work always relies on either stochasticity or second
order curvature information to avoid saddles. Second the link that signSGD has to both Adam-like
algorithms and gradient quantisation schemes is enticing. In future work we intend to investigate
whether this connection can be exploited to develop large scale machine learning algorithms that get
the best of both worlds in terms of optimisation speed and communication efficiency.
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Communication-
Efficient Stochastic Gradient Descent, with Applications to Neural Networks. December 2017.
Zeyuan Allen-Zhu. Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex
Parameter. ICML, 2017a.
Zeyuan Allen-Zhu. Natasha 2: Faster Non-Convex Optimization Than SGD. arXiv:1708.08694 [cs,
math, stat], August 2017b. arXiv: 1708.08694.
Anonymous. Dissecting adam: The sign, magnitude and variance of stochastic gradients. Interna-
tional Conference on Learning Representations, 2018. URL https://openreview.net/
forum?id=S1EwLkW0W.
Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and Attacking the Saddle Point Problem in High-dimensional Non-convex
Optimization. In NIPS, 2014.
Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, and Aarti Singh. Gradient
Descent Can Take Exponential Time to Escape Saddle Points. arXiv:1705.10412, May 2017.
Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network
optimization problems. ICLR, 2015. arXiv: 1412.6544.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. DRAW: A Recur-
rent Neural Network For Image Generation. ICML, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to Escape
Saddle Points Efficiently. ICML, 2017. arXiv: 1703.00887.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. ICLR, 2015.
arXiv: 1412.6980.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, 2012.
Kfir Y. Levy. The Power of Normalization: Faster Evasion of Saddle Points. CoRR, abs/1611.04831,
2016.
Mu Li, Ziqi Liu, Alexander J Smola, and Yu-Xiang Wang. Difacto: Distributed factorization ma-
chines. In WSDM, 2016.
9
Under review as a conference paper at ICLR 2018
Yurii Nesterov and B.T. Polyak. Cubic regularization of Newton method and its global performance.
Mathematical Programming, (1):177-205, AUgUst 2006.
M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: the
RPROP algorithm. In IEEE International Conference on Neural Networks, 1993.
Frank Seide, Hao FU, Jasha Droppo, Gang Li, and Dong YU. 1-Bit Stochastic Gradient Descent
and Application to Data-Parallel DistribUted Training of Speech DNNs. In Interspeech 2014,
September 2014.
Nikko Strom. Scalable distribUted dnn training Using commodity gpU cloUd compUting. In Sixteenth
Annual Conference of the International Speech Communication Association, 2015.
Tijmen Tieleman and Geoffrey Hinton. RMSprop. Coursera: Neural Networks for Machine Learn-
ing, LectUre 6.5, 2012.
Ryota Tomioka and Milan Vojnovic. Qsgd: CommUnication-optimal stochastic gradient descent,
with applications to training neUral networks.
Wei Wen, Cong XU, Feng Yan, ChUnpeng WU, Yandan Wang, Yiran Chen, and Hai Li. Tern-
grad: Ternary gradients to redUce commUnication in distribUted deep learning. arXiv preprint
arXiv:1705.07878, 2017.
Kelvin XU, Jimmy Ba, Ryan Kiros, KyUnghyUn Cho, Aaron CoUrville, RUslan SalakhUdinov, Rich
Zemel, and YoshUa Bengio. Show, Attend and Tell: NeUral Image Caption Generation with VisUal
Attention. ICML, 2015.
10
Under review as a conference paper at ICLR 2018
A Experimental details
Here we describe the experimental setup for the CIFAR-10 (Krizhevsky, 2009) experiments using
the Resnet-20 architecture (He et al., 2016). We tuned over {weight decay, momentum, initial
learning rate} for optimisers in {SGD, signSGD, Adam}.
We used our own implementation of each optimisiation algorithm. Adam was implemented as in
(Kingma & Ba, 2015)with β2 = 0.999 and e = 10-8, and βι was tuned over. For both SGD and
signSGD we used a momentum sequence
mk+1 = βmk + (1 - β)gk
(11)
and then used the following updates:
SGD :	xk+1	=	xk	-	δkmk+1	(12)
signSGD :	xk+1	=	xk	-	δksign(mk+1)	(13)
Weight decay was implemented in the traditional manner of augmenting the objective function with
a quadratic penalty.
All other details not mentioned (learning rate schedules, network architecture, data augmentation,
etc.) are as in (He et al., 2016). In particular for signSGD we did not use the learning rate or
mini-batch schedules as provided by our theory. Code will be released if the paper is accepted.
B Proving the convergence rate of the sign gradient method
Theorem 1	(Non-Convex convergence rate of SignSGD). Apply Algorithm 1 under Assump-
tions 1, 2 and 3. Schedule the learning rate and mini-batch size as
δ
√k + 1
nk = k +1
(4)
Let NK be the cumulative number ofstochastic gradient calls UP to step K, i.e. NK = O (K2)
Then we have
E min Ilgk k1
0≤k≤κ-ι"ykl 11
p⅛ K + d(2 + lθg(2Nκ-ι))(σ +
(5)
≤
Proof. Our general strategy will be to show that the expected objective improvement at each step
will be good enough to guarantee a convergence rate in expectation. First let’s bound the improve-
ment of the objective during a single step of the algorithm for one instantiation of the noise. Note
that I[.] is the indicator function, and gk,i denotes the ith component of the vector gk.
First use Assumption 2, plug in the step from Algorithm 1, and decompose the improvement to
expose the stochasticity-induced error:
fk + 1 - fk ≤ gk (Xk+1 - Xk) + 2 kxk+1 - xkk2
=-δkgk Sign(Uk ) + δ2 2 d
d 2L
= -δk Igk I1 +2δkΣ |gk,i| I[sign(gk,i) = sign(gk,i)] + δ2 - d
i=1
Next we find the expected improvement at time k +1 conditioned on the previouS iterateS.
dL
E[fk+1 - fk |xk] ≤ -δk kgkkl +2δk): |gk,i | P[sign(gk,i) = sign(gk,i)] + δk ^-d
i=1
11
Under review as a conference paper at ICLR 2018
Note that the expected improvement crucially depends on the probability that each component of
the sign vector is correct. Intuition suggests that when the magnitude of the gradient |gk,i | is much
larger than the typical scale of the noise σ, then the sign of the stochastic gradient will most likely
be correct. Mistakes will typically only be made when |gk,i | is smaller than σ. We can make this
intuition rigorous using Markov’s inequality and our variance bound on the noise (Assumption 3).
P[sign(gk,i) = sign(gk,i)] ≤ P[∣gk,i - gk,i∣ ≥ |gk,i|]
≤ E[|gk,i - gk,i |]
|gk,i |
≤ PE[(gk,i- gk,i)2]
|gk,i |
≤ σ
|gk,i |
relaxation
Markov’s inequality
Jensen’s inequality
Assumption 3
This says explicitly that the probability of the sign being incorrect is controlled by the relative scale
of the noise to each component of the gradient magnitude. We denote the noise scale as σk since
it refers to the stochastic gradient with a mini-batch size of nk = k +1. We can plug this result
into the previous expression, take the sum over i, and substitute in our learning rate and mini-batch
schedules as follows:
Elfk+1 - fk∖xk ] ≤ -δk ∣∣gk k ι +2δk dσk + δ2 ɪd
=--，—kgk kι +2d -σ- + Ld
√k+1 kgkk1 + k + 1 + k + 1 2
δ	2δd
≤-√K kgkk1 + k+1(σ + δL)
In the last line we made some relaxations which will not affect the general scaling of the rate. Now
take the expectation over the noise in all previous iterates, and sum over k:
fo- f * ≥	f0	- E[fK]			Assumption 1
		K-1			
=	E	fk - fk+1			telescope
		k=0			
≥	E	K-1 - X √K kgkk1	2δd -k+1(σ + δL)		previous result
≥	E	K-1 X √K kgk kι k=0	- 2δd(1 +logK)(σ+δL)		harmonic sum
We can rearrange this inequality to yield a rate:
-1 1
E 0≤miK-1kgk k1 ≤ E X K kgk k1
k=0
≤ ɪ f0- f
√ √k[ δ
+ 2d(1 + log K)(σ +
Since We are growing our mini-batch size, it will take Nκ-ι = K(K+1) gradient evaluations to
reach step K-1. Using that 2NK-2 ≤ K2 ≤ 2NK-1 yields the result. For the sake of presentation,
we take the final step of squaring the bound, to make it more comparable with the SGD bound.
□
12
Under review as a conference paper at ICLR 2018
C Proving the convergence rate of stochastic gradient descent
Theorem 2	(Non-Convex convergence rate of stochastic gradient descent). Apply Algorithm 2
under Assumptions 1, 2 and 3. Schedule the learning rate and mini-batch size as
δk = √δ=
√k + 1
nk = 1
(6)
Let NK be the cumulative number of stochastic gradient calls UP to step K, i.e. NK = K.
Then we have that
E min kgk ∣∣2
0≤k≤K -Jgk"2
≤ ɪ [ f。- f *
-√Nk [δ(l - δL)
δL
+ d(I+ log NK) -~~2JL σ2
1 G
⑺
Proof. Consider the objective improvement in a single step, under one instantiation of the noise.
Use Assumption 2 followed by the definition of the algorithm.
fk+1 - fk ≤ gT (xk+1 - xk) + ^2 kxk+1 - xk ∣∣2
=-δkgT gk + δ2 2 IlUk ∣∣2
Take the expectation conditioned on previous iterates, and decompose the mean squared stochastic
gradient into its mean and variance. Note that since σ 2 is the variance bound for each component,
the variance bound for the full vector will be dσ2.
E[fk+1 - fk |xk] ≤ -δkkgkk2 + δ2 2 (kgk∣∣2 + dσ2)
Plugging in the learning rate schedule, and using that ^^ ≤ √+, We get that
δ	δ2 L	δ2 L
E[fk+ι - fk∣χk] ≤ -√= ∣gkk2 + E2∣gkk2 + E2σ2d
k+1 k+12 k+12
≤ -kgkk2 (1 - δL) + 上Lσ2d
—√k+!"k"2	2 k +12
Take the expectation over xk, sum over k, and We get that
f。 - f* ≥ f。 - E[fK]
K-1
=E X fk - fk+1
k=。
≥ X=。[√⅛Ehkgkk2i (1 - δL)- ⅛r2σ2d
≥ K [ɪe[ min kgkk21(1 - δL)1 - KX1[上Lσ2d
≥ [√K [o≤k≤K-Ikgkk2」l	2 n 公[k +12 ,
≥ √Kδ En Vmin J∣gk k2 (1- δL) -(1 + log κ)δ2 L σ2d
0≤k≤K-1	∖ 2 / 2
And rearranging yields the result.
□
13