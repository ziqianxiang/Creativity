Table 1: MNIST performance comparisonMethod	Test Accuracy(%)	Training Time(s)H-ELM (Tang et al., 2016)	99.13	281.37SVMSFC10	99.39	27.98Table 2: MNIST Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	99.37±0.05	15.28±0.75SFC30	99.38±0.05	46.88±0.97SFC100	99.46±0.02	154.40±0.14CNN0.1	99.24±0.04	164.49±1.86CNN0.01	99.22±0.07	154.57±0.343.2	CIFAR- 1 0The Fig. 1 presents the experiments on CIFAR-10. The SVM, ELM, and even H-ELM present lowaccuracy when trained on raw features. It happens because CIFAR-10, unlikely MNIST, is a non-sparse dataset and since the importance of using high-quality features is high in this case. The useof the SFC features significantly reduces the training time. Moreover, the SVM and ELM classifiersimproved their performance on the test set in about 30% and 50%, respectively.
Table 2: MNIST Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	99.37±0.05	15.28±0.75SFC30	99.38±0.05	46.88±0.97SFC100	99.46±0.02	154.40±0.14CNN0.1	99.24±0.04	164.49±1.86CNN0.01	99.22±0.07	154.57±0.343.2	CIFAR- 1 0The Fig. 1 presents the experiments on CIFAR-10. The SVM, ELM, and even H-ELM present lowaccuracy when trained on raw features. It happens because CIFAR-10, unlikely MNIST, is a non-sparse dataset and since the importance of using high-quality features is high in this case. The useof the SFC features significantly reduces the training time. Moreover, the SVM and ELM classifiersimproved their performance on the test set in about 30% and 50%, respectively.
Table 3: CIFAR-10 performance comparisonMethod	Type	Test Accuracy(%)SIFT (Bo et al., 2010)	Prior Knowl.	65.6LIFT (Sohn & Lee, 2012)	Unsup. Deep	82.2RotoTrans. Scat. (Oyallon & Mallat, 2015)	Prior Knowl.	82.3NOMP (Lin & EDU, 2014)	Unsup. Dict.	82.9RFL (Yangqing Jia et al., 2012)	Unsup. Dict.	83.1SVMSFC10	Supervised	88.40±0.24Table 4: CIFAR-10 fully trained VGG19 and Simple Feature ConvolutionalModel	Accuracy(%) [VGG19(%)]	Time(s) [VGG19(%)]SVMSFC10	88.40 [95.88]	308.40 [12.06]ELMSFC10	88.34 [95.82]	275.20 [10.76]VGG19	92.19 [100.00]	2556.29 [100.00]Table 5: CIFAR-10 Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	88.44±0.27	266.18±0.86SFC30	91.17±0.14	805.83±0.88SFC100	92.19±0.19	2556.29±3.74CNN0.1	85.12±0.37	2611.74±7.52CNN0.01	87.57±0.10	2571.10±6.58
Table 4: CIFAR-10 fully trained VGG19 and Simple Feature ConvolutionalModel	Accuracy(%) [VGG19(%)]	Time(s) [VGG19(%)]SVMSFC10	88.40 [95.88]	308.40 [12.06]ELMSFC10	88.34 [95.82]	275.20 [10.76]VGG19	92.19 [100.00]	2556.29 [100.00]Table 5: CIFAR-10 Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	88.44±0.27	266.18±0.86SFC30	91.17±0.14	805.83±0.88SFC100	92.19±0.19	2556.29±3.74CNN0.1	85.12±0.37	2611.74±7.52CNN0.01	87.57±0.10	2571.10±6.585Under review as a conference paper at ICLR 20183.3 CIFAR- 1 00 ExperimentsThe Fig. 2 shows that training on raw data produces even worst results on this dataset and thatSFC, in fact, improves the performance of the test accuracy by providing high-quality features to theclassifiers. Once again, the results presented by the two used classifiers are mostly the same, withall the differential of the solution being the feature representation used.
Table 5: CIFAR-10 Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	88.44±0.27	266.18±0.86SFC30	91.17±0.14	805.83±0.88SFC100	92.19±0.19	2556.29±3.74CNN0.1	85.12±0.37	2611.74±7.52CNN0.01	87.57±0.10	2571.10±6.585Under review as a conference paper at ICLR 20183.3 CIFAR- 1 00 ExperimentsThe Fig. 2 shows that training on raw data produces even worst results on this dataset and thatSFC, in fact, improves the performance of the test accuracy by providing high-quality features to theclassifiers. Once again, the results presented by the two used classifiers are mostly the same, withall the differential of the solution being the feature representation used.
Table 6: CIFAR-100 performance comparisonMethod	Type	Test Accuracy(%)RFL (Yangqing Jia et al., 2012)	Unsup. Dict.	54.2RotoTrans. Scat. (Oyallon & Mallat, 2015)	Prior Knowl.	56.8NOMP (Lin & EDU, 2014)	Unsup. Dict.	60.8SVMSFC30	Supervised	65.03±0.31	Table 7: CIFAR-100 Simple Fast Convolution performanceMethod	Test Accuracy(%)	Training Time(s)SFC10	52.96±1.01	256.87±1.37SFC30	65.40±0.27	798.42±2.01SFC100	69.31±0.38	2554.85±2.99CNN0.1 CNN0.01	54.26±0.25	2545.86±3.67 59.27±0.41	2559.71±6.506Under review as a conference paper at ICLR 2018The results presented in the Tables 2, 5 and 7 essentially show that, when compared to classicalapproach of numerous epochs of constant learning rate (CNN0.1 and CNN0.01), the SFC variantsprovide better test accuracy despite being many times faster. Therefore, a fast changing in thelearning rate is indeed a viable alternative to speed up deep neural networks training time.
