Table 1: NMI scores from varying the dimensionality of the latent space in the NATAC and baselinemodels. The baselines use k-means with the same number of clusters as the repsective NATACmodel converged to. We include NATAC models with a latent dimensionality of d = 3, whose latentrepresentations can be viewed without dimensionality reduction. Appendix A contains links to thevisualizations hosted on the TensorFlow embedding projector.
Table 2: Comparison of our best performing NATAC model (with d = 10) on the entire MNISTdataset. NMI and classification error are calculated from the entire data set. We report the evaluationmetric used by the authors of each respective model. Precision of values are the same as thosereported by the original paper. Note that many of the best-performing methods (DCD, IMSAT,Adversarial Autoencoders) also assume a uniform class distribution along with a pre-set number ofclusters.
Table 3: Clustering results for the 20 Newsgroups dataset.
Table 4: Comparison of our best performing NATAC model (with d = 4) on the entire 20 News-groups dataset. NMI is calculated from the entire data set. Figures for other methods taken fromYang et al. (2016b)Along with NATAC-k and AE-k comparisons, we also use a spherical k-means model. Sphericalk-means is a commonly used technique of unsupervised document clustering, a good description ofit can be found in Buchta et al. (2012).
Table 5: Clustering results for the Twitter dataset. Spherical k-means models trained with a vocab-ulary size of 5, 000 (same as the 20 Newsgroups baselines).
Table 6: Mean NMI and converged number of centroids when training a NATAC model with varyingamounts of pre-training. Mean taken from 5 consecutive runs using the same hyperparameters.
Table 7: Mean NMI (left) and converged number of clusters (right) of NATAC models with differentvalues for λinital and λfinal. Mean values taken from 5 consecutive runs using the same hyperparam-eters. Models trained on the train set of 20 Newsgroups and evaluated on the test set.
