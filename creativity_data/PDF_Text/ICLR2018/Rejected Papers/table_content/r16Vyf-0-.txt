Table 1: Three outputs of a CelebA super-resolution model followed by three image completions bya conditional CIFAR-10 model, with input, model output and the original from left to right1	IntroductionRecent advances in modeling the distribution of natural images with neural networks allow them togenerate increasingly natural-looking images.
Table 2: Conditional image generations for all CIFAR-10 categories. Images on the left are froma model that achieves 3.03 bits/dim on the test set. Images on the right are from our best non-averaged model with 2.99 bits/dim. Both models are able to generate convincing cars, trucks, andships. Generated horses, planes, and birds also look reasonable.
Table 3: Negative log-likelihoods on the CIFAR-10 test and ImageNet validation sets. The Image Transformer outperforms all models but PixelCNN++, achieving a new state of the art on ImageNet. Larger memory blocks significantly improve its performance.			Model Type	Memory Block Size	CIFAR-10 (Test)	NLL ImageNet (Validation)Pixel CNN	-	3.14	-Row Pixel RNN	-	3.00	3.86Gated Pixel CNN	-	3.03	3.83Pixel CNN++	-	2.92	-Image Transformer 1D local	1	4.06	-	16	3.47	-	64	3.13	-	256	2.99	3.78with checkpoint averaging	256	2.98	3.775.1	Generative Image ModelingOur unconditioned and class-conditioned image generation models both use 1D local attention, withlq = 256 and a total memory size of 512. On CIFAR-10 our best class-conditioned model (2.99bits/dim) uses 8 self-attention and feed-forward layers, d = 1024, 16 attention heads, 2048 di-mensions in the feed-forward layers, and a dropout of 0.3. Our smaller CIFAR-10 models (3.03bits/dim) have d = 512, 1024 dimensions in the feed-forward layers, 8 attention heads and usedropout = 0.1. Our state of the art ImageNet unconditioned generation model is significantlylarger, with 12 self-attention and feed-forward layers, d = 1024, 4096-dimensional feed-forwardlayers, 16 attention heads, and dropout = 0.1.
Table 4: Negative log-likelihood and human eval performance for the Image Transformer on CelebA.
Table 5:	Images from our 1D and 2D local attention super-resolution models trained on CelebA,sampled with different temperatures. 2D local attention with Ï„ = 0.9 scored highest in our humanevaluation study.
Table 6:	On the left are image completions from our best conditional generation model, where wesample the second half. On the right are samples from our four-fold super-resolution model trainedon CIFAR-10. Our images look realistic and plausible, show good diversity among the completionsamples and observe the outputs carry surprising details for coarse inputs in super-resolution.
