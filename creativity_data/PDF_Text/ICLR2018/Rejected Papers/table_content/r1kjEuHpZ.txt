Table 1: Representative words of 9 exemplar basis vectorsLDD-LI - is set to 1, the same regularization parameter λ imposes the same level of sparsity forboth LDD-L1 and L1-only. Since LDD-L1 encourages the vectors to be mutually orthogonal, theintersection between vectors’ supports is small, which consequently results in small overlap. This isnot the case for L1-only, which hence is less effective in reducing overlap.
Table 2: Classification accuracy on the test sets of 20-News and RCV1, and the gap between trainingand test accuracy.
Table 3: Word-level perplexities on PTB test setCNN for Image Classification The CNN architecture follows that of the wide residual network(WideResNet) (Zagoruyko & Komodakis, 2016). The depth and width are set to 28 and 10 respec-tively. The networks are trained using SGD, where the epoch number is 200, the learning rate isset to 0.1 initially and is dropped by 0.2 at 60, 120 and 160 epochs, the minibatch size is 128 andthe Nesterov momentum is 0.9. The dropout probability is 0.3 and the L2 weight decay is 0.0005.
Table 4: Classification error (%) on CIFAR-10 test setrithm to interpret the learned features of each neuron. Our method is orthogonal to these existingapproaches and can be potentially used with them together to further improve interpretability.
