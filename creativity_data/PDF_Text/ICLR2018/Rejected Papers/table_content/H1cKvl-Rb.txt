Table 1: Double DQN hyperparameters. These hyperparameters are selected based on performancesof seven Atari games: Beam Rider, Breakout, Pong, Enduro, Qbert, Seaquest, and Space Invaders.
Table 2: Comparison of maximal mean rewards achieved by agents. Maximal mean reward iscalculated in a window of 100 consecutive episodes. Bold denotes the highest value in each row.
Table 3: Comparison of Ensemble Voting, UCB Exploration, both trained with 40 million frames andA3C+ of Bellemare et al. (2016), trained with 200 million frames11Under review as a conference paper at ICLR 2018Category	Total	Bootstrapped DQN	Double DQN	Ensemble Voting	UCB-ExplorationHuman Optimal	23	0	3	5	15Score Explicit	8	0	2	1	5Dense Reward	8	0	1	1	6Sparse Reward	5	1	0	2	2Table 4: Comparison of each method across different game categories. The Atari games are separatedinto four categories: human optimal, score explicit, dense reward, and sparse reward. In each row,we present the number of games in this category, the total number of games where each algorithmachieves the optimal performance according to Table 2. The game categories follow the taxonomy inTable 1 of Ostrovski et al. (2017)C	APPROXIMATING BAYESIAN Q-LEARNING WITH Q-ENSEMBLESIn this section, We first derive a posterior update formula for the Q *-function under full explorationassumption and this formula turns out to depend on the transition Markov chain. Next, we approximatethe posterior update With Q-ensembles {Qk } and demonstrate that the Bellman equation emerges asthe approximate update rule for each Qk .
Table 4: Comparison of each method across different game categories. The Atari games are separatedinto four categories: human optimal, score explicit, dense reward, and sparse reward. In each row,we present the number of games in this category, the total number of games where each algorithmachieves the optimal performance according to Table 2. The game categories follow the taxonomy inTable 1 of Ostrovski et al. (2017)C	APPROXIMATING BAYESIAN Q-LEARNING WITH Q-ENSEMBLESIn this section, We first derive a posterior update formula for the Q *-function under full explorationassumption and this formula turns out to depend on the transition Markov chain. Next, we approximatethe posterior update With Q-ensembles {Qk } and demonstrate that the Bellman equation emerges asthe approximate update rule for each Qk .
