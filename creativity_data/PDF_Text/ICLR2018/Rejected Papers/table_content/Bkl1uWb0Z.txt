Table 1: Statistics of the data used in our experiment.
Table 2: Results for translating En-De and En-Ru both directions. In all of our experiments, whilethe SA-NMT-shared model does not statistically outperform FA-NMT-shared it does outperformFA-NMT with separate attentions in three benchmarks. The results show that our proposed shared-attention is a benefit for NMT.
Table 3: Directed and Undirected (DA/UA) accuracies of our models on both English and Germandata as compared to branching baselines. Punctuation is removed during the evaluation. Our resultsshow an intriguing effect of the target language on grammar induction. We observe a huge boost inDA/UA scores in FA-NMT and SA-NMT-shared models when the target language is morphologicallyrich (Russian). In comparison to previous work (Belinkov et al., 2017; Shi et al., 2016) on theencoder’s ability to capture source side syntax, we show a stronger result that even when the encodersare designed to capture syntax explicitly, the choice of the target language has a great influence onthe amount of syntax learned by the encoder.
Table 4: Most common grammar rules and their production percentages in EN and DE. English’sstrict left branching structure makes it difficult to outperform, but we see substantial gains by ourapproach on the more syntactic elements of language (e.g. DET/ADJ/ADP attachments). For EN,we use en→ru systems.
