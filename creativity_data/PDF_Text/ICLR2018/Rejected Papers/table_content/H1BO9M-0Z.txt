Table 1: F1 score of fine-tuning on base meta-learner	Cmptr. Components	Kitch. Storage & Org.	Cats Supples	Laptops10MB	0.832	0.841	0.856	0.81730MB	0.847		0.859		0.876	0.854We sample 2 (l) sub-corpora over the set of reviews from each domain and limit the size of thesub-corpora to 10 MB. We select top 5000 words as word features (f). We randomly select 500words (|Vmeta | = 500) from each domain and ignore words with zero counts on co-occurrence toobtain pairwise examples. This ends up with 80484 training examples, 6234 validation examples,and 20740 testing examples. The f1-score of meta-learner is 81%.
Table 2: Accuracy of different embeddings on product type classification tasks (numbers in paren-thesis indicates number of classes)	Cmptr. Cmpnts. (13)	Kitch. Strg. & Org. (17)	Cats Supples (11)NE	0.596	0.653	0.696fastText	0.705	0.717	0.809GoogleNeWs	0.76	0.722	0.814GloVe.Twitter.27B	0.696	0.707	0.80GloVe.6B	0.701	0.725	0.823GloVe.840B	0.803	0.758	0.855ND 10M	0.77011	0.74905	0.85ND 30M	0.794	0.766	0.87200D + ND 30M	0.793	0.759	0.859LL 200D + ND 10M	0791	0761	0.872LL 50D + ND 30M	0.795	0.768	0.868LL 100D + ND 30M	0.803	0.773	0.874LL 200D + ND 30M	0.809		0.775		0.883New Domain 10M (ND 10M): This is a baseline embedding pre-trained only from the new domain10 MB corpus. We show that the embeddings trained from a small corpus alone are not good enough.
Table 3: Performance of different embeddings on aspect extraction task	Precision	Recall	FI-ScoreNE	0.596	0.493	0.54fastText	0.655	0.47	0.547GoogleNeWs	0.7	0.638	0.667GloVe.Twitter.27B	0.642	0.468	0.541GloVe.6B	0.68	0.505	0.579GloVe.840B	0.722	0.6406	0.679ND 10M	0.663	0.57	0.613ND 30M	0.713	0.62	0.663200D + ND 30M	0.731	0.65	0.688LL 200D + ND 10M	-0724-	0.636	-0.677-LL 50D + ND 30M	0.736	0.637	0.683LL 100D + ND 30M	0.723	0.65	0.685LL 200D + ND 30M	0.734	0.659	0.694Table 4: Accuracy of different embeddings on sentiment classification task	Cmptr. Cmpnts.	Kitch. Strg. & Org.	Cats Supples^NE	0777	0.764	0.67GoogleNews	0.847	0.815	0.732GloVe.Twitter.27B	0.776	0.813	0.727
Table 4: Accuracy of different embeddings on sentiment classification task	Cmptr. Cmpnts.	Kitch. Strg. & Org.	Cats Supples^NE	0777	0.764	0.67GoogleNews	0.847	0.815	0.732GloVe.Twitter.27B	0.776	0.813	0.727GloVe.840B	0.877	0.859	0.779ND 10M	0.885	0.849	0.795ND 30M	0.889	0.867	0.806200D + ND 30M	0.886	0.87	0.807LL 200D + ND 10M	0.882	085	0.773LL 200D + ND 30M	0.891	0.872	0.8086.6	Aspect ExtractionAspect extraction is an important task in sentiment analysis (Liu (2012; 2015)). We use the datasetfrom SemEval-2014 Task 4: Aspect-based sentiment analysis (Pontiki et al. (2014)) as a down-stream new domain task. This dataset contains human annotated Laptop aspects and their polarities.
Table 5: Performance of Concatenation with GloVe.860B	CC	KSO	CS	Laptops (f1)GloVe.840B&ND 30M	0.811	0.78	0.885	0.723	=GloVe.840B&LL 200D + ND 30M	0.817	0.783	0.887	0.729difference is small. This is close to our previous experience. A possible explanation is that sentimentclassification relies on sentiment words like “good” or “bad”. However, those words have similarcontext words, e.g., “This phone is good.” and “This phone is bad.”. So the co-occurrence-basedtraining corpus of embedding is not good for learning the embeddings of sentiment words.
