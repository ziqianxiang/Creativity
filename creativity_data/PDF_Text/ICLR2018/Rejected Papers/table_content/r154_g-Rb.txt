Table 1: Task success rate on Mazebase environments. Our Attribute Planner (AP) approach per-forms much better than the reactive policies trained with Reinforce. The addition of the attributegraph (over Reinforce only model) is crucial for multi-step evaluation tasks.
Table 2: Model comparison on block stacking task accuracy. Baselines marked ‘multi-step’ or ‘cur-riculum‘ get to see complex multi-step tasks at train time. The Attribute Planner (AP) generalizesfrom one-step training to multi-step and underspecified tasks with high accuracy, while reinforce-ment learning and inverse model training do not. AP outperforms A3C even with a curriculum oftasks. Ablating the normalized graph transition table cπ degrades AP performance substantially onmulti-step tasks due to aliasing. Inverse one-step model was trained on 2 million examples, in-verse multi-step and AP models were trained on 1 million examples, A3C models were trained toconvergence.
Table 3: Effect of the number of (one-step) training examples on one-step and multi-step perfor-mance, for an inverse model and the Attribute Planner model. The inverse models are trained on 2xthe samples, including the samples generated from learning cπ in our AP method.
Table 4: Task success rate on multi-step task in the hard crafting environment. Our Attribute Plannerclearly outperforms other baseline approaches.
Table 5: Effect of the number of (one-step) training examples on one-step and multi-step perfor-mance, for an inverse model and the Attribute Planner model. The inverse models are trained on 2xthe samples, including the samples generated from learning cπ in our AP method.
