Table 1: Untargeted black-box attacks: Each entry has the attack success rate for the attackmethod given in that column on the model in each row. The number in parentheses for each entryis ∆(X, Xadv), the average distortion over all samples used in the attack. In each row, the entry inbold represents the black-box attack with the best performance on that model. Gradient Estimationusing Finite Differences is our method, which has performance matching white-box attacks. Above:MNIST, L∞ constraint of = 0.3. Below: CIFAR-10, L∞ constraint of = 8.
Table 2: Comparison of untargeted query-based black-box attack methods. All results are forattacks using the first 1000 samples from the MNIST dataset on Model A and with an L∞ constraintof 0.3. The logit loss is used for all methods expect PSO, which uses the class probabilities.
Table 3: Untargeted black-box attacks for models with adversarial training: adversarial successrates and average distortion ∆(X, Xadv) over the samples. Above: MNIST, = 0.3. Below:CIFAR-10, = 8.
Table 4: Accuracy of models on the benign test data				C.3 Alternative adversarial success metricNote that the adversarial success rate can also be computed by considering only the fraction of inputsthat meet the adversary’s objective given that the original sample was correctly classified. That is,one would count the fraction of correctly classified inputs (i.e. f(x) = y) for which f(xadv) 6= yin the untargeted case, and ft (xadv) = T in the targeted case. In a sense, this fraction representsthose samples which are truly adversarial, since they are misclassified solely due to the adversarialperturbation added and not due to the classifier’s failure to generalize well. In practice, both thesemethods of measuring the adversarial success rate lead to similar results for classifiers with highaccuracy on the test data.
Table 5: Adversarial success rates for transferability-based attacks on Model A (MNIST) at= 0.3. Numbers in parentheses beside each entry give the average distortion ∆(X, Xadv) over thetest set. This table compares the effectiveness of using a single local model to generate adversarialexamples versus the use of a local ensemble.
Table 6: Targeted black-box attacks: adversarial success rates. The number in parentheses () foreach entry is ∆(X, Xadv), the average distortion over all samples used in the attack. Above: MNIST,= 0.3. Below: CIFAR-10, = 8.
Table 7: Attacks evaluated in this paper(c)	Transferability attack (local model ensemble) using FGS and IFGS samples generatedon a source model for both loss functions (Transfer models FGS/IFGS-loss); e.g.,Transfer Model B, Model C IFGS-logit2.	Query based attacks(a)	Finite-difference and Iterative Finite-difference attacks for the gradient estimationattack for both loss functions (FD/IFD-loss); e.g., FD-logit(b)	Gradient Estimation and Iterative Gradient Estimation with Query reduction attacks(IGE/GE-QR (Technique-k, loss)) using two query reduction techniques, randomgrouping (RG) and principal component analysis components (PCA); e.g., GE-QR(PCA-k, logit)3.	White-box FGS and IFGS attacks for both loss functions (WB FGS/IFGS (loss))H Adversarial samplesIn Figure 4, we show some examples of successful untargeted adversarial samples against Model Aon MNIST and Resnet-32 on CIFAR-10. These images were generated with an L∞ constraint of= 0.3 for MNIST and = 8 for CIFAR-10. Clearly, the amount of perturbation added by iterativeattacks is much smaller, barely being visible in the images.
Table 8: Untargeted white-box attacks: adversarial success rates and average distortion ∆(X, Xadv)over the test set. Above: MNIST, = 0.3. Below: CIFAR-10, = 8.
Table 9: Targeted white-box attacks: adversarial success rates and average distortion ∆(X, Xadv)over the test set. Above: MNIST, = 0.3. Below: CIFAR-10, = 8.
Table 10: Untargeted white-box attacks for models with adversarial training: adversarial successrates and average distortion ∆(X, Xadv) over the test set. Above: MNIST, = 0.3. Below: CIFAR-10,= 8.
