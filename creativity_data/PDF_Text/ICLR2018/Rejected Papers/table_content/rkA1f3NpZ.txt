Table 1: MNIST Network ArchitectureLayer Type	ParametersRelu Convolutional	32 filters (3×3)Relu Convolutional	32 filters (3×3)Max Pooling	2×2Relu Fully Connected	128 unitsDropout	0.5Relu Fully Connected	10 unitsSoftmax	10 units4Under review as a conference paper at ICLR 2018Table 2: Experimental results on the MNIST and the CIFAR-10 data setsMNIST AccuracyTest Data		No Attack		Grad. 1		Grad. 2Type	Method	Single	Ensemble	Single	Ensemble	Ensemble	Rand. Ini.	0.9912	0.9942	0.3791	0.6100	0.4517FGSM	Mix. Mod.	0.9918	0.9942	0.3522	0.5681	0.4609	Bagging	0.9900	0.9927	0.4045	0.6738	0.5716	Gauss Noise	0.9898	0.9920	0.5587	0.7816	0.7043	Rand. Ini.	0.9912	0.9942	0.0906	0.6518	0.8875
Table 2: Experimental results on the MNIST and the CIFAR-10 data setsMNIST AccuracyTest Data		No Attack		Grad. 1		Grad. 2Type	Method	Single	Ensemble	Single	Ensemble	Ensemble	Rand. Ini.	0.9912	0.9942	0.3791	0.6100	0.4517FGSM	Mix. Mod.	0.9918	0.9942	0.3522	0.5681	0.4609	Bagging	0.9900	0.9927	0.4045	0.6738	0.5716	Gauss Noise	0.9898	0.9920	0.5587	0.7816	0.7043	Rand. Ini.	0.9912	0.9942	0.0906	0.6518	0.8875BIM	Mix. Mod.	0.9918	0.9942	0.0582	0.6656	0.9076	Bagging	0.9900	0.9927	0.1110	0.7068	0.9233	Gauss Noise	0.9898	0.9920	0.5429	0.9152	0.9768CIFAR-10 AccuracyTest Data		No Attack		Grad. 1		Grad. 2Type	Method	Single	Ensemble	Single	Ensemble	Ensemble	Rand. Ini.	0.7984	0.8448	0.1778	0.4538	0.3302FGSM	Mix. Mod.	0.7898	0.8400	0.1643	0.4339	0.3140	Bagging	0.7815	0.8415	0.1822	0.4788	0.3571	Gauss Noise	0.7160	0.7687	0.2966	0.6097	0.4707	Rand. Ini.	0.7984	0.8448	0.1192	0.5232	0.6826
Table 3: CIFAR-10 Network ArchitectureLayer Type	ParametersRelu Convolutional	32 filters (3×3)Relu Convolutional	32 filters (3×3)Max Pooling	2×2Dropout	0.2Relu Convolutional	64 filters (3×3)Relu Convolutional	64 filters (3×3)Max Pooling	2×2Dropout	0.3Relu Convolutional	128 filters (3×3)Relu Convolutional	128 filters (3×3)Max Pooling	2×2Dropout	0.4Relu Fully Connected	512 unitsDropout	0.5Relu Fully Connected	10 unitsSoftmax	10 unitsWhen using BIM attacks accuracies for single classifiers lie between 11% and 31%. Again, theensemble methods outperform the single classifiers reaching accuracies of 52%-67% when attacked
Table 4: Accuracies of different defense mechanismsMNIST					CIFAR-10	Methods	No Attack	FGSM	BIM	No Attack	FGSM	BIMBagging	0.9927*^~	0.5716	0.9233	0.8415*	0.3571	0.7166*Adv. Train.	0.9902	0.3586	0.5420	0.7712	0.1778	0.3107Def. Dist.	0.9840	0.0798	0.3829	0.7140	0.1828	0.3635Bagging + Adv. Train.	0.9927*	0.8703*	0.9840*	0.8320	0.5010*	0.7017Bagging + Def. Dist.	0.9875	0.0954	0.4514	0.7323	0.1839	0.4569In defensive distillation a teacher model F is trained on a training data set X. Then smoothed labelsat temperature T are computed byFT(X)exp(Fi(X )/T)PN=i exp(Fi(X)/T)i∈{1,...,N}where Fi(X) refers to the probability of the i-th out of N possible classes. A distilled network is anetwork that is trained on the training data X using the smoothed labels FT (X). In the following,we use T = 10 based on the experimental results in Papernot et al. (2016d).
