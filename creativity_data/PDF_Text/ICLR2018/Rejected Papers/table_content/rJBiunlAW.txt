Table 1: Classification (Section 4.1) test accuracies on six benchmarks.
Table 2: Mean validation accuracies (y-axis) ofLSTM, CNN, and SRU for the first 100 epochs onthe six classification benchmarks. X-axis: training time used (in seconds).
Table 3: Exact match (EM) and F1 scores of various models on SQuAD (Section 4.2). We also reportthe total processing time per epoch and the time spent in RNN computations. SRU outperforms theLSTM models, and is more than six times faster than cuDNN LSTM. We also list the state-of-the-art test results for the EM and F1 metrics as listed on the leaderboard on December, 2017. Bothstate-of-the-art methods use RNNs, and can potentially benefit from our approach.
Table 4: Language modeling perplexities on the PTB dataset. Models in comparison are trained usingsimilar regularization and learning strategy: variational dropout is used except for (Zaremba et al.,2014), (Press & Wolf, 2017) and cuDNN LSTM; input and output word embeddings are tied exceptfor (Zaremba et al., 2014); SGD with learning rate decay is used for all models.(Section 4.3). Wealso report time per training epoch, including for the entire architecture (Total) and for the RNN only.
Table 5: English-German translation results (Section 4.4). We list the total number of parameters(Size all) and the number excluding word embeddings (Size w/o Emb.). Our setup disables ht-1input, which significantly reduces the training time. Timings are performed on a single Nvidia TitanX Pascal GPU. We also list recent state-of-the-art results.
Table 6: Word error rate (WER) for speech recognition (Section 4.5). The timing numbers are basedon a naive implementation of SRU in CNTK. No CUDA-level optimizations are performed.
Table 7: Word error rate (WER) for LSTM baselines on the Switchboard-1 corpus (Section 4.5).
Table 8: Word error rate (WER) comparison of the effect of transformation in the highway connection.
Table 9: Word error rate (WER) and time per training epoch as function of SRU depth for speechrecognition (Section 4.5). The timing numbers are based on a naive implementation of SRU in CNTK.
