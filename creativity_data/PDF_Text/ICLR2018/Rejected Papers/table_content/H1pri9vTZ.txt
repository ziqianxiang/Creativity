Table 1: Unification of Infinite Dimensional Neural Network TheoryName	Form	DFM	AuthorsInNfinite	b+p∞=ι Vjh(x； Uj)	N∞: RnE- →			→		Neal(1996); Williams (1998)		C=IR		Rm	Functional	p MLPs	∑2i=1 βig (J wiξ dμ)	F :					Stinchcombe	L1(R)	→回→		回	(1999); Rossi et al. (2002) Le Roux & Bengio (2007)					OnnNnUOUS	Rωι(u)g(x ∙ ωo(u)) du C : ∣ Rn ∣ → J		t⅜b])	→	Rm	Non-					Le Roux & Bengio (2007)Conamutrus	R皿回回* d	C0: 叵 →		L1(R)	→[	Rm	NNs Infinite	same as non-parametric	same as non-parametric Layer NNs	continuous NNs	continuous NNs					Globerson & Livni (2016); Hazan & Jaakkola (2015)Another variant of infinite dimensional neural networks, which we hope to generalize, is the func-tional multilayer perceptron (Functional MLP). This body of work is not referenced in any of theaforementioned work on infinite layer neural networks, but it is clearly related. The fundamental ideais that given some f ∈ V = C(X), where X is a locally compact Hausdorff space, there exists ageneralization of neural networks which approximates arbitrary continuous bounded functionals onV (maps f → a ∈ R). These functional MLPs take the form PiP=i βig (ʃ ωɪ(x)f (x) dμ(x)). Theauthors show the power of such an approximation using the functional analysis results of Stinchcombe(1999) and additionally provide statistical consistency results defining well defined optimal parameterestimation in the infinite dimensional case.
