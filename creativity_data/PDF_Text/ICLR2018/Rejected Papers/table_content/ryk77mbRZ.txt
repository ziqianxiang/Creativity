Table 1: Summary statistics of the datasets used for the experiments.
Table 2: NOISIN improves generalization performance in recurrent neural network language mod-els. The results below correspond to perplexity scores on the test set of the Penn Treebank. Thelower the perplexity the better. We compare to the version of dropout implemented in (Gal andGhahramani, 2016). An asterisk next to NOISIN results indicates that the variance parameter of theGaussian noise distribution was learned. Overall we find that learned noise had more stable perfor-mance than fixed noise. The computational cost of NOISIN and the traditional RNN were similar,while dropout was noticeably slower.
Table 3: Test set perplexity scores on the Wikitext-2 dataset. The lower the better. NOISIN leads tobetter performance.
