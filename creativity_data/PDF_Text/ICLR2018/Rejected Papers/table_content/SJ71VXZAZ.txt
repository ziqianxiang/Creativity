Table 2: IMDB sentiment analysisTable 1: Small dataset classification accuracies76961 total labels compared to the 6920 sentence level labels. As a demonstration of the capability ofunsupervised representation learning to simplify data collection and remove steps from a traditionalNLP pipeline, our reported results ignore these dense labels and computed parse trees, using only theraw text and sentence level labels.
Table 1: Small dataset classification accuracies76961 total labels compared to the 6920 sentence level labels. As a demonstration of the capability ofunsupervised representation learning to simplify data collection and remove steps from a traditionalNLP pipeline, our reported results ignore these dense labels and computed parse trees, using only theraw text and sentence level labels.
Table 3: SICK semantic relatedness subtaskTable 4: Microsoft Paraphrase Corpus4.3	Capacity CeilingEncouraged by these results, we were curioushow well the model’s representation scales tolarger datasets. We try our approach on the bi-nary version of the Yelp Dataset Challenge in2015 as introduced in Zhang et al. (2015). Thisdataset contains 598,000 examples which is anorder of magnitude larger than any other datasetswe tested on. When visualizing performance asa function of number of training examples inFigure 4, we observe a "capacity ceiling" wherethe test accuracy of our approach only improvesby a little over 1% across a four order of mag-nitude increase in training data. Using the fulldataset, we achieve 95.22% test accuracy. Thisbetter than a BoW TFIDF baseline at 93.66%Figure 4: Performance on the binary version of theYelp reviews dataset as a function of labeled train-
Table 4: Microsoft Paraphrase Corpus4.3	Capacity CeilingEncouraged by these results, we were curioushow well the model’s representation scales tolarger datasets. We try our approach on the bi-nary version of the Yelp Dataset Challenge in2015 as introduced in Zhang et al. (2015). Thisdataset contains 598,000 examples which is anorder of magnitude larger than any other datasetswe tested on. When visualizing performance asa function of number of training examples inFigure 4, we observe a "capacity ceiling" wherethe test accuracy of our approach only improvesby a little over 1% across a four order of mag-nitude increase in training data. Using the fulldataset, we achieve 95.22% test accuracy. Thisbetter than a BoW TFIDF baseline at 93.66%Figure 4: Performance on the binary version of theYelp reviews dataset as a function of labeled train-ing examples. The model’s performance plateaus
Table 5: Random samples from the model generated when the value of sentiment unit hidden state isfixed to either -1 or 1 for all steps. The sentiment unit has a strong influence on the model’s generativeprocess.
