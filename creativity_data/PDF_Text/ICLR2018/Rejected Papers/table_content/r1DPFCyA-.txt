Table 1: Accuracy on 5-way classification on miniImageNet. Our best method, an isotropic Gaussianmodel using ResNet-34 features consistently outperforms all competing methods by a wide margin.
Table 2: Training classes for miniImageNet as proposed by Ravi & Larochelle (2017)15Under review as a conference paper at ICLR 2018n03075370	combination lockn02971356	cartonn03980874	ponchon02114548	white wolf, Arctic wolf, Canis lupus tundrarumn03535780	horizontal bar, high barn03584254	iPodn02981792	catamarann03417042	garbage truck, dustcartn03770439	miniskirt, minin02091244	Ibizan hound, Ibizan Podencon02174001	rhinoceros beetlen09256479	coral reefn02950826	cannonn01855672	goosen02138441	meerkat, mierkatn03773504	missilesTable 3: Validation classes for miniImageNet as proposed by Ravi & Larochelle (2017)
Table 3: Validation classes for miniImageNet as proposed by Ravi & Larochelle (2017)n02116738n02110063n02443484n03146219n03775546n03544143n04149813n03127925n04418357n02099601n02219486n03272010n04146614n02129165n04522168n07613480n02871525n01981276n02110341
Table 4: Test classes for miniImageNet as proposed by Ravi & Larochelle (2017)16Under review as a conference paper at ICLR 2018ResNet-34 inspired for miniImageNet	Output size	Layers84 × 84 × 3	Input patch42 × 42 × 32	5 × 5, 32, stride 2	3 × 3, 3242 × 42 × 32	3 × 3, 32 × 3	3 × 3, 6421 × 21 × 64	×4	3 × 3, 64	3 × 3, 12811 × 11 × 128	×6	3 × 3, 128	3 × 3, 2566 × 6 × 256	3 X 3, 256 X 3256	global average pooling〜 C	fully connected, softmaxTable 5: Network architecture. All unnamed layers are 2D convolutions with stated kernel size and
Table 5: Network architecture. All unnamed layers are 2D convolutions with stated kernel size andpadding SAME; the output of the shaded layer corresponds to Φφ(u), the feature space representa-tion of the image u, which is used as input for probabilistic k-shot learning.
Table 6: Network architectures. All 2D convolutions have kernel size 3 X 3 and padding SAME;max-pooling is performed with stride 2. The output of the shaded layer corresponds to Φφ(u), thefeature space representation of the image u, which is used as input for probabilistic k-shot learningThe network architecture was inspired by the VGG networks Simonyan & Zisserman, 2014, but doesnot employ batch normalisation Ioffe & Szegedy, 2015. To speed up training, we employ exponentiallinear units (ELUs), which have been reported to lead to faster convergence as compared to ordinaryReLUs Clevert et al., 2015. To regularise the networks, we employ dropout (Srivastava, Hinton, et al.,2014) and regularisation of the weights in the fully connected layers. The networks are trained withthe ADAM optimiser Kingma & Ba, 2014 with decaying learning rate.
Table 7: Description of the inference for the parameters of the prior in phase 2 (concept learning) forthe models in from Fig. 10. This specifies the inference procedure for θ in p(w | θ) after observing.1 . . . . . .
Table 8: Methods and inference procedure during phase 3 (k-shot learning) for the models usedin Fig. 10. This specifies the inference procedure used when computing p(W | D, Wf) for the specifiedprior distribution.
Table 9: Held-out log probabilities on random 70/10-splits of the training weights for the differentmodels on CIFAR-100. Values are averaged over 50 splits.
