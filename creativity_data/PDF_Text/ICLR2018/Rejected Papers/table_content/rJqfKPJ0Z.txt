Table 1: Classifiers accuracies on 1000 images before attacksIncV3,a	IncV4	InCReS_a	IncV3,b	IncRes _b96,1%	97,3%	99,7%	94,8%	97,5%The first experiment is to attack only one classifier and check the transferability to other classifiers.
Table 2: Results of attacking IncV3_b alone			IncV3,a	IncV4	IncRes _a	IncV3,b	InCReS_b△二	4.0	attacks success rate	^00%	0.0%	^00%	97.6%-	^00%		misclassification rate	"4.6%	3.5%	^03%	98.8%-	"3TT%△二	8.0	attacks success rate	^00%	0.0%	^00%	98.9%-	^00%		misclassification rate	"3ΓT%	3.6%	^03%	99.7%-	"3.9%△二	16.0	attacks success rate	^00%	0.0%	^00%	99.1%-	^00%		misclassification rate	7.0%	4.4%	0.6%	99.9%	5.5%As can be seen on Table 2, the transferability is inexistent whatever the maximum perturbation usedwhen looking at the targeted attacks success rate. However, a small increase in misclassification rateis noticed especially with IncV3_a, raising from 3.9% to 7%. This was verified when attacking anyother classifier alone and checking the impact on the others. This demonstrates clearly the need toattack many classifiers at once.
Table 3: Results of attacking all five classifiers at once			IncV3,a	IncV4	IncRes_a	IncV3,b	IncRes _b	Vote△=	二 4.0	attacks success rate	95.6%-	59.2%	30.6%-	92.5%-	32.2%-	83.0%		misclassification rate	97.2%-	76.0%	57.0%-	96.2%-	59.4%-	84.2%△=	二 8.0	attacks success rate	99.4%-	85.8%	66.5%-	94.4%-	53.5%-	96.9%		misclassification rate	99.5%-	94.6%	85.4%-	98.0%-	79.9%-	97.3%△=	二 16.0	attacks success rate	^T00%-	96.8%	92.5%-	95.2%-	66.6%-	99.8%		misclassification rate	100%	98.9%	98.3%	99.2%	86.8%	99.8%As we can see on Table 3, the success rate of the targeted attacks against the voting ensemble ishigh, around 80% for ∆ = 4.0 and approaching 100% for ∆ = 16.0. It is also interesting to noticethat the success rate of attacking IncV3_b has decreased compared to the case when it was attacked6Under review as a conference paper at ICLR 2018alone. With ∆ = 4.0 for instance, it went from 97.6% to 92,5%. This can be explained by the factthat the gradients are balanced in a way to change the input in a direction that minimizes all thelosses at the same time.
Table 4: Results of attacking IncV3_a defending using spatial smoothing		IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b∆ = 4.0	attacks success rate	95.4%-	0.0%	^00%	^00%	^00%	misclassification rate	99.7%	6.5%	2.0%	10.3%	6.1%As we can notice in Table 4, the success rate of targeted attacks and the miss-classification rate arenearly 100%. This means that the spatial smoothing based defense is not effective. Once again, thetransferability between models is inexistent.
Table 5: Results of attacking InceV3 _a with defense not using spatial smoothing		IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b∆ = 4.0	attacks success rate	^.2%	0.0%	^00%	^00%	^00%	misclassification rate	18.2%	3.6%	0.5%	6.4%	3.3%Then, how to overcome this issue for a more robust attack ?To answer this question, we consider a hybrid network where both filtered and non filtered inputsare used for optimization as represented on Figure. 5.
Table 6: Results of attacking IncV3_a with a hybrid loss functionNo Filter in defense			IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b△二	4.0	attacks success rate	99.9%-	0.0%	^00%	^00%	^00%		misclassification rate	99.9%-	3.8%	0.3%	"3.9%	2.9%3x3 Filter used in defense			IncV3,a	IncV4	InCReS_a	IncV3,b	IncRes_b△二	4.0	attacks success rate	98.4%-	0.0%	^00%	^00%	^00%		misclassification rate	98.7%	6.6%	2.7%	9.4%	6.3%Table 6 shows that the success rate is this time very high in both cases: 98.4% in filter based defenseand nearly 100% in no filter defense. We conclude that this attack is robust whether filtering is usedor not in defense.
Table 7: Results of attacking all five classifiers with a hybrid lossesNo Filter in defense			IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b	Vote△二	4.0	attacks success rate	93.7%-	59.0%	34.9%-	89.2%-	29.4%	78.9%		misclassification rate	96.2%-	76.5%	58.9%	94.7%-	54.6%	81.8%3x3 Filter used in defense			IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b	Vote-△=	4.0	attacks success rate	78.0%-	43.1%	26.3%-	30.6%-	10.3%	43.1%		misclassification rate	87.7%	67.4%	51.4%	52.5%	30.6%	52.9%Even with ∆ = 4.0 and considering only targeted attacks, the success and miss-classification ratesare high, around 50% when all classifiers use filters and much higher (around 80%) when no filtersare used. Other experiments we conducted showed that attacks rate for filter based defense can beimproved by assigning a greater weight b (twice the weight a) in the hybrid loss equation (5).
Table 8: Results of attacking JPEG encoding defense△ = 16.0			Q = 80	Q = 70	Q = 50	Q = 20Non targeted attacks		99.3%	96.6%	84.8%	43.4%Targeted attacks	Success rate	20.4%	-3.6%-	^02%-	^00%-	MiS-CIaSSifiCation rate	60.5%	44.7%	33.3%	24.1%Table 8 shows that CIA is robust when performing non targeted attacks as almost 100% of them aresuccessful with Q = 80 and around 50% with Q = 20. The targeted attacks are less successful withthe highest score of 20% when Q = 80 and 0% when Q = 20.
Table 9: Results of attacking JPEG encoding defense using approximation∆ = 16.0			Q = 80	Q = 70	Q = 50	Q = 20Non targeted attacks		99.2%	97.2%	83.5%	44.9%Targeted attacks	Success rate	20.5%	-3.0%-	^03%-	^00%-	Mis-classification rate	59.9%	44.3%	34.8%	23.2%As can be noticed, the results are almost the same as those of Table 8. This result is a bit untriguingas if all the filters used in the approximation are inexistent. However, a similar remark as in the filter-based defense case can be made. The attacks crafted using JPEG encoding are no longer attacks ifJPEG is not used by the defense. This means that the considered approximation of JPEG encodingis not that bad but not enough accurate to give strong attacks. It has obviously to be improved. Weare working on it.
Table 10: Results of attacking all classifiers but IncV4∆ = 16.0	IncV3,a	IncV4	InCReS_a	IncV3,b	InCReS_b	Voteattacks success rate	99.9%	34.0%	98.3%	99.9%	98.2%	96.9%4 ConclusionIn this paper we presented anew strategy called CIA for crafting adversarial examples while insuringthe maximum perturbation added to the original data to be smaller than a fixed threshold. Wedemonstrated also its robustness against some defenses, feature squeezing, ensemble defenses andeven JPEG encoding. For future work, it would be interesting to investigate the transferability of CIAattacks to the physical world as itis shown in [14] that only a very limited amount of FGDM attacks,around 20%, survive this transfer. Another interesting perspective is to consider partial craftingattacks while selecting regions taking into account the content of the data. With regard to images forinstance, this would be interesting to hide attacks with big but imperceptible perturbations.
