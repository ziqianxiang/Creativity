Table 1: Number of Parameters to Embed Vocabulary	The Wheel of Time	Gutenbergspelling	12, 304, 800	13, 020,000token	13, 997, 600	82,170, 800The number of parameters required to embed the entire vocabulary was controlled in order to preventspelling embeddings from gaining an unfair advantage over tokens. This was accomplished bylimiting the number of nodes in each layer. One of the main benefits of spelling embeddings is thatthe number of parameters does not grow necessarily with the vocabulary size as it does with tokenembeddings. The number of parameters needed to embed the vocabulary using token embeddings iscomputed by (V × Nt) + (Nt × M). The dominant term is generally the vocabulary size, V , whichis much larger than the embedding dimension.
Table 2: Final Per-Word Perplexity	The Wheel of Time	Gutenbergspelling + dropout	81.81 土 2.79	93.26spelling	75.95 土 0.75	84.19position agnostic chars	77.35 土 0.75	94.12(10epochs)token + dropout	77.09 土 0.65	86.86token	80.70 土 0.36	88.23spelling embeddings are far more sparse than those of token embedding. Raw token embeddingsexhibit the least amount of sparsity.
