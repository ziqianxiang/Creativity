Table 1: The performance of simplified LSTM architectures on language modeling benchmarks,measured by perplexity.
Table 2: The performance of simplified LSTM architectures on the question answering benchmark,SQuAD, measured by exact match (EM) and span overlap (F1).
Table 3: The performance of simplified LSTM architectures on the universal dependencies parsingbenchmark, measured by unlabeled attachment score (UAS) and labeled attachment score (LAS).
Table 4: The performance of simplified LSTM architectures on the WMT 2016 multi-modal Englishto German translation benchmark, measured by BLEU.
Table 5: Visualization of the weights on context words learned by the memory cell. Each columnrepresents the current word t, and each row represents a context word j . The gating mechanismimplicitly computes element-wise weighted sums over each column. The darkness of each squareindicates the L2-norm of the vector weights wjt from Equation 11. Figures on the left show weightslearned by a language model. Figures on the right show weights learned by a dependency parser.
