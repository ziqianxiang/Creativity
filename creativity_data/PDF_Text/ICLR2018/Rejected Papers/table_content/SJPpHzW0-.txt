Table 1: Comparison of the influence-directed explanations proposed here to prior related workincluding Integrated Gradients (Sundararajan et al., 2017), Sensitivity Analysis (Simonyan et al.,2014), Deconvolution (Zeiler & Fergus, 2014), Layer-wise Relevance Propagation Bach et al. (2015),and Simple Taylor Decomposition (Bach et al., 2015). The first three columns refer to capabilities ofthe corresponding explanation framework: Quantity refers to flexibility in the choice of quantity to beexplained; Distribution refers to flexibility in the distribution of instances to be explained; Internalrefers to the ability to produce explanations that characterize the role of internal neurons. The latterthree columns describe properties of the underlying influence measure used to build explanations:Faithfulness refers to distributional faithfulness; Sensitivity requires that if two instances differin one feature and yield different predictions, then that feature is assigned non-zero influence;Completeness requires that the aggregate difference between influence on two instances sums tothe difference of their outputs. See Sundararajan et al. (2017) for a more detailed discussion ofSensitivity and Completeness. Cells marked X denote that the cited explanation framework has thecorresponding feature, whereas those marked X* denote that the framework may have the featureunder certain parameterizations.
