Table 1: Comparison of previous deep-learning studies for TFBS and three closely related deeplearning papers in the recent literature. The columns indicate properties: (1) whether the study hasa joint deep architecture for multi-label prediction or not, (2) if the study learns prototype features(”motifs” in the TFBS literature), (3) whether the study models how input samples match prototypes,(4) if it uses RNN to model high-order combinations of labels, and finally (5) if the method considerscurrent sample inputs for modeling label combinations. All previous TFBS studies do not modellabel interactions. PMN combines several key strategies from deep learning literature including: (a)learning label-specific prototype embedding (Snell et al., 2017) through prototype-matching loss, (b)using RNN to model higher-order label combinations (Wang et al., 2016), and (c) using LSTM tomodel such combinations dynamically (conditioned on the current input) (Vinyals et al., 2016). PMNis the only model that exhibits all desirable properties.
Table 2: Comparison to similar matching models	Task	Output	Comparison	Support SetVinyals et al. 2015	Few shot	Single task	Cosine Similarity^^	Individual support set imagesSnell et al. 2017	Few shot	Single task	Squared Euclidean	Mean of each class from support setOurs	Large-scale	Multi-task	Cosine Similarity	Learned prototype for each classPrototype Features and Prototypical Networks: While standard deep learning architectures haveproven to work in many tasks, most operate under the feature-matching theory of pattern recognitionwhere an input is decomposed into a set of features, and then are compared with those stored in thememory (Krotov & Hopfield, 2016). Prototype theory, on the other hand, proposes that objects arerecognized as a whole and prototypes do not necessarily match the object precisely (Wallis et al.,2008). In this sense, the prototypes are blurred abstract representations which include all of theobject’s features. Krotov & Hopfield (2016) show that pattern recognition is likely a combinationof both feature-matching and prototype-matching. Transcription factors bind to motifs on DNAsequences, which we view as prototypes, the blurred features are constructed from a CNN (feature-matching). Our method is motivated by the prototype-matching theory, where instead of searchingfor exact features to match against, the model tests an unseen sample against a set of prototypes usinga defined similarity metric to make a classification.
Table 3: Dataset Overall Summary. In each split, about 40% of its samples have more than 1 TFbinding (i.e. a combination of TFs binding together), and each sample has an average of about 5.7TFs binding.
Table 4: TFBS Prediction Across 86 TFs in GM12878 Cell Line. We compare our PMN model totwo CNN baseline models (single-label and multi-label). We use the same CNN model and extend itusing prototypes and the combinationLSTM to create our PMN model. λ represents the weighting ofthe prototype loss in eq. 10. Results are shown using statistics across all 86 TFs, where our PMNmodel outperforms the CNN models based on all 3 metrics used. The PMN also outperforms bothCNN models significantly using a pairwise t-test.
Table 5: Column “TF-A” represents a TF inone of the clusters obtained from hierarchi-cal clustering. The subsequent column con-tains TFs that belong to the same cluster and,according to TRRUST database (Han et al.,2015), share target genes with TF-A. Remain-ing columns show the number of overlappingtarget genes between each TF and TF-A pairas well as their p-values for TF-TF coopera-tivity obtained from TRRUST. An interestingthing to note here is that in Cluster 1 and 2,TF that is away from TF-A in the cluster haslower p-value than the TF that is closer.
Table 6: MNIST using 3-Layer CNNModel	Accuracy-CNN	99.37PMN (λ =	0)	9943PMN (λ =	0.5)	99.39PMN (λ =	1)	99.50Figure 4: MNIST Per-Epoch Accuracy (Left: Train, Right: Test)Figure 5: 2d T-SNE Embedding of final output vector before classification. Left: CNN, Right: PMN1516Figure 8 Hierarchic-dusCenng Of prototypes Of 86 TF∙NRF1-----,EZH2-----ΓKAΓ2BP0LR3A——NFYA-----,TP53----r-ZZZ3---1NR2C2---1BRD4----1---
