Table 1: Average costs at different sampling temperatures Taverage costs of the solver = 24.76		Sampling Tempera- ture T	Average Costs	No of Instances with a better solution06	51345	-3751.0	42.938	5461.2	43.056	5651.5	36.646	544	22		29.417	503	Table 2: Average costs for the untrained modelaverage costs of the solver = 24.76	Sampling Tempera- ture T	Average Costs10	1.4211.2	3.8411.5	0.517	22		-2.574	5.3	ResultsWe define a metric to test the quality of solutions produced by the model, we define average costsover N Binary LP instances:PN Cost(i)Average Costs =	n~~—	(9)
Table 2: Average costs for the untrained modelaverage costs of the solver = 24.76	Sampling Tempera- ture T	Average Costs10	1.4211.2	3.8411.5	0.517	22		-2.574	5.3	ResultsWe define a metric to test the quality of solutions produced by the model, we define average costsover N Binary LP instances:PN Cost(i)Average Costs =	n~~—	(9)All the instances in training data and testing data are maximization problems, so the higher theaverage costs the better. We sample only feasible solutions using the output probability distributionwith temperatures [0.6, 1.0, 1.2, 1.5, 2.2].
Table 3: Objective values generated by baseline solver and object values calculated using the sam-pled variables assignments from LTMN output probability distribution at different sampling temper-atures for large Binary LP instances.
Table 4: Average costs (memory is reset between examples)average costs of the solver = 24.76	Sampling Tempera- ture T	Average Costs06	46.8961.0	42.5261.2	39.7271.5	35.621	22		28.063	we reset the memory each time a new example is processed. Table 4 shows that the average costs isslightly dropped when the memory is reset between examples.
Table 5: Memory faults, memory trues and memory equalsMetric Value PercentageMemory faults	66	≈36.4%Memory trues	87	≈48.0%Memory equals	28	≈15.4%6 conclusionThis paper introduced a long term memory coupled with a neural network, that is able to memorizeuseful input features to solve similar instances. We applied LTMN model to solve Binary LP in-stances. The LTMN was able to learn from supervised targets provided by a handcrafted solver, andgenerate better solutions than the solver. The LTMN model was able to generalize to more complexinstances beyond those in the training set.
