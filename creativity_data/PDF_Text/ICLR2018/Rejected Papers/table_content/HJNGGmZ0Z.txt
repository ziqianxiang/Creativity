Table 1: Results on the MSCOCO test split, where we vary only the image representation and keepother parameters constant. The captions are generated with beam = 1. We report BLEU (1-4),Meteor, CIDEr and SPICE scores.
Table 2: Performance of compressed Pool5 representationsTable 3: Performance of models on Flickr30kThe most surprising result is the performance of the pseudo-random vectors. We notice that both thepseudo-random binary and the pseudo-random count based vectors perform almost as good as theGold objects. This suggests that the conditioned RNN is able to remove noise and learn some sortof a common ‘visual-linguistic’ semantic subspace.
Table 3: Performance of models on Flickr30kThe most surprising result is the performance of the pseudo-random vectors. We notice that both thepseudo-random binary and the pseudo-random count based vectors perform almost as good as theGold objects. This suggests that the conditioned RNN is able to remove noise and learn some sortof a common ‘visual-linguistic’ semantic subspace.
Table 4: Unique captions with beam = 1.	Table 5: k-nearest neighbor experimentTo perform this experiment, we begin by generating captions for every training image using thebag of objects model (with frequency counts). We then compute the k-nearest training imagesfor each given test image using both the bag of objects representation and its projection (Eq. 2).
