Table 1: Average Reward (sentence-level BLEU) and corpus-level BLEU (standard evaluationmetric) scores for image captioning task with different τ .
Table 2: Token accuracy and official F1 for NER.
Table 3: UAS scores for dependency parsing.
Table 4: Sentence-level BLEU (S-B, training reward) and corpus-level BLEU (C-B, standard evalua-tion metric) scores for RAML with different τ .
Table 5: Comparison of our proposed approach with previous works. All previous methods requirepre-training using an ML baseline, while RAML learns from scratch.
Table 6: Performance of ML and RAML under different metrics for the three tasks on test sets. E.M.
Table 7: Dataset statistics. #Sent and #Token refer to the number of sentences and tokens in each dataset, respectively (for IWSLT, they refer to the number of sentence pairs and tokens of source/targetlanguages).
Table 8: Corpus-level BLEU score of RAML using importance samplingMethod		BLEUML BaSelinl		27.66τ	二 0.60	27.96τ	0.65	27.94τ	0.70	28.18τ	0.75	27.96τ	0.80	27.93τ	0.85	27.97τ	0.90	28.39τ	0.95	28.30τ	1.00	28.32τ	二 1.05	27.92Impt. Sample		28.61N -GRAM		28.77Table 9: Corpus-level BLEU score of RAML using negative Hamming distance as the reward functionwhere the context vector ct is a weighted sum of the source encodings {hi } via attention (Bahdanauet al., 2015). The probability of the target word yt is then given byp(yt∣y<t,x) = SOftmax(WsSt).
Table 9: Corpus-level BLEU score of RAML using negative Hamming distance as the reward functionwhere the context vector ct is a weighted sum of the source encodings {hi } via attention (Bahdanauet al., 2015). The probability of the target word yt is then given byp(yt∣y<t,x) = SOftmax(WsSt).
