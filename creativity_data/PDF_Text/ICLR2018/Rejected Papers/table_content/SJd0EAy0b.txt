Table 1: Link prediction results on WN18, FB15K and WN18RR, FB15K-237 (symbols: ? denotesthe value is cited from the original source, f denotes the result comes from (Dettmers et al., 2017))Datasets	WN18			WN18RR			FB15K			FB15K-237		Measures	P@1	P@3	P@10	P@1	P@3	P@10	P@1	P@3	P@10	P@1	P@3	P@10TransE	745^^	85.9	93.8	~2∏^^	33.1	42.7	36.1	59.0	76.2	17.6	29.6	44.6TransH	33.7	79.3	87.4	1.9	33.7	40.4	33.0	59.1	70.7	19.3	34.0	44.7HoIE	93.0 ?	94.5?	94.9?	35.6	37.8	39.3	40.2 ?	61.3?	73.9?	8.2	15.2	26.1Analogy	93.9 ?	94.4?	94.7?	37.9	39.2	41.0	64.6 ?	78.5?	85.4?	13.2	22.8	37.2DistMult	72.8 ?	91.4?	93.6?	38.9 *	43.9*	49.1*	54.6 ?	73.3?	82.4?	15.5 *	26.3*	41.9*ComplEX	93.6 ?	94.5?	94.7?	41.1t	45.8*	50.7*	59.9 ?	75.9?	84.0?	15.2 *	26.3*	41.9*ConvE	93.5*	94.7*	95.5*	30.6 *	36.0*	41.1*	67.0 *	80.1*	87.3*	22.0 *	33.0*	45.8*ProjE	75.7	87.8	95.1	31.8	41.7	46.0	57.5	66.32	88.4?	17.3	28.0	43.0GEN(avg.)	64.2	91.8	94.1	37.8	40.2	43.0	76.4	84.1	88.8	20.4	31.3	45.8GEN(opt)	90.6	94.1	94.5	38.3	40.5	43.1	77.7	84.7	89.0	20.8	32.1	46.2GEN(tail)	65.0	91.8	94.2	39.0	41.7	44.5	78.9	86.9	91.6	29.5	42.3	57.7model named GEN(tail). Besides, we found that our model tends to remember the reverse factswith regard to the triples that has been processed during the training phase. We argue that this isan inherent characteristic of our modeling methodology, since it would treat such reverse facts asconceptually correct. Therefore, we also report P@N scores after screening out such reverse facts,this model is named as GEN(opt). We consider that under certain practical circumstances, it is
Table 2: Empirical comparison of the embedding schemes on FB15K datasetTasks	Head Entity Prediction		Tail Entity Prediction		Relation Prediction	Models \ Measures	P@1	P@10	P@1	P@10	P@1	P@10GEN(GloVe)	-39.79	68.80	-44.64~~	74.72	-85.24^^	98.57GEN(word2vec)	48.05	75.81	52.09	81.34	86.50	98.77GEN(HoIE)	30.55	58.86	35.66	64.84	92.28	99.68GEN(TransE)	47.91	77.58	52.25	82.75	93.15	99.71GEN	73.85	86.01	78.86	91.64	93.99	99.75GEN(h, r ⇒ t)	36.18	62.88	36.85	63.38	-86.61 ~~	98.49GEN(t, r ⇒ h)	32.47	58.11	40.40	67.72	86.44	98.41GEN(h, t ⇒ r)	26.34	49.42	30.11	54.41	94.11	99.75beddings can be enhanced. In considering the ratio of the number of facts to relations involved, thisproblem seems much easier than the link prediction problem. (2) The validity of the multi-shot fra-mework has been verified, since each of the one-shot GEN model performs significantly worse thanthe multi-shot model for almost all the tests, except that in relation prediction tasks, GEN(h, t ⇒ r)performs comparable to GEN, this is probably because that it was exclusively trained for that task,which is prone to overfit the data. (3) Comparing with their performance on link prediction tasks, weargue that the embeddings generated by GEN are probably more representative and informative thanother embedding schemes, which we will provide more empirical (visual) evidence in Appendix C.
Table 3: Multi-label classification results on BlogCatalog datasetMeasures	Models	10%	20%	30%	40%	50%	60%	70%	80%	90%	DeepWalk	36.00	38.20	39.60	40.30	41.00	41.30	41.50	41.50	42.00	Node2veC	34.64	36.15	36.63	37.01	37.20	37.38	38.05	38.27	40.91MiCro-FI	TransE	16.71	17.10	17.44	17.64	17.77	18.50	19.13	19.62	20.50	HoIE	30.88	33.31	34.63	35.70	36.17	37.31	40.21	38.79	40.69	GEN	27.61	31.38	35.02	38.55	41.19	44.40	45.78	48.87	51.84	DeepWalk	21.30	23.80	25.30	26.30	27.30	27.60	27.90	28.20	28.90	Node2veC	16.52	18.81	19.81	20.09	20.97	21.50	22.37	23.16	24.60MaCro-FI	TransE	2.69	3.09	3.33	3.52	3.41	3.85	4.14	4.63	5.33	HoIE	13.86	17.10	18.98	20.84	20.77	22.65	25.64	23.06	27.79	GEN	19.32	23.26	26.74	31.06	33.53	36.57	38.83	40.27	44.60Table 4: Multi-label classification results on PPI datasetMeasures	Models	10%	20%	30%	40%	50%	60%	70%	80%	90%	DeepWalk	15.36	17.40	18.26	19.41	19.75	20.23	20.46	21.52	21.79	Node2veC	16.32	17.94	19.14	19.68	20.32	21.80	21.76	22.50	22.88MiCro-FI	TransE	12.80	17.69	20.94	23.57	24.58	27.32	30.42	31.84	35.20	HoIE	14.85	18.95	21.52	24.58	27.55	29.34	31.03	33.56	35.71	GEN	16.36	27.31	27.97	32.73	38.10	42.85	46.43	51.09	55.16	DeepWalk	12.93	14.46	15.94	17.05	17.74	18.05	18.41	18.52	20.03
Table 4: Multi-label classification results on PPI datasetMeasures	Models	10%	20%	30%	40%	50%	60%	70%	80%	90%	DeepWalk	15.36	17.40	18.26	19.41	19.75	20.23	20.46	21.52	21.79	Node2veC	16.32	17.94	19.14	19.68	20.32	21.80	21.76	22.50	22.88MiCro-FI	TransE	12.80	17.69	20.94	23.57	24.58	27.32	30.42	31.84	35.20	HoIE	14.85	18.95	21.52	24.58	27.55	29.34	31.03	33.56	35.71	GEN	16.36	27.31	27.97	32.73	38.10	42.85	46.43	51.09	55.16	DeepWalk	12.93	14.46	15.94	17.05	17.74	18.05	18.41	18.52	20.03	Node2veC	13.00	15.56	16.82	17.28	17.92	18.37	19.60	20.72	21.28MaCro-FI	TransE	8.71	11.45	16.43	19.00	20.37	22.69	25.42	27.35	30.53	HoIE	9.36	16.10	17.55	20.76	23.96	24.92	26.82	30.26	32.45	GEN	14.74	25.83	27.04	31.27	35.98	40.82	45.02	50.35	52.92and 4 demonstrate that, comparing with other embedding schemes, GEN performs more stable (andbetter) in both common and rare categories, which indicates that the embeddings generated by GENare probably more representative and informative than other solutions, thus the supervised modelbuilt on top of it is less vulnerable to global under-fitting and local over-fitting.
Table 5: Statistics of the data setsDataset	# entities	# relations	# training set	# validation set	# test setWN18	40,943	18	141,442	5,000	5,000WN18RR	40,943	11	86,835	3,034	3,134FB15K	14,951	1,345	483,142	50,000	59,071FB15K-327	14,541	237	272,115	17,535	20,466Dataset	# nodes	# edges	# categories	# labels	Avg. labelsBlogCatalog	10,312	333,983	39	14,476	1.40PPI	3,890	38,292	50	6,640	1.70The intuition behind is, for any given fact (h, r, t), one would instantly recognize the bidirectionalsemantic connection between h and t, without need of translating it into (t, r0, h) explicitly in his/hermind. We believe this is crucial for efficient utilization of the structure information of the KGs forrepresentation learning, empirical evidence is provided in Section 4 and Appendix C, respectively.
