Table 1: Penalty loss functions of regularizers	Penalty loss function		Derivatives		ωLI-Weight	= X X |wki | ki		d^^L1-weiqht	. /	、 ——-———=Sign (Wki) ∂wki		CL2-weight	= X X wk2i ki		d、L2-weight	Z —————=2Wki ∂wki		CL1-rep	= X X |hi,n|		d^^L1-rep ∂hi,n	= sign(hi,n)	Ωvr	= X vi i		∂Ωvr = ∂hi,n	2 N (hi,n - μi)	ωCR	=XX(ci,j)2-X ij	i	(vi)2	∂Ωcr = ∂hi,n	N X Cij (hj,n - μj ) j6=i	ωCw-V R	= XX vim mi		∂Ωcw-VR	2 =(hi,n - μi ), n	∈ Sm			∂hi,n		ωCw-CR	=X(XX(ci,j)2- mij	X(vi)2) i	∂Ωcw-CR ∂hi,n	=∣Sm∣ X cm- (hj,n- m j 6=i	μm),n ∈ Sm∂ Ωv RCR, it can be observed that they have similar structures. If VR S derivative -.....becomes zero∂ hi,n「	∂ . T …，F . ..	∂Ωcr 1	„	F	,………for all i, then CR S derivative -----becomes zero as well. The ViCe versa does not hold, but the∂ hi,neffects of VR and CR can be expected to be similar or at least related to each other for the learningprocess. In the same way, the relationship between cw-VR and cw-CR is the same as the relationshipbetween VR and CR. Therefore, we can expect cw-VR and cw-CR to have similar effects, too. Onthe other hand, the derivative of L1-rep has a distinct formulation, and it can be expected to have a
Table 3: Error performance of representation regularizers (MNIST)Layer	All classes			Class-wise		L1-rep	VR	CR	cw-VR	cw-CROutput	2.61±0.04	2.67±0.15	2.62±0.07^^	2.56±0.02	2.55±0.08Layer 5	2.61±0.11	2.70±0.03	2.67±0.04	2.63±0.05	2.61±0.06Layer 4	2.75±0.05	2.89±0.11	2.69±0.13	2.67±0.12	2.71±0.04Layer 3	3.35±0.08	3.16±0.09	3.11±0.13	3.22±0.06	3.22±0.06Layer 2	3.40±0.11	3.15±0.21	3.01±0.10	3.14±0.10	3.24±0.11Layer 1	4.31±0.14	2.98±0.09	3.13±0.09	3.25±0.04	3.14±0.03Table 4: Error performance of representation regularizers - multiple layers (MNIST)	L1-rep	VR	CR	cw-VR	cw-CROutput	2.61±0.04	2.67±0.15	2.62±0.07	2.56±0.02	2.55±0.08Output, 5	2.48±0.12	2.67±0.11	2.43±0.08	2.46±0.07	2.55±0.10Output, 5, 4	2.78±0.11	2.58±0.06	2.80±0.12	2.53±0.07	2.48±0.07Output, 5, 4, 3	2.79±0.10	2.78±0.08	2.83±0.14	2.80±0.10	2.72±0.04Output, 5, 4, 3, 2	3.19±0.10	2.91±0.13	2.77±0.07	2.90±0.10	2.75±0.07All	3.26±0.09	2.86±0.07	2.80±0.08	2.83±0.07	2.85±0.124.1	Performance resultsFor each regularization term, the level of regularization was determined by tuning the penalty lossweight using a validation dataset and a grid search. Then, we trained each model five-times and
Table 4: Error performance of representation regularizers - multiple layers (MNIST)	L1-rep	VR	CR	cw-VR	cw-CROutput	2.61±0.04	2.67±0.15	2.62±0.07	2.56±0.02	2.55±0.08Output, 5	2.48±0.12	2.67±0.11	2.43±0.08	2.46±0.07	2.55±0.10Output, 5, 4	2.78±0.11	2.58±0.06	2.80±0.12	2.53±0.07	2.48±0.07Output, 5, 4, 3	2.79±0.10	2.78±0.08	2.83±0.14	2.80±0.10	2.72±0.04Output, 5, 4, 3, 2	3.19±0.10	2.91±0.13	2.77±0.07	2.90±0.10	2.75±0.07All	3.26±0.09	2.86±0.07	2.80±0.08	2.83±0.07	2.85±0.124.1	Performance resultsFor each regularization term, the level of regularization was determined by tuning the penalty lossweight using a validation dataset and a grid search. Then, we trained each model five-times andcalculated the test error performance as the average and one standard deviation over the five perfor-mance results. In Table 2 and Table 3, the results show that representation regularizers outperformthe popular regularizers and that the representation strategies perform better when applied to upperlayers of DNN. Interestingly, the best performance is achieved by applying representation regular-ization to the output layer as shown in Table 3. This might be because the regularizer directly affectsonly the regularizing layer and the layers below, or because manipulating statistical properties ismore effective for the higher layer representations that have stronger or codeword-like structures.
Table 5: Evaluation of statistical properties (layer 5) - popular strategiesMetric	Baseline	Penalty on weight		Implicit method			L1-weight	L2-weight	Dropout	BNCL1-weight (alI)	100.00	88.05	92.25	99.14	84.82ω L2-weight (all)	100.00	100.00	100.00	100.00	100.00ω L/1.-rep	100.00	115.28	110.26	36.11	16.94ωVR	100.00	113.97	109.58	61.18	27.69Ωcr	100.00	111.55	107.51	39.35	5.80ωCw-V R	100.00	114.08	109.68	72.91	50.50ωCw-CR	100.00	112.68	108.55	78.49	20.54Avg-Act-Class	5.24	554^	5.35	4.60	2.48Ratio-DeadJJ nit	14%	5%	9%	0%	1%Table 6: Evaluation of statistical properties (layer 5) - representation regularizersMetric	All classes			Class-wise		L1-rep	VR	CR	CW-VR	cw-CRCLI-weight (alI)	93.08	-^96.42	95.83	86.85	84.14CL2-weight (alI)	100.00	100.00	100.00	100.00	100.00CL1-rep	1.07	9Λ6Γ~	9.73	3.41	5.49ωVR	7.77	9.24	9.42	3.91	5.28Ωcr	0.33	0.64	0.63	0.15	0.27
Table 6: Evaluation of statistical properties (layer 5) - representation regularizersMetric	All classes			Class-wise		L1-rep	VR	CR	CW-VR	cw-CRCLI-weight (alI)	93.08	-^96.42	95.83	86.85	84.14CL2-weight (alI)	100.00	100.00	100.00	100.00	100.00CL1-rep	1.07	9Λ6Γ~	9.73	3.41	5.49ωVR	7.77	9.24	9.42	3.91	5.28Ωcr	0.33	0.64	0.63	0.15	0.27ω cw-VR	19.85	-^28.12	29.61	11.25	14.27ωCw-CR	3.69	6.79	7.15	1.66	2.37Avg_Actdass	0.23	51Γ~	5.38	4.14	5.29Ratio-DeadJJ nit	77%	9%	5%	23%	7%We can observe two distinct groups of regularizers by investigating Table 5 and Table 6. We canobserve that the representation regularizers have much smaller values for the representation metrics.
Table 7: Error performance of regularizers (CIFAR-100)Regularizer		Train error	Test errorBaseline	None	25.50	56.02Penalty on weight	LI-Weight L2-weight	18.16 33.75	55.99 54.93	Dropout (fc)	28.02	55.28Implicit method	Dropout (all)	79.28	80.08	BN (fc)	28.28	55.33	BN (all)	8.63	57.82	L1-rep	98.93	99.00	VR	27.02	53.66Single	CR	33.24	54.67	cw-VR	22.85	54.15Penalty on	cw-CR	27.84	53.78	VR + CR	13.88	54.68representation	VR + cw-VR	19.43	56.12	VR + cw-CR	28.53	54.94	CR + cw-VR	21.11	53.30Combination	CR + cw-CR	18.05	54.75	cw-VR + cw-CR	25.77	55.64	L1-rep + VR	98.93	99.00
Table 8: Error performance of regularizers on ResNet-32 (CIFAR-10)Model	He et al.	OursResNet-32	7.51	7.39ResNet-32 + L1-rep		7.27ResNet-32 + VR		7.22ResNet-32 + CR		7.27ResNet-32 + cw-VR		7.17ResNet-32 + CW-CR		7.216	ConclusionIn this work, we have investigated five different penalty regularizers for manipulating statisticalproperties of DNN representations. The regularizers were conceived by examining optimal code-words of well-known channel coding problems, and the three statistical properties of sparsity, vari-ance, and covariance were integrated into the regularizers along with the concept of class-wise reg-ularization. It was found that many statistical properties including cross-covariance, co-adaptation,per-class variance, average number of active class per-unit, and the ratio of dead units can be ma-nipulated. Each regularizer, however, tended to manipulate multiple properties at the same time,making it difficult to manipulate each property individually. While manipulation was shown to bepossible and helpful for improving the performance of all three DNN classification problems thatwere investigated, it is still unclear if any statistical property of representation is generally helpfulwhen strengthened. Due to the complicated nature of learning process where back-propagation af-
Table 9: Error performance of popular regularizers - applied to each layerLayer	Baseline	Penalty on weight		Implicit method			L1-weight	L2-weight	Dropout	BN-An-		2.90±0.08	2.96±0.09	4.08±0.06	2.69±0.06Output		3.02±0.15	2.96±0.06	2.99±0.18	2.97±0.08Layer 5		2.98±0.05	2.99±0.13	2.80±0.08	3.04±0.09Layer 4	3.06±0.15	2.98±0.08	2.98±0.09	2.67±0.05	2.84±0.15Layer 3		3.04±0.09	3.03±0.18	2.67±0.16	2.94±0.13Layer 2		2.91±0.05	2.76±0.16	2.70±0.08	2.84±0.16Layer 1		2.93±0.05	2.52±0.10	3.07±0.07	2.58±0.07B Evaluation of statistical propertiesTable 10: Evaluation of statistical properties (raw) - popular regularizersProperty	Baseline	Penalty on weight		Implicit method			L1-weight	L2-weight	Dropout	BNCL1-weight (alI)	9795.03	7504.60	8220.21	9461.52	9488.60CL2-weight (alI)	607.46	459.85	502.71	576.60	792.24ω L/1.-rep	3.24 × 106	3.25 X 106	3.25 × 106	1.14 × 106	6.27 × 105Ωvr	865.69	851.24	860.34	307.64	86.59Ωcr	58178.00	54803.10	55650.20	8551.84	255.31ωCw-V R	2398.03	2327.79	2377.46	610.58	265.46
Table 10: Evaluation of statistical properties (raw) - popular regularizersProperty	Baseline	Penalty on weight		Implicit method			L1-weight	L2-weight	Dropout	BNCL1-weight (alI)	9795.03	7504.60	8220.21	9461.52	9488.60CL2-weight (alI)	607.46	459.85	502.71	576.60	792.24ω L/1.-rep	3.24 × 106	3.25 X 106	3.25 × 106	1.14 × 106	6.27 × 105Ωvr	865.69	851.24	860.34	307.64	86.59Ωcr	58178.00	54803.10	55650.20	8551.84	255.31ωCw-V R	2398.03	2327.79	2377.46	610.58	265.46ω Cw-CR	63726.20	58891.60	60610.20	21795.60	193.26Table 11: Evaluation of statistical properties (raw) - representation regularizersProperty	All classes			Class-wise		L1-rep	VR	CR	cw-VR	cw-CRCLI-weight (alI)	9975.62	9732.16	9826.06	9843.84	9772.89CL2-weight (alI)	727.22	645.03	665.57	813.20	853.98CL1-rep	38183.20	3.06 × 105	3.39 × 105	1.28 × 105	2.11 × 105ωVR	6.26	7.85	8.43	1.78	3.40Ωcr	0.80	2.55	2.55	0.19	0.64ω cw-VR	5.34	16.91	22.15	0.69	1.97ωCw-CR	0.17	1.53	2.00	8.83 × 10-3	0.04
Table 11: Evaluation of statistical properties (raw) - representation regularizersProperty	All classes			Class-wise		L1-rep	VR	CR	cw-VR	cw-CRCLI-weight (alI)	9975.62	9732.16	9826.06	9843.84	9772.89CL2-weight (alI)	727.22	645.03	665.57	813.20	853.98CL1-rep	38183.20	3.06 × 105	3.39 × 105	1.28 × 105	2.11 × 105ωVR	6.26	7.85	8.43	1.78	3.40Ωcr	0.80	2.55	2.55	0.19	0.64ω cw-VR	5.34	16.91	22.15	0.69	1.97ωCw-CR	0.17	1.53	2.00	8.83 × 10-3	0.0412Under review as a conference paper at ICLR 2018C MetricsActivated class and dead unitReLU’s output becomes positive when the input has a positive value. In this work, we say a classis activated for a nueron if the probability of the neuron’s output being positive is above a thresholdfor the given class. We use the entire test dataset to check the probability, and threshold value of 0.9is used for the evaluations. If many classes are activated for a neuron, it indicates that the neuron isused for representations of many classes. On the other hand, if only a single class is activated fora neuron, it indicates that the neuron is used for representations of only one class and kept zero for
