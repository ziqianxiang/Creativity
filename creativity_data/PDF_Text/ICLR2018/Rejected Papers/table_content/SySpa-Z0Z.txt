Table 1: Medium-model perplexity on Penn Treebank with different types of medium sized models.
Table 2: Full-model perplexity on Penn Treebank. Activation norm penalty outperforms other regu-larization methods and can improve the perplexity on the state of the art architecture.
Table 3: Single-model perplexity on Wikitext-2 with different types of models and regularizationmethods. We report the variational LSTM perplexity from Merity et al. (2016). When combining aregularization technique with data noising, we are still able to achieve a comparable performance tomodels that are more expressive.
Table 4: Wide-Residual network accuracy on CIFAR-10 and CIFAR-100.
Table 5: Validation perplexity on exploring the activation norm penalty techniqueThe result in Table 5 fits with our understanding, an LSTM unit can be considered as having 2layers of computation, first ct is computed, then ht is computed. Once we decide to compress thesource representation, in a neural network Markov chain X - h1 - h2 - ... - hn, by data processinginequality I(X; h1) ≥ I(X; hi>2), we cannot recover lost information in a later stage of processing.
Table 6: Exploring the difference between activation norm penalty, weight decay, and variationaldropout. (a). LSTM+WD refers to LSTM with weight decay at ηw = 10-2 ; (b). Weight decay isapplied to medium-sized LSTM with feedforward dropout on Penn TreebankModel		Validationηw	二 10-5	84.3ηw	10-4	84.2ηw	10-3	84.3ηw	二 10-2	83.5(b) Weight decay’s effect on perplexity in PTBFrom previous literature (Achille & Soatto, 2017) and our derivations, we know that weight decay,dropout, and activation norm penalty all reduce correlation between input and output. From Table6 (a), we can see that all three methods decrease the L2 norm of the output to various extents. Wealso show that in Table 6 (b), when applied to vanilla LSTM with feedforward dropout, weight decayand activation norm penalty have different effects on final task performance as well.
