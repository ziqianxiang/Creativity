Table 1: Experimental results on the WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks. Our proposed model outperforms the state-of-the-art modelsincluding the Transformer (Vaswani et al., 2017). The small model corresponds to configuration (A)in Table 2 while large corresponds to configuration (B).
Table 2: Experimental comparison between different variants of the Transformer (Vaswani et al.,2017) architecture and our proposed Weighted Transformer. Reported BLEU scores are evaluatedon the English-to-German translation development set, newstest2013.
Table 3: Model ablations of Weighted Transformer on the newstest2013 English-to-German taskfor configuration (C). This shows that the learning both (α, κ) and retaining the simplex constraintsare critical for its performance.
Table 4: Performance of the architecture with random and uniform normalization weights onthe newstest2013 English-to-German task for configuration (C). This shows that the learned (α, κ)weights of the Weighted Transformer are crucial to its performance.
