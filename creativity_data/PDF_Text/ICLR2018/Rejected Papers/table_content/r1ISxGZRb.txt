Table 1: Comparing the nearest training example L1 distance of code sampling and buffer samplingbased recollections. We report averages across 10,000 random samples. Reconstruction distortionof the autoencoder is measured on the test set and is not influenced by the sampling strategy.
Table 2: Generative knowledge distillation random sampling experiments with a CNN teacher andstudent model on MNIST.
Table 3: Generative knowledge distillation active and diverse sampling experiments with a CNNteacher and student model on MNIST. The real input baselines are randomly sampled.
Table 4: Lifelong learning retention results on incremental CIFAR-100 for low effective buffersizes with an incremental storage resource constraint.
Table 5: Lifelong learning retention results on incremental CIFAR-100 with a 200 lossless episodetotal storage resource constraint. GEM and iCaRL baselines are from (Lopez-Paz & Ranzato, 2017).
Table 6: This table provide more specifics about the discrete latent variable architectures involvedin Figure 1.
Table 7: This table provide more specifics about the continuous latent variable architectures in-volved in Figure 1.
Table 8: Generative knowledge distillation random sampling experiments with a CNN teacher andMLP student model on MNIST.
