Table 1: Accuracy on all CLEVR question types for baselines and competitive models. The Humanbaseline is from the original CLEVR work. * denotes additional program supervision. SA refers tostacked spatial attention Yang et al. (2015)rate and rough network size. The network overall has 9M parameters. We exclude the ResNetfeature extractor from all calculations because it is also present in the best FiLM model. Their workfurther demonstrated it is fairly straightforward to replace it with a from-scratch feature extractorwith minimal loss in accuracy.
Table 2: Architectural details of subnetworks in DDRprog as referenced in Figure 1 and Algorithm1. Finegrained layer details are provided in tables 4-8. Source will be released pending publication.
Table 3: Hyperparameter details for DDRprog. Only the learning rate and model size were coarselycross validated due to hardware limitations: hyperparameter are not optimal.
Table 4: ResNetFeaturizerIndex	Layer	Output Size(1)	Input Image	3 × 224 × 224(2)	ResNet101 conv4_6	1024 × 14 × 14(3)	Conv(3 × 3, 1024 → h)	h× 14× 14(4)	ReLU	h× 14× 14(5)	Conv(3 × 3, h → h)	h× 14× 14(6)	ReLU	h× 14× 1411Under review as a conference paper at ICLR 2018Table 5: Unary ModuleIndex	Layer	Output Size(1)	Previous Module Output	h×	14	× 14(2)	Conv(3 × 3, h → h)	h×	14	× 14(3)	ReLU	h×	14	× 14(4)	Conv(3 × 3, h → h)	h×	14	× 14(5)	Residual: Add (1) and (4)	h×	14	× 14(6)	ReLU	h×	14	× 14(7)	InstanceNorm	h×	14	× 14Table 6: Binary Module
Table 5: Unary ModuleIndex	Layer	Output Size(1)	Previous Module Output	h×	14	× 14(2)	Conv(3 × 3, h → h)	h×	14	× 14(3)	ReLU	h×	14	× 14(4)	Conv(3 × 3, h → h)	h×	14	× 14(5)	Residual: Add (1) and (4)	h×	14	× 14(6)	ReLU	h×	14	× 14(7)	InstanceNorm	h×	14	× 14Table 6: Binary ModuleIndex	Layer	Output Size(1)	Previous Module Output	h×14×14(2)	Previous Module Output	h×14×14(3)	Concatenate (1) and (2)	2h × 14 × 14(4)	Conv(1 × 1, 2h → h)	h×14×14(5)	ReLU	h×14×14(6)	Conv(3 × 3, h → h)	h×14×14(7)	ReLU	h×14×14(8)	Conv(3 × 3, h → h)	h×14×14(9)	Add (5) and (8)	h×14×14
Table 6: Binary ModuleIndex	Layer	Output Size(1)	Previous Module Output	h×14×14(2)	Previous Module Output	h×14×14(3)	Concatenate (1) and (2)	2h × 14 × 14(4)	Conv(1 × 1, 2h → h)	h×14×14(5)	ReLU	h×14×14(6)	Conv(3 × 3, h → h)	h×14×14(7)	ReLU	h×14×14(8)	Conv(3 × 3, h → h)	h×14×14(9)	Add (5) and (8)	h×14×14(10)	ReLU	h×14×14Table 7: Fork ModuleIndex	Layer	Output Size(1)	Previous Module Output	h× 14×14(2)	Previous Module Output	h× 14×14(3)	Concatenate (1) and (2)	2h × 14 × 14(4)	Conv(1 × 1, 2h →6h)	6h × 14 × 14(5)	ReLU	6h × 14 × 14(6)	Conv(3 × 3, 6h →6h)	6h × 14 × 14
Table 7: Fork ModuleIndex	Layer	Output Size(1)	Previous Module Output	h× 14×14(2)	Previous Module Output	h× 14×14(3)	Concatenate (1) and (2)	2h × 14 × 14(4)	Conv(1 × 1, 2h →6h)	6h × 14 × 14(5)	ReLU	6h × 14 × 14(6)	Conv(3 × 3, 6h →6h)	6h × 14 × 14(7)	ReLU	6h × 14 × 14(8)	Conv(3 × 3, 6h →6h)	6h × 14 × 14(9)	Add (5) and (8)	6h × 14 × 14(10)	ReLU	6h × 14 × 14(11)	Conv(1 × 1, 6h → h)	h× 14×14	Table 8: CNN	Index	Layer	Output Size(1)	Previous Module Output	h×14× 14(2)	Conv(3 × 3, h → h)	h×14× 14(3)	ReLU	h×14× 14(4)	Conv(3 × 3, h → h)	h×14× 14(5)	Residual: Add (1) and (4)	h×14× 14
Table 9: Success examples on CLEVR. The numerical prefix on each program function is its arity.
Table 10: Failure examples on CLEVR. The numerical prefix on each program function is its arity.
