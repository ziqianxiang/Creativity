Table 1: Citation Network DatasetExperimental setup. The accuracy of the baseline methods are all taken from existing literature. Ifa baseline result is not reported in the existing literature, we intentionally left those cells empty in thetable for fairness, as opposed to running those experiments ourselves on untuned hyperparameters.
Table 2: Classification accuracy with a fixed split of data from (Yang et al., 2016).
Table 3: Classification accuracy with random splits of the data.
Table 4: Classification accUracy with larger sets of labelled nodes.
Table 5: Fraction of edges from top 100 most relevant edges and bottom 100 least relevant edgeswhich are connecting two distinct nodes from the same class.
Table 6: Hyper-parameters for AGNN modelFor the Graph Linear Network (GLN) as define in (3), we use the same hyper-parameters as GCN(Kipf & Welling, 2016) for all the experimental settings: hidden dimension of 16, learning rate of0.01, weight decay of 5 Ã— 10-4, dropout of 0.5, 200 epochs and early stopping criteria with a windowsize of 10.
Table 7: Average testing error of AGNN for different number of Propagation Layers.
Table 8: Classification accuracy with random splits of the data.
Table 9: Classification accuracy with larger sets of labelled nodes.
