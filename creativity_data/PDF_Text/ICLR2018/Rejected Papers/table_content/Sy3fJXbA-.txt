Table 1: CIFAR-100 accuracies (single crop) achieved by different architectures trained using thepredefined full connectivity of ResNeXt (Fixed-Full) versus the connectivity learned by our algorithm(Learned). We also include models trained using random, fixed connectivity (Fixed-Random) definedby setting K = 4 random active connections per branch. Each model was trained 4 times, usingdifferent random initializations. For each model we report the best test performance as well as themean test performance computed from the 4 runs. For our method, we report performance usingK = 1 as well as K = 4. We also list the number of parameters used during training (Params-Train)and the number of parameters obtained after pruning the unused blocks (Params-Test). Our learnedconnectivity using K = 4 produces accuracy gains of up 2.2% compared to the strong ResNeXtmodel, while using K = 1 yields results equivalent to ResNeXt but it induces a significant reductionin number of parameters at test time (a saving of 40% for model {29,64,8}).
Table 2: ImageNet accuracies (single crop) achieved by different architectures using the predefinedconnectivity of ResNeXt (Fixed-Full) versus the connectivity learned by our algorithm (Learned).
Table 3: CIFAR-10 accuracies (single crop) achieved by different multi-branch architectures trainedusing the predefined connectivity of ResNeXt (Fixed-Full) versus the connectivity learned by ouralgorithm (Learned). Each model was trained 4 times, using different random initializations. Foreach model we report the best test performance as well as the mean test performance computed fromthe 4 runs.
Table 4: Mini-ImageNet accuracies achieved by different multi-branch networks trained using thepredefined full connectivity of ResNeXt (Fixed-Full) versus the connectivity learned by our algorithm(Learned). Additionally, we include models trained using random fixed connectivity (Fixed-Random)for K = 4. For each model we report the best and the mean test performance computed from 4different training runs. Our method for joint learning of weights and connectivity yields a gain ofover 3% in Top-1 accuracy over ResNeXt, which uses the same architectures but a fixed branchconnectivity.
Table 5: Specifications of the architectures used in our experiments on the CIFAR-10 and CIFAR-100datasets. The architectures differ in terms of depth (D), bottleneck width (w), and cardinality (C).
Table 6: Mini-ImageNet architectures with varying depth (D), and bottleneck width (w). Insidethe brackets we specify the residual block used in each multi-branch module by listing the numberof input channels, the size of the convolutional filters, as well as the number of filters (number ofoutput channels). To the right of each bracket we list the cardinality (C) (i.e., the number of parallelbranches in the module). Ã—2 means that the same multi-branch module is stacked twice.
