Table 1: StatiSticS of the data Setnum. samples	500k	max. turn length	19max. dialogue length	298	avg. turns	-15.3-num. db	-13-	book rate	55.34%cancel	1.18%	change	0.8%resv. not found	18%	flight not found	24.68%num. vocab	5,553	Training Set	150kEvaluation Set	25k	Inference Set	-25k-Self-Play Training Set	250k	Self-play Eval Set	-50k-2.2.1	Dialogue GenerationDifferent from the method we uSe during the SuperviSed training in Equation 3, here we generate thedialogue by Sampling from policy rather than taking the argmax from itS logitS.
Table 2: Features of User Restrictionsno.	1	2	3	4	5	6feat.	Dep. City	Ret. City	Dep. Month	Ret. Month	Dep. Day	Ret. DayVaL	cate.	cate.	1-12	—	1-12	-	1-31	1-31feature	Dep. Time	Ret. Time	Name	Flight class	Price	ConnectionVaL	mor/aft/eve/all	mor/aft/eve/all	cate.	eco/bus/all	500-5000	0/1/allfeature	Airline					val	norm/all					needs to learn to generalize between those concepts in order to succeed. To limit the amount ofdatabase data to be generated for each sample, we only generate 30 candidate flights for each dia-logue. Both user restrictions and database are generated according to a preset prior. For example,we limit the percentage of people who requires business class to be 1%. Those priors ensure that wecan have relatively high booking rate even when the number of flights in the database is small. Wehave observed that 55.34% of the user restrictions resulted in a book status while 24.68% of of themresults in a flight not found state. In addition to the flight database, we also generate a single entryindicating whether a user has reservation in the system.
Table 3: Features of Flight Databasesno.	1	2	3	4	5	6	7feat.	Dep. City	Ret. City	Dep. Month	Ret. Month	Dep. Day	Ret. Day	Dep. Timeval.	cate.	cate.	ΓΓ2	1-Γ2	-1-31-	1-31	00-24feature	Ret. Time	Flight class	Price	Connection	-Airline-	flight no.	val.	00-24	eco/bus	0-5000 一	0/1/2	cate.	cate.	6Under review as a conference paper at ICLR 2018Table 4: Experimental Results - Evaluations on Supervised TrainingExperiments	Eval ppl	Eval BLEU	Eval Reward	Eval name	Eval Flight	Eval ActionSupervised	1.214	33.6	1	—	0.948	1.00	1.00	-Table 5: Experimental Results - Evaluations on Self-playExperiments	self-play eval rewardSupervised	0.321SeIf-PIay —	0.441on the predicted dialogue. Those rules include things such as dialogue must contain end of dialoguetoken or dialogue must not have consecutive repeating tokens. If those language rule check fails,reward will be zero. These discourages the model from generating very back utterances that mightbenefit the performance in short term but will hurt in the long term. We then check if the predictedaction state is exactly the same as the ground truth state. If not the reward will be zero. In the final
Table 4: Experimental Results - Evaluations on Supervised TrainingExperiments	Eval ppl	Eval BLEU	Eval Reward	Eval name	Eval Flight	Eval ActionSupervised	1.214	33.6	1	—	0.948	1.00	1.00	-Table 5: Experimental Results - Evaluations on Self-playExperiments	self-play eval rewardSupervised	0.321SeIf-PIay —	0.441on the predicted dialogue. Those rules include things such as dialogue must contain end of dialoguetoken or dialogue must not have consecutive repeating tokens. If those language rule check fails,reward will be zero. These discourages the model from generating very back utterances that mightbenefit the performance in short term but will hurt in the long term. We then check if the predictedaction state is exactly the same as the ground truth state. If not the reward will be zero. In the finalcase, we check whether the name matches the correct action, which will counts for 50% of the finalreward. The other 50% goes to database recommendation checks, where we calculates a squarederror on each dimension of the flight and normalized the distance against the optimal one. Thissetup of generating rewards discourages wrong action state and invalid dialogues strongly, whichhelped to generate dialogues that are of high quality.
Table 5: Experimental Results - Evaluations on Self-playExperiments	self-play eval rewardSupervised	0.321SeIf-PIay —	0.441on the predicted dialogue. Those rules include things such as dialogue must contain end of dialoguetoken or dialogue must not have consecutive repeating tokens. If those language rule check fails,reward will be zero. These discourages the model from generating very back utterances that mightbenefit the performance in short term but will hurt in the long term. We then check if the predictedaction state is exactly the same as the ground truth state. If not the reward will be zero. In the finalcase, we check whether the name matches the correct action, which will counts for 50% of the finalreward. The other 50% goes to database recommendation checks, where we calculates a squarederror on each dimension of the flight and normalized the distance against the optimal one. Thissetup of generating rewards discourages wrong action state and invalid dialogues strongly, whichhelped to generate dialogues that are of high quality.
