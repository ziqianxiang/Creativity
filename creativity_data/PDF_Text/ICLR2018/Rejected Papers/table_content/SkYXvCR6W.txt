Table 1: Example of coding using English language ranking of characters. Characters shown are theones used in the subsequent examples.
Table 2: Example of coding using English language ranking of characters. Prefix common to morethan an word are underscoredTEXT ENCODED TEXTscience	IOOOOOIIOOOOOOOOOOOOIIOOOOIIOIIOOOOOOOI10000000000001101scientist	10000011000000000000110000110110000000110 0 0110 0 0 0110 0 0 0 0110 0 01art	1001100000000110001artist	1001100000000110001100001100000110001tl；dr	100011000000000110000000000000000000000000000011000000000011000000001u2	1000000000000001100000000000000000000000000000001ea	1011000000000000000000000000000000000000000000000000000000000011001In a matrix of number of words × code size representing the document, each row representsa properly encoded word, where the code is embedded with its first symbol in the first column.
Table 3: Architectures of the ‘large’ and ‘small’ CNNs used by Zhang et al. (2015).
Table 4: Training environment and parametersDESCRIPTION	PARAMETERS	OBSERVATIONSNeural Net Lib.	Keras 2.0	Tensor Backend	Theano 0.9	GPU Interface	Cuda 8	with cuBLAS Patch UpdateCNN optimizer	Nvidia Cudnn 5.1	Program. Lang.	Python 3.6	using Anaconda 4.4.0Superbatch	10000	Number of matrixes sent to gpu each timeMinibatch	32	Batch to update the network weightsOptimizer	ADAM (Zeiler, 2012)	lr = 10-3, β1 = 0.9, β2 = 0.999, = 10-8Epochs	5,12	Op. System	Windows 10	GPU	Nvidia GeForce 1080ti	RAM Memory	16GB	•	Bag-of-ngrams (Ngrams) and its TFIDF (Ngrams TFIDF): The bag-of-ngrams modelswere constructed by selecting the 500,000 most frequent n-grams (up to 5-grams) from thetraining subset for each dataset. The feature values were computed the same way as in thebag-of-words model (Zhang et al., 2015).
Table 5: Test set accuracy comparison among traditional models the Zhang et al. (2015) models‘large’ and ‘small’ and our approaches when applied to the AG’s news (AG), Sogou news (SOGOU),DBpedia (DBP), Yelp polarity (YLP-P), Yelp full (YLP), Yahoo! answers (YAH), Amazon full(AMZ) and Amazon polarity (AMZ-P) datasets.
Table 6: Time per epoch as reported by Zhang et al. (2015) models and the ones used by CNN1 andCNN2 on an NVidia GeForce 1080ti GPU.
