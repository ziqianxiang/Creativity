Table 1: MNIST test set NLL with generative models G1 and G2 (lower is better K is number ofConvBlocks)MNIST (static binarization)	- log p(x) on G1	- log p(x) on G2VAE (Burda et al., 2015)	87.88	85.65IWAE (IW = 50) (Burda et al., 2015)	86.10	84.04VAE+NF (Rezende & Mohamed, 2015)	-	≤ 85.10VAE+ (K =1)	87.56	85.53VAE+ (K= 4)	87.40	85.23VAE+ (K = 8)	87.28	85.07VAE+IAF (K =1)	88.50	86.00VAE+IAF (K = 2)	88.27	85.86VAE+IAF (K = 4)	88.03	85.95VAE+IAF (K = 8)	87.97	85.50VAE+ConvFlow (K =1)	86.91	85.45VAE+ConvFlow (K = 2)	86.40	85.37VAE+ConvFlow (K= 4)	84.78	81.64VAE+ConvFlow (K= 8)	83.89	81.21IWAE+ConvFlow (K = 8, IW = 50)	79.78	78.51subspace issue in IAF. Lastly, combining convolutional normalizing flows with multiple importanceweighted samples, as shown in last row of Table 1, further improvement on the test set log-likelihood
Table 2: OMNIGLOT test set NLL with generative models G1 and G2 (lower is better, K is numberof ConvBlocks)Omniglot	- log p(x) on G1	- log p(x) on G2VAE (Burda et al., 2015)	108.86	107.93IWAE (IW = 50)(Burda et al., 2015)	104.87	103.93VAE+ (K = 1)	108.80	107.89VAE+ (K = 4)	108.64	107.80VAE+ (K = 8)	108.53	107.67VAE+IAF (K =1)	109.44	108.74VAE+IAF (K = 2)	109.69	108.36VAE+IAF (K= 4)	109.47	107.61VAE+IAF (K = 8)	109.34	107.43VAE+ConvFlow (K = 1)	107.41	106.32VAE+ConvFlow (K = 2)	107.05	105.80VAE+ConvFlow (K = 4)	106.24	104.35VAE+ConvFlow (K = 8)	105.87	103.58IWAE+ConvFlow (K = 8,IW = 50)	104.21	103.02d 7-s-1 uʃ?6dG3 夕">y∙0cv9o64 g3 t∏ 5 l∖/
