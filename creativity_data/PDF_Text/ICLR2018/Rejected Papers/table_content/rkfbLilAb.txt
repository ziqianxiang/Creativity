Table 1: Probe intent actionsAction	Descriptionprobe use case probe to refine cluster categories	ask about where assets will be used ask the user to further refine query if less relevant search results are retrieved ask the user to select from categorical options related to her queryTable 2: General actionsAction	Descriptionshow results add to cart ask to download ask to purchase provide discount sign up ask for feedback provide help salutation	display results corresponding to most recent user query suggest user to bookmark assets for later reference suggest user to download some results if they suit her requirement advise the user to buy some paid assets offer special discounts to the user based on search history ask the user to create an account to receive updates regarding her search take feedback about the search so far list possible ways in which the agent can assist the user greet the user at the beginning; say goodbye when user concludes the searchThe set G consists of generic actions like displaying assets retrieved corresponding to the user query,providing help to the user etc. While probe intent actions are useful to gauge user intent, set G com-prises of actions for carrying out the functionality which the conventional search interface provideslike ”presenting search results”. We also include actions which promote the business use cases (suchas prompting the user to signup with her email, purchase assets etc). The agent is rewarded appro-priately for such prompts depending on the subsequent user actions. Our experiments show that theagent learns to perform different actions at appropriate time steps in search episodes.
Table 2: General actionsAction	Descriptionshow results add to cart ask to download ask to purchase provide discount sign up ask for feedback provide help salutation	display results corresponding to most recent user query suggest user to bookmark assets for later reference suggest user to download some results if they suit her requirement advise the user to buy some paid assets offer special discounts to the user based on search history ask the user to create an account to receive updates regarding her search take feedback about the search so far list possible ways in which the agent can assist the user greet the user at the beginning; say goodbye when user concludes the searchThe set G consists of generic actions like displaying assets retrieved corresponding to the user query,providing help to the user etc. While probe intent actions are useful to gauge user intent, set G com-prises of actions for carrying out the functionality which the conventional search interface provideslike ”presenting search results”. We also include actions which promote the business use cases (suchas prompting the user to signup with her email, purchase assets etc). The agent is rewarded appro-priately for such prompts depending on the subsequent user actions. Our experiments show that theagent learns to perform different actions at appropriate time steps in search episodes.
Table 3: Mapping between query logs and user actionsUser action	Mapping usednew query refine query request more click result add to cart cluster category click search similar	first query or most recent query with no intersection with previous ones query searched by user has some intersection with previous queries clicking on next set of results for same query user clicking on search results being shown when user adds some of searched assets to her cart for later reference when user clicks on filter options like orientation or size search assets with similar series, model etcThe virtual user is modeled as a finite state machine by extracting conditional probabilities - P(UserAction u| H istor y h of User Actions). These probabilities are employed for sampling next useraction given the fixed length history of her actions in an episode. The agent performs an action inresponse to the sampled user action. Subsequent to the action performed by the agent, next useraction is sampled which modifies the state and is used to determine the reward the agent gets for itsprevious action. Table 4 shows a snippet of conditional probability matrix of user actions given thehistory of last 3 user actions.
Table 4: Snippet of conditional probability matrix obtained from session data on query logsUser action User action history	P(User action / history)Click assets	More assets, click assets, similar click	0.41More assets	New qUery, refine qUery, add to cart	0.13Refine qUery	similar click, new qUery, new qUery	0.40The qUery and session log data has been taken from an asset search platform where the marketercan define certain offers/promotions which kick in when the User takes certain actions, for instancethe User can be prompted to add some images to cart (via a pop-Up box). Users response to sUchprompts on the search interface is Used as proxy to model the effect of RL agent on virtUal User’ssampled action sUbseqUent to different probe actions by the agent. This ensUres that oUr conditional6Under review as a conference paper at ICLR 2018probability distribution covers the whole probability space of user behavior. In order to incorporatethe effect of other agent actions such as sign up which are not present in the query logs, we tweakedthe probability distribution realistically in order to bootstrap the agent.
Table 5: Human Evaluation Ratings for Agent Trained Through A3CMetric	Average RatingInformation Flow	^.58Appropriateness	^.67Repetitiveness	^.50In addition to the above metrics, we also asked the designers to compare our system to conventionalsearch interface of the stock photography marketplace in terms of following metrics :1.	Engagement : This is to measure how interactive and engaging conversational search iscompared to conventional search on scale of 1 to 5 where 5 represents that conversationalsearch is much more engaging and 1 represents same engagement as conventional search.
