Table 1: Accuracy based comparison of the two different threshold selection methodsThreshold selection methods	Achievable Sparsity (% of Zero Weights)One fixed threshold	86.30 %Different dynamically selected thresholds	93.47 %	—While initializing the sparsity levels for each layer, we increased the above mentioned percentageby 5 to 6% following our algorithm. Based on these defined sparsity levels, the initial threshold hasbeen calculated for each layer. We prune each layer of the network with these threshold values. Aplot of the weight distribution of second convolutional layer before and after first pruning is shownin figure 5.
Table 2: Comparison of our pruning results on LeNet-5 with that of Han et al. (2015)Model	Layer	Percentage of remaining parameters [ours]	Number of remaining parameters [ours]	Percentage of remaining parameters (Han et al., 2015)	Number of remaining parameters (Han et al., 2015)LeNet-5	F2	5%	250	19%	950	^Tr1-	6%	-24000-	8%	32000	C2	15%	-3750-	12%	3000	C1	50%	90	66%	330Total		≈ 6.5% 一	28090	≈ 8%	36280	—Accuracy			99.27%				99.26 %		Storage Requirement		6 KB		44 KB	4.2 Results for FCN on cityscapesWe conducted second experiment on FCN (Yang et al., 2016a) performing semantic segmentationtask on the Cityscapes dataset. Our trained FCN8 has achieved the baseline IU of 64.75% andbaseline error rate of 35.25% on the validation set of cityscapes.
Table 3: Comparison of all the weight sharing techniques discussed aboveWeight sharing technique	Number of clusters found	Accuracy achievedMean shift	12	99.05%k-means with random initialization	8	98.94%k-means with linear initialization within layers	24	99.14%k-means with linear initialization across all the layers	15	99.27%8Under review as a conference paper at ICLR 2018Table 4: Accuracy statistics and reduction in size after each stage of pipelineStages of PiPleline	Storage requirement of parameters (in MB)	Reduction in storage requirement (in %)	AccuracyBaseline	1.7 MB	-	99.30%Pruning	0.11MB	93.52%	99.26%Pruning + Weight sharing	0.008 MB	99.50%	99.28%Pruning + Weight sharing + Pruned code book	0.007 MB	99.58%	99.27%Pruning + Weight sharing + Pruned codebook + Quantization	0.006 MB	99.59%	99.27%runs for 32 iterations for the whole network. Figure 7 depicts the convergence of error rate andaccuracy rate after each iteration at initialized sparsity level. Plot of the weight distribution of firstconvolutional layer before and after first pruning is shown in figure 8. Table 5 depicts the compres-sion statistics after pruning and table 6 depicts the mean IU statistics and reduction in storage afterpruning.
Table 4: Accuracy statistics and reduction in size after each stage of pipelineStages of PiPleline	Storage requirement of parameters (in MB)	Reduction in storage requirement (in %)	AccuracyBaseline	1.7 MB	-	99.30%Pruning	0.11MB	93.52%	99.26%Pruning + Weight sharing	0.008 MB	99.50%	99.28%Pruning + Weight sharing + Pruned code book	0.007 MB	99.58%	99.27%Pruning + Weight sharing + Pruned codebook + Quantization	0.006 MB	99.59%	99.27%runs for 32 iterations for the whole network. Figure 7 depicts the convergence of error rate andaccuracy rate after each iteration at initialized sparsity level. Plot of the weight distribution of firstconvolutional layer before and after first pruning is shown in figure 8. Table 5 depicts the compres-sion statistics after pruning and table 6 depicts the mean IU statistics and reduction in storage afterpruning.
Table 5: FCN compression results after pruningLayers of the network to be pruned	Total parameters before pruning	Achieved sparsity in each layer	Total non-zero parameters after pruning	Remaining non-zero weights after pruning in % (P)C1	1792	49%	913	5Γ%C2	36928	84%	5908	16%C3	73856	69%	22895	31%C4	147584	69%	45751	31%C5	295168	69%	91502	31%C6	590080	70%	177024	30%C7	590080	69%	182924	31%C8	1180160-	79%	247833	21%C9	-2359808	81%	448363	19%C10	-2359808	84%	377569	16%Cn	-2359808	83%	401167	17%C12	-2359808	83%	401167	17%C13	-2359808	83%	401167	17%C14	-102764544-	89%	11304099	11%C15	-16781312-	89%	1845944	11%C16	77843	89%	8562	11%C17	9747	89%	1072	11%C18	3268	89%	359	11%
Table 6: Reduction in storage requirement after pruning and quantizationStages of PiPleline	Storage requirement of parameters (in MB)	Reduction in storage requirement (in %)	IOUBaseline	537.47 MB	-	64.75 %Pruning	72.93 MB	86.43%	61.25 %Pruning + Quantization	18.23 MB	96.60%	59.82 %5 Conclusion and Future WorkDeep learning approaches have demonstrated that they can outperform many traditional techniques,but because of their complex architecture in terms of more stacked layers and a large number ofparameters, it is challenging to deploy these deep networks on mobile devices with limited hard-ware requiting real time predictions. This work contributes to the previous research on compressionof deep networks performing classification. Moreover, we have also presented the compression ofa network that performs semantic segmentation. We implemented a three stage deep compressionpipeline of pruning, weight sharing, and quantization. Using different sparsity levels, we calcu-late different thresholds in each layer to perform pruning. Our “layerwise threshold” initializationmethod has shown promise in providing a good trade-off between sparsity and network performance.
