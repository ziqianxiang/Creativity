Table 1: The average of log-likelihood per sequence on Blizzard and TIMIT testsetModel	Blizzard	TIMITRNN-Gauss	3539	-1900RNN-GMM	7413	26643VRNN-I-Gauss	≥ 8933	≥ 28340VRNN-Gauss	≥ 9223	≥ 28805VRNN-GMM	≥ 9392	≥ 28982SRNN (smooth+resq)	≥ 11991	≥ 60550Z-Forcing (Sordoni et al., 2017)	≥ 14315	≥ 68852Variational Bi-LSTM	≥ 17319	≥ 73315TIMIT: Another speech modeling dataset is TIMIT with 6300 English sentences read by 630 speak-ers. Like the work done in (Fraccaro et al., 2016), our model is trained on raw sequences of 200dimensional frames. In our experiments, we use 1024 hidden units, 2048 LSTM units and 128 latentvariables, and batch size of 128. We train the model using learning rate of 0.0001, α = 1 and β = 0.
Table 2: The average of negative log-likelihood on sequential MNISTModels	Seq-MNISTDBN 2hl (Germain et al., 2015)	≈ 84.55NADE (Uria et al., 2016)	88.33EoNADE-5 2hl (Raiko et al., 2014)	84.68DLGM 8 (Salimans et al., 2014)	≈ 85.51DARN 1hl (Gregor et al., 2015)	≈ 84.13BiHM (Bornschein et al., 2015)	≈ 84.23DRAW (Gregor et al., 2015)	≤ 80.97PixelVAE (Gulrajani et al., 2016)	≈ 79.02Prof. Forcing (Goyal et al., 2016)	79.58PixelRNN(1-layer) (Oord et al., 2016)	80.75PixelRNN(7-layer) (Oord et al., 2016)	79.20Z-Forcing (Sordoni et al., 2017)	≤ 80.09Variational Bi-LSTM	≤ 79.78IMDB: It is a dataset consists of 350000 movie reviews (Diao et al., 2014) in which each sentencehas less than 16 words and the vocabulary size is fixed to 16000 words. In this experiment, we use500 hidden units, 500 LSTM units and latent variables of size 64. The model is trained with a batchsize of 32 and a learning rate of 0.001 and we set α = β = 1. The word perplexity on valid and testdataset is shown in Table 3.
Table 3: Word perplexity on IMDB on valid and test setsModel	Valid TestGated Word-Char	70.60	70.87Z-Forcing (Sordoni et al., 2017)	56.48	65.68Variational Bi-LSTM	51.43	51.60Poolnax--loo—J①旨① ><Figure 2: Evolution of the average of log-likelihood during training of Variational Bi-LSTMs withand without using skip gradient and auxiliary costs on PTB and Blizzard.
Table 4: Bits Per Character (BPC) on PTB valid and test setsModel	Valid	TestUnregularized LSTM	1.47	1.36Weight noise	1.51	1.34Norm stabilizer	1.46	1.35Stochastic depth	1.43	1.34Recurrent dropout	1.40	1.29Zoneout (Krueger et al. (2016))	1.36	1.25RBN (Cooijmans et al. (2016))	-	1.32H-LSTM + LN (Ha et al. (2016))	1.28	1.253-HM-LSTM + LN (Chung et al., 2016)	-	1.242-H-LSTM + LN (Ha et al. (2016))	1.25	1.22Z-Forcing	1.29	1.26Variational Bi-LSTM	1.26	1.234 Ablation StudiesThe goal of this section is to study the importance of the various components in our model to avoidany triviality. The experiments are as follows:1. Reconstruction loss on ht vs activity regularization on ht6Under review as a conference paper at ICLR 2018
Table 5:	Perplexity on IMDB using different coefficient γ for activity regularizationY	0.001	1.	4.	8.	16.
Table 6:	KL divergence of the Variational Bi-LSTMDataset	PTB	Seq-MNIST	IMDB	TIMrT	BlizzardKL	0.001	0.02	0.18	3204.71	3799.792. Use of parametric encoder prior vs. fixed Gaussian priorIn our variational Bi-LSTM model, we propose to have the encoder prior over zt as a function ofthe previous forward LSTM hidden state ht-1. This is done to omit the need of the backwardLSTM during inference because it is unavailable in practical scenarios since predictions are madein the forward direction. However, to study whether the model learns to use this encoder or not, werecord the KL divergence value of the best validation model for the various datasets. The results arereported in table 6. We can see that the KL divergence values are large in the case of IMDB, TIMITand Blizzard datasets, but small in the case of Seq-MNIST and PTB. To further explore, we ranexperiments on these datasets with fixed standard Gaussian prior like in the case of traditional VAE.
