Table 1: Results on collection 1952. Downsampling reflects the down-scaling of brain volumes.
Table 2: Results on collection 2138. The synthetic images used here were generated from theICW-GAN. Input represents the input of a classifier: Real means only use real training data, while‘Real+Synth.’ means real training data plus generated data. Two classifiers are utilized: SVMand neural networks (NN). Results show that augmenting real data with synthetic data improvesclassification performance.
Table 3: Results on collection 503. Similarly, downsampling reflects the down-scaling of brainvolumes. Input represents the input of a classifier: Real means only use real training data, while‘Real+Synth.’ means real training data plus generated data. Two classifiers are utilized: SVMand neural networks (NN). Results show that augmenting real data with synthetic data improvesclassification performance.
Table 4: Results of ACD-GAN on the three datasets in the low resolution setting. We find theACD-GAN to perform slightly better than the ICW-GAN.
Table 5: Accuracy, F1, Precision, Recall and their variance (column 3,5,7,9) for 3-fold, 5-foldand 10-fold cross validation. We conducted this experiment with the training data of mixed‘Real+Synth.’ data of collection 1952 in the low resolution (4.0×) setup.
Table 6: Results of synthesizing data using collection 1952 (low resolution). Comparison betweenthe GMM and ICW-GAN. We list 6 training data strategies: in the 1st, 2nd , 4th and 5th row, weonly use synthetic data to train the deep net classifier while in the 3rd and 6th row, we mix real andsynthetic data together to train the same classifier.
Table 7: Multilabel results for collection 1952 at 4.0× downsampling and synthetic data were fromthe ICW-GAN. Even in this highly demanding task, accuracy scores with mixed data (the secondrow) outperform the baseline (the first row).
