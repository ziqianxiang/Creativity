Table 1: A varitey of commercially available ARM Cortex-M based microcontrollersWe offer the following contributions: (1) a new architecture that generates the convolution filtersby using a weighted combination of orthogonal binary basis that can be generated on-the-fly veryefficiently and offers competitive results on ImageNet. This approach translates into not having tostore the filters and therefore reduce the modelâ€™s memory footprint. (2) The number of parametersneeded to be updated during training is greatly reduced since we only need to update the weights andnot the entire filter, leading to a faster training stage. (3) We present an scenario where the on-the-flynature of BinaryFlex is benefitial when the model does not fit in RAM.
Table 2: BinaryFlex architectural dimensions and parameters for the 3.4MB model trained on ImageNet. AllBinaryFlex configurations for ImageNet maintain the above parameters and only vary the ratios. For MNISTand CIFAR-10 the final layers are slightly modified to accommodate for the number of classes in those datasets.
Table 3: Accuracy (%) of Binaryflex in three popular image datasets at different model sizes (i.e. differentOVSF ratios). For ImageNet, accuracy values are shown as Top-1/Top-5.
Table 4: Accuracy of BinaryFlex and baselines on three sim-ple classification datasets, shown in the format: accuracy %/ model size (MB). The naming BinaryFlex-Xq8 refers to aBinaryFlex model of X MB (when representing the weightsas 32-bit values) whose weights have been 8-bit quantized.
Table 5: Accuracies (%), model sizes (MB) andsize-to-accuracy ratios of binary networks onImageNet shown as Top-1/Top-5 values. Ratiosare computed using Top-5 accuracy values.
