Table 1: Average Top-1 errors (%) of “PyramidNet + ShakeDrop” with several ranges of parametersof 4 runs at the final (300th) epoch on CIFAR-100 dataset in the “Batch” level. In some settings, itis equivalent to PyramidNet and PyramidDrop.
Table 2: Average Top-1 errors (%) of “PyramidNet + ShakeDrop” with different levels of 4 runs atthe final (300th) epoch on CIFAR-100 dataset.
Table 3: Top-1 errors (%) at the final (300th) epoch of ResNet and its improvements to whichdifferent regularization methods are applied on CIFAR-100 dataset. Method names are followedby components of their residual blocks. For ResNet and ResNeXt, in addition to the original form,ones without ReLU unit in the end of residual blocks following EraseReLU (Dong et al., 2017) arealso examined. For Wide ResNet, ones with bath normalization added in the end of residual blocks,referred as “w/ BN,” are also examined. “Type A” and “Type B” of ResDrop and ShakeDrop meanthat regularization unit is inserted before and after “add” unit for residual branches, respectively.
Table 4: Top-1 errors (%) at the final (300th) epoch of ResNet and its improvements to whichdifferent regularization methods are applied on Tiny ImageNet dataset. Method names are followedby components of their residual blocks. For ResNet and ResNeXt, in addition to the original form,ones without ReLU unit in the end of residual blocks following EraseReLU (Dong et al., 2017) arealso examined. For Wide ResNet, ones with bath normalization added in the end of residual blocks,referred as “w/ BN,” are also examined. “Type A” and “Type B” of ResDrop and ShakeDrop meanthat regularization unit is inserted before and after “add” unit for residual branches, respectively.
Table 5: Top-1 errors (%) at the final epoch (300th or 1800th) on the CIFAR-10/100 datasets. Repre-sentative methods and the proposed ShakeDrop applied to PyramidNet are compared. “Reg” repre-sents regularization methods including ResDrop (RD), Shake-Shake (SS) and proposed ShakeDrop(SD). If “Cos” is checked, 1800-epoch cosine annealing schedule (Loshchilov & Hutter, 2016) isused following Gastaldi (2017). Otherwise, 300-epoch multi-step learning rate decay schedule isused following each method. If “Fil” is checked, the data augmentation used in Cutout (CO) (De-Vries & Taylor, 2017b) or Random Erasing (RE) (Zhong et al., 2017), which randomly fills a partof learning images, is used. * indicates the result is quoted from the literature. + indicates the resultis quoted from Gastaldi (2017). Compared to the same condition of Cutout, the state-of-the-art, theproposed method reduced the error rate by 0.25% on CIFAR-10 and 3.01% on CIFAR-100.
