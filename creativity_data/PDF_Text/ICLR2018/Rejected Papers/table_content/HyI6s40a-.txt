Table 1: PCL performance against different attack methodologies for MNIST, CIFAR10, and Ima-geNet benchmarks. The reported numbers correspond to the pertinent false positives for achievingparticular detection rates in each scenario. The JSMA attack for the ImageNet benchmark is compu-tationally expensive (e.g., it took more than 20min to generate one adversarial sample on an NVIDIATITAN Xp GPU). As such, we could not generate the adversarial samples of this attack using theJSMA library provide by (Nicolas Papernot (2017)).
Table 2: Baseline (victim) network architectures for evaluated benchmarks. Here, 128C3(2) de-notes a convolutional layer with 128 maps and 3 × 3 filters applied with a stride of 2, MP3(2) indi-cates a max-pooling layer over regions of size 3 × 3 and stride of 2, and 300FC is a fully-connectedlayer consisting of 300 neurons. All convolution and fully connected layers (except the last layer)are followed by ReLU activation. A Softmax activation is applied to the last layer of each network.
Table 3: Details of attack algorithms for each evaluated application. The FGS method (Goodfel-low et al. (2014)) is characterized with a single ε parameter. The JSMA attack (Papernot et al.
