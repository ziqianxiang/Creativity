Table 1: Perf: top-1 accuracy (%, higher is better) on various datasets and parameter cost (#par.,lower is better) for a few baselines and several variants of our method. Rows 1,2: independentbaseline performance. VGG-B: VGG (Simonyan and Zisserman (2014)) architecture B. (S) - trainedfrom scratch. (P) - pre-trained on ImageNet. Rows 3-7: (ours) controller network performance;DANsketch as a base network outperforms DANcaltech-256 on most datasets. A controller networkbased on random weights (DANnoise) works quite well given that its number of learned parametersis a fifth of the other methods. DANimagenet : controller networks initialized from VGG-B modelpretrained on ImageNet. DANimagenet+sketch: selective control network based on both VGG-B(P)& Sketch. We color code the first, second and third highest values in each column (lowest for #par).
Table 2: Mean transfer learning performance. We show the mean top-1 accuracy (%) attained byfine-tuning a network from each domain to all domains. Out of the datasets above, starting withCaltech-256 proves most generic as a feature extractor (ft-last). However, fine tuning is best wheninitially training on the Sketch dataset (ft-full).
Table 3: Results on Visual Decathlon Challenge. Scratch: training on each task independently.
