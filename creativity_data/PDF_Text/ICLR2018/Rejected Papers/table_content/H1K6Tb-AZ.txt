Table 1: Hyper-parameters in ExperimentsExperiment	MLP on MNIST	VGG-16 on CIFAR-10	VGG-16 on CIFAR-100TaskS atDP%	=	100, 70, 40,10	100,50, 20	100,70,50,30Learning rate	0.001	0.004	-	0.004	~Optimizer	Adam	SGD momentum = 0.9	SGD momentum = 0.9-Batch size	28	32	64Aggregation coefficient	0.5	.	0.5	-	0.5	-TESLA stopping criteria	not improve in 4 epochs	not improve in 4 epochs	not improve in 4 epochsR-TESLA stopping criteria	# epochs over 50	# epochs over35	# epochs over 35Initial weights	random	pre-trained on ImageNet	pre-trained on ImageNettask-wise loss aggregation, these losses are aggregated incrementally and can be jointly optimizedto learn a shared representation to be relevant to all tasks.
