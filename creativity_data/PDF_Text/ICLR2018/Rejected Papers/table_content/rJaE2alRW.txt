Table 1: Detailed results for all datasets. For each model, we present the mean squared error obtainedon the out-of-sample test set. The best results for each dataset are marked by bold font. SOCNN1(SOCNN1+) denote proposed models with one (10 or 7) offset sub-network layers. For quotesdataset the presented values are averaged mean-squared errors from 6 separate prediction tasks,normalized according to the error obtained by VAR model.
Table 2: MSE for different valuesof α for two artificial datasets.
Table 3: Configurations of the trained models. f - number of convolutional filters/memory cellsize in LSTM, ks - kernel size, p - dropout rate, clip - gradient clipping threshold, conv - (k × 1)convolution with kernel size k indicated in the ks column, conv1 - (1 × 1) convolution. Apart fromthe listed layers, each network has a single fully connected layer on the top. Kernel sizes (3, 1) ((1,3, 1)) denote alternating kernel sizes 3 and 1 (1, 3 and 1) in successive convolutional layersArtificial & Electricity DatasetsModel	layers	f	ks	p	clipSOCNN	10conv + {1, 10}conv1	{8,16}	{(3, 1), 3}	0	{1,.001}CNN	7conv + 3pool	{16, 32}	{(3, 1), 3}	{0, .5}	{1, .001}LSTM	{1,2,3,4}	{16, 32}	-	{0, .5}	{1, .001}ResNet	22conv + 3pool	16	(1, 3, 1)	{0, .5}	{1, .001}Quotes Dataset					Model	layers	f	ks	p	clipSOCNN	7conv + {1, 7}conv1	8	{(3, 1), 3}	.5	.01CNN	7conv + 3pool	{16, 32}	{(3, 1), 3}	.5	.01LSTM	{1, 2, 3}	{32}	-	.5	.000111ResNet	22conv + 3pool	16	(1, 3, 1)	.5	.01C.3 Network Training					The training and validation sets were sampled randomly from the first 80% of timesteps in eachseries, with ratio 3 to 1. The remaining 20% of data was used as a test set. All models were trained
Table 4: Detailed results for each prediction task for the quotes dataset. Each task involves predictionof the next quote by one of the banks. Numbers represent the mean squared errors on out-of-sampletest set.
