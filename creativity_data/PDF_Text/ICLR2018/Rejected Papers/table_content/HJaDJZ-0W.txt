Table 1: Heuristics to pick hyper-parameters for block-pruningHYPER-PARAM	DESCRIPTION	HEURISTIC VALUESStartdtr ramp_itr enddtr	Iteration to start pruning	Start of second epoch Iteration to increase the rate of pruning	Start of 20% of total epochs Iteration to stop pruning more parame-	Start of 40% of total epochs tersStartslope θ	Initial rate of increasing the threshold	See Equation 2(θ) ramp slope (φ) freq	Rate of increasing threshold after ramp	1.2θ to 1.7θ iteration Number of iterations after which is 100 updatedNarang et al. (2017) use six hyper-parameters to determine the threshold at a given iteration. Table 1provides the description and heuristics (adapted for block pruning) for these hyper-parameters. Thestartslope and ramp_SloPe determine the rate at which the threshold increases. In order to determineStartslope, they recommend using weights from an existing dense model. To achieve 90% sparsity,they assign q to the weight that is the 90th percentile of the absolute values in a weight matrix.
Table 2: Bidirectional RNN model results. Block-Sparse models are trained with 4x4 blocksMODEL	LAYER SIZE	# PARAMS (in millions)	CER (% LOSS)	EPOCHS	PRUNING ALGORITHMRNN Dense	1760	67	15.36 (0.0%)	25	N/ARNN Block-Sparse	1760	10.9	30.14 (-96%)	25	Group lassoRNN Sparse	1760	8.3	18.91 (-23%)	25	Yu et al. (2012)RNN Sparse	1760	7.3	17.32 (-13%)	25	Narang et al. (2017)RNN Sparse	1760	7.1	15.41 (-0.3%)	60	Han et al. (2015)RNN Block-Sparse	1760	7.3	17.93 (-17%)	25	Ours (BP)RNN Block-Sparse	2560	12.9	15.89 (-3.4%)	25	Ours (GLP)RNN Block-Sparse	3072	25.8	15.66 (-1.9%)	25	Ours (BP)Table 3: GRU model results with 4×4 blocksMODEL	LAYER SIZE	# PARAMS (in millions)	CER (% LOSS)	EPOCHS	PRUNING ALGORITHMGRU Dense	2560	115	15.42 (0.0%)	25	N/AGRU Block-Sparse	2560	10.8	16.78 (-8.8%)	25	Ours (GLP)GRU Block-Sparse	3584	25.6	16.23 (-5.3%)	25	Ours (BP)Comparison to other pruning methods: In addition to group lasso, we compare our block prun-ing approach with three existing pruning methods. As shown in Table 2, our block-sparse modelachieves better accuracy than the hard thresholding scheme in Yu et al. (2012). Sparse RNNs gen-erated using Narang et al. (2017) is about 4% better than the block-sparse model. The sparse RNNmodel generated using iterative pruning (Han et al., 2015) is significantly better than than block-
Table 3: GRU model results with 4×4 blocksMODEL	LAYER SIZE	# PARAMS (in millions)	CER (% LOSS)	EPOCHS	PRUNING ALGORITHMGRU Dense	2560	115	15.42 (0.0%)	25	N/AGRU Block-Sparse	2560	10.8	16.78 (-8.8%)	25	Ours (GLP)GRU Block-Sparse	3584	25.6	16.23 (-5.3%)	25	Ours (BP)Comparison to other pruning methods: In addition to group lasso, we compare our block prun-ing approach with three existing pruning methods. As shown in Table 2, our block-sparse modelachieves better accuracy than the hard thresholding scheme in Yu et al. (2012). Sparse RNNs gen-erated using Narang et al. (2017) is about 4% better than the block-sparse model. The sparse RNNmodel generated using iterative pruning (Han et al., 2015) is significantly better than than block-sparse model. However, this approach requires training the model for 60 epochs instead of25 epochsfor all other approaches. This results in 180 hours of additional training time for the RNN model.
Table 4: Results for GRU model with 2560 layer size and bidirectional RNN model with 170 layersize pruned with different block sizes using BP.
Table 5: Word language modelling results on Penn Tree Bank using BP for 4×4 blocks.
Table 6: Accuracy, speed-up and memory reduction for sparse layers. Block-sparse layers achievehigher speedup and memory reduction with some loss in accuracy.
Table 7: '1 and '1/2 results with the bidirectional RNN model with 1760 hidden unitsMODEL	# PARAMS (in millions)	SPARSrTY	CER	RELATIVE PERF	PRUNING ALGORITHMRNN Dense	67	0.0%	15.36	0.0%	N/ARNN Sparse	7.3	89.2%	17.32	-12.8%	Weight pruningRNN Sparse	11.2	83.6%	24.8	-61.5%	'1RNN Sparse	7.4	89.1%	17.28	-12.5%	'1 with pruningRNN Sparse	6.6	90.3%	18.50	-20.4%	'1/2 with pruningWithout pruning, '1 model results in significantly worse accuracy compared to the dense baseline.
