Table 1: Subsets of Netflix Prize training set used in our experiments. We made sure that these splitsmatch exactly the ones used in (Wu et al., 2017).
Table 2: Depth helps generalization. Evaluation RMSE of the models with different number oflayers. In all cases the hidden layer dimension is 128.
Table 3: Test RMSE of different models. I-AR, U-AR and RRN numbers are taken from (Wu et al.,2017)	DataSet	I-AR	U-AR	RRN	DeepRec Netflix 3 months	0.9778	0.9836	0.9427	0.9373 Netfix Full	0.9364	0.9647	0.9224	0.9099impact on the model performance. However, in conjunction with the higher learning rate, it didsignificantly increase the model performance. Note, that with this higher learning rate (0.005) butwithout dense re-feeding, the model started to diverge. See Figure 5 for details.
Table 4: Test RMSE achieved by DeepRec on different Netflix subsets. All models are trained withone iterative output re-feeding step per each iteration.
