Figure 1: Latent Sympathetic Examples (LSE). The original input image is encoded and recon-structed . x0 is then perturbed, by iteratively calculating the gradients with respect to the correctclass and updating . By keeping as small as possible, much of the reconstruction error should becanceled out by the reconstructed image.
Figure 2: This figure shows the predictions ofc(g(z)) given a grid of values for z. As can beseen here every MNIST class is present. The starlabels the point where z1 and z2 is zero and thetriangle where the original image is encoded.
Figure 3: The predictions of c(x0) given a gridof values for . It can be seen that the classifiergives different results for the perturbed image x0as opposed to the image reconstruction (g(z))).
Figure 4: The plot on the right shows the path of z + . The different colored areas show the classpredictions made for a given value of z + . These areas are annotated with the corresponding class.
Figure 5: Comparison of the quality of the perturbed MNIST images and their latent dimensionality.
Figure 6: Examples of using the LSE technique to perturb images to be correctly classified. Theleft of center image is the original image, the center image is the perturbation (x0 - x) and the rightimage is the perturbed image. The labels on the left denote the original (wrong) prediction and thelabels on the right is the correct class. In the first row we see an image which is misclassified asnot wearing earrings. By making the earring brighter the image is correctly classified. The secondimage from the top is of an image where the person was misclassified as having a mustache (theperson has a beard), after making the hair below the nose lighter, the image was correctly classifiedas not having a mustache. The third image from the top was misclassified as not having a mustache.
Figure 7: This plot shows the results of perturbing a misclassified image after different amounts ofiterations. As can be seen the perturbation is qualitatively much more fine grained when comparedto the perturbation in figure 4.
Figure 8: Plotting every fifth step through the latent space, alongside the perturbation (x0 - x). Itcan be seen that our method perturbs the original image to be correctly classified. The perturbationsare normalized using min-max normalization, where the minimum is set to -1 and the maximum isset to 1.
Figure 9: This plot shows our method getting stuck in a local minimum. The image was originallymisclassified as a zero. The correct class, eight, exists, however a local minimum blocks the way.
Figure 10: Image showing various men with long hair which were misclassified as “not male”.
