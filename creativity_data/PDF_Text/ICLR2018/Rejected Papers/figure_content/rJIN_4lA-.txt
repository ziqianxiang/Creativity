Figure 1: In two Markov social dilemmas we find that standard self-play converges to defectingstrategies while modified self-play finds cooperative, but exploitable strategies. We use the results ofthese two training schedules to construct πC and πD.
Figure 2: In two Markov social dilemmas, amTFT satisfies the Axelrod desiderata: it mostlycooperates with itself, is robust against defectors, and incentivizes cooperation from its partner. The‘Grim’ strategy based on de Cote & Littman (2008) behaves almost identically to pure defection inthese social dilemmas. The result of standard self-play is πD . The full tournament of all strategiesagainst each other is shown in the Appendix.
Figure 3: Both purely selfish and purely cooperative Teachers lead Learners to exploitative strategies.
Figure 4: Results from training one-memory strategies using policy gradient in the repeated Prisoner’sDilemma. Even in extremely favorable conditions self-play fails to discover cooperation maintainingstrategies. Note that temptation payoff .5 is not a PD and here C is a dominant strategy in the stagegame.
Figure 5: Results of the tournament in two Markov social dilemmas. Each cell contains the averagetotal reward of the row strategy against the column strategy. amTFT achieves close to cooperativepayoffs with itself and achieves close to the defect payoff against defectors. Its partner also receives ahigher payoff for cooperation than defection.
