Figure 1: A motivating example for episodic backward update. Left: Simple navigation domainwith 4 states and a single rewarded transition. Right: The probability of learning the optimal path(s1 → s3 → s4) after updating the Q-values with sample transitions.
Figure 2: 2D MNIST mazeWe compare the performance of EBU to uniform one-step Q-learning and n-step Q-learning. Forn-step Q-learning, we set the value of n as the length of the episode. We use 10 by 10 mazeswith randomly placed walls. The agent starts at (0,0) and has to reach the goal position at (9,9)as soon as possible. Wall density indicates the probability of having a wall at each position. Weassign a reward of 1000 for reaching the goal and a reward of -1 for bumping into a wall. For eachwall density, we generate 50 random mazes with different wall locations. We train a total of 50independent agents, one agent for one maze over 200,000 steps each. The MNIST images for staterepresentation are randomly selected every time the agent visits each state. The relative length is5Under review as a conference paper at ICLR 2018Figure 3: Median relative lengths of EBU and other baseline algorithms. As the wall density in-creases, EBU outperforms other baselines more significantly. The filled regions indicate the standarddeviation of results from 50 random mazes.
Figure 3: Median relative lengths of EBU and other baseline algorithms. As the wall density in-creases, EBU outperforms other baselines more significantly. The filled regions indicate the standarddeviation of results from 50 random mazes.
Figure 4: Relative performance (Eq.(3)) of EBU over Nature DQN in percents (%) after 10 millionframes of training.
Figure 5: Scores of EBU and baselines on 4 games: ‘Assault’, ‘Breakout’, ‘Gopher’ and ‘VideoPinball’. Moving average test scores of40 epochs with window size 4 are plotted. The filled regionsindicate the standard deviation of results from 8 random seeds.
Figure 6: Learning curves of EBU and baselines on 49 games of the Arcade Learning Environment.
Figure 7: Target generation process from the sampled episode E14Under review as a conference paper at ICLR 2018Appendix E	Theoretical GuaranteesNow, we will prove that the episodic backward update algorithm converges to the true action-valuefunction Q* in the case of finite and deterministic environment.
