Figure 1: Deep Generative Replay to train a Deep Generative MemoryDeep Generative Memory (DGM): We first introduce a sub-model called the Deep GenerativeMemory (see figure 1) which has three elements: (i) a generative model (the generator G), (ii) afeedforward network (the learner L), and (iii) a dictionary (Ddgm) with task IDs of learnt tasksand the number of times they were encountered. We call this a memory because of its weights andlearning capacity, not due to any recurrent connections. We assume availability of unique task IDsfor replay and to identify repetition. In practice, a task identification system (e.g., a HMM-basedinference model) like in previous works (Kirkpatrick et al., 2017) suffices for this purpose. We choosevariational autoencoder (VAE) (Kingma & Welling, 2014) for the generator, since our generativemodel requires reconstruction capabilities (see section 3.2).
Figure 2: Deep Generative Dual Memory Network (DGDMN)Our architecture (DGDMN) shown in figure 2 comprises of a large deep generative memory (DGM)called the long-term memory (LTM) which stores information of all previously learnt tasks like theneocortex, and a short-term memory (STM) which behaves similar to the hippocampus and learnsnew incoming tasks quickly without interference from previous tasks. The STM is a collection ofsmall, dedicated, task-specific deep generative memories (called short-term task memory - STTM),which can each learn one unique task. If an incoming task comes is already in an STTM, the sameSTTM is used to retrain on it, otherwise a fresh STTM is allocated to the task. Additionally, if thetask has been previously consolidated then the LTM reconstructs the incoming samples for that taskusing the generator (hence we use a VAE), predicts labels for the reconstructions using its learner andsends these newly generated samples to the STTM allocated to this task. This provides extra sampleson tasks which have been learnt previously and helps to learn them better, while also preserving theprevious performance on that task to some extent.
Figure 3: Accuracy curves for Permnist (x: tasks seen, y: classification accuracy on task).
Figure 4: Accuracy curves for Digits (x: tasks seen, y: classification accuracy on task).
Figure 5:	Forgetting curves (x: tasks seen, y: avg classification accuracy on tasks seen).
Figure 6:	Accuracy curves when tasks are revised: (a) EWC, (b) DGDMN.
Figure 7: Accuracy curves for TDigits on: (a) tasks seen so far, (b) last 10 tasks seen.
Figure 8: (a) LTM reconstruction from noisy and occluded digits, (b) Classification accuracy withincreasing gaussian noise, and (c) Classification accuracy with increasing occlusion factor.
Figure 9: (a) Training time for DGDMN and DGR, (b) Accuracy curves: DGDMN (no STM).
Figure 10: Accuracy curves for Shapes (x: tasks seen, y: classification accuracy on task).
Figure 11: Accuracy curves for Hindi (x: tasks seen, y: classification accuracy on task).
Figure 12: Forgetting curves on Shapes and Hindi dataset (x: tasks seen, y: avg classification accuracyon tasks seen).
Figure 13: t-SNE embedding for latent vectors of the VAE generator on Digits dataset when: (a)tasks are learnt jointly, and (b) tasks are learnt sequentially.
Figure 14: Visualization of digits from LTM when trained: (a) jointly, (b) sequentially8	Appendix B8.1	Dataset preprocessingAll our datasets have images with intensities normalized in the range [0.0, 1.0] and size (28 × 28),except Hindi which has (32 × 32) size images.
