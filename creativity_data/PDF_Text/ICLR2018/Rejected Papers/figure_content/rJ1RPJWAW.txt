Figure 1: Shallow network with 1024 filtersFigure 2: VGG11For both the networks, we see that the off-diagonal entries are quite close to each other. This seemsto suggest that while the different SGD solutions are not same as functions, they agree on a commonsubset (93% for shallow network and 73% for VGG-11) of examples. Furthermore, for VGG-11, theoff-diagonal entries are very close to the test accuracy - this behavior of VGG-11 seems commonto other popular architectures as well. This seems to suggest that different SGD solutions agree onprecisely those examples which they predict correctly, which in turn means that the subset of exam-ples on which different SGD solutions agree with each other are precisely the correctly predictedexamples. However this does not seem to be the case. Figures 1 and 2 show the histograms ofthe number of distinct predictions for shallow network and VGG-11 respectively. For each numberi ∈ [k], it shows the fraction of examples for which the k SGD solutions make exactly i distinctpredictions. The number of examples for which there is exactly 1 prediction, or equivalently allthe SGD solutions agree is significantly smaller than the test accuracies reported above.
Figure 2: VGG11For both the networks, we see that the off-diagonal entries are quite close to each other. This seemsto suggest that while the different SGD solutions are not same as functions, they agree on a commonsubset (93% for shallow network and 73% for VGG-11) of examples. Furthermore, for VGG-11, theoff-diagonal entries are very close to the test accuracy - this behavior of VGG-11 seems commonto other popular architectures as well. This seems to suggest that different SGD solutions agree onprecisely those examples which they predict correctly, which in turn means that the subset of exam-ples on which different SGD solutions agree with each other are precisely the correctly predictedexamples. However this does not seem to be the case. Figures 1 and 2 show the histograms ofthe number of distinct predictions for shallow network and VGG-11 respectively. For each numberi ∈ [k], it shows the fraction of examples for which the k SGD solutions make exactly i distinctpredictions. The number of examples for which there is exactly 1 prediction, or equivalently allthe SGD solutions agree is significantly smaller than the test accuracies reported above.
Figure 3: Plot of learnability and generalization error vs epochs for shallow 2-layer CNNsThe experimental results so far show a clear correlation between learnability and generalizability oflearned neural networks. This naturally leads to the question of why this is the case. We hypothesizethat learnability captures the inductive bias of SGD training of neural networks. More precisely,when we start training, intuitively, the initial random network generalizes well (i.e., both train andtest errors are high) and is also simple (learnability is high). As SGD changes the network to re-duce the training error, it becomes more complex (learnability decreases) and generalization errorincreases. Figure 3 which shows the plots of learnability and generalizability of shallow 2-layerCNNs supports this hypothesis.
