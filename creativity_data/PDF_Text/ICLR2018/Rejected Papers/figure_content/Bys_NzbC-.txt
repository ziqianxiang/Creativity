Figure 1: (a,b) Validation accuracies for different λ when L1 and L2 regularization is applied toVGG-16 and AlexNet on the CIFAR-100 data set. Note the sharp accuracy drop. (c) Training losswhen different λ for L2 regularization is used for VGG-16 on CIFAR-100. Note that the regulariza-tion loss is excluded from the training loss. Best shown in color.
Figure 2: Gradients for different λ by VGG-16 on CIFAR-100. (a) Average amount of gradient fromL when L2 regularization is applied. (b) A close-up version of (a) with y-axis in log-scale. (c) Theproportion of gradients from L to all gradients. Best shown in color.
Figure 3: Accuracies obtained by VGG-16 with L1 and L2 regularization on CIFAR-100 (a,b,c) andCIFAR-10 (d,e,f). A green dotted horizontal line is an accuracy obtained by a model without L1/L2regularization (but with dropout). Accuracy for different sparsity is shown in (c) and (f). The errorbars indicate 95% confidence interval.
Figure 4: Accuracies obtained by AlexNet with L1 and L2 regularization on CIFAR-100 (a,b,c) andCIFAR-10 (d,e,f). A green dotted horizontal line is an accuracy obtained by a model without L1/L2regularization (but with dropout). Accuracy for different sparsity is shown in (c) and (f). The errorbars indicate 95% confidence interval.
Figure 5: Experiment results by L2 regularization with VGG-16 on CIFAR-100. The results arefrom baseline method unless it is labeled as “ours”. Our proposed Delayed Strong Regularizationdoes not fail in learning.
Figure 6: Accuracies obtained by variations of VGG with L2 regularization on SVHN. A green dot-ted horizontal line is an accuracy obtained by a model without L2 regularization (but with dropout).
Figure 7: Accuracies obtained by variations of VGG with L1 regularization on SVHN. A green dot-ted horizontal line is an accuracy obtained by a model without L1 regularization (but with dropout).
