Figure 1: Modifying the gradient by projecting onto the direction orthogonal to direction of gradientat st+1involves the product of three expectations, which is handled by keeping a separate set of weightsthat approximate two of these expectations, and is updated at a faster scale. The idea also does notimmediately scale to nonlinear function approximation. Bhatnagar et al. (2009) propose a solutionby projecting the error on the tangent plane to the function at the point at which it is evaluated.
Figure 2: Baird’s Counterexample is specified by 6 states and 7 parameters. The value of each stateis calculated as given inside the state. At each step, the agent is initialized at one of the 6 statesuniformly at random, and transitions to the state at the bottom, shown by the arrows.
Figure 3: A 10 × 10 Gridworld with a goal at location (0, 4), which is midway between one of thewalls. Both DQN and Constrained DQN are used to approximate the value function for a softmaxpolicy.
Figure 4: Comparison of DQN and Constrained on the Cartpole Problem, taken over 10 runs. Theshaded area specifies std deviation in the scores of the agent across independent runs. The agent iscut off after it’s average performance exceeds 199 over a running window of 100 episodeswhere the policy π is a softmax over the Q-values. The room can be seen in Figure 3 with the goalin red, along with a comparison of the value functions learnt for the 2 methods we compare.
