Figure 1: Percentage of data (or simply one batch) used to calculate gradient used in SHG method.
Figure 2: Average training loss versus epochs of SGD, SHG, GD, SR1, SQN on MNIST dataset.
Figure 3: Average training loss versus epochs of SGD, SHG, GD, SR1, SQN on CIFAR-10 dataset.
Figure 4: Average training loss versus time of SGD,SHG,GD,SR1,SQN on MNIST dataset.
Figure 5: Average training loss versus time of SGD,SHG,GD,SR1,SQN on CIFAR-10 dataset. SGDin practice requires minimal computations and consequently faster than second-order methods.
Figure 6: Effect of changing batch size of SQN method. SQN fails to generalize the performance tolarger batches. In principle SQN is sentitive to hyper-parameters.
Figure 7: Effect of changing batch size of SGD and SR1 methods. SR1 is sensitive to batch size too.
Figure 8: Effect of changing batch size of SGD and SHG methods. SGD benefits a bit when batchsize grows to 400 but not larger. SHG outperforms SGD significantly when batch size keeps grow-ing. Notice that large batch size means smaller number of updates in each epoch.
Figure 9: Results of training regular DRN with ReLu activation. Although SHG canDRN on MNIST well, it fails to converge to better training loss on CIFAR-10 dataset.
Figure 10: Effect of using ReLu in AlextNet. Left figure is Regular AlexNet with tanh activationand right figure is AlexNet with ReLu activation. Apparently using ReLu in deep neural networksdeteriorates performance of SHG but not SR1.
Figure 11: Effect of adding Identity expression in AlexNet. Left figure is Regular AlexNet with tanhactivation and right figure is AlexNet with last convolutional layer replaced by a residual block. Theinfluence of identity link is not as strong as ReLu but still affects the performance of SHG.
Figure 12: Effect of using fixed learning ratecannot work with fixed learning rate.
