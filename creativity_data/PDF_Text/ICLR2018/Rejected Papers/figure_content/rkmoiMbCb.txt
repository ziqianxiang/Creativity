Figure 1: The five tandem blocks used in all of our experiments. The two left-most blocks Bid (2, w)and Bid(1, w) correspond to traditional resnets and the others to more general tandem nets.
Figure 2: Plots of the test accuracy by epoch from the third-best of each architecture’s five runs forall CIFAR-10 experiments. In each case, the tandem model B1×1(1, w) (light blue) performed best,beating both the resnet models Bid(1, w) and Bid(2, w). The model B1×1(1, w) was consistentlythe best in the different runs. Average final accuracy for the five runs is listed in Table 2.
Figure 3:	Plots of the test accuracy by epoch from the third-best of each architecture’s five runs forall CIFAR-100 experiments. In all cases the tandem models were clear winners, with B1×1(1, w)(light blue) performing best or near best each time, and both resnet models Bid(1, w) and Bid(2, w)at or near the bottom. The tandem model B3×3(1, w) performed better than the model B1×1(2, w)in the shallower networks (14 and 20 layers), but B1×1(2, w) did better in the deeper (26-layer)network. Average final accuracy for the five runs is listed in Table 2.
Figure 4:	Plots of the test accuracy by epoch from the third-best of each architecture’s five runs for allSVHN experiments. Again, in all three cases the tandem models B1×1(1, w) performed best or nearbest, and outperformed the resnet models Bid(1, w) and Bid(2, w), although in the last experiment,all the models—both resnet and more general tandem—have similar performance. Average finalaccuracy for the five runs is listed in Table 2.
Figure 5:	Plots of the test accuracy by epoch from the third-best of each architecture’s five runs forall Fashion-MNIST experiments. Again the tandem model B1×1(1, w) performed at or near best,although the tandem model B3×3(1, w) did best in the deeper (14-layer) experiment. The modelswith two nonlinear layers Bid(2, w) and B1×1(2, w) performed worst. Average final accuracy forthe five runs is listed in Table 2.
Figure 6: These plots show the singular values of the weight matrix of the linear 1 × 1 convolutionin a B1×1(1, w) block. We tried initializing these weights with identity matrices, zero matrices, andrandom matrices. All the singular values of the identity matrix are equal to 1, as seen in the initialepoch of the far left panel. Similarly all the singular values for the zero matrix are 0, as seen in theinitial epoch of the middle panel. In each case, the network learned a weight matrix that was quitedifferent from any of the initializations. In particular, this shows that identity maps are not evenlocally optimal in these applications.
