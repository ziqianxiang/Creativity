Figure 1: Descending the tube function of (Du et al., 2017). To reach the minimum, the algorithmmust navigate a series of saddle points. Optimisers tested were gradient descent (GD), perturbedgradient descent (PGD) (Jin et al., 2017), sign gradient descent (signGD) and the rescaled gradientmethod (noiseless version of (Levy, 2016)). No learning rate tuning was attempted, so we suggestonly focusing on the qualitative behaviour. Left: signGD appears not to ‘see’ the saddle pointsin the original objective function. Middle: after breaking the objective function’s axis alignment byrotating it, the sign method’s performance is still quantitatively different. Also the numerical error inour rotation operation appears to help unstick GD from the saddle points, illustrating the brittlenessof Du et al. (2017)’s construction. Right: for some rotations, the sign method (with fixed learningrate and zero stochasticity) can get stuck in perfectly periodic orbits around saddle points.
Figure 2: Results for training Resnet-20 (He et al., 2016) on CIFAR-10 (Krizhevsky, 2009) for SGD,signSGD and Adam. We plot test errors over a large grid of initial learning rate, weight decay andmomentum combinations. (In signSGD, momentum corresponds to taking the sign of a moving av-erage of gradients—see Appendix A for the detailed experimental setup.) All algorithms at the leastget close to the baseline reported in (He et al., 2016) of 91.25%. Note the broad similarity in generalshape of the heatmap between Adam and signSGD, supporting a notion of algorithmic similarity.
