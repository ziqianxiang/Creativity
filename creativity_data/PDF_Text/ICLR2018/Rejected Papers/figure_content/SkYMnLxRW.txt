Figure 1: Our proposed network architecture.
Figure 2: Testing v/s Training Loss for the newstest2013 English-to-German task. The WeightedTransformer has lower testing loss compared to the baseline Transformer for the same training loss,suggesting a regularizing effect.
Figure 3: Convergence of the (α, κ) weights for the second encoder layer of Configuration (C) forthe English-to-German newstest2013 task. We smoothen the curves using a mean filter. This showsthat the network does prioritize some branches more than others and that the architecture does notexploit a subset of the branches while ignoring others.
