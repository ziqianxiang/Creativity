Figure 1: Test and train accuracy on MNIST digit classification for a dense multilayer perceptronWithout dropout or normalization. Traces and envelopes represent the mean ± standard deviation ofthe traces of 10 runs. HCGD converges faster than SGD and generalizes better both for n = 1 andn = 10 corrections to the gradient. Both SGD and HCGD employ momentum (β) in (a) and (b), butuse no momentum in (c).
Figure 2: Test and train accuracy on MNIST digit classification for a dense multilayer perceptron,plotted against the total passes performed (where total passes = inference passes + backprop passes).
Figure 3: Test and train accuracy on MNIST digit classification for a convolutional neural networktrained with Batch Normalization. Traces and envelopes represent the mean ± standard deviation ofthe traces of 10 runs.
Figure 4: Results of a Squeezenet v1.1 trained on CIFAR10. The learning rate is decreased bya factor of 10 at epoch 150. HCGD requires a higher learning rate to a achieve a similar level ofgradient noise, which is important to decrease overfitting.
Figure 5: The cumulative squared distance traveled through L2-space, top row, and through param-eter space, bottom row, for an MLP trained on MNIST. It can be seen that SGD continues to drift inL2-space during the overfitting regime, while HCGD plateaus. This is true for networks trained withmomentum, left column, and without momentum, right column. Note that momentum significantlydecreases the scale of distance traveled. Individual traces represent random seeds. We measuredistance in L2-space in the same manner as in the HCGD algorithm: by registering the Euclideandistance between the network’s outputs on a single validation batch before and after an update.
