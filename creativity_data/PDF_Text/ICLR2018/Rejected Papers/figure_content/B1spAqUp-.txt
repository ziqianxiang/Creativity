Figure 1: Comparison of semantic segmentation results. The first and second rows are imagesand ground true labels, respectively. The third and fourth rows are the results of using regulardeconvolution and our proposed pixel deconvolution PixelDCL, respectively.
Figure 2: Illustration of 1D deconvolutional operation. In this deconvolutional layer, a4× 1 featuremap is up-sampled to an 8×1 feature map. The left figure shows that each input unit passes throughan 1×4 kernel. The output feature map is obtained as the sum of values in each column. It can beseen from this figure that the purple outputs are only related to (1, 3) entries in the kernel, whilethe orange outputs are only related to (2, 4) entries in the kernel. Therefore, 1D deconvolution canbe decomposed as two convolutional operations shown in the right figure. The two intermediatefeature maps generated by convolutional operations are dilated and combined to obtain the finaloutput. This indicates that the standard deconvolutional operation can be decomposed into multipleconvolutional operations.
Figure 3: Illustration of 2D deconvolutional operation. In this deconvolutional layer, a4×4 featuremap is up-sampled to an 8 × 8 feature map. Four intermediate feature maps (purple, orange, blue,and red) are generated using four different convolutional kernels. Then these four intermediatefeature maps are shuffled and combined to produce the final 8×8 feature map. Note that the fourintermediate feature maps rely on the input feature map but with no direct relationship among them.
Figure 4: Illustration of the checkerboard problem in semantic segmentation using deconvolutionallayers. The first and second rows are the original images and semantic segmentation results, respec-tively.
Figure 5: Illustration of iPixelDCL and PixelDCL described in section 2.2. In iPixelDCL, thereare additional dependencies among intermediate feature maps. Specifically, the four intermediatefeature maps are generated sequentially. The purple feature map is generated from the input featuremap (blue). The orange feature map is conditioned on both the input feature map and the purplefeature map that has been generated previously. In this way, the green feature map relies on the inputfeature map, purple and orange intermediate feature maps. The red feature map is generated basedon the input feature map, purple, orange, and green intermediate feature maps. We also proposeto move one step further and allow only the first intermediate feature map to depend on the inputfeature map. This gives rise to PixelDCL. That is, the connections indicated by dashed lines areremoved to avoid repeated influence of the input feature map. In this way, only the first feature mapis generated from the input and other feature maps do not directly rely on the input. In PixelDCL,the orange feature map only depends on the purple feature map. The green feature map relies onthe purple and orange feature maps. The red feature map is conditioned on the purple, orange, andgreen feature maps. The information of the input feature map is delivered to other intermediatefeature maps through the first intermediate feature map (purple).
Figure 6: An efficient implementation of the pixel deconvolutional layer. In this layer, a 4×4feature map is up-sampled toa8 × 8 feature map. The purple feature map is generated through a3 × 3convolutional operation from the input feature map (step 1). After that, another 3×3 convolutionaloperation is applied on the purple feature map to produce the orange feature map (step 2). Thepurple and orange feature maps are dilated and added together to form a larger feature map (step3). Since there is no relationship between the last two intermediate feature maps, we can apply amasked 3×3 convolutional operation, instead of two separate 3×3 convolutional operations (step 4).
Figure 7: Sample segmentation results on the PASCAL 2012 segmentation dataset using trainingfrom scratch models. The first and second rows are the original images and the correspondingground truth, respectively. The third, fourth, and fifth rows are the segmentation results of modelsusing deconvolutional layers, iPixelDCL, and PixelDCL, respectively.
Figure 8: Sample segmentation results on the MSCOCO 2015 detection dataset using training fromscratch models. The first and second rows are the original images and the corresponding groundtruth, respectively. The third, fourth, and fifth rows are the segmentation results of models usingdeconvolutional layers, iPixelDCL, and PixelDCL, respectively.
Figure 9: Sample face images generated by VAEs when trained on the CelebA dataset. The firsttwo rows are images generated by a standard VAE with deconvolutional layers for up-sampling. Thelast two rows generated by the same VAE model, but using PixelDCL for up-sampling.
