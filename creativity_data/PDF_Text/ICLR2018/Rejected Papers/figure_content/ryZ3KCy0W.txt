Figure 1: A simplified version of Model R with 1 input layer (red), 3 hidden layers (green), and 1output layer (blue). The embedding layer and the input layer each has two channels: one channelfor the source node and one channel for the destination node. Only layers and their connections areshown, while the units in each layer and their connections are not shown.
Figure 2: A small skip-gram neural network model with 1 input layer (red), 1 hidden layer (green),and 1 output layer (blue), vocabulary size 4 and embedding size 2. The hidden layer uses linearunits. The output layer uses softmax units.
Figure 3: Model S with 1 input layer (red), 2 hidden layers (green), and 1 output layer (blue). Theinput layer has two channels: one channel for the source node and one channel for the destinationnode. Only layers and their connections are shown, while the units in each layer and their connec-tions are not shown.
Figure 4: Model S performs better when using 3 different embedding techniques (LLE, Model Rand node2vec) on 3 out of 4 datasets (Airport, Collaboration, Congress and Forum).
Figure 5: Model S is robust against changes of the model parameter number of hidden layers whenusing 3 different embedding techniques (LLE, Model R and node2vec) on 2 datasets (Authors andFacebook).
