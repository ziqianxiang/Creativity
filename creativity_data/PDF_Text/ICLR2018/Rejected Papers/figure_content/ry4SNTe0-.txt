Figure 1: Architecture for CTW-GAN3Under review as a conference paper at ICLR 20183.1	Nonvanishing gradients theorem for CTW- GANBearing in mind the generator vanishing gradients theorem for Improved GAN, we may ask if asimilar problem exists for our proposed CTW-GAN. In the following, we prove that our proposedmethod does not have the vanishing gradients issue on the generator, which therefore improves thetraining stability of the original Improved GAN.
Figure 2: Network architectures used for MNISTWe only tune the parameter λ = 0.1, 0.5 from two values on the MNIST dataset. We do not tuneany other parameters, such as learning rate, step size, etc.: we keep these as in the original ImprovedGAN. The results shown in Table 1 are reported with λ = 0.1, the threshold for gradient penalty is10 and ncritic = 5:From the results, we can easily see that the original improved GAN has one or two out of nineruns for training failure (unexpected high error rates and poor generate image quality). However,for our proposed method, no training failure occurs. This shows that our method improves thetraining stability indeed. On the other hand, besides making the training process more stable, ourproposed method does not reduce the classification accuracy at all, which is beyond our originalpurpose of avoiding training instability of the Improved GAN. Reasoning it, it may imply that theinformation explored by the two discriminators may be very different, thus reflecting a distinct5Under review as a conference paper at ICLR 2018Method	n=50	n=100	n=200DGN (Kingma et al., 2014)	= Virtual Adversarial (Miyato et al., 2015) Cat-GAN (SPringenberg, 2015) Skip Deep Generative Model (Maal0e et al., 2016) Ladder network (Rasmus et al., 2015) Auxiliary Deep Generative Model (Maal0e et al., 2016) Improved-GAN (including failure cases) Improved-GAN (only success cases) Ours	6.46 (±6.95)(1F1) 4.15(±2.49) 2.47(±1.37)(0F)	3.33(±0.14) 2.12 1.91 (±0.10) 1.32 (±0.07) 1.06 (±0.37) 0.96 (±0.02) 3.73(±5.62)(2F) 1.01(±0.31) 0.85(±0.12)(0F)	1.96(±3.11)(1F) 0.92(±0.13) 0.80(±0.05)(0F)Table 1: Number of incorrectly classified test examples for the semi-supervised setting on permuta-tion invariant MNIST. Results are averaged over 9 seeds. Here “nF” means the number of trainingfailure, i.e., instable training, occur during the training of Improved GAN.
Figure 3: Network architectures used for CIFAR-10: The left net (Dc); the top right (G); the bottomright (Dw).
