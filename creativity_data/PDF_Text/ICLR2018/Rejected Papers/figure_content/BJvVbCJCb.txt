Figure 1: Training diagram for a NATAC model. In this case the model is clustering MNIST digitsand uses an autoencoder loss as the auxiliary objective. The latent representation at the top of thecircle is assigned to the top-left target, even though the target at the top-right of the circle is nearer.
Figure 2: Outputs of the decoder when fed centroids from the MNIST experiment (top) and Fashion-MNIST experiment (bottom). The models used are those with the highest NMI, d = 10 and d = 9for MNIST and Fashion-MNIST respectively. The top rows show decoded images from the cen-troids of the densest clusters (cluster sizes are shown underneath). Similarly, the bottom rows showdecoded images from the least dense clusters.
Figure 3: Box plots the converged number of clusters (above) and NMI (below) of running 50training runs on the whole of 20 Newsgroups. Every run has the same hyperparameters as the bestperforming NATAC model from the previous experiments (d = 4).
Figure 4:Γ1I1BagAnkleBootCoatC Additional Discussion on Training NATAC ModelsC.1 Geometry of Latent SpaceWe experimented with using polar coordinates early on in our experiments. Rather than using eu-Clidean coordinates as the latent representation, Z is considered a list of angles θ1,θ2 …θn whereθι ∙∙∙ θn-ι ∈ [0, ∏] and θn ∈ [0,2∏]. However, We found that the models using polar geometryperformed significantly worse than those with euclidean geometry.
Figure 5: Architecture of the encoder (left) and decoder (right) used for the MNIST experiments.
