Figure 1: Gantt charts comparing pipelined synchronous model parallelism and asynchronous modelparallelism. Orange, blue, and yellow boxes correspond to forward, backward, and parameter updateoperations, respectively. The numbers in the boxes indicate instance IDs.
Figure 2: Variable-length RNN in IR and pseudocode (colors denote IR node types)Algorithm 1 Asynchronous Model-Parallel trainingwhile training not done dofor all workers in parallel doWait until a message arrivesmsg —the highest priority message from the input queueop — sink node of the message msgif msg is forward type thenCompute forward: out_msgs — op(msg)Enqueue the resulting message(s) into the queue(s) of the workers hosting the child nodes.
Figure 3: Distribution based on dynamic informationwhere communication is over a physical network affinitization will become more critical for highperformance.
Figure 4: IR graphs for (a) Gated Graph Neural Network and (b) RNN-with-replicas. The RNNCellin (a) denotes a recurrent structure (e.g. GRU, LSTM), the details of which we omit. Cond modId in(b) implements a round-robin branching to different replicas by computing (instance ID mod 3).
Figure 5: Throughput-convergence trade-off as a function of mode of parallelism and asynchronyhyper-parameters. Solid gray lines show constant convergence time trajectories; mak and mui standfor max_active_keys and min_update_interval, respectively.
Figure 6: Convergence plots.
