Figure 1: Left: The learnable policy mean and standard deviation during training for Smoothie andDDPG on a simple one-shot synthetic task. The standard deviation for DDPG is the exploratorynoise kept constant during training. Right: The reward function for the synthetic task along withits Gaussian-smoothed version. We find that Smoothie can successfully escape the lower-rewardlocal optimum. We also notice Smoothie increases and decreases its policy variance as the convex-ity/concavity of the smoothed reward function changes.
Figure 2: Results of Smoothie, DDPG, and TRPO on continuous control benchmarks. The x-axisis in millions of environment steps. Each plot shows the average reward and standard deviationclipped at the min and max of six randomly seeded runs after choosing best hyperparameters. Wesee that Smoothie is competitive with DDPG even when DDPG uses a hyperparameter-tuned noisescale, and Smoothie learns the optimal noise scale (the covariance) during training. Moreoever, weobserve significant advantages in terms of final reward performance, especially in the more difficulttasks like Hopper, Walker2d, and Humanoid. Across all tasks, TRPO is not sufficiently sample-efficient to provide a competitive baseline.
Figure 3: Results of Smoothie with and without a KL-penalty. The x-axis is in millions of en-vironment steps. We observe benefits of using a proximal policy optimization method, especiallyin Hopper and Humanoid, where the performance improvement is significant without sacrificingsample efficiency.
