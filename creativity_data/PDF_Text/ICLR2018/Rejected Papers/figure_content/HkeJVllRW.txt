Figure 1: Different approaches for kernel approximation. (a) Regular convolution, (b) Sequentialseparate kernels and (c) Mixed-shape kernels. For (b) and (c), the shape of kernels can be arbitraryshape, not limited to 1 × k or k × 1 kernels. The major difference between (b) and (c) is that (c)mixes different shapes of kernels in one layer to extract different features and then the next layer canfuse those distinct features rather than extracting one-type features once, as in (b).
Figure 2: Kernel representations of proposed SW-SC and CW-SC kernels (3 X 3 kernel), gray colordenotes the locations of trainable weights. (a) 3D-view of SW-SC kernels, and (b) 2D-view of SW-SC kernels, for a 3 X 3 convolution, the shapes are + and X. (c) 3D-view of CW-SC kernels, eachkernel either samples even- or odd-indexed feature maps for convolution.
Figure 3: Receptive fields of different convolutional kernels of a location at a layer (across multiplechannels). (a) Conventional kernels. (b) and (c) apply SW-SC kernels sequentially ((b) applies ×-shape at layer L - 2 and +-shape at layer L - 1, (c) reverses the order in (b)). (d) and (e) useSW-SC kernels. Due to the fact that feature maps from the previous layer are extracted by eithereven-indexed sparse kernels or odd-indexed sparse kernels, some uncovered receptive field can berecovered by complementary kernels (highlighted by yellow color).
Figure 4: Error rate (%), FLOPs and # of parameters on the CIFAR-10/100 (C10/C100) datasets,including baselines and our wider and deeper networks.
Figure 5: Comparison between baselines and our models for all ResNets on imageNet. Our modelsconsistently provide better accuracy and model utilization.
Figure 6: Another SW-SC kernels.
