Figure 1: Visualization of a stack of dilated causal convolutional layers with dilations 1, 2, 4, and 8.
Figure 2: Illustration of HybridNet. It generates more than one sample each time. The light-coloredsamples are generated by the long-term context model while the dark-colored samples are generatedby WaveNet.
Figure 3: Top-level Architecture of HybridNet: WaveNet produces a window of probability distri-butions; LSTMs takes a sample for the current time-step from WaveNet output and generates nextsamples using WaveNet hidden state and conditioners. The generated samples from WaveNet andLSTMs are fed to the input audio.
Figure 4: Illustration of Input and WaveNet Hidden State Embeddings for a LSTM Unit. The outputsample from WaveNet is conditioned locally with conditioners corresponding to the current time-step. WaveNet hidden state containing long-term context is embedded into the conditioned input.
Figure 5: Illustration for Training of a 3-sample Prediction HyrbidNet. Input audio and conditionersare shifted for LSTMs.
Figure 6: Validation Error vs. Inference Time. Change the number of layers in WaveNet and theWaveNet component in HybridNet changes the inference time and validation error. For a giveninference time, HybridNet always outperform WaveNet in terms of validation error.
Figure 7: Validation Error vs. Number of Layers in WaveNet. Change the number of layers inWaveNet and the WaveNet component in HybridNet changes validation error. A HybridNet with an-layer WaveNet component always outperforms a n-layer WaveNet in terms of validation error.
