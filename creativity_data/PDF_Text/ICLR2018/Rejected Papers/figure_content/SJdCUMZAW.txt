Figure 1: Simulation rendering of the Lego task in different completion stages (also correspondingto different subtasks): (a) starting state, (b) reaching, (c) grasping, and (d) stackingThis paper makes several contributions: 1. We build on the Deep Deterministic Policy Gradient(DDPG; (Lillicrap et al., 2016)), a general purpose model-free reinforcement learning algorithm forcontinuous actions, and extend it in two ways: firstly, we improve the data efficiency of the algorithmby scheduling updates of the network parameters independently of interactions with the environ-ment. Secondly, we overcome the computational and experimental bottlenecks of single-machinesingle-robot learning by introducing a distributed version of DDPG which allows data collection andnetwork training to be spread out over multiple computers and robots. 2. We show how to use thesestraightforward algorithmic developments to solve a complex, multi-stage manipulation problem.
Figure 2: Left: (a,b) Mean episode return as a function of number of transitions seen (in millions)of DPG-R (single worker) on the Grasp (left) and StackInHand (right) task with 1 (blue), 5 (green),10 (red), 20 (yellow) and 40 (purple) mini-batch updates per environment step. Right: (c,d) Meanepisode return as a function of number of transitions seen (in millions) of ADPG-R (16 workers) onthe Grasp (c) and StackInHand (d) task. Same colors as in (a,b).
Figure 3: Data-efficiency and computational efficiency of ADPG-R. Left: Performance of 16 work-ers vs single worker in terms of environment transitions (x-axis is millions of transitions; total for allworkers) for Grasp and StackInHand tasks. Right: Performance as a function of “wallclock” time(per-worker). Both with best replay step and learning rate selection.
Figure 4: Effect of different reward shaping strategies and starting state distributions for the com-posite Stack task. Left to right: (a) No reward shaping; (b,c,d) reward shaping as explained in maintext. Colors indicate starting states: Both bricks on the table (blue); manually defined initial states(green); and initial states continuously on solution trajectories (red). On all plots, x-axis is millionsof transitions of total experience and y-axis is mean episode return. Policies with mean return over100 robustly perform the full Stack from different starting states. Without reward shaping and basicstart states only (a, blue) there is no learning progress. Instructive start states allow learning evenwith very uninformative sparse rewards indicating only overall task success (a,red).
