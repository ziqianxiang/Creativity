Figure 1: Example of learned representation space created by AANNThe AANN transforms the n-dimensional input data into a space whose number of dimensions areequal to the number of labels used in the training dataset. For instance, presume that, the task isto classify images of cats and dogs and there is a labelled dataset present for achieving this clas-sification. So, the learned representations will contain two dimensions corresponing to each label:cat and dog. The input images are transformed into 2-dimensional vectors by the AANN in such away that the vectors are as close as possible to their ideal axes. This is achieved by constructing thecost function in a manner that it maximises the cosine value of the angle formed by the vector withits ideal axis. As a result, the representation space generated by this AANN can be visualized asshown in the Figure 1. The label axes in the representation space are mutually orthogonal; thus theresulting representation vectors become very interpretable.
Figure 2: Bidirectional artificial neuron: the building block of an AANN.
Figure 3: Forward pass of the AANN.
Figure 4: Reverse pass of the AANN.
Figure 5: Images generated in the reverse direction by different activation function settings of anAANN. (a) Use of ReLU activation function. (b) Linear activation function. (c) ReLU in the forwarddirection and Abs in the backward direction. (d) Abs forward and ReLU backward. (e) Use ofSigmoid activation function.
Figure 6: Cost plots obtained upon training the AANN on the MNIST digit dataset. (a) Forwardcost. (b) Reverse cost. (c) Final cost.
Figure 7: Outputs generated by the AANN in the reverse direction. (a) Original images fed into thenetwork. (b) Images reconstructed by the network in the reverse direction.
