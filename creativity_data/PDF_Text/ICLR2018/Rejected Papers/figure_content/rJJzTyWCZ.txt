Figure 1: Model and human’s performance on questions with different types. LM and 1B-LM denotelanguage model trained on our dataset and the 1-Billion-Word corpus respectively. Our final model isintroduced in Section 4.2. Training on large external corpus leads to improvements on all categories,showing that a large amount of data leads to substantial improvement in learning complex languageregularities. When the human only has access to the context of one sentence, 1-billion-language-model is close to human’s performance on most categories. Note that the accuracies on singlecategories may have high variance because of the relative small size of samples in each category.
Figure 2: Representativeness prediction for each word. Lighter color means less representative. Thewords deleted by human as blanks are in bold text.
