Figure 1:	Profiles of three test inputs under adversarial training. White corresponds to a high-confidence correct classification (Pt1 ,t2 ≈ 1) and black to an incorrect classification with highconfidence (Pt1,t2 ≈ 0). Note that MaxMin corresponds to first taking the maximum over each row,and then the minimum over the resulting column. MinMax, in contrast, takes first the minimum overthe columns and then the maximum over the resulting row.
Figure 2:	Illustration of the cost change when transferring a perturbation to other inputs from thetraining set (“sharedness”) vs. cost change of a perturbation for future classifier parameters (“stabil-ity”) under adversarial training for the same input. Please note the log-scales.
Figure 3:	Accuracy under universal perturbations for ε = 10. Shown is the accuracy of the univer-sal perturbation generated for the model from the respective epoch as well as the accuracy of theuniversal perturbations generated for the model in epoch 0, 5, and 51.
Figure 4: Illustration of universal perturbations for ε = 10 generated for different epochs of adver-sarial training. The perturbations are amplified by a factor of 10 for readability.
Figure 5: Destruction rate of adversarial perturbations for 4 kind of transformations: additive bright-ness changes, multiplicative contrast changes, Gaussian blurring, and additive Gaussian noise. Re-sults are for basic iterative adversary, results for other adversaries can be found in Appendix F.
Figure 6: Detectability of adversarial perturbations on test data. Shown is the performance of detec-tors trained on the respective epoch as well as the performance of detectors trained in fixed epochs.
Figure 7: Illustration of perturbations that maximally activate a detector on the training data (forε = 10). The perturbations are amplified by a factor of 10 for readability. Shown is also the meanprobability of an input with the perturbation being adversarial predicted by the detector.
Figure 8: Learning curve during adversarial training.
Figure 9: Error rate of white-box and black-box attacks using BI on the classifier during adversarialtraining on training data (left) and test data (right). ”Respective epoch” denotes a white-box attackwhere the perturbation is generated on the model which is also the target model to be fooled. Theother settings denote black-box attacks, where the perturbations are generated on an other modellearned during the course of adversarial training. The error rate is evaluated on the same 256 inputssampled randomly from the test data.
Figure 10: Scatter plot of MinMax versus MaxMin on 256 test inputs.
Figure 11: Sharedness of adversarial perturbations on test data during adversarial training.
Figure 12: Accuracy on test data for universal perturbations with = 10 for perturbations generatedon training and test data. Note that both lines are nearly identical.
Figure 13: Illustration of universal perturbations forε = 10 generated on test data. The perturbationsare amplified by a factor of 10 for readability.
Figure 14: Destruction rate of adversarial perturbations of FGSM adversary for4 kind of transforma-tions: additive brightness changes, multiplicative contrast changes, Gaussian blurring, and additiveGaussian noise.
Figure 15: Destruction rate of adversarial perturbations of DeepFool adversary for 4 kind of trans-formations: additive brightness changes, multiplicative contrast changes, Gaussian blurring, andadditive Gaussian noise.
Figure 16: Detectability of adversarial perturbations on test data. Shown is the performance ofdetectors trained on the respective epoch as well as the performance of detectors trained in fixedepochs. Upper plot: Detectability of FGSM. Bottom plot: Detectability of DeepFool. The evaluationof the detectability of DeepFool has been restricted to the “respective epoch” setting because of thelarge computational cost of computing the respective perturbations.
Figure 17: Learning curve on GTSRB for a residual net. First 25 epochs are vanilla training, there-upon we perform adversarial training with ε = 8.
Figure 18: Accuracy under universal perturbations for ε = 20 on GTSRB. Shown is the accuracy ofthe universal perturbation generated for the model from the respective epoch.
Figure 19: Illustration of universal perturbations for ε = 20 generated for different epochs of adver-sarial training. The perturbations are amplified by a factor of 5 for readability.
Figure 20: Destruction rate of adversarial perturbations for4 kind of transformations on GTSRB: ad-ditive brightness changes, multiplicative contrast changes, Gaussian blurring, and additive Gaussiannoise. Results are for basic iterative adversary.
Figure 21: Detectability of adversarial perturbations on GTSRB. Shown is the performance of detec-tors trained on the respective epoch as well as the performance of detectors trained in fixed epochs.
Figure 22: Learning curve on GTSRB for a convolutional, non-residual net. First 25 epochs arevanilla training, thereupon we perform adversarial training with ε = 8.
Figure 23: Accuracy under universal perturbations for ε = 20 on GTSRB for a convolutional, non-residual net. Shown is the accuracy of the universal perturbation generated for the model from therespective epoch.
Figure 24: Illustration of universal perturbations for ε = 20 generated for different epochs of adver-sarial training for a convolutional, non-residual net. The perturbations are amplified by a factor of 5for readability.
Figure 25: Destruction rate of adversarial perturbations for 4 kind of transformations on GTSRBfor a convolutional, non-residual net: additive brightness changes, multiplicative contrast changes,Gaussian blurring, and additive Gaussian noise. Results are for basic iterative adversary.
Figure 26: Detectability of adversarial perturbations on GTSRB for a convolutional, non-residualnet. Shown is the performance of detectors trained on the respective epoch as well as the perfor-mance of detectors trained in fixed epochs. Results are for basic iterative adversary, results for otheradversaries can be found in Appendix G.
