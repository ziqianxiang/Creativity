Figure 1: (a) Training error, (b) Validation error across epochs for different activation functions (reluand clipping) with and without quantization for the ResNet20 model using the CIFAR10 datasetfunctions, on the other hand, do not have any trainable parameters, and therefore the errors arisingfrom quantizing activations cannot be directly compensated using back-propagation.
Figure 2:	Evolution ofα values during training using a ResNet20 model on the CIFAR10 dataset.
Figure 3:	Cross-entropy vs α for SVHN image classification.
Figure 4: (a-e) Training and valid error with different bit-precision for various CNNs. (f) Comparisonof accuracy degradation for ResNet18 (left) and ResNet50 (right). The lower the better.
Figure 5: Comparison of accuracy degradation (Top-1) for (a) AlexNet, (b) ResNet18, and (c)ResNet50.
Figure 6: (a)System architecture and parameters, (b) Variation in MAC area with bit-precision and(b)	Speedup at different quantizations for inference using ResNet50 DNNNext, to translate the reduction in area to improvement in overall performance, we built a precision-configurable MAC unit, whose bit precision can be modulated dynamically. The peak computecapability (FLOPs) of the MAC unit varied such that we achieve iso-area at each precision. Note thatthe total on-chip memory and external bandwidth remains constant at all precisions. We estimatethe overall system performance using DeepMatrix, a detailed performance modelling framework forDNN accelerators (Venkataramani et al.).
Figure 7: (a) Training error and (b) validation error of PACT for ResNet20 model on the CIFAR10dataset. Note that the convergence curve for PACT is almost identical to ReLU, although the dynamicrange via trained clipping levels are much lower than ReLU.
Figure 8: Experiment on CIFAR10-ResNet20 to validate that PACT balances clipping and quanti-zation errors. (a) Trade-off between clipping and quantization error. (b) PACT achieving lowestvalidation error that clipping activation can achieve without exhaustive search over clipping level α.
Figure 9: Training and validation error of CIFAR10-ResNet20 for PACT with different scope of α.
Figure 10: Training and validation error of quantized CIFAR10-ResNet20 for PACT with differentregularization parameter λα.
Figure 11: Comparison of accuracy of CIFAR10-ResNet20 with and without quantization of the firstand last layers.
