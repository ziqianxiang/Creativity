Figure 1:	Stronger regularization can help obtain faster convergence and better test performance.
Figure 2:	Neural network policies both have increasing performance with increasing number oftraining data, while models with regularization have faster convergence rate and better performance.
Figure 3: Results from different training schemes suggest that separately minimizing reweightedloss and divergence is better compared to training the two losses together.
Figure 4: a) The decreasing stochasticity of h0 makes it harder to obtain an improved NN policy,and our regularization can help the model be more robust and achieve better generalization perfor-mance. b) As h0 improves, the models constantly outperform the baselines, however, the difficultyis increasing with the quality of h0. Note: more visualizations of other metrics can be found in theappendix 7.
Figure 5: As the logging policy becomes more deterministic, NN policies are still able to findimprovement over h0 in a) expected loss and b) loss with MAP predictions. c) We cannot observea clear trend in terms of the performance of MAP predictions. We hypothesize it results from thath0 policy already has good MAP prediction performance by centering some of the masses. WhileNN policies can easily pick up the patterns, it will be difficult to beat the baselines. We believe thisphenomenon worth further investigation.
Figure 6: a) As the quality of the logging policy increases, NN policies are still able to find improve-ment over h0 in expected loss. and b) c) For MAP predictions, however, it will be really difficult forNN policies to beat if the logging policy was already exposed to full training data and trained in asupervised fashion.
