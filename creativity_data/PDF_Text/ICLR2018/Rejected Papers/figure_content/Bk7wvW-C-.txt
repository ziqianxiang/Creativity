Figure 1: Our proposed model is composed of an RNN encoder, and a CNN decoder. During training,a batch of sentences are sent to the model, and the RNN encoder computes a vector representationfor each of sentences; then the CNN decoder needs to reconstruct the paired target sequence, whichcontains 30 contiguous words right after the input sentence, given the vector representation. 300 isthe dimension of word vectors. D is the dimension of sentence representation, and it varies alongwith the change of the RNN encoder size. (Better view in color.)sentence, and the performance on the downstream tasks is similar to that of their implementation ofthe Skip-thought model.
