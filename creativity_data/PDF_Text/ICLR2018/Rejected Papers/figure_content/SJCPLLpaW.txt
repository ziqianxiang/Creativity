Figure 1:	Relative performance for trainingdifferent convolutional layers. Computationthroughput is calculated by dividing the batchsize with computation time (both forward pro-cessing and back propagation) and is normal-ized by the worst case.
Figure 2:	Computation and data transfer timeto process a batch of 512 images using imageparallelism for the third layer, an intermediatelayer, and the last layer of Inception-v3.
Figure 3: Different configurations for parallelizing the first fully-connected layer of VGG-16. Rect-angles with solid lines indicate tensors managed by the local GPU, while rectangles with dot linesare tensors managed by a remote GPU. The shadow rectangles indicate data transfers for each step.
Figure 4: Example configurations that parallelize an operation in a single dimension or combinationsof multiple dimensions. The figure shows how each image is partitioned in different configurations.
Figure 5:	Performing a node/edge elimination on a computation graph.
Figure 6:	Iteratively performing node/edge eliminations on an Inception module.
Figure 7: Training throughput (images/second)for AlexNet, VGG-16, and Inception-v3 on 16GPUs. The purple and green bar shows theDeePa performance by restricting DeePa to useimage and OWT parallelism, respectively.
Figure 8: Total amount of data transferred ineach step for training AlexNet, VGG-16, andInception-v3 on 16 GPUs with a minibatch sizeof 512.
Figure 9:	The global configuration for parallelizing AlexNet on 16 GPU workers.
Figure 10:	The global configuration for parallelizing VGG-16 on 16 GPU workers.
Figure 11: The global configuration for parallelizing Inception-v3 on 16 GPU workers. Each moduleis shown as a single node for simplicity.
Figure 12: The configurations for parallelizing the InceptionE1 module.
Figure 13: Performance comparisons among DeePa, PyTorch, and TensorFlow with different mini-batch sizes.
Figure 14: Performance results for DeePa, PyTorch, and TensorFlow on up to 4 nodes. We performweak-scaling, where each GPU worker processes a batch of 32 images in every iteration. We use all4 GPUs on each node in the last two subfigures.
Figure 15: Timelines for training VGG-16 with a minibatch size of 128 images on 4 GPUs. The firsthorizontal line in each figure shows the overall GPU utilization at different timesteps, and each ofthe following lines shows the run times for individual operations on each GPU.
Figure 16: Performance comparisons among DeePa, PyTorch, and TensorFlow on the ImageNet-22kdataset.
