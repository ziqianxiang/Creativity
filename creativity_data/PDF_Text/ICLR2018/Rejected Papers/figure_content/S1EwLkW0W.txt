Figure 1: Conceptual sketch of variance adaptation, ignoring the sign aspect of adam. The left panelshows the true gradient VL = (2,1) and stochastic gradients scattered around it with (σι, σ2)=(1, 1.5). In the right panel, we employ a variance adaptation (to be derived in §3.2) that scales thei-th coordinate by (1 + ηi2)-1. In this example, the θ2-coordinate has much higher relative variance(η22 = 2.25) than the θ1-coordinate (η12 = 0.25) and is thus shortened. This reduces the variance ofthe update direction at the expense of biasing it away from the true gradient in expectation.
Figure 2: Performance of sgd and ssd on 100-dimensional stochastic quadratic problems. Rowscorrespond to different QPs: the eigenspectrum is shown and each is used with a randomly rotatedand an axis-aligned eigenbasis. Columns correspond to different noise levels. Horizontal axis isnumber of steps; vertical axis is log function value and is shared per row for comparability.
Figure 3: Experimental results on the three test problems. Plots display training and test loss overthe number of steps. Curves for the different optimization methods are color-coded. The shaded areaspans plus/minus one standard deviation, obtained from ten replications. The table below containstest accuracies evaluated after the last iteration.
Figure 4: Probability density functions (pdf) of three Gaussian distributions, all with μ = 1, butdifferent variances σ2 = 0.5 (left), σ2 = 1.0 (middle), σ2 = 4.0 (right). The shaded area under thecurve corresponds to the probability that a sample from the distribution has the opposite sign than itsmean. For the Gaussian distribution, this probability is uniquely determined by the fraction σ∕∣μ∣,as shown in Lemma 2.
Figure 5: Variance adaptation factors asfunctions of the relative standard deviationη. (1 + η2)-1 is the optimal variance adap-tation factor for sgd (Eq. 16). The optimalfactor for the sign of a stochastic gradient iserf((√2η)-1) under the GaUSSian assump-tion (Eq. 15). It is closely approximated by(1 + η2 )-1/2 , which is the factor implicitlyemployed by adam (Eq. 6).
Figure 6:	Comparison of the original adam algorithm to the variants in Algs. 4 and 5. Set-up of theplots as in Fig. 3. All three algorithms exhibit very similar performance on both problems.
Figure 7:	Comparison of the two variants of the m-svag algorithm. Set-up of the plots as in Fig. 3.
