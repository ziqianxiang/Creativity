Figure 1: Left: A discrete vector v ∈ Rl×w representation of an image. Right: The true continuousfunction f : R2 → R from which it was sampled.
Figure 2: Left: Resolution refinement of an input signal by simple functions. Right: An illustrationof the extension of neural networks to infinite dimensions. Note that x ∈ RN is a sample of f(N), asimple function with kf (N) - ξk → 0 as N → ∞. Furthermore, the process is not actually countable,as depicted here.
Figure 3: Examples of three different deep function machines with activations ommited and T'replaced with the actual type. Left: A standard feed forward binary classifier (without convolution),Middle: An operator neural network. Right: A complicated DFM with residues.
Figure 4: DFM construction of resolu-tion invariant n-discrete layer.
Figure 5: The WaveLayer architecture of RippLeNet for MNIST. Bracketed numbers denote numberof wave coefficients. The images following each WaveLayer are the example activations of neuronsafter training given by feeding 000 into RippLeNet.
Figure 6: A plot of test error versus number oftrainable parameters for RippLeNet in comparisonwith early LeNet architectures.
Figure 7: A plot of training time (normalized foreach layer type with respect to the training timefor 28 × 28 baseline) as the resolution of MNISTscales.
