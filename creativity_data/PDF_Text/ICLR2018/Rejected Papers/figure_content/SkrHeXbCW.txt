Figure 1: Analytically and empirically estimated costs of an approximate nearest neighbor search asa function of the angle between the query and the nearest neighbor. The analytical cost is given as thenumber of distance computations between the query point and the candidate points returned by theLSH table. The empirical cost is given as the total time per query on a laptop computer from 2015(CPU: Intel Core i7-4980HQ).
Figure 2: The effect of batch normalization on the evaluation accuracy, mean correct angle, and normof the representation (pre-softmax) vectors. The plots show two training runs on our multiMNISTdataset with 1,000 classes. The norm of the representation vectors is now nearly constant, and themean correct angle improves significantly. The normalization also improves training speed.
Figure 3: The effect of swapping ReLU and batch normalization before the softmax layer. Theplots show two training runs on our multiMNIST dataset with 10,000 classes. The norm of therepresentation vectors show less variation during training, and the mean correct angle improves. Thenormalization also improves training speed.
Figure 4: The effect of normalizing the class vectors in the softmax layer. The plots show two trainingruns on our multiMNIST dataset with 10,000 classes. The average norm of class vectors is nowconstant and significantly smaller than before. Moreover, the mean correct angle improves while theaccuracy is not negatively affected. Importantly, the positive effects are cumulative with the previousmodification of swapping the order of ReLU and batch normalization (we are not comparing to abaseline without batch normalization because we could not train a 10,000 class multiMNIST datasetwithout batch normalization).
Figure 5:	The effect of our training modifications on a multiMNIST_100K dataset. The mean correctangle improves significantly while the evaluation accuracy is not affected negatively.
Figure 6:	The effect of our training modifications on the Sporst1M dataset (the legend is the same asin Figure 5 above). The mean correct angle is significantly smaller. The evaluation accuracy alsoimproves. Moreover, the points remain evenly spread over the unit sphere, i.e., the “table balance”quantity M(P, h)/M* remains close to 1, especially for the best-performing combination of ourtechniques.
Figure 7:	Effect of our training modifications on the query times of nearest neighbor algorithms. Wereport the relative accuracies, i.e., the probability of finding the correct nearest neighbor conditionedon the model being correct. For LSH as implemented in the FALCONN library (left), our trainingyields a 5× speed-up in the relevant high accuracy regime. The variant of kd-trees implemented inthe Annoy library (right) does not reach relative accuracy 1 when the softmax is trained using thestandard approach. In contrast, the softmax resulting from our training techniques is more amenableto the kd-tree algorithm. Again, we obtain faster query times for fixed accuracy.
Figure 8: Visualization of our training modifications for a two-dimensional softmax trained on asynthetic dataset. Each column shows the effect of one modification. The top row contains thebaseline result without the respective training modification and the bottom row shows the result withmodification.
Figure 10: Comparison of various Normalization Options on MultiMNIST 1KFigure 9: An example from multiMNIST_100k; label 73536.
Figure 9: An example from multiMNIST_100k; label 73536.
