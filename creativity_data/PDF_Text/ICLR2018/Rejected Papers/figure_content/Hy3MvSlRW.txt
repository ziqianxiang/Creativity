Figure 1: Adversarial learning protocolAlgorithm 1 Pseudo-code of the adversarial trainingSplit dataset into 3 pieces (A) train (80%), (B) valid (10%) and (C) test (10%)Create D an empty datasetepoch = 0while epoch < NB-MAX卫POCHS doSplit A into A1 (80%) and A2 (20%)if epoch = 0 thenRandomly corrupt 20% of A1 and 100% of A2elseReinitialize all the parameters of the narratorTrain the narrator on DThe narrator corrupts 20% of A1 and 100% of A2end ifTrain one epoch of the reader on A1Let A2_clear be the dataset that contains the same data as in A2 but without corruptionTest the reader on A2 and on A2 _clearfor all (d ∈ A2, d.clear ∈ A2_clear) doLet r be the reward given to the narratorif The reader succeed on d.clear and fails on d then
Figure 2: Narrator output distribution after 100 rounds over a Cambridge dialog.
Figure 3: Reader attentionsconsequence of the encoding which is not only a representation of a word but a representation of aword in its context.
