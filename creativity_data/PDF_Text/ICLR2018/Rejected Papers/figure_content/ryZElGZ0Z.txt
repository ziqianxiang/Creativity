Figure 1: The inputs—observations ot+i andGVF predictions Pt from the last time step-passthrough a nonlinear expansion, such as a fixed neu-ral network or tile coding, producing the featurevector pt+i. The feature vector is weighted lin-early to produce the next set of predictions Pt+i.
Figure 2: (a) An example trajectory for a single run on the robotic platform. The True Return iscomputed after collecting the data, and the blue and red lines corresponds to the predictions of theexpected return, from that given time step. (b) We test how much the magnitude of the weights isspread across the features. With more spread, it becomes more difficult to identify the importantfeatures. The solid lines depict the number of features with weight magnitude above the threshold0.001/the number of active features. AdaGain-R spreads out values less, choosing to place highermagnitude on a subset of features. Nonetheless, AdaGain-R does not suffer in performance, as shownin the dotted lines corresponding to the y-axis on the left.
Figure 3: (a) Prediction error on the five difficult GVFs for variously sized networks. Solid linescorrespond to networks where compositional GVFs were not generated, and dotted to networks wherecompositional GVFs were generated. Using only the observations (black line) results in almost nolearning. (b) Composition of GVF network after 100 million steps of learning.
Figure 4: The progression of improved stability, with addition of components of our system. Evenwithout a nonlinear transformation, AdaGain can maintain stability in the system (in (a)), though themeta-step size needs to be set more aggressively small. Interestingly, the addition of regularizationsignificantly improved stability, and even convergence rate (in (b)). The seventh GVF is used moreaggressively after Phase 1—once it becomes useful. The addition of a nonlinear transformation (in(c)) then finally lets the system react quickly once the seventh GVF becomes accurate. Again, withoutregularization, the magnitude of weights is more spread out, but otherwise the performance is almostexactly the same as (c). For both (b) and (c), the meta-parameters are η = 0.1, = 0.01 normalizedwith the number of features.
Figure 5: 45 expert GVFs and 155 random GVFs, pruning 20 GVFs every 2 million steps averaged over 10runs.
