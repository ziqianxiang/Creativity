Figure 1: Generating block-sparse masks from a weight matrixTable 1: Heuristics to pick hyper-parameters for block-pruningHYPER-PARAM	DESCRIPTION	HEURISTIC VALUESStartdtr ramp_itr enddtr	Iteration to start pruning	Start of second epoch Iteration to increase the rate of pruning	Start of 20% of total epochs Iteration to stop pruning more parame-	Start of 40% of total epochs tersStartslope θ	Initial rate of increasing the threshold	See Equation 2(θ) ramp slope (φ) freq	Rate of increasing threshold after ramp	1.2θ to 1.7θ iteration Number of iterations after which is 100 updatedNarang et al. (2017) use six hyper-parameters to determine the threshold at a given iteration. Table 1provides the description and heuristics (adapted for block pruning) for these hyper-parameters. Thestartslope and ramp_SloPe determine the rate at which the threshold increases. In order to determineStartslope, they recommend using weights from an existing dense model. To achieve 90% sparsity,they assign q to the weight that is the 90th percentile of the absolute values in a weight matrix.
Figure 2: Speed-up for sparse matrix multiply over GEMM. RNN matrix sizes are (1760,1760)with 90% sparsity and (1760, batch_size). GRU matrix sizes are (7680,2560) with 95% sparsity and(2560, batch_size). Block-sparse matrices achieve consistently good speedup across batch-sizesFigure 3: Relative accuracy for different block sizes (4x4, 16x16) and WP for varying sparsity onthe RNN 1760 model. Any models with relative accuracy worse than -75% are capped at 75%.
Figure 3: Relative accuracy for different block sizes (4x4, 16x16) and WP for varying sparsity onthe RNN 1760 model. Any models with relative accuracy worse than -75% are capped at 75%.
Figure 4: Figure 4a shows the pruning schedule for two layers in the network for WP, GLP and BPmodels. The GLP and BP models use block size of 4x4. Figure 4b plots the histogram of the numberof output connections for all neurons in the network using block pruning with 4×4 blocks.
Figure 5: Sparsity of different recurrent layers in the network in the RNN model, pruned using BPand WP.
