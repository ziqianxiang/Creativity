Figure 1: Illustration of post-training applied to a neural network. During the post-training, only theweights of the blue edges are updated. The blue nodes can be seen as the embedding of x in thefeature space XL .
Figure 2: Illustration of the neural network structure used for CIFAR-10. The last layer, representedin blue, is the one trained during the post-training. The layers are composed with classical layers:5x5 convolutional layers (5x5 conv), max pooling activation (max pool), local response normaliza-tion (lrn) and fully connected linear layers (fc).
Figure 3: Evolution of the performances of the neural network on the CIFAR10 data set, (dashed)with the usual training and (solid) with the post-training phase. For the post-training, the value ofthe curve at iteration q is the error for a network trained for q - 100 iterations with the regulartraining strategy and then trained for 100 iterations with post-training. The top figure presents theclassification error on the training set and the bottom figure displays the loss cost on the test set. Thecurves have been smoothed to increase readability.
Figure 4: Evolution of the performances of the Recurrent network on the PTB data set. The topfigure presents the train perplexity and the bottom figure displays the test perplexity. For the post-training, the value of the curve at iteration q is the error for a network trained for q - 100 iterationswith the regular training strategy and then trained for 100 iterations with post-training.
