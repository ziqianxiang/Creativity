Figure 1: The architecture of 3C-GAN. The σ stands for softmax normalization, and the standsfor element-wise multiplication. Note that classifier network C is reused three times for differentinput and label pairs.
Figure 2: composite generated samples xf for MNIST-BACKGROUND datasetFigure 3: generated samples from G1 for MNIST-BACKGROUND dataset3.2	Results for SVHN datasetSVHN is also a dataset with 10 different digits classes. In an image, a digit with the class label is inthe center; however, there could be digits on the side of an image that does not related to the image’slabel. When training on this dataset, we set the channel number of the model to be 64, and the inputnoise code number to be 64. There are 32 context codes (zu), 32 object codes (zv), and 10 labelcodes (zl). Therefore, the input dimensionality for G1 is 32 and for G2 is 74. α is set to 10, and β isset to 0.5. The two generators also shared all structure except the bottom layer, and model is trainedfor 100000 iterations.
Figure 3: generated samples from G1 for MNIST-BACKGROUND dataset3.2	Results for SVHN datasetSVHN is also a dataset with 10 different digits classes. In an image, a digit with the class label is inthe center; however, there could be digits on the side of an image that does not related to the image’slabel. When training on this dataset, we set the channel number of the model to be 64, and the inputnoise code number to be 64. There are 32 context codes (zu), 32 object codes (zv), and 10 labelcodes (zl). Therefore, the input dimensionality for G1 is 32 and for G2 is 74. α is set to 10, and β isset to 0.5. The two generators also shared all structure except the bottom layer, and model is trainedfor 100000 iterations.
Figure 4: generated sample from G2 for MNIST-BACKGROUND datasetFigure 5: composite generated samples xf for SVHN dataset3.3	Results for CelebA datasetThere are 20599 images on this dataset. We pre-process the dataset by resizing the images to 64-by-64. We choose the attribute of smiling as image label for the conditional generator in our modelbecause the number of images of smiling and not-smiling is similar. When training on this dataset,we set the channel number of the model to be 128, and the input noise code number to be 128 ( 64context codes (zu), 64 object codes (zv), and 2 label codes (zl). α is set to 100, and β is set to 0.5.
Figure 5: composite generated samples xf for SVHN dataset3.3	Results for CelebA datasetThere are 20599 images on this dataset. We pre-process the dataset by resizing the images to 64-by-64. We choose the attribute of smiling as image label for the conditional generator in our modelbecause the number of images of smiling and not-smiling is similar. When training on this dataset,we set the channel number of the model to be 128, and the input noise code number to be 128 ( 64context codes (zu), 64 object codes (zv), and 2 label codes (zl). α is set to 100, and β is set to 0.5.
Figure 6: generated samples from G1 for SVHN datasetFigure 7: generated sample from G2 for SVHN dataset4	ConclusionIn this paper, we present a novel GAN structure that has one generator generates the context whilethe other conditional generator generate the part of the image based on the label it has. Comparedto previous multi-generators model, our model has fewer parameters and generate the different partsimultaneously. In addition, we proposed a new cost that makes the conditional generator learns togenerate only the essential part of the condition changing. Also, we proposed an exclusive prior sothat the two generators do not generate the same pixel. Experiments show our model separated thedata as mentioned, and therefore; provide more controllability over original GAN.
Figure 7: generated sample from G2 for SVHN dataset4	ConclusionIn this paper, we present a novel GAN structure that has one generator generates the context whilethe other conditional generator generate the part of the image based on the label it has. Comparedto previous multi-generators model, our model has fewer parameters and generate the different partsimultaneously. In addition, we proposed a new cost that makes the conditional generator learns togenerate only the essential part of the condition changing. Also, we proposed an exclusive prior sothat the two generators do not generate the same pixel. Experiments show our model separated thedata as mentioned, and therefore; provide more controllability over original GAN.
Figure 8: composite generated samples xf for CelebA datasetFigure 9: generated samples from G1 for CelebA datasetTrevor Darrell and Alexander Pentland. Robust estimation of a multi-layered motion representation.
Figure 9: generated samples from G1 for CelebA datasetTrevor Darrell and Alexander Pentland. Robust estimation of a multi-layered motion representation.
Figure 10: generated sample from G2 for CelebA datasetIan Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprintarXiv:1701.00160, 2016.
