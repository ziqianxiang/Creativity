Figure 1: Verification Model: The network is fed with data pairs. gΘ is a feature extractor function.
Figure 2: gΘ is the feature extractor, which is the same gΘ in the validation model. During k-shotlearning, features of the data are calculated, and a linear multiclass SVM (figure shows one-vs-restapproach) is used with the same C parameter as the validation model’s loss function.
Figure 3: Omniglot dataset: all the characters are grayscale 105 x 105 images.
Figure 4: Feature extractor Convolutional neural network2convolution+ ReLU,64 @ 10x10Table 1: Results on the Omniglot dataset 3Model	Fine Tune	5-way Acc		20-way Acc			1-shot	5-shot	1-shot	5-shotBASELINE CLASSIFIER	N	80.0%	95.0%	69.5%	89.1%BASELINE CLASSIFIER	Y	86.0%	97.6%	72.9%	92.3%CONVOLUTIONAL SIAMESE NET	N	96.7%	98.4%	88.0%	96.5%CONVOLUTIONAL SIAMESE NET	Y	97.3%	98.4%	88.1%	97.0%MATCHING NET	N	98.1%	98.9%	93.8%	98.5%MATCHING NET	Y	97.9%	98.7%	93.5%	98.7%NEURAL STATISTICIAN Edwards & Storkey (2016)	N	98.1%	99.5%	93.2%	98.1%TCML Mishra et al. (2017)	N	99.0%	99.8%	97.6%	99.4%PROTOTYPICAL NETWORKS Snell et al. (2017)	N	98.8%	99.7%	96.0%	98.9%METANET Munkhdalai & Yu (2017)	N	99.0%	98.7%	97.1%	97.0%SIAMESE KERNEL SVM	N	98.5%	99.3%	94.0%	98.0%SIAMESE KERNEL SVM	Y	98.4%	99.2%	94.1%	98.2%4.2	TIMIT
Figure 5: Spectrograms of two different people when the same text has been read. There are observ-able differences in tempo and tone as well.
Figure 6: Feature extractor CNN for audio: used input’s resolution is 64x64 pixel.
