Figure 1: CER dependence on λrec and λnonrec for trace norm regularization (left) and `2 regular-ization (right).
Figure 2: Nondimensional trace norm coefficient versus strength of regularization by type of regu-larization used during training. On the left are the results for the non-recurrent weight of the thirdGRU layer, with λrec = 0. On the right are the results for the recurrent weight of the third GRUlayer, with λnonrec = 0. The plots for the other weights are similar.
Figure 3: The truncated SVD rank required to explain 90 % of the variance of the weight matrixversus CER by type of regularization used during training. Shown here are results for the non-recurrent (left) and recurrent (right) weights of the third GRU layer. The plots for the other weightsare similar.
Figure 4: Number of parameters versus CER of stage 2 models colored by the type of regularizationused for training the stage 1 model.
Figure 5:	Left: CER versus transition epoch, colored by the type of regularization used for trainingthe stage 1 model. Right: CER as training progresses colored by the type of regularization used instage 1. The dotted line indicates the transition epoch.
Figure 6:	Comparison of our kernels (farm) and the gemmlowp library for matrix multiplication oniPhone 7 (left), iPhone 6 (middle), and Raspberry Pi 3 (right). The benchmark computes Ax = bwhere A is a random matrix with dimension 6144 × 320, and x is a random matrix with dimension320× batch size. All matrices are in unsigned 8-bit integer format.
Figure 7: Contours of ∣∣σ∣∣'i and ∣∣σ∣∣'2. ∣∣σ∣∣'2 is kept constant at σ. For this case, ∣∣σ∣∣'i can varyfrom σ to vz2σ.
Figure 8: CER versus parameter on an internal dataset, colored by parameter reduction technique.
