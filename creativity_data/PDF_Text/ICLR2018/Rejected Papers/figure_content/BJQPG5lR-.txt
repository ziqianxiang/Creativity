Figure 1: Results on MNIST (left) and fashion-MNIST (right) for various different architecturesas the depth of the network varies from 1 to 30. Mean average test accuracy over 10 independenttraining sessions is shown. We note that with the exception of plain networks, the performance ofall remaining architectures is stable as the number of layers increases.
Figure 2: Left: Results on CIFAR-10 dataset are shown as the depth of networks increase. We notethat the performance of both VAN and plain networks deteriorates as the depth increases, but theeffect is far less pronounced for VAN networks. Right: Training and test error curves are shown fornetworks With 26 layers. We also plot the mean α residuals: L PL=I(I - αl)2 on the right axis.
Figure 3: Left: Results on CIFAR-100 dataset are shown as the depth increases from 10 to 34 layers.
Figure 4: Results are shown VAN and ResNet networks with various different residual blocks. Wenote that the use of residual blocks with non-identity scaling coefficients leads to a larger drop inperformance as the network depth increases. This drop is attributed to vanishing contributions fromlower blocks (as all scalings are less than one).
Figure 5: Lagrange multipliers, λl are shown on the left panel for networks of varying depth. After acertain number of iterations, the values of the Lagrange multiplier plateau as the constraint to removeskip-connections is satisfied. This results in no updates to the values of the Lagrange multipliers(see equation (8)). The right panel shows the mean αl residual. This residual directly modulates themagnitude of changes in Lagrange multipliers.
