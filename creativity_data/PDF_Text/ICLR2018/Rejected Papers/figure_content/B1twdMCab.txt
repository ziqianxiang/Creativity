Figure 1: Illustration of our context-dependent, refinement strategy for word representations on anexample from the SNLI dataset comprising the premise, hypothesis and additional external infor-mation in form of free-text assertions. The reading architecture constructs refinements of word rep-resentations incrementally (conceptually represented as columns in a series of embedding matrices)E' are incrementally refined by reading the input text and textual renderings of relevant backgroundknowledge before computing the representations used by the task model (in this figure, RTE).
Figure 2: Performance differences when ignoring certain types of knowledge, i.e., relation predicatesduring evaluation. Normalized performance differences are measured on the subset of examples forwhich an assertion of the respective relation predicate occurs.
