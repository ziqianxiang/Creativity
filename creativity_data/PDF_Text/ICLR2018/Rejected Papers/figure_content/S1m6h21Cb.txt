Figure 1: Leftmost. Target distribution. One outcome (10) is significantly more distant than thetwo others (0, 1). Rest. Distributions minimizing the divergences discussed in this paper, under theconstraint Q(1) = Q(10). Both Wasserstein metric and Cramer distance underemphasize Q(0) tobetter match the cumulative distribution function. The sample Wasserstein loss result is for m = 1.
Figure 2: Left. Wasserstein distance in terms of SGD updates, minimizing the true or sampleWaSSerStein losses. Also shown are the distances for the KL and Cramer solutions. Results areaveraged over 10 random initializations, with error-bands indicating one standard deviation. Center.
Figure 4: Approximate Wassersteindistances between CelebA test set andthe generators. Nu is the numbercritic updates per generator update.
Figure 5: Wasserstein loss (black curve) θ → ∣θ* - θ∣ versus expected sample Wasserstein loss (redcurve) θ → E[∣θ - θ∣], for different values of m and θ* and P = 1. Left: m = 1, θ* = 0.6. Astochastic gradient using a one-sample Wasserstein gradient estimate will converge to 1 instead ofθ*. Middle: m = 6, θ* =0.6. The minimum of the expected sample Wasserstein loss is the medianof θ which is here θ = 2 = θ* = 0.6. Right: m = 5, P = 0.9. The minimum of the expectedsample Wasserstein is θ = 1 and not θ* = 0.9.
Figure 6: WaSSerStein while training to minimize different loss functions (Wasserstein, KL, Cramer).
Figure 7: Ordinal regression on the year prediction MSD dataset. Each loss function trained withvarious minibatch sizes. Training progress shown in terms of: Left. RMSE, Middle. Wassersteindistance, Right. Negative log-likelihood.
Figure 8: Left, middle. Sample Wasserstein and cross-entropy loss curves on the CelebA validationdata set. Right. Test loss at the end of training, in function of loss minimized (see text for details).
Figure 9: Generated right halves for WGAN-GP (left) and Cramer GAN (right) for left halves fromthe validation set of Downsampled ImageNet 64x64 (Van den Oord et al., 2016). The low diversityin WGAN-GP samples is consistent with the observations of Isola et al. (2016): “the generatorsimply learned to ignore the noise.”B.2	Image Modelling with PixelCNNAs additional supporting material, we provide here the results of experiments on learning a proba-bilistic generative model on images using either the 1-Wasserstein, Cramer, or KL loss. We traineda PixelCNN model (Van den Oord et al., 2016) on the CelebA 32x32 dataset (Liu et al., 2015),which is constituted of 202,599 images of celebrity faces. At a high level, probabilistic image mod-elling involves defining a joint probability Qθ over the space of images. PixelCNN forms this jointprobability autoregressively, by predicting each pixel using a histogram distribution conditional ona probability-respecting subset of its neighbours. This kind of modelling task is a perfect settingto study Wasserstein-type losses, as there is a natural ordering on pixel intensities. This is also asetting in which full distributions are almost never available, because each prediction is conditionedon very different context; and hence we require a loss that can be optimized from single samples.
Figure 10: Left. Generated images from a generator trained to minimize the energy distance of rawimages, E(X, Y). Right. Generated images if minimizing the Cramer GAN loss, E(h(X), h(Y)).
Figure 11: Left. Inception score and FID on CIFAR-10. The Surrogate GAN is a Cramer GANwith the generator trained to minimize the surrogate loss (13). Right. Inception Energy Distanceon conditional CIFAR-10. The network conditioned on the left half of the CIFAR-10 images. Theshaded area denotes the standard deviation from 3 runs.
