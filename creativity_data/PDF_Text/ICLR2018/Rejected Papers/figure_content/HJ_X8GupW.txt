Figure 1: Latent Variable Model(HsU et al. (2012) and Song et al. (2010)), Bayesian Non-parametrics TUng & Smola (2014), TopicModels Anandkumar et al. (2012), and in various NLP applications (Dhillon et al. (2012) and Cohenet al. (2014)). We first derive a closed form expression to estimate the model parameters using theMethod of Moments. Further, we derive the convergence bounds on the estimated parameters, andthen compare the performance of our algorithm with the discriminative low-rank models both fromthe perspective of accuracy and computation time.
Figure 2: AUC and Memory vs. Latent Dimensionality (K)(d) AUC and Memory (GB) of Wiki-31K Dataset(f) AUC and Memory (GB) of WikiLSHTC DatasetDataset	LEML	MoM	Speed-up (×)Bibtex —	160s. —	300s.	0.53 —Delicious 		60s. 		150s.	0.4NYtimes	1 hour	6min. 		10 	Wiki-31K	3 hr. 40 min.	15 min.	15AmazonCat	13hrJ	1 hr. 15 min.	10WikiLSHTC	>2 days t	3hr.	16t Runtime on i2.4xlarge instance of Amazon EC2Table 3: Training TimeWe computed AUC for every test documents and performed a macro-averaging across the docu-ments, and repeated the experiments for K = {50,75,100,125,150} (Figure 2). Both LEML andMethod of Moments perform very similarly, but the memory footprint of MoM is significantly lessthan LEML. MoM takes longer to finish for the small datasets since tensor factorisation takes muchmore time compared to the LEML iterations. However, as the size of the datasets grows, the LEMLiterations become more and more costly. For the medium and large datasets, MoM takes a fractionof the time taken by LEML. For WikiLSHTC, LEML takes more than two days to finish, whileMoM finishes within a few hours. The training times of LEML and MoM for different datasets are
