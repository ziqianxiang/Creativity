Figure 1: Three simple network examples, a pure feed forward F), a skip S), and a recurrent networkR), are shown (left most column), illustrating the difference between sequential (middle column)and layerwise-parallel (right column) network execution. For both network types, inference on foursucceeding (from left to right) time frames (pictograms: empty - / - \ - empty) is drawn. Encirclednodes indicate currently updated / computed / inferred layers and grey underlayed areas indicatealready computed network parts. Pictograms (empty, /, \) inside layers indicate information fromthis stimulus in a layer at a specific time frame. To increase clarity for the layerwise-parallel case,we omitted information from previous stimuli still memorized by the network. For the layerwise-sequential recurrent network (bottom left network), we used a 1-step rollout window. Local framesfor layerwise-sequential networks differ between architectures (3 frames for F) and S), and 4 framesfor R).
Figure 2: Visualization example of a simple classification network using the provided toolbox (bestviewed in color). The network is shown as graph (green nodes are neuron-pools, blue nodes aresynapse-pools) together with information about the network.
Figure 3:	Illustration of a 2-path classification network. (a) The graph of the network.
Figure 4:	Rolled out network for maximal path length 4 and the two sub-networks used to computethe loss for the plasticities. All continuous lines represent synapse-pools which are initialized ran-domly and trained by the plasticities. All dotted lines are initialized as identity and are not trained.
