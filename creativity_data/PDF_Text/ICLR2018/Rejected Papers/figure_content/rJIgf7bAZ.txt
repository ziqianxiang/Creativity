Figure 1: Graphical Model for Option TrajectoryTτRecognizing the hidden Markov model-like structure of the trajectories shown in Fig. 1 reveals thatthe P(ωi |s[0:i] , a[0:i-1]) term can be expressed in a recursive form, simply as an application of theforward algorithm:P(ωi∣S[0.], a[θ:i-i]) = E C-IPQi-i|s[0：i-i], aRi-2])∏3i-ι,e(ai-i∣Si-i)∏Ω,θ,ξ("佃—卜 Si)ωi-1where Ci is a normalization factor, given by:Ci = EP("i-i|s[0：i-i], aRi-2])∏ωi-ι,e(ai-i∣Si-i).
Figure 2: Training curves for 2 million time steps averaged across 10 random seeds for severalcontinuous RL domains. The shaded area represents the 95% confidence interval.
Figure 3: Typical option activity as a function of state. The axes represent T-SNE embeddings ofhigher dimensional states. Each state is coloured according to the option that was active at thatpoint in the trajectory. We can see that the options learned by Option-Critic (Right) are not visiblycorrelated in state-space, while options learned by IOPG (Left) are.
Figure 4: Analysis of the learned options in the Walker2d environment.
