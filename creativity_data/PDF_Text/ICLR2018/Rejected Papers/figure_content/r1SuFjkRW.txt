Figure 1: Demonstration of a transformed environment with three dimensional action space. Newstates, u are introduced to keep the action dimension at each transition one dimensional. The values ofthese states are shown bellow the circles. Each circle represents a state in the MDP. The transformedenvironment’s replicated states are now augmented with the previously selected action. When allthree action dimensions are chosen, the underlying environment progresses to st+1. Equality of Qvalues is noted where marked with vertical lines.
Figure 2: Left: Final reward/Q surface for each algorithm tested. Final policy is marked with a green×. Policies at previous points in training are denoted with red dots. The SDQN model is capable ofperforming global search and thus finds the global maximum. The top row contains data collecteduniformly over the action space. SDQN and DDPG use this to accurately reconstruct the target Qsurface. In the bottom row, actions are sampled from a normal distribution centered on the policy.
Figure 3: Learning curves of highest performing hyper parameters trained on Mujoco tasks. Weshow a smoothed median (solid line) with 25 and 75 percentiles range (transparent line) from the 10random seeds run. SDQN quickly achieves good performance on these tasks.
Figure 4: Maximum reward achieved over training averaged over a 25,000 step window withevaluations every 5,000 steps. Results are averaged over 10 randomly initialized trials with fixedhyper parameters. SDQN models perform competitively as compared to DDPG.
Figure 5: Hyper parameter sensitivity run on Half Cheetah. Left: Learning curves of differentnumbers of Bins. Center: Comparison of reward versus number of bins evaluated at 3 time pointsduring training. Error bars show 1 std. The number of bins negatively impacts performance for smallvalues of 2 and 4. For values larger than this, however, there is very little change in performance.
