Figure 1: Discretized approximation (right) of a continuous-time flow (left). Densities {Pk} of {zk}evolve via transformations {Tk}, with Pk → Phk when h → 0 for each k due to Lemma 1.
Figure 2: Amortized learning of continuous-time flows for VAEs. From left to right: the initialarchitecture with K-step transformations; For each step k, qφ(∙) is trained to match the distributin ofZk in CTFs; In the end, the CTF is distilled into qφ(∙).
Figure 3: Learning a generator withCTF. The goal is to match the samplesxo from qφ to those after a CTF (XT),or equivalently samples from pθ.
Figure 4: Knowledge distillation from the CTF (left) and ELBO versus epochs on MNIST (right).
Figure 5: Generated images for CIFAR-10 (top) and CelebA (middle) datasets with MacGAN (left)and SteinGAN (right). The bottom are images generated by a random walk on the ω space for thegenerator of MacGAN, i.e., ωt = ωt-ι + 0.03 X rand([-1,1]).
Figure 6: Inception score versus epochs fordifferent models.
Figure 7: Generated images for MNIST datasets with MacGAN (top) and SteinGAN (bottom).
Figure 8: Generated images for CelebA datasets with MacGAN.
Figure 13: Log-likelihoods vs discretization stepsize for MacGAN on MNIST.
Figure 9: Generated images for CIFAR-10 datasets with MacGAN.
Figure 10: Generated images for CelebA datasets with SteinGAN.
Figure 11: Generated images for CIFAR-10 datasets with SteinGAN.
Figure 12: Generated images with a random walk on the ω space for CelebA datasets with MacGAN,ωt = ωt-1 + 0.02 × rand([-1, 1]).
