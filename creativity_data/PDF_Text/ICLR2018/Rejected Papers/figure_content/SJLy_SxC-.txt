Figure 1: Connection illustration for L = 24. Layer 0 is the initial convolution. (i, j) is black meansxj takes input from xi ; it is white if otherwise. We assume there is a block transition at depth 12 forLog-DenseNet V2. (d) illustrates NearestHalfAndLog, a connection pattern with O(L2) connectionbudget for experiments in Sec. 4.1; it greatly improves over NearestHalf.
Figure 2: Each row: input image, ground truth labeling, and any scene parsing results at 1/4, 1/2,3/4 and the final layer. Noting that the first half of the network downsamples feature maps, and thesecond half upsamples, we have the lowest resolution of predictions at 1/2, so that its predictionappear blurred.
Figure 3: (a) Using the same FLOPS, Log-DenseNet V2 achieves about the same prediction accu-racy as DenseNets on CIFAR100. The DenseNets have block compression and are trained with drop-outs. (b) On ILSVRC2012, Log-DenseNet 169, 265 have the same block sizes as DenseNets169,265. Log-DenseNet369 has block sizes 8, 16, 80, 80.
Figure 4: (a)The tree of recursive calls of lglg_conn. (b) LogLog-DenseNet augments each xiof lglg_conn(0, L) with Log-DenseNet connections until xi has at least four inputs.
Figure 5: (a) In lglg_conn(0, L), i.e., min inputs = 1, each layer on average takes input from 3 to4 layers. Ifwe force input size to be four when possible using Log-DenseNet connection pattern, i.e.,min inputs = 4, we increase the average input size by 1 to 1.5. (b) Computational cost (in FLOPS)distribution through the 11 blocks in FC-DenseNet and FC-Log-DenseNet. Half of the computationsare from the final two blocks due to the high final resolutions. We compute the FLOPS assumingthe input is a single 224x224 image.
Figure 6: On CIFAR10 and SVHN, Log-DenseNet V2 and DenseNets have very close error rates(< 5% relatively difference) at each budget.
Figure 7: The number of parameter used in the naive implementation versus the error rates onvarious data-sets.
Figure 8: Performance of LogLog-DenseNet (red) with different hub multiplier (1 and 3). Largerhubs allow more information to be passed by the hub layers, so the predictions are more accurate.
Figure 9: Each row: input image, ground truth labeling, and any scene parsing results at 1/4, 1/2,3/4 and the final layer.
