Figure 1: The environ-ment of reacher task.
Figure 2: Performance of RL for reacher. The dash lines are results usingthe human designed reward, solid lines are results using the estimatedreward based on demonstration data. The evaluation scores (y-axis) arenormalized based on max and min reward. The corresponding equationnumbers are referred to within bracket.
Figure 3: These show the reward values (blue) for each end-effector position, and target posi-tion (red). The GM rewards are dependent on state value st+1. Therefore, these are average valuestaken over 1000 different states values for the same end-effector position. Color bars for panels (b),(c) and (d) are the same.
Figure 4: The environmentof reacher with an obstacle.
Figure 5: The performance for reacher with obstacle environ-ment.
Figure 6: The performance forFlappy Bird.
Figure 7: The performance for Super Mario Bros.
Figure 8: These are scatter-plots of end-effector positions (blue) for each state of captured demon-stration τ 500, τ1k, τ2k, each point is drawn by α is 0.01. And the fixed target position is also plot-ted (red). Notes that this is just plotting end-effector position, there is more variation in other statevalues. For example, even if the end-effector position were same, arms’ pose (joint values) might bedifferent. Note that τ 500 is not used in the experiment.
