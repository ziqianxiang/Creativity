Figure 1: An illustration of how L2 weight decay and ND-Adam control the effective learning rate.
Figure 2: The training losses and test accuracies of the same network trained with SGD, Adam, andND-Adam. Batch normalization with scaling factors is used.
Figure 3: The training losses and test accuracies of the same network trained with SGD, Adam, andND-Adam. Batch normalization without scaling factors, and BN-Softmax are used.
