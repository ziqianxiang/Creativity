Figure 1: The base Neural Architecture Search framework.
Figure 2: Overview of the multitask controller RNN. (1) A task embedding table is maintainedand updated with controller gradients to learn differentiated task embeddings over time. (2) At eachiteration of the multitask training, a task is randomly sampled. The task embedding is passed into thecontroller RNN along with the sampled action embedding at each RNN timestep. The full sequenceof output actions defines the child architecture trained on the chosen task.
Figure 3: Smoothed sampled model accuracy curves for multitask NMS when training simul-taneously on the Spanish language identification and SST tasks, and the best validation accuracyachieved for each task. Curves shown use Savitzky-Golay filtering with n=101 for clarity. Thereference benchmark accuracy by Socher (2013) was achieved on the SST task.
Figure 4: Heatmap showing the learned per-task distributions over the parameter search space froma representative MNMS model.
Figure 5:	Smoothed sampled model accuracy curves for n=3 MNMS models trained on IMDB andCorpus Cine, comparing models trained from scratch without transfer learning, and models transferlearned after pre-training. Curves smoothed using Savitzky-Golay filtering (n=101) for clarity.
Figure 6:	Heatmap showing correlations between learned task embeddings in a pre-trained MNMScontroller transfer learned on (a) the Corpus Cine and (b) the IMDB sentiment classification tasks.
