Figure 1: A visualization of FAME where the solid lines denote the variational approximation (infer-ence/encoder/recognition) network and dashed lines denote the generative model (decoder) networkfor training. When performing reconstructions during training, the input image is concatenated withthe output of the generative model (blue) and when generating the model follows a normal autore-gressive sampling flow (red) while also using the stochastic latent variables z = z1, ..., zL. Both thevariational approximation and the generative model follow a top-down hierarchical structure whichenables precision weighted stochastic variables in the variational approximation.
Figure 2: 10 randomly picked CIFAR10 images (left) and 200 random samples drawn from aN(0, I) distribution and propagated through the generative model (right).
Figure 3: Negative log-likelihood performance onOMNIGLOT in nats. The evidence lower-boundis computed with 5000 importance weighted sam-ples L5000 (θ, φ; x).
Figure 4:	Random samples drawn fromaN(0, I) distribution and propagated through the generativemodel of FAME for the dynamically binarized MNIST (a) and OMNIGLOT (b) dataset.
Figure 5:	MNIST reconstructions when masking the output from the FAME stochastic variables (a)and the concatenated input image (b) prior to feeding them to the autoregressive PixelCNN. It isinteresting to see how the edge information comes from the autoregressive dependency on the inputimage.
