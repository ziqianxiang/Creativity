Figure 1: Snapshots of the path taken by the agent while evaluating the model trained on the same random mapwith random goal and random spawn. The first row shows the top view of the robot moving through the mazewith the goal location marked orange, the agent marked black and the agent’s orientation marked red. Thesecond row shows the first person view, which, besides reward, is the only input available to the agent and thetop view is available only for human analysis.
Figure 2: Modified NavA3C+D1D2L (Mirowski et al. (2016)) architecture. The architecture is has three inputsthe current image It and previous action at-1 and previous reward rt-1. As shown by Mirowski et al. (2016),the architecture improves upon vanilla A3C architecture by using auxiliary outputs of loop-closure signal Land predicted depth D1 and D2. Since we use a smaller action space than Mirowski et al. (2016) and our agentmoves with constant velocity, we do not use velocity at previous time step as input signal.
Figure 3: The ten randomly chosen mazes for evaluation. We generate 1100 random mazes and choose ten toevaluate our experiments that require testing and training on the same maps.
Figure 4: We evaluate the NavA3C+D1D2L Mirowski et al. (2016) algorithm on ten randomly chosen maps,shown in Fig. ??, with increasing difficulty as described in Sec. 4.1. The figure is best viewed in color. Verticalaxis is one of the ten map ID’s on which the agent was trained (except for Rnd. Maze) and evaluated. Horizontalaxis are different evaluation metrics. We note that when the goal is static then rewards are consistently higheras compared to random goal. With static goals, the metric Distance-inefficiency is close to 1, indicating that thealgorithms are able to find shortest path. However, with random goals, the agents struggle to find the shortestpath. From the Latency 1 :> 1 results we note that the algorithm do well when trained and tested on the samemap but fail to generalize to new maps when evaluated on ability to exploit the information about goal location.
Figure 5: Plots showing the effect of number of training maps with random texture (Rnd Texture) and presenceof apples (With apples), when evaluated on unseen maps. We note that although the difference between meanmetrics is negligible as compared to standard deviation of the metrics. Hence we say that the effect of applesor textures can be ignored. The only clear trend is apparent Latency 1 :> 1 metric which suggest that randomtexture along without apples is advantageous in exploiting goal location while finding the goal second timeon-wards.
Figure 6: Snapshots of path taken by the agent to reach the goal in a single episode when model trained on 1000maps is evaluated Square, Wrench and Goal map. The top row shows an evaluation example on Square map,the agent takes the shortest path 6/10 times but when averaged over 100 episodes, the percentage of shortestpath taken is not better than random 50.4% (±12.8%). Although for the example of Wrench map the agenttakes the shortest path 8/10 times but when averaged over 100 episodes, the percentage of shortest path taken isreduced to 32.9% (±25.1%). For the Goal map, the example chosen here shows that the shortest path is onlytaken 1/6 times, on an average over 100 episodes, the shortest path is taken 42.6% (±35.1%) times.
Figure 7: Visualizing attention for two sequences. The first two rows show the sequence when the model istrained on and evaluated on the same map. The last two rows shows the sequence for a model trained on 1000maps and evaluated on one of the maps. We observe that the attention is uniformly distributed on the imagewhen the agent spawns. The attention narrows down few pixels in the center when the agent is navigatingthrough the corridor. It spreads to the entire image around turns and junctions. The algorithm also pays closeattention to important objects like goal, apples and unique decals.
