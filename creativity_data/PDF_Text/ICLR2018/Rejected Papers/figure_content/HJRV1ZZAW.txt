Figure 1: Samples from Trivia QA (Joshi et al., 2017)adversary example of a question that is answered incorrectly by matching the first occurrence of thequery word “composed” in the answer text. This study will use two popular reading comprehensiontasks - Trivia QA (Joshi et al., 2017), and SQUAD (RajPUrkar et al., 2016) - as its test bed. Bothtasks have openly available training and validation data sets and are associated with competitionsover a hidden test set on a PUblic leaderboard.
Figure 3: An illustration of dilated convolution. With a dilation of 1 (left), dilated convolution revertsto standard convolution. With a dilation of 2 (right) every other word is skipped, allowing the outputy3 to relate words x1 and x5 despite their large distance and a relatively small convolutional kernel.
Figure 4: The recep-tive field of repeateddilated convolutiongrows exponentiallywith network depth.
Figure 5: Schematic layouts of the BiDAF (left) and DrQA (right) architectures.
Figure 6: Dimensionalityreduction through convolu-tion.
Figure 7: Dimensionalitypreserving residual block.
Figure 8: Dev F1 Score on SQuAD vs Inference GPU Timedoc length = 282(mean of top 10% longest docs)speed-up： 95χsρe≡d-—R-Net▲ BiDAF• DrQA▲ Conv BiDAF• Conv DrQAInferenceGPUTimetbatch size = 1)3 https://github.com/allenai/bi- att- flow4https://github.com/facebookresearch/DrQA5http://pytorch.org/6Under review as a conference paper at ICLR 2018Model	BiDAF	Conv BiDAF (5-17-3 conv layers)# of params	2.70M	2.76MDev EM (Multiplied by 100)	67.66 ± 0.41	68.28 ± 0.56Dev F1 (Multiplied by 100)	77.29 ± 0.38~	77.27 ± 0.41
Figure 9: Inference GPU time of four models with batch size 1 or 64. The time spent on data pre-processing and decoding on CPUs are not included. We suspect the difference in speed-up is causedby implementation difference between TensorFlow and PyTorch. The missing points for BiDAF andConV BiDAF were caused by running of out GPU memories.
