Figure 1: Network iterative learning intuition. Instead of (a) directlyoptimizing the objective function f (x). We first (b) optimize asimpler approximate function f(1) (x), then (c) use its minimumx(G1) as an initialization to further optimize f(2) (x) = f(x).
Figure 2: Network iterative learning vs. learning with a fixed architecture. (a) The degradation problem, i.e., thedeeper network produces higher errors in both training and testing. (b) The degradation problem is overcome bythe proposed learning scheme. (c,d) Comparison results of the two learning schemes on PlainNet and ResNet. (e)Testing error curves using network iterative learning. (a-c): PlainNet on CIFAR10, (d,e): ResNet on CIFAR10(residual ratio 0.5).
Figure 3: The DynamicNet architecture. A networkin the DynamicNet family will allow for architecturechange. D: fast down-sampling block, C: channelswitching block, T: template block, L: level block, F:fully connected layer block; n: number of blocks, k:number of levels.
Figure 4: Network iteration processes for PlainNet andResNet in the DynamicNet family.
Figure 5: Network iteration process for PlainNet from 7-layer to 10-layer.
Figure 6: The proposed optGD algorithm for net-work morphing. (a) Computational graph for themorphism equation (5). (b) Calculation of the opti-mal learning rate.
Figure 7: Network iterative learning vs. learning with a fixedarchitecture for PlainNet on CIFAR100.
Figure 8: Network iterative learn-ing vs. learning with a fixed archi-tecture for PlainNet on ImageNet.
Figure 9: Network iterative learning v.s. traditional optimizationalgorithms.
Figure 10: The proposed optGDalgorithm v.s. the LSQ-based al-gorithm (Wei et al., 2016) solvingfor the network morphism equa-tion.
