Figure 1: PhaseCond: our proposed attention model structure overview. We use the colored rectan-gle to highlight the focus of this paper. The question and passage encoder layers and attention layersare colored in blue, the fusion layers are colored in green.
Figure 2: Improved question-passage attention model. We use blue color to denote question repre-sentations and use green color for passage representations.
Figure 3: Dynamic attention changes of multiple layers on a visualized example. The matricesare the attention weights computed by the dot-product attention function before any normalization.
