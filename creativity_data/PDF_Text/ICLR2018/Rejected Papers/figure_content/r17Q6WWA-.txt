Figure 1:	Example of our collaborative block applied on the feature maps of five task-specific net-works. The input feature maps (shown in part 1) are first concatenated depth-wise and transformedinto a global feature map (part 2). The global feature map is then concatenate with each input fea-ture map individually and transformed into task-specific feature maps (part 3). Each resulting featuremap is then added back to the input feature map using a skip connection (part 4), which gives thefinal outputs of the block (part 5).
Figure 2:	Deep Collaboration Network (DCNet) using ResNet18 as underlying network in a MTLsetting on the MTFL dataset. The top part shows the block structure of ResNet18 interleaved withour proposed collaborative block. While the detailed composition of each ResNet block and thetask-specific fully-connected blocks are shown at the bottom left and bottom right respectively, werefer to Fig. 1 for the description of our collaborative block.
Figure 3:	Landmark failure rates (%) on the MTFL task. The reported values are the average overthe last five epochs, averaged over three tries. The left plot presents our results with AlexNet asthe underlying network, while the right one with ResNet18. AN-S and RN-S stand for single-task training, AN and RN for multi-task training with a single central network, ANx and RNx formulti-task training with a single central network widen to match the number of parameters of ourapproach, HF for HyperFace, TCDCN for Zhang et al. (2014)’s approach and XS for Cross-Stitch.
Figure 4: Example predictions of our DCNet with pre-trained ResNet18 as underlying network onthe MTFL task. The first two contains failure cases, while last two contains successes. Elementsin green correspond to ground truth, while those in blue correspond to our prediction. In additionto providing the facial landmarks (the small dots), we also include the labels of the related tasks:gender, smiling, wearing glasses and face profile. As shown in the first example, over-expositioncan have a large impact on the prediction quality.
Figure 5: Landmark failure rates (%) on the AFW task. Each network is trained on the MTFL taskand tested without fine-tuning on the AFW task. The left plot presents our results with AlexNetas the underlying network, while the right one with ResNet18. AN-S and RN-S stand for single-task learning, AN and RN for multi-task learning with a single central network, ANx and RNx formulti-task learning with a single central network widen to match the number of parameters of ourapproach, HF for HyperFace, TCDCN for Zhang et al. (2014)’s approach and XS for Cross-Stitch. Ineach instance, the left column (blue) is for un-pretrained networks, while the right column (green)is for pre-trained networks. Our proposed approach (last column) obtains the lowest failure ratesoverall.
Figure 6: Landmark failure rate progression (in %) on the AFLW dataset with varying train / testratio using a pre-trained ResNet18 as base network. Each curve is the average over three tries. Eventhough our approach has the slowest convergence rate, it outperforms the others in four of the fivecases.
Figure 7: Landmark failure rate improvement (in %) of our approach compared to XS when sam-pling random task weights. We used a pre-trained ResNet18 as underlying network. The histogramat the left and the plot at the top right represents performance improvement achieved by our proposedapproach (positive value means lower failure rates), while the plot at the bottom right correspondsto the log of the task weights. Our approach outperformed XS in 86 out of the 100 tries, thus em-pirically demonstrating that our learning framework was not unfavorable towards XS and that ourapproach is less sensitive to the task weights λ.
Figure 8: Results of our ablation study on the MTFL dataset with an un-pretrained ResNet18 asunderlying network. We remove each task-specific features from each respective central aggrega-tion layer and evaluate the effect on landmark failure rate. The columns represent the task-specificnetwork, while the rows correspond to the network block structure. Blocks with a high saturatedcolor were found to have a large impact on performance. For instance, this ablative study showsthat the influence of high-level face profile features is large within our proposed architecture, whichcorroborates with the well-known fact that facial landmark locations are highly correlated to profileorientation. This thus constitutes an empirical evidence of domain-specific information sharing viaour approach.
