Figure 1: Overview of AdvGANet al. (2017) proposed to train a local substitute model with queries to the target model to generateadversarial samples, but this strategy still relies on transferability. In contrast, we show that theproposed AdvGAN can perform black-box attacks without depending on transferability.
Figure 2: Adversarial examples generated from the same original image to different targets by Adv-GAN on MNIST with semi-whitebox attack, (a), (b), and (c), and black-box attack, (c), (d), and (e).
Figure 3: Adversarial examples generated by AdvGAN on CIFAR-10 for (a) semi-whitebox attackand (b) black-box attack. Image from each class is perturbed to other different classes. On thediagonal, the original images are shown. The corresponding perturbations (amplified by 10Ã—) areshown in (c) and (d).
Figure 4: Adversarial examples (a) generated by AdvGAN on ImageNet in the semi-whitebox set-ting, which are classified as (from left to right) poodle, ambulance, basketball, and electric guitar.
Figure 5: Adversarial examples generated by AdvGAN on MNIST against different models in thesemi-whitebox setting. Here the adversarial examples are randomly sampled corresponding to dif-ferent original images.
Figure 6: Adversarial examples generated by AdvGAN on MNIST against different models in theblack-box setting. Here the adversarial examples are randomly sampled corresponding to differentoriginal images.
Figure 7: Adversarial examples generated by AdvGAN on CIFAR-10. Here the adversarial exam-ples are randomly sampled corresponding to different original images.
Figure 8: Examples from an ImageNet-compatible set. Left: original image; right: adversarialimage generated by AdvGAN against Inception_v3.
