Figure 1: Results on the CONV network on two-class MNIST. (top) Training error (under 0-1loss) for SGLD on the empirical risk -TRS under a variety of thermal noise ,2∕τ settings. SGDcorresponds to zero thermal noise. (top-left) The large markers on the right indicate test error. Thegap is an estimate of the generalization error. On true labels, SGLD finds classifiers with relativelysmall generalization error. At low thermal noise settings, SGLD (and its zero limit, SGD), achievesmall empirical risk. As we increase the thermal noise, the empirical 0-1 error increases, but thegeneralization error decreases. At 0.1 thermal noise, risk is close to 50%. (top-right) On randomlabels, SGLD has high generalization error for thermal noise values 0.01 and below. (True error is50%). (middle-left) On true labels, Entropy-SGD, like SGD and SGLD, has small generalizationerror. For the same settings of thermal noise, empirical risk is lower. (middle-right) On randomlabels, Entropy-SGD overfits for thermal noise values 0.005 and below. Thermal noise 0.01 producesgood performance on both true and random labels. (bottom row) Entropy-SGLD is configured tobe ε-differentially private with ε ≈ 0.0327 by setting T = √m, where m is the number of trainingsamples. (bottom-left) On true labels, the generalization error for networks learned by Entropy-SGLD is close to zero. Generalization bounds are relatively tight. (bottom-right) On random label,Entropy-SGLD does not overfit. See Fig. 3 for SGLD bounds at same privacy setting.
Figure 2: Fully connected networks trained on binarized MNIST with a differentially privateEntropy-SGLD algorithm. (left) Entropy-SGLD applied to FC600 network trained on true labels.
Figure 3: Results on CONV architecture, running SGLD configured to have the same differentialprivacy as EntroPy-SGLD with T = √m. On true labels, SGLD learns a network with approximately3% higher training and test error than the mean and Gibbs networks learned by Entropy-SGLD.
Figure 4: (?) Local here refers to the mean classifier. Entropy-SGLD results on MNIST. (top-left)FC1024 network trained on true labels. The train and test error suggest that the generalization gapis close to zero, while all three bounds exceed the test error by slightly more than 3%. (bottom-left) CONV network trained on true labels. Both the train and the test errors are lower than thoseachieved by the FC1024 network. We still do not observe overfitting. The C-bound and PAC-Bayesbounds exceed the test error by ≈ 3%. (top-right) FC1024 network trained on random labels. Afterapproximately 1000 epochs, we notice overfitting by ≈ 2%. Running Entropy-SGLD further doesnot cause an additional overfitting. Theory suggests that our choice of τ prevents overfitting viadifferential privacy. (bottom-right) CONV network trained on random labels. We observe almostno overfitting (less than 1%). Both training and test error coincide and remain close to the guessingrate (90%).
