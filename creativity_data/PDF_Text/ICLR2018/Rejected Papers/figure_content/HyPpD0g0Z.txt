Figure 1: Examples from a) the subsampled CelebA dataset and b) the augmented MNIST dataset. Connectedimages are counterfactual examples as they share the same realization of the ID which is the identity of theperson in a) and the original image used for data augmentation in b). The comparison is a training of exactlythe same network architecture that does not make use of the grouping information but using a standard ridgepenalty. In a) exploiting the grouping information reduces the test error by 32% compared to pooling over allsamples. In b) the test error on rotated digits is reduced by 50%.
Figure 2: Left: data generating process for the considered model as in Gong et al. (2016), where the effectof the domain on the orthogonal features X ⊥ is mediated via unobserved noise ∆. Right: our setting. Thedomain itself is unobserved but we can now observe the ID variable we use for grouping.
Figure 3: a) Examples from the stickmen training set. The first three images from the left have y ≡ child;the remaining three images have y ≡ adult. Connected images are counterfactual examples. b) Misclassifiedobservations from test set 2. c) Misclassification rates for c = 50. Results for c ∈ {20, 500, 2000} can befound in Figure C.10.
Figure 4: a) Examples from the CelebA image quality dataset. The first three images from the left havey ≡ no glasses; the remaining three images have y ≡ glasses. Connected images are counterfactual examples.
Figure 5: a) Examples from the subsampled and augmented AwA2 dataset. The first three images from theleft shows horses, the remaining three images show elephants. Connected images are counterfactual examples.
