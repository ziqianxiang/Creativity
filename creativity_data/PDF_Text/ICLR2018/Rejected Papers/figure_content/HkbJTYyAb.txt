Figure 1: (a) Illustration of 1-D convolution, where the dimensions of the input/output variable areboth 8 (the input vector is padded with 0), the width of the convolution filter is 3 and dilation is 1; (b)A block of ConvFlow layers stacked with different dilations.
Figure 2: Approximation performance with different number of ConvBlocks4.2	Handwritten digits and characters4.2	. 1 SetupsTo test the proposed ConvFlow for variational inference we use standard benchmark datasets MNIST3and OMNIGLOT4 (Lake et al., 2013). Our method is general and can be applied to any formulationof the generative model pθ (x, z); For simplicity and fair comparison, in this paper, we focus ondensities defined by stochastic neural networks, i.e., a broad family of flexible probabilistic generativemodels with its parameters defined by neural networks. Specifically, we consider the following twofamily of generative modelsGi : Pθ(x, Z) = Pθ(Z)pθ(x|z)	(18)G2 : Pθ(x,Zi,Z2) = Pθ(zι)pθ(Z2∣Zι)Pθ(x∣Z2)	(19)where p(Z) and p(Z1) are the priors defined over Z and Z1 for G1 and G2, respectively. All otherconditional densities are specified with their parameters θ defined by neural networks, thereforeending up with two stochastic neural networks. This network could have any number of layers,however in this paper, we focus on the ones which only have one and two stochastic layers, i.e., G1and G2, to conduct a fair comparison with previous methods on similar network architectures, suchas VAE, IWAE and Normalizing Flows.
Figure 3: Training data and generated samplesincreasing model parameters. A Revert Layer is used to maximize the opportunity that all dimensionsget as much warping as possible. Experimental results on inferring target complex density and densityestimation on generative modeling on real world handwritten digits data demonstrates the strongperformance of ConvFlow. Particularly, density estimates on MNIST show significant improvementsover state-of-the-art methods, validating the power of ConvFlow in warping multivariate densities.
