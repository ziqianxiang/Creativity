Figure 1 : FSA constructed from φ = b U a. (right): Specification amendment example. Aφ1 is constructedfrom φ1 = ♦a ∧ ♦b. Aφ2 is constructed from φ2 = bU a. Aφ is constructed from φ = φ1 ∧ φ2. Theautomaton state pair in parenthesis denote the corresponding states from Qφ1 and Qφ2 that the product state isconstructed from. qi0 denotes the initial state of Aφi , qi,j denotes the jth state of Qφi .
Figure 2 : upper left: Optimal policy for φ1 = ♦a ∧ ♦b trained using Q-Learning. The arrows represent theaction at each state and the dot represents stay still at that state. (upper right): Optimal policy for φ2 = b U a.
Figure 3 : (left): Baxter simulation Environment with three square regions (black,red, blue), two circularregions (red, blue), two boxes (red, blue) that the robot can manipulate and an interactive ball that the user canplace anywhere on the table. Tasks are specified using these elements in Appendix A. (upper right): Learningcurve for task φ1 over 5 random seeds. (lower right): Policy deployment success rateleft) shows the robustness comparison at each state. The policy with lower robustness is chosenfollowing Equation (15). We can see that the robustness of both policies are the same from s = -5to s = 0. And their policies agree in this range (Figures 3 and4 ). As s becomes larger, disagreementemerge because ∏φι (∙,qo) wants to stay closer to b but ∏φ2 (∙, qo) wants otherwise. To maximize therobustness of their conjunction, the decisions of π°? (∙, qo) are chosen for states s > 0.
Figure 4 : (left): Learning curves for tasks φ6 and φ7 (task definitions provided in Appendix A). (right): Policydeployment success rate for tasks φ6 and φ7The results are illustrated in Figure 3 (right). The upper right plot shows the average robustness overtraining iterations. Robustness is chosen as the comparison metric for its semantic rigor (robustnessgreater than zero satisfies the task specification). The reported values are averaged over 60 episodesand the plot shows the mean and 2 standard deviations over 5 random seeds. From the plot we canobserve that the FSA augmented MDP and the terminal robustness reward performed comparativelyin terms of convergence rate, whereas the heuristic reward fails to learn the task. The FSA augmentedMDP also learns a policy with lower variance in final performance.
