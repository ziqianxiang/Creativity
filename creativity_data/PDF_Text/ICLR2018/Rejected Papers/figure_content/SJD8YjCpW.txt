Figure 1: A visualization of a one-layer MLP ArbNet with the identifiers numbered sequentially.
Figure 2: Heatmap of multinomial parameters drawn from different values of αNeighborhood hash, which involves the composition of a modulus hash and a uniform distributionaround a specified radius. This is given by the following hash function:radius, radius])) mod n(10)When the radius is 0, the Neighborhood hash reduces to the modulus hash, and when the radius isat least half the size of the hash table, it reduces to the uniform hash. Controlling the radius thusallows us to control the intuitive notion of ‘noise’ in the specific setting where the expected load ofall the table entries is the same.
Figure 3: Effect of α (Balance) in Dirichlet hash on network accuracy across different levels ofsparsity)6)5l4)3)2Mloi989999998(％)sparsity* 0-1• 0.3• 0.5・ 0-7・ 0-94.5	5.0	5.5	β-0	β-5Shannon EntropyFigure 4:	Effect of Shannon entropy (Balance) in Dirichlet hash on network accuracy across differentlevels of sparsityWe observe in Figure 3 that on the MNIST dataset, increasing α has a direct positive effect on testaccuracy, across different levels of sparsity. On the CIFAR10 dataset, when the weights are sparse,increasing α has a small positive effect, but at lower levels of sparsity, it has a huge positive effect.
Figure 4:	Effect of Shannon entropy (Balance) in Dirichlet hash on network accuracy across differentlevels of sparsityWe observe in Figure 3 that on the MNIST dataset, increasing α has a direct positive effect on testaccuracy, across different levels of sparsity. On the CIFAR10 dataset, when the weights are sparse,increasing α has a small positive effect, but at lower levels of sparsity, it has a huge positive effect.
Figure 5:	Effect of radius (Noise) in Neighborhood hash on network accuracy across differentlevels of sparsity6	ConclusionWeight-sharing is very important to the success of deep neural networks, and is a worthy topicof study in its own right. We proposed the use of ArbNets as a general framework under whichweight-sharing can be studied, and investigated experimentally, for the first time, how balance andnoise affects neural network performance in the specific case of an MLP ArbNet and two imageclassification datasets. We hope to carry out more extensive experimental and theoretical researchon the role of weight-sharing in deep networks, and hope that others will do so too.
