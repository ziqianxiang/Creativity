Figure 1: Generic autoencoder architecture used in the geometric experiments.
Figure 2: Investigating the latent space in the case of disks. On the left side, we have interpolatedz in the latent space between two encoded input disks (one small and one large), and show thedecoded, output image. It can be seen that the training works well, with the resulting code spacebeing meaningful. On the right, we plot the radii of the input disks against their codes z ∈ R. Theautoencoder appears to represent the disks with their area.
Figure 3: Autoencoding of disks when the autoencoder is trained with no bias. The autoencoderlearns a function f which is multiplied by a constant scalar, h(r), for each radius. This behaviour isformalised in Equation (5).
Figure 4: Autoencoding of disks with a database with limited radii. The autoencoder is not ableto extrapolate further than the largest observed radius. The images with a green border representdisks whose radii have been observed during training, while those in red have not been observed.
Figure 5: Input and output of our network when autoencoding examples of disks when thedatabase contains a “hole”. Disks of radii between 11 and 18 pixels (out of 32) were not observedin the database. In green, the disks whose radii have been observed in the database, in red thosewhich have not.
Figure 6: Result of different types of regularisation on autoencoding in an “unknown region”of the training database. We have encoded/decoded a disk which was not observed in the trainingdataset. We show the results of four experiments: no regularisation, `2 regularisation in the latentspace (“Type 1”), `2 weight penalisation of the encoder and decoder (“Type 2”) and `2 weightpenalisation of the encoder only (“Type 3”).
Figure 7: Verification of the hypothesis that y(t, r) = h(r)f(t) for decoding in the case wherethe autoencoder contains no bias.. We have determined the average profile of the output of theautoencoder when no biases are involved. On the left, we have divided several random experimen-tal profiles y by the function h, and plotted the result, which is close to constant (spatially) for afixed radius of the input disk. On the right, we plot z against the theoretically optimal value ofh (C hf, IBr i, where C is some constant accounting for the arbitrary normalization of f). Thisexperimental sanity check confirms our theoretical derivations.
Figure 8: Comparison of the empirical function f of the autoencoder without biases with thenumerical minimisation of Equation (7). We have determined the empirical function f of theautoencoder and compared it with the minimisation of Equation (7). The resulting profiles aresimilar, showing that the autoencoder indeed succeeds in minimising this energy.
Figure 9: Profile of the encoding/decoding of centred disks, with a restricted database. Thedecoder learns a profile f which only extends to the largest observed radius R = 18. Beyond thisradius, another profile is learned that has is obviously not tuned to any data.
Figure 10: Input and output of the network of Zhu et al.Zhu et al. (2016) (“IGAN”) for diskswhen the database is missing disks of certain radii. We have applied the IGAN with a code sizeof d = 100, as in the original paper, and d = 1 as in our autoencoder. In both cases the IGANinterpolates incorrectly in the unknown region. Outlined in green are the images with observed radiiand in red the unobserved radii.
