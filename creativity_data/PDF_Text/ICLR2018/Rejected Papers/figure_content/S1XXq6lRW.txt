Figure 1: Illustration of how the Universal Encoder is trained (left) and how it is used when trainingand making predictions using the classifier (right). All Universal Encoders (gray background) areexactly identical by sharing architecture and weightsThe method presented in this article is conceptually similar in spirit to Googleâ€™s zero-shot machinetranslation model (Johnson et al., 2016), which is used in the Google translate API. That model alsouses a shared vocabulary and a language independent encoder. It does, however, require a large cor-pus of aligned sentences for training. Additionally, translating a text is a much harder problem thanmerely extracting discriminative features since it requires encoding of e.g. syntactic informationthat is not necessary for text classification. Therefore such a model is much more complex than itneeds to be, and a more parsimonious model is therefore preferable. We will compare our zero-shotclassification model with an equivalent model based on the zero-shot translation model in section 3.
