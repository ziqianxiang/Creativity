Figure 1: left: a deterministic one-layer RNN whose hidden state at time step t is defined asht = f (xt-1, ht-1)). right: a noisy one-layer RNN with latent hidden state at time step t definedas ht 〜 ExPFam(ht； η(xt-ι, ht-ι)). Choosing one exponential family distribution over anotheramounts to changing the nature of the injected noise and thus the regularization procedure for theRNN.
Figure 2: Left: validation perplexity for the deterministic RNN and the RNN with NOISIN regular-ization. Right: the corresponding training perplexity. The settings were the same in both cases. Weused the sigmoid activation function on a one-layer recurrent neural network with 256 hidden units.
