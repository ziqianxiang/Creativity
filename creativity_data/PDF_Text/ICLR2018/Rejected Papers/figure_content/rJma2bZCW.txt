Figure 1:	Impact on SGD with ratio of learning rate η and batch size S for 4 layer ReLU MLP onFashionMNIST.
Figure 2:	Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,SS. α (x-axis) corresponds to the interpolation coefficient. As predicted by our theory, lower Sn ratioleads to sharper minima (as shown by the left and middle plot).
Figure 3:	Interpolation between parameters of models trained with the same learning rate (η) tobatch-size (S) ratio: S=0.1×β, but different η and S values determined by β. As predicted by ourtheory, the minima for models with identical noise levels should be qualitatively similar as can beseen by these plots.
Figure 4:	Learning rate schedule can be replaced by an equivalent batch size schedule. The ratio oflearning rate to batch size is equal at all times for both red and blue curves in each plot. Above plotsshow train and test accuracy for experiments involving VGG-11 architecture on CIFAR10 dataset.
Figure 5: Impact of IS on memorization of MNIST when 25% and 50% of labels in the training setare replaced with random labels, using no momentum (on the right) or a momentum with param-eter 0.9 (on the left). We observe that for a specific level of memorization, high η leads to bettergeneralization. Red has higher value of S than blue.
Figure 6: Breaking point analysis: Our theory suggests the final performance should be similar whenthe SGD noise level β∣××η is kept the same. Here We study its breaking point in terms of too largea learning rate or too small a batch size. (a,b,c)- Validation accuracy for different dataset sizes anddifferent β values for a VGG-11 architecture trained on CIFAR10. In each experiment, We multiplythe learning rate (η) and batch size (S) with β such that the ratio β××(S=50) is fixed. We observethat for the same ratio, increasing the learning rate and batch size yields similar performance upto a certain β value, for which the performance drops significantly. (d)- Breaking point analysiswhen half the noise level .nS=00^ is used. The breaking point happens for much larger β whenusing a smaller noise. All experiments are repeated 5 times with different random seeds. The graphsdenote the mean validation accuracies and the numbers in the brackets denote the mean and standarddeviation of the maximum validation accuracy across different runs. The * denotes at least one seedlead to divergence.
Figure 7: Experiments involving Resnet56 architecture on CIFAR10 dataset. In each curve, wemultiply the IS ratio by a given factor (increasing both batch size and learning rate). We observe thatmultiplying the ratio by a factor up to 5 results in similar performances. However, the performancedegrades for factor superior to 5.
Figure 8:	Validation accuracy of Resnet56 networks against different ratios learning rate to batchsize on CIFAR10. Trained with SGD.
Figure 9:	ImpaCt on SGD With ratio of learning rate η and batCh size S for 20 layer ReLU MLPWithout BatCh Normalization on FashionMNIST.
Figure 10: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using adifferent random initialization.
Figure 11: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using adifferent random initialization.
Figure 12: Interpolation of Resnet56 networks trained with different learning rate to batch size ratio,SS. α (x-axis) corresponds to the interpolation coefficient. Each plots correspond to model using adifferent random initialization.
Figure 13: We show in more detail exchangeability of batch size and learning rate in a one-to-oneratio. In blue, cyclic batch size schedule between size 128 and 640 and fixed learning rate 0.005,is exchangeable with orange cyclic learning rate schedule between learning rates 0.001 and 0.005with fixed batch size 128. In green, constant batch size 640 and constant learning rate 0.005 isexchangeable with, in red, constant batch size 128 and constant learning rate 0.005.
Figure 14: Learning curves for memorization experiment with momentum 0.0. Solid lines representtraining accuracy, dotted validation accuracy. Warm color indicates higher η ratio.
Figure 15: Correlation between (approximate) norm of Hessian of best validation minima and learn-ing rate to batch-size ratio for 0.0 (left) and 0.9 (right) momentum.
Figure 16: Cyclical schemes oscillate between sharp and wide regions. Additionally, cyclicalschemes find wider minima than baseline run for same level of loss, which might explain theirbetter generalization. All cyclical schedules use base η = 0.005 and cycle length 15 epochs, whichapproximate convergence at the end of each cycle. Plots from left to right: discrete S, discrete η,triangle η, constant learning rate η = 0.001, constant learning rate η = 0.005. On vertical axis wereport loss (red) and approximated norm of Hessian (blue).
