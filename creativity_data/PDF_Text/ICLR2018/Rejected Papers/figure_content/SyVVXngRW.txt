Figure 1: Concept. (a) Feature-sharing multi-task learning models such as Go-MTL suffers from negativetransfer from unreliable predictors, which can result in learning noisy representations. (b) AMTL, an inter-tasktransfer asymmetric multi-task learning model that enforces the parameter of each task predictor to be generatedas a linear combination of the parameters of relevant task predictors, may not make sense when the tasks are onlypartially related. (c) Our asymmetric multi-task feature learning enforces the learning of shared representationsto be affected only by reliable predictors, and thus the learned features can transfer knowledge to unreliablepredictors.
Figure 2: (a) An illustration of negative transfer in common latent bases model. (b) The effects of inter-task `2regularization on top of common latent bases model. (c) Asymmetric task-to-basis transfer. (d) An illustration ofReLU transformation with a bias term.
Figure 3: Deep-AMTFL. The green lines denote feedback connections with `2,1 constraints on the features.
Figure 4: Visualization of the learned features and paramters on the synthetic dataset. (a-b) True param-eter for generating the dataset. (c) Reconstructed parameters from AMTL (d-e) Reconstructed parameters fromGo-MTL. (f-h) Reconstructed parameters from AMTFL.
Figure 5: Results of synthetic dataset experiment. (a) Average RMSE for clean/noisy/all tasks. (a) Per-taskRMSE reduction over STL. (b) RMSE and training time for increasing number of tasks.
Figure 6: Per-class performance improvements on AWA dataset.
