Figure 1: Empirical maximum inner products across pairs of randomly sampled class keys, Withrespect to key dimensionality. Class key vectors are generated by sampling from from either N (0, 1)or U (-1,1). X-axis represents size of the vectors and Y-axis shoWs the maximum of maximum dotproduct among 100 samples, over 1000 different trials.
Figure 2: From left to right, mean participant accuracies obtained in collaborative learning over 2,3, and 5 participants on MNIST (top row) and Olivetti Faces (bottom row) datasets, respectively.
Figure 3: We split MNIST among 2 participants, one being the adversary. The victim has samples ofdigits 0, 1, 2, 3, and 4. When the exact same key of one of these digits is guessed by the adversary,the local generator of the adversary is able to obtain highly accurate digit reconstructions, whichdemonstrates the power of GAN attack.
Figure 4: We split Olivetti Faces among 2 participants, one being the adversary. Each participant hasface photos of 20 people. We randomly select 5 class labels from the victim, whose private key isavailable to the adversary. The local generator generates highly successful reconstructions (bottomrow) that are highly similar to the original ones (top row), as expected.
Figure 5: We approximate the maximum Euclidean distance between any class key and ψ(cattack)necessary for the adversary to succeed in attack. From left to right, reconstructions of the adversarywhen it generates random keys that are δ ∈ {1.3, 1.2, 1.1, 1.0, 0.5, 0.1, 0.01} far from class key ofdigit-0 in MNIST (upper row) and person-24 in Olivetti Faces (bottom row), respectively.
Figure 6: We split MNIST among 5 participants which are also attackers. Each participant creates arandom class key, and, trains a local generator with that key. Rows correspond to sample reconstruc-tions obtained by the GAN attack, when dkey is {64, 1024, 4096, 16834}, respectively. We run theexperiments until each participant achieves 97% accuracy on its local dataset. As we increase thekey dimensionality, it takes longer for local models to exceed that accuracy threshold thus generatorstrain more. One can see in the last two rows that generators capture a mode, however, the mode isclearly not similar to any of one the MNIST digits. Therefore, the GAN attack fails.
Figure 7: All keys are randomly generated by the participants, including the adversary. From left toright, reconstructions of adversary for dkey ∈ {128, 1024, 4096, 16834}, respectively. We show thatthe mode that GAN learns is likely to belong one of the classes in CLF, for small dkey. Yet whendkey is sufficiently large, reconstructions are not realistic faces.
Figure 8: Simulating the case when two participants have samples for a shared class. (a) ψi (c)and ψj (c) are two nearly orthogonal `2 normalized vectors. If we introduce a third random vec-tor, and tune it to maximize its overlapping with ψi(c) and ψj (c), we would obtain a φ(xc) suchthat ɑι and a2 are approximately n/4 (See text for definitions). (b) In order to verify this be-havior in higher dimensions, we randomly generate `2 normalized ψi (c), ψj (c), φ(xc) ∈ Rdkeyfor dkey ∈ {64, 128, 1024, 4096, 16384}. Then we find the optimal φ(xc) such that its dot prod-uct with ψi (c) and ψj (c) are maximized. Bars indicate that as we increase the dimensionality,ψi (c) and ψj (c) are more likely to be orthogonal, therefore φ(xc) settles in between them mak-ing score of approximately 0.7 with both. (c) We generate new `2 normalized vectors in Rdkey fordkey ∈ {64, 128, 1024, 4096, 16384}, and check their maximum absolute dot product with the finalφ(xc). We see that as we increase dkey, φ(xc) converges towards the plane spanned by ψi (c) andψj (c). This confirms that the approach is likely to behave well when training over shared classes,despite using different private keys across the participants.
