Figure 1: Diagram of the Latent Attention Network (LAN) framework.
Figure 2: Visualization of attention maps for different translated MNIST digits. For each pairof images, the original translated MNIST digit is displayed on the top, and a visualization of theattention map is displayed on the bottom (where warmer colors indicate more important regions to thepre-trained classifier). Notice the blobs of network importance around each digit, and the seeminglyconstant “griding” pattern present in each of the samples.
Figure 3: (a) Constant grid pattern observed in the attention masks on Translated MNIST. (b)Accuracy of the pre-trained classifier on 7 × 7 digits centered at different pixels. Each pixel in theimages is colored according to the estimated normalized accuracy on digits centered at that pixelwhere warmer colors indicate higher normalized accuracy. Only pixels that correspond to a possibledigit center are represented in these images, with other pixels colored dark blue. (c-d) Duplicate of(a) and (b) for a pre-trained network on a modified Translated MNIST domain where no digits canappear in the bottom right hand corner.
Figure 4: Each frame pairs an input image (left) with its LAN attention mask (right). Each columnrepresents a different category: horse, plane, truck, bird, ship, and deer.
Figure 5: Each image pair contains a CIFAR-10 image and its corresponding sample-specific attentionmask. Each column contains images from a different category: car, cat deer, dog, horse and ship.
