Figure 1: Illustration of the difference between common RNN architectures (left) and our approach(right). In common architectures, the entire computation (gray block) for each step xt, t = 1,...,ndepends on completing the previous step. This impedes any parallelization between steps. In contrast,we propose to process the input at each step independently of the other inputs (larger gray block)and do the recurrent combination with relatively lightweight computation (small gray block). Themajority of the computation (surrounded by the dashed line) can then be easily parallelized.
Figure 2: Average processing time (in milliseconds) of a batch of 32 samples using CuDNN LSTM,word-level convolution conv2d, and the proposed SRU. l number of tokens per sequence, d: featuredimension and k: feature width. See Section 4 for details of the setup used.
Figure 3: Left: relative speed improvement of fused kernel (SRU) over fo-pooling kernel(QUaSi-RNN) on various benchmarks. Timings are performed on a desktop machine with GeForceGTX 1070 and Intel Core i7-7700K Processor. Right: mean exact match (EM) score of 5-layerSRU and Quasi-RNN on SQUAD as a function of the number of epochs. Models are trained for amaximum of 100 epochs using Admax optimizer with learning rate 0.001.
Figure 4:	Comparison between Quasi-RNN and SRU on classification benchmarks. We perform 3independent trials of 10-fold cross validation (3 X 10 runs) for each model and dataset. We report theaverage test accuracy of model configurations that achieve the best dev result. All models are trainedusing Adam optimizer with default learning rate = 0.001, weight decay = 0 and dropout probabilitytuned from values {0.1,0.3,0.5,0.7}.
Figure 5:	Comparison between Quasi-RNN and SRU on the SQuAD benchmark. We perform 3independent trials with a maximum of 100 training epochs. We report the average exact match (EM)score of each model configuration. Models are trained using Admax with learning rate 0.001.
