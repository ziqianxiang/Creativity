Figure 1: Angle bias causes a horizontal stripe pattern in W A. (Best viewed in color)that show that it is possible to efficiently train a 100-layered MLP by reducing the angle bias usingLCW. Finally, we conclude with a discussion of future works.
Figure 2: Activations in layers 1, 3, 5, 7, and 9 of the 10 layered MLP with sigmoid activationfunctions. Inputs are randomly sampled from CIFAR-10.
Figure 3: Boxplot summaries of θil in the MLP with sigmoid activation functions. Results for onlythe first ten neurons in layers 1,3,5,7, and 9 are displayed. Samples shown in Figure 2 are used toevaluate θil .
Figure 4: Activations in layers 1, 10, 20, 30, and 40 of the 50 layered MLP with ReLU activationfunctions. Inputs are randomly sampled from CIFAR-10.
Figure 5: Boxplot summaries of θil in the MLP with ReLU activation functions. Results for only thefirst ten neurons in layers 1, 10, 20, 30, and 40 are displayed. Samples shown in Figure 4 are usedto evaluate θil .
Figure 6: Activations in layers 1, 3, 5, 7, and 9 of the MLP with sigmoid activation functions withLCW. The input samples are the same as those used in Figure 2.
Figure 7: Boxplot summaries of θil on the first ten neurons in layers 1,3,5,7, and 9 of the MLP withsigmoid activation functions with LCW.
Figure 8: Activations in layers of the MLP with sigmoid activation functions with LCW after 10epochs training.
Figure 9: Boxplot summaries of θil of the MLP with sigmoid activation functions with LCW after10 epochs training.
Figure 10: Activations in layers 1, 10, 20, 30, and 40 of the MLP with ReLU activation functionswith LCW. The input samples are the same as those used in Figure 4.
Figure 11: Boxplot summaries of θil on the first ten neurons in layers 1, 10, 20, 30, and 40 of theMLP with ReLU activation functions with LCW.
Figure 12: Training accuracy, elapsed time, and test accuracy for CIFAR-10 dataset: (a-c) resultsof MLP(100, 128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activationfunctions are used in each model.
Figure 13: Distributions of weight gradients in each layer of MLP(20, 256) or MLPLCW(20, 256)with rectified linear activation functions.
Figure 14:	Training accuracy, elapsed time, and test accuracy for SVHN dataset: (a-c) results ofMLP(100, 128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activationfunctions are used in each model.
Figure 15:	Training accuracy, elapsed time, and test accuracy for CIFAR-100 dataset: (a-c) resultsof MLP(100,128), (d-f) results of MLP(50, 256), (g-i) results of MLP(5, 512). Sigmoid activationfunctions are used in each model.
