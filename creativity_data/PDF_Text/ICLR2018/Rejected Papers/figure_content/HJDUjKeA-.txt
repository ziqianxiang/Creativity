Figure 1: The pixel-to-object model.
Figure 2: Our calculation architecture consists of three streams: a global stream, a per-object stream,and a per-object-pair stream. Each “layer” consists of mixing, followed by a linear transformationand a ReLU activation (for non-final layers). The layers are repeated several times.
Figure 3: One surprising lesson was that the interaction network-style architectures need to be quitedeep to be effective (and learn Q-values), despite having “clear” representations for input. This issurprising as DQN and A3C have success with relative shallow architectures (4 to 5 layers includingboth convolutional and fully connected layers). These graphs show the effect on average episodescore on some of the Atari games, according to the number of layers in the interaction networkalone. The x-axis scale is one tick is 800k agent steps.
Figure 4: https://goo.gl/AEXEQP. Learning to play Seaquest from objects (from left toright, images are: inputs, decoded, mask, objects, and Q-values). The “oxygen remaining” bar isnot learnt in this run, but the submarine and other objects are learnt.
