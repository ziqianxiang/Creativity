Figure 1: A schematic depicting the Long Short-Term Memory (LSTM) model architecture. (A)Gating and activation functions for a single unit. (B) Unrolled LSTM network with Softmax outputat every time step during training.
Figure 2: A schematic depicting the LSTM transfer learning model architecture. Where gray indi-cates fixed and white indicates learned weights, both (A) and (B) depict a 2-step training processwith Subject 1 (S1) training in step 1 and Subject 2 (S2) training of only the affine layer in step 2.
Figure 3: Model performance comparison for representative Subject C. (A) Accuracy as a functionof the amount of training samples and (B) Accuracy as a function of time with respect to movementonset evaluated for different models and using all available training data. Error bars show standarderror of mean using 20 random partitions of the shuffled data.
Figure 4: Transfer learning LSTM performance comparison for a representative subject. The TL-Finetuned, TL, and random LSTM all utilize Subject B as S1 and Subject C as S2 . The subjectspecific model uses Subject C. (A) Accuracy as a function of the amount of training samples. (B)Cross-entropy error on the test set across epochs, using 10 training samples per class. Error barsshow standard error of mean averaging results from 20 random partitions of the shuffled data.
Figure 5: Visualizations of model parameters. (A) Weights of the learned affine mapping from S2(Subject C) to S1 (Subject B). Electrodes mapped from motor to sensorimotor and sensorimotor +occipital to sensorimotor regions of S2 to S1, respectively. Both TL and TL-Finetuned weights aredepicted. (B) Two-dimensional t-SNE embedding of the learned signal representation correspondingto different fingers for both subjects.
