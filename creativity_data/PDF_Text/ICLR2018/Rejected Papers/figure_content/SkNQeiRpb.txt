Figure 1: AutoEncoder consists of two neural networks, encoder and decoder, fused together onthe “representation” layer z. Encoder has 2 layers e1 and e2 and decoder has 2 layers d1 and d2 .
Figure 2: Training RMSE per mini-batch. All lines correspond to 4-layers autoencoder (2 layerencoder and 2 layer decoder) with hidden unit dimensions of 128. Different line colors correspondto different activation functions. TANH and SIGMOID lines are very similar as well as lines forELU and SELU. The best performing activation functions are ELU and SELU.
Figure 3: Single layer autoencoder with 128, 256, 512 and 1024 hidden units in the coding layer. A:training RMSE per epoch; B: evaluation RMSE per epoch.
Figure 4: Effects of dropout. Y-axis: evaluation RMSE, X-axis: epoch number. Model with nodropout (Drop Prob 0.0) clearly over-fits. Model with drop probability of 0.5 over-fits as well (butmuch slowly). Models with drop probabilities of 0.65 and 0.8 result in RMSEs of 0.9192 and 0.9183correspondingly.
Figure 5: Effects of dense re-feeding. Y-axis: evaluation RMSE, X-axis: epoch number. Baselinemodel was trained with learning rate of 0.001. Applying re-feeding step with the same learningrate almost did not help (Baseline RF). Learning rate of 0.005 (Baseline LR 0.005) is too big forbaseline model without re-feeding. However, increasing both learning rate and applying re-feedingstep clearly helps (Baseline LR 0.005 RF).
