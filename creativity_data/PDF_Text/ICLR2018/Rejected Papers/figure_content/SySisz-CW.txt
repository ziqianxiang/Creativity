Figure 1:	Left: schematic composition of coarse and finer scale features using two convolutionkernels in successive layers to form the eyes of a cartoon face. Right: Example of violation ofindependence of mechanisms between two successive layers. In both cases crosses indicate centerof patches (in light grey) affected by the activation of pixel in the previous layer.
Figure 2:	Architeture of the pretrained DCGAN generator used in our experiments. FC indicatesa fully connected layer, z is a 100-dimensional isotropic Gaussian vector, horizontal dimensionsindicate the number of channels of each layer. The output image size is 64 by 64 pixels and thesedimensions drop by a factor 2 from layer to layer.
Figure 3: 3a Example convolution kernels and corresponding Fourier transforms (zero frequenciesare located at the center of each picture), taken from layer 1 (for h) and 2 (for g) of the generatorof a trained DCGAN. 3b Illustration of the meaning of SDR values. 3c Illustration of the multiplecompositions of convolution kernels belonging to successive layers. The pathway depends on theconsidered input layer (blue), output layer (red) and intermediate layer (green) channels.
Figure 4: Superimposed histograms of spectral density ratios of a trained GAN generator and dis-criminator for layers at the same level of resolution (left to right from finer to coarser).
Figure 5: Superimposed histograms of spectral density ratios of trained VAE generator and discrim-inator for layers at the same level of resolution (left to right from finer to coarser).
Figure 6: Random face generated by a pretrained DCGAN (left column). Second column: outputof the same network when removing low energy filters from the third layer (reduction from 8,192to 4,260). Third column: output of the same network when removing the filters leading to thelowest average SDR (P < .9, leading to 3,684 filters). Fourth column: same when removing thefilters leading to the largest SDR (ρ > 1.45, leading to 3,660 filters). More examples are shown inappendix Fig. 10.
Figure 7: Example faces generated by VAEs with different combinations of network parameters:number channels in the last hidden layer and β parameter. Top: 32 channels, bottom: 64 channels.
Figure 8: SDR encoder histograms for different choices of VAE parameters.
Figure 9: Comparison of SDR decoder histograms between worst and best performing VAEs (32Channels∕β = 1 and 64 Channels∕β = .1, respectively).
Figure 10: Example generated figures using a pretrained DCGAN (left column). Second column:the output of the same network when removing low energy filters from the third layer (reductionof the number of filters from 8,192 to 4,260). Third column: the output of the same network whenremoving the filters leading to the lowest average SDR (ρ < .9, leading to 3,684 filters). Fourthcolumn: same when removing the filters leading to the largest average SDR (ρ > 1.45, leading to3,660 filters).
Figure 11: Random examples generated by a trained VAE.
Figure 12: Evolution of the spectral density ratios between successive layers as a function of trainingiteration when dropout is used between the two finer scale layers of the generator.
Figure 13: Evolution of generated examples (for a fixed latent input) as function of training iteration(same as Fig. 12) when dropout is used.
