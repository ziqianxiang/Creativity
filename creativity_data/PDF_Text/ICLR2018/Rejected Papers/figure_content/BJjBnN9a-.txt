Figure 1: 1D example of the joint learning of weights and kernels parameters. Crosses indicate datapoints, dots indicate cluster centers (red for positive weights and blue for negative), horizontal linesindicate length-scales and the magenta line indicates optimal occupancy probabilities. The clustercenters are vertically offset purely to facilitate length-scale visualization.
Figure 2: 1D example of continuous convolution between two occupancy functions. The input in(a) is convolved with the filter in (b) to produce the convolution results depicted in (c).
Figure 3: Diagram ofa 2-layer CCNN for image classification.
Figure 4: Diagram of a 2-layer CCAE for image reconstruction and unsupervised parameter initial-ization.
Figure 5: Examples of data used during experiments. The top row shows the original images, andthe bottom row shows their corresponding reconstructions using the proposed HL framework.
Figure 6: Comparison between classification results for the MNIST dataset, using different tech-niques. Lines indicate average values from 20 independent runs, and shades indicate min-max valuesbetween all runs.
Figure 7: Examples of CCNN filters for the MNIST dataset.
