Figure 1: (a) A 2-layer neural network with activation function φ, (b) Training and test accuracyon CIFAR10 with true and random labels on a 2-layer neural network with 512 ReLU hidden units,regularized with an additive penalty: (b1) no penalty, (b2) '2-norm, (b3) χ2-group path norm, (b4)'ι-path norm. The χ2-group path norm and 'ι-path norm were successful to close the generalizationgap for both true and random labels.
Figure 2:	Training an test performance on cat and airplane CIFAR10 images with true and randomlabels. Sine activation and mean-squared-error loss were used.
Figure 3:	Training and test performance on cat and airplane CIFAR10 images with true and randomlabels. ReLU activation and cross-entropy loss were used.
