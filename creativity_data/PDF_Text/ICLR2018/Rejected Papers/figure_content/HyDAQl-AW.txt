Figure 1: Illustrations of color-coded state-values and policies overlaid on our Two-Goal Gridworldproblem with two rewarding terminal states (50 for reaching the top-right corner and 20 for thebottom-left), a penalty of -1 for moving, and a time limit T = 3. (a) A standard agent withouttime-awareness which cannot distinguish between timeout terminations and environmental ones.
Figure 2: The color-coded learned action probabili-ties overlaid on our Queue of Cars problem (black andwhite indicate 0 and 1, respectively). For each block,the top row represents the dangerous action and thebottom row the safe one. The 9 non-terminal statesare represented horizontally. Left: a time-aware PPOagent at various times: the agent learns to optimally se-lect the dangerous action. Right: 5 different instancesof the time-unaware PPO agent.
Figure 3: Comparison of PPO with and without the remaining time in input. (a) Performance onthe Reacher-v1. (b) Performance on the InvertedPendulum-v1 (Left) and the learned state-valueestimations against episodic time steps and training progress (Right). Top: The results for Î³ = 0.99.
Figure 4: Comparison of PPO with and without the remaining time in input on Hopper-v1(T = 300). Left: Performance evaluations. Middle: The average last pose of the time-awarePPO agent, reproduced with aligned x-axes. A green mark indicates the last measured y coordinateof the agent used in the task with the termination threshold of 0.7 meters indicated with a red line.
Figure 5: Performance evaluations of PPO with and without partial-episode bootstrapping. (a) OnHopper-v1 with T = 200 during the training and T = 106 during the evaluations. (b) On Infinite-CubePusher with T = 50 during the training and T = 103 during the evaluations. The standardPPO agent degrades drastically after some time.
