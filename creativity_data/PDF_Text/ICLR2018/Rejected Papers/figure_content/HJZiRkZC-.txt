Figure 1: The autoencoder modelcharacters (Zhang et al., 2015) (Kim et al., 2016) and bytes (Gillick et al., 2016) (Zhang & Le-Cun, 2017) - is also being explored due to its promise in handling distinct languages in the samefashion. In particular, the work by Zhang & LeCun (2017) shows that simple one-hot encoding onbytes could give the best results for text classification in a variety of languages. The reason is that itachieved the best balance between computational performance and classification accuracy. Inspiredby these results, this article explores auto-encoding for text using byte-level convolutional networksthat has a recursive structure, as a first step towards low-level and non-sequential text generation.
Figure 2: Sequential and non-sequential decoders illustrated in graphical models. U is a vectorcontaining encoded representation. yi,s are output entities. hi,s are hidden representations. Notethat they both imply conditional independence between outputs conditioned by the representation u.
Figure 3: The reshaping process. This demon-strates the reshaping process for transforming arepresentation of feature size 4 and length 8 tofeature size 2 and length 16. Differen colors rep-resent different source features, and the numbersare indices in length dimension.
Figure 5: Byte-level errors with respect to randomly mutated samplesRegardless of dataset, all of our text auto-encoders are trained with the same hyper-parameters using stochastic gradient de-scent (SGD) with momentum (Polyak,1964) (Sutskever et al., 2013). The modelWe used has n = 8 - that is, there are 8 pa-rameterized layers in each of prefix, recur-sion and postfix module groups, for boththe encoder and decoder. Each trainingepoch contains 1,000,000 steps, and eachstep is trained on a randomly selected sam-ple with length up to 1024 bytes. There-fore, the maximum model depth is 160.
Figure 4: The histogram of length difference4	DiscussionThis section offers comparisons with recurrent networks, and studies on a set of different propertiesof our proposed auto-encoding model. Most of these results are performed using the enwiki dataset.
Figure 6: Byte-level error by length6Under review as a conference paper at ICLR 2018Figure 7: Histogram of sample frequencies in different lengthsthen the byte-level errors should be near 0 regardless of the mutation probability. Figure 5 showsthe results in these 2 ways of computing errors, and the result strongly indicates that the model hasnot degenerated to learning the identity function.
Figure 7: Histogram of sample frequencies in different lengthsthen the byte-level errors should be near 0 regardless of the mutation probability. Figure 5 showsthe results in these 2 ways of computing errors, and the result strongly indicates that the model hasnot degenerated to learning the identity function.
Figure 8: Errors during training for recursive and static models.
