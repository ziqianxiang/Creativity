Figure 1: A recurrent relational network on a fully connected graph with 3 nodes. The nodesâ€™hidden states hit are highlighted. The dashed lines indicate the recurrent connections. Subscriptsdenote node indices and superscripts denote steps t. For a figure of the same graph unrolled over 2steps see the appendix.
Figure 2: Example of how the trained network solves part of a Sudoku. Only the first column of afull 9x9 Sudoku is shown for clarity. See appendix for the full Sudoku. Each cell displays the digits1-9 with the font size scaled (non-linearly for legibility) to the probability the network assigns toeach digit. We only show steps 0, 1, 4, 8 and 12 due to space constraints. Notice how the networkeliminates the given digits 4 and 8 from the other cells in the first step. For this particular Sudokuthe network converges to the solution after approximately 20 steps. Animations showing how thetrained network solves Sodukos, including a failure case can be found at imgur.com/a/ALsfB.
Figure 3: Accuracy of our trained network on Sudokus as a function of number of steps. Evensimple Sudokus with 33 givens require about 10 steps of relational reasoning to be solved. Thedashed vertical line indicates the 32 steps the network was trained for. The network appears to havelearned a convergent relational reasoning algorithm such that more steps beyond 32 improve on thehardest Sudokus.
