Figure 1: AlexNet-BN: the generalization gap between training and testing loss is practically thesame for small (B=256) and large (B=8K) batches.
Figure 2: Training with LARS: AlexNet-BN with B=8KWe observed that the optimal LR do incresae with batch size, but not in linear or square root proportionas was suggested in theory: there is a relatively wide interval of base LRs which gives the "best"accuracy. For AlexNet-BN with B=16K for example, all LRs from [13;22] interval give almost thesame accuracy â‰ˆ 59.3.
Figure 3: AlexNet-BN, B=16K and 32k: Accuracy as function of LRDuring testing we used one model and 1 central crop. The baseline (B=256) accuracy is 73.8% forminimal augmentation. To match the state-of-the art accuracy from Goyal et al. (2017) and Cho et al.
Figure 4: Scaling ResNet-50 (no data augmentation) up to B=32K with LARS.
