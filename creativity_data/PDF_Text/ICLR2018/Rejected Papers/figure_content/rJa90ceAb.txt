Figure 1: The framework of the proposed method. The autoencoder network in the first row is usedto extract features from the input image. The obtained feature maps are fed to a dimension reductionmodule to reduce the dimension of the feature maps. Then the reduced features are used to generatenew filters in the filter generation module. Finally, the prediction network takes in the same inputimage and the generated filters to make the final prediction for high level tasks such as detection,classification and so on. “*” indicates the convolution operation.
Figure 2: Filter generation process. The input feature vector is fed to a fully connected layer and thelayer outputs sets of coefficients. Those coefficients linearly combine the base filters from a filterrepository and generate new filters.
Figure 3: Facial landmark detection results on MTFL dataset. D-Rot-FG indicates the baselinemodel trained on D-Rot with our generated filters. LE means left eye. RE means right eye. NTmeans nose tip. LMC means left mouth center. RMC means right mouth center. AVG means theaveraged mean error across all five landmarks. (a) Facial landmark detection using baseline modelM odel32 .(b) Facial landmark detection using baseline model M odel64 .
Figure 4: Training accuracy (a) and test accuracy (b) on CIFAR10.
Figure 5: Visualization of the distributions of the generated coefficients, filters, and feature mapsfrom the first (top row) and the second (bottom row) convolutional layer.
Figure 6: (a) Image samples from dataset D-Align. (b) Image samples from dataset D-Rot.
