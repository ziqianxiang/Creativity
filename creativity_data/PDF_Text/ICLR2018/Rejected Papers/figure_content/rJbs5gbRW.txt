Figure 1: Validation accuracy with different number of skip connections in CIFAR 100.
Figure 2: Validation accuracy with different network architectures in CIFAR100. (a) Absolute vali-dation accuracy with same number of parameters. (b) Relative validation accuracy with same num-ber of parameters. (c) Absolute validation accuracy with different number of parameters.
Figure 3: The absolute validation accuracy with varying training data size of different networkstructures in MNIST datasetOooooo9 8 7 6 5 420 ^∣≠-⅛ Dense_6_64_95956 "∙-∙ Plain_6_81_958431°0	20	40noise rateFigure 4: The validation accuracy with varying noise rate of different network structures in MNISTdatasetsimilar numbers of parameters among the three network architectures. Figure 3 illustrates that thedense skip connections have made the network more robust against the paucity of the training datasize.
Figure 4: The validation accuracy with varying noise rate of different network structures in MNISTdatasetsimilar numbers of parameters among the three network architectures. Figure 3 illustrates that thedense skip connections have made the network more robust against the paucity of the training datasize.
Figure 5: Learning process of sinc curve (training data number for DenseNet is 30 while for PlainNet is 400)In order to analyze the results more in detail, we both plot the network’s final learned curve andextract the output from each layer.
Figure 6: Loss with depth—statistical results of noisy training data (training data number: 60).
Figure 7:	15-layer Nets’ learning process of sinc curve (noise training data number: 60).
Figure 8:	Validation accuracy with different network architectures in CIFAR100. (a) Absolute vali-dation accuracy with same number of parameters. (b) Relative validation accuracy with same num-ber of parameters. (c) Absolute validation accuracy with different number of parameters.
Figure 9: Intermediate decision boundaries for three different models.
