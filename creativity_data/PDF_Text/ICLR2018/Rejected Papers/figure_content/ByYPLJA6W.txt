Figure 1: (Left) An example DRN with multiple input probability distributions and multiple hiddenlayers mapping to an output probability distribution. (Right) A connection unit in the network,with 3 input nodes in layer l - 1 connecting to a node in layer l. Each node encodes a probabilitydistribution, as illustrated by the probability density function Pk(l) . The tunable parameters are theconnecting weights and the bias parameters at the output node.
Figure 2: Propagation behavior for a connection unit with one input node. The biases are set as zeroin these examples. When weight is zero, the output distribution is flat. Positive weights causes theoutput distribution to have the same peak position as the input distribution while negative weightscauses the output pdf to ‘repel’ away from the input peak. When the weight is a sufficiently largepositive number, the propagation tends towards the identity mapping.
Figure 3: (a) Nonlinear transformation of the input means and standard deviations of gaussiansfor the synthetic dataset. (b) Example input-output pairs from the synthetic data, illustrating thecomplexity of the regression task.
Figure 4: (a) Comparison of L2 loss on the synthetic data test set. Note that the x-axis denotes thenumber of model parameters using the log scale. (b) Train and test loss for the individual methodsas number of model parameters increases. There is no overfitting as the gaps between train and testlosses are not significant.
Figure 5: (a) The regression by DRN on two test samples. (b) The learnt parameters for DRN areinterpreted as follows. The positive weight of 75.3 reflects the positive correlation between input andoutput peak positions and that the peak spreads out over time. The negative position of the absolutebias (λa) shows that the output peak is displaced leftwards of the input peak.
Figure 6: Single-layer network used in DRN for the stock dataset with 7 model parameters (3weights, 4 bias parameters).
Figure 7: Comparison of the (a) mean and (b) variance of the label distributions and predicteddistributions on the test set, for various k-days ahead predictions. The diagonal line represents aperfect fit where the predicted and labelled moments are equal. DRN outperforms the rest as its datapoints are closest to the diagonal line and it has the highest correlation coefficient (denoted by R)for all experiments.
Figure 8: The average absolute error of mean and variance across the three methods for predic-tion with varying number of days-ahead. DRN’s error is consistently the lowest compared to thebenchmark methods. The standard errors are smaller than the data point symbols.
