Figure 1: Left: Model architecture, where A is the normalized normalized adjacency matrix, I is theidentity matrix, X is node features matrix, and × is matrix-matrix multiply operator. We calculateK powers of the A, feeding each power into r GCNs, along with X. The output of all K × r GCNscan be concatenated along the column dimension, then fed into fully-connected layers, outputtingC channels per node, where C is size of label space. We calculate cross entropy error, betweenrows prediction N × C with known labels, and use them to update parameters of classification sub-network and all GCNs. Right: pre-relu activations after the first fully-connected layer of a 2-layerclassification sub-network. Activations are PCA-ed to 50 dimensions then visualized using t-SNE.
Figure 2: Sensitivity Analysis. Model performance when varying random walk steps K and replica-tion factor r. Best viewed with zoom. Overall, model performance increases with larger values of Kand r . In addition, having random walk steps (larger K) boosts performance more than increasingmodel capacity (larger r), as seen by the cross-section cuts on along the K-axis versus the r-axis.
Figure 3: Classification accuracy for the Cora dataset with 20 labeled nodes per class (|V| = 20×C),but features removed at random, averaging 10 runs. We use a different random seed for every run(i.e. removing different features per node), but the same 10 random seeds across models.
Figure 4: Attention weights (m) for N-GCNa When trained with feature removal perturbation on theCora dataset. Removing features shifts the attention weights to the right, suggesting the model isrelying more on long range dependencies.
