Figure 1: Traces of locations accessed by read/write heads for the Flip3rd task in three differenttraining setups. The y-axis represents time (descending); x-axis represents head locations. First twoNTMs are trained without partial trace information. White represents the distribution of the writehead; orange the distribution of the read head; (b) and (c) generalize and are more interpretable than(a); (c) was trained using partial trace information and is more interpretable than (b).
Figure 2: (a) is depiction of the generic NCM structure (b) is a high-level overview of an NTM and(c) is a high level overview of the NRAM architecture. The controller outputs a circuit, which inthis case contains the modules READ, ADD, SUB, LT, and WRITE. The controller encodes the twoinputs to the modules as probability vectors, a, b and c, over the possible choices. The most likelychoice is shown in green. The only input to the controller are the registers r1 and r2 .
Figure 3: An example circuit for the task of adding one to all memory cells. The arrows for registerupdates are shown in red. Technically, modules take two arguments, but some ignore an argument,such as INC or READ. For them, we show only the relevant arrows.
Figure 4:	Relative percentage of training instances which generalized out of ten runs per task for theNTM. We provide a subtrace 100% of the time and use λ = 1. x-axis shows the type of supervision.
Figure 5:	The number of initial runs which generalized for Flip3rd. The first dimension listedin the rows controls the execution details revealed in a subtrace, while the second dimension (thedensity column) controls the proportion of examples that receive extra subtrace supervision.
Figure 6: Execution traces of two NTMs trained on Flip3rd until generalization. First is baseline(no trace supervision); second is trained with corner hints. Time flows top to bottom. The first panefrom every pair shows the value written to tape; second shows head locations. Figures show that alittle extra supervision helps the NTM write sharper 0-1 values and have more focused heads.
Figure 7:	(a) average number of errors after training had completed for NRAM. Observe that fulltraining results in a significantly higher percent of generalization after training stopped. (b) shows thedistribution of errors to problem length for Permute (one character of noise in 10% of samples).
Figure 8:	A heirarchy of supervision types (but not quantities) for NTMs.
Figure 9:	Baselines have generalization on over 40 different initial weights. Other tests use 10.
Figure 10:	The number of runs which completed for each task and subtrace type. The Data in thegraphs below is taken by averaging the results of these runs.
Figure 11:	The average time (in seconds) to finish training for each task and subtrace type. For mosttasks it is clear that Full traces while introducing extra computations to individual timesteps, reducethe amount of time to finish training over not using supervision.
Figure 12: The average number of errors on the test set for each task and subtrace type once trained.
Figure 13: Comparing average generalization tosequence length for Swaploan。。U- sə-duJESISəi JO %• Circuita SingIeHint■ Registers* None♦ Corners* Full• SingIeTimest▲ NoisyFuIITest DifficultyFigure 14: Comparing average generalization tosequence length for Increment17Under review as a conference paper at ICLR 2018s3t8uω3-dEBS ISælt5% 36alφ>4T est DifficultyFigure 15: Comparing average generalization tosequence length for Permute
Figure 14: Comparing average generalization tosequence length for Increment17Under review as a conference paper at ICLR 2018s3t8uω3-dEBS ISælt5% 36alφ>4T est DifficultyFigure 15: Comparing average generalization tosequence length for Permutes3t8uω3-dEBS ISSt5%T est DifficultyFigure 16: Comparing average generalization tosequence length for ListK• SingIeHintχ None■ Corners• Full♦ SingIeTimestepTest DifficultyFigure 17: Comparing average generalization tosequence length for Merge
Figure 15: Comparing average generalization tosequence length for Permutes3t8uω3-dEBS ISSt5%T est DifficultyFigure 16: Comparing average generalization tosequence length for ListK• SingIeHintχ None■ Corners• Full♦ SingIeTimestepTest DifficultyFigure 17: Comparing average generalization tosequence length for Mergelɔəhoɔu-sə-dEES ISəljθ 求Figure 18: Comparing average generalization of the DNGPU Freivalds & Liepins(2017) with that of the NRAM using the full tracer for Merge. For this exper-iment, a maximum of 10000 samples were used for the DNGPU and 5000 forthe NRAM. The DNGPU was run out of the box from the code supplied by theauthors. 20 runs were averaged for the DNGPU and 38 runs for the NRAM. One
Figure 16: Comparing average generalization tosequence length for ListK• SingIeHintχ None■ Corners• Full♦ SingIeTimestepTest DifficultyFigure 17: Comparing average generalization tosequence length for Mergelɔəhoɔu-sə-dEES ISəljθ 求Figure 18: Comparing average generalization of the DNGPU Freivalds & Liepins(2017) with that of the NRAM using the full tracer for Merge. For this exper-iment, a maximum of 10000 samples were used for the DNGPU and 5000 forthe NRAM. The DNGPU was run out of the box from the code supplied by theauthors. 20 runs were averaged for the DNGPU and 38 runs for the NRAM. Onecan deduce that while neither is able to generalize this task perfectly, the simplerand easier to understand architecture, NRAM, does generalize better with fewerexamples when those examples come with richer supervision.
Figure 17: Comparing average generalization tosequence length for Mergelɔəhoɔu-sə-dEES ISəljθ 求Figure 18: Comparing average generalization of the DNGPU Freivalds & Liepins(2017) with that of the NRAM using the full tracer for Merge. For this exper-iment, a maximum of 10000 samples were used for the DNGPU and 5000 forthe NRAM. The DNGPU was run out of the box from the code supplied by theauthors. 20 runs were averaged for the DNGPU and 38 runs for the NRAM. Onecan deduce that while neither is able to generalize this task perfectly, the simplerand easier to understand architecture, NRAM, does generalize better with fewerexamples when those examples come with richer supervision.
Figure 18: Comparing average generalization of the DNGPU Freivalds & Liepins(2017) with that of the NRAM using the full tracer for Merge. For this exper-iment, a maximum of 10000 samples were used for the DNGPU and 5000 forthe NRAM. The DNGPU was run out of the box from the code supplied by theauthors. 20 runs were averaged for the DNGPU and 38 runs for the NRAM. Onecan deduce that while neither is able to generalize this task perfectly, the simplerand easier to understand architecture, NRAM, does generalize better with fewerexamples when those examples come with richer supervision.
