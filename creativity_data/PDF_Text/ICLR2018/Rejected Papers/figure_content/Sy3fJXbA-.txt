Figure 1: Different types of building blocks for modular network design: (a) a prototypical residualblock with bottleneck convolutional layers (He et al., 2016); (b) the multi-branch RexNeXt moduleconsisting of C parallel residual blocks (Xie et al., 2017); (c) our approach replaces the fixedaggregation points of RexNeXt with learnable masks m defining the input connections for eachindividual residual block.
Figure 2: Varying the fan-in (K) of our model, i.e., thenumber of active branches provided as input to eachresidual block. The plot reports accuracy achieved onCIFAR-100 using a network stack of L = 6 ResNeXtmodules having cardinality C = 8 and bottleneckwidth W = 4. All models have the same number ofparameters (0.28M). The best accuracy is obtained forK = 4.
Figure 3: A visualization of the fixed branch connectivity of ResNeXt (left)versus the connectivity learned by our method (right) using (K = 1). Eachgreen square is a residual block, each row of C = 8 square is a multi-branch module. The network consists of a stack of L = 9 modules. Arrowsindicate pathways connecting residual blocks of adjacent modules. In eachnet, the top red circle is a convolutional layer, the bottom circle is the finalfully-connected layer. It can be noticed that MaskConnect learns sparseconnections. The squares without in/out edges are those deemed superfluousby our algorithm and can be pruned at the end of learning. This gives rise toa branching factor that varies along the depth of the net.
Figure 4: Number of active branches as a function of module depth for model {D = 29, w = 4, C =8} trained on CIFAR-100. We report how the number of active branches varies for model trainedwith fan-in K = 1 as well as for the net trained with K = 4. The setting K = 1 tends to leave manyblocks unused, especially in the early modules of the network.
Figure 5: Number of active branches as a function of module depth for model {D = 50, w = 4, C32} trained on ImageNet, using fan-in K = 16.
