Figure 1: Illustration from left to right: (a) a flexible transformations of variables with an independentautoregressive scheme; (b) no transformations of variables with a flexible autoregressive scheme;and (c) a transformation autogressive network (TAN) that has a flexible transformation and a flexibleautoregressive scheme.
Figure 2: Illustration of both LAM (left) and RAM (right) models. Hidden states hk ’s are updatedand then used to compute the parameters of the next conditional density for xk. Note that in LAMthe hidden states hj’s are not tied together, where in RAM the hidden state hj along with xj are usedto compute the hidden state hj+i which determines the parameters of p(xj+i ∣ hj+i).
Figure 3: RNN+4xSRNN+Re & RAM model samples. Each plot shows a single sample. We plot thesample values of unpermuted dimensions y4, . . . , y32 ∣ y1 , y2 , y3 in blue and the expected value ofthese dimensions (i.e. without the Markovian noise) in green. One may see that the model is able tocorrectly capture both the sinusoidal and random walk behavior of our data.
Figure 4: Samples from L RNN+4xAdd+Re & LAM, and L RNN & RAM models on unit scaled,and logit transformed pixels.
