title,year,conference
 On the emergence of invariance and disentangling in deeprepresentations,2017, arXiv preprint arXiv:1706
 Deep variational informationbottleneck,2016, arXiv preprint arXiv:1612
 Bayesian recurrent neural networks,2017, arXivpreprint arXiv:1704
 Sequential neural modelswith stochastic layers,2016, In Advances in Neural Information Processing Systems
 Dropout as a bayesian approximation: Representing modeluncertainty in deep learning,2015, arXiv preprint arXiv:1506
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in Neural Information Processing Systems
 Identity mappings in deep residualnetworks,2016, In European Conference on Computer Vision
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Regularizing rnns by stabilizing activations,2015, arXiv preprintarXiv:1511
 Training rnns as fast as cnns,2017, arXiv preprint arXiv:1709
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Statistical language models based on neural networks,2012, Presentation at Google
 Regularizingneural networks by penalizing confident output distributions,2017, arXiv preprint arXiv:1701
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Recurrenthighway networks,2016, arXiv preprint arXiv:1607
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
