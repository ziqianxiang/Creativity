title,year,conference
 Learning to learn by gradient descent by gradient descent,2016, arXiv preprintarXiv:1606
 Learning a synaptic learning rule,1991, In Neural Networks
 Gradient-based optimization of hyperparameters,2000, Neural computation
 3D hand tracking by rapidstochastic gradient descent using a skinning model,2004, In Visual Media Production
 Ranking learning algorithms: Usingibl and meta-learning on accuracy and time results,2003, Machine Learning
 Learning step size controllers for robustneural network training,2016, In Thirtieth AAAI Conference on Artificial Intelligence
 Initializing bayesian hyperparameteroptimization via meta-learning,2015, In AAAI
 Deep q-networksfor accelerating the training of deep neural networks,2016, arXiv preprint arXiv:1606
 Using deep q-learning to control optimization hyperparameters,2016, arXiv preprintarXiv:1602
 Long short-term memory,1997, Neural computation
 Sequential model-based optimization forgeneral algorithm configuration,2011, In Learning and Intelligent Optimization
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 End-to-end training of deep visuo-motor policies,2015, arXiv preprint arXiv:1504
 Learning to optimize,2016, CoRR
 Gradient-based hyperparameter optimiza-tion through reversible learning,2015, arXiv preprint arXiv:1502
 Optimization on a budget: A reinforcementlearning approach,2009, In Advances in Neural Information Processing Systems
 Optimal ordered problem solver,2004, Machine Learning
 Practical bayesian optimization of machinelearning algorithms,2012, In Advances in neural information processing systems
 Multi-task bayesian optimization,2013, In Advancesin neural information processing systems
 A perspective view and survey of meta-learning,2002, ArtificialIntelligence Review
 Bregman alternating direction method of multipliers,2014, CoRR
