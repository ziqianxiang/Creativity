title,year,conference
 Theano: A Python framework for fast computation of mathematical expressions,2016, arXivpreprint arXiv:1605
 Natural gradient works efficiently in learning,1998, Neural computation
 Amari and H,2007, Nagaoka
 Automatic differentiation in machine learning:a survey,2015, arXiv preprint arXiv:1502
 Sample size selection in optimization methods for machinelearning,2012, Mathematical Programming
 Revisiting distributed synchronous SGD,2016, arXiv preprintarXiv:1604
 Large Scale Machine Learning,2004, PhD thesis
 Distributeddeep learning using synchronous stochastic gradient descent,2016, arXiv preprint arXiv:1602
 Identifying and attacking the saddlepoint problem in high-dimensional non-convex optimization,2014, In Advances in Neural Information ProcessingSystems
 Largescale distributed deep networks,2012, In Advances in Neural Information Processing Systems
 Lasagne: First release,2015,
 Sharp minima can generalize for deep nets,2017, arXiv preprintarXiv:1703
 Deep residual learning for image recognition,2016, In The IEEE Conference onComputer Vision and Pattern Recognition (CVPR)
 Large scale distributed Hessian-free optimization for deepneural network,2016, arXiv preprint arXiv:1606
 A fast learning algorithm for deep belief nets,2006, Neural Computation
 Long short-term memory,1997, Neural COmpUtatiOn
 On large-batch training for deeplearning: Generalization gap and sharp minima,2016, CoRR
 Adam: A method for stochastic optimization,2014, arXiv preprint arXiv:1412
 Training neural networks with stochastic Hessian-free optimization,2013, arXiv preprint arXiv:1301
 Topmoumoute online natural gradient algorithm,2008, In Advances inNeural Information Processing Systems
 Gradient-based learning applied to document recognition,2001, InIntelligent Signal Processing
 Efficient backprop,1998, In Neural networks: Tricks Ofthetrade
 Deep learning via Hessian-free optimization,2010, In International Conference on Machine Learning(ICML)
 Second-order optimization for neural networks,2016, PhD thesis
 Optimizing neural networks with Kronecker-factored approximate curvature,2015, InInternational Conference on Machine Learning (ICML)
 Training deep and recurrent networks with Hessian-free optimization,2012, In Neuralnetworks: Tricks of the trade
 Revisiting natural gradient for deep networks,2014, In International Conference onLearning Representations (ICLR)
 Fast exact multiplication by the Hessian,1994, Neural computation
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Hogwild: A lock-free approach to parallelizing stochastic gradientdescent,2011, In Advances in Neural Information Processing Systems
 Fast curvature matrix-vector products for second-order gradient descent,2002, Neural computation
 On the importance of initialization and momentum indeep learning,2013, International Conference on Machine Learning (ICML)
 Mini-batch primal and dual methods for SVMs,2013, InInternational Conference on Machine Learning (ICML)
 On the truncated conjugate gradient method,2000, Mathematical Programming
 Deep learning with elastic averaging SGD,2015, In Advances in NeuralInformation Processing Systems
