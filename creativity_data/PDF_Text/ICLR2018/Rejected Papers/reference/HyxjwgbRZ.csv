title,year,conference
 Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-ConvexParameter,2017, ICML
 Natasha 2: Faster Non-Convex Optimization Than SGD,1708, arXiv:1708
 Identifying and Attacking the Saddle Point Problem in High-dimensional Non-convexOptimization,2014, In NIPS
 GradientDescent Can Take Exponential Time to Escape Saddle Points,2017, arXiv:1705
 Qualitatively characterizing neural networkoptimization problems,1412, ICLR
 DRAW: A Recur-rent Neural Network For Image Generation,2015, ICML
 Deep residual learning for image recog-nition,2016, In CVPR
 How to EscapeSaddle Points Efficiently,1703, ICML
 Adam: A Method for Stochastic Optimization,2015, ICLR
 Learning multiple layers of features from tiny images,2009, Technical report
 Imagenet classification with deep convo-lutional neural networks,2012, In Advances in neural information processing systems
 Difacto: Distributed factorization ma-chines,2016, In WSDM
 A direct adaptive method for faster backpropagation learning: theRPROP algorithm,1993, In IEEE International Conference on Neural Networks
 Scalable distribUted dnn training Using commodity gpU cloUd compUting,2015, In SixteenthAnnual Conference of the International Speech Communication Association
 Tern-grad: Ternary gradients to redUce commUnication in distribUted deep learning,2017, arXiv preprintarXiv:1705
