title,year,conference
 High-dimensional dynamics of generalization error in neuralnetworks,2017, arXiv preprint arXiv:1710
 Deep speech 2: End-to-end speech recognition in english and mandarin,2016, In International Conference on MachineLearning
 A closer look at memorization in deep netWorks,2017, In Doina Precup and Yee WhyeTeh (eds
 Deeprelaxation: partial differential equations for optimizing deep neural networks,2017, arXiv preprintarXiv:1704
 Stochastic gradient Hamiltonian Monte Carlo,2014, In Proceedingsof the 31st International Conference on Machine Learning
 Qualitatively characterizing neural networkoptimization problems,2014, arXiv preprint arXiv:1412
 Flat minima,1997, Neural Computation
 Bayesian methods for adaptive models,1992, PhD thesis
 APProximation analysis of stochastic gradient langevin dynam-ics by using fokker-Planck equation and ito Process,2014, In Eric P
 Covariance-controlledadaPtive Langevin thermostat for large-scale Bayesian samPling,2015, In C
 OPening the black box of deeP neural networks via informa-tion,2017, CoRR
 Very deeP convolutional networks for large-scale imagerecognition,2014, arXiv preprint arXiv:1409
 Towards understanding generalization of deep learning: Perspectiveof loss landscapes,2017, arXiv preprint arXiv:1706
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
2 WhiCh shoW the exChange-ability of learning rate and batCh size,2018, In Fig
