title,year,conference
 Automizing stochastic optimization withgradient variance estimates,2017, In Automatic Machine Learning Workshop at ICML 2017
 Improving the convergence of back-propagation learning with secondorder methods,1988, In Proceedings of the 1988 Connectionist Models Summer School
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 The subgroup algorithm for generating uniform randomvariables,1987, Probability in the Engineering and Informational Sciences
 ADAM: A method for stochastic optimization,2015, The InternationalConference on Learning Representations (ICLR)
 Learning multiple layers of features from tiny images,2009, Technical report
 Gradient-based learning applied todocument recognition,1998, Proceedings of the IEEE
 Probabilistic line searches for stochastic optimization,2015, InAdvances in Neural Information Processing Systems 28
 Early stopping without avalidation set,2017, arXiv preprint arXiv:1703
 A direct adaptive method for faster backpropagation learn-ing: The RPROP algorithm,1993, In Neural Networks
 No more pesky learning rates,2013, In Proceedings of the30th International Conference on Machine Learning (ICML)
 1-bit stochastic gradient descent and itsapplication to data-parallel distributed training of speech DNNs,2014, In Fifteenth Annual Conferenceof the International Speech Communication Association
 Striving forsimplicity: The all convolutional net,2014, arXiv preprint arXiv:1412
 RMSPROP: Divide the gradient by a running average of itsrecent magnitude,2012, COURSERA: Neural networks for machine learning
 ADADELTA: An adaptive learning rate method,2012, arXiv preprint arXiv:1212
