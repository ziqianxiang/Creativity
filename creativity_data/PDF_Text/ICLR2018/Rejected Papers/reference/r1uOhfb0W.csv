title,year,conference
 Learning the number of neurons in deep networks,2016, InAdvances in Neural Information Processing Systems
 Optimizing performance of recurrent neuralnetworks on gpus,2016, arXiv preprint arXiv:1604
 Bayesian group-sparse modeling and varia-tional inference,2014, IEEE transactions on signal processing
 Bayesian dark knowl-edge,2015, In Advances in Neural Information Processing Systems
 Entropy-sgd: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Stochastic gradient hamiltonian monte carlo,2014, InInternational Conference on Machine Learning
 A theoretically grounded application of dropout in recurrentneural networks,2016, In Advances in neural information processing systems
 Ese: Efficient speech recognition engine with sparse lstm on fpga,2017, InFPGA
 Neural network ensembles,1990, IEEE transactions on patternanalysis and machine intelligence
 Distilling the knowledge in a neural network,2015, arXivpreprint arXiv:1503
 Long short-term memory,1997, Neural computation
 Network trimming: A data-drivenneuron Pruning aPProach towards efficient deeP architectures,2016, arXiv preprint arXiv:1607
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Preconditioned stochasticgradient langevin dynamics for deeP neural networks,2016, In AAAI
 Sgdr: stochastic gradient descent with restarts,2016, arXiv preprintarXiv:1608
 Building a large annotatedcorPus of english: The Penn treebank,1993, Computational linguistics
 Regularizing and oPtimizing lstm lan-guage models,2017, arXiv preprint arXiv:1708
 ExPloring sParsity in recurrentneural networks,2017, arXiv preprint arXiv:1704
 GrouP sParse regu-larization for deeP neural networks,2017, Neurocomputing
 Learning structured sParsity indeeP neural networks,2016, In Advances in Neural Information Processing Systems
 Breaking the softmaxbottleneck: A high-rank rnn language model,2017, arXiv preprint arXiv:1711
