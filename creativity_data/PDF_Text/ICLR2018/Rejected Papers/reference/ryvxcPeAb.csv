title,year,conference
 Synthesizing robust adversarial examples,2017, arXiv preprintarXiv:1707
 Towards evaluating the robustness of neural networks,2016, arXivpreprint arXiv:1608
 Adversarial examples are not easily detected: Bypassing tendetection methods,2017, arXiv preprint arXiv:1705
 Detecting adversarialsamples from artifacts,2017, arXiv preprint arXiv:1703
 Explaining and harnessing adversarialexamples,2014, arXiv preprint arXiv:1412
 Adversarial examples detection in deep networks with convolutional filterstatistics,2016, arXiv preprint arXiv:1612
 Delving into transferable adversarial exam-ples and black-box attacks,2016, arXiv preprint arXiv:1611
 No need to worry about adversarialexamples in object detection in autonomous vehicles,2017, arXiv preprint arXiv:1707
 Deepfool: a simple andaccurate method to fool deep neural networks,2015, arXiv preprint arXiv:1511
 Distilla-tion as a defense to adversarial perturbations against deep neural networks,2015, arXiv preprintarXiv:1511
 Smoothgrad:removing noise by adding noise,2017, arXiv preprint arXiv:1706
 Intriguing properties of neural networks,2013, arXiv preprint arXiv:1312
 Ensembleadversarial training: Attacks and defenses,2017, arXiv preprint arXiv:1705
 The spaceof transferable adversarial examples,2017, arXiv preprint arXiv:1704
