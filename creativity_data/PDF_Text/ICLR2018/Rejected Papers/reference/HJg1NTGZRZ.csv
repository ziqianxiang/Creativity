title,year,conference
 Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and NeuralNetworks,2017, arXiv preprint arXiv:1704
 Learning Separable Fixed-Point Kernels for Deep Convolu-tional Neural Networks,2016, In Acoustics
 Compressing NeuralNetworks with the Hashing Trick,2015, In ICML
 Compressing ConvolutionalNeural Networks,2015, arXiv preprint arXiv:1506
 Towards the Limit of Network Quantization,2016, arXiv preprintarXiv:1612
 Memory Bounded Deep Convolutional Networks,2014, arXiv preprintarXiv:1412
 Exploiting Linear StructureWithin Convolutional Networks for Efficient Evaluation,2014, In Advances in Neural Information ProcessingSystems
 Asymptotically Efficient Quantizing,1968, IEEE Transactions on Information Theory
 Compressing Deep Convolutional Networks UsingVector Quantization,2014, arXiv preprint arXiv:1412
 Deep Learning With LimitedNumerical Precision,2015, In ICML
 Learning Both Weights And Connections For EfficientNeural Network,2015, In Advances in Neural Information Processing Systems
 Understanding the impactof precision quantization on the accuracy and energy of neural networks,2016, arXiv preprint arXiv:1612
 Deep Residual Learning for Image Recognition,2016, InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition
 Quantized Neu-ral Networks: Training Neural Networks With Low Precision Weights and Activations,2016, arXiv preprintarXiv:1609
 Batch Normalization: Accelerating Deep Network Training By ReducingInternal Covariate Shift,2015, arXiv preprint arXiv:1502
 Training Skinny Deep Neural Networks WithIterative Hard Thresholding Methods,2016, arXiv preprint arXiv:1607
 Proteus: Exploiting Numerical Precision Variability In Deep Neural Networks,2016, In Proceedings ofthe 2016 International Conference on Supercomputing
 Alternating Direction Method of Multipliers forSparse Convolutional Neural Networks,2016, arXiv preprint arXiv:1611
 Adam: A Method For Stochastic Optimization,2014, arXiv preprint arXiv:1412
 Learning Multiple Layers Of Features From Tiny Images,2009, 2009
 Gradient-Based Learning Applied To DocumentRecognition,1998, Proceedings ofthe IEEE
 Pruning Filters For EfficientConvnets,2016, arXiv preprint arXiv:1608
 Towards Convolutional Neural Networks Compres-sion Via Global Error Reconstruction,2016, International Joint Conferences on Artificial Intelligence
 Neural Networks With FewMultiplications,2015, arXiv preprint arXiv:1510
 Mixed Low-Precision Deep Learning Inference Using Dynamic Fixed Point,2017, arXiv preprint arXiv:1701
 Deep Neu-ral Networks Are Robust To Weight Binarization And Other Non-Linear Distortions,2016, arXiv preprintarXiv:1606
 Learning Separable Filters,2013, In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition
 Fixed-Point Optimization Of Deep Neural Networks WithAdaptive Step Size Retraining,2017, arXiv preprint arXiv:1702
 Convolutional Neural Networks With Low-RankRegularization,2015, arXiv preprint arXiv:1511
 Reducing The Model Order Of Deep Neural Networks UsingInformation Theory,2016, In VLSI (ISVLSI)
 Soft Weight-Sharing For Neural Network Compression,2017, arXivpreprint arXiv:1702
 Accelerating Deep Convolutional Networks UsingLow-Precision And Sparsity,2016, arXiv preprint arXiv:1610
 Scalable Compression Of Deep Neural Networks,2016, In Proceedings of the 2016 ACMon Multimedia Conference
 Training Ternary Neural Networks With ExactProximal Operator,2016, arXiv preprint arXiv:1612
 Less Is More: Towards Compact CNNs,2016, In European Conferenceon Computer Vision (ECCV)
 Dorefa-Net: Training LowBitwidth Convolutional Neural Networks With Low Bitwidth Gradients,2016, arXiv preprint arXiv:1606
 Trained Ternary Quantization,2016, arXiv preprintarXiv:1612
