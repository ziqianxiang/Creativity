title,year,conference
 BranchConnect: Large-Scale Visual Recognition withLearned Branch Connections,2017, arXiv preprint arXiv:1704
 Layer normalization,2016, arXiv preprintarXiv:1607
 Neural machine translation by jointlylearning to align and translate,2014, arXiv preprint arXiv:1409
 Quasi-recurrent neural net-works,2016, arXiv preprint arXiv:1611
 Learning phrase representations using RNN encoder-decoderfor statistical machine translation,2014, arXiv preprint arXiv:1406
 Shake-Shake regularization,2017, arXiv preprint arXiv:1705
 A convolutional encoder modelfor neural machine translation,2016, arXiv preprint arXiv:1611
 ConvolutionalSequence to Sequence Learning,2017, arXiv preprint arXiv:1705
 Speech recognition with deep recur-rent neural networks,2013, In Acoustics
 Long short-term memory,1997, Neural computation
 Tying Word Vectors and Word Classifiers:A Loss Framework for Language Modeling,2016, arXiv preprint arXiv:1611
 Neural machine translation in linear time,2016, arXiv preprint arXiv:1610
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Recurrent neural networks for speech modelingand speech recognition,1995, In Acoustics
 Training RNNs as fast as CNNs,2017, arXiv preprint arXiv:1709
 A structured self-attentive sentence embedding,2017, arXiv preprintarXiv:1703
 Effective approaches to attention-based neural machine translation,2015, arXiv preprint arXiv:1508
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Regularizing and optimizing LSTMlanguage models,2017, arXiv preprint arXiv:1708
 A decomposable attentionmodel for natural language inference,2016, arXiv preprint arXiv:1606
 A deep reinforced model for abstractivesummarization,2017, arXiv preprint arXiv:1705
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 Neural machine translation of rare words withsubword units,2015, arXiv preprint arXiv:1508
 Attention is all you need,2017, arXiv preprint arXiv:1706
 Aggregated residual trans-formations for deep neural networks,2016, arXiv preprint arXiv:1611
 Deep recurrent models With fast-forWardconnections for neural machine translation,2016, arXiv preprint arXiv:1606
