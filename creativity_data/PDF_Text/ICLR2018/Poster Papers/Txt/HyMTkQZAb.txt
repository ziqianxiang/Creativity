Published as a conference paper at ICLR 2018
Kronecker-factored Curvature Approxima-
tions for Recurrent Neural Networks
James Martens
DeepMind
jamesmartens@google.com
Jimmy Ba
Department of Computer Science
University of Toronto
Toronto, Canada
jimmy@psi.toronto.edu
Matthew Johnson
Google Brain
mattjj@google.com
Ab stract
Kronecker-factor Approximate Curvature (Martens & Grosse, 2015) (K-FAC) is
a 2nd-order optimization method which has been shown to give state-of-the-art
performance on large-scale neural network optimization tasks (Ba et al., 2017). It
is based on an approximation to the Fisher information matrix (FIM) that makes
assumptions about the particular structure of the network and the way it is pa-
rameterized. The original K-FAC method was applicable only to fully-connected
networks, although it has been recently extended by Grosse & Martens (2016)
to handle convolutional networks as well. In this work we extend the method
to handle RNNs by introducing a novel approximation to the FIM for RNNs.
This approximation works by modelling the statistical structure between the gra-
dient contributions at different time-steps using a chain-structured linear Gaussian
graphical model, summing the various cross-moments, and computing the inverse
in closed form. We demonstrate in experiments that our method significantly out-
performs general purpose state-of-the-art optimizers like SGD with momentum
and Adam on several challenging RNN training tasks.
1	Introduction
As neural networks have become ubiquitous in both research and applications the need to efficiently
train has never been greater. The main workhorses for neural net optimization are stochastic gradient
descent (SGD) with momentum and various 2nd-order optimizers that use diagonal curvature-matrix
approximations, such as RMSprop (Tieleman & Hinton, 2012) and Adam (Ba & Kingma, 2015).
While the latter are typically easier to tune and work better out of the box, they unfortunately only
offer marginal performance improvements over well-tuned SGD on most problems.
Because modern neural networks have many millions of parameters it is computationally too ex-
pensive to compute and invert an entire curvature matrix and so approximations are required. While
early work on non-diagonal curvature matrix approximations such as TONGA (Le Roux et al., 2008)
and the Hessian-free (HF) approach (Martens, 2010; Martens & Sutskever, 2011; 2012; Desjardins
et al., 2013; Sainath et al., 2013) demonstrated the potential of such methods, they never achieved
wide adoption due to issues of scalability (to large models in the case of the former, and large
datasets in the case of the latter).
Motivated in part by these older results and by the more recent success of centering and normaliza-
tion methods (e.g. Schraudolph, 1998; Vatanen et al., 2013; Ioffe & Szegedy, 2015) a new family
of methods has emerged that are based on non-diagonal curvature matrix approximations the rely
on the special structure of neural networks. Such methods, which include Kronecker-factored ap-
proximated curvature (K-FAC) (Martens & Grosse, 2015), Natural Neural Nets (Desjardins et al.,
2015), Practical Riemannian Neural Networks (Marceau-Caron & Ollivier, 2016), and others (Povey
1
Published as a conference paper at ICLR 2018
et al., 2015), have achieved state-of-the-art optimization performance on various challenging neural
network training tasks and benchmarks.
While the original K-FAC method is applicable only to standard feed-forward networks with fully
connected layers, it has recently been extended to handle convolutional networks (Grosse & Martens,
2016) through the introduction of the “Kronecker Factors for Convolution” (KFC) approximation.
Ba et al. (2017) later developed a distributed asynchronous version which proposed additional ap-
proximations to handle very large hidden layers.
In this work we develop a new family of curvature matrix approximations for recurrent neural net-
works (RNNs) within the same design space. As in the original K-FAC approximation and the KFC
approximation, we focus on the Fisher information matrix (a popular choice of curvature matrix),
and show how it can be approximated in different ways through the adoption of various approximat-
ing assumptions on the statistics of the network’s gradients. Our main novel technical contribution
is an approximation which uses a chain-structured linear Gaussian graphical model to describe the
statistical relationship between gradient contributions coming from different time-steps. Somewhat
remarkably, it is possible to sum the required cross-moments to obtain a Fisher approximations
which has enough special algebraic structure that it can still be efficiently inverted. In experiments
we demonstrate the usefulness of our approximations on several challenging RNN training tasks.
2	Notation and background
2.1	Network, loss, and objective function
We denote by f (x, θ) the neural network function associated evaluated on input x, where θ are the
parameters. We will assume a loss function of the form L(y, z) = - logr(y|z), where r is the den-
sity function associated with a predictive distribution R. The loss associated with a single training
case is then given by L(y, f(x, θ)) ≡ - logr(y|f(x, θ)). Throughout the rest of this document we
will use the following special notation for derivatives of the single-case loss w.r.t. some arbitrary
variable Z (possibly matrix-valued):
DZ _ dL(y,f (X,θ))
=	dZ
The objective function which We wish to minimize is the expected loss h(θ) = EQ[L(y, f (x, θ))]
over the training distribution Q on x and y .
2.2	The Fisher, the natural gradient, and 2nd-order optimization
The Fisher information matrix (aka “the Fisher”) associated with the model’s predictive distribution
Py∣x(θ) is given by
F = IE [DθDθ>] = CoV(Dθ, Dθ).
Note that here, and for the remainder of this paper, y is taken to be distributed according to the
model's predictive conditional distribution Py∣χ(θ), so that E[DZ] = 0 for any variable Z that is
conditionally independent of y given the value of f(x, θ) (this includes Dθ). All expectations and
covariances are defined accordingly. This is done because the expectation that defines the Fisher
information matrix uses Py∣χ(θ). If we were to instead use the training distribution Qy∣χ on y,
we would essentially be computing to the “empirical Fisher” (or approximations thereof), which as
argued by Martens (2014) is a less appropriate choice for a curvature matrix than the true Fisher.
The natural gradient is defined as F-1Vh, and is the update direction used in natural gradient
descent. As argued by Amari (1998), natural gradient descent has the two key advantages: it is
invariant to the parameterization of the model, and has “Fisher efficient” convergence1. However,
as shown by Martens (2014) these two facts have several important caveats. First, the parameteri-
zation invariance only holds approximately in practice when non-infinitesimal step-sizes are used.
Second, Fisher efficiency is actually a weak property possessed by simpler methods like SGD with
1Roughly speaking, this means that it converges at the asymptotically optimal rate (with optimal constant)
for arbitrary statistical estimation procedures (as a function of the amount of samples from Q observed).
2
Published as a conference paper at ICLR 2018
Polyak/parameter averaging (Polyak & Juditsky, 1992), and even then will only be achieved when
the method converges to a global minimizer and the model is capable of perfectly capturing the true
distribution of y given x.
An alternative explanation for the empirical success of the natural gradient method is that it is a 2nd-
order method, whose update minimizes the following local quadratic approximation to the objective
h(θ + δ):
1 δ>Fδ + Vh(θ)>δ + h(θ).	(1)
This is similar to the 2nd-order Taylor series approximation of h(θ + δ), but with the Fisher substi-
tuted in for the Hessian. This substitution can be justified by the observation that the Fisher is a kind
of PSD approximation to the Hessian (Pascanu & Bengio, 2014; Martens, 2014). And as argued
by Martens (2014), while stochastic 2nd-order methods like natural gradient descent cannot beat
the asymptotically optimal Fisher efficient convergence achieved by SGD with Polyak averaging,
they can enjoy better pre-asymptotic convergence rates. Moreover, insofar as gradient noise can be
mitigated through the use of large mini-batches - so that stochastic optimization starts to resemble
deterministic optimization - the theoretical advantages of 2nd-order methods become further pro-
nounced, which agrees with the empirical observation that the use of large-minibatches speeds up
2nd-methods much more than 1st-order methods (Martens & Grosse, 2015; Ba et al., 2017).
In addition to providing an arguably better theoretical argument for the success of natural gradient
methods, their interpretation as 2nd-order methods also justifies the common practice of computing
the natural gradient as (F + λI)-1Vh instead of F-1Vh. In particular, this practice can be viewed
as a type of “update damping/regularization”, where one encourages δ to lie within some region
around δ = 0 where eqn. 1 remains a trustworthy approximation (e.g. Nocedal & Wright, 2006;
Martens & Sutskever, 2012).
2.3 Kronecker-factored approximate curvature (K-FAC)
Because modern neural network have millions (or even billions) of parameters it is computation-
ally too expensive to compute and invert the Fisher. To address this problem, the K-FAC method
of Martens & Grosse (2015) uses a block-diagonal approximation of the Fisher (where the blocks
correspond to entire layers/weight matrices), and where the blocks are further approximated as Kro-
necker products between much smaller matrices. The details of this approximation are given in the
brief derivation below.
Let W be a weight matrix in the network which computes the mapping
s = Wa,
where a and s are vector-valued inputs and outputs respectively and denote
g = Ds.
As in the original K-FAC paper we will assume that a includes a homogeneous coordinate with
value 1 so that the bias vector may be folded into the matrix W.
Here and throughout the rest of this document, F will refer to the block of the Fisher corresponding
to this particular weight-matrix W.
The Kronecker product of matrices B and C, denoted by B 0 C for matrices B ∈ Rm×n and C of
arbitrary dimensions, is a block matrix defined by
■ [B]1,1C	…	[B]ι,nC -
B 0 C ≡	.	..	.
.	..
[B]m,1C …[B]m,nC
Note that the Kronecker product has many convenient properties that we will make use of in this
paper. (See Van Loan (2000) for a good discussion of the Kronecker product and its properties.)
A simple application of the chain rule gives DW = ga>. Ifwe approximate g and a as statistically
independent, we can write F as
F = E[vec(DW)vec(DW)>] = E[vec(ga>) vec(ga>)>] = E[(a 0 g)(a 0 g)>]
= E[(aa>) 0 (gg>)] = E[aa>] 0 E[gg>] = A 0 G,
3
Published as a conference paper at ICLR 2018
where we have defined
A = E[aa>] and G = E[gg>].
The matrices A and G can be estimated using simple Monte Carlo methods, and averaged over lots
of data by taking an exponentially decaying average across mini-batches.
This is the basic Kronecker factored approximation (Heskes, 2000; Martens & Grosse, 2015; Povey
et al., 2015), which is related to the approximation made in the Natural Neural Nets approach (Des-
jardins et al., 2015). It is shown by Martens & Grosse (2015) that the approximation is equiva-
lent to neglecting the higher-order cumulants of the as and gs, or equivalently, assuming that they
are Gaussian distributed. To see why this approximation is useful, we observe that inversion and
multiplication of a vector by F amounts to inverting the factor matrices A and G and performing
matrix-matrix multiplications with them, due to the following two basic identities:
(B 乳 C)T = BT 乳 CT and	(B 乳 C)VeC(X) = vec(CXB>).	(2)
The required inversion and matrix multiplication operations are usually computational feasible be-
cause the factor matrices have dimensions equal to the size of the layers, which is typically just
a few thousand. And when they are not, additional approximations can be applied, such as ap-
proximate/iterative inversion (Povey et al., 2015), or additional Kronecker-factorization applied to
either A or G (Ba et al., 2017). Moreover, the computation of the inverses can be amortized across
iterations of the optimizer at the cost of introducing some staleness into the estimates.
3	APPROXIMATING F FOR RNNS
The basic Kronecker-factored approximation to the Fisher block F described in the previous section
assumed that the weight matrix W was used to compute a single mapping of the form s = Wa.
When W is used to compute multiple such mappings, as is often the case for RNNs, or a mapping
of a different flavor, as is the case for convolutional networks (CNNs), the approximation is not
applicable, strictly speaking.
Grosse & Martens (2016) recently showed that by making additional approximating assumptions,
the basic Kronecker-factored approximation can be extended to convolutional layers. This new
approximation, called “KFC”, is derived by assuming that gradient contributions coming from dif-
ferent spatial locations are uncorrelated, and that their intra and inter-location statistics are spatially
homogeneous, in the sense that they look the same from all reference locations. These assumptions
are referred to “spatially uncorrelated derivatives” and “spatial homogeneity,” respectively.
In this section we give the main technical contribution of this paper, which is a family of Kronecker-
based approximations of F that can be applied to RNNs. To build this we will apply various combi-
nations of the approximating assumptions used to derive the original K-FAC and KFC approaches,
along with several new ones, including an approximation which works by modelling the statistical
structure between the gradient contributions from time-steps using a chain-structured linear Gaus-
sian graphical model.
3.1	Preliminaries
Let W be some weight matrix which is used at T different time-steps (or positions) to compute the
mapping
st = Wat,
where t indexes the time-step. T is allowed to vary between different training cases.
Defining gt = Dst , the gradient of the single-case loss with respect to W can be written as
TT
DW =Xgtat> =XDtW,
t=1	t=1
where DtW = gtat> denotes the contribution to the gradient from time-step t. When it is more
convenient to work with the vector-representations of the matrix-valued variables Dt W we will use
the notation
wt = veC(DtW),
4
Published as a conference paper at ICLR 2018
so that vec(DW) = PtT=1 wt.
Let FT denote the conditional Fisher of DW for a particular value of T. We have
gr ∖∕T∖>] T T
Wt	XWt	T = XXe [wtw>∣τ].
t=1	t=1 s=1
(3)
Observe that F can be computed from FT via F = IET[FT].
To proceed with our goal of obtaining a tractable approximation to F we will make several approx-
imating assumptions, as discussed in the next section.
3.2	Basic initial approximations
3.2.1	INDEPENDENCE OFT
One simplifying approximation we will make immediately is that T is independent of the Wt ’s, so
that E[wtw7 |T] = E[wtw>]. In this case eqn. 3 can be written as
TT	TT
FT=XXE[wtw> ]= XXVt,s,	(4)
t=1 s=1	t=1 s=1
where we have defined %,§ = E[wt w>].
Independence of T and the Wt’s is a reasonable approximation assumption to make because 1) for
many datasets T is constant (which formally implies independence), and 2) even when T varies
substantially, shorter sequences will typically have similar statistical properties to longer ones (e.g.
short paragraphs of text versus longer paragraphs).
3.2.2	Temporal homogeneity
Another convenient and natural approximating assumption we will make is that the Wt’s are tempo-
rally homogeneous, which is to say that the statistical relationship between any Wt and Ws depends
only on their distance in time (d = t - s). This is analogous to the “spatial homogeneity” as-
sumption of KFC. Under this assumption the following single-subscript notation is well-defined:
Vt-s = !E[wtw>]. We note that V-d = V2.
Applying this notation to eqn. 4 we have
TT	T	T	T
FT=XXVt,s= X (T-|d|)Vd=X(T-d)Vd+X(T-d)Vd>-TI,	(5)
t=1 s=1	d=-T	d=0	d=0
where we have used the fact that there are T - |d| ways to write d as t - s for t, s ∈ {1, 2, . . . , T}.
Temporal homogeneity is a pretty mild approximation, and is analogous to the frequently used
“steady-state assumption” from dynamical systems. Essentially, itis the assumption that the Markov
chain defined by the system “mixes” and reaches its equilibrium distribution. If the system has any
randomness, and its external inputs reach steady-state, the steady-state assumption is quite accurate
for states sufficiently far from the beginning of the sequence (which will be most of them).
3.2.3	INDEPENDENCE BETWEEN THE at’S AND THE gt’S
If we have that at and gs are pair-wise independent for each t and s, which is the obvious gener-
alization of the basic approximation used to derive the K-FAC approach, then following a similar
derivation to the one from Section 2.3 we have
½,s = E[(ata>) 0 (gtg>)] = E[(ata>) 0 (DstDs>)] = At,s 0 Gt,s,
where we have defined
At,s = IEiata>] and	Gt,s = W[gtg>].
5
Published as a conference paper at ICLR 2018
Extending our temporal homogeneity assumption from the wt’s to the at’s and gt’s (which is natural
to do since wt = vec(gtat>)), the following notation becomes well-defined:
At-s = At,s and Gt-s = Gt,s,
which allows us to write
Vd = Ad 0 Gd.
3.3	An initial attempt to obtain a tractable Fisher approximation
Given the approximating assumptions made in the previous subsections we have
TT
FT = X (T - |d|)Vd = X (T - |d|)(Ad 0 Gd).
d=-T	d=-T
Assuming for the moment that all of the training sequences have the same length, so that F = FT0
for some T0, we have that F will be the sum of 2T0 + 1 Kronecker products.
Without assuming any additional structure, such as a relationship between the various Ad ’s or Gd’s,
there doesn’t appear to be any efficient way to invert such a sum. One can use the elementary identity
(B0C)-1 = B-10C-1 to invert a single Kronecker product, and there exists decomposition-based
methods to efficiently invert sums of two Kronecker products (see Martens & Grosse (2015)), how-
ever there is no known efficient algorithm for inverting sums of three or more Kronecker products.
Thus is appears that we must make additional approximating assumptions in order to proceed.
3.4	ASSUMING INDEPENDENCE OF THE wt’S ACROSS TIME
If we assume that the contributions to the gradient (the wt ’s) are independent across time, or at least
uncorrelated, this means that Vd = 0 for d 6= 0. This is analogous to the “spatially uncorrelated
derivatives” assumption of KFC.
In this case eqn. 5 simplifies to
T
FT = X (T - |d|)Vd = (T - 0)V0 =TV0,
d=-T
so that
F = IET [FT ] = WT [T V0] = IET [T] V0.
Using the identities in eqn. 2, and the symmetry of A0 and G0, we can thus efficiently multiply F-1
by a vector z = vec(Z) using the formula
FTz = w1^vec(G-1ZA-1).	(6)
IKT[/ ]
This is, UP to normalization by IET[T], identical to the inverse multiplication formula used in the
original K-FAC approximation for fully-connected layers.
We note that IET [T] = Pi ωiTi, where T are the different values of T, and ωi > 0 are normalized
weights (with Pi ωi = 1) that measure their proportions in the training set.
3.5	MODELING THE RELATIONSHIPS BETWEEN THE wt’S USING AN LGGM
As we saw in Section 3.3, the approximation assumptions made in Section 3.2 (independence ofT,
temporal homogeneity, and independence between the at’s and the gt’s), aren’t sufficient to yield a
tractable formula for F-1. And while additionally assuming independence across time of the wt’s
is sufficient (as shown in Section 3.4), it seems like an overly severe approximation to make.
In this section we consider a less severe approximation which we will show still produces a tractable
F-1. In particular, we will assume that the statistical relationship of the wt’s is described by a
simple linear Gaussian graphical model (LGGM) with a compact parameterization (whose size is
independent of T). Such an approach to computing a tractable Fisher approximations was first
6
Published as a conference paper at ICLR 2018
explored by Grosse & Salakhutdinov (2015) for RBMs, although our use of it here is substantially
different, and requires additional mathematical machinery.
The model we will use is a fairly natural one. It is a linear Gaussian graphical model with a one-
dimensional chain structure corresponding to time. The graphical structure of our model is given by
the following picture:
Variables in the model evolve forward in time according to the following equation:
wt = Ψwt-1 + t
where Ψ is a square matrix and t are i.i.d. from N (0, Σ) for some positive definite matrix Σ (which
is the conditional covariance of wt given wt-1).
Due to the well-known equivalence between directed and undirected Gaussian graphical models for
tree-structured graphs like this one, the decision of whether to make the edges directed or undirected,
and whether to have them point forwards or backwards in time, are irrelevant from a modeling
perspective (and thus to the Fisher approximation we eventually compute). We will use a directed
representation purely for mathematical convenience.
We will assume that our model extends infinitely in both directions, with indices in the range
(-∞, ∞), so that the wt’s are all in their stationary distribution (with respect to time). For this
to yield a well-defined model we require that Ψ has spectral radius < 1.
The intuition behind this model structure is clear. The correlations between gradient contributions
(the wt ’s) at two different time-steps should be reasonably well explained by the gradient contribu-
tions made at time-steps between them. In other words, they should be approximately Markovian.
We know that the gradient computations are generated by a process, Back-prop Through Time
(BPTT), where information flows only between consecutive time-steps (forwards through time dur-
ing the “forward pass”, and backwards during the “backwards pass”). This process involves temporal
quantities which are external to the wt ’s, such as the inputs x and activations for other layers, which
essentially act as “hidden variables”. The evolution of these external quantities may be described by
their own separate temporal dynamics (e.g. the unknown process which generates the true x’s), and
thus the wt’s won’t be Markovian in general. But insofar as the wt’s (or equivalently the at’s and
gt’s) encode the relevant information contained in these external variables, they should be approxi-
mately Markovian. (If they contained all of the information they would be exactly Markovian.)
A similar approximation across consecutive layers was made in the “block-tridiagonal” version of
the original K-FAC approach. It was shown by Martens & Grosse (2015) that this approximation was
a pretty reasonable one. The linear-Gaussian assumption meanwhile is a more severe one to make,
but it seems necessary for there to be any hope that the required expectations remain tractable.
3.5.1	Initial computations
Define the following “transformed” versions of FT and Ψ:
FT = V'1r2Fτ VI1/2 and Ψ = VI = VlT〃ΨV010
As shown in Section A.1 of the appendix we have
FT = XX(T- d)Ψd + (X(T- d)Ψd! -TI
d=0	d=0
=ZT (Ψ) + ZT (Ψ >)-TI	⑺
7
Published as a conference paper at ICLR 2018
where
T(1 - x) - x(1 - xT )
ZT (x) =-------(1-^p----------.
(Note that rational functions can be evaluated with matrix arguments in this way, as discussed in
Section A.1.)
Our goal is to compute F-1, from which We can recover FT via the simple relation FT =
V0-1/2Fb-1V0-1/2.
Unfortunately it doesn’t appear to be possible to simplify this formula sufficiently enough to allow
ʌ ʌ ʌ
for the efficient computation of FT = IET[FT]-1 when Ψ is a Kronecker product (which it will be
when V0 and Vi are). The difficulty is due to both the appearance of Ψ and its transpose (which are
not COdiagonalizable/commutative in general), and various higher powers of Ψ.
To proceed from this point and obtain a formula which can be efficiently evaluated when Ψ is a
Kronecker product, we will make one of two simplifying assumptions/approximations, which we
call “Option 1” and “Option 2” respectively. These are explained in the next two subsections.
3.5.2	OPTION 1: V1 IS SYMMETRIC
If Vi (the cross-moment over time) is symmetric, this implies that Ψ =忆=V-1/2V1V-1/2 is
also symmetric. Thus by eqn. 7 we have
ʌ ʌ ʌ ʌ
ft = ZT (W) + ZT (W)-Ti = ητ (W),
where
ηT (x) = 2ZT (x) - T
2(T (1 — x) — x(1 — xτ))
(1 — x)2
T (1 — x2) — 2x(1 — xτ)
(1 — x)2
Let U diag(ψ)U > = W be the eigen-decomposition of W. By the above expression for FT we have
that FT = U diag(ηT(ψ))U>, where f(b) denotes the component-wise evaluation of a function f
for each component of the vector b, i.e. [f (b)]i = f ([b]i). We thus have
F = IET [Ft ] = IET [U diag(ητ (ψ))U >] = U diag(!E T [ητ (ψ)])U >.
Inverting both sides of this yields
FT = U diag(γ(ψ))U >	(8)
where we have defined γ(x) = 1/!ET[ητ(x)].
This expression can be efficiently evaluated when Ψ is a Kronecker product since the eigendecom-
position of a Kronecker product can be easily obtained from the eigendecomposition of the factors.
ʌ ʌ ʌ
Evaluation of γ(ψ ) is done component-wise (i.e. [γ(ψ )]i = γ([ψ]i)) and is thus easy to perform.
See Section 3.5.5 for further details.
Vi is symmetric if and only if Ψ is symmetric. And as shown in the proof of Proposition 1 (see
Appendix A.1) Ψ has the interpretation of being the transition matrix of an LGGM which describes
the evolution of “whitened” versions of the wt's (given by Wt = Vr-1/2wt). Linear dynamical
systems with symmetric transition matrices arise frequently in machine learning and related areas
(Huang et al., 2016; Hazan et al., 2017), particularly because of the algorithmic techniques they
enable. Intuitively, a symmetric transition matrix allows allows one to model exponential decay
of different basis components of the signal over time, but not rotations between these components
(which are required to model sinusoidal/oscillating signals).
Note that the observed/measured Vi may or may not be exactly symmetric up to numerical precision,
even if it well approximated as symmetric. For these calculations to make sense it must be exactly
symmetric, and so even if it turns out to be approximately symmetric one should ensure that it is
exactly so by using the symmetrized version (Vi + Vi> )/2.
8
Published as a conference paper at ICLR 2018
3.5.3	Option 2: Computing the limiting value instead
If V1 is not well approximated as symmetric, another option is to approximate
F = IET [F∞)]
ʌ ʌ
(instead of F = IET [FT]), where We define
FT∞)≡ T0i→∞ ToFT 0 ∙
This is essentially equivalent to the assumption that the training sequences are all infinitely long,
which may be a reasonable one to make in practice. We re-scale by the factor T to achieve the
ʌ
proper scaling characteristics of FT, and to ensure that the limit actually exists.
As shown in Section A.2 of the appendix this yields the following remarkably simple expression for
F-1:
F1-1 = ≡Λπ(I - Ψ)(I- Ψ>Ψ)-1(I - Ψ>)∙	(9)
TT[/]
Despite the fact that it includes both Ψ and Ψ>, this formula can be efficiently evaluated when Ψ is
a Kronecker product due to the existance of decomposition-based techniques for inverting matrices
of the form A 0 B + C 0 D. See Section 3.5.5 for further details.
This approximation can break down if some of the linear components of Wt have temporal autocor-
relations close to 1 (i.e. [ψ]i ≈ 1 for some i) and T is relatively small. In such a case we will have
that [ψ5]T is large for some i (despite being raised to the T-th power) so that FT∞ may essentially
“overcount” the amount of temporal correlation that contributes to the sum.
This can be made more concrete by noting that the approximation is essentially equivalent to taking
ZT(x) ≈ IimTo→∞ TZT0 (x) ≡ κ(x) for each X = [ψ]i. We can express the error of this as
IK(X) — ZT(X)I
T T(1 - x) - x(1 - xT )
--------------；-------7-----
1 — X	(1 — x)2
X(1 - XT)
(1 — x)2
ʌ ʌ
It is easy to see how this expression, when evaluated at X = [ψ]i, might be large when [ψ ]i is close
to 1, and T is relatively small.
3.5.4	Estimating Ψ
The formulae for F-1 from the previous sections depend on the quantity Ψ =忆 = V0^^ 1∕2ΨV01/2,
and so it remains to compute Ψ. We observe that
Vi = Vι,0 = E[wιw>] = E[(Ψw0 + s)w>] = ΨIE[w0w>] + IEkIw>] = ΨV0 + 0 = ΨV0∙
Right-multiplying both sides by V0 yields Ψ = V1V0-1. Thus, given estimates ofV0 and V1, we may
compute an estimate of Ψ as
Ψ = V-1/2VIV)T/2∙
in practice we estimate V0 and V1 by forming estimates of their Kronecker factors and taking the
product. The factors themselves are estimated using exponentially decayed averages over mini-batch
estimates. And the mini-batch estimates are in turn computed by averaging over cases and summing
across time-steps, before divide by the expected number of time-steps.
For example, for A)and A1 these the mini-batch estimates are averages of IEJT] PT=I ata> and
IE,S PT-LI at+1a>, respectively. Note that as long as V)is computed as the 2nd-order moment
of some empirical data, and V1 computed as the 2nd-order moment between that same data and a
temporally shifted version, the spectral radius of Ψ = V0-1/2V1V0-1/2 (and similarly Ψ = V1V0-1)
will indeed be less than or equal to 1, as we prove in Section B.2 of the appendix. This bound on the
9
Published as a conference paper at ICLR 2018
spectral radius is a necessary condition for our infinite chain-structured Gaussian graphical model
to be well-defined, and for our calculations to make sense.
The sufficient condition that the spectral radius is actually less than 1 will most often be satisfied too,
except in the unlikely event that some eigen component remains perfectly constant across time. But
even if this somehow happens, the inclusion within the given V0 of some damping/regularization
term such as λI will naturally deal with this problem.
3.5.5	EFFICIENT IMPLEMENTATION ASSUMING V0 AND V1 ARE KRONECKER-FACTORED
It remains to show that the approximations developed in Section 3.5 can be combined with the
Kronecker-factored approximations for V0 and V1 from Section 3.2.3 to yield an efficient algorithm
for computing F -1z for an arbitrary vector z = vec(Z). This is a straightforward although very
long computation which we leave to Section C of the appendix.
Full pseudo-code for the resulting algorithms is given in Section C.3. As they only involve sym-
metric eigen-decomposition and matrix-matrix products with matrices the size of A0 and G0 they
are only several times more expensive to compute than eqn. 6. This extra overhead will often be
negligible since the gradient computation via BPTT, whose costs scales with the sequence length T,
tends to dominate all the other costs.
4	Experiments
To demonstrate the benefit of our novel curvature matrix approximations for RNNs, we empirically
evaluated them within the standard “distributed K-FAC” framework (Ba et al., 2017) on two different
RNN training tasks.
The 2nd-order statistics (i.e. the Kronecker factors A0, A1, G0, and G1) are accumulated through
an exponential moving average during training. When computing our approximate inverse Fisher,
factored Tikhonov damping (Martens & Grosse, 2015) was applied to V0 = Go 0 A0.
We used a single machine with 16 CPU cores and a Nvidia K40 GPU for all the experiments. The
additional computations required to get the approximate Fisher inverse from these statistics (i.e. the
“pre-processing steps” described in Section C.3) are performed asynchronously on the CPUs, while
the GPU is used for the usual forward evaluation and back-propagation to compute the gradient.
Updates are computed using the most recently computed values of these (which are allowed to be
stale), so there is minimal per-iteration computational overhead compared SGD.
We adopted the step-size selection technique described in Section 5 of Ba et al. (2017), as we found
it let us use larger learning rates without compromising the stability of the optimization. The hyper-
parameters of our approach, which include the max learning rate and trust-region size for the afore-
mentioned step-size selection procedure, as well as the momentum, damping constants, and the
decay-rate for the second-order statistics, as well as the hyper-parameters of the baseline methods,
were tuned using a grid search.
Word-level language model: We start by applying our method to a two-layer RNN based on the
well-studied Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997)
for a word-level language modeling task on the Penn-TreeBank (PTB) dataset (Marcus et al., 1993)
following the experimental setup in Zaremba et al. (2014). The gradients are computed using a
fixed sequence length truncated back-propagation scheme in which the initial states of the recurrent
hidden units are inherited from the final state of the preceding sequence. The truncation length used
in the experiments is 35 timesteps. The learning rate is given by a carefully tuned decaying schedule
(whose base value we tune along with the other hyperparamters).
In our experiments we simply substitute their optimizer with our modified distributed K-FAC op-
timizer that uses our proposed RNN Fisher approximations. We performed experiments on two
different sizes of the same architecture, which use two-layer 650 and 1024 LSTM units respectively.
LSTMs have 4 groups of internal units: input gates, output gates, forget gates, and update candidates.
We treat the 4 weight matrices that compute the pre-activations to each of these as distinct for the
purposes of defining Fisher blocks (whereas many LSTM implementations treat them as one big
matrix). This results in smaller Kronecker factors that are cheaper to compute and invert.
10
Published as a conference paper at ICLR 2018
Updates
Seconds
Adam
Adam÷ LayerNorm
K-FAC indep.
K-FAC optionl
K-FAC option2
SGD
Figure 1: Optimization performance of our method compared to the baselines in perplexity-per-
word on length-35 word sequences from Penn-TreeBank. All the methods used a mini-batch size
of 200. K-FAC indep. uses the update in eqn. 6, K-FAC option1 uses eqn. 8, and K-FAC option2
uses eqn. 9. (left) Training perplexity v.s. the number of updates. Dashed lines denote the training
curves for RNNs with 1024 LSTM units and solid lines denote the training curves for RNNs with
650 LSTM units. (right) Training perplexity v.s. the wall-clock time.
4.1	Language Modeling with Long Short-Term Memory units
Because the typical vocabulary size used for PTB is 10,000, the Fisher blocks for the input embed-
ding layer and output layer (computing the logits to the softmax) each contain a 10,000 by 10,000
sizes Kronecker factor, which is too large to be inverted with any reasonable frequency. Given that
the input vector uses a one-hot encoding it is easy to see its associated factor is actually diagonal,
and so we can store and invert it as such. Meanwhile the large factor associated with the output isn’t
diagonal, but we nonetheless approximate it as such for the sake of efficiency.
In our experiments we found that each parameter update of our method required about 80% more
wall-clock time than an SGD update (using mini-batch size of 200) although the updates made more
much progress.
In Figure 1, we plot the training progress as a function of the number of parameter updates. While
Adam outperforms SGD in the first few epochs, SGD obtains a lower loss at the end of training. We
found the recent layer-normalization technique (Ba et al., 2016) helps speed up Adam considerably,
but it hurts the SGD performance. Such an observation is consistent with previous findings. In com-
parison, our proposed method still significantly outperform both the Adam and the SGD baselines
even with the help of layer-normalization.
While optimization performance, not generalization performance, is the focus of this paper, we have
included validation performance data in the appendix for the sake of completeness. (See Figure 4 in
Appendix D.) Not surprisingly, we found that the 2nd-order methods, including our approach and
diagonal ones like Adam, tended to overfit more than SGD on these tasks.
The tendency for SGD w/ early-stopping to self-regularize is well-documented, and there are many
compelling theories about why this happens (e.g. Duvenaud et al., 2016; Hardt et al., 2015). Itis also
well-known that 2nd-order methods, including K-FAC and diagonal methods like Adam/RMSprop,
dont self-regularize nearly as much (e.g. Wilson et al., 2017; Keskar & Socher, 2017). We feel that
this problem can likely be addressed through the careful application of additional explicit regular-
ization (e.g. increased weight decay, drop-out, etc) and/or model modifications, but that exploring
this is outside of the scope of this paper.
Character-level model: To further investigate the optimization performance of our proposed
Fisher approximation, we use a small two layer LSTM with 128 units to model the character se-
quences on the Penn-TreeBank (PTB) dataset (Marcus et al., 1993). We employ the same data parti-
tion in Mikolov et al. (2012). We plotted the bits-per-character vs the number of parameter updates
and the wall-clock times in Figure 2. The K-FAC updates were roughly twice as time-consuming
to compute as the Adam updates in our implementation. Despite this, our results demonstrate that
K-FAC has a significant advantage over the Adam baseline in terms of wall-clock time.
11
Published as a conference paper at ICLR 2018
6u-u-eH
Adam batchsize 200
—— Adam batchsize 50
—K-FAC indep.
—K-FAC option 1
——K-FAC option2
■ 0	1000	2000	3000	4000	5000	, 0	200	400	600	800	1000
Updates	Seconds
Figure 2: Optimization performance in bit-per-character on length-100 character sequences from
Penn-TreeBank. batchsize indicates the mini-batch size used to train the baseline methods (our
method always used a mini-batch size of 200). K-FAC indep. uses the update in eqn. 6, K-FAC
option1 uses eqn. 8, and K-FAC option2 uses eqn. 9. (left) Training perplexity v.s. the number of
updates. (right) Training perplexity v.s. the wall-clock time.
Figure 3: Optimization performance for differentiable Neural Computers (DNC) on a repeated copy
task. K-FAC indep. uses the update in eqn. 6, K-FAC option1 uses eqn. 8, and K-FAC option2 uses
eqn. 9. (left) Training cross entropy loss v.s. the number of updates. (right) Training cross entropy
loss v.s. the wall-clock time.
4.2	Learning differentiable Neural Computers
To further investigate the potential benefits of using our approach over existing methods, we applied
it to the Differentiable Neural Computer (DNC) model (Graves et al., 2016) for learning simple
algorithmic programs. Recently, there have been several attempts (Weston et al., 2014; Graves et al.,
2016) to extend the existing RNN models to incorporate more long-term memory storage devices
in order to help solve problems beyond simple sequence prediction tasks. Although these extended
RNNs could potentially be more powerful than simple LSTMs, they often require thousands of
parameter updates to learn simple copy tasks (Graves et al., 2016). Both the complexity of these
models and the difficulty of the learning tasks have posed a significant challenge to commonly used
optimization methods.
The DNC model is designed to solve structured algorithmic tasks by using an LSTM to control an
external read-write memory. We applied the Fisher-based precondition to compute the updates for
both the weights in the LSTM controller and the read-write weight matrices used to interface with the
memory. We trained the model on a simple repeated copy task in which the DNC needs to recreate
a series of two random binary sequences after they are presented as inputs. The total length of the
sequence is fixed to 22 time-steps. From Figure 3, we see that our method significantly outperforms
the Adam baseline in terms of update count, although only provides a modest improvement in wall-
clock time.
This gap is explained by the fact that the iterations were significantly more time-consuming to com-
pute relative to the gradient computations than they were in previous two experiments on language
models. This is likely due to a different trade-off in terms of the gradient computation vs the over-
heads specific to our method owing to smallness of the model and dataset. With more careful
engineering to reduce the communication costs, and/or a larger model and dataset, we would expect
to see a bigger improvement in wall-clock time.
12
Published as a conference paper at ICLR 2018
5	Conclusion
We have presented a new family of approximations to the Fisher information matrix of recurrent
neural networks (RNNs), extending previous work on Kronecker-factored approximations. With this
contribution, recurrent networks can now finally be trained with the K-FAC optimization method.
We have demonstrated that our new approximations substantially reduce the required number of
iterations for convergence vs standard baseline optimizers on several realistic tasks. And we have
also shown that in a modern distributed training setup this results in a substantial savings in wall-
clock time as well.
References
ShUn-Ichi Amari. Natural gradient works efficiently in learning. Neural COmputatiOn, 10(2):251-
276, 1998.
Jimmy Ba and Diederik Kingma. Adam: A method for stochastic optimization. In ICLR, 2015.
Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using
kronecker-factored approximations. In InternatiOnal COnference On Learning RepresentatiOns
(ICLR’2017), 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Guillaume Desjardins, Razvan Pascanu, Aaron Courville, and Yoshua Bengio. Metric-free natu-
ral gradient for joint-training of boltzmann machines. In InternatiOnal COnference On Learning
RepresentatiOns (ICLR’2013), 2013.
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, and Koray Kavukcuoglu. Natural neural
networks. In Advances in Neural InfOrmatiOn PrOcessing Systems, pp. 2071-2079, 2015.
David Duvenaud, Dougal Maclaurin, and Ryan Adams. Early stopping as nonparametric variational
inference. In Artificial Intelligence and Statistics, pp. 1070-1077, 2016.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471-476, 2016.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In PrOceedings Of the 33rd InternatiOnal COnference On Machine Learning (ICML-16),
2016.
Roger Grosse and Ruslan Salakhutdinov. Scaling up natural gradient by factorizing fisher informa-
tion. In PrOceedings Of the 32nd InternatiOnal COnference On Machine Learning (ICML), 2015.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Elad Hazan, Karan Singh, and Cyril Zhang. Learning linear dynamical systems via spectral filtering.
In Advances in Neural InfOrmatiOn PrOcessing Systems, pp. 6705-6715, 2017.
Tom Heskes. On “natural” learning and pruning in multilayered perceptrons. Neural COmputatiOn,
12(4):881-901, 2000.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural COmputatiOn, 9(8):1735-1780,
1997.
Wenbing Huang, Fuchun Sun, Lele Cao, Deli Zhao, Huaping Liu, and Mehrtash Harandi. Sparse
coding and dictionary learning with linear dynamical systems. In PrOceedings Of the IEEE COn-
ference On COmputer VisiOn and Pattern RecOgnitiOn, pp. 3938-3947, 2016.
13
Published as a conference paper at ICLR 2018
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by re-
ducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine
Learning,pp. 448-456, 2015.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
adam to sgd. arXiv preprint arXiv:1712.07628, 2017.
Nicolas Le Roux, Pierre-antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in Neural Information Processing Systems 20, pp. 849-856. MIT
Press, 2008.
Gaetan Marceau-Caron and Yann Ollivier. Practical riemannian neural networks. 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313-330, 1993.
J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International
Conference on Machine Learning (ICML), 2010.
J. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In
Proceedings of the 28th International Conference on Machine Learning (ICML), pp. 1033-1040,
2011.
J. Martens and I. Sutskever. Training deep and recurrent networks with Hessian-free optimization.
In Neural Networks: Tricks of the Trade, pp. 479-535. Springer, 2012.
James Martens. New insights and perspectives on the natural gradient method. 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15),
pp. 2408-2417, 2015.
Tomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan Kombrink. SUbWord lan-
guage modeling with neural networks. 2012.
Jorge Nocedal and Stephen J. Wright. Numerical optimization. Springer, 2. ed. edition, 2006.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International
Conference on Learning Representations (ICLR), 2014.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM J.
Control Optim., 30(4), July 1992.
Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of DNNs with natural
gradient and parameter averaging. In International Conference on Learning Representations:
Workshop track, 2015.
Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin, and Bhuvana Ramabhadran.
Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and
sampling. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),
2013, pp. 303-308, 2013.
Nicol N. Schraudolph. Centering neural network gradient factors. In Genevieve B. Orr and Klaus-
Robert Muller (eds.), Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in
Computer Science, pp. 207-226. Springer Verlag, Berlin, 1998.
T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Charles F Van Loan. The ubiquitous kronecker product. Journal of computational and applied
mathematics, 123(1):85-100, 2000.
Tommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient towards
second-order methods - backpropagation learning with transformations in nonlinearities. 2013.
14
Published as a conference paper at ICLR 2018
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint
arXiv:1410.3916, 2014.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht.
The marginal value of adaptive gradient methods in machine learning. arXiv preprint
arXiv:1705.08292, 2017.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329, 2014.
Fuzhen Zhang. The Schur complement and its applications, volume 4. Springer Science & Business
Media, 2006.
15
Published as a conference paper at ICLR 2018
A Supplementary computations
A.1 Proofs for section 3.5.1
Proposition 1 Given
FT = V01/2FT V01/2 and Ψ = V1 =监1/2田弘1/2.
we have
FT = ZT(W) + ZT(WT)- TI
where
T(1 - x) - x(1 - xT)
ZT (X) =------(1-^p---------.
Proof
For d > 0 we have that Vd = WVd-1 , which can be seen as follows:
Vd = Vd,0 = E[wdw>] = E[Φwd-1 + 6dw>]
= ΨE[wd-iw>] + IEkdw>]
= WVd-1 + 0 = WVd-1 .
Applying Vd = WVd-1 recursively yields
Vd = WdV0 for d > 0.
And using V-d = Vd> it also follows that
Vd=V0(Wd)> for d60.
Setting d = 1 and multiplying both sides by V0 (which is assumed to be invertible) one can also
derive the following simple formula for W:
W = V1V0-1.	(10)
To proceed from here we define a “transformed” version of the original chain-structured linear-
Gaussian graphical model whose variables are wt = V0-1/2wt. (Here we assume that V0 is invertible
-it is symmetric by definition.) All quantities related to the original model have their analogues in
the transformed model, which We indicate with the hat symbol ∙.
In the transformed model the 2nd-order moments of the wt'sare given by
Vd = E ](v0T∕2wd) (V0T/2w0)> = V0T/2]E[wdw>] V0T∕2 = V0T∕2VdV0—1/2.
We observe that Vθ = I.
Analogously to the original model, the transformed version obeys
ʌ
wt = Ψ wt-1 + ^t,
-1/2	1
with ^t = V0and Ψ = ½V0 1 = V1 (using V0 = I). This can be seen by noting that
wt = V0—1/2wt
= V0-1/2 (Wwt-1 + t)
= V0-1/2V1V0-1wt-1 +V0-1/2t
=(叫T∕2V1 跖”)(叫T∕2wt-1) + ^t
=VIwt-I + ^t∙
It also remains true that the spectral radius of Ψ is less than 1, which can be seen in at least one of
two ways: by noticing that the transformed model is well-defined in the infinite limit if and only if
16
Published as a conference paper at ICLR 2018
the original one is, or that Ψ = VI = V0T∕2ΨV01/2 is a similar matrix to Ψ (in the technical sense)
and hence has the same eigenvalues.
As the transformed model is isomorphic to the original one, all of the previously derived relation-
ships which held for it also hold here, simply by replacing each quantity with its transformed version
(denoted by the hat symbol ∙).
Given these relations (included the transformed analogue of equation 5) We can express FT as
T
FT = X(T- d)Ψd +
d=0
-d)Ψd ) - TI.
It is a Well-knoWn fact that one can evaluate rational functions, and functions that are the limit-
ing values of sequences of rational functions, With matrix arguments. This is done by replacing
scalar multiplication With matrix multiplication, division With matrix inversion, and scalar constants
With scalar multiples of the identity matrix, etc. Note that because sums of poWers and inverses of
matrices are co-diagonalizable/commutative When the matrices themselves are, there is no issue of
ambiguity caused by mixing commutative and non-commutative algebra in this Way.
Moreover, the value of some such function f (x), given a matrix argument B, is
f(B) = V diag(f(b))V -1,
Where V diag(b)V -1 = B is eigendecomposition of B, and Where f(b) denotes the component-
Wise evaluation of f for each component of the vector b, i.e. [f (b)]i = f ([b]i). Note that if [f (b)]i is
undefined from some i, either because ofa division by zero, or because the limit Which defines f(x)
doesn’t converge for x = [b]i, then f(B) doesn’t exist for that particular B (and otherWise it does).
We observe that our
above expression for FbT
can be reWritten as
FT = ZT(W) + ZT(WT)- TI,
Where ZT (x) = PdT=0(T - d)xd. By Proposition 3 in Appendix B.1, We have for x 6= 1 that
ZT (x)
T (1 — x) — x(1 — XT)
(1 — x)2
Let U diag(ψ)U -1 = W be the eigendecomposition of W. Because W has a spectral radius less than
1, we have | [ψ]i ] < 1 for each i (so that in particular [ψ]i = 1), and thus we can evaluate ZT(Ψ) and
Zt(Ψ>) according to the above formula for ZT(x).
A.2 Proofs for section 3.5.3
Proposition 2 Suppose we approximate F = IET[FT∞)], where we have defined
F∞) ≡ lim TFTo.
T	T0 →m∞ T0 T .
Then we have
FT = =^π(I - Ψ)(I - Ψ>Ψ)-1(I - Ψ>).
此T[/]
Proof
From eqn. 7 we have that
TTTT
F∞)
lim
T0 →∞
(ZT 0 (Ψ) + ZT 0 (Ψ >)-T0I)
lim
T0 →∞
ZT0(ψ)+ lim XZT0(山T) - lim XTL
T0 →∞ T0	T0 →∞ T0
17
Published as a conference paper at ICLR 2018
To evaluate this we first term note that
Iim TT0T0I = TI and	Iim ɪoCTO(A) = K(A),
T0 →∞ T0	T0 →∞ T0
where we have defined
K(X) = Tl0im∞ 下 CT 0(X)∙
For |X| < 1 we have that limT0→∞ XT0 = 0, from which it follows that
K(X)
ιim Tr(I- XiI-XT 0)
T0→∞ T0 1	(1 — x)2
T T0
lim 〜
T0→∞ T0 1 -
T X(1 - X
lim 一 /_	、
T0→∞ T0 (1 - X)
—
X
2
lim TT-
T0→∞ 1 - X
—
lim T lim x(1-XTj
T0→∞ T0 T0→∞ (1 - X)2
T	T X(1 — 0)
1-X - Tl→∞∞ T0 (1 — X)2
T
.
1-X
ʌ ʌ ʌ ʌ
Let U diag(ψ)UT = Ψ be the eigendeComPosition of Ψ. Using the fact that ∣[ψ]/ < 1 (as estab-
lished in Section A.1) we can use the above expression to evaluate K(X) at both X = Ψ and X = Ψ>,
which yields
FF = κ(Ψ) + κ(Ψ >) = T ((I — Ψ)-1 + (I — Ψ >)-1 — I).
Pre-multiplying both sides by I — Ψ>, and post-multiplying both sides by I — Ψ, We have
(I — Ψ >)FT∞)(I — Ψ) = T ((I — Ψ >) + (I — Ψ) — (I — Ψ >)(I — Ψ))
=T (I — Ψ > + I — Ψ — I + Ψ > + Ψ  Ψ >Ψ)
=T (I — Ψ >Ψ).
Then applying the reverse operation gives
FTO) = T (I — Ψ >)-1(I — Ψ > Ψ)(I — Ψ)-1.
Taking the expectation over T gives
F = IET [FT∞)] = IET [T](I — Ψ >)-1(I — Ψ >Ψ)(I — Ψ)-1.
Finally, inverting both sides yields
FT = ir⅛π(I — Ψ)(I — Ψ >Ψ)-1(I — Ψ >).
IKT[/ ] B
B Additional technical proofs
B.1
Proposition 3 Suppose X ∈ (C, x = 0, and T is a non-negative integer. We have
T
X(T — i)Xi
i=0
T (1 — x) — x(1 — xt )
(1 — x)2
18
Published as a conference paper at ICLR 2018
Proof Observe that
X d⅛i
TT
x X ixi-1 = X ixi.
Another way to express this is to use the geometric series formula PT=0 Xi = 1--^+1 (which holds
for x 6= 0) before computing the derivative. This gives
X d P= Xi = dx Thus we have	d(1 1—x-)	( (1 + T )xt	1 — X 丁 +1 A	x(1 — X 丁 +1 — (1 + T )(1 — X)XT) X	dX	"(	1 — X + (1 — x)2 )	(1 — x)2 T i X(1 - XT+1 - (1 + T)(1 - X)XT ) TiX =	1X2	. i=0
And so T X(T - i)Xi i=0	TT = TXXi—XiXi T(1 —XT+1)	X(1 — XT +1 — (1 +T)(1 — X)XT ) =		-	；		7	 1 — X	(1 — x)2 T (1 — x)(1 — xt +1) — x(1 — xt +1 — (1 + T )(1 — X)XT) (1 — x)2 T-Tx — Txt +1 + TXT+2 — x + XT+2 + XT +1(1 — x + T — Tx) (1 — x)2 _ T — TX — TXτ +1 + TXt +2 — X + Xt +2 + Xt +1 — Xt +2 + TXt +1 — TXt +2 (1 — x)2 _ T — (T +1)x + Xt +1 (1 — x)2 T (1 — x) — x(1 — xt ) (1 — x)2
where We have again used the geometric series formula PT=0 Xi = 1-1—+1 on the second line. □
B.2 Spectral bound for estimate of Ψ
In what follows all quantities are computed using their defining formulae, starting from the estimated
values ofA0, A1, G0, and G1.
First We observe that since Ψ =忆=V0^1∕2ΨV011/2 is similar to Ψ (in the technical sense of the
word), they share the same eigenvalues. Thus it suffices bound to the spectral radius of Ψ = V1V0-1.
Next We observe that Vo = A0 0 Go and V1 = A1 0 G1, so that
V1V0^1 = (A1 0 G1)(Ao 0 Go)- = (A1A-1) 0 (G1G-1).
Because the eigendecomposition of a Kronecker product is the Kronecker product of the decom-
positions of the factors we have that ρ(V1V0-1) = ρ(A1A0-1)ρ(G1G0-1), where ρ(X) denotes the
spectral radius of a matrix X .
Thus it suffices to show that ρ(A1A0-1) ≤ 1 and ρ(G1G0-1) ≤ 1 for Ai and Gi as computed by the
estimation scheme outlined in Section 3.5.4. Recall that this is the exponentially decayed average of
mini-batch averages of estimators of the form IEjPT= 1 a⅛a> and IEjPT-LI at+1a>.
In the remainder of this section we will show that ρ(A1A0-1) ≤ 1. The argument to show that
ρ(G1 G0-1 ) ≤ 1 is identical.
19
Published as a conference paper at ICLR 2018
Define
Mo = [0 aι a? .…aτ ]
and
Mi = [aι a? .…	aτ 0]
We observe that M0 M0> = M1 M1> = PtT=1 atat> and M1M0> = PtT=-11 at+1at>.
Provided that the exponentially decayed averages for A0 and A1 are computed in the same way and
use the same normalizers (i.e. 1 /(m⅛T[T])) We thus have that the matrix
A0 A1
A = A1> A0
is a positively-Weighted linear combination of terms of the form
M1
M0
Where the various at ’s are computed on different data using current and previous model parameters.
It thus folloWs that A	0. The inequality ρ(A1A0-1) ≤ 1 noW folloWs from the folloWing lemma.
M1
M0
0
Lemma 1 Consider a real, symmetric, positive semi-definite block matrix
B
CT
C
B
0.
(11)
If B is invertible then we have ρ(CB-1) ≤ 1, and further if the block matrix is positive definite we
have ρ(CB-1) < 1.
Proof Because similarity transformations preserve eigenvalues, the statement is equivalent to
P(B-2CB-1) ≤ 1. Define X，B- 1 CB-2. Because any induced matrix norm is an upper-
bound on the spectral radius, it suffices to shoW kX k? = σmax(X) ≤ 1, Where σmax(X) denotes the
largest singular value of X .
By taking Schur complements of the block matrix (11) We have
0 W B - CBTCT = B2 (I - (B-2CB-2)(B-2CB- 1 ))B2
=B1 (I — XX t)B 1.
Note that the Schur complement is PSD because the original block matrix itself is (e.g. Zhang,
2006).
Using the fact that Z → B 2 ZB 2 maps positive semidefinite matrices to positive semidefinite
matrices, this implies
0 W I- XXT ⇒ XXT W I ⇒ kXk?? ≤ 1 ⇒ ρ(X) ≤ 1.
When the block matrix is positive definite, the inequalities become strict.
C EFFICIENT IMPLEMENTATION ASSUMING V0 AND V1 ARE
Kronecker-factored
The ultimate goal of our calculations is to efficiently compute the matrix-vector product of some
arbitrary vector z (Which Will often be the gradient vec(DW)) With our inverse Fisher approximation
F-1. That is, We Wish to compute F -1z.
It Will be convenient to assume that z is given as a matrix Z (With the same dimensions as DW) so
that z = vec(Z).
Multiplying the vector Z by FT = V0-1/2F-1V0)t/2 amounts to first multiplying by V-1/2, then
by F-1, and then by V0-1/2 again.
20
Published as a conference paper at ICLR 2018
We will suppose that we are given A0, A1, G0, and G1 such that
V0 = A0 0 G0 and Vi = Al 0 G>
We then have
V0-1/2 = A0-1/2 0 G0-1/2
and thus to multiply by V0-1/2 we can use the identity (C 0 B) vec(X) = vec(BXC>), giving
V0-1/2z = vec(G0-1/2ZA0-1/2).
In matrix form this is simply G0-1/2ZA0-1/2. Note that A0-1/2 and G0-1/2 can be computed using
the eigendecompositions of A0 and G0, for example.
The procedure to efficiently multiply a vector Z by 户 T is more involved and depends on which
approximation “option” we are using. However one immediate useful insight we can make before
specializing to Option 1 or Option 2 is that Ψ can be written as a Kronecker product as follows:
Ψ = V0T∕2VιV0T∕2
= (A0 0 G0)-1/2(A1 0 G1)(A0 0 G0)-1/2
= (A0-1/2 0 G0-1/2)(A1 0 G1)(A0-1/2 0 G0-1/2)
=	(A0-1/2A1A0-1/2) 0 (G0-1/2G1G0-1/2)
=ΨA 0 ΨG.
where We have defined ΨA = A-1/2AIA-1/2 and ΨG = G-1/2GIG-1/2.
C.1 Option 1
For Option 1 We assume that V1, and hence Ψ = VI = V0-1/2V1V0-1/2, is symmetric. We note that
V1 = A1 0 G1 will be symmetric if and only if both A1 and G1 are.
Our task is to compute the matrix-vector product
FTz = U diag(γ(ψ))U>z
where U diag(ψ)U >is the eigendecomposition of Ψ, and γ(x) is defined as in Section 3.5.3, eqn. 8.
To do this We first multiply the vector by U>, then by diag(γ(ψ)), and then finally by U.
The eigendecomposition of Ψ can be computed efficiently using its Kronecker product structure. In
particular, we compute the eigendecompositions of each factor as
UA diag(ψA)UA> = ΨA and	UG diag(ψG)UG> = ΨG,
from which we can write the eigendecomposition U diag(ψ)U -1 of Ψ as
Ψ = Ψ A 0 Ψ g
=(UA diag(ψa)U>) 0 (UG diag(ψ⅛G)U>)
=(UA 0 UG)(diag(ψa) 0 diag(ψ⅛G))(U> 0 U>)
=(UA 0 UG)diag(vec(ψGψ>))(U> 0 U>).
In other words, we have U = UA 0 UG and ψ = vec(ψGψA> ).
To multiply z by U> = UA> 0 UG> we use the identity (C0B) vec(X) = vec(BXC>) which gives
U>z = (UA> 0 UG>) vec(Z) = vec(UG>ZUA).
Similarly, the multiplication of z by U = UA 0 UG can be computed as vec(UGZUA>).
Finally, multiplying z by diag(γ(ψ)) = diag(γ(vec(ψGψA>))) corresponds to entry-wise mul-
tiplication of Z by a matrix Y , where vec(Y ) = γ(vec(ψGψA>)), or in other words [Y]i,j =
γ([ψGψA>]i,j) = γ([ψG]i[ψA]j). Thus we have
ʌ
diag(γ(ψ))z = diag(Vec(Y)) Vec(Z) = Vec(Z Θ Y),
21
Published as a conference paper at ICLR 2018
where denotes entry-wise multiplication of matrices.
In summary We have that 户Tz can be computed in matrix form as
UG((UG>ZUA)	Y)UA>
for Y s.t. [Y]i,j = y([Ψg]i[ψa]j).
We note that computing γ([ψG]i [ψA]j) is trivial since it is just a scalar evaluation of the rational
function γ(x).
C.2 Option 2
For Option 2 We must compute the matrix-vector product
1
FTz =————(I - Ψ)(I - Ψ>Ψ)-1(I - Ψ>)v.
i ωiTi
To do this we will first multiply by I - Ψ>, then by (I - Ψ>Ψ)-1, and then by I - Ψ, before finally
dividing the result by Pi ωiTi .
To compute the matrix-vector product (I - Ψ 1 )z we use the identity (C 0 B)Vec(X) =
vec(BXC>) while noting that Ψ> = (ΨA0 ΨG)> = ΨA> 0 ΨG>. This gives
(I — Ψ>)vec(Z) = Vec(Z) — (Ψ> 0 ΨG) Vec(Z)
=Vec(Z) — vec(Ψ >Z Ψ a).
The matrix form of this is simply Z - ΨGZΨ A.
We may similarly compute (I - Ψ)z in matrix form as Z - ΨGZ ΨA>.
The harder task is to compute (I - Ψ>Ψ)-1z, which is what we tackle next.
We first observe that
I - Ψ> Ψ = I0 I - (ΨA> 0 Ψ>G)(ΨA 0 ΨG)
=10 I - (Ψ>Ψa) 0 (ΨGΨG)
≡ I0 I - MA 0 MG.
Given the eigendecompositions EA diag(mA)EA> = MA and EG diag(mG)EG> = MG we can thus
compute the larger eigendecomposition as
I 0 I - MA 0 MG = (EA 0 EG)(I 0 I - diag(mA) 0 diag(mG))(EA> 0 EG>)
=(EA 0 EG) diag(l 01- mA 0 mg)(E> 0 E>)
=(EA 0 EG) diag(Vec(Il 1> - mgm>))(E> 0 E>),
where 1 is the vector of ones.
Using the eigendecomposition the inverse can then be easily computed as
(I 0 I - MA 0 Mg)-1 = (EA 0 EG) diag(Vec(Il 1> - mGm>))-1 (E> 0 E>).
Thus (I - Ψ >Ψ)-1z maybe computed by first multiplying by (E> 0 E>), thenby diag(Vec(Il 1> -
mGm>))-1 (which in matrix form corresponds to element-wise division by > 1> - mgmA), and
then by EA 0 EG . The matrix form of this is
Eg((E>ZEa) 0 (口 1> - m&m>))E>,
where B C denotes element-wise division of the matrix B by the matrix C.
22
Published as a conference paper at ICLR 2018
C.3 PSEUDO-CODE FOR THE COMPUTATION OF F-1z
Given Ao, Ai, Go, and Gi such that V0 = Ao 0 Go and V1 = Ai 0 Gi, the procedure to compute
F-iz for an arbitrary vector z = vec(Z) is as follows.
Pre-processing for Option 1 only:
•	Ensure that Ai and Gi are exactly symmetric, and if not, symmertrize them via:
Ai + Ai>	Gi + Gi>
Ai V-----2----- and Gi V-----------2----
Pre-processing steps (both options):
•	Compute the matrix square roots of the factors of Vo (e.g. using eigendecompositions):
Ao-i/2 and	Go-i/2
ʌ
•	Compute the factors of the transformed transition matrix Ψ:
Ψ A = A-i/2AiA-i/2 and	Ψ G = G-i/2GiG-i/2
Pre-processing for Option 1 only:
•	Compute the eigendecompositions of the factors of Ψ
UA diag(ψA)UA> = ΨA and	UG diag(ψG)UG> = ΨG
•	Sum across the temporal correlations and then invert by performing the element-wise com-
putation in eigenspace:
(1 - x)2
[Y]ij = γαψG]i[ψA]j) Where Y(X) = Pi3i(Ti(i i-2x(i-XTi))
Pre-processing for Option 2 only:
•	Compute the eigendecompositions of the factors of Ψ> Ψ (or equivalently the SVD’s of ΨA
and ΨG ):
EA diag(mA)E> = Ψ>ΨA and	EG diag(mG)E> = ΨGΨG
z-dependent calculations for Option 1:
•	Multiply the input vector z = vec(Z) by Vo-i/2 :
Zo = Go-i/2ZAo-i/2
•	Multiply by F-I using its eigendeComPosition:
Zi =UG((UG>ZoUA)Y)UA>
where B C denotes the element-wise product between matrices B and C .
•	Multiply the result by Vo-i/2 and express as a vector:
Z2 = Go-i/2ZiAo-i/2 and F -iz = vec(Z2)
z-dependent calculations for Option 2:
•	Multiply the input vector z = vec(Z) by Vo-i/2 :
Zo = Go-i/2ZAo-i/2
23
Published as a conference paper at ICLR 2018
•	Multiply the result by I - Ψ> :
Zi = Zo — Ψ gZoΨ a
•	Multiply the previous result by (I 一 Ψ>Ψ)-1 using its eigendeComPosition:
Z2 = EG((EGZIEA) 0 (1 口 - mgm>))E>
where X Y denotes the element-wise division of X by Y .
•	Multiply the result by I 一 Ψ
Z3 = Z2 一 ψGZ2ψA
•	Normalize the result by IET [T] = Pi ωiT (for T and ωi as defined at the bottom of
Section 3.4):
Z4 =	---ʃ Z3
i ωiTi
•	Multiply the result by V0-1/2 and express as a vector:
Z5 = G0-1/2Z4A0-1/2 and F-1z = vec(Z5)
24
Published as a conference paper at ICLR 2018
——Adam
—Adam+LayerNorm
—K-FAC indep.
——K-FAC optionl
—K-FAC option2
-SGD
O 50	100	150	200	250	300	350	0	1000	2000	3000	4000	5000	6000
Updates	Seconds
Figure 4: Generalization performance of our method compared to the baselines in perplexity-per-
word on length-35 word sequences from Penn-TreeBank. All the methods used a mini-batch size of
200. K-FAC indep. uses the update in eqn. 6, K-FAC option1 uses eqn. 8, and K-FAC option2 uses
eqn. 9. (left) Validation perplexity v.s. the number of updates. The dashed lines correspond to the
experiments that used RNNs with 1024 LSTM units, and the solid lines correspond to experiments
that used RNNs with 650 LSTM units. (right) Validation perplexity v.s. the wall-clock time.
D Test performance on Penn-TreeBank
25