Published as a conference paper at ICLR 2018
Guide Actor-Critic for Continuous Control
Voot Tangkaratt	Abbas Abdolmaleki
RIKEN AIP, Tokyo, Japan	The University of Aveiro,	Aveiro, Portugal
voot.tangkaratt@riken.jp	abbas.a@ua.pt
Masashi Sugiyama
RIKEN AIP, Tokyo, Japan
The University of Tokyo, Tokyo, Japan
masashi.sugiyama@riken.jp
Ab stract
Actor-critic methods solve reinforcement learning problems by updating a param-
eterized policy known as an actor in a direction that increases an estimate of the
expected return known as a critic. However, existing actor-critic methods only
use values or gradients of the critic to update the policy parameter. In this pa-
per, we propose a novel actor-critic method called the guide actor-critic (GAC).
GAC firstly learns a guide actor that locally maximizes the critic and then it up-
dates the policy parameter based on the guide actor by supervised learning. Our
main theoretical contributions are two folds. First, we show that GAC updates the
guide actor by performing second-order optimization in the action space where
the curvature matrix is based on the Hessians of the critic. Second, we show that
the deterministic policy gradient method is a special case of GAC when the Hes-
sians are ignored. Through experiments, we show that our method is a promising
reinforcement learning method for continuous controls.
1	Introduction
The goal of reinforcement learning (RL) is to learn an optimal policy that lets an agent achieve the
maximum cumulative rewards known as the return (Sutton & Barto, 1998). Reinforcement learning
has been shown to be effective in solving challenging artificial intelligence tasks such as playing
games (Mnih et al., 2015; Silver et al., 2016) and controlling robots (Deisenroth et al., 2013; Levine
et al., 2016).
Reinforcement learning methods can be classified into three categories: value-based, policy-based,
and actor-critic methods. Value-based methods learn an optimal policy by firstly learning a value
function that estimates the expected return. Then, they infer an optimal policy by choosing an
action that maximizes the learned value function. Choosing an action in this way requires solving a
maximization problem which is not trivial for continuous controls. While extensions to continuous
controls were considered recently, they are restrictive since specific structures of the value function
are assumed (Gu et al., 2016; Amos et al., 2017).
On the other hand, policy-based methods, also called policy search methods (Deisenroth et al.,
2013), learn a parameterized policy maximizing a sample approximation of the expected re-
turn without learning the value function. For instance, policy gradient methods such as REIN-
FORCE (Williams, 1992) use gradient ascent to update the policy parameter so that the probability
of observing high sample returns increases. Compared with value-based methods, policy search
methods are simpler and naturally applicable to continuous problems. Moreover, the sample return
is an unbiased estimator of the expected return and methods such as policy gradients are guaranteed
to converge to a locally optimal policy under standard regularity conditions (Sutton et al., 1999).
However, sample returns usually have high variance and this makes such policy search methods
converge too slowly.
Actor-critic methods combine the advantages of value-based and policy search methods. In these
methods, the parameterized policy is called an actor and the learned value-function is called a critic.
1
Published as a conference paper at ICLR 2018
The goal of these methods is to learn an actor that maximizes the critic. Since the critic is a low vari-
ance estimator of the expected return, these methods often converge much faster than policy search
methods. Prominent examples of these methods are actor-critic (Sutton et al., 1999; Konda & Tsit-
siklis, 2003), natural actor-critic (Peters & Schaal, 2008), trust-region policy optimization (Schul-
man et al., 2015a), and asynchronous advantage actor-critic (Mnih et al., 2016). While their ap-
proaches to learn the actor are different, they share a common property that they only use the value
of the critic, i.e., the zero-th order information, and ignore higher-order ones such as gradients and
Hessians w.r.t. actions of the critic1. To the best of our knowledge, the only actor-critic methods
that use gradients of the critic to update the actor are deterministic policy gradients (DPG) (Silver
et al., 2014) and stochastic value gradients (Heess et al., 2015). However, these two methods do not
utilize the second-order information of the critic.
In this paper, we argue that the second-order information of the critic is useful and should not be
ignored. A motivating example can be seen by comparing gradient ascent to the Newton method: the
Newton method which also uses the Hessian converges to a local optimum in a fewer iterations when
compared to gradient ascent which only uses the gradient (Nocedal & Wright, 2006). This suggests
that the Hessian of the critic can accelerate actor learning which leads to higher data efficiency.
However, the computational complexity of second-order methods is at least quadratic in terms of
the number of optimization variables. For this reason, applying second-order methods to optimize
the parameterized actor directly is prohibitively expensive and impractical for deep reinforcement
learning which represents the actor by deep neural networks.
Our contribution in this paper is a novel actor-critic method for continuous controls which we call
guide actor-critic (GAC). Unlike existing methods, the actor update of GAC utilizes the second-
order information of the critic in a computationally efficient manner. This is achieved by separating
actor learning into two steps. In the first step, we learn a non-parameterized Gaussian actor that
locally maximizes the critic under a Kullback-Leibler (KL) divergence constraint. Then, the Gaus-
sian actor is used as a guide for learning a parameterized actor by supervised learning. Our analysis
shows that learning the mean of the Gaussian actor is equivalent to performing a second-order update
in the action space where the curvature matrix is given by Hessians of the critic and the step-size is
controlled by the KL constraint. Furthermore, we establish a connection between GAC and DPG
where we show that DPG is a special case of GAC when the Hessians and KL constraint are ignored.
2	Background
In this section, we firstly give a background of reinforcement learning. Then, we discuss existing
second-order methods for policy learning and their issue in deep reinforcement learning.
2.1	Reinforcement Learning
We consider discrete-time Markov decision processes (MDPs) with continuous state space S J Rds
and continuous action space A J Rda. We denote the state and action at time step t P N by
st and at, respectively. The initial state s1 is determined by the initial state density s1 „ ppsq.
At time step t, the agent in state St takes an action at according to a policy at 〜∏(a∣st) and
obtains a reward rt “ rpst , atq. Then, the next state st`1 is determined by the transition function
st`1 〜 pps1|st, atq. A trajectory τ “ ps1, a1, r1, s2, . . . q gives us the cumulative rewards or return
defined as 28“i YtTr(st, at), where the discount factor 0 V Y V 1 assigns different weights to
rewards given at different time steps. The expected return of π after executing an action a in a state
s can be expressed through the action-value function which is defined as
8
Qn ps, aq = En(at|st)t",p(St'i|st,at)Ai £ YtTr(St, at)|s1 = s, a1 = a ,	⑴
t“1
where Ep [•] denotes the expectation over the density P and the subscript t21 indicates that the
expectation is taken over the densities at time steps t21. We can define the expected return as
8
J(πq =
EP(si),n(at|st)t》i,P(st'i|st,at)t》i γt´ YtTr(St, at) “ Ep(s),π(a∣s) rQπ (S, a)s.	(2)
t“1
1This is different from using the gradient of the critic w.r.t. critic parameters to update the critic itself.
2
Published as a conference paper at ICLR 2018
The goal of reinforcement learning is to find an optimal policy that maximizes the expected return.
The policy search approach (Deisenroth et al., 2013) parameterizes π by a parameter θ P Rdθ and
finds θ< which maximizes the expected return:
θ< “ argmaxEp(s),n0(a|s)[Qπθ (s, a)].	(3)
θ
Policy gradient methods such as REINFORCE (Williams, 1992) solve this optimization problem by
gradient ascent:
θ D θ + αEp(s),∏θ(a∣s) [Vθ log∏θ(a∣s)Qπθ (s, a)],	(4)
where α > 0 is a step-size. In policy search, the action-value function is commonly estimated by the
sample return: Qπθ (s, a) « N Xn t Yt´1r(st,n, at,n) obtained by collecting N trajectories using
πθ . The sample return is an unbiased estimator of the action-value function. However, it often has
high variance which leads to slow convergence.
An alternative approach is to estimate the action-value function by a critic denoted by Q(s, a)
whose parameter is learned such that Q(s, a) « Qπθ (s, a). By replacing the action-value function
in Eq.(3) with the critic, we obtain the following optimization problem:
θ< “ argmax Ep(s),∏°(a|s)∣Q(s, a)] .	(5)
The actor-critic method (Sutton et al., 1999) solves this optimization problem by gradient ascent:
θ D θ + αEp(sq,∏θ(a|s)∣Vθ log ∏θ(a∣s)Q(s, a)] .	(6)
The gradient in Eq.(6) often has less variance than that in Eq.(4), which leads to faster convergence2.
A large class of actor-critic methods is based on this method (Peters & Schaal, 2008; Mnih et al.,
2016). As shown in these papers, these methods only use the value of the critic to learn the actor.
The deterministic policy gradients (DPG) method (Silver et al., 2014) is an actor-critic method that
uses the first-order information of the critic. DPG updates a deterministic actor πθ (s) by
θ D θ + αEp(sq [Vθ∏θ(S)VaQ(S, a)∣α=∏θ(s)].	⑺
A method related to DPG is the stochastic value gradients (SVG) method (Heess et al., 2015) that
is able to learn a stochastic policy but it requires learning a model of the transition function.
The actor-critic methods described above only use up to the first-order information of the critic when
learning the actor and ignore higher-order ones. Below, we discuss existing approaches that utilize
the second-order information by applying second-order optimization methods to solve Eq.(5).
2.2	Second-order Methods for Policy Learning
The actor-critic methods described above are first-order methods which update the optimization
variables based on the gradient of the objective function. First-order methods are popular in deep
learning thanks to their computational efficiency. However, it is known in machine learning that
second-order methods often lead to faster learning because they use the curvature information to
compute a better update direction, i.e., the steepest ascent direction along the curvature3.
The main idea of second-order methods is to rotate the gradient by the inverse ofa curvature matrix.
For instance, second-order updates for the actor-critic method in Eq.(6) are in the following form:
θ D θ + αGT {Ep(s),∏θ(a∣s) ∣Vθ log ∏(a∣s)Q(s, a)]),	⑻
2This gradient is a biased estimator of the policy gradient in Eq.(4). However, it is unbiased under some
regularity conditions such as compatible function conditions (Sutton et al., 1999).
3We use the term “second-order methods” in a broad sense here, including quasi-Newton and natural gradi-
ent methods which approximate the curvature matrix by the first-order information.
3
Published as a conference paper at ICLR 2018
where G P Rdθ xdθ is a curvature matrix. The behavior of second-order methods depend on the
definition of a curvature matrix. The most well-known second-order method is the Newton method
where its curvature matrix is the Hessian of the objective function w.r.t. the optimization variables:
GHessian “ Ep(s),n0(a|s)[(Vθ log∏θ(a∣s)Ve log∏θ(a∣s)J + Vθ log∏θ(a∣s)) Qp(s, a)] . (9)
The natural gradient method is another well-known second-order method which uses the Fisher
information matrix (FIM) as the curvature matrix (Amari, 1998):
GFIM “ Ep(s),∏θ(a|s)[Vθ log∏θ(a∣s)Vθ log∏θ(a∣s)J‰ .	(10)
Unlike the Hessian matrix, FIM provides information about changes of the policy measured by an
approximatedKLdivergence: Ep(S) [KL(∏θ(a∣s)∣∣∏θi(a∣s))S « (θ — Θ1)jGfim(Θ — θ1) (Kakade,
2001). We can see that GHessian and GFIM are very similar but the former also contains the critic
and the Hessian of the actor while the latter does not. This suggests that the Hessian provides more
information than that in FIM. However, FIM is always positive semi-definite while the Hessian may
be indefinite. Please see Furmston et al. (2016) for detailed comparisons between the two curvature
matrices in policy search4 . Nonetheless, actor-critic methods based on natural gradient were shown
to be very efficient (Peters & Schaal, 2008; Schulman et al., 2015a).
We are not aware of existing work that considers second-order updates for DPG or SVG. However,
their second-order updates can be trivially derived. For example, a Newton update for DPG is
θ D θ + α0τ {Ep(s) ∣Vθ∏θ(S)VaQ(s,叽小(S)]},	(11)
where the (i, j)-th entry of the Hessian matrix D P Rdθ X dθ is
Dij = Ep(S)I 'T S) VaQ(s, αqla=∏θ (s) ^nθθ^ɪ + TnB: VaQ(s，a)la=∏θ (S) ∙	(12)
Bθi	Bθj	Bθi Bθj
Note that Bπn(S){Bθ and B2πn (S){BθBθ1 are vectors since πn(S) is a vector-valued function. In-
terestingly, the Hessian of DPG contains the Hessians of the actor and the critic. In contrast, the
Hessian of the actor-critic method contains the Hessian of the actor and the value of the critic.
Second-order methods are appealing in reinforcement learning because they have high data effi-
ciency. However, inverting the curvature matrix (or solving a linear system) requires cubic compu-
tational complexity in terms of the number of optimization variables. For this reason, the second-
order updates in Eq.(8) and Eq.(11) are impractical in deep reinforcement learning due to a large
number of weight parameters in deep neural networks. In such a scenario, an approximation of
the curvature matrix is required to reduce the computational burden. For instance, Furmston et al.
(2016) proposed to use only diagonal entries of an approximated Hessian matrix. However, this
approximation clearly leads to a loss of useful curvature information since the gradient is scaled but
not rotated. More recently, Wu et al. (2017) proposed a natural actor-critic method that approxi-
mates block-diagonal entries of FIM. However, this approximation corresponds to ignoring useful
correlations between weight parameters in different layers of neural networks.
3	Guide Actor-Critic
In this section, we propose the guide actor-critic (GAC) method that performs second-order updates
without the previously discussed computational issue. Unlike existing methods that directly learn
the parameterized actor from the critic, GAC separates the problem of learning the parameterized
actor into problems of 1) learning a guide actor that locally maximizes the critic, and 2) learning a
parameterized actor based on the guide actor. This separation allows us to perform a second-order
update for the guide actor where the dimensionality of the curvature matrix is independent of the
parameterization of the actor.
We formulate an optimization problem for learning the guide actor in Section 3.1 and present its so-
lution in Section 3.2. Then in Section 3.3 and Section 3.4, we show that the solution corresponds to
performing second-order updates. Finally, Section 3.5 presents the learning step for the parameter-
ized actor using supervised learning. The pseudo-code of our method is provided in Appendix B and
the source code is available at https://github.com/voot- t/guide- actor- critic.
4 Furmston et al. (2016) proposed an approximate Newton method for policy search. Their policy search
method was shown to perform better than methods based on gradient ascent and natural gradient ascent.
4
Published as a conference paper at ICLR 2018
3.1	Optimization Problem for Guide Actor
Our first goal is to learn a guide actor that maximizes the critic. However, greedy maximization
should be avoided since the critic is a noisy estimate of the expected return and a greedy actor
may change too abruptly across learning iterations. Such a behavior is undesirable in real-world
problems, especially in robotics (Deisenroth et al., 2013). Instead, we maximize the critic with
additional constraints:
max Epe(s),∏(a∣s) ∣Qp(s, a)],
subject to Epe(Sq [KL(Π(a∣s)∣∣∏θ(a∣s))S ≤ e,
Epe(S) [H(∏(a∣s))S2 κ,
Epe(s)π(a∣s) = 1,	(13)
where ∏(a∣s) is the guide actor to be learned, ∏θ(a∣s) is the current parameterized actor that We
want to improve upon, and pβ (s) is the state distribution induced by past trajectories. The objective
function differs from the one in Eq.(5) in two important aspects. First, we maximize for a policy
function ∏ and not for the policy parameter. This is more advantageous than optimizing for a policy
parameter since the policy function can be obtained in a closed form, as will be shown in the next
subsection. Second, the expectation is defined over a state distribution from past trajectories and this
gives us off-policy methods with higher data efficiency. The first constraint is the Kullback-Leibler
(KL) divergence constraint where KL(p(x)||q(x)) “ Ep(x) rlog p(x) ´ log q(x)s. The second con-
straint is the Shannon entropy constraint where H(P(X)) “ 一Ep(x)[logp(x)S. The KL constraint
is commonly used in reinforcement learning to prevent unstable behavior due to excessively greedy
update (Peters & Schaal, 2008; Peters et al., 2010; Levine & Koltun, 2013; Schulman et al., 2015a).
The entropy constraint is crucial for maintaining stochastic behavior and preventing premature con-
vergence (Ziebart et al., 2010; Abdolmaleki et al., 2015; Mnih et al., 2016; Haarnoja et al., 2017).
The final constraint ensures that the guide actor is a proper probability density. The KL bound
E > 0 and the entropy bound —8 V K < 8 are hyper-parameters which control the exploration-
exploitation trade-off of the method. In practice, we fix the value of and adaptively reduce the
value of κ based on the current actor’s entropy, as suggested by Abdolmaleki et al. (2015). More
details of these tuning parameters are given in Appendix C.
This optimization problem can be solved by the method of Lagrange multipliers. The solution is
π(a∣s) 9 ∏θ(a|s) η*'ω* exp ∣ @"，。)∣
∖η*' ω< I
(14)
where η< > 0 and ω< > 0 are dual variables corresponding to the KL and entropy constraints,
respectively. The dual variable corresponding to the probability density constraint is contained in the
normalization term and is determined by η< and ω<. These dual variables are obtained by minimizing
the dual function:
g(η, ωq = η 一 ωκ + (η + ωqEpe(S)
log f ∏θ(a|s)η⅛ eχp(Q^) da
η+ω
(15)
All derivations and proofs are given in Appendix A. The solution in Eq.(14) tells us that the
guide actor is obtained by weighting the current actor with Q(s, a). If we set E → 0 then we
have ∏ « ∏θ and the actor is not updated. On the other hand, if we set E → 8 then we have
π(a∣s) 9 exp(Q(s, a){ω<), which is a softmax policy where ω< is the temperature parameter.
3.2	Learning Guide Actor
Computing π(a|s) and evaluating g(η, ω) are intractable for an arbitrary ∏θ(a|s). We overcome
this issue by imposing two assumptions. First, we assume that the actor is the Gaussian distribution:
∏θ(a|s) = N(a∖φθ(s), Σθ(s)),	(16)
where the mean φθ (s) and covariance Σθ (s) are functions parameterized by a policy parameter
θ. Second, we assume that Taylor’s approximation of Q(s, a) is locally accurate up to the second-
order. More concretely, the second-order Taylor’s approximation using an arbitrary action a0 is
5
Published as a conference paper at ICLR 2018
given by
Qp(s, a) « Qp(s, ao) + (a — ao)Jgo(s) ' 1 (a — ao)JHo(s)(a — a0) ' O(}a}3),	(17)
where g°(s) “ Va<5(s, α)∣a=ao and Ho(s) “ VaQp(s, aja=。。are the gradient and Hessian of
the critic w.r.t. a evaluated at a0, respectively. By assuming that the higher order term O(}a}3) is
sufficiently small, we can rewrite Taylor’s approximation at a0 as
1
Qo(s, a)“ 2aJHo(s)a ' aJΨo(s) ' ξo(s),	(18)
where Ψo(S) “ go(s) — H0(s)a0 and ξo(S) “ 1 aJH0(s)a0 — aJgo(s) ' Q(s, ao). Note that
H0 (s), ψ0 (s), and ξ0 (s) depend on the value of a0 and do not depend on the value of a. This
dependency is explicitly denoted by the subscript. The choice ofa0 will be discussed in Section 3.3.
Substituting the Gaussian distribution and Taylor’s approximation into Eq.(14) yields another Gaus-
sian distribution ∏(a∣s) “ N(a∣φ'(s), Σ'(s)), where the mean and covariance are given by
φ'(s) = FT(S)L(s),	Σ'(s) = (η< ' ω*)FT(s).
The matrix F(s) P RdaXda and vector L(s) P Rda are defined as
F(S)= η*∑e1(S) — Ho(s),	L(S) = η<∑θ 1(s)Φθ(S) ' Ψo(s)∙
The dual variables η< and ω< are obtained by minimizing the following dual function:
(19)
(20)
ʌz ʌ	, Z , w	1	∖2π(η + ω)Fη1 (S)I
g(n,ω)= η ― ωκ ` (n ` ω)Epβ (S) logʌ —一 口… 丁—
V	∣2π∑θ (s)| η'ω
' 1 Epe(s) “Ln(S)JF´1(S)Ln (s) — ηφθ(S)J£e1(S)0e(s)‰ ' Const,
where Fn(s) and Ln(s) are defined similarly to F(s) and L(s) but with η instead of η<.
(21)
The practical advantage of using the Gaussian distribution and Taylor’s approximation is that the
guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-
vector products. The expectation over pβ (S) can be approximated by e.g., samples drawn from a
replay buffer (Mnih et al., 2015). We require inverting Fn (S) to evaluate the dual function. However,
these matrices are computationally cheap to invert when the dimension of actions is not large.
As shown in Eq.(19), the mean and covariance of the guide actor is computed using both the gradient
and Hessian of the critic. Yet, these computations do not resemble second-order updates discussed
previously in Section 2.2. Below, we show that for a particular choice of a0, the mean computation
corresponds to a second-order update that rotates gradients by a curvature matrix.
3.3	Guide Actor Learning as Second-order Optimization
For now we assume that the critic is an accurate estimator of the true action-value function. In this
case, the quality of the guide actor depends on the accuracy of sample approximation in gp(η, ω) and
the accuracy of Taylor’s approximation. To obtain an accurate Taylor’s approximation of Q(S, a)
using an action a0, the action a0 should be in the vicinity of a. However, we did not directly use
any individual a to compute the guide actor, but we weight ∏θ(a∣S) by exp(Q(S, a)) (see Eq.(14)).
Thus, to obtain an accurate Taylor’s approximation of the critic, the action a0 needs to be similar to
actions sampled from ∏θ(a∣S). Based on this observation, we propose two approaches to perform
Taylor’s approximation.
Taylor’s approximation around the mean. In this approach, we perform Taylor’s approximation
using the mean of ∏θ(a∣S). More specifically, we use a° = E∏θ(。⑼[a] = φθ(s) for Eq.(18). In
this case, we can show that the mean update in Eq.(19) corresponds to performing a second-order
update in the action space to maximize Q(S, a):
Φ'(s) = Φθ(s) ' F´1(S)VaQ(s, a)∣a=φθ(s),	(22)
6
Published as a conference paper at ICLR 2018
where Fe@ (S) “ η<∑θ1 (S) — H力@ (Sqand Hφθ (S) “ ▽?◊(，, a)∣a=φθ(s)∙ This equivalence can
be shown by substitution and the proof is given in Appendix A.2. This update equation reveals that
the guide actor maximizes the critic by taking a step in the action space similarly to the Newton
method. However, the main difference lies in the curvature matrix where the Newton method uses
Hessians He@ (s) but we use a damped Hessian Fe@ (s). The damping term η*∑θ`(s) corresponds
to the effect of the KL constraint and can be viewed as a trust-region that controls the step-size. This
damping term is particularly important since Taylor’s approximation is accurate only locally and we
should not take a large step in each update (Nocedal & Wright, 2006).
Expectation of Taylor’s approximations. Instead of using Taylor’s approximation around the
mean, we may use an expectation of Taylor’s approximation over the distribution. More concretely,
we define Q(S, a) to be an expectation of Q0(S, a) over πθ (a0 |S):
1
Q(S, aq = 2 ajE∏θPa0∣s) rH 0(s)s a ' aJE∏θ Pao∣s) rψ0 (s)s+ E∏θ (a0|s) r&(s)s.	(23)
Note that E∏θ(ao∣s)[Ho(s)S “ E∏θ(a0|s)[▽?©(，，a)|a“a。S and the expectation is computed w.r.t.
the distribution ∏θ of a°. We use this notation to avoid confusion even though ∏θ (a0 |s) and ∏θ (a|s)
are the same distribution. When Eq.(23) is used, the mean update does not directly correspond
to any second-order optimization step. However, under an (unrealistic) independence assumption
E∏θ(a0∣s)[H0(S)a0S = E∏θ(αo∣s)[Ho(s)SE∏θ(aois)[ɑos, we can show that the mean update corre-
sponds to the following second-order optimization step:
Φ'(S) = Φθ (s)' E∏θ (ao|s) [F 0(s)]T E∏θ (αo∣s) ∣VaQ(S,叽“。。],	(24)
where E∏θ①⑻[Fo(s)S = η*∑θ 1(s) — E∏θ①⑻[Ho(s)]. Interestingly, the mean is updated
by rotating an expected gradient using an expected Hessians. In practice, the expectations can be
approximated using sampled actions [ao,i}S“i 〜 ∏θ(a∣S). We believe that this sampling can be
advantageous for avoiding local optima. Note that when the expectation is approximated by a single
sample a° 〜∏θ(a∣S), we obtain the update in Eq.(24) regardless of the independence assumption.
In the remainder, we use F(S) to denote both of Fe@ (S) and E∏θ⑶⑻[Fo(s)S, and use H(S) to
denote both of He@ (s) and E∏θ⑶⑼[Ho(s)S. In the experiments, we use GAC-0 to refer to GAC
with Taylor’s approximation around the mean, and we use GAC-1 to refer to GAC with Taylor’s
approximation by a single sample a° 〜∏θ(a|s).
3.4	Gauss-Newton Approximation of Hessian
The covariance update in Eq.(19) indicates that F(s) = η*∑G1 (S) — H(s) needs to be positive
definite. The matrix F(S) is guaranteed to be positive definite if the Hessian matrix H(S) is negative
semi-definite. However, this is not guaranteed in practice unless Q(S, a) is a concave function in
terms of a. To overcome this issue, we firstly consider the following identity:
H(S) = V2aQp(S, a) = —VaQp(S, a)VaQp(S, a)J ` V2aexp(Qp(S, a)) exp(—Qp(S, a)).	(25)
The proof is given in Appendix A.3. The first term is always negative semi-definite while the second
term is indefinite. Therefore, a negative semi-definite approximation of the Hessian can be obtained
as
H0(S) « — Va Qp(S, a)Va Qp(S, a)J
a“a0
(26)
The second term in Eq.(25) is proportional to exp(—Q(s, a)) and it will be small for high values
of Q(S, a). This implies that the approximation should gets more accurate as the policy approach
a local maxima of Q(S, a). We call this approximation Gauss-Newton approximation since it is
similar to the Gauss-Newton approximation for the Newton method (Nocedal & Wright, 2006).
3.5	Learning Parameterized Actor
The second step of GAC is to learn a parameterized actor that well represents the guide actor. Below,
we discuss two supervised learning approaches for learning a parameterized actor.
7
Published as a conference paper at ICLR 2018
3.5.1	Fully-Parameterized Gaussian Policy
Since the guide actor is a Gaussian distribution with a state-dependent mean and covariance, a
natural choice for the parameterized actor is again a parameterized Gaussian distribution with a
state-dependent mean and covariance: ∏θ(a|s) “ N(a∣Φθ(s), ∑θ(s)). The parameter θ can be
learned by minimizing the expected KL divergence to the guide actor:
LKL(θ) = Epe (S) [KL pπθ (a|s)||n(a|s))s
“ Epe(S) „Ir(F"⑶)´ log I∑Θ(s)l] + ? + const，	(27)
y J η< + ω<	J η< + ω<
where Lw(θ) = Epe⑶ ∣}Φe(s)´ Φ+(s)}F⑸[is the weighted-mean-squared-error (WMSE)
which only depends on θ of the mean function. The const term does not depend on θ .
Minimizing the KL divergence reveals connections between GAC and deterministic policy gradients
(DPG) (Silver et al., 2014). By computing the gradient of the WMSE, it can be shown that
“L2 ⑻=´Epβ(s) ∣Vθ Φθ (S)VaQ(s, a)lα=φθ (s)l + Epe (s) [Vθ Φθ (S)VaQ(s, a)∣α=φ'(s)l
+ η*Epβ(s)[Vθφθ(s)∑T(s)H´l(s)VaQ(S, a)∣a=φθ(S)I
´ η*Epe(S) [VθΦθ(s)∑T(s)H´l(s)VaQ(s,叫。”/)] .	(28)
The proof is given in Appendix A.4. The negative of the first term is precisely equivalent to DPG.
Thus, updating the mean parameter by minimizing the KL loss with gradient descent can be regarded
as updating the mean parameter with biased DPG where the bias terms depend on η*. We can verify
that VaQ(s, a)∣a=φ+(S) = 0 when η = 0 and this is the case of E → 8. Thus, all bias terms vanish
when the KL constraint is ignored and the mean update of GAC coincides with DPG. However,
unlike DPG which learns a deterministic policy, we can learn both the mean and covariance in GAC.
3.5.2	Gaussian Policy with Parameterized Mean
While a state-dependent parameterized covariance function is flexible, we observe that learning
performance is sensitive to the initial parameter of the covariance function. For practical pur-
poses, we propose using a parametrized Gaussian distribution with state-independent covariance:
∏θ(a|s) = N(a∣φθ (s), Σ). This class of policies subsumes deterministic policies with additive in-
dependent Gaussian noise for exploration. To learn θ, we minimize the mean-squared-error (MSE):
LM(θ) = 1 Epe(S) [}Φθ(s) ´ Φ'(s)}2‰ .	(29)
For the covariance, we use the average of the guide covariances: Σ =E + ω*)Epe(S) [FT(S)].
For computational efficiency, we execute a single gradient update in each learning iteration instead
of optimizing this loss function until convergence.
Similarly to the above analysis, the gradient of the MSE w.r.t. θ can be expanded and rewritten into
VθLM(θ) = Epe(S) [VθΦθ(s)HT(S)(VaQ(s,叫。”°⑶ ´ VaQ(s,叫。”+))] .	(30)
Again, the mean update of GAC coincides with DPG when we minimize the MSE and set η< = 0
and H(S) = —I where I is the identity matrix. We can also substitute these values back into
Eq.(22). By doing so, we can interpret DPG as a method that performs first-order optimization in
the action space:
Φ'(s) = Φθ(s) + VaQ(S, a)∣a=φθ(S),	(31)
and then uses the gradient in Eq.(30) to update the policy parameter. This interpretation shows that
DPG is a first-order method that only uses the first-order information of the critic for actor learning.
Therefore in principle, GAC, which uses the second-order information of the critic, should learn
faster than DPG.
8
Published as a conference paper at ICLR 2018
3.6	Policy Evaluation for Critic
Beside actor learning, the performance of actor-critic methods also depends on the accuracy of the
critic. We assume that the critic Qν ps, aq is represented by neural networks with a parameter ν.
We adopt the approach proposed by Lillicrap et al. (2015) with some adjustment to learn ν. More
concretely, we use gradient descent to minimize the squared Bellman error with a slowly moving
target critic:
V D- V ´ α^νEPeps),β(a∣s),p(s1 |s,a)
(QV Ps, a) - y)
(32)
i	♦ ,ι .	♦ EI .	t	/	λ	ττn	「公 /	/ 八1♦	t λ
where α > 0 is the SteP-size. The target value y “ r(s, a) ' γE∏pai∣siq [Q〃Ps , a1)] is computed by
the target critic QVPs , a1) whose parameter V is updated by V — TV + (1 — T)ν for 0 V T V 1.
As suggested by Lillicrap et al. (2015), the target critic improves the learning stability and we set
T “ 0.001 in experiments. The expectation for the squared error is approximated using mini-batch
samples tPsn, an, rn, s1n)unN“1 drawn from a replay buffer. The expectation over the current actor
∏Pa1∣s1) is approximated using samples {a^ EuM— „ ∏θPais^) for each si. We do not use a
n,m m“	n	n
target actor to compute y since the KL upper-bound already constrains the actor update and a target
actor will further slow it down. Note that we are not restricted to this evaluation method and more
efficient methods such as Retrace (Munos et al., 2016) can also be used.
Our method requires computing VaQv Ps, a) and its outer product for the Gauss-Newton approxi-
mation. The computational complexity of the outer product operation is OPd2a) and is inexpensive
when compared to the dimension of V. For a linear-in-parameter model QVPs, a) “ V J μPs, a),
the gradient can be efficiently computed for common choices of the basis function μ such as the
Gaussian function. For deep neural network models, the gradient can be computed by the automatic-
differentiation (Goodfellow et al., 2016) where its cost depends on the network architecture.
4	Related Work
Besides the connections to DPG, our method is also related to existing methods as follows.
A similar optimization problem to Eq.(13) was considered by the model-free trajectory optimization
(MOTO) method (Akrour et al., 2016). Our method can be viewed as a non-trivial extension of
MOTO with two significant novelties. First, MOTO learns a sequence of time-dependent log-linear
Gaussian policies ∏tPa∣s) “ NPa∖Bts + bt, ∑t), while our method learns a log-nonlinear Gaussian
policy. Second, MOTO learns a time-dependent critic given by QtPs, a) “ 11 aτCta + aτDts +
aJ ct +ξtPs) and performs policy update with these functions. In contrast, our method learns a more
complex critic and performs Taylor’s approximation in each training step.
Besides MOTO, the optimization problem also resembles that of trust region policy optimization
(TRPO) (Schulman et al., 2015a). TRPO solves the following optimization problem:
maaX Ep∏θ (s),∏θ,(a|s) ∣QPs, a)] subject to Ep∏θ (S) [KLP∏θPa∖s)∖∖∏θ1 Pa∖s))S W e, (33)
where QPs, a) may be replaced by an estimate of the advantage function (Schulman et al., 2015b).
There are two major differences between the two problems. First, TRPO optimizes the policy param-
eter while we optimize the guide actor. Second, TRPO solves the optimization problem by conjugate
gradient where the KL divergence is approximated by the Fisher information matrix, while we solve
the optimization problem in a closed form with a quadratic approximation of the critic.
Our method is also related to maximum-entropy RL (Ziebart et al., 2010; Azar et al., 2012; Haarnoja
et al., 2017; Nachum et al., 2017), which maximizes the expected cumulative reward with an addi-
tional entropy bonus: 2t“i Epn(S) KPst, at) + αHP∏Pat∖st))], where α > 0 is a trade-off parame-
ter. The optimal policy in maximum-entropy RL is the softmax policy given by
∏MaxEntPa∖s) “ exp ^Q:oftPs, aa´ VoftPs)) 9exp ^Qfosaq)，	(34)
9
Published as a conference paper at ICLR 2018
where Qs‹oft ps, aq and Vs‹oft psq are the optimal soft action-value and state-value functions, respec-
tively (Haarnoja et al., 2017; Nachum et al., 2017). For a policy π, these are defined as
QnoftPs, aq = Ns, aq + YEp(si∣s,a) “VnftPs1q‰ ,	(35)
Vsπoftpsq “ αlog
exp
Qsnoft Ps, aq
da.
(36)
(
α
The softmax policy and the soft state-value function in maximum-entropy RL closely resemble the
guide actor in Eq.(14) when η< = 0 and the log-integral term in Eq.(15) when η = 0, respectively,
except for the definition of action-value functions. To learn the optimal policy of maximum-entropy
RL, Haarnoja et al. (2017) proposed soft Q-learning which uses importance sampling to compute
the soft value functions and approximates the intractable policy using a separate policy function.
Our method largely differs from soft Q-learning since we use Taylor’s approximation to convert the
intractable integral into more convenient matrix-vector products.
The idea of firstly learning a non-parameterized policy and then later learning a parameterized policy
by supervised learning was considered previously in guided policy search (GPS) (Levine & Koltun,
2013). However, GPS learns the guide policy by trajectory optimization methods such as an iterative
linear-quadratic Gaussian regulator (Li & Todorov, 2004), which requires a model of the transition
function. In contrast, we learn the guide policy via the critic without learning the transition function.
5	Experimental Results
We evaluate GAC on the OpenAI gym platform (Brockman et al., 2016) with the Mujoco Physics
simulator (Todorov et al., 2012). The actor and critic are neural networks with two hidden layers of
400 and 300 units, as described in Appendix C. We compare GAC-0 and GAC-1 against deep DPG
(DDPG) (Lillicrap et al., 2015), Q-learning with a normalized advantage function (Q-NAF) (Gu
et al., 2016), and TRPO (Schulman et al., 2015a;b). Figure 1 shows the learning performance on
9 continuous control tasks. Overall, both GAC-0 and GAC-1 perform comparably with existing
methods and they clearly outperform the other methods in Half-Cheetah.
The performance of GAC-0 and GAC-1 is comparable on these tasks, except on Humanoid where
GAC-1 learns faster. We expect GAC-0 to be more stable and reliable but easier to get stuck at
local optima. On the other hand, the randomness introduced by GAC-1 leads to high variance
approximation but this could help escape poor local optima. We conjecture GAC-S that uses S > 1
samples for the averaged Taylor’s approximation should outperform both GAC-0 and GAC-1. While
this is computationally expensive, we can use parallel computation to reduce the computation time.
The expected returns of both GAC-0 and GAC-1 have high fluctuations on the Hopper and Walker2D
tasks when compared to TRPO as can be seen in Figure 1g and Figure 1h. We observe that they can
learn good policies for these tasks in the middle of learning. However, the policies quickly diverge
to poor ones and then they are quickly improved to be good policies again. We believe that this
happens because the step-size FTPs) = (η*∑τ — HPs)) of the guide actor in Eq. (22) can
be very large near local optima for Gauss-Newton approximation. That is, the gradients near local
optima have small magnitude and this makes the approximation HPs) = VaQPs, a)VaQPs, a) J
small as well. If η*Σ-1 is also relatively small then the matrix FTPs) can be very large. Thus,
under these conditions, GAC may use too large step sizes to compute the guide actor and this results
in high fluctuations in performance. We expect that this scenario can be avoided by reducing the KL
bound or adding a regularization constant to the Gauss-Newton approximation.
Table 1 in Appendix C shows the wall-clock computation time. DDPG is computationally the most
efficient method on all tasks. GAC has low computation costs on tasks with low dimensional actions
and its cost increases as the dimensionality of action increases. This high computation cost is due to
the dual optimization for finding the step-size parameters η and ω. We believe that the computation
cost of GAC can be significantly reduced by letting η and ω be external tuning parameters.
6	Conclusion and Future Work
Actor-critic methods are appealing for real-world problems due to their good data efficiency and
learning speed. However, existing actor-critic methods do not use second-order information of the
10
Published as a conference paper at ICLR 2018

——GAC-1 ——GAC-0 ——DDPG ……QNAF ——TRPO
(c) Reacher	(d) Swimmer (e) Half-Cheetah
(a) Inverted-Pend. (b) Inv-Double-Pend.
O MO «W TO KO KW	O ax> «W TO TO MOO
τ*τ∙>⅛a γλoooi	TimMnMMOl
<βa βaa βaa ιβoa
Time weps<*1000)
a 20a ua caa β∞ IaM
"Πmesβps⅛Wθa)
J (W
o aɑɑ 4oα βαα β∞ ιβoa
FnleaePS <a WO S
二
(f) Ant	(g) Hopper	(h) Walker2D	(i) Humanoid
Figure 1: Expected returns averaged over 10 trials. The x-axis indicates training time steps. The y-
axis indicates averaged return and higher is better. More clear figures are provided in Appendix C.2.
critic. In this paper, we established a novel framework that distinguishes itself from existing work
by utilizing Hessians of the critic for actor learning. Within this framework, we proposed a prac-
tical method that uses Gauss-Newton approximation instead of the Hessians. We showed through
experiments that our method is promising and thus the framework should be further investigated.
Our analysis showed that the proposed method is closely related to deterministic policy gradients
(DPG). However, DPG was also shown to be a limiting case of the stochastic policy gradients when
the policy variance approaches zero (Silver et al., 2014). It is currently unknown whether our frame-
work has a connection to the stochastic policy gradients as well, and finding such a connection is
our future work.
Our main goal in this paper was to provide anew actor-critic framework and we do not claim that our
method achieves the state-of-the-art performance. However, its performance can still be improved in
many directions. For instance, we may impose a KL constraint for a parameterized actor to improve
its stability, similarly to TRPO (Schulman et al., 2015a). We can also apply more efficient policy
evaluation methods such as Retrace (Munos et al., 2016) to achieve better critic learning.
Acknowledgments
MS was partially supported by KAKENHI 17H00757.
References
Abbas Abdolmaleki, Rudolf Lioutikov, Jan Peters, NUno Lau, Luls Paulo Reis, and Gerhard NeU-
mann. Model-Based Relative Entropy Stochastic Search. In Advances in Neural Information
Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.
Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-Free Trajec-
tory Optimization for Reinforcement Learning. In Proceedings of the 33nd International Confer-
ence on Machine Learning, June 19-24, 2016, New York City, NY, USA, 2016.
Shun-ichi Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):
251-276,1998.
Brandon Amos, Lei Xu, and J. Zico Kolter. Input Convex Neural Networks. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
August 6-11, 2017, Sydney, Australia, 2017.
Mohammad Gheshlaghi Azar, Viceng Gomez, and Hilbert J. Kappen. Dynamic Policy Program-
ming. Journal of Machine Learning Research, 13:3207-3245, 2012.
11
Published as a conference paper at ICLR 2018
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on Policy Search for Robotics.
Foundations and Trends in Robotics, 2(1-2):1-142, 2013.
Thomas Furmston, Guy Lever, and David Barber. Approximate Newton Methods for Policy Search
in Markov Decision Processes. Journal of Machine Learning Research, 17(227):1-51, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the Difficulty of Training Deep Feedforward
Neural Networks. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the 13th In-
ternational Conference on Artificial Intelligence and Statistics, May 13-15, 2010, Sardinia, Italy,
2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
Shixiang Gu, Timothy P. Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning
with Model-based Acceleration. In Proceedings of the 33nd International Conference on Machine
Learning, June 19-24, 2016, New York City, NY, USA, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with
Deep Energy-Based Policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, August 6-11, 2017, Sydney, Australia, 2017.
Nicolas Heess, Gregory Wayne, David Silver, Timothy P. Lillicrap, Tom Erez, and Yuval Tassa.
Learning Continuous Control Policies by Stochastic Value Gradients. In Advances in Neural
Information Processing Systems 28, December 7-12, 2015, Montreal, Quebec, Canada, 2015.
Sham Kakade. A Natural Policy Gradient. In Advances in Neural Information Processing Systems
14, December 3-8, 2001, Vancouver, British Columbia, Canada, 2001.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. CoRR,
abs/1412.6980, 2014.
Vijay R. Konda and John N. Tsitsiklis. On Actor-Critic Algorithms. SIAM Journal on Control and
Optimization, 42(4):1143-1166, April 2003.
Sergey Levine and Vladlen Koltun. Guided Policy Search. In Proceedings of the 30th International
Conference on Machine Learning, June 16-21, 2013, Atlanta, GA, USA, 2013.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end Training of Deep
Visuomotor Policies. Journal of Machine Learning Research, 17(1):1334-1373, January 2016.
ISSN 1532-4435.
Weiwei Li and Emanuel Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Bio-
logical Movement Systems. In Proceedings of the 1st International Conference on Informatics in
Control, Automation and Robotics, August 25-28, 2004, Setubal, Portugal,pp. 222-229, 2004.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning. CoRR,
abs/1509.02971, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement
Learning. Nature, 518(7540):529-533, February 2015. ISSN 00280836.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement
Learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
International Conference on Machine Learning, June 19-24, 2016, New York City, NY, USA,
2016.
12
Published as a conference paper at ICLR 2018
Remi Munos, Tom StePleton, Anna Harutyunyan, and Marc G. Bellemare. Safe and Efficient Off-
Policy Reinforcement Learning. In Advances in Neural Information Processing Systems 29, De-
cember 5-10, 2016, Barcelona, Spain, 2016.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the GaP Between
Value and Policy Based Reinforcement Learning. In Advances in Neural Information Processing
Systems 30, 4-9 December 2017, Long Beach, CA, USA, 2017.
Jorge Nocedal and StePhen J. Wright. Numerical Optimization, Second Edition. World Scientific,
2006.
Jan Peters and Stefan Schaal. Natural Actor-Critic. NeurocomPuting,71(7-9):1180-1190, 2008.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative Entropy Policy Search. In Proceedings
of the 24th AAAI Conference on Artificial Intelligence, July 11-15, 2010, Atlanta, Georgia, USA,
2010.
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel. Trust Region
Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning,
July 6-11, 2015, Lille, France, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel.
High-Dimensional Continuous Control Using Generalized Advantage Estimation.	CoRR,
abs/1506.02438, 2015b.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin A. Riedmiller.
Deterministic Policy Gradient Algorithms. In Proceedings of the 31st International Conference
on Machine Learning, June 21-26, 2014, Beijing, China, 2014.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the Game
of Go with Deep Neural Networks and Tree Search. Nature, 529(7587):484-489, 2016.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning - an Introduction. Adaptive
computation and machine learning. MIT Press, 1998.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy Gradi-
ent Methods for Reinforcement Learning with Function Approximation. In Advances in Neural
Information Processing Systems 12, November 29 - December 4, 1999, Colorado, USA, 1999.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A Physics Engine for Model-Based Control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, October 7-12, 2012,
Vilamoura, Algarve, Portugal, 2012.
Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforce-
ment Learning. Machine Learning, 8(3):229-256, 1992. doi: 10.1007/BF00992696.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable Trust-Region
Method for Deep Reinforcement Learning using Kronecker-factored Approximation. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, 4-9 December 2017, Long Beach, CA, USA,
2017.
Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling Interaction via the Principle
of Maximum Causal Entropy. In Proceedings of the 27th International Conference on Machine
Learning, June 21-24, 2010, Haifa, Israel, 2010.
13
Published as a conference paper at ICLR 2018
A Derivations and Proofs
A. 1 Derivation of Solution and Dual Function of Guide Actor
The solution of the optimization problem:
max
π
subject to
Epe (s),∏(a∣s) ∣Qps, aq],
Epe(S) [KL(∏(a∣s)∣∣∏θ(α∣s))]W e,
Epe(s)旧(n(。怕))]》κ,
Epe(s)π(a∣s) = 1,
(37)
can be obtained by the method of Lagrange multipliers. The derivation here follows the derivation of
similar optimization problems by Peters et al. (2010) and Abdolmaleki et al. (2015). The Lagrangian
of this optimization problem is
L(∏,η,ω,ν)= Epe(s),∏(a∣s) ∣Q(s, a)] ' η(e ´ Epe(S) [KL(∏(a∣s)∣∣∏θ(a∣s))S)
' ω(Epe(s) rH(n(a|s))S ´ κq ' V(Epe (s)∏(a|s) ´ 1q,	(38)
where η, ω, and ν are the dual variables. Then, by taking derivative of L w.r.t. πr we obtain
Bπr L “ Epe (S)
Q(s, a)´(n ' ω) log π(ɑ∣s) ' ηlog∏θ(a|s)) da ´ (η ' ω ´ V).
(39)
We set this derivation to zero in order to obtain
0 “ Epe(S)
Qps, a) ´ (η ' ω) log π(α∣s) ' ηlog∏θ(a|s)) da
´ (η ` ω ´ V)
A /	∖	/	∖ 1	〜∕l∖,	1	/ I ∖	/	∖
“ Q(s, a) ´ (η ' ω) log π(a∣s) ' ηlog∏θ(a|s) ´ (η ' ω ´ ν).
(40)
Then the solution is given by
π(a∣s) “ ∏θ(a|s) η'ω exp
(登@) exp (´ η + ω ´ V )
∖ η + ω) η η + ω J
∏θ(a|s) η'ω exp
(Q(s, a)
η+ω
(41)
(42)
9
)
To obtain the dual function g(η, ω), we substitute the solution to the constraint terms of the La-
grangian and this gives us
Lpη, ω, Vq = Epe (s),∏(a∣S)I(P(s, a)]
Q(s, a)	η	η + ω ´ V
G + Klog πθ PaIs)´ ~+ω^
+ ηEpe(s),∏(a∣s) rlog πθ(a|s)s + V (Epe ,∏(a∣s) ´ 1) + % ´ ωκ∙
After some calculation, we obtain
L(η, ω, V) = η ´ ωκ + Epe (S) rη + ω ´ Vs
= ηe ´ ωκ + (η + ω)Epe(S) Ij∏θ(a|s)η⅛ exp (QPsLaq) da
= g(η, ω),
(43)
(44)
where in the second line We use the fact that exp(´ η''JV) is the normalization term of ∏(a∣s).
14
Published as a conference paper at ICLR 2018
A.2 Proof of Second-order Optimization in Action Space
Firstly, we show that GAC performs second-order optimization in the action space when Taylor’s
approximation is performed with a0 “ E∏(a∣s) [a] “ φθ(s). Recall that Taylor's approximation
with φθ is given by
Q(S, aq = 1 aJHΦθ psqa + aJψφθ (sq + ξφθ (s),	(45)
where ψφθ(s) “ VaQ(s, a)∣α=φθ(s) — Hφθ(s)φθ(s). By substituting ψφθ(s) into L(S) “
η*Σj1(s)φθ(s) — ψθ(s), We obtain
L(Sq = n*£e1(s)0e(Sq + VaQ(S, aqla=φθ(s) ´ HΦθ (s)φθ(Sq
1
=(n ςθ (s) — H Φθ (Syq φθ (s) + NaQks aqia=Φθ (S)
=F (s)Φθ (s) + VaQ(S, aq∣α=φθ (s).	(46)
Therefore, the mean update is equivalent to
φ'(Sq = FT(SqL(Sq
=Φθ(s) + F 1(SqVaQ(s, aq∣a=φθ(s),	(47)
which is a second-order optimization step with a curvature matrix F(s) = η*∑G1 (Sq — H力@ (s).
Similarly, for the case where a set of samples {a。}〜 ∏θ(。。⑸ = N(ao∣Φθ(s), Σ(sq) is used to
compute the averaged Taylor’s approximation, we obtain
L(s) = η*∑θ'(s)Φθ (s) + E∏θ (αo∣s) ∣VaQ(s, aq|a“a。] ´ E∏° (a0|s) [H O^ao^] .	(48)
Then, by assuming that E∏0包⑸[Ho(sqao(sqS = E∏θ(a°∣s) [Ho] E∏°(a°∣s) [a。], we obtain
L(s) = η<∑Θ1(s)Φθ(s) + E∏θ(ao∣s) ∣VaQ(s, aq∣α=αo] ´ E∏°(a°|s) [Hθ]Eπθ (a0 |s) [a。]
=η<∑Θ1(s)Φθ(s) + E∏θ(ao|s) ∣VaQ(s, aq|a“a。] ´ E∏0") [Hθ] Φθ(s)
=(η<∑θ1(sq —Eπθ (a0 |s) [H o(sq]qφθ (s) + E∏Θ(ao∣s) IVaQ(s, aq|a=aol
=F(s)Φθ(s) + E∏θ(ao∣s) ∣VaQ(s, aqi。=。。] .	(49)
Therefore, we have a second-order optimization step
Φ'(s) = Φθ(s) + FT(s)E∏θ(ao∣s) ∣VaQ(s, aqi。=。。] ,	(50)
where F-1(Sq = η*∑G1(Sq — E∏θ(。。⑻[Ho(s)] is a curvature matrix. As described in
the main paper, this interpretation is only valid when the equality Eπθ(a0 |s) [H。 (sqa。 (sq] =
E∏θ(ao∣s) [Ho] E∏θ(ao∣s) [a。] holds. While this equality does not hold in general, it holds when
only one sample a。〜∏θ(ao∣sq is used. Nonetheless, we can still use the expectation of Taylor's
approximation to perform policy update regardless of this assumption.
A.3 Proof of Gauss-Newton Approximation
Let f (s, aq = exp(Qp(s, aqq, then the Hessian H(sq = V2aQp(s, aq can be expressed as
H(sq = Va [Va log f (s, aq]
=Va “Vaf (s, aqf(s, aq-1‰
=Vaf (s, aq 'Vaf (s, aq - 1)J + Vtaf (s, a)f(s, a)´1
=Vaf (s, aqVf f (s, aq-1 (Vaf (s, aqqJ + Vtaf (s, a)f(s, a)´1
=—Vaf (S, aqf (s, aq-2 (Vaf (S, aqqJ + Vtaf (s, aqf (s, aq-1
=—'Vaf (S, aqf (s, aq-1)(Va f (S, aqf (s, aq-1)J + Vtaf (s, a)f(s, a)´1
=—Va log f (S, aqVa log f (S, a)J + Vaf (S, aqf (s, a)´1
= —VaQp(s, aqVaQp(s, aqJ + Vta exp(Qp(s, aqq exp(—Qp(s, aqq,	(51)
15
Published as a conference paper at ICLR 2018
which concludes the proof.
Beside Gauss-Newton approximation, an alternative approach is to impose a special structure on
Qps, aq so that Hessians are always negative semi-definite. In literature, there exists two special
structures that satisfies this requirement.
Normalized advantage function (NAF) (Gu et al., 2016): NAF represents the critic by a quadratic
function with a negative curvature:
QNAF(s, a) “ 1 (a ´ b(s)) JW(s)(a ´ b(s)) + V(s),	(52)
where a negative-definite matrix-valued function Wpsq, a vector-valued function bpsq and a scalar-
valued function V(s) are parameterized functions whose their parameters are learned by policy
evaluation methods such as Q-learning (Sutton & Barto, 1998). With NAF, negative definite Hes-
sians can be simply obtained as H(s) “ W (s). However, a significant disadvantage of NAF is that
it assumes the action-value function is quadratic regardless of states and this is generally not true for
most reward functions. Moreover, the Hessians become action-independent even though the critic is
a function of actions.
Input convex neural networks (ICNNs) (Amos et al., 2017): ICNNs are neural networks with
special structures which make them convex w.r.t. their inputs. Since Hessians of concave functions
are always negative semi-definite, we may use ICNNs to represent a negative critic and directly use
its Hessians. However, similarly to NAF, ICNNs implicitly assume that the action-value function is
concave w.r.t. actions regardless of states and this is generally not true for most reward functions.
A.4 Gradient of the Loss Functions for Learning Parameterized Actor
We first consider the weight mean-squared-error loss function where the guide actor is
N(a∣φ'(s), Σ'(s)) and the current actor is N(a∣φθ(s), ∑θ(s)). Taylor's approximation of
Q(s, a) at a0 “ φθ(s) is
Q(S,aq = 1 aJHφθ(s)a + aJψφθ(S) + ξφθ(s).	(53)
By assuming that Hφθ (s) is strictly negative definite5, we can take a derivative of this approxima-
tion w.r.t. a and set it to zero to obtain a = HNl(s)VaQ(s, a) — Hφ1(s)ψφθ (s). Replacing a by
φθ(s) and φ'(s) yields
φθ(s) = H´l(s)VaQ(s, a)lɑ=Φθ(S) — He1(s)Ψφθ(s),	(54)
φ'(s) = H´l(s)VaQ(S, a)∣α=φ'(s) — H´θ(s)Ψφθ(s).	(55)
Recall that the weight mean-squared-error is defined as
LW(θ) = Epe(S) ∣}φθ(S) — Φ'(s)}F(S)I .	(56)
5This can be done by subtracting a small positive value to the diagonal entries of Gauss-Newton approxi-
mation.
16
Published as a conference paper at ICLR 2018
Then, we consider its gradient w.r.t. θ as follows:
VθLW(θ) “ 2Epβ(S) [Vθφθ(S)F(s) 'Φθ(S) ´ φ`(s))‰
=2Epβ(s) [Vθφθ(s)(η*∑e1(S) ´ Hφ° (s)) (φθ(s) ´ φ`(s))‰
“ 2Epβ(s) [Vθφθ(s)(η*∑e1(S) ´ Hφ° (s))
X (He1(s)VaQ(s, a)∣a=φθ(S) ´ Hφ1(s)VaQ(s,叽咒⑶)]
=2η<Epβ (S) [Vθ φθ (s)∑θ1 (S)H ´l(s) ´VaQ(S, a)∣a=φθ (s)´ VaQ(s, a)∣α=φ+(s))l
+ 2Epβ (S) [Vθ φθ (S) (Va Q(s, a)∣α=φ'(s) ´ VaQ(S, a)∣a=φθ ⑶)]
=-2Epe(S) ∣Vθ Φθ (S)VaQ(S, a)∣a=φθ (S)] + 2Epβ (s) ∣Vθ Φθ (S)Va Q(S, a)∣α=φ`(s)]
+ 2η<Epβ(S) [Vθφθ(s)∑T(s)Hφ1(S)VaQ(S, a)∣a=φθ(S) 1
´ 2η<Epβ(S) [Vθφθ(s)∑φ1(s)Hφ1(S)VaQ(S, α)∣a=φ`(S)] .	(57)
This concludes the proof for the gradient in Eq.(28). Note that we should not directly replace
the mean functions in the weight mean-square-error by Eq.(54) and Eq.(55) before expanding the
gradient. This is because the analysis would require computing the gradient w.r.t. θ inside the
Hessians and this is not trivial. Moreover, when we perform gradient descent in practice, the mean
Φ'(s) is considered as a constant w.r.t. θ similarly to an output function in supervised learning.
The gradient for the mean-squared-error can be derived similarly. Let the mean-squared-error be
defined as
LM(θ) = 1 Epe(S) [}Φθ(s) ´ Φ'(s)}2‰ .	(58)
Its gradient w.r.t. θ is given by
VθLM(θ) =
Epe (s) [vΘ φθ (S) 'φθ (S) ´ φ+(s))‰
=Epe(S) [Vθφθ(s)Hφ1(s) (VaQ(s, a)∣a=φθ⑶ ´ V°Q(s,叽血⑹)],(59)
which concludes the proof for the gradient in Eq.(30).
To show that VaQ(s, a)∣a=φ+(S) = 0 when η = 0, we directly substitute η = 0 into φ'(s)=
SςT(S)- HΦθ (S))T "ςT(S)φθ(s) + ψφθ(s))) and this yield
φ+(S) = -H φ1(s)ψφθ(s).	(6O)
Since φ'(s) = Hφ1(s)VaQ(s,。儿^十⑶-Hφ(s)ψφθ (s) from Eq.(55) and the Hessians are
non-zero, it has to be that VaQ(s, a)|a=φ+(S) = 0. ThiS is intuitive since without the KL constraint,
the mean of the guide actor always be at the optima of the second-order Taylor’s approximation and
the gradients are zero at the optima.
A.5 Relation to Q-learning with Normalized Advantage Function
The normalized advantage function (NAF) (Gu et al., 2016) is defined as
QNAF(s, a) = 1 (a - b(s))JW(s)(a - b(s)) + V(s),	(61)
where W(s) is a negative definite matrix. Gu et al. (2016) proposed to perform Q-learning with
NAF by using the fact that argmaxa QpNAF(s, a) = b(s) and maxa QpNAF(s, a) = V (s).
Here, we show that GAC includes the Q-learning method by Gu et al. (2016) as its special case. This
can be shown by using NAF as a critic instead of performing Taylor’s approximation of the critic.
17
Published as a conference paper at ICLR 2018
Firstly, we expand the quadratic term of NAF as follows:
QNAF(s, a) “ 1 (a ´ b(s))JW(s)(a ´ b(s)) + V(S)
“ BaJW(s)a ´ aJ W(s)b(s) + ɪb(s)J W(s)b(s) + V(s)
=2 aJ W (s)a + aJΨ(s) + ξ(s),	(62)
where ψ(s) = —W(s)b(s) and ξ(s) = 2b(s)JW(s)b(s) + V(s). By substituting the quadratic
model obtained by NAF into the GAC framework, the guide actor is now given by ∏(a|s) =
N(a∣Φ'(s),∑'(s))) with
φ'(s) = (η*∑T(s)) — W (S))T(η*∑T(s)Φθ (s) — W (s)b(s))	(63)
Σ'(s) = (η< + ω*)(η*∑T(s)) — W (s)).	(64)
To obtain Q-learning with NAF, we set η< = 0, i.e., we perform a greedy maximization where the
KL upper-bound approaches infinity, and this yields
φ'(s) = —W (S)T(—W (s)b(s))
= b(S),	(65)
which is the policy obtained by performing Q-learning with NAF. Thus, NAF with Q-learning is a
special case of GAC if Q-learning is also used in GAC to learn the critic.
B Pseudo-code of GAC
The pseudo-code of GAC is given in Algorithm 1. The source code is available at https://
github.com/voot-t/guide-actor-critic.
C Experiment details
C.1 Implementation
We try to follow the network architecture proposed by the authors of each baseline method as close as
possible. For GAC and DDPG, we use neural networks with two hidden layers for the actor network
and the critic network. For both networks the first layer has 400 hidden units and the second layer
has 300 units. For NAF, we use neural networks with two hidden layers to represent each of the
functions b(S), W(S) and V(S) where each layer has 200 hidden units. All hidden units use the
relu activation function except for the output of the actor network where we use the tanh activation
function to bound actions. We use the Adam optimizer (Kingma & Ba, 2014) with learning rate
0.001 and 0.0001 for the critic network and the actor network, respectively. The moving average
step for target networks is set to τ = 0.001. The maximum size of the replay buffer is set to
1000000. The mini-batches size is set to N = 256. The weights of the actor and critic networks
are initialized as described by Glorot & Bengio (2010), except for the output layers where the initial
weights are drawn uniformly from U(—0.003, 0.003), as described by Lillicrap et al. (2015). The
initial covariance Σ in GAC is set to be an identity matrix. DDPG and QNAF use the OU-process
with noise parameters θ = 0.15 and σ = 0.2 for exploration .
For TRPO, we use the implementation publicly available at https://github.com/openai/
baselines. We also use the provided network architecture and hyper-parameters except the batch
size where we use 1000 instead of 1024 since this is more suitable in our test setup.
For GAC, the KL upper-bound is fixed to = 0.0001. The entropy lower-bound κ is adjusted
heuristically by
κ = max(0.99(E — E0) + E0, E0),	(71)
where E « EpePsq [H(∏θ(a∣s))S denotes the expected entropy of the current policy and E° denotes
the entropy of a base policy N (a|0, 0.01I). This heuristic ensures that the lower-bound gradually
decreases but the lower-bound cannot be too small. We apply this heuristic update once every 5000
training steps. The dual function is minimize by the sequential least-squares quadratic programming
(SLSQP) method with an initial values η = 0.05 and ω = 0.05. The number of samples for
computing the target critic value is M = 10.
18
Published as a conference paper at ICLR 2018
Algorithm 1 Guide actor critic
1:	Input: Initial actor ∏θ(a|s) “ N(a∣Φθ(s), ∑), critic QV(s, a), target critic network
QV(s, a), KL bound e, entropy bound κ, learning rates ɑι, α2, and data buffer D = 0.
2:	for t “ 1, . . . , Tmax do
3:	procedure COLLECT TRANSITION SAMPLE
4:	Observe state St and sample action at „ N(a∣φθ(st), Σ).
5:	Execute at, receive reward rt and next state s1t .
6:	Add transition t(st, at, rt, s1t)u to D.
7:	end procedure
8:	procedure LEARN
9:	Sample N mini-batch samples t(sn, an, rn, s1n)unN“1 uniformly from D.
10:	procedure UPDATE CRITIC
11:	Sample actions {an,m}M=ι „ N(a∣Φθ(Sn), ∑) for each Sn.
12:	Compute yn update V by, e.g., Adam, and update V by moving average:
1M yn = rn + YM X QV (鼠,an,m), m“1		(66)
1N V D V ´ a1 N X VV (QV (Sn, an) ´ ↑ n“1	Q2,	(67)
V D TV ' (1 — T)ν.		(68)
13:	end procedure
14:	procedure LEARN GUIDE ACTOR
15:	Compute a%。for each Sn by a%。= φθ(Sn) or a%。„ N(a∣φθ(Sn), ∑).
16:	Compute g0(S) = VaQ(Sn, a)|a“an,0 and H 0(Ss) = ´go (Sn)g0(Sn) J.
17:	Solve for (η<, ω<) = argminη>0,ω>0 p(η, ω) by a non-linear optimization method.
18:	Compute the guide actor Π(a∣Sn) = N(a∣φ'(Sn), Σ'(Sn)) for each Sn
19:	end procedure
20:	procedure UPDATE PARAMETERIZED ACTOR
21:	Update policy parameter by, e.g., Adam, to minimize the MSE:
1N
θ D θ ´ α2 N X vθ }φθ (Sn) ´ 0'(Sn)}2.	(69)
n“1
22:	Update policy covariance by averaging the guide covariances:
∑ D N Σ' (Sn).	(70)
23:	end procedure
24:	end procedure
25:	end for
26:	Output: Learned actor πθ(a∣S).
C.2 Environments and Results
We perform experiments on the OpenAI gym platform (Brockman et al., 2016) with Mujoco Physics
simulator (Todorov et al., 2012) where all environments are v1. We use the state space, action space
and the reward function as provided and did not perform any normalization or gradient clipping.
The maximum time horizon in each episode is set to 1000. The discount factor γ = 0.99 is only
used for learning and the test returns are computed without it.
Experiments are repeated for 10 times with different random seeds. The total computation time are
reported in Table 1. The figures below show the results averaged over 10 trials. The y-axis indicates
the averaged test returns where the test returns in each trial are computed once every 5000 training
time steps by executing 10 test episodes without exploration. The error bar indicates standard error.
19
Published as a conference paper at ICLR 2018
Table 1: The total computation time for training the policy for 1 million steps (0.1 million steps for
the Invert-Pendulum task). The mean and standard error are computed over 10 trials with the unit in
hours. TRPO is not included since it performs a lesser amount of update using batch data samples.
Task		GAC-1	GAC-0	DDPG	QNAF
InV-Pend.	1.13(0.09)	0.80(0.04)	0.45(0.02)	0.40(0.03)
InV-DoUble-Pend.	15.56(0.93)	15.67(0.77)	9.04(0.29)	7.47(0.22)
Reacher	17.23(0.87)	12.64(0.38)	10.67(0.37)	30.91(2.09)
Swimmer	12.43(0.74)	11.94(0.74)	9.61(0.52)	32.44(2.21)
Half-Cheetah	29.82(1.76)	32.64(1.41)	10.13(0.41)	27.94(2.37)
Ant	37.84(1.80)	40.57(3.06)	9.75(0.37)	27.09(0.94)
Hopper	18.99(1.06)	14.22(0.56)	8.74(0.45)	26.48(1.42)
Walker2D	33.42(0.96)	31.71(1.84)	7.92(0.11)	26.94(1.99)
Humanoid	111.07(2.99)	106.80(6.80)	13.90(1.60)	30.43(2.21)
EmaJ P①6e」①>4
Figure 2: Performance averaged over 10 trials on the Inverted Pendulum task.
20
Published as a conference paper at ICLR 2018
8000-
6000-
4000-
2000-
200	400	600	800	1000
Time steps (xl000)
Figure 3: Performance averaged over 10 trials on the Inverted Double Pendulum task.
0-
0
En4aJ ^φσ2φ><
Time steps (xlOOO)
Figure 4: Performance averaged over 10 trials on the Reacher task.
En4aJ ^φσ2φ><
21
Published as a conference paper at ICLR 2018
——GAC-I
——GAC-O
——DDPG
——QNAF
——TRPO
Ooooooo
2 0 8 6 4 2
ɪ ɪ
Ernal ^φσ2φ><
O
O
O
O
'O
8
O
'O
6
O
40
O
'O
2
'O
Time steps (xl000)
Figure 5:	Performance averaged over 10 trials on the Swimmer task.
En4aJ ^φσ2φ><
6000-
5000-
4000-
3000-
2000-
IOOO-
O-
O 200	400	600	800 IOOO
Time steps (xlOOO)
Figure 6:	Performance averaged over 10 trials on the Half-Cheetah task.
22
Published as a conference paper at ICLR 2018
En4aJ ^φσ2φ><
Figure 7: Performance averaged over 10 trials on the Ant task.
En4aJ ^φσ2φ><
3500-
3000-
2500-
2000-
1500-
1000-
500-
0-
0	200	400	600	800	10
Time steps (xl000)
Figure 8: Performance averaged over 10 trials on the Hopper task.
O
O
23
Published as a conference paper at ICLR 2018
En4aJ ^φσ2φ><

4000-
3500-
3000-
2500-
2000-
1500-
IOOO-
500-
O-
O 200	400	600	800 IOOO
Time steps (xl000)
Figure 9:	Performance averaged over 10 trials on the Walker2D task.
1000
——GAC-I
2000- --GAC'0
---DDPG
——QNAF
——TRPO
1500
__l________I____________I_________I__________I__________
0	200	400	600	800	1000
Time steps (xl000)
Figure 10:	Performance averaged over 10 trials on the Humanoid task.
24