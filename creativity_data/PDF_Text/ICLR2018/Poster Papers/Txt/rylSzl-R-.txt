Published as a conference paper at ICLR 2018
On Unifying Deep Generative Models
Zhiting Hu1,2	Zichao Yang1 Ruslan Salakhutdinov1	Eric P. Xing1,2
Carnegie Mellon University1, Petuum Inc.2
Ab stract
Deep generative models have achieved impressive success in recent years. Gen-
erative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as
powerful frameworks for deep generative model learning, have largely been con-
sidered as two distinct paradigms and received extensive independent studies
respectively. This paper aims to establish formal connections between GANs and
VAEs through a new formulation of them. We interpret sample generation in
GANs as performing posterior inference, and show that GANs and VAEs involve
minimizing KL divergences of respective posterior and inference distributions
with opposite directions, extending the two learning phases of classic wake-sleep
algorithm, respectively. The unified view provides a powerful tool to analyze a
diverse set of existing model variants, and enables to transfer techniques across
research lines in a principled way. For example, we apply the importance weighting
method in VAE literatures for improved GAN learning, and enhance VAEs with
an adversarial mechanism that leverages generated samples. Experiments show
generality and effectiveness of the transfered techniques.
1	Introduction
Deep generative models define distributions over a set of variables organized in multiple layers. Early
forms of such models dated back to works on hierarchical Bayesian models (Neal, 1992) and neural
network models such as Helmholtz machines (Dayan et al., 1995), originally studied in the context of
unsupervised learning, latent space modeling, etc. Such models are usually trained via an EM style
framework, using either a variational inference (Jordan et al., 1999) or a data augmentation (Tanner
& Wong, 1987) algorithm. Of particular relevance to this paper is the classic wake-sleep algorithm
dates by Hinton et al. (1995) for training Helmholtz machines, as it explored an idea of minimizing a
pair of KL divergences in opposite directions of the posterior and its approximation.
In recent years there has been a resurgence of interests in deep generative modeling. The emerging
approaches, including Variational Autoencoders (VAEs) (Kingma & Welling, 2013), Generative
Adversarial Networks (GANs) (Goodfellow et al., 2014), Generative Moment Matching Networks
(GMMNs) (Li et al., 2015; Dziugaite et al., 2015), auto-regressive neural networks (Larochelle
& Murray, 2011; Oord et al., 2016), and so forth, have led to impressive results in a myriad of
applications, such as image and text generation (Radford et al., 2015; Hu et al., 2017; van den Oord
et al., 2016), disentangled representation learning (Chen et al., 2016; Kulkarni et al., 2015), and
semi-supervised learning (Salimans et al., 2016; Kingma et al., 2014).
The deep generative model literature has largely viewed these approaches as distinct model training
paradigms. For instance, GANs aim to achieve an equilibrium between a generator and a discrimi-
nator; while VAEs are devoted to maximizing a variational lower bound of the data log-likelihood.
A rich array of theoretical analyses and model extensions have been developed independently for
GANs (Arjovsky & Bottou, 2017; Arora et al., 2017; Salimans et al., 2016; Nowozin et al., 2016)
and VAEs (Burda et al., 2015; Chen et al., 2017; Hu et al., 2017), respectively. A few works
attempt to combine the two objectives in a single model for improved inference and sample gener-
ation (Mescheder et al., 2017; Larsen et al., 2015; Makhzani et al., 2015; S0nderby et al., 2017).
Despite the significant progress specific to each method, it remains unclear how these apparently
divergent approaches connect to each other in a principled way.
In this paper, we present a new formulation of GANs and VAEs that connects them under a unified
view, and links them back to the classic wake-sleep algorithm. We show that GANs and VAEs
1
Published as a conference paper at ICLR 2018
involve minimizing opposite KL divergences of respective posterior and inference distributions, and
extending the sleep and wake phases, respectively, for generative model learning. More specifically,
we develop a reformulation of GANs that interprets generation of samples as performing posterior
inference, leading to an objective that resembles variational inference as in VAEs. As a counterpart,
VAEs in our interpretation contain a degenerated adversarial mechanism that blocks out generated
samples and only allows real examples for model training.
The proposed interpretation provides a useful tool to analyze the broad class of recent GAN- and VAE-
based algorithms, enabling perhaps a more principled and unified view of the landscape of generative
modeling. For instance, one can easily extend our formulation to subsume InfoGAN (Chen et al.,
2016) that additionally infers hidden representations of examples, VAE/GAN joint models (Larsen
et al., 2015; Che et al., 2017a) that offer improved generation and reduced mode missing, and adver-
sarial domain adaptation (ADA) (Ganin et al., 2016; Purushotham et al., 2017) that is traditionally
framed in the discriminative setting.
The close parallelisms between GANs and VAEs further ease transferring techniques that were
originally developed for improving each individual class of models, to in turn benefit the other
class. We provide two examples in such spirit: 1) Drawn inspiration from importance weighted
VAE (IWAE) (Burda et al., 2015), we straightforwardly derive importance weighted GAN (IWGAN)
that maximizes a tighter lower bound on the marginal likelihood compared to the vanilla GAN. 2)
Motivated by the GAN adversarial game we activate the originally degenerated discriminator in
VAEs, resulting in a full-fledged model that adaptively leverages both real and fake examples for
learning. Empirical results show that the techniques imported from the other class are generally
applicable to the base model and its variants, yielding consistently better performance.
2	Related Work
There has been a surge of research interest in deep generative models in recent years, with remarkable
progress made in understanding several class of algorithms. The wake-sleep algorithm (Hinton et al.,
1995) is one of the earliest general approaches for learning deep generative models. The algorithm
incorporates a separate inference model for posterior approximation, and aims at maximizing a
variational lower bound of the data log-likelihood, or equivalently, minimizing the KL divergence
of the approximate posterior and true posterior. However, besides the wake phase that minimizes
the KL divergence w.r.t the generative model, the sleep phase is introduced for tractability that
minimizes instead the reversed KL divergence w.r.t the inference model. Recent approaches such as
NVIL (Mnih & Gregor, 2014) and VAEs (Kingma & Welling, 2013) are developed to maximize the
variational lower bound w.r.t both the generative and inference models jointly. To reduce the variance
of stochastic gradient estimates, VAEs leverage reparametrized gradients. Many works have been
done along the line of improving VAEs. Burda et al. (2015) develop importance weighted VAEs to
obtain a tighter lower bound. As VAEs do not involve a sleep phase-like procedure, the model cannot
leverage samples from the generative model for model training. Hu et al. (2017) combine VAEs with
an extended sleep procedure that exploits generated samples for learning.
Another emerging family of deep generative models is the Generative Adversarial Networks
(GANs) (Goodfellow et al., 2014), in which a discriminator is trained to distinguish between real and
generated samples and the generator to confuse the discriminator. The adversarial approach can be
alternatively motivated in the perspectives of approximate Bayesian computation (Gutmann et al.,
2014) and density ratio estimation (Mohamed & Lakshminarayanan, 2016). The original objective of
the generator is to minimize the log probability of the discriminator correctly recognizing a generated
sample as fake. This is equivalent to minimizing a lower bound on the Jensen-Shannon divergence
(JSD) of the generator and data distributions (Goodfellow et al., 2014; Nowozin et al., 2016; Huszar,
2016; Li, 2016). Besides, the objective suffers from vanishing gradient with strong discriminator.
Thus in practice people have used another objective which maximizes the log probability of the
discriminator recognizing a generated sample as real (Goodfellow et al., 2014; Arjovsky & Bottou,
2017). The second objective has the same optimal solution as with the original one. We base our
analysis of GANs on the second objective as it is widely used in practice yet few theoretic analysis
has been done on it. Numerous extensions of GANs have been developed, including combination
with VAEs for improved generation (Larsen et al., 2015; Makhzani et al., 2015; Che et al., 2017a),
and generalization of the objectives to minimize other f-divergence criteria beyond JSD (Nowozin
2
Published as a conference paper at ICLR 2018
et al., 2016; S0nderby et al., 2017). The adversarial principle has gone beyond the generation setting
and been applied to other contexts such as domain adaptation (Ganin et al., 2016; Purushotham et al.,
2017), and Bayesian inference (Mescheder et al., 2017; Tran et al., 2017; Huszar, 2017; Rosca et al.,
2017) which uses implicit variational distributions in VAEs and leverage the adversarial approach for
optimization. This paper starts from the basic models of GANs and VAEs, and develops a general
formulation that reveals underlying connections of different classes of approaches including many of
the above variants, yielding a unified view of the broad set of deep generative modeling.
3	Bridging the Gap
The structures of GANs and VAEs are at the first glance quite different from each other. VAEs are
based on the variational inference approach, and include an explicit inference model that reverses
the generative process defined by the generative model. On the contrary, in traditional view GANs
lack an inference model, but instead have a discriminator that judges generated samples. In this
paper, a key idea to bridge the gap is to interpret the generation of samples in GANs as performing
inference, and the discrimination as a generative process that produces real/fake labels. The resulting
new formulation reveals the connections of GANs to traditional variational inference. The reversed
generation-inference interpretations between GANs and VAEs also expose their correspondence to
the two learning phases in the classic wake-sleep algorithm.
For ease of presentation and to establish a systematic notation for the paper, we start with a new
interpretation of Adversarial Domain Adaptation (ADA) (Ganin et al., 2016), the application of
adversarial approach in the domain adaptation context. We then show GANs are a special case of
ADA, followed with a series of analysis linking GANs, VAEs, and their variants in our formulation.
3.1	Adversarial Domain Adaptation (ADA)
ADA aims to transfer prediction knowledge learned from a source domain to a target domain, by
learning domain-invariant features (Ganin et al., 2016). That is, it learns a feature extractor whose
output cannot be distinguished by a discriminator between the source and target domains.
We first review the conventional formulation of ADA. Figure 1(a) illustrates the computation flow.
Let z be a data example either in the source or target domain, and y ∈ {0, 1} the domain indicator
with y = 0 indicating the target domain and y = 1 the source domain. The data distributions
conditioning on the domain are then denoted as p(z|y). The feature extractor Gθ parameterized with
θ maps z to feature x = Gθ(z). To enforce domain invariance of feature x, a discriminator Dφ is
learned. Specifically, Dφ(x) outputs the probability that x comes from the source domain, and the
discriminator is trained to maximize the binary classification accuracy of recognizing the domains:
maχφ Lφ = Eχ = Gθ (z),z 〜p(z∣y=1) [log Dφ(X)] + Ex = Gθ(z),z 〜p(z∣y = 0) [log(I - Dφ(X))] .
The feature extractor Gθ is then trained to fool the discriminator:
maχθ Lθ = Eχ = Gθ (Z) ,z^p(z∣y = 1) [log(1 - Dφ (X))] + Ex = Gθ (Z) ,z〜p(z|y = 0) [log Dφ (X)] .
(1)
(2)
Please see the supplementary materials for more details of ADA.
With the background of conventional formulation, we now frame our new interpretation of ADA. The
data distribution p(z|y) and deterministic transformation Gθ together form an implicit distribution
over x, denoted as pθ(x|y), which is intractable to evaluate likelihood but easy to sample from. Let
p(y) be the distribution of the domain indicator y, e.g., a uniform distribution as in Eqs.(1)-(2). The
discriminator defines a conditional distribution qφ(y∣x) = Dφ(x). Let qφ(y|x) = qφ(1 - y|x) be
the reversed distribution over domains. The objectives of ADA are therefore rewritten as (omitting
the constant scale factor 2):
maxφ Lφ = Epθ (x|y)p(y) [log qφ (y |X)]
maxθ Lθ = Epθ(x∣y)p(y) [log qφ(y|x)].
(3)
Note that Z is encapsulated in the implicit distribution pθ (x|y). The only difference of the objectives
of θ from φ is the replacement of q(y|x) with qr(y|x). This is where the adversarial mechanism
comes about. We defer deeper interpretation of the new objectives in the next subsection.
3
qη(z∖x,y)
qη{z∖x,y)
∙^data
(C)
(d)
(e)
feature
(a)
code data/gen
(b)

Published as a conference paper at ICLR 2018
pθ0(χ∖y = 1= pda,ta(χ)	pθ0(χ∖y = 0= p3θo(χ)
qT(x\y=&J ∖	/	'∖	/ ∖	Pθ0ew(Xly = 0)= %/ew (W)
X	ʃ:m
Figure 2: One optimization step of the parameter θ through Eq.(6) at point θ0. The posterior qr (x∖y) is a
mixture of pe。(x∖y = 0) (blue) and p®。(x∖y = 1) (red in the left panel) with the mixing weights induced
from qφo(y∖x). Minimizing the KLD drives pe(x∖y = 0) towards the respective mixture qr(x∖y = 0)
(green), resulting in a new state where penew (x∖y = 0) = pg§new (x) (red in the right panel) gets closer to
pθo (x∖y = 1) = Pdata (x). Due to the asymmetry of KLD, pgθnew (x) missed the smaller mode of the mixture
qr (x∖y = 0) which is a mode of Pdata (x).
Lemma 1. Let p(y) be the uniform distribution. Let pg。(x) = Ep(y) [pθo (x|y)], and qr (x|y) H
qro (y∣x)pθo (x). Therefore, the updates of θ at θo have
▽e [一 Epθ(x∣y)p(y) [log qφ0 (y∖x)]]l o =
θ = θ0
▽e[Ep(y)[KL (pe(x∖y)∣∣qr(x∖y))] — JSD (pe(χ∖y = 0)∣∣Pθ(x∖y = 1)) i[_e ,
where KL(∙∣∣∙) and JSD(∙∣∣∙) are the KL and Jensen-Shannon Divergences, respectively.
(6)
Proofs are in the supplements (sec.B). Eq.(6) offers several insights into the GAN generator learning:
•	Resemblance to variational inference. As above, We see X as latent and pg(x|y) as the inference
distribution. The pg。(x) is fixed to the starting state of the current update step, and can naturally be
seen as the prior over x. By definition qr(x|y) that combines the prior pg。 (x) and the generative
distribution qφr (y|x) thus serves as the posterior. Therefore, optimizing the generator Gg is
equivalent to minimizing the KL divergence between the inference distribution and the posterior
(a standard from of variational inference), minus a JSD between the distributions pgθ (x) and
pdata(x). The interpretation further reveals the connections to VAEs, as discussed later.
•	Training dynamics. By definition, pg。 (x) = (pgθ (x)+pdata(x))/2 is a mixture of pgθ (x) and
pdata(x) with uniform mixing weights, so the posterior qr(x|y) H qφr (y|x)pg。 (x) is also a mix-
ture of pgθ (x) andpdata(x) with mixing weights induced from the discriminator qφr (y|x). For
the KL divergence to minimize, the component with y = 1 is KL (pg(x|y = 1)kqr(x|y = 1)) =
KL (pdata(x)kqr(x|y = 1)) which is a constant. The active component for optimization is with
y = 0, i.e., KL (pg(x|y = 0)kqr(x|y = 0)) = KL 跖(X)Ilqr(x|y = 0)). Thus, minimizing the
KL divergence in effect drives pgθ (x) to a mixture of pgθ (x) and pdata(x). Since pdata(x) is
fixed, pgθ (X) gets closer topdata(X). Figure 2 illustrates the training dynamics schematically.
•	The JSD term. The negative JSD term is due to the introduction of the prior pg。 (X). This term
pushes pgθ (X) away from pdata(X), which acts oppositely from the KLD term. However, we
show that the JSD term is upper bounded by the KLD term (sec.C). Thus, if the KLD term is
sufficiently minimized, the magnitude of the JSD also decreases. Note that we do not mean the
JSD is insignificant or negligible. Instead conclusions drawn from Eq.(6) should take the JSD term
into account.
•	Explanation of missing mode issue. JSD is a symmetric divergence measure while KLD is
non-symmetric. The missing mode behavior widely observed in GANs (Metz et al., 2017; Che
et al., 2017a) is thus explained by the asymmetry of the KLD which tends to concentratepg(X|y)
to large modes of qr(X|y) and ignore smaller ones. See Figure 2 for the illustration. Concentration
to few large modes also facilitates GANs to generate sharp and realistic samples.
•	Optimality assumption of the discriminator. Previous theoretical works have typically assumed
(near) optimal discriminator (Goodfellow et al., 2014; Arjovsky & Bottou, 2017):
(y∖χ)〜	Pθ0 (X♦ = 1)	=	Pdata(X)
0	Pθo (χ∖y = 0) + Peo (χ∖y = ι)	Pge。(χ) + Pdata(X)
(7)
which can be unwarranted in practice due to limited expressiveness of the discriminator (Arora
et al., 2017). In contrast, our result does not rely on the optimality assumptions. Indeed, our result
is a generalization of the previous theorem in (Arjovsky & Bottou, 2017), which is recovered by
5
Published as a conference paper at ICLR 2018
plugging Eq.(7) into Eq.(6):
▽e [ - Epθ (x|y)p(y) [logqφο(yIx)]][ a = ▽e
θ=θ0
1KL (pgθ kPdata) - JSD (pgθ kPdata) I ,	(8)
2	θ=θ0
which gives simplified explanations of the training dynamics and the missing mode issue only when
the discriminator meets certain optimality criteria. Our generalized result enables understanding
of broader situations. For instance, when the discriminator distribution qφ0 (y|x) gives uniform
guesses, or when pgθ = pdata that is indistinguishable by the discriminator, the gradients of the
KL and JSD terms in Eq.(6) cancel out, which stops the generator learning.
InfoGAN Chen et al. (2016) developed InfoGAN which additionally recovers (part of) the latent
code z given sample x. This can straightforwardly be formulated in our framework by introducing an
extra conditional qη(z|x, y) parameterized by η. As discussed above, GANs assume a degenerated
code space for real examples, thus qη∣(z|x, y = 1) is fixed without free parameters to learn, and η is
only associated to y = 0. The InfoGAN is then recovered by combining qη(z|x, y) with qφ(y∣x) in
Eq.(3) to perform full reconstruction of both z and y:
maxφLφ = Epθ(x∣y)p(y) [logqη(z∣x,y)qφ(y∣x)]
maxθ,η Le,η = Ep@(x∣y)p(y) [log Qη(z∣x,y)qφ(y∣x)].
(9)
Again, note that Z is encapsulated in the implicit distribution pθ(x|y). The model is expressed as
the schematic graphical model in Figure 1(d). Let qr (x|z, y) a q砒 (z|x, y)qφo (y∣x)pθ0 (x) be the
augmented “posterior”, the result in the form of Lemma.1 still holds by adding z-related conditionals:
▽e
- Epθ (x|y)p(y) [log qη0 (z|x, y)qφr0 (y|x)] II	=
θ=θ0
▽e ∣Ep(y) [kl (pe(χ∣y)∣∣qr(x∣z,y))] — JSD (pe(x∣y = 0)∣∣pθ(x∣y = 1))
(10)
The new formulation is also generally applicable to other GAN-related variants, such as Adversar-
ial Autoencoder (Makhzani et al., 2015), Predictability Minimization (Schmidhuber, 1992), and
cycleGAN (Zhu et al., 2017). In the supplements we provide interpretations of the above models.
3.3	Variational Autoencoders (VAEs)
We next explore the second family of deep generative modeling. The resemblance of GAN generator
learning to variational inference (Lemma.1) suggests strong relations between VAEs (Kingma &
Welling, 2013) and GANs. We build correspondence between them, and show that VAEs involve
minimizing a KLD in an opposite direction, with a degenerated adversarial discriminator.
The conventional definition of VAEs is written as:
maxθ,η Lean = Epdata(X) IEqn (z|x) [log pθ(x|z)] ― KL(qη (ZIx)kp(Z))i ,
(11)
wherepθ(x|z) is the generator, Gn(z|x) the inference model, andp(z) the prior. The parameters to
learn are intentionally denoted with the notations of corresponding modules in GANs. VAEs appear
to differ from GANs greatly as they use only real examples and lack adversarial mechanism.
To connect to GANs, we assume a perfect discriminator q*(y∣x) which always predicts y = 1
with probability 1 given real examples, and y = 0 given generated samples. Again, for notational
simplicity, let qj(y∣x) = q*(1 — y|x) be the reversed distribution.
Lemma 2. Letpθ(z, y|x) a pθ (x|z, y)p(z |y)p(y). The VAE objective Lvθa,ηe in Eq.(11) is equivalent
to (omitting the constant scale factor 2):
Lvθen = Epθ0(X) IEqn(z∣χ,y)qr(y|x) [logPe(x|z,y)] ― KL ⑷(zlx,yMr(yIx)IIp(ZIy)p(y川
Epθ0 (X) [ — KL (qn (z∖x,y)qrr (y∣x)∣∣pe (z,y∣x))].
(12)
Here most of the components have exact correspondences (and the same definitions) in GANs and
InfoGAN (see Table 1), except that the generation distribution pθ(x|z, y) differs slightly from its
6
Published as a conference paper at ICLR 2018
Components	ADA	GANs / InfoGAN	VAEs
x	features	data/generations	data/generations
y	domain indicator	real/fake indicator	real/fake indicator (degenerated)
z	data examples	code vector	code vector
pθ (x|y)	feature distr.	[I] generator, Eq.4	[G] Pθ(x∣z,y), generator, Eq.13
qφ(y∣χ)	discriminator	[G] discriminator	[I] q* (y|x), discriminator (degenerated)
qη (z∣χ,y)	—	[G] infer net (InfoGAN)	[I] infer net
KLD to min	same as GANs	KL (pθ(x∣y)kqr(x∣y))	KL (qη(z∣x,y)qr(y∣x)∣∣pθ(z,y∣x))
Table 1: Correspondence between different approaches in the proposed formulation. The label “[G]” in bold
indicates the respective component is involved in the generative process within our interpretation, while “[I]”
indicates inference process. This is also expressed in the schematic graphical models in Figure 1.
counterpart pθ (x|y) in Eq.(4) to additionally account for the uncertainty of generating x given z:
Pθ(χ∣z,y) = (pθ(x义 y = 0	(13)
pdata (x) y = 1.
We provide the proof of Lemma 2 in the supplementary materials. Figure 1(e) shows the schematic
graphical model of the new interpretation of VAEs, where the only difference from InfoGAN
(Figure 1(d)) is swapping the solid-line arrows (generative process) and dashed-line arrows (inference).
As in GANs and InfoGAN, for the real example domain with y = 1, both qη(z∣x,y = 1) and
Pθ(x|z, y = 1) are constant distributions. Since given a fake sample X from pg。(x), the reversed
perfect discriminator q；(y|x) always predicts y = 1 with probability 1, the loss on fake samples is
therefore degenerated to a constant, which blocks out fake samples from contributing to learning.
3.4	Connecting GANs and VAEs
Table 1 summarizes the correspondence between the approaches. Lemma.1 and Lemma.2 have
revealed that both GANs and VAEs involve minimizing a KLD of respective inference and posterior
distributions. In particular, GANs involve minimizing the KL pp((x∣y)∣∣qr (x∣y)) while VAEs the
KL qqη(z|x, y)qr (y∣x)∣∣Pθ(z, y∣x)). This exposes several new connections between the two model
classes, each of which in turn leads to a set of existing research, or can inspire new research directions:
1)	As discussed in Lemma.1, GANs now also relate to the variational inference algorithm as with
VAEs, revealing a unified statistical view of the two classes. Moreover, the new perspective
naturally enables many of the extensions of VAEs and vanilla variational inference algorithm to be
transferred to GANs. We show an example in the next section.
2)	The generator parameters θ are placed in the opposite directions in the two KLDs. The asymmetry
of KLD leads to distinct model behaviors. For instance, as discussed in Lemma.1, GANs are
able to generate sharp images but tend to collapse to one or few modes of the data (i.e., mode
missing). In contrast, the KLD of VAEs tends to drive generator to cover all modes of the data
distribution but also small-density regions (i.e., mode covering), which usually results in blurred,
implausible samples. This naturally inspires combination of the two KLD objectives to remedy
the asymmetry. Previous works have explored such combinations, though motivated in different
perspectives (Larsen et al., 2015; Che et al., 2017a; Pu et al., 2017). We discuss more details in the
supplements.
3)	VAEs within our formulation also include an adversarial mechanism as in GANs. The discriminator
is perfect and degenerated, disabling generated samples to help with learning. This inspires
activating the adversary to allow learning from samples. We present a simple possible way in the
next section.
4)	GANs and VAEs have inverted latent-visible treatments of (z, y) and x, since we interpret sample
generation in GANs as posterior inference. Such inverted treatments strongly relates to the
symmetry of the sleep and wake phases in the wake-sleep algorithm, as presented shortly. In sec.6,
we provide a more general discussion on a symmetric view of generation and inference.
3.5	Connecting to Wake Sleep Algorithm (WS)
Wake-sleep algorithm (Hinton et al., 1995) was proposed for learning deep generative models such
as Helmholtz machines (Dayan et al., 1995). WS consists of wake phase and sleep phase, which
7
Published as a conference paper at ICLR 2018
optimize the generative model and inference model, respectively. We follow the above notations, and
introduce new notations h to denote general latent variables and λ to denote general parameters. The
wake sleep algorithm is thus written as:
Wake : maxθ Eqλ(h∣x)pdata(x) [logPθ(x|h)]
Sleep ：	maxλ Epθ(χ∣h)p(h) [log qλ(h∣x)].
(14)
Briefly, the wake phase updates the generator parameters θ by fitting pθ(x|h) to the real data and
hidden code inferred by the inference model qλ(h∣x). On the other hand, the sleep phase updates the
parameters λ based on the generated samples from the generator.
The relations between WS and VAEs are clear in previous discussions (Bornschein & Bengio, 2014;
Kingma & Welling, 2013). Indeed, WS was originally proposed to minimize the variational lower
bound as in VAEs (Eq.11) with the sleep phase approximation (Hinton et al., 1995). Alternatively,
VAEs can be seen as extending the wake phase. Specifically, if we let h be z and λ be η, the
wake phase objective recovers VAEs (Eq.11) in terms of generator optimization (i.e., optimizing θ).
Therefore, we can see VAEs as generalizing the wake phase by also optimizing the inference model
qη , with additional prior regularization on code z .
On the other hand, GANs closely resemble the sleep phase. To make this clearer, let h be y and λ
be φ. This results in a sleep phase objective identical to that of optimizing the discriminator qφ in
Eq.(3), which is to reconstruct y given sample x. We thus can view GANs as generalizing the sleep
phase by also optimizing the generative model pθ to reconstruct reversed y. InfoGAN (Eq.9) further
extends the correspondence to reconstruction of latents z.
4	Transferring Techniques
The new interpretation not only reveals the connections underlying the broad set of existing ap-
proaches, but also facilitates to exchange ideas and transfer techniques across the two classes of
algorithms. For instance, existing enhancements on VAEs can straightforwardly be applied to improve
GANs, and vice versa. This section gives two examples. Here we only outline the main intuitions
and resulting models, while providing the details in the supplement materials.
4.1	Importance Weighted GANs (IWGAN)
Burda et al. (2015) proposed importance weighted autoencoder (IWAE) that maximizes a tighter
lower bound on the marginal likelihood. Within our framework it is straightforward to develop
importance weighted GANs by copying the derivations of IWAE side by side, with little adaptations.
Specifically, the variational inference interpretation in Lemma.1 suggests GANs can be viewed as
maximizing a lower bound of the marginal likelihood on y (putting aside the negative JSD term):
log q(y)
log
/ pθ (x|y)
qφ 0 (UIx)Pθo(X)
pθ (x|y)
dx ≥ -KL(pθ (x|y)kqr (x|y)) + const.
(15)
Following (Burda et al., 2015), we can derive a tighter lower bound through a k-sample importance
weighting estimate of the marginal likelihood. With necessary approximations for tractability,
optimizing the tighter lower bound results in the following update rule for the generator learning:
VθLk(y) = Ezι,...,zk^p(z∣y) Xk=I ffffVθ logqφ0(y∣x(zi, θ))] .	(16)
As in GANs, only y = 0 (i.e., generated samples) is effective for learning parameters θ. Compared
to the vanilla GAN update (Eq.(6)), the only difference here is the additional importance weight wfi
qφr (y|xi )
which is the normalization of Wi = qφ ⑻⑦.)over k samples. Intuitively, the algorithm assigns higher
weights to samples that are more realistic and fool the discriminator better, which is consistent to
IWAE that emphasizes more on code states providing better reconstructions. Hjelm et al. (2017);
Che et al. (2017b) developed a similar sample weighting scheme for generator training, while their
generator of discrete data depends on explicit conditional likelihood. In practice, the k samples
correspond to sample minibatch in standard GAN update. Thus the only computational cost added by
the importance weighting method is by evaluating the weight for each sample, and is negligible. The
discriminator is trained in the same way as in standard GANs.
8
Published as a conference paper at ICLR 2018
	GAN	IWGAN
MNIST	8.34±.03	8.45±.04
SVHN	5.18±.03	5.34±.03
CIFAR10	7.86±.05	7.89± .04
	CGAN	IWCGAN	SVAE		AASVAE
MNIST	0.985±.002	0.987±.002	1%	0.9412	0.9425
SVHN	0.797±.005	0.798±.006	10%	0.9768	0.9797
Table 2: Left: Inception scores of GANs and the importance weighted extension. Middle: Classification
accuracy of the generations by conditional GANs and the IW extension. Right: Classification accuracy of
semi-supervised VAEs and the AA extension on MNIST test set, with 1% and 10% real labeled training data.
Train Data Size	VAE	AA-VAE	CVAE	AA-CVAE	SVAE	AA-SVAE
1%	-122.89	-122.15	-125.44	-122.88	-108.22	-107.61
10%	-104.49	-103.05	-102.63	-101.63	-99.44	-98.81
100%	-92.53	-92.42	-93.16	-92.75	—	—
Table 3: Variational lower bounds on MNIST test set, trained on 1%, 10%, and 100% training data, respectively.
In the semi-supervised VAE (SVAE) setting, remaining training data are used for unsupervised training.
4.2	Adversary Activated VAEs (AAVAE)
By Lemma.2, VAEs include a degenerated discriminator which blocks out generated samples from
contributing to model learning. We enable adaptive incorporation of fake samples by activating the
adversarial mechanism. Specifically, We replace the perfect discriminator q*(y∣x) in VAEs with a
discriminator network qφ(y∣x) parameterized with φ, resulting in an adapted objective of Eq.(12):
maxLθavae = Epθ0(X)IEqη(z∣x,y)qφ(y∣x) [logPθ(x∣z,y)] - KL(qη(z∣x,y)qφ(y∣x)kp(z∣y)p(y))] .	(17)
As detailed in the supplementary material, the discriminator is trained in the same way as in GANs.
The activated discriminator enables an effective data selection mechanism. First, AAVAE uses not
only real examples, but also generated samples for training. Each sample is weighted by the inverted
discriminator qφr (y|x), so that only those samples that resemble real data and successfully fool the
discriminator will be incorporated for training. This is consistent with the importance weighting
strategy in IWGAN. Second, real examples are also weighted by qφr (y|x). An example receiving
large weight indicates it is easily recognized by the discriminator, which means the example is hard
to be simulated from the generator. That is, AAVAE emphasizes more on harder examples.
5	Experiments
We conduct preliminary experiments to demonstrate the generality and effectiveness of the importance
weighting (IW) and adversarial activating (AA) techniques. In this paper we do not aim at achieving
state-of-the-art performance, but leave it for future work. In particular, we show the IW and AA
extensions improve the standard GANs and VAEs, as well as several of their variants, respectively.
We present the results here, and provide details of experimental setups in the supplements.
5.1	Importance Weighted GANs
We extend both vanilla GANs and class-conditional GANs (CGAN) with the IW method. The base
GAN model is implemented with the DCGAN architecture and hyperparameter setting (Radford et al.,
2015). Hyperparameters are not tuned for the IW extensions. We use MNIST, SVHN, and CIFAR10
for evaluation. For vanilla GANs and its IW extension, we measure inception scores (Salimans et al.,
2016) on the generated samples. For CGANs we evaluate the accuracy of conditional generation (Hu
et al., 2017) with a pre-trained classifier. Please see the supplements for more details.
Table 2, left panel, shows the inception scores of GANs and IW-GAN, and the middle panel gives the
classification accuracy of CGAN and and its IW extension. We report the averaged results ± one
standard deviation over 5 runs. The IW strategy gives consistent improvements over the base models.
5.2	Adversary Activated VAEs
We apply the AA method on vanilla VAEs, class-conditional VAEs (CVAE), and semi-supervised
VAEs (SVAE) (Kingma et al., 2014), respectively. We evaluate on the MNIST data. We measure the
variational lower bound on the test set, with varying number of real training examples. For each batch
of real examples, AA extended models generate equal number of fake samples for training.
9
Published as a conference paper at ICLR 2018
Z
X
〜
〜
Pdata(X)
f black-box(.^)
Figure 3: Symmetric view of generation and inference. There is little difference of the two processes
in terms of formulation: with implicit distribution modeling, both processes only need to perform
simulation through black-box neural transformations between the latent and visible spaces.
Table 3 shows the results of activating the adversarial mechanism in VAEs. Generally, larger
improvement is obtained with smaller set of real training data. Table 2, right panel, shows the
improved accuracy of AA-SVAE over the base semi-supervised VAE.
6	Discussions: Symmetric view of generation and inference
Our new interpretations of GANs and VAEs have revealed strong connections between them, and
linked the emerging new approaches to the classic wake-sleep algorithm. The generality of the
proposed formulation offers a unified statistical insight of the broad landscape of deep generative
modeling, and encourages mutual exchange of techniques across research lines. One of the key ideas
in our formulation is to interpret sample generation in GANs as performing posterior inference. This
section provides a more general discussion of this point.
Traditional modeling approaches usually distinguish between latent and visible variables clearly and
treat them in very different ways. One of the key thoughts in our formulation is that it is not necessary
to make clear boundary between the two types of variables (and between generation and inference),
but instead, treating them as a symmetric pair helps with modeling and understanding. For instance,
we treat the generation space x in GANs as latent, which immediately reveals the connection between
GANs and adversarial domain adaptation, and provides a variational inference interpretation of the
generation. A second example is the classic wake-sleep algorithm, where the wake phase reconstructs
visibles conditioned on latents, while the sleep phase reconstructs latents conditioned on visibles (i.e.,
generated samples). Hence, visible and latent variables are treated in a completely symmetric manner.
•	Empirical data distributions are usually implicit, i.e., easy to sample from but intractable for
evaluating likelihood. In contrast, priors are usually defined as explicit distributions, amiable for
likelihood evaluation.
•	The complexity of the two distributions are different. Visible space is usually complex while latent
space tends (or is designed) to be simpler.
However, the adversarial approach in GANs and other techniques such as density ratio estimation (Mo-
hamed & Lakshminarayanan, 2016) and approximate Bayesian computation (Beaumont et al., 2002)
have provided useful tools to bridge the gap in the first point. For instance, implicit generative
models such as GANs require only simulation of the generative process without explicit likelihood
evaluation, hence the prior distributions over latent variables are used in the same way as the empirical
data distributions, namely, generating samples from the distributions. For explicit likelihood-based
models, adversarial autoencoder (AAE) leverages the adversarial approach to allow implicit prior
distributions over latent space. Besides, a few most recent work (Mescheder et al., 2017; Tran et al.,
2017; Huszar, 2017; Rosca et al., 2017) extends VAEs by using implicit variational distributions as
the inference model. Indeed, the reparameterization trick in VAEs already resembles construction
of implicit variational distributions (as also seen in the derivations of IWGANs in Eq.37). In these
algorithms, adversarial approach is used to replace intractable minimization of the KL divergence
between implicit variational distributions and priors.
The second difference in terms of space complexity guides us to choose appropriate tools (e.g., adver-
sarial approach v.s. reconstruction optimization, etc) to minimize the distance between distributions to
learn and their targets. However, the tools chosen do not affect the underlying modeling mechanism.
10
Published as a conference paper at ICLR 2018
For instance, VAEs and adversarial autoencoder both regularize the model by minimizing the distance
between the variational posterior and certain prior, though VAEs choose KL divergence loss while
AAE selects adversarial loss.
We can further extend the symmetric treatment of visible/latent x/z pair to data/label x/t pair, leading
to a unified view of the generative and discriminative paradigms for unsupervised and semi-supervised
learning. Specifically, conditional generative models create (data, label) pairs by generating data x
given label t. These pairs can be used for classifier training (Hu et al., 2017; Odena et al., 2017).
In parallel, discriminative approaches such as knowledge distillation (Hinton et al., 2015; Hu et al.,
2016) create (data, label) pairs by generating label t conditioned on data x. With the symmetric view
of x and t spaces, and neural network based black-box mappings across spaces, we can see the two
approaches are essentially the same.
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. In
ICLR, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative
adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Mark A Beaumont, Wenyang Zhang, and David J Balding. Approximate Bayesian computation in population
genetics. Genetics,162(4):2025-2035, 2002.
Jorg Bornschein and Yoshua Bengio. Reweighted wake-sleep. arXivpreprint arXiv:1406.2751, 2014.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial
networks. ICLR, 2017a.
Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua Bengio.
Maximum-likelihood augmented discrete generative adversarial networks. arXiv preprint:1702.07983, 2017b.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable
representation learning by information maximizing generative adversarial nets. In NIPS, 2016.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, and
Pieter Abbeel. Variational lossy autoencoder. ICLR, 2017.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural
computation, 7(5):889-904, 1995.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via
maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680, 2014.
Michael U Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Statistical inference of intractable
generative models via classification. arXiv preprint arXiv:1407.4981, 2014.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake-sleep” algorithm for
unsupervised neural networks. Science, 268(5214):1158, 1995.
R Devon Hjelm, Athul Paul Jacob, Tong Che, Kyunghyun Cho, and Yoshua Bengio. Boundary-seeking generative
adversarial networks. arXiv preprint arXiv:1702.08431, 2017.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. Harnessing deep neural networks with
logic rules. In ACL, 2016.
11
Published as a conference paper at ICLR 2018
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled generation
of text. In ICML, 2017.
Ferenc Huszar.	InfoGAN: using the variational bound on mutual informa-
tion (twice).	Blogpost,	2016.	URL http://www.inference.vc/
infogan- variational- bound- on- mutual- information- twice.
FerenC HuszaE Variational inference using implicit distributions. arXivpreprint arXiv:1702.08235, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational
methods for graphical models. Machine learning, 37(2):183-233,1999.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning
with deep generative models. In NIPS, pp. 3581-3589, 2014.
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse
graphics network. In NIPS, pp. 2539-2547, 2015.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond
pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Yingzhen Li. GANs, mutual information, and possibly algorithm selection? Blogpost, 2016. URL http:
//www.yingzhenli.net/home/blog/?p=421.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In ICML, 2015.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoen-
coders. arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational Bayes: Unifying variational
autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722, 2017.
Luke Metz, Ben Poole, David Pfau, and Sohl-Dickstein. Unrolled generative adversarial networks. ICLR, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint
arXiv:1402.0030, 2014.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv preprint
arXiv:1610.03483, 2016.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71-113, 1992.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using
variational divergence minimization. In NIPS, pp. 271-279, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier
GANs. ICML, 2017.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv
preprint arXiv:1601.06759, 2016.
Yunchen Pu, Liqun Chen, Shuyang Dai, Weiyao Wang, Chunyuan Li, and Lawrence Carin. Symmetric variational
autoencoder and connections to adversarial learning. arXiv preprint arXiv:1709.01846, 2017.
Sanjay Purushotham, Wilka Carvalho, Tanachat Nilanon, and Yan Liu. Variational recurrent adversarial deep
domain adaptation. In ICLR, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, and Shakir Mohamed. Variational approaches
for auto-encoding generative adversarial networks. arXiv preprint arXiv:1706.04987, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training GANs. In NIPS, pp. 2226-2234, 2016.
12
Published as a conference paper at ICLR 2018
Jurgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 1992.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised MAP inference
for image super-resolution. ICLR, 2017.
Martin A Tanner and Wing Hung Wong. The calculation of posterior distributions by data augmentation. JASA,
82(398):528-540,1987.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv preprint
arXiv:1702.08896, 2017.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu.
Conditional image generation with pixelCNN decoders. In NIPS, 2016.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
13
Published as a conference paper at ICLR 2018
A Adversarial Domain Adaptation (ADA)
ADA aims to transfer prediction knowledge learned from a source domain with labeled data to a
target domain without labels, by learning domain-invariant features. Let Dφ(x) = qφ(y∣x) be the
domain discriminator. The conventional formulation of ADA is as following:
m.aχφ Lφ = Eχ = Gθ (z) ,z^p(z | y=1) [log Dφ (X)] + Ex = Gg (z) ,z〜p(z | y = 0) [log (I - Dφ (X))] ,
maχθ Lθ = Eχ = Gg (Z) ,z〜p(z | y=1) [log(1 - Dφ (X))] + Ex = Gg (Z) ,z〜p(z | y=0) [log Dφ (X)] .
(18)
Further add the supervision objective of predicting label t(z) of data z in the source domain, with a
classifier fω (t|x) parameterized with π:
maxω,θ Lω,θ = Ez^p(z∣y=1) [log fω(t(z)∣Gθ(z))].
(19)
We then obtain the conventional formulation of adversarial domain adaptation used or similar
in (Ganin et al., 2016; Purushotham et al., 2017).
B Proof of Lemma 1
Proof.
Epg (x|y)p(y) [log qr (y|X)] =
-Ep(y) [KL (pθ (χ∣y)kqr (χ∣y)) - KL(pθ(χ∣y)kPθo (χ))],
where
Ep(y) [KL(Pθ(χ∣y)kPθo (x))]
=P(y = 0) ∙ KL (pθ(x|y = 0)kpθ° (XIy = 0)+ pθ° (XIy = I))
+ P(y = 1) ∙ KL Q(X|y = ι)kpθ°(XIy = 0)+ pθ°(XIy = I)).
(20)
(21)
Note thatpθ(x|y = 0) = pg§ (x), andpθ(x|y = 1) = Pdata(x). Letpm§ = pgθtf. Eq.(21) can
be simplified as:
Ep(y) [KL(Pθ (XIy) kPe0(X))] = 1 KL (pgθ kPMθ0) + 1KL (PdatakpMθ0).
(22)
On the other hand,
JSD(pgθ kpdata) = 2Epgg
2 EPgg
log p⅛.
+ 2 EPdata
log py+
2 EPgg
pdata
,pMθ
PMθo ^
pMθ .
+ 2 EPdata
pdata
log -----
pMg0
+ 1 " log 』
Pdata
2 data	pMg
(23)
2 EPgg
log PM;
+ 2 EPdata
2 kl pMg；) + 2 kl
log P⅛ + EPMgkOg pM
pMg0	- KL pMg kpMg0 .
Note that
VθKL(PMg kpMθo) ]θ=θ0 = 0.
Taking derivatives of Eq.(22) w.r.t θ at θ0 we get
VθEP(y) [KL(pθ(XIy)kpθ;(x))] Iθ=θ;
=Re (2KL (pgg kpMg；) iΘ=Θ; + 2KL (PdatakpMg0 ))1e=0;
=VθJSD(Pgg kPdata) Iθ = θ0 .
(24)
(25)
Taking derivatives of the both sides of Eq.(20) at w.r.t θ at θ0 and plugging the last equation of
Eq.(25), we obtain the desired results.	□
14
Published as a conference paper at ICLR 2018
Figure 4: Left: Graphical model of InfoGAN. Right: Graphical model of Adversarial AUtoencoder
(AAE), which is obtained by swapping data X and code Z in InfoGAN.
C	Proof of JSD Upper Bound in Lemma 1
We show that, in Lemma.1 (Eq.6), the JSD term is upper bounded by the KL term, i.e.,
JSD(Pθ(x|y = 0)kPθ(x|y = 1)) ≤ Ep(y)[KL(pθ(χ∣y)kqr(x|y))].	(26)
Proof. From Eq.(20), we have
Ep(y) [KL(Pθ(x∣y)kPθo(x))] ≤ Ep(y) [KL (Pθ(x∣y)kqr(x|y))] .	(27)
From Eq.(22) and Eq.(23), we have
JSD(pθ(χ∣y = 0)kpθ(χ∣y = 1)) ≤ Ep(y)[kl(pθ(χ∣y)kPθ0(χ))].	(28)
Eq.(27) and Eq.(28) lead to Eq.(26).	□
D	S chematic Graphical Models and AAE/PM/CycleGAN
Adversarial Autoencoder (AAE) (Makhzani et al., 2015) can be obtained by swapping code variable
z and data variable x of InfoGAN in the graphical model, as shown in Figure 4. To see this, we
directly write down the objectives represented by the graphical model in the right panel, and show
they are precisely the original AAE objectives proposed in (Makhzani et al., 2015). We present
detailed derivations, which also serve as an example for how one can translate a graphical model
representation to the mathematical formulations. Readers can do similarly on the schematic graphical
models of GANs, InfoGANs, VAEs, and many other relevant variants and write down the respective
objectives conveniently.
We stick to the notational convention in the paper that parameter θ is associated with the distribution
over x, parameter η with the distribution over z, and parameter φ with the distribution over y.
Besides, we use p to denote the distributions over x, and q the distributions over z and y.
From the graphical model, the inference process (dashed-line arrows) involves implicit distribution
qη(z∖y) (where X is encapsulated). As in the formulations of GANs (Eq.4 in the paper) and VAEs
(Eq.13 in the paper), y = 1 indicates the real distribution we want to approximate and y = 0 indicates
the approximate distribution with parameters to learn. So we have
qη(z∣y) = (qη F = 0) y = 0	(29)
q(z)	y = 1,
where, as z is the hidden code, q(z) is the prior distribution over z1, and the space of X is degenerated.
Here qη(z∖y = 0) is the implicit distribution such that
Z 〜qη(z∖y = 0)	^⇒ Z = En(x), X 〜Pdata(x),	(30)
1See section 6 of the paper for the detailed discussion on prior distributions of hidden variables and empirical
distribution of visible variables
15
Published as a conference paper at ICLR 2018
where Eη (x) is a deterministic transformation parameterized with η that maps data x to code z.
Note that as x is a visible variable, the pre-fixed distribution of x is the empirical data distribution.
On the other hand, the generative process (solid-line arrows) involves pθ(x|z, y)q(r)(y∣z) (here q(r)
means we will swap between qr and q). As the space of X is degenerated given y = 1, thus pθ(x∣z,y)
is fixed without parameters to learn, and θ is only associated to y = 0.
With the above components, we maximize the log likelihood of the generative distributions
logpθ(x|z, y)q(r)(y∣z) conditioning on the variable Z inferred by qη(z|y). Adding the prior distri-
butions, the objectives are then written as
maxφ Lφ = Eqn(z|y)p(y)[logPθ(x∣z,y)qφ(y∣z)]
maxθ,η Lθ,η = Eqn(z∣y)p(y) [logPθ(x∣z,y)qφ(y|z)].
(31)
Again, the only difference between the objectives of φ and {θ, η} is swapping between qφ (y|z) and
its reverse qφr (y|z).
To make it clearer that Eq.(31) is indeed the original AAE proposed in (Makhzani et al., 2015), we
transform Lφ as
maχφ Lφ = Eqn (z∣y)p(y) [log Qφ(UIz)]
2Eqn(Z|y=0) [log qφ(y = 0Iz)] + 1 Eqn(z∣y=1) [log qφ(U = IIz)]
2 Ez = En (x),x-pdata(x) [log qφ (y = OIz)] + 2 Ez~q(z) [log qφ (y = IIz)] .
(32)
That is, the discriminator with parameters φ is trained to maximize the accuracy of distinguishing the
hidden code either sampled from the true prior p(z) or inferred from observed data example x. The
objective Lθ,η optimizes θ and η to minimize the reconstruction loss of observed data x and at the
same time to generate code z that fools the discriminator. We thus get the conventional view of the
AAE model.
Predictability Minimization (PM) (Schmidhuber, 1992) is the early form of adversarial approach
which aims at learning code z from data such that each unit of the code is hard to predict by the
accompanying code predictor based on remaining code units. AAE closely resembles PM by seeing
the discriminator as a special form of the code predictors.
CycleGAN (Zhu et al., 2017) is the model that learns to translate examples of one domain (e.g.,
images of horse) to another domain (e.g., images of zebra) and vice versa based on unpaired data.
Let x and z be the variables of the two domains, then the objectives of AAE (Eq.31) is precisely
the objectives that train the model to translate x into z. The reversed translation is trained with the
objectives of InfoGAN (Eq.9 in the paper), the symmetric counterpart of AAE.
E	Proof of Lemme 2
Proof. For the reconstruction term:
Epθ0 (x) [Eqn(z|x,y)qr(y|x) [log pθ (X1z, y)]]
=2EPθ0 (XIy=I) [Eqn(z|x,y=0),y=0~qr(y|x) [logpθ(xIz，y = O)]]
+ 2 EPθo (XIy=O) [Eqn(z∣x,y=1),y=1~q[(y∣x) [logpθ(X]z,y = I)]]
=2 Epdata(X) [Eqn (z∣x) [log pθ(XIz)]] + const,
(33)
where y = 0 〜qr (y|x) means qr (y|x) predicts y = 0 with probability 1. Note that both q” (ζ∣x,y =
1) andpθ(x|z, y = 1) are constant distributions without free parameters to learn; qr∣(z|x, y = 0)=
Gn(z|x), andp(x|z, y = 0)= p(x|z).
16
Published as a conference paper at ICLR 2018
For the KL prior regularization term:
Epθo(x) [KL(qη(z∣χ,y)qr(y∣χ)kp(z∣y)p(y))]
=Epθ0 (x) / qr(y∣x)KL (qη(z∣x,y) ∣∣p(z∣y)) dy + KL (qr(y∣x)∣∣p(y))
1	1	(34)
=2Epθ0(χ∣y=i) [KL (qη(z∣x,y = 0)kp(z∣y = 0)) + const] + 2Epθ0 (χ∣y=i) [const]
=2 Epdata(X) [KL(qn(ZIx)kP(Z))].
Combining Eq.(33) and Eq.(34) We recover the conventional VAE objective in Eq.(7) in the paper. □
F VAE/GAN Joint Models for Mode Missing/Covering
Previous Works have explored combination of VAEs and GANs. This can be naturally motivated by
the asymmetric behaviors of the KL divergences that the tWo algorithms aim to optimize respectively.
Specifically, the VAE/GAN joint models (Larsen et al., 2015; Pu et al., 2017) that improve the
sharpness of VAE generated images can be alternatively motivated by remedying the mode covering
behavior of the KLD in VAEs. That is, the KLD tends to drive the generative model to cover all
modes of the data distribution as Well as regions With small values of pdata , resulting in blurred,
implausible samples. Incorporation of GAN objectives alleviates the issue as the inverted KL enforces
the generator to focus on meaningful data modes. From the other perspective, augmenting GANs
With VAE objectives helps addressing the mode missing problem, Which justifies the intuition of (Che
et al., 2017a).
G Importance Weighted GANs (IWGAN)
From Eq.(6) in the paper, We can vieW GANs as maximizing a loWer bound of the “marginal
log-likelihood” on y:
log q(y) =log Zpθ(x|y)q (PIxxpyO)(X) dx
≥ Zpθ(x∣y)log qr(ylx)pθ00(X) dx	(35)
P	Pθ(x∣y)
=-KL(pθ (x∣y)kqr (x∣y)) + const.
We can apply the same importance Weighting method as in IWAE (Burda et al., 2015) to derive a
tighter bound.
k
ι ( 、 ι ιp 1 Lq (y∣xi)pθο (xi)
logq(y)=logE [k ∑	Pθ(xi∣y)	_
k
≥ E log1 X q (y|xi『xi)
—	k i=1	Pθ(xi|y)	(36)
1k
=E log k∑wi
:= Lk(y)
where we have denoted Wi = q ("：(?，爵Xi), which is the unnormalized importance weight. We
recover the loWer bound of Eq.(35) When setting k = 1.
To maximize the importance weighted lower bound Lk(y), we take the derivative w.r.t θ and apply
the reparameterization trick on samples x:
Vθ Lk(y) = VθEχι,...,χk
1k
log k∑wi
i=1
Ezι,…,Zk vθ log k X w(y, x(zi, θ))
i=1
k
Ez1,...,zk	wfiVθ log w(y, x(Zi, θ))
i=1
(37)
17
Published as a conference paper at ICLR 2018
where wfi = wi / Pik=1 wi are the normalized importance weights. We expand the weight at θ = θ0
W	= qr(y|x;)p：0 (Xi) = qr(y|Xi)2pθ0 (XiIy = 0) + 1 pθ0 (XiIy = I) ∣θ	.	(38)
pθ (XiIy)	Pθo (Xi |y)
The ratio of pθ0 (xi|y = 0) and pθ0 (xi|y = 1) is intractable. Using the Bayes’ rule and approximating
with the discriminator distribution, we have
p(X∣y = 0)	= p(y	=	0∣X)p(y	= ι) ≈	q(y = 0∣x)	网
p(X∣y = 1) p(y	=	ι∣X)p(y	= 0) ~	q(y = i∣x) .
Plug Eq.(39) into the above we have	Wij ≈ 鹏	(40)
+ 1E	Pθ°(x)	I
+ 2 Epθ (xly=1) pθ (X∣y = 1) lθ=θ0
In Eq.(37),the derivative Vθ log Wi is
Vθ logw(y, X(zi, θ)) = Vθ logqr(y∣X(zi, θ)) + Vθ log pθ0(Xi).	(41)
pθ(XiIy)
The second term in the RHS of the equation is intractable as it involves evaluating the likelihood of
implicit distributions. However, if we take k = 1, it can be shown that
-Ep⑼P(ZIy)Vθlog pθ(黑θ 案 lθ=θo _
__v 1E	Γ Pθo (x)
=-vθ2Epθ(xly=0) [pθ(X∣y = 0)
= Vθ JSD(pgθ (X)kpdata (X))Iθ=θ0 ,
where the last equation is based on Eq.(23). That is, the second term in the RHS of Eq.(41) is (when
k = 1) indeed the gradient of the JSD, which is subtracted away in the standard GANs as shown in
Eq.(6) in the paper. We thus follow the standard GANs and also remove the second term even when
k > 1. Therefore, the resulting update rule for the generator parameter θ is
VθLk(y) = Ezι,…,zk~p(z∣y) [χk=ι WiVθ logqφφ0 (y∣X(zi, θ))] .	(43)
H	Adversary Activated VAEs (AAVAE)
In our formulation, VAEs include a degenerated adversarial discriminator which blocks out generated
samples from contributing to model learning. We enable adaptive incorporation of fake samples by
activating the adversarial mechanism. Again, derivations are straightforward by making symbolic
analog to GANs.
We replace the perfect discriminator q* (y |x) in vanilla VAEs with the discriminator network qφ(y |x)
parameterized with φ as in GANs, resulting in an adapted objective of Eq.(12) in the paper:
maχθ,ηLavae = Epθ0(X)IEqn(z∣χ,y)qφ(y|x)[logpθ(X|z,y)] — KL(qη(z\X,y)q^(y|X)kp(z|y)p(y))].
(44)
The form of Eq.(44) is precisely symmetric to the objective of InfoGAN in Eq.(9) with the additional
KL prior regularization. Before analyzing the effect of adding the learnable discriminator, we first
look at how the discriminator is learned. In analog to GANs in Eq.(3) and InfoGANs in Eq.(9), the
objective of optimizing φ is obtained by simply replacing the inverted distribution qφr (y|x) with
qφ (y|x):
maxφ Laavae = Epθ0 (X)IEqn(z∣x,y)qφ(y∣x) [logPθ(x∖z,u)] — KL(qη(z∣X,y)qφ(y∣X)∣∣p(z∖y)p(y))] . (45)
Intuitively, the discriminator is trained to distinguish between real and fake instances by predicting
appropriate y that selects the components of qη(z∣x,y) andpθ(x|z, y) to best reconstruct x. The
difficulty of Eq.(45) is that pθ(x|z, y = 1) = Pdata(X) is an implicit distribution which is intractable
for likelihood evaluation. We thus use the alternative objective as in GANs to train a binary classifier:
maxφ Lφ	= Epθ (X|z,y)p(z|y)p(y) [log qφ (y IX)] .	(46)
18
Published as a conference paper at ICLR 2018
I	Experiments
I.1	Importance Weighted GANs
We extend both vanilla GANs and class-conditional GANs (CGAN) with the importance weighting
method. The base GAN model is implemented with the DCGAN architecture and hyperparameter
setting (Radford et al., 2015). We do not tune the hyperparameters for the importance weighted
extensions. We use MNIST, SVHN, and CIFAR10 for evaluation. For vanilla GANs and its IW
extension, we measure inception scores (Salimans et al., 2016) on the generated samples. We train
deep residual networks provided in the tensorflow library as evaluation networks, which achieve
inception scores of 9.09, 6.55, and 8.77 on the test sets of MNIST, SVHN, and CIFAR10, respectively.
For conditional GANs we evaluate the accuracy of conditional generation (Hu et al., 2017). That is,
we generate samples given class labels, and then use the pre-trained classifier to predict class labels
of the generated samples. The accuracy is calculated as the percentage of the predictions that match
the conditional labels. The evaluation networks achieve accuracy of 0.990 and 0.902 on the test sets
of MNIST and SVHN, respectively.
I.2	Adversary Activated VAEs
We apply the adversary activating method on vanilla VAEs, class-conditional VAEs (CVAE), and
semi-supervised VAEs (SVAE) (Kingma et al., 2014). We evaluate on the MNIST data. The generator
networks have the same architecture as the generators in GANs in the above experiments, with
sigmoid activation functions on the last layer to compute the means of Bernoulli distributions over
pixels. The inference networks, discriminators, and the classifier in SVAE share the same architecture
as the discriminators in the GAN experiments.
We evaluate the lower bound value on the test set, with varying number of real training examples.
For each minibatch of real examples we generate equal number of fake samples for training. In the
experiments we found it is generally helpful to smooth the discriminator distributions by setting the
temperature of the output sigmoid function larger than 1. This basically encourages the use of fake
data for learning. We select the best temperature from {1, 1.5, 3, 5} through cross-validation. We do
not tune other hyperparameters for the adversary activated extensions.
Table 4 reports the full results of SVAE and AA-SVAE, with the average classification accuracy and
standard deviations over 5 runs.
1%
10%
SVAE	0.9412±.0039	0.9768±.0009
AASVAE	0.9425±.0045	0.9797±.0010
Table 4: Classification accuracy of semi-supervised VAEs and the adversary activated extension on
the MNIST test set, with varying size of real labeled training examples.
19