Published as a conference paper at ICLR 2018
Generative networks as inverse problems
with Scattering transforms
Tomas Angles & Stephane Mallat
Ecole normale SUPeneure, College de France, PSL Research University
75005 Paris, France
tomas.angles@ens.fr
Ab stract
Generative Adversarial Nets (GANs) and Variational AUto-Encoders (VAEs) Pro-
vide imPressive image generations from GaUssian white noise, bUt the Underlying
mathematics are not well Understood. We comPUte deeP convolUtional network
generators by inverting a fixed embedding oPerator. Therefore, they do not reqUire
to be oPtimized with a discriminator or an encoder. The embedding is LiPschitz
continUoUs to deformations so that generators transform linear interPolations be-
tween inPUt white noise vectors into deformations between oUtPUt images. This
embedding is comPUted with a wavelet Scattering transform. NUmerical exPeri-
ments demonstrate that the resUlting Scattering generators have similar ProPerties
as GANs or VAEs, withoUt learning a discriminative network or an encoder.
1	Introduction
Generative Adversarial Networks (GANs) and Variational AUto-Encoders (VAEs) allow training
generative networks to synthesize images of remarkable qUality and comPlexity from GaUssian white
noise. This work shows that one can train generative networks having similar ProPerties to those
obtained with GANs or VAEs withoUt learning a discriminator or an encoder. The generator is a
deeP convolUtional network that inverts a Predefined embedding oPerator. To reProdUce relevant
ProPerties of GAN image synthesis the embedding oPerator is chosen to be LiPschitz continUoUs to
deformations, and it is imPlemented with a wavelet Scattering transform. Defining image generators
as the solUtion ofan inverse Problem Provides a mathematical framework, which is closer to standard
Probabilistic models sUch as GaUssian aUtoregressive models.
GANs were introdUced by Goodfellow et al. (2014) as an UnsUPervised learning framework to esti-
mate imPlicit generative models of comPlex data (sUch as natUral images) by training a generative
model (the generator) and a discriminative model (the discriminator) simUltaneoUsly. An imPlicit
generative model of the random vector X consists in an oPerator G which transforms a GaUssian
white noise random vector Z into a model X = G(Z) of X. The oPerator G is called a genera-
tive network or generator when it is a deeP convolUtional network. Radford et al. (2016) introdUced
deeP convolUtional architectUres for the generator and the discriminator, which resUlt in high-qUality
image synthesis. They also showed that linearly modifying the vector z resUlts in a Progressive de-
formation of the image X = G(z).
Goodfellow et al. (2014) and Arjovsky et al. (2017) argUe that GANs select the generator G by min-
imizing the Jensen-Shannon divergence or the Wasserstein distance calcUlated from emPirical esti-
mations of these distances with generated and training images. However, Arora et al. (2017) Prove
that this exPlanation fails to Pass the cUrse of dimensionality since estimates of Jensen-Shannon or
Wasserstein distances do not generalize with a nUmber of training examPles which is Polynomial on
the dimension of the images. Therefore, the reason behind the generalization caPacities of generative
networks remains an oPen Problem.
VAEs, introdUced by Kingma & Welling (2014), Provide an alternative aPProach to GANs, by oP-
timizing G together with its inverse on the training samPles, instead of Using a discriminator. The
inverse Φ is an embedding oPerator (the encoder) that is trained to transform X into a GaUssian
white noise Z . Therefore, the loss fUnction to train a VAE is based on Probabilistic distances which
1
Published as a conference paper at ICLR 2018
also suffer from the same dimensionality curse shown in Arora et al. (2017). Furthermore, a signifi-
cant disadvantage of VAEs is that the resulting generative models produce blurred images compared
with GANs.
Generative Latent Optimization (GLO) was introduced in Bojanowski et al. (2017) to eliminate
the need for a GAN discriminator while restoring sharper images than VAEs. GLO still uses an
autoencoder computational structure, where the latent space variables z are optimized together with
the generator G. Despite good results, linear variations of the embedding space variables are not
mapped as clearly into image deformations as in GANs, which reduces the quality of generated
images.
GANs and VAEs raise many questions. Where are the deformation properties coming from? What
are the characteristics of the embedding operator Φ? Why do these algorithms seem to generalize
despite the curse of dimensionality? Learning a stable embedding which maps X into a Gaussian
white noise is intractable without strong prior information (Arora et al., 2017). This paper shows that
this prior information is available for image generation and that one can predefine the embedding
up to a linear operator. The embedding must be Lipschitz continuous to translations and deforma-
tions so that modifications of the input noise result in deformations of Xb . Lipschitz continuity to
deformations requires separating the signal variations at different scales, which leads to the use of
wavelet transforms. We concentrate on wavelet Scattering transforms (Mallat, 2012), which lin-
earize translations and provide appropriate Gaussianization. We then define the generative model
as an inversion of the Scattering embedding on training data, with a deep convolutional network.
The inversion is regularized by the architecture of the generative network, which is the same as the
generator of a DCGAN (Radford et al., 2016). Experiments in Section 4 show that these generative
Scattering networks have similar properties as GAN generators, and the synthesized images have
the same quality as the ones obtained with VAEs or GLOs.
2	Computing a Generator from an Embedding
2.1	Generator Calculation as an Inverse Problem
Unsupervised learning consists in estimating a model Xb of a random vector X of dimension p
from n realizations {xi }i≤n of X. Autoregressive Gaussian processes are simple models Xb =
G(Z) computed from an input Gaussian white noise Z by estimating a parametrized linear operator
G. This operator is obtained by inverting a linear operator, whose coefficients are calculated from
the realizations {xi}i≤n of X. We shall similarly compute models X = G(Z) from a Gaussian
white noise Z , by estimating a parametrized operator G, but which is a deep convolutional network
instead of a linear operator. G is obtained by inverting an embedding {Φ(xi)}i≤n of the realizations
{xi}i≤n of X, with a predefined operator Φ(x).
Let us denote by G the set of all parametrized convolutional network generators defined by a partic-
ular architecture. We impose that G(Φ(xi)) ≈ xi by minimizing an L1 loss over the convolutional
network class G :
n
Gb = argmin n-1 X kxi - G(Φ(xi))k1.
G∈G	i=1
(1)
We use the L1 norm because it has been reported (e.g., Bojanowski et al. (2017)) to give better
results for natural signals such as images.
The resulting generator Gb depends upon the training
examples {xi }i≤n and on the regularization imposed by the network class G. We shall say that the
network generalizes over Φ(X) if E(kX - G(Φ(X))k1) is small and comparable to the empirical
error n-1 Pin=1 kxi - Gb(Φ(xi))k1. We say that the network generalizes over the Gaussian white
noise Z if realizations of X = G(Z) are realizations of X. If G generalizes over Φ(X), then a
sufficient condition to generalize over Z is that Φ transforms X into Gaussian white noise, i.e. Φ
gaussianizes X . Besides this condition, the role of the embedding Φ is to specify the properties of
ʌ A∕∖,l,l	1 1	1 . Γ∙	f t' J	C
X = G(Z) that should result from modifications of z.
2
Published as a conference paper at ICLR 2018
We shall define Φ = AΦ, where Φ is a fixed normalized operator, and A is an affine operator which
performs a whitening and a projection to a lower dimensional space. We impose that {Φ(xi)}i≤n
are realizations of a Gaussian process_and that A transforms this process into a lower dimensional
Gaussian white noise. We normalize Φ by imposing that Φ(0) = 0 and that it is contractive, this is:
∀(x, x0) ∈ R2p , ∣∣Φ(x) — Φ(x0) k ≤ ∣∣x - x0∣∣ .
The affine operator A performs a whitening of the distribution of the {Φ(χ∕}i≤n by subtracting
the empirical mean μ and normalizing the largest eigenvalues of the empirical covariance matrix Σ.
For this purpose, we calculate the eigendecomposition of the covariance matrix Σ = Q D QT . Let
PVd = QdQdT be the orthogonal projection in the space generated by the d eigenvectors having the
largest eigenvalues. We choose d so that PVd Φ(χ) does not contract too much the distances between
the {χi}i≤n, formally, this means that PVd Φ(χ) defines a bi-Lipschitz embedding of these samples,
and hence that there exists a constant α > 0 so that:
一,	1 ..	................................ ,,
∀i,i ≤ n , α∣Xi - XiO k ≤ kPVd Φ(xi) - PVd Φ(xio)k ≤ l∣Xi - X k .	⑵
This bi-Lipschitz property must be satisfied when d is equal to the dimension of Φ and hence when
PVd is the identity. The dimension d should then be adjusted to avoid reducing too much the dis-
tances between the {xi}i≤n.
We choose A = Σ-1∕2(Id - μ) with Σ-1/2 = D-1/2QT, where Id is the identity. The resulting
embedding is:
φ(X) = ς-"2(φ(X)- μ).
This embedding computes a d-dimensional whitening of the {Φ(Xi)}i≤n. The generator network G
which inverts Φ over the training samples is then computed according to (1).
Associative Memory: The existence of a known embedding operator Φ allows us to use the gener-
^
ative network G as an associative or content addressable memory. The input Z can be interpreted as
an address, of lower dimension d than
the generated image Xb . Any training image Xi is associated
to the address zi = Φ(Xi). The network is optimized to generate the training images {Xi}i≤n from
these lower dimensional addresses. The inner network coefficients thus include a form of distributed
memory of these training images. Moreover, if the network generalizes then one can approximately
reconstruct a realization X of the random process X from its embedding address z = Φ(X). In this
sense, the memory is content addressable.
2.2	Gaussianization and Continuity to Deformations
We now describe the properties of the normalized embedding operator Φ to build a generator hav-
ing similar properties as GANs or VAEs. We mentioned that we would like Φ(X) to be nearly a
Gaussian white noise and since Φ = A Φ where A is affine then Φ(X) should be Gaussian. There-
fore, {Φ(χi)}i≤n should be realizations of a Gaussian process and hence be concentrated over an
ellipsoid.
The normalized embedding operator Φ must also be covariant to translations because it will be
inverted by several layers of a deep convolutional generator, which are covariant to translations.
Indeed, the generator belongs to G which is defined by a DCGAN architecture (Radford et al., 2016).
In this family of networks, the first layer is a linear operator which reshapes and adjusts the mean and
covariance of the input white noise Z. The non-stationary part of the process X = G(Z) is captured
by this first affine transformation. The next layers are all convolutional and hence covariant to
translations. These layers essentially invert the normalized embedding operator Φ over the training
samples.
The normalized embedding operator Φ must also linearize translations and small deformations. In-
deed, if the input Z = AΦ(x) is linearly modified then, to reproduce GAN properties, the output
3
Published as a conference paper at ICLR 2018
G(z) should be continuously deformed. Therefore, we require Φ to be Lipschitz continuous to
translations and deformations. A translation and a deformation of an image x(u) can be written as
xτ(u) = χ(u — T(u)), where U denotes the spatial coordinates. Let ∣τ∣∞ = maxu ∣τ(u)| be the
maximum translation amplitude. Let Vτ(U) be the Jacobian of T at U and ∣Vτ(u)| be the norm
of this Jacobian matrix. The deformation induced by T is measured by ∣Vτ∣∞ = maxu ∣Vτ(u)|,
which specifies the maximum scaling factor induced by the deformation. The value ∣Vτ∣∞ defines
a_metric on diffeomorphisms (Mallat, 2012) and thus specifies the deformation “size”. The operator
Φ is said to be Lipschitz continuous to translations and deformations over domains of scale 2J if
there exists a constant C such that for all x and all T we have:
kΦ(x) - Φ(xτ)k ≤ Ckxk(2-J|t∣∞ + |Vt∣∞).	⑶
This inequality implies that translations of x that are Sma^relative to 2j and small deformations are
mapped by Φ into small quasi-linear variations of Z = AΦ(x).
The Gaussianization property means that {Φ(xi)}i≤n should be concentrated on an ellipsoid. This
is always true if n ≤ p, but it can be difficult to achieve if n p. In one dimension x ∈ R, an
invertible differentiable operator Φ which Gaussianizes a random variable X can be computed as the
solution of a differential equation that transports the histogram into a Gaussian (Friedman, 1987).
In higher dimensions, this strategy has been extended by iteratively performing a Gaussianization of
one-dimensional variables, through independent component analysis (Chen & Gopinath, 2000) or
with random rotations (Laparra et al., 2011). However, these approaches do not apply in this con-
text because they do not necessarily define operators which are translation covariant and Lipschitz
continuous to deformations.
Another Gaussianization strategy comes from the Central Limit Theorem by averaging nearly in-
dependent random variables having variances of the same order of magnitude. This averaging can
be covariant to translations if implemented with convolutions with a low-pass filter. The resulting
operator will also be Lipschitz continuous to deformations. However, an averaging operator loses
all high-frequency information. To define an operator Φ which satisfies the bi-Lipschitz condition
(2), we must preserve the high-frequency content despite the averaging. The next section explains
how to do so with a Scattering transform.
3 Generative S cattering Networks
In this section, we show that a Scattering transform (Mallat, 2012; Bruna & Mallat, 2013) provides
an appropriate embedding for image generation, without learning. It does so by taking advantage of
prior information on natural signals, such as translation and deformation properties. We also specify
the architecture of the deep convolutional generator that inverts this embedding, and we summarize
the algorithm to perform this regularized inversion.
Since the first order term of a deformation is a local scaling, defining an operator that is Lipschitz
continuous to deformations, and hence satisfies (3), requires decomposing the signal at different
scales, which is done by a wavelet transform (Mallat, 2012). The linearization of translations at
a scale 2J is obtained by an averaging implemented with a convolution with a low-pass filter at
this scale. A Scattering transform uses non-linearities to compute interactions between coefficients
across multiple scales, which restores some information lost due to the averaging. It also defines
a bi-Lipschitz embedding (2) and the averaging at the scale 2J Gaussianizes the random vector X .
This scale 2J adjusts the trade-off between Gaussianization and contraction of distances due to the
averaging.
A Scattering operator SJ transforms x(U) into a tensor xJ (U, k), where the spatial parameter U is
sampled at intervals of size 2J and the channels are indexed by k. The number KJ of channels
increases with J to partially compensate for the loss of spatial resolution. These KJ channels are
computed by a non-linear translation covariant transformation ΨJ. In this paper, ΨJ is computed as
successive convolutions with complex two-dimensional wavelets followed by a pointwise complex
modulus, with no channel interactions. Following Bruna & Mallat (2013), we choose a Morlet
wavelet ψ, scaled by 2' for different values of ' and rotated along Q angles θ = qn/Q:
4
Published as a conference paper at ICLR 2018
ψ',q(U) = 2-2' ψ(2-'rθU) for 0 ≤ q <Q.
To obtain an order two Scattering operator SJ, the operator ΨJ computes sequences of up to two
wavelet convolutions and complex modulus:
ψJ(X) = hx，lx?ψ',q1，llx?ψ',q1?ψ'0,q0liι≤'<'o≤j,.
Therefore, there are KJ = 1 + QJ + Q2 J(J - 1)/2 channels. A Scattering transform is then
obtained by averaging each channel with a Gaussian low-pass filter φJ (U) = 2-2J φ(2-J U) whose
spatial width is proportional to 2J:
SJ (x) = Ψj (x) ?φj = x*φj , ∣x?ψ',q | ?φj , ∣∣X?ψ',q | ?ψ'0,q0 | *φj
L	」1≤'<'0 ≤J, 1≤q,q0≤Q
Convolutions with φJ are followed by a subsampling of 2J; as a result, if x has p pixels then SJ is
of dimension p αJ where:
αj = 2-2J(1 + QJ + Q2J (J - 1)/2).
The maximum scale 2J is limited by the image width 2J ≤ p1/2. For our experiments we used
Q = 8 and images x of size p = 1282. In this case α4 = 1.63 and α5 = 0.67. Since αJ > 1 for
J ≤ 4, SJ (x) has more coefficients than x. Based on this coefficient counting, we expect SJ to be
invertible for J ≤ 4, but not for J ≥ 5.
The wavelets separate the variations of x at different scales 2' along different directions qn/Q,
and second order Scattering coefficients compute interactions across scales. Because of this scale
separation, one can prove that SJ is Lipschitz continuous to translations and deformations (Mallat,
2012) in the sense of eq. (3). Wavelets also satisfy a Littlewood-Paley condition which guarantees
that the wavelet transform and also SJ are contractive operators (Mallat, 2012).
If wavelet coefficients become nearly independent when they are sufficiently far away then SJ(X)
becomes progressively more Gaussian as the scale 2J increases, because of the spatial averaging by
φJ. Indeed, if X(U) is independent from X(v) for |x - v| ≥ ∆ then the Central Limit Theorem
proves that SJ(X) converges to a Gaussian distribution when 2j/∆ increases. However, as the
scale increases, the averaging produces progressively more instabilities which can deteriorate the
bi-Lipschitz bounds in (2). This trade-off between Gaussianization and stability defines the optimal
choice of the scale 2J.
As explained in (Mallat, 2016), convolutions with wavelets ψ',q and the low-pass filter φJ can also
be implemented as convolutions with small size filters and subsamplings. As a result, a Scattering
transform is obtained by cascading convolution matrices Vj and the complex modulus as a non-
linearity:
Sj = |Vj Sj-1| for 1 ≤ j ≤ J .
A Scattering transform is thus an instance of a deep convolutional network whose filters are spec-
ified by wavelets and where the non-linearity is chosen to be a modulus. The choice of filters is
flexible and other multiscale representations, such as the ones in Portilla & Simoncelli (2000); Lyu
& Simoncelli (2009); Malo & Laparra (2010), may also be used.
Following the notations in 2.1, the normalized embedding operator Φ is chosen to be SJ, thus the
embedding operator is defined by Φ(x) = Σ-"2(Sj(x) - μ). A generative Scattering network is
a deep convolutional network which implements a regularized inversion of this embedding. Both
networks are illustrated in Figure 1.
More specifically, a generative Scattering network is a deep convolutional network Gb which inverts
the whitened Scattering embedding Φ on training samples. It is obtained by minimizing the L1 loss
5
Published as a conference paper at ICLR 2018
Figure 1: Top: the embedding operator Φ consists of a Scattering transform SJ (X) computed by
cascading J convolutional wavelet operators Vj followed by a pointwise complex modulus and then
an affine whitening. Bottom: The generative network is computed by cascading Relus ρ and linear
operators plus biases Wj, which are convolutional along spatial variables for j ≥ 1.
n-1 in=1 kG(Φ(xi)) - xi k1, as explained in Section 2.1. The minimization is done with the Adam
optimizer (Kingma & Ba, 2014), using the default hyperparameters. The generator illustrated in
Figure 1, is a DCGAN generator (Radford et al., 2016), of depth J + 2:
G=ρWJ+1ρWJ...ρW1ρW0.
The non-linearity ρ is a ReLU. The first operator W0 is linear (fully-connected) plus a bias, it trans-
forms Z into a 4 × 4 array of 1024 channels. The next operators Wj for 1 ≤ j ≤ J perform a bilinear
upsampling of their input, followed by a multichannel convolution along the spatial variables, and
the addition of a constant bias for each channel. The operators ρWj compute a progressive inver-
sion of SJ (x), calculated with the convolutional operators |Vj | for 1 ≤ j ≤ J. The last operator
WJ+1 does not perform an upsampling. All the convolutional layers have filters of size 7, with sym-
metric padding at the boundaries. All experiments are performed with color images of dimension
p = 1282 × 3 pixels.
4 Numerical experiments
This section evaluates generative Scattering networks with several experiments. The accuracy of the
inversion given by the generative network is first computed by calculating the reconstruction error
of training images. We assess the generalization capabilities by computing the reconstruction error
on test images. Then, we evaluate the visual quality of images generated by sampling the Gaussian
white noise Z. Finally, we verify that linear interpolations of the embedding variable z = Φ(x)
produce a morphing of the generated images, as in GANs. The code to reproduce the experiments
can be found in 1.
We consider three datasets that have different levels of variabilities: CelebA (Liu et al., 2015), LSUN
(bedrooms) (Yu et al., 2015) and Polygon5. The last dataset consists of images of random convex
polygons of at most five vertices, with random colors. All datasets consist of RGB color images with
shape 1282 × 3. For each dataset, we consider only 65536 training images and 16384 test images.
In all experiments, the Scattering averaging scale is 2J = 24 = 16, which linearizes translations
and deformations of up to 16 pixels. For an RGB image x, S4 (x) is computed for each of the three
color channels. Because of the subsampling by 2-4, S4 (x) has a spatial resolution of 8 × 8, with
417 × 3 = 1251 channels, and is thus of dimension ≈ 8 × 104. Since it has more coefficients than the
input image x, we expect it to be invertible, and numerical experiments indicate that this is the case.
This dimension is reduced to d = 512 by the whitening operator. The resulting operator remains a
bi-Lipschitz embedding of the training images of the three datasets in the sense of (2). The Lipschitz
constant is α = 5, and 99.5% of the distances between image pairs (xi, xi0 ) are preserved with a
Lipschitz constant smaller than 3. Further reducing the dimension d to 100, which is often used
in numerical experiments (Radford et al., 2016; Bojanowski et al., 2017), has a marginal effect on
the Lipschitz bound and on numerical results, but it slightly affects the recovery of high-frequency
details.
We now assess the generalization properties of Gb by comparing the reconstructions of training and
test samples from Φ(X) = Σ-1/2(S4(X) - μ); figures 2 and 3 ShoW SUCh reconstructions. Table 1
1https://github.com/tomas-angles/generative-scattering-networks
6
Published as a conference paper at ICLR 2018
x in the training set. Bottom: reconstructions from
Φ(x) using Gb.
Figure 2: Top:
BB

Figure 3: Top: original images x in the test set. Bottom: reconstructions from Φ(x) using Gb.
gives the average training and test reconstruction errors in dB for each dataset. The training error is
between 3dB and 8dB above the test error, which is a sign of overfitting. However, this overfitting
is not large compared to the variability of errors from one dataset to the next. Overfitting is not
good for unsupervised learning where the intent is to model a probability density, but if we consider
this network as an associative memory, it is not a bad property. Indeed, it means that the network
performs a better recovery of known images used in training than unknown images in the test, which
is needed for high precision storage of particular images.
Polygons are simple images that are much better recovered than faces in CelebA, which are simpler
images than the ones in LSUN. This simplicity is related to the sparsity of their wavelet coefficients,
which is higher for polygons than for faces or bedrooms. Wavelet sparsity drives the properties of
Scattering coefficients which provide localized l1 norms of wavelet coefficients. The network reg-
7
Published as a conference paper at ICLR 2018
ularizes the inversion by storing the information needed to reconstruct the training images, which
is a form of memorization. LSUN images require more memory because their wavelet coefficients
are less sparse than polygons or faces; this might explain the difference of accuracies over datasets.
However, the link between sparsity and the memory capacity of the network is not yet fully un-
derstood. The generative network has itself sparse activations with about 70% of them being equal
to zero on average over all images of the three datasets. Sparsity thus seems to be an essential
regularization property of the network.
CelebA
Train 25.95
Polygon5 LSUN (bedrooms)
42.43	21.77
Test 21.17	34.44
18.53
Table 1: PSNR reconstruction errors in dB of train and test images, from their whitened Scattering
embedding, over three datasets.
Similarly to VAEs and GLOs, Scattering image generations eliminate high-frequency details, even
on training samples. This is due to a lack of memory capacity of the generator. This was verified by
reducing the number of training images. Indeed, when using only n = 256 training images, all high-
frequencies are recovered, but there are not enough images to generalize well on test samples. This
is different from GAN generations, where we do not observe this attenuation of high-frequencies
over generated images. GANs seem to use a different strategy to cope with the memory limitation;
instead of reducing precision, they seem to ”forget” some training samples (mode-dropping), as
shown in Bojanowski et al. (2017). Therefore, GANs versus VAEs or generative Scattering networks
Figure 4: Images X = G(Z) generated from a Gaussian white noise Z.
To evaluate the generalization properties of the network on Gaussian white noise Z, Figure 4 shows
images X = G(Z) generated from random samplings of Z. Generated images have strong similari-
ties with the ones in the training set for polygons and faces. The network recovers colored geometric
shapes in the case of Polygon5 even tough they are not exactly polygons, and it recovers faces for
CelebA with a blurred background. For LSUN, the images are piecewise regular, and most high
frequencies are missing; this is due to the complexity of the dataset and the lack of memory capacity
of the generative network.
Figure 5 evaluates the deformation properties of the network. Given two input images x and x0 , we
modify α ∈ [0, 1] to obtain the interpolated images:
xα = Gb (1 - α)z + αz0 for z = Φ(x) and z0 = Φ(x0) .	(4)
The linear interpolation over the embedding variable z produces a continuous deformation from one
image to the other while colors and image intensities are also adjusted. It reproduces the morphing
properties of GANs. In our case, these properties result from the Lipschitz continuity to deforma-
tions of the Scattering transform.
8
Published as a conference paper at ICLR 2018
Figure 5: Each row shows the image morphing obtained by linearly interpolating the latent variables
of the left-most and right-most images, according to (4). For each of the three datasets, the first row
is computed with two training images and the second row with two test images.
5 Conclusion
This paper shows that most properties of GANs and VAEs can be reproduced with an embedding
computed with a Scattering transform, which avoids using a discriminator as in GANs or learning
the embedding as in VAEs or GLOs. It also provides a mathematical framework to analyze the
statistical properties of these generators through the resolution of an inverse problem, regularized
by the convolutional network architecture and the sparsity of the obtained activations. Because the
embedding function is known, numerical results can be evaluated on training as well as test samples.
We report preliminary numerical results with no hyperparameter optimization. The architecture
of the convolutional generator may be adapted to the properties of the Scattering operator Sj as j
increases. Also, the paper uses a “plain” Scattering transform which does not take into account inter-
actions between angle and scale variables, which may also improve the representation as explained
in Oyallon & Mallat (2015).
Acknowledgements
This work was funded by the ERC grant InvariantClass 320959.
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. WaSSerStein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In Proceedings of the 34th International Conference on
Machine Learning, pp. 224-232, 06-11 Aug 2017.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the la-
tent space of generative networks, 2017. URL https://openreview.net/forum?id=
ryj38zWRb.
J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach.
Intell., 35(8):1872-1886, 2013.
S.S. Chen and R. A. Gopinath. Gaussianization. In Proc. NIPS, 2000.
J.H. Friedman. Exploratory projection pursuit. J. American Statistical Association, 82:249-266,
1987.
9
Published as a conference paper at ICLR 2018
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, pp.
2672-2680, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Representations (ICLR), 2014.
D.P. Kingma and M. Welling. Auto-encoding variational bayes. In Proc. of ICLR, 2014.
V. Laparra, G. Camps-Valls, and J. Malo. Iterative gaussianization: From ica to random rotations.
IEEE Transactions on Neural Networks, 22(4):537-549, April 2011. ISSN 1045-9227. doi:
10.1109/TNN.2011.2106511.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Siwei Lyu and Eero P. Simoncelli. Reducing statistical dependencies in natural signals using radial
gaussianization. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou (eds.), Advances in
Neural Information Processing Systems 21, pp. 1009-1016. Curran Associates, Inc., 2009.
S. Mallat. Group invariant scattering. Commun. Pure Appl. Math., 65(10):1331-1398, 2012.
S. Mallat. Understanding deep convolutional networks. Philos. Trans. Royal Society A, March 2016.
Jess Malo and Valero Laparra. Psychophysically tuned divisive normalization approximately
factorizes the pdf of natural images. Neural Computation, 22(12):3179-3206, 2010. doi:
10.1162∕NECO∖_a\_00046. PMID: 20858127.
E. Oyallon and S. Mallat. Deep roto-translation scattering for object classification. In Proc. of
CVPR, 2015.
Javier Portilla and Eero P. Simoncelli. A parametric texture model based on joint statistics of com-
plex wavelet coefficients. Int. J. Comput. Vision, 40(1):49-70, October 2000. ISSN 0920-5691.
doi: 10.1023/A:1026553619983.
A. Radford, L. Metz, and R. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. In Proc. of ICLR, 2016.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of
a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.
10