Published as a conference paper at ICLR 2018
Initialization matters: Orthogonal
Predictive State Recurrent Neural Networks
Krzysztof Choromanski*
Google Brain
kchoro@google.com
Carlton Downey**
Carnegie Mellon University
cmdowney@cs.cmu.edu
Byron Boots ^
Georgia Tech
bboots@cc.gatech.edu
Ab stract
Learning to predict complex time-series data is a fundamental challenge in a range
of disciplines including Machine Learning, Robotics, and Natural Language Pro-
cessing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.,
2017) are a state-of-the-art approach for modeling time-series data which com-
bine the benefits of probabilistic filters and Recurrent Neural Networks into a
single model. PSRNNs leverage the concept of Hilbert Space Embeddings of
distributions (Smola et al., 2007) to embed predictive states into a Reproducing
Kernel Hilbert Space, then estimate, predict, and update these embedded states
using Kernel Bayes Rule. Practical implementations of PSRNNs are made pos-
sible by the machinery of Random Features, where input features are mapped
into a new space where dot products approximate the kernel well. Unfortunately
PSRNNs often require a large number of RFs to obtain good results, resulting
in large models which are slow to execute and slow to train. Orthogonal Random
Features (ORFs)(Yu et al., 2016) is an improvement on RFs which has been shown
to decrease the number of RFs required for pointwise kernel approximation. Un-
fortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely
on Kernel Ridge Regression as a core component of their learning algorithm, and
the theoretical guarantees of ORF do not apply in this setting. In this paper, we
extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can
be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster
than PSRNNs. In particular, we show that OPSRNN models clearly outperform
LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order
of magnitude smaller number of features needed.
1	Introduction
Learning to predict temporal sequences of observations is a fundamental challenge in a range of
disciplines including machine learning, robotics, and natural language processing. There exist a
wide variety of approaches to modeling time series data, however recurrent neural networks (RNNs)
have emerged as the clear frontrunner, achieving state-of-the-art performance in applications such
as speech recognition (Heigold et al., 2016), language modeling (Mikolov et al., 2010), translation
(Cho et al., 2014b), and image captioning (Xu et al., 2015).
Predictive State Recurrent Neural Networks (PSRNNs) are a state-of-the-art RNN architecture re-
cently introduced by Downey et al. (2017) that combine the strengths of probabilistic models and
RNNs in a single model. Specifically PSRNNs offer strong statistical theory, globally consistent
model initializations, and a rich functional form which is none-the-less amenable to refinement via
backpropogation through time (BPTT). Despite consisting ofa simple bi-linear operations, PSRNNs
have been shown to significantly outperform more complex RNN architectures (Downey et al.,
2017), such as the widely used LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al.,
2014a).
* Equal Contribution
^ Work done while at Google
^ Work done while at Google Brain
1
Published as a conference paper at ICLR 2018
PSRNNs leverage the concept of Hilbert Space embeddings of distributions (Smola et al., 2007) to
embed predictive states into a Reproducing Kernel Hilbert Space (RKHS), then estimate, predict,
and update these embedded states using Kernel Bayes Rule (KBR) (Smola et al., 2007). Because
PSRNNs directly manipulate (kernel embeddings of) distributions over observations, they can be
initialized via a globally consistent method-of-moments algorithm which reduces to a series of linear
ridge regressions.
Practical implementations of PSRNNs are made possible by the machinery of Random Features
(RFs): input features are mapped into a new space where dot products approximate the kernel well
(Rahimi & Recht, 2008). RFs are crucial to the success of PSRNNs, however PSRNNs often require
a significant number of RFs in order to obtain good results. And, unfortunately, the number of
required RFs grows with the dimensionality of the input, resulting in models which can be large,
slow to execute, and slow to train.
One technique that has proven to be effective for reducing the required number of RFs for kernel
machines is Orthogonal Random Features (ORFs) (Yu et al., 2016). When using ORFs, the ma-
trix of RFs is replaced by a properly scaled random orthogonal matrix, resulting in significantly
decreased kernel approximation error. A particularly nice feature of ORFs is that (Yu et al., 2016;
Choromanski et al., 2017) prove that using ORFs results in a guaranteed improvement in pointwise
kernel approximation error when compared with RFs.
Unfortunately the guarantees in Yu et al. (2016) are not directly applicable to the PSRNN setting.
PSRNNs first obtain a set of model parameters via ridge regression, then use these model parameters
to calculate inner products in RF space. This “downstream” application of RFs goes beyond the
results proven in Yu et al. (2016) and Choromanski et al. (2017). Hence it is not clear whether or
not ORF can be applied to obtain an improvement in the PSRNN setting.
In this work, we show that ORFs can be used to obtain OPSRNNs: PSRNNs initialized using ORFs
which are smaller, faster to execute and train than PSRNNs initialized using conventional unstruc-
tured RFs. We theoretically analyze the orthogonal version of the KRR algorithm that is used to
initialize OPSRNNs. We show that orthogonal RNNs lead to kernel algorithms with strictly better
spectral properties and explain how this translates to strictly smaller upper bounds on failure prob-
abilities regarding KRR empirical risk. We compare the performance of OPSRNNs with that of
LSTMs as well as conventional PSRNNs on a number of robotics tasks, and show that OPSRRNs
are consistently superior on all tasks. In particular, we show that OPSRNN models can achieve
accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.
2	Related Work
Orthogonal random features were introduced in Yu et al. (2016) as an alternative to the standard
approach for constructing random feature maps to scale kernel methods (Rahimi & Recht, 2007).
Several other structured constructions were known before (Ailon & Chazelle, 2006; Hinrichs &
Vybral, 2011; Vyblral, 2011; Zhang & Cheng, 2013; Choromanski & Sindhwani, 2016; Choro-
manska et al., 2016; Bojarski et al., 2017), however these were motivated by computational and
space complexity gains and led to weaker accuracy guarantees. In contrast to this previous work,
orthogonal random features were proposed to improve accuracy of the estimators relying on them.
Such an improvement was theoretically and experimentally verified, but only for pointwise kernel
approximation (Yu et al., 2016; Choromanski et al., 2017) and for specific kernels (such as Gaus-
sian for dimensionality large enough, as well as dot-product and angular kernels). It was not clear
whether these pointwise gains translate to downstream guarantees for algorithms relying on kernels
(for instance kernel ridge regression), even though there was some partial empirical evidence that
this might be the case (in Choromanski et al. (2017) orthogonal random features were experimen-
tally tested to provide more accurate approximation of the groundtruth kernel matrix in terms of the
Frobenius norm error). Even for the pointwise estimators and for the selected kernels, the guaran-
tees were given only with the use of second moment methods (variance computation) and thus did
not lead to strong concentration results with exponentially small probabilities of failure, which we
obtain in this paper.
To the best of our knowledge, we are the first to apply orthogonal random features via kernel ridge
regression for recurrent neural networks. There is however vast related literature on orthogonal
2
Published as a conference paper at ICLR 2018
recurrent neural networks, where the matrices are enforced to be orthogonal or initialized to be ran-
dom orthogonal. Probably some of the most exploited recent directions are unitary evolution RNN
architectures (Arjovsky et al., 2016), where orthogonal and unitary matrices are used to address a
key problem of RNN training - vanishing or exploding gradients. Related results are presented in
Henaff et al. (2016), Saxe et al. (2013) (orthogonal initialization for training acceleration), Ganguli
et al. (2008) and White et al. (2004). Most of these results do not provide any strict theoretical
guarantees regarding the superiority of the orthogonal approach. Even though these approaches are
only loosely related to our work, there is a common denominator: orthogonality, whether applied in
our context or the aforementioned ones, seems to to be responsible for disentangling in (deep) repre-
sentations (Achille & Soatto, 2017). Our rigorous theoretical analysis shows that this phenomenon
occurs for the orthogonal KRR that is used as a subroutine of OPSRNNs, but the general mechanism
is still not completely understood from the theoretical point of view.
3	Predictive State Recurrent Neural Networks
PSRNNs (Downey et al., 2017) are a recently developed RNN architecture which combine the ideas
of predictive state (Boots et al., 2013) and RFs. The key advantage of PSRNNs is that their state
update can be interpreted in terms of Bayes Rule. This allows them to be initialized as a fully
functional model via a consistent method of moments algorithm, in contrast to conventional RNN
architectures which are initialized at random. Below we describe PSRNNs in more detail. We pay
particular attention to how PSRNNS utilize RFs, which will be replaced with ORFs in OPSRNNs.
The explicit construction of ORFs is given in Section 4.
A predictive state (Littman et al., 2001) is a set of expectations of features of future observations.
A predictive state is best understood in contrast with the more typical latent state: predictive states
are distributions over known functions of observations, whereas latent states are distributions over
unobserved latent quantities. Formally, a predictive state is a vector qt = E [ft | ht], where
ft = f (ot:t+k-1) are features of future observations and ht = h(o1:t-1) are features of histori-
cal observations.
PSRNNs use a predictive state, but embed it in a Reproducing Kernel Hilbert Space (RKHS) using
RFs: Let kf, kh, ko be translation invariant kernels (Rahimi & Recht, 2008) defined on ft, ht,
and ot respectively. Define projections φt = RF (ft), ηt = RF (ht), and ωt = RF (ot) such that
kf(fi, fj) = φiTφj, kh(hi, hj) = ηiTηj, ko(oi, oj) = ωiTωj. Then the PSRNN predictive state
is qt = E[φt | ηt].
PSRNN model parameters consist of an initial state q1 and a 3-mode update tensor W. The PSRNN
state update equation is:
_ W ×3 qt ×2 0t
qt+1 = kW ×3 qt ×2 Otk
(1)
Figure 1: PSRNN Update, displayed on the left as a neural network and on the right as an equation
3.1 Two-stage Regression for PSRNNs
PSRNNs can be initialized using the Two Stage Regression (2SR) approach of Hefny et al. (2015).
This approach is fast, statistically consistent, and reduces to simple linear algebra operations. In
2SR q1 and W are learned by solving three Kernel Ridge Regression problems in two stages. Ridge
regression is required in order to obtain a stable model, as it allows us to minimize the destabilizing
effect of rare events while preserving statistical consistency.
In stage one we regress from past φt to future ηt , and from past φt to the outer product of shifted
future ψ := ηt+1 with observations ωt . Let Xφ be the matrix whose tth column is φt , Xη the matrix
whose tth column is ηt, and Xψ0ω be the matrix whose tth column is ψt 0 ωt:
3
Published as a conference paper at ICLR 2018
WnlΦ
Wψω∣φ
XnXT (XφXT + λI)-1,
Xψ0ωXT (XφXT + λI)-1.
Using Wηlφ and Wψωlφ we obtain predictive state estimates Xηlφ and Xψωlφ at each time step:
Xηlφ = WηlφXφ
Xψωlφ = WψωlφXφ
In stage two we regress from Cηlφ to Cψωlφ to obtain the model weights W :
q1 = Xηlφ1,
W=XψωlφXηTlφXηlφXηTlφ+λI-1,
where λ ∈ R is the ridge parameter and I is the identity matrix and 1 is a column vector of ones.
Once the state update parameters have been learned via 2SR we train a kernel ridge regression model
to predict ωt from qt . 1
The 2SR algorithm is provably consistent in the realizable setting, meaning that in the limit we are
guaranteed to recover the true model parameters. Unfortunately this result relies on exact kernel
values, while scalable implementations work with approximate kernel values via the machinery of
RFs. In practice we often require a large number of RFs in order to obtain a useful model. This can
result in large models which are slow to execute and slow to train.
We now introduce ORFs and show that they can be used to obtain smaller, faster models. The key
challenge with applying ORFs to PSRNNs is extending the theoretical guarantees ofYu et al. (2016)
to the kernel ridge regression setting.
4 Orthogonal Random Features
We explain here how to construct orthogonal random features to approximate values of kernels
defined by the prominent family of radial basis functions and consequently, conduct kernel ridge
regression for the OPSRNN model. A class of RBF kernels K (RBFs in shorthand) is a family of
functions: Kn : Rn × Rn → R for n = 1, 2, ... such that Kn(x, y) = φ(z), for z = kx-yk2, where
φ : R → R is a fixed positive definite function (not parametrized by n). An important example is
the class of Gaussian kernels.
Every RBF kernel K is shift-invariant, thus in particular its values can be described by an integral
via Bochner’s Theorem (Rahimi & Recht, 2007):
K(x, y) = Re / exp(iw>(x - y))μκ(dw),	⑵
Rn
where μκ ∈ M(Rn) stands for some finite Borel measure. Some commonly used RBF kernels
K together with the corresponding functions φ and probability density functions for measures μκ
are given in Table 2. The above formula leads straightforwardly to the standard unbiased Monte-
Carlo estimator of K(x, y) given as: K(x, y) = Φ>m,n(x)Φm,n(y), where a random embedding
Φm,n : Rn → R2m is given as:
Φm,n(x) = (√1m Cos(W>x), √1m sin(w>x))	,	(3)
vectors Wi ∈ Rn are sampled independently from μκ and m stands for the number of random
features used. In this scenario we will often use the notation Wiiid, to emphasize that different Wi s
are sampled independently from the same distribution.
1 Note that we can train a regression model to predict any quantity from the state
4
Published as a conference paper at ICLR 2018
Name
Gaussian
Laplacian
Positive-definite function φ
Probability density function
exp(-z)
(2∏λ2)n∕2 exp (- 2λ2 llwk2)
n 1
i=1 π(1+wi)
π
Figure 2: Common RBF kernels, the corresponding functions φ, and probability density functions
(here: w = (w1, ..., wn)>).
For a datasets X , random features provide an equivalent description of the original kernel via the
linear kernel in the new dataset Φ(X) = {Φ(x) : x ∈ X} obtained by the nonlinear transformation
Φ and lead to scalable kernel algorithms if the number of random features needed to accurately
approximate kernel values satisfies: m N = |X |.
Orthogonal random features are obtained by replacing a standard mechanism of constructing vectors
wi and described above with the one where the sequence (w1, ..., wm) is sampled from a “related”
joint distribution μKrtm n on Rn X ... X Rn satisfying the orthogonality condition, namely: with
probability p = 1 different vectors wi are pairwise orthogonal. Since in practice we often need
m > n, the sequence (wi)i=1,...,m is obtained by stacking some number of blocks, each of length
l ≤ n, where the blocks are sampled independently from μKrtmn.
It remains to explain how μKrtm ln is constructed. We consider two main architectures. For the
first one the marginal distributions (distributions of individual vectors Wi) are μκ. A sample
(w1ort, ..., womrt) from the joint distribution might be obtained for instance by constructing a ran-
dom matrix G = [(Wi1id)>; ...; (Wmiid)>] ∈ Rm×n, performing Gram-Schmidt orthogonalization
and then renormalizing the rows such that the length of the renormalized row is sampled from the
distribution from which lWiiid ls are sampled. Thus the Gram-Schmidt orthogonalization is used
just to define the directions of the vectors. From now on, we will call such a joint distribution
continuous-orthogonal. The fact that for RBF kernels the marginal distributions are exactly μκ and
thus, kernel estimator is still unbiased, is a direct consequence of the isotropicity of distributions
fom which directions of vectors Wiiid are sampled. For this class of orthogonal estimators we prove
strong theoretical guarantees showing that they lead to kernel ridge regression models superior to
state-of-the-art ones based on vectors Wiiid .
Another class of orthogonal architectures considered by us is based on discrete matrices. We denote
by D a random diagonal matrix with nonzero entries taken independently and uniformly at random
from the two-element set {-1, +1}. Furthermore, we will denote by H a Hadamard matrix obtained
via Kronecker-products (see: Choromanski et al. (2017)). An m-vector sample from the discrete-
orthogonal joint distribution is obtained by taking m first rows of matrix G defined as GHAD =
HDi ∙ ... ∙ HDk for: fixed k > 0, independent copies Di of D and then renormalizing each
row in exactly the same way as we did it for continuous-orthogonal joint distributions. Note that
GHAD is a product of orthogonal matrices, thus its rows are also orthogonal. The advantage of a
discrete approach is that it leads to a more time and space efficient method for computing random
feature maps (with the use of Fast Walsh-Hadamard Transform; notice that the Hadamard matrix
does not even need to be stored explicitly). This is not our focus in this paper though. Accuracy-
wise discrete-orthogonal distributions lead to slightly biased estimators (the bias is a decreasing
function of the dimensionality n). However as we have observed, in practice they give as accurate
PSRNN models as continuous-orthogonal distributions, consistently beating approaches based on
unstructured random features. One intuitive explanation of that phenomenon is that even though
in that setting kernel estimators are biased, they are still characterized by much lower variance
than those based on unstructured features. We leave a throughout theoretical analysis of discrete-
orthogonal joint distributions in the RNN context to future work.
5	The theory of the orthogonal kernel ridge regression
In this section we extend the theoretical guarantees of Yu et al. (2016) to give rigorous theoretical
analysis of the initialization phase of OPSRNN. Specifically, we provide theoretical guarantees for
kernel ridge regression with orthogonal random features, showing that they provide strictly better
spectral approximation of the ground-truth kernel matrix than unstructured random features. As a
5
Published as a conference paper at ICLR 2018
corollary, we prove that orthogonal random features lead to strictly smaller empirical risk of the
model. Our results go beyond second moment guarantees and enable us to provide the first expo-
nentially small bounds on the probability of a failure for random orthogonal transforms.
Before we state our main results, we will introduce some basic notation and summarize previous
results. Assume that labeled datapoints (xi, yi), where xi ∈ Rn, yi ∈ R for i = 1, 2, ..., are
generated as follows: yi = f *(Xi) + νi, where f * : Rn → R is a function that the model aims
to learn, and νi for i = 1, 2, ... are independent Gaussians with zero mean and standard deviation
σ > 0. The empirical risk of the estimator f : Rn → R is defined as follows:
1N
R(f) ≡ E{νi}i=ι,...,N [N £(f (Xi)- f *(Xi)沟，	(4)
N j=1
where N stands for a dataset size.
By fv*ec ∈ RN we denote a vector whose jth entry is f* (Xj). Denote by fKRR a kernel ridge
regression estimator applying exact kernel method (no random feature map approximation). Assume
that we analyze kernel K : Rn × Rn → R with the corresponding kernel matrix K. It is a well
known result (Alaoui & Mahoney, 2015; Bach, 2013) that the empirical risk of fKRR is given by the
formula:
R(fKRR) =N-1λ2(fv*ec)>(K+λNIN)-2fv*ec +N-1σ2Tr(K2(K+λNIN)-2),	(5)
where λ > 0 stands for the regularization parameter and IN ∈ RN ×N is an identity matrix.
Denote by fKRR an estimator based on some random feature map mechanism and by K the corre-
sponding approximate kernel matrix.
The expression that is used in several bounds on the empirical risk for kernel ridge regression (see
for instance Avron et al. (2017)) is the modified version of the above formula for R(fKRR), namely:
Rκ(fVec) ≡ N-1λ2(f*ec)>(K + λNIN尸工 + N-1σ2Sλ(K), where Sλ(K) ≡ Tr(K(K +
λNIN)-1). It can be easily proven that R(∕krr) ≤ RK(f*ec).
To measure how similar to the exact kernel matrix (in terms of spectral properties) a kernel matrix
obtained with random feature maps is, we use the notion of ∆-spectral approximation (Avron et al.,
2017).
Definition 1. For a given 0 < ∆ < 1, matrix A ∈ RN×N is a ∆-spectral approximation of a
matrix B ∈ RN×N if (1 - ∆)B	A (1 + ∆)B.
It turns out that one can upper-bound the risk R(fKRR) for the estimator fKRR in terms of the ∆
parameter if matrix Kb + λNIN is a ∆-spectral approximation of the matrix K + λNIN, as the next
result (Avron et al., 2017) shows:
Theorem 1. Suppose that kKk2 ≥ 1 and that matrix Kb + λNIN obtained with the use of random
features is a ∆-spectral approximation of matrix K + λNIN. Then the empirical risk R(fKRR) of
the estimator fKRR satisfies:
Λ	Λ	1 ∕T≥∖
R(f≡R) ≤ γ1δ R Kfec) +	J σ2 .	(6)
1-∆	1+∆	N
5.1	Superiority of the orthogonal features for kernel ridge regression
Consider the following RBF kernels, that we call smooth RBFs. As we show next, Gaussian kernels
are smooth.
Definition 2 (smooth RBFs). We say that the class of RBF kernels defined by a fixed φ : R → R
(different elements of the class corresponds to different input dimensionalities) and with associ-
ated Sequence of probabilistic measures {μ1,μ2,…} (μi ∈ M(Ri)) is smooth if there exists a
6
Published as a conference paper at ICLR 2018
nonincreasing function f : R → R such that f (x) → 0 as x → ∞ and furthermore the
kth moments of random variables Xn = ∣∣w∣∣, where W 〜 μn satisfy for every n,k ≥ 0:
E[Xn] ≤ (n - 1)(n + 1) ∙ ... ∙ (n + 2k - 3)k!fk(k).
Many important classes of RBF kernels are smooth, in particular the class of Gaussian kernels. This
follows immediately from the well-known fact that for Gaussian kernels the above kth moments are
given by the following formula: E[Xn] = 2k (n(+--)I)! for n > 1.
Our main result is given below and shows that orthogonal random features lead to tighter bounds on
∆ for the spectral approximation ofK + λNIN. Tighter bounds on ∆, as Theorem 1 explains, lead
to tighter upper bounds also on the empirical risk of the estimator. We will prove it for the setting
where each structured block consists of a fixed number l > 1 of rows (note that many independent
structured blocks are needed if m > n), however our experiments suggest that the results are valid
also without this assumption.
Theorem 2 (spectral approximation). Consider a smooth RBF (in particular Gaussian kernel). Let
∆iid denote the smallest positive number such that Kiid + λNIN is a ∆-approximation of K +
λNIN, where Kiid isan approximate kernel matrix obtained by using unstructured random features.
Then for any a > 0,
P[∆iid >a] ≤ PNuaσmin),	⑺
where: piNid,m is given as: piNid,m (x) = N 2e-Cmx2 for some universal constant C > 0, m is the
number of random features used, σmin is the smallest singular value ofK+ λN IN and N is dataset
size. If instead orthogonal random features are used then for the corresponding spectral parameter
∆ort the following holds:
P[∆ort >a] ≤ PNtm( aσmin ),	(8)
where function poNr,tm satisfies: poNr,tm < piNid,m for n large enough.
We see that both constructions lead to exponentially small (in the number of random features m
used) probabilities of failure, however the bounds are tighter for the orthogonal case. An exact
formula on PoNr,tm can be derived from the proof that we present in the Appendix, however for clarity
we do not give it here.
Theorem 2 combined with Theorem 1 lead to risk bounds for the kernel ridge regression model based
on random unstructured and random orthogonal features. We use the notation introduced before and
obtain the following:
Theorem 3. Under the assumptions of Theorem 1 and Theorem 2, the following holds for the kernel
ridge regression risk and any c > 0 if m-dimensional unstructured random feature maps are used to
R	R	Rk (f*一)
approximate a kernel:	P[R(∕krr)	>	c]	≤	PNdm(	N ), where	ac IS g^ven as:	ac	= 1-----mvec1
,	C 2N
and the probability is taken with respect to the random choices of features. If instead random
aσ
OrthogonalfeatureS are used, we obtain thefollowing bound: P[R(∕krr) > c] ≤ PNtm( Nm ).
As before, since for large n function PoNr,tm satisfies PoNr,tm < PiNid,m , for orthogonal random features
we obtain strictly smaller upper bounds on the failure probability regarding empirical risk than for
the state-of-the-art unstructured ones. In practice, as we will show in the experimental section, we
see gains also in the regimes of moderate dimensionalities n.
6	Experiments
In section 5 we extended the theoretical guarantees for ORFs to the case of the initialization phase of
OPSRNNs. In this section we confirm these results experimentally and show that they imply better
performance of the entire model by comparing the performance of PSRNNs with that of OPSRNNs
on a selection of robotics time-series datasets. Since OPSRNN models obtained via continuous-
orthogonal and discrete-orthogonal joint sampling (see: Section 4) gave almost the same results,
presented OPSRNN-curves are for the continuous-orthogonal setting.
7
Published as a conference paper at ICLR 2018
6.1	Experimental Setup
We now describe the datasets and model hyperparameters used in our experiments. All models were
implemented using the Tensorflow framework in Python.
We use the following datasets in our experiments:
•	Swimmer We consider the 3-link simulated swimmer robot from the open-source package
OpenAI gym.2. The observation model returns the angular position of the nose as well as
the (2D) angles of the two joints, giving in a total of 5 features. We collect 25 trajectories
from a robot that is trained to swim forward (via the cross entropy with a linear policy),
with a train/test split of 20/5.
•	Mocap A Human Motion Capture dataset consisting of48 skeletal tracks from three human
subjects collected while they were walking. The tracks have 300 time steps each, and
are from a Vicon motion capture system. We use a train/test split of 40/8. There are 22
total features consisting of the 3D positions of the skeletal parts (e.g., upper back, thorax,
clavicle).
•	Handwriting This is a digital database available on the UCI repository (Alpaydin & Al-
imoglu, 1998) created using a pressure sensitive tablet and a cordless stylus. Features are
x and y tablet coordinates and pressure levels of the pen at a sampling rate of 100 millisec-
onds giving a total of 3 features. We use 25 trajectories with a train/test split of 20/5.
•	Moving MNIST Pairs of MNIST digits bouncing around inside of abox according to ideal
physics. http://www.cs.toronto.edu/~nitish/unsupervised_video/.
Each video is 64x64 pixels single channel (4096 features) and 20 frames long. We use
1000 randomly selected videos, split evenly between train and test.
In two-stage regression we use history (similarly future) features consisting of the past (next) 2
observations concatenated together. We use a ridge-regression parameter of 10(-2) (this is consistent
with the values suggested in Boots et al. (2013); Downey et al. (2017)). The kernel width is set to
the median pairwise (Euclidean) distance between neighboring data points. We use a fixed learning
rate of 0.1 for BPTT with a BPTT horizon of 20. We use a single layer PSRNN.
We optimize and evaluate all models with respect to the Mean Squared Error (MSE) of one step
predictions (this should not be confused with the MSE of the pointwise kernel approximation which
does not give the downstream guarantees we are interested in here). This means that to evaluate the
model we perform recursive filtering on the test set to produce states, then use these states to make
predictions about observations one time step in the future.
6.2	Results
6.2.1	Orthogonal RF for 2SR
In our first experiment we examine the effectiveness of Orthogonal RF with respect to learning a
good PSRNN via 2SR. In figure 3 we compare the MSE for a PSRNN learned via Orthogonal RF
with that of one learned using Standard RF for varying numbers of random features. Note that these
models were initialized using 2SR but were not refined using BPTT. We see that in all cases when
the ratio of RF to input dimension is small Orthogonal RF significantly outperforms Standard RF.
This difference decreases as the number of RF increases, with both approaches resulting in similar
MSE for large RF to input ratios.
6.2.2	Orthogonal RF for BPTT
In our second experiment we examine the effectiveness of Orthogonal RF with respect to learning
a good PSRNN via 2SR initialization combined with refinement via BPTT. In figure 4 we compare
the MSE for a PSRNN learned via Orthogonal RF with that of one learned using Standard RF over
a number of epochs of BPTT. We see that on all datasets, for both Orthogonal RF and Standard
RF, MSE decreases as the number of epochs increases. However it is interesting to note that in all
datasets Orthogonal RF converges to a better MSE than Standard RF.
2https://gym.openai.com/
8
Published as a conference paper at ICLR 2018
Figure 3: MSE for Orthogonal RF vs Standard RF after two stage regression
Mocap Data Set
Figure 4: MSE for Orthogonal RF vs Standard RF after two stage regression and BPTT
6.3	Discussion
These results demonstrate the effectiveness of Orthogonal RF as a technique for improving the per-
formance of downstream applications. First we have shown that Orthogonal RF can offer significant
performance improvements for kernel ridge regression, specifically in the context of the 2SR algo-
rithm for PSRNNs. Furthermore we have shown that not only does the resulting model have lower
error, it is also a better initialization for the BPTT gradient descent procedure. In other words, using
a model initialization based on orthogonal RF results in BPTT converging to a superior final model.
While the focus of these experiments was to compare the performance of PSRNNs and OPSRNNs,
for the sake of completeness we also include error plots for LSTMs. We see that OPSRNNs signifi-
cantly outperform LSTMs on all data sets.
7	Conclusions
We showed how structured orthogonal constructions can be effectively integrated with recurrent
neural network based architectures to provide models that consistently achieve performance superior
to the baselines. They also provide significant compression, achieving similar accuracy as PSRNNs
with an order of magnitude smaller number of features needed. Furthermore, we gave the first
theoretical guarantees showing that orthogonal random features lead to exponentially small bounds
on the failure probability regarding empirical risk of the kernel ridge regression model. The latter
one is an important component of the RNN based architectures for state prediction that we consider
in this paper. Finally, we proved that these bounds are strictly better than for the standard non-
orthogonal random feature map mechanism. Exhaustive experiments conducted on several robotics
task confirm our theoretical findings.
References
Alessandro Achille and Stefano Soatto. On the emergence of invariance and disentangling in
deep representations. CoRR, abs/1706.01350, 2017. URL http://arxiv.org/abs/1706.
01350.
N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss trans-
form. In STOC, 2006.
A. El Alaoui and M. Mahoney. Fast randomized kernel ridge regression with statistical guaran-
tees. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural
9
Published as a conference paper at ICLR 2018
Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp.
775-783, 2015.
E Alpaydin and Fevzi Alimoglu. Pen-based recognition of handwritten digits data set. University of
California, Irvine, Machine Learning Repository. Irvine: University of California, 1998.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New
York City, NY, USA, June 19-24, 2016, pp. 1120-1128, 2016. URL http://jmlr.org/
proceedings/papers/v48/arjovsky16.html.
H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker, and A. Zandieh. Random fourier features
for kernel ridge regression: Approximation bounds and statistical guarantees. In Proceedings of
the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017, pp. 253-262, 2017. URL http://proceedings.mlr.press/v70/
avron17a.html.
F. Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT 2013 - The 26th Annual
Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pp. 185-209,
2013. URL http://jmlr.org/proceedings/papers/v30/Bach13.html.
M. Bojarski, A. Choromanska, K. Choromanski, F. Fagan, C. Gouy-Pailler, A. Morvan, N. Sakr,
T. Sarlos, and J. Atif. Structured adaptive and random spinners for fast machine learning compu-
tations. In AISTATS, 2017.
Byron Boots, Geoffrey J. Gordon, and Arthur Gretton. Hilbert space embeddings of predictive state
representations. CoRR, abs/1309.6819, 2013. URL http://arxiv.org/abs/1309.6819.
KyungHyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259, 2014a.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014b.
A. Choromanska, K. Choromanski, M. Bojarski, T. Jebara, S. Kumar, and Y. LeCun. Binary em-
beddings with structured hashed projections. In ICML, 2016.
K. Choromanski and V. Sindhwani. Recycling randomness with structure for sublinear time kernel
expansions. In ICML, 2016.
K. Choromanski, M. Rowland, and A. Weller. The unreasonable effectiveness of structured random
orthogonal embeddings. In to appear in NIPS, volume arXiv abs/1703.00864, 2017.
Carlton Downey, Ahmed Hefny, Boyue Li, Byron Boots, and Geoffrey Gordon. Predictive state
recurrent neural networks. arXiv preprint arXiv:1705.09353, 2017.
Surya Ganguli, Dongsung Huh, and Haim Sompolinsky. Memory traces in dynamical systems.
105(48):18970 18975, 2008. doi: 10.1073/pnas.0804451105., 2008.
Ahmed Hefny, Carlton Downey, and Geoffrey J. Gordon. Supervised learning for dynamical system
learning. In Advances in Neural Information Processing Systems, pp. 1963-1971, 2015.
Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end text-dependent
speaker verification. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE Interna-
tional Conference on, pp. 5115-5119. IEEE, 2016.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016,
New York City, NY, USA, June 19-24, 2016, pp. 2034-2042, 2016. URL http://jmlr.org/
proceedings/papers/v48/henaff16.html.
A. Hinrichs and J. Vyblral. Johnson-Lindenstrauss lemma for circulant matrices. Random Structures
& Algorithms, 39(3):391-398, 2011.
10
Published as a conference paper at ICLR 2018
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural ComPut, 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.
Michael L. Littman, Richard S. Sutton, and Satinder Singh. Predictive rePresentations of state. In
In Advances In Neural Information Processing Systems 14, PP. 1555-1561. MIT Press, 2001.
Tomas Mikolov, Martin Karafiat, Lukas BurgeL Jan Cernocky, and Sanjeev Khudanpur. Recurrent
neural network based language model. In IntersPeech, volume 2, PP. 3, 2010.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information Processing systems, PP. 1177-1184, 2008.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlin-
ear dynamics of learning in deeP linear neural networks. CoRR, abs/1312.6120, 2013. URL
http://arxiv.org/abs/1312.6120.
I.	Schoenberg. Metric SPaces and ComPletely Monotone Functions. The Annals of Mathematics, 39
(4):811-841, 1938.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory, PP. 13-31. SPringer,
2007.
J.	Vyblral. A variant of the Johnson-Lindenstrauss lemma for circulant matrices. Journal of Func-
tional Analysis, 260(4):1096-1105, 2011.
O. White, D. Lee, and H. Sompolinsky. Short-term memory in orthogonal neural networks. Physical
Review Letters, 92, 2004. ISSN 14., 2004.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International Conference on Machine Learning, pp. 2048-2057, 2015.
F. Yu, A. Suresh, K. Choromanski, D. Holtmann-Rice, and S. Kumar. Orthogonal random features.
In NIPS, pp. 1975-1983, 2016.
H. Zhang and L. Cheng. New bounds for circulant Johnson-Lindenstrauss embeddings. CoRR,
abs/1308.6339, 2013.
11
Published as a conference paper at ICLR 2018
Appendix: Initialization Matters: Orthogonal Predictive State
Recurrent Neural Networks
We will use the notation from the main body of the paper.
8 Proof of Theorem 2
We will assume that a dataset X = {x1, ..., xN} under consideration is taken from a ball B of a
fixed radius r (that does not depend on data dimensionality n and dataset size N) and center x0 . We
begin with the following lemma:
Lemma 1. Fix an RBF kernel K : Rn × Rn . Consider a randomized kernel estimator Kb with
a corresponding random feature map: Φm,n : Rn → R2m and assume that for any fixed i, j ∈
{1,…，N} the followig holds for any C > 0: P[∣Φm,n(xi)>Φm,n(xj) 一 K (xi, Xj )| > c] ≤ g(c)
for some fixed function g : R → R. Then with probability at least 1 - N2g(c), matrix Kb + λIN is
a ∆-spectral approximation of matrix K + XIN for ∆ = Nc-, where Gm® stands for the minimal
σmin
singular value ofK + λIN.
Proof. Denote K + λNIN = V> Σ2V, where an orthonormal matrix V ∈ RN×N and a diagonal
matrix Σ ∈ RN×N define the eigendecomposition of K + λNIN. Following Avron et al. (2017),
we
notice that in order to prove that Kb + λIN is a ∆-
to show that:
kΣ-1VKb V>Σ-1 一 Σ-1VKV>Σ-1 k2 ≤ ∆.
spectral approximation of K + λIN, it suffices
(9)
From basic properties of the spectral norm kk2 and the Frobenius norm kkF we have:
P[kΣ-1VKb V>Σ-1 一 Σ-1VKV>Σ-1k2 > ∆] ≤ P[kΣ-1Vk2kKb 一 KkF kV>Σ-1k2 > ∆]
(10)
The latter probability is equal toP = P[∣∣K — KkF > k∑-1vk2∆V>∑-ik2].
Furthermore, since V is an isometry matrix, We have: k∑-1V∣∣2 ≤ σ1- and ∣∣V>∑-1k2 ≤ σ1-.
Thus we have:
P ≤ P[kK — KkF > ∆2σmm].	(11)
NoW notice that from the union bound We get:
P ≤ N 2P[∣K (i,j) — K(i,j )| > ∆mn ] = N 2P[∣Φm,n(Xi)>Φm,n(Xj ) - K(i,j)∣ > ^mn ].
(12)
Therefore the probability that Kb + λIN is a ∆-spectral approximation of K + λIN is at least
1 — N2g(c) for c = ʌʒmin and that completes the proof.
□
Our goal right noW is to compute function g from Lemma 1 for random feature maps constructed
according to tWo procedures: the standard one based on independent sampling and the orthogonal
one, Where marginal distributions corresponding to the joint distribution (w1, ..., wm) are the same,
but vectors wi are conditioned to be orthogonal.
We start With a standard random feature map mechanism. Note first that from basic properties of the
trigonometric functions We conclude that for any tWo vectors X, y ∈ Rn , the random feature map
approximation of the RBF kernel K(x, y) which is of the form K(x, y) = ∣Φm,n(x)>Φm,n(y) can
be equivalently rewritten as: K(x, y)= * Pm=I cos(w>Z) for Z = X - y. This is true for any
joint distribution (w1, ..., wm).
Lemma 2. If mapping Φm,n is based on the standard mechanism of independent sampling then one
can take as function g from Lemma 1 a function given by the following formula: g(x) = e-Cmx2
for some universal constant C > 0.
12
Published as a conference paper at ICLR 2018
Proof. Notice first that by the remark above, we get: P[∣Φm,n(xi)>Φm,n(xj) - K (xi, Xj )| > x]=
PPm=I Zi > x], where Zi = 煮 cos(w>Z)- *φ(z), Z = ∣∣x - y∣∣2 and φ is a positive definite
function associated with an RBF kernel K. From the unbiasedness of the estimator we see that
E[Zi] = 0. Also, notice that: |Zi | ≤ m and different Zis are independent. Thus, from the standard
application of Chernoff inequality we get: P[Pin=1 Zi > x] ≤ e-Cmx2 for some universal constant
C > 0 (that does not depend on m). That completes the proof.	□
By combining Lemma 1 with the formula on g for the standard unstructured case from Lemma 2,
we already obtain the formula for piNid,m from the statement of the theorem. It remains to show that:
ort iid
pN,m < pN,m.
Note that in the previous proof the upper bound on g is derived as a monotonic function of
E[et im=1 Zi] for a parameter t > 0 (that is then being optimized),as it is the case for the stan-
dard Chernoff’s argument. Now, since variables Zi are independent, we obtained: E[et im=1 Zi] =
Qim=1 E[etZi]. Thus, if we can prove that for the continuous-orthogonal distribution we have:
E[et im=1 Zi] < Qim=1 E[etZi], then we complete the proof of Theorem 2 (note that the marginal dis-
tributions of Zi are the same for both: standard mechanism based on unstructured random features
and the one based on continuous-orthogonal sampling of the m-tuple of n-dimensional vectors).
This is what we prove below.
Lemma 3. Fix some z ∈ Rn and t > 0. For a sample (w1ort, ..., wmort) from the continuous-
orthogonal distribution the following holds for n large enough:
m
E[et Pm=IcOs((Wort)>z)] < Y E[e*os((wOrt)>z)].	(⑶
i=1
Proof. Since different blocks of vectors wi used to construct the orthogonal feature map are inde-
pendent (the number of blocks is greater than one if m > n), it suffices to prove the inequality just
for one block. Thus from now on we will focus just on one block and thus without loss of generality
we will assume that m ≤ n.
Note first that
m
Y E[en cos((Wort)TZ)] = E[en Pm=IcOs((Wiid)TZ)],	(14)
i=1
where (w1iid, ..., wimid) stands for the m-tuple sample constructed by the standard unstructured mech-
anism.
Thus we need to prove that
E[ent Pm=I cos((wort)>z)] < E[eɪ Pm=IcOs((Wiid)>z)]	(15)
□
Using Taylor expansion for ex, we conclude that it sufficies to prove that:
X	(马jι+...+jmE[cos((wort)>z)jι ∙…∙cos((Wmt)>z)jm]
乙J	ri	j1! ∙ ... ∙ jk!
j1,j2,...,jm	1	k
<
X	(马jι + ...+jmE[cos((w1id)>z)jl ∙ ... ∙ cos((Wmd)>z)jm ]
乙J	rι	jι! ∙... ∙ jk!	，
j1,j2,...,jm	1	k
(16)
i.e. that:
X (:)j1
j1,j2,...,jm
+...+jm
1
• ... ∙ jm !
Λ(j1,..., jm) > 0,
(17)
13
Published as a conference paper at ICLR 2018
where:
Λ(jι,…,jk) = E[cos((w1id)>z)j1 •…∙ Cos((Wmd)>z)jm - cos((w0rt)>z)j1 •…∙cos((w^Tt)>z)jm]
(18)
By applying the trigonometric formula:
cos(α) cos(β) = ɪ(eos(a + β) + cos(α — β)),	(19)
we get:
A(ji，…,jk ) = 2jl + l.+jm	X	E[
s1 ,...,sj1 +...+jm ∈{-1,+1}	(20)
Cos (((WIid ZlsI w1id 0S2 ... XsjI-I WIid) 0Sjι ...)>z)—
Cos (((Wort Zsi Wort Zs2 ... ZsjI-I Wort) ZsjI …)>z)],
where Z1 stands for vector-addition operator and Z-1 stands for vector-subtraction operator.
Note that without loss of generality we can assume that sj1 = sj1+j2 = ... = +1, since for other
configurations we obtain a random variable of the same distribution. Consider a fixed configuration
(s1, s2, ..., sj1+...+jm) and the corresponding term of the sum above that can be rewritten in the
compressed way as:
F = E[Cos(n1Wi1id +n2Wi2id +...+nm Wmiid)>z]—E[Cos(n1W1ort+n2W2ort+...+nm Wmort)>z], (21)
for some n1, ..., nm ∈ Z. Without loss of generality, we can assume that n1, ..., nm ∈ N, since the
distribution of the random variables under consideration does not change if ni is replaced with —ni.
Without loss of generality we can also assume that there exists i ∈ {1, ..., m} such that ni > 0,
since otherwise the corresponding term F is equal to 0.
Denote by R1, ..., Rm the set of independent random variables, where each is characterized by
the distribution which is the distribution of vectors Wiiid (and thus also of vectors Wiort). Denote:
R = n2R2+ + ... + n2nRn. Note that nWort + nW(Or + ... 十 nmWort 〜 Rv, where v is a
unit L2-norm vector taken uniformly at random from the sphere of radius 1 and furthermore: R and
v are chosen independently. That is implied by the isotropicity of vectors Wiort. Similarly, denote
Rb = R2+ Pi,j∈{1,...,m} ninjRiRjvi>vj, where v1, ..., vm stand for the independent copies of
v. Note that, by the similar analysis as before, We conclude that nιWfd + n2W(id + ... + nmWimd 〜
Rv and furthermore, R and v are chosen independently.
Therefore, by expanding Cos(x) using Taylor expansion, We get:
∞
F=X
k=0
kzk2k(—1)kE[(v>bz)2k]
(2k)!
∞
E[Rb2k] — X
k=0
kzk2k(—1)kE[(v>zb)2k]
(2k)!
E[R2k],	(22)
where: b =亩.Denote: A(k, n) = E[(v>b)k] (note that v, b ∈ Rn). It is easy to see that for odd
k We have: A(n, k) = 0. We obtain:
F = X kzk2k(-1)kA(2k,n) (E[R2k] — E[R2k]).	(23)
k=0	(2 )
The following technical fact will be useful:
Lemma 4. Expression A(k, n) is given by the following formula:
1π
Alknn= R0∏ smn-2(θ)dθ J。Hi	(24)
14
Published as a conference paper at ICLR 2018
which can be equivalently rewritten (by computing the integrals explicitly) as:
A(2k n) = (n -	2)(n	-	4)	•…∙	δ(n = 2)_____________(2k - 1)!!
，	(n —	3)(n	—	5)	•	...	∙	γ(n = 2)	(n — 1)(n + 1)...(n + 2k	— 3)
(n + 2k	—	3)(n + 2k — 5)... ∙	γ(n	= 2)
(n + 2k	—	2)(n + 2k — 4)... ∙	δ(n	= 2)
(25)
where: δ(n = 2) = 2 if n = 2 and δ(n = 2) = 1 otherwise and: γ(n = 2) = 1 if n = 2 and
γ(n = 2) = 2 otherwise.
In particular, the following is true:
(2k— 1)!!
|A(2k,n)| ≤ (n — 1)(n +1) • ... ∙ (n + 2k — 3).
(26)
We will use that later. Note that v> Vj 〜v>b. Therefore We obtain:
F
∞
X
k=0
kzk2k ( —1)k A(2k,n)
(W
αk,
(27)
where
αk
E[(R2)k-iλi],
(28)
and λ = Pi,j∈{1,...,m} ninjRiRjvi>vj .
Note that E[(R2)k-1λ] = 0 since E[(vi>vj)] = 0 and furthermore, directions v1, ..., vm are chosen
independently from lengths R1, ..., Rm. Therefore we have:
αk = k2 E[(R2)k-2λ2] +βk,	(29)
where:
βk = Xk	kiE[(R2)k-iλi].	(30)
i=3
Now let us focus on a single term ρ = E[(R2)k-lλl] for some fixed l ≥ 3.
Note that the following is true:
P ≤ E[(R2)kT(	∑	ninjRiRj)l] ∙ jXxljl EHvjι l∙ ... ∙Kvjι1]，
i,j∈{1,...,m}
(31)
where the maximum is taken over i1, j1, ..., il, jl ∈ {1, ..., m} such that is 6= js for s = 1, ..., l.
Note first that:
E[(R2)k-l (	X	ninjRiRj)l] ≤
i,j∈{1,...,m}
E[(R2)kτ(	X	(niRi)2 + (njRj)2 )l] ≤ E[(R2)i(m — 1)l(R2)l] ≤ (m — 1)lE[R2k].
i,j∈{1,...,m}
(32)
Let Us focus now on the expression maxxιjι,…,xljl E[∣v>Vjι ∣∙ ... ∙ ∣v>Vjl |].
We will prove the following upper bound on maxxiji . iljl E[∣v> vj∙∕∙... ∙ ∣v>Vjl |].
15
Published as a conference paper at ICLR 2018
Lemma 5. The following is true:
E[lv>vjι | ∙ ... ∙ |v>vjl |] ≤ (
log(n)
n- - √n log(n)
、l 7 /	log2(n)
)l + l(e--
log2(n)、
+ e——L).
(33)
Proof. Note that from the isotropicity of Gaussian vectors we can conclude that each single |Vi>Vjs |
is distributed as:
gι
ʌ/ g2+...+gn
, where g1 , ..., gn stand for n independent copies of a random variable
taken from N (0, 1). Note that g12 + ... + gn2 is taken from the χ2n- distribution. Using the well-
known bounds for the tails of χ2n- distributions, we get: P[g12 + ... + gn2 - n ≤ a] ≤ e
x2
——=—
a2一
4n . Note
also that P[∣gι∣ > x] ≤ 2e√∏. Thus, by taking: a = √log(n), X = log(n) and applying
union bound, we conclude that with probability at least 1 - e
log2 (n)
log2 (n)
a fixed random
variable |Vi>sVjs | satisfies: |Vi>sVjs | ≤
ιog(n) ==. Thus, by the union bound We conclude that
√n-√n log(n)
for any fixed i1,j1,…,iι,jι random variable |v> Vji ∣∙... ∙ ∣v>Vjι | satisfies： |v› Vji ∣∙…∙ ∣v>Vjι | ≤
(/ log(n) == )l with probability at least 1 一 l(e- log4(n)
√n-√n log(n)
is upper bounded by one, we ConClude that:
+ e-log2ni ). SinCe lv>vji | ∙ ... ∙ |v> vjl |
E[|v>vji | ∙ ... ∙ |v>vjl |] ≤ (
log(n)
n- - √n log(n)
)l + l(e-
log2 (n)
4^
log2 (n)
).
(34)


4— e 2

+ e 2

□
Using Lemma 5, we can conclude that:
P ≤ (m — 1)lE[R2k] ∙(
log(n)
-—-√-log(n)
、l 7 /	log2(n)
)l + l(e--
+e
log2(n)
)
(35)

Therefore we have:
βk ≤ XX C)(m- 1)iE[R2k]∙((
log(n)
Jn - √log(n)
)i + i(e-
log2 (n)
log2 (n)
(36)
4+ e 2

)
Thus we can conclude that for n large enough:
βk ≤ k(2m)k(2log3(n) + 2ke-吗(n1)E[R2k].	(37)
n 2
Thus we get:
F = XX kzk2k (-1)；A(2k，n) (k)E[(R2)f2]+Γ,	(38)
k=0	(2k)!	2
where:
∣Γ∣ ≤ XX *户k(2m)k(2⅛) + 2ke-0)E[R2k]
k=0	(2k)!	n 2	(39)
=2log3(n) A + 2e-吗区 B,
n 2
B satisfies: B = Ak and A is given as:
∞
A=X
k=0
kzk2kE[R2k]A(2k, n)
k(2m)k.
(40)
(2k)!
16
Published as a conference paper at ICLR 2018
Now note that since data is taken from the ball of radius r, we have: kzk ≤ 2r. Furthermore, from
the smoothness of the considered class of RBF kernels, we obtain:
E[R2k] ≤ max n2kmk(n — 1)(n + 1) ∙ .. ∙ (n + 2k — 3)fk(k)k!.	(41)
i=1,...,m
Denote h = argmaxi=ι...,mni. Note that (2k 一 1)!! = (2k). Thus, by applying the above upper
bound on A(2k, n), we obtain:
∞∞
A, B ≤ X(m2(2r)2n2hf(k))kk2 ≤ X(4m2 (2r)2n2hf(k))k.	(42)
k=0	k=0
Now, notice that for a given nh, m and r, the above upper bound is of the form Pk∞=0 qkk, where
qk → 0 as k → ∞ (since fk → 0 as k → ∞ for smooth RBF kernels). Then we can conclude that
the contribution of the terms Γ to the expression T = Aj1,…,^ from the LHS of 17 is of the order
j	jι !•…jm!
O(3)(notice that every n satisfies: n ≤ ji). Now let us consider F 一 Γ. We have:
n 2
F 一 Γ
∞
X
k=0
kzk2k( —1)kA(2k,η) (k
(2k)!
E[(R2)k-2λ2]
(43)
2
By the same analysis as before we conclude that
F 一 γ = P + on( — ),
n
(44)
where
∞
ρ=X
k=0
kzk2k( —1)kA(2k,n) A
(2k)!
E[(Rb2)k-2λ2]
(45)
2
and futhermore: P = np + 0n(ɪ), where
ρ
∞
WX
k=0
kzk2k( —1)kA(2k,n) (k∖
(W	3
E[(Rb2k)]
(46)
and W is some constant (that depends on m). Thus to show Inequality 17 for n large enough, it
suffices to show that P > 0 and that P does not depend on n (even though n is explicitly embedded
in the formula on P via A(2k, n)). This follows from the straightforward extension (for different
nis) of the fact that for nι = ... = nm, P can be rewritten in terms of the positive definite function
φ describing an RBF kernel under consideration, namely:
1	2 d2(φm(x))	d(φm(x))
P = 8n((HnIZk	dχ2	)lχ=knιzk 一 (kn1zk	dχ	)|x=kniZk).	(47)
That the above expression is positive is implied by the fact that every positive definite function φ
(not parametrized by n) can be rewritten as φ(r) = σ(r2 ), where σ is completely monotone on
[0, +∞] and from the basic properties of completely monotone functions (see: Schoenberg (1938)).
That completes the proof of the theorem.
9 Proof of Theorem 3
The theorem follows straightforwardly from Theorem 1, Theorem 2 and the fact that rank(K) ≤ m
(since m-dimensional random feature maps are used for kernel approximation).
17