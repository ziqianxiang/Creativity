Published as a conference paper at ICLR 2018
Boosting the Actor with Dual Critic
Bo Dai*1, Albert Shaw*1, Niao He2, Lihong Li3, Le Song1, 4
1	Georgia Institute of Technology, 2 University of Illinois at Urbana-Champaign
3	Google AI, 4 Ant Financial Services Group
1	{bodai, ashaw596}@gatech.edu, lsong@cc.gatech.edu
2	niaohe@illinois.edu, 3 lihongli.cs@gmail.com
Ab stract
This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or
Dual-AC. It is derived in a principled way from the Lagrangian dual form of the
Bellman optimality equation, which can be viewed as a two-player game between
the actor and a critic-like function, which is named as dual critic. Compared to
its actor-critic relatives, Dual-AC has the desired property that the actor and dual
critic are updated cooperatively to optimize the same objective function, provid-
ing a more transparent way for learning the critic that is directly related to the
objective function of the actor. We then provide a concrete algorithm that can
effectively solve the minimax optimization problem, using techniques of multi-
step bootstrapping, path regularization, and stochastic dual ascent algorithm. We
demonstrate that the proposed algorithm achieves state-of-the-art performance
across several benchmarks.
1	Introduction
Reinforcement learning (RL) algorithms aim to learn a policy that maximizes the long-term return
by sequentially interacting with an unknown environment. Value-function-based algorithms first
approximate the optimal value function, which can then be used to derive a good policy. These
methods (Sutton, 1988; Watkins, 1989) often take advantage of the Bellman equation and use boot-
strapping to make learning more sample efficient than Monte Carlo estimation (Sutton & Barto,
1998). However, the relation between the quality of the learned value function and the quality of the
derived policy is fairly weak (Bertsekas & Tsitsiklis, 1996). Policy-search-based algorithms such
as REINFORCE (Williams, 1992) and others (Kakade, 2002; Schulman et al., 2015a), on the other
hand, assume a fixed space of parameterized policies and search for the optimal policy parameter
based on unbiased Monte Carlo estimates. The parameters are often updated incrementally along
stochastic directions that on average are guaranteed to increase the policy quality. Unfortunately,
they often have a greater variance that results in a higher sample complexity.
Actor-critic methods combine the benefits of these two classes, and have proved successful in a
number of challenging problems such as robotics (Deisenroth et al., 2013), meta-learning (Bello
et al., 2016), and games (Mnih et al., 2016). An actor-critic algorithm has two components: the
actor (policy) and the critic (value function). As in policy-search methods, actor is updated towards
the direction of policy improvement. However, the update directions are computed with the help of
the critic, which can be more efficiently learned as in value-function-based methods (Sutton et al.,
1999; Konda & Tsitsiklis, 2003; Peters et al., 2005; Bhatnagar et al., 2009; Schulman et al., 2015b).
Although the use of a critic may introduce bias in learning the actor, its reduces variance and thus
the sample complexity as well, compared to pure policy-search algorithms.
While the use of a critic is important for the efficiency of actor-critic algorithms, it is not entirely
clear how the critic should be optimized to facilitate improvement of the actor. For some parametric
family of policies, it is known that a certain compatibility condition ensures the actor parameter
update is an unbiased estimate of the true policy gradient (Sutton et al., 1999). In practice, temporal-
difference methods are perhaps the most popular choice to learn the critic, especially when nonlinear
function approximation is used (e.g., Schulman et al. (2015b)).
*Both authors equally contributed to the paper.
1
Published as a conference paper at ICLR 2018
In this paper, we propose a new actor-critic-style algorithm where the actor and the critic-like func-
tion, which we named as dual critic, are trained cooperatively to optimize the same objective func-
tion. The algorithm, called Dual Actor-Critic , is derived in a principled way by solving a dual
form of the Bellman equation (Bertsekas & Tsitsiklis, 1996). The algorithm can be viewed as a
two-player game between the actor and the dual critic, and in principle can be solved by standard
optimization algorithms like stochastic gradient descent (Section 2). We emphasize the dual critic
is not fitting the value function for current policy, but that of the optimal policy. We then show
that, when function approximation is used, direct application of standard optimization techniques
can result in instability in training, because of the lack of convex-concavity in the objective function
(Section 3). Inspired by the augmented Lagrangian method (Luenberger & Ye, 2015; Boyd et al.,
2010), we propose path regularization for enhanced numerical stability. We also generalize the
two-player game formulation to the multi-step case to yield a better bias/variance tradeoff. The full
algorithm is derived and described in Section 4, and is compared to existing algorithms in Section 5.
Finally, our algorithm is evaluated on several locomotion tasks in the MuJoCo benchmark (Todorov
et al., 2012), and compares favorably to state-of-the-art algorithms across the board.
Notation. We denote a discounted MDP by M = (S, A, P, R, γ), where S is the state space, A
the action space, P(∙∣s, a) the transition probability kernel defining the distribution over next-state
upon taking action a in state x, R(s, a) the corresponding immediate rewards, and γ ∈ (0, 1) the
discount factor. If there is no ambiguity, we will use Pa f(a) and f (a)da interchangeably.
2	Duality of Bellman Optimality Equation
In this section, we first describe the linear programming formula of the Bellman optimality equa-
tion (Bertsekas, 1995; Puterman, 2014), paving the path fora duality view of reinforcement learning
via Lagrangian duality. In the main text, we focus on MDPs with finite state and action spaces
for simplicity of exposition. We extend the duality view to continuous state and action spaces in
Appendix A.2.
Given an initial state distribution μ(s), the reinforcement learning problem aims to find a policy
∏(∙∣s) : S → P(A) that maximizes the total expected discounted reward with P(A) denoting all
the probability measures over A, i.e.,
Eso〜μ(s)E∏ [P∞=0 YiR(Si,ai)] ,	(1)
where Si+ι 〜P(∙∣Si, ai), ai 〜∏(∙∣Si).
Define V*(s) := max∏∈p(∕) E [P∞=0 γiR(si, ai)∣so = s] , the Bellman optimality equation states
that:
V*(S) = (TV*)(S) := max {R(s,a) + γEs[s,a [V*(s')]} ,	(2)
which can be formulated as a linear program (Puterman, 2014; Bertsekas, 1995):
P* := min (1 - Y)E§〜兴⑶[V(s)]	(3)
s.t. V(s) > R(s,a)+ γEs0∣s,a [V(s0)], ∀(s,a) ∈ S ×A.
For completeness, we provide the derivation of the above equivalence in Appendix A. Without loss
of generality, we assume there exists an optimal policy for the given MDP, namely, the linear pro-
gramming is solvable. The optimal policy can be obtained from the solution to the linear program (3)
via
π*(s) = argmax {R(s, a) + YEs0|s,a [V* (s0)]} .	(4)
The dual form of the LP below is often easier to solve and yield more direct relations to the optimal
policy.
D* := maxρ>0	R(s, a)ρ(s, a)	(5)
(s,a)∈S×A
s.t.	Pa∈A ρ(s0,a) = (I-Y )μ(s0) + Y Ps,a∈S×A ρ(s,a)P(SlS,a)ds, ∀s0 ∈ S
Since the primal LP is solvable, the dual LP is also solvable, and P* - D* = 0. The optimal dual
variables ρ*(s, a) and optimal policy π* (a|s) are closely related in the following manner:
2
Published as a conference paper at ICLR 2018
Theorem 1	(Policy from dual variables) Ps °∈s×a ρ*(s, a) = 1, and π*(a∣s) = P P (；；1 Ο).
Since the goal of reinforcement learning is to learn an optimal policy, it is appealing to deal with
the Lagrangian dual which optimizes the policy directly, or its equivalent saddle point problem that
jointly learns the optimal policy and value function.
Theorem 2	(Competition in one-step setting) The optimal policy π*, actor, and its corresponding
valuefunction V *, dual critic, is the solution to the following saddle-point problem
max min
α∈P (S,π∈P (A V
L(V,α,π) := (1 - Y)Es〜“(s) [V4 (S)] + E	α(s)π⑷S)δ∣v]Ga),⑹
(s,a∈S ×A
where ∆[V](s,a) := R(s,a) + γEso∣s,a[V(s0)] - V(S).
The saddle point optimization (6) provides a game perspective in understanding the reinforcement
learning problem (Goodfellow et al., 2014). The learning procedure can be thought as a game be-
tween the dual critic, i.e., value function for optimal policy, and the weighted actor, i.e., α(s)π(a∣s):
the dual critic V seeks the value function to satisfy the Bellman equation, while the actor π tries to
generate state-action pairs that break the satisfaction. Such a competition introduces new roles for
the actor and the dual critic, and more importantly, bypasses the unnecessary separation of policy
evaluation and policy improvement procedures needed in a traditional actor-critic framework.
3 Sources of Instability
To solve the dual problem in (6), a straightforward idea is to apply stochastic mirror prox (Ne-
mirovski et al., 2009) or stochastic primal-dual algorithm (Chen et al., 2014) to address the saddle
point problem in (6). Unfortunately, such algorithms have limited use beyond special cases. For
example, for an MDP with finite state and action spaces, the one-step saddle-point problem (6) with
tabular parametrization is convex-concave, and finite-sample convergence rates can be established;
see e.g., Chen & Wang (2016) and Wang (2017). However, when the state/action spaces are large
or continuous so that function approximation must be used, such convergence guarantees no longer
hold due to lack of convex-concavity. Consequently, directly solving (6) can suffer from severe bias
and numerical issues, resulting in poor performance in practice (see, e.g., Figure 1):
1.	Large bias in one-step Bellman operator: It is well-known that one-step bootstrapping
in temporal difference algorithms has lower variance than Monte Carlo methods and often
require much fewer samples to learn. But it produces biased estimates, especially when
function approximation is used. Such a bias is especially troublesome in our case as it
introduces substantial noise in the gradients to update the policy parameters.
2.	Absence of local convexity and duality: Using nonlinear parametrization will easily break
the local convexity and duality between the original LP and the saddle point problem, which
are known as the necessary conditions for the success of applying primal-dual algorithm
to constrained problems (Luenberger & Ye, 2015). Thus none of the existing primal-dual
type algorithms will remain stable and convergent when directly optimizing the saddle
point problem without local convexity.
3.	Biased stochastic gradient estimator with under-fitted value function: In the absence
of local convexity, the stochastic gradient w.r.t. the policy π constructed from under-fitted
value function will presumably be biased and futile to provide any meaningful improvement
of the policy. Hence, naively extending the stochastic primal-dual algorithms in Chen &
Wang (2016); Wang (2017) for the parametrized Lagrangian dual, will also lead to biased
estimators and sample inefficiency.
4	Dual Actor-Critic
In this section, we will introduce several techniques to bypass the three instability issues in the
previous section: (1) generalization of the minimax game to the multi-step case to achieve a better
bias-variance tradeoff; (2) use of path regularization in the objective function to promote local
convexity and duality; and (3) use of stochastic dual ascent to ensure unbiased gradient estimates.
3
Published as a conference paper at ICLR 2018
4.1	Competition in multi-step setting
In this subsection, we will extend the minimax game between the actor and critic to the multi-step
setting, which has been widely utilized in temporal-difference algorithms for better bias/variance
tradeoffs (Sutton & Barto, 1998; Kearns & Singh, 2000). By the definition of the optimal value
function, it is easy to derive the k-step Bellman optimality equation as
V*(s)= (TkV*) (S) =max∏∈P {eπ [P：=。YiR(si,a/ + γk+1Eπ [V*国+。]}.⑺
Similar to the one-step case, we can reformulate the multi-step Bellman optimality equation into a
form similar to the LP formulation, and then we establish the duality, which leads to the following
mimimax problem:
Theorem 3 (Competition in multi-step setting) The optimal policy π* and its CorresPonding
valuefunction V * is the solution to the following saddle point problem
MmaXM 八 min Lk(V,α,π) = (I-Yk+I)E” [V(S)] + Ea hδ f{si,ai}k=0 ,sk + l)i ,	⑻
α∈P(S),π∈P(A) V
where δ {Si, ai}ik=0 , Sk+1 = Pik=0 YiR(Si, ai) + Yk+1V (Sk+1) - V(S) and
k
Eπα δ {Si,ai}ik=0, Sk+1	= X	α(so) Y ∏(ai∣Si)p(si+ι∣Si, ai)δ ({si, ai}：=。, s：+i).
{si ,ai}ik=0,sk+1	i=0
The saddle-point problem (8) is similar to the one-step Lagrangian (6): the dual critic, V, and
weighted k-step actor, a(s。) Qk=o ∏(a∕si), are competing for an equilibrium, in which critic and
actor become the optimal value function and optimal policy. However, it should be emphasized
that due to the existence of max-operator over the space of distributions P(A), rather than A, in
the multi-step Bellman optimality equation (7), the establishment of the competition in multi-step
setting in Theorem 3 is not straightforward: i), its corresponding optimization is no longer a linear
programming; ii), the strong duality in (8) is not obvious because of the lack of the convex-concave
structure. We first generalize the duality to multi-step setting. Due to space limit, detailed analyses
for generalizing the competition to multi-step setting are provided in Appendix B.
4.2 Path Regularization
When function approximation is used, the one-step or multi-step saddle-point problems (8) will no
longer be convex in the primal parameters. This could lead to instability and even divergence when
solved by brute-force stochastic primal-dual algorithms. One then desires to partially convexify the
objectives without affecting the optimal solutions. The augmented Lagrangian method (Boyd et al.,
2010; Luenberger & Ye, 2015), also known as the method of multipliers, is designed and widely
used for such purposes. However, directly applying this method would require introducing penalty
functions of the multi-step Bellman operator, which renders extra complexity and challenges in
optimization. Interested readers are referred to Appendix B.2 for further details.
Instead, we propose to use path regularization, as a stepping stone for promoting local convex-
ity and computation efficiency. The regularization term is motivated by the fact that the op-
timal value function satisfies the constraint V(s) = Eπ* [P∞=oYiR(si,ai)∣s]. In the same
spirit as augmented Lagrangian, we will introduce to the objective the simple penalty function
Es〜μ(s) [(Eπb [P∞=0 YiR(si,ai)] - V(s))2i, leading to the following:
Lr (V,α,π) :=	(1 - γ k+1)Eμ [V(s)] + Ea [δ ({s，，&}：=。,s：+i^]	⑼
+ ηvEs〜μ(s) [(Eπb [P∞=0 YiR(Si, ai)] - V(S)),
where ηV > 0 is a hyper-parameter controlling the strength of the regularization.
Note that in the penalty function above we use a behavior policy πb instead of an optimal policy,
since the latter is unknown. Adding such a regularization enables local duality in the primal parame-
ters. Indeed, this can be easily verified by showing the positive definiteness of the Hessian at a local
solution. We call this approach path regularization, since it exploits the rewards in the sample path
4
Published as a conference paper at ICLR 2018
to regularize the solution path of value function V in the optimization procedure. As a by-product,
the regularization also provides a mechanism to utilize off-policy samples from behavior policy πb .
One can also see that the regularization indeed provides guidance and preference to search for the
solution path. Specifically, in each update of V the learning procedure, it tries to move towards the
optimal value function while staying close to the value function of the behavior policy πb. Intuitively,
such regularization restricts the feasible domain of candidates V to be a ball centered at V πb. Besides
enhancing local convexity, such a penalty also avoids unboundedness of V in the learning procedure,
and thus more numerical robust. As long as the optimal value function is indeed in such a region,
the introduced side-effect can be controlled. Formally, we can show that with appropriate ηV , the
optimal solution (V*,α*,∏*) is not affected. The main results of this subsection are summarized
by the following theorem.
Theorem 4 (Property of path regularization) The local duality holds for Lr(V, α, π). De-
note (V*,α*,π*) as the solution to Bellman optimality equation, with some appropriate ηv,
(V*,α*,∏*) = argmaXα∈p(s),∏∈p(A) argminy Lr(V,α,π).
The proof of the theorem is given in Appendix B.3. We emphasize that the theorem holds when V is
given enough capacity, i.e., in the nonparametric limit. With parametrization introduced, definitely
approximation error will be introduced, and the valid range of ηV, which keeps optimal solution
unchanged, will be affected. However, the function approximation error is still an open problem
for general class of parametrization, we omit such discussion here which is out of the range of this
paper.
4.3 Stochastic Dual Ascent Update
Rather than the primal form, i.e., minV maXα∈P(S),π∈P(A) Lr(V, α, π), we focus on optimiz-
ing the dual form maXα∈P(S),π∈P(A) minV Lr (V, α, π). The major reason is due to the sam-
ple efficiency consideration. In the primal form, to apply the stochastic gradient descent algo-
rithm at Vt, one needs to solve maXα∈P(S),π∈P(A) Lr (Vt, α, π) which involves sampling from
each π and α during the solution path for the subproblem. We define the regularized dual func-
tion 'r(α,π) := minv Lr(V,α,π). We first show the unbiased gradient estimator of ' w.r.t.
θρ = (θα, θπ ), which are parameters associated with α and π. Then, we incorporate the stochas-
tic update rule to the dual ascent algorithm (Boyd et al., 2010), resulting in the dual actor-critic
(Dual-AC) algorithm.
The gradient estimators of the dual functions can be derived using chain rule and are provided below.
Theorem 5 The regularized dual function `r (α, π) has gradients estimators
Vθa'r (θα,θ∏) = Ea [δ ({sSi,a}^k=^0θ ,Sk+1) V©。log α(s)],	(10)
Vθ∏'r (θa,θ∏) = Ea [δ ({si,ai}k=0 ,Sk+ι)pk=0 Vθ∏ logπ(a∣s) .	(11)
Therefore, we can apply stochastic mirror descent algorithm with the gradient estimator given in
Theorem 5 to the regularized dual function `r (α, π). Since the dual variables are probabilistic
distributions, it is natural to use KL-divergence as the prox-mapping to characterize the geometry
in the family of parameters (Amari & Nagaoka, 1993; Nemirovski et al., 2009). Specifically, in the
t-th iteration,
θp = argminθρ -θρ>0PT + ζ1tKL(Pθρ (S, a) llPθρt-ι(S, O)),	(12)
where gp-1 = Vθ° 'r (θtt-1,θ∏-1) denotes the stochastic gradients estimated through (10) and (11)
via given samples and KL(q(s, a) ∣∣p(s, a)) = R q(s, a) log P(Sadsda. Intuitively, such update rule
emphasizes a trade-off between the current policy and possible improvements based on samples. The
update of π shares some similarity to the TRPO, which is derived to ensure monotonic improvement
of the new policy Schulman et al. (2015a). We discuss the details in Section 4.4.
Rather than just update V once via the stochastic gradient of VVLr (V, α, π) in each itera-
tion for solving saddle-point problem (Nemirovski et al., 2009), which is only valid in convex-
concave setting, Dual-AC exploits the stochastic dual ascent algorithm which requires V t =
5
Published as a conference paper at ICLR 2018
Algorithm 1 Dual Actor-Critic (Dual-AC)
1:	Initialize θV, θαand θ∏ randomly, set β ∈ [ 1, 1].
2:	for episode t = 1, . . . , T do
3:	Start from S 〜 at-1 (s), collect samples {τι}J==ι follows behavior policy πt-1.
4:	Update θV = argmin&v Lr(V, αt-1, πt-1) by SGD based on {τι}m=i.
5:	Update at(s) according to closed-form (14).
6:	Decay the stepsize: Zt = n0+C/铲.
7:	Compute the stochastic gradients for θπ following (11).
8:	Update θπt according to the exact prox-mapping (16) or the approximate closed-form (17).
9:	end for
argminv Lr (V, αt-1, πt-1) in t-th iteration for estimating ▽6「'r (θɑ, θ∏). AS We discussed, such
operation will keep the gradient estimator of dual variables unbiased, which provides better direction
for convergence.
In Algorithm 1, we update Vt by solving optimization minV Lr(V, αt-1, πt-1). In fact, the V func-
tion in the path-regularized Lagrangian Lr(V, α, π) plays two roles: i), inherited from the original
Lagrangian, the first two terms in regularized Lagrangian (9) push the V towards the value function
of the optimal policy with on-policy samples; ii), on the other hand, the path regularization enforces
V to be close to the value function of behavior policy πb with off-policy samples. Therefore, the V
function in the Dual-AC algorithm can be understood as an interpolation between these two value
functions learned from both on and off policy samples.
4.4 Practical Implementation
In above, we have introduced path regularization for recovering local duality property of the
parametrized multi-step Lagrangian dual form and tailored stochastic mirror descent algorithm for
optimizing the regularized dual function. Here, we present several strategies for practical computa-
tion considerations.
Update rule of Vt. In each iteration, we need to solve Vt = argminθV Lr (V, αt-1, πt-1), which
depends on ∏b and ηv, for estimating the gradient for dual variables. In fact, the closer ∏ to ∏* is,
the smaller Es〜*(§)[(Eπb [P∞=0 γiR(si, aj] - V*(s))2] will be. Therefore, we can set ηv to be
large for better local convexity and faster convergence. Intuitively, the ∏t-1 is approaching to ∏* as
the algorithm iterates. Therefore, we can exploit the policy obtained in previous iteration, i.e., πt-1,
as the behavior policy. The experience replay can also be used.
Furthermore, notice the L(V, αt-1, πt-1) is a expectation of functions of V, we will use stochastic
gradient descent algorithm for the subproblem. Other efficient optimization algorithms can be used
too. Specifically, the unbiased gradient estimator for Vθv L(V, αt-1, ∏t-1) is
Vθv Lr (V, αtτ,πt-1) = (1 - γk+1 )Eμ [Vθv V (s)] + Ea 付 θv δ ({si,电忆,Sk+1)](13)
-2ηvEμb [(P∞=0 γiR(si,ai) - V(S)) VV(s)].
We can use k-step Monte Carlo approximation for Enb [P∞=0 YiR(si, ai)] in the gradient estimator.
As k is large enough, the truncate error is negligible (Sutton & Barto, 1998). We will iterate via
θVt,i = θVt,i-1 + κiVb θt,i-1Lr(V, αt-1, πt-1) until the algorithm converges.
It should be emphasized that in our algorithm, Vt is not trying to approximate the value or advantage
function of πt, in contrast to most actor-critic algorithms. Although Vt eventually becomes an
approximation of the optimal value function once the solution reaches the global optimum, in each
update Vt is merely a function that helps the current policy to be improved. From this perspective,
the Dual-AC bypasses the policy evaluation step.
Update rule of αt . In practice, we may face with the situation that the initial sampling distribution
is fixed, e.g., in MuJoCo tasks. Therefore, we cannot obtain samples from αt (S) at each iteration.
We assume that ∃η* ∈ (0,1], such that α(s) = (1 - η*)β(s) + η*μ(s) with β(s) ∈ P (S). Hence,
we have
Ea hδ({sig}：=。，sk+ι)i = Eμ h(α(s) + ημ)δ({si,@}：=。，sk+ι)i,
6
Published as a conference paper at ICLR 2018
where α(S) = (I - ημ)需. Note that such an assumption is much weaker comparing with the
requirement for popular policy gradient algorithms (e.g., Sutton et al. (1999); Silver et al. (2014))
that assumes μ(s) to be a stationary distribution. In fact, we can obtain a closed-form update for ɑ
if a square-norm regularization term is introduced into the dual function. Specifically,
Theorem 6 In t-th iteration, given V t andπ t-1 ,
argmax E"(s)∏-i(s)
α>0
m max fθ, e" ι
ηα
[(a(S) + ημ) δ ({si, ai}k=0 , sk+l) ] - ηɑ kαkμ
δ {Si, ai}ik=0 , Sk+1	.
(14)
(15)
Then, We can update c^t through (14) with Monte Carlo approximation of
Ent 1 [δ ({si，。，}：=。，Sk+ι)], avoiding the parametrization of α. As we can see, the at(s)
reweights the samples based on the temporal differences and this offers a principled justification for
the heuristic prioritized reweighting trick used in (Schaul et al., 2015).
Update rule of θπt . The parameters for dual function, θρ , are updated by the prox-mapping opera-
tor (12) following the stochastic mirror descent algorithm for the regularized dual function. Specifi-
cally, in t-th iteration, given V t and αt, for θπ, the prox-mapping (12) reduces to
θΠ = argminθπ -θπ>gΠ + ζ1tKL 卜θ∏ ⑷S) Uπθ∏-1 ⑷ S)) ,
(16)
where g∏ = Vθπ'r (θα,θ∏). Then, the update rule will become exactly the natural policy gradi-
ent (Kakade, 2002) with a principled way to compute the “policy gradient” g∏. This can be un-
derstood as the penalty version of the trust region policy optimization (Schulman et al., 2015a), in
which the policy parameters conservative update in terms of KL-divergence is achieved by adding
explicit constraints.
Exactly solving the prox-mapping for θπ requires another optimization, which may be expensive.
To further accelerate the prox-mapping, we approximate the KL-divergence with the second-order
Taylor expansion, and obtain an approximate closed-form update given by
θπ	≈	argmin 卜%>gπ	+ ∣ιιθ∏—θπ-1ι∣Ft} =	θπ-1+ζtFrιgπ
(17)
where Ft := Eαtπt-1 V2 log πθt-1 denotes the Fisher information matrix. Empirically, we may
normalize the gradient by its norm gπt Ft-1gπt (Rajeswaran et al., 2017) for better performances.
Combining these practical tricks to the stochastic mirror descent update eventually gives rise to the
dual actor-critic algorithm outlined in Algorithm 1.
5	Related Work
The dual actor-critic algorithm includes both the learning of optimal value function and optimal
policy in a unified framework based on the duality of the linear programming (LP) representation of
Bellman optimality equation. The linear programming formulation of Bellman optimality equation
and its duality have been used for (approximate) planning problem (Schweitzer & Seidmann, 1985;
de Farias & Van Roy, 2003; Wang et al., 2007; O’Donoghue et al., 2011; Cogill, 2015), in which the
transition probability of the MDP is known and the value function or policy are in tabular form. Chen
& Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for
the one-step Lagrangian of the LP problem in reinforcement learning setting. However, as we dis-
cussed in Section 3, their algorithm is restricted to tabular parametrization and are not applicable to
MDPs with large or continuous state/action spaces. Other authors have also considered approximate
LP formulations to solve large-scale problems Pazis & Parr (2011); Abbasi-Yadkori et al. (2014);
Lakshminarayanan et al. (2017), but they either do not focus on concrete algorithms for solving the
optimization problem, or require certain knowledge of the transition probability function that may
be hard to obtain in practice.
The duality view has also been exploited in Neu et al. (2017). Their algorithm is based on the
duality of entropy-regularized Bellman equation (Todorov, 2007; Rubin et al., 2012; Fox et al.,
7
Published as a conference paper at ICLR 2018
2016; Haarnoja et al., 2017; Asadi & Littman, 2017; Nachum et al., 2017), rather than the exact
Bellman optimality equation we try to solve in this work.
Our dual actor-critic algorithm can be understood as a nontrivial extension of the (approximate) dual
gradient method (Bertsekas, 1999, Chapter 6.3) using stochastic gradient and Bregman divergence,
which essentially parallels the view of (approximate) stochastic mirror descent algorithm (Ne-
mirovski et al., 2009) in the primal space. As a result, the algorithm converges with diminishing
stepsizes and decaying errors from solving subproblems.
Particularly, the update rules of α and π in the dual actor-critic are related to several existing algo-
rithms. As we see in the update of α, the algorithm reweighs the samples which are not fitted well.
This is related to the heuristic prioritized experience replay (Schaul et al., 2015). For the update in
π, the proposed algorithm bears some similarities with trust region poicy gradient (TRPO) (Schul-
man et al., 2015a) and natural policy gradient (Kakade, 2002; Rajeswaran et al., 2017). Indeed,
TRPO and NPR solve the same prox-mapping but are derived from different perspectives. We em-
phasize that although the updating rules share some resemblance to several reinforcement learning
algorithms in the literature, they are purely originated from a stochastic dual ascent algorithm for
solving the two-play game derived from Bellman optimality equation.
6	Experiments
We evaluated the dual actor-critic (Dual-AC) algorithm on several continuous control environ-
ments from the OpenAI Gym (Brockman et al., 2016) with MuJoCo physics simulator (Todorov
et al., 2012). We compared Dual-AC with several representative actor-critic algorithms, including
trust region policy optimization (TRPO) (Schulman et al., 2015a) and proximal policy optimization
(PPO) (Schulman et al., 2017)1. We ran the algorithms with 5 random seeds and reported the av-
erage rewards with 50% confidence interval. Details of the tasks and setups of these experiments
including the policy/value function architectures and the hyperparameters values, are provided in
Appendix C.
6.1	Ablation Study
To justify our analysis in identifying the sources of instability in directly optimizing the parametrized
one-step Lagrangian duality and the effect of the corresponding components in the dual actor-critic
algorithm, we perform comprehensive Ablation study in InvertedDoublePendulum-v1, Swimmer-
v1, and Hopper-v1 environments. We also considered the effect of k = {10, 50} besides the one-step
result in the study to demonstrate the benefits of multi-step.
We conducted comparison between the Dual-AC and its variants, including Dual-AC w/o multi-step,
Dual-AC w/o path-regularization, Dual-AC w/o unbiased V , and the naive Dual-AC, for demon-
strating the three instability sources in Section 3, respectively, as well as varying the k = {10, 50}
in Dual-AC. Specifically, Dual-AC w/o path-regularization removes the path-regularization com-
ponents; Dual-AC w/o multi-step removes the multi-step extension and the path-regularization;
Dual-AC w/o unbiased V calculates the stochastic gradient without achieving the convergence of
inner optimization on V ; and the naive Dual-AC is the one without all components. Moreover,
Dual-AC with k = 10 and Dual-AC with k = 50 denote the length of steps set to be 10 and 50,
respectively.
The empirical performances on InvertedDoublePendulum-v1, Swimmer-v1, and Hopper-v1 tasks
are shown in Figure 1. The results are consistent across the tasks with the analysis. The naive Dual-
AC performs the worst. The performances of the Dual-AC found the optimal policy which solves the
problem much faster than the alternative variants. The Dual-AC w/o unbiased V converges slower,
showing its sample inefficiency caused by the bias in gradient calculation. The Dual-AC w/o multi-
step and Dual-AC w/o path-regularization cannot converge to the optimal policy, indicating the
importance of the path-regularization in recovering the local duality. Meanwhile, the performance
of Dual-AC w/o multi-step is worse than Dual-AC w/o path-regularization, showing the bias in one-
1As discussed in Henderson et al. (2018), different implementations of TRPO and PPO can provide different
performances. For a fair comparison, We use the codes from https://github.com/joschu/modUlarJl reported
to have achieved the best scores in Henderson et al. (2018).
8
Published as a conference paper at ICLR 2018
P,J"MW≈©Poss wσεφ><
——DuaI-AC
---- DuaI-AC w/o multi-step
DuaI-AC w/o unbiased V
----DuaI-AC w/o path-reg
DuaI-AC with k=10
---- DuaI-AC with k=50
Naive Dual-AC
0	25	50	75 IOO 125 150 175 200 225
Iteration
(a) InVertedDoUblePendUlUm-VI
0 5 0 5 0 5
5 2 0 7 5 2
PJeBH 3pod3623><
2500
2000
1500
1000
500
(b) SWimmer-VI
25	50	75 100 125 150 175 200 225
Iteration
Hopper-vl
——DuaI-AC
---- DuaI-AC w/o multi-step
DuaI-AC w/o unbiased V
----DuaI-AC w/o path-reg
—— DuaI-AC with k=10
---- DuaI-AC with k=50
Naive DuaI-AC
50	100	150	200	250
Iteration
(c) HoPPer-VI

FigUre 1: Comparison betWeen the DUal-AC and its Variants for jUstifying the analysis of the soUrce
of instability.
steP can be alleViated Via mUlti-steP trajectories. The Performances of DUal-AC become better With
the length of steP k increasing on these three tasks. We conjectUre that the main reason may be
that in these three MUJoCo enVironments, the bias dominates the Variance. Therefore, With the k
increasing, the ProPosed DUal-AC obtains more accUmUlate reWards.
6.2	Comparison in Continuous Control Tasks
In this section, We eValUated the DUal-AC against TRPO and PPO across mUltiPle tasks, inclUding
the InVertedDoUblePendUlUm-V1, HoPPer-V1, HalfCheetah-V1, SWimmer-V1 and Walker-V1. These
tasks haVe different dynamic ProPerties, ranging from Unstable to stable, Therefore, they ProVide
sUfficient benchmarks for testing the algorithms. In FigUre 2, We rePorted the aVerage reWards
across 5 rUns of each algorithm With 50% confidence interVal dUring the training stage. We also
rePorted the aVerage final reWards in Table 1.
Table 1: The aVerage final Performances of the Policies learned from DUal-AC and the comPetitors.
EnVironment	DUal-AC	PPO	TRPO
PendUlUm	-155.45	-266.98	-245.11
InVertedDoUblePendUlUm	8599.47	1776.26	3070.96
SWimmer	234.56	223.13	232.89
HoPPer	2983.79	2376.15	2483.57
HalfCheetah	3041.47	2249.10	2347.19
Walker	4103.60	3315.45	2838.99
The ProPosed DUal-AC achieVes the best Performance in almost all enVironments, inclUding Pen-
dUlUm, InVertedDoUblePendUlUm, HoPPer, HalfCheetah and Walker. These resUlts demonstrate that
DUal-AC is a Viable and comPetitiVe RL algorithm for a Wide sPectrUm of RL tasks With different
dynamic ProPerties.
A notable case is the InVertedDoUblePendUlUm, Where DUal-AC sUbstantially oUtPerforms TRPO
and PPO in terms of the learning sPeed and samPle efficiency, imPlying that DUal-AC is Preferable
to Unstable dynamics. We conjectUre this adVantage might come from the different meaning of V in
oUr algorithm. For Unstable system, the failUre Will haPPen freqUently, resUlting the collected data
are far aWay from the oPtimal trajectories. Therefore, the Policy imProVement throUgh the ValUe
fUnction corresPonding to cUrrent Policy is sloWer, While oUr algorithm learns the oPtimal ValUe
fUnction and enhances the samPle efficiency.
7	Conclusion
In this PaPer, We reVisited the linear Program formUlation of the Bellman oPtimality eqUation, Whose
Lagrangian dUal form yields a game-theoretic VieW for the roles of the actor and the dUal critic. Al-
thoUgh sUch a frameWork for actor and dUal critic alloWs them to be oPtimized for the same objectiVe
fUnction, Parametering the actor and dUal critic UnfortUnately indUces instablity in oPtimization. We
analyze the soUrces of instability, Which is corroborated by nUmerical exPeriments. We then Pro-
Pose Dual Actor-Critic , Which exPloits stochastic dual ascent algorithm for the path regularized,
9
Published as a conference paper at ICLR 2018
Pendu um-vθ
PJeM3≈3pα-duj"60∙J0><
20	40	60	80	100
Iteration
(a) Pendulum
Hopper-vl
——TRPO
PPO
——Dual-AC
PJeM"≈B6eJS4
Iteration
201=105
p∙JeM33pοs-uraEυ>⅞
(b) InVertedDoUblePendUlUm-VI
HalfCheetah-Vl
(c) SWimmer-VI
Walker-Vl
50	100	150	200	250	50	100	150	200	250	100	20。	300	400	500
Iteration	Iteration	Iteration
(d) HoPPer-VI	(e) HalfCheetah-Vl	⑴ Walker-v1
FigUre 2: The resUlts of DUal-AC against TRPO and PPO baselines. Each plot shows aVerage reward
dUring training across 5 random seeded rUns, with 50% confidence interVal. The x-axis is the nUmber
of training iterations. The DUal-AC achieVes comParable Performances comParing with TRPO and
PPO in some tasks, bUt oUtPerforms on more challenging tasks.
multi-step bootstrapping two-Player game, to byPass these issUes. The algorithm achieVes the state-
of-the-art Performances on seVeral MUJoCo benchmarks.
Acknowledgments
LSis sUPPorted in Part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-
1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC,
NVIDIA and Amazon AWS. NH is sUPPorted by NSF CRII-1755829. We thank Mengdi Wang for
helPfUl feedback and discUssions.
References
Yasin Abbasi-Yadkori, Peter Bartlett, and Alan Malek. Linear Programming for large-scale markoV
decision problems. In ICML, pp. 496-504, 2014.
ShUn-ichi Amari and H. Nagaoka. Methods of Information Geometry. Oxford UniVersity Press,
1993.
KaVosh Asadi and Michael L. Littman. An alternatiVe softmax operator for reinforcement learning.
In ICML, pp. 243-252, 2017.
Irwan Bello, HieU Pham, QUoc V Le, Mohammad NoroUzi, and Samy Bengio. NeUral combinatorial
optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, second edition, 1999.
Dimitri P Bertsekas. Dynamic programming and optimal control, VolUme 1. Athena Scientific
Belmont, MA, 1995.
Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific,
September 1996. ISBN 1-886529-10-8.
Shalabh Bhatnagar, Richard S. SUtton, Mohammad GhaVamzadeh, and Mark Lee. NatUral actorcritic
algorithms. Automatica, 45(11):2471-2482, 2009.
10
Published as a conference paper at ICLR 2018
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning ,3(1):1-123,2010.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540, 2016.
Martin Burger. Infinite-dimensional optimization and optimal design. 2003.
Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of rein-
forcement learning. arXiv:1612.02516, 2016.
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang. Optimal primal-dual methods for a class of
saddle point problems. SIAM Journal on Optimization, 24(4):1779-1814, 2014.
Randy Cogill. Primal-dual algorithms for discounted Markov decision processes. In Control Con-
ference (ECC), 2015 European, pp. 260-265. IEEE, 2015. Longer version at arXiv:1601.04175.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable
kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing
Systems, pp. 3041-3049, 2014.
Daniela P. de Farias and Benjamin Van Roy. The linear programming approach to approximate
dynamic programming. Operations Research, 51(6):850-865, 2003.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics.
Foundations and TrendsR in Robotics, 2(1-2):1-142, 2013.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. In UAI, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In ICML, pp. 1352-1361, 2017.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In AAAI, 2018.
S. Kakade. A natural policy gradient. In T. G. Dietterich, S. Becker, and Z. Ghahramani (eds.),
Advances in Neural Information Processing Systems 14, pp. 1531-1538. MIT Press, 2002.
M. Kearns and S. Singh. Bias-variance error bounds for temporal difference updates. In Proc. 13th
Annu. Conference on Comput. Learning Theory, pp. 142-147. Morgan Kaufmann, San Francisco,
2000.
Vijay R. Konda and John N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and
Optimization, 42(4):1143-1166, 2003.
Chandrashekar Lakshminarayanan, Shalabh Bhatnagar, and Csaba Szepesvari. A linearly relaxed
approximate linear program for Markov decision processes, 2017. CoRR abs/1704.02544.
David G. Luenberger and Yinyu Ye. Linear and Nonlinear Programming. Springer Publishing
Company, Incorporated, 2015. ISBN 3319188410, 9783319188416.
Volodymyr Mnih, Adria PUigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning, pp. 1928-
1937, 2016.
Ofir NachUm, Mohammad NoroUzi, Kelvin XU, and Dale SchUUrmans. Bridging the gap between
valUe and policy based reinforcement learning. In Advances in Neural Information Processing
Systems 30, pp. 2772-2782, 2017.
11
Published as a conference paper at ICLR 2018
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM J. on OPtimization, 19(4):1574-1609, January 2009. ISSN
1052-6234.
Yurii Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127-152,
2005.
Gergely Neu, Anders Jonsson, and Vicenc Gomez. A unified view of entropy-regularized Markov
decision processes. arXiv:1705.07798, 2017.
Brendan O’Donoghue, Yang Wang, and Stephen Boyd. Min-max approximate dynamic program-
ming. In Computer-Aided Control System Design (CACSD), 2011 IEEE International Symposium
on, pp. 424-431. IEEE, 2011.
Jason Pazis and Ronald Parr. Non-parametric approximate linear programming for MDPs. In AAAI,
2011.
Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In ECML, pp. 280-291.
Springer, 2005.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
Aravind Rajeswaran, Kendall Lowrey, Emanuel Todorov, and Sham Kakade. Towards generalization
and simplicity in continuous control. In Advances in Neural Information Processing Systems 30,
2017.
Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in MDPs. Deci-
sion Making with Imperfect Decision Makers, pp. 57-74, 2012.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region
policy optimization. In ICML, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Paul J. Schweitzer and Abraham Seidmann. Generalized polynomial approximations in Markovian
decision processes. Journal of Mathematical Analysis and Applications, 110(2):568-582, 1985.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):
9-44, 1988.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems 12, pp. 1057-1063, 1999.
R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information
processing systems, pp. 1369-1376, 2007.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-
5033. IEEE, 2012.
12
Published as a conference paper at ICLR 2018
Mengdi Wang. Randomized Linear Programming Solves the Discounted Markov Decision Problem
In Nearly-Linear Running Time. ArXiv e-prints, 2017.
Tao Wang, Daniel J. Lizotte, Michael H. Bowling, and Dale Schuurmans. Stable dual dynamic
programming. In Advances in Neural Information Processing Systems 20, pp. 1569-1576, 2007.
C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Oxford, May
1989. (To be reprinted by MIT Press.).
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8:229-256, 1992.
13
Appendix
A Details of the Proofs for Section 2
A. 1 Duality of Bellman Optimality Equation
Puterman (2014); Bertsekas (1995) provide details in deriving the linear programming form of the
Bellman optimality equation. We provide a briefly proof here.
Proof We rewrite the linear programming 3 as
V * = argmin Eμ [V (s)].	(18)
V>TV
Recall the T is monotonic, i.e., if V > TV ⇒ TV > T2V and V* = T∞V for arbitrary V, we
have for ∀V feasible, V > T V > T 2V > ... > T ∞V = V *.	■
Theorem 1 (Optimal policy from occupancy) Esa∈s×aρ*(s,a) = 1, and π*(a∣s) =
ρ*(S,a)
Pa∈A P*(S,a厂
Proof For the optimal occupancy measure, it must satisfy
Xρ*(s0,a) =γ * X ρ*(s, a)p(s0∣s, a) + (1 - γ)μ(s0),	∀s0 ∈ S
a∈A	S,a∈S ×A
⇒ (I- Y)μ +	X (YP -I)ρ*(s,a) = 0,
S,a∈S ×A
where P denotes the transition distribution and I denotes a |S | × |SA| matrix where Iij = 1 if and
only if j ∈ [(i - 1) |A| + 1,...,i |A|]. Multiply both sides with 1, due to μ and P are probabilities,
we have h1, ρ* i = 1.
Without loss of generality, we assume there is only one best action in each state. Therefore, by the
KKT complementary conditions of (3), i.e.,
ρ(s, a) (R(s, a) + γEs0∣s,a [V(s0)] - V(S)) = 0,
which implies ρ*(s, a) = 0 if and only if a = a*, therefore, the π* by normalization.	■
Theorem 2 The optimal policy π* and its corresponding value function V * is the solution to the
following saddle problem
max min
α∈P(S),π∈P(A) V
L(V,α,π) =(I- Y)Es〜μ(s) [V(S)] +	E	α(s)π (a|s)∆[V](s,a)
(S,a)∈S×A
where ∆[V](S, a) = R(S, a) + YES0|S,a [V(S0)] - V(S).
Proof Due to the strong duality of the optimization (3), we have
min max
V ρ(S,a)>0
(I- Y)Es〜μ(s) [V(s)]+	E	P(s, a)∆[V](s,a)
(S,a)∈S×A
max min (1 - Y) Es〜μ(s) [V(s)] + V"	ρ(s,a)∆[V](s,a).
ρ(s,a)>0 V	(s,a)∈S×A
Then, plugging the property of the optimum in Theorem 1, We achieve the final optimization (6). ■
A.2 Continuous State and Action MDP Extension
In this section, we extend the linear programming and its duality to continuous state and action
MDP. In general, the only weak duality holds for infinite constraints, i.e., P* > D*. With a mild
assumption, we will recover the strong duality for continuous state and action MDP, and most of the
conclusions in discrete state and action MDP still holds.
14
Published as a conference paper at ICLR 2018
Specifically, without loss of generality, we consider the solvable MDP, i.e., the optimal policy,
π*(a∣s),exists.If kR(s,a)k∞ 6 Cr, kV*k∞ 6 ICR. Moreover,
kV *k2,μ
6
6
6
6
/ (V*(s))2 μ(s)ds = / (R(s,a) + YEs0∣s,a [V*(s0)])2 π*(a∣s)μ(s)d(s,a)
2 / (R(s,a))2 π*(a∣s)μ(s)ds + 2γ2 ʃ 区,⑶。[V*(s0)])2 π*(a∣s)μ(s)ds
2max kR(s,a)∣∣μ + 2γ2 / (/P*(s0∣s)μ(s)ds) (V*(s0))2 ds0
2max kR(s,a)kμ + 2Y2 kV *(SO)k∞∕ / p *(SlS)μ(S)dsds0
2max kR(s,a)kμ + 2γ2∣∣V*(s0)k∞ ,
where the first inequality comes from 2hf (x), g(x)i2 6 kf k22 + kgk22.
IlV* - YEs0∣s,a [V(s0)]∣∣μ∏b 6 2 kV*kμ + 2Y2 ||Es，|s,a [V*3)]值.6 2 kV*k； + 2Y2 kV*(Ck∞ ,
for some ∏ ∈ P that ∏b(a∣s) > 0 for ∀ (s,a) ∈ SX A. Therefore, with the assumption that
∣∣R(s, a)kj 6 CR ,∀a ∈ A, we have R(s, a) ∈ Lμl∏b (S × A) and V *(s0) ∈ L'(S). Theconstraints
in the primal form of linear programming can be written as
(I-YP) V - R Lμ∏b0,
where I 一 YP : Lμ(S) → 品不匕(S × A) without any effect on the optimality. For simplicity, We
denote 占 as ≥l^ and hf,gi = J f (s,a)g(s,a)μ(s)∏b(a∣s)dsda. ApplytheLagrangianmultiplier
for constraints in ordered Banach space in Burger (2003), we have
P* = min max (1 — Y)Eμ [V(s)] — h%, (I - YP) V - Ri.	(19)
V ∈L %0
The solution (V*, %*) also satisfies the KKT conditions,
(1-Y)1-(I-YP)>%*	=	0,	(20)
%*	0,	(21)
(I-YP)V* -R	0,	(22)
h%*,(I-YP)V* -Ri	=	0.	(23)
where > denotes the conjugate operation. By the KKT condition, we have
D1,(1-Y)1-(I-YP)>%*E=0⇒h1,%i=1.	(24)
The strongly duality also holds, i.e.,
P* = D* := max	hR(S, a), %(S, a)i	(25)
%0
s.t.	(1 -Y)1 -(I-YP)>%=0	(26)
Proof We compute the duality gap
(1-Y)h1,V*i -hR,%*i
= h%*,(I-YP)V*i-hR,%*i
= h%*,(I-YP)V* -Ri=0,
which shows the strongly duality holds.
15
Published as a conference paper at ICLR 2018
B Details of The Proofs for Section 4
B.1 Competition in Multi-Step Setting
Once we establish the k-step Bellman optimality equation (7), it is easy to derive the λ-Bellman
optimality equation, i.e.,
∞k
V*(s)=max(1 - λ) X λkEπ X YiiR(Si,ai)+ γk+1V*(sk+ι) := (TλV*)(s).	(27)
π∈	k=0	i=0
Proof Denote the optimal policy as ∏* (a∣s),we have
k
V*(s) = E∏[}i=0∣s XYR(Si,ai) + Yk+1E∏]∣s [V*国+1)],
i=0
holds for arbitrary ∀k ∈ N. Then, we conduct k 〜Geo(λ) and take expectation over the countable
infinite many equation, resulting
∞k
V*(S) = (1- λ) X λkEπ* X YiR(Si,ai) + γk+1V*国+。
k=0	i=0
k
X YiR(Si,ai)+ γk+1 V *(sk+ι)
i=0
∞
max (1 - λ) X λkEπ
π∈P	k=0
Next, we investigate the equivalent optimization form of the k-step and λ-Bellman optimality equa-
tion, which requires the following monotonic property of Tk and Tλ .
Lemma 7 Both Tk and Tλ are monotonic.
Proof Assume U and V are the value functions corresponding to π1 and π2, and U > V, i.e.,
U(S) > V (S), ∀S ∈ S, apply the operator Tk on U and V, we have
k
(TkU)(s) = max Efsi}3∣s XYiR(Si,ai) + Yk+1E∏k+ι∣s [U(s®+i)],
π∈	i=0
k
(TkV)(s) = m∈x Eπsi}k=1∣s XYiR(Si,ai) + Yk+1E∏k+ι∣s [V国+斓∙
π∈	i=0
Due to U > V, we have Esπ	|s [U (Sk+1)] > Esπ	|s [V (Sk+1)], ∀π ∈ P, which leads to the first
conclusion, TkU > TkV.
Since Tλ = (1 - λ) P∞=1 Tk = Ek〜ge0(λ) [Tk], therefore, Tλ is also monotonic.	■
With the monotonicity of T and Tλ, We can rewrite the V * as the solution to an optimization,
Theorem 8 The optimal value function V * is the solution to the optimization
V * = argmin(1 - Yk+1) Es 〜μ(s) [V (s)] ,	(28)
V >Tk V
where μ(c) is an arbitrary distribution over S.
Proof Recall the Tk is monotonic, i.e., V > TkV ⇒ TkV > Tk2V and V* = Tk∞ V for arbitrary
V, we have for ∀V, V > TkV > Tk2V > . . . > Tk∞ V = V*, where the last equality comes
from the Banach fixed point theorem (Puterman, 2014). Similarly, we can also show that ∀V,
V > T∞V = V*. By combining these two inequalities, we achieve the optimization.	■
16
Published as a conference paper at ICLR 2018
We rewrite the optimization as
min (1 — Yk+1)Es〜μ(s) [V(s)]	(29)
k
s.t. V(s) > R(s, a) + max Eπ k+1	X γiR(si, ai) + γk+1V (sk+1) ,
π∈P	{si}i=1|s i=1
(s, a) ∈ S × A,
We emphasize that this optimization is no longer linear programming since the existence of max-
operator over distribution space in the constraints. However, Theorem 1 still holds for the dual
variables in (32).
Proof Denote the optimal policy as ∏V = argmax∏∈p
E{πsi}ik=+11
Pik=1 γiR(si
ai) + γk+1V (sk+1) ,
the KKT condition of the optimization (29) can be written as
s
k-1	k
(1 - γk+1) μ(s0) + γk+1	X P(Slsk, ak) Y p(si+ι∣Si, ai) Y∏V(a∕si)ρ*(s0, ao)
{si ,ai}ik=0	i=0	i=1
kk
=E	Up(si+ι∣si,ai)ρ*(s0,a) ∏∏V(ai∣si).
a0 ,{si ,ai }ik=1 i=0	i=1
Denote Pn(sk+ι∣s,a) = £{%ɑ玳=ɪ p(sk+ι∣sk,ak) Qk=-o P(si+1 lsi,ai) Qk=I ∏3lsi), We sim-
plify the condition, i.e.,
*
(1 - γk+1) μ(s0) + Yk+1 EPnV(s0∣s, a)ρ*(s, a) = £p*(s0, a).
s,a	a
n*
Due to the Pk V (s0|s, a) is a conditional probability for ∀V, With similar argument in Theorem 1,
we have Es,0 ρ*(s, a) = 1.
By the KKT complementary condition, the primal and dual solutions, i.e., V * and ρ*, satisfy
ρ*(s,a) (R(s,a) + E∏*}k+js X YiR(si,ai) + Yk+1V *(sk+ι) - V *(s))=0∙	(30)
Recall V * denotes the value function of the optimal policy, then, based on the definition, ∏V * = ∏*
which denotes the optimal policy. Then, the condition (30) implies ρ(s, a) 6= 0 if and only ifa = a*,
therefore, we can decompose ρ*(s,a) = α*(s)π*(a∣s).
The corresponding Lagrangian of optimization (29) is
min max Lk (V, ρ)
V ρ(s,a)>o
(1 - Yk+1)Eμ [V(s)]+	E	ρ(s,a)
(s,a)∈S×A
mn∈aPx ∆kn [V](s, a)
(31)
where∆kn[V](s,a) = R(s, a) + E{ns }k+1|s Pik=1 YiR(si, ai) + Yk+1V (sk+1) - V (s).
We further simplify the optimization. Since the dual variables are positive, we have
min max
V ρ(s,a)>o,n∈P
Lk(V,ρ)
(1 - Yk+1)Eμ [V(s)]+	E	P(s,a) (∆∏[V](s,a)).
(s,a)∈S×A
(32)
After clarifying these properties of the optimization corresponding to the multi-step Bellman opti-
mality equation, we are ready to prove the Theorem 3.
17
Published as a conference paper at ICLR 2018
Theorem 3 The optimal policy π* and its corresponding value function V * is the solution to the
following saddle point problem
AmaxA 八 min Lk (Ven) := (I-Yk+1 )Eμ [V (S)]	⑻
α∈P(S),π∈P(A) V
k
+ X α(so) Y π(ai∣Si)p(si+ι∣Si,ai)δ[V] ({si,ai}：=° , Sk+1
{si,ai}i=0,sk+1	=
where δ[V] {Si, ai}ik=0 , Sk+1 = Pik=0γiR(Si, ai) + γk+1V (Sk+1) - V(S).
Proof By Theorem 1 in multi-step setting, We can decompose ρ(s, a) = α(s)π(a∣s) without any
loss. Plugging such decomposition into the Lagrangian 32 and realizing the equivalence among
the optimal policies, we arrive the optimization as minV maxα∈P(S),π∈P(A) Lk (V, α, π). Then,
because of the strong duality as we proved in Lemma 9, we can switch min and max operators in
optimization 8 without any loss.	■
Lemma 9 The strong duality holds in optimization (8).
Proof Specifically, for every α ∈ P(S), π ∈ P (A),
'(α,π) = minLk(V,α,π) 6 min ∣Lk(V,α,π); δ[V] {[si,αik=o0 , Sk+ι) 6 θ}
6 min (	(I-7：加产[V(St< O) = (I-Yk+1)Es^μ(s) [V*(s)].
V [s.t∙ δ[V] ({si,aiki=0 ,Sk+1) 6 0J '	7 s μ(s) l j
On the other hand, since Lk(V, α*, π*) is convex w.r.t. V, we have V* ∈ argminV Lk(V, α*, π*),
by checking the first-order optimality. Therefore, we have
max	'(α,π) =	max	Lk (V,α,π)
α∈P(S),π∈P(A)	α∈P(S),π∈P(A),V ∈argminV Lk (V,α,π)
> L(V*,α*,π*) = (1 - γk+1)Es〜“(s) [V*(s)].
Combine these two conditions, we achieve the strong duality even without convex-concave property
(1 -γk+1)Es〜μ(s) [V*(s)] 6	max^.八'(α,∏) 6 (1 — Yk+1)Es〜μ(s) [V*(s)].
α∈P(S),π∈P(A)
B.2	The Composition in Applying Augmented Lagrangian Method
We consider the one-step Lagrangian duality first. Following the vanilla augmented Lagrangian
method, one can achieve the dual function as
'(α,π) = min(1 - Y)Es〜“⑸[V(s)]+	X	Pc (∆[V](s, a), α(s)π(a∣s)),
(s,a)∈S×A
where
Pc (∆[V](s, a), α(s)π(a∣s)) = ɪ {[max (0, α(s)π(a∣s) + c∆[V](s, a))]2 — α2(s)π2(a∣s)}.
The computation of Pc is in general intractable due to the composition of max and the condition ex-
pectation in ∆[V](S, a), which makes the optimization for augmented Lagrangian method difficult.
For the multi-step Lagrangian duality, the objective will become even more difficult due to con-
straints are on distribution family P(S) and P(A), rather than S × A.
B.3	Path Regularization
Theorem 4 The local duality holds for Lr(V, α, π).	Denote (V*, α*, π*) as the
solution to Bellman optimality equation, with some appropriate ηV, (V*, α*, π*) =
argmaxα∈P(S),π∈P(A) argminVLr(V,α,π).
18
Published as a conference paper at ICLR 2018
Proof The local duality can be verified by checking the Hessian of Lr (θv*). We apply the local
duality theorem (LUenberger & Ye, 2015)[Chapter 14]. Suppose (V*,α*,π*) is a local solution
to minV maxα∈P(S),π∈P(A) Lr(V, α, π), then, maxα∈P(S),π∈P(A) minV Lr(V, α, π) has a local
solution V* with corresponding α*,π*.
Next, we show that with some appropriate ηV, the path regularization does not change the optimum.
Let Uπ(S) = Eπ [Pi∞=0 γiR(Si, ai)|S, and thus, Uπ* = V*. We first show that for ∀πb ∈ P(A),
we have
E
E
(Enb [P∞=0 YiR(si, ai)] - V*(s))[ = E [(U∏(S)- Uπ* (S) + Uπ* (S)- V*(s))2]
(Uπb(s) - Uπ*(s))2i
6
6
6
n∏b(a∕si) - n∏*(a∕si) Y[p(si+ι∣Si, ai)
i=0
∞
γiR(si, ai)
i=1
i=0
2
∞
i=0
X γiR(Si, ai)! d {Si, ai}i∞=0!
f7∏b(ai∣Si) — ɪɪ∏*(ai∣Si) Y[p(si+ι∣Si,ai)
4Pi∞=1γiR(si,ai)2∞ 6
i=0	i=0
d⅛ ∣R(s,a)k∞
i=0
E
E
∞
∞
∞
∞
∞
2
∞
1
where the last second inequality comes from the fact that ∏b(a∕si)p(si+ι∣Si, ai) is distribution.
We then rewrite the optimization minV maxα∈P(S),π∈P(A) Lr (V, α, π) as
min max	Lk(V, α, π)
V α∈P(S),π∈P(A)	2
s.t.	V ∈ Ωe,∏b := { V : Es〜*(s)[(Enb [P∞=0 γiR(si, ai)] - V(s))[ 6 小
due to the well-known one-to-one correspondence between regularization ηV and Nesterov (2005).
If We set ηv with appropriate value so that its corresponding e(ηv) > ɪ-2Y ∣∣R(s, a)k∞, we will have
V * ∈ Ωe(ηv), which means adding such constraint, or equivalently, adding the path regularization,
does not affect the optimality. Combine with the local duality, we achieve the conclusion.
In fact, based on the proof, the closer
Es〜μ(s) h(Eπb [P∞=0 YiR(Si,ai)] - V*(s))2i will be.
better local convexity, which resulting faster convergence.
πb	to	π*	is, the smaller
Therefore, we can set ηV bigger for
B.4 Stochastic Dual Ascent Update
Corollary 5 The regularized dual function `r (α, π) has gradients estimators
Vθa'r (θα,θ∏) = Ea [δ ({si,ai}k=0 ,Sk+1)Vθα log α(s)
Vθ∏'r (θa,θπ ) = Ea hδ ({si,ai}k=0 ,Sk+l) Pk=0 Vθ∏ log n(a|s)
Proof We mainly focus on deriving Vθπ `r (θa, θπ). The derivation of Vθα `r (θa, θπ) is similar.
By chain rule, we have
Vθ∏4r (θa,θ∏) = (VvLk(V(α,θ),α, θ) - 2ηv (Enb [P∞=° YiR(Si,&)] - V*(S))) V V(α, θ)
X------------------------------------------{--------------------------}
0
k
+Ea δ({si,&}：=。，sk+ι)Xvθ∏logn(a|s)
i=0
k
=Ea δ({si,&}：=。，sk+ι)Xvθ∏logn(a|s) .
i=0
19
Published as a conference paper at ICLR 2018
The first term in RHS equals to zero due to the first-order optimality condition for
V(α,π) = argmi□v Lr (V,a,π).	■
B.5 Practical Algorithm
Theorem 6 In t-th iteration, given Vt and πt-1,
argmax Eμ(s)∏t-i(s)
α>0
m max (0, Ent ι
ηα
[(α(s) + ημ) δ {ssia ai}i=0 , sk+l) ] -ηα kαkμ
δ {si, ai}i=0 , sk+1	.
Proof Recall the optimization w.r.t. a is maxα>o Eμ [α(s)En [δ ({si,@}：=。，sk+ι) ] - ηαα2(s)],
denote τ(s) as the dual variables of the optimization, we have the KKT condition as
ηα α
T (s)α(s
α
τ
= τ + Eπ δ {si,ai}ik=0 , sk+1
= 0,
>	0,
>	0,
'〜	= T+Eπ [δ({si,ai}k=0,sk+ι)]
ηɑ
τ(s) τ(s) + Eπ δ {si,ai}ik=0 , sk+1	= 0,
α	> 0,
T	> 0,
⇒ τ(s)
δ {si,ai}ik=0, sk+1
Eπ δ {si,ai}ik=0 , sk+1
Eπ hδ {si,ai}ik=0 , sk+1
<0
>0
Therefore, in t-th iteration, αt(s) = n- max(0, En
δ {si,ai}ik=0, sk+1	.
C Experiment Details
Policy and value function parametrization. For fairness, we use the same parametrization across
all the algorithms. The parametrization of policy and value functions are largely based on the recent
paper by Rajeswaran et al. (2017), which shows the natural policy gradient with the RBF neural
network achieves the state-of-the-art performances of TRPO on MuJoCo. For the policy distribution,
we parametrize it as ∏θπ (a|s) = N(μθπ (s), ∑θπ ), where μθπ (S) is a two-layer neural nets with
the random features of RBF kernel as the hidden layer and the Σθπ is a diagonal matrix. The
RBF kernel bandwidth is chosen via median trick (Dai et al., 2014; Rajeswaran et al., 2017). The
same as Rajeswaran et al. (2017), we use 100 hidden nodes in Pendulum, InvertedDoublePendulum,
Swimmer, Hopper, and use 500 hidden nodes in HalfCheetah and Walker. Since the TRPO and PPO
uses GAE (Schulman et al., 2015b) with linear baseline as V, we also use the parametrization for
V in our algorithm. However, the Dual-AC can adopt arbitrary function approximator without any
change.
Training details. We report the hyperparameters for each algorithms here. We use the γ = 0.995
for all the algorithms. We keep constant stepsize and tuned for TRPO, PPO and Dual-AC in
{0.001, 0.01, 0.1}. The batchsize are set to be 52 trajectories for comparison to the competitors
in Section 6.2. For the Ablation study, we set batchsize to be 24 trajectories for faster runtime. The
CG damping parameter for TRPO is set to be 10-4. We iterate 20 steps for the Fisher information
matrix computation. For the ηv, η*, n- in DUaI-AC from {0.001,0.01,0.1,1}.
20