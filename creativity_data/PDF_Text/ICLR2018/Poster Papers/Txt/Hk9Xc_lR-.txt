Published as a conference paper at ICLR 2018
On the Discrimination-Generalization Trade-
off in GANs
Pengchuan Zhang
Microsoft Research, Redmond
penzhan@microsoft.com
Dengyong Zhou
Google
dennyzhou@google.com
Qiang Liu
Computer Science, Dartmouth College
qiang.liu@dartmouth.edu
Tao Xu
Computer Science, Lehigh University
tax313@lehigh.edu
Xiaodong He
Microsoft Research, Redmond
xiaohe@microsoft.com
Ab stract
Generative adversarial training can be generally understood as minimizing certain
moment matching loss defined by a set of discriminator functions, typically neu-
ral networks. The discriminator set should be large enough to be able to uniquely
identify the true distribution (discriminative), and also be small enough to go be-
yond memorizing samples (generalizable). In this paper, we show that a discrim-
inator set is guaranteed to be discriminative whenever its linear span is dense in
the set of bounded continuous functions. This is a very mild condition satisfied
even by neural networks with a single neuron. Further, we develop generaliza-
tion bounds between the learned distribution and true distribution under different
evaluation metrics. When evaluated with neural distance, our bounds show that
generalization is guaranteed as long as the discriminator set is small enough, re-
gardless of the size of the generator or hypothesis set. When evaluated with KL
divergence, our bound provides an explanation on the counter-intuitive behaviors
of testing likelihood in GAN training. Our analysis sheds lights on understanding
the practical performance of GANs.
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) and their variants can be gen-
erally understood as minimizing certain moment matching loss defined by a set of discriminator
functions. Mathematically, GANs minimize the integral probability metric (IPM) (Muller, 1997),
that is,
min d d，F(^m,ν) := SUp {Eχ〜μm[f(x)] - Ex〜”[f(x)]}
ν∈G	m	f∈F
(1)
where μm is the empirical measure of the observed data, and F and G are the sets of discriminators
and generators, respectively.
1.	Wasserstain GAN (W-GAN) (Arjovsky et al., 2017). F = Lip1(X) := {f : ||f ||Lip ≤ 1},
corresponding to the Wasserstain-1 distance.
2.	MMD-GAN (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017a). F is taken as the unit ball in
a Reproducing Kernel Hilbert Space (RKHS), corresponding to the Maximum Mean Discrepency
(MMD).
3.	Energy-based GANs (Zhao et al., 2016). F is taken as the set of continuous functions bounded
between 0 and M for some constant M > 0, corresponding to the total variation distance (Ar-
jovsky et al., 2017).
1
Published as a conference paper at ICLR 2018
4.	f-GAN (Nowozin et al., 2016) minimizes the f -divergence, which can be viewed a form of
regularized moment matching loss defined over all possible functions as shown by Liu et al.
(2017). See also Appedix B.
Due to computational tractability, however, the practical GANs take F as a parametric function
class, typically, Fnn = {fθ(x) : θ ∈ Θ} where fθ(x) is a neural network indexed by parameters θ
that take values in Θ ⊂ Rp. Consequently, the related dFnn (μ, V) is called neural network distance,
or neural distance (Arora et al., 2017). Although dFnn (μ, V) is meant to be a surrogate, its properties
can be fundamentally different from the original objective functions. For example, in W-GAN,
because Fnn is a much smaller discriminator set than Lip1(X), it is unclear from the current GAN
literature whether dFnn (μ, V) is a discriminative metric in that dʃnn (μ, ν) = 0 implies μ = ν. This
discrimination is critical to ensure the consistency of the learning result. This motivated us to study
the properties of dFnn (μ, V) with parametric function sets Fnn, instead of the original Wasserstein
distance or f -divergence.
A broader question is in developing learning bounds and studying how they depend on the discrim-
inator set F and the generator set G, under different evaluation metrics of interest. Specifically,
assuming Vm is an (approximate) solution of (1), we are interested in obtaining bounds between Vm
and the underlying true distribution μ, under a given evaluation metric deval(μ, Vm). Existing analy-
sis has been mostly focusing on the case when the evaluation metric coincides with the optimization
metric, that is, deval(μ, V) = dF(μ, V), which, however, favors smaller discriminator sets that define
“easier” evaluation metrics. It is of interest to develop bounds for evaluation metrics independent
of F, such as bounded Lipschitz distance that metrizes weak convergence, and KL divergence that
connects to testing likelihood.
Contribution. We show that the role of discriminators F is best illustrated by the conditions under
which dF (μ, V) metrizes weak convergence (or convergence in distribution), that is,
dF(μ, Vm) → 0 if and only if Vm * μ,	(2)
for any probability measures μ and Vm,. The choice of F should strike a balance to achieve (2):
i)	F should be large enough to make &于(μ, v) discrimiantive in that dF(μ, Vm) → 0 can imply
that Vm weakly converges to μ. Further, with a given metric dɛval(μ, ν), the discriminator set
F should be large enough so that a small dF(μ, ν) implies a small deval(μ, ν) in certain sense.
These are basic requirements in justifying &于(μ, V) as a valid learning objective function.
ii)	F should also be relatively small so that Vm * μ implies that dF(μ, Vm) approaches to zero.
This is essential to guarantee that the training and testing loss are similar to each other and hence
the algorithm is generalizable. Further, in order to obtain a low sample complexity, F should be
sufficiently small so that dF(μ, Vm) decays with a fast rate, preferably O(1∕√m).
The theme of this work is to characterize the conditions under which i) and ii) hold and develop
bounds of deval(μ, Vm) that characterize the role of discriminators F and generators G. Our contri-
butions are summarized as follows.
1.	We show that a discriminator set F is discriminative once the linear span of F is dense in the
bounded continuous (or Lipschitz) function space. This is a mild condition that can satisfied, for
example, even for neural networks consists of a single neuron. See Section 2.
2.	We develop techniques using neural distance dF(μ, V) to provide upper bounds of different eval-
uation metrics deval(μ, V) of interest, including bounded Lipschitz (BL) distance and KL diver-
gence, which provides a key step for developing learning bounds of GANs under these metrics.
See Section 2.1.
3.	We characterize the generalizability of GANs using the Rademacher complexity of discriminator
set F and put together bounds between the true distributions μ and GAN estimators Vm under
different evaluation metrics in Section 3. Under the neural distance, our bounds (Corollary 3.2-
3.3) show that generalization is guaranteed as long as the discriminator set is small enough,
regardless of the size of the generator or hypothesis set G . This seemingly-surprising result is
reasonable because in this case the evaluation metric &f (μ, Vm) depends on the discriminator
set.
2
Published as a conference paper at ICLR 2018
4.	When the KL divergence is used as the evaluation metric, our bound (Corollary 3.5) suggests
that the generator and discriminator sets have to be compatible in that the log density ratios of
the generators and the true distributions should exist and be included inside the linear span of
the discriminator set. The strong condition that log-density ratio should exist partially explains
the counter-intuitive behavior of testing likelihood in flow GANs (e.g., Danihelka et al., 2017;
Grover et al., 2017).
5.	We extend our analysis to study neural f -divergences that are the learning objective of f -GANs,
and establish similar results on the discrimination and generalization properties of neural f-
divergences; see Appedix B. Different from neural distance, a neural f -divergence is discrimi-
native if linear span of its discriminators without the output activation function is dense in the
bounded continuous function space.
1.1 Notations
We use X to denote a subset of Rd. For each continuous function f: X → R, we define the maxi-
mum norm as kfk∞ = supx∈X |f (x)|, and the Lipschitz norm kf kLip = sup{|f (x) - f (y)|/kx -
yk : x,y ∈ X, x 6= y}, and the bounded Lipschitz (BL) norm kf kBL = max{kf kLip, kfk∞}. The
set of continuous functions on X is denoted by C(X), and the Banach space of bounded continuous
function is Cb (X) = {f ∈ C(X) : kf k∞ < ∞}.
The set of Borel probability measures on X is denoted by PB (X). In this paper, we assume that all
measures involved belong to PB(X), which is sufficient in all practical applications. We denote by
Eμ [f ] the integral of f with respect to probability measure μ. The weak convergence, or convergence
in distribution, is denoted by νn * ν. Given a base measures τ (e.g., Lebesgue measure), the density
of μ ∈ PB(X), if it exists, is denoted by ρμ = 黑. We do not assume density exists in our main
theoretical results, except the cases when discussing KL divergence.
2 Discriminative properties of neural distances for GAN
As listed in the introduction, many variants of GAN can be viewed as minimizing the integral prob-
ability metric (1). Without loss of generality, we assume that the discriminator set F is even, i.e.,
f ∈ F implies -f ∈ F. Intuitively speaking, minimizing (1) towards zero corresponds to match-
ing the moments Eμ [f ] = EV [f ] for all discriminators f ∈ F. In their original formulation, all
those discriminator sets are non-parametric, infinite dimensional, and large enough to guarantee that
dF(μ, ν) = 0 implies μ = V.
In practice, however, the discriminator set is typically restricted to parametric function classes of
form Fnn = {fθ : θ ∈ Θ}. When fθ is a neural network, we call dFnn(μ, V) a neural distance
following Arora et al. (2017). Neural distances are the actual object function that W-GAN optimizes
in practice because they can be practically optimized and can leverage the representation power of
neural networks. Therefore, it is of great importance to directly study neural distances, instead of
Wasserstein metric, in order to understand practical performance of GANs.
Because the parameter function set Fnn is much smaller than the non-parametric sets like Lip1(X), a
key question is whether Fnn is large enough so that moment matching on Fnn (i.e., dʃnn (μ, V) = 0)
implies μ = V. It turns out the answer is affirmative once Fnn is large enough so that its linear span
(instead of Fnn itself) forms a universal approximator. This is a rather weak condition, which is
satisfied even by very small sets such as neural networks with a single neuron.
We make this concrete in the following.
Definition 2.1. Let (X, dX ) be a metric space and F be a set of functions on X. We say that
dF(μ, ν) (and F) is discriminative if
dF (μ, ν) = 0	^⇒ μ = ν,
for any two Borel probability measures μ, ν ∈ PB(X). In other words, F is discriminative if the
moment matching on F, i.e., Eμ[f] = EV [f] for any f ∈ F, implies μ = V.
The key observation is that Eμ [f ] = EV [f ] for any f ∈ F implies the same holds true for all f in
the linear span ofF. Therefore, it is sufficient to require the linear span ofF, instead of F itself, to
be large enough to well approximate all the indicator test functions.
3
Published as a conference paper at ICLR 2018
Theorem 2.2. For a given function set F ⊂ Cb(X), define
n
spanF := {α° + ɪ2 αifi ： a ∈ R, fi ∈ F, n ∈ N}.	(3)
i=1
Then dF (μ, ν) is discriminative if spanF is dense in the space of bounded continuous functions
Cb(X) under the uniform norm || ∙ ∣∣∞, that is, for any f ∈ Cb(X) and e > 0, there exists an
f ∈ spanF such that kf - f k∞ ≤ . An equivalent way to put is that Cb(X) is included in the
closure of spanF, that is,
cl(spanF) ⊇ Cb(X).	(4)
Further, (4) is a necessary condition for dF (μ, V) to be discriminative if X is a compact space.
Remark 2.1. The basic idea of characterizing probability measures using functions in Cb(X) is
closely related to the concept of weak convergence. Recall that a sequence νn weakly converges to
μ, i.e., Vn * μ, if and only if Eνn[f] → Eμ [f] for all f ∈ Cb(X). Proof of the SUffiCient part of
Theorem 2.2 is standard and the same with proof of the uniqueness of weak convergence; see, e.g.,
Lemma 9.3.2 in Dudley (2002).
Remark 2.2. We obtain similar results for neural f -divergence dφF (μ∣∣ν) in Theorem B.1. The
difficulty in analyzing neural f -divergence is that moment matching on the discriminator set is only
a sufficient condition for minimizing neural f -divergence, i.e.,
{ν : Eμ[f] = Eν[f], ∀f ∈ F} ⊆ {ν : dφ,F (μ∣∣ν) = arg min dφ,F (μ∣∣ν)}.
Consequently, cl(spanF) ⊇ Cb(X) is only necessary but not sufficient for a neural f -divergence
to be discriminative. When the discriminators are neural networks, we show that moment matching
on the function set that consists of discriminators without their output activation function, denoted
as F0, is a necessary condition for minimizing neural f -divergence, i.e.,
{ν : dφ,F (μ∣∣ν) = arg min dφ,F (μ∣∣ν)} ⊆ {ν : Eμ[fo] = EV [fo], ∀fo ∈ Fo}.
We refer to Theorem B.1 (ii) for a precise statement. Therefore, a neural f -divergence is discrimina-
tive if linear span of its discriminators without the output activation function is dense in the bounded
continuous function space.
Remark 2.3. Because the set of bounded Lipschitz functions BL(X) = {f ∈ Cb(X) : ||f ||Lip <
∞} is dense in Cb(X), the condition in (4) can be replaced by a weaker condition cl(spanF) ⊇
BL(X). One can define a norm ∣∣ ∙ ∣∣bl forfunctions in BL(X) by IlfkBL = max{kf ∣∣Lip, kf k∞}.
This defines the bounded Lipschitz (BL) distance,
dBL (μ,V) =
max
f∈BL(X)
{Eμ f - EV f : IIfIlBL
≤ 1}.
The BL distance is known to metrize weak convergence in sense that dBL(μ, Vn) → 0 is equivalent
to Vn * μ for all Borel probability measures on Rd; see section 8.3 in Bogachev (2007).
Neural distances are discriminative. The key message of Theorem 2.2 is that it is sufficient to
require cl(spanF) ⊇ Cb(X) (Condition (4)), which is a much weaker condition than the perhaps
more straightforward condition cl(F) ⊇ Cb(X). In fact, (4) is met by function sets that are much
smaller than what we actually use in practice. For example, it is satisfied by the neural networks
with only a single neuron, i.e.,
Fnn = {σ(w>x + b): W ∈ Rd, b ∈ R}.	(5)
This is because its span spanFnn includes neural networks with infinite numbers of neurons, which
are well known to be universal approximators in Cb(X) according to classical theories (e.g., Cy-
benko, 1989; Hornik et al., 1989; Hornik, 1991; Leshno et al., 1993; Barron, 1993). We recall the
following classical result.
Theorem 2.3 (Theorem 1 in Leshno et al. (1993)). Let σ : R → beaeo continuous activation
function and X ⊂ Rd be any compact set. Let Fnn be the set of neural networks with a single
neuron as defined in (5), then spanFnn is dense in C(X) if and only ifσ is not a polynomial.
4
Published as a conference paper at ICLR 2018
The above result requires that the parameters [w,b] take values in Rd+1. In practice, however,
we can only efficiently search in bounded parameter sets of [w, b] using local search methods like
gradient descent. We observe that it is sufficient to replace Rd+1 with a bounded parameter set Θ
for non-decreasing homogeneous activation functions such as σ(u) = max{u, 0}ɑ with α ∈ N;
note that α = 1 is the widely used rectified linear unit (ReLU).
Corollary 2.4. Let X ⊂ Rd be any compact set, and σ(u) = max{u, 0}α (α ∈ )^a and Fnn =
{σ(w>x + b) : [w, b] ∈ Θ}. Then spanFnn is dense in Cb(X) if
{λθ : λ ≥ 0,θ ∈ Θ} = Rd+1.
For the case when Θ = {θ ∈ Rd+1 : |网必 ≤ 1}, Bach (2017) not only proves that SpanFnn is
dense in Lip1(X) (and thus dense in Cb(X)), but also gives the convergence rate.
Therefore, for ReLU activation functions, Fnn with bounded parameter sets, like {θ : kθk ≤ 1}
or {θ : ∣∣θk = 1} for any norm on Rd+1, is sufficient to discriminate any two Borel probability
measures. Note that this is not true for some other activation functions such as tanh or sigmoid,
because there is an approximation gap between span{σ(w>x + b) : [w,b] ∈ Θ ⊂ Rd+1} and
Cb(X) when Θ ⊂ Rd+1 is bounded; see e.g., Barron (1993) (Theorem 3). From this perspective,
homogeneous activation functions such as ReLU are preferred as discriminators.
One advantage of using bounded parameter set Θ is that it makes Fnn have a bounded Lipschitz
norm, and hence the corresponding neural distance is upper bounded by Wasserstein distance. In
fact, W-GAN uses weight clipping to explicitly enforce ∣θ∣∞ ≤ δ. However, we should point out
that the Lipschitz constraint does not help in making F discriminative since the constraint decreases,
instead of enlarges, the function set F. Instead, the role of the Lipschitz constraint should be mostly
in stabilizing the training (Arjovsky et al., 2017) and assuring a generalization bound as we discuss
in Section 3. Another related way to justify the Lipschitz constraint is its relation to metrizing weak
convergence, as we discuss in the sequel.
Neural distance and weak convergence. If F is discriminative, then dF(μ, ν) = 0 implies μ =
ν. In practice, however, we often cannot achieve &于(μ,ν) = 0 strictly. Instead, we often have
dF (μ, Vn) → 0 fora sequence of Vn and want to establish the weak convergence Vn * μ.
Theorem 2.5. Let (X, dX ) be any metric space. If spanF is dense in Cb(X), we have
limn→∞ dF(μ, Vn) = 0 implies Vn weakly converges to μ.
Additionally, if F is contained in a bounded Lipchitz function space, i.e., there exists 0 < C < ∞
such that ||f ||bl ≤ C forall f ∈ F, then Vn weakly converges to μ implies limn→∞ dF(μ, Vn) = 0.
Theorem 10 of Liu et al. (2017) states a similar result for generic adversarial divergences, but does
not obtain the specific weak convergence result for neural distances due to lacking of Theorem 2.2.
Another difference is that Theorem 10 of Liu et al. (2017) heavily relies on the compactness assump-
tion ofX, while our result does not need this assumption. We provide the proof for Theorem 2.5 in
Appendix C.
When X is compact, Wasserstein distance and the BL distance are equivalent and both metrize
weak convergence. As we discussed earlier, the condition cl(spanF) = Cb(X) and F ⊆ LipK (X)
are satisfied by neural networks Fnn with ReLU activation function and bounded parameter set Θ.
Therefore, the related neural distance dFnn is topologically equivalent to the Wasserstein and BL
distance, because all of them metrize the weak convergence. This does not imply, however, that they
are equivalent in the metric sense (or strongly equivalent) since the ratio "bl(m, v) /dFnn (μ, V) can
be unbounded. In general, the neural distances are weaker than the BL distance because of smaller
F. In Section 2.1 (and particularly Corollary 2.8), we draw more discussions on the bounds between
BL distance and neural distances.
2.1 Discriminative power of neural distances
Theorem 2.2 characterizes the condition under which a neural distance is discriminative, and shows
that even neural networks with a single neuron are sufficient to be discriminative. This does not
explain, however, why it is beneficial to use larger and deeper networks as we do in practice. What
is missing here is to frame and understand how discriminative or strong a neural distance is. This is
5
Published as a conference paper at ICLR 2018
because even if dF(μ, V) is discriminative, it can be relatively weak in that d，F(μ, V) may be small
when μ and V are very different under standard metrics (e.g., BL distance). Obviously, a larger F
yields a stronger neural distance, that is, if F ⊂ F0, then dF(μ, V) ≤ dF0(μ, V). For example,
because it is reasonable to assume that neural networks are bounded Lipschitz when X and Θ are
bounded, we can control a neural distance with the BL distance:
dF (μ, V) ≤ CdBL(μ, V),
where C := supf ∈F {||f ||BL} < ∞. A more difficult question is if we can establish inequalities in
the other direction, that is, controlling dBL(μ, V), or in general a stronger dF0(μ, V), with a weaker
dF (μ, V) in some way. In this section, we characterize conditions under which this is possible
and develop bounds that allow us to use neural distances to control stronger distances such as BL
distance, and even KL divergence. These bounds are used in Section 3 to translate generalization
bounds in dF (μ, V) to that in BL distance and KL divergence.
The core of the discussion involves understanding how dF (μ, V) can be used to control the difference
of the moment | Eμ g - EV g| for g outside of F. We address this problem by two steps: first
controlling functions in spanF, and then functions in cl(spanF) that is large enough to include
Cb(X) for neural networks.
Controlling functions in SPanF. We start with understanding how &于(μ, V) can bound | E* g -
Eν g | for g ∈ spanF. This can be characterized by introducing a notion of norm on spanF.
Proposition 2.6. For each g ∈ SpanF that can be decomposed into g = in=1 wi fi + w0 as we
define in (3), the F -variation norm ||g ||F,1 ofg is the infimum of in=1 |wi| among all possible
decompositions of g, that is,
nn
∣∣g∣∣F,1 = inf < EIwiI: g = Ewifi + W0, ∀n ∈ N, wo, Wi ∈ R, fi ∈ F 卜
i=1	i=1
Then we have
I Eμ g - EV g| ≤ ∣∣g∣∣F,1 dF(μ, v),	∀g ∈ spanF.
Intuitively speaking, IIgIIF,1 denotes the “minimum number” of functions in F needed to represent
g. As F becomes larger,	∣∣g∣∣F,ι	decreases and	dF(μ,	V)	can better control	∣ Eμ	g -	EV g∣.	Pre-
cisely, if F ⊆ F0 then IIgIIF0,1 ≤ IIgIIF,1. Therefore, although adding more neurons in F may not
necessarily enlarge SpanF, it decreases IIgIIF,1 and yields a stronger neural distance.
Controlling functions in cl(SpanF). A more critical question is how the neural distance dF (μ, V)
can also control the discrepancy Eμ g - EV g for functions outside of spanF but inside Cl(SpanF).
The bound in this case is characterized by a notion of error decay function defined as follows.
Proposition 2.7. Given a function g, we say that g is approximated by F with error decay function
(r) iffor any r ≥ 0, there exists an fr ∈ SpanF with IIfr IIF,1 ≤ r such that IIf - fr II∞ ≤ (r).
Therefore, g ∈ cl(SpanF) if and only ifinfr≥0 (r) = 0. We have
I Eμ g - EV g∣≤ inf {2e(r) + r &F(μ, ν)}.
r≥0
In particular, if e(r) = O(r-κ) for some κ > 0, then ∣ E* g — EV g∣ = O(&f (μ, V)κ+1).
It requires further efforts to derive the error decay function for specific F and g. For example,
Proposition 6 of Bach (2017) allows us to derive the decay rate of approximating bounded Lipschitz
functions with rectified neurons, yielding a bound between BL distance and neural distance.
Corollary 2.8. Let X be the unit ball of Rd undernorm ∣∣∙∣∣q for some q ∈ [2, ∞), that is, X = {x ∈
Rd ： ∣∣x∣∣q ≤ 1}. Consider F consisting ofa single rectified neuron F = {max(v>[x; 1], 0)α: V ∈
Rd+1, ∣∣v∣∣p = 1} where α ∈ N, P + ɪ = 1. Then we have
〜	____1___
dBL(μ, V) = O(dF(μ, ν) α+(d+1)∕2),	(6)
where O denotes the big-O notation ignoring the logarithm factor.
The result in (6) shows that dF (μ, V) gives an increasingly weaker bound when the dimension d
increases. This is expected because we approximate a non-parametric set with a parametric one.
6
Published as a conference paper at ICLR 2018
Likelihood and KL divergence. Maximum likelihood has been the predominant approach in sta-
tistical learning, and testing likelihood forms a standard criterion for testing unsupervised models.
The recent advances in deep unsupervised learning, however, make it questionable whether likeli-
hood is the right objective for training and evaluation (e.g., Theis et al., 2015). For example, some
recent empirical studies (e.g., Danihelka et al., 2017; Grover et al., 2017) showed a counter-intuitive
phenomenon that both the testing and training likelihood (assuming generators with valid densities
are used) tend to decrease, instead of increase, as the GAN loss is minimized. A hypothesis for
explaining this is that the neural distances used in GANs are too weak to control the KL divergence
properly. Therefore, from the theoretical perspective, it is desirable to understand under what con-
ditions (even if it is a very strong one), the neural distance can be strong enough to control KL
divergence. This can be done by the following simple result.
Proposition 2.9. Assume μ and V have positive density functions ρ*(x) and PV (x), respectively.
Then
KL(μ∣∣V)+KL(ν∣∣μ) = Eμ log(ρμ∕ρν) - EV log(ρμ∕ρν).
If log(ρμ∕ρν) ∈ SpanF, then
KL(μ∣∣ν) + KL(ν∣∣μ) ≤ || log(ρμ∕ρν)∣∣F,1 dF(μ, V).	⑺
If log(ρμ∕ρν) ∈ CI(SpanF) with an error decay function e(r) = O(r-κ), then
KL(MIIV) +KL(V"μ) = O(dF (μ,ν) κ+1).	⑻
This result shows that We require that the density ratio log(ρμ∕ρν) should exist and behave nicely
in SpanF or cl(spanF) in order to bound KL divergence with dF(μ, ν). If either μ or V is an
empirical measure, the bound is vacuum since KL(μ, V) + KL(μ, V) equals infinite, while dF(μ, V)
remains finite once F is bounded, i.e., IIf II∞ ≤ ∆ < ∞ for all f ∈ F.
Obviously, this strong condition is hard to satisfy in practice, because practical data distributions
and generators in GANs often have no densities or at least highly peaky densities. We draw more
discussions in Corollary 3.5.
3	Generalization property of GANs
Section 2 suggests that it is better to use larger discriminator set F in order to obtain stronger neural
distance. However, why do regularization techniques, which effectively shrink the discriminator
set, help GAN training in practice? The answer has to do with the fact that we observe the true
model μ only through an i.i.d. sample of size m (whose empirical measure is denoted by μm), and
hence can only optimize the empirical loss dF(μm, V), instead of the exact loss dF(μ, v). Therefore,
generalization bounds are required to control the exact loss dF (μ, V) when we can only minimize
its empirical version dF(μm, V). Specifically, let G be a class of generators that may or may not
include the unknown true distribution μ. Assume Vm minimizes the GAN loss dF(μm, V) up to an
( ≥ 0) accuracy, that is,
dF(μm, νm) ≤ inf dF(μm, ν) + E∙	⑼
V∈G
We are interested in bounding the difference between Vm and the unknown μ under certain evaluation
metric. Depending on what we care about, we may be interested in the generalization error in
terms of the neural distance dF(μ, Vm), or other standard quantities of interest such as BL distance
dBL(μ, Vm) and KL divergence KL(μ, Vm) or the testing likelihood.
In this section, we adapt the standard Rademacher complexity argument to establish generalization
bounds for GANs. We show that the discriminator set F should be small enough to be generaliz-
able, striking a tradeoff with the other requirement that it should be large enough to be discrimina-
tive. We first present the generalization bound under neural distance, which purely depends on the
Rademacher complexity of the discriminator set F and is independent of the generator set G. Then
using the results in Section (2.1), we discuss the generalization bounds under other standard metrics,
like BL distance and KL divergence.
7
Published as a conference paper at ICLR 2018
3.1	Generalization under neural distance
Using the standard derivation and the optimality condition (9), we have (see Appendix D)
dF(μ,νm) - inf dF(μ,ν) ≤ 2sup ∣Eμ[f] - E^m[f]| + E
ν∈G	f∈F	(10)
=2dF(μ, μm) + C.
This reduces the problem to bounding the discrepancy dF(μ, μm) := SuPf ∈f ∣Eμ [f] - Eμm [f]]
between the true model μ and its empirical version μ^m. This can be achieved by the uniform
concentration bounds developed in statistical learning theory (e.g., Vapnik & Vapnik, 1998) and
empirical process (e.g., Van de Geer, 2000). In particular, the concentration property related to
SuPf ∈f ∣Eμ [f ] - Eμm [f ]| can be characterized by the Rademacher complexity of F (w.r.t. measure
μ), defined as
Rmɔ(F):= E supɪ XTif(Xi) ,	(11)
m	f∈F m i
where the expectation is taken w.r.t. Xi 〜μ, and Rademacher random variable τi: Prob(Ti =
1) = Prob(Ti = -1) = 1/2. Intuitively, Rm (F) characterizes the ability of overfitting with pure
random labels using functions in F and hence relates to the generalization bounds. Standard results
in learning theory show that
SuP ∣Eμ[f] - E^m [f ]| ≤ Rmt)(F) + O(∆Jl°g≡¾
f∈F	m
where ∆ = SuPf∈f ||f ∣∣∞. Combining this with (10), we obtain the following result.
Theorem 3.1. Assume that the discriminator set F is even, i.e., f ∈ F implies -f ∈ F, and that
all discriminators are bounded by ∆, i.e., ∣∣f k∞ ≤ ∆ for any f ∈ F. Let μm be an empirical
measure of an i.i.d. sample of size m drawn from μ. Assume Vm ∈ G satisfies dF(μm, νm) ≤
infν∈G dF(μm, V) + c. Then with probability at least 1 - δ, we have
dF(μ, Vm)-
VnG dF(μ, V) ≤ 2R(μ)(F)+2δ∖∣	gm /)+c,
(12)
where Rmμ)(F) is the Rademacher complexity of F defined in (11).
We obtain nearly the same generalization bound for neural f -divergence in Theorem B.3. Theo-
rem 3.1 relates the generalization error of GANs to the Rademacher complexity of the discriminator
set F. The smaller the discriminator set F is, the more generalizable the result is. Therefore, the
choice ofF should strike a subtle balance between the generalizability and the discriminative power:
F should be large enough to make dF(μ, ν) discriminative as we discuss in Section 2.1, and simul-
taneously should be small enough to have a small generalization error in (12). It turns out parametric
neural discriminators strike a good balance for this purpose, given that it is both discriminative as
we show in Section 2.1, and give small generalization bound as we show in the following.
Corollary 3.2. Let X be the unit ball of Rd under norm || ∙ ∣∣2, that is, X = {x ∈
Rd ： ∣∣x∣∣2 ≤ 1}. Assume that F is neural networks with a single rectified linear unit (ReLU)
F = {max(v>[x; 1], 0): v ∈ Rd+1, ∣∣v∣∣2 = 1}. Then with probability at least 1 - δ,
C
dF(μ,νm) ≤ inf dF(μ, ν) + √m + C	(13)
and
dBL(μ, Vm)
inf dF (μ, V) +
ν∈G
1
(d+3)∕2
(14)
where C = 4√2 + 4,log(1∕δ) and O denotes the big-O notation ignoring the logarithm factor.
8
Published as a conference paper at ICLR 2018
Note that the three terms in Eqn. (13) take into account the modeling error (infv∈g d，F(μ, ν)), sample
complexity and generalization error (C∕√m), and optimization error (e), respectively. Assuming
zero modeling error and optimization error, We have (1) dF(μ, Vm) = O(m-1/2), which achieves
the typical parametric convergence rate; (2) dBL(μ, Vm) = O(m-d+3), which becomes slower as
the dimension d increases. This decrease is because of the non-parametric nature of BL distance,
instead of learning algorithm. As we show in Appendix A, we obtain a similar rate of dBL(μ, Vm) =
O(m- d), even if we directly use BL distance as the learning objective.
Similar results can be obtained for general parametric discriminator class as follows.
Corollary 3.3. Under the condition of Theorem 3.1, we further assume that (1) F = {fθ : θ ∈ Θ ⊂
[-1, 1]p} is a parametric function class with p parameters in a bounded set Θ and that (2) every fθ
is L-Lipschitz continuous with respect to the parameters θ, i.e., kfθ - fθ0 k∞ ≤ Lkθ - θ0k2. Then
with probability at least 1 - δ, we have
C
dF(μ, νm) ≤ inf dF(μ, ν) + √m + e,	(15)
where C = 16√2πpL + 2∆p2 log(1∕δ).
This result can be easily applied to neural discriminators, since neural networks fθ(x) are generally
Lipschitz w.r.t. the parameter θ, once the input domain X is bounded. For neural discriminators,
we also apply the bound on the Rademacher complexity of DNNs recently derived in Bartlett et al.
(2017), which gives a sharper bound than that in Corollary 3.3; see Appendix A.1.
With the basic result in Theorem 3.1, we can also discuss the learning bounds of GANs with choices
of non-parametric discriminators. Making use of Rademacher complexity of bounded sets in a
RKHS (e.g., Lemma 22 in Bartlett & Mendelson (2003)), we give the learning bound of MMD-
based GANs (Li et al., 2015; Dziugaite et al., 2015) as follows. We present the results for Wasser-
stein distance and total variance distance in Appendix A.2, and highlight the advantages of using
parametric neural discriminators.
Corollary 3.4. Under the condition of Theorem 3.1, we further assume that F = {f ∈ H : kfkH ≤
1} where H is a RKHS whose positive definite kernel k(x, x0) satisfies k(x, x) ≤ Ck < +∞ for all
x ∈ X. Then with probability at least 1 - δ,
C
dF(μ, νm) ≤ inf dF(μ, ν) + √m + 3	(16)
where C = 2(2+ P2 log(1∕δ)) √Ck.
Remark 3.1 (Comparisons with results in Arora et al. (2017)). Arora et al. (2017) also discussed
the generalization properties of GANs under a similar framework. In particular, they developed
bounds of form ∣dF (μ, V) 一 dF (μm, Vm,) | where μm, and Vm are empirical versions of the target
distribution μ and V with sample size m. Our framework is similar, but considers bounding the
quantity dF (μ, Vm) 一 infν∈G dF (μ, V), which is of more direct interest. In fact, our Eqn. (10) shows
that our generalization error can be bounded by the generalization error studied in Arora et al.
(2017). Another difference is that we adapt the Rademacher complexity argument to derive the
bound, while Arora et al. (2017) made use of the 3-net argument.
Bounding the KL divergence and testing likelihood. The above results depend on the evaluation
metric we use, which is dF (μ, V) or dBL(μ, V). If we are interested in evaluating the model using
even stronger metrics, such as KL divergence or equivalently testing likelihood, then the generator
set G enters the scene in a more subtle way, in that a larger generator set G should be companioned
with a larger discriminator set F in order to provide meaningful bounds on KL divergence. This is
illustrated in the following result obtained by combining Theorem 3.1 and Proposition 2.9.
Corollary 3.5. Assume both the true μ and all the generators V ∈ G have positive densities ρ* and
PV, respectively. Assume F consists ofboundedfunctions with ∆ := SuPf∈f ||f ∣∣∞ < ∞.
Further, assume the discriminator set F is compatible with the generator set G in the sense that
log(Pν∕ρμ) ∈ SpanF, ∀v ∈ G, with a compatible coefficient defined as
Λf,g ：= SuP||log(ρν/Pμ)∣∣f,i < ∞∙
ν∈G
9
Published as a conference paper at ICLR 2018
Then
KL(μ, Vm) ≤ Λf g(2Rm) (F) + 2∆√2 log(1∕δ)∕m + ∆ inf √KL(μ, ν) + E).	(17)
ν∈G
Different from the earlier bounds, the bound in (17) depends on the compatibility coefficient ΛF,G
that casts a more interesting trade-off on the choice of the generator set G : the generator set G
should be small and have well-behaved density functions to ensure a small ΛF,G, while should be
large enough to have a small modeling error infν∈G ∖∕KL(μ, ν). Related, the discriminator set
should be large enough to include all density ratios log(ρμ∕ρν) in a ball of radius Λf,g of SpanF,
and should also be small to have a low Rademacher complexity Rm)(F). Obviously, one can also
extend Corollary 3.5 using (8) in Proposition 2.7, to allow log(ρμ∕ρν) ∈ CI(SpanF) in which case
the compatibility of G and F should be mainly characterized by the error decay function E(r).
KL(μ, Vm) = Eμ [logpμ] - Eμ [logpνm] is the difference between the testing likelihood Eμ [logpνm]
of estimated model Vm and the optimal testing likelihood Eμ[logpμ]. Therefore, Corollary 3.5
also provides a bound for testing likelihood. Unfortunately, the condition in Corollary 3.5 is rather
strong, in that it requires that both the true distribution μ and the generators V have positive densities
and that the log-density ratio log(ρμ∕ρν) be well-behaved. In practical applications of computer
vision, however, both μ and V tend to concentrate on local regions or sub-manifolds of X, with
very peaky densities, or even no valid densities; this causes the compatibility coefficient ΛF,G very
large, or infinite, making the bound in (17) loose or vacuum. This provides a potential explanation
for some of the recent empirical findings (e.g., Danihelka et al., 2017; Grover et al., 2017) that the
negative testing likelihood is uncorrelated with the GAN loss functions, or even increases during
the GAN training progress. The underlying reason here is that the neural distance is not strong
enough to provide meaningful bound for KL divergence. See Appendix E for an illustration using
toy examples.
4	Related work
There is a surge of research interest in GANs; however, most of the work has been empirical in
nature. There has been some theoretical literature on understanding GANs, including the discrimi-
nation and generalization properties of GANs.
The discriminative power of GANs is typically justified by assuming that the discriminator set F
has enough capacity. For example, Goodfellow et al. (2014) assumes that F contains the optimal
discriminator Pdtpd；+x)(). Similar capacity assumptions have been made in nearly all other GANs
to prove their discriminative power; see, e.g., Zhao et al. (2016); Nowozin et al. (2016); Arjovsky
et al. (2017). However, discriminators are in practice taken as certain parametric function class, like
neural networks, which violates these capacity assumptions. The universal approximation property
of neural networks is used to justify the discriminative power empirically. In this work, we show that
the GAN loss is discriminative if SpanF can approximate any continuous functions. This condition
is very weak and can be satisfied even when none of the discriminators is close to the optimal
discriminator. The MMD-based GANs (Li et al., 2015; Dziugaite et al., 2015; Li et al., 2017a) avoid
the parametrization of discriminators by taking advantage of the close-form solution of the optimal
discriminator in the non-parametric RKHS space. Therefore, the capacity assumption is satisfied in
MMD-based GANs, and their discriminative power is easily justified.
Liu et al. (2017) defines a notion of adversarial divergences that include a number of GAN objective
functions. They show that if the objective function is an adversarial divergence with some additional
conditions, then using a restricted discriminator family has a moment-matching effect. Our treat-
ment of the neural divergence is directly inspired by them. We refer to Remark B.1 for a detailed
comparison. Liu et al. (2017) also shows that for objective functions that are strict adversarial di-
vergence, convergence in the objective function implies weak convergence. However, they do not
provide a condition under which an adversarial divergence is strict. A major contribution of our
work is to fill this gap, and to provide such a condition that is sufficient and necessary.
Dziugaite et al. (2015) studies generalization error, defined as dF(μ, Vm) - infν∈G dF(μ, V) in our
notation, for MMD-GAN in terms of fat-shattering dimension. Moreover, Dziugaite et al. (2015)
obtains a generalization bound that incorporates the complexity of the hypothesis set G . Although
10
Published as a conference paper at ICLR 2018
their overall error bound is still O(m-1/2), their work shows the possibility to sharpen our G-
independent bound. Arora et al. (2017) studies the generalization properties of GANs through the
quantity dF(μ, V) - dF(μm, Vmb) (in our notations). The main difference between our work and
Arora et al. (2017) is the definition of generalization error; see more discussions in Remark 3.1.
Moreover, Arora et al. (2017) allows only polynomial number of samples from the generated distri-
bution because the training algorithm should run in polynomial time. We do not consider this issue
because in this work we only study the statistical properties of the objective functions and do not
touch the optimization method. Finally, Arora et al. (2017) shows that the GAN loss can approach
its optimal value even if the generated distribution has very low support, and (Arora & Zhang, 2017)
provides empirical evidence for this problem. Our result is consistent with their results because our
generalization error is measured by the neural distance/divergence.
Finally, there are some other lines of research on understanding GANs. Li et al. (2017b) studies
the dynamics of GAN’s training and finds that: a GAN with an optimal discriminator provably
converges, while a first order approximation of the discriminator leads to unstable dynamics and
mode collapse. Lei et al. (2017) studies WGAN and optimal transportation by convex geometry and
provides a close-form formula for the optimal transportation map. Hu et al. (2017) provides a new
formulation of GANs and variational autoencoders (VAEs), and thus unifies the most two popular
methods to train deep generative models. We’d like to mention other recent interesting research
on GANs, e.g., (Guo et al., 2017; Sinn & Rawat, 2017; Nock et al., 2017; Mescheder et al., 2017;
Tolstikhin et al., 2017; Heusel et al., 2017).
5	Conclusions
We studied the discrimination and generalization properties of GANs with parameterized discrimi-
nator class such as neural networks. A neural distance is guaranteed to be discriminative whenever
the linear span of its discriminator set is dense in the bounded continuous function space. On the
other hand, a neural divergence is discriminative whenever the linear span of features defined by the
last linear layer of its discriminators is dense in the bounded continuous function space. We also pro-
vided generalization bounds for GANs in different evaluation metrics. In terms of neural distance,
our bounds show that generalization is guaranteed as long as the discriminator set is small enough,
regardless of the size of the generator or hypothesis set. This raises an interesting discrimination-
generalization balance in GANs. Fortunately, several GAN methods in practice already choose their
discriminator set at the sweet point, where both the discrimination and generalization hold. Finally,
our generalization bound in KL divergence provides an explanation on the counter-intuitive behav-
iors of testing likelihood in GAN training.
There are several directions that we would like to explore in the future. First of all, in this pa-
per, we do not talk about methods to compute the neural distance/divergence. This is typically a
non-concave maximization problem and is extremely difficult to solve. Many methods have been
proposed to solve this kind of minimax problems, but both stable training methods and theoretical
analysis of these algorithms are still missing. Secondly, our generalization bound depends purely
on the discriminator set. It is possible to obtain sharper bounds by incorporating structural infor-
mation from the generator set. Finally, we would like to extend our analysis to conditional GANs
(see, e.g., Mirza & Osindero (2014); Springenberg (2015); Chen et al. (2016); Odena et al. (2016)),
which have demonstrated impressive performance (Reed et al., 2016a;b; Zhang et al., 2017).
References
Anonymous. Spectral normalization for generative adversarial networks. International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
B1QRgziT-.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214-223, 2017.
Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv
preprint arXiv:1706.08224, 2017.
11
Published as a conference paper at ICLR 2018
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Ma-
chine Learning Research, 18(19):1-53, 2017.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945, 1993.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. J. Mach. Learn. Res., 3:463-482, March 2003. ISSN 1532-4435. URL
http://dl.acm.org/citation.cfm?id=944919.944944.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6241-6250, 2017.
Vladimir I Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172-2180, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems (MCSS), 2(4):303-314, 1989.
Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra, and Peter Dayan. Compari-
son of maximum likelihood and gan-based training of real nvps. arXiv preprint arXiv:1705.05263,
2017.
R. M. Dudley. Real Analysis and Probability. Cambridge Studies in Advanced Mathematics. Cam-
bridge University Press, 2 edition, 2002. doi: 10.1017/CBO9780511755347.
Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neu-
ral networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First
Conference on Uncertainty in Artificial Intelligence, UAI’15, pp. 258-267, Arlington, Virginia,
United States, 2015. AUAI Press. ISBN 978-0-9966431-0-8. URL http://dl.acm.org/
citation.cfm?id=3020847.3020875.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Bridging implicit and prescribed learn-
ing in generative models. arXiv preprint arXiv:1705.08868, 2017.
Jianbo Guo, Guangxiang Zhu, and Jian Li. Generative adversarial mapping networks. arXiv preprint
arXiv:1709.09820, 2017.
Martin HeUseL HUbert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Gunter Klambauer, and
Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium.
arXiv preprint arXiv:1706.08500, 2017.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251-257, 1991.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-
versal approximators. Neural networks, 2(5):359-366, 1989.
Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P Xing. On unifying deep generative
models. arXiv preprint arXiv:1706.00550, 2017.
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in neural information processing
systems, pp. 793-800, 2009.
12
Published as a conference paper at ICLR 2018
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.
Na Lei, Kehua Su, Li Cui, Shing-Tung Yau, and David Xianfeng Gu. A geometric view of optimal
transportation and generative model. arXiv preprint arXiv:1710.05488, 2017.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward net-
works with a nonpolynomial activation function can approximate any function. Neural networks,
6(6):861-867,1993.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan:
Towards deeper understanding of moment matching network. arXiv preprint arXiv:1705.08584,
2017a.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dy-
namics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017b.
Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings
of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718-1727, 2015.
Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence proper-
ties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. arXiv preprint
arXiv:1705.10461, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances
in Applied Probability, 29(2):429-443, 1997. ISSN 00018678. URL http://www.jstor.
org/stable/1428011.
Richard Nock, Zac Cranko, Aditya Krishna Menon, Lizhen Qu, and Robert C Williamson. f-gans
in an information geometric nutshell. arXiv preprint arXiv:1707.04385, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learn-
ing what and where to draw. In NIPS, 2016a.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text-to-image synthesis. In ICML, 2016b.
Mathieu Sinn and Ambrish Rawat. Towards consistency of adversarial training for generative mod-
els. arXiv preprint arXiv:1705.09199, 2017.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative
adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Scholkopf, and Gert RG
Lanckriet. On integral probability metrics,\phi-divergences and binary classification. arXiv
preprint arXiv:0901.2698, 2009.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard
Scholkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.
13
Published as a conference paper at ICLR 2018
Sara A Van de Geer. Applications of empirical process theory, volume 91. Cambridge University
Press Cambridge, 2000.
Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learning theory, volume 1. Wiley
New York, 1998.
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial
networks. ICCV, 2017.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016.
14
Published as a conference paper at ICLR 2018
A GENERALIZATION ERROR OF OTHER DIS CRIMINATOR SETS F
A. 1 Generalization bound for neural discriminators
For neural discriminators, we can use the following bound on the Rademacher complexity of DNNs,
which was recently proposed in Bartlett et al. (2017).
Theorem A.1. Let fixed activation functions (σ1 , . . . , σL) and reference matrices (M1, . . . , ML)
be given, where σi is ρi-Lipschitz and σi (0) = 0. Let spectral norm bounds (s1, . . . , sL) and matrix
(2,1) norm bounds (b1, . . . , bL) be given. LetF denote the discriminator set consisting of all choices
of neural network fA:
Fnn := {fA	: A=:	(A1,...,AL),kAikσ ≤	si,	kAiT	- MiT k2,1	≤	bi},	(18)
where kAkσ := σmax (A) and kAk2,1 := k(kA:,1 k2, . . . , kA:,mk2)k1 are the matrix spectral norm
and (2, 1) norm, respectively, and
fA(x) := σL(ALσL-1(AL-1 . . .σ1(A1x) . . .))
is the neural network associated with weight matrices (A1, . . . , AL). Moreover, assume that each
matrix in (A1, . . . , AL) has dimension at most W along each axis and define the spectral normalized
complexity R as
R := Plog(2W2) (∏L=ιSj∙Pj) (XX (Si)2/3)	.	(19)
Let data matrix X ∈ Rm ×d be given, where the m rows correspond to data points. When the sample
size m ≥ 3kX kF R, the empirical Rademacher complexity satisfies
RRm(Fnn) ：= ET	SUP - X Tif(Xi) ≤ ^" kF R (l+lθg ∣∣^	) ,	(20)
f∈Fnn m i	m	3kXkFR
where τ = (τ1, . . . , τm) are the Rademacher random variables which are iid with Pr[τi = 1] =
Pr[τi = -1] = 1/2, kXkF is the Frobenius norm of X.
Proof. The proof is the same with the proof of Lemma A.8 in Bartlett et al. (2017). When m ≥
3∣∣X∣∣FR, we use the optimal α = 3∣∣X∣∣FR/√m to obtain the above result.	□
Combined with our Theorem 3.1, we obtain the following generalization bound for the neural dis-
criminator set defined in (18).
Corollary A.2. Suppose that the discriminator set Fnn is taken as (18) and that ∣f∣∞ ≤ ∆for any
f ∈ Fnn. Let data matrix X ∈ m×ry d be the m data points that define the empirical distribution
μm. Then with probability at least 1 一 δ, we have
dFnn (μ, νm)	≤ inf dF (μ, ν)	+ 48kX kFR (1 +	log 3∣ 温 R )	+6δJ 网聘但	+ e, QI)
nn	ν∈G	m	3∣X ∣FR	m
where R is the spectral normalized complexity defined in (19) and is the optimization error defined
in (9).
Proof. In the proof of Theorem 3.1, instead of using
sup ∣Eμ[f] - Eμm [f]| ≤ Rmt)(F) + 2∆J 号&
f∈F	2m
we use
涔1叼力-Eμm [f]l≤ Rm(F ) + 6√l°g(≡
to revise the generalization bound (12) as
dF(μ, Vm)- VnG (IF(μ, V) ≤ 2Rm(F) + 6△严I画 + J
Combining the revised bound with Equation (20), we conclude the proof.
(22)
□
15
Published as a conference paper at ICLR 2018
Compared to Corollary 3.3, the bound in (22) gets rid of the number of parameters p, which can
be prohibitively large in practice. Moreover, Corollary A.2 can be directly applied to the spectral
normalized GANs (Anonymous, 2018), and may give an explanation of the empirical success of the
spectral normalization technique.
A.2 Generalization bounds for non-parametric discriminator sets
With the basic result in Theorem 3.1, we can also discuss the learning bounds of GANs with other
choices of non-parametric discriminator sets F . This allows us to highlight the advantages of using
parametric neural discriminators. For simplicity, we assume zero model error and optimization so
that the bound is solely based on the generalization error dF(μ, μm) between μ and its empirical
version μm.
1.	Bounded Lipschitz distance, F = {f ∈ C(X) : ||f ||BL ≤ 1}, which is equivalent to
Wasserstein distance when X is compact. When X is a convex bounded set in Rd, We have
Rm)(F) ≤ m-1/d for d > 2 (see Corollary 12 in Sriperumbudur et al. (2009)), and hence
dBL (μ, ν) = O(m-1/d). This is comparable with Corollary 3.2.
This bound is tight. Assume that μ is the uniform distribution on X . A simple derivation
(similar to Lemma 1 in Arora et al. (2017)) shows that dF(μ, μm) ≥ c(1 - m exp(-Ω(d)))
for some constant only depending on X. Therefore, one must need at least m = exp(Ω(d))
samples to reduce dF(μ, μm), and hence the generalization bound, to O(e).
2.	Total variation (TV) distance, F = {f ∈ C(X) : kfk ≤ min{1, ∆}}. It is easy to verify
that Rm)(F) = 2. Therefore, Eqn. (12) cannot guarantee generalization even when we
have infinite number of samples, i.e., m → ∞.
The estimate given in Eqn. (12) is tight. Assume that μ is the uniform distribution on X .
It is easy to see that dτv(μ, μm) = 2 almost surely. Therefore, Vm is close to μm implies
that it is order 1 away from μ, which means that generalization does not hold in this case.
With the statement that training with the TV distance does not generalize, we mean that training
with TV distance does not generalize in TV distance. More precisely, even if the training loss on
empirical samples is very small, i.e., TV(μm,, Vm) = O(e), the TV distance to the unknown target
distribution can be large, i.e., dTV (μ, νm) = O(1). However, this does not imply that training with
TV distance is useless, because it is possible that training with a stronger metric leads to asymptotic
vanishing in a weaker metric. For example, dτv(μm,, Vm)= O(C) implies dFnn(μm,, Vm) = O(e),
and thus a small dFnn (μ, Vm).
Take the Wasserstein metric as another example, even though we only establish dW (μ, Vm) =
O(m-1/d) (assuming zero model error (μ ∈ G) and optimization C = 0), it does not eliminate the
possibility that the weaker neural distance has a faster convergence rate dFnn (μ, Vm) = O(m-1/2).
From the practical perspective, however, TV and Wasserstein distances are less clearly favorable
than neural distance because the difficulty of calculating and optimizing them.
B NEURAL φ-DIVERGENCE
f -GAN is another broad family of GANs that are based on minimizing f -divergence (also called
φ-divergence) (Nowozin et al., 2016), which includes the original GAN by Goodfellow et al. (2014).
1 However, φ-divergence has substantially different properties from IPM (see e.g., Sriperumbudur
et al. (2009)), and is not defined as the intuitive moment matching form as IPM. In this Appendix,
we extend our analysis to φ-divergence by interpreting it as a form of penalized moment matching.
Similar to the case of IPM, we analyze the neural φ-divergence that restricts the discriminators to
parametric function set F for practical computability, and establish its discrimination and general-
ization properties under mild conditions that practical f -GANs satisfy.
Assume that μ and V are two distributions on X . Given a convex, lower-semicontinuous univariate
function φ that satisfies φ(1) = 0, the related φ-divergence is dφ(μ || V) = EV [φ(务)].If φ is
strictly convex, then a standard derivation based on Jensen’s inequality shows that φ-divergence is
1I	n this appendix, we call it φ-divergence because f has been used for discriminators.
16
Published as a conference paper at ICLR 2018
nonnegative and discriminative: dφ(μ || V) ≥ φ(1) = 0 and the equality holds iff μ = ν. Different
choices of φ recover popular divergences as special cases. For example, φ(t) = (t - 1)2 recovers
Pearson χ2 divergence, and φ(t) = (u + 1) log((u + 1)/2) + ulog u gives the Jensen-Shannon
divergence used in the vanilla GAN Goodfellow et al. (2014).
B.1	DISCRIMINATIVE POWER OF NEURAL φ-DIVERGENCE
In this work, we find it helps to develop intuition by introducing another convex function ψ(t) :=
φ(t + 1), defined by shifting the input variable of φ by +1; the φ-divergence becomes
dφ(μ || V) = EV
ψ (dν- 1)] = ZXPV (x)ψ(M- 1)τ (dx),
(23)
where we should require that ψ(0) = 0; in right hand side of (23), we assume ρμ and PV are the
density functions of μ and V, respectively, under a base measure T. The key advantage of introducing
ψ is that it gives a suggestive variational representation that can be viewed as a regularized moment
matching. Specially, assume ψ* is the convex conjugate of ψ, that is, ψ*(t) = SuPy{yt - ψ(y)}.
By standard derivation, we can show that
dφ(μ || V) ≥ sup(Eμ[f] - EV[f] - Ψν,ψ*[f]), with Ψν,ψ*[f]:= Ex〜V[ψ*(f(x))],	(24)
f∈A
where A is the class of all functions f : X → dom(ψ*) where dom(ψ*) = {t: ψ*(t) ∈ R}, and
the equality holds if 夕"( ρPμ(∣) - 1) ∈ A where 夕 is the inverse function of ψK In (24), the term
ΨV,ψ* [f], as we show in Lemma B.1 in sequel, can be viewed as a type of complexity penalty on f
that ensures the supreme is finite. This is in contrast with the IPM dF(μ, V) in which the complexity
constraint is directly imposed using the function class F , instead of a regularization term.
Lemma B.1. Assume ψ : → → R∪ {∞} is a convex, Iower-SemicontinuousfUnction with conjugate
ψ* and ψ(0) = 0. The penalty Ψv,ψ* [f] in (24) has the following properties
i)	ΨV,ψ* [f] is a convex functional off, and ΨV,ψ* [f] ≥ 0for any f.
ii)	There exists a constant bo ∈ R ∪ {∞} SuCh that ψ*(bo) = 0. Further, if ψ is strictly convex, then
ΨV,ψ* [f] = 0 implies f(x) = b0 almost surely under measure V.
Proof. i) It is obvious that Ψ/ψ* [f ] is convex given that f * is convex. By the convex conjugate, we
have ψ(t) = SuPy {ty - ψ*(y)}. Take t = 0 and note that ψ(0) = 0, then we have ψ*(y) ≥ 0, ∀y.
This proves ΨV,ψ* [f] ≥ 0.
ii)	If ψ is strictly convex, then ψ* is also strictly convex. This implies there exists at most a single
value bo such that ψ*(c) = 0. Given that ψ*(y) ≥ 0 for ∀y, we arrive that Ex〜V[ψ*(f (x))] = 0
implies ψ*(f (x)) = 0 almost surely under X 〜 V, which then implies f (x) = bo almost surely. □
In practice, it is impossible to numerically optimize over the class of all functions in (24). Instead,
practical f -GANs restrict the optimization to a parametric set F of neural networks, yielding the
following neural φ-divergence:
dφ,F(μ || V) = SUP (E”[f] - EVf] - Ψv,Ψ* [f]).
f∈F
(25)
Note that this can be viewed as a generalization of the F-related IPM dF (μ, V) by considering
ψ * = 0. However, the properties of the neural φ-divergence can be significantly different from
that of &f(μ, V). For example, dφF(μ || v) is not even guaranteed to be non-negative for arbitrary
discriminator sets F because of the negative regularization term. Fortunately, we can still establish
the non-negativity and discriminative property of dφF(μ || V) under certain weak conditions on F.
Moreover, the property that &于(μ, V) = 0 implies moment matching on F, which is the key step to
establish the discriminative power, is not necessarily true for neural divergence. Fortunately, it turns
out that dφF(μ || ν) = 0 implies moment matching on features defined by the last linear layer of
discriminators.
Theorem B.1. Assume F includes the constant function bo ∈ R, which satisfies ψ*(bo) = 0 as
defined in Lemma B.1. We have
17
Published as a conference paper at ICLR 2018
i)	0 ≤ dφ,F(μ || V) ≤ d，F(μ, V) for ∀μ, ν. As a result,
dF(μ,ν) = 0 implies	dφ,F(μ || ν) = 0.
In other words, moment matching on F is a sufficient condition of zero neural φ-divergence.
ii)	Further, we assume F has the following form:
F⊇{σ(αfo + co): ∀∣α∣≤ α∕0, and fo eFo},	(26)
where F0 is any function set, and αf0 > 0 is positive number associated with each f0 ∈ F0, and
co is a Constant and σ: R → R is any function that satisfies σ(co) = bo and σ0(co) > 0. Here σ
can be viewed as the output activation function of a deep neural network whose previous layers are
specified by Fo. Assume ψ*(y) is differentiable at y = bo. Then
dφ,F(μ || ν) = 0 implies	"f。(μ , ν) = 0.
In other words, moment matching on Fo is a necessary condition of zero neural φ-divergence.
iii)	Cl(SpanFo) ⊇ Cb(X) is a sufficient Conditionfor dφ,F to be discriminative, i.e., dφ,F(μ || V)=
0 implies μ = V.
Condition (26) defines a commonly used structure of F that naturally satisfied by the f -GANs
used in practice; in particular, the output activation function σ plays the role of ensuring the output
of F respects the input domain of the convex function ψ*. For example, the vanilla GAN has
ψ* = - log(1 - exp(t)) - t with an input domain of (-∞, 0), and activation function is taken to
be σ(t) = - log(1 + exp(-t)). See Table 2 of Nowozin et al. (2016) for the list of output activation
functions related to commonly used ψ .
ProofofTheoremB.1. i) because bo ∈ F and ψ*(bo) = 0, We have dφ,F(μ || V) ≥ Eμ[bo] -
Eν[bo] - Ψν,ψ* [bo] = 0. Because Ψν,ψ* [f] ≥ 0, we obtain dφ,F(μ || V) ≤ "f(μ, V) by comparing
(25) with d，F(μ, V) = SuPf ∈f{Eμ f - EV f}.
ii), note that dψ,F(μ || V) = 0 implies Eμ[f] - EV[f] ≤ Ψν,ψ* [f], ∀f ∈ F. Therefore,
Eμ[σ(αfo + co)] - EV[σ(αfo + co)] ≤ Ψν,ψ* [σ(αfo + co)], ∀fo ∈ Fo, ∣α∣ ≤ αf°.
This implies that
1(Eχ〜μ[σ(afo(x) + co)] - Ex〜ν[σ(afo(x) + co)]) ≤ 1 Ex〜ν[Ψ*(σ(αfo(x) + co))].	(27)
By the differentiability assumptions,
lim
α→o
lim
α→o
σ(αfo(x) + co) - σ(co)	0
-----------α----------=σ (co)f (x),
ψ*(σ(afo(χ) + CO))- ψ*(σ(CO)) = ψ*0(bo)σ0(co)fo(χ) = 0,
α
where we used the fact that ψ*(σ(co)) = ψ*(bo) = 0 and ψ*0(bo) = 0 because bo is a differentiable
minimum point of ψ*. Taking the limit of α → 0 on both sides of (27), we get
σ0(co)[Ex〜μ[fo(x)] - Ex”[fo(x)]] ≤ 0, ∀fo ∈ Fo∙
Because σ0(co) > 0 by assumption, this implies Ex〜μ[fo(x)] - Ex〜V[fo(x)]. The same argument
applies to -fo, and we thus we finally obtain Ex〜μ[fo(χ)] = Ex〜V[fo(χ)].
iii) Combining Theorem 2.2 and the last point, we directly get the result.	□
Remark B.1. Our results on neural φ-divergence can in general extended to the more unified frame-
work of Liu et al. (2017) in which divergences of form maxf E(x,y)〜μ & ”[f (x, y)] are studied. We
choose to focus on φ-divergence because of its practical importance. Our Theorem B.1 i) can be
viewed as a special case of Theorem 4 of Liu et al. (2017) and our Theorem B.1 ii) is related to
Theorem 5 of Liu et al. (2017). However, Theorem 5 of Liu et al. (2017) requires a rather counter-
intuitive condition, while our condition in Theorem B.1 ii) is clear and satisfied by all φ-divergence
listed in Nowozin et al. (2016).
18
Published as a conference paper at ICLR 2018
Similar to Theorem 2.5, under the conditions of Theorem B.1, we have similar results for neural
φ-divergence.
Theorem B.2. Using the same notions in Theorem B.1, assume that (X, dX) is a compact metric
space and that Cl(SpanF0) ⊇ Cb(X). Then if limn→∞ dφ,F(μ || Vn) = 0, Vn converges to μ in the
distribution sense.
Further, if there exists C > 0 such that F ⊂ {f ∈ C(X) : kf kLip ≤ C}, we have
lim dφ,F(μ || Vn) = 0 < ⇒ Vn -* μ.
n→∞
Notice that we assume that (X, dX) be a compact metric space here for simplicity. A non-compact
result is available but its proof is messy and non-intuitive.
Proof. The first half is a direct application of Theorem B.1 and Theorem 10 in Liu et al. (2017).
For the second half, we have
dφ,F(μ || Vn) ≤ dF(μ,Vn) ≤ CdW(μ,Vn),
where we use Theorem B.1 i) in the first inequality and the Lipschitz condition of F in the second
ineqaulity. Since dw metrizes the weak convergence for compact X, We obtain dw(μ, Vn) → 0 and
thus dφ,F(μ || Vn) → 0.	□
B.2 GENERALIZATION PROPERTIES OF NEURAL φ-DIVERGENCE
Similar to the case of neural distance, we can establish generalization bounds for neural φ-
divergence.
Theorem B.3. Assume that kf k∞ ≤ ∆ for any f ∈ F. μm is an empirical distribution with m
samples from μ, and Vm ∈ G satisfies dφ,F (μm || Vm) ≤ inf ν∈G dφ,F (μm || V) + e. Then with
probability at least 1 - 2δ, we have
dφ,F (μ ||
Vm ) ≤ inf dφ,F (μ
ν∈G
Il V)+ 2Rmt)(F) + 2∆J2log(1∕δ) + e
m
(28)
where Rmμ(F) is the Rademacher complexity of F.
Notice that the only difference between Theorem 3.1 and Theorem B.3 is that the failure probability
change from δ to 2δ. This comes from the fact that F is typically not even in the neural divergence
case. For example, the vanilla GAN takes σ(t) = - log(1 + exp(-t)) as the output activation
function, and thus f ≤ 0 for all f ∈ F.
Proof of Theorem B.3. With the same argument in Equation (10), we obtain
dφF(〃||Vm)-VnG dφ,F (〃11V) ≤2 s∈FlE"[f]-Eμm[f]l+J
Although F is not even, we have
(
泮 lEμ[f]- Eμm [f]l
max
泮(Eμ[f]- Eμm [f]),泮(E^m [f]— E“[f ])
)
Standard argument on Rademacher complexity (same in the proof of Theorem 3.1) gives with
probalibity at least 1 - δ,
八og(1∕δ)
V 2m
E sup(Eμ[f] — E^m[f]) ≤ Rm(F)+2∆
f∈F
With the same argument, we obtain that with probalibity at least 1 - δ,
E sup (Eμm [f] - Eμ[f]) ≤ Rm(F)+2∆JlTδ).
f∈F	2m
Combining all the results above, we conclude the proof.	□
19
Published as a conference paper at ICLR 2018
With Theorem B.3, we obtain generalization bounds for difference choices of F , as we had in
section 3. For example, we have an analog of Corollary 3.3 in the neural divergence setting as
follows.
Corollary B.4. Under the condition of Theorem B.3, we further assume that (1) F = Fnn = {fθ :
θ ∈ Θ ⊂ [-1, 1]p} is a parametric function class with p parameters in a bounded set Θ and that (2)
every fθ is L-Lipschitz continuous with respect to the parameters θ. Then with probability at least
1 - 2δ, we have
C
dφ,Fnn (μ 11 Vm) ≤ VinG dΦ,Fnn (仙 11 V) + √m + €,	(29)
where C = 16√2πpL + 2∆,2 log(1∕δ).
C Proof of results in section 2
Proof of Theorem 2.2. For the sufficient part, the proof is standard and the same as that of the
uniqueness of weak convergence. We refer to Lemma 9.3.2 in Dudley (2002) for a complete proof.
For the necessary part, suppose that F ⊂ Cb(X) is discriminative in PB(X). Assume that
cl(span(F ∪ {1})) is a strictly closed subspace of Cb(X). Take g ∈ Cb(X)\cl(span(F)) and
∣∣gk∞ = 1. By the Hahn-Banach theorem, there exists a bounded linear functional L : C(X) → R
such that L(f) = 0 for any f ∈ cl(span(F ∪ {1})) and L 6= 0. Thanks to the Riesz representation
theorem for compact metric spaces, there exists a signed, regular Borel measure m ∈ MB(X) such
that
L(f) =
m
f ∀f ∈ Cb(X).
Suppose m = μ 一 V are the Hahn decomposition of m, where μ and V are two nonnegative Borel
measures. Then we have L(f) = fμ f 一 JV f for any f ∈ Cb(X). Thanks to L(1) = 0, we have
0 < μ(X) = v(X) < ∞. We can assume that μ and V are Borel probability measures. (Otherwise,
we can use the normalized nonzero linear functional L∕μ(X) whose Hahn decomposition consists
of two Borel probability measures.) Since L(f) = 0 for any f ∈ cl(span(F)), we have fμ f = JV f
for any f ∈ F. Since F ⊂ Cb(X) is discriminative, we have μ = V and thus L = 0, which leads to
a contradiction.	□
ProofofCorollary 2.4. Thanks to {λθ : λ ≥ 0,θ ∈ Θ} = Rd+1, for any [w, b] ∈ Rn+1, there
exists [w0, b0] ∈ Θ and λ > 0 such that
σ(w>x + b) = σ(λ(w0>x + b0)) = λασ(w0>x + b0),
where we used σ(u) = max{u, 0}α in the last step. Therefore, we have
SpanFnn ⊂ span{σ(w~>x + b): [w, b] ∈ Rd+1}.
Thanks to Theorem 2.3, we know that SpanFnn is dense in Cb(X).	□
Proof of Theorem 2.5. Given a function g ∈ Cb(X), we say that g is approximated by F with
error decay function (r) if for any r ≥ 0, there exists fr ∈ SpanF with ||fr||F,1 ≤ r such that
||f 一 fr ∣∣∞ ≤ e(r). Obviously, e(r) is an non-increasing function w.r.t. r. Thanks to CI(SpanF)=
Cb(X), we have limr→∞ e(r) = 0. Now denote rn := dʃ(μ, Vn)T/2 and correspondingly fn :=
frn. We have
| Eμ g 一 EVn g| ≤ | Eμ g 一 Eμ fn | + | EV g - EV fn | + | Eμ fn - E匕 fn |
≤ 2e(Irn) + rn dF(μ, Vn). = 2e(Irn) + 1∕rn∙
If limn→∞ dF-(μ, Vn) = 0, we have limn→∞ rn = ∞. Thanks to limr→∞ e(r) = 0, we prove that
limn→∞ | Eμ g 一 EVn g| = 0. Since this holds true for any g ∈ Cb(X), we conclude that Vn weakly
converges to μ.
If F ⊆ BLC(X) for some C > 0, we have dF-(μ, v) ≤ CdBL(μ, v) for any μ, V. Because the
bounded Lipschitz distance (also called FortetMourier distance) metrizes the weak convergence, we
obtain that vn * μ implies dBL(μ, vn) → 0, and thus dF-(μ, vn) → 0.	□
20
Published as a conference paper at ICLR 2018
Proof of Proposition 2.6. Let g =	in=1 wifi + w0 . Then we have
dF(μ, V).
The result is obtain by taking infimum over all possible wi .
□
Proof of Proposition 2.7. For any r ≥ 0, we have
| Eμ g - EV g| ≤ | Eμ g - Eμ fr | + | EV g - EV fr | + | Eμ fr - EV fr | ≤ 2E(T) + r dF(μ, V).
Taking the infimum on r > 0 on the right side gives the result.	□
Proof of Corollary 2.8. Proposition 5 of Bach (2017) shows that for any bounded Lips-
Chitz function g that satisfies ∣∣g∣∣BL:	= max{∣∣g∣∣∞, ∣∣g∣∣Lip} ≤ η, we have e(r) =
O(n(r/n)T/(a+(dT)/2) log(r∕η)). Using Proposition 2.7, We get
| Eμ g - EV g| ≤ O(l∣g∣∣BL dF(μ, v)α+(d+1)/2),
The result follows BL(μ, V) = SuPg{∣ Eμ g - EV g|: ∣∣g∣∣BL ≤ 1}.	□
D Proof of results in section 3
Proof of Equation (10) Using the standard derivation and the optimality condition (9), we have
dF(μ,Vm) - inf dF(μ, V)
V∈G
=dF (μ, Vm ) - dF (μm, Vm) + dF (μm, Vm) - inf dF (μ, V)
V∈G
≤ dF (μ, Vm) - dF (μm, Vm) + inf dF (μm, V) - inf dF (μ, V) + e∙
V∈G	V∈G
Therefore, we obtain
dF (μ, Vm) - inf dF(μ, V) ≤ 2 SuP |dF(μ, v) - dF(μm, v) | + C.
V∈G	V∈G
Combining with the definition (1), we obtain
dF(μ,Vm) - inf dF(μ,ν) ≤ 2sup |E*[f] - Eμw, [f]] + c.
V∈G	f∈F
ProofofTheorem 3.1. First of all, since F is even, we have SuPf∈f ∣Eμ[f] - Eμm[f]| =
SuPf ∈f (Eμ[f] - Eμm [f]). Consider the function
h(Xl, X2, . . . , Xm) = SuP (Eμ[f] - Eμm [f]) ∙
f∈F
Since f takes values in [-∆, ∆], changing Xi to another independent copy Xi0 can change h by no
more than 2∆∕m. McDiarmid,s inequality implies that with probability at least 1 - δ,
sup(Eμ[f] - Eμm [f]) ≤ E sup (Eμ[f] - Eμm [f]) +2∆/l°g^.
f∈F	f∈F	2m
Standard argument on Rademacher complexity gives
E SuF(Eμf]- Eμm[f])
≤ 2 Eτ ,X Sup ɪ X Tif (Xi) := Rm(F).
f∈Fm i
Combining the two estimates above and Eqn. (10), we conclude the proof.
□
21
Published as a conference paper at ICLR 2018
Proof of Corollary 3.2. Part of the proof is from Proposition 7 in Bach (2017). More accurately, the
discriminator set we use here is
(Xnn
wi max(vi> [x; 1], 0) : X |wi| ≤ 1,	kvik2 = 1∀1 ≤ i ≤ n
i=1
for a fix n ∈ N. Since ∣∣χk2 ≤ 1 and ∣∣vk2 ≤ 1, it is easy to see that kf k∞ ≤ √2 for all f ∈ F.
We want to estimate R((μ) (F) and then use Theorem 3.1 to prove the result. First, it's easy to verify
that
sup
f∈F
N Tif(Xi)
i
sup
kvk2=1
2
一 TTi max(v>[Xi; 1], 0)
m
i
Then we have
RMF) = E
sup
kvk2=1
≤E
sup
kvk2=1
2
—ETiv>[Xi；1]
m
i
mE XTiXi； 1]∣ ；
where we use the 1-Lipschitz property of max(x, 0) and Talagrand’s contraction lemma (Ledoux
& Talagrand, 2013) in the inequality step. From Kakade et al. (2009), we get the Rademacher
complexity of linear functions
E ETiXi; 1]∣ J ≤ √2m.
Therefore, We obtain
Rm)(F) ≤ √√2.
mm
Combined with ∣∣f ∣∞ ≤ √2 and Theorem 3.1, we finish the proof.
Proof of Corollary 3.3. We need to derive a bound for the Rademacher complexity (11). For fixed
{χi}m=ι, let,s consider Xθ = ɪ√m Pm=I Ti fθ(xi). First of all, {Xθ : θ ∈ Θ} is a SUb-GaUSSian
process with respect to the Eulidean distance on Θ, i.e., for all θ, θ0 ∈ Θ and all λ > 0
E [ep (λ(Xθ - Xθ0))] ≤ exp
λ2∣θ - θ0∣22
This is a standard result and can be derived by the Hoeffding’s lemma. Secondly, we have the
following bound for the -cover number of Θ:
log N(J Θ,k∙k2) ≤ P log(√p∕e).	(30)
This bound is from the following simple construction. Consider a uniform grid with grid size 2j/√p.
The balls with centers as the grid points and with radius E cover the unit cubic on Rp, i.e., [-1,1]p.
The number of balls in this construction is (√p∕e)p. Finally, by applying Dudley,s entropy integral,
we have
E sup Xθ
θ∈Θ
Combing (30) and (31) and taking T = log(√p∕e), we obtain
E sup Xθ
θ∈Θ
Zp
≤ 8√2	/log N(e, Θ,∣H∣2)de
0
(31)
≤ 8√2p / T1/2e-τdT = 4√2πp.
(32)
Notice that Eqn. (32) holds true for arbitrary samples {xi}im=1, and thus we conclude that
Rμ(F) ≤ c∕√m,
where C = 8√2∏pL.
□
2
□
22
Published as a conference paper at ICLR 2018
Proof of Corollary 3.4. Lemma 22 in Bartlett & Mendelson (2003) shows that if supx∈X k(x, x) ≤
Ck ≤ +∞, We have R(μ)(F) ≤ 2pCk/m for any μ ∈ PB(X). Also, note that f(χ) ≤
||f ||H∕k(χ,χ) ≤ ||f ||H√Ck. Combined with Theorem 3.1, we conclude the proof.	□
ProofofCorollary 3.5. Use Proposition 3.1 and note that KL(μ, νm) ≤ Λf,g "f(μ,ν) and
dF(μ, V) ≤ ∆TV(μ, V) ≤ ∆,2KL(μ, V) by Pinsker's inequality.	□
E Inconsistency between GAN’s loss and testing likelihood
In this section, we will test our analysis of the consistency of GAN objective and likelihood objective
on two toy datasets, e.g., a 2D Gaussian dataset and a 2D 8-Gaussian mixture dataset.
E.1 A 2D Gaussian example
The underlying ground-truth distribution is a 2D Gaussian with mean (0.5, -0.5) and covariance
matrix 忐 17 15 . We take 105 samples for training, and 1000 samples for testing.
For a 2D Gaussian distribution, we use the following generator
x1	1
x2 = l
z1	b1
es2	z2 + b2
(33)
where Z = j； is a standard 2D normal random vector, and l ∈ R, S = ；；
b1 ∈ R2 are trainable parameters in the generator.
∈ R2 and b
We train the generative model by WGAN with weight clipping. In the first experiment, the discrim-
inator set is a neural network with one hidden layer and 500 hidden neurons, i.e.,
500
Fnn = {X αi max(wi> [x; 1], 0) : -0.05 ≤ α ≤ 0.05, -0.05 ≤ wi ≤ 0.05	∀i}.
i=1
Motivated by Corollary 3.5, in the second experiment, we take the discriminators to be the log-
density ratio between two Gaussian distributions, which are quadratic polynomials:
Fquad = {x>Ax + b>x : -0.05 ≤ A ≤ 0.05, -0.05 ≤ b ≤ 0.05}.
We plot their results in Figure 1. We can see that both discriminators behave well: The training
loss (the neural distance) converge to zero, and the testing log likelihood increases monotonically
during the training. However, the quadratic polynomial discriminators Fquad yields higher testing log
likelihood and better generative model at the convergence. This is expected because Corollary 3.5
guarantees that the testing log likelihood is bounded by the GAN loss (up to a constant), while it is
not true for Fnn.
We can also maximize the likelihood (MLE) on the training dataset to train the model, and we show
its result in Figure 3. We can see that both MLE and Q-GAN (refers to WGAN with the quadratic
discriminator Fquad) yield similar results. However, directly maximizing the likelihood converges
much faster than the WGAN in this example.
In this simple Gaussian example, the WGAN loss and the testing log likelihood are consistent. We
indeed observe that by carefully choosing the discriminator set (as suggested in Corollary 3.5), the
testing log likelihood can be simultaneously optimized as we optimize the GAN objective.
E.2 An example of 2D 8-Gaussian mixture
The underlying ground truth distribution is a 2D Gaussian mixture with 8 Gaussians and with equal
weights. Their centers are distributed equally on the circle centered at the origin and with radius
23
Published as a conference paper at ICLR 2018
Figure 2: Samples from trained generators. Left: WGAN with quadratic polynomials as discrimi-
nator. Right: WGAN with a neural discriminator with 500 neurons.
Figure 1: Negative GAN losses and testing likelihood. qgan: WGAN with quadratic polynomials as
discriminator. wgan: WGAN with neural discriminator.
√2, and their standard deviations are all 0.01414. We take 105 samples as training dataset, and 1000
samples as testing dataset. We show one batch (256) of training dataset and the testing dataset in
Figure 4. Note that that the density of the ground-truth distribution is highly singular.
We still use Eqn. (33) as the generator for a single Gaussian component. Our generator assume
that there are 8 Gaussian components and they have equal weights, and thus our generator does not
have any modeling error. The training parameters are eight sets of scaling and biasing parameters in
Eqn. (33), each for one Gaussian component.
We first train the model by WGAN with clipping. We use an MLP with 4 hidden layers and relu
activations as the discriminator set. We show the result in Figure 5. We can see that the generator’s
samples are nearly indistinguishable from the real samples. However, the GAN loss and the log
likelihood are not consistent. In the initial stage of training, both the negative GAN loss and log
likelihood are increasing. As the training goes on, the generator’s density gets more and more
singular, the log likelihood behaves erratically in the latter stage of training. Although the negative
GAN loss is still increasing, the log likelihood oscillates a lot, and in fact over half of time the log
likelihood is -∞. We show the generated samples at intermediate steps in Figure 6, and we indeed
see that the likelihood starts to oscillate violently when the generator’s distribution gets singular.
This inconsistency between GAN loss and likelihood is observed by other works as well. The reason
for this consistency is that the neural discriminators are not a good approximation of the singular
density ratios.
We also train the model by maximizing likelihood on the training dataset. We show the result in
Figure 7. We can see that the maximal likelihood training got stuck in a local minimum, and failed
to exactly recover all 8 components. The log likelihood on training and testing datasets are consistent
as expected. Although the log likelihood (≈ 2.7) obtained by maximizing likelihood is higher than
24
Published as a conference paper at ICLR 2018
Figure 3: Left: samples from the maximal likelihood estimate. Right: log likelihood on training and
testing datasets, trained with SGD.
Figure 4: Samples from training and testing datasets.
that (≈ 2.0) obtained by WGAN training, its generator is obviously worse than what we obtained in
WGAN training. The reason for this is that the negative log-likelihood loss has many local minima,
and maximizing likelihood is easy to get trapped in a local minimum.
The FlowGAN (Grover et al., 2017) proposed to combine the WGAN loss and the log likelihood
to solve the inconsistency problem. We showed the FlowGAN result on this dataset in Figure 8.
We can see that training by FlowGAN indeed makes the training loss and log likelihood consis-
tent. However, FlowGAN got stuck in a local minimum as maximizing likelihood did, which is not
desirable.
25
Published as a conference paper at ICLR 2018
PoO≡⅛X= 8
Figure 5: Left: samples from training dataset (yellow) and samples from generator (green). Right:
negative GAN loss and log likelihood (evaluated on the testing dataset).
Figure 6: Left to right: generated samples

3
at step 100, 200 and 300, respectively.
Pcoln= g
::
0.5
0.0
-0.5
-1.0
-1.5
Figure 7: Left: samples from training dataset and samples from generator. Right: log likelihood on
training and testing dataset.
Figure 8: Left: samples from training dataset and samples from generator. Right: negative Flow-
GAN loss and log likelihood on testing dataset.
26