Published as a conference paper at ICLR 2018
Empirical Risk Landscape Analysis for Under-
standing Deep Neural Networks
Pan Zhou & Jiashi Feng
Department of Electrical and Computer Engineering
National University of Singapore
Singapore, 117583
{pzhou@u.nus.edu, elefjia@nus.edu.sg}
Ab stract
This work aims to provide comprehensive landscape analysis of empirical risk
in deep neural networks (DNNs), including the convergence behavior of its gra-
dient, its stationary points and the empirical risk itself to their corresponding
population counterparts, which reveals how various network parameters deter-
mine the convergence performance. In particular, for an l-layer linear neural
network consisting of di neurons in the i-th layer, we prove the gradient of its
empirical risk uniformly converges to the one of its population risk, at the rate of
O(r2l ∖∕l√maxidis log(d/l)/n). Here d is the total weight dimension, S is the
number of nonzero entries of all the weights and the magnitude of weights per
layer is upper bounded by r. Moreover, we prove the one-to-one correspondence
of the non-degenerate stationary points between the empirical and population risks
and provide convergence guarantee for each pair. We also establish the uniform
convergence of the empirical risk to its population counterpart and further derive
the stability and generalization bounds for the empirical risk. In addition, we ana-
lyze these properties for deep nonlinear neural networks with sigmoid activation
functions. We prove similar results for convergence behavior of their empirical risk
gradients, non-degenerate stationary points as well as the empirical risk itself.
To our best knowledge, this work is the first one theoretically characterizing the
uniform convergence of the gradient and stationary points of the empirical risk
of DNN models, which benefits the theoretical understanding on how the neural
network depth l, the layer width di, the network size d, the sparsity in weight and
the parameter magnitude r determine the neural network landscape.
1	Introduction
Deep learning has achieved remarkable success in many fields, such as computer vision (Hinton
et al., 2006; Szegedy et al., 2015; He et al., 2016), natural language processing (Collobert & Weston,
2008; Bakshi & Stephanopoulos, 1993), and speech recognition (Hinton et al., 2012; Graves et al.,
2013). However, theoretical understanding on the properties of deep learning models still lags
behind their practical achievements (Shalev-Shwartz et al., 2017; Kawaguchi, 2016) due to their
high non-convexity and internal complexity. In practice, parameters of deep learning models are
learned by minimizing the empirical risk via (stochastic-)gradient descent. Therefore, some recent
works (Bartlett & Maass, 2003; Neyshabur et al., 2015) analyzed the convergence of the empirical
risk to the population risk, which are however still far from fully understanding the landscape of the
empirical risk in deep learning models. Beyond the convergence properties of the empirical risk itself,
the convergence and distribution properties of its gradient and stationary points are also essential
in landscape analysis. A comprehensive landscape analysis can reveal important information on
the optimization behavior and practical performance of deep neural networks, and will be helpful
to designing better network architectures. Thus, in this work we aim to provide comprehensive
landscape analysis by looking into the gradients and stationary points of the empirical risk.
Formally, we consider a DNN model f (w; x, y) : Rd0 × Rdl → R parameterized by w ∈ Rd
consisting of l layers (l ≥ 2) that is trained by minimizing the commonly used squared loss function
1
Published as a conference paper at ICLR 2018
over sample pairs {(x, y)} ⊂ Rd0 × Rdl from an unknown distribution D, where y is the target
output for the sample x. Ideally, the model can find its optimal parameter w* by minimizing the
population risk through (stochastic-)gradient descent by backpropagation:
mwn J(W)，E(χ,y)〜D f(w; x, y),
where f (w; x, y) = 2 ||v(l) - yk2 is the squared loss associated to the sample (x, y)〜D in which
v(l) is the output of the l-th layer. In practice, as the sample distribution D is usually unknown and
only finite training samples (x(i), y(i)) in=1 i.i.d. drawn from D are provided, the network model is
usually trained by minimizing the empirical risk:
1n
mwn Jn(W)，n[f(w； x(i)，y(i))	⑴
Understanding the convergence behavior of Jn(w) to J(w) is critical to statistical machine learning
algorithms. In this work, we aim to go further and characterize the landscape of the empirical risk
Jn(w) of deep learning models by analyzing the convergence behavior of its gradient and stationary
points to their corresponding population counterparts. We provide analysis for both multi-layer linear
and nonlinear neural networks. In particular, we obtain following new results.
•	We establish the uniform convergence of empirical gradient Nw Jn(W) to its popu-
lation counterpart NW J(w). Specifically, when the sample size n is not less than
O (max(l3r2∕(ε2s log(d/l)), S log(d/l)/l)), with probability at least 1 - ε the conver-
gence rate is O(r2l√l√maxTdiSlog(d/l)/n), where there are S nonzero entries in the
parameter W, the output dimension of the i-th layer is di and the magnitude of the weight
parameter of each layer is upper bounded by r. This result implies that as long as the training
sample size n is sufficiently large, any stationary point of Jn(W) is also a stationary point
of J(W) and vise versa, although both Jn(W) and J(W) are very complex.
•	We then prove the exact correspondence of non-degenerate stationary points between Jn (W)
and J (W). Indeed, the corresponding non-degenerate stationary points also uniformly
converge to each other at the same convergence rate as the one revealed above with an extra
factor 2/Z. Here Z > 0 accounts for the geometric topology of non-degenerate stationary
points (see Definition 1).
Based on the above two new results, we also derive the uniform convergence of the empirical risk
Jn(W) to its population risk J(W), which helps understand the generalization error of deep learning
models and stability of their empirical risk. These analyses reveal the role of the depth l of a neural
network model in determining its convergence behavior and performance. Also, the results tell that the
width factor √maxi di, the nonzero entry number S of weights, and the total network size d are also
critical to the convergence and performance. In addition, controlling magnitudes of the parameters
(weights) in DNNs are demonstrated to be important for performance. To our best knowledge, this
work is the first one theoretically characterizing the uniform convergence of empirical gradient and
stationary points in both deep linear and nonlinear neural networks.
2	Related Work
To date, only a few theories have been developed for understanding DNNs which can be roughly
divided into following three categories. The first category aims to analyze training error of DNNs.
Baum (1988) pointed out that zero training error can be obtained when the last layer of a neural
network has more units than training samples. Later, Soudry & Carmon (2016) proved that for DNNs
with leaky rectified linear units (ReLU) and a single output, the training error achieves zero at any of
their local minima as long as the product of the number of units in the last two layers is larger than
the training sample size.
The second category of analysis works (Dauphin et al., 2014; Choromanska et al., 2015a; Kawaguchi,
2016; Tian, 2017) focus on analyzing loss surfaces of DNNs, e.g., how the stationary points are
distributed. Those results are helpful to understanding performance difference of large- and small-size
2
Published as a conference paper at ICLR 2018
networks (Choromanska et al., 2015b). Among them, Dauphin et al. (2014) experimentally verified
that a large number of saddle points indeed exist for DNNs. With strong assumptions, Choromanska
et al. (2015a) connected the loss function of a deep ReLU network with the spherical spin-class
model and described locations of the local minima. Later, Kawaguchi (2016) proved the existence
of degenerate saddle points for deep linear neural networks with squared loss function. They also
showed that any local minimum is also a global minimum. By utilizing techniques from dynamical
system analysis, Tian (2017) gave guarantees that for two-layer bias-free networks with ReLUs,
the gradient descent algorithm with certain symmetric weight initialization can converge to the
ground-truth weights globally, if the inputs follow Gaussian distribution. Recently, Nguyen & Hein
(2017) proved that for a fully connected network with squared loss and analytic activation functions,
almost all the local minima are globally optimal if one hidden layer has more units than training
samples and the network structure after this layer is pyramidal. Besides, some recent works, e.g.,
(Zhang et al., 2016; 2017), tried to alleviate analysis difficulties by relaxing the involved highly
nonconvex functions into ones easier.
In addition, some existing works (Bartlett & Maass, 2003; Neyshabur et al., 2015) analyze the
generalization performance of a DNN model. Based on the VaPnik-Chervonenkis (VC) theory,
Bartlett & Maass (2003) proved that for a feedforward neural network with one-dimensional output,
the best convergence rate of the empirical risk to its population risk on the sample distribution can
be bounded by its fat-shattering dimension. Recently, Neyshabur et al. (2015) adopted Rademacher
complexity to analyze learning capacity of a fully-connected neural network model with ReLU
activation functions and bounded inputs.
However, although gradient descent with backpropagation is the most common optimization technique
for DNNs, none of existing works analyzes convergence properties of gradient and stationary points
of the DNN empirical risk. For single-layer optimization problems, some previous works analyze
their empirical risk but essentially differ from our analysis method. For example, Negahban et al.
(2009) proved that for a regularized convex program, the minimum of the empirical risk uniformly
converges to the true minimum of the population risk under certain conditions. Gonen & Shalev-
Shwartz (2017) proved that for nonconvex problems without degenerated saddle points, the difference
between empirical risk and population risk can be bounded. Unfortunately, the loss of DNNs is
highly nonconvex and has degenerated saddle points (Fyodorov & Williams, 2007; Dauphin et al.,
2014; Kawaguchi, 2016), thus their analysis results are not applicable. Mei et al. (2017) analyzed
the convergence behavior of the empirical risk for nonconvex problems, but they only considered
the single-layer nonconvex problems and their analysis demands strong sub-Gaussian and sub-
exponential assumptions on the gradient and Hessian of the empirical risk respectively. Their analysis
also assumes a linearity property on gradient which is difficult to hold or verify. In contrast, our
analysis requires much milder assumptions. Besides, we prove that for deep networks which are
highly nonconvex, the non-degenerate stationary points of empirical risk can uniformly converge
to their corresponding stationary points of population risk at the rate of O( /s/n) which is faster
than the rate O(，d/n) for single-layer optimization problems in (Mei et al., 2017). Also, Mei et al.
(2017) did not analyze the convergence rate of the empirical risk, stability or generalization error of
DNNs as this work.
3	Preliminaries
Throughout the paper, we denote matrices by boldface capital letters, e.g. A. Vectors are denoted by
boldface lowercase letters, e.g. a, and scalars are denoted by lowercase letters, e.g. a. We define the
r-radius ball as Bd(r) , {z ∈ Rd | kzk2 ≤ r}. To explain the results, we also need the vectorization
operation νec(∙). It is defined as Vec(A) = (A(:, 1);…；A(:, t)) ∈ Rst that vectorizes A ∈ Rs×t
along its columns. We use d= Plj=1dj dj-1 to denote the total dimension of weight parameters,
where dj denotes the output dimension of the j -th layer.
In this work, we consider both linear and nonlinear DNNs. Suppose both networks consist of l layers.
We use u(j) and v(j) to respectively denote the input and output of the j-th layer, ∀j = 1, . . . , l.
Deep linear neural networks: The function of the j -th layer is formulated as
UCj) , wCj)Vcj-I) ∈ Rdj ,	Vcj)，UCj) ∈ Rdj, Vj = 1,…，l,
3
Published as a conference paper at ICLR 2018
where v(0) = x is the input and W(j) ∈ Rdj ×dj-1 is the weight matrix of the j-th layer.
Deep nonlinear neural networks: We adopt the sigmoid function as the non-linear activation
function. The function within the j -th layer can be written as
UCj) , wCj)Vej-I)	∈	Rdj	,	Vej)，hj(UCj))	=	(σ(uj));…；σ(u^)) ∈ Rdj,	∀j	= 1,…，l,
where Uej) denotes the i-th entry of Uj) and σ(∙) is the sigmoid function, i.e., σ(a) = 1/(1 + e-a).
Following the common practice, both DNN models adopt the squared loss function defined as
f (w; x, y) = 2∣∣v(l) - yk∣, where W = (w(i)；… ;w(i)) ∈ Rd contains all the weight pa-
rameters and w(j) = Vec (W(j)) ∈ Rdjdj-1. Then the empirical risk Jn(W) is Jn(W)=
n Pn=ιf(w； x(i), y(i)) = ∣n Pn=ι l∣v(i) - y(i)k∣, wherev(i) is the network’s output of xCi).
4	Results for Deep Linear Neural Networks
We first analyze linear neural network models and present following new results: (1) the uniform
convergence of the empirical risk gradient to its population counterpart and (2) the convergence
properties of non-degenerate stationary points of the empirical risk. As a corollary, we also derive
the uniform convergence of the empirical risk to the population one, which further gives stability
and generalization bounds. In the next section, we extend the analysis to non-linear neural network
models.
We assume the input datum x is τ∣-sub-Gaussian and has bounded magnitude, as formally stated in
Assumption 1.
Assumption 1. The input datum x ∈ Rd0 has zero mean and is τ ∣ -sub-Gaussian, i.e.,
E[exp(hλ, xi)] ≤ exp Qτ∣∣λ∣∣^ , ∀λ ∈ Rd0.
Besides, the magnitude x is bounded as ∣x∣∣ ≤ rx, where rx is a positive universal constant.
Note that any random vector z consisting of independent entries with bounded magnitude is sub-
Gaussian and satisfies Assumption 1 (Vershynin, 2012). Moreover, for such a random z, we have
τ=∣z∣∞ ≤ ∣z∣∣ ≤rx.Such an assumption on bounded magnitude generally holds for natural
data, e.g., images and speech signals. Besides, we assume the weight parameters W(j) of each layer
are bounded as W ∈ Ω = {w | w(j) ∈ Bdjdj-1 (rj), ∀j = 1, ∙∙∙ ,l} where rj is a constant. For
notational simplicity, we let r = maxj rj. Such an assumption is common (Xu & Mannor, 2012).
Here we assume the entry value of y falls in [0, 1]. For any bounded target output y, we can always
scale it to satisfy such a requirement.
The results presented for linear neural networks here can be generalized to deep ReLU neural networks
by applying the results from Choromanska et al. (2015a) and Kawaguchi (2016), which transform
deep ReLU neural networks into deep linear neural networks under proper assumptions.
4.1	Uniform Convergence of Empirical Risk Gradient
We first analyze the convergence of gradients for the DNN empirical and population risks. To our
best knowledge, these results are the first ones giving guarantees on gradient convergence, which help
better understand the landscape of DNNs and their optimization behavior. The results are stated blow.
Theorem 1. Suppose Assumption 1 on the input datum x holds and the activation functions in a
deep neural network are linear. Then the empirical gradient uniformly converges to the population
gradient in Euclidean norm. Specifically, there exist two universal constants cg0 and cg such that
if n ≥ CgO max(l3r∣r4/(cqS log(d∕l)ε∣τ4 log(1∕ε)), s log(d∕l)∕(lτ∣)) where Cq = ,maxo≤i≤ι di,
then	_____________________
SUp IlVJn(W)- VJ(w)∣∣ ≤ eg , CgτωgPlCq Jslog(dn∕l) + log(12∕ε)
w∈Ω	∣	n
holds with probability at least 1 - ε, where s denotes the number of nonzero entries of all weight
parameters and ωg = max τr∣l-1, r∣l-1, rl-1 .
4
Published as a conference paper at ICLR 2018
From Theorem 1, one can observe that with an increasingly larger sample size n, the difference
between empirical risk and population risk gradients decreases monotonically at the rate of O(1∕√n)
(up to a log factor). Theorem 1 also characterizes how the depth l contributes to obtaining small
difference between the empirical and population risk gradients. Specifically, a deeper neural network
needs more training samples to mitigate the difference. Also, due to the factor d, training a network
of larger size using gradient descent also requires more training samples. We observe a factor of
√maXi di (i.e. Cq), which prefers a DNN architecture ofbalanced layer sizes (without extremely wide
layers). This result also matches the trend and empirical performance in deep learning applications
advocating deep but thin networks (He et al., 2016; Szegedy et al., 2015).
By observing Theorem 1, imposing certain regularizations on the weight parameters is useful. For
example, reducing the number of nonzero entries s encourages sparsity regularization like kwk1.
The results also suggest not choosing large-magnitude weights w in order for a smaller factor r by
adopting regularization like kw k22 .
Theorem 1 also reveals the point derived from optimizing that the empirical and population risks have
similar properties when the sample size n is sufficiently large. For example, an e/2-stationary point W
of Jn(W) is also an e-stationary point of J(w) with probability 1 - ε if n ≥ ce(τωg/e)2lcqS log(d/l)
with c being a constant. Here e-stationary point for a function F means the point w satisfying
kVw FIl2 ≤ e. Understanding such properties is useful, since in practice one usually computes an
e-stationary point of Jn(W). These results guarantee the computed point is at most a 2e-stationary
point of J(W) and is thus close to the optimum.
4.2	Uniform Convergence of Stationary Points
We then proceed to analyze the distribution and convergence properties of stationary points of the
DNN empirical risk. Here we consider non-degenerate stationary points which are geometrically
isolated and thus unique in local regions. Since degenerate stationary points are not unique in a local
region, we cannot expect to establish one-to-one corresponding relationship (see below) between
them in empirical risk and population risk.
Definition 1. (Non-degenerate stationary points) (Gromoll & Meyer, 1969) If a stationary point W
is said to be a non-degenerate stationary point of J(W), then it satisfies
innf∣%(V2J(w))∣ ≥ Z,
where λi V2J (W) denotes the i-th eigenvalue of the Hessian V2J (W) and ζ is a positive constant.
Non-degenerate stationary points include local minima/maxima and non-degenerate saddle points,
while degenerate stationary points refer to degenerate saddle points. Then we introduce the index of
non-degenerate stationary points which can characterize their geometric properties.
Definition 2. (Index of non-degenerate stationary points) (Dubrovin et al., 2012) The index of
a symmetric non-degenerate matrix is the number of its negative eigenvalues, and the index of a
non-degenerate stationary point w ofa smooth function F is simply the index of its Hessian V2F (w).
Suppose that J(w) has m non-degenerate stationary points that are denoted as {w(1),
w(2),…，w(m)}. We prove following convergence behavior of these stationary points.
Theorem 2. Suppose Assumption 1 on the input datum x holds and the activation functions in a deep
neural network are linear Then if n ≥ Ch max(l3r2r4/(cqs log(d∕l)ε2τ4 log(1∕ε)), s log(d∕l)∕Z2)
where Ch is a constant, for k ∈ {1, ∙∙∙ , m}, there exists a non-degenerate stationary point Wnk) of
Jn(W) which corresponds to the non-degenerate stationary point WIk) of J (w) with probability at
least 1 - ε. In addition, wn(k) and w(k) have the same non-degenerate index and they satisfy
kwnk) - w(k)∣2 ≤ 十PCqr S log(dn/l)Jog(12/W,	(k =1,…，m)
with probability at least 1 - ε, where the parameters Cq, ωg, and Cg are given in Theorem 1.
Theorem 2 guarantees the one-to-one correspondence between the non-degenerate stationary points
of the empirical risk Jn(w) and the popular risk J (w). The distances of the corresponding pairs
5
Published as a conference paper at ICLR 2018
become smaller as n increases. In addition, the corresponding pairs have the same non-degenerate
index. This implies that the corresponding stationary points have the same geometric properties,
such as whether they are saddle points. Accordingly, we can develop more efficient algorithms,
e.g. escaping saddle points (Ge et al., 2015), since Dauphin et al. (2014) empirically proved that
saddle points are usually surrounded by high error plateaus. Also when n is sufficiently large, the
properties of stationary points of Jn(w) are similar to the points of the population risk J(w) in
the sense that they have exactly matching local minima/maxima and non-degenerate saddle points.
By comparing Theorems 1 and 2, we find that the requirement for sample number in Theorem 2
is more restrict, since establishing exact one-to-one correspondence between the non-degenerate
stationary points of Jn (w) and J (w) and bounding their uniform convergence rate to each other
are more challenging. From Theorems 1 and 2, we also notice that the uniform convergence rate of
non-degenerate stationary points has an extra factor 1/Z. This is because bounding stationary points
needs to access not only the gradient itself but also the Hessian matrix. See more details in proof.
Kawaguchi (2016) pointed out that degenerate stationary points indeed exist for DNNs. However,
since degenerate stationary points are not isolated, such as forming flat regions, it is hard to establish
the unique correspondence for them as for non-degenerate ones. Fortunately, by Theorem 1, the
gradients at these points of Jn(w) and J(w) are close. This implies that a degenerate stationary
point of J(w) will also give a near-zero gradient for Jn (w), i.e., it is also a stationary point for
Jn(w).
In the proof, we consider the essential multi-layer architecture of the deep linear network, and do not
transform it into a linear regression model and directly apply existing results (see Loh & Wainwright
(2015) and Negahban et al. (2011)). This is because we care more about deep ReLU networks which
cannot be reduced in this way. Our proof technique is more suitable for analyzing the multi-layer
neural networks which paves a way for analyzing deep ReLU networks. Also such an analysis
technique can reveal the role of network parameters (dimension, norm, etc.) of each weight matrix
in the results which may benefit the design of networks. Besides, the obtained results are more
consistent with those for deep nonlinear networks (see Sec. 5).
4.3	Uniform Convergence, Stability and Generalization of Empirical Risk
Based on the above results, we can derive the uniform convergence of empirical risk to population
risk easily. In this subsection, we first give the uniform convergence rate of empirical risk for deep
linear neural networks in Theorem 3, and then use this result to derive the stability and generalization
bounds for DNNs in Corollary 1.
Theorem 3. Suppose Assumption 1 on the input datum x holds and the activation functions in a
deep neural network are linear. Then there exist two universal constants cf0 and cf such that if
n ≥ Cf o max(l3r4∕(dι S log(d∕l)ε2 τ4 log(1∕ε)),s log(d∕l)∕(τ2dι)) ,then
SUp Jn(W) - J(w)∣ ≤ ef , Cfτmax (Pd^τr2l,rl) JSIOg(Idn/l：+ log(8/)	(2)
holds with probability at least 1 - ε. Here l is the number of layers in the neural network, n is the
sample size and dl is the dimension of the final layer.
From Theorem 3, when n → +∞, we have |Jn(w) -J(w)| → 0. According to the definition of
uniform convergence (Vapnik & Vapnik, 1998; Shalev-Shwartz et al., 2010), under the distribution
D, the empirical risk of a deep linear neural network converges to its population risk uniformly at
the rate of O(1∕√n). Theorem 3 also explains the roles of the depth l, the network size d, and the
number of nonzero weight parameters S in a DNN model.
Based on VC-dimension techniques, Bartlett & Maass (2003) proved that for a feedforward neural
network with polynomial activation functions and one-dimensional output, with probability at least
- ε the convergence bound satisfies |Jn (w) -
1
inff J(w)| ≤ 0(，(Y log2(n) + log(1∕ε))∕n).
Here γ is the shattered parameter and can be as large as the VC-dimension of the network model, i.e.
at the order of O(ld log(d) +l2d) (Bartlett & Maass, 2003). Note that Bartlett & Maass (2003) did not
reveal the role of the magnitude of weight in their results. In contrast, our uniform convergence bound
IS suPw∈ω | Jn(w) - J (w)| ≤ O(VZ(S log(dn∕l) + log(1∕ε))∕n). So our convergence rate is tighter.
6
Published as a conference paper at ICLR 2018
Neyshabur et al. (2015) proved that the Rademacher complexity of a fully-connected neural network
model with ReLU activation functions and one-dimensional output is O (rl∕√n) (see Corollary 2
in (Neyshabur et al., 2015)). Then by applying Rademacher complexity based argument (Shalev-
Shwartz & Ben-David, 2014a), We have | SuPf (Jn(W) - J(w))| ≤ O((rl + plog(1∕ε))∕√n) with
probability at least 1 - ε where the loss function is the training error g = 1(v(l) 6=y) in which v(l) is
the output of the l-th layer in the network model f(w; x, y). The convergence rate in our theorem
is O(r2l，(s log(d/l) + log(1∕ε))∕n) and has the same convergence speed O(1/√n) w.r.t. sample
number n. Note that our convergence rate involves r2l since we use squared loss instead of the
training error in (Neyshabur et al., 2015). The extra parameters s and d are involved since we consider
the parameter space rather than the function hypothesis f in (Neyshabur et al., 2015), which helps
people more transparently understand the roles of the network parameters. Besides, the Rademacher
complexity cannot be applied to analyzing convergence properties of the empirical risk gradient and
stationary points as our techniques.
Based on Theorem 3, we proceed to analyze the stability property of the empirical risk and the
convergence rate of the generalization error in expectation. Let S = {(x(i), y(i)), ∙∙∙ , (x(n), y(n))}
denote the sample set in which the samples are i.i.d. drawn from D. When the optimal solution
wn to problem (1) is computed by deterministic algorithms, the generalization error is defined as
g = Jn(wn) - J(wn). But one usually employs randomized algorithms, e.g. stochastic gradient
descent (SGD), for computing wn . In this case, stability and generalization error in expectation
defined in Definition 3 are more applicable.
Definition 3. (Stability and generalization in expectation) (Vapnik & Vapnik, 1998; Shalev-Shwartz
et al., 2010; Gonen & Shalev-Shwartz, 2017) Assume a randomized algorithm A is employed,
((x(i), y0i)),…,(x(n), y0n)))〜 D and w = argmmw Jn(W) Is the empirical risk mιnιmιzer
(ERM). For every j ∈ [n], suppose Wjj = argmi□w n-ɪ Pi=j fi(w; x(i), y(i)). We say that the
ERM is on average stable with stability rate a under distribution D if IES〜d,a,(x0.))〜D
ɪ Pn=Iifj (wj ； x(j), y0j)) - fj (Wn ； x(j), y0j))]∣ ≤ ek. The ERM is said to have generalization
error with convergence rate e^ under distribution D ifwe have ∣Es-d,a(J(Wn)- Jn(Wn)) | ≤
ek0.
Stability measures the sensibility of the empirical risk to the input and generalization error measures
the effectiveness of ERM on new data. Generalization error in expectation is especially important for
applying DNNs considering their internal randomness, e.g. from SGD optimization. Now we present
the results on stability and generalization performance of deep linear neural networks.
Corollary 1. Suppose Assumption 1 on the input datum x holds and the activation functions in a
deep neural network are linear. Then with probability at least 1 - ε, both the stability rate and the
generalization error rate of ERM of a deep linear neural network are at least ef:
∣	1n	∣	∣	∣
IES 〜D,A,(x(j),y0j))〜D — X (fj	- fj) |	≤ ef and 归S 〜D,A (J(W	) - Jn (W	)) |	≤ ef,
n j = 1
where fjj and fj respectively denote fj (Wjj; x0(j), y(0j)) and fj (Wn; x0(j), y(0j)), and ef is defined in
Eqn. (2).
According to Corollary 1, both the stability rate and the convergence rate of generalization error are
O(ef). This result indicates that deep learning empirical risk is stable and its output is robust to small
perturbation over the training data. When n is sufficiently large, small generalization error of DNNs
is guaranteed.
5	Results for Deep Nonlinear Neural Networks
In the above section, we analyze the empirical risk optimization landscape for deep linear neural
network models. In this section, we extend our analysis to deep nonlinear neural networks which
adopt the sigmoid activation function. Our analysis techniques are also applicable to other third-order
7
Published as a conference paper at ICLR 2018
differentiable activation functions, e.g., tanh function with different convergence rate. Here we
assume the input data are i.i.d. Gaussian variables.
Assumption 2. The input datum x is a vector of i.i.d. Gaussian variables from N(0, τ2).
Since for any input, the sigmoid function always maps it to the range [0, 1]. Thus, we do not require
the input x to have bounded magnitude. Such an assumption is common. For instance, Tian (2017)
and Soudry & Hoffer (2017) both assumed that the entries in the input vector are from Gaussian
distribution. We also assume W ∈ Ω as in (Xu & Mannor, 2012). Here We also assume that the entry
value of the target output y falls in [0, 1]. Similar to the analysis of deep linear neural networks, here
We also aim to characterize the empirical risk gradient, stationary points and empirical risk for deep
nonlinear neural netWorks.
5.1	Uniform Convergence of Gradient and S tationary Points
Here We analyze convergence properties of gradients of the empirical risk for deep nonlinear neural
netWorks.
Theorem 4.	Assume the input sample x obeys Assumption 2 and the activation functions in a deep
neural network are sigmoid functions. Then the empirical gradient uniformly converges to the
population gradient in Euclidean norm. Specifically, there are two universal constants cy and cy0
such that if n ≥ Cyo CdarII/(s log(d)τ2ε2 log(1∕ε)) where Cd = maxo≤i≤ι d%, then with probability
at least 1 - ε
SUp IlVJn(W)- VJ(w)∣∣ ≤ eι , T∖∕512CJ(l+2^mζdCrrSlog(dn/l) + log(4/。,
w∈Ω	2	7 729	n
where Cr = max(r2∕16, (r2∕16)l 1), and S denotes the nonzero entry number ofall weights.
Similar to deep linear neural netWorks, the layer number l, Width di , number of nonzero parameter
entries S, netWork size d and magnitude of Weights are all critical to the convergence rate. Also,
since there is a factor maxi di in the convergence rate, it is better to avoid choosing an extremely
Wide layer. Interestingly, When analyzing the representation ability of deep learning, Eldan & Shamir
(2016) also suggested non-extreme-Wide layers, though the conclusion Was derived from a different
perspective. By comparing Theorems 1 and 4, one can observe that there is a factor (1/16)l-1 in the
convergence rate in Theorem 4. This is because the convergence rate accesses the Lipschitz constant
and When We bound it, sigmoid activation function brings the factor 1/16 for each layer.
NoW We analyze the non-degenerate stationary points of the empirical risk for deep nonlinear neural
netWorks. Here We also assume that the population risk has m non-degenerate stationary points
denoted by {w(1), w(2),…，w(m)}.
Theorem 5.	Assume the input sample x obeys Assumption 2 and the activation functions in a
deep neural network are sigmoid functions. Then if n ≥ Cs max (cd∕3r2∕(s log(d)τ 2ε2 log(1∕ε)),
s log(d∕l)∕Z2) where Cs is a constant,for k ∈ {1, •一,m}, there exists a non-degenerate stationary
point Wnk) of Jn(W) which corresponds to the non-degenerate stationary point w(k) of J (W) with
probability at least 1 - ε. Moreover, Wn(k) and W(k) have the same non-degenerate index and they
obey
∣∣Wnk)-W(k)∣∣2 ≤ 2T r722Cyl(l + 2)(lCr + 1) CdCr Jlog(dn/丁ogM), (k =1,…，m)
with probability at least 1 - ε, where Cy, Cd and Cr are the same parameters in Theorem 4.
According to Theorem 5, there is one-to-one correspondence betWeen the non-degenerate stationary
points of Jn(W) and J (W). Also the corresponding pair has the same non-degenerate index, implying
they have exactly matching local minima/maxima and non-degenerate saddle points. When n is
sufficiently large, the non-degenerate stationary point Wnk) in Jn (w) is very close to its corresponding
non-degenerate stationary point W(k) in J (W). As for the degenerate stationary points, Theorem 4
guarantees the gradients at these points of J(W) and Jn(W) are very close to each other.
8
Published as a conference paper at ICLR 2018
5.2 Uniform Convergence, Stability and Generalization of Empirical Risk
Here we first give the uniform convergence analysis of the empirical risk and then analyze its stability
and generalization.
Theorem 6.	Assume the input sample x obeys Assumption 2 and the activation functions in a deep
neural network are the Sigmoidfunctions. If n ≥ 18l2r2/(s log(d)τ2ε2 log(1∕ε)), then
SUP Jn(W)-J (W)I ≤ en，T J 8 Cy Cd (1+ Cr (l - 1))尸*El	⑶
holds with probability at least 1 -ε, where cy, cd and cr are given in Theorem 4.
From Theorem 6, We obtain that under the distribution D, the empirical risk of a deep nonlinear
neural network converges at the rate of O(1/√n) (up to a log factor). Theorem 6 also gives similar
results as Theorem 3, including the inclination of regularization penalty on Weight and suggestion on
non-extreme-wide layers. Similar to linear networks, our risk convergence rate is also tighter than the
convergence rate on the networks with polynomial activation functions and one-dimensional output in
(Bartlett & Maass, 2003) since ours is at the order of O(P(l - 1)(s log(dn/l) + log(1∕ε))∕n), while
the later is
Oq (Y log2(n) + log(1∕ε))∕n)
where γ is at the order of O(ld log(d) + l2d) (Bartlett
& Maass, 2003).
We then establish the stability property and the generalization error of the empirical risk for nonlinear
neural networks. By Theorem 6, we can obtain the following results.
Corollary 2. Assume the input sample x obeys Assumption 2 and the activation functions in a deep
neural network are sigmoid functions. Then with probability at least 1 - ε, we have
where ∈n is defined in Eqn. (3). The notations fj and f here are the same in Corollary 1.
By Corollary 2, we know that both the stability convergence rate and the convergence rate of
generalization error are O(1∕√n). This result accords with Theorems 8 and 9 in (Shalev-Shwartz
et al., 2010) which implies O(1∕√n) is the bottleneck of the stability and generalization convergence
rate for generic learning algorithms. From this result, we have that if n is sufficiently large, the
empirical risk can be expected to be very stable. This also dispels misgivings of the random selection
of training samples in practice. Such a result indicates that the deep nonlinear neural network can
offer good performance on testing data if it achieves small training error.
6	Proof Roadmap
Here we briefly introduce our proof roadmap. Due to space limitation, all the proofs of Theorems 1
〜6 and Corollaries 1 and 2 as well as technical lemmas are deferred to the supplementary material.
The proofs of Theorems 1 and 4 are similar but essentially differ in some techniques
for bounding probability due to their different assumptions. For explanation simplic-
ity, we define four events: E = {suPw∈Ω ∣∣VJn(w) - VJ(w)∣∣2 > t}, Ei =
{suPw∈Ω k 1 Pn=I(Vf(w, X(i))-Vf(Wkw, X(i))) ∣2	> t∕3}, E2	=	{supwkw∈Ni,i∈[i]
Il n Pn=IVf(Wkw, x(i)) - EVf(Wkw, x)k2	> t∕3}, and E3 = {SUPw∈Ω kEvf (wkw , x)
-EVf (w, x)∣2 > t∕3}, where Wkw = [w1w; w]的;…；WIkiW] is constructed by selecting WkW ∈
Rdidi-1 from didi-1∕d-net Ni such that ∣W - Wkw ∣2 ≤ . Note that in Theorems 1 and 4, t is
respectively set to g and l. Then we have P(E) ≤ P(E1) + P(E2) + P(E3). So we only need to
separately bound P(E1), P(E2) and P(E3). For P(E1) and P(E3), we use the gradient Lipschitz
constant and the properties of -net to prove P(E1) ≤ ε∕2 and P(E3) = 0, while bounding P(E2)
needs more efforts. Here based on the assumptions, we prove that P(E2 ) has sub-exponential tail
associated to the sample number n and the networks parameters, and it satisfies P(E2 ) ≤ ε∕2 with
proper conditions. Finally, combining the bounds of the three terms, we obtain the desired results.
9
Published as a conference paper at ICLR 2018
To prove Theorems 2 and 5, we first prove the uniform convergence of the empirical Hessian
to its population Hessian. Then, We define such a set D = {w ∈ Ω : ∣∣VJ(w)∣b < e and
infi∣λi (V2J(W))I ≥ ζ}. In this Way, D can be decomposed into countably components, With
each component containing either exactly one or zero non-degenerate stationary point. For each
component, the uniform convergence of gradient and the results in differential topology guarantee
that if J(w) has no stationary points, then Jn (w) also has no stationary points and vise versa.
Similarly, for each component, the uniform convergence of Hessian and the results in differential
topology guarantee that if J(w) has a unique non-degenerate stationary point, then Jn(w) also has a
unique non-degenerate stationary point With the same index. After establishing exact correspondence
betWeen the non-degenerate stationary points of empirical risk and population risk, We use the
uniform convergence of gradient and Hessian to bound the distance betWeen the corresponding pairs.
We adopt a similar strategy to prove Theorems 3 and 6. Specifically, We divide the event
suPw∈ω I Jn(w) -VJ(w) | > t into Ei, E2 and E3 which have the same forms as their counterparts
in the proofs of Theorem 1 with the gradient replaced by the loss function. To prove P(Ei) ≤ ε∕2
and P(E3 ) = 0, we can use the Lipschitz constant of the loss function and the e-net properties. The
remaining is to prove P(E2 ). We also prove that it has sub-exponential tail associated to the sample
number n and the networks parameters and it obeys P(E2) ≤ ε∕2 with proper conditions. Then we
utilize the uniform convergence of Jn(w) to prove the stability and generalization bounds of Jn(w)
(i.e. Corollaries 1 and 2).
7	Conclusion
In this work, we provided theoretical analysis on the landscape of empirical risk optimization for
deep linear/nonlinear neural networks with (stochastic-)gradient descent, including the properties
of the gradient and stationary points of empirical risk as well as the uniform convergence, stability,
and generalization of the empirical risk itself. To our best knowledge, most of the results are new to
deep learning community. These results also reveal that the depth l, the nonzero entry number s of
all weights, the network size d and the width of a network are critical to the convergence rates. We
also prove that the weight parameter magnitude is important to the convergence rate. Indeed, small
magnitude of the weights is suggested. All the results are consistent with the widely used network
architectures in practice.
Acknowledgment
This work is partially supported by National University of Singapore startup grant R-263-000-C08-
133, Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112, NUS IDS
R-263-000-C67-646 and ECRA R-263-000-C87-133.
References
R. Alessandro. Lecture notes of advanced statistical theory I, CMU. http://www.stat.cmu.edu/
~arinaldo/36755/F16/Scribed_Lectures/LEC0914.pdf, 2016.
B. Bakshi and G. Stephanopoulos. Wave-net: A multiresolution, hierarchical neural network with localized
learning. AIChEJoumal, 39(1):57-81,1993.
P. Bartlett and W. Maass. Vapnik-chervonenkis dimension of neural nets. The handbook of brain theory and
neural networks, pp. 1188-1192, 2003.
E. Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193-215, 1988.
A.	Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In
AISTATS, 2015a.
A. Choromanska, Y. LeCun, and G. Arous. Open problem: The landscape of the loss surfaces of multilayer
networks. In COLT, pp. 1756-1760, 2015b.
R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with
multitask learning. In ICML, pp. 160-167, 2008.
10
Published as a conference paper at ICLR 2018
Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle
point problem in high-dimensional non-convex optimization. In NIPS, pp. 2933-2941, 2014.
B.	Dubrovin, A. Fomenko, and S. Novikov. Modern geometry—methods and applications: Part II: The geometry
and topology of manifolds, volume 104. Springer Science & Business Media, 2012.
R. Eldan and O. Shamir. The power of depth for feedforward neural networks. In COLT, pp. 907-940, 2016.
Y. Fyodorov and I. Williams. Replica symmetry breaking condition exposed by random matrix calculation of
landscape complexity. Journal of Statistical Physics, 129(5-6):1081-1116, 2007.
R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic gradient for tensor
decomposition. In COLT, pp. 797-842, 2015.
A. Gonen and S. Shalev-Shwartz. Fast rates for empirical risk minimization of strict saddle problems. COLT,
2017.
A.	Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP,
pp. 6645-6649, 2013.
D.	Gromoll and W. Meyer. On differentiable functions with isolated critical points. Topology, 8(4):361-369,
1969.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pp. 770-778,
2016.
G.	Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):
1527-1554, 2006.
G.	Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath,
et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.
K. Kawaguchi. Deep learning without poor local minima. In NIPS, pp. 1097-1105, 2016.
P. Loh and M. J. Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory
for local optima. JMLR, 16(Mar):559-616, 2015.
S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for non-convex losses. Annals of Statistics,
2017.
S. Negahban, B. Yu, M. Wainwright, and P. Ravikumar. A unified framework for high-dimensional analysis of
M-estimators with decomposable regularizers. In NIPS, pp. 1348-1356, 2009.
S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of
m-estimators with decomposable regularizers. In NIPS, 2011.
B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In COLT, pp.
1376-1401, 2015.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In ICML, 2017.
P. Rigollet. Statistic s997 lecture notes, MIT mathematics. MIT OpenCourseWare, pp. 23-24, 2015.
M. Rudelson and R. Vershynin. Hanson-wright inequality and sub-gaussian concentration. Electronic Communi-
cations in Probability, 18(82):1-9, 2013.
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge
Univ. Press, Cambridge, pp. 375-382, 2014a.
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algorithms. Cambridge
university press, 2014b.
S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence.
JMLR, 11:2635-2670, 2010.
S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of deep learning. ICML, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer
neural networks. arXiv preprint arXiv:1605.08361, 2016.
11
Published as a conference paper at ICLR 2018
D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks.
arXiv preprint arXiv:1702.05777, 2017.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper With convolutions. In CVPR, pp. 1-9, 2015.
Y. Tian. An analytical formula of population gradient for two-layered relu network and its applications in
convergence and critical point analysis. ICML, 2017.
V. N. Vapnik and V. Vapnik. Statistical learning theory, volume 1. Wiley NeW York, 1998.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, compressed sensing. Cambridge
Univ. Press, Cambridge, pp. 210-268, 2012.
H. Xu and S. Mannor. Robustness and generalization. Machine Learning, 86(3):391-423, 2012.
Y. Zhang, J. Lee, and M. Jordan. `1 -regularized neural netWorks are improperly learnable in polynomial time. In
ICML, pp. 993-1001, 2016.
Y. Zhang, P. Liang, and M. WainWright. Convexified convolutional neural netWorks. ICML, 2017.
12
Published as a conference paper at ICLR 2018
Supplementary Material of Empirical Risk Landscape Analysis for
Understanding Deep Neural Networks
A Structure of This Document
This document gives some other necessary notations and preliminaries for our analysis in Sec. B.
Then We prove Theorems 1 〜3 and Corollary 1 for deep linear neural networks in Sec. C. Then We
present the proofs of Theorems 4 〜6 and Corollary 2 for deep nonlinear neural networks in Sec. D.
In both Sec. C and D, We first present the technical lemmas for proving our final results and
subsequently present the proofs of these lemmas. Then we utilize these technical lemmas to prove
our desired results. Finally, we give the proofs of other auxiliary lemmas.
B	Notations and Preliminary Tools
Beyond the notations introduced in the manuscript, we need some other notations used in this
document. Then we introduce several lemmas that will be used later.
B.1	Notations
Throughout this document, we use(•, •)to denote the inner product. A 0 C denotes the Kronecker
product between A and C. Note that A and C in A 0 C can be matrices or vectors. For a matrix
A ∈ Rn1 ×n2, we use kAkF = Pi,j Ai2j to denote its Frobenius norm, where Aij is the (i, j)-th
entry of A. We use k A∣∣op = maxi ∣λi(A)∣ to denote the operation norm of a matrix A ∈ Rn1×n1,
where λi(A) denotes the i-th eigenvalue of the matrix A. For a 3-way tensor A ∈ Rn1 ×n2 ×n3, its
operation norm is computed as
IIAkop = SUP lλ , A) = X Aijkλiλjλk,
kλk2≤1	i,j,k
where Aijk denotes the (i, j, k)-th entry of A. Also we denote the vectorization of W(j) (the weight
matrix of the j -th layer) as
w(j) = vec W (j) ∈ Rdj dj-1 .
We denote Ik as the identity matrix of size k × k.
For notational simplicity, we further define e , v (l) - y as the output error vector. Then the squared
loss is defined as f (w; x, y) = 2∣∣ek2, where W = (w(i)；…；w(i)) ∈ Rd contains all the weight
parameters.
B.2	Technical Lemmas
We first introduce Lemmas 1 and 2 which are respectively used for bounding the '2-norm of a vector
and the operation norm of a matrix. Then we introduce Lemmas 3 and 4 which discuss the topology
of functions. In Lemma 5, we give the relationship between the stability and generalization of
empirical risk.
Lemma 1. (Vershynin, 2012) For any vector X ∈ Rd, its '2-norm can be bounded as
kx∣2 ≤ 1--^ sup hλ, Xi .
where λ = {λ1, . . . , λkw} be an -covering net of Bd(1).
Lemma 2. (Vershynin, 2012) For any symmetric matrix X ∈ Rd×d, its operator norm can be
bounded as
IIXkop ≤ ∑----5 sup lhλ, xλil.
1-	2 λ∈λ
where λ = {λ1, . . . , λkw} be an -covering net of Bd(1).
13
Published as a conference paper at ICLR 2018
Lemma 3. (Mei et al., 2017) Let D ⊆ Rd be a compact set with a C2 boundary ∂D, and f, g :
A → R be C2 functions defined on an open set A, with D ⊆ A. Assume that for all w ∈ ∂D
and all t ∈ [0,1], tVf (w) + (1 一 t)Vg(w) = 0. Finally, assume that the HeSSian V2f (w) is
non-degenerate and has index equal to r for all w ∈ D. Then the following properties hold:
(1)	If g has no critical point in D, then f has no critical point in D.
(2)	If g has a unique critical point w in D that is non-degenerate with an index of r, then f also has
a unique critical point w0 in D with the index equal to r.
Lemma 4. (Mei et al., 2017) Suppose that F(w) : Θ → R is a C2 function where w ∈ Θ. Assume
that {w(1), . . . , w(m)} is its non-degenerate critical points and let D = {w ∈ Θ : kVF(w)k2 <
and infi λi V2F (w)	≥ ζ}. Then D can be decomposed into (at most) countably components,
with each component containing either exactly one critical point, or no critical point. Concretely,
there exist disjoint open sets {Dk}k∈N, with Dk possibly empty for k ≥ m + 1, such that
D = ∪k∞=1Dk .
Furthermore, w(k) ∈ Dk for 1 ≤ k ≤ m and each Di, k ≥ m + 1 contains no stationary points.
Lemma 5. (Shalev-Shwartz & Ben-David, 2014b; Gonen & Shalev-Shwartz, 2017) Assume that
D is a sample distribution and randomized algorithm A is employed for optimization. Suppose
that ((x(i), y0i))，…，(x(n), y0n)))〜D and Wn = argmmw Jn(W). For every j ∈ {1,…,n},
suppose wjj = argmi□w n-ɪ Pi=j fi(w; x(i)，y(i)). For arbitrary distribution D, we have
n
ES〜D,A,(xj),yj))〜Dn XUj- fj) = ES〜D, A (J(Wn)- Jn(Wn)).
j=1
where fj and f respectively denote f (Wj; xj)，yj)) and f (wn; xj)，yj)).
C Proofs for Deep Linear Neural Networks
In this section, we first present the technical lemmas in Sec. C.1 and then we give the proofs of
these lemmas in Sec. C.2. Next, We utilize these lemmas to prove the results in Theorems 1 〜3 and
Corollary 1 in Sec. C.3. Finally, we give the proofs of other lemmas in Sec. C.4.
C.1 Technical Lemmas
Here We present the technical lemmas for proving our desired results. For brevity, We also define
Bj:s as folloWs:
Bs：t，W(S)W(s-1)…W(t) ∈ Rds×dt-1，(s ≥ t);	Bs，I，(s<t).	(4)
Lemma 6. Assume that the activation functions in the deep neural network f(W， x) are linear
functions. Then the gradient of f(W， x) with respect to W(j) can be written as
Vwj) f(W，x) = ((Bj—i：ix)乳 BT∙+ι) e，(j = 1, •••，/)，
where 0 denotes the Kronecke product. Then we can compute the Hessian matrix as follows:
-Vw ⑴(Vw(i)f(W，X))…Vw ⑴(Vw(I) f(W，x))
2 r, ∖	Vw(2) (Vw(1)f (W，X))…Vw(2) (Vw(I) f (W，X))
V2 f (W，X)=	.	.	^	，
...
_Vw(I) (Vwa)f(W，X))… Vw(I)(Vw(1)f(W，X))
where Qst , Vw(s) Vw(t) f(W， x) is defined as
((Br 1.s+l)0(Bs-1:IXeT BTt+l) + (Bs-1:1XxT B 乙:ɪ)0 (BTs+1 Bl.t+l)，if s<t，
Qst = (Bs-1.1 XXTBs-1.1) 0(Bi：s+1TBι.s+ι)，	if S = t，
I(BTs+ 1eXTBT-1:1)0Bs-1.t+1 + (Bs-1:1XXTBT11：1)0 (BTs+1Bl：t+1)， if s>t.
14
Published as a conference paper at ICLR 2018
Lemma 7. Suppose Assumption 1 on the input data x holds and the activation functions in deep
neural network are linear functions. Then for any t > 0, the objective f (w, x) obeys
P (1 X(f(w, X(i))-E(f(w, X(i)))) >t) ≤ 2exp (-Cf,nmin (--------f---------
i=1	ωf max dlωfτ4, τ2
where cf0 is a positive constant and ωf = rl.
Lemma 8. Suppose Assumption 1 on the input data x holds and the activation functions in deep
neural network are linear functions. Then for any t > 0 and arbitrary unit vector λ ∈ Sd-1, the
gradient Vf (w, x) obeys
P(1 X (<λ, Vwf(w, X(i)) -EVwf(w, X(i))))>t
»
t2
t
≤ 3exp( cg0nmin lmax (ωgτ2,ωgT4,3§，τ2), pωΓg max(τ,τ2)
where Cg, is a constant; ωg = Cqr2(2l-1) and ωg, = Cqr2(I-I) in which Cq = ,maxo≤i≤ι di.
Lemma 9. Suppose Assumption 1 on the input data x holds and the activation functions in deep
neural network are linear functions. Then for any t > 0 and arbitrary unit vector λ ∈ Sd-1, the
Hessian V2f (w, x) obeys
1n
P n E (<λ, (Vw f(w, X(i)) - EVw f(w, X(i)))λ>) >
n i=1
≤ 5exp (-ch0 n min (T 2l2 max(ωg—T 2 ,ω%)，5 l max(τ,τ 2)
where ωg = r4(l-1) and ωh = r2(l-2).
Lemma 10. Suppose the activation functions in deep neural network are linear functions. Then for
any w ∈ Bd (r) and x ∈ Bd0 (rx), we have
∣∣Vwf(w, x)k2 ≤ √αg, where αg = Ctlr4r4l-2.
in which Ct is a constant. Further, for any w ∈ Bd (r) and x ∈ Bd0 (rx), we also have
∣∣V2 f(w, x)∣∣ op ≤ ∣∣V2f (w, X)IlF ≤ l√αl,	where αι，Ct r4 r4l-2.
in which Ct0 is a constant. With the same condition, we can bound the operation norm of V3f (w, x).
That is, there exists a universal constant αp such that ∣V3f (w, x)∣op ≤ αp.
Lemma 11. Suppose Assumption 1 on the input data x holds and the activation functions in deep neu-
ral network are linear functions. Then there exist two universal constant Cg and Ch such that the sam-
ple Hessian converges uniformly to the population Hessian in operator norm. Specifically, there exit
α2r2
two universal constants c刈 and Ch? such that if n ≥ Ch? max( /2噂-2./3 iog(d∕i)，S log(d∕l)∕(lτ2)),
then
SUp∣∣V2Jn (Wu (W)]	≤ ChITl3h Jd lθg(nl)+lθg(2%)
w∈Ω	op	n
holds with probability at least 1 - ε, where ωh = max T r2(l-1), r2(l-2), rl-2.
C.2 Proofs of Technical Lemmas
To prove the above lemmas, we first introduce some useful results.
Lemma 12. (RudelSon & Vershynin, 2013) Assume that X = (xi； X2; •一；Xk) ∈ Rk is a random
vector with independent components xi which have zero mean and are independent Ti2-sub-Gaussian
variables. Here maxi Ti2 ≤ T2. Let A be an k × k matrix. Then we have
E exp ( λ ( X Aij XiXj - E( X Aij Xixj))) ≤ exp (2τ 2λ2∣A∣F), ∣λ∣ ≤ 1∕(2τ ∣∣A∣∣2).
i,j:i6=j	i,j:i6=j
15
Published as a conference paper at ICLR 2018
Lemma 13. Assume that X = (xi； x2; ∙一；Xk) ∈ Rk is a random vector with independent Compo-
nents xi which have zero mean and are independent τi2-sub-Gaussian variables. Here maxi τi2 ≤ τ2.
Let a be an n-dimensional vector. Then we have
Eeχp (λ (Xaiχ2-E (Xaiχ2))) ≤ Eeχp (l28λ2τ4(Xa，), lλl ≤ τ2m1xa.
Lemma 14. For Bj:t defined in Eqn. (4), we have the following properties:
kBs:t kop ≤ kBs:t kF ≤ ωr and kBl:1 kop ≤ kBl:1 kF ≤ ωf,
where ωr = rs-t+1 ≤ maχ r, rl and ωf = rl.
Lemma 13 is useful for bounding probability. The two inequalities in Lemma 14 can be obtained by
using ∣∣w(j) ∣∣2 ≤ r (Vj = 1, ∙∙∙ ,l). We defer the proofs of Lemmas 13 and 14 to Sec. C.4.2.
C.2. 1 Proof of Lemma 6
Proof. When the activation functions are linear functions, we can easily compute the gradient of
f(w, X) with respect to w(j):
Vw(j.)f (w, x) = ((Bj-Lix)乳 BT-+i) e, (j = 1,…，l),
where 0 denotes the Kronecker product. Now We consider the computation of the Hessian matrix.
For brevity, let Qs = ((Bs-i：ix) 0 BTS+J. Then We can compute NWS f(w, x) as follows:
▽2	f (	) = ∂2f (w, x) = ∂2f(w, x) = ∂(Qse) = ∂Vec(Qse)
W(S) W X -dWs)dw⑸-dW;/%) — dWT) —	dwTs)
_dvec(QsBi：s+iW(t)Bs-rιx)
∂WT、
(s)
_d ((Bs-1:1X)T 0 (QsB.+ι)) Vec (W (S))
∂wT∖
(s)
=(Bs-i:iX)T 0 (((Bs-i：ix) 0 BTTs+ι) Bi：s+i)
=(Bs-i：IX)T 0 ((Bs-i：ix) 0 (BTs+iBi：s+i))
=((Bs-i：iX)T 0 (Bs-i：ix)) 0 (BTs+iBi：s+i)
=((Bs-i：ix)(Bs-i：IX)T) 0 (BTs+iBi：s+i),
where ① holds since Bj-i：iX is a vector and for any vector x, we have (x 0 A)B = X 0 (AB).
② holds because for any four matrices Zi 〜Z3 of proper sizes, we have (Zi 0 Z2) 0 Z3 =
Z1 0 (Z2 0 Z3). ③ holds because for any two matrices z1, z2 of proper sizes, we have z1z2T =
zi 0 z2T = z2T 0 zi .
Then, we consider the case s > t:
▽	(▽	)	_ ∂2f(w, x)	_	∂2f(w, x)	_	∂(Qse) _ ∂vec (Qse)
vw(t) (Vw(sf(w, X))= ∂w(T)∂w(s)	=	∂w(T)∂w(s)=	F =	∂W(T)
= ∂vec (QsBi：t+iW(t)Bt-i：ix)	∂Vec (((Bs-∏x) 0 BTs+i) e)
=	∂wT	+	∂wT
∂w(t)	∂w(t)
Notice, here we just think that Qs in the
∂Vec(QsBl
:t+1W(t)Bt-1:1x)
dwT
is a constant matrix and is not
related to W(t). Similarly, we also take e in
we have
dvec(((Bs-i：ix 冶 BTs+ 'e)
dwT)
as a constant vector. Since
∂Vec (QsBmIW (t)Bt-i：ix)
可
(Bs-i:iXXTBT-i:i) 0 (BTs+iBl：t+i
16
Published as a conference paper at ICLR 2018
we only need to consider
∂vec (((Bs-i：ix) 0 BTs+J e) ∂vec ((Bs-gx) 0 (BTs+ιe))
∂vec((Bs-1:1X)(BTs+ 1e))
=	d WT)
_Jvec (Bsτt+ιW㈤(BtτιxeTBi：s+i))
∂wT
_d (Bt-1:IxeTBi：s+i)T 0 Bs-I：t+1vec (W㈤)
∂ WT
=(Bt-1:IxeTBι-.s+ι^τ 0 Bs-i：t+i.
Therefore, for s > t, by combining the above two terms, we can obtain
▽w⑴(Vw(S) f(w, X)) = (BTs+ 1exTBT-1：l)0Bs-1:t+1 + (Bs-1:1XxTBT-i：i) 0(BrTs + iBi：t+i)
Then, by similar method, we can compute the Hessian for the case s < t as follows:
▽w(t)(▽ w (Sf(W, X)) = (Bt-1：s+J 0 (Bs-1：1xeTBlTt+1) + (Bs-1:1XxTBN1:1) 0 (BlTs+1Bl：t+1
The proof is completed.	□
C.2.2 Proof of Lemma 7
Proof. We first prove that v(l), which is defined in Eqn. (5), is sub-Gaussian.
V(I) = W(I)... W⑴x = Bl:1 x.	(5)
Then by the convexity in λ of exp(λt) and Lemma 14, we can obtain
E (exp (Dλ, v(l) -E(v(l))E)) =E (exp (hλ, Bl:1x - EBl:1xi))
≤E (eχp (〈BTI' x〉))
≤ eχp (kBTI『声2 )	(6)
where ① Uses the conclusion that ∣∣Bldkop ≤ kBld∣∣F ≤ ωf in Lemma 14. This means that V(I) is
centered and is ωf2 τ 2 -sub-Gaussian. Accordingly, we can obtain that the k-th entry of v(l) is also
zk τ 2 -sub-Gaussian, where zk is a universal positive constant. Note that maχk zk ≤ ωf2 . Let Vi(l)
17
Published as a conference paper at ICLR 2018
denotes the output of the i-th sample x(i). By Lemma 13, we have that for s > 0,
p(i XX (kv(l)k2 - eh%) > t!=p 卜 XX (kv(l)k2 - EM%) >nst!
≤ exp ' snt) E t X (kv(l)k2- Ekvi(I) k2)
≤ exp (-Snt )YY E (S (kv(l)k2 - Ekv(I)k2))
i=1
③
≤ exp ( --- ) Yexp (I28dιS2ω4τ4)	|s| ≤
2 i=1	ωfτ
④
≤ exp
Note that ① holds because of Chebyshev,s inequality.② holds since x(i)are independent.③ is
established by applying Lemma 13. We have ④ by optimizing s. Since V(I) is sub-Gaussian, we have
n
n X (yTv(l)- EyT
i=1
nt2
8ωfτ 2kyk2
②
≤ exp -
where ① holds because of Eqn. (6) and We have ② since We optimize s.
Since the loss function f(w, x) is defined as f(w, x) = kv(l) - yk22, we have
f(w,x) - E(f(w, x)) = kv(l) - yk22-E(kv(l)-yk22)= (kv(l)k22-Ekv(l)k22)+(yT v(l)-EyT v(l)) .
Therefore, we have
P (1 X (f (w, X(i)) - E(f (w, X(i)))) > t)
≤p (1X (kv(l)k2 - Ekv(I)k2) > 2! + P (1X (yTv(I)- EyTv(I)) > 2!
i=1	i=1
≤2exp 卜 Cf 0n min dfp
where cf0 is a constant. Note that kyk22 is the label of x, then it can also be bounded. The proof is
completed.	□
C.2.3 Proof of Lemma 8
Proof. For brevity, let Qj denote Vw(j)f (w, x). Then, by Lemma 6 we have
Vw(3) f (W)= ((Bj-1：1x)③ BTj+1) e = (Bj-1:1X)T(BT∙+1e) = (Bj-1:1 乳 Bj+1) (X ㊈ e),
(7)
18
Published as a conference paper at ICLR 2018
where ① holds since Bj-i：iX is a vector, and ② holds because for any four matrices Zi 〜Z4 of
proper sizes, we have (Z1Z3)0(Z2Z4) = (Zi③Z2)(Z30Z4). Note that e = v(l) —y = Bi：ix — y.
Then we know that the i-th entry Qij has the form Qij = Pp,q zpijqxpxq + Pp ypij xp + rij (Step 1
blow will give the detailed analysis) where xp denotes the p-th entry in x. Note that zpijq , ypij and rij
are constants and independent on x.
We divide λ ∈ RPj=I djdj-1 into λ = (λι;…;λι) where λj ∈ Rdjdj-1. Let λj denote the i-th
entry in λj . Accordingly, we have
l
E，hλ, Vw f(w, X)- EVwf (w, x)i = X hλj, Qj — EQj)= Ei + E? + E3,
j=i
where Ei , E2 , and E3 are defined as
(l dj dj-1	l dj dj-1	∖
X X	λjZpq	(XpXq-	EXpXq) ,	E2= X X X	λjZpp	(Xp -	EXp),
j=i i=i	p j =i i=i
(L dj dj-1	∖
X X λij ypij	(Xp-EXp).	(8)
j =i i=i
Thus, we can further separate the event as:
P(E >t) ≤ P(1X Ek > f)+P(1X 瑛>3)+P(1X Ek > t).
nnn
k=i	k=i	k=i
Thus, to prove our conclusion, we can respectively establish the upper bounds of the three events.
To the end, for each input sample X(i), we divide its corresponding Qj - EQj into Ei, E2 and
E3 . Then we bound the three events separately. Before that, we first give several equalities. Since
Bj：s = W(j)W(j-i)…W(S) (j ≥ s), by Lemma 14 we have
kBj:sk2F ≤ r2(j-s+i) and kBl:t+ik2F kBt-i:s+ik2F kBs-i:ik2F ≤ r2(l-2),	(9)
These two inequalities can be obtained by using kW (i) k2F = kw(i) k22 ≤ r2.
Step 1. Divide Qj- EQj: Note that e = V(I) - y = Bi：ix — y. Let Hj = Bj-i：i 0 BTj+「Then
we can further write Eqn. (7) as
Qj = Vw(j) f (w) = Hj (X 0 (Bl:iX) - X 0 y) = Hj ((Id0 0 Bl:i) (X 0 X) - X 0 y) , (10)
where Id0 ∈ Rd0×d0 is the identity matrix. According to Eqn. (10), we can write the i-th entry of
Qj as the form Qij = Pp,q ZpijqXpXq + Pp ypij Xp + rij where Xp denotes the p-th entry in X. Let
Zj = Hj (Id0 0 Bl:i) ∈ Rdj dj -1 ×d20. Then, we know that the i-th entry Qij = Z(i, :)X0, where
X0 = X 0 X = [xix; X2X；…,Xdox] ∈ Rd2. In this way, we have Zpq = Zj(i, (P - 1)do + q)
which further implies
X(Zpijq)2 ≤ cqkZj(i, :)k22,	(11)
p,q
where Cq = vzmax0≤i≤Γdi.
We divide the i-th row Hj(i,:) into Hj(i,:) = [Hji, Hji,…，Hdi] where Hji ∈ R1×dl. Then
we have ypij = yT Hjpi . This yields
X(ypj)2 ≤ CqX(yTHPi)2 ≤ CqX kyk2kHik2 = Cqkyk2kHj@ ：)k2.	(12)
pp	p
Let λij denote the i-th entry of λj . Then, by Eqn. (8), we can obtain
hλj, (Qj - E(Qj))i = apq (XpXq - EXpXq)+ app X2p - EX2p + bp (Xp - EXp)
j	p,q:p6=q	p	p
= Ei + E2 + E3,
19
Published as a conference paper at ICLR 2018
where apq and bp are defined as
l dj dj-1	l dj dj-1
apq=X X λijzpijq and bp=X X λijypij.
j=1 i=1	j=1 i=1
Note that for any four matrices of proper sizes, we have (Qi 0 Q2)(Q3 0 Q4) = (Q1Q3) 0 (Q2Q4),
indicating Zj = (Bj-i：i 0 BTj+1) (Id。0 Bl：i) = Bj-i：i 0 (BTj+1Bi：i). This gives
Cq kZj kF ≤ Cq kBj-1：1kF ∣Rj + 1kF kB，:1kF ≤ Cq ZT)T〃 = Cq r2(2l-1)，ω. (13)
Note that Eqn. (13) uses the conclusion in Eqn. (9). Therefore, we can have the following bound:
djdj-1	dj dj-1	dj dj-1
X	(zpijq)2	≤ X	X(zpijq)2	≤Cq	X	kZj(i,:)k22=CqkZjk2F	≤ω, (14)
i=1	i=1	p,q	i=1
where ① uses Eqn. (11). Then We can utilize Eqn. (14) and
apq as follows:
Plj=1	Pid=jd1j-1 (λij)2
= 1 to bound
apq ≤ l
l l	/ dj dj-1
X I X	λj Zpjq
j=1 i=1
l	dj dj-1	djdj-1
l Xl X (λj)2)( X (Zpjq )2) ≤ lω.
j=1	i=1	i=1
which further gives
l	dj dj-1	dj dj-1	①
X apq ≤	l Xl	X	(λj)2)∣ X	X(Zpjq )2∣ ≤ lω.
p,q	j=1	i=1	i=1	p,q
where ① USeS Eqn. (14).
Similarly, we can obtain
dj dj-1	dj dj-1
X (ypij)2 ≤ X Cqkyk22kHj(i,:)k22=Cqkyk22kHjk2F ≤ Cqkyk22r2(l-1).	(15)
i=1	i=1
So we can bound bp as
l	djdj-1	2	l	djdj-1	dj dj-1
bp ≤	l Xl	X	λj yj	≤ l X(	X	(λj )21 ( X	(ypj )2) ≤ lω0,
j=1	i=1	j=1	i=1	i=1
where ω0 = Cqkyk22r2(l-1). Accordingly, we can have
l	djdj-1	dj dj-1	①
Xbp≤lXl X (λj)21 I X X(ypj)21 ≤lω0,
p	j=1	i=1	i=1	p
where ① uses (15).
20
Published as a conference paper at ICLR 2018
Step 2. Bound P(E1 > t/3), P(E2 > t/3) and P(E3 > t/3): Let Ehk1 denotes the Eh1 which
corresponds to the k-th sample x(k). Therefore, we can bound
P (n XX Ek > I)=P (S XX( X apq (xp x” Exp xk))>sn
k=1	k=1 p,q:p6=q
- Expkxqk
≤ exp (-nst) E exP (S X( X apq (xpxk
k=1 p,q:p6=q
- Expkxqk
|s| ≤ —1=
2τ√ιω
where ① holds because of Chebyshev's inequality. ② holds since x(i)are independent. ③ is
established by applying Lemma 12. We have ④ by optimizing s. Similarly, by Lemma 13 we can
bound P (n Pn=I Ek > 3) as follows:
P ∣n XX Ek> I) ≤ exp (-等)Y E exp
k=1	k=1
k
app
((xp)2- E(Xp)2)))
n
nst
≤exp 卜 下
1
exp (128τ4s2lω)	|s| ≤ ɪ
k=1	τ2 lω
≤ exp -c00 n min
t
，√lωτ2
Finally, since x(i) are independent sub-Gaussian, we can use Hoeffding inequality and obtain
P n XX 瑛 > I ≤P n XX
k=1	k=1
k t	C000 nt2
-Exp)	> 3 eχp - F
Step 3. Bound P(E>t): By comparing the values of ω and ω0, we can obtain
P(E >t)≤P(1XX Ej > 1)+P(1XXE2 > 3)+P
n	3n	3
k=1
k=1
t2
1XX Ej> 3
n
k=1
t
≤3eχp -Cgo n min --------------------,  -------------
lmax(ωgτ2, ωgτ4, ωg0τ2)	lωg max (τ,τ2)
,
where ωg = Cqr2(2l-1) and ωgo = Cqr2(l-1) in which Cq = Vzmax0≤i≤1 di. The proof is completed.
口
C.2.4 Proofs of Lemma 9
Proof. For brevity, let Qts denote Nwg (Vw(s)f (w, x)). Then, by Lemma 6 we have
((BTs+ 1exTBT-1:1)% Bs-1：t+1 + (Bs-1:IxxTBT-1:1)乳(BTs+ 1Bl：t+1), if s > t,
Qts =	(Bs-1：IxxTBs-1：1)乳 B3s++TτB(：s+1),
if s = t,
(BT-I:s+1)1 (Bs-1:1XeTBTt+1) + (Bs-1:1XxTBT-1:1)乳(BTs+ ιBl"+l) , if s<t.
21
Published as a conference paper at ICLR 2018
Then we know that the (i, k)-th entry Qitks has the form Qitks = Pp,qzpikqxpxq + Ppypikxp + rik
(explained in the following Step 1. I) where xp denotes the p-th entry in x. Note that zpikq , ypik
and rik are constant and independent on x. For convenience, we let Qts = Hts + Gts, where
Gts =(Bs-i：IxxT BT-i：J ③(BTs+iB(：t+i) and Hts is defined as
((BTs+1exTBT-1:1)® Bs-1：t+1,	if s>t,
Hts =	0,	ifs=t,
I(BT-I：s + i)③(Bs-1：1xeTBTt+1), if s <t.
Let
nn
E = - X <λ, (Vw f(w, x) - EVw f(w, x)) λ , Eh = - XXhλt,(Hts - E(Hts)) λ ,
j=1	j=1 t,s
-n
Eg = - E Ehλt,(Gts -E(Gts)) λsi.
j=1 t,s
Then we divide the event as two events:
P(E > t) =P(Eh +Eg > t) ≤ P(Eh > t/2) +P(Eg > t/2) .
Now we look each event separately. Similar to Qts, the (i, k)-th entry Htisk has the form Htisk =
Pp q ZpqXpXq + Pp ypkxp + rik. We divide the unit vector λ ∈ Rd as λ = (λι;…;λι) where
λj ∈ Rdj dj-1. For input vector x, let Pt,shλt, (Hts - E(Hts)) λsi = Eh1 + Eh2 + Eh3, where
Ehi= X ιXX(λtλk)zpq i(xpxq-Expxq), Eh2= χ∣χχ(λtλk)zpq,ρ-ExP),
p,q:p6=q t,s i,k	p t,s i,k
Eh3 = X (XX(λtλk)ypk) (xp - Exp),	(16)
where xp denotes the p-th entry in x and λij denotes the i-th entry of λj . Let Ehj , Ehj , and Ehj
denote the Eh1, Eh2, and Ehj of the j-th sample. Thus, considering - samples, we can further
separately divide the two events above as:
P
1
+p( - XEh 2 > 1)+P (-XEh 3 > 6
Similarly, we can define Eg1, Eg2 and Eg3.
PfE > n <P 八 X Ej > t P 1 X Ej > t P 1 X Ej ' t
P(Eg>2； ≤P(-MEgI>6 ) +P(-MEg2>6)+P (-MEg3>6
Thus, to prove our conclusion, We can respectively establish the upper bounds of P (Eh > 2) and
P(Eg > 2).
Step 1: Bound P(Eh > 2)
To achieve our goal, for each input sample x(i), we divide its corresponding Pt,s (Hts - EHts)
as Eh1 , Eh2 and Eh3 . Then we bound the three events separately. Before that, we first give two
equalities. Since Bj：s = W(j) W(j-i) …W(s) (j ≥ s), by Lemma 14 we have
kBj：skF ≤ r2(j-s+1) and ∣Rt+ιkF |田—+“除 ∣∣BsτιkF ≤ r2(l-2),	(。)
These two inequalities can be obtained by using kW (i) k2F = kw(i) k22 ≤ r2 .
22
Published as a conference paper at ICLR 2018
I.	Divide Hts - EHts : For t 6= s, we can write the (i, k)-th entry Htisk as the form Htisk =
Pp,q zpikqxpxq + Pp ypikxp +rik. Now we try to bound zpikq and ypik. We first consider the case s < t.
Note that e = v(l) - y = Bl:1x - y. Specifically, we have
Hts = (BT-I:s + 1)③(Bs-1:1XxTBTIBTt+1 - Bs-1:1xyTBTt+J .
So the (i0, k0)-th entry in the matrix Bs-1:1xxT BlT:1BlT:t+1 is [Bs-1:1xxT BlT:1BlT:t+1]i0k0 =
(Bs-1:1)(i0, :)x(Bl:1Bl:t+1)(k0, :)x = xT((Bs-1:1)(i0, :))T (Bl:1Bl:t+1)(k0, :)x, where A(i0, :)
denotes the i0-th row of	A.	Let	i0i	= mod(i, ds),	kk0	=	mod(k,	dt-1),	i0i0	= bi/dsc and
kk00 = bk/dt-1c. In this case, the (i, k)-th entry Htisk = [Bt-1:s+1]kk00i0i0xT((Bs-1:1)(i0i, :
))T(Bl:1Bl:t+1)(kk0 , :)x + [Bt-1:s+1]k0k0i0i0yT(Bl:t+1)(kk0 , :)T(Bs-1:1)(i0i, :)x. Therefore, we have
X(zpikq)2=[Bt-1:s+1]2kk00i0i0 ((Bs-1:1)(i0i,:))T(Bl:1Bl:t+1)(kk0,:)2F
p,q
≤[Bt-1..s+1]l,,ii, k(Bs-i：i)(ii, ：)k2 k(B，：iB，：t+i)(kk, :)k2.
Therefore, we can further establish
XX(zpikq)2 ≤X[Bt-1:s+1]2kk00i0i0 k(Bs-1:1)(i0i, :)k22 k(Bl:1Bl:t+1)(kk0 , :)k22
i,k p,q	i,k
≤ X[Bt-1:s+1]2kk00i0i0 k(Bs-1:1)(i0i, :)k22 k(Bl:1Bl:t+1)(kk0 , :)k22
i,k
=Xk(Bt-1:s+1)(kk00,:)k22 kBs-1:1k2F k(Bl:1Bl:t+1)(kk0,:)k22	(18)
k
=kBt-1:s+1k2F kBs-1:1k2F kBl:1Bl :t+1k2F
≤r4(IT)，ω.
where ① uses Eqn. (17). Similarly, We can bound
X(ypik)2 =[Bt-1:s+1]2kk00i0i0 yT (Bl:t+1)(kk0,:)T (Bs-1:1)(i0i,:)2F
p
≤[Btτs+ι]kk0iio ky∣∣2 k(B,t+1)(kk, :)k2 k(Bsτι)(ii, :)k2.
So it further yields
XX(ypk)2 ≤ X[Btτs+%ii0 kyk2 k(Blt+ι)(kk, :)k2 k(Bs-i：i)(ii, :)k；
i,k p	i,k
≤ kyk2 kBt-i：s+ikF kBi：t+ikF kBs-i：ikF ≤ kyk2 r2(l-2)，ω0,
(19)
where ① uses Eqn. (17). Note that for the case S ≥ t, Eqn. (18) and (19) also holds. Let λj denote
the i-th entry of λj. Then, by Eqn. (16), we can obtain
(hλt, (Hts - E(Hts))λs i) =	apq (xpxq-Expxq)+	app x2p - Ex2p +	bp (xp-Exp)
t,s	p,q:p6=q	p	p
= Eh1 + Eh2 + Eh3 ,
where apq and bp are defined as
apq =XX(λitλsk)zpikq and bp =XX(λitλsk)ypik.
t,s i,k	t,s i,k
Then according to Eqn. (18) and Pt,s Pi,k(λitλsk)2 = 1, we can bound apq as follows:
apq ≤ I2 χ(χ(λiλk )zpq) ≤ I2 χ(χ(λiλk)2](χ(zpq ] ≤ ωi2 x(x(wk ] ≤ ωi2.
t,s i,k	t,s i,k	i,k	t,s i,k
23
Published as a conference paper at ICLR 2018
which further yields
Xapq≤I2X(X(λiλk)j (XX(Zpq)2) ≤ωl2X(X(λtλk)2) ≤ωl2.
Similarly, by using Eqn. (19), we have
bp ≤ l2 X fχ(λiλk)ypk∖ ≤ l2 X fχ(λtλk)2) (X(ypk)2] ≤ ω0l2.
t,s i,k	t,s i,k	i,k
Accordingly, we can have
Xbp≤ 12X (X(λtλk)2)(XX(ypk)2) ≤ω0l2.
p	t,s i,k	i,k p
II.	Bound P(Eh1 > t/6), P(Eh2 > t/6) and P(Eh3 > t/6): Let Ehj1 denotes the Ehj1 which
corresponds to the j-th sample x(i). Therefore, we can bound
PI: X Ej 1 > 61 ≤P (SXI X apq (Xpxq-Expxq ))>s?
j=1	j=1 p,q:p6=q
≤ eχρ (-nst)E eχp (S X( X apq (χpxq- Expxq)
j=1 p,q:p6=q
where ① holds because of Chebyshev’s inequality. ② holds since x(i) are independent. ③
is established because of Lemma 12. We have ④ by optimizing s. Similarly, We can bound
P (1 Pj=I Eh2 > 6) as follows:
P (1X Eh 2 > 6) ≤ exp (-nst) YE exp
αpp ((xp)2 - E(xp)2)))
≤ eχp (-nst) Y eχp(l28τ4s2l2ω)	|s| ≤ τ2√ω
≤ eχp -c00n min
t2	t
ωl2τ4，√ωlτ2
Finally, since x(i) are independent sub-Gaussian, we can use Hoeffding inequality and obtain
P (1 XEh 3 > t j =P (1X (Xbp(Xj -Exp)! > t j
≤ eχp
(c000nt2 ∖
V ωι2τ2).
24
Published as a conference paper at ICLR 2018
Since S = t P(1 Pj=ι Eh 1 > t) = P(1 j Eh2 > t) = P(1 Pj=ι Eh3 > t)=。，
the above upper bounds also hold.
III:	Bound P(Eh > 2) By comparing the values of ω and ω0, We can obtain
1XX Eh 1 > "+p( n XX Eh 2 > 6)+p( 1XX Eh 3 >
j=1	j=1	j=1
t2	t
l2 max (ωτ2,ωτ4, ωqT2)，λ∕ωl max (τ, T2)
Where ωq = r2(l-2) .
≤3 exp -c02 n min
Step 2: Bound P(Eg > 2 To achieve our goal, for each input sample x(i), we also divide its
corresponding Pt,s(Gts - EGts) as Eh1, Eh2 and Eh3. Then We bound the three events separately.
Before that, we first give several equalities.
I.	Divide Gts - EGts : Dividing Gts - EGts is more easy than dividing Hts - EHts since the later
has more complex form. Since Gts = (Bs-i：iXxTBT1±J 0 (BTs+iBi：t+i). we also can write
the (i, k)-th entry Gitks as the form Gitks = Pp,q zpikqxpxq + Pp ypikxp + rik. But here ypik = 0.
Then similar to the step in dividing Hts - EHts, we can bound
a2pq ≤ ωgl2 and	a2pq ≤ ωgl2 where ωg = r4(l-1) .
p,q
II.	Bound P(Eg1 > t/6), P(Eg2 > t/6) and P(Eg3 > t/6): Since ypik = 0, P(Eh3 > t/6) = 0.
Similar to the above methods, we can bound
P 1X XX Eji
n	g1
j=1
> 6	≤ exp (-cl n
t2	t
ωgl2τ2，√ωglτ
and
P ( 1 X Eg 2
j=1
> 6 I ≤ exp (Yn
t
,√ω iτ2
III: Bound P(Eh > 2) We can obtain P(Eg > 2) as follows:
P
> 6 1+ P( 1 X Eg2 > !(+P ( 1 X Eg3 > 6
≤2 exp -c02n min
j=1
t2
j=1
t
max (T2,τ4), √ωgl max (τ,τ2)
Step 3: Bound P(E >t) Finally, we combine the above results and obtain
P (E > t) ≤P
+P
t2
≤5 exp -ch0 n min
t
max (ωg, ωgT2, ωh)，√ωgl max (τ, T2)
where ωg = r4(l-1) and ωh = r2(l-2).
□
C.2.5 Proof of Lemma 10
Proof. Before proving our conclusion, we first give an inequality:
Ilek2 = kBIix - yk2 ≤ kBiixk2 + 2 IyTBi：ix| + Ilyll2 ≤ rXωf + 2rxωf Ilyll2 + Ilyll2,
25
Published as a conference paper at ICLR 2018
where ωf = rl. Notice,① holds since by Lemma 14, We have ||Bi：ikF ≤ r2l.
Then we consider Nwf (w, x). Firstly, by Lemma6 we can bound ||Vw(j)f (w, x)12 as follows:
kVwj) f(w, χ)k2 = Il((Bj-i：ix)乳 BT∙+ι) e∣∣2 ≤ kBj-Lik2 Iixk2 kBij+ιk2 ιιek2
≤rXωfι (r2ωf + 2rχωfkyk2 + kyk2),
where ω% = r(l-i).① holds since we have kB1j+1kF kBj-i：ikF ≤ r2(l-1) by using kW(i)k2F =
kw(i) k22 ≤ r2. Therefore, we can further obtain
l
kVw f (w, x)k22 = X kVw(i) f (w, x)k22 ≤ lrx2ωf21 rx2ωf2 +2rxωfkyk2 + kyk22 .
i=1
Notice, y is the label of sample and the weight magnitude r is usually lager than 1. Then we have
kyk2 ≤ rl. Also, the values in input data are usually smaller than rl. Thus, we have
kVw f (w, x)k22 ≤ ctlrx4r4l-2 , αg,
where ct is a constant. Then we use the inequality ∣V2f (w, x)∣op ≤ ∣V2f (w, x)∣F to bound
∣V2f (w, x)∣op. Next we only need to give the upper bound of ∣V2f (w, x)∣F. Let ωf2 = rl-2.
We first consider Qst , Vw(s) Vw(t) f (w, x) . By Lemma 6, if s < t, we have
kQstkF = || (Bt-i:s+l)③(Bs-1:1 xeTBTt+l) + (Bs-1:1 XxTBT-1：1)③(BTs+iBl：t+i) ∣∣f
≤2 (|(Bt-1：s+1)0(Bs-1：1xeT8覆+1)∣∣F+1∣(Bs-1：IxxT吕二门)乳(BTs+1Bm1)∣∣F)
≤2 kBt-1：s+1kFkBs-1：IkFkxk2 kek2 kB31kF
+2kBs-1:1k2F kxk22 kxk22 kBt-1:1k2F kBl:s+1k2F kBl:t+1k2F
≤2ω22rX (r2ωf + rχωf kyk2 + kyk2) +2ω4ιr4,
where ① holds since we use kBi：t+1 kF kBt-1：s+1kF kBs-1：1kF ≤ ωf2 and kBs-1：IkFkBl：s+1 kF
≤ ωf2 . Note that when s ≥ t, the above inequality also holds. Similarly, consider the values in input
data and the values in label, we have
kQst k2F ≤ ct0rx4r4l-2 , αl,
where ct0 is a constant. Therefore, we can bound
ll
∣∣v2f(w,x)∣∣op ≤ ∣∣v2f(W,x)∣∣F ≤ t EEkQstkF ≤ l√ɑl.
s=1 t=1
On the other hand, if the activation functions are linear functions, f(w, x) is fourth order differentiable
when l ≥ 2. This means that VxV3wf(w, x) exists. Also since for any input x ∈ Bd0 (rx) and
w ∈ Ω, we can always find a UniVerSai constant a? such that
kV3w f (w, x)kop
SUp W, Vwf (w, x)) = X[Vwf (w, x)]ijkλiλjλk ≤ αp < +∞.
kλk2≤1	i,j,k
We complete the proofs.
□
C.2.6 Proof of Lemma 11
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. kw(j) k2 ≤ r.
Assume that w(j) has sj non-zero entries. Then we have Plj=1 sj = s. So here we separately assume
wj = {wj, ∙∙∙ , Wn j} is the djdj-1 e/d-covering net of the ball Bdjdj-1 (r) which corresponds
26
PUbHShed as a COnferenCe PaPer at ICLR 2018
-O -he Weighr 5(7) Ofrhe )—th Iayen Ler nmj be -he e、Z—covering number，By e—covering theory in
(VerShynm 222)“ We Can have
也 ≤ (4一 ) 串 ≤ exPτog (l⅞⅛⅛)) " expτog (Y)).
LetlrmQbeaIl arbitrary VeCtOr Since W H -Ir(I)" ••二 lυs- Where W-) is the Weight Ofthe J—th
yg we CaIl always find a VeCtOr WL in w~ SUCh that 一 一 W) — WL =2 ≤ dAj——一 c∕For brevity"
Iet" m medenote the hideX Of WL in c—net w3* TheIl Iet WkgU *•二 W二•二
ThiS means that We CaIl always find a VeCtOr Wkg SUCh that =Ir I WkG =2 ≤ 介 NOW We USe the
decomposition Strategy to bound OUr goah
<2 j-ɪl <2 J？)
OP
-L
IM<jsas) — E(<jsa))
3 E OP
1 S
31M (V2 j(e Hs) — WFe
E
1 3
R≡) +3|》<二§三«0)—同<2二§2包)
Hl
Λ
+H(V2 j?Ka)) — IE(V2 j(£ 包)
OP
M (V2 j(e) — <2 )邕&))
2=1
+
OP
sl∑s<2 j(SE^m)) — IE(V-(SE 包)
2=1
2
+ 1E(V2 —IE(V2 j(£ 包)
OP
Here We also define four events E9E-EN and E⅛ as
Eo
El
≥ -
OP —
— V-(SE^v)
slMV2j(S©a3) —IE(V2j(sE 包)
2=1
.E⅛ = ( SUP=IE<2j (SEaJ)) —E(<j?a))rIvQ
Accordingly We have
亏(E。) ≤ 亏(El) +亏(e2)+亏(E3)∙
SO We CaIl respectiveIy bound IP (ElIP (E2) and IP (E3) to bound IP (Eo)∙
27
V I
— 3
OP
Published as a conference paper at ICLR 2018
Step 1. Bound P (E1): We first bound P (E1) as follows:
P (E1 ) =P sup
w∈Ω
1n
n E (v2 f(w, x(i))- v2f (wkw, x(i)))
n i=1
≥ I
2
①3
≤-E sup
t	w∈Ω
1n
n E(V2 f(w, x(i))- V2f (wkw, x(i)))
i=1
2
≤3E (sup ∣∣V2f(w, x) - V2f(Wkw, x)∣∣2
t	∖w∈Ω
3
≤-E sup
t	w∈Ω
l n Pn=I(V2f (w, x(i)) - V2f(Wkw, x(i)
kW-Wkwk2
) sup kW-Wkwk2
w∈Ω
② 3ape
≤十
where ① holds since by Markov inequality and ② holds because of Lemma 10.
Therefore, we can set
t ≥ ∙⅛
ε
Then we can bound P(E1):
p(Eι) ≤ ε.
Step 2. Bound P (E2): By Lemma 2, we know that for any matrix X ∈ Rd×d, its operator norm
can be computed as
IIXIIoP ≤ ：；—/ SUP lhλ,χλil.
1 - 2 λ∈λ
where λ = {λ1, . . . , λkw} be an -covering net of Bd(1).
Let λ1∕4 be the ɪ-covering net of Bd(1) but it has only S nonzero entries. So the size of its e-net is
d
s
s
≤ exp (s log (12d)) .
Recall that we use jw to denote the index of Wkj in -net Wj and we have jw ∈ [nj], (nj
exp (Sjlog (3rd))). Then we can bound P (E2) as follows:
P (E2) =P sup
jw ∈[nj ] j∈[l]
1n
n Ev2f (Wkw, x(i))- E(V2f (Wkw, x))
n i=1
≤P sup
IjW ∈[nj] j∈[l],λ∈λl/4
t
≥ —
-3
2
(λ, (1 X V2f (Wkw , x(i)) - E (V2 f (Wkw , x))) λ: ≥
l
≤ exp (s log (12d)) exp	sj log
j=1
sup P
jw ∈[nj] j∈ [l],λ∈λ1 /4
(V2f (Wkw, XCi))-E (V2f (Wkw, X)))，卜
≤ exp (S log (36rd-)) 10exp (-Ch0n min (36τ2l2πiax
e	36T max
where ① holds since by Lemma 9, We have
t2
P ŋɪ X(3 (Vw f(w, X)- EVw f(w, x))λ>)
>t
t2
1
n
(ωg,ωgτ2,ωh), 6√ωgl max (τ, τ2)
≤ 10exp (-ch0 n min (T 2/2 max(ωg ,3g T"，Fg l max(τ,τ 2)
≤
2
t
3
t
t
28
Published as a conference paper at ICLR 2018
where ωg = r4(l-1) and ωh = r2(l-2) .
Let de = S log(36d2r∕e)+log(20∕ε). Thus, if we set
then we have
t ≥ max
36τ2l2 max
ch0n
6√ωgl max (τ, T2) de
ch0 n
ε
P(E2) ≤ 2.
Step 3. Bound P (E3): We first bound P (E3) as follows:
P (E3) =P supE(N2f(wkw,x))-E(N2f(w,x))2 ≥
∖w∈Ω
≤P (E sup ∣∣(V2f (wkw, x) - N2f (w, x)∣∣2 ≥ t-∖
W w∈Ω	3)
≤P sup
∖ w∈Ω
11 Pn=I (V2f(w, X(Q-N2f(wkw, X(i)))∣
kw- wkw k2
sup kw — WkWk2 ≥ t
where ① holds because of Lemma 10. We set e enough small such that a?e < t/3 always holds.
Then it yields P (E3) = 0.
Step 4. Final result: For brevity, let ω2 = 36τ2l2 max (ω3, ωgT2, ωh) and ω3 = 6√ωgl max(τ, T2).
To ensure P(E0) ≤ ε, we just set e = 36rl/n and
t ≥ max (乎,3αpe, J
ω2(s log(36d2r∕e)+log(20∕ε)) ω3(s log(36d2r∕e)+log(20∕ε))
ch0 n
ch0 n
216αpr
max	, ↑ ■
nε
ω2(slog(d2nl)+log(20∕ε)) ω3(slog(36d2n∕l)+log(20∕ε))
ch0n
ch0 n
Thus, there exit two universal constants ch1
s log(d/l)/(lT 2)), then
αr
and ch2 such that if n ≥ ch? max(τ2l2ω2εps log(d∕l),
WuΩ∣∣VJn (W)-V2J (w)∣∣op ≤ Chιτlωh ∕⅛+0g≡
holds with probability at least 1 - ε, where ωh = max T r2(l-1), r2(l-2), rl-2 . The proof is
completed.	□
C.3 Proofs of Main Theorems
C.3.1 Proof of Theorem 1
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. kw(j) k2 ≤ r.
Assume that w(j) has sj non-zero entries. Then we have Plj=1sj = s. So here we separately assume
Wj = {wj, ∙∙∙ , Wn j} is the djdj-1 e/d-covering net of the ball Bdjdj-1 (r) which corresponds
to the weight w(j ) of the j-th layer. Let nej be the e/l-covering number. By e-covering theory in
(Vershynin, 2012), we can have
nej ≤
djdj-1
sj
dj^j‰) sj ≤exp 卜 jlog (S⅛⅛))=exp Ilog(3rd)).
t
3
29
OE
^(⅛ > (?Z ∕)d[gqι"
OjqBTJBA UloPURl 9M1B§9UUOU AjBJJTqjB UB JOJ lBψ 9ΛBq 9M ζAl∏B∏b9UT Λ05[JBJV Aq əɔuɪs SPloq ① 9J9qM
iQ∣(≈^)V,Δ∣∣ dns^)aA>
研
I53(n
B∣fm — aι∖∖ dns
e∣∣((ω≈-M∕∆-(ω≈^)∕∆) τ^Zτll
ʊə«ɪʌ ?_
dns 3[->
ζ
E _
-<
?
1=?
((ω≈ιm^)∕∆ - ((?)^ 1^)∕∆) W γ
U L
ʊə«ɪʌ ?_
dns 3[->
/ Eφ
1=2	、 ∖
:；u ʊəni ∖
(((f)ʃ ιm¾aι)∕∆ - ((f)ʃ 1aι)∕∆) 2ɪ - dns j J= (Imdl
u ɪ	/
:sMonoJ SB (Im j punoqISJg 啾：。豆)j PlmoH 'l ⅛JS
'(0a^) d P≡oq OJ (εa^) d PUe (¾) d '(团 d PUnOq XwAHgdsat Ueo əm OS
` (εa^)d + (¾)d + (τa^)d > (0a^)d
5Λ∏q əm ζAjSuτpJooov
ζ
E -
-<
?
I53(n
((ʃ 1aι)∕∆)a - ((ʃιmM∕∆)a dns
((a√ m¾ι)∕2χ)ai ―八力廿 m¾ι)∕2χ R 不
U L
ζ
E -
-<
?
[∣] = Ci[eu]3^C
dns
1=?
(((f)ajιm¾aι)∕∆ —八讪⑹/△) Rl
u ɪ
dns }二
1 O < l(^)rʌ - (^)ur∆∣∣ dns I = O3-
sb ¾ PUB	Tq '。@ sjuəʌə jnoj əugəp osɪŋ əM əjəH
ζ
■ ((ʃ 1^)∕∆)a - ((ʃ ιmM∕∆)a +
((ajιm¾aι)∕∆)a-	+
11. ɪ
《⑶a√m¾ι)∕a -	1aι)∕∆)
>
Z
I -
¾
%
((ʃ 1^)∕∆)a - ((≈ιmM∕∆)a +
1=?
1=?
((ajιm¾aι)∕∆)a-	+《⑶a√n¾n)/4 -	M -
u ɪ	u ɪ
((a!1aι)∕∆)a-
U L
(^)rʌ - 3)“ρx
sb ∣∣(^)ΓΔ - (i∏)V∆∣∣ əsodmoɔəp
uŋɔ əm ζAjSuτpJooov ɔ > ζ∣∣ aι"m — aι∣∣ IBm qons cn^aι jojɔəʌ B PUU sAbmjb ubə əm IBm suŋəm
sτqχ ∙[z^aι:…F}aι ⅛ ∙ ∙ ∙ ⅛ τ%m] = Rm jəɪ uaqχ jəu-ə u∖ ‘%n Jo xapuτ aqι əjouəp [f9u] m cn, iəɪ
ζXjτΛ0jq jo^ ^p>TpCp >	- (Qmil IBm qons jcn uτ f^aι jojɔəʌ B PUU sAbmjb ubə əm ζjgAπj
qi-, Qqj jo 叫次əM sqj st(，)m əjəgʌv [”)mt ∙ ∙ ∙ “[)m] = aι əɔuɪs -jojɔəʌ XiEJjτqiE ub əg ʊ ə aι jə^
8【0C mɔl JB jədŋd əɔuəjəjuoɔ B sb P叫SUqnd
Published as a conference paper at ICLR 2018
Now We only need to bound E (suPw∈Ω (▽ Jn(w, x)∣∣ ). NoW We utilize Lemma 10 to achieve
this goal:
E(wUΩ∣∣VJnMxS ≤= e(
where αl = ct0rx4r4l-2. Therefore, we have
SUP ∣∣V2f(w,x) - v2∕(w*,x)∣∣2 ) ≤ 1√ɑ1.
.w∈Ω
P (Ei) ≤ 千
We further let
t ≥ 61√⅞e
ε
Then we can bound P(E1):
ε
P(Ei) ≤ 2∙∙
Step 2. Bound P (E2): By Lemma 1, we know that for any vector X ∈ Rd, its '2-norm can be
computed as
ι∣χ∣∣2 ≤ i-e sup hλ, Xi.
where λ = {λ1, . . . , λkw} be an e-covering net of Bd(1).
Let λ∖∕2 be the ɪ-covering net of Bd(1) but it has only S nonzero entries. So the size of its e-net is
d
s
s
≤ exp (s log (6d)) .
Recall that We use jw	to denote the index of Wkj	in	e-net Wj	and We have	jw	∈	[nj],	(nj ≤
exp (Sjlog (3rd))). Then We can bound P (E2) as follows:
P (E2) =P	sup
jw ∈[nej],j=[l]
1n
n EVf(Wkw, XCi))- E(Vf(Wkw, X))
i=1
=P	sup
jw ∈[nj],j=[l],λ∈λ1/
t
≥ —
_ 3
2
(λ,1 X Vf(Wkw, x(i)) - E (Vf(Wkw, X)); ≥ 3∙)
≤ exp (s log (6d)) exp	sj log
j=1
sup PG X (λ,
jw∈ [nej ],j=[l],λ∈λ1∕2	∖n i=1 ∖
2
2
l
Vf(Wkw,…E (Vf(Wkw, G ≥ t)
①	18rd
≤ exp I s log I —
t2
6eχpI cg0nmin 361 maχ(ωgτ2,ωgT4,ωg0τ2), 6piωgmax (τ,τ2)
where ① holds since by Lemma 8, we have
P(1 X (<λ, Vwf(w, X(i)) -EVwf(w, X(i))〉) >t
t2
≤ 3expy cg0nmin 1 max (ωgτ2,ωgT4f τ2), pωTg max(τ,τ2)
where c§，is a constant; ωg = Cqr2(2l-i) and ωg0 = Cqr2(l-i) in which Cq = ,maxo≤i≤ι di.
Let ω2 = 361 max (ωgT2,ωgT4,ωg0T2) and ω3 = 6,1ωg max(τ, τ2). Thus, if we set
t ≥ max
ω2(s log(18dr∕e)+log(12∕ε)) ω3(s log(18dr∕e)+log(12∕ε))
cg0n
cg0 n
t
t
»
,
31
Published as a conference paper at ICLR 2018
then we have
pg ≤ ∣.
Step 3. Bound P (E3): We first bound P (E3) as follows:
P (E3) =P (SUp kE(f (Wkw, X))- E(f (w, x))k2 ≥ t
∖w∈Ω	3
P	kE (f(wkw, X)- f(w, x)k2)
—P I SUp
∖w∈Ω
SUp kw - Wkwk2 ≥ I
w∈Ω	3
kw-wkwk2
≤p GE WudVJnM x"L≥ I)
≤p (l√ale ≥ 3).
We set e enough small such that l√αle < t/3 always holds. Then it yields P (E3) = 0.
Step 4. Final result: Finally, to ensure P(E0) ≤ ε, we just set e = 18lr∕n and
t ≥ max
e,
max (108l2√αir, J
nε
ω2(s log(18dr∕e)+log(12∕ε)) ω3(s log(18dr∕e)+log(12∕ε))
cg0 n
cg0 n
ω2(s log(dn∕l)+log(12∕ε)) ω3(s log(dn∕l)+log(12∕ε))
cg0 n
cg0 n
Notice, we have αl = ct0 rx4r4l-2 where ct0 is a constant. Therefore, there exists two universal
constants cg and cg0 such that n ≥ cg0 max(
l3r2rX
CqS log(d∕l)ε2τ4 log(1∕ε)
, s log(d∕l)∕(lτ 2)), then
sup IVJn(W)-VJ (w)∣∣2 ≤ Cg T3g pιcq ↑Js log(dn/l)^logθj/f)
holds with probability at least 1 - ε, where ωg = max τ r2l-1, r2l-1, rl-1
□
C.3.2 Proof of Theorem 2
Proof. Suppose that {w(1), w(2),…，w(m)} are the non-degenerate critical points of J(w). So for
any W(k), it obeys
inifλik V2J(W(k))	≥ζ,
where λik V2J (W(k)) denotes the i-th eigenvalue of the Hessian V2J (W(k)) and ζ is a constant. We
further define a set D — {w ∈ Rd | kVJ(w)k2 ≤ e and infi λ (V2 J(w(k))) | ≥ Z}. According
to Lemma 4, D = ∪k∞=1Dk where each Dk is a disjoint component with W(k) ∈ Dk for k ≤ m and
Dk does not contain any critical point of J(W) for k ≥ m + 1. On the other hand, by the continuity
of VJ(W), it yields kVJ (W)k2 = e for W ∈ ∂Dk. Notice, we set the value of e blow which is
actually a function related to n.
Then by utilizing Theorem 1, we let sample number n sufficient large such that
sup ∣∣VJn(W)- VJ(w)∣L ≤ Zg，e
holds with probability at least 1 一 ε, where if n ≥ c§，max( CqS 丁5/：；^ ιog(i∕ε)，S '。即"), Zg —
「 丁，. ∏-q IS log(dn/l)+log(12为
Cg τ3g √ lcq V------n--------.
32
Published as a conference paper at ICLR 2018
This further gives that for arbitrary w ∈ Dk , we have
inf KvJn(w) + (1 - t)VJ(w)∣∣2 = inf ∣∣t ZJn(W) -VJ(W)) + VJ(w)∣∣2
≥ inf ∣∣VJ(w)k2 - sup t∣∣vJn(w)-VJ(W)∣∣
w∈Dk	w∈Dk	2
≥Q	(20)
Similarly, by utilizing Lemma 11, let n be sufficient large such that
WudIV2 Jn(W)-V2j (W)UoP ≤ Zz ≤ 2
22
holds with probability at least 1 - ε, where if n ≥	max(产理/εPs iog(d/i), s lοg(d∕l)∕(lτ2)),
7—人 q-l, . L q 卜 log(nl)+log(20/S)
zs - chi τlωhy-------n----------.
Assume that b ∈ Rd is a vector and satisfies bτb = 1. In this case, We can bound λk(V2 J∕w))
for arbitrary W ∈ Dk as follows:
WinD Jλk (V2 Jn(W 川=WinD k ,mnWVJn(W)b
=inf min IbT(V2 Jn(W) — V2 J(w)) b + bTV2J(W)b∣
W∈Dk bT b=1
≥ inf	min	∣∣bTV2J(W)b∣∣	- min ∣bτ(V2 Jn(W) — V2J(w))	b∣
≥ inf	min	∣∣bTV2J(W)b∣∣	- max ∣bτ	(V2Jn(W) -V2J(w))	b∣
=WinDJnf lλk (V2f (W(k), X)) | - ∣∣v2 Jn(W)-V2 J(W)IIoP
≥ζ. fc
-2
This means that in each set Dk, V2Jn(W) has no zero eigenvalues. Then, combine this and Eqn. (20),
by Lemma 3 we know that if the population risk J(W) has no critical point in Dk, then the empirical
risk Jn(W) has also no critical point in Dk; otherwise it also holds. By Lemma 3, we can also obtain
that in Dk, if J(W) has a unique critical point W(k) with non-degenerate index sk, then Jn(W) also
has a unique critical point W(nk) in Dk with the same non-degenerate index sk. The first conclusion is
proved.
Now we bound the distance between the corresponding critical points of J(W) and Jn (W). Assume
that in Dk, J(w) has a unique critical point W(k) and Jn(W) also has a unique critical point Wnk).
Then, there exists t ∈ [0, 1] such that for any z ∈ ∂Bd(1), we have
≥∣VJ(Wn(k))∣2
= max hVJ (Wn(k)), zi
zTz=1
max hVJ(W(k)), zi + hV2J (W(k) + t(Wn(k) -W(k)))(Wn(k) - W(k)), zi
zTz=1
①
≥
2	1/2
V2J(W(k))	(Wn(k) - W(k)), (Wn(k) - W(k))
≥ζ ∣∣wnk)- w(k)∣2
where ① holds since VJ(w(k)) = 0 and ② holds since W(k) + t(Wn(k) - W(k)) is in Dk and for any
w ∈ Dk we have infi ∣λi (V2J(W)) | ≥ Z. Consider the conditions in Lemma 11 and Theorem 1,
we can obtain that if n ≥ ch max( C S 卜虱d∕3)srT4 iog(i∕s), S log(d∕l)∕Z2) where ch is a constant, then
∣Wn(k) - W(k) ∣2 ≤
2cg τωg r— /s log(dn∕l)+log(12∕ε)
~^cqM------------------n-----------
33
Published as a conference paper at ICLR 2018
holds with probability at least 1 - ε.
□
C.3.3 Proof of Theorem 3
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. kw(j) k2 ≤ r.
Assume that w(j) has sj non-zero entries. Then we have Plj=1 sj = s. So here we separately assume
Wj = {wj, ∙∙∙ , Wn j} is the djdj-ιe/d-covering net of the ball Bdjdj-1 (r) which corresponds
to the weight w(j) of the j-th layer. Let nj be the /l-covering number. By -covering theory in
(Vershynin, 2012), we can have
nj ≤( dj2-1)( j/d y ≤ exp 卜，log( S⅛⅛))=exp Ilog( 3rd)).
Let w ∈ Ω be an arbitrary vector. Since W = [w(i), •一，w(i)] where w(j)is the weight of the j-th
layer, we can always find a vector Wkj in Wj such that kW(j) - Wkj k2 ≤ djdj-1e/d. For brevity,
let jw ∈ [nej] denote the index of Wj in e-net wj. Then let Wkw = [wj;… ;wj;…;WjJ.
This means that we can always find a vector Wkw such that kW - Wkw k2 ≤ e. Now we use the
decomposition strategy to bound our goal:
1n
Jn(W)- J (W)I= - X f(W, X(i)) - E(f(W, x))
n i=1
1n	1n
n E(f (W, x(i))-f (Wkw, x(i)))+n∑f (Wkw
i=1
i=1
x(i))-Ef(Wkw, x)+Ef(Wkw, x)-Ef(W, x)
n
n
I1n	II1n	II	I
≤ nE(f(W, X(i))-f(Wkw, X(i))) + nɪf (Wjw, X(i))-Ef (Wjw，x) + Ef(Wkw, X)-Ef (w, x) ∙
i=1
i=1
Then, we define four events E0, E1, E2 and E3 as
Eθ = I SUP IJn(W)- J(W)| ≥ t},
(
(
(
E1
sup
w∈Ω
1n
一E f(w, x(i)) - f (Wkw, x(i)))
n i=1
≥ t},
E2
sup
jw ∈[nj],j=[l]
1n
nɪf (Wkw, x(i))-E(f (Wkw, x))
i=1
≥ l),
E3
SUp E(f(Wkw, X))-E(f (w, x)) ≥ t
w∈Ω
.
Accordingly, we have
P(E0) ≤P(E1)+P(E2)+P(E3).
So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).
Step 1. Bound P (E1): We first bound P (E1) as follows:
P(EI)=P wUΩ1 XfwX(i))-f(Wkw，W" ≥ t
≤ 3E (SUp1 X(f(w, x(i))- f (Wkw, x(i)))∣)
t	∖w∈ω n M	i√
,3κ (	∣1 Pn=I (f(w, x(i)) - f (Wkw, x(i)))∣	ll	ll
≤ 了E sup J-----------II”,二,-∏---------kz-k sup kw - wkwk2
t	∖w∈Ω	k w Wkw k2	w∈Ω
≤ 3te E (wud VJn(W，x”2
34
Published as a conference paper at ICLR 2018
where ① holds since by Markov inequality, We have that for an arbitrary nonnegative random variable
x, then
P(X ≥ t) ≤ Ex).
Now we only need to bound E 卜uPw∈ω ,Jn(w, x)∣∣ ). Therefore, by Lemma 10, we have
E(SuP ∣∣Vefn(w, x)∣∣ )= EI sup
∖w∈Ω	2J	∖ w∈Ω
1n
n SVf(W, W)
=E(SUPkVf(w, x)k2 ≤√αg.
∖w∈Ω
where αg = ctlrx4r4l-2 . Therefore, we have
P(Ei) ≤ 3^√αg.
We further let
6e√αg
t ≥ -------.
ε
Then we can bound P(E1):
P(Ei) ≤ ε.
Step 2. Bound P (E2): Recall that we use jw to denote the index of Wkj in -net Wj and we have
jw ∈ [nej], (nej ≤ exp (Sjlog (等))).We can bound P (E2) as follows:
P (E2 ) =P	sup
jw ∈[nj] j∈[l]
l
1n
n E f(wkw, x(i))- Ef(Wkw, X))
n i=i
≤ exp	sj log
j=i
suP	P ( I 1 S f(wkw, Xei))- Ef(Wkw, X))
jw∈[nj]j∈[l]	ni=i
≥ 3
≥ t
①	3dr s
≤4 I --- I exp I -Cf0n mm
t2
9ωf2 max dlωf2τ4,τ2
where ① holds because in Lemma 7, we have
P (1 S(f(w, X(i))-E(f (w, X(i))))〉) ≤ 2exp (-Cf0nmin
t2
max dlωf2τ4 , τ2
2T2
where cf0 is a positive constant and ωf = rl. Thus, if we set
t ≥ max
9ω2(s log(3rd∕e) + log(8∕ε)) max (d?ωf τ4, τ2
Cf 0n
3ω2τ2(s log(3rd∕e) + log(8∕ε))
Cf 0n
then we have
P (E2) ≤ ε.
Step 3. Bound P (E3): We first bound P (E3) as follows:
P(E3)=P (sup kE(f (Wkw , X))- E(f (w, X)) k2 ≥ ŋ
∖w∈Ω	3)
=P
sup
∖w∈Ω
kE (f(wkw, X)— f (w, X)k2)
kw - Wkwk2
sup kW - Wkwk2 ≥ I
w∈Ω	3
t
∖
≤P (eE sup IlVJw(w, x)∣∣2 ≥ :
∖	w∈Ω	3
①
≤P
35
Published as a conference paper at ICLR 2018
where ① holds since We utilize Lemma 10. We set e enough small such that √√αge < t/3 always
holds. Then it yields P (E3) = 0.
Step 4. Final result: To ensure P(Eo) ≤ ε, we just set e = 3rl∕n. Note that &√αg > 3√αge. Thus
we can obtain
t ≥max(6√αge, t
9ω2(s log(3rd∕e)+log(8∕ε))max gιω2 T 4,τ 2) 3ω2 τ 2(s log(3rd∕e)+log(8∕ε))
cf0n
cf0n
max
/
18l√αgr
9ω2(s log(dn∕l)+log(8∕ε))max gιω2τ 4,τ 2) 3ω2τ2(s log(dn∕l)+log(8∕ε))
nε
∖
cf0n
cf0n
,
Note that we have αg = Ct lrx4 r4l-2 where Ct is a constant. Then Then there exist four universal
constants Cf and Cf 0 such that if n ≥ cf max (^s 1.5；炉4 ιog(i∕ε), S log(d)∕(τ2 d)) ,then
sup Jn(W) — J(w)U ≤ CfωfTmax (pdlω∕τ,
holds with probability at least 1 - ε.
s log(dn∕l) + log(8∕ε)
n
□
C.3.4 Proof of Corollary 1
Proof. By Lemma 5, we know es = eg. Thus, the remaining work is to bound es . Actually, we can
have
≤Es〜D I SUp Jn(W) — J(W)
w∈Ω
≤ sup Jn (W) — J(W)
w∈Ω
≤ef.
Thus, we have g = s ≤ f. The proof is completed.
□
C.4 Proof of Other Lemmas
C.4.1 Proof of Lemma 13
Lemma 15. (Rigollet, 2015) Suppose a random variable x is τ 2 -sub-Gaussian, then the random
variable x2 - Ex2 is sub-exponential and obeys:
E (exp λ (x2 - Ex2)) ≤ exp
256λ2τ4	1
), lλl≤ 而.
(21)
Proof. Here we utilize Lemma 15 to prove our conclusion. We have
E exp
ai* - E (XaiX2))) = YEexp (λai (x2 - Ex2))
②k-r
≤ ∏Eexp (l28λ2a2Ti4) ,	∣λ∣
i=1
1
≤ maxΓαiτ2
≤E exp 128λ2 τ 4 X ai2
where ① holds since Xi are independent and ② holds because of Lemma 15.
□
36
Published as a conference paper at ICLR 2018
C.4.2 Proof of Lemma 14
Proof. Since the '2-norm of each w(j)is bounded, i.e. kw(j)∣∣2 ≤ r (1 ≤ j ≤ l), We can obtain
kBs：tkF ≤∣∣W (SRFlIW (ST)IIF∙TW 叫 ≤ r2(t-s+I)，ω ≤max (r2, r2l),
where ① holds since the function r2x obtains its maximum at two endpoints X = 1 and X = l for case
r < 1 and r ≥ 1, respectively. On the other hand, we have ∣BsMbp ≤ kBsMIf ≤ ωr. Specifically,
we have ||Bi：i|F ≤ r2l，ωf.	□
D Proofs for Deep Nonlinear Neural Networks
In this section, we first present the technical lemmas in Sec. D.1. Then in Sec. D.2 we give the proofs
of these lemmas. Next, we utilize these technical lemmas to prove the results in Theorems 4 〜6 and
Corollary 2 in Sec. D.3. Finally, we give the proofs of other lemmas in Sec. D.4.
D.1 Technical Lemmas
Here we present the key lemmas and theorems for proving our desired results. For brevity, we define
an operation G which maps an arbitrary vector z ∈ Rk into a diagonal matrix G(z) ∈ Rk×k with its
i-th diagonal entry equal to σ(zi)(1 - σ(zi)) in which zi denotes the i-th entry of z. We further
define Ai ∈ Rdi-1×di as follows:
Ai = (W(i))TG(u(i)) ∈ Rdi—1 ×di	(i = 1,…，l),	(22)
where W(i) is the weight matrix in the i-th layer and u(i) is the linear output of the i-th layer. In this
section, we define
BS：t = AsAs+1 …At ∈ Rdsτ×dt, (s ≤ t) and BS：t = I, (s>t).	(23)
Lemma 16. Suppose that the activation function in deep neural network are sigmoid functions. Then
the gradient off(w, x) with respect to w(j) can be formulated as
Vw(j) f(w, X) = Vec ((G(Uj))Bj+1:1(V(I)- y" (v(jT))T) , (j = 1,…，l - 1),
and
Vw(0 f(w, x) = Vec ((G(U(I))(V(I)- y)) (V(IT))t).
Besides, the loss f(w, x) is α-Lipschitz,
∣∣Vwf(w, x)∣∣2 ≤ α,
where α =《飞CyCd (1 + Cr (l 一 1)) in which Cy, Cd and Cr are defined as
r2	r2 l-1
kv(I)- yk2 ≤ Cy < +∞, cd = max(d0, d1,…,dl ) and cr=max 16, 16
Lemma 17. Suppose that the activation functions in deep neural network are sigmoid functions.
Then there exists two universal constants cS1 and cS2 such that
Wwf(W,x)||OP ≤ Wwf(w,x)||F ≤
where ς =，c、1Cr Cd l4 in which Cd = maxi di and Cr = max
gradient Vwf (w, x) is ς-Lipschitz, i.e.
l-1
. Moreover, the
∣Vwf(w1,x) - Vwf(w2,x)∣2 ≤ς∣w1 - w2∣2.
Similarly, there also exist a universal constant ξ such that
IIV3w f (w, x)IIop ≤ IIV3w f (w, x)IIF ≤ ξ.
37
Published as a conference paper at ICLR 2018
Lemma 18. Suppose that the activation function in deep neural network are sigmoid functions. Then
we have
l∣VwVχf(w,x)kop ≤ ∣∣VwVχf(w,x)∣∣F ≤β,
where β
2828l(l + 2)cyCrCd (lcr + 1) in which cy, Cd and Cr are defined in Lemma 16.
Lemma 19. Suppose that the input sample x obeys Assumption 2 and the activation functions in deep
neural network are sigmoid functions. The gradient of the loss is 8β2 τ2 -sub-Gaussian. Specifically,
for any λ ∈ Rd, we have
E (hλ, Vw f (w, x) - EVw f (w, x)i) ≤ exp
8β2τ2∣∣λ∣∣∣
2
where β
∣8328l(l + 2)cyCrCd (lcr + 1) in which Cy, Cd and Cr are defined in Lemma 16.
Lemma 20. Suppose that the input sample x obeys Assumption 2 and the activation functions in
deep neural network are sigmoid functions. The Hessian of the loss, evaluated on a unit vector, is
sub-Gaussian. Specifically, for any unit λ ∈ Sd-1 (i.e. lλl2 = 1), there exist universal constant γ
such that
E (t〈入,(Vwf (w, X)- EVwf (w, x)) λ>) ≤ exp
(彳)
Notice, γobeys γ≥ lVxV2wf (w, x)lop.
Lemma 21. Assume that the input sample x obeys Assumption 2 and the activation functions in
deep neural network are sigmoid functions. Then the sample Hessian uniformly converges to the
population Hessian in operator norm. That is, there exists such two universal constants Cm0 and Cm
c 0ξ2l2r2
Such that if n ≥ γ2τ2ε2m°g(d)log(1∕ε) ,then
MVIJn(W)-V2J(w)∣∣ opM Syl≡≡
holds with probability at least 1 - ε. Here γis the same parameter in Lemma 20.
D.2 Proofs of Technical Lemmas
For brevity, we also define
Ds:t = kW(S)IIF …kW(t)kF (S ≤ t) and Ds：t = 1, (s>t).
We define a matrix Pk ∈ Rdk×dk whose ((S — 1)dk + s,s) (s = 1,…，dk) entry equal to
σ(u(sk))(1 - σ(u(sk)))(1 - 2σ(u(sk))) and rest entries are all 0. On the other hand, since the values in
v(l) belong to the range [0, 1] and y is the label, kv(l) - yk22 can be bounded:
kv(l) - y k22 ≤ Cy < +∞,
where Cy is a universal constant. We further define Cd = max(d0, di,…，d().
Then we give a lemma to summarize the properties of G(u(i)) defined in Eqn. (22), Bs:t defined in
Eqn. (23), Ds:t and Pk.
Lemma 22. For G(u(i)) defined in Eqn. (22), Bs:t defined in Eqn. (23), Ds:t and Pk, we have the
following properties:
(1)	For arbitrary matrices M and N of proper sizes, we have
∣G(u(i))MkF ≤ ɪIMkF and ∣NG(u(i))kF ≤ ɪ∣NkF∙
16	16
(2)	For arbitrary matrices M and N of proper sizes, we have
26	26
kPkM kF ≤ 38 kM kF and kNPk kF ≤ 38 kN kF ∙
38
Published as a conference paper at ICLR 2018
(3)	For arbitrary matrices M and N of proper sizes, we have
21	1
kBs：tkF ≤ 16t-s + ι Dst and	16t-s+1 Dst ≤ Cst ≤ Cr,
where Cst = (4)2(t s+1) and Cr = max
(4)	For arbitrary matrices M, N and I of proper sizes, let m = vec CM). Then we have
k(N 乳 I )mkF ≤ IMkF kN kF and k(I 乳 N)m∣∣F ≤ IMkF kN kF .
It should be pointed out that we defer the proof of Lemma 22 to Sec. D.4.
D.2.1 Proof of Lemma 16
Proof. We use chain rule to compute the gradient of fCw, x) with respect to w(j). We first compute
several basis gradient. According to the relationship between u(j), v(j), W(j) and fCw, x), we have
Vv(l) f Cw, x) = v(l) - y,
Vv(i)f(w, x)
Vu(i) f (w, x)
VW(i)f(w,x)
∂u(i+1) ∂f(w, x)
∂v(i)	∂u(i+1)
∂v(i) ∂f(w, x)
∂u(i) ∂ v(i)
∂u(i) ∂f Cw, x)
∂w(i)	∂ u(i)
=(W (i+1))T f+滞,
∂ u(i+1)
G(u(i)) df⅛2,
∂v (i)
y=V(T( 1)T
(i	=1, ∙ ∙	• ,l - 1),
(i=	=1, ∙ ∙	∙ ,1),
(i	=1, 一	•• ,l).
(24)
Then by chain rule, we can easily compute the gradient of fCw, x) with respect to w(j) which is
formulated as
Vw(j)f(w, x) = Vec ( v(j-1)(G(u(j))Aj+iAj+2 …Aι(v(l) -
Cj = 1,…，l — 1),
and
Vw(l) f Cw, x) = vec v(l-1) GCu(l))Cv(l) - y)	.
Besides, since the values in v(l) belong to the range [0, 1]. Combine with Lemma 22, we can bound
kVw f Cw, x)k2 as follows:
l
kVw f Cw, x)k22 = X Vw(j) f Cw, x)22
j=1
v(l-1) GCu(l))Cv(l) - y)
2	l-1
+X
F	j=1
v(j-1) G(u(j))Bj+1:l (v(l) -
T
2
F
≤ 116 dl-1 W)- y[+16 W)- y∣∣2 X dj-1 kBj+1:IkF
2	2 j=1
①1	1
≤ 16Cy cd + 16Cy Cdcr (l - 1),
where cy , cd, cr are defined as
r2 r2 l-1
kv(I)- yk2 ≤ cy, cd=max(d0, d1, …, dl) and cr=maχ 16 , 16
Notice，① holds since in Lemma 22, We have
2 r 2(t-s+1)	r2	r2 l-1
kBs"kF≤ (4)	≤ max 16, (16)
39
Published as a conference paper at ICLR 2018
Thus, we can obtain
∣∣Vwf (w,x)k2 ≤ 4ACyCd (1 + Cr(l - 1)) ，α.
The proof is completed.	□
D.2.2 Proof of Lemma 17
For convenience, we first give the computation of some gradients.
Lemma 23. Assume the activation functions in deep neural network are sigmoid functions. Then the
following properties hold:
f f ) f(w f(w f(wιxi) f( ff(w OyCaiPHtq "f (w,X) CKf "f (w,X) ∩
(J-) e can ComiPUte tιιe graents ∂u(i) anu ∂v(i) as
fwfi = G(u(i))Bi+i：i(v(l)- y) and fwx) = Bi+i：i (v(l)- y).
∂ui)	∂v(i)
(2)	We can compute the gradient 照⑸ as
∂w(j)
∂W(^ = (VjT))T X (G(Uj))Bj+u(W⑴)T)T ∈ Rdi×djdj-1, (i>j).
∂Wi) = (V(I))t X Idi ∈ Rdi×didi-1, (i = j).
(i)
(3)	We can compute the gradient ∂∂v.	as
∂w(j)
(i)	T
∂w^ = (VjT))T X (G(Uj))Bj+Li) ∈ Rdi×djdj-1, (i ≥ j).
It should be pointed out that the proof of Lemma 23 can be founded Sec. D.4.
Proof. To prove our conclusion, we have two steps: computing the Hessian and bounding its operation
norm.
Step 1. Compute the Hessian: We first consider the computation of f T(W,x):
∂w(Ti) ∂w(j)
∂2f (w, x)	∂ (Vec ((G(Uj))Aj+1Aj+2 …Al(V(I)- y)) (VjT))T))
dw(i)dw(j)	d wT)	.
Recall that we define
Bs:t = AsAs+1 …At ∈ Rdsτ×dt, (s ≤ t) and Bs：t = I, (s>t).
Then we have
fx = (Vj-1)(V(l) - y)T B"l) X (Idj) M(GTUj))) (，QIj)
∂w(Ti)∂w(j)	j+1:l	j	∂w(Ti)	1
+	XX (VjT)(V(I)- y)TBT+rl) X (g(Uj))Bj+i:k-iWT) dvecd(Gyk))) (, Qj)
k=j +1	∂w(i)
+	(VjT)(V(I)- y)TBT+i：lG(u(i))) X (G(Uj))Bj+i:i-i) dv^WTl(，Qj)
∂w
(i)
+	VjT) X (G(Uj))Bj+1:1) d(，- y) (, Qj)
∂w(i)
(j -1)
+	Idj-I X (G(Uj))Bj+1:1(V(I)- y)) ɪɪ(, Q5j)
∂w(i)
40
Published as a conference paper at ICLR 2018
Case I:	i > j. We first consider the case that i >j. In this is case, Qy = 0 since dvec(GTu__)) = 0.
dw(i)
(k)
Computing Q2j needs more efforts. By utilizing the computation of ∂uv , in Lemma 23, We have
dve⅛^ = dve⅛⅛u≡ 第=PG(T)T	B"1(W "))(k>i)
where Pk is a matrix of size dk × dk whose ((S - 1)dk + s, s) (s = 1, ∙∙∙ , dk) entry equal to
σ(uR)(1 - σ(uSk)))(1 - 2σ(十，)and rest entries are all 0. When k = i,
∂vec(G(u(k)))	∂vec(G(u(k))) du(k)
∂ W(k)
du(k)	∂W(k)
Pk ((v(kT))T ③ Idk) ∈ Rdk×dkdk-1.
Note that for k < i, we have ”(；())= 0. For brevity, let
dw(i)
Dk，(MT)(V(I)- y)τBT+ιj ㊈(G(Uj))Bj+ik-iWT)) (k = i, ∙∙∙ ,l).	(25)
Therefore, we have
Qj = DiPi ((v(iT))T 乳 Idi) + E DkPk ((v(iT))T 乳(G(u(i))Bi+i：k-i(W(k)
k=i+1	1
Then we consider Qj.
Qj = (VjT)(V(I)- y)TBTrLIG(u(i))) X (G(Uj))Bj+ii-i).
Also we can use the computation of 您；in Lemma 23 and compute Qj as follows:
Q4 =VjT)X (G(Uj))Bj+1: l) dV^
d	J	d w(i)
=(VjT)X(G(Uj))Bj+ Li)) ((V(iT))TX(G(U(i))Bi+i：ι)).
Finally, since i > j, we can compute Qj = 0.
Case II:	i = j. We first consider dG(U()):
∂W(k)
∂vec(G(u(k)))	∂vec(G(u(k))) du(k)
d wTk)
du(k)	dwTk)
Pk ((v(kT))T ㊈ Idk) ∈ Rd2×dfcdfc-1,
where Pk is a matrix of size d ×dk whose (s, (s-1)dk + s) entry equal to σ(uSk))(I-σ(uSk)))(1 -
2σ(uSk))) and rest entries are all 0. Qy can be computed as
Qy=MT)(M)- y)T BTs) X (Idj)d VetdWTUj)))
(j)
=((v(jT)(V(I)- y)τ BT+11) X (IdJ)(Pj ((vjT))T X IdJ).
As for Qj2, by Eqn. (25) we have
l
Qj = E DkPk kST))T X (G(Uj))Bj+ik-ι(W(k)
k=j+1	、
Since i = j, Qj3 does not exist. For convenience, wejust set Qjj = 0.
41
Published as a conference paper at ICLR 2018
Now we consider Qj4j which can be computed as follows:
Q4j =v(j-1) ”G(u(j))Bj+i：i) d⅞⅛-y)
4	∂w(Tj)
=(VjT)乳(G(Uj))Bj+1:1)) ((VjT))T 乳(G(Uj))Bj+1：1)T).
Finally, since i = j , we can compute Qj5j = 0.
Case III: i < j. Since "∂WWX is symmetrical, We have Qj = Qki (k = 1, .…,5).
Step 2. Bound the operation norm of Hessian: We mainly use Lemma 22 to achieve this goal.
From Lemma 22, We have
(1)	For arbitrary matrices M and N of proper size, We have
kG(u⑺)MkF ≤ ɪIMkF and ∣∣NG(U⑴)kF ≤ ɪ∣∣NkF.
16	16
(2)	For arbitrary matrices M and N of proper size, We have
26	26
kPk M kF ≤ 38 IMkF and k NPk kF ≤ 承 kN kF.
(3)	For Bs:t and Ds:t, We have
21	1
kBs:tkF ≤ I6t-s+ιDs t and I6t-s+ιDs t ≤ Cr,
where Cr = max (r6,(喧)).
(4)	For arbitrary matrices M, N and I of proper sizes, let m = vec (M). Then We have
k(N 0 I )mkF ≤kM kF kN kF and k(I 0 N )mkF ≤∣MkF IN kF .
The values of entries in V(h) are bounded by 0 ≤ σ(U(hi)) ≤ 1 which leads to V(h) 2F ≤ dh ≤ Cd,
where Cd = maxi di. On the other hand, since the values in V(l) belong to the range [0, 1] andy is
the label, kV(l) - yk22 can be bounded:
kV (l) - yk22 ≤ Cy < +∞,
where Cy is a universal constant.
We first define
Cj = DkPk (V(I))T 乳(G(U⑴)Bi+i：k-i(W(k))τ)T)
and
Cj=DiP ((VCiT))T ㊈ Idi) = ((V(I)㊈ Idi) (DiPi)T)T = WT)㊈(DiPi)T)T
=(VJ))T 乳(DiPi),
where Dk is defined in Eqn. (25). ① holds since for an arbitrary vector U ∈ Rk and an arbitrary
matrix M ∈ Rk×k, we have (u 0 Ik) M = U 0 M.
Case I:	i > j. According to the definition of Cij and Ckij , we have Qi2j = Cij + Plk=i+1 Ckij . So
we have
2
∂2f(w, x)
dwT)dw(j)
l
Cij + X	Ckij + Qi3j + Qi4j
k=i+1
2
F
F
=(l	-i+3) Cij2F+	Xl	Ckij2F	+	Qi3j2F+	Qi4j2F	.
k=i+1
42
Published as a conference paper at ICLR 2018
Here we bound each term separately:
IICijIlF ≤ MT)IlF 卜(I)- y∣∣F kBi+i：i kF 116 ∣∣Bj+iJWlF 舅心T)IlF
26	1
≤ 38 Cy djτdiτ ι6ι-i
26	1
≤ 38 Cydjτdiτ ι6ι-j
Di+1：l 16i-7 Dj+1：i
Dj+1:l
26
≤ 38 Cy dj-1di-1Cr .
Similarly, we can bound kCkij k2F as follows:
Ckij
2
F
≤ IIv(j-1)IIFIIv(l) - yIIjBk+Ll kF 16 I1Bj + 1:k-1wtiif 2JwtiF IyBi+1：ɪW (k))T
2
F
26
≤ 38 Cy dj-ιdi
26
=38 Cy dj-ιdi
1
T 16l-k
1
Dk+1：l I6k-j-1 Dj + 1：k i6k-i-1 Di+1：k
T 16l-j-1
Dj+1：l i6k-i-1 Di+1：k
214	2
≤ ^38Cy dj-1di-1cr .
We also bound IIIQi3j III	as
(j-1)llF llv(I)- y||F 16 kBi+1：lkF 16 kBj+UkF ≤ 28Cydj-1Cr
Finally, we bound IIIQi4j III	as follows:
16 llBj+1：lkF ||v(iT)IIF 16 kBi+1：l kF ≤ 28 dj-1di-1Cr.
Note that di ≤ cd. Thus, we can bound
∂2f(w,x)
dw(j)dwTi)
2
as
F
∂ 2f(w,x)
dwT)d w(j)
2
F
26	l 214	1	1
38 Cy dj-1di-1cr + E -38 Cy dj-1di-1cr + 方 Cy dj-1cr + 方 dj-1 di-1cr
k=i+1
64	2	4096	2 2	1	1	2
≤(I + 1)(6561CyCdCr + 6561Cy(I- 2)cdcr + 256CyCdCr + 256CdCJ .
Case II:	i = j . According to the definition of Cij and Ckij , we have Qj2j = Plk=j+1 Ckjj .
Similarly, we have
∂2f(w,x)
d wT)dw(j)
2
=IIIQj1j+Qj2j+Qj3j+Qj4j+Qj5jIII2
F
2
l
Qj1j + X Ckjj + Qi4j
k=j+1
F
≤(l-j+2) ("F +XX /F+ll 叫 F).
k=j+1
43
Published as a conference paper at ICLR 2018
Thus, we can bound
Qj1j	first:
忡 IF ≤ WTlFlW)- yt kBj+1：1 kF 26 Bv(j-1)BF ≤ 36 Cy "-勺
As for Qj2j , we have
lllCkijlll2F
≤ Mjr ∣∣F W)- y∣j kBk+rιkF IyBj+LiWTiiF 28MjTlF IyBj+…(W (k)HlF
26	2	1	1	1
=38Cydj-ι i6i-k Dk+1:l i6k-j-1Dj+1:k i6k-j-1Dj+1:k
214
≤ "38 Cy dj-icr.
Then we bound kQj4j k2F :
llQWF ≤ MjTiF ι16 kBjEF Bv(j-1)BF	限+倔 ≤ 28 …
Note that for any input, we have	Cv	=	maxj	llv(j-1)(v(l)	-	y)T	ll2F	≤	maxj	kv(j-1) k2F
(v(l) - y
∂2f (w,x)
ll2F ≤ CyCd, where llv(l) - yll2F can be bounded by a constant Cy . Thus, we can bound
dwT)dw(j)
as
F
∂ 2f(w,x)
dwT)d w(j)
26	l 214	1
38Cyd2-ιcr+ E ^38^Cydj-ιcr + 28dj-ιcr
k=i+1
64	2	4096	2 2	1	2 2
≤(l + 2)(6561 CcCdCc + 6561 Cc(I- I)CdCr + 256CdCJ .
CaSe III: i < j. Since dfW,T) is symmetrical, We have Qkj = Qki (k = 1, •一 , 5). Thus, it yields
∂2 f (w, x)
dwT)dw(j)
2
64	2	4096	2 2	1	1	2
≤(l + 1)(6561CyCdCr + 656lCy(I- 2)CdCr + 256CyCdCr + 256CdCr
F
Final reSult: Thus We can bound
WWf(w, X)IIoP ≤ WWf(w, x)||f
≤t(l - 1)l im,j:ai6=xj
∂2f (w,x)
dw(j)d WT)
2l
F + Xj =1
∂2f(w, x)
dw(j)dwT) F
≤ (l - 1)l(l+1)
64	2	4096	2 2	1	1	2
^6561CyCdCr +6561Cy(I- 2)CdCr +256CyCdCr + 256CdCr
6 " 64	2	4096 ,,…2 2	1 , 2 2∖ A1
+(l + 2) (k6561CyCdCr + 6561Cy (I- »*% + 标ICdC))
≤	Cs1 Cr C2dl4 ,
Where Cs1 and Cs2 are tWo constants.
SinCe UvW f (w, x)|op ≤ || VWf (w, x)∣∣f, we know that the gradient VWf (w, x) is ς-Lipschitz,
where ς =，c§i Cr °dl4.
44
Published as a conference paper at ICLR 2018
On the other hand, since for any input x, σ(x) belongs to [0,1], the values of the entries of VW f (w, x)
can be bounded. Thus, we can bound
kV3w f (w, x)kop
sup
kλk2≤1
(λ03, Vw f(w, x))= [VW f(w, x)]ijkλiλjλk
≤ ξ < +∞.
We complete the proof.
□
D.2.3 Proof of Lemma 18
For convenience, we first give the computation of some gradients.
Lemma 24. Assume the activation functions in deep neural network are sigmoid functions. Then we
(j)	(j)
can compute the gradients ∂u⑴ and ∂U⑴ as
(j)	T
∂u(1) = (G(U(I))A2 …Aj-ι(Wj)T) ∈ Rdj×d1, (j > 1).
Ivj) = (G(U(I))A2 一Aj)T ∈ Rdj×d1, (j> 1).
∂ u(1)
It should be pointed out that the proof of Lemma 24 can be founded Sec. D.4.
Proof. To prove our conclusion, we have two steps: computing VxVwf(w, x) and bounding its
operation norm.
Step 1. Compute VxVwf(w, x):
We first consider the computation of ∂f∂WwX) :
∂2f(w, x) = ∂ (VeC ((G(Uj))Aj+1Aj+2 …Al(V(I)- y)) (VjT))T))
∂xT ∂ w(j)	∂xT
Recall that we define
Ai = (W(i))TG(U(i)) ∈ Rdi-1×di.
Bs：t = As As+ι …At ∈ Rds-1×dt, (s ≤ t) and Bs：t = I, (s>t).
Then we have
∂2f⅛x) = (VjT)(V(I)- y)TBT+rl)㊈(Idj) -vec∂GTUj))) (, Q1)
∂x ∂w(j)	∂x
+ XX (VjT)(V(I)- y)TBT+1： A(G(Uj))Bj+i:IWT) -Vecdy))据 Q2)
k=j+1
+ V(j-1)
+ Idj-1
)“G(u(j))Bj+i：l) d(v∂x^(, Qj)
(j-1)
③(G(Uj))Bj+i:l(v(l) - y)) -dxT-(, Qj)
By using Lemma 24, we can compute Qi1j as
-vec"k))) = -vecJG((U(k))) -uT =Pk (G(U(I))B2：k-i(Wk)T)T
∂xT	∂U(k)	∂xT
Thus, we have
Q1 = (VjT) (V(I)-y)T BT+ι∙l)㊈ Idjd VeCdXTUj)))
=((VjT)(V(I)- y)τBj+i：l)0 Idj) Pk (G(U(I))B2：k-i(Wk)t)T .
45
Published as a conference paper at ICLR 2018
As for Qj2 , we also can utilize Lemma 24 to compute it:
l
Qj = X (VjT)(V(I)- y)TBT+1：1)乳(G(Uj))Bj+1:1 WT)
k=j+1
∂vec(G(U(k)))
∂ XT
X ((v(jT)(V(I)- y)TBT+1： A(G(U(j))Bj+nWT)) Pk (G(U(I))B2%-1(Wk)T)T .
k=i+1
Then we consider Qi3j .
Qj = VjT)乳(G(u(j))Bj+i：i) d(v∂lX- y) = (VjT)乳(G(Uj))Bj+1:1)) (G(U(I))B2:l)T .
Qj4 can be computed as follows:
(j -1)	T
Q4 = Idj-I 乳(G(Uj))Bj+1:1(V(I)-y)) ~∂χτ- = (Idj-1 乳(G(Uj))Bj+1:1(V(I)-y))) (G(U(I))B2：，.
Step 2. Bound the operation norm of Hessian: We mainly use Lemma 22 to achieve this goal.
From Lemma 22, we have
(1)	For arbitrary matrices M and N of proper size, we have
kG(U(i))M kF ≤ ɪ kM kF and kN G(U(i) )kF ≤ ɪ kN kF ∙
16	16
(2)	For arbitrary matrices M and N of proper size, we have
26	26
IP MkF ≤ 38 kM kF and k NPk kF ≤ 承 kN kF ∙
(3)	For Bs:t and Ds:t, we have
21	1
kBsM∣F ≤ ]6t-s + 1 Ds:t and	]6t-s + 1 Ds:t ≤ CT,
where Cr = max (r2,(%)	).
(4)	For arbitrary matrices M, N and I of proper sizes, let m = vec (M). Then we have
k(N ③ I )mkF ≤kM kF kN kF and k(I ③ N )mkF ≤IMkF kN kF .
The values of entries in V(h) are bounded by 0 ≤ σ(U(hi)) ≤ 1 which leads to V(h) 2F ≤ dh ≤ Cd,
where Cd = maxi di. On the other hand, since the values in V(1) belong to the range [0, 1] and y is
the label, kV(1) - y k2 *2 can be bounded:
kV(1) - yk22 ≤ Cy < +∞,
where Cy is a universal constant.
We first define
Cj = ((VjT)(V(I)- y)TBT+1:1)㊈(G(Uj))Bj+1:k-1WT)) Pk (G(U(I)g1(Wk)t)T.
Then we have Qj2 = P1k=j+1 Ckj . So we have
∂2f(w,x)
∂xτ ∂ Wj)
= Qj1 + Qj2 + Qj3 + Qj4
2
1
Qj1 + X Ckj +Qj3+Qj4
k=j+1
F
=(i-j+3)(同F+ X hC+BQjBF+同F).
k=j+1
46
Published as a conference paper at ICLR 2018
Then we bound each term separately:
IIQIlIF ≤ MiIIFw)- y∣∣F k"kF 38 116 修：I(W k )t∣∣f ≤ 28 Cy d”.
2
Similarly, we bound :
F
帕儿=IIv(jT)IUv(I)- yL kBk+i：ikF ι16 IBj+1:IWTIIF M
26	1	1	1
=38Cydj-1 161-k Dk+1:l 16k-j-1 Dj+1:k I6k_1 Dzk
26	2
≤38Cy dj-1cr .
We also bound
IIIQi3jIII2F as
2
B2:k-1(W (k))T
F
除，≤ Mj—IL《kBj+i：IkF《kB2：IkF ≤ 28 dj-1 cr.
Finally, We bound IIQj4 II	as folloWs:
IIq4IIf = 16kBj+LlkF IIv(I)- y，16kB2jkF ≤ 28cyCT.
Since Cd
∂2f(w,x)
∂xT ∂w(j)
maχidi,We Can bound II ∂jX (F as
2	26	l 26
≤(I - j + 3) I 38cydj-ιcr +	38cydj
F	k=j+1
Tcr + 28 cy dj-1cr + 28 cy Cr
26	l	26	1	1
38cydj-1cr + E 38cydj-1cr + 28cydj-1cr + 28cycr I ∙
k=j+1
Final result: Thus We Can bound
∣∣VwVxf (w, x)kop ≤ ∣∣VwVxf (w, x)∣∣f ≤
ul
tuXj=1
∂2f(w,x) 2
dw(j)dxT F
≤
t
j=1
k=j+1
∕26^"^^^^	^	^^
≤ V 38 ( + 2)cy cr Cd (Icr + I),
Where cd = maχj dj . The proof is Completed.
□
D.2.4 Proof of Lemmas 19 and 20
Lemma 25. (Alessandro, 2016; Rigollet, 2015) Let (xι, ∙∙∙ , Xk) be a vector of i.i.d. Gaussian
variables from N(0, τ2) and let f : Rd0 → R be L-Lipschitz. Then the variable f(x) - Ef (x) is
sub-Gaussian. That is, we have
P (f (x) - Ef(X) >t) ≤ exp (—「22) ,	(∀t ≥ 0),
2L2 τ 2
or
E (λ(f (x) - Ef(X))) ≤ exp (4λ2L2τ2),	(∀λ ≥ 0).
Remarkably, this is a dimension free inequality.
47
Published as a conference paper at ICLR 2018
ProofofLemma 19. We first define a function g(x) = ZTVwf (w, x) where Z ∈ Rd is a constant
vector. Then we have Vxg(X) = Vx (ZTVwf (w, x)) = VxVwf (w, x)z. Then by Lemma 18, we
can obtain kVxg(x)k2
∣∣VxVwf(w, x)z∣∣2 ≤ β∣∣z∣∣2, where β = J228l(l + 2)CyCrCd (lcr + 1)
in which cy, cd and cr are defined in Lemma 18. This means g(x) is βkZk2-Lipschitz. Thus, by
Lemma 25, we have
E (t hz, Vwf(w, x) - EVwf(w, x)i) = E (t (g(x) - Eg(X))) ≤ exp (4t2β2∣z∣2τ2
Let λ = tZ . This further gives
E (hλ, Vwf(w, x) - EVwf(w, x)i) ≤ exp (4β2τ2∣λ∣2),
which means <λ, Vwf (w, x) 一 EVwf (w, x)i is 8β2τ2-sub-Gaussian.	□
Proof of Lemma 20. We first define a function h(x) = ZT V2wf (w, x)Z where Z ∈ Sd-1, i.e.
∣Z∣2 = 1. Then h(w) is a γ-Lipschitz function, where γ = ∣VxV2wf(w, x)∣op. Note that since
the sigmoid function is infinitely differentiable function, VxV2wf (w, x) exists. Also since for any
input x, σ(x) belongs to [0, 1]. Thus, the values of the entries in VxV2wf(w, x) can be bounded.
So according to the definition of the operation norm of a 3-way tensor, the operation norm of
VxV2wf(w, x) can be bounded by a constant. Without loss of generality, let ∣VxV2wf(w, x)∣op ≤
γ < +∞. Thus, by Lemma 25, we have
E (t〈z, (Vw f(w, x) - EVwf (w, x)) z〉) = E (t (h(x) - Eh(x))) ≤ exp
(T )
This means that the hessian of the loss evaluated on a unit vector is 8γ2τ2-sub-Gaussian.	□
D.2.5 Proof of Lemma 21
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. ∣w(j) ∣2 ≤ r.
Assume that w(j) has sj non-zero entries. Then we have Plj=1 sj = s. So here we separately assume
Wj = {wj, ∙∙∙ , Wn j} is the djdj-1 e/d-covering net of the ball Bdjdj-1 (r) which corresponds
to the weight w(j) of the j-th layer. Let nj be the /l-covering number. By -covering theory in
(Vershynin, 2012), we can have
nj ≤	djdj-1
sj
3r
djdj-1e/(ld)
sj	3rd
≤ exp sj log
djdj
exp Rlog (F
Let w ∈ Ω be an arbitrary vector. Since W = [w(i),…,w(i)] where w(j)is the weight of the j-th
layer, we can always find a vector Wkj in Wj such that ∣W(j) - Wkj ∣2 ≤ djdj-1e/d. For brevity,
let jw ∈ [nj denote the index of Wjin e-net wj. Then let Wkw = [wj;…;WkJ …;Wk J. This
means that we can always find a vector Wkw such that ∣W - Wkw ∣2 ≤ e. Accordingly, we can
48
PUbHShed as a COnferenCe PaPer at ICLR 2018
decompose =V2 j-ɪ— V2 J？)= as follow 沼
一<2>>?) —<Jxop
-L
IM<jsas) — E(<jsa))
3 E OP
1 7Z 1 7Z
slM (V2 j(tυas) — Vj(S©a3)) +sl3 <2 j(st^ο) — IE(V2 包)
+H(V2 j?Ka)) — IE(V2 j(£ 包)
OP
Λ
IM (V2 j(e^°) — <2 j(st^v)
S E
+
OP
+ 1E(V-(§◎3)) — IE(V2 ;£ 包)∙
OP
Here We also define four events EEE⅛ and E⅛ as
E。U(SUP=‹bs)lv2J?)= ≥T
Ltυ∈- --0P
——∑s<2 j(ss^2)) — IE(V-(§◎ 7包)
∙ 1
2=1




SUP
tυ∈Q
1 3
31M(V2 jsas) — V2 j(SEη≡)
2=1
SUP
jE∈≡3Lj=≡
V I
— 3
OP
1 3
——MV2 j(sea≡l E(<j(§w 包)
- 1
2=1
SUPw<2j?EaJ)) —E(<j?a)=ODIVΛI
Accordingly We have
亏(E。) ≤ 亏(El) +亏(e2)+亏(E3)∙
SO We CaIl respectiveIy bound IP (El-IP (E2) and IP (E3) to bound IP (Eo)∙
SteP L BOlnId IP (EIX We first bound IP (El) as f1OWE
^E1) H¾- SUP
L 0Q
Θ3L
≤ihSUP
0 L 0∈Q
3L
≤ihSUP
B L 0∈Q
八——E
A®傍
V I
— 3
OP
1 S
31M(V2 jsa≡— V2 j(sea≡)
工 心
IM (V2 jsas) — V2 j(§E H≡)
-PHl
V I
— 3
=WMΓ(V2 jsas) — q j?EaJS
SUP
tυ∈Q
T Isr =
op.
SUP T Isr =
2
Where Θ holds SinCe by MarkoV inequality andΘholds because OfLemlna 17.
TherefOr尸 We Can Ser
6me
E ≥卜.
Published as a conference paper at ICLR 2018
Then we can bound P(E1):
p(Eι) ≤ ε.
Step 2. Bound P (E2): By Lemma 2, we know that for any matrix X ∈ Rd×d, its operator norm
can be computed as
kXIlop ≤ ；----su SUP lhλ, Xλil .
1	- 2 λ∈λ
where λ = {λ1, . . . , λkw} be an -covering net of Bd(1).
Let λ1∕4 be the 1- -covering net of Bd(1) but it has only S nonzero entries. So the size of its e-net is
d
s
≤ exp (s log (12d)) .
Recall that we use jw	to denote the index of wkj	in	e-net	wj	and we have jw	∈	[nj],	(nj ≤
exp (Sjlog (3rd))). Then We can bound P (E2) as follows:
P(E2 ) =P	sup
jw∈[nj],j=[l]
1n
—£ Vfwkw, x(i)) - E(V2f(wkw, X))
n
i=1
t
≥ —
_ 3
2
=P sup
∖jw∈ [ne j],j=[l],λ∈λ1 /4
卜,(1 X V2f (wkw , x(i)) - E (v2f(wkw, X))
)λ>l≥ t)
l
≤ exp (s log (12d)) exp	sj log
j=1
V2f (wkw, X(i))-E (V2f (wkw, x
≥ 6
sup P
jw ∈[ne j],j=[l],λ∈λ1 /4
这《
i=1
2
Since by Lemma 20, (λ, "W f(w, x) 一 EVW f(w, x)) λ) where λ ∈ Bd⑴ is 8γ2τ2-sub-
Gaussian, i.e.
8t2γ2τ2
E (t〈入，(VWf(w, X)- EVWf(w, x))入〉)≤ exp ( —2—
Thus, 1 pn=ι <λ, (VWf (w, x) 一 EVWf (w, x)) λ is 8γ2τ2/n-sub-Gaussian random variable. So
we can obtain
PG
E〈y, (vWf(w, X)- evWf(w, X)) y〉≥ 6
i=1
nt2
≤ 2exp V- 72Y272
n
Note d =	j djdj-1. Then the probability of E2 is upper bounded as
nt2	36d2r
P (E2)≤ 2eχp - 72γ2T2 + S log (
Thus, if we set
t〉T ∕72(s log(36d2r/e) + log(4∕ε)j
≥ YTV	n
then we have
ε
P (E2) ≤ 2 .
50
Published as a conference paper at ICLR 2018
Step 3. Bound P (E3): We first bound P (E3) as follows:
P(E3) =P (SUp IlE(V2f(wkw, X))- E(V2f (w, x))∣∣2 ≥ t
∖w∈Ω	3
≤P (E SUp ∣∣(V2f(wkw, x) - V2f(w, x)∣∣2 ≥ t
∖ w∈Ω	3
，	∣∣(v2f (W, X)-V2f (Wkw, X)) ∣∣2	ll	ll
=P (E WUΩ---------MFl---------------WUPΩkw - Wkj2
≤p(e OUPIIV3fM χ)∣∣op ≥ t)
≤P (ξe ≥ I).
We set enough small such that ξ < t/3 always holds. Then it yields P (E3) = 0.
Step 4. Final result: To ensure P(E0) ≤ ε, we just set = 36rl2 /n and
t ≥ max (", γτ r72(s log(36rd7)+log(4/。)) =maχ (牛,小, r d log(nl)+log(4∕ε)
Therefore, there exists such two universal constants cm0 and cm such that if n ≥
then
CmO ξ2l2r2
γ2τ2ε2s log(d)log(1∕ε)
WUWVJn(W)-V2J(w)∣∣op ≤ cmγτ∕]≡≡
holds with probability at least 1 - ε.
□
D.3 Proofs of Main Theories
D.3.1	Proof of Theorem 4
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. kW(j) k2 ≤ r.
Assume that W(j ) has sj non-zero entries. Then we have Plj=1 sj = s. So here we separately assume
Wj = {wj, ∙∙∙ , Wn j} is the djdj-ιe/d-covering net of the ball Bdjdj-1 (r) which corresponds
to the weight W(j ) of the j-th layer. Let nj be the /l-covering number. By -covering theory in
(Vershynin, 2012), we can have
nj ≤
)(djdj-ie∕(ld)
≤ exp
sj log
j/d) )=exp
sj log
Let W ∈ Ω be an arbitrary vector. Since W = [w(i),…,w(“ where W(j)is the weight of the j-th
layer, we can always find a vector Wkj in Wj such that kW(j) - Wkj k2 ≤ djdj-1e/d. For brevity,
let jw ∈ [nj denote the index of Wjin e-net Wj Then let Wkw = Wj;…;WkJ …;Wk J. This
means that we can always find a vector Wkw such that kW - Wkw k2 ≤ e. Accordingly, we can
51

⅛> hl(ʃw^ʌll >d0∣K≈^)∕™zi
QABq əm Z[ buiuiə[ Aq əɔms sp∣oq ① əjəηm
/	一 、/乙
/	ʊe 个 /
15> CIIK'm)/HdnS) 3=(
T=?
8 叫zxM
u L
ʊem
dns
3>h∣(aι1aι) γε∆∣∣dns Ja
ʊe外
：SMonoJ
(æ c^n)VζΔ ʊeedns)回 punoq əM 'uəg[ ∙Aι∏Bnbouτ AowlBW Jo əsnnɔəg sp∣oq ① əiəgM
Se(I
(]∣"m)"M△恤 S)
? _
31—>
9E
ʊem
δ∣∣m^ai — m∣∣ dns
z∖∖^aι-aι∖∖
11()/△ T5 ⑺/△) IWa 和
dns 3[->
ʒ
Tn U
T ⑶多 iλi)∕δ) R -
u L
dns 3->
/ E®
ʒ
-<
T =2	- \
::u ʊə^ɪ \
((⑶a√'7⅛m)∕2χ - (0)a√m)∕2χ)R- dns j J=(也)就
U ɪ J
:sMonoJ SB (1/)j punoq ISJg OM ：(【豆)d PUnoH •【dəjs
'(0a-)d PUnoq oj (ɛg-) j put? (δg-) j ʧɪg-) j punoq XwADɔodsoɪ ueɔ əm OS
` (εa^)d + (εa^)d + (τa^)d > (0a^)d
əabq əM '丸既U!pioɔɔv
ʒ
ɛ -
-<
?
((ʃ 1aι)∕∆)a - ((ʃ 1'¾)∕∆)a dns
εa^
^	τ=∙ U
((æ^^m)ʃʌ)aɪ - (‰c"M∕∆ Z γ
u L
[↑]=P[eu∖3^-C
dns

ʒ
ɛ -
-<
?
T=?
■⑶x 口Cn)J△—。多⑹/△) Rl
u ɪ
ʊem
dns
τa^
II ʊem
(^)uΓΔ dns
0a^
1 V <6
sŋ 过 PUB 过'工豆'。豆 SluəAə jnoj əuɪjəp osɪB əm əjəH
ʒ
. ((æ 'in)ʃʌ)aɪ - ((æ ^M∕δ)si +
ʒ	T=?
((≈ιmM∕∆)a- (ω≈ιmM∕∆Zγ
u L
ʒ
+
((⑶	—(⑶ a√m)∕a)
>
3
■ c^)∕∆)3I -(0 Sm)必)回 +
((æ^^m)ʃʌ)aɪ_((修')/△ 2K^ + ((c∙^c"M∕∆-(⑶况⑹/△)2ɪ-=
u ɪ	u ɪ
乙	I=?
(K'm)/zX)3I—3丘=
u L
z∣∣(^)r∆ - (λi)uγδ∣∣
:s/ʌoɪɪoj sb δ∣∣(^)r∆ — (rn)uΓΔ∣∣ əsoduɪoɔəp
SIO% mɔl IB iodBd əɔuəiəjuoɔ B sb poqs∏qnj
Published as a conference paper at ICLR 2018
where ς
，c§i Cr Cdl4 in which Cd = maxi d% and Cr = max
l-1
Therefore, we have
P (Ei) ≤ 十.
We further let
Then we can bound P(E1):
6ς
t ≥ —.
ε
P(Ei) ≤ ε.
Step 2. Bound P (E2 ): By Lemma 1, we know that for any vector x ∈ Rd, its `2 -norm can be
computed as
I∣xk2 ≤ 1—sup hλ, Xi .
where λ = {λi, . . . , λkw} be an -covering net of Bd(1).
Let λ1∕2 be the 11 -covering net of Bd(1) but it has only S nonzero entries. So the size of its e-net is
≤ exp (s log (6d)) .
Recall that we use jw to denote the index of Wkj in e-net Wj and we have jw ∈ [nj], (nj ≤
exp (sj log (3rd)). Then We can bound P (E1) as follows:
P (E2) =P	sup
jw ∈[nj],j=[l]
1n
n EVf(Wkw, x(i))- E(Vf(Wkw, X))
n i=i
≥t
2
=P	sup
jw∈[nj],j=[l],λ∈λ1/
2
2
l
(λ, 1 X Vf(Wkw , x(i)) - E (Vf(Wkw , X)); ≥ ∙t)
≤ exp (s log(6d)) exp	sj log
j=i
▽f (Wkw , x(i)) - E (Vf(Wkw, X))
sup	p(1 X ",
jw ∈[nej ],j=[l],λ∈λ1∕2	∖n i=1 ∖
)≥力.
Since by Lemma 19, hy, Vf (W, X)i is 8β2τ2-sub-Gaussian, i.e.
E (hλ, Vw f(W, x) - EVw f(W, x)i) ≤ exp (8*；。12
where β = J∣6l(l + 2)CyCrCd (lcr + 1) in which Cy, Cd and Cr are defined in Lemma 16. Thus,
* Pn=I hy, Vf (w, x)i is 8β2τ2∕n-sub-Gaussian random variable. Thus, we can obtain
P (1 X〈y, Vf(Wkw, x(i))- E(Vf(Wkw, x))〉≥ t) ≤ exp (-TnITI).
Notice, Pj djdj-i = d. In this case, the probability of E2 is upper bounded as
nt2	18r
P(EI) ≤ exp 1 vw + d log(
Thus, if we set
∕72(s log(18d2r∕e) + log(4∕ε))
t ≥ βτ∖
n
53
Published as a conference paper at ICLR 2018
then we have
p (E2) ≤ ε.
Step 3. Bound P (E3): We first bound P (E3) as follows:
P(E3)=P (SUP kE(Vf (Wkw, X))- E(Vf (w, x))k2 ≥ I
∖w∈Ω	3
P	IIE (Vf(Wkw, X)-Vf(W, x)k2)
—P I sup
∖w∈Ω
SUP kw — Wkwk2 ≥ t
w∈Ω	3
kw-wkwk2
≤P GE Wu-VJnM TL≥ t)
≤PGe ≥ t).
where ① holds since by Lemma 17. We set e enough small such that ςe < t/3 always holds. Then it
yields P (E3 ) = 0.
Step 4. Final result: To ensure P(E0) ≤ ε, we just set e = 18r/n and
t ≥ max
72 (s log(18d2r∕e) + log(4∕ε))
n
max ( ≡T, βτ∖/72 (Slog(nl) + log(4且
nε	n
Note that ς — O(√7cdβ). Therefore, there exists a universal constant Cyo such that if n ≥
cy0 cdl3r2∕(s log(d)τ 2ε2 log(1∕ε)), then
SuP U VJn(W)-VJ (w)∣∣ 2≤ τy72∣ Cy l(l + 2) (lCr + 1) Cr Crf J log(dn/：+log(4/⅛
holds with probability at least 1 一 ε, where Cy, Cd and Cr are defined in Lemma 16.	□
D.3.2	Proof of Theorem 5
Proof. Suppose that {w(1), w(2),…，w(m)} are the non-degenerate critical points of J(w). So for
any W(k), it obeys
inifλik V2J(W(k))	≥ζ,
where λik V2J (W(k)) denotes the i-th eigenvalue of the Hessian V2J (W(k)) and ζ is a constant. We
further define a set D — {w ∈ Rd | ∣VJ(w)k2 ≤ e and infi ∣λi (V2J(w(k))) | ≥ Z}. According
to Lemma 4, D — ∪k∞=1Dk where each Dk is a disjoint component with W(k) ∈ Dk for k ≤ m and
Dk does not contain any critical point of J(W) for k ≥ m + 1. On the other hand, by the continuity
of VJ(W), it yields kVJ (W)k2 — e for W ∈ ∂Dk. Notice, we set the value of e blow which is
actually a function related n.
Then by utilizing Theorem 4, we let sample number n sufficient large such that
SUPUVJn(w) - VJ(w)∣l2 ≤ β，I
holds with probability at least 1 - ε, where β — τjm Cy l(l + 2) (lCr + 1) Cr Cd Js log(dn/n+log(4/I.
This further gives that for arbitrary W ∈ Dk, we have
inf UtVJn(W)+ (1 - t)VJ(w)∣∣2 — inf ∣∣t (VJn(W) -VJ(W)) + VJ(W)U?
≥ inf ∣∣VJ(w)∣∣2 - sup t ∣∣VJn(W)-VJ(W)U
w∈Dk	w∈Dk	2
e
≥ 了	(26)
54
Published as a conference paper at ICLR 2018
Similarly, by utilizing Lemma 21, let n be sufficient large such that
WuΩ Hn(WiJ(w"op ≤ cmγ√≡n≡ ≤ 2
holds with probability at least 1 - ε. Assume that b ∈ Rd is a vector and satisfies bTb = 1. In this
case, We can bound λk (V2 Jn(W)) for arbitrary W ∈ Dk as follows:
WinDJλk (V2 Jn(W)) I=WinD ^ ,mbnJbT V2Jn(W)N
=inf ðminɪ ∣bτ «2 Jn(W) - V2J(W)) b + bτV2J(W)b∣
≥ inf min II * *bTV2J(W)bII - min ∣bτ (V2Jn(W) -V2J(w)) b
≥ inf min bTV2J(W)b - max bT
w∈Dk bT b=1	bT b=1
Jn(W)-V2J(W)) b
=inf inf∣λk (V2f (w(k), x)) - ∣∣V2Jn(W)-V2 J(w)∣∣
w∈Dk i	op
≥ ζ.
-2
This means that in each set Dk, V2 Jn(w) has no zero eigenvalues. Then, combining this and
Eqn. (26), by Lemma 3 we know that if the population risk J(W) has no critical point in Dk, then
the empirical risk Jn(W) has also no critical point in Dk; otherwise it also holds. By Lemma 3, we
can also obtain that in Dk, if J(W) has a unique critical point W(k) with non-degenerate index sk,
then Jn(W) also has a unique critical point Wnk) in Dk with the same non-degenerate index Sk. The
first conclusion is proved.
Now we bound the distance between the corresponding critical points of J(W) and Jn (W). Assume
that in Dk, J(W) has a unique critical point W(k) and Jn(W) also has a unique critical point Wnk).
Then, there exists t ∈ [0, 1] such that for any z ∈ ∂Bd(1), we have
≥kVJ(Wn(k))k2
= max hVJ (Wn(k)), zi
zT z=1
= max hVJ(W(k)), zi + hV2J (W(k) + t(Wn(k) -W(k)))(Wn(k) - W(k)), zi
zT z=1
①	2	1/2
≥	V2J(W(k))	(Wn(k) - W(k)), (Wn(k) - W(k))
≥ζ kwnk) - w(k)k2,
where ① holds since VJ (W(k)) = 0 and ② holds since W(k) + t(Wn(k) - W(k)) is
in Dk and for any W ∈ Dk we have infi ∣λi (V2J(w)) | ≥ Z. Then if n ≥
Cs max (cdl3r2∕(s log(d)τ2ε2 log(1∕ε)), S log(d∕l)∕Z2) where Cs is a constant, then
ll (k)	(k)∣∣ J 2τ /512―^^一—T	-	IS log(dn∕l)+log(4∕ε)
kWnk)- W(k)k2 ≤ — √729Cyl(l + 2)(lCr + 1) CrCdV
ζ 729	n
holds with probability at least 1 一 ε. The proof is completed.	□
D.3.3 Proof of Theorem 6
Proof. Recall that the weight of each layer has magnitude bound separately, i.e. kW(j) k2 ≤ r.
Assume that W(j ) has sj non-zero entries. Then we have Plj=1 sj = S. So here we separately assume
Wj = {wj, ∙∙∙ , Wn j} is the djdj-ιe∕d-covering net of the ball Bdjdj-1 (r) which corresponds
55
Published as a conference paper at ICLR 2018
to the weight w(j)of the j-th layer. Let nej be the e∕∕-cOVering number. By e-covering theory in
(Vershynin, 2012), we can have
n，j ≤ 广 j)( j≡ y ≤ exp GIOg( ≡⅛))=exp LIOg
Let W ∈ Ω be an arbitrary vector. Since W = [w(i),…)w(()] where w(j)is the weight of the j-th
layer, we can always find a vector Wkjin Wj such that ||w(j)- Wkjk2 ≤ ⅛∙d7∙-ιe∕d. For brevity,
let jw ∈ [nej] denote the index of Wkjin e-net Wj Then let WkW = [wki ; ∙ ∙ ∙ ; WkJ …;WkJ. This
means that we can always find a vector WkW such that ∣∣w - WkW ∣∣2 ≤ e. Accordingly, we can
decompose Jn(W) - J(w)l as
1 n
Jn(W) - J (w)∣= - X f (W, X(i)) - E(f(w, x))
n i=1
1n	1n
n X(f(w, x(i))-f (Wkw, x(i)))+n X (WkW
i=1
i=1
X(i)) -Ef(WkW, x) +Ef (Wkw, X)-Ef (w, x)
1n
≤ n∑S(f(W, x(i))-f(wkW, x(i)))
i=1
1n
nXf (WkW
i=1
x(i))-Ef (wkW, x) + Ef(WkW, x)-Ef (w, x)
+
Then, we define four events Eo, E1, E2 and E3 as
Eθ = SUp I Jn(W)- J(w) I ≥ t
w∈Ω
E1
SUP
w∈Ω
1n
一E f (W, x(i))- f (wkW, x(i)))
n i=1
t
≥ —
,
一3
E2
S	suP	1 Xf (WkW
Ijw ∈[nej ],j=[l] n i=1
x(i“-E(f(WkW, X)) ≥ 3t
t
)
E3
sup
w∈Ω
E(f(wkW, x))-E(f (w, x))
Accordingly, we have
P (E0) ≤ P (E1) + P (E2) + P (E3).
So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).
Step 1. Bound P (E1): We first bound P (E1) as follows:
P (E1 ) =P sup
w∈Ω
1n
—E(f(w, xcd - f(wkW, x(i)))
n i=1
①3 J
≤-E sup
t ∖ w∈Ω
1n
-∑S f (w, x(i) ) - f (WkW , x(i)
n i=1
≤ 3 E (SUP l1 Pn=I (f (Wr x2 - f(WkW, XI)))I sup |W - WkW ∣2
t	∖w∈ω	llw - WkW Il2	w∈Ω
≤ 3teE GudVJn(W, X)II 2
where ① holds since by Markov inequality, for an arbitrary nonnegative random variable x, then We
have
P(x ≥ t) ≤ EX).
≥ —
-3
≥t
56
Published as a conference paper at ICLR 2018
Now We only need to bound E (SuPw∈ω
follows:
E (wud VJnM TL)
∣∣VJn(w, X)∣∣J
Then by Lemma 16, we can bound it as
where α
ʌ/1⅛CyCd (I + Cr(I - I))
Therefore, we have
We further let
Then we can bound P(E1 ):
≤E suP
∖ w∈Ω
1n
n X VfMX⑺)
≤ α,
2
in which cy, cd and cr are defined in Lemma 16.
3α
P (E1) ≤ -f.
t≥
6α
ε
P(E1) ≤ 2
Step 2. Bound P (E2): Recall that we use jw to denote the index of Wkj in e-net Wj and we have
jw ∈ [nej], (ηej ≤ exp (Sjlog (3rd)). We can bound P (E2) as follows:
n
P (E2 ) =P	Sup
jw ∈[n j],j =[l]
(1
≤ exp	sj log
j=1
1n	I t
n汇f(Wkw，x(i)) - E(f(Wkw，x)) ≥ 3
=1
SuP	P ( I - X f (Wj, Xei))-
jw∈[nj],j=[l]	I n i=1
E(f(wj, χ)) ≥ tj.
Since when the activation functions are sigmoid functions, the loss f(w, x) is α-Lipschitz. Besides,
we assume x to be a vector of i.i.d. Gaussian variables from N (0, τ2). Then by Lemma 25, we know
that the variable f(x) 一 Ef (x) is 8α2τ2-sub-Gaussian. Thus, we have
P (|f (x) - Ef (x)| > t) ≤ 2exP
t2
2α2τ2
, (∀t ≥ 0),
where α
116看CyCd (1 + cr(1 一 1)) in which Cy, Cd and Cr are defined in Lemma 16. Thus,
n Pin,=ι f (wj, x(i)) 一 E(f (wj, x)) is 8α2τ2/n-sub-Gaussian random variable. Thus, We can obtain
P(II X f (wj, x(i)) -
E(f(wj, x)) ≥ 3) ≤ 2eχp (-9
i=1
Notice Plj=1	sj = s. In this case, the probability of E2 is upper bounded as
nt2	3dr
P(E2) ≤ 2exp (--80272 + S log ( — ))-
Thus, if we set
then we have
t ≥ ατ∖/18 (Slog(3dr∕e) + log4Zl
n
p (E2) ≤ ε.
Step 3. Bound P (E3): We first bound P (E3) as follows:
P(E3) =P (Sup ∣E(f(wkw, x)) - E(f(w, x))| ≥ t
∖w∈Ω	3
=P suP
∖w∈Ω
|E(f(wkw,x) -f(w,x))|
kw - Wkwk2
suP kw 一 wkw k2
w∈Ω
—
t
≥ —
一3
≤P (eE sup ∣∣VJw(w, x)k2 ≥ I
∖ w∈Ω	3
≤ P (αe ≥t),
57
Published as a conference paper at ICLR 2018
where ① holds since by Lemma 16, for arbitrary X and W ∈ Ω, we have ∣∣Vwf (w, x)k2 ≤ α. We
set enough small such that α < t/3 always holds. Then it yields P (E3) = 0.
Step 4. Final result: Notice, we have 等 ≥ 3αe. To ensure P(E0) ≤ ε, wejust set e = 3r/n and
t ≥ max(601,ατ J18(S log(3dr/e) +log(4/。)=皿@*",.「J 18(s log(nd)+logG4∕≡
ε	n	nε	n
Therefore, if n ≥ 18l2r2/(s log(d)τ2ε2 log(1∕ε)), then
SUp Jn(W) — J (w)| ≤ T J 8 Cy Cd (1 + Cr(l — 1)) J S log(nd/l)n+ log(4/W
holds with probability at least 1 - ε, where cy, cd, and cr are defined as
∣v(l) - y ∣22≤	Cy	<	+∞,	Cd = max(do,	di,…，di) and	Cr	= max
The proof is completed.	□
D.3.4 Proof of Corollary 2
Proof. By Lemma 5, we know es = eg. Thus, the remaining work is to bound es. Actually, we can
have
≤Es〜D ( sup IJn(W) - J(W)
w∈Ω
≤ sup Jn (w) — J (w)
w∈Ω
≤en.
Thus, we have eg = es ≤ en . The proof is completed.
□
D.4 Proof of Other Lemmas
D.4.1 Proof of Lemma 22
Proof. Since G(u(i)) is a diagonal matrix and its diagonal values are upper bounded by σ(u(hi))(1 -
σ(u(hi))) ≤ 1/4 where u(hi) denotes the h-th entry of u(i), we can conclude
∣G(u(i))MkF ≤ ɪ∣MkF and ∣NG(u(i))kF ≤ ɪ∣NkF.
16	16
Note that Pk is a matrix of size dk X dk whose ((S — 1)dk + s, s) (s = 1, ∙∙∙ , dk) entry equal to
σ(u(sk))(1 - σ(u(sk)))(1 - 2σ(u(sk))) and rest entries are all 0. This gives
σ(uSk))(1 — σ(uSk)))(1 - 2σ(uSk))) =∣(3σ(uSk)))(1 - σ(uSk)))(1 - 2σ(uSk)))
≤ 3(
23
≤ 34.
3
3σ(us ) + 1 - σ(us ) + 1 - 2σ(us )
3
This means the maximal value in Pk is at most 33. Consider the structure in Pk, we can obtain
26	26
∣PkMkF ≤ 38kMkF and ∣NPkkF ≤ 38∣NkF∙
58
Published as a conference paper at ICLR 2018
As for Bs:t, we have
kBs：tkF ≤kAskFkAs+ιkF …kAtkF
= ∣∣(W S)TG(U(S)U2 ∣∣(W (S + I))TG(U(S + I)U2 ∙∙∙ ∣∣(W ㈤)T G(u(t))∣∣2
/	1
≤ i6t-S+ι
_	1
16t-S+1
W (S)∣∣F∣∣w (S+1)∣∣F ∙∙∙∣∣w ⑴
2
F
Since the '2-norm of each wj) is bounded, i.e. kw(j) ∣∣2 ≤ r,we can obtain
1 D t≤	1	r2(t-S+i) = Uy(J+1), C
I6t-S+1 DS:t ≤ 16t-S+1r	= 4)	~t'st-
Now we prove the final result. According to the property of Kronecker product that for any matrices
A, B and X of proper sizes, Vec (AXB) = (BT ③ A)Vec (X), we have
vec (MNT) = (N 乳 I)Vec (M) = (N 型 I)m.
This further yields
k(N 乳 I)m∣F = kvec (MNT) kF = ∣MNTkF ≤ IMkF ∣NkF .
By similar way, we can obtain
k(I 乳 N )mkF ≤ kM kF kN kF .
The proof is completed.	□
D.4.2 Proof of Lemma 23
Proof. By utilizing the chain rule in Eqn. (24) in Sec. D.2.1, we can easily compute fUWR and
fwX as follows:	It
∂v(i)
f Wi X) = G(u(i))Ai+ι ∙∙∙ Al(V(I)- y) = G(U⑴)Bi+i：i(V(I)- y)
∂ U(i)
and
f W?) = A'+1•…Al(V(I) - y) = Bi+i：l(v(l) - y).
Therefore, we can further obtain
∂f(w, x)
d w(j)
=Vec((G(Uj))Aj+1Aj+2 …Al(V(I)- y)) (v(jT))T)
=Vec ((G(Uj))Aj+1Aj+2 …Ai-I(W(i))T)(G(U⑺)Ai+1 …Al(V(I)- y)) (v(jT))T)
=(VjT)乳(G(Uj))Aj+ιAj+2 …Ai-1 (W(i))T))Vec (G(U(i))Ai+ι ∙∙∙ Al(V(I)- y))
=(VjT) “G(Uj))Aj+1Aj+2 …Ai-I(W⑺)T)) (fW,)x)).
Nntp that u/p h df(w,x) ∂u⑶ df(w,x) ThX Oi Qa
NOte that we have ∂wj) = ∂Wj) (-∂U(i) .This gives
IU(I = (VjT))T 0 (G(Uj))Bj+i：i-i(W⑴)T)T ∈ Rdi×djdj-1 (i > j).
When i = j , we have
丁㈠=(V(iT))T 0 Idi ∈ Rdi×didi-1.
Similarly, we can obtain
(i)	T	T
∂W^ = (VjT))T0(G(Uj))Aj+1Aj+2 …Ai) =(VjT) )t0 (G(Uj))Bj+1”) ∈Rdi×dj d。-1 (i ≥ j).
The proof is completed.	□
59
Published as a conference paper at ICLR 2018
D.4.3 Proof of Lemma 24
Proof. By Lemma 23, we have
df(w)x) = G(u(i))Bi+i：，(v(l)-y) and fw^ = Ba.
∂u(i)	∂v(i)
Therefore, we can further obtain
fwx) =G(U ⑴)A2 …AlS y)
∂ u(1)
=G(U(I))A2 …Aj-ι(Wj)TG(u⑶)Aj+ι …Al(V(I)- y)
=(G(U(I))A2 …Aj-I(Wj)T)(d⅛)).
Note that We have df(wx) = (∂uj) T ( df(wχ)) .This gives
∂u(1)	∂u(1)	∂u(j)
(j)	T	T
du(ŋ = (G(U(I))A2 …Aj-I(Wj)τ) = (G(U(I))B2j-1(Wj)τ) ∈ Rdj×d∣ (j > 1).
Similarly, We can obtain
黑=(g(u(1))A2 …Aj)T = (G(U(I))B2j)T e Rdj×d1 (j > 1).
The proof is completed.	□
60