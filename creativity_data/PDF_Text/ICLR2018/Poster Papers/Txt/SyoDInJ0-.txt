Published as a conference paper at ICLR 2018
Reinforcement Learning Algorithm Selection
Romain Laroche1 and Raphael Feraud2
1	Microsoft Research, MontreaL Canada
2	Orange Labs, Lannion, France
Ab stract
This paper formalises the problem of online algorithm selection in the context of
Reinforcement Learning (RL). The setup is as follows: given an episodic task and
a finite number of off-policy RL algorithms, a meta-algorithm has to decide which
RL algorithm is in control during the next episode so as to maximize the expected
return. The article presents a novel meta-algorithm, called Epochal Stochastic
Bandit Algorithm Selection (ESBAS). Its principle is to freeze the policy updates
at each epoch, and to leave a rebooted stochastic bandit in charge of the algorithm
selection. Under some assumptions, a thorough theoretical analysis demonstrates
its near-optimality considering the structural sampling budget limitations. ESBAS
is first empirically evaluated on a dialogue task where it is shown to outperform
each individual algorithm in most configurations. ESBAS is then adapted to a true
online setting where algorithms update their policies after each transition, which
we call SSBAS. SSBAS is evaluated on a fruit collection task where it is shown to
adapt the stepsize parameter more efficiently than the classical hyperbolic decay,
and on an Atari game, where it improves the performance by a wide margin.
1	Introduction
Reinforcement Learning (RL, Sutton & Barto (1998)) is a machine learning framework for optimising
the behaviour of an agent interacting with an unknown environment. For the most practical problems,
such as dialogue or robotics, trajectory collection is costly and sample efficiency is the main key
performance indicator. Consequently, when applying RL to a new problem, one must carefully choose
in advance a model, a representation, an optimisation technique and their parameters. Facing the
complexity of choice, RL and domain expertise is not sufficient. Confronted to the cost of data, the
popular trial and error approach shows its limits.
We develop an online learning version (Gagliolo & Schmidhuber, 2006; 2010) of Algorithm Selection
(AS, Rice (1976); Smith-Miles (2009); Kotthoff (2012)). It consists in testing several algorithms on
the task and in selecting the best one at a given time. For clarity, throughout the whole article, the
algorithm selector is called a meta-algorithm, and the set of algorithms available to the meta-algorithm
is called a portfolio. The meta-algorithm maximises an objective function such as the RL return.
Beyond the sample efficiency objective, the online AS approach besides addresses four practical
problems for online RL-based systems. First, it improves robustness: if an algorithm fails to terminate,
or outputs to an aberrant policy, it will be dismissed and others will be selected instead. Second,
convergence guarantees and empirical efficiency may be united by covering the empirically efficient
algorithms with slower algorithms that have convergence guarantees. Third, it enables curriculum
learning: shallow models control the policy in the early stages, while deep models discover the best
solution in late stages. And four, it allows to define an objective function that is not an RL return.
A fair algorithm selection implies a fair budget allocation between the algorithms, so that they can
be equitably evaluated and compared. In order to comply with this requirement, the reinforcement
algorithms in the portfolio are assumed to be off-policy, and are trained on every trajectory, regardless
which algorithm controls it. Section 2 provides a unifying view of RL algorithms, that allows infor-
mation sharing between algorithms, whatever their state representations and optimisation techniques.
It also formalises the problem of online selection of off-policy RL algorithms.
1
Published as a conference paper at ICLR 2018
Next, Section 3 presents the Epochal Stochastic Bandit AS (ESBAS), a novel meta-algorithm
addressing the online off-policy RL AS problem. Its principle relies on a doubling trick: it divides the
time-scale into epochs of exponential length inside which the algorithms are not allowed to update
their policies. During each epoch, the algorithms have therefore a constant policy and a stochastic
multi-armed bandit can be in charge of the AS with strong pseudo-regret theoretical guaranties. A
thorough theoretical analysis provides for ESBAS upper bounds. Then, Section 4 evaluates ESBAS
on a dialogue task where it outperforms each individual algorithm in most configurations.
Afterwards, in Section 5, ESBAS, which is initially designed for a growing batch RL setting, is
adapted to a true online setting where algorithms update their policies after each transition, which we
call SSBAS. It is evaluated on a fruit collection task where it is shown to adapt the stepsize parameter
more efficiently than the classical hyperbolic decay, and on Q*bert, where running several DQN with
different network size and depth in parallel allows to improve the final performance by a wide margin.
Finally, Section 6 concludes the paper with prospective ideas of improvement.
2	Algorithm Selection for RL
2.1 Unifying view of RL algorithms
The goal of this section is to enable information
sharing between algorithms, even though they
are considered as black boxes. We propose to
share their trajectories expressed in a universal
format: the interaction process.
Reinforcement Learning (RL) consists in learn-
ing through trial and error to control an agent
behaviour in a stochastic environment: at each
蓝 Agent --------------
o(t + 1)	r(t + 1)	a(t)
--------- Stochastic 
environment
Figure 1: RL framework: after performing action
a(t), the agent perceives observation o(t + 1) and
receives reward r(t + 1).
time step t ∈ N, the agent performs an action
a(t) ∈ A, and then perceives from its environment a signal o(t) ∈ Ω called observation, and receives
a reward r(t) ∈ R, bounded between Rmin and Rmax. Figure 1 illustrates the RL framework. This
interaction process is not Markovian: the agent may have an internal memory.
In this article, the RL problem is assumed to be episodic. Let us introduce two time scales with
different notations. First, let us define meta-time as the time scale for AS: at one meta-time τ
corresponds a meta-algorithm decision, i.e. the choice of an algorithm and the generation of a full
episode controlled with the policy determined by the chosen algorithm. Its realisation is called
a trajectory. Second, RL-time is defined as the time scale inside a trajectory, at one RL-time t
corresponds one triplet composed of an observation, an action, and a reward.
Let E denote the space of trajectories. A trajectory ετ ∈ E collected at meta-time τ is formalised
as a sequence of (observation, action, reward) triplets: ε = hθτ(t), aτ(t),rT(t)it∈jι,∣ετ∣k ∈ E,
where ∣ετ | is the length of trajectory ε. The objective is, given a discount factor 0 ≤ γ < 1,
to generate trajectories with high discounted cumulative reward, also called return, and noted
μ(ετ) = Ptετ1 γt-1rτ(t). Since γ < 1 and R is bounded, the return is also bounded. The trajectory
set at meta-time T is denoted by DT = {ετ}τ ∈J1,T K ∈ ET. A sub-trajectory of ετ until RL-time t is
called the history at RL-time t and written £「(t) with t ≤ ∣ε". The history records what happened in
episode ετ until RL-time t: ετ (t) = hoτ (t0), aτ (t0), rτ (t0)it0∈J1,tK ∈ E.
The goal of each RL algorithm α is to find a policy ∏* : E → A which yields optimal expected
returns. Such an algorithm α is viewed as a black box that takes as an input a trajectory set D ∈ E+,
where E+ is the ensemble of trajectory sets of undetermined size: E+ = ST∈N ET, and that outputs
a policy πDα . Consequently, a RL algorithm is formalised as follows: α : E+ → (E → A).
Such a high level definition of the RL algorithms allows to share trajectories between algorithms: a
trajectory as a sequence of observations, actions, and rewards can be interpreted by any algorithm
in its own decision process and state representation. For instance, RL algorithms classically rely
on an MDP defined on a explicit or implicit state space representation SDα thanks to a projection
ΦαD : E → SDα . Then, α trains its policy πDαT on the trajectories projected on its state space
representation. Off-policy RL optimisation techniques compatible with this approach are numerous in
2
Published as a conference paper at ICLR 2018
the literature (Watkins, 1989; Ernst et al., 2005; Mnih et al., 2013). As well, any post-treatment of the
state set, any alternative decision process (Lovejoy, 1991), and any off-policy algorithm may be used.
The algorithms are defined here as black boxes and the considered meta-algorithms will be indifferent
to how the algorithms compute their policies, granted they satisfy the off-policy assumption.
2.2 Online algorithm selection
The online learning approach is tackled in
this article: different algorithms are experi-
enced and evaluated during the data collec-
tion. Since it boils down to a classical explo-
ration/exploitation trade-off, multi-armed ban-
dit (Bubeck & Cesa-Bianchi, 2012) have been
used for combinatorial search AS (Gagliolo &
Schmidhuber, 2006; 2010) and evolutionary al-
gorithm meta-learning (Fialho et al., 2010). The
online AS problem for off-policy RL is novel
and we define it as follows:
Pseudo-code 1: Online RL AS setting
Data: Do ^ 0: trajectory set
Data: PJ {αk}k∈jι,κ^: algorithm portfolio
Data: μ : E → R: the objective function
for τ J 1 to ∞ do
Select σ (Dτ-1) = σ(τ) ∈ P;
Generate trajectory ετ with policy πDσ(ττ ) ;
Get return μ(ετ);
Dτ J Dτ-1 ∪ {ετ};
end
•	D ∈ E+ is the current trajectory set;
•	P = {αk}k∈J1,KK is the portfolio of off-policy RL algorithms;
•	μ : E → R is the objective function, generally set as the RL return.
Pseudo-code 1 formalises the online RL AS setting. A meta-algorithm is defined as a function from
a trajectory set to the selection of an algorithm: σ : E+ → P. The meta-algorithm is queried at
each meta-time T = |D「-ι |+1, with input DT_1, and it ouputs algorithm σ (DT-ι) = σ(τ) ∈ P
controlling with its policy πDσ (ττ) the generation of the trajectory ετ in the stochastic environment.
Figure 2 illustrates the algorithm with a diagram flow. The final goal is to optimise the cumulative
expected return. It is the expectation of the sum of rewards obtained after a run of T trajectories:
T
Eσ fμ(ετ) = Eσ
T=1
T
∑EμD(σ-
T=1
(1)
with EμD = Ena [μ (ε)] as a condensed notation for the expected return of policy ∏D, trained on
trajectory set D by algorithm α. Equation 1 transforms the cumulative expected return into two nested
Figure 2: Algorithm selection for reinforcement learning flow diagram
3
Published as a conference paper at ICLR 2018
expectations. The outside expectation Eσ assumes the meta-algorithm σ fixed and averages over
the trajectory set generation and the corresponding algorithms policies. The inside expectation Eμ
assumes the policy fixed and averages over its possible trajectories in the stochastic environment.
Nota bene: there are three levels of decision: meta-algorithm σ selects algorithm α that computes
policy π that is in control. In this paper, the focus is at the meta-algorithm level.
2.3 Meta-algorithm evaluation
In this paper, we focus on sample efficiency, where a sample is meant to be a trajectory. This is
motivated by the following reasons. First, in most real-world systems, the major regret is on the task
failure. The time expenditure is only a secondary concern that is already assessed by the discount
factor dependency in the return. Second, it would be inconsistent to consider regret on a different
time scale as the algorithm selection. Also, policy selection on non-episodic RL is known as a very
difficult task where state-of-the-art algorithms only obtain regrets of the order of O(√Tlog(T)) on
stationary policies Azar et al. (2013). Third, the regret on the decision steps cannot be assessed, since
the rewards are discounted in the RL objective function. And finally, the bandit rewards (defined as
the objective function in Section 2.2) may account for the length of the episode.
In order to evaluate the meta-algorithms, let us formulate two additional notations. First, the optimal
expected return Eμ∞ is defined as the highest expected return achievable by a policy of an algorithm
in portfolio P. Second, for every algorithm α in the portfolio, let us define σα as its canonical meta-
algorithm, i.e. the meta-algorithm that always selects algorithm α: ∀τ, σα (τ) = α. The absolute
pseudo-regret Pfbs (T) defines the regret as the loss for not having controlled the trajectory with an
optimal policy:
T
Pabs(T)= TEμ∞ - Eσ £e〃D(；)	(2)
τ-1
τ =1
It is worth noting that an optimal meta-algorithm will unlikely yield a null regret because a large
part of the absolute pseudo-regret is caused by the sub-optimality of the algorithm policies when
the trajectory set is still of limited size. Indeed, the absolute pseudo-regret considers the regret for
not selecting an optimal policy: it takes into account both the pseudo-regret of not selecting the best
algorithm and the pseudo-regret of the algorithms for not finding an optimal policy. Since the meta-
algorithm does not interfere with the training of policies, it ought not account for the pseudo-regret
related to the latter.
2.4 Related work
Related to AS for RL, Schweighofer & Doya (2003) use meta-learning to tune a fixed RL algorithm
in order to fit observed animal behaviour, which is a very different problem to ours. In Cauwet et al.
(2014); Liu & Teytaud (2014), the RL AS problem is solved with a portfolio composed of online
RL algorithms. The main limitation from these works relies on the fact that on-policy algorithms
were used, which prevents them from sharing trajectories among algorithms (Cauwet et al., 2015).
Meta-learning specifically for the eligibility trace parameter has also been studied in White & White
(2016). Wang et al. (2016) study the learning process of RL algorithms and selects the best one for
learning faster on a new task. This work is related to batch AS.
An intuitive way to solve the AS problem is to consider algorithms as arms in a multi-armed bandit
setting. The bandit meta-algorithm selects the algorithm controlling the next trajectory ε and the
objective function μ(ε) constitutes the reward of the bandit. The aim of prediction with expert advice
is to minimise the regret against the best expert of a set of predefined experts. When the experts learn
during time, their performances evolve and hence the sequence of expert rewards is non-stationary.
The exponential weight algorithms (Auer et al., 2002b; Cesa-Bianchi & Lugosi, 2006) are designed
for prediction with expert advice when the sequence of rewards of experts is generated by an oblivious
adversary. This approach has been extended for competing against the best sequence of experts by
adding in the update of weights a forgetting factor proportional to the mean reward (see Exp3.S in
Auer et al. (2002b)), or by combining Exp3 with a concept drift detector Allesiardo & Feraud (2015).
4
Published as a conference paper at ICLR 2018
The exponential weight algorithms have been extended to the case where the rewards are generated
by any sequence of stochastic processes of unknown means (Besbes et al., 2014).
A recent article Graves et al. (2017) uses Exp3.S algorithm Auer et al. (2002b) for curriculum
learning Bengio et al. (2009). The drawback of adversarial approaches is that they lead to very
conservative algorithms which has to work against an adversary.
For handeling non-stationarity of rewards, another way is to assume that the rewards generated by
each arm are not i.i.d., but are governed by some more complex stochastic processes. The stochastic
bandit algorithm such as UCB can be extended to the case of switching bandits using a discount
factor or a window to forget the past Garivier & Moulines (2011). Restless bandits Whittle (1988);
Ortner et al. (2012) assume that a Markov chain governs the reward of arms independently of whether
the learner is played or not the arm. These classes of bandit algorithms are not designed for experts
that learn and hence evolve at each time step.
Our approach takes the opposite view of adversarial bandits: we design a stochastic algorithm
specifically for curriculum learning based on the doubling trick. This reduction of the algorithm
selection problem into several stochastic bandit problems with doubling time horizon begins to favour
fast algorithms, and then more efficient algorithms.
3 Epochal Stochastic Bandit
ESBAS description - To solve the off-policy RL AS problem, We propose a novel meta-algorithm
called Epochal Stochastic Bandit AS (ESBAS). Because of the non-stationarity induced by the
algorithm learning, the stochastic bandit cannot directly select algorithms. Instead, the stochastic
bandit can choose fixed policies. To comply With this constraint, the meta-time scale is divided into
epochs inside Which the algorithms policies cannot be updated: the algorithms optimise their policies
only When epochs start, in such a Way that the policies are constant inside each epoch. This can be
seen as a doubling trick. As a consequence and since the returns are bounded, at each neW epoch, the
problem can rigorously be cast into an independent stochastic K-armed bandit Ξ, With K = |P |.
The ESBAS meta-algorithm is formally
sketched in Pseudo-code 2 embedding
UCB1 Auer et al. (2002a) as the stochastic K-
armed bandit Ξ. The meta-algorithm takes as
an input the set of algorithms in the portfolio.
Meta-time scale is fragmented into epochs of
exponential size. The βth epoch lasts 2β meta-
time steps, so that, at meta-time τ = 2β , epoch
β starts. At the beginning of each epoch, the
ESBAS meta-algorithm asks each algorithm
in the portfolio to update their current policy.
Inside an epoch, the policy is never updated
anymore. At the beginning of each epoch, a
neW Ξ instance is reset and run. During the
Whole epoch, Ξ selects at each meta-time step
the algorithm in control of the next trajectory.
Pseudo-code 2: ESBAS With UCB1
Data: Do, P, μ: the online RL AS setting
for β — 0 to ∞ do
for αk ∈ P do
I ∏DJ policy learnt by ak on D2β-ι
end
Theoretical analysis - ESBAS intends to
minimise the regret for not choosing the algo-
rithm yielding the maximal return at a given
meta-time τ . It is short-sighted: it does not
intend to optimise the algorithms learning. We
define the short-sighted pseudo-regret as fol-
loWs:
n — 0, ∀ak ∈ P, nk — 0, and Xk - 0
for T _ 2β to 2β+1 - 1 do
k	( k l log(n)ʌ
αkmaχ = argmaχ Xk +ʌ ξ---------
αk∈P	nk
Generate trajectory ετ With policy πDkmβax
Get return μ(ετ), DT - Dτ-1 ∪ {ετ}
k	nkmax Xkmax + μ㈡)
Xkmax 《_ __________________
nkmax + 1
nkmax _ nkmax + 1 and n _ n + 1
end
end
Ps(T) = Eσ
T
∑(max EμDσ-ι- EμD(τ)ι
τ=1
(3)
5
Published as a conference paper at ICLR 2018
The short-sighted pseudo-regret depends on the gaps ∆βα: the difference of expected return between
the best algorithm during epoch β and algorithm α. The smallest non null gap at epoch β is noted
△e. We write its limit when β tends to infinity with ∆∞.
Analysis relies on three assumptions that are formalised in Section B of the supplementary material.
First, more data is better data states that algorithms improve on average from having additional data.
Second, order compatibility assumes that, if a dataset enables to generate a better policy than another
dataset, then, on average, adding new samples to both datasets should not change the dataset ordering.
Third and last, let us introduce and discuss more in depth the learning is fair assumption. The fairness
of budget distribution has been formalised in Cauwet et al. (2015). It is the property stating that every
algorithm in the portfolio has as much resources as the others, in terms of computational time and
data. It is an issue in most online AS problems, since the algorithm that has been the most selected
has the most data, and therefore must be the most advanced one. A way to circumvent this issue is to
select them equally, but, in an online setting, the goal of AS is precisely to select the best algorithm as
often as possible. Our answer is to require that all algorithms in the portfolio are learning off-policy,
i.e. without bias induced by the behavioural policy used in the learning dataset. By assuming that all
algorithms learn off-policy, we allow information sharing Cauwet et al. (2015) between algorithms.
They share the trajectories they generate. As a consequence, we can assume that every algorithm,
the least or the most selected ones, will learn from the same trajectory set. Therefore, the control
unbalance does not directly lead to unfairness in algorithms performances: all algorithms learn equally
from all trajectories. However, unbalance might still remain in the exploration strategy if, for instance,
an algorithm takes more benefit from the exploration it has chosen than the one chosen by another
algorithm. For analysis purposes, we assumes the complete fairness of AS.
Based on those assumptions, three theorems show that ESBAS absolute pseudo-regret can be ex-
pressed in function of the absolute pseudo-regret of the best canonical algorithm and ESBAS short-
sighted pseudo-regret. They also provide upper bounds on the ESBAS short-sighted pseudo-regret
as a function of the order of magnitude of the gap ∆g. Indeed, the stochastic multi-armed bandit
algorithms have bounds that are, counter-intuitively, inversely proportional to the gaps between the
best arm and the other ones. In particular if ∆g tends to 0, the algorithm selection might prove to be
difficult, depending on the order of magnitude of it tending to 0. The full theoretical analysis can be
found in the supplementary material, Section B. We provide here an intuitive overlook of its results.
Table 1 numerically reports those bounds for a two-fold portfolio, depending on the nature of the
algorithms. It must be read by line. According to the first column: the order of magnitude of △, the
ESBAS short-sighted pseudo-regret bounds are displayed in the second column, and the third and
fourth columns display the ESBAS absolute pseudo-regret bounds also depending on the order of
magnitude of the best canonical algorithm absolute pseudo-regret: pσ,bs(T)∙
Regarding the short-sighted upper bounds, the main result appears in the last line, when the algorithms
converge to policies with different performance: ESBAS converges with a regret in O (log2 (T)/∆∞).
Also, one should notice that the bounds of the first two lines are obtained by summing the gaps:
this means that the algorithms are perceived equally good and that their gap goes beyond the
threshold of distinguishability. This threshold is structurally at ∆g ∈ O(1∕∕T). The impossibility
to determine which is the better algorithm is interpreted in Cauwet et al. (2014) as a budget issue.
The meta-time necessary to distinguish through evaluation arms that are ∆g apart takes Θ(1∕∆β2)
meta-time steps. If the budget is inferior, then we are under the distinguishability threshold and
the best bounds are obtained by summing up the gaps. As a consequence, if ∆g ∈ O(1∕∕T), then
∖/△； ∈ Ω(T). However, the budget, i.e. the length of epoch β starting at meta-time T = 2β, equals
T. ∆β ∈ O(1∕∕T) can therefore be considered as the structural limit of distinguishability between
the algorithms.
Additionally, the absolute upper bounds are logarithmic in the best case and still inferior to O(∕T) in
the worst case, which compares favorably with those of discounted UCB and Exp3.S in O( T log(T))
and Rexp3 in O(T2/3), or the RL with Policy Advice,s regret bounds of O(√T log(T)) on stationary
policies Azar et al. (2013) (on non-episodic RL tasks).
6
Published as a conference paper at ICLR 2018
ESBAS	ESBAS
Table 1: Bounds on p：§	(T) and 祝^	(T) given various settings for a two-fold portfolio AS.
△e		/SBAS ,一、 ρσs	(τ)	∣ESBAS ,一、	b* ,一、 ^Pabs	(T) in functiOn Of Pabs(T)	
			_ * ,	.	,	,一 Pabs(T) ∈ o(iog(TL	ρa*s(T) ∈O(T1-c*)
Θ(1∕T)		O (log(T))	O (log(T))	
Θ(T	c'), and Ct ≥ 0.5	O(T1-ct)	O(T1-J)	O(TI-J)
Θ(T -	c'), and Ct < 0.5	O(τJ log(T))	O(T Jlog(T))	O(T 1-c* ),if Ct < 1 - C* O(TJlog(T)), if Ct ≥ 1 - c*
Θ⑴		O (log2(T)∕∆∞)	O (log2(T)∕∆∞)	O(T 1-c*)
4	ESBAS Dialogue Experiments
ESBAS is particularly designed for RL tasks when it is impossible to update the policy after every
transition or episode. Policy update is very costly in most real-world applications, such as dialogue
systems (Khouzaimi et al., 2016) for which a growing batch setting is preferred (Lange et al.,
2012). ESBAS practical efficiency is therefore illustrated on a dialogue negotiation game (Laroche
& Genevay, 2016) that involves two players: the system ps and a user pu . Their goal is to find an
agreement among 4 alternative options. At each dialogue, for each option η, players have a private
uniformly drawn cost Vp ~ U [0,1] to agree on it. Each player is considered fully empathetic to the
other one. The details of the experiment can be found in the supplementary material, Section C.1.1.
All learning algorithms are using Fitted-Q Iteration (Ernst et al., 2005), with a linear parametri-
sation and an β-greedy exploration : β = 0.6β, β being the epoch number. Several algorithms
differing by their state space representation Φα are considered: simple, fast, simple-2, fast-2, n-ζ-
{simple/fast/Simple-2∕fast-2}, and constant-μ. See Section C.1.2 for their full descriptions.
The algorithms and ESBAS are playing with a stationary user simulator built through Imitation
Learning from real-human data. All the results are averaged over 1000 runs. The performance figures
plot the curves of algorithms individual performance σα against the ESBAS portfolio control σESBAS
in function of the epoch (the scale is therefore logarithmic in meta-time). The performance is the
average return of the RL problem. The ratio figures plot the average algorithm selection proportions
of ESBAS at each epoch. We define the relative pseudo regret as the difference between the ESBAS
absolute pseudo-regret and the absolute pseudo-regret of the best canonical meta-algorithm. Relative
pseudo-regrets have a 95% confidence interval about ±6 ≈ ±1.5 × 10-4 per trajectory. Extensive
numerical results are provided in Table 2 of the supplementary material.
Figures 3a and 3b plot the typical curves obtained with ESBAS selecting from a portfolio of two
learning algorithms. On Figure 3a, the ESBAS curve tends to reach more or less the best algorithm
in each point as expected. Surprisingly, Figure 3b reveals that the algorithm selection ratios are not
very strong in favour of one or another at any time. Indeed, the variance in trajectory set collection
makes simple better on some runs until the end. ESBAS proves to be efficient at selecting the best
algorithm for each run and unexpectedly obtains a negative relative pseudo-regret of -90. Figures
3c and 3d plot the typical curves obtained with ESBAS selecting from a portfolio constituted of a
learning algorithm and an algorithm with a deterministic and stationary policy. ESBAS succeeds in
remaining close to the best algorithm at each epoch and saves 5361 return value for not selecting
the constant algorithm, but overall yields a regret for not using only the best algorithm. ESBAS also
performs well on larger portfolios of 8 learners (see Figure 3e) with negative relative pseudo-regrets:
-10, even if the algorithms are, on average, almost selected uniformly as Figure 3f reveals. Each
individual run may present different ratios, depending on the quality of the trained policies. ESBAS
also offers some curriculum learning, but more importantly, early bad policies are avoided.
Algorithms with a constant policy do not improve over time and the full reset of the K-multi armed
bandit urges ESBAS to unnecessarily explore again and again the same underachieving algorithm.
One easy way to circumvent this drawback is to use this knowledge and to not reset their arms. By
operating this way, when the learning algorithm(s) start(s) outperforming the constant one, ESBAS
7
Published as a conference paper at ICLR 2018
(3a) simple vs simple-2
(3b) simple vs simple-2
(3d) simple-2 vs constant-1.009
(3e) 8 learners
Figure 3: The figures on the top plot the performance over time. The figures on the bottom show the
ESBAS selection ratios over the epochs.
(3f) 8 learners
simply neither exploits nor explores the constant algorithm anymore. Without arm reset for constant
algorithms, ESBAS’s learning curve follows perfectly the learning algorithm’s learning curve when
this one outperforms the constant algorithm and achieves strong negative relative pseudo-regrets.
Again, the interested reader may refer to Table 2 in supplementary material for the numerical
results. Still, another harmful phenomenon may happen: the constant algorithm overrides the natural
exploration of the learning algorithm in the early stages, and when the learning algorithm finally
outperforms the constant algorithm, its exploration parameter is already low. This can be observed in
experiments with constant algorithm of expected return inferior to 1, as reported in Table 2.
5	Sliding Stochastic Bandit Algorithm Selection
We propose to adapt ESBAS to a true online setting where algorithms update their policies after each
transition. The stochastic bandit is now trained on a sliding window containing the last τ /2 selections.
Even though the arms are not stationary over this window, the bandit eventually forgets the oldest
arm pulls. This algorithm is called SSBAS for Sliding Stochastic Bandit AS. Despite the lack of
8
Published as a conference paper at ICLR 2018
Q-Learning with learning rate: 0.001
Q-Learning with learning rate: 0.01
□ Q-Learning with learning rate: 0.1
Q-Learninq with learning rate: 0.5
20000
40000
60000
80000
100000
Figure 5: ratios (averaged over 3000 runs) obtained with SSBAS on the gridworld task.
theoretical convergence bounds, we demonstrate on two domains and two different meta-optimisation
tasks that SSBAS impressively outperforming all algorithms in the portfolio by a wide margin.
5.1	Gridworld domain
The goal here is to demonstrate that SSBAS can perform efficient
hyper-parameter optimisation on a simple tabular domain: a 5x5
gridworld problem (see Figure 4), where the goal is to collect the
fruits placed at each corner as fast as possible. The episodes terminate
when all fruits have been collected or after 100 transitions. The
objective function μ used to optimise the stochastic bandit Ψ is no
longer the RL return, but the time spent to collect all the fruits (200
in case of it did not). The agent has 18 possible positions and there
are 24 - 1 = 15 non-terminal fruits configurations, resulting in 270
states. The action set is A = {N, E, S, W }. The reward function
mean is 1 when eating a fruit, 0 otherwise. The reward function is
corrupted with a strong Gaussian white noise of variance ζ2 = 1.
The portfolio is composed of 4 Q-learning algorithms varying from
each other by their learning rates: {0.001, 0.01, 0.1, 0.5}. They all
have the same linearly annealing τ -greedy exploration.
Figure 4: gridworld
The selection ratios displayed in 5 show that SSBAS selected the algorithm with the highest (0.5)
learning rate in the first stages, enabling to propagate efficiently the reward signal through the visited
states, then, over time preferentially chooses the algorithm with a learning rate of 0.01, which is
less sensible to the reward noise, finally, SSBAS favours the algorithm with the finest learning rate
(0.001). After 1 million episodes, SSBAS enables to save half a transition per episode on average
as compared to the best fixed learning rate value (0.1), and two transitions against the worst fixed
learning rate in the portfolio (0.001).
We compare SSBAS to the efficiency of a linearly annealing learning rate: 1/(1 + 0.0001τ): SSBAS
performs under 21 steps on average after 105, while the linearly annealing learning rate algorithm still
performs a bit over 21 steps after 106 steps. This is because SSBAS can adapt the best performing
learning rate over time.
9
Published as a conference paper at ICLR 2018
Figure 7: Comparative performance over time of SSBAS versus the algorithms in its portfolio on
Q*bert (1 run).
We also compare SSBAS performance to Exp3.S’s performance (Auer et al., 2002b). The analysis of
the algorithm selection history shows that Exp3.S is too conservative and fails at efficiently select
the shallowest algorithms in the beginning of the learning (number of steps at the 10000th episode:
28.3 for SSBAS vs 39.1 for Exp3.S), producing trajectories of lesser quality and therefore critically
delaying the general training of all algorithms (number of steps at the 100000th episode: 20.9 for
SSBAS vs 22.5 for Exp3.S). Overall, SSBAS outperforms Exp3.S by a significant and wide margin:
number of steps averaged over all the training 10^5 episodes: 28.7 for SSBAS vs 33.6 for Exp3.S.
5.2	Atari domain: Q*bert
We investigate here AS for deep RL on the Arcade Learning Envi-
ronment (ALE, Bellemare et al. (2013)) and more precisely the game
Q*bert (see a frame on Figure 6), where the goal is to step once on
each block. Then a new similar level starts. In later levels, one needs
to step twice on each block, and even later stepping again on the
same blocks will cancel the colour change. We used three different
settings of DQN instances: small uses the setting described in Mnih
et al. (2013), large uses the setting in Mnih et al. (2015), and finally
huge uses an even larger network (see the supplementary material,
Section C.2 for details). DQN is known to reach a near-human level
performance at Q*bert.
Our SSBAS instance runs 6 algorithms with 2 different random
initialisations of each DQN setting. Disclaimer: contrarily to other
experiments, each curve is the result of a single run, and the im-
provement might be aleatory. Indeed, the DQN training is very
long and SSBAS needs to train all the models in parallel. A more
computationally-efficient solution might be to use the same archi-
tecture as Osband et al. (2016).
Figure 6: Q*bert
7 reveals that SSBAS experiences a slight delay keeping in touch with the best setting performance
during the initial learning phase, but, surprisingly, finds a better policy than the single algorithms
in its portfolio and than the ones reported in the previous DQN articles. We observe that the large
setting is surprisingly by far the worst one on the Q*bert task, implying the difficulty to predict which
model is the most efficient for a new task. SSBAS allows to select online the best one.
10
Published as a conference paper at ICLR 2018
6 Conclusion
In this article, we tackle the problem of selecting online off-policy RL algorithms. The problem
is formalised as follows: from a fixed portfolio of algorithms, a meta-algorithm learns which one
performs the best on the task at hand. Fairness of algorithm evaluation implies that the RL algorithms
learn off-policy. ESBAS, a novel meta-algorithm, is proposed. Its principle is to divide the meta-time
scale into epochs. Algorithms are allowed to update their policies only at the start each epoch. As
the policies are constant inside each epoch, the problem can be cast into a stochastic multi-armed
bandit. An implementation is detailed and a theoretical analysis leads to upper bounds on the regrets.
ESBAS is designed for the growing batch RL setting. This limited online setting is required in many
real-world applications where updating the policy requires a lot of resources.
Experiments are first led on a negotiation dialogue game, interacting with a human data-built
simulated user. In most settings, not only ESBAS demonstrates its efficiency to select the best
algorithm, but it also outperforms the best algorithm in the portfolio thanks to curriculum learning,
and variance reduction similar to that of Ensemble Learning. Then, ESBAS is adapted to a full online
setting, where algorithms are allowed to update after each transition. This meta-algorithm, called
SSBAS, is empirically validated on a fruit collection task where it performs efficient hyper-parameter
optimisation. SSBAS is also evaluated on the Q*bert Atari game, where it achieves a substantial
improvement over the single algorithm counterparts.
We interpret ESBAS/SSBAS’s success at reliably outperforming the best algorithm in the portfolio as
the result of the four following potential added values. First, curriculum learning: ESBAS/SSBAS
selects the algorithm that is the most fitted with the data size. This property allows for instance to use
shallow algorithms when having only a few data and deep algorithms once collected a lot. Second,
diversified policies: ESBAS/SSBAS computes and experiments several policies. Those diversified
policies generate trajectories that are less redundant, and therefore more informational. As a result, the
policies trained on these trajectories should be more efficient. Third, robustness: if one algorithm fails
at finding good policies, it will soon be discarded. This property prevents the agent from repeating
again and again the same obvious mistakes. Four and last, run adaptation: of course, there has to be
an algorithm that is the best on average for one given task at one given meta-time. But depending on
the variance in the trajectory collection, it did not necessarily train the best policy for each run. The
ESBAS/SSBAS meta-algorithm tries and selects the algorithm that is the best at each run. Some of
those properties are inherited by algorithm selection similarity with ensemble learning (Dietterich,
2002). Wiering & Van Hasselt (2008) uses a vote amongst the algorithms to decide the control of the
next transition. Instead, ESBAS/SSBAS selects the best performing algorithm.
Regarding the portfolio design, it mostly depends on the available computational power per sample
ratio. For practical implementations, we recommend to limit the use of two highly demanding
algorithms, paired with several faster algorithms that can take care of first learning stages, and to use
algorithms that are diverse regarding models, hypotheses, etc. Adding two algorithms that are too
similar adds inertia, while they are likely to not be distinguishable by ESBAS/SSBAS. More detailed
recommendations for building an efficient RL portfolio are left for future work.
References
Robin Allesiardo and Raphael FeraUd. Exp3 with drift detection for the switching bandit problem.
In Proceedings of the 2nd IEEE International Conference on the Data Science and Advanced
Analytics (DSAA), pp.1-7.IEEE, 2015.
Jean-Yves AUdibert, Sebastien BUbeck, and Remi MUnos. Best Arm Identification in MUlti-Armed
Bandits. In Proceedings of the 23th Conference on Learning Theory (COLT), Haifa, Israel, JUne
2010.
Peter Auer, Nicold Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 2002a. doi: 10.1023/A:1013689704352.
Peter Auer, Nicold Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002b.
Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Regret bounds for rein-
forcement learning with policy advice. In Proceedings of the 23rd Joint European Conference on
11
Published as a conference paper at ICLR 2018
Machine Learning and Knowledge Discovery in Databases (ECML-PKDD),pp. 97-112. Springer,
2013.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
YoshUa Bengio, J6r6me Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual International Conference on Machine Learning (ICML), pp. 41-48.
ACM, 2009.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-
stationary rewards. In Proceedings of the 27th Advances in Neural Information Processing Systems
(NIPS), pp. 199-207, 2014.
SebaStien Bubeck and Nicold Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-
armed bandit problems. Foundations and Trends in Machine Learning, 2012. doi: 10.1561/
2200000024.
Marie-Liesse Cauwet, Jialin Liu, and Olivier Teytaud. Algorithm portfolios for noisy optimization:
Compare solvers early. In Learning and Intelligent Optimization. Springer, 2014.
Marie-Liesse Cauwet, Jialin Liu, Baptiste RoziEe, and Olivier Teytaud. Algorithm Portfolios for
Noisy Optimization. ArXiv e-prints, November 2015.
Nicold Cesa-Bianchi and Gdbor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Thomas G. Dietterich. Ensemble learning. The handbook of brain theory and neural networks, 2:
110-125, 2002.
Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 2005.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov
decision processes. In Proceedings of the 15th International Conference on Computational
Learning Theory (COLT), pp. 255-270. Springer, 2002.
Alvaro Fialho, Luis Da Costa, Marc Schoenauer, and MiCheIe Sebag. Analyzing bandit-based
adaptive operator selection mechanisms. Annals of Mathematics and Artificial Intelligence, 2010.
Matteo Gagliolo and Jurgen Schmidhuber. Learning dynamic algorithm portfolios. Annals of
Mathematics and Artificial Intelligence, 2006.
Matteo Gagliolo and Jurgen Schmidhuber. Algorithm selection as a bandit problem with unbounded
losses. In Learning and Intelligent Optimization. Springer, 2010.
Aurelien Garivier and Eric Moulines. On Upper-Confidence Bound Policies for Switching Bandit
Problems, pp. 174-188. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. ISBN 978-3-
642-24412-4. doi: 10.1007/978-3-642-24412-4_16. URL http://dx.doi.org/10.1007/
978-3-642-24412-4_16.
Alex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated
curriculum learning for neural networks. arXiv preprint arXiv:1704.03003, 2017.
Hatim Khouzaimi, Romain Laroche, and Fabrice Lefevre. Optimising turn-taking strategies with
reinforcement learning. In Proceedings of the 16th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (Sigdial), 2015.
Hatim Khouzaimi, Romain Laroche, and Fabrice Lefevre. Reinforcement learning for turn-taking
management in incremental spoken dialogue systems. In Proceedings of the 25th International
Joint Conference on Artificial Intelligence (IJCAI), pp. 2831-2837, 2016.
12
Published as a conference paper at ICLR 2018
Lars Kotthoff. Algorithm selection for combinatorial search problems: A survey. arXiv preprint
arXiv:1210.7959, 2012.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement
learning, pp. 45-73. Springer, 2012.
Romain Laroche and Aude Genevay. A negotiation dialogue game. In Proceedings of the 7th
International Workshop on Spoken Dialogue Systems (IWSDS), Finland, 2016.
Jialin Liu and Olivier Teytaud. Meta online learning: experiments on a unit commitment problem.
In Proceedings of the 22nd European Symposium on Artificial Neural Networks, Computational
Intelligence and Machine Learning (ESANN), 2014.
William S. Lovejoy. Computationally feasible bounds for partially observed markov decision
processes. Operational Research, 1991.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
R6mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy
reinforcement learning. In Proceedings of the 29th Annual Conference on Neural Information
Processing Systems (NIPS), pp. 1054-1062, 2016.
Ronald Ortner, Daniil Ryabko, Peter Auer, and Remi Munos. Regret bounds for restless markov
bandits. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory
(ALT), pp. 214-228, 2012.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Proceedings of the 29th Advances in Neural Information Processing Systems
(NIPS), 2016.
John R. Rice. The algorithm selection problem. Advances in Computers, 1976.
Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks,
2003.
Kate A. Smith-Miles. Cross-disciplinary perspectives on meta-learning for algorithm selection. ACM
Computational Survey, 2009.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT Press, March 1998. ISBN 0262193981.
Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. CoRR,
abs/1611.05763, 2016.
Christopher J.C.H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University,
Cambridge (England), May 1989.
Martha White and Adam White. Adapting the trace parameter in reinforcement learning. In
Proceedings of the 15th International Conference on Autonomous Agents and Multi-Agent Systems
(AAMAS). International Foundation for Autonomous Agents and Multiagent Systems, 2016.
Peter Whittle. Restless bandits: Activity allocation in a changing world. Journal of Applied Probabil-
ity, 25:287-298, 1988. ISSN 00219002.
Marco A. Wiering and Hado Van Hasselt. Ensemble algorithms in reinforcement learning. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 38(4):930-936, 2008.
13
Published as a conference paper at ICLR 2018
A Glossary
Symbol	Designation	First use
t	Reinforcement learning time aka RL-time	Section 2
τ,T	Meta-algorithm time aka meta-time	Section 2
a(t)	Action taken at RL-time t	Figure 1
o(t)	Observation made at RL-time t	Figure 1
r(t)	Reward received at RL-time t	Figure 1
A	Action set	Section 2
Ω	Observation set	Section 2
Rmin	Lower bound of values taken by R	Section 2
RmaX	Upper bound of values taken by R	Section 2
ετ	Trajectory collected at meta-time τ	Section 2
|X I	Size of finite set/list/collection X	Section 2
Ja,bK	Ensemble of integers comprised between a and b	Section 2
E	Space of trajectories	Section 2
Y	Discount factor of the decision process	Section 2
μ(ετ )	Return of trajectory ε aka objective function	Section 2
DT	Trajectory set collected until meta-time T	Section 2
ετ(t)	History of ετ until RL-time t	Section 2
π	Policy	Section 2
π*	Optimal policy	Section 2
α	Algorithm	Section 2
E+	Ensemble of trajectory sets	Section 2
αα SD	State space of algorithm α from trajectory set D	Section 2
ΦD	State space projection of algorithm α from trajectory set D	Section 2
πD	Policy learnt by algorithm α from trajectory set D	Section 2
P	Algorithm set aka portfolio	Section 2.2
K	Size of the portfolio	Section 2.2
σ	Meta-algorithm	Section 2.2
σ(τ)	Algorithm selected by meta-algorithm σ at meta-time T	Section 2.2
Eχ0[f(x0)]	Expected value of f (x) conditionally to X = x0	Equation 1
EμD	Expected return of trajectories controlled by policy ∏D	Equation 1
Eμ∞	Optimal expected return	Section 2.3
σα	Canonical meta-algorithm exclusively selecting algorithm ɑ	Section 2.3
ρσbs(τ)	Absolute pseudo-regret	Definition 1
O(f(x))	Set of functions that get asymptotically dominated by Kf (x)	Section 3
K	Constant number	Theorem 3
Ξ	Stochastic K-armed bandit algorithm	Section 3
β	Epoch index	Section 3
ξ	Parameter of the UCB algorithm	Pseudo-code 2
ρσs(τ)	Short-sighted pseudo-regret	Definition 2
δ	Gap between the best arm and another arm	Theorem 2
t	Index of the second best algorithm	Theorem 2
∆	Gap of the second best arm at epoch β	Theorem 2
bxc	Rounding of X at the closest integer below	Theorem 2
σESBAS	The ESBAS meta-algorithm	Theorem 2
Θ(f (x))	Set of functions asymptotically dominating Kf (x) and dominated by κ0f (x)	Table 1
σ*		Best meta-algorithm among the canonical ones	Theorem 3
14
Published as a conference paper at ICLR 2018
Symbol	Designation	First use
p,ps ,Pu	Player, system player, and (simulated) user player	Section C.1.1
η	Option to agree or disagree on	Section C.1.1
Vn	Cost ofbooking/Selecting option V for player P	Section C.1.1
U [a, b]	Uniform distribution between a and b	Section C.1.1
Sf	Final state reached in a trajectory	Section C.1.1
Rps(Sf)	Immediate reward received by the system player at the end of the dialogue	Section C.1.1
REFPROP(η)	Dialogue act consisting in proposing option η	Section C.1.1
ASKREPEAT	Dialogue act consisting in asking the other player to repeat what he said	Section C.1.1
ACCEPT(η)	Dialogue act consisting in accepting proposition η	Section C.1.1
ENDDIAL	Dialogue act consisting in ending the dialogue	Section C.1.1
SERU S	Sentence error rate of system Ps listening to user PU	Section C.1.1
SCOreasr	Speech recognition score	Section C.1.1
N (χ,v)	Normal distribution of centre X and variance v2	Section C.1.1
RefInsist	REFPRθP(η), with η being the last proposed option	Section C.1.1
RefnewProp	REFPRθP(η), with η being the best option that has not been proposed yet	Section C.1.1
Accept	AcCEPT(η), with η being the last understood option proposition	Section C.1.1
eβ	e-greedy exploration in function of epoch β	SectionC.1.2
Φα	Set of features of algorithm ɑ	SectionC.1.2
φo	Constant feature: always equal to 1 α	SectionC.1.2
φasr	ASR feature: equal to the last recognition score	SectionC.1.2
φdif	Cost feature: equal to the difference of cost of proposed and targeted options	SectionC.1.2
φt	RL-time feature	SectionC.1.2
φnoise	Noise feature	SectionC.1.2
simple	FQI with Φ = {φθ,φαsr ,φdif ,Φt}	SectionC.1.2
fast	FQI with Φ = {φθ,φasr ,φdif }	SectionC.1.2
simple-2	FQI with φ = {φ0, φasr, φdif ,φt, φasr φdif ,φtφasr ,φdif φt, φLr, φdif ,φ2}	SectionC.1.2
fast-2	FQI with φ = {φ0, φasr, φdif ,φasr φdif, φlsr, φdif }	SectionC.1.2
n-1-simple	FQI with Φ = {φθ,φasr ,φdif , Φt, Φnoise}	SectionC.1.2
n-1-fast	FQI with Φ = {φθ,φasr ,φdif , Φnoise}	SectionC.1.2
n-1-simple-2	FQI with Φ = {φo, φasr, φdif ,φt, φnoise, Φasr Φdif, ΦtΦnoise, Φasr Φt, φdif φnoise, φasr φnoise, φdif φt, φlsr , φdif, φ2, φnoise}	SectionC.1.2
n-1-fast-2	FQI with Φ = {φo,φasr ,Φdif ,Φt}	SectionC.1.2
constant-μ	Non-learning algorithm with average performance μ	SectionC.1.2
Z	Number of noisy features added to the feature set	SectionC.1.2
P(Xy		Probability that X = X conditionally to Y = y	Equation 35
15
Published as a conference paper at ICLR 2018
B Theoretical analysis
The theoretical aspects of algorithm selection for reinforcement learning in general, and Epochal
Stochastic Bandit Algorithm Selection in particular, are thoroughly detailed in this section. The
proofs of the Theorems are provided in Sections E, F, and G. We recall and formalise the absolute
pseudo-regret definition provided in Section 2.3.
Definition 1 (Absolute pseudo-regret). The absolute pseudo-regret Pabs(T) compares the meta-
algorithm’s expected return with the optimal expected return:
T
PbsD )= T Eμ∞ — Eσ EEμD(<τ)
τ-
τ =1
(4)
B.1	Assumptions
The theoretical analysis is hindered by the fact that AS, not only directly influences the return
distribution, but also the trajectory set distribution and therefore the policies learnt by algorithms for
next trajectories, which will indirectly affect the future expected returns. In order to allow policy
comparison, based on relation on trajectory sets they are derived from, our analysis relies on two
assumptions.
Assumption 1 (More data is better data). The algorithms train better policies with a larger trajectory
set on average, whatever the algorithm that controlled the additional trajectory:
∀D∈ E+, ∀α, α0 ∈ P, EμD ≤ E。，匣〃多3，］.	(5)
Assumption 1 states that algorithms are off-policy learners and that additional data cannot lead to
performance degradation on average. An algorithm that is not off-policy could be biased by a specific
behavioural policy and would therefore transgress this assumption.
Assumption 2 (Order compatibility). If an algorithm trains a better policy with one trajectory set
than with another, then it remains the same, on average, after collecting an additional trajectory from
any algorithm:
∀D, D0	∈	E+,	∀α, α ∈ P,	EμD	<	E〃D，⇒	E。，忸〃多3，］≤	E。，忸〃多，3，］.(6)
Assumption 2 states that a performance relation between two policies trained on two trajectory sets
is preserved on average after adding another trajectory, whatever the behavioural policy used to
generate it. From these two assumptions, Theorem 1 provides an upper bound in order of magnitude
in function of the worst algorithm in the portfolio. It is verified for any meta-algorithm σ .
Theorem 1 (Not worse than the worst). The absolute pseudo-regret is bounded by the worst algorithm
absolute pseudo-regret in order of magnitude:
∀σ,	pσbs(τ)∈o(m∈χpa；。)).	⑺
Contrarily to what the name of Theorem 1 suggests, a meta-algorithm might be worse than the
worst algorithm (similarly, it can be better than the best algorithm), but not in order of magnitude.
Its proof is rather complex for such an intuitive result because, in order to control all the possible
outcomes, one needs to translate the selections of algorithm α with meta-algorithm σ into the
canonical meta-algOrithm σɑ's view.
B.2	Short-sighted pseudo-regret analysis of ESBAS
ESBAS intends to minimise the regret for not choosing the best algorithm at a given meta-time τ . It
is short-sighted: it does not intend to optimise the algorithms learning.
16
Published as a conference paper at ICLR 2018
Definition 2 (Short-sighted pseudo-regret). The short-sighted pseudo-regret p>σss(T) is the difference
between the immediate best expected return algorithm and the one selected:
PS(T) = Eσ
T
∑(m∈ix e〃Dl - E〃D?i
τ=1
(8)
Theorem 2 (ESBAS short-sighted pseudo-regret). If the stochastic multi-armed bandit Ξ guarantees
a regret of order of magnitude O(log(T )∕∆g), then:
ESBAS
P；s	(T) ∈ O
blog(T)c
X
β=0
(9)
Theorem 2 expresses in order of magnitude an upper bound for the short-sighted pseudo-regret of
ESBAS. But first, let define the gaps: ∆β = max0o∈p Eμa σESBAs - Eμa ctesbas ∙ It is the difference
D2β -1 D2β -1
of expected return between the best algorithm during epoch β and algorithm α. The smallest non null
gap at epoch β is noted: ∆g = minα∈p,∆α>o ^^If ∆β does not exist, i.e. if there is no non-null
gap, the regret is null.
Several upper bounds in order of magnitude on PSS(T) can be easily deduced from Theorem 2,
depending on an order of magnitude of ∆g. See the corollaries in Section F.1, Table 1 and more
generally Section 3 for a discussion.
B.3 ESBAS absolute pseudo-regret analysis
The short-sighted pseudo-regret optimality depends on the meta-algorithm itself. For instance, a poor
deterministic algorithm might be optimal at meta-time τ but yield no new information, implying the
same situation at meta-time τ + 1, and so on. Thus, a meta-algorithm that exclusively selects the
deterministic algorithm would achieve a short-sighted pseudo-regret equal to 0, but selecting other
algorithms are, in the long run, more efficient. Theorem 2 is a necessary step towards the absolute
pseudo-regret analysis.
The absolute pseudo-regret can be decomposed between the absolute pseudo-regret of the best
canonical meta-algorithm (i.e. the algorithm that finds the best policy), the regret for not always
selecting the best algorithm, and potentially not learning as fast, and the short-sighted regret: the
regret for not gaining the returns granted by the best algorithm. This decomposition leads to Theorem
3 that provides an upper bound of the absolute pseudo-regret in function of the best canonical
meta-algorithm, and the short-sighted pseudo-regret.
The fairness of budget distribution is the property stating that every algorithm in the portfolio has
as much resources as the others, in terms of computational time and data. Section 3 discusses it at
length. For analysis purposes, Theorem 3 assumes the fairness of AS:
Assumption 3 (Learning is fair). If one trajectory set is better than another for training one given
algorithm, it is the same for other algorithms.
∀α,α0 ∈	P,	∀D, D0	∈	E+,	Eμ%	< Eμ%,	⇒ Eμ%'	≤ EμD'o.	(IO)
Theorem 3 (ESBAS absolute pseudo-regret upper bound). Under assumption 3, if the stochastic
multi-armed bandit Ξ guarantees that the best arm has been selected in the T first episodes at least
T∕K times, with high probability 1 - δT, with δT ∈ O(1∕T), then:
∃κ> 0, ∀τ ≥ 9K2,	ρσESBAS (T) ≤ (3K +1)ρσbs (3K) + 廿S (T) + κ log(T),	(II)
where meta-algorithm σ* selects exclusively algorithm ɑ* = argmina∈p Pjbs (T).
Successive and Median Elimination (Even-Dar et al., 2002) and Upper Confidence Bound (Auer
et al., 2002a) under some conditions (Audibert et al., 2010) are examples of appropriate Ξ satisfying
both conditions stated in Theorems 2 and 3. Again, see Table 1 and more generally Section 3 for a
discussion of those bounds.
17
Published as a conference paper at ICLR 2018
C Experimental details
C.1 Dialogue experiments details
C.1.1 The Negotiation Dialogue Game
ESBAS practical efficiency is illustrated on a dialogue negotiation game (Laroche & Genevay, 2016)
that involves two players: the system ps and a user pu . Their goal is to find an agreement among
4 alternative options. At each dialogue, for each option η, players have a private uniformly drawn
cost Vp ~ U [0,1] to agree on it. Each player is considered fully empathetic to the other one. As a
result, if the players come to an agreement, the system’s immediate reward at the end of the dialogue
is Rps (sf) = 2 - νηps - νηpu, where sf is the state reached by player ps at the end of the dialogue,
and η is the agreed option; if the players fail to agree, the final immediate reward is Rps (sf) = 0,
and finally, if one player misunderstands and agrees on a wrong option, the system gets the cost of
selecting option η without the reward of successfully reaching an agreement: Rps (sf) = -νηps - νηp0u .
Players act each one in turn, starting randomly by one or the other. They have four possible actions.
First, RefProp(η): the player makes a proposition: option η. If there was any option previously
proposed by the other player, the player refuses it. Second, AskRepeat: the player asks the other
player to repeat its proposition. Third, Accept(η): the player accepts option η that was understood
to be proposed by the other player. This act ends the dialogue either way: whether the understood
proposition was the right one or not. Four, EndDial: the player does not want to negotiate anymore
and ends the dialogue with a null reward.
Understanding through speech recognition of system ps is assumed to be noisy: with a sentence error
rate of probability SERsu = 0.3, an error is made, and the system understands a random option in-
stead of the one that was actually pronounced. In order to reflect human-machine dialogue asymmetry,
the simulated user always understands what the system says: SERsu = 0. We adopt the way Khouza-
imi et al. (2015) generate speech recognition confidence scores: Scoreasr = [+；-X where X ~
N(x, 0.2). If the player understood the right option x = 1, otherwise x = 0.
The system, and therefore the portfolio algorithms, have their action set restrained to five non para-
metric actions: RefInsist ⇔ RefProp(ηt-1), ηt-1 being the option lastly proposed by the system;
RefNewProp ⇔ RefProp(η), η being the preferred one after ηt-1, AskRepeat, Accept⇔
Accept(η), η being the last understood option proposition and EndDial.
C.1.2 Learning algorithms
All learning algorithms are using Fitted-Q Iteration (Ernst et al., 2005), with a linear parametrisation
and an β -greedy exploration : β = 0.6β, β being the epoch number. Six algorithms differing by
their state space representation Φα are considered:
•	simple: state space representation of four features: the constant feature φ0 = 1, the last recog-
nition score feature φasr, the difference between the cost of the proposed option and the next
best option φdif, and finally an RL-time feature φt = 001+τ. Φα = {φ0, φasr ,φdif, φt}.
•	fast: Φα = {φ0, φasr, φdif}.
•	simple-2: state space representation of ten second order polynomials of simple features.
Φα = {φ0, φasr, φdif, φt, φ2asr, φ2dif, φt2, φasr φdif, φasrφt, φtφdif}.
•	fast-2: state space representation of six second order polynomials of fast features. Φα = {φ0,
φasr, φdif, φ2asr, φ2dif, φasr φdif}.
•	n-Z-{simple∕fast∕SimPle-2∕fast-2} Versions of previous algorithms with Z additional features
of noise, randomly drawn from the uniform distribution in [0, 1].
•	constant-μ: the algorithm follows a deterministic policy of average performance μ without
exploration nor learning. Those constant policies are generated with simPle-2 learning from
a predefined batch of limited size.
18
Published as a conference paper at ICLR 2018
C.1.3 Evaluation protocol
In all our experiments, ESBAS has been run with UCB parameter ξ = 1/4. We consider 12 epochs.
The first and second epochs last 20 meta-time steps, then their lengths double at each new epoch,
for a total of 40,920 meta-time steps and as many trajectories. γ is set to 0.9. The algorithms and
ESBAS are playing with a stationary user simulator built through Imitation Learning from real-
human data. All the results are averaged over 1000 runs. The performance figures plot the curves of
algorithms individual performance σα against the ESBAS portfolio control σESBAS in function of the
epoch (the scale is therefore logarithmic in meta-time). The performance is the average return of the
reinforcement learning return: it equals γlelRps (sf) in the negotiation game. The ratio figures plot
the average algorithm selection proportions of ESBAS at each epoch. We define the relative pseudo
regret as the difference between the ESBAS absolute pseudo-regret and the absolute pseudo-regret
of the best canonical meta-algorithm. All relative pseudo-regrets, as well as the gain for not having
chosen the worst algorithm in the portfolio, are provided in Table 2. Relative pseudo-regrets have a
95% confidence interval about ±6 ≈ ±1.5 × 10-4 per trajectory.
C.1.4 Assumptions transgressions
Several results show that, in practice, the assumptions are transgressed. Firstly, we observe that
Assumption 3 is transgressed. Indeed, it states that if a trajectory set is better than another for a
given algorithm, then it’s the same for the other algorithms. Still, this assumption infringement does
not seem to harm the experimental results. It even seems to help in general: while this assumption
is consistent curriculum learning, it is inconsistent with the run adaptation property advanced in
Subsection 6 that states that an algorithm might be the best on some run and another one on other
runs.
And secondly, off-policy reinforcement learning algorithms exist, but in practice, we use state space
representations that distort their off-policy property (Munos et al., 2016). However, experiments do
not reveal any obvious bias related to the off/on-policiness of the trajectory set the algorithms train
on.
C.2 Q*bert experiment details
The three DQN networks (small, large, and huge) are built in a similar fashion, with relu activations
at each layer except for the output layer which is linear, with RMSprop optimizer (ρ = 0.95 and
= 10-7), and with He uniform initialisation. The hyperparameters used for training them are also
the same and equal to the ones presented in the table hereinafter:
hyperparameter	value
minibatch size	32
replay memory size	1 X 106
agent history length	4
target network update frequency	5 × 104
discount factor	0.99
action repeat	20
update frequency	20
learning rate	2.5 × 10-4
exploration parameter E	5 × t-1 × 10-6
replay start size	5 × 10-4
no-op max	30
19
Published as a conference paper at ICLR 2018
Only their shapes differ:
•	small has a first convolution layer with a 4x4 kernel and a 2x2 stride, and a second convo-
lution layer with a 4x4 kernel and a 2x2 stride, followed by a dense layer of size 128, and
finally the output layer is also dense.
•	large has a first convolution layer with a 8x8 kernel and a4x4 stride, and a second convolution
layer with a 4x4 kernel and a 2x2 stride, followed by a dense layer of size 256, and finally
the output layer is also dense.
•	huge has a first convolution layer with a 8x8 kernel and a 4x4 stride, a second convolution
layer with a 4x4 kernel and a 2x2 stride, and a third convolution layer with a 3x3 kernel and
a 1x1 stride, followed by a dense layer of size 512, and finally the output layer is also dense.
20
Published as a conference paper at ICLR 2018
D Extended results of the dialogue experiments
Portfolio	w. best	w. worst
simple-2 + fast-2	3^	-181
simple + n-1-simple-2	-73	-131
simple + n-1-simple	3	-2
simple-2 + n-1-simple-2	-12	-38
all-4 + constant-1.10	21	-2032
all-4 + constant-1.11	-21	-1414
all-4 + constant-1.13	-10	-561
all-4	-28	-275
all-2-simple + constant-1.08	-41	-2734
all-2-simple + constant-1.11	-40	-2013
all-2-simple + constant-1.13	-123	-799
all-2-simple	-90	-121
fast + simple-2	-39	-256
simple-2 + constant-1.01	169	-5361
simple-2 + constant-1.11	53	-1380
simple-2 + constant-1.11	57	-1288
simple + constant-1.08	54	-2622
simple + constant-1.10	88	-1565
simple + constant-1.14	-6	-297
all-4 + all-4-n-1 + constant-1.09	25	-2308
all-4 + all-4-n-1 + constant-1.11	20	-1324
all-4 + all-4-n-1 + constant-1.14	-16	-348
all-4 + all-4-n-1	-10	-142
all-2-simple + all-2-n-1-simple	-80	-181
4* n-2-simple	-20	-20
4* n-3-simple	-13	-13
8* n-1-simple-2	-22	-22
simple-2 + constant-0.97 (no reset)	113	-7131
simple-2 + constant-1.05 (no reset)	23	-3756
simple-2 + constant-1.09 (no reset)	-19	-2170
simple-2 + constant-1.13 (no reset)	-16	-703
simple-2 + constant-1.14 (no reset)	-125	-319
Table 2: ESBAS pseudo-regret after 12 epochs (i.e. 40,920 trajectories) compared with the best and
the worst algorithms in the portfolio, in function of the algorithms in the portfolio (described in the
first column). The ’+’ character is used to separate the algorithms. all-4 means all the four learning
algorithms described in Section C.1.2 simple + fast + simple-2 + fast-2. all-4-n-1 means the same four
algorithms with one additional feature of noise. Finally, all-2-simple means simple + simple-2 and
all-2-n-1-simple means n-1-simple + n-1-simple-2. On the second column, the redder the colour, the
worse ESBAS is achieving in comparison with the best algorithm. Inversely, the greener the colour
of the number, the better ESBAS is achieving in comparison with the best algorithm. If the number
is neither red nor green, it means that the difference between the portfolio and the best algorithm is
insignificant and that they are performing as good. This is already an achievement for ESBAS to
be as good as the best. On the third column, the bluer the cell, the weaker is the worst algorithm
in the portfolio. One can notice that positive regrets are always triggered by a very weak worst
algorithm in the portfolio. In these cases, ESBAS did not allow to outperform the best algorithm in
the portfolio, but it can still be credited with the fact it dismissed efficiently the very weak algorithms
in the portfolio.
21
Published as a conference paper at ICLR 2018
E Not worse than the worst
Theorem 1 (Not worse than the worst). The absolute pseudo-regret is bounded by the worst algorithm
absolute pseudo-regret in order of magnitude:
∀σ,	ρσbs(T) ∈o(max ρσbs
(7)
Proof. From Definition 1:
T
说bs(T )= T Eμ∞ - Eσ X EμD(<τ), ,	(12a)
τ-1
τ =1
[subα(DT )|	「
pσbs(T )= T Eμ∞ - X Eσ	X	EμDσa ɪ ,	(12b)
τα-1
α∈P i=1 i
∣subα(DT )|
pσbs(T )= X Eσ ∣subα(DT )∣Eμ∞ - X	EμDσa-1 ,	(12c)
α∈P	i=1 i
where subα (D) is the subset of D with all the trajectories generated with algorithm α, where τiα is
the index of the ith trajectory generated with algorithm α, and where |S | is the cardinality of finite set
S. By convention, let Us state that EμDσα
τiα -1
Eμ∞ if ∣subα(DT)|< i. Then:
说bs(T) = XXEσ [Eμ∞ - EμDσa-1
α∈P i=1	i
To conclUde, let Us prove by mathematical indUction the following ineqUality:
(13)
is true by vacuity for i = 0: both left and right terms equal Eμa. Now let Us assume the property true
for i and prove it for i + 1:
Eμa
Dτσα-1∪εα∪
(14a)
Eμα	τa 1 -i ,,
Dτσiα-1∪εα∪Sτi=+τ1iα+1 εσ(τ)
(14b)
Eμa
Dτσiα-1
∪εα∪Sττiα=+11-τiα-1
(14c)
If |Suba(DT)∣≥ i + 1,by applying mathematical induction assumption, then by applying Assumption
2 and finally by applying Assumption 1 recursively, we infer that:
Eσ
EμDσα-ι∪εα
(15a)
(15b)
22
Published as a conference paper at ICLR 2018
Eσ EμDσα ]∪εα ≥ Eσα ∣EμDσα] ,	(15c)
Eσ Eμα	TaLa-I …… ≥ Eσɑ[EμDσα].	(15d)
σ	Dσα ∪εα∪S i+1 i	εσ(τiα+τ)	σ	Di
τα-1	τ=1
i
If ∣subα(DT)|< i + 1, the same inequality is straightforwardly obtained, since, by convention
EμD	= Eμ∞, and since, by definition ∀D ∈ E+, ∀ɑ ∈ P, Eμ∞ ≥ Eμ%.
Dτik+1	D
The mathematical induction proof is complete. This result leads to the following inequalities:
T
pσf1s(T) ≤ ∑ EEσa 忸μ∞ - EμD巴卜	(16a)
α∈P i=1
α
Pabs (T),	(16b)
α∈P
α
Plbs(T) ≤ K max Pbs (T),	(16c)
α∈P
which leads directly to the result:
∀σ, Pabs(T) ∈ O mmax Pabs(T)) .	(17)
αk∈P
This proof may seem to the reader rather complex for such an intuitive and loose result but algorithm
selection σ and the algorithms it selects may act tricky. For instance selecting algorithm α only
when the collected trajectory sets contains misleading examples (i.e. with worse expected return
than with an empty trajectory set) implies that the following unintuitive inequality is always true:
EμDσ	≤ EμDσα . In order to control all the possible outcomes, one needs to translate the selections
τ-1	τ-1
of algorithm a into σa s view.	□
23
Published as a conference paper at ICLR 2018
F ESBAS short-sighted pseudo-regret upper order of magnitude
Theorem 2 (ESBAS short-sighted pseudo-regret). If the stochastic multi-armed bandit Ξ guarantees
a regret of order of magnitude O(log(T )∕∆g), then:
ESBAS
限(T) ∈ O
blog(T)c
X
β=0
(9)
Proof. By simplification of notation, Eμ%β = Eμα ,sbas . From Definition 2:
D2β -1
ESBAS
Pss	(T) = EσESBAS
T
E max EμDσESBAS
α∈P	Dτ-1
τ=1
σESBAS (τ)
DτσESBAS
ESBAS
Pss	(T) = EσESBAS
max
α∈P
τ=1
(18a)
(18b)
ESBAS
Pss	(T) ≤ EσESBAS
blog2(T)c 2β+1-1
XX
β=0	τ=2β
(18c)
blog2(T)c
ESBAS	ESBAS
ρσs	(τ) ≤ E	ρσs	(β),
β=0
(18d)
ES BAS
where βτ is the epoch of meta-time T. A bound on short-sighted pseudo-regret ρ∖s	(β) for each
epoch β can then be obtained by the stochastic bandit Ξ regret bounds in O (log(T)∕∆):
⇔ ∃κ1 > 0,
where
ES BAS
ρσs	(β)
ES BAS
ρσs	(β)
ES BAS
ρσs	(β)
ES BAS
ρσs	(β)
1 = X ɪ
δ α⅛ ∆α
(19a)
(19b)
(19c)
(19d)
(19e)
and where
+∞
maxα0∈p Eμα — Eμα
if Eμα = maxα0∈p Eμα0,
otherwise.
(19f)
Since we are interested in the order of magnitude, we can once again only consider the upper bound
of ∆1β:
1
∆β ∈
O
α∈P
(⅛)
(20a)
24
Published as a conference paper at ICLR 2018
⇔ ∃κ2 > 0,
∈- ∈ O I max —ɪ-
∆β	∖α∈P ∆ɑ
(20b)
(20c)
ɪ ≤ &2
3 ∆β,
where the second best algorithm at epoch β such that ∆g > 0 is noted ag .Injected in Equation 18d,
it becomes:
blog2(T)c
ESBAS	β
Pss	(T) ≤ κ1κ2 E —J,
β=0	∆β
(21)
which proves the result.
□
F.1 Corollaries of Theorem 2
Corollary 1.
If ∆β ∈ Θ(1), then ρσESBAS (T) ∈O
(log2 (T),
∖ δ∞ ,
,where ∆∞
μ∞ - μ∞
> 0.
Proof. ∆β ∈ Ω(1) means that only one algorithm a* converges to the optimal asymptotic perfor-
mance μ∞ and that ∃∆∞ = μ∞ - μ∞ > 0 such that ∀62 > 0, ∃βι ∈ N, such that ∀β ≥ βι,
∆β > ∆∞ - e. In this case, the following bound can be deduced from equation 21:
blog(T)c
ES BAS
ρσs	(t ) ≤ κ4 + E
β=β1
κ1κ2
∆∞ —
β,
(22a)
P；：SBAS (T) ≤ K4 + κ1κ2 ∣θg2(T),
2(∆∞ - e)，
where κ4 is a constant equal to the short-sighted pseudo-regret before epoch β1 :
ESBAS
κ4=ρσs	(2βι-1)
(22b)
(23)
Equation 22b directly leads to the corollary.
□
Corollary 2. If ∆β ∈ Θ (β-mɔ, then p；：SBAS (T) ∈ O (logm』2
Proof If ∆β decreases slower than polynomially in epochs, which implies decreasing polylogarith-
mically in meta-time, i.e. ∃κ5 > 0, ∃m* > 0, ∃β2 ∈ N, such that ∀β ≥ β2, ∆g > κβ~m, then,
from Equation 21:
blog(T)c
ESBAS
ρσs
ESBAS
ρσs
ESBAS
ρσs
(T ) ≤ κ6 +	Σ β=β2	κ1κ2 β κ5β-m ,	(24a)
	blog(T)c		
(T ) ≤ κ6 +	X	K1K2βmt+1	(24b)
	β=β2	κ5	
κ1κ2 (t ) ≤ q	logm』2(T),		(24c)
κ5
25
Published as a conference paper at ICLR 2018
where κ6 is a constant equal to the short-sighted pseudo-regret before epoch β2 :
ESBAS
K6=aS	(2β2-1) .	(25)
Equation 24c directly leads to the corollary.	□
Corollary 3. If ∆β ∈ Θ (T-C),then pσESBAS (T) ∈ O (T Ct log(T)).
Proof. If ∆β decreases slower than a fractional power of meta-time T, then ∃κ7 > 0, 0 < c* < 1,
∃β3 ∈ N, such that ∀β ≥ β3, ∆g > κ7T-ct, and therefore, from Equation 21:
blog(T)c				
ESBAS ρσs	(T) ≤ κ8 +	Σ β=β3	-⅛ β, κ7τ-C	(26a)
ESBAS ρσs	(T) ≤ κ8 +	blog(T)c X β=β3	:β,	(26b)
ESBAS ρσs	(T) ≤ κ8 +	blog(T)c X β=β3	曹β (N)∖	(26c)
where κ8 is a constant equal to the short-sighted pseudo-regret before epoch β3 :
ESBAS
K8 =回	(2β3-1) .	(27)
The sum in Equation 26c is solved as follows:
nn
X ixi = x X ixi-1
i =i0	i=i0
n
X ixi
i =i0
n
xX
i=i0
n
X ixi
i =i0
xd( — Xi).
dx
n
ixi
i=i0
xn+1 - xi0
d ------r-
x-1
dx
n
^X ixi =  -------)2 ((X — 1)nxn — Xn — (x — 1)ioxi0-1 + xi0
i=i0
(28a)
(28b)
(28c)
(28d)
(28e)
x
This result, injected in Equation 26c, induces that ∀3 > 0, ∃T1 ∈ N, ∀T ≥ T1:
S (T) ≤ K8 +	log(T)2c log(T), κ7(2C — 1)	(29a)
ESBAS	κ1κ2(1 + 0)2Ct	t 戒S	(T) ≤ K8 +	-1) TC log(T),	(29b)
which proves the corollary.	□
26
Published as a conference paper at ICLR 2018
G ESBAS abolute pseudo-regret b ound
Theorem 3 (ESBAS absolute pseudo-regret upper bound). Under assumption 3, if the stochastic
multi-armed bandit Ξ guarantees that the best arm has been selected in the T first episodes at least
T/K times, with high probability 1 - δT, with δT ∈ O(1/T), then:
∃κ> 0, ∀τ ≥ 9K2,	PabsBAS (T) ≤ (3K +1)ρσbs (3Tk) + ρσΓBAS (T) + κ log(T),	(II)
where meta-algorithm σ* selects exclusively algorithm ɑ* = argmi□a∈p pσbs (T)∙
Proof. The ESBAS absolute pseudo-regret is written with the following notation simplifications :
ES BAS
Dτ-1 = Dτσ-1 and kτ = σESBAS (τ):
T
ES BAS	ESBAS
Pbs	(T)= TEμ∞ - EaESBAS ∑Eμ σESBAS
τ=1	Dτ-1
T
ES BAS
Pabs	(T) = tEμ∞ - EaESBAS): EμDτ 1 .
τ=1
(30a)
(30b)
Let σ* denote the algorithm selection selecting exclusively a*, and ɑ* be the algorithm minimising
the algorithm absolute pseudo-regret:
k
α = argmin PabS(T).
αk∈P
(31)
Note that σ* is the optimal constant algorithm selection at horizon T, but it is not necessarily the
optimal algorithm selection: there might exist, and there probably exists a non constant algorithm
selection yielding a smaller pseudo-regret.
ESBAS
The ESBAS absolute pseudo-regret PbS (T) can be decomposed into the pseudo-regret for not
having followed the optimal constant algorithm selection σ* and the pseudo-regret for not having
selected the algorithm with the highest return, i.e. between the pseudo-regret on the trajectory and the
pseudo-regret on the immediate optimal return:
T
ES BAS
PabS	(T) = TEμ∞ - EaESBAS EEμsub*(Dτ-1)
τ=1
(32)
+ EaES BAS
T
EEμ=ub*(Dτ-l)
τ=1
T
-EaESBAS X EμDτ-ι
τ=1
where Eμ^b*(Dτ-) is the expected return of policy nɪub*(Pt_ 1), learnt by algorithm α* on trajectory
set sub (DT-ι), which is the trajectory subset of DT-ι obtained by removing all trajectories that
were not generated with algorithm α*.
First line of Equation 32 can be rewritten as follows:
T
TEμ∞ - EaESBAS X E〃；ub* (Dτ-1)
T=1
T
X (Eμ∞ - EaESBAS [Eμ
T=1
*
sub* (Dτ-1
(33)
The key point in Equation 33 is to evaluate the size of sub* (DT-1).
On the one side, Assumption 3 of fairness states that one algorithm learns as fast as any another over
any history. The asymptotically optimal algorithm(s) when τ → ∞ is(are) therefore the same one(s)
27
Published as a conference paper at ICLR 2018
whatever the the algorithm selection is. On the other side, let 1 - δτ denote the probability, that at
time τ , the following inequality is true:
τ-1
|sub* (Dτ-i) ∣≥	.	(34)
3K
With probability δτ, inequality 34 is not guaranteed and nothing can be inferred about Eμ*ub*(Dτ-1),
except it is bounded under by Rmin/(1 - γ). Let E3τK-1 be the subset of Eτ-1 such that ∀D ∈
E3κ-1, ∣sub* (D) l≥ b(τ - 1)∕3KC. Then, δτ can be expressed as follows:
δτ =	X	P(DQesbas).	(35)
D∈Eτ-1∖ETκ-1
With these new notations:
R
Eμ∞ - EσESBAs 叵;ub*(Dτ-/ ≤ Eμ∞ - E P(D∣σESBAS廊薪⑶-δτTmn , (36a)
D∈E3τK-1	γ
Eμ∞ - EσESBAS [EμSJub*(Dτ-1)]
≤ (1 - δτ)Eμ∞ - X P(D∣σESBAS)Eμ=ub*(D) + δτ
D∈E3τK-1
Let consider E*(α, N) the set of all sets D such that ∣subα(D)∣= N and such that last trajectory
in D was generated by α. Since ESBAS, with Ξ, a stochastic bandit with regret in O(log(T)∕∆),
guarantees that all algorithms will eventually be selected an infinity of times, we know that :
∀α ∈ P, ∀N ∈ N,	X	P(D∣σESBAS) = 1.	(37)
D ∈E+(α,N)
By applying recursively Assumption 2, one demonstrates that:
X	P(D∣σESBAS)Eμαubα(D) ≥ X P(D∣σɑ)EμD,	(38a)
D ∈E+(α,N)	D∈EN
X	P(D∣σESBAS)Eμαubα(D) ≥ Eσɑ hEμDNa i .	(38b)
D ∈E+ (α,N)
One also notices the following piece-wisely domination from applying recursively Assumption 1:
(1 - δτ)Eμ∞ - X P(D∣σesbas)E〃；“b*(D)= X P(D∣σESBAS) (Eμ∞ -旧”薪⑶),
D ∈E3τK-1	D∈E3τK-1
(39a)
(1 - δτ)Eμ∞
-X P(D∣σesbas廊薪⑶ ≤	X	P(D∣σESBAS) (Eμ∞ -旧”薪⑶),
D ∈E3k'	D∈E+(α*,bT-KIC)&|DRt - 1
(39b)
(1 - δτ )Eμ∞
-X P(D∣σesbas廊薪⑶ ≤ X	P(D∣σESBAS) (⅛μ∞ -旧”薪⑶), C
D∈ETκ1	D∈E+(α*,b ⅞⅛ C)
28
Published as a conference paper at ICLR 2018
(1 - δτ)Eμ∞ - E	P(D∣σESBAS)Eμ=ub*(D) ≤ Eμ∞ - E	P(D∣σESBAS廊薪⑶.
D ∈E3κ'	D∈E+ (α*,bττ-κ1 C)
(39d)
Then, by applying results from Equations 38b and 39d into Equation 36b, one obtains:
Eμ∞ — EσESBAS
忸“MD-)] ≤ Eμ∞ - Eσ*
EμDσ*
.[TKIJ
(40)
Next, the terms in the first line of Equation 32 are bounded as follows:
T Eμ∞ — EσESBAS
T
£E〃；ub*(DT-1)
τ=1
(41a)
T Eμ∞ — EσESBAS
≤ TEμ∞ - Eσ*
T
£E〃；ub*(D
τ=1
T
X EμDσ⅛J
RT
R—injτ=1 δT4ib)
Again, for T ≥ 9K2 :
T
TEμ∞ — EσESBAS £E〃；ub*(DT-i)
τ=1
≤ (3K+1)pσ*s( 3K)+(Eμ∞ -1—in)X δτ.
(42)
Regarding the first term in the second line of Equation 32, from applying recursively Assumption 2:
EσES BAS
回;ub*(Dτ)] ≤ EσESBAS [EμDτ],
(43a)
EσES BAS
忸〃；ub*(DJ ≤ EσESBAS
max
α∈P
(43b)
From this observation, one directly concludes the following inequality:
T
EσESBAS EEμ=ub*(Dτ)
τ=1
T
—EσESBAS)： EμDτ
τ=1
TT
≤ EσESBAS X max EμDτ — X EμDT
τ=1	τ=1
(44a)
T
EσESBAS EEμ=ub*(Dτ)
τ=1
T
—EσESBAS)： EμDτ
τ=1
≤ ρσESBAS (T).
(44b)
Injecting results from Equations 42 and 44b into Equation 32 provides the result:
ES BAS	* T	ESBAS	Rmin T
Pabs	(T)	≤	(3K	+	1)pabs (3κ) + Pss	(T) +	(Eμ∞ — ι — γ J	Eδτ ∙	(45)
We recall here that the stochastic bandit algorithm Ξ was assumed to guarantee to try the best
algorithm a* at least T/K times with high probability 1 — δτ and δτ ∈ O(T-1). Now, We show
that at any time, the longest stochastic bandit run (i.e. the epoch that experienced the biggest number
of pulls) lasts at least N = T: at epoch βτ, the meta-time spent on epochs before βτ — 2 is equal to
29
Published as a conference paper at ICLR 2018
Pββτ=-02 2β = 2βτ-1; the meta-time spent on epoch βτ - 1 is equal to 2βτ-1; the meta-time spent
on epoch β is either below 2βτ-1, in which case, the meta-time spent on epoch βτ - 1 is higher
than 3, or the meta-time spent on epoch β is over 2βτ T and therefore higher than T. Thus, ESBAS
is guaranteed to try the best algorithm a* at least τ∕3K times with high probability 1 - δτ and
δτ ∈ O(τ -1). As a result:
T
zτESBAS , 、	,	、k* T T ∖	zτESBAS , 、	/	Rmin λ K3
∃κ3 >	0,	Pabs (T)	≤ (3K +	1)ρabs	(3κ)	+	Pss	(T) +	(Eμ∞ -	J - YJ E ~Γ,	(46)
∃κ >	0,	ρσESBAS (T)	≤ (3K +	1)ρσbs	(3K)	+	PσESBAS	(T) +	K log(T),	(47)
with κ = κ3
Rmin
1-γ
, which proves the theorem.
□
30