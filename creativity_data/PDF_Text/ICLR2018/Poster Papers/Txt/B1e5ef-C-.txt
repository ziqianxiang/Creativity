Published as a conference paper at ICLR 2018
A compressed sensing view of unsupervised
TEXT EMBEDDINGS, BAG-OF-n-GRAMS, AND LSTMS
Sanjeev Arora, Mikhail Khodak, Nikunj Saunshi
Princeton University
{arora,mkhodak,nsaunshi}@cs.princeton.edu
Kiran Vodrahalli
Columbia University
kiran.vodrahalli@columbia.edu
Ab stract
Low-dimensional vector embeddings, computed using LSTMs or simpler tech-
niques, are a popular approach for capturing the “meaning” of text and a form of
unsupervised learning useful for downstream tasks. However, their power is not
theoretically understood. The current paper derives formal understanding by look-
ing at the subcase of linear embedding schemes. Using the theory of compressed
sensing we show that representations combining the constituent word vectors
are essentially information-preserving linear measurements of Bag-of-n-Grams
(BonG) representations of text. This leads to a new theoretical result about LSTMs:
low-dimensional embeddings derived from a low-memory LSTM are provably at
least as powerful on classification tasks, up to small error, as a linear classifier
over BonG vectors, a result that extensive empirical work has thus far been unable
to show. Our experiments support these theoretical findings and establish strong,
simple, and unsupervised baselines on standard benchmarks that in some cases are
state of the art among word-level methods. We also show a surprising new property
of embeddings such as GloVe and word2vec: they form a good sensing matrix for
text that is more efficient than random matrices, the standard sparse recovery tool,
which may explain why they lead to better representations in practice.
1	Introduction
Much attention has been paid to using LSTMs (Hochreiter & Schmidhuber, 1997) and similar models
to compute text embeddings (Bengio et al., 2003; Collobert & Weston, 2008). Once trained, the
LSTM can sweep once or twice through a given piece of text, process it using only limited memory,
and output a vector with moderate dimensionality (a few hundred to a few thousand), which can be
used to measure text similarity via cosine similarity or as a featurization for downstream tasks.
The powers and limitations of this method have not been formally established. For example, can
such neural embeddings compete with and replace traditional linear classifiers trained on trivial
Bag-of-n-Grams (BonG) representations? Tweaked versions of BonG classifiers are known to be a
surprisingly powerful baseline (Wang & Manning, 2012) and have fast implementations (Joulin et al.,
2017). They continue to give better performance on many downstream supervised tasks such as IMDB
sentiment classification (Maas et al., 2011) than purely unsupervised LSTM representations (Kiros
et al., 2015; Hill et al., 2016; Pagliardini et al., 2017). Even a very successful character-level (and thus
computation-intensive, taking a month of training) approach does not reach BonG performance on
datasets larger than IMDB (Radford et al., 2017). Meanwhile there is evidence suggesting that simpler
linear schemes give compact representations that provide most of the benefits of word-level LSTM
embeddings (Wieting et al., 2016; Arora et al., 2017). These linear schemes consist of simply adding
up, with a few modifications, standard pretrained word embeddings such as GloVe or word2vec
(Mikolov et al., 2013; Pennington et al., 2014).
1
Published as a conference paper at ICLR 2018
The current paper ties these disparate threads together by giving an information-theoretic account
of linear text embeddings. We describe linear schemes that preserve n-gram information as low-
dimensional embeddings with provable guarantees for any text classification task. The previous linear
schemes, which used unigram information, are subcases of our approach, but our best schemes can
also capture n-gram information with low additional overhead. Furthermore, we show that the original
unigram information can be (approximately) extracted from the low-dimensional embedding using
sparse recovery/ComPressed sensing (Candes & Tao, 2005). Our approach also fits in the tradition
of the older work on distributed representations of structured objects, especially the works of Plate
(1995) and Kanerva (2009). The following are the main results achieved by this new world-view:
1.	Using random vectors as word embeddings in our linear scheme (instead of pretrained
vectors) already allows us to rigorously show that low-memory LSTMs are provably at
least as good as every linear classifier operating on the full BonG vector. This is a novel
theoretical result in deep learning, obtained relatively easily. By contrast, extensive empirical
study of this issue has been inconclusive (apart from character-level models, and even then
only on smaller datasets (Radford et al., 2017)). Note also that empirical work by its nature
can only establish performance on some available datasets, not on all possible classification
tasks. We prove this theorem in Section 4 by providing a nontrivial generalization of a result
combining compressed sensing and learning (Calderbank et al., 2009). In fact, before our
work we do not know of any provable quantification of the power of any text embedding.
2.	We study theoretically and experimentally how our linear embedding scheme improves
when it uses pretrained embeddings (GloVe etc.) instead of random vectors. Empirically we
find that this improves the ability to preserve Bag-of-Words (BoW) information, which has
the following restatement in the language of sparse recovery: word embeddings are better
than random matrices for “sensing” BoW signals (see Section 5). We give some theoretical
justification for this surprising finding using a new sparse recovery property characterizing
when nonnegative signals can be reconstructed by '1-minimization.
3.	Section 6 provides empirical results supporting the above theoretical work, reporting ac-
curacy of our linear schemes on multiple standard classification tasks. Our embeddings
are consistently competitive with recent results and perform much better than all previous
linear methods. Among unsupervised word-level representations they achieve state of the art
performance on both the binary and fine-grained SST sentiment classification tasks (Socher
et al., 2013). Since our document representations are fast, compositional, and simple to
implement given standard word embeddings, they provide strong baselines for future work.
L⅜∙∙
Text Transducer Embedding
2	Related Work
Neural text embeddings are instances of distributed representations, long studied in connectionist
approaches because they decay gracefully with noise and allow distributed processing. Hinton
(1990) provided an early problem formulation, and Plate (1995) provided an elementary solution,
the holographic distributed representation, which represents structured objects using circular vector
convolution and has an easy and more compact implementation using the fast Fourier transform
(FFT). Plate suggested applying such ideas to text, where “structure” can be quantified using parse
trees and other graph structures. Our method is also closely related in form and composition to the
sparse distributed memory system of Kanerva (2009). In the unigram case our embedding reduces to
the familiar sum of word embeddings, which is known to be surprisingly powerful (Wieting et al.,
2016), and with a few modifications even more so (Arora et al., 2017).
2
Published as a conference paper at ICLR 2018
Representations of BonG vectors have been studied through the lens of compression by Paskov et al.
(2013), who computed representations based on classical lossless compression algorithms using a
linear program (LP). Their embeddings are still high-dimensional (d > 100K) and quite complicated
to implement. In contrast, linear projection schemes are simpler, more compact, and can leverage
readily available word embeddings. Pagliardini et al. (2017) also used a linear scheme, representing
documents as an average of learned word and bigram embeddings. However, the motivation and
benefits of encoding BonGs in low-dimensions are not made explicit. The novelty in the current paper
is the connection to compressed sensing, which is concerned with recovering high-dimensional sparse
signals x ∈ RN from low-dimensional linear measurements Ax, specifically by studying conditions
on matrix A ∈ Rd×N when this is possible (see Appendix A for some background on compressed
sensing and the previous work of Calderbank et al. (2009) that we build upon).
3	Document Embeddings
In this section we define the two types of representations that our analysis will relate:
1.	high-dimensional BonG vectors counting the occurrences of each k-gram for k ≤ n
2.	low-dimensional embeddings, from simple vector sums to novel n-gram-based embeddings
Although some of these representations have been previously studied and used, we define them so as
to make clear their connection via compressed sensing, i.e. that representations of the second type
are simply linear measurements of the first.
We now define some notation. Let V be the number of words in the vocabulary and Vn be the number
of n-grams (independent of word order), so that V = V1. Furthermore set Vnsum = Pk≤n Vk and
Vnmax = maxk≤n Vk. We will use words/n-grams and indices interchangeably, e.g. if (a, b) is the
ith of V2 bigrams then the one-hot vector e(a,b) will be 1 at index i. Where necessary we will use
{, } to denote a multi-set and (, ) to denote a tuple. For any m vectors vi ∈ Rd for i = 1, . . . , m
we define [v1, . . . , vm] to be their concatenation, which is thus an element of Rmd. Finally, for any
subset X ⊂ RN we denote by ∆X the set {x - x0 : x, x0 ∈ X}.
3.1 THE BAG-OF-n-GRAMS VECTOR
Assigning to each word a unique index i ∈ [V] we define the Bag-of-Words (BoW) representation
xBoW of a document to be the V -dimensional vector whose ith entry is the number of times word i
occurs in the document. The n-gram extension of BoW is the Bag-of-n-Grams (BonG) representation,
which counts the number of times any k-gram for k ≤ n appears in a document. Linear classification
over such vectors has been found to be a strong baseline (Wang & Manning, 2012).
For ease of analysis we simplify the BonG approach by merging all n-grams in the vocabulary that
contain the same words but in a different order. We call these features n-cooccurrences and find that
the modification does not affect performance significantly (see Table 3 in Appendix F.1). Formally for
a document w1 , . . . , wT we define the Bag-of-n-Cooccurrences (BonC) vector as the concatenation
T	T -n+1
xBonC =	ewt ,	... ,	e{wt,...,wt+n-1}	(1)
t=1	t=1
which is thus a Vnsum-dimensional vector. Note that for unigrams this is equivalent to the BoW vector.
3.2 LOW-DIMENSIONAL n-GRAM EMBEDDINGS
Now suppose each word w has a vector vw ∈ Rd for some d V . Then given a document
w1, . . . , wT we define its unigram embedding as zu = PtT=1 vwt. While this is a simple and widely
used featurization, we focus on the following straightforward relation with BoW: if A ∈ Rd×V is
a matrix whose columns are word vectors vw then AxBoW = PtT=1 Aewt = PtT=1 vwt = zu. Thus
in terms of compressed sensing the unigram embedding of a document is a d-dimensional linear
measurement of its Bag-of-Words vector.
3
Published as a conference paper at ICLR 2018
We could extend this unigram embedding to n-grams by first defining a representation for each n-
gram as the tensor product of the vectors of its constituent words. Thus for each bigram b = (w1, w2)
we would have vb = vw1 vwT2 and more generally vg = Ntn=1 vwt for each n-gram g = (w1 , . . . , wn).
The document embedding would then be the sum of the tensor representations of all n-grams.
The major drawback of this approach is of course the blowup in dimension, which in practice
prevents its use beyond n = 2. To combat this a low-dimensional sketch or projection of the
tensor product can be used, such as the circular convolution operator of Plate (1995). Since we are
interested in representations that can also be constructed by an LSTM, we instead sketch this tensor
product using the element-wise multiplication operation, which we find also usually works better
than circular convolution in practice (see Table 4 in Appendix F.1). Thus for the n-cooccurrence
g = {wι,..., Wn}, We define the distributed cooccurrence (DisC) embedding Vg = dn-^ Jn=I Vwt.
The coefficient is required when the vectors vw are random and unit norm to ensure that the product
also has close to unit norm (see Lemma B.1). In addition to their convenient form, DisC embeddings
have nice theoretical and practical properties: they preserve the original embedding dimension, they
reduce to unigram (Word) embeddings for n = 1, and under mild assumptions they satisfy useful
compressed sensing properties With overWhelming probability (Lemma 4.1).
We then define the DisC document embedding to be the nd-dimensional Weighted concatenation,
over k ≤ n, of the sum of the DisC vectors of all k-grams in a document:
T	T-n+1
Z⑺=Cl X Vwt ,	... , Cn X V{wt,..,wt + n-i}	⑵
t=1	t=1
Here scaling factors Ck are set so that all spans of d coordinates have roughly equal norm (for random
embeddings Ck = 1; for word embeddings Ck = 1/k works well). Note that since vwt = Vwt we
have z(1) = zu in the unigram case. Furthermore, as With unigram embeddings by comparing (1) and
(2) one can easily construct a Pkn=1 dn × Vnsum matrix A(n) such that z(n) = A(n)xBonC.
3.3 LSTM Representations
As discussed previously, LSTMs have become a common way to apply the expressive power of
RNNs, with success on a variety of classification, representation, and sequence-to-sequence tasks.
For document representation, starting with h0 = 0m an m-memory LSTM initialized with word
vectors Vw ∈ Rd takes in words w1 , . . . , wT one-by-one and computes the document representation
ht = f(Tf(Vwt,ht-1)) ◦ ht-1 + i(Ti(Vwt,ht-1)) ◦ g(Tg(Vwt, ht-1))	(3)
where ht ∈ Rm is the hidden representation at time t, the forget gate f, input gate i, and input
function g are a.e. differentiable nondecreasing elementwise “activation” functions Rm 7→ Rm, and
affine transformations T (x, y) = W*x + U*y + b* have weight matrices W* ∈ Rm×d, U ∈ Rm×m
and bias vectors b* ∈ Rm. The LSTM representation of a document is then the state at the last time
step, i.e. zLSTM = hT. Note that we will follow the convention of using LSTM memory to refer to
the dimensionality of the hidden states. Since the LSTM is initialized with an embedding for each
word it requires O(m2 + md + V d) computer memory, but the last term is just a lookup table so the
vocabulary size does not factor into iteration or representation complexity.
From our description of LSTMs it is intuitive to see that one can initialize the gates and input functions
so as to construct the DisC embeddings defined in the previous section. We state this formally and
give the proof in the unigram case (the full proof appears in Appendix B.3):
Proposition 3.1. Given word vectors Vw ∈ Rd, one can initialize an O(nd)-memory LSTM (3) that
takes in words w1 , . . . , wT (padded by an end-of-document token assigned vector 0d) and constructs
the DisC embedding (2) (up to zero padding), i.e. such that for all documents zLSTM = z(n).
Proof (Unigram Case). Set f(x) = i(x) = g(x) = x, Tf (Vwt, ht-1) = Ti (Vwt, ht-1) = 1d, and
Tg (Vwt, ht-1) = C1Vwt. Then ht = ht-1 + C1Vwt, so since h0 = 0d we have the final LSTM
representation ZLSTM = hτ = Ci Pt=ι Vwt = Z⑴.	□
By Proposition 3.1 we can construct a fixed LSTM that can compute compressed BonC representations
on the fly and be further trained by stochastic gradient descent using the same memory.
4
Published as a conference paper at ICLR 2018
4 LSTMs as Compressed Learners
Our main contribution is to provide the first rigorous analysis of the performance of the text embed-
dings that we are aware of, showing that the embeddings of Section 3.2 can provide performance on
downstream classification tasks at least as well any linear classifier over BonCs. Before stating the
theorem we make two mild simplifying assumptions on the BonC vectors:
1.	The vectors are scaled by τ√n, where T is the maximum document length. This assumption
is made without loss of generality.
2.	No n-cooccurrence contains a word more than once. While this is (infrequently) violated in
practice, the problem can be circumvented by merging words as a preprocessing step.
Theorem 4.1. Let S = {(xi , yi)}im=1 be drawn i.i.d. from a distribution D over BonC vectors of
documents of length at most T satisfying assumptions 1 and2 above and let w0 be the linear classifier
minimizing the logistic loss'd. Thenfor dimension d = Ω (JT log nVn ) and appropriate choice of
regularization coefficient one can initialize an O(nd)-memory LSTM over i.i.d. word embeddings
Vw 〜 Ud{±1∕√d} such that w.p. (1 一 γ)(1 — 2δ) the classifier W minimizing the '2-regularized
logistic loss over its representations satisfies
'd (W) ≤'d (wo) + O
kw0k2 J + glθg 1
(4)
The above theoretical bound shows that LSTMs match BonC performance as ε → 0, which can be
realized by increasing the embedding dimension d (c.f. Figure 5).
4.1	Compressed Sensing and Learning
Compressed sensing is concerned with recovering a high-dimensional k-sparse signal x ∈ RN from
a few linear measurements; given a design matrix A ∈ Rd×N this is formulated as
minimize kWk0 subject to AW = z	(5)
where z = Ax is the measurement vector. As l0-minimization is NP-hard, research has focused on
sufficient conditions for tractable recovery. One such condition is the Restricted Isometry Property
(RIP), for which Candes & Tao (2005) proved that (5) can be solved by convex relaxation:
Definition 4.1. A ∈ Rd×N is (X , ε)-RIP for some subset X ⊂ RN if ∀ x ∈ X
(1 一 ε)kxk2 ≤ kAxk2 ≤ (1 + ε)kxk2	(6)
We will abuse notation and say (k, ε)-RIP when X is the set of k-sparse vectors. This is the more
common definition, but ours allows a more general Theorem 4.2 and a tighter bound in Theorem 4.1.
Following these breakthroughs, Calderbank et al. (2009) studied whether it is possible to use the
low-dimensional output of compressed sensing as a surrogate representation for classification. They
proved a learning-theoretic bound on the loss of an SVM classifier in the compressed domain
compared to the best classifier in the original domain. In this work we are interested in comparing
the performance of LSTMs with BonC representations, so we need to generalize the Calderbank et al.
(2009) result to handle Lipschitz losses and an arbitrary set X ⊂ RN of high-dimensional signals:
Theorem 4.2. For any subset X ⊂ RN containing the origin let A ∈ Rd×N be (∆X, ε)-RIP and
letm samples S = {(xi, yi)}im=1 ⊂ X × {一1, 1} be drawn i.i.d. from some distribution D over X
with kxk2 ≤ R. If ' is a λ-Lipschitz convex loss function and W0 ∈ RN is its minimizer over D
then w.p. 1 — 2δ the linear classifier WA ∈ Rd minimizing the '2-regularized empirical loss function
'sa (W) + 2C ∣∣w∣∣2 over the compressed sample SA = {(Axi, yi)}i=1 ⊂ Rd X { — 1,1} satisfies
'd(WA) ≤ 'd(wo) + O
xRkWok2 J+mlogδ
(7)
for appropriate choice of C. Recall that ∆X = {x — x0 : x, x0 ∈ X} for any X ⊂ RN.
5
Published as a conference paper at ICLR 2018
While a detailed proof of this theorem is spelled out in Appendix C, the main idea is to compare the
distributional loss incurred by a classifier W in the original space to the loss incurred by Aw in the
compressed space. We show that the minimizer of the regularized empirical loss in the original space
(W) is a bounded-CoeffiCient linear combination of samples in S, so its loss depends only on inner
products between points in X. Thus using RIP and a generalization error result by Sridharan et al.
(2008) We can bound the loss of Wa,the regularized classifier in the compressed domain. Note that
to get back from Theorem 4.2 the O(√ε) bound for k-sparse inputs of Calderbank et al. (2009) we
can set X to the be the set of k-sparse vectors and assume A is (2k, ε)-RIP.
4.2	Proof of Main Result
To apply Theorem 4.2 we need the design matrix A(n) transforming BonCs into the DisC embeddings
of Section 3.2 to satisfy the following RIP condition (Lemma 4.1), which we prove using a restricted
isometry result for structured random sampling matrices in Appendix D:
Lemma 4.1. Assume the setting of Theorem 4.1 and let A(n) be the nd × Vnsum matrix relating DisC
and BonC representations of any document by z(n)
A(n)xBonC. If d = Ω (εT2 log nVn ) then A(n)
is
∆XT(n),ε -RIP w.p. 1
- γ, where XT(n) is the set of BonCs of documents of length at most T.
ProofofTheorem 4.1. Let S = {(A(n)Xi, yi) : (xi, yi) ∈ S}, where A(n) is as in Lemma 4.1. Then
by the same lemma A(n) is ∆XT(n), ε -RIP w.p. 1 - γ, where XT(n) is the set of BonC vectors of
documents of length at most T. By BonC assumption (1) all BonCs lie within the unit ball, so we can
apply Theorem 4.2 with ' the logistic loss, λ = 1, and R = 1 to get that a classifier W trained using '2-
regularized logistic loss over S will satisfy the required bound (4). Since by Proposition 3.1 one can
initialize an O (nd)-memory LSTM that takes in i.i.d. Rademacher word vectors Vw 〜U d{±1∕√d}
such that ZLSTM = z(n) = A(n)χ ∀ X ∈ XTn), this completes the proof.	□
5 Sparse Rec overy with Pretrained Embeddings
Theorem 4.1 is proved using random vectors as the word embeddings in the scheme of Section 3.
However, in practice LSTMs are often initialized with standard word vectors such as GloVe. Such
embeddings cannot satisfy traditional compressed sensing properties such as RIP or incoherence. This
follows essentially from the definition: word embeddings seek to capture word similarity, so similar
words (e.g. synonyms) have embeddings with high inner product, which violates both properties.
Thus the efficacy of real-life LSTMs must have some other explanation. But in this section we present
the surprising empirical finding that pretrained word embeddings are more efficient than random
vectors at encoding and recovering BoW information via compressed sensing. We further sketch a
potential explanation for this result, though a rigorous explanation is left for subsequent work.
5.1	Pretrained Embeddings Preserve Sparse Information
In recent years word embeddings have been discovered to have many remarkable properties, most
famously the ability to solve analogies (Mikolov et al., 2013). Our connection to compressed sensing
indicates that they should have another: preservation of sparse signals as low-dimensional linear
measurements. To examine this we subsample documents from the SST (Socher et al., 2013) and
IMDB (Maas et al., 2011) classification datasets, embed them as d-dimensional unigram embeddings
z = Ax for d = 50, 100, 200, . . . , 1600 (where A ∈ Rd×V is the matrix of word embeddings and
x is a document’s BoW vector), solve the following LP, known as Basis Pursuit (BP), which is the
standard '1-minimization problem for sparse recovery in the noiseless case (see Appendix A):
minimize kWk1 subject to AW = z	(8)
Success is measured as the F1 score of retrieved words. We use Squared Norm (SN) vectors (Arora
et al., 2016) trained on a corpus of Amazon reviews (McAuley et al., 2015) and normalized i.i.d.
Rademacher vectors as a baseline. SN is used due to similarity to GloVe and its formulation via
an easy-to-analyze generative model that may provide a framework to understand the results (see
Appendix F.2), while the Amazon corpus is used for its semantic closeness to the sentiment datasets.
6
Published as a conference paper at ICLR 2018
3』00s—TLLω^Sω><
50 Loo 200	400
Vector Dimension
a18s,ILLΦCTrt⅛><
Figure 1: Average F1 -score of 200 recovered
BoW vectors from SST (left) and IMDB (right)
compared to dimension. Pretrained word embed-
dings (SN trained on Amazon reviews) need half
the dimensionality of normalized Rademacher
vectors to achieve near-perfect recovery. Note
that IMDB documents are on average more than
ten times longer than SST documents.
Figure 2: F1 -score of 1000 recovered BoWs
compared to number of unique words. Real doc-
uments (left) are drawn from the SST and IMDB
corpora; random signals (right) are created by
picking words at random. For d = 200, pre-
trained embeddings are better than Rademacher
vectors as sensing vectors for natural language
BoW but are worse for random sparse signals.
Recovery Of RandOm Documents,
1.0 ≡5s~≡	× Pretrained Vectors
n a	Rademacher Vectors
50	100	150	200
Number of Unique Words
Figures 1 and 2 show that pretrained embeddings require a lower dimension d than random vectors
to recover natural language BoW. This is surprising as the training objective goes against standard
conditions such as approximate isometry and incoherence; indeed as shown in Figure 2 recovery
is poor for randomly generated word collections. The latter outcome indicates that the fact that a
document is a set of mutually meaningful words is important for sparse recovery using embeddings
trained on co-occurrences. We achieve similar results with other objectives (e.g. GloVe/word2vec)
and other corpora (see Appendix F.1), although there is some sensitivity to the sparse recovery method,
as other 'ι-minimization methods work well but greedy methods, such as Orthogonal Matching
Pursuit (OMP), work poorly, likely due to their dependence on incoherence (Tropp, 2004).
For the n-gram case (i.e. BonC recovery for n > 1), although we know by Lemma 4.1 that DisC
embeddings composed from random vectors satisfy RIP, for pretrained vectors it is unclear how to
reason about suitable n-gram embeddings without a rigorous understanding of the unigram case,
and experiments do not show the same recovery benefits. One could perhaps do well by training on
cooccurrences of word tuples, but such embeddings could not be used by a word-level LSTM.
5.2	Understanding Sparse Recovery of Natural Language Documents
As shown in Figure 2, the success of pretrained embeddings for linear sensing is a local phenomenon;
recovery is only efficient for naturally occurring collections of words. However, applying statistical
RIP/incoherence ideas (Barg et al., 2015) to explain this is ruled out since they require collections to
be incoherent with high probability, whereas word embeddings are trained to give high inner product
to words appearing together. Thus an explanation must come from some other, weaker condition.
The usual necessary and sufficient requirement for recovering all signals with support S ⊂ [N] is the
local nullspace property (NSP), which stipulates that vectors in the kernel of A not have too much
mass on S (see Definition A.2). While NSP and related properties such as restricted eigenvalue (see
Definition A.3) are hard to check, we can impose some additional structure to formulate an intuitive,
verifiable perfect recovery condition for our setting. Specifically, since our signals (BoW vectors) are
nonnegative, we can improve upon solving BP (8) by instead solving nonnegative BP (BP+):
minimize kwk1 subject to Aw = z, w ≥ 0d	(9)
The following geometric result then characterizes when solutions of BP+ recover the correct signal:
Theorem 5.1 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N]
of size k. Then any nonnegative vector x ∈ R+N with support supp(x) = S is recovered from Ax by
BP+ iff the set AS of columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.
This theorem equates perfect recovery of a BoW vector via BP+ with the vectors of its words being
the vertices of some face of the polytope conv(A). The property holds for incoherent columns since
the vectors are far enough that no one vector is inside the simplex formed by any k others. On the
other hand, pretrained embeddings satisfy it by having commonly co-occurring words close together
and other words far away, making it easier to form a face from columns indexed by the support of a
BoW. We formalize this intuition as the Supporting Hyperplane Property (SHP):
7
Published as a conference paper at ICLR 2018
100	200
Vector Dimension
100	200
Vector Dimension
GIoVe Embeddinqs
pə,iəaooəb UOnJodO-Id
100	2∞
Vector Dimension
SN Embeddmgs
400	800	1600
Vector Dimension
P9」①>0□9α UoEOdO.Id
pφ,lφ>ooφα UOEOdO.Id
Rademacher Embeddmgs
200 400	800	1600
Vector Dimension
Figure 3: Proportion of 500 randomly sampled documents from SST (top) and IMDB (bottom) that
are perfectly recovered from linear measurements.
Definition 5.1. A matrix A ∈ Rd×N satisfies S-SHP for subset S ⊂ [N] if its columns are in general
position and there is a hyperplane containing the set AS of columns of A indexed by S such that the
set of all other columns of A together with the origin are on one side of the hyperplane.
SHP is a very weak property implied by NSP (Corollary E.1). However, it can be checked by using
convex optimization to see if the hyperplane exists (Appendix E.2). Furthermore, we show (full
proof in Appendix E.1) that this hyperplane is the supporting hyperplane of the face of conv(A) with
vertices AS, from which it follows by Theorem 5.1 that SHP characterizes recovery using BP+:
Corollary 5.1. BP+ recovers any x ∈ R+N with supp(x) = S from Ax iff A satisfies S-SHP.
Proof Sketch. By Theorem 5.1 it suffices to show equivalence of S-SHP with the column set AS
comprising the vertices of a k-dimensional face of conv(A). A face F of polytope P is defined as its
intersection with some hyperplane such that all points in P\F lie on one side of the hyperplane.
( =⇒ ) Let F be the face of conv(A) formed by the columns AS . Then there must be a supporting
hyperplane H containing F. Since the columns of A are in general position, all columns AS = A\As
lie in conv(A)\F and hence must all be on one side of H, so H is the desired hyperplane.
(^= ) Let H be the hyperplane supporting AS, with all other columns on one side of H. By
convexity, H contains the simplex F of AS. Any point in conv(A)\F can be written as a convex
combination of points in F and columns AS, with a positive coefficient on at least one of the columns,
and so must lie on the same side of H as AS. Thus AS comprises the vertices of a face F.	□
Thus perfect recovery of a BoW via BP+ is equivalent to the existence of a hyperplane separating
embeddings of words in the document from those of the rest of the vocabulary. Intuitively, words
in the same document are trained to have similar embeddings and so will be easier to separate out,
providing some justification for why pretrained vectors are better for sensing. We verify that SHP
is indeed more likely to be satisfied by such designs in Figure 3, which also serves as an empirical
check of Corollary 5.1 since SHP satisfaction implies BP recovery as the latter can do no better
than BP+. We further compare to recovery using OMP/OMP+ (the latter removes negative values
and recomputes the set of atoms at each iteration); interestingly, while OMP+ recovers the correct
signal from SN almost as often as BP/BP+, it performs quite poorly for GloVe, indicating that these
embeddings may have quite different sensing properties despite similar training objectives.
As similarity properties that may explain these results also relate to downstream task performance, we
conjecture a relationship between embeddings, recovery, and classification that may be understood
under a generative model (see Appendix F.2). However, the Section 4 bounds depend on RIP, not
recovery, so these experiments by themselves do not apply. They do show that the compressed sensing
framework remains relevant even in the case of non-random, pretrained word embeddings.
8
Published as a conference paper at ICLR 2018
Representation n	d*	MR	CR	SUBJ	MPQA	TREC	SST (±1)	SST	IMDB
1	V1	77.1	77.0	91.0	85.1	86.8	80.7	36.8	88.3
BonC (1)	2	V2sum	77.8	78.1	91.8	85.8	90.0	80.9	39.0	90.0
3	V3sum	77.8	78.3	91.4	85.6	89.8	80.1	42.3	89.8
1	1600	79.6	81.0	92.4	87.8	85.2	84.6	45.7	89.2
DisC (2)	2	3200	80.1	81.5	92.6	87.9	89.6	85.5	46.4	89.4
3	4800	80.0	81.3	92.6	87.9	90.0	85.2	46.7	89.6
SIF1	1	1600	79.6	81.1	92.5	87.7	85.6	84.4	45.8	89.2
Sent2Vec2	1	700	76.2	78.7	91.2	87.2	85.8	80.2	31.0	85.5
Sent2Vec2	2	700	76.3	79.1	91.1	86.6	84.2	80.0	30.7	85.3
CFL3	5	100K+								90.4
Paragraph Vec.4		74.8	78.1	90.5	74.2	91.8			
skip-thoughts4	4800	80.3	83.8	94.2	88.9	93.0	85.1	45.8	
SDAE5	2400	74.6	78.0	90.8	86.9	78.4			
CNN-LSTM6	4800	77.8	82.0	93.6	89.4	92.6			
byte mLSTM7	4096	86.8	90.6	94.7	88.8	90.4	91.7	54.6	92.2
* Vocabulary sizes (i.e. BonC dimensions) vary by task; usually 10K-100K.
1 Arora et al. (2017) Reported performance of best hyperparameter using Amazon GloVe embeddings.
2,4,7 Pagliardini et al. (2017); Kiros et al. (2015); Radford et al. (2017) Evaluated latest pretrained models.
Note that the available skip-thoughts implementation fails on the IMDB and MRPC tasks
3,5,6 Paskov et al. (2013); Hill et al. (2016); Gan et al. (2017) From publication (+emb version of last two).
Table 1:	Evaluation of DisC and recent unsupervised word-level approaches on standard classification
tasks, with the character LSTM of Radford et al. (2017) shown for comparison. The top three results
for each dataset are bolded, the best is italicized, and the best word-level performance is underlined.
6 Empirical Findings
Our theoretical results show that simple tensor product sketch-based n-gram embeddings can approach
BonG performance and be computed by a low-memory LSTM. In this section we compare these text
representations and others on several standard tasks, verifying that DisC performance approaches that
of BonCs as dimensionality increases and establishing several baselines for text classification. Code to
reproduce results is provided at https://github.com/NLPrinceton/text_embedding.
Tasks: We test classification on MR movie reviews (Pang & Lee, 2005), CR customer reviews (Hu &
Liu, 2004), SUBJ subjectivity dataset (Pang & Lee, 2004), MPQA opinion polarity subtask (Wiebe
et al., 2005), TREC question classification (Li & Roth, 2002), SST sentiment classification (binary
and fine-grained) (Socher et al., 2013), and IMDB movie reviews (Maas et al., 2011). The first four
are evaluated using 10-fold cross-validation, while the others have train-test splits. In all cases we use
logistic regression with '2-regularization determined by cross-validation. We further test DisC on the
SICK relatedness and entailment tasks (Marelli et al., 2014) and the MRPC paraphrase detection task
(Dolan & Brockett, 2005). The inputs here are sentences pairs (a, b) and the standard featurization
for document embeddings xa and xb of a and b is [|xa - xb|, xa xb] (Tai et al., 2015). We use
logistic regression for SICK entailment and MRPC and use ridge regression to predict similarity
scores for SICK relatedness, with '2-regularization determined by cross-validation. Since BonGs
are not used for pairwise tasks our theory says nothing about performance here; we include these
evaluations to show that our representations are also useful for other tasks.
Embeddings: In the main evaluation (Table 1) we use normalized 1600-dimensional GloVe embed-
dings (Pennington et al., 2014) trained on the Amazon Product Corpus (McAuley et al., 2015), which
are released at http://nlp.cs.princeton.edu/DisC. We also compare the SN vectors of
Section 5 trained on the same corpus with random vectors when varying the dimension (Figure 5).
9
Published as a conference paper at ICLR 2018
Rep. n	SICK-R (r∕ρ)	SICK-E MRPC (Acc./Fi)
	1	73.6 / 71.0	81.9	73.1 / 81.8
DisC (2)	2	75.6 / 72.1	83.2	70.8 / 79.0
	3	76.2 / 72.2	82.5	73.1 / 81.6
SIF	1	73.7/68.5	82.4	73.5 / 82.1
Sent2Vec	1	69.3 / 64.6	78.6	71.3 / 80.7
Sent2Vec	2	70.1 / 65.3	78.7	70.0 / 79.3
82.4 / 76.0
83.2
73.7 / 80.7
76.4 / 83.8
skip-thoughts
SDAE
CNN-LSTM
byte mLSTM 78.5 / 72.1	80.0	73.8 / 81.4
Model WaJI-CIock Comparison
Figure 4: Time needed to initialize
model, construct document representa-
tions, and train a linear classifier on a
16-core compute node.
(n=3) (a=lE-2) (n=3) thoughts mLSTM
Model information is the same as that in Table 1.
Table 2:	Performance of DisC and other recent approaches
on pairwise similarity and classification tasks. The top three
results for each task are bolded and the best is Underlined.
0 5 0 5 0 5
9 8 8 7 7 6
BamUO XOBJn∞< UoqBO≡sseQ
Figure 5: IMDB performance of unigram (left) and bigram
(right) DisC embeddings compared to the original dimension.
908580757065
man- uo xoaln∞< Uo=Bo≡SSEO
Ooooo
9 8 7 6 5
m□z- co&aJnUU4 Uo-sU≡ss-°
Figure 6: IMDB performance com-
pared to training sample size.
Results: We find that DisC representation performs consistently well relative to recent unsupervised
methods; among word-level approaches itis the top performer on the SST tasks and competes on many
others with skip-thoughts and CNN-LSTM, both concatenations of two LSTM representations. While
success may be explained by training on a large and in-domain corpus, being able to use so much text
without extravagant computing resources is one of the advantages of a simple approach. Overall our
method is useful as a strong baseline, often beating BonCs and many more complicated approaches
while taking much less time to represent and train on documents than neural representations (Figure 4).
Finally, we analyze empirically how well our model approximates BonC performance. As predicted
by Theorem 4.1, the performance of random embeddings on IMDB approaches that of BonC as
dimension increases and the isometry distortion ε decreases (Figure 5). Using pretrained (SN) vectors,
DisC embeddings approach BonC performance much earlier, surpassing it in the unigram case.
7 Conclusion
In this paper we explored the connection between compressed sensing, learning, and natural language
representation. We first related LSTM and BonG methods via word embeddings, coming up with sim-
ple new document embeddings based on tensor product sketches. Then we studied their classification
performance, proving a generalization of the compressed learning result of Calderbank et al. (2009)
to convex Lipschitz losses and a bound on the loss of a low-dimensional LSTM classifier in terms
of its (modified) BonG counterpart, an issue which neither experiments nor theory have been able
to resolve. Finally, we showed how pretrained embeddings fit into this sparse recovery framework,
demonstrating and explaining their ability to efficiently preserve natural language information.
10
Published as a conference paper at ICLR 2018
Acknowledgments
We thank Rong Ge, Holden Lee, and Divyarthi Mohan for helpful discussions at various stages of this
effort. The work in this paper was in part supported by NSF grants CCF-1302518 and CCF-1527371,
Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329
References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model
approach to pmi-based word embeddings. Transactions ofthe ACL, 4:385-399, 2016.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence
embeddings. In Proceedings of the International Conference on Learning Representations, 2017.
Alexander Barg, Arya Mazumdar, and Rongrong Wang. Restricted isometry property of random
subdictionaries. IEEE Transactions on Information Theory, 61, 2015.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal of Machine Learning Research, 3:1137-1155, 2003.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Robert Calderbank, Sina Jafarpour, and Robert Schapire. Compressed learning: Universal sparse
dimensionality reduction and learning in the measurement domain. Technical report, 2009.
Emmanuel CandeS and Terence Tao. Decoding by linear programming. IEEE Transactions on
Information Theory, 51:4203-4215, 2005.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference on
Machine Learning, 2008.
Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In
Third International Workshop on Paraphrasing, 2005.
David L. Donoho and Jared Tanner. Sparse nonnegative solution of underdetermined linear equations
by linear programming. Proceedings of the National Academy of Sciences of the United States of
America., 102:9446-9451, 2005.
Simon Foucart and David Koslicki. Sparse recovery by means of nonnegative least squares. IEEE
Signal Processing Letters, 21:498-502, 2014.
Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. 2013.
Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, and Lawrence Carin Carin.
Learning generic sentence representations using convolutional neural networks. In Proceedings of
Empirical Methods in Natural Language Processing, 2017.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences
from unlabelled data. In Proceedings of the North American Chapter of the ACL, 2016.
Geoffrey Hinton. Mapping part-whole hierarchies into connectionist networks. Artificial Intelligence,
46:47-75, 1990.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9:
1735-1780, 1997.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient
text classification. In Proceedings of the 15th Conference of the European Chapter of the ACL,
2017.
11
Published as a conference paper at ICLR 2018
Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed represen-
tation with high-dimensional random vectors. Cognitive CompUtation, 1:139-159, 2009.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun,
and Sanja Fidler. Skip-thought vectors. In Neural Information Processing Systems, 2015.
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th International
Conference on Computational Linguistics, 2002.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of
the ACL: Human Language Technologies, 2011.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto
Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic model
on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th
International Workshop on Semantic Evaluation, 2014.
Julian McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and com-
plementary products. In Proceedings of the 21th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Neural Information Processing Systems,
2013.
Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings
using compositional n-gram features. arXiv, 2017.
Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the ACL,
2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the ACL, 2005.
Hristo S. Paskov, Robert West, John C. Mitchell, and Trevor J. Hastie. Compressive feature learning.
In Neural Information Processing Systems, 2013.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Proceedings of Empirical Methods in Natural Language Processing, 2014.
Tony Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 1995.
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering
sentiment, 2017. arXiv.
Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated
gaussian designs. Journal of Machine Learning Research, 11(Aug):2241-2259, 2010.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y.
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of Empirical Methods in Natural Language Processing, 2013.
Karthik Sridharan, Nathan Srebro, and Shai Shalev-Schwartz. Fast rates for regularized objectives.
In Neural Information Processing Systems. 2008.
Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations
from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting
of the ACL, 2015.
Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on
Information Theory, 50, 2004.
12
Published as a conference paper at ICLR 2018
Sida Wang and Christopher D. Manning. Baselines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual Meeting of the ACL, 2012.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation, 39:165-210, 2005.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic
sentence embeddings. In Proceedings of the International Conference on Learning Representations,
2016.
A Compressed Sensing Background
The field of compressed sensing is concerned with recovering a high-dimensional k-sparse signal
x ∈ RN from few linear measurements. In the noiseless case this is formulated as
minimize kwk0 subject to Aw = z	(10)
where A ∈ Rd×N is the design matrix and z = Ax is the measurement vector. Since `0 -minimization
is NP-hard, a foundational approach is to use its convex surrogate, the 'ι-norm, and characterize
when the solution to (10) is equivalent to that of the following LP, known as basis pursuit (BP):
minimize kwk1 subject to Aw = z	(11)
Related approaches such as Basis Pursuit Denoising (LASSO) and the Dantzig Selector generalize
BP to handle signal or measurement noise (Foucart & Rauhut, 2013); however, the word embeddings
case is noiseless so these methods reduce to BP. Note that throughout Section 5 and the Appendix
We say that an '1-minimization method recovers X from Ax if its optimal solution is unique and
equivalent to the optimal solution of (10).
An alternative Way to approximately solve (10) is to use a greedy algorithm such as matching pursuit
(MP) or orthogonal matching pursuit (OMP), Which pick basis vectors one at a time by multiplying
the measurement vector by AT and choosing the column With the largest inner product (Tropp, 2004).
A. 1 Some Common Sparse Recovery Conditions
One condition through Which recovery can be guaranteed is the Restricted Isometry Property (RIP):
Definition A.1. A ∈ Rd×N is (k, ε)-RIP if for all k-sparse x ∈ RN
(1 - ε)kxk2 ≤ kAxk2 ≤ (1 + ε)kxk2
A line of work started by Candes & Tao (2005) used the RIP property to characterize matrices A such
that (10) and (11) have the same minimizer for any k-sparse signal x; this occurs with overwhelming
probability when d = Ω (klog Nk) and √dAij 〜N(0,1) ∀ i,j or √dAj 〜U{-1,1} ∀ i,j.
Since the ability to recover a signal x from a representation Ax implies information preservation, a
natural next step is to consider learning after compression. Calderbank et al. (2009) show that for
m i.i.d. k-sparse samples {(xi, yi)}im=1 and a (2k, ε)-RIP matrix A, the hinge loss of a classifier
trained on {(Axi, yi)}im=1 is bounded by that of the best linear classifier over the original samples.
Theorem 4.2 provides a generalization of this result to any convex Lipschitz loss function.
RIP is a strong requirement, both because it is not necessary for perfect, stable recovery of k-sparse
vectors using O(k) measurements and because in certain settings we are interested in using the above
ideas to recover specific signals — those statistically likely to occur—rather than all k-sparse signals.
The usual necessary and sufficient condition to recover any vector x ∈ RN with index support set
S ⊂ [N] is the local nullspace property (NSP), which is implied by RIP:
Definition A.2 (Foucart & Rauhut, 2013). A matrix A ∈ Rd×N satisfies NSP for a set S ⊂ [N] if
IlWSkι < IlwSkι forall nonzero W ∈ ker(A) = {v : Av = 0d}.
Theorem A.1 (Foucart & Rauhut, 2013). BP (11) recovers any x ∈ R+N with supp(x) = S from Ax
iff A satisfies NSP for S.
13
Published as a conference paper at ICLR 2018
A related condition that implies NSP is the local restricted eigenvalue property (REP):
Definition A.3 (RaskUtti et al., 2010). A matrix A ∈ Rd×N satisfies Y-REPfor a set S ⊂ [N] if
∣∣Aw∣∣2 ≥ γ√d∣∣w∣∣2 whenever ∣∣ws∣∣ι ≤ ∣∣ws∣∣ι.
Lastly, a simple condition that can sometimes provide recovery gUarantees is mutual incoherence:
Definition A.4. A ∈ Rd×N is μ-incoherent if maxa,aθ IaTa0∣ ≤ μ, where the maximum is taken over
any two distinct columns a, a0 of A.
While incoherence is easy to verify (Unlike the previoUs recovery properties), word embeddings tend
to have high coherence dUe to the training objective pUshing together vectors of co-occUrring words.
A.2 Conditions for Recovering Nonnegative Signals
Apart from incoherence, the properties above are hard show empirically. However, we are compress-
ing BoW vectors, so oUr signals are nonnegative and we can impose an additional constraint on (11):
minimize ∣w∣1 sUbject to Aw = z, w ≥ 0d	(12)
The following geometric resUlt provides gUarantees for this nonnegative basis pursuit (BP+) problem:
Theorem A.2 (Donoho & Tanner, 2005). Consider a matrix A ∈ Rd×N and an index subset S ⊂ [N]
of size k. Then any nonnegative vector x ∈ RN+ with support supp(x) = S is recovered from Ax
by BP+ (12) iff the columns of A indexed by S comprise the vertices of a k-dimensional face of the
convex hull conv(A) of the columns of A together with the origin.
The polytope condition is eqUivalent to nonnegative NSP (NSP+), a weaker form of NSP:
Definition A.5 (FoUcart & Koslicki, 2014). A matrix A ∈ Rd×N satisfies NSP+ for a set S ⊂ [N] if
N
WS ≥ On =⇒ EWi > 0 for all nonzero W ∈ ker(A).
i=1
Lemma A.1. IfA ∈ Rd×N satisfies NSPfor some S ⊂ [N] then it also satisfies NSP+ for S.
Proof(Adaptedfrom Foucart & Koslicki (2014)). Since A satisfies NSP, we have ∣∣wS∣∣ι < ∣wS∣∣ι.
Then for a nonzero W ∈ ker(A) such that WS ≥ 0 We will have
N
Ewi = Ewi+Ewj ≥ -EIwiI+£ |Wj | = -∣ws i∣i + i∣ws i∣i > 0
i=1	i∈s j∈S	i∈s	j∈S
□
Lemma A.2. BP+ recovers any x ∈ R+N with supp(x) = S from Ax iffA satisfies NSP+ for S.
Proof. ( =⇒ ): For any nonzero w ∈ ker(A) such that WS ≥ 0, ∃ λ > 0 such that X + λw ≥ On
and A(x + λW) = Ax. Since BP+ uniquely recovers x, we have ∣x + λW∣1 > ∣x∣1, so NSP+
follows from the following inequality and the fact that λ is positive:
N	NN	N
0 < ∣x + λW∣1 - ∣x∣1 =	(xi + λWi) -	xi = λ	Wi =⇒	Wi > 0
(U= ): For any x0 ≥ 0 such that Ax0 = Ax we have that w = x0 一 X ∈ ker(A) and wS = X— ≥ 0
N
since the support of x is S. Thus by NSP+ we have that P Wi > 0, which yields
i=1
NNN
∣x0 ∣1 - ∣x∣1 =	x0i -	xi =	Wi > 0
i=1	i=1	i=1
Thus BP+ will recover X uniquely.	□
Lemma A.2 shows that NSP+ is equivalent to the polytope condition in Theorem A.2, as they are
both necessary and sufficient conditions for BP+ recovery.
14
Published as a conference paper at ICLR 2018
Model	Representation	MR	CR	SUBJ	MPQA	TREC	SST (±1)	SST	IMDB
Bigram	BonG	78.3	78.2	91.9	85.7	90.0	80.8	39.1	90.0
	BonC	77.8	78.1	91.8	85.8	90.0	80.9	39.0	90.0
Trigram	BonG	77.9	78.3	91.5	85.6	89.2	80.2	39.1	90.2
	BonC	77.8	78.3	91.4	85.6	89.8	80.1	42.3	89.8
Table 3: The performance of an l2-regularized logit classifier over Bag-of-n-Grams (BonG) vectors
is generally similar to that of Bag-of-n-Cooccurrences (BonC) vectors for n = 2, 3 (largest differ-
ences bolded). Evaluation settings are the same as in Section 6. Note that for unigrams the two
representations are equivalent.
Model	Representation	MR	CR	SUBJ	MPQA	TREC	SST (±1)	SST	IMDB
Bigram	DisC (2)	80.1	81.5	92.6	87.9	89.6	85.5	46.4	89.4
	Conv. (13)	79.8	81.4	92.6	87.9	89.4	84.3	45.5	89.5
Trigram	DisC (2)	80.0	81.3	92.6	87.9	90.0	85.2	46.7	89.6
	Conv. (13)	79.7	81.6	92.8	87.9	89.8	84.2	45.8	89.5
Table 4: Performance comparison of element-wise product (DisC) and circular convolution for
encoding local cooccurrences (best result for each task is bolded). Evaluation settings are the same
as in Section 6. Note that for unigrams the two representations are equivalent.
B Document Embeddings
B.1	Performance Comparison of Alternative Representations
In this section we compare the performance of several alternative representations with the ones
presented in the main evaluation (Table 1). Table 3 provides a numerical justification for our use of
unordered n-grams (cooccurrences) instead of n-grams, as the performance of the two featurizations
are closely comparable. In Table 4 we examine the use of circular convolution instead of elementwise
multiplication as linear measurements of BonC vectors Plate (1995). To construct the former from a
document w1 , . . . , wT we compute
T	T -n+1
vwt,..., F-1
t=1	t=1
t+n-1
F(vwτ)
(13)
where F is the discrete Fourier transform and F-1 its inverse. Note that for n = 1 this is equivalent
to the simple unigram embedding (and thus also to the DisC embedding in (2)).
B.2	Scaling Factor for DisC Embeddings
Lemma B.1. If word vectors Vw ∈ Rd are drawn i.i.d. from Ud{±1∕√d} then for any n-gram
g = (wι,..., Wn) we have Ekvg ∣∣2 = 1. The same result holds true with the additional assumption
that all words in g are distinct if the word vectors are i.i.d. d-dimensional spherical Gaussians.
Proof.
d	n	2	d	n	dn
Ekvgk2	= XE	dn-1	Yvwki	= dn-1XE	Yvwki	=	dn-1XYd = 1
i=1	k=1	i=1	k=1	i=1 i=1
□
15
Published as a conference paper at ICLR 2018
B.3 Proof OF Proposition 3.1
Let f (x) = i(x) = g(x) = x with
Tf (vwt,h"= (0Kd)
∖0(n-i)d/
0	0d×nd	…
.
.	I(n-2)d
Ti(VWt,	ht-1)=	.	...
.	..
..
0(n-2)d×nd 1(n-2)d
/ CiId	∖
.
.
.
Tg (Vwt ,ht-1)=	Cnd 2 Id VWt
Id
.
.
.
Id
0d×d
0(n-2)d×d
Id	ht-1+
/	Id	∖
0(n-1)d I
id	I
0(n-2)d
Substituting these parameters into the LSTM update (3) and using h0 = 0 We have ∀ t > 0 that
t
C1 P VWT
τ =1
t
C1 P VWT
τ =1
ht
_ι t—n+1
Cnd ɪ	P	On=1 VW i—1
τ =1
vWt
t—n+1
〉I ”{wt ,...,wt+n - 1 }
T =1
vWt
On— 1
k=1 vWt + k —n+1
v{Wt-n+2,∙∙∙,Wt }
/
∖
/
∖
∖

∖
/
Thus
/
T
C1 P Vwt
t=1
∖
(
z(n)
∖
hτ
T-n+1
Σ
t=1
“
VWT
v{wt,...,Wt + n-1}
WT
WT — n十2,...,WT -'})
∖	V{wT — n+2,...,wT }	)
Note that ht ∈ R(2n-1)d so as desired the LSTM has O (nd)-memory. Although hτ contains (n — 1)d
more dimensions than Z(n), by padding the end of the document with an end-of-document token
whose word vector is 0d the entries in those dimensions will be set to zero by the update at the last
step. Thus up to zero padding we will have ZLSTM = hτ = Z(n).
C Proof OF Theorem 4.2
Throughout this section we assume the setting described in Theorem 4.2. Furthermore for some
positive constant C define the '2-regularization of the loss function ' as
L(W) = '(W) + ɪ Ilwk2
2C
16
Published as a conference paper at ICLR 2018
m
Lemma C.1. Let W be the classifier obtained minimizing LS(W) = ml E '(wτxi,yi) + 备 Ilwll2,
i=1
where '(∙, ∙) is a convex λ-Lipschitzfunction in the first cordinate. Then
m
W = Eaiyixi	(14)
i=1
where ∣a∕ ≤ λC ∀ i. This result holds in the compressed domain as well.
Proof. If ` is an λ-Lipschitz function, its sub-gradient at every point is bounded by λ. So by convexity,
the unique optimizer is given by taking first-order conditions:
W 1m
0 = ∂wLS(w) = c + — E ∂wTXi'(wτxi, yi)xi
m i=1
Cm
=⇒ W = — E -IyidwTXi'(Wτxi, yi)yixi	(15)
m i=1
Since ' is Lipschitz, ∣∂wτx∕(wtxi, yi)∣ ≤ λ. Therefore the first-order optimal solution (15) of W
can be expressed as (14) for some α1,...,am satisfying |ai| ≤ M ∀ i, which is the desired result.
□
Lemma C.2. x, x0 ∈ X =⇒ (1 + ε)xT x0 - 2R2ε ≤ (Ax)T(Ax0) ≤ (1 - ε)xT x0 + 2R2ε
Proof. Since A is (∆X , ε)-RIP we have (1 - ε)Ix - x0 I2 ≤ IA(x - x0)I2 ≤ (1 + ε)Ix - x0 I2 .
Also since 0N ∈ X, A is also (X , ε)-RIP and the result then follows by the same argument as in
(Calderbank et al., 2009, Lemma 4.2-3).	□
Corollary C.1. kW∣2 ≤ λ2C2R2 and IlWAk2 ≤ λ2C2(1 + ε)2R2.
Proof. The first bound follows by expanding kW∣2 and using ∣∣χ∣2 ≤ R; the second follows by
expanding ∣∣Wa∣2, applying Lemma C.2 to bound inner product distortion, and using ∣∣x∣2 ≤ R. □
Lemma C.3. Let W be the linear classifier minimizing LS. Then
LD(AW) ≤ LD(W) + O(λ2CR2ε)
Proof. By Lemma C.1 we can re-express W using Equation 14 and then apply the inequality from
Lemma C.2 to get
m
(AW)T (Ax) = X αi yi(Axi )t (Ax)
i=1
≤ X aiyi ((1 - ε)xτx + 2R2ε) + X aiyi ((1 + ε)xτx - 2R2ε)
i∙aayi≥0	i:aiyi<0
mm
=WTx — ε^X ∣aiyi∣xτx + 2R2ε^X Qyi| ≤ WTx + 3λCR2ε
i=1	i=1
m
(AW)T (Ax) = X ai yi (Axi)T (Ax)
i=1
≥ X aiyi ((1 + ε)xTx — 2R2ε) + X ay ((1 — ε)xTx + 2R2ε)
i∙aayi≥0	i:aiyi<0
mm
=WTx + ε ^X ∣aiyi∣xTx — 2R2ε ^X ∣aiy∕ ≥ WTx — 3λCR2ε
i=1	i=1
17
Published as a conference paper at ICLR 2018
for any x ∈ RN. Since ` is λ-Lipschitz taking expectations over D implies
'd (Aw) ≤'d (W) + 3λ2CR2ε	(16)
Substituting Equation 14 applying Lemma C.2 also yields
mm
kAwk2 = XXαiαjyiyj(Axi)T(Axj)
i=1 j=1
≤	αiαjyiyj (1 - ε)xiT xj + 2R2ε
i,j:aiajyiyj ≥0
+	αiαjyiyj (1 + ε)xiT xj - 2R2ε
i,j:aiajyiyj <0
≤ EaiajyyXTXj +£-∣αiajyiy∕εxTXj +2R2∣αiajyy∕ε
i,j	i,j
≤ kwk2 + 3λ2C2R2ε
which implies
1	13
CkIlAwll2 ≤ CkIlwll2 + χλ CR ε
2C	2C	2
(17)
Together the inequalities bounding the loss term (16) and the regularization term (17) imply the result.
□
Lemma C.4. Let W be the linear classifier minimizing LS and let w* be the linear classifier
minimizing LD. Then with probability 1 - γ
λ2 CR2	1
LD(w) ≤ LD(w*) + O -----log —
mγ
This result holds in the compressed domain as well.
Proof. By Corollary C.1 We have that w is contained in a closed convex subset independent of S.
Therefore since ' is λ-Lipschitz, L is C-strongly convex, and ∣∣χ∣2 ≤ O(R), we have by (Sridharan
et al., 2008, Theorem 1) that With probability 1 - γ
λ2CR2	1
LD(w) - LD(w ) ≤ 2 [Ls(w) - LS(w )]+ + O -----log 一
+	mγ
Then since by definition w minimizes LS(w) we have that LS(w) ≤ LS(w*), which substituted into
the previous equation completes the proof.	□
Proof of Theorem 4.2. Applying Lemma C.4 in the compressed domain yields
1	λ2CR2	1
'd (wa) ≤ 'd (wa) + 鬲 IlwAII2 = LD (wa) ≤ LD (wa) + O I ---------log 一 )
2C	m	γ
where wA minimizes LD. By definition of wA, LD(wA) ≤ LD(Aw), so together with Lemma C.3
and the previous inequality we have
'd(Wa) ≤ LD(Aw) + O (λ CR log 1) ≤ LD(w) + O (λ2CR2 (ε + — log 1))
m γ	mγ
We now apply Lemma C.4 in the sparse domain to get
'd(Wa) ≤ Ld(w*) + O (λ2CR2 (ε + W log ；))
where w* minimizes LD. By definition of w*, LD(w*) ≤ LD(wo) = 'd(wo) + 击kwo∣∣2, so by
the previous inequality we have
'd(Wa) ≤'d(wo) + ɪIlwo∣∣2 + O (λ2CR2 (ε + — log 1))
2C	2	m γ
Substituting the C that minimizes the r.h.s. of this inequality completes the proof.	□
18
Published as a conference paper at ICLR 2018
D Proof of Lemma 4.1
We assume the setting described in Lemma 4.1, where we are concerned with the RIP condition of
the matrix A(n) when multiplying vectors x ∈ XT(n), the set of BonC vectors for documents of length
at most T . This matrix can be written as
/ Al	0d×V2	…	0d×Vn ∖
A(n) =	0d×Vι	.	.	.
..	.
.	..	..	0d×Vn
∖0d×Vι	…	0d×Vn-ι	An )
where Ap is the d × Vp matrix whose columns are the DisC embeddings of all p-grams in the
vocabulary (and thus A(1) = A1 = A, the matrix of the original word embeddings). Note that from
(1) any x ∈ XT(n) can be written as x = [x1, . . . , xn], where xp is a T -sparse vector whose entries
correspond to p-grams. Thus we also have A(n)x = [A1x1, . . . , Anxn].
Lemma D.1. If Ap is (2k, ε)-RIP w.p. 1 - γ ∀ p ∈ [n] then A(n) is ∆Xk(n), ε -RIP w.p. at least
1 - nγ.
Proof. By union bound we have that Ap is (2k, ε)-RIP ∀ p ∈ [n] with probability at least 1 - nγ.
Thus by Definition 4.1 we have w.p. 1 - nγ that ∀ x ∈ ∆Xk(n)
nn
kA(n)xk22=XkApxpk22 ≤ X(1 + ε)2kxpk22 = (1 + ε)2kxk22
p=1	p=1
Similarly, kA(n)xk22 ≥ (1 - ε)2 kxk22. From Definition 4.1, taking the square root of both sides of
both inequalities completes the proof.	□
Definition D.1 (Foucart & Rauhut, 2013). Let D be a distribution over a subset S ⊂ Rn. Then
the set Φ = {φ1, . . . , φN} of functions φi : S 7→ R is a bounded orthonormal system (BOS) with
constant B ifwe have ED(φiφj) = 1i=j ∀ i,j and suPs∈s ∣Φi(s)∣ ≤ B ∀ i. Note that by definition
B≥1.
Theorem D.1 (Foucart & RaUhUt, 2013). If d = ΩΩ (B2k log Nj for (ε, Y) ∈ (0,1) and √dA is a
d × N matrix associated with a BOS with constant B then A is (k, ε)-RIP w.p. 1 - γ.
Lemma D.2. If d = ΩΩ 仕 log Vp) and the word embeddings are drawn i.i.d. from Ud{±1∕√d}
then for any p ∈ [n] the matrix Ap ∈ Rd×Vp of DisC embeddings is (T, ε)-RIP w.p. 1 - γ.
Proof. Note that by Theorem D.1 it suffices to show that VdAp is a random sampling matrix
associated with a BOS with constant B = 1. Let D = UV{±1} be the uniform distribution over
V i.i.d. Rademacher random variables indexed by words in the vocabulary. Then by definition the
matrix Ap ∈ Rd×Vp can be constructed by drawing random variables x(1), . . . , x(d) i.i.d. from D
and assigning to the ij th entry of √dAp corresponding to the p-gram g = {gι,...,gp} the value
φj (χ(i)) = Qp=ι xgt), where each function φj : {±1}V → R is uniquely associated to its p-gram.
It remains to be shown that this set of functions is a BOS with constant B = 1.
For any two p-grams g, g0 and their functions φi, φj We have ED(φiφj) = Ex〜D (Qp=I XgtXg0),
which will be 1 iff each word in g ∪ g0 occurs an even number of times in the product and 0 otherwise.
Because all p-grams are uniquely defined under any permutation of its words (i.e. we are in fact using
p-cooccurrences) and we have assumed that no p-gram contains a word more than once, each word
occurs an even number of times in the product iff g = g0 q⇒ i = j. Furthermore We have that
∣Φi(x)∣ ≤ 1 ∀ x ∈ {士1}V ∀ i by construction. Thus according to Definition D.1 the set of functions
{φι,..., φvp } associated to the p-grams in the vocabulary is a BOS with constant B = 1.	□
〜
ProofofLemma 4.1. Since d = Ω
nVn ), Lemma D.2 implies that Ap is (2T, ε)-RIP w.p.
1 - n ∀ P ∈ [n]. Applying Lemma D.1 yields the result.
□
19
Published as a conference paper at ICLR 2018
E Details of the S upporting Hyperplane Property
In Section 5.2, Definition 5.1 we introduced the Supporting Hyperplane Property (SHP), which
by Corollary 5.1 characterizes when BP+ perfectly recovers a nonnegative signal. Together with
Lemmas A.1 and A.2 this fact also shows that SHP is a weaker condition than the well-known
nullspace property (NSP):
Corollary E.1. If a matrix A ∈ Rd×N with columns in general position satisfies NSP for some
S ⊂ [N] then it also satisfies S-SHP.
In this section we give the entire proof of Corollary 5.1 and describe how to verify SHP given a
design matrix and a set of support indices.
E.1 Proof of Corollary 5.1
Recall that it suffices to show equivalence of A being S-SHP with the columns AS forming the
vertices of a k-dimensional face of conv(A), where we can abuse notation to set A ∈ Rd×(N+1),
with the extra column being the origin 0d , so long as we constrain N + 1 6∈ S.
( =⇒ ): The proof of the forward direction appeared in full in the proof sketch (see Section 5.2).
(^= ): A subset F ⊂ Rd is a face of Conv(A) if for some hyperplane H = {v : aτV 一 b = 0} We
have F = conv(A) ∩ H and conv(A)\F ⊆ H- = {v : aTv - b < 0}, where H- is the negative
halfspace of H. Define the simplex ∆m = {λ ∈ [0, 1]m : Pim=1 λi = 1}.
Since A is S-SHP We have a hyperplane H = {v : aTv 一 b = 0} containing the columns AS such
that AS ⊂ H-. Thus aτAi — b = 0 ∀ i ∈ S and ατAi — b < 0 ∀ i ∈ S. We also know that
F = {Pi∈s λiAi : λ ∈ ∆∣s∣} ⊆ H by convexity of H. Since any point y ∈ Conv(A)\F can be
written as y = PiN=+11 λiAi for some λ ∈ ∆N+1 such that ∃ j ∈/ S such that λj 6= 0, we have that
aτy — b = X λi(aτA, — b) + X λj(aτAj — b) = X λj (aτAj — b) < 0
i∈S	j∈S	j∈S
This implies that Conv(A)\F ⊆ H- and F = Conv(A) ∩H, so since the columns ofA are in general
position F is a k-dimensional face of Conv(A) whose vertices are the columns AS.
E.2 Verifying SHP
Recall that a matrix Rd×N satisfies S-SHP for S ⊂ [N] if there is a hyperplane containing the set of
all columns of A indexed by S and the set of all other columns together with the origin are on one
side of it. Due to Corollary 5.1, checking S-SHP allows us to know whether all nonnegative signals
with index support S will be recovered by BP+ without actually running the optimization on any one
of them. The property can be checked by solving a convex problem of the form
min max【ATh + ε, 0)
h∈Rd+1	i
i6∈S
subject to JITh = 0∣S∣
where A =
01d	and ε > 0, p ≥ 1
The constraint enforces the property that the hyperplane contains all support embeddings, while the
optimal objective value is zero iff SHP is satisfied (this follows from the fact that scaling h does
not affect the constraint so if the minimal objective is zero for any single ε > 0 it is zero for all
ε > 0). The problem can be solved via using standard first or second-order equality-constrained
convex optimization algorithms. We set ε = 1 and p = 3 (to get a C2 objective) and adapt the
second-order method from Boyd & Vandenberghe (2004, Chapter 10). Our implementation can be
found at https://github.com/NLPrinceton/sparse_recovery.
20
Published as a conference paper at ICLR 2018
Recovery of SST Documents
Random Common Google	Amazon	English
Vectors Crawl News	Reviews	Wikipedia
Training Corpus
Legend
SN (normalized/not normalized)
GlOVe (normalized/not normalized)
word2vec (normalized/not normalized)
■ Normalized Rademacher Vectors
Spherical Gaussian Vectors
Figure 7: Efficiency of pretrained embeddings as sensing vectors at d = 300 dimensions, measured
via the F1 -score of the original BoW. 200 documents from each dataset were compressed and
recovered in this experiment. For fairness, the number of words V is the same for all embeddings
so all documents are required to be subsets of the vocabulary of all corpora. word2vec embeddings
trained on Google News and GloVe vectors trained on Common Crawl were obtained from public
repositories (Mikolov et al., 2013; Pennington et al., 2014) while Amazon and Wikipedia embeddings
were trained for 100 iterations using a symmetric window of size 10, amin count of 100, for SN/GloVe
a cooccurrence cutoff of 1000, and for word2vec a down-sampling frequency cutoff of 10-5 and a
negative example setting of 3. 300-dimensional normalized random vectors are used as a baseline.
F	Sparse Recovery with Pretrained Embeddings
F.1 Performance Comparison of Alternative Embeddings
We show in Figure 7 that the surprising effectiveness of word embeddings as linear measurement
vectors for BoW signals holds for other embedding objectives and corpora as well. Specifically,
we see that widely used embeddings, when normalized, match the efficiency of random vectors for
retrieving SST BoW and are more efficient when retrieving IMDB BoW. Interestingly, SN vectors are
most efficient and are also the only embeddings for normalizing is not needed for good performance.
F.2 A Model-Based Theoretical Approach
In Section 5.2 we gave some intuition for why pretrained word embeddings are efficient sensing
vectors for natural language BoW by examining a geometric characterization of local equivalence due
to Donoho & Tanner (2005) in light of the usual similarity properties of word embeddings. However,
this analysis does not provide a rigorous theory for our empirical results. In this section we briefly
discuss a model-based justification that may lead to a stronger understanding.
We need a model relating BoW generation to the word embeddings trained over words co-occurring
in the same BoW. As a starting point consider the model of Arora et al. (2016), in which a corpus is
generated by a random walk ct over the surface of a ball in Rd ; at each t a word w is emitted w.p.
P(w∣ct) H exp<ct, Vwi	(18)
Minimizing the SN objective approximately maximizes corpus likelihood under this model.
Thus in an approximate sense a document of length T is generated by setting a context vector c
and emitting T words via (18) with ct = c. This model is a convenient one for analysis due its
simplicity and invariance to word order as well as the fact that the approximate maximum likelihood
document vector is the sum of the embeddings of words in the document. Building upon the intuition
established following Corollary 5.1 one can argue that, if we have the true latent SN vectors, then
embeddings of words in the same document (i.e. emitted by the same context vector) will be close to
each other and thus easy to separate from the embeddings of other words in the vocabulary.
However, we find empirically that not all of the T words closest to the sum of the word embeddings
(i.e. the context vector) are the ones emitted; indeed individual word vectors in a document may
have small, even negative inner product with the context vector and still be recovered via BP. Thus
any further theoretical argument must also be able to handle the recovery of lower probability words
whose vectors are further away from the context vector than those of words that do not appear in the
document. We thus leave to future work the challenge of explaining why embeddings resulting from
this (or another) model provide such efficient sensing matrices for natural language BoW.
21