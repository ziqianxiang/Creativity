Published as a conference paper at ICLR 2018
Loss-aware Weight Quantization of Deep Net-
WORKS
Lu Hou, James T. Kwok
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong
{lhouab, jamesk}@cse.ust.hk
Ab stract
The huge size of deep networks hinders their use in small computing devices. In
this paper, we consider compressing the network by weight quantization. We ex-
tend a recently proposed loss-aware weight binarization scheme to ternarization,
with possibly different scaling parameters for the positive and negative weights,
and m-bit (where m > 2) quantization. Experiments on feedforward and recur-
rent neural networks show that the proposed scheme outperforms state-of-the-art
weight quantization algorithms, and is as accurate (or even more accurate) than
the full-precision network.
1	Introduction
The last decade has witnessed huge success of deep neural networks in various domains. Examples
include computer vision, speech recognition, and natural language processing (LeCun et al., 2015).
However, their huge size often hinders deployment to small computing devices such as cell phones
and the internet of things. Many attempts have been recently made to reduce the model size. One
common approach is to prune a trained dense network (Han et al., 2015; 2016). However, most of
the pruned weights may come from the fully-connected layers where computations are cheap, and
the resultant time reduction is insignificant. Li et al. (2017b) and Molchanov et al. (2017) proposed
to prune filters in the convolutional neural networks based on their magnitudes or significance to the
loss. However, the pruned network has to be retrained, which is again expensive.
Another direction is to use more compact models. GoogleNet (Szegedy et al., 2015) and ResNet (He
et al., 2016) replace the fully-connected layers with simpler global average pooling. However, they
are also deeper. SqueezeNet (Iandola et al., 2016) reduces the model size by replacing most of the
3 × 3 filters with 1 × 1 filters. This is less efficient on smaller networks because the dense 1 × 1
convolutions are costly. MobileNet (Howard et al., 2017) compresses the model using separable
depth-wise convolution. ShuffleNet (Zhang et al., 2017) utilizes pointwise group convolution and
channel shuffle to reduce the computation cost while maintaining accuracy. However, highly opti-
mized group convolution and depth-wise convolution implementations are required. Alternatively,
Novikov et al. (2015) compressed the model by using a compact multilinear format to represent the
dense weight matrix. The CP and Tucker decompositions have also been used on the kernel tensor
in CNNs (Lebedev et al., 2014; Kim et al., 2016). However, they often need expensive fine-tuning.
Another effective approach to compress the network and accelerate training is by quantizing each
full-precision weight to a small number of bits. This can be further divided to two sub-categories,
depending on whether pre-trained models are used (Lin et al., 2016a; Mellempudi et al., 2017) or
the quantized model is trained from scratch (Courbariaux et al., 2015; Li et al., 2017a). Some of
these also directly learn with low-precision weights, but they usually suffer from severe accuracy
deterioration (Li et al., 2017a; Miyashita et al., 2016). By keeping the full-precision weights during
learning, Courbariaux et al. (2015) pioneered the BinaryConnect algorithm, which uses only one bit
for each weight while still achieving state-of-the-art classification results. Rastegari et al. (2016)
further incorporated weight scaling, and obtained better results. Instead of simply finding the clos-
est binary approximation of the full-precision weights, a loss-aware scheme is proposed in (Hou
et al., 2017). Beyond binarization, TernaryConnect (Lin et al., 2016b) quantizes each weight to
1
Published as a conference paper at ICLR 2018
{-1, 0, 1}. Li & Liu (2016) and Zhu et al. (2017) added scaling to the ternarized weights, and
DoReFa-Net (Zhou et al., 2016) further extended quantization to more than three levels. However,
these methods do not consider the effect of quantization on the loss, and rely on heuristics in their
procedures (Zhou et al., 2016; Zhu et al., 2017). Recently, a loss-aware low-bit quantized neural
network is proposed in (Leng et al., 2017). However, it uses full-precision weights in the forward
pass and the extra-gradient method (Vasilyev et al., 2010) for update, both of which are expensive.
In this paper, we propose an efficient and disciplined ternarization scheme for network compression.
Inspired by (Hou et al., 2017), we explicitly consider the effect of ternarization on the loss. This
is formulated as an optimization problem which is then solved efficiently by the proximal Newton
algorithm. When the loss surface’s curvature is ignored, the proposed method reduces to that of (Li
& Liu, 2016), and is also related to the projection step of (Leng et al., 2017). Next, we extend it
to (i) allow the use of different scaling parameters for the positive and negative weights; and (ii)
the use of m bits (where m > 2) for weight quantization. Experiments on both feedforward and
recurrent neural networks show that the proposed quantization scheme outperforms state-of-the-art
algorithms.
Notations: For a vector x, √x denotes the element-wise square root (i.e., [√x]i = √χi), |x| is
the element-wise absolute value, kxkp = (Pi ∣Xi∣p)P is its p-norm, and Diag(x) returns a diagonal
matrix with x on the diagonal. For two vectors x and y, x y denotes the element-wise multi-
plication and x y the element-wise division. Given a threshold ∆, I∆ (x) returns a vector such
that [I∆(x)]i = 1 ifxi > ∆, -1 ifxi < -∆, and 0 otherwise. I+∆(x) considers only the positive
threshold, i.e., [I+∆(x)]i = 1 ifxi > ∆, and 0 otherwise. Similarly, [I-∆(x)]i = -1 ifxi < -∆, and
0 otherwise. For a matrix X, vec(X) returns a vector by stacking all the columns ofX, and diag(X)
returns a vector whose entries are from the diagonal of X.
2	Related Work
Let the full-precision weights from all L layers be w = [w1> , w2> , . . . , wL>]>, where wl =
vec(Wl), and Wl is the weight matrix at layer l. The corresponding quantized weights will be
denoted w^ = [W^>, w^ >,..., w^ >]>.
2.1	Weight B inarized Networks
In BinaryConnect (Courbariaux et al., 2015), each element of wl is binarized to -1 or +1 by using
the sign function: Binarize(wl) = sign(wl). In the Binary-Weight-Network (BWN) (Rastegari
et al., 2016), a scaling parameter is also included, i.e., Binarize(wl) = αlbl, where αl > 0, bl ∈
{-1, +1}nl and nl is the number of weights in wl. By minimizing the difference between wl and
αlbl, the optimal αl, bl have the simple form: αl = kwlk1/nl, and bl = sign(wl).
Instead of simply finding the best binary approximation for the full-precision weight wlt at iteration
t, the loss-aware binarized network (LAB) directly minimizes the loss w.r.t. the binarized weight
αltblt (Hou et al., 2017). Let dlt-1 be a vector containing the diagonal of an approximate Hessian of
the loss. It can be shown that αlt = kdlt-1 wltk1/kdlt-1 k1 and blt = sign(wlt).
2.2	Weight Ternarized Networks
In a weight ternarized network, zero is used as an additional quantized value. In TernaryConnect (Lin
et al., 2016b), each weight value is clipped to [-1, 1] before quantization, and then a non-negative
weight [wlt]i is stochastically quantized to 1 with probability [wlt]i (and 0 otherwise). When [wlt]i
is negative, it is quantized to -1 with probability -[wlt]i, and 0 otherwise.
In the ternary weight network (TWN) (Li & Liu, 2016), Wt is quantized to w^t = atI∆t(wt),
where ∆t is a threshold (i.e., [Wt]i = Ot if [wf]i > ∆lt, -αlt if [Wlt]i < -∆lt and 0 otherwise).
To obtain △，and Ot TWN minimizes the '2-distance between the full-precision and ternarized
2
Published as a conference paper at ICLR 2018
weights, leading to
2
∆lt
arg max 万「~产丁
∆>0 kI∆(wlt)k1
|[wlt]i|
∕i"[wt]i∣>∆t
αlt
1
必(wt)kι
Σ
id[wt]i∣>∆t
|[wlt]i|.
(1)
1
However, N in (1) is difficult to solve. Instead, TWN simply sets ∆t = 0.7 ∙ E(∣wf∣) in practice.
In TWN, one scaling parameter (αlt) is used for both the positive and negative weights at layer l. In
the trained ternary quantization (TTQ) network (Zhu et al., 2017), different scaling parameters (αlt
and βt) are used. The weight Wt is thus quantized to Wtt = ɑ：玄? (Wt) + /江公? (wf)∙ The ScaIing
parameters are learned by gradient descent. As for ∆lt, two heuristics are used. The first sets ∆lt to
a constant fraction ofmax(|Wlt|), while the second sets ∆lt such that at all layers are equally sparse.
2.3	Weight Quantized Networks
In a weight quantized network, m bits (where m ≥ 2) are used to represent each
weight. Let Q be a set of (2k + 1) quantized values, where k = 2m-1 - 1. The
two popular choices of Q are {-1, - k-1,..., - k, 0, k,..., k-1, 1} (linear quantization), and
{-1, -1,..., - 21-1, 0, 2k1-ι,..., 2, 1} (logarithmic quantization). By limiting the quantized val-
ues to powers of two, logarithmic quantization is advantageous in that expensive floating-point op-
erations can be replaced by cheaper bit-shift operations. When m = 2, both schemes reduce to
Q={-1,0,1}.
In the DoReFa-Net (Zhou et al., 2016), weight Wlt is heuristically quantized to m-bit, with:1
[Wt]i = 2 ∙ quantizem (-~tanh"：；[i)t])口 + 1) - 1
m 2 max(| tanh([Wlt]i)|)	2
in {-1, - Imm-I,..., - 2m-1, 2m-1,..., Im-1, 1}, where quantizem (x) = 2m1-1 round((2m -
1)x). Similar to loss-aware binarization (Hou et al., 2017), Leng et al. (2017) proposed a loss-aware
quantized network called low-bit neural network (LBNN). The alternating direction method of mul-
tipliers (ADMM) (Boyd et al., 2011) is used for optimization. At the tth iteration, the full-precision
weight Wlt is first updated by the method of extra-gradient (Vasilyev et al., 2010):
Wt = WtT- ηtVιL(WtT), Wtl = WtT-ηtVιL(Wt),	(2)
where L is the augmented Lagrangian in the ADMM formulation, and ηt is the stepsize. Next, Wlt
is projected to the space of m-bit quantized weights so that Wt is of the form αιbι, where aι > 0,
and bl ∈ {-1, - 1 ,.∙∙, - Ik-1,0, Ik-I ,∙∙∙, 1, 1}∙
3	Loss-Aware Quantization
3.1	Ternarization using Proximal Newton Algorithm
In weight ternarization, TWN simply finds the closest ternary approximation of the full precision
weight at each iteration, while TTQ sets the ternarization threshold heuristically. Inspired by LAB
(for binarization), we consider the loss explicitly during quantization and obtain the quantization
thresholds and scaling parameter by solving an optimization problem.
As in TWN, the weight WI is ternarized as Wι = αιbι, where aι > 0 and bι ∈ {-1,0,1}nl. Given
a loss function `, we formulate weight ternarization as the following optimization problem:
min '(W) : WI = αιbι, αι > 0, bι ∈Qnl, l =1,...,L,	(3)
W
where Q is the set of desired quantized values. As in LAB, we will solve this using the proximal
Newton method (Lee et al., 2014; Rakotomamonjy et al., 2016). At iteration t, the objective is
replaced by the second-order expansion
'(Wt-1) + V'(Wt-1)>(W - Wt-1) + ；(W - Wt-1)>HtT(W - Wt-1),	(4)
1 Note that the quantized value of 0 is not used in DoReFa-Net.
3
Published as a conference paper at ICLR 2018
where Ht-1 is an estimate of the Hessian of ' at w^t-1. We use the diagonal equilibration Pre-
conditioner (Dauphin et al., 2015), which is robust in the presence of saddle points and also readily
available in PoPular stochastic deeP network oPtimizers such as Adam (Kingma & Ba, 2015). Let Dl
be the aPProximate diagonal Hessian at layer l. We use D = Diag([diag(D1)>, . . . , diag(DL)>]>)
as an estimate ofH. Substituting (4) into (3), we solve the following subProblem at the tth iteration:
minwt V'(w^t-1)>(w^t - WtT) + g(Wt - WtT)>Dt-1(W^t - WtT)	(5)
s.t.	Wt	=	αtbt,	at	>	0,	bt	∈	Qnl,	l = 1,...,L.
Proposition 3.1 The objective in (5) can be rewritten as
L	>	2
min χ X (VZd-T (Wt- Wt)),	⑹
W t 2	)
where dlt-1 ≡ diag(Dlt-1), and
Wt ≡ Wt-1 - V1'(Wt-1) 0 dt-1.	(7)
Obviously, this objective can be minimized layer by layer. Each Proximal Newton iteration thus
consists of two steps: (i) Obtain Wt in (7) by gradient descent along Vι'(Wt-1), which is precondi-
tioned by the adaPtive learning rate 1 0 dlt-1 so that the rescaled dimensions have similar curvatures;
(ii) Quantize Wt to Wt by minimizing the scaled difference between W∖ and Wt in (6). Intuitively,
when the curvature is low ([dlt-1]i is small), the loss is not sensitive to the weight and ternarization
error can be less penalized. When the loss surface is steep, ternarization has to be more accurate.
Though the constraint in (6) is more complicated than that in LAB, interestingly the following simple
relationship can still be obtained for weight ternarization.
Proposition 3.2 With Q = {-1,0,1} ,and the optimal W t in (6) of the form ab. For a fixed b,
α =厂晨1 dt?Wl k1; whereas when a is fixed, b = ∖ɑ∕2Wt).
Equivalently, b can be written as ∏Q(Wt∕a), where ∏q(∙) projects each entry of the input argu-
ment to the nearest element in Q. Further discussions on how to solve for alt will be presented in
Sections 3.1.1 and 3.1.2. When the curvature is the same for all dimensions at layer l, the following
Corollary shows that the solution above reduces that of TWN.
Corollary 3.1 When Dlt-1 = λI, alt reduces to the TWN solution in (1) with ∆lt = alt/2.
In other words, TWN corresponds to using the proximal gradient algorithm, while the proposed
method corresponds to using the proximal Newton algorithm with diagonal Hessian. In composite
optimization, it is known that the proximal Newton algorithm is more efficient than the proximal gra-
dient algorithm (Lee et al., 2014; Rakotomamonjy et al., 2016). Moreover, note that the interesting
relationship ∆lt = alt/2 is not observed in TWN, while TTQ completely neglects this relationship.
In LBNN (Leng et al., 2017), its projection step uses an objective which is similar to (6), but without
using the curvature information. Besides, their Wlt is updated with the extra-gradient in (2), which
doubles the number of forward, backward and update steps, and can be costly. Moreover, LBNN
uses full-precision weights in the forward pass, while all other quantization methods including ours
use quantized weights (which eliminates most of the multiplications and thus faster training).
When (i) ` is continuously differentiable with Lipschitz-continuous gradient (i.e., there exists β > 0
such that kV'(u) 一 V'(v)∣b ≤ β ∣∣u 一 v∣b for any u, v); (ii) ' is bounded from below; and (iii)
[dlt]k > β ∀l, k, t, it can be shown that the objective of (3) produced by the proximal Newton algo-
rithm (with solution in Proposition 3.2) converges (Hou et al., 2017). In practice, it is important to
keep the full-precision weights during update (Courbariaux et al., 2015). Hence, we replace (7) by
Wtl J Wt-1 - Vι'(Wt-1) 0 dt-1. The whole procedure, which is called Loss-Aware Ternarization
(LAT), is shown in Algorithm 3 of Appendix B. It is similar to Algorithm 1 of LAB (Hou et al.,
2017), except that alt and blt are computed differently. In step 4, following (Li & Liu, 2016), we first
rescale input xlt-1 with al, so that multiplications in dot products and convolutions become addi-
tions. Algorithm 3 can also be easily extended to ternarize weights in recurrent networks. Interested
readers are referred to (Hou et al., 2017) for details.
4
Published as a conference paper at ICLR 2018
3.1.1	EXACT SOLUTION OF αlt
To simplify notations, we drop the superscripts and subscripts. From Proposition 3.2,
∣∣b Θ d Θ w∣∣ι
kb θ dk1
b = Iα∕2(w).
(8)
We now consider how to solve for α. First, we introduce some notations. Given a vector
x = [x1, x2, . . . , xn], and an indexing vector s ∈ Rn whose entries are a permutation of {1, . . . , n},
perms(x) returns the vector [xs1 , xs2, . . . xsn], and cum(x) = [x1, Pi1 2 3 4 * 6 7 8=1 xi, . . . , Pin=1 xi] returns
partial sums for elements in x. For example, let a = [1, -1, -2], and b = [3, 1, 2]. Then,
permb(a) = [-2, 1, -1] and cum(a) = [1, 0, -2].
We sort elements of |w| in descending order, and let the vector containing the sorted indices be s.
For example, if w = [1, 0, -2], then s = [3, 1, 2]. From (8),
=∣Iα∕2(W) θ d θ wkι = [cum(perms(∣d Θ w∣))j = 2
α = kIɑ∕2(w) Θ d∣ι =	[cum(perms(∣d∣))j = Cj,	( )
where c = cum(perms(|d Θ w|)) cum(perms(d))	2, and j is the index such that
[perms(|w|)]j > cj > [perms(|w|)]j+1.	(10)
For simplicity of notations, let the dimensionality of w (and thus also of c) be n, and the operation
find(condition(x)) returns all indices in x that satisfies the condition. It is easy to see that any j
satisfying (10) is in S ≡ find([perms(|w|)][1:(n-1)] - c[1:(n-1)]) Θ ([perms(|w|)][2:n] -c[1:n-1]) <
0), where c[1:(n-1)] is the subvector of c with elements in the index range 1 to n - 1. The optimal
α (= 2cj ) is then the one which yields the smallest objective in (6), which can be simplified by
Proposition 3.3 below. The procedure is shown in Algorithm 1.
Proposition 3.3 The optimal Ot of (6) equals 2 argmax%. j∈s cj ∙ [ cum (perm s(dt-1))j∙
Algorithm 1 Exact solver of (6)
1:
2:
3:
4:
5:
6:
7:
Input: full-precision weight wlt, diagonal entries of the approximate Hessian dlt-1.
s = arg sort(|wlt|);
c = cum(perms(|dlt-1 Θ wlt|))	cum(perms(dlt-1))	2;
S = find(([perms(|wlt|)][1:(n-1)] - c[1:(n-1)]) Θ ([perms(|wlt|)][2:n] - c[1:n-1]) < 0);
αt = 2argmaxcjj∈s cj ∙ [cum(perms(dt-1))j;
blt = Iαlt∕2 (wlt);
Output: Wl = αtbt.
3.1.2	APPROXIMATE SOLUTION OF αlt
In case the sorting operation in step 2 is expensive, αlt and blt can be obtained by alternating the
iteration in Proposition 3.2 (Algorithm 2). Empirically, it converges very fast, usually in 5 iterations.
Algorithm 2 Approximate solver for (6).
1: Input: blt-1, full-precision weight wlt, diagonal entries of the approximate Hessian dlt-1.
2: Initialize: α = 1.0, αold = 0.0, b = blt-1, = 10-6;
3: while ∣α 一 00id∣ > e do
4:	αold = α;
kbΘdt-1Θwtkι.
=kbΘdt-1kι ;
6:	b = Iα∕j (wlt);
7: end while
8: Output: Wt = αb.
5
Published as a conference paper at ICLR 2018
3.2	Extension to Ternarization with Two Scaling Parameters
As in TTQ (Zhu et al., 2017), we can use different scaling parameters for the positive and negative
weights in each layer. The optimization subproblem at the tth iteration then becomes:
minw t
s.t.
v`(wt-1)>(wt - Wt-1) + 1(wt - Wt-1)>DtT(Wt - Wt-1
Wt ∈ {-βt, 0,αt}nl, at > 0, βt > 0, l = 1,...,L.
(11)
Proposition 3.4 The optimal	Wt	in (5) is of the form	Wt	=	afpf	+	β∕qf,	where	Ot
kptΘdt-1Θwtkι t
kptΘdt-1kι ,pl
I+t/2 (Wt), β = k%：3片：k1 ,and q = I-t/2 (Wt).
The exact and approximate solutions for αlt and βlt can be obtained in a similar way as in Sec-
tions 3.1.1 and 3.1.2. Details are in Appendix C.
3.3	Extension to Low-Bit Quantization
For m-bit quantization, we simply change the set Q of desired quantized values in (3) to one with
k = 2m-1 - 1 quantized values. The optimization still contains a gradient descent step with adap-
tive learning rates like LAT, and a quantization step which can be solved efficiently by alternating
minimization of (α, b) (similar to the procedure in Algorithm 2) using the following Proposition.
Proposition 3.5 Let the optimal W t in (6) be of the form ab. For a fixed b, a = kbΘbΘ df； l k1 ；
whereas when a is fixed, b = Πq( WaL), where Q = { -1, 一 k-1,..., 一 1,0, 1,..., k-1, 1} for
linear quantization and Q = { -1, — 1,..., — 2k-τ, 0, 2k-τ,..., 2, 1} for logarithmic quantization.
4 Experiments
In this section, we perform experiments on both feedforward and recurrent neural networks. The
following methods are compared: (i) the original full-precision network; (ii) weight-binarized net-
works, including BinaryConnect (Courbariaux et al., 2015), Binary-Weight-Network (BWN) (Raste-
gari et al., 2016), and Loss-Aware Binarized network (LAB) (Hou et al., 2017); (iii) weight-
ternarized networks, including Ternary Weight Networks (TWN) (Li & Liu, 2016), Trained Ternary
Quantization (TTQ)2 (Zhu et al., 2017), the proposed Loss-Aware Ternarized network with exact so-
lution (LATe), approximate solution (LATa), and with two scaling parameters (LAT2e and LAT2a);
(iv) m-bit-quantized networks (where m > 2), including DoReFa-Netm (Zhou et al., 2016), the
proposed loss-aware quantized network with linear quantization (LAQm(linear)), and logarithmic
quantization (LAQm(log)). Since weight quantization can be viewed as a form of regularization
(Courbariaux et al., 2015), we do not use other regularizers such as dropout and weight decay.
4.1	Feedforward Networks
In this section, we perform experiments with the multilayer perceptron (on the MNIST data set)
and convolutional neural networks (on CIFAR-10, CIFAR-100 and SVHN). For MNIST, CIFAR-10,
and SVHN, the setup is similar to that in (Courbariaux et al., 2015; Hou et al., 2017). Details can
be found in Appendix D. For CIFAR-100, we use 45, 000 images for training, another 5, 000 for
validation, and the remaining 10, 000 for testing. The testing errors are shown in Table 1.
Ternarization: On MNIST, CIFAR100 and SVHN, the weight-ternarized networks perform better
than weight-binarized networks, and are comparable to the full-precision networks. Among the
weight-ternarized networks, the proposed LAT and its variants have the lowest errors. On CIFAR-10,
LATa has similar performance as the full-precision network, but is outperformed by BinaryConnect.
Figure 1(a) shows convergence of the training loss for LATa on CIFAR-10, and Figure 1(b) shows
the scaling parameter obtained at each CNN layer. As can be seen, the scaling parameters for the
2For TTQ, We follow the CIFAR-10 setting in (Zhu etal., 2017), and set ∆t = 0.005max(∣wt∣).
6
Published as a conference paper at ICLR 2018
Table 1: Testing errors (%) on the feedforward networks. Algorithm with the lowest error in each
group is highlighted.
		MNIST	CIFAR-10	CIFAR-100	SVHN
no binarization	full-precision	1.11	-10.38-	39.06	2.28
	BinaryConnect	1.28	9.86	46.42	2.45
binarization	BWN	-1.31-	-10.51-	-43.62-	2.54
	LAB	1.18	10.50	43.06	2.35
	TWN	1.23	10.64	43.49	2.37
1 scaling	LATe	-∏3-	-1047-	-39∏0-	2.30
ternarization	LATa	-T:14-	-10.38-	-39∏9-	2.30
	TTQ		-1059-	-4209-	2.38
2 scaling	LAT2e	-ra-	-1045-	-3901	2.34
	LAT2a	1.19	10.48	38.84	2.35
	DoReFa-Net3	1.31	10.54	45.05	2.39
3-bit quantization	LAQ3(linear)	-ra-	-1067-	-3870-	2.34
	LAQ3(log)	1.16	10.52	38.50	2.29
first and last layers (conv1 and linear3, respectively) are larger than the others. This agrees with the
finding that, to maintain the activation variance and back-propagated gradients variance during the
forward and backward propagations, the variance of the weights between the lth and (l + 1)th layers
should roughly follow 2/(nl +nl+1) (Glorot & Bengio, 2010). Hence, as the input and output layers
are small, larger scaling parameters are needed for their high-variance weights.
Figure 1:	Convergence of the training loss and scaling parameter by LATa on CIFAR-10.
Using Two Scaling Parameters: Compared to TTQ, the proposed LAT2 always has better per-
formance. However, the extra flexibility of using two scaling parameters does not always translate
to lower testing error. As can be seen, it outperforms algorithms with one scaling parameter only
on CIFAR-100. We speculate this is because the capacities of deep networks are often larger than
needed, and so the limited expressiveness of quantized weights may not significantly deteriorate
performance. Indeed, as pointed out in (Courbariaux et al., 2015), weight quantization is a form of
regularization, and can contribute positively to the performance.
Using More Bits: Among the 3-bit quantization algorithms, the proposed scheme with logarithmic
quantization has the best performance. It also outperforms the other quantization algorithms on
CIFAR-100 and SVHN. However, as discussed above, more quantization flexibility is useful only
when the weight-quantized network does not have enough capacity.
4.2 Recurrent Networks
In this section, we follow (Hou et al., 2017) and perform character-level language modeling exper-
iments on the long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997). The training
objective is the cross-entropy loss over all target sequences. Experiments are performed on three
7
Published as a conference paper at ICLR 2018
data sets: (i) Leo Tolstoy’s War and Peace; (ii) source code of the Linux Kernel; and (iii) Penn Tree-
bank Corpus (Taylor et al., 2003). For the first two, we follow the setting in (Karpathy et al., 2016;
Hou et al., 2017). For Penn Treebank, we follow the setting in (Mikolov & Zweig, 2012). In the
experiment, we tried different initializations for TTQ and then report the best. Cross-entropy values
on the test set are shown in Table 2.
Table 2: Testing cross-entropy values on the LSTM. Algorithm with the lowest cross-entropy value
in each group is highlighted.
		War and Peace	Linux Kernel	Penn Treebank
no binarization	full-precision	1.268	1.326	1.083
	BinaryConnect	2.942 =	3.532	=	1.737 =
binarization	BWN	T3T3	1.307	1.078
	LAB	1291	1.305	1.081
	TWN	1.290 =	1.280	=	1.045 =
1 scaling	LATe	1.248	1.256	1.022
ternarization	LATa	1.253	1.264	1.024
	TTQ	1.272	1.302	1.031
2 scaling	LAT2e	1.239	1.258	1.018
	LAT2a	1245	1.258	1.015
	DoReFa-Net3	1.349 =	1.276	=	1.017 =
3-bit quantization	LAQ3(linear)	1.282	1.327	1.017
	LAQ3(log)	1268	1.273	1.009
	DoReFa-Net4	1.328 =	1.320	=	1.019 =
4-bit quantization	LAQ4 (linear)	1.294	1.337	1.046
	LAQ4 (log)	1.272 —	1.319 一	1.016 —
Ternarization: As in Section 4.1, the proposed LATe and LATa outperform the other weight ternar-
ization schemes, and are even better than the full-precision network on all three data sets. Figure 2
shows convergence of the training and validation losses on War and Peace. Among the ternarization
methods, LAT and its variants converge faster than both TWN and TTQ.
full-precision
TWN
TTQ
LATe
LATa
LAT2e
LAT2a
DoReFa- Net3
LAQ 3(Iinear)
LAQ3(log)
epochs	epochs
(a) Training loss.	(b) Validation loss.
Figure 2:	Convergence of the training and validation losses on War and Peace.
Using Two Scaling Parameters: LAT2e and LAT2a outperform TTQ on all three data sets. They
also perform better than using one scaling parameter on War and Peace and Penn Treebank.
Using More Bits: The proposed LAQ always outperforms DoReFa-Net when 3 or 4 bits are used.
As noted in Section 4.1, using more bits does not necessarily yield better generalization perfor-
mance, and ternarization (using 2 bits) yields the lowest validation loss on War and Peace and Linux
Kernel. Moreover, logarithmic quantization is better than linear quantization. Figure 3 shows distri-
butions of the input-to-hidden (full-precision and quantized) weights of the input gate trained after
20 epochs using LAQ3(linear) and LAQ3(log) (results on the other weights are similar). As can
be seen, distributions of the full-precision weights are bell-shaped. Hence, logarithmic quantization
can give finer resolutions to many of the weights which have small magnitudes.
8
Published as a conference paper at ICLR 2018
s-uəeə-əjo Φ6E⊂ΦO-Iφd
SlUSlU8QJ0 86ElU80」8d
(a) Full-precision weights. (b) Quantized weights. (c) Full-precision weights. (d) Quantized weights.
Figure 3:	Distributions of the full-precision and LAQ3-quantized weights on War and Peace. Left
((a) and (b)): Linear quantization; Right ((c) and (d)): Logarithmic quantization.
Quantized vs Full-precision Networks: The quantized networks often perform better than the full-
precision networks. We speculate that this is because deep networks often have larger-than-needed
capacities, and so are less affected by the limited expressiveness of quantized weights. Moreover,
low-bit quantization acts as regularization, and so contributes positively to the performance.
5 Conclusion
In this paper, we proposed a loss-aware weight quantization algorithm that directly considers the ef-
fect of quantization on the loss. The problem is solved using the proximal Newton algorithm. Each
iteration consists of a preconditioned gradient descent step and a quantization step that projects full-
precision weights onto a set of quantized values. For ternarization, an exact solution and an efficient
approximate solution are provided. The procedure is also extended to the use of different scaling
parameters for the positive and negative weights, and to m-bit (where m > 2) quantization. Ex-
periments on both feedforward and recurrent networks show that the proposed quantization scheme
outperforms the current state-of-the-art.
Acknowledgments
This research was supported in part by the Research Grants Council of the Hong Kong Special
Administrative Region (Grant 614513). We thank the developers of Theano (Theano Development
Team, 2016), Pylearn2 (Goodfellow et al., 2013) and Lasagne. We also thank NVIDIA for the gift
of GPU card.
References
S.	Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning, 3(1):1-122, 2011.
M. Courbariaux, Y. Bengio, and J. P. David. BinaryConnect: Training deep neural networks with
binary weights during propagations. In Advances in Neural Information Processing Systems, pp.
3105-3113, 2015.
Y. Dauphin, H. de Vries, and Y. Bengio. Equilibrated adaptive learning rates for non-convex opti-
mization. In Advances in Neural Information Processing Systems, pp. 1504-1512, 2015.
X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In International Conference on Artificial Intelligence and Statistics, pp. 249-256, 2010.
I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra,
F. Bastien, and Y. Bengio. Pylearn2: a machine learning research library. Preprint, 2013.
S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both weights and connections for efficient neural
network. In Advances in Neural Information Processing Systems, pp. 1135-1143, 2015.
9
Published as a conference paper at ICLR 2018
S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural network with
pruning, trained quantization and Huffman coding. In International Conference on Learning
Representations, 2016.
K.	He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In International
Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, pp. 1735-1780,
1997.
L.	Hou, Q. Yao, and J. T. Kwok. Loss-aware binarization of deep networks. In International
Conference on Learning Representations, 2017.
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and
H. Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications.
Preprint arXiv:1704.04861, 2017.
F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer. Squeezenet:
Alexnet-level accuracy with 50x fewer parameters and <0.5MB model size. Preprint
arXiv:1602.07360, 2016.
A. Karpathy, J. Johnson, and F. F. Li. Visualizing and understanding recurrent networks. In Inter-
national Conference on Learning Representations, 2016.
Y. D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural
networks for fast and low power mobile applications. In International Conference on Learning
Representations, 2016.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on
Learning Representations, 2015.
V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky. Speeding-up convolutional
neural networks using fine-tuned cp-decomposition. Preprint arXiv:1412.6553, 2014.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436-444, 2015.
J. D. Lee, Y. Sun, and M. A. Saunders. Proximal Newton-type methods for minimizing composite
functions. SIAM Journal on Optimization, 24(3):1420-1443, 2014.
C.	Leng, H. Li, S. Zhu, and R. Jin. Extremely low bit neural network: Squeeze the last bit out with
admm. Preprint arXiv:1707.09870, 2017.
F. Li and B. Liu. Ternary weight networks. Preprint arXiv:1605.04711, 2016.
H. Li, S. De, Z. Xu, C. Studer, H. Samet, and Goldstein T. Training quantized nets: A deeper
understanding. In Advances in Neural Information Processing Systems, 2017a.
H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning filters for efficient convnets. In
International Conference on Learning Representations, 2017b.
D.	Lin, S. Talathi, and S. Annapureddy. Fixed point quantization of deep convolutional networks.
In International Conference on Machine Learning, pp. 2849-2858, 2016a.
Z. Lin, M. Courbariaux, R. Memisevic, and Y. Bengio. Neural networks with few multiplications.
In International Conference on Learning Representations, 2016b.
N. Mellempudi, A. Kundu, D. Mudigere, D. Das, B. Kaul, and P. Dubey. Ternary neural networks
with fine-grained quantization. Preprint arXiv:1705.01462, 2017.
T. Mikolov and G. Zweig. Context dependent recurrent neural network language model. IEEE
Spoken Language Technology Workshop, 12:234-239, 2012.
D. Miyashita, E. H. Lee, and B. Murmann. Convolutional neural networks using logarithmic data
representation. Preprint arXiv:1603.01025, 2016.
10
Published as a conference paper at ICLR 2018
P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks
for resource efficient transfer learning. In International Conference on Learning Representations,
2017.
A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov. Tensorizing neural networks. In Advances
in Neural Information Processing Systems,pp. 442-450, 2015.
A. Rakotomamonjy, R. Flamary, and G. Gasso. DC proximal Newton for nonconvex optimization
problems. IEEE Transactions on Neural Networks and Learning Systems, 27(3):636-647, 2016.
M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. XNOR-Net: ImageNet classification using
binary convolutional neural networks. In European Conference on Computer Vision, 2016.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich. Going deeper with convolutions. In International Conference on Computer Vision and
Pattern Recognition, pp. 1-9, 2015.
A. Taylor, M. Marcus, and B. Santorini. The Penn treebank: An overview. In Treebanks, pp. 5-22.
Springer, 2003.
Theano Development Team. Theano: A Python framework for fast computation of mathematical
expressions. Preprint arXiv:1605.02688, 2016.
F. P. Vasilyev, E. V. Khoroshilova, and A. S. Antipin. An extragradient method for finding the
saddle point in an optimal control problem. Moscow University Computational Mathematics and
Cybernetics, 34(3):113-118, 2010.
X. Zhang, X. Zhou, M. Lin, and J. Sun. ShuffleNet: An extremely efficient convolutional neural
network for mobile devices. Preprint arXiv:1707.01083, 2017.
S. Zhou, Z. Ni, X. Zhou, H. Wen, Y. Wu, and Y. Zou. DoReFa-Net: Training low bitwidth convolu-
tional neural networks with low bitwidth gradients. Preprint arXiv:1606.06160, 2016.
C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary quantization. In International Conference
on Learning Representations, 2017.
11
Published as a conference paper at ICLR 2018
A Proofs
A.1 Proof of Proposition 3.1
With wlt in (7), the objective in (5) can be rewritten as
v`(wt-1)>(wt - Wt-1) + 1(wt - Wt-1)>DtT(Wt - Wt-1)
ι Ll / t-->	、2
=2 X √dt-ɪ (Wt-(WtT-Vι'(Wt-1) 0 dt-1))	+ ci
2 l=1
>	2
=2 X (Pdl-I (WtI- Wl)) + ci
l=1
L	>	2
=2 X (VZd-T (αtbt - Wt)) + ci
2 l=i
1 L nl
=?Σ∑[dt-1]i(αt[bt]i - [Wt]i)2 + ci,
l=i i=i
where ci = — 2(√dt-i>(Vι'(Wt-i) 0 dt-i))2 is independent of at and bt.
A.2 Proof of Proposition 3.2
To simplify notations, we drop the subscript and superscript. Considering one particular layer,
problem (6) is of the form:
1n
mina,b	2): di(abi - Wi)
i=i
s.t. a > 0,bi ∈ {-1, 0, 1}.
When α is fixed,
bi =argmin1 di(αbi - Wi)2 = 1 dia2(bi - Wi/a)2 = I«/2(Wi).
When b is fixed,
α
1n	2
arg min - ∖∕d (ab - Wi)2
α2
i=i
arg min ɪkb Θ b Θ d∣iα2 - ∣∣b Θ d Θ W∣ia + c2,
α2
1	∣bΘdΘW∣i 2
argmin2 kb θ b θ dki (° - kb Θ b Θ dki )
∣∣b Θ d Θ W∣i
IIb Θ b Θ d∣∣i
∣∣b Θ d Θ W∣i
kb Θ dki .
1 ∣∣b Θ d Θ W∣∣i
2 ∣b Θ b Θ d∣i + c2
—
A.3 Proof of Corollary 3.1
When Dlt-i = λI, i.e., the curvature is the same for all dimensions in the lth layer, From Proposi-
tion 3.2,
alt
kb Θ dt-i Θ Wtki _ klat∕2(wt) θ WtkI
1
kb Θ dt-iki
k%∕2(Wt)ki - ILt(Wt)IIi
|[Wlt]i|,
i:[wlt]i >∆lt
12
Published as a conference paper at ICLR 2018
1 kIat∕2 O wtk1
2	kIɑt∕2k1
1
arg max 万「~产丁
∆>0 kI∆(wlt)k1
X |[wt]i|)
i:[wlt]i >∆
This is the same as the TWN solution in (1).
A.4 Proof of Proposition 3.3
For simplicity of notations, we drop the subscript and superscript. For each layer, we have an
optimization problem of the form
argmin(√d (αb - w))1 2
α
arg min kb O b O dk1 α -
α
∣∣b Θ d Θ w∣∣ι ∖2	∣∣b Θ d Θ w∣∣1
IIb Θ b Θ d∣∣ι ) - Ilb Θ b Θ d∣∣ι
arg min ∣∣Iα∕2(w) Θ Iα∕2(w) Θ d∣ι (α —
∣Iɑ∕2(w) Θ d Θ w∣1 Y - ∣Iα∕2(w) Θ Ia/2(w) Θ w∣2
l∣Iα∕2(w) Θ Iα∕2(w) Θ d∣ιj	∣∣Iα∕2(w) Θ Iα∕2(w) Θ d∣ι
arg min -
α
l∣Iα∕2(w) Θ d Θ w∣2
kIα∕2(w) Θ d∣ι
where the second equality holds as b = Iα∕2(w). From (9), we have
∣∣Iα∕2(w) Θ d Θ w∣1
kIα∕2(w) Θ d∣ι
∣∣Iα∕2(w) Θ d Θ w∣ι
∣Iα∕2(w) Θ d∣ι
∣∣Iα∕2(w) Θ d Θ w∣ι
∣Iα∕2(w) Θ d∣ι
∣Iα∕2(w) Θ d∣1
= -2cj ∙ 2cj ∙ [cum(perms(d))]j
= —2c2 ∙ [cum(perms(d))]j.
Thus, the a that minimizes (√d>(αb — w))2 is a = 2 arg maxcj ,j∈s cj ∙ [cum(perms(d))j.
A.5 Proof for Proposition 3.4
For simplicity of notations, we drop the subscript and superscript, and consider the optimization
problem:
1n
minα,b 2 ɪ^di(wi - Wi)
i=1
s.t.	Wi ∈ {—β, 0, 十α}.
Let f (Wi) = (Wi — Wi)2. Then, f(α) = (α — Wi)2,f (0) = wj, and f (—β) = (β + Wi)2. Itis easy
to see that (i) if Wi > α∕2, f (α) is the smallest; (ii) if Wi < —β∕2, f ( —1) is the smallest; (iii) if
—β∕2 ≤ Wi ≤ α∕2, f (0) is the smallest. In other words, the optimal Wi satisfies
Wi = αI+∕j(Wi) + βI-∕2(Wi),
or equivalently, W = αp + βq, where P = l+∕2(w), and q = I-(w).
Define w+ and w- such that [w+]i
Wi
0
Wi > 0
otherwise,
and [w-]i
Wi
0
Wi < 0
otherwise.
. Then,
1n	1n	1n
2 X di(Wi- Wi)2 = 2 X di(αPi - w+ )2 + 2 X di(βqi — W-)2.	(12)
i=1	i=1	i=1
The objective in (12) has two parts, and each part can be viewed as a special case of the ternarization
step in Proposition 3.1 (considering only with positive or negative weights). Similar to the proof for
Proposition 3.2, We can obtain that the optimal W = αp + βq satisfies
—kpQd0wkι	n — l+ (W)
α =	∣∣pΘdkι ， P = Iα∕2 (W)，
kq0d0w∣∣ι
kqodki
q = Iβ-∕2(w).
13
Published as a conference paper at ICLR 2018
A.6 Proof of Proposition 3.5
For simplicity of notations, We drop the subscript and superscript. Since 2(√d>(αb 一 w))2
1 Pn= ι di(αbi 一 Wi)2 for each layer, we simply consider the optimization problem:
1n
minα,b	2): di (abi - Wi)
s.t. α > 0, bi ∈ Q.
When α is fixed,
bi
When b is fixed,
α=
arg min 1 di(abi — Wi)2 = 1 dia2(bi — Wi∕a)2 =∏q
1n
arg min -废 di(αbi — Wi)
α2
i=1
arg min Lkb Θ b Θ d∣ια2 — ∣∣b Θ d Θ w∣ια + C2
α2
1	IbΘdΘwI1 2
argmin 5kb θ b θ dkι α — IIkCkC m I
α 2	Ib Θb Θ dI1
∣∣b Θ d Θ w∣∣ι
IIb Θ b Θ d∣∣ι
∣∣b Θ d Θ w∣ι
kb Θ dkι .
1 IIb Θ d Θ wk1
2 Ilb Θ b Θ d∣∣ι
—
B	Loss-Aware Ternarization Algorithm (LAT)
The whole procedure of LAT is shown in Algorithm 3.
C Exact and Approximate Solutions for Ternarization with Two
Scaling Parameters
Let there be n1 positive elements and n2 negative elements in wl . For a n-dimensional vector
x = [x1, x2, . . . , xn], define inverse(x) = [xn , xn-1, . . . , x1]. As is shown in (12), the objective
can be separated into two parts, and each part can be viewed as a special case of ternarization step
in Proposition 3.1, dealing only with positive or negative weights. Thus the exact and approximate
solutions for αlt and βlt can separately be derived in a similar way as that of using one scaling
parameter. The exact and approximate solutions for αlt and βlt for layer-l at the tth time step are
shown in Algorithms 4 and 5.
D Experimental Details
D. 1 Setup for Feedforward Networks
The setup for the four data sets are as follows:
1. MNIST : This contains 28 × 28 gray images from 10 digit classes. We use 50, 000 images
for training, another 10, 000 for validation, and the remaining 10, 000 for testing. We use
the 4-layer model:
784F C 一 2048F C 一 2048F C 一 2048F C 一 10SV M,
where FC is a fully-connected layer, and SVM is a '2 -SVM output layer using the square
hinge loss. Batch normalization with a minibatch size 100, is used to accelerate learning.
The maximum number of epochs is 50. The learning rate starts at 0.01, and decays by a
factor of 0.1 at epochs 15 and 25.
14
Published as a conference paper at ICLR 2018
Algorithm 3 Loss-Aware Ternarization (LAT) for training a feedforward neural network.
Input: Minibatch {(xt0, yt)}, current full-precision weights {wlt}, first moment {mlt-1}, second
moment {vlt-1}, and learning rate ηt.
1:	Forward Propagation
2:	for l = 1 to L do
3:	compute αlt and blt using Algorithm 1 or 2;
4:	rescale the layer-l input: xtt-ι = α∣x∣-ι;
5:	compute Zt with input Xt_1 and binary weight bf;
6:	apply batch-normalization and nonlinear activation to zlt to obtain xlt ;
7:	end for
8:	compute the loss ` using xtL and yt ;
9:	Backward Propagation
10:	initialize output layer,s activation,s gradient ∂Xt;
11:	for l = L to 2 do
12:	compute ∂χ∂`γ using 翁,αt and bf;
13:	end for
14:	Update parameters using Adam
15:	for l = 1 to L do
16:	compute gradients V∕'(w^t) using 备 and xt-ι ；
17:	update first moment mf = βιmt-1 + (1 — βι)Vι'(W^t);
18:	update second moment vt = β2vt-1 + (1 — β2)(Vι'(W^t) Θ Vi'(w^t));
19:	compute unbiased first moment mt = mt∕(1 一 βt);
20:	compute unbiased second moment Vt = vit/(1 一 β2t );
21:	compute current curvature matrix df =今 61 + Pvt);
22:	update full-precision weights wt+1 = wt — mt 0 df;
23:	update learning rate ηt+1 = UpdateLearningrate(ηt, t + 1);
24:	end for
Algorithm 4 Exact solver for Wt with two scaling parameters.
1:	Input: full-precision weight wit, diagonal entries of the approximate Hessian dit-1.
2:	s1 = arg sort(wit);
3:	c1 = cum(perms1 (|dit-1 Θ wit|)) 0 cum(perms1(|dit-1|)) 0 2;
4:	S1 = find[([perms1(wit)][1:(n1-1)] — [c1][1:(n1-1)]) Θ [perms1(wit)][2:n1] — [c1][1:n1-1]) < 0);
5:	αt = 2argmaxci,i∈sι[cι]2 ∙ [cum(perms1 (|dt-1 |)儿；
6:	Pt = I+/2(Wt);
7:	s2 = inverse(s1);
8:	c2 = cum(perms2 (|dit-1 Θ wit|)) 0 cum(perms2(|dit-1|)) 0 2;
9： S2 = find(([—perms2 (Wt)][i：(n2-i)] — [c2][L(n2-i)]) θ ([—perms? (Wt)]⑵^] 一 [c2] 口皿 - 1]) <
0);
10:	βit = 2 arg maxci,i∈S2 [c2]i2 Θ [cum(perms2(|dit-1|))]i;
11:	qt = I-/2(wt);
12:	Output: W Wi = αtpt + βtqt.
2. CIFAR-10: This contains 32 × 32 color images from 10 object classes. We use 45, 000
images for training, another 5, 000 for validation, and the remaining 10, 000 for testing.
The images are preprocessed with global contrast normalization and ZCA whitening. We
use the VGG-like architecture:
(2×128C 3)—MP 2—(2×256C 3)—MP 2—(2×512C 3)—MP 2—(2×1024FC) —10SVM,
where C3 is a 3 × 3 ReLU convolution layer, and MP2 is a 2 × 2 max-pooling layer.
Batch normalization with a minibatch size of 50, is used. The maximum number of epochs
15
Published as a conference paper at ICLR 2018
Algorithm 5 Approximate solver for WIt with two scaling parameters
1:	Input: blt-1, full-precision weight wlt, and diagonal entries of approximate Hessian dlt-1.
2:	Initialize: α = 1.0, αold = 0.0, β = 1.0, βo = 0.0, b = blt-1, p = I0+(b), q = I0- (b),
10-6.
3:
4:
5:
6:
7:
8:
9:
10:
while ∣α - a0id∣ > C and ∣β - β0id∣ > e do
αold = α, βold = β ;
α = kPl 2 3Odt-1lOwtkι .
α =	kpΘdt-1kι ;
P = I+/2(Wt)；
β = kqΘdtTΘwtkι .
β =	kqΘdt-1kι ；
q = I-/2(Wt )；
end while
Output: Wt = αp + βq.
is 200. The learning rate for the weight-binarized network starts at 0.03 while for all the
other networks starts at 0.002, and decays by a factor of 0.5 after every 15 epochs.
3. CIFAR-100: This contains 32 × 32 color images from 100 object classes. We use 45, 000
images for training, another 5, 000 for validation, and the remaining 10, 000 for testing.
The images are preprocessed with global contrast normalization and ZCA whitening. We
use the VGG-like architecture:
(2×128C3)-MP2-(2×256C3)-MP2-(2×512C3)-MP2-(2×1024FC)-100SV M.
Batch normalization with a minibatch size of 100, is used. The maximum number of epochs
is 200. The learning rate starts at 0.0005, and decays by a factor of 0.5 after every 15
epochs.
4. SVHN: This contains 32 × 32 color images from 10 digit classes. We use 598, 388 images
for training, another 6, 000 for validation, and the remaining 26, 032 for testing. The images
are preprocessed with global and local contrast normalization. The model used is:
(2×64C3)-MP2-(2×128C3)-MP2-(2×256C3)-MP2-(2×1024FC)-10SV M.
Batch normalization with a minibatch size of 50, is used. The maximum number of epochs
is 50. The learning rate starts at 0.001 for the weight-binarized network, and 0.0005 for the
other networks. It then decays by a factor of 0.1 at epochs 15 and 25.
D.2 Setup for Recurrent Networks
The setup for the three data sets are as follows:
1. Leo Tolstoy’s War and Peace: It consists of 3258K characters of almost entirely En-
glish text with minimal markup and a vocabulary size of 87. We use the same train-
ing/validation/test set split as in (Karpathy et al., 2016； Hou et al., 2017).
2. The source code of the Linux Kernel: This consists of 621K characters and a vocabulary
size of 101. We use the same training/validation/test set split as in (Karpathy et al., 2016；
Hou et al., 2017).
3. The Penn Treebank data set (Taylor et al., 2003): This has been frequently used for language
modeling. It contains 50 different characters, including English characters, numbers, and
punctuations. We follow the setting in (Mikolov & Zweig, 2012), with 5,017K characters
for training, 393K for validation, and 442K characters for testing.
We use a one-layer LSTM with 512 cells. The maximum number of epochs is 200, and the number
of time steps is 100. The initial learning rate is 0.002. After 10 epochs, it is decayed by a factor
of 0.98 after each epoch. The weights are initialized uniformly in [0.08, 0.08]. After each iteration,
the gradients are clipped to the range [-5, 5]. All the updated weights are clipped to [-1, 1] for
binarization and ternarization methods, but not for m-bit (where m > 2) quantization methods.
16