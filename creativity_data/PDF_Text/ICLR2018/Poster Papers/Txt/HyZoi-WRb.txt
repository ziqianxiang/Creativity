Published as a conference paper at ICLR 2018
Debiasing Evidence Approximations:
On Importance-Weighted Autoencoders and
Jackknife Variational Inference
Sebastian Nowozin
Machine Intelligence and Perception
Microsoft Research, Cambridge, UK
Sebastian.Nowozin@microsoft.com
Ab stract
The importance-weighted autoencoder (IWAE) approach of Burda et al. (2015) de-
fines a sequence of increasingly tighter bounds on the marginal likelihood of latent
variable models. Recently, Cremer et al. (2017) reinterpreted the IWAE bounds
as ordinary variational evidence lower bounds (ELBO) applied to increasingly
accurate variational distributions. In this work, we provide yet another perspective
on the IWAE bounds. We interpret each IWAE bound as a biased estimator of the
true marginal likelihood where for the bound defined on K samples we show the
bias to be of order O(K-1). In our theoretical analysis of the IWAE objective we
derive asymptotic bias and variance expressions. Based on this analysis we develop
jackknife variational inference (JVI), a family of bias-reduced estimators reducing
the bias to O(K-(m+1)) for any given m < K while retaining computational
efficiency. Finally, we demonstrate that JVI leads to improved evidence estimates
in variational autoencoders. We also report first results on applying JVI to learning
variational autoencoders.1
1 Introduction
Variational autoencoders (VAE) are a class of expressive probabilistic deep learning models useful
for generative modeling, representation learning, and probabilistic regression. Originally proposed
in Kingma & Welling (2013) and Rezende et al. (2014), VAEs consist of a probabilistic model as
well as an approximate method for maximum likelihood estimation. In the generative case, the model
is defined as
p(x) =
Pθ(x|z)p(z) dz,
(1)
where z is a latent variable, typically a high dimensional vector; the corresponding prior distribution
p(z) is fixed and typically defined as a standard multivariate Normal distribution N (0, I). To achieve
an expressive marginal distribution p(x), We define pθ (x|z) through a neural network, making the
model (1) a deep probabilistic model.
Maximum likelihood estimation of the parameters θ in (1) is intractable, but Kingma & Welling
(2013) and Rezende et al. (2014) propose to instead maximize the evidence lower-bound (ELBO),
log p(x)
Ez 〜qω (z|x)
LE.
l Pθ (XIz) P(Z) 一
.°g qω (z∣χ) _
(2)
(3)
≥
Here, qω (z∣x) is an auxiliary inference network, parametrized by ω. Simultaneous optimization of (2)
over both θ and ω performs approximate maximum likelihood estimation in the model p(x) of (1)
and forms the standard VAE estimation method.
1The implementation is available at https://github.com/Microsoft/
jackknife-variational-inference
1
Published as a conference paper at ICLR 2018
In practice LE is estimated using Monte Carlo: We draw K samples Zi ~ qω(z∣χ), then use the
unbiased estimator LE of LE,
^	1 S] Pθ (XIzi) P(Zi)
LE = K i=1 log qω (ZiIx).
(4)
The VAE approach is empirically very successful but are there fundamental limitations? One
limitation is the quality of the modelpθ(xIz): this model needs to be expressive enough to model the
true distribution over x. Another limitation is that LE is only a lower-bound to the true likelihood. Is
this bound tight? It can be shown, Kingma & Welling (2013), that when q(zIx) = p(zIx) we have
LE = log p(x), hence (2) becomes exact. Therefore, we should attempt to choose an expressive class
of distributions q(zIx) and indeed recent work has extensively investigated richer variational families.
We discuss these methods in Section 7 but now review the importance weighted autoencoder (IWAE)
method we build upon.
2 Burda’s Importance-Weighted Autoencoder (IWAE) B ound
The importance weighted autoencoder (IWAE) method Burda et al. (2015) seemingly deviates
from (2) in that they propose the IWAE objective, defined for an integer K ≥ 1,
1 K pθ (xIzi) p(z)
log P(X) ≥ Ezι,...,zκ ~qω (ZIx) lθg K ʌ, 一《1J (Z ∣x)一	⑸
=: LK.	(6)
We denote with LK the empirical version which takes one sample zι,...,zκ 〜qω(z|x) and
evaluates the inner expression in (6). We can see that L1 = LE, and indeed Burda et al. (2015)
further show that
LE = L1 ≤L2 ≤ …≤ logp(x),	⑺
and limK→∞ LK= log P(X). These results are a strong motivation for the use of LKto estimate
θ and the IWAE method can often significantly improve over LE . The bounds LK seem quite
different from LE, but recently Cremer et al. (2017) and Naesseth et al. (2017) showed that an exact
correspondence exists: any LK can be converted into the standard form LE by defining a modified
distribution qIW(zIX) through an importance sampling construction.
We now analyze the IWAE bound LKin more detail. Independently of our work Rainforth et al.
(2017a) has analysed nested Monte Carlo objectives, including the IWAE bound as special case.
Their analysis includes results equivalent to our Proposition 1 and 2.
3 Analysis of the IWAE B ound
We now analyze the statistical properties of the IWAE estimator of the log-marginal likelihood. Basic
consistency results have been shown in Burda et al. (2015); here we provide more precise results
and add novel asymptotic results regarding the bias and variance of the IWAE method. Our results
are given as expansions in the order K of the IWAE estimator but do involve moments μ% which
are unknown to us. The jackknife method in the following sections will effectively circumvent the
problem of not knowing these moments.
Proposition 1 (Expectation of LK). Let P be a distribution supported on the positive real line
and let P have finite moments of all order. Let K ≥ 1 be an integer. Let w1,w2,..., WK 〜 P
independently. Then we have asymptotically, for K → ∞,
1K
E[Lk] = E log KEwi
i=1
logEM - K 养+K (3μ3- 4μ2)
-K (4μ4-4μ2-誓)+o(K-
(8)
where μi := EP [(w 一 EP [w])i] is the i 'th central moment of P and μ := EP [w] is the mean.
2
Published as a conference paper at ICLR 2018
Proof. See Appendix A, page 12.
□
The above result directly gives the bias of the IWAE method as follows.
Corollary 1 (Bias of LK). If we see LK as an estimator of log p(x), then for K → ∞ the bias of
LK is
-人 -
B[Lk ]
(9)
_ . ʌ	,	_	一「 r
E[Lk ] — log E[w]
(10)
Proof. The bias (10) follows directly by subtracting the true value log p(x) = log E[w] from the
right hand side of (8).	□
The above result shows that the bias is reduced at a rate of O(1/K). This is not surprising because the
IWAE estimator is a smooth function applied to a sample mean. The coefficient of the leading O(1/K)
bias term uses the ratio μ2∕μ2, the variance divided by the squared mean of the P distribution. The
quantity ,μ2 ∕μ2 is known as the coefficient ofvariation and is a common measure of dispersion of
a distribution. Hence, for large K the bias of LK is small when the coefficient of variation is small;
this makes sense because in case the dispersion is small the logarithm function behaves like a linear
function and few bias results. The second-order and higher-order terms takes into account higher
order properties of P .
The bias is the key quantity we aim to reduce, but every estimator is also measured on its variance.
We now quantify the variance of the IWAE estimator.
Proposition 2 (Variance of LK). For K → ∞, the variance of LK is given asfollows.
v[Lk ]
1 μ2	1
-- -  -
K μ2	K2
+ o(K-2).
(11)
Proof. See Appendix A, page 13.
□
Both the bias B[LK] and the variance V[LK] vanish for K → ∞ at a rate of O(1∕K) with similar
coefficients. This leads to the following result which was already proven in Burda et al. (2015).
Corollary 2 (Consistency of LK). For K → ∞ the estimator LK is consistent, that is, for all > 0
lim P(|Lk — logp(x)∣ ≥ e) = 0.	(12)
K→∞
Proof. See Appendix A, page 13.
□
How good are the asymptotic results? This is hard to say in general because it depends on the
particular distribution P(w) of the weights. In Figure 1 we show both a simple and challenging case
to demonstrate the accuracy of the asymptotics.
The above results are reassuring evidence for the IWAE method, however, they cannot be directly
applied in practice because we do not know the moments μi . One approach is to estimate the moments
from data, and this is in fact what the delta method variational inference (DVI) method does, Teh
et al. (2007), (see Appendix B, page 14); however, estimating moments accurately is difficult. We
avoid the difficulty of estimating moments by use of the jackknife, a classic debiasing method. We
now review this method.
3
Published as a conference paper at ICLR 2018
0.1∣-
0.0 -
-0.1-
S -0.2-
- -0.3 -
-0.4-
-0.5-
-0.6-
0
BiaS of log p(x) evidence approximations, P=Gamma(1；1)
K
ι0Variance of log p(x) evidence approximations, P=Gamma(1；1)
— IWAE, I[l^k ], empirical
— IWAE, ⅛[Lk ], asymptotic
— JVI, ⅛[LK ], empirical
o 1
10W
0ouee>
10-2 ---------------1---------------1---------------i--------------
0	5	10	15	20
K

(b) Asymptotic variance for a simple case.
(a) Asymptotic bias for a simple case.
Bias of log p(x) evidence approximations, P=Gamma(0.1,1)
IWAE, .^[l^k ], empirical
IWAE, ⅛[j^k ], asymptotic
JVI, ⅛[j^K ], empirical
JVI, . [j^K ], asymptotic
--5
tn
-10
lV
ariance of log p(x) evidence approximations, P=Gamma(0.1,1)
一- IWAE,兄LK], empirical :
— IWAE, ,.[LK], asymptotic -
一- JVI,兄LK], empirical :
5	10
15	20
10
K
15
20
(c) Asymptotic bias for a challenging case.	(d) Asymptotic variance for a challenging case.
Figure 1: Comparing asymptotics with empirical values of bias and variance on P = Gamma(1, 1)
using 100,000 independent evaluations: (a)-(b) shows a simple case, P = Gamma(1, 1), and (c)-(d)
shows a challenging case. Observation: (a) the IWAE is negatively biased, underestimating log p(x),
with asymptotic expression (10) agreeing very well with empirical bias; (b) empirical and asymptotic
variance (11) in good agreement. (c) for a challenging case, the bias asymptotics match empirical
estimates for K ≥ 10; (d) in the challenging case, the variance asymptotics match empirical estimates
for K ≥ 10.
4 A B rief Review of the Jackknife
We now provide a brief review of the jackknife and generalized jackknife methodology. Our presenta-
tion deviates from standard textbook introductions, Miller (1974), in that we also review higher-order
variants.
The jackknife methodology is a classic resampling technique originating with Quenouille (1949;
1956) in the 1950s. It is a generally applicable technique for estimating the bias B[T] = E[T] - T
and the variance V[T] of an estimator T. Our focus is on estimating and correcting for bias.
The basic intuition is as follows: in many cases it is possible to write the expectation of a consistent
estimator Tn evaluated on n samples as an asymptotic expansion in the sample size n, that is, for
large n → ∞ we have
E[Tn]= T + a1 + a2 + ....	(13)
n	n2
In particular, this is possible in case the estimator is consistent and a smooth function of linear
statistics. If an expansion (13) is possible, then we can take a linear combination of two estimators
Tn and Tn-1 to cancel the first order term,
一 . ^ , . ^ -
ElnTn - (n - I)Tn-1]
n (T + a1 + a2) - (n - 1)
n	n2
T+≡ - n¾ + O"2)
T+
a1
n-1
a2
(n — 1)2
+ O(n-2)
(14)
a
n(n - 1)
+ O(n-2)
T+O(n-2).
(15)
(16)
Therefore, the jackknife bias-corrected estimator TJ := nTn - (n - 1)Tn-1 achieves a reduced bias
of O(n-2). For Tn-I any estimator which preserves the expectation (13) can be used. In practice
+
4
Published as a conference paper at ICLR 2018
we use the original sample of size n to create n subsets of size n - 1 by removing each individual
sample once. Then, the empirical average of n estimates R-1, i = 1,...,n is used in place of Tn-1.
In Sharot (1976) this construction was proved optimal in terms of maximally reducing the variance of
TJ for any given sample size n.
In principle, the above bias reduction (16) can be repeated to further reduce the bias to O(n-3) and
beyond. The possibility of this was already hinted at in Quenouille (1956) by means of an example.2
A fully general and satisfactory solution to higher-order bias removal was only achieved by the
generalized jackknife of Schucany et al. (1971), considering estimators TG of order m, each having
the form,
m
TGm)= X c(n,m,j) Tn-j.	(17)
j=0
The form of the coefficients c(n, m,j) in (17) are defined by the ratio of determinants of certain
Vandermonde matrices, see Schucany et al. (1971). In a little known result, an analytic solution
for c(n, m, j ) is given by Sharot (1976). We call this form the Sharot coefficients, (Sharot, 1976,
Equation (2.5) with r = 1), defined for m < n and 0 ≤ j ≤ m,
c(n, m,j) = (-1)j
(n - j)m
(m - j)! j!
(18)
The generalizedjackknife estimator TGm achieves a bias of order O(m-(j+1)), see Schucany et al.
(1971). For example, the classic jackknife is recovered because c(n, 1, 0) = n and c(n, 1, 1) =
-(n - 1). As an example of the second-order generalized jackknife we have
n2	(n - 2)2
c(n, 2, O) = ɪ,	c(n, 2, I) = -(n - I)2,	c(n, 2, 2) = -----2—.	(19)
The variance of generalized jackknife estimators is more difficult to characterize and may in general
decrease or increase compared to Tn. Typically We have V[TGm+1)] > V[TGm)] with asymptotic
rates being the same.
The generalized jackknife is not the only method for debiasing estimators systematically. One classic
method is the delta method for bias correction Small (2010). Two general methods for debiasing are
the iterated bootstrap for bias correction (Hall, 2016, page 29) and the debiasing lemma McLeish
(2010); Strathmann et al. (2015); Rhee & Glynn (2015). Remarkably, the debiasing lemma exactly
debiases a large class of estimators.
The delta method bias correction has been applied to variational inference by Teh et al. (2007); we
provide novel theoretical results for the method in Appendix B, page 14.
5	Jackknife Variational Inference (JVI)
We now propose to apply the generalized jackknife for bias correction to variational inference by
debiasing the IWAE estimator. The resulting estimator of the log-marginal likelihood will have
significantly reduced bias, however, in contrast to the ELBO and IWAE, it is no longer a lower bound
on the true log-marginal likelihood. Moreover, it can have increased variance compared to both IWAE
and ELBO estimators. We will empirically demonstrate that the variance is comparable to the IWAE
estimate and that the bias reduction is very effective in improving our estimates.
Definition 1 (Jackknife Variational Inference (JVI)). Let K ≥ 1 and m < K. The jackknife
variational inference estimator of the evidence of order m with K samples is
m
LKm := X C(K,m,j) LK-j,	QO)
j=0
where LK-j is the empirical average of one or more IWAE estimates obtained from a subsample
of size K - j, and c(K, m, j) are the Sharot coefficients defined in (18). In this paper we use all
2Which was subtly wrong and did not reduce the bias to O(n-2) as claimed, see Schucany et al. (1971).
5
Published as a conference paper at ICLR 2018
possible KK-j subsets, that is,
1	(KK-j)
LK-j =( K、 X LK-j (Z(K j)),	QI)
K-j	i=1
where Z(K-j) is the i ’h subset of size K 一 j among all (K-j) subsets from the original samples
Z = (z1,z2,..., ZK). WefUrtherdefine LKm = EZ[LJKm].
From the above definition we can see that JVI strictly generalizes the IWAE bound and therefore also
includes the standard ELBO objective: We have the IWAE case for LJK = LK, and the ELBO case
for LJ = LE.
5.1	Analysis of LKm
The proposed family of JVI estimators has less bias than the IWAE estimator. The folloWing result is
a consequence of the existing theory on the generalized jackknife bias correction.
Proposition 3 (Bias of LKm). For any K ≥ 1 and m < K we have that the bias of the JVI estimate
satisfies
B[LKm] = E[LKm - logp(x)] = LKm - logp(x) = O(K-(m+1)).	(22)
Proof. The JVI estimator LKm is the application of the higher-order jackknife to the IWAE estimator
Which has an asymptotic expansion of the bias (10) in terms of orders of 1/K. The stated result is
then a special case of (Schucany et al., 1971, Theorem 4.2).	□
We shoW an illustration of higher-order bias removal in Appendix C, page 15. It is more difficult to
characterize the variance of LJKm. Empirically we observe that V[LKm] < V[LKm ] for m < m0, but
We have been unable to derive a formal result to this end. Note that the variance is over the sampling
distribution of q(z|x), so we can always reduce the variance by averaging multiple estimates LJlKm,
whereas we cannot reduce bias this way. Therefore, reducing bias while increasing variance is a
sensible tradeoff in our application.
5.2	Efficient Computation of LKm
We now discuss how to efficiently compute (20). For typical applications, for example in variational
autoencoders, we will use small values of K, say K < 100. However, even with K = 50 and m = 2
there are already 1276 IWAE estimates to compute in (20-21). Therefore efficient computation
is important to consider. one property that helps us is that all these iWAE estimates are related
because they are based on subsets of the same weights. The other property that is helpful is that
computation of the K weights is typically orders of magnitude more expensive than elementary
summation operations required for computation of (21).
We now give a general algorithm for computing the JVI estimator LJKm, then give details for efficient
implementation on modern GPUs and state complexity results.
Algorithm 1 computes log-weights and implements equations (20-21) in a numerically robust
3
manner.3
Proposition 4 (Complexity of Algorithm 1). Given K ≥ 1 and m ≤ K/2 the complexity of
Kem (K 厂).	(23)
Proof. see Appendix C, page 15.
□
3As usual, the log-sum-exp operation needs to be numerically robustly implemented.
6
Published as a conference paper at ICLR 2018
Algorithm 1 Computing LKm,thejaCkknife variational inference estimator
1:	function COMPUTEJVI(m, K, p, q, x)
2:	for i = 1, . . . , K do
3:	Sample Zi 〜q(z∣x)
4:	Vi — logp(x∣Zi) + logP(Zi) - log q(zi∣x)
5:	end for
6:	L — 0
7:	for j = 0, . . . , m do
8:	L — 0
9:	for S ∈ EnumerateSubsets({1, . . . , K}, K - j) do
10:	L — L + log ps∈s exp Vs - log(K - j)
11:	end for
12:	L — L + C((Kimj L
13:	end for
14:	return L
15:	end function
.LK-j
. list all subsets of size K - j
. IWAE estimate for subset S
. Using equation (18)
.JVI estimate LKm
The above algorithm is suitable for CPU implementation; to utilize modern GPU hardware efficiently
we can instead represent the second part of the algorithm using matrix operations. We provide further
details in Appendix C, page 16. Figure 2 demonstrates experimental runtime evaluation on the
MNIST test set for different JVI estimators. We show all JVI estimators with less than 5,000 total
summation terms. The result demonstrates that runtime is largely independent of the order of the JVI
correction and only depends linearly on K .
5.3 Variations of the JVI Estimator
Variations of the JVI estimator with improved runtime exist. Such reduction in runtime are possible
if we consider evaluating only a fraction of all possible subsets in (21). When tractable, our choice of
evaluating all subsets is generally preferable in terms of variance of the resulting estimator. However,
to show that we can even reduce bias to order O(K-K) at cost O(K) we consider the estimator
K-1
LX := X c(K,K - 1,j) LK-j(Zi：(K-j))	(24)
j=0
= c(K, K - 1,K - 1) log(exp(vK))	(25)
+ c(K,K - 1,K - 2) log Q(exp(vχ-i) +exp(vχ)))	(26)
+ …+ c(K,K - 1, 0) log (K X exp®)) .	(27)
The sum (25-27) can be computed in time O(K) by keeping a running partial sum Pk=I exp(vi)
for k ≤ K and by incrementally updating this sum4, meaning that (24) can be computed in O(K)
4To do this in a numerically stable manner, we need to use streaming log-sum-
exp computations, see for example http://www.nowozin.net/sebastian/blog/
streaming-log-sum-exp-computation.html
7
Published as a conference paper at ICLR 2018
overall. As a generalized jackknife estimate LX has bias O(K-K). We do not recommend its use in
practice because its variance is large, however, developing estimators between the two extremes of
taking one set and taking all sets of subsets of a certain size seems a good way to achieve high-order
bias reduction while controlling variance.
6	Experiments
We now empirically validate our key claims regarding the JVI method: 1. JVI produces better
estimates of the marginal likelihood by reducing bias, even for small K ; and 2. Higher-order bias
reduction is more effective than lower-order bias reduction;
To this end we will use variational autoencoders trained on MNIST. Our setup is purposely identical
to the setup of Tomczak & Welling (2016), where we use the dynamically binarized MNIST data
set of Salakhutdinov & Murray (2008). Our numbers are therefore directly comparable to the
numbers reported in the above works. Our implementation is available at https://github.
com/Microsoft/jackknife-variational-inference.
We first evaluate the accuracy of evidence estimates given a fixed model. This setting is useful for
assessing model performance and for model comparison.
6.1	JVI as Evaluation Method
We train a regular VAE on the dynamically binarized MNIST dataset using either the ELBO, IWAE,
or JVI-1 objective functions. We use the same two-layer neural network architecture with 300 hidden
units per layer as in (Tomczak & Welling, 2016). We train on the first 50,000 training images, using
10,000 images for validation. We train with SGD for 5,000 epochs and take as the final model the
model with the maximum validation objective, evaluated after every training epoch. Hyperparameters
are the batch size in {1024, 4096} and the SGD step size in {0.1, 0.05, 0.01, 0.005, 0.001}. The final
model achieving the best validation score is evaluated once on the MNIST test set. All our models
are implemented using Chainer (Tokui et al., 2015) and run on a NVidia Titan X.
For three separate models, trained using the ordinary ELBO, IWAE, and JVI-1 objectives, we then
estimate the marginal log-likelihood (evidence) on the MNIST test set. For evaluation we use JVI
estimators up to order five in order to demonstrate higher-order bias reduction. Among all possible
JVI estimators up to order five we evaluate only those JVI estimators whose total sum of IWAE
estimates has less than 5,000 terms. For example, We do not evaluate LJ3 because it contains
(302) + (312) + (322) + (32) = 5489 terms.5
Figure 3 shoWs the evidence estimates for three models. We make the folloWing observations,
applying to all plots: 1. Noting the logarithmic x-axis We can see that higher-order JVI estimates
are more than one order of magnitude more accurate than IWAE estimates. 2. The quality of the
evidence estimates empirically improves monotonically With the order of the JVI estimator; 3. In
absolute terms the improvements in evidence estimates is larges for small values of K, Which is What
is typically used in practice; 4. The higher-order JVI estimators remove loW-order bias but significant
higher-order bias remains even for K = 64, shoWing that on real VAE log-Weights the contribution
of higher-order bias to the evidence error is large; 5. The standard error of each test set marginal
likelihood (shoWn as error bars, best visible in a zoomed version of the plot) is comparable across all
JVI estimates; this empirically shoWs that higher-order bias reduction does not lead to high variance.
6.2	JVI as a Training Objective
We noW report preliminary results on learning models using the JVI objectives. The setting is the
same as in Section 6.1 and We report the average performance of five independent runs.
Table 1 reports the results. We make the folloWing observations: 1. When training on the IWAE and
JVI-1 objectives, the respective score by the ELBO objective is impoverished and this effect makes
5We do this because We discovered numerical issues for large sums of varying size and found all summations
of less than a feW thousand terms not to have this problem but We are looking into a Way to compute more
summation terms in a fast and robust manner.
8
Published as a conference paper at ICLR 2018
Evidence Estimates on ELBO-traιned VAE (MNIST test set)	Evidence Estimates on IWAE-trained VAE (MNIST test set)
(l)xw。一 J。04eE-fjs0 gu0p->w
100	101	102	103
latent sample count K
(a) Evidence estimates on VAE-trained MNIST model.
(l)x
-86
--88
O)
E -90
tɔ
O)
ai -92
U
O)
P _
> 一94
100	101	102	103
latent sample count K
(b) Evidence estimates on IWAE-trained MNIST model.
-84
-86
-88
-90
-92
-94
-96
-98
Evidence Estimates OnJVI-1-trained VAE (MNIST test Set)
(l)xw。一 J。04eE-fjs0 :0OU0P->W
ELBO
IWAE (JVI-0)
,JVI-1
,JVI-2
,JVI-3
,JVI-4
,JVI-5
LJ0
LK0.
LK1
LK2
LK3
LK4
LK5
latent sample count K
(C) Evidence estimates on JVI-1-trained MNIST model.
Figure 3: Comparing evidence approximations on MNIST variational autoencoders: (a) VAE trained
一 _ _ . ^ . . 一 , . ^ .. ..
using the ELBO objective LE; (b) VAE trained using the IWAE objective LK With K = 32; (c) VAE
trained using the JVI-1 objective LKI with K = 32.
Training objective (K =	32)	ELBO	Evaluation objective (nats), K = 32		JVI-2
			IWAE	JVI-1	
ELBO		-93.38 ± 0.03	-89.22 ± 0.02	-88.66 ± 0.02	-88.40 ± 0.02
IWAE		-95.30 ± 0.05	-86.05 ± 0.01	-85.28 ± 0.03	-85.01 ± 0.02
JVI-1		-99.19 ± 0.06	-86.56 ± 0.02	-85.43 ± 0.02	-85.14 ± 0.01
Table 1: Evaluating models trained using ELBO, IWAE, and JVI-1 learning objectives.
sense in light of the work of Cremer et al. (2017). Interestingly the effect is stronger for JVI-1. 2. The
model trained using the JVI-1 objective falls slightly behind the IWAE model, which is surprising
because the evidence is clearly better approximated as demonstrated in Section 6.1. We are not
sure what causes this issue, but have two hypotheses: First, in line with recent findings (Rainforth
et al., 2017b) a tighter log-evidence approximation could lead to poor encoder models. In such case
it is worth exploring two separate learning objectives for the encoder and decoder; for example,
using an ELBO for training the encoder, and an IWAE or JVI-1 objective for training the decoder.
Second, because JVI estimators are no longer bounds it could be the case that during optimization
of the learning objective a decoder is systematically learned in order to amplify positive bias in the
log-evidence.
7	Related Work
The IWAE bound and other Monte Carlo objectives have been analyzed by independently by Rainforth
et al. (2017a). Their analysis is more general than our IWAE analysis, but does not propose a method
to reduce bias.
Delta-method variational inference (DVI) proposed by Teh et al. (2007) is the closest method we are
aware of and we discuss it in detail as well as provide novel results in Appendix B, page 14. Another
exciting recent work is perturbative variational inference (Bamler et al., 2017) which considers
different objective functions for variational inference; we are not sure whether there exists a deeper
relationship to debiasing schemes.
9
Published as a conference paper at ICLR 2018
There also exists a large body of work that uses the ELBO objective but considers ways to enlarge
the variational family. This is useful because the larger the variational family, the smaller the bias.
Non-linear but invertible transformations of reference densities have been used initially for density
estimation in NICE (Dinh et al., 2014) and for variational inference in Hamiltonian variational
inference (Salimans et al., 2015). Around the same time the general framework of normalizing
flows (Rezende & Mohamed, 2015) unified the previous works as some invertible continuous transfor-
mation of a distribution. Since then a large number of specialized flows with different computational
requirements and flexibility have been constructed: inverse autoregressive flows (Kingma et al.,
2016), masked autoregressive flows Papamakarios et al. (2017), and Householder flows (Tomczak &
Welling, 2016).
Another way to improve the flexibility of the variational family has been to use implicit models (Mo-
hamed & Lakshminarayanan, 2016) for variational inference; this line of work includes adversarial
variational Bayes (Mescheder et al., 2017), wild variational inference (Li & Liu, 2016), deep implicit
models (Tran et al., 2017), implicit variational models (HUszar, 2017), and adversarial message
passing approximations (Karaletsos, 2016).
8	Conclusion
In sUmmary we proposed to leverage classic higher-order bias removal schemes for evidence es-
timation. OUr approach is simple to implement, compUtationally efficient, and clearly improves
over existing evidence approximations based on variational inference. More generally oUr jackknife
variational inference debiasing formUla can also be Used to debias log-evidence estimates coming
from annealed importance sampling.
However, one sUrprising finding from oUr work is that Using oUr debiased estimates for training VAE
models did not improve over the IWAE training objective and this is sUrprising becaUse apriori a
better evidence estimate shoUld allow for improved model learning.
One possible extension to oUr work is to stUdy the Use of other resampling methods for bias redUction;
promising candidates are the iterated bootstrap, the Bayesian bootstrap, and the debiasing lemma.
These methods coUld offer fUrther improvements on bias redUction or redUced variance, however, the
key challenge is to overcome compUtational reqUirements of these methods or, alternatively, to derive
key qUantities analytically.6 Application of the debiasing lemma in particUlar reqUires the carefUl
constrUction of a trUncation distribUtion and often prodUces estimators of high variance.
While variance redUction plays a key role in certain areas of machine learning, oUr hope is that oUr
work shows that bias redUction techniqUes are also widely applicable.
References
Jordanka A Angelova. On moments of sample mean and variance. Int J. Pure Appl. Math, 79:67-85,
2012.
Robert Bamler, Cheng Zhang, Manfred Opper, and Stephan Mandt. PertUrbative black box variational
inference. arXiv preprint arXiv:1709.07433, 2017.
YUri BUrda, Roger Grosse, and RUslan SalakhUtdinov. Importance weighted aUtoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Chris Cremer, QUaid Morris, and David DUvenaUd. Reinterpreting importance-weighted aUtoencoders.
arXiv preprint arXiv:1704.02916, 2017.
LaUrent Dinh, David KrUeger, and YoshUa Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Peter Hall. Methodology and theory for the bootstrap, 2016. URL http://anson.ucdavis.
edu∕~peterh∕sta251∕bootstrap-leCtures-to-may-16.pdf.
6For example, if We could analytically compute Eu〜U[log PK=I UiWi] for the sampling distributions U
appearing in the jackknife and bootstrap methods, we coUld develop improved closed-form estimators of the log
marginal likelihood.
10
Published as a conference paper at ICLR 2018
Ferenc Huszar. Variational inference using implicit distributions. arXiv preprint arXiv:1702.08235,
2017.
Theofanis Karaletsos. Adversarial message passing for graphical models. arXiv preprint
arXiv:1612.05048, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.
Yingzhen Li and Qiang Liu. Wild variational approximations. In NIPS workshop on advances in
approximate Bayesian inference, 2016.
Don McLeish. A general method for debiasing a monte carlo estimator. arXiv preprint
arXiv:1005.2228, 2010.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722,
2017.
Rupert G Miller. ThejaCkknife-a review. Biometrika,61(1):1-15, 1974.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
Christian A Naesseth, Scott W Linderman, Rajesh Ranganath, and David M Blei. Variational
sequential monte carlo. arXiv preprint arXiv:1705.11140, 2017.
George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2335-2344, 2017.
Maurice H Quenouille. Approximate tests of correlation in time-series. Journal of the Royal
Statistical Society. Series B (Methodological), 11(1):68-84, 1949.
Maurice H Quenouille. Notes on bias in estimation. Biometrika, 43(3/4):353-360, 1956.
Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, and Frank Wood. On the
opportunities and pitfalls of nesting monte carlo estimators. arXiv preprint arXiv:1709.06181,
2017a.
Tom Rainforth, Tuan Anh Le, Maximilian Igl, Chris J. Maddison, Yee Whye Teh, and Frank
Wood. Tighter variational bounds are not necessarily better. In Bayesian Deep Learn-
ing Workshop at Neural Information Processing Systems (NIPS) 2017, 2017b. URL http:
//bayesiandeeplearning.org/2017/papers/55.pdf.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Chang-han Rhee and Peter W Glynn. Unbiased estimation with square root convergence for sde
models. Operations Research, 63(5):1026-1043, 2015.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th international conference on Machine learning, pp. 872-879. ACM, 2008.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 1218-1226, 2015.
WR Schucany, HL Gray, and DB Owen. On bias reduction in estimation. Journal of the American
Statistical Association, 66(335):524-533, 1971.
11
Published as a conference paper at ICLR 2018
Trevor Sharot. The generalized jackknife: finite samples and subsample sizes. Journal of the
American Statistical Association, 71(354):451-454, 1976.
Christopher G. Small. Expansions and Asymptotics for Statistics. CRC Press, 2010.
Heiko Strathmann, Dino Sejdinovic, and Mark Girolami. Unbiased bayes for big data: Paths of
partial posteriors. arXiv preprint arXiv:1501.03326, 2015.
Yee W Teh, David Newman, and Max Welling. A collapsed variational bayesian inference algorithm
for latent dirichlet allocation. In Advances in neural information processing systems, pp. 1353-
1360, 2007.
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open
source framework for deep learning. In Proceedings of workshop on machine learning systems
(LearningSys) in the twenty-ninth annual conference on Neural Information Processing Systems
(NIPS), volume 5, 2015.
Jakub M Tomczak and Max Welling. Improving variational auto-encoders using householder flow.
arXiv preprint arXiv:1611.09630, 2016.
Dustin Tran, Rajesh Ranganath, and David M Blei. Deep and hierarchical implicit models. arXiv
preprint arXiv:1702.08896, 2017.
Lingyun Zhang. Sample mean and sample variance: Their covariance and their (in) dependence. The
American Statistician, 61(2):159-160, 2007.
Appendix A: Proofs for the IWAE Analysis
— — 二
EXPECTATION AND BIAS OF LK
Proof. (Of Proposition 1, page 2) To show (8) we apply the delta method for moments (Small, 2010,
Section 4.3). First, we define the random variable YK := KK PK=1 Wi corresponding to the sample
mean of w1, . . . , wK . Because of linearity of expectation we have E[YK ] = E[w]. We expand the
logarithm function log YK = log(E[w] + (YK - E[w])) around E[w] to obtain
log(E[w] + (YK - E[w]))
∞ ( 1)j
log EM- X jE[Wj (YK - EM尸.
(28)
Note that only YK is random in (28), all other quantities are constant. Therefore, by taking the
expectation on the left and right side of (28) we obtain
E[log YK]
∞ ( 1)j
log E[w] - ∑ jE[Wj E[(YK - E[w])j].
(29)
The right hand side of (29) is expressed in terms of the central moments for i ≥ 2, γi := E[(YK -
E[YK])i ] of YK, whereas we are interested in an expression using the central moments i ≥ 2,
μi := E[(w - E[w])i] of P. With Y = μ = E[w] We denote the shared first non-central moment.
Because Yk is a sample mean We can use existing results that relate Yi to μi. In particular (Angelova,
2012, Theorem 1) gives the relations
Y
Y2
Y3
Y4
Y5
μ
μ2
K
μ3
K2
Kμ2 + K (μ4- 3μ2)
10	1
K3μ3μ2 + K4 (μ5 - 10μ3μ2) .
(30)
(31)
(32)
(33)
(34)
12
Published as a conference paper at ICLR 2018
Expanding (29) to order five and using the relations (30) to (34) gives
E[log YK]	= log E[w]	- 5^^2	μ	+ γ-3	μ3k	- -τ^	(记μ2	+ 记 (μ4
2μ2	K 3μ3	K2	4μ4	∖K2 K3
+ 5μ5 (K3μ3μ2 + K4 (μ - 10μ3μ2)^ + O(K-3).
Regrouping the terms by order of K produces the result (8).
—
(35)
□
…	久
VARIANCE OF LK
Proof. (Of Proposition 2, page 3) We use the definition of the variance and the series expansion of
the logarithm function, obtaining
V[log YK] = E[(log YK -E[logYK])2]
∞ ( 1)i	∞
I log μ - i i (YK - μ) - log μ +
∖	i=1 μ	i=1
(X( iμi (E[(Yκ - μ)i] -(YK - 〃y))
(36)
WL E[(YK-E
(37)
E
E
By expanding (37) to third order and expanding all products we obtain a moment expansion of YK as
follows.
V[logYκ]	≈	μ2	-	μ3(Y3	- γιγ2)+3μ4(Y4	- γιγ3)+4μ4% -Y)	(38)
-3μ5 (Y5 - 7273) + 9μ6(Y6 - γ3).	(39)
By substituting the sample moments γi of YK with the central moments μi of the original distribution
P and simplifying we obtain
V[log YK ] = ⅛ μ2 - K (μ3 - 5μ2)+O(K-2)∙	(40)
□
CONSISTENCY OF LK
Proof. We have
_	, . Λ	_	, .	、	_	, . Λ	—一八、	一-八、	_	..	、
P(∣LK - logp(x)∣	≥ e)	=	P(∣LK	-	E[LK] + E[LK] - logp(x)∣ ≥ e)	(41)
≤	P(∣LK	-	E[LK]| + ∣E[LK] - logp(x)∣≥e)	(42)
≤	P(∣LK	-	E[LK ]| ≥ I)+ P(IE[LK ] - log P(χ)l≥	∣).	(43)
The second term in (43) does not involve a random variable therefore is either zero or one. For large
enough K it will always be zero due to (10).
For the first term in (43) we apply Chebyshev,s inequality. We set T = -l e ʌ and have
2 √V[Lk ]
I
P(∣LK - E[LK]| ≥ -) = P(∣LK - E[LK]| ≥ T，V[LK])	(44)
≤ 3	(45)
T2
=4 VLN	(46)
-2
= O(1/K ).	(47)
Thus, for K → ∞ and any- > 0 we have that (43) has a limit of zero. This establishes convergence
in probability and hence consistency.	□
13
Published as a conference paper at ICLR 2018
Appendix B: Analysis of Delta-Method Variational Inference (DVI)
Definition 2 (Delta method Variational Inference (DVI), (Teh et al., 2007)).
where
K
D	1	w2
LK := Ezι,…,zκ~qω(ZIx) log K / vwi + 2κw
i=1
wi	_	P(XIzi) P(Zi)	._] 一	7 i~ʌ-,	i = 1, . ■ qω (zi∣w)	. , K,
w2	:二	1K —	1	X^^(w. _ w)2	
	-K - 1 2(Wi	w) ,	
w :二	i=1 1K =K X wi,	
so that W2 corresponds to the sample variance and W corresponds to the sample mean.
The practical Monte Carlo estimator of (48) is defined as follows.
Zi 〜qω (z|x),	i =1,...,K,
K
LK := log K X wi + 2KW^.
i=1
(48)
(49)
(50)
(51)
(52)
(53)
Analysis of Delta Method Variational Inference
Proposition 5 (Bias of LK). We evaluate the bias of LK in (53) asfollows.
B[LK]	= -K 仪-» + O(K-2).
(54)
Proof. Consider the function f (x, y) = y2 and its second order Taylor expansion around (x, y)=
(μ2,μ),
f(μ2 + (μ2 - μ2),μ + (μ - μ)) ≈ ” +—2(μ2 - μ2) —ɪ(μ - μ)	(55)
μ2	μ2	μ3
-μ3 (μ2 - μ2)(μ—μ) + 2μ∣ (μ —μ)2.	(56)
Taking expectations on both sides cancels all linear terms and yields
E 与 ≈ 与-----------3 E[(&2 - μ∣)(μ — μ)] + ~∣E[E[(μ - μ)∣].	(57)
μ	μ μ	μ
By classic results we have that the expected variance of the sample mean around the true mean is
related to the variance by E[(μ 一 μ)∣] = μ∣∕K. Furthermore, Zhang (2007) showed a beautiful result
about the covariance of sample mean and sample variance for arbitrary random variables, namely that
Cov[μ∣,μ] = E[(μ∣ - μ∣)(μ - μ)] = μ3∕K.	(58)
Using both results in (57) produces
E [i] = μ∣ - ⅛ (专 — 号)+ o(K-1).	(59)
We can now decompose the expectation of LK as follows.
一一 「1Λ 1	「 μ 1
E[LK] = E log- Xwi + E 罢∣	(60)
K 七	L2Kμ2J
log E[w] -	μ∣ 2Kμ2	1+ 2K '	μ∣	1	2μ3 lμ∣ - K(Tr--	者)+ O(K-1))	(61)
log E[w] -	K∣ (	μ3	3μ - μ3	2A	∣4∣) +O(K-∣).		(62)
14
Published as a conference paper at ICLR 2018
(a) Asymptotic bias for a simple case.
ι0Variance of log p(x) evidence approximations, P=Gamma(1；1)
一■ IWAE, i[Lκ ], empirical ：
— IWAE, ,.[Lk], asymptotic1
— DVI,兄LK ], empirical ：
一■ JVI, i[LK], empirical :
O 1
10W
0ouee>
10-2 ---------------1---------------1---------------i--------------
0	5	10	15	20
K
(b)	Asymptotic variance for a simple case.

lV
Bias of log p(x) evidence approximations, P=Gamma(0.1,1)
IWAE, MLk ], empirical
IWAE, MLk ], asymptotic
DVI, .⅛[jLK ], empirical
DVI, H[L^K ], asymptotic
JVI, ⅛[j^K ], empirical
JVI, H [j^K ], asymptotic
--5
tn
-10
ariance of log p(x) evidence approximations, P=Gamma(0.1,1)
一- IWAE,兄LK], empirical :
— IWAE, ,.[LK], asymptotic -
— DVI,兄LK ], empirical	:
一- JVI,兄LK], empirical -
2 10
Ooo
111
0ou∙!e>
5	10
15	20
10
K
15	20
(c)	Asymptotic bias for a challenging case.	(d) Asymptotic variance for a challenging case.
Figure 4: Comparing asymptotics with empirical values of bias and variance on P = Gamma(1, 1)
using 100,000 independent evaluations: (a)-(b) shows a simple case, P = Gamma(1, 1), and (c)-(d)
shows a challenging case. Observation: (a) both DVI and JVI correct for bias efficiently; (b) DVI and
JVI variance closely match. (c) for a challenging case, the JVI bias is considerably smaller than the
DVI bias; (d) in the challenging case, JVI has a higher variance than both DVI and IWAE.
Notably, in (62) the 1/K term is cancelled exactly by the delta method correction, even though we
used an empirical ratio estimator μ2∕μ2. Subtracting the true mean logp(x) = log E[w] from (62)
yields the bias (54) and completes the proof.	□
Experimental Comparison of DVI and JVI
We perform the experiment shown in Figure 1 including the DVI estimator. The result is shown in
Figure 4 and confirms that DVI reduces bias but that for the challenging case JVI is superior in terms
of bias reduction.
Appendix C: More JVI Details
Complexity Proof
Proof. The first for loop of the algorithm has complexity O(K). The second part of the algorithm
considers all subsets of size K, K - 1, . . . , K - m. In total these are S(K, m) = Pjm=0 KK-j =
Pjm=0 Kj sets. Justin Melvin derived a bound on this partial binomial sum7, as
S(K,m) ≤ em (K) .	(63)
m
For each of the S(K, m) sets we have to perform at most K operations to compute the log-sum-exp
operation, which yields the stated complexity bound.	□
Higher-order Bias Removal Demonstration
We illustrate the behaviour of the higher-order JVI estimators on the same P = Gamma(0.1, 1) ex-
ample we used previously. Figure 5 demonstrates the increasing order of bias removal, O(K-(m+1) )
for the LKm estimators.
7See https://mathoverflow.net/questions/17202/sum- of- the- first- k- binomial- coefficients- for- f
15
Published as a conference paper at ICLR 2018
Figure 5: Absolute bias as a function of K.
GPU Implementation of JVI
To this end let K ≥ 1 and m < K be fixed and assume the log-weights vi are concatenated in one
column vector of K elements. We then construct a matrix B of size (|S|, K), where S is the set of
all subsets that will be considered,
m
S = [ EnumerateSubsets({1, . . . , K}, K - j).	(64)
j=0
There are |S | rows in B and each row in B corresponds to a subset S ∈ S of samples so that we can
use S to index the rows in B . We set
BS,i
|S| Ii∈S,
(65)
where Ipred is one if the predicate is true and zero otherwise. We furthermore construct a vector A
with |S| elements. We set
AS = C(K, m, K -∖S)/(K K|S|) = TKTSIK!(JS-Klm∣S∣)!.	(66)
Using these definitions we can express the estimator as A> log(B exp(v)), with the log and exp
operations being elementwise. However, this is not numerically robust. Instead we can compute
the estimator in the log domain as logsumexp2 (IS ×1 v> + log B) A, where logsumexp2 denotes a
log-sum-exp operation along the second axis. This can be easily implemented in modern neural
network frameworks and we plan to make our implementation available.
16