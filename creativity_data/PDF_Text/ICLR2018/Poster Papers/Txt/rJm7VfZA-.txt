Published as a conference paper at ICLR 2018
Learning Parametric Closed-Loop Policies
for Markov Potential Games
Sergio Valcarcel Macua
PROWLER.io
Cambridge, UK
sergio@prowler.io
Javier Zazo, Santiago Zazo
Information Processing and Telecommunications Center
Universidad Politecnica de Madrid
Madrid, Spain
javier.zazo.ruiz@upm.es
santiago@gaps.ssr.upm.es
Ab stract
Multiagent systems where the agents interact among themselves and with an
stochastic environment can be formalized as stochastic games. We study a subclass
of these games, named Markov potential games (MPGs), that appear often in eco-
nomic and engineering applications when the agents share some common resource.
We consider MPGs with continuous state-action variables, coupled constraints and
nonconvex rewards. Previous analysis followed a variational approach that is only
valid for very simple cases (convex rewards, invertible dynamics, and no coupled
constraints); or considered deterministic dynamics and provided open-loop (OL)
analysis, studying strategies that consist in predefined action sequences, which are
not optimal for stochastic environments. We present a closed-loop (CL) analysis
for MPGs and consider parametric policies that depend on the current state and
where agents adapt to stochastic transitions. We provide easily verifiable, sufficient
and necessary conditions for a stochastic game to be an MPG, even for complex
parametric functions (e.g., deep neural networks); and show that a closed-loop
Nash equilibrium (NE) can be found (or at least approximated) by solving a related
optimal control problem (OCP). This is useful since solving an OCP—which is a
single-objective problem—is usually much simpler than solving the original set
of coupled OCPs that form the game—which is a multiobjective control problem.
This is a considerable improvement over the previously standard approach for the
CL analysis of MPGs, which gives no approximate solution if no NE belongs to
the chosen parametric family, and which is practical only for simple parametric
forms. We illustrate the theoretical contributions with an example by applying our
approach to a noncooperative communications engineering game. We then solve
the game with a deep reinforcement learning algorithm that learns policies that
closely approximates an exact variational NE of the game.
1	Introduction
In a noncooperative stochastic dynamic game, the agents compete in a time-varying environment,
which is characterized by a discrete-time dynamical system equipped with a set of states and a
state-transition probability distribution. Each agent has an instantaneous reward function, which can
be stochastic and depends on agents’ actions and current system state. We consider that both the
state and action sets are subsets of real vector spaces and subject to coupled constraints, as usually
required by engineering applications.
A dynamic game starts at some initial state. Then, the agents take some action and the game moves to
another state and gives some reward values to the agents. This process is repeated at every time step
over a (possibly) infinite time horizon. The aim of each agent is to find the policy that maximizes its
expected long term return given other agents’ policies. Thus, a game can be represented as a set of
coupled optimal-control-problems (OCPs), which are difficult to solve in general.
OCPs are usually analyzed for two cases namely open-loop (OL) or closed-loop (CL), depending on
the information that is available to the agents when making their decisions. In the OL analysis, the
1
Published as a conference paper at ICLR 2018
action is a function of time, so that we find an optimal sequence of actions that will be executed in
order, without feedback after any action. In the CL setting, the action is a mapping from the state,
usually referred as feedback policy or simply policy, so the agent can adapt its actions based on
feedback from the environment (the state transition) at every time step. For deterministic systems,
both OL and CL solutions can be optimal and coincide in value. But for stochastic system, an OL
strategy consisting in a precomputed sequence of actions cannot adapt to the stochastic dynamics so
that it is unlikely to be optimal. Thus, CL are usually preferred over OL solutions.
For dynamic games, the situation is more involved than for OCPs, see, e.g., (Basar and Olsder, 1999).
In an OL dynamic game, agents’ actions are functions of time, so that an OL equilibrium can be
visualized as a set of state-action trajectories. In a CL dynamic game, agents’ actions depend on
the current state variable, so that, at every time step, they have to consider how their opponents
would react to deviations from the equilibrium trajectory that they have followed so far, i.e., a CL
equilibrium might be visualized as a set of trees of state-action trajectories. The sets of OL and CL
equilibria are generally different even for deterministic dynamic games (Kydland, 1975; Fudenberg
and Levine, 1988).The CL analysis of dynamic games with continuous variables is challenging and
has only be addressed for simple cases.
The situation is even more complicated when we consider coupled constraints, since each agent’s
actions must belong to a set that depends on the other agents’ actions. These games, where the agents
interact strategically not only with their rewards but also at the level of the feasible sets, are known as
generalized Nash equilibrium problems (Facchinei and Kanzow, 2010).
There is a class of games, named Markov potential games (MPGs), for which the OL analysis shows
that NE can be found by solving a single OCP; See (Gonzglez-Sgnchez and Herndndez-Lerma, 2013;
Zazo et al., 2016a) for recent surveys on MPGs. Thus, the benefit of MPGs is that solving a single
OCP is generally simpler than solving a set of coupled OCPs. MPGs appear often in economics
and engineering applications, where multiple agents share a common resource (a raw material, a
communication link, a transportation link, an electrical transmission line) or limitations (a common
limit on the total pollution in some area). Nevertheless, to our knowledge, none previous study has
provided a practical method for finding CL Nash equilibrium (CL-NE) for continuous MPGs.
Indeed, to our knowledge, no previous work has proposed a practical method for finding or approxi-
mating CL-NE for any class of Markov games with continuous variables and coupled constraints.
State-of-the-art works on learning CL-NE for general-sum Markov games did not consider coupled
constraints and assumed finite state-action sets (Prasad et al., 2015; P6rolat et al., 2017).
In this work, we extend previous OL analysis due to Zazo et al. (2016b); Valcarcel Macua et al. (2016)
and tackle the CL analysis of MPGs with coupled constraints. We assume that the agents’ policies
lie in a parametric set. This assumption makes derivations simpler, allowing us to prove that, under
some potentiality conditions on the reward functions, a game is an MPG. We also show that, similar
to the OL case, the Nash equilibrium (NE) for the approximate game can be found as an optimal
policy of a related OCP. This is a practical approach for finding or at least approximating NE, since if
the parametric family is expressive enough to represent the complexities of the problem under study,
we can expect that the parametric solution will approximate an equilibrium of the original MPG well
(under mild continuity assumptions, small deviations in the parametric policies should translate to
small perturbations in the value functions). We remark that this parametric policy assumption has
been widely used for learning the solution of single-agent OCPs with continuous state-action sets;
see, e.g., (Konda and Tsitsiklis, 2003; Melo and Lopes, 2008; Powell and Ma, 2011; Van Hasselt,
2012; Lillicrap et al., 2015; Heess et al., 2015; Schulman et al., 2015). Here, we show that the same
idea can be extended to MPGs in a principled manner.
Moreover, once we have formulated the related OCP, we can apply reinforcement learning techniques
to find an optimal solution. Some recent works have applied deep reinforcement learning (DRL) to
cooperative Markov games (Foerster et al., 2017; Sunehag et al., 2017), which are a particular case
of MPGs. Our results show that similar approaches can be used for more general MPGs.
Summary of contributions. We provide sufficient and necessary conditions on the agents’ reward
function for a stochastic game to be an MPG. Then, we show that a closed-loop Nash equilibrium
can be found (or at least approximated) by solving a related optimal control problem (OCP) that is
similar to the MPG but with a single-objective reward function. We provide two ways to obtain the
reward function of this OCP: i) computing the line integral of a vector field composed of the partial
2
Published as a conference paper at ICLR 2018
derivatives of the agents’ reward, which is theoretically appealing since it has the form of a potential
function but difficult to obtain for complex parametric policies; ii) and as a separable term in the
agents’ reward function, which can be obtained easily by inspection for any arbitrary parametric
policy. We illustrate the proposed approach by applying DRL to a noncoooperative Markov game that
models a communications engineering application (in addition, we illustrate the differences with the
previous standard approach by solving a classic resource sharing game analytically in the appendix).
2	Problem Setting for Closed-Loop MPG
LetN , {1, . . . , N} denote the set of agents. Let ak,i be the real vector of length Ak that represents
the action taken by agent k ∈ N at time i, where Ak ⊆ RAk is the set of actions of agent k ∈ N . Let
A , Qk∈N Ak denote the set of actions of all agents that is the Cartesian product of every agent’s
action space, such that A ⊆ RA, where A = Pk∈N Ak . The vector that contains the actions of all
agents at time i is denoted ai ∈ A. Let X ⊆ RS denote the set of states of the game, such that xi is a
real vector of length S that represents the state of the game at time i, with components xi(s):
xi , (xi(s))sS=1 ∈ X.	(1)
Note that the dimensionality of the state set can be different from the number of agents (i.e., S 6= N).
State transitions are determined by a probability distribution over the future state, conditioned on the
current state-action pair: Xi+ι 〜Pχ(∙∣Xi,ai); where We use boldface notation for denoting random
variables. State transitions can be equivalently expressed as a function, f : X × A × Θ → X, that
depends on some random variable θi ∈ Θ, with distribution pθ(∙∣Xi, ai), such that
xi+1 = f(xi, ai, θi).	(2)
We include a vector of C constraint functions, g , (gc)cC=1, where gc : X × A 7→ R; and define the
constraint sets for i = 0: C0 , A ∩ {a0 : g(x0, a0) ≤ 0}; and for i = 0, . . . , ∞: Ci , {X ∩ {xi :
xi = f(xi-1, ai-1, θi-1)}} × A ∩ {(xi, ai) : g(xi, ai) ≤ 0}, which determine the feasible states
and actions. The instantaneous reward of each agent, rk,i, is also a random variable conditioned on
the current state-action pair: rk,i 〜p% (∙∣Xi, ai). Given random variable σk,i ∈ ∑k with distribution
Pσk (∙∖xi, ai), we define reward function rk : X X A X ∑k → R for every agent k ∈ N:
rk,i = rk(xi, ai, σk,i).	(3)
We assume that θi and σk,i are independent of each other and of any other θj and σk,j, at every time
step j 6= i, given xi and ai .
Let πk : X → Ak and π : X → A denote the policy for agent k and all agents, respectively, such that:
ak,i = πk(xi), ai = π(xi), and π , (πk)k∈N .	(4)
Let Ωk and Ω = Qk∈N Ωk denote the policy spaces for agent k and for all agents, respectively, such
that ∏k ∈ Ωk and ∏ ∈ Ω. Note that Ω(X) = A. Introduce also ∏-k : X → A-k as the policy of all
agents except that of agent k. Then, by slightly abusing notation, we write: π = (πk, π-k), ∀k ∈ N.
The general (i.e., nonparametric) stochastic game with Markov dynamics consists in a multiobjective
variational problem with design space Ω and objective space RN, where each agent aims to find a
stationary policy that maximizes its expected discounted cumulative reward, for which the vector of
constraints, g, is satisfied almost surely:
∞
maximize E	γirk (xi, πk(xi), π-k(xi), σi)
π k ∈ iΩ k
G1 : ∀k ∈ N	i=0	(5)
s.t. xi+1 = f (xi, π(xi), θi),
g(xi, π(xi)) ≤ 0.
Similar to static games, since there might not exist a policy that maximizes every agent’s objective,
we will rely on Nash equilibrium (NE) as solution concept. But rather than trying to find a variational
NE solution for (5), we propose a more tractable approximate game by constraining the policies to
belong to some finite-dimensional parametric family.
Introduce the set of parametric policies, Ωw, as a finite-dimensional function space with parameter
W ∈ W ⊆ RW: Ωw，{π(∙, w) : W ∈ W}. Note that for a given w, the parametric policy is still
3
Published as a conference paper at ICLR 2018
a mapping from states to actions: ∏(∙, w) : X → A. Let Wk ∈ Wk ⊆ RWk denote the parameter
vector of length Wk for the parametrized policy πk, so that it lies in the finite-dimensional space
ΩW，{∏k(∙,Wk) : Wk ∈ Wk}, such that Ωw，Qk∈N Ω，W，Qk∈N Wk, W，Pk∈N Wk, and
W , (wk)k∈N, π(∙,w) , (πk(∙,wk))k∈N .	⑹
Let W-k denote the parameters of all agents except that of agent k, so that we can also write:
W = (Wk,W-k), ∏(∙,w) = ∏(∙, (wk,w-k)) = (∏k(∙,Wk),∏-k(∙,W-k)) .	(7)
In addition, we use Wk (`) to denote the `-th component of Wk, such that Wk , (Wk ('))Wkι∙
By constraining the policy of Gi to lie in Ωf, We obtain a multiobjective optimization problem with
design space W:
G2	:
∀k ∈ N
maximize
wk∈Wk
s.t.
∞
E γ rk (xi , πk (xi, Wk ), π-k (xi , W-k ), σk,i )
i=0
xi+1 = f (xi, π(xi, (Wk, W-k)), θi),
g(xi, π(xi, (Wk, W-k))) ≤ 0.
(8)
The solution concept in which we are interested is the parametric closed-loop Nash equilibrium (PCL-
NE), which consists in a parametric policy for which no agent has incentive to deviate unilaterally.
Definition 1 A parametric closed-loop Nash equilibrium (PCL-NE) of G2 is a vector W
Wk?, W-? k ∈ RW that satisfies:
∞
E	γirk xi,πk(xi,Wk?), π-k(xi, W-? k), σk,i
i=0
∞
≥ E	γirk xi, πk(xi, Wk), π-k(xi,W-? k), σk,i	, ∀k ∈ N, ∀x0 = x0 ∈ X,
i=0
∀Wk ∈	Wk	∈	Wk	:	ai	=	πk(xi, Wk), π-k(xi, W-? k)	∈	C0,	(xi,	ai)	∈	Ci, i =	1,	. . . , ∞ . (9)
Since G2 is similar to G1 but with an extra constraint on the policy set, loosely speaking, we can see
a PCL-NE as a projection of some NE of G1 onto the manifold spanned by parametric family of
choice. Hence, if the parametric family has arbitrary expressive capacity (e.g., a neural network with
enough neurons in the hidden layers), we can expect that the resulting PCL-NE evaluated on G1 will
approximate arbitrarily close the performance of an exact variational equilibrium.
We consider the following general assumptions.
Assumption 1 The state and parameter sets, X and W, are nonempty and convex.
Assumption 2 The reward functions rk are twice continuously differentiable in X × W, ∀k ∈ N.
Assumption 3 The state-transition function, f, and constraints, g, are continuously differentiable in
X × W, and satisfy some regularity conditions (e.g., Mangasarian-Fromovitz).
Assumption 4 The reward functions rk are proper, and there exists a scalar B such that the level
sets {a0 ∈ C0, (xi, ai) ∈ Ci : E [rk (xi, ai, σk,i)] ≥ B}i∞=0 are nonempty and bounded ∀k ∈ N.
Assumptions 1-2 usually hold in engineering applications. Assumption 3 ensures the existence of
feasible dual variables, which is required for establishing the optimality conditions. Assumption 4
will allow us to ensure the existence of PCL-NE. We say that rk is proper if: i) E [rk(xi, ai, σk)] >
-∞ for at least one (xi, ai) ∈ Ci, and ii) E [rk (xi, ai, σk,i)] < ∞, ∀a0 ∈ C0, ∀(xi, ai) ∈ Ci
(i = 1, . . . , ∞).
4
Published as a conference paper at ICLR 2018
3 Standard Approach to Closed-Loop Markov games
In this section, We review the standard approach for tackling CL dynamic games (Gonzalez-Sanchez
and Herndndez-Lerma, 2013). For simplicity, we consider deterministic game and no constraints:
∞
maximize	γirk (xi, πk(xi), π-k(xi))
Gstd : ∀k ∈ N	πk∈Qk	i=0	(10)
s.t. xi+1 = f (xi, π(xi)).
First, it inverts f to express the policy in reduced form, i.e., as a function of current and future states:
π(xi) = h(xi, xi+1).
(11)
This implicitly assumes that such function h : X × X → A exists, which might not be the case if f is
not invertible. Next, πk is replaced with (11) in each rk:
rk (xi, πk (xi), π-k (xi)) = rk (xi, h(xi, xi+1)) , rk0 (xi,xi+1) ,	(12)
where rk0 : X × X → R is the reward in reduced-form. Then, the Euler equation (EE) and transversality
condition (TC) are obtained from rk0 for all k ∈ N and used as necessary optimality conditions:
Nxirk (xi-ι,xi) + Nxirk (Xi,x∙∏ι) = 0	(EE),	(13)
lim x>Nxirk (xi-ι,Xi) = 0	(TC).	(14)
i→∞
When rk0 are concave for all agents, and X ⊆ R+ (i.e., X = {xi : xi ≥ 0, xi ∈ RS}), these optimality
conditions become sufficient for Nash equilibrium (Gonzalez-Sanchez and Hernandez-Lerma, 2013,
Theorem 4.1). Thus, the standard approach consists in guessing parametric policies from the space
of functions Ω, and check whether any of these functions satisfies the optimality conditions. We
illustrate this procedure with a well known resource-sharing game named “the great fish war” due to
Levhari and Mirman (1980), with Example 1 in Appendix A.
Although the standard approach sketched above (see also Appendix A) has been the state-of-the-art
for the analysis of CL dynamic games, it has some drawbacks: i) The reduced form might not exist;
ii) constraints are not handled easily and we have to rely in ad hoc arguments for ensuring feasibility;
iii) finding a specific parametric form that satisfies the optimality conditions can be extremely difficult
since the space of functions is too large; and iv) the rewards have to be concave for all agents in order
to guarantee that any policy that satisfies the conditions is an equilibrium.
In order to overcome these issues, we propose to first constrain the set of policies to some parametric
family, and then derive the optimality conditions for this parametric problem; as opposed to the
standard approach that first derives the optimality conditions of G1 , and then guesses a parametric
form that satisfies them. Based on this insight, we will introduce MPG with parametric policies as a
class of games that can be solved with standard DRL techniques by finding the solution of a related
(single-objective) OCP. We explain the details in the following section.
4 Closed-Loop Markov Potential Games
In this section, we extend the OL analysis of Zazo et al. (2016a) to the CL case. We define MPGs with
CL information structure; introduce a parametric OCP; provide verifiable conditions for a parametric
approximate game to be an MPG in the CL setting; show that when the game is an MPG, we can find
a PCL-NE by solving the parametric OCP with a specific objective function; and provide a practical
method for obtaining such objective function.
First, we define MPGs with CL information structure and parametric policies as follows.
Definition 2 Given a policy family ∏(∙,w) ∈ Ωw, game (8) is an MPG if and only if there is a
function J : X × W × Σ → R, named the potential, that satisfies the following condition ∀k ∈ N:
∞
E	γi (rk(xi, πk(xi, wk), π-k(xi, w-k), σk,i) - rk(xi, πk(xi, vk), π-k(xi, w-k), σk,i))
i=0
∞
= E γi J(xi, πk(xi, wk), π-k(xi, w-k), σi) - J(xi, πk(xi, vk), π-k(xi, w-k), σi)	,
i=0
∀xi ∈ X, ∀wk, vk ∈ Wk .	(15)
5
Published as a conference paper at ICLR 2018
Definition 2 means that there exists some potential function, J, shared by all agents, such that if some
agent k changes its policy unilaterally, the change in its reward, rk, equals the change in J.
The main contribution of this paper is to show that when (8) is a MPG, we can find one PCL-NE by
solving a related parametric OCP. The generic form of such parametric OCP is as follows:
∞
maximize E	γiJ(xi, π(xi, w), σi)
w∈W
P1 :	i=0	(16)
s.t. xi+1 = f (xi, π(xi, w), θi),
g(xi, π(xi, w)) ≤ 0.
where we replaced the multiple objectives (one per agent) with the potential J as single objective.
This is convenient since solving a single objective OCP is generally much easier than solving the
Markov game. However, we still have to find out how to obtain J. The following Theorem formalizes
the relationship between G2 and P1 and shows one way to obtain J (proof in Appendix C).
Theorem 1 LetAssumptions 1-4 hold. Let the reward functions satisfy thefollowing ∀k, j ∈ N:
E [Vwj [Vχirk(xi,π(xi,w), σk,i)]∖ = E [Vwk [VχirjE,π(xi,w), σj,i)]],	(17)
E [Vχi [Vχir (xi,∏(xi,w),σk,i)]] = E [Vχi [Vχjj(xi,∏(xi,w), σj,i)]],	(18)
E Vwj [Vwk rk(xi, π(xi, w), σk,i)] = E Vwk Vwj rj (xi, π(xi, w), σj,i)	,	(19)
where the expected value is taken component-wise. Then, game (8) is an MPG that has a PCL-NE
equal to the solution of OCP (16). The potential J that is the instantaneous reward for the OCP is
given by line integral:
J(xi, π(xi, w), σi)
Z1X	XS
0 k∈N	m=1
∂rk (η(z),∏k(η(z),wk),∏-k((η(z),w-k),σk,i') dqm(z)
∂xi(m)	dz
Ak
+X
'=1
∂rk (xi, ∏k(xi, ξk (z)), ∏-k(xi, w-k), σk,i) dξk,'(z) ʌ ”2
∂ak,i(')	dz )
(20)
where η(z) , (ηk (z))Sm=1 and ξ(z) , (ξk (z))k∈N are piecewise smooth paths in X and W, respec-
tively, with components ξk(z)，(ξk,'(z))Wr Such that the initial and final state-action conditions
are given by (η(0), ξ(0)) and (η(1) = xi, ξ(1) = w).
From (20), we can see that J is obtained through the line integral of a vector field with components
the partial derivatives of the agents’ rewards (see Appendix C), and so the name potential function.
Note also that Theorem 1 proves that any solution to P1 is also a PCL-NE of G2, but we remark that
there may be more equilibria of the game that are not solutions to P1 (see Appendix C).
The usefulness of Theorem 1 is that, once we have the potential function, we can formulate and solve
the related OCP for any specific parametric policy family. This is a considerable improvement over
the standard approach. On one hand, if the chosen parametric policy contains the optimal solution,
then we will obtain the same equilibrium as the standard approach. On the other hand, if the chosen
parametric family does not have the optimal solution, the standard approach will fail, while our
approach will always provide a solution that is an approximation (a projection over Ωw) of an exact
variational equilibrium. Moreover, as mentioned above, we can expect that the more expressive the
parametric family, the more accurate the approximation to the variational equilibrium. In Appendix
B, we show how to to solve “the great fish war” game with the proposed framework, yielding the
same solution as with the standard approach, with no loss of accuracy.
Although expressing J as a line integral of a field is theoretically appealing, if the parametric family
is involved—as it is usually the case for expressive policies like deep neural-networks—then (20)
might be difficult to evaluate. The following results show how to obtain J easily by visual inspection.
First, the following corollary follows trivially from (17)-(19) and shows that cooperative games,
where all agents have the same reward, are MPGs, and the potential equals the reward:
6
Published as a conference paper at ICLR 2018
Corollary 1 Cooperative games, where all agents have a common reward, such that
rk (xi, π(xi, w), σk,i) = J(xi, π(xi, w), σi), ∀k ∈ N ,	(21)
are MPGs; and the potential function (20) equals the common reward function in (21).
Second, we address noncooperative games, and show that the potential can be found by inspection as
a separable term that is common to all agents’ reward functions. Interestingly, we will also show that
a game is an MPG in the CL setting if and only if all agents’ policies depend on disjoint subsets of
components of the state vector. More formally, introduce Xkπ as the set of state vector components
that influence the policy of agent k and introduce a new state vector, xkπ, and let xπ-k,i be the vector
of components that do not influence the policy of agent k:
x∏,i , (xi(m))m∈χπ , x-k,i , (Xi(I))l∈Xn .	(22)
In addition, introduce Xkr as the set of components of the state vector that influence the reward of
agent k directly (not indirectly through any other agent’s policy), and define the state vectors:
xk,i , (xi(m))m∈χkr , x-k,i , (Xi(I))l∈X% .	(23)
Introduce also the union of these two subsets, XkΘ = Xkπ ∪ Xkr, and its corresponding vectors:
xθi , (xi(m))m∈χΘ , x-k,i , (xi(m))m∕χθ .	(24)
Then, the following theorem allows us to obtain the potential function (proof in Appendix D).
Theorem 2 Let Assumptions 1-4 hold. Then, game (8) is an MPG if and only if: i) the reward
function of every agent can be expressed as the sum of a term common to all agents plus another
term that depends neither on its own state-component vector, nor on its policy parameter:
rk Xrk,i, π(Xkπ,i, wk), π(Xπ-k,i, w-k), σk,i = J (Xi, π(Xi, w), σi)
+ Θk Xr-k,i, π(Xπ-k,i, w-k), σi , ∀k ∈ N ; (25)
and ii) the following condition on the non-common term holds:
E hVχθ,i θk (x-k,i, π(X-k,i, w-k ), σi )i = 0.	(26)
Moreover, if (26) holds, then the common term in (25), J, equals the potential function (20).
Note that (26) holds in the following cases: i) when Θk = 0, as the cooperative case described
in Corollary 1; ii) when Θk does not depend on the state but only on the parameter vector, i.e.,
Θk : Qj∈N,j6=k Wj 7→ R, as in “the great fish war” example described in Appendix B; or iii) when
all agents have disjoint state-component subsets, i.e., Xf ∩ Xjθ = 0, ∀(k, j) ∈ {N XN : k = j}.
An interesting insight from Theorem 2 is that a dynamic game that is potential when it is analyzed in
the OL case (i.e., the policy is a predefined sequence of actions), might not be potential when analyzed
in the CL parametric setting. This conclusion is straightforward since the potentiality condition in the
OL case provided by (Valcarcel Macua et al., 2016, Cor. 1) is equal to (25), without requiring (26).
In order to apply Theorems 1 and 2, we are implicitly assuming that there exists solution to the OCP.
We finish this section, by showing that this is actually the case in our setting (proof in Appendix E).
Proposition 1 Under Assumption 4, OCP (16) has nonempty solution set.
In other words, Prop. 1 shows that there exists a deterministic policy that achieves the optimal value
of Pi, which is also an NE of G2 if conditions (17)-(19) or equivalently (25)-(26) hold. We remark
that there might be many other—possibly stochastic—policies that are also NE of the game.
5	Experiment
In this section, we show how to use the proposed MPGs framework to learn an equilibrium of a
communications engineering application. We extend the Medium Access Control (MAC) game
presented in (Zazo et al., 2016a) to stochastic dynamics and rewards (where previous OL solutions
7
Published as a conference paper at ICLR 2018
would fail), and use the Trust Region Policy Optimization (TRPO) algorithm (Schulman et al., 2015),
which is a reliable reinforcement learning method policy search method that approximates the policy
with a deep-neural network, to learn a policy that is a PCL-NE of the game.
We consider a MAC uplink scenario with N = 4 agents, where each agent is a user that sets its
transmitter power aiming to maximize its data rate and battery lifespan. If multiple users transmit
at the same time, they will interfere with each other and decrease their rate, using their batteries
inefficiently, so that they have to find an equilibrium. Let xk,i ∈ [0, Bk,max] , Xk denote the
battery level for each agent k ∈ N , which is discharged proportionally to the transmitted power,
Let ak,i ∈ [0, Pk,max] , Ak be the transmitted power for the k-th user, where constants Pk,max and
Bk,max stand for the maximum allowed transmitter power and battery level, respectively. The system
state is the vector with all user’s battery levels: xi = (xk,i)k∈N ∈ X; such that S = N and all
state vector components are unshared, i.e., X = Qk∈N Xk ⊂ RN, and Xk = {k}. We remark that
although each agent’s battery depletion level depends directly on its action and its previous battery
level only, it also depends indirectly on the strategies and battery levels of the rest of agents. The
game can be formalized as follows:
Gmac :
∀k ∈ N
maximize
wk ∈A
s.t.
W / A .	∣hk,i∣2 ∏(xk,i,Wk)	∖	ʌ
TY log 1+ 一 L	―k 2f …+ αxk,i
i=0	∖	∖	1 + ∑j∈N j=k |hj | π(Xj,i,wj))	(	(27)
xk,i+1 = xk,i - δiπ(xk,i, wk ),	xk,0 = Bk,max
0 ≤ π(xk,i , wk ) ≤ Pk,max, 0 ≤ xk,i ≤ Bk,max, i = 0, . . . , ∞
where hk is the random fading channel coefficient for user k, α is the weight for the battery reward
term, and δ is the discharging factor.
First of all, note that each agent’s policy and reward depend only on its own battery level, xk,i .
Therefore, we can apply Theorem 2 and establish that the game is a MPG, with potential function:
J(xi,∏(xi,w)) =log 1+ £ ∣hk,i∣2∏(xk,i,Wk) + α £ Xk,i
k∈N	k∈N
Thus, we can formulate OCP (16) with single objective given by (28).
28)
Since the battery level is a positive term in the reward, the optimal policy will make the battery deplete
in finite time (formal argument can be derived from transversality condition (54)). Moreover, since
δk,i ≥ 0, the episode gets into a stationary (i.e., terminal) state once the battery has been depleted. We
have chosen the reward to be convex. The reason is that in order to compute a benchmark solution, we
can solve the finite time-horizon convex OCP exactly with a convex optimization solver, e.g., CVX
(Grant and Boyd, 2014), and use the result as a baseline for comparing with the solution learned by a
DRL algorithm. Nevertheless, standard solvers do not allow to include random variables. To surmount
this issue, we generated 100 independent sequences of samples of hk,i and δk,i for all k ∈ N and
length T = 100 time steps each, and obtain two solutions with them. We set |hk,i |2 = |hk|2 vk,i,
where vk,i is uniform in [0.5, 1], |h1 |2 = 2.019, |h2 |2 = 1.002, |h3|2 = 0.514 and |h4 |2 = 0.308;
and δk,i is uniform in [0.7, 1.3]. The first solution is obtained by averaging the sequences, and
building a deterministic convex problem with the average sequence, which yielded an optimal value
Vc?vx = 33.19. We consider Vc?vx to be an estimator of the optimal value of the stochastic OCP. The
second solution is obtained by building 100 deterministic problems, solving them, and averaging
their optimal values, which yielded an optimal value Va?vg,cvx = 34.90. We consider Va?vg,cvx to be an
upper bound estimate of the optimal value of the stochastic OCP (Jensen’s inequality). The batteries
depleted at a level xT < 10-6 in all cases, concluding that time horizon of T = 100 steps is valid.
We remark that these benchmark solutions required complete knowledge of the game.
When we have no prior knowledge of the dynamics and rewards, the proposed approach allows as to
learn a PCL-NE of (27) by using any DRL method that is suitable for continuous state and actions,
like TRPO (Schulman et al., 2015), DDPG (Lillicrap et al., 2015) or A3C (Mnih et al., 2016). DRL
methods learn by interacting with a black-box simulator, such that at every time step i, agents observe
state xi, take action ai = πw(xi) and observe the new stochastic battery levels and reward values,
with no prior knowledge of the reward or state-dynamic functions.
As a proof of concept, we perform simulations with TRPO, approximating the policy with a neural
network with 3 hidden layers of size 32 neurons per layer and RELU activation function, and an
8
Published as a conference paper at ICLR 2018
Figure 1: Results for the MAC game (27) obtained with TRPO and the averaged solutions given by
the convex optimization solver).
output layer that is the mean of a Gaussian distribution. Each iteration of TRPO uses a batch of
size 4000 simulation steps (i.e., tuples of state transition, action and rewards). The step-size is 0.01.
Figure 1 shows the results. After 400 iterations, TRPO achieves an optimal value Vt?rpo = 32.34,
which is 97.44% of Vc?vx, and 92.7% of the upper bound Va?vg,cvx.
6	Conclusions
We have extended previous results on MPGs with constrained continuous state-action spaces providing
practical conditions and a detailed analysis of Nash equilibrium with parametric policies, showing
that a PCL-NE can be found by solving a related OCP. Having established a relationship between a
MPG and an OCP is a significant step for finding an NE, since we can apply standard optimal control
and reinforcement learning techniques. We illustrated the theoretical results by applying TRPO (a
well known DRL method) to an example engineering application, obtaining a PCL-NE that yields
near optimal results, very close to an exact variational equilibrium.
7	Acknowledgements
We thank David Mguni, Enrique Munoz de Cote, and Haitham Bou-Ammar for insightful discussions.
This work was partially supported by the Spanish Ministry of Science and Innovation under the grant
TEC2016-76038-C3-1-R (HERAKLES) and the COMONSENS Network of Excellence TEC2015-
69648-REDC.
References
T. Apostol. Calculus: Multi-variable Calculus and Linear Algebra, with Applications to Differential
Equations and Probability. Wiley, 1969.
T. Basar and G. J. Olsder. Dynamic Noncooperative Game Theory. Society for Industrial and Applied
Mathematics, 1999.
D. P. Bertsekas. Dynamic Programming and Optimal Control, volume 2. Athena Scientific, 3rd
edition, 2007.
F. Facchinei and C. Kanzow. Generalized nash equilibrium problems. Annals of Operations Research,
175(1):177-211,2010.
J. N. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policy gradients. arXiv preprint 1705.08926, 2017.
D. Fudenberg and D. K. Levine. Open-loop and closed-loop equilibria in dynamic games with many
players. Journal of Economic Theory, 44(1):1-18, 1988.
D. Gonzdlez-Sdnchez and O. Herndndez-Lerma. Discrete-Time Stochastic Control and Dynamic
Potential Games: The Euler-Equation Approach. Springer, 2013.
9
Published as a conference paper at ICLR 2018
M.	Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 2.1.
http://cvxr.com/cvx, Mar. 2014.
N.	Heess, G. Wayne, D. Silver, T. Lillicrap, T. Erez, and Y. Tassa. Learning continuous control
policies by stochastic value gradients. In Advances in Neural Information Processing Systems 28
(NIPS), pages 2926-2934. 2015.
V.	R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimiza-
tion, 42(4):1143-1166, Apr. 2003.
F. Kydland. Noncooperative and dominant player solutions in discrete dynamic games. International
Economic Review, pages 321-335, 1975.
D. Levhari and L. J. Mirman. The great fish war: An example using a dynamic Cournot-Nash solution.
The Bell Journal of Economics, 11(1):322-334, 1980.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. arXiv preprint 1509.02971v1, 2015.
F.	S. Melo and M. Lopes. Fitted natural actor-critic: A new algorithm for continuous state-action
MDPs. In Machine Learning and Knowledge Discovery in Databases, volume 5212, pages 66-81.
Springer, 2008.
V.	Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proc. Int. Conf. on Machine Learning
(ICML), pages 1928-1937, 2016.
J. Pero山t, F. Strub, B. Piot, and O. PietqUin. Learning nash equilibrium for general-sum markov
games from batch data. In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS),
pages 232-241, 2017.
W.	B. Powell and J. Ma. A review of stochastic algorithms with continuous value function approxi-
mation and some new approximate policy iteration algorithms for multidimensional continuous
applications. Journal of Control Theory and Applications, 9(3):336-352, 2011.
H.	Prasad, P. LA, and S. Bhatnagar. Two-timescale algorithms for learning nash equilibria in general-
sum stochastic games. In Proc. Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS),
pages 1371-1379, 2015.
A. P. Sage and C. C. White. Optimum Systems Control. Prentice-Hall, 2nd ed. edition, 1977.
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. arXiv preprint 1506.02438, 2015.
G. Scutari, F. Facchinei, J. Pang, and D. Palomar. Real and complex monotone communication games.
IEEE Transactions on Information Theory, 60(7):4197-4231, 2014.
P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. F. Zambaldi, M. Jaderberg, M. Lanctot,
N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for cooperative
multi-agent learning. CoRR, abs/1706.05296, 2017.
S. Valcarcel Macua, S. Zazo, and J. Zazo. Learning in constrained stochastic dynamic potential
games. In IEEE Int. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
4568-4572, 2016.
H. Van Hasselt. Reinforcement learning in continuous state and action spaces. In Reinforcement
Learning, pages 207-251. Springer, 2012.
S. Zazo, S. V. Macua, M. Sanchez-Fernandez, and J. Zazo. Dynamic potential games with constraints:
Fundamentals and applications in communications. IEEE Transactions on Signal Processing, 64
(14):3806-3821, July 2016a.
S. Zazo, S. Valcarcel Macua, M. Sanchez-Fernandez, and J. Zazo. Dynamic potential games with
constraints: Fundamentals and applications in communications. IEEE Transactions on Signal
Processing, 64(14):3806-3821, 2016b.
10
Published as a conference paper at ICLR 2018
A Example： THE “Great FISH war” GAME - Standard Approach
Let us illustrate the standard approach described in Section 3 with a well known resource-sharing
game named “the great fish war" due to Levhari and Mirman (1980). We follow (Gonzglez-Sgnchez
and Herngndez-Lerma, 2013, Sec. 4.2).
Example 1.	Let xi be the stock of fish at time i, in some fishing area. Suppose there are N countries
obtaining reward from fish consumption, so that they aim to solve the following game:
maximize
Gfish ：…"
∀k ∈ N
s.t.
∞
γi log (πk(xi))
i=0
xi+1 = xi -	πk (xi)
k∈N
(29)
xi ≥ 0, πk(xi) ≥ 0, i = 0, . . . , ∞,
where x0 ≥ 0 and 0 < α < 1 are given.
In order to solve Gfish, let us express each agent’s action as:
∏k (Xi) = Xi - x1+l - E	∏j (Xi),
j∈N :j 6=k
(30)
so that the rewards can be also expressed in reduced form, as required by the standard-approach:
rk(Xi) = log I Xi-X1+α - E πj(Xi) I.
j∈N :j6=k
Thus, the Euler equations for every agent k ∈ N and all t = 0, . . . , ∞ become:
___________-XgL-Ia______________+ Y 1 - Pj∈Nj=k dπjIxi)Idxi
Xi-I - X1/a - Pj∈N：j=k πj (Xi-I)	Xi - X1+l - Pj∈N：j=k ∏ (Xi)
(31)
(32)
Now, the standard method consists in guessing a family of parametric functions that replaces the
policy, and checking whether such parametric policy satisfies (32) for some parameter vector. Let us
try with policies that are linear mappings of the state:
πk (Xi) = wkXi.
(33)
By replacing (33) in (32), we obtain the following set of equations:
αγ 1 + wk - X wj	= 1 - X wj , ∀k ∈ N .	(34)
j∈N	j∈N
Fortunately, it turns out that (34) has solution (which might not be the case for other policy parametriza-
tion), with parameters given by:
wk =-----J-：Y-------T,	∀k ∈ N.	(35)
αγ + N(1 - αγ)
Since 0 < α < 1 and 0 ≤ γ < 1, it is apparent that wk > 0 and the constraint πk(Xi) ≥ 0 holds
for all Xi ≥ 0. Moreover, since Pk∈N wk < 1, we have that Xi+1 ≥ 0 for any X0 ≥ 0. In addition,
since Xi is a resource and the actions must be nonnegative, it follows that limi→∞ Xi = 0 (there is no
reason to save some resource). Therefore, the transversality condition holds. Since the rewards are
concave, the states are non-negative and the linear policies with these coefficients satisfy the Euler
and transversality equations, we conclude that they constitute an equilibrium (Gonzglez-Sgnchez and
Herngndez-Lerma, 2013, Theorem 4.1).	4
B	EXAMPLe： “Great Fish War” Game — Proposed Approach
In this section, we illustrate how to apply the proposed approach with the same “the great fish war”
example, obtaining the same results as with the standard approach.
Example 2.	consider “the great fish war” game described in Example 1. In order to use our
approach, we replace the generic policy with the specific policy mapping of our preference. We
11
Published as a conference paper at ICLR 2018
choose the linear mapping, πk (xi) = wkxi, to be able to compare the results with those obtained
with the standard approach. Thus, we have the following game:
maximize
wk ∈Wk
Gfish,w :
∀k ∈ N
s.t.
∞
γi log (wkxi)
i=0
xi+1 = xi -	wkxi	,
k∈N
xi ≥ 0,	wkxi ≥ 0, i = 0, . . . , ∞.
(36)
Let Us verify conditions (67)-(68). For all k,j ∈ N We have:
rk (xi, π(xi, w)) = log(wkxi) ,	(37)
RxiYk (xi,∏(xi,w)) = 1/xi,	(38)
▽wkIrk (xi,∏(xi,w) = 1/wk,	(39)
Rxi Rxi rk (xi, π(xi,w))= Rxi Rxi rj (xi,π(xi,w))= -1/xi2,	(40)
▽wj ▽xi rk (xi, π(xi,w))= ▽wk ▽xi rj (xi, π(xi, w))= 0,	(41)
▽wj ▽wk rk (xi, π(xi,w))= ▽wk ▽wj rj (xi, π(xi,w))= 0.	(42)
Since conditions (67)-(68) hold, We conclUde that (36) is an MPG. By applying the line integral (20),
We obtain:
J(xi, wi)= log(xi)+	log (wk).
k∈N
(43)
NoW, We can solve OCP (16) With potential fUnction (43). For this particUlar problem, it is easy to
solve the KKT system in closed form. IntrodUce a shorthand:
W，Ewk.	(44)
k∈N
The EUler-Lagrange eqUation (62) for this problem becomes:
Yi + βiαχα (1 — w)α — βi-iχ = 0.	(45)
The optimality condition (64) With respect to the policy parameter becomes:
Yi — βiαχα (1 — w)α-1 wk = O.
Let Us solve for βi in (46):
Yi
βi =--------------二1----.
αχα (1 — w)a~ wk
(46)
(47)
Replacing (47) and the state-transition dynamics in (45), We obtain the folloWing set of eqUations:
αγ (1 + wk — w) = 1 — w, ∀k ∈ N.
Hence, the parameters can be obtained as:
wk = -Jn；-----T, ∀k ∈ N.
αY + N(1 — αY)
(48)
(49)
This is exactly the same solUtion that We obtained in Example 1 With the standard approach. We
remark that for the standard approach, We Were able to obtain the policy parameters since We pUt the
correct parametric form of the policy in the EUler eqUation. If We had Used another parametric family
WithoUt a linear term, the EUler eqUations (32) might have no solUtion and We WoUld have got stUck.
In contrast, With oUr approach, We coUld freely choose any other form of the parametric policy, and
alWays solve the KKT system of the approximate game. Broadly speaking, We can say that the more
expressive the parametric family, the more likely that the optimal policy of the original game Will be
accUrately approximated by the optimal solUtion of the approximate game.	4
12
Published as a conference paper at ICLR 2018
C Proof of Theorem 1
Proof: The proof mimics the OL analysis from Zazo et al. (2016a). Let us build the KKT systems
for the game and the OCP with parametric policies. For game (8), each agent’s Lagrangian is given
∀k ∈ N by
Lk(x0ι∞,w, σk,0i∞,θ0ι∞, λk,0ι∞, μk,0ι∞) = E
∞
X γi	rk (Xi, π(Xi, w), σk,i)
i=0
+ λ>,i (f (Xi, ∏(Xi, w), θi) - Xi+ι) + μ>,i g (Xi, ∏(Xi, W))
(50)
where λk,i，(λk,s,i)S=ι ∈ RS and μk,i，(μk,c,i)C=ι ∈ RC are the vectors of multipliers at time i
(which are random since they depend on θi and Xi), and we introduced:
∞∞	∞	∞
x0：8 — (Xi)i=0 , a0：8 — (ai)i=0 , λk,0=∞ — (λk,i)i=0 , μkQ∞ — (μk,i)i=o .
Introduce a shorthand for the instantaneous Lagrangian of agent k:
Φk (xi, Xi+ι, w, σk,i,θi, λk,i, μk,i)，E Irk(Xi, ∏(xi, w), σk,i)
+ λk,i(f (Xi, π(xi, w), θi) - xi+1) + μ>,i g(Xi, π(xi, w)) ].
(51)
(52)
The discrete time stochastic Euler-Lagrange equations applied to each agent’s Lagrangian are different
from the OL case studied in Zazo et al. (2016a) (see also (Sage and White, 1977, Sec. 6.1)), since we
only take into account the variation with respect to the state:
E[VχiΦk (xi, Xi+1,W, σk,i, θi, λk,i,μk,i)]
+Vxi Φk (Xi-1,Xi,w, σk,i-1, θi-1, λk,i-1, μk,i-1) ] = 0S, i = 1,..∙, ∞,
where 0S denotes the vector of length S. The transversality condition is given by
lim E[Xk>,iVxk iφk (xi, xi+1, w, σk,i, θi, λk,i, μk,i) ] = 0S.
i→∞	,
In addition, we have an optimality condition for the policy parameter Wk :
E [V Wk φ k (Xi, xi+1,w, σk,i, θi, λk,i, μk,i ) ] = 0Wk .
(53)
(54)
(55)
From these first-order optimality conditions, we obtain the KKT system for every agent k ∈ N and
all time steps i = 1, . . . , ∞:
ElVxi [rk (xi,∏(xi,w),σk,i) + λk,i f (xi,π(xi,w) θi)]]
+ Vxi [μ>,i g (χi,π(xi,w))] — λk,i-ι = 0Sk,
lim E Xk>,iVxi [rk (Xi, π(Xi, W), σk,i) + λk>,i f (Xi,π(Xi, W), θi) ]
+ Vxi [μ>,i g (χi,π(xi,w))] =0Sk,
(56)
(57)
E [Vwk [rk (xi,π(xi,w),σk,i) + λk,i f (xi,π(xi,w) θi)]]
+ Vwk [μ>,i g (Xi, π(Xi, W))] = 0
Wk ,
Xi+1 = f (xi, π(xi, w), θi) ,	g (xi, π(xi, w)) ≤ 0C,
μk,i ≤ 0c,	μ>,i g (χi, ∏(χi, W)) = 0,
where λk,i-1 is considered deterministic since it is known at time i.
(58)
(59)
(60)
Now, we derive the KKT system of optimality conditions for the OCP (16). The Lagrangian for (16)
is given by:
L	(x0ι∞ ,w,σk,0ι∞, θ0ι∞, β0ι∞, δ0ι∞) = E
∞
X γi J (Xi, π(Xi, w), σi)
i=0
+ βi> (f (Xi, π(xi, w), θi) - Xi+1) + δi>g (Xi, π(xi, w))
(61)
13
Published as a conference paper at ICLR 2018
where βi , (βk,s,i)sS=1 ∈ RS and δi , (δk,c,i)cC=1 ∈ RC are the corresponding multipliers, which
are random variables since they depend on θi and xi . By taking the discrete time stochastic Euler-
Lagrange equations and the optimality condition with respect to the policy parameter for the OCP,
we obtain are a KKT system for the OCP: i = 1, . . . , ∞:
E[Vχi [J (xi,π(xi,w), σi) + β>f (xi,π(xi,w), θi)]]
+ Vxi [δ>g (Xi,π(Xi, W))] - βi-1 = 0Sk ,
lim E xi>Vxi [J(xi,π(xi, w), σi) + βi>f (xi, π(xi, w), θi)]
+ Vxi δi>g (Xi, π(Xi, w)) = 0Sk,
E Vw [J(Xi,π(Xi, w), σi) + βi>f (Xi, π(Xi, w), θi)]
+ Vwk δi>g (Xi, π(Xi, w)) = 0A,
xi+1 = f (Xi, π(Xi, w), θi) , g (Xi, π(Xi, w)) ≤ 0C,
δi ≤ 0C,	δi>g (Xi, π(Xi, w)) = 0,
(62)
(63)
(64)
(65)
(66)
where βi-1 is known at time i and includes the multipliers related to Xi-1.
By comparing (56)-(60) and (62)-(66), We conclude that both KKT systems are equal if the following
holds ∀k ∈ N and i = 1, . . . , ∞:
E [Vxi rk (Xi, π(Xi, w), σk,i)] = E [VxiJ (Xi, π(Xi, w), σi)] ,	(67)
E [Vwk rk (Xi, π(Xi, w), σk,i)] = E [VwkJ (Xi, π(Xi, w), σi)] ,	(68)
λk,i = βi ,	μk,i = δi.	(69)
Since Assumption 4 ensures existence of primal variable for the OCP, Assumption 3 guarantee the
existence of dual variables that satisfy its KKT system. By applying (69) and replacing the dual
variables of the KKT of the game with the OCP dual variables for every agent, we obtain a system of
equations where the only unknowns are the user strategies. This system is similar to the OCP in the
primal variables. Therefore, the OCP primal solution also satisfies the KKT necessary conditions of
the game. Moreover, from the potentiality condition, it is straightforward to show that this primal
solution of the OCP is also a PCL-NE of the MPG (see also (Zazo et al., 2016a, Theorem 1)).
Introduce the following vector field:
F (Xi, w, σi) , V(xi,w)J (Xi, π(Xi, w), σi) .	(70)
Since F is conservative by construction (Apostol, 1969, Theorems 10.4, 10.5 and 10.9), conditions
(67)-(68) are equivalent to (17)-(19) and we can calculate a potential J through line integral (20). ■
D Proof of Theorem 2
Proof: We can rewrite game (8) by making explicit that the actions result from the policy mapping,
which yields an expression that reminds the OL problem but with extra constraints:
G3 :
∀k ∈ N
maximize
wk ∈Wk , {ak,i }0∞ ∈Qi∞=0 Ak
s.t.
∞
E	γirk xrk,i , ak,i , a-k,i, σk,i
i=0
ak,i = π(xkπ,i, wk),	a-k,i = π(xπ-k,i, w-k),	(71)
xi+1 = f(xi, ai, θi),
g(xi, ai) ≤ 0,
where it is clear that: ai , (ak,i, a-k,i) = π(xi, w) Rewrite also OCP (16) with explicit dependence
on the actions:
P2 :
maximize
w∈W, {ai}0∞∈Qi∞=0A
s.t.
∞
E	γiJ(xi, ai, σi)
i=0
ai = π(xi, w),
xi+1 = f(xi, ai, θi),
g(xi, ai) ≤ 0.
(72)
14
Published as a conference paper at ICLR 2018
By following the Euler-Lagrange approach described in Theorem 1, we have that the KKT systems
for game and OCP are equal if the dual variables are equal (including new extra dual variables for the
equality constraints that relate the action and the policy) and the following first-order conditions hold
∀k ∈ N and i = 1, . . . , ∞:
E Vχk irk	(xk,i,	ak,i,	a-k,i, σk,i)	= E	Rxki J	(Xi, ai,	σi)
E R ak,i rk	(xk,i,	ak,i,	a-k,i, σk,i)	= E	Rak,i J	(Xi, ai,	σi)
(73)
(74)
The benefit of this reformulation is that the gradient in (73) is taken with respect to the components
in Xkr only (instead of the whole set X), at the cost of replacing (68) with the sequence of conditions
(74). We have to realize that ak,i is indeed a function of variables Xkπ,i and wk. In order to understand
the influence of this variable change, we use the identity ak,i = πwk (Xkπ,i) and apply the chain rule to
both sides of (74), obtaining:
EhRxkπ,irki = E hRxrk,irki> Rxkπ,iXrk,i +E Rak,irk> Rxkπ,iak,i,	(75)
E Rwk rk = E Rak,i rk> Rwk ak,i,	(76)
E hRxkπ,iJi = E hRxrk,iJi>Rxkπ,iXrk,i +E Rak,iJ>Rxkπ,iak,i,	(77)
E[RwkJ]= E Rak,i J > Rwk ak,i.	(78)
From (73)-(74), it is clear that the right side of (75) and (77) are equal. Similarly, from (74), the right
side of (76) and (78) are equal, so that their left side must be also equal. Hence, we can replace (74)
with the two following conditions:
E [Rxn,irk (Xk,i，πwk (xΠ,i) , a-k,i, σk,i
E [Rwk rk (xk,i, πwk (xk,i) , a-k,i, σk,i
E Rx∏,i J (xi, πwk (xk,i) , a-k,i, σi)J
E RwkJ Xi,πwk
(xk,i) , a-k,i, σi)].
Moreover, we can combine (73) and (79) in one single equation:
E IRxθ,irk (xk,i, πWk (Xn,i) , a-k,i, σk,i)] = E IRxθ,i J (xi, πWk (Xn) , a-k,i,
By using the identity a-k,i = πw-k (Xπ-k,i) in (80)-(81), we have:
EiRxθ,irk (xk,i, πwk	(Xn,i)	, πw-k	(x-k,i) , σk,i)	= EIRx% J (Xi, πu (Xi) , σi)],
E [RWkrk (Xi, πWk	(Xn,i)	, πw-k	(X-k,i) , σk,i)	= E [RWk J (Xi, πu (Xi) , σi)] .
(79)
(80)
(81)
(82)
(83)
Note that under conditions (25)-(26), conditions (82)-(83) are equivalent to (67)-(68), with potential
function J equal to the objective of OCP (16).	■
E Proof of Proposition 1
Proof: Once that Theorem 2 has shown that the individual rewards can be expressed in separable
form, it follows from the definition of proper function that: rk being proper implies that J is also
proper. Since J is proper, it has nonempty level sets. Let B ∈ R define a nonempty level set of J:
Since γ < 1, we have:
{a0 ∈ C0, (Xi, ai) ∈ Ci : E [J (Xi, ai, σi)] ≥ B}i∞=0 .
∞∞
XγiE[J(Xi,ai,σi)]≥BXγi
i=0	i=0
Hence, the following level sets are also nonempty:
∞
(Xi,ai) :	γiE [J (Xi, ai, σi)]
i=0
∞
i=0
(84)
(85)
(86)
B
1 - Y
(
≥
B
1 - Y
)
15
Published as a conference paper at ICLR 2018
U
1 - Y
(88)
≤
In addition, since J is proper, it must be upper bounded, i.e., ∃U ∈ R, such that J ≤ U. Then, we
have:
∞∞
X YiE [J (Xi,ai,σi)] ≤ U X γi = 1—.	(87)
i=0	i=0	γ
Since B ≤ U, we have that
B
1 - Y
Therefore, the level sets (86) are bounded.
From Assumption 2 the fact that J can be obtained from line integral (20), and fundamental theorem
of calculus, we deduce that J is continuous. Therefore, we conclude that these level sets are also
compact. Thus, we can use (Bertsekas, 2007, Prop. 3.1.7, see also Sections 1.2 and 3.6) to ensure
existence of an optimal policy.	■
16