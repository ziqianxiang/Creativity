Published as a conference paper at ICLR 2018
Deep Learning with Logged Bandit Feedback
Thorsten Joachims	Adith Swaminathan	Maarten de Rijke
Cornell University	Microsoft Research	University of Amsterdam
tj@cs.cornell.edu	adswamin@microsoft.com	derijke@uva.nl
Ab stract
We propose a new output layer for deep neural networks that permits the use of
logged contextual bandit feedback for training. Such contextual bandit feedback
can be available in huge quantities (e.g., logs of search engines, recommender sys-
tems) at little cost, opening up a path for training deep networks on orders of mag-
nitude more data. To this effect, we propose a counterfactual risk minimization
approach for training deep networks using an equivariant empirical risk estima-
tor with variance regularization, BanditNet, and show how the resulting objective
can be decomposed in a way that allows stochastic gradient descent training. We
empirically demonstrate the effectiveness of the method by showing how deep
networks - ResNets in particular - can be trained for object recognition without
conventionally labeled images.
1	Introduction
Log data can be recorded from online systems such as search engines, recommender systems, or
online stores at little cost and in huge quantities. For concreteness, consider the interaction logs
of an ad-placement system for banner ads. Such logs typically contain a record of the input to the
system (e.g., features describing the user, banner ad, and page), the action that was taken by the
system (e.g., a specific banner ad that was placed) and the feedback furnished by the user (e.g.,
clicks on the ad, or monetary payoff). This feedback, however, provides only partial information
- “contextual-bandit feedback” - limited to the actions taken by the system. We do not get to see
how the user would have responded, if the system had chosen a different action (e.g., other ads or
banner types). Thus, the feedback for all other actions the system could have taken is typically
not known. This makes learning from log data fundamentally different from traditional supervised
learning, where “correct” predictions and a loss function provide feedback for all actions.
In this paper, we propose a new output layer for deep neural networks that allows training on logged
contextual bandit feedback. By circumventing the need for full-information feedback, our approach
opens a new and intriguing pathway for acquiring knowledge at unprecedented scale, giving deep
neural networks access to this abundant and ubiquitous type of data. Similarly, it enables the appli-
cation of deep learning even in domains where manually labeling full-information feedback is not
viable.
In contrast to online learning with contextual bandit feedback (e.g., (Williams, 1992; Agarwal et al.,
2014)), we perform batch learning from bandit feedback (BLBF) (Beygelzimer & Langford, 2009;
Swaminathan & Joachims, 2015a;b;c) and the algorithm does not require the ability to make inter-
active interventions. At the core of the new output layer for BLBF training of deep neural networks
lies a counterfactual training objective that replaces the conventional cross-entropy objective. Our
approach - called BanditNet - follows the view of a deep neural network as a stochastic policy.
We propose a counterfactual risk minimization (CRM) objective that is based on an equivariant
estimator of the true error that only requires propensity-logged contextual bandit feedback. This
makes our training objective fundamentally different from the conventional cross-entropy objective
for supervised classification, which requires full-information feedback. Equivariance in our context
means that the learning result is invariant to additive translations of the loss, and it is more formally
defined in Section 3.2. To enable large-scale training, we show how this training objective can be
decomposed to allow stochastic gradient descent (SGD) optimization.
In addition to the theoretical derivation of BanditNet, we present an empirical evaluation that verifies
the applicability of the theoretical argument. It demonstrates how a deep neural network architec-
1
Published as a conference paper at ICLR 2018
ture can be trained in the BLBF setting. In particular, we derive a BanditNet version of ResNet (He
et al., 2016) for visual object classification. Despite using potentially much cheaper data, we find
that Bandit-ResNet can achieve the same classification performance given sufficient amounts of con-
textual bandit feedback as ResNet trained with cross-entropy on conventionally (full-information)
annotated images. To easily enable experimentation on other applications, we share an implementa-
tion of BanditNet.1
2	Related Work
Several recent works have studied weak supervision approaches for deep learning. Weak super-
vision has been used to pre-train good image features (Joulin et al., 2016) and for information
retrieval (Dehghani et al., 2017). Closely related works have studied label corruption on CIFAR-
10 recently (Zhang et al., 2016). However, all these approaches use weak supervision/corruption
to construct noisy proxies for labels, and proceed with traditional supervised training (using cross-
entropy or mean-squared-error loss) with these proxies. In contrast, we work in the BLBF setting,
which is an orthogonal data-source, and modify the loss functions optimized by deep nets to directly
implement risk minimization.
Virtually all previous methods that can learn from logged bandit feedback employ some form of
risk minimization principle (Vapnik, 1998) over a model class. Most of the methods (Beygelz-
imer & Langford, 2009; Bottou et al., 2013; Swaminathan & Joachims, 2015a) employ an inverse
propensity scoring (IPS) estimator (Rosenbaum & Rubin, 1983) as empirical risk and use stochastic
gradient descent (SGD) to optimize the estimate over large datasets. Recently, the self-normalized
estimator (Trotter & Tukey, 1956) has been shown to be a more suitable estimator for BLBF (Swami-
nathan & Joachims, 2015c). The self-normalized estimator, however, is not amenable to stochastic
optimization and scales poorly with dataset size. In our work, we demonstrate how we can efficiently
optimize a reformulation of the self-normalized estimator using SGD.
Previous BLBF methods focus on simple model classes: log-linear and exponential models (Swami-
nathan & Joachims, 2015a) or tree-based reductions (Beygelzimer & Langford, 2009). In con-
trast, we demonstrate how current deep learning models can be trained effectively via batch learn-
ing from bandit feedback (BLBF), and compare these with existing approaches on a benchmark
dataset (Krizhevsky & Hinton, 2009).
Our work, together with independent concurrent work (Serban et al., 2017), demonstrates success
with off-policy variants of the REINFORCE (Williams, 1992) algorithm. In particular, our algorithm
employs a Lagrangian reformulation of the self-normalized estimator, and the objective and gradi-
ents of this reformulation are similar in spirit to the updates of the REINFORCE algorithm. This
connection sheds new light on the role of the baseline hyper-parameters in REINFORCE: rather
than simply reduce the variance of policy gradients, our work proposes a constructive algorithm for
selecting the baseline in the off-policy setting and it suggests that the baseline is instrumental in
creating an equivariant counterfactual learning objective.
3	BanditNet: Counterfactual Risk Minimization for Deep Nets
To formalize the problem of batch learning from bandit feedback for deep neural networks, consider
the contextual bandit setting where a policy π takes as input x ∈ X and outputs an action y ∈ Y .
In response, we observe the loss (or payoff) δ(x, y) of the selected action y, where δ(x, y) is an
arbitrary (unknown) function that maps actions and contexts to a bounded real number. For example,
in display advertising, the context x could be a representation of the user and page, y denotes the
displayed ad, and δ(x, y) could be the monetary payoff from placing the ad (zero ifno click, or dollar
amount if clicked). The contexts are drawn i.i.d. from a fixed but unknown distribution Pr(X).
In this paper, a (deep) neural network is viewed as implementing a stochastic policy π. We can think
of such a network policy as a conditional distribution πw(Y | x) over actions y ∈ Y , where w are
the parameters of the network. The network makes a prediction by sampling an action y 〜∏w (Y |
x), where deterministic πw (Y | x) are a special case. As we will show as part of the empirical
1http://www.joachims.org/banditnet/
2
Published as a conference paper at ICLR 2018
evaluation, many existing network architectures are compatible with this stochastic-policy view. For
example, any network fw(x, y) with a softmax output layer
πw(y | x)
exp(fw (x,y))
Pyo∈γeχpfw(X, y0))
(1)
can be re-purposed as a conditional distribution from which one can sample actions, instead of
interpreting it as a conditional likelihood like in full-information supervised learning.
The goal of learning is to find a policy πw that minimizes the risk (analogously: maximizes the
payoff) defined as
R(πw )=	E E	[δ(x, y)].	(2)
x~Pr(X) y~∏w(Y|x)
Any data collected from an interactive system depends on the policy π0 that was running on the
system at the time, determining which actions y and losses δ(x, y) are observed. We call π0 the
logging policy, and for simplicity assume that it is stationary. The logged data D are n tuples of
observed context Xi 〜Pr(X), action yi 〜∏o(Y | Xi) taken by the logging policy, the probability
of this action pi ≡ π0(yi | xi), which we call the propensity, and the received loss δi ≡ δ(xi, yi):
D = [(X1,y1,p1, δ1) , . . . , (Xn, yn,pn, δn)] .
(3)
We will now discuss how we can use this logged contextual bandit feedback to train a neural network
policy πw(Y | X) that has low risk R(πw).
3.1	Counterfactual Risk Minimization
While conditional maximum likelihood is a standard approach for training deep neural networks,
it requires that the loss δ(Xi, y) is known for all y ∈ Y. However, we only know δ(Xi, yi) for the
particular yi chosen by the logging policy π0 . We therefore take a different approach following
(Langford et al., 2008; Swaminathan & Joachims, 2015b), where we directly minimize an empirical
risk that can be estimated from the logged bandit data D. This approach is called counterfactual risk
minimization (CRM) (Swaminathan & Joachims, 2015b), since for any policy πw it addresses the
counterfactual question of how well that policy would have performed, if it had been used instead
of π0.
While minimizing an empirical risk as an estimate of the true risk R(πw ) is a common principle
in machine learning (Vapnik, 1998), getting a reliable estimate based on the training data D pro-
duced by π0 is not straightforward. The logged bandit data D is not only incomplete (i.e., we lack
knowledge of δ(Xi, y) for many y ∈ Y that πw would have chosen differently from π0), but it is also
biased (i.e., the actions preferred by π0 are over-represented). This is why existing work on training
deep neural networks either requires full knowledge of the loss function, or requires the ability to
interactively draw new samples y% 〜∏w (Y | Xi) for any new policy ∏w. In our setting we can do
neither - we have a fixed dataset D that is limited to samples from ∏o.
To nevertheless get a useful estimate of the empirical risk, we explicitly address both the bias and
the variance of the risk estimate. To correct for sampling bias and handle missing data, we approach
the risk estimation problem using importance sampling and thus remove the distribution mismatch
between π0 and πw (Langford et al., 2008; Owen, 2013; Swaminathan & Joachims, 2015b):
R(πw)=	E E	[δ(x,y)]=	E E	[δ(x,y)"w(y J x)].	(4)
x~Pr(X) y 〜∏w (Y | x)	x~Pr(X) y~π°(Y∣x)	∏0 (y | X)
The latter expectation can be estimated on a sample D of n bandit-feedback examples using the
following IPS estimator (Langford et al., 2008; Owen, 2013; Swaminathan & Joachims, 2015b):
RIPS (∏w ) = 1 X δi πw叫 Xi) .	(5)
n i=1	π0(yi | Xi)
This IPS estimator is unbiased and has bounded variance, if the logging policy has full support in
the sense that ∀X, y : π0 (y | X) ≥	> 0. While at first glance it may seem natural to directly
train the parameters w of a network to optimize this IPS estimate as an empirical risk, there are at
least three obstacles to overcome. First, we will argue in the following section that the naive IPS
3
Published as a conference paper at ICLR 2018
estimator’s lack of equivariance makes it sub-optimal for use as an empirical risk for high-capacity
models. Second, we have to find an efficient algorithm for minimizing the empirical risk, especially
making it accessible to stochastic gradient descent (SGD) optimization. And, finally, we are faced
with an unusual type of bias-variance trade-off since “distance” from the exploration policy impacts
the variance of the empirical risk estimate for different w .
3.2 Equivariant counterfactual risk minimization
While Eq. (5) provides an unbiased empirical risk estimate, it exhibits the - possibly severe - Prob-
lem of “propensity overfitting” when directly optimized within a learning algorithm (Swaminathan
& Joachims, 2015c). It is a problem of overfitting to the choices yi of the logging policy, and it
occurs on top of the normal overfitting to the δi . Propensity overfitting is linked to the lack of
equivariance of the IPS estimator: while the minimizer of true risk R(πw ) does not change when
translating the loss by a constant (i.e., ∀x, y : δ(x, y) + c) by linearity of expectation,
C + mWnxJE(X) y~∏%Jδ(X，y)] = mWnxJE(X) y5%J(X，y) + c]	⑹
the minimizer of the IPS-estimated empirical risk RIPS (πw) can change dramatically for finite train-
ing samples, and
c + min
w
1XX δ ∏w(y I Xi)
n i=1 i ∏o(yi I Xi)
6=
min
w
1X (δ + 冷 πw Iy | Xi)
n i=1 i	∏o(yi I Xi)
(7)
Intuitively, when c shifts losses to be positive numbers, policies πw that put as little probability mass
as possible on the observed actions have low risk estimates. If c shifts the losses to the negative
range, the exact opposite is the case. For either choice of c, the choice of the policy eventually
selected by the learning algorithm can be dominated by where π0 happens to sample data, not by
which actions have low loss.
The following self-normalized IPS estimator (SNIPS) addresses the propensity overfitting prob-
lem (Swaminathan & Joachims, 2015c) and is equivariant:
1 Pn δ ■ ∏w(yi∣χi)
^	(T n _ n 乙i=1 i πo(yi∣xi)	/oʌ
RSNlPS (πw) =	1 Pn ∏w(yi∣k.	⑻
n 乙i=1 ∏o(yi ∣Xi)
In addition to being equivariant, this estimate can also have substantially lower variance than Eq. (5),
since it exploits the knowledge that the denominator
S :=	1 X ∏w (yi I Xi)
n 白 ∏o(yi I Xi)
(9)
always has expectation 1:
E[S] = 1 χ/ ：：(；]；； ∏o(yi I Xi) Pr(Xi)dyidXi = n χ/ 1 Pr(Xi)dXi = 1.	(10)
The SNIPS estimator uses this knowledge as a multiplicative control variate (Swaminathan &
Joachims, 2015c). While the SNIPS estimator has some bias, this bias asymptotically vanishes
at a rate of O(n) (Hesterberg, 1995). Using the SNIPS estimator as our empirical risk implies that
we need to solve the following optimization problem for training:
W = arg min RSNIPS(∏w).	(11)
w∈<N
Thus, we now turn to designing efficient optimization methods for this training objective.
3.3 Training algorithm
Unfortunately, the training objective in Eq. (11) does not permit stochastic gradient descent (SGD)
optimization in the given form (see Appendix C), which presents an obstacle to efficient and effective
training of the network. To remedy this problem, we will now develop a reformulation that retains
4
Published as a conference paper at ICLR 2018
both the desirable properties of the SNIPS estimator, as well as the ability to reuse established SGD
training algorithms. Instead of optimizing a ratio as in Eq. (11), we will reformulate the problem into
a series of constrained optimization problems. Let W be a solution of Eq. (11), and at that solution
let S* be the value of the control variate for ∏w as defined in Eq. (9). For simplicity, assume that
the minimizer W is unique. If We knew S*, We could equivalently solve the following constrained
optimization problem:
W = argmin1 XX δi πw(yi | Xi)	subject to 1	XX	π(叫	Xi)	= S*.	(12)
w∈<N n i=1 π0(yi | xi)	n	i=1	π0(yi |	xi)
Of course, we do not actually know S*. However, we can do a grid search in {S1, . . . , Sk} for S*
and solve the above optimization problem for each value, giving us a set of solutions {Wι,...,Wk }.
Note that S is just a one-dimensional quantity, and that the sensible range we need to search for
S * concentrates around 1 as n increases (see Appendix B). To find the overall (approximate) W that
optimizes the SNIPS estimate, we then simply take the minimum:
1 sp,n ʌ πWj (yi|Xi)
W = arg min n °i=1 : n0(yi|xL.	(13)
(Wj,Sj)	Sj
This still leaves the question of how to solve each equality constrained risk minimization problem
using SGD. Fortunately, we can perform an equivalent search for S* without constrained optimiza-
tion. To this effect, consider the Lagrangian of the constrained optimization problem in Eq. (12)
with Sj in the constraint instead of S* :
1n
L(W,λ) = n∑
δi∏w (yi | Xi)
∏o(yi | Xi)
-λ
1 (XX ∏w (yi | Xi)
n =-∏ ∏o(yi | Xi)，
- Sj
1 XX (δi - λ)πw (yi 1 Xi) + λS
n = ∏o(yi | Xi)	j
The variable λ is an unconstrained Lagrange multiplier. To find the minimum of Eq. (12) for a
particular Sj, we need to minimize L(W, λ) w.r.t. W and maximize w.r.t. λ.
Wj = arg min max L(w, λ)	(14)
w∈<N	λ
However, we are not actually interested in the constrained solution of Eq. (12) for any specific Sj .
We are merely interested in exploring a certain range S ∈ [S1, Sk] in our search for S*. So, we can
reverse the roles ofλ and S, where we keep λ fixed and determine the corresponding S in hindsight.
In particular, for each {λ1 , . . . , λk } we solve
Wj = arg min L(w, λj).	(15)
w∈<N
Note that the solution Wj does not depend on Sj, so we can compute Sj after we have found the
minimum Wj. In particular, we can determine the Sj that corresponds to the given λj∙ using the
necessary optimality conditions,
∂L _ 1X dπw((Ji | Xi) (δ -λj) _o . 生 _ 1Xπw3i | Xi) _ S. _o (16)
∂w n = ∂w	∏o(yi	|	Xi)	∂λj	n =	∏o(yi	|	Xi)	j ,
by solving the second equality of Eq. (16). In this way, the sequence of λj∙ produces solutions Wj
corresponding to a sequence of {S1, . . . , Sk}.
To identify the sensible range of S to explore, we can make use of the fact that Eq. (9) concentrates
around its expectation of 1 for each πw as n increases. Theorem 2 in Appendix B provides a
characterization of how large the range needs to be. Furthermore, we can steer the exploration
of S via λ, since the resulting S changes monotonically with λ:
(λa < λb) and (Wa = Wb are not equivalent optima in Eq. (15)) ⇒ (Sa < Sb).	(17)
A more formal statement and proof are given as Theorem 1 in Appendix A. In the simplest form one
could therefore perform a grid search on λ, but more sophisticated search methods are possible too.
After this reformulation, the key computational problem is finding the solution of Eq. (15) for each
λj. Note that in this unconstrained optimization problem, the Lagrange multiplier effectively trans-
lates the loss values in the conventional IPS estimate:
Wj = arg min1 X (δi - λj) πw(yi | Xi) = arg min RRIPS(∏w).	(18)
w n i=1	π0(yi | Xi)	w
5
Published as a conference paper at ICLR 2018
Number of Bandit-Feedback Examples
Figure 1: Learning curve of BanditNet. The x-axis is the amount of bandit feedback, the y-axis is
the test error. Given enough bandit feedback, Bandit-ResNet converges to the skyline performance.
We denote this λ-translated IPS estimate with R)ps(∏w). Note that each such optimization problem
is now in the form required for SGD, where we merely weight the derivative of the stochastic policy
network ∏w (y | x) by a factor (δi - λj)∕∏o(yi | Xi). This opens the door for re-purposing existing
fast methods for training deep neural networks, and we demonstrate experimentally that SGD with
momentum is able to optimize our objective scalably.
Similar loss translations have previously been used in on-policy reinforcement learning (Williams,
1992), where they are motivated as minimizing the variance of the gradient estimate (Weaver &
Tao, 2001; Greensmith et al., 2004). However, the situation is different in the off-policy setting we
consider. First, we cannot sample new roll-outs from the current policy under consideration, which
means we cannot use the standard variance-optimal estimator used in REINFORCE. Second, we
tried using the (estimated) expected loss of the learned policy as the baseline as is commonly done
in REINFORCE, but will see in the experiment section that this value for λ is far from optimal.
Finally, it is unclear whether gradient variance, as opposed to variance of the ERM objective, is
really the key issue in batch learning from bandit feedback. In this sense, our approach provides
a rigorous justification and a constructive way of picking the value of λ in the off-policy setting 一
namely the value for which the corresponding Sj minimizes Eq. (13). In addition, one can further
add variance regularization (Swaminathan & Joachims, 2015b) to improve the robustness of the risk
estimate in Eq. (18) (see Appendix D for details).
4	Empirical Evaluation
The empirical evaluation is designed to address three key questions. First, it verifies that deep
models can indeed be trained effectively using our approach. Second, we will compare how the
same deep neural network architecture performs under different types of data and training objectives
一 in particular, conventional cross-entropy training using full-information data. In order to be able
to do this comparison, we focus on synthetic contextual bandit feedback data for training BanditNet
that is sampled from the full-information labels. Third, we explore the effectiveness and fidelity of
the approximate SNIPS objective.
For the following BanditNet experiments, we adapted the ResNet20 architecture (He et al., 2016)
by replacing the conventional cross-entropy objective with our counterfactual risk minimization
objective. We evaluate the performance of this Bandit-ResNet on the CIFAR-10 (Krizhevsky &
Hinton, 2009) dataset, where we can compare training on full-information data with training on
bandit feedback, and where there is a full-information test set for estimating prediction error.
To simulate logged bandit feedback, we perform the standard supervised to bandit conversion
(Beygelzimer & Langford, 2009). We use a hand-coded logging policy that achieves about 49%
error rate on the training data, which is substantially worse than what we hope to achieve after learn-
ing. This emulates a real world scenario where one would bootstrap an operational system with a
mediocre policy (e.g., derived from a small hand-labeled dataset) and then deploys it to log bandit
feedback. This logged bandit feedback data is then used to train the Bandit-ResNet.
We evaluate the trained model using error rate on the held out (full-information) test set. We compare
this model against the skyline of training a conventional ResNet using the full-information feedback
6
Published as a conference paper at ICLR 2018
)tset( etaR rorrE
05 05 0
3221 1
0
0.65	0.7	0.75	0.8	0.85	0.9	0.95	1
Lagrange Multiplier (lambda)
30
SNIPS Error Estimate (train)
Control Variate -	/
)niart( etamitsErorrE SPINS
0
0.65	0.7	0.75	0.8	0.85	0.9	0.95
0.6
1	1.05
Lagrange Multiplier (lambda)
Figure 2: The x-axis shows the value of the Lagrange multiplier λ used for training. Left plot shows
the test error. Right plot shows the value of the SNIPS objective and the normalizer S . The size of
the training set is 50k bandit-feedback examples.
θ≡μB>5tuoo--O Θ-B>
1.05
from the 50,000 training examples. Both the conventional full-information ResNet as well as the
Bandit-ResNet use the same network architecture, the same hyperparameters, the same data aug-
mentation scheme, and the same optimization method that were set in the CNTK implementation of
ResNet20. Since CIFAR10 does not come with a validation set for tuning the variance-regularization
constant γ, we do not use variance regularization for Bandit-ResNet. The Lagrange multiplier
λ ∈ {0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05} is selected on the training set via Eq. (13). The
only parameter we adjusted for Bandit-ResNet is lowering the learning rate to 0.1 and slowing down
the learning rate schedule. The latter was done to avoid confounding the Bandit-ResNet results with
potential effects from early stopping, and we report test performance after 1000 training epochs,
which is well beyond the point of convergence in all runs.
Learning curve. Figure 1 shows the prediction error of the Bandit-ResNet as more and more ban-
dit feedback is provided for training. First, even though the logging policy that generated the bandit
feedback has an error rate of 49%, the prediction error of the policy learned by the Bandit-ResNet is
substantially better. It is between 13% and 8.2%, depending on the amount of training data. Second,
the horizontal line is the performance of a conventional ResNet trained on the full-information train-
ing set. It serves as a skyline of how good Bandit-ResNet could possibly get given that it is sampling
bandit feedback from the same full-information training set. The learning curve in Figure 1 shows
that Bandit-ResNet converges to the skyline performance given enough bandit feedback training
data, providing strong evidence that our training objective and method can effectively extract the
available information provided in the bandit feedback.
Effect of the choice of Lagrange multiplier. The left-hand plot in Figure 2 shows the test error
of solutions Wj depending on the value of the Lagrange multiplier λj used during training. It shows
that λ in the range 0.8 to 1.0 results in good prediction performance, but that performance degrades
outside this area. The SNIPS estimates in the right-hand plot of Figure 2 roughly reflects this optimal
range, given empirical support for both the SNIPS estimator and the use of Eq. (13).
We also explored two other methods for selecting λ. First, we used the straightforward IPS estimator
as the objective (i.e., λ = 0), which leads to prediction performance worse than that of the logging
policy (not shown). Second, we tried using the (estimated) expected loss of the learned policy as the
baseline as is commonly done in REINFORCE. As Figure 1 shows, it is between 0.130 and 0.083
for the best policies we found. Figure 2 (left) shows that these baseline values are well outside of
the optimum range.
Also shown in the right-hand plot of Figure 2 is the value of the control variate in the denominator
of the SNIPS estimate. As expected, it increases from below 1 to above 1 as λ is increased. Note
that large deviations of the control variate from 1 are a sign of propensity overfitting (Swaminathan
& Joachims, 2015c). In particular, for all solutions Wj the estimated standard error of the control
variate Sj was less than 0.013, meaning that the normal 95% confidence interval for each Sj is
contained in [0.974,1.026]. If We see a Wj with control variate Sj outside this range, We should be
suspicious of propensity overfitting to the choices of the logging policy and discard this solution.
7
Published as a conference paper at ICLR 2018
5	Conclusions and Future Work
We proposed a new output layer for deep neural networks that enables the use of logged contextual
bandit feedback for training. This type of feedback is abundant and ubiquitous in the form of inter-
action logs from autonomous systems, opening up the possibility of training deep neural networks
on unprecedented amounts of data. In principle, this new output layer can replace the conventional
cross-entropy layer for any network architecture. We provide a rigorous derivation of the training
objective, linking it to an equivariant counterfactual risk estimator that enables counterfactual risk
minimization. Most importantly, we show how the resulting training objective can be decomposed
and reformulated to make it feasible for SGD training. We find that the BanditNet approach applied
to the ResNet architecture achieves predictive accuracy comparable to conventional full-information
training for visual object recognition.
The paper opens up several directions for future work. First, it enables many new applications
where contextual bandit feedback is readily available. Second, in settings where it is infeasible to
log propensity-scored data, it would be interesting to combine BanditNet with propensity estimation
techniques. Third, there may be improvements to BanditNet, like smarter search techniques for S,
more efficient counterfactual estimators beyond SNIPS, and the ability to handle continuous outputs.
Acknowledgments
This research was supported in part by NSF Award IIS-1615706, a gift from Bloomberg, the Criteo
Faculty Research Award program, and the Netherlands Organisation for Scientific Research (NWO)
under project nr. 612.001.116. All content represents the opinion of the authors, which is not neces-
sarily shared or endorsed by their respective employers and/or sponsors.
References
A. Agarwal, D. Hsu, S. Kale, J. Langford, Lihong Li, and R. Schapire. Taming the monster: A fast
and simple algorithm for contextual bandits. In ICML, 2014.
A. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In KDD, pp.
129-138, 2009.
L.	Bottou, J. Peters, J. Quinonero-Candela, D. Charles, M. Chickering, E. Portugaly, D. Ray,
P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of
computational advertising. JMLR, 14:3207-3260, 2013.
M.	Dehghani, H. Zamani, A. Severyn, J. Kamps, and W. B. Croft. Neural ranking models with weak
supervision. In SIGIR, pp. 65-74, 2017.
E. Greensmith, P.L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in
reinforcement learning. JMLR, 5:1471-1530, 2004.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
T. Hesterberg. Weighted average importance sampling and defensive mixture distributions. Techno-
metrics, 37:185-194, 1995.
A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache. Learning visual features from large weakly
supervised data. In ECCV, pp. 67-84, 2016.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical
report, Computer Science Department, University of Toronto, 2009.
J. Langford, A. Strehl, and J. Wortman. Exploration scavenging. In ICML, pp. 528-535, 2008.
A.B. Owen. Monte Carlo theory, methods and examples. 2013.
P. Rosenbaum and D. Rubin. The central role of propensity score in observational studies for causal
effects. Biometrica, 70:41-55, 1983.
8
Published as a conference paper at ICLR 2018
I. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian, T. Kim, M. Pieper, S. Chandar,
N. R. Ke, S. Mudumba, A. de Brebisson, J. M. R. Sotelo, D. Suhubdy, V. Michalski, A. Nguyen,
J. Pineau, and Y. Bengio. A deep reinforcement learning chatbot. ArXiv e-prints, September
2017.
A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit
feedback. In ICML,pp. 814-823, 2015a.
A. Swaminathan and T. Joachims. Batch learning from logged bandit feedback through counterfac-
tual risk minimization. JMLR, 16:1731-1755, 2015b.
A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In
NIPS, 2015c.
H. F. Trotter and J. W. Tukey. Conditional monte carlo for normal samples. In Symposium on Monte
Carlo Methods, pp. 64-79, 1956.
V. Vapnik. Statistical Learning Theory. Wiley, Chichester, GB, 1998.
L. Weaver and N. Tao. The optimal reward baseline for gradient-based reinforcement learning. In
UAI, pp. 538-545, 2001.
R. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learn-
ing. Machine Learning, 8(3-4), May 1992.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. CoRR, abs/1611.03530, 2016.
9
Published as a conference paper at ICLR 2018
A APPENDIX: STEERING THE EXPLORATION OF S THROUGH λ.
Theorem 1.	Let λa < λb and let
Wa = arg min RIPS (∏w)	(19)
w
Wb = arg min RPS (∏w).	(20)
w
λλ
If the optima Wa and Wb are not equivalent in the sense that RlPS (∏wa) = RIPS (π1^6) and
RIPS (nWa ) = RIPS (nWb ), ^then
Sa < Sb.	(21)
Proof Abbreviate f (W) = 1 pi=ι δi⅛⅛⅛ and g(w) = 1 Pi=ι πw⅛xi) .Then
rrIps(πw) = f (w) - Xg(W),	(22)
where g(W) corresponds to the value of the control variate S. Since Wa and Wb are not equivalent
optima, we know that
f (Wa)- λa g(Wa) < f(Wb)-入& g(Wb)	(23)
f (Wb) - λb g(Wb) < f (Wa) - λb g(Wa)	(24)
Adding the two inequalities and solving implies that
⇒	f (Wa) - λa g(Wa) + f (Wb) - λb g(Wb)	<	f (Wb)-入& g(Wb) + f(Wa ) - λb g(Wa)	(25)
⇔	λa g(Wa) + λb g(Wb) > λa g(Wb) + λb g(Wa)	(26)
⇔	(λb - λa) g(Wb) > (λb - λa) g(Wa)	(27)
⇔	g(Wb) > g(Wa)	(28)
⇔	Sb >Sa □	(29)
B	APPENDIX: CHARACTERIZING THE RANGE OF S TO EXPLORE.
Theorem 2.	Let p ≤ π0 (y | x) be a lower bound on the propensity for the logging policy, then
constraining the solution of Eq. (11) to the W with control variate S ∈ [1 - , 1 + ] for a training
set of size n will not exclude the minimizer of the true risk w* = argmi□w∈w R(∏w) in the policy
space W with probability at least
1 — 2exp (-2ηe2p2) .	(30)
Proof. For the optimal W* , let
Q _ V πw*(yi | Xi)	z71λ
=i=1 ∏03 I Xi)	(31)
be the control variate in the denominator of the SNIPS estimator. S is a random variable that is a
sum of bounded random variables between 0 and
max ∏W;(⅛) ≤ 1.	(32)
x,y π0 (y | X) p
We can bound the probability that the control variate S of the optimum W* lies outside of [1 -, 1+]
via Hoeffding’s inequality:
-2n22
P (IS - 1∣≥ E) ≤ 2exp --2	(33)
n(1/p)2
= 2exp (-2nE2p2). □	(34)
The same argument applies to any individual policy πw, not just W*. Note, however, that it can still
be highly likely that at least one policy πw with W ∈ W shows a large deviation in the control variate
for high-capacity W, which can lead to propensity overfitting when using the naive IPS estimator.
10
Published as a conference paper at ICLR 2018
C Appendix: Why Direct Stochastic Optimization of Ratio
Estimators is Not Possible.
Suppose we have a dataset of n BLBF samples D = {(x1, y1, δ1, p1) . . . (xn, yn, δn,pn)} where
each instance is an i.i.d. sample from the data generating distribution. In the sequel we will be con-
sidering two datasets ofn+ 1 samples, D0 = D∪ {(x0, y0, δ0, p0)} and D00 = D∪ {(x00, y00, δ00,p00)}
where (x0,y0,δ0,p0) = (x00,y00,δ00,p00) and (x0,y0,δ0,p0), (x00,y00,δ00,p00) / D.
For notational convenience, let f := δi ：：(；：|Xi), and f := Vw fi; gi :=；嗽|；：), and gi := Vwgi.
First consider the vanilla IPS risk estimate of Eq. (5).
RIPS(∏w) = 1 XXδi"w(yi || Xi) =1 XXfi.
n i=1 π0(yi | xi) n i=1
To maximize this estimate using stochastic optimization, we must construct an unbiased gradient
estimate. That is, we randomly select one sample from D and compute a gradient α((xi, yi, δi,pi))
and we require that
1n
▽ w RIPS (πw ) = —〉J fi = Ei~D [α((xi, yi ,δi,Pi))] .
i=1
Here the expectation is over our random choice of 1 out of n samples. Observe that
α((xi, yi, δi,pi)) = fi suffices (and indeed, this corresponds to vanilla SGD):
n1	1n
Ei~D [α((xi,yi,δi,Pi))] = E nα((xi,yi,δi,Pi)) = £ £ fi = RwRIPS(∏w).
i=1
i=1
Other choices of α(∙) can also produce unbiased gradient estimates, and this leads to the study of
stochastic variance-reduced gradient optimization.
Now let us attempt to construct an unbiased gradient estimate for Eq. (8):
Pn fi
RSNIPS (∏w) = ^n-----.
i=1 gi
Suppose such a gradient estimate exists, β((xi, yi, δi , pi)). Then,
Pn fi	1 n
▽w RSNIPS (∏w ) = Vw ^^n :=Ei~D [β((xi,yi, δi,pi))]= n∑β((xi, yi, δi, pi)).
This identity is true for any sample of BLBF instances - in particular, for D0 and D00:
Vw
P=If + f =X
,∑"=1 gi + g0 = ⅛
Pn=I fi+f0 = X
Pn=I gi + g00 ⅛
n+1
β((xi, yi, δi, pi)) +
β((x0, y0, δ0, p0))
n+1
n+1
β((xi, yi, δi, pi)) +
β((x00, y00, δ00, p00))
n+1
1
1
Subtracting these two equations,
(Pn=I fi+f _ Pn=ι fi+fo ∖ = β((χ0,y0,δo,p0))- β (H,/*",”0))
IPn=I gi + g0	Pn=1 gi + g" )	n + 1
The LHS clearly depends on {(xi, yi, δi,pi)}in=1 in general, while the RHS does not! This contra-
diction indicates that no construction of β that only looks at a sub-sample of the data can yield an
unbiased gradient estimate of RSNIPS(πw).
11
Published as a conference paper at ICLR 2018
D	Appendix: Variance Regularization
Unlike in conventional supervised learning, a counterfactual empirical risk estimator like RIPS (πw)
can have vastly different variances Var(RIPS (πw)) for different πw in the hypothesis space (and
RSNIPS (πw) as well) (Swaminathan & Joachims, 2015b). Intuitively, the “closer” the particular
πw is to the exploration policy π0, the larger the effective sample size (Owen, 2013) will be and
the smaller the variance of the empirical risk estimate. For the optimization problems we solve in
Eq. (18), this means that We should trust the λ-translated risk estimate RRIPS (∏w) more for some
W than for others, as we use RIPS(∏w) only as a proxy for finding the policy that minimizes its
expected value (i.e., the true loss). To this effect, generalization error bounds that account for this
variance difference (Swaminathan & Joachims, 2015b) motivate a new type of overfitting control.
This leads to the following training objective (Swaminathan & Joachims, 2015b), which can be
thought of as a more reliable version of Eq. (18):
Wj = arg min Rj S (∏w) + Y jVar(RIjS (nw))
(35)
λj	λj
Here, Var(RIPj S (πw)) is the estimated variance of RIPjS(πw) on the training data, and γ is a reg-
ularization constant to be selected via cross validation. The intuition behind this objective is that
we optimize the upper confidence interval, which depends on the variance of the risk estimate for
each πw . While this objective again does not permit SGD optimization in its given form, it has been
shown that a Taylor-majorization can be used to successively upper bound the objective in Eq. (35),
and that typically a small number of iterations suffices to converge to a local optimum (Swaminathan
& Joachims, 2015b). Each such Taylor-majorization is again ofa form
1XX A (πw(y | Xi) ∖ + B (πw(y | Xi) Y
n i=1	∖∏o(yi | Xi))	[∏o(yi | Xi))
(36)
for easily computable constants A and B (Swaminathan & Joachims, 2015b), which allows for SGD
optimization.
12