Table 1: CIFAR10 accuracy. Teacher model: 5.3M param, 21 MB, accuracy 89.71 %.Details aboutthe resulting size of the models are reported in table 11 in the appendix.
Table 2:	CIFAR10 accuracy. Teacher model: 5.3M param, 21 MB, accuracy 89.71%. Details aboutthe resulting size of the models are reported in table 14 in the appendix.
Table 3:	CIFAR10 accuracy (Wide Residual Network). Teacher model: 145M param, 580 MB,accuracy 95.7 %. Details about the resulting size of the models are reported in table 17 in theappendix.
Table 4:	CIFAR100 accuracy and model size. Teacher: 36.5M param, 146 MB, acc. 77.21 %.
Table 5: OpenNMT dataset BLEU score and perplexity (ppl). Teacher model: 84.8M param, 340MB, 26.1 ppl, 15.88 BLEU. Details about the resulting size of the models are reported in table 23 inthe appendix.
Table 6: WMT13 dataset BLEU score and perplexity (ppl). Teacher model: 84.8M param, 340 MB,5.8 ppl, 34.7 BLEU. Details about model size are reported in Table 26.
Table 7: Imagenet accuracy and model size. Bucket size = 256.
Table 8: CIFAR10: model specifications76c2-mp-dp-126c2-mp-dp-148c4-mp-dp-1200fc-dp-1200fc75c-mp-dp-50c2-mp-dp-25c-mp-dp-500fc-dp50c-mp-dp-25c2-mp-dp-10c-mp-dp-400fc-dp25c-mp-dp-10c2-mp-dp-5c-mp-dp-300fc-dpFollowing the authors of the paper, we don’t use dropout layers when training the models usingdistillation loss. Distillation loss is computed with a temperature of T = 5.
Table 9: CIFAR10: Teacher and distilled model accuracy, full precisionModel name Teacher model	Test accuracy 89.7%	# of parameters 5.3 millions	Size (MB) 21.3 MBSmaller model 1	84.5%	1.0 millions	4.00 MBDistilled model 1	88.8 %	1.0 millions	4.00 MBSmaller model 2	80.3 %	0.3 millions	1.27 MBDistilled model 2	84.3 %	0.3 millions	1.27 MBSmaller model 3	71.6%	0.1 millions	0.45 MBDistilled model 3	78.2%	0.1 millions	0.45 MBTable 10: CIFAR10: Test accuracy for quantized models. Results computed with bucket size = 256	2bits	4 bits	8 bitsPM Quant. 1 (No bucket)	9.30%	67.99 %	88.91 %PM Quant. 1 (with bucket)	10.53 %	87.18 %	88.80 %Quantized Distill. 1	82.4%	88.00 %	88.82 %Differentiable Quant. 1	80.43%	88.31 %	—PM Quant. 2 (No bucket)	10.15%	68.05 %	84.38 %PM Quant. 2 (with bucket)	11.89%	81.96 %	84.38 %Quantized Distill. 2	74.22 %	83.92 %	84.22 %Differentiable Quant. 2	72.79 %	83.49 %	—PM Quant. 3 (No bucket)	10.15%	61.30 %	78.04 %PM Quant. 3 (with bucket)	10.38 %	72.44 %	78.10 %
Table 10: CIFAR10: Test accuracy for quantized models. Results computed with bucket size = 256	2bits	4 bits	8 bitsPM Quant. 1 (No bucket)	9.30%	67.99 %	88.91 %PM Quant. 1 (with bucket)	10.53 %	87.18 %	88.80 %Quantized Distill. 1	82.4%	88.00 %	88.82 %Differentiable Quant. 1	80.43%	88.31 %	—PM Quant. 2 (No bucket)	10.15%	68.05 %	84.38 %PM Quant. 2 (with bucket)	11.89%	81.96 %	84.38 %Quantized Distill. 2	74.22 %	83.92 %	84.22 %Differentiable Quant. 2	72.79 %	83.49 %	—PM Quant. 3 (No bucket)	10.15%	61.30 %	78.04 %PM Quant. 3 (with bucket)	10.38 %	72.44 %	78.10 %Quantized Distill. 3	67.02 %	77.75 %	77.92 %Differentiable Quant. 3	57.84 %	77.36 %	—We also performed an experiment with a deeper student model. The architecture is 76c3-mp-dp-126c3-mp-dp-148c5-mp-dp-1000fc-dp-1000fc-dp-1000fc (following the same notation as in table8). We use the same teacher as in the previous experiments. Results are in table 13.
Table 11:	CIFAR10: Optimal length Huffman encoding and resulting model size. Bucket size = 256PM Quant. 1 (No bucket)PM Quant. 1 (with bucket)Quantized Distill. 1Differentiable Quant. 12 bits1.34 bits - 0.17 MB1.58 bits - 0.22 MB1.7 bits - 0.24 MB3.18 bits - 0.43 MB4 bits2.43 bits - 0.3 MB3.52 - 0.47 MB3.64 bits - 0.48 MB5.34 bits - 0.7 MB8 bits6.48 bits - 0.81 MB7.58 bits - 0.98 MB7.70 bits - 1 MBPM Quant. 2 (No bucket)
Table 12:	CIFAR10: Teacher and distilled model accuracy, full precisionModel nameTeacher modelTest accuracy89.7 %# of parameters5.3 millionsSize (MB)21.3 MBDeeper normal model 1Deeper distilled model 193.22 %92.60 %5.8 millions5.8 millions23.40 MB23.40 MBTable 13:	CIFAR10: Test accuracy for quantized deeper student model. Results computed withbucket size = 256PM Quant. (No bucket)
Table 13:	CIFAR10: Test accuracy for quantized deeper student model. Results computed withbucket size = 256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 14:	CIFAR10: Optimal length Huffman encoding and resulting model size for deeper studentmodel. Bucket size = 256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 15:	CIFAR10: Teacher and distilled model accuracy, full precision, wide resnetModel nameTeacher modelSmaller modelDistilled modelStructuredepth = 28, wide_factor = 20depth = 22, wide_factor =16depth = 22, wide_factor =16Test accuracy # of parameters95.74 %	145	millions95.19 %	82.7	millions94.19 %	82.7	millionsSize (MB)580 MB330 MB330 MBA.2 CIFAR 1 00For our CIFAR100 experiments, we use the same implementation of wide residual networks as inour CIFAR10 experiments. The wide factor is a multiplicative factor controlling the amount of filters
Table 16:	CIFAR10: Test accuracy for quantized models. Results computed with bucket size = 256PM Quant. (with bucket)Quantized Distill.
Table 17:	CIFAR10: Optimal length Huffman encoding and resulting model size. Bucket size = 256PM Quant. (with bucket)Quantized Distill.
Table 18:	CIFAR100: Teacher and distilled model accuracy, full precisionModel nameTeacher modelSmaller modelDistilled modelStructuredepth = 28, wide_factor = 10depth = 22, widefactor = 8depth = 22, widefactor = 8Test accuracy	# of parameters77.21 %	36.5	millions77.08 %	17.2 millions77.24 %	17.2 millionsSize (MB)146 MB68.8 MB68.8 MBTable 19: CIFAR100: Test accuracy for quantized models. Results computed with bucket size = 256	2 bits	4 bitsPM Quant. (No bucket)	1.38%	1.29%
Table 19: CIFAR100: Test accuracy for quantized models. Results computed with bucket size = 256	2 bits	4 bitsPM Quant. (No bucket)	1.38%	1.29%PM Quant. (with bucket)	1.00%	73.47%Quantized Distill.	27.84%	76.31%Differentiable Quant.	49.32%	77.07%Table 20: CIFAR100: Optimal length Huffman encoding and resulting model size. Bucket size =256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 20: CIFAR100: Optimal length Huffman encoding and resulting model size. Bucket size =256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 21: openNMT integ: Teacher and distilled models perplexity and BLEU, full precisionModel name Teacher model	Structure 4 LSTM layer, 500 cell size	Perplexity 26.21	BLEU 15.88	# of parameters 84.8 millions	Size (MB) 339.28 MBSmaller model 1	2LSTM layer, 512 cell size	33.03	14.97	81.6 millions	326.57 MBDistilled model 1	2 LSTM layer, 512 cell size	25.55	16.13	81.6 millions	326.57 MBSmaller model 2	2 LSTM layer, 256 cell size	34.5	14.22	64.8 millions	249.56 MBDistilled model 2	2 LSTM layer, 256 cell size	27.7	15.48	64.8 millions	249.56 MBSmaller model 3	2 LSTM layer, 128 cell size	39.5	12.45	57.2 millions	228.85 MBDistilled model 3	2 LSTM layer, 128 cell size	33.78	13.8	57.2 millions	228.85 MBTable 22:	openNMT integ: Test accuracy for quantized models. Results computed with bucket size= 256PM Quant. 1 (No bucket)PM Quant. 1 (with bucket)Quantized Distill. 1Differentiable Quant. 1PM Quant. 2 (No bucket)PM Quant. 2 (with bucket)Quantized Distill. 2Differentiable Quant. 2PM Quant. 3 (No bucket)PM Quant. 3 (with bucket)
Table 22:	openNMT integ: Test accuracy for quantized models. Results computed with bucket size= 256PM Quant. 1 (No bucket)PM Quant. 1 (with bucket)Quantized Distill. 1Differentiable Quant. 1PM Quant. 2 (No bucket)PM Quant. 2 (with bucket)Quantized Distill. 2Differentiable Quant. 2PM Quant. 3 (No bucket)PM Quant. 3 (with bucket)Quantized Distill. 3Differentiable Quant. 32 bits2 ∙ 1017 PPl - 0.00 BLEU125.1 ppl - 4.12 BLEU6645 PPl - 0.00 BLEU249 PPl - 0.7 BLEU5 ∙ 108 ppl - 0.00 BLEU
Table 23:	openNMT integ: Optimal length Huffman encoding and resulting model size. Bucket size= 256PM Quant. 1 (No bucket)PM Quant. 1 (with bucket)Quantized Distill. 1Differentiable Quant. 1PM Quant. 2 (No bucket)PM Quant. 2 (with bucket)Quantized Distill. 2Differentiable Quant. 2PM Quant. 3 (No bucket)PM Quant. 3 (with bucket)Quantized Distill. 3Differentiable Quant. 32 bits1.36 bits - 13.93 MB1.65 bits - 19.47 MB1.75 bits - 20.4 MB1.72 bits - 20.1 MB1.34 bits - 10.89 MB
Table 24: WMT13: Teacher and distilled models perplexity and BLEU, full precisionModel name	Structure	Perplexity	BLEU	# of parameters	Size (MB)Teacher model	4 LSTM layer, 500 cell size	5.83	34.77	84.8 millions	339.28 MBSmaller model 1	2 LSTM layer, 500 size	7.98	30.22	80.8 millions	323.25 MBDistilled model 1	2 LSTM layer, 550 cell size	7.18	30.21	84.3 millions	337.21 MBTable 25:	WMT13: Test accuracy for quantized models. Results computed with bucket size = 256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 25:	WMT13: Test accuracy for quantized models. Results computed with bucket size = 256PM Quant. (No bucket)PM Quant. (with bucket)Quantized Distill.
Table 26:	WMT13: Optimal length Huffman encoding and resulting model size. Bucket size = 256PM Quant. 1 (No bucket)PM Quant. 1 (with bucket)Quantized Distill. 14 bits1.98 bits - 20.92 MB3.63 bits - 41.02 MB3.65 bits - 41.16 MBTable 27 shows the results on the CIFAR10 dataset; the models we train have the same structure asthe Smaller model 1, see Section A.1.
Table 27:	CIFAR10: Distillation loss vs normal loss when quantizingNormal lossDistillation loss2 bits67.22 %82.40 %4 bits86.01 %88.00 %Table 28:	openNMT integ: Distillation loss vs normal loss when quantizingNormal lossDistillation loss4 bits32.67 ppl 15.03 BLEU25.43 ppl 15.73 BLEUThese results suggest that quantization works better when combined with distillation, and that weshould try to take advantage of this whenever we are quantizing a neural network.
Table 28:	openNMT integ: Distillation loss vs normal loss when quantizingNormal lossDistillation loss4 bits32.67 ppl 15.03 BLEU25.43 ppl 15.73 BLEUThese results suggest that quantization works better when combined with distillation, and that weshould try to take advantage of this whenever we are quantizing a neural network.
Table 29: Results with automatically redistributed bits2 bits	Distillation loss Normal loss	Quantiles Uniform 82.94 %	78.76 % 83.67 %	76.60 %4 bits	Distillation loss Normal loss	88.93 %^^88.50 % 88.80 %	88.74 %Table 30: Results without automatically redistributed bits2 bits	Distillation loss Normal loss	Quantiles Uniform 19.69 %	22.81 % 25.28 %	22.11 %4 bits	Distillation loss Normal loss	88.39 %^^88.67 % 88.43 %	88.44 %B Quantization is Equivalent to Asymptotically NormallyDistributed NoiseIn this section we will prove some results about the uniform quantization function, including thefact that is asymptotically normally distributed, see subsection B.1 below. Clearly, we refer to thestochastic version, see Section 2.1.
Table 30: Results without automatically redistributed bits2 bits	Distillation loss Normal loss	Quantiles Uniform 19.69 %	22.81 % 25.28 %	22.11 %4 bits	Distillation loss Normal loss	88.39 %^^88.67 % 88.43 %	88.44 %B Quantization is Equivalent to Asymptotically NormallyDistributed NoiseIn this section we will prove some results about the uniform quantization function, including thefact that is asymptotically normally distributed, see subsection B.1 below. Clearly, we refer to thestochastic version, see Section 2.1.
