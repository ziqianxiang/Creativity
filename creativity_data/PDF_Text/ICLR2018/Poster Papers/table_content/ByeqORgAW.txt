Table 1: Full batch loss for conjugate gradient Versions of ProxProp and BackProp after training for50 epochs, where “-” indicates that the algorithm diverged to NaN. The implicit ProxProP algorithmsremain stable for a significantly wider range of step sizes.
