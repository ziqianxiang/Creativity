Table 1: Stacked MNIST: Number of generated classes out of 1000 possible combinations, and the reverse KLdivergence score. The results are averaged over 10 runs.
Table 2: CIFAR10: MSE between target images from the train and test set and the best rank which consists ofthe percentage of minibatches containing target images that can be reconstructed with the lowest loss acrossvarious models. We use 20 different minibatches, each containing 64 target images. Increasing the number ofpast states for Chekhov GAN allows the model to better match the real images.
Table 3: CIFAR10: Target images from the test set are shown on the left. The images from each model thatbest resemble the target image are shown for different number of past states: 0 (GAN), 10 and 25 (ChekhovGAN ). The reconstruction MSE loss is indicated above each image.
Table 4: CelebA: Number of images from the test set that the auxiliary discriminator classifies as not real.
Table 5: Inception score (higher is better) and FID (lower is better) on Cifar10: The first section shows GANvariants and the second shows WGAN variants. The results are averaged over 8 runs.
Table 6: Stacked MNIST: Generator ArchitectureLayer	Number of outputsInPUt Z 〜N(0,1256厂	Fully Connected	512 (reshape to [-1, 4, 4, 64])Deconvolution	32Deconvolution	16Deconvolution	8Deconvolution	3Table 7: Stacked MNIST: Discriminator ArchitectureLayer	Number of outputsConvolution	4Convolution	8Convolution	16Flatten and Fully Connected	1We use a simplified version of the DCGAN architecture as suggested by Metz et al. (2016). It contains"deconvolutional layers" which are implemented as transposed convolutions. All convolutions anddeconvolutions use kernel size of 3 × 3 with a stride of 2. The weights are initialized using the Xavierinitialization Glorot & Bengio (2010). The activation units for the discriminator are leaky ReLUswith a leak of 0.3, whereas the generator uses standard ReLUs. We train all models for 20 epochswith a batch size of 32, using the RMSProp optimizer with batch normalization. The optimal learning
Table 7: Stacked MNIST: Discriminator ArchitectureLayer	Number of outputsConvolution	4Convolution	8Convolution	16Flatten and Fully Connected	1We use a simplified version of the DCGAN architecture as suggested by Metz et al. (2016). It contains"deconvolutional layers" which are implemented as transposed convolutions. All convolutions anddeconvolutions use kernel size of 3 × 3 with a stride of 2. The weights are initialized using the Xavierinitialization Glorot & Bengio (2010). The activation units for the discriminator are leaky ReLUswith a leak of 0.3, whereas the generator uses standard ReLUs. We train all models for 20 epochswith a batch size of 32, using the RMSProp optimizer with batch normalization. The optimal learningrate for GAN is 0.001, and for CHEKHOV GAN is 0.01. For all CHEKHOV GAN models we useregularization of 0.1 for the discriminator and 0.0001 for the generator. The regularization is L2regularization only on the fully connected layers. For K = 5, the increase parameter inc is set to 50.
Table 8: CIFAR10/CelebA Generator ArchitectureLayer	Number of outputsInPUt Z 〜N(0,1256厂	Fully Connected	32,768 (reshape to [-1, 4, 4, 512])Deconvolution	256Deconvolution	128Deconvolution	74Deconvolution	3Table 9: CIFAR10/CelebA Discriminator ArchitectureLayer	Number of outputsConvolution	64Convolution	128Convolution	256Convolution	5Γ2Flatten and Fully Connected	1As for MNIST, we apply batch normalization. The activation functions for the generator are ReLUs,whereas the discriminator uses leaky ReLUs with a leak of 0.3. The learning rate for all the models is0.0002 for both the generator and the discriminator and the updates are performed using the Adamoptimizer. The regularization for CHEKHOV GAN is 0.1 and the increase parameter inc is 10.
Table 9: CIFAR10/CelebA Discriminator ArchitectureLayer	Number of outputsConvolution	64Convolution	128Convolution	256Convolution	5Γ2Flatten and Fully Connected	1As for MNIST, we apply batch normalization. The activation functions for the generator are ReLUs,whereas the discriminator uses leaky ReLUs with a leak of 0.3. The learning rate for all the models is0.0002 for both the generator and the discriminator and the updates are performed using the Adamoptimizer. The regularization for CHEKHOV GAN is 0.1 and the increase parameter inc is 10.
Table 10: CIFAR10: MSE for other baselines on target images that come from the training set.
Table 11: CIFAR10: Comparison of average MSE on the training set between Unrolled GAN and CHEKHOVGAN for 5 and 10 unrolling steps and past states, respectively. The numbers for Unrolled GAN are taken fromMetz et al. (2016).
