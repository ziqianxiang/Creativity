Table 1: Success rate (%) and average distortion (RMS) of adversarial examples generated by dif-ferent attacks. On MNIST, the level of distortion in OptMargin examples is visible to humans,but the original class is still distinctly visible (see Figure 5 in the appendix for sample images).
Table 2: Accuracy of region classification and point classification on examples from different at-tacks. More effective attacks result in lower accuracy. The attacks that achieve the lowest accuracyfor each configuration of defenses are shown in bold. We omit comparison with OptStrong dueto its disproportionately high distortion and low attack success rate.
Table 3: False positive and false negative rates for the decision boundary classifier, trained on ex-amples from one attack and evaluated examples generated by the same or a different attack. Weconsider the accuracy under the worst-case benign/adversarial data split (all-benign if false positiverate is higher; all-adversarial if false negative rate is higher), and we select the best choice of basemodel and training set. These best-of-worst-case numbers are shown in bold and compared withCao & Gongâ€™s approach from Table 2.
Table 4: False positive and false negative rates for the decision boundary classifier, trained andevaluated on FGSM examples.
Table 5: Effectiveness of attacks on ImageNet. Reported in this table: average distortion (RMS) ofsuccessful examples, top-1 accuracy under point classification and region classification, and falsepositive and false negative rates of a decision boundary classifier trained on examples of the sameattack.
