Table 1: Single model perplexity on validation and test sets for the Penn Treebank language mod-eling task. Parameter numbers with åŽ» are estimates based upon our understanding of the model andwith reference to (Merity et al., 2016). Models noting tied use weight tying on the embedding andsoftmax weights. Our model, AWD-LSTM, stands for AvSGD Weight-Dropped LSTM.
Table 2: Single model perplexity over WikiText-2. Models noting tied use weight tying on theembedding and softmax weights. Our model, AWD-LSTM, stands for AvSGD Weight-DroppedLSTM.
Table 3: The sum total difference in loss (log perplexity) that a given word results in over allinstances in the validation data set of WikiText-2 when the continuous cache pointer is introduced.
Table 4: Comparison of AWD-LSTM and AWD-QRNN for the same model size on the PTB andWikiText-2 data sets.
Table 5: Model ablations for our best LSTM models reporting results over the validation and test seton Penn Treebank and WikiText-2. Ablations are split into optimization and regularization variants,sorted according to the achieved validation perplexity on WikiText-2.
