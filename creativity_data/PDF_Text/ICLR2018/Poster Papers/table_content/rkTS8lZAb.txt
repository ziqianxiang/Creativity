Table 1: Important weights and nonlinearities that ensureImportance weights for f -divergences			f -divergence	V (J)		W(X) = (∂f*∕∂T )(T (x))	GAN	-log(1 + e-y)	1	=eFΦ(X)		-1-e-TΦ	Jensen-Shannon	log2 - log (1 + e-y)	1	=eFΦ(X)		- 2-e-TΦ	KL	y + 1		e(Tφ(x)-1)	=eFΦ(X)Reverse KL	-e-y	1	— ———，—— tΦ (X)	eFΦ(X)Squared-Hellinger	1 - e-v/2	1	=pFφ(χ)		(1-Tφ(x))2	=eProof. Following the definition of the f -divergence and the convex conjugate, we have:Df (p∣IQθ) = Eqθ [f (篙)suptf?(t)	.
Table 2: Adversarial classification on CIFAR-10. All methods are BGAN with importance sampling(left) or REINFORCE (right) except for the baseline (cross-entropy) and Wasserstein GAN (WGAN)	Measure	Error(%)		Baseline	26.6		WGAN (CliPPing)	72.3		GAN Jensen-Shannon KL Reverse KL SqUared-Hellinger	IS 26.2 26.0 28.1 27.8 27.0	REINFORCE 27.1 27.7 28.0 28.2 28.0Our results are summarized in Table 2. Overall, BGAN performed similarly to the baseline on thetest set, with the REINFORCE method performing only slightly worse. For WGAN, despite ourbest efforts, we could only achieve an error rate of 72.3% on the test set, and this was after a total of600 epochs to train. Our efforts to train WGAN using gradient penalty failed completely, despite itworking with higher-dimension discrete data (see Appendix).
Table 3: Random samples drawn from a generator trained with the discrete BGAN objective. Themodel is able to successfully learn many important character-level English language patterns.
Table 4: Estimated Jensen-Shannon and KL-divergences and Wasserstein distance by a discrim-inator trained to maximize the respective lowerbound (lower is better). Numbers are estimatesaveraged ovwe 12 batches of 5000 samples with standard devations provided in parentheses. Alldiscriminators were trained using samples drawn from the softmax probabilities, with exceptionto an additional discriminator used to evaluate WGAN-GP where the softmax probabilities wereused directly. In general, BGAN out-performs WGAN-GP even when comparing the Wassersteindistances.
