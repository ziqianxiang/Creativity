Table 1: AlexNet top-1 validation set accuracy % as precision of activations (A) and weight(W) changes. All results are with end-to-end training					Table 2: accuracy weights (	AlexNet 2x-wide top-1 validation set % as precision of activations (A) and W) changes.				of the network from scratch. we did not experiment for.				- is a data-point							32b A	8b A	4bA	2bA	1bA		32b A	8b A	4bA	2b A	1bA32bW	57.2	54.3	54.4	52.7	-	32b W	60.5	58.9	58.6	57.5	52.08bW	-	54.5	53.2	51.5	-	8b W	-	59.0	58.8	57.1	50.84b W	-	54.2	54.4	52.4	-	4b W	-	58.8	58.6	57.3	-2b W	57.5	50.2	50.5	51.3	-	2b W	-	57.6	57.2	55.8	-1bW	56.8	-	-	-	44.2	1bW	-	-	-	-	48.3To re-gain the model accuracy while working with reduced-precision operands, we increase thenumber of filter maps in a layer. Although the number of raw compute operations increase withwidening the filter maps in a layer, the bits required per compute operation is now a fraction of whatis required when using full-precision operations. As a result, with appropriate hardware support,one can significantly reduce the dynamic memory requirements, memory bandwidth, computationalenergy and speed up the training and inference process.
Table 3: Compute cost of AlexNet 2x-wide vs. 1x-wide as preci-sion of activations (A) and weights (W) changes.
Table 4: ResNet-34 top-1 validation accuracy % and compute cost asprecision of activations (A) and weights (W) varies.
Table 5: Batch-normalized Inception top-1 validation accuracy % andcompute cost as precision of activations (A) and weights (W) varies.
