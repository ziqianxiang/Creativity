Table 1: Random rehearsal vs learning without forgetting. For LwF mode λ is a coefficient of theground truth probability distribution in the loss function (1)-(2). For random rehearsal mode λ is aportion of user training data in on-device training.
Table 2: Averaging vs transfer learning for server-side model update.
Table 3: Uploaded data analysisNumber of parameters	Size of the model	Nodes per round	Uploaded data per round4.57 ∙ 107	174.43Mb	10	1.70GbTable 4: Communication costs comparisonCommunication efficiency improving scheme	Perplexity	Uploaded data	Number of uploadsSeveral epochs of on-device training	-71.06-	-21.29Gb-	∏2	DGC (Lin et al.(2017))		72.24	21.85Gb	5.3 ∙ 104transmit only some part of data from devices to the server in a single round of averaging Lin et al.
Table 4: Communication costs comparisonCommunication efficiency improving scheme	Perplexity	Uploaded data	Number of uploadsSeveral epochs of on-device training	-71.06-	-21.29Gb-	∏2	DGC (Lin et al.(2017))		72.24	21.85Gb	5.3 ∙ 104transmit only some part of data from devices to the server in a single round of averaging Lin et al.
Table 5: Results of the Lilliefors testExperiment	1	2	3	4	5	6	7	8	9	10b	-15.8-	20.9	15.1	16.6	16.5	17.6	14.9	19.2	15.6	15.2^ C	3.25	5.64	2.02	2.48	2.70	4.19	1.47	3.31	1.65	1.83KS statistic	0.49	0.91	0.48	0.62	0.83	0.59	1.39	0.41	0.93	0.51Experiment		12	~~L3~	14	~~L5~	~~L6~	~~∏~	~~L8~	19	~o^Γb	-165^	14.4	19.5	18.2	16.2	17.2	17.3	14.8	17.1	20.5C	3.00	1.53	3.67	2.20	3.42	2.66	1.68	2.18	2.87	4.60KS statistic	0.76	0.89	0.66	0.94	0.67	0.85	0.73	0.97	0.65	0.94Table 6: Differential privacy resultsδ	10-4	10-5	10-6ε	0.67	0.83	0.994 ConclusionWe have presented our results in distributed fine-tuning of neural language models. We paid specialattention to preventing a catastrophic forgetting of the general language after a model fine-tuning onthe user devices. Our experiments showed that the performance of an initial model of the generalEnglish on user data can be improved significantly almost without a performance degradation onthe standard English training data. We found that a combination of on-device training with randomrehearsal and server-side model averaging provides the best performance for such distributed fine-tuning. Users’ models were trained for the whole epoch that reduced communication costs while at
Table 6: Differential privacy resultsδ	10-4	10-5	10-6ε	0.67	0.83	0.994 ConclusionWe have presented our results in distributed fine-tuning of neural language models. We paid specialattention to preventing a catastrophic forgetting of the general language after a model fine-tuning onthe user devices. Our experiments showed that the performance of an initial model of the generalEnglish on user data can be improved significantly almost without a performance degradation onthe standard English training data. We found that a combination of on-device training with randomrehearsal and server-side model averaging provides the best performance for such distributed fine-tuning. Users’ models were trained for the whole epoch that reduced communication costs while atthe same time being quite fast - it took less than 3 minutes with a realistic assessment of volumeof the available user data. Finally, we provided an experimental evaluation of differential privacy ofour method and showed that the method has a reasonable level of differential privacy compared toother solutions. We still have to note that we provided an empirical estimation of differential privacywhich holds with some high probability but not almost surely.
