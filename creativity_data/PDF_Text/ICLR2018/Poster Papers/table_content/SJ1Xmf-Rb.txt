Table 2: State-of-the-art comparison on CIFAR-100, CUB-200, and AudioSet. The best。。〃 foreach dataset are in bold. Ωbase and。。〃 are normalized by the offline MLP baseline.
Table 3: FearNet performance when the location of the associated memory is known using an oracleversus using BLA.
Table 4: Multi-modal incrementallearning experiment. FearNet wastrained with various base-knowledgesets (column-header) and then incre-mentally trained on all remaining data.
Table 5: Memory requirements to trainCIFAR-100 and the amount of memorythat would be required if these modelswere trained up to 1,000 classes.
Table 6: Using a diagonal covariancematrix for FearNet’s class statisticsinstead of a full covariance matrix onCIFAR-100.
Table S1:	FearNet Training ParametersTable S2 shows the training parameters for the iCaRL framework used in this paper. We adaptedthe code from the author’s GitHub page for our own experiments. The ResNet-18 convolutionalneural network was replaced with a fully-connected neural network. We experimented with variousregularization strategies to increase the initial base-knowledge accuracy with weight decay working12Published as a conference paper at ICLR 2018the best. The values that are given as a range of values are the hyperparameter search spaces.
Table S2:	iCaRL Training ParametersTable S3 shows the training parameters for GeppNet and GeppNet+STM. Parameters not listed hereare the default parameters defined by Gepperth & Karaoguz (2016). The values that are given as arange of values are the hyperparameter search spaces.
Table S3:	GeppNet Training ParametersTable S4 shows the training parameters for the Fixed Expansion Layer (FEL). The number of unitsin the FEL layer is given byFEL Units = H +HK	⑹where His the number of units in the first hidden-layer and K is the maximum number of classesin the dataset. The values that are given as a range of values are the hyperparameter search spaces.
Table S4:	FEL Training ParametersA.2 iCaRL Performance with More ExemplarsTable S5 provides additional experimental results for when there are more exemplars per class(EPC) for the iCaRL framework. Rebuffi et al. (2017) used 20 EPC in their original paper; however,we increased the number to 100 EPC to see if storing more training data helped iCaRL. Althougha higher EPC does increase iCaRL performance, it still does not outperform FearNet. Note thatCUB-200 only has about 30 training samples per class, so iCaRL is storing the entire training setfor 100 EPC. Our main results use the default value of 20.
Table S5: iCaRL’s performance when the stored EPC is increased from 20 to 100.
Table S6: PerformanCe of different BLA variants.
