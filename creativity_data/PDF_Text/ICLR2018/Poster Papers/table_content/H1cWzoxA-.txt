Table 1: Experimental results for different methods on SNLI. | θ |: the number of parameters (excluding wordembedding part). Train Accu and Test Accu: the accuracies on training and test sets respectively.
Table 2: Time cost and memory consumption of the different methods on SNLI. Time(s)/epoch: averagetraining time (second) per epoch. Memory(MB): Training GPU memory consumption (Megabyte). InferenceTime(s): average inference time (second) for all dev data on SNLI With test batch size of 100.
Table 3: An ablation study of Bi-BloSAN. “Local” denotes the local context representations h and “Global”denotes the global context representations E. “Bi-BloSAN w/o mBloSA” equals to word embeddings directlyfollowed by a source2token attention and “Bi-BloSAN w/o mBloSA & source2token self-attn.” equals to wordembeddings plus a vanilla attention without q.
Table 4: Experimental results for different methods on modified SQuAD task.
Table 5: Experimental results for different methods on SICK sentence relatedness dataset. The reportedaccuracies are the mean of five runs (standard deviations in parentheses).
Table 6: Experimental results for different methods on various sentence classification benchmarks. The re-ported accuracies on CR, MPQA and SUBJ are the mean of 10-fold cross validation, the accuracies on TRECare the mean of dev accuracies of five runs, and the accuracies on SST-1 and SST-2 are the mean of test accu-racies of five runs. All standard deviations are in parentheses.
