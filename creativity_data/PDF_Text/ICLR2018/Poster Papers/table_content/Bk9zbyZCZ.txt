Table 1: Results of several different agent architectures on the “Goal-Search” environment. The “train”columns represents the number of mazes solved (in %) when sampling from the same distribution as usedduring training. The “test” columns represents the number of mazes solved when run on a set of held-out mazesamples which are guaranteed not to have been sampled during training.
Table 2: Doom results on mazes not observed during training for the three tasks: Indicator, Repeating andMinotaur. Acc stands for Accuracy and Rew for Reward. Accuracy for Indicator means % of correct goalsreached, while for Minotaur it means % of episodes where the agent successfully reached the goal and thenbacktracked to the beginning. Reward for Repeating is number of times correct goal was visited within theallotted time steps (+1 for correct goal, -1 for incorrect goal). Reward for Minotaur is +0.5 for reaching thegoal and then +1.0 for backtracking to start after reaching goal (max episode reward is +1.5). We tested onmaze sizes between [4,8] with 10 test mazes for each size. For each of the 50 total test mazes we ran 100episodes with random goal locations and averaged the result.
Table 3: Results on the three 3D Doom maze tasks for the fixed-frame Neural Map with ControllerLSTM. We can see that adding small compounding error does not largely affect the ability of theNeural Map to learn memory tasks and even has a beneficial effect for some tasks. Hyperparametersand architectures used were the same as presented in the main results.
Table 4:	Figure showing accuracy of a logistic model to determine indicator identity from the storedNeural MaP features.
Table 5:	Visitation scores of the Neural Map models which measure how much of a maze is exploredwithin a set time limit. We can see that the egocentric neural map explores more of the mazes thanthe allocentric model, exploring on average 77.6% of the test mazes. The allocentric neural mapexplores 71.6% of the test mazes. The LSTM is reported to provide a point of comparison.
