Table 1: Prior works on neural architectures for sequence tagging, and their corresponding designchoices.
Table 2: Example formatted sentence. To avoid clutter, [BOW] and [EOW] symbols are not shown.
Table 4: Evaluations on the test set of OntoNotes 5.0 English. (*) Training speed of CNN-LSTM-CRFmodel was measured with our own implementation of it.
Table 5: Effect of beam size in LSTM decoder. We used a single LSTM-LSTM-LSTM model, andevaluated on OntoNotes 5.0 English dataset.
