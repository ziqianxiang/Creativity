Table 1: Latent Algebra Score for CelebA (lower is better)4	Related WorkLearned latent representations often allow for vector space arithmetic to translate to semantic op-erations in the data space Radford et al. (2015); Larsen et al. (2015). Early observations showingthat the latent space of a GAN finds semantic directions in the data space (e.g. corresponding toeyeglasses and smiles) were made in Radford et al. (2015). Recent work has also focused on learn-ing better similarity metrics Larsen et al. (2015) or providing a finer semantic decomposition of thelatent space Donahue et al. (2017). As a consequence, the evaluation of current GAN models isoften done by sampling pair of points and linear interpolating between them in the latent space, orperforming other types of noise vector arithmetic Bojanowski et al. (2017). This results in samplingthe latent space from locations with very low probability mass. This observation was also madein White (2016) who suggested replacing linear interpolation with spherical linear interpolationwhich prevents diverging from the modelâ€™s prior distribution.
Table 2:	Straight traversal in generators with different latent space dimensionality on LSUN kutchenwith a normal (top) and gamma (bottom) prior.
Table 3:	Straight traversal in generators with different latent space dimensionality on LSUN bed-room with a normal (top) and gamma (bottom) prior.
Table 4: Straight traversal in generators with different latent space dimensionality on MNIST witha normal (top) and gamma (bottom) prior.
Table 5:	Straight traversal in generators with different latent space dimensionality on SVHN with anormal (top) and gamma (bottom) prior.
Table 6:	Straight traversal in generators with different latent space dimensionality on CIFAR10 witha normal (top) and gamma (bottom) prior.
