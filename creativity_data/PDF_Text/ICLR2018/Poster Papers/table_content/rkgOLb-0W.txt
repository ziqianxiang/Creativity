Table 1: BPC on the Penn Treebank test setoutperform baseline models. It is worth noting that HM-LSTM (Chung et al., 2016) also unsuper-visedly induce similar structure from data. But discrete operations in HM-LSTM make their trainingprocedure more complicated then ours.
Table 2: PPL on the Penn Treebank test set8Published as a conference paper at ICLR 2018Model	PPLPRPN	61.98- Parsing Net	64.42- Reading Net Attention	64.63- Predict Net Attention	63.65OUr 2-layerLSTM	65.81Table 3: Ablation test on the Penn Treebank. “- Parsing Net” means that we remove Parsing Net-work and replace Structured Attention with normal attention mechanism; “- Reading Net Attention”means that we remove Structured Attention from Reading Network, that is equivalent to replaceReading Network with a normal 2-layer LSTM; “- Predict Net Attention” means that we removeStructured Attention from Predict Network, that is equivalent to have a standard projection layer;“Our 2-layer LSTM” is equivalent to remove Parsing Network and remove Structured Attentionfrom both Reading and Predict Network.
Table 3: Ablation test on the Penn Treebank. “- Parsing Net” means that we remove Parsing Net-work and replace Structured Attention with normal attention mechanism; “- Reading Net Attention”means that we remove Structured Attention from Reading Network, that is equivalent to replaceReading Network with a normal 2-layer LSTM; “- Predict Net Attention” means that we removeStructured Attention from Predict Network, that is equivalent to have a standard projection layer;“Our 2-layer LSTM” is equivalent to remove Parsing Network and remove Structured Attentionfrom both Reading and Predict Network.
Table 4: PPL on the Text8 valid setIn Table 2, our results are comparable to the state-of-the-art methods. Since we do not have thesame computational resource used in (Melis et al., 2017) to tune hyper-parameters at large scale, weexpect that our model could achieve better performance after an aggressive hyperparameter tuningprocess. As shown in Table 4, our method outperform baseline methods. It is worth noticing thatthe continuous cache pointer can also be applied to output of our Predict Network without modifi-cation. Visualizations of tree structure generated from learned PTB language model are included inAppendix A. In Table 3, we show the value of test perplexity for different variants of PRPN, eachvariant remove part of the model. By removing Parsing Network, we observe a significant drop ofperformance. This stands as empirical evidence regarding the benefit of having structure informationto control attention.
Table 5: Parsing Performance on the WSJ10 datasetTable 5 summarizes the results. Our model significantly outperform the RANDOM baseline indicatea high consistency with human annotation. Our model also shows a comparable performance withCCM model. In fact our parsing network and CCM both focus on the relation between successivetokens. As described in Section 4.2, our model computes syntactic distance between all successivepair of tokens, then our parsing algorithm recursively assemble tokens into constituents accordingto the learned distance. CCM also recursively model the probability whether a contiguous subse-quences of a sentence is a constituent. Thus, one can understand how our model is outperformedby DMV+CCM and UML-DOP models. The DMV+CCM model has extra information from a de-pendency parser. The UML-DOP approach captures both contiguous and non-contiguous lexicaldependencies (Bod, 2006).
