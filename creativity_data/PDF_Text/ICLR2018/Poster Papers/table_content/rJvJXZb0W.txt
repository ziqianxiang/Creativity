Table 1: Comparison of sentence representations on downstream tasks. The baseline methods areGloVe bag-of-words representation, De-noising auto-encoders and FastSent from Hill et al. (2016),the paragraph vector distributed memory model (Le & Mikolov, 2014), skip-thought vectors (Kiroset al., 2015) and the CNN model of Gan et al. (2016). Training times indicated using * refers to CPUtrained models and * assumes concatenated representations are trained independently. Performancefigures for SDAE, FastSent and ParagraphVec were obtained from Hill et al. (2016). Higher numbersare better in all columns except for the last (MSE). The table is divided into different sections.
Table 2: Comparison against supervised representation learning methods on downstream tasks.
Table 3: Comparison against task-specific supervised models. The models are AdaSent (Zhao et al.,2015), CNN (Kim, 2014), TF-KLD (Ji & Eisenstein, 2013) and Dependency-Tree LSTM (Tai et al.,2015). Note that our performance values correspond to a linear classifier trained on fixed pre-trainedembeddings, while the task-specific methods are tuned end-to-end.
Table 4: Image-caption retrieval. The purely supervised models are respectively from (Karpathy &Fei-Fei, 2015), (Klein et al., 2015), (Mao et al., 2014) and (Vendrov et al., 2015). Best pre-trainedrepresentations and best task-specific methods are highlighted.
Table 5: Nearest neighbors retrieved by the skip-thought model (ST) and our model (QT).
Table 6: Analogy task - Retrieval performance.
Table 7: Analogy task - Qualitative results. In each table cell the first three sentences form the queryand the last sentence is the answer retrieved by the model.
Table 8: Comparison (Pearson score) of sentence representations on Semantic Textual Similarity(STS14) tasks. SDAE, CBOW, Skipgram and FastSent are from Hill et al. (2016). The other base-lines are Skip-Thoughts (Kiros et al., 2015) and Siamese CBOW (Kenter et al., 2016). QT (RNN)and QT (BoW) are our models trained with RNN and BoW encoders, respectively.
Table 9: Training time and performance for different embedding sizes. The reported performance isthe mean accuracy over the classification benchmarks (MSRP, TREC, MR, CR, SUBJ, MPQA).
