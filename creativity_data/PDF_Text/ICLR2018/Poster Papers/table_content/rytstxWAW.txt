Table 1: Dataset StatisticsDataset	Nodes	Edges	Classes	Features	Training/Validation/TestCora	2, 708	5, 429	7	1, 433	1, 208/500/1, 000Pubmed	19, 717	44, 338	3	500	18, 217/500/1, 000Reddit	232, 965	11, 606, 919	41	602	152, 410/23, 699/55, 334Implementation details are as following. All networks (including those under comparison) containtwo layers as usual. The codes of GraphSAGE and GCN are downloaded from the accompanywebsites and the latter is adapted for FastGCN. Inference with FastGCN is done with the full GCNnetwork, as mentioned in Section 3.2. Further details are contained in the appendix.
Table 2: Benefit of precomputing AHâ‘¼ forthe input layer. Data set: Pubmed. Train-ing time is in seconds, per-epoch (batch size1024). Accuracy is measured by using microF1 score.
Table 3: Further comparison of per-batch training time (in seconds) with new implementation ofGraphSAGE for small graphs. The new implementation is in PyTorch whereas the rest are in Ten-sorFlow.
Table 4: Total training time (in seconds).
Table 5: Total training time and test accuracy for Cora and Pubmed, original data split. Time is inseconds.
