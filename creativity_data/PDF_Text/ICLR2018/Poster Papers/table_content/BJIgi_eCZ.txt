Table 1: A summarized view on the fusion processes used Figure 2: A conceptual architecture il-in several state-of-the-art architectures.	lustrating recent advances in MRC.
Table 2: The performance of FusionNet and competing approaches on SQuAD hidden test set at the time of writing (Oct. 4th, 2017).		Table 4: Comparison on AddOneSent. (S: Single model, E: Ensemble)			adversarial datasets during training. The results for AddSent and AddOneSent are shown in Table 3and Table 4, respectively.* 2From the results, we can see that our models not only perform well on the original SQuAD dataset,but also outperform all previous models by more than 5% in EM score on the adversarial datasets.
Table 5: Comparison of different attention func-tions S(x, y) on SQuAD dev set.
Table 6: Comparison of different configurationsdemonstrates the effectiveness of history-of-word.
Table 7: Ablation study on input vectors (GloVeand CoVe) for SQUAD dev set.
Table 8: Additional results for AddSent. (S: Sin-gle model, E: Ensemble)642-040	41	42	43	44	45	46FigUre 7: Single model performance (EM) onAddSent over 10 training rUns. (dashed verticalline indicates previoUs best performance)Configuration	EM/F1FusionNet (S, 10-run best)	54.8 / 60.9FusionNet (S, 10-run mean)	53.1 /59.3FusionNet (S, without CoVe)	55.2/61.2FUSionNet (E)		54.7 / 60.7Previous SotA (E)	48.7/55.3Table 9: Additional results for AddOneSent. (S:Single model, E: Ensemble)We have conducted experiments on input vectors (GloVe and CoVe) for the original SQuAD asshown in Table 7. From the ablation study, we can see that FusionNet outperforms previous state-
Table 9: Additional results for AddOneSent. (S:Single model, E: Ensemble)We have conducted experiments on input vectors (GloVe and CoVe) for the original SQuAD asshown in Table 7. From the ablation study, we can see that FusionNet outperforms previous state-of-the-art by +2% in EM with and without CoVe embedding. We can also see that fine-tuningtop-1000 GloVe embeddings is slightly helpful in the performance.
Table 10: The performance (accuracy) of ESIM with our proposed attention enhancement onMultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)BiLSTM are concatenated to yield {uiP}, {ujH} ⊂ R600. The final hidden vector for the P, H pairis obtained byhp,H = [ n X UP ； max(up,…，UP )； -m X UH ； max(uH,..., uH)].
