Table 1: An illustration of the multi-level method with 3 cycles. The ResNet model has 3stages; # Residual Blocks column represents the number of blocks in each stage. In cycle 1, thetraining starts with a 2-2-2 model using h = 1. After N1 training steps, the first interpolation hap-pens: the model becomes 4-4-4, and the step size is halved to 0.5. Similarly, N2 training steps later,the second interpolation doubles the number of blocks to 8-8-8 and halves h to 0.25. Cycle 3 lastsfor N3 training steps.
Table 2: Number of interpolations vs theoretical time saved, relative to the full model. Theo-retically, time saved is monotonically increasing as the number of interpolation increases, but themarginal benefit is diminishing.
Table 3: Main multi-level method results for ResNets with different depths. The model namewith i corresponds to the multi-level method. Our multi-level training method achieves superior oron-par accuracy with the last cycle model while saving about 40% of training time. The unit oftraining time is a minute.
Table 4: Main multi-level method results for Wide ResNets (WResNets) with different depths.
Table 5: The number of parameters for each network model.
