Table 1: Training configuration and ImageNet-1K classification accuracyModels 		Batch-size / Epochs 		Baseline		Mixed precision DFP16			Top-1	Top-5	Top-1	Top-5ResNet-50	1024/90	75.70%	92.78%	75.77%	92.84%GoogLeNet-v1	1024/ 80	69.26%	89.31%	69.34%	89.31%VGG-16	256/60	68.23%	88.47%	68.12%	88.18%AlexNet	1024/88	57.43%	80.65%	56.94%	80.06%5.1	Accuracy Results for CNNsWe trained several CNNs for the ImageNet-1K classification task using mixed precision DFP16:AlexNet Krizhevsky et al. (2012), VGG-16 Simonyan & Zisserman (2014), GoogLeNet-v1 Szegedyet al. (2015), ResNet-50 He et al. (2016). We use exactly the same batch-size and hyper-parameterconfiguration for both the baseline FP32 and DFP16 training runs (Table.1). In both cases, the modelsare trained from scratch using synchronous SGD on multiple nodes. In our experiments the firstconvolution layer (C1) and the fully connected layers are in FP32 (constituting about 5 - 10% ofcompute for modern CNNs). Table.1 shows ImageNet-1K classification accuracies, training withDFP16 achieve SOTA accuracy for all four models and in several cases even better than the baselinefull precision result.
