Table 1: Learning ISS sparsity from scratch in stacked LSTMs.
Table 2: Learning ISS sparsity from scratch in RHNs.				Method	λ	Perplexity (validate, test)	RHN width	Parameter #baseline	0.0	(67.9, 65.4)	830	23.5M* ISS*	0.004	(67.5, 65.0)	726	18.9M* ISS*	0.005	(68.1, 65.4)	517	11.1M* ISS*	0.006	(70.3, 67.7)	403	7.6M* ISS*	0.007	(74.5, 71.2)	328	5.7M* All dropout ratios		are multiplied by 0.6×.		4.1.2	Extension to Recurrent Highway NetworksRecurrent Highway Networks (RHN) (Zilly et al. (2017)) is a class of state-of-the-art recurrent mod-els, which enable “step-to-step transition depths larger than one”. In a RHN, we define the numberof units per layer as RHN width. Specifically, we select the “Variational RHN + WT” model in Table1 of Zilly et al. (2017) as the baseline. It has depth 10 and width 830, with totally 23.5M parameters.
Table 5: TheISSinBiDAELSTM name	Dimensions of weight matrix	Receivers of hidden states	Size of “ISS weight group”ModFwd1	900 × 400	ModFwd2 ModBwd2	4800ModBwd1	900 × 400	ModFwd2 ModBwd2	4800		OutFwd	ModFwd2	300 × 400	OutBwd	3201		logit layer for start index			OutFwd	ModBwd2	300 × 400	OutBwd	3201		logit layer for start index	OutFwd	1500 × 400	logit layer for end index	6401OutBwd	1500 × 400	logit layer for end index	640113Published as a conference paper at ICLR 2018Appendix C	Histogram of vector lengths of ISS weight groups inBIDAFModFwdI	ModBwdI	ModFwd2ModBwd2	OutFwd	OutBwdFigure 6: Histogram of vector lengths of “ISS weight groups” in BiDAF. The ISS-Iearned BiDAFis the one in the third row of Table 4 with EM 66.32 and F1 76.22. Using our approach, the lengths
