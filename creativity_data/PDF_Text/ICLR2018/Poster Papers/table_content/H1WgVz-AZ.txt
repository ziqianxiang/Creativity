Table 1: Test F1 when comparing methods on multi-label classification datasets.
Table 2: Training and test-time inference speed comparison (examples/sec).
Table 3: Comparison of SPEN hinge losses and showing the impact of retuning (Twitter POSvalidation accuracies). Inference networks are trained with the cross entropy term.
Table 4: Twitter POS accuracies of BLSTM, CRF, and SPEN (InfNet), using our tuned SPENconfiguration (slack-rescaled hinge, inference network trained with cross entropy term). Thoughslowest to train, the SPEN matches the test-time speed of the BLSTM while achieving the highestaccuracies.
Table 5: Twitter POS validation/test accuracies when adding tag language model (TLM) energy termto a SPEN trained with margin-rescaled hinge.
Table 6: Comparison of test-time inference algorithms for a trained CRF (Twitter POS tagging). Weshow the test accuracy for the inference network setting that does best on validation. All inferencenetworks use the same architecture and therefore have essentially the same speed.
Table 7: Statistics of the multi-label classification datasets.
Table 8: Development F1 for Bookmarks when comparing hinge losses for SPEN (InfNet) andwhether to retune the inference network.
Table 9: Comparison of inference network stabilization terms and showing impact of retuning whentraining SPENs with margin-rescaled hinge (Twitter POS validation accuracies).
Table 10: Examples of improvements in Twitter POS tagging when using tag language model (TLM).
Table 11: Named entity recognition F1 of BLSTM, CRF, and SPEN (InfNet) with slack-rescaledhinge where inference networks used cross entropy stabilization term. Though slowest to train, theSPEN matches the test-time speed of the BLSTM while improving F1 by 2 points, though it lagsbehind the CRF.
