Table 1: Evaluation of the unsupervised learned features by measuring the classification accuracythat they achieve when we train a non-linear object classifier on top of them. The reported resultsare from CIFAR-10. The size of the ConvB1 feature maps is 96 × 16 × 16 and the size of the restfeature maps is 192 × 8 × 8.
Table 2: Exploring the quality of the self-supervised learned features w.r.t. the number of recognizedrotations. For all the entries we trained a non-linear classifier with 3 fully connected layers (similarto Table 1) on top of the feature maps generated by the 2nd conv. block of a RotNet model with 4conv. blocks in total. The reported results are from CIFAR-10.
Table 3: Evaluation of unsupervised feature learning methods on CIFAR-10. The Supervised NINand the (Ours) RotNet + conv entries have exactly the same architecture but the first was trained fullysupervised while on the second the first 2 conv. blocks were trained unsupervised with our rotationprediction task and the 3rd block only was trained in a supervised manner. In the Random Init. +conv entry a conv. classifier (similar to that of (Ours) RotNet + conv) is trained on top of two NINconv. blocks that are randomly initialized and stay frozen. Note that each of the prior approacheshas a different ConvNet architecture and thus the comparison with them is just indicative.
Table 4: Task Generalization: ImageNet top-1 classification with non-linear layers. We com-pare our unsupervised feature learning approach with other unsupervised approaches by trainingnon-linear classifiers on top of the feature maps of each layer to perform the 1000-way ImageNetclassification task, as proposed by Noroozi & Favaro (2016). For instance, for the conv5 featuremap we train the layers that follow the conv5 layer in the AlexNet architecture (i.e., fc6, fc7, andfc8). Similarly for the conv4 feature maps. We implemented those non-linear classifiers with batchnormalization units after each linear layer (fully connected or convolutional) and without employ-ing drop out units. All approaches use AlexNet variants and were pre-trained on ImageNet withoutlabels except the ImageNet labels and Random entries. During testing we use a single crop and donot perform flipping augmentation. We report top-1 classification accuracy.
Table 5: Task Generalization: ImageNet top-1 classification with linear layers. We compareour unsupervised feature learning approach with other unsupervised approaches by training logisticregression classifiers on top of the feature maps of each layer to perform the 1000-way ImageNetclassification task, as proposed by Zhang et al. (2016a). All weights are frozen and feature maps arespatially resized (with adaptive max pooling) so as to have around 9000 elements. All approachesuse AlexNet variants and were pre-trained on ImageNet without labels except the ImageNet labelsand Random entries.
Table 6: Task & Dataset Generalization: Places top-1 classification with linear layers. Wecompare our unsupervised feature learning approach with other unsupervised approaches by traininglogistic regression classifiers on top of the feature maps of each layer to perform the 205-way Placesclassification task (Zhou et al., 2014). All unsupervised methods are pre-trained (in an unsupervisedway) on ImageNet. All weights are frozen and feature maps are spatially resized (with adaptive maxpooling) so as to have around 9000 elements. All approaches use AlexNet variants and were pre-trained on ImageNet without labels except the Place labels, ImageNet labels, and Random entries.
Table 7: Task & Dataset Generalization: PASCAL VOC 2007 classification and detection re-sults, and PASCAL VOC 2012 segmentation results. We used the publicly available testingframeworks of Krahenbuhl et al. (2015) for classification, of Girshick(2015) for detection, andof Long et al. (2015) for segmentation. For classification, we either fix the features before conv5(column fc6-8) or we fine-tune the whole model (column all). For detection we use multi-scaletraining and single scale testing. All approaches use AlexNet variants and were pre-trained on Ima-geNet without labels except the ImageNet labels and Random entries. After unsupervised training,we absorb the batch normalization units on the linear layers and we use the weight rescaling tech-nique proposed by Krahenbuhl et al. (2015) (which is common among the unsupervised methods).
Table 8: Per class PASCAL VOC 2007 detection performance. As usual, we report the averageprecision metric. The results of the supervised model (i.e., ImageNet labels entry) come from Doer-sch et al. (2015).
Table 9: Per class CIFAR-10 classification accuracy.
