Table 1: Techniques in Deep Gradient CompressionTechniques	Gradient Dropping (Aji & Heafield, 2017)	Deep Gradient Compression	Reduce Bandwidth	Ensure Convergence	Overcome Staleness						Improve Accuracy	Maintain Convergence IterationsGradient Sparsification	X	X	X	-	-	-Local Gradient Accumulation	X	X	-	X	-	-Momentum Correction	-	X	-	-	X	-Local Gradient Clipping	-	X	-	X	-	XMomentum Factor Masking	-	X	-	-	X	XWarm-up Training	-	X	-	-	X	Xrate during the first several epochs, we exponentially increase the gradient sparsity from a relativelysmall value to the final value, in order to help the training adapt to the gradients of larger sparsity.
Table 2: ResNet-110 trained on Cifar10 Dataset# GPUs in total	Batchsize in total per iteration	Training Method	Top 1 Accuracy		Baseline	93.75%4	128	Gradient Dropping (Aji & Heafield, 2017)	92.75%	-1.00%		Deep Gradient Compression	93.87%	+0.12%		Baseline	92.92%8	256	Gradient Dropping (Aji & Heafield, 2017)	93.02%	+0.10%		Deep Gradient Compression	93.28%	+0.37%		Baseline	93.14%16	512	Gradient Dropping (Aji & Heafield, 2017)	92.93%	-0.21%		Deep Gradient Compression	93.20%	+0.06%		Baseline	93.10%32	1024	Gradient Dropping (Aji & Heafield, 2017)	92.10%	-1.00%		Deep Gradient Compression	93.18%	+0.08%Table 3: Comparison of gradient compression ratio on ImageNet DatasetModel	Training Method	Top-1 Accuracy	Top-5 Accuracy	Gradient Size	Compression Ratio	Baseline	58.17%	80.19%	232.56 MB	1×AlexNet	TernGrad (Wen etal., 2017)	57.28% (-0.89%)	80.23% (+0.04%)	1- 29.18 MB	8 X	Deep Gradient Compression	58.20% (+0.03%)	80.20% (+0.01%)	2 0.39 MB	597 X	Baseline	75.96	92.91%	97.49 MB	ΓX
Table 3: Comparison of gradient compression ratio on ImageNet DatasetModel	Training Method	Top-1 Accuracy	Top-5 Accuracy	Gradient Size	Compression Ratio	Baseline	58.17%	80.19%	232.56 MB	1×AlexNet	TernGrad (Wen etal., 2017)	57.28% (-0.89%)	80.23% (+0.04%)	1- 29.18 MB	8 X	Deep Gradient Compression	58.20% (+0.03%)	80.20% (+0.01%)	2 0.39 MB	597 X	Baseline	75.96	92.91%	97.49 MB	ΓXResNet-50	Deep Gradient Compression	76.15 (+0.19%)	92.97% (+0.06%)	0.35 MB	277 XSpeech Recognition The AN4 dataset contains 948 training and 130 test utterances (Acero, 1990)while Librispeech corpus contains 960 hours of reading speech (Panayotov et al., 2015). We useDeepSpeech architecture without n-gram language model, which is a multi-layer RNN following astack of convolution layers (Hannun et al., 2014). We train a 5-layer LSTM of 800 hidden units perlayer for AN4, and a 7-layer GRU of 1200 hidden units per layer for LibriSpeech, with Nesterovmomentum SGD and gradient clipping, while learning rate anneals every epoch. The warm-up periodfor DGC is 1 epoch out of 80 epochs.
Table 4: Training results of language modeling and speech recognition with 4 nodesTask	Language Modeling on PTB			Speech Recognition on LibriSPeeCh			Training Method	Perplexity	Gradient Size	Compression Ratio	Word Error Rate (WER)		Gradient Size	Compression Ratio				test-clean	test-other		Baseline	72.30	194.68 MB	Γ×	-9.45%	27.07%	488.08 MB	Γ×Deep Gradient Compression	72.24 (-0.06)	0.42 MB	462 ×	9.06% (-0.39%)	27.04% (-0.03%)	0.74 MB	608 Xbetter compression than Terngrad with no loss of accuracy. For ResNet-50, the compression ratio isslightly lower (277× vs. 597×) with a slight increase in accuracy.
