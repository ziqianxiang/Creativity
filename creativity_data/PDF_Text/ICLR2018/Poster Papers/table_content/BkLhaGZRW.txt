Table 1: BRE on various architectures: no-BRE is the baseline in each case; with BRE weight inother cases is set to 1.; single and multi signify whether BRE is applied on one layer in the middleof D or multiple (see Appendix A for more details); ln for layer normalization in G and D (defaultis batchnorm); tanh means the softsign nonlinearity in BRE is replaced by tanh.
Table 2: Semi supervised learning on CIFAR10: feature matching (FM) from Salimans et al.
Table 3: Reconstruction as an auxiliary task worsens results. Î»recon is the weight of the l2 recon-struction loss term. With both batch or layer normalization, reconstruction auxiliary task hurts thefinal results.
