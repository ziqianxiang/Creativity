Table 1: Comparison of different coding approaches. To support N basis vectors, a binary codewill have N/2 bits and the embedding computation is a summation over N/2 vectors. For thecompositional approach with M codebooks and K codewords in each codebook, each code hasM log2 K bits, and the computation is a summation over M vectors.
Table 2: Reconstruction loss and the size of embedding layer (MB) of difference settingsTo see how the reconstructed loss translates to the classification accuracy, we train the sentimentanalysis model for different settings of code schemes and report the results in Table 3. The baselinemodel using 75k GloVe embeddings achieves an accuracy of 87.18 with an embedding matrix using78 MB of storage. In this task, forcing a high compression rate with iterative pruning degrades theclassification accuracy.
Table 3: Trade-off between the model performance and the size of embedding layer on IMDBsentiment analysis taskWe also show the results using normalized product quantization (NPQ) (Joulin et al., 2016). Wequantize the filtered GloVe embeddings with the codes provided by the authors, and train the modelsbased on the quantized embeddings. To make the results comparable, we report the codebook sizein numpy format. For our proposed methods, the maximum loss-free compression rate is achievedby a 16 × 32 coding scheme. In this case, the total size of the embedding layer is 1.23 MB, whichis equivalent to a compression rate of 98.4%. We also found the classification accuracy can besubstantially improved with a slightly lower compression rate. The improved model performancemay be a byproduct of the strong regularization.
Table 4: Trade-off between the model performance and the size of embedding layer in machinetranslation tasks6	Qualitative Analysis6.1	Examples of Learned CodesIn Table 5, we show some examples of learned codes based on the 300-dimensional uncased GloVeembeddings used in the sentiment analysis task. We can see that the model learned to assign similarcodes to the words with similar meanings. Such a code-sharing mechanism can significantly reducethe redundancy of the word embeddings, thus helping to achieve a high compression rate.
Table 5: Examples of learned compositional codes based on GloVe embedding vectors6.2	Analysis of Code EfficiencyBesides the performance, we also care about the storage efficiency of the codes. In the ideal situation,all codewords shall be fully utilized to convey a fraction of meaning. However, as the codes areautomatically learned, it is possible that some codewords are abandoned during the training. Inextreme cases, some “dead” codewords can be used by none of the words.
Table 6: Examples of words sharing same codes when using a 8 × 8 code decompositionB Appendix: Semantics of CodesIn order to see whether each component captures semantic meaning. we learned a set of codes usinga 3 x 256 coding scheme, this will force the model to decompose each embedding into 3 vectors.
Table 7: Some code examples using a 3 × 256 coding scheme.
