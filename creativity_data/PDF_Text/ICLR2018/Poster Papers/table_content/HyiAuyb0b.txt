Table 1: Calibration against published results on standard environments. We report the averagescore at the end of an episode for Atari games, health for the Navigation scenario, and frags for theBattle scenarios. In all cases, higher is better.
Table 3: Separate training ofperception and control on theBattle scenario. Higher is bet-ter.
Table S1:	Difference between using multiple or constant rollouts within one train step.
Table S2:	Performance of 20-step Q and QMC with a pretrained and frozen perception, higher isbetter.
Table S3:	Difference between using multiple or constant rollouts within one train step.
Table S4: Network architecture of QMC for l actions.
