Table 1: Adversarial examples of MNIST. The top row shows images from original test data, andthe others show corresponding adversaries generated by FGSM against LeNet and our approachagainst both RF and LeNet. Predictions from the classifier are shown in the corner of each image.
Table 2: Adversarial examples against MLP classifier of LSUN by our approach. 4 originalTable 3: Textual Entailment. For a pair of premise (p : ) and hypothesis (h : ), we present thegenerated adversaries for three classifiers by perturbing the hypothesis (h0 : ). The last columnprovides the true label, followed by the changes in the prediction for each classifier.
Table 3: Textual Entailment. For a pair of premise (p : ) and hypothesis (h : ), we present thegenerated adversaries for three classifiers by perturbing the hypothesis (h0 : ). The last columnprovides the true label, followed by the changes in the prediction for each classifier.
Table 4: Machine Translation. “Adversary” that introduces the word “stehen” into the translation.
Table 5: “Adversaries” to find dropped verbs. The left column contains the original sentence s and its adversary s0, while the right contains their translations, with English translation in red.		Source Sentence (English)		Generated Translation (German)s:	People sitting in a dim restaurant eating.	Leute, die in einem dim Restaurant essen sitzen.
Table 6: Statistics of adversaries against models for both MNIST and TE. We include the average∆z for the adversaries and the proportion Where each classifier’s adversary has the largest ∆zcompared to the others for the same instance (significant With p < 0.0005 using the sign test). Thehigher values correspond to stronger robustness, as is demonstrated by higher test accuracy.
Table 7: Pilot study with MNIST	Table 8: Pilot study with Textual Entailment	RF	LeNet	LSTM		TreeLSTMLooks handwritten?	0.88	0.71	Is adversary grammatical?	0.86	0.78Which closer to original?	0.87	0.13	Is it similar to the original?	0.81	0.58We now consider evaluation on a broader set of classifiers, and study the effect of changing hyper-parameters of models on the results (focusing on MNIST). We train a set of neural networks withone hidden layer by varying the number of neurons exponentially from 2 to 1024. In Figure 4a,we observe that the average ∆z of adversaries against these models has a similar trend as their testaccuracy. The generated adversaries for a single digit “3” in Figure 4d verify this observation: theadversaries become increasingly different from the original input as classifiers become more complex.
Table 9: Text perturbations. Examples are generated by perturbing the origins in semantic space.
Table 10: Textual Entailment. For a pair of premise (p : ) and hypothesis (h : ), we present thegenerated adversaries for three classifiers by perturbing the hypothesis (h0 : ). The last columnprovides the true label, followed by the changes in the prediction from each classifier.
Table 11: Machine Translation. “Adversaries” that introduce the word “stehen” into the Googletranslation system by perturbing English sentences.
Table 12: “Adversaries” that find dropped verbs in English-To-German translation. The leftcolumn contains the original sentence s and its adversary s0. The right column contains the translationsof s and s0 , with English translation provided for legibility.
