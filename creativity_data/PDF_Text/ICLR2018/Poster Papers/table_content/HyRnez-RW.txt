Table 1: TriviaQA results on the test set. EM stands for Exact Match accuracy.
Table 2: Model ablations on the full Wikipedia development set. For row labeled **, explanation provided in Section ยง4.3.		considerable decrease in perfor- mance, signifying the effect of using a multi-loss architecture. It is unclear if combining the two submodels in level 1 into a singlefeed-forward network that is a function of the question, span and short context (3-Level Cascade,Combined Level 1) is significantly better than our final model. Although we found it could obtainhigh results, it was less consistent across different runs and gave lower scores on average (49.30)compared to our approach averaged over 4 runs (51.03). Finally, the last three rows show the resultsof using only smaller components of our model instead of the entire architecture. In particular, ourmodel without the aggregation submodel (Level 1 + Level 2 Only) performs considerably worse,showing the value of aggregating multiple mentions of the same span across the document. As ex-pected, the level 1 only models are the weakest, showing that attending to the document is a powerfulmethod for reading comprehension. Figure 3 (left) shows the behavior of the k-best predictions ofthese smaller components. While the difference between the level 1 models becomes more enhancedwhen considering the top-k candidates, the difference between the model without the aggregationsubmodel (Level 1 + Level 2 Only) and our full model is no longer significant, indicating that theformer might not be able to distinguish between the best answer candidates as well as the latter.
Table 3: Example predictions from different levels of our model. Evidence context and aggrega-tion are helpful for model performance. The model confuses between entities of the same type,particularly in the lower levels.
