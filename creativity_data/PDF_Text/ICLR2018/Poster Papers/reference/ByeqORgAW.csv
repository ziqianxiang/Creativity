title,year,conference
 Optimization methods for large-scale machinelearning,2016, arXiv preprint arXiv:1606
 Entropy-SGD: Biasing gradi-ent descent into wide valleys,2017, In Proceedings of the 5th International Conference on LearningRepresentations
 DeepRelaxation: partial differential equations for optimizing deep neural networks,2017, arXiv preprintarXiv:1704
 Identifying and attacking the saddle point problem in high-dimensional non-convex opti-mization,2014, In Proceedings of the 27th International Conference on Neural Information ProcessingSystems
 Gradient flow in recurrent nets: the difficultyof learning long-term dependencies,2001, In Field Guide to Dynamical Recurrent Networks
 Adam: A method for stochastic optimization,2015, In Proceedingsof the 3rd International Conference on Learning Representations
 ImageNet classification with deep convolu-tional neural networks,2012, In Proceedings ofthe 25th International Conference of Neural InformationProcessing Systems
 On optimization methods for deeplearning,2011, In Proceedings of The 28th International Conference on Machine Learning
 Proximite et dualite dans un espace hilbertien,1965, Bulletin de la Societemathematique de France
 Learning representations by back-propagating errors,1986, Nature
 Mastering the game of go withdeep neural networks and tree search,2016, Nature
 On the importance of initial-ization and momentum in deep learning,2013, In Proceedings of the 30th International Conference onInternational Conference on Machine Learning
 Trainingneural networks without gradients: A scalable ADMM approach,2016, In Proceedings of the 33rdInternational Conference on Machine Learning
