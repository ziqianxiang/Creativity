title,year,conference
 Extremely large minibatch sgd: Training resnet-50 on imagenet in 15 minutes,2017, arXiv preprint arXiv:1711
 Optimization methods for large-scale machinelearning,2016, arXiv preprint arXiv:1606
 Sample size selection in opti-mization methods for machine learning,2012, Mathematical programming
 Entropy-SGD: Biasinggradient descent into wide valleys,2016, arXiv preprint arXiv:1611
 Automated inference with adaptivebatches,2017, In Artificial Intelligence and Statistics
 Large scale distributed deep networks,2012, In Advances inneural information processing systems
 Flat minima,1997, Neural Computation
 In-datacenter performance analy-sis of a tensor processing unit,2017, In Proceedings of the 44th Annual International Symposium onComputer Architecture
 On large-batch training for deep learning: Generalization gap and sharp minima,2016, arXivpreprint arXiv:1609
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Stochastic modified equations and adaptive stochasticgradient algorithms,2017, arXiv preprint arXiv:1511
 SGDR: stochastic gradient descent with restarts,2016, arXiv preprintarXiv:1608
 Stochastic gradient descent as approximatebayesian inference,2017, arXiv preprint arXiv:1704
 Optimizing neural networks with kronecker-factored approximatecurvature,2015, In International Conference on Machine Learning
 Hogwild: A lock-free approach toparallelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 A bayesian perspective on generalization and stochastic gradientdescent,2017, arXiv preprint arXiv:1710
 Wide residual networks,2016, arXiv preprintarXiv:1605
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
