title,year,conference
 The Curse of Dimensionality forLocal Kernel Machines,2005, Technical Report 1258
 Weight Uncertainty inNeural Networks,2015, International Conference on Machine Learning (ICML)
 Kernel Methods for Deep Learning,2009, Advances in NeuralInformation Processing Systems (NIPS)
 Deep Gaussian Processes,2013, International Conferenceon Artificial Intelligence and Statistics (AISTATS)
 Avoiding Pathologies invery Deep Networks,2014, International Conference on Artificial Intelligence and Statistics (AISTATS)
 Practical Variational Inference for Neural Networks,2011, Advances in Neural InformationProcessing Systems (NIPS)
 Probabilistic BackProPagation for Scalable Learn-ing of Bayesian Neural Networks,2015, International Conference on Machine Learning (ICML)
 Black-box alPha divergence minimization,2016, International Conference onMachine Learning (ICML)
 Self-NormalizingNeural Networks,2017, CoRR
 AutoGP: Exploring thecapabilities and limitations of Gaussian Process models,2017, Conference on Uncertainty in ArtificialIntelligence (UAI)
 Elliptical Slice Sampling,2010, InternationalConference on Artificial Intelligence and Statistics (AISTATS)
 Bayesian Learning for Neural Networks,1996, Springer
 MCMC using Hamiltonian Dynamics,2010, Handbook of Markov Chain Monte Carlo
 Ex-ponential expressivity in Deep Neural Networks through Transient Chaos,2016, Advances in NeuralInformation Processing Systems (NIPS)
 Gaussian Processes for Machine Learning,2006, TheMIT Press
 Deep InformationPropagation,2017, International Conference on Learning Representations (ICLR)
 Hamiltonian Annealed Importance Sampling forpartition function estimation,2012, CoRR
 Computing with Infinite Networks,1998, Advances in Neural InformationProcessing Systems (NIPS)
 Stochastic VariationalDeep Kernel Learning,2016, Advances in Neural Information Processing Systems (NIPS)
	â–¡Proof of Proposition 2,1999, The idea of the proof is to chain Proposition 1 together across the layersof the network
