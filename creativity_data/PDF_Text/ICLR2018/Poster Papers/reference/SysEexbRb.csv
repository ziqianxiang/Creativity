title,year,conference
 Neural networks and principal component analysis: Learning from exampleswithout local minima,1989, Neural Networks
 Complex-valued autoencoders,2012, Neural Networks
 Open problem: The landscape of the loss surfaces ofmultilayer networks,2015, In Proc
 Identifying and attack-ing the saddle point problem in high-dimensional non-convex optimization,2014, In Proc
 Deep Learning,2016, MIT Press
 On the problem of local minima in backpropagation,1992, IEEE Transactions onPattern Analysis and Machine Intelligence
 Identity matters in deep learning,2017, Proc
 Deep learning without poor local minima,2016, In Proc
 Demystifying resnet,2016, Arxiv: 1611
 Depth creates no bad local minima,1702, ArXiv: 1702
 The loss surface of deep and wide neural networks,2017, In Proc
 Theoretical insights into the optimization landscapeof over-parameterized shallow neural networks,2017, ArXiv:1707
 No bad local minima: Data independent training error guarantees formultilayer neural networks,2016, ArXiv: 1605
 Exponentially vanishing sub-optimal local minima in multilayer neuralnetworks,2017, ArXiv:1702
 How regularization affects the critical points in linear net-works,2017, In Proc
 An analytical formula of population gradient for two-layered ReLU network and its appli-cations in convergence and critical point analysis,2017, In Proc
 On the local minima free condition of backpropagation learning,1995, IEEETransactions on Neural Networks
 Global optimality conditions for deep neural networks,2017, ArXiv:1707
 Recovery guarantees for one-hidden-layerneural networks,2017, In Proc
 Characterization of gradient dominance and regularity conditions for neuralnetworks,2017, ArXiv: 1710
