title,year,conference
 Tree-structured decoding with doubly-recurrent neuralnetworks,2016, 2016
 Layer normalization,2016, arXiv preprintarXiv:1607
 Learning deep architectures for ai,2009, Foundations and trendsR in MachineLearning
 A fast unified model for parsing and sentence understanding,2016, arXiv preprintarXiv:1603
 Generative incremental dependency parsing with neural networks,2015, InProceedings ofthe 53rd Annual Meeting ofthe Association for Computational Linguistics and the7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)
 Two experiments on learning probabilistic dependency gram-mars from corpora,1992, Department of Computer Science
 Immediate-head parsing for language models,2001, In Proceedings ofthe 39th AnnualMeeting on Association for Computational Linguistics
 Long short-term memory-networks for machinereading,2016, arXiv preprint arXiv:1601
 Hierarchical multiscale recurrent neural net-works,2016, arXiv preprint arXiv:1609
 Recurrentbatch normalization,2016, arXiv preprint arXiv:1603
 Recurrent neural networkgrammars,2016, arXiv preprint arXiv:1602
 Hierarchical recurrent neural networks for long-term dependen-cies,1996, 1996
 Improving neural language models with acontinuous cache,2016, arXiv preprint arXiv:1612
 An evaluation of parser robustness for ungrammatical sen-tences,2016, In EMNLP
 Long short-term memory,1997, Neural computation
 Tying word vectors and word classifiers: Aloss framework for language modeling,2016, arXiv preprint arXiv:1611
 Character-aware neural languagemodels,2016, In AAAI
 A generative constituent-context model for imProved gram-mar induction,2002, In Proceedings of the 40th Annual Meeting on Association for ComputationalLinguistics
 CorPus-based induction of syntactic structure: Models ofdePendency and constituency,2004, In Proceedings of the 42nd Annual Meeting on Association forComputational Linguistics
 Natural language grammar induction with a generativeconstituent-context model,2005, Pattern recognition
 Regularizing rnns by stabilizing activations,2015, arXiv preprintarXiv:1511
 Zoneout:Regularizing rnns by randomly Preserving hidden activations,2016, arXiv preprint arXiv:1606
 DeeP learning,2015, Nature
 Learning long-term dePendencies is notas difficult with narx recurrent neural networks,1998, Technical rePort
 Building a large annotatedcorPus of english: The Penn treebank,1993, Computational linguistics
 On the state of the art of evaluation in neural languagemodels,2017, arXiv preprint arXiv:1707
 Pointer sentinel mixturemodels,2016, arXiv preprint arXiv:1609
 Recurrentneural network based language model,2010, In Interspeech
 Efficient estimation of word rePresen-tations in vector sPace,2013, arXiv preprint arXiv:1301
 Deep learning in neural networks: An overview,2015, Neural networks
 Neural sequence chunkers,1991, Technical report
 End-to-end memory networks,2015, In Advancesin neural information processing systems
 Improved semantic representationsfrom tree-structured long short-term memory networks,2015, arXiv preprint arXiv:1503
 A latent variable model for generative dependency parsing,2010, InTrends in Parsing Technology
 Sequence-to-dependencyneural machine translation,2017, In Proceedings of the 55th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers)
 Generativeneural machine for tree structures,2017, arXiv preprint arXiv:1705
 Recurrenthighway networks,2016, arXiv preprint arXiv:1607
 Neural architecture search with reinforcement learning,2016, arXiv preprintarXiv:1611
