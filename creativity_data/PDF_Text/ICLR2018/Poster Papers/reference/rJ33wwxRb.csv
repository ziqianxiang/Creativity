title,year,conference
 Neural network learning: Theoretical foundations,2009, cambridgeuniversity press
 Spectrally-normalized margin bounds for neuralnetworks,2017, arXiv preprint arXiv:1706
 Practical recommendations for gradient-based training of deep architectures,2012, InNeural networks: Tricks ofthe trade
 Globally optimal gradient descent for a convnet with gaussianinputs,2017, arXiv preprint arXiv:1702
 Successes and failures of backpropagation: A theoretical,1997, Progressin Neural Networks: Architecture
 Relating data compression and learnability,1986, Technicalreport
 In search of the real inductive bias: On therole of implicit regularization in deep learning,2014, arXiv preprint arXiv:1412
 Norm-based capacity control in neuralnetworks,2015, In Conference on Learning Theory
 Exploring gener-alization in deep learning,2017, arXiv preprint arXiv:1706
 A pac-bayesian approach to spectrally-normalized margin bounds for neural networks,2017, arXiv preprintarXiv:1707
 The loss surface of deep and wide neural networks,2017, arXivpreprint arXiv:1704
 Understanding machine learning: From theory to algo-rithms,2014, Cambridge university press
 Theoretical insights into the optimiza-tion landscape of over-parameterized shallow neural networks,2017, arXiv preprint arXiv:1707
 Exponentially vanishing sub-optimal local minima in multilayerneural networks,2017, arXiv preprint arXiv:1702
 An analytical formula of population gradient for two-layered relu network and itsapplications in convergence and critical point analysis,2017, arXiv preprint arXiv:1703
 Robustness and generalization,2012, Machine learning
 Understandingdeep learning requires rethinking generalization,2016, arXiv preprint arXiv:1611
 Recovery guaranteesfor one-hidden-layer neural networks,2017, arXiv preprint arXiv:1706
1	Missing Proofs for Section 59,2018,1
