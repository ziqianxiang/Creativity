title,year,conference
 Acoustical and environmental robustness in automatic speech recognition,1990, In Proc
 Sparse communication for distributed gradient descent,2017, In EmpiricalMethods in Natural Language Processing (EMNLP)
 Qsgd: Randomized quantization forcommunication-optimal stochastic gradient descent,2016, arXiv preprint arXiv:1610
 Learning long-term dependencies with gradient descent isdifficult,1994, IEEE transactions on neural networks
 Efficient algorithmsfor all-to-all communications in multiport message-passing systems,1997, IEEE Transactions on parallel anddistributed systems
 Project adam: Building anefficient and scalable deep learning training system,2014, In OSDI
 Introduction to algorithms,2009, MIT press
 Large scale distributed deep networks,2012, In Advances in neural informationprocessing systems
 ImageNet: A Large-Scale Hierarchical ImageDatabase,2009, In CVPR09
 Communication quantization for data-parallel training of deep neural networks,2016, In Proceedings of the Workshop on Machine Learning in HighPerformance Computing Environments
 Tying word vectors and word classifiers: A loss frame-work for language modeling,2016, arXiv preprint arXiv:1611
 Learning multiPle layers of features from tiny images,2009, 2009
 Communication efficient distributed machinelearning with the parameter server,2014, In Advances in Neural Information Processing Systems
 Building a large annotated corpus ofenglish: The penn treebank,1993, COMPUTATIONAL LINGUISTICS
 Communication-efficient learningof deep networks from decentralized data,2016, arXiv preprint arXiv:1602
 Recurrent neuralnetwork based language model,2010, In Interspeech
 Librispeech: an asr corpus based onpublic domain audio books,2015, In Acoustics
 Using the output embedding to improve language models,2016, arXiv preprintarXiv:1608
 On the momentum term in gradient descent learning algorithms,1999, Neural networks
 Hogwild: A lock-free approach to paral-lelizing stochastic gradient descent,2011, In Advances in neural information processing systems
 1-bit stochastic gradient descent and its applicationto data-parallel distributed training of speech dnns,2014, In Fifteenth Annual Conference of the InternationalSpeech Communication Association
 Scalable distributed dnn training using commodity gpu cloud computing,2015, In Sixteenth AnnualConference of the International Speech Communication Association
 Terngrad: Ternary gra-dients to reduce communication in distributed deep learning,2017, In Advances in Neural Information ProcessingSystems
 Petuum: A new platform for distributed machine learning on big data,2015, IEEETransactions on Big Data
 Dorefa-net: Training lowbitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv preprint arXiv:1606
 Parallelized stochastic gradient descent,2010, InAdvances in neural information processing systems
