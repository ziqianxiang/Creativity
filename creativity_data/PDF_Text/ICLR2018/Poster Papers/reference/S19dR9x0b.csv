title,year,conference
 Learning phrase representations using RNN encoder-decoder forstatistical machine translation,2014, arXiv:1406
 An overview of bilevel optimization,2007, Annals ofOperations Research
 Recurrentbatch normalization,2017, In ICLR
 Compressing deep convolutionalnetworks using vector quantization,2014, arXiv:1412
 Speech recognition with deep recurrentneural networks,2013, In ICASSP
 Network sketching: Exploiting binarystructure in deep cnns,2017, In CVPR
 Long short-term memory,1997, Neural Computation
 Loss-aware binarization of deep networks,2017, In ICLR
 Binarizedneural networks,2016, In NIPS
 Batch normalization: Accelerating deep network training byreducing internal covariate shift,2015, In ICML
 Speeding up convolutional neural networkswith low rank expansions,2014, arXiv:1405
 Adam: A method for stochastic optimization,2015, In ICLR
 Speeding-up convolutional neural networks using fine-tuned cp-decomposition,2014, arXiv:1412
 Performance guaranteednetwork acceleration via high-order residual quantization,2017, In ICCV
 Sparse convolu-tional neural networks,2015, In CVPR
 Building a large annotatedcorpus of english: The penn treebank,1993, Computational Linguistics
 Pointer sentinel mixturemodels,2017, In ICLR
 Statistical Language Models Based on Neural Networks,2012, PhD thesis
 Recurrentneural network based language model,2010, In INTERSPEECH
 XNOR-Net: Imagenetclassification using binary convolutional neural networks,2016, In ECCV
 Low-rank matrix factorization for deep neural network training with high-dimensional output targets,2013, InICASSP
 Very deep convolutional networks for large-scale imagerecognition,2015, In ICLR
 Show and tell: A neural imagecaption generator,2015, In CVPR
 Learning structured sparsity indeep neural networks,2016, In NIPS
 Datanoising as smoothing in neural network language models,2017, In ICLR
 Dorefa-net: Traininglow bitwidth convolutional neural networks with low bitwidth gradients,2016, arXiv:1606
 Trained ternary quantization,2017, In ICLR
