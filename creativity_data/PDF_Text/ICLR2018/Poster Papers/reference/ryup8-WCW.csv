title,year,conference
 Estimating the intrinsic dimension of data with afractal-based method,2002, IEEE Transactions on pattern analysis and machine intelligence
 Compressingneural networks with the hashing trick,2015, CoRR
 Big neural networks waste capacity,2013, CoRR
 Identifying and attacking the saddle point problem in high-dimensional non-convex op-timization,2014, CoRR
 Predicting parameters in deeplearning,2013, In Advances in Neural Information Processing Systems
 Understanding the difficulty of training deep feedforward neuralnetworks,2010, In Artificial Intelligence and Statistics
 Qualitatively characterizing neural networkoptimization problems,2015, In International Conference on Learning Representations (ICLR 2015)
 Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification,2015, In CVPR
 One model to learn them all,2017, CoRR
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Overcoming catastrophic forget-ting in neural networks,2017, Proceedings of the National Academy of Sciences
 Fastfood-approximating kernel expansions in Ioglineartime,2013, In ICML
 MaximUm likelihood estimation of intrinsic dimension,2005, InAdvances in neural information processing systems
 Very sparse random projections,2006, In KDD
 Faster r-cnn: Towards real-time objectdetection with region proposal networks,2015, In Advances in neural information processing systems
 Modeling by shortest data description,1978, Automatica
 A global geometric framework fornonlinear dimensionality reduction,2000, Science
 Learning structured sparsity indeep neural networks,2016, In NIPS
 Deep fried convnets,2015, In Proceedings of the IEEE International Conference on ComputerVision
 Understandingdeep learning requires rethinking generalization,2017, ICLR
 As seen in Fig,2018, 5
