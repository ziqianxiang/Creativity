title,year,conference
 Wasserstein gan,2017, arXiv preprint arXiv:1701
 The cramer distance as a solution to biased WaSSerStein gradients,2017, arXiv preprintarXiv:1705
 Fromoptimal transport to generative modeling: the vegan cookbook,2017, arXiv preprint arXiv:1705
 Foundations of mathematical economics,2001, MIT Press
 Sinkhorn distances: Lightspeed computation of optimal transport,2013, In Advances in NeuralInformation Processing Systems
 Language modeling With gated convolutionalnetWorks,2016, arXiv preprint arXiv:1612
 Gan and vae from an optimal transport point of view,2017, arXivpreprint arXiv:1706
 Sinkhorn-autodiff: Tractable Wasserstein learning of genera-tive models,2017, arXiv preprint arXiv:1706
 The reversible residual network: Back-propagation without storing activations,2017, In Advances in Neural Information Processing Systems
 Improved trainingof wasserstein gans,2017, arXiv preprint arXiv:1704
 Adam: A method for stochastic optimization,2014, arXiv preprintarXiv:1412
 Learning multiple layers of features from tiny images,2009, TeChniCal report
 Distributional adversarialnetworks,2017, arXiv:1706
 Unrolled generative adversarial networks,2017, InICLR
 Unsupervised representation learning with deep Convolutionalgenerative adversarial networks,2015, arXiv preprint arXiv:1511
 Learning whatand where to draw,2016, In NIPS
 Generativeadversarial text-to-image synthesis,2016, In ICML
 Weight normalization: A simple reparameterization to accelerate train-ing of deep neural networks,2016, In Advances in Neural Information Processing Systems
 Improvedtechniques for training gans,2016, In NIPS
 Equivalence of distance-basedand rkhs-based statistics in hypothesis testing,2013, The Annals of Statistics
 Understanding and improving convolutionalneural networks via concatenated rectified linear units,2016, In International Conference on Machine Learning
