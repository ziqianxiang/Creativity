Figure 1: Mujoco environments used in our experiments. Form left to right: Swimmer, Half Cheetah,Snake, Ant, Hopper, and Humanoid.
Figure 2: Learning curves of our method versus state-of-the-art methods. The horizontal axis, inlog-scale, indicates the number of time steps of real world data. The vertical axis denotes the averagereturn. These figures clearly demonstrate that our proposed method significantly outperforms othermethods in comparison (best viewed in color).
Figure 3: Comparison among different policy optimization techniques with one model. Using TRPOfor model-based optimization leads to the best policy learning across the different domains (Bestviewed in color).
Figure 4: Comparison among different number of models that the policy is trained on. TRPO is usedfor the policy optimization. We illustrate the improvement when using 5, 10 and 20 models over asingle model (Best viewed in color).
Figure 5: Predicted and real performances during the training process using our approach with onemodel instead of an ensemble in Swimmer. The policy overfits to the dynamics model which degradesthe real performance.
Figure 6:	Comparison among different sampling techniques for simulating roll-outs. By samplingeach step from a different model, we prevent overfitting and enhance the learning performance (Bestviewed in color).
Figure 7:	Comparison among validation techniques. The ensemble of models yields to goodperformance across environments (Best viewed in color).
